Begin epxeriment: noisy_level: 0.2 cont_weight: 50 epoch: der1:0.3 der2:0 dfr1:0.4 dfr2:0
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.0, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106380])
remove edge: torch.Size([2, 71058])
updated graph: torch.Size([2, 88790])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 530.2046508789062 = 1.090732455253601 + 50.0 * 10.58227825164795
Epoch 0, val loss: 1.0906001329421997
Epoch 10, training loss: 530.1845092773438 = 1.0869998931884766 + 50.0 * 10.581950187683105
Epoch 10, val loss: 1.0868686437606812
Epoch 20, training loss: 530.1159057617188 = 1.0829225778579712 + 50.0 * 10.580658912658691
Epoch 20, val loss: 1.0827921628952026
Epoch 30, training loss: 529.8323974609375 = 1.0784088373184204 + 50.0 * 10.575079917907715
Epoch 30, val loss: 1.0782684087753296
Epoch 40, training loss: 528.7142944335938 = 1.073463797569275 + 50.0 * 10.552816390991211
Epoch 40, val loss: 1.0733145475387573
Epoch 50, training loss: 525.3964233398438 = 1.0680348873138428 + 50.0 * 10.486567497253418
Epoch 50, val loss: 1.067879557609558
Epoch 60, training loss: 517.0105590820312 = 1.0622828006744385 + 50.0 * 10.318965911865234
Epoch 60, val loss: 1.062129020690918
Epoch 70, training loss: 498.1023864746094 = 1.0558817386627197 + 50.0 * 9.940930366516113
Epoch 70, val loss: 1.0557054281234741
Epoch 80, training loss: 477.2181091308594 = 1.0502386093139648 + 50.0 * 9.523357391357422
Epoch 80, val loss: 1.0504094362258911
Epoch 90, training loss: 466.88677978515625 = 1.04628586769104 + 50.0 * 9.31680965423584
Epoch 90, val loss: 1.0467159748077393
Epoch 100, training loss: 461.85107421875 = 1.0427991151809692 + 50.0 * 9.216165542602539
Epoch 100, val loss: 1.0433770418167114
Epoch 110, training loss: 460.50933837890625 = 1.0397260189056396 + 50.0 * 9.18939208984375
Epoch 110, val loss: 1.0404305458068848
Epoch 120, training loss: 458.5375061035156 = 1.0372809171676636 + 50.0 * 9.150004386901855
Epoch 120, val loss: 1.038191795349121
Epoch 130, training loss: 455.5334777832031 = 1.0362122058868408 + 50.0 * 9.089944839477539
Epoch 130, val loss: 1.0373821258544922
Epoch 140, training loss: 450.9974670410156 = 1.0363655090332031 + 50.0 * 8.999221801757812
Epoch 140, val loss: 1.0376269817352295
Epoch 150, training loss: 445.44476318359375 = 1.0372672080993652 + 50.0 * 8.888150215148926
Epoch 150, val loss: 1.0385043621063232
Epoch 160, training loss: 441.77728271484375 = 1.0378879308700562 + 50.0 * 8.814787864685059
Epoch 160, val loss: 1.0389695167541504
Epoch 170, training loss: 439.77728271484375 = 1.0376043319702148 + 50.0 * 8.77479362487793
Epoch 170, val loss: 1.0385193824768066
Epoch 180, training loss: 438.57763671875 = 1.036805272102356 + 50.0 * 8.750816345214844
Epoch 180, val loss: 1.0376940965652466
Epoch 190, training loss: 437.6396484375 = 1.0360363721847534 + 50.0 * 8.732071876525879
Epoch 190, val loss: 1.0370049476623535
Epoch 200, training loss: 436.6635437011719 = 1.0354571342468262 + 50.0 * 8.71256160736084
Epoch 200, val loss: 1.0364738702774048
Epoch 210, training loss: 435.71514892578125 = 1.0350213050842285 + 50.0 * 8.693602561950684
Epoch 210, val loss: 1.0360907316207886
Epoch 220, training loss: 434.9510803222656 = 1.034548044204712 + 50.0 * 8.678330421447754
Epoch 220, val loss: 1.035642385482788
Epoch 230, training loss: 434.3562316894531 = 1.0339235067367554 + 50.0 * 8.6664457321167
Epoch 230, val loss: 1.0350555181503296
Epoch 240, training loss: 433.779296875 = 1.0332435369491577 + 50.0 * 8.654921531677246
Epoch 240, val loss: 1.0344105958938599
Epoch 250, training loss: 433.11895751953125 = 1.0325851440429688 + 50.0 * 8.641727447509766
Epoch 250, val loss: 1.0338066816329956
Epoch 260, training loss: 432.4249572753906 = 1.0320427417755127 + 50.0 * 8.62785816192627
Epoch 260, val loss: 1.0333081483840942
Epoch 270, training loss: 431.70556640625 = 1.0315953493118286 + 50.0 * 8.613479614257812
Epoch 270, val loss: 1.0328587293624878
Epoch 280, training loss: 431.0164794921875 = 1.0311084985733032 + 50.0 * 8.59970760345459
Epoch 280, val loss: 1.032396674156189
Epoch 290, training loss: 430.3813171386719 = 1.0306062698364258 + 50.0 * 8.587014198303223
Epoch 290, val loss: 1.0318955183029175
Epoch 300, training loss: 429.7915954589844 = 1.0300233364105225 + 50.0 * 8.575231552124023
Epoch 300, val loss: 1.031327247619629
Epoch 310, training loss: 429.2752685546875 = 1.029400110244751 + 50.0 * 8.56491756439209
Epoch 310, val loss: 1.0307105779647827
Epoch 320, training loss: 428.906494140625 = 1.0287508964538574 + 50.0 * 8.557555198669434
Epoch 320, val loss: 1.0300853252410889
Epoch 330, training loss: 428.4740905761719 = 1.0280240774154663 + 50.0 * 8.548921585083008
Epoch 330, val loss: 1.0293546915054321
Epoch 340, training loss: 428.1100158691406 = 1.0272667407989502 + 50.0 * 8.541655540466309
Epoch 340, val loss: 1.0286201238632202
Epoch 350, training loss: 427.8011474609375 = 1.0264910459518433 + 50.0 * 8.535492897033691
Epoch 350, val loss: 1.0278650522232056
Epoch 360, training loss: 427.51544189453125 = 1.0256860256195068 + 50.0 * 8.529794692993164
Epoch 360, val loss: 1.027084231376648
Epoch 370, training loss: 427.2658386230469 = 1.0248624086380005 + 50.0 * 8.524819374084473
Epoch 370, val loss: 1.0262938737869263
Epoch 380, training loss: 427.1347961425781 = 1.0239653587341309 + 50.0 * 8.522216796875
Epoch 380, val loss: 1.0254039764404297
Epoch 390, training loss: 426.84844970703125 = 1.0229965448379517 + 50.0 * 8.516509056091309
Epoch 390, val loss: 1.0244548320770264
Epoch 400, training loss: 426.60992431640625 = 1.022047996520996 + 50.0 * 8.511757850646973
Epoch 400, val loss: 1.023542881011963
Epoch 410, training loss: 426.3962707519531 = 1.021085262298584 + 50.0 * 8.507503509521484
Epoch 410, val loss: 1.0226000547409058
Epoch 420, training loss: 426.1946105957031 = 1.0200824737548828 + 50.0 * 8.503490447998047
Epoch 420, val loss: 1.021622896194458
Epoch 430, training loss: 426.0013732910156 = 1.0190088748931885 + 50.0 * 8.49964714050293
Epoch 430, val loss: 1.0205755233764648
Epoch 440, training loss: 425.810302734375 = 1.017897129058838 + 50.0 * 8.495848655700684
Epoch 440, val loss: 1.0194979906082153
Epoch 450, training loss: 425.5812683105469 = 1.0168144702911377 + 50.0 * 8.491289138793945
Epoch 450, val loss: 1.0184446573257446
Epoch 460, training loss: 425.37493896484375 = 1.0157021284103394 + 50.0 * 8.487184524536133
Epoch 460, val loss: 1.0173592567443848
Epoch 470, training loss: 425.2038269042969 = 1.014550805091858 + 50.0 * 8.483785629272461
Epoch 470, val loss: 1.0162347555160522
Epoch 480, training loss: 425.0608825683594 = 1.0132887363433838 + 50.0 * 8.480952262878418
Epoch 480, val loss: 1.015014410018921
Epoch 490, training loss: 424.88531494140625 = 1.011991024017334 + 50.0 * 8.477466583251953
Epoch 490, val loss: 1.013754963874817
Epoch 500, training loss: 424.6906433105469 = 1.0107049942016602 + 50.0 * 8.47359848022461
Epoch 500, val loss: 1.0125277042388916
Epoch 510, training loss: 424.5643310546875 = 1.0094115734100342 + 50.0 * 8.471098899841309
Epoch 510, val loss: 1.0112850666046143
Epoch 520, training loss: 424.4768981933594 = 1.008016586303711 + 50.0 * 8.469377517700195
Epoch 520, val loss: 1.0099365711212158
Epoch 530, training loss: 424.3560791015625 = 1.0065557956695557 + 50.0 * 8.46699047088623
Epoch 530, val loss: 1.008545160293579
Epoch 540, training loss: 424.1768493652344 = 1.0051344633102417 + 50.0 * 8.463434219360352
Epoch 540, val loss: 1.0071771144866943
Epoch 550, training loss: 424.043701171875 = 1.0036903619766235 + 50.0 * 8.460800170898438
Epoch 550, val loss: 1.0057933330535889
Epoch 560, training loss: 423.9305114746094 = 1.0022156238555908 + 50.0 * 8.458565711975098
Epoch 560, val loss: 1.004388451576233
Epoch 570, training loss: 423.820068359375 = 1.000699758529663 + 50.0 * 8.456387519836426
Epoch 570, val loss: 1.0029456615447998
Epoch 580, training loss: 423.7666015625 = 0.9991270303726196 + 50.0 * 8.455349922180176
Epoch 580, val loss: 1.0014344453811646
Epoch 590, training loss: 423.76080322265625 = 0.9974018335342407 + 50.0 * 8.455267906188965
Epoch 590, val loss: 0.9998209476470947
Epoch 600, training loss: 423.5672607421875 = 0.9956918954849243 + 50.0 * 8.451431274414062
Epoch 600, val loss: 0.9981891512870789
Epoch 610, training loss: 423.4389343261719 = 0.9940096139907837 + 50.0 * 8.448898315429688
Epoch 610, val loss: 0.9965817928314209
Epoch 620, training loss: 423.3358154296875 = 0.9923020005226135 + 50.0 * 8.446869850158691
Epoch 620, val loss: 0.994950532913208
Epoch 630, training loss: 423.4745788574219 = 0.9905566573143005 + 50.0 * 8.44968032836914
Epoch 630, val loss: 0.9932688474655151
Epoch 640, training loss: 423.22918701171875 = 0.988662838935852 + 50.0 * 8.444809913635254
Epoch 640, val loss: 0.9914731979370117
Epoch 650, training loss: 423.07562255859375 = 0.986808717250824 + 50.0 * 8.441776275634766
Epoch 650, val loss: 0.9896902441978455
Epoch 660, training loss: 422.9911193847656 = 0.9849509000778198 + 50.0 * 8.440123558044434
Epoch 660, val loss: 0.9879218339920044
Epoch 670, training loss: 422.9014892578125 = 0.9830542802810669 + 50.0 * 8.438368797302246
Epoch 670, val loss: 0.9861008524894714
Epoch 680, training loss: 422.8273620605469 = 0.9811186790466309 + 50.0 * 8.436924934387207
Epoch 680, val loss: 0.9842525124549866
Epoch 690, training loss: 422.9181213378906 = 0.9790995121002197 + 50.0 * 8.438780784606934
Epoch 690, val loss: 0.982289731502533
Epoch 700, training loss: 422.7156982421875 = 0.9769778847694397 + 50.0 * 8.434774398803711
Epoch 700, val loss: 0.9802689552307129
Epoch 710, training loss: 422.5827331542969 = 0.974928081035614 + 50.0 * 8.432156562805176
Epoch 710, val loss: 0.978321373462677
Epoch 720, training loss: 422.5302429199219 = 0.9728800058364868 + 50.0 * 8.431147575378418
Epoch 720, val loss: 0.9763725399971008
Epoch 730, training loss: 422.424560546875 = 0.9706999659538269 + 50.0 * 8.4290771484375
Epoch 730, val loss: 0.9742501974105835
Epoch 740, training loss: 422.3747863769531 = 0.968504786491394 + 50.0 * 8.428125381469727
Epoch 740, val loss: 0.972172737121582
Epoch 750, training loss: 422.2524719238281 = 0.9663792848587036 + 50.0 * 8.425722122192383
Epoch 750, val loss: 0.9701327085494995
Epoch 760, training loss: 422.15362548828125 = 0.9642213582992554 + 50.0 * 8.423788070678711
Epoch 760, val loss: 0.9680566787719727
Epoch 770, training loss: 422.06646728515625 = 0.9620495438575745 + 50.0 * 8.422088623046875
Epoch 770, val loss: 0.9659740328788757
Epoch 780, training loss: 422.1886901855469 = 0.9598302245140076 + 50.0 * 8.424576759338379
Epoch 780, val loss: 0.9638208746910095
Epoch 790, training loss: 421.9703369140625 = 0.9573628306388855 + 50.0 * 8.420259475708008
Epoch 790, val loss: 0.9614310264587402
Epoch 800, training loss: 421.8215637207031 = 0.9550024271011353 + 50.0 * 8.417330741882324
Epoch 800, val loss: 0.9591814875602722
Epoch 810, training loss: 421.74896240234375 = 0.9526633620262146 + 50.0 * 8.415925979614258
Epoch 810, val loss: 0.956945538520813
Epoch 820, training loss: 421.8132629394531 = 0.950253963470459 + 50.0 * 8.41726016998291
Epoch 820, val loss: 0.9546414613723755
Epoch 830, training loss: 421.6454772949219 = 0.9477000832557678 + 50.0 * 8.413955688476562
Epoch 830, val loss: 0.9521294236183167
Epoch 840, training loss: 421.57342529296875 = 0.9451404809951782 + 50.0 * 8.412566184997559
Epoch 840, val loss: 0.9496846795082092
Epoch 850, training loss: 421.4881896972656 = 0.9425811171531677 + 50.0 * 8.41091251373291
Epoch 850, val loss: 0.9471902251243591
Epoch 860, training loss: 421.41131591796875 = 0.9399642944335938 + 50.0 * 8.40942668914795
Epoch 860, val loss: 0.9446772336959839
Epoch 870, training loss: 421.36480712890625 = 0.9372864365577698 + 50.0 * 8.408550262451172
Epoch 870, val loss: 0.9420876502990723
Epoch 880, training loss: 421.4068603515625 = 0.934501588344574 + 50.0 * 8.409446716308594
Epoch 880, val loss: 0.9393883943557739
Epoch 890, training loss: 421.2608947753906 = 0.9315997362136841 + 50.0 * 8.406585693359375
Epoch 890, val loss: 0.9365723133087158
Epoch 900, training loss: 421.2911376953125 = 0.9287073612213135 + 50.0 * 8.407248497009277
Epoch 900, val loss: 0.9337456822395325
Epoch 910, training loss: 421.2123718261719 = 0.9256427884101868 + 50.0 * 8.40573501586914
Epoch 910, val loss: 0.9307985305786133
Epoch 920, training loss: 421.108154296875 = 0.9226499795913696 + 50.0 * 8.40371036529541
Epoch 920, val loss: 0.9279145002365112
Epoch 930, training loss: 421.0626220703125 = 0.9196600317955017 + 50.0 * 8.402859687805176
Epoch 930, val loss: 0.9249984622001648
Epoch 940, training loss: 421.0059509277344 = 0.9166086912155151 + 50.0 * 8.401786804199219
Epoch 940, val loss: 0.9220498204231262
Epoch 950, training loss: 420.9882507324219 = 0.9135114550590515 + 50.0 * 8.401494979858398
Epoch 950, val loss: 0.9190579056739807
Epoch 960, training loss: 421.0761413574219 = 0.9102725386619568 + 50.0 * 8.40331745147705
Epoch 960, val loss: 0.9159017205238342
Epoch 970, training loss: 420.9312438964844 = 0.9069778323173523 + 50.0 * 8.400485038757324
Epoch 970, val loss: 0.9126893877983093
Epoch 980, training loss: 420.84454345703125 = 0.9037009477615356 + 50.0 * 8.39881706237793
Epoch 980, val loss: 0.9095333814620972
Epoch 990, training loss: 420.7989807128906 = 0.9004152417182922 + 50.0 * 8.397971153259277
Epoch 990, val loss: 0.9063431620597839
Epoch 1000, training loss: 420.92730712890625 = 0.8970102667808533 + 50.0 * 8.400606155395508
Epoch 1000, val loss: 0.903013288974762
Epoch 1010, training loss: 420.75494384765625 = 0.8934930562973022 + 50.0 * 8.397229194641113
Epoch 1010, val loss: 0.8996454477310181
Epoch 1020, training loss: 420.6645202636719 = 0.8900803327560425 + 50.0 * 8.395488739013672
Epoch 1020, val loss: 0.896327793598175
Epoch 1030, training loss: 420.62896728515625 = 0.886652410030365 + 50.0 * 8.394845962524414
Epoch 1030, val loss: 0.8930065035820007
Epoch 1040, training loss: 420.6635437011719 = 0.8831623196601868 + 50.0 * 8.395607948303223
Epoch 1040, val loss: 0.8896272778511047
Epoch 1050, training loss: 420.57635498046875 = 0.8793795704841614 + 50.0 * 8.393939018249512
Epoch 1050, val loss: 0.8859059810638428
Epoch 1060, training loss: 420.5688171386719 = 0.8756776452064514 + 50.0 * 8.3938627243042
Epoch 1060, val loss: 0.8823517560958862
Epoch 1070, training loss: 420.4916076660156 = 0.8720909357070923 + 50.0 * 8.392390251159668
Epoch 1070, val loss: 0.8788671493530273
Epoch 1080, training loss: 420.430419921875 = 0.8684547543525696 + 50.0 * 8.391239166259766
Epoch 1080, val loss: 0.8753397464752197
Epoch 1090, training loss: 420.39593505859375 = 0.8647875189781189 + 50.0 * 8.390623092651367
Epoch 1090, val loss: 0.8717808127403259
Epoch 1100, training loss: 420.3558349609375 = 0.8610630035400391 + 50.0 * 8.38989543914795
Epoch 1100, val loss: 0.8681526780128479
Epoch 1110, training loss: 420.3428955078125 = 0.857268214225769 + 50.0 * 8.3897123336792
Epoch 1110, val loss: 0.8644691109657288
Epoch 1120, training loss: 420.3169860839844 = 0.8532950282096863 + 50.0 * 8.389273643493652
Epoch 1120, val loss: 0.8605487942695618
Epoch 1130, training loss: 420.3998107910156 = 0.8493410348892212 + 50.0 * 8.391009330749512
Epoch 1130, val loss: 0.8567464351654053
Epoch 1140, training loss: 420.2557067871094 = 0.8454236388206482 + 50.0 * 8.388205528259277
Epoch 1140, val loss: 0.8529658913612366
Epoch 1150, training loss: 420.20703125 = 0.8415856957435608 + 50.0 * 8.387309074401855
Epoch 1150, val loss: 0.8492531180381775
Epoch 1160, training loss: 420.14947509765625 = 0.8377714157104492 + 50.0 * 8.386234283447266
Epoch 1160, val loss: 0.8455725312232971
Epoch 1170, training loss: 420.1255187988281 = 0.8338919878005981 + 50.0 * 8.385832786560059
Epoch 1170, val loss: 0.841809868812561
Epoch 1180, training loss: 420.232666015625 = 0.8299388885498047 + 50.0 * 8.388054847717285
Epoch 1180, val loss: 0.8379859924316406
Epoch 1190, training loss: 420.0549011230469 = 0.8258803486824036 + 50.0 * 8.384580612182617
Epoch 1190, val loss: 0.8340663313865662
Epoch 1200, training loss: 420.01434326171875 = 0.8219015002250671 + 50.0 * 8.383849143981934
Epoch 1200, val loss: 0.8302316665649414
Epoch 1210, training loss: 419.9911804199219 = 0.8179314732551575 + 50.0 * 8.383464813232422
Epoch 1210, val loss: 0.8264035582542419
Epoch 1220, training loss: 419.95703125 = 0.8139505982398987 + 50.0 * 8.382861137390137
Epoch 1220, val loss: 0.8225669860839844
Epoch 1230, training loss: 419.970458984375 = 0.809929370880127 + 50.0 * 8.383210182189941
Epoch 1230, val loss: 0.8186818361282349
Epoch 1240, training loss: 419.96612548828125 = 0.8057676553726196 + 50.0 * 8.383207321166992
Epoch 1240, val loss: 0.8146523833274841
Epoch 1250, training loss: 419.8979797363281 = 0.8016074299812317 + 50.0 * 8.381927490234375
Epoch 1250, val loss: 0.8106528520584106
Epoch 1260, training loss: 419.8564453125 = 0.7975035905838013 + 50.0 * 8.381178855895996
Epoch 1260, val loss: 0.8067229986190796
Epoch 1270, training loss: 419.8161315917969 = 0.7934324741363525 + 50.0 * 8.380454063415527
Epoch 1270, val loss: 0.802813708782196
Epoch 1280, training loss: 419.830810546875 = 0.7893627285957336 + 50.0 * 8.380828857421875
Epoch 1280, val loss: 0.7989101409912109
Epoch 1290, training loss: 419.8755798339844 = 0.7851247787475586 + 50.0 * 8.38180923461914
Epoch 1290, val loss: 0.7948211431503296
Epoch 1300, training loss: 419.7469787597656 = 0.7809174656867981 + 50.0 * 8.379321098327637
Epoch 1300, val loss: 0.7908045649528503
Epoch 1310, training loss: 419.7120056152344 = 0.7767931222915649 + 50.0 * 8.378704071044922
Epoch 1310, val loss: 0.7868726849555969
Epoch 1320, training loss: 419.68731689453125 = 0.7727038860321045 + 50.0 * 8.378292083740234
Epoch 1320, val loss: 0.7829492092132568
Epoch 1330, training loss: 419.6619567871094 = 0.7686044573783875 + 50.0 * 8.377866744995117
Epoch 1330, val loss: 0.7790502309799194
Epoch 1340, training loss: 419.6423034667969 = 0.7644791007041931 + 50.0 * 8.377556800842285
Epoch 1340, val loss: 0.7751039266586304
Epoch 1350, training loss: 419.81549072265625 = 0.7603136301040649 + 50.0 * 8.381103515625
Epoch 1350, val loss: 0.7711536884307861
Epoch 1360, training loss: 419.71173095703125 = 0.7560170292854309 + 50.0 * 8.379114151000977
Epoch 1360, val loss: 0.7669504880905151
Epoch 1370, training loss: 419.58331298828125 = 0.7518300414085388 + 50.0 * 8.376629829406738
Epoch 1370, val loss: 0.7630230784416199
Epoch 1380, training loss: 419.5679931640625 = 0.7477344870567322 + 50.0 * 8.376404762268066
Epoch 1380, val loss: 0.7591512799263
Epoch 1390, training loss: 419.55279541015625 = 0.7436605095863342 + 50.0 * 8.376182556152344
Epoch 1390, val loss: 0.7552756071090698
Epoch 1400, training loss: 419.5743103027344 = 0.7395379543304443 + 50.0 * 8.37669563293457
Epoch 1400, val loss: 0.7513420581817627
Epoch 1410, training loss: 419.6299133300781 = 0.7353919744491577 + 50.0 * 8.377890586853027
Epoch 1410, val loss: 0.7474043369293213
Epoch 1420, training loss: 419.4986572265625 = 0.7312155365943909 + 50.0 * 8.375349044799805
Epoch 1420, val loss: 0.7434586882591248
Epoch 1430, training loss: 419.4741516113281 = 0.7271562814712524 + 50.0 * 8.374939918518066
Epoch 1430, val loss: 0.7396228909492493
Epoch 1440, training loss: 419.4255676269531 = 0.7231239676475525 + 50.0 * 8.374049186706543
Epoch 1440, val loss: 0.7358224987983704
Epoch 1450, training loss: 419.4216613769531 = 0.719130277633667 + 50.0 * 8.374051094055176
Epoch 1450, val loss: 0.7320380806922913
Epoch 1460, training loss: 419.5800476074219 = 0.7150988578796387 + 50.0 * 8.377299308776855
Epoch 1460, val loss: 0.7282459139823914
Epoch 1470, training loss: 419.44464111328125 = 0.7109333276748657 + 50.0 * 8.374673843383789
Epoch 1470, val loss: 0.7242910265922546
Epoch 1480, training loss: 419.3750305175781 = 0.7069197297096252 + 50.0 * 8.37336254119873
Epoch 1480, val loss: 0.7205526828765869
Epoch 1490, training loss: 419.34490966796875 = 0.703016996383667 + 50.0 * 8.372838020324707
Epoch 1490, val loss: 0.7168980240821838
Epoch 1500, training loss: 419.3194580078125 = 0.6991581320762634 + 50.0 * 8.372406005859375
Epoch 1500, val loss: 0.7132930159568787
Epoch 1510, training loss: 419.3381042480469 = 0.6952913403511047 + 50.0 * 8.372856140136719
Epoch 1510, val loss: 0.7096741199493408
Epoch 1520, training loss: 419.2939147949219 = 0.6913954615592957 + 50.0 * 8.372050285339355
Epoch 1520, val loss: 0.7060296535491943
Epoch 1530, training loss: 419.24786376953125 = 0.6875782608985901 + 50.0 * 8.37120532989502
Epoch 1530, val loss: 0.7024847269058228
Epoch 1540, training loss: 419.2450866699219 = 0.6838001608848572 + 50.0 * 8.371225357055664
Epoch 1540, val loss: 0.6989613771438599
Epoch 1550, training loss: 419.2834777832031 = 0.6800200343132019 + 50.0 * 8.372069358825684
Epoch 1550, val loss: 0.6954262852668762
Epoch 1560, training loss: 419.20538330078125 = 0.6762236952781677 + 50.0 * 8.370583534240723
Epoch 1560, val loss: 0.6919063329696655
Epoch 1570, training loss: 419.2561340332031 = 0.6725088953971863 + 50.0 * 8.371672630310059
Epoch 1570, val loss: 0.6884334683418274
Epoch 1580, training loss: 419.1980895996094 = 0.6687204241752625 + 50.0 * 8.370587348937988
Epoch 1580, val loss: 0.684969425201416
Epoch 1590, training loss: 419.1396179199219 = 0.6650542616844177 + 50.0 * 8.369491577148438
Epoch 1590, val loss: 0.6815767288208008
Epoch 1600, training loss: 419.1106262207031 = 0.6614685654640198 + 50.0 * 8.368983268737793
Epoch 1600, val loss: 0.6783300042152405
Epoch 1610, training loss: 419.09381103515625 = 0.6579596996307373 + 50.0 * 8.368717193603516
Epoch 1610, val loss: 0.6750615239143372
Epoch 1620, training loss: 419.21441650390625 = 0.6544371843338013 + 50.0 * 8.371199607849121
Epoch 1620, val loss: 0.6718513369560242
Epoch 1630, training loss: 419.07525634765625 = 0.6508374810218811 + 50.0 * 8.368488311767578
Epoch 1630, val loss: 0.6685308218002319
Epoch 1640, training loss: 419.0428771972656 = 0.6473649740219116 + 50.0 * 8.367910385131836
Epoch 1640, val loss: 0.665307879447937
Epoch 1650, training loss: 419.01708984375 = 0.6439791917800903 + 50.0 * 8.367462158203125
Epoch 1650, val loss: 0.6622256636619568
Epoch 1660, training loss: 419.0320739746094 = 0.6406458020210266 + 50.0 * 8.367828369140625
Epoch 1660, val loss: 0.6591888666152954
Epoch 1670, training loss: 419.0497131347656 = 0.6373110413551331 + 50.0 * 8.368247985839844
Epoch 1670, val loss: 0.6561384201049805
Epoch 1680, training loss: 418.9699401855469 = 0.633954644203186 + 50.0 * 8.366720199584961
Epoch 1680, val loss: 0.6531184911727905
Epoch 1690, training loss: 418.9451599121094 = 0.6307162046432495 + 50.0 * 8.366289138793945
Epoch 1690, val loss: 0.6501461863517761
Epoch 1700, training loss: 418.9233093261719 = 0.6275320649147034 + 50.0 * 8.365915298461914
Epoch 1700, val loss: 0.6472790241241455
Epoch 1710, training loss: 418.98968505859375 = 0.6243764758110046 + 50.0 * 8.367305755615234
Epoch 1710, val loss: 0.644471287727356
Epoch 1720, training loss: 418.9350280761719 = 0.6211568117141724 + 50.0 * 8.366277694702148
Epoch 1720, val loss: 0.6414710283279419
Epoch 1730, training loss: 418.8890075683594 = 0.6180420517921448 + 50.0 * 8.365419387817383
Epoch 1730, val loss: 0.6387017369270325
Epoch 1740, training loss: 418.8599548339844 = 0.6150079369544983 + 50.0 * 8.364898681640625
Epoch 1740, val loss: 0.635995090007782
Epoch 1750, training loss: 418.8387451171875 = 0.6120534539222717 + 50.0 * 8.364533424377441
Epoch 1750, val loss: 0.6333189606666565
Epoch 1760, training loss: 418.8981628417969 = 0.6091266870498657 + 50.0 * 8.3657808303833
Epoch 1760, val loss: 0.6307482719421387
Epoch 1770, training loss: 418.8445739746094 = 0.6060933470726013 + 50.0 * 8.36476993560791
Epoch 1770, val loss: 0.6279469728469849
Epoch 1780, training loss: 418.8141784667969 = 0.6031479239463806 + 50.0 * 8.36422061920166
Epoch 1780, val loss: 0.6254298686981201
Epoch 1790, training loss: 418.7900085449219 = 0.600341260433197 + 50.0 * 8.36379337310791
Epoch 1790, val loss: 0.6228429675102234
Epoch 1800, training loss: 418.764892578125 = 0.5975832939147949 + 50.0 * 8.363346099853516
Epoch 1800, val loss: 0.6204425692558289
Epoch 1810, training loss: 418.7782897949219 = 0.5948737263679504 + 50.0 * 8.363668441772461
Epoch 1810, val loss: 0.6179959177970886
Epoch 1820, training loss: 418.85260009765625 = 0.5921341180801392 + 50.0 * 8.365209579467773
Epoch 1820, val loss: 0.6156098246574402
Epoch 1830, training loss: 418.7971496582031 = 0.5893554091453552 + 50.0 * 8.364155769348145
Epoch 1830, val loss: 0.6132123470306396
Epoch 1840, training loss: 418.7079162597656 = 0.5867182016372681 + 50.0 * 8.36242389678955
Epoch 1840, val loss: 0.6108447909355164
Epoch 1850, training loss: 418.6986083984375 = 0.5841473340988159 + 50.0 * 8.362289428710938
Epoch 1850, val loss: 0.6085925102233887
Epoch 1860, training loss: 418.6780090332031 = 0.5816188454627991 + 50.0 * 8.36192798614502
Epoch 1860, val loss: 0.606394350528717
Epoch 1870, training loss: 418.68597412109375 = 0.5791345238685608 + 50.0 * 8.362136840820312
Epoch 1870, val loss: 0.6042179465293884
Epoch 1880, training loss: 418.89227294921875 = 0.5766271948814392 + 50.0 * 8.366312980651855
Epoch 1880, val loss: 0.6019732356071472
Epoch 1890, training loss: 418.69110107421875 = 0.5740736126899719 + 50.0 * 8.362340927124023
Epoch 1890, val loss: 0.599861204624176
Epoch 1900, training loss: 418.63134765625 = 0.5716668963432312 + 50.0 * 8.361193656921387
Epoch 1900, val loss: 0.5977190732955933
Epoch 1910, training loss: 418.6069641113281 = 0.5693204998970032 + 50.0 * 8.360753059387207
Epoch 1910, val loss: 0.5957373976707458
Epoch 1920, training loss: 418.60272216796875 = 0.5670217275619507 + 50.0 * 8.360713958740234
Epoch 1920, val loss: 0.5937584042549133
Epoch 1930, training loss: 418.7473449707031 = 0.5647366642951965 + 50.0 * 8.363652229309082
Epoch 1930, val loss: 0.5917540788650513
Epoch 1940, training loss: 418.6644592285156 = 0.562383234500885 + 50.0 * 8.362041473388672
Epoch 1940, val loss: 0.5897610187530518
Epoch 1950, training loss: 418.5968322753906 = 0.5601290464401245 + 50.0 * 8.360733985900879
Epoch 1950, val loss: 0.5878435969352722
Epoch 1960, training loss: 418.54937744140625 = 0.5579734444618225 + 50.0 * 8.359827995300293
Epoch 1960, val loss: 0.5859981179237366
Epoch 1970, training loss: 418.5442810058594 = 0.5558605790138245 + 50.0 * 8.359768867492676
Epoch 1970, val loss: 0.5842217206954956
Epoch 1980, training loss: 418.60498046875 = 0.5537738800048828 + 50.0 * 8.361023902893066
Epoch 1980, val loss: 0.5824283361434937
Epoch 1990, training loss: 418.65228271484375 = 0.551604151725769 + 50.0 * 8.362013816833496
Epoch 1990, val loss: 0.5805733799934387
Epoch 2000, training loss: 418.5218505859375 = 0.549469530582428 + 50.0 * 8.359447479248047
Epoch 2000, val loss: 0.5788555145263672
Epoch 2010, training loss: 418.4870910644531 = 0.5474722981452942 + 50.0 * 8.358792304992676
Epoch 2010, val loss: 0.5771154761314392
Epoch 2020, training loss: 418.4706115722656 = 0.5455228686332703 + 50.0 * 8.358501434326172
Epoch 2020, val loss: 0.5755358934402466
Epoch 2030, training loss: 418.4604187011719 = 0.5436096787452698 + 50.0 * 8.358336448669434
Epoch 2030, val loss: 0.5739535093307495
Epoch 2040, training loss: 418.4524841308594 = 0.5417152047157288 + 50.0 * 8.35821533203125
Epoch 2040, val loss: 0.572395920753479
Epoch 2050, training loss: 418.5357666015625 = 0.5398352742195129 + 50.0 * 8.359918594360352
Epoch 2050, val loss: 0.5708956122398376
Epoch 2060, training loss: 418.53302001953125 = 0.5379117131233215 + 50.0 * 8.359902381896973
Epoch 2060, val loss: 0.5690740942955017
Epoch 2070, training loss: 418.46649169921875 = 0.5359877943992615 + 50.0 * 8.358610153198242
Epoch 2070, val loss: 0.5676949620246887
Epoch 2080, training loss: 418.425048828125 = 0.534193217754364 + 50.0 * 8.357817649841309
Epoch 2080, val loss: 0.5661339163780212
Epoch 2090, training loss: 418.3955383300781 = 0.5324455499649048 + 50.0 * 8.357261657714844
Epoch 2090, val loss: 0.5647538900375366
Epoch 2100, training loss: 418.3753967285156 = 0.5307409167289734 + 50.0 * 8.356893539428711
Epoch 2100, val loss: 0.563383936882019
Epoch 2110, training loss: 418.37115478515625 = 0.5290532112121582 + 50.0 * 8.356842041015625
Epoch 2110, val loss: 0.5620245933532715
Epoch 2120, training loss: 418.6361999511719 = 0.5273475646972656 + 50.0 * 8.362176895141602
Epoch 2120, val loss: 0.5607335567474365
Epoch 2130, training loss: 418.4629211425781 = 0.5255972146987915 + 50.0 * 8.358746528625488
Epoch 2130, val loss: 0.5591936707496643
Epoch 2140, training loss: 418.3716125488281 = 0.5239181518554688 + 50.0 * 8.356953620910645
Epoch 2140, val loss: 0.5578633546829224
Epoch 2150, training loss: 418.32177734375 = 0.5223329067230225 + 50.0 * 8.355988502502441
Epoch 2150, val loss: 0.5566563606262207
Epoch 2160, training loss: 418.31085205078125 = 0.520799458026886 + 50.0 * 8.35580062866211
Epoch 2160, val loss: 0.5553895235061646
Epoch 2170, training loss: 418.3209228515625 = 0.5192683339118958 + 50.0 * 8.356033325195312
Epoch 2170, val loss: 0.5542107224464417
Epoch 2180, training loss: 418.4677429199219 = 0.5177240967750549 + 50.0 * 8.359000205993652
Epoch 2180, val loss: 0.5529146194458008
Epoch 2190, training loss: 418.33062744140625 = 0.5161793828010559 + 50.0 * 8.35628890991211
Epoch 2190, val loss: 0.5517985820770264
Epoch 2200, training loss: 418.2801208496094 = 0.5146902203559875 + 50.0 * 8.355308532714844
Epoch 2200, val loss: 0.5506057739257812
Epoch 2210, training loss: 418.29083251953125 = 0.5132514834403992 + 50.0 * 8.355551719665527
Epoch 2210, val loss: 0.5494749546051025
Epoch 2220, training loss: 418.4826354980469 = 0.511782705783844 + 50.0 * 8.359416961669922
Epoch 2220, val loss: 0.5482969880104065
Epoch 2230, training loss: 418.304931640625 = 0.5102947354316711 + 50.0 * 8.3558931350708
Epoch 2230, val loss: 0.5472320914268494
Epoch 2240, training loss: 418.2368469238281 = 0.5089085102081299 + 50.0 * 8.354558944702148
Epoch 2240, val loss: 0.546123206615448
Epoch 2250, training loss: 418.20819091796875 = 0.5075533390045166 + 50.0 * 8.354012489318848
Epoch 2250, val loss: 0.545128583908081
Epoch 2260, training loss: 418.2008972167969 = 0.5062216520309448 + 50.0 * 8.353893280029297
Epoch 2260, val loss: 0.5441054701805115
Epoch 2270, training loss: 418.20892333984375 = 0.504909336566925 + 50.0 * 8.354080200195312
Epoch 2270, val loss: 0.5431387424468994
Epoch 2280, training loss: 418.33843994140625 = 0.5035420656204224 + 50.0 * 8.356698036193848
Epoch 2280, val loss: 0.5421286821365356
Epoch 2290, training loss: 418.1875 = 0.5021860003471375 + 50.0 * 8.353706359863281
Epoch 2290, val loss: 0.5410518050193787
Epoch 2300, training loss: 418.1775207519531 = 0.5009057521820068 + 50.0 * 8.353531837463379
Epoch 2300, val loss: 0.5400926470756531
Epoch 2310, training loss: 418.1476135253906 = 0.4996734857559204 + 50.0 * 8.352958679199219
Epoch 2310, val loss: 0.5391802191734314
Epoch 2320, training loss: 418.13995361328125 = 0.49845829606056213 + 50.0 * 8.352829933166504
Epoch 2320, val loss: 0.5383074879646301
Epoch 2330, training loss: 418.31976318359375 = 0.4972643554210663 + 50.0 * 8.356450080871582
Epoch 2330, val loss: 0.5373471975326538
Epoch 2340, training loss: 418.1486511230469 = 0.4959118068218231 + 50.0 * 8.353055000305176
Epoch 2340, val loss: 0.5364565849304199
Epoch 2350, training loss: 418.1939697265625 = 0.49469149112701416 + 50.0 * 8.353985786437988
Epoch 2350, val loss: 0.5354791283607483
Epoch 2360, training loss: 418.1147155761719 = 0.4935140907764435 + 50.0 * 8.352424621582031
Epoch 2360, val loss: 0.5346871018409729
Epoch 2370, training loss: 418.08660888671875 = 0.4924028515815735 + 50.0 * 8.351883888244629
Epoch 2370, val loss: 0.5338987112045288
Epoch 2380, training loss: 418.0776672363281 = 0.4912986755371094 + 50.0 * 8.351727485656738
Epoch 2380, val loss: 0.533084511756897
Epoch 2390, training loss: 418.0672912597656 = 0.49020329117774963 + 50.0 * 8.351541519165039
Epoch 2390, val loss: 0.5323387980461121
Epoch 2400, training loss: 418.05755615234375 = 0.48911580443382263 + 50.0 * 8.35136890411377
Epoch 2400, val loss: 0.531568169593811
Epoch 2410, training loss: 418.1212463378906 = 0.48803001642227173 + 50.0 * 8.35266399383545
Epoch 2410, val loss: 0.5307856202125549
Epoch 2420, training loss: 418.0601806640625 = 0.48688623309135437 + 50.0 * 8.351466178894043
Epoch 2420, val loss: 0.5299811959266663
Epoch 2430, training loss: 418.1059265136719 = 0.4857499599456787 + 50.0 * 8.35240364074707
Epoch 2430, val loss: 0.5291688442230225
Epoch 2440, training loss: 418.0525207519531 = 0.4847038686275482 + 50.0 * 8.351356506347656
Epoch 2440, val loss: 0.5284391641616821
Epoch 2450, training loss: 418.01263427734375 = 0.4836970567703247 + 50.0 * 8.350578308105469
Epoch 2450, val loss: 0.5277775526046753
Epoch 2460, training loss: 418.0079345703125 = 0.48270735144615173 + 50.0 * 8.350504875183105
Epoch 2460, val loss: 0.5271141529083252
Epoch 2470, training loss: 417.9947204589844 = 0.4817214608192444 + 50.0 * 8.350259780883789
Epoch 2470, val loss: 0.5264423489570618
Epoch 2480, training loss: 417.9876708984375 = 0.4807426333427429 + 50.0 * 8.350138664245605
Epoch 2480, val loss: 0.5257776379585266
Epoch 2490, training loss: 418.0958557128906 = 0.4797680675983429 + 50.0 * 8.35232162475586
Epoch 2490, val loss: 0.5251724720001221
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7899543378995433
0.8133014562051729
=== training gcn model ===
Epoch 0, training loss: 530.2154541015625 = 1.1031142473220825 + 50.0 * 10.582246780395508
Epoch 0, val loss: 1.1033031940460205
Epoch 10, training loss: 530.1895141601562 = 1.098591923713684 + 50.0 * 10.581818580627441
Epoch 10, val loss: 1.0987303256988525
Epoch 20, training loss: 530.1055908203125 = 1.0937408208847046 + 50.0 * 10.580236434936523
Epoch 20, val loss: 1.0938401222229004
Epoch 30, training loss: 529.781005859375 = 1.0885387659072876 + 50.0 * 10.573848724365234
Epoch 30, val loss: 1.0886120796203613
Epoch 40, training loss: 528.5156860351562 = 1.0830731391906738 + 50.0 * 10.548652648925781
Epoch 40, val loss: 1.0831340551376343
Epoch 50, training loss: 524.213623046875 = 1.0775858163833618 + 50.0 * 10.46272087097168
Epoch 50, val loss: 1.0776909589767456
Epoch 60, training loss: 511.69671630859375 = 1.0729726552963257 + 50.0 * 10.212474822998047
Epoch 60, val loss: 1.0731223821640015
Epoch 70, training loss: 487.0985107421875 = 1.0679994821548462 + 50.0 * 9.720610618591309
Epoch 70, val loss: 1.0680592060089111
Epoch 80, training loss: 473.01702880859375 = 1.0628713369369507 + 50.0 * 9.439083099365234
Epoch 80, val loss: 1.063042163848877
Epoch 90, training loss: 466.2205505371094 = 1.0585722923278809 + 50.0 * 9.303239822387695
Epoch 90, val loss: 1.0588852167129517
Epoch 100, training loss: 461.9646911621094 = 1.0547486543655396 + 50.0 * 9.218198776245117
Epoch 100, val loss: 1.0551303625106812
Epoch 110, training loss: 459.7059020996094 = 1.0513979196548462 + 50.0 * 9.173089981079102
Epoch 110, val loss: 1.0518468618392944
Epoch 120, training loss: 457.2491760253906 = 1.0490134954452515 + 50.0 * 9.124003410339355
Epoch 120, val loss: 1.049597144126892
Epoch 130, training loss: 453.8184509277344 = 1.0478230714797974 + 50.0 * 9.055412292480469
Epoch 130, val loss: 1.0484975576400757
Epoch 140, training loss: 450.58673095703125 = 1.0472902059555054 + 50.0 * 8.990788459777832
Epoch 140, val loss: 1.04796302318573
Epoch 150, training loss: 448.8763732910156 = 1.0467437505722046 + 50.0 * 8.956592559814453
Epoch 150, val loss: 1.047366976737976
Epoch 160, training loss: 446.98931884765625 = 1.045915126800537 + 50.0 * 8.918868064880371
Epoch 160, val loss: 1.0465219020843506
Epoch 170, training loss: 444.30157470703125 = 1.0452927350997925 + 50.0 * 8.86512565612793
Epoch 170, val loss: 1.0459519624710083
Epoch 180, training loss: 441.7752685546875 = 1.0451278686523438 + 50.0 * 8.814602851867676
Epoch 180, val loss: 1.0457887649536133
Epoch 190, training loss: 439.8055419921875 = 1.0448038578033447 + 50.0 * 8.775215148925781
Epoch 190, val loss: 1.0454540252685547
Epoch 200, training loss: 438.0211181640625 = 1.0442085266113281 + 50.0 * 8.739538192749023
Epoch 200, val loss: 1.0448753833770752
Epoch 210, training loss: 436.4581604003906 = 1.0436114072799683 + 50.0 * 8.708291053771973
Epoch 210, val loss: 1.0443105697631836
Epoch 220, training loss: 435.25091552734375 = 1.04305899143219 + 50.0 * 8.684157371520996
Epoch 220, val loss: 1.0437830686569214
Epoch 230, training loss: 434.3457336425781 = 1.0425209999084473 + 50.0 * 8.666064262390137
Epoch 230, val loss: 1.0432724952697754
Epoch 240, training loss: 433.59130859375 = 1.0419540405273438 + 50.0 * 8.650986671447754
Epoch 240, val loss: 1.0427325963974
Epoch 250, training loss: 432.9366760253906 = 1.0413793325424194 + 50.0 * 8.637906074523926
Epoch 250, val loss: 1.0421991348266602
Epoch 260, training loss: 432.2377624511719 = 1.0408258438110352 + 50.0 * 8.62393856048584
Epoch 260, val loss: 1.04165518283844
Epoch 270, training loss: 431.55450439453125 = 1.0403012037277222 + 50.0 * 8.610283851623535
Epoch 270, val loss: 1.041155457496643
Epoch 280, training loss: 430.93402099609375 = 1.0397640466690063 + 50.0 * 8.597885131835938
Epoch 280, val loss: 1.040644884109497
Epoch 290, training loss: 430.3367614746094 = 1.0392051935195923 + 50.0 * 8.58595085144043
Epoch 290, val loss: 1.0401071310043335
Epoch 300, training loss: 429.7808837890625 = 1.0386751890182495 + 50.0 * 8.574844360351562
Epoch 300, val loss: 1.0395880937576294
Epoch 310, training loss: 429.23675537109375 = 1.0381501913070679 + 50.0 * 8.563972473144531
Epoch 310, val loss: 1.0390702486038208
Epoch 320, training loss: 428.65460205078125 = 1.037642478942871 + 50.0 * 8.552339553833008
Epoch 320, val loss: 1.0385757684707642
Epoch 330, training loss: 428.16864013671875 = 1.0371464490890503 + 50.0 * 8.542630195617676
Epoch 330, val loss: 1.0380898714065552
Epoch 340, training loss: 427.7891540527344 = 1.0365920066833496 + 50.0 * 8.535051345825195
Epoch 340, val loss: 1.0375175476074219
Epoch 350, training loss: 427.3841552734375 = 1.0359925031661987 + 50.0 * 8.526963233947754
Epoch 350, val loss: 1.0369142293930054
Epoch 360, training loss: 427.0814514160156 = 1.0353724956512451 + 50.0 * 8.52092170715332
Epoch 360, val loss: 1.036283016204834
Epoch 370, training loss: 426.8357238769531 = 1.0346975326538086 + 50.0 * 8.516020774841309
Epoch 370, val loss: 1.035597562789917
Epoch 380, training loss: 426.6746520996094 = 1.0339463949203491 + 50.0 * 8.51281452178955
Epoch 380, val loss: 1.0348259210586548
Epoch 390, training loss: 426.414794921875 = 1.0331507921218872 + 50.0 * 8.507633209228516
Epoch 390, val loss: 1.0340237617492676
Epoch 400, training loss: 426.18475341796875 = 1.032361388206482 + 50.0 * 8.503047943115234
Epoch 400, val loss: 1.0332303047180176
Epoch 410, training loss: 426.017822265625 = 1.0315405130386353 + 50.0 * 8.499725341796875
Epoch 410, val loss: 1.0323939323425293
Epoch 420, training loss: 425.813720703125 = 1.0306520462036133 + 50.0 * 8.495661735534668
Epoch 420, val loss: 1.0314853191375732
Epoch 430, training loss: 425.5768737792969 = 1.0297377109527588 + 50.0 * 8.49094295501709
Epoch 430, val loss: 1.0305851697921753
Epoch 440, training loss: 425.3632507324219 = 1.0288140773773193 + 50.0 * 8.486688613891602
Epoch 440, val loss: 1.029650092124939
Epoch 450, training loss: 425.26702880859375 = 1.0278478860855103 + 50.0 * 8.484783172607422
Epoch 450, val loss: 1.0286575555801392
Epoch 460, training loss: 425.1282653808594 = 1.0267657041549683 + 50.0 * 8.482029914855957
Epoch 460, val loss: 1.0276117324829102
Epoch 470, training loss: 424.9453125 = 1.025692105293274 + 50.0 * 8.478392601013184
Epoch 470, val loss: 1.0265357494354248
Epoch 480, training loss: 424.8061828613281 = 1.0245879888534546 + 50.0 * 8.475631713867188
Epoch 480, val loss: 1.0254385471343994
Epoch 490, training loss: 424.7288818359375 = 1.0234321355819702 + 50.0 * 8.474108695983887
Epoch 490, val loss: 1.0242948532104492
Epoch 500, training loss: 424.6758728027344 = 1.0221974849700928 + 50.0 * 8.473073959350586
Epoch 500, val loss: 1.0230401754379272
Epoch 510, training loss: 424.50872802734375 = 1.0209251642227173 + 50.0 * 8.469756126403809
Epoch 510, val loss: 1.0217959880828857
Epoch 520, training loss: 424.3597717285156 = 1.0196466445922852 + 50.0 * 8.466802597045898
Epoch 520, val loss: 1.02053701877594
Epoch 530, training loss: 424.2908630371094 = 1.018325924873352 + 50.0 * 8.465450286865234
Epoch 530, val loss: 1.0192474126815796
Epoch 540, training loss: 424.1529541015625 = 1.0170093774795532 + 50.0 * 8.462718963623047
Epoch 540, val loss: 1.017960548400879
Epoch 550, training loss: 424.0177001953125 = 1.0157009363174438 + 50.0 * 8.460040092468262
Epoch 550, val loss: 1.0166856050491333
Epoch 560, training loss: 423.88592529296875 = 1.014384150505066 + 50.0 * 8.457430839538574
Epoch 560, val loss: 1.0154012441635132
Epoch 570, training loss: 423.9578552246094 = 1.013042688369751 + 50.0 * 8.45889663696289
Epoch 570, val loss: 1.0140900611877441
Epoch 580, training loss: 423.6725158691406 = 1.0116082429885864 + 50.0 * 8.453218460083008
Epoch 580, val loss: 1.0126832723617554
Epoch 590, training loss: 423.51898193359375 = 1.0102241039276123 + 50.0 * 8.450175285339355
Epoch 590, val loss: 1.0113533735275269
Epoch 600, training loss: 423.36474609375 = 1.0088560581207275 + 50.0 * 8.447117805480957
Epoch 600, val loss: 1.0100064277648926
Epoch 610, training loss: 423.242919921875 = 1.0074576139450073 + 50.0 * 8.444709777832031
Epoch 610, val loss: 1.0086438655853271
Epoch 620, training loss: 423.35052490234375 = 1.0060391426086426 + 50.0 * 8.446889877319336
Epoch 620, val loss: 1.0072470903396606
Epoch 630, training loss: 423.12164306640625 = 1.0044450759887695 + 50.0 * 8.442343711853027
Epoch 630, val loss: 1.0056805610656738
Epoch 640, training loss: 422.94970703125 = 1.0028878450393677 + 50.0 * 8.438936233520508
Epoch 640, val loss: 1.0041857957839966
Epoch 650, training loss: 422.8222961425781 = 1.0013840198516846 + 50.0 * 8.436418533325195
Epoch 650, val loss: 1.0027097463607788
Epoch 660, training loss: 422.71319580078125 = 0.9998435974121094 + 50.0 * 8.434267044067383
Epoch 660, val loss: 1.0011980533599854
Epoch 670, training loss: 422.617431640625 = 0.9982486367225647 + 50.0 * 8.43238353729248
Epoch 670, val loss: 0.9996278882026672
Epoch 680, training loss: 422.7144775390625 = 0.9965603947639465 + 50.0 * 8.434358596801758
Epoch 680, val loss: 0.9979549646377563
Epoch 690, training loss: 422.5250549316406 = 0.9947299361228943 + 50.0 * 8.430606842041016
Epoch 690, val loss: 0.9961878657341003
Epoch 700, training loss: 422.38818359375 = 0.9929249882698059 + 50.0 * 8.427905082702637
Epoch 700, val loss: 0.9943845272064209
Epoch 710, training loss: 422.31280517578125 = 0.9911265969276428 + 50.0 * 8.426433563232422
Epoch 710, val loss: 0.99262934923172
Epoch 720, training loss: 422.2293701171875 = 0.9893243312835693 + 50.0 * 8.424800872802734
Epoch 720, val loss: 0.9908512234687805
Epoch 730, training loss: 422.18365478515625 = 0.9874930381774902 + 50.0 * 8.42392349243164
Epoch 730, val loss: 0.9890215396881104
Epoch 740, training loss: 422.3169860839844 = 0.9854618310928345 + 50.0 * 8.426630020141602
Epoch 740, val loss: 0.9870817065238953
Epoch 750, training loss: 422.0632019042969 = 0.9833565950393677 + 50.0 * 8.42159652709961
Epoch 750, val loss: 0.984986424446106
Epoch 760, training loss: 421.9971008300781 = 0.9813686609268188 + 50.0 * 8.42031478881836
Epoch 760, val loss: 0.9830194711685181
Epoch 770, training loss: 421.90985107421875 = 0.9793664813041687 + 50.0 * 8.418609619140625
Epoch 770, val loss: 0.9810177683830261
Epoch 780, training loss: 421.8411865234375 = 0.9773135781288147 + 50.0 * 8.417277336120605
Epoch 780, val loss: 0.9789791107177734
Epoch 790, training loss: 422.0270080566406 = 0.9752126932144165 + 50.0 * 8.421035766601562
Epoch 790, val loss: 0.97690349817276
Epoch 800, training loss: 421.7872619628906 = 0.9729166626930237 + 50.0 * 8.41628646850586
Epoch 800, val loss: 0.9746001362800598
Epoch 810, training loss: 421.69964599609375 = 0.9707267880439758 + 50.0 * 8.414578437805176
Epoch 810, val loss: 0.9724345207214355
Epoch 820, training loss: 421.6005859375 = 0.9685583710670471 + 50.0 * 8.412640571594238
Epoch 820, val loss: 0.9702962040901184
Epoch 830, training loss: 421.5465393066406 = 0.9663402438163757 + 50.0 * 8.411603927612305
Epoch 830, val loss: 0.9681103825569153
Epoch 840, training loss: 421.68829345703125 = 0.9640657305717468 + 50.0 * 8.414484024047852
Epoch 840, val loss: 0.9658542275428772
Epoch 850, training loss: 421.7001647949219 = 0.9614713191986084 + 50.0 * 8.414773941040039
Epoch 850, val loss: 0.9632176756858826
Epoch 860, training loss: 421.46844482421875 = 0.9589563608169556 + 50.0 * 8.410189628601074
Epoch 860, val loss: 0.9607710838317871
Epoch 870, training loss: 421.3595886230469 = 0.9565795063972473 + 50.0 * 8.408060073852539
Epoch 870, val loss: 0.9584302306175232
Epoch 880, training loss: 421.2998352050781 = 0.954185426235199 + 50.0 * 8.406912803649902
Epoch 880, val loss: 0.9560564160346985
Epoch 890, training loss: 421.24908447265625 = 0.9517397880554199 + 50.0 * 8.405946731567383
Epoch 890, val loss: 0.9536349177360535
Epoch 900, training loss: 421.1988220214844 = 0.9492211937904358 + 50.0 * 8.40499210357666
Epoch 900, val loss: 0.9511459469795227
Epoch 910, training loss: 421.1535339355469 = 0.9466453790664673 + 50.0 * 8.40413761138916
Epoch 910, val loss: 0.9486095905303955
Epoch 920, training loss: 421.3401184082031 = 0.9440227746963501 + 50.0 * 8.40792179107666
Epoch 920, val loss: 0.9460107088088989
Epoch 930, training loss: 421.1764831542969 = 0.94110107421875 + 50.0 * 8.404707908630371
Epoch 930, val loss: 0.9431368708610535
Epoch 940, training loss: 421.0582580566406 = 0.9383037686347961 + 50.0 * 8.402399063110352
Epoch 940, val loss: 0.9403741359710693
Epoch 950, training loss: 420.99285888671875 = 0.935599148273468 + 50.0 * 8.401144981384277
Epoch 950, val loss: 0.9377042055130005
Epoch 960, training loss: 420.94500732421875 = 0.9328452944755554 + 50.0 * 8.400243759155273
Epoch 960, val loss: 0.934990406036377
Epoch 970, training loss: 420.9093322753906 = 0.9300456643104553 + 50.0 * 8.399585723876953
Epoch 970, val loss: 0.9322212934494019
Epoch 980, training loss: 421.1571350097656 = 0.9271271824836731 + 50.0 * 8.404600143432617
Epoch 980, val loss: 0.929326593875885
Epoch 990, training loss: 420.8878173828125 = 0.9240440130233765 + 50.0 * 8.399275779724121
Epoch 990, val loss: 0.9262971878051758
Epoch 1000, training loss: 420.775146484375 = 0.9210835695266724 + 50.0 * 8.39708137512207
Epoch 1000, val loss: 0.9233996868133545
Epoch 1010, training loss: 420.7395935058594 = 0.9181500673294067 + 50.0 * 8.396429061889648
Epoch 1010, val loss: 0.9205306768417358
Epoch 1020, training loss: 420.6981506347656 = 0.9151872396469116 + 50.0 * 8.395659446716309
Epoch 1020, val loss: 0.9175822734832764
Epoch 1030, training loss: 420.8345642089844 = 0.9120999574661255 + 50.0 * 8.398448944091797
Epoch 1030, val loss: 0.914512038230896
Epoch 1040, training loss: 420.7467956542969 = 0.9087584018707275 + 50.0 * 8.396760940551758
Epoch 1040, val loss: 0.9113156199455261
Epoch 1050, training loss: 420.595703125 = 0.9055212140083313 + 50.0 * 8.393803596496582
Epoch 1050, val loss: 0.9081358313560486
Epoch 1060, training loss: 420.5526428222656 = 0.9023676514625549 + 50.0 * 8.39300537109375
Epoch 1060, val loss: 0.9049851298332214
Epoch 1070, training loss: 420.5379943847656 = 0.899176299571991 + 50.0 * 8.392776489257812
Epoch 1070, val loss: 0.901848554611206
Epoch 1080, training loss: 420.6067810058594 = 0.8958114385604858 + 50.0 * 8.394219398498535
Epoch 1080, val loss: 0.8984997868537903
Epoch 1090, training loss: 420.48040771484375 = 0.8923444747924805 + 50.0 * 8.391761779785156
Epoch 1090, val loss: 0.8951708674430847
Epoch 1100, training loss: 420.4158630371094 = 0.8890266418457031 + 50.0 * 8.390536308288574
Epoch 1100, val loss: 0.8918704390525818
Epoch 1110, training loss: 420.4006652832031 = 0.8856700658798218 + 50.0 * 8.390299797058105
Epoch 1110, val loss: 0.8885624408721924
Epoch 1120, training loss: 420.5171203613281 = 0.8821699619293213 + 50.0 * 8.392699241638184
Epoch 1120, val loss: 0.8851311802864075
Epoch 1130, training loss: 420.3586730957031 = 0.878610372543335 + 50.0 * 8.389601707458496
Epoch 1130, val loss: 0.881615400314331
Epoch 1140, training loss: 420.2904052734375 = 0.8751364946365356 + 50.0 * 8.3883056640625
Epoch 1140, val loss: 0.8782294988632202
Epoch 1150, training loss: 420.2481384277344 = 0.8716598153114319 + 50.0 * 8.387529373168945
Epoch 1150, val loss: 0.874792218208313
Epoch 1160, training loss: 420.21087646484375 = 0.8681663870811462 + 50.0 * 8.38685417175293
Epoch 1160, val loss: 0.8713693022727966
Epoch 1170, training loss: 420.22686767578125 = 0.8646382689476013 + 50.0 * 8.387245178222656
Epoch 1170, val loss: 0.8678723573684692
Epoch 1180, training loss: 420.2115173339844 = 0.860734224319458 + 50.0 * 8.387015342712402
Epoch 1180, val loss: 0.8640333414077759
Epoch 1190, training loss: 420.2112731933594 = 0.856838583946228 + 50.0 * 8.387088775634766
Epoch 1190, val loss: 0.86020427942276
Epoch 1200, training loss: 420.13885498046875 = 0.8531942963600159 + 50.0 * 8.385713577270508
Epoch 1200, val loss: 0.8566718697547913
Epoch 1210, training loss: 420.0754699707031 = 0.8495919108390808 + 50.0 * 8.384517669677734
Epoch 1210, val loss: 0.8531453013420105
Epoch 1220, training loss: 420.0394287109375 = 0.8459826111793518 + 50.0 * 8.383869171142578
Epoch 1220, val loss: 0.8496332764625549
Epoch 1230, training loss: 420.0162048339844 = 0.8423535227775574 + 50.0 * 8.383477210998535
Epoch 1230, val loss: 0.8460863828659058
Epoch 1240, training loss: 420.161865234375 = 0.8386461138725281 + 50.0 * 8.38646411895752
Epoch 1240, val loss: 0.8424539566040039
Epoch 1250, training loss: 420.0975646972656 = 0.8346736431121826 + 50.0 * 8.385257720947266
Epoch 1250, val loss: 0.8385562300682068
Epoch 1260, training loss: 419.9502868652344 = 0.830849826335907 + 50.0 * 8.382389068603516
Epoch 1260, val loss: 0.8348594307899475
Epoch 1270, training loss: 419.924072265625 = 0.8271175622940063 + 50.0 * 8.381938934326172
Epoch 1270, val loss: 0.8312362432479858
Epoch 1280, training loss: 419.8864440917969 = 0.8234028816223145 + 50.0 * 8.381260871887207
Epoch 1280, val loss: 0.8276194334030151
Epoch 1290, training loss: 419.87066650390625 = 0.8197000622749329 + 50.0 * 8.381019592285156
Epoch 1290, val loss: 0.8240183591842651
Epoch 1300, training loss: 420.0299072265625 = 0.8158970475196838 + 50.0 * 8.38428020477295
Epoch 1300, val loss: 0.8203175067901611
Epoch 1310, training loss: 419.85614013671875 = 0.8119735717773438 + 50.0 * 8.38088321685791
Epoch 1310, val loss: 0.8165106773376465
Epoch 1320, training loss: 419.796630859375 = 0.8081673383712769 + 50.0 * 8.379769325256348
Epoch 1320, val loss: 0.8128469586372375
Epoch 1330, training loss: 419.7597351074219 = 0.8043997883796692 + 50.0 * 8.379106521606445
Epoch 1330, val loss: 0.8092153668403625
Epoch 1340, training loss: 419.7788391113281 = 0.8006194233894348 + 50.0 * 8.37956428527832
Epoch 1340, val loss: 0.8055925965309143
Epoch 1350, training loss: 419.816162109375 = 0.7966927289962769 + 50.0 * 8.380389213562012
Epoch 1350, val loss: 0.8017299771308899
Epoch 1360, training loss: 419.740234375 = 0.7927387952804565 + 50.0 * 8.378950119018555
Epoch 1360, val loss: 0.7979487776756287
Epoch 1370, training loss: 419.6807861328125 = 0.7889362573623657 + 50.0 * 8.377837181091309
Epoch 1370, val loss: 0.7943018078804016
Epoch 1380, training loss: 419.64385986328125 = 0.7851958274841309 + 50.0 * 8.37717342376709
Epoch 1380, val loss: 0.7906844615936279
Epoch 1390, training loss: 419.6167297363281 = 0.7814705967903137 + 50.0 * 8.376705169677734
Epoch 1390, val loss: 0.7871106863021851
Epoch 1400, training loss: 419.60894775390625 = 0.7777203917503357 + 50.0 * 8.376625061035156
Epoch 1400, val loss: 0.7835184335708618
Epoch 1410, training loss: 419.7336120605469 = 0.7738825678825378 + 50.0 * 8.379194259643555
Epoch 1410, val loss: 0.7798289060592651
Epoch 1420, training loss: 419.7947998046875 = 0.7699087262153625 + 50.0 * 8.380497932434082
Epoch 1420, val loss: 0.7760581970214844
Epoch 1430, training loss: 419.5823974609375 = 0.7659647464752197 + 50.0 * 8.376328468322754
Epoch 1430, val loss: 0.7722507119178772
Epoch 1440, training loss: 419.5059509277344 = 0.7622107267379761 + 50.0 * 8.37487506866455
Epoch 1440, val loss: 0.7686626315116882
Epoch 1450, training loss: 419.4841003417969 = 0.7585132122039795 + 50.0 * 8.37451171875
Epoch 1450, val loss: 0.7651841044425964
Epoch 1460, training loss: 419.4583740234375 = 0.7548410296440125 + 50.0 * 8.374070167541504
Epoch 1460, val loss: 0.7616549134254456
Epoch 1470, training loss: 419.44293212890625 = 0.7511582970619202 + 50.0 * 8.373835563659668
Epoch 1470, val loss: 0.7581777572631836
Epoch 1480, training loss: 419.6447448730469 = 0.7473968863487244 + 50.0 * 8.377946853637695
Epoch 1480, val loss: 0.7546044588088989
Epoch 1490, training loss: 419.48175048828125 = 0.743530809879303 + 50.0 * 8.374764442443848
Epoch 1490, val loss: 0.7508536577224731
Epoch 1500, training loss: 419.454345703125 = 0.7397949695587158 + 50.0 * 8.374290466308594
Epoch 1500, val loss: 0.747317910194397
Epoch 1510, training loss: 419.4302673339844 = 0.7359501123428345 + 50.0 * 8.373886108398438
Epoch 1510, val loss: 0.7436964511871338
Epoch 1520, training loss: 419.3841857910156 = 0.732194185256958 + 50.0 * 8.373039245605469
Epoch 1520, val loss: 0.7401716709136963
Epoch 1530, training loss: 419.3287048339844 = 0.7285940051078796 + 50.0 * 8.372001647949219
Epoch 1530, val loss: 0.7367497682571411
Epoch 1540, training loss: 419.2939453125 = 0.7250034213066101 + 50.0 * 8.371378898620605
Epoch 1540, val loss: 0.7333822250366211
Epoch 1550, training loss: 419.30419921875 = 0.7214332818984985 + 50.0 * 8.371655464172363
Epoch 1550, val loss: 0.7300119400024414
Epoch 1560, training loss: 419.5606994628906 = 0.717678964138031 + 50.0 * 8.376860618591309
Epoch 1560, val loss: 0.7264180183410645
Epoch 1570, training loss: 419.2477722167969 = 0.7138410806655884 + 50.0 * 8.370678901672363
Epoch 1570, val loss: 0.7228273749351501
Epoch 1580, training loss: 419.2622375488281 = 0.7102637887001038 + 50.0 * 8.371039390563965
Epoch 1580, val loss: 0.7195349335670471
Epoch 1590, training loss: 419.201171875 = 0.7067596912384033 + 50.0 * 8.369888305664062
Epoch 1590, val loss: 0.7162283062934875
Epoch 1600, training loss: 419.1885986328125 = 0.7033302783966064 + 50.0 * 8.369705200195312
Epoch 1600, val loss: 0.713018000125885
Epoch 1610, training loss: 419.1737060546875 = 0.6998968124389648 + 50.0 * 8.369476318359375
Epoch 1610, val loss: 0.7098050713539124
Epoch 1620, training loss: 419.417724609375 = 0.6964578628540039 + 50.0 * 8.374424934387207
Epoch 1620, val loss: 0.706505298614502
Epoch 1630, training loss: 419.2291564941406 = 0.6926276087760925 + 50.0 * 8.37073040008545
Epoch 1630, val loss: 0.7030108571052551
Epoch 1640, training loss: 419.17462158203125 = 0.6892056465148926 + 50.0 * 8.369708061218262
Epoch 1640, val loss: 0.6997975707054138
Epoch 1650, training loss: 419.1129150390625 = 0.6858004331588745 + 50.0 * 8.368542671203613
Epoch 1650, val loss: 0.6966376900672913
Epoch 1660, training loss: 419.0857849121094 = 0.6825060248374939 + 50.0 * 8.36806583404541
Epoch 1660, val loss: 0.6935904622077942
Epoch 1670, training loss: 419.0682678222656 = 0.6792288422584534 + 50.0 * 8.367780685424805
Epoch 1670, val loss: 0.6905450224876404
Epoch 1680, training loss: 419.07196044921875 = 0.6759661436080933 + 50.0 * 8.367919921875
Epoch 1680, val loss: 0.6874898672103882
Epoch 1690, training loss: 419.20697021484375 = 0.6725996136665344 + 50.0 * 8.370687484741211
Epoch 1690, val loss: 0.6843327283859253
Epoch 1700, training loss: 419.04248046875 = 0.6691862344741821 + 50.0 * 8.36746597290039
Epoch 1700, val loss: 0.6812381744384766
Epoch 1710, training loss: 419.0402526855469 = 0.6659578084945679 + 50.0 * 8.367486000061035
Epoch 1710, val loss: 0.6782532334327698
Epoch 1720, training loss: 419.12689208984375 = 0.662717342376709 + 50.0 * 8.369283676147461
Epoch 1720, val loss: 0.675212562084198
Epoch 1730, training loss: 418.9997863769531 = 0.6594921946525574 + 50.0 * 8.366806030273438
Epoch 1730, val loss: 0.6723145246505737
Epoch 1740, training loss: 418.96624755859375 = 0.6563658118247986 + 50.0 * 8.36619758605957
Epoch 1740, val loss: 0.6694188714027405
Epoch 1750, training loss: 418.9571533203125 = 0.653290331363678 + 50.0 * 8.366077423095703
Epoch 1750, val loss: 0.6666105389595032
Epoch 1760, training loss: 419.0109558105469 = 0.6502160429954529 + 50.0 * 8.367215156555176
Epoch 1760, val loss: 0.6637870073318481
Epoch 1770, training loss: 419.00750732421875 = 0.6469863057136536 + 50.0 * 8.367210388183594
Epoch 1770, val loss: 0.6607755422592163
Epoch 1780, training loss: 418.9241027832031 = 0.6438826322555542 + 50.0 * 8.365604400634766
Epoch 1780, val loss: 0.6579362750053406
Epoch 1790, training loss: 418.89556884765625 = 0.6409017443656921 + 50.0 * 8.365093231201172
Epoch 1790, val loss: 0.6552517414093018
Epoch 1800, training loss: 418.90948486328125 = 0.6380041837692261 + 50.0 * 8.365429878234863
Epoch 1800, val loss: 0.652567982673645
Epoch 1810, training loss: 418.9754943847656 = 0.6350162625312805 + 50.0 * 8.366809844970703
Epoch 1810, val loss: 0.6498433351516724
Epoch 1820, training loss: 418.8610534667969 = 0.6320802569389343 + 50.0 * 8.364579200744629
Epoch 1820, val loss: 0.6471656560897827
Epoch 1830, training loss: 418.8423156738281 = 0.629266619682312 + 50.0 * 8.36426067352295
Epoch 1830, val loss: 0.644671618938446
Epoch 1840, training loss: 418.8335876464844 = 0.6265138387680054 + 50.0 * 8.364141464233398
Epoch 1840, val loss: 0.642123281955719
Epoch 1850, training loss: 418.92041015625 = 0.6237713098526001 + 50.0 * 8.36593246459961
Epoch 1850, val loss: 0.6396615505218506
Epoch 1860, training loss: 418.8017578125 = 0.6209033131599426 + 50.0 * 8.363616943359375
Epoch 1860, val loss: 0.6370216012001038
Epoch 1870, training loss: 418.78912353515625 = 0.6181843280792236 + 50.0 * 8.363418579101562
Epoch 1870, val loss: 0.6346009373664856
Epoch 1880, training loss: 418.77630615234375 = 0.6155639886856079 + 50.0 * 8.363214492797852
Epoch 1880, val loss: 0.6322312951087952
Epoch 1890, training loss: 418.77032470703125 = 0.6129705905914307 + 50.0 * 8.363146781921387
Epoch 1890, val loss: 0.6298776865005493
Epoch 1900, training loss: 418.9639587402344 = 0.6104133129119873 + 50.0 * 8.367071151733398
Epoch 1900, val loss: 0.6275338530540466
Epoch 1910, training loss: 418.7932434082031 = 0.6075495481491089 + 50.0 * 8.363714218139648
Epoch 1910, val loss: 0.6250375509262085
Epoch 1920, training loss: 418.7605895996094 = 0.6049996614456177 + 50.0 * 8.36311149597168
Epoch 1920, val loss: 0.6226670145988464
Epoch 1930, training loss: 418.7268371582031 = 0.6025199890136719 + 50.0 * 8.362486839294434
Epoch 1930, val loss: 0.620509147644043
Epoch 1940, training loss: 418.70611572265625 = 0.6001214981079102 + 50.0 * 8.362119674682617
Epoch 1940, val loss: 0.6183232069015503
Epoch 1950, training loss: 418.7393798828125 = 0.5977282524108887 + 50.0 * 8.362833023071289
Epoch 1950, val loss: 0.6162013411521912
Epoch 1960, training loss: 418.7916564941406 = 0.5952621698379517 + 50.0 * 8.363927841186523
Epoch 1960, val loss: 0.613965630531311
Epoch 1970, training loss: 418.70587158203125 = 0.5928452014923096 + 50.0 * 8.362260818481445
Epoch 1970, val loss: 0.6118662357330322
Epoch 1980, training loss: 418.6615905761719 = 0.5905271172523499 + 50.0 * 8.361421585083008
Epoch 1980, val loss: 0.6098216772079468
Epoch 1990, training loss: 418.6422119140625 = 0.5882672667503357 + 50.0 * 8.361079216003418
Epoch 1990, val loss: 0.6078224182128906
Epoch 2000, training loss: 418.6778869628906 = 0.5860267877578735 + 50.0 * 8.361837387084961
Epoch 2000, val loss: 0.6058430671691895
Epoch 2010, training loss: 418.7230224609375 = 0.5837021470069885 + 50.0 * 8.362786293029785
Epoch 2010, val loss: 0.6038033962249756
Epoch 2020, training loss: 418.64385986328125 = 0.581455647945404 + 50.0 * 8.361248016357422
Epoch 2020, val loss: 0.6017510294914246
Epoch 2030, training loss: 418.6325378417969 = 0.5792617201805115 + 50.0 * 8.361065864562988
Epoch 2030, val loss: 0.5998632907867432
Epoch 2040, training loss: 418.65289306640625 = 0.5770964622497559 + 50.0 * 8.361515998840332
Epoch 2040, val loss: 0.5979107618331909
Epoch 2050, training loss: 418.6446838378906 = 0.5749709606170654 + 50.0 * 8.361393928527832
Epoch 2050, val loss: 0.5961222052574158
Epoch 2060, training loss: 418.6592102050781 = 0.5728228092193604 + 50.0 * 8.361727714538574
Epoch 2060, val loss: 0.5942011475563049
Epoch 2070, training loss: 418.56884765625 = 0.570766270160675 + 50.0 * 8.35996150970459
Epoch 2070, val loss: 0.5924498438835144
Epoch 2080, training loss: 418.551025390625 = 0.5687732696533203 + 50.0 * 8.359644889831543
Epoch 2080, val loss: 0.590733528137207
Epoch 2090, training loss: 418.5345458984375 = 0.5668115019798279 + 50.0 * 8.359354972839355
Epoch 2090, val loss: 0.5890289545059204
Epoch 2100, training loss: 418.5990905761719 = 0.5648835301399231 + 50.0 * 8.360684394836426
Epoch 2100, val loss: 0.5873484015464783
Epoch 2110, training loss: 418.5380554199219 = 0.5628432631492615 + 50.0 * 8.359504699707031
Epoch 2110, val loss: 0.5855591297149658
Epoch 2120, training loss: 418.5425109863281 = 0.5608929395675659 + 50.0 * 8.35963249206543
Epoch 2120, val loss: 0.5839107632637024
Epoch 2130, training loss: 418.5543518066406 = 0.558996319770813 + 50.0 * 8.359907150268555
Epoch 2130, val loss: 0.5823010802268982
Epoch 2140, training loss: 418.50079345703125 = 0.5571539402008057 + 50.0 * 8.358872413635254
Epoch 2140, val loss: 0.5806849598884583
Epoch 2150, training loss: 418.4732971191406 = 0.5553761720657349 + 50.0 * 8.358358383178711
Epoch 2150, val loss: 0.5791994333267212
Epoch 2160, training loss: 418.4888000488281 = 0.5536273121833801 + 50.0 * 8.35870361328125
Epoch 2160, val loss: 0.5777318477630615
Epoch 2170, training loss: 418.70379638671875 = 0.551842987537384 + 50.0 * 8.363039016723633
Epoch 2170, val loss: 0.576215922832489
Epoch 2180, training loss: 418.5507507324219 = 0.5499558448791504 + 50.0 * 8.360015869140625
Epoch 2180, val loss: 0.5745036005973816
Epoch 2190, training loss: 418.474365234375 = 0.5482388138771057 + 50.0 * 8.358522415161133
Epoch 2190, val loss: 0.5731714367866516
Epoch 2200, training loss: 418.43255615234375 = 0.5465771555900574 + 50.0 * 8.357719421386719
Epoch 2200, val loss: 0.5717002153396606
Epoch 2210, training loss: 418.4121398925781 = 0.5449666380882263 + 50.0 * 8.357343673706055
Epoch 2210, val loss: 0.5704094171524048
Epoch 2220, training loss: 418.42242431640625 = 0.5433627367019653 + 50.0 * 8.35758113861084
Epoch 2220, val loss: 0.5690447092056274
Epoch 2230, training loss: 418.71099853515625 = 0.5416877865791321 + 50.0 * 8.363386154174805
Epoch 2230, val loss: 0.5676264762878418
Epoch 2240, training loss: 418.4656066894531 = 0.5399399995803833 + 50.0 * 8.358512878417969
Epoch 2240, val loss: 0.5661461353302002
Epoch 2250, training loss: 418.41033935546875 = 0.5383906364440918 + 50.0 * 8.357439041137695
Epoch 2250, val loss: 0.5649307370185852
Epoch 2260, training loss: 418.3770751953125 = 0.5368713736534119 + 50.0 * 8.356803894042969
Epoch 2260, val loss: 0.5636078715324402
Epoch 2270, training loss: 418.3486633300781 = 0.5354089736938477 + 50.0 * 8.3562650680542
Epoch 2270, val loss: 0.5624417662620544
Epoch 2280, training loss: 418.3366394042969 = 0.5339592695236206 + 50.0 * 8.356053352355957
Epoch 2280, val loss: 0.5612703561782837
Epoch 2290, training loss: 418.3451843261719 = 0.5325126647949219 + 50.0 * 8.356253623962402
Epoch 2290, val loss: 0.5601003766059875
Epoch 2300, training loss: 418.70477294921875 = 0.5309821963310242 + 50.0 * 8.363475799560547
Epoch 2300, val loss: 0.5588701367378235
Epoch 2310, training loss: 418.3794860839844 = 0.529465913772583 + 50.0 * 8.357000350952148
Epoch 2310, val loss: 0.5575051307678223
Epoch 2320, training loss: 418.3094787597656 = 0.5280545353889465 + 50.0 * 8.355628967285156
Epoch 2320, val loss: 0.5564424991607666
Epoch 2330, training loss: 418.3011474609375 = 0.526684582233429 + 50.0 * 8.355489730834961
Epoch 2330, val loss: 0.5553454756736755
Epoch 2340, training loss: 418.2969970703125 = 0.5253652930259705 + 50.0 * 8.355432510375977
Epoch 2340, val loss: 0.5543093085289001
Epoch 2350, training loss: 418.5845642089844 = 0.5240184664726257 + 50.0 * 8.361210823059082
Epoch 2350, val loss: 0.5532862544059753
Epoch 2360, training loss: 418.3802795410156 = 0.5225687026977539 + 50.0 * 8.35715389251709
Epoch 2360, val loss: 0.5519241094589233
Epoch 2370, training loss: 418.2947082519531 = 0.5212559700012207 + 50.0 * 8.35546875
Epoch 2370, val loss: 0.5509806871414185
Epoch 2380, training loss: 418.25592041015625 = 0.5200058221817017 + 50.0 * 8.354718208312988
Epoch 2380, val loss: 0.5499725937843323
Epoch 2390, training loss: 418.24212646484375 = 0.5187667608261108 + 50.0 * 8.354467391967773
Epoch 2390, val loss: 0.5489900708198547
Epoch 2400, training loss: 418.3559875488281 = 0.517534613609314 + 50.0 * 8.356768608093262
Epoch 2400, val loss: 0.5479386448860168
Epoch 2410, training loss: 418.2207336425781 = 0.5162132382392883 + 50.0 * 8.354090690612793
Epoch 2410, val loss: 0.5469942688941956
Epoch 2420, training loss: 418.2142639160156 = 0.5149912238121033 + 50.0 * 8.353985786437988
Epoch 2420, val loss: 0.5460000038146973
Epoch 2430, training loss: 418.2059020996094 = 0.5138107538223267 + 50.0 * 8.353841781616211
Epoch 2430, val loss: 0.5450745820999146
Epoch 2440, training loss: 418.29498291015625 = 0.5126470327377319 + 50.0 * 8.355647087097168
Epoch 2440, val loss: 0.5441197156906128
Epoch 2450, training loss: 418.1882629394531 = 0.5114359855651855 + 50.0 * 8.353536605834961
Epoch 2450, val loss: 0.5433013439178467
Epoch 2460, training loss: 418.1833190917969 = 0.510299801826477 + 50.0 * 8.353460311889648
Epoch 2460, val loss: 0.5423891544342041
Epoch 2470, training loss: 418.1606140136719 = 0.5091917514801025 + 50.0 * 8.353028297424316
Epoch 2470, val loss: 0.5416111350059509
Epoch 2480, training loss: 418.1513977050781 = 0.5080918669700623 + 50.0 * 8.352866172790527
Epoch 2480, val loss: 0.5407545566558838
Epoch 2490, training loss: 418.1697082519531 = 0.5069998502731323 + 50.0 * 8.353254318237305
Epoch 2490, val loss: 0.5399139523506165
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7843734145104008
0.8114902557415056
=== training gcn model ===
Epoch 0, training loss: 530.21826171875 = 1.1060770750045776 + 50.0 * 10.582243919372559
Epoch 0, val loss: 1.105768084526062
Epoch 10, training loss: 530.1910400390625 = 1.101662278175354 + 50.0 * 10.581787109375
Epoch 10, val loss: 1.1013567447662354
Epoch 20, training loss: 530.09375 = 1.0970659255981445 + 50.0 * 10.579934120178223
Epoch 20, val loss: 1.0967506170272827
Epoch 30, training loss: 529.6871337890625 = 1.0923138856887817 + 50.0 * 10.571897506713867
Epoch 30, val loss: 1.0919768810272217
Epoch 40, training loss: 528.0552368164062 = 1.0872875452041626 + 50.0 * 10.539359092712402
Epoch 40, val loss: 1.0869253873825073
Epoch 50, training loss: 522.3260498046875 = 1.081908941268921 + 50.0 * 10.424882888793945
Epoch 50, val loss: 1.08151376247406
Epoch 60, training loss: 506.44134521484375 = 1.0760186910629272 + 50.0 * 10.107306480407715
Epoch 60, val loss: 1.0755233764648438
Epoch 70, training loss: 483.7444763183594 = 1.0693092346191406 + 50.0 * 9.65350341796875
Epoch 70, val loss: 1.0688719749450684
Epoch 80, training loss: 477.24981689453125 = 1.0637179613113403 + 50.0 * 9.523721694946289
Epoch 80, val loss: 1.063538670539856
Epoch 90, training loss: 469.69903564453125 = 1.059931993484497 + 50.0 * 9.372781753540039
Epoch 90, val loss: 1.059966802597046
Epoch 100, training loss: 463.0462646484375 = 1.0570043325424194 + 50.0 * 9.239785194396973
Epoch 100, val loss: 1.057106375694275
Epoch 110, training loss: 460.5876159667969 = 1.0541096925735474 + 50.0 * 9.190670013427734
Epoch 110, val loss: 1.054254174232483
Epoch 120, training loss: 458.8251953125 = 1.0514509677886963 + 50.0 * 9.155474662780762
Epoch 120, val loss: 1.0516947507858276
Epoch 130, training loss: 455.9563293457031 = 1.0494801998138428 + 50.0 * 9.098136901855469
Epoch 130, val loss: 1.0498614311218262
Epoch 140, training loss: 451.5831298828125 = 1.0483185052871704 + 50.0 * 9.010696411132812
Epoch 140, val loss: 1.048835277557373
Epoch 150, training loss: 446.8831787109375 = 1.047381043434143 + 50.0 * 8.916715621948242
Epoch 150, val loss: 1.048006296157837
Epoch 160, training loss: 443.62213134765625 = 1.0466203689575195 + 50.0 * 8.851510047912598
Epoch 160, val loss: 1.0473122596740723
Epoch 170, training loss: 440.48858642578125 = 1.0458687543869019 + 50.0 * 8.788854598999023
Epoch 170, val loss: 1.0466152429580688
Epoch 180, training loss: 438.35711669921875 = 1.0451416969299316 + 50.0 * 8.74623966217041
Epoch 180, val loss: 1.0459392070770264
Epoch 190, training loss: 437.03717041015625 = 1.0443689823150635 + 50.0 * 8.719856262207031
Epoch 190, val loss: 1.0452110767364502
Epoch 200, training loss: 435.70953369140625 = 1.0435112714767456 + 50.0 * 8.693320274353027
Epoch 200, val loss: 1.0444060564041138
Epoch 210, training loss: 434.52020263671875 = 1.0426794290542603 + 50.0 * 8.669549942016602
Epoch 210, val loss: 1.0436350107192993
Epoch 220, training loss: 433.2098388671875 = 1.04202139377594 + 50.0 * 8.643356323242188
Epoch 220, val loss: 1.043027639389038
Epoch 230, training loss: 432.23175048828125 = 1.0415067672729492 + 50.0 * 8.623805046081543
Epoch 230, val loss: 1.0425413846969604
Epoch 240, training loss: 431.3452453613281 = 1.0409239530563354 + 50.0 * 8.606086730957031
Epoch 240, val loss: 1.0419938564300537
Epoch 250, training loss: 430.6292419433594 = 1.0403026342391968 + 50.0 * 8.591778755187988
Epoch 250, val loss: 1.0414189100265503
Epoch 260, training loss: 430.02301025390625 = 1.0396367311477661 + 50.0 * 8.579667091369629
Epoch 260, val loss: 1.0407755374908447
Epoch 270, training loss: 429.513671875 = 1.038906455039978 + 50.0 * 8.56949520111084
Epoch 270, val loss: 1.04006028175354
Epoch 280, training loss: 429.0605773925781 = 1.0380935668945312 + 50.0 * 8.560449600219727
Epoch 280, val loss: 1.0392850637435913
Epoch 290, training loss: 428.6496276855469 = 1.0372589826583862 + 50.0 * 8.552247047424316
Epoch 290, val loss: 1.0384788513183594
Epoch 300, training loss: 428.3594665527344 = 1.0363984107971191 + 50.0 * 8.54646110534668
Epoch 300, val loss: 1.03764009475708
Epoch 310, training loss: 427.9236145019531 = 1.0354902744293213 + 50.0 * 8.537762641906738
Epoch 310, val loss: 1.036773681640625
Epoch 320, training loss: 427.60260009765625 = 1.0345946550369263 + 50.0 * 8.531359672546387
Epoch 320, val loss: 1.0359342098236084
Epoch 330, training loss: 427.323486328125 = 1.0337015390396118 + 50.0 * 8.525795936584473
Epoch 330, val loss: 1.0350886583328247
Epoch 340, training loss: 427.09722900390625 = 1.0327707529067993 + 50.0 * 8.521288871765137
Epoch 340, val loss: 1.034187912940979
Epoch 350, training loss: 426.8177795410156 = 1.0318143367767334 + 50.0 * 8.515719413757324
Epoch 350, val loss: 1.033277153968811
Epoch 360, training loss: 426.5881652832031 = 1.030855655670166 + 50.0 * 8.511146545410156
Epoch 360, val loss: 1.0323724746704102
Epoch 370, training loss: 426.3793640136719 = 1.0298875570297241 + 50.0 * 8.506989479064941
Epoch 370, val loss: 1.031449556350708
Epoch 380, training loss: 426.1741943359375 = 1.0288926362991333 + 50.0 * 8.50290584564209
Epoch 380, val loss: 1.0304828882217407
Epoch 390, training loss: 425.94384765625 = 1.0278676748275757 + 50.0 * 8.498319625854492
Epoch 390, val loss: 1.0295159816741943
Epoch 400, training loss: 425.73565673828125 = 1.0268415212631226 + 50.0 * 8.494175910949707
Epoch 400, val loss: 1.0285358428955078
Epoch 410, training loss: 425.67559814453125 = 1.0258020162582397 + 50.0 * 8.492996215820312
Epoch 410, val loss: 1.0275394916534424
Epoch 420, training loss: 425.4289855957031 = 1.0246561765670776 + 50.0 * 8.488086700439453
Epoch 420, val loss: 1.0264445543289185
Epoch 430, training loss: 425.1392822265625 = 1.0235323905944824 + 50.0 * 8.482315063476562
Epoch 430, val loss: 1.0253673791885376
Epoch 440, training loss: 424.959716796875 = 1.0224097967147827 + 50.0 * 8.47874641418457
Epoch 440, val loss: 1.0243055820465088
Epoch 450, training loss: 424.7713928222656 = 1.0212680101394653 + 50.0 * 8.47500228881836
Epoch 450, val loss: 1.023217797279358
Epoch 460, training loss: 424.59661865234375 = 1.0201002359390259 + 50.0 * 8.471529960632324
Epoch 460, val loss: 1.0221041440963745
Epoch 470, training loss: 424.5255432128906 = 1.0188547372817993 + 50.0 * 8.470133781433105
Epoch 470, val loss: 1.0209108591079712
Epoch 480, training loss: 424.30926513671875 = 1.0175468921661377 + 50.0 * 8.465834617614746
Epoch 480, val loss: 1.019661784172058
Epoch 490, training loss: 424.143798828125 = 1.0162358283996582 + 50.0 * 8.46255111694336
Epoch 490, val loss: 1.018399715423584
Epoch 500, training loss: 424.0157470703125 = 1.014878749847412 + 50.0 * 8.460017204284668
Epoch 500, val loss: 1.0170937776565552
Epoch 510, training loss: 423.98431396484375 = 1.0134650468826294 + 50.0 * 8.459417343139648
Epoch 510, val loss: 1.0157411098480225
Epoch 520, training loss: 423.82794189453125 = 1.0119614601135254 + 50.0 * 8.456319808959961
Epoch 520, val loss: 1.0143115520477295
Epoch 530, training loss: 423.6733093261719 = 1.0104302167892456 + 50.0 * 8.45325756072998
Epoch 530, val loss: 1.0128461122512817
Epoch 540, training loss: 423.5845031738281 = 1.0088540315628052 + 50.0 * 8.451513290405273
Epoch 540, val loss: 1.0113295316696167
Epoch 550, training loss: 423.5057067871094 = 1.0072323083877563 + 50.0 * 8.449969291687012
Epoch 550, val loss: 1.009788990020752
Epoch 560, training loss: 423.41973876953125 = 1.0055317878723145 + 50.0 * 8.448284149169922
Epoch 560, val loss: 1.008148193359375
Epoch 570, training loss: 423.3703308105469 = 1.003749966621399 + 50.0 * 8.447331428527832
Epoch 570, val loss: 1.0064557790756226
Epoch 580, training loss: 423.2604675292969 = 1.0019737482070923 + 50.0 * 8.445169448852539
Epoch 580, val loss: 1.0047577619552612
Epoch 590, training loss: 423.21923828125 = 1.000152587890625 + 50.0 * 8.444381713867188
Epoch 590, val loss: 1.0030090808868408
Epoch 600, training loss: 423.0820007324219 = 0.9982324838638306 + 50.0 * 8.441675186157227
Epoch 600, val loss: 1.0011706352233887
Epoch 610, training loss: 423.01068115234375 = 0.9962972402572632 + 50.0 * 8.440287590026855
Epoch 610, val loss: 0.9993155002593994
Epoch 620, training loss: 422.9392395019531 = 0.9943329691886902 + 50.0 * 8.438898086547852
Epoch 620, val loss: 0.9974372386932373
Epoch 630, training loss: 422.914794921875 = 0.9923141598701477 + 50.0 * 8.43844985961914
Epoch 630, val loss: 0.995522677898407
Epoch 640, training loss: 422.9209289550781 = 0.9902302026748657 + 50.0 * 8.438613891601562
Epoch 640, val loss: 0.993442177772522
Epoch 650, training loss: 422.73760986328125 = 0.9880730509757996 + 50.0 * 8.434990882873535
Epoch 650, val loss: 0.9914202094078064
Epoch 660, training loss: 422.6737060546875 = 0.9859524369239807 + 50.0 * 8.433754920959473
Epoch 660, val loss: 0.9893937706947327
Epoch 670, training loss: 422.61505126953125 = 0.9838315844535828 + 50.0 * 8.432624816894531
Epoch 670, val loss: 0.9873669743537903
Epoch 680, training loss: 422.7079162597656 = 0.98163902759552 + 50.0 * 8.434525489807129
Epoch 680, val loss: 0.9852310419082642
Epoch 690, training loss: 422.4915466308594 = 0.9793632626533508 + 50.0 * 8.430243492126465
Epoch 690, val loss: 0.9830492734909058
Epoch 700, training loss: 422.42901611328125 = 0.9770975112915039 + 50.0 * 8.429038047790527
Epoch 700, val loss: 0.9808480143547058
Epoch 710, training loss: 422.3382873535156 = 0.974824070930481 + 50.0 * 8.427268981933594
Epoch 710, val loss: 0.978661060333252
Epoch 720, training loss: 422.28692626953125 = 0.972517192363739 + 50.0 * 8.426288604736328
Epoch 720, val loss: 0.9764386415481567
Epoch 730, training loss: 422.3928527832031 = 0.970099687576294 + 50.0 * 8.428455352783203
Epoch 730, val loss: 0.9740716814994812
Epoch 740, training loss: 422.21533203125 = 0.9675506353378296 + 50.0 * 8.424955368041992
Epoch 740, val loss: 0.9716053605079651
Epoch 750, training loss: 422.0977783203125 = 0.9650592803955078 + 50.0 * 8.422654151916504
Epoch 750, val loss: 0.9692070484161377
Epoch 760, training loss: 422.02996826171875 = 0.9625884294509888 + 50.0 * 8.421347618103027
Epoch 760, val loss: 0.9668202996253967
Epoch 770, training loss: 421.98577880859375 = 0.9600974321365356 + 50.0 * 8.420514106750488
Epoch 770, val loss: 0.9643900990486145
Epoch 780, training loss: 422.0487060546875 = 0.9575070142745972 + 50.0 * 8.421823501586914
Epoch 780, val loss: 0.9618501663208008
Epoch 790, training loss: 421.8764953613281 = 0.9548971056938171 + 50.0 * 8.418432235717773
Epoch 790, val loss: 0.9593216776847839
Epoch 800, training loss: 421.79901123046875 = 0.9523082375526428 + 50.0 * 8.4169340133667
Epoch 800, val loss: 0.9568085670471191
Epoch 810, training loss: 421.7274475097656 = 0.949734091758728 + 50.0 * 8.41555404663086
Epoch 810, val loss: 0.9543150663375854
Epoch 820, training loss: 421.689453125 = 0.9471490979194641 + 50.0 * 8.414846420288086
Epoch 820, val loss: 0.9517791867256165
Epoch 830, training loss: 421.7488708496094 = 0.9444465637207031 + 50.0 * 8.416088104248047
Epoch 830, val loss: 0.9490922093391418
Epoch 840, training loss: 421.6619873046875 = 0.9416605830192566 + 50.0 * 8.414406776428223
Epoch 840, val loss: 0.9464309811592102
Epoch 850, training loss: 421.52044677734375 = 0.9389572143554688 + 50.0 * 8.411629676818848
Epoch 850, val loss: 0.9437749981880188
Epoch 860, training loss: 421.4554443359375 = 0.9362868666648865 + 50.0 * 8.410383224487305
Epoch 860, val loss: 0.9411643147468567
Epoch 870, training loss: 421.5042724609375 = 0.933556079864502 + 50.0 * 8.41141414642334
Epoch 870, val loss: 0.9384822249412537
Epoch 880, training loss: 421.34771728515625 = 0.9307751655578613 + 50.0 * 8.40833854675293
Epoch 880, val loss: 0.9357514977455139
Epoch 890, training loss: 421.29095458984375 = 0.9280401468276978 + 50.0 * 8.407258033752441
Epoch 890, val loss: 0.9330710172653198
Epoch 900, training loss: 421.3319396972656 = 0.9253157377243042 + 50.0 * 8.408132553100586
Epoch 900, val loss: 0.9303760528564453
Epoch 910, training loss: 421.267578125 = 0.9224289655685425 + 50.0 * 8.406903266906738
Epoch 910, val loss: 0.9276046752929688
Epoch 920, training loss: 421.18084716796875 = 0.9195400476455688 + 50.0 * 8.405226707458496
Epoch 920, val loss: 0.9247419238090515
Epoch 930, training loss: 421.1302185058594 = 0.9166629314422607 + 50.0 * 8.404271125793457
Epoch 930, val loss: 0.9219481348991394
Epoch 940, training loss: 421.1080017089844 = 0.9137675166130066 + 50.0 * 8.403884887695312
Epoch 940, val loss: 0.9191156029701233
Epoch 950, training loss: 421.0757141113281 = 0.9108474850654602 + 50.0 * 8.403297424316406
Epoch 950, val loss: 0.9161961674690247
Epoch 960, training loss: 420.97283935546875 = 0.9078792929649353 + 50.0 * 8.401299476623535
Epoch 960, val loss: 0.9133375287055969
Epoch 970, training loss: 420.9310607910156 = 0.9049227833747864 + 50.0 * 8.400522232055664
Epoch 970, val loss: 0.9104194045066833
Epoch 980, training loss: 420.9784851074219 = 0.9019138216972351 + 50.0 * 8.401531219482422
Epoch 980, val loss: 0.9074454307556152
Epoch 990, training loss: 420.8707580566406 = 0.8988565802574158 + 50.0 * 8.39943790435791
Epoch 990, val loss: 0.9044872522354126
Epoch 1000, training loss: 420.8570861816406 = 0.8958145976066589 + 50.0 * 8.399225234985352
Epoch 1000, val loss: 0.90150386095047
Epoch 1010, training loss: 420.8268127441406 = 0.8927330374717712 + 50.0 * 8.398681640625
Epoch 1010, val loss: 0.8984173536300659
Epoch 1020, training loss: 420.8220520019531 = 0.88956218957901 + 50.0 * 8.398650169372559
Epoch 1020, val loss: 0.8953418135643005
Epoch 1030, training loss: 420.6930236816406 = 0.8864454030990601 + 50.0 * 8.39613151550293
Epoch 1030, val loss: 0.8922803401947021
Epoch 1040, training loss: 420.64794921875 = 0.8833363652229309 + 50.0 * 8.395292282104492
Epoch 1040, val loss: 0.8892431259155273
Epoch 1050, training loss: 420.64727783203125 = 0.8802081942558289 + 50.0 * 8.395340919494629
Epoch 1050, val loss: 0.8861719965934753
Epoch 1060, training loss: 420.69146728515625 = 0.8769815564155579 + 50.0 * 8.396289825439453
Epoch 1060, val loss: 0.8830070495605469
Epoch 1070, training loss: 420.5856018066406 = 0.8737179636955261 + 50.0 * 8.394237518310547
Epoch 1070, val loss: 0.8798158168792725
Epoch 1080, training loss: 420.5260009765625 = 0.8704655766487122 + 50.0 * 8.393111228942871
Epoch 1080, val loss: 0.87662672996521
Epoch 1090, training loss: 420.5117492675781 = 0.8672201037406921 + 50.0 * 8.392890930175781
Epoch 1090, val loss: 0.8734517097473145
Epoch 1100, training loss: 420.5081481933594 = 0.863935649394989 + 50.0 * 8.392884254455566
Epoch 1100, val loss: 0.8702264428138733
Epoch 1110, training loss: 420.4378967285156 = 0.8606024384498596 + 50.0 * 8.391546249389648
Epoch 1110, val loss: 0.8669680953025818
Epoch 1120, training loss: 420.53778076171875 = 0.8572543859481812 + 50.0 * 8.393610000610352
Epoch 1120, val loss: 0.8636423349380493
Epoch 1130, training loss: 420.425048828125 = 0.8537740111351013 + 50.0 * 8.391426086425781
Epoch 1130, val loss: 0.8602514863014221
Epoch 1140, training loss: 420.35760498046875 = 0.8503490090370178 + 50.0 * 8.390145301818848
Epoch 1140, val loss: 0.8569092750549316
Epoch 1150, training loss: 420.32232666015625 = 0.8469528555870056 + 50.0 * 8.389507293701172
Epoch 1150, val loss: 0.85361248254776
Epoch 1160, training loss: 420.3623352050781 = 0.8435651659965515 + 50.0 * 8.390375137329102
Epoch 1160, val loss: 0.8502926230430603
Epoch 1170, training loss: 420.3020935058594 = 0.8400002121925354 + 50.0 * 8.389242172241211
Epoch 1170, val loss: 0.8467653393745422
Epoch 1180, training loss: 420.26336669921875 = 0.8364598155021667 + 50.0 * 8.388538360595703
Epoch 1180, val loss: 0.8433441519737244
Epoch 1190, training loss: 420.2326354980469 = 0.8329874277114868 + 50.0 * 8.387992858886719
Epoch 1190, val loss: 0.8399558067321777
Epoch 1200, training loss: 420.19757080078125 = 0.8295326232910156 + 50.0 * 8.387360572814941
Epoch 1200, val loss: 0.8366181254386902
Epoch 1210, training loss: 420.21600341796875 = 0.8260473608970642 + 50.0 * 8.387799263000488
Epoch 1210, val loss: 0.8332682847976685
Epoch 1220, training loss: 420.1651611328125 = 0.8224313259124756 + 50.0 * 8.386855125427246
Epoch 1220, val loss: 0.829667866230011
Epoch 1230, training loss: 420.1488952636719 = 0.818814218044281 + 50.0 * 8.386601448059082
Epoch 1230, val loss: 0.826192319393158
Epoch 1240, training loss: 420.1407470703125 = 0.8152508735656738 + 50.0 * 8.386509895324707
Epoch 1240, val loss: 0.8226593732833862
Epoch 1250, training loss: 420.13677978515625 = 0.8116686344146729 + 50.0 * 8.386502265930176
Epoch 1250, val loss: 0.8192005157470703
Epoch 1260, training loss: 420.0613708496094 = 0.8080725073814392 + 50.0 * 8.385066032409668
Epoch 1260, val loss: 0.8157159090042114
Epoch 1270, training loss: 420.079345703125 = 0.8044916987419128 + 50.0 * 8.385497093200684
Epoch 1270, val loss: 0.812215268611908
Epoch 1280, training loss: 420.1567687988281 = 0.8008301258087158 + 50.0 * 8.387118339538574
Epoch 1280, val loss: 0.8086503744125366
Epoch 1290, training loss: 420.0306396484375 = 0.7971253395080566 + 50.0 * 8.38467025756836
Epoch 1290, val loss: 0.8050557374954224
Epoch 1300, training loss: 420.0172119140625 = 0.7934799194335938 + 50.0 * 8.384474754333496
Epoch 1300, val loss: 0.8015105724334717
Epoch 1310, training loss: 420.04852294921875 = 0.7898190021514893 + 50.0 * 8.385173797607422
Epoch 1310, val loss: 0.7979719638824463
Epoch 1320, training loss: 419.94732666015625 = 0.7861579060554504 + 50.0 * 8.383223533630371
Epoch 1320, val loss: 0.7944290637969971
Epoch 1330, training loss: 419.9393615722656 = 0.7825339436531067 + 50.0 * 8.383136749267578
Epoch 1330, val loss: 0.7909325957298279
Epoch 1340, training loss: 419.91046142578125 = 0.7789299488067627 + 50.0 * 8.382630348205566
Epoch 1340, val loss: 0.7874472737312317
Epoch 1350, training loss: 419.9087219238281 = 0.7753055095672607 + 50.0 * 8.382668495178223
Epoch 1350, val loss: 0.783945620059967
Epoch 1360, training loss: 420.00177001953125 = 0.7716187238693237 + 50.0 * 8.384603500366211
Epoch 1360, val loss: 0.7803717255592346
Epoch 1370, training loss: 419.8946533203125 = 0.7679342031478882 + 50.0 * 8.38253402709961
Epoch 1370, val loss: 0.7768047451972961
Epoch 1380, training loss: 419.8358459472656 = 0.7642443776130676 + 50.0 * 8.381431579589844
Epoch 1380, val loss: 0.7732741236686707
Epoch 1390, training loss: 419.8241882324219 = 0.7606689929962158 + 50.0 * 8.381270408630371
Epoch 1390, val loss: 0.7698014974594116
Epoch 1400, training loss: 419.86749267578125 = 0.7570489048957825 + 50.0 * 8.382208824157715
Epoch 1400, val loss: 0.7663083076477051
Epoch 1410, training loss: 419.7735595703125 = 0.7533960938453674 + 50.0 * 8.380403518676758
Epoch 1410, val loss: 0.7628647685050964
Epoch 1420, training loss: 419.7671203613281 = 0.7498114705085754 + 50.0 * 8.380346298217773
Epoch 1420, val loss: 0.7594109773635864
Epoch 1430, training loss: 419.89862060546875 = 0.7461531162261963 + 50.0 * 8.383049011230469
Epoch 1430, val loss: 0.7559048533439636
Epoch 1440, training loss: 419.7223205566406 = 0.7424970865249634 + 50.0 * 8.379596710205078
Epoch 1440, val loss: 0.7523377537727356
Epoch 1450, training loss: 419.6820983886719 = 0.7389021515846252 + 50.0 * 8.378864288330078
Epoch 1450, val loss: 0.7489064931869507
Epoch 1460, training loss: 419.6653137207031 = 0.7353590726852417 + 50.0 * 8.378599166870117
Epoch 1460, val loss: 0.7455207705497742
Epoch 1470, training loss: 419.8335876464844 = 0.731814444065094 + 50.0 * 8.382035255432129
Epoch 1470, val loss: 0.7420363426208496
Epoch 1480, training loss: 419.6918029785156 = 0.7280874252319336 + 50.0 * 8.379274368286133
Epoch 1480, val loss: 0.7385983467102051
Epoch 1490, training loss: 419.613037109375 = 0.7245324850082397 + 50.0 * 8.37777042388916
Epoch 1490, val loss: 0.7351216673851013
Epoch 1500, training loss: 419.6076354980469 = 0.72099769115448 + 50.0 * 8.377732276916504
Epoch 1500, val loss: 0.7317972779273987
Epoch 1510, training loss: 419.7642822265625 = 0.7173331379890442 + 50.0 * 8.380938529968262
Epoch 1510, val loss: 0.7282968759536743
Epoch 1520, training loss: 419.5545349121094 = 0.7136646509170532 + 50.0 * 8.37681770324707
Epoch 1520, val loss: 0.7247931361198425
Epoch 1530, training loss: 419.5461120605469 = 0.7101781964302063 + 50.0 * 8.376718521118164
Epoch 1530, val loss: 0.7214555144309998
Epoch 1540, training loss: 419.49688720703125 = 0.7067852020263672 + 50.0 * 8.375802040100098
Epoch 1540, val loss: 0.7182531952857971
Epoch 1550, training loss: 419.4853210449219 = 0.7034159898757935 + 50.0 * 8.375638008117676
Epoch 1550, val loss: 0.7150599956512451
Epoch 1560, training loss: 419.6673278808594 = 0.7000362277030945 + 50.0 * 8.379345893859863
Epoch 1560, val loss: 0.7117444276809692
Epoch 1570, training loss: 419.5220947265625 = 0.6964151859283447 + 50.0 * 8.376513481140137
Epoch 1570, val loss: 0.7085060477256775
Epoch 1580, training loss: 419.4565124511719 = 0.6930156946182251 + 50.0 * 8.375269889831543
Epoch 1580, val loss: 0.705171525478363
Epoch 1590, training loss: 419.41351318359375 = 0.689626932144165 + 50.0 * 8.37447738647461
Epoch 1590, val loss: 0.7020415663719177
Epoch 1600, training loss: 419.4879455566406 = 0.6862996220588684 + 50.0 * 8.376032829284668
Epoch 1600, val loss: 0.6989142298698425
Epoch 1610, training loss: 419.374755859375 = 0.682839035987854 + 50.0 * 8.373838424682617
Epoch 1610, val loss: 0.6956140995025635
Epoch 1620, training loss: 419.3610534667969 = 0.6794865727424622 + 50.0 * 8.373631477355957
Epoch 1620, val loss: 0.6924428939819336
Epoch 1630, training loss: 419.3313903808594 = 0.6761854887008667 + 50.0 * 8.373104095458984
Epoch 1630, val loss: 0.6893530488014221
Epoch 1640, training loss: 419.3146667480469 = 0.6729196310043335 + 50.0 * 8.372835159301758
Epoch 1640, val loss: 0.6862967014312744
Epoch 1650, training loss: 419.3343200683594 = 0.6696751713752747 + 50.0 * 8.373292922973633
Epoch 1650, val loss: 0.6832566857337952
Epoch 1660, training loss: 419.4366455078125 = 0.6663157939910889 + 50.0 * 8.375406265258789
Epoch 1660, val loss: 0.6800365447998047
Epoch 1670, training loss: 419.2698974609375 = 0.66291743516922 + 50.0 * 8.372139930725098
Epoch 1670, val loss: 0.6768677830696106
Epoch 1680, training loss: 419.2710266113281 = 0.6596525311470032 + 50.0 * 8.372227668762207
Epoch 1680, val loss: 0.6738026142120361
Epoch 1690, training loss: 419.2350769042969 = 0.6564984321594238 + 50.0 * 8.37157154083252
Epoch 1690, val loss: 0.6708706021308899
Epoch 1700, training loss: 419.2068176269531 = 0.653389036655426 + 50.0 * 8.371068954467773
Epoch 1700, val loss: 0.667970597743988
Epoch 1710, training loss: 419.1938171386719 = 0.6502999067306519 + 50.0 * 8.370870590209961
Epoch 1710, val loss: 0.665094256401062
Epoch 1720, training loss: 419.3382568359375 = 0.6471734046936035 + 50.0 * 8.373821258544922
Epoch 1720, val loss: 0.6622152924537659
Epoch 1730, training loss: 419.21759033203125 = 0.643971860408783 + 50.0 * 8.371472358703613
Epoch 1730, val loss: 0.6591998338699341
Epoch 1740, training loss: 419.20623779296875 = 0.6408059597015381 + 50.0 * 8.371308326721191
Epoch 1740, val loss: 0.6562781929969788
Epoch 1750, training loss: 419.2551574707031 = 0.6377504467964172 + 50.0 * 8.372347831726074
Epoch 1750, val loss: 0.6534468531608582
Epoch 1760, training loss: 419.1114196777344 = 0.6347037553787231 + 50.0 * 8.369534492492676
Epoch 1760, val loss: 0.6505346298217773
Epoch 1770, training loss: 419.1085510253906 = 0.6317451000213623 + 50.0 * 8.369536399841309
Epoch 1770, val loss: 0.6477769613265991
Epoch 1780, training loss: 419.0759582519531 = 0.6288386583328247 + 50.0 * 8.368942260742188
Epoch 1780, val loss: 0.645131528377533
Epoch 1790, training loss: 419.10223388671875 = 0.6259422898292542 + 50.0 * 8.369525909423828
Epoch 1790, val loss: 0.642466127872467
Epoch 1800, training loss: 419.1717224121094 = 0.622986912727356 + 50.0 * 8.37097454071045
Epoch 1800, val loss: 0.6396417021751404
Epoch 1810, training loss: 419.0551452636719 = 0.6200478672981262 + 50.0 * 8.368701934814453
Epoch 1810, val loss: 0.6369196772575378
Epoch 1820, training loss: 419.03961181640625 = 0.6171731352806091 + 50.0 * 8.368448257446289
Epoch 1820, val loss: 0.6343309879302979
Epoch 1830, training loss: 419.1102294921875 = 0.6143817901611328 + 50.0 * 8.369916915893555
Epoch 1830, val loss: 0.6317031383514404
Epoch 1840, training loss: 419.0056457519531 = 0.611545205116272 + 50.0 * 8.367881774902344
Epoch 1840, val loss: 0.6290698647499084
Epoch 1850, training loss: 418.9984130859375 = 0.6088029146194458 + 50.0 * 8.367792129516602
Epoch 1850, val loss: 0.626582682132721
Epoch 1860, training loss: 418.97186279296875 = 0.6061039566993713 + 50.0 * 8.367315292358398
Epoch 1860, val loss: 0.624082088470459
Epoch 1870, training loss: 419.12359619140625 = 0.6034085154533386 + 50.0 * 8.370404243469238
Epoch 1870, val loss: 0.6215362548828125
Epoch 1880, training loss: 418.9848937988281 = 0.6006650924682617 + 50.0 * 8.367684364318848
Epoch 1880, val loss: 0.619073748588562
Epoch 1890, training loss: 418.931396484375 = 0.5980420708656311 + 50.0 * 8.366666793823242
Epoch 1890, val loss: 0.6166322231292725
Epoch 1900, training loss: 418.9117431640625 = 0.5954684019088745 + 50.0 * 8.366325378417969
Epoch 1900, val loss: 0.6143015623092651
Epoch 1910, training loss: 418.978271484375 = 0.5929444432258606 + 50.0 * 8.367706298828125
Epoch 1910, val loss: 0.6119157075881958
Epoch 1920, training loss: 418.9066162109375 = 0.5903232097625732 + 50.0 * 8.366325378417969
Epoch 1920, val loss: 0.6095669865608215
Epoch 1930, training loss: 418.8813171386719 = 0.5878077149391174 + 50.0 * 8.365870475769043
Epoch 1930, val loss: 0.6072889566421509
Epoch 1940, training loss: 418.9039611816406 = 0.5853452086448669 + 50.0 * 8.366372108459473
Epoch 1940, val loss: 0.6049748659133911
Epoch 1950, training loss: 418.93194580078125 = 0.5828861594200134 + 50.0 * 8.366981506347656
Epoch 1950, val loss: 0.6027522087097168
Epoch 1960, training loss: 418.8651428222656 = 0.5804758071899414 + 50.0 * 8.365693092346191
Epoch 1960, val loss: 0.6005824208259583
Epoch 1970, training loss: 418.82464599609375 = 0.5781090259552002 + 50.0 * 8.364931106567383
Epoch 1970, val loss: 0.5984159111976624
Epoch 1980, training loss: 418.8415832519531 = 0.575798749923706 + 50.0 * 8.365315437316895
Epoch 1980, val loss: 0.5962749123573303
Epoch 1990, training loss: 418.84124755859375 = 0.573485791683197 + 50.0 * 8.365355491638184
Epoch 1990, val loss: 0.5942063927650452
Epoch 2000, training loss: 418.8865051269531 = 0.5711902976036072 + 50.0 * 8.36630630493164
Epoch 2000, val loss: 0.5921942591667175
Epoch 2010, training loss: 418.78778076171875 = 0.5687997937202454 + 50.0 * 8.3643798828125
Epoch 2010, val loss: 0.5899676084518433
Epoch 2020, training loss: 418.7625427246094 = 0.5665895342826843 + 50.0 * 8.363919258117676
Epoch 2020, val loss: 0.5879706144332886
Epoch 2030, training loss: 418.7468566894531 = 0.5644203424453735 + 50.0 * 8.363648414611816
Epoch 2030, val loss: 0.5860041975975037
Epoch 2040, training loss: 418.7278747558594 = 0.5623432993888855 + 50.0 * 8.363310813903809
Epoch 2040, val loss: 0.5841564536094666
Epoch 2050, training loss: 418.710693359375 = 0.5602757930755615 + 50.0 * 8.363008499145508
Epoch 2050, val loss: 0.5822846293449402
Epoch 2060, training loss: 418.76263427734375 = 0.5582331418991089 + 50.0 * 8.36408805847168
Epoch 2060, val loss: 0.5804111957550049
Epoch 2070, training loss: 418.7940368652344 = 0.5560568571090698 + 50.0 * 8.36475944519043
Epoch 2070, val loss: 0.5784406661987305
Epoch 2080, training loss: 418.72503662109375 = 0.5539443492889404 + 50.0 * 8.363421440124512
Epoch 2080, val loss: 0.5765142440795898
Epoch 2090, training loss: 418.6898193359375 = 0.5519482493400574 + 50.0 * 8.362757682800293
Epoch 2090, val loss: 0.5747836828231812
Epoch 2100, training loss: 418.66253662109375 = 0.5500274300575256 + 50.0 * 8.362250328063965
Epoch 2100, val loss: 0.5730674266815186
Epoch 2110, training loss: 418.6773376464844 = 0.5481389760971069 + 50.0 * 8.362584114074707
Epoch 2110, val loss: 0.5713298916816711
Epoch 2120, training loss: 418.8247985839844 = 0.5462214946746826 + 50.0 * 8.365571022033691
Epoch 2120, val loss: 0.5695309042930603
Epoch 2130, training loss: 418.69036865234375 = 0.544284462928772 + 50.0 * 8.362921714782715
Epoch 2130, val loss: 0.5680075883865356
Epoch 2140, training loss: 418.63861083984375 = 0.5424416065216064 + 50.0 * 8.361923217773438
Epoch 2140, val loss: 0.5662610530853271
Epoch 2150, training loss: 418.6429748535156 = 0.5406471490859985 + 50.0 * 8.362046241760254
Epoch 2150, val loss: 0.5647302269935608
Epoch 2160, training loss: 418.7995300292969 = 0.538830041885376 + 50.0 * 8.365214347839355
Epoch 2160, val loss: 0.563019871711731
Epoch 2170, training loss: 418.6853332519531 = 0.5370081663131714 + 50.0 * 8.362966537475586
Epoch 2170, val loss: 0.5615171194076538
Epoch 2180, training loss: 418.6004943847656 = 0.5352557897567749 + 50.0 * 8.361305236816406
Epoch 2180, val loss: 0.5599120855331421
Epoch 2190, training loss: 418.57855224609375 = 0.5335757732391357 + 50.0 * 8.360899925231934
Epoch 2190, val loss: 0.558499813079834
Epoch 2200, training loss: 418.5941162109375 = 0.5319228172302246 + 50.0 * 8.361244201660156
Epoch 2200, val loss: 0.5570037364959717
Epoch 2210, training loss: 418.6534729003906 = 0.530241847038269 + 50.0 * 8.362464904785156
Epoch 2210, val loss: 0.5555496215820312
Epoch 2220, training loss: 418.6224670410156 = 0.5285971164703369 + 50.0 * 8.36187744140625
Epoch 2220, val loss: 0.5540647506713867
Epoch 2230, training loss: 418.5755615234375 = 0.5269386768341064 + 50.0 * 8.36097240447998
Epoch 2230, val loss: 0.5526061058044434
Epoch 2240, training loss: 418.53704833984375 = 0.5253795385360718 + 50.0 * 8.360233306884766
Epoch 2240, val loss: 0.5513455271720886
Epoch 2250, training loss: 418.5155334472656 = 0.5238406658172607 + 50.0 * 8.359833717346191
Epoch 2250, val loss: 0.549945056438446
Epoch 2260, training loss: 418.5005187988281 = 0.5223498344421387 + 50.0 * 8.359563827514648
Epoch 2260, val loss: 0.5486670136451721
Epoch 2270, training loss: 418.58929443359375 = 0.520864725112915 + 50.0 * 8.361368179321289
Epoch 2270, val loss: 0.5473664402961731
Epoch 2280, training loss: 418.5090637207031 = 0.5193001627922058 + 50.0 * 8.359795570373535
Epoch 2280, val loss: 0.5460271239280701
Epoch 2290, training loss: 418.4916076660156 = 0.5178064703941345 + 50.0 * 8.359476089477539
Epoch 2290, val loss: 0.5447755455970764
Epoch 2300, training loss: 418.4940185546875 = 0.5163684487342834 + 50.0 * 8.359553337097168
Epoch 2300, val loss: 0.5435457825660706
Epoch 2310, training loss: 418.5677490234375 = 0.5149344801902771 + 50.0 * 8.361056327819824
Epoch 2310, val loss: 0.5423493385314941
Epoch 2320, training loss: 418.4706726074219 = 0.5135244727134705 + 50.0 * 8.359143257141113
Epoch 2320, val loss: 0.5411233901977539
Epoch 2330, training loss: 418.5914306640625 = 0.51214200258255 + 50.0 * 8.36158561706543
Epoch 2330, val loss: 0.5399836301803589
Epoch 2340, training loss: 418.4659118652344 = 0.5107388496398926 + 50.0 * 8.359103202819824
Epoch 2340, val loss: 0.5387070178985596
Epoch 2350, training loss: 418.4310607910156 = 0.5094276666641235 + 50.0 * 8.35843276977539
Epoch 2350, val loss: 0.5376167893409729
Epoch 2360, training loss: 418.40966796875 = 0.5081385374069214 + 50.0 * 8.358030319213867
Epoch 2360, val loss: 0.5365173816680908
Epoch 2370, training loss: 418.424560546875 = 0.5068715810775757 + 50.0 * 8.358353614807129
Epoch 2370, val loss: 0.5354552268981934
Epoch 2380, training loss: 418.51995849609375 = 0.505574107170105 + 50.0 * 8.3602876663208
Epoch 2380, val loss: 0.5343127846717834
Epoch 2390, training loss: 418.54010009765625 = 0.504279613494873 + 50.0 * 8.360716819763184
Epoch 2390, val loss: 0.5331703424453735
Epoch 2400, training loss: 418.40155029296875 = 0.5029790997505188 + 50.0 * 8.35797119140625
Epoch 2400, val loss: 0.5322253704071045
Epoch 2410, training loss: 418.37774658203125 = 0.5017493963241577 + 50.0 * 8.35752010345459
Epoch 2410, val loss: 0.5312179327011108
Epoch 2420, training loss: 418.3801574707031 = 0.5005652904510498 + 50.0 * 8.35759162902832
Epoch 2420, val loss: 0.5302030444145203
Epoch 2430, training loss: 418.41448974609375 = 0.4993886351585388 + 50.0 * 8.358302116394043
Epoch 2430, val loss: 0.5292820930480957
Epoch 2440, training loss: 418.3634948730469 = 0.49821949005126953 + 50.0 * 8.357305526733398
Epoch 2440, val loss: 0.5283505916595459
Epoch 2450, training loss: 418.4093017578125 = 0.4970763325691223 + 50.0 * 8.358244895935059
Epoch 2450, val loss: 0.527367889881134
Epoch 2460, training loss: 418.35498046875 = 0.49590155482292175 + 50.0 * 8.357181549072266
Epoch 2460, val loss: 0.5263842940330505
Epoch 2470, training loss: 418.3853454589844 = 0.4947628378868103 + 50.0 * 8.35781192779541
Epoch 2470, val loss: 0.5255109071731567
Epoch 2480, training loss: 418.3823547363281 = 0.4936531186103821 + 50.0 * 8.357773780822754
Epoch 2480, val loss: 0.5246304273605347
Epoch 2490, training loss: 418.3061828613281 = 0.4925483167171478 + 50.0 * 8.35627269744873
Epoch 2490, val loss: 0.5236790180206299
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7970573313039067
0.8126494240382526
The final CL Acc:0.79046, 0.00519, The final GNN Acc:0.81248, 0.00075
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 111044])
remove edge: torch.Size([2, 66302])
updated graph: torch.Size([2, 88698])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 530.2054443359375 = 1.0923144817352295 + 50.0 * 10.582262992858887
Epoch 0, val loss: 1.0928698778152466
Epoch 10, training loss: 530.1765747070312 = 1.0881452560424805 + 50.0 * 10.581768989562988
Epoch 10, val loss: 1.0886359214782715
Epoch 20, training loss: 530.0509643554688 = 1.0834680795669556 + 50.0 * 10.579349517822266
Epoch 20, val loss: 1.0838885307312012
Epoch 30, training loss: 529.4915771484375 = 1.0782846212387085 + 50.0 * 10.568265914916992
Epoch 30, val loss: 1.0786426067352295
Epoch 40, training loss: 527.5017700195312 = 1.0728164911270142 + 50.0 * 10.528578758239746
Epoch 40, val loss: 1.073099136352539
Epoch 50, training loss: 522.5259399414062 = 1.0673123598098755 + 50.0 * 10.42917251586914
Epoch 50, val loss: 1.0675057172775269
Epoch 60, training loss: 512.411376953125 = 1.0624209642410278 + 50.0 * 10.22697925567627
Epoch 60, val loss: 1.0624295473098755
Epoch 70, training loss: 495.6329650878906 = 1.0570887327194214 + 50.0 * 9.891517639160156
Epoch 70, val loss: 1.056776523590088
Epoch 80, training loss: 480.49530029296875 = 1.0527784824371338 + 50.0 * 9.588850975036621
Epoch 80, val loss: 1.0525169372558594
Epoch 90, training loss: 474.7604064941406 = 1.048854112625122 + 50.0 * 9.474230766296387
Epoch 90, val loss: 1.0487250089645386
Epoch 100, training loss: 469.8851013183594 = 1.0459028482437134 + 50.0 * 9.376784324645996
Epoch 100, val loss: 1.045966386795044
Epoch 110, training loss: 464.67144775390625 = 1.043677568435669 + 50.0 * 9.272555351257324
Epoch 110, val loss: 1.0439114570617676
Epoch 120, training loss: 459.3262939453125 = 1.0417697429656982 + 50.0 * 9.165690422058105
Epoch 120, val loss: 1.042130708694458
Epoch 130, training loss: 454.8976135253906 = 1.0401206016540527 + 50.0 * 9.077149391174316
Epoch 130, val loss: 1.0405620336532593
Epoch 140, training loss: 450.7903137207031 = 1.0390254259109497 + 50.0 * 8.995025634765625
Epoch 140, val loss: 1.0395457744598389
Epoch 150, training loss: 447.693359375 = 1.0381571054458618 + 50.0 * 8.933104515075684
Epoch 150, val loss: 1.0386698246002197
Epoch 160, training loss: 445.8350524902344 = 1.036773920059204 + 50.0 * 8.895965576171875
Epoch 160, val loss: 1.037236213684082
Epoch 170, training loss: 443.3890075683594 = 1.0351836681365967 + 50.0 * 8.847076416015625
Epoch 170, val loss: 1.035667061805725
Epoch 180, training loss: 441.1583557128906 = 1.0345425605773926 + 50.0 * 8.802475929260254
Epoch 180, val loss: 1.0350830554962158
Epoch 190, training loss: 439.8189392089844 = 1.0347567796707153 + 50.0 * 8.775683403015137
Epoch 190, val loss: 1.0352264642715454
Epoch 200, training loss: 438.280029296875 = 1.0343401432037354 + 50.0 * 8.744914054870605
Epoch 200, val loss: 1.0345895290374756
Epoch 210, training loss: 437.0202331542969 = 1.0335712432861328 + 50.0 * 8.719733238220215
Epoch 210, val loss: 1.033834457397461
Epoch 220, training loss: 435.7572937011719 = 1.0329663753509521 + 50.0 * 8.694486618041992
Epoch 220, val loss: 1.0331789255142212
Epoch 230, training loss: 434.8247375488281 = 1.0320565700531006 + 50.0 * 8.675853729248047
Epoch 230, val loss: 1.0322599411010742
Epoch 240, training loss: 433.83001708984375 = 1.030975103378296 + 50.0 * 8.655981063842773
Epoch 240, val loss: 1.031214952468872
Epoch 250, training loss: 432.75 = 1.0301214456558228 + 50.0 * 8.634397506713867
Epoch 250, val loss: 1.030379056930542
Epoch 260, training loss: 431.7218933105469 = 1.0295305252075195 + 50.0 * 8.613846778869629
Epoch 260, val loss: 1.0298291444778442
Epoch 270, training loss: 430.9020690917969 = 1.0288245677947998 + 50.0 * 8.597464561462402
Epoch 270, val loss: 1.0291061401367188
Epoch 280, training loss: 430.1219787597656 = 1.0279576778411865 + 50.0 * 8.581880569458008
Epoch 280, val loss: 1.0282891988754272
Epoch 290, training loss: 429.42974853515625 = 1.0270925760269165 + 50.0 * 8.568053245544434
Epoch 290, val loss: 1.0274591445922852
Epoch 300, training loss: 428.8499450683594 = 1.0262280702590942 + 50.0 * 8.556474685668945
Epoch 300, val loss: 1.0265697240829468
Epoch 310, training loss: 428.2545166015625 = 1.025296688079834 + 50.0 * 8.544584274291992
Epoch 310, val loss: 1.0257121324539185
Epoch 320, training loss: 427.7091369628906 = 1.024379014968872 + 50.0 * 8.533695220947266
Epoch 320, val loss: 1.024820327758789
Epoch 330, training loss: 427.2352294921875 = 1.0234185457229614 + 50.0 * 8.524236679077148
Epoch 330, val loss: 1.0238760709762573
Epoch 340, training loss: 426.9614562988281 = 1.0224285125732422 + 50.0 * 8.518780708312988
Epoch 340, val loss: 1.0228530168533325
Epoch 350, training loss: 426.416015625 = 1.0213038921356201 + 50.0 * 8.507894515991211
Epoch 350, val loss: 1.021813154220581
Epoch 360, training loss: 426.0251770019531 = 1.0202710628509521 + 50.0 * 8.50009822845459
Epoch 360, val loss: 1.0208494663238525
Epoch 370, training loss: 425.68634033203125 = 1.019230842590332 + 50.0 * 8.493342399597168
Epoch 370, val loss: 1.019859790802002
Epoch 380, training loss: 425.3464660644531 = 1.0181022882461548 + 50.0 * 8.486567497253418
Epoch 380, val loss: 1.0187597274780273
Epoch 390, training loss: 424.9971618652344 = 1.0169804096221924 + 50.0 * 8.47960376739502
Epoch 390, val loss: 1.0176899433135986
Epoch 400, training loss: 424.7123107910156 = 1.0158443450927734 + 50.0 * 8.473929405212402
Epoch 400, val loss: 1.0166171789169312
Epoch 410, training loss: 424.55694580078125 = 1.0145878791809082 + 50.0 * 8.470847129821777
Epoch 410, val loss: 1.0153056383132935
Epoch 420, training loss: 424.2082214355469 = 1.0132499933242798 + 50.0 * 8.463899612426758
Epoch 420, val loss: 1.014086365699768
Epoch 430, training loss: 424.0000305175781 = 1.0119640827178955 + 50.0 * 8.459761619567871
Epoch 430, val loss: 1.0128687620162964
Epoch 440, training loss: 423.7811279296875 = 1.0106343030929565 + 50.0 * 8.45541000366211
Epoch 440, val loss: 1.0115537643432617
Epoch 450, training loss: 423.5890808105469 = 1.0092120170593262 + 50.0 * 8.451597213745117
Epoch 450, val loss: 1.0101724863052368
Epoch 460, training loss: 423.5809631347656 = 1.007707953453064 + 50.0 * 8.451464653015137
Epoch 460, val loss: 1.0087778568267822
Epoch 470, training loss: 423.27520751953125 = 1.0060535669326782 + 50.0 * 8.445383071899414
Epoch 470, val loss: 1.0070899724960327
Epoch 480, training loss: 423.1303405761719 = 1.0044022798538208 + 50.0 * 8.442519187927246
Epoch 480, val loss: 1.0054965019226074
Epoch 490, training loss: 422.94219970703125 = 1.002764344215393 + 50.0 * 8.438788414001465
Epoch 490, val loss: 1.003893494606018
Epoch 500, training loss: 422.8082275390625 = 1.0010470151901245 + 50.0 * 8.43614387512207
Epoch 500, val loss: 1.0022228956222534
Epoch 510, training loss: 422.6693115234375 = 0.9992998838424683 + 50.0 * 8.43340015411377
Epoch 510, val loss: 1.0004619359970093
Epoch 520, training loss: 422.5434875488281 = 0.9974172711372375 + 50.0 * 8.43092155456543
Epoch 520, val loss: 0.99863600730896
Epoch 530, training loss: 422.3659973144531 = 0.9954858422279358 + 50.0 * 8.427410125732422
Epoch 530, val loss: 0.9967901706695557
Epoch 540, training loss: 422.2466125488281 = 0.9935587644577026 + 50.0 * 8.425061225891113
Epoch 540, val loss: 0.9948504567146301
Epoch 550, training loss: 422.1944274902344 = 0.9914752244949341 + 50.0 * 8.42405891418457
Epoch 550, val loss: 0.9928354620933533
Epoch 560, training loss: 421.9838562011719 = 0.9893674850463867 + 50.0 * 8.419889450073242
Epoch 560, val loss: 0.990792453289032
Epoch 570, training loss: 421.8166809082031 = 0.9873371124267578 + 50.0 * 8.416586875915527
Epoch 570, val loss: 0.9888160824775696
Epoch 580, training loss: 421.6555480957031 = 0.9853293895721436 + 50.0 * 8.41340446472168
Epoch 580, val loss: 0.9868382215499878
Epoch 590, training loss: 421.5094299316406 = 0.9832981824874878 + 50.0 * 8.4105224609375
Epoch 590, val loss: 0.9848253130912781
Epoch 600, training loss: 421.650634765625 = 0.9812197089195251 + 50.0 * 8.4133882522583
Epoch 600, val loss: 0.9826095104217529
Epoch 610, training loss: 421.26324462890625 = 0.9787911772727966 + 50.0 * 8.405689239501953
Epoch 610, val loss: 0.9804286360740662
Epoch 620, training loss: 421.1324462890625 = 0.976599931716919 + 50.0 * 8.403117179870605
Epoch 620, val loss: 0.9783304333686829
Epoch 630, training loss: 420.9868469238281 = 0.9744246006011963 + 50.0 * 8.400248527526855
Epoch 630, val loss: 0.9761767983436584
Epoch 640, training loss: 420.8623352050781 = 0.972141444683075 + 50.0 * 8.397804260253906
Epoch 640, val loss: 0.9739537835121155
Epoch 650, training loss: 420.8282775878906 = 0.969782829284668 + 50.0 * 8.397170066833496
Epoch 650, val loss: 0.9716526865959167
Epoch 660, training loss: 420.7060852050781 = 0.9671215415000916 + 50.0 * 8.394779205322266
Epoch 660, val loss: 0.9690823554992676
Epoch 670, training loss: 420.5462341308594 = 0.964625358581543 + 50.0 * 8.391632080078125
Epoch 670, val loss: 0.9666352868080139
Epoch 680, training loss: 420.4557800292969 = 0.9621518850326538 + 50.0 * 8.389872550964355
Epoch 680, val loss: 0.9641892910003662
Epoch 690, training loss: 420.3673400878906 = 0.9595848917961121 + 50.0 * 8.388154983520508
Epoch 690, val loss: 0.961692214012146
Epoch 700, training loss: 420.4820556640625 = 0.9568427801132202 + 50.0 * 8.390503883361816
Epoch 700, val loss: 0.9591396450996399
Epoch 710, training loss: 420.21685791015625 = 0.9539836049079895 + 50.0 * 8.385257720947266
Epoch 710, val loss: 0.9561561346054077
Epoch 720, training loss: 420.1465148925781 = 0.951136589050293 + 50.0 * 8.383907318115234
Epoch 720, val loss: 0.9534955620765686
Epoch 730, training loss: 420.0574035644531 = 0.9483879804611206 + 50.0 * 8.382180213928223
Epoch 730, val loss: 0.9507645964622498
Epoch 740, training loss: 419.9690856933594 = 0.9455220103263855 + 50.0 * 8.380471229553223
Epoch 740, val loss: 0.947986364364624
Epoch 750, training loss: 419.8975524902344 = 0.9426026940345764 + 50.0 * 8.379098892211914
Epoch 750, val loss: 0.9451699256896973
Epoch 760, training loss: 420.1486511230469 = 0.9394991397857666 + 50.0 * 8.384182929992676
Epoch 760, val loss: 0.9423381686210632
Epoch 770, training loss: 419.8318786621094 = 0.9363953471183777 + 50.0 * 8.377909660339355
Epoch 770, val loss: 0.9390025734901428
Epoch 780, training loss: 419.7330322265625 = 0.9333577156066895 + 50.0 * 8.375993728637695
Epoch 780, val loss: 0.9360401034355164
Epoch 790, training loss: 419.61737060546875 = 0.9302955865859985 + 50.0 * 8.373741149902344
Epoch 790, val loss: 0.933110237121582
Epoch 800, training loss: 419.55670166015625 = 0.927190363407135 + 50.0 * 8.372590065002441
Epoch 800, val loss: 0.9301550388336182
Epoch 810, training loss: 419.5283508300781 = 0.9240798950195312 + 50.0 * 8.372085571289062
Epoch 810, val loss: 0.927077054977417
Epoch 820, training loss: 419.4866943359375 = 0.9207158088684082 + 50.0 * 8.371319770812988
Epoch 820, val loss: 0.9237439632415771
Epoch 830, training loss: 419.3671569824219 = 0.9173337817192078 + 50.0 * 8.368996620178223
Epoch 830, val loss: 0.9204980134963989
Epoch 840, training loss: 419.31622314453125 = 0.9140239357948303 + 50.0 * 8.368043899536133
Epoch 840, val loss: 0.9172810316085815
Epoch 850, training loss: 419.2574768066406 = 0.9106859564781189 + 50.0 * 8.366935729980469
Epoch 850, val loss: 0.9140418767929077
Epoch 860, training loss: 419.389892578125 = 0.9072425365447998 + 50.0 * 8.36965274810791
Epoch 860, val loss: 0.9106769561767578
Epoch 870, training loss: 419.19561767578125 = 0.9036343693733215 + 50.0 * 8.365839958190918
Epoch 870, val loss: 0.9070873856544495
Epoch 880, training loss: 419.1138916015625 = 0.8999993205070496 + 50.0 * 8.364277839660645
Epoch 880, val loss: 0.9036838412284851
Epoch 890, training loss: 419.06903076171875 = 0.8964290022850037 + 50.0 * 8.363451957702637
Epoch 890, val loss: 0.9002330899238586
Epoch 900, training loss: 419.10723876953125 = 0.8927376866340637 + 50.0 * 8.364290237426758
Epoch 900, val loss: 0.8967251777648926
Epoch 910, training loss: 418.9529113769531 = 0.8890304565429688 + 50.0 * 8.36127758026123
Epoch 910, val loss: 0.8929889798164368
Epoch 920, training loss: 418.9330139160156 = 0.8853399157524109 + 50.0 * 8.360953330993652
Epoch 920, val loss: 0.8893932700157166
Epoch 930, training loss: 418.8623046875 = 0.881544291973114 + 50.0 * 8.359615325927734
Epoch 930, val loss: 0.8857933282852173
Epoch 940, training loss: 418.9488830566406 = 0.8777087330818176 + 50.0 * 8.36142349243164
Epoch 940, val loss: 0.8821183443069458
Epoch 950, training loss: 418.8902587890625 = 0.8737009167671204 + 50.0 * 8.360331535339355
Epoch 950, val loss: 0.8781501650810242
Epoch 960, training loss: 418.7593688964844 = 0.8698213696479797 + 50.0 * 8.35779094696045
Epoch 960, val loss: 0.8743877410888672
Epoch 970, training loss: 418.8405456542969 = 0.8659555315971375 + 50.0 * 8.359491348266602
Epoch 970, val loss: 0.870543360710144
Epoch 980, training loss: 418.7148742675781 = 0.8618178963661194 + 50.0 * 8.357061386108398
Epoch 980, val loss: 0.8668869137763977
Epoch 990, training loss: 418.6466369628906 = 0.8580148816108704 + 50.0 * 8.355772972106934
Epoch 990, val loss: 0.8630070090293884
Epoch 1000, training loss: 418.6507263183594 = 0.8539595603942871 + 50.0 * 8.355935096740723
Epoch 1000, val loss: 0.8592092990875244
Epoch 1010, training loss: 418.5585632324219 = 0.8499243855476379 + 50.0 * 8.354172706604004
Epoch 1010, val loss: 0.8552616238594055
Epoch 1020, training loss: 418.5067138671875 = 0.8458350896835327 + 50.0 * 8.353218078613281
Epoch 1020, val loss: 0.8514419198036194
Epoch 1030, training loss: 418.53192138671875 = 0.8417064547538757 + 50.0 * 8.353804588317871
Epoch 1030, val loss: 0.8475388884544373
Epoch 1040, training loss: 418.4638671875 = 0.8373450040817261 + 50.0 * 8.352530479431152
Epoch 1040, val loss: 0.8432387709617615
Epoch 1050, training loss: 418.4316101074219 = 0.8330832123756409 + 50.0 * 8.351970672607422
Epoch 1050, val loss: 0.8392189741134644
Epoch 1060, training loss: 418.3597412109375 = 0.8290742039680481 + 50.0 * 8.350613594055176
Epoch 1060, val loss: 0.8352599143981934
Epoch 1070, training loss: 418.3040771484375 = 0.8249684572219849 + 50.0 * 8.349581718444824
Epoch 1070, val loss: 0.8313948512077332
Epoch 1080, training loss: 418.2640380859375 = 0.820879340171814 + 50.0 * 8.348862648010254
Epoch 1080, val loss: 0.8274582028388977
Epoch 1090, training loss: 418.24407958984375 = 0.816692054271698 + 50.0 * 8.34854793548584
Epoch 1090, val loss: 0.8235065340995789
Epoch 1100, training loss: 418.32098388671875 = 0.8122985363006592 + 50.0 * 8.350173950195312
Epoch 1100, val loss: 0.8192840814590454
Epoch 1110, training loss: 418.2196044921875 = 0.8079471588134766 + 50.0 * 8.348233222961426
Epoch 1110, val loss: 0.8150350451469421
Epoch 1120, training loss: 418.11810302734375 = 0.8036978840827942 + 50.0 * 8.346287727355957
Epoch 1120, val loss: 0.8110209107398987
Epoch 1130, training loss: 418.07928466796875 = 0.7995418906211853 + 50.0 * 8.345595359802246
Epoch 1130, val loss: 0.807009756565094
Epoch 1140, training loss: 418.0561218261719 = 0.7953188419342041 + 50.0 * 8.345215797424316
Epoch 1140, val loss: 0.8030371069908142
Epoch 1150, training loss: 418.07958984375 = 0.7908936738967896 + 50.0 * 8.345773696899414
Epoch 1150, val loss: 0.7988182902336121
Epoch 1160, training loss: 418.0163879394531 = 0.7865002751350403 + 50.0 * 8.344597816467285
Epoch 1160, val loss: 0.794614851474762
Epoch 1170, training loss: 417.9400939941406 = 0.7822021245956421 + 50.0 * 8.343157768249512
Epoch 1170, val loss: 0.7904894351959229
Epoch 1180, training loss: 417.9355163574219 = 0.777934730052948 + 50.0 * 8.343152046203613
Epoch 1180, val loss: 0.7864325046539307
Epoch 1190, training loss: 417.96697998046875 = 0.773513913154602 + 50.0 * 8.34386920928955
Epoch 1190, val loss: 0.7822765707969666
Epoch 1200, training loss: 417.85357666015625 = 0.7692617774009705 + 50.0 * 8.341686248779297
Epoch 1200, val loss: 0.778053879737854
Epoch 1210, training loss: 417.7800598144531 = 0.7649753093719482 + 50.0 * 8.340301513671875
Epoch 1210, val loss: 0.7740358710289001
Epoch 1220, training loss: 417.77069091796875 = 0.7606985569000244 + 50.0 * 8.34019947052002
Epoch 1220, val loss: 0.7700069546699524
Epoch 1230, training loss: 417.87359619140625 = 0.7563626766204834 + 50.0 * 8.342345237731934
Epoch 1230, val loss: 0.7657798528671265
Epoch 1240, training loss: 417.76971435546875 = 0.7518576979637146 + 50.0 * 8.340356826782227
Epoch 1240, val loss: 0.7614218592643738
Epoch 1250, training loss: 417.6740417480469 = 0.7473644018173218 + 50.0 * 8.338533401489258
Epoch 1250, val loss: 0.7573340535163879
Epoch 1260, training loss: 417.6273193359375 = 0.7431078553199768 + 50.0 * 8.337684631347656
Epoch 1260, val loss: 0.7532564401626587
Epoch 1270, training loss: 417.5892028808594 = 0.7388404607772827 + 50.0 * 8.337007522583008
Epoch 1270, val loss: 0.7491634488105774
Epoch 1280, training loss: 417.55767822265625 = 0.7345383167266846 + 50.0 * 8.33646297454834
Epoch 1280, val loss: 0.7450718283653259
Epoch 1290, training loss: 417.76116943359375 = 0.7302671670913696 + 50.0 * 8.340618133544922
Epoch 1290, val loss: 0.7407788634300232
Epoch 1300, training loss: 417.6220703125 = 0.7253433465957642 + 50.0 * 8.337934494018555
Epoch 1300, val loss: 0.736505925655365
Epoch 1310, training loss: 417.5074768066406 = 0.7210438847541809 + 50.0 * 8.335728645324707
Epoch 1310, val loss: 0.7322056889533997
Epoch 1320, training loss: 417.43121337890625 = 0.7166796922683716 + 50.0 * 8.334290504455566
Epoch 1320, val loss: 0.7281912565231323
Epoch 1330, training loss: 417.3992004394531 = 0.7124201059341431 + 50.0 * 8.333735466003418
Epoch 1330, val loss: 0.7241520881652832
Epoch 1340, training loss: 417.37713623046875 = 0.7081589698791504 + 50.0 * 8.333379745483398
Epoch 1340, val loss: 0.7201071381568909
Epoch 1350, training loss: 417.5780334472656 = 0.7037690877914429 + 50.0 * 8.337485313415527
Epoch 1350, val loss: 0.7160038352012634
Epoch 1360, training loss: 417.37591552734375 = 0.6993053555488586 + 50.0 * 8.333532333374023
Epoch 1360, val loss: 0.7116828560829163
Epoch 1370, training loss: 417.2882385253906 = 0.694991946220398 + 50.0 * 8.331865310668945
Epoch 1370, val loss: 0.7076295018196106
Epoch 1380, training loss: 417.2690124511719 = 0.690780520439148 + 50.0 * 8.331564903259277
Epoch 1380, val loss: 0.703680157661438
Epoch 1390, training loss: 417.2527160644531 = 0.6866122484207153 + 50.0 * 8.331321716308594
Epoch 1390, val loss: 0.6996839046478271
Epoch 1400, training loss: 417.3055725097656 = 0.6823647022247314 + 50.0 * 8.332464218139648
Epoch 1400, val loss: 0.6956331133842468
Epoch 1410, training loss: 417.2611083984375 = 0.6781282424926758 + 50.0 * 8.331659317016602
Epoch 1410, val loss: 0.6915959119796753
Epoch 1420, training loss: 417.20416259765625 = 0.6738169193267822 + 50.0 * 8.330606460571289
Epoch 1420, val loss: 0.687538206577301
Epoch 1430, training loss: 417.1667785644531 = 0.6695730686187744 + 50.0 * 8.329943656921387
Epoch 1430, val loss: 0.683655858039856
Epoch 1440, training loss: 417.1206970214844 = 0.6655303239822388 + 50.0 * 8.329103469848633
Epoch 1440, val loss: 0.6798115372657776
Epoch 1450, training loss: 417.10870361328125 = 0.6614450812339783 + 50.0 * 8.32894515991211
Epoch 1450, val loss: 0.676006555557251
Epoch 1460, training loss: 417.2455749511719 = 0.6572643518447876 + 50.0 * 8.331766128540039
Epoch 1460, val loss: 0.6721539497375488
Epoch 1470, training loss: 417.13604736328125 = 0.6532216668128967 + 50.0 * 8.329656600952148
Epoch 1470, val loss: 0.6680559515953064
Epoch 1480, training loss: 417.0419006347656 = 0.6490570902824402 + 50.0 * 8.32785701751709
Epoch 1480, val loss: 0.6642954349517822
Epoch 1490, training loss: 417.006591796875 = 0.645156979560852 + 50.0 * 8.327228546142578
Epoch 1490, val loss: 0.6605305075645447
Epoch 1500, training loss: 416.97735595703125 = 0.6412496566772461 + 50.0 * 8.326722145080566
Epoch 1500, val loss: 0.6568726301193237
Epoch 1510, training loss: 416.9779357910156 = 0.6373932957649231 + 50.0 * 8.326810836791992
Epoch 1510, val loss: 0.6531788110733032
Epoch 1520, training loss: 417.0461730957031 = 0.6334596872329712 + 50.0 * 8.328254699707031
Epoch 1520, val loss: 0.649425745010376
Epoch 1530, training loss: 416.942626953125 = 0.6294669508934021 + 50.0 * 8.326263427734375
Epoch 1530, val loss: 0.6457959413528442
Epoch 1540, training loss: 416.89935302734375 = 0.625710666179657 + 50.0 * 8.325472831726074
Epoch 1540, val loss: 0.6422080993652344
Epoch 1550, training loss: 417.09259033203125 = 0.6219736933708191 + 50.0 * 8.329412460327148
Epoch 1550, val loss: 0.6385512948036194
Epoch 1560, training loss: 416.9142150878906 = 0.6179338693618774 + 50.0 * 8.325925827026367
Epoch 1560, val loss: 0.6349116563796997
Epoch 1570, training loss: 416.832275390625 = 0.6142160296440125 + 50.0 * 8.324360847473145
Epoch 1570, val loss: 0.6314244866371155
Epoch 1580, training loss: 416.80059814453125 = 0.6106510162353516 + 50.0 * 8.323799133300781
Epoch 1580, val loss: 0.6280678510665894
Epoch 1590, training loss: 416.779296875 = 0.6070993542671204 + 50.0 * 8.323444366455078
Epoch 1590, val loss: 0.6247422099113464
Epoch 1600, training loss: 416.7572937011719 = 0.6035752296447754 + 50.0 * 8.323074340820312
Epoch 1600, val loss: 0.6214452385902405
Epoch 1610, training loss: 416.7435302734375 = 0.6000611782073975 + 50.0 * 8.322869300842285
Epoch 1610, val loss: 0.6181545257568359
Epoch 1620, training loss: 416.9796142578125 = 0.5964416861534119 + 50.0 * 8.32766342163086
Epoch 1620, val loss: 0.6149162650108337
Epoch 1630, training loss: 416.8383483886719 = 0.5928277373313904 + 50.0 * 8.324910163879395
Epoch 1630, val loss: 0.6112787127494812
Epoch 1640, training loss: 416.6882019042969 = 0.589320182800293 + 50.0 * 8.321977615356445
Epoch 1640, val loss: 0.6080546975135803
Epoch 1650, training loss: 416.6781921386719 = 0.5859495997428894 + 50.0 * 8.321845054626465
Epoch 1650, val loss: 0.6049414873123169
Epoch 1660, training loss: 416.64471435546875 = 0.5827361941337585 + 50.0 * 8.321239471435547
Epoch 1660, val loss: 0.6019061803817749
Epoch 1670, training loss: 416.62823486328125 = 0.5795252919197083 + 50.0 * 8.320974349975586
Epoch 1670, val loss: 0.5989199280738831
Epoch 1680, training loss: 416.7305603027344 = 0.5762614011764526 + 50.0 * 8.32308578491211
Epoch 1680, val loss: 0.5959488153457642
Epoch 1690, training loss: 416.6270751953125 = 0.5729590654373169 + 50.0 * 8.32108211517334
Epoch 1690, val loss: 0.5927758812904358
Epoch 1700, training loss: 416.606201171875 = 0.569717526435852 + 50.0 * 8.32072925567627
Epoch 1700, val loss: 0.5898491740226746
Epoch 1710, training loss: 416.5618591308594 = 0.5666729211807251 + 50.0 * 8.319903373718262
Epoch 1710, val loss: 0.5869446992874146
Epoch 1720, training loss: 416.5554504394531 = 0.5636304616928101 + 50.0 * 8.319836616516113
Epoch 1720, val loss: 0.5841260552406311
Epoch 1730, training loss: 416.8861999511719 = 0.5605883598327637 + 50.0 * 8.326512336730957
Epoch 1730, val loss: 0.5811237692832947
Epoch 1740, training loss: 416.5567626953125 = 0.5571575164794922 + 50.0 * 8.319992065429688
Epoch 1740, val loss: 0.5781858563423157
Epoch 1750, training loss: 416.505126953125 = 0.5542463660240173 + 50.0 * 8.31901741027832
Epoch 1750, val loss: 0.5753903985023499
Epoch 1760, training loss: 416.4798583984375 = 0.5513817667961121 + 50.0 * 8.31856918334961
Epoch 1760, val loss: 0.5728417038917542
Epoch 1770, training loss: 416.4555358886719 = 0.5486081838607788 + 50.0 * 8.318138122558594
Epoch 1770, val loss: 0.5702464580535889
Epoch 1780, training loss: 416.4360656738281 = 0.5458352565765381 + 50.0 * 8.317804336547852
Epoch 1780, val loss: 0.5676717758178711
Epoch 1790, training loss: 416.421142578125 = 0.5430970191955566 + 50.0 * 8.317561149597168
Epoch 1790, val loss: 0.5651443004608154
Epoch 1800, training loss: 416.4345397949219 = 0.5403885841369629 + 50.0 * 8.317883491516113
Epoch 1800, val loss: 0.562602698802948
Epoch 1810, training loss: 416.6219482421875 = 0.5376462936401367 + 50.0 * 8.321685791015625
Epoch 1810, val loss: 0.5599466562271118
Epoch 1820, training loss: 416.4313049316406 = 0.5346925258636475 + 50.0 * 8.31793212890625
Epoch 1820, val loss: 0.5574682950973511
Epoch 1830, training loss: 416.37078857421875 = 0.5320686101913452 + 50.0 * 8.316774368286133
Epoch 1830, val loss: 0.5550729036331177
Epoch 1840, training loss: 416.3454895019531 = 0.5295122265815735 + 50.0 * 8.316319465637207
Epoch 1840, val loss: 0.5526894330978394
Epoch 1850, training loss: 416.331298828125 = 0.527013897895813 + 50.0 * 8.316085815429688
Epoch 1850, val loss: 0.5504005551338196
Epoch 1860, training loss: 416.34185791015625 = 0.5245363712310791 + 50.0 * 8.316346168518066
Epoch 1860, val loss: 0.5480756759643555
Epoch 1870, training loss: 416.5633239746094 = 0.5220205187797546 + 50.0 * 8.320825576782227
Epoch 1870, val loss: 0.5456663966178894
Epoch 1880, training loss: 416.3622741699219 = 0.5192699432373047 + 50.0 * 8.31686019897461
Epoch 1880, val loss: 0.5434335470199585
Epoch 1890, training loss: 416.2916259765625 = 0.5169335007667542 + 50.0 * 8.3154935836792
Epoch 1890, val loss: 0.541175901889801
Epoch 1900, training loss: 416.2626037597656 = 0.5145348906517029 + 50.0 * 8.314961433410645
Epoch 1900, val loss: 0.5390518307685852
Epoch 1910, training loss: 416.2474670410156 = 0.5122289061546326 + 50.0 * 8.314704895019531
Epoch 1910, val loss: 0.5369259119033813
Epoch 1920, training loss: 416.24029541015625 = 0.5099161863327026 + 50.0 * 8.314607620239258
Epoch 1920, val loss: 0.5348593592643738
Epoch 1930, training loss: 416.4571228027344 = 0.5076342225074768 + 50.0 * 8.318989753723145
Epoch 1930, val loss: 0.5327773094177246
Epoch 1940, training loss: 416.3022766113281 = 0.50510174036026 + 50.0 * 8.315943717956543
Epoch 1940, val loss: 0.5304811000823975
Epoch 1950, training loss: 416.228271484375 = 0.5028296709060669 + 50.0 * 8.314508438110352
Epoch 1950, val loss: 0.5284581780433655
Epoch 1960, training loss: 416.19287109375 = 0.5006417036056519 + 50.0 * 8.313844680786133
Epoch 1960, val loss: 0.5265247821807861
Epoch 1970, training loss: 416.2151184082031 = 0.49849390983581543 + 50.0 * 8.314332008361816
Epoch 1970, val loss: 0.524589478969574
Epoch 1980, training loss: 416.23846435546875 = 0.4962993562221527 + 50.0 * 8.31484317779541
Epoch 1980, val loss: 0.5226600766181946
Epoch 1990, training loss: 416.15875244140625 = 0.4942326247692108 + 50.0 * 8.3132905960083
Epoch 1990, val loss: 0.5206890106201172
Epoch 2000, training loss: 416.1430358886719 = 0.4921780526638031 + 50.0 * 8.313016891479492
Epoch 2000, val loss: 0.5188318490982056
Epoch 2010, training loss: 416.15093994140625 = 0.4901292622089386 + 50.0 * 8.313216209411621
Epoch 2010, val loss: 0.5170257687568665
Epoch 2020, training loss: 416.19769287109375 = 0.4880787134170532 + 50.0 * 8.314192771911621
Epoch 2020, val loss: 0.5151382088661194
Epoch 2030, training loss: 416.1324768066406 = 0.4860382378101349 + 50.0 * 8.312928199768066
Epoch 2030, val loss: 0.513357937335968
Epoch 2040, training loss: 416.1274108886719 = 0.48402664065361023 + 50.0 * 8.312867164611816
Epoch 2040, val loss: 0.5115931630134583
Epoch 2050, training loss: 416.24786376953125 = 0.48197612166404724 + 50.0 * 8.31531810760498
Epoch 2050, val loss: 0.5098976492881775
Epoch 2060, training loss: 416.08538818359375 = 0.4800780415534973 + 50.0 * 8.312106132507324
Epoch 2060, val loss: 0.5080744028091431
Epoch 2070, training loss: 416.0562438964844 = 0.47817832231521606 + 50.0 * 8.311561584472656
Epoch 2070, val loss: 0.5064254999160767
Epoch 2080, training loss: 416.05047607421875 = 0.47634902596473694 + 50.0 * 8.311482429504395
Epoch 2080, val loss: 0.5048285722732544
Epoch 2090, training loss: 416.0301513671875 = 0.4745776951313019 + 50.0 * 8.311111450195312
Epoch 2090, val loss: 0.5032336711883545
Epoch 2100, training loss: 416.0467529296875 = 0.472819447517395 + 50.0 * 8.311478614807129
Epoch 2100, val loss: 0.5016928911209106
Epoch 2110, training loss: 416.1172180175781 = 0.4709935784339905 + 50.0 * 8.3129243850708
Epoch 2110, val loss: 0.500079333782196
Epoch 2120, training loss: 416.04364013671875 = 0.46915557980537415 + 50.0 * 8.311490058898926
Epoch 2120, val loss: 0.49850839376449585
Epoch 2130, training loss: 416.1239929199219 = 0.46738603711128235 + 50.0 * 8.313132286071777
Epoch 2130, val loss: 0.4970177710056305
Epoch 2140, training loss: 415.97821044921875 = 0.4656371772289276 + 50.0 * 8.310251235961914
Epoch 2140, val loss: 0.4954351782798767
Epoch 2150, training loss: 415.9917907714844 = 0.4640170633792877 + 50.0 * 8.310555458068848
Epoch 2150, val loss: 0.49400439858436584
Epoch 2160, training loss: 415.9603271484375 = 0.4623563289642334 + 50.0 * 8.309959411621094
Epoch 2160, val loss: 0.4926174581050873
Epoch 2170, training loss: 415.9500427246094 = 0.4607571065425873 + 50.0 * 8.309785842895508
Epoch 2170, val loss: 0.4912426173686981
Epoch 2180, training loss: 415.9620361328125 = 0.4591737985610962 + 50.0 * 8.310057640075684
Epoch 2180, val loss: 0.4899345338344574
Epoch 2190, training loss: 416.06768798828125 = 0.45752519369125366 + 50.0 * 8.312203407287598
Epoch 2190, val loss: 0.4885362982749939
Epoch 2200, training loss: 415.9334411621094 = 0.4559204876422882 + 50.0 * 8.309550285339355
Epoch 2200, val loss: 0.4870326519012451
Epoch 2210, training loss: 415.9149169921875 = 0.4543886184692383 + 50.0 * 8.309210777282715
Epoch 2210, val loss: 0.48570093512535095
Epoch 2220, training loss: 415.9079284667969 = 0.45290064811706543 + 50.0 * 8.309100151062012
Epoch 2220, val loss: 0.484480082988739
Epoch 2230, training loss: 415.98382568359375 = 0.4514646530151367 + 50.0 * 8.310647010803223
Epoch 2230, val loss: 0.48318544030189514
Epoch 2240, training loss: 415.9823913574219 = 0.4498661458492279 + 50.0 * 8.310650825500488
Epoch 2240, val loss: 0.48184144496917725
Epoch 2250, training loss: 415.895263671875 = 0.44831302762031555 + 50.0 * 8.308938980102539
Epoch 2250, val loss: 0.4805821478366852
Epoch 2260, training loss: 415.85772705078125 = 0.44693994522094727 + 50.0 * 8.308216094970703
Epoch 2260, val loss: 0.479386568069458
Epoch 2270, training loss: 415.8413391113281 = 0.44554877281188965 + 50.0 * 8.307915687561035
Epoch 2270, val loss: 0.47822126746177673
Epoch 2280, training loss: 415.8323974609375 = 0.4441893696784973 + 50.0 * 8.307764053344727
Epoch 2280, val loss: 0.4770607054233551
Epoch 2290, training loss: 415.8247985839844 = 0.4428340494632721 + 50.0 * 8.307639122009277
Epoch 2290, val loss: 0.47592148184776306
Epoch 2300, training loss: 415.9002990722656 = 0.4414762258529663 + 50.0 * 8.309176445007324
Epoch 2300, val loss: 0.4747731387615204
Epoch 2310, training loss: 415.8453674316406 = 0.44008466601371765 + 50.0 * 8.30810546875
Epoch 2310, val loss: 0.473611444234848
Epoch 2320, training loss: 415.90924072265625 = 0.4387107789516449 + 50.0 * 8.309410095214844
Epoch 2320, val loss: 0.4724031984806061
Epoch 2330, training loss: 415.8114013671875 = 0.43731868267059326 + 50.0 * 8.30748176574707
Epoch 2330, val loss: 0.4713813066482544
Epoch 2340, training loss: 415.78411865234375 = 0.43605509400367737 + 50.0 * 8.306961059570312
Epoch 2340, val loss: 0.4702621400356293
Epoch 2350, training loss: 415.7784729003906 = 0.43483543395996094 + 50.0 * 8.306872367858887
Epoch 2350, val loss: 0.4692446291446686
Epoch 2360, training loss: 415.7875061035156 = 0.43358561396598816 + 50.0 * 8.30707836151123
Epoch 2360, val loss: 0.46818169951438904
Epoch 2370, training loss: 415.8414001464844 = 0.4323034882545471 + 50.0 * 8.308181762695312
Epoch 2370, val loss: 0.46715494990348816
Epoch 2380, training loss: 415.82598876953125 = 0.43102556467056274 + 50.0 * 8.307899475097656
Epoch 2380, val loss: 0.46620413661003113
Epoch 2390, training loss: 415.75738525390625 = 0.4298596978187561 + 50.0 * 8.306550025939941
Epoch 2390, val loss: 0.4651266634464264
Epoch 2400, training loss: 415.7204895019531 = 0.428660124540329 + 50.0 * 8.30583667755127
Epoch 2400, val loss: 0.464176744222641
Epoch 2410, training loss: 415.7140197753906 = 0.42750489711761475 + 50.0 * 8.305730819702148
Epoch 2410, val loss: 0.46326524019241333
Epoch 2420, training loss: 415.7492370605469 = 0.4263572692871094 + 50.0 * 8.30645751953125
Epoch 2420, val loss: 0.46234792470932007
Epoch 2430, training loss: 415.7509460449219 = 0.42516717314720154 + 50.0 * 8.30651569366455
Epoch 2430, val loss: 0.4612770080566406
Epoch 2440, training loss: 415.7290344238281 = 0.4240398705005646 + 50.0 * 8.306099891662598
Epoch 2440, val loss: 0.46041664481163025
Epoch 2450, training loss: 415.74224853515625 = 0.4228995144367218 + 50.0 * 8.306386947631836
Epoch 2450, val loss: 0.4594436287879944
Epoch 2460, training loss: 415.68243408203125 = 0.42176297307014465 + 50.0 * 8.305213928222656
Epoch 2460, val loss: 0.45858854055404663
Epoch 2470, training loss: 415.67669677734375 = 0.4206884205341339 + 50.0 * 8.305120468139648
Epoch 2470, val loss: 0.4576825201511383
Epoch 2480, training loss: 415.68804931640625 = 0.4196172058582306 + 50.0 * 8.305368423461914
Epoch 2480, val loss: 0.45688146352767944
Epoch 2490, training loss: 415.7069091796875 = 0.41854235529899597 + 50.0 * 8.305767059326172
Epoch 2490, val loss: 0.4560457170009613
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8183663115169963
0.8632181409838442
=== training gcn model ===
Epoch 0, training loss: 530.2183837890625 = 1.1069058179855347 + 50.0 * 10.582229614257812
Epoch 0, val loss: 1.1046464443206787
Epoch 10, training loss: 530.1857299804688 = 1.1028647422790527 + 50.0 * 10.581657409667969
Epoch 10, val loss: 1.1006288528442383
Epoch 20, training loss: 530.0628662109375 = 1.0983598232269287 + 50.0 * 10.579289436340332
Epoch 20, val loss: 1.0961387157440186
Epoch 30, training loss: 529.572998046875 = 1.0931562185287476 + 50.0 * 10.569597244262695
Epoch 30, val loss: 1.0909901857376099
Epoch 40, training loss: 527.7412719726562 = 1.0873587131500244 + 50.0 * 10.53307819366455
Epoch 40, val loss: 1.0852786302566528
Epoch 50, training loss: 522.0046997070312 = 1.0809038877487183 + 50.0 * 10.418475151062012
Epoch 50, val loss: 1.078933835029602
Epoch 60, training loss: 506.5921325683594 = 1.0738579034805298 + 50.0 * 10.110365867614746
Epoch 60, val loss: 1.0719962120056152
Epoch 70, training loss: 479.2611389160156 = 1.0655139684677124 + 50.0 * 9.563912391662598
Epoch 70, val loss: 1.0638319253921509
Epoch 80, training loss: 465.9530029296875 = 1.0578721761703491 + 50.0 * 9.297903060913086
Epoch 80, val loss: 1.0566315650939941
Epoch 90, training loss: 462.5695495605469 = 1.0517230033874512 + 50.0 * 9.230356216430664
Epoch 90, val loss: 1.050781488418579
Epoch 100, training loss: 461.0794372558594 = 1.0466042757034302 + 50.0 * 9.20065689086914
Epoch 100, val loss: 1.0459465980529785
Epoch 110, training loss: 459.86749267578125 = 1.042834997177124 + 50.0 * 9.176492691040039
Epoch 110, val loss: 1.0424113273620605
Epoch 120, training loss: 458.7666320800781 = 1.040109395980835 + 50.0 * 9.15453052520752
Epoch 120, val loss: 1.0398260354995728
Epoch 130, training loss: 457.3986511230469 = 1.0380213260650635 + 50.0 * 9.127212524414062
Epoch 130, val loss: 1.0378215312957764
Epoch 140, training loss: 455.5317687988281 = 1.036433219909668 + 50.0 * 9.089906692504883
Epoch 140, val loss: 1.0362749099731445
Epoch 150, training loss: 452.9244079589844 = 1.035300612449646 + 50.0 * 9.037781715393066
Epoch 150, val loss: 1.0351576805114746
Epoch 160, training loss: 449.5755920410156 = 1.0344842672348022 + 50.0 * 8.97082233428955
Epoch 160, val loss: 1.034364938735962
Epoch 170, training loss: 446.38153076171875 = 1.0342155694961548 + 50.0 * 8.906946182250977
Epoch 170, val loss: 1.0341171026229858
Epoch 180, training loss: 444.1338806152344 = 1.034429669380188 + 50.0 * 8.86198902130127
Epoch 180, val loss: 1.0343283414840698
Epoch 190, training loss: 441.6399230957031 = 1.0349469184875488 + 50.0 * 8.81209945678711
Epoch 190, val loss: 1.0348159074783325
Epoch 200, training loss: 439.4399719238281 = 1.0354151725769043 + 50.0 * 8.768091201782227
Epoch 200, val loss: 1.035211443901062
Epoch 210, training loss: 437.6264343261719 = 1.0355262756347656 + 50.0 * 8.731818199157715
Epoch 210, val loss: 1.035295009613037
Epoch 220, training loss: 435.66204833984375 = 1.0354362726211548 + 50.0 * 8.692532539367676
Epoch 220, val loss: 1.0352190732955933
Epoch 230, training loss: 433.9142150878906 = 1.0352357625961304 + 50.0 * 8.65757942199707
Epoch 230, val loss: 1.0350353717803955
Epoch 240, training loss: 432.6674499511719 = 1.0348011255264282 + 50.0 * 8.63265323638916
Epoch 240, val loss: 1.0345721244812012
Epoch 250, training loss: 431.6084289550781 = 1.0342122316360474 + 50.0 * 8.61148452758789
Epoch 250, val loss: 1.034054160118103
Epoch 260, training loss: 430.7035827636719 = 1.0338079929351807 + 50.0 * 8.593395233154297
Epoch 260, val loss: 1.03373384475708
Epoch 270, training loss: 429.9423828125 = 1.0334973335266113 + 50.0 * 8.578177452087402
Epoch 270, val loss: 1.0334692001342773
Epoch 280, training loss: 429.3039245605469 = 1.0330324172973633 + 50.0 * 8.565418243408203
Epoch 280, val loss: 1.0330560207366943
Epoch 290, training loss: 428.6719055175781 = 1.0325064659118652 + 50.0 * 8.552787780761719
Epoch 290, val loss: 1.0325490236282349
Epoch 300, training loss: 428.0829772949219 = 1.0319654941558838 + 50.0 * 8.541020393371582
Epoch 300, val loss: 1.0320225954055786
Epoch 310, training loss: 427.54248046875 = 1.0313405990600586 + 50.0 * 8.53022289276123
Epoch 310, val loss: 1.0314034223556519
Epoch 320, training loss: 427.0673828125 = 1.030634880065918 + 50.0 * 8.520734786987305
Epoch 320, val loss: 1.030697226524353
Epoch 330, training loss: 426.6307067871094 = 1.0298552513122559 + 50.0 * 8.512017250061035
Epoch 330, val loss: 1.029948353767395
Epoch 340, training loss: 426.2274169921875 = 1.0290473699569702 + 50.0 * 8.50396728515625
Epoch 340, val loss: 1.0291608572006226
Epoch 350, training loss: 425.89190673828125 = 1.0281932353973389 + 50.0 * 8.497274398803711
Epoch 350, val loss: 1.0283406972885132
Epoch 360, training loss: 425.50604248046875 = 1.0273139476776123 + 50.0 * 8.489574432373047
Epoch 360, val loss: 1.0274440050125122
Epoch 370, training loss: 425.2013244628906 = 1.0263845920562744 + 50.0 * 8.483498573303223
Epoch 370, val loss: 1.0265135765075684
Epoch 380, training loss: 424.994140625 = 1.0253716707229614 + 50.0 * 8.479375839233398
Epoch 380, val loss: 1.0255306959152222
Epoch 390, training loss: 424.6749267578125 = 1.0242823362350464 + 50.0 * 8.473012924194336
Epoch 390, val loss: 1.0244214534759521
Epoch 400, training loss: 424.44439697265625 = 1.0231696367263794 + 50.0 * 8.468424797058105
Epoch 400, val loss: 1.023322343826294
Epoch 410, training loss: 424.2167663574219 = 1.0220537185668945 + 50.0 * 8.46389389038086
Epoch 410, val loss: 1.022232174873352
Epoch 420, training loss: 424.0687255859375 = 1.0209190845489502 + 50.0 * 8.460956573486328
Epoch 420, val loss: 1.0211297273635864
Epoch 430, training loss: 423.8160400390625 = 1.0197895765304565 + 50.0 * 8.455924987792969
Epoch 430, val loss: 1.0200035572052002
Epoch 440, training loss: 423.5511169433594 = 1.0186975002288818 + 50.0 * 8.450648307800293
Epoch 440, val loss: 1.0189026594161987
Epoch 450, training loss: 423.3367919921875 = 1.0176030397415161 + 50.0 * 8.446383476257324
Epoch 450, val loss: 1.017806887626648
Epoch 460, training loss: 423.1614685058594 = 1.016456127166748 + 50.0 * 8.442900657653809
Epoch 460, val loss: 1.0166696310043335
Epoch 470, training loss: 423.0555419921875 = 1.015217900276184 + 50.0 * 8.44080638885498
Epoch 470, val loss: 1.0154690742492676
Epoch 480, training loss: 422.80401611328125 = 1.0139416456222534 + 50.0 * 8.43580150604248
Epoch 480, val loss: 1.0141732692718506
Epoch 490, training loss: 422.6439514160156 = 1.0126628875732422 + 50.0 * 8.432625770568848
Epoch 490, val loss: 1.012901782989502
Epoch 500, training loss: 422.4906311035156 = 1.0113493204116821 + 50.0 * 8.429585456848145
Epoch 500, val loss: 1.0116119384765625
Epoch 510, training loss: 422.37518310546875 = 1.0099854469299316 + 50.0 * 8.4273042678833
Epoch 510, val loss: 1.0103073120117188
Epoch 520, training loss: 422.35601806640625 = 1.0085400342941284 + 50.0 * 8.426949501037598
Epoch 520, val loss: 1.0087690353393555
Epoch 530, training loss: 422.1134948730469 = 1.007018804550171 + 50.0 * 8.42212963104248
Epoch 530, val loss: 1.0073108673095703
Epoch 540, training loss: 421.9682312011719 = 1.005529522895813 + 50.0 * 8.419254302978516
Epoch 540, val loss: 1.0058714151382446
Epoch 550, training loss: 421.8678283691406 = 1.003999948501587 + 50.0 * 8.417276382446289
Epoch 550, val loss: 1.0043684244155884
Epoch 560, training loss: 421.7156066894531 = 1.0023787021636963 + 50.0 * 8.414264678955078
Epoch 560, val loss: 1.0027724504470825
Epoch 570, training loss: 421.59942626953125 = 1.000738501548767 + 50.0 * 8.41197395324707
Epoch 570, val loss: 1.0011500120162964
Epoch 580, training loss: 421.49542236328125 = 0.9990410208702087 + 50.0 * 8.409927368164062
Epoch 580, val loss: 0.9994686841964722
Epoch 590, training loss: 421.39691162109375 = 0.9972652792930603 + 50.0 * 8.40799331665039
Epoch 590, val loss: 0.9977325797080994
Epoch 600, training loss: 421.4708251953125 = 0.9953780174255371 + 50.0 * 8.40950870513916
Epoch 600, val loss: 0.9958924055099487
Epoch 610, training loss: 421.22161865234375 = 0.9934128522872925 + 50.0 * 8.404563903808594
Epoch 610, val loss: 0.9939199686050415
Epoch 620, training loss: 421.1535339355469 = 0.9914345145225525 + 50.0 * 8.403242111206055
Epoch 620, val loss: 0.9919795393943787
Epoch 630, training loss: 421.0659484863281 = 0.9893908500671387 + 50.0 * 8.401531219482422
Epoch 630, val loss: 0.9899624586105347
Epoch 640, training loss: 421.0666198730469 = 0.9872381687164307 + 50.0 * 8.40158748626709
Epoch 640, val loss: 0.9878942966461182
Epoch 650, training loss: 420.95428466796875 = 0.9850027561187744 + 50.0 * 8.399385452270508
Epoch 650, val loss: 0.985633909702301
Epoch 660, training loss: 420.973876953125 = 0.9827330112457275 + 50.0 * 8.399823188781738
Epoch 660, val loss: 0.9833243489265442
Epoch 670, training loss: 420.8357238769531 = 0.9802740216255188 + 50.0 * 8.397109031677246
Epoch 670, val loss: 0.9810311198234558
Epoch 680, training loss: 420.7368469238281 = 0.9779035449028015 + 50.0 * 8.39517879486084
Epoch 680, val loss: 0.9786677956581116
Epoch 690, training loss: 420.6571044921875 = 0.9754588603973389 + 50.0 * 8.393632888793945
Epoch 690, val loss: 0.9762982130050659
Epoch 700, training loss: 420.58685302734375 = 0.972970724105835 + 50.0 * 8.392277717590332
Epoch 700, val loss: 0.9738464951515198
Epoch 710, training loss: 420.7213439941406 = 0.970358669757843 + 50.0 * 8.39501953125
Epoch 710, val loss: 0.9713481664657593
Epoch 720, training loss: 420.47802734375 = 0.9675573706626892 + 50.0 * 8.390209197998047
Epoch 720, val loss: 0.9685212969779968
Epoch 730, training loss: 420.39105224609375 = 0.9648346304893494 + 50.0 * 8.388524055480957
Epoch 730, val loss: 0.9659096002578735
Epoch 740, training loss: 420.30633544921875 = 0.9621541500091553 + 50.0 * 8.386883735656738
Epoch 740, val loss: 0.9632619023323059
Epoch 750, training loss: 420.22900390625 = 0.9593789577484131 + 50.0 * 8.385392189025879
Epoch 750, val loss: 0.9605703353881836
Epoch 760, training loss: 420.15838623046875 = 0.9565567970275879 + 50.0 * 8.384037017822266
Epoch 760, val loss: 0.9578320980072021
Epoch 770, training loss: 420.33917236328125 = 0.9535685777664185 + 50.0 * 8.387711524963379
Epoch 770, val loss: 0.9550255537033081
Epoch 780, training loss: 420.0306701660156 = 0.9505008459091187 + 50.0 * 8.381603240966797
Epoch 780, val loss: 0.9518928527832031
Epoch 790, training loss: 419.98297119140625 = 0.9474639296531677 + 50.0 * 8.38071060180664
Epoch 790, val loss: 0.9489088654518127
Epoch 800, training loss: 419.9080505371094 = 0.9443626403808594 + 50.0 * 8.379273414611816
Epoch 800, val loss: 0.9458727836608887
Epoch 810, training loss: 419.8397216796875 = 0.9411870837211609 + 50.0 * 8.377970695495605
Epoch 810, val loss: 0.9427931904792786
Epoch 820, training loss: 419.8907165527344 = 0.9379251003265381 + 50.0 * 8.379055976867676
Epoch 820, val loss: 0.9395362734794617
Epoch 830, training loss: 419.7654113769531 = 0.9343961477279663 + 50.0 * 8.376620292663574
Epoch 830, val loss: 0.9362152814865112
Epoch 840, training loss: 419.68060302734375 = 0.930968165397644 + 50.0 * 8.374992370605469
Epoch 840, val loss: 0.9328121542930603
Epoch 850, training loss: 419.9533996582031 = 0.9273985624313354 + 50.0 * 8.38051986694336
Epoch 850, val loss: 0.9293844699859619
Epoch 860, training loss: 419.60601806640625 = 0.9236351847648621 + 50.0 * 8.373647689819336
Epoch 860, val loss: 0.9256582856178284
Epoch 870, training loss: 419.52178955078125 = 0.9199778437614441 + 50.0 * 8.37203598022461
Epoch 870, val loss: 0.9220936298370361
Epoch 880, training loss: 419.4740295410156 = 0.9163041710853577 + 50.0 * 8.37115478515625
Epoch 880, val loss: 0.9184964895248413
Epoch 890, training loss: 419.4230041503906 = 0.9125534892082214 + 50.0 * 8.370208740234375
Epoch 890, val loss: 0.9148489236831665
Epoch 900, training loss: 419.3738708496094 = 0.908708930015564 + 50.0 * 8.369302749633789
Epoch 900, val loss: 0.9111316204071045
Epoch 910, training loss: 419.4451599121094 = 0.9047383069992065 + 50.0 * 8.370808601379395
Epoch 910, val loss: 0.9073632955551147
Epoch 920, training loss: 419.3753662109375 = 0.9005934000015259 + 50.0 * 8.369495391845703
Epoch 920, val loss: 0.9031073451042175
Epoch 930, training loss: 419.2646179199219 = 0.8964276313781738 + 50.0 * 8.367363929748535
Epoch 930, val loss: 0.8991568684577942
Epoch 940, training loss: 419.18658447265625 = 0.8923426866531372 + 50.0 * 8.365884780883789
Epoch 940, val loss: 0.8951746225357056
Epoch 950, training loss: 419.13299560546875 = 0.8882009387016296 + 50.0 * 8.364895820617676
Epoch 950, val loss: 0.8911764621734619
Epoch 960, training loss: 419.0810546875 = 0.8840001821517944 + 50.0 * 8.363941192626953
Epoch 960, val loss: 0.8870861530303955
Epoch 970, training loss: 419.0409240722656 = 0.8797397017478943 + 50.0 * 8.363224029541016
Epoch 970, val loss: 0.8829241394996643
Epoch 980, training loss: 419.10675048828125 = 0.8752595782279968 + 50.0 * 8.364629745483398
Epoch 980, val loss: 0.8785533905029297
Epoch 990, training loss: 418.9951477050781 = 0.8705676198005676 + 50.0 * 8.362491607666016
Epoch 990, val loss: 0.8740543127059937
Epoch 1000, training loss: 418.9049987792969 = 0.8661062121391296 + 50.0 * 8.360777854919434
Epoch 1000, val loss: 0.8697361946105957
Epoch 1010, training loss: 418.8325500488281 = 0.8616538047790527 + 50.0 * 8.359417915344238
Epoch 1010, val loss: 0.8654626607894897
Epoch 1020, training loss: 418.7719421386719 = 0.8571889400482178 + 50.0 * 8.358295440673828
Epoch 1020, val loss: 0.8611131906509399
Epoch 1030, training loss: 418.75067138671875 = 0.8526244759559631 + 50.0 * 8.35796070098877
Epoch 1030, val loss: 0.8567756414413452
Epoch 1040, training loss: 418.68646240234375 = 0.8478964567184448 + 50.0 * 8.356771469116211
Epoch 1040, val loss: 0.8521226644515991
Epoch 1050, training loss: 418.6322326660156 = 0.8432044982910156 + 50.0 * 8.355780601501465
Epoch 1050, val loss: 0.8476358652114868
Epoch 1060, training loss: 418.5814208984375 = 0.8385844230651855 + 50.0 * 8.354856491088867
Epoch 1060, val loss: 0.8431012034416199
Epoch 1070, training loss: 418.51666259765625 = 0.83387291431427 + 50.0 * 8.353655815124512
Epoch 1070, val loss: 0.8386166095733643
Epoch 1080, training loss: 418.4678955078125 = 0.8291682004928589 + 50.0 * 8.352774620056152
Epoch 1080, val loss: 0.834065318107605
Epoch 1090, training loss: 418.656982421875 = 0.8242689371109009 + 50.0 * 8.356654167175293
Epoch 1090, val loss: 0.8295307159423828
Epoch 1100, training loss: 418.39691162109375 = 0.8192604780197144 + 50.0 * 8.351552963256836
Epoch 1100, val loss: 0.8245062232017517
Epoch 1110, training loss: 418.3475341796875 = 0.814408004283905 + 50.0 * 8.350662231445312
Epoch 1110, val loss: 0.8198105692863464
Epoch 1120, training loss: 418.2969970703125 = 0.8095788955688477 + 50.0 * 8.349748611450195
Epoch 1120, val loss: 0.8151752948760986
Epoch 1130, training loss: 418.2464904785156 = 0.8047512769699097 + 50.0 * 8.348834991455078
Epoch 1130, val loss: 0.8105090856552124
Epoch 1140, training loss: 418.2042236328125 = 0.799867570400238 + 50.0 * 8.348087310791016
Epoch 1140, val loss: 0.8057909607887268
Epoch 1150, training loss: 418.5232238769531 = 0.7948678135871887 + 50.0 * 8.354567527770996
Epoch 1150, val loss: 0.8008754253387451
Epoch 1160, training loss: 418.1612854003906 = 0.789405107498169 + 50.0 * 8.347437858581543
Epoch 1160, val loss: 0.7958043217658997
Epoch 1170, training loss: 418.1138610839844 = 0.7843987345695496 + 50.0 * 8.346589088439941
Epoch 1170, val loss: 0.7909393310546875
Epoch 1180, training loss: 418.0604553222656 = 0.7794620990753174 + 50.0 * 8.345620155334473
Epoch 1180, val loss: 0.786161482334137
Epoch 1190, training loss: 418.021728515625 = 0.7745440006256104 + 50.0 * 8.34494400024414
Epoch 1190, val loss: 0.7813922166824341
Epoch 1200, training loss: 418.0882873535156 = 0.7695713043212891 + 50.0 * 8.34637451171875
Epoch 1200, val loss: 0.7764772176742554
Epoch 1210, training loss: 417.97076416015625 = 0.7643613815307617 + 50.0 * 8.344127655029297
Epoch 1210, val loss: 0.7717053890228271
Epoch 1220, training loss: 417.91168212890625 = 0.7593715190887451 + 50.0 * 8.343046188354492
Epoch 1220, val loss: 0.7667898535728455
Epoch 1230, training loss: 417.86761474609375 = 0.7543676495552063 + 50.0 * 8.342265129089355
Epoch 1230, val loss: 0.7620351910591125
Epoch 1240, training loss: 417.957275390625 = 0.7493690848350525 + 50.0 * 8.344158172607422
Epoch 1240, val loss: 0.7571755051612854
Epoch 1250, training loss: 417.8997497558594 = 0.744097888469696 + 50.0 * 8.34311294555664
Epoch 1250, val loss: 0.7521747350692749
Epoch 1260, training loss: 417.77484130859375 = 0.7390286922454834 + 50.0 * 8.340716361999512
Epoch 1260, val loss: 0.7473267912864685
Epoch 1270, training loss: 417.74078369140625 = 0.7340934872627258 + 50.0 * 8.340133666992188
Epoch 1270, val loss: 0.7425723075866699
Epoch 1280, training loss: 417.70550537109375 = 0.729176938533783 + 50.0 * 8.339526176452637
Epoch 1280, val loss: 0.7378345727920532
Epoch 1290, training loss: 417.6749267578125 = 0.7242724895477295 + 50.0 * 8.33901309967041
Epoch 1290, val loss: 0.7331258654594421
Epoch 1300, training loss: 417.8118591308594 = 0.7193940281867981 + 50.0 * 8.341849327087402
Epoch 1300, val loss: 0.7282652854919434
Epoch 1310, training loss: 417.7120666503906 = 0.7141041159629822 + 50.0 * 8.339959144592285
Epoch 1310, val loss: 0.7234684824943542
Epoch 1320, training loss: 417.59466552734375 = 0.7091938853263855 + 50.0 * 8.337709426879883
Epoch 1320, val loss: 0.7187091708183289
Epoch 1330, training loss: 417.5573425292969 = 0.7043682932853699 + 50.0 * 8.337059020996094
Epoch 1330, val loss: 0.714073121547699
Epoch 1340, training loss: 417.5257568359375 = 0.6995620727539062 + 50.0 * 8.33652400970459
Epoch 1340, val loss: 0.7095215320587158
Epoch 1350, training loss: 417.4964599609375 = 0.6948168873786926 + 50.0 * 8.33603286743164
Epoch 1350, val loss: 0.7049499154090881
Epoch 1360, training loss: 417.6724853515625 = 0.6900697350502014 + 50.0 * 8.339648246765137
Epoch 1360, val loss: 0.7002890706062317
Epoch 1370, training loss: 417.5137939453125 = 0.6849387288093567 + 50.0 * 8.336577415466309
Epoch 1370, val loss: 0.6956360340118408
Epoch 1380, training loss: 417.427001953125 = 0.6802238821983337 + 50.0 * 8.334935188293457
Epoch 1380, val loss: 0.691082775592804
Epoch 1390, training loss: 417.3918762207031 = 0.6755901575088501 + 50.0 * 8.334325790405273
Epoch 1390, val loss: 0.686692476272583
Epoch 1400, training loss: 417.3617858886719 = 0.6710214614868164 + 50.0 * 8.333815574645996
Epoch 1400, val loss: 0.682340681552887
Epoch 1410, training loss: 417.3365173339844 = 0.6664337515830994 + 50.0 * 8.333401679992676
Epoch 1410, val loss: 0.6780190467834473
Epoch 1420, training loss: 417.3240966796875 = 0.661861002445221 + 50.0 * 8.333244323730469
Epoch 1420, val loss: 0.6737085580825806
Epoch 1430, training loss: 417.4510498046875 = 0.6571213006973267 + 50.0 * 8.335878372192383
Epoch 1430, val loss: 0.669328510761261
Epoch 1440, training loss: 417.28253173828125 = 0.6524779796600342 + 50.0 * 8.332601547241211
Epoch 1440, val loss: 0.6647622585296631
Epoch 1450, training loss: 417.2639465332031 = 0.6479703187942505 + 50.0 * 8.332319259643555
Epoch 1450, val loss: 0.6605252623558044
Epoch 1460, training loss: 417.2403869628906 = 0.6435537934303284 + 50.0 * 8.331936836242676
Epoch 1460, val loss: 0.6563724279403687
Epoch 1470, training loss: 417.2558898925781 = 0.6391171216964722 + 50.0 * 8.332335472106934
Epoch 1470, val loss: 0.652206540107727
Epoch 1480, training loss: 417.1994323730469 = 0.6346741914749146 + 50.0 * 8.331295013427734
Epoch 1480, val loss: 0.6481191515922546
Epoch 1490, training loss: 417.18927001953125 = 0.6303384304046631 + 50.0 * 8.331178665161133
Epoch 1490, val loss: 0.6440287232398987
Epoch 1500, training loss: 417.16485595703125 = 0.6260512471199036 + 50.0 * 8.33077621459961
Epoch 1500, val loss: 0.6400213837623596
Epoch 1510, training loss: 417.1528015136719 = 0.6217800974845886 + 50.0 * 8.330620765686035
Epoch 1510, val loss: 0.6360182762145996
Epoch 1520, training loss: 417.1020202636719 = 0.6175931096076965 + 50.0 * 8.329689025878906
Epoch 1520, val loss: 0.6320580244064331
Epoch 1530, training loss: 417.1177673339844 = 0.6134200096130371 + 50.0 * 8.330086708068848
Epoch 1530, val loss: 0.6281948685646057
Epoch 1540, training loss: 417.11285400390625 = 0.6092008948326111 + 50.0 * 8.330073356628418
Epoch 1540, val loss: 0.6243162751197815
Epoch 1550, training loss: 417.1063537597656 = 0.6050813794136047 + 50.0 * 8.330025672912598
Epoch 1550, val loss: 0.6203829646110535
Epoch 1560, training loss: 417.04498291015625 = 0.6010062098503113 + 50.0 * 8.328879356384277
Epoch 1560, val loss: 0.6166104674339294
Epoch 1570, training loss: 417.0083923339844 = 0.5970580577850342 + 50.0 * 8.328227043151855
Epoch 1570, val loss: 0.6128886342048645
Epoch 1580, training loss: 416.9898986816406 = 0.5931780338287354 + 50.0 * 8.327934265136719
Epoch 1580, val loss: 0.6092544198036194
Epoch 1590, training loss: 417.0304870605469 = 0.589320719242096 + 50.0 * 8.32882308959961
Epoch 1590, val loss: 0.6056391000747681
Epoch 1600, training loss: 416.9761047363281 = 0.5853829383850098 + 50.0 * 8.327814102172852
Epoch 1600, val loss: 0.6019837260246277
Epoch 1610, training loss: 416.9414978027344 = 0.581527590751648 + 50.0 * 8.327199935913086
Epoch 1610, val loss: 0.598572850227356
Epoch 1620, training loss: 416.91949462890625 = 0.5778336524963379 + 50.0 * 8.326833724975586
Epoch 1620, val loss: 0.5951117277145386
Epoch 1630, training loss: 416.89990234375 = 0.5741913318634033 + 50.0 * 8.32651424407959
Epoch 1630, val loss: 0.5917510986328125
Epoch 1640, training loss: 417.0133056640625 = 0.5705409646034241 + 50.0 * 8.328855514526367
Epoch 1640, val loss: 0.5884382724761963
Epoch 1650, training loss: 416.9062194824219 = 0.5668680667877197 + 50.0 * 8.326786994934082
Epoch 1650, val loss: 0.5849428176879883
Epoch 1660, training loss: 416.8717041015625 = 0.5633049607276917 + 50.0 * 8.326168060302734
Epoch 1660, val loss: 0.5817465782165527
Epoch 1670, training loss: 416.8451843261719 = 0.5598458647727966 + 50.0 * 8.325706481933594
Epoch 1670, val loss: 0.5785560607910156
Epoch 1680, training loss: 416.8310241699219 = 0.556423008441925 + 50.0 * 8.325491905212402
Epoch 1680, val loss: 0.5754281878471375
Epoch 1690, training loss: 416.81866455078125 = 0.553068995475769 + 50.0 * 8.325311660766602
Epoch 1690, val loss: 0.5722709894180298
Epoch 1700, training loss: 416.9214172363281 = 0.5496917366981506 + 50.0 * 8.327434539794922
Epoch 1700, val loss: 0.5691419839859009
Epoch 1710, training loss: 416.7930908203125 = 0.546238362789154 + 50.0 * 8.324936866760254
Epoch 1710, val loss: 0.5662307739257812
Epoch 1720, training loss: 416.7481689453125 = 0.5430667400360107 + 50.0 * 8.324102401733398
Epoch 1720, val loss: 0.5632761120796204
Epoch 1730, training loss: 416.7262878417969 = 0.5399266481399536 + 50.0 * 8.32372760772705
Epoch 1730, val loss: 0.5604339838027954
Epoch 1740, training loss: 416.71539306640625 = 0.5368407368659973 + 50.0 * 8.32357120513916
Epoch 1740, val loss: 0.557635486125946
Epoch 1750, training loss: 416.8880310058594 = 0.5337235331535339 + 50.0 * 8.327086448669434
Epoch 1750, val loss: 0.5548476576805115
Epoch 1760, training loss: 416.7405700683594 = 0.5305756330490112 + 50.0 * 8.324199676513672
Epoch 1760, val loss: 0.5520066022872925
Epoch 1770, training loss: 416.6788635253906 = 0.5275794863700867 + 50.0 * 8.323025703430176
Epoch 1770, val loss: 0.5492984056472778
Epoch 1780, training loss: 416.6594543457031 = 0.5246491432189941 + 50.0 * 8.3226957321167
Epoch 1780, val loss: 0.5466915369033813
Epoch 1790, training loss: 416.6518859863281 = 0.5218121409416199 + 50.0 * 8.322601318359375
Epoch 1790, val loss: 0.544154942035675
Epoch 1800, training loss: 416.7823181152344 = 0.5189026594161987 + 50.0 * 8.325268745422363
Epoch 1800, val loss: 0.5416784882545471
Epoch 1810, training loss: 416.64910888671875 = 0.5160618424415588 + 50.0 * 8.322661399841309
Epoch 1810, val loss: 0.5389036536216736
Epoch 1820, training loss: 416.61163330078125 = 0.5132891535758972 + 50.0 * 8.321967124938965
Epoch 1820, val loss: 0.5365448594093323
Epoch 1830, training loss: 416.5838623046875 = 0.5106096863746643 + 50.0 * 8.321464538574219
Epoch 1830, val loss: 0.5341256260871887
Epoch 1840, training loss: 416.56396484375 = 0.5079754590988159 + 50.0 * 8.321120262145996
Epoch 1840, val loss: 0.531821072101593
Epoch 1850, training loss: 416.5668640136719 = 0.5053659677505493 + 50.0 * 8.321229934692383
Epoch 1850, val loss: 0.5295422077178955
Epoch 1860, training loss: 416.6297912597656 = 0.5027211308479309 + 50.0 * 8.322541236877441
Epoch 1860, val loss: 0.5272358655929565
Epoch 1870, training loss: 416.54473876953125 = 0.500127911567688 + 50.0 * 8.320892333984375
Epoch 1870, val loss: 0.5248986482620239
Epoch 1880, training loss: 416.5205383300781 = 0.4976080060005188 + 50.0 * 8.32045841217041
Epoch 1880, val loss: 0.5226920247077942
Epoch 1890, training loss: 416.671142578125 = 0.4950668513774872 + 50.0 * 8.323521614074707
Epoch 1890, val loss: 0.5205427408218384
Epoch 1900, training loss: 416.5283203125 = 0.4926159977912903 + 50.0 * 8.320713996887207
Epoch 1900, val loss: 0.5182532072067261
Epoch 1910, training loss: 416.4720153808594 = 0.49022355675697327 + 50.0 * 8.319635391235352
Epoch 1910, val loss: 0.5162182450294495
Epoch 1920, training loss: 416.452880859375 = 0.48793351650238037 + 50.0 * 8.31929874420166
Epoch 1920, val loss: 0.5142166614532471
Epoch 1930, training loss: 416.4555358886719 = 0.48567286133766174 + 50.0 * 8.31939697265625
Epoch 1930, val loss: 0.5121966004371643
Epoch 1940, training loss: 416.5752868652344 = 0.4833757281303406 + 50.0 * 8.32183837890625
Epoch 1940, val loss: 0.5101507902145386
Epoch 1950, training loss: 416.44964599609375 = 0.4810716509819031 + 50.0 * 8.319371223449707
Epoch 1950, val loss: 0.5083441138267517
Epoch 1960, training loss: 416.4000549316406 = 0.47889387607574463 + 50.0 * 8.3184232711792
Epoch 1960, val loss: 0.5063949227333069
Epoch 1970, training loss: 416.3784484863281 = 0.4767736494541168 + 50.0 * 8.318033218383789
Epoch 1970, val loss: 0.5046412348747253
Epoch 1980, training loss: 416.4023742675781 = 0.4746842682361603 + 50.0 * 8.318553924560547
Epoch 1980, val loss: 0.50288325548172
Epoch 1990, training loss: 416.4349670410156 = 0.472489595413208 + 50.0 * 8.319249153137207
Epoch 1990, val loss: 0.5010166764259338
Epoch 2000, training loss: 416.33990478515625 = 0.4704340994358063 + 50.0 * 8.317389488220215
Epoch 2000, val loss: 0.4991835951805115
Epoch 2010, training loss: 416.3238220214844 = 0.4684203565120697 + 50.0 * 8.317108154296875
Epoch 2010, val loss: 0.4975036680698395
Epoch 2020, training loss: 416.30877685546875 = 0.4664822816848755 + 50.0 * 8.316845893859863
Epoch 2020, val loss: 0.495896577835083
Epoch 2030, training loss: 416.28955078125 = 0.4645861089229584 + 50.0 * 8.316498756408691
Epoch 2030, val loss: 0.49426814913749695
Epoch 2040, training loss: 416.2741394042969 = 0.46269819140434265 + 50.0 * 8.316228866577148
Epoch 2040, val loss: 0.492708295583725
Epoch 2050, training loss: 416.304443359375 = 0.4608237147331238 + 50.0 * 8.316872596740723
Epoch 2050, val loss: 0.49115458130836487
Epoch 2060, training loss: 416.3126525878906 = 0.4588192105293274 + 50.0 * 8.317076683044434
Epoch 2060, val loss: 0.489483118057251
Epoch 2070, training loss: 416.282470703125 = 0.4569157660007477 + 50.0 * 8.316511154174805
Epoch 2070, val loss: 0.4879002273082733
Epoch 2080, training loss: 416.22998046875 = 0.455089271068573 + 50.0 * 8.315498352050781
Epoch 2080, val loss: 0.486329585313797
Epoch 2090, training loss: 416.2224426269531 = 0.4533407986164093 + 50.0 * 8.31538200378418
Epoch 2090, val loss: 0.48488637804985046
Epoch 2100, training loss: 416.29974365234375 = 0.451616108417511 + 50.0 * 8.316962242126465
Epoch 2100, val loss: 0.483397513628006
Epoch 2110, training loss: 416.1892395019531 = 0.4498136341571808 + 50.0 * 8.314788818359375
Epoch 2110, val loss: 0.4820074439048767
Epoch 2120, training loss: 416.17498779296875 = 0.44809454679489136 + 50.0 * 8.31453800201416
Epoch 2120, val loss: 0.48064738512039185
Epoch 2130, training loss: 416.1545104980469 = 0.4464512765407562 + 50.0 * 8.31416130065918
Epoch 2130, val loss: 0.4792843163013458
Epoch 2140, training loss: 416.1446533203125 = 0.4448278248310089 + 50.0 * 8.313996315002441
Epoch 2140, val loss: 0.477967232465744
Epoch 2150, training loss: 416.1391906738281 = 0.4432271718978882 + 50.0 * 8.313919067382812
Epoch 2150, val loss: 0.4766245186328888
Epoch 2160, training loss: 416.3194885253906 = 0.4416245222091675 + 50.0 * 8.317557334899902
Epoch 2160, val loss: 0.47515302896499634
Epoch 2170, training loss: 416.2218017578125 = 0.43987536430358887 + 50.0 * 8.315638542175293
Epoch 2170, val loss: 0.47405216097831726
Epoch 2180, training loss: 416.1230163574219 = 0.4382343590259552 + 50.0 * 8.313695907592773
Epoch 2180, val loss: 0.47262752056121826
Epoch 2190, training loss: 416.10882568359375 = 0.43668970465660095 + 50.0 * 8.313443183898926
Epoch 2190, val loss: 0.47137516736984253
Epoch 2200, training loss: 416.07586669921875 = 0.4351945221424103 + 50.0 * 8.312813758850098
Epoch 2200, val loss: 0.47021615505218506
Epoch 2210, training loss: 416.06170654296875 = 0.4337354004383087 + 50.0 * 8.312559127807617
Epoch 2210, val loss: 0.4691006541252136
Epoch 2220, training loss: 416.04638671875 = 0.43230152130126953 + 50.0 * 8.312281608581543
Epoch 2220, val loss: 0.4679243862628937
Epoch 2230, training loss: 416.0379333496094 = 0.43087002635002136 + 50.0 * 8.312141418457031
Epoch 2230, val loss: 0.4667994976043701
Epoch 2240, training loss: 416.0975036621094 = 0.42944255471229553 + 50.0 * 8.313361167907715
Epoch 2240, val loss: 0.4656754434108734
Epoch 2250, training loss: 416.09454345703125 = 0.42792513966560364 + 50.0 * 8.313332557678223
Epoch 2250, val loss: 0.46462583541870117
Epoch 2260, training loss: 416.0365905761719 = 0.4264809191226959 + 50.0 * 8.312202453613281
Epoch 2260, val loss: 0.46333715319633484
Epoch 2270, training loss: 416.0216064453125 = 0.42507967352867126 + 50.0 * 8.311930656433105
Epoch 2270, val loss: 0.4623115360736847
Epoch 2280, training loss: 416.0094909667969 = 0.4237227141857147 + 50.0 * 8.311715126037598
Epoch 2280, val loss: 0.4612707793712616
Epoch 2290, training loss: 416.0177001953125 = 0.42238134145736694 + 50.0 * 8.311905860900879
Epoch 2290, val loss: 0.4602205753326416
Epoch 2300, training loss: 415.9764709472656 = 0.4210434854030609 + 50.0 * 8.311108589172363
Epoch 2300, val loss: 0.459191232919693
Epoch 2310, training loss: 415.955078125 = 0.4197479784488678 + 50.0 * 8.310707092285156
Epoch 2310, val loss: 0.4581597149372101
Epoch 2320, training loss: 415.94976806640625 = 0.41846534609794617 + 50.0 * 8.310626029968262
Epoch 2320, val loss: 0.45718711614608765
Epoch 2330, training loss: 415.9861755371094 = 0.4171912670135498 + 50.0 * 8.311379432678223
Epoch 2330, val loss: 0.45618411898612976
Epoch 2340, training loss: 416.0459289550781 = 0.4158751964569092 + 50.0 * 8.312601089477539
Epoch 2340, val loss: 0.45510369539260864
Epoch 2350, training loss: 415.94183349609375 = 0.41453656554222107 + 50.0 * 8.310545921325684
Epoch 2350, val loss: 0.4541887938976288
Epoch 2360, training loss: 415.97479248046875 = 0.4132853150367737 + 50.0 * 8.311229705810547
Epoch 2360, val loss: 0.4531157612800598
Epoch 2370, training loss: 415.90228271484375 = 0.41204285621643066 + 50.0 * 8.309804916381836
Epoch 2370, val loss: 0.45232757925987244
Epoch 2380, training loss: 415.8963928222656 = 0.41085758805274963 + 50.0 * 8.309710502624512
Epoch 2380, val loss: 0.4514425992965698
Epoch 2390, training loss: 415.87530517578125 = 0.4096856713294983 + 50.0 * 8.309311866760254
Epoch 2390, val loss: 0.45056018233299255
Epoch 2400, training loss: 415.8631591796875 = 0.4085388779640198 + 50.0 * 8.30909252166748
Epoch 2400, val loss: 0.44968825578689575
Epoch 2410, training loss: 415.8791198730469 = 0.40739500522613525 + 50.0 * 8.309433937072754
Epoch 2410, val loss: 0.44889968633651733
Epoch 2420, training loss: 415.9801940917969 = 0.4062052369117737 + 50.0 * 8.311479568481445
Epoch 2420, val loss: 0.4480518400669098
Epoch 2430, training loss: 415.898193359375 = 0.4050082862377167 + 50.0 * 8.309864044189453
Epoch 2430, val loss: 0.44705915451049805
Epoch 2440, training loss: 415.8815612792969 = 0.40387827157974243 + 50.0 * 8.309554100036621
Epoch 2440, val loss: 0.44626113772392273
Epoch 2450, training loss: 415.822998046875 = 0.4027470350265503 + 50.0 * 8.308404922485352
Epoch 2450, val loss: 0.4454377591609955
Epoch 2460, training loss: 415.8055725097656 = 0.4016849398612976 + 50.0 * 8.308077812194824
Epoch 2460, val loss: 0.4446013569831848
Epoch 2470, training loss: 415.8127746582031 = 0.40062928199768066 + 50.0 * 8.308242797851562
Epoch 2470, val loss: 0.44381946325302124
Epoch 2480, training loss: 415.9119873046875 = 0.39955654740333557 + 50.0 * 8.310248374938965
Epoch 2480, val loss: 0.4429764747619629
Epoch 2490, training loss: 415.781005859375 = 0.39843666553497314 + 50.0 * 8.30765151977539
Epoch 2490, val loss: 0.44229021668434143
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8340943683409436
0.8623487647612839
=== training gcn model ===
Epoch 0, training loss: 530.227294921875 = 1.1141492128372192 + 50.0 * 10.582262992858887
Epoch 0, val loss: 1.1125544309616089
Epoch 10, training loss: 530.1984252929688 = 1.1092816591262817 + 50.0 * 10.581783294677734
Epoch 10, val loss: 1.1077258586883545
Epoch 20, training loss: 530.0924682617188 = 1.1041311025619507 + 50.0 * 10.579766273498535
Epoch 20, val loss: 1.102630376815796
Epoch 30, training loss: 529.66748046875 = 1.0985875129699707 + 50.0 * 10.571378707885742
Epoch 30, val loss: 1.0971554517745972
Epoch 40, training loss: 528.1412353515625 = 1.092522144317627 + 50.0 * 10.540973663330078
Epoch 40, val loss: 1.091149091720581
Epoch 50, training loss: 523.7420043945312 = 1.085425853729248 + 50.0 * 10.453130722045898
Epoch 50, val loss: 1.0840823650360107
Epoch 60, training loss: 513.2147827148438 = 1.076992392539978 + 50.0 * 10.242755889892578
Epoch 60, val loss: 1.0756900310516357
Epoch 70, training loss: 495.19134521484375 = 1.067087173461914 + 50.0 * 9.882485389709473
Epoch 70, val loss: 1.065861463546753
Epoch 80, training loss: 481.96954345703125 = 1.056323766708374 + 50.0 * 9.618264198303223
Epoch 80, val loss: 1.0554319620132446
Epoch 90, training loss: 477.2244567871094 = 1.0477160215377808 + 50.0 * 9.523534774780273
Epoch 90, val loss: 1.0473275184631348
Epoch 100, training loss: 472.6572265625 = 1.0415666103363037 + 50.0 * 9.432312965393066
Epoch 100, val loss: 1.0414814949035645
Epoch 110, training loss: 468.2071533203125 = 1.0370330810546875 + 50.0 * 9.343401908874512
Epoch 110, val loss: 1.0372132062911987
Epoch 120, training loss: 464.35791015625 = 1.0335673093795776 + 50.0 * 9.266487121582031
Epoch 120, val loss: 1.0339471101760864
Epoch 130, training loss: 461.7780456542969 = 1.0310248136520386 + 50.0 * 9.214940071105957
Epoch 130, val loss: 1.031597375869751
Epoch 140, training loss: 460.4044494628906 = 1.0292854309082031 + 50.0 * 9.18750286102295
Epoch 140, val loss: 1.0299179553985596
Epoch 150, training loss: 459.50067138671875 = 1.0275664329528809 + 50.0 * 9.169462203979492
Epoch 150, val loss: 1.0281784534454346
Epoch 160, training loss: 458.3910217285156 = 1.0253479480743408 + 50.0 * 9.147313117980957
Epoch 160, val loss: 1.0260273218154907
Epoch 170, training loss: 456.86688232421875 = 1.0232512950897217 + 50.0 * 9.116872787475586
Epoch 170, val loss: 1.0241385698318481
Epoch 180, training loss: 454.6814270019531 = 1.0220001935958862 + 50.0 * 9.073188781738281
Epoch 180, val loss: 1.0231307744979858
Epoch 190, training loss: 451.606689453125 = 1.0220078229904175 + 50.0 * 9.011693954467773
Epoch 190, val loss: 1.0233231782913208
Epoch 200, training loss: 448.5708312988281 = 1.022854208946228 + 50.0 * 8.950959205627441
Epoch 200, val loss: 1.0241676568984985
Epoch 210, training loss: 446.5574951171875 = 1.0231273174285889 + 50.0 * 8.910687446594238
Epoch 210, val loss: 1.0243370532989502
Epoch 220, training loss: 444.3745422363281 = 1.02263605594635 + 50.0 * 8.867037773132324
Epoch 220, val loss: 1.0238803625106812
Epoch 230, training loss: 442.1863708496094 = 1.0222060680389404 + 50.0 * 8.823283195495605
Epoch 230, val loss: 1.023436427116394
Epoch 240, training loss: 441.1545104980469 = 1.0210011005401611 + 50.0 * 8.8026704788208
Epoch 240, val loss: 1.0220967531204224
Epoch 250, training loss: 440.2915344238281 = 1.0194106101989746 + 50.0 * 8.785442352294922
Epoch 250, val loss: 1.0206760168075562
Epoch 260, training loss: 439.14056396484375 = 1.0192606449127197 + 50.0 * 8.762426376342773
Epoch 260, val loss: 1.0206738710403442
Epoch 270, training loss: 437.6498107910156 = 1.019755244255066 + 50.0 * 8.732601165771484
Epoch 270, val loss: 1.0212743282318115
Epoch 280, training loss: 436.0697326660156 = 1.0201205015182495 + 50.0 * 8.700992584228516
Epoch 280, val loss: 1.021707534790039
Epoch 290, training loss: 435.0754699707031 = 1.0198988914489746 + 50.0 * 8.681111335754395
Epoch 290, val loss: 1.0214229822158813
Epoch 300, training loss: 434.0841064453125 = 1.0190086364746094 + 50.0 * 8.661301612854004
Epoch 300, val loss: 1.0205744504928589
Epoch 310, training loss: 433.0142822265625 = 1.0184977054595947 + 50.0 * 8.639915466308594
Epoch 310, val loss: 1.0201332569122314
Epoch 320, training loss: 432.0722961425781 = 1.0182939767837524 + 50.0 * 8.62108039855957
Epoch 320, val loss: 1.0199795961380005
Epoch 330, training loss: 431.2604064941406 = 1.0178123712539673 + 50.0 * 8.604851722717285
Epoch 330, val loss: 1.0195685625076294
Epoch 340, training loss: 430.470703125 = 1.0173527002334595 + 50.0 * 8.589066505432129
Epoch 340, val loss: 1.0192073583602905
Epoch 350, training loss: 429.7739562988281 = 1.0169717073440552 + 50.0 * 8.575139999389648
Epoch 350, val loss: 1.018829584121704
Epoch 360, training loss: 429.0765686035156 = 1.0164825916290283 + 50.0 * 8.561202049255371
Epoch 360, val loss: 1.0183123350143433
Epoch 370, training loss: 428.60589599609375 = 1.0159770250320435 + 50.0 * 8.551797866821289
Epoch 370, val loss: 1.0177332162857056
Epoch 380, training loss: 427.9810485839844 = 1.0152146816253662 + 50.0 * 8.539316177368164
Epoch 380, val loss: 1.0170270204544067
Epoch 390, training loss: 427.48492431640625 = 1.0144182443618774 + 50.0 * 8.529410362243652
Epoch 390, val loss: 1.0162619352340698
Epoch 400, training loss: 427.0723571777344 = 1.0134575366973877 + 50.0 * 8.521178245544434
Epoch 400, val loss: 1.0153414011001587
Epoch 410, training loss: 426.7335510253906 = 1.0123814344406128 + 50.0 * 8.514423370361328
Epoch 410, val loss: 1.0143239498138428
Epoch 420, training loss: 426.3310241699219 = 1.011246919631958 + 50.0 * 8.50639533996582
Epoch 420, val loss: 1.0132087469100952
Epoch 430, training loss: 425.98065185546875 = 1.0101561546325684 + 50.0 * 8.499409675598145
Epoch 430, val loss: 1.012150526046753
Epoch 440, training loss: 425.69598388671875 = 1.0089775323867798 + 50.0 * 8.49374008178711
Epoch 440, val loss: 1.0109773874282837
Epoch 450, training loss: 425.44793701171875 = 1.0076062679290771 + 50.0 * 8.48880672454834
Epoch 450, val loss: 1.0096750259399414
Epoch 460, training loss: 425.19635009765625 = 1.0062148571014404 + 50.0 * 8.483802795410156
Epoch 460, val loss: 1.008325457572937
Epoch 470, training loss: 424.9766845703125 = 1.004787564277649 + 50.0 * 8.479437828063965
Epoch 470, val loss: 1.0069611072540283
Epoch 480, training loss: 424.7801208496094 = 1.003341794013977 + 50.0 * 8.47553539276123
Epoch 480, val loss: 1.0055766105651855
Epoch 490, training loss: 424.6218566894531 = 1.0018527507781982 + 50.0 * 8.472399711608887
Epoch 490, val loss: 1.0041040182113647
Epoch 500, training loss: 424.4954833984375 = 1.0001405477523804 + 50.0 * 8.4699068069458
Epoch 500, val loss: 1.0025171041488647
Epoch 510, training loss: 424.22930908203125 = 0.9985751509666443 + 50.0 * 8.464614868164062
Epoch 510, val loss: 1.0009913444519043
Epoch 520, training loss: 424.0090026855469 = 0.9970532059669495 + 50.0 * 8.46023941040039
Epoch 520, val loss: 0.9995604157447815
Epoch 530, training loss: 423.8109130859375 = 0.9954861998558044 + 50.0 * 8.456308364868164
Epoch 530, val loss: 0.9981012940406799
Epoch 540, training loss: 423.72320556640625 = 0.9937008619308472 + 50.0 * 8.45458984375
Epoch 540, val loss: 0.9964030981063843
Epoch 550, training loss: 423.41180419921875 = 0.9919937252998352 + 50.0 * 8.448395729064941
Epoch 550, val loss: 0.9947562217712402
Epoch 560, training loss: 423.190185546875 = 0.990363359451294 + 50.0 * 8.44399642944336
Epoch 560, val loss: 0.9932011961936951
Epoch 570, training loss: 423.0006103515625 = 0.9886519312858582 + 50.0 * 8.440238952636719
Epoch 570, val loss: 0.9915784001350403
Epoch 580, training loss: 422.98651123046875 = 0.9868207573890686 + 50.0 * 8.439993858337402
Epoch 580, val loss: 0.9897459149360657
Epoch 590, training loss: 422.68414306640625 = 0.9847975373268127 + 50.0 * 8.43398666381836
Epoch 590, val loss: 0.9878466725349426
Epoch 600, training loss: 422.518798828125 = 0.9828153848648071 + 50.0 * 8.430719375610352
Epoch 600, val loss: 0.9860121607780457
Epoch 610, training loss: 422.3644714355469 = 0.9808037281036377 + 50.0 * 8.42767333984375
Epoch 610, val loss: 0.984123170375824
Epoch 620, training loss: 422.2320861816406 = 0.9788152575492859 + 50.0 * 8.425065040588379
Epoch 620, val loss: 0.9822019338607788
Epoch 630, training loss: 422.121337890625 = 0.9766441583633423 + 50.0 * 8.422893524169922
Epoch 630, val loss: 0.9801381230354309
Epoch 640, training loss: 421.9399719238281 = 0.97453773021698 + 50.0 * 8.41930866241455
Epoch 640, val loss: 0.9781370759010315
Epoch 650, training loss: 421.7812805175781 = 0.9724522233009338 + 50.0 * 8.416176795959473
Epoch 650, val loss: 0.976157009601593
Epoch 660, training loss: 421.6334533691406 = 0.9703220725059509 + 50.0 * 8.413262367248535
Epoch 660, val loss: 0.9741370677947998
Epoch 670, training loss: 421.6537780761719 = 0.968022882938385 + 50.0 * 8.413715362548828
Epoch 670, val loss: 0.9719459414482117
Epoch 680, training loss: 421.38629150390625 = 0.965694010257721 + 50.0 * 8.408411979675293
Epoch 680, val loss: 0.9696907997131348
Epoch 690, training loss: 421.2365417480469 = 0.9633654356002808 + 50.0 * 8.405463218688965
Epoch 690, val loss: 0.9674967527389526
Epoch 700, training loss: 421.1246032714844 = 0.9609764814376831 + 50.0 * 8.40327262878418
Epoch 700, val loss: 0.9652673602104187
Epoch 710, training loss: 421.0225830078125 = 0.9585301280021667 + 50.0 * 8.401281356811523
Epoch 710, val loss: 0.9629419445991516
Epoch 720, training loss: 421.0841064453125 = 0.9560065269470215 + 50.0 * 8.402562141418457
Epoch 720, val loss: 0.9604603052139282
Epoch 730, training loss: 420.87274169921875 = 0.9531702995300293 + 50.0 * 8.398391723632812
Epoch 730, val loss: 0.9579184055328369
Epoch 740, training loss: 420.8014221191406 = 0.9505603313446045 + 50.0 * 8.397017478942871
Epoch 740, val loss: 0.9554210305213928
Epoch 750, training loss: 420.7101135253906 = 0.9478808641433716 + 50.0 * 8.395244598388672
Epoch 750, val loss: 0.9529389142990112
Epoch 760, training loss: 420.65496826171875 = 0.9451238512992859 + 50.0 * 8.394196510314941
Epoch 760, val loss: 0.9503616690635681
Epoch 770, training loss: 420.6156005859375 = 0.9422025084495544 + 50.0 * 8.393467903137207
Epoch 770, val loss: 0.947441577911377
Epoch 780, training loss: 420.5377197265625 = 0.9391364455223083 + 50.0 * 8.391971588134766
Epoch 780, val loss: 0.9447558522224426
Epoch 790, training loss: 420.4378662109375 = 0.9362319707870483 + 50.0 * 8.390032768249512
Epoch 790, val loss: 0.9419731497764587
Epoch 800, training loss: 420.3630676269531 = 0.9333036541938782 + 50.0 * 8.388595581054688
Epoch 800, val loss: 0.9391937255859375
Epoch 810, training loss: 420.30596923828125 = 0.9302961230278015 + 50.0 * 8.387513160705566
Epoch 810, val loss: 0.9363665580749512
Epoch 820, training loss: 420.45001220703125 = 0.9270936250686646 + 50.0 * 8.390458106994629
Epoch 820, val loss: 0.9334098100662231
Epoch 830, training loss: 420.1864013671875 = 0.9238829612731934 + 50.0 * 8.385250091552734
Epoch 830, val loss: 0.9303145408630371
Epoch 840, training loss: 420.12823486328125 = 0.9208132028579712 + 50.0 * 8.384148597717285
Epoch 840, val loss: 0.927307665348053
Epoch 850, training loss: 420.0474548339844 = 0.9175764918327332 + 50.0 * 8.382597923278809
Epoch 850, val loss: 0.9243866801261902
Epoch 860, training loss: 419.98468017578125 = 0.9143489599227905 + 50.0 * 8.381406784057617
Epoch 860, val loss: 0.9213329553604126
Epoch 870, training loss: 420.1087951660156 = 0.9108567237854004 + 50.0 * 8.38395881652832
Epoch 870, val loss: 0.9181379675865173
Epoch 880, training loss: 419.9060974121094 = 0.9073910117149353 + 50.0 * 8.379974365234375
Epoch 880, val loss: 0.9147642254829407
Epoch 890, training loss: 419.8370361328125 = 0.9041044116020203 + 50.0 * 8.378658294677734
Epoch 890, val loss: 0.9115345478057861
Epoch 900, training loss: 419.7518005371094 = 0.9006744027137756 + 50.0 * 8.377022743225098
Epoch 900, val loss: 0.9083266258239746
Epoch 910, training loss: 419.7134094238281 = 0.8972116112709045 + 50.0 * 8.376323699951172
Epoch 910, val loss: 0.9051048755645752
Epoch 920, training loss: 419.65447998046875 = 0.8936281800270081 + 50.0 * 8.37521743774414
Epoch 920, val loss: 0.9016918540000916
Epoch 930, training loss: 419.6018371582031 = 0.8900505900382996 + 50.0 * 8.374236106872559
Epoch 930, val loss: 0.8982309103012085
Epoch 940, training loss: 419.5682373046875 = 0.8863745331764221 + 50.0 * 8.373637199401855
Epoch 940, val loss: 0.894903838634491
Epoch 950, training loss: 419.59112548828125 = 0.8826524019241333 + 50.0 * 8.37416934967041
Epoch 950, val loss: 0.8912778496742249
Epoch 960, training loss: 419.45947265625 = 0.8788933753967285 + 50.0 * 8.371611595153809
Epoch 960, val loss: 0.8877514600753784
Epoch 970, training loss: 419.3996276855469 = 0.8752207159996033 + 50.0 * 8.370488166809082
Epoch 970, val loss: 0.8841811418533325
Epoch 980, training loss: 419.3407287597656 = 0.8713958263397217 + 50.0 * 8.369386672973633
Epoch 980, val loss: 0.8806872367858887
Epoch 990, training loss: 419.2891845703125 = 0.8676892518997192 + 50.0 * 8.368430137634277
Epoch 990, val loss: 0.8771196007728577
Epoch 1000, training loss: 419.2562255859375 = 0.8638609051704407 + 50.0 * 8.367847442626953
Epoch 1000, val loss: 0.8735092878341675
Epoch 1010, training loss: 419.37921142578125 = 0.8598722219467163 + 50.0 * 8.370387077331543
Epoch 1010, val loss: 0.8695632219314575
Epoch 1020, training loss: 419.2684631347656 = 0.8556298613548279 + 50.0 * 8.368256568908691
Epoch 1020, val loss: 0.8657063841819763
Epoch 1030, training loss: 419.1426086425781 = 0.8517577052116394 + 50.0 * 8.365817070007324
Epoch 1030, val loss: 0.8619887828826904
Epoch 1040, training loss: 419.0765075683594 = 0.8478296995162964 + 50.0 * 8.36457347869873
Epoch 1040, val loss: 0.8583024144172668
Epoch 1050, training loss: 419.0299072265625 = 0.8439511060714722 + 50.0 * 8.36371898651123
Epoch 1050, val loss: 0.8546221852302551
Epoch 1060, training loss: 418.9860534667969 = 0.8400300145149231 + 50.0 * 8.362920761108398
Epoch 1060, val loss: 0.8509172201156616
Epoch 1070, training loss: 418.9450988769531 = 0.8360546231269836 + 50.0 * 8.362180709838867
Epoch 1070, val loss: 0.8471304178237915
Epoch 1080, training loss: 419.0374450683594 = 0.8319870829582214 + 50.0 * 8.36410903930664
Epoch 1080, val loss: 0.8433057069778442
Epoch 1090, training loss: 418.95330810546875 = 0.8275870680809021 + 50.0 * 8.36251449584961
Epoch 1090, val loss: 0.8389803767204285
Epoch 1100, training loss: 418.8436279296875 = 0.8234930634498596 + 50.0 * 8.360403060913086
Epoch 1100, val loss: 0.8351682424545288
Epoch 1110, training loss: 418.7887878417969 = 0.8194172978401184 + 50.0 * 8.359387397766113
Epoch 1110, val loss: 0.8313130736351013
Epoch 1120, training loss: 418.7395324707031 = 0.8152838349342346 + 50.0 * 8.358485221862793
Epoch 1120, val loss: 0.827389657497406
Epoch 1130, training loss: 418.8252258300781 = 0.8110331296920776 + 50.0 * 8.360283851623535
Epoch 1130, val loss: 0.8235049843788147
Epoch 1140, training loss: 418.72314453125 = 0.8068854808807373 + 50.0 * 8.358325004577637
Epoch 1140, val loss: 0.8192307949066162
Epoch 1150, training loss: 418.6119689941406 = 0.8026689291000366 + 50.0 * 8.356185913085938
Epoch 1150, val loss: 0.8153526782989502
Epoch 1160, training loss: 418.5770568847656 = 0.7985169887542725 + 50.0 * 8.355570793151855
Epoch 1160, val loss: 0.8114705085754395
Epoch 1170, training loss: 418.5476989746094 = 0.7944263219833374 + 50.0 * 8.35506534576416
Epoch 1170, val loss: 0.8075351715087891
Epoch 1180, training loss: 418.68768310546875 = 0.7899731993675232 + 50.0 * 8.357954025268555
Epoch 1180, val loss: 0.8032541275024414
Epoch 1190, training loss: 418.51141357421875 = 0.7856369614601135 + 50.0 * 8.354515075683594
Epoch 1190, val loss: 0.7991832494735718
Epoch 1200, training loss: 418.421630859375 = 0.781555712223053 + 50.0 * 8.352801322937012
Epoch 1200, val loss: 0.7952706813812256
Epoch 1210, training loss: 418.37762451171875 = 0.7774799466133118 + 50.0 * 8.35200309753418
Epoch 1210, val loss: 0.7913785576820374
Epoch 1220, training loss: 418.337890625 = 0.7733583450317383 + 50.0 * 8.351290702819824
Epoch 1220, val loss: 0.7875057458877563
Epoch 1230, training loss: 418.2972412109375 = 0.769216775894165 + 50.0 * 8.350560188293457
Epoch 1230, val loss: 0.7835910320281982
Epoch 1240, training loss: 418.2671203613281 = 0.765108048915863 + 50.0 * 8.350040435791016
Epoch 1240, val loss: 0.7797194719314575
Epoch 1250, training loss: 418.38592529296875 = 0.760836660861969 + 50.0 * 8.35250186920166
Epoch 1250, val loss: 0.7757290005683899
Epoch 1260, training loss: 418.2177734375 = 0.7566332221031189 + 50.0 * 8.349223136901855
Epoch 1260, val loss: 0.771664023399353
Epoch 1270, training loss: 418.2295837402344 = 0.7524529099464417 + 50.0 * 8.349542617797852
Epoch 1270, val loss: 0.7677239179611206
Epoch 1280, training loss: 418.1155090332031 = 0.748280942440033 + 50.0 * 8.347344398498535
Epoch 1280, val loss: 0.7637530565261841
Epoch 1290, training loss: 418.098388671875 = 0.7442423105239868 + 50.0 * 8.34708309173584
Epoch 1290, val loss: 0.7598720788955688
Epoch 1300, training loss: 418.073974609375 = 0.7400712370872498 + 50.0 * 8.346677780151367
Epoch 1300, val loss: 0.755914032459259
Epoch 1310, training loss: 418.0726623535156 = 0.735863447189331 + 50.0 * 8.346735954284668
Epoch 1310, val loss: 0.7519969940185547
Epoch 1320, training loss: 418.0242614746094 = 0.7316523194313049 + 50.0 * 8.34585189819336
Epoch 1320, val loss: 0.7481260895729065
Epoch 1330, training loss: 417.939208984375 = 0.7275279760360718 + 50.0 * 8.344233512878418
Epoch 1330, val loss: 0.7441104650497437
Epoch 1340, training loss: 417.9215087890625 = 0.723468005657196 + 50.0 * 8.343960762023926
Epoch 1340, val loss: 0.7402455806732178
Epoch 1350, training loss: 417.8654479980469 = 0.7193880081176758 + 50.0 * 8.342921257019043
Epoch 1350, val loss: 0.7364226579666138
Epoch 1360, training loss: 417.85107421875 = 0.7153638601303101 + 50.0 * 8.342714309692383
Epoch 1360, val loss: 0.7325916886329651
Epoch 1370, training loss: 417.87652587890625 = 0.7112351655960083 + 50.0 * 8.343305587768555
Epoch 1370, val loss: 0.7285854816436768
Epoch 1380, training loss: 417.80914306640625 = 0.7070446610450745 + 50.0 * 8.342041969299316
Epoch 1380, val loss: 0.7248551845550537
Epoch 1390, training loss: 417.89947509765625 = 0.7028951048851013 + 50.0 * 8.343932151794434
Epoch 1390, val loss: 0.7209309935569763
Epoch 1400, training loss: 417.7269287109375 = 0.698718786239624 + 50.0 * 8.340563774108887
Epoch 1400, val loss: 0.716973066329956
Epoch 1410, training loss: 417.6756591796875 = 0.6947446465492249 + 50.0 * 8.339618682861328
Epoch 1410, val loss: 0.7132706046104431
Epoch 1420, training loss: 417.64239501953125 = 0.6907289624214172 + 50.0 * 8.339033126831055
Epoch 1420, val loss: 0.7095177173614502
Epoch 1430, training loss: 417.613037109375 = 0.686733067035675 + 50.0 * 8.338525772094727
Epoch 1430, val loss: 0.7057301998138428
Epoch 1440, training loss: 417.6229553222656 = 0.6827118396759033 + 50.0 * 8.338805198669434
Epoch 1440, val loss: 0.7019344568252563
Epoch 1450, training loss: 417.6310119628906 = 0.6785118579864502 + 50.0 * 8.33905029296875
Epoch 1450, val loss: 0.6979259848594666
Epoch 1460, training loss: 417.5532531738281 = 0.6744565367698669 + 50.0 * 8.337575912475586
Epoch 1460, val loss: 0.694261908531189
Epoch 1470, training loss: 417.59393310546875 = 0.67047518491745 + 50.0 * 8.338469505310059
Epoch 1470, val loss: 0.6904855966567993
Epoch 1480, training loss: 417.4916076660156 = 0.6662740707397461 + 50.0 * 8.336506843566895
Epoch 1480, val loss: 0.6866475939750671
Epoch 1490, training loss: 417.4678955078125 = 0.6623502969741821 + 50.0 * 8.336111068725586
Epoch 1490, val loss: 0.6829582452774048
Epoch 1500, training loss: 417.43817138671875 = 0.6584439873695374 + 50.0 * 8.335594177246094
Epoch 1500, val loss: 0.6792544722557068
Epoch 1510, training loss: 417.4244689941406 = 0.6544849276542664 + 50.0 * 8.335399627685547
Epoch 1510, val loss: 0.6755815148353577
Epoch 1520, training loss: 417.5099792480469 = 0.6505303978919983 + 50.0 * 8.337188720703125
Epoch 1520, val loss: 0.6716673374176025
Epoch 1530, training loss: 417.5211181640625 = 0.6462342143058777 + 50.0 * 8.33749771118164
Epoch 1530, val loss: 0.6678972840309143
Epoch 1540, training loss: 417.3656005859375 = 0.642236590385437 + 50.0 * 8.334466934204102
Epoch 1540, val loss: 0.6641997694969177
Epoch 1550, training loss: 417.3342590332031 = 0.6383352875709534 + 50.0 * 8.333918571472168
Epoch 1550, val loss: 0.6605716943740845
Epoch 1560, training loss: 417.3092956542969 = 0.6344943642616272 + 50.0 * 8.33349609375
Epoch 1560, val loss: 0.6569915413856506
Epoch 1570, training loss: 417.2814025878906 = 0.6306577920913696 + 50.0 * 8.333015441894531
Epoch 1570, val loss: 0.6534324884414673
Epoch 1580, training loss: 417.26873779296875 = 0.6268132925033569 + 50.0 * 8.332839012145996
Epoch 1580, val loss: 0.6498458981513977
Epoch 1590, training loss: 417.4492492675781 = 0.6228771805763245 + 50.0 * 8.336527824401855
Epoch 1590, val loss: 0.6460126042366028
Epoch 1600, training loss: 417.27252197265625 = 0.6187921166419983 + 50.0 * 8.333074569702148
Epoch 1600, val loss: 0.642601728439331
Epoch 1610, training loss: 417.21649169921875 = 0.6150748133659363 + 50.0 * 8.33202838897705
Epoch 1610, val loss: 0.6389650702476501
Epoch 1620, training loss: 417.1799011230469 = 0.6113366484642029 + 50.0 * 8.331371307373047
Epoch 1620, val loss: 0.6356500387191772
Epoch 1630, training loss: 417.1531066894531 = 0.6076527833938599 + 50.0 * 8.33090877532959
Epoch 1630, val loss: 0.6322205066680908
Epoch 1640, training loss: 417.1474609375 = 0.6039422154426575 + 50.0 * 8.330870628356934
Epoch 1640, val loss: 0.6288145184516907
Epoch 1650, training loss: 417.2660217285156 = 0.6000289916992188 + 50.0 * 8.333319664001465
Epoch 1650, val loss: 0.6252735257148743
Epoch 1660, training loss: 417.0955505371094 = 0.5963210463523865 + 50.0 * 8.329984664916992
Epoch 1660, val loss: 0.6217969059944153
Epoch 1670, training loss: 417.0939636230469 = 0.5927417874336243 + 50.0 * 8.330024719238281
Epoch 1670, val loss: 0.6184849143028259
Epoch 1680, training loss: 417.05364990234375 = 0.5891658663749695 + 50.0 * 8.329289436340332
Epoch 1680, val loss: 0.6152052283287048
Epoch 1690, training loss: 417.04443359375 = 0.585603654384613 + 50.0 * 8.329176902770996
Epoch 1690, val loss: 0.6119337677955627
Epoch 1700, training loss: 417.2222595214844 = 0.5820201635360718 + 50.0 * 8.332804679870605
Epoch 1700, val loss: 0.608456015586853
Epoch 1710, training loss: 417.14288330078125 = 0.5780431628227234 + 50.0 * 8.331296920776367
Epoch 1710, val loss: 0.6051010489463806
Epoch 1720, training loss: 416.9907531738281 = 0.5746433734893799 + 50.0 * 8.328322410583496
Epoch 1720, val loss: 0.6019221544265747
Epoch 1730, training loss: 416.97479248046875 = 0.5712456703186035 + 50.0 * 8.328070640563965
Epoch 1730, val loss: 0.5988369584083557
Epoch 1740, training loss: 416.94244384765625 = 0.5679115056991577 + 50.0 * 8.32749080657959
Epoch 1740, val loss: 0.5957999229431152
Epoch 1750, training loss: 416.9242248535156 = 0.5645992755889893 + 50.0 * 8.327192306518555
Epoch 1750, val loss: 0.5927703976631165
Epoch 1760, training loss: 416.90277099609375 = 0.5612917542457581 + 50.0 * 8.32682991027832
Epoch 1760, val loss: 0.5897654294967651
Epoch 1770, training loss: 416.89117431640625 = 0.5580167770385742 + 50.0 * 8.32666301727295
Epoch 1770, val loss: 0.5867956280708313
Epoch 1780, training loss: 417.16632080078125 = 0.5547406673431396 + 50.0 * 8.332231521606445
Epoch 1780, val loss: 0.5837576389312744
Epoch 1790, training loss: 416.95904541015625 = 0.5510033965110779 + 50.0 * 8.328161239624023
Epoch 1790, val loss: 0.5804935693740845
Epoch 1800, training loss: 416.8369445800781 = 0.547927975654602 + 50.0 * 8.325779914855957
Epoch 1800, val loss: 0.5775817632675171
Epoch 1810, training loss: 416.8218078613281 = 0.5447832942008972 + 50.0 * 8.325540542602539
Epoch 1810, val loss: 0.5747920274734497
Epoch 1820, training loss: 416.8060302734375 = 0.5417007207870483 + 50.0 * 8.325286865234375
Epoch 1820, val loss: 0.5719324946403503
Epoch 1830, training loss: 416.99639892578125 = 0.5386051535606384 + 50.0 * 8.329155921936035
Epoch 1830, val loss: 0.5688634514808655
Epoch 1840, training loss: 416.8200378417969 = 0.5352382659912109 + 50.0 * 8.325695991516113
Epoch 1840, val loss: 0.5663768649101257
Epoch 1850, training loss: 416.7662353515625 = 0.5323255658149719 + 50.0 * 8.324678421020508
Epoch 1850, val loss: 0.5634443759918213
Epoch 1860, training loss: 416.72857666015625 = 0.5293391942977905 + 50.0 * 8.32398509979248
Epoch 1860, val loss: 0.5609584450721741
Epoch 1870, training loss: 416.7034606933594 = 0.5264409780502319 + 50.0 * 8.323540687561035
Epoch 1870, val loss: 0.5582598447799683
Epoch 1880, training loss: 416.6861572265625 = 0.5235474705696106 + 50.0 * 8.323251724243164
Epoch 1880, val loss: 0.5556865930557251
Epoch 1890, training loss: 416.66851806640625 = 0.520645797252655 + 50.0 * 8.322957038879395
Epoch 1890, val loss: 0.55307936668396
Epoch 1900, training loss: 416.67523193359375 = 0.5177668929100037 + 50.0 * 8.323149681091309
Epoch 1900, val loss: 0.5504844784736633
Epoch 1910, training loss: 416.7863464355469 = 0.5147548913955688 + 50.0 * 8.325431823730469
Epoch 1910, val loss: 0.5476831793785095
Epoch 1920, training loss: 416.69140625 = 0.5118533372879028 + 50.0 * 8.323591232299805
Epoch 1920, val loss: 0.5450632572174072
Epoch 1930, training loss: 416.6125183105469 = 0.508979082107544 + 50.0 * 8.322071075439453
Epoch 1930, val loss: 0.5425902605056763
Epoch 1940, training loss: 416.5813293457031 = 0.5062800645828247 + 50.0 * 8.321500778198242
Epoch 1940, val loss: 0.5402593016624451
Epoch 1950, training loss: 416.5712585449219 = 0.5035852789878845 + 50.0 * 8.321353912353516
Epoch 1950, val loss: 0.5379486680030823
Epoch 1960, training loss: 416.6960754394531 = 0.500883936882019 + 50.0 * 8.323904037475586
Epoch 1960, val loss: 0.5357416272163391
Epoch 1970, training loss: 416.556884765625 = 0.49804890155792236 + 50.0 * 8.321176528930664
Epoch 1970, val loss: 0.532774806022644
Epoch 1980, training loss: 416.5249938964844 = 0.4954482913017273 + 50.0 * 8.32059097290039
Epoch 1980, val loss: 0.530684769153595
Epoch 1990, training loss: 416.4938049316406 = 0.49294668436050415 + 50.0 * 8.320016860961914
Epoch 1990, val loss: 0.5283418297767639
Epoch 2000, training loss: 416.4797668457031 = 0.49043092131614685 + 50.0 * 8.31978702545166
Epoch 2000, val loss: 0.5262039303779602
Epoch 2010, training loss: 416.57598876953125 = 0.4879387319087982 + 50.0 * 8.321761131286621
Epoch 2010, val loss: 0.5239927768707275
Epoch 2020, training loss: 416.4755859375 = 0.4853129982948303 + 50.0 * 8.319805145263672
Epoch 2020, val loss: 0.5216343402862549
Epoch 2030, training loss: 416.4469299316406 = 0.48292359709739685 + 50.0 * 8.319280624389648
Epoch 2030, val loss: 0.51956707239151
Epoch 2040, training loss: 416.4295959472656 = 0.4805205762386322 + 50.0 * 8.318981170654297
Epoch 2040, val loss: 0.5174524188041687
Epoch 2050, training loss: 416.6180114746094 = 0.4781063199043274 + 50.0 * 8.322797775268555
Epoch 2050, val loss: 0.5154549479484558
Epoch 2060, training loss: 416.443603515625 = 0.475515216588974 + 50.0 * 8.319361686706543
Epoch 2060, val loss: 0.5129483938217163
Epoch 2070, training loss: 416.392578125 = 0.4733255207538605 + 50.0 * 8.318385124206543
Epoch 2070, val loss: 0.5110481977462769
Epoch 2080, training loss: 416.35382080078125 = 0.4710329473018646 + 50.0 * 8.317655563354492
Epoch 2080, val loss: 0.5090567469596863
Epoch 2090, training loss: 416.33062744140625 = 0.46885809302330017 + 50.0 * 8.317234992980957
Epoch 2090, val loss: 0.5072066783905029
Epoch 2100, training loss: 416.3251037597656 = 0.46665528416633606 + 50.0 * 8.317169189453125
Epoch 2100, val loss: 0.5053473114967346
Epoch 2110, training loss: 416.530517578125 = 0.4643714725971222 + 50.0 * 8.321322441101074
Epoch 2110, val loss: 0.5035871267318726
Epoch 2120, training loss: 416.3670654296875 = 0.46207672357559204 + 50.0 * 8.318099975585938
Epoch 2120, val loss: 0.5011020302772522
Epoch 2130, training loss: 416.2740173339844 = 0.45996472239494324 + 50.0 * 8.31628131866455
Epoch 2130, val loss: 0.49930059909820557
Epoch 2140, training loss: 416.2691345214844 = 0.45788830518722534 + 50.0 * 8.316225051879883
Epoch 2140, val loss: 0.4976855218410492
Epoch 2150, training loss: 416.2427673339844 = 0.45588499307632446 + 50.0 * 8.3157377243042
Epoch 2150, val loss: 0.4958151876926422
Epoch 2160, training loss: 416.22747802734375 = 0.45388296246528625 + 50.0 * 8.315471649169922
Epoch 2160, val loss: 0.49410367012023926
Epoch 2170, training loss: 416.2789306640625 = 0.4518924057483673 + 50.0 * 8.316540718078613
Epoch 2170, val loss: 0.492372989654541
Epoch 2180, training loss: 416.2161560058594 = 0.4497411847114563 + 50.0 * 8.315328598022461
Epoch 2180, val loss: 0.4904334247112274
Epoch 2190, training loss: 416.1925048828125 = 0.44777101278305054 + 50.0 * 8.314894676208496
Epoch 2190, val loss: 0.48883214592933655
Epoch 2200, training loss: 416.179443359375 = 0.4458627998828888 + 50.0 * 8.314671516418457
Epoch 2200, val loss: 0.487114816904068
Epoch 2210, training loss: 416.36767578125 = 0.4439789652824402 + 50.0 * 8.318473815917969
Epoch 2210, val loss: 0.4851182997226715
Epoch 2220, training loss: 416.2312316894531 = 0.44177961349487305 + 50.0 * 8.315789222717285
Epoch 2220, val loss: 0.4839105010032654
Epoch 2230, training loss: 416.18450927734375 = 0.4399864375591278 + 50.0 * 8.31489086151123
Epoch 2230, val loss: 0.4819616675376892
Epoch 2240, training loss: 416.12103271484375 = 0.4382096230983734 + 50.0 * 8.3136568069458
Epoch 2240, val loss: 0.4805483818054199
Epoch 2250, training loss: 416.11016845703125 = 0.4364900588989258 + 50.0 * 8.31347370147705
Epoch 2250, val loss: 0.47915464639663696
Epoch 2260, training loss: 416.0917663574219 = 0.43478044867515564 + 50.0 * 8.313139915466309
Epoch 2260, val loss: 0.47762322425842285
Epoch 2270, training loss: 416.0760498046875 = 0.4330884516239166 + 50.0 * 8.312859535217285
Epoch 2270, val loss: 0.47621282935142517
Epoch 2280, training loss: 416.0638427734375 = 0.4313996434211731 + 50.0 * 8.31264877319336
Epoch 2280, val loss: 0.4747462570667267
Epoch 2290, training loss: 416.1153259277344 = 0.4297269582748413 + 50.0 * 8.313712120056152
Epoch 2290, val loss: 0.47336116433143616
Epoch 2300, training loss: 416.06146240234375 = 0.4278901517391205 + 50.0 * 8.312671661376953
Epoch 2300, val loss: 0.4716338515281677
Epoch 2310, training loss: 416.060791015625 = 0.4262736141681671 + 50.0 * 8.312690734863281
Epoch 2310, val loss: 0.47043588757514954
Epoch 2320, training loss: 416.0141906738281 = 0.42463985085487366 + 50.0 * 8.311790466308594
Epoch 2320, val loss: 0.4690030515193939
Epoch 2330, training loss: 416.01043701171875 = 0.42305314540863037 + 50.0 * 8.311747550964355
Epoch 2330, val loss: 0.4676131308078766
Epoch 2340, training loss: 416.1942138671875 = 0.4214712381362915 + 50.0 * 8.315454483032227
Epoch 2340, val loss: 0.4660254120826721
Epoch 2350, training loss: 416.0420837402344 = 0.4197174310684204 + 50.0 * 8.312447547912598
Epoch 2350, val loss: 0.46495410799980164
Epoch 2360, training loss: 415.9996032714844 = 0.4181947112083435 + 50.0 * 8.311628341674805
Epoch 2360, val loss: 0.4635181128978729
Epoch 2370, training loss: 415.9612121582031 = 0.41670721769332886 + 50.0 * 8.310890197753906
Epoch 2370, val loss: 0.46238526701927185
Epoch 2380, training loss: 415.9469299316406 = 0.41524216532707214 + 50.0 * 8.310633659362793
Epoch 2380, val loss: 0.461130291223526
Epoch 2390, training loss: 416.00323486328125 = 0.4138036370277405 + 50.0 * 8.311788558959961
Epoch 2390, val loss: 0.4598848819732666
Epoch 2400, training loss: 415.94219970703125 = 0.41222888231277466 + 50.0 * 8.310599327087402
Epoch 2400, val loss: 0.4586374759674072
Epoch 2410, training loss: 415.9346008300781 = 0.4108051359653473 + 50.0 * 8.310476303100586
Epoch 2410, val loss: 0.45728161931037903
Epoch 2420, training loss: 415.9576110839844 = 0.4093957841396332 + 50.0 * 8.310964584350586
Epoch 2420, val loss: 0.4561588168144226
Epoch 2430, training loss: 415.93115234375 = 0.40796130895614624 + 50.0 * 8.310463905334473
Epoch 2430, val loss: 0.4548947215080261
Epoch 2440, training loss: 415.8835754394531 = 0.40658295154571533 + 50.0 * 8.309539794921875
Epoch 2440, val loss: 0.45400163531303406
Epoch 2450, training loss: 415.8810729980469 = 0.4052678346633911 + 50.0 * 8.309515953063965
Epoch 2450, val loss: 0.45296767354011536
Epoch 2460, training loss: 415.8968811035156 = 0.4039430022239685 + 50.0 * 8.309859275817871
Epoch 2460, val loss: 0.45198938250541687
Epoch 2470, training loss: 415.9187316894531 = 0.4025932550430298 + 50.0 * 8.310322761535645
Epoch 2470, val loss: 0.45089584589004517
Epoch 2480, training loss: 415.83245849609375 = 0.4012928307056427 + 50.0 * 8.308623313903809
Epoch 2480, val loss: 0.44961610436439514
Epoch 2490, training loss: 415.8472900390625 = 0.40004676580429077 + 50.0 * 8.308944702148438
Epoch 2490, val loss: 0.4484912157058716
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8457635717909691
0.8633630370209375
The final CL Acc:0.83274, 0.01123, The final GNN Acc:0.86298, 0.00045
Begin epxeriment: noisy_level: 0.2 cont_weight: 50 epoch: der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106298])
remove edge: torch.Size([2, 71136])
updated graph: torch.Size([2, 88786])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 530.2115478515625 = 1.0958067178726196 + 50.0 * 10.582314491271973
Epoch 0, val loss: 1.094637393951416
Epoch 10, training loss: 530.1962280273438 = 1.0916705131530762 + 50.0 * 10.582091331481934
Epoch 10, val loss: 1.0904990434646606
Epoch 20, training loss: 530.1414184570312 = 1.0871201753616333 + 50.0 * 10.581086158752441
Epoch 20, val loss: 1.0859394073486328
Epoch 30, training loss: 529.8909301757812 = 1.0820391178131104 + 50.0 * 10.576177597045898
Epoch 30, val loss: 1.0808401107788086
Epoch 40, training loss: 528.8084716796875 = 1.0764445066452026 + 50.0 * 10.554640769958496
Epoch 40, val loss: 1.075228214263916
Epoch 50, training loss: 525.0719604492188 = 1.0704941749572754 + 50.0 * 10.480029106140137
Epoch 50, val loss: 1.0692503452301025
Epoch 60, training loss: 515.1817626953125 = 1.064653754234314 + 50.0 * 10.282342910766602
Epoch 60, val loss: 1.0634444952011108
Epoch 70, training loss: 502.4264221191406 = 1.0594810247421265 + 50.0 * 10.027338981628418
Epoch 70, val loss: 1.058333396911621
Epoch 80, training loss: 491.7639465332031 = 1.0556097030639648 + 50.0 * 9.814167022705078
Epoch 80, val loss: 1.054610252380371
Epoch 90, training loss: 474.7999572753906 = 1.0519297122955322 + 50.0 * 9.474960327148438
Epoch 90, val loss: 1.05099618434906
Epoch 100, training loss: 463.7986145019531 = 1.0483077764511108 + 50.0 * 9.255005836486816
Epoch 100, val loss: 1.0474656820297241
Epoch 110, training loss: 461.10888671875 = 1.0448464155197144 + 50.0 * 9.20128059387207
Epoch 110, val loss: 1.0440624952316284
Epoch 120, training loss: 457.9800109863281 = 1.0416538715362549 + 50.0 * 9.13876724243164
Epoch 120, val loss: 1.0410494804382324
Epoch 130, training loss: 453.203369140625 = 1.0393012762069702 + 50.0 * 9.043281555175781
Epoch 130, val loss: 1.0389032363891602
Epoch 140, training loss: 448.9461364746094 = 1.0372473001480103 + 50.0 * 8.95817756652832
Epoch 140, val loss: 1.036967158317566
Epoch 150, training loss: 446.04913330078125 = 1.0346739292144775 + 50.0 * 8.900289535522461
Epoch 150, val loss: 1.0344350337982178
Epoch 160, training loss: 443.86163330078125 = 1.0313783884048462 + 50.0 * 8.856605529785156
Epoch 160, val loss: 1.0312180519104004
Epoch 170, training loss: 442.142333984375 = 1.0278077125549316 + 50.0 * 8.822290420532227
Epoch 170, val loss: 1.0277459621429443
Epoch 180, training loss: 440.617919921875 = 1.024144172668457 + 50.0 * 8.791875839233398
Epoch 180, val loss: 1.0242185592651367
Epoch 190, training loss: 439.3572998046875 = 1.0203686952590942 + 50.0 * 8.766738891601562
Epoch 190, val loss: 1.0205892324447632
Epoch 200, training loss: 438.3272705078125 = 1.0163331031799316 + 50.0 * 8.74621868133545
Epoch 200, val loss: 1.0166921615600586
Epoch 210, training loss: 437.49456787109375 = 1.0119086503982544 + 50.0 * 8.729653358459473
Epoch 210, val loss: 1.0124033689498901
Epoch 220, training loss: 436.806396484375 = 1.007035255432129 + 50.0 * 8.715987205505371
Epoch 220, val loss: 1.0076879262924194
Epoch 230, training loss: 436.2602844238281 = 1.0017045736312866 + 50.0 * 8.705171585083008
Epoch 230, val loss: 1.0025216341018677
Epoch 240, training loss: 435.8702392578125 = 0.9959203600883484 + 50.0 * 8.697486877441406
Epoch 240, val loss: 0.9969154596328735
Epoch 250, training loss: 435.2396545410156 = 0.9897127747535706 + 50.0 * 8.684998512268066
Epoch 250, val loss: 0.990921139717102
Epoch 260, training loss: 434.65802001953125 = 0.9831476211547852 + 50.0 * 8.673497200012207
Epoch 260, val loss: 0.9845967888832092
Epoch 270, training loss: 434.1022033691406 = 0.9762184619903564 + 50.0 * 8.662519454956055
Epoch 270, val loss: 0.9779156446456909
Epoch 280, training loss: 433.4663391113281 = 0.9688383340835571 + 50.0 * 8.64995002746582
Epoch 280, val loss: 0.9708119630813599
Epoch 290, training loss: 432.8236389160156 = 0.9610121846199036 + 50.0 * 8.637252807617188
Epoch 290, val loss: 0.9632874727249146
Epoch 300, training loss: 432.2640380859375 = 0.9527221322059631 + 50.0 * 8.626226425170898
Epoch 300, val loss: 0.9552994966506958
Epoch 310, training loss: 431.7708435058594 = 0.9438902139663696 + 50.0 * 8.616539001464844
Epoch 310, val loss: 0.9467906355857849
Epoch 320, training loss: 431.3709411621094 = 0.9344943165779114 + 50.0 * 8.608728408813477
Epoch 320, val loss: 0.9377672076225281
Epoch 330, training loss: 430.9222106933594 = 0.9246750473976135 + 50.0 * 8.599950790405273
Epoch 330, val loss: 0.9283073544502258
Epoch 340, training loss: 430.5416259765625 = 0.914466917514801 + 50.0 * 8.592543601989746
Epoch 340, val loss: 0.918507993221283
Epoch 350, training loss: 430.496826171875 = 0.9039159417152405 + 50.0 * 8.59185791015625
Epoch 350, val loss: 0.9084291458129883
Epoch 360, training loss: 429.9710388183594 = 0.8930621147155762 + 50.0 * 8.581559181213379
Epoch 360, val loss: 0.8980307579040527
Epoch 370, training loss: 429.55926513671875 = 0.8820549845695496 + 50.0 * 8.5735445022583
Epoch 370, val loss: 0.8875113129615784
Epoch 380, training loss: 429.2453308105469 = 0.8709494471549988 + 50.0 * 8.567487716674805
Epoch 380, val loss: 0.8769484162330627
Epoch 390, training loss: 428.9397888183594 = 0.859811007976532 + 50.0 * 8.561599731445312
Epoch 390, val loss: 0.8663649559020996
Epoch 400, training loss: 428.74530029296875 = 0.8486965894699097 + 50.0 * 8.557931900024414
Epoch 400, val loss: 0.8557981848716736
Epoch 410, training loss: 428.4863586425781 = 0.837537944316864 + 50.0 * 8.552976608276367
Epoch 410, val loss: 0.8452512621879578
Epoch 420, training loss: 428.1703796386719 = 0.8264733552932739 + 50.0 * 8.54687786102295
Epoch 420, val loss: 0.8348304033279419
Epoch 430, training loss: 427.935791015625 = 0.8155792355537415 + 50.0 * 8.542404174804688
Epoch 430, val loss: 0.8245808482170105
Epoch 440, training loss: 427.7051696777344 = 0.8048680424690247 + 50.0 * 8.538005828857422
Epoch 440, val loss: 0.8145174980163574
Epoch 450, training loss: 427.57135009765625 = 0.7943241000175476 + 50.0 * 8.535540580749512
Epoch 450, val loss: 0.8046619296073914
Epoch 460, training loss: 427.340087890625 = 0.783941924571991 + 50.0 * 8.531123161315918
Epoch 460, val loss: 0.7950009107589722
Epoch 470, training loss: 427.135986328125 = 0.7738017439842224 + 50.0 * 8.527243614196777
Epoch 470, val loss: 0.785557746887207
Epoch 480, training loss: 426.93170166015625 = 0.7639104723930359 + 50.0 * 8.523355484008789
Epoch 480, val loss: 0.7764483690261841
Epoch 490, training loss: 426.8734436035156 = 0.7542737126350403 + 50.0 * 8.522383689880371
Epoch 490, val loss: 0.7675981521606445
Epoch 500, training loss: 426.6767272949219 = 0.7449101209640503 + 50.0 * 8.518636703491211
Epoch 500, val loss: 0.7589176297187805
Epoch 510, training loss: 426.4518127441406 = 0.7358084917068481 + 50.0 * 8.514320373535156
Epoch 510, val loss: 0.7506645917892456
Epoch 520, training loss: 426.2994384765625 = 0.7270459532737732 + 50.0 * 8.51144790649414
Epoch 520, val loss: 0.7427117824554443
Epoch 530, training loss: 426.1589660644531 = 0.7185859084129333 + 50.0 * 8.508807182312012
Epoch 530, val loss: 0.7350842356681824
Epoch 540, training loss: 426.0101013183594 = 0.710416316986084 + 50.0 * 8.505993843078613
Epoch 540, val loss: 0.7277505397796631
Epoch 550, training loss: 425.9267578125 = 0.702549397945404 + 50.0 * 8.504484176635742
Epoch 550, val loss: 0.7207237482070923
Epoch 560, training loss: 425.7787170410156 = 0.6950321197509766 + 50.0 * 8.501673698425293
Epoch 560, val loss: 0.714072585105896
Epoch 570, training loss: 425.6280517578125 = 0.687797486782074 + 50.0 * 8.498805046081543
Epoch 570, val loss: 0.7077568173408508
Epoch 580, training loss: 425.5008239746094 = 0.6808857321739197 + 50.0 * 8.49639892578125
Epoch 580, val loss: 0.7017394304275513
Epoch 590, training loss: 425.3707275390625 = 0.6742861866950989 + 50.0 * 8.493928909301758
Epoch 590, val loss: 0.6960540413856506
Epoch 600, training loss: 425.484130859375 = 0.6680043339729309 + 50.0 * 8.496322631835938
Epoch 600, val loss: 0.690690279006958
Epoch 610, training loss: 425.1650085449219 = 0.6619622707366943 + 50.0 * 8.490060806274414
Epoch 610, val loss: 0.6855046153068542
Epoch 620, training loss: 425.0460205078125 = 0.6562272906303406 + 50.0 * 8.48779582977295
Epoch 620, val loss: 0.680702269077301
Epoch 630, training loss: 424.9355163574219 = 0.6508135795593262 + 50.0 * 8.48569393157959
Epoch 630, val loss: 0.6762023568153381
Epoch 640, training loss: 424.82470703125 = 0.6456823945045471 + 50.0 * 8.483580589294434
Epoch 640, val loss: 0.6720150709152222
Epoch 650, training loss: 424.7748107910156 = 0.6408184170722961 + 50.0 * 8.482680320739746
Epoch 650, val loss: 0.6680684685707092
Epoch 660, training loss: 424.6970520019531 = 0.6361408829689026 + 50.0 * 8.481218338012695
Epoch 660, val loss: 0.664353609085083
Epoch 670, training loss: 424.5740661621094 = 0.6317335367202759 + 50.0 * 8.478846549987793
Epoch 670, val loss: 0.6608216166496277
Epoch 680, training loss: 424.4955749511719 = 0.627587080001831 + 50.0 * 8.477359771728516
Epoch 680, val loss: 0.6576008200645447
Epoch 690, training loss: 424.43853759765625 = 0.6236714124679565 + 50.0 * 8.476297378540039
Epoch 690, val loss: 0.6546090245246887
Epoch 700, training loss: 424.36151123046875 = 0.6199416518211365 + 50.0 * 8.474831581115723
Epoch 700, val loss: 0.6517836451530457
Epoch 710, training loss: 424.3179016113281 = 0.6164094805717468 + 50.0 * 8.474029541015625
Epoch 710, val loss: 0.6491523385047913
Epoch 720, training loss: 424.2035217285156 = 0.6130844354629517 + 50.0 * 8.471808433532715
Epoch 720, val loss: 0.6467107534408569
Epoch 730, training loss: 424.13458251953125 = 0.6099464297294617 + 50.0 * 8.470492362976074
Epoch 730, val loss: 0.6444926857948303
Epoch 740, training loss: 424.06591796875 = 0.6069819331169128 + 50.0 * 8.469178199768066
Epoch 740, val loss: 0.6423906087875366
Epoch 750, training loss: 424.0076599121094 = 0.604167640209198 + 50.0 * 8.468070030212402
Epoch 750, val loss: 0.640447199344635
Epoch 760, training loss: 424.1931457519531 = 0.6014652252197266 + 50.0 * 8.471833229064941
Epoch 760, val loss: 0.6385427713394165
Epoch 770, training loss: 423.9078674316406 = 0.5988821983337402 + 50.0 * 8.466179847717285
Epoch 770, val loss: 0.6367853283882141
Epoch 780, training loss: 423.8567810058594 = 0.5964733362197876 + 50.0 * 8.465206146240234
Epoch 780, val loss: 0.6352254748344421
Epoch 790, training loss: 423.7772521972656 = 0.5942002534866333 + 50.0 * 8.463661193847656
Epoch 790, val loss: 0.6337714195251465
Epoch 800, training loss: 423.70361328125 = 0.5920391082763672 + 50.0 * 8.462231636047363
Epoch 800, val loss: 0.632432758808136
Epoch 810, training loss: 423.6637268066406 = 0.5899742841720581 + 50.0 * 8.461475372314453
Epoch 810, val loss: 0.6311874389648438
Epoch 820, training loss: 423.6653747558594 = 0.5879673957824707 + 50.0 * 8.4615478515625
Epoch 820, val loss: 0.6298691630363464
Epoch 830, training loss: 423.6081237792969 = 0.5860280990600586 + 50.0 * 8.460441589355469
Epoch 830, val loss: 0.6287069916725159
Epoch 840, training loss: 423.4891052246094 = 0.5842175483703613 + 50.0 * 8.458097457885742
Epoch 840, val loss: 0.6276281476020813
Epoch 850, training loss: 423.4389953613281 = 0.5825021266937256 + 50.0 * 8.457130432128906
Epoch 850, val loss: 0.6266568899154663
Epoch 860, training loss: 423.3793640136719 = 0.5808529257774353 + 50.0 * 8.455970764160156
Epoch 860, val loss: 0.6257244348526001
Epoch 870, training loss: 423.3255920410156 = 0.5792655348777771 + 50.0 * 8.454926490783691
Epoch 870, val loss: 0.624843180179596
Epoch 880, training loss: 423.30975341796875 = 0.5777303576469421 + 50.0 * 8.45464038848877
Epoch 880, val loss: 0.6239922046661377
Epoch 890, training loss: 423.3033447265625 = 0.5762129426002502 + 50.0 * 8.454543113708496
Epoch 890, val loss: 0.6231465935707092
Epoch 900, training loss: 423.1893615722656 = 0.5747294425964355 + 50.0 * 8.452292442321777
Epoch 900, val loss: 0.6223362684249878
Epoch 910, training loss: 423.139404296875 = 0.5733379125595093 + 50.0 * 8.451321601867676
Epoch 910, val loss: 0.6215883493423462
Epoch 920, training loss: 423.0877685546875 = 0.5720049142837524 + 50.0 * 8.450315475463867
Epoch 920, val loss: 0.6208817958831787
Epoch 930, training loss: 423.0425109863281 = 0.5707195997238159 + 50.0 * 8.44943618774414
Epoch 930, val loss: 0.6202036738395691
Epoch 940, training loss: 423.01690673828125 = 0.569466233253479 + 50.0 * 8.448948860168457
Epoch 940, val loss: 0.6195444464683533
Epoch 950, training loss: 423.29217529296875 = 0.5682137608528137 + 50.0 * 8.454479217529297
Epoch 950, val loss: 0.6188921332359314
Epoch 960, training loss: 422.9427185058594 = 0.5669931173324585 + 50.0 * 8.447514533996582
Epoch 960, val loss: 0.6181988716125488
Epoch 970, training loss: 422.9148864746094 = 0.565834641456604 + 50.0 * 8.446981430053711
Epoch 970, val loss: 0.6176139116287231
Epoch 980, training loss: 422.8520202636719 = 0.5647086501121521 + 50.0 * 8.445746421813965
Epoch 980, val loss: 0.6170766353607178
Epoch 990, training loss: 422.8126525878906 = 0.5636174082756042 + 50.0 * 8.44498062133789
Epoch 990, val loss: 0.6165302991867065
Epoch 1000, training loss: 422.7751159667969 = 0.5625557899475098 + 50.0 * 8.44425106048584
Epoch 1000, val loss: 0.6160051822662354
Epoch 1010, training loss: 423.15692138671875 = 0.5615110397338867 + 50.0 * 8.451908111572266
Epoch 1010, val loss: 0.61542809009552
Epoch 1020, training loss: 422.7640686035156 = 0.5604349374771118 + 50.0 * 8.444072723388672
Epoch 1020, val loss: 0.6149870753288269
Epoch 1030, training loss: 422.6950378417969 = 0.5594378709793091 + 50.0 * 8.44271183013916
Epoch 1030, val loss: 0.6144277453422546
Epoch 1040, training loss: 422.6429443359375 = 0.5584612488746643 + 50.0 * 8.441689491271973
Epoch 1040, val loss: 0.61394864320755
Epoch 1050, training loss: 422.59063720703125 = 0.5575200319290161 + 50.0 * 8.440662384033203
Epoch 1050, val loss: 0.6134958267211914
Epoch 1060, training loss: 422.56475830078125 = 0.5565909743309021 + 50.0 * 8.440163612365723
Epoch 1060, val loss: 0.61304771900177
Epoch 1070, training loss: 422.8703918457031 = 0.5556620955467224 + 50.0 * 8.446294784545898
Epoch 1070, val loss: 0.6125412583351135
Epoch 1080, training loss: 422.5184020996094 = 0.5547312498092651 + 50.0 * 8.439273834228516
Epoch 1080, val loss: 0.6121599674224854
Epoch 1090, training loss: 422.4596862792969 = 0.5538460612297058 + 50.0 * 8.438117027282715
Epoch 1090, val loss: 0.6117679476737976
Epoch 1100, training loss: 422.4216003417969 = 0.5529869198799133 + 50.0 * 8.437372207641602
Epoch 1100, val loss: 0.6113430261611938
Epoch 1110, training loss: 422.4890441894531 = 0.5521459579467773 + 50.0 * 8.438737869262695
Epoch 1110, val loss: 0.6109646558761597
Epoch 1120, training loss: 422.34344482421875 = 0.5512781739234924 + 50.0 * 8.435843467712402
Epoch 1120, val loss: 0.6105523705482483
Epoch 1130, training loss: 422.2771911621094 = 0.5504389405250549 + 50.0 * 8.434535026550293
Epoch 1130, val loss: 0.6101765036582947
Epoch 1140, training loss: 422.2391357421875 = 0.5496324896812439 + 50.0 * 8.43379020690918
Epoch 1140, val loss: 0.6098464131355286
Epoch 1150, training loss: 422.2500305175781 = 0.5488362908363342 + 50.0 * 8.4340238571167
Epoch 1150, val loss: 0.6095072031021118
Epoch 1160, training loss: 422.2265319824219 = 0.5480248332023621 + 50.0 * 8.43356990814209
Epoch 1160, val loss: 0.6091291904449463
Epoch 1170, training loss: 422.1486511230469 = 0.547213613986969 + 50.0 * 8.432028770446777
Epoch 1170, val loss: 0.608759880065918
Epoch 1180, training loss: 422.1053466796875 = 0.5464429259300232 + 50.0 * 8.431178092956543
Epoch 1180, val loss: 0.6083505153656006
Epoch 1190, training loss: 422.0343017578125 = 0.5456874966621399 + 50.0 * 8.42977237701416
Epoch 1190, val loss: 0.6080430746078491
Epoch 1200, training loss: 422.0876770019531 = 0.544938325881958 + 50.0 * 8.430854797363281
Epoch 1200, val loss: 0.6077378988265991
Epoch 1210, training loss: 421.9621887207031 = 0.5441763401031494 + 50.0 * 8.428359985351562
Epoch 1210, val loss: 0.6073049306869507
Epoch 1220, training loss: 421.9491271972656 = 0.5434141755104065 + 50.0 * 8.42811393737793
Epoch 1220, val loss: 0.6069815754890442
Epoch 1230, training loss: 421.9558410644531 = 0.5426895618438721 + 50.0 * 8.428262710571289
Epoch 1230, val loss: 0.6065711975097656
Epoch 1240, training loss: 421.9282531738281 = 0.5419546365737915 + 50.0 * 8.427725791931152
Epoch 1240, val loss: 0.6062700152397156
Epoch 1250, training loss: 421.8280944824219 = 0.5412449836730957 + 50.0 * 8.425736427307129
Epoch 1250, val loss: 0.6059619188308716
Epoch 1260, training loss: 421.8133239746094 = 0.5405318737030029 + 50.0 * 8.425456047058105
Epoch 1260, val loss: 0.605621874332428
Epoch 1270, training loss: 421.8592529296875 = 0.5398147702217102 + 50.0 * 8.42638874053955
Epoch 1270, val loss: 0.6052845120429993
Epoch 1280, training loss: 421.7492370605469 = 0.5391030311584473 + 50.0 * 8.424202919006348
Epoch 1280, val loss: 0.6049590110778809
Epoch 1290, training loss: 421.6951904296875 = 0.5384030342102051 + 50.0 * 8.423135757446289
Epoch 1290, val loss: 0.6046649813652039
Epoch 1300, training loss: 421.81695556640625 = 0.5377091765403748 + 50.0 * 8.42558479309082
Epoch 1300, val loss: 0.6043108105659485
Epoch 1310, training loss: 421.6748352050781 = 0.5369940400123596 + 50.0 * 8.422757148742676
Epoch 1310, val loss: 0.604024350643158
Epoch 1320, training loss: 421.6557312011719 = 0.5362810492515564 + 50.0 * 8.422389030456543
Epoch 1320, val loss: 0.6036238074302673
Epoch 1330, training loss: 421.55145263671875 = 0.5356136560440063 + 50.0 * 8.420316696166992
Epoch 1330, val loss: 0.6033385396003723
Epoch 1340, training loss: 421.5221252441406 = 0.5349504351615906 + 50.0 * 8.419743537902832
Epoch 1340, val loss: 0.6030925512313843
Epoch 1350, training loss: 421.49798583984375 = 0.5343008041381836 + 50.0 * 8.419273376464844
Epoch 1350, val loss: 0.6028201580047607
Epoch 1360, training loss: 421.81683349609375 = 0.5336320996284485 + 50.0 * 8.425663948059082
Epoch 1360, val loss: 0.6026361584663391
Epoch 1370, training loss: 421.5362243652344 = 0.5329388976097107 + 50.0 * 8.420065879821777
Epoch 1370, val loss: 0.6021152138710022
Epoch 1380, training loss: 421.4346618652344 = 0.5322632789611816 + 50.0 * 8.418047904968262
Epoch 1380, val loss: 0.6017496585845947
Epoch 1390, training loss: 421.3975830078125 = 0.5316067934036255 + 50.0 * 8.417319297790527
Epoch 1390, val loss: 0.6015593409538269
Epoch 1400, training loss: 421.3780212402344 = 0.5309675931930542 + 50.0 * 8.416940689086914
Epoch 1400, val loss: 0.6013041734695435
Epoch 1410, training loss: 421.41265869140625 = 0.5303242802619934 + 50.0 * 8.417646408081055
Epoch 1410, val loss: 0.6010026931762695
Epoch 1420, training loss: 421.3463134765625 = 0.5296661257743835 + 50.0 * 8.416333198547363
Epoch 1420, val loss: 0.6007185578346252
Epoch 1430, training loss: 421.48638916015625 = 0.529007613658905 + 50.0 * 8.419147491455078
Epoch 1430, val loss: 0.600417971611023
Epoch 1440, training loss: 421.3052062988281 = 0.5283281207084656 + 50.0 * 8.41553783416748
Epoch 1440, val loss: 0.6000947952270508
Epoch 1450, training loss: 421.2586975097656 = 0.5276789665222168 + 50.0 * 8.414620399475098
Epoch 1450, val loss: 0.5998633503913879
Epoch 1460, training loss: 421.23651123046875 = 0.5270442962646484 + 50.0 * 8.414189338684082
Epoch 1460, val loss: 0.5995487570762634
Epoch 1470, training loss: 421.39654541015625 = 0.5263985395431519 + 50.0 * 8.417403221130371
Epoch 1470, val loss: 0.5993078351020813
Epoch 1480, training loss: 421.31134033203125 = 0.5257373452186584 + 50.0 * 8.415712356567383
Epoch 1480, val loss: 0.5990345478057861
Epoch 1490, training loss: 421.2056884765625 = 0.5250747203826904 + 50.0 * 8.413612365722656
Epoch 1490, val loss: 0.5987480878829956
Epoch 1500, training loss: 421.1432189941406 = 0.5244386792182922 + 50.0 * 8.412375450134277
Epoch 1500, val loss: 0.598484218120575
Epoch 1510, training loss: 421.1299743652344 = 0.5238081812858582 + 50.0 * 8.412123680114746
Epoch 1510, val loss: 0.598248302936554
Epoch 1520, training loss: 421.3310852050781 = 0.5231776833534241 + 50.0 * 8.416158676147461
Epoch 1520, val loss: 0.5979984402656555
Epoch 1530, training loss: 421.2776794433594 = 0.5224786400794983 + 50.0 * 8.415103912353516
Epoch 1530, val loss: 0.5976351499557495
Epoch 1540, training loss: 421.1574401855469 = 0.5218157768249512 + 50.0 * 8.412712097167969
Epoch 1540, val loss: 0.5973793864250183
Epoch 1550, training loss: 421.0582275390625 = 0.5211672186851501 + 50.0 * 8.410740852355957
Epoch 1550, val loss: 0.5970892906188965
Epoch 1560, training loss: 421.0346984863281 = 0.5205390453338623 + 50.0 * 8.410283088684082
Epoch 1560, val loss: 0.5968244075775146
Epoch 1570, training loss: 421.0177307128906 = 0.5199092030525208 + 50.0 * 8.409956932067871
Epoch 1570, val loss: 0.5965840220451355
Epoch 1580, training loss: 421.02374267578125 = 0.5192714929580688 + 50.0 * 8.410089492797852
Epoch 1580, val loss: 0.5963284373283386
Epoch 1590, training loss: 421.375244140625 = 0.5186164975166321 + 50.0 * 8.417132377624512
Epoch 1590, val loss: 0.5961219072341919
Epoch 1600, training loss: 421.0811462402344 = 0.5179133415222168 + 50.0 * 8.411264419555664
Epoch 1600, val loss: 0.5956634879112244
Epoch 1610, training loss: 421.021728515625 = 0.5172252058982849 + 50.0 * 8.410090446472168
Epoch 1610, val loss: 0.5952372550964355
Epoch 1620, training loss: 420.93975830078125 = 0.5165754556655884 + 50.0 * 8.408463478088379
Epoch 1620, val loss: 0.5950304269790649
Epoch 1630, training loss: 420.92352294921875 = 0.5159345865249634 + 50.0 * 8.408151626586914
Epoch 1630, val loss: 0.5947473645210266
Epoch 1640, training loss: 420.8949279785156 = 0.5152902603149414 + 50.0 * 8.4075927734375
Epoch 1640, val loss: 0.5944666862487793
Epoch 1650, training loss: 420.8800354003906 = 0.5146409273147583 + 50.0 * 8.407307624816895
Epoch 1650, val loss: 0.5942086577415466
Epoch 1660, training loss: 420.9056396484375 = 0.5139837861061096 + 50.0 * 8.407833099365234
Epoch 1660, val loss: 0.5939528942108154
Epoch 1670, training loss: 421.0522155761719 = 0.5133042931556702 + 50.0 * 8.410778045654297
Epoch 1670, val loss: 0.5936691761016846
Epoch 1680, training loss: 420.9070739746094 = 0.512604296207428 + 50.0 * 8.407889366149902
Epoch 1680, val loss: 0.5932695865631104
Epoch 1690, training loss: 420.84130859375 = 0.5119297504425049 + 50.0 * 8.406587600708008
Epoch 1690, val loss: 0.5930108428001404
Epoch 1700, training loss: 420.8182678222656 = 0.5112646818161011 + 50.0 * 8.406140327453613
Epoch 1700, val loss: 0.5927134156227112
Epoch 1710, training loss: 421.0028991699219 = 0.510601282119751 + 50.0 * 8.409846305847168
Epoch 1710, val loss: 0.592507541179657
Epoch 1720, training loss: 420.8898010253906 = 0.5098994374275208 + 50.0 * 8.407598495483398
Epoch 1720, val loss: 0.591941773891449
Epoch 1730, training loss: 420.8091125488281 = 0.5091989040374756 + 50.0 * 8.405998229980469
Epoch 1730, val loss: 0.5918942093849182
Epoch 1740, training loss: 420.7525939941406 = 0.5085248947143555 + 50.0 * 8.404881477355957
Epoch 1740, val loss: 0.5914759039878845
Epoch 1750, training loss: 420.7338562011719 = 0.5078580379486084 + 50.0 * 8.404520034790039
Epoch 1750, val loss: 0.5912598967552185
Epoch 1760, training loss: 421.0115966796875 = 0.5071784853935242 + 50.0 * 8.410088539123535
Epoch 1760, val loss: 0.5910229086875916
Epoch 1770, training loss: 420.7826232910156 = 0.506472647190094 + 50.0 * 8.405523300170898
Epoch 1770, val loss: 0.5905731320381165
Epoch 1780, training loss: 420.6896057128906 = 0.5057768821716309 + 50.0 * 8.403676986694336
Epoch 1780, val loss: 0.5903675556182861
Epoch 1790, training loss: 420.673583984375 = 0.5050970315933228 + 50.0 * 8.403369903564453
Epoch 1790, val loss: 0.5900282263755798
Epoch 1800, training loss: 420.6708679199219 = 0.5044119358062744 + 50.0 * 8.403328895568848
Epoch 1800, val loss: 0.5897950530052185
Epoch 1810, training loss: 421.2012939453125 = 0.5037031173706055 + 50.0 * 8.413951873779297
Epoch 1810, val loss: 0.5894927978515625
Epoch 1820, training loss: 420.7794189453125 = 0.502956748008728 + 50.0 * 8.405529022216797
Epoch 1820, val loss: 0.5890635848045349
Epoch 1830, training loss: 420.6221008300781 = 0.5022360682487488 + 50.0 * 8.402397155761719
Epoch 1830, val loss: 0.5887101888656616
Epoch 1840, training loss: 420.603515625 = 0.5015292763710022 + 50.0 * 8.402039527893066
Epoch 1840, val loss: 0.5884189605712891
Epoch 1850, training loss: 420.585693359375 = 0.5008310675621033 + 50.0 * 8.401697158813477
Epoch 1850, val loss: 0.5880882143974304
Epoch 1860, training loss: 420.5860290527344 = 0.5001196265220642 + 50.0 * 8.401718139648438
Epoch 1860, val loss: 0.587824285030365
Epoch 1870, training loss: 420.823486328125 = 0.4993889629840851 + 50.0 * 8.406481742858887
Epoch 1870, val loss: 0.5875332355499268
Epoch 1880, training loss: 420.61669921875 = 0.4986419677734375 + 50.0 * 8.402360916137695
Epoch 1880, val loss: 0.5870516300201416
Epoch 1890, training loss: 420.55474853515625 = 0.4978913366794586 + 50.0 * 8.401137351989746
Epoch 1890, val loss: 0.5868117213249207
Epoch 1900, training loss: 420.52288818359375 = 0.497160941362381 + 50.0 * 8.400514602661133
Epoch 1900, val loss: 0.5864300727844238
Epoch 1910, training loss: 420.51324462890625 = 0.4964226484298706 + 50.0 * 8.400336265563965
Epoch 1910, val loss: 0.5861263871192932
Epoch 1920, training loss: 420.8851623535156 = 0.49567723274230957 + 50.0 * 8.407790184020996
Epoch 1920, val loss: 0.5858823657035828
Epoch 1930, training loss: 420.54052734375 = 0.49486449360847473 + 50.0 * 8.40091323852539
Epoch 1930, val loss: 0.5853605270385742
Epoch 1940, training loss: 420.48724365234375 = 0.49408718943595886 + 50.0 * 8.399863243103027
Epoch 1940, val loss: 0.5851023197174072
Epoch 1950, training loss: 420.4664001464844 = 0.49332645535469055 + 50.0 * 8.39946174621582
Epoch 1950, val loss: 0.5847102999687195
Epoch 1960, training loss: 420.4441223144531 = 0.4925599694252014 + 50.0 * 8.399031639099121
Epoch 1960, val loss: 0.5843764543533325
Epoch 1970, training loss: 420.53936767578125 = 0.49178624153137207 + 50.0 * 8.400951385498047
Epoch 1970, val loss: 0.5840362310409546
Epoch 1980, training loss: 420.4820251464844 = 0.49097901582717896 + 50.0 * 8.399821281433105
Epoch 1980, val loss: 0.5836670398712158
Epoch 1990, training loss: 420.4373779296875 = 0.49017003178596497 + 50.0 * 8.398943901062012
Epoch 1990, val loss: 0.5832346677780151
Epoch 2000, training loss: 420.41033935546875 = 0.4893774092197418 + 50.0 * 8.398419380187988
Epoch 2000, val loss: 0.5829053521156311
Epoch 2010, training loss: 420.39129638671875 = 0.48859062790870667 + 50.0 * 8.398054122924805
Epoch 2010, val loss: 0.5825222134590149
Epoch 2020, training loss: 420.3839111328125 = 0.487798273563385 + 50.0 * 8.39792251586914
Epoch 2020, val loss: 0.5821186304092407
Epoch 2030, training loss: 420.82568359375 = 0.4869985282421112 + 50.0 * 8.406773567199707
Epoch 2030, val loss: 0.5816507339477539
Epoch 2040, training loss: 420.42022705078125 = 0.48613011837005615 + 50.0 * 8.398681640625
Epoch 2040, val loss: 0.5814353227615356
Epoch 2050, training loss: 420.3582763671875 = 0.48529744148254395 + 50.0 * 8.397459983825684
Epoch 2050, val loss: 0.5810391306877136
Epoch 2060, training loss: 420.34033203125 = 0.4844931662082672 + 50.0 * 8.397116661071777
Epoch 2060, val loss: 0.5806093215942383
Epoch 2070, training loss: 420.3250427246094 = 0.4836781919002533 + 50.0 * 8.396827697753906
Epoch 2070, val loss: 0.580239474773407
Epoch 2080, training loss: 420.52197265625 = 0.4828563630580902 + 50.0 * 8.400782585144043
Epoch 2080, val loss: 0.5797653794288635
Epoch 2090, training loss: 420.35504150390625 = 0.4819871485233307 + 50.0 * 8.3974609375
Epoch 2090, val loss: 0.5795727372169495
Epoch 2100, training loss: 420.3946838378906 = 0.4811321198940277 + 50.0 * 8.398270606994629
Epoch 2100, val loss: 0.5790771245956421
Epoch 2110, training loss: 420.3851013183594 = 0.48026204109191895 + 50.0 * 8.398097038269043
Epoch 2110, val loss: 0.5787462592124939
Epoch 2120, training loss: 420.3105773925781 = 0.47939857840538025 + 50.0 * 8.396623611450195
Epoch 2120, val loss: 0.5782561302185059
Epoch 2130, training loss: 420.2552795410156 = 0.4785430133342743 + 50.0 * 8.39553451538086
Epoch 2130, val loss: 0.5779152512550354
Epoch 2140, training loss: 420.2533264160156 = 0.47768843173980713 + 50.0 * 8.395512580871582
Epoch 2140, val loss: 0.577551543712616
Epoch 2150, training loss: 420.25213623046875 = 0.4768260717391968 + 50.0 * 8.395505905151367
Epoch 2150, val loss: 0.5771467089653015
Epoch 2160, training loss: 420.46453857421875 = 0.47595104575157166 + 50.0 * 8.399771690368652
Epoch 2160, val loss: 0.5767508149147034
Epoch 2170, training loss: 420.33526611328125 = 0.4750259518623352 + 50.0 * 8.397204399108887
Epoch 2170, val loss: 0.5762830972671509
Epoch 2180, training loss: 420.23358154296875 = 0.4741228222846985 + 50.0 * 8.39518928527832
Epoch 2180, val loss: 0.5759188532829285
Epoch 2190, training loss: 420.2100524902344 = 0.47322115302085876 + 50.0 * 8.394736289978027
Epoch 2190, val loss: 0.5755040049552917
Epoch 2200, training loss: 420.21624755859375 = 0.47231894731521606 + 50.0 * 8.394878387451172
Epoch 2200, val loss: 0.5751126408576965
Epoch 2210, training loss: 420.3550720214844 = 0.4713968336582184 + 50.0 * 8.397673606872559
Epoch 2210, val loss: 0.5746623873710632
Epoch 2220, training loss: 420.1636962890625 = 0.47045233845710754 + 50.0 * 8.393864631652832
Epoch 2220, val loss: 0.5743445754051208
Epoch 2230, training loss: 420.17486572265625 = 0.4695090353488922 + 50.0 * 8.3941068649292
Epoch 2230, val loss: 0.5739534497261047
Epoch 2240, training loss: 420.2478332519531 = 0.46856367588043213 + 50.0 * 8.395585060119629
Epoch 2240, val loss: 0.5736365914344788
Epoch 2250, training loss: 420.20721435546875 = 0.46757611632347107 + 50.0 * 8.394792556762695
Epoch 2250, val loss: 0.5729564428329468
Epoch 2260, training loss: 420.1606750488281 = 0.4665847420692444 + 50.0 * 8.393881797790527
Epoch 2260, val loss: 0.5727566480636597
Epoch 2270, training loss: 420.1451416015625 = 0.4656165540218353 + 50.0 * 8.393590927124023
Epoch 2270, val loss: 0.5720887184143066
Epoch 2280, training loss: 420.0973205566406 = 0.46465572714805603 + 50.0 * 8.392653465270996
Epoch 2280, val loss: 0.5718039870262146
Epoch 2290, training loss: 420.0841064453125 = 0.46369144320487976 + 50.0 * 8.39240837097168
Epoch 2290, val loss: 0.5713958740234375
Epoch 2300, training loss: 420.1178894042969 = 0.4627162218093872 + 50.0 * 8.39310359954834
Epoch 2300, val loss: 0.5709830522537231
Epoch 2310, training loss: 420.31719970703125 = 0.4617116451263428 + 50.0 * 8.397109985351562
Epoch 2310, val loss: 0.5706039667129517
Epoch 2320, training loss: 420.196044921875 = 0.46069037914276123 + 50.0 * 8.394706726074219
Epoch 2320, val loss: 0.5700578689575195
Epoch 2330, training loss: 420.1261291503906 = 0.4596690535545349 + 50.0 * 8.393329620361328
Epoch 2330, val loss: 0.5697343945503235
Epoch 2340, training loss: 420.04296875 = 0.4586625397205353 + 50.0 * 8.39168643951416
Epoch 2340, val loss: 0.5693075060844421
Epoch 2350, training loss: 420.05291748046875 = 0.45766642689704895 + 50.0 * 8.391904830932617
Epoch 2350, val loss: 0.5690008401870728
Epoch 2360, training loss: 420.0525207519531 = 0.45666036009788513 + 50.0 * 8.39191722869873
Epoch 2360, val loss: 0.5685977935791016
Epoch 2370, training loss: 420.0893249511719 = 0.4556454122066498 + 50.0 * 8.39267349243164
Epoch 2370, val loss: 0.5682308673858643
Epoch 2380, training loss: 420.0350646972656 = 0.45461148023605347 + 50.0 * 8.391609191894531
Epoch 2380, val loss: 0.5677494406700134
Epoch 2390, training loss: 419.9856872558594 = 0.45358437299728394 + 50.0 * 8.390642166137695
Epoch 2390, val loss: 0.5673623085021973
Epoch 2400, training loss: 420.17529296875 = 0.4525589644908905 + 50.0 * 8.394454956054688
Epoch 2400, val loss: 0.566989004611969
Epoch 2410, training loss: 420.01361083984375 = 0.4515012800693512 + 50.0 * 8.391242027282715
Epoch 2410, val loss: 0.566483199596405
Epoch 2420, training loss: 420.0224609375 = 0.4504493176937103 + 50.0 * 8.391440391540527
Epoch 2420, val loss: 0.5661276578903198
Epoch 2430, training loss: 420.01812744140625 = 0.4493933320045471 + 50.0 * 8.391374588012695
Epoch 2430, val loss: 0.5656846165657043
Epoch 2440, training loss: 420.0090637207031 = 0.44833511114120483 + 50.0 * 8.391214370727539
Epoch 2440, val loss: 0.5654045343399048
Epoch 2450, training loss: 420.17529296875 = 0.44726255536079407 + 50.0 * 8.394560813903809
Epoch 2450, val loss: 0.5648488402366638
Epoch 2460, training loss: 419.997314453125 = 0.4461706578731537 + 50.0 * 8.391022682189941
Epoch 2460, val loss: 0.5645729899406433
Epoch 2470, training loss: 419.92718505859375 = 0.44509679079055786 + 50.0 * 8.389641761779785
Epoch 2470, val loss: 0.5642235279083252
Epoch 2480, training loss: 419.8988037109375 = 0.44403451681137085 + 50.0 * 8.389095306396484
Epoch 2480, val loss: 0.5638303160667419
Epoch 2490, training loss: 419.90484619140625 = 0.442962110042572 + 50.0 * 8.389237403869629
Epoch 2490, val loss: 0.5635287165641785
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7879249112125823
0.8161993769470406
=== training gcn model ===
Epoch 0, training loss: 530.2276000976562 = 1.1135176420211792 + 50.0 * 10.582282066345215
Epoch 0, val loss: 1.113504409790039
Epoch 10, training loss: 530.2037353515625 = 1.10868501663208 + 50.0 * 10.581900596618652
Epoch 10, val loss: 1.1086323261260986
Epoch 20, training loss: 530.10888671875 = 1.1034321784973145 + 50.0 * 10.580108642578125
Epoch 20, val loss: 1.1033480167388916
Epoch 30, training loss: 529.6896362304688 = 1.0978977680206299 + 50.0 * 10.571834564208984
Epoch 30, val loss: 1.0977742671966553
Epoch 40, training loss: 528.1333618164062 = 1.0918748378753662 + 50.0 * 10.5408296585083
Epoch 40, val loss: 1.0916239023208618
Epoch 50, training loss: 524.11279296875 = 1.0849024057388306 + 50.0 * 10.46055793762207
Epoch 50, val loss: 1.0844509601593018
Epoch 60, training loss: 516.2517700195312 = 1.0778402090072632 + 50.0 * 10.303479194641113
Epoch 60, val loss: 1.077306866645813
Epoch 70, training loss: 506.1399841308594 = 1.0708191394805908 + 50.0 * 10.101383209228516
Epoch 70, val loss: 1.0700434446334839
Epoch 80, training loss: 498.6905822753906 = 1.0644137859344482 + 50.0 * 9.952523231506348
Epoch 80, val loss: 1.0635278224945068
Epoch 90, training loss: 486.1543884277344 = 1.0592788457870483 + 50.0 * 9.701902389526367
Epoch 90, val loss: 1.0584514141082764
Epoch 100, training loss: 479.4556579589844 = 1.0555226802825928 + 50.0 * 9.568002700805664
Epoch 100, val loss: 1.0547895431518555
Epoch 110, training loss: 477.28619384765625 = 1.0523797273635864 + 50.0 * 9.524676322937012
Epoch 110, val loss: 1.0517703294754028
Epoch 120, training loss: 473.30322265625 = 1.049718976020813 + 50.0 * 9.445070266723633
Epoch 120, val loss: 1.049155831336975
Epoch 130, training loss: 465.40679931640625 = 1.0470367670059204 + 50.0 * 9.287195205688477
Epoch 130, val loss: 1.0465986728668213
Epoch 140, training loss: 456.54180908203125 = 1.044922113418579 + 50.0 * 9.10993766784668
Epoch 140, val loss: 1.044682264328003
Epoch 150, training loss: 451.9025573730469 = 1.0431252717971802 + 50.0 * 9.017189025878906
Epoch 150, val loss: 1.0429245233535767
Epoch 160, training loss: 449.3160400390625 = 1.0408904552459717 + 50.0 * 8.965502738952637
Epoch 160, val loss: 1.0406591892242432
Epoch 170, training loss: 446.4035339355469 = 1.0381250381469727 + 50.0 * 8.907308578491211
Epoch 170, val loss: 1.0377339124679565
Epoch 180, training loss: 443.82257080078125 = 1.0354125499725342 + 50.0 * 8.855743408203125
Epoch 180, val loss: 1.0350030660629272
Epoch 190, training loss: 442.0057678222656 = 1.0327773094177246 + 50.0 * 8.819459915161133
Epoch 190, val loss: 1.0323820114135742
Epoch 200, training loss: 440.237548828125 = 1.0301573276519775 + 50.0 * 8.784148216247559
Epoch 200, val loss: 1.0298006534576416
Epoch 210, training loss: 438.5902099609375 = 1.0275986194610596 + 50.0 * 8.751252174377441
Epoch 210, val loss: 1.0272784233093262
Epoch 220, training loss: 437.24530029296875 = 1.0248091220855713 + 50.0 * 8.724410057067871
Epoch 220, val loss: 1.0245429277420044
Epoch 230, training loss: 436.29473876953125 = 1.0216130018234253 + 50.0 * 8.705462455749512
Epoch 230, val loss: 1.0213972330093384
Epoch 240, training loss: 435.5400085449219 = 1.0179924964904785 + 50.0 * 8.69044017791748
Epoch 240, val loss: 1.0178420543670654
Epoch 250, training loss: 434.91778564453125 = 1.014042615890503 + 50.0 * 8.678074836730957
Epoch 250, val loss: 1.0139720439910889
Epoch 260, training loss: 434.2950744628906 = 1.0097951889038086 + 50.0 * 8.665705680847168
Epoch 260, val loss: 1.0098134279251099
Epoch 270, training loss: 433.7378234863281 = 1.0052599906921387 + 50.0 * 8.654651641845703
Epoch 270, val loss: 1.0053963661193848
Epoch 280, training loss: 433.23687744140625 = 1.0003952980041504 + 50.0 * 8.644729614257812
Epoch 280, val loss: 1.0006555318832397
Epoch 290, training loss: 432.7630920410156 = 0.9951449632644653 + 50.0 * 8.635358810424805
Epoch 290, val loss: 0.9955422878265381
Epoch 300, training loss: 432.3417053222656 = 0.9894713163375854 + 50.0 * 8.627044677734375
Epoch 300, val loss: 0.9900123476982117
Epoch 310, training loss: 431.91046142578125 = 0.983380138874054 + 50.0 * 8.618541717529297
Epoch 310, val loss: 0.9841505289077759
Epoch 320, training loss: 431.5414123535156 = 0.9769628047943115 + 50.0 * 8.611289024353027
Epoch 320, val loss: 0.9779302477836609
Epoch 330, training loss: 431.14837646484375 = 0.9701451659202576 + 50.0 * 8.603564262390137
Epoch 330, val loss: 0.9713506698608398
Epoch 340, training loss: 430.8082275390625 = 0.9629484415054321 + 50.0 * 8.596905708312988
Epoch 340, val loss: 0.9643955230712891
Epoch 350, training loss: 430.4971618652344 = 0.9553884267807007 + 50.0 * 8.590835571289062
Epoch 350, val loss: 0.9571148753166199
Epoch 360, training loss: 430.3983154296875 = 0.9474958777427673 + 50.0 * 8.58901596069336
Epoch 360, val loss: 0.9495056867599487
Epoch 370, training loss: 429.93035888671875 = 0.939258873462677 + 50.0 * 8.579821586608887
Epoch 370, val loss: 0.9416254758834839
Epoch 380, training loss: 429.66827392578125 = 0.9308675527572632 + 50.0 * 8.574748039245605
Epoch 380, val loss: 0.9336041808128357
Epoch 390, training loss: 429.3577880859375 = 0.9223491549491882 + 50.0 * 8.568708419799805
Epoch 390, val loss: 0.9254598617553711
Epoch 400, training loss: 429.08746337890625 = 0.913709282875061 + 50.0 * 8.563475608825684
Epoch 400, val loss: 0.9172132611274719
Epoch 410, training loss: 429.09716796875 = 0.9048681855201721 + 50.0 * 8.56384563446045
Epoch 410, val loss: 0.9087967872619629
Epoch 420, training loss: 428.6554260253906 = 0.8959649205207825 + 50.0 * 8.55518913269043
Epoch 420, val loss: 0.9003024697303772
Epoch 430, training loss: 428.4084777832031 = 0.8870288133621216 + 50.0 * 8.550429344177246
Epoch 430, val loss: 0.891855001449585
Epoch 440, training loss: 428.1993408203125 = 0.8780590891838074 + 50.0 * 8.546425819396973
Epoch 440, val loss: 0.8833411335945129
Epoch 450, training loss: 428.0306091308594 = 0.8690634369850159 + 50.0 * 8.543231010437012
Epoch 450, val loss: 0.8748183250427246
Epoch 460, training loss: 427.85418701171875 = 0.8600239157676697 + 50.0 * 8.539883613586426
Epoch 460, val loss: 0.8662669658660889
Epoch 470, training loss: 427.71417236328125 = 0.8509860634803772 + 50.0 * 8.537263870239258
Epoch 470, val loss: 0.8577315807342529
Epoch 480, training loss: 427.5298767089844 = 0.8420119881629944 + 50.0 * 8.533757209777832
Epoch 480, val loss: 0.8492447137832642
Epoch 490, training loss: 427.4637756347656 = 0.8330448865890503 + 50.0 * 8.532614707946777
Epoch 490, val loss: 0.8407962918281555
Epoch 500, training loss: 427.2236022949219 = 0.8241186738014221 + 50.0 * 8.527989387512207
Epoch 500, val loss: 0.8323479294776917
Epoch 510, training loss: 427.06134033203125 = 0.8152847290039062 + 50.0 * 8.524921417236328
Epoch 510, val loss: 0.8239902257919312
Epoch 520, training loss: 426.915283203125 = 0.8065416812896729 + 50.0 * 8.522174835205078
Epoch 520, val loss: 0.8157183527946472
Epoch 530, training loss: 426.82061767578125 = 0.797839343547821 + 50.0 * 8.520455360412598
Epoch 530, val loss: 0.8074923157691956
Epoch 540, training loss: 426.605712890625 = 0.7891939282417297 + 50.0 * 8.51633071899414
Epoch 540, val loss: 0.7993325591087341
Epoch 550, training loss: 426.43170166015625 = 0.7806483507156372 + 50.0 * 8.513021469116211
Epoch 550, val loss: 0.7912757396697998
Epoch 560, training loss: 426.50341796875 = 0.7721498012542725 + 50.0 * 8.514625549316406
Epoch 560, val loss: 0.7832502126693726
Epoch 570, training loss: 426.131103515625 = 0.763708770275116 + 50.0 * 8.50734806060791
Epoch 570, val loss: 0.7752766013145447
Epoch 580, training loss: 425.9696044921875 = 0.7554957270622253 + 50.0 * 8.504281997680664
Epoch 580, val loss: 0.7674945592880249
Epoch 590, training loss: 425.8819885253906 = 0.7474396228790283 + 50.0 * 8.502691268920898
Epoch 590, val loss: 0.7598416805267334
Epoch 600, training loss: 425.70037841796875 = 0.7394359111785889 + 50.0 * 8.499218940734863
Epoch 600, val loss: 0.7523386478424072
Epoch 610, training loss: 425.5956726074219 = 0.7315735816955566 + 50.0 * 8.497282028198242
Epoch 610, val loss: 0.7448751926422119
Epoch 620, training loss: 425.4696350097656 = 0.7238854765892029 + 50.0 * 8.494915008544922
Epoch 620, val loss: 0.7377048134803772
Epoch 630, training loss: 425.3622131347656 = 0.7164582014083862 + 50.0 * 8.492915153503418
Epoch 630, val loss: 0.7306889891624451
Epoch 640, training loss: 425.3734436035156 = 0.7091836333274841 + 50.0 * 8.493285179138184
Epoch 640, val loss: 0.7238734364509583
Epoch 650, training loss: 425.21197509765625 = 0.7021145820617676 + 50.0 * 8.49019718170166
Epoch 650, val loss: 0.7172945737838745
Epoch 660, training loss: 425.079833984375 = 0.6953502297401428 + 50.0 * 8.487689971923828
Epoch 660, val loss: 0.7110407948493958
Epoch 670, training loss: 424.9886474609375 = 0.688883900642395 + 50.0 * 8.485995292663574
Epoch 670, val loss: 0.7050597071647644
Epoch 680, training loss: 424.9021301269531 = 0.68270343542099 + 50.0 * 8.48438835144043
Epoch 680, val loss: 0.6993958353996277
Epoch 690, training loss: 425.3370056152344 = 0.6767433285713196 + 50.0 * 8.493205070495605
Epoch 690, val loss: 0.6940421462059021
Epoch 700, training loss: 424.88726806640625 = 0.6709727644920349 + 50.0 * 8.484326362609863
Epoch 700, val loss: 0.6887794137001038
Epoch 710, training loss: 424.6853332519531 = 0.6656133532524109 + 50.0 * 8.48039436340332
Epoch 710, val loss: 0.6839067935943604
Epoch 720, training loss: 424.5560607910156 = 0.6606014966964722 + 50.0 * 8.477909088134766
Epoch 720, val loss: 0.67939692735672
Epoch 730, training loss: 424.4579772949219 = 0.655850350856781 + 50.0 * 8.476042747497559
Epoch 730, val loss: 0.6751706600189209
Epoch 740, training loss: 424.4702453613281 = 0.6513341069221497 + 50.0 * 8.476378440856934
Epoch 740, val loss: 0.6711873412132263
Epoch 750, training loss: 424.4439697265625 = 0.6469630002975464 + 50.0 * 8.475939750671387
Epoch 750, val loss: 0.6673622131347656
Epoch 760, training loss: 424.1944885253906 = 0.6428903341293335 + 50.0 * 8.47103214263916
Epoch 760, val loss: 0.663851261138916
Epoch 770, training loss: 424.08111572265625 = 0.6390364170074463 + 50.0 * 8.468841552734375
Epoch 770, val loss: 0.660536527633667
Epoch 780, training loss: 424.0283508300781 = 0.6353774070739746 + 50.0 * 8.467859268188477
Epoch 780, val loss: 0.6574407815933228
Epoch 790, training loss: 424.0028076171875 = 0.6318488717079163 + 50.0 * 8.467419624328613
Epoch 790, val loss: 0.6544163227081299
Epoch 800, training loss: 423.85357666015625 = 0.6285120844841003 + 50.0 * 8.46450138092041
Epoch 800, val loss: 0.6516892313957214
Epoch 810, training loss: 423.74639892578125 = 0.6253703236579895 + 50.0 * 8.462420463562012
Epoch 810, val loss: 0.6491307020187378
Epoch 820, training loss: 423.6488342285156 = 0.6223798990249634 + 50.0 * 8.460529327392578
Epoch 820, val loss: 0.6466783881187439
Epoch 830, training loss: 423.5654296875 = 0.6195332407951355 + 50.0 * 8.458917617797852
Epoch 830, val loss: 0.6444149613380432
Epoch 840, training loss: 424.04150390625 = 0.6167938113212585 + 50.0 * 8.468494415283203
Epoch 840, val loss: 0.6421692371368408
Epoch 850, training loss: 423.56695556640625 = 0.6141001582145691 + 50.0 * 8.459056854248047
Epoch 850, val loss: 0.6402063369750977
Epoch 860, training loss: 423.4070739746094 = 0.6116156578063965 + 50.0 * 8.455909729003906
Epoch 860, val loss: 0.6382487416267395
Epoch 870, training loss: 423.29779052734375 = 0.6092580556869507 + 50.0 * 8.453770637512207
Epoch 870, val loss: 0.63648521900177
Epoch 880, training loss: 423.2186584472656 = 0.6069939732551575 + 50.0 * 8.45223331451416
Epoch 880, val loss: 0.6347922086715698
Epoch 890, training loss: 423.1746826171875 = 0.6048243641853333 + 50.0 * 8.451396942138672
Epoch 890, val loss: 0.6331800818443298
Epoch 900, training loss: 423.1804504394531 = 0.6027018427848816 + 50.0 * 8.451555252075195
Epoch 900, val loss: 0.6317013502120972
Epoch 910, training loss: 423.16705322265625 = 0.6006588339805603 + 50.0 * 8.45132827758789
Epoch 910, val loss: 0.630180835723877
Epoch 920, training loss: 422.9971923828125 = 0.5987259745597839 + 50.0 * 8.447969436645508
Epoch 920, val loss: 0.6287404298782349
Epoch 930, training loss: 422.92041015625 = 0.5968801379203796 + 50.0 * 8.446470260620117
Epoch 930, val loss: 0.6274924874305725
Epoch 940, training loss: 422.86328125 = 0.5951040983200073 + 50.0 * 8.445363998413086
Epoch 940, val loss: 0.6262818574905396
Epoch 950, training loss: 423.1319885253906 = 0.5933873653411865 + 50.0 * 8.450772285461426
Epoch 950, val loss: 0.6249655485153198
Epoch 960, training loss: 422.86651611328125 = 0.5916767120361328 + 50.0 * 8.445496559143066
Epoch 960, val loss: 0.6240674257278442
Epoch 970, training loss: 422.6940002441406 = 0.5900606513023376 + 50.0 * 8.442078590393066
Epoch 970, val loss: 0.6229737997055054
Epoch 980, training loss: 422.6532287597656 = 0.5885176658630371 + 50.0 * 8.441293716430664
Epoch 980, val loss: 0.6218981146812439
Epoch 990, training loss: 422.5911560058594 = 0.587030291557312 + 50.0 * 8.440082550048828
Epoch 990, val loss: 0.6210219860076904
Epoch 1000, training loss: 422.66009521484375 = 0.5855849981307983 + 50.0 * 8.441490173339844
Epoch 1000, val loss: 0.6201695799827576
Epoch 1010, training loss: 422.6507263183594 = 0.5841450691223145 + 50.0 * 8.44133186340332
Epoch 1010, val loss: 0.6190688014030457
Epoch 1020, training loss: 422.4873352050781 = 0.5827600955963135 + 50.0 * 8.438091278076172
Epoch 1020, val loss: 0.6183280944824219
Epoch 1030, training loss: 422.40576171875 = 0.5814529657363892 + 50.0 * 8.43648624420166
Epoch 1030, val loss: 0.6174551844596863
Epoch 1040, training loss: 422.350830078125 = 0.5801903605461121 + 50.0 * 8.435412406921387
Epoch 1040, val loss: 0.616776704788208
Epoch 1050, training loss: 422.38995361328125 = 0.5789658427238464 + 50.0 * 8.436219215393066
Epoch 1050, val loss: 0.6160431504249573
Epoch 1060, training loss: 422.2803955078125 = 0.5777260661125183 + 50.0 * 8.434053421020508
Epoch 1060, val loss: 0.6153286695480347
Epoch 1070, training loss: 422.2743225097656 = 0.5765208601951599 + 50.0 * 8.433956146240234
Epoch 1070, val loss: 0.6145063042640686
Epoch 1080, training loss: 422.18798828125 = 0.5753452777862549 + 50.0 * 8.432252883911133
Epoch 1080, val loss: 0.6139062643051147
Epoch 1090, training loss: 422.3754577636719 = 0.5742042660713196 + 50.0 * 8.43602466583252
Epoch 1090, val loss: 0.6133348941802979
Epoch 1100, training loss: 422.2651062011719 = 0.5730565190315247 + 50.0 * 8.43384075164795
Epoch 1100, val loss: 0.6124365329742432
Epoch 1110, training loss: 422.0657653808594 = 0.5719463229179382 + 50.0 * 8.429876327514648
Epoch 1110, val loss: 0.6119226813316345
Epoch 1120, training loss: 422.0399475097656 = 0.5708795189857483 + 50.0 * 8.429381370544434
Epoch 1120, val loss: 0.6113774180412292
Epoch 1130, training loss: 421.99957275390625 = 0.5698465704917908 + 50.0 * 8.428594589233398
Epoch 1130, val loss: 0.6108039021492004
Epoch 1140, training loss: 422.4908447265625 = 0.5688260793685913 + 50.0 * 8.438440322875977
Epoch 1140, val loss: 0.6104019284248352
Epoch 1150, training loss: 421.9271545410156 = 0.5677564740180969 + 50.0 * 8.4271879196167
Epoch 1150, val loss: 0.6096341013908386
Epoch 1160, training loss: 421.9325256347656 = 0.5667526721954346 + 50.0 * 8.427315711975098
Epoch 1160, val loss: 0.6089829206466675
Epoch 1170, training loss: 421.8767395019531 = 0.5657822489738464 + 50.0 * 8.42621898651123
Epoch 1170, val loss: 0.6084145307540894
Epoch 1180, training loss: 421.82855224609375 = 0.5648442506790161 + 50.0 * 8.425273895263672
Epoch 1180, val loss: 0.6079464554786682
Epoch 1190, training loss: 421.80059814453125 = 0.5639219880104065 + 50.0 * 8.42473316192627
Epoch 1190, val loss: 0.6074318289756775
Epoch 1200, training loss: 421.8348083496094 = 0.5630079507827759 + 50.0 * 8.425436019897461
Epoch 1200, val loss: 0.6068401336669922
Epoch 1210, training loss: 421.7541809082031 = 0.5620675086975098 + 50.0 * 8.423842430114746
Epoch 1210, val loss: 0.6063375473022461
Epoch 1220, training loss: 421.7940368652344 = 0.5611515641212463 + 50.0 * 8.424657821655273
Epoch 1220, val loss: 0.6057559847831726
Epoch 1230, training loss: 421.75482177734375 = 0.5602658987045288 + 50.0 * 8.423891067504883
Epoch 1230, val loss: 0.6052852869033813
Epoch 1240, training loss: 421.68231201171875 = 0.5594056844711304 + 50.0 * 8.422457695007324
Epoch 1240, val loss: 0.6047484278678894
Epoch 1250, training loss: 421.6462707519531 = 0.5585563778877258 + 50.0 * 8.421753883361816
Epoch 1250, val loss: 0.6042714715003967
Epoch 1260, training loss: 421.7458801269531 = 0.5577143430709839 + 50.0 * 8.423763275146484
Epoch 1260, val loss: 0.6037670969963074
Epoch 1270, training loss: 421.6292419433594 = 0.5568654537200928 + 50.0 * 8.42144775390625
Epoch 1270, val loss: 0.6032959222793579
Epoch 1280, training loss: 421.74639892578125 = 0.5560324788093567 + 50.0 * 8.423807144165039
Epoch 1280, val loss: 0.6027042269706726
Epoch 1290, training loss: 421.59112548828125 = 0.5551981925964355 + 50.0 * 8.4207181930542
Epoch 1290, val loss: 0.6024404764175415
Epoch 1300, training loss: 421.5174560546875 = 0.5543985366821289 + 50.0 * 8.41926097869873
Epoch 1300, val loss: 0.6018235087394714
Epoch 1310, training loss: 421.4718017578125 = 0.5536181330680847 + 50.0 * 8.418363571166992
Epoch 1310, val loss: 0.6014379858970642
Epoch 1320, training loss: 421.4485168457031 = 0.5528515577316284 + 50.0 * 8.417913436889648
Epoch 1320, val loss: 0.6010143756866455
Epoch 1330, training loss: 421.4847717285156 = 0.5520920157432556 + 50.0 * 8.41865348815918
Epoch 1330, val loss: 0.600629985332489
Epoch 1340, training loss: 421.5907897949219 = 0.5513147711753845 + 50.0 * 8.42078971862793
Epoch 1340, val loss: 0.6001461148262024
Epoch 1350, training loss: 421.47540283203125 = 0.5505259037017822 + 50.0 * 8.418497085571289
Epoch 1350, val loss: 0.5995908379554749
Epoch 1360, training loss: 421.3979187011719 = 0.5497725605964661 + 50.0 * 8.416962623596191
Epoch 1360, val loss: 0.599120020866394
Epoch 1370, training loss: 421.3282165527344 = 0.5490532517433167 + 50.0 * 8.415583610534668
Epoch 1370, val loss: 0.5986449122428894
Epoch 1380, training loss: 421.2986145019531 = 0.5483479499816895 + 50.0 * 8.415005683898926
Epoch 1380, val loss: 0.5982707738876343
Epoch 1390, training loss: 421.4154052734375 = 0.5476502776145935 + 50.0 * 8.41735553741455
Epoch 1390, val loss: 0.5976936221122742
Epoch 1400, training loss: 421.3106384277344 = 0.5469166040420532 + 50.0 * 8.415274620056152
Epoch 1400, val loss: 0.5973937511444092
Epoch 1410, training loss: 421.2851257324219 = 0.5462040901184082 + 50.0 * 8.414778709411621
Epoch 1410, val loss: 0.5969073176383972
Epoch 1420, training loss: 421.1930236816406 = 0.5455182790756226 + 50.0 * 8.412949562072754
Epoch 1420, val loss: 0.5964927673339844
Epoch 1430, training loss: 421.181396484375 = 0.5448447465896606 + 50.0 * 8.412731170654297
Epoch 1430, val loss: 0.5960521697998047
Epoch 1440, training loss: 421.48004150390625 = 0.5441646575927734 + 50.0 * 8.418717384338379
Epoch 1440, val loss: 0.5955803990364075
Epoch 1450, training loss: 421.2370910644531 = 0.5434533953666687 + 50.0 * 8.413872718811035
Epoch 1450, val loss: 0.5953080058097839
Epoch 1460, training loss: 421.1231384277344 = 0.5427721738815308 + 50.0 * 8.411606788635254
Epoch 1460, val loss: 0.5947782397270203
Epoch 1470, training loss: 421.0731506347656 = 0.5421118140220642 + 50.0 * 8.41062068939209
Epoch 1470, val loss: 0.594419002532959
Epoch 1480, training loss: 421.10369873046875 = 0.541459858417511 + 50.0 * 8.41124439239502
Epoch 1480, val loss: 0.5940018892288208
Epoch 1490, training loss: 421.1311950683594 = 0.5407882928848267 + 50.0 * 8.411808013916016
Epoch 1490, val loss: 0.593514621257782
Epoch 1500, training loss: 421.0506286621094 = 0.5401128530502319 + 50.0 * 8.410210609436035
Epoch 1500, val loss: 0.5932441353797913
Epoch 1510, training loss: 421.0119934082031 = 0.5394614338874817 + 50.0 * 8.40945053100586
Epoch 1510, val loss: 0.5927476286888123
Epoch 1520, training loss: 420.97454833984375 = 0.5388197302818298 + 50.0 * 8.408714294433594
Epoch 1520, val loss: 0.5924451947212219
Epoch 1530, training loss: 420.9960632324219 = 0.5381813049316406 + 50.0 * 8.409157752990723
Epoch 1530, val loss: 0.5920807719230652
Epoch 1540, training loss: 421.0290222167969 = 0.5375120043754578 + 50.0 * 8.409830093383789
Epoch 1540, val loss: 0.5915671586990356
Epoch 1550, training loss: 420.93310546875 = 0.5368395447731018 + 50.0 * 8.407925605773926
Epoch 1550, val loss: 0.5911649465560913
Epoch 1560, training loss: 420.9181823730469 = 0.5361945629119873 + 50.0 * 8.407639503479004
Epoch 1560, val loss: 0.5906558036804199
Epoch 1570, training loss: 420.87939453125 = 0.5355576276779175 + 50.0 * 8.406876564025879
Epoch 1570, val loss: 0.5903018116950989
Epoch 1580, training loss: 420.87615966796875 = 0.5349289774894714 + 50.0 * 8.406824111938477
Epoch 1580, val loss: 0.5899499654769897
Epoch 1590, training loss: 421.2539367675781 = 0.5342859029769897 + 50.0 * 8.414393424987793
Epoch 1590, val loss: 0.5896536707878113
Epoch 1600, training loss: 420.9505310058594 = 0.5335979461669922 + 50.0 * 8.40833854675293
Epoch 1600, val loss: 0.5889047384262085
Epoch 1610, training loss: 420.8490905761719 = 0.5329427123069763 + 50.0 * 8.406323432922363
Epoch 1610, val loss: 0.5885722637176514
Epoch 1620, training loss: 420.8007507324219 = 0.5323098301887512 + 50.0 * 8.40536880493164
Epoch 1620, val loss: 0.5881906151771545
Epoch 1630, training loss: 420.788330078125 = 0.5316850543022156 + 50.0 * 8.405133247375488
Epoch 1630, val loss: 0.5877476334571838
Epoch 1640, training loss: 420.814208984375 = 0.531062662601471 + 50.0 * 8.405662536621094
Epoch 1640, val loss: 0.5873803496360779
Epoch 1650, training loss: 420.78662109375 = 0.5304267406463623 + 50.0 * 8.405123710632324
Epoch 1650, val loss: 0.5869266390800476
Epoch 1660, training loss: 420.8486633300781 = 0.5297889709472656 + 50.0 * 8.406377792358398
Epoch 1660, val loss: 0.5864993929862976
Epoch 1670, training loss: 420.7502746582031 = 0.5291366577148438 + 50.0 * 8.404422760009766
Epoch 1670, val loss: 0.5860781669616699
Epoch 1680, training loss: 420.7628479003906 = 0.5284980535507202 + 50.0 * 8.40468692779541
Epoch 1680, val loss: 0.5855701565742493
Epoch 1690, training loss: 420.82373046875 = 0.5278518199920654 + 50.0 * 8.405917167663574
Epoch 1690, val loss: 0.5852038264274597
Epoch 1700, training loss: 420.7037048339844 = 0.527197003364563 + 50.0 * 8.40353012084961
Epoch 1700, val loss: 0.584820032119751
Epoch 1710, training loss: 420.67230224609375 = 0.5265560746192932 + 50.0 * 8.402915000915527
Epoch 1710, val loss: 0.5843635201454163
Epoch 1720, training loss: 420.6591796875 = 0.5259183645248413 + 50.0 * 8.402665138244629
Epoch 1720, val loss: 0.583970844745636
Epoch 1730, training loss: 420.70147705078125 = 0.525280773639679 + 50.0 * 8.403524398803711
Epoch 1730, val loss: 0.5834641456604004
Epoch 1740, training loss: 420.823974609375 = 0.524615466594696 + 50.0 * 8.405986785888672
Epoch 1740, val loss: 0.5830141305923462
Epoch 1750, training loss: 420.6233215332031 = 0.5239267945289612 + 50.0 * 8.40198802947998
Epoch 1750, val loss: 0.5827041864395142
Epoch 1760, training loss: 420.5951232910156 = 0.5232778787612915 + 50.0 * 8.401436805725098
Epoch 1760, val loss: 0.5823125243186951
Epoch 1770, training loss: 420.57537841796875 = 0.5226433873176575 + 50.0 * 8.401054382324219
Epoch 1770, val loss: 0.5818582773208618
Epoch 1780, training loss: 420.5746765136719 = 0.5220087766647339 + 50.0 * 8.401053428649902
Epoch 1780, val loss: 0.5814851522445679
Epoch 1790, training loss: 420.891845703125 = 0.5213621258735657 + 50.0 * 8.40740966796875
Epoch 1790, val loss: 0.5810404419898987
Epoch 1800, training loss: 420.58782958984375 = 0.5206747651100159 + 50.0 * 8.40134334564209
Epoch 1800, val loss: 0.5806507468223572
Epoch 1810, training loss: 420.5270080566406 = 0.520015299320221 + 50.0 * 8.400139808654785
Epoch 1810, val loss: 0.5802210569381714
Epoch 1820, training loss: 420.5423278808594 = 0.5193713307380676 + 50.0 * 8.400459289550781
Epoch 1820, val loss: 0.5798394083976746
Epoch 1830, training loss: 420.8463134765625 = 0.5187076926231384 + 50.0 * 8.4065523147583
Epoch 1830, val loss: 0.5793882608413696
Epoch 1840, training loss: 420.5706481933594 = 0.5180217623710632 + 50.0 * 8.401052474975586
Epoch 1840, val loss: 0.5789586901664734
Epoch 1850, training loss: 420.47076416015625 = 0.5173518657684326 + 50.0 * 8.399067878723145
Epoch 1850, val loss: 0.5785468816757202
Epoch 1860, training loss: 420.4502868652344 = 0.5166961550712585 + 50.0 * 8.398672103881836
Epoch 1860, val loss: 0.5781401991844177
Epoch 1870, training loss: 420.4374084472656 = 0.5160428881645203 + 50.0 * 8.39842700958252
Epoch 1870, val loss: 0.5777212977409363
Epoch 1880, training loss: 420.54425048828125 = 0.515380859375 + 50.0 * 8.400577545166016
Epoch 1880, val loss: 0.577308177947998
Epoch 1890, training loss: 420.5174255371094 = 0.514685332775116 + 50.0 * 8.400054931640625
Epoch 1890, val loss: 0.5769898295402527
Epoch 1900, training loss: 420.5144348144531 = 0.513974130153656 + 50.0 * 8.400009155273438
Epoch 1900, val loss: 0.5765053629875183
Epoch 1910, training loss: 420.3945617675781 = 0.5132907032966614 + 50.0 * 8.397624969482422
Epoch 1910, val loss: 0.5759401917457581
Epoch 1920, training loss: 420.3921203613281 = 0.5126231908798218 + 50.0 * 8.397589683532715
Epoch 1920, val loss: 0.5755438804626465
Epoch 1930, training loss: 420.4173278808594 = 0.5119525790214539 + 50.0 * 8.398107528686523
Epoch 1930, val loss: 0.5750861167907715
Epoch 1940, training loss: 420.4298400878906 = 0.5112601518630981 + 50.0 * 8.398371696472168
Epoch 1940, val loss: 0.5746651291847229
Epoch 1950, training loss: 420.3309631347656 = 0.5105608105659485 + 50.0 * 8.396408081054688
Epoch 1950, val loss: 0.5744437575340271
Epoch 1960, training loss: 420.36663818359375 = 0.5098754167556763 + 50.0 * 8.397134780883789
Epoch 1960, val loss: 0.5740869045257568
Epoch 1970, training loss: 420.3891906738281 = 0.50917649269104 + 50.0 * 8.397600173950195
Epoch 1970, val loss: 0.5735881924629211
Epoch 1980, training loss: 420.4270935058594 = 0.5084649920463562 + 50.0 * 8.398372650146484
Epoch 1980, val loss: 0.5730669498443604
Epoch 1990, training loss: 420.3077697753906 = 0.507731020450592 + 50.0 * 8.396000862121582
Epoch 1990, val loss: 0.5728554725646973
Epoch 2000, training loss: 420.34002685546875 = 0.5070151090621948 + 50.0 * 8.396659851074219
Epoch 2000, val loss: 0.5723965764045715
Epoch 2010, training loss: 420.34619140625 = 0.5062879323959351 + 50.0 * 8.396798133850098
Epoch 2010, val loss: 0.5719387531280518
Epoch 2020, training loss: 420.24676513671875 = 0.5055469274520874 + 50.0 * 8.394824028015137
Epoch 2020, val loss: 0.5717408657073975
Epoch 2030, training loss: 420.2345886230469 = 0.5048203468322754 + 50.0 * 8.3945951461792
Epoch 2030, val loss: 0.5713542699813843
Epoch 2040, training loss: 420.22674560546875 = 0.504090428352356 + 50.0 * 8.394453048706055
Epoch 2040, val loss: 0.5709495544433594
Epoch 2050, training loss: 420.26922607421875 = 0.503354549407959 + 50.0 * 8.395317077636719
Epoch 2050, val loss: 0.5705569386482239
Epoch 2060, training loss: 420.2895202636719 = 0.5025984048843384 + 50.0 * 8.39573860168457
Epoch 2060, val loss: 0.570173978805542
Epoch 2070, training loss: 420.29351806640625 = 0.5018212795257568 + 50.0 * 8.395833969116211
Epoch 2070, val loss: 0.5696830153465271
Epoch 2080, training loss: 420.1693420410156 = 0.5010468363761902 + 50.0 * 8.393365859985352
Epoch 2080, val loss: 0.5692798495292664
Epoch 2090, training loss: 420.15191650390625 = 0.5002834796905518 + 50.0 * 8.393033027648926
Epoch 2090, val loss: 0.5688527226448059
Epoch 2100, training loss: 420.14495849609375 = 0.4995286762714386 + 50.0 * 8.392908096313477
Epoch 2100, val loss: 0.568479597568512
Epoch 2110, training loss: 420.2838134765625 = 0.49877408146858215 + 50.0 * 8.395700454711914
Epoch 2110, val loss: 0.5679101943969727
Epoch 2120, training loss: 420.09356689453125 = 0.4979637563228607 + 50.0 * 8.391912460327148
Epoch 2120, val loss: 0.5677947998046875
Epoch 2130, training loss: 420.1033935546875 = 0.4971774220466614 + 50.0 * 8.39212417602539
Epoch 2130, val loss: 0.5673990249633789
Epoch 2140, training loss: 420.07403564453125 = 0.49639037251472473 + 50.0 * 8.391552925109863
Epoch 2140, val loss: 0.5669187307357788
Epoch 2150, training loss: 420.2233581542969 = 0.49559977650642395 + 50.0 * 8.39455509185791
Epoch 2150, val loss: 0.5664803981781006
Epoch 2160, training loss: 420.1163330078125 = 0.49477627873420715 + 50.0 * 8.392431259155273
Epoch 2160, val loss: 0.566219687461853
Epoch 2170, training loss: 420.0445556640625 = 0.493953675031662 + 50.0 * 8.391012191772461
Epoch 2170, val loss: 0.5657521486282349
Epoch 2180, training loss: 420.0145263671875 = 0.4931396245956421 + 50.0 * 8.390427589416504
Epoch 2180, val loss: 0.5654079914093018
Epoch 2190, training loss: 420.0613708496094 = 0.49232733249664307 + 50.0 * 8.391380310058594
Epoch 2190, val loss: 0.5651110410690308
Epoch 2200, training loss: 420.18389892578125 = 0.4914891719818115 + 50.0 * 8.393848419189453
Epoch 2200, val loss: 0.5647889971733093
Epoch 2210, training loss: 420.0499267578125 = 0.4906226694583893 + 50.0 * 8.391185760498047
Epoch 2210, val loss: 0.5640960335731506
Epoch 2220, training loss: 419.98406982421875 = 0.4897768795490265 + 50.0 * 8.389885902404785
Epoch 2220, val loss: 0.5638877749443054
Epoch 2230, training loss: 419.97198486328125 = 0.48894089460372925 + 50.0 * 8.389660835266113
Epoch 2230, val loss: 0.5634395480155945
Epoch 2240, training loss: 420.0673522949219 = 0.48809510469436646 + 50.0 * 8.391585350036621
Epoch 2240, val loss: 0.5631938576698303
Epoch 2250, training loss: 419.9947509765625 = 0.4872328042984009 + 50.0 * 8.39015007019043
Epoch 2250, val loss: 0.5627462267875671
Epoch 2260, training loss: 419.90850830078125 = 0.4863625466823578 + 50.0 * 8.388442993164062
Epoch 2260, val loss: 0.5623660087585449
Epoch 2270, training loss: 419.878662109375 = 0.4855004847049713 + 50.0 * 8.387863159179688
Epoch 2270, val loss: 0.5619199872016907
Epoch 2280, training loss: 419.8885498046875 = 0.48463886976242065 + 50.0 * 8.388077735900879
Epoch 2280, val loss: 0.5614868998527527
Epoch 2290, training loss: 420.1797180175781 = 0.483760803937912 + 50.0 * 8.393918991088867
Epoch 2290, val loss: 0.5609450936317444
Epoch 2300, training loss: 419.9444580078125 = 0.48282390832901 + 50.0 * 8.389232635498047
Epoch 2300, val loss: 0.5608099699020386
Epoch 2310, training loss: 419.8862609863281 = 0.4819144308567047 + 50.0 * 8.388087272644043
Epoch 2310, val loss: 0.5603238940238953
Epoch 2320, training loss: 419.85528564453125 = 0.4810168445110321 + 50.0 * 8.38748550415039
Epoch 2320, val loss: 0.5599797368049622
Epoch 2330, training loss: 420.06781005859375 = 0.48011189699172974 + 50.0 * 8.391754150390625
Epoch 2330, val loss: 0.5597113966941833
Epoch 2340, training loss: 419.8636779785156 = 0.4791789948940277 + 50.0 * 8.387689590454102
Epoch 2340, val loss: 0.5592541098594666
Epoch 2350, training loss: 419.804931640625 = 0.47824886441230774 + 50.0 * 8.386533737182617
Epoch 2350, val loss: 0.5587905645370483
Epoch 2360, training loss: 419.7690734863281 = 0.47732800245285034 + 50.0 * 8.385834693908691
Epoch 2360, val loss: 0.5584506392478943
Epoch 2370, training loss: 419.7537841796875 = 0.4764052629470825 + 50.0 * 8.385547637939453
Epoch 2370, val loss: 0.5581027865409851
Epoch 2380, training loss: 419.8147888183594 = 0.4754762053489685 + 50.0 * 8.386786460876465
Epoch 2380, val loss: 0.5576995611190796
Epoch 2390, training loss: 419.8792724609375 = 0.474523663520813 + 50.0 * 8.388094902038574
Epoch 2390, val loss: 0.5572351217269897
Epoch 2400, training loss: 419.8081359863281 = 0.4735390841960907 + 50.0 * 8.38669204711914
Epoch 2400, val loss: 0.5569708943367004
Epoch 2410, training loss: 419.7203674316406 = 0.47256535291671753 + 50.0 * 8.384956359863281
Epoch 2410, val loss: 0.5566491484642029
Epoch 2420, training loss: 419.70648193359375 = 0.4716068208217621 + 50.0 * 8.384696960449219
Epoch 2420, val loss: 0.5563815236091614
Epoch 2430, training loss: 419.8712463378906 = 0.47064125537872314 + 50.0 * 8.388011932373047
Epoch 2430, val loss: 0.5560879707336426
Epoch 2440, training loss: 419.6992492675781 = 0.46963638067245483 + 50.0 * 8.384592056274414
Epoch 2440, val loss: 0.5555264949798584
Epoch 2450, training loss: 419.7406311035156 = 0.46863144636154175 + 50.0 * 8.3854398727417
Epoch 2450, val loss: 0.555134117603302
Epoch 2460, training loss: 419.7641296386719 = 0.4676109254360199 + 50.0 * 8.385930061340332
Epoch 2460, val loss: 0.55475252866745
Epoch 2470, training loss: 419.6741638183594 = 0.46656832098960876 + 50.0 * 8.384151458740234
Epoch 2470, val loss: 0.55470210313797
Epoch 2480, training loss: 419.6891784667969 = 0.4655461311340332 + 50.0 * 8.384472846984863
Epoch 2480, val loss: 0.5542932152748108
Epoch 2490, training loss: 419.7494201660156 = 0.46451282501220703 + 50.0 * 8.385698318481445
Epoch 2490, val loss: 0.5538510680198669
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7818366311516995
0.8161269289284939
=== training gcn model ===
Epoch 0, training loss: 530.2130126953125 = 1.098007082939148 + 50.0 * 10.58229923248291
Epoch 0, val loss: 1.096312165260315
Epoch 10, training loss: 530.1928100585938 = 1.093776822090149 + 50.0 * 10.581981658935547
Epoch 10, val loss: 1.0920737981796265
Epoch 20, training loss: 530.1133422851562 = 1.0891047716140747 + 50.0 * 10.580484390258789
Epoch 20, val loss: 1.087377667427063
Epoch 30, training loss: 529.73046875 = 1.0838927030563354 + 50.0 * 10.572932243347168
Epoch 30, val loss: 1.0821506977081299
Epoch 40, training loss: 528.1077880859375 = 1.078180193901062 + 50.0 * 10.540592193603516
Epoch 40, val loss: 1.0764100551605225
Epoch 50, training loss: 522.9269409179688 = 1.0719258785247803 + 50.0 * 10.437100410461426
Epoch 50, val loss: 1.0701708793640137
Epoch 60, training loss: 509.9139404296875 = 1.0660450458526611 + 50.0 * 10.176958084106445
Epoch 60, val loss: 1.0643330812454224
Epoch 70, training loss: 492.89080810546875 = 1.060365080833435 + 50.0 * 9.83660888671875
Epoch 70, val loss: 1.0587931871414185
Epoch 80, training loss: 481.0390930175781 = 1.0556226968765259 + 50.0 * 9.599669456481934
Epoch 80, val loss: 1.0543694496154785
Epoch 90, training loss: 475.10455322265625 = 1.0518784523010254 + 50.0 * 9.481053352355957
Epoch 90, val loss: 1.0510092973709106
Epoch 100, training loss: 468.3296203613281 = 1.0491808652877808 + 50.0 * 9.345608711242676
Epoch 100, val loss: 1.0485904216766357
Epoch 110, training loss: 460.5271301269531 = 1.047195315361023 + 50.0 * 9.18959903717041
Epoch 110, val loss: 1.0467523336410522
Epoch 120, training loss: 456.32659912109375 = 1.0450198650360107 + 50.0 * 9.105631828308105
Epoch 120, val loss: 1.0446234941482544
Epoch 130, training loss: 451.5636291503906 = 1.0428861379623413 + 50.0 * 9.010415077209473
Epoch 130, val loss: 1.0426009893417358
Epoch 140, training loss: 447.9185791015625 = 1.0415489673614502 + 50.0 * 8.937541007995605
Epoch 140, val loss: 1.0414378643035889
Epoch 150, training loss: 445.1712646484375 = 1.0400545597076416 + 50.0 * 8.882623672485352
Epoch 150, val loss: 1.039971113204956
Epoch 160, training loss: 442.3675537109375 = 1.0381971597671509 + 50.0 * 8.826586723327637
Epoch 160, val loss: 1.0381205081939697
Epoch 170, training loss: 440.1434020996094 = 1.0362213850021362 + 50.0 * 8.782143592834473
Epoch 170, val loss: 1.036177635192871
Epoch 180, training loss: 438.1998291015625 = 1.0341711044311523 + 50.0 * 8.74331283569336
Epoch 180, val loss: 1.0342410802841187
Epoch 190, training loss: 436.40252685546875 = 1.0321495532989502 + 50.0 * 8.70740795135498
Epoch 190, val loss: 1.032318115234375
Epoch 200, training loss: 435.01800537109375 = 1.0300933122634888 + 50.0 * 8.679758071899414
Epoch 200, val loss: 1.0303093194961548
Epoch 210, training loss: 434.15386962890625 = 1.02778959274292 + 50.0 * 8.662521362304688
Epoch 210, val loss: 1.0280619859695435
Epoch 220, training loss: 433.1556396484375 = 1.0252031087875366 + 50.0 * 8.642608642578125
Epoch 220, val loss: 1.025589108467102
Epoch 230, training loss: 432.4295654296875 = 1.0224778652191162 + 50.0 * 8.628141403198242
Epoch 230, val loss: 1.0229567289352417
Epoch 240, training loss: 431.7479248046875 = 1.0195987224578857 + 50.0 * 8.614566802978516
Epoch 240, val loss: 1.0201677083969116
Epoch 250, training loss: 431.1439514160156 = 1.0165643692016602 + 50.0 * 8.602547645568848
Epoch 250, val loss: 1.0172045230865479
Epoch 260, training loss: 430.6107177734375 = 1.0133063793182373 + 50.0 * 8.591948509216309
Epoch 260, val loss: 1.0140337944030762
Epoch 270, training loss: 430.06402587890625 = 1.0097657442092896 + 50.0 * 8.581085205078125
Epoch 270, val loss: 1.0105940103530884
Epoch 280, training loss: 429.5897216796875 = 1.0059655904769897 + 50.0 * 8.571675300598145
Epoch 280, val loss: 1.006925344467163
Epoch 290, training loss: 429.1357421875 = 1.0019339323043823 + 50.0 * 8.562676429748535
Epoch 290, val loss: 1.002987265586853
Epoch 300, training loss: 428.9231872558594 = 0.9975779056549072 + 50.0 * 8.558511734008789
Epoch 300, val loss: 0.9987865090370178
Epoch 310, training loss: 428.3943786621094 = 0.9929156303405762 + 50.0 * 8.548028945922852
Epoch 310, val loss: 0.9942198991775513
Epoch 320, training loss: 427.94219970703125 = 0.9879702925682068 + 50.0 * 8.539084434509277
Epoch 320, val loss: 0.9894189834594727
Epoch 330, training loss: 427.60418701171875 = 0.9827675819396973 + 50.0 * 8.532428741455078
Epoch 330, val loss: 0.9843671917915344
Epoch 340, training loss: 427.280029296875 = 0.9772173762321472 + 50.0 * 8.526056289672852
Epoch 340, val loss: 0.9790262579917908
Epoch 350, training loss: 426.99462890625 = 0.9713343381881714 + 50.0 * 8.520465850830078
Epoch 350, val loss: 0.9733484387397766
Epoch 360, training loss: 426.7334899902344 = 0.9651211500167847 + 50.0 * 8.51536750793457
Epoch 360, val loss: 0.9673614501953125
Epoch 370, training loss: 426.4825134277344 = 0.9586052298545837 + 50.0 * 8.510478019714355
Epoch 370, val loss: 0.9610840678215027
Epoch 380, training loss: 426.2279357910156 = 0.951751172542572 + 50.0 * 8.505523681640625
Epoch 380, val loss: 0.9545184969902039
Epoch 390, training loss: 426.0240783691406 = 0.9445654153823853 + 50.0 * 8.50158977508545
Epoch 390, val loss: 0.9476509094238281
Epoch 400, training loss: 425.96466064453125 = 0.9370195865631104 + 50.0 * 8.500553131103516
Epoch 400, val loss: 0.9404477477073669
Epoch 410, training loss: 425.71771240234375 = 0.9291483163833618 + 50.0 * 8.495771408081055
Epoch 410, val loss: 0.9329349994659424
Epoch 420, training loss: 425.4960021972656 = 0.9210327863693237 + 50.0 * 8.491499900817871
Epoch 420, val loss: 0.925254762172699
Epoch 430, training loss: 425.34912109375 = 0.9127013683319092 + 50.0 * 8.488728523254395
Epoch 430, val loss: 0.9173778891563416
Epoch 440, training loss: 425.2124938964844 = 0.9041455388069153 + 50.0 * 8.486166954040527
Epoch 440, val loss: 0.9093217849731445
Epoch 450, training loss: 425.1233825683594 = 0.8953186869621277 + 50.0 * 8.4845609664917
Epoch 450, val loss: 0.9010068774223328
Epoch 460, training loss: 425.0122985839844 = 0.8862745761871338 + 50.0 * 8.482521057128906
Epoch 460, val loss: 0.8925209045410156
Epoch 470, training loss: 424.84320068359375 = 0.8771315813064575 + 50.0 * 8.479321479797363
Epoch 470, val loss: 0.8839676380157471
Epoch 480, training loss: 424.6880798339844 = 0.8679298162460327 + 50.0 * 8.47640323638916
Epoch 480, val loss: 0.8753716945648193
Epoch 490, training loss: 424.8181457519531 = 0.8586323857307434 + 50.0 * 8.4791898727417
Epoch 490, val loss: 0.8667578101158142
Epoch 500, training loss: 424.4950256347656 = 0.8493273854255676 + 50.0 * 8.47291374206543
Epoch 500, val loss: 0.858062744140625
Epoch 510, training loss: 424.3687744140625 = 0.8400440812110901 + 50.0 * 8.470574378967285
Epoch 510, val loss: 0.8495035767555237
Epoch 520, training loss: 424.2420654296875 = 0.8308524489402771 + 50.0 * 8.46822452545166
Epoch 520, val loss: 0.8410462737083435
Epoch 530, training loss: 424.383056640625 = 0.8217736482620239 + 50.0 * 8.47122573852539
Epoch 530, val loss: 0.8326261043548584
Epoch 540, training loss: 424.0589599609375 = 0.8126791715621948 + 50.0 * 8.464925765991211
Epoch 540, val loss: 0.8243550062179565
Epoch 550, training loss: 423.9512634277344 = 0.8038198947906494 + 50.0 * 8.4629487991333
Epoch 550, val loss: 0.8162853717803955
Epoch 560, training loss: 423.8441162109375 = 0.7951886057853699 + 50.0 * 8.460978507995605
Epoch 560, val loss: 0.8084419965744019
Epoch 570, training loss: 423.7389831542969 = 0.7867624759674072 + 50.0 * 8.459044456481934
Epoch 570, val loss: 0.8008139729499817
Epoch 580, training loss: 423.6518249511719 = 0.7785686254501343 + 50.0 * 8.457465171813965
Epoch 580, val loss: 0.793404757976532
Epoch 590, training loss: 423.5901794433594 = 0.7705671191215515 + 50.0 * 8.456392288208008
Epoch 590, val loss: 0.7862114310264587
Epoch 600, training loss: 423.677734375 = 0.762690544128418 + 50.0 * 8.458300590515137
Epoch 600, val loss: 0.7791512608528137
Epoch 610, training loss: 423.4584045410156 = 0.7551171779632568 + 50.0 * 8.454065322875977
Epoch 610, val loss: 0.7724077701568604
Epoch 620, training loss: 423.3359069824219 = 0.7478874921798706 + 50.0 * 8.451760292053223
Epoch 620, val loss: 0.7659146785736084
Epoch 630, training loss: 423.24212646484375 = 0.7408429980278015 + 50.0 * 8.45002555847168
Epoch 630, val loss: 0.759653627872467
Epoch 640, training loss: 423.1536865234375 = 0.7341100573539734 + 50.0 * 8.448391914367676
Epoch 640, val loss: 0.7536494135856628
Epoch 650, training loss: 423.09979248046875 = 0.7276134490966797 + 50.0 * 8.447443962097168
Epoch 650, val loss: 0.7479132413864136
Epoch 660, training loss: 423.1192626953125 = 0.7213944792747498 + 50.0 * 8.447957038879395
Epoch 660, val loss: 0.7424152493476868
Epoch 670, training loss: 423.0461730957031 = 0.7154313325881958 + 50.0 * 8.446615219116211
Epoch 670, val loss: 0.7371325492858887
Epoch 680, training loss: 422.9107666015625 = 0.7098082900047302 + 50.0 * 8.444019317626953
Epoch 680, val loss: 0.7321812510490417
Epoch 690, training loss: 422.8183288574219 = 0.7044530510902405 + 50.0 * 8.442276954650879
Epoch 690, val loss: 0.7275592684745789
Epoch 700, training loss: 422.8168029785156 = 0.699357271194458 + 50.0 * 8.44234848022461
Epoch 700, val loss: 0.7231554388999939
Epoch 710, training loss: 422.6888122558594 = 0.6944705843925476 + 50.0 * 8.439887046813965
Epoch 710, val loss: 0.718910813331604
Epoch 720, training loss: 422.6362609863281 = 0.6898189187049866 + 50.0 * 8.438928604125977
Epoch 720, val loss: 0.7149038314819336
Epoch 730, training loss: 422.5861511230469 = 0.6853879690170288 + 50.0 * 8.43801498413086
Epoch 730, val loss: 0.711124062538147
Epoch 740, training loss: 422.53009033203125 = 0.6811813712120056 + 50.0 * 8.436978340148926
Epoch 740, val loss: 0.7075644135475159
Epoch 750, training loss: 422.6842956542969 = 0.6771454811096191 + 50.0 * 8.440142631530762
Epoch 750, val loss: 0.70425945520401
Epoch 760, training loss: 422.4924011230469 = 0.6732892990112305 + 50.0 * 8.436382293701172
Epoch 760, val loss: 0.700826108455658
Epoch 770, training loss: 422.4076232910156 = 0.6696205139160156 + 50.0 * 8.434760093688965
Epoch 770, val loss: 0.697731614112854
Epoch 780, training loss: 422.30255126953125 = 0.6661543250083923 + 50.0 * 8.432727813720703
Epoch 780, val loss: 0.6948965787887573
Epoch 790, training loss: 422.25152587890625 = 0.6628632545471191 + 50.0 * 8.43177318572998
Epoch 790, val loss: 0.6922152042388916
Epoch 800, training loss: 422.2021484375 = 0.6597182750701904 + 50.0 * 8.430848121643066
Epoch 800, val loss: 0.6896361708641052
Epoch 810, training loss: 422.8363342285156 = 0.6566921472549438 + 50.0 * 8.44359302520752
Epoch 810, val loss: 0.6871597170829773
Epoch 820, training loss: 422.1871643066406 = 0.6537041068077087 + 50.0 * 8.430668830871582
Epoch 820, val loss: 0.684767484664917
Epoch 830, training loss: 422.0873107910156 = 0.650921642780304 + 50.0 * 8.428728103637695
Epoch 830, val loss: 0.6825484037399292
Epoch 840, training loss: 422.0310974121094 = 0.6482818722724915 + 50.0 * 8.427656173706055
Epoch 840, val loss: 0.680484414100647
Epoch 850, training loss: 421.9907531738281 = 0.6457597613334656 + 50.0 * 8.426899909973145
Epoch 850, val loss: 0.6784828901290894
Epoch 860, training loss: 421.9721374511719 = 0.6433320045471191 + 50.0 * 8.426575660705566
Epoch 860, val loss: 0.676581859588623
Epoch 870, training loss: 422.0954895019531 = 0.6409581899642944 + 50.0 * 8.42909049987793
Epoch 870, val loss: 0.6747310757637024
Epoch 880, training loss: 421.9134521484375 = 0.6386542320251465 + 50.0 * 8.425496101379395
Epoch 880, val loss: 0.6729527115821838
Epoch 890, training loss: 421.86669921875 = 0.6364758014678955 + 50.0 * 8.424604415893555
Epoch 890, val loss: 0.671230673789978
Epoch 900, training loss: 421.81561279296875 = 0.6343938112258911 + 50.0 * 8.423624038696289
Epoch 900, val loss: 0.6696588397026062
Epoch 910, training loss: 421.7835998535156 = 0.6323822736740112 + 50.0 * 8.42302417755127
Epoch 910, val loss: 0.668161928653717
Epoch 920, training loss: 421.7478332519531 = 0.6304402351379395 + 50.0 * 8.422348022460938
Epoch 920, val loss: 0.6666823029518127
Epoch 930, training loss: 421.7173156738281 = 0.6285561919212341 + 50.0 * 8.421774864196777
Epoch 930, val loss: 0.6652912497520447
Epoch 940, training loss: 422.4490051269531 = 0.6267239451408386 + 50.0 * 8.436446189880371
Epoch 940, val loss: 0.6637958884239197
Epoch 950, training loss: 421.88421630859375 = 0.6248185634613037 + 50.0 * 8.425188064575195
Epoch 950, val loss: 0.6625069379806519
Epoch 960, training loss: 421.6788024902344 = 0.6230499148368835 + 50.0 * 8.421114921569824
Epoch 960, val loss: 0.661174476146698
Epoch 970, training loss: 421.59796142578125 = 0.6213669776916504 + 50.0 * 8.41953182220459
Epoch 970, val loss: 0.659934937953949
Epoch 980, training loss: 421.5675048828125 = 0.6197462677955627 + 50.0 * 8.418954849243164
Epoch 980, val loss: 0.658748209476471
Epoch 990, training loss: 421.59661865234375 = 0.6181708574295044 + 50.0 * 8.41956901550293
Epoch 990, val loss: 0.6575549244880676
Epoch 1000, training loss: 421.53729248046875 = 0.6165807247161865 + 50.0 * 8.418414115905762
Epoch 1000, val loss: 0.6564958095550537
Epoch 1010, training loss: 421.5037536621094 = 0.6150517463684082 + 50.0 * 8.417774200439453
Epoch 1010, val loss: 0.6552802324295044
Epoch 1020, training loss: 421.4694519042969 = 0.6135506629943848 + 50.0 * 8.417118072509766
Epoch 1020, val loss: 0.654349684715271
Epoch 1030, training loss: 421.42474365234375 = 0.6121233105659485 + 50.0 * 8.416252136230469
Epoch 1030, val loss: 0.6532548666000366
Epoch 1040, training loss: 421.652099609375 = 0.6107083559036255 + 50.0 * 8.420827865600586
Epoch 1040, val loss: 0.6522757411003113
Epoch 1050, training loss: 421.4866027832031 = 0.6092789769172668 + 50.0 * 8.417546272277832
Epoch 1050, val loss: 0.6512528657913208
Epoch 1060, training loss: 421.34820556640625 = 0.6078941226005554 + 50.0 * 8.414806365966797
Epoch 1060, val loss: 0.6502658724784851
Epoch 1070, training loss: 421.304443359375 = 0.606559693813324 + 50.0 * 8.413957595825195
Epoch 1070, val loss: 0.649343729019165
Epoch 1080, training loss: 421.2715759277344 = 0.6052576303482056 + 50.0 * 8.413326263427734
Epoch 1080, val loss: 0.6484596729278564
Epoch 1090, training loss: 421.2395324707031 = 0.6039784550666809 + 50.0 * 8.412711143493652
Epoch 1090, val loss: 0.6475658416748047
Epoch 1100, training loss: 421.83099365234375 = 0.6026924848556519 + 50.0 * 8.424566268920898
Epoch 1100, val loss: 0.6468440294265747
Epoch 1110, training loss: 421.2958068847656 = 0.6013544797897339 + 50.0 * 8.413888931274414
Epoch 1110, val loss: 0.6457653641700745
Epoch 1120, training loss: 421.20672607421875 = 0.6000787019729614 + 50.0 * 8.41213321685791
Epoch 1120, val loss: 0.644822895526886
Epoch 1130, training loss: 421.1470947265625 = 0.5988548994064331 + 50.0 * 8.410964965820312
Epoch 1130, val loss: 0.6439191699028015
Epoch 1140, training loss: 421.1029357910156 = 0.597680926322937 + 50.0 * 8.410104751586914
Epoch 1140, val loss: 0.6431413292884827
Epoch 1150, training loss: 421.06524658203125 = 0.5965179800987244 + 50.0 * 8.409374237060547
Epoch 1150, val loss: 0.6423003673553467
Epoch 1160, training loss: 421.0328369140625 = 0.5953660607337952 + 50.0 * 8.4087495803833
Epoch 1160, val loss: 0.641541600227356
Epoch 1170, training loss: 421.0077819824219 = 0.5942302346229553 + 50.0 * 8.408270835876465
Epoch 1170, val loss: 0.6407244205474854
Epoch 1180, training loss: 421.6190490722656 = 0.5931041836738586 + 50.0 * 8.42051887512207
Epoch 1180, val loss: 0.6397774815559387
Epoch 1190, training loss: 421.0015563964844 = 0.5918790102005005 + 50.0 * 8.408193588256836
Epoch 1190, val loss: 0.6390625238418579
Epoch 1200, training loss: 420.9351806640625 = 0.5907399654388428 + 50.0 * 8.406888961791992
Epoch 1200, val loss: 0.6382288932800293
Epoch 1210, training loss: 420.903564453125 = 0.5896385908126831 + 50.0 * 8.406278610229492
Epoch 1210, val loss: 0.6375092267990112
Epoch 1220, training loss: 420.8752136230469 = 0.5885540843009949 + 50.0 * 8.405733108520508
Epoch 1220, val loss: 0.636786937713623
Epoch 1230, training loss: 420.8692321777344 = 0.5874841809272766 + 50.0 * 8.405634880065918
Epoch 1230, val loss: 0.6360786557197571
Epoch 1240, training loss: 421.08172607421875 = 0.5863915085792542 + 50.0 * 8.409906387329102
Epoch 1240, val loss: 0.6353099942207336
Epoch 1250, training loss: 420.8217468261719 = 0.5852931141853333 + 50.0 * 8.404728889465332
Epoch 1250, val loss: 0.6345694661140442
Epoch 1260, training loss: 420.7980651855469 = 0.5842293500900269 + 50.0 * 8.404276847839355
Epoch 1260, val loss: 0.6337385177612305
Epoch 1270, training loss: 420.8949890136719 = 0.5831582546234131 + 50.0 * 8.40623664855957
Epoch 1270, val loss: 0.633083701133728
Epoch 1280, training loss: 420.7382507324219 = 0.582106351852417 + 50.0 * 8.403122901916504
Epoch 1280, val loss: 0.632401168346405
Epoch 1290, training loss: 420.7014465332031 = 0.5810697078704834 + 50.0 * 8.4024076461792
Epoch 1290, val loss: 0.6316556334495544
Epoch 1300, training loss: 420.6784973144531 = 0.5800418257713318 + 50.0 * 8.401968955993652
Epoch 1300, val loss: 0.6310185194015503
Epoch 1310, training loss: 420.6785888671875 = 0.5790188312530518 + 50.0 * 8.401991844177246
Epoch 1310, val loss: 0.6303402781486511
Epoch 1320, training loss: 420.8423156738281 = 0.5779784321784973 + 50.0 * 8.40528678894043
Epoch 1320, val loss: 0.6296975612640381
Epoch 1330, training loss: 420.6742248535156 = 0.5769457221031189 + 50.0 * 8.401946067810059
Epoch 1330, val loss: 0.6288971900939941
Epoch 1340, training loss: 420.6167297363281 = 0.5758959054946899 + 50.0 * 8.400816917419434
Epoch 1340, val loss: 0.6282360553741455
Epoch 1350, training loss: 420.56610107421875 = 0.5748883485794067 + 50.0 * 8.399824142456055
Epoch 1350, val loss: 0.6275106072425842
Epoch 1360, training loss: 420.5744934082031 = 0.5738902688026428 + 50.0 * 8.400012016296387
Epoch 1360, val loss: 0.626844048500061
Epoch 1370, training loss: 420.7413635253906 = 0.5728681683540344 + 50.0 * 8.403369903564453
Epoch 1370, val loss: 0.6262487769126892
Epoch 1380, training loss: 420.60089111328125 = 0.5718450546264648 + 50.0 * 8.400581359863281
Epoch 1380, val loss: 0.6254873275756836
Epoch 1390, training loss: 420.6366271972656 = 0.5708077549934387 + 50.0 * 8.40131664276123
Epoch 1390, val loss: 0.6249672770500183
Epoch 1400, training loss: 420.4892272949219 = 0.5697956681251526 + 50.0 * 8.398388862609863
Epoch 1400, val loss: 0.624189019203186
Epoch 1410, training loss: 420.4674987792969 = 0.568793773651123 + 50.0 * 8.397974014282227
Epoch 1410, val loss: 0.6235793232917786
Epoch 1420, training loss: 420.4353332519531 = 0.5678119659423828 + 50.0 * 8.397350311279297
Epoch 1420, val loss: 0.6229468584060669
Epoch 1430, training loss: 420.5729675292969 = 0.5668368935585022 + 50.0 * 8.40012264251709
Epoch 1430, val loss: 0.6222421526908875
Epoch 1440, training loss: 420.60418701171875 = 0.5657694339752197 + 50.0 * 8.400768280029297
Epoch 1440, val loss: 0.6218785643577576
Epoch 1450, training loss: 420.4864807128906 = 0.564751148223877 + 50.0 * 8.39843463897705
Epoch 1450, val loss: 0.6209540963172913
Epoch 1460, training loss: 420.36053466796875 = 0.5637228488922119 + 50.0 * 8.395936012268066
Epoch 1460, val loss: 0.6204874515533447
Epoch 1470, training loss: 420.3462829589844 = 0.5627426505088806 + 50.0 * 8.395670890808105
Epoch 1470, val loss: 0.6199086308479309
Epoch 1480, training loss: 420.336669921875 = 0.5617688298225403 + 50.0 * 8.395498275756836
Epoch 1480, val loss: 0.6192908883094788
Epoch 1490, training loss: 420.5588684082031 = 0.5607695579528809 + 50.0 * 8.399962425231934
Epoch 1490, val loss: 0.6187687516212463
Epoch 1500, training loss: 420.35614013671875 = 0.5597519278526306 + 50.0 * 8.395927429199219
Epoch 1500, val loss: 0.6180771589279175
Epoch 1510, training loss: 420.31097412109375 = 0.5587395429611206 + 50.0 * 8.395044326782227
Epoch 1510, val loss: 0.6174870729446411
Epoch 1520, training loss: 420.34228515625 = 0.5577481389045715 + 50.0 * 8.39569091796875
Epoch 1520, val loss: 0.6169211268424988
Epoch 1530, training loss: 420.3124694824219 = 0.5567435026168823 + 50.0 * 8.39511489868164
Epoch 1530, val loss: 0.6162839531898499
Epoch 1540, training loss: 420.2539978027344 = 0.5557340979576111 + 50.0 * 8.393965721130371
Epoch 1540, val loss: 0.6157789826393127
Epoch 1550, training loss: 420.21881103515625 = 0.5547502040863037 + 50.0 * 8.393280982971191
Epoch 1550, val loss: 0.6151984333992004
Epoch 1560, training loss: 420.1910095214844 = 0.5537760257720947 + 50.0 * 8.392745018005371
Epoch 1560, val loss: 0.6146193146705627
Epoch 1570, training loss: 420.44964599609375 = 0.552790641784668 + 50.0 * 8.397936820983887
Epoch 1570, val loss: 0.6141247153282166
Epoch 1580, training loss: 420.3163757324219 = 0.5517448782920837 + 50.0 * 8.395292282104492
Epoch 1580, val loss: 0.6134724617004395
Epoch 1590, training loss: 420.1629333496094 = 0.5507228374481201 + 50.0 * 8.392244338989258
Epoch 1590, val loss: 0.6128405928611755
Epoch 1600, training loss: 420.1414794921875 = 0.5497167110443115 + 50.0 * 8.39183521270752
Epoch 1600, val loss: 0.6123281717300415
Epoch 1610, training loss: 420.30743408203125 = 0.5487312078475952 + 50.0 * 8.395174026489258
Epoch 1610, val loss: 0.6117272973060608
Epoch 1620, training loss: 420.1189270019531 = 0.5476775169372559 + 50.0 * 8.391425132751465
Epoch 1620, val loss: 0.6111461520195007
Epoch 1630, training loss: 420.1072998046875 = 0.5466445684432983 + 50.0 * 8.391213417053223
Epoch 1630, val loss: 0.610594630241394
Epoch 1640, training loss: 420.08544921875 = 0.5456344485282898 + 50.0 * 8.390796661376953
Epoch 1640, val loss: 0.6100826859474182
Epoch 1650, training loss: 420.0702209472656 = 0.5446420311927795 + 50.0 * 8.390511512756348
Epoch 1650, val loss: 0.609548032283783
Epoch 1660, training loss: 420.3119201660156 = 0.5436282157897949 + 50.0 * 8.395365715026855
Epoch 1660, val loss: 0.6091351509094238
Epoch 1670, training loss: 420.0907897949219 = 0.5426027774810791 + 50.0 * 8.390963554382324
Epoch 1670, val loss: 0.6083130836486816
Epoch 1680, training loss: 420.0414733886719 = 0.5415646433830261 + 50.0 * 8.389998435974121
Epoch 1680, val loss: 0.607904851436615
Epoch 1690, training loss: 420.0096130371094 = 0.540564239025116 + 50.0 * 8.389381408691406
Epoch 1690, val loss: 0.6073075532913208
Epoch 1700, training loss: 420.1147766113281 = 0.5395440459251404 + 50.0 * 8.391504287719727
Epoch 1700, val loss: 0.6069998145103455
Epoch 1710, training loss: 419.97998046875 = 0.5385269522666931 + 50.0 * 8.388829231262207
Epoch 1710, val loss: 0.6061744093894958
Epoch 1720, training loss: 419.96710205078125 = 0.5374812483787537 + 50.0 * 8.388592720031738
Epoch 1720, val loss: 0.6058153510093689
Epoch 1730, training loss: 419.94537353515625 = 0.5364736318588257 + 50.0 * 8.388177871704102
Epoch 1730, val loss: 0.6051768660545349
Epoch 1740, training loss: 419.9403991699219 = 0.5354475378990173 + 50.0 * 8.38809871673584
Epoch 1740, val loss: 0.6047537922859192
Epoch 1750, training loss: 420.32208251953125 = 0.5344286561012268 + 50.0 * 8.395752906799316
Epoch 1750, val loss: 0.6041191220283508
Epoch 1760, training loss: 420.0301513671875 = 0.5333240032196045 + 50.0 * 8.389936447143555
Epoch 1760, val loss: 0.6036778688430786
Epoch 1770, training loss: 419.9198303222656 = 0.5322771668434143 + 50.0 * 8.387750625610352
Epoch 1770, val loss: 0.6030422449111938
Epoch 1780, training loss: 419.87371826171875 = 0.531235933303833 + 50.0 * 8.386849403381348
Epoch 1780, val loss: 0.6025605797767639
Epoch 1790, training loss: 419.8588562011719 = 0.5302039384841919 + 50.0 * 8.38657283782959
Epoch 1790, val loss: 0.6020951867103577
Epoch 1800, training loss: 420.1601257324219 = 0.5291669368743896 + 50.0 * 8.392619132995605
Epoch 1800, val loss: 0.6016075611114502
Epoch 1810, training loss: 419.9732666015625 = 0.5280654430389404 + 50.0 * 8.388903617858887
Epoch 1810, val loss: 0.6009613275527954
Epoch 1820, training loss: 419.9305419921875 = 0.5269889831542969 + 50.0 * 8.388071060180664
Epoch 1820, val loss: 0.6004466414451599
Epoch 1830, training loss: 419.8387145996094 = 0.5259011387825012 + 50.0 * 8.386256217956543
Epoch 1830, val loss: 0.5998971462249756
Epoch 1840, training loss: 419.81817626953125 = 0.5248244404792786 + 50.0 * 8.38586711883545
Epoch 1840, val loss: 0.599420428276062
Epoch 1850, training loss: 419.7754211425781 = 0.5237666368484497 + 50.0 * 8.385032653808594
Epoch 1850, val loss: 0.598921537399292
Epoch 1860, training loss: 419.7552490234375 = 0.5227161049842834 + 50.0 * 8.384651184082031
Epoch 1860, val loss: 0.5983697772026062
Epoch 1870, training loss: 419.77020263671875 = 0.521661102771759 + 50.0 * 8.384970664978027
Epoch 1870, val loss: 0.5978003740310669
Epoch 1880, training loss: 419.91839599609375 = 0.5205902457237244 + 50.0 * 8.387955665588379
Epoch 1880, val loss: 0.5972196459770203
Epoch 1890, training loss: 419.9143981933594 = 0.5194360017776489 + 50.0 * 8.387899398803711
Epoch 1890, val loss: 0.596985399723053
Epoch 1900, training loss: 419.78839111328125 = 0.518334150314331 + 50.0 * 8.385400772094727
Epoch 1900, val loss: 0.5962375998497009
Epoch 1910, training loss: 419.706787109375 = 0.517227292060852 + 50.0 * 8.383790969848633
Epoch 1910, val loss: 0.5958936214447021
Epoch 1920, training loss: 419.6805114746094 = 0.5161573886871338 + 50.0 * 8.38328742980957
Epoch 1920, val loss: 0.5953146815299988
Epoch 1930, training loss: 419.6600341796875 = 0.5150628089904785 + 50.0 * 8.382899284362793
Epoch 1930, val loss: 0.5949191451072693
Epoch 1940, training loss: 419.9267883300781 = 0.5139648914337158 + 50.0 * 8.388256072998047
Epoch 1940, val loss: 0.5944178104400635
Epoch 1950, training loss: 419.7746276855469 = 0.5127803087234497 + 50.0 * 8.385236740112305
Epoch 1950, val loss: 0.5939305424690247
Epoch 1960, training loss: 419.69464111328125 = 0.5116214752197266 + 50.0 * 8.383660316467285
Epoch 1960, val loss: 0.5933581590652466
Epoch 1970, training loss: 419.61260986328125 = 0.5104860663414001 + 50.0 * 8.382041931152344
Epoch 1970, val loss: 0.5929023027420044
Epoch 1980, training loss: 419.60711669921875 = 0.5093609094619751 + 50.0 * 8.38195514678955
Epoch 1980, val loss: 0.5923977494239807
Epoch 1990, training loss: 419.9202880859375 = 0.5082182884216309 + 50.0 * 8.3882417678833
Epoch 1990, val loss: 0.5920177698135376
Epoch 2000, training loss: 419.6490173339844 = 0.5070503354072571 + 50.0 * 8.38283920288086
Epoch 2000, val loss: 0.5913739204406738
Epoch 2010, training loss: 419.5909118652344 = 0.5058789253234863 + 50.0 * 8.38170051574707
Epoch 2010, val loss: 0.5908969044685364
Epoch 2020, training loss: 419.5570373535156 = 0.5047438144683838 + 50.0 * 8.381046295166016
Epoch 2020, val loss: 0.5904002785682678
Epoch 2030, training loss: 419.537353515625 = 0.5036192536354065 + 50.0 * 8.380674362182617
Epoch 2030, val loss: 0.5898928046226501
Epoch 2040, training loss: 419.66400146484375 = 0.5024920701980591 + 50.0 * 8.383230209350586
Epoch 2040, val loss: 0.5893580913543701
Epoch 2050, training loss: 419.5392150878906 = 0.501305341720581 + 50.0 * 8.380758285522461
Epoch 2050, val loss: 0.58894944190979
Epoch 2060, training loss: 419.52178955078125 = 0.5001330971717834 + 50.0 * 8.380433082580566
Epoch 2060, val loss: 0.588441789150238
Epoch 2070, training loss: 419.4891662597656 = 0.4989902377128601 + 50.0 * 8.379803657531738
Epoch 2070, val loss: 0.5879020094871521
Epoch 2080, training loss: 419.4745788574219 = 0.4978365898132324 + 50.0 * 8.379534721374512
Epoch 2080, val loss: 0.5874817371368408
Epoch 2090, training loss: 419.59613037109375 = 0.4966961145401001 + 50.0 * 8.381988525390625
Epoch 2090, val loss: 0.5868520140647888
Epoch 2100, training loss: 419.4990234375 = 0.49548617005348206 + 50.0 * 8.380070686340332
Epoch 2100, val loss: 0.5865006446838379
Epoch 2110, training loss: 419.4891052246094 = 0.49429064989089966 + 50.0 * 8.37989616394043
Epoch 2110, val loss: 0.5859491229057312
Epoch 2120, training loss: 419.446044921875 = 0.493112176656723 + 50.0 * 8.379058837890625
Epoch 2120, val loss: 0.5855035781860352
Epoch 2130, training loss: 419.4341125488281 = 0.4919516146183014 + 50.0 * 8.378843307495117
Epoch 2130, val loss: 0.5850472450256348
Epoch 2140, training loss: 419.52880859375 = 0.49078062176704407 + 50.0 * 8.380760192871094
Epoch 2140, val loss: 0.584596574306488
Epoch 2150, training loss: 419.4990234375 = 0.4895763695240021 + 50.0 * 8.380188941955566
Epoch 2150, val loss: 0.5841485857963562
Epoch 2160, training loss: 419.4507751464844 = 0.4883885383605957 + 50.0 * 8.379247665405273
Epoch 2160, val loss: 0.5835226774215698
Epoch 2170, training loss: 419.4156188964844 = 0.4871860146522522 + 50.0 * 8.378568649291992
Epoch 2170, val loss: 0.5831631422042847
Epoch 2180, training loss: 419.3922424316406 = 0.4860036373138428 + 50.0 * 8.378125190734863
Epoch 2180, val loss: 0.5826315879821777
Epoch 2190, training loss: 419.443115234375 = 0.4848323166370392 + 50.0 * 8.379165649414062
Epoch 2190, val loss: 0.5820845365524292
Epoch 2200, training loss: 419.3876647949219 = 0.4836113750934601 + 50.0 * 8.378081321716309
Epoch 2200, val loss: 0.5817806124687195
Epoch 2210, training loss: 419.3927307128906 = 0.4824073910713196 + 50.0 * 8.378206253051758
Epoch 2210, val loss: 0.5812917351722717
Epoch 2220, training loss: 419.3507080078125 = 0.481199711561203 + 50.0 * 8.377389907836914
Epoch 2220, val loss: 0.5807679295539856
Epoch 2230, training loss: 419.33349609375 = 0.4799772799015045 + 50.0 * 8.377070426940918
Epoch 2230, val loss: 0.5803748369216919
Epoch 2240, training loss: 419.4947509765625 = 0.47874411940574646 + 50.0 * 8.38032054901123
Epoch 2240, val loss: 0.5798916220664978
Epoch 2250, training loss: 419.3287658691406 = 0.47745221853256226 + 50.0 * 8.377026557922363
Epoch 2250, val loss: 0.5794080495834351
Epoch 2260, training loss: 419.3051452636719 = 0.47616714239120483 + 50.0 * 8.376579284667969
Epoch 2260, val loss: 0.579002857208252
Epoch 2270, training loss: 419.2866516113281 = 0.47491663694381714 + 50.0 * 8.376235008239746
Epoch 2270, val loss: 0.5784028768539429
Epoch 2280, training loss: 419.3620300292969 = 0.47366294264793396 + 50.0 * 8.377767562866211
Epoch 2280, val loss: 0.5778759121894836
Epoch 2290, training loss: 419.238037109375 = 0.47235459089279175 + 50.0 * 8.375313758850098
Epoch 2290, val loss: 0.5776455402374268
Epoch 2300, training loss: 419.2475280761719 = 0.4710608124732971 + 50.0 * 8.375529289245605
Epoch 2300, val loss: 0.5772333145141602
Epoch 2310, training loss: 419.29473876953125 = 0.4697903096675873 + 50.0 * 8.37649917602539
Epoch 2310, val loss: 0.5766763091087341
Epoch 2320, training loss: 419.35943603515625 = 0.4684850871562958 + 50.0 * 8.377819061279297
Epoch 2320, val loss: 0.5763868093490601
Epoch 2330, training loss: 419.3155517578125 = 0.4672034978866577 + 50.0 * 8.376967430114746
Epoch 2330, val loss: 0.5757833123207092
Epoch 2340, training loss: 419.252685546875 = 0.4659213125705719 + 50.0 * 8.37573528289795
Epoch 2340, val loss: 0.5754413604736328
Epoch 2350, training loss: 419.3396911621094 = 0.46464797854423523 + 50.0 * 8.377500534057617
Epoch 2350, val loss: 0.5751450657844543
Epoch 2360, training loss: 419.1925964355469 = 0.46335288882255554 + 50.0 * 8.374585151672363
Epoch 2360, val loss: 0.5747213363647461
Epoch 2370, training loss: 419.17462158203125 = 0.46207064390182495 + 50.0 * 8.374251365661621
Epoch 2370, val loss: 0.5744709372520447
Epoch 2380, training loss: 419.1681823730469 = 0.46080243587493896 + 50.0 * 8.374147415161133
Epoch 2380, val loss: 0.5742053985595703
Epoch 2390, training loss: 419.2117614746094 = 0.45953187346458435 + 50.0 * 8.375044822692871
Epoch 2390, val loss: 0.5738972425460815
Epoch 2400, training loss: 419.2357177734375 = 0.4582456350326538 + 50.0 * 8.37554931640625
Epoch 2400, val loss: 0.573456346988678
Epoch 2410, training loss: 419.17279052734375 = 0.45695164799690247 + 50.0 * 8.374317169189453
Epoch 2410, val loss: 0.5730345845222473
Epoch 2420, training loss: 419.20013427734375 = 0.45565932989120483 + 50.0 * 8.374889373779297
Epoch 2420, val loss: 0.5727308392524719
Epoch 2430, training loss: 419.2464904785156 = 0.454353392124176 + 50.0 * 8.375843048095703
Epoch 2430, val loss: 0.5723239779472351
Epoch 2440, training loss: 419.1373596191406 = 0.4530433118343353 + 50.0 * 8.373686790466309
Epoch 2440, val loss: 0.5719186067581177
Epoch 2450, training loss: 419.10443115234375 = 0.4517410099506378 + 50.0 * 8.373053550720215
Epoch 2450, val loss: 0.5716491937637329
Epoch 2460, training loss: 419.0804443359375 = 0.45045560598373413 + 50.0 * 8.372599601745605
Epoch 2460, val loss: 0.571296751499176
Epoch 2470, training loss: 419.0792541503906 = 0.44917428493499756 + 50.0 * 8.372601509094238
Epoch 2470, val loss: 0.5709823966026306
Epoch 2480, training loss: 419.2183532714844 = 0.4479045271873474 + 50.0 * 8.375409126281738
Epoch 2480, val loss: 0.5705443024635315
Epoch 2490, training loss: 419.1593322753906 = 0.4465476870536804 + 50.0 * 8.374256134033203
Epoch 2490, val loss: 0.5705791711807251
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7767630644342973
0.8124320799826126
The final CL Acc:0.78217, 0.00456, The final GNN Acc:0.81492, 0.00176
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110938])
remove edge: torch.Size([2, 66348])
updated graph: torch.Size([2, 88638])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 530.2025756835938 = 1.0874682664871216 + 50.0 * 10.58230209350586
Epoch 0, val loss: 1.0873186588287354
Epoch 10, training loss: 530.1824340820312 = 1.0834616422653198 + 50.0 * 10.581979751586914
Epoch 10, val loss: 1.0832855701446533
Epoch 20, training loss: 530.1068115234375 = 1.0790462493896484 + 50.0 * 10.580554962158203
Epoch 20, val loss: 1.078869104385376
Epoch 30, training loss: 529.77392578125 = 1.0742249488830566 + 50.0 * 10.573993682861328
Epoch 30, val loss: 1.0740721225738525
Epoch 40, training loss: 528.405517578125 = 1.0690174102783203 + 50.0 * 10.546730041503906
Epoch 40, val loss: 1.0689078569412231
Epoch 50, training loss: 524.1296997070312 = 1.063514232635498 + 50.0 * 10.461322784423828
Epoch 50, val loss: 1.0634397268295288
Epoch 60, training loss: 513.9208374023438 = 1.0580278635025024 + 50.0 * 10.257255554199219
Epoch 60, val loss: 1.058038353919983
Epoch 70, training loss: 495.5489196777344 = 1.052423119544983 + 50.0 * 9.88992977142334
Epoch 70, val loss: 1.0523706674575806
Epoch 80, training loss: 480.92425537109375 = 1.0457769632339478 + 50.0 * 9.597569465637207
Epoch 80, val loss: 1.0456411838531494
Epoch 90, training loss: 477.10986328125 = 1.0390725135803223 + 50.0 * 9.521415710449219
Epoch 90, val loss: 1.0389080047607422
Epoch 100, training loss: 471.6523132324219 = 1.0330631732940674 + 50.0 * 9.412384986877441
Epoch 100, val loss: 1.0329996347427368
Epoch 110, training loss: 464.1305847167969 = 1.0277960300445557 + 50.0 * 9.262055397033691
Epoch 110, val loss: 1.027867317199707
Epoch 120, training loss: 457.8868713378906 = 1.0228091478347778 + 50.0 * 9.13728141784668
Epoch 120, val loss: 1.0230751037597656
Epoch 130, training loss: 454.046142578125 = 1.0179320573806763 + 50.0 * 9.060564041137695
Epoch 130, val loss: 1.0183610916137695
Epoch 140, training loss: 450.5750732421875 = 1.0127612352371216 + 50.0 * 8.991246223449707
Epoch 140, val loss: 1.0133496522903442
Epoch 150, training loss: 448.6266174316406 = 1.0074161291122437 + 50.0 * 8.952383995056152
Epoch 150, val loss: 1.0081994533538818
Epoch 160, training loss: 447.2042541503906 = 1.0020956993103027 + 50.0 * 8.924042701721191
Epoch 160, val loss: 1.0031015872955322
Epoch 170, training loss: 445.1148376464844 = 0.9971797466278076 + 50.0 * 8.882352828979492
Epoch 170, val loss: 0.9984418153762817
Epoch 180, training loss: 442.88519287109375 = 0.9929612874984741 + 50.0 * 8.837844848632812
Epoch 180, val loss: 0.994422197341919
Epoch 190, training loss: 441.789306640625 = 0.9883391261100769 + 50.0 * 8.816019058227539
Epoch 190, val loss: 0.9898390769958496
Epoch 200, training loss: 440.7840881347656 = 0.9824944138526917 + 50.0 * 8.796031951904297
Epoch 200, val loss: 0.9840770363807678
Epoch 210, training loss: 439.43243408203125 = 0.9761960506439209 + 50.0 * 8.769124984741211
Epoch 210, val loss: 0.9779698252677917
Epoch 220, training loss: 437.8602294921875 = 0.9702392816543579 + 50.0 * 8.737799644470215
Epoch 220, val loss: 0.9722740054130554
Epoch 230, training loss: 436.58148193359375 = 0.9645021557807922 + 50.0 * 8.712339401245117
Epoch 230, val loss: 0.9666963219642639
Epoch 240, training loss: 435.56854248046875 = 0.9581886529922485 + 50.0 * 8.692207336425781
Epoch 240, val loss: 0.9605178236961365
Epoch 250, training loss: 434.5929260253906 = 0.9511974453926086 + 50.0 * 8.672834396362305
Epoch 250, val loss: 0.9537161588668823
Epoch 260, training loss: 433.73345947265625 = 0.943903923034668 + 50.0 * 8.655791282653809
Epoch 260, val loss: 0.9466562271118164
Epoch 270, training loss: 433.044677734375 = 0.9363860487937927 + 50.0 * 8.642166137695312
Epoch 270, val loss: 0.939386248588562
Epoch 280, training loss: 432.4117126464844 = 0.928413450717926 + 50.0 * 8.629666328430176
Epoch 280, val loss: 0.931689441204071
Epoch 290, training loss: 431.7486267089844 = 0.9201379418373108 + 50.0 * 8.616569519042969
Epoch 290, val loss: 0.9236798286437988
Epoch 300, training loss: 431.04437255859375 = 0.9117949604988098 + 50.0 * 8.602651596069336
Epoch 300, val loss: 0.9156640768051147
Epoch 310, training loss: 430.3764953613281 = 0.9033303260803223 + 50.0 * 8.589463233947754
Epoch 310, val loss: 0.9074932932853699
Epoch 320, training loss: 429.8213195800781 = 0.8945626616477966 + 50.0 * 8.578535079956055
Epoch 320, val loss: 0.8989754319190979
Epoch 330, training loss: 429.28863525390625 = 0.885239839553833 + 50.0 * 8.56806755065918
Epoch 330, val loss: 0.8899481892585754
Epoch 340, training loss: 428.81829833984375 = 0.8754913806915283 + 50.0 * 8.558856010437012
Epoch 340, val loss: 0.8805345892906189
Epoch 350, training loss: 428.4190979003906 = 0.8654549717903137 + 50.0 * 8.55107307434082
Epoch 350, val loss: 0.8708546757698059
Epoch 360, training loss: 427.95941162109375 = 0.8551374077796936 + 50.0 * 8.542085647583008
Epoch 360, val loss: 0.860950767993927
Epoch 370, training loss: 427.6333923339844 = 0.8446229100227356 + 50.0 * 8.535775184631348
Epoch 370, val loss: 0.850785493850708
Epoch 380, training loss: 427.2103576660156 = 0.83393794298172 + 50.0 * 8.527528762817383
Epoch 380, val loss: 0.8405452966690063
Epoch 390, training loss: 426.8094482421875 = 0.8232026100158691 + 50.0 * 8.51972484588623
Epoch 390, val loss: 0.8302262425422668
Epoch 400, training loss: 426.46807861328125 = 0.8123646974563599 + 50.0 * 8.513113975524902
Epoch 400, val loss: 0.8198201060295105
Epoch 410, training loss: 426.16632080078125 = 0.8014066219329834 + 50.0 * 8.507298469543457
Epoch 410, val loss: 0.8092902898788452
Epoch 420, training loss: 426.0924072265625 = 0.7903324365615845 + 50.0 * 8.506041526794434
Epoch 420, val loss: 0.7986170053482056
Epoch 430, training loss: 425.80548095703125 = 0.7790566086769104 + 50.0 * 8.500528335571289
Epoch 430, val loss: 0.7878057956695557
Epoch 440, training loss: 425.5875549316406 = 0.767892062664032 + 50.0 * 8.496393203735352
Epoch 440, val loss: 0.7771271467208862
Epoch 450, training loss: 425.4109802246094 = 0.7568596005439758 + 50.0 * 8.493082046508789
Epoch 450, val loss: 0.7665821313858032
Epoch 460, training loss: 425.26007080078125 = 0.7459629774093628 + 50.0 * 8.49028205871582
Epoch 460, val loss: 0.7561637163162231
Epoch 470, training loss: 425.11651611328125 = 0.7352578043937683 + 50.0 * 8.487625122070312
Epoch 470, val loss: 0.7459456920623779
Epoch 480, training loss: 424.9708557128906 = 0.7247800230979919 + 50.0 * 8.4849214553833
Epoch 480, val loss: 0.7359533309936523
Epoch 490, training loss: 424.8286437988281 = 0.7145731449127197 + 50.0 * 8.482281684875488
Epoch 490, val loss: 0.7262229919433594
Epoch 500, training loss: 424.69818115234375 = 0.7045912146568298 + 50.0 * 8.47987174987793
Epoch 500, val loss: 0.7166851758956909
Epoch 510, training loss: 424.6343078613281 = 0.6948559284210205 + 50.0 * 8.478789329528809
Epoch 510, val loss: 0.7074514627456665
Epoch 520, training loss: 424.4327697753906 = 0.6855850219726562 + 50.0 * 8.474944114685059
Epoch 520, val loss: 0.6986318826675415
Epoch 530, training loss: 424.3044738769531 = 0.6766040921211243 + 50.0 * 8.472557067871094
Epoch 530, val loss: 0.6900789141654968
Epoch 540, training loss: 424.1923828125 = 0.6679131984710693 + 50.0 * 8.470489501953125
Epoch 540, val loss: 0.6818159818649292
Epoch 550, training loss: 424.08660888671875 = 0.6595538258552551 + 50.0 * 8.468541145324707
Epoch 550, val loss: 0.6738811135292053
Epoch 560, training loss: 423.99127197265625 = 0.6514928340911865 + 50.0 * 8.466795921325684
Epoch 560, val loss: 0.6662214398384094
Epoch 570, training loss: 423.9024658203125 = 0.6437382102012634 + 50.0 * 8.465174674987793
Epoch 570, val loss: 0.6588624119758606
Epoch 580, training loss: 423.8515930175781 = 0.6362977623939514 + 50.0 * 8.464305877685547
Epoch 580, val loss: 0.6518211364746094
Epoch 590, training loss: 423.8551025390625 = 0.6291286945343018 + 50.0 * 8.464519500732422
Epoch 590, val loss: 0.644987165927887
Epoch 600, training loss: 423.69952392578125 = 0.6222700476646423 + 50.0 * 8.46154499053955
Epoch 600, val loss: 0.6385312676429749
Epoch 610, training loss: 423.5835266113281 = 0.6158313751220703 + 50.0 * 8.45935344696045
Epoch 610, val loss: 0.6324256658554077
Epoch 620, training loss: 423.5055847167969 = 0.6097046136856079 + 50.0 * 8.457917213439941
Epoch 620, val loss: 0.6266295909881592
Epoch 630, training loss: 423.4130859375 = 0.603873074054718 + 50.0 * 8.456184387207031
Epoch 630, val loss: 0.621120274066925
Epoch 640, training loss: 423.3288269042969 = 0.5983356833457947 + 50.0 * 8.454609870910645
Epoch 640, val loss: 0.615882396697998
Epoch 650, training loss: 423.411865234375 = 0.5930290818214417 + 50.0 * 8.456377029418945
Epoch 650, val loss: 0.6108686923980713
Epoch 660, training loss: 423.19757080078125 = 0.5880131721496582 + 50.0 * 8.452191352844238
Epoch 660, val loss: 0.6061272621154785
Epoch 670, training loss: 423.0655517578125 = 0.5833045244216919 + 50.0 * 8.449645042419434
Epoch 670, val loss: 0.6017109751701355
Epoch 680, training loss: 422.950927734375 = 0.5788686275482178 + 50.0 * 8.447441101074219
Epoch 680, val loss: 0.5975297689437866
Epoch 690, training loss: 422.91461181640625 = 0.5746537446975708 + 50.0 * 8.446799278259277
Epoch 690, val loss: 0.5935701727867126
Epoch 700, training loss: 422.7972717285156 = 0.5706408619880676 + 50.0 * 8.44453239440918
Epoch 700, val loss: 0.5898089408874512
Epoch 710, training loss: 422.6285400390625 = 0.5668815970420837 + 50.0 * 8.441232681274414
Epoch 710, val loss: 0.5863029956817627
Epoch 720, training loss: 422.51776123046875 = 0.5633480548858643 + 50.0 * 8.439087867736816
Epoch 720, val loss: 0.5829952955245972
Epoch 730, training loss: 422.58770751953125 = 0.5600206851959229 + 50.0 * 8.440553665161133
Epoch 730, val loss: 0.5799084305763245
Epoch 740, training loss: 422.37347412109375 = 0.5567060708999634 + 50.0 * 8.436335563659668
Epoch 740, val loss: 0.5768314599990845
Epoch 750, training loss: 422.1759033203125 = 0.5536701679229736 + 50.0 * 8.43244457244873
Epoch 750, val loss: 0.574023962020874
Epoch 760, training loss: 422.0814208984375 = 0.5507596731185913 + 50.0 * 8.43061351776123
Epoch 760, val loss: 0.5713273286819458
Epoch 770, training loss: 422.1869201660156 = 0.5479333996772766 + 50.0 * 8.432779312133789
Epoch 770, val loss: 0.5687156915664673
Epoch 780, training loss: 421.92645263671875 = 0.5451658368110657 + 50.0 * 8.42762565612793
Epoch 780, val loss: 0.566176176071167
Epoch 790, training loss: 421.83795166015625 = 0.5425405502319336 + 50.0 * 8.425908088684082
Epoch 790, val loss: 0.5637750029563904
Epoch 800, training loss: 421.7511901855469 = 0.5400137305259705 + 50.0 * 8.424223899841309
Epoch 800, val loss: 0.561443567276001
Epoch 810, training loss: 421.8918762207031 = 0.5375676155090332 + 50.0 * 8.427085876464844
Epoch 810, val loss: 0.5591506958007812
Epoch 820, training loss: 421.6387023925781 = 0.5351802110671997 + 50.0 * 8.422070503234863
Epoch 820, val loss: 0.5570273399353027
Epoch 830, training loss: 421.58282470703125 = 0.5329456329345703 + 50.0 * 8.420997619628906
Epoch 830, val loss: 0.5550263524055481
Epoch 840, training loss: 421.53363037109375 = 0.5308338403701782 + 50.0 * 8.420056343078613
Epoch 840, val loss: 0.5531184673309326
Epoch 850, training loss: 421.59295654296875 = 0.528815507888794 + 50.0 * 8.421282768249512
Epoch 850, val loss: 0.5512979030609131
Epoch 860, training loss: 421.42864990234375 = 0.5268200635910034 + 50.0 * 8.418036460876465
Epoch 860, val loss: 0.5495028495788574
Epoch 870, training loss: 421.3457336425781 = 0.5250122547149658 + 50.0 * 8.416414260864258
Epoch 870, val loss: 0.5478845238685608
Epoch 880, training loss: 421.28662109375 = 0.5232717990875244 + 50.0 * 8.415266990661621
Epoch 880, val loss: 0.5463338494300842
Epoch 890, training loss: 421.2744445800781 = 0.5216299891471863 + 50.0 * 8.415056228637695
Epoch 890, val loss: 0.5449142456054688
Epoch 900, training loss: 421.19439697265625 = 0.5199970006942749 + 50.0 * 8.413488388061523
Epoch 900, val loss: 0.5433998107910156
Epoch 910, training loss: 421.1165466308594 = 0.518458902835846 + 50.0 * 8.411961555480957
Epoch 910, val loss: 0.5420939922332764
Epoch 920, training loss: 421.04815673828125 = 0.5170161724090576 + 50.0 * 8.410622596740723
Epoch 920, val loss: 0.5407945513725281
Epoch 930, training loss: 421.0591125488281 = 0.5155915021896362 + 50.0 * 8.410870552062988
Epoch 930, val loss: 0.5395578742027283
Epoch 940, training loss: 420.9088439941406 = 0.5142354369163513 + 50.0 * 8.407892227172852
Epoch 940, val loss: 0.5384197235107422
Epoch 950, training loss: 420.83758544921875 = 0.5129590630531311 + 50.0 * 8.406492233276367
Epoch 950, val loss: 0.5373517274856567
Epoch 960, training loss: 420.7450866699219 = 0.5117400884628296 + 50.0 * 8.404666900634766
Epoch 960, val loss: 0.5362827777862549
Epoch 970, training loss: 420.71038818359375 = 0.5105699896812439 + 50.0 * 8.403996467590332
Epoch 970, val loss: 0.5353120565414429
Epoch 980, training loss: 420.6168212890625 = 0.5093777179718018 + 50.0 * 8.402149200439453
Epoch 980, val loss: 0.5343014001846313
Epoch 990, training loss: 420.5740661621094 = 0.5082523226737976 + 50.0 * 8.40131664276123
Epoch 990, val loss: 0.533397912979126
Epoch 1000, training loss: 420.4796447753906 = 0.5071825981140137 + 50.0 * 8.399449348449707
Epoch 1000, val loss: 0.5324936509132385
Epoch 1010, training loss: 420.776123046875 = 0.5061583518981934 + 50.0 * 8.405399322509766
Epoch 1010, val loss: 0.5316627621650696
Epoch 1020, training loss: 420.3856201171875 = 0.505031406879425 + 50.0 * 8.397611618041992
Epoch 1020, val loss: 0.5306852459907532
Epoch 1030, training loss: 420.2784729003906 = 0.5040299892425537 + 50.0 * 8.395488739013672
Epoch 1030, val loss: 0.5299150943756104
Epoch 1040, training loss: 420.20306396484375 = 0.5030655264854431 + 50.0 * 8.394000053405762
Epoch 1040, val loss: 0.5291070342063904
Epoch 1050, training loss: 420.1333923339844 = 0.5021330118179321 + 50.0 * 8.392624855041504
Epoch 1050, val loss: 0.5283927917480469
Epoch 1060, training loss: 420.207763671875 = 0.501221776008606 + 50.0 * 8.39413070678711
Epoch 1060, val loss: 0.5277124047279358
Epoch 1070, training loss: 420.1109924316406 = 0.5002507567405701 + 50.0 * 8.39221477508545
Epoch 1070, val loss: 0.5268428921699524
Epoch 1080, training loss: 419.94366455078125 = 0.49933111667633057 + 50.0 * 8.388886451721191
Epoch 1080, val loss: 0.5261439681053162
Epoch 1090, training loss: 419.8907775878906 = 0.4984488785266876 + 50.0 * 8.387846946716309
Epoch 1090, val loss: 0.5254504084587097
Epoch 1100, training loss: 419.83978271484375 = 0.4975694715976715 + 50.0 * 8.386844635009766
Epoch 1100, val loss: 0.524730384349823
Epoch 1110, training loss: 419.89056396484375 = 0.49664175510406494 + 50.0 * 8.38787841796875
Epoch 1110, val loss: 0.5240136384963989
Epoch 1120, training loss: 419.7954406738281 = 0.49569645524024963 + 50.0 * 8.385994911193848
Epoch 1120, val loss: 0.5232990384101868
Epoch 1130, training loss: 419.6867370605469 = 0.494821697473526 + 50.0 * 8.383838653564453
Epoch 1130, val loss: 0.5226129293441772
Epoch 1140, training loss: 419.6348876953125 = 0.4939803183078766 + 50.0 * 8.382818222045898
Epoch 1140, val loss: 0.5219529271125793
Epoch 1150, training loss: 419.5831604003906 = 0.49314993619918823 + 50.0 * 8.381799697875977
Epoch 1150, val loss: 0.5213268399238586
Epoch 1160, training loss: 419.536865234375 = 0.49232247471809387 + 50.0 * 8.380890846252441
Epoch 1160, val loss: 0.520668625831604
Epoch 1170, training loss: 419.597412109375 = 0.49150604009628296 + 50.0 * 8.382118225097656
Epoch 1170, val loss: 0.5200223326683044
Epoch 1180, training loss: 419.5703430175781 = 0.49061504006385803 + 50.0 * 8.38159465789795
Epoch 1180, val loss: 0.5193833708763123
Epoch 1190, training loss: 419.4189147949219 = 0.48977774381637573 + 50.0 * 8.378582954406738
Epoch 1190, val loss: 0.5187422037124634
Epoch 1200, training loss: 419.3708801269531 = 0.48897865414619446 + 50.0 * 8.37763786315918
Epoch 1200, val loss: 0.5181095600128174
Epoch 1210, training loss: 419.3311767578125 = 0.48818522691726685 + 50.0 * 8.376859664916992
Epoch 1210, val loss: 0.5174835324287415
Epoch 1220, training loss: 419.40228271484375 = 0.487392395734787 + 50.0 * 8.378297805786133
Epoch 1220, val loss: 0.5168234705924988
Epoch 1230, training loss: 419.42864990234375 = 0.48654744029045105 + 50.0 * 8.3788423538208
Epoch 1230, val loss: 0.5162739753723145
Epoch 1240, training loss: 419.234375 = 0.48572948575019836 + 50.0 * 8.37497329711914
Epoch 1240, val loss: 0.5156811475753784
Epoch 1250, training loss: 419.1695251464844 = 0.48496338725090027 + 50.0 * 8.37369155883789
Epoch 1250, val loss: 0.5150730013847351
Epoch 1260, training loss: 419.1590270996094 = 0.48421528935432434 + 50.0 * 8.373496055603027
Epoch 1260, val loss: 0.5145324468612671
Epoch 1270, training loss: 419.2539978027344 = 0.4834541082382202 + 50.0 * 8.375411033630371
Epoch 1270, val loss: 0.5139427781105042
Epoch 1280, training loss: 419.09814453125 = 0.48265528678894043 + 50.0 * 8.372309684753418
Epoch 1280, val loss: 0.5132921934127808
Epoch 1290, training loss: 419.0279235839844 = 0.48191705346107483 + 50.0 * 8.370920181274414
Epoch 1290, val loss: 0.5127598643302917
Epoch 1300, training loss: 418.979248046875 = 0.48118457198143005 + 50.0 * 8.369961738586426
Epoch 1300, val loss: 0.512188732624054
Epoch 1310, training loss: 418.94183349609375 = 0.4804660975933075 + 50.0 * 8.369227409362793
Epoch 1310, val loss: 0.5116562247276306
Epoch 1320, training loss: 418.9322509765625 = 0.479746013879776 + 50.0 * 8.369050025939941
Epoch 1320, val loss: 0.5110957026481628
Epoch 1330, training loss: 418.9611511230469 = 0.47900187969207764 + 50.0 * 8.369643211364746
Epoch 1330, val loss: 0.5104905366897583
Epoch 1340, training loss: 418.9525451660156 = 0.4782505929470062 + 50.0 * 8.369485855102539
Epoch 1340, val loss: 0.5099126100540161
Epoch 1350, training loss: 418.84112548828125 = 0.4774986803531647 + 50.0 * 8.36727237701416
Epoch 1350, val loss: 0.5094199776649475
Epoch 1360, training loss: 418.78094482421875 = 0.47678154706954956 + 50.0 * 8.366083145141602
Epoch 1360, val loss: 0.5088419914245605
Epoch 1370, training loss: 418.73388671875 = 0.4760701358318329 + 50.0 * 8.365156173706055
Epoch 1370, val loss: 0.5083028078079224
Epoch 1380, training loss: 418.738525390625 = 0.4753595292568207 + 50.0 * 8.365262985229492
Epoch 1380, val loss: 0.5077322721481323
Epoch 1390, training loss: 418.726806640625 = 0.47461798787117004 + 50.0 * 8.365043640136719
Epoch 1390, val loss: 0.5071547627449036
Epoch 1400, training loss: 418.7809753417969 = 0.4738529324531555 + 50.0 * 8.366142272949219
Epoch 1400, val loss: 0.506550669670105
Epoch 1410, training loss: 418.63800048828125 = 0.4731139540672302 + 50.0 * 8.363297462463379
Epoch 1410, val loss: 0.5060597062110901
Epoch 1420, training loss: 418.60723876953125 = 0.4724026620388031 + 50.0 * 8.362696647644043
Epoch 1420, val loss: 0.5055544376373291
Epoch 1430, training loss: 418.56280517578125 = 0.4716976583003998 + 50.0 * 8.361822128295898
Epoch 1430, val loss: 0.5049803256988525
Epoch 1440, training loss: 418.5614929199219 = 0.4709987938404083 + 50.0 * 8.361809730529785
Epoch 1440, val loss: 0.5044463276863098
Epoch 1450, training loss: 418.5894775390625 = 0.4702661633491516 + 50.0 * 8.362383842468262
Epoch 1450, val loss: 0.5039108395576477
Epoch 1460, training loss: 418.5160827636719 = 0.46952691674232483 + 50.0 * 8.360931396484375
Epoch 1460, val loss: 0.50339275598526
Epoch 1470, training loss: 418.56512451171875 = 0.4688025116920471 + 50.0 * 8.361926078796387
Epoch 1470, val loss: 0.5028548240661621
Epoch 1480, training loss: 418.43017578125 = 0.4680602252483368 + 50.0 * 8.35924243927002
Epoch 1480, val loss: 0.5022447109222412
Epoch 1490, training loss: 418.4349060058594 = 0.4673456847667694 + 50.0 * 8.35935115814209
Epoch 1490, val loss: 0.5016394853591919
Epoch 1500, training loss: 418.41070556640625 = 0.46663522720336914 + 50.0 * 8.358880996704102
Epoch 1500, val loss: 0.5011341571807861
Epoch 1510, training loss: 418.5023193359375 = 0.4659145176410675 + 50.0 * 8.36072826385498
Epoch 1510, val loss: 0.5005590319633484
Epoch 1520, training loss: 418.4898376464844 = 0.46515488624572754 + 50.0 * 8.360493659973145
Epoch 1520, val loss: 0.5001394748687744
Epoch 1530, training loss: 418.3442687988281 = 0.4643961787223816 + 50.0 * 8.357597351074219
Epoch 1530, val loss: 0.4994313716888428
Epoch 1540, training loss: 418.28680419921875 = 0.4636758267879486 + 50.0 * 8.356462478637695
Epoch 1540, val loss: 0.49889007210731506
Epoch 1550, training loss: 418.2818908691406 = 0.4629666209220886 + 50.0 * 8.356378555297852
Epoch 1550, val loss: 0.4983741343021393
Epoch 1560, training loss: 418.3431701660156 = 0.4622438848018646 + 50.0 * 8.35761833190918
Epoch 1560, val loss: 0.4977676272392273
Epoch 1570, training loss: 418.2822265625 = 0.4614887833595276 + 50.0 * 8.356414794921875
Epoch 1570, val loss: 0.4972102642059326
Epoch 1580, training loss: 418.27545166015625 = 0.46073105931282043 + 50.0 * 8.356294631958008
Epoch 1580, val loss: 0.49659910798072815
Epoch 1590, training loss: 418.2060546875 = 0.4599848985671997 + 50.0 * 8.354921340942383
Epoch 1590, val loss: 0.49614015221595764
Epoch 1600, training loss: 418.16961669921875 = 0.4592455327510834 + 50.0 * 8.354207038879395
Epoch 1600, val loss: 0.49552810192108154
Epoch 1610, training loss: 418.1391296386719 = 0.45851191878318787 + 50.0 * 8.353611946105957
Epoch 1610, val loss: 0.49498042464256287
Epoch 1620, training loss: 418.1827087402344 = 0.4577733278274536 + 50.0 * 8.354498863220215
Epoch 1620, val loss: 0.49444496631622314
Epoch 1630, training loss: 418.157958984375 = 0.45699572563171387 + 50.0 * 8.354019165039062
Epoch 1630, val loss: 0.4938364028930664
Epoch 1640, training loss: 418.19732666015625 = 0.4562198221683502 + 50.0 * 8.354822158813477
Epoch 1640, val loss: 0.4932897984981537
Epoch 1650, training loss: 418.1022644042969 = 0.45541855692863464 + 50.0 * 8.352936744689941
Epoch 1650, val loss: 0.49260276556015015
Epoch 1660, training loss: 418.0727233886719 = 0.4546472132205963 + 50.0 * 8.352361679077148
Epoch 1660, val loss: 0.49196290969848633
Epoch 1670, training loss: 418.0270080566406 = 0.45388737320899963 + 50.0 * 8.351462364196777
Epoch 1670, val loss: 0.4914543330669403
Epoch 1680, training loss: 418.0146179199219 = 0.4531216323375702 + 50.0 * 8.351229667663574
Epoch 1680, val loss: 0.4908764958381653
Epoch 1690, training loss: 418.1024475097656 = 0.4523453414440155 + 50.0 * 8.353002548217773
Epoch 1690, val loss: 0.49038833379745483
Epoch 1700, training loss: 418.0188903808594 = 0.4515035152435303 + 50.0 * 8.351347923278809
Epoch 1700, val loss: 0.48964613676071167
Epoch 1710, training loss: 417.9700012207031 = 0.45067182183265686 + 50.0 * 8.350386619567871
Epoch 1710, val loss: 0.4890229105949402
Epoch 1720, training loss: 417.94805908203125 = 0.44986817240715027 + 50.0 * 8.349964141845703
Epoch 1720, val loss: 0.48841843008995056
Epoch 1730, training loss: 417.96435546875 = 0.4490663707256317 + 50.0 * 8.350305557250977
Epoch 1730, val loss: 0.48783451318740845
Epoch 1740, training loss: 417.95489501953125 = 0.4482492506504059 + 50.0 * 8.350132942199707
Epoch 1740, val loss: 0.4872327744960785
Epoch 1750, training loss: 417.9832458496094 = 0.44742926955223083 + 50.0 * 8.350716590881348
Epoch 1750, val loss: 0.4865458309650421
Epoch 1760, training loss: 417.9938049316406 = 0.44657889008522034 + 50.0 * 8.350944519042969
Epoch 1760, val loss: 0.4859602451324463
Epoch 1770, training loss: 417.89593505859375 = 0.44573476910591125 + 50.0 * 8.349003791809082
Epoch 1770, val loss: 0.48533159494400024
Epoch 1780, training loss: 417.8639831542969 = 0.4449058175086975 + 50.0 * 8.348381042480469
Epoch 1780, val loss: 0.4847085773944855
Epoch 1790, training loss: 417.8622741699219 = 0.44408494234085083 + 50.0 * 8.348363876342773
Epoch 1790, val loss: 0.48414096236228943
Epoch 1800, training loss: 418.07025146484375 = 0.44324105978012085 + 50.0 * 8.352540016174316
Epoch 1800, val loss: 0.4835456311702728
Epoch 1810, training loss: 417.8828125 = 0.4423488676548004 + 50.0 * 8.348809242248535
Epoch 1810, val loss: 0.4827632009983063
Epoch 1820, training loss: 417.8133544921875 = 0.4414888024330139 + 50.0 * 8.347436904907227
Epoch 1820, val loss: 0.48214709758758545
Epoch 1830, training loss: 417.78692626953125 = 0.44064420461654663 + 50.0 * 8.346925735473633
Epoch 1830, val loss: 0.48150634765625
Epoch 1840, training loss: 417.7861633300781 = 0.439790815114975 + 50.0 * 8.346927642822266
Epoch 1840, val loss: 0.48087069392204285
Epoch 1850, training loss: 418.0016784667969 = 0.4389154314994812 + 50.0 * 8.351255416870117
Epoch 1850, val loss: 0.48012813925743103
Epoch 1860, training loss: 417.81597900390625 = 0.4380027651786804 + 50.0 * 8.347559928894043
Epoch 1860, val loss: 0.47963571548461914
Epoch 1870, training loss: 417.7620849609375 = 0.4371039569377899 + 50.0 * 8.3464994430542
Epoch 1870, val loss: 0.4788408577442169
Epoch 1880, training loss: 417.763427734375 = 0.4362260401248932 + 50.0 * 8.34654426574707
Epoch 1880, val loss: 0.4782294034957886
Epoch 1890, training loss: 417.7537841796875 = 0.43532103300094604 + 50.0 * 8.346368789672852
Epoch 1890, val loss: 0.47753050923347473
Epoch 1900, training loss: 417.7117919921875 = 0.43441107869148254 + 50.0 * 8.345547676086426
Epoch 1900, val loss: 0.4769431948661804
Epoch 1910, training loss: 417.6940612792969 = 0.43350788950920105 + 50.0 * 8.345211029052734
Epoch 1910, val loss: 0.4762638509273529
Epoch 1920, training loss: 417.8171691894531 = 0.43259474635124207 + 50.0 * 8.347691535949707
Epoch 1920, val loss: 0.4755556881427765
Epoch 1930, training loss: 417.6865539550781 = 0.43163588643074036 + 50.0 * 8.345098495483398
Epoch 1930, val loss: 0.4748566448688507
Epoch 1940, training loss: 417.6675109863281 = 0.4306994378566742 + 50.0 * 8.344736099243164
Epoch 1940, val loss: 0.4741712212562561
Epoch 1950, training loss: 417.6700744628906 = 0.4297657608985901 + 50.0 * 8.344805717468262
Epoch 1950, val loss: 0.4734841287136078
Epoch 1960, training loss: 417.6221008300781 = 0.4288307726383209 + 50.0 * 8.343865394592285
Epoch 1960, val loss: 0.47283434867858887
Epoch 1970, training loss: 417.65667724609375 = 0.4278976023197174 + 50.0 * 8.344575881958008
Epoch 1970, val loss: 0.4720996618270874
Epoch 1980, training loss: 417.7191467285156 = 0.4269298315048218 + 50.0 * 8.345844268798828
Epoch 1980, val loss: 0.4713900089263916
Epoch 1990, training loss: 417.6007080078125 = 0.425942599773407 + 50.0 * 8.34349536895752
Epoch 1990, val loss: 0.4708206057548523
Epoch 2000, training loss: 417.571044921875 = 0.4249868392944336 + 50.0 * 8.342921257019043
Epoch 2000, val loss: 0.4701017141342163
Epoch 2010, training loss: 417.5624694824219 = 0.42403608560562134 + 50.0 * 8.342768669128418
Epoch 2010, val loss: 0.46943554282188416
Epoch 2020, training loss: 417.6739196777344 = 0.4230816066265106 + 50.0 * 8.345016479492188
Epoch 2020, val loss: 0.4688018262386322
Epoch 2030, training loss: 417.5286560058594 = 0.42207369208335876 + 50.0 * 8.342131614685059
Epoch 2030, val loss: 0.4680747091770172
Epoch 2040, training loss: 417.5118408203125 = 0.4210861921310425 + 50.0 * 8.341814994812012
Epoch 2040, val loss: 0.46732765436172485
Epoch 2050, training loss: 417.501708984375 = 0.42011407017707825 + 50.0 * 8.341631889343262
Epoch 2050, val loss: 0.4666602909564972
Epoch 2060, training loss: 417.721435546875 = 0.41913041472435 + 50.0 * 8.346046447753906
Epoch 2060, val loss: 0.4658765196800232
Epoch 2070, training loss: 417.5671081542969 = 0.41809317469596863 + 50.0 * 8.34298038482666
Epoch 2070, val loss: 0.4653129279613495
Epoch 2080, training loss: 417.4913330078125 = 0.417076975107193 + 50.0 * 8.341485023498535
Epoch 2080, val loss: 0.464555561542511
Epoch 2090, training loss: 417.4451599121094 = 0.41608697175979614 + 50.0 * 8.340581893920898
Epoch 2090, val loss: 0.4638751745223999
Epoch 2100, training loss: 417.4356689453125 = 0.41509485244750977 + 50.0 * 8.340411186218262
Epoch 2100, val loss: 0.46321266889572144
Epoch 2110, training loss: 417.5651550292969 = 0.41408807039260864 + 50.0 * 8.343021392822266
Epoch 2110, val loss: 0.4625838100910187
Epoch 2120, training loss: 417.49676513671875 = 0.41302862763404846 + 50.0 * 8.3416748046875
Epoch 2120, val loss: 0.46178552508354187
Epoch 2130, training loss: 417.42108154296875 = 0.41196832060813904 + 50.0 * 8.340182304382324
Epoch 2130, val loss: 0.46100953221321106
Epoch 2140, training loss: 417.3895568847656 = 0.41094037890434265 + 50.0 * 8.339571952819824
Epoch 2140, val loss: 0.46028101444244385
Epoch 2150, training loss: 417.37158203125 = 0.4099136292934418 + 50.0 * 8.3392333984375
Epoch 2150, val loss: 0.45958229899406433
Epoch 2160, training loss: 417.47955322265625 = 0.4088759422302246 + 50.0 * 8.341413497924805
Epoch 2160, val loss: 0.45882710814476013
Epoch 2170, training loss: 417.3857421875 = 0.4077877104282379 + 50.0 * 8.339559555053711
Epoch 2170, val loss: 0.458111435174942
Epoch 2180, training loss: 417.3484802246094 = 0.4067121744155884 + 50.0 * 8.338835716247559
Epoch 2180, val loss: 0.4573532044887543
Epoch 2190, training loss: 417.3414001464844 = 0.4056563675403595 + 50.0 * 8.338714599609375
Epoch 2190, val loss: 0.4566943347454071
Epoch 2200, training loss: 417.4492492675781 = 0.4045984745025635 + 50.0 * 8.340892791748047
Epoch 2200, val loss: 0.45601871609687805
Epoch 2210, training loss: 417.3103332519531 = 0.40349122881889343 + 50.0 * 8.338136672973633
Epoch 2210, val loss: 0.4551156461238861
Epoch 2220, training loss: 417.2778015136719 = 0.40240776538848877 + 50.0 * 8.337508201599121
Epoch 2220, val loss: 0.45440995693206787
Epoch 2230, training loss: 417.2767333984375 = 0.4013322591781616 + 50.0 * 8.337508201599121
Epoch 2230, val loss: 0.4536891579627991
Epoch 2240, training loss: 417.4340515136719 = 0.40024176239967346 + 50.0 * 8.340676307678223
Epoch 2240, val loss: 0.45301553606987
Epoch 2250, training loss: 417.3441467285156 = 0.3990958333015442 + 50.0 * 8.338900566101074
Epoch 2250, val loss: 0.45218273997306824
Epoch 2260, training loss: 417.2700500488281 = 0.3979656398296356 + 50.0 * 8.337441444396973
Epoch 2260, val loss: 0.4514044225215912
Epoch 2270, training loss: 417.2319030761719 = 0.3968755006790161 + 50.0 * 8.336700439453125
Epoch 2270, val loss: 0.4506399631500244
Epoch 2280, training loss: 417.2059020996094 = 0.39578381180763245 + 50.0 * 8.336202621459961
Epoch 2280, val loss: 0.4499194324016571
Epoch 2290, training loss: 417.2221374511719 = 0.39468666911125183 + 50.0 * 8.336548805236816
Epoch 2290, val loss: 0.4491289258003235
Epoch 2300, training loss: 417.40911865234375 = 0.39355555176734924 + 50.0 * 8.340311050415039
Epoch 2300, val loss: 0.4483148157596588
Epoch 2310, training loss: 417.2360534667969 = 0.39239808917045593 + 50.0 * 8.336873054504395
Epoch 2310, val loss: 0.4477323591709137
Epoch 2320, training loss: 417.1778259277344 = 0.3912613093852997 + 50.0 * 8.335731506347656
Epoch 2320, val loss: 0.4469214677810669
Epoch 2330, training loss: 417.1557922363281 = 0.3901435434818268 + 50.0 * 8.335312843322754
Epoch 2330, val loss: 0.4462392032146454
Epoch 2340, training loss: 417.2389221191406 = 0.3890107572078705 + 50.0 * 8.336997985839844
Epoch 2340, val loss: 0.4455479681491852
Epoch 2350, training loss: 417.221923828125 = 0.38783836364746094 + 50.0 * 8.336681365966797
Epoch 2350, val loss: 0.44468140602111816
Epoch 2360, training loss: 417.1229248046875 = 0.3866516351699829 + 50.0 * 8.334725379943848
Epoch 2360, val loss: 0.44404396414756775
Epoch 2370, training loss: 417.11676025390625 = 0.38550570607185364 + 50.0 * 8.334625244140625
Epoch 2370, val loss: 0.4432958662509918
Epoch 2380, training loss: 417.0883483886719 = 0.3843670189380646 + 50.0 * 8.33407974243164
Epoch 2380, val loss: 0.4426116347312927
Epoch 2390, training loss: 417.0784912109375 = 0.38322553038597107 + 50.0 * 8.333905220031738
Epoch 2390, val loss: 0.4418922960758209
Epoch 2400, training loss: 417.1101989746094 = 0.3820734918117523 + 50.0 * 8.334562301635742
Epoch 2400, val loss: 0.4411468803882599
Epoch 2410, training loss: 417.1616516113281 = 0.38089272379875183 + 50.0 * 8.335615158081055
Epoch 2410, val loss: 0.4403437674045563
Epoch 2420, training loss: 417.2252502441406 = 0.37969905138015747 + 50.0 * 8.33691120147705
Epoch 2420, val loss: 0.43954476714134216
Epoch 2430, training loss: 417.08807373046875 = 0.37849748134613037 + 50.0 * 8.33419132232666
Epoch 2430, val loss: 0.43889379501342773
Epoch 2440, training loss: 417.05078125 = 0.37732648849487305 + 50.0 * 8.33346939086914
Epoch 2440, val loss: 0.43824413418769836
Epoch 2450, training loss: 417.00933837890625 = 0.37616682052612305 + 50.0 * 8.332663536071777
Epoch 2450, val loss: 0.43748992681503296
Epoch 2460, training loss: 417.00311279296875 = 0.3750132620334625 + 50.0 * 8.332562446594238
Epoch 2460, val loss: 0.43678373098373413
Epoch 2470, training loss: 417.0255126953125 = 0.37385401129722595 + 50.0 * 8.333033561706543
Epoch 2470, val loss: 0.4360637664794922
Epoch 2480, training loss: 417.0853576660156 = 0.37267109751701355 + 50.0 * 8.334253311157227
Epoch 2480, val loss: 0.43541187047958374
Epoch 2490, training loss: 417.14776611328125 = 0.37148725986480713 + 50.0 * 8.335525512695312
Epoch 2490, val loss: 0.43484222888946533
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8244545915778791
0.863507933058031
=== training gcn model ===
Epoch 0, training loss: 530.1986083984375 = 1.0845279693603516 + 50.0 * 10.582282066345215
Epoch 0, val loss: 1.0857951641082764
Epoch 10, training loss: 530.1702880859375 = 1.0815633535385132 + 50.0 * 10.581774711608887
Epoch 10, val loss: 1.0827388763427734
Epoch 20, training loss: 530.0252075195312 = 1.0783215761184692 + 50.0 * 10.578938484191895
Epoch 20, val loss: 1.0793819427490234
Epoch 30, training loss: 529.321044921875 = 1.0746660232543945 + 50.0 * 10.56492805480957
Epoch 30, val loss: 1.075576901435852
Epoch 40, training loss: 526.7913208007812 = 1.0703564882278442 + 50.0 * 10.514418601989746
Epoch 40, val loss: 1.0710618495941162
Epoch 50, training loss: 520.360595703125 = 1.065278172492981 + 50.0 * 10.385906219482422
Epoch 50, val loss: 1.0657858848571777
Epoch 60, training loss: 510.3066101074219 = 1.059615969657898 + 50.0 * 10.184940338134766
Epoch 60, val loss: 1.0598094463348389
Epoch 70, training loss: 502.3651123046875 = 1.0526984930038452 + 50.0 * 10.02624797821045
Epoch 70, val loss: 1.0526984930038452
Epoch 80, training loss: 490.6132507324219 = 1.0466406345367432 + 50.0 * 9.791332244873047
Epoch 80, val loss: 1.0468403100967407
Epoch 90, training loss: 475.2919616699219 = 1.0423680543899536 + 50.0 * 9.484992027282715
Epoch 90, val loss: 1.042944073677063
Epoch 100, training loss: 463.1397705078125 = 1.0403612852096558 + 50.0 * 9.241988182067871
Epoch 100, val loss: 1.041347861289978
Epoch 110, training loss: 458.037109375 = 1.0390843152999878 + 50.0 * 9.139960289001465
Epoch 110, val loss: 1.0400424003601074
Epoch 120, training loss: 454.5494384765625 = 1.0358151197433472 + 50.0 * 9.070272445678711
Epoch 120, val loss: 1.0365879535675049
Epoch 130, training loss: 449.14422607421875 = 1.031921148300171 + 50.0 * 8.96224594116211
Epoch 130, val loss: 1.0327287912368774
Epoch 140, training loss: 444.1469421386719 = 1.0294159650802612 + 50.0 * 8.862350463867188
Epoch 140, val loss: 1.0303107500076294
Epoch 150, training loss: 441.87335205078125 = 1.027266025543213 + 50.0 * 8.816922187805176
Epoch 150, val loss: 1.0280117988586426
Epoch 160, training loss: 439.94464111328125 = 1.0237151384353638 + 50.0 * 8.77841854095459
Epoch 160, val loss: 1.0243618488311768
Epoch 170, training loss: 438.0694885253906 = 1.0196306705474854 + 50.0 * 8.740997314453125
Epoch 170, val loss: 1.0203396081924438
Epoch 180, training loss: 436.34954833984375 = 1.0157301425933838 + 50.0 * 8.706676483154297
Epoch 180, val loss: 1.0165553092956543
Epoch 190, training loss: 435.0918273925781 = 1.0116746425628662 + 50.0 * 8.681602478027344
Epoch 190, val loss: 1.012629747390747
Epoch 200, training loss: 434.0005798339844 = 1.0070914030075073 + 50.0 * 8.659870147705078
Epoch 200, val loss: 1.0081205368041992
Epoch 210, training loss: 433.0408935546875 = 1.0019656419754028 + 50.0 * 8.640778541564941
Epoch 210, val loss: 1.0030773878097534
Epoch 220, training loss: 432.20428466796875 = 0.9964316487312317 + 50.0 * 8.624156951904297
Epoch 220, val loss: 0.997681736946106
Epoch 230, training loss: 431.5290832519531 = 0.9904745817184448 + 50.0 * 8.610772132873535
Epoch 230, val loss: 0.9919065237045288
Epoch 240, training loss: 430.87542724609375 = 0.9840776324272156 + 50.0 * 8.597826957702637
Epoch 240, val loss: 0.9856718182563782
Epoch 250, training loss: 430.355224609375 = 0.9772269129753113 + 50.0 * 8.587559700012207
Epoch 250, val loss: 0.9790175557136536
Epoch 260, training loss: 429.9407653808594 = 0.9698985815048218 + 50.0 * 8.57941722869873
Epoch 260, val loss: 0.971911609172821
Epoch 270, training loss: 429.5782165527344 = 0.9621291160583496 + 50.0 * 8.572321891784668
Epoch 270, val loss: 0.9643790125846863
Epoch 280, training loss: 429.3830871582031 = 0.9539281129837036 + 50.0 * 8.568583488464355
Epoch 280, val loss: 0.9564350247383118
Epoch 290, training loss: 429.0105285644531 = 0.9453027248382568 + 50.0 * 8.561304092407227
Epoch 290, val loss: 0.9481370449066162
Epoch 300, training loss: 428.6755676269531 = 0.9364268183708191 + 50.0 * 8.55478286743164
Epoch 300, val loss: 0.9395633935928345
Epoch 310, training loss: 428.565673828125 = 0.9272837042808533 + 50.0 * 8.552767753601074
Epoch 310, val loss: 0.930730938911438
Epoch 320, training loss: 428.1161193847656 = 0.9178359508514404 + 50.0 * 8.543965339660645
Epoch 320, val loss: 0.9216632843017578
Epoch 330, training loss: 427.781005859375 = 0.9082321524620056 + 50.0 * 8.537455558776855
Epoch 330, val loss: 0.9124419689178467
Epoch 340, training loss: 427.48486328125 = 0.8984484076499939 + 50.0 * 8.531728744506836
Epoch 340, val loss: 0.9030462503433228
Epoch 350, training loss: 427.23876953125 = 0.8884596824645996 + 50.0 * 8.527006149291992
Epoch 350, val loss: 0.8934577107429504
Epoch 360, training loss: 426.86785888671875 = 0.8783259987831116 + 50.0 * 8.519790649414062
Epoch 360, val loss: 0.8837488293647766
Epoch 370, training loss: 426.5631103515625 = 0.868128776550293 + 50.0 * 8.513899803161621
Epoch 370, val loss: 0.8739412426948547
Epoch 380, training loss: 426.2637939453125 = 0.8578312397003174 + 50.0 * 8.508119583129883
Epoch 380, val loss: 0.8640733957290649
Epoch 390, training loss: 426.1607360839844 = 0.8474502563476562 + 50.0 * 8.506265640258789
Epoch 390, val loss: 0.8541380167007446
Epoch 400, training loss: 425.87176513671875 = 0.8369508385658264 + 50.0 * 8.500696182250977
Epoch 400, val loss: 0.8440336585044861
Epoch 410, training loss: 425.5256042480469 = 0.8264368176460266 + 50.0 * 8.493983268737793
Epoch 410, val loss: 0.8339418768882751
Epoch 420, training loss: 425.247802734375 = 0.8159136772155762 + 50.0 * 8.488637924194336
Epoch 420, val loss: 0.8238905668258667
Epoch 430, training loss: 425.04071044921875 = 0.8053683042526245 + 50.0 * 8.48470687866211
Epoch 430, val loss: 0.8137990236282349
Epoch 440, training loss: 424.7822570800781 = 0.7947311401367188 + 50.0 * 8.479750633239746
Epoch 440, val loss: 0.8035875558853149
Epoch 450, training loss: 424.5685119628906 = 0.7840710878372192 + 50.0 * 8.475688934326172
Epoch 450, val loss: 0.793417751789093
Epoch 460, training loss: 424.64361572265625 = 0.7733759880065918 + 50.0 * 8.477404594421387
Epoch 460, val loss: 0.7831818461418152
Epoch 470, training loss: 424.24517822265625 = 0.762582004070282 + 50.0 * 8.46965217590332
Epoch 470, val loss: 0.7729213237762451
Epoch 480, training loss: 423.9756164550781 = 0.7518299221992493 + 50.0 * 8.464475631713867
Epoch 480, val loss: 0.7626943588256836
Epoch 490, training loss: 423.7842712402344 = 0.7411457896232605 + 50.0 * 8.460862159729004
Epoch 490, val loss: 0.7525368332862854
Epoch 500, training loss: 424.072998046875 = 0.730475127696991 + 50.0 * 8.466850280761719
Epoch 500, val loss: 0.7423256039619446
Epoch 510, training loss: 423.644287109375 = 0.7197360396385193 + 50.0 * 8.458491325378418
Epoch 510, val loss: 0.7321851849555969
Epoch 520, training loss: 423.3139343261719 = 0.7091266512870789 + 50.0 * 8.452095985412598
Epoch 520, val loss: 0.7221797704696655
Epoch 530, training loss: 423.1937561035156 = 0.6986504793167114 + 50.0 * 8.449902534484863
Epoch 530, val loss: 0.7122777700424194
Epoch 540, training loss: 423.0501403808594 = 0.6882230043411255 + 50.0 * 8.447237968444824
Epoch 540, val loss: 0.702455997467041
Epoch 550, training loss: 422.9287414550781 = 0.6778852939605713 + 50.0 * 8.445016860961914
Epoch 550, val loss: 0.6927451491355896
Epoch 560, training loss: 422.9966735839844 = 0.6676726341247559 + 50.0 * 8.446579933166504
Epoch 560, val loss: 0.6832066178321838
Epoch 570, training loss: 422.849365234375 = 0.657555341720581 + 50.0 * 8.443836212158203
Epoch 570, val loss: 0.673654317855835
Epoch 580, training loss: 422.621337890625 = 0.6476827263832092 + 50.0 * 8.439473152160645
Epoch 580, val loss: 0.6645019054412842
Epoch 590, training loss: 422.5474548339844 = 0.638039767742157 + 50.0 * 8.438188552856445
Epoch 590, val loss: 0.655547559261322
Epoch 600, training loss: 422.4432678222656 = 0.6286231279373169 + 50.0 * 8.43629264831543
Epoch 600, val loss: 0.6468167901039124
Epoch 610, training loss: 422.40435791015625 = 0.619476318359375 + 50.0 * 8.435697555541992
Epoch 610, val loss: 0.6383613348007202
Epoch 620, training loss: 422.3677062988281 = 0.6105496287345886 + 50.0 * 8.43514347076416
Epoch 620, val loss: 0.6301801800727844
Epoch 630, training loss: 422.1988525390625 = 0.6019382476806641 + 50.0 * 8.431938171386719
Epoch 630, val loss: 0.6223474144935608
Epoch 640, training loss: 422.12640380859375 = 0.5936998128890991 + 50.0 * 8.430654525756836
Epoch 640, val loss: 0.6148406267166138
Epoch 650, training loss: 422.0500793457031 = 0.5857988595962524 + 50.0 * 8.429286003112793
Epoch 650, val loss: 0.6076861023902893
Epoch 660, training loss: 422.1649169921875 = 0.578208327293396 + 50.0 * 8.431734085083008
Epoch 660, val loss: 0.6008169054985046
Epoch 670, training loss: 421.9261779785156 = 0.5709002017974854 + 50.0 * 8.427105903625488
Epoch 670, val loss: 0.5943920016288757
Epoch 680, training loss: 421.8345947265625 = 0.5639983415603638 + 50.0 * 8.42541217803955
Epoch 680, val loss: 0.5882730484008789
Epoch 690, training loss: 421.81884765625 = 0.5574056506156921 + 50.0 * 8.4252290725708
Epoch 690, val loss: 0.5824601650238037
Epoch 700, training loss: 421.6602478027344 = 0.551154613494873 + 50.0 * 8.422182083129883
Epoch 700, val loss: 0.5769201517105103
Epoch 710, training loss: 421.5854797363281 = 0.5452236533164978 + 50.0 * 8.420804977416992
Epoch 710, val loss: 0.5718002319335938
Epoch 720, training loss: 421.6745910644531 = 0.5395859479904175 + 50.0 * 8.422699928283691
Epoch 720, val loss: 0.5669068098068237
Epoch 730, training loss: 421.49310302734375 = 0.5342152118682861 + 50.0 * 8.419178009033203
Epoch 730, val loss: 0.5623076558113098
Epoch 740, training loss: 421.3813171386719 = 0.5291576981544495 + 50.0 * 8.417043685913086
Epoch 740, val loss: 0.558020830154419
Epoch 750, training loss: 421.4394226074219 = 0.5243511199951172 + 50.0 * 8.418301582336426
Epoch 750, val loss: 0.5539047122001648
Epoch 760, training loss: 421.2461242675781 = 0.5197722315788269 + 50.0 * 8.41452693939209
Epoch 760, val loss: 0.5501782298088074
Epoch 770, training loss: 421.14739990234375 = 0.5154905915260315 + 50.0 * 8.412637710571289
Epoch 770, val loss: 0.5466240644454956
Epoch 780, training loss: 421.16064453125 = 0.5114197134971619 + 50.0 * 8.412984848022461
Epoch 780, val loss: 0.5432462096214294
Epoch 790, training loss: 421.0536193847656 = 0.5075095295906067 + 50.0 * 8.410922050476074
Epoch 790, val loss: 0.5401408672332764
Epoch 800, training loss: 420.95111083984375 = 0.503851056098938 + 50.0 * 8.408945083618164
Epoch 800, val loss: 0.5371726155281067
Epoch 810, training loss: 420.8885498046875 = 0.5003702640533447 + 50.0 * 8.407763481140137
Epoch 810, val loss: 0.5344076156616211
Epoch 820, training loss: 421.0179443359375 = 0.4970206618309021 + 50.0 * 8.410418510437012
Epoch 820, val loss: 0.5317751169204712
Epoch 830, training loss: 420.75054931640625 = 0.4938396215438843 + 50.0 * 8.405134201049805
Epoch 830, val loss: 0.5292623043060303
Epoch 840, training loss: 420.73223876953125 = 0.49085861444473267 + 50.0 * 8.404828071594238
Epoch 840, val loss: 0.5268693566322327
Epoch 850, training loss: 420.6472473144531 = 0.48801088333129883 + 50.0 * 8.40318489074707
Epoch 850, val loss: 0.5247269868850708
Epoch 860, training loss: 420.7413635253906 = 0.48531052470207214 + 50.0 * 8.405120849609375
Epoch 860, val loss: 0.5226054191589355
Epoch 870, training loss: 420.68792724609375 = 0.4826613962650299 + 50.0 * 8.404105186462402
Epoch 870, val loss: 0.5206454396247864
Epoch 880, training loss: 420.4881896972656 = 0.48017820715904236 + 50.0 * 8.400160789489746
Epoch 880, val loss: 0.5187642574310303
Epoch 890, training loss: 420.4458923339844 = 0.47781139612197876 + 50.0 * 8.399361610412598
Epoch 890, val loss: 0.5170024037361145
Epoch 900, training loss: 420.3797302246094 = 0.47554370760917664 + 50.0 * 8.398083686828613
Epoch 900, val loss: 0.5153178572654724
Epoch 910, training loss: 420.48797607421875 = 0.4733601212501526 + 50.0 * 8.40029239654541
Epoch 910, val loss: 0.5137407779693604
Epoch 920, training loss: 420.5126647949219 = 0.47119763493537903 + 50.0 * 8.400829315185547
Epoch 920, val loss: 0.5121796131134033
Epoch 930, training loss: 420.32861328125 = 0.4691427946090698 + 50.0 * 8.397189140319824
Epoch 930, val loss: 0.5106801986694336
Epoch 940, training loss: 420.2095642089844 = 0.4672119617462158 + 50.0 * 8.39484691619873
Epoch 940, val loss: 0.50931715965271
Epoch 950, training loss: 420.1422424316406 = 0.4653553366661072 + 50.0 * 8.393537521362305
Epoch 950, val loss: 0.5079902410507202
Epoch 960, training loss: 420.0984191894531 = 0.46355971693992615 + 50.0 * 8.39269733428955
Epoch 960, val loss: 0.5067399740219116
Epoch 970, training loss: 420.1158142089844 = 0.46182653307914734 + 50.0 * 8.39307975769043
Epoch 970, val loss: 0.5055068731307983
Epoch 980, training loss: 420.203369140625 = 0.46007928252220154 + 50.0 * 8.394865989685059
Epoch 980, val loss: 0.5043169856071472
Epoch 990, training loss: 419.9964294433594 = 0.458401083946228 + 50.0 * 8.39076042175293
Epoch 990, val loss: 0.5032364726066589
Epoch 1000, training loss: 419.9325256347656 = 0.45682159066200256 + 50.0 * 8.389513969421387
Epoch 1000, val loss: 0.5021326541900635
Epoch 1010, training loss: 419.8897399902344 = 0.4552995562553406 + 50.0 * 8.388689041137695
Epoch 1010, val loss: 0.5011063814163208
Epoch 1020, training loss: 419.9875183105469 = 0.45381781458854675 + 50.0 * 8.390673637390137
Epoch 1020, val loss: 0.5000417828559875
Epoch 1030, training loss: 419.86212158203125 = 0.4523191750049591 + 50.0 * 8.388195991516113
Epoch 1030, val loss: 0.49920156598091125
Epoch 1040, training loss: 419.8225402832031 = 0.4508920907974243 + 50.0 * 8.387433052062988
Epoch 1040, val loss: 0.4982040822505951
Epoch 1050, training loss: 419.74102783203125 = 0.44952166080474854 + 50.0 * 8.38582992553711
Epoch 1050, val loss: 0.497320294380188
Epoch 1060, training loss: 419.69830322265625 = 0.4481925964355469 + 50.0 * 8.385002136230469
Epoch 1060, val loss: 0.4964834451675415
Epoch 1070, training loss: 419.8261413574219 = 0.44689467549324036 + 50.0 * 8.387584686279297
Epoch 1070, val loss: 0.4955988824367523
Epoch 1080, training loss: 419.7041320800781 = 0.4455793499946594 + 50.0 * 8.385170936584473
Epoch 1080, val loss: 0.49489012360572815
Epoch 1090, training loss: 419.6655578613281 = 0.44431886076927185 + 50.0 * 8.384425163269043
Epoch 1090, val loss: 0.49403634667396545
Epoch 1100, training loss: 419.5666809082031 = 0.44310468435287476 + 50.0 * 8.382471084594727
Epoch 1100, val loss: 0.49327361583709717
Epoch 1110, training loss: 419.5287170410156 = 0.44192999601364136 + 50.0 * 8.381735801696777
Epoch 1110, val loss: 0.4925600290298462
Epoch 1120, training loss: 419.5338439941406 = 0.44078168272972107 + 50.0 * 8.381860733032227
Epoch 1120, val loss: 0.4917941689491272
Epoch 1130, training loss: 419.5242614746094 = 0.43961769342422485 + 50.0 * 8.381692886352539
Epoch 1130, val loss: 0.49109330773353577
Epoch 1140, training loss: 419.4366149902344 = 0.4384751617908478 + 50.0 * 8.379962921142578
Epoch 1140, val loss: 0.4903784990310669
Epoch 1150, training loss: 419.4215393066406 = 0.43738728761672974 + 50.0 * 8.379683494567871
Epoch 1150, val loss: 0.4897679090499878
Epoch 1160, training loss: 419.43548583984375 = 0.43632635474205017 + 50.0 * 8.379982948303223
Epoch 1160, val loss: 0.4891055226325989
Epoch 1170, training loss: 419.42291259765625 = 0.4352427124977112 + 50.0 * 8.379753112792969
Epoch 1170, val loss: 0.4884241223335266
Epoch 1180, training loss: 419.3487854003906 = 0.43417781591415405 + 50.0 * 8.378292083740234
Epoch 1180, val loss: 0.48773902654647827
Epoch 1190, training loss: 419.30792236328125 = 0.4331711530685425 + 50.0 * 8.377494812011719
Epoch 1190, val loss: 0.48714518547058105
Epoch 1200, training loss: 419.2608337402344 = 0.4321863353252411 + 50.0 * 8.376572608947754
Epoch 1200, val loss: 0.48653560876846313
Epoch 1210, training loss: 419.2269287109375 = 0.4312194883823395 + 50.0 * 8.375914573669434
Epoch 1210, val loss: 0.4859604835510254
Epoch 1220, training loss: 419.1988220214844 = 0.43026596307754517 + 50.0 * 8.375370979309082
Epoch 1220, val loss: 0.48539066314697266
Epoch 1230, training loss: 419.42626953125 = 0.42932432889938354 + 50.0 * 8.379939079284668
Epoch 1230, val loss: 0.48475131392478943
Epoch 1240, training loss: 419.43914794921875 = 0.4283232092857361 + 50.0 * 8.380216598510742
Epoch 1240, val loss: 0.4842967987060547
Epoch 1250, training loss: 419.22381591796875 = 0.4273695647716522 + 50.0 * 8.37592887878418
Epoch 1250, val loss: 0.4836752116680145
Epoch 1260, training loss: 419.1309509277344 = 0.4264698922634125 + 50.0 * 8.374089241027832
Epoch 1260, val loss: 0.4831385314464569
Epoch 1270, training loss: 419.0820007324219 = 0.4255896806716919 + 50.0 * 8.373127937316895
Epoch 1270, val loss: 0.48260554671287537
Epoch 1280, training loss: 419.06109619140625 = 0.4247228503227234 + 50.0 * 8.372727394104004
Epoch 1280, val loss: 0.4820668697357178
Epoch 1290, training loss: 419.4012145996094 = 0.4238530397415161 + 50.0 * 8.379547119140625
Epoch 1290, val loss: 0.4814316928386688
Epoch 1300, training loss: 419.132080078125 = 0.4229211211204529 + 50.0 * 8.374183654785156
Epoch 1300, val loss: 0.4809607267379761
Epoch 1310, training loss: 419.0411682128906 = 0.4220447540283203 + 50.0 * 8.372382164001465
Epoch 1310, val loss: 0.48047760128974915
Epoch 1320, training loss: 418.9737854003906 = 0.4212087392807007 + 50.0 * 8.371051788330078
Epoch 1320, val loss: 0.4799128472805023
Epoch 1330, training loss: 418.9325866699219 = 0.4203810691833496 + 50.0 * 8.370244026184082
Epoch 1330, val loss: 0.47942766547203064
Epoch 1340, training loss: 418.9102478027344 = 0.41956186294555664 + 50.0 * 8.369813919067383
Epoch 1340, val loss: 0.4789621829986572
Epoch 1350, training loss: 418.9745178222656 = 0.4187449514865875 + 50.0 * 8.371115684509277
Epoch 1350, val loss: 0.47851404547691345
Epoch 1360, training loss: 418.9068908691406 = 0.417891263961792 + 50.0 * 8.369780540466309
Epoch 1360, val loss: 0.47792375087738037
Epoch 1370, training loss: 418.85382080078125 = 0.4170529544353485 + 50.0 * 8.368735313415527
Epoch 1370, val loss: 0.47743210196495056
Epoch 1380, training loss: 418.85015869140625 = 0.4162600636482239 + 50.0 * 8.368678092956543
Epoch 1380, val loss: 0.47687259316444397
Epoch 1390, training loss: 418.8150329589844 = 0.4154791235923767 + 50.0 * 8.36799144744873
Epoch 1390, val loss: 0.476444274187088
Epoch 1400, training loss: 418.9000244140625 = 0.41470521688461304 + 50.0 * 8.369706153869629
Epoch 1400, val loss: 0.4758792519569397
Epoch 1410, training loss: 418.77398681640625 = 0.41389599442481995 + 50.0 * 8.367201805114746
Epoch 1410, val loss: 0.4754877984523773
Epoch 1420, training loss: 418.7583312988281 = 0.41310957074165344 + 50.0 * 8.366904258728027
Epoch 1420, val loss: 0.4749428629875183
Epoch 1430, training loss: 418.7215270996094 = 0.41234561800956726 + 50.0 * 8.366183280944824
Epoch 1430, val loss: 0.4745068848133087
Epoch 1440, training loss: 418.6907958984375 = 0.41159021854400635 + 50.0 * 8.365584373474121
Epoch 1440, val loss: 0.47401857376098633
Epoch 1450, training loss: 418.79180908203125 = 0.41083741188049316 + 50.0 * 8.367619514465332
Epoch 1450, val loss: 0.4736267030239105
Epoch 1460, training loss: 418.6907653808594 = 0.410041481256485 + 50.0 * 8.365614891052246
Epoch 1460, val loss: 0.47294846177101135
Epoch 1470, training loss: 418.7524719238281 = 0.4092675745487213 + 50.0 * 8.366864204406738
Epoch 1470, val loss: 0.4725605845451355
Epoch 1480, training loss: 418.6653747558594 = 0.4085221290588379 + 50.0 * 8.365137100219727
Epoch 1480, val loss: 0.47205376625061035
Epoch 1490, training loss: 418.5979919433594 = 0.407791405916214 + 50.0 * 8.36380386352539
Epoch 1490, val loss: 0.4716090261936188
Epoch 1500, training loss: 418.58099365234375 = 0.4070715606212616 + 50.0 * 8.363478660583496
Epoch 1500, val loss: 0.47115111351013184
Epoch 1510, training loss: 418.6116943359375 = 0.4063490629196167 + 50.0 * 8.364107131958008
Epoch 1510, val loss: 0.4707199037075043
Epoch 1520, training loss: 418.8504333496094 = 0.40561556816101074 + 50.0 * 8.368896484375
Epoch 1520, val loss: 0.4701445996761322
Epoch 1530, training loss: 418.6010437011719 = 0.40484464168548584 + 50.0 * 8.363924026489258
Epoch 1530, val loss: 0.46979355812072754
Epoch 1540, training loss: 418.53546142578125 = 0.4041103720664978 + 50.0 * 8.362627029418945
Epoch 1540, val loss: 0.46923142671585083
Epoch 1550, training loss: 418.47650146484375 = 0.40339672565460205 + 50.0 * 8.361461639404297
Epoch 1550, val loss: 0.4688693583011627
Epoch 1560, training loss: 418.4419860839844 = 0.4026893675327301 + 50.0 * 8.360786437988281
Epoch 1560, val loss: 0.46838435530662537
Epoch 1570, training loss: 418.422119140625 = 0.40198007225990295 + 50.0 * 8.360403060913086
Epoch 1570, val loss: 0.46794238686561584
Epoch 1580, training loss: 418.66436767578125 = 0.4012717604637146 + 50.0 * 8.365262031555176
Epoch 1580, val loss: 0.4674869179725647
Epoch 1590, training loss: 418.54461669921875 = 0.40053048729896545 + 50.0 * 8.362881660461426
Epoch 1590, val loss: 0.46708396077156067
Epoch 1600, training loss: 418.39215087890625 = 0.3998042047023773 + 50.0 * 8.359847068786621
Epoch 1600, val loss: 0.46664273738861084
Epoch 1610, training loss: 418.37091064453125 = 0.3991018235683441 + 50.0 * 8.35943603515625
Epoch 1610, val loss: 0.4661819636821747
Epoch 1620, training loss: 418.4486083984375 = 0.39840346574783325 + 50.0 * 8.361003875732422
Epoch 1620, val loss: 0.4657996892929077
Epoch 1630, training loss: 418.5916748046875 = 0.39768439531326294 + 50.0 * 8.363880157470703
Epoch 1630, val loss: 0.46534574031829834
Epoch 1640, training loss: 418.3914794921875 = 0.39695122838020325 + 50.0 * 8.359890937805176
Epoch 1640, val loss: 0.4648386240005493
Epoch 1650, training loss: 418.31134033203125 = 0.39625850319862366 + 50.0 * 8.358301162719727
Epoch 1650, val loss: 0.4644131362438202
Epoch 1660, training loss: 418.27142333984375 = 0.3955761790275574 + 50.0 * 8.35751724243164
Epoch 1660, val loss: 0.46405231952667236
Epoch 1670, training loss: 418.2462158203125 = 0.3949003517627716 + 50.0 * 8.357026100158691
Epoch 1670, val loss: 0.4636208117008209
Epoch 1680, training loss: 418.288818359375 = 0.39421695470809937 + 50.0 * 8.357892036437988
Epoch 1680, val loss: 0.46329420804977417
Epoch 1690, training loss: 418.3747863769531 = 0.3935088515281677 + 50.0 * 8.359625816345215
Epoch 1690, val loss: 0.4628060460090637
Epoch 1700, training loss: 418.2359924316406 = 0.39279672503471375 + 50.0 * 8.356863975524902
Epoch 1700, val loss: 0.46230781078338623
Epoch 1710, training loss: 418.1767578125 = 0.39212021231651306 + 50.0 * 8.355692863464355
Epoch 1710, val loss: 0.4619176983833313
Epoch 1720, training loss: 418.1609191894531 = 0.3914557993412018 + 50.0 * 8.355389595031738
Epoch 1720, val loss: 0.4615475833415985
Epoch 1730, training loss: 418.1960144042969 = 0.3907919228076935 + 50.0 * 8.356104850769043
Epoch 1730, val loss: 0.4611819386482239
Epoch 1740, training loss: 418.2916259765625 = 0.3901101052761078 + 50.0 * 8.358030319213867
Epoch 1740, val loss: 0.46074116230010986
Epoch 1750, training loss: 418.1804504394531 = 0.3894137144088745 + 50.0 * 8.355820655822754
Epoch 1750, val loss: 0.4603036344051361
Epoch 1760, training loss: 418.0966491699219 = 0.38873425126075745 + 50.0 * 8.354158401489258
Epoch 1760, val loss: 0.4599204659461975
Epoch 1770, training loss: 418.08062744140625 = 0.38807159662246704 + 50.0 * 8.353851318359375
Epoch 1770, val loss: 0.45958787202835083
Epoch 1780, training loss: 418.094970703125 = 0.3874126672744751 + 50.0 * 8.354150772094727
Epoch 1780, val loss: 0.4592185616493225
Epoch 1790, training loss: 418.4422302246094 = 0.3867342472076416 + 50.0 * 8.361109733581543
Epoch 1790, val loss: 0.45887747406959534
Epoch 1800, training loss: 418.1572265625 = 0.38602787256240845 + 50.0 * 8.355423927307129
Epoch 1800, val loss: 0.4583348333835602
Epoch 1810, training loss: 418.0561218261719 = 0.3853549063205719 + 50.0 * 8.353415489196777
Epoch 1810, val loss: 0.4580022394657135
Epoch 1820, training loss: 418.0205383300781 = 0.3846992254257202 + 50.0 * 8.352716445922852
Epoch 1820, val loss: 0.45766177773475647
Epoch 1830, training loss: 418.0032043457031 = 0.3840460777282715 + 50.0 * 8.352383613586426
Epoch 1830, val loss: 0.45731544494628906
Epoch 1840, training loss: 418.2757568359375 = 0.38338857889175415 + 50.0 * 8.357847213745117
Epoch 1840, val loss: 0.45697182416915894
Epoch 1850, training loss: 418.1902770996094 = 0.38268300890922546 + 50.0 * 8.356151580810547
Epoch 1850, val loss: 0.456416517496109
Epoch 1860, training loss: 417.9848937988281 = 0.3819965124130249 + 50.0 * 8.352058410644531
Epoch 1860, val loss: 0.4561735987663269
Epoch 1870, training loss: 417.9471130371094 = 0.38134121894836426 + 50.0 * 8.35131549835205
Epoch 1870, val loss: 0.45580926537513733
Epoch 1880, training loss: 418.03460693359375 = 0.3806946575641632 + 50.0 * 8.35307788848877
Epoch 1880, val loss: 0.4554331600666046
Epoch 1890, training loss: 417.91473388671875 = 0.3800269365310669 + 50.0 * 8.350693702697754
Epoch 1890, val loss: 0.45512810349464417
Epoch 1900, training loss: 417.9078369140625 = 0.37936845421791077 + 50.0 * 8.350569725036621
Epoch 1900, val loss: 0.45477041602134705
Epoch 1910, training loss: 417.9436950683594 = 0.37871628999710083 + 50.0 * 8.351299285888672
Epoch 1910, val loss: 0.4544523358345032
Epoch 1920, training loss: 418.12969970703125 = 0.378055602312088 + 50.0 * 8.355032920837402
Epoch 1920, val loss: 0.4541712701320648
Epoch 1930, training loss: 417.9259948730469 = 0.3773787021636963 + 50.0 * 8.350972175598145
Epoch 1930, val loss: 0.45366594195365906
Epoch 1940, training loss: 417.84332275390625 = 0.37672749161720276 + 50.0 * 8.349331855773926
Epoch 1940, val loss: 0.4534090757369995
Epoch 1950, training loss: 417.8297119140625 = 0.376087486743927 + 50.0 * 8.349072456359863
Epoch 1950, val loss: 0.45308515429496765
Epoch 1960, training loss: 417.8236083984375 = 0.37544724345207214 + 50.0 * 8.348962783813477
Epoch 1960, val loss: 0.4527590572834015
Epoch 1970, training loss: 417.8948669433594 = 0.3748027980327606 + 50.0 * 8.350400924682617
Epoch 1970, val loss: 0.45241639018058777
Epoch 1980, training loss: 417.86932373046875 = 0.3741292953491211 + 50.0 * 8.34990406036377
Epoch 1980, val loss: 0.4521247446537018
Epoch 1990, training loss: 417.8070373535156 = 0.373452752828598 + 50.0 * 8.348671913146973
Epoch 1990, val loss: 0.45176514983177185
Epoch 2000, training loss: 417.8147888183594 = 0.3727976381778717 + 50.0 * 8.34883975982666
Epoch 2000, val loss: 0.4514900743961334
Epoch 2010, training loss: 417.8041687011719 = 0.3721484839916229 + 50.0 * 8.348640441894531
Epoch 2010, val loss: 0.45115751028060913
Epoch 2020, training loss: 417.7904968261719 = 0.37149935960769653 + 50.0 * 8.348380088806152
Epoch 2020, val loss: 0.4508628249168396
Epoch 2030, training loss: 418.18194580078125 = 0.3708471953868866 + 50.0 * 8.356222152709961
Epoch 2030, val loss: 0.45046985149383545
Epoch 2040, training loss: 417.8652038574219 = 0.3701455295085907 + 50.0 * 8.34990119934082
Epoch 2040, val loss: 0.4503191113471985
Epoch 2050, training loss: 417.7136535644531 = 0.3694836497306824 + 50.0 * 8.346883773803711
Epoch 2050, val loss: 0.44995972514152527
Epoch 2060, training loss: 417.677490234375 = 0.36884644627571106 + 50.0 * 8.346173286437988
Epoch 2060, val loss: 0.4496578574180603
Epoch 2070, training loss: 417.6646423339844 = 0.3682149052619934 + 50.0 * 8.345928192138672
Epoch 2070, val loss: 0.4494340121746063
Epoch 2080, training loss: 417.6588439941406 = 0.36757978796958923 + 50.0 * 8.3458251953125
Epoch 2080, val loss: 0.4491575360298157
Epoch 2090, training loss: 417.94244384765625 = 0.36694008111953735 + 50.0 * 8.351510047912598
Epoch 2090, val loss: 0.4489741027355194
Epoch 2100, training loss: 417.6767272949219 = 0.36624953150749207 + 50.0 * 8.346209526062012
Epoch 2100, val loss: 0.4485153555870056
Epoch 2110, training loss: 417.64251708984375 = 0.36558765172958374 + 50.0 * 8.345538139343262
Epoch 2110, val loss: 0.448303759098053
Epoch 2120, training loss: 417.62127685546875 = 0.3649413287639618 + 50.0 * 8.34512710571289
Epoch 2120, val loss: 0.4479830861091614
Epoch 2130, training loss: 417.67919921875 = 0.3643014132976532 + 50.0 * 8.346298217773438
Epoch 2130, val loss: 0.44775280356407166
Epoch 2140, training loss: 417.671630859375 = 0.3636380732059479 + 50.0 * 8.346159934997559
Epoch 2140, val loss: 0.4474610388278961
Epoch 2150, training loss: 417.6676025390625 = 0.36297428607940674 + 50.0 * 8.346092224121094
Epoch 2150, val loss: 0.44715985655784607
Epoch 2160, training loss: 417.6043701171875 = 0.36230719089508057 + 50.0 * 8.344841003417969
Epoch 2160, val loss: 0.4469223618507385
Epoch 2170, training loss: 417.5570373535156 = 0.361652135848999 + 50.0 * 8.343907356262207
Epoch 2170, val loss: 0.4466051161289215
Epoch 2180, training loss: 417.5418701171875 = 0.3610026240348816 + 50.0 * 8.34361743927002
Epoch 2180, val loss: 0.4463597238063812
Epoch 2190, training loss: 417.6340637207031 = 0.3603479266166687 + 50.0 * 8.345474243164062
Epoch 2190, val loss: 0.44606155157089233
Epoch 2200, training loss: 417.6037902832031 = 0.35966137051582336 + 50.0 * 8.34488296508789
Epoch 2200, val loss: 0.4457731246948242
Epoch 2210, training loss: 417.5451354980469 = 0.35897916555404663 + 50.0 * 8.34372329711914
Epoch 2210, val loss: 0.4455626308917999
Epoch 2220, training loss: 417.51739501953125 = 0.35831770300865173 + 50.0 * 8.343181610107422
Epoch 2220, val loss: 0.4453001022338867
Epoch 2230, training loss: 417.50335693359375 = 0.3576681613922119 + 50.0 * 8.342913627624512
Epoch 2230, val loss: 0.44502565264701843
Epoch 2240, training loss: 417.7250671386719 = 0.35700851678848267 + 50.0 * 8.34736156463623
Epoch 2240, val loss: 0.44475919008255005
Epoch 2250, training loss: 417.60552978515625 = 0.35632938146591187 + 50.0 * 8.34498405456543
Epoch 2250, val loss: 0.44430428743362427
Epoch 2260, training loss: 417.4771728515625 = 0.355643630027771 + 50.0 * 8.342430114746094
Epoch 2260, val loss: 0.44419175386428833
Epoch 2270, training loss: 417.4582824707031 = 0.35498830676078796 + 50.0 * 8.342065811157227
Epoch 2270, val loss: 0.4439028799533844
Epoch 2280, training loss: 417.4554748535156 = 0.35433900356292725 + 50.0 * 8.342022895812988
Epoch 2280, val loss: 0.4437091052532196
Epoch 2290, training loss: 417.5362854003906 = 0.35368385910987854 + 50.0 * 8.34365177154541
Epoch 2290, val loss: 0.4434877932071686
Epoch 2300, training loss: 417.4737854003906 = 0.35300588607788086 + 50.0 * 8.342415809631348
Epoch 2300, val loss: 0.4431478977203369
Epoch 2310, training loss: 417.4426574707031 = 0.35233885049819946 + 50.0 * 8.341806411743164
Epoch 2310, val loss: 0.4428224563598633
Epoch 2320, training loss: 417.5152587890625 = 0.35166922211647034 + 50.0 * 8.34327220916748
Epoch 2320, val loss: 0.44259560108184814
Epoch 2330, training loss: 417.4876403808594 = 0.35098084807395935 + 50.0 * 8.342733383178711
Epoch 2330, val loss: 0.4423511326313019
Epoch 2340, training loss: 417.41583251953125 = 0.3502970337867737 + 50.0 * 8.341310501098633
Epoch 2340, val loss: 0.44205358624458313
Epoch 2350, training loss: 417.390625 = 0.34961995482444763 + 50.0 * 8.3408203125
Epoch 2350, val loss: 0.44184762239456177
Epoch 2360, training loss: 417.4380187988281 = 0.3489508032798767 + 50.0 * 8.341781616210938
Epoch 2360, val loss: 0.44153597950935364
Epoch 2370, training loss: 417.41943359375 = 0.34826594591140747 + 50.0 * 8.341423034667969
Epoch 2370, val loss: 0.44129109382629395
Epoch 2380, training loss: 417.3689270019531 = 0.34758254885673523 + 50.0 * 8.340426445007324
Epoch 2380, val loss: 0.44110527634620667
Epoch 2390, training loss: 417.40936279296875 = 0.3468995690345764 + 50.0 * 8.341249465942383
Epoch 2390, val loss: 0.4407922923564911
Epoch 2400, training loss: 417.4538269042969 = 0.3462030589580536 + 50.0 * 8.34215259552002
Epoch 2400, val loss: 0.4404371380805969
Epoch 2410, training loss: 417.3672790527344 = 0.34549906849861145 + 50.0 * 8.340435981750488
Epoch 2410, val loss: 0.44027072191238403
Epoch 2420, training loss: 417.3110656738281 = 0.34480568766593933 + 50.0 * 8.339324951171875
Epoch 2420, val loss: 0.43999335169792175
Epoch 2430, training loss: 417.3487854003906 = 0.34411677718162537 + 50.0 * 8.340093612670898
Epoch 2430, val loss: 0.43967700004577637
Epoch 2440, training loss: 417.4064025878906 = 0.34340938925743103 + 50.0 * 8.341259956359863
Epoch 2440, val loss: 0.4394225478172302
Epoch 2450, training loss: 417.3262939453125 = 0.34269630908966064 + 50.0 * 8.339672088623047
Epoch 2450, val loss: 0.43920135498046875
Epoch 2460, training loss: 417.2818603515625 = 0.34199264645576477 + 50.0 * 8.338797569274902
Epoch 2460, val loss: 0.4388558268547058
Epoch 2470, training loss: 417.3307800292969 = 0.3412908613681793 + 50.0 * 8.339790344238281
Epoch 2470, val loss: 0.4386717975139618
Epoch 2480, training loss: 417.34295654296875 = 0.3405723571777344 + 50.0 * 8.340047836303711
Epoch 2480, val loss: 0.43837329745292664
Epoch 2490, training loss: 417.3294677734375 = 0.3398434817790985 + 50.0 * 8.339792251586914
Epoch 2490, val loss: 0.4380634129047394
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.823947234906139
0.8650293414475115
=== training gcn model ===
Epoch 0, training loss: 530.218505859375 = 1.1040979623794556 + 50.0 * 10.582287788391113
Epoch 0, val loss: 1.1044844388961792
Epoch 10, training loss: 530.1958618164062 = 1.099175214767456 + 50.0 * 10.581933975219727
Epoch 10, val loss: 1.0995163917541504
Epoch 20, training loss: 530.1102905273438 = 1.0937974452972412 + 50.0 * 10.580329895019531
Epoch 20, val loss: 1.0940383672714233
Epoch 30, training loss: 529.726806640625 = 1.087660789489746 + 50.0 * 10.572783470153809
Epoch 30, val loss: 1.087780475616455
Epoch 40, training loss: 528.2047729492188 = 1.0805543661117554 + 50.0 * 10.542484283447266
Epoch 40, val loss: 1.0805169343948364
Epoch 50, training loss: 524.0488891601562 = 1.0723516941070557 + 50.0 * 10.4595308303833
Epoch 50, val loss: 1.072123646736145
Epoch 60, training loss: 514.9111328125 = 1.0641117095947266 + 50.0 * 10.27694034576416
Epoch 60, val loss: 1.0637876987457275
Epoch 70, training loss: 499.7464904785156 = 1.0559332370758057 + 50.0 * 9.973811149597168
Epoch 70, val loss: 1.055268406867981
Epoch 80, training loss: 484.30194091796875 = 1.0490615367889404 + 50.0 * 9.665057182312012
Epoch 80, val loss: 1.0486308336257935
Epoch 90, training loss: 474.734375 = 1.0441733598709106 + 50.0 * 9.473804473876953
Epoch 90, val loss: 1.04407799243927
Epoch 100, training loss: 466.37860107421875 = 1.040087103843689 + 50.0 * 9.306770324707031
Epoch 100, val loss: 1.0402272939682007
Epoch 110, training loss: 462.0117492675781 = 1.0360264778137207 + 50.0 * 9.219513893127441
Epoch 110, val loss: 1.0363061428070068
Epoch 120, training loss: 460.15386962890625 = 1.0316674709320068 + 50.0 * 9.182443618774414
Epoch 120, val loss: 1.0320061445236206
Epoch 130, training loss: 458.760009765625 = 1.0271114110946655 + 50.0 * 9.154658317565918
Epoch 130, val loss: 1.0274813175201416
Epoch 140, training loss: 456.8025207519531 = 1.022778034210205 + 50.0 * 9.115594863891602
Epoch 140, val loss: 1.0232126712799072
Epoch 150, training loss: 454.0402526855469 = 1.0190972089767456 + 50.0 * 9.060422897338867
Epoch 150, val loss: 1.0196471214294434
Epoch 160, training loss: 450.3463134765625 = 1.0162650346755981 + 50.0 * 8.986600875854492
Epoch 160, val loss: 1.0169557332992554
Epoch 170, training loss: 446.8817138671875 = 1.01387619972229 + 50.0 * 8.917356491088867
Epoch 170, val loss: 1.0145978927612305
Epoch 180, training loss: 444.2275695800781 = 1.0104607343673706 + 50.0 * 8.864341735839844
Epoch 180, val loss: 1.011177659034729
Epoch 190, training loss: 441.3443603515625 = 1.0065473318099976 + 50.0 * 8.806756019592285
Epoch 190, val loss: 1.007473349571228
Epoch 200, training loss: 439.15557861328125 = 1.0033241510391235 + 50.0 * 8.763045310974121
Epoch 200, val loss: 1.004404902458191
Epoch 210, training loss: 437.7815246582031 = 0.9997769594192505 + 50.0 * 8.735634803771973
Epoch 210, val loss: 1.000817894935608
Epoch 220, training loss: 436.8529357910156 = 0.9950599074363708 + 50.0 * 8.717157363891602
Epoch 220, val loss: 0.9959542155265808
Epoch 230, training loss: 435.7828674316406 = 0.9895318150520325 + 50.0 * 8.695866584777832
Epoch 230, val loss: 0.9905457496643066
Epoch 240, training loss: 434.77752685546875 = 0.9837149381637573 + 50.0 * 8.67587661743164
Epoch 240, val loss: 0.9849139451980591
Epoch 250, training loss: 433.6247253417969 = 0.977792501449585 + 50.0 * 8.652938842773438
Epoch 250, val loss: 0.9792100191116333
Epoch 260, training loss: 432.5065612792969 = 0.9718794822692871 + 50.0 * 8.630693435668945
Epoch 260, val loss: 0.9735528230667114
Epoch 270, training loss: 431.5361328125 = 0.9658055901527405 + 50.0 * 8.611406326293945
Epoch 270, val loss: 0.9677377343177795
Epoch 280, training loss: 430.7574768066406 = 0.9592599868774414 + 50.0 * 8.595964431762695
Epoch 280, val loss: 0.9613802433013916
Epoch 290, training loss: 430.06964111328125 = 0.9520071744918823 + 50.0 * 8.582352638244629
Epoch 290, val loss: 0.954228401184082
Epoch 300, training loss: 429.43212890625 = 0.9438733458518982 + 50.0 * 8.569765090942383
Epoch 300, val loss: 0.9463693499565125
Epoch 310, training loss: 428.87237548828125 = 0.9352452158927917 + 50.0 * 8.55874252319336
Epoch 310, val loss: 0.9379929304122925
Epoch 320, training loss: 428.4208068847656 = 0.9262422919273376 + 50.0 * 8.549891471862793
Epoch 320, val loss: 0.9292795658111572
Epoch 330, training loss: 427.9074401855469 = 0.9168819189071655 + 50.0 * 8.539811134338379
Epoch 330, val loss: 0.9202439188957214
Epoch 340, training loss: 427.50982666015625 = 0.9071792364120483 + 50.0 * 8.532052993774414
Epoch 340, val loss: 0.9108946919441223
Epoch 350, training loss: 427.2151184082031 = 0.8970224857330322 + 50.0 * 8.526361465454102
Epoch 350, val loss: 0.9011898636817932
Epoch 360, training loss: 426.8324890136719 = 0.8865885138511658 + 50.0 * 8.51891803741455
Epoch 360, val loss: 0.8911627531051636
Epoch 370, training loss: 426.519287109375 = 0.8759543299674988 + 50.0 * 8.512866973876953
Epoch 370, val loss: 0.8809552192687988
Epoch 380, training loss: 426.2304382324219 = 0.8650470972061157 + 50.0 * 8.507308006286621
Epoch 380, val loss: 0.8705391883850098
Epoch 390, training loss: 425.9897766113281 = 0.8538958430290222 + 50.0 * 8.502717971801758
Epoch 390, val loss: 0.8598641157150269
Epoch 400, training loss: 425.7264404296875 = 0.842552661895752 + 50.0 * 8.49767780303955
Epoch 400, val loss: 0.8491144776344299
Epoch 410, training loss: 425.48089599609375 = 0.8311808705329895 + 50.0 * 8.49299430847168
Epoch 410, val loss: 0.838320791721344
Epoch 420, training loss: 425.2666320800781 = 0.8196831345558167 + 50.0 * 8.48893928527832
Epoch 420, val loss: 0.8274081945419312
Epoch 430, training loss: 425.0397644042969 = 0.8081474900245667 + 50.0 * 8.48463249206543
Epoch 430, val loss: 0.8164520859718323
Epoch 440, training loss: 424.83880615234375 = 0.7965392470359802 + 50.0 * 8.48084545135498
Epoch 440, val loss: 0.8054801225662231
Epoch 450, training loss: 424.6754455566406 = 0.7849242091178894 + 50.0 * 8.477810859680176
Epoch 450, val loss: 0.7946186661720276
Epoch 460, training loss: 424.50189208984375 = 0.773329496383667 + 50.0 * 8.474571228027344
Epoch 460, val loss: 0.7836698293685913
Epoch 470, training loss: 424.37713623046875 = 0.7617846727371216 + 50.0 * 8.472307205200195
Epoch 470, val loss: 0.7729187607765198
Epoch 480, training loss: 424.2269592285156 = 0.7503737211227417 + 50.0 * 8.469532012939453
Epoch 480, val loss: 0.7622461318969727
Epoch 490, training loss: 424.08770751953125 = 0.7390521168708801 + 50.0 * 8.466973304748535
Epoch 490, val loss: 0.7517066597938538
Epoch 500, training loss: 424.05084228515625 = 0.7277956008911133 + 50.0 * 8.466461181640625
Epoch 500, val loss: 0.74130779504776
Epoch 510, training loss: 423.8687744140625 = 0.7167616486549377 + 50.0 * 8.463040351867676
Epoch 510, val loss: 0.7310566306114197
Epoch 520, training loss: 423.73297119140625 = 0.7059786915779114 + 50.0 * 8.460539817810059
Epoch 520, val loss: 0.7210829854011536
Epoch 530, training loss: 423.5898132324219 = 0.6953645944595337 + 50.0 * 8.45788860321045
Epoch 530, val loss: 0.7113321423530579
Epoch 540, training loss: 423.45361328125 = 0.6850255727767944 + 50.0 * 8.455371856689453
Epoch 540, val loss: 0.7018537521362305
Epoch 550, training loss: 423.3095703125 = 0.6749704480171204 + 50.0 * 8.452692031860352
Epoch 550, val loss: 0.6926286816596985
Epoch 560, training loss: 423.3268737792969 = 0.6651789546012878 + 50.0 * 8.45323371887207
Epoch 560, val loss: 0.6836100220680237
Epoch 570, training loss: 423.092041015625 = 0.65543532371521 + 50.0 * 8.448732376098633
Epoch 570, val loss: 0.6749292612075806
Epoch 580, training loss: 422.935791015625 = 0.6461890339851379 + 50.0 * 8.445792198181152
Epoch 580, val loss: 0.6665629148483276
Epoch 590, training loss: 422.7744445800781 = 0.6372247934341431 + 50.0 * 8.442744255065918
Epoch 590, val loss: 0.6584745049476624
Epoch 600, training loss: 422.63739013671875 = 0.6285417079925537 + 50.0 * 8.440176963806152
Epoch 600, val loss: 0.650680422782898
Epoch 610, training loss: 422.7589111328125 = 0.6200718879699707 + 50.0 * 8.442776679992676
Epoch 610, val loss: 0.6430885195732117
Epoch 620, training loss: 422.43914794921875 = 0.611825704574585 + 50.0 * 8.436546325683594
Epoch 620, val loss: 0.6357759237289429
Epoch 630, training loss: 422.2877502441406 = 0.6039861440658569 + 50.0 * 8.433675765991211
Epoch 630, val loss: 0.6288601160049438
Epoch 640, training loss: 422.1753234863281 = 0.5964158177375793 + 50.0 * 8.431578636169434
Epoch 640, val loss: 0.622163712978363
Epoch 650, training loss: 422.0506286621094 = 0.5891270637512207 + 50.0 * 8.429229736328125
Epoch 650, val loss: 0.6158427000045776
Epoch 660, training loss: 421.9343566894531 = 0.5821478962898254 + 50.0 * 8.427043914794922
Epoch 660, val loss: 0.6097495555877686
Epoch 670, training loss: 421.86468505859375 = 0.5754598379135132 + 50.0 * 8.42578411102295
Epoch 670, val loss: 0.6040207743644714
Epoch 680, training loss: 421.7538146972656 = 0.5690265893936157 + 50.0 * 8.42369556427002
Epoch 680, val loss: 0.5983067154884338
Epoch 690, training loss: 421.652099609375 = 0.5628583431243896 + 50.0 * 8.421784400939941
Epoch 690, val loss: 0.593163251876831
Epoch 700, training loss: 421.54608154296875 = 0.5569992065429688 + 50.0 * 8.419781684875488
Epoch 700, val loss: 0.5882320404052734
Epoch 710, training loss: 421.52783203125 = 0.5513970255851746 + 50.0 * 8.41952896118164
Epoch 710, val loss: 0.5835155248641968
Epoch 720, training loss: 421.3870849609375 = 0.5460236072540283 + 50.0 * 8.416821479797363
Epoch 720, val loss: 0.578971266746521
Epoch 730, training loss: 421.2785339355469 = 0.540908932685852 + 50.0 * 8.414752006530762
Epoch 730, val loss: 0.5747063755989075
Epoch 740, training loss: 421.1963195800781 = 0.5360051393508911 + 50.0 * 8.413206100463867
Epoch 740, val loss: 0.5706730484962463
Epoch 750, training loss: 421.1588439941406 = 0.5312984585762024 + 50.0 * 8.412550926208496
Epoch 750, val loss: 0.5668395161628723
Epoch 760, training loss: 421.0927429199219 = 0.5267716646194458 + 50.0 * 8.411319732666016
Epoch 760, val loss: 0.5630073547363281
Epoch 770, training loss: 421.00213623046875 = 0.5224084854125977 + 50.0 * 8.409594535827637
Epoch 770, val loss: 0.5595908761024475
Epoch 780, training loss: 420.9196472167969 = 0.5183019638061523 + 50.0 * 8.408026695251465
Epoch 780, val loss: 0.5563493371009827
Epoch 790, training loss: 420.8413391113281 = 0.5143892168998718 + 50.0 * 8.406538963317871
Epoch 790, val loss: 0.5531908869743347
Epoch 800, training loss: 420.87750244140625 = 0.5105969905853271 + 50.0 * 8.40733814239502
Epoch 800, val loss: 0.5501895546913147
Epoch 810, training loss: 420.8084411621094 = 0.5069473385810852 + 50.0 * 8.40602970123291
Epoch 810, val loss: 0.5473214983940125
Epoch 820, training loss: 420.67840576171875 = 0.5034914016723633 + 50.0 * 8.403498649597168
Epoch 820, val loss: 0.544579267501831
Epoch 830, training loss: 420.62744140625 = 0.5001727938652039 + 50.0 * 8.402544975280762
Epoch 830, val loss: 0.542019784450531
Epoch 840, training loss: 420.6033020019531 = 0.4969867169857025 + 50.0 * 8.40212631225586
Epoch 840, val loss: 0.5395408868789673
Epoch 850, training loss: 420.5715026855469 = 0.493892639875412 + 50.0 * 8.401552200317383
Epoch 850, val loss: 0.5371930599212646
Epoch 860, training loss: 420.558349609375 = 0.49091753363609314 + 50.0 * 8.401349067687988
Epoch 860, val loss: 0.535012423992157
Epoch 870, training loss: 420.4626159667969 = 0.48809245228767395 + 50.0 * 8.399490356445312
Epoch 870, val loss: 0.5328258275985718
Epoch 880, training loss: 420.40447998046875 = 0.48538193106651306 + 50.0 * 8.398382186889648
Epoch 880, val loss: 0.5307925939559937
Epoch 890, training loss: 420.3681945800781 = 0.48277315497398376 + 50.0 * 8.39770793914795
Epoch 890, val loss: 0.5288341045379639
Epoch 900, training loss: 420.4239807128906 = 0.4802500009536743 + 50.0 * 8.398874282836914
Epoch 900, val loss: 0.5269630551338196
Epoch 910, training loss: 420.3218688964844 = 0.4777662456035614 + 50.0 * 8.396882057189941
Epoch 910, val loss: 0.5252709984779358
Epoch 920, training loss: 420.2468566894531 = 0.4754412770271301 + 50.0 * 8.395428657531738
Epoch 920, val loss: 0.5235545635223389
Epoch 930, training loss: 420.189697265625 = 0.4731907248497009 + 50.0 * 8.394330024719238
Epoch 930, val loss: 0.5219846963882446
Epoch 940, training loss: 420.14447021484375 = 0.4710473120212555 + 50.0 * 8.393468856811523
Epoch 940, val loss: 0.5204497575759888
Epoch 950, training loss: 420.2638244628906 = 0.4689391553401947 + 50.0 * 8.39589786529541
Epoch 950, val loss: 0.5189663171768188
Epoch 960, training loss: 420.07861328125 = 0.466872900724411 + 50.0 * 8.392234802246094
Epoch 960, val loss: 0.5175974369049072
Epoch 970, training loss: 420.1094970703125 = 0.46491190791130066 + 50.0 * 8.392891883850098
Epoch 970, val loss: 0.5162057280540466
Epoch 980, training loss: 419.9833679199219 = 0.46299663186073303 + 50.0 * 8.39040756225586
Epoch 980, val loss: 0.5149354934692383
Epoch 990, training loss: 419.91290283203125 = 0.46116843819618225 + 50.0 * 8.389034271240234
Epoch 990, val loss: 0.5137113928794861
Epoch 1000, training loss: 419.8942565917969 = 0.45939889550209045 + 50.0 * 8.388696670532227
Epoch 1000, val loss: 0.5125044584274292
Epoch 1010, training loss: 420.0173645019531 = 0.4576415717601776 + 50.0 * 8.391194343566895
Epoch 1010, val loss: 0.5113441944122314
Epoch 1020, training loss: 419.8020935058594 = 0.4559052288532257 + 50.0 * 8.386923789978027
Epoch 1020, val loss: 0.5102642774581909
Epoch 1030, training loss: 419.758544921875 = 0.45427194237709045 + 50.0 * 8.386085510253906
Epoch 1030, val loss: 0.5093322992324829
Epoch 1040, training loss: 419.700927734375 = 0.45267969369888306 + 50.0 * 8.384964942932129
Epoch 1040, val loss: 0.508273184299469
Epoch 1050, training loss: 419.6504211425781 = 0.45112210512161255 + 50.0 * 8.383986473083496
Epoch 1050, val loss: 0.5073503255844116
Epoch 1060, training loss: 419.6094665527344 = 0.4496001899242401 + 50.0 * 8.383197784423828
Epoch 1060, val loss: 0.506395161151886
Epoch 1070, training loss: 419.8060302734375 = 0.4481050968170166 + 50.0 * 8.387158393859863
Epoch 1070, val loss: 0.5054700970649719
Epoch 1080, training loss: 419.7422180175781 = 0.44653743505477905 + 50.0 * 8.385913848876953
Epoch 1080, val loss: 0.5045602321624756
Epoch 1090, training loss: 419.5636901855469 = 0.44510024785995483 + 50.0 * 8.38237190246582
Epoch 1090, val loss: 0.5037110447883606
Epoch 1100, training loss: 419.4709167480469 = 0.44370704889297485 + 50.0 * 8.380544662475586
Epoch 1100, val loss: 0.5028815865516663
Epoch 1110, training loss: 419.4129943847656 = 0.44234976172447205 + 50.0 * 8.379412651062012
Epoch 1110, val loss: 0.5021171569824219
Epoch 1120, training loss: 419.3758544921875 = 0.44102510809898376 + 50.0 * 8.37869644165039
Epoch 1120, val loss: 0.501385509967804
Epoch 1130, training loss: 419.56146240234375 = 0.4397183358669281 + 50.0 * 8.382434844970703
Epoch 1130, val loss: 0.5007020235061646
Epoch 1140, training loss: 419.44049072265625 = 0.4383721351623535 + 50.0 * 8.38004207611084
Epoch 1140, val loss: 0.49993979930877686
Epoch 1150, training loss: 419.3134460449219 = 0.43712714314460754 + 50.0 * 8.37752628326416
Epoch 1150, val loss: 0.4992953836917877
Epoch 1160, training loss: 419.2451477050781 = 0.43590599298477173 + 50.0 * 8.376184463500977
Epoch 1160, val loss: 0.4985603094100952
Epoch 1170, training loss: 419.197509765625 = 0.43471720814704895 + 50.0 * 8.375255584716797
Epoch 1170, val loss: 0.497956782579422
Epoch 1180, training loss: 419.16162109375 = 0.4335517883300781 + 50.0 * 8.374561309814453
Epoch 1180, val loss: 0.49733996391296387
Epoch 1190, training loss: 419.2501525878906 = 0.4324038028717041 + 50.0 * 8.376355171203613
Epoch 1190, val loss: 0.49663636088371277
Epoch 1200, training loss: 419.2002258300781 = 0.43121862411499023 + 50.0 * 8.375380516052246
Epoch 1200, val loss: 0.4961852431297302
Epoch 1210, training loss: 419.07366943359375 = 0.4301173985004425 + 50.0 * 8.372871398925781
Epoch 1210, val loss: 0.4955768883228302
Epoch 1220, training loss: 419.0378112792969 = 0.4290330111980438 + 50.0 * 8.372175216674805
Epoch 1220, val loss: 0.49504685401916504
Epoch 1230, training loss: 419.0025634765625 = 0.4279767572879791 + 50.0 * 8.371491432189941
Epoch 1230, val loss: 0.4945676922798157
Epoch 1240, training loss: 419.2414855957031 = 0.42692258954048157 + 50.0 * 8.376291275024414
Epoch 1240, val loss: 0.4941001534461975
Epoch 1250, training loss: 419.0093078613281 = 0.4258626699447632 + 50.0 * 8.371668815612793
Epoch 1250, val loss: 0.493495911359787
Epoch 1260, training loss: 418.91094970703125 = 0.4248504340648651 + 50.0 * 8.369722366333008
Epoch 1260, val loss: 0.4930543005466461
Epoch 1270, training loss: 418.8707275390625 = 0.4238574206829071 + 50.0 * 8.368937492370605
Epoch 1270, val loss: 0.49262261390686035
Epoch 1280, training loss: 418.8294677734375 = 0.4228847920894623 + 50.0 * 8.368131637573242
Epoch 1280, val loss: 0.49217063188552856
Epoch 1290, training loss: 418.7977600097656 = 0.421928346157074 + 50.0 * 8.36751651763916
Epoch 1290, val loss: 0.4917519688606262
Epoch 1300, training loss: 418.83404541015625 = 0.4209831655025482 + 50.0 * 8.368261337280273
Epoch 1300, val loss: 0.4913070797920227
Epoch 1310, training loss: 418.7857360839844 = 0.4200139045715332 + 50.0 * 8.367314338684082
Epoch 1310, val loss: 0.4910103380680084
Epoch 1320, training loss: 418.7424011230469 = 0.4190790057182312 + 50.0 * 8.366466522216797
Epoch 1320, val loss: 0.49045273661613464
Epoch 1330, training loss: 418.7056884765625 = 0.41816967725753784 + 50.0 * 8.365750312805176
Epoch 1330, val loss: 0.4901816248893738
Epoch 1340, training loss: 418.772216796875 = 0.41727060079574585 + 50.0 * 8.367098808288574
Epoch 1340, val loss: 0.48975327610969543
Epoch 1350, training loss: 418.6518249511719 = 0.41637468338012695 + 50.0 * 8.36470890045166
Epoch 1350, val loss: 0.48931819200515747
Epoch 1360, training loss: 418.60687255859375 = 0.4155026078224182 + 50.0 * 8.3638277053833
Epoch 1360, val loss: 0.4889819324016571
Epoch 1370, training loss: 418.6676940917969 = 0.4146338105201721 + 50.0 * 8.365060806274414
Epoch 1370, val loss: 0.4886109232902527
Epoch 1380, training loss: 418.543212890625 = 0.41376373171806335 + 50.0 * 8.362588882446289
Epoch 1380, val loss: 0.4883374571800232
Epoch 1390, training loss: 418.48638916015625 = 0.4129219353199005 + 50.0 * 8.361469268798828
Epoch 1390, val loss: 0.4879688322544098
Epoch 1400, training loss: 418.4956970214844 = 0.41209277510643005 + 50.0 * 8.361672401428223
Epoch 1400, val loss: 0.48759278655052185
Epoch 1410, training loss: 418.6217956542969 = 0.4112435281276703 + 50.0 * 8.364211082458496
Epoch 1410, val loss: 0.48727160692214966
Epoch 1420, training loss: 418.4310302734375 = 0.4104079306125641 + 50.0 * 8.36041259765625
Epoch 1420, val loss: 0.4869478940963745
Epoch 1430, training loss: 418.3758850097656 = 0.40959790349006653 + 50.0 * 8.359325408935547
Epoch 1430, val loss: 0.4866676926612854
Epoch 1440, training loss: 418.3515319824219 = 0.4087986648082733 + 50.0 * 8.358854293823242
Epoch 1440, val loss: 0.48632681369781494
Epoch 1450, training loss: 418.40576171875 = 0.4080069661140442 + 50.0 * 8.359954833984375
Epoch 1450, val loss: 0.48608487844467163
Epoch 1460, training loss: 418.326171875 = 0.4071919620037079 + 50.0 * 8.358379364013672
Epoch 1460, val loss: 0.4856697916984558
Epoch 1470, training loss: 418.3113098144531 = 0.4064079523086548 + 50.0 * 8.358098030090332
Epoch 1470, val loss: 0.4854556918144226
Epoch 1480, training loss: 418.3230895996094 = 0.4056369960308075 + 50.0 * 8.358348846435547
Epoch 1480, val loss: 0.48507019877433777
Epoch 1490, training loss: 418.2891540527344 = 0.4048481583595276 + 50.0 * 8.357686042785645
Epoch 1490, val loss: 0.48479825258255005
Epoch 1500, training loss: 418.2305603027344 = 0.4040817320346832 + 50.0 * 8.356529235839844
Epoch 1500, val loss: 0.48458296060562134
Epoch 1510, training loss: 418.1880187988281 = 0.4033321738243103 + 50.0 * 8.355693817138672
Epoch 1510, val loss: 0.4842813313007355
Epoch 1520, training loss: 418.1728820800781 = 0.402593195438385 + 50.0 * 8.355405807495117
Epoch 1520, val loss: 0.4840105473995209
Epoch 1530, training loss: 418.170654296875 = 0.40185731649398804 + 50.0 * 8.355376243591309
Epoch 1530, val loss: 0.48375022411346436
Epoch 1540, training loss: 418.3460388183594 = 0.40110933780670166 + 50.0 * 8.358898162841797
Epoch 1540, val loss: 0.4834688901901245
Epoch 1550, training loss: 418.17474365234375 = 0.40034908056259155 + 50.0 * 8.355487823486328
Epoch 1550, val loss: 0.4832029938697815
Epoch 1560, training loss: 418.1142883300781 = 0.3996109366416931 + 50.0 * 8.354293823242188
Epoch 1560, val loss: 0.48295101523399353
Epoch 1570, training loss: 418.08050537109375 = 0.39888882637023926 + 50.0 * 8.353631973266602
Epoch 1570, val loss: 0.4826700985431671
Epoch 1580, training loss: 418.06463623046875 = 0.3981756269931793 + 50.0 * 8.3533296585083
Epoch 1580, val loss: 0.48244696855545044
Epoch 1590, training loss: 418.26007080078125 = 0.3974665105342865 + 50.0 * 8.35725212097168
Epoch 1590, val loss: 0.4821711480617523
Epoch 1600, training loss: 418.1530456542969 = 0.3967197835445404 + 50.0 * 8.35512638092041
Epoch 1600, val loss: 0.4820292592048645
Epoch 1610, training loss: 418.0390319824219 = 0.3960087299346924 + 50.0 * 8.352860450744629
Epoch 1610, val loss: 0.48181310296058655
Epoch 1620, training loss: 418.0431213378906 = 0.3953041732311249 + 50.0 * 8.352956771850586
Epoch 1620, val loss: 0.4815829396247864
Epoch 1630, training loss: 418.00213623046875 = 0.39460545778274536 + 50.0 * 8.352150917053223
Epoch 1630, val loss: 0.4813931882381439
Epoch 1640, training loss: 417.993408203125 = 0.3939131796360016 + 50.0 * 8.35198974609375
Epoch 1640, val loss: 0.48123136162757874
Epoch 1650, training loss: 418.0548400878906 = 0.3932229280471802 + 50.0 * 8.353232383728027
Epoch 1650, val loss: 0.48103222250938416
Epoch 1660, training loss: 418.04473876953125 = 0.3925212025642395 + 50.0 * 8.353044509887695
Epoch 1660, val loss: 0.48075243830680847
Epoch 1670, training loss: 417.9327392578125 = 0.3918188214302063 + 50.0 * 8.350818634033203
Epoch 1670, val loss: 0.48069342970848083
Epoch 1680, training loss: 417.9105224609375 = 0.3911450207233429 + 50.0 * 8.350387573242188
Epoch 1680, val loss: 0.48041093349456787
Epoch 1690, training loss: 417.8955993652344 = 0.3904816508293152 + 50.0 * 8.350102424621582
Epoch 1690, val loss: 0.4802864193916321
Epoch 1700, training loss: 417.8909912109375 = 0.38982152938842773 + 50.0 * 8.35002326965332
Epoch 1700, val loss: 0.48012247681617737
Epoch 1710, training loss: 418.1003112792969 = 0.389156699180603 + 50.0 * 8.354223251342773
Epoch 1710, val loss: 0.4798924922943115
Epoch 1720, training loss: 417.9195556640625 = 0.38846656680107117 + 50.0 * 8.350622177124023
Epoch 1720, val loss: 0.479792982339859
Epoch 1730, training loss: 417.8486022949219 = 0.38779932260513306 + 50.0 * 8.34921646118164
Epoch 1730, val loss: 0.479594886302948
Epoch 1740, training loss: 417.834228515625 = 0.38713493943214417 + 50.0 * 8.348941802978516
Epoch 1740, val loss: 0.4793819189071655
Epoch 1750, training loss: 417.8195495605469 = 0.3864821493625641 + 50.0 * 8.348661422729492
Epoch 1750, val loss: 0.4792603850364685
Epoch 1760, training loss: 417.825927734375 = 0.3858242630958557 + 50.0 * 8.348801612854004
Epoch 1760, val loss: 0.4790942668914795
Epoch 1770, training loss: 417.9722595214844 = 0.38515591621398926 + 50.0 * 8.351741790771484
Epoch 1770, val loss: 0.4789218008518219
Epoch 1780, training loss: 417.9240417480469 = 0.3844839036464691 + 50.0 * 8.350790977478027
Epoch 1780, val loss: 0.47869500517845154
Epoch 1790, training loss: 417.8077697753906 = 0.3838081657886505 + 50.0 * 8.348479270935059
Epoch 1790, val loss: 0.47862955927848816
Epoch 1800, training loss: 417.7687683105469 = 0.3831588327884674 + 50.0 * 8.347712516784668
Epoch 1800, val loss: 0.478462815284729
Epoch 1810, training loss: 417.73748779296875 = 0.38251793384552 + 50.0 * 8.347099304199219
Epoch 1810, val loss: 0.4783363938331604
Epoch 1820, training loss: 417.7216796875 = 0.3818800449371338 + 50.0 * 8.346796035766602
Epoch 1820, val loss: 0.4781958758831024
Epoch 1830, training loss: 417.7290344238281 = 0.3812384307384491 + 50.0 * 8.346956253051758
Epoch 1830, val loss: 0.47803235054016113
Epoch 1840, training loss: 417.93939208984375 = 0.38058343529701233 + 50.0 * 8.351176261901855
Epoch 1840, val loss: 0.47786614298820496
Epoch 1850, training loss: 417.7331237792969 = 0.37991419434547424 + 50.0 * 8.347064018249512
Epoch 1850, val loss: 0.4778594970703125
Epoch 1860, training loss: 417.67340087890625 = 0.3792647123336792 + 50.0 * 8.345882415771484
Epoch 1860, val loss: 0.47766923904418945
Epoch 1870, training loss: 417.6585388183594 = 0.3786332607269287 + 50.0 * 8.345598220825195
Epoch 1870, val loss: 0.4775002598762512
Epoch 1880, training loss: 417.6390075683594 = 0.37800514698028564 + 50.0 * 8.345220565795898
Epoch 1880, val loss: 0.47741052508354187
Epoch 1890, training loss: 417.7763977050781 = 0.3773767352104187 + 50.0 * 8.347980499267578
Epoch 1890, val loss: 0.4772765338420868
Epoch 1900, training loss: 417.6900329589844 = 0.37670645117759705 + 50.0 * 8.346266746520996
Epoch 1900, val loss: 0.4770581126213074
Epoch 1910, training loss: 417.6418762207031 = 0.3760637044906616 + 50.0 * 8.345315933227539
Epoch 1910, val loss: 0.4769347012042999
Epoch 1920, training loss: 417.5776672363281 = 0.37542757391929626 + 50.0 * 8.34404468536377
Epoch 1920, val loss: 0.4768362045288086
Epoch 1930, training loss: 417.5713806152344 = 0.3748050332069397 + 50.0 * 8.343931198120117
Epoch 1930, val loss: 0.4766937494277954
Epoch 1940, training loss: 417.7156066894531 = 0.37418222427368164 + 50.0 * 8.34682846069336
Epoch 1940, val loss: 0.47662365436553955
Epoch 1950, training loss: 417.5451965332031 = 0.3735254108905792 + 50.0 * 8.343433380126953
Epoch 1950, val loss: 0.47640419006347656
Epoch 1960, training loss: 417.5276794433594 = 0.37289851903915405 + 50.0 * 8.343095779418945
Epoch 1960, val loss: 0.47631141543388367
Epoch 1970, training loss: 417.5118713378906 = 0.37227708101272583 + 50.0 * 8.342791557312012
Epoch 1970, val loss: 0.4761708080768585
Epoch 1980, training loss: 417.49957275390625 = 0.3716692328453064 + 50.0 * 8.342557907104492
Epoch 1980, val loss: 0.47610223293304443
Epoch 1990, training loss: 417.5226135253906 = 0.3710564374923706 + 50.0 * 8.34303092956543
Epoch 1990, val loss: 0.4760027229785919
Epoch 2000, training loss: 417.57421875 = 0.3704202473163605 + 50.0 * 8.344076156616211
Epoch 2000, val loss: 0.4758480489253998
Epoch 2010, training loss: 417.4810485839844 = 0.36977994441986084 + 50.0 * 8.342225074768066
Epoch 2010, val loss: 0.47566118836402893
Epoch 2020, training loss: 417.45953369140625 = 0.36915865540504456 + 50.0 * 8.34180736541748
Epoch 2020, val loss: 0.475543349981308
Epoch 2030, training loss: 417.43280029296875 = 0.368540495634079 + 50.0 * 8.341285705566406
Epoch 2030, val loss: 0.47539639472961426
Epoch 2040, training loss: 417.469970703125 = 0.3679238557815552 + 50.0 * 8.342041015625
Epoch 2040, val loss: 0.4753008484840393
Epoch 2050, training loss: 417.4398498535156 = 0.3672839105129242 + 50.0 * 8.341451644897461
Epoch 2050, val loss: 0.4751547873020172
Epoch 2060, training loss: 417.4876708984375 = 0.3666495382785797 + 50.0 * 8.34242057800293
Epoch 2060, val loss: 0.4750267565250397
Epoch 2070, training loss: 417.4606018066406 = 0.366000235080719 + 50.0 * 8.34189224243164
Epoch 2070, val loss: 0.474841445684433
Epoch 2080, training loss: 417.40118408203125 = 0.3653641939163208 + 50.0 * 8.340716361999512
Epoch 2080, val loss: 0.47474831342697144
Epoch 2090, training loss: 417.34881591796875 = 0.3647347092628479 + 50.0 * 8.339681625366211
Epoch 2090, val loss: 0.47461751103401184
Epoch 2100, training loss: 417.34002685546875 = 0.3641132712364197 + 50.0 * 8.339518547058105
Epoch 2100, val loss: 0.47449707984924316
Epoch 2110, training loss: 417.3345642089844 = 0.3634893596172333 + 50.0 * 8.339421272277832
Epoch 2110, val loss: 0.47439953684806824
Epoch 2120, training loss: 417.4947509765625 = 0.3628539741039276 + 50.0 * 8.34263801574707
Epoch 2120, val loss: 0.4743216931819916
Epoch 2130, training loss: 417.41015625 = 0.3622027337551117 + 50.0 * 8.340958595275879
Epoch 2130, val loss: 0.47402673959732056
Epoch 2140, training loss: 417.32330322265625 = 0.36155369877815247 + 50.0 * 8.339235305786133
Epoch 2140, val loss: 0.4739702343940735
Epoch 2150, training loss: 417.2629699707031 = 0.36091503500938416 + 50.0 * 8.338041305541992
Epoch 2150, val loss: 0.4738016128540039
Epoch 2160, training loss: 417.2673034667969 = 0.3602878451347351 + 50.0 * 8.338140487670898
Epoch 2160, val loss: 0.4736846387386322
Epoch 2170, training loss: 417.387939453125 = 0.3596530258655548 + 50.0 * 8.34056568145752
Epoch 2170, val loss: 0.47355177998542786
Epoch 2180, training loss: 417.2505187988281 = 0.35899272561073303 + 50.0 * 8.337830543518066
Epoch 2180, val loss: 0.47336164116859436
Epoch 2190, training loss: 417.24267578125 = 0.3583439588546753 + 50.0 * 8.337686538696289
Epoch 2190, val loss: 0.4732292890548706
Epoch 2200, training loss: 417.2774963378906 = 0.35770097374916077 + 50.0 * 8.338396072387695
Epoch 2200, val loss: 0.4730968475341797
Epoch 2210, training loss: 417.2048034667969 = 0.3570536971092224 + 50.0 * 8.336955070495605
Epoch 2210, val loss: 0.47297531366348267
Epoch 2220, training loss: 417.1762390136719 = 0.35641369223594666 + 50.0 * 8.336396217346191
Epoch 2220, val loss: 0.47288182377815247
Epoch 2230, training loss: 417.24810791015625 = 0.3557753562927246 + 50.0 * 8.337846755981445
Epoch 2230, val loss: 0.4727441668510437
Epoch 2240, training loss: 417.2207946777344 = 0.3551153540611267 + 50.0 * 8.337313652038574
Epoch 2240, val loss: 0.4725503921508789
Epoch 2250, training loss: 417.1623840332031 = 0.35446518659591675 + 50.0 * 8.336158752441406
Epoch 2250, val loss: 0.4723973274230957
Epoch 2260, training loss: 417.1414794921875 = 0.3538215756416321 + 50.0 * 8.335753440856934
Epoch 2260, val loss: 0.4722849726676941
Epoch 2270, training loss: 417.2201843261719 = 0.35318127274513245 + 50.0 * 8.337340354919434
Epoch 2270, val loss: 0.47207212448120117
Epoch 2280, training loss: 417.1083679199219 = 0.35252058506011963 + 50.0 * 8.33511734008789
Epoch 2280, val loss: 0.472016841173172
Epoch 2290, training loss: 417.1134033203125 = 0.35186758637428284 + 50.0 * 8.335230827331543
Epoch 2290, val loss: 0.4718594551086426
Epoch 2300, training loss: 417.1602478027344 = 0.3512183725833893 + 50.0 * 8.336180686950684
Epoch 2300, val loss: 0.4716687500476837
Epoch 2310, training loss: 417.09674072265625 = 0.3505609333515167 + 50.0 * 8.33492374420166
Epoch 2310, val loss: 0.4715941250324249
Epoch 2320, training loss: 417.05828857421875 = 0.34990546107292175 + 50.0 * 8.33416748046875
Epoch 2320, val loss: 0.471346914768219
Epoch 2330, training loss: 417.048095703125 = 0.34925222396850586 + 50.0 * 8.333976745605469
Epoch 2330, val loss: 0.4712110161781311
Epoch 2340, training loss: 417.05621337890625 = 0.3485998809337616 + 50.0 * 8.334152221679688
Epoch 2340, val loss: 0.47113120555877686
Epoch 2350, training loss: 417.2044982910156 = 0.34793806076049805 + 50.0 * 8.33713150024414
Epoch 2350, val loss: 0.47099101543426514
Epoch 2360, training loss: 417.0811462402344 = 0.3472547233104706 + 50.0 * 8.334677696228027
Epoch 2360, val loss: 0.47084444761276245
Epoch 2370, training loss: 417.0040283203125 = 0.34658190608024597 + 50.0 * 8.333148956298828
Epoch 2370, val loss: 0.47069406509399414
Epoch 2380, training loss: 416.98028564453125 = 0.34592288732528687 + 50.0 * 8.332687377929688
Epoch 2380, val loss: 0.47056421637535095
Epoch 2390, training loss: 416.96356201171875 = 0.3452630937099457 + 50.0 * 8.332365989685059
Epoch 2390, val loss: 0.4704338312149048
Epoch 2400, training loss: 417.0045166015625 = 0.3445999026298523 + 50.0 * 8.333198547363281
Epoch 2400, val loss: 0.4702255129814148
Epoch 2410, training loss: 417.0555114746094 = 0.34391796588897705 + 50.0 * 8.33423137664795
Epoch 2410, val loss: 0.47017231583595276
Epoch 2420, training loss: 416.95074462890625 = 0.343220978975296 + 50.0 * 8.33215045928955
Epoch 2420, val loss: 0.4698958396911621
Epoch 2430, training loss: 416.9545593261719 = 0.34254729747772217 + 50.0 * 8.332240104675293
Epoch 2430, val loss: 0.46988579630851746
Epoch 2440, training loss: 416.9337463378906 = 0.34187018871307373 + 50.0 * 8.33183765411377
Epoch 2440, val loss: 0.46968814730644226
Epoch 2450, training loss: 416.99346923828125 = 0.3411944508552551 + 50.0 * 8.333045959472656
Epoch 2450, val loss: 0.46951717138290405
Epoch 2460, training loss: 416.94384765625 = 0.34050253033638 + 50.0 * 8.332066535949707
Epoch 2460, val loss: 0.4694918096065521
Epoch 2470, training loss: 416.92340087890625 = 0.3398064374923706 + 50.0 * 8.331671714782715
Epoch 2470, val loss: 0.46932560205459595
Epoch 2480, training loss: 416.9562683105469 = 0.339119553565979 + 50.0 * 8.332343101501465
Epoch 2480, val loss: 0.46912550926208496
Epoch 2490, training loss: 416.9271545410156 = 0.33841872215270996 + 50.0 * 8.331774711608887
Epoch 2490, val loss: 0.4690471589565277
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8163368848300354
0.8633630370209375
The final CL Acc:0.82158, 0.00371, The final GNN Acc:0.86397, 0.00075
Begin epxeriment: noisy_level: 0.2 cont_weight: 50 epoch: der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106434])
remove edge: torch.Size([2, 70966])
updated graph: torch.Size([2, 88752])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 530.2156372070312 = 1.1004317998886108 + 50.0 * 10.582304954528809
Epoch 0, val loss: 1.0982929468154907
Epoch 10, training loss: 530.1994018554688 = 1.0961709022521973 + 50.0 * 10.582063674926758
Epoch 10, val loss: 1.0940665006637573
Epoch 20, training loss: 530.14501953125 = 1.091640591621399 + 50.0 * 10.58106803894043
Epoch 20, val loss: 1.0895782709121704
Epoch 30, training loss: 529.9276123046875 = 1.086616039276123 + 50.0 * 10.576820373535156
Epoch 30, val loss: 1.0845990180969238
Epoch 40, training loss: 529.0120239257812 = 1.0809996128082275 + 50.0 * 10.55862045288086
Epoch 40, val loss: 1.0790601968765259
Epoch 50, training loss: 525.6045532226562 = 1.0746628046035767 + 50.0 * 10.490598678588867
Epoch 50, val loss: 1.0728260278701782
Epoch 60, training loss: 515.10302734375 = 1.0680620670318604 + 50.0 * 10.280699729919434
Epoch 60, val loss: 1.066472053527832
Epoch 70, training loss: 490.1041564941406 = 1.061927318572998 + 50.0 * 9.780844688415527
Epoch 70, val loss: 1.0605034828186035
Epoch 80, training loss: 478.2099914550781 = 1.0564848184585571 + 50.0 * 9.543069839477539
Epoch 80, val loss: 1.0553762912750244
Epoch 90, training loss: 474.9822082519531 = 1.0530489683151245 + 50.0 * 9.478583335876465
Epoch 90, val loss: 1.0521130561828613
Epoch 100, training loss: 468.9355163574219 = 1.0507375001907349 + 50.0 * 9.357695579528809
Epoch 100, val loss: 1.0499287843704224
Epoch 110, training loss: 463.697265625 = 1.0487239360809326 + 50.0 * 9.252970695495605
Epoch 110, val loss: 1.0479979515075684
Epoch 120, training loss: 461.4341125488281 = 1.0461249351501465 + 50.0 * 9.207759857177734
Epoch 120, val loss: 1.0454143285751343
Epoch 130, training loss: 459.888427734375 = 1.0427221059799194 + 50.0 * 9.17691421508789
Epoch 130, val loss: 1.0420852899551392
Epoch 140, training loss: 457.3934326171875 = 1.039663314819336 + 50.0 * 9.1270751953125
Epoch 140, val loss: 1.0392204523086548
Epoch 150, training loss: 453.568359375 = 1.0378695726394653 + 50.0 * 9.050609588623047
Epoch 150, val loss: 1.0377020835876465
Epoch 160, training loss: 448.6661071777344 = 1.0371257066726685 + 50.0 * 8.952579498291016
Epoch 160, val loss: 1.0371638536453247
Epoch 170, training loss: 445.2654113769531 = 1.0366653203964233 + 50.0 * 8.884574890136719
Epoch 170, val loss: 1.0367027521133423
Epoch 180, training loss: 442.8327331542969 = 1.0352544784545898 + 50.0 * 8.835949897766113
Epoch 180, val loss: 1.0353480577468872
Epoch 190, training loss: 441.2674255371094 = 1.0333032608032227 + 50.0 * 8.804682731628418
Epoch 190, val loss: 1.0334351062774658
Epoch 200, training loss: 439.6457214355469 = 1.0313316583633423 + 50.0 * 8.772287368774414
Epoch 200, val loss: 1.0315608978271484
Epoch 210, training loss: 438.0713195800781 = 1.0296586751937866 + 50.0 * 8.740833282470703
Epoch 210, val loss: 1.0300045013427734
Epoch 220, training loss: 436.8755187988281 = 1.0280941724777222 + 50.0 * 8.716948509216309
Epoch 220, val loss: 1.0285276174545288
Epoch 230, training loss: 435.9294738769531 = 1.0263588428497314 + 50.0 * 8.6980619430542
Epoch 230, val loss: 1.026849389076233
Epoch 240, training loss: 435.15557861328125 = 1.024430513381958 + 50.0 * 8.682622909545898
Epoch 240, val loss: 1.0250005722045898
Epoch 250, training loss: 434.5150146484375 = 1.0223418474197388 + 50.0 * 8.669853210449219
Epoch 250, val loss: 1.0229722261428833
Epoch 260, training loss: 433.9601745605469 = 1.0200060606002808 + 50.0 * 8.65880298614502
Epoch 260, val loss: 1.0207149982452393
Epoch 270, training loss: 433.3092041015625 = 1.0175986289978027 + 50.0 * 8.645832061767578
Epoch 270, val loss: 1.018399953842163
Epoch 280, training loss: 432.6726989746094 = 1.0152058601379395 + 50.0 * 8.633150100708008
Epoch 280, val loss: 1.016102910041809
Epoch 290, training loss: 432.016845703125 = 1.0127867460250854 + 50.0 * 8.620080947875977
Epoch 290, val loss: 1.0137892961502075
Epoch 300, training loss: 431.4012145996094 = 1.010293960571289 + 50.0 * 8.607818603515625
Epoch 300, val loss: 1.011398434638977
Epoch 310, training loss: 431.0107727050781 = 1.0075840950012207 + 50.0 * 8.60006332397461
Epoch 310, val loss: 1.0087451934814453
Epoch 320, training loss: 430.2935485839844 = 1.0046412944793701 + 50.0 * 8.58577823638916
Epoch 320, val loss: 1.0059229135513306
Epoch 330, training loss: 429.7021789550781 = 1.001591444015503 + 50.0 * 8.57401180267334
Epoch 330, val loss: 1.0030053853988647
Epoch 340, training loss: 429.1356201171875 = 0.9984142780303955 + 50.0 * 8.562744140625
Epoch 340, val loss: 0.9999418258666992
Epoch 350, training loss: 428.79229736328125 = 0.9950966238975525 + 50.0 * 8.555944442749023
Epoch 350, val loss: 0.9967513680458069
Epoch 360, training loss: 428.2248840332031 = 0.9915621876716614 + 50.0 * 8.544666290283203
Epoch 360, val loss: 0.99334317445755
Epoch 370, training loss: 427.9089660644531 = 0.9878307580947876 + 50.0 * 8.538422584533691
Epoch 370, val loss: 0.9897419810295105
Epoch 380, training loss: 427.52728271484375 = 0.9838665127754211 + 50.0 * 8.530868530273438
Epoch 380, val loss: 0.9859226942062378
Epoch 390, training loss: 427.2292785644531 = 0.9797224998474121 + 50.0 * 8.524991035461426
Epoch 390, val loss: 0.9819389581680298
Epoch 400, training loss: 426.97222900390625 = 0.9754037261009216 + 50.0 * 8.519936561584473
Epoch 400, val loss: 0.9777741432189941
Epoch 410, training loss: 426.80474853515625 = 0.9709116816520691 + 50.0 * 8.516676902770996
Epoch 410, val loss: 0.9734191298484802
Epoch 420, training loss: 426.5404968261719 = 0.9661993384361267 + 50.0 * 8.511486053466797
Epoch 420, val loss: 0.9689033627510071
Epoch 430, training loss: 426.27581787109375 = 0.9614070057868958 + 50.0 * 8.506288528442383
Epoch 430, val loss: 0.9642659425735474
Epoch 440, training loss: 426.040771484375 = 0.9564881324768066 + 50.0 * 8.501686096191406
Epoch 440, val loss: 0.9595399498939514
Epoch 450, training loss: 425.9055480957031 = 0.9514210820198059 + 50.0 * 8.499082565307617
Epoch 450, val loss: 0.9546090960502625
Epoch 460, training loss: 425.6076354980469 = 0.9462061524391174 + 50.0 * 8.493228912353516
Epoch 460, val loss: 0.9495974183082581
Epoch 470, training loss: 425.3796081542969 = 0.9408937096595764 + 50.0 * 8.488774299621582
Epoch 470, val loss: 0.9444760680198669
Epoch 480, training loss: 425.25604248046875 = 0.935384213924408 + 50.0 * 8.48641300201416
Epoch 480, val loss: 0.9391602277755737
Epoch 490, training loss: 425.0257263183594 = 0.9297158122062683 + 50.0 * 8.48192024230957
Epoch 490, val loss: 0.9336739182472229
Epoch 500, training loss: 424.8381042480469 = 0.9239224195480347 + 50.0 * 8.478283882141113
Epoch 500, val loss: 0.9281033277511597
Epoch 510, training loss: 424.7060852050781 = 0.9180122017860413 + 50.0 * 8.475761413574219
Epoch 510, val loss: 0.9224113821983337
Epoch 520, training loss: 424.5826416015625 = 0.9119409322738647 + 50.0 * 8.473414421081543
Epoch 520, val loss: 0.9165463447570801
Epoch 530, training loss: 424.42694091796875 = 0.9057597517967224 + 50.0 * 8.470423698425293
Epoch 530, val loss: 0.9106266498565674
Epoch 540, training loss: 424.3372497558594 = 0.8995169401168823 + 50.0 * 8.468754768371582
Epoch 540, val loss: 0.9046236872673035
Epoch 550, training loss: 424.1941833496094 = 0.8931476473808289 + 50.0 * 8.466020584106445
Epoch 550, val loss: 0.8985529541969299
Epoch 560, training loss: 424.08026123046875 = 0.8867048621177673 + 50.0 * 8.463871002197266
Epoch 560, val loss: 0.8924141526222229
Epoch 570, training loss: 424.0231628417969 = 0.8801599144935608 + 50.0 * 8.462860107421875
Epoch 570, val loss: 0.8861778378486633
Epoch 580, training loss: 423.87261962890625 = 0.8735281229019165 + 50.0 * 8.459981918334961
Epoch 580, val loss: 0.8798536658287048
Epoch 590, training loss: 424.1245422363281 = 0.8668350577354431 + 50.0 * 8.465154647827148
Epoch 590, val loss: 0.8734574317932129
Epoch 600, training loss: 423.67333984375 = 0.8599857091903687 + 50.0 * 8.456267356872559
Epoch 600, val loss: 0.8670240640640259
Epoch 610, training loss: 423.60284423828125 = 0.8531908392906189 + 50.0 * 8.45499324798584
Epoch 610, val loss: 0.860650360584259
Epoch 620, training loss: 423.5052795410156 = 0.8464057445526123 + 50.0 * 8.453177452087402
Epoch 620, val loss: 0.8542905449867249
Epoch 630, training loss: 423.4045715332031 = 0.8396201133728027 + 50.0 * 8.451298713684082
Epoch 630, val loss: 0.8479135036468506
Epoch 640, training loss: 423.5209655761719 = 0.8328182697296143 + 50.0 * 8.453763008117676
Epoch 640, val loss: 0.8415468335151672
Epoch 650, training loss: 423.2752685546875 = 0.8259641528129578 + 50.0 * 8.448986053466797
Epoch 650, val loss: 0.83517986536026
Epoch 660, training loss: 423.1604919433594 = 0.8191953897476196 + 50.0 * 8.446825981140137
Epoch 660, val loss: 0.8289003372192383
Epoch 670, training loss: 423.1018371582031 = 0.8125088810920715 + 50.0 * 8.445786476135254
Epoch 670, val loss: 0.8227301239967346
Epoch 680, training loss: 423.32757568359375 = 0.80585777759552 + 50.0 * 8.450434684753418
Epoch 680, val loss: 0.8166008591651917
Epoch 690, training loss: 422.9851379394531 = 0.7992320656776428 + 50.0 * 8.443717956542969
Epoch 690, val loss: 0.810529351234436
Epoch 700, training loss: 422.8686218261719 = 0.7927753329277039 + 50.0 * 8.441516876220703
Epoch 700, val loss: 0.8046931028366089
Epoch 710, training loss: 422.7936096191406 = 0.7864652276039124 + 50.0 * 8.440142631530762
Epoch 710, val loss: 0.7989938259124756
Epoch 720, training loss: 422.7103271484375 = 0.7802985906600952 + 50.0 * 8.438600540161133
Epoch 720, val loss: 0.7934337258338928
Epoch 730, training loss: 422.6625671386719 = 0.7742558121681213 + 50.0 * 8.437766075134277
Epoch 730, val loss: 0.7880196571350098
Epoch 740, training loss: 422.6554870605469 = 0.7682795524597168 + 50.0 * 8.437744140625
Epoch 740, val loss: 0.7827096581459045
Epoch 750, training loss: 422.5369873046875 = 0.7624220848083496 + 50.0 * 8.435491561889648
Epoch 750, val loss: 0.777466893196106
Epoch 760, training loss: 422.46282958984375 = 0.7567692995071411 + 50.0 * 8.434121131896973
Epoch 760, val loss: 0.7724950909614563
Epoch 770, training loss: 422.3876037597656 = 0.751305341720581 + 50.0 * 8.43272590637207
Epoch 770, val loss: 0.7677066326141357
Epoch 780, training loss: 422.3930358886719 = 0.7459997534751892 + 50.0 * 8.432940483093262
Epoch 780, val loss: 0.7630751132965088
Epoch 790, training loss: 422.2680358886719 = 0.7407656908035278 + 50.0 * 8.430545806884766
Epoch 790, val loss: 0.7585681080818176
Epoch 800, training loss: 422.20672607421875 = 0.7357329726219177 + 50.0 * 8.429420471191406
Epoch 800, val loss: 0.7542142271995544
Epoch 810, training loss: 422.15667724609375 = 0.7308853268623352 + 50.0 * 8.428515434265137
Epoch 810, val loss: 0.7500644326210022
Epoch 820, training loss: 422.1842956542969 = 0.7262057662010193 + 50.0 * 8.42916202545166
Epoch 820, val loss: 0.7460530996322632
Epoch 830, training loss: 422.0803527832031 = 0.7216036319732666 + 50.0 * 8.42717456817627
Epoch 830, val loss: 0.7422760725021362
Epoch 840, training loss: 421.99578857421875 = 0.7172160148620605 + 50.0 * 8.42557144165039
Epoch 840, val loss: 0.7385295629501343
Epoch 850, training loss: 421.91778564453125 = 0.7129884362220764 + 50.0 * 8.42409610748291
Epoch 850, val loss: 0.7350475192070007
Epoch 860, training loss: 421.8943176269531 = 0.7089199423789978 + 50.0 * 8.423707962036133
Epoch 860, val loss: 0.7317178845405579
Epoch 870, training loss: 422.14971923828125 = 0.7049373984336853 + 50.0 * 8.428895950317383
Epoch 870, val loss: 0.7283940315246582
Epoch 880, training loss: 421.8174133300781 = 0.7010375261306763 + 50.0 * 8.422327041625977
Epoch 880, val loss: 0.7252346873283386
Epoch 890, training loss: 421.7362060546875 = 0.6973965764045715 + 50.0 * 8.4207763671875
Epoch 890, val loss: 0.722320020198822
Epoch 900, training loss: 421.6831970214844 = 0.6939069628715515 + 50.0 * 8.419785499572754
Epoch 900, val loss: 0.7195544838905334
Epoch 910, training loss: 421.6258544921875 = 0.69056636095047 + 50.0 * 8.418705940246582
Epoch 910, val loss: 0.7169133424758911
Epoch 920, training loss: 421.5730895996094 = 0.6873429417610168 + 50.0 * 8.417715072631836
Epoch 920, val loss: 0.7143850922584534
Epoch 930, training loss: 421.5364074707031 = 0.6842321157455444 + 50.0 * 8.417043685913086
Epoch 930, val loss: 0.711969256401062
Epoch 940, training loss: 421.7525939941406 = 0.6811985373497009 + 50.0 * 8.421427726745605
Epoch 940, val loss: 0.7096861600875854
Epoch 950, training loss: 421.6228332519531 = 0.6782516241073608 + 50.0 * 8.418891906738281
Epoch 950, val loss: 0.7072833776473999
Epoch 960, training loss: 421.4518127441406 = 0.6753755807876587 + 50.0 * 8.415528297424316
Epoch 960, val loss: 0.7051180601119995
Epoch 970, training loss: 421.3887023925781 = 0.6726639270782471 + 50.0 * 8.414320945739746
Epoch 970, val loss: 0.7030935883522034
Epoch 980, training loss: 421.3465881347656 = 0.670086681842804 + 50.0 * 8.413530349731445
Epoch 980, val loss: 0.7011899948120117
Epoch 990, training loss: 421.3484191894531 = 0.6676058769226074 + 50.0 * 8.413616180419922
Epoch 990, val loss: 0.6993439793586731
Epoch 1000, training loss: 421.39111328125 = 0.665169894695282 + 50.0 * 8.414519309997559
Epoch 1000, val loss: 0.6975346803665161
Epoch 1010, training loss: 421.2906188964844 = 0.6627869009971619 + 50.0 * 8.412556648254395
Epoch 1010, val loss: 0.6958314776420593
Epoch 1020, training loss: 421.228515625 = 0.6605244278907776 + 50.0 * 8.411359786987305
Epoch 1020, val loss: 0.6942213177680969
Epoch 1030, training loss: 421.1912536621094 = 0.6583291888237 + 50.0 * 8.410658836364746
Epoch 1030, val loss: 0.6926558613777161
Epoch 1040, training loss: 421.3438720703125 = 0.6561898589134216 + 50.0 * 8.413753509521484
Epoch 1040, val loss: 0.6911503076553345
Epoch 1050, training loss: 421.1888732910156 = 0.6540917158126831 + 50.0 * 8.410696029663086
Epoch 1050, val loss: 0.689681887626648
Epoch 1060, training loss: 421.16046142578125 = 0.6520577073097229 + 50.0 * 8.410167694091797
Epoch 1060, val loss: 0.6882480382919312
Epoch 1070, training loss: 421.1195068359375 = 0.6500932574272156 + 50.0 * 8.409388542175293
Epoch 1070, val loss: 0.6869122982025146
Epoch 1080, training loss: 421.0472412109375 = 0.6482039093971252 + 50.0 * 8.407980918884277
Epoch 1080, val loss: 0.6855770945549011
Epoch 1090, training loss: 421.025390625 = 0.6463695764541626 + 50.0 * 8.407580375671387
Epoch 1090, val loss: 0.6843405961990356
Epoch 1100, training loss: 421.2499084472656 = 0.6445854902267456 + 50.0 * 8.41210651397705
Epoch 1100, val loss: 0.6830623745918274
Epoch 1110, training loss: 421.0599365234375 = 0.6427163481712341 + 50.0 * 8.408344268798828
Epoch 1110, val loss: 0.6819143891334534
Epoch 1120, training loss: 421.054443359375 = 0.6409961581230164 + 50.0 * 8.408268928527832
Epoch 1120, val loss: 0.680647075176239
Epoch 1130, training loss: 420.9271545410156 = 0.6392549276351929 + 50.0 * 8.405757904052734
Epoch 1130, val loss: 0.6795623302459717
Epoch 1140, training loss: 420.8971862792969 = 0.6376075148582458 + 50.0 * 8.405191421508789
Epoch 1140, val loss: 0.6785160303115845
Epoch 1150, training loss: 420.8682556152344 = 0.6360258460044861 + 50.0 * 8.404644966125488
Epoch 1150, val loss: 0.6774436235427856
Epoch 1160, training loss: 420.8347473144531 = 0.6344638466835022 + 50.0 * 8.404006004333496
Epoch 1160, val loss: 0.6764792799949646
Epoch 1170, training loss: 420.840576171875 = 0.6329321265220642 + 50.0 * 8.404152870178223
Epoch 1170, val loss: 0.6755067706108093
Epoch 1180, training loss: 421.1204528808594 = 0.631363034248352 + 50.0 * 8.409781455993652
Epoch 1180, val loss: 0.6743986010551453
Epoch 1190, training loss: 420.90325927734375 = 0.6297730207443237 + 50.0 * 8.40546989440918
Epoch 1190, val loss: 0.6734322309494019
Epoch 1200, training loss: 420.7538757324219 = 0.6282920837402344 + 50.0 * 8.402511596679688
Epoch 1200, val loss: 0.6724755167961121
Epoch 1210, training loss: 420.7145690917969 = 0.626884937286377 + 50.0 * 8.401753425598145
Epoch 1210, val loss: 0.6715406775474548
Epoch 1220, training loss: 420.6913146972656 = 0.6254965662956238 + 50.0 * 8.40131664276123
Epoch 1220, val loss: 0.670670747756958
Epoch 1230, training loss: 420.6836853027344 = 0.6241350173950195 + 50.0 * 8.401190757751465
Epoch 1230, val loss: 0.6697800755500793
Epoch 1240, training loss: 420.81866455078125 = 0.6227547526359558 + 50.0 * 8.403918266296387
Epoch 1240, val loss: 0.6688573956489563
Epoch 1250, training loss: 420.6219482421875 = 0.6213296055793762 + 50.0 * 8.400012016296387
Epoch 1250, val loss: 0.6680542230606079
Epoch 1260, training loss: 420.602294921875 = 0.6199870705604553 + 50.0 * 8.399645805358887
Epoch 1260, val loss: 0.6671672463417053
Epoch 1270, training loss: 420.6050720214844 = 0.6186716556549072 + 50.0 * 8.399727821350098
Epoch 1270, val loss: 0.6663815975189209
Epoch 1280, training loss: 420.7298583984375 = 0.6173511743545532 + 50.0 * 8.402250289916992
Epoch 1280, val loss: 0.6655423045158386
Epoch 1290, training loss: 420.5607604980469 = 0.6160498261451721 + 50.0 * 8.398894309997559
Epoch 1290, val loss: 0.6647138595581055
Epoch 1300, training loss: 420.48992919921875 = 0.614762544631958 + 50.0 * 8.397502899169922
Epoch 1300, val loss: 0.6638827919960022
Epoch 1310, training loss: 420.5372314453125 = 0.613497257232666 + 50.0 * 8.39847469329834
Epoch 1310, val loss: 0.6631680727005005
Epoch 1320, training loss: 420.4302673339844 = 0.6122312545776367 + 50.0 * 8.396360397338867
Epoch 1320, val loss: 0.6623037457466125
Epoch 1330, training loss: 420.4136657714844 = 0.6109853982925415 + 50.0 * 8.396053314208984
Epoch 1330, val loss: 0.6615214943885803
Epoch 1340, training loss: 420.4718933105469 = 0.6097599864006042 + 50.0 * 8.397242546081543
Epoch 1340, val loss: 0.6607062816619873
Epoch 1350, training loss: 420.42083740234375 = 0.6084885597229004 + 50.0 * 8.396246910095215
Epoch 1350, val loss: 0.6599587798118591
Epoch 1360, training loss: 420.3778991699219 = 0.6072363257408142 + 50.0 * 8.395413398742676
Epoch 1360, val loss: 0.6591894626617432
Epoch 1370, training loss: 420.3807067871094 = 0.6060209274291992 + 50.0 * 8.395493507385254
Epoch 1370, val loss: 0.6584292054176331
Epoch 1380, training loss: 420.3403625488281 = 0.6047949194908142 + 50.0 * 8.3947114944458
Epoch 1380, val loss: 0.6576017141342163
Epoch 1390, training loss: 420.2807312011719 = 0.6035711765289307 + 50.0 * 8.393543243408203
Epoch 1390, val loss: 0.6568527817726135
Epoch 1400, training loss: 420.2583312988281 = 0.6023752093315125 + 50.0 * 8.393118858337402
Epoch 1400, val loss: 0.6560595035552979
Epoch 1410, training loss: 420.2522888183594 = 0.6011795997619629 + 50.0 * 8.393022537231445
Epoch 1410, val loss: 0.6553266644477844
Epoch 1420, training loss: 420.4383850097656 = 0.5999466180801392 + 50.0 * 8.396768569946289
Epoch 1420, val loss: 0.6545786261558533
Epoch 1430, training loss: 420.2691955566406 = 0.59872967004776 + 50.0 * 8.393409729003906
Epoch 1430, val loss: 0.6536583304405212
Epoch 1440, training loss: 420.2055358886719 = 0.5975013971328735 + 50.0 * 8.392160415649414
Epoch 1440, val loss: 0.6529324650764465
Epoch 1450, training loss: 420.2658996582031 = 0.5963149666786194 + 50.0 * 8.393391609191895
Epoch 1450, val loss: 0.652068555355072
Epoch 1460, training loss: 420.1297302246094 = 0.5950688719749451 + 50.0 * 8.390693664550781
Epoch 1460, val loss: 0.6512961387634277
Epoch 1470, training loss: 420.1064758300781 = 0.5938743948936462 + 50.0 * 8.390252113342285
Epoch 1470, val loss: 0.6504518985748291
Epoch 1480, training loss: 420.12530517578125 = 0.5926690101623535 + 50.0 * 8.390652656555176
Epoch 1480, val loss: 0.6497128009796143
Epoch 1490, training loss: 420.08551025390625 = 0.5914579033851624 + 50.0 * 8.389881134033203
Epoch 1490, val loss: 0.6488783359527588
Epoch 1500, training loss: 420.0262145996094 = 0.5902441143989563 + 50.0 * 8.38871955871582
Epoch 1500, val loss: 0.6480628848075867
Epoch 1510, training loss: 420.0357971191406 = 0.589035153388977 + 50.0 * 8.388935089111328
Epoch 1510, val loss: 0.6472451090812683
Epoch 1520, training loss: 420.2193908691406 = 0.5877909660339355 + 50.0 * 8.392631530761719
Epoch 1520, val loss: 0.6463466286659241
Epoch 1530, training loss: 420.0087890625 = 0.5864708423614502 + 50.0 * 8.388446807861328
Epoch 1530, val loss: 0.6455731987953186
Epoch 1540, training loss: 419.9615173339844 = 0.5852257609367371 + 50.0 * 8.38752555847168
Epoch 1540, val loss: 0.6446465253829956
Epoch 1550, training loss: 419.9128112792969 = 0.5839853882789612 + 50.0 * 8.386576652526855
Epoch 1550, val loss: 0.6438442468643188
Epoch 1560, training loss: 419.94921875 = 0.5827524065971375 + 50.0 * 8.3873291015625
Epoch 1560, val loss: 0.6429912447929382
Epoch 1570, training loss: 420.0177307128906 = 0.5814638733863831 + 50.0 * 8.388725280761719
Epoch 1570, val loss: 0.6420873403549194
Epoch 1580, training loss: 419.9091491699219 = 0.5801727175712585 + 50.0 * 8.386579513549805
Epoch 1580, val loss: 0.6411950588226318
Epoch 1590, training loss: 419.85736083984375 = 0.5788891911506653 + 50.0 * 8.38556957244873
Epoch 1590, val loss: 0.6402599811553955
Epoch 1600, training loss: 419.8214111328125 = 0.5776081681251526 + 50.0 * 8.384876251220703
Epoch 1600, val loss: 0.6393985748291016
Epoch 1610, training loss: 419.8480224609375 = 0.576328694820404 + 50.0 * 8.3854341506958
Epoch 1610, val loss: 0.6384246945381165
Epoch 1620, training loss: 419.8572692871094 = 0.574998676776886 + 50.0 * 8.385644912719727
Epoch 1620, val loss: 0.6374692320823669
Epoch 1630, training loss: 419.88507080078125 = 0.5736260414123535 + 50.0 * 8.386228561401367
Epoch 1630, val loss: 0.6364517211914062
Epoch 1640, training loss: 419.7806091308594 = 0.5722382068634033 + 50.0 * 8.384167671203613
Epoch 1640, val loss: 0.6356162428855896
Epoch 1650, training loss: 419.7442321777344 = 0.5709007382392883 + 50.0 * 8.383466720581055
Epoch 1650, val loss: 0.6345977187156677
Epoch 1660, training loss: 419.7177429199219 = 0.5695626139640808 + 50.0 * 8.382964134216309
Epoch 1660, val loss: 0.6336341500282288
Epoch 1670, training loss: 419.70477294921875 = 0.5682104229927063 + 50.0 * 8.382731437683105
Epoch 1670, val loss: 0.6326937675476074
Epoch 1680, training loss: 419.83453369140625 = 0.566838264465332 + 50.0 * 8.385354042053223
Epoch 1680, val loss: 0.6316784620285034
Epoch 1690, training loss: 419.9129638671875 = 0.5654022097587585 + 50.0 * 8.386951446533203
Epoch 1690, val loss: 0.6306707859039307
Epoch 1700, training loss: 419.75445556640625 = 0.5639050602912903 + 50.0 * 8.383810997009277
Epoch 1700, val loss: 0.6296198964118958
Epoch 1710, training loss: 419.6690368652344 = 0.5624687075614929 + 50.0 * 8.382131576538086
Epoch 1710, val loss: 0.6285072565078735
Epoch 1720, training loss: 419.63934326171875 = 0.5610345005989075 + 50.0 * 8.381566047668457
Epoch 1720, val loss: 0.6275132298469543
Epoch 1730, training loss: 419.6070861816406 = 0.5596119165420532 + 50.0 * 8.380949974060059
Epoch 1730, val loss: 0.6264673471450806
Epoch 1740, training loss: 419.5968017578125 = 0.5581674575805664 + 50.0 * 8.380772590637207
Epoch 1740, val loss: 0.6254271864891052
Epoch 1750, training loss: 419.6392517089844 = 0.5567061305046082 + 50.0 * 8.381650924682617
Epoch 1750, val loss: 0.6243240237236023
Epoch 1760, training loss: 419.78131103515625 = 0.5551769733428955 + 50.0 * 8.384522438049316
Epoch 1760, val loss: 0.623193621635437
Epoch 1770, training loss: 419.64947509765625 = 0.5535901188850403 + 50.0 * 8.381917953491211
Epoch 1770, val loss: 0.6221347451210022
Epoch 1780, training loss: 419.5657958984375 = 0.5520622134208679 + 50.0 * 8.380274772644043
Epoch 1780, val loss: 0.6209303736686707
Epoch 1790, training loss: 419.53692626953125 = 0.5505175590515137 + 50.0 * 8.379728317260742
Epoch 1790, val loss: 0.6198290586471558
Epoch 1800, training loss: 419.5481872558594 = 0.5489807724952698 + 50.0 * 8.379983901977539
Epoch 1800, val loss: 0.6186432838439941
Epoch 1810, training loss: 419.7109680175781 = 0.5473864674568176 + 50.0 * 8.383271217346191
Epoch 1810, val loss: 0.6174675226211548
Epoch 1820, training loss: 419.53033447265625 = 0.5457671284675598 + 50.0 * 8.379691123962402
Epoch 1820, val loss: 0.6162726283073425
Epoch 1830, training loss: 419.4792785644531 = 0.5441523194313049 + 50.0 * 8.378702163696289
Epoch 1830, val loss: 0.6150639057159424
Epoch 1840, training loss: 419.4877624511719 = 0.5425394177436829 + 50.0 * 8.378904342651367
Epoch 1840, val loss: 0.6138244867324829
Epoch 1850, training loss: 419.7093811035156 = 0.5409219264984131 + 50.0 * 8.383369445800781
Epoch 1850, val loss: 0.6124805808067322
Epoch 1860, training loss: 419.4842224121094 = 0.5391885638237 + 50.0 * 8.378900527954102
Epoch 1860, val loss: 0.6114463806152344
Epoch 1870, training loss: 419.41455078125 = 0.5375266075134277 + 50.0 * 8.377540588378906
Epoch 1870, val loss: 0.6101544499397278
Epoch 1880, training loss: 419.3919677734375 = 0.5358652472496033 + 50.0 * 8.377121925354004
Epoch 1880, val loss: 0.6089038252830505
Epoch 1890, training loss: 419.4105529785156 = 0.5341877937316895 + 50.0 * 8.377527236938477
Epoch 1890, val loss: 0.6076904535293579
Epoch 1900, training loss: 419.71807861328125 = 0.5324520468711853 + 50.0 * 8.383712768554688
Epoch 1900, val loss: 0.6064063310623169
Epoch 1910, training loss: 419.4377746582031 = 0.5306850075721741 + 50.0 * 8.378142356872559
Epoch 1910, val loss: 0.6050440669059753
Epoch 1920, training loss: 419.3456115722656 = 0.528905987739563 + 50.0 * 8.376334190368652
Epoch 1920, val loss: 0.6038088798522949
Epoch 1930, training loss: 419.3158264160156 = 0.5271546244621277 + 50.0 * 8.375773429870605
Epoch 1930, val loss: 0.6024758219718933
Epoch 1940, training loss: 419.30487060546875 = 0.5254020094871521 + 50.0 * 8.375589370727539
Epoch 1940, val loss: 0.6011834144592285
Epoch 1950, training loss: 419.32159423828125 = 0.5236330032348633 + 50.0 * 8.375959396362305
Epoch 1950, val loss: 0.5998988747596741
Epoch 1960, training loss: 419.4705505371094 = 0.5218279957771301 + 50.0 * 8.378974914550781
Epoch 1960, val loss: 0.598569929599762
Epoch 1970, training loss: 419.352294921875 = 0.5199738144874573 + 50.0 * 8.376646041870117
Epoch 1970, val loss: 0.5973541140556335
Epoch 1980, training loss: 419.2920837402344 = 0.5181497931480408 + 50.0 * 8.375478744506836
Epoch 1980, val loss: 0.5959200859069824
Epoch 1990, training loss: 419.2768249511719 = 0.5163156390190125 + 50.0 * 8.37520980834961
Epoch 1990, val loss: 0.594684362411499
Epoch 2000, training loss: 419.2913818359375 = 0.5144827365875244 + 50.0 * 8.375537872314453
Epoch 2000, val loss: 0.5933136343955994
Epoch 2010, training loss: 419.31146240234375 = 0.5126197934150696 + 50.0 * 8.3759765625
Epoch 2010, val loss: 0.5919923782348633
Epoch 2020, training loss: 419.21734619140625 = 0.5107588171958923 + 50.0 * 8.374131202697754
Epoch 2020, val loss: 0.5905687808990479
Epoch 2030, training loss: 419.2121887207031 = 0.5089004039764404 + 50.0 * 8.374065399169922
Epoch 2030, val loss: 0.5892418622970581
Epoch 2040, training loss: 419.28253173828125 = 0.5070332288742065 + 50.0 * 8.375510215759277
Epoch 2040, val loss: 0.5878520011901855
Epoch 2050, training loss: 419.21331787109375 = 0.5051232576370239 + 50.0 * 8.374163627624512
Epoch 2050, val loss: 0.5866478085517883
Epoch 2060, training loss: 419.2803649902344 = 0.503225564956665 + 50.0 * 8.375542640686035
Epoch 2060, val loss: 0.585376501083374
Epoch 2070, training loss: 419.1385192871094 = 0.501350462436676 + 50.0 * 8.372743606567383
Epoch 2070, val loss: 0.5839544534683228
Epoch 2080, training loss: 419.1347961425781 = 0.499473512172699 + 50.0 * 8.372706413269043
Epoch 2080, val loss: 0.5827192068099976
Epoch 2090, training loss: 419.1190490722656 = 0.4975987374782562 + 50.0 * 8.372428894042969
Epoch 2090, val loss: 0.5814803838729858
Epoch 2100, training loss: 419.2261962890625 = 0.49571067094802856 + 50.0 * 8.37460994720459
Epoch 2100, val loss: 0.5802915096282959
Epoch 2110, training loss: 419.1535339355469 = 0.493814080953598 + 50.0 * 8.373194694519043
Epoch 2110, val loss: 0.5788990259170532
Epoch 2120, training loss: 419.11077880859375 = 0.49191445112228394 + 50.0 * 8.372377395629883
Epoch 2120, val loss: 0.577652096748352
Epoch 2130, training loss: 419.07122802734375 = 0.49003466963768005 + 50.0 * 8.371623992919922
Epoch 2130, val loss: 0.5763419270515442
Epoch 2140, training loss: 419.20050048828125 = 0.48816752433776855 + 50.0 * 8.374246597290039
Epoch 2140, val loss: 0.57505863904953
Epoch 2150, training loss: 419.05255126953125 = 0.48624300956726074 + 50.0 * 8.371326446533203
Epoch 2150, val loss: 0.5740274786949158
Epoch 2160, training loss: 419.0324401855469 = 0.4843566417694092 + 50.0 * 8.370962142944336
Epoch 2160, val loss: 0.5728279948234558
Epoch 2170, training loss: 419.00482177734375 = 0.48250246047973633 + 50.0 * 8.37044620513916
Epoch 2170, val loss: 0.571607768535614
Epoch 2180, training loss: 418.9952392578125 = 0.4806478023529053 + 50.0 * 8.370291709899902
Epoch 2180, val loss: 0.5704467296600342
Epoch 2190, training loss: 419.13494873046875 = 0.47879159450531006 + 50.0 * 8.373123168945312
Epoch 2190, val loss: 0.5692323446273804
Epoch 2200, training loss: 419.0361633300781 = 0.47688964009284973 + 50.0 * 8.371185302734375
Epoch 2200, val loss: 0.5682673454284668
Epoch 2210, training loss: 419.03155517578125 = 0.4750177562236786 + 50.0 * 8.37113094329834
Epoch 2210, val loss: 0.5671401619911194
Epoch 2220, training loss: 418.941162109375 = 0.47317248582839966 + 50.0 * 8.369359970092773
Epoch 2220, val loss: 0.5659376382827759
Epoch 2230, training loss: 418.92132568359375 = 0.4713466465473175 + 50.0 * 8.368999481201172
Epoch 2230, val loss: 0.5648463368415833
Epoch 2240, training loss: 418.94012451171875 = 0.4695351719856262 + 50.0 * 8.36941146850586
Epoch 2240, val loss: 0.5637757778167725
Epoch 2250, training loss: 419.2293701171875 = 0.4677342176437378 + 50.0 * 8.375232696533203
Epoch 2250, val loss: 0.5626229643821716
Epoch 2260, training loss: 418.96563720703125 = 0.4658699035644531 + 50.0 * 8.3699951171875
Epoch 2260, val loss: 0.5617212057113647
Epoch 2270, training loss: 418.931884765625 = 0.4640851318836212 + 50.0 * 8.369356155395508
Epoch 2270, val loss: 0.5606340169906616
Epoch 2280, training loss: 418.90631103515625 = 0.46230512857437134 + 50.0 * 8.368880271911621
Epoch 2280, val loss: 0.5596632361412048
Epoch 2290, training loss: 418.8583984375 = 0.4605516493320465 + 50.0 * 8.36795711517334
Epoch 2290, val loss: 0.5586787462234497
Epoch 2300, training loss: 418.8592224121094 = 0.45881783962249756 + 50.0 * 8.36800765991211
Epoch 2300, val loss: 0.5577041506767273
Epoch 2310, training loss: 418.8818359375 = 0.457095205783844 + 50.0 * 8.368494987487793
Epoch 2310, val loss: 0.5566901564598083
Epoch 2320, training loss: 419.0007629394531 = 0.455403596162796 + 50.0 * 8.370906829833984
Epoch 2320, val loss: 0.5555748343467712
Epoch 2330, training loss: 418.8674621582031 = 0.45362865924835205 + 50.0 * 8.368276596069336
Epoch 2330, val loss: 0.5550464391708374
Epoch 2340, training loss: 418.82293701171875 = 0.4519461393356323 + 50.0 * 8.367420196533203
Epoch 2340, val loss: 0.5539776682853699
Epoch 2350, training loss: 418.8140563964844 = 0.45027750730514526 + 50.0 * 8.36727523803711
Epoch 2350, val loss: 0.5532074570655823
Epoch 2360, training loss: 418.9722900390625 = 0.44861307740211487 + 50.0 * 8.370473861694336
Epoch 2360, val loss: 0.5524173974990845
Epoch 2370, training loss: 418.797119140625 = 0.4469550549983978 + 50.0 * 8.367003440856934
Epoch 2370, val loss: 0.5514585971832275
Epoch 2380, training loss: 418.7899169921875 = 0.4453044533729553 + 50.0 * 8.366891860961914
Epoch 2380, val loss: 0.550654411315918
Epoch 2390, training loss: 418.75830078125 = 0.4436659514904022 + 50.0 * 8.366292953491211
Epoch 2390, val loss: 0.5498836040496826
Epoch 2400, training loss: 418.7370910644531 = 0.44204115867614746 + 50.0 * 8.365900993347168
Epoch 2400, val loss: 0.5491182804107666
Epoch 2410, training loss: 418.73504638671875 = 0.44042718410491943 + 50.0 * 8.36589241027832
Epoch 2410, val loss: 0.548376739025116
Epoch 2420, training loss: 419.01776123046875 = 0.43881550431251526 + 50.0 * 8.37157917022705
Epoch 2420, val loss: 0.5476003885269165
Epoch 2430, training loss: 418.8705139160156 = 0.43720749020576477 + 50.0 * 8.368666648864746
Epoch 2430, val loss: 0.5467254519462585
Epoch 2440, training loss: 418.7981872558594 = 0.43557754158973694 + 50.0 * 8.367252349853516
Epoch 2440, val loss: 0.5461938381195068
Epoch 2450, training loss: 418.7008056640625 = 0.43398913741111755 + 50.0 * 8.365336418151855
Epoch 2450, val loss: 0.5454840064048767
Epoch 2460, training loss: 418.69525146484375 = 0.4324339032173157 + 50.0 * 8.365256309509277
Epoch 2460, val loss: 0.5447545051574707
Epoch 2470, training loss: 418.68243408203125 = 0.43089666962623596 + 50.0 * 8.365030288696289
Epoch 2470, val loss: 0.5440711975097656
Epoch 2480, training loss: 418.84918212890625 = 0.42939233779907227 + 50.0 * 8.368395805358887
Epoch 2480, val loss: 0.5433318614959717
Epoch 2490, training loss: 418.7175598144531 = 0.4278051257133484 + 50.0 * 8.365795135498047
Epoch 2490, val loss: 0.543152928352356
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7737189244038558
0.8145330725204666
=== training gcn model ===
Epoch 0, training loss: 530.20947265625 = 1.0946218967437744 + 50.0 * 10.582297325134277
Epoch 0, val loss: 1.0933256149291992
Epoch 10, training loss: 530.1913452148438 = 1.090837001800537 + 50.0 * 10.582010269165039
Epoch 10, val loss: 1.0895179510116577
Epoch 20, training loss: 530.1286010742188 = 1.0868120193481445 + 50.0 * 10.580836296081543
Epoch 20, val loss: 1.0854860544204712
Epoch 30, training loss: 529.853759765625 = 1.0824470520019531 + 50.0 * 10.57542610168457
Epoch 30, val loss: 1.0811142921447754
Epoch 40, training loss: 528.6998901367188 = 1.0775753259658813 + 50.0 * 10.552446365356445
Epoch 40, val loss: 1.0762332677841187
Epoch 50, training loss: 525.0125732421875 = 1.072068691253662 + 50.0 * 10.478809356689453
Epoch 50, val loss: 1.0707789659500122
Epoch 60, training loss: 515.5415649414062 = 1.066960096359253 + 50.0 * 10.289491653442383
Epoch 60, val loss: 1.065863013267517
Epoch 70, training loss: 497.16375732421875 = 1.0618093013763428 + 50.0 * 9.922039031982422
Epoch 70, val loss: 1.060814380645752
Epoch 80, training loss: 481.34661865234375 = 1.0569217205047607 + 50.0 * 9.605793952941895
Epoch 80, val loss: 1.056089997291565
Epoch 90, training loss: 473.5751647949219 = 1.0521589517593384 + 50.0 * 9.450460433959961
Epoch 90, val loss: 1.051410436630249
Epoch 100, training loss: 466.7015075683594 = 1.0477619171142578 + 50.0 * 9.313075065612793
Epoch 100, val loss: 1.0471749305725098
Epoch 110, training loss: 462.1186828613281 = 1.0441153049468994 + 50.0 * 9.221490859985352
Epoch 110, val loss: 1.0437296628952026
Epoch 120, training loss: 460.0579833984375 = 1.0410622358322144 + 50.0 * 9.180337905883789
Epoch 120, val loss: 1.040856122970581
Epoch 130, training loss: 457.1900634765625 = 1.0385630130767822 + 50.0 * 9.123029708862305
Epoch 130, val loss: 1.0384966135025024
Epoch 140, training loss: 453.0295104980469 = 1.036790132522583 + 50.0 * 9.039854049682617
Epoch 140, val loss: 1.0368545055389404
Epoch 150, training loss: 448.8931579589844 = 1.0358233451843262 + 50.0 * 8.957146644592285
Epoch 150, val loss: 1.0360007286071777
Epoch 160, training loss: 445.997314453125 = 1.0350830554962158 + 50.0 * 8.89924430847168
Epoch 160, val loss: 1.0353599786758423
Epoch 170, training loss: 442.9835510253906 = 1.0342620611190796 + 50.0 * 8.838985443115234
Epoch 170, val loss: 1.0345731973648071
Epoch 180, training loss: 439.8568420410156 = 1.0332075357437134 + 50.0 * 8.776473045349121
Epoch 180, val loss: 1.0336159467697144
Epoch 190, training loss: 437.68359375 = 1.032041311264038 + 50.0 * 8.733031272888184
Epoch 190, val loss: 1.032492995262146
Epoch 200, training loss: 436.1882629394531 = 1.0305006504058838 + 50.0 * 8.703155517578125
Epoch 200, val loss: 1.0309466123580933
Epoch 210, training loss: 435.01776123046875 = 1.0285210609436035 + 50.0 * 8.679784774780273
Epoch 210, val loss: 1.0289809703826904
Epoch 220, training loss: 434.0426330566406 = 1.0262781381607056 + 50.0 * 8.660326957702637
Epoch 220, val loss: 1.0267918109893799
Epoch 230, training loss: 433.3566589355469 = 1.0238080024719238 + 50.0 * 8.64665699005127
Epoch 230, val loss: 1.0244063138961792
Epoch 240, training loss: 432.66259765625 = 1.021067500114441 + 50.0 * 8.632830619812012
Epoch 240, val loss: 1.0217254161834717
Epoch 250, training loss: 432.0679016113281 = 1.0180602073669434 + 50.0 * 8.620996475219727
Epoch 250, val loss: 1.0187875032424927
Epoch 260, training loss: 431.4784851074219 = 1.0148255825042725 + 50.0 * 8.609272956848145
Epoch 260, val loss: 1.015636682510376
Epoch 270, training loss: 430.95068359375 = 1.0113203525543213 + 50.0 * 8.598787307739258
Epoch 270, val loss: 1.012253761291504
Epoch 280, training loss: 430.40679931640625 = 1.0075486898422241 + 50.0 * 8.587985038757324
Epoch 280, val loss: 1.0085735321044922
Epoch 290, training loss: 429.90521240234375 = 1.0034726858139038 + 50.0 * 8.578034400939941
Epoch 290, val loss: 1.004593014717102
Epoch 300, training loss: 429.54754638671875 = 0.9990295767784119 + 50.0 * 8.57097053527832
Epoch 300, val loss: 1.000232458114624
Epoch 310, training loss: 429.07928466796875 = 0.994189977645874 + 50.0 * 8.561701774597168
Epoch 310, val loss: 0.9954780340194702
Epoch 320, training loss: 428.7173156738281 = 0.9889969825744629 + 50.0 * 8.554566383361816
Epoch 320, val loss: 0.990414023399353
Epoch 330, training loss: 428.3984375 = 0.9834465980529785 + 50.0 * 8.548299789428711
Epoch 330, val loss: 0.9849986433982849
Epoch 340, training loss: 428.103271484375 = 0.9775278568267822 + 50.0 * 8.54251480102539
Epoch 340, val loss: 0.9792226552963257
Epoch 350, training loss: 427.97808837890625 = 0.9711894989013672 + 50.0 * 8.540138244628906
Epoch 350, val loss: 0.9730575084686279
Epoch 360, training loss: 427.63433837890625 = 0.9644526243209839 + 50.0 * 8.533397674560547
Epoch 360, val loss: 0.9664619565010071
Epoch 370, training loss: 427.4171447753906 = 0.9573683142662048 + 50.0 * 8.529195785522461
Epoch 370, val loss: 0.9595643281936646
Epoch 380, training loss: 427.21453857421875 = 0.9499621391296387 + 50.0 * 8.525291442871094
Epoch 380, val loss: 0.9523564577102661
Epoch 390, training loss: 427.0342712402344 = 0.9422456622123718 + 50.0 * 8.52184009552002
Epoch 390, val loss: 0.9448482394218445
Epoch 400, training loss: 426.945068359375 = 0.9342625141143799 + 50.0 * 8.52021598815918
Epoch 400, val loss: 0.9370241761207581
Epoch 410, training loss: 426.5952453613281 = 0.9259709715843201 + 50.0 * 8.513385772705078
Epoch 410, val loss: 0.9290213584899902
Epoch 420, training loss: 426.38482666015625 = 0.9175401329994202 + 50.0 * 8.509346008300781
Epoch 420, val loss: 0.9208683967590332
Epoch 430, training loss: 426.16033935546875 = 0.9089822769165039 + 50.0 * 8.505026817321777
Epoch 430, val loss: 0.9125807881355286
Epoch 440, training loss: 425.93780517578125 = 0.9003303050994873 + 50.0 * 8.500749588012695
Epoch 440, val loss: 0.9041820764541626
Epoch 450, training loss: 425.8005065917969 = 0.8915477991104126 + 50.0 * 8.49817943572998
Epoch 450, val loss: 0.8956664204597473
Epoch 460, training loss: 425.5435791015625 = 0.88260418176651 + 50.0 * 8.493219375610352
Epoch 460, val loss: 0.8870471715927124
Epoch 470, training loss: 425.3744812011719 = 0.8736783862113953 + 50.0 * 8.490015983581543
Epoch 470, val loss: 0.8784434795379639
Epoch 480, training loss: 425.1918029785156 = 0.8647761940956116 + 50.0 * 8.486540794372559
Epoch 480, val loss: 0.8698615431785583
Epoch 490, training loss: 425.05712890625 = 0.8558510541915894 + 50.0 * 8.484025001525879
Epoch 490, val loss: 0.8612403869628906
Epoch 500, training loss: 424.8634338378906 = 0.8469449877738953 + 50.0 * 8.480329513549805
Epoch 500, val loss: 0.8526726961135864
Epoch 510, training loss: 424.73736572265625 = 0.8380923271179199 + 50.0 * 8.477985382080078
Epoch 510, val loss: 0.8441950678825378
Epoch 520, training loss: 424.64654541015625 = 0.8292993307113647 + 50.0 * 8.47634506225586
Epoch 520, val loss: 0.835757851600647
Epoch 530, training loss: 424.5266418457031 = 0.8204889893531799 + 50.0 * 8.474123001098633
Epoch 530, val loss: 0.8273788690567017
Epoch 540, training loss: 424.4095764160156 = 0.811796247959137 + 50.0 * 8.471955299377441
Epoch 540, val loss: 0.8190727233886719
Epoch 550, training loss: 424.32305908203125 = 0.8032245635986328 + 50.0 * 8.470396995544434
Epoch 550, val loss: 0.8109444975852966
Epoch 560, training loss: 424.2306823730469 = 0.794762372970581 + 50.0 * 8.468718528747559
Epoch 560, val loss: 0.8029356598854065
Epoch 570, training loss: 424.12054443359375 = 0.7864289283752441 + 50.0 * 8.466682434082031
Epoch 570, val loss: 0.7950926423072815
Epoch 580, training loss: 424.03350830078125 = 0.7782699465751648 + 50.0 * 8.465105056762695
Epoch 580, val loss: 0.7874264121055603
Epoch 590, training loss: 424.0634765625 = 0.7702727317810059 + 50.0 * 8.465864181518555
Epoch 590, val loss: 0.7799789309501648
Epoch 600, training loss: 423.9267883300781 = 0.7624146342277527 + 50.0 * 8.463287353515625
Epoch 600, val loss: 0.7725460529327393
Epoch 610, training loss: 423.82037353515625 = 0.7547134757041931 + 50.0 * 8.461313247680664
Epoch 610, val loss: 0.7654141187667847
Epoch 620, training loss: 423.6766357421875 = 0.7472478151321411 + 50.0 * 8.458587646484375
Epoch 620, val loss: 0.7585127949714661
Epoch 630, training loss: 423.5851135253906 = 0.7399847507476807 + 50.0 * 8.456902503967285
Epoch 630, val loss: 0.7518107891082764
Epoch 640, training loss: 423.61767578125 = 0.7328957319259644 + 50.0 * 8.457695007324219
Epoch 640, val loss: 0.7453183531761169
Epoch 650, training loss: 423.4871826171875 = 0.7259965538978577 + 50.0 * 8.45522403717041
Epoch 650, val loss: 0.7389806509017944
Epoch 660, training loss: 423.3471984863281 = 0.7192692756652832 + 50.0 * 8.452558517456055
Epoch 660, val loss: 0.7328494787216187
Epoch 670, training loss: 423.2635498046875 = 0.7127447128295898 + 50.0 * 8.451016426086426
Epoch 670, val loss: 0.7269418835639954
Epoch 680, training loss: 423.1471862792969 = 0.7064344882965088 + 50.0 * 8.44881534576416
Epoch 680, val loss: 0.7212463021278381
Epoch 690, training loss: 423.1199645996094 = 0.7003099322319031 + 50.0 * 8.448392868041992
Epoch 690, val loss: 0.7157509922981262
Epoch 700, training loss: 422.9855651855469 = 0.6943256258964539 + 50.0 * 8.44582462310791
Epoch 700, val loss: 0.7104352116584778
Epoch 710, training loss: 423.1091003417969 = 0.6885958313941956 + 50.0 * 8.448410034179688
Epoch 710, val loss: 0.7053125500679016
Epoch 720, training loss: 422.8315124511719 = 0.6829455494880676 + 50.0 * 8.442971229553223
Epoch 720, val loss: 0.7003799080848694
Epoch 730, training loss: 422.7479553222656 = 0.6775873303413391 + 50.0 * 8.441407203674316
Epoch 730, val loss: 0.695654571056366
Epoch 740, training loss: 422.6553955078125 = 0.6724246740341187 + 50.0 * 8.439659118652344
Epoch 740, val loss: 0.6911618113517761
Epoch 750, training loss: 422.7982482910156 = 0.6674008369445801 + 50.0 * 8.442617416381836
Epoch 750, val loss: 0.6867721676826477
Epoch 760, training loss: 422.57403564453125 = 0.662514328956604 + 50.0 * 8.438230514526367
Epoch 760, val loss: 0.6825723648071289
Epoch 770, training loss: 422.4619445800781 = 0.6578412055969238 + 50.0 * 8.436081886291504
Epoch 770, val loss: 0.678612232208252
Epoch 780, training loss: 422.382568359375 = 0.6533887386322021 + 50.0 * 8.43458366394043
Epoch 780, val loss: 0.6748271584510803
Epoch 790, training loss: 422.3357238769531 = 0.6490838527679443 + 50.0 * 8.433732986450195
Epoch 790, val loss: 0.6711933016777039
Epoch 800, training loss: 422.3114013671875 = 0.6448777914047241 + 50.0 * 8.433330535888672
Epoch 800, val loss: 0.6676777601242065
Epoch 810, training loss: 422.3011169433594 = 0.6407724618911743 + 50.0 * 8.433206558227539
Epoch 810, val loss: 0.6642640233039856
Epoch 820, training loss: 422.19049072265625 = 0.6368378400802612 + 50.0 * 8.431073188781738
Epoch 820, val loss: 0.6610419750213623
Epoch 830, training loss: 422.09735107421875 = 0.6330915689468384 + 50.0 * 8.429285049438477
Epoch 830, val loss: 0.6580250859260559
Epoch 840, training loss: 422.1595764160156 = 0.6294953227043152 + 50.0 * 8.430602073669434
Epoch 840, val loss: 0.6552136540412903
Epoch 850, training loss: 422.04388427734375 = 0.6260333061218262 + 50.0 * 8.428357124328613
Epoch 850, val loss: 0.6525354385375977
Epoch 860, training loss: 422.0129699707031 = 0.6227594017982483 + 50.0 * 8.427803993225098
Epoch 860, val loss: 0.6500726342201233
Epoch 870, training loss: 421.93902587890625 = 0.6196297407150269 + 50.0 * 8.426387786865234
Epoch 870, val loss: 0.6477194428443909
Epoch 880, training loss: 421.861328125 = 0.6166538000106812 + 50.0 * 8.424893379211426
Epoch 880, val loss: 0.6455239057540894
Epoch 890, training loss: 421.8175048828125 = 0.6137945055961609 + 50.0 * 8.424074172973633
Epoch 890, val loss: 0.6434915065765381
Epoch 900, training loss: 421.7715759277344 = 0.6111223697662354 + 50.0 * 8.423209190368652
Epoch 900, val loss: 0.6415631771087646
Epoch 910, training loss: 422.0169677734375 = 0.6085354685783386 + 50.0 * 8.428169250488281
Epoch 910, val loss: 0.6397312879562378
Epoch 920, training loss: 421.7402648925781 = 0.6060261130332947 + 50.0 * 8.422684669494629
Epoch 920, val loss: 0.6380069851875305
Epoch 930, training loss: 421.6514587402344 = 0.6036325097084045 + 50.0 * 8.4209566116333
Epoch 930, val loss: 0.6363402009010315
Epoch 940, training loss: 421.59942626953125 = 0.6013858914375305 + 50.0 * 8.419960975646973
Epoch 940, val loss: 0.6348688006401062
Epoch 950, training loss: 421.5620422363281 = 0.5992416143417358 + 50.0 * 8.419256210327148
Epoch 950, val loss: 0.6334816217422485
Epoch 960, training loss: 421.84552001953125 = 0.5971692800521851 + 50.0 * 8.424966812133789
Epoch 960, val loss: 0.6321840286254883
Epoch 970, training loss: 421.5456848144531 = 0.5951022505760193 + 50.0 * 8.419012069702148
Epoch 970, val loss: 0.6308317184448242
Epoch 980, training loss: 421.4500427246094 = 0.5931700468063354 + 50.0 * 8.417137145996094
Epoch 980, val loss: 0.6296232342720032
Epoch 990, training loss: 421.59149169921875 = 0.5912988185882568 + 50.0 * 8.420003890991211
Epoch 990, val loss: 0.628482460975647
Epoch 1000, training loss: 421.475830078125 = 0.5894994139671326 + 50.0 * 8.417726516723633
Epoch 1000, val loss: 0.6274756193161011
Epoch 1010, training loss: 421.319091796875 = 0.5877357125282288 + 50.0 * 8.414627075195312
Epoch 1010, val loss: 0.6264314651489258
Epoch 1020, training loss: 421.287353515625 = 0.5860810875892639 + 50.0 * 8.41402530670166
Epoch 1020, val loss: 0.6254931092262268
Epoch 1030, training loss: 421.294677734375 = 0.5844941139221191 + 50.0 * 8.414203643798828
Epoch 1030, val loss: 0.6246338486671448
Epoch 1040, training loss: 421.250244140625 = 0.5828906297683716 + 50.0 * 8.413347244262695
Epoch 1040, val loss: 0.6237422227859497
Epoch 1050, training loss: 421.2111511230469 = 0.581348717212677 + 50.0 * 8.412595748901367
Epoch 1050, val loss: 0.6229248046875
Epoch 1060, training loss: 421.1473693847656 = 0.5798616409301758 + 50.0 * 8.41135025024414
Epoch 1060, val loss: 0.6221687197685242
Epoch 1070, training loss: 421.1025085449219 = 0.5784382224082947 + 50.0 * 8.410481452941895
Epoch 1070, val loss: 0.6214549541473389
Epoch 1080, training loss: 421.25701904296875 = 0.5770445466041565 + 50.0 * 8.413599014282227
Epoch 1080, val loss: 0.6207819581031799
Epoch 1090, training loss: 421.1373596191406 = 0.5756506323814392 + 50.0 * 8.411233901977539
Epoch 1090, val loss: 0.6200585961341858
Epoch 1100, training loss: 421.0455627441406 = 0.5742958784103394 + 50.0 * 8.409424781799316
Epoch 1100, val loss: 0.6194090843200684
Epoch 1110, training loss: 420.9942626953125 = 0.5729925632476807 + 50.0 * 8.408425331115723
Epoch 1110, val loss: 0.618793249130249
Epoch 1120, training loss: 420.96917724609375 = 0.5717271566390991 + 50.0 * 8.407949447631836
Epoch 1120, val loss: 0.6181682348251343
Epoch 1130, training loss: 420.97698974609375 = 0.5704916715621948 + 50.0 * 8.408129692077637
Epoch 1130, val loss: 0.6176104545593262
Epoch 1140, training loss: 420.9171447753906 = 0.5692640542984009 + 50.0 * 8.406957626342773
Epoch 1140, val loss: 0.6170535087585449
Epoch 1150, training loss: 420.8554992675781 = 0.5680807828903198 + 50.0 * 8.40574836730957
Epoch 1150, val loss: 0.6165367960929871
Epoch 1160, training loss: 420.8196716308594 = 0.566938579082489 + 50.0 * 8.405055046081543
Epoch 1160, val loss: 0.6160637140274048
Epoch 1170, training loss: 420.881103515625 = 0.565824031829834 + 50.0 * 8.406305313110352
Epoch 1170, val loss: 0.6155920624732971
Epoch 1180, training loss: 420.771484375 = 0.5646872520446777 + 50.0 * 8.404135704040527
Epoch 1180, val loss: 0.6150893568992615
Epoch 1190, training loss: 420.74884033203125 = 0.5635780692100525 + 50.0 * 8.403705596923828
Epoch 1190, val loss: 0.614608645439148
Epoch 1200, training loss: 420.73858642578125 = 0.5625116229057312 + 50.0 * 8.403521537780762
Epoch 1200, val loss: 0.6141985654830933
Epoch 1210, training loss: 420.7720031738281 = 0.5614560842514038 + 50.0 * 8.404211044311523
Epoch 1210, val loss: 0.6137380599975586
Epoch 1220, training loss: 420.6789855957031 = 0.5604302287101746 + 50.0 * 8.402371406555176
Epoch 1220, val loss: 0.6133525967597961
Epoch 1230, training loss: 420.6812438964844 = 0.5594028830528259 + 50.0 * 8.402436256408691
Epoch 1230, val loss: 0.612910270690918
Epoch 1240, training loss: 420.6689758300781 = 0.5583898425102234 + 50.0 * 8.402212142944336
Epoch 1240, val loss: 0.6125888228416443
Epoch 1250, training loss: 420.5877685546875 = 0.5573852062225342 + 50.0 * 8.40060806274414
Epoch 1250, val loss: 0.6121324896812439
Epoch 1260, training loss: 420.5389404296875 = 0.5564090609550476 + 50.0 * 8.399650573730469
Epoch 1260, val loss: 0.6117874979972839
Epoch 1270, training loss: 420.5299987792969 = 0.5554661154747009 + 50.0 * 8.399490356445312
Epoch 1270, val loss: 0.6114579439163208
Epoch 1280, training loss: 420.71124267578125 = 0.5545290112495422 + 50.0 * 8.4031343460083
Epoch 1280, val loss: 0.6111283302307129
Epoch 1290, training loss: 420.5810241699219 = 0.5535425543785095 + 50.0 * 8.40054988861084
Epoch 1290, val loss: 0.6106550097465515
Epoch 1300, training loss: 420.4844665527344 = 0.5525821447372437 + 50.0 * 8.398637771606445
Epoch 1300, val loss: 0.6102924942970276
Epoch 1310, training loss: 420.4295349121094 = 0.5516546368598938 + 50.0 * 8.397557258605957
Epoch 1310, val loss: 0.6099717617034912
Epoch 1320, training loss: 420.410400390625 = 0.5507639646530151 + 50.0 * 8.39719295501709
Epoch 1320, val loss: 0.6096238493919373
Epoch 1330, training loss: 420.43389892578125 = 0.5498883724212646 + 50.0 * 8.397680282592773
Epoch 1330, val loss: 0.60933917760849
Epoch 1340, training loss: 420.4233093261719 = 0.5489754676818848 + 50.0 * 8.397486686706543
Epoch 1340, val loss: 0.6089497208595276
Epoch 1350, training loss: 420.40789794921875 = 0.5480622053146362 + 50.0 * 8.397196769714355
Epoch 1350, val loss: 0.6086089015007019
Epoch 1360, training loss: 420.37823486328125 = 0.5471596717834473 + 50.0 * 8.396621704101562
Epoch 1360, val loss: 0.6082710027694702
Epoch 1370, training loss: 420.317138671875 = 0.5462536215782166 + 50.0 * 8.395417213439941
Epoch 1370, val loss: 0.607900857925415
Epoch 1380, training loss: 420.3391418457031 = 0.5453917384147644 + 50.0 * 8.395874977111816
Epoch 1380, val loss: 0.6075712442398071
Epoch 1390, training loss: 420.34185791015625 = 0.5445137619972229 + 50.0 * 8.395946502685547
Epoch 1390, val loss: 0.6072453856468201
Epoch 1400, training loss: 420.26910400390625 = 0.5436389446258545 + 50.0 * 8.394509315490723
Epoch 1400, val loss: 0.6068990230560303
Epoch 1410, training loss: 420.2609558105469 = 0.5427817106246948 + 50.0 * 8.394363403320312
Epoch 1410, val loss: 0.6065390110015869
Epoch 1420, training loss: 420.333740234375 = 0.541935920715332 + 50.0 * 8.395835876464844
Epoch 1420, val loss: 0.6062380075454712
Epoch 1430, training loss: 420.2099304199219 = 0.541063666343689 + 50.0 * 8.393377304077148
Epoch 1430, val loss: 0.6058964133262634
Epoch 1440, training loss: 420.2085266113281 = 0.5402104258537292 + 50.0 * 8.393365859985352
Epoch 1440, val loss: 0.6055976748466492
Epoch 1450, training loss: 420.15032958984375 = 0.5393533706665039 + 50.0 * 8.392219543457031
Epoch 1450, val loss: 0.6052138209342957
Epoch 1460, training loss: 420.17486572265625 = 0.5385034084320068 + 50.0 * 8.39272689819336
Epoch 1460, val loss: 0.6048590540885925
Epoch 1470, training loss: 420.1671447753906 = 0.5376525521278381 + 50.0 * 8.392589569091797
Epoch 1470, val loss: 0.6045566201210022
Epoch 1480, training loss: 420.1253356933594 = 0.5368155241012573 + 50.0 * 8.391770362854004
Epoch 1480, val loss: 0.6043002605438232
Epoch 1490, training loss: 420.1177673339844 = 0.53597092628479 + 50.0 * 8.39163589477539
Epoch 1490, val loss: 0.6039412021636963
Epoch 1500, training loss: 420.1126403808594 = 0.5351178050041199 + 50.0 * 8.391550064086914
Epoch 1500, val loss: 0.6035749912261963
Epoch 1510, training loss: 420.10284423828125 = 0.5342725515365601 + 50.0 * 8.391371726989746
Epoch 1510, val loss: 0.6033095717430115
Epoch 1520, training loss: 420.0149841308594 = 0.5334126949310303 + 50.0 * 8.389631271362305
Epoch 1520, val loss: 0.6029284000396729
Epoch 1530, training loss: 419.9864807128906 = 0.5325729846954346 + 50.0 * 8.389078140258789
Epoch 1530, val loss: 0.6026145219802856
Epoch 1540, training loss: 419.9736328125 = 0.5317522883415222 + 50.0 * 8.388837814331055
Epoch 1540, val loss: 0.6023422479629517
Epoch 1550, training loss: 420.16455078125 = 0.5309122204780579 + 50.0 * 8.392672538757324
Epoch 1550, val loss: 0.602017879486084
Epoch 1560, training loss: 419.9733581542969 = 0.530029833316803 + 50.0 * 8.388866424560547
Epoch 1560, val loss: 0.6016319990158081
Epoch 1570, training loss: 419.9708251953125 = 0.5291725397109985 + 50.0 * 8.388833045959473
Epoch 1570, val loss: 0.6013039350509644
Epoch 1580, training loss: 419.979248046875 = 0.5283039808273315 + 50.0 * 8.389019012451172
Epoch 1580, val loss: 0.6009449362754822
Epoch 1590, training loss: 419.94635009765625 = 0.527443528175354 + 50.0 * 8.388378143310547
Epoch 1590, val loss: 0.6005504727363586
Epoch 1600, training loss: 419.8542785644531 = 0.5265830755233765 + 50.0 * 8.386553764343262
Epoch 1600, val loss: 0.6002489924430847
Epoch 1610, training loss: 419.81903076171875 = 0.5257453918457031 + 50.0 * 8.385865211486816
Epoch 1610, val loss: 0.5999077558517456
Epoch 1620, training loss: 419.9523010253906 = 0.5248947739601135 + 50.0 * 8.388547897338867
Epoch 1620, val loss: 0.5995332598686218
Epoch 1630, training loss: 419.8478088378906 = 0.523996114730835 + 50.0 * 8.386476516723633
Epoch 1630, val loss: 0.5992001295089722
Epoch 1640, training loss: 419.8314514160156 = 0.5230788588523865 + 50.0 * 8.386167526245117
Epoch 1640, val loss: 0.5987684726715088
Epoch 1650, training loss: 419.7446594238281 = 0.5222212672233582 + 50.0 * 8.384449005126953
Epoch 1650, val loss: 0.5984154343605042
Epoch 1660, training loss: 419.77642822265625 = 0.5213707089424133 + 50.0 * 8.385101318359375
Epoch 1660, val loss: 0.5980497598648071
Epoch 1670, training loss: 419.77984619140625 = 0.5204915404319763 + 50.0 * 8.385187149047852
Epoch 1670, val loss: 0.5977135896682739
Epoch 1680, training loss: 419.7816467285156 = 0.5196132063865662 + 50.0 * 8.38524055480957
Epoch 1680, val loss: 0.5974240899085999
Epoch 1690, training loss: 419.7033386230469 = 0.5187104940414429 + 50.0 * 8.383692741394043
Epoch 1690, val loss: 0.596937358379364
Epoch 1700, training loss: 419.8479919433594 = 0.5178290605545044 + 50.0 * 8.386603355407715
Epoch 1700, val loss: 0.5966041684150696
Epoch 1710, training loss: 419.7195739746094 = 0.5169034004211426 + 50.0 * 8.384053230285645
Epoch 1710, val loss: 0.5961220264434814
Epoch 1720, training loss: 419.6353454589844 = 0.5159957408905029 + 50.0 * 8.382387161254883
Epoch 1720, val loss: 0.5958070158958435
Epoch 1730, training loss: 419.6058654785156 = 0.5150989294052124 + 50.0 * 8.381814956665039
Epoch 1730, val loss: 0.5953771471977234
Epoch 1740, training loss: 419.6009521484375 = 0.5142134428024292 + 50.0 * 8.381734848022461
Epoch 1740, val loss: 0.5950132012367249
Epoch 1750, training loss: 419.7689514160156 = 0.5133060812950134 + 50.0 * 8.385112762451172
Epoch 1750, val loss: 0.5945755839347839
Epoch 1760, training loss: 419.623046875 = 0.5123416781425476 + 50.0 * 8.382214546203613
Epoch 1760, val loss: 0.5941341519355774
Epoch 1770, training loss: 419.6182861328125 = 0.5113930106163025 + 50.0 * 8.3821382522583
Epoch 1770, val loss: 0.5936530828475952
Epoch 1780, training loss: 419.58172607421875 = 0.5104398727416992 + 50.0 * 8.381425857543945
Epoch 1780, val loss: 0.5932794213294983
Epoch 1790, training loss: 419.5238037109375 = 0.5094928741455078 + 50.0 * 8.38028621673584
Epoch 1790, val loss: 0.5928615927696228
Epoch 1800, training loss: 419.49859619140625 = 0.5085529088973999 + 50.0 * 8.379800796508789
Epoch 1800, val loss: 0.5924481749534607
Epoch 1810, training loss: 419.5076599121094 = 0.507603645324707 + 50.0 * 8.380001068115234
Epoch 1810, val loss: 0.5919992923736572
Epoch 1820, training loss: 419.6371765136719 = 0.5066197514533997 + 50.0 * 8.382611274719238
Epoch 1820, val loss: 0.5914936065673828
Epoch 1830, training loss: 419.526611328125 = 0.5055995583534241 + 50.0 * 8.380420684814453
Epoch 1830, val loss: 0.5909026861190796
Epoch 1840, training loss: 419.4789733886719 = 0.5045956373214722 + 50.0 * 8.379487037658691
Epoch 1840, val loss: 0.5904402732849121
Epoch 1850, training loss: 419.5357971191406 = 0.5035865306854248 + 50.0 * 8.380643844604492
Epoch 1850, val loss: 0.5899078249931335
Epoch 1860, training loss: 419.4367370605469 = 0.5025891661643982 + 50.0 * 8.378683090209961
Epoch 1860, val loss: 0.5894209146499634
Epoch 1870, training loss: 419.3945007324219 = 0.5015844106674194 + 50.0 * 8.37785816192627
Epoch 1870, val loss: 0.588939368724823
Epoch 1880, training loss: 419.4022521972656 = 0.5005831718444824 + 50.0 * 8.378033638000488
Epoch 1880, val loss: 0.5884119868278503
Epoch 1890, training loss: 419.9248962402344 = 0.4995436668395996 + 50.0 * 8.388506889343262
Epoch 1890, val loss: 0.5877810716629028
Epoch 1900, training loss: 419.4067077636719 = 0.4984307587146759 + 50.0 * 8.378165245056152
Epoch 1900, val loss: 0.5872947573661804
Epoch 1910, training loss: 419.397216796875 = 0.49735358357429504 + 50.0 * 8.377997398376465
Epoch 1910, val loss: 0.586738109588623
Epoch 1920, training loss: 419.3349609375 = 0.4963173270225525 + 50.0 * 8.3767728805542
Epoch 1920, val loss: 0.5861726403236389
Epoch 1930, training loss: 419.3230285644531 = 0.4952862858772278 + 50.0 * 8.376554489135742
Epoch 1930, val loss: 0.5856412053108215
Epoch 1940, training loss: 419.3165283203125 = 0.4942464530467987 + 50.0 * 8.376445770263672
Epoch 1940, val loss: 0.585108757019043
Epoch 1950, training loss: 419.4582824707031 = 0.49318355321884155 + 50.0 * 8.379302024841309
Epoch 1950, val loss: 0.5844684839248657
Epoch 1960, training loss: 419.324462890625 = 0.4920700490474701 + 50.0 * 8.37664794921875
Epoch 1960, val loss: 0.5839737057685852
Epoch 1970, training loss: 419.35107421875 = 0.49094507098197937 + 50.0 * 8.377202987670898
Epoch 1970, val loss: 0.5832788944244385
Epoch 1980, training loss: 419.3983459472656 = 0.4898183345794678 + 50.0 * 8.37817096710205
Epoch 1980, val loss: 0.5827575325965881
Epoch 1990, training loss: 419.2958068847656 = 0.4886828660964966 + 50.0 * 8.376142501831055
Epoch 1990, val loss: 0.5821638703346252
Epoch 2000, training loss: 419.26959228515625 = 0.4875558912754059 + 50.0 * 8.375640869140625
Epoch 2000, val loss: 0.5815689563751221
Epoch 2010, training loss: 419.23931884765625 = 0.4864274859428406 + 50.0 * 8.3750581741333
Epoch 2010, val loss: 0.5809407234191895
Epoch 2020, training loss: 419.23004150390625 = 0.48530253767967224 + 50.0 * 8.374895095825195
Epoch 2020, val loss: 0.5803525447845459
Epoch 2030, training loss: 419.24554443359375 = 0.48416832089424133 + 50.0 * 8.375227928161621
Epoch 2030, val loss: 0.579753577709198
Epoch 2040, training loss: 419.417724609375 = 0.48300665616989136 + 50.0 * 8.378694534301758
Epoch 2040, val loss: 0.5791693329811096
Epoch 2050, training loss: 419.4087219238281 = 0.4817952513694763 + 50.0 * 8.378539085388184
Epoch 2050, val loss: 0.5783995985984802
Epoch 2060, training loss: 419.2516174316406 = 0.4805908501148224 + 50.0 * 8.375420570373535
Epoch 2060, val loss: 0.5777866840362549
Epoch 2070, training loss: 419.2032775878906 = 0.47940942645072937 + 50.0 * 8.37447738647461
Epoch 2070, val loss: 0.5771133303642273
Epoch 2080, training loss: 419.2701110839844 = 0.47823193669319153 + 50.0 * 8.375837326049805
Epoch 2080, val loss: 0.5765359997749329
Epoch 2090, training loss: 419.1802062988281 = 0.47701480984687805 + 50.0 * 8.374063491821289
Epoch 2090, val loss: 0.5757966041564941
Epoch 2100, training loss: 419.17529296875 = 0.47579845786094666 + 50.0 * 8.373990058898926
Epoch 2100, val loss: 0.5750880241394043
Epoch 2110, training loss: 419.13824462890625 = 0.47459736466407776 + 50.0 * 8.373272895812988
Epoch 2110, val loss: 0.5744612812995911
Epoch 2120, training loss: 419.162353515625 = 0.4733894467353821 + 50.0 * 8.373779296875
Epoch 2120, val loss: 0.5738280415534973
Epoch 2130, training loss: 419.33172607421875 = 0.47215110063552856 + 50.0 * 8.377191543579102
Epoch 2130, val loss: 0.57310950756073
Epoch 2140, training loss: 419.2393493652344 = 0.4708765149116516 + 50.0 * 8.37536907196045
Epoch 2140, val loss: 0.5723164081573486
Epoch 2150, training loss: 419.24224853515625 = 0.4696264863014221 + 50.0 * 8.375452041625977
Epoch 2150, val loss: 0.57174152135849
Epoch 2160, training loss: 419.11126708984375 = 0.46834585070610046 + 50.0 * 8.372858047485352
Epoch 2160, val loss: 0.57094806432724
Epoch 2170, training loss: 419.09326171875 = 0.46709686517715454 + 50.0 * 8.372523307800293
Epoch 2170, val loss: 0.5703286528587341
Epoch 2180, training loss: 419.0978698730469 = 0.46585288643836975 + 50.0 * 8.372640609741211
Epoch 2180, val loss: 0.5696249008178711
Epoch 2190, training loss: 419.2550354003906 = 0.4645865559577942 + 50.0 * 8.375808715820312
Epoch 2190, val loss: 0.5688248872756958
Epoch 2200, training loss: 419.1298522949219 = 0.4632801413536072 + 50.0 * 8.373331069946289
Epoch 2200, val loss: 0.5682356953620911
Epoch 2210, training loss: 419.1054992675781 = 0.4619843661785126 + 50.0 * 8.372870445251465
Epoch 2210, val loss: 0.5675672888755798
Epoch 2220, training loss: 419.0953063964844 = 0.4606972336769104 + 50.0 * 8.372692108154297
Epoch 2220, val loss: 0.566925048828125
Epoch 2230, training loss: 419.08837890625 = 0.4593998193740845 + 50.0 * 8.372579574584961
Epoch 2230, val loss: 0.5662370920181274
Epoch 2240, training loss: 419.0490417480469 = 0.4580984115600586 + 50.0 * 8.371818542480469
Epoch 2240, val loss: 0.565542995929718
Epoch 2250, training loss: 419.0692138671875 = 0.4567955732345581 + 50.0 * 8.372248649597168
Epoch 2250, val loss: 0.5648587942123413
Epoch 2260, training loss: 419.0645751953125 = 0.4554840624332428 + 50.0 * 8.37218189239502
Epoch 2260, val loss: 0.5642023086547852
Epoch 2270, training loss: 419.151611328125 = 0.4541560411453247 + 50.0 * 8.37394905090332
Epoch 2270, val loss: 0.5634756684303284
Epoch 2280, training loss: 419.0419006347656 = 0.4527955651283264 + 50.0 * 8.371782302856445
Epoch 2280, val loss: 0.5626818537712097
Epoch 2290, training loss: 418.99200439453125 = 0.4514703154563904 + 50.0 * 8.370810508728027
Epoch 2290, val loss: 0.5620788931846619
Epoch 2300, training loss: 419.0088195800781 = 0.45013901591300964 + 50.0 * 8.371173858642578
Epoch 2300, val loss: 0.5613808631896973
Epoch 2310, training loss: 419.14642333984375 = 0.44879406690597534 + 50.0 * 8.373952865600586
Epoch 2310, val loss: 0.5606905817985535
Epoch 2320, training loss: 419.0584716796875 = 0.447402685880661 + 50.0 * 8.372221946716309
Epoch 2320, val loss: 0.5598764419555664
Epoch 2330, training loss: 418.9956970214844 = 0.44602933526039124 + 50.0 * 8.370993614196777
Epoch 2330, val loss: 0.5591773986816406
Epoch 2340, training loss: 418.9513854980469 = 0.4446600675582886 + 50.0 * 8.370134353637695
Epoch 2340, val loss: 0.5584508180618286
Epoch 2350, training loss: 418.927978515625 = 0.443302184343338 + 50.0 * 8.369693756103516
Epoch 2350, val loss: 0.5577389597892761
Epoch 2360, training loss: 419.0248107910156 = 0.4419417381286621 + 50.0 * 8.371657371520996
Epoch 2360, val loss: 0.5570383667945862
Epoch 2370, training loss: 419.0223693847656 = 0.4405383765697479 + 50.0 * 8.371636390686035
Epoch 2370, val loss: 0.5563125610351562
Epoch 2380, training loss: 418.9199523925781 = 0.439115434885025 + 50.0 * 8.369616508483887
Epoch 2380, val loss: 0.5555106997489929
Epoch 2390, training loss: 418.8960266113281 = 0.43771880865097046 + 50.0 * 8.369166374206543
Epoch 2390, val loss: 0.5549191832542419
Epoch 2400, training loss: 418.8775329589844 = 0.4363452196121216 + 50.0 * 8.368824005126953
Epoch 2400, val loss: 0.5542431473731995
Epoch 2410, training loss: 418.8846130371094 = 0.43497809767723083 + 50.0 * 8.368992805480957
Epoch 2410, val loss: 0.5536529421806335
Epoch 2420, training loss: 419.0321044921875 = 0.433601051568985 + 50.0 * 8.371970176696777
Epoch 2420, val loss: 0.5531069040298462
Epoch 2430, training loss: 418.87945556640625 = 0.4321710765361786 + 50.0 * 8.368946075439453
Epoch 2430, val loss: 0.5521103143692017
Epoch 2440, training loss: 418.935302734375 = 0.4307727515697479 + 50.0 * 8.37009048461914
Epoch 2440, val loss: 0.5515881180763245
Epoch 2450, training loss: 418.8739929199219 = 0.4293583333492279 + 50.0 * 8.368892669677734
Epoch 2450, val loss: 0.5508044958114624
Epoch 2460, training loss: 418.8581848144531 = 0.42795804142951965 + 50.0 * 8.36860466003418
Epoch 2460, val loss: 0.5502308011054993
Epoch 2470, training loss: 418.8253479003906 = 0.4265575110912323 + 50.0 * 8.367976188659668
Epoch 2470, val loss: 0.5495542287826538
Epoch 2480, training loss: 418.83428955078125 = 0.42516347765922546 + 50.0 * 8.368182182312012
Epoch 2480, val loss: 0.5488660335540771
Epoch 2490, training loss: 418.9896240234375 = 0.4237532615661621 + 50.0 * 8.371316909790039
Epoch 2490, val loss: 0.5481938123703003
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7732115677321156
0.8158371368543071
=== training gcn model ===
Epoch 0, training loss: 530.1847534179688 = 1.0711318254470825 + 50.0 * 10.582273483276367
Epoch 0, val loss: 1.0705915689468384
Epoch 10, training loss: 530.164306640625 = 1.0686228275299072 + 50.0 * 10.581913948059082
Epoch 10, val loss: 1.0681345462799072
Epoch 20, training loss: 530.0865478515625 = 1.0659983158111572 + 50.0 * 10.580411911010742
Epoch 20, val loss: 1.0655921697616577
Epoch 30, training loss: 529.758544921875 = 1.0633490085601807 + 50.0 * 10.573904037475586
Epoch 30, val loss: 1.0630335807800293
Epoch 40, training loss: 528.4502563476562 = 1.0606051683425903 + 50.0 * 10.547792434692383
Epoch 40, val loss: 1.0603750944137573
Epoch 50, training loss: 524.2930908203125 = 1.0577558279037476 + 50.0 * 10.464707374572754
Epoch 50, val loss: 1.0576391220092773
Epoch 60, training loss: 513.2445678710938 = 1.0549894571304321 + 50.0 * 10.243791580200195
Epoch 60, val loss: 1.0550121068954468
Epoch 70, training loss: 491.3607482910156 = 1.0516637563705444 + 50.0 * 9.806181907653809
Epoch 70, val loss: 1.0518070459365845
Epoch 80, training loss: 477.40814208984375 = 1.0476418733596802 + 50.0 * 9.527210235595703
Epoch 80, val loss: 1.0479727983474731
Epoch 90, training loss: 469.91522216796875 = 1.0433475971221924 + 50.0 * 9.377437591552734
Epoch 90, val loss: 1.0437088012695312
Epoch 100, training loss: 465.44281005859375 = 1.0385935306549072 + 50.0 * 9.288084030151367
Epoch 100, val loss: 1.0389496088027954
Epoch 110, training loss: 463.270751953125 = 1.0335438251495361 + 50.0 * 9.244744300842285
Epoch 110, val loss: 1.0339720249176025
Epoch 120, training loss: 461.9049072265625 = 1.0288673639297485 + 50.0 * 9.217520713806152
Epoch 120, val loss: 1.0294361114501953
Epoch 130, training loss: 459.9969787597656 = 1.0247541666030884 + 50.0 * 9.179444313049316
Epoch 130, val loss: 1.0254474878311157
Epoch 140, training loss: 456.955078125 = 1.0211155414581299 + 50.0 * 9.11867904663086
Epoch 140, val loss: 1.0219485759735107
Epoch 150, training loss: 452.90484619140625 = 1.0184963941574097 + 50.0 * 9.037727355957031
Epoch 150, val loss: 1.0194332599639893
Epoch 160, training loss: 449.7970275878906 = 1.01625394821167 + 50.0 * 8.975615501403809
Epoch 160, val loss: 1.0171195268630981
Epoch 170, training loss: 447.5171203613281 = 1.0136594772338867 + 50.0 * 8.930068969726562
Epoch 170, val loss: 1.0144248008728027
Epoch 180, training loss: 444.9765930175781 = 1.0109459161758423 + 50.0 * 8.879312515258789
Epoch 180, val loss: 1.0116780996322632
Epoch 190, training loss: 443.43212890625 = 1.0078521966934204 + 50.0 * 8.848485946655273
Epoch 190, val loss: 1.0085303783416748
Epoch 200, training loss: 442.09027099609375 = 1.0039736032485962 + 50.0 * 8.821725845336914
Epoch 200, val loss: 1.0046793222427368
Epoch 210, training loss: 440.6636657714844 = 0.9998800158500671 + 50.0 * 8.793275833129883
Epoch 210, val loss: 1.0006433725357056
Epoch 220, training loss: 439.3262023925781 = 0.995952308177948 + 50.0 * 8.766605377197266
Epoch 220, val loss: 0.9967361092567444
Epoch 230, training loss: 438.0421447753906 = 0.9916225075721741 + 50.0 * 8.741010665893555
Epoch 230, val loss: 0.9925016164779663
Epoch 240, training loss: 437.05181884765625 = 0.9867338538169861 + 50.0 * 8.721302032470703
Epoch 240, val loss: 0.9876986742019653
Epoch 250, training loss: 436.2112731933594 = 0.981451153755188 + 50.0 * 8.704596519470215
Epoch 250, val loss: 0.9824612736701965
Epoch 260, training loss: 435.5297546386719 = 0.9757161736488342 + 50.0 * 8.691081047058105
Epoch 260, val loss: 0.9768590927124023
Epoch 270, training loss: 434.8694152832031 = 0.969623863697052 + 50.0 * 8.677995681762695
Epoch 270, val loss: 0.9708361029624939
Epoch 280, training loss: 434.171630859375 = 0.9633232355117798 + 50.0 * 8.664166450500488
Epoch 280, val loss: 0.9646454453468323
Epoch 290, training loss: 433.5353088378906 = 0.9568568468093872 + 50.0 * 8.651569366455078
Epoch 290, val loss: 0.9581975936889648
Epoch 300, training loss: 432.72869873046875 = 0.9501197338104248 + 50.0 * 8.635571479797363
Epoch 300, val loss: 0.9516041874885559
Epoch 310, training loss: 432.00457763671875 = 0.9432600736618042 + 50.0 * 8.62122631072998
Epoch 310, val loss: 0.9448444247245789
Epoch 320, training loss: 431.3594055175781 = 0.9361932277679443 + 50.0 * 8.608464241027832
Epoch 320, val loss: 0.9378839135169983
Epoch 330, training loss: 430.78521728515625 = 0.9287689924240112 + 50.0 * 8.597128868103027
Epoch 330, val loss: 0.9305344820022583
Epoch 340, training loss: 430.2434997558594 = 0.9210376739501953 + 50.0 * 8.586448669433594
Epoch 340, val loss: 0.9229029417037964
Epoch 350, training loss: 429.8249206542969 = 0.9131615161895752 + 50.0 * 8.578235626220703
Epoch 350, val loss: 0.9150707721710205
Epoch 360, training loss: 429.3708190917969 = 0.9050472378730774 + 50.0 * 8.569314956665039
Epoch 360, val loss: 0.9070653319358826
Epoch 370, training loss: 428.9961853027344 = 0.8968684077262878 + 50.0 * 8.561985969543457
Epoch 370, val loss: 0.8989636898040771
Epoch 380, training loss: 428.6683044433594 = 0.8886623978614807 + 50.0 * 8.55559253692627
Epoch 380, val loss: 0.8908445239067078
Epoch 390, training loss: 428.3650817871094 = 0.8804231882095337 + 50.0 * 8.54969310760498
Epoch 390, val loss: 0.8826091289520264
Epoch 400, training loss: 428.078857421875 = 0.8721672892570496 + 50.0 * 8.544134140014648
Epoch 400, val loss: 0.8744353652000427
Epoch 410, training loss: 427.7956848144531 = 0.8639794588088989 + 50.0 * 8.538634300231934
Epoch 410, val loss: 0.8663206696510315
Epoch 420, training loss: 427.5228271484375 = 0.8558541536331177 + 50.0 * 8.533339500427246
Epoch 420, val loss: 0.858271598815918
Epoch 430, training loss: 427.3581848144531 = 0.8477462530136108 + 50.0 * 8.530208587646484
Epoch 430, val loss: 0.8502129912376404
Epoch 440, training loss: 427.0556335449219 = 0.8396551609039307 + 50.0 * 8.524319648742676
Epoch 440, val loss: 0.842150866985321
Epoch 450, training loss: 426.8642272949219 = 0.8316471576690674 + 50.0 * 8.520651817321777
Epoch 450, val loss: 0.8342157602310181
Epoch 460, training loss: 426.6520080566406 = 0.8236965537071228 + 50.0 * 8.516566276550293
Epoch 460, val loss: 0.8263399600982666
Epoch 470, training loss: 426.4968566894531 = 0.8158107995986938 + 50.0 * 8.51362133026123
Epoch 470, val loss: 0.8185193538665771
Epoch 480, training loss: 426.3960876464844 = 0.8079385161399841 + 50.0 * 8.511762619018555
Epoch 480, val loss: 0.8106931447982788
Epoch 490, training loss: 426.2104797363281 = 0.800106942653656 + 50.0 * 8.508207321166992
Epoch 490, val loss: 0.8029454350471497
Epoch 500, training loss: 426.0596618652344 = 0.7923702001571655 + 50.0 * 8.505346298217773
Epoch 500, val loss: 0.795306921005249
Epoch 510, training loss: 425.92425537109375 = 0.7847225069999695 + 50.0 * 8.502790451049805
Epoch 510, val loss: 0.7877742648124695
Epoch 520, training loss: 425.83367919921875 = 0.7771440148353577 + 50.0 * 8.501131057739258
Epoch 520, val loss: 0.7803319692611694
Epoch 530, training loss: 425.69744873046875 = 0.7696673274040222 + 50.0 * 8.498556137084961
Epoch 530, val loss: 0.7729976773262024
Epoch 540, training loss: 425.5548095703125 = 0.762333333492279 + 50.0 * 8.495849609375
Epoch 540, val loss: 0.7658491134643555
Epoch 550, training loss: 425.4364318847656 = 0.7551499605178833 + 50.0 * 8.49362564086914
Epoch 550, val loss: 0.758849561214447
Epoch 560, training loss: 425.6671447753906 = 0.7480834722518921 + 50.0 * 8.498381614685059
Epoch 560, val loss: 0.7519592642784119
Epoch 570, training loss: 425.2693786621094 = 0.7410150170326233 + 50.0 * 8.490567207336426
Epoch 570, val loss: 0.7451834678649902
Epoch 580, training loss: 425.146240234375 = 0.7341829538345337 + 50.0 * 8.488241195678711
Epoch 580, val loss: 0.7386292815208435
Epoch 590, training loss: 425.0009765625 = 0.72756028175354 + 50.0 * 8.485467910766602
Epoch 590, val loss: 0.7322940230369568
Epoch 600, training loss: 424.90374755859375 = 0.7211096882820129 + 50.0 * 8.48365306854248
Epoch 600, val loss: 0.7261825799942017
Epoch 610, training loss: 424.7975158691406 = 0.7148348689079285 + 50.0 * 8.481653213500977
Epoch 610, val loss: 0.7202589511871338
Epoch 620, training loss: 424.7028503417969 = 0.7087370157241821 + 50.0 * 8.47988224029541
Epoch 620, val loss: 0.7145140171051025
Epoch 630, training loss: 424.73907470703125 = 0.7027644515037537 + 50.0 * 8.48072624206543
Epoch 630, val loss: 0.7089861631393433
Epoch 640, training loss: 424.5207214355469 = 0.6968977451324463 + 50.0 * 8.476476669311523
Epoch 640, val loss: 0.7035942077636719
Epoch 650, training loss: 424.40966796875 = 0.6913172006607056 + 50.0 * 8.474367141723633
Epoch 650, val loss: 0.6984872817993164
Epoch 660, training loss: 424.3062438964844 = 0.6859859228134155 + 50.0 * 8.472405433654785
Epoch 660, val loss: 0.6936159133911133
Epoch 670, training loss: 424.1983642578125 = 0.6808615326881409 + 50.0 * 8.47035026550293
Epoch 670, val loss: 0.6889780163764954
Epoch 680, training loss: 424.1703796386719 = 0.6759274005889893 + 50.0 * 8.469888687133789
Epoch 680, val loss: 0.6845352649688721
Epoch 690, training loss: 424.10272216796875 = 0.6710897088050842 + 50.0 * 8.468632698059082
Epoch 690, val loss: 0.6802470684051514
Epoch 700, training loss: 423.89788818359375 = 0.6664484739303589 + 50.0 * 8.464629173278809
Epoch 700, val loss: 0.676135778427124
Epoch 710, training loss: 423.8144836425781 = 0.66203373670578 + 50.0 * 8.463048934936523
Epoch 710, val loss: 0.6722570061683655
Epoch 720, training loss: 423.737060546875 = 0.6578010320663452 + 50.0 * 8.46158504486084
Epoch 720, val loss: 0.6686224937438965
Epoch 730, training loss: 423.7039794921875 = 0.6536905169487 + 50.0 * 8.461006164550781
Epoch 730, val loss: 0.6650224924087524
Epoch 740, training loss: 423.51568603515625 = 0.6497249007225037 + 50.0 * 8.457319259643555
Epoch 740, val loss: 0.6616950035095215
Epoch 750, training loss: 423.4132080078125 = 0.6459399461746216 + 50.0 * 8.455345153808594
Epoch 750, val loss: 0.6585103273391724
Epoch 760, training loss: 423.3362121582031 = 0.6423056125640869 + 50.0 * 8.453878402709961
Epoch 760, val loss: 0.6554826498031616
Epoch 770, training loss: 423.4169616699219 = 0.6387872695922852 + 50.0 * 8.45556354522705
Epoch 770, val loss: 0.6525323390960693
Epoch 780, training loss: 423.1669006347656 = 0.6353338956832886 + 50.0 * 8.450631141662598
Epoch 780, val loss: 0.6498015522956848
Epoch 790, training loss: 423.1122131347656 = 0.6320512294769287 + 50.0 * 8.449603080749512
Epoch 790, val loss: 0.6471848487854004
Epoch 800, training loss: 423.0190734863281 = 0.6289028525352478 + 50.0 * 8.447803497314453
Epoch 800, val loss: 0.6446490287780762
Epoch 810, training loss: 422.9754638671875 = 0.62586510181427 + 50.0 * 8.446991920471191
Epoch 810, val loss: 0.6423026919364929
Epoch 820, training loss: 422.94049072265625 = 0.6228797435760498 + 50.0 * 8.446352005004883
Epoch 820, val loss: 0.6399293541908264
Epoch 830, training loss: 422.8973693847656 = 0.6199765801429749 + 50.0 * 8.445548057556152
Epoch 830, val loss: 0.6377307772636414
Epoch 840, training loss: 422.7655334472656 = 0.6172291040420532 + 50.0 * 8.44296646118164
Epoch 840, val loss: 0.6356079578399658
Epoch 850, training loss: 422.7230529785156 = 0.6146024465560913 + 50.0 * 8.442169189453125
Epoch 850, val loss: 0.6336519718170166
Epoch 860, training loss: 422.6812744140625 = 0.6120650172233582 + 50.0 * 8.441384315490723
Epoch 860, val loss: 0.6318052411079407
Epoch 870, training loss: 422.6360168457031 = 0.6095820665359497 + 50.0 * 8.440528869628906
Epoch 870, val loss: 0.6299276947975159
Epoch 880, training loss: 422.59765625 = 0.6071661710739136 + 50.0 * 8.439809799194336
Epoch 880, val loss: 0.6282323002815247
Epoch 890, training loss: 422.5252685546875 = 0.6048558950424194 + 50.0 * 8.438407897949219
Epoch 890, val loss: 0.6265294551849365
Epoch 900, training loss: 422.4853515625 = 0.6026191115379333 + 50.0 * 8.437654495239258
Epoch 900, val loss: 0.6249669194221497
Epoch 910, training loss: 422.520263671875 = 0.6004390716552734 + 50.0 * 8.438396453857422
Epoch 910, val loss: 0.6234920620918274
Epoch 920, training loss: 422.5671691894531 = 0.5982666611671448 + 50.0 * 8.439377784729004
Epoch 920, val loss: 0.6219119429588318
Epoch 930, training loss: 422.3744812011719 = 0.5961393713951111 + 50.0 * 8.435566902160645
Epoch 930, val loss: 0.6204253435134888
Epoch 940, training loss: 422.33990478515625 = 0.5941260457038879 + 50.0 * 8.434915542602539
Epoch 940, val loss: 0.6190447807312012
Epoch 950, training loss: 422.2987365722656 = 0.592194676399231 + 50.0 * 8.434130668640137
Epoch 950, val loss: 0.6177659630775452
Epoch 960, training loss: 422.26397705078125 = 0.5903221368789673 + 50.0 * 8.433472633361816
Epoch 960, val loss: 0.6165074706077576
Epoch 970, training loss: 422.41729736328125 = 0.5884886980056763 + 50.0 * 8.436575889587402
Epoch 970, val loss: 0.6151975989341736
Epoch 980, training loss: 422.3509216308594 = 0.5866152048110962 + 50.0 * 8.435286521911621
Epoch 980, val loss: 0.6141010522842407
Epoch 990, training loss: 422.1775207519531 = 0.58481365442276 + 50.0 * 8.431854248046875
Epoch 990, val loss: 0.6128484606742859
Epoch 1000, training loss: 422.1542663574219 = 0.5831019282341003 + 50.0 * 8.43142318725586
Epoch 1000, val loss: 0.6116718649864197
Epoch 1010, training loss: 422.1066589355469 = 0.5814456343650818 + 50.0 * 8.430503845214844
Epoch 1010, val loss: 0.610654890537262
Epoch 1020, training loss: 422.07208251953125 = 0.5798326134681702 + 50.0 * 8.429844856262207
Epoch 1020, val loss: 0.6096041202545166
Epoch 1030, training loss: 422.04632568359375 = 0.5782478451728821 + 50.0 * 8.429361343383789
Epoch 1030, val loss: 0.6085938811302185
Epoch 1040, training loss: 422.337158203125 = 0.5766696333885193 + 50.0 * 8.435210227966309
Epoch 1040, val loss: 0.6075839400291443
Epoch 1050, training loss: 422.11248779296875 = 0.5750474333763123 + 50.0 * 8.43074893951416
Epoch 1050, val loss: 0.6065579652786255
Epoch 1060, training loss: 421.9905700683594 = 0.5735273957252502 + 50.0 * 8.428340911865234
Epoch 1060, val loss: 0.6055858731269836
Epoch 1070, training loss: 421.9344787597656 = 0.5720691680908203 + 50.0 * 8.427248001098633
Epoch 1070, val loss: 0.6046645045280457
Epoch 1080, training loss: 421.9212341308594 = 0.5706455111503601 + 50.0 * 8.427011489868164
Epoch 1080, val loss: 0.6037628054618835
Epoch 1090, training loss: 421.9490661621094 = 0.5692188143730164 + 50.0 * 8.427597045898438
Epoch 1090, val loss: 0.6028717756271362
Epoch 1100, training loss: 421.8459777832031 = 0.5677889585494995 + 50.0 * 8.42556381225586
Epoch 1100, val loss: 0.6020607948303223
Epoch 1110, training loss: 421.804443359375 = 0.5664162039756775 + 50.0 * 8.424760818481445
Epoch 1110, val loss: 0.601186215877533
Epoch 1120, training loss: 421.7725830078125 = 0.5650839805603027 + 50.0 * 8.424149513244629
Epoch 1120, val loss: 0.6003546118736267
Epoch 1130, training loss: 421.98370361328125 = 0.5637653470039368 + 50.0 * 8.428399085998535
Epoch 1130, val loss: 0.5994892716407776
Epoch 1140, training loss: 421.9251708984375 = 0.5623807907104492 + 50.0 * 8.427255630493164
Epoch 1140, val loss: 0.5987575650215149
Epoch 1150, training loss: 421.6783752441406 = 0.561045229434967 + 50.0 * 8.422347068786621
Epoch 1150, val loss: 0.5978953242301941
Epoch 1160, training loss: 421.6632080078125 = 0.5597862601280212 + 50.0 * 8.42206859588623
Epoch 1160, val loss: 0.597133994102478
Epoch 1170, training loss: 421.6126403808594 = 0.5585707426071167 + 50.0 * 8.42108154296875
Epoch 1170, val loss: 0.5964155197143555
Epoch 1180, training loss: 421.58026123046875 = 0.5573702454566956 + 50.0 * 8.42045783996582
Epoch 1180, val loss: 0.5957250595092773
Epoch 1190, training loss: 421.5692138671875 = 0.5561752915382385 + 50.0 * 8.420260429382324
Epoch 1190, val loss: 0.5950750112533569
Epoch 1200, training loss: 421.5752868652344 = 0.5549519658088684 + 50.0 * 8.420406341552734
Epoch 1200, val loss: 0.5942621231079102
Epoch 1210, training loss: 421.5157165527344 = 0.5537257790565491 + 50.0 * 8.41923999786377
Epoch 1210, val loss: 0.5935606360435486
Epoch 1220, training loss: 421.5079650878906 = 0.5525562763214111 + 50.0 * 8.419108390808105
Epoch 1220, val loss: 0.5927176475524902
Epoch 1230, training loss: 421.4374694824219 = 0.5513880848884583 + 50.0 * 8.41772174835205
Epoch 1230, val loss: 0.5920094847679138
Epoch 1240, training loss: 421.3999328613281 = 0.5502504706382751 + 50.0 * 8.416993141174316
Epoch 1240, val loss: 0.5914021730422974
Epoch 1250, training loss: 421.3765563964844 = 0.5491241812705994 + 50.0 * 8.416548728942871
Epoch 1250, val loss: 0.5907405614852905
Epoch 1260, training loss: 421.38922119140625 = 0.5479953289031982 + 50.0 * 8.416824340820312
Epoch 1260, val loss: 0.5900764465332031
Epoch 1270, training loss: 421.4137268066406 = 0.5468502044677734 + 50.0 * 8.417337417602539
Epoch 1270, val loss: 0.589425265789032
Epoch 1280, training loss: 421.3906555175781 = 0.545699954032898 + 50.0 * 8.416899681091309
Epoch 1280, val loss: 0.5885910987854004
Epoch 1290, training loss: 421.288818359375 = 0.5445718169212341 + 50.0 * 8.414884567260742
Epoch 1290, val loss: 0.5880029201507568
Epoch 1300, training loss: 421.2231140136719 = 0.5434694886207581 + 50.0 * 8.413593292236328
Epoch 1300, val loss: 0.5872958898544312
Epoch 1310, training loss: 421.18975830078125 = 0.5423834919929504 + 50.0 * 8.412947654724121
Epoch 1310, val loss: 0.5866975784301758
Epoch 1320, training loss: 421.1730041503906 = 0.5413062572479248 + 50.0 * 8.412633895874023
Epoch 1320, val loss: 0.5860553979873657
Epoch 1330, training loss: 421.41058349609375 = 0.5402210354804993 + 50.0 * 8.417407035827637
Epoch 1330, val loss: 0.5854340195655823
Epoch 1340, training loss: 421.1624755859375 = 0.5390563607215881 + 50.0 * 8.412467956542969
Epoch 1340, val loss: 0.5846856832504272
Epoch 1350, training loss: 421.13128662109375 = 0.5379456877708435 + 50.0 * 8.411867141723633
Epoch 1350, val loss: 0.5839753150939941
Epoch 1360, training loss: 421.0682678222656 = 0.5368847250938416 + 50.0 * 8.410627365112305
Epoch 1360, val loss: 0.5834055542945862
Epoch 1370, training loss: 421.05120849609375 = 0.5358514785766602 + 50.0 * 8.410306930541992
Epoch 1370, val loss: 0.5827749371528625
Epoch 1380, training loss: 421.04473876953125 = 0.5348223447799683 + 50.0 * 8.410198211669922
Epoch 1380, val loss: 0.5822062492370605
Epoch 1390, training loss: 421.3317565917969 = 0.5337711572647095 + 50.0 * 8.415959358215332
Epoch 1390, val loss: 0.5816201567649841
Epoch 1400, training loss: 421.08148193359375 = 0.5326602458953857 + 50.0 * 8.41097640991211
Epoch 1400, val loss: 0.5808383226394653
Epoch 1410, training loss: 420.9794006347656 = 0.5316093564033508 + 50.0 * 8.408955574035645
Epoch 1410, val loss: 0.5802727341651917
Epoch 1420, training loss: 420.9462585449219 = 0.5305883288383484 + 50.0 * 8.408313751220703
Epoch 1420, val loss: 0.5796789526939392
Epoch 1430, training loss: 420.92681884765625 = 0.5295749306678772 + 50.0 * 8.407944679260254
Epoch 1430, val loss: 0.5791015625
Epoch 1440, training loss: 420.9273681640625 = 0.5285618305206299 + 50.0 * 8.407976150512695
Epoch 1440, val loss: 0.5785675644874573
Epoch 1450, training loss: 421.0777893066406 = 0.5275221467018127 + 50.0 * 8.411005020141602
Epoch 1450, val loss: 0.5780236124992371
Epoch 1460, training loss: 420.8994140625 = 0.5264686346054077 + 50.0 * 8.407459259033203
Epoch 1460, val loss: 0.5773228406906128
Epoch 1470, training loss: 420.9075622558594 = 0.5254462361335754 + 50.0 * 8.407642364501953
Epoch 1470, val loss: 0.5765906572341919
Epoch 1480, training loss: 420.84637451171875 = 0.5244357585906982 + 50.0 * 8.406438827514648
Epoch 1480, val loss: 0.5760390162467957
Epoch 1490, training loss: 420.8507080078125 = 0.5234454274177551 + 50.0 * 8.406545639038086
Epoch 1490, val loss: 0.5755096077919006
Epoch 1500, training loss: 420.9566955566406 = 0.5224514007568359 + 50.0 * 8.408684730529785
Epoch 1500, val loss: 0.5749106407165527
Epoch 1510, training loss: 420.80731201171875 = 0.5214185118675232 + 50.0 * 8.405717849731445
Epoch 1510, val loss: 0.5741997361183167
Epoch 1520, training loss: 420.76678466796875 = 0.5204237103462219 + 50.0 * 8.404927253723145
Epoch 1520, val loss: 0.5736348628997803
Epoch 1530, training loss: 420.8153076171875 = 0.5194429159164429 + 50.0 * 8.405917167663574
Epoch 1530, val loss: 0.5730451345443726
Epoch 1540, training loss: 420.76678466796875 = 0.5184311866760254 + 50.0 * 8.404967308044434
Epoch 1540, val loss: 0.5723921656608582
Epoch 1550, training loss: 420.7524719238281 = 0.5174300074577332 + 50.0 * 8.404701232910156
Epoch 1550, val loss: 0.5717983841896057
Epoch 1560, training loss: 420.7384338378906 = 0.5164446830749512 + 50.0 * 8.404439926147461
Epoch 1560, val loss: 0.5712231397628784
Epoch 1570, training loss: 420.7613830566406 = 0.5154533386230469 + 50.0 * 8.404918670654297
Epoch 1570, val loss: 0.5706486105918884
Epoch 1580, training loss: 420.6996154785156 = 0.5144596695899963 + 50.0 * 8.403702735900879
Epoch 1580, val loss: 0.5699052810668945
Epoch 1590, training loss: 420.6469421386719 = 0.513475239276886 + 50.0 * 8.402668952941895
Epoch 1590, val loss: 0.5693562626838684
Epoch 1600, training loss: 420.6581115722656 = 0.5125024914741516 + 50.0 * 8.402912139892578
Epoch 1600, val loss: 0.5687761902809143
Epoch 1610, training loss: 420.6385498046875 = 0.5115254521369934 + 50.0 * 8.40254020690918
Epoch 1610, val loss: 0.5681223273277283
Epoch 1620, training loss: 420.7287292480469 = 0.5105484127998352 + 50.0 * 8.404363632202148
Epoch 1620, val loss: 0.5675304532051086
Epoch 1630, training loss: 420.57763671875 = 0.5095358490943909 + 50.0 * 8.401362419128418
Epoch 1630, val loss: 0.5669429898262024
Epoch 1640, training loss: 420.5386047363281 = 0.5085559487342834 + 50.0 * 8.400601387023926
Epoch 1640, val loss: 0.5663345456123352
Epoch 1650, training loss: 420.5164489746094 = 0.5076033473014832 + 50.0 * 8.400177001953125
Epoch 1650, val loss: 0.5657617449760437
Epoch 1660, training loss: 420.5003356933594 = 0.5066657662391663 + 50.0 * 8.399873733520508
Epoch 1660, val loss: 0.5652158260345459
Epoch 1670, training loss: 420.5481262207031 = 0.5057211518287659 + 50.0 * 8.400848388671875
Epoch 1670, val loss: 0.5645920634269714
Epoch 1680, training loss: 420.491943359375 = 0.504740834236145 + 50.0 * 8.399744033813477
Epoch 1680, val loss: 0.5640435218811035
Epoch 1690, training loss: 420.5422058105469 = 0.5037575364112854 + 50.0 * 8.400769233703613
Epoch 1690, val loss: 0.5634239912033081
Epoch 1700, training loss: 420.4308166503906 = 0.5027715563774109 + 50.0 * 8.398560523986816
Epoch 1700, val loss: 0.5628799796104431
Epoch 1710, training loss: 420.423095703125 = 0.5018064379692078 + 50.0 * 8.398426055908203
Epoch 1710, val loss: 0.5623376369476318
Epoch 1720, training loss: 420.7757263183594 = 0.5008442401885986 + 50.0 * 8.405497550964355
Epoch 1720, val loss: 0.5616085529327393
Epoch 1730, training loss: 420.51177978515625 = 0.49981358647346497 + 50.0 * 8.400238990783691
Epoch 1730, val loss: 0.561211884021759
Epoch 1740, training loss: 420.38763427734375 = 0.4988270103931427 + 50.0 * 8.397775650024414
Epoch 1740, val loss: 0.5605379939079285
Epoch 1750, training loss: 420.3318786621094 = 0.4978811740875244 + 50.0 * 8.396679878234863
Epoch 1750, val loss: 0.5600304007530212
Epoch 1760, training loss: 420.3111877441406 = 0.49694278836250305 + 50.0 * 8.396285057067871
Epoch 1760, val loss: 0.5595129132270813
Epoch 1770, training loss: 420.2955627441406 = 0.49599897861480713 + 50.0 * 8.395991325378418
Epoch 1770, val loss: 0.5589672923088074
Epoch 1780, training loss: 420.31475830078125 = 0.4950557053089142 + 50.0 * 8.396393775939941
Epoch 1780, val loss: 0.5583494901657104
Epoch 1790, training loss: 420.38641357421875 = 0.4940635859966278 + 50.0 * 8.397847175598145
Epoch 1790, val loss: 0.557863175868988
Epoch 1800, training loss: 420.2674865722656 = 0.4930444657802582 + 50.0 * 8.395488739013672
Epoch 1800, val loss: 0.55721116065979
Epoch 1810, training loss: 420.26434326171875 = 0.49206307530403137 + 50.0 * 8.395445823669434
Epoch 1810, val loss: 0.556834876537323
Epoch 1820, training loss: 420.29010009765625 = 0.49110859632492065 + 50.0 * 8.395979881286621
Epoch 1820, val loss: 0.5562395453453064
Epoch 1830, training loss: 420.2771911621094 = 0.49009767174720764 + 50.0 * 8.39574146270752
Epoch 1830, val loss: 0.5557196140289307
Epoch 1840, training loss: 420.1947326660156 = 0.4890901744365692 + 50.0 * 8.394112586975098
Epoch 1840, val loss: 0.5551692247390747
Epoch 1850, training loss: 420.1895751953125 = 0.48811978101730347 + 50.0 * 8.394028663635254
Epoch 1850, val loss: 0.5547048449516296
Epoch 1860, training loss: 420.1650695800781 = 0.4871600866317749 + 50.0 * 8.393558502197266
Epoch 1860, val loss: 0.5541756749153137
Epoch 1870, training loss: 420.1514587402344 = 0.48620280623435974 + 50.0 * 8.393304824829102
Epoch 1870, val loss: 0.553690493106842
Epoch 1880, training loss: 420.285888671875 = 0.48523375391960144 + 50.0 * 8.396013259887695
Epoch 1880, val loss: 0.5531569123268127
Epoch 1890, training loss: 420.28778076171875 = 0.4842129051685333 + 50.0 * 8.396071434020996
Epoch 1890, val loss: 0.5527969598770142
Epoch 1900, training loss: 420.152099609375 = 0.4831808805465698 + 50.0 * 8.393378257751465
Epoch 1900, val loss: 0.5520964860916138
Epoch 1910, training loss: 420.10211181640625 = 0.48218274116516113 + 50.0 * 8.392398834228516
Epoch 1910, val loss: 0.5515976548194885
Epoch 1920, training loss: 420.0752868652344 = 0.48121172189712524 + 50.0 * 8.391881942749023
Epoch 1920, val loss: 0.5511546730995178
Epoch 1930, training loss: 420.0623474121094 = 0.4802454710006714 + 50.0 * 8.391641616821289
Epoch 1930, val loss: 0.5506556630134583
Epoch 1940, training loss: 420.11566162109375 = 0.47927388548851013 + 50.0 * 8.392727851867676
Epoch 1940, val loss: 0.5502120852470398
Epoch 1950, training loss: 420.1640930175781 = 0.4782509505748749 + 50.0 * 8.393716812133789
Epoch 1950, val loss: 0.5496925115585327
Epoch 1960, training loss: 420.0470886230469 = 0.47720104455947876 + 50.0 * 8.391397476196289
Epoch 1960, val loss: 0.5491439700126648
Epoch 1970, training loss: 420.0144958496094 = 0.47620970010757446 + 50.0 * 8.390766143798828
Epoch 1970, val loss: 0.5485939383506775
Epoch 1980, training loss: 419.997314453125 = 0.4752158522605896 + 50.0 * 8.39044189453125
Epoch 1980, val loss: 0.5481550693511963
Epoch 1990, training loss: 419.9832763671875 = 0.4742303490638733 + 50.0 * 8.390180587768555
Epoch 1990, val loss: 0.5476404428482056
Epoch 2000, training loss: 420.0602722167969 = 0.47323963046073914 + 50.0 * 8.391740798950195
Epoch 2000, val loss: 0.5470536947250366
Epoch 2010, training loss: 420.0721740722656 = 0.4721677005290985 + 50.0 * 8.392000198364258
Epoch 2010, val loss: 0.546666145324707
Epoch 2020, training loss: 419.9673156738281 = 0.47111234068870544 + 50.0 * 8.389924049377441
Epoch 2020, val loss: 0.5460276007652283
Epoch 2030, training loss: 419.9507141113281 = 0.4700923562049866 + 50.0 * 8.389612197875977
Epoch 2030, val loss: 0.5457018613815308
Epoch 2040, training loss: 419.922119140625 = 0.469102680683136 + 50.0 * 8.389060020446777
Epoch 2040, val loss: 0.5451288223266602
Epoch 2050, training loss: 419.911376953125 = 0.4681036174297333 + 50.0 * 8.38886547088623
Epoch 2050, val loss: 0.5447316765785217
Epoch 2060, training loss: 419.9897766113281 = 0.4670979082584381 + 50.0 * 8.390453338623047
Epoch 2060, val loss: 0.5442779660224915
Epoch 2070, training loss: 419.892578125 = 0.4660463035106659 + 50.0 * 8.388530731201172
Epoch 2070, val loss: 0.5437796115875244
Epoch 2080, training loss: 419.8736877441406 = 0.46501243114471436 + 50.0 * 8.38817310333252
Epoch 2080, val loss: 0.5432657599449158
Epoch 2090, training loss: 419.8997802734375 = 0.4639875590801239 + 50.0 * 8.388715744018555
Epoch 2090, val loss: 0.5428699851036072
Epoch 2100, training loss: 420.1056823730469 = 0.46293905377388 + 50.0 * 8.392854690551758
Epoch 2100, val loss: 0.5423735976219177
Epoch 2110, training loss: 419.90936279296875 = 0.46184849739074707 + 50.0 * 8.38895034790039
Epoch 2110, val loss: 0.5418158769607544
Epoch 2120, training loss: 419.8417053222656 = 0.4608202874660492 + 50.0 * 8.387618064880371
Epoch 2120, val loss: 0.541363000869751
Epoch 2130, training loss: 419.8173828125 = 0.4597984850406647 + 50.0 * 8.387151718139648
Epoch 2130, val loss: 0.5409351587295532
Epoch 2140, training loss: 419.8012390136719 = 0.45878180861473083 + 50.0 * 8.386849403381348
Epoch 2140, val loss: 0.540530264377594
Epoch 2150, training loss: 419.79736328125 = 0.4577585458755493 + 50.0 * 8.386792182922363
Epoch 2150, val loss: 0.5400566458702087
Epoch 2160, training loss: 420.1047668457031 = 0.45671337842941284 + 50.0 * 8.392960548400879
Epoch 2160, val loss: 0.5396004319190979
Epoch 2170, training loss: 419.8288879394531 = 0.455615758895874 + 50.0 * 8.387465476989746
Epoch 2170, val loss: 0.5391664505004883
Epoch 2180, training loss: 419.7891540527344 = 0.45455825328826904 + 50.0 * 8.38669204711914
Epoch 2180, val loss: 0.5387148261070251
Epoch 2190, training loss: 419.7486267089844 = 0.4535280466079712 + 50.0 * 8.385902404785156
Epoch 2190, val loss: 0.5382837653160095
Epoch 2200, training loss: 419.7377014160156 = 0.45249882340431213 + 50.0 * 8.385704040527344
Epoch 2200, val loss: 0.5378889441490173
Epoch 2210, training loss: 419.78741455078125 = 0.45147114992141724 + 50.0 * 8.38671875
Epoch 2210, val loss: 0.537472128868103
Epoch 2220, training loss: 419.8761901855469 = 0.45038899779319763 + 50.0 * 8.388516426086426
Epoch 2220, val loss: 0.537216305732727
Epoch 2230, training loss: 419.73492431640625 = 0.4492771625518799 + 50.0 * 8.385712623596191
Epoch 2230, val loss: 0.5366455912590027
Epoch 2240, training loss: 419.70257568359375 = 0.4482157528400421 + 50.0 * 8.385087013244629
Epoch 2240, val loss: 0.5362721681594849
Epoch 2250, training loss: 419.6918029785156 = 0.4471744894981384 + 50.0 * 8.384892463684082
Epoch 2250, val loss: 0.5358095169067383
Epoch 2260, training loss: 419.69610595703125 = 0.4461252689361572 + 50.0 * 8.38499927520752
Epoch 2260, val loss: 0.5353757739067078
Epoch 2270, training loss: 419.8761291503906 = 0.44506195187568665 + 50.0 * 8.38862133026123
Epoch 2270, val loss: 0.5348633527755737
Epoch 2280, training loss: 419.6717834472656 = 0.4439430832862854 + 50.0 * 8.384556770324707
Epoch 2280, val loss: 0.5346729755401611
Epoch 2290, training loss: 419.64019775390625 = 0.4428658187389374 + 50.0 * 8.383946418762207
Epoch 2290, val loss: 0.534196138381958
Epoch 2300, training loss: 419.6349182128906 = 0.44179767370224 + 50.0 * 8.383862495422363
Epoch 2300, val loss: 0.5338043570518494
Epoch 2310, training loss: 419.6265563964844 = 0.44073399901390076 + 50.0 * 8.383716583251953
Epoch 2310, val loss: 0.5334688425064087
Epoch 2320, training loss: 419.75872802734375 = 0.4396638870239258 + 50.0 * 8.386381149291992
Epoch 2320, val loss: 0.5331300497055054
Epoch 2330, training loss: 419.6180725097656 = 0.43854305148124695 + 50.0 * 8.383590698242188
Epoch 2330, val loss: 0.5325146913528442
Epoch 2340, training loss: 419.70159912109375 = 0.4374283254146576 + 50.0 * 8.385283470153809
Epoch 2340, val loss: 0.5322514176368713
Epoch 2350, training loss: 419.6134948730469 = 0.4363318979740143 + 50.0 * 8.383543014526367
Epoch 2350, val loss: 0.5317689180374146
Epoch 2360, training loss: 419.57781982421875 = 0.43525025248527527 + 50.0 * 8.382851600646973
Epoch 2360, val loss: 0.5314455628395081
Epoch 2370, training loss: 419.5708923339844 = 0.43418189883232117 + 50.0 * 8.382734298706055
Epoch 2370, val loss: 0.5310721397399902
Epoch 2380, training loss: 419.64306640625 = 0.43310150504112244 + 50.0 * 8.384199142456055
Epoch 2380, val loss: 0.530735433101654
Epoch 2390, training loss: 419.59033203125 = 0.43199071288108826 + 50.0 * 8.383167266845703
Epoch 2390, val loss: 0.5303155183792114
Epoch 2400, training loss: 419.5658874511719 = 0.4308878183364868 + 50.0 * 8.382699966430664
Epoch 2400, val loss: 0.5300058126449585
Epoch 2410, training loss: 419.5196533203125 = 0.42979612946510315 + 50.0 * 8.381796836853027
Epoch 2410, val loss: 0.5296003222465515
Epoch 2420, training loss: 419.69964599609375 = 0.4287276864051819 + 50.0 * 8.385417938232422
Epoch 2420, val loss: 0.5290263891220093
Epoch 2430, training loss: 419.5565185546875 = 0.4275675117969513 + 50.0 * 8.38257884979248
Epoch 2430, val loss: 0.5290848612785339
Epoch 2440, training loss: 419.50555419921875 = 0.4264671802520752 + 50.0 * 8.381582260131836
Epoch 2440, val loss: 0.5284889340400696
Epoch 2450, training loss: 419.47833251953125 = 0.42536669969558716 + 50.0 * 8.381059646606445
Epoch 2450, val loss: 0.5283324718475342
Epoch 2460, training loss: 419.4668273925781 = 0.4242803454399109 + 50.0 * 8.380850791931152
Epoch 2460, val loss: 0.5279107093811035
Epoch 2470, training loss: 419.6683349609375 = 0.4231831431388855 + 50.0 * 8.384902954101562
Epoch 2470, val loss: 0.5276614427566528
Epoch 2480, training loss: 419.7039489746094 = 0.42201298475265503 + 50.0 * 8.385638236999512
Epoch 2480, val loss: 0.527258038520813
Epoch 2490, training loss: 419.5114440917969 = 0.4208415746688843 + 50.0 * 8.38181209564209
Epoch 2490, val loss: 0.5268673896789551
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7787924911212583
0.8151126566688401
The final CL Acc:0.77524, 0.00252, The final GNN Acc:0.81516, 0.00053
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110806])
remove edge: torch.Size([2, 66754])
updated graph: torch.Size([2, 88912])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 530.2431640625 = 1.1290611028671265 + 50.0 * 10.582282066345215
Epoch 0, val loss: 1.1278239488601685
Epoch 10, training loss: 530.2158813476562 = 1.1235848665237427 + 50.0 * 10.581846237182617
Epoch 10, val loss: 1.1223530769348145
Epoch 20, training loss: 530.10595703125 = 1.1175004243850708 + 50.0 * 10.579769134521484
Epoch 20, val loss: 1.1162655353546143
Epoch 30, training loss: 529.6425170898438 = 1.1106343269348145 + 50.0 * 10.570637702941895
Epoch 30, val loss: 1.1094027757644653
Epoch 40, training loss: 527.9732055664062 = 1.102850317955017 + 50.0 * 10.537406921386719
Epoch 40, val loss: 1.1016144752502441
Epoch 50, training loss: 523.6478271484375 = 1.0939618349075317 + 50.0 * 10.451078414916992
Epoch 50, val loss: 1.0927298069000244
Epoch 60, training loss: 514.542724609375 = 1.0847814083099365 + 50.0 * 10.269158363342285
Epoch 60, val loss: 1.0836392641067505
Epoch 70, training loss: 499.1756896972656 = 1.0753142833709717 + 50.0 * 9.962007522583008
Epoch 70, val loss: 1.0741451978683472
Epoch 80, training loss: 485.5451354980469 = 1.0657132863998413 + 50.0 * 9.68958854675293
Epoch 80, val loss: 1.064712643623352
Epoch 90, training loss: 481.79730224609375 = 1.0576478242874146 + 50.0 * 9.614792823791504
Epoch 90, val loss: 1.0568393468856812
Epoch 100, training loss: 479.4422302246094 = 1.0520161390304565 + 50.0 * 9.567804336547852
Epoch 100, val loss: 1.0513508319854736
Epoch 110, training loss: 477.4449768066406 = 1.0483758449554443 + 50.0 * 9.527932167053223
Epoch 110, val loss: 1.0478111505508423
Epoch 120, training loss: 475.4611511230469 = 1.045712947845459 + 50.0 * 9.488308906555176
Epoch 120, val loss: 1.0451489686965942
Epoch 130, training loss: 473.3918762207031 = 1.0431406497955322 + 50.0 * 9.446974754333496
Epoch 130, val loss: 1.0425736904144287
Epoch 140, training loss: 471.093505859375 = 1.0397366285324097 + 50.0 * 9.40107536315918
Epoch 140, val loss: 1.0391864776611328
Epoch 150, training loss: 468.4243469238281 = 1.0353378057479858 + 50.0 * 9.347780227661133
Epoch 150, val loss: 1.0348970890045166
Epoch 160, training loss: 464.98077392578125 = 1.0304275751113892 + 50.0 * 9.279006958007812
Epoch 160, val loss: 1.0301793813705444
Epoch 170, training loss: 460.58184814453125 = 1.0257701873779297 + 50.0 * 9.191122055053711
Epoch 170, val loss: 1.0258233547210693
Epoch 180, training loss: 455.8790588378906 = 1.0224202871322632 + 50.0 * 9.097132682800293
Epoch 180, val loss: 1.0227704048156738
Epoch 190, training loss: 452.1661071777344 = 1.020226240158081 + 50.0 * 9.022917747497559
Epoch 190, val loss: 1.020590901374817
Epoch 200, training loss: 449.7994384765625 = 1.0172476768493652 + 50.0 * 8.9756441116333
Epoch 200, val loss: 1.017390251159668
Epoch 210, training loss: 448.6175842285156 = 1.0125226974487305 + 50.0 * 8.952101707458496
Epoch 210, val loss: 1.0125563144683838
Epoch 220, training loss: 447.5393981933594 = 1.007280707359314 + 50.0 * 8.930642127990723
Epoch 220, val loss: 1.0074782371520996
Epoch 230, training loss: 446.22528076171875 = 1.003089189529419 + 50.0 * 8.904443740844727
Epoch 230, val loss: 1.0035433769226074
Epoch 240, training loss: 444.43829345703125 = 0.9996633529663086 + 50.0 * 8.868772506713867
Epoch 240, val loss: 1.0003342628479004
Epoch 250, training loss: 441.87615966796875 = 0.9970178604125977 + 50.0 * 8.817583084106445
Epoch 250, val loss: 0.9979586005210876
Epoch 260, training loss: 439.3556213378906 = 0.9952924847602844 + 50.0 * 8.767206192016602
Epoch 260, val loss: 0.9963008165359497
Epoch 270, training loss: 437.5721130371094 = 0.9918592572212219 + 50.0 * 8.731605529785156
Epoch 270, val loss: 0.9927242994308472
Epoch 280, training loss: 436.14410400390625 = 0.9864936470985413 + 50.0 * 8.703152656555176
Epoch 280, val loss: 0.9874269962310791
Epoch 290, training loss: 435.0555725097656 = 0.9804574251174927 + 50.0 * 8.681502342224121
Epoch 290, val loss: 0.9816963076591492
Epoch 300, training loss: 433.9588928222656 = 0.974831759929657 + 50.0 * 8.65968132019043
Epoch 300, val loss: 0.976495623588562
Epoch 310, training loss: 433.049560546875 = 0.9696370363235474 + 50.0 * 8.64159870147705
Epoch 310, val loss: 0.9716292023658752
Epoch 320, training loss: 432.36541748046875 = 0.9642176032066345 + 50.0 * 8.628024101257324
Epoch 320, val loss: 0.966421902179718
Epoch 330, training loss: 431.75390625 = 0.9582774043083191 + 50.0 * 8.615912437438965
Epoch 330, val loss: 0.9607946872711182
Epoch 340, training loss: 431.3118591308594 = 0.951804518699646 + 50.0 * 8.607200622558594
Epoch 340, val loss: 0.9545441269874573
Epoch 350, training loss: 430.97576904296875 = 0.9448056817054749 + 50.0 * 8.600619316101074
Epoch 350, val loss: 0.9478071331977844
Epoch 360, training loss: 430.70916748046875 = 0.9374630451202393 + 50.0 * 8.595434188842773
Epoch 360, val loss: 0.9407813549041748
Epoch 370, training loss: 430.4060974121094 = 0.9300132989883423 + 50.0 * 8.589521408081055
Epoch 370, val loss: 0.9337095022201538
Epoch 380, training loss: 430.0923156738281 = 0.9225206971168518 + 50.0 * 8.583395957946777
Epoch 380, val loss: 0.9266116619110107
Epoch 390, training loss: 429.7610778808594 = 0.9149381518363953 + 50.0 * 8.576922416687012
Epoch 390, val loss: 0.9194610714912415
Epoch 400, training loss: 429.445556640625 = 0.9073154330253601 + 50.0 * 8.570764541625977
Epoch 400, val loss: 0.9121951460838318
Epoch 410, training loss: 429.0113830566406 = 0.8995093703269958 + 50.0 * 8.562237739562988
Epoch 410, val loss: 0.9048906564712524
Epoch 420, training loss: 428.5286865234375 = 0.8917056918144226 + 50.0 * 8.552740097045898
Epoch 420, val loss: 0.8976150155067444
Epoch 430, training loss: 428.1416015625 = 0.8837947845458984 + 50.0 * 8.545156478881836
Epoch 430, val loss: 0.8901956677436829
Epoch 440, training loss: 427.7789306640625 = 0.8755700588226318 + 50.0 * 8.538066864013672
Epoch 440, val loss: 0.8824746012687683
Epoch 450, training loss: 427.4275207519531 = 0.8671230673789978 + 50.0 * 8.531208038330078
Epoch 450, val loss: 0.874670147895813
Epoch 460, training loss: 427.0865173339844 = 0.8586665987968445 + 50.0 * 8.524557113647461
Epoch 460, val loss: 0.8668525815010071
Epoch 470, training loss: 426.7906799316406 = 0.8502051830291748 + 50.0 * 8.51880931854248
Epoch 470, val loss: 0.8590218424797058
Epoch 480, training loss: 426.4985656738281 = 0.8415185809135437 + 50.0 * 8.513140678405762
Epoch 480, val loss: 0.8510337471961975
Epoch 490, training loss: 426.236572265625 = 0.8327825665473938 + 50.0 * 8.508075714111328
Epoch 490, val loss: 0.8429173827171326
Epoch 500, training loss: 425.99395751953125 = 0.8238996267318726 + 50.0 * 8.503400802612305
Epoch 500, val loss: 0.8347744941711426
Epoch 510, training loss: 425.7658386230469 = 0.8150464296340942 + 50.0 * 8.499015808105469
Epoch 510, val loss: 0.8266599178314209
Epoch 520, training loss: 425.5413818359375 = 0.8063085675239563 + 50.0 * 8.494701385498047
Epoch 520, val loss: 0.8186628818511963
Epoch 530, training loss: 425.333984375 = 0.7976710200309753 + 50.0 * 8.490726470947266
Epoch 530, val loss: 0.8107427358627319
Epoch 540, training loss: 425.2277526855469 = 0.789028525352478 + 50.0 * 8.488774299621582
Epoch 540, val loss: 0.8027933835983276
Epoch 550, training loss: 424.985107421875 = 0.7803477644920349 + 50.0 * 8.484095573425293
Epoch 550, val loss: 0.7948791980743408
Epoch 560, training loss: 424.8163146972656 = 0.771712601184845 + 50.0 * 8.480892181396484
Epoch 560, val loss: 0.7870213389396667
Epoch 570, training loss: 424.6661071777344 = 0.7631626129150391 + 50.0 * 8.478058815002441
Epoch 570, val loss: 0.7792192101478577
Epoch 580, training loss: 424.6001892089844 = 0.7546883225440979 + 50.0 * 8.476909637451172
Epoch 580, val loss: 0.7714837789535522
Epoch 590, training loss: 424.3959655761719 = 0.7462045550346375 + 50.0 * 8.472994804382324
Epoch 590, val loss: 0.7637904286384583
Epoch 600, training loss: 424.2185363769531 = 0.7379809617996216 + 50.0 * 8.469611167907715
Epoch 600, val loss: 0.7563290596008301
Epoch 610, training loss: 424.0416259765625 = 0.7299323678016663 + 50.0 * 8.46623420715332
Epoch 610, val loss: 0.7490618824958801
Epoch 620, training loss: 424.0484619140625 = 0.721921980381012 + 50.0 * 8.466530799865723
Epoch 620, val loss: 0.741871178150177
Epoch 630, training loss: 423.67657470703125 = 0.7140603065490723 + 50.0 * 8.459250450134277
Epoch 630, val loss: 0.7347370386123657
Epoch 640, training loss: 423.4959716796875 = 0.7064365148544312 + 50.0 * 8.455790519714355
Epoch 640, val loss: 0.7278695106506348
Epoch 650, training loss: 423.2877502441406 = 0.698901355266571 + 50.0 * 8.451776504516602
Epoch 650, val loss: 0.7211278676986694
Epoch 660, training loss: 423.4241943359375 = 0.6914207935333252 + 50.0 * 8.454655647277832
Epoch 660, val loss: 0.7144123911857605
Epoch 670, training loss: 423.0448913574219 = 0.6838268041610718 + 50.0 * 8.447220802307129
Epoch 670, val loss: 0.7076095938682556
Epoch 680, training loss: 422.7957458496094 = 0.6764148473739624 + 50.0 * 8.442386627197266
Epoch 680, val loss: 0.7009772062301636
Epoch 690, training loss: 422.6741027832031 = 0.669079065322876 + 50.0 * 8.44010066986084
Epoch 690, val loss: 0.6944121718406677
Epoch 700, training loss: 422.54400634765625 = 0.6618121862411499 + 50.0 * 8.437644004821777
Epoch 700, val loss: 0.6879308819770813
Epoch 710, training loss: 422.6609802246094 = 0.6545816659927368 + 50.0 * 8.440128326416016
Epoch 710, val loss: 0.6815041303634644
Epoch 720, training loss: 422.4067687988281 = 0.6473957300186157 + 50.0 * 8.435187339782715
Epoch 720, val loss: 0.6750759482383728
Epoch 730, training loss: 422.227783203125 = 0.6404600143432617 + 50.0 * 8.431746482849121
Epoch 730, val loss: 0.6689542531967163
Epoch 740, training loss: 422.1233825683594 = 0.6337175369262695 + 50.0 * 8.429793357849121
Epoch 740, val loss: 0.6630101799964905
Epoch 750, training loss: 422.0025329589844 = 0.6271764039993286 + 50.0 * 8.427507400512695
Epoch 750, val loss: 0.6572552919387817
Epoch 760, training loss: 421.9601745605469 = 0.620816171169281 + 50.0 * 8.426787376403809
Epoch 760, val loss: 0.6516823172569275
Epoch 770, training loss: 421.8154296875 = 0.6146256923675537 + 50.0 * 8.424015998840332
Epoch 770, val loss: 0.6463050246238708
Epoch 780, training loss: 421.6690673828125 = 0.6086509823799133 + 50.0 * 8.421208381652832
Epoch 780, val loss: 0.6411486268043518
Epoch 790, training loss: 421.6023864746094 = 0.6029084324836731 + 50.0 * 8.419989585876465
Epoch 790, val loss: 0.6362099051475525
Epoch 800, training loss: 421.4321594238281 = 0.5972967743873596 + 50.0 * 8.41669750213623
Epoch 800, val loss: 0.6313716769218445
Epoch 810, training loss: 421.3319396972656 = 0.5919162034988403 + 50.0 * 8.414800643920898
Epoch 810, val loss: 0.6267876029014587
Epoch 820, training loss: 421.2121276855469 = 0.5867672562599182 + 50.0 * 8.412507057189941
Epoch 820, val loss: 0.6223973035812378
Epoch 830, training loss: 421.0973815917969 = 0.581754744052887 + 50.0 * 8.41031265258789
Epoch 830, val loss: 0.6181482672691345
Epoch 840, training loss: 421.1560974121094 = 0.5769116878509521 + 50.0 * 8.41158390045166
Epoch 840, val loss: 0.6140579581260681
Epoch 850, training loss: 420.9200744628906 = 0.5721495151519775 + 50.0 * 8.40695858001709
Epoch 850, val loss: 0.610051691532135
Epoch 860, training loss: 420.8185119628906 = 0.5676490664482117 + 50.0 * 8.405016899108887
Epoch 860, val loss: 0.6063157320022583
Epoch 870, training loss: 420.7346496582031 = 0.5633600354194641 + 50.0 * 8.403426170349121
Epoch 870, val loss: 0.6027549505233765
Epoch 880, training loss: 420.9234313964844 = 0.5591970086097717 + 50.0 * 8.4072847366333
Epoch 880, val loss: 0.5992817282676697
Epoch 890, training loss: 420.5908508300781 = 0.5550434589385986 + 50.0 * 8.400715827941895
Epoch 890, val loss: 0.5959221720695496
Epoch 900, training loss: 420.5196533203125 = 0.5512257814407349 + 50.0 * 8.399368286132812
Epoch 900, val loss: 0.5928412675857544
Epoch 910, training loss: 420.4148864746094 = 0.54756760597229 + 50.0 * 8.397346496582031
Epoch 910, val loss: 0.5898480415344238
Epoch 920, training loss: 420.3428039550781 = 0.544028103351593 + 50.0 * 8.395975112915039
Epoch 920, val loss: 0.586997389793396
Epoch 930, training loss: 420.2705078125 = 0.5406091213226318 + 50.0 * 8.394598007202148
Epoch 930, val loss: 0.5842639803886414
Epoch 940, training loss: 420.2303466796875 = 0.5372812151908875 + 50.0 * 8.393860816955566
Epoch 940, val loss: 0.5816007256507874
Epoch 950, training loss: 420.27056884765625 = 0.5340315699577332 + 50.0 * 8.394730567932129
Epoch 950, val loss: 0.5790250897407532
Epoch 960, training loss: 420.1323547363281 = 0.5309006571769714 + 50.0 * 8.39202880859375
Epoch 960, val loss: 0.5765756368637085
Epoch 970, training loss: 420.0347595214844 = 0.5279947519302368 + 50.0 * 8.390135765075684
Epoch 970, val loss: 0.5743088126182556
Epoch 980, training loss: 419.9710388183594 = 0.5251879692077637 + 50.0 * 8.388916969299316
Epoch 980, val loss: 0.572100818157196
Epoch 990, training loss: 420.0146484375 = 0.5224472880363464 + 50.0 * 8.389843940734863
Epoch 990, val loss: 0.5699936151504517
Epoch 1000, training loss: 419.86669921875 = 0.5198094844818115 + 50.0 * 8.386938095092773
Epoch 1000, val loss: 0.5679957270622253
Epoch 1010, training loss: 419.7862854003906 = 0.5172974467277527 + 50.0 * 8.385379791259766
Epoch 1010, val loss: 0.5661015510559082
Epoch 1020, training loss: 419.7305603027344 = 0.5148946046829224 + 50.0 * 8.384313583374023
Epoch 1020, val loss: 0.5642656087875366
Epoch 1030, training loss: 419.69268798828125 = 0.5125859975814819 + 50.0 * 8.383602142333984
Epoch 1030, val loss: 0.5625607371330261
Epoch 1040, training loss: 419.70330810546875 = 0.5103073120117188 + 50.0 * 8.383859634399414
Epoch 1040, val loss: 0.5608488321304321
Epoch 1050, training loss: 419.57232666015625 = 0.5080962777137756 + 50.0 * 8.381284713745117
Epoch 1050, val loss: 0.5592713356018066
Epoch 1060, training loss: 419.54156494140625 = 0.5060329437255859 + 50.0 * 8.38071060180664
Epoch 1060, val loss: 0.5577797889709473
Epoch 1070, training loss: 419.54180908203125 = 0.5040397047996521 + 50.0 * 8.380755424499512
Epoch 1070, val loss: 0.5563059449195862
Epoch 1080, training loss: 419.4205322265625 = 0.5020825862884521 + 50.0 * 8.378369331359863
Epoch 1080, val loss: 0.554926335811615
Epoch 1090, training loss: 419.3892517089844 = 0.5002127885818481 + 50.0 * 8.37778091430664
Epoch 1090, val loss: 0.5536142587661743
Epoch 1100, training loss: 419.3392333984375 = 0.4983942210674286 + 50.0 * 8.376816749572754
Epoch 1100, val loss: 0.5523408055305481
Epoch 1110, training loss: 419.3305358886719 = 0.4966124892234802 + 50.0 * 8.376678466796875
Epoch 1110, val loss: 0.5510984659194946
Epoch 1120, training loss: 419.3241271972656 = 0.49486398696899414 + 50.0 * 8.376585006713867
Epoch 1120, val loss: 0.5499392151832581
Epoch 1130, training loss: 419.2720031738281 = 0.49315884709358215 + 50.0 * 8.375576972961426
Epoch 1130, val loss: 0.548743486404419
Epoch 1140, training loss: 419.2060852050781 = 0.4915049970149994 + 50.0 * 8.37429141998291
Epoch 1140, val loss: 0.5475984811782837
Epoch 1150, training loss: 419.1578063964844 = 0.48991772532463074 + 50.0 * 8.373357772827148
Epoch 1150, val loss: 0.5465652942657471
Epoch 1160, training loss: 419.1209411621094 = 0.4883636236190796 + 50.0 * 8.372651100158691
Epoch 1160, val loss: 0.5455336570739746
Epoch 1170, training loss: 419.0982360839844 = 0.4868507385253906 + 50.0 * 8.372227668762207
Epoch 1170, val loss: 0.5445094704627991
Epoch 1180, training loss: 419.1667785644531 = 0.48535650968551636 + 50.0 * 8.373628616333008
Epoch 1180, val loss: 0.5435371994972229
Epoch 1190, training loss: 419.0556945800781 = 0.48385629057884216 + 50.0 * 8.371437072753906
Epoch 1190, val loss: 0.5425519347190857
Epoch 1200, training loss: 419.02313232421875 = 0.48242154717445374 + 50.0 * 8.370814323425293
Epoch 1200, val loss: 0.5416459441184998
Epoch 1210, training loss: 418.9897766113281 = 0.4810433089733124 + 50.0 * 8.370174407958984
Epoch 1210, val loss: 0.5407044291496277
Epoch 1220, training loss: 419.0353698730469 = 0.47967755794525146 + 50.0 * 8.371113777160645
Epoch 1220, val loss: 0.5398089289665222
Epoch 1230, training loss: 418.9439392089844 = 0.47832822799682617 + 50.0 * 8.369312286376953
Epoch 1230, val loss: 0.5389787554740906
Epoch 1240, training loss: 418.8995666503906 = 0.47702983021736145 + 50.0 * 8.368451118469238
Epoch 1240, val loss: 0.5381110906600952
Epoch 1250, training loss: 418.87017822265625 = 0.47575643658638 + 50.0 * 8.367888450622559
Epoch 1250, val loss: 0.5373040437698364
Epoch 1260, training loss: 418.9321594238281 = 0.4745047986507416 + 50.0 * 8.369153022766113
Epoch 1260, val loss: 0.53648841381073
Epoch 1270, training loss: 418.9632873535156 = 0.47320619225502014 + 50.0 * 8.36980152130127
Epoch 1270, val loss: 0.535618245601654
Epoch 1280, training loss: 418.80108642578125 = 0.47196781635284424 + 50.0 * 8.366582870483398
Epoch 1280, val loss: 0.5348342657089233
Epoch 1290, training loss: 418.7741394042969 = 0.4707964062690735 + 50.0 * 8.366066932678223
Epoch 1290, val loss: 0.5340819954872131
Epoch 1300, training loss: 418.7418212890625 = 0.46966105699539185 + 50.0 * 8.365443229675293
Epoch 1300, val loss: 0.5333746075630188
Epoch 1310, training loss: 418.7117919921875 = 0.4685460925102234 + 50.0 * 8.36486530303955
Epoch 1310, val loss: 0.532630443572998
Epoch 1320, training loss: 418.6838684082031 = 0.4674457907676697 + 50.0 * 8.364328384399414
Epoch 1320, val loss: 0.5319316387176514
Epoch 1330, training loss: 418.6618957519531 = 0.4663618206977844 + 50.0 * 8.363910675048828
Epoch 1330, val loss: 0.5312798023223877
Epoch 1340, training loss: 418.830810546875 = 0.46528246998786926 + 50.0 * 8.367310523986816
Epoch 1340, val loss: 0.5306177735328674
Epoch 1350, training loss: 418.6289367675781 = 0.4641757607460022 + 50.0 * 8.363295555114746
Epoch 1350, val loss: 0.5298484563827515
Epoch 1360, training loss: 418.6678161621094 = 0.46313413977622986 + 50.0 * 8.364093780517578
Epoch 1360, val loss: 0.5291755199432373
Epoch 1370, training loss: 418.5589904785156 = 0.46205785870552063 + 50.0 * 8.3619384765625
Epoch 1370, val loss: 0.5284821391105652
Epoch 1380, training loss: 418.53411865234375 = 0.4610442519187927 + 50.0 * 8.361461639404297
Epoch 1380, val loss: 0.5278292298316956
Epoch 1390, training loss: 418.504150390625 = 0.46006688475608826 + 50.0 * 8.360881805419922
Epoch 1390, val loss: 0.5271878838539124
Epoch 1400, training loss: 418.467529296875 = 0.45911553502082825 + 50.0 * 8.36016845703125
Epoch 1400, val loss: 0.526553213596344
Epoch 1410, training loss: 418.4330139160156 = 0.4581727683544159 + 50.0 * 8.3594970703125
Epoch 1410, val loss: 0.5259374380111694
Epoch 1420, training loss: 418.428955078125 = 0.4572369456291199 + 50.0 * 8.359434127807617
Epoch 1420, val loss: 0.5252991914749146
Epoch 1430, training loss: 418.4051818847656 = 0.45627865195274353 + 50.0 * 8.358978271484375
Epoch 1430, val loss: 0.5246837139129639
Epoch 1440, training loss: 418.34423828125 = 0.45534104108810425 + 50.0 * 8.35777759552002
Epoch 1440, val loss: 0.5240956544876099
Epoch 1450, training loss: 418.41888427734375 = 0.4544338881969452 + 50.0 * 8.359289169311523
Epoch 1450, val loss: 0.5235874652862549
Epoch 1460, training loss: 418.319091796875 = 0.45349743962287903 + 50.0 * 8.357312202453613
Epoch 1460, val loss: 0.5228367447853088
Epoch 1470, training loss: 418.3147888183594 = 0.4525964856147766 + 50.0 * 8.357243537902832
Epoch 1470, val loss: 0.5222963690757751
Epoch 1480, training loss: 418.23016357421875 = 0.45171424746513367 + 50.0 * 8.355568885803223
Epoch 1480, val loss: 0.5216688513755798
Epoch 1490, training loss: 418.1823425292969 = 0.45085611939430237 + 50.0 * 8.354629516601562
Epoch 1490, val loss: 0.5211124420166016
Epoch 1500, training loss: 418.148681640625 = 0.45001643896102905 + 50.0 * 8.353973388671875
Epoch 1500, val loss: 0.5205696225166321
Epoch 1510, training loss: 418.2361145019531 = 0.44918230175971985 + 50.0 * 8.355738639831543
Epoch 1510, val loss: 0.520020604133606
Epoch 1520, training loss: 418.23651123046875 = 0.44825562834739685 + 50.0 * 8.355765342712402
Epoch 1520, val loss: 0.5193959474563599
Epoch 1530, training loss: 418.0679931640625 = 0.44738247990608215 + 50.0 * 8.352412223815918
Epoch 1530, val loss: 0.5188148021697998
Epoch 1540, training loss: 418.0301208496094 = 0.44656437635421753 + 50.0 * 8.35167121887207
Epoch 1540, val loss: 0.5182381272315979
Epoch 1550, training loss: 418.0018615722656 = 0.44577276706695557 + 50.0 * 8.35112190246582
Epoch 1550, val loss: 0.5177538990974426
Epoch 1560, training loss: 417.96026611328125 = 0.44498857855796814 + 50.0 * 8.350305557250977
Epoch 1560, val loss: 0.5172185301780701
Epoch 1570, training loss: 417.9482421875 = 0.4442056715488434 + 50.0 * 8.350080490112305
Epoch 1570, val loss: 0.516656756401062
Epoch 1580, training loss: 417.9422607421875 = 0.44338491559028625 + 50.0 * 8.349977493286133
Epoch 1580, val loss: 0.5161645412445068
Epoch 1590, training loss: 417.8843994140625 = 0.4425700008869171 + 50.0 * 8.348836898803711
Epoch 1590, val loss: 0.5155771970748901
Epoch 1600, training loss: 417.85546875 = 0.4417681396007538 + 50.0 * 8.348274230957031
Epoch 1600, val loss: 0.515101432800293
Epoch 1610, training loss: 417.89569091796875 = 0.44097626209259033 + 50.0 * 8.34909439086914
Epoch 1610, val loss: 0.5145690441131592
Epoch 1620, training loss: 417.8026428222656 = 0.44014716148376465 + 50.0 * 8.347249984741211
Epoch 1620, val loss: 0.5140357613563538
Epoch 1630, training loss: 417.7833557128906 = 0.4393508732318878 + 50.0 * 8.346879959106445
Epoch 1630, val loss: 0.5135201215744019
Epoch 1640, training loss: 417.7460021972656 = 0.43857091665267944 + 50.0 * 8.346148490905762
Epoch 1640, val loss: 0.5130007863044739
Epoch 1650, training loss: 417.71234130859375 = 0.43780097365379333 + 50.0 * 8.345490455627441
Epoch 1650, val loss: 0.5124775171279907
Epoch 1660, training loss: 417.68634033203125 = 0.437031626701355 + 50.0 * 8.344985961914062
Epoch 1660, val loss: 0.5119744539260864
Epoch 1670, training loss: 417.79949951171875 = 0.4362575113773346 + 50.0 * 8.347265243530273
Epoch 1670, val loss: 0.5115957856178284
Epoch 1680, training loss: 417.80059814453125 = 0.4354190230369568 + 50.0 * 8.34730339050293
Epoch 1680, val loss: 0.5107702016830444
Epoch 1690, training loss: 417.6395568847656 = 0.434608519077301 + 50.0 * 8.344099044799805
Epoch 1690, val loss: 0.5102530717849731
Epoch 1700, training loss: 417.6007385253906 = 0.43382886052131653 + 50.0 * 8.343338012695312
Epoch 1700, val loss: 0.5097467303276062
Epoch 1710, training loss: 417.5812683105469 = 0.43306270241737366 + 50.0 * 8.342964172363281
Epoch 1710, val loss: 0.5092660784721375
Epoch 1720, training loss: 417.5604248046875 = 0.4322945177555084 + 50.0 * 8.342562675476074
Epoch 1720, val loss: 0.5087432861328125
Epoch 1730, training loss: 417.6191711425781 = 0.4315190315246582 + 50.0 * 8.34375286102295
Epoch 1730, val loss: 0.5082130432128906
Epoch 1740, training loss: 417.531005859375 = 0.43067511916160583 + 50.0 * 8.34200668334961
Epoch 1740, val loss: 0.5076189637184143
Epoch 1750, training loss: 417.5497131347656 = 0.42986297607421875 + 50.0 * 8.34239673614502
Epoch 1750, val loss: 0.5070701241493225
Epoch 1760, training loss: 417.50152587890625 = 0.4290724992752075 + 50.0 * 8.341448783874512
Epoch 1760, val loss: 0.5064706206321716
Epoch 1770, training loss: 417.4710693359375 = 0.4282982051372528 + 50.0 * 8.340855598449707
Epoch 1770, val loss: 0.5059453845024109
Epoch 1780, training loss: 417.4546203613281 = 0.42752769589424133 + 50.0 * 8.34054183959961
Epoch 1780, val loss: 0.5053864121437073
Epoch 1790, training loss: 417.4613342285156 = 0.42675095796585083 + 50.0 * 8.340691566467285
Epoch 1790, val loss: 0.5048251748085022
Epoch 1800, training loss: 417.5173645019531 = 0.42594388127326965 + 50.0 * 8.341828346252441
Epoch 1800, val loss: 0.5042425990104675
Epoch 1810, training loss: 417.4214172363281 = 0.4251333475112915 + 50.0 * 8.339925765991211
Epoch 1810, val loss: 0.5036686062812805
Epoch 1820, training loss: 417.4034118652344 = 0.4243425130844116 + 50.0 * 8.339581489562988
Epoch 1820, val loss: 0.5031140446662903
Epoch 1830, training loss: 417.42633056640625 = 0.4235546588897705 + 50.0 * 8.340055465698242
Epoch 1830, val loss: 0.5025158524513245
Epoch 1840, training loss: 417.375 = 0.4227551519870758 + 50.0 * 8.339044570922852
Epoch 1840, val loss: 0.5019651651382446
Epoch 1850, training loss: 417.357177734375 = 0.4219679832458496 + 50.0 * 8.338704109191895
Epoch 1850, val loss: 0.5013619661331177
Epoch 1860, training loss: 417.3359069824219 = 0.4211760461330414 + 50.0 * 8.338294982910156
Epoch 1860, val loss: 0.5007960200309753
Epoch 1870, training loss: 417.3327331542969 = 0.4203859567642212 + 50.0 * 8.338247299194336
Epoch 1870, val loss: 0.5002168416976929
Epoch 1880, training loss: 417.4927978515625 = 0.41957104206085205 + 50.0 * 8.341464042663574
Epoch 1880, val loss: 0.49966347217559814
Epoch 1890, training loss: 417.3204650878906 = 0.4187403619289398 + 50.0 * 8.338034629821777
Epoch 1890, val loss: 0.49897676706314087
Epoch 1900, training loss: 417.2760314941406 = 0.4179386496543884 + 50.0 * 8.337162017822266
Epoch 1900, val loss: 0.49838048219680786
Epoch 1910, training loss: 417.2691345214844 = 0.4171442687511444 + 50.0 * 8.337039947509766
Epoch 1910, val loss: 0.497834175825119
Epoch 1920, training loss: 417.2723083496094 = 0.4163563847541809 + 50.0 * 8.337119102478027
Epoch 1920, val loss: 0.4972212016582489
Epoch 1930, training loss: 417.3428039550781 = 0.4155477285385132 + 50.0 * 8.338544845581055
Epoch 1930, val loss: 0.4966510832309723
Epoch 1940, training loss: 417.28033447265625 = 0.41473445296287537 + 50.0 * 8.337311744689941
Epoch 1940, val loss: 0.49599573016166687
Epoch 1950, training loss: 417.2368469238281 = 0.41392621397972107 + 50.0 * 8.336458206176758
Epoch 1950, val loss: 0.49539774656295776
Epoch 1960, training loss: 417.19915771484375 = 0.4131277799606323 + 50.0 * 8.335721015930176
Epoch 1960, val loss: 0.4948064982891083
Epoch 1970, training loss: 417.1875915527344 = 0.4123319387435913 + 50.0 * 8.335505485534668
Epoch 1970, val loss: 0.4942295551300049
Epoch 1980, training loss: 417.24993896484375 = 0.41153043508529663 + 50.0 * 8.33676815032959
Epoch 1980, val loss: 0.4936208128929138
Epoch 1990, training loss: 417.24383544921875 = 0.4106932282447815 + 50.0 * 8.336662292480469
Epoch 1990, val loss: 0.4929445683956146
Epoch 2000, training loss: 417.1623229980469 = 0.4098605811595917 + 50.0 * 8.335049629211426
Epoch 2000, val loss: 0.49239087104797363
Epoch 2010, training loss: 417.1347961425781 = 0.40905192494392395 + 50.0 * 8.334514617919922
Epoch 2010, val loss: 0.4917101562023163
Epoch 2020, training loss: 417.1163635253906 = 0.4082504212856293 + 50.0 * 8.334161758422852
Epoch 2020, val loss: 0.49114102125167847
Epoch 2030, training loss: 417.101806640625 = 0.4074490964412689 + 50.0 * 8.333887100219727
Epoch 2030, val loss: 0.4905104637145996
Epoch 2040, training loss: 417.1258850097656 = 0.40663906931877136 + 50.0 * 8.33438491821289
Epoch 2040, val loss: 0.48983970284461975
Epoch 2050, training loss: 417.1307678222656 = 0.405781626701355 + 50.0 * 8.33449935913086
Epoch 2050, val loss: 0.489277720451355
Epoch 2060, training loss: 417.1017761230469 = 0.40492770075798035 + 50.0 * 8.33393669128418
Epoch 2060, val loss: 0.48854511976242065
Epoch 2070, training loss: 417.06268310546875 = 0.4040958881378174 + 50.0 * 8.333171844482422
Epoch 2070, val loss: 0.4880211055278778
Epoch 2080, training loss: 417.0408935546875 = 0.40327516198158264 + 50.0 * 8.332752227783203
Epoch 2080, val loss: 0.4873831570148468
Epoch 2090, training loss: 417.0278625488281 = 0.40246278047561646 + 50.0 * 8.332508087158203
Epoch 2090, val loss: 0.48675674200057983
Epoch 2100, training loss: 417.0517578125 = 0.40164175629615784 + 50.0 * 8.333002090454102
Epoch 2100, val loss: 0.4861595928668976
Epoch 2110, training loss: 417.1296691894531 = 0.40075448155403137 + 50.0 * 8.334578514099121
Epoch 2110, val loss: 0.485516220331192
Epoch 2120, training loss: 417.077392578125 = 0.39986714720726013 + 50.0 * 8.333550453186035
Epoch 2120, val loss: 0.4848537743091583
Epoch 2130, training loss: 416.97796630859375 = 0.39902693033218384 + 50.0 * 8.331579208374023
Epoch 2130, val loss: 0.4841867983341217
Epoch 2140, training loss: 416.97711181640625 = 0.3982003927230835 + 50.0 * 8.331578254699707
Epoch 2140, val loss: 0.48362088203430176
Epoch 2150, training loss: 416.952392578125 = 0.3973841369152069 + 50.0 * 8.331100463867188
Epoch 2150, val loss: 0.482994019985199
Epoch 2160, training loss: 416.94140625 = 0.39656269550323486 + 50.0 * 8.330896377563477
Epoch 2160, val loss: 0.4823843240737915
Epoch 2170, training loss: 416.92803955078125 = 0.3957338333129883 + 50.0 * 8.330646514892578
Epoch 2170, val loss: 0.48176926374435425
Epoch 2180, training loss: 416.93304443359375 = 0.3948976993560791 + 50.0 * 8.33076286315918
Epoch 2180, val loss: 0.4811173975467682
Epoch 2190, training loss: 417.1224060058594 = 0.39403456449508667 + 50.0 * 8.334567070007324
Epoch 2190, val loss: 0.48046398162841797
Epoch 2200, training loss: 416.9364013671875 = 0.39314454793930054 + 50.0 * 8.330864906311035
Epoch 2200, val loss: 0.47989141941070557
Epoch 2210, training loss: 416.8891296386719 = 0.39228060841560364 + 50.0 * 8.329936981201172
Epoch 2210, val loss: 0.47927549481391907
Epoch 2220, training loss: 416.876953125 = 0.39143046736717224 + 50.0 * 8.329710960388184
Epoch 2220, val loss: 0.47857561707496643
Epoch 2230, training loss: 416.8609924316406 = 0.3905814290046692 + 50.0 * 8.329407691955566
Epoch 2230, val loss: 0.47799259424209595
Epoch 2240, training loss: 416.85516357421875 = 0.38973554968833923 + 50.0 * 8.32930850982666
Epoch 2240, val loss: 0.47735852003097534
Epoch 2250, training loss: 417.03704833984375 = 0.38887205719947815 + 50.0 * 8.332962989807129
Epoch 2250, val loss: 0.47675877809524536
Epoch 2260, training loss: 416.8857727050781 = 0.38797351717948914 + 50.0 * 8.3299560546875
Epoch 2260, val loss: 0.4761049449443817
Epoch 2270, training loss: 416.8167419433594 = 0.38710713386535645 + 50.0 * 8.328592300415039
Epoch 2270, val loss: 0.47546905279159546
Epoch 2280, training loss: 416.8046569824219 = 0.38624799251556396 + 50.0 * 8.328368186950684
Epoch 2280, val loss: 0.47486189007759094
Epoch 2290, training loss: 416.8034973144531 = 0.3853911757469177 + 50.0 * 8.328362464904785
Epoch 2290, val loss: 0.47423312067985535
Epoch 2300, training loss: 416.9360656738281 = 0.38451895117759705 + 50.0 * 8.33103084564209
Epoch 2300, val loss: 0.4735908508300781
Epoch 2310, training loss: 416.8073425292969 = 0.3836037516593933 + 50.0 * 8.328474998474121
Epoch 2310, val loss: 0.4731189012527466
Epoch 2320, training loss: 416.7750549316406 = 0.382713258266449 + 50.0 * 8.32784652709961
Epoch 2320, val loss: 0.47236713767051697
Epoch 2330, training loss: 416.7524719238281 = 0.3818298578262329 + 50.0 * 8.327412605285645
Epoch 2330, val loss: 0.471824049949646
Epoch 2340, training loss: 416.7391357421875 = 0.3809542655944824 + 50.0 * 8.327163696289062
Epoch 2340, val loss: 0.4711543917655945
Epoch 2350, training loss: 416.7485046386719 = 0.3800729215145111 + 50.0 * 8.32736873626709
Epoch 2350, val loss: 0.4706169664859772
Epoch 2360, training loss: 416.88409423828125 = 0.37915486097335815 + 50.0 * 8.330099105834961
Epoch 2360, val loss: 0.47000664472579956
Epoch 2370, training loss: 416.7192077636719 = 0.37823593616485596 + 50.0 * 8.32681941986084
Epoch 2370, val loss: 0.4692858159542084
Epoch 2380, training loss: 416.70159912109375 = 0.3773331046104431 + 50.0 * 8.326485633850098
Epoch 2380, val loss: 0.46863505244255066
Epoch 2390, training loss: 416.68902587890625 = 0.37643963098526 + 50.0 * 8.326251983642578
Epoch 2390, val loss: 0.468082994222641
Epoch 2400, training loss: 416.67694091796875 = 0.3755507171154022 + 50.0 * 8.326027870178223
Epoch 2400, val loss: 0.4674210250377655
Epoch 2410, training loss: 416.6988525390625 = 0.3746573030948639 + 50.0 * 8.326483726501465
Epoch 2410, val loss: 0.46682870388031006
Epoch 2420, training loss: 416.7102966308594 = 0.3737381398677826 + 50.0 * 8.326730728149414
Epoch 2420, val loss: 0.4661770462989807
Epoch 2430, training loss: 416.6664733886719 = 0.3728151321411133 + 50.0 * 8.325873374938965
Epoch 2430, val loss: 0.4655803143978119
Epoch 2440, training loss: 416.80621337890625 = 0.3718990683555603 + 50.0 * 8.328686714172363
Epoch 2440, val loss: 0.4649401903152466
Epoch 2450, training loss: 416.6609802246094 = 0.37095579504966736 + 50.0 * 8.325800895690918
Epoch 2450, val loss: 0.46433043479919434
Epoch 2460, training loss: 416.62921142578125 = 0.3700391948223114 + 50.0 * 8.325183868408203
Epoch 2460, val loss: 0.4636235535144806
Epoch 2470, training loss: 416.60736083984375 = 0.3691408336162567 + 50.0 * 8.324764251708984
Epoch 2470, val loss: 0.4630236327648163
Epoch 2480, training loss: 416.5916442871094 = 0.3682422339916229 + 50.0 * 8.324467658996582
Epoch 2480, val loss: 0.4623987376689911
Epoch 2490, training loss: 416.5833740234375 = 0.36734041571617126 + 50.0 * 8.324320793151855
Epoch 2490, val loss: 0.4617585837841034
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.828513444951801
0.8639426211693111
=== training gcn model ===
Epoch 0, training loss: 530.2214965820312 = 1.1071524620056152 + 50.0 * 10.582286834716797
Epoch 0, val loss: 1.1064082384109497
Epoch 10, training loss: 530.1981201171875 = 1.102442741394043 + 50.0 * 10.581913948059082
Epoch 10, val loss: 1.1017452478408813
Epoch 20, training loss: 530.103759765625 = 1.0972846746444702 + 50.0 * 10.580129623413086
Epoch 20, val loss: 1.0966322422027588
Epoch 30, training loss: 529.6748657226562 = 1.0916508436203003 + 50.0 * 10.571663856506348
Epoch 30, val loss: 1.0910674333572388
Epoch 40, training loss: 527.9596557617188 = 1.085343360900879 + 50.0 * 10.537487030029297
Epoch 40, val loss: 1.084808111190796
Epoch 50, training loss: 522.9967041015625 = 1.0781383514404297 + 50.0 * 10.438371658325195
Epoch 50, val loss: 1.0776758193969727
Epoch 60, training loss: 512.7616577148438 = 1.0709763765335083 + 50.0 * 10.233813285827637
Epoch 60, val loss: 1.0706793069839478
Epoch 70, training loss: 496.43505859375 = 1.06414794921875 + 50.0 * 9.907418251037598
Epoch 70, val loss: 1.0639216899871826
Epoch 80, training loss: 477.1358642578125 = 1.0570015907287598 + 50.0 * 9.521576881408691
Epoch 80, val loss: 1.0568218231201172
Epoch 90, training loss: 467.0163269042969 = 1.0496777296066284 + 50.0 * 9.31933307647705
Epoch 90, val loss: 1.049500584602356
Epoch 100, training loss: 462.3993225097656 = 1.0424541234970093 + 50.0 * 9.227137565612793
Epoch 100, val loss: 1.0422921180725098
Epoch 110, training loss: 460.7767639160156 = 1.0355572700500488 + 50.0 * 9.19482421875
Epoch 110, val loss: 1.035486102104187
Epoch 120, training loss: 459.7723083496094 = 1.0298434495925903 + 50.0 * 9.174849510192871
Epoch 120, val loss: 1.029931664466858
Epoch 130, training loss: 458.65179443359375 = 1.0252676010131836 + 50.0 * 9.152530670166016
Epoch 130, val loss: 1.0255082845687866
Epoch 140, training loss: 457.1634521484375 = 1.0212136507034302 + 50.0 * 9.122844696044922
Epoch 140, val loss: 1.0215864181518555
Epoch 150, training loss: 454.9644775390625 = 1.0173708200454712 + 50.0 * 9.07894229888916
Epoch 150, val loss: 1.0178853273391724
Epoch 160, training loss: 451.67529296875 = 1.0140975713729858 + 50.0 * 9.013223648071289
Epoch 160, val loss: 1.0147727727890015
Epoch 170, training loss: 447.87841796875 = 1.0115444660186768 + 50.0 * 8.937337875366211
Epoch 170, val loss: 1.0123419761657715
Epoch 180, training loss: 445.28802490234375 = 1.0091638565063477 + 50.0 * 8.885577201843262
Epoch 180, val loss: 1.0099923610687256
Epoch 190, training loss: 443.1039733886719 = 1.0061569213867188 + 50.0 * 8.84195613861084
Epoch 190, val loss: 1.007008671760559
Epoch 200, training loss: 441.0528259277344 = 1.002996563911438 + 50.0 * 8.800996780395508
Epoch 200, val loss: 1.0039020776748657
Epoch 210, training loss: 439.54229736328125 = 0.9997784495353699 + 50.0 * 8.77085018157959
Epoch 210, val loss: 1.0006303787231445
Epoch 220, training loss: 437.7659606933594 = 0.9966760873794556 + 50.0 * 8.73538589477539
Epoch 220, val loss: 0.9974803328514099
Epoch 230, training loss: 436.44207763671875 = 0.9936888217926025 + 50.0 * 8.708968162536621
Epoch 230, val loss: 0.9945338368415833
Epoch 240, training loss: 435.37371826171875 = 0.9903872013092041 + 50.0 * 8.687666893005371
Epoch 240, val loss: 0.991291880607605
Epoch 250, training loss: 434.2825012207031 = 0.9867862462997437 + 50.0 * 8.665914535522461
Epoch 250, val loss: 0.987794041633606
Epoch 260, training loss: 432.9738464355469 = 0.9833183288574219 + 50.0 * 8.639810562133789
Epoch 260, val loss: 0.9844668507575989
Epoch 270, training loss: 431.933349609375 = 0.9802380800247192 + 50.0 * 8.619062423706055
Epoch 270, val loss: 0.9814107418060303
Epoch 280, training loss: 430.878173828125 = 0.9763942360877991 + 50.0 * 8.59803581237793
Epoch 280, val loss: 0.9776331782341003
Epoch 290, training loss: 430.2139892578125 = 0.9717229008674622 + 50.0 * 8.584845542907715
Epoch 290, val loss: 0.9730255007743835
Epoch 300, training loss: 429.64306640625 = 0.9664459228515625 + 50.0 * 8.573532104492188
Epoch 300, val loss: 0.9678454995155334
Epoch 310, training loss: 429.09649658203125 = 0.960892379283905 + 50.0 * 8.562711715698242
Epoch 310, val loss: 0.9624282717704773
Epoch 320, training loss: 428.5953369140625 = 0.9552204608917236 + 50.0 * 8.552802085876465
Epoch 320, val loss: 0.9568745493888855
Epoch 330, training loss: 428.15093994140625 = 0.9493521451950073 + 50.0 * 8.544032096862793
Epoch 330, val loss: 0.9511131048202515
Epoch 340, training loss: 427.8741149902344 = 0.9431864023208618 + 50.0 * 8.538619041442871
Epoch 340, val loss: 0.9450275897979736
Epoch 350, training loss: 427.58380126953125 = 0.9365420341491699 + 50.0 * 8.532944679260254
Epoch 350, val loss: 0.9385195970535278
Epoch 360, training loss: 427.1605529785156 = 0.9296596646308899 + 50.0 * 8.524618148803711
Epoch 360, val loss: 0.9318781495094299
Epoch 370, training loss: 426.8351135253906 = 0.9226946830749512 + 50.0 * 8.518248558044434
Epoch 370, val loss: 0.9250473380088806
Epoch 380, training loss: 426.52899169921875 = 0.9154388904571533 + 50.0 * 8.5122709274292
Epoch 380, val loss: 0.9180023074150085
Epoch 390, training loss: 426.2447509765625 = 0.9079499244689941 + 50.0 * 8.506735801696777
Epoch 390, val loss: 0.910709023475647
Epoch 400, training loss: 425.9747009277344 = 0.9002125859260559 + 50.0 * 8.501489639282227
Epoch 400, val loss: 0.9031875729560852
Epoch 410, training loss: 425.7044982910156 = 0.8922476172447205 + 50.0 * 8.496245384216309
Epoch 410, val loss: 0.895462155342102
Epoch 420, training loss: 425.5196533203125 = 0.8841040730476379 + 50.0 * 8.492711067199707
Epoch 420, val loss: 0.8875702023506165
Epoch 430, training loss: 425.2244567871094 = 0.8756723999977112 + 50.0 * 8.48697566986084
Epoch 430, val loss: 0.8794466853141785
Epoch 440, training loss: 424.9265441894531 = 0.867184579372406 + 50.0 * 8.481186866760254
Epoch 440, val loss: 0.8712713122367859
Epoch 450, training loss: 424.69390869140625 = 0.8585788011550903 + 50.0 * 8.476706504821777
Epoch 450, val loss: 0.8629940748214722
Epoch 460, training loss: 424.55572509765625 = 0.8498374819755554 + 50.0 * 8.47411823272705
Epoch 460, val loss: 0.8545830249786377
Epoch 470, training loss: 424.36065673828125 = 0.8408077359199524 + 50.0 * 8.470396995544434
Epoch 470, val loss: 0.8460041284561157
Epoch 480, training loss: 424.1130676269531 = 0.8317938446998596 + 50.0 * 8.465625762939453
Epoch 480, val loss: 0.8374053239822388
Epoch 490, training loss: 423.9212646484375 = 0.8227940201759338 + 50.0 * 8.461969375610352
Epoch 490, val loss: 0.8288384079933167
Epoch 500, training loss: 423.7421569824219 = 0.813782274723053 + 50.0 * 8.45856761932373
Epoch 500, val loss: 0.8203084468841553
Epoch 510, training loss: 423.5701599121094 = 0.8048010468482971 + 50.0 * 8.455307006835938
Epoch 510, val loss: 0.8118395209312439
Epoch 520, training loss: 423.53363037109375 = 0.7958455681800842 + 50.0 * 8.454755783081055
Epoch 520, val loss: 0.8034330010414124
Epoch 530, training loss: 423.2392578125 = 0.78680819272995 + 50.0 * 8.44904899597168
Epoch 530, val loss: 0.7949473857879639
Epoch 540, training loss: 423.0941162109375 = 0.777957022190094 + 50.0 * 8.44632339477539
Epoch 540, val loss: 0.7866995334625244
Epoch 550, training loss: 422.910888671875 = 0.7692775130271912 + 50.0 * 8.442831993103027
Epoch 550, val loss: 0.7786422967910767
Epoch 560, training loss: 422.7333679199219 = 0.7607237100601196 + 50.0 * 8.439453125
Epoch 560, val loss: 0.7707474231719971
Epoch 570, training loss: 422.63751220703125 = 0.752186119556427 + 50.0 * 8.437705993652344
Epoch 570, val loss: 0.7628712058067322
Epoch 580, training loss: 422.4591064453125 = 0.7437119483947754 + 50.0 * 8.434308052062988
Epoch 580, val loss: 0.7551151514053345
Epoch 590, training loss: 422.24859619140625 = 0.7354881167411804 + 50.0 * 8.430262565612793
Epoch 590, val loss: 0.7476070523262024
Epoch 600, training loss: 422.0734558105469 = 0.7274019718170166 + 50.0 * 8.426920890808105
Epoch 600, val loss: 0.7402424216270447
Epoch 610, training loss: 421.9593505859375 = 0.7194036245346069 + 50.0 * 8.424798965454102
Epoch 610, val loss: 0.7329963445663452
Epoch 620, training loss: 421.85662841796875 = 0.7113298773765564 + 50.0 * 8.422905921936035
Epoch 620, val loss: 0.7256767749786377
Epoch 630, training loss: 421.74493408203125 = 0.7033421397209167 + 50.0 * 8.420831680297852
Epoch 630, val loss: 0.7184981107711792
Epoch 640, training loss: 421.6163024902344 = 0.695588231086731 + 50.0 * 8.418414115905762
Epoch 640, val loss: 0.7115477919578552
Epoch 650, training loss: 421.5137023925781 = 0.6879735589027405 + 50.0 * 8.41651439666748
Epoch 650, val loss: 0.7047668695449829
Epoch 660, training loss: 421.42669677734375 = 0.680482029914856 + 50.0 * 8.414924621582031
Epoch 660, val loss: 0.698127806186676
Epoch 670, training loss: 421.3489074707031 = 0.6731533408164978 + 50.0 * 8.413515090942383
Epoch 670, val loss: 0.6916557550430298
Epoch 680, training loss: 421.3121643066406 = 0.6659863591194153 + 50.0 * 8.412923812866211
Epoch 680, val loss: 0.6853751540184021
Epoch 690, training loss: 421.33697509765625 = 0.6588851809501648 + 50.0 * 8.413561820983887
Epoch 690, val loss: 0.679131031036377
Epoch 700, training loss: 421.1633605957031 = 0.6520218253135681 + 50.0 * 8.410226821899414
Epoch 700, val loss: 0.673183262348175
Epoch 710, training loss: 421.0849609375 = 0.645449161529541 + 50.0 * 8.408790588378906
Epoch 710, val loss: 0.6675026416778564
Epoch 720, training loss: 421.0333557128906 = 0.6390804648399353 + 50.0 * 8.407885551452637
Epoch 720, val loss: 0.6620119214057922
Epoch 730, training loss: 421.283935546875 = 0.6327469348907471 + 50.0 * 8.413023948669434
Epoch 730, val loss: 0.6565571427345276
Epoch 740, training loss: 420.9245300292969 = 0.6264761090278625 + 50.0 * 8.405961036682129
Epoch 740, val loss: 0.6511860489845276
Epoch 750, training loss: 420.83685302734375 = 0.6206216812133789 + 50.0 * 8.404324531555176
Epoch 750, val loss: 0.646243691444397
Epoch 760, training loss: 420.7853698730469 = 0.6150095462799072 + 50.0 * 8.403407096862793
Epoch 760, val loss: 0.6415271162986755
Epoch 770, training loss: 420.7082214355469 = 0.6095412373542786 + 50.0 * 8.401973724365234
Epoch 770, val loss: 0.6369300484657288
Epoch 780, training loss: 420.6451416015625 = 0.6042007803916931 + 50.0 * 8.400818824768066
Epoch 780, val loss: 0.6324455738067627
Epoch 790, training loss: 420.57879638671875 = 0.5990140438079834 + 50.0 * 8.399596214294434
Epoch 790, val loss: 0.6281227469444275
Epoch 800, training loss: 420.5638732910156 = 0.5940116047859192 + 50.0 * 8.399396896362305
Epoch 800, val loss: 0.623975396156311
Epoch 810, training loss: 420.59515380859375 = 0.5890859961509705 + 50.0 * 8.400121688842773
Epoch 810, val loss: 0.6198528409004211
Epoch 820, training loss: 420.4264831542969 = 0.5842819213867188 + 50.0 * 8.396843910217285
Epoch 820, val loss: 0.6158336997032166
Epoch 830, training loss: 420.3359069824219 = 0.5796954035758972 + 50.0 * 8.395124435424805
Epoch 830, val loss: 0.612067461013794
Epoch 840, training loss: 420.2731628417969 = 0.5753135681152344 + 50.0 * 8.393957138061523
Epoch 840, val loss: 0.6084893941879272
Epoch 850, training loss: 420.48602294921875 = 0.5710227489471436 + 50.0 * 8.398300170898438
Epoch 850, val loss: 0.6049318313598633
Epoch 860, training loss: 420.1993713378906 = 0.5667276978492737 + 50.0 * 8.39265251159668
Epoch 860, val loss: 0.6014184355735779
Epoch 870, training loss: 420.1322937011719 = 0.5627101063728333 + 50.0 * 8.39139175415039
Epoch 870, val loss: 0.5981286764144897
Epoch 880, training loss: 420.06634521484375 = 0.5588518381118774 + 50.0 * 8.39015007019043
Epoch 880, val loss: 0.5949904322624207
Epoch 890, training loss: 420.0017395019531 = 0.5551254749298096 + 50.0 * 8.388932228088379
Epoch 890, val loss: 0.5919607877731323
Epoch 900, training loss: 420.1080017089844 = 0.5514990091323853 + 50.0 * 8.391129493713379
Epoch 900, val loss: 0.5890198349952698
Epoch 910, training loss: 420.0147399902344 = 0.5478196740150452 + 50.0 * 8.389338493347168
Epoch 910, val loss: 0.5859732031822205
Epoch 920, training loss: 419.8470458984375 = 0.5443921685218811 + 50.0 * 8.386053085327148
Epoch 920, val loss: 0.5832066535949707
Epoch 930, training loss: 419.8111572265625 = 0.5410947203636169 + 50.0 * 8.385400772094727
Epoch 930, val loss: 0.5805237293243408
Epoch 940, training loss: 419.74908447265625 = 0.5378991961479187 + 50.0 * 8.384223937988281
Epoch 940, val loss: 0.5779418349266052
Epoch 950, training loss: 419.704345703125 = 0.5348085761070251 + 50.0 * 8.383390426635742
Epoch 950, val loss: 0.5754148364067078
Epoch 960, training loss: 419.91778564453125 = 0.5317284464836121 + 50.0 * 8.387721061706543
Epoch 960, val loss: 0.5728816390037537
Epoch 970, training loss: 419.6165771484375 = 0.5286616683006287 + 50.0 * 8.381758689880371
Epoch 970, val loss: 0.5704253315925598
Epoch 980, training loss: 419.59033203125 = 0.5258082151412964 + 50.0 * 8.381290435791016
Epoch 980, val loss: 0.5681127309799194
Epoch 990, training loss: 419.5420837402344 = 0.5230302214622498 + 50.0 * 8.380380630493164
Epoch 990, val loss: 0.5658647418022156
Epoch 1000, training loss: 419.5032043457031 = 0.5203264355659485 + 50.0 * 8.379657745361328
Epoch 1000, val loss: 0.5636913180351257
Epoch 1010, training loss: 419.69671630859375 = 0.5176624655723572 + 50.0 * 8.383581161499023
Epoch 1010, val loss: 0.5615050196647644
Epoch 1020, training loss: 419.4817199707031 = 0.5150352120399475 + 50.0 * 8.37933349609375
Epoch 1020, val loss: 0.5594676733016968
Epoch 1030, training loss: 419.3794250488281 = 0.5125449299812317 + 50.0 * 8.377337455749512
Epoch 1030, val loss: 0.5574371814727783
Epoch 1040, training loss: 419.34320068359375 = 0.5101410150527954 + 50.0 * 8.37666130065918
Epoch 1040, val loss: 0.5555016398429871
Epoch 1050, training loss: 419.3037414550781 = 0.5078238248825073 + 50.0 * 8.3759183883667
Epoch 1050, val loss: 0.553701639175415
Epoch 1060, training loss: 419.2781982421875 = 0.5055585503578186 + 50.0 * 8.375452995300293
Epoch 1060, val loss: 0.5518757700920105
Epoch 1070, training loss: 419.3952941894531 = 0.5032908320426941 + 50.0 * 8.377840042114258
Epoch 1070, val loss: 0.5501219034194946
Epoch 1080, training loss: 419.3321228027344 = 0.5010722279548645 + 50.0 * 8.37662124633789
Epoch 1080, val loss: 0.5483192205429077
Epoch 1090, training loss: 419.1873474121094 = 0.4989483058452606 + 50.0 * 8.373767852783203
Epoch 1090, val loss: 0.5466148257255554
Epoch 1100, training loss: 419.1336364746094 = 0.49693071842193604 + 50.0 * 8.372734069824219
Epoch 1100, val loss: 0.5450550317764282
Epoch 1110, training loss: 419.08349609375 = 0.49498751759529114 + 50.0 * 8.371769905090332
Epoch 1110, val loss: 0.5435760617256165
Epoch 1120, training loss: 419.0526428222656 = 0.4931008815765381 + 50.0 * 8.371191024780273
Epoch 1120, val loss: 0.5420895218849182
Epoch 1130, training loss: 419.328125 = 0.4912181794643402 + 50.0 * 8.376738548278809
Epoch 1130, val loss: 0.5405783653259277
Epoch 1140, training loss: 419.02032470703125 = 0.4892929792404175 + 50.0 * 8.370620727539062
Epoch 1140, val loss: 0.5391613841056824
Epoch 1150, training loss: 418.9813537597656 = 0.4875149428844452 + 50.0 * 8.369876861572266
Epoch 1150, val loss: 0.5377921462059021
Epoch 1160, training loss: 418.9131774902344 = 0.48579949140548706 + 50.0 * 8.368547439575195
Epoch 1160, val loss: 0.5364698171615601
Epoch 1170, training loss: 418.90045166015625 = 0.484129399061203 + 50.0 * 8.368326187133789
Epoch 1170, val loss: 0.5352020859718323
Epoch 1180, training loss: 418.9476318359375 = 0.4824426472187042 + 50.0 * 8.369303703308105
Epoch 1180, val loss: 0.533951997756958
Epoch 1190, training loss: 418.8810729980469 = 0.48075392842292786 + 50.0 * 8.368006706237793
Epoch 1190, val loss: 0.5326992273330688
Epoch 1200, training loss: 418.8168029785156 = 0.4791414737701416 + 50.0 * 8.366752624511719
Epoch 1200, val loss: 0.5314198732376099
Epoch 1210, training loss: 418.7633056640625 = 0.47760531306266785 + 50.0 * 8.365714073181152
Epoch 1210, val loss: 0.5303065776824951
Epoch 1220, training loss: 418.7323303222656 = 0.4760942757129669 + 50.0 * 8.365124702453613
Epoch 1220, val loss: 0.5291938781738281
Epoch 1230, training loss: 418.7989807128906 = 0.47459715604782104 + 50.0 * 8.366487503051758
Epoch 1230, val loss: 0.5280969738960266
Epoch 1240, training loss: 418.8439025878906 = 0.47307083010673523 + 50.0 * 8.367416381835938
Epoch 1240, val loss: 0.5268984436988831
Epoch 1250, training loss: 418.6772155761719 = 0.47158822417259216 + 50.0 * 8.364112854003906
Epoch 1250, val loss: 0.5258744955062866
Epoch 1260, training loss: 418.6199035644531 = 0.4701865017414093 + 50.0 * 8.362994194030762
Epoch 1260, val loss: 0.5248785018920898
Epoch 1270, training loss: 418.58056640625 = 0.46881982684135437 + 50.0 * 8.362235069274902
Epoch 1270, val loss: 0.5238950848579407
Epoch 1280, training loss: 418.5478515625 = 0.46747079491615295 + 50.0 * 8.361607551574707
Epoch 1280, val loss: 0.5229440927505493
Epoch 1290, training loss: 418.54266357421875 = 0.46615877747535706 + 50.0 * 8.361530303955078
Epoch 1290, val loss: 0.5220873951911926
Epoch 1300, training loss: 418.72344970703125 = 0.46477317810058594 + 50.0 * 8.36517333984375
Epoch 1300, val loss: 0.521094024181366
Epoch 1310, training loss: 418.530029296875 = 0.46336567401885986 + 50.0 * 8.361332893371582
Epoch 1310, val loss: 0.5200864672660828
Epoch 1320, training loss: 418.43603515625 = 0.4621080160140991 + 50.0 * 8.359478950500488
Epoch 1320, val loss: 0.5191724300384521
Epoch 1330, training loss: 418.4172668457031 = 0.4608987867832184 + 50.0 * 8.359127044677734
Epoch 1330, val loss: 0.518366813659668
Epoch 1340, training loss: 418.3699951171875 = 0.45971181988716125 + 50.0 * 8.358205795288086
Epoch 1340, val loss: 0.5175589323043823
Epoch 1350, training loss: 418.3605041503906 = 0.45853739976882935 + 50.0 * 8.358039855957031
Epoch 1350, val loss: 0.5168104767799377
Epoch 1360, training loss: 418.3565979003906 = 0.4573502242565155 + 50.0 * 8.357985496520996
Epoch 1360, val loss: 0.5159963369369507
Epoch 1370, training loss: 418.3381042480469 = 0.45616501569747925 + 50.0 * 8.357638359069824
Epoch 1370, val loss: 0.5152000784873962
Epoch 1380, training loss: 418.2868347167969 = 0.45503854751586914 + 50.0 * 8.356636047363281
Epoch 1380, val loss: 0.5145184993743896
Epoch 1390, training loss: 418.239501953125 = 0.45394662022590637 + 50.0 * 8.355710983276367
Epoch 1390, val loss: 0.5137990117073059
Epoch 1400, training loss: 418.2156066894531 = 0.45286330580711365 + 50.0 * 8.355255126953125
Epoch 1400, val loss: 0.5130753517150879
Epoch 1410, training loss: 418.2270812988281 = 0.4517812430858612 + 50.0 * 8.35550594329834
Epoch 1410, val loss: 0.5124121904373169
Epoch 1420, training loss: 418.16949462890625 = 0.45072275400161743 + 50.0 * 8.354375839233398
Epoch 1420, val loss: 0.5118284225463867
Epoch 1430, training loss: 418.2122497558594 = 0.4496522843837738 + 50.0 * 8.355252265930176
Epoch 1430, val loss: 0.5111725330352783
Epoch 1440, training loss: 418.13897705078125 = 0.4485523998737335 + 50.0 * 8.353808403015137
Epoch 1440, val loss: 0.5104573965072632
Epoch 1450, training loss: 418.06683349609375 = 0.4475088119506836 + 50.0 * 8.352386474609375
Epoch 1450, val loss: 0.5097624659538269
Epoch 1460, training loss: 418.0245666503906 = 0.44652286171913147 + 50.0 * 8.351560592651367
Epoch 1460, val loss: 0.5092455148696899
Epoch 1470, training loss: 417.98876953125 = 0.4455569386482239 + 50.0 * 8.35086441040039
Epoch 1470, val loss: 0.5086652636528015
Epoch 1480, training loss: 417.95343017578125 = 0.4445926249027252 + 50.0 * 8.350176811218262
Epoch 1480, val loss: 0.50812828540802
Epoch 1490, training loss: 417.9669189453125 = 0.4436330795288086 + 50.0 * 8.350465774536133
Epoch 1490, val loss: 0.507634699344635
Epoch 1500, training loss: 418.00238037109375 = 0.4425734281539917 + 50.0 * 8.3511962890625
Epoch 1500, val loss: 0.5069647431373596
Epoch 1510, training loss: 417.9969177246094 = 0.44151151180267334 + 50.0 * 8.35110855102539
Epoch 1510, val loss: 0.5063934326171875
Epoch 1520, training loss: 417.8572998046875 = 0.44054701924324036 + 50.0 * 8.348335266113281
Epoch 1520, val loss: 0.5058510899543762
Epoch 1530, training loss: 417.8251647949219 = 0.4396292567253113 + 50.0 * 8.347710609436035
Epoch 1530, val loss: 0.5053113102912903
Epoch 1540, training loss: 417.7991638183594 = 0.43873029947280884 + 50.0 * 8.347208976745605
Epoch 1540, val loss: 0.5048303008079529
Epoch 1550, training loss: 417.77734375 = 0.43782490491867065 + 50.0 * 8.346790313720703
Epoch 1550, val loss: 0.5043862462043762
Epoch 1560, training loss: 418.04449462890625 = 0.43688637018203735 + 50.0 * 8.352151870727539
Epoch 1560, val loss: 0.5039081573486328
Epoch 1570, training loss: 417.7781982421875 = 0.4358910322189331 + 50.0 * 8.346846580505371
Epoch 1570, val loss: 0.5032926797866821
Epoch 1580, training loss: 417.712158203125 = 0.434971421957016 + 50.0 * 8.34554386138916
Epoch 1580, val loss: 0.50284343957901
Epoch 1590, training loss: 417.6838073730469 = 0.4340684115886688 + 50.0 * 8.34499454498291
Epoch 1590, val loss: 0.5023651123046875
Epoch 1600, training loss: 417.7101135253906 = 0.43317514657974243 + 50.0 * 8.345539093017578
Epoch 1600, val loss: 0.5018850564956665
Epoch 1610, training loss: 417.6365966796875 = 0.4322376847267151 + 50.0 * 8.344086647033691
Epoch 1610, val loss: 0.5014389157295227
Epoch 1620, training loss: 417.6383056640625 = 0.431316614151001 + 50.0 * 8.34414005279541
Epoch 1620, val loss: 0.5008968710899353
Epoch 1630, training loss: 417.5955505371094 = 0.4304327964782715 + 50.0 * 8.343302726745605
Epoch 1630, val loss: 0.5004989504814148
Epoch 1640, training loss: 417.564697265625 = 0.42956238985061646 + 50.0 * 8.342702865600586
Epoch 1640, val loss: 0.5000549554824829
Epoch 1650, training loss: 417.6109619140625 = 0.42870375514030457 + 50.0 * 8.343645095825195
Epoch 1650, val loss: 0.49964314699172974
Epoch 1660, training loss: 417.5776062011719 = 0.4277544915676117 + 50.0 * 8.342996597290039
Epoch 1660, val loss: 0.4990962743759155
Epoch 1670, training loss: 417.57037353515625 = 0.4268445372581482 + 50.0 * 8.342870712280273
Epoch 1670, val loss: 0.49868708848953247
Epoch 1680, training loss: 417.4917297363281 = 0.4259789288043976 + 50.0 * 8.341315269470215
Epoch 1680, val loss: 0.4982158839702606
Epoch 1690, training loss: 417.46429443359375 = 0.4251382052898407 + 50.0 * 8.34078311920166
Epoch 1690, val loss: 0.497800350189209
Epoch 1700, training loss: 417.4423828125 = 0.42431190609931946 + 50.0 * 8.340361595153809
Epoch 1700, val loss: 0.49743056297302246
Epoch 1710, training loss: 417.4333801269531 = 0.423486590385437 + 50.0 * 8.340197563171387
Epoch 1710, val loss: 0.49704819917678833
Epoch 1720, training loss: 417.614990234375 = 0.4226410686969757 + 50.0 * 8.343847274780273
Epoch 1720, val loss: 0.4966990053653717
Epoch 1730, training loss: 417.4288024902344 = 0.4217219948768616 + 50.0 * 8.340141296386719
Epoch 1730, val loss: 0.49616289138793945
Epoch 1740, training loss: 417.3851623535156 = 0.4208686053752899 + 50.0 * 8.339285850524902
Epoch 1740, val loss: 0.49567174911499023
Epoch 1750, training loss: 417.3550109863281 = 0.4200475513935089 + 50.0 * 8.338699340820312
Epoch 1750, val loss: 0.4953463077545166
Epoch 1760, training loss: 417.321533203125 = 0.41923266649246216 + 50.0 * 8.338046073913574
Epoch 1760, val loss: 0.4949420094490051
Epoch 1770, training loss: 417.302978515625 = 0.4184264540672302 + 50.0 * 8.337691307067871
Epoch 1770, val loss: 0.4945654273033142
Epoch 1780, training loss: 417.3892822265625 = 0.41761451959609985 + 50.0 * 8.339433670043945
Epoch 1780, val loss: 0.49421367049217224
Epoch 1790, training loss: 417.3532409667969 = 0.41673940420150757 + 50.0 * 8.338729858398438
Epoch 1790, val loss: 0.493763267993927
Epoch 1800, training loss: 417.35601806640625 = 0.41586658358573914 + 50.0 * 8.3388032913208
Epoch 1800, val loss: 0.49332931637763977
Epoch 1810, training loss: 417.2630920410156 = 0.4150274693965912 + 50.0 * 8.336960792541504
Epoch 1810, val loss: 0.4928726553916931
Epoch 1820, training loss: 417.2126159667969 = 0.41421395540237427 + 50.0 * 8.335968017578125
Epoch 1820, val loss: 0.49248048663139343
Epoch 1830, training loss: 417.2018127441406 = 0.41341960430145264 + 50.0 * 8.33576774597168
Epoch 1830, val loss: 0.4921272397041321
Epoch 1840, training loss: 417.2210388183594 = 0.4126172959804535 + 50.0 * 8.33616828918457
Epoch 1840, val loss: 0.49171122908592224
Epoch 1850, training loss: 417.24664306640625 = 0.41178396344184875 + 50.0 * 8.336697578430176
Epoch 1850, val loss: 0.49126166105270386
Epoch 1860, training loss: 417.43072509765625 = 0.4109279215335846 + 50.0 * 8.3403959274292
Epoch 1860, val loss: 0.49079933762550354
Epoch 1870, training loss: 417.1865234375 = 0.4100496172904968 + 50.0 * 8.335529327392578
Epoch 1870, val loss: 0.49043408036231995
Epoch 1880, training loss: 417.119384765625 = 0.4092281758785248 + 50.0 * 8.334202766418457
Epoch 1880, val loss: 0.49000832438468933
Epoch 1890, training loss: 417.0943603515625 = 0.4084351062774658 + 50.0 * 8.333718299865723
Epoch 1890, val loss: 0.4896228611469269
Epoch 1900, training loss: 417.07464599609375 = 0.40764492750167847 + 50.0 * 8.33333969116211
Epoch 1900, val loss: 0.4892300069332123
Epoch 1910, training loss: 417.0559997558594 = 0.4068547189235687 + 50.0 * 8.332983016967773
Epoch 1910, val loss: 0.48885378241539
Epoch 1920, training loss: 417.07452392578125 = 0.40605446696281433 + 50.0 * 8.333369255065918
Epoch 1920, val loss: 0.4884265065193176
Epoch 1930, training loss: 417.1051025390625 = 0.4052165150642395 + 50.0 * 8.33399772644043
Epoch 1930, val loss: 0.48803213238716125
Epoch 1940, training loss: 417.0099182128906 = 0.4043860137462616 + 50.0 * 8.332110404968262
Epoch 1940, val loss: 0.4876314401626587
Epoch 1950, training loss: 417.0024719238281 = 0.4035821557044983 + 50.0 * 8.331977844238281
Epoch 1950, val loss: 0.48723116517066956
Epoch 1960, training loss: 417.1124572753906 = 0.40278732776641846 + 50.0 * 8.334193229675293
Epoch 1960, val loss: 0.48688405752182007
Epoch 1970, training loss: 417.0142822265625 = 0.4019583463668823 + 50.0 * 8.332246780395508
Epoch 1970, val loss: 0.48638489842414856
Epoch 1980, training loss: 416.989501953125 = 0.40114614367485046 + 50.0 * 8.331767082214355
Epoch 1980, val loss: 0.4860290288925171
Epoch 1990, training loss: 416.93927001953125 = 0.4003519117832184 + 50.0 * 8.330778121948242
Epoch 1990, val loss: 0.48554375767707825
Epoch 2000, training loss: 416.9193420410156 = 0.39957982301712036 + 50.0 * 8.330395698547363
Epoch 2000, val loss: 0.48520275950431824
Epoch 2010, training loss: 416.95147705078125 = 0.3988088369369507 + 50.0 * 8.331053733825684
Epoch 2010, val loss: 0.48475661873817444
Epoch 2020, training loss: 417.0634765625 = 0.3979894518852234 + 50.0 * 8.3333101272583
Epoch 2020, val loss: 0.4843694567680359
Epoch 2030, training loss: 416.9190368652344 = 0.39715543389320374 + 50.0 * 8.330437660217285
Epoch 2030, val loss: 0.4839661419391632
Epoch 2040, training loss: 416.8689270019531 = 0.3963715732097626 + 50.0 * 8.329451560974121
Epoch 2040, val loss: 0.4835335314273834
Epoch 2050, training loss: 416.8462829589844 = 0.395609587430954 + 50.0 * 8.32901382446289
Epoch 2050, val loss: 0.4831838607788086
Epoch 2060, training loss: 416.8316955566406 = 0.39485427737236023 + 50.0 * 8.328736305236816
Epoch 2060, val loss: 0.48278602957725525
Epoch 2070, training loss: 416.8420715332031 = 0.3940941393375397 + 50.0 * 8.328959465026855
Epoch 2070, val loss: 0.4824070930480957
Epoch 2080, training loss: 416.9145812988281 = 0.39329269528388977 + 50.0 * 8.330426216125488
Epoch 2080, val loss: 0.4820248484611511
Epoch 2090, training loss: 416.8319396972656 = 0.3924812078475952 + 50.0 * 8.328788757324219
Epoch 2090, val loss: 0.48155951499938965
Epoch 2100, training loss: 416.7916564941406 = 0.39169663190841675 + 50.0 * 8.327999114990234
Epoch 2100, val loss: 0.48114022612571716
Epoch 2110, training loss: 416.7619323730469 = 0.3909335732460022 + 50.0 * 8.327420234680176
Epoch 2110, val loss: 0.480759859085083
Epoch 2120, training loss: 416.7630920410156 = 0.3901762366294861 + 50.0 * 8.327458381652832
Epoch 2120, val loss: 0.4803354740142822
Epoch 2130, training loss: 416.8439636230469 = 0.38940560817718506 + 50.0 * 8.32909107208252
Epoch 2130, val loss: 0.47992268204689026
Epoch 2140, training loss: 416.8921813964844 = 0.3886149227619171 + 50.0 * 8.330071449279785
Epoch 2140, val loss: 0.4796159267425537
Epoch 2150, training loss: 416.7470703125 = 0.38778039813041687 + 50.0 * 8.32718563079834
Epoch 2150, val loss: 0.47898533940315247
Epoch 2160, training loss: 416.7181396484375 = 0.38699430227279663 + 50.0 * 8.32662296295166
Epoch 2160, val loss: 0.4786393642425537
Epoch 2170, training loss: 416.6889953613281 = 0.3862321674823761 + 50.0 * 8.326055526733398
Epoch 2170, val loss: 0.47821253538131714
Epoch 2180, training loss: 416.6686096191406 = 0.385481595993042 + 50.0 * 8.325662612915039
Epoch 2180, val loss: 0.47778525948524475
Epoch 2190, training loss: 416.6842346191406 = 0.3847271502017975 + 50.0 * 8.325989723205566
Epoch 2190, val loss: 0.4774254262447357
Epoch 2200, training loss: 416.7934875488281 = 0.383933961391449 + 50.0 * 8.328190803527832
Epoch 2200, val loss: 0.4769895672798157
Epoch 2210, training loss: 416.63568115234375 = 0.3831140995025635 + 50.0 * 8.325051307678223
Epoch 2210, val loss: 0.476567804813385
Epoch 2220, training loss: 416.6261291503906 = 0.3823379576206207 + 50.0 * 8.324875831604004
Epoch 2220, val loss: 0.47607940435409546
Epoch 2230, training loss: 416.6100158691406 = 0.381578266620636 + 50.0 * 8.324568748474121
Epoch 2230, val loss: 0.47571584582328796
Epoch 2240, training loss: 416.65313720703125 = 0.3808189928531647 + 50.0 * 8.325446128845215
Epoch 2240, val loss: 0.4753535985946655
Epoch 2250, training loss: 416.72369384765625 = 0.3800109922885895 + 50.0 * 8.326873779296875
Epoch 2250, val loss: 0.4749234616756439
Epoch 2260, training loss: 416.58966064453125 = 0.3792046010494232 + 50.0 * 8.324209213256836
Epoch 2260, val loss: 0.4744546115398407
Epoch 2270, training loss: 416.55535888671875 = 0.37842699885368347 + 50.0 * 8.323538780212402
Epoch 2270, val loss: 0.47400185465812683
Epoch 2280, training loss: 416.5394592285156 = 0.3776760995388031 + 50.0 * 8.323235511779785
Epoch 2280, val loss: 0.4736303687095642
Epoch 2290, training loss: 416.5266418457031 = 0.37692829966545105 + 50.0 * 8.322994232177734
Epoch 2290, val loss: 0.4732477366924286
Epoch 2300, training loss: 416.52484130859375 = 0.37617725133895874 + 50.0 * 8.322973251342773
Epoch 2300, val loss: 0.47288644313812256
Epoch 2310, training loss: 416.68682861328125 = 0.3754141926765442 + 50.0 * 8.326228141784668
Epoch 2310, val loss: 0.47253739833831787
Epoch 2320, training loss: 416.5574035644531 = 0.37461307644844055 + 50.0 * 8.32365608215332
Epoch 2320, val loss: 0.47202184796333313
Epoch 2330, training loss: 416.51483154296875 = 0.3738345801830292 + 50.0 * 8.322819709777832
Epoch 2330, val loss: 0.4716021716594696
Epoch 2340, training loss: 416.4814147949219 = 0.37306949496269226 + 50.0 * 8.322166442871094
Epoch 2340, val loss: 0.4711587727069855
Epoch 2350, training loss: 416.468994140625 = 0.37231770157814026 + 50.0 * 8.32193374633789
Epoch 2350, val loss: 0.47079455852508545
Epoch 2360, training loss: 416.5933532714844 = 0.37156039476394653 + 50.0 * 8.32443618774414
Epoch 2360, val loss: 0.47044965624809265
Epoch 2370, training loss: 416.5118408203125 = 0.3707585334777832 + 50.0 * 8.322821617126465
Epoch 2370, val loss: 0.4698425233364105
Epoch 2380, training loss: 416.4915771484375 = 0.3699702024459839 + 50.0 * 8.322432518005371
Epoch 2380, val loss: 0.46949219703674316
Epoch 2390, training loss: 416.4269714355469 = 0.36919304728507996 + 50.0 * 8.321155548095703
Epoch 2390, val loss: 0.46894705295562744
Epoch 2400, training loss: 416.41796875 = 0.36843496561050415 + 50.0 * 8.320990562438965
Epoch 2400, val loss: 0.4685598909854889
Epoch 2410, training loss: 416.45770263671875 = 0.3676750063896179 + 50.0 * 8.321800231933594
Epoch 2410, val loss: 0.468063622713089
Epoch 2420, training loss: 416.4436950683594 = 0.3668963313102722 + 50.0 * 8.32153606414795
Epoch 2420, val loss: 0.46767643094062805
Epoch 2430, training loss: 416.52850341796875 = 0.36611154675483704 + 50.0 * 8.323247909545898
Epoch 2430, val loss: 0.4672265648841858
Epoch 2440, training loss: 416.3940734863281 = 0.3653077781200409 + 50.0 * 8.320575714111328
Epoch 2440, val loss: 0.46679967641830444
Epoch 2450, training loss: 416.3738098144531 = 0.3645334839820862 + 50.0 * 8.320185661315918
Epoch 2450, val loss: 0.4663292169570923
Epoch 2460, training loss: 416.342529296875 = 0.36377716064453125 + 50.0 * 8.319575309753418
Epoch 2460, val loss: 0.46590539813041687
Epoch 2470, training loss: 416.33795166015625 = 0.3630273938179016 + 50.0 * 8.319498062133789
Epoch 2470, val loss: 0.46547675132751465
Epoch 2480, training loss: 416.3809509277344 = 0.3622709810733795 + 50.0 * 8.32037353515625
Epoch 2480, val loss: 0.46503496170043945
Epoch 2490, training loss: 416.48358154296875 = 0.36147916316986084 + 50.0 * 8.322442054748535
Epoch 2490, val loss: 0.4645712971687317
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.830542871638762
0.8629283489096574
=== training gcn model ===
Epoch 0, training loss: 530.222900390625 = 1.1081265211105347 + 50.0 * 10.582295417785645
Epoch 0, val loss: 1.1072566509246826
Epoch 10, training loss: 530.1987915039062 = 1.103074312210083 + 50.0 * 10.581913948059082
Epoch 10, val loss: 1.1022030115127563
Epoch 20, training loss: 530.1035766601562 = 1.0975878238677979 + 50.0 * 10.580120086669922
Epoch 20, val loss: 1.0967215299606323
Epoch 30, training loss: 529.6972045898438 = 1.0916454792022705 + 50.0 * 10.572111129760742
Epoch 30, val loss: 1.0907942056655884
Epoch 40, training loss: 528.1927490234375 = 1.0851069688796997 + 50.0 * 10.542153358459473
Epoch 40, val loss: 1.0842223167419434
Epoch 50, training loss: 524.1423950195312 = 1.0775893926620483 + 50.0 * 10.461296081542969
Epoch 50, val loss: 1.0767171382904053
Epoch 60, training loss: 516.0313110351562 = 1.0701816082000732 + 50.0 * 10.299222946166992
Epoch 60, val loss: 1.069413423538208
Epoch 70, training loss: 505.9730224609375 = 1.0629171133041382 + 50.0 * 10.098201751708984
Epoch 70, val loss: 1.062155842781067
Epoch 80, training loss: 488.95306396484375 = 1.0560065507888794 + 50.0 * 9.757941246032715
Epoch 80, val loss: 1.0555301904678345
Epoch 90, training loss: 473.7061767578125 = 1.0507006645202637 + 50.0 * 9.453109741210938
Epoch 90, val loss: 1.0502885580062866
Epoch 100, training loss: 465.397216796875 = 1.0459579229354858 + 50.0 * 9.287025451660156
Epoch 100, val loss: 1.0456304550170898
Epoch 110, training loss: 461.723388671875 = 1.0418787002563477 + 50.0 * 9.213630676269531
Epoch 110, val loss: 1.0416070222854614
Epoch 120, training loss: 460.0459899902344 = 1.0381948947906494 + 50.0 * 9.180155754089355
Epoch 120, val loss: 1.0379177331924438
Epoch 130, training loss: 458.78204345703125 = 1.0345577001571655 + 50.0 * 9.154950141906738
Epoch 130, val loss: 1.0342828035354614
Epoch 140, training loss: 457.2704772949219 = 1.0310089588165283 + 50.0 * 9.124789237976074
Epoch 140, val loss: 1.0307297706604004
Epoch 150, training loss: 455.1406555175781 = 1.0277326107025146 + 50.0 * 9.082258224487305
Epoch 150, val loss: 1.0274511575698853
Epoch 160, training loss: 452.10589599609375 = 1.0249017477035522 + 50.0 * 9.02161979675293
Epoch 160, val loss: 1.0246838331222534
Epoch 170, training loss: 448.43145751953125 = 1.0224961042404175 + 50.0 * 8.948179244995117
Epoch 170, val loss: 1.022341251373291
Epoch 180, training loss: 445.41595458984375 = 1.0198379755020142 + 50.0 * 8.887922286987305
Epoch 180, val loss: 1.01966392993927
Epoch 190, training loss: 443.4435119628906 = 1.0162960290908813 + 50.0 * 8.848544120788574
Epoch 190, val loss: 1.0160772800445557
Epoch 200, training loss: 442.4011535644531 = 1.0118417739868164 + 50.0 * 8.827786445617676
Epoch 200, val loss: 1.0115758180618286
Epoch 210, training loss: 441.6357727050781 = 1.0069050788879395 + 50.0 * 8.812577247619629
Epoch 210, val loss: 1.0066416263580322
Epoch 220, training loss: 440.7298889160156 = 1.0022014379501343 + 50.0 * 8.794553756713867
Epoch 220, val loss: 1.0020133256912231
Epoch 230, training loss: 439.64544677734375 = 0.9979041814804077 + 50.0 * 8.772951126098633
Epoch 230, val loss: 0.9978021383285522
Epoch 240, training loss: 438.399169921875 = 0.9937279224395752 + 50.0 * 8.748108863830566
Epoch 240, val loss: 0.9936981797218323
Epoch 250, training loss: 437.36328125 = 0.9893062710762024 + 50.0 * 8.727478981018066
Epoch 250, val loss: 0.9893272519111633
Epoch 260, training loss: 436.7174987792969 = 0.9841222167015076 + 50.0 * 8.714667320251465
Epoch 260, val loss: 0.9841551780700684
Epoch 270, training loss: 436.18731689453125 = 0.9782400727272034 + 50.0 * 8.704181671142578
Epoch 270, val loss: 0.978297770023346
Epoch 280, training loss: 435.60235595703125 = 0.9722704887390137 + 50.0 * 8.692602157592773
Epoch 280, val loss: 0.9724021553993225
Epoch 290, training loss: 434.93035888671875 = 0.9664256572723389 + 50.0 * 8.679278373718262
Epoch 290, val loss: 0.9666562080383301
Epoch 300, training loss: 434.1052551269531 = 0.9606286287307739 + 50.0 * 8.66289234161377
Epoch 300, val loss: 0.9610058069229126
Epoch 310, training loss: 433.2186584472656 = 0.954951822757721 + 50.0 * 8.64527416229248
Epoch 310, val loss: 0.9554182887077332
Epoch 320, training loss: 432.2696533203125 = 0.9490972757339478 + 50.0 * 8.626411437988281
Epoch 320, val loss: 0.9497274160385132
Epoch 330, training loss: 431.5673522949219 = 0.9428982734680176 + 50.0 * 8.612488746643066
Epoch 330, val loss: 0.9437352418899536
Epoch 340, training loss: 430.7325134277344 = 0.9363186955451965 + 50.0 * 8.595924377441406
Epoch 340, val loss: 0.9372593760490417
Epoch 350, training loss: 429.8979187011719 = 0.9294325113296509 + 50.0 * 8.57936954498291
Epoch 350, val loss: 0.9305193424224854
Epoch 360, training loss: 429.4215087890625 = 0.9220719337463379 + 50.0 * 8.569989204406738
Epoch 360, val loss: 0.9233676195144653
Epoch 370, training loss: 428.5614929199219 = 0.9140458106994629 + 50.0 * 8.552948951721191
Epoch 370, val loss: 0.9154163599014282
Epoch 380, training loss: 427.96832275390625 = 0.905600905418396 + 50.0 * 8.541254043579102
Epoch 380, val loss: 0.9070987105369568
Epoch 390, training loss: 427.47503662109375 = 0.8967722654342651 + 50.0 * 8.53156566619873
Epoch 390, val loss: 0.8985009789466858
Epoch 400, training loss: 427.0794982910156 = 0.8876317143440247 + 50.0 * 8.523837089538574
Epoch 400, val loss: 0.8895846605300903
Epoch 410, training loss: 426.8353576660156 = 0.8782010674476624 + 50.0 * 8.519143104553223
Epoch 410, val loss: 0.8803790807723999
Epoch 420, training loss: 426.53466796875 = 0.8684640526771545 + 50.0 * 8.513323783874512
Epoch 420, val loss: 0.870943546295166
Epoch 430, training loss: 426.2319641113281 = 0.8586477637290955 + 50.0 * 8.507466316223145
Epoch 430, val loss: 0.8614472150802612
Epoch 440, training loss: 425.9903869628906 = 0.8488320112228394 + 50.0 * 8.502830505371094
Epoch 440, val loss: 0.8519231081008911
Epoch 450, training loss: 425.7474060058594 = 0.8389453887939453 + 50.0 * 8.4981689453125
Epoch 450, val loss: 0.842397153377533
Epoch 460, training loss: 425.7403564453125 = 0.8289505839347839 + 50.0 * 8.498228073120117
Epoch 460, val loss: 0.8328666090965271
Epoch 470, training loss: 425.398193359375 = 0.8189151287078857 + 50.0 * 8.491585731506348
Epoch 470, val loss: 0.8231388330459595
Epoch 480, training loss: 425.21978759765625 = 0.8089417219161987 + 50.0 * 8.4882173538208
Epoch 480, val loss: 0.8135470151901245
Epoch 490, training loss: 425.0303649902344 = 0.799045979976654 + 50.0 * 8.484626770019531
Epoch 490, val loss: 0.8040493130683899
Epoch 500, training loss: 424.8599548339844 = 0.7892290949821472 + 50.0 * 8.481414794921875
Epoch 500, val loss: 0.7946460843086243
Epoch 510, training loss: 424.6875915527344 = 0.7795187830924988 + 50.0 * 8.478161811828613
Epoch 510, val loss: 0.7853580117225647
Epoch 520, training loss: 424.52838134765625 = 0.7699648141860962 + 50.0 * 8.475168228149414
Epoch 520, val loss: 0.7762547731399536
Epoch 530, training loss: 424.3990173339844 = 0.760537326335907 + 50.0 * 8.472769737243652
Epoch 530, val loss: 0.7671583294868469
Epoch 540, training loss: 424.2119140625 = 0.7512399554252625 + 50.0 * 8.469213485717773
Epoch 540, val loss: 0.758347749710083
Epoch 550, training loss: 424.0311584472656 = 0.7421584129333496 + 50.0 * 8.465780258178711
Epoch 550, val loss: 0.7496227622032166
Epoch 560, training loss: 423.9496765136719 = 0.7331655621528625 + 50.0 * 8.464330673217773
Epoch 560, val loss: 0.741018533706665
Epoch 570, training loss: 423.7897033691406 = 0.7242483496665955 + 50.0 * 8.461309432983398
Epoch 570, val loss: 0.7325766086578369
Epoch 580, training loss: 423.6417541503906 = 0.7155691981315613 + 50.0 * 8.458523750305176
Epoch 580, val loss: 0.7243149876594543
Epoch 590, training loss: 423.50286865234375 = 0.7070760726928711 + 50.0 * 8.455916404724121
Epoch 590, val loss: 0.7162355184555054
Epoch 600, training loss: 423.68212890625 = 0.6988117098808289 + 50.0 * 8.45966625213623
Epoch 600, val loss: 0.7082508206367493
Epoch 610, training loss: 423.3168029785156 = 0.6905564665794373 + 50.0 * 8.45252513885498
Epoch 610, val loss: 0.7005266547203064
Epoch 620, training loss: 423.1347961425781 = 0.6826823949813843 + 50.0 * 8.449042320251465
Epoch 620, val loss: 0.6931490898132324
Epoch 630, training loss: 422.9793395996094 = 0.6750537753105164 + 50.0 * 8.446085929870605
Epoch 630, val loss: 0.6859776973724365
Epoch 640, training loss: 422.82464599609375 = 0.6676452159881592 + 50.0 * 8.443140029907227
Epoch 640, val loss: 0.679000735282898
Epoch 650, training loss: 422.89984130859375 = 0.6604348421096802 + 50.0 * 8.444787979125977
Epoch 650, val loss: 0.6721868515014648
Epoch 660, training loss: 422.5829162597656 = 0.6533389091491699 + 50.0 * 8.438591003417969
Epoch 660, val loss: 0.6655300259590149
Epoch 670, training loss: 422.4264831542969 = 0.6465113162994385 + 50.0 * 8.435599327087402
Epoch 670, val loss: 0.6591290235519409
Epoch 680, training loss: 422.2795104980469 = 0.6398683190345764 + 50.0 * 8.432792663574219
Epoch 680, val loss: 0.6528998017311096
Epoch 690, training loss: 422.5040588378906 = 0.6333946585655212 + 50.0 * 8.437413215637207
Epoch 690, val loss: 0.6467146873474121
Epoch 700, training loss: 422.130859375 = 0.6268908381462097 + 50.0 * 8.430079460144043
Epoch 700, val loss: 0.6407840251922607
Epoch 710, training loss: 421.9299621582031 = 0.6207784414291382 + 50.0 * 8.426183700561523
Epoch 710, val loss: 0.6350519061088562
Epoch 720, training loss: 421.8202819824219 = 0.6148208379745483 + 50.0 * 8.42410945892334
Epoch 720, val loss: 0.6295164227485657
Epoch 730, training loss: 421.6993713378906 = 0.6090521812438965 + 50.0 * 8.421806335449219
Epoch 730, val loss: 0.6241089701652527
Epoch 740, training loss: 421.6278991699219 = 0.6034871339797974 + 50.0 * 8.420488357543945
Epoch 740, val loss: 0.6188750267028809
Epoch 750, training loss: 421.5011901855469 = 0.5979840159416199 + 50.0 * 8.41806411743164
Epoch 750, val loss: 0.6138054132461548
Epoch 760, training loss: 421.45623779296875 = 0.5927153825759888 + 50.0 * 8.41727066040039
Epoch 760, val loss: 0.6088705062866211
Epoch 770, training loss: 421.3071594238281 = 0.5876267552375793 + 50.0 * 8.414390563964844
Epoch 770, val loss: 0.6042030453681946
Epoch 780, training loss: 421.2125244140625 = 0.5827450752258301 + 50.0 * 8.412595748901367
Epoch 780, val loss: 0.5997198820114136
Epoch 790, training loss: 421.13751220703125 = 0.5780572891235352 + 50.0 * 8.411189079284668
Epoch 790, val loss: 0.5954265594482422
Epoch 800, training loss: 421.065185546875 = 0.5734633207321167 + 50.0 * 8.409834861755371
Epoch 800, val loss: 0.5911924839019775
Epoch 810, training loss: 420.9610900878906 = 0.5690850615501404 + 50.0 * 8.40783977508545
Epoch 810, val loss: 0.5871784687042236
Epoch 820, training loss: 420.8685302734375 = 0.5648305416107178 + 50.0 * 8.406074523925781
Epoch 820, val loss: 0.5833795070648193
Epoch 830, training loss: 420.8612976074219 = 0.5607410669326782 + 50.0 * 8.406011581420898
Epoch 830, val loss: 0.5796812176704407
Epoch 840, training loss: 420.7216796875 = 0.5567765831947327 + 50.0 * 8.403298377990723
Epoch 840, val loss: 0.5760396122932434
Epoch 850, training loss: 420.6224365234375 = 0.5529872179031372 + 50.0 * 8.401389122009277
Epoch 850, val loss: 0.5726394653320312
Epoch 860, training loss: 420.8081359863281 = 0.5492783784866333 + 50.0 * 8.405177116394043
Epoch 860, val loss: 0.5693581700325012
Epoch 870, training loss: 420.470947265625 = 0.5457329750061035 + 50.0 * 8.398504257202148
Epoch 870, val loss: 0.5661124587059021
Epoch 880, training loss: 420.4193115234375 = 0.542365312576294 + 50.0 * 8.397539138793945
Epoch 880, val loss: 0.5631032586097717
Epoch 890, training loss: 420.3288269042969 = 0.5391098856925964 + 50.0 * 8.395793914794922
Epoch 890, val loss: 0.5602190494537354
Epoch 900, training loss: 420.2673645019531 = 0.5359589457511902 + 50.0 * 8.394628524780273
Epoch 900, val loss: 0.5574473738670349
Epoch 910, training loss: 420.36749267578125 = 0.5329262018203735 + 50.0 * 8.39669132232666
Epoch 910, val loss: 0.5547252297401428
Epoch 920, training loss: 420.2450866699219 = 0.5299013257026672 + 50.0 * 8.394303321838379
Epoch 920, val loss: 0.5521495342254639
Epoch 930, training loss: 420.14056396484375 = 0.5271049737930298 + 50.0 * 8.392269134521484
Epoch 930, val loss: 0.5496756434440613
Epoch 940, training loss: 420.0394287109375 = 0.5243964791297913 + 50.0 * 8.390300750732422
Epoch 940, val loss: 0.5473914742469788
Epoch 950, training loss: 419.96844482421875 = 0.5217771530151367 + 50.0 * 8.388933181762695
Epoch 950, val loss: 0.5451454520225525
Epoch 960, training loss: 419.9099426269531 = 0.5192646384239197 + 50.0 * 8.387813568115234
Epoch 960, val loss: 0.543013870716095
Epoch 970, training loss: 419.96527099609375 = 0.5168498158454895 + 50.0 * 8.388968467712402
Epoch 970, val loss: 0.5410017371177673
Epoch 980, training loss: 419.8773193359375 = 0.5144472718238831 + 50.0 * 8.38725757598877
Epoch 980, val loss: 0.5389384031295776
Epoch 990, training loss: 419.781494140625 = 0.5121871829032898 + 50.0 * 8.38538646697998
Epoch 990, val loss: 0.5371048450469971
Epoch 1000, training loss: 419.68463134765625 = 0.5100173950195312 + 50.0 * 8.383492469787598
Epoch 1000, val loss: 0.5353987812995911
Epoch 1010, training loss: 419.63336181640625 = 0.5079329609870911 + 50.0 * 8.382508277893066
Epoch 1010, val loss: 0.5337150692939758
Epoch 1020, training loss: 419.595703125 = 0.5059237480163574 + 50.0 * 8.381795883178711
Epoch 1020, val loss: 0.5321382284164429
Epoch 1030, training loss: 419.71240234375 = 0.5039610266685486 + 50.0 * 8.38416862487793
Epoch 1030, val loss: 0.5305340886116028
Epoch 1040, training loss: 419.725341796875 = 0.501943051815033 + 50.0 * 8.384468078613281
Epoch 1040, val loss: 0.5290680527687073
Epoch 1050, training loss: 419.509033203125 = 0.5001304149627686 + 50.0 * 8.380178451538086
Epoch 1050, val loss: 0.5276085734367371
Epoch 1060, training loss: 419.4192199707031 = 0.49840039014816284 + 50.0 * 8.378416061401367
Epoch 1060, val loss: 0.5263307690620422
Epoch 1070, training loss: 419.3546142578125 = 0.49670177698135376 + 50.0 * 8.377158164978027
Epoch 1070, val loss: 0.5250367522239685
Epoch 1080, training loss: 419.30877685546875 = 0.4950658679008484 + 50.0 * 8.376274108886719
Epoch 1080, val loss: 0.523818850517273
Epoch 1090, training loss: 419.2658386230469 = 0.49348172545433044 + 50.0 * 8.375447273254395
Epoch 1090, val loss: 0.5226892232894897
Epoch 1100, training loss: 419.22491455078125 = 0.4919370412826538 + 50.0 * 8.374659538269043
Epoch 1100, val loss: 0.5215907096862793
Epoch 1110, training loss: 419.2298278808594 = 0.49044790863990784 + 50.0 * 8.374787330627441
Epoch 1110, val loss: 0.5205587148666382
Epoch 1120, training loss: 419.26129150390625 = 0.48893535137176514 + 50.0 * 8.375447273254395
Epoch 1120, val loss: 0.5194002985954285
Epoch 1130, training loss: 419.13641357421875 = 0.4874759614467621 + 50.0 * 8.372978210449219
Epoch 1130, val loss: 0.5184035301208496
Epoch 1140, training loss: 419.0898742675781 = 0.4861280918121338 + 50.0 * 8.372075080871582
Epoch 1140, val loss: 0.5175180435180664
Epoch 1150, training loss: 419.0528564453125 = 0.48480263352394104 + 50.0 * 8.371360778808594
Epoch 1150, val loss: 0.5166721343994141
Epoch 1160, training loss: 419.0293884277344 = 0.48351168632507324 + 50.0 * 8.370917320251465
Epoch 1160, val loss: 0.515807569026947
Epoch 1170, training loss: 419.09832763671875 = 0.4822605550289154 + 50.0 * 8.372321128845215
Epoch 1170, val loss: 0.5149807333946228
Epoch 1180, training loss: 419.09625244140625 = 0.4810067415237427 + 50.0 * 8.372304916381836
Epoch 1180, val loss: 0.5142391920089722
Epoch 1190, training loss: 418.9632568359375 = 0.4797802269458771 + 50.0 * 8.369669914245605
Epoch 1190, val loss: 0.5134735107421875
Epoch 1200, training loss: 418.9071044921875 = 0.4786251187324524 + 50.0 * 8.368569374084473
Epoch 1200, val loss: 0.5127348303794861
Epoch 1210, training loss: 418.8847961425781 = 0.4774964153766632 + 50.0 * 8.368145942687988
Epoch 1210, val loss: 0.5120791792869568
Epoch 1220, training loss: 418.86712646484375 = 0.4763888716697693 + 50.0 * 8.367815017700195
Epoch 1220, val loss: 0.511394739151001
Epoch 1230, training loss: 418.9795837402344 = 0.4753021001815796 + 50.0 * 8.370085716247559
Epoch 1230, val loss: 0.5107753276824951
Epoch 1240, training loss: 418.8298645019531 = 0.47419893741607666 + 50.0 * 8.36711311340332
Epoch 1240, val loss: 0.5101204514503479
Epoch 1250, training loss: 418.7755126953125 = 0.4731743633747101 + 50.0 * 8.366046905517578
Epoch 1250, val loss: 0.5095596313476562
Epoch 1260, training loss: 418.74462890625 = 0.4721582233905792 + 50.0 * 8.365448951721191
Epoch 1260, val loss: 0.5090022683143616
Epoch 1270, training loss: 418.7252502441406 = 0.4711664617061615 + 50.0 * 8.365081787109375
Epoch 1270, val loss: 0.5084160566329956
Epoch 1280, training loss: 418.917236328125 = 0.47019386291503906 + 50.0 * 8.368941307067871
Epoch 1280, val loss: 0.5079647302627563
Epoch 1290, training loss: 418.7491455078125 = 0.46919310092926025 + 50.0 * 8.365598678588867
Epoch 1290, val loss: 0.5073716044425964
Epoch 1300, training loss: 418.66473388671875 = 0.46825268864631653 + 50.0 * 8.363929748535156
Epoch 1300, val loss: 0.5068054795265198
Epoch 1310, training loss: 418.63275146484375 = 0.4673430621623993 + 50.0 * 8.36330795288086
Epoch 1310, val loss: 0.5064319372177124
Epoch 1320, training loss: 418.8048095703125 = 0.46643713116645813 + 50.0 * 8.366767883300781
Epoch 1320, val loss: 0.5059133768081665
Epoch 1330, training loss: 418.62969970703125 = 0.4654998481273651 + 50.0 * 8.36328411102295
Epoch 1330, val loss: 0.5053793787956238
Epoch 1340, training loss: 418.56317138671875 = 0.4646376967430115 + 50.0 * 8.361970901489258
Epoch 1340, val loss: 0.5049932599067688
Epoch 1350, training loss: 418.5176086425781 = 0.46377840638160706 + 50.0 * 8.361076354980469
Epoch 1350, val loss: 0.5045549273490906
Epoch 1360, training loss: 418.4927978515625 = 0.46293210983276367 + 50.0 * 8.360597610473633
Epoch 1360, val loss: 0.5040878653526306
Epoch 1370, training loss: 418.5756530761719 = 0.4620997905731201 + 50.0 * 8.362271308898926
Epoch 1370, val loss: 0.5036025047302246
Epoch 1380, training loss: 418.4948425292969 = 0.4612348973751068 + 50.0 * 8.360671997070312
Epoch 1380, val loss: 0.5033944845199585
Epoch 1390, training loss: 418.4703063964844 = 0.4604124426841736 + 50.0 * 8.360198020935059
Epoch 1390, val loss: 0.502738356590271
Epoch 1400, training loss: 418.3886413574219 = 0.45963001251220703 + 50.0 * 8.358580589294434
Epoch 1400, val loss: 0.502515435218811
Epoch 1410, training loss: 418.36114501953125 = 0.4588448107242584 + 50.0 * 8.358046531677246
Epoch 1410, val loss: 0.5021096467971802
Epoch 1420, training loss: 418.3523864746094 = 0.4580729901790619 + 50.0 * 8.35788631439209
Epoch 1420, val loss: 0.501734733581543
Epoch 1430, training loss: 418.4271240234375 = 0.4572789967060089 + 50.0 * 8.359396934509277
Epoch 1430, val loss: 0.5014040470123291
Epoch 1440, training loss: 418.45489501953125 = 0.4564783275127411 + 50.0 * 8.359968185424805
Epoch 1440, val loss: 0.5009544491767883
Epoch 1450, training loss: 418.27459716796875 = 0.45570459961891174 + 50.0 * 8.356377601623535
Epoch 1450, val loss: 0.500590443611145
Epoch 1460, training loss: 418.24053955078125 = 0.45496436953544617 + 50.0 * 8.355711936950684
Epoch 1460, val loss: 0.500268280506134
Epoch 1470, training loss: 418.2173156738281 = 0.4542323648929596 + 50.0 * 8.35526180267334
Epoch 1470, val loss: 0.49991342425346375
Epoch 1480, training loss: 418.238037109375 = 0.45350173115730286 + 50.0 * 8.355690956115723
Epoch 1480, val loss: 0.4996255934238434
Epoch 1490, training loss: 418.26611328125 = 0.4527437686920166 + 50.0 * 8.356266975402832
Epoch 1490, val loss: 0.499301016330719
Epoch 1500, training loss: 418.19439697265625 = 0.45199719071388245 + 50.0 * 8.35484790802002
Epoch 1500, val loss: 0.49891507625579834
Epoch 1510, training loss: 418.13726806640625 = 0.4512792229652405 + 50.0 * 8.353719711303711
Epoch 1510, val loss: 0.49866124987602234
Epoch 1520, training loss: 418.1499938964844 = 0.45055750012397766 + 50.0 * 8.353988647460938
Epoch 1520, val loss: 0.49831974506378174
Epoch 1530, training loss: 418.1429138183594 = 0.44981423020362854 + 50.0 * 8.353861808776855
Epoch 1530, val loss: 0.49811047315597534
Epoch 1540, training loss: 418.0928955078125 = 0.4490830898284912 + 50.0 * 8.352875709533691
Epoch 1540, val loss: 0.4975706934928894
Epoch 1550, training loss: 418.0459899902344 = 0.44837161898612976 + 50.0 * 8.35195255279541
Epoch 1550, val loss: 0.49740684032440186
Epoch 1560, training loss: 418.0190124511719 = 0.44766271114349365 + 50.0 * 8.35142707824707
Epoch 1560, val loss: 0.4970620274543762
Epoch 1570, training loss: 417.99908447265625 = 0.44696271419525146 + 50.0 * 8.351042747497559
Epoch 1570, val loss: 0.49676513671875
Epoch 1580, training loss: 418.2319030761719 = 0.44625574350357056 + 50.0 * 8.355712890625
Epoch 1580, val loss: 0.49650007486343384
Epoch 1590, training loss: 418.1802978515625 = 0.44548678398132324 + 50.0 * 8.354696273803711
Epoch 1590, val loss: 0.49595609307289124
Epoch 1600, training loss: 417.9485168457031 = 0.44478103518486023 + 50.0 * 8.350074768066406
Epoch 1600, val loss: 0.49567854404449463
Epoch 1610, training loss: 417.9477233886719 = 0.44410544633865356 + 50.0 * 8.350072860717773
Epoch 1610, val loss: 0.495522141456604
Epoch 1620, training loss: 417.9095458984375 = 0.4434279203414917 + 50.0 * 8.349322319030762
Epoch 1620, val loss: 0.4951545298099518
Epoch 1630, training loss: 417.8921813964844 = 0.4427543580532074 + 50.0 * 8.34898853302002
Epoch 1630, val loss: 0.4948801100254059
Epoch 1640, training loss: 417.8713684082031 = 0.44208189845085144 + 50.0 * 8.348586082458496
Epoch 1640, val loss: 0.4945930242538452
Epoch 1650, training loss: 417.856201171875 = 0.4414103329181671 + 50.0 * 8.348296165466309
Epoch 1650, val loss: 0.4943249523639679
Epoch 1660, training loss: 417.99444580078125 = 0.4407447874546051 + 50.0 * 8.35107421875
Epoch 1660, val loss: 0.494157612323761
Epoch 1670, training loss: 417.9999084472656 = 0.44002285599708557 + 50.0 * 8.351197242736816
Epoch 1670, val loss: 0.49366751313209534
Epoch 1680, training loss: 417.8468933105469 = 0.43932250142097473 + 50.0 * 8.348151206970215
Epoch 1680, val loss: 0.4933321475982666
Epoch 1690, training loss: 417.7958068847656 = 0.4386654496192932 + 50.0 * 8.347143173217773
Epoch 1690, val loss: 0.4931224286556244
Epoch 1700, training loss: 417.7889404296875 = 0.4380078613758087 + 50.0 * 8.347018241882324
Epoch 1700, val loss: 0.4927777945995331
Epoch 1710, training loss: 417.7646179199219 = 0.43735218048095703 + 50.0 * 8.346545219421387
Epoch 1710, val loss: 0.4924892485141754
Epoch 1720, training loss: 417.7607727050781 = 0.43670088052749634 + 50.0 * 8.346481323242188
Epoch 1720, val loss: 0.49220913648605347
Epoch 1730, training loss: 418.0078125 = 0.43603965640068054 + 50.0 * 8.351435661315918
Epoch 1730, val loss: 0.4919569492340088
Epoch 1740, training loss: 417.81890869140625 = 0.4353277385234833 + 50.0 * 8.347671508789062
Epoch 1740, val loss: 0.49146193265914917
Epoch 1750, training loss: 417.7318420410156 = 0.43467211723327637 + 50.0 * 8.345943450927734
Epoch 1750, val loss: 0.49124735593795776
Epoch 1760, training loss: 417.6968688964844 = 0.43402135372161865 + 50.0 * 8.345256805419922
Epoch 1760, val loss: 0.49093592166900635
Epoch 1770, training loss: 417.68853759765625 = 0.43337133526802063 + 50.0 * 8.34510326385498
Epoch 1770, val loss: 0.4906255900859833
Epoch 1780, training loss: 417.8488464355469 = 0.43271803855895996 + 50.0 * 8.348322868347168
Epoch 1780, val loss: 0.4904561936855316
Epoch 1790, training loss: 417.7115783691406 = 0.4320332407951355 + 50.0 * 8.345590591430664
Epoch 1790, val loss: 0.48984232544898987
Epoch 1800, training loss: 417.66595458984375 = 0.43138670921325684 + 50.0 * 8.344691276550293
Epoch 1800, val loss: 0.4897368252277374
Epoch 1810, training loss: 417.63690185546875 = 0.4307387173175812 + 50.0 * 8.344123840332031
Epoch 1810, val loss: 0.4893105924129486
Epoch 1820, training loss: 417.62237548828125 = 0.43010151386260986 + 50.0 * 8.34384536743164
Epoch 1820, val loss: 0.4890851378440857
Epoch 1830, training loss: 417.97198486328125 = 0.42945143580436707 + 50.0 * 8.350851058959961
Epoch 1830, val loss: 0.48881199955940247
Epoch 1840, training loss: 417.7772216796875 = 0.42874667048454285 + 50.0 * 8.346969604492188
Epoch 1840, val loss: 0.48830702900886536
Epoch 1850, training loss: 417.583984375 = 0.4280949532985687 + 50.0 * 8.343117713928223
Epoch 1850, val loss: 0.48804351687431335
Epoch 1860, training loss: 417.5864562988281 = 0.42746713757514954 + 50.0 * 8.343179702758789
Epoch 1860, val loss: 0.48783013224601746
Epoch 1870, training loss: 417.5549621582031 = 0.42683145403862 + 50.0 * 8.342562675476074
Epoch 1870, val loss: 0.4874788224697113
Epoch 1880, training loss: 417.5414123535156 = 0.42619791626930237 + 50.0 * 8.342304229736328
Epoch 1880, val loss: 0.48720619082450867
Epoch 1890, training loss: 417.5292053222656 = 0.4255577325820923 + 50.0 * 8.342072486877441
Epoch 1890, val loss: 0.4869084656238556
Epoch 1900, training loss: 417.5166320800781 = 0.4249128997325897 + 50.0 * 8.34183406829834
Epoch 1900, val loss: 0.48662078380584717
Epoch 1910, training loss: 417.507568359375 = 0.42426374554634094 + 50.0 * 8.341666221618652
Epoch 1910, val loss: 0.4862945079803467
Epoch 1920, training loss: 417.81201171875 = 0.4236134886741638 + 50.0 * 8.34776782989502
Epoch 1920, val loss: 0.48578235507011414
Epoch 1930, training loss: 417.6087951660156 = 0.42290785908699036 + 50.0 * 8.343717575073242
Epoch 1930, val loss: 0.48570817708969116
Epoch 1940, training loss: 417.5156555175781 = 0.42224058508872986 + 50.0 * 8.34186840057373
Epoch 1940, val loss: 0.4854373037815094
Epoch 1950, training loss: 417.4606628417969 = 0.4216044843196869 + 50.0 * 8.340781211853027
Epoch 1950, val loss: 0.48506760597229004
Epoch 1960, training loss: 417.45330810546875 = 0.42097118496894836 + 50.0 * 8.340646743774414
Epoch 1960, val loss: 0.48473113775253296
Epoch 1970, training loss: 417.4591369628906 = 0.4203345477581024 + 50.0 * 8.340775489807129
Epoch 1970, val loss: 0.48441290855407715
Epoch 1980, training loss: 417.610595703125 = 0.4196784794330597 + 50.0 * 8.343818664550781
Epoch 1980, val loss: 0.48411306738853455
Epoch 1990, training loss: 417.47637939453125 = 0.4190041422843933 + 50.0 * 8.341147422790527
Epoch 1990, val loss: 0.48383477330207825
Epoch 2000, training loss: 417.4285583496094 = 0.41834670305252075 + 50.0 * 8.340204238891602
Epoch 2000, val loss: 0.483498215675354
Epoch 2010, training loss: 417.4045104980469 = 0.4177123010158539 + 50.0 * 8.339735984802246
Epoch 2010, val loss: 0.4831772446632385
Epoch 2020, training loss: 417.3806457519531 = 0.4170736074447632 + 50.0 * 8.339271545410156
Epoch 2020, val loss: 0.4828892946243286
Epoch 2030, training loss: 417.369140625 = 0.41643792390823364 + 50.0 * 8.339054107666016
Epoch 2030, val loss: 0.4826085865497589
Epoch 2040, training loss: 417.3647766113281 = 0.415798544883728 + 50.0 * 8.338979721069336
Epoch 2040, val loss: 0.4823145568370819
Epoch 2050, training loss: 417.8663330078125 = 0.4151458144187927 + 50.0 * 8.349023818969727
Epoch 2050, val loss: 0.482065886259079
Epoch 2060, training loss: 417.489501953125 = 0.41443371772766113 + 50.0 * 8.341501235961914
Epoch 2060, val loss: 0.48155471682548523
Epoch 2070, training loss: 417.3429260253906 = 0.4137748181819916 + 50.0 * 8.338582992553711
Epoch 2070, val loss: 0.4813089668750763
Epoch 2080, training loss: 417.33355712890625 = 0.41313448548316956 + 50.0 * 8.338408470153809
Epoch 2080, val loss: 0.4809902608394623
Epoch 2090, training loss: 417.30670166015625 = 0.41250064969062805 + 50.0 * 8.337883949279785
Epoch 2090, val loss: 0.48072415590286255
Epoch 2100, training loss: 417.3569030761719 = 0.4118611812591553 + 50.0 * 8.338900566101074
Epoch 2100, val loss: 0.4803726077079773
Epoch 2110, training loss: 417.2947082519531 = 0.41119804978370667 + 50.0 * 8.33767032623291
Epoch 2110, val loss: 0.48007068037986755
Epoch 2120, training loss: 417.2771911621094 = 0.4105420708656311 + 50.0 * 8.337332725524902
Epoch 2120, val loss: 0.4798201024532318
Epoch 2130, training loss: 417.2623596191406 = 0.40989214181900024 + 50.0 * 8.33704948425293
Epoch 2130, val loss: 0.47947800159454346
Epoch 2140, training loss: 417.2520751953125 = 0.40924566984176636 + 50.0 * 8.336856842041016
Epoch 2140, val loss: 0.4791730046272278
Epoch 2150, training loss: 417.2464904785156 = 0.40860092639923096 + 50.0 * 8.33675765991211
Epoch 2150, val loss: 0.4788569509983063
Epoch 2160, training loss: 417.3442687988281 = 0.4079483449459076 + 50.0 * 8.338726043701172
Epoch 2160, val loss: 0.4784322679042816
Epoch 2170, training loss: 417.2381896972656 = 0.40726980566978455 + 50.0 * 8.336618423461914
Epoch 2170, val loss: 0.47831299901008606
Epoch 2180, training loss: 417.2159423828125 = 0.40660810470581055 + 50.0 * 8.336186408996582
Epoch 2180, val loss: 0.4779145121574402
Epoch 2190, training loss: 417.20257568359375 = 0.40595924854278564 + 50.0 * 8.335932731628418
Epoch 2190, val loss: 0.4777086675167084
Epoch 2200, training loss: 417.1979675292969 = 0.4053070843219757 + 50.0 * 8.335853576660156
Epoch 2200, val loss: 0.47738030552864075
Epoch 2210, training loss: 417.2842102050781 = 0.40465062856674194 + 50.0 * 8.337591171264648
Epoch 2210, val loss: 0.4770650267601013
Epoch 2220, training loss: 417.2140808105469 = 0.40396004915237427 + 50.0 * 8.336202621459961
Epoch 2220, val loss: 0.4767593741416931
Epoch 2230, training loss: 417.177978515625 = 0.403283029794693 + 50.0 * 8.335494041442871
Epoch 2230, val loss: 0.47646135091781616
Epoch 2240, training loss: 417.170654296875 = 0.4026143252849579 + 50.0 * 8.335360527038574
Epoch 2240, val loss: 0.476158082485199
Epoch 2250, training loss: 417.20703125 = 0.4019430875778198 + 50.0 * 8.336101531982422
Epoch 2250, val loss: 0.4758908748626709
Epoch 2260, training loss: 417.1629943847656 = 0.4012627899646759 + 50.0 * 8.335234642028809
Epoch 2260, val loss: 0.4755830466747284
Epoch 2270, training loss: 417.12762451171875 = 0.40058568120002747 + 50.0 * 8.334541320800781
Epoch 2270, val loss: 0.47526806592941284
Epoch 2280, training loss: 417.1203308105469 = 0.3999127149581909 + 50.0 * 8.3344087600708
Epoch 2280, val loss: 0.47495904564857483
Epoch 2290, training loss: 417.197021484375 = 0.3992345929145813 + 50.0 * 8.335955619812012
Epoch 2290, val loss: 0.47462302446365356
Epoch 2300, training loss: 417.15338134765625 = 0.3985304832458496 + 50.0 * 8.335097312927246
Epoch 2300, val loss: 0.47444474697113037
Epoch 2310, training loss: 417.0966796875 = 0.3978324234485626 + 50.0 * 8.333976745605469
Epoch 2310, val loss: 0.4741211235523224
Epoch 2320, training loss: 417.0746765136719 = 0.39715278148651123 + 50.0 * 8.333550453186035
Epoch 2320, val loss: 0.4737958014011383
Epoch 2330, training loss: 417.0617980957031 = 0.39648106694221497 + 50.0 * 8.333306312561035
Epoch 2330, val loss: 0.47353440523147583
Epoch 2340, training loss: 417.06182861328125 = 0.3958083987236023 + 50.0 * 8.333320617675781
Epoch 2340, val loss: 0.4732789993286133
Epoch 2350, training loss: 417.1967468261719 = 0.3951321244239807 + 50.0 * 8.336031913757324
Epoch 2350, val loss: 0.47317469120025635
Epoch 2360, training loss: 417.2051696777344 = 0.39442017674446106 + 50.0 * 8.336215019226074
Epoch 2360, val loss: 0.47275310754776
Epoch 2370, training loss: 417.06842041015625 = 0.393717497587204 + 50.0 * 8.333494186401367
Epoch 2370, val loss: 0.472423255443573
Epoch 2380, training loss: 417.01611328125 = 0.3930395245552063 + 50.0 * 8.3324613571167
Epoch 2380, val loss: 0.4722467064857483
Epoch 2390, training loss: 417.0052795410156 = 0.3923644423484802 + 50.0 * 8.332258224487305
Epoch 2390, val loss: 0.4719880521297455
Epoch 2400, training loss: 417.021240234375 = 0.3916853070259094 + 50.0 * 8.33259105682373
Epoch 2400, val loss: 0.47172874212265015
Epoch 2410, training loss: 417.1576843261719 = 0.39099329710006714 + 50.0 * 8.335333824157715
Epoch 2410, val loss: 0.4714721143245697
Epoch 2420, training loss: 417.0052490234375 = 0.3902885913848877 + 50.0 * 8.33229923248291
Epoch 2420, val loss: 0.4711063504219055
Epoch 2430, training loss: 416.96124267578125 = 0.3896012306213379 + 50.0 * 8.331433296203613
Epoch 2430, val loss: 0.47090810537338257
Epoch 2440, training loss: 416.9661560058594 = 0.3889194130897522 + 50.0 * 8.331544876098633
Epoch 2440, val loss: 0.4707057774066925
Epoch 2450, training loss: 417.1383972167969 = 0.38824236392974854 + 50.0 * 8.335002899169922
Epoch 2450, val loss: 0.47060421109199524
Epoch 2460, training loss: 417.0020446777344 = 0.3875099718570709 + 50.0 * 8.332290649414062
Epoch 2460, val loss: 0.4698340892791748
Epoch 2470, training loss: 416.9687194824219 = 0.3868189752101898 + 50.0 * 8.33163833618164
Epoch 2470, val loss: 0.46993812918663025
Epoch 2480, training loss: 416.9527587890625 = 0.3861260712146759 + 50.0 * 8.331332206726074
Epoch 2480, val loss: 0.4694605767726898
Epoch 2490, training loss: 417.0023498535156 = 0.38543546199798584 + 50.0 * 8.332338333129883
Epoch 2490, val loss: 0.4693034589290619
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8092338914256723
0.8633630370209375
The final CL Acc:0.82276, 0.00960, The final GNN Acc:0.86341, 0.00042
Begin epxeriment: noisy_level: 0.2 cont_weight: 50 epoch: der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106266])
remove edge: torch.Size([2, 70610])
updated graph: torch.Size([2, 88228])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 530.2364501953125 = 1.1202619075775146 + 50.0 * 10.582324028015137
Epoch 0, val loss: 1.1186168193817139
Epoch 10, training loss: 530.2198486328125 = 1.1145398616790771 + 50.0 * 10.58210563659668
Epoch 10, val loss: 1.113004207611084
Epoch 20, training loss: 530.1682739257812 = 1.1084442138671875 + 50.0 * 10.581196784973145
Epoch 20, val loss: 1.1070317029953003
Epoch 30, training loss: 529.9571533203125 = 1.101807713508606 + 50.0 * 10.577106475830078
Epoch 30, val loss: 1.1005586385726929
Epoch 40, training loss: 529.079345703125 = 1.0945227146148682 + 50.0 * 10.559696197509766
Epoch 40, val loss: 1.0935087203979492
Epoch 50, training loss: 526.170166015625 = 1.0865428447723389 + 50.0 * 10.50167179107666
Epoch 50, val loss: 1.08586847782135
Epoch 60, training loss: 518.9017333984375 = 1.0787030458450317 + 50.0 * 10.356461524963379
Epoch 60, val loss: 1.0784844160079956
Epoch 70, training loss: 505.2373352050781 = 1.0712698698043823 + 50.0 * 10.083321571350098
Epoch 70, val loss: 1.0714359283447266
Epoch 80, training loss: 489.36968994140625 = 1.0643061399459839 + 50.0 * 9.766107559204102
Epoch 80, val loss: 1.0648128986358643
Epoch 90, training loss: 476.5853576660156 = 1.0585914850234985 + 50.0 * 9.51053524017334
Epoch 90, val loss: 1.0591635704040527
Epoch 100, training loss: 467.7575988769531 = 1.0535428524017334 + 50.0 * 9.334081649780273
Epoch 100, val loss: 1.0540491342544556
Epoch 110, training loss: 464.3172302246094 = 1.0490257740020752 + 50.0 * 9.265364646911621
Epoch 110, val loss: 1.0495967864990234
Epoch 120, training loss: 462.6872253417969 = 1.0454541444778442 + 50.0 * 9.23283576965332
Epoch 120, val loss: 1.046157717704773
Epoch 130, training loss: 460.85711669921875 = 1.0426554679870605 + 50.0 * 9.1962890625
Epoch 130, val loss: 1.0434390306472778
Epoch 140, training loss: 458.6850280761719 = 1.0398833751678467 + 50.0 * 9.152902603149414
Epoch 140, val loss: 1.040765404701233
Epoch 150, training loss: 455.3279113769531 = 1.0378187894821167 + 50.0 * 9.08580207824707
Epoch 150, val loss: 1.0387643575668335
Epoch 160, training loss: 450.1965637207031 = 1.0367536544799805 + 50.0 * 8.983196258544922
Epoch 160, val loss: 1.0376901626586914
Epoch 170, training loss: 445.9860534667969 = 1.0360138416290283 + 50.0 * 8.899001121520996
Epoch 170, val loss: 1.0368657112121582
Epoch 180, training loss: 443.93682861328125 = 1.0345351696014404 + 50.0 * 8.85804557800293
Epoch 180, val loss: 1.0353585481643677
Epoch 190, training loss: 442.53076171875 = 1.0322859287261963 + 50.0 * 8.82996940612793
Epoch 190, val loss: 1.0331658124923706
Epoch 200, training loss: 441.3174743652344 = 1.0296131372451782 + 50.0 * 8.805757522583008
Epoch 200, val loss: 1.0305814743041992
Epoch 210, training loss: 440.3267517089844 = 1.026832938194275 + 50.0 * 8.785998344421387
Epoch 210, val loss: 1.0278743505477905
Epoch 220, training loss: 439.50750732421875 = 1.0239051580429077 + 50.0 * 8.769672393798828
Epoch 220, val loss: 1.024999737739563
Epoch 230, training loss: 438.78546142578125 = 1.0208088159561157 + 50.0 * 8.755292892456055
Epoch 230, val loss: 1.021952748298645
Epoch 240, training loss: 438.0394287109375 = 1.0175104141235352 + 50.0 * 8.740438461303711
Epoch 240, val loss: 1.0187159776687622
Epoch 250, training loss: 437.2135009765625 = 1.014075517654419 + 50.0 * 8.72398853302002
Epoch 250, val loss: 1.0153716802597046
Epoch 260, training loss: 436.42327880859375 = 1.0105211734771729 + 50.0 * 8.70825481414795
Epoch 260, val loss: 1.0119062662124634
Epoch 270, training loss: 435.6138000488281 = 1.0067859888076782 + 50.0 * 8.692140579223633
Epoch 270, val loss: 1.0082734823226929
Epoch 280, training loss: 434.8697814941406 = 1.002807855606079 + 50.0 * 8.677339553833008
Epoch 280, val loss: 1.004403829574585
Epoch 290, training loss: 434.31109619140625 = 0.9985255599021912 + 50.0 * 8.666251182556152
Epoch 290, val loss: 1.0002341270446777
Epoch 300, training loss: 433.6183166503906 = 0.9938760995864868 + 50.0 * 8.652488708496094
Epoch 300, val loss: 0.9957528114318848
Epoch 310, training loss: 433.10772705078125 = 0.9888845682144165 + 50.0 * 8.642376899719238
Epoch 310, val loss: 0.9909351468086243
Epoch 320, training loss: 432.7079772949219 = 0.9835782051086426 + 50.0 * 8.634488105773926
Epoch 320, val loss: 0.9858301281929016
Epoch 330, training loss: 432.32550048828125 = 0.9779400825500488 + 50.0 * 8.626951217651367
Epoch 330, val loss: 0.9804111123085022
Epoch 340, training loss: 431.9190368652344 = 0.9719951152801514 + 50.0 * 8.618941307067871
Epoch 340, val loss: 0.9747071266174316
Epoch 350, training loss: 431.554931640625 = 0.965785801410675 + 50.0 * 8.611783027648926
Epoch 350, val loss: 0.9687684774398804
Epoch 360, training loss: 431.2117919921875 = 0.9592955112457275 + 50.0 * 8.605050086975098
Epoch 360, val loss: 0.9625862836837769
Epoch 370, training loss: 430.9250183105469 = 0.9524549245834351 + 50.0 * 8.599451065063477
Epoch 370, val loss: 0.956051766872406
Epoch 380, training loss: 430.4852600097656 = 0.9453935623168945 + 50.0 * 8.590797424316406
Epoch 380, val loss: 0.9493404626846313
Epoch 390, training loss: 430.0623474121094 = 0.9381500482559204 + 50.0 * 8.582484245300293
Epoch 390, val loss: 0.9424675107002258
Epoch 400, training loss: 429.7093505859375 = 0.9306650757789612 + 50.0 * 8.575573921203613
Epoch 400, val loss: 0.9353718161582947
Epoch 410, training loss: 429.3707275390625 = 0.9229517579078674 + 50.0 * 8.568955421447754
Epoch 410, val loss: 0.9280780553817749
Epoch 420, training loss: 429.0610046386719 = 0.9150638580322266 + 50.0 * 8.562918663024902
Epoch 420, val loss: 0.9206168055534363
Epoch 430, training loss: 428.9090270996094 = 0.9069298505783081 + 50.0 * 8.560042381286621
Epoch 430, val loss: 0.9129494428634644
Epoch 440, training loss: 428.5390625 = 0.8986336588859558 + 50.0 * 8.55280876159668
Epoch 440, val loss: 0.9051112532615662
Epoch 450, training loss: 428.276611328125 = 0.890322744846344 + 50.0 * 8.547725677490234
Epoch 450, val loss: 0.8972761034965515
Epoch 460, training loss: 427.99609375 = 0.8819332718849182 + 50.0 * 8.542283058166504
Epoch 460, val loss: 0.8894021511077881
Epoch 470, training loss: 427.8700256347656 = 0.8734868168830872 + 50.0 * 8.539931297302246
Epoch 470, val loss: 0.8814612030982971
Epoch 480, training loss: 427.65521240234375 = 0.8648818135261536 + 50.0 * 8.535806655883789
Epoch 480, val loss: 0.8733957409858704
Epoch 490, training loss: 427.3359375 = 0.8563377261161804 + 50.0 * 8.529592514038086
Epoch 490, val loss: 0.8653922080993652
Epoch 500, training loss: 427.10614013671875 = 0.8478649854660034 + 50.0 * 8.525165557861328
Epoch 500, val loss: 0.8574667572975159
Epoch 510, training loss: 426.8999938964844 = 0.8394273519515991 + 50.0 * 8.521211624145508
Epoch 510, val loss: 0.8495808839797974
Epoch 520, training loss: 426.7496337890625 = 0.830989420413971 + 50.0 * 8.518372535705566
Epoch 520, val loss: 0.841688334941864
Epoch 530, training loss: 426.74591064453125 = 0.8224647641181946 + 50.0 * 8.518468856811523
Epoch 530, val loss: 0.833757221698761
Epoch 540, training loss: 426.4420471191406 = 0.8139596581459045 + 50.0 * 8.512561798095703
Epoch 540, val loss: 0.8258224725723267
Epoch 550, training loss: 426.2706298828125 = 0.8055889010429382 + 50.0 * 8.509300231933594
Epoch 550, val loss: 0.8180291652679443
Epoch 560, training loss: 426.1204528808594 = 0.7973358035087585 + 50.0 * 8.506462097167969
Epoch 560, val loss: 0.8103689551353455
Epoch 570, training loss: 426.0207214355469 = 0.7892079949378967 + 50.0 * 8.504630088806152
Epoch 570, val loss: 0.8028363585472107
Epoch 580, training loss: 425.9953918457031 = 0.7811927199363708 + 50.0 * 8.504283905029297
Epoch 580, val loss: 0.7954195737838745
Epoch 590, training loss: 425.812255859375 = 0.7733312249183655 + 50.0 * 8.500778198242188
Epoch 590, val loss: 0.788176417350769
Epoch 600, training loss: 425.69488525390625 = 0.7656714916229248 + 50.0 * 8.498583793640137
Epoch 600, val loss: 0.7811341881752014
Epoch 610, training loss: 425.6138916015625 = 0.7581847310066223 + 50.0 * 8.497114181518555
Epoch 610, val loss: 0.774267315864563
Epoch 620, training loss: 425.5172119140625 = 0.7508581280708313 + 50.0 * 8.49532699584961
Epoch 620, val loss: 0.7675753831863403
Epoch 630, training loss: 425.4241027832031 = 0.7437792420387268 + 50.0 * 8.493606567382812
Epoch 630, val loss: 0.7611192464828491
Epoch 640, training loss: 425.3326721191406 = 0.7369649410247803 + 50.0 * 8.491913795471191
Epoch 640, val loss: 0.7549586296081543
Epoch 650, training loss: 425.2575378417969 = 0.7304031848907471 + 50.0 * 8.4905424118042
Epoch 650, val loss: 0.7490403652191162
Epoch 660, training loss: 425.3806457519531 = 0.7240715622901917 + 50.0 * 8.493131637573242
Epoch 660, val loss: 0.7433499693870544
Epoch 670, training loss: 425.1514587402344 = 0.7179795503616333 + 50.0 * 8.488669395446777
Epoch 670, val loss: 0.7379422783851624
Epoch 680, training loss: 425.0324401855469 = 0.7122058868408203 + 50.0 * 8.486404418945312
Epoch 680, val loss: 0.7328252196311951
Epoch 690, training loss: 424.95208740234375 = 0.7066958546638489 + 50.0 * 8.484908103942871
Epoch 690, val loss: 0.7279898524284363
Epoch 700, training loss: 424.9822082519531 = 0.7014141082763672 + 50.0 * 8.485615730285645
Epoch 700, val loss: 0.7233949303627014
Epoch 710, training loss: 424.8713684082031 = 0.6963534951210022 + 50.0 * 8.483500480651855
Epoch 710, val loss: 0.7190101146697998
Epoch 720, training loss: 424.7662658691406 = 0.691565752029419 + 50.0 * 8.481493949890137
Epoch 720, val loss: 0.7148969769477844
Epoch 730, training loss: 424.6863708496094 = 0.6870181560516357 + 50.0 * 8.479987144470215
Epoch 730, val loss: 0.7110574841499329
Epoch 740, training loss: 424.5967102050781 = 0.6827100515365601 + 50.0 * 8.478280067443848
Epoch 740, val loss: 0.7074349522590637
Epoch 750, training loss: 424.5475769042969 = 0.6786164045333862 + 50.0 * 8.477378845214844
Epoch 750, val loss: 0.7040382623672485
Epoch 760, training loss: 424.9238586425781 = 0.6746845841407776 + 50.0 * 8.484983444213867
Epoch 760, val loss: 0.700813353061676
Epoch 770, training loss: 424.5099792480469 = 0.6708353757858276 + 50.0 * 8.47678279876709
Epoch 770, val loss: 0.6976382732391357
Epoch 780, training loss: 424.3840637207031 = 0.6672571897506714 + 50.0 * 8.474335670471191
Epoch 780, val loss: 0.6947608590126038
Epoch 790, training loss: 424.3173828125 = 0.6638862490653992 + 50.0 * 8.47307014465332
Epoch 790, val loss: 0.6920989155769348
Epoch 800, training loss: 424.244384765625 = 0.6606770157814026 + 50.0 * 8.471673965454102
Epoch 800, val loss: 0.6895821690559387
Epoch 810, training loss: 424.1902160644531 = 0.6576197147369385 + 50.0 * 8.470651626586914
Epoch 810, val loss: 0.687216579914093
Epoch 820, training loss: 424.1554260253906 = 0.6546933054924011 + 50.0 * 8.470014572143555
Epoch 820, val loss: 0.6849996447563171
Epoch 830, training loss: 424.1986389160156 = 0.6518499255180359 + 50.0 * 8.470935821533203
Epoch 830, val loss: 0.6828404068946838
Epoch 840, training loss: 424.0895690917969 = 0.6491034626960754 + 50.0 * 8.468809127807617
Epoch 840, val loss: 0.6807695627212524
Epoch 850, training loss: 423.9906311035156 = 0.6465367674827576 + 50.0 * 8.46688175201416
Epoch 850, val loss: 0.6788959503173828
Epoch 860, training loss: 423.941650390625 = 0.6440902352333069 + 50.0 * 8.465950965881348
Epoch 860, val loss: 0.6771417856216431
Epoch 870, training loss: 423.9725646972656 = 0.6417324542999268 + 50.0 * 8.4666166305542
Epoch 870, val loss: 0.6754769682884216
Epoch 880, training loss: 423.84100341796875 = 0.6394352912902832 + 50.0 * 8.464031219482422
Epoch 880, val loss: 0.6738389134407043
Epoch 890, training loss: 423.7859802246094 = 0.6372485160827637 + 50.0 * 8.462974548339844
Epoch 890, val loss: 0.6723210215568542
Epoch 900, training loss: 423.7450256347656 = 0.6351616382598877 + 50.0 * 8.462197303771973
Epoch 900, val loss: 0.6709173321723938
Epoch 910, training loss: 423.6645202636719 = 0.633147120475769 + 50.0 * 8.460627555847168
Epoch 910, val loss: 0.6695688962936401
Epoch 920, training loss: 423.6929931640625 = 0.6312034130096436 + 50.0 * 8.461236000061035
Epoch 920, val loss: 0.6682758927345276
Epoch 930, training loss: 423.6116943359375 = 0.6292564272880554 + 50.0 * 8.459649085998535
Epoch 930, val loss: 0.6670024394989014
Epoch 940, training loss: 423.5173645019531 = 0.6273781061172485 + 50.0 * 8.457799911499023
Epoch 940, val loss: 0.6657889485359192
Epoch 950, training loss: 423.4684143066406 = 0.6255853176116943 + 50.0 * 8.456856727600098
Epoch 950, val loss: 0.664661705493927
Epoch 960, training loss: 423.4912414550781 = 0.6238481998443604 + 50.0 * 8.457347869873047
Epoch 960, val loss: 0.6635841131210327
Epoch 970, training loss: 423.3671569824219 = 0.6221460700035095 + 50.0 * 8.454900741577148
Epoch 970, val loss: 0.6625636219978333
Epoch 980, training loss: 423.3332214355469 = 0.6205054521560669 + 50.0 * 8.454254150390625
Epoch 980, val loss: 0.6615898013114929
Epoch 990, training loss: 423.2708740234375 = 0.6189250946044922 + 50.0 * 8.453039169311523
Epoch 990, val loss: 0.6606456637382507
Epoch 1000, training loss: 423.5920104980469 = 0.6173676252365112 + 50.0 * 8.459492683410645
Epoch 1000, val loss: 0.6597071886062622
Epoch 1010, training loss: 423.28125 = 0.6157584190368652 + 50.0 * 8.453310012817383
Epoch 1010, val loss: 0.6587890982627869
Epoch 1020, training loss: 423.1908264160156 = 0.6142548322677612 + 50.0 * 8.451531410217285
Epoch 1020, val loss: 0.6578848958015442
Epoch 1030, training loss: 423.10443115234375 = 0.6128218173980713 + 50.0 * 8.44983196258545
Epoch 1030, val loss: 0.6571033000946045
Epoch 1040, training loss: 423.03607177734375 = 0.6114341020584106 + 50.0 * 8.448493003845215
Epoch 1040, val loss: 0.6563423871994019
Epoch 1050, training loss: 422.9804992675781 = 0.6100742816925049 + 50.0 * 8.447408676147461
Epoch 1050, val loss: 0.6556090116500854
Epoch 1060, training loss: 422.9422607421875 = 0.6087321639060974 + 50.0 * 8.446670532226562
Epoch 1060, val loss: 0.6548891067504883
Epoch 1070, training loss: 423.03900146484375 = 0.6074051856994629 + 50.0 * 8.44863224029541
Epoch 1070, val loss: 0.6541804671287537
Epoch 1080, training loss: 423.0441589355469 = 0.6060057878494263 + 50.0 * 8.448762893676758
Epoch 1080, val loss: 0.6534125804901123
Epoch 1090, training loss: 422.8255615234375 = 0.6046632528305054 + 50.0 * 8.444417953491211
Epoch 1090, val loss: 0.6526490449905396
Epoch 1100, training loss: 422.8069763183594 = 0.6033971309661865 + 50.0 * 8.444071769714355
Epoch 1100, val loss: 0.6519723534584045
Epoch 1110, training loss: 422.7617492675781 = 0.6021662950515747 + 50.0 * 8.443191528320312
Epoch 1110, val loss: 0.6513471007347107
Epoch 1120, training loss: 422.7181396484375 = 0.6009502410888672 + 50.0 * 8.442343711853027
Epoch 1120, val loss: 0.6507331132888794
Epoch 1130, training loss: 422.6888732910156 = 0.5997478365898132 + 50.0 * 8.441781997680664
Epoch 1130, val loss: 0.6501278877258301
Epoch 1140, training loss: 422.872314453125 = 0.5985463261604309 + 50.0 * 8.445475578308105
Epoch 1140, val loss: 0.6495331525802612
Epoch 1150, training loss: 422.6673278808594 = 0.5973033308982849 + 50.0 * 8.441400527954102
Epoch 1150, val loss: 0.6488184332847595
Epoch 1160, training loss: 422.6791687011719 = 0.5960842370986938 + 50.0 * 8.441661834716797
Epoch 1160, val loss: 0.6481999158859253
Epoch 1170, training loss: 422.5994567871094 = 0.5949335098266602 + 50.0 * 8.44009017944336
Epoch 1170, val loss: 0.6475980281829834
Epoch 1180, training loss: 422.5347595214844 = 0.5937991142272949 + 50.0 * 8.43881893157959
Epoch 1180, val loss: 0.6470522880554199
Epoch 1190, training loss: 422.5105895996094 = 0.5926954746246338 + 50.0 * 8.438358306884766
Epoch 1190, val loss: 0.6464987993240356
Epoch 1200, training loss: 422.6725158691406 = 0.5915762782096863 + 50.0 * 8.441618919372559
Epoch 1200, val loss: 0.6459326148033142
Epoch 1210, training loss: 422.4504699707031 = 0.5904512405395508 + 50.0 * 8.437200546264648
Epoch 1210, val loss: 0.6453734040260315
Epoch 1220, training loss: 422.409912109375 = 0.5893570780754089 + 50.0 * 8.436410903930664
Epoch 1220, val loss: 0.6448185443878174
Epoch 1230, training loss: 422.39630126953125 = 0.5882877111434937 + 50.0 * 8.43616008758545
Epoch 1230, val loss: 0.644307553768158
Epoch 1240, training loss: 422.7185363769531 = 0.5872035622596741 + 50.0 * 8.442626953125
Epoch 1240, val loss: 0.643787145614624
Epoch 1250, training loss: 422.42877197265625 = 0.5860998034477234 + 50.0 * 8.436853408813477
Epoch 1250, val loss: 0.6431677341461182
Epoch 1260, training loss: 422.331787109375 = 0.585014283657074 + 50.0 * 8.434935569763184
Epoch 1260, val loss: 0.6426543593406677
Epoch 1270, training loss: 422.26214599609375 = 0.583992063999176 + 50.0 * 8.433563232421875
Epoch 1270, val loss: 0.6421505808830261
Epoch 1280, training loss: 422.2133483886719 = 0.5829755067825317 + 50.0 * 8.432607650756836
Epoch 1280, val loss: 0.6416731476783752
Epoch 1290, training loss: 422.246337890625 = 0.5819666981697083 + 50.0 * 8.433287620544434
Epoch 1290, val loss: 0.6412280797958374
Epoch 1300, training loss: 422.18109130859375 = 0.5809080600738525 + 50.0 * 8.43200397491455
Epoch 1300, val loss: 0.6406924724578857
Epoch 1310, training loss: 422.2072448730469 = 0.5798687934875488 + 50.0 * 8.432547569274902
Epoch 1310, val loss: 0.6401769518852234
Epoch 1320, training loss: 422.1145324707031 = 0.5788502097129822 + 50.0 * 8.430713653564453
Epoch 1320, val loss: 0.6397114992141724
Epoch 1330, training loss: 422.2458190917969 = 0.5778615474700928 + 50.0 * 8.433359146118164
Epoch 1330, val loss: 0.6392459869384766
Epoch 1340, training loss: 422.04669189453125 = 0.5768275856971741 + 50.0 * 8.429397583007812
Epoch 1340, val loss: 0.638793408870697
Epoch 1350, training loss: 422.0404968261719 = 0.5758201479911804 + 50.0 * 8.429293632507324
Epoch 1350, val loss: 0.6383541226387024
Epoch 1360, training loss: 422.0084533691406 = 0.5748481750488281 + 50.0 * 8.428671836853027
Epoch 1360, val loss: 0.6379320621490479
Epoch 1370, training loss: 421.9754943847656 = 0.573882520198822 + 50.0 * 8.428031921386719
Epoch 1370, val loss: 0.6375067234039307
Epoch 1380, training loss: 421.9949951171875 = 0.5729151964187622 + 50.0 * 8.428442001342773
Epoch 1380, val loss: 0.6370717883110046
Epoch 1390, training loss: 422.152587890625 = 0.5719262361526489 + 50.0 * 8.431612968444824
Epoch 1390, val loss: 0.6366065144538879
Epoch 1400, training loss: 421.9129333496094 = 0.5708878636360168 + 50.0 * 8.426840782165527
Epoch 1400, val loss: 0.6361477971076965
Epoch 1410, training loss: 421.9056701660156 = 0.5698922872543335 + 50.0 * 8.426715850830078
Epoch 1410, val loss: 0.6356698274612427
Epoch 1420, training loss: 421.87713623046875 = 0.5689326524734497 + 50.0 * 8.426163673400879
Epoch 1420, val loss: 0.6352339386940002
Epoch 1430, training loss: 421.80743408203125 = 0.5679616928100586 + 50.0 * 8.424789428710938
Epoch 1430, val loss: 0.6348032355308533
Epoch 1440, training loss: 421.7796936035156 = 0.567016065120697 + 50.0 * 8.424253463745117
Epoch 1440, val loss: 0.6343889832496643
Epoch 1450, training loss: 421.8317565917969 = 0.5660566687583923 + 50.0 * 8.425313949584961
Epoch 1450, val loss: 0.6339675784111023
Epoch 1460, training loss: 421.89239501953125 = 0.5650429129600525 + 50.0 * 8.426547050476074
Epoch 1460, val loss: 0.633456826210022
Epoch 1470, training loss: 421.8105163574219 = 0.5640121698379517 + 50.0 * 8.42492961883545
Epoch 1470, val loss: 0.6329329609870911
Epoch 1480, training loss: 421.6784973144531 = 0.5630455613136292 + 50.0 * 8.422308921813965
Epoch 1480, val loss: 0.6325103044509888
Epoch 1490, training loss: 421.66729736328125 = 0.5621071457862854 + 50.0 * 8.422103881835938
Epoch 1490, val loss: 0.6321069598197937
Epoch 1500, training loss: 421.6305847167969 = 0.5611648559570312 + 50.0 * 8.421388626098633
Epoch 1500, val loss: 0.6316974759101868
Epoch 1510, training loss: 421.71337890625 = 0.5602163672447205 + 50.0 * 8.423063278198242
Epoch 1510, val loss: 0.6312891840934753
Epoch 1520, training loss: 421.70220947265625 = 0.5592092275619507 + 50.0 * 8.422860145568848
Epoch 1520, val loss: 0.6308002471923828
Epoch 1530, training loss: 421.5675048828125 = 0.5582055449485779 + 50.0 * 8.420186042785645
Epoch 1530, val loss: 0.6302878260612488
Epoch 1540, training loss: 421.5411071777344 = 0.5572356581687927 + 50.0 * 8.419677734375
Epoch 1540, val loss: 0.6298512816429138
Epoch 1550, training loss: 421.5106506347656 = 0.5562793612480164 + 50.0 * 8.419087409973145
Epoch 1550, val loss: 0.6294386386871338
Epoch 1560, training loss: 421.50225830078125 = 0.5553252696990967 + 50.0 * 8.418938636779785
Epoch 1560, val loss: 0.629002571105957
Epoch 1570, training loss: 421.8445129394531 = 0.5543393492698669 + 50.0 * 8.425803184509277
Epoch 1570, val loss: 0.6285557746887207
Epoch 1580, training loss: 421.5296630859375 = 0.5533256530761719 + 50.0 * 8.419527053833008
Epoch 1580, val loss: 0.6280295848846436
Epoch 1590, training loss: 421.43304443359375 = 0.5523198843002319 + 50.0 * 8.417614936828613
Epoch 1590, val loss: 0.6275662183761597
Epoch 1600, training loss: 421.4040832519531 = 0.5513539910316467 + 50.0 * 8.417054176330566
Epoch 1600, val loss: 0.6271387338638306
Epoch 1610, training loss: 421.3627624511719 = 0.550382673740387 + 50.0 * 8.416247367858887
Epoch 1610, val loss: 0.6266919374465942
Epoch 1620, training loss: 421.3426208496094 = 0.549416720867157 + 50.0 * 8.415863990783691
Epoch 1620, val loss: 0.6262590289115906
Epoch 1630, training loss: 421.6456298828125 = 0.5484487414360046 + 50.0 * 8.421943664550781
Epoch 1630, val loss: 0.6257926225662231
Epoch 1640, training loss: 421.3590087890625 = 0.5473636984825134 + 50.0 * 8.41623306274414
Epoch 1640, val loss: 0.6253211498260498
Epoch 1650, training loss: 421.32659912109375 = 0.5463394522666931 + 50.0 * 8.415605545043945
Epoch 1650, val loss: 0.6247946619987488
Epoch 1660, training loss: 421.27508544921875 = 0.5453411340713501 + 50.0 * 8.414594650268555
Epoch 1660, val loss: 0.6243802905082703
Epoch 1670, training loss: 421.2333984375 = 0.5443509221076965 + 50.0 * 8.41378116607666
Epoch 1670, val loss: 0.6239257454872131
Epoch 1680, training loss: 421.2593994140625 = 0.5433563590049744 + 50.0 * 8.414320945739746
Epoch 1680, val loss: 0.6234868764877319
Epoch 1690, training loss: 421.242919921875 = 0.5423131585121155 + 50.0 * 8.41401195526123
Epoch 1690, val loss: 0.622985303401947
Epoch 1700, training loss: 421.20123291015625 = 0.5412675142288208 + 50.0 * 8.413199424743652
Epoch 1700, val loss: 0.6224762797355652
Epoch 1710, training loss: 421.171875 = 0.5402253270149231 + 50.0 * 8.412632942199707
Epoch 1710, val loss: 0.6219609975814819
Epoch 1720, training loss: 421.1216735839844 = 0.5391967296600342 + 50.0 * 8.411649703979492
Epoch 1720, val loss: 0.6214819550514221
Epoch 1730, training loss: 421.14666748046875 = 0.5381619930267334 + 50.0 * 8.41217041015625
Epoch 1730, val loss: 0.620976448059082
Epoch 1740, training loss: 421.45513916015625 = 0.5371043682098389 + 50.0 * 8.418360710144043
Epoch 1740, val loss: 0.6204214692115784
Epoch 1750, training loss: 421.1297912597656 = 0.535961389541626 + 50.0 * 8.411876678466797
Epoch 1750, val loss: 0.6198714375495911
Epoch 1760, training loss: 421.0745849609375 = 0.5348942875862122 + 50.0 * 8.410794258117676
Epoch 1760, val loss: 0.6193664073944092
Epoch 1770, training loss: 421.0245056152344 = 0.5338290929794312 + 50.0 * 8.409812927246094
Epoch 1770, val loss: 0.61887526512146
Epoch 1780, training loss: 420.99334716796875 = 0.5327759981155396 + 50.0 * 8.409211158752441
Epoch 1780, val loss: 0.6183609366416931
Epoch 1790, training loss: 421.0513000488281 = 0.5317069888114929 + 50.0 * 8.410391807556152
Epoch 1790, val loss: 0.6178233623504639
Epoch 1800, training loss: 421.0931701660156 = 0.5305889248847961 + 50.0 * 8.41125202178955
Epoch 1800, val loss: 0.6172639727592468
Epoch 1810, training loss: 420.96734619140625 = 0.5294409394264221 + 50.0 * 8.408758163452148
Epoch 1810, val loss: 0.6166835427284241
Epoch 1820, training loss: 420.9132995605469 = 0.5283328890800476 + 50.0 * 8.407699584960938
Epoch 1820, val loss: 0.6161304116249084
Epoch 1830, training loss: 420.89727783203125 = 0.5272281765937805 + 50.0 * 8.407401084899902
Epoch 1830, val loss: 0.6155679821968079
Epoch 1840, training loss: 420.91986083984375 = 0.5261241793632507 + 50.0 * 8.407875061035156
Epoch 1840, val loss: 0.6150079369544983
Epoch 1850, training loss: 421.1239929199219 = 0.524986982345581 + 50.0 * 8.411979675292969
Epoch 1850, val loss: 0.6144094467163086
Epoch 1860, training loss: 420.87725830078125 = 0.5237932801246643 + 50.0 * 8.407069206237793
Epoch 1860, val loss: 0.613802433013916
Epoch 1870, training loss: 420.8327941894531 = 0.5226390361785889 + 50.0 * 8.406203269958496
Epoch 1870, val loss: 0.6132146120071411
Epoch 1880, training loss: 420.7999267578125 = 0.5214883089065552 + 50.0 * 8.405569076538086
Epoch 1880, val loss: 0.6126046776771545
Epoch 1890, training loss: 420.8106994628906 = 0.5203332901000977 + 50.0 * 8.405807495117188
Epoch 1890, val loss: 0.6120151281356812
Epoch 1900, training loss: 420.8955993652344 = 0.5191540122032166 + 50.0 * 8.4075288772583
Epoch 1900, val loss: 0.6113753914833069
Epoch 1910, training loss: 420.91900634765625 = 0.5179439783096313 + 50.0 * 8.408020973205566
Epoch 1910, val loss: 0.6107541918754578
Epoch 1920, training loss: 420.86944580078125 = 0.5167000889778137 + 50.0 * 8.407054901123047
Epoch 1920, val loss: 0.6100611686706543
Epoch 1930, training loss: 420.781005859375 = 0.5154588222503662 + 50.0 * 8.40531063079834
Epoch 1930, val loss: 0.6094493865966797
Epoch 1940, training loss: 420.6896667480469 = 0.5142473578453064 + 50.0 * 8.403508186340332
Epoch 1940, val loss: 0.6088090538978577
Epoch 1950, training loss: 420.6878662109375 = 0.513031542301178 + 50.0 * 8.403496742248535
Epoch 1950, val loss: 0.6081889271736145
Epoch 1960, training loss: 420.6922607421875 = 0.5118051767349243 + 50.0 * 8.403609275817871
Epoch 1960, val loss: 0.6075661778450012
Epoch 1970, training loss: 420.87384033203125 = 0.510556161403656 + 50.0 * 8.407265663146973
Epoch 1970, val loss: 0.606921374797821
Epoch 1980, training loss: 420.6812744140625 = 0.5092531442642212 + 50.0 * 8.403440475463867
Epoch 1980, val loss: 0.6062263250350952
Epoch 1990, training loss: 420.6502685546875 = 0.5079854130744934 + 50.0 * 8.40284538269043
Epoch 1990, val loss: 0.6055957078933716
Epoch 2000, training loss: 420.9298095703125 = 0.5066945552825928 + 50.0 * 8.408462524414062
Epoch 2000, val loss: 0.6049155592918396
Epoch 2010, training loss: 420.6097412109375 = 0.505351185798645 + 50.0 * 8.402088165283203
Epoch 2010, val loss: 0.6041369438171387
Epoch 2020, training loss: 420.571044921875 = 0.5040341019630432 + 50.0 * 8.40134048461914
Epoch 2020, val loss: 0.6034391522407532
Epoch 2030, training loss: 420.5481262207031 = 0.5027287602424622 + 50.0 * 8.400908470153809
Epoch 2030, val loss: 0.6027608513832092
Epoch 2040, training loss: 420.53009033203125 = 0.5014187097549438 + 50.0 * 8.40057373046875
Epoch 2040, val loss: 0.6020761728286743
Epoch 2050, training loss: 420.5281982421875 = 0.5000916719436646 + 50.0 * 8.400562286376953
Epoch 2050, val loss: 0.6013801097869873
Epoch 2060, training loss: 420.7052917480469 = 0.4987426996231079 + 50.0 * 8.404130935668945
Epoch 2060, val loss: 0.6006949543952942
Epoch 2070, training loss: 420.5762634277344 = 0.49732497334480286 + 50.0 * 8.401578903198242
Epoch 2070, val loss: 0.5998606085777283
Epoch 2080, training loss: 420.4882507324219 = 0.4959397315979004 + 50.0 * 8.399846076965332
Epoch 2080, val loss: 0.5991572737693787
Epoch 2090, training loss: 420.4471130371094 = 0.4945540130138397 + 50.0 * 8.39905071258545
Epoch 2090, val loss: 0.598415732383728
Epoch 2100, training loss: 420.4310607910156 = 0.4931686222553253 + 50.0 * 8.398757934570312
Epoch 2100, val loss: 0.5976981520652771
Epoch 2110, training loss: 420.4617004394531 = 0.49177470803260803 + 50.0 * 8.399398803710938
Epoch 2110, val loss: 0.5969868898391724
Epoch 2120, training loss: 420.5831604003906 = 0.49033764004707336 + 50.0 * 8.401856422424316
Epoch 2120, val loss: 0.5962069630622864
Epoch 2130, training loss: 420.4819641113281 = 0.4888823926448822 + 50.0 * 8.399861335754395
Epoch 2130, val loss: 0.5954775810241699
Epoch 2140, training loss: 420.692626953125 = 0.48741793632507324 + 50.0 * 8.404104232788086
Epoch 2140, val loss: 0.594717264175415
Epoch 2150, training loss: 420.3960876464844 = 0.48590418696403503 + 50.0 * 8.39820384979248
Epoch 2150, val loss: 0.5938844680786133
Epoch 2160, training loss: 420.3723449707031 = 0.4844169616699219 + 50.0 * 8.397758483886719
Epoch 2160, val loss: 0.5931052565574646
Epoch 2170, training loss: 420.3544616699219 = 0.48294341564178467 + 50.0 * 8.397430419921875
Epoch 2170, val loss: 0.5924189686775208
Epoch 2180, training loss: 420.3212585449219 = 0.48145928978919983 + 50.0 * 8.396796226501465
Epoch 2180, val loss: 0.5916910767555237
Epoch 2190, training loss: 420.3189392089844 = 0.47996294498443604 + 50.0 * 8.396780014038086
Epoch 2190, val loss: 0.5909514427185059
Epoch 2200, training loss: 420.37933349609375 = 0.47844716906547546 + 50.0 * 8.398017883300781
Epoch 2200, val loss: 0.5902060866355896
Epoch 2210, training loss: 420.44757080078125 = 0.4768803119659424 + 50.0 * 8.3994140625
Epoch 2210, val loss: 0.5894305109977722
Epoch 2220, training loss: 420.4001159667969 = 0.47528380155563354 + 50.0 * 8.398496627807617
Epoch 2220, val loss: 0.5886712074279785
Epoch 2230, training loss: 420.3013000488281 = 0.4736979007720947 + 50.0 * 8.396552085876465
Epoch 2230, val loss: 0.5879234671592712
Epoch 2240, training loss: 420.2572326660156 = 0.47211527824401855 + 50.0 * 8.395702362060547
Epoch 2240, val loss: 0.5871875882148743
Epoch 2250, training loss: 420.2430114746094 = 0.4705241024494171 + 50.0 * 8.3954496383667
Epoch 2250, val loss: 0.5864573121070862
Epoch 2260, training loss: 420.3949279785156 = 0.4689158797264099 + 50.0 * 8.398520469665527
Epoch 2260, val loss: 0.5857577323913574
Epoch 2270, training loss: 420.2496643066406 = 0.46724411845207214 + 50.0 * 8.395648002624512
Epoch 2270, val loss: 0.5849674940109253
Epoch 2280, training loss: 420.2508239746094 = 0.4655855596065521 + 50.0 * 8.395705223083496
Epoch 2280, val loss: 0.5842218995094299
Epoch 2290, training loss: 420.2037353515625 = 0.46393126249313354 + 50.0 * 8.394796371459961
Epoch 2290, val loss: 0.583496630191803
Epoch 2300, training loss: 420.1729736328125 = 0.46227648854255676 + 50.0 * 8.394213676452637
Epoch 2300, val loss: 0.5827803611755371
Epoch 2310, training loss: 420.3126525878906 = 0.46061059832572937 + 50.0 * 8.397041320800781
Epoch 2310, val loss: 0.5820992588996887
Epoch 2320, training loss: 420.2028503417969 = 0.4588817059993744 + 50.0 * 8.394879341125488
Epoch 2320, val loss: 0.581247866153717
Epoch 2330, training loss: 420.1596374511719 = 0.4571654200553894 + 50.0 * 8.394049644470215
Epoch 2330, val loss: 0.5805452466011047
Epoch 2340, training loss: 420.1363220214844 = 0.4554479420185089 + 50.0 * 8.393617630004883
Epoch 2340, val loss: 0.5797947645187378
Epoch 2350, training loss: 420.31109619140625 = 0.4537274241447449 + 50.0 * 8.397147178649902
Epoch 2350, val loss: 0.5790980458259583
Epoch 2360, training loss: 420.1222229003906 = 0.4519249498844147 + 50.0 * 8.39340591430664
Epoch 2360, val loss: 0.5783563256263733
Epoch 2370, training loss: 420.0994567871094 = 0.45014601945877075 + 50.0 * 8.392986297607422
Epoch 2370, val loss: 0.577704906463623
Epoch 2380, training loss: 420.0713195800781 = 0.4483772814273834 + 50.0 * 8.39245891571045
Epoch 2380, val loss: 0.5769967436790466
Epoch 2390, training loss: 420.0649108886719 = 0.4466085135936737 + 50.0 * 8.392366409301758
Epoch 2390, val loss: 0.5763263702392578
Epoch 2400, training loss: 420.0613708496094 = 0.4448328912258148 + 50.0 * 8.39233112335205
Epoch 2400, val loss: 0.575672447681427
Epoch 2410, training loss: 420.1633605957031 = 0.443040132522583 + 50.0 * 8.39440631866455
Epoch 2410, val loss: 0.575003445148468
Epoch 2420, training loss: 420.1286926269531 = 0.44119518995285034 + 50.0 * 8.393750190734863
Epoch 2420, val loss: 0.5743222832679749
Epoch 2430, training loss: 420.0458984375 = 0.4393491744995117 + 50.0 * 8.392130851745605
Epoch 2430, val loss: 0.5736986994743347
Epoch 2440, training loss: 420.0351257324219 = 0.4375108480453491 + 50.0 * 8.391952514648438
Epoch 2440, val loss: 0.5730260610580444
Epoch 2450, training loss: 420.00836181640625 = 0.43567779660224915 + 50.0 * 8.391453742980957
Epoch 2450, val loss: 0.5723939538002014
Epoch 2460, training loss: 420.08197021484375 = 0.43384265899658203 + 50.0 * 8.392962455749512
Epoch 2460, val loss: 0.571758508682251
Epoch 2470, training loss: 420.0689392089844 = 0.43196383118629456 + 50.0 * 8.392739295959473
Epoch 2470, val loss: 0.571060299873352
Epoch 2480, training loss: 420.0049133300781 = 0.43008649349212646 + 50.0 * 8.391496658325195
Epoch 2480, val loss: 0.5704113245010376
Epoch 2490, training loss: 420.0155029296875 = 0.4282209575176239 + 50.0 * 8.391745567321777
Epoch 2490, val loss: 0.5697644948959351
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7716894977168949
0.8182279214663479
=== training gcn model ===
Epoch 0, training loss: 530.1953125 = 1.0790200233459473 + 50.0 * 10.58232593536377
Epoch 0, val loss: 1.0789611339569092
Epoch 10, training loss: 530.1830444335938 = 1.0759464502334595 + 50.0 * 10.58214282989502
Epoch 10, val loss: 1.0759143829345703
Epoch 20, training loss: 530.1367797851562 = 1.0728178024291992 + 50.0 * 10.581279754638672
Epoch 20, val loss: 1.0728098154067993
Epoch 30, training loss: 529.9310302734375 = 1.0695183277130127 + 50.0 * 10.577230453491211
Epoch 30, val loss: 1.0695374011993408
Epoch 40, training loss: 529.07568359375 = 1.065956950187683 + 50.0 * 10.56019401550293
Epoch 40, val loss: 1.0659990310668945
Epoch 50, training loss: 526.3558349609375 = 1.0619513988494873 + 50.0 * 10.505877494812012
Epoch 50, val loss: 1.061989426612854
Epoch 60, training loss: 519.3017578125 = 1.0576461553573608 + 50.0 * 10.364883422851562
Epoch 60, val loss: 1.0577784776687622
Epoch 70, training loss: 504.0521545410156 = 1.0532084703445435 + 50.0 * 10.059978485107422
Epoch 70, val loss: 1.0533348321914673
Epoch 80, training loss: 489.323486328125 = 1.0482683181762695 + 50.0 * 9.765503883361816
Epoch 80, val loss: 1.048392415046692
Epoch 90, training loss: 478.9042053222656 = 1.0431071519851685 + 50.0 * 9.557221412658691
Epoch 90, val loss: 1.0433156490325928
Epoch 100, training loss: 471.5313415527344 = 1.0384124517440796 + 50.0 * 9.409858703613281
Epoch 100, val loss: 1.0388540029525757
Epoch 110, training loss: 465.7503967285156 = 1.0350781679153442 + 50.0 * 9.294306755065918
Epoch 110, val loss: 1.0358381271362305
Epoch 120, training loss: 463.95941162109375 = 1.032174825668335 + 50.0 * 9.258544921875
Epoch 120, val loss: 1.0330278873443604
Epoch 130, training loss: 462.5225524902344 = 1.0288110971450806 + 50.0 * 9.229874610900879
Epoch 130, val loss: 1.029658555984497
Epoch 140, training loss: 460.6019287109375 = 1.025339961051941 + 50.0 * 9.191532135009766
Epoch 140, val loss: 1.0261889696121216
Epoch 150, training loss: 457.8420104980469 = 1.0222810506820679 + 50.0 * 9.136394500732422
Epoch 150, val loss: 1.0231809616088867
Epoch 160, training loss: 454.1312255859375 = 1.0198687314987183 + 50.0 * 9.062227249145508
Epoch 160, val loss: 1.0208712816238403
Epoch 170, training loss: 451.7744140625 = 1.017228364944458 + 50.0 * 9.015143394470215
Epoch 170, val loss: 1.0181034803390503
Epoch 180, training loss: 449.960693359375 = 1.013171911239624 + 50.0 * 8.978950500488281
Epoch 180, val loss: 1.014112949371338
Epoch 190, training loss: 448.0387268066406 = 1.0089354515075684 + 50.0 * 8.940595626831055
Epoch 190, val loss: 1.009967565536499
Epoch 200, training loss: 446.4848327636719 = 1.0048238039016724 + 50.0 * 8.909600257873535
Epoch 200, val loss: 1.005957841873169
Epoch 210, training loss: 445.4764099121094 = 1.0001893043518066 + 50.0 * 8.889524459838867
Epoch 210, val loss: 1.0013293027877808
Epoch 220, training loss: 444.62164306640625 = 0.9947060346603394 + 50.0 * 8.872538566589355
Epoch 220, val loss: 0.9958617091178894
Epoch 230, training loss: 443.69647216796875 = 0.9888100028038025 + 50.0 * 8.854153633117676
Epoch 230, val loss: 0.9900525212287903
Epoch 240, training loss: 442.588623046875 = 0.9828492999076843 + 50.0 * 8.832115173339844
Epoch 240, val loss: 0.9842280745506287
Epoch 250, training loss: 441.4748840332031 = 0.9766743779182434 + 50.0 * 8.809964179992676
Epoch 250, val loss: 0.9781190752983093
Epoch 260, training loss: 440.5496520996094 = 0.9698114991188049 + 50.0 * 8.791596412658691
Epoch 260, val loss: 0.9712767601013184
Epoch 270, training loss: 439.8642272949219 = 0.9622661471366882 + 50.0 * 8.77803897857666
Epoch 270, val loss: 0.963752806186676
Epoch 280, training loss: 439.313232421875 = 0.9542616605758667 + 50.0 * 8.767179489135742
Epoch 280, val loss: 0.9558238387107849
Epoch 290, training loss: 438.7469177246094 = 0.946047842502594 + 50.0 * 8.756017684936523
Epoch 290, val loss: 0.9477450251579285
Epoch 300, training loss: 438.12359619140625 = 0.9377370476722717 + 50.0 * 8.743717193603516
Epoch 300, val loss: 0.9395496845245361
Epoch 310, training loss: 437.44793701171875 = 0.9292258024215698 + 50.0 * 8.730374336242676
Epoch 310, val loss: 0.9312241077423096
Epoch 320, training loss: 436.8136901855469 = 0.9204927086830139 + 50.0 * 8.717864036560059
Epoch 320, val loss: 0.9226470589637756
Epoch 330, training loss: 436.2933044433594 = 0.9115479588508606 + 50.0 * 8.707634925842285
Epoch 330, val loss: 0.9138131141662598
Epoch 340, training loss: 435.8276062011719 = 0.9021314978599548 + 50.0 * 8.698509216308594
Epoch 340, val loss: 0.9045606851577759
Epoch 350, training loss: 435.3556823730469 = 0.8923718929290771 + 50.0 * 8.689266204833984
Epoch 350, val loss: 0.8949190974235535
Epoch 360, training loss: 434.9577331542969 = 0.8823946714401245 + 50.0 * 8.681507110595703
Epoch 360, val loss: 0.8851205110549927
Epoch 370, training loss: 434.6202392578125 = 0.8722804188728333 + 50.0 * 8.674959182739258
Epoch 370, val loss: 0.8751864433288574
Epoch 380, training loss: 434.1694030761719 = 0.8621596693992615 + 50.0 * 8.666145324707031
Epoch 380, val loss: 0.8652596473693848
Epoch 390, training loss: 433.8968811035156 = 0.8521041870117188 + 50.0 * 8.660895347595215
Epoch 390, val loss: 0.8553688526153564
Epoch 400, training loss: 433.4347839355469 = 0.8420095443725586 + 50.0 * 8.65185546875
Epoch 400, val loss: 0.845504879951477
Epoch 410, training loss: 433.0533447265625 = 0.8320329785346985 + 50.0 * 8.644426345825195
Epoch 410, val loss: 0.8357671499252319
Epoch 420, training loss: 432.7366027832031 = 0.8221487402915955 + 50.0 * 8.638289451599121
Epoch 420, val loss: 0.826113224029541
Epoch 430, training loss: 432.4289855957031 = 0.8124088644981384 + 50.0 * 8.632331848144531
Epoch 430, val loss: 0.8166199922561646
Epoch 440, training loss: 432.4448547363281 = 0.802879810333252 + 50.0 * 8.63283920288086
Epoch 440, val loss: 0.8073282837867737
Epoch 450, training loss: 431.823974609375 = 0.793509840965271 + 50.0 * 8.620609283447266
Epoch 450, val loss: 0.7982749938964844
Epoch 460, training loss: 431.53521728515625 = 0.7845345735549927 + 50.0 * 8.615013122558594
Epoch 460, val loss: 0.7896137833595276
Epoch 470, training loss: 431.2871398925781 = 0.7758500576019287 + 50.0 * 8.610225677490234
Epoch 470, val loss: 0.7812309861183167
Epoch 480, training loss: 430.84552001953125 = 0.767500638961792 + 50.0 * 8.601560592651367
Epoch 480, val loss: 0.7732024192810059
Epoch 490, training loss: 430.47882080078125 = 0.7595268487930298 + 50.0 * 8.594386100769043
Epoch 490, val loss: 0.7655763030052185
Epoch 500, training loss: 430.14306640625 = 0.7518559694290161 + 50.0 * 8.587823867797852
Epoch 500, val loss: 0.7582660913467407
Epoch 510, training loss: 429.865478515625 = 0.7444087266921997 + 50.0 * 8.58242130279541
Epoch 510, val loss: 0.7512195706367493
Epoch 520, training loss: 429.5072937011719 = 0.7372071146965027 + 50.0 * 8.575401306152344
Epoch 520, val loss: 0.744442343711853
Epoch 530, training loss: 429.1306457519531 = 0.730301558971405 + 50.0 * 8.56800651550293
Epoch 530, val loss: 0.7380067706108093
Epoch 540, training loss: 428.852783203125 = 0.7237203121185303 + 50.0 * 8.562581062316895
Epoch 540, val loss: 0.7318962216377258
Epoch 550, training loss: 428.9385986328125 = 0.7173845171928406 + 50.0 * 8.564424514770508
Epoch 550, val loss: 0.7260796427726746
Epoch 560, training loss: 428.40960693359375 = 0.7112693786621094 + 50.0 * 8.553966522216797
Epoch 560, val loss: 0.7204853892326355
Epoch 570, training loss: 428.1117248535156 = 0.7054900527000427 + 50.0 * 8.548125267028809
Epoch 570, val loss: 0.7152619957923889
Epoch 580, training loss: 427.90264892578125 = 0.6999958157539368 + 50.0 * 8.544053077697754
Epoch 580, val loss: 0.7103492021560669
Epoch 590, training loss: 427.7088317871094 = 0.694771409034729 + 50.0 * 8.540281295776367
Epoch 590, val loss: 0.7057347893714905
Epoch 600, training loss: 427.5979309082031 = 0.6897692084312439 + 50.0 * 8.538163185119629
Epoch 600, val loss: 0.7014085054397583
Epoch 610, training loss: 427.4487609863281 = 0.684951663017273 + 50.0 * 8.535276412963867
Epoch 610, val loss: 0.697295606136322
Epoch 620, training loss: 427.19256591796875 = 0.6804702877998352 + 50.0 * 8.530241966247559
Epoch 620, val loss: 0.693465530872345
Epoch 630, training loss: 427.0473937988281 = 0.6762339472770691 + 50.0 * 8.527422904968262
Epoch 630, val loss: 0.6899601221084595
Epoch 640, training loss: 427.01336669921875 = 0.6721892356872559 + 50.0 * 8.526823997497559
Epoch 640, val loss: 0.6866472363471985
Epoch 650, training loss: 426.82177734375 = 0.6682848334312439 + 50.0 * 8.523070335388184
Epoch 650, val loss: 0.683559238910675
Epoch 660, training loss: 426.6387023925781 = 0.6645984649658203 + 50.0 * 8.519481658935547
Epoch 660, val loss: 0.68064945936203
Epoch 670, training loss: 426.4779357910156 = 0.6611067652702332 + 50.0 * 8.516336441040039
Epoch 670, val loss: 0.6779763698577881
Epoch 680, training loss: 426.36578369140625 = 0.657779335975647 + 50.0 * 8.51416015625
Epoch 680, val loss: 0.6754834651947021
Epoch 690, training loss: 426.4371337890625 = 0.6545925736427307 + 50.0 * 8.515650749206543
Epoch 690, val loss: 0.6731778383255005
Epoch 700, training loss: 426.1753845214844 = 0.6515122056007385 + 50.0 * 8.510477066040039
Epoch 700, val loss: 0.6709569096565247
Epoch 710, training loss: 426.0939636230469 = 0.6486008763313293 + 50.0 * 8.508907318115234
Epoch 710, val loss: 0.6689497232437134
Epoch 720, training loss: 426.01348876953125 = 0.6458328366279602 + 50.0 * 8.507352828979492
Epoch 720, val loss: 0.6671081185340881
Epoch 730, training loss: 426.15032958984375 = 0.6431617736816406 + 50.0 * 8.510143280029297
Epoch 730, val loss: 0.6653629541397095
Epoch 740, training loss: 425.86492919921875 = 0.6405738592147827 + 50.0 * 8.504487037658691
Epoch 740, val loss: 0.6637252569198608
Epoch 750, training loss: 425.8035583496094 = 0.6381397843360901 + 50.0 * 8.503308296203613
Epoch 750, val loss: 0.6622377038002014
Epoch 760, training loss: 425.7179870605469 = 0.635826826095581 + 50.0 * 8.501643180847168
Epoch 760, val loss: 0.6608977317810059
Epoch 770, training loss: 425.65057373046875 = 0.6336166262626648 + 50.0 * 8.50033950805664
Epoch 770, val loss: 0.6596594452857971
Epoch 780, training loss: 425.5857849121094 = 0.6314939260482788 + 50.0 * 8.499085426330566
Epoch 780, val loss: 0.6585198640823364
Epoch 790, training loss: 425.62371826171875 = 0.6294488310813904 + 50.0 * 8.499885559082031
Epoch 790, val loss: 0.6574432253837585
Epoch 800, training loss: 425.57080078125 = 0.6274489164352417 + 50.0 * 8.49886703491211
Epoch 800, val loss: 0.6565326452255249
Epoch 810, training loss: 425.4242858886719 = 0.6255477070808411 + 50.0 * 8.49597454071045
Epoch 810, val loss: 0.6556187868118286
Epoch 820, training loss: 425.348388671875 = 0.6237429976463318 + 50.0 * 8.494492530822754
Epoch 820, val loss: 0.6548118591308594
Epoch 830, training loss: 425.4540100097656 = 0.6220118403434753 + 50.0 * 8.4966402053833
Epoch 830, val loss: 0.6540727615356445
Epoch 840, training loss: 425.2785949707031 = 0.6203160285949707 + 50.0 * 8.493165016174316
Epoch 840, val loss: 0.653424084186554
Epoch 850, training loss: 425.1872863769531 = 0.6186859011650085 + 50.0 * 8.491372108459473
Epoch 850, val loss: 0.6528136730194092
Epoch 860, training loss: 425.10198974609375 = 0.6171314120292664 + 50.0 * 8.489697456359863
Epoch 860, val loss: 0.6522867679595947
Epoch 870, training loss: 425.0458068847656 = 0.6156244874000549 + 50.0 * 8.488603591918945
Epoch 870, val loss: 0.6517922282218933
Epoch 880, training loss: 425.1280212402344 = 0.6141568422317505 + 50.0 * 8.490277290344238
Epoch 880, val loss: 0.6513561606407166
Epoch 890, training loss: 424.9476013183594 = 0.6126911640167236 + 50.0 * 8.486698150634766
Epoch 890, val loss: 0.6509106755256653
Epoch 900, training loss: 424.8563232421875 = 0.6113041639328003 + 50.0 * 8.48490047454834
Epoch 900, val loss: 0.6505416035652161
Epoch 910, training loss: 424.7893981933594 = 0.6099606156349182 + 50.0 * 8.483589172363281
Epoch 910, val loss: 0.6502187252044678
Epoch 920, training loss: 424.7799072265625 = 0.6086505651473999 + 50.0 * 8.48342514038086
Epoch 920, val loss: 0.6498743295669556
Epoch 930, training loss: 424.7740478515625 = 0.6073389053344727 + 50.0 * 8.4833345413208
Epoch 930, val loss: 0.6496032476425171
Epoch 940, training loss: 424.62811279296875 = 0.6060542464256287 + 50.0 * 8.480441093444824
Epoch 940, val loss: 0.6493343710899353
Epoch 950, training loss: 424.5762023925781 = 0.6048198938369751 + 50.0 * 8.479427337646484
Epoch 950, val loss: 0.6490885615348816
Epoch 960, training loss: 424.5472717285156 = 0.6036162376403809 + 50.0 * 8.478873252868652
Epoch 960, val loss: 0.6489244103431702
Epoch 970, training loss: 424.578369140625 = 0.6024056077003479 + 50.0 * 8.47951889038086
Epoch 970, val loss: 0.6486451029777527
Epoch 980, training loss: 424.4759216308594 = 0.6012049317359924 + 50.0 * 8.477494239807129
Epoch 980, val loss: 0.6485293507575989
Epoch 990, training loss: 424.3775634765625 = 0.6000546813011169 + 50.0 * 8.475549697875977
Epoch 990, val loss: 0.6483069658279419
Epoch 1000, training loss: 424.3327331542969 = 0.5989357233047485 + 50.0 * 8.474676132202148
Epoch 1000, val loss: 0.6481388211250305
Epoch 1010, training loss: 424.4192810058594 = 0.5978406667709351 + 50.0 * 8.476428985595703
Epoch 1010, val loss: 0.6480605602264404
Epoch 1020, training loss: 424.3653869628906 = 0.5967109799385071 + 50.0 * 8.475373268127441
Epoch 1020, val loss: 0.647869348526001
Epoch 1030, training loss: 424.2096862792969 = 0.5956104397773743 + 50.0 * 8.472281455993652
Epoch 1030, val loss: 0.6477255821228027
Epoch 1040, training loss: 424.1507263183594 = 0.5945744514465332 + 50.0 * 8.471122741699219
Epoch 1040, val loss: 0.6476576924324036
Epoch 1050, training loss: 424.07440185546875 = 0.59356689453125 + 50.0 * 8.469616889953613
Epoch 1050, val loss: 0.6475861668586731
Epoch 1060, training loss: 424.0241394042969 = 0.592577338218689 + 50.0 * 8.46863079071045
Epoch 1060, val loss: 0.6475194096565247
Epoch 1070, training loss: 424.2431335449219 = 0.591593861579895 + 50.0 * 8.473031044006348
Epoch 1070, val loss: 0.6474351286888123
Epoch 1080, training loss: 424.04864501953125 = 0.5905599594116211 + 50.0 * 8.469161987304688
Epoch 1080, val loss: 0.6473817229270935
Epoch 1090, training loss: 423.9295959472656 = 0.5895645022392273 + 50.0 * 8.466800689697266
Epoch 1090, val loss: 0.6473255753517151
Epoch 1100, training loss: 423.8329162597656 = 0.5886214971542358 + 50.0 * 8.464885711669922
Epoch 1100, val loss: 0.6472687125205994
Epoch 1110, training loss: 423.7667541503906 = 0.5877002477645874 + 50.0 * 8.463581085205078
Epoch 1110, val loss: 0.6472386717796326
Epoch 1120, training loss: 423.7168884277344 = 0.5867903232574463 + 50.0 * 8.462601661682129
Epoch 1120, val loss: 0.6472234129905701
Epoch 1130, training loss: 423.6719970703125 = 0.5858827829360962 + 50.0 * 8.461722373962402
Epoch 1130, val loss: 0.6471993923187256
Epoch 1140, training loss: 424.1855773925781 = 0.5849537253379822 + 50.0 * 8.472012519836426
Epoch 1140, val loss: 0.6471518278121948
Epoch 1150, training loss: 423.7153625488281 = 0.5839521288871765 + 50.0 * 8.462628364562988
Epoch 1150, val loss: 0.6470537185668945
Epoch 1160, training loss: 423.5546875 = 0.5830186605453491 + 50.0 * 8.459433555603027
Epoch 1160, val loss: 0.6469958424568176
Epoch 1170, training loss: 423.4927673339844 = 0.5821295976638794 + 50.0 * 8.458212852478027
Epoch 1170, val loss: 0.6469765305519104
Epoch 1180, training loss: 423.4352722167969 = 0.581253170967102 + 50.0 * 8.457079887390137
Epoch 1180, val loss: 0.6469672322273254
Epoch 1190, training loss: 423.3921203613281 = 0.5803800225257874 + 50.0 * 8.4562349319458
Epoch 1190, val loss: 0.6469711661338806
Epoch 1200, training loss: 423.3504638671875 = 0.5795051455497742 + 50.0 * 8.455419540405273
Epoch 1200, val loss: 0.6469559669494629
Epoch 1210, training loss: 423.3265380859375 = 0.5786295533180237 + 50.0 * 8.454957962036133
Epoch 1210, val loss: 0.6469539999961853
Epoch 1220, training loss: 423.7379455566406 = 0.5777297019958496 + 50.0 * 8.463204383850098
Epoch 1220, val loss: 0.6469295024871826
Epoch 1230, training loss: 423.2695007324219 = 0.5767704844474792 + 50.0 * 8.45385456085205
Epoch 1230, val loss: 0.6468387842178345
Epoch 1240, training loss: 423.2579040527344 = 0.5758640170097351 + 50.0 * 8.453640937805176
Epoch 1240, val loss: 0.6468388438224792
Epoch 1250, training loss: 423.1702575683594 = 0.5749962329864502 + 50.0 * 8.451905250549316
Epoch 1250, val loss: 0.6467320919036865
Epoch 1260, training loss: 423.12615966796875 = 0.5741430521011353 + 50.0 * 8.451040267944336
Epoch 1260, val loss: 0.6466994285583496
Epoch 1270, training loss: 423.07867431640625 = 0.5732909440994263 + 50.0 * 8.45010757446289
Epoch 1270, val loss: 0.6466779708862305
Epoch 1280, training loss: 423.0299987792969 = 0.5724347233772278 + 50.0 * 8.449151039123535
Epoch 1280, val loss: 0.6466277837753296
Epoch 1290, training loss: 422.982666015625 = 0.5715648531913757 + 50.0 * 8.448222160339355
Epoch 1290, val loss: 0.6465815901756287
Epoch 1300, training loss: 423.1980285644531 = 0.5706837177276611 + 50.0 * 8.452547073364258
Epoch 1300, val loss: 0.6465458273887634
Epoch 1310, training loss: 423.03759765625 = 0.5697215795516968 + 50.0 * 8.449357032775879
Epoch 1310, val loss: 0.6463598012924194
Epoch 1320, training loss: 422.8901062011719 = 0.5687984228134155 + 50.0 * 8.446426391601562
Epoch 1320, val loss: 0.6462482213973999
Epoch 1330, training loss: 422.8163146972656 = 0.5679043531417847 + 50.0 * 8.444968223571777
Epoch 1330, val loss: 0.6461739540100098
Epoch 1340, training loss: 422.77618408203125 = 0.5670220851898193 + 50.0 * 8.444183349609375
Epoch 1340, val loss: 0.646134614944458
Epoch 1350, training loss: 422.73553466796875 = 0.5661410689353943 + 50.0 * 8.443387985229492
Epoch 1350, val loss: 0.646077036857605
Epoch 1360, training loss: 422.69989013671875 = 0.5652611255645752 + 50.0 * 8.442692756652832
Epoch 1360, val loss: 0.6460050940513611
Epoch 1370, training loss: 422.6790771484375 = 0.5643765926361084 + 50.0 * 8.442294120788574
Epoch 1370, val loss: 0.6459174156188965
Epoch 1380, training loss: 423.1036071777344 = 0.5634749531745911 + 50.0 * 8.45080280303955
Epoch 1380, val loss: 0.6457936763763428
Epoch 1390, training loss: 422.72760009765625 = 0.5625090599060059 + 50.0 * 8.443302154541016
Epoch 1390, val loss: 0.6457148790359497
Epoch 1400, training loss: 422.5977478027344 = 0.5615993142127991 + 50.0 * 8.440723419189453
Epoch 1400, val loss: 0.6456150412559509
Epoch 1410, training loss: 422.5716247558594 = 0.5607179403305054 + 50.0 * 8.440217971801758
Epoch 1410, val loss: 0.6455163955688477
Epoch 1420, training loss: 422.5338439941406 = 0.5598393082618713 + 50.0 * 8.43947982788086
Epoch 1420, val loss: 0.6454581022262573
Epoch 1430, training loss: 422.50958251953125 = 0.5589569807052612 + 50.0 * 8.43901252746582
Epoch 1430, val loss: 0.6454041600227356
Epoch 1440, training loss: 422.488525390625 = 0.5580697655677795 + 50.0 * 8.43860912322998
Epoch 1440, val loss: 0.6453137397766113
Epoch 1450, training loss: 422.5332336425781 = 0.5571746230125427 + 50.0 * 8.439521789550781
Epoch 1450, val loss: 0.6452271938323975
Epoch 1460, training loss: 422.50836181640625 = 0.5562353134155273 + 50.0 * 8.439042091369629
Epoch 1460, val loss: 0.6450788378715515
Epoch 1470, training loss: 422.46429443359375 = 0.555295467376709 + 50.0 * 8.438179969787598
Epoch 1470, val loss: 0.6449519991874695
Epoch 1480, training loss: 422.42327880859375 = 0.554380476474762 + 50.0 * 8.4373779296875
Epoch 1480, val loss: 0.6448279023170471
Epoch 1490, training loss: 422.3908996582031 = 0.5534709095954895 + 50.0 * 8.436748504638672
Epoch 1490, val loss: 0.6447380781173706
Epoch 1500, training loss: 422.3794250488281 = 0.5525544881820679 + 50.0 * 8.436537742614746
Epoch 1500, val loss: 0.6446016430854797
Epoch 1510, training loss: 422.6058349609375 = 0.5516188144683838 + 50.0 * 8.441084861755371
Epoch 1510, val loss: 0.644417941570282
Epoch 1520, training loss: 422.4092712402344 = 0.5506263971328735 + 50.0 * 8.437172889709473
Epoch 1520, val loss: 0.6443681716918945
Epoch 1530, training loss: 422.331787109375 = 0.5496755242347717 + 50.0 * 8.43564224243164
Epoch 1530, val loss: 0.6441572308540344
Epoch 1540, training loss: 422.4355163574219 = 0.5487306714057922 + 50.0 * 8.437735557556152
Epoch 1540, val loss: 0.6440588235855103
Epoch 1550, training loss: 422.2671203613281 = 0.5477496385574341 + 50.0 * 8.43438720703125
Epoch 1550, val loss: 0.6439313888549805
Epoch 1560, training loss: 422.2580871582031 = 0.5467914342880249 + 50.0 * 8.434226036071777
Epoch 1560, val loss: 0.6437773704528809
Epoch 1570, training loss: 422.24432373046875 = 0.5458436012268066 + 50.0 * 8.433969497680664
Epoch 1570, val loss: 0.6437138319015503
Epoch 1580, training loss: 422.2051696777344 = 0.5448926687240601 + 50.0 * 8.433205604553223
Epoch 1580, val loss: 0.6435912847518921
Epoch 1590, training loss: 422.1968688964844 = 0.5439354181289673 + 50.0 * 8.433058738708496
Epoch 1590, val loss: 0.6434943675994873
Epoch 1600, training loss: 422.29486083984375 = 0.542963981628418 + 50.0 * 8.435037612915039
Epoch 1600, val loss: 0.6433711051940918
Epoch 1610, training loss: 422.1751708984375 = 0.5419478416442871 + 50.0 * 8.432663917541504
Epoch 1610, val loss: 0.6431704759597778
Epoch 1620, training loss: 422.1804504394531 = 0.5409371852874756 + 50.0 * 8.432790756225586
Epoch 1620, val loss: 0.643013060092926
Epoch 1630, training loss: 422.11077880859375 = 0.5399293303489685 + 50.0 * 8.431417465209961
Epoch 1630, val loss: 0.6429321765899658
Epoch 1640, training loss: 422.091796875 = 0.538937509059906 + 50.0 * 8.43105697631836
Epoch 1640, val loss: 0.6428205370903015
Epoch 1650, training loss: 422.1579284667969 = 0.5379384756088257 + 50.0 * 8.43239974975586
Epoch 1650, val loss: 0.6426953673362732
Epoch 1660, training loss: 422.0833435058594 = 0.5368888974189758 + 50.0 * 8.430929183959961
Epoch 1660, val loss: 0.6425459980964661
Epoch 1670, training loss: 422.04217529296875 = 0.5358413457870483 + 50.0 * 8.430127143859863
Epoch 1670, val loss: 0.6423231959342957
Epoch 1680, training loss: 422.0353698730469 = 0.5348001718521118 + 50.0 * 8.430011749267578
Epoch 1680, val loss: 0.6421691179275513
Epoch 1690, training loss: 422.108154296875 = 0.5337387323379517 + 50.0 * 8.431488037109375
Epoch 1690, val loss: 0.6419395208358765
Epoch 1700, training loss: 421.9697570800781 = 0.5326371192932129 + 50.0 * 8.428742408752441
Epoch 1700, val loss: 0.64180588722229
Epoch 1710, training loss: 421.9710693359375 = 0.5315525531768799 + 50.0 * 8.428790092468262
Epoch 1710, val loss: 0.6415848731994629
Epoch 1720, training loss: 421.9400634765625 = 0.5304614305496216 + 50.0 * 8.428192138671875
Epoch 1720, val loss: 0.6414040923118591
Epoch 1730, training loss: 421.98492431640625 = 0.5293582677841187 + 50.0 * 8.42911148071289
Epoch 1730, val loss: 0.6412286758422852
Epoch 1740, training loss: 421.8812561035156 = 0.5282117128372192 + 50.0 * 8.427061080932617
Epoch 1740, val loss: 0.6409035921096802
Epoch 1750, training loss: 421.86859130859375 = 0.5270663499832153 + 50.0 * 8.426830291748047
Epoch 1750, val loss: 0.6406721472740173
Epoch 1760, training loss: 422.00140380859375 = 0.5259230732917786 + 50.0 * 8.429510116577148
Epoch 1760, val loss: 0.6403787136077881
Epoch 1770, training loss: 421.8319091796875 = 0.5247121453285217 + 50.0 * 8.426143646240234
Epoch 1770, val loss: 0.6401065587997437
Epoch 1780, training loss: 421.8136901855469 = 0.5235159397125244 + 50.0 * 8.425803184509277
Epoch 1780, val loss: 0.6397948265075684
Epoch 1790, training loss: 421.791748046875 = 0.5223343968391418 + 50.0 * 8.42538833618164
Epoch 1790, val loss: 0.6394965648651123
Epoch 1800, training loss: 421.77691650390625 = 0.5211357474327087 + 50.0 * 8.425115585327148
Epoch 1800, val loss: 0.6392111778259277
Epoch 1810, training loss: 421.7675476074219 = 0.5199190378189087 + 50.0 * 8.424952507019043
Epoch 1810, val loss: 0.638864278793335
Epoch 1820, training loss: 422.0113830566406 = 0.5186859965324402 + 50.0 * 8.429854393005371
Epoch 1820, val loss: 0.638511598110199
Epoch 1830, training loss: 421.8344421386719 = 0.5173905491828918 + 50.0 * 8.42634105682373
Epoch 1830, val loss: 0.6380953788757324
Epoch 1840, training loss: 421.7326965332031 = 0.516109824180603 + 50.0 * 8.424331665039062
Epoch 1840, val loss: 0.6377248167991638
Epoch 1850, training loss: 421.69927978515625 = 0.5148430466651917 + 50.0 * 8.423688888549805
Epoch 1850, val loss: 0.6373482346534729
Epoch 1860, training loss: 421.7090148925781 = 0.5135634541511536 + 50.0 * 8.423909187316895
Epoch 1860, val loss: 0.6370356678962708
Epoch 1870, training loss: 421.9833984375 = 0.5122515559196472 + 50.0 * 8.429423332214355
Epoch 1870, val loss: 0.6366970539093018
Epoch 1880, training loss: 421.7190246582031 = 0.5108858942985535 + 50.0 * 8.424162864685059
Epoch 1880, val loss: 0.6361778378486633
Epoch 1890, training loss: 421.6519775390625 = 0.5095484256744385 + 50.0 * 8.42284870147705
Epoch 1890, val loss: 0.6358819603919983
Epoch 1900, training loss: 421.6207275390625 = 0.508231520652771 + 50.0 * 8.422249794006348
Epoch 1900, val loss: 0.6354814767837524
Epoch 1910, training loss: 421.60699462890625 = 0.5069068074226379 + 50.0 * 8.422001838684082
Epoch 1910, val loss: 0.63509601354599
Epoch 1920, training loss: 421.631591796875 = 0.5055580139160156 + 50.0 * 8.422520637512207
Epoch 1920, val loss: 0.6347436904907227
Epoch 1930, training loss: 421.80364990234375 = 0.5041702389717102 + 50.0 * 8.425989151000977
Epoch 1930, val loss: 0.6343279480934143
Epoch 1940, training loss: 421.6262512207031 = 0.5027501583099365 + 50.0 * 8.422470092773438
Epoch 1940, val loss: 0.6338794827461243
Epoch 1950, training loss: 421.5477294921875 = 0.501348614692688 + 50.0 * 8.420928001403809
Epoch 1950, val loss: 0.6334964632987976
Epoch 1960, training loss: 421.52935791015625 = 0.4999462068080902 + 50.0 * 8.420588493347168
Epoch 1960, val loss: 0.6330825686454773
Epoch 1970, training loss: 421.5748291015625 = 0.49853068590164185 + 50.0 * 8.421525955200195
Epoch 1970, val loss: 0.6326321959495544
Epoch 1980, training loss: 421.5777587890625 = 0.4970576763153076 + 50.0 * 8.421613693237305
Epoch 1980, val loss: 0.6322546601295471
Epoch 1990, training loss: 421.5197448730469 = 0.4955700635910034 + 50.0 * 8.420483589172363
Epoch 1990, val loss: 0.6318377256393433
Epoch 2000, training loss: 421.48077392578125 = 0.49408936500549316 + 50.0 * 8.419734001159668
Epoch 2000, val loss: 0.631427526473999
Epoch 2010, training loss: 421.4564208984375 = 0.4926075339317322 + 50.0 * 8.419276237487793
Epoch 2010, val loss: 0.6309460401535034
Epoch 2020, training loss: 421.49664306640625 = 0.49111273884773254 + 50.0 * 8.420110702514648
Epoch 2020, val loss: 0.6305515170097351
Epoch 2030, training loss: 421.6603698730469 = 0.4895772933959961 + 50.0 * 8.423416137695312
Epoch 2030, val loss: 0.6300923228263855
Epoch 2040, training loss: 421.4727478027344 = 0.4880051016807556 + 50.0 * 8.419694900512695
Epoch 2040, val loss: 0.6296354532241821
Epoch 2050, training loss: 421.4029541015625 = 0.48646801710128784 + 50.0 * 8.418329238891602
Epoch 2050, val loss: 0.6291506886482239
Epoch 2060, training loss: 421.375 = 0.4849432408809662 + 50.0 * 8.417800903320312
Epoch 2060, val loss: 0.6287524700164795
Epoch 2070, training loss: 421.34918212890625 = 0.48341187834739685 + 50.0 * 8.417315483093262
Epoch 2070, val loss: 0.6283469796180725
Epoch 2080, training loss: 421.33306884765625 = 0.4818752706050873 + 50.0 * 8.417023658752441
Epoch 2080, val loss: 0.6279798746109009
Epoch 2090, training loss: 421.47381591796875 = 0.4803345203399658 + 50.0 * 8.419869422912598
Epoch 2090, val loss: 0.6276298761367798
Epoch 2100, training loss: 421.3641052246094 = 0.47873806953430176 + 50.0 * 8.417707443237305
Epoch 2100, val loss: 0.6270000338554382
Epoch 2110, training loss: 421.4341735839844 = 0.4771419167518616 + 50.0 * 8.419140815734863
Epoch 2110, val loss: 0.6266031265258789
Epoch 2120, training loss: 421.2929382324219 = 0.4755488634109497 + 50.0 * 8.41634750366211
Epoch 2120, val loss: 0.6261547803878784
Epoch 2130, training loss: 421.2481384277344 = 0.47397324442863464 + 50.0 * 8.415483474731445
Epoch 2130, val loss: 0.6257104873657227
Epoch 2140, training loss: 421.25384521484375 = 0.47240492701530457 + 50.0 * 8.415628433227539
Epoch 2140, val loss: 0.6253650188446045
Epoch 2150, training loss: 421.49371337890625 = 0.47081875801086426 + 50.0 * 8.42045783996582
Epoch 2150, val loss: 0.6248821020126343
Epoch 2160, training loss: 421.2687072753906 = 0.46917665004730225 + 50.0 * 8.415990829467773
Epoch 2160, val loss: 0.6244343519210815
Epoch 2170, training loss: 421.2145080566406 = 0.4675616919994354 + 50.0 * 8.414938926696777
Epoch 2170, val loss: 0.6240875124931335
Epoch 2180, training loss: 421.1819763183594 = 0.46596720814704895 + 50.0 * 8.41431999206543
Epoch 2180, val loss: 0.6236337423324585
Epoch 2190, training loss: 421.1534729003906 = 0.4643695056438446 + 50.0 * 8.413782119750977
Epoch 2190, val loss: 0.623327374458313
Epoch 2200, training loss: 421.3061828613281 = 0.46276718378067017 + 50.0 * 8.416868209838867
Epoch 2200, val loss: 0.6229652762413025
Epoch 2210, training loss: 421.1972961425781 = 0.4611104428768158 + 50.0 * 8.41472339630127
Epoch 2210, val loss: 0.622552216053009
Epoch 2220, training loss: 421.1763000488281 = 0.4594501554965973 + 50.0 * 8.414337158203125
Epoch 2220, val loss: 0.6221075057983398
Epoch 2230, training loss: 421.1099853515625 = 0.45780423283576965 + 50.0 * 8.413043975830078
Epoch 2230, val loss: 0.6217264533042908
Epoch 2240, training loss: 421.0840148925781 = 0.45616260170936584 + 50.0 * 8.412556648254395
Epoch 2240, val loss: 0.6213785409927368
Epoch 2250, training loss: 421.27880859375 = 0.4545198082923889 + 50.0 * 8.416485786437988
Epoch 2250, val loss: 0.6210910081863403
Epoch 2260, training loss: 421.0664978027344 = 0.4528253376483917 + 50.0 * 8.412273406982422
Epoch 2260, val loss: 0.6205750107765198
Epoch 2270, training loss: 421.0148620605469 = 0.4511568546295166 + 50.0 * 8.411273956298828
Epoch 2270, val loss: 0.6201625466346741
Epoch 2280, training loss: 421.0124206542969 = 0.4495165944099426 + 50.0 * 8.41125774383545
Epoch 2280, val loss: 0.6197823286056519
Epoch 2290, training loss: 420.9996032714844 = 0.44788166880607605 + 50.0 * 8.41103458404541
Epoch 2290, val loss: 0.6194055080413818
Epoch 2300, training loss: 421.0536804199219 = 0.44625046849250793 + 50.0 * 8.412148475646973
Epoch 2300, val loss: 0.6189513206481934
Epoch 2310, training loss: 420.99139404296875 = 0.4445701241493225 + 50.0 * 8.41093635559082
Epoch 2310, val loss: 0.6186349987983704
Epoch 2320, training loss: 420.943603515625 = 0.44289135932922363 + 50.0 * 8.410014152526855
Epoch 2320, val loss: 0.6182299256324768
Epoch 2330, training loss: 420.9254455566406 = 0.4412318766117096 + 50.0 * 8.409684181213379
Epoch 2330, val loss: 0.6178690195083618
Epoch 2340, training loss: 420.98004150390625 = 0.43957796692848206 + 50.0 * 8.410809516906738
Epoch 2340, val loss: 0.6174955368041992
Epoch 2350, training loss: 421.09600830078125 = 0.4378848671913147 + 50.0 * 8.413162231445312
Epoch 2350, val loss: 0.6171937584877014
Epoch 2360, training loss: 420.95526123046875 = 0.4361773431301117 + 50.0 * 8.410381317138672
Epoch 2360, val loss: 0.6166753172874451
Epoch 2370, training loss: 420.8875732421875 = 0.43449875712394714 + 50.0 * 8.409061431884766
Epoch 2370, val loss: 0.6162837147712708
Epoch 2380, training loss: 420.85723876953125 = 0.4328337013721466 + 50.0 * 8.408488273620605
Epoch 2380, val loss: 0.615985095500946
Epoch 2390, training loss: 420.826904296875 = 0.43117156624794006 + 50.0 * 8.407914161682129
Epoch 2390, val loss: 0.6156333684921265
Epoch 2400, training loss: 420.8275146484375 = 0.42950356006622314 + 50.0 * 8.407959938049316
Epoch 2400, val loss: 0.6152947545051575
Epoch 2410, training loss: 421.14764404296875 = 0.4278331398963928 + 50.0 * 8.414396286010742
Epoch 2410, val loss: 0.6148932576179504
Epoch 2420, training loss: 420.9728698730469 = 0.42607489228248596 + 50.0 * 8.410935401916504
Epoch 2420, val loss: 0.6146045327186584
Epoch 2430, training loss: 420.7764587402344 = 0.42435136437416077 + 50.0 * 8.407042503356934
Epoch 2430, val loss: 0.6143413782119751
Epoch 2440, training loss: 420.76483154296875 = 0.4226682484149933 + 50.0 * 8.406843185424805
Epoch 2440, val loss: 0.6140412092208862
Epoch 2450, training loss: 420.77105712890625 = 0.42099085450172424 + 50.0 * 8.407001495361328
Epoch 2450, val loss: 0.6138635873794556
Epoch 2460, training loss: 420.9527893066406 = 0.4192977845668793 + 50.0 * 8.410669326782227
Epoch 2460, val loss: 0.6136370897293091
Epoch 2470, training loss: 420.7593994140625 = 0.41756895184516907 + 50.0 * 8.40683650970459
Epoch 2470, val loss: 0.6132690906524658
Epoch 2480, training loss: 420.7119445800781 = 0.41584834456443787 + 50.0 * 8.405921936035156
Epoch 2480, val loss: 0.6129469275474548
Epoch 2490, training loss: 420.6845397949219 = 0.4141401946544647 + 50.0 * 8.405407905578613
Epoch 2490, val loss: 0.6127962470054626
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.763064434297311
0.8152575527059336
=== training gcn model ===
Epoch 0, training loss: 530.2218627929688 = 1.106269359588623 + 50.0 * 10.582311630249023
Epoch 0, val loss: 1.1057438850402832
Epoch 10, training loss: 530.205322265625 = 1.1013612747192383 + 50.0 * 10.58207893371582
Epoch 10, val loss: 1.1008503437042236
Epoch 20, training loss: 530.1417846679688 = 1.0959980487823486 + 50.0 * 10.580915451049805
Epoch 20, val loss: 1.0954879522323608
Epoch 30, training loss: 529.8443603515625 = 1.0901634693145752 + 50.0 * 10.575084686279297
Epoch 30, val loss: 1.0897011756896973
Epoch 40, training loss: 528.6226806640625 = 1.0837504863739014 + 50.0 * 10.550779342651367
Epoch 40, val loss: 1.0833237171173096
Epoch 50, training loss: 524.9716186523438 = 1.0765421390533447 + 50.0 * 10.477901458740234
Epoch 50, val loss: 1.0762369632720947
Epoch 60, training loss: 516.8513793945312 = 1.06973397731781 + 50.0 * 10.315631866455078
Epoch 60, val loss: 1.0697121620178223
Epoch 70, training loss: 506.2539367675781 = 1.0638976097106934 + 50.0 * 10.103800773620605
Epoch 70, val loss: 1.064125895500183
Epoch 80, training loss: 500.7969970703125 = 1.059356451034546 + 50.0 * 9.994752883911133
Epoch 80, val loss: 1.0597376823425293
Epoch 90, training loss: 490.5355529785156 = 1.05521821975708 + 50.0 * 9.789607048034668
Epoch 90, val loss: 1.0554754734039307
Epoch 100, training loss: 479.9917907714844 = 1.0504961013793945 + 50.0 * 9.578825950622559
Epoch 100, val loss: 1.050563931465149
Epoch 110, training loss: 472.2364196777344 = 1.0461809635162354 + 50.0 * 9.423805236816406
Epoch 110, val loss: 1.0462393760681152
Epoch 120, training loss: 464.24407958984375 = 1.042869210243225 + 50.0 * 9.264023780822754
Epoch 120, val loss: 1.043059229850769
Epoch 130, training loss: 460.3614196777344 = 1.0399729013442993 + 50.0 * 9.186429023742676
Epoch 130, val loss: 1.0403361320495605
Epoch 140, training loss: 457.1935119628906 = 1.0372626781463623 + 50.0 * 9.123125076293945
Epoch 140, val loss: 1.037833571434021
Epoch 150, training loss: 452.5254211425781 = 1.0350712537765503 + 50.0 * 9.029807090759277
Epoch 150, val loss: 1.0357544422149658
Epoch 160, training loss: 448.62054443359375 = 1.033289909362793 + 50.0 * 8.95174503326416
Epoch 160, val loss: 1.0339925289154053
Epoch 170, training loss: 446.01092529296875 = 1.0310709476470947 + 50.0 * 8.89959716796875
Epoch 170, val loss: 1.031777024269104
Epoch 180, training loss: 443.6113586425781 = 1.0284157991409302 + 50.0 * 8.851658821105957
Epoch 180, val loss: 1.0291423797607422
Epoch 190, training loss: 441.6282958984375 = 1.0254290103912354 + 50.0 * 8.812057495117188
Epoch 190, val loss: 1.0262030363082886
Epoch 200, training loss: 439.9245910644531 = 1.0222448110580444 + 50.0 * 8.778046607971191
Epoch 200, val loss: 1.0230544805526733
Epoch 210, training loss: 438.57403564453125 = 1.0188382863998413 + 50.0 * 8.751104354858398
Epoch 210, val loss: 1.0196727514266968
Epoch 220, training loss: 437.4455871582031 = 1.0152205228805542 + 50.0 * 8.728607177734375
Epoch 220, val loss: 1.0160632133483887
Epoch 230, training loss: 436.6080017089844 = 1.0113630294799805 + 50.0 * 8.711933135986328
Epoch 230, val loss: 1.0122326612472534
Epoch 240, training loss: 435.8989562988281 = 1.0072523355484009 + 50.0 * 8.697834014892578
Epoch 240, val loss: 1.0081698894500732
Epoch 250, training loss: 435.2414855957031 = 1.0029051303863525 + 50.0 * 8.684771537780762
Epoch 250, val loss: 1.0038866996765137
Epoch 260, training loss: 434.5818176269531 = 0.9983306527137756 + 50.0 * 8.671669960021973
Epoch 260, val loss: 0.9993614554405212
Epoch 270, training loss: 433.9278259277344 = 0.9935115575790405 + 50.0 * 8.658686637878418
Epoch 270, val loss: 0.9946179986000061
Epoch 280, training loss: 433.32183837890625 = 0.9884523749351501 + 50.0 * 8.64666748046875
Epoch 280, val loss: 0.9896224141120911
Epoch 290, training loss: 432.8579406738281 = 0.9831082820892334 + 50.0 * 8.637496948242188
Epoch 290, val loss: 0.9843300580978394
Epoch 300, training loss: 432.4416809082031 = 0.9773532748222351 + 50.0 * 8.629286766052246
Epoch 300, val loss: 0.978649377822876
Epoch 310, training loss: 432.0310363769531 = 0.9712525010108948 + 50.0 * 8.621195793151855
Epoch 310, val loss: 0.9726123213768005
Epoch 320, training loss: 431.6871337890625 = 0.9648436307907104 + 50.0 * 8.614445686340332
Epoch 320, val loss: 0.966290295124054
Epoch 330, training loss: 431.36163330078125 = 0.9581369161605835 + 50.0 * 8.608070373535156
Epoch 330, val loss: 0.9596579670906067
Epoch 340, training loss: 431.05242919921875 = 0.9511495232582092 + 50.0 * 8.602025985717773
Epoch 340, val loss: 0.9527550935745239
Epoch 350, training loss: 430.7753601074219 = 0.9439011812210083 + 50.0 * 8.59662914276123
Epoch 350, val loss: 0.945584774017334
Epoch 360, training loss: 430.4631042480469 = 0.9363508224487305 + 50.0 * 8.590535163879395
Epoch 360, val loss: 0.9380930662155151
Epoch 370, training loss: 430.18731689453125 = 0.9285396337509155 + 50.0 * 8.585175514221191
Epoch 370, val loss: 0.9303569793701172
Epoch 380, training loss: 429.90008544921875 = 0.9205350875854492 + 50.0 * 8.579590797424316
Epoch 380, val loss: 0.9224167466163635
Epoch 390, training loss: 429.64837646484375 = 0.9123321175575256 + 50.0 * 8.574721336364746
Epoch 390, val loss: 0.9142805933952332
Epoch 400, training loss: 429.4471740722656 = 0.903894305229187 + 50.0 * 8.570865631103516
Epoch 400, val loss: 0.9059072136878967
Epoch 410, training loss: 429.03765869140625 = 0.8952820897102356 + 50.0 * 8.562847137451172
Epoch 410, val loss: 0.897355854511261
Epoch 420, training loss: 428.7112731933594 = 0.8865495324134827 + 50.0 * 8.55649471282959
Epoch 420, val loss: 0.8886793851852417
Epoch 430, training loss: 428.4244384765625 = 0.8776856660842896 + 50.0 * 8.550934791564941
Epoch 430, val loss: 0.8798749446868896
Epoch 440, training loss: 428.14276123046875 = 0.8686944246292114 + 50.0 * 8.54548168182373
Epoch 440, val loss: 0.8709398508071899
Epoch 450, training loss: 427.9673156738281 = 0.8595952391624451 + 50.0 * 8.542154312133789
Epoch 450, val loss: 0.861896812915802
Epoch 460, training loss: 427.68988037109375 = 0.8503577709197998 + 50.0 * 8.536789894104004
Epoch 460, val loss: 0.8527294397354126
Epoch 470, training loss: 427.4274597167969 = 0.8411228656768799 + 50.0 * 8.531726837158203
Epoch 470, val loss: 0.8435626029968262
Epoch 480, training loss: 427.21185302734375 = 0.8319308161735535 + 50.0 * 8.52759838104248
Epoch 480, val loss: 0.8344407677650452
Epoch 490, training loss: 427.00457763671875 = 0.822785496711731 + 50.0 * 8.523635864257812
Epoch 490, val loss: 0.8253724575042725
Epoch 500, training loss: 427.02423095703125 = 0.8137059807777405 + 50.0 * 8.524209976196289
Epoch 500, val loss: 0.816375195980072
Epoch 510, training loss: 426.7073059082031 = 0.8046861886978149 + 50.0 * 8.518052101135254
Epoch 510, val loss: 0.8074401617050171
Epoch 520, training loss: 426.5259094238281 = 0.7958281636238098 + 50.0 * 8.514601707458496
Epoch 520, val loss: 0.7987013459205627
Epoch 530, training loss: 426.3829650878906 = 0.7871726155281067 + 50.0 * 8.511916160583496
Epoch 530, val loss: 0.7901636362075806
Epoch 540, training loss: 426.2720642089844 = 0.7787086367607117 + 50.0 * 8.509866714477539
Epoch 540, val loss: 0.7818345427513123
Epoch 550, training loss: 426.2008972167969 = 0.7704294919967651 + 50.0 * 8.508609771728516
Epoch 550, val loss: 0.7737112641334534
Epoch 560, training loss: 426.0687561035156 = 0.7623937129974365 + 50.0 * 8.50612735748291
Epoch 560, val loss: 0.7658361196517944
Epoch 570, training loss: 425.8863220214844 = 0.754663348197937 + 50.0 * 8.502633094787598
Epoch 570, val loss: 0.7583080530166626
Epoch 580, training loss: 425.7794494628906 = 0.7472295761108398 + 50.0 * 8.50064468383789
Epoch 580, val loss: 0.7510828971862793
Epoch 590, training loss: 425.71612548828125 = 0.7400636076927185 + 50.0 * 8.499521255493164
Epoch 590, val loss: 0.744145929813385
Epoch 600, training loss: 425.5504455566406 = 0.733121395111084 + 50.0 * 8.496346473693848
Epoch 600, val loss: 0.7374772429466248
Epoch 610, training loss: 425.47039794921875 = 0.7264769077301025 + 50.0 * 8.494878768920898
Epoch 610, val loss: 0.731104850769043
Epoch 620, training loss: 425.3238525390625 = 0.7201414108276367 + 50.0 * 8.492074012756348
Epoch 620, val loss: 0.725082516670227
Epoch 630, training loss: 425.2156677246094 = 0.7140967845916748 + 50.0 * 8.490031242370605
Epoch 630, val loss: 0.7193767428398132
Epoch 640, training loss: 425.1302795410156 = 0.7083145976066589 + 50.0 * 8.488439559936523
Epoch 640, val loss: 0.7139509916305542
Epoch 650, training loss: 425.0868225097656 = 0.7027538418769836 + 50.0 * 8.48768138885498
Epoch 650, val loss: 0.7087823152542114
Epoch 660, training loss: 425.0379943847656 = 0.6974031925201416 + 50.0 * 8.486811637878418
Epoch 660, val loss: 0.7038285732269287
Epoch 670, training loss: 424.88092041015625 = 0.6923147439956665 + 50.0 * 8.483772277832031
Epoch 670, val loss: 0.6991735100746155
Epoch 680, training loss: 424.7578430175781 = 0.6874740719795227 + 50.0 * 8.481407165527344
Epoch 680, val loss: 0.6947870850563049
Epoch 690, training loss: 424.7580871582031 = 0.6828595399856567 + 50.0 * 8.481504440307617
Epoch 690, val loss: 0.6906493306159973
Epoch 700, training loss: 424.6336364746094 = 0.6784319877624512 + 50.0 * 8.479104042053223
Epoch 700, val loss: 0.6867958307266235
Epoch 710, training loss: 424.47998046875 = 0.6742382645606995 + 50.0 * 8.476115226745605
Epoch 710, val loss: 0.6831730604171753
Epoch 720, training loss: 424.4508361816406 = 0.670261800289154 + 50.0 * 8.475611686706543
Epoch 720, val loss: 0.6797872185707092
Epoch 730, training loss: 424.3222351074219 = 0.6664584875106812 + 50.0 * 8.473114967346191
Epoch 730, val loss: 0.676613450050354
Epoch 740, training loss: 424.2483825683594 = 0.6628641486167908 + 50.0 * 8.471710205078125
Epoch 740, val loss: 0.6736449599266052
Epoch 750, training loss: 424.1319580078125 = 0.6594476699829102 + 50.0 * 8.469449996948242
Epoch 750, val loss: 0.6709132790565491
Epoch 760, training loss: 424.1484680175781 = 0.6562037467956543 + 50.0 * 8.46984577178955
Epoch 760, val loss: 0.6683662533760071
Epoch 770, training loss: 424.12042236328125 = 0.6531051397323608 + 50.0 * 8.469346046447754
Epoch 770, val loss: 0.6660031080245972
Epoch 780, training loss: 423.91522216796875 = 0.6501321196556091 + 50.0 * 8.465301513671875
Epoch 780, val loss: 0.6637254953384399
Epoch 790, training loss: 423.82720947265625 = 0.6473371386528015 + 50.0 * 8.463597297668457
Epoch 790, val loss: 0.6616781949996948
Epoch 800, training loss: 423.744384765625 = 0.6447086334228516 + 50.0 * 8.461993217468262
Epoch 800, val loss: 0.6597875356674194
Epoch 810, training loss: 423.6678771972656 = 0.6421976089477539 + 50.0 * 8.4605131149292
Epoch 810, val loss: 0.6580415368080139
Epoch 820, training loss: 423.5866394042969 = 0.6398068070411682 + 50.0 * 8.45893669128418
Epoch 820, val loss: 0.6564315557479858
Epoch 830, training loss: 423.5677490234375 = 0.6375201344490051 + 50.0 * 8.45860481262207
Epoch 830, val loss: 0.6549359560012817
Epoch 840, training loss: 423.6328430175781 = 0.6352791786193848 + 50.0 * 8.459951400756836
Epoch 840, val loss: 0.653436005115509
Epoch 850, training loss: 423.4349670410156 = 0.6330965757369995 + 50.0 * 8.456037521362305
Epoch 850, val loss: 0.6520916223526001
Epoch 860, training loss: 423.33941650390625 = 0.6310485005378723 + 50.0 * 8.454167366027832
Epoch 860, val loss: 0.6508344411849976
Epoch 870, training loss: 423.2713623046875 = 0.6291135549545288 + 50.0 * 8.452844619750977
Epoch 870, val loss: 0.6497209668159485
Epoch 880, training loss: 423.20281982421875 = 0.6272559762001038 + 50.0 * 8.45151138305664
Epoch 880, val loss: 0.6486824750900269
Epoch 890, training loss: 423.146728515625 = 0.6254565119743347 + 50.0 * 8.450425148010254
Epoch 890, val loss: 0.6476972699165344
Epoch 900, training loss: 423.2048645019531 = 0.6237089037895203 + 50.0 * 8.45162296295166
Epoch 900, val loss: 0.6467426419258118
Epoch 910, training loss: 423.03924560546875 = 0.6219399571418762 + 50.0 * 8.448346138000488
Epoch 910, val loss: 0.6458591222763062
Epoch 920, training loss: 423.03765869140625 = 0.6202633380889893 + 50.0 * 8.448348045349121
Epoch 920, val loss: 0.6449549794197083
Epoch 930, training loss: 422.9627990722656 = 0.6186519265174866 + 50.0 * 8.446883201599121
Epoch 930, val loss: 0.6442030668258667
Epoch 940, training loss: 422.909912109375 = 0.6171101927757263 + 50.0 * 8.445856094360352
Epoch 940, val loss: 0.6434590220451355
Epoch 950, training loss: 422.9814758300781 = 0.6155713200569153 + 50.0 * 8.447318077087402
Epoch 950, val loss: 0.6427633762359619
Epoch 960, training loss: 422.8312072753906 = 0.6140637993812561 + 50.0 * 8.444342613220215
Epoch 960, val loss: 0.6420480012893677
Epoch 970, training loss: 422.7515869140625 = 0.6126161217689514 + 50.0 * 8.442779541015625
Epoch 970, val loss: 0.6414307951927185
Epoch 980, training loss: 422.701171875 = 0.6112176179885864 + 50.0 * 8.44179916381836
Epoch 980, val loss: 0.6408483386039734
Epoch 990, training loss: 422.6540832519531 = 0.6098528504371643 + 50.0 * 8.440884590148926
Epoch 990, val loss: 0.6402927041053772
Epoch 1000, training loss: 422.61370849609375 = 0.6085118651390076 + 50.0 * 8.440103530883789
Epoch 1000, val loss: 0.6397625207901001
Epoch 1010, training loss: 423.4412536621094 = 0.6071587800979614 + 50.0 * 8.456682205200195
Epoch 1010, val loss: 0.6392924189567566
Epoch 1020, training loss: 422.5535583496094 = 0.6057488918304443 + 50.0 * 8.438956260681152
Epoch 1020, val loss: 0.6386386156082153
Epoch 1030, training loss: 422.5008239746094 = 0.604449987411499 + 50.0 * 8.43792724609375
Epoch 1030, val loss: 0.6380869746208191
Epoch 1040, training loss: 422.4724426269531 = 0.6032177805900574 + 50.0 * 8.437384605407715
Epoch 1040, val loss: 0.6376327276229858
Epoch 1050, training loss: 422.4394836425781 = 0.6020142436027527 + 50.0 * 8.436749458312988
Epoch 1050, val loss: 0.6371903419494629
Epoch 1060, training loss: 422.3990173339844 = 0.6008222699165344 + 50.0 * 8.43596363067627
Epoch 1060, val loss: 0.6367653608322144
Epoch 1070, training loss: 422.3659362792969 = 0.5996354222297668 + 50.0 * 8.435325622558594
Epoch 1070, val loss: 0.6363544464111328
Epoch 1080, training loss: 422.3357849121094 = 0.5984566807746887 + 50.0 * 8.434746742248535
Epoch 1080, val loss: 0.6359356045722961
Epoch 1090, training loss: 422.3547058105469 = 0.5972851514816284 + 50.0 * 8.435148239135742
Epoch 1090, val loss: 0.6355143785476685
Epoch 1100, training loss: 422.29833984375 = 0.5960865020751953 + 50.0 * 8.43404483795166
Epoch 1100, val loss: 0.6350879669189453
Epoch 1110, training loss: 422.30145263671875 = 0.5949072241783142 + 50.0 * 8.434130668640137
Epoch 1110, val loss: 0.6346291303634644
Epoch 1120, training loss: 422.2413330078125 = 0.5937752723693848 + 50.0 * 8.432950973510742
Epoch 1120, val loss: 0.6342522501945496
Epoch 1130, training loss: 422.20831298828125 = 0.5926614999771118 + 50.0 * 8.432312965393066
Epoch 1130, val loss: 0.6338858604431152
Epoch 1140, training loss: 422.1835632324219 = 0.5915621519088745 + 50.0 * 8.431839942932129
Epoch 1140, val loss: 0.6335093975067139
Epoch 1150, training loss: 422.54827880859375 = 0.5904509425163269 + 50.0 * 8.439156532287598
Epoch 1150, val loss: 0.6330986022949219
Epoch 1160, training loss: 422.14599609375 = 0.589305579662323 + 50.0 * 8.431134223937988
Epoch 1160, val loss: 0.6326630115509033
Epoch 1170, training loss: 422.131591796875 = 0.5882179737091064 + 50.0 * 8.430867195129395
Epoch 1170, val loss: 0.6323051452636719
Epoch 1180, training loss: 422.0992736816406 = 0.5871569514274597 + 50.0 * 8.430242538452148
Epoch 1180, val loss: 0.6319653987884521
Epoch 1190, training loss: 422.0742492675781 = 0.5861105918884277 + 50.0 * 8.429762840270996
Epoch 1190, val loss: 0.6316055059432983
Epoch 1200, training loss: 422.48101806640625 = 0.5850697755813599 + 50.0 * 8.437918663024902
Epoch 1200, val loss: 0.6311782002449036
Epoch 1210, training loss: 422.1910705566406 = 0.5839284658432007 + 50.0 * 8.432143211364746
Epoch 1210, val loss: 0.6308349370956421
Epoch 1220, training loss: 422.0152893066406 = 0.5828853249549866 + 50.0 * 8.428647994995117
Epoch 1220, val loss: 0.630418598651886
Epoch 1230, training loss: 421.9696350097656 = 0.581864595413208 + 50.0 * 8.427755355834961
Epoch 1230, val loss: 0.6300698518753052
Epoch 1240, training loss: 421.9460754394531 = 0.5808625817298889 + 50.0 * 8.4273042678833
Epoch 1240, val loss: 0.6297533512115479
Epoch 1250, training loss: 421.9217529296875 = 0.5798666477203369 + 50.0 * 8.426837921142578
Epoch 1250, val loss: 0.6294158697128296
Epoch 1260, training loss: 421.9039611816406 = 0.5788766741752625 + 50.0 * 8.426501274108887
Epoch 1260, val loss: 0.6290825009346008
Epoch 1270, training loss: 422.09075927734375 = 0.5778921842575073 + 50.0 * 8.430257797241211
Epoch 1270, val loss: 0.6287729144096375
Epoch 1280, training loss: 421.95904541015625 = 0.5768416523933411 + 50.0 * 8.427643775939941
Epoch 1280, val loss: 0.6283305287361145
Epoch 1290, training loss: 421.8299560546875 = 0.5758488774299622 + 50.0 * 8.425082206726074
Epoch 1290, val loss: 0.6280125975608826
Epoch 1300, training loss: 421.90704345703125 = 0.5748843550682068 + 50.0 * 8.426643371582031
Epoch 1300, val loss: 0.6276745200157166
Epoch 1310, training loss: 421.9017028808594 = 0.5738781690597534 + 50.0 * 8.426556587219238
Epoch 1310, val loss: 0.6273238658905029
Epoch 1320, training loss: 421.7826843261719 = 0.5728714466094971 + 50.0 * 8.424196243286133
Epoch 1320, val loss: 0.6269664764404297
Epoch 1330, training loss: 421.74249267578125 = 0.5719209313392639 + 50.0 * 8.42341136932373
Epoch 1330, val loss: 0.6266565918922424
Epoch 1340, training loss: 421.70440673828125 = 0.5709910988807678 + 50.0 * 8.42266845703125
Epoch 1340, val loss: 0.6263304352760315
Epoch 1350, training loss: 421.7012023925781 = 0.5700631737709045 + 50.0 * 8.422622680664062
Epoch 1350, val loss: 0.6260038018226624
Epoch 1360, training loss: 422.00653076171875 = 0.5691065788269043 + 50.0 * 8.428749084472656
Epoch 1360, val loss: 0.6256161332130432
Epoch 1370, training loss: 421.7023620605469 = 0.5680847764015198 + 50.0 * 8.422685623168945
Epoch 1370, val loss: 0.6252931356430054
Epoch 1380, training loss: 421.5979919433594 = 0.5671482086181641 + 50.0 * 8.42061710357666
Epoch 1380, val loss: 0.6249318718910217
Epoch 1390, training loss: 421.55767822265625 = 0.5662268996238708 + 50.0 * 8.419829368591309
Epoch 1390, val loss: 0.6246377825737
Epoch 1400, training loss: 421.53155517578125 = 0.5653194785118103 + 50.0 * 8.41932487487793
Epoch 1400, val loss: 0.6243171691894531
Epoch 1410, training loss: 421.7098693847656 = 0.5644049644470215 + 50.0 * 8.4229097366333
Epoch 1410, val loss: 0.6240025162696838
Epoch 1420, training loss: 421.5970764160156 = 0.563418984413147 + 50.0 * 8.420673370361328
Epoch 1420, val loss: 0.6236078143119812
Epoch 1430, training loss: 421.4571533203125 = 0.5624741315841675 + 50.0 * 8.417893409729004
Epoch 1430, val loss: 0.6232880353927612
Epoch 1440, training loss: 421.43304443359375 = 0.5615590214729309 + 50.0 * 8.41742992401123
Epoch 1440, val loss: 0.6229816675186157
Epoch 1450, training loss: 421.5995178222656 = 0.5606363415718079 + 50.0 * 8.420777320861816
Epoch 1450, val loss: 0.6226953268051147
Epoch 1460, training loss: 421.3994445800781 = 0.5596780776977539 + 50.0 * 8.416794776916504
Epoch 1460, val loss: 0.6222285628318787
Epoch 1470, training loss: 421.3588562011719 = 0.5587480068206787 + 50.0 * 8.41600227355957
Epoch 1470, val loss: 0.6219132542610168
Epoch 1480, training loss: 421.2925109863281 = 0.5578368306159973 + 50.0 * 8.414693832397461
Epoch 1480, val loss: 0.6215632557868958
Epoch 1490, training loss: 421.28021240234375 = 0.5569278597831726 + 50.0 * 8.41446590423584
Epoch 1490, val loss: 0.6212270855903625
Epoch 1500, training loss: 421.4201965332031 = 0.5560035109519958 + 50.0 * 8.41728401184082
Epoch 1500, val loss: 0.6209120154380798
Epoch 1510, training loss: 421.2380676269531 = 0.5550386905670166 + 50.0 * 8.413660049438477
Epoch 1510, val loss: 0.6204976439476013
Epoch 1520, training loss: 421.18743896484375 = 0.5540921092033386 + 50.0 * 8.412667274475098
Epoch 1520, val loss: 0.6200710535049438
Epoch 1530, training loss: 421.1614074707031 = 0.5531760454177856 + 50.0 * 8.412164688110352
Epoch 1530, val loss: 0.619755208492279
Epoch 1540, training loss: 421.1412048339844 = 0.5522684454917908 + 50.0 * 8.411778450012207
Epoch 1540, val loss: 0.6194047331809998
Epoch 1550, training loss: 421.2995910644531 = 0.5513461232185364 + 50.0 * 8.41496467590332
Epoch 1550, val loss: 0.61899733543396
Epoch 1560, training loss: 421.11138916015625 = 0.5503365993499756 + 50.0 * 8.411221504211426
Epoch 1560, val loss: 0.6186152696609497
Epoch 1570, training loss: 421.09027099609375 = 0.5493821501731873 + 50.0 * 8.410818099975586
Epoch 1570, val loss: 0.6182218194007874
Epoch 1580, training loss: 421.0450744628906 = 0.5484381914138794 + 50.0 * 8.409933090209961
Epoch 1580, val loss: 0.6178252696990967
Epoch 1590, training loss: 421.1910400390625 = 0.5475066304206848 + 50.0 * 8.412870407104492
Epoch 1590, val loss: 0.617419421672821
Epoch 1600, training loss: 420.9841613769531 = 0.5464916229248047 + 50.0 * 8.408753395080566
Epoch 1600, val loss: 0.6169877052307129
Epoch 1610, training loss: 420.97247314453125 = 0.5455216765403748 + 50.0 * 8.408538818359375
Epoch 1610, val loss: 0.6166089177131653
Epoch 1620, training loss: 420.93316650390625 = 0.544567346572876 + 50.0 * 8.407772064208984
Epoch 1620, val loss: 0.6161798238754272
Epoch 1630, training loss: 420.9570007324219 = 0.5436161756515503 + 50.0 * 8.408267974853516
Epoch 1630, val loss: 0.6157840490341187
Epoch 1640, training loss: 420.97845458984375 = 0.5426249504089355 + 50.0 * 8.408716201782227
Epoch 1640, val loss: 0.615364134311676
Epoch 1650, training loss: 420.88897705078125 = 0.541617751121521 + 50.0 * 8.406947135925293
Epoch 1650, val loss: 0.6149285435676575
Epoch 1660, training loss: 420.83966064453125 = 0.5406252145767212 + 50.0 * 8.405981063842773
Epoch 1660, val loss: 0.6144343614578247
Epoch 1670, training loss: 420.82086181640625 = 0.5396568179130554 + 50.0 * 8.405624389648438
Epoch 1670, val loss: 0.614022970199585
Epoch 1680, training loss: 421.0185241699219 = 0.5386737585067749 + 50.0 * 8.409597396850586
Epoch 1680, val loss: 0.6135179996490479
Epoch 1690, training loss: 420.7690734863281 = 0.5375983119010925 + 50.0 * 8.404629707336426
Epoch 1690, val loss: 0.6131353378295898
Epoch 1700, training loss: 420.7510681152344 = 0.5365565419197083 + 50.0 * 8.404290199279785
Epoch 1700, val loss: 0.6125897765159607
Epoch 1710, training loss: 420.7279968261719 = 0.5355479121208191 + 50.0 * 8.403848648071289
Epoch 1710, val loss: 0.6121795773506165
Epoch 1720, training loss: 420.6991882324219 = 0.5345309376716614 + 50.0 * 8.403292655944824
Epoch 1720, val loss: 0.6116971373558044
Epoch 1730, training loss: 420.6872253417969 = 0.5335036516189575 + 50.0 * 8.403074264526367
Epoch 1730, val loss: 0.6111863255500793
Epoch 1740, training loss: 421.1257019042969 = 0.5324532389640808 + 50.0 * 8.411865234375
Epoch 1740, val loss: 0.610666811466217
Epoch 1750, training loss: 420.7360534667969 = 0.5313172936439514 + 50.0 * 8.404094696044922
Epoch 1750, val loss: 0.6101003289222717
Epoch 1760, training loss: 420.6448669433594 = 0.5302286148071289 + 50.0 * 8.402292251586914
Epoch 1760, val loss: 0.6095831394195557
Epoch 1770, training loss: 420.6175231933594 = 0.529167890548706 + 50.0 * 8.401766777038574
Epoch 1770, val loss: 0.6090515851974487
Epoch 1780, training loss: 420.83172607421875 = 0.5281037092208862 + 50.0 * 8.406072616577148
Epoch 1780, val loss: 0.6084540486335754
Epoch 1790, training loss: 420.568603515625 = 0.5269321799278259 + 50.0 * 8.400833129882812
Epoch 1790, val loss: 0.6079176068305969
Epoch 1800, training loss: 420.54486083984375 = 0.5258031487464905 + 50.0 * 8.400381088256836
Epoch 1800, val loss: 0.6073049902915955
Epoch 1810, training loss: 420.5211486816406 = 0.5246882438659668 + 50.0 * 8.39992904663086
Epoch 1810, val loss: 0.6067119240760803
Epoch 1820, training loss: 420.5022888183594 = 0.5235756635665894 + 50.0 * 8.399574279785156
Epoch 1820, val loss: 0.6061402559280396
Epoch 1830, training loss: 420.4884338378906 = 0.5224519968032837 + 50.0 * 8.399319648742676
Epoch 1830, val loss: 0.6055414080619812
Epoch 1840, training loss: 420.49639892578125 = 0.5213029980659485 + 50.0 * 8.39950180053711
Epoch 1840, val loss: 0.6049413681030273
Epoch 1850, training loss: 420.64501953125 = 0.5201107263565063 + 50.0 * 8.402498245239258
Epoch 1850, val loss: 0.6043150424957275
Epoch 1860, training loss: 420.51434326171875 = 0.5188582539558411 + 50.0 * 8.399909973144531
Epoch 1860, val loss: 0.6035423278808594
Epoch 1870, training loss: 420.4391784667969 = 0.5176414251327515 + 50.0 * 8.398430824279785
Epoch 1870, val loss: 0.6027997732162476
Epoch 1880, training loss: 420.4031066894531 = 0.5164324641227722 + 50.0 * 8.397733688354492
Epoch 1880, val loss: 0.6021710634231567
Epoch 1890, training loss: 420.45928955078125 = 0.5152134895324707 + 50.0 * 8.398880958557129
Epoch 1890, val loss: 0.6014370322227478
Epoch 1900, training loss: 420.4947204589844 = 0.5139565467834473 + 50.0 * 8.399615287780762
Epoch 1900, val loss: 0.6007121205329895
Epoch 1910, training loss: 420.3545837402344 = 0.5126489996910095 + 50.0 * 8.396839141845703
Epoch 1910, val loss: 0.5999999046325684
Epoch 1920, training loss: 420.35711669921875 = 0.5113679766654968 + 50.0 * 8.3969144821167
Epoch 1920, val loss: 0.599303662776947
Epoch 1930, training loss: 420.3345642089844 = 0.5100850462913513 + 50.0 * 8.396490097045898
Epoch 1930, val loss: 0.5985720753669739
Epoch 1940, training loss: 420.3409118652344 = 0.5087864398956299 + 50.0 * 8.396642684936523
Epoch 1940, val loss: 0.597847580909729
Epoch 1950, training loss: 420.4804382324219 = 0.5074554681777954 + 50.0 * 8.399459838867188
Epoch 1950, val loss: 0.5971084833145142
Epoch 1960, training loss: 420.27105712890625 = 0.5060845017433167 + 50.0 * 8.395299911499023
Epoch 1960, val loss: 0.5962146520614624
Epoch 1970, training loss: 420.2948913574219 = 0.5047354102134705 + 50.0 * 8.395803451538086
Epoch 1970, val loss: 0.595389723777771
Epoch 1980, training loss: 420.2464599609375 = 0.5033729672431946 + 50.0 * 8.394861221313477
Epoch 1980, val loss: 0.594619631767273
Epoch 1990, training loss: 420.2334289550781 = 0.5020058155059814 + 50.0 * 8.394628524780273
Epoch 1990, val loss: 0.5938273668289185
Epoch 2000, training loss: 420.3946838378906 = 0.5006155371665955 + 50.0 * 8.397881507873535
Epoch 2000, val loss: 0.5929805040359497
Epoch 2010, training loss: 420.2879943847656 = 0.4991334080696106 + 50.0 * 8.395776748657227
Epoch 2010, val loss: 0.5920283794403076
Epoch 2020, training loss: 420.3351745605469 = 0.4976580739021301 + 50.0 * 8.396750450134277
Epoch 2020, val loss: 0.5911055207252502
Epoch 2030, training loss: 420.1938171386719 = 0.49618905782699585 + 50.0 * 8.393952369689941
Epoch 2030, val loss: 0.5902491807937622
Epoch 2040, training loss: 420.17974853515625 = 0.4947340190410614 + 50.0 * 8.39370059967041
Epoch 2040, val loss: 0.5893785357475281
Epoch 2050, training loss: 420.1549377441406 = 0.49327173829078674 + 50.0 * 8.393233299255371
Epoch 2050, val loss: 0.5884801745414734
Epoch 2060, training loss: 420.14764404296875 = 0.4917871952056885 + 50.0 * 8.39311695098877
Epoch 2060, val loss: 0.5875294208526611
Epoch 2070, training loss: 420.70684814453125 = 0.4902836084365845 + 50.0 * 8.40433120727539
Epoch 2070, val loss: 0.5864522457122803
Epoch 2080, training loss: 420.3901062011719 = 0.4886356294155121 + 50.0 * 8.398029327392578
Epoch 2080, val loss: 0.5856606364250183
Epoch 2090, training loss: 420.14434814453125 = 0.4870544373989105 + 50.0 * 8.393145561218262
Epoch 2090, val loss: 0.5845890641212463
Epoch 2100, training loss: 420.1084899902344 = 0.48551303148269653 + 50.0 * 8.392459869384766
Epoch 2100, val loss: 0.5836336016654968
Epoch 2110, training loss: 420.0976257324219 = 0.48397529125213623 + 50.0 * 8.39227294921875
Epoch 2110, val loss: 0.5827675461769104
Epoch 2120, training loss: 420.07757568359375 = 0.48241961002349854 + 50.0 * 8.391902923583984
Epoch 2120, val loss: 0.5817935466766357
Epoch 2130, training loss: 420.0638732910156 = 0.48083412647247314 + 50.0 * 8.391660690307617
Epoch 2130, val loss: 0.5808451175689697
Epoch 2140, training loss: 420.0525817871094 = 0.47922831773757935 + 50.0 * 8.391467094421387
Epoch 2140, val loss: 0.579841673374176
Epoch 2150, training loss: 420.0526123046875 = 0.47760310769081116 + 50.0 * 8.391500473022461
Epoch 2150, val loss: 0.5788530707359314
Epoch 2160, training loss: 420.4013671875 = 0.4759409427642822 + 50.0 * 8.398508071899414
Epoch 2160, val loss: 0.5778288841247559
Epoch 2170, training loss: 420.131103515625 = 0.47424304485321045 + 50.0 * 8.393136978149414
Epoch 2170, val loss: 0.5768334865570068
Epoch 2180, training loss: 420.0378723144531 = 0.47254738211631775 + 50.0 * 8.39130687713623
Epoch 2180, val loss: 0.5757576823234558
Epoch 2190, training loss: 420.0279235839844 = 0.4708883762359619 + 50.0 * 8.391140937805176
Epoch 2190, val loss: 0.5747564435005188
Epoch 2200, training loss: 420.04608154296875 = 0.46922704577445984 + 50.0 * 8.391536712646484
Epoch 2200, val loss: 0.5737823843955994
Epoch 2210, training loss: 419.9967041015625 = 0.4675458073616028 + 50.0 * 8.390583038330078
Epoch 2210, val loss: 0.5727813839912415
Epoch 2220, training loss: 419.9775390625 = 0.465859979391098 + 50.0 * 8.390233993530273
Epoch 2220, val loss: 0.5717757940292358
Epoch 2230, training loss: 420.150146484375 = 0.46415963768959045 + 50.0 * 8.393719673156738
Epoch 2230, val loss: 0.5706976056098938
Epoch 2240, training loss: 420.0113830566406 = 0.4623884856700897 + 50.0 * 8.390979766845703
Epoch 2240, val loss: 0.5698362588882446
Epoch 2250, training loss: 419.9745788574219 = 0.4606279134750366 + 50.0 * 8.390278816223145
Epoch 2250, val loss: 0.5687755942344666
Epoch 2260, training loss: 419.94879150390625 = 0.45892295241355896 + 50.0 * 8.38979721069336
Epoch 2260, val loss: 0.5678869485855103
Epoch 2270, training loss: 419.92974853515625 = 0.45723506808280945 + 50.0 * 8.389450073242188
Epoch 2270, val loss: 0.5669914484024048
Epoch 2280, training loss: 419.9543762207031 = 0.4555434286594391 + 50.0 * 8.389976501464844
Epoch 2280, val loss: 0.5660641193389893
Epoch 2290, training loss: 420.03558349609375 = 0.45381104946136475 + 50.0 * 8.39163589477539
Epoch 2290, val loss: 0.5651205778121948
Epoch 2300, training loss: 419.90863037109375 = 0.45206815004348755 + 50.0 * 8.389131546020508
Epoch 2300, val loss: 0.5641972422599792
Epoch 2310, training loss: 419.8800354003906 = 0.4503575265407562 + 50.0 * 8.388593673706055
Epoch 2310, val loss: 0.5632449388504028
Epoch 2320, training loss: 419.8802185058594 = 0.4486561119556427 + 50.0 * 8.388630867004395
Epoch 2320, val loss: 0.562335729598999
Epoch 2330, training loss: 420.0457763671875 = 0.4469521641731262 + 50.0 * 8.391976356506348
Epoch 2330, val loss: 0.5613505840301514
Epoch 2340, training loss: 419.8543701171875 = 0.4452022314071655 + 50.0 * 8.38818359375
Epoch 2340, val loss: 0.5605402588844299
Epoch 2350, training loss: 419.8780822753906 = 0.44347989559173584 + 50.0 * 8.388691902160645
Epoch 2350, val loss: 0.5596860647201538
Epoch 2360, training loss: 419.84283447265625 = 0.44177669286727905 + 50.0 * 8.388021469116211
Epoch 2360, val loss: 0.558736264705658
Epoch 2370, training loss: 419.825439453125 = 0.44008752703666687 + 50.0 * 8.387706756591797
Epoch 2370, val loss: 0.5578913688659668
Epoch 2380, training loss: 420.0508728027344 = 0.43839505314826965 + 50.0 * 8.392250061035156
Epoch 2380, val loss: 0.5571045875549316
Epoch 2390, training loss: 419.8700866699219 = 0.4366632103919983 + 50.0 * 8.388668060302734
Epoch 2390, val loss: 0.5561662316322327
Epoch 2400, training loss: 419.8042907714844 = 0.4349595904350281 + 50.0 * 8.387386322021484
Epoch 2400, val loss: 0.5553974509239197
Epoch 2410, training loss: 419.77587890625 = 0.4332733154296875 + 50.0 * 8.386852264404297
Epoch 2410, val loss: 0.5545746684074402
Epoch 2420, training loss: 419.7929992675781 = 0.4315958321094513 + 50.0 * 8.387228012084961
Epoch 2420, val loss: 0.5538302659988403
Epoch 2430, training loss: 420.2826843261719 = 0.4299076497554779 + 50.0 * 8.397055625915527
Epoch 2430, val loss: 0.5531541109085083
Epoch 2440, training loss: 419.8901672363281 = 0.4281606674194336 + 50.0 * 8.389240264892578
Epoch 2440, val loss: 0.5521653890609741
Epoch 2450, training loss: 419.7637023925781 = 0.42648792266845703 + 50.0 * 8.386744499206543
Epoch 2450, val loss: 0.5515254139900208
Epoch 2460, training loss: 419.7300109863281 = 0.42485594749450684 + 50.0 * 8.386102676391602
Epoch 2460, val loss: 0.5507883429527283
Epoch 2470, training loss: 419.7141418457031 = 0.42323198914527893 + 50.0 * 8.385818481445312
Epoch 2470, val loss: 0.5500901937484741
Epoch 2480, training loss: 419.744873046875 = 0.4216209650039673 + 50.0 * 8.386465072631836
Epoch 2480, val loss: 0.5493990182876587
Epoch 2490, training loss: 419.8946838378906 = 0.41998034715652466 + 50.0 * 8.389493942260742
Epoch 2490, val loss: 0.5486491918563843
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7823439878234398
0.8133739042237196
The final CL Acc:0.77237, 0.00789, The final GNN Acc:0.81562, 0.00200
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110816])
remove edge: torch.Size([2, 66408])
updated graph: torch.Size([2, 88576])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 530.209228515625 = 1.093509316444397 + 50.0 * 10.582314491271973
Epoch 0, val loss: 1.093260645866394
Epoch 10, training loss: 530.1929321289062 = 1.0895841121673584 + 50.0 * 10.582066535949707
Epoch 10, val loss: 1.0893279314041138
Epoch 20, training loss: 530.1339111328125 = 1.0854402780532837 + 50.0 * 10.580968856811523
Epoch 20, val loss: 1.0851895809173584
Epoch 30, training loss: 529.888427734375 = 1.0809277296066284 + 50.0 * 10.576149940490723
Epoch 30, val loss: 1.0806841850280762
Epoch 40, training loss: 528.8832397460938 = 1.0759332180023193 + 50.0 * 10.556146621704102
Epoch 40, val loss: 1.0756921768188477
Epoch 50, training loss: 525.468994140625 = 1.0702956914901733 + 50.0 * 10.487974166870117
Epoch 50, val loss: 1.070054292678833
Epoch 60, training loss: 516.316162109375 = 1.0641968250274658 + 50.0 * 10.305038452148438
Epoch 60, val loss: 1.0639857053756714
Epoch 70, training loss: 497.9565734863281 = 1.0574884414672852 + 50.0 * 9.937981605529785
Epoch 70, val loss: 1.057306170463562
Epoch 80, training loss: 479.4874572753906 = 1.0500894784927368 + 50.0 * 9.568747520446777
Epoch 80, val loss: 1.05009925365448
Epoch 90, training loss: 474.44134521484375 = 1.0428470373153687 + 50.0 * 9.46796989440918
Epoch 90, val loss: 1.0431171655654907
Epoch 100, training loss: 469.7872009277344 = 1.0355253219604492 + 50.0 * 9.375033378601074
Epoch 100, val loss: 1.036064624786377
Epoch 110, training loss: 465.52099609375 = 1.0283708572387695 + 50.0 * 9.289852142333984
Epoch 110, val loss: 1.0291892290115356
Epoch 120, training loss: 462.7175598144531 = 1.021970272064209 + 50.0 * 9.233911514282227
Epoch 120, val loss: 1.0230039358139038
Epoch 130, training loss: 461.40093994140625 = 1.01631498336792 + 50.0 * 9.20769214630127
Epoch 130, val loss: 1.0174554586410522
Epoch 140, training loss: 460.5754089355469 = 1.0106252431869507 + 50.0 * 9.191295623779297
Epoch 140, val loss: 1.0118374824523926
Epoch 150, training loss: 459.6541442871094 = 1.0045151710510254 + 50.0 * 9.172992706298828
Epoch 150, val loss: 1.0059027671813965
Epoch 160, training loss: 458.3199768066406 = 0.9982290863990784 + 50.0 * 9.146434783935547
Epoch 160, val loss: 0.9998792409896851
Epoch 170, training loss: 456.130859375 = 0.9922279715538025 + 50.0 * 9.10277271270752
Epoch 170, val loss: 0.9941801428794861
Epoch 180, training loss: 452.94378662109375 = 0.9871124625205994 + 50.0 * 9.039133071899414
Epoch 180, val loss: 0.9893025755882263
Epoch 190, training loss: 450.02838134765625 = 0.9822415709495544 + 50.0 * 8.98092269897461
Epoch 190, val loss: 0.984532356262207
Epoch 200, training loss: 448.081298828125 = 0.976246178150177 + 50.0 * 8.942100524902344
Epoch 200, val loss: 0.9785901308059692
Epoch 210, training loss: 445.97906494140625 = 0.9689769744873047 + 50.0 * 8.900201797485352
Epoch 210, val loss: 0.97145015001297
Epoch 220, training loss: 443.6026306152344 = 0.9615321159362793 + 50.0 * 8.852822303771973
Epoch 220, val loss: 0.9641945958137512
Epoch 230, training loss: 442.0692138671875 = 0.9544674754142761 + 50.0 * 8.822295188903809
Epoch 230, val loss: 0.9572071433067322
Epoch 240, training loss: 441.0147705078125 = 0.9470927715301514 + 50.0 * 8.801353454589844
Epoch 240, val loss: 0.949918806552887
Epoch 250, training loss: 439.74334716796875 = 0.9393115043640137 + 50.0 * 8.776081085205078
Epoch 250, val loss: 0.9424782991409302
Epoch 260, training loss: 438.5893249511719 = 0.9315363168716431 + 50.0 * 8.753155708312988
Epoch 260, val loss: 0.9350898861885071
Epoch 270, training loss: 437.7994079589844 = 0.9233109951019287 + 50.0 * 8.73752212524414
Epoch 270, val loss: 0.9272054433822632
Epoch 280, training loss: 437.3916015625 = 0.913989245891571 + 50.0 * 8.729552268981934
Epoch 280, val loss: 0.9181778430938721
Epoch 290, training loss: 436.8546142578125 = 0.903560221195221 + 50.0 * 8.71902084350586
Epoch 290, val loss: 0.9081333875656128
Epoch 300, training loss: 436.3665771484375 = 0.8927671313285828 + 50.0 * 8.709476470947266
Epoch 300, val loss: 0.897800087928772
Epoch 310, training loss: 435.8266296386719 = 0.8821347951889038 + 50.0 * 8.69888973236084
Epoch 310, val loss: 0.88773113489151
Epoch 320, training loss: 435.1778869628906 = 0.8719784617424011 + 50.0 * 8.686118125915527
Epoch 320, val loss: 0.8781724572181702
Epoch 330, training loss: 434.4010925292969 = 0.8622244596481323 + 50.0 * 8.670777320861816
Epoch 330, val loss: 0.8690083026885986
Epoch 340, training loss: 433.621337890625 = 0.8523277044296265 + 50.0 * 8.655380249023438
Epoch 340, val loss: 0.8596428036689758
Epoch 350, training loss: 432.94256591796875 = 0.8419829607009888 + 50.0 * 8.642011642456055
Epoch 350, val loss: 0.8497668504714966
Epoch 360, training loss: 432.3536682128906 = 0.8311601877212524 + 50.0 * 8.630450248718262
Epoch 360, val loss: 0.8394611477851868
Epoch 370, training loss: 431.8472595214844 = 0.8200409412384033 + 50.0 * 8.62054443359375
Epoch 370, val loss: 0.8289355635643005
Epoch 380, training loss: 431.32452392578125 = 0.8087387084960938 + 50.0 * 8.610315322875977
Epoch 380, val loss: 0.8181419968605042
Epoch 390, training loss: 430.9328918457031 = 0.797406017780304 + 50.0 * 8.602709770202637
Epoch 390, val loss: 0.8074522018432617
Epoch 400, training loss: 430.5956726074219 = 0.7859321236610413 + 50.0 * 8.596195220947266
Epoch 400, val loss: 0.7965673804283142
Epoch 410, training loss: 430.3115539550781 = 0.7743676900863647 + 50.0 * 8.590744018554688
Epoch 410, val loss: 0.7856900691986084
Epoch 420, training loss: 430.0339660644531 = 0.7628415822982788 + 50.0 * 8.58542251586914
Epoch 420, val loss: 0.7748521566390991
Epoch 430, training loss: 429.7889404296875 = 0.7514510154724121 + 50.0 * 8.58074951171875
Epoch 430, val loss: 0.7641416788101196
Epoch 440, training loss: 429.4999694824219 = 0.7401365041732788 + 50.0 * 8.575196266174316
Epoch 440, val loss: 0.7535723447799683
Epoch 450, training loss: 429.1508483886719 = 0.7291534543037415 + 50.0 * 8.56843376159668
Epoch 450, val loss: 0.7433958053588867
Epoch 460, training loss: 428.81591796875 = 0.7184771299362183 + 50.0 * 8.561948776245117
Epoch 460, val loss: 0.7334840893745422
Epoch 470, training loss: 428.53607177734375 = 0.7080071568489075 + 50.0 * 8.556561470031738
Epoch 470, val loss: 0.7237183451652527
Epoch 480, training loss: 428.1475524902344 = 0.6977028846740723 + 50.0 * 8.548996925354004
Epoch 480, val loss: 0.7142032384872437
Epoch 490, training loss: 427.8089294433594 = 0.687475860118866 + 50.0 * 8.542428970336914
Epoch 490, val loss: 0.7047521471977234
Epoch 500, training loss: 427.50360107421875 = 0.6772726774215698 + 50.0 * 8.536526679992676
Epoch 500, val loss: 0.6953278183937073
Epoch 510, training loss: 427.2662048339844 = 0.6670731902122498 + 50.0 * 8.531982421875
Epoch 510, val loss: 0.685942530632019
Epoch 520, training loss: 426.9332275390625 = 0.6569157838821411 + 50.0 * 8.52552604675293
Epoch 520, val loss: 0.6766232252120972
Epoch 530, training loss: 426.6666564941406 = 0.6470063924789429 + 50.0 * 8.520393371582031
Epoch 530, val loss: 0.6676071286201477
Epoch 540, training loss: 426.4709777832031 = 0.6373198628425598 + 50.0 * 8.51667308807373
Epoch 540, val loss: 0.6588022708892822
Epoch 550, training loss: 426.2310791015625 = 0.6277762651443481 + 50.0 * 8.512065887451172
Epoch 550, val loss: 0.6502130031585693
Epoch 560, training loss: 426.02056884765625 = 0.6185069680213928 + 50.0 * 8.508041381835938
Epoch 560, val loss: 0.6418583989143372
Epoch 570, training loss: 425.8839416503906 = 0.6094732880592346 + 50.0 * 8.505489349365234
Epoch 570, val loss: 0.6337695717811584
Epoch 580, training loss: 425.73895263671875 = 0.6005885004997253 + 50.0 * 8.502767562866211
Epoch 580, val loss: 0.6259347796440125
Epoch 590, training loss: 425.4859924316406 = 0.5921176075935364 + 50.0 * 8.49787712097168
Epoch 590, val loss: 0.6184139847755432
Epoch 600, training loss: 425.319091796875 = 0.5839700102806091 + 50.0 * 8.494702339172363
Epoch 600, val loss: 0.6112160682678223
Epoch 610, training loss: 425.30694580078125 = 0.576113224029541 + 50.0 * 8.494616508483887
Epoch 610, val loss: 0.6042943000793457
Epoch 620, training loss: 425.02557373046875 = 0.5684541463851929 + 50.0 * 8.489142417907715
Epoch 620, val loss: 0.5976671576499939
Epoch 630, training loss: 424.90875244140625 = 0.561162531375885 + 50.0 * 8.48695182800293
Epoch 630, val loss: 0.5913910269737244
Epoch 640, training loss: 424.7756652832031 = 0.5542116761207581 + 50.0 * 8.484429359436035
Epoch 640, val loss: 0.5854207277297974
Epoch 650, training loss: 424.6910095214844 = 0.5475185513496399 + 50.0 * 8.482870101928711
Epoch 650, val loss: 0.5796924233436584
Epoch 660, training loss: 424.55047607421875 = 0.5410944819450378 + 50.0 * 8.48018741607666
Epoch 660, val loss: 0.5742657780647278
Epoch 670, training loss: 424.4404602050781 = 0.5350922346115112 + 50.0 * 8.478107452392578
Epoch 670, val loss: 0.5692663192749023
Epoch 680, training loss: 424.30389404296875 = 0.5294165015220642 + 50.0 * 8.475489616394043
Epoch 680, val loss: 0.5645250082015991
Epoch 690, training loss: 424.3232727050781 = 0.5240013599395752 + 50.0 * 8.475985527038574
Epoch 690, val loss: 0.5600486993789673
Epoch 700, training loss: 424.1033020019531 = 0.5187922716140747 + 50.0 * 8.47169017791748
Epoch 700, val loss: 0.5558363795280457
Epoch 710, training loss: 423.9364318847656 = 0.5139377117156982 + 50.0 * 8.468449592590332
Epoch 710, val loss: 0.5518805980682373
Epoch 720, training loss: 423.8125915527344 = 0.5092886686325073 + 50.0 * 8.466066360473633
Epoch 720, val loss: 0.54812091588974
Epoch 730, training loss: 423.803466796875 = 0.5048125386238098 + 50.0 * 8.465972900390625
Epoch 730, val loss: 0.5445145964622498
Epoch 740, training loss: 423.6278076171875 = 0.5004836320877075 + 50.0 * 8.462546348571777
Epoch 740, val loss: 0.5410799384117126
Epoch 750, training loss: 423.5373229980469 = 0.49638158082962036 + 50.0 * 8.460819244384766
Epoch 750, val loss: 0.5378625392913818
Epoch 760, training loss: 423.4032287597656 = 0.4924798607826233 + 50.0 * 8.45821475982666
Epoch 760, val loss: 0.5347945094108582
Epoch 770, training loss: 423.2747802734375 = 0.48874104022979736 + 50.0 * 8.455720901489258
Epoch 770, val loss: 0.5319361686706543
Epoch 780, training loss: 423.1647644042969 = 0.4851994216442108 + 50.0 * 8.453591346740723
Epoch 780, val loss: 0.5291852951049805
Epoch 790, training loss: 423.0766906738281 = 0.48182231187820435 + 50.0 * 8.451897621154785
Epoch 790, val loss: 0.5265942215919495
Epoch 800, training loss: 423.0857238769531 = 0.4785704016685486 + 50.0 * 8.452142715454102
Epoch 800, val loss: 0.5240836143493652
Epoch 810, training loss: 422.8834533691406 = 0.4754263758659363 + 50.0 * 8.448160171508789
Epoch 810, val loss: 0.5217474102973938
Epoch 820, training loss: 422.7613525390625 = 0.4725065529346466 + 50.0 * 8.44577693939209
Epoch 820, val loss: 0.5194826722145081
Epoch 830, training loss: 422.6666564941406 = 0.4696956276893616 + 50.0 * 8.443939208984375
Epoch 830, val loss: 0.517364501953125
Epoch 840, training loss: 422.7850341796875 = 0.46693816781044006 + 50.0 * 8.446361541748047
Epoch 840, val loss: 0.5153462886810303
Epoch 850, training loss: 422.5664978027344 = 0.4642963111400604 + 50.0 * 8.442044258117676
Epoch 850, val loss: 0.5133548378944397
Epoch 860, training loss: 422.41949462890625 = 0.4618244767189026 + 50.0 * 8.439153671264648
Epoch 860, val loss: 0.5114966630935669
Epoch 870, training loss: 422.3083801269531 = 0.459460586309433 + 50.0 * 8.436978340148926
Epoch 870, val loss: 0.5097436308860779
Epoch 880, training loss: 422.21282958984375 = 0.45718106627464294 + 50.0 * 8.435112953186035
Epoch 880, val loss: 0.5080633163452148
Epoch 890, training loss: 422.28558349609375 = 0.4549787640571594 + 50.0 * 8.436612129211426
Epoch 890, val loss: 0.506399929523468
Epoch 900, training loss: 422.13983154296875 = 0.45279020071029663 + 50.0 * 8.433740615844727
Epoch 900, val loss: 0.504789412021637
Epoch 910, training loss: 422.0906677246094 = 0.4507054090499878 + 50.0 * 8.432799339294434
Epoch 910, val loss: 0.503255307674408
Epoch 920, training loss: 421.9477233886719 = 0.44874995946884155 + 50.0 * 8.42997932434082
Epoch 920, val loss: 0.5017591118812561
Epoch 930, training loss: 421.8623962402344 = 0.44685491919517517 + 50.0 * 8.42831039428711
Epoch 930, val loss: 0.5003601312637329
Epoch 940, training loss: 421.79376220703125 = 0.44504693150520325 + 50.0 * 8.426974296569824
Epoch 940, val loss: 0.4990185499191284
Epoch 950, training loss: 421.9598388671875 = 0.4432596266269684 + 50.0 * 8.430331230163574
Epoch 950, val loss: 0.49772003293037415
Epoch 960, training loss: 421.7056579589844 = 0.44150251150131226 + 50.0 * 8.425283432006836
Epoch 960, val loss: 0.49636033177375793
Epoch 970, training loss: 421.61346435546875 = 0.4398581087589264 + 50.0 * 8.42347240447998
Epoch 970, val loss: 0.4951394498348236
Epoch 980, training loss: 421.5623779296875 = 0.43827545642852783 + 50.0 * 8.42248249053955
Epoch 980, val loss: 0.4939313232898712
Epoch 990, training loss: 421.54241943359375 = 0.43674758076667786 + 50.0 * 8.422113418579102
Epoch 990, val loss: 0.49279582500457764
Epoch 1000, training loss: 421.49322509765625 = 0.4351734220981598 + 50.0 * 8.421160697937012
Epoch 1000, val loss: 0.4916602671146393
Epoch 1010, training loss: 421.41015625 = 0.433717280626297 + 50.0 * 8.41952896118164
Epoch 1010, val loss: 0.4905535876750946
Epoch 1020, training loss: 421.3484802246094 = 0.4323492646217346 + 50.0 * 8.418322563171387
Epoch 1020, val loss: 0.4895264208316803
Epoch 1030, training loss: 421.2823181152344 = 0.4310237467288971 + 50.0 * 8.417025566101074
Epoch 1030, val loss: 0.4885593056678772
Epoch 1040, training loss: 421.212158203125 = 0.4297385811805725 + 50.0 * 8.415648460388184
Epoch 1040, val loss: 0.4876464009284973
Epoch 1050, training loss: 421.16729736328125 = 0.4284921884536743 + 50.0 * 8.414775848388672
Epoch 1050, val loss: 0.4867740273475647
Epoch 1060, training loss: 421.24114990234375 = 0.42724522948265076 + 50.0 * 8.416277885437012
Epoch 1060, val loss: 0.48574766516685486
Epoch 1070, training loss: 421.0790100097656 = 0.4260067343711853 + 50.0 * 8.413060188293457
Epoch 1070, val loss: 0.4849599003791809
Epoch 1080, training loss: 420.977783203125 = 0.4248734712600708 + 50.0 * 8.41105842590332
Epoch 1080, val loss: 0.4841180443763733
Epoch 1090, training loss: 420.91217041015625 = 0.4237780272960663 + 50.0 * 8.409768104553223
Epoch 1090, val loss: 0.4833620488643646
Epoch 1100, training loss: 420.85125732421875 = 0.42271992564201355 + 50.0 * 8.408570289611816
Epoch 1100, val loss: 0.4826350510120392
Epoch 1110, training loss: 420.7928466796875 = 0.42167773842811584 + 50.0 * 8.40742301940918
Epoch 1110, val loss: 0.481915682554245
Epoch 1120, training loss: 420.7928466796875 = 0.4206621050834656 + 50.0 * 8.40744400024414
Epoch 1120, val loss: 0.48118162155151367
Epoch 1130, training loss: 420.73883056640625 = 0.4196072816848755 + 50.0 * 8.406384468078613
Epoch 1130, val loss: 0.48052623867988586
Epoch 1140, training loss: 420.6498107910156 = 0.41860100626945496 + 50.0 * 8.404623985290527
Epoch 1140, val loss: 0.4797894358634949
Epoch 1150, training loss: 420.68060302734375 = 0.41763734817504883 + 50.0 * 8.405259132385254
Epoch 1150, val loss: 0.479126900434494
Epoch 1160, training loss: 420.554931640625 = 0.41665199398994446 + 50.0 * 8.402765274047852
Epoch 1160, val loss: 0.4784144461154938
Epoch 1170, training loss: 420.4966125488281 = 0.4157068729400635 + 50.0 * 8.401618003845215
Epoch 1170, val loss: 0.47784414887428284
Epoch 1180, training loss: 420.4495849609375 = 0.41479000449180603 + 50.0 * 8.40069580078125
Epoch 1180, val loss: 0.4771946668624878
Epoch 1190, training loss: 420.40447998046875 = 0.4138728976249695 + 50.0 * 8.399811744689941
Epoch 1190, val loss: 0.47663018107414246
Epoch 1200, training loss: 420.50946044921875 = 0.4129678010940552 + 50.0 * 8.40192985534668
Epoch 1200, val loss: 0.4760633707046509
Epoch 1210, training loss: 420.3981628417969 = 0.41203588247299194 + 50.0 * 8.3997220993042
Epoch 1210, val loss: 0.4753836691379547
Epoch 1220, training loss: 420.3426818847656 = 0.4111371338367462 + 50.0 * 8.39863109588623
Epoch 1220, val loss: 0.47485771775245667
Epoch 1230, training loss: 420.29315185546875 = 0.41024330258369446 + 50.0 * 8.397658348083496
Epoch 1230, val loss: 0.4742707908153534
Epoch 1240, training loss: 420.2424621582031 = 0.4093659222126007 + 50.0 * 8.396661758422852
Epoch 1240, val loss: 0.4736812114715576
Epoch 1250, training loss: 420.390625 = 0.40848347544670105 + 50.0 * 8.399642944335938
Epoch 1250, val loss: 0.47312131524086
Epoch 1260, training loss: 420.2290344238281 = 0.40760374069213867 + 50.0 * 8.396429061889648
Epoch 1260, val loss: 0.47260648012161255
Epoch 1270, training loss: 420.1560363769531 = 0.4067418873310089 + 50.0 * 8.394986152648926
Epoch 1270, val loss: 0.47199711203575134
Epoch 1280, training loss: 420.11663818359375 = 0.4059121608734131 + 50.0 * 8.394214630126953
Epoch 1280, val loss: 0.4715138375759125
Epoch 1290, training loss: 420.08984375 = 0.4050963819026947 + 50.0 * 8.393694877624512
Epoch 1290, val loss: 0.4710014760494232
Epoch 1300, training loss: 420.14715576171875 = 0.4043021500110626 + 50.0 * 8.394857406616211
Epoch 1300, val loss: 0.4704701602458954
Epoch 1310, training loss: 420.051513671875 = 0.40346771478652954 + 50.0 * 8.392960548400879
Epoch 1310, val loss: 0.47003600001335144
Epoch 1320, training loss: 420.0429382324219 = 0.4026711583137512 + 50.0 * 8.392805099487305
Epoch 1320, val loss: 0.4694962203502655
Epoch 1330, training loss: 419.99749755859375 = 0.40188488364219666 + 50.0 * 8.391912460327148
Epoch 1330, val loss: 0.4690393805503845
Epoch 1340, training loss: 419.9702453613281 = 0.40111541748046875 + 50.0 * 8.391382217407227
Epoch 1340, val loss: 0.46856921911239624
Epoch 1350, training loss: 420.2945556640625 = 0.4003327488899231 + 50.0 * 8.397884368896484
Epoch 1350, val loss: 0.4680771231651306
Epoch 1360, training loss: 420.0174255371094 = 0.3995555341243744 + 50.0 * 8.392356872558594
Epoch 1360, val loss: 0.46764466166496277
Epoch 1370, training loss: 419.8957214355469 = 0.3987962603569031 + 50.0 * 8.389938354492188
Epoch 1370, val loss: 0.4671855568885803
Epoch 1380, training loss: 419.88421630859375 = 0.39804980158805847 + 50.0 * 8.389723777770996
Epoch 1380, val loss: 0.4667663872241974
Epoch 1390, training loss: 419.84600830078125 = 0.39732617139816284 + 50.0 * 8.388973236083984
Epoch 1390, val loss: 0.46637338399887085
Epoch 1400, training loss: 419.85113525390625 = 0.3965941369533539 + 50.0 * 8.389090538024902
Epoch 1400, val loss: 0.46596628427505493
Epoch 1410, training loss: 419.83197021484375 = 0.39584311842918396 + 50.0 * 8.38872241973877
Epoch 1410, val loss: 0.46554404497146606
Epoch 1420, training loss: 419.820556640625 = 0.39510777592658997 + 50.0 * 8.388508796691895
Epoch 1420, val loss: 0.4651423692703247
Epoch 1430, training loss: 419.78375244140625 = 0.39441004395484924 + 50.0 * 8.387786865234375
Epoch 1430, val loss: 0.4647656977176666
Epoch 1440, training loss: 419.8412170410156 = 0.39371025562286377 + 50.0 * 8.38895034790039
Epoch 1440, val loss: 0.4644177258014679
Epoch 1450, training loss: 419.7377624511719 = 0.3929978907108307 + 50.0 * 8.386895179748535
Epoch 1450, val loss: 0.46396973729133606
Epoch 1460, training loss: 419.6912536621094 = 0.39231380820274353 + 50.0 * 8.385978698730469
Epoch 1460, val loss: 0.46365106105804443
Epoch 1470, training loss: 419.6890869140625 = 0.39163199067115784 + 50.0 * 8.38594913482666
Epoch 1470, val loss: 0.46333593130111694
Epoch 1480, training loss: 419.7939147949219 = 0.39094340801239014 + 50.0 * 8.388059616088867
Epoch 1480, val loss: 0.46299418807029724
Epoch 1490, training loss: 419.688232421875 = 0.39025816321372986 + 50.0 * 8.38595962524414
Epoch 1490, val loss: 0.4625363349914551
Epoch 1500, training loss: 419.7389221191406 = 0.3895798921585083 + 50.0 * 8.38698673248291
Epoch 1500, val loss: 0.46221452951431274
Epoch 1510, training loss: 419.5995788574219 = 0.38890203833580017 + 50.0 * 8.3842134475708
Epoch 1510, val loss: 0.46192994713783264
Epoch 1520, training loss: 419.5675964355469 = 0.388249009847641 + 50.0 * 8.383586883544922
Epoch 1520, val loss: 0.4615611135959625
Epoch 1530, training loss: 419.5723876953125 = 0.387590616941452 + 50.0 * 8.383695602416992
Epoch 1530, val loss: 0.46128010749816895
Epoch 1540, training loss: 419.73345947265625 = 0.38691437244415283 + 50.0 * 8.386931419372559
Epoch 1540, val loss: 0.4609324634075165
Epoch 1550, training loss: 419.54833984375 = 0.3862570524215698 + 50.0 * 8.383241653442383
Epoch 1550, val loss: 0.46063244342803955
Epoch 1560, training loss: 419.4850158691406 = 0.3856026530265808 + 50.0 * 8.381988525390625
Epoch 1560, val loss: 0.46035000681877136
Epoch 1570, training loss: 419.4590759277344 = 0.3849697411060333 + 50.0 * 8.381482124328613
Epoch 1570, val loss: 0.4600619673728943
Epoch 1580, training loss: 419.48095703125 = 0.384333997964859 + 50.0 * 8.381932258605957
Epoch 1580, val loss: 0.4598303735256195
Epoch 1590, training loss: 419.4599304199219 = 0.3836800456047058 + 50.0 * 8.381525039672852
Epoch 1590, val loss: 0.45945993065834045
Epoch 1600, training loss: 419.3985290527344 = 0.38303038477897644 + 50.0 * 8.38031005859375
Epoch 1600, val loss: 0.45915335416793823
Epoch 1610, training loss: 419.4322204589844 = 0.38239336013793945 + 50.0 * 8.380996704101562
Epoch 1610, val loss: 0.4588335156440735
Epoch 1620, training loss: 419.3565673828125 = 0.3817473351955414 + 50.0 * 8.379496574401855
Epoch 1620, val loss: 0.4586086571216583
Epoch 1630, training loss: 419.3398132324219 = 0.38111260533332825 + 50.0 * 8.37917423248291
Epoch 1630, val loss: 0.45830100774765015
Epoch 1640, training loss: 419.3210754394531 = 0.3804883658885956 + 50.0 * 8.378811836242676
Epoch 1640, val loss: 0.4580467641353607
Epoch 1650, training loss: 419.4954833984375 = 0.37986600399017334 + 50.0 * 8.382312774658203
Epoch 1650, val loss: 0.4577181339263916
Epoch 1660, training loss: 419.34417724609375 = 0.379207968711853 + 50.0 * 8.37929916381836
Epoch 1660, val loss: 0.4575473666191101
Epoch 1670, training loss: 419.32440185546875 = 0.37857186794281006 + 50.0 * 8.37891674041748
Epoch 1670, val loss: 0.4572215974330902
Epoch 1680, training loss: 419.2460021972656 = 0.3779255747795105 + 50.0 * 8.377361297607422
Epoch 1680, val loss: 0.45693305134773254
Epoch 1690, training loss: 419.22509765625 = 0.37729957699775696 + 50.0 * 8.37695598602295
Epoch 1690, val loss: 0.45669329166412354
Epoch 1700, training loss: 419.1933288574219 = 0.37667450308799744 + 50.0 * 8.376333236694336
Epoch 1700, val loss: 0.45640724897384644
Epoch 1710, training loss: 419.3167724609375 = 0.37604859471321106 + 50.0 * 8.378814697265625
Epoch 1710, val loss: 0.45612215995788574
Epoch 1720, training loss: 419.2105712890625 = 0.375406414270401 + 50.0 * 8.376703262329102
Epoch 1720, val loss: 0.4559151530265808
Epoch 1730, training loss: 419.1444091796875 = 0.3747691810131073 + 50.0 * 8.37539291381836
Epoch 1730, val loss: 0.45558178424835205
Epoch 1740, training loss: 419.0882263183594 = 0.3741559684276581 + 50.0 * 8.374281883239746
Epoch 1740, val loss: 0.4553571939468384
Epoch 1750, training loss: 419.06689453125 = 0.37353697419166565 + 50.0 * 8.37386703491211
Epoch 1750, val loss: 0.45511242747306824
Epoch 1760, training loss: 419.0907287597656 = 0.37291219830513 + 50.0 * 8.374356269836426
Epoch 1760, val loss: 0.4548204243183136
Epoch 1770, training loss: 419.1549072265625 = 0.3722624182701111 + 50.0 * 8.375653266906738
Epoch 1770, val loss: 0.4545927941799164
Epoch 1780, training loss: 419.1333312988281 = 0.37161508202552795 + 50.0 * 8.375234603881836
Epoch 1780, val loss: 0.45428815484046936
Epoch 1790, training loss: 419.0010986328125 = 0.37098199129104614 + 50.0 * 8.372602462768555
Epoch 1790, val loss: 0.45407241582870483
Epoch 1800, training loss: 418.9879455566406 = 0.3703685402870178 + 50.0 * 8.37235164642334
Epoch 1800, val loss: 0.4538610279560089
Epoch 1810, training loss: 418.9535827636719 = 0.36975497007369995 + 50.0 * 8.371676445007324
Epoch 1810, val loss: 0.453630268573761
Epoch 1820, training loss: 418.9554443359375 = 0.3691428303718567 + 50.0 * 8.371726036071777
Epoch 1820, val loss: 0.4533953070640564
Epoch 1830, training loss: 419.07464599609375 = 0.3685213029384613 + 50.0 * 8.374122619628906
Epoch 1830, val loss: 0.4531746804714203
Epoch 1840, training loss: 419.03253173828125 = 0.3678797483444214 + 50.0 * 8.373292922973633
Epoch 1840, val loss: 0.4528454840183258
Epoch 1850, training loss: 418.91314697265625 = 0.3672412037849426 + 50.0 * 8.370918273925781
Epoch 1850, val loss: 0.452639102935791
Epoch 1860, training loss: 418.8802795410156 = 0.36661285161972046 + 50.0 * 8.37027359008789
Epoch 1860, val loss: 0.45236852765083313
Epoch 1870, training loss: 418.8519592285156 = 0.3659912049770355 + 50.0 * 8.369719505310059
Epoch 1870, val loss: 0.4521459639072418
Epoch 1880, training loss: 418.84814453125 = 0.3653618097305298 + 50.0 * 8.36965560913086
Epoch 1880, val loss: 0.4518771469593048
Epoch 1890, training loss: 418.914306640625 = 0.36472487449645996 + 50.0 * 8.370991706848145
Epoch 1890, val loss: 0.45161840319633484
Epoch 1900, training loss: 418.8485412597656 = 0.3640936315059662 + 50.0 * 8.369688987731934
Epoch 1900, val loss: 0.4513581395149231
Epoch 1910, training loss: 418.76904296875 = 0.3634500205516815 + 50.0 * 8.368111610412598
Epoch 1910, val loss: 0.45112431049346924
Epoch 1920, training loss: 418.7863464355469 = 0.3628237843513489 + 50.0 * 8.368470191955566
Epoch 1920, val loss: 0.45090728998184204
Epoch 1930, training loss: 418.77191162109375 = 0.36218684911727905 + 50.0 * 8.368194580078125
Epoch 1930, val loss: 0.4506506621837616
Epoch 1940, training loss: 418.72686767578125 = 0.361542284488678 + 50.0 * 8.36730670928955
Epoch 1940, val loss: 0.45041701197624207
Epoch 1950, training loss: 418.6788330078125 = 0.3609142303466797 + 50.0 * 8.366358757019043
Epoch 1950, val loss: 0.45018717646598816
Epoch 1960, training loss: 418.6860046386719 = 0.3602816164493561 + 50.0 * 8.366514205932617
Epoch 1960, val loss: 0.4499831199645996
Epoch 1970, training loss: 418.77166748046875 = 0.35963454842567444 + 50.0 * 8.368240356445312
Epoch 1970, val loss: 0.44979649782180786
Epoch 1980, training loss: 418.81219482421875 = 0.3589692711830139 + 50.0 * 8.369064331054688
Epoch 1980, val loss: 0.4495449662208557
Epoch 1990, training loss: 418.6913146972656 = 0.35830414295196533 + 50.0 * 8.366660118103027
Epoch 1990, val loss: 0.44922110438346863
Epoch 2000, training loss: 418.640380859375 = 0.3576414883136749 + 50.0 * 8.365654945373535
Epoch 2000, val loss: 0.44900402426719666
Epoch 2010, training loss: 418.64813232421875 = 0.3569919764995575 + 50.0 * 8.365822792053223
Epoch 2010, val loss: 0.4488031268119812
Epoch 2020, training loss: 418.567138671875 = 0.3563399016857147 + 50.0 * 8.364215850830078
Epoch 2020, val loss: 0.4485839307308197
Epoch 2030, training loss: 418.5484619140625 = 0.3556904196739197 + 50.0 * 8.363855361938477
Epoch 2030, val loss: 0.4484025537967682
Epoch 2040, training loss: 418.52972412109375 = 0.3550385534763336 + 50.0 * 8.363493919372559
Epoch 2040, val loss: 0.44821590185165405
Epoch 2050, training loss: 418.67083740234375 = 0.3543727993965149 + 50.0 * 8.366329193115234
Epoch 2050, val loss: 0.4479760527610779
Epoch 2060, training loss: 418.5360412597656 = 0.35368314385414124 + 50.0 * 8.3636474609375
Epoch 2060, val loss: 0.44777917861938477
Epoch 2070, training loss: 418.4885559082031 = 0.35300302505493164 + 50.0 * 8.362710952758789
Epoch 2070, val loss: 0.4475448727607727
Epoch 2080, training loss: 418.4630432128906 = 0.3523363471031189 + 50.0 * 8.362214088439941
Epoch 2080, val loss: 0.4473806917667389
Epoch 2090, training loss: 418.4525451660156 = 0.35166698694229126 + 50.0 * 8.362017631530762
Epoch 2090, val loss: 0.44716763496398926
Epoch 2100, training loss: 418.5140075683594 = 0.3509962260723114 + 50.0 * 8.363260269165039
Epoch 2100, val loss: 0.4469757080078125
Epoch 2110, training loss: 418.4055480957031 = 0.3503091633319855 + 50.0 * 8.361104965209961
Epoch 2110, val loss: 0.4467664361000061
Epoch 2120, training loss: 418.41729736328125 = 0.34962940216064453 + 50.0 * 8.361352920532227
Epoch 2120, val loss: 0.4465944468975067
Epoch 2130, training loss: 418.5875244140625 = 0.3489542603492737 + 50.0 * 8.364770889282227
Epoch 2130, val loss: 0.44646093249320984
Epoch 2140, training loss: 418.46142578125 = 0.3482450842857361 + 50.0 * 8.362263679504395
Epoch 2140, val loss: 0.44604575634002686
Epoch 2150, training loss: 418.3511962890625 = 0.34754306077957153 + 50.0 * 8.36007308959961
Epoch 2150, val loss: 0.4458840787410736
Epoch 2160, training loss: 418.3395080566406 = 0.3468608260154724 + 50.0 * 8.35985279083252
Epoch 2160, val loss: 0.44574764370918274
Epoch 2170, training loss: 418.3313293457031 = 0.34617215394973755 + 50.0 * 8.359703063964844
Epoch 2170, val loss: 0.4455142617225647
Epoch 2180, training loss: 418.34991455078125 = 0.34547972679138184 + 50.0 * 8.360088348388672
Epoch 2180, val loss: 0.4453638195991516
Epoch 2190, training loss: 418.3332824707031 = 0.3447728753089905 + 50.0 * 8.359769821166992
Epoch 2190, val loss: 0.4451194405555725
Epoch 2200, training loss: 418.394287109375 = 0.34406065940856934 + 50.0 * 8.361004829406738
Epoch 2200, val loss: 0.44487813115119934
Epoch 2210, training loss: 418.4362487792969 = 0.3433375656604767 + 50.0 * 8.361858367919922
Epoch 2210, val loss: 0.4446776509284973
Epoch 2220, training loss: 418.29205322265625 = 0.3426101505756378 + 50.0 * 8.358988761901855
Epoch 2220, val loss: 0.4444586932659149
Epoch 2230, training loss: 418.2437744140625 = 0.34188899397850037 + 50.0 * 8.358037948608398
Epoch 2230, val loss: 0.4442790448665619
Epoch 2240, training loss: 418.2351379394531 = 0.34116512537002563 + 50.0 * 8.357879638671875
Epoch 2240, val loss: 0.44405946135520935
Epoch 2250, training loss: 418.2704162597656 = 0.34043604135513306 + 50.0 * 8.358599662780762
Epoch 2250, val loss: 0.4439120888710022
Epoch 2260, training loss: 418.3153076171875 = 0.3396896421909332 + 50.0 * 8.359512329101562
Epoch 2260, val loss: 0.44368135929107666
Epoch 2270, training loss: 418.2541809082031 = 0.33892887830734253 + 50.0 * 8.358304977416992
Epoch 2270, val loss: 0.4434511065483093
Epoch 2280, training loss: 418.2074890136719 = 0.3381851017475128 + 50.0 * 8.357385635375977
Epoch 2280, val loss: 0.4433057904243469
Epoch 2290, training loss: 418.1710510253906 = 0.3374292850494385 + 50.0 * 8.356672286987305
Epoch 2290, val loss: 0.4430896043777466
Epoch 2300, training loss: 418.2086181640625 = 0.33667486906051636 + 50.0 * 8.357439041137695
Epoch 2300, val loss: 0.44292786717414856
Epoch 2310, training loss: 418.2715148925781 = 0.3358970582485199 + 50.0 * 8.358712196350098
Epoch 2310, val loss: 0.44272926449775696
Epoch 2320, training loss: 418.2016906738281 = 0.33510538935661316 + 50.0 * 8.357331275939941
Epoch 2320, val loss: 0.4424491226673126
Epoch 2330, training loss: 418.1603088378906 = 0.33433207869529724 + 50.0 * 8.35651969909668
Epoch 2330, val loss: 0.44229990243911743
Epoch 2340, training loss: 418.13201904296875 = 0.33355042338371277 + 50.0 * 8.355969429016113
Epoch 2340, val loss: 0.44207870960235596
Epoch 2350, training loss: 418.1241455078125 = 0.3327678143978119 + 50.0 * 8.355827331542969
Epoch 2350, val loss: 0.4419203996658325
Epoch 2360, training loss: 418.1726989746094 = 0.33197516202926636 + 50.0 * 8.35681438446045
Epoch 2360, val loss: 0.4417101740837097
Epoch 2370, training loss: 418.19195556640625 = 0.3311710059642792 + 50.0 * 8.357215881347656
Epoch 2370, val loss: 0.44151222705841064
Epoch 2380, training loss: 418.0655517578125 = 0.33035022020339966 + 50.0 * 8.354703903198242
Epoch 2380, val loss: 0.4413454532623291
Epoch 2390, training loss: 418.0461730957031 = 0.3295435309410095 + 50.0 * 8.35433292388916
Epoch 2390, val loss: 0.4411599636077881
Epoch 2400, training loss: 418.0428466796875 = 0.32873740792274475 + 50.0 * 8.35428237915039
Epoch 2400, val loss: 0.44100669026374817
Epoch 2410, training loss: 418.1461181640625 = 0.3279357850551605 + 50.0 * 8.356363296508789
Epoch 2410, val loss: 0.44094154238700867
Epoch 2420, training loss: 418.0326232910156 = 0.3270898759365082 + 50.0 * 8.354110717773438
Epoch 2420, val loss: 0.44059664011001587
Epoch 2430, training loss: 418.05908203125 = 0.3262593746185303 + 50.0 * 8.354656219482422
Epoch 2430, val loss: 0.44053465127944946
Epoch 2440, training loss: 418.02154541015625 = 0.32542333006858826 + 50.0 * 8.353922843933105
Epoch 2440, val loss: 0.4402584135532379
Epoch 2450, training loss: 417.9858093261719 = 0.32459473609924316 + 50.0 * 8.353224754333496
Epoch 2450, val loss: 0.44017595052719116
Epoch 2460, training loss: 417.99261474609375 = 0.3237587511539459 + 50.0 * 8.353377342224121
Epoch 2460, val loss: 0.43997180461883545
Epoch 2470, training loss: 418.28912353515625 = 0.3229134678840637 + 50.0 * 8.35932445526123
Epoch 2470, val loss: 0.4398464262485504
Epoch 2480, training loss: 418.0716552734375 = 0.32205143570899963 + 50.0 * 8.354991912841797
Epoch 2480, val loss: 0.43966513872146606
Epoch 2490, training loss: 417.9500427246094 = 0.3211931586265564 + 50.0 * 8.352577209472656
Epoch 2490, val loss: 0.4395264685153961
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8168442415017757
0.8653191335216983
=== training gcn model ===
Epoch 0, training loss: 530.2225341796875 = 1.1080318689346313 + 50.0 * 10.582290649414062
Epoch 0, val loss: 1.108717441558838
Epoch 10, training loss: 530.1993408203125 = 1.1031516790390015 + 50.0 * 10.581923484802246
Epoch 10, val loss: 1.1038200855255127
Epoch 20, training loss: 530.1158447265625 = 1.097699522972107 + 50.0 * 10.580362319946289
Epoch 20, val loss: 1.0983692407608032
Epoch 30, training loss: 529.7696533203125 = 1.0917397737503052 + 50.0 * 10.57355785369873
Epoch 30, val loss: 1.0923928022384644
Epoch 40, training loss: 528.4287109375 = 1.0851517915725708 + 50.0 * 10.546871185302734
Epoch 40, val loss: 1.0857616662979126
Epoch 50, training loss: 524.4108276367188 = 1.0775017738342285 + 50.0 * 10.466666221618652
Epoch 50, val loss: 1.0780147314071655
Epoch 60, training loss: 515.3706665039062 = 1.069034218788147 + 50.0 * 10.286032676696777
Epoch 60, val loss: 1.0695043802261353
Epoch 70, training loss: 501.1125793457031 = 1.0601223707199097 + 50.0 * 10.001049041748047
Epoch 70, val loss: 1.0604106187820435
Epoch 80, training loss: 485.1948547363281 = 1.050978422164917 + 50.0 * 9.682877540588379
Epoch 80, val loss: 1.0512073040008545
Epoch 90, training loss: 478.369873046875 = 1.0440056324005127 + 50.0 * 9.546517372131348
Epoch 90, val loss: 1.0442261695861816
Epoch 100, training loss: 471.18896484375 = 1.03848397731781 + 50.0 * 9.403009414672852
Epoch 100, val loss: 1.038737177848816
Epoch 110, training loss: 463.9159851074219 = 1.0340163707733154 + 50.0 * 9.257638931274414
Epoch 110, val loss: 1.0343713760375977
Epoch 120, training loss: 460.8897399902344 = 1.030094027519226 + 50.0 * 9.197193145751953
Epoch 120, val loss: 1.0304933786392212
Epoch 130, training loss: 459.1358337402344 = 1.0254502296447754 + 50.0 * 9.16220760345459
Epoch 130, val loss: 1.0258748531341553
Epoch 140, training loss: 456.76123046875 = 1.0205254554748535 + 50.0 * 9.114813804626465
Epoch 140, val loss: 1.0210977792739868
Epoch 150, training loss: 453.4263000488281 = 1.0165653228759766 + 50.0 * 9.048194885253906
Epoch 150, val loss: 1.0172477960586548
Epoch 160, training loss: 450.09283447265625 = 1.0130854845046997 + 50.0 * 8.981595039367676
Epoch 160, val loss: 1.013744831085205
Epoch 170, training loss: 448.20330810546875 = 1.0088858604431152 + 50.0 * 8.943888664245605
Epoch 170, val loss: 1.0095332860946655
Epoch 180, training loss: 446.544921875 = 1.0040550231933594 + 50.0 * 8.91081714630127
Epoch 180, val loss: 1.0047862529754639
Epoch 190, training loss: 444.56341552734375 = 0.9992483854293823 + 50.0 * 8.871283531188965
Epoch 190, val loss: 1.0000749826431274
Epoch 200, training loss: 443.07135009765625 = 0.9944887161254883 + 50.0 * 8.841537475585938
Epoch 200, val loss: 0.9953484535217285
Epoch 210, training loss: 442.10198974609375 = 0.9889382719993591 + 50.0 * 8.822260856628418
Epoch 210, val loss: 0.9897934794425964
Epoch 220, training loss: 440.99530029296875 = 0.9826111793518066 + 50.0 * 8.800253868103027
Epoch 220, val loss: 0.9836245179176331
Epoch 230, training loss: 439.7465515136719 = 0.9765270352363586 + 50.0 * 8.775400161743164
Epoch 230, val loss: 0.9778293967247009
Epoch 240, training loss: 438.5176696777344 = 0.9706337451934814 + 50.0 * 8.750940322875977
Epoch 240, val loss: 0.9720420241355896
Epoch 250, training loss: 437.77459716796875 = 0.9637241363525391 + 50.0 * 8.736217498779297
Epoch 250, val loss: 0.9652508497238159
Epoch 260, training loss: 437.1793518066406 = 0.9554913640022278 + 50.0 * 8.72447681427002
Epoch 260, val loss: 0.9571593403816223
Epoch 270, training loss: 436.52960205078125 = 0.9468218088150024 + 50.0 * 8.711655616760254
Epoch 270, val loss: 0.9487658143043518
Epoch 280, training loss: 435.8053894042969 = 0.938463032245636 + 50.0 * 8.697338104248047
Epoch 280, val loss: 0.9407650828361511
Epoch 290, training loss: 434.99365234375 = 0.9302146434783936 + 50.0 * 8.681268692016602
Epoch 290, val loss: 0.9328121542930603
Epoch 300, training loss: 434.1600036621094 = 0.9215962290763855 + 50.0 * 8.66476821899414
Epoch 300, val loss: 0.9245364665985107
Epoch 310, training loss: 433.5328369140625 = 0.9124131202697754 + 50.0 * 8.652408599853516
Epoch 310, val loss: 0.9156699180603027
Epoch 320, training loss: 433.0768127441406 = 0.902406632900238 + 50.0 * 8.643487930297852
Epoch 320, val loss: 0.9059926271438599
Epoch 330, training loss: 432.68865966796875 = 0.8918611407279968 + 50.0 * 8.63593578338623
Epoch 330, val loss: 0.8957927227020264
Epoch 340, training loss: 432.2510681152344 = 0.8811919093132019 + 50.0 * 8.627397537231445
Epoch 340, val loss: 0.8854939937591553
Epoch 350, training loss: 431.7864990234375 = 0.8704063892364502 + 50.0 * 8.618322372436523
Epoch 350, val loss: 0.8750961422920227
Epoch 360, training loss: 431.22698974609375 = 0.8595592379570007 + 50.0 * 8.607348442077637
Epoch 360, val loss: 0.8646064400672913
Epoch 370, training loss: 430.65814208984375 = 0.8486565351486206 + 50.0 * 8.596189498901367
Epoch 370, val loss: 0.8540939688682556
Epoch 380, training loss: 430.2115783691406 = 0.8374900221824646 + 50.0 * 8.587481498718262
Epoch 380, val loss: 0.8432735204696655
Epoch 390, training loss: 429.68365478515625 = 0.825980544090271 + 50.0 * 8.577153205871582
Epoch 390, val loss: 0.8321725130081177
Epoch 400, training loss: 429.2062072753906 = 0.8143265247344971 + 50.0 * 8.567837715148926
Epoch 400, val loss: 0.8208815455436707
Epoch 410, training loss: 428.90985107421875 = 0.8025583028793335 + 50.0 * 8.562146186828613
Epoch 410, val loss: 0.809455931186676
Epoch 420, training loss: 428.4437561035156 = 0.7904995083808899 + 50.0 * 8.553065299987793
Epoch 420, val loss: 0.7978610396385193
Epoch 430, training loss: 428.06781005859375 = 0.7782928943634033 + 50.0 * 8.545790672302246
Epoch 430, val loss: 0.7861461639404297
Epoch 440, training loss: 427.73492431640625 = 0.766045093536377 + 50.0 * 8.539377212524414
Epoch 440, val loss: 0.7744320631027222
Epoch 450, training loss: 427.5960388183594 = 0.7538095712661743 + 50.0 * 8.536844253540039
Epoch 450, val loss: 0.7627226710319519
Epoch 460, training loss: 427.10418701171875 = 0.7415624260902405 + 50.0 * 8.527252197265625
Epoch 460, val loss: 0.751096785068512
Epoch 470, training loss: 426.80859375 = 0.7295083999633789 + 50.0 * 8.521581649780273
Epoch 470, val loss: 0.7396697998046875
Epoch 480, training loss: 426.5303039550781 = 0.717490017414093 + 50.0 * 8.516256332397461
Epoch 480, val loss: 0.7282887101173401
Epoch 490, training loss: 426.2913818359375 = 0.7054556608200073 + 50.0 * 8.51171875
Epoch 490, val loss: 0.716958224773407
Epoch 500, training loss: 426.0343933105469 = 0.6935886740684509 + 50.0 * 8.506815910339355
Epoch 500, val loss: 0.705776035785675
Epoch 510, training loss: 425.7919006347656 = 0.6819421052932739 + 50.0 * 8.502199172973633
Epoch 510, val loss: 0.6948570609092712
Epoch 520, training loss: 425.5785827636719 = 0.6705578565597534 + 50.0 * 8.498160362243652
Epoch 520, val loss: 0.6842442750930786
Epoch 530, training loss: 425.4248962402344 = 0.659447193145752 + 50.0 * 8.495308876037598
Epoch 530, val loss: 0.673956573009491
Epoch 540, training loss: 425.1169128417969 = 0.6487764120101929 + 50.0 * 8.489362716674805
Epoch 540, val loss: 0.6640534400939941
Epoch 550, training loss: 424.9187316894531 = 0.6384330987930298 + 50.0 * 8.48560619354248
Epoch 550, val loss: 0.6545341610908508
Epoch 560, training loss: 424.7237548828125 = 0.6284087300300598 + 50.0 * 8.48190689086914
Epoch 560, val loss: 0.645359992980957
Epoch 570, training loss: 424.55169677734375 = 0.6187807321548462 + 50.0 * 8.478658676147461
Epoch 570, val loss: 0.6365910768508911
Epoch 580, training loss: 424.5308532714844 = 0.6094867587089539 + 50.0 * 8.478426933288574
Epoch 580, val loss: 0.6281353235244751
Epoch 590, training loss: 424.2313537597656 = 0.6005662679672241 + 50.0 * 8.472616195678711
Epoch 590, val loss: 0.6201199889183044
Epoch 600, training loss: 424.07598876953125 = 0.5920923948287964 + 50.0 * 8.469677925109863
Epoch 600, val loss: 0.6125056743621826
Epoch 610, training loss: 423.9288024902344 = 0.5839460492134094 + 50.0 * 8.466897010803223
Epoch 610, val loss: 0.6052131652832031
Epoch 620, training loss: 423.8589172363281 = 0.5761057138442993 + 50.0 * 8.465656280517578
Epoch 620, val loss: 0.5982020497322083
Epoch 630, training loss: 423.7427673339844 = 0.5685763359069824 + 50.0 * 8.463483810424805
Epoch 630, val loss: 0.5916078090667725
Epoch 640, training loss: 423.5623474121094 = 0.5614784955978394 + 50.0 * 8.460017204284668
Epoch 640, val loss: 0.5853252410888672
Epoch 650, training loss: 423.45611572265625 = 0.5547088384628296 + 50.0 * 8.458027839660645
Epoch 650, val loss: 0.5793994665145874
Epoch 660, training loss: 423.3830871582031 = 0.5482364296913147 + 50.0 * 8.456696510314941
Epoch 660, val loss: 0.5737915635108948
Epoch 670, training loss: 423.290771484375 = 0.5420315265655518 + 50.0 * 8.454975128173828
Epoch 670, val loss: 0.5684096813201904
Epoch 680, training loss: 423.15032958984375 = 0.5361900925636292 + 50.0 * 8.452282905578613
Epoch 680, val loss: 0.5633870959281921
Epoch 690, training loss: 423.0777893066406 = 0.5306420922279358 + 50.0 * 8.450942993164062
Epoch 690, val loss: 0.5586719512939453
Epoch 700, training loss: 422.96942138671875 = 0.525346040725708 + 50.0 * 8.448881149291992
Epoch 700, val loss: 0.5542201399803162
Epoch 710, training loss: 422.88397216796875 = 0.5203343629837036 + 50.0 * 8.447273254394531
Epoch 710, val loss: 0.5500404238700867
Epoch 720, training loss: 422.90411376953125 = 0.5155822038650513 + 50.0 * 8.447770118713379
Epoch 720, val loss: 0.5461580157279968
Epoch 730, training loss: 422.8028869628906 = 0.5109736919403076 + 50.0 * 8.44583797454834
Epoch 730, val loss: 0.5422755479812622
Epoch 740, training loss: 422.6797790527344 = 0.506646990776062 + 50.0 * 8.443462371826172
Epoch 740, val loss: 0.5387933850288391
Epoch 750, training loss: 422.6015319824219 = 0.5025502443313599 + 50.0 * 8.44197940826416
Epoch 750, val loss: 0.5355551838874817
Epoch 760, training loss: 422.66754150390625 = 0.4986391067504883 + 50.0 * 8.443378448486328
Epoch 760, val loss: 0.5324379801750183
Epoch 770, training loss: 422.49798583984375 = 0.4948701560497284 + 50.0 * 8.440062522888184
Epoch 770, val loss: 0.529545783996582
Epoch 780, training loss: 422.39886474609375 = 0.49133503437042236 + 50.0 * 8.438150405883789
Epoch 780, val loss: 0.5268191695213318
Epoch 790, training loss: 422.3132019042969 = 0.4879872798919678 + 50.0 * 8.436504364013672
Epoch 790, val loss: 0.5242469310760498
Epoch 800, training loss: 422.3377380371094 = 0.48477602005004883 + 50.0 * 8.43705940246582
Epoch 800, val loss: 0.5218468904495239
Epoch 810, training loss: 422.2517395019531 = 0.48167333006858826 + 50.0 * 8.435401916503906
Epoch 810, val loss: 0.5195329785346985
Epoch 820, training loss: 422.087158203125 = 0.47874903678894043 + 50.0 * 8.432168006896973
Epoch 820, val loss: 0.5173791646957397
Epoch 830, training loss: 422.0204162597656 = 0.4759640693664551 + 50.0 * 8.430889129638672
Epoch 830, val loss: 0.5154036283493042
Epoch 840, training loss: 421.95135498046875 = 0.47326233983039856 + 50.0 * 8.429561614990234
Epoch 840, val loss: 0.5134599804878235
Epoch 850, training loss: 421.888671875 = 0.47068315744400024 + 50.0 * 8.428359985351562
Epoch 850, val loss: 0.5116627812385559
Epoch 860, training loss: 421.78338623046875 = 0.4682252109050751 + 50.0 * 8.426302909851074
Epoch 860, val loss: 0.5099847316741943
Epoch 870, training loss: 421.7120361328125 = 0.4658564329147339 + 50.0 * 8.42492389678955
Epoch 870, val loss: 0.5083761215209961
Epoch 880, training loss: 421.712646484375 = 0.46357864141464233 + 50.0 * 8.424981117248535
Epoch 880, val loss: 0.5069081783294678
Epoch 890, training loss: 421.7720947265625 = 0.46133488416671753 + 50.0 * 8.426215171813965
Epoch 890, val loss: 0.5052459836006165
Epoch 900, training loss: 421.5240478515625 = 0.4592059552669525 + 50.0 * 8.421297073364258
Epoch 900, val loss: 0.5038987398147583
Epoch 910, training loss: 421.44647216796875 = 0.45718449354171753 + 50.0 * 8.419785499572754
Epoch 910, val loss: 0.5026743412017822
Epoch 920, training loss: 421.3768310546875 = 0.4552251100540161 + 50.0 * 8.418432235717773
Epoch 920, val loss: 0.5013987421989441
Epoch 930, training loss: 421.3097229003906 = 0.45333626866340637 + 50.0 * 8.41712760925293
Epoch 930, val loss: 0.5001882314682007
Epoch 940, training loss: 421.4222412109375 = 0.4515036642551422 + 50.0 * 8.419414520263672
Epoch 940, val loss: 0.49911144375801086
Epoch 950, training loss: 421.1954650878906 = 0.4496908187866211 + 50.0 * 8.414916038513184
Epoch 950, val loss: 0.49796703457832336
Epoch 960, training loss: 421.1109313964844 = 0.4479660391807556 + 50.0 * 8.413259506225586
Epoch 960, val loss: 0.49685001373291016
Epoch 970, training loss: 421.03009033203125 = 0.4463130235671997 + 50.0 * 8.411675453186035
Epoch 970, val loss: 0.49588853120803833
Epoch 980, training loss: 421.020751953125 = 0.44470763206481934 + 50.0 * 8.411520957946777
Epoch 980, val loss: 0.4949173331260681
Epoch 990, training loss: 420.9100341796875 = 0.44313567876815796 + 50.0 * 8.409337997436523
Epoch 990, val loss: 0.4940289855003357
Epoch 1000, training loss: 420.9108581542969 = 0.44160711765289307 + 50.0 * 8.409384727478027
Epoch 1000, val loss: 0.4931928813457489
Epoch 1010, training loss: 420.7227783203125 = 0.4401175379753113 + 50.0 * 8.40565299987793
Epoch 1010, val loss: 0.49231085181236267
Epoch 1020, training loss: 420.66143798828125 = 0.438685804605484 + 50.0 * 8.404455184936523
Epoch 1020, val loss: 0.4915176331996918
Epoch 1030, training loss: 420.5915222167969 = 0.437285840511322 + 50.0 * 8.403084754943848
Epoch 1030, val loss: 0.49079111218452454
Epoch 1040, training loss: 420.71270751953125 = 0.4359215497970581 + 50.0 * 8.405535697937012
Epoch 1040, val loss: 0.49008801579475403
Epoch 1050, training loss: 420.5677185058594 = 0.4345603585243225 + 50.0 * 8.402663230895996
Epoch 1050, val loss: 0.489329069852829
Epoch 1060, training loss: 420.4597473144531 = 0.43326619267463684 + 50.0 * 8.400529861450195
Epoch 1060, val loss: 0.48867079615592957
Epoch 1070, training loss: 420.38592529296875 = 0.4320034384727478 + 50.0 * 8.399078369140625
Epoch 1070, val loss: 0.487919420003891
Epoch 1080, training loss: 420.3514099121094 = 0.43077579140663147 + 50.0 * 8.398412704467773
Epoch 1080, val loss: 0.4873300790786743
Epoch 1090, training loss: 420.3114013671875 = 0.429576575756073 + 50.0 * 8.397636413574219
Epoch 1090, val loss: 0.486679345369339
Epoch 1100, training loss: 420.214111328125 = 0.4284074008464813 + 50.0 * 8.395713806152344
Epoch 1100, val loss: 0.48608535528182983
Epoch 1110, training loss: 420.16748046875 = 0.4272722899913788 + 50.0 * 8.394804000854492
Epoch 1110, val loss: 0.4855439364910126
Epoch 1120, training loss: 420.1164855957031 = 0.4261709153652191 + 50.0 * 8.393806457519531
Epoch 1120, val loss: 0.4850200116634369
Epoch 1130, training loss: 420.071533203125 = 0.4250929653644562 + 50.0 * 8.392929077148438
Epoch 1130, val loss: 0.484500527381897
Epoch 1140, training loss: 420.28912353515625 = 0.4240272343158722 + 50.0 * 8.39730167388916
Epoch 1140, val loss: 0.4840661585330963
Epoch 1150, training loss: 420.0570373535156 = 0.4229709804058075 + 50.0 * 8.392681121826172
Epoch 1150, val loss: 0.4834668040275574
Epoch 1160, training loss: 419.9564208984375 = 0.42197051644325256 + 50.0 * 8.3906888961792
Epoch 1160, val loss: 0.48299580812454224
Epoch 1170, training loss: 419.9007873535156 = 0.42099204659461975 + 50.0 * 8.389595985412598
Epoch 1170, val loss: 0.48254796862602234
Epoch 1180, training loss: 419.8757019042969 = 0.4200345277786255 + 50.0 * 8.389113426208496
Epoch 1180, val loss: 0.48210465908050537
Epoch 1190, training loss: 420.05963134765625 = 0.41908934712409973 + 50.0 * 8.392810821533203
Epoch 1190, val loss: 0.48168158531188965
Epoch 1200, training loss: 419.88287353515625 = 0.4181578755378723 + 50.0 * 8.389294624328613
Epoch 1200, val loss: 0.4811364710330963
Epoch 1210, training loss: 419.8010559082031 = 0.4172457754611969 + 50.0 * 8.387676239013672
Epoch 1210, val loss: 0.4808031916618347
Epoch 1220, training loss: 419.73382568359375 = 0.41635632514953613 + 50.0 * 8.38634967803955
Epoch 1220, val loss: 0.4804609417915344
Epoch 1230, training loss: 419.6986389160156 = 0.41548481583595276 + 50.0 * 8.385663032531738
Epoch 1230, val loss: 0.4800405502319336
Epoch 1240, training loss: 419.6717224121094 = 0.41462773084640503 + 50.0 * 8.385141372680664
Epoch 1240, val loss: 0.47965091466903687
Epoch 1250, training loss: 419.8456726074219 = 0.41377490758895874 + 50.0 * 8.38863754272461
Epoch 1250, val loss: 0.4792026877403259
Epoch 1260, training loss: 419.66424560546875 = 0.41293442249298096 + 50.0 * 8.385025978088379
Epoch 1260, val loss: 0.4789464771747589
Epoch 1270, training loss: 419.57666015625 = 0.4121072292327881 + 50.0 * 8.383291244506836
Epoch 1270, val loss: 0.47857847809791565
Epoch 1280, training loss: 419.54400634765625 = 0.4112985134124756 + 50.0 * 8.382654190063477
Epoch 1280, val loss: 0.4782252609729767
Epoch 1290, training loss: 419.806640625 = 0.4104999005794525 + 50.0 * 8.387923240661621
Epoch 1290, val loss: 0.4779294729232788
Epoch 1300, training loss: 419.6007080078125 = 0.40967458486557007 + 50.0 * 8.383820533752441
Epoch 1300, val loss: 0.47747519612312317
Epoch 1310, training loss: 419.4931335449219 = 0.40888744592666626 + 50.0 * 8.381685256958008
Epoch 1310, val loss: 0.47712042927742004
Epoch 1320, training loss: 419.4294738769531 = 0.40811532735824585 + 50.0 * 8.380427360534668
Epoch 1320, val loss: 0.476812481880188
Epoch 1330, training loss: 419.4071044921875 = 0.4073515832424164 + 50.0 * 8.379995346069336
Epoch 1330, val loss: 0.47652557492256165
Epoch 1340, training loss: 419.3774719238281 = 0.4065961539745331 + 50.0 * 8.379417419433594
Epoch 1340, val loss: 0.4762074649333954
Epoch 1350, training loss: 419.5165710449219 = 0.4058505594730377 + 50.0 * 8.382214546203613
Epoch 1350, val loss: 0.47600752115249634
Epoch 1360, training loss: 419.3802185058594 = 0.40510472655296326 + 50.0 * 8.379502296447754
Epoch 1360, val loss: 0.4754413366317749
Epoch 1370, training loss: 419.3083190917969 = 0.40437451004981995 + 50.0 * 8.37807846069336
Epoch 1370, val loss: 0.4752442240715027
Epoch 1380, training loss: 419.2624206542969 = 0.4036608636379242 + 50.0 * 8.377175331115723
Epoch 1380, val loss: 0.4749145209789276
Epoch 1390, training loss: 419.4151611328125 = 0.4029525816440582 + 50.0 * 8.380244255065918
Epoch 1390, val loss: 0.47448915243148804
Epoch 1400, training loss: 419.3352966308594 = 0.40224459767341614 + 50.0 * 8.378661155700684
Epoch 1400, val loss: 0.4743325412273407
Epoch 1410, training loss: 419.1700134277344 = 0.4015420079231262 + 50.0 * 8.37536907196045
Epoch 1410, val loss: 0.4740464687347412
Epoch 1420, training loss: 419.1506042480469 = 0.4008576273918152 + 50.0 * 8.374995231628418
Epoch 1420, val loss: 0.47370895743370056
Epoch 1430, training loss: 419.120849609375 = 0.400178462266922 + 50.0 * 8.37441349029541
Epoch 1430, val loss: 0.47343605756759644
Epoch 1440, training loss: 419.2162780761719 = 0.3995000720024109 + 50.0 * 8.376335144042969
Epoch 1440, val loss: 0.4731960594654083
Epoch 1450, training loss: 419.04779052734375 = 0.3988293409347534 + 50.0 * 8.372979164123535
Epoch 1450, val loss: 0.472849041223526
Epoch 1460, training loss: 419.09259033203125 = 0.3981800079345703 + 50.0 * 8.37388801574707
Epoch 1460, val loss: 0.47255754470825195
Epoch 1470, training loss: 419.00408935546875 = 0.39752399921417236 + 50.0 * 8.37213134765625
Epoch 1470, val loss: 0.4723483622074127
Epoch 1480, training loss: 418.94598388671875 = 0.39688384532928467 + 50.0 * 8.37098217010498
Epoch 1480, val loss: 0.47208815813064575
Epoch 1490, training loss: 418.9200134277344 = 0.39625808596611023 + 50.0 * 8.370474815368652
Epoch 1490, val loss: 0.4718034565448761
Epoch 1500, training loss: 418.9401550292969 = 0.3956308662891388 + 50.0 * 8.370890617370605
Epoch 1500, val loss: 0.4715629518032074
Epoch 1510, training loss: 418.8387756347656 = 0.39500370621681213 + 50.0 * 8.368875503540039
Epoch 1510, val loss: 0.4713284969329834
Epoch 1520, training loss: 418.8473205566406 = 0.3943766951560974 + 50.0 * 8.369058609008789
Epoch 1520, val loss: 0.47104161977767944
Epoch 1530, training loss: 418.78265380859375 = 0.39376771450042725 + 50.0 * 8.367777824401855
Epoch 1530, val loss: 0.47086259722709656
Epoch 1540, training loss: 418.7294006347656 = 0.39315682649612427 + 50.0 * 8.366724967956543
Epoch 1540, val loss: 0.4705718755722046
Epoch 1550, training loss: 418.7605895996094 = 0.39256203174591064 + 50.0 * 8.367361068725586
Epoch 1550, val loss: 0.47041553258895874
Epoch 1560, training loss: 418.7386169433594 = 0.3919573128223419 + 50.0 * 8.36693286895752
Epoch 1560, val loss: 0.4699867069721222
Epoch 1570, training loss: 418.6830139160156 = 0.39134496450424194 + 50.0 * 8.365833282470703
Epoch 1570, val loss: 0.4699045717716217
Epoch 1580, training loss: 418.6328125 = 0.39075714349746704 + 50.0 * 8.36484146118164
Epoch 1580, val loss: 0.46968522667884827
Epoch 1590, training loss: 418.561767578125 = 0.39017269015312195 + 50.0 * 8.363431930541992
Epoch 1590, val loss: 0.46942466497421265
Epoch 1600, training loss: 418.52752685546875 = 0.38958805799484253 + 50.0 * 8.36275863647461
Epoch 1600, val loss: 0.4691692888736725
Epoch 1610, training loss: 418.6100158691406 = 0.38900503516197205 + 50.0 * 8.364419937133789
Epoch 1610, val loss: 0.4688429534435272
Epoch 1620, training loss: 418.505859375 = 0.3884037435054779 + 50.0 * 8.362349510192871
Epoch 1620, val loss: 0.4688481092453003
Epoch 1630, training loss: 418.50164794921875 = 0.38780027627944946 + 50.0 * 8.362277030944824
Epoch 1630, val loss: 0.4684792757034302
Epoch 1640, training loss: 418.4462585449219 = 0.3872130513191223 + 50.0 * 8.361181259155273
Epoch 1640, val loss: 0.46832382678985596
Epoch 1650, training loss: 418.4453125 = 0.386623740196228 + 50.0 * 8.361173629760742
Epoch 1650, val loss: 0.4681433439254761
Epoch 1660, training loss: 418.34857177734375 = 0.3860301971435547 + 50.0 * 8.359251022338867
Epoch 1660, val loss: 0.4678262770175934
Epoch 1670, training loss: 418.3634338378906 = 0.38544610142707825 + 50.0 * 8.359560012817383
Epoch 1670, val loss: 0.467637836933136
Epoch 1680, training loss: 418.34783935546875 = 0.3848608136177063 + 50.0 * 8.359259605407715
Epoch 1680, val loss: 0.46739837527275085
Epoch 1690, training loss: 418.48931884765625 = 0.3842793107032776 + 50.0 * 8.362100601196289
Epoch 1690, val loss: 0.46727168560028076
Epoch 1700, training loss: 418.3191223144531 = 0.38368260860443115 + 50.0 * 8.358708381652832
Epoch 1700, val loss: 0.46699827909469604
Epoch 1710, training loss: 418.270751953125 = 0.38310545682907104 + 50.0 * 8.357752799987793
Epoch 1710, val loss: 0.46682316064834595
Epoch 1720, training loss: 418.3321533203125 = 0.3825259804725647 + 50.0 * 8.358992576599121
Epoch 1720, val loss: 0.46659761667251587
Epoch 1730, training loss: 418.1875 = 0.3819528818130493 + 50.0 * 8.356110572814941
Epoch 1730, val loss: 0.46635714173316956
Epoch 1740, training loss: 418.151611328125 = 0.3813766539096832 + 50.0 * 8.3554048538208
Epoch 1740, val loss: 0.4661855101585388
Epoch 1750, training loss: 418.1363525390625 = 0.3808044493198395 + 50.0 * 8.355111122131348
Epoch 1750, val loss: 0.4659835696220398
Epoch 1760, training loss: 418.1130676269531 = 0.38023123145103455 + 50.0 * 8.354657173156738
Epoch 1760, val loss: 0.4657878279685974
Epoch 1770, training loss: 418.1774597167969 = 0.37966158986091614 + 50.0 * 8.355956077575684
Epoch 1770, val loss: 0.46564191579818726
Epoch 1780, training loss: 418.2001037597656 = 0.37908655405044556 + 50.0 * 8.356420516967773
Epoch 1780, val loss: 0.465247243642807
Epoch 1790, training loss: 418.1140441894531 = 0.37849903106689453 + 50.0 * 8.354710578918457
Epoch 1790, val loss: 0.46514734625816345
Epoch 1800, training loss: 418.04779052734375 = 0.37793421745300293 + 50.0 * 8.353397369384766
Epoch 1800, val loss: 0.4649156332015991
Epoch 1810, training loss: 418.0033874511719 = 0.3773765563964844 + 50.0 * 8.352519989013672
Epoch 1810, val loss: 0.46470239758491516
Epoch 1820, training loss: 418.0207214355469 = 0.37682148814201355 + 50.0 * 8.352877616882324
Epoch 1820, val loss: 0.46455225348472595
Epoch 1830, training loss: 418.1553039550781 = 0.37625452876091003 + 50.0 * 8.355581283569336
Epoch 1830, val loss: 0.4643224775791168
Epoch 1840, training loss: 417.9467468261719 = 0.37566938996315 + 50.0 * 8.351421356201172
Epoch 1840, val loss: 0.4640953540802002
Epoch 1850, training loss: 417.9542236328125 = 0.37510111927986145 + 50.0 * 8.351582527160645
Epoch 1850, val loss: 0.4638223350048065
Epoch 1860, training loss: 417.9211120605469 = 0.3745381534099579 + 50.0 * 8.350931167602539
Epoch 1860, val loss: 0.4636760950088501
Epoch 1870, training loss: 417.8989562988281 = 0.37397295236587524 + 50.0 * 8.350500106811523
Epoch 1870, val loss: 0.46345457434654236
Epoch 1880, training loss: 418.2675476074219 = 0.3734082877635956 + 50.0 * 8.357882499694824
Epoch 1880, val loss: 0.46339166164398193
Epoch 1890, training loss: 417.98651123046875 = 0.372814804315567 + 50.0 * 8.352273941040039
Epoch 1890, val loss: 0.4629686772823334
Epoch 1900, training loss: 417.8780517578125 = 0.3722341060638428 + 50.0 * 8.350116729736328
Epoch 1900, val loss: 0.46283331513404846
Epoch 1910, training loss: 417.8319396972656 = 0.37166327238082886 + 50.0 * 8.349205017089844
Epoch 1910, val loss: 0.46262767910957336
Epoch 1920, training loss: 417.8152770996094 = 0.3710944354534149 + 50.0 * 8.348883628845215
Epoch 1920, val loss: 0.4624694883823395
Epoch 1930, training loss: 417.8012390136719 = 0.370525598526001 + 50.0 * 8.348614692687988
Epoch 1930, val loss: 0.4622664451599121
Epoch 1940, training loss: 417.9777526855469 = 0.3699510097503662 + 50.0 * 8.352155685424805
Epoch 1940, val loss: 0.4620400667190552
Epoch 1950, training loss: 417.8498229980469 = 0.3693617582321167 + 50.0 * 8.349609375
Epoch 1950, val loss: 0.4619297385215759
Epoch 1960, training loss: 417.7987976074219 = 0.36877360939979553 + 50.0 * 8.348600387573242
Epoch 1960, val loss: 0.4617401957511902
Epoch 1970, training loss: 417.7413635253906 = 0.36819013953208923 + 50.0 * 8.347463607788086
Epoch 1970, val loss: 0.4615149199962616
Epoch 1980, training loss: 417.72430419921875 = 0.3676088750362396 + 50.0 * 8.34713363647461
Epoch 1980, val loss: 0.46136897802352905
Epoch 1990, training loss: 417.7110290527344 = 0.3670259714126587 + 50.0 * 8.346879959106445
Epoch 1990, val loss: 0.46117261052131653
Epoch 2000, training loss: 417.8318176269531 = 0.3664444386959076 + 50.0 * 8.3493070602417
Epoch 2000, val loss: 0.4609490931034088
Epoch 2010, training loss: 417.80615234375 = 0.3658394515514374 + 50.0 * 8.348806381225586
Epoch 2010, val loss: 0.4608811140060425
Epoch 2020, training loss: 417.6958923339844 = 0.3652362823486328 + 50.0 * 8.346612930297852
Epoch 2020, val loss: 0.46058914065361023
Epoch 2030, training loss: 417.6814270019531 = 0.3646466135978699 + 50.0 * 8.346335411071777
Epoch 2030, val loss: 0.46045419573783875
Epoch 2040, training loss: 417.64886474609375 = 0.36405545473098755 + 50.0 * 8.345696449279785
Epoch 2040, val loss: 0.4603356719017029
Epoch 2050, training loss: 417.6338806152344 = 0.36346104741096497 + 50.0 * 8.34540843963623
Epoch 2050, val loss: 0.46018272638320923
Epoch 2060, training loss: 417.626708984375 = 0.3628626763820648 + 50.0 * 8.345276832580566
Epoch 2060, val loss: 0.46002256870269775
Epoch 2070, training loss: 418.07501220703125 = 0.36225658655166626 + 50.0 * 8.354255676269531
Epoch 2070, val loss: 0.45995840430259705
Epoch 2080, training loss: 417.7335205078125 = 0.36164090037345886 + 50.0 * 8.347437858581543
Epoch 2080, val loss: 0.45964255928993225
Epoch 2090, training loss: 417.6028747558594 = 0.3610202670097351 + 50.0 * 8.344837188720703
Epoch 2090, val loss: 0.4595627784729004
Epoch 2100, training loss: 417.5884094238281 = 0.36040428280830383 + 50.0 * 8.344559669494629
Epoch 2100, val loss: 0.4593614935874939
Epoch 2110, training loss: 417.5617980957031 = 0.3597938120365143 + 50.0 * 8.344039916992188
Epoch 2110, val loss: 0.45926621556282043
Epoch 2120, training loss: 417.55133056640625 = 0.35918059945106506 + 50.0 * 8.343842506408691
Epoch 2120, val loss: 0.45910027623176575
Epoch 2130, training loss: 417.5404357910156 = 0.35856306552886963 + 50.0 * 8.343637466430664
Epoch 2130, val loss: 0.45897966623306274
Epoch 2140, training loss: 417.652587890625 = 0.3579471707344055 + 50.0 * 8.345892906188965
Epoch 2140, val loss: 0.45895013213157654
Epoch 2150, training loss: 417.5337829589844 = 0.35730814933776855 + 50.0 * 8.34352970123291
Epoch 2150, val loss: 0.4585549235343933
Epoch 2160, training loss: 417.5394287109375 = 0.3566790521144867 + 50.0 * 8.34365463256836
Epoch 2160, val loss: 0.458583801984787
Epoch 2170, training loss: 417.5089416503906 = 0.3560488820075989 + 50.0 * 8.343057632446289
Epoch 2170, val loss: 0.45828813314437866
Epoch 2180, training loss: 417.5334167480469 = 0.3554222881793976 + 50.0 * 8.343560218811035
Epoch 2180, val loss: 0.458177387714386
Epoch 2190, training loss: 417.5411071777344 = 0.35478469729423523 + 50.0 * 8.34372615814209
Epoch 2190, val loss: 0.45807844400405884
Epoch 2200, training loss: 417.4938659667969 = 0.3541509509086609 + 50.0 * 8.342794418334961
Epoch 2200, val loss: 0.4579629600048065
Epoch 2210, training loss: 417.46173095703125 = 0.3535161316394806 + 50.0 * 8.342164039611816
Epoch 2210, val loss: 0.4578036665916443
Epoch 2220, training loss: 417.4575500488281 = 0.35288646817207336 + 50.0 * 8.342093467712402
Epoch 2220, val loss: 0.4577302038669586
Epoch 2230, training loss: 417.62921142578125 = 0.3522529602050781 + 50.0 * 8.345539093017578
Epoch 2230, val loss: 0.4576704800128937
Epoch 2240, training loss: 417.4726257324219 = 0.3516032099723816 + 50.0 * 8.34242057800293
Epoch 2240, val loss: 0.4573833644390106
Epoch 2250, training loss: 417.4261169433594 = 0.3509640097618103 + 50.0 * 8.341503143310547
Epoch 2250, val loss: 0.45736631751060486
Epoch 2260, training loss: 417.3978576660156 = 0.3503205180168152 + 50.0 * 8.340950965881348
Epoch 2260, val loss: 0.4572339951992035
Epoch 2270, training loss: 417.38311767578125 = 0.34968137741088867 + 50.0 * 8.340668678283691
Epoch 2270, val loss: 0.4571574032306671
Epoch 2280, training loss: 417.43048095703125 = 0.3490368723869324 + 50.0 * 8.341629028320312
Epoch 2280, val loss: 0.4570483863353729
Epoch 2290, training loss: 417.4356994628906 = 0.34838178753852844 + 50.0 * 8.34174633026123
Epoch 2290, val loss: 0.45700132846832275
Epoch 2300, training loss: 417.39263916015625 = 0.34771546721458435 + 50.0 * 8.340898513793945
Epoch 2300, val loss: 0.4568701684474945
Epoch 2310, training loss: 417.370361328125 = 0.34705862402915955 + 50.0 * 8.340466499328613
Epoch 2310, val loss: 0.45678091049194336
Epoch 2320, training loss: 417.3380126953125 = 0.3463956117630005 + 50.0 * 8.339832305908203
Epoch 2320, val loss: 0.4566751718521118
Epoch 2330, training loss: 417.37274169921875 = 0.3457324206829071 + 50.0 * 8.340539932250977
Epoch 2330, val loss: 0.4566372036933899
Epoch 2340, training loss: 417.3571472167969 = 0.3450614809989929 + 50.0 * 8.340241432189941
Epoch 2340, val loss: 0.45650801062583923
Epoch 2350, training loss: 417.432861328125 = 0.3443833887577057 + 50.0 * 8.341769218444824
Epoch 2350, val loss: 0.45636633038520813
Epoch 2360, training loss: 417.3168640136719 = 0.3436964452266693 + 50.0 * 8.339463233947754
Epoch 2360, val loss: 0.4562678933143616
Epoch 2370, training loss: 417.2801208496094 = 0.34301918745040894 + 50.0 * 8.33874225616455
Epoch 2370, val loss: 0.4561534821987152
Epoch 2380, training loss: 417.257080078125 = 0.3423411250114441 + 50.0 * 8.338294982910156
Epoch 2380, val loss: 0.45612236857414246
Epoch 2390, training loss: 417.25823974609375 = 0.34166109561920166 + 50.0 * 8.33833122253418
Epoch 2390, val loss: 0.4560900926589966
Epoch 2400, training loss: 417.3365478515625 = 0.34098386764526367 + 50.0 * 8.339911460876465
Epoch 2400, val loss: 0.45612508058547974
Epoch 2410, training loss: 417.2929992675781 = 0.3402906060218811 + 50.0 * 8.339054107666016
Epoch 2410, val loss: 0.4558649957180023
Epoch 2420, training loss: 417.2972717285156 = 0.33960047364234924 + 50.0 * 8.339153289794922
Epoch 2420, val loss: 0.4558439254760742
Epoch 2430, training loss: 417.22137451171875 = 0.338910847902298 + 50.0 * 8.33764934539795
Epoch 2430, val loss: 0.45570889115333557
Epoch 2440, training loss: 417.2052001953125 = 0.33822688460350037 + 50.0 * 8.337339401245117
Epoch 2440, val loss: 0.4557276964187622
Epoch 2450, training loss: 417.19384765625 = 0.3375391364097595 + 50.0 * 8.337126731872559
Epoch 2450, val loss: 0.45565271377563477
Epoch 2460, training loss: 417.2030944824219 = 0.3368486166000366 + 50.0 * 8.337325096130371
Epoch 2460, val loss: 0.45562461018562317
Epoch 2470, training loss: 417.2952575683594 = 0.3361549377441406 + 50.0 * 8.339181900024414
Epoch 2470, val loss: 0.45555010437965393
Epoch 2480, training loss: 417.20745849609375 = 0.3354419469833374 + 50.0 * 8.337440490722656
Epoch 2480, val loss: 0.4555759131908417
Epoch 2490, training loss: 417.2408142089844 = 0.3347405195236206 + 50.0 * 8.33812141418457
Epoch 2490, val loss: 0.4554252028465271
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8158295281582952
0.8630007969282041
=== training gcn model ===
Epoch 0, training loss: 530.2249755859375 = 1.1095236539840698 + 50.0 * 10.58230972290039
Epoch 0, val loss: 1.1087154150009155
Epoch 10, training loss: 530.2044067382812 = 1.1047074794769287 + 50.0 * 10.581993103027344
Epoch 10, val loss: 1.1038998365402222
Epoch 20, training loss: 530.1167602539062 = 1.0997451543807983 + 50.0 * 10.580340385437012
Epoch 20, val loss: 1.0989261865615845
Epoch 30, training loss: 529.7066040039062 = 1.094509243965149 + 50.0 * 10.572242736816406
Epoch 30, val loss: 1.093677282333374
Epoch 40, training loss: 528.1026000976562 = 1.0887408256530762 + 50.0 * 10.540277481079102
Epoch 40, val loss: 1.0878572463989258
Epoch 50, training loss: 523.830078125 = 1.0824471712112427 + 50.0 * 10.454952239990234
Epoch 50, val loss: 1.0815727710723877
Epoch 60, training loss: 516.2827758789062 = 1.0764137506484985 + 50.0 * 10.30412769317627
Epoch 60, val loss: 1.0756843090057373
Epoch 70, training loss: 508.8221130371094 = 1.0708972215652466 + 50.0 * 10.155024528503418
Epoch 70, val loss: 1.070257306098938
Epoch 80, training loss: 501.2602233886719 = 1.06600821018219 + 50.0 * 10.003884315490723
Epoch 80, val loss: 1.065516471862793
Epoch 90, training loss: 488.0743408203125 = 1.0610296726226807 + 50.0 * 9.740265846252441
Epoch 90, val loss: 1.0607014894485474
Epoch 100, training loss: 471.3579406738281 = 1.054931879043579 + 50.0 * 9.406060218811035
Epoch 100, val loss: 1.054904818534851
Epoch 110, training loss: 461.2091979980469 = 1.049028754234314 + 50.0 * 9.203203201293945
Epoch 110, val loss: 1.0493453741073608
Epoch 120, training loss: 458.66168212890625 = 1.0438371896743774 + 50.0 * 9.15235710144043
Epoch 120, val loss: 1.0443229675292969
Epoch 130, training loss: 455.5340881347656 = 1.039360761642456 + 50.0 * 9.08989429473877
Epoch 130, val loss: 1.039922833442688
Epoch 140, training loss: 451.1875 = 1.035237193107605 + 50.0 * 9.003045082092285
Epoch 140, val loss: 1.0358868837356567
Epoch 150, training loss: 446.4078674316406 = 1.0314645767211914 + 50.0 * 8.907527923583984
Epoch 150, val loss: 1.032263994216919
Epoch 160, training loss: 443.0653381347656 = 1.0277341604232788 + 50.0 * 8.840751647949219
Epoch 160, val loss: 1.0286906957626343
Epoch 170, training loss: 441.3438415527344 = 1.0233384370803833 + 50.0 * 8.80640983581543
Epoch 170, val loss: 1.0242570638656616
Epoch 180, training loss: 439.5611572265625 = 1.0181723833084106 + 50.0 * 8.770859718322754
Epoch 180, val loss: 1.0192465782165527
Epoch 190, training loss: 438.5806579589844 = 1.0127530097961426 + 50.0 * 8.751358032226562
Epoch 190, val loss: 1.0139199495315552
Epoch 200, training loss: 437.9267272949219 = 1.0071827173233032 + 50.0 * 8.738390922546387
Epoch 200, val loss: 1.008550763130188
Epoch 210, training loss: 437.3334045410156 = 1.0014015436172485 + 50.0 * 8.726639747619629
Epoch 210, val loss: 1.0029985904693604
Epoch 220, training loss: 436.7225036621094 = 0.9953963160514832 + 50.0 * 8.714542388916016
Epoch 220, val loss: 0.9971979856491089
Epoch 230, training loss: 436.06219482421875 = 0.9891917109489441 + 50.0 * 8.701459884643555
Epoch 230, val loss: 0.9912022948265076
Epoch 240, training loss: 435.33941650390625 = 0.9828725457191467 + 50.0 * 8.68713092803955
Epoch 240, val loss: 0.9851062893867493
Epoch 250, training loss: 434.58746337890625 = 0.9764385223388672 + 50.0 * 8.672220230102539
Epoch 250, val loss: 0.9788915514945984
Epoch 260, training loss: 433.8785095214844 = 0.9697926044464111 + 50.0 * 8.658174514770508
Epoch 260, val loss: 0.9724642038345337
Epoch 270, training loss: 433.3216552734375 = 0.9627639651298523 + 50.0 * 8.647177696228027
Epoch 270, val loss: 0.9656398296356201
Epoch 280, training loss: 432.6676330566406 = 0.9552935361862183 + 50.0 * 8.634246826171875
Epoch 280, val loss: 0.9584221839904785
Epoch 290, training loss: 432.00537109375 = 0.9476417303085327 + 50.0 * 8.62115478515625
Epoch 290, val loss: 0.95102459192276
Epoch 300, training loss: 431.782470703125 = 0.9397888779640198 + 50.0 * 8.616853713989258
Epoch 300, val loss: 0.943490743637085
Epoch 310, training loss: 430.90643310546875 = 0.9315841197967529 + 50.0 * 8.599496841430664
Epoch 310, val loss: 0.9355202317237854
Epoch 320, training loss: 430.3147277832031 = 0.923103392124176 + 50.0 * 8.5878324508667
Epoch 320, val loss: 0.9273205995559692
Epoch 330, training loss: 429.8388977050781 = 0.9142667055130005 + 50.0 * 8.578492164611816
Epoch 330, val loss: 0.918770432472229
Epoch 340, training loss: 429.3788757324219 = 0.905123233795166 + 50.0 * 8.569475173950195
Epoch 340, val loss: 0.909946858882904
Epoch 350, training loss: 428.9359436035156 = 0.8956913948059082 + 50.0 * 8.560805320739746
Epoch 350, val loss: 0.9008226990699768
Epoch 360, training loss: 429.0318298339844 = 0.8860011696815491 + 50.0 * 8.56291675567627
Epoch 360, val loss: 0.8915024399757385
Epoch 370, training loss: 428.2018127441406 = 0.8759523630142212 + 50.0 * 8.546517372131348
Epoch 370, val loss: 0.8817370533943176
Epoch 380, training loss: 427.81195068359375 = 0.8658444881439209 + 50.0 * 8.538922309875488
Epoch 380, val loss: 0.8720273375511169
Epoch 390, training loss: 427.44512939453125 = 0.8555986881256104 + 50.0 * 8.531790733337402
Epoch 390, val loss: 0.862146258354187
Epoch 400, training loss: 427.1111755371094 = 0.8452719449996948 + 50.0 * 8.525318145751953
Epoch 400, val loss: 0.8521866202354431
Epoch 410, training loss: 426.8034973144531 = 0.8348333835601807 + 50.0 * 8.519372940063477
Epoch 410, val loss: 0.8421533703804016
Epoch 420, training loss: 426.58831787109375 = 0.8242148160934448 + 50.0 * 8.515281677246094
Epoch 420, val loss: 0.8319376111030579
Epoch 430, training loss: 426.2901611328125 = 0.8134797215461731 + 50.0 * 8.509533882141113
Epoch 430, val loss: 0.8216791152954102
Epoch 440, training loss: 426.0098876953125 = 0.8027138113975525 + 50.0 * 8.504143714904785
Epoch 440, val loss: 0.8113798499107361
Epoch 450, training loss: 425.7691650390625 = 0.7918928265571594 + 50.0 * 8.499545097351074
Epoch 450, val loss: 0.8010327219963074
Epoch 460, training loss: 425.5474548339844 = 0.7809907793998718 + 50.0 * 8.495328903198242
Epoch 460, val loss: 0.790587306022644
Epoch 470, training loss: 425.35357666015625 = 0.7700453996658325 + 50.0 * 8.491670608520508
Epoch 470, val loss: 0.780179500579834
Epoch 480, training loss: 425.149169921875 = 0.7590998411178589 + 50.0 * 8.487801551818848
Epoch 480, val loss: 0.7697586417198181
Epoch 490, training loss: 424.9342956542969 = 0.7480954527854919 + 50.0 * 8.483723640441895
Epoch 490, val loss: 0.7592878937721252
Epoch 500, training loss: 424.9732971191406 = 0.737011730670929 + 50.0 * 8.484725952148438
Epoch 500, val loss: 0.7488008141517639
Epoch 510, training loss: 424.5799865722656 = 0.7259725332260132 + 50.0 * 8.477080345153809
Epoch 510, val loss: 0.7383460998535156
Epoch 520, training loss: 424.40972900390625 = 0.7151168584823608 + 50.0 * 8.473892211914062
Epoch 520, val loss: 0.7281163930892944
Epoch 530, training loss: 424.2516784667969 = 0.7043992280960083 + 50.0 * 8.470945358276367
Epoch 530, val loss: 0.7180808186531067
Epoch 540, training loss: 424.12249755859375 = 0.6938648819923401 + 50.0 * 8.468572616577148
Epoch 540, val loss: 0.7082152962684631
Epoch 550, training loss: 423.9889221191406 = 0.6834957003593445 + 50.0 * 8.466108322143555
Epoch 550, val loss: 0.6985151767730713
Epoch 560, training loss: 423.9599609375 = 0.6733315587043762 + 50.0 * 8.46573257446289
Epoch 560, val loss: 0.6890398859977722
Epoch 570, training loss: 423.72894287109375 = 0.663430392742157 + 50.0 * 8.461310386657715
Epoch 570, val loss: 0.6799973845481873
Epoch 580, training loss: 423.5624084472656 = 0.6538923978805542 + 50.0 * 8.458169937133789
Epoch 580, val loss: 0.6711792945861816
Epoch 590, training loss: 423.46832275390625 = 0.644662082195282 + 50.0 * 8.456473350524902
Epoch 590, val loss: 0.662738561630249
Epoch 600, training loss: 423.4109802246094 = 0.6357119083404541 + 50.0 * 8.45550537109375
Epoch 600, val loss: 0.654548704624176
Epoch 610, training loss: 423.2065124511719 = 0.6270725727081299 + 50.0 * 8.45158863067627
Epoch 610, val loss: 0.6468421816825867
Epoch 620, training loss: 423.0725402832031 = 0.6188004016876221 + 50.0 * 8.449074745178223
Epoch 620, val loss: 0.639350414276123
Epoch 630, training loss: 423.1350402832031 = 0.6108309030532837 + 50.0 * 8.450484275817871
Epoch 630, val loss: 0.6323198676109314
Epoch 640, training loss: 422.82745361328125 = 0.6031796932220459 + 50.0 * 8.444485664367676
Epoch 640, val loss: 0.6254802942276001
Epoch 650, training loss: 422.70458984375 = 0.5958999395370483 + 50.0 * 8.442173957824707
Epoch 650, val loss: 0.6191486120223999
Epoch 660, training loss: 422.56475830078125 = 0.5889756083488464 + 50.0 * 8.439515113830566
Epoch 660, val loss: 0.6130626201629639
Epoch 670, training loss: 422.4553527832031 = 0.5823834538459778 + 50.0 * 8.437458992004395
Epoch 670, val loss: 0.6073796153068542
Epoch 680, training loss: 422.5638122558594 = 0.5760422945022583 + 50.0 * 8.4397554397583
Epoch 680, val loss: 0.601896345615387
Epoch 690, training loss: 422.3049621582031 = 0.5699666142463684 + 50.0 * 8.434700012207031
Epoch 690, val loss: 0.5967991352081299
Epoch 700, training loss: 422.1274108886719 = 0.5642786622047424 + 50.0 * 8.431262969970703
Epoch 700, val loss: 0.5920671224594116
Epoch 710, training loss: 422.0200500488281 = 0.558883786201477 + 50.0 * 8.42922306060791
Epoch 710, val loss: 0.5875623226165771
Epoch 720, training loss: 422.111572265625 = 0.5537874698638916 + 50.0 * 8.43115520477295
Epoch 720, val loss: 0.5833896994590759
Epoch 730, training loss: 421.9195556640625 = 0.5488542318344116 + 50.0 * 8.427413940429688
Epoch 730, val loss: 0.5794317126274109
Epoch 740, training loss: 421.7681884765625 = 0.5442409515380859 + 50.0 * 8.424478530883789
Epoch 740, val loss: 0.5756618976593018
Epoch 750, training loss: 421.6496887207031 = 0.5398551225662231 + 50.0 * 8.422196388244629
Epoch 750, val loss: 0.5721864700317383
Epoch 760, training loss: 421.5635986328125 = 0.5356924533843994 + 50.0 * 8.420557975769043
Epoch 760, val loss: 0.5688905715942383
Epoch 770, training loss: 421.50250244140625 = 0.5317249298095703 + 50.0 * 8.419415473937988
Epoch 770, val loss: 0.5658565759658813
Epoch 780, training loss: 421.49493408203125 = 0.5279115438461304 + 50.0 * 8.419340133666992
Epoch 780, val loss: 0.562934398651123
Epoch 790, training loss: 421.41033935546875 = 0.5242557525634766 + 50.0 * 8.41772174835205
Epoch 790, val loss: 0.5601696968078613
Epoch 800, training loss: 421.33184814453125 = 0.5208153128623962 + 50.0 * 8.416220664978027
Epoch 800, val loss: 0.5575646162033081
Epoch 810, training loss: 421.2307434082031 = 0.5175530314445496 + 50.0 * 8.414263725280762
Epoch 810, val loss: 0.5551673769950867
Epoch 820, training loss: 421.1987609863281 = 0.5144507884979248 + 50.0 * 8.41368579864502
Epoch 820, val loss: 0.5529575347900391
Epoch 830, training loss: 421.24627685546875 = 0.5114613771438599 + 50.0 * 8.414695739746094
Epoch 830, val loss: 0.5508027672767639
Epoch 840, training loss: 421.0943298339844 = 0.50859534740448 + 50.0 * 8.411714553833008
Epoch 840, val loss: 0.5486891865730286
Epoch 850, training loss: 421.0345458984375 = 0.5058842301368713 + 50.0 * 8.41057300567627
Epoch 850, val loss: 0.5467942357063293
Epoch 860, training loss: 421.0086364746094 = 0.5033076405525208 + 50.0 * 8.410106658935547
Epoch 860, val loss: 0.5450427532196045
Epoch 870, training loss: 421.027099609375 = 0.5008190274238586 + 50.0 * 8.41052532196045
Epoch 870, val loss: 0.543303370475769
Epoch 880, training loss: 421.0693664550781 = 0.4984052777290344 + 50.0 * 8.411418914794922
Epoch 880, val loss: 0.5415931940078735
Epoch 890, training loss: 420.9365234375 = 0.4961107671260834 + 50.0 * 8.408807754516602
Epoch 890, val loss: 0.5400988459587097
Epoch 900, training loss: 420.8554992675781 = 0.4939347505569458 + 50.0 * 8.407231330871582
Epoch 900, val loss: 0.5386174321174622
Epoch 910, training loss: 420.81451416015625 = 0.4918555021286011 + 50.0 * 8.406453132629395
Epoch 910, val loss: 0.5372521877288818
Epoch 920, training loss: 420.7793884277344 = 0.4898568391799927 + 50.0 * 8.405790328979492
Epoch 920, val loss: 0.5359617471694946
Epoch 930, training loss: 420.8321228027344 = 0.4879292845726013 + 50.0 * 8.40688419342041
Epoch 930, val loss: 0.5347164869308472
Epoch 940, training loss: 420.7611389160156 = 0.4860215187072754 + 50.0 * 8.405502319335938
Epoch 940, val loss: 0.533433198928833
Epoch 950, training loss: 420.95184326171875 = 0.4841981828212738 + 50.0 * 8.409353256225586
Epoch 950, val loss: 0.5322415232658386
Epoch 960, training loss: 420.7160949707031 = 0.4824344217777252 + 50.0 * 8.40467357635498
Epoch 960, val loss: 0.5312580466270447
Epoch 970, training loss: 420.64794921875 = 0.48076561093330383 + 50.0 * 8.403343200683594
Epoch 970, val loss: 0.5301344990730286
Epoch 980, training loss: 420.60833740234375 = 0.4791787266731262 + 50.0 * 8.402583122253418
Epoch 980, val loss: 0.5292025804519653
Epoch 990, training loss: 420.567626953125 = 0.47764965891838074 + 50.0 * 8.401799201965332
Epoch 990, val loss: 0.5282832384109497
Epoch 1000, training loss: 420.5500183105469 = 0.4761606454849243 + 50.0 * 8.401476860046387
Epoch 1000, val loss: 0.5273424983024597
Epoch 1010, training loss: 420.7305603027344 = 0.47470176219940186 + 50.0 * 8.40511703491211
Epoch 1010, val loss: 0.5263847708702087
Epoch 1020, training loss: 420.52655029296875 = 0.4732564687728882 + 50.0 * 8.401065826416016
Epoch 1020, val loss: 0.5257160663604736
Epoch 1030, training loss: 420.4676208496094 = 0.4718813896179199 + 50.0 * 8.399914741516113
Epoch 1030, val loss: 0.5247546434402466
Epoch 1040, training loss: 420.4627380371094 = 0.4705740213394165 + 50.0 * 8.399843215942383
Epoch 1040, val loss: 0.5240995287895203
Epoch 1050, training loss: 420.3961486816406 = 0.469266414642334 + 50.0 * 8.398537635803223
Epoch 1050, val loss: 0.5232203006744385
Epoch 1060, training loss: 420.36590576171875 = 0.4680168330669403 + 50.0 * 8.397957801818848
Epoch 1060, val loss: 0.5225747227668762
Epoch 1070, training loss: 420.3345031738281 = 0.4668053686618805 + 50.0 * 8.397354125976562
Epoch 1070, val loss: 0.521776556968689
Epoch 1080, training loss: 420.3426818847656 = 0.4656344950199127 + 50.0 * 8.397541046142578
Epoch 1080, val loss: 0.5211321115493774
Epoch 1090, training loss: 420.3650207519531 = 0.46443429589271545 + 50.0 * 8.398011207580566
Epoch 1090, val loss: 0.5202906727790833
Epoch 1100, training loss: 420.3086242675781 = 0.46327805519104004 + 50.0 * 8.396906852722168
Epoch 1100, val loss: 0.5198282599449158
Epoch 1110, training loss: 420.202392578125 = 0.46217650175094604 + 50.0 * 8.394804000854492
Epoch 1110, val loss: 0.5190237164497375
Epoch 1120, training loss: 420.1737060546875 = 0.46111905574798584 + 50.0 * 8.394251823425293
Epoch 1120, val loss: 0.5184637904167175
Epoch 1130, training loss: 420.1374816894531 = 0.46008458733558655 + 50.0 * 8.393548011779785
Epoch 1130, val loss: 0.5178155303001404
Epoch 1140, training loss: 420.36279296875 = 0.45904871821403503 + 50.0 * 8.398075103759766
Epoch 1140, val loss: 0.5171245336532593
Epoch 1150, training loss: 420.1415710449219 = 0.458014577627182 + 50.0 * 8.393671035766602
Epoch 1150, val loss: 0.516701340675354
Epoch 1160, training loss: 420.0380554199219 = 0.45701655745506287 + 50.0 * 8.391620635986328
Epoch 1160, val loss: 0.5160191655158997
Epoch 1170, training loss: 420.27789306640625 = 0.45603856444358826 + 50.0 * 8.396437644958496
Epoch 1170, val loss: 0.5152979493141174
Epoch 1180, training loss: 420.0869140625 = 0.45504721999168396 + 50.0 * 8.392637252807617
Epoch 1180, val loss: 0.5149809718132019
Epoch 1190, training loss: 419.9828796386719 = 0.4540979564189911 + 50.0 * 8.390575408935547
Epoch 1190, val loss: 0.514270007610321
Epoch 1200, training loss: 419.908447265625 = 0.45320266485214233 + 50.0 * 8.389104843139648
Epoch 1200, val loss: 0.5137913823127747
Epoch 1210, training loss: 419.8639221191406 = 0.4523126482963562 + 50.0 * 8.388232231140137
Epoch 1210, val loss: 0.5132925510406494
Epoch 1220, training loss: 419.8302001953125 = 0.45144838094711304 + 50.0 * 8.387575149536133
Epoch 1220, val loss: 0.5128067135810852
Epoch 1230, training loss: 420.072021484375 = 0.45057281851768494 + 50.0 * 8.39242935180664
Epoch 1230, val loss: 0.5123620629310608
Epoch 1240, training loss: 419.8106689453125 = 0.44966015219688416 + 50.0 * 8.38722038269043
Epoch 1240, val loss: 0.5117279291152954
Epoch 1250, training loss: 419.7542419433594 = 0.4488140344619751 + 50.0 * 8.3861083984375
Epoch 1250, val loss: 0.511212170124054
Epoch 1260, training loss: 419.7100830078125 = 0.4479915201663971 + 50.0 * 8.385241508483887
Epoch 1260, val loss: 0.5108212828636169
Epoch 1270, training loss: 419.72918701171875 = 0.4471888244152069 + 50.0 * 8.385640144348145
Epoch 1270, val loss: 0.5103721022605896
Epoch 1280, training loss: 419.73712158203125 = 0.44635510444641113 + 50.0 * 8.385815620422363
Epoch 1280, val loss: 0.5098472833633423
Epoch 1290, training loss: 419.6562805175781 = 0.4455263018608093 + 50.0 * 8.384215354919434
Epoch 1290, val loss: 0.5093517899513245
Epoch 1300, training loss: 419.55340576171875 = 0.44474029541015625 + 50.0 * 8.382173538208008
Epoch 1300, val loss: 0.5088573694229126
Epoch 1310, training loss: 419.56219482421875 = 0.4439568817615509 + 50.0 * 8.382364273071289
Epoch 1310, val loss: 0.5085004568099976
Epoch 1320, training loss: 419.58038330078125 = 0.4431699514389038 + 50.0 * 8.382743835449219
Epoch 1320, val loss: 0.507969319820404
Epoch 1330, training loss: 419.6087341308594 = 0.44236066937446594 + 50.0 * 8.38332748413086
Epoch 1330, val loss: 0.5074307918548584
Epoch 1340, training loss: 419.52099609375 = 0.44157400727272034 + 50.0 * 8.38158893585205
Epoch 1340, val loss: 0.5070454478263855
Epoch 1350, training loss: 419.44293212890625 = 0.4407976269721985 + 50.0 * 8.380043029785156
Epoch 1350, val loss: 0.5065702795982361
Epoch 1360, training loss: 419.3712158203125 = 0.4400588870048523 + 50.0 * 8.378623008728027
Epoch 1360, val loss: 0.5061779618263245
Epoch 1370, training loss: 419.3668212890625 = 0.43932756781578064 + 50.0 * 8.378549575805664
Epoch 1370, val loss: 0.5057036876678467
Epoch 1380, training loss: 419.6546325683594 = 0.43856531381607056 + 50.0 * 8.384321212768555
Epoch 1380, val loss: 0.5052298307418823
Epoch 1390, training loss: 419.3753967285156 = 0.43780890107154846 + 50.0 * 8.378751754760742
Epoch 1390, val loss: 0.5050084590911865
Epoch 1400, training loss: 419.26953125 = 0.43707510828971863 + 50.0 * 8.376648902893066
Epoch 1400, val loss: 0.5044201016426086
Epoch 1410, training loss: 419.2233581542969 = 0.436375230550766 + 50.0 * 8.375740051269531
Epoch 1410, val loss: 0.5041720867156982
Epoch 1420, training loss: 419.1915283203125 = 0.4356803596019745 + 50.0 * 8.375117301940918
Epoch 1420, val loss: 0.5037062764167786
Epoch 1430, training loss: 419.32373046875 = 0.4349806308746338 + 50.0 * 8.377775192260742
Epoch 1430, val loss: 0.5031918883323669
Epoch 1440, training loss: 419.223388671875 = 0.43424877524375916 + 50.0 * 8.37578296661377
Epoch 1440, val loss: 0.5031459331512451
Epoch 1450, training loss: 419.15277099609375 = 0.43354368209838867 + 50.0 * 8.374384880065918
Epoch 1450, val loss: 0.5025644302368164
Epoch 1460, training loss: 419.1324157714844 = 0.43286243081092834 + 50.0 * 8.373991012573242
Epoch 1460, val loss: 0.5022057294845581
Epoch 1470, training loss: 419.2132873535156 = 0.43217259645462036 + 50.0 * 8.375622749328613
Epoch 1470, val loss: 0.5018924474716187
Epoch 1480, training loss: 419.1270751953125 = 0.4314788281917572 + 50.0 * 8.37391185760498
Epoch 1480, val loss: 0.5015320777893066
Epoch 1490, training loss: 419.0296936035156 = 0.4308062195777893 + 50.0 * 8.371977806091309
Epoch 1490, val loss: 0.5012350678443909
Epoch 1500, training loss: 418.9996643066406 = 0.4301506280899048 + 50.0 * 8.371390342712402
Epoch 1500, val loss: 0.5009037256240845
Epoch 1510, training loss: 418.9738464355469 = 0.42950814962387085 + 50.0 * 8.37088680267334
Epoch 1510, val loss: 0.5005959272384644
Epoch 1520, training loss: 419.1206970214844 = 0.428866982460022 + 50.0 * 8.373836517333984
Epoch 1520, val loss: 0.5003350377082825
Epoch 1530, training loss: 418.92181396484375 = 0.42816832661628723 + 50.0 * 8.369873046875
Epoch 1530, val loss: 0.49991005659103394
Epoch 1540, training loss: 418.9065856933594 = 0.427506685256958 + 50.0 * 8.36958122253418
Epoch 1540, val loss: 0.49955126643180847
Epoch 1550, training loss: 418.88372802734375 = 0.42687779664993286 + 50.0 * 8.369136810302734
Epoch 1550, val loss: 0.4992767572402954
Epoch 1560, training loss: 418.8561096191406 = 0.4262562692165375 + 50.0 * 8.368597030639648
Epoch 1560, val loss: 0.49896693229675293
Epoch 1570, training loss: 418.83612060546875 = 0.42563891410827637 + 50.0 * 8.368209838867188
Epoch 1570, val loss: 0.4986889064311981
Epoch 1580, training loss: 418.8934020996094 = 0.42501717805862427 + 50.0 * 8.369367599487305
Epoch 1580, val loss: 0.49837082624435425
Epoch 1590, training loss: 418.9549255371094 = 0.4243646562099457 + 50.0 * 8.370611190795898
Epoch 1590, val loss: 0.49797460436820984
Epoch 1600, training loss: 418.8797912597656 = 0.42369794845581055 + 50.0 * 8.369121551513672
Epoch 1600, val loss: 0.49769753217697144
Epoch 1610, training loss: 418.795654296875 = 0.4230608642101288 + 50.0 * 8.367451667785645
Epoch 1610, val loss: 0.4974247217178345
Epoch 1620, training loss: 418.7427062988281 = 0.4224391579627991 + 50.0 * 8.366405487060547
Epoch 1620, val loss: 0.49707263708114624
Epoch 1630, training loss: 418.7057800292969 = 0.42184045910835266 + 50.0 * 8.365678787231445
Epoch 1630, val loss: 0.49689415097236633
Epoch 1640, training loss: 418.6942138671875 = 0.42124176025390625 + 50.0 * 8.365459442138672
Epoch 1640, val loss: 0.49660545587539673
Epoch 1650, training loss: 418.83416748046875 = 0.4206364154815674 + 50.0 * 8.368270874023438
Epoch 1650, val loss: 0.49643370509147644
Epoch 1660, training loss: 419.0160827636719 = 0.41995298862457275 + 50.0 * 8.371922492980957
Epoch 1660, val loss: 0.4957674443721771
Epoch 1670, training loss: 418.6931457519531 = 0.41930481791496277 + 50.0 * 8.365476608276367
Epoch 1670, val loss: 0.495718389749527
Epoch 1680, training loss: 418.6424865722656 = 0.41868314146995544 + 50.0 * 8.364476203918457
Epoch 1680, val loss: 0.4952693581581116
Epoch 1690, training loss: 418.5966491699219 = 0.41808435320854187 + 50.0 * 8.363571166992188
Epoch 1690, val loss: 0.49503058195114136
Epoch 1700, training loss: 418.5746765136719 = 0.4174911677837372 + 50.0 * 8.363143920898438
Epoch 1700, val loss: 0.4947563409805298
Epoch 1710, training loss: 418.5733337402344 = 0.41689789295196533 + 50.0 * 8.363128662109375
Epoch 1710, val loss: 0.49456390738487244
Epoch 1720, training loss: 418.8807067871094 = 0.4162905812263489 + 50.0 * 8.369288444519043
Epoch 1720, val loss: 0.4943547248840332
Epoch 1730, training loss: 418.6270751953125 = 0.4156215488910675 + 50.0 * 8.364229202270508
Epoch 1730, val loss: 0.4937952756881714
Epoch 1740, training loss: 418.5548095703125 = 0.4150065779685974 + 50.0 * 8.36279582977295
Epoch 1740, val loss: 0.49360156059265137
Epoch 1750, training loss: 418.50689697265625 = 0.4144066870212555 + 50.0 * 8.361849784851074
Epoch 1750, val loss: 0.4932611286640167
Epoch 1760, training loss: 418.4769287109375 = 0.41381779313087463 + 50.0 * 8.361262321472168
Epoch 1760, val loss: 0.4930354356765747
Epoch 1770, training loss: 418.4817199707031 = 0.41322287917137146 + 50.0 * 8.361370086669922
Epoch 1770, val loss: 0.49278244376182556
Epoch 1780, training loss: 418.6893310546875 = 0.4126092791557312 + 50.0 * 8.365534782409668
Epoch 1780, val loss: 0.492522656917572
Epoch 1790, training loss: 418.44927978515625 = 0.4119526445865631 + 50.0 * 8.360746383666992
Epoch 1790, val loss: 0.49214842915534973
Epoch 1800, training loss: 418.4426574707031 = 0.4113353192806244 + 50.0 * 8.360626220703125
Epoch 1800, val loss: 0.4918097257614136
Epoch 1810, training loss: 418.3984680175781 = 0.41073450446128845 + 50.0 * 8.35975456237793
Epoch 1810, val loss: 0.4916141629219055
Epoch 1820, training loss: 418.3945007324219 = 0.41014206409454346 + 50.0 * 8.359686851501465
Epoch 1820, val loss: 0.491319477558136
Epoch 1830, training loss: 418.4832458496094 = 0.4095377027988434 + 50.0 * 8.36147403717041
Epoch 1830, val loss: 0.4911291003227234
Epoch 1840, training loss: 418.4001770019531 = 0.4088972210884094 + 50.0 * 8.359825134277344
Epoch 1840, val loss: 0.49078598618507385
Epoch 1850, training loss: 418.38836669921875 = 0.4082690477371216 + 50.0 * 8.359601974487305
Epoch 1850, val loss: 0.4903842806816101
Epoch 1860, training loss: 418.31707763671875 = 0.40765994787216187 + 50.0 * 8.35818862915039
Epoch 1860, val loss: 0.49017655849456787
Epoch 1870, training loss: 418.3577880859375 = 0.407060444355011 + 50.0 * 8.359014511108398
Epoch 1870, val loss: 0.48996198177337646
Epoch 1880, training loss: 418.400634765625 = 0.4064306914806366 + 50.0 * 8.359884262084961
Epoch 1880, val loss: 0.48967254161834717
Epoch 1890, training loss: 418.3109436035156 = 0.40578457713127136 + 50.0 * 8.358102798461914
Epoch 1890, val loss: 0.48918139934539795
Epoch 1900, training loss: 418.27044677734375 = 0.4051680564880371 + 50.0 * 8.357305526733398
Epoch 1900, val loss: 0.4889567792415619
Epoch 1910, training loss: 418.3840026855469 = 0.40455982089042664 + 50.0 * 8.359588623046875
Epoch 1910, val loss: 0.48852041363716125
Epoch 1920, training loss: 418.3007507324219 = 0.403905987739563 + 50.0 * 8.35793685913086
Epoch 1920, val loss: 0.4883411228656769
Epoch 1930, training loss: 418.253173828125 = 0.4032646119594574 + 50.0 * 8.356998443603516
Epoch 1930, val loss: 0.48803290724754333
Epoch 1940, training loss: 418.2210388183594 = 0.40263620018959045 + 50.0 * 8.356368064880371
Epoch 1940, val loss: 0.48765820264816284
Epoch 1950, training loss: 418.19140625 = 0.4020254909992218 + 50.0 * 8.35578727722168
Epoch 1950, val loss: 0.4874460697174072
Epoch 1960, training loss: 418.1807861328125 = 0.40141043066978455 + 50.0 * 8.35558795928955
Epoch 1960, val loss: 0.4871714413166046
Epoch 1970, training loss: 418.17559814453125 = 0.4007924795150757 + 50.0 * 8.355496406555176
Epoch 1970, val loss: 0.4868749678134918
Epoch 1980, training loss: 418.4148254394531 = 0.4001607298851013 + 50.0 * 8.3602933883667
Epoch 1980, val loss: 0.4865804612636566
Epoch 1990, training loss: 418.2818298339844 = 0.39947593212127686 + 50.0 * 8.357646942138672
Epoch 1990, val loss: 0.4861690402030945
Epoch 2000, training loss: 418.2288818359375 = 0.39882123470306396 + 50.0 * 8.356600761413574
Epoch 2000, val loss: 0.4858311116695404
Epoch 2010, training loss: 418.1177062988281 = 0.39817631244659424 + 50.0 * 8.354391098022461
Epoch 2010, val loss: 0.4855753779411316
Epoch 2020, training loss: 418.0997009277344 = 0.39754846692085266 + 50.0 * 8.354043006896973
Epoch 2020, val loss: 0.4852850139141083
Epoch 2030, training loss: 418.1286315917969 = 0.3969171345233917 + 50.0 * 8.354634284973145
Epoch 2030, val loss: 0.4850521981716156
Epoch 2040, training loss: 418.40765380859375 = 0.39625588059425354 + 50.0 * 8.360227584838867
Epoch 2040, val loss: 0.4847942292690277
Epoch 2050, training loss: 418.1678771972656 = 0.39554938673973083 + 50.0 * 8.355446815490723
Epoch 2050, val loss: 0.4841727316379547
Epoch 2060, training loss: 418.08831787109375 = 0.3948962390422821 + 50.0 * 8.35386848449707
Epoch 2060, val loss: 0.4839475154876709
Epoch 2070, training loss: 418.0513916015625 = 0.39424559473991394 + 50.0 * 8.353142738342285
Epoch 2070, val loss: 0.4835679233074188
Epoch 2080, training loss: 418.03680419921875 = 0.3936002552509308 + 50.0 * 8.352864265441895
Epoch 2080, val loss: 0.4832955002784729
Epoch 2090, training loss: 418.19293212890625 = 0.3929424583911896 + 50.0 * 8.355999946594238
Epoch 2090, val loss: 0.48304852843284607
Epoch 2100, training loss: 418.0465087890625 = 0.392236590385437 + 50.0 * 8.3530855178833
Epoch 2100, val loss: 0.4825475811958313
Epoch 2110, training loss: 418.0184631347656 = 0.39156121015548706 + 50.0 * 8.352538108825684
Epoch 2110, val loss: 0.48227477073669434
Epoch 2120, training loss: 417.99981689453125 = 0.39088815450668335 + 50.0 * 8.352178573608398
Epoch 2120, val loss: 0.48200201988220215
Epoch 2130, training loss: 418.0262451171875 = 0.3902217149734497 + 50.0 * 8.352720260620117
Epoch 2130, val loss: 0.48175516724586487
Epoch 2140, training loss: 418.1974792480469 = 0.3895255923271179 + 50.0 * 8.356159210205078
Epoch 2140, val loss: 0.4813915193080902
Epoch 2150, training loss: 418.00677490234375 = 0.3888013958930969 + 50.0 * 8.352359771728516
Epoch 2150, val loss: 0.4807947278022766
Epoch 2160, training loss: 417.9364318847656 = 0.38810962438583374 + 50.0 * 8.350966453552246
Epoch 2160, val loss: 0.48056840896606445
Epoch 2170, training loss: 417.9346618652344 = 0.3874271512031555 + 50.0 * 8.350944519042969
Epoch 2170, val loss: 0.4802226126194
Epoch 2180, training loss: 417.9234313964844 = 0.38673678040504456 + 50.0 * 8.350733757019043
Epoch 2180, val loss: 0.4798565208911896
Epoch 2190, training loss: 417.95703125 = 0.3860393166542053 + 50.0 * 8.351419448852539
Epoch 2190, val loss: 0.4795350432395935
Epoch 2200, training loss: 417.9993896484375 = 0.38532984256744385 + 50.0 * 8.35228157043457
Epoch 2200, val loss: 0.47933200001716614
Epoch 2210, training loss: 417.9837951660156 = 0.384596586227417 + 50.0 * 8.351984024047852
Epoch 2210, val loss: 0.47887805104255676
Epoch 2220, training loss: 417.9236755371094 = 0.3838556110858917 + 50.0 * 8.350796699523926
Epoch 2220, val loss: 0.4784495532512665
Epoch 2230, training loss: 417.8647155761719 = 0.38313987851142883 + 50.0 * 8.349631309509277
Epoch 2230, val loss: 0.47798648476600647
Epoch 2240, training loss: 417.85601806640625 = 0.38243070244789124 + 50.0 * 8.349472045898438
Epoch 2240, val loss: 0.47774821519851685
Epoch 2250, training loss: 417.8760986328125 = 0.3817184269428253 + 50.0 * 8.34988784790039
Epoch 2250, val loss: 0.4773367643356323
Epoch 2260, training loss: 418.08685302734375 = 0.3809765875339508 + 50.0 * 8.354117393493652
Epoch 2260, val loss: 0.4769221842288971
Epoch 2270, training loss: 417.9082336425781 = 0.3802144229412079 + 50.0 * 8.350560188293457
Epoch 2270, val loss: 0.4764951765537262
Epoch 2280, training loss: 417.829345703125 = 0.3794620633125305 + 50.0 * 8.348998069763184
Epoch 2280, val loss: 0.47622641921043396
Epoch 2290, training loss: 417.8040771484375 = 0.3787223696708679 + 50.0 * 8.348506927490234
Epoch 2290, val loss: 0.47574886679649353
Epoch 2300, training loss: 417.81463623046875 = 0.3779815137386322 + 50.0 * 8.348732948303223
Epoch 2300, val loss: 0.47535812854766846
Epoch 2310, training loss: 418.1474914550781 = 0.3772115111351013 + 50.0 * 8.355405807495117
Epoch 2310, val loss: 0.47475239634513855
Epoch 2320, training loss: 417.85650634765625 = 0.3764123022556305 + 50.0 * 8.349601745605469
Epoch 2320, val loss: 0.47486987709999084
Epoch 2330, training loss: 417.7850036621094 = 0.3756260275840759 + 50.0 * 8.348187446594238
Epoch 2330, val loss: 0.4741500914096832
Epoch 2340, training loss: 417.7601318359375 = 0.3748670816421509 + 50.0 * 8.347704887390137
Epoch 2340, val loss: 0.47384828329086304
Epoch 2350, training loss: 417.7415771484375 = 0.374103307723999 + 50.0 * 8.347349166870117
Epoch 2350, val loss: 0.4734959602355957
Epoch 2360, training loss: 417.7470397949219 = 0.37333226203918457 + 50.0 * 8.347474098205566
Epoch 2360, val loss: 0.47316989302635193
Epoch 2370, training loss: 418.02984619140625 = 0.3725409507751465 + 50.0 * 8.35314655303955
Epoch 2370, val loss: 0.47272971272468567
Epoch 2380, training loss: 417.7856140136719 = 0.37171831727027893 + 50.0 * 8.348278045654297
Epoch 2380, val loss: 0.4724215269088745
Epoch 2390, training loss: 417.71600341796875 = 0.3709125220775604 + 50.0 * 8.346901893615723
Epoch 2390, val loss: 0.4719621241092682
Epoch 2400, training loss: 417.7781677246094 = 0.3701116740703583 + 50.0 * 8.348160743713379
Epoch 2400, val loss: 0.4715714156627655
Epoch 2410, training loss: 417.7928771972656 = 0.36929062008857727 + 50.0 * 8.348471641540527
Epoch 2410, val loss: 0.4712851643562317
Epoch 2420, training loss: 417.688720703125 = 0.36846640706062317 + 50.0 * 8.346405029296875
Epoch 2420, val loss: 0.4706951677799225
Epoch 2430, training loss: 417.69232177734375 = 0.36764928698539734 + 50.0 * 8.3464937210083
Epoch 2430, val loss: 0.47032666206359863
Epoch 2440, training loss: 417.659423828125 = 0.36683085560798645 + 50.0 * 8.34585189819336
Epoch 2440, val loss: 0.4699404835700989
Epoch 2450, training loss: 417.65863037109375 = 0.3660062253475189 + 50.0 * 8.345852851867676
Epoch 2450, val loss: 0.46955204010009766
Epoch 2460, training loss: 417.85546875 = 0.36516809463500977 + 50.0 * 8.34980583190918
Epoch 2460, val loss: 0.4690083861351013
Epoch 2470, training loss: 417.6260681152344 = 0.36428847908973694 + 50.0 * 8.345235824584961
Epoch 2470, val loss: 0.4688689410686493
Epoch 2480, training loss: 417.6474304199219 = 0.3634333312511444 + 50.0 * 8.345680236816406
Epoch 2480, val loss: 0.4683685004711151
Epoch 2490, training loss: 417.6439208984375 = 0.36258065700531006 + 50.0 * 8.345626831054688
Epoch 2490, val loss: 0.4679020643234253
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148148
0.8634354850394842
The final CL Acc:0.81583, 0.00083, The final GNN Acc:0.86392, 0.00101
Begin epxeriment: noisy_level: 0.2 cont_weight: 50 epoch: der1:0.3 der2:0.2 dfr1:0.4 dfr2:0
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.0, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106556])
remove edge: torch.Size([2, 70756])
updated graph: torch.Size([2, 88664])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 530.2086791992188 = 1.096314549446106 + 50.0 * 10.582246780395508
Epoch 0, val loss: 1.095519781112671
Epoch 10, training loss: 530.1802368164062 = 1.0925347805023193 + 50.0 * 10.581753730773926
Epoch 10, val loss: 1.091689944267273
Epoch 20, training loss: 530.0791625976562 = 1.088423490524292 + 50.0 * 10.579814910888672
Epoch 20, val loss: 1.0875086784362793
Epoch 30, training loss: 529.6691284179688 = 1.0837563276290894 + 50.0 * 10.571707725524902
Epoch 30, val loss: 1.082761287689209
Epoch 40, training loss: 528.080322265625 = 1.078403115272522 + 50.0 * 10.540038108825684
Epoch 40, val loss: 1.0773612260818481
Epoch 50, training loss: 522.6701049804688 = 1.0722657442092896 + 50.0 * 10.431957244873047
Epoch 50, val loss: 1.0712140798568726
Epoch 60, training loss: 508.30804443359375 = 1.0654022693634033 + 50.0 * 10.144852638244629
Epoch 60, val loss: 1.0644234418869019
Epoch 70, training loss: 491.796630859375 = 1.058950424194336 + 50.0 * 9.814753532409668
Epoch 70, val loss: 1.0584388971328735
Epoch 80, training loss: 484.861572265625 = 1.0549076795578003 + 50.0 * 9.676133155822754
Epoch 80, val loss: 1.054955005645752
Epoch 90, training loss: 479.7650451660156 = 1.0523487329483032 + 50.0 * 9.574254035949707
Epoch 90, val loss: 1.0527423620224
Epoch 100, training loss: 473.2328796386719 = 1.0501697063446045 + 50.0 * 9.44365406036377
Epoch 100, val loss: 1.0507514476776123
Epoch 110, training loss: 465.5282897949219 = 1.0485620498657227 + 50.0 * 9.289594650268555
Epoch 110, val loss: 1.0492823123931885
Epoch 120, training loss: 461.40875244140625 = 1.0471256971359253 + 50.0 * 9.207232475280762
Epoch 120, val loss: 1.0479185581207275
Epoch 130, training loss: 457.3270568847656 = 1.0459609031677246 + 50.0 * 9.125621795654297
Epoch 130, val loss: 1.0468993186950684
Epoch 140, training loss: 452.01416015625 = 1.0454957485198975 + 50.0 * 9.019372940063477
Epoch 140, val loss: 1.0465998649597168
Epoch 150, training loss: 447.9955139160156 = 1.0454232692718506 + 50.0 * 8.93900203704834
Epoch 150, val loss: 1.0465675592422485
Epoch 160, training loss: 445.6207580566406 = 1.0450245141983032 + 50.0 * 8.891514778137207
Epoch 160, val loss: 1.046152949333191
Epoch 170, training loss: 443.6991271972656 = 1.0440928936004639 + 50.0 * 8.853100776672363
Epoch 170, val loss: 1.0451780557632446
Epoch 180, training loss: 441.54595947265625 = 1.043089747428894 + 50.0 * 8.810057640075684
Epoch 180, val loss: 1.044313669204712
Epoch 190, training loss: 439.57635498046875 = 1.0428297519683838 + 50.0 * 8.770670890808105
Epoch 190, val loss: 1.044156551361084
Epoch 200, training loss: 437.9844970703125 = 1.0427260398864746 + 50.0 * 8.738835334777832
Epoch 200, val loss: 1.044093132019043
Epoch 210, training loss: 436.771484375 = 1.0424115657806396 + 50.0 * 8.714581489562988
Epoch 210, val loss: 1.0437489748001099
Epoch 220, training loss: 435.77899169921875 = 1.0419224500656128 + 50.0 * 8.694741249084473
Epoch 220, val loss: 1.0432788133621216
Epoch 230, training loss: 434.95465087890625 = 1.0413817167282104 + 50.0 * 8.678265571594238
Epoch 230, val loss: 1.0427751541137695
Epoch 240, training loss: 434.24920654296875 = 1.0408157110214233 + 50.0 * 8.664168357849121
Epoch 240, val loss: 1.0422335863113403
Epoch 250, training loss: 433.66058349609375 = 1.0402705669403076 + 50.0 * 8.652405738830566
Epoch 250, val loss: 1.0416945219039917
Epoch 260, training loss: 433.1382751464844 = 1.0397053956985474 + 50.0 * 8.641971588134766
Epoch 260, val loss: 1.041151523590088
Epoch 270, training loss: 432.7315368652344 = 1.0390878915786743 + 50.0 * 8.633849143981934
Epoch 270, val loss: 1.04050612449646
Epoch 280, training loss: 432.2779846191406 = 1.0383939743041992 + 50.0 * 8.624792098999023
Epoch 280, val loss: 1.039850115776062
Epoch 290, training loss: 431.8908996582031 = 1.03767728805542 + 50.0 * 8.617064476013184
Epoch 290, val loss: 1.0391182899475098
Epoch 300, training loss: 431.572998046875 = 1.0368319749832153 + 50.0 * 8.610723495483398
Epoch 300, val loss: 1.0382827520370483
Epoch 310, training loss: 431.28985595703125 = 1.0359028577804565 + 50.0 * 8.605079650878906
Epoch 310, val loss: 1.0373629331588745
Epoch 320, training loss: 431.0199279785156 = 1.034952998161316 + 50.0 * 8.599699974060059
Epoch 320, val loss: 1.0364338159561157
Epoch 330, training loss: 430.7823181152344 = 1.034014344215393 + 50.0 * 8.594965934753418
Epoch 330, val loss: 1.0355136394500732
Epoch 340, training loss: 430.4517517089844 = 1.0330522060394287 + 50.0 * 8.588374137878418
Epoch 340, val loss: 1.034624457359314
Epoch 350, training loss: 430.18597412109375 = 1.032114863395691 + 50.0 * 8.583077430725098
Epoch 350, val loss: 1.0337297916412354
Epoch 360, training loss: 429.9667663574219 = 1.0311483144760132 + 50.0 * 8.578712463378906
Epoch 360, val loss: 1.0327829122543335
Epoch 370, training loss: 429.7060852050781 = 1.030159831047058 + 50.0 * 8.573518753051758
Epoch 370, val loss: 1.0318337678909302
Epoch 380, training loss: 429.4838562011719 = 1.0291796922683716 + 50.0 * 8.569093704223633
Epoch 380, val loss: 1.030860185623169
Epoch 390, training loss: 429.2735595703125 = 1.028173804283142 + 50.0 * 8.564908027648926
Epoch 390, val loss: 1.0298877954483032
Epoch 400, training loss: 429.0823059082031 = 1.027127742767334 + 50.0 * 8.561103820800781
Epoch 400, val loss: 1.0289177894592285
Epoch 410, training loss: 428.88372802734375 = 1.0260831117630005 + 50.0 * 8.55715274810791
Epoch 410, val loss: 1.0278639793395996
Epoch 420, training loss: 428.6575622558594 = 1.0250085592269897 + 50.0 * 8.552651405334473
Epoch 420, val loss: 1.0268279314041138
Epoch 430, training loss: 428.44427490234375 = 1.0239431858062744 + 50.0 * 8.548406600952148
Epoch 430, val loss: 1.0257998704910278
Epoch 440, training loss: 428.25958251953125 = 1.0228385925292969 + 50.0 * 8.544734954833984
Epoch 440, val loss: 1.0247312784194946
Epoch 450, training loss: 428.399169921875 = 1.0216845273971558 + 50.0 * 8.5475492477417
Epoch 450, val loss: 1.0236642360687256
Epoch 460, training loss: 428.0608825683594 = 1.0204211473464966 + 50.0 * 8.540809631347656
Epoch 460, val loss: 1.0223946571350098
Epoch 470, training loss: 427.84521484375 = 1.0192291736602783 + 50.0 * 8.536520004272461
Epoch 470, val loss: 1.0212149620056152
Epoch 480, training loss: 427.6640930175781 = 1.0179964303970337 + 50.0 * 8.53292179107666
Epoch 480, val loss: 1.0200488567352295
Epoch 490, training loss: 427.5303649902344 = 1.016742467880249 + 50.0 * 8.530272483825684
Epoch 490, val loss: 1.0188335180282593
Epoch 500, training loss: 427.3874816894531 = 1.0154635906219482 + 50.0 * 8.527440071105957
Epoch 500, val loss: 1.0175992250442505
Epoch 510, training loss: 427.3849182128906 = 1.0141619443893433 + 50.0 * 8.52741527557373
Epoch 510, val loss: 1.0163253545761108
Epoch 520, training loss: 427.1639404296875 = 1.0127280950546265 + 50.0 * 8.523024559020996
Epoch 520, val loss: 1.0149562358856201
Epoch 530, training loss: 427.0111999511719 = 1.0113328695297241 + 50.0 * 8.519997596740723
Epoch 530, val loss: 1.0136123895645142
Epoch 540, training loss: 426.8694152832031 = 1.0099215507507324 + 50.0 * 8.517189979553223
Epoch 540, val loss: 1.0122675895690918
Epoch 550, training loss: 426.9724426269531 = 1.0084542036056519 + 50.0 * 8.519279479980469
Epoch 550, val loss: 1.010887861251831
Epoch 560, training loss: 426.6228332519531 = 1.0069071054458618 + 50.0 * 8.51231861114502
Epoch 560, val loss: 1.0093574523925781
Epoch 570, training loss: 426.50244140625 = 1.0053666830062866 + 50.0 * 8.509941101074219
Epoch 570, val loss: 1.0078742504119873
Epoch 580, training loss: 426.3900146484375 = 1.0038001537322998 + 50.0 * 8.507723808288574
Epoch 580, val loss: 1.0063682794570923
Epoch 590, training loss: 426.28192138671875 = 1.002179741859436 + 50.0 * 8.505595207214355
Epoch 590, val loss: 1.0048168897628784
Epoch 600, training loss: 426.17938232421875 = 1.0005182027816772 + 50.0 * 8.50357723236084
Epoch 600, val loss: 1.0032150745391846
Epoch 610, training loss: 426.1214599609375 = 0.998729944229126 + 50.0 * 8.50245475769043
Epoch 610, val loss: 1.0015157461166382
Epoch 620, training loss: 426.0017395019531 = 0.9968981146812439 + 50.0 * 8.500097274780273
Epoch 620, val loss: 0.9997701644897461
Epoch 630, training loss: 425.9049987792969 = 0.9950894713401794 + 50.0 * 8.498198509216309
Epoch 630, val loss: 0.9980587959289551
Epoch 640, training loss: 425.81549072265625 = 0.993250846862793 + 50.0 * 8.496444702148438
Epoch 640, val loss: 0.9962980151176453
Epoch 650, training loss: 425.7374572753906 = 0.99135822057724 + 50.0 * 8.494921684265137
Epoch 650, val loss: 0.9945005774497986
Epoch 660, training loss: 425.8668212890625 = 0.9893917441368103 + 50.0 * 8.497549057006836
Epoch 660, val loss: 0.9926317930221558
Epoch 670, training loss: 425.6029968261719 = 0.9873397350311279 + 50.0 * 8.492313385009766
Epoch 670, val loss: 0.9906492829322815
Epoch 680, training loss: 425.52685546875 = 0.9852983951568604 + 50.0 * 8.49083137512207
Epoch 680, val loss: 0.988730251789093
Epoch 690, training loss: 425.4396667480469 = 0.9832189679145813 + 50.0 * 8.489129066467285
Epoch 690, val loss: 0.9867722988128662
Epoch 700, training loss: 425.39532470703125 = 0.9810805320739746 + 50.0 * 8.488285064697266
Epoch 700, val loss: 0.9847628474235535
Epoch 710, training loss: 425.34368896484375 = 0.9788329005241394 + 50.0 * 8.487297058105469
Epoch 710, val loss: 0.9826073050498962
Epoch 720, training loss: 425.27899169921875 = 0.9765592217445374 + 50.0 * 8.486048698425293
Epoch 720, val loss: 0.9804766178131104
Epoch 730, training loss: 425.1911315917969 = 0.9742879271507263 + 50.0 * 8.484336853027344
Epoch 730, val loss: 0.9783381819725037
Epoch 740, training loss: 425.1308898925781 = 0.9719939827919006 + 50.0 * 8.48317813873291
Epoch 740, val loss: 0.9761623740196228
Epoch 750, training loss: 425.1048278808594 = 0.9695621728897095 + 50.0 * 8.482705116271973
Epoch 750, val loss: 0.973858118057251
Epoch 760, training loss: 425.0238342285156 = 0.9670999646186829 + 50.0 * 8.481134414672852
Epoch 760, val loss: 0.9715389013290405
Epoch 770, training loss: 424.94281005859375 = 0.9646425843238831 + 50.0 * 8.47956371307373
Epoch 770, val loss: 0.9692244529724121
Epoch 780, training loss: 424.9370422363281 = 0.9621534943580627 + 50.0 * 8.479497909545898
Epoch 780, val loss: 0.9668501615524292
Epoch 790, training loss: 424.8316650390625 = 0.95945143699646 + 50.0 * 8.477444648742676
Epoch 790, val loss: 0.9643260836601257
Epoch 800, training loss: 424.80206298828125 = 0.9568309187889099 + 50.0 * 8.47690486907959
Epoch 800, val loss: 0.961845874786377
Epoch 810, training loss: 424.7289733886719 = 0.9541904926300049 + 50.0 * 8.475495338439941
Epoch 810, val loss: 0.9593811631202698
Epoch 820, training loss: 424.66583251953125 = 0.9515255689620972 + 50.0 * 8.474286079406738
Epoch 820, val loss: 0.956853985786438
Epoch 830, training loss: 424.60992431640625 = 0.948784351348877 + 50.0 * 8.473222732543945
Epoch 830, val loss: 0.9542840123176575
Epoch 840, training loss: 424.7651672363281 = 0.9459443092346191 + 50.0 * 8.476384162902832
Epoch 840, val loss: 0.9515889883041382
Epoch 850, training loss: 424.5409851074219 = 0.9429914951324463 + 50.0 * 8.471960067749023
Epoch 850, val loss: 0.9488383531570435
Epoch 860, training loss: 424.4665832519531 = 0.9401084780693054 + 50.0 * 8.470529556274414
Epoch 860, val loss: 0.9461258053779602
Epoch 870, training loss: 424.4095458984375 = 0.9372356534004211 + 50.0 * 8.469446182250977
Epoch 870, val loss: 0.943383514881134
Epoch 880, training loss: 424.4327392578125 = 0.9343052506446838 + 50.0 * 8.469968795776367
Epoch 880, val loss: 0.9406629800796509
Epoch 890, training loss: 424.34661865234375 = 0.9311751127243042 + 50.0 * 8.468308448791504
Epoch 890, val loss: 0.9377235174179077
Epoch 900, training loss: 424.2564392089844 = 0.9282078742980957 + 50.0 * 8.466564178466797
Epoch 900, val loss: 0.9349250197410583
Epoch 910, training loss: 424.182373046875 = 0.9252018928527832 + 50.0 * 8.465143203735352
Epoch 910, val loss: 0.9320975542068481
Epoch 920, training loss: 424.1534423828125 = 0.9221762418746948 + 50.0 * 8.464625358581543
Epoch 920, val loss: 0.9292463660240173
Epoch 930, training loss: 424.10626220703125 = 0.9190182089805603 + 50.0 * 8.4637451171875
Epoch 930, val loss: 0.9262952208518982
Epoch 940, training loss: 424.0424499511719 = 0.915870189666748 + 50.0 * 8.462532043457031
Epoch 940, val loss: 0.9233600497245789
Epoch 950, training loss: 423.9670715332031 = 0.9127135872840881 + 50.0 * 8.461087226867676
Epoch 950, val loss: 0.9204068779945374
Epoch 960, training loss: 423.9208984375 = 0.9095984697341919 + 50.0 * 8.460226058959961
Epoch 960, val loss: 0.9174475073814392
Epoch 970, training loss: 424.02947998046875 = 0.9064008593559265 + 50.0 * 8.462461471557617
Epoch 970, val loss: 0.9143949151039124
Epoch 980, training loss: 423.90264892578125 = 0.9029883146286011 + 50.0 * 8.459993362426758
Epoch 980, val loss: 0.9113060832023621
Epoch 990, training loss: 423.7973327636719 = 0.8997384309768677 + 50.0 * 8.457951545715332
Epoch 990, val loss: 0.9082674980163574
Epoch 1000, training loss: 423.7162170410156 = 0.8965184092521667 + 50.0 * 8.45639419555664
Epoch 1000, val loss: 0.9052449464797974
Epoch 1010, training loss: 423.6822509765625 = 0.8933091163635254 + 50.0 * 8.455779075622559
Epoch 1010, val loss: 0.9022169709205627
Epoch 1020, training loss: 423.7177429199219 = 0.8900122046470642 + 50.0 * 8.456554412841797
Epoch 1020, val loss: 0.8991212844848633
Epoch 1030, training loss: 423.61126708984375 = 0.8865834474563599 + 50.0 * 8.454493522644043
Epoch 1030, val loss: 0.8959437608718872
Epoch 1040, training loss: 423.54833984375 = 0.8832734823226929 + 50.0 * 8.453301429748535
Epoch 1040, val loss: 0.8928523063659668
Epoch 1050, training loss: 423.4989013671875 = 0.8799675703048706 + 50.0 * 8.452378273010254
Epoch 1050, val loss: 0.889758288860321
Epoch 1060, training loss: 423.4556884765625 = 0.8766586780548096 + 50.0 * 8.451581001281738
Epoch 1060, val loss: 0.8866816759109497
Epoch 1070, training loss: 423.563720703125 = 0.8732887506484985 + 50.0 * 8.453808784484863
Epoch 1070, val loss: 0.8835586905479431
Epoch 1080, training loss: 423.4350280761719 = 0.8698009252548218 + 50.0 * 8.45130443572998
Epoch 1080, val loss: 0.8802472949028015
Epoch 1090, training loss: 423.3505859375 = 0.8664084076881409 + 50.0 * 8.449684143066406
Epoch 1090, val loss: 0.8770881295204163
Epoch 1100, training loss: 423.3363952636719 = 0.8630134463310242 + 50.0 * 8.449467658996582
Epoch 1100, val loss: 0.8739648461341858
Epoch 1110, training loss: 423.2934875488281 = 0.8595836758613586 + 50.0 * 8.448678016662598
Epoch 1110, val loss: 0.8707634210586548
Epoch 1120, training loss: 423.2330322265625 = 0.8561679720878601 + 50.0 * 8.447537422180176
Epoch 1120, val loss: 0.8676024079322815
Epoch 1130, training loss: 423.28173828125 = 0.8526906967163086 + 50.0 * 8.448580741882324
Epoch 1130, val loss: 0.864376425743103
Epoch 1140, training loss: 423.1863708496094 = 0.8492249846458435 + 50.0 * 8.44674301147461
Epoch 1140, val loss: 0.8611816763877869
Epoch 1150, training loss: 423.1224670410156 = 0.8457981944084167 + 50.0 * 8.445533752441406
Epoch 1150, val loss: 0.8579981923103333
Epoch 1160, training loss: 423.1334228515625 = 0.84238201379776 + 50.0 * 8.445820808410645
Epoch 1160, val loss: 0.8548750877380371
Epoch 1170, training loss: 423.0754699707031 = 0.838778018951416 + 50.0 * 8.444733619689941
Epoch 1170, val loss: 0.8514569401741028
Epoch 1180, training loss: 423.0503234863281 = 0.8352932333946228 + 50.0 * 8.444300651550293
Epoch 1180, val loss: 0.8482999801635742
Epoch 1190, training loss: 422.9913330078125 = 0.8318628072738647 + 50.0 * 8.44318962097168
Epoch 1190, val loss: 0.8450835943222046
Epoch 1200, training loss: 422.9591064453125 = 0.828467071056366 + 50.0 * 8.442612648010254
Epoch 1200, val loss: 0.8419654369354248
Epoch 1210, training loss: 423.0362548828125 = 0.8249829411506653 + 50.0 * 8.444225311279297
Epoch 1210, val loss: 0.8387819528579712
Epoch 1220, training loss: 422.9397277832031 = 0.8214423656463623 + 50.0 * 8.442365646362305
Epoch 1220, val loss: 0.8354220986366272
Epoch 1230, training loss: 422.90643310546875 = 0.8179863095283508 + 50.0 * 8.441768646240234
Epoch 1230, val loss: 0.8322832584381104
Epoch 1240, training loss: 422.8575439453125 = 0.8145316243171692 + 50.0 * 8.4408597946167
Epoch 1240, val loss: 0.8290743231773376
Epoch 1250, training loss: 422.8280029296875 = 0.8111057877540588 + 50.0 * 8.440338134765625
Epoch 1250, val loss: 0.8259133100509644
Epoch 1260, training loss: 422.8537902832031 = 0.8076263070106506 + 50.0 * 8.440923690795898
Epoch 1260, val loss: 0.8227138519287109
Epoch 1270, training loss: 422.7675476074219 = 0.8041139245033264 + 50.0 * 8.439269065856934
Epoch 1270, val loss: 0.8194699883460999
Epoch 1280, training loss: 422.76531982421875 = 0.8006603717803955 + 50.0 * 8.439292907714844
Epoch 1280, val loss: 0.816300630569458
Epoch 1290, training loss: 422.8072204589844 = 0.7971863150596619 + 50.0 * 8.440200805664062
Epoch 1290, val loss: 0.8130720853805542
Epoch 1300, training loss: 422.691162109375 = 0.7936093211174011 + 50.0 * 8.43795108795166
Epoch 1300, val loss: 0.809817373752594
Epoch 1310, training loss: 422.67462158203125 = 0.7901936769485474 + 50.0 * 8.437688827514648
Epoch 1310, val loss: 0.8066803812980652
Epoch 1320, training loss: 422.6701965332031 = 0.7867501378059387 + 50.0 * 8.437668800354004
Epoch 1320, val loss: 0.8035025000572205
Epoch 1330, training loss: 422.6773376464844 = 0.7832656502723694 + 50.0 * 8.437881469726562
Epoch 1330, val loss: 0.8003054857254028
Epoch 1340, training loss: 422.623779296875 = 0.7798235416412354 + 50.0 * 8.43687915802002
Epoch 1340, val loss: 0.7972192168235779
Epoch 1350, training loss: 422.5952453613281 = 0.7763817310333252 + 50.0 * 8.43637752532959
Epoch 1350, val loss: 0.794021487236023
Epoch 1360, training loss: 422.5948791503906 = 0.7729352116584778 + 50.0 * 8.43643856048584
Epoch 1360, val loss: 0.7909136414527893
Epoch 1370, training loss: 422.5658264160156 = 0.7695136070251465 + 50.0 * 8.43592643737793
Epoch 1370, val loss: 0.7876954674720764
Epoch 1380, training loss: 422.5646057128906 = 0.7659997940063477 + 50.0 * 8.435972213745117
Epoch 1380, val loss: 0.784504234790802
Epoch 1390, training loss: 422.5057678222656 = 0.7625747919082642 + 50.0 * 8.434864044189453
Epoch 1390, val loss: 0.7813283801078796
Epoch 1400, training loss: 422.4713439941406 = 0.7591962814331055 + 50.0 * 8.434243202209473
Epoch 1400, val loss: 0.7783126831054688
Epoch 1410, training loss: 422.4566345214844 = 0.7558691501617432 + 50.0 * 8.434015274047852
Epoch 1410, val loss: 0.7752176523208618
Epoch 1420, training loss: 422.5556640625 = 0.7524827122688293 + 50.0 * 8.436063766479492
Epoch 1420, val loss: 0.7721148133277893
Epoch 1430, training loss: 422.4241943359375 = 0.749015748500824 + 50.0 * 8.433503150939941
Epoch 1430, val loss: 0.7690098881721497
Epoch 1440, training loss: 422.37823486328125 = 0.7456963658332825 + 50.0 * 8.432650566101074
Epoch 1440, val loss: 0.7659723162651062
Epoch 1450, training loss: 422.3534240722656 = 0.7423909902572632 + 50.0 * 8.432220458984375
Epoch 1450, val loss: 0.762956440448761
Epoch 1460, training loss: 422.3513488769531 = 0.7390779256820679 + 50.0 * 8.432245254516602
Epoch 1460, val loss: 0.7599711418151855
Epoch 1470, training loss: 422.49603271484375 = 0.7356270551681519 + 50.0 * 8.435208320617676
Epoch 1470, val loss: 0.7568080425262451
Epoch 1480, training loss: 422.2957458496094 = 0.7322022318840027 + 50.0 * 8.431270599365234
Epoch 1480, val loss: 0.7536903023719788
Epoch 1490, training loss: 422.2964782714844 = 0.72893226146698 + 50.0 * 8.431350708007812
Epoch 1490, val loss: 0.7506673336029053
Epoch 1500, training loss: 422.25244140625 = 0.7257073521614075 + 50.0 * 8.430534362792969
Epoch 1500, val loss: 0.7477816939353943
Epoch 1510, training loss: 422.2314453125 = 0.7224670052528381 + 50.0 * 8.430179595947266
Epoch 1510, val loss: 0.7448320388793945
Epoch 1520, training loss: 422.2189025878906 = 0.7192588448524475 + 50.0 * 8.42999267578125
Epoch 1520, val loss: 0.7418851852416992
Epoch 1530, training loss: 422.3681945800781 = 0.7159780859947205 + 50.0 * 8.43304443359375
Epoch 1530, val loss: 0.7388210892677307
Epoch 1540, training loss: 422.2801513671875 = 0.712601363658905 + 50.0 * 8.431350708007812
Epoch 1540, val loss: 0.7358614802360535
Epoch 1550, training loss: 422.173583984375 = 0.709327220916748 + 50.0 * 8.429285049438477
Epoch 1550, val loss: 0.7329035997390747
Epoch 1560, training loss: 422.1586608886719 = 0.7061682939529419 + 50.0 * 8.429049491882324
Epoch 1560, val loss: 0.7300045490264893
Epoch 1570, training loss: 422.11944580078125 = 0.7030598521232605 + 50.0 * 8.428327560424805
Epoch 1570, val loss: 0.727189302444458
Epoch 1580, training loss: 422.1518249511719 = 0.6999348998069763 + 50.0 * 8.429038047790527
Epoch 1580, val loss: 0.7243464589118958
Epoch 1590, training loss: 422.13140869140625 = 0.6967020630836487 + 50.0 * 8.428693771362305
Epoch 1590, val loss: 0.7213897705078125
Epoch 1600, training loss: 422.0797119140625 = 0.6935955882072449 + 50.0 * 8.427721977233887
Epoch 1600, val loss: 0.7185699939727783
Epoch 1610, training loss: 422.05950927734375 = 0.6905156373977661 + 50.0 * 8.427379608154297
Epoch 1610, val loss: 0.7158123850822449
Epoch 1620, training loss: 422.0511169433594 = 0.6874812245368958 + 50.0 * 8.42727279663086
Epoch 1620, val loss: 0.7130137085914612
Epoch 1630, training loss: 422.1538391113281 = 0.6843532919883728 + 50.0 * 8.429389953613281
Epoch 1630, val loss: 0.7101312279701233
Epoch 1640, training loss: 422.02410888671875 = 0.681166410446167 + 50.0 * 8.426858901977539
Epoch 1640, val loss: 0.7073259949684143
Epoch 1650, training loss: 421.9836730957031 = 0.6781553030014038 + 50.0 * 8.42611026763916
Epoch 1650, val loss: 0.7046045660972595
Epoch 1660, training loss: 421.958251953125 = 0.6752102375030518 + 50.0 * 8.425661087036133
Epoch 1660, val loss: 0.7019246816635132
Epoch 1670, training loss: 421.94232177734375 = 0.6722765564918518 + 50.0 * 8.425400733947754
Epoch 1670, val loss: 0.6992841362953186
Epoch 1680, training loss: 421.93621826171875 = 0.669331967830658 + 50.0 * 8.425337791442871
Epoch 1680, val loss: 0.6965874433517456
Epoch 1690, training loss: 422.01898193359375 = 0.666358470916748 + 50.0 * 8.42705249786377
Epoch 1690, val loss: 0.6938292384147644
Epoch 1700, training loss: 422.0113220214844 = 0.6633313298225403 + 50.0 * 8.426959991455078
Epoch 1700, val loss: 0.6912286877632141
Epoch 1710, training loss: 421.9290466308594 = 0.6603511571884155 + 50.0 * 8.425374031066895
Epoch 1710, val loss: 0.6884983777999878
Epoch 1720, training loss: 421.86419677734375 = 0.6574531197547913 + 50.0 * 8.424135208129883
Epoch 1720, val loss: 0.685896635055542
Epoch 1730, training loss: 421.8514709472656 = 0.654664933681488 + 50.0 * 8.423935890197754
Epoch 1730, val loss: 0.6833696961402893
Epoch 1740, training loss: 421.8296813964844 = 0.6518569588661194 + 50.0 * 8.423556327819824
Epoch 1740, val loss: 0.6808099746704102
Epoch 1750, training loss: 421.8116455078125 = 0.6490764617919922 + 50.0 * 8.423251152038574
Epoch 1750, val loss: 0.6783287525177002
Epoch 1760, training loss: 421.8293762207031 = 0.6462783217430115 + 50.0 * 8.423662185668945
Epoch 1760, val loss: 0.6757506728172302
Epoch 1770, training loss: 421.848876953125 = 0.643387496471405 + 50.0 * 8.42410945892334
Epoch 1770, val loss: 0.6732008457183838
Epoch 1780, training loss: 421.8529968261719 = 0.6404661536216736 + 50.0 * 8.424250602722168
Epoch 1780, val loss: 0.6705354452133179
Epoch 1790, training loss: 421.80291748046875 = 0.6377249360084534 + 50.0 * 8.423303604125977
Epoch 1790, val loss: 0.6681123971939087
Epoch 1800, training loss: 421.745849609375 = 0.6350288987159729 + 50.0 * 8.422216415405273
Epoch 1800, val loss: 0.6656879782676697
Epoch 1810, training loss: 421.73211669921875 = 0.6324343085289001 + 50.0 * 8.421993255615234
Epoch 1810, val loss: 0.6633147597312927
Epoch 1820, training loss: 421.7406921386719 = 0.6298044919967651 + 50.0 * 8.422218322753906
Epoch 1820, val loss: 0.6609725952148438
Epoch 1830, training loss: 421.76580810546875 = 0.6271093487739563 + 50.0 * 8.422774314880371
Epoch 1830, val loss: 0.6585248708724976
Epoch 1840, training loss: 421.7087097167969 = 0.6244942545890808 + 50.0 * 8.421684265136719
Epoch 1840, val loss: 0.6561915278434753
Epoch 1850, training loss: 421.8008117675781 = 0.6218483448028564 + 50.0 * 8.423579216003418
Epoch 1850, val loss: 0.6538334488868713
Epoch 1860, training loss: 421.6834716796875 = 0.619287371635437 + 50.0 * 8.421283721923828
Epoch 1860, val loss: 0.6515088677406311
Epoch 1870, training loss: 421.6517639160156 = 0.6167703866958618 + 50.0 * 8.420700073242188
Epoch 1870, val loss: 0.6492604613304138
Epoch 1880, training loss: 421.6319274902344 = 0.6143184304237366 + 50.0 * 8.4203519821167
Epoch 1880, val loss: 0.6470744013786316
Epoch 1890, training loss: 421.624267578125 = 0.6118736863136292 + 50.0 * 8.420248031616211
Epoch 1890, val loss: 0.6449034810066223
Epoch 1900, training loss: 421.76910400390625 = 0.6093791723251343 + 50.0 * 8.423194885253906
Epoch 1900, val loss: 0.6426783204078674
Epoch 1910, training loss: 421.60772705078125 = 0.6068114638328552 + 50.0 * 8.420018196105957
Epoch 1910, val loss: 0.6402930617332458
Epoch 1920, training loss: 421.5830383300781 = 0.6043937802314758 + 50.0 * 8.419572830200195
Epoch 1920, val loss: 0.6381823420524597
Epoch 1930, training loss: 421.5647277832031 = 0.6020623445510864 + 50.0 * 8.4192533493042
Epoch 1930, val loss: 0.6361067295074463
Epoch 1940, training loss: 421.5525207519531 = 0.5997628569602966 + 50.0 * 8.419054985046387
Epoch 1940, val loss: 0.6340644359588623
Epoch 1950, training loss: 421.5797424316406 = 0.5974578261375427 + 50.0 * 8.419646263122559
Epoch 1950, val loss: 0.6320508718490601
Epoch 1960, training loss: 421.5822448730469 = 0.5950194597244263 + 50.0 * 8.419744491577148
Epoch 1960, val loss: 0.6298511624336243
Epoch 1970, training loss: 421.53759765625 = 0.5927208065986633 + 50.0 * 8.41889762878418
Epoch 1970, val loss: 0.627860963344574
Epoch 1980, training loss: 421.54193115234375 = 0.5904582738876343 + 50.0 * 8.419029235839844
Epoch 1980, val loss: 0.6258159279823303
Epoch 1990, training loss: 421.4974670410156 = 0.5882806181907654 + 50.0 * 8.418183326721191
Epoch 1990, val loss: 0.6238607168197632
Epoch 2000, training loss: 421.4835510253906 = 0.5861252546310425 + 50.0 * 8.417948722839355
Epoch 2000, val loss: 0.621921181678772
Epoch 2010, training loss: 421.491943359375 = 0.5839746594429016 + 50.0 * 8.418159484863281
Epoch 2010, val loss: 0.6200048327445984
Epoch 2020, training loss: 421.61016845703125 = 0.5817729830741882 + 50.0 * 8.420567512512207
Epoch 2020, val loss: 0.6180661916732788
Epoch 2030, training loss: 421.5234680175781 = 0.579585075378418 + 50.0 * 8.418877601623535
Epoch 2030, val loss: 0.6163066625595093
Epoch 2040, training loss: 421.4649353027344 = 0.5774312615394592 + 50.0 * 8.417750358581543
Epoch 2040, val loss: 0.6142026782035828
Epoch 2050, training loss: 421.4645080566406 = 0.5754033923149109 + 50.0 * 8.417781829833984
Epoch 2050, val loss: 0.612517774105072
Epoch 2060, training loss: 421.43402099609375 = 0.5733501315116882 + 50.0 * 8.417213439941406
Epoch 2060, val loss: 0.6106711030006409
Epoch 2070, training loss: 421.4034118652344 = 0.5713589787483215 + 50.0 * 8.416641235351562
Epoch 2070, val loss: 0.6089910864830017
Epoch 2080, training loss: 421.39788818359375 = 0.5693790316581726 + 50.0 * 8.416570663452148
Epoch 2080, val loss: 0.6072760820388794
Epoch 2090, training loss: 421.43243408203125 = 0.5674160718917847 + 50.0 * 8.4173002243042
Epoch 2090, val loss: 0.6055969595909119
Epoch 2100, training loss: 421.4012451171875 = 0.5654376745223999 + 50.0 * 8.416716575622559
Epoch 2100, val loss: 0.6038123369216919
Epoch 2110, training loss: 421.4532470703125 = 0.5634862184524536 + 50.0 * 8.417795181274414
Epoch 2110, val loss: 0.6020225882530212
Epoch 2120, training loss: 421.365478515625 = 0.5615565180778503 + 50.0 * 8.416078567504883
Epoch 2120, val loss: 0.600508451461792
Epoch 2130, training loss: 421.34417724609375 = 0.5596956610679626 + 50.0 * 8.415689468383789
Epoch 2130, val loss: 0.5987486839294434
Epoch 2140, training loss: 421.5460510253906 = 0.5578492879867554 + 50.0 * 8.419763565063477
Epoch 2140, val loss: 0.5972092151641846
Epoch 2150, training loss: 421.3794860839844 = 0.555896520614624 + 50.0 * 8.416471481323242
Epoch 2150, val loss: 0.5955289602279663
Epoch 2160, training loss: 421.3368225097656 = 0.5540764927864075 + 50.0 * 8.415655136108398
Epoch 2160, val loss: 0.5939499139785767
Epoch 2170, training loss: 421.2915344238281 = 0.5523875951766968 + 50.0 * 8.414782524108887
Epoch 2170, val loss: 0.5925185680389404
Epoch 2180, training loss: 421.2714538574219 = 0.5506901144981384 + 50.0 * 8.41441535949707
Epoch 2180, val loss: 0.5910775661468506
Epoch 2190, training loss: 421.2589416503906 = 0.5490198731422424 + 50.0 * 8.414198875427246
Epoch 2190, val loss: 0.5896401405334473
Epoch 2200, training loss: 421.2482604980469 = 0.5473428964614868 + 50.0 * 8.414018630981445
Epoch 2200, val loss: 0.5882039070129395
Epoch 2210, training loss: 421.2801513671875 = 0.5456705689430237 + 50.0 * 8.414689064025879
Epoch 2210, val loss: 0.5867277383804321
Epoch 2220, training loss: 421.295166015625 = 0.54387366771698 + 50.0 * 8.41502571105957
Epoch 2220, val loss: 0.5852034687995911
Epoch 2230, training loss: 421.23614501953125 = 0.5422258377075195 + 50.0 * 8.413878440856934
Epoch 2230, val loss: 0.5838445425033569
Epoch 2240, training loss: 421.2243347167969 = 0.5406292080879211 + 50.0 * 8.413674354553223
Epoch 2240, val loss: 0.5825319290161133
Epoch 2250, training loss: 421.1951599121094 = 0.539093017578125 + 50.0 * 8.413121223449707
Epoch 2250, val loss: 0.5812009572982788
Epoch 2260, training loss: 421.18865966796875 = 0.5375564098358154 + 50.0 * 8.4130220413208
Epoch 2260, val loss: 0.5799062252044678
Epoch 2270, training loss: 421.33050537109375 = 0.5360147356987 + 50.0 * 8.415889739990234
Epoch 2270, val loss: 0.5786040425300598
Epoch 2280, training loss: 421.2265319824219 = 0.5343707799911499 + 50.0 * 8.413843154907227
Epoch 2280, val loss: 0.5773166418075562
Epoch 2290, training loss: 421.22662353515625 = 0.5328389406204224 + 50.0 * 8.413875579833984
Epoch 2290, val loss: 0.5758787393569946
Epoch 2300, training loss: 421.1655578613281 = 0.531374990940094 + 50.0 * 8.412683486938477
Epoch 2300, val loss: 0.5747553110122681
Epoch 2310, training loss: 421.1336364746094 = 0.5299541354179382 + 50.0 * 8.412073135375977
Epoch 2310, val loss: 0.5735806226730347
Epoch 2320, training loss: 421.1319274902344 = 0.5285577774047852 + 50.0 * 8.412067413330078
Epoch 2320, val loss: 0.5724272727966309
Epoch 2330, training loss: 421.1921081542969 = 0.527144730091095 + 50.0 * 8.413299560546875
Epoch 2330, val loss: 0.5712140202522278
Epoch 2340, training loss: 421.116455078125 = 0.525675892829895 + 50.0 * 8.411815643310547
Epoch 2340, val loss: 0.5700457692146301
Epoch 2350, training loss: 421.1309509277344 = 0.5242849588394165 + 50.0 * 8.41213321685791
Epoch 2350, val loss: 0.568934977054596
Epoch 2360, training loss: 421.1181335449219 = 0.522929310798645 + 50.0 * 8.411904335021973
Epoch 2360, val loss: 0.5677105188369751
Epoch 2370, training loss: 421.08721923828125 = 0.5215641260147095 + 50.0 * 8.4113130569458
Epoch 2370, val loss: 0.5666859149932861
Epoch 2380, training loss: 421.1094970703125 = 0.5202363133430481 + 50.0 * 8.411785125732422
Epoch 2380, val loss: 0.5656542181968689
Epoch 2390, training loss: 421.0846252441406 = 0.5189275145530701 + 50.0 * 8.411314010620117
Epoch 2390, val loss: 0.564533531665802
Epoch 2400, training loss: 421.10662841796875 = 0.5175936222076416 + 50.0 * 8.41178035736084
Epoch 2400, val loss: 0.5635672807693481
Epoch 2410, training loss: 421.0418701171875 = 0.5163406729698181 + 50.0 * 8.410511016845703
Epoch 2410, val loss: 0.5625762343406677
Epoch 2420, training loss: 421.017822265625 = 0.5150807499885559 + 50.0 * 8.410055160522461
Epoch 2420, val loss: 0.5615044832229614
Epoch 2430, training loss: 421.0614318847656 = 0.5138645172119141 + 50.0 * 8.410951614379883
Epoch 2430, val loss: 0.5605555176734924
Epoch 2440, training loss: 421.05517578125 = 0.5125545859336853 + 50.0 * 8.410852432250977
Epoch 2440, val loss: 0.5595147013664246
Epoch 2450, training loss: 420.9970703125 = 0.5113299489021301 + 50.0 * 8.409714698791504
Epoch 2450, val loss: 0.5585864186286926
Epoch 2460, training loss: 420.97430419921875 = 0.5101457238197327 + 50.0 * 8.409283638000488
Epoch 2460, val loss: 0.5576468110084534
Epoch 2470, training loss: 420.9801330566406 = 0.5090031623840332 + 50.0 * 8.409422874450684
Epoch 2470, val loss: 0.5568089485168457
Epoch 2480, training loss: 421.09014892578125 = 0.507826566696167 + 50.0 * 8.411646842956543
Epoch 2480, val loss: 0.556008517742157
Epoch 2490, training loss: 420.9898681640625 = 0.5066225528717041 + 50.0 * 8.40966510772705
Epoch 2490, val loss: 0.5547710061073303
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.778285134449518
0.8098963993334783
=== training gcn model ===
Epoch 0, training loss: 530.2062377929688 = 1.0943433046340942 + 50.0 * 10.582237243652344
Epoch 0, val loss: 1.0947428941726685
Epoch 10, training loss: 530.1799926757812 = 1.0900661945343018 + 50.0 * 10.581798553466797
Epoch 10, val loss: 1.0904195308685303
Epoch 20, training loss: 530.0934448242188 = 1.085389494895935 + 50.0 * 10.580161094665527
Epoch 20, val loss: 1.0857305526733398
Epoch 30, training loss: 529.75146484375 = 1.0803676843643188 + 50.0 * 10.573421478271484
Epoch 30, val loss: 1.0806944370269775
Epoch 40, training loss: 528.4076538085938 = 1.0750401020050049 + 50.0 * 10.546652793884277
Epoch 40, val loss: 1.0753800868988037
Epoch 50, training loss: 523.6832275390625 = 1.0695477724075317 + 50.0 * 10.452274322509766
Epoch 50, val loss: 1.0699634552001953
Epoch 60, training loss: 508.3587646484375 = 1.0645605325698853 + 50.0 * 10.145883560180664
Epoch 60, val loss: 1.0651099681854248
Epoch 70, training loss: 482.53173828125 = 1.0597918033599854 + 50.0 * 9.629439353942871
Epoch 70, val loss: 1.0605006217956543
Epoch 80, training loss: 471.75372314453125 = 1.05579674243927 + 50.0 * 9.413958549499512
Epoch 80, val loss: 1.05665123462677
Epoch 90, training loss: 466.01226806640625 = 1.0521388053894043 + 50.0 * 9.299202919006348
Epoch 90, val loss: 1.0531113147735596
Epoch 100, training loss: 463.29052734375 = 1.0490612983703613 + 50.0 * 9.244829177856445
Epoch 100, val loss: 1.050136923789978
Epoch 110, training loss: 460.2387390136719 = 1.0469175577163696 + 50.0 * 9.183836936950684
Epoch 110, val loss: 1.0481301546096802
Epoch 120, training loss: 456.1627197265625 = 1.0460227727890015 + 50.0 * 9.102334022521973
Epoch 120, val loss: 1.0473155975341797
Epoch 130, training loss: 453.03759765625 = 1.0458073616027832 + 50.0 * 9.039835929870605
Epoch 130, val loss: 1.047071099281311
Epoch 140, training loss: 451.163330078125 = 1.0452584028244019 + 50.0 * 9.002361297607422
Epoch 140, val loss: 1.0464547872543335
Epoch 150, training loss: 448.6897277832031 = 1.044435739517212 + 50.0 * 8.952905654907227
Epoch 150, val loss: 1.045622706413269
Epoch 160, training loss: 445.9082336425781 = 1.043988823890686 + 50.0 * 8.897285461425781
Epoch 160, val loss: 1.0452187061309814
Epoch 170, training loss: 443.7794494628906 = 1.0439937114715576 + 50.0 * 8.854708671569824
Epoch 170, val loss: 1.0452200174331665
Epoch 180, training loss: 442.0348205566406 = 1.0439823865890503 + 50.0 * 8.819816589355469
Epoch 180, val loss: 1.0451617240905762
Epoch 190, training loss: 440.5376892089844 = 1.0438021421432495 + 50.0 * 8.789877891540527
Epoch 190, val loss: 1.044946551322937
Epoch 200, training loss: 439.3850402832031 = 1.0434377193450928 + 50.0 * 8.76683235168457
Epoch 200, val loss: 1.044549584388733
Epoch 210, training loss: 438.32086181640625 = 1.0429551601409912 + 50.0 * 8.74555778503418
Epoch 210, val loss: 1.0440802574157715
Epoch 220, training loss: 437.27191162109375 = 1.04253351688385 + 50.0 * 8.724587440490723
Epoch 220, val loss: 1.0436463356018066
Epoch 230, training loss: 436.3827819824219 = 1.0420656204223633 + 50.0 * 8.706814765930176
Epoch 230, val loss: 1.0431702136993408
Epoch 240, training loss: 435.5489501953125 = 1.0415494441986084 + 50.0 * 8.69014835357666
Epoch 240, val loss: 1.04266357421875
Epoch 250, training loss: 434.72216796875 = 1.0410932302474976 + 50.0 * 8.67362117767334
Epoch 250, val loss: 1.0422407388687134
Epoch 260, training loss: 433.808349609375 = 1.0406972169876099 + 50.0 * 8.655352592468262
Epoch 260, val loss: 1.0418391227722168
Epoch 270, training loss: 433.0272216796875 = 1.0403066873550415 + 50.0 * 8.639738082885742
Epoch 270, val loss: 1.0414481163024902
Epoch 280, training loss: 432.4313049316406 = 1.0398223400115967 + 50.0 * 8.627829551696777
Epoch 280, val loss: 1.0409737825393677
Epoch 290, training loss: 431.885498046875 = 1.039244532585144 + 50.0 * 8.616925239562988
Epoch 290, val loss: 1.040390133857727
Epoch 300, training loss: 431.4803771972656 = 1.0385946035385132 + 50.0 * 8.608835220336914
Epoch 300, val loss: 1.0397533178329468
Epoch 310, training loss: 431.1402587890625 = 1.0378596782684326 + 50.0 * 8.60204792022705
Epoch 310, val loss: 1.039049506187439
Epoch 320, training loss: 430.7498474121094 = 1.0370920896530151 + 50.0 * 8.594255447387695
Epoch 320, val loss: 1.038304328918457
Epoch 330, training loss: 430.44830322265625 = 1.0363054275512695 + 50.0 * 8.588239669799805
Epoch 330, val loss: 1.0375398397445679
Epoch 340, training loss: 430.1816101074219 = 1.035449504852295 + 50.0 * 8.58292293548584
Epoch 340, val loss: 1.036723256111145
Epoch 350, training loss: 429.8106994628906 = 1.0345869064331055 + 50.0 * 8.575522422790527
Epoch 350, val loss: 1.0358830690383911
Epoch 360, training loss: 429.5328063964844 = 1.0337326526641846 + 50.0 * 8.569981575012207
Epoch 360, val loss: 1.0350617170333862
Epoch 370, training loss: 429.33001708984375 = 1.0328816175460815 + 50.0 * 8.565942764282227
Epoch 370, val loss: 1.0342305898666382
Epoch 380, training loss: 429.0074462890625 = 1.031972050666809 + 50.0 * 8.55950927734375
Epoch 380, val loss: 1.033358097076416
Epoch 390, training loss: 428.75189208984375 = 1.0310670137405396 + 50.0 * 8.55441665649414
Epoch 390, val loss: 1.0324883460998535
Epoch 400, training loss: 428.48297119140625 = 1.0301295518875122 + 50.0 * 8.549057006835938
Epoch 400, val loss: 1.031615138053894
Epoch 410, training loss: 428.2081604003906 = 1.0292000770568848 + 50.0 * 8.5435791015625
Epoch 410, val loss: 1.0307104587554932
Epoch 420, training loss: 427.9315185546875 = 1.0282611846923828 + 50.0 * 8.538064956665039
Epoch 420, val loss: 1.0298328399658203
Epoch 430, training loss: 427.8284606933594 = 1.0273005962371826 + 50.0 * 8.536023139953613
Epoch 430, val loss: 1.028928279876709
Epoch 440, training loss: 427.5472412109375 = 1.0262824296951294 + 50.0 * 8.53041934967041
Epoch 440, val loss: 1.0279382467269897
Epoch 450, training loss: 427.2395324707031 = 1.025221586227417 + 50.0 * 8.524286270141602
Epoch 450, val loss: 1.0269453525543213
Epoch 460, training loss: 427.00482177734375 = 1.024156928062439 + 50.0 * 8.519613265991211
Epoch 460, val loss: 1.025924563407898
Epoch 470, training loss: 426.812255859375 = 1.02305269241333 + 50.0 * 8.51578426361084
Epoch 470, val loss: 1.024870753288269
Epoch 480, training loss: 426.724609375 = 1.0218583345413208 + 50.0 * 8.514055252075195
Epoch 480, val loss: 1.0237199068069458
Epoch 490, training loss: 426.49078369140625 = 1.0205429792404175 + 50.0 * 8.509405136108398
Epoch 490, val loss: 1.0224874019622803
Epoch 500, training loss: 426.36761474609375 = 1.0192193984985352 + 50.0 * 8.506967544555664
Epoch 500, val loss: 1.0212390422821045
Epoch 510, training loss: 426.21710205078125 = 1.0178732872009277 + 50.0 * 8.503984451293945
Epoch 510, val loss: 1.0199577808380127
Epoch 520, training loss: 426.0848083496094 = 1.0164774656295776 + 50.0 * 8.50136661529541
Epoch 520, val loss: 1.0186316967010498
Epoch 530, training loss: 426.0496826171875 = 1.0150275230407715 + 50.0 * 8.500693321228027
Epoch 530, val loss: 1.0172401666641235
Epoch 540, training loss: 425.96905517578125 = 1.0134626626968384 + 50.0 * 8.499112129211426
Epoch 540, val loss: 1.015785813331604
Epoch 550, training loss: 425.7644958496094 = 1.0118763446807861 + 50.0 * 8.495052337646484
Epoch 550, val loss: 1.0142768621444702
Epoch 560, training loss: 425.6651916503906 = 1.0102722644805908 + 50.0 * 8.493098258972168
Epoch 560, val loss: 1.0127489566802979
Epoch 570, training loss: 425.57904052734375 = 1.008636474609375 + 50.0 * 8.491408348083496
Epoch 570, val loss: 1.0111956596374512
Epoch 580, training loss: 425.4869689941406 = 1.0069218873977661 + 50.0 * 8.489601135253906
Epoch 580, val loss: 1.0095763206481934
Epoch 590, training loss: 425.4096984863281 = 1.0051512718200684 + 50.0 * 8.488090515136719
Epoch 590, val loss: 1.0078997611999512
Epoch 600, training loss: 425.32403564453125 = 1.0033748149871826 + 50.0 * 8.48641300201416
Epoch 600, val loss: 1.0062215328216553
Epoch 610, training loss: 425.463623046875 = 1.0015571117401123 + 50.0 * 8.489241600036621
Epoch 610, val loss: 1.0045092105865479
Epoch 620, training loss: 425.2127990722656 = 0.9996445178985596 + 50.0 * 8.48426342010498
Epoch 620, val loss: 1.002676248550415
Epoch 630, training loss: 425.1161193847656 = 0.9977270364761353 + 50.0 * 8.482367515563965
Epoch 630, val loss: 1.0008773803710938
Epoch 640, training loss: 425.00799560546875 = 0.9958131313323975 + 50.0 * 8.480243682861328
Epoch 640, val loss: 0.9990665912628174
Epoch 650, training loss: 424.93524169921875 = 0.9938659071922302 + 50.0 * 8.478827476501465
Epoch 650, val loss: 0.9972293376922607
Epoch 660, training loss: 424.8635559082031 = 0.9918835759162903 + 50.0 * 8.477433204650879
Epoch 660, val loss: 0.9953473210334778
Epoch 670, training loss: 425.03411865234375 = 0.9898315072059631 + 50.0 * 8.48088550567627
Epoch 670, val loss: 0.9933683276176453
Epoch 680, training loss: 424.8028564453125 = 0.9876546263694763 + 50.0 * 8.476304054260254
Epoch 680, val loss: 0.9913250207901001
Epoch 690, training loss: 424.675048828125 = 0.9855273365974426 + 50.0 * 8.473790168762207
Epoch 690, val loss: 0.9893314838409424
Epoch 700, training loss: 424.5894470214844 = 0.9834094643592834 + 50.0 * 8.472121238708496
Epoch 700, val loss: 0.9873307347297668
Epoch 710, training loss: 424.7635192871094 = 0.9812347292900085 + 50.0 * 8.475646018981934
Epoch 710, val loss: 0.9852822422981262
Epoch 720, training loss: 424.457763671875 = 0.9789688587188721 + 50.0 * 8.469575881958008
Epoch 720, val loss: 0.9831153750419617
Epoch 730, training loss: 424.4054260253906 = 0.9767032265663147 + 50.0 * 8.468574523925781
Epoch 730, val loss: 0.9809799194335938
Epoch 740, training loss: 424.31280517578125 = 0.9744388461112976 + 50.0 * 8.466767311096191
Epoch 740, val loss: 0.9788392186164856
Epoch 750, training loss: 424.2611999511719 = 0.972144365310669 + 50.0 * 8.465781211853027
Epoch 750, val loss: 0.9766700863838196
Epoch 760, training loss: 424.43096923828125 = 0.9697705507278442 + 50.0 * 8.469223976135254
Epoch 760, val loss: 0.9744157195091248
Epoch 770, training loss: 424.1594543457031 = 0.967273473739624 + 50.0 * 8.46384334564209
Epoch 770, val loss: 0.972021758556366
Epoch 780, training loss: 424.0955505371094 = 0.9648025035858154 + 50.0 * 8.462615013122559
Epoch 780, val loss: 0.9696791768074036
Epoch 790, training loss: 424.03326416015625 = 0.9623332619667053 + 50.0 * 8.461418151855469
Epoch 790, val loss: 0.9673399925231934
Epoch 800, training loss: 423.9858703613281 = 0.9598252773284912 + 50.0 * 8.46052074432373
Epoch 800, val loss: 0.9649528861045837
Epoch 810, training loss: 424.0416564941406 = 0.9572120308876038 + 50.0 * 8.461688995361328
Epoch 810, val loss: 0.9624561071395874
Epoch 820, training loss: 423.95794677734375 = 0.9544687271118164 + 50.0 * 8.46006965637207
Epoch 820, val loss: 0.9598493576049805
Epoch 830, training loss: 423.8575439453125 = 0.951751708984375 + 50.0 * 8.458115577697754
Epoch 830, val loss: 0.9572522640228271
Epoch 840, training loss: 423.79315185546875 = 0.9490223526954651 + 50.0 * 8.45688247680664
Epoch 840, val loss: 0.9546561241149902
Epoch 850, training loss: 423.7676696777344 = 0.946258008480072 + 50.0 * 8.456428527832031
Epoch 850, val loss: 0.9520321488380432
Epoch 860, training loss: 423.82098388671875 = 0.9434096813201904 + 50.0 * 8.457551002502441
Epoch 860, val loss: 0.9493234753608704
Epoch 870, training loss: 423.688720703125 = 0.9404846429824829 + 50.0 * 8.454964637756348
Epoch 870, val loss: 0.9465082287788391
Epoch 880, training loss: 423.62158203125 = 0.9375413656234741 + 50.0 * 8.453680992126465
Epoch 880, val loss: 0.9437185525894165
Epoch 890, training loss: 423.6645812988281 = 0.9345716834068298 + 50.0 * 8.45460033416748
Epoch 890, val loss: 0.9409089088439941
Epoch 900, training loss: 423.60565185546875 = 0.9314295053482056 + 50.0 * 8.453484535217285
Epoch 900, val loss: 0.9378560781478882
Epoch 910, training loss: 423.5258483886719 = 0.928278386592865 + 50.0 * 8.451951026916504
Epoch 910, val loss: 0.9348835945129395
Epoch 920, training loss: 423.4809875488281 = 0.925170361995697 + 50.0 * 8.451116561889648
Epoch 920, val loss: 0.9319359064102173
Epoch 930, training loss: 423.4270935058594 = 0.922068178653717 + 50.0 * 8.450100898742676
Epoch 930, val loss: 0.9289679527282715
Epoch 940, training loss: 423.3883361816406 = 0.9189148545265198 + 50.0 * 8.44938850402832
Epoch 940, val loss: 0.9259588122367859
Epoch 950, training loss: 423.6719665527344 = 0.9156978726387024 + 50.0 * 8.455124855041504
Epoch 950, val loss: 0.9228310585021973
Epoch 960, training loss: 423.42913818359375 = 0.9121704697608948 + 50.0 * 8.450339317321777
Epoch 960, val loss: 0.9195382595062256
Epoch 970, training loss: 423.3261413574219 = 0.908740758895874 + 50.0 * 8.448348045349121
Epoch 970, val loss: 0.9162760972976685
Epoch 980, training loss: 423.2596130371094 = 0.9053807854652405 + 50.0 * 8.447084426879883
Epoch 980, val loss: 0.9130619168281555
Epoch 990, training loss: 423.20587158203125 = 0.9020127654075623 + 50.0 * 8.446077346801758
Epoch 990, val loss: 0.9098501801490784
Epoch 1000, training loss: 423.1665954589844 = 0.898587167263031 + 50.0 * 8.44536018371582
Epoch 1000, val loss: 0.9065787196159363
Epoch 1010, training loss: 423.1313781738281 = 0.8950952291488647 + 50.0 * 8.44472599029541
Epoch 1010, val loss: 0.9032483696937561
Epoch 1020, training loss: 423.1748046875 = 0.8915454149246216 + 50.0 * 8.44566535949707
Epoch 1020, val loss: 0.8998498916625977
Epoch 1030, training loss: 423.24664306640625 = 0.8877441883087158 + 50.0 * 8.44717788696289
Epoch 1030, val loss: 0.8962355256080627
Epoch 1040, training loss: 423.07574462890625 = 0.8839452862739563 + 50.0 * 8.443836212158203
Epoch 1040, val loss: 0.8926265835762024
Epoch 1050, training loss: 423.011962890625 = 0.8802345991134644 + 50.0 * 8.442634582519531
Epoch 1050, val loss: 0.8890768885612488
Epoch 1060, training loss: 422.96685791015625 = 0.8765316605567932 + 50.0 * 8.44180679321289
Epoch 1060, val loss: 0.885556697845459
Epoch 1070, training loss: 422.93096923828125 = 0.8727875351905823 + 50.0 * 8.441163063049316
Epoch 1070, val loss: 0.8819844126701355
Epoch 1080, training loss: 422.90216064453125 = 0.8689842820167542 + 50.0 * 8.44066333770752
Epoch 1080, val loss: 0.8783515095710754
Epoch 1090, training loss: 423.1007995605469 = 0.8650872111320496 + 50.0 * 8.444714546203613
Epoch 1090, val loss: 0.8746126890182495
Epoch 1100, training loss: 422.91192626953125 = 0.8609455823898315 + 50.0 * 8.441020011901855
Epoch 1100, val loss: 0.8706735968589783
Epoch 1110, training loss: 422.8524475097656 = 0.8568894267082214 + 50.0 * 8.439910888671875
Epoch 1110, val loss: 0.8668150901794434
Epoch 1120, training loss: 422.78070068359375 = 0.8528841137886047 + 50.0 * 8.438556671142578
Epoch 1120, val loss: 0.8630103468894958
Epoch 1130, training loss: 422.7510986328125 = 0.8488870859146118 + 50.0 * 8.438044548034668
Epoch 1130, val loss: 0.8591943979263306
Epoch 1140, training loss: 422.756103515625 = 0.844828724861145 + 50.0 * 8.438225746154785
Epoch 1140, val loss: 0.8553527593612671
Epoch 1150, training loss: 422.73468017578125 = 0.840541660785675 + 50.0 * 8.437882423400879
Epoch 1150, val loss: 0.8512126803398132
Epoch 1160, training loss: 422.7168273925781 = 0.8362264633178711 + 50.0 * 8.437612533569336
Epoch 1160, val loss: 0.8471445441246033
Epoch 1170, training loss: 422.638916015625 = 0.8320094347000122 + 50.0 * 8.436138153076172
Epoch 1170, val loss: 0.8431013822555542
Epoch 1180, training loss: 422.6094055175781 = 0.8278295397758484 + 50.0 * 8.43563175201416
Epoch 1180, val loss: 0.839117705821991
Epoch 1190, training loss: 422.5762023925781 = 0.8236318230628967 + 50.0 * 8.435050964355469
Epoch 1190, val loss: 0.835128903388977
Epoch 1200, training loss: 422.5782775878906 = 0.8193936347961426 + 50.0 * 8.43517780303955
Epoch 1200, val loss: 0.8310791254043579
Epoch 1210, training loss: 422.5631408691406 = 0.8149324655532837 + 50.0 * 8.434964179992676
Epoch 1210, val loss: 0.826805591583252
Epoch 1220, training loss: 422.5648498535156 = 0.8104051947593689 + 50.0 * 8.435089111328125
Epoch 1220, val loss: 0.8224990963935852
Epoch 1230, training loss: 422.4769592285156 = 0.8060077428817749 + 50.0 * 8.433419227600098
Epoch 1230, val loss: 0.8183660507202148
Epoch 1240, training loss: 422.449462890625 = 0.8016696572303772 + 50.0 * 8.432955741882324
Epoch 1240, val loss: 0.8142377138137817
Epoch 1250, training loss: 422.4150695800781 = 0.797328770160675 + 50.0 * 8.432354927062988
Epoch 1250, val loss: 0.8101148009300232
Epoch 1260, training loss: 422.3890686035156 = 0.7929394245147705 + 50.0 * 8.431922912597656
Epoch 1260, val loss: 0.8059489130973816
Epoch 1270, training loss: 422.3629150390625 = 0.7885141968727112 + 50.0 * 8.431488037109375
Epoch 1270, val loss: 0.8017518520355225
Epoch 1280, training loss: 422.3369140625 = 0.7840685844421387 + 50.0 * 8.43105697631836
Epoch 1280, val loss: 0.7975383996963501
Epoch 1290, training loss: 422.3676452636719 = 0.7795946002006531 + 50.0 * 8.431760787963867
Epoch 1290, val loss: 0.7933397889137268
Epoch 1300, training loss: 422.40167236328125 = 0.7748691439628601 + 50.0 * 8.432536125183105
Epoch 1300, val loss: 0.7887693643569946
Epoch 1310, training loss: 422.2690734863281 = 0.7701882719993591 + 50.0 * 8.429977416992188
Epoch 1310, val loss: 0.7843641042709351
Epoch 1320, training loss: 422.2480163574219 = 0.765684962272644 + 50.0 * 8.429646492004395
Epoch 1320, val loss: 0.7801101803779602
Epoch 1330, training loss: 422.2210388183594 = 0.761263906955719 + 50.0 * 8.429195404052734
Epoch 1330, val loss: 0.7759616374969482
Epoch 1340, training loss: 422.1939392089844 = 0.7568519711494446 + 50.0 * 8.428741455078125
Epoch 1340, val loss: 0.7717875838279724
Epoch 1350, training loss: 422.1783142089844 = 0.7524163126945496 + 50.0 * 8.428518295288086
Epoch 1350, val loss: 0.7676149010658264
Epoch 1360, training loss: 422.3339538574219 = 0.7479273676872253 + 50.0 * 8.431720733642578
Epoch 1360, val loss: 0.7633683681488037
Epoch 1370, training loss: 422.1668395996094 = 0.7433363199234009 + 50.0 * 8.42846965789795
Epoch 1370, val loss: 0.7590285539627075
Epoch 1380, training loss: 422.2096252441406 = 0.7388418912887573 + 50.0 * 8.429415702819824
Epoch 1380, val loss: 0.7548133134841919
Epoch 1390, training loss: 422.0854187011719 = 0.734311044216156 + 50.0 * 8.427021980285645
Epoch 1390, val loss: 0.7505300045013428
Epoch 1400, training loss: 422.0528259277344 = 0.7298722267150879 + 50.0 * 8.426459312438965
Epoch 1400, val loss: 0.7463505864143372
Epoch 1410, training loss: 422.0266418457031 = 0.7255184650421143 + 50.0 * 8.42602252960205
Epoch 1410, val loss: 0.7422661781311035
Epoch 1420, training loss: 422.0072937011719 = 0.7212151885032654 + 50.0 * 8.425721168518066
Epoch 1420, val loss: 0.7382280230522156
Epoch 1430, training loss: 421.9841003417969 = 0.7169071435928345 + 50.0 * 8.42534351348877
Epoch 1430, val loss: 0.7341858148574829
Epoch 1440, training loss: 422.0569763183594 = 0.712615966796875 + 50.0 * 8.426887512207031
Epoch 1440, val loss: 0.7302088737487793
Epoch 1450, training loss: 422.0284423828125 = 0.7081533074378967 + 50.0 * 8.426405906677246
Epoch 1450, val loss: 0.725975513458252
Epoch 1460, training loss: 421.97918701171875 = 0.7038037180900574 + 50.0 * 8.425507545471191
Epoch 1460, val loss: 0.7218656539916992
Epoch 1470, training loss: 421.9002990722656 = 0.6995832324028015 + 50.0 * 8.4240140914917
Epoch 1470, val loss: 0.7179469466209412
Epoch 1480, training loss: 421.8771057128906 = 0.6954460144042969 + 50.0 * 8.423633575439453
Epoch 1480, val loss: 0.7140514850616455
Epoch 1490, training loss: 421.8937683105469 = 0.6913496851921082 + 50.0 * 8.42404842376709
Epoch 1490, val loss: 0.7102125287055969
Epoch 1500, training loss: 421.9811096191406 = 0.6871408224105835 + 50.0 * 8.42587947845459
Epoch 1500, val loss: 0.7062578201293945
Epoch 1510, training loss: 421.8202209472656 = 0.6828912496566772 + 50.0 * 8.422746658325195
Epoch 1510, val loss: 0.7023153901100159
Epoch 1520, training loss: 421.79779052734375 = 0.678848385810852 + 50.0 * 8.422378540039062
Epoch 1520, val loss: 0.6985637545585632
Epoch 1530, training loss: 421.778564453125 = 0.6748915910720825 + 50.0 * 8.422073364257812
Epoch 1530, val loss: 0.6948665380477905
Epoch 1540, training loss: 421.7553405761719 = 0.670978307723999 + 50.0 * 8.421687126159668
Epoch 1540, val loss: 0.6912270784378052
Epoch 1550, training loss: 421.74761962890625 = 0.6670900583267212 + 50.0 * 8.421610832214355
Epoch 1550, val loss: 0.6875763535499573
Epoch 1560, training loss: 422.0229797363281 = 0.66313636302948 + 50.0 * 8.427196502685547
Epoch 1560, val loss: 0.6837784051895142
Epoch 1570, training loss: 421.7152404785156 = 0.6591411232948303 + 50.0 * 8.421121597290039
Epoch 1570, val loss: 0.6802070736885071
Epoch 1580, training loss: 421.7107849121094 = 0.6553284525871277 + 50.0 * 8.421109199523926
Epoch 1580, val loss: 0.6767093539237976
Epoch 1590, training loss: 421.6653137207031 = 0.651621401309967 + 50.0 * 8.420273780822754
Epoch 1590, val loss: 0.6732494235038757
Epoch 1600, training loss: 421.6434020996094 = 0.6479812264442444 + 50.0 * 8.41990852355957
Epoch 1600, val loss: 0.669861376285553
Epoch 1610, training loss: 421.626220703125 = 0.6443684697151184 + 50.0 * 8.419636726379395
Epoch 1610, val loss: 0.66654372215271
Epoch 1620, training loss: 421.65875244140625 = 0.6407763957977295 + 50.0 * 8.42035961151123
Epoch 1620, val loss: 0.6631996035575867
Epoch 1630, training loss: 421.6239929199219 = 0.6370633840560913 + 50.0 * 8.41973876953125
Epoch 1630, val loss: 0.6597378849983215
Epoch 1640, training loss: 421.6663513183594 = 0.6333892941474915 + 50.0 * 8.420659065246582
Epoch 1640, val loss: 0.6563575863838196
Epoch 1650, training loss: 421.57470703125 = 0.6298828125 + 50.0 * 8.418896675109863
Epoch 1650, val loss: 0.6531203389167786
Epoch 1660, training loss: 421.5501708984375 = 0.6265100240707397 + 50.0 * 8.418473243713379
Epoch 1660, val loss: 0.6500247716903687
Epoch 1670, training loss: 421.5897521972656 = 0.6231624484062195 + 50.0 * 8.419331550598145
Epoch 1670, val loss: 0.6469604969024658
Epoch 1680, training loss: 421.5543212890625 = 0.6197887063026428 + 50.0 * 8.41869068145752
Epoch 1680, val loss: 0.6438393592834473
Epoch 1690, training loss: 421.52020263671875 = 0.6165023446083069 + 50.0 * 8.418073654174805
Epoch 1690, val loss: 0.6407952308654785
Epoch 1700, training loss: 421.4736328125 = 0.6132662296295166 + 50.0 * 8.417206764221191
Epoch 1700, val loss: 0.6378291845321655
Epoch 1710, training loss: 421.4693298339844 = 0.6100884675979614 + 50.0 * 8.417184829711914
Epoch 1710, val loss: 0.6349080204963684
Epoch 1720, training loss: 421.53564453125 = 0.606927752494812 + 50.0 * 8.418574333190918
Epoch 1720, val loss: 0.6319363117218018
Epoch 1730, training loss: 421.529052734375 = 0.6037371158599854 + 50.0 * 8.418506622314453
Epoch 1730, val loss: 0.6290254592895508
Epoch 1740, training loss: 421.41864013671875 = 0.6005545854568481 + 50.0 * 8.416361808776855
Epoch 1740, val loss: 0.6261851191520691
Epoch 1750, training loss: 421.41253662109375 = 0.5975257158279419 + 50.0 * 8.416299819946289
Epoch 1750, val loss: 0.6234476566314697
Epoch 1760, training loss: 421.3898620605469 = 0.5945807695388794 + 50.0 * 8.415905952453613
Epoch 1760, val loss: 0.6207186579704285
Epoch 1770, training loss: 421.3670654296875 = 0.5916923880577087 + 50.0 * 8.415507316589355
Epoch 1770, val loss: 0.618093729019165
Epoch 1780, training loss: 421.3623046875 = 0.5888415575027466 + 50.0 * 8.4154691696167
Epoch 1780, val loss: 0.6154752373695374
Epoch 1790, training loss: 421.47900390625 = 0.586000382900238 + 50.0 * 8.41786003112793
Epoch 1790, val loss: 0.6127867102622986
Epoch 1800, training loss: 421.364990234375 = 0.5831069946289062 + 50.0 * 8.415637969970703
Epoch 1800, val loss: 0.6103897094726562
Epoch 1810, training loss: 421.3097839355469 = 0.5802978277206421 + 50.0 * 8.414589881896973
Epoch 1810, val loss: 0.607728123664856
Epoch 1820, training loss: 421.2925109863281 = 0.5775997042655945 + 50.0 * 8.414298057556152
Epoch 1820, val loss: 0.6053631901741028
Epoch 1830, training loss: 421.33721923828125 = 0.5749556422233582 + 50.0 * 8.415245056152344
Epoch 1830, val loss: 0.6029807925224304
Epoch 1840, training loss: 421.2631530761719 = 0.5722928047180176 + 50.0 * 8.413817405700684
Epoch 1840, val loss: 0.6005516648292542
Epoch 1850, training loss: 421.2688293457031 = 0.5696931481361389 + 50.0 * 8.413982391357422
Epoch 1850, val loss: 0.59818434715271
Epoch 1860, training loss: 421.5096435546875 = 0.5671390295028687 + 50.0 * 8.41884994506836
Epoch 1860, val loss: 0.5959282517433167
Epoch 1870, training loss: 421.29693603515625 = 0.5644797682762146 + 50.0 * 8.41464900970459
Epoch 1870, val loss: 0.5934954285621643
Epoch 1880, training loss: 421.22283935546875 = 0.5620115399360657 + 50.0 * 8.413216590881348
Epoch 1880, val loss: 0.5913365483283997
Epoch 1890, training loss: 421.1895446777344 = 0.5596024394035339 + 50.0 * 8.412598609924316
Epoch 1890, val loss: 0.5891571044921875
Epoch 1900, training loss: 421.1690673828125 = 0.5572655200958252 + 50.0 * 8.412236213684082
Epoch 1900, val loss: 0.5871073603630066
Epoch 1910, training loss: 421.18115234375 = 0.5549448132514954 + 50.0 * 8.412524223327637
Epoch 1910, val loss: 0.5850344896316528
Epoch 1920, training loss: 421.2773132324219 = 0.5525726675987244 + 50.0 * 8.414494514465332
Epoch 1920, val loss: 0.5829095840454102
Epoch 1930, training loss: 421.12908935546875 = 0.5502352118492126 + 50.0 * 8.411577224731445
Epoch 1930, val loss: 0.5808544754981995
Epoch 1940, training loss: 421.1360778808594 = 0.5479934811592102 + 50.0 * 8.411761283874512
Epoch 1940, val loss: 0.5789074897766113
Epoch 1950, training loss: 421.1900329589844 = 0.5458090901374817 + 50.0 * 8.412884712219238
Epoch 1950, val loss: 0.5770419836044312
Epoch 1960, training loss: 421.1382141113281 = 0.5435994863510132 + 50.0 * 8.41189193725586
Epoch 1960, val loss: 0.5750651955604553
Epoch 1970, training loss: 421.0718078613281 = 0.5414442420005798 + 50.0 * 8.41060733795166
Epoch 1970, val loss: 0.5731152296066284
Epoch 1980, training loss: 421.06207275390625 = 0.5393748879432678 + 50.0 * 8.410453796386719
Epoch 1980, val loss: 0.571262776851654
Epoch 1990, training loss: 421.0489196777344 = 0.5373532772064209 + 50.0 * 8.410231590270996
Epoch 1990, val loss: 0.569520115852356
Epoch 2000, training loss: 421.08355712890625 = 0.5353625416755676 + 50.0 * 8.410964012145996
Epoch 2000, val loss: 0.567782998085022
Epoch 2010, training loss: 421.14642333984375 = 0.5333287119865417 + 50.0 * 8.412261962890625
Epoch 2010, val loss: 0.566063642501831
Epoch 2020, training loss: 421.10382080078125 = 0.5313024520874023 + 50.0 * 8.411450386047363
Epoch 2020, val loss: 0.5644027590751648
Epoch 2030, training loss: 421.023681640625 = 0.529349684715271 + 50.0 * 8.409886360168457
Epoch 2030, val loss: 0.5626121163368225
Epoch 2040, training loss: 420.98663330078125 = 0.527459979057312 + 50.0 * 8.409183502197266
Epoch 2040, val loss: 0.5609402656555176
Epoch 2050, training loss: 420.9752197265625 = 0.5256319642066956 + 50.0 * 8.408991813659668
Epoch 2050, val loss: 0.5593276619911194
Epoch 2060, training loss: 421.0868835449219 = 0.5238072872161865 + 50.0 * 8.411261558532715
Epoch 2060, val loss: 0.5576221346855164
Epoch 2070, training loss: 420.9465026855469 = 0.5219563245773315 + 50.0 * 8.408491134643555
Epoch 2070, val loss: 0.5562698245048523
Epoch 2080, training loss: 420.9288024902344 = 0.5201790928840637 + 50.0 * 8.408172607421875
Epoch 2080, val loss: 0.554691731929779
Epoch 2090, training loss: 420.9227600097656 = 0.5184438824653625 + 50.0 * 8.408086776733398
Epoch 2090, val loss: 0.5531793236732483
Epoch 2100, training loss: 420.9892883300781 = 0.5167542099952698 + 50.0 * 8.40945053100586
Epoch 2100, val loss: 0.5517330765724182
Epoch 2110, training loss: 420.93035888671875 = 0.5150186419487 + 50.0 * 8.408307075500488
Epoch 2110, val loss: 0.5502460598945618
Epoch 2120, training loss: 420.8794860839844 = 0.5133251547813416 + 50.0 * 8.407322883605957
Epoch 2120, val loss: 0.5489447712898254
Epoch 2130, training loss: 420.86065673828125 = 0.5117014050483704 + 50.0 * 8.40697956085205
Epoch 2130, val loss: 0.5475183725357056
Epoch 2140, training loss: 420.86236572265625 = 0.510124921798706 + 50.0 * 8.407044410705566
Epoch 2140, val loss: 0.5461658239364624
Epoch 2150, training loss: 420.9539794921875 = 0.5085604786872864 + 50.0 * 8.408907890319824
Epoch 2150, val loss: 0.5448799729347229
Epoch 2160, training loss: 420.8760986328125 = 0.5069808959960938 + 50.0 * 8.407382011413574
Epoch 2160, val loss: 0.543691873550415
Epoch 2170, training loss: 420.8551330566406 = 0.5054334998130798 + 50.0 * 8.406993865966797
Epoch 2170, val loss: 0.5422617197036743
Epoch 2180, training loss: 421.0305480957031 = 0.5038976669311523 + 50.0 * 8.41053295135498
Epoch 2180, val loss: 0.5409727096557617
Epoch 2190, training loss: 420.84893798828125 = 0.5023849606513977 + 50.0 * 8.406930923461914
Epoch 2190, val loss: 0.5397124886512756
Epoch 2200, training loss: 420.7908020019531 = 0.5009121894836426 + 50.0 * 8.405797958374023
Epoch 2200, val loss: 0.5385416150093079
Epoch 2210, training loss: 420.7709655761719 = 0.4995092451572418 + 50.0 * 8.405428886413574
Epoch 2210, val loss: 0.5374292731285095
Epoch 2220, training loss: 420.759765625 = 0.4981370270252228 + 50.0 * 8.405232429504395
Epoch 2220, val loss: 0.5363315343856812
Epoch 2230, training loss: 420.8126525878906 = 0.49677687883377075 + 50.0 * 8.406317710876465
Epoch 2230, val loss: 0.5353596806526184
Epoch 2240, training loss: 420.7994689941406 = 0.4953671097755432 + 50.0 * 8.406082153320312
Epoch 2240, val loss: 0.5341387987136841
Epoch 2250, training loss: 420.7424011230469 = 0.49398931860923767 + 50.0 * 8.40496826171875
Epoch 2250, val loss: 0.5328323245048523
Epoch 2260, training loss: 420.717041015625 = 0.4926730692386627 + 50.0 * 8.404487609863281
Epoch 2260, val loss: 0.5319131016731262
Epoch 2270, training loss: 420.7074890136719 = 0.4913907051086426 + 50.0 * 8.404321670532227
Epoch 2270, val loss: 0.5308228135108948
Epoch 2280, training loss: 420.7882385253906 = 0.4901343882083893 + 50.0 * 8.405961990356445
Epoch 2280, val loss: 0.5298766493797302
Epoch 2290, training loss: 420.7131652832031 = 0.4888065755367279 + 50.0 * 8.404487609863281
Epoch 2290, val loss: 0.5287428498268127
Epoch 2300, training loss: 420.6841125488281 = 0.4875176250934601 + 50.0 * 8.403931617736816
Epoch 2300, val loss: 0.5277876853942871
Epoch 2310, training loss: 420.6725769042969 = 0.4863012433052063 + 50.0 * 8.403725624084473
Epoch 2310, val loss: 0.5268421769142151
Epoch 2320, training loss: 420.7668151855469 = 0.4851011633872986 + 50.0 * 8.405633926391602
Epoch 2320, val loss: 0.5259708762168884
Epoch 2330, training loss: 420.6435546875 = 0.48389264941215515 + 50.0 * 8.403193473815918
Epoch 2330, val loss: 0.5249342918395996
Epoch 2340, training loss: 420.6217041015625 = 0.4827220141887665 + 50.0 * 8.402779579162598
Epoch 2340, val loss: 0.5240309834480286
Epoch 2350, training loss: 420.6155700683594 = 0.4815923869609833 + 50.0 * 8.402679443359375
Epoch 2350, val loss: 0.5231913924217224
Epoch 2360, training loss: 420.63848876953125 = 0.48047390580177307 + 50.0 * 8.403160095214844
Epoch 2360, val loss: 0.5224116444587708
Epoch 2370, training loss: 420.7451171875 = 0.47934237122535706 + 50.0 * 8.405315399169922
Epoch 2370, val loss: 0.5216495990753174
Epoch 2380, training loss: 420.62603759765625 = 0.47817263007164 + 50.0 * 8.40295696258545
Epoch 2380, val loss: 0.5205540060997009
Epoch 2390, training loss: 420.5902099609375 = 0.47708162665367126 + 50.0 * 8.402262687683105
Epoch 2390, val loss: 0.5196563005447388
Epoch 2400, training loss: 420.5769348144531 = 0.4760277271270752 + 50.0 * 8.402018547058105
Epoch 2400, val loss: 0.5188627243041992
Epoch 2410, training loss: 420.5699157714844 = 0.4750014841556549 + 50.0 * 8.401898384094238
Epoch 2410, val loss: 0.5180858373641968
Epoch 2420, training loss: 420.7450866699219 = 0.4739798605442047 + 50.0 * 8.40542221069336
Epoch 2420, val loss: 0.5173588395118713
Epoch 2430, training loss: 420.6054382324219 = 0.4728975296020508 + 50.0 * 8.402650833129883
Epoch 2430, val loss: 0.5166707038879395
Epoch 2440, training loss: 420.5531311035156 = 0.471855491399765 + 50.0 * 8.401625633239746
Epoch 2440, val loss: 0.515764057636261
Epoch 2450, training loss: 420.518798828125 = 0.4708670377731323 + 50.0 * 8.400959014892578
Epoch 2450, val loss: 0.5150566697120667
Epoch 2460, training loss: 420.505859375 = 0.4699089825153351 + 50.0 * 8.400718688964844
Epoch 2460, val loss: 0.5143005847930908
Epoch 2470, training loss: 420.51971435546875 = 0.4689640998840332 + 50.0 * 8.401015281677246
Epoch 2470, val loss: 0.5135631561279297
Epoch 2480, training loss: 420.6884460449219 = 0.4680096209049225 + 50.0 * 8.40440845489502
Epoch 2480, val loss: 0.512640118598938
Epoch 2490, training loss: 420.53936767578125 = 0.46702176332473755 + 50.0 * 8.401447296142578
Epoch 2490, val loss: 0.512296199798584
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7853881278538812
0.8075056147214374
=== training gcn model ===
Epoch 0, training loss: 530.1943359375 = 1.0816649198532104 + 50.0 * 10.582253456115723
Epoch 0, val loss: 1.0810978412628174
Epoch 10, training loss: 530.1713256835938 = 1.0783449411392212 + 50.0 * 10.581859588623047
Epoch 10, val loss: 1.0778213739395142
Epoch 20, training loss: 530.0902709960938 = 1.0748084783554077 + 50.0 * 10.58030891418457
Epoch 20, val loss: 1.074364185333252
Epoch 30, training loss: 529.7520751953125 = 1.0711060762405396 + 50.0 * 10.57361888885498
Epoch 30, val loss: 1.0707961320877075
Epoch 40, training loss: 528.423095703125 = 1.0673061609268188 + 50.0 * 10.547115325927734
Epoch 40, val loss: 1.0671523809432983
Epoch 50, training loss: 524.1185302734375 = 1.0634765625 + 50.0 * 10.461100578308105
Epoch 50, val loss: 1.0635888576507568
Epoch 60, training loss: 512.5339965820312 = 1.0601451396942139 + 50.0 * 10.229476928710938
Epoch 60, val loss: 1.0605571269989014
Epoch 70, training loss: 493.0387268066406 = 1.0569350719451904 + 50.0 * 9.839635848999023
Epoch 70, val loss: 1.057629108428955
Epoch 80, training loss: 483.52691650390625 = 1.0541105270385742 + 50.0 * 9.649456024169922
Epoch 80, val loss: 1.0549827814102173
Epoch 90, training loss: 477.1568908691406 = 1.0519778728485107 + 50.0 * 9.522098541259766
Epoch 90, val loss: 1.052965521812439
Epoch 100, training loss: 468.5241394042969 = 1.0505331754684448 + 50.0 * 9.349472045898438
Epoch 100, val loss: 1.05158269405365
Epoch 110, training loss: 461.0740661621094 = 1.0491002798080444 + 50.0 * 9.200499534606934
Epoch 110, val loss: 1.050155758857727
Epoch 120, training loss: 456.8636779785156 = 1.0475280284881592 + 50.0 * 9.116323471069336
Epoch 120, val loss: 1.0486656427383423
Epoch 130, training loss: 453.02593994140625 = 1.046440839767456 + 50.0 * 9.039589881896973
Epoch 130, val loss: 1.0476758480072021
Epoch 140, training loss: 450.68145751953125 = 1.0459089279174805 + 50.0 * 8.992711067199707
Epoch 140, val loss: 1.0471562147140503
Epoch 150, training loss: 448.0328369140625 = 1.0453591346740723 + 50.0 * 8.939749717712402
Epoch 150, val loss: 1.0465854406356812
Epoch 160, training loss: 446.03076171875 = 1.0447465181350708 + 50.0 * 8.899720191955566
Epoch 160, val loss: 1.045922040939331
Epoch 170, training loss: 444.6712646484375 = 1.0440233945846558 + 50.0 * 8.872544288635254
Epoch 170, val loss: 1.0451345443725586
Epoch 180, training loss: 442.8453063964844 = 1.0432615280151367 + 50.0 * 8.836040496826172
Epoch 180, val loss: 1.044357419013977
Epoch 190, training loss: 440.8968505859375 = 1.0425878763198853 + 50.0 * 8.79708480834961
Epoch 190, val loss: 1.0436782836914062
Epoch 200, training loss: 439.2398681640625 = 1.0419269800186157 + 50.0 * 8.763958930969238
Epoch 200, val loss: 1.042992115020752
Epoch 210, training loss: 437.9078369140625 = 1.0411807298660278 + 50.0 * 8.737333297729492
Epoch 210, val loss: 1.042250156402588
Epoch 220, training loss: 436.6648254394531 = 1.0404701232910156 + 50.0 * 8.71248722076416
Epoch 220, val loss: 1.0415607690811157
Epoch 230, training loss: 435.645751953125 = 1.0398353338241577 + 50.0 * 8.692118644714355
Epoch 230, val loss: 1.0409245491027832
Epoch 240, training loss: 434.68359375 = 1.0391477346420288 + 50.0 * 8.67288875579834
Epoch 240, val loss: 1.0402374267578125
Epoch 250, training loss: 433.9627685546875 = 1.0383775234222412 + 50.0 * 8.658487319946289
Epoch 250, val loss: 1.03948175907135
Epoch 260, training loss: 433.370849609375 = 1.0375245809555054 + 50.0 * 8.646666526794434
Epoch 260, val loss: 1.03862464427948
Epoch 270, training loss: 432.8656921386719 = 1.0366100072860718 + 50.0 * 8.636581420898438
Epoch 270, val loss: 1.0377215147018433
Epoch 280, training loss: 432.4374694824219 = 1.0356565713882446 + 50.0 * 8.628036499023438
Epoch 280, val loss: 1.036779761314392
Epoch 290, training loss: 432.0458068847656 = 1.0346730947494507 + 50.0 * 8.620223045349121
Epoch 290, val loss: 1.0358173847198486
Epoch 300, training loss: 431.63702392578125 = 1.033664345741272 + 50.0 * 8.612067222595215
Epoch 300, val loss: 1.034826636314392
Epoch 310, training loss: 431.1964111328125 = 1.032674789428711 + 50.0 * 8.60327434539795
Epoch 310, val loss: 1.0338590145111084
Epoch 320, training loss: 430.8146667480469 = 1.0317004919052124 + 50.0 * 8.595659255981445
Epoch 320, val loss: 1.0329042673110962
Epoch 330, training loss: 430.341552734375 = 1.0307111740112305 + 50.0 * 8.586216926574707
Epoch 330, val loss: 1.0319395065307617
Epoch 340, training loss: 429.885009765625 = 1.0297269821166992 + 50.0 * 8.577105522155762
Epoch 340, val loss: 1.0309865474700928
Epoch 350, training loss: 429.4834899902344 = 1.028732180595398 + 50.0 * 8.569095611572266
Epoch 350, val loss: 1.0300205945968628
Epoch 360, training loss: 429.2872314453125 = 1.0276832580566406 + 50.0 * 8.565191268920898
Epoch 360, val loss: 1.0289894342422485
Epoch 370, training loss: 428.8742980957031 = 1.0265095233917236 + 50.0 * 8.556955337524414
Epoch 370, val loss: 1.0278418064117432
Epoch 380, training loss: 428.56231689453125 = 1.0253130197525024 + 50.0 * 8.550740242004395
Epoch 380, val loss: 1.0266889333724976
Epoch 390, training loss: 428.31927490234375 = 1.0240923166275024 + 50.0 * 8.545904159545898
Epoch 390, val loss: 1.0255001783370972
Epoch 400, training loss: 428.2753601074219 = 1.022795557975769 + 50.0 * 8.545051574707031
Epoch 400, val loss: 1.0242406129837036
Epoch 410, training loss: 427.9040832519531 = 1.021411418914795 + 50.0 * 8.537652969360352
Epoch 410, val loss: 1.022886037826538
Epoch 420, training loss: 427.7475280761719 = 1.019999623298645 + 50.0 * 8.534550666809082
Epoch 420, val loss: 1.0215072631835938
Epoch 430, training loss: 427.564208984375 = 1.0185397863388062 + 50.0 * 8.530913352966309
Epoch 430, val loss: 1.0200904607772827
Epoch 440, training loss: 427.4153137207031 = 1.0170292854309082 + 50.0 * 8.527965545654297
Epoch 440, val loss: 1.018624186515808
Epoch 450, training loss: 427.2923889160156 = 1.015464186668396 + 50.0 * 8.525538444519043
Epoch 450, val loss: 1.0171048641204834
Epoch 460, training loss: 427.196044921875 = 1.0138270854949951 + 50.0 * 8.52364444732666
Epoch 460, val loss: 1.0155119895935059
Epoch 470, training loss: 427.1240539550781 = 1.0121201276779175 + 50.0 * 8.522238731384277
Epoch 470, val loss: 1.0138556957244873
Epoch 480, training loss: 426.9743347167969 = 1.010353684425354 + 50.0 * 8.519279479980469
Epoch 480, val loss: 1.0121288299560547
Epoch 490, training loss: 426.8546447753906 = 1.008551836013794 + 50.0 * 8.516921997070312
Epoch 490, val loss: 1.010387659072876
Epoch 500, training loss: 426.7527160644531 = 1.0067179203033447 + 50.0 * 8.514920234680176
Epoch 500, val loss: 1.008606195449829
Epoch 510, training loss: 426.6411437988281 = 1.0048062801361084 + 50.0 * 8.512726783752441
Epoch 510, val loss: 1.006744384765625
Epoch 520, training loss: 426.56201171875 = 1.0028525590896606 + 50.0 * 8.511183738708496
Epoch 520, val loss: 1.0048620700836182
Epoch 530, training loss: 426.5240478515625 = 1.0008689165115356 + 50.0 * 8.51046371459961
Epoch 530, val loss: 1.0029394626617432
Epoch 540, training loss: 426.36724853515625 = 0.9988253116607666 + 50.0 * 8.507368087768555
Epoch 540, val loss: 1.0009465217590332
Epoch 550, training loss: 426.2554016113281 = 0.9967533946037292 + 50.0 * 8.505172729492188
Epoch 550, val loss: 0.9989564418792725
Epoch 560, training loss: 426.178955078125 = 0.9946588277816772 + 50.0 * 8.50368595123291
Epoch 560, val loss: 0.9969342350959778
Epoch 570, training loss: 426.0840759277344 = 0.992463231086731 + 50.0 * 8.501832008361816
Epoch 570, val loss: 0.9947966933250427
Epoch 580, training loss: 425.9893798828125 = 0.9902245402336121 + 50.0 * 8.499982833862305
Epoch 580, val loss: 0.9926468729972839
Epoch 590, training loss: 425.9093017578125 = 0.9879910349845886 + 50.0 * 8.49842643737793
Epoch 590, val loss: 0.9904998540878296
Epoch 600, training loss: 425.9761962890625 = 0.9856981635093689 + 50.0 * 8.499810218811035
Epoch 600, val loss: 0.9882979989051819
Epoch 610, training loss: 425.8465881347656 = 0.9832934737205505 + 50.0 * 8.497265815734863
Epoch 610, val loss: 0.9859586954116821
Epoch 620, training loss: 425.67095947265625 = 0.9808927774429321 + 50.0 * 8.49380111694336
Epoch 620, val loss: 0.9836580157279968
Epoch 630, training loss: 425.60101318359375 = 0.9784830212593079 + 50.0 * 8.492450714111328
Epoch 630, val loss: 0.9813503623008728
Epoch 640, training loss: 425.5950927734375 = 0.9760443568229675 + 50.0 * 8.49238109588623
Epoch 640, val loss: 0.9789929986000061
Epoch 650, training loss: 425.5283203125 = 0.9734569787979126 + 50.0 * 8.491097450256348
Epoch 650, val loss: 0.9765053987503052
Epoch 660, training loss: 425.40362548828125 = 0.9708859920501709 + 50.0 * 8.488655090332031
Epoch 660, val loss: 0.9740427732467651
Epoch 670, training loss: 425.3154296875 = 0.968312680721283 + 50.0 * 8.486942291259766
Epoch 670, val loss: 0.971576988697052
Epoch 680, training loss: 425.2401428222656 = 0.9657090306282043 + 50.0 * 8.485488891601562
Epoch 680, val loss: 0.9690757393836975
Epoch 690, training loss: 425.4049072265625 = 0.9630401134490967 + 50.0 * 8.488837242126465
Epoch 690, val loss: 0.9664990305900574
Epoch 700, training loss: 425.16241455078125 = 0.9601812362670898 + 50.0 * 8.484045028686523
Epoch 700, val loss: 0.9637558460235596
Epoch 710, training loss: 425.0803527832031 = 0.9574016332626343 + 50.0 * 8.48245906829834
Epoch 710, val loss: 0.9611071944236755
Epoch 720, training loss: 424.9873962402344 = 0.9546372890472412 + 50.0 * 8.4806547164917
Epoch 720, val loss: 0.9584620594978333
Epoch 730, training loss: 424.911376953125 = 0.9518346190452576 + 50.0 * 8.479190826416016
Epoch 730, val loss: 0.9557703733444214
Epoch 740, training loss: 425.0769958496094 = 0.948958158493042 + 50.0 * 8.482561111450195
Epoch 740, val loss: 0.9530186653137207
Epoch 750, training loss: 424.8427734375 = 0.9458411335945129 + 50.0 * 8.477938652038574
Epoch 750, val loss: 0.9499937295913696
Epoch 760, training loss: 424.7287902832031 = 0.9428188800811768 + 50.0 * 8.475719451904297
Epoch 760, val loss: 0.9471235871315002
Epoch 770, training loss: 424.6733093261719 = 0.9398572444915771 + 50.0 * 8.474669456481934
Epoch 770, val loss: 0.944292426109314
Epoch 780, training loss: 424.61212158203125 = 0.9368636012077332 + 50.0 * 8.473505020141602
Epoch 780, val loss: 0.9414172172546387
Epoch 790, training loss: 424.5523986816406 = 0.9338183999061584 + 50.0 * 8.472372055053711
Epoch 790, val loss: 0.938499391078949
Epoch 800, training loss: 424.5026550292969 = 0.9307188987731934 + 50.0 * 8.47143840789795
Epoch 800, val loss: 0.9355263113975525
Epoch 810, training loss: 424.62811279296875 = 0.927455723285675 + 50.0 * 8.474013328552246
Epoch 810, val loss: 0.9323488473892212
Epoch 820, training loss: 424.4778747558594 = 0.924029529094696 + 50.0 * 8.471076965332031
Epoch 820, val loss: 0.9290986061096191
Epoch 830, training loss: 424.3631896972656 = 0.9207829236984253 + 50.0 * 8.46884822845459
Epoch 830, val loss: 0.9260003566741943
Epoch 840, training loss: 424.2945251464844 = 0.9175270199775696 + 50.0 * 8.46753978729248
Epoch 840, val loss: 0.9228817224502563
Epoch 850, training loss: 424.2400817871094 = 0.9142403602600098 + 50.0 * 8.466516494750977
Epoch 850, val loss: 0.9197315573692322
Epoch 860, training loss: 424.1961669921875 = 0.9108989238739014 + 50.0 * 8.465705871582031
Epoch 860, val loss: 0.9165305495262146
Epoch 870, training loss: 424.3952331542969 = 0.9074520468711853 + 50.0 * 8.469756126403809
Epoch 870, val loss: 0.9132245779037476
Epoch 880, training loss: 424.126708984375 = 0.9037550687789917 + 50.0 * 8.464459419250488
Epoch 880, val loss: 0.909662127494812
Epoch 890, training loss: 424.0837707519531 = 0.9001771211624146 + 50.0 * 8.463671684265137
Epoch 890, val loss: 0.9062539935112
Epoch 900, training loss: 424.02728271484375 = 0.8966687917709351 + 50.0 * 8.46261215209961
Epoch 900, val loss: 0.9028941988945007
Epoch 910, training loss: 423.9817810058594 = 0.8931219577789307 + 50.0 * 8.461772918701172
Epoch 910, val loss: 0.8995006084442139
Epoch 920, training loss: 423.93756103515625 = 0.8895388841629028 + 50.0 * 8.460960388183594
Epoch 920, val loss: 0.8960689306259155
Epoch 930, training loss: 423.9000549316406 = 0.8858996629714966 + 50.0 * 8.460283279418945
Epoch 930, val loss: 0.8925840854644775
Epoch 940, training loss: 423.9919128417969 = 0.8821926116943359 + 50.0 * 8.462194442749023
Epoch 940, val loss: 0.8890292048454285
Epoch 950, training loss: 423.86517333984375 = 0.8782327771186829 + 50.0 * 8.459738731384277
Epoch 950, val loss: 0.8852443099021912
Epoch 960, training loss: 423.843505859375 = 0.8743934631347656 + 50.0 * 8.459382057189941
Epoch 960, val loss: 0.8815916180610657
Epoch 970, training loss: 423.7637634277344 = 0.8705771565437317 + 50.0 * 8.457863807678223
Epoch 970, val loss: 0.8779420256614685
Epoch 980, training loss: 423.70977783203125 = 0.866786539554596 + 50.0 * 8.456859588623047
Epoch 980, val loss: 0.8743042349815369
Epoch 990, training loss: 423.68145751953125 = 0.8629550933837891 + 50.0 * 8.45637035369873
Epoch 990, val loss: 0.8706344962120056
Epoch 1000, training loss: 423.790283203125 = 0.8590511679649353 + 50.0 * 8.458624839782715
Epoch 1000, val loss: 0.8668635487556458
Epoch 1010, training loss: 423.6624450683594 = 0.8549843430519104 + 50.0 * 8.456149101257324
Epoch 1010, val loss: 0.8630372881889343
Epoch 1020, training loss: 423.58056640625 = 0.8510075807571411 + 50.0 * 8.454590797424316
Epoch 1020, val loss: 0.8592717051506042
Epoch 1030, training loss: 423.55419921875 = 0.8470848202705383 + 50.0 * 8.454142570495605
Epoch 1030, val loss: 0.8554970622062683
Epoch 1040, training loss: 423.572265625 = 0.8431004285812378 + 50.0 * 8.454583168029785
Epoch 1040, val loss: 0.8517125248908997
Epoch 1050, training loss: 423.48809814453125 = 0.8389759659767151 + 50.0 * 8.452981948852539
Epoch 1050, val loss: 0.8477768301963806
Epoch 1060, training loss: 423.46575927734375 = 0.8349342942237854 + 50.0 * 8.452616691589355
Epoch 1060, val loss: 0.8439291715621948
Epoch 1070, training loss: 423.4570007324219 = 0.8308998346328735 + 50.0 * 8.452522277832031
Epoch 1070, val loss: 0.8401027917861938
Epoch 1080, training loss: 423.40972900390625 = 0.8268023729324341 + 50.0 * 8.451658248901367
Epoch 1080, val loss: 0.8361815214157104
Epoch 1090, training loss: 423.36944580078125 = 0.8227142095565796 + 50.0 * 8.450934410095215
Epoch 1090, val loss: 0.8323128819465637
Epoch 1100, training loss: 423.34576416015625 = 0.8186470866203308 + 50.0 * 8.450542449951172
Epoch 1100, val loss: 0.8284422755241394
Epoch 1110, training loss: 423.33380126953125 = 0.814541220664978 + 50.0 * 8.450385093688965
Epoch 1110, val loss: 0.8245357275009155
Epoch 1120, training loss: 423.35888671875 = 0.8103712201118469 + 50.0 * 8.450970649719238
Epoch 1120, val loss: 0.8205516934394836
Epoch 1130, training loss: 423.2364196777344 = 0.8061735033988953 + 50.0 * 8.448604583740234
Epoch 1130, val loss: 0.8165854215621948
Epoch 1140, training loss: 423.2015686035156 = 0.8020917773246765 + 50.0 * 8.447989463806152
Epoch 1140, val loss: 0.8127347826957703
Epoch 1150, training loss: 423.17425537109375 = 0.7980098128318787 + 50.0 * 8.447525024414062
Epoch 1150, val loss: 0.8088436722755432
Epoch 1160, training loss: 423.20489501953125 = 0.7938957810401917 + 50.0 * 8.448220252990723
Epoch 1160, val loss: 0.8049503564834595
Epoch 1170, training loss: 423.12744140625 = 0.7896740436553955 + 50.0 * 8.446755409240723
Epoch 1170, val loss: 0.8009464740753174
Epoch 1180, training loss: 423.138916015625 = 0.7854948043823242 + 50.0 * 8.447068214416504
Epoch 1180, val loss: 0.7969724535942078
Epoch 1190, training loss: 423.0752258300781 = 0.7813594937324524 + 50.0 * 8.445877075195312
Epoch 1190, val loss: 0.79306560754776
Epoch 1200, training loss: 423.0330505371094 = 0.7772523164749146 + 50.0 * 8.44511604309082
Epoch 1200, val loss: 0.7891746163368225
Epoch 1210, training loss: 423.03948974609375 = 0.7731488347053528 + 50.0 * 8.445326805114746
Epoch 1210, val loss: 0.7852892279624939
Epoch 1220, training loss: 423.0572204589844 = 0.7689502835273743 + 50.0 * 8.445765495300293
Epoch 1220, val loss: 0.7813341021537781
Epoch 1230, training loss: 422.9469299316406 = 0.7646501660346985 + 50.0 * 8.443645477294922
Epoch 1230, val loss: 0.7772535085678101
Epoch 1240, training loss: 422.9358215332031 = 0.7605094313621521 + 50.0 * 8.443506240844727
Epoch 1240, val loss: 0.773357093334198
Epoch 1250, training loss: 422.8857421875 = 0.7564505338668823 + 50.0 * 8.442585945129395
Epoch 1250, val loss: 0.7695185542106628
Epoch 1260, training loss: 422.8577575683594 = 0.7524047493934631 + 50.0 * 8.442107200622559
Epoch 1260, val loss: 0.7656857967376709
Epoch 1270, training loss: 422.9576110839844 = 0.7483022809028625 + 50.0 * 8.444186210632324
Epoch 1270, val loss: 0.761782169342041
Epoch 1280, training loss: 422.8443298339844 = 0.7441069483757019 + 50.0 * 8.442004203796387
Epoch 1280, val loss: 0.7578625082969666
Epoch 1290, training loss: 422.8737487792969 = 0.7400093674659729 + 50.0 * 8.44267463684082
Epoch 1290, val loss: 0.7539846897125244
Epoch 1300, training loss: 422.7515869140625 = 0.7357940673828125 + 50.0 * 8.440316200256348
Epoch 1300, val loss: 0.7499936819076538
Epoch 1310, training loss: 422.7228698730469 = 0.7317262887954712 + 50.0 * 8.439823150634766
Epoch 1310, val loss: 0.7462019920349121
Epoch 1320, training loss: 422.69903564453125 = 0.7277454733848572 + 50.0 * 8.439425468444824
Epoch 1320, val loss: 0.7424374222755432
Epoch 1330, training loss: 422.67193603515625 = 0.7237952351570129 + 50.0 * 8.438962936401367
Epoch 1330, val loss: 0.7387343049049377
Epoch 1340, training loss: 422.6871643066406 = 0.7198348045349121 + 50.0 * 8.439346313476562
Epoch 1340, val loss: 0.7350532412528992
Epoch 1350, training loss: 422.6499328613281 = 0.7156997919082642 + 50.0 * 8.438684463500977
Epoch 1350, val loss: 0.7310336232185364
Epoch 1360, training loss: 422.642822265625 = 0.7116305232048035 + 50.0 * 8.438623428344727
Epoch 1360, val loss: 0.7273329496383667
Epoch 1370, training loss: 422.5725402832031 = 0.7077130675315857 + 50.0 * 8.437296867370605
Epoch 1370, val loss: 0.7235920429229736
Epoch 1380, training loss: 422.55853271484375 = 0.7038472294807434 + 50.0 * 8.437093734741211
Epoch 1380, val loss: 0.719946563243866
Epoch 1390, training loss: 422.7781982421875 = 0.6999106407165527 + 50.0 * 8.44156551361084
Epoch 1390, val loss: 0.716229259967804
Epoch 1400, training loss: 422.57891845703125 = 0.6958770751953125 + 50.0 * 8.437661170959473
Epoch 1400, val loss: 0.7125095725059509
Epoch 1410, training loss: 422.4931945800781 = 0.6920448541641235 + 50.0 * 8.436022758483887
Epoch 1410, val loss: 0.7088866233825684
Epoch 1420, training loss: 422.4505310058594 = 0.6882827877998352 + 50.0 * 8.4352445602417
Epoch 1420, val loss: 0.7054114937782288
Epoch 1430, training loss: 422.4471740722656 = 0.6845612525939941 + 50.0 * 8.43525218963623
Epoch 1430, val loss: 0.7019140124320984
Epoch 1440, training loss: 422.5117492675781 = 0.68071448802948 + 50.0 * 8.436620712280273
Epoch 1440, val loss: 0.6983211040496826
Epoch 1450, training loss: 422.3843994140625 = 0.6768967509269714 + 50.0 * 8.434149742126465
Epoch 1450, val loss: 0.6947422027587891
Epoch 1460, training loss: 422.35089111328125 = 0.6732046604156494 + 50.0 * 8.433553695678711
Epoch 1460, val loss: 0.6913195252418518
Epoch 1470, training loss: 422.32537841796875 = 0.6695837378501892 + 50.0 * 8.43311595916748
Epoch 1470, val loss: 0.6879515647888184
Epoch 1480, training loss: 422.47930908203125 = 0.6659506559371948 + 50.0 * 8.436266899108887
Epoch 1480, val loss: 0.6845624446868896
Epoch 1490, training loss: 422.430908203125 = 0.6620948910713196 + 50.0 * 8.435376167297363
Epoch 1490, val loss: 0.6809291839599609
Epoch 1500, training loss: 422.263916015625 = 0.6584270596504211 + 50.0 * 8.432109832763672
Epoch 1500, val loss: 0.6775690913200378
Epoch 1510, training loss: 422.2459411621094 = 0.654928982257843 + 50.0 * 8.431819915771484
Epoch 1510, val loss: 0.6743497848510742
Epoch 1520, training loss: 422.2062683105469 = 0.6515258550643921 + 50.0 * 8.431095123291016
Epoch 1520, val loss: 0.6712095737457275
Epoch 1530, training loss: 422.1830139160156 = 0.6481356620788574 + 50.0 * 8.430697441101074
Epoch 1530, val loss: 0.66806960105896
Epoch 1540, training loss: 422.1766357421875 = 0.6447606682777405 + 50.0 * 8.43063735961914
Epoch 1540, val loss: 0.6649768948554993
Epoch 1550, training loss: 422.406494140625 = 0.6413101553916931 + 50.0 * 8.435303688049316
Epoch 1550, val loss: 0.6617894172668457
Epoch 1560, training loss: 422.1965026855469 = 0.6377891898155212 + 50.0 * 8.431174278259277
Epoch 1560, val loss: 0.6585079431533813
Epoch 1570, training loss: 422.1068115234375 = 0.6344642043113708 + 50.0 * 8.429447174072266
Epoch 1570, val loss: 0.6554734706878662
Epoch 1580, training loss: 422.0851745605469 = 0.6312324404716492 + 50.0 * 8.429079055786133
Epoch 1580, val loss: 0.6525385975837708
Epoch 1590, training loss: 422.1282653808594 = 0.6280328631401062 + 50.0 * 8.430005073547363
Epoch 1590, val loss: 0.6495984196662903
Epoch 1600, training loss: 422.03717041015625 = 0.624796986579895 + 50.0 * 8.428247451782227
Epoch 1600, val loss: 0.6466564536094666
Epoch 1610, training loss: 422.020263671875 = 0.6216615438461304 + 50.0 * 8.427971839904785
Epoch 1610, val loss: 0.6437650322914124
Epoch 1620, training loss: 422.0646057128906 = 0.6185736656188965 + 50.0 * 8.42892074584961
Epoch 1620, val loss: 0.6409216523170471
Epoch 1630, training loss: 421.99072265625 = 0.6154389977455139 + 50.0 * 8.427505493164062
Epoch 1630, val loss: 0.6381625533103943
Epoch 1640, training loss: 422.0509033203125 = 0.6123644113540649 + 50.0 * 8.428771018981934
Epoch 1640, val loss: 0.6354057788848877
Epoch 1650, training loss: 421.926513671875 = 0.6093102097511292 + 50.0 * 8.42634391784668
Epoch 1650, val loss: 0.6325318217277527
Epoch 1660, training loss: 421.91619873046875 = 0.6063746809959412 + 50.0 * 8.426196098327637
Epoch 1660, val loss: 0.6298967599868774
Epoch 1670, training loss: 421.9109802246094 = 0.6035060286521912 + 50.0 * 8.426149368286133
Epoch 1670, val loss: 0.6273019313812256
Epoch 1680, training loss: 421.96343994140625 = 0.6006108522415161 + 50.0 * 8.42725658416748
Epoch 1680, val loss: 0.6247010231018066
Epoch 1690, training loss: 421.87152099609375 = 0.5977173447608948 + 50.0 * 8.42547607421875
Epoch 1690, val loss: 0.622049868106842
Epoch 1700, training loss: 421.8314208984375 = 0.5949106216430664 + 50.0 * 8.42473030090332
Epoch 1700, val loss: 0.6195957660675049
Epoch 1710, training loss: 421.8209228515625 = 0.5921908020973206 + 50.0 * 8.424574851989746
Epoch 1710, val loss: 0.6171886920928955
Epoch 1720, training loss: 421.9320983886719 = 0.5894852876663208 + 50.0 * 8.426852226257324
Epoch 1720, val loss: 0.6146528720855713
Epoch 1730, training loss: 421.9288635253906 = 0.5866575241088867 + 50.0 * 8.426843643188477
Epoch 1730, val loss: 0.6121647357940674
Epoch 1740, training loss: 421.76983642578125 = 0.5839585065841675 + 50.0 * 8.423717498779297
Epoch 1740, val loss: 0.6098330020904541
Epoch 1750, training loss: 421.742431640625 = 0.5813857316970825 + 50.0 * 8.42322063446045
Epoch 1750, val loss: 0.607498288154602
Epoch 1760, training loss: 421.7149963378906 = 0.5788754224777222 + 50.0 * 8.422721862792969
Epoch 1760, val loss: 0.6053523421287537
Epoch 1770, training loss: 421.7265930175781 = 0.576398491859436 + 50.0 * 8.423004150390625
Epoch 1770, val loss: 0.6031467318534851
Epoch 1780, training loss: 421.8126220703125 = 0.5738554000854492 + 50.0 * 8.424775123596191
Epoch 1780, val loss: 0.6008896827697754
Epoch 1790, training loss: 421.7241516113281 = 0.5713114738464355 + 50.0 * 8.423056602478027
Epoch 1790, val loss: 0.5986403226852417
Epoch 1800, training loss: 421.64764404296875 = 0.5688779950141907 + 50.0 * 8.421575546264648
Epoch 1800, val loss: 0.5964459776878357
Epoch 1810, training loss: 421.62213134765625 = 0.5665409564971924 + 50.0 * 8.421112060546875
Epoch 1810, val loss: 0.5944448709487915
Epoch 1820, training loss: 421.6197509765625 = 0.5642449855804443 + 50.0 * 8.421110153198242
Epoch 1820, val loss: 0.5924230217933655
Epoch 1830, training loss: 421.8933410644531 = 0.5619145035743713 + 50.0 * 8.426628112792969
Epoch 1830, val loss: 0.5903334021568298
Epoch 1840, training loss: 421.6782531738281 = 0.5595059990882874 + 50.0 * 8.422374725341797
Epoch 1840, val loss: 0.5882871747016907
Epoch 1850, training loss: 421.5939636230469 = 0.5572533011436462 + 50.0 * 8.420734405517578
Epoch 1850, val loss: 0.586337149143219
Epoch 1860, training loss: 421.5524597167969 = 0.5550996661186218 + 50.0 * 8.419946670532227
Epoch 1860, val loss: 0.5845019221305847
Epoch 1870, training loss: 421.5343933105469 = 0.5529667139053345 + 50.0 * 8.419628143310547
Epoch 1870, val loss: 0.582706093788147
Epoch 1880, training loss: 421.67620849609375 = 0.550839364528656 + 50.0 * 8.422507286071777
Epoch 1880, val loss: 0.5810188055038452
Epoch 1890, training loss: 421.571533203125 = 0.5486515760421753 + 50.0 * 8.42045783996582
Epoch 1890, val loss: 0.578870415687561
Epoch 1900, training loss: 421.5754699707031 = 0.5464645624160767 + 50.0 * 8.42057991027832
Epoch 1900, val loss: 0.5770689845085144
Epoch 1910, training loss: 421.4878234863281 = 0.5443729162216187 + 50.0 * 8.418869018554688
Epoch 1910, val loss: 0.5752971768379211
Epoch 1920, training loss: 421.4620361328125 = 0.5423926115036011 + 50.0 * 8.4183931350708
Epoch 1920, val loss: 0.5736083388328552
Epoch 1930, training loss: 421.44281005859375 = 0.5404620170593262 + 50.0 * 8.418046951293945
Epoch 1930, val loss: 0.571968138217926
Epoch 1940, training loss: 421.4263916015625 = 0.538552463054657 + 50.0 * 8.417757034301758
Epoch 1940, val loss: 0.5704024434089661
Epoch 1950, training loss: 421.4108581542969 = 0.5366644859313965 + 50.0 * 8.417484283447266
Epoch 1950, val loss: 0.5687843561172485
Epoch 1960, training loss: 421.39727783203125 = 0.5347853302955627 + 50.0 * 8.41724967956543
Epoch 1960, val loss: 0.5672346353530884
Epoch 1970, training loss: 421.39447021484375 = 0.532924234867096 + 50.0 * 8.417230606079102
Epoch 1970, val loss: 0.5656973719596863
Epoch 1980, training loss: 422.1212463378906 = 0.5309669971466064 + 50.0 * 8.431805610656738
Epoch 1980, val loss: 0.5641270875930786
Epoch 1990, training loss: 421.3951721191406 = 0.5288428068161011 + 50.0 * 8.417326927185059
Epoch 1990, val loss: 0.562291145324707
Epoch 2000, training loss: 421.3606262207031 = 0.5269927382469177 + 50.0 * 8.416672706604004
Epoch 2000, val loss: 0.5607095956802368
Epoch 2010, training loss: 421.3603515625 = 0.5253048539161682 + 50.0 * 8.416701316833496
Epoch 2010, val loss: 0.5593189597129822
Epoch 2020, training loss: 421.3296203613281 = 0.5236485004425049 + 50.0 * 8.416119575500488
Epoch 2020, val loss: 0.5579830408096313
Epoch 2030, training loss: 421.3095397949219 = 0.5220150947570801 + 50.0 * 8.415750503540039
Epoch 2030, val loss: 0.5566860437393188
Epoch 2040, training loss: 421.3018493652344 = 0.5203908085823059 + 50.0 * 8.415629386901855
Epoch 2040, val loss: 0.555395245552063
Epoch 2050, training loss: 421.48046875 = 0.5187532901763916 + 50.0 * 8.419234275817871
Epoch 2050, val loss: 0.5541167259216309
Epoch 2060, training loss: 421.3782043457031 = 0.5170468091964722 + 50.0 * 8.41722297668457
Epoch 2060, val loss: 0.5526217818260193
Epoch 2070, training loss: 421.2760925292969 = 0.5154057145118713 + 50.0 * 8.415213584899902
Epoch 2070, val loss: 0.5513584613800049
Epoch 2080, training loss: 421.25604248046875 = 0.5138514637947083 + 50.0 * 8.414843559265137
Epoch 2080, val loss: 0.5501675605773926
Epoch 2090, training loss: 421.2434997558594 = 0.5123277902603149 + 50.0 * 8.414623260498047
Epoch 2090, val loss: 0.5489528775215149
Epoch 2100, training loss: 421.24615478515625 = 0.5108169317245483 + 50.0 * 8.41470718383789
Epoch 2100, val loss: 0.5478334426879883
Epoch 2110, training loss: 421.4140625 = 0.5092805027961731 + 50.0 * 8.418095588684082
Epoch 2110, val loss: 0.5467100739479065
Epoch 2120, training loss: 421.270263671875 = 0.5077142715454102 + 50.0 * 8.415250778198242
Epoch 2120, val loss: 0.5452854633331299
Epoch 2130, training loss: 421.2008056640625 = 0.5062228441238403 + 50.0 * 8.413891792297363
Epoch 2130, val loss: 0.5441182851791382
Epoch 2140, training loss: 421.1993408203125 = 0.5047997832298279 + 50.0 * 8.413890838623047
Epoch 2140, val loss: 0.5430827736854553
Epoch 2150, training loss: 421.1845397949219 = 0.5033981800079346 + 50.0 * 8.413622856140137
Epoch 2150, val loss: 0.5419720411300659
Epoch 2160, training loss: 421.2034606933594 = 0.5020085573196411 + 50.0 * 8.414029121398926
Epoch 2160, val loss: 0.5409160256385803
Epoch 2170, training loss: 421.20458984375 = 0.5006071925163269 + 50.0 * 8.414079666137695
Epoch 2170, val loss: 0.5398246049880981
Epoch 2180, training loss: 421.1718444824219 = 0.4992295205593109 + 50.0 * 8.4134521484375
Epoch 2180, val loss: 0.5387980937957764
Epoch 2190, training loss: 421.18994140625 = 0.4978770911693573 + 50.0 * 8.413841247558594
Epoch 2190, val loss: 0.5377824306488037
Epoch 2200, training loss: 421.17462158203125 = 0.49652209877967834 + 50.0 * 8.413561820983887
Epoch 2200, val loss: 0.5367251634597778
Epoch 2210, training loss: 421.1379089355469 = 0.4952138066291809 + 50.0 * 8.412854194641113
Epoch 2210, val loss: 0.5357123613357544
Epoch 2220, training loss: 421.0967102050781 = 0.4939340651035309 + 50.0 * 8.412055969238281
Epoch 2220, val loss: 0.5348288416862488
Epoch 2230, training loss: 421.1144104003906 = 0.4926827549934387 + 50.0 * 8.412434577941895
Epoch 2230, val loss: 0.5338783860206604
Epoch 2240, training loss: 421.34130859375 = 0.491417795419693 + 50.0 * 8.416997909545898
Epoch 2240, val loss: 0.5328041911125183
Epoch 2250, training loss: 421.1629333496094 = 0.4900875389575958 + 50.0 * 8.413456916809082
Epoch 2250, val loss: 0.53209388256073
Epoch 2260, training loss: 421.0924377441406 = 0.48886024951934814 + 50.0 * 8.412071228027344
Epoch 2260, val loss: 0.5310395956039429
Epoch 2270, training loss: 421.05767822265625 = 0.48768171668052673 + 50.0 * 8.411399841308594
Epoch 2270, val loss: 0.5303068161010742
Epoch 2280, training loss: 421.04827880859375 = 0.4865328073501587 + 50.0 * 8.411234855651855
Epoch 2280, val loss: 0.5294362902641296
Epoch 2290, training loss: 421.1824951171875 = 0.48537150025367737 + 50.0 * 8.413942337036133
Epoch 2290, val loss: 0.5286293625831604
Epoch 2300, training loss: 421.047119140625 = 0.4841808080673218 + 50.0 * 8.411258697509766
Epoch 2300, val loss: 0.5278019309043884
Epoch 2310, training loss: 421.01025390625 = 0.4830438792705536 + 50.0 * 8.410544395446777
Epoch 2310, val loss: 0.5269902944564819
Epoch 2320, training loss: 421.000244140625 = 0.48194581270217896 + 50.0 * 8.41036605834961
Epoch 2320, val loss: 0.526274561882019
Epoch 2330, training loss: 421.0851135253906 = 0.480849951505661 + 50.0 * 8.41208553314209
Epoch 2330, val loss: 0.5255089402198792
Epoch 2340, training loss: 421.0333251953125 = 0.47972437739372253 + 50.0 * 8.41107177734375
Epoch 2340, val loss: 0.5246832966804504
Epoch 2350, training loss: 420.99652099609375 = 0.47862640023231506 + 50.0 * 8.410357475280762
Epoch 2350, val loss: 0.5240998268127441
Epoch 2360, training loss: 420.9674377441406 = 0.47757336497306824 + 50.0 * 8.409797668457031
Epoch 2360, val loss: 0.5232948660850525
Epoch 2370, training loss: 420.94671630859375 = 0.47655239701271057 + 50.0 * 8.409402847290039
Epoch 2370, val loss: 0.5226011276245117
Epoch 2380, training loss: 420.93682861328125 = 0.47554516792297363 + 50.0 * 8.409225463867188
Epoch 2380, val loss: 0.5219207406044006
Epoch 2390, training loss: 420.9883117675781 = 0.47454938292503357 + 50.0 * 8.41027545928955
Epoch 2390, val loss: 0.5212701559066772
Epoch 2400, training loss: 421.00506591796875 = 0.4735018312931061 + 50.0 * 8.41063117980957
Epoch 2400, val loss: 0.5205823183059692
Epoch 2410, training loss: 420.95367431640625 = 0.4724869132041931 + 50.0 * 8.409624099731445
Epoch 2410, val loss: 0.5198825597763062
Epoch 2420, training loss: 420.9131164550781 = 0.4715139865875244 + 50.0 * 8.408831596374512
Epoch 2420, val loss: 0.519359827041626
Epoch 2430, training loss: 420.91912841796875 = 0.47056490182876587 + 50.0 * 8.408971786499023
Epoch 2430, val loss: 0.5187915563583374
Epoch 2440, training loss: 421.06878662109375 = 0.46959468722343445 + 50.0 * 8.411983489990234
Epoch 2440, val loss: 0.5182568430900574
Epoch 2450, training loss: 420.9212341308594 = 0.468600869178772 + 50.0 * 8.409052848815918
Epoch 2450, val loss: 0.5174419283866882
Epoch 2460, training loss: 420.86480712890625 = 0.4676668345928192 + 50.0 * 8.407942771911621
Epoch 2460, val loss: 0.5168073177337646
Epoch 2470, training loss: 420.85345458984375 = 0.46677523851394653 + 50.0 * 8.407733917236328
Epoch 2470, val loss: 0.5163429379463196
Epoch 2480, training loss: 420.8470764160156 = 0.46590346097946167 + 50.0 * 8.407623291015625
Epoch 2480, val loss: 0.515723466873169
Epoch 2490, training loss: 420.98089599609375 = 0.4650259017944336 + 50.0 * 8.410317420959473
Epoch 2490, val loss: 0.5151397585868835
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7879249112125823
0.8108382235745853
The final CL Acc:0.78387, 0.00408, The final GNN Acc:0.80941, 0.00140
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110740])
remove edge: torch.Size([2, 66490])
updated graph: torch.Size([2, 88582])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 530.2332153320312 = 1.11970853805542 + 50.0 * 10.582270622253418
Epoch 0, val loss: 1.1189444065093994
Epoch 10, training loss: 530.2088623046875 = 1.1146477460861206 + 50.0 * 10.58188533782959
Epoch 10, val loss: 1.1138767004013062
Epoch 20, training loss: 530.1152954101562 = 1.1090525388717651 + 50.0 * 10.580123901367188
Epoch 20, val loss: 1.1082452535629272
Epoch 30, training loss: 529.6956787109375 = 1.1025524139404297 + 50.0 * 10.571863174438477
Epoch 30, val loss: 1.1016584634780884
Epoch 40, training loss: 528.1494140625 = 1.0948517322540283 + 50.0 * 10.541090965270996
Epoch 40, val loss: 1.0938833951950073
Epoch 50, training loss: 524.2535400390625 = 1.0863162279129028 + 50.0 * 10.46334457397461
Epoch 50, val loss: 1.0853345394134521
Epoch 60, training loss: 516.3185424804688 = 1.078158974647522 + 50.0 * 10.30480670928955
Epoch 60, val loss: 1.0772769451141357
Epoch 70, training loss: 502.5600280761719 = 1.0698562860488892 + 50.0 * 10.029803276062012
Epoch 70, val loss: 1.0688610076904297
Epoch 80, training loss: 486.8879089355469 = 1.0613124370574951 + 50.0 * 9.716531753540039
Epoch 80, val loss: 1.0603379011154175
Epoch 90, training loss: 477.71337890625 = 1.054850459098816 + 50.0 * 9.533170700073242
Epoch 90, val loss: 1.0542616844177246
Epoch 100, training loss: 470.63616943359375 = 1.0496540069580078 + 50.0 * 9.391730308532715
Epoch 100, val loss: 1.0492854118347168
Epoch 110, training loss: 463.720947265625 = 1.046060562133789 + 50.0 * 9.253498077392578
Epoch 110, val loss: 1.0458966493606567
Epoch 120, training loss: 458.9925842285156 = 1.044293761253357 + 50.0 * 9.158966064453125
Epoch 120, val loss: 1.0442843437194824
Epoch 130, training loss: 455.0528564453125 = 1.0434716939926147 + 50.0 * 9.080187797546387
Epoch 130, val loss: 1.0435165166854858
Epoch 140, training loss: 451.8367004394531 = 1.0426324605941772 + 50.0 * 9.015881538391113
Epoch 140, val loss: 1.0426496267318726
Epoch 150, training loss: 449.9046325683594 = 1.0409836769104004 + 50.0 * 8.977272987365723
Epoch 150, val loss: 1.0410419702529907
Epoch 160, training loss: 448.5242919921875 = 1.0394139289855957 + 50.0 * 8.949697494506836
Epoch 160, val loss: 1.0395963191986084
Epoch 170, training loss: 446.5924377441406 = 1.0385617017745972 + 50.0 * 8.911077499389648
Epoch 170, val loss: 1.0388997793197632
Epoch 180, training loss: 443.8077697753906 = 1.0384899377822876 + 50.0 * 8.855385780334473
Epoch 180, val loss: 1.0389528274536133
Epoch 190, training loss: 440.4607849121094 = 1.0387868881225586 + 50.0 * 8.788439750671387
Epoch 190, val loss: 1.0393078327178955
Epoch 200, training loss: 437.7794189453125 = 1.038926601409912 + 50.0 * 8.734809875488281
Epoch 200, val loss: 1.0394290685653687
Epoch 210, training loss: 436.11822509765625 = 1.0385288000106812 + 50.0 * 8.701593399047852
Epoch 210, val loss: 1.038999080657959
Epoch 220, training loss: 434.7734375 = 1.0378749370574951 + 50.0 * 8.674711227416992
Epoch 220, val loss: 1.0383497476577759
Epoch 230, training loss: 433.50335693359375 = 1.037381649017334 + 50.0 * 8.649319648742676
Epoch 230, val loss: 1.0378633737564087
Epoch 240, training loss: 432.487060546875 = 1.0369751453399658 + 50.0 * 8.62900161743164
Epoch 240, val loss: 1.037432312965393
Epoch 250, training loss: 431.5498962402344 = 1.0364658832550049 + 50.0 * 8.610268592834473
Epoch 250, val loss: 1.0369023084640503
Epoch 260, training loss: 430.79571533203125 = 1.0358749628067017 + 50.0 * 8.595196723937988
Epoch 260, val loss: 1.0363188982009888
Epoch 270, training loss: 430.15771484375 = 1.035213589668274 + 50.0 * 8.582449913024902
Epoch 270, val loss: 1.0356860160827637
Epoch 280, training loss: 429.69482421875 = 1.0344998836517334 + 50.0 * 8.573206901550293
Epoch 280, val loss: 1.035005807876587
Epoch 290, training loss: 429.1018981933594 = 1.0337780714035034 + 50.0 * 8.561362266540527
Epoch 290, val loss: 1.0343226194381714
Epoch 300, training loss: 428.5997009277344 = 1.0331108570098877 + 50.0 * 8.551331520080566
Epoch 300, val loss: 1.0336816310882568
Epoch 310, training loss: 428.158935546875 = 1.0324305295944214 + 50.0 * 8.542530059814453
Epoch 310, val loss: 1.0330225229263306
Epoch 320, training loss: 427.83087158203125 = 1.0316818952560425 + 50.0 * 8.53598403930664
Epoch 320, val loss: 1.0322914123535156
Epoch 330, training loss: 427.3802795410156 = 1.030885100364685 + 50.0 * 8.52698802947998
Epoch 330, val loss: 1.0315382480621338
Epoch 340, training loss: 427.0369567871094 = 1.030124545097351 + 50.0 * 8.520136833190918
Epoch 340, val loss: 1.0308222770690918
Epoch 350, training loss: 426.7389221191406 = 1.0293253660202026 + 50.0 * 8.514191627502441
Epoch 350, val loss: 1.0300602912902832
Epoch 360, training loss: 426.39764404296875 = 1.0284717082977295 + 50.0 * 8.507383346557617
Epoch 360, val loss: 1.0292361974716187
Epoch 370, training loss: 426.03643798828125 = 1.0276323556900024 + 50.0 * 8.500176429748535
Epoch 370, val loss: 1.0284405946731567
Epoch 380, training loss: 425.70135498046875 = 1.0267924070358276 + 50.0 * 8.493491172790527
Epoch 380, val loss: 1.027631163597107
Epoch 390, training loss: 425.4496154785156 = 1.0258797407150269 + 50.0 * 8.48847484588623
Epoch 390, val loss: 1.0267456769943237
Epoch 400, training loss: 425.1637268066406 = 1.0248887538909912 + 50.0 * 8.482776641845703
Epoch 400, val loss: 1.0258067846298218
Epoch 410, training loss: 424.9309997558594 = 1.0238770246505737 + 50.0 * 8.478142738342285
Epoch 410, val loss: 1.0248292684555054
Epoch 420, training loss: 424.7276611328125 = 1.022811770439148 + 50.0 * 8.47409725189209
Epoch 420, val loss: 1.0237948894500732
Epoch 430, training loss: 424.5898132324219 = 1.0216857194900513 + 50.0 * 8.471362113952637
Epoch 430, val loss: 1.0227071046829224
Epoch 440, training loss: 424.46453857421875 = 1.0204609632492065 + 50.0 * 8.468881607055664
Epoch 440, val loss: 1.0215102434158325
Epoch 450, training loss: 424.2599792480469 = 1.019237995147705 + 50.0 * 8.464815139770508
Epoch 450, val loss: 1.0203393697738647
Epoch 460, training loss: 424.1003112792969 = 1.0180001258850098 + 50.0 * 8.46164608001709
Epoch 460, val loss: 1.01914381980896
Epoch 470, training loss: 423.96875 = 1.0167359113693237 + 50.0 * 8.459040641784668
Epoch 470, val loss: 1.017914056777954
Epoch 480, training loss: 423.9591369628906 = 1.015359878540039 + 50.0 * 8.45887565612793
Epoch 480, val loss: 1.0165989398956299
Epoch 490, training loss: 423.69305419921875 = 1.0140142440795898 + 50.0 * 8.453580856323242
Epoch 490, val loss: 1.01528799533844
Epoch 500, training loss: 423.5388488769531 = 1.0126760005950928 + 50.0 * 8.450523376464844
Epoch 500, val loss: 1.0139976739883423
Epoch 510, training loss: 423.3999938964844 = 1.011304259300232 + 50.0 * 8.447773933410645
Epoch 510, val loss: 1.0126879215240479
Epoch 520, training loss: 423.2745666503906 = 1.0098985433578491 + 50.0 * 8.445293426513672
Epoch 520, val loss: 1.0113269090652466
Epoch 530, training loss: 423.2002258300781 = 1.0084354877471924 + 50.0 * 8.443836212158203
Epoch 530, val loss: 1.0099108219146729
Epoch 540, training loss: 423.13873291015625 = 1.0068632364273071 + 50.0 * 8.44263744354248
Epoch 540, val loss: 1.008387804031372
Epoch 550, training loss: 422.98565673828125 = 1.0052958726882935 + 50.0 * 8.439606666564941
Epoch 550, val loss: 1.0068817138671875
Epoch 560, training loss: 422.8711853027344 = 1.0037107467651367 + 50.0 * 8.437349319458008
Epoch 560, val loss: 1.0053495168685913
Epoch 570, training loss: 422.7724609375 = 1.0020675659179688 + 50.0 * 8.435407638549805
Epoch 570, val loss: 1.0037641525268555
Epoch 580, training loss: 422.7148132324219 = 1.000391960144043 + 50.0 * 8.434288024902344
Epoch 580, val loss: 1.002143383026123
Epoch 590, training loss: 422.7454833984375 = 0.9985620379447937 + 50.0 * 8.434938430786133
Epoch 590, val loss: 1.000373125076294
Epoch 600, training loss: 422.5274963378906 = 0.996733546257019 + 50.0 * 8.430615425109863
Epoch 600, val loss: 0.9986177086830139
Epoch 610, training loss: 422.45428466796875 = 0.9949252605438232 + 50.0 * 8.429186820983887
Epoch 610, val loss: 0.9968652725219727
Epoch 620, training loss: 422.3647766113281 = 0.9930784106254578 + 50.0 * 8.427433967590332
Epoch 620, val loss: 0.9950792193412781
Epoch 630, training loss: 422.2847900390625 = 0.9911591410636902 + 50.0 * 8.425872802734375
Epoch 630, val loss: 0.9932230114936829
Epoch 640, training loss: 422.49517822265625 = 0.9891742467880249 + 50.0 * 8.430120468139648
Epoch 640, val loss: 0.9912795424461365
Epoch 650, training loss: 422.1976013183594 = 0.9869738817214966 + 50.0 * 8.424212455749512
Epoch 650, val loss: 0.9891546964645386
Epoch 660, training loss: 422.1161193847656 = 0.9848824143409729 + 50.0 * 8.422624588012695
Epoch 660, val loss: 0.9871383905410767
Epoch 670, training loss: 422.01458740234375 = 0.9827801585197449 + 50.0 * 8.420636177062988
Epoch 670, val loss: 0.9851099252700806
Epoch 680, training loss: 421.9417724609375 = 0.9806116223335266 + 50.0 * 8.419222831726074
Epoch 680, val loss: 0.9830054640769958
Epoch 690, training loss: 421.87744140625 = 0.9783867001533508 + 50.0 * 8.417981147766113
Epoch 690, val loss: 0.9808477163314819
Epoch 700, training loss: 421.9006042480469 = 0.9761233925819397 + 50.0 * 8.418489456176758
Epoch 700, val loss: 0.9786417484283447
Epoch 710, training loss: 421.796875 = 0.9737194776535034 + 50.0 * 8.416462898254395
Epoch 710, val loss: 0.9763284921646118
Epoch 720, training loss: 421.69024658203125 = 0.9713741540908813 + 50.0 * 8.414377212524414
Epoch 720, val loss: 0.9740474820137024
Epoch 730, training loss: 421.6063232421875 = 0.9689964652061462 + 50.0 * 8.41274642944336
Epoch 730, val loss: 0.9717415571212769
Epoch 740, training loss: 421.53729248046875 = 0.9665745496749878 + 50.0 * 8.41141414642334
Epoch 740, val loss: 0.9693940877914429
Epoch 750, training loss: 421.4917907714844 = 0.9641324281692505 + 50.0 * 8.410552978515625
Epoch 750, val loss: 0.9670307636260986
Epoch 760, training loss: 421.427001953125 = 0.9615342617034912 + 50.0 * 8.409309387207031
Epoch 760, val loss: 0.9644847512245178
Epoch 770, training loss: 421.34075927734375 = 0.958916187286377 + 50.0 * 8.407636642456055
Epoch 770, val loss: 0.9619722962379456
Epoch 780, training loss: 421.26580810546875 = 0.9563627243041992 + 50.0 * 8.40618896484375
Epoch 780, val loss: 0.9594925045967102
Epoch 790, training loss: 421.2472229003906 = 0.95374596118927 + 50.0 * 8.405869483947754
Epoch 790, val loss: 0.9569597244262695
Epoch 800, training loss: 421.21282958984375 = 0.9509469270706177 + 50.0 * 8.405237197875977
Epoch 800, val loss: 0.954224169254303
Epoch 810, training loss: 421.0734558105469 = 0.9481518864631653 + 50.0 * 8.402505874633789
Epoch 810, val loss: 0.9515408277511597
Epoch 820, training loss: 421.0198059082031 = 0.9454018473625183 + 50.0 * 8.401488304138184
Epoch 820, val loss: 0.9488858580589294
Epoch 830, training loss: 420.96588134765625 = 0.9426271915435791 + 50.0 * 8.40046501159668
Epoch 830, val loss: 0.9462047219276428
Epoch 840, training loss: 420.9828796386719 = 0.9397725462913513 + 50.0 * 8.400862693786621
Epoch 840, val loss: 0.943434476852417
Epoch 850, training loss: 420.8618469238281 = 0.9368278384208679 + 50.0 * 8.398500442504883
Epoch 850, val loss: 0.9405927658081055
Epoch 860, training loss: 420.78759765625 = 0.9338900446891785 + 50.0 * 8.397073745727539
Epoch 860, val loss: 0.9377486705780029
Epoch 870, training loss: 420.73956298828125 = 0.9308913946151733 + 50.0 * 8.396173477172852
Epoch 870, val loss: 0.9348604083061218
Epoch 880, training loss: 420.86541748046875 = 0.9278050661087036 + 50.0 * 8.398752212524414
Epoch 880, val loss: 0.9318606853485107
Epoch 890, training loss: 420.67022705078125 = 0.9245777130126953 + 50.0 * 8.394912719726562
Epoch 890, val loss: 0.9287490248680115
Epoch 900, training loss: 420.60693359375 = 0.9214290976524353 + 50.0 * 8.393710136413574
Epoch 900, val loss: 0.9257062673568726
Epoch 910, training loss: 420.55810546875 = 0.9182335138320923 + 50.0 * 8.392797470092773
Epoch 910, val loss: 0.9226181507110596
Epoch 920, training loss: 420.5608215332031 = 0.9149688482284546 + 50.0 * 8.392916679382324
Epoch 920, val loss: 0.919440507888794
Epoch 930, training loss: 420.4866943359375 = 0.9114550948143005 + 50.0 * 8.391504287719727
Epoch 930, val loss: 0.9160721898078918
Epoch 940, training loss: 420.4893798828125 = 0.9080380797386169 + 50.0 * 8.391626358032227
Epoch 940, val loss: 0.9127526879310608
Epoch 950, training loss: 420.4078063964844 = 0.9046252369880676 + 50.0 * 8.390063285827637
Epoch 950, val loss: 0.9094708561897278
Epoch 960, training loss: 420.3765869140625 = 0.901183545589447 + 50.0 * 8.389508247375488
Epoch 960, val loss: 0.9061610102653503
Epoch 970, training loss: 420.3410339355469 = 0.8977017402648926 + 50.0 * 8.388866424560547
Epoch 970, val loss: 0.9027941823005676
Epoch 980, training loss: 420.490234375 = 0.8941296339035034 + 50.0 * 8.391921997070312
Epoch 980, val loss: 0.8993532657623291
Epoch 990, training loss: 420.3359680175781 = 0.8903611302375793 + 50.0 * 8.388912200927734
Epoch 990, val loss: 0.8956983685493469
Epoch 1000, training loss: 420.2575378417969 = 0.8867027759552002 + 50.0 * 8.38741683959961
Epoch 1000, val loss: 0.8921744227409363
Epoch 1010, training loss: 420.206298828125 = 0.8830452561378479 + 50.0 * 8.386465072631836
Epoch 1010, val loss: 0.8886618614196777
Epoch 1020, training loss: 420.17578125 = 0.8793684244155884 + 50.0 * 8.3859281539917
Epoch 1020, val loss: 0.8851173520088196
Epoch 1030, training loss: 420.16436767578125 = 0.8756402134895325 + 50.0 * 8.385774612426758
Epoch 1030, val loss: 0.8815099000930786
Epoch 1040, training loss: 420.1480712890625 = 0.8716888427734375 + 50.0 * 8.385527610778809
Epoch 1040, val loss: 0.8776787519454956
Epoch 1050, training loss: 420.1007080078125 = 0.8677537441253662 + 50.0 * 8.384658813476562
Epoch 1050, val loss: 0.8739096522331238
Epoch 1060, training loss: 420.0561218261719 = 0.8638889789581299 + 50.0 * 8.383844375610352
Epoch 1060, val loss: 0.8702136874198914
Epoch 1070, training loss: 420.0137023925781 = 0.8600221872329712 + 50.0 * 8.383073806762695
Epoch 1070, val loss: 0.8664981126785278
Epoch 1080, training loss: 419.98382568359375 = 0.8561174869537354 + 50.0 * 8.382554054260254
Epoch 1080, val loss: 0.8627296686172485
Epoch 1090, training loss: 420.0180969238281 = 0.8521215915679932 + 50.0 * 8.383319854736328
Epoch 1090, val loss: 0.8588958978652954
Epoch 1100, training loss: 420.0020751953125 = 0.847818911075592 + 50.0 * 8.383085250854492
Epoch 1100, val loss: 0.8547220826148987
Epoch 1110, training loss: 419.8975524902344 = 0.8436317443847656 + 50.0 * 8.381078720092773
Epoch 1110, val loss: 0.85071861743927
Epoch 1120, training loss: 419.8739929199219 = 0.839570164680481 + 50.0 * 8.380688667297363
Epoch 1120, val loss: 0.8468145728111267
Epoch 1130, training loss: 419.843994140625 = 0.835504412651062 + 50.0 * 8.380169868469238
Epoch 1130, val loss: 0.8429073095321655
Epoch 1140, training loss: 419.8077087402344 = 0.8313958048820496 + 50.0 * 8.379526138305664
Epoch 1140, val loss: 0.8389500379562378
Epoch 1150, training loss: 419.7796325683594 = 0.8272156715393066 + 50.0 * 8.379048347473145
Epoch 1150, val loss: 0.8349325656890869
Epoch 1160, training loss: 419.76214599609375 = 0.8229842185974121 + 50.0 * 8.378783226013184
Epoch 1160, val loss: 0.8308714628219604
Epoch 1170, training loss: 419.932861328125 = 0.818554162979126 + 50.0 * 8.382286071777344
Epoch 1170, val loss: 0.8266107439994812
Epoch 1180, training loss: 419.7030029296875 = 0.8140842914581299 + 50.0 * 8.377778053283691
Epoch 1180, val loss: 0.8223044276237488
Epoch 1190, training loss: 419.6914978027344 = 0.8097653388977051 + 50.0 * 8.37763500213623
Epoch 1190, val loss: 0.8181443214416504
Epoch 1200, training loss: 419.6465148925781 = 0.8054357767105103 + 50.0 * 8.376821517944336
Epoch 1200, val loss: 0.8140074610710144
Epoch 1210, training loss: 419.622314453125 = 0.8011026978492737 + 50.0 * 8.376423835754395
Epoch 1210, val loss: 0.8098500370979309
Epoch 1220, training loss: 419.7564697265625 = 0.7966729402542114 + 50.0 * 8.379196166992188
Epoch 1220, val loss: 0.8055604100227356
Epoch 1230, training loss: 419.64154052734375 = 0.7920195460319519 + 50.0 * 8.37699031829834
Epoch 1230, val loss: 0.8011164665222168
Epoch 1240, training loss: 419.5528564453125 = 0.7875398993492126 + 50.0 * 8.375306129455566
Epoch 1240, val loss: 0.796814501285553
Epoch 1250, training loss: 419.51751708984375 = 0.7830984592437744 + 50.0 * 8.374688148498535
Epoch 1250, val loss: 0.7925704121589661
Epoch 1260, training loss: 419.4888000488281 = 0.7786627411842346 + 50.0 * 8.374202728271484
Epoch 1260, val loss: 0.7883020043373108
Epoch 1270, training loss: 419.5363464355469 = 0.7741350531578064 + 50.0 * 8.375244140625
Epoch 1270, val loss: 0.7839947938919067
Epoch 1280, training loss: 419.5045471191406 = 0.769463837146759 + 50.0 * 8.374701499938965
Epoch 1280, val loss: 0.7794326543807983
Epoch 1290, training loss: 419.417236328125 = 0.764826238155365 + 50.0 * 8.373047828674316
Epoch 1290, val loss: 0.7750352621078491
Epoch 1300, training loss: 419.39654541015625 = 0.7603129744529724 + 50.0 * 8.372724533081055
Epoch 1300, val loss: 0.7707271575927734
Epoch 1310, training loss: 419.3857727050781 = 0.7558375597000122 + 50.0 * 8.372598648071289
Epoch 1310, val loss: 0.7664005160331726
Epoch 1320, training loss: 419.3839416503906 = 0.7511578798294067 + 50.0 * 8.372655868530273
Epoch 1320, val loss: 0.7619026899337769
Epoch 1330, training loss: 419.30987548828125 = 0.7465330362319946 + 50.0 * 8.371267318725586
Epoch 1330, val loss: 0.7574802041053772
Epoch 1340, training loss: 419.2841491699219 = 0.7419921159744263 + 50.0 * 8.370842933654785
Epoch 1340, val loss: 0.7531546354293823
Epoch 1350, training loss: 419.2573547363281 = 0.7374899387359619 + 50.0 * 8.370397567749023
Epoch 1350, val loss: 0.7488328814506531
Epoch 1360, training loss: 419.2562255859375 = 0.7329665422439575 + 50.0 * 8.370465278625488
Epoch 1360, val loss: 0.7444840669631958
Epoch 1370, training loss: 419.2841491699219 = 0.7282171845436096 + 50.0 * 8.371118545532227
Epoch 1370, val loss: 0.7398936748504639
Epoch 1380, training loss: 419.17999267578125 = 0.7234905958175659 + 50.0 * 8.36913013458252
Epoch 1380, val loss: 0.7354192733764648
Epoch 1390, training loss: 419.1708068847656 = 0.7189405560493469 + 50.0 * 8.369037628173828
Epoch 1390, val loss: 0.7310934662818909
Epoch 1400, training loss: 419.1331787109375 = 0.7144629955291748 + 50.0 * 8.36837387084961
Epoch 1400, val loss: 0.7267885804176331
Epoch 1410, training loss: 419.1043701171875 = 0.7099496126174927 + 50.0 * 8.367888450622559
Epoch 1410, val loss: 0.7224827408790588
Epoch 1420, training loss: 419.1601257324219 = 0.7053962349891663 + 50.0 * 8.369094848632812
Epoch 1420, val loss: 0.718110203742981
Epoch 1430, training loss: 419.10400390625 = 0.7005299925804138 + 50.0 * 8.368069648742676
Epoch 1430, val loss: 0.7134467959403992
Epoch 1440, training loss: 419.06475830078125 = 0.6958855390548706 + 50.0 * 8.367377281188965
Epoch 1440, val loss: 0.7090045809745789
Epoch 1450, training loss: 419.02618408203125 = 0.6914189457893372 + 50.0 * 8.366695404052734
Epoch 1450, val loss: 0.704760730266571
Epoch 1460, training loss: 418.985595703125 = 0.687025785446167 + 50.0 * 8.365971565246582
Epoch 1460, val loss: 0.7005600929260254
Epoch 1470, training loss: 418.96282958984375 = 0.6826139092445374 + 50.0 * 8.365604400634766
Epoch 1470, val loss: 0.6963376998901367
Epoch 1480, training loss: 418.9375305175781 = 0.6781740784645081 + 50.0 * 8.365187644958496
Epoch 1480, val loss: 0.692099392414093
Epoch 1490, training loss: 418.9362487792969 = 0.6737160086631775 + 50.0 * 8.365250587463379
Epoch 1490, val loss: 0.687858521938324
Epoch 1500, training loss: 419.03692626953125 = 0.669063925743103 + 50.0 * 8.36735725402832
Epoch 1500, val loss: 0.6833268404006958
Epoch 1510, training loss: 418.9088134765625 = 0.6642483472824097 + 50.0 * 8.364891052246094
Epoch 1510, val loss: 0.678837239742279
Epoch 1520, training loss: 418.8640441894531 = 0.6598736643791199 + 50.0 * 8.364083290100098
Epoch 1520, val loss: 0.6746336221694946
Epoch 1530, training loss: 418.83203125 = 0.6556035876274109 + 50.0 * 8.36352825164795
Epoch 1530, val loss: 0.6705565452575684
Epoch 1540, training loss: 418.8076171875 = 0.6513314247131348 + 50.0 * 8.363125801086426
Epoch 1540, val loss: 0.6664712429046631
Epoch 1550, training loss: 418.7878112792969 = 0.647050142288208 + 50.0 * 8.362814903259277
Epoch 1550, val loss: 0.6623797416687012
Epoch 1560, training loss: 418.8180847167969 = 0.6427267789840698 + 50.0 * 8.363507270812988
Epoch 1560, val loss: 0.6582713723182678
Epoch 1570, training loss: 418.7625732421875 = 0.6382374167442322 + 50.0 * 8.362486839294434
Epoch 1570, val loss: 0.6539486646652222
Epoch 1580, training loss: 418.7261962890625 = 0.6339050531387329 + 50.0 * 8.361845970153809
Epoch 1580, val loss: 0.6498461961746216
Epoch 1590, training loss: 418.7111511230469 = 0.6297263503074646 + 50.0 * 8.361628532409668
Epoch 1590, val loss: 0.6458512544631958
Epoch 1600, training loss: 418.6863098144531 = 0.6255722045898438 + 50.0 * 8.361214637756348
Epoch 1600, val loss: 0.6419050693511963
Epoch 1610, training loss: 418.666748046875 = 0.6214465498924255 + 50.0 * 8.360905647277832
Epoch 1610, val loss: 0.6379768252372742
Epoch 1620, training loss: 418.87762451171875 = 0.6172568202018738 + 50.0 * 8.36520767211914
Epoch 1620, val loss: 0.6340023875236511
Epoch 1630, training loss: 418.7149963378906 = 0.612905740737915 + 50.0 * 8.362041473388672
Epoch 1630, val loss: 0.6298050880432129
Epoch 1640, training loss: 418.61822509765625 = 0.6087741851806641 + 50.0 * 8.360189437866211
Epoch 1640, val loss: 0.6258995532989502
Epoch 1650, training loss: 418.59381103515625 = 0.6047847270965576 + 50.0 * 8.359780311584473
Epoch 1650, val loss: 0.6221371293067932
Epoch 1660, training loss: 418.57061767578125 = 0.600871741771698 + 50.0 * 8.359395027160645
Epoch 1660, val loss: 0.61838698387146
Epoch 1670, training loss: 418.6031494140625 = 0.5969147682189941 + 50.0 * 8.360124588012695
Epoch 1670, val loss: 0.6146205067634583
Epoch 1680, training loss: 418.5325622558594 = 0.5928906798362732 + 50.0 * 8.358793258666992
Epoch 1680, val loss: 0.6108222603797913
Epoch 1690, training loss: 418.5180969238281 = 0.5890107750892639 + 50.0 * 8.35858154296875
Epoch 1690, val loss: 0.6071634292602539
Epoch 1700, training loss: 418.5077819824219 = 0.585167407989502 + 50.0 * 8.358451843261719
Epoch 1700, val loss: 0.6034993529319763
Epoch 1710, training loss: 418.619140625 = 0.5813119411468506 + 50.0 * 8.360756874084473
Epoch 1710, val loss: 0.5998620390892029
Epoch 1720, training loss: 418.4886474609375 = 0.5774601101875305 + 50.0 * 8.358223915100098
Epoch 1720, val loss: 0.5962086915969849
Epoch 1730, training loss: 418.4436340332031 = 0.5737382769584656 + 50.0 * 8.35739803314209
Epoch 1730, val loss: 0.5927057266235352
Epoch 1740, training loss: 418.4139709472656 = 0.5701053142547607 + 50.0 * 8.356877326965332
Epoch 1740, val loss: 0.58925861120224
Epoch 1750, training loss: 418.4097595214844 = 0.5664905905723572 + 50.0 * 8.356864929199219
Epoch 1750, val loss: 0.5858294367790222
Epoch 1760, training loss: 418.59051513671875 = 0.5627695918083191 + 50.0 * 8.360554695129395
Epoch 1760, val loss: 0.5822498202323914
Epoch 1770, training loss: 418.4187316894531 = 0.5590769052505493 + 50.0 * 8.357192993164062
Epoch 1770, val loss: 0.5788886547088623
Epoch 1780, training loss: 418.34259033203125 = 0.5555638670921326 + 50.0 * 8.355740547180176
Epoch 1780, val loss: 0.5755646824836731
Epoch 1790, training loss: 418.31646728515625 = 0.5521706938743591 + 50.0 * 8.35528564453125
Epoch 1790, val loss: 0.5723711848258972
Epoch 1800, training loss: 418.32952880859375 = 0.5488170981407166 + 50.0 * 8.355613708496094
Epoch 1800, val loss: 0.5692467093467712
Epoch 1810, training loss: 418.3667907714844 = 0.5453463792800903 + 50.0 * 8.356429100036621
Epoch 1810, val loss: 0.5659343004226685
Epoch 1820, training loss: 418.261474609375 = 0.5419328808784485 + 50.0 * 8.354391098022461
Epoch 1820, val loss: 0.562789797782898
Epoch 1830, training loss: 418.246337890625 = 0.5386951565742493 + 50.0 * 8.35415267944336
Epoch 1830, val loss: 0.5597511529922485
Epoch 1840, training loss: 418.2299499511719 = 0.5355322360992432 + 50.0 * 8.353888511657715
Epoch 1840, val loss: 0.5568257570266724
Epoch 1850, training loss: 418.2534484863281 = 0.5323681235313416 + 50.0 * 8.354421615600586
Epoch 1850, val loss: 0.5538853406906128
Epoch 1860, training loss: 418.21807861328125 = 0.5291361808776855 + 50.0 * 8.353778839111328
Epoch 1860, val loss: 0.5508595108985901
Epoch 1870, training loss: 418.2013244628906 = 0.5260303616523743 + 50.0 * 8.353506088256836
Epoch 1870, val loss: 0.5479367971420288
Epoch 1880, training loss: 418.210205078125 = 0.5229300260543823 + 50.0 * 8.353745460510254
Epoch 1880, val loss: 0.545067310333252
Epoch 1890, training loss: 418.1573486328125 = 0.5198929309844971 + 50.0 * 8.35274887084961
Epoch 1890, val loss: 0.5422104597091675
Epoch 1900, training loss: 418.22674560546875 = 0.5168774127960205 + 50.0 * 8.35419750213623
Epoch 1900, val loss: 0.5393727421760559
Epoch 1910, training loss: 418.1238098144531 = 0.5139263272285461 + 50.0 * 8.352197647094727
Epoch 1910, val loss: 0.5367559194564819
Epoch 1920, training loss: 418.08758544921875 = 0.5110781788825989 + 50.0 * 8.351530075073242
Epoch 1920, val loss: 0.5340928435325623
Epoch 1930, training loss: 418.0841064453125 = 0.508278489112854 + 50.0 * 8.351516723632812
Epoch 1930, val loss: 0.5315150618553162
Epoch 1940, training loss: 418.1310119628906 = 0.5054923892021179 + 50.0 * 8.352510452270508
Epoch 1940, val loss: 0.5289972424507141
Epoch 1950, training loss: 418.0623779296875 = 0.5026429295539856 + 50.0 * 8.351194381713867
Epoch 1950, val loss: 0.5263639092445374
Epoch 1960, training loss: 418.042724609375 = 0.49988293647766113 + 50.0 * 8.35085678100586
Epoch 1960, val loss: 0.5237958431243896
Epoch 1970, training loss: 418.01611328125 = 0.49724099040031433 + 50.0 * 8.350377082824707
Epoch 1970, val loss: 0.5213863253593445
Epoch 1980, training loss: 418.02520751953125 = 0.4946343004703522 + 50.0 * 8.350611686706543
Epoch 1980, val loss: 0.5189949870109558
Epoch 1990, training loss: 418.0423889160156 = 0.49196064472198486 + 50.0 * 8.351008415222168
Epoch 1990, val loss: 0.5165311098098755
Epoch 2000, training loss: 417.9772033691406 = 0.4893631339073181 + 50.0 * 8.349757194519043
Epoch 2000, val loss: 0.5142154693603516
Epoch 2010, training loss: 417.95660400390625 = 0.48686665296554565 + 50.0 * 8.349394798278809
Epoch 2010, val loss: 0.511871337890625
Epoch 2020, training loss: 418.1671447753906 = 0.484336256980896 + 50.0 * 8.353655815124512
Epoch 2020, val loss: 0.509487509727478
Epoch 2030, training loss: 417.96209716796875 = 0.48182281851768494 + 50.0 * 8.349605560302734
Epoch 2030, val loss: 0.5073955059051514
Epoch 2040, training loss: 417.90179443359375 = 0.4794154763221741 + 50.0 * 8.348447799682617
Epoch 2040, val loss: 0.5051935315132141
Epoch 2050, training loss: 417.8835144042969 = 0.47712960839271545 + 50.0 * 8.348127365112305
Epoch 2050, val loss: 0.503118634223938
Epoch 2060, training loss: 417.87628173828125 = 0.4748747646808624 + 50.0 * 8.348028182983398
Epoch 2060, val loss: 0.5011305809020996
Epoch 2070, training loss: 418.0460205078125 = 0.4725799560546875 + 50.0 * 8.351469039916992
Epoch 2070, val loss: 0.4990694224834442
Epoch 2080, training loss: 417.8935241699219 = 0.47018882632255554 + 50.0 * 8.348466873168945
Epoch 2080, val loss: 0.4968893229961395
Epoch 2090, training loss: 417.8354187011719 = 0.4680023789405823 + 50.0 * 8.3473482131958
Epoch 2090, val loss: 0.49494948983192444
Epoch 2100, training loss: 417.80718994140625 = 0.46585655212402344 + 50.0 * 8.346826553344727
Epoch 2100, val loss: 0.493053674697876
Epoch 2110, training loss: 417.857177734375 = 0.4637616276741028 + 50.0 * 8.347867965698242
Epoch 2110, val loss: 0.4911699891090393
Epoch 2120, training loss: 417.83197021484375 = 0.4615127742290497 + 50.0 * 8.34740924835205
Epoch 2120, val loss: 0.4891263246536255
Epoch 2130, training loss: 417.7867126464844 = 0.45942509174346924 + 50.0 * 8.346546173095703
Epoch 2130, val loss: 0.4873959422111511
Epoch 2140, training loss: 417.75146484375 = 0.45738381147384644 + 50.0 * 8.345881462097168
Epoch 2140, val loss: 0.4855887293815613
Epoch 2150, training loss: 417.726806640625 = 0.4554341733455658 + 50.0 * 8.345427513122559
Epoch 2150, val loss: 0.4838446378707886
Epoch 2160, training loss: 417.71722412109375 = 0.45349735021591187 + 50.0 * 8.345274925231934
Epoch 2160, val loss: 0.4821770191192627
Epoch 2170, training loss: 417.8443908691406 = 0.45157045125961304 + 50.0 * 8.347856521606445
Epoch 2170, val loss: 0.4805889129638672
Epoch 2180, training loss: 417.7648620605469 = 0.4494433104991913 + 50.0 * 8.346308708190918
Epoch 2180, val loss: 0.47853702306747437
Epoch 2190, training loss: 417.70184326171875 = 0.4475487768650055 + 50.0 * 8.345086097717285
Epoch 2190, val loss: 0.4769887626171112
Epoch 2200, training loss: 417.6633605957031 = 0.4457162022590637 + 50.0 * 8.344352722167969
Epoch 2200, val loss: 0.47533196210861206
Epoch 2210, training loss: 417.7257080078125 = 0.44393327832221985 + 50.0 * 8.345635414123535
Epoch 2210, val loss: 0.4737655222415924
Epoch 2220, training loss: 417.64569091796875 = 0.44207802414894104 + 50.0 * 8.344072341918945
Epoch 2220, val loss: 0.4721967875957489
Epoch 2230, training loss: 417.6300354003906 = 0.440317302942276 + 50.0 * 8.343794822692871
Epoch 2230, val loss: 0.4707556664943695
Epoch 2240, training loss: 417.61328125 = 0.4386110305786133 + 50.0 * 8.343493461608887
Epoch 2240, val loss: 0.4692389965057373
Epoch 2250, training loss: 417.5911560058594 = 0.436940997838974 + 50.0 * 8.343084335327148
Epoch 2250, val loss: 0.46787208318710327
Epoch 2260, training loss: 417.5759582519531 = 0.4352841079235077 + 50.0 * 8.342813491821289
Epoch 2260, val loss: 0.4664653241634369
Epoch 2270, training loss: 417.5606994628906 = 0.4336433410644531 + 50.0 * 8.342540740966797
Epoch 2270, val loss: 0.4650610685348511
Epoch 2280, training loss: 417.5514221191406 = 0.4320206940174103 + 50.0 * 8.342388153076172
Epoch 2280, val loss: 0.4637054204940796
Epoch 2290, training loss: 417.7641296386719 = 0.4303843677043915 + 50.0 * 8.346674919128418
Epoch 2290, val loss: 0.46239331364631653
Epoch 2300, training loss: 417.8185119628906 = 0.42859068512916565 + 50.0 * 8.347798347473145
Epoch 2300, val loss: 0.46055883169174194
Epoch 2310, training loss: 417.5325012207031 = 0.42691075801849365 + 50.0 * 8.342111587524414
Epoch 2310, val loss: 0.459359347820282
Epoch 2320, training loss: 417.53125 = 0.4253826141357422 + 50.0 * 8.342117309570312
Epoch 2320, val loss: 0.4580967426300049
Epoch 2330, training loss: 417.491943359375 = 0.4239400029182434 + 50.0 * 8.341360092163086
Epoch 2330, val loss: 0.4569091796875
Epoch 2340, training loss: 417.4770202636719 = 0.4225098192691803 + 50.0 * 8.341090202331543
Epoch 2340, val loss: 0.4556906521320343
Epoch 2350, training loss: 417.463134765625 = 0.4210754632949829 + 50.0 * 8.340841293334961
Epoch 2350, val loss: 0.4545183777809143
Epoch 2360, training loss: 417.45428466796875 = 0.4196537733078003 + 50.0 * 8.340692520141602
Epoch 2360, val loss: 0.4533475339412689
Epoch 2370, training loss: 417.6307067871094 = 0.4182215631008148 + 50.0 * 8.344249725341797
Epoch 2370, val loss: 0.45199939608573914
Epoch 2380, training loss: 417.5299377441406 = 0.4166802167892456 + 50.0 * 8.342265129089355
Epoch 2380, val loss: 0.4509684443473816
Epoch 2390, training loss: 417.43292236328125 = 0.41528964042663574 + 50.0 * 8.340353012084961
Epoch 2390, val loss: 0.4497687816619873
Epoch 2400, training loss: 417.41912841796875 = 0.4139617085456848 + 50.0 * 8.340103149414062
Epoch 2400, val loss: 0.44872626662254333
Epoch 2410, training loss: 417.39617919921875 = 0.412670373916626 + 50.0 * 8.339670181274414
Epoch 2410, val loss: 0.44769415259361267
Epoch 2420, training loss: 417.3897399902344 = 0.4113823175430298 + 50.0 * 8.339567184448242
Epoch 2420, val loss: 0.44662103056907654
Epoch 2430, training loss: 417.44952392578125 = 0.41008898615837097 + 50.0 * 8.340788841247559
Epoch 2430, val loss: 0.44552817940711975
Epoch 2440, training loss: 417.3839416503906 = 0.4087260663509369 + 50.0 * 8.33950424194336
Epoch 2440, val loss: 0.44446346163749695
Epoch 2450, training loss: 417.3614196777344 = 0.40745532512664795 + 50.0 * 8.339079856872559
Epoch 2450, val loss: 0.44353199005126953
Epoch 2460, training loss: 417.35595703125 = 0.4062167704105377 + 50.0 * 8.338994979858398
Epoch 2460, val loss: 0.44248631596565247
Epoch 2470, training loss: 417.3876953125 = 0.4049989879131317 + 50.0 * 8.339653968811035
Epoch 2470, val loss: 0.4415042996406555
Epoch 2480, training loss: 417.3434143066406 = 0.40378835797309875 + 50.0 * 8.33879280090332
Epoch 2480, val loss: 0.4405806362628937
Epoch 2490, training loss: 417.3305358886719 = 0.40261444449424744 + 50.0 * 8.338558197021484
Epoch 2490, val loss: 0.4397050142288208
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.837138508371385
0.8660436137071652
=== training gcn model ===
Epoch 0, training loss: 530.2097778320312 = 1.0964332818984985 + 50.0 * 10.582266807556152
Epoch 0, val loss: 1.0974171161651611
Epoch 10, training loss: 530.1868286132812 = 1.0922019481658936 + 50.0 * 10.581892013549805
Epoch 10, val loss: 1.0931802988052368
Epoch 20, training loss: 530.1021728515625 = 1.087641716003418 + 50.0 * 10.580290794372559
Epoch 20, val loss: 1.0886191129684448
Epoch 30, training loss: 529.7582397460938 = 1.0827300548553467 + 50.0 * 10.57351016998291
Epoch 30, val loss: 1.0836920738220215
Epoch 40, training loss: 528.549072265625 = 1.0773979425430298 + 50.0 * 10.549433708190918
Epoch 40, val loss: 1.0783348083496094
Epoch 50, training loss: 525.4379272460938 = 1.0718104839324951 + 50.0 * 10.487321853637695
Epoch 50, val loss: 1.072745442390442
Epoch 60, training loss: 519.4722290039062 = 1.0667645931243896 + 50.0 * 10.368108749389648
Epoch 60, val loss: 1.0677435398101807
Epoch 70, training loss: 510.323486328125 = 1.0620269775390625 + 50.0 * 10.185229301452637
Epoch 70, val loss: 1.0628639459609985
Epoch 80, training loss: 498.7405700683594 = 1.0563715696334839 + 50.0 * 9.953683853149414
Epoch 80, val loss: 1.0571266412734985
Epoch 90, training loss: 484.3656311035156 = 1.0510324239730835 + 50.0 * 9.666292190551758
Epoch 90, val loss: 1.051973581314087
Epoch 100, training loss: 476.48504638671875 = 1.0465266704559326 + 50.0 * 9.508769989013672
Epoch 100, val loss: 1.0477311611175537
Epoch 110, training loss: 470.7117004394531 = 1.042462706565857 + 50.0 * 9.39338493347168
Epoch 110, val loss: 1.0437872409820557
Epoch 120, training loss: 465.7817077636719 = 1.0379972457885742 + 50.0 * 9.29487419128418
Epoch 120, val loss: 1.039347767829895
Epoch 130, training loss: 462.2360534667969 = 1.0335431098937988 + 50.0 * 9.224050521850586
Epoch 130, val loss: 1.0348670482635498
Epoch 140, training loss: 459.9200134277344 = 1.0302952527999878 + 50.0 * 9.177794456481934
Epoch 140, val loss: 1.031638503074646
Epoch 150, training loss: 457.6056823730469 = 1.028620719909668 + 50.0 * 9.13154125213623
Epoch 150, val loss: 1.030021071434021
Epoch 160, training loss: 454.5146179199219 = 1.0278290510177612 + 50.0 * 9.069735527038574
Epoch 160, val loss: 1.0293325185775757
Epoch 170, training loss: 450.8106384277344 = 1.027672529220581 + 50.0 * 8.995658874511719
Epoch 170, val loss: 1.0291502475738525
Epoch 180, training loss: 447.5921630859375 = 1.0275861024856567 + 50.0 * 8.931291580200195
Epoch 180, val loss: 1.0289030075073242
Epoch 190, training loss: 445.181884765625 = 1.0265209674835205 + 50.0 * 8.88310718536377
Epoch 190, val loss: 1.0276840925216675
Epoch 200, training loss: 443.9312438964844 = 1.0245249271392822 + 50.0 * 8.858134269714355
Epoch 200, val loss: 1.0256121158599854
Epoch 210, training loss: 443.3356018066406 = 1.0220565795898438 + 50.0 * 8.846270561218262
Epoch 210, val loss: 1.0231479406356812
Epoch 220, training loss: 442.8492126464844 = 1.0195938348770142 + 50.0 * 8.836592674255371
Epoch 220, val loss: 1.0207942724227905
Epoch 230, training loss: 442.37774658203125 = 1.0176526308059692 + 50.0 * 8.827201843261719
Epoch 230, val loss: 1.0189515352249146
Epoch 240, training loss: 441.83392333984375 = 1.0160750150680542 + 50.0 * 8.816356658935547
Epoch 240, val loss: 1.0174390077590942
Epoch 250, training loss: 441.1108703613281 = 1.0148212909698486 + 50.0 * 8.801920890808105
Epoch 250, val loss: 1.0162396430969238
Epoch 260, training loss: 440.02008056640625 = 1.01386559009552 + 50.0 * 8.78012466430664
Epoch 260, val loss: 1.0153414011001587
Epoch 270, training loss: 438.4534606933594 = 1.0132958889007568 + 50.0 * 8.74880313873291
Epoch 270, val loss: 1.0148420333862305
Epoch 280, training loss: 436.9111022949219 = 1.0127440690994263 + 50.0 * 8.71796703338623
Epoch 280, val loss: 1.0143342018127441
Epoch 290, training loss: 435.8617248535156 = 1.011552333831787 + 50.0 * 8.697003364562988
Epoch 290, val loss: 1.0131498575210571
Epoch 300, training loss: 434.9637756347656 = 1.0098319053649902 + 50.0 * 8.679079055786133
Epoch 300, val loss: 1.0114930868148804
Epoch 310, training loss: 434.0632019042969 = 1.0085029602050781 + 50.0 * 8.661093711853027
Epoch 310, val loss: 1.0102916955947876
Epoch 320, training loss: 433.3844299316406 = 1.0073939561843872 + 50.0 * 8.647541046142578
Epoch 320, val loss: 1.0092395544052124
Epoch 330, training loss: 432.8302917480469 = 1.0059670209884644 + 50.0 * 8.636486053466797
Epoch 330, val loss: 1.0079214572906494
Epoch 340, training loss: 432.2803955078125 = 1.0044585466384888 + 50.0 * 8.625518798828125
Epoch 340, val loss: 1.006516933441162
Epoch 350, training loss: 431.6547546386719 = 1.0030471086502075 + 50.0 * 8.61303424835205
Epoch 350, val loss: 1.0052627325057983
Epoch 360, training loss: 430.96722412109375 = 1.001765489578247 + 50.0 * 8.599308967590332
Epoch 360, val loss: 1.0040727853775024
Epoch 370, training loss: 430.2448425292969 = 1.0006585121154785 + 50.0 * 8.584883689880371
Epoch 370, val loss: 1.0030795335769653
Epoch 380, training loss: 429.560791015625 = 0.999623715877533 + 50.0 * 8.571223258972168
Epoch 380, val loss: 1.0021339654922485
Epoch 390, training loss: 429.05242919921875 = 0.9984725713729858 + 50.0 * 8.561079025268555
Epoch 390, val loss: 1.001031756401062
Epoch 400, training loss: 428.4597473144531 = 0.9970327615737915 + 50.0 * 8.549254417419434
Epoch 400, val loss: 0.9997454881668091
Epoch 410, training loss: 427.9771728515625 = 0.9954769611358643 + 50.0 * 8.539633750915527
Epoch 410, val loss: 0.9982882142066956
Epoch 420, training loss: 427.59246826171875 = 0.9936934113502502 + 50.0 * 8.531975746154785
Epoch 420, val loss: 0.9966293573379517
Epoch 430, training loss: 427.2257995605469 = 0.9918404817581177 + 50.0 * 8.524679183959961
Epoch 430, val loss: 0.9948955178260803
Epoch 440, training loss: 427.0307922363281 = 0.989893913269043 + 50.0 * 8.520817756652832
Epoch 440, val loss: 0.9930086135864258
Epoch 450, training loss: 426.59197998046875 = 0.9878368973731995 + 50.0 * 8.512083053588867
Epoch 450, val loss: 0.9910813570022583
Epoch 460, training loss: 426.24786376953125 = 0.9859037399291992 + 50.0 * 8.505239486694336
Epoch 460, val loss: 0.9892442226409912
Epoch 470, training loss: 425.9722900390625 = 0.9838988184928894 + 50.0 * 8.499768257141113
Epoch 470, val loss: 0.9873262047767639
Epoch 480, training loss: 425.72894287109375 = 0.9818043112754822 + 50.0 * 8.494942665100098
Epoch 480, val loss: 0.9853159785270691
Epoch 490, training loss: 425.5440673828125 = 0.9796256422996521 + 50.0 * 8.491289138793945
Epoch 490, val loss: 0.9831976294517517
Epoch 500, training loss: 425.39715576171875 = 0.9771711230278015 + 50.0 * 8.488399505615234
Epoch 500, val loss: 0.9809067249298096
Epoch 510, training loss: 425.1750793457031 = 0.9747025966644287 + 50.0 * 8.484007835388184
Epoch 510, val loss: 0.9785201549530029
Epoch 520, training loss: 424.99896240234375 = 0.9721923470497131 + 50.0 * 8.480535507202148
Epoch 520, val loss: 0.9761168360710144
Epoch 530, training loss: 424.8441467285156 = 0.9696242809295654 + 50.0 * 8.477490425109863
Epoch 530, val loss: 0.9736561179161072
Epoch 540, training loss: 424.8049621582031 = 0.966991662979126 + 50.0 * 8.476759910583496
Epoch 540, val loss: 0.9711673855781555
Epoch 550, training loss: 424.5303955078125 = 0.9643023610115051 + 50.0 * 8.471322059631348
Epoch 550, val loss: 0.9685631990432739
Epoch 560, training loss: 424.3415832519531 = 0.9616853594779968 + 50.0 * 8.467597961425781
Epoch 560, val loss: 0.9661027193069458
Epoch 570, training loss: 424.1524353027344 = 0.9590864181518555 + 50.0 * 8.4638671875
Epoch 570, val loss: 0.9636154174804688
Epoch 580, training loss: 424.07037353515625 = 0.9564356207847595 + 50.0 * 8.462279319763184
Epoch 580, val loss: 0.9610995054244995
Epoch 590, training loss: 423.8257141113281 = 0.9536562561988831 + 50.0 * 8.457441329956055
Epoch 590, val loss: 0.9583796262741089
Epoch 600, training loss: 423.62451171875 = 0.9509074091911316 + 50.0 * 8.453472137451172
Epoch 600, val loss: 0.9557612538337708
Epoch 610, training loss: 423.45465087890625 = 0.9481079578399658 + 50.0 * 8.450130462646484
Epoch 610, val loss: 0.9530710577964783
Epoch 620, training loss: 423.4884948730469 = 0.9451494216918945 + 50.0 * 8.45086669921875
Epoch 620, val loss: 0.9502981305122375
Epoch 630, training loss: 423.185791015625 = 0.9419087767601013 + 50.0 * 8.444877624511719
Epoch 630, val loss: 0.947100043296814
Epoch 640, training loss: 423.0951232910156 = 0.9386826157569885 + 50.0 * 8.44312858581543
Epoch 640, val loss: 0.9440491199493408
Epoch 650, training loss: 422.99017333984375 = 0.9354851245880127 + 50.0 * 8.441093444824219
Epoch 650, val loss: 0.9409707188606262
Epoch 660, training loss: 422.878662109375 = 0.932202160358429 + 50.0 * 8.438929557800293
Epoch 660, val loss: 0.9378212690353394
Epoch 670, training loss: 422.78106689453125 = 0.9288069009780884 + 50.0 * 8.437045097351074
Epoch 670, val loss: 0.9345703125
Epoch 680, training loss: 422.7814636230469 = 0.9253073930740356 + 50.0 * 8.43712329864502
Epoch 680, val loss: 0.9312630295753479
Epoch 690, training loss: 422.6199645996094 = 0.9217831492424011 + 50.0 * 8.433963775634766
Epoch 690, val loss: 0.9278494715690613
Epoch 700, training loss: 422.5191650390625 = 0.9182581901550293 + 50.0 * 8.432018280029297
Epoch 700, val loss: 0.9244353175163269
Epoch 710, training loss: 422.4249267578125 = 0.9146734476089478 + 50.0 * 8.430205345153809
Epoch 710, val loss: 0.92099928855896
Epoch 720, training loss: 422.52020263671875 = 0.9110655784606934 + 50.0 * 8.432182312011719
Epoch 720, val loss: 0.9174858927726746
Epoch 730, training loss: 422.2975769042969 = 0.9072849750518799 + 50.0 * 8.42780590057373
Epoch 730, val loss: 0.9139046669006348
Epoch 740, training loss: 422.18743896484375 = 0.9036205410957336 + 50.0 * 8.425676345825195
Epoch 740, val loss: 0.910396933555603
Epoch 750, training loss: 422.0956115722656 = 0.8999171257019043 + 50.0 * 8.423913955688477
Epoch 750, val loss: 0.9068477749824524
Epoch 760, training loss: 422.0140380859375 = 0.8961586356163025 + 50.0 * 8.422357559204102
Epoch 760, val loss: 0.9032666087150574
Epoch 770, training loss: 422.23126220703125 = 0.892221212387085 + 50.0 * 8.426780700683594
Epoch 770, val loss: 0.8995729088783264
Epoch 780, training loss: 421.8938903808594 = 0.8881645798683167 + 50.0 * 8.420114517211914
Epoch 780, val loss: 0.8956401348114014
Epoch 790, training loss: 421.7899169921875 = 0.8842861652374268 + 50.0 * 8.418112754821777
Epoch 790, val loss: 0.8918914198875427
Epoch 800, training loss: 421.71051025390625 = 0.8804001212120056 + 50.0 * 8.41660213470459
Epoch 800, val loss: 0.888136625289917
Epoch 810, training loss: 421.6422119140625 = 0.8764322996139526 + 50.0 * 8.415315628051758
Epoch 810, val loss: 0.884326159954071
Epoch 820, training loss: 421.56878662109375 = 0.8723601698875427 + 50.0 * 8.413928985595703
Epoch 820, val loss: 0.8804112076759338
Epoch 830, training loss: 421.6761779785156 = 0.868249773979187 + 50.0 * 8.416158676147461
Epoch 830, val loss: 0.8763561248779297
Epoch 840, training loss: 421.4434509277344 = 0.8638172149658203 + 50.0 * 8.411592483520508
Epoch 840, val loss: 0.8721891641616821
Epoch 850, training loss: 421.38287353515625 = 0.8596089482307434 + 50.0 * 8.410465240478516
Epoch 850, val loss: 0.8681538105010986
Epoch 860, training loss: 421.3157653808594 = 0.855400025844574 + 50.0 * 8.409207344055176
Epoch 860, val loss: 0.8641138672828674
Epoch 870, training loss: 421.2481689453125 = 0.8511414527893066 + 50.0 * 8.407940864562988
Epoch 870, val loss: 0.8600342869758606
Epoch 880, training loss: 421.20098876953125 = 0.8468111753463745 + 50.0 * 8.407083511352539
Epoch 880, val loss: 0.855889081954956
Epoch 890, training loss: 421.1388854980469 = 0.8421568274497986 + 50.0 * 8.40593433380127
Epoch 890, val loss: 0.8513871431350708
Epoch 900, training loss: 421.08251953125 = 0.8375878930091858 + 50.0 * 8.404898643493652
Epoch 900, val loss: 0.8470357656478882
Epoch 910, training loss: 421.00482177734375 = 0.8332249522209167 + 50.0 * 8.40343189239502
Epoch 910, val loss: 0.8428161144256592
Epoch 920, training loss: 420.9460754394531 = 0.8288127779960632 + 50.0 * 8.402344703674316
Epoch 920, val loss: 0.8385583162307739
Epoch 930, training loss: 420.8894348144531 = 0.8243899345397949 + 50.0 * 8.401300430297852
Epoch 930, val loss: 0.8343061804771423
Epoch 940, training loss: 420.832275390625 = 0.8198716640472412 + 50.0 * 8.400247573852539
Epoch 940, val loss: 0.8299480080604553
Epoch 950, training loss: 420.7830810546875 = 0.815277099609375 + 50.0 * 8.3993558883667
Epoch 950, val loss: 0.8255482912063599
Epoch 960, training loss: 420.7366638183594 = 0.8104706406593323 + 50.0 * 8.398523330688477
Epoch 960, val loss: 0.8208735585212708
Epoch 970, training loss: 420.6796569824219 = 0.8056963682174683 + 50.0 * 8.397479057312012
Epoch 970, val loss: 0.8162873983383179
Epoch 980, training loss: 420.6195983886719 = 0.801042377948761 + 50.0 * 8.396370887756348
Epoch 980, val loss: 0.8118512630462646
Epoch 990, training loss: 420.5675354003906 = 0.7963759899139404 + 50.0 * 8.39542293548584
Epoch 990, val loss: 0.8073474168777466
Epoch 1000, training loss: 420.53314208984375 = 0.7916092276573181 + 50.0 * 8.394830703735352
Epoch 1000, val loss: 0.8027889728546143
Epoch 1010, training loss: 420.5997009277344 = 0.7865898013114929 + 50.0 * 8.396262168884277
Epoch 1010, val loss: 0.797976553440094
Epoch 1020, training loss: 420.4324645996094 = 0.7816221714019775 + 50.0 * 8.393016815185547
Epoch 1020, val loss: 0.7931249737739563
Epoch 1030, training loss: 420.3883056640625 = 0.7767674922943115 + 50.0 * 8.392230987548828
Epoch 1030, val loss: 0.7883977890014648
Epoch 1040, training loss: 420.3282775878906 = 0.7719010710716248 + 50.0 * 8.391127586364746
Epoch 1040, val loss: 0.7837252616882324
Epoch 1050, training loss: 420.2798156738281 = 0.7669385075569153 + 50.0 * 8.390257835388184
Epoch 1050, val loss: 0.7789629697799683
Epoch 1060, training loss: 420.2333068847656 = 0.7620024681091309 + 50.0 * 8.389426231384277
Epoch 1060, val loss: 0.7742003798484802
Epoch 1070, training loss: 420.1969909667969 = 0.7570409178733826 + 50.0 * 8.388798713684082
Epoch 1070, val loss: 0.7694584727287292
Epoch 1080, training loss: 420.21038818359375 = 0.751873791217804 + 50.0 * 8.38917064666748
Epoch 1080, val loss: 0.7645591497421265
Epoch 1090, training loss: 420.25653076171875 = 0.7466355562210083 + 50.0 * 8.39019775390625
Epoch 1090, val loss: 0.7594981789588928
Epoch 1100, training loss: 420.1000671386719 = 0.741507887840271 + 50.0 * 8.387170791625977
Epoch 1100, val loss: 0.7544424533843994
Epoch 1110, training loss: 420.0212707519531 = 0.736453115940094 + 50.0 * 8.385696411132812
Epoch 1110, val loss: 0.7496531009674072
Epoch 1120, training loss: 419.9643859863281 = 0.7315324544906616 + 50.0 * 8.38465690612793
Epoch 1120, val loss: 0.7449305057525635
Epoch 1130, training loss: 419.9200439453125 = 0.7266035079956055 + 50.0 * 8.383869171142578
Epoch 1130, val loss: 0.7402281761169434
Epoch 1140, training loss: 419.8987121582031 = 0.7217050194740295 + 50.0 * 8.383540153503418
Epoch 1140, val loss: 0.7354682683944702
Epoch 1150, training loss: 419.9202575683594 = 0.7166293859481812 + 50.0 * 8.384072303771973
Epoch 1150, val loss: 0.7305781841278076
Epoch 1160, training loss: 419.7964172363281 = 0.7115465998649597 + 50.0 * 8.381697654724121
Epoch 1160, val loss: 0.7258148789405823
Epoch 1170, training loss: 419.7638854980469 = 0.7066658735275269 + 50.0 * 8.381144523620605
Epoch 1170, val loss: 0.7212826013565063
Epoch 1180, training loss: 419.70849609375 = 0.7019046545028687 + 50.0 * 8.380131721496582
Epoch 1180, val loss: 0.7166348099708557
Epoch 1190, training loss: 419.672607421875 = 0.6971344947814941 + 50.0 * 8.379508972167969
Epoch 1190, val loss: 0.7121011018753052
Epoch 1200, training loss: 419.91741943359375 = 0.692365825176239 + 50.0 * 8.384501457214355
Epoch 1200, val loss: 0.7074447274208069
Epoch 1210, training loss: 419.6343688964844 = 0.687250554561615 + 50.0 * 8.378942489624023
Epoch 1210, val loss: 0.702744722366333
Epoch 1220, training loss: 419.5651550292969 = 0.6825708746910095 + 50.0 * 8.377652168273926
Epoch 1220, val loss: 0.6982617378234863
Epoch 1230, training loss: 419.5176696777344 = 0.6779735088348389 + 50.0 * 8.37679386138916
Epoch 1230, val loss: 0.693866491317749
Epoch 1240, training loss: 419.4742126464844 = 0.6734164953231812 + 50.0 * 8.376015663146973
Epoch 1240, val loss: 0.6895769834518433
Epoch 1250, training loss: 419.44464111328125 = 0.6689420938491821 + 50.0 * 8.375514030456543
Epoch 1250, val loss: 0.6852962374687195
Epoch 1260, training loss: 419.5593566894531 = 0.6643881797790527 + 50.0 * 8.377899169921875
Epoch 1260, val loss: 0.6809529066085815
Epoch 1270, training loss: 419.3895263671875 = 0.6597993969917297 + 50.0 * 8.374594688415527
Epoch 1270, val loss: 0.6766147613525391
Epoch 1280, training loss: 419.342529296875 = 0.6554183959960938 + 50.0 * 8.37374210357666
Epoch 1280, val loss: 0.6724957823753357
Epoch 1290, training loss: 419.3110656738281 = 0.6511566042900085 + 50.0 * 8.373198509216309
Epoch 1290, val loss: 0.6684656739234924
Epoch 1300, training loss: 419.2714538574219 = 0.6469126343727112 + 50.0 * 8.372490882873535
Epoch 1300, val loss: 0.6644579768180847
Epoch 1310, training loss: 419.2662658691406 = 0.6427338719367981 + 50.0 * 8.37247085571289
Epoch 1310, val loss: 0.6605063676834106
Epoch 1320, training loss: 419.3028259277344 = 0.6383459568023682 + 50.0 * 8.373290061950684
Epoch 1320, val loss: 0.656374990940094
Epoch 1330, training loss: 419.2507629394531 = 0.63419508934021 + 50.0 * 8.372331619262695
Epoch 1330, val loss: 0.6524202227592468
Epoch 1340, training loss: 419.1574401855469 = 0.6302222609519958 + 50.0 * 8.37054443359375
Epoch 1340, val loss: 0.6486597061157227
Epoch 1350, training loss: 419.12286376953125 = 0.6263465881347656 + 50.0 * 8.369930267333984
Epoch 1350, val loss: 0.6449697613716125
Epoch 1360, training loss: 419.0954284667969 = 0.6225430369377136 + 50.0 * 8.369458198547363
Epoch 1360, val loss: 0.6413707137107849
Epoch 1370, training loss: 419.1194152832031 = 0.6188018918037415 + 50.0 * 8.370012283325195
Epoch 1370, val loss: 0.6376984715461731
Epoch 1380, training loss: 419.0714111328125 = 0.6147283911705017 + 50.0 * 8.369133949279785
Epoch 1380, val loss: 0.6341255307197571
Epoch 1390, training loss: 419.0338439941406 = 0.6110625267028809 + 50.0 * 8.36845588684082
Epoch 1390, val loss: 0.630513072013855
Epoch 1400, training loss: 418.9864501953125 = 0.6073977947235107 + 50.0 * 8.367581367492676
Epoch 1400, val loss: 0.6270502805709839
Epoch 1410, training loss: 418.9544982910156 = 0.6037641763687134 + 50.0 * 8.36701488494873
Epoch 1410, val loss: 0.6236828565597534
Epoch 1420, training loss: 418.94464111328125 = 0.6002362370491028 + 50.0 * 8.366888046264648
Epoch 1420, val loss: 0.6203221678733826
Epoch 1430, training loss: 419.0834655761719 = 0.5966238379478455 + 50.0 * 8.369736671447754
Epoch 1430, val loss: 0.616812527179718
Epoch 1440, training loss: 418.8917236328125 = 0.5929041504859924 + 50.0 * 8.365976333618164
Epoch 1440, val loss: 0.6134888529777527
Epoch 1450, training loss: 418.8470153808594 = 0.5895254611968994 + 50.0 * 8.36514949798584
Epoch 1450, val loss: 0.6103399395942688
Epoch 1460, training loss: 418.81768798828125 = 0.5862030386924744 + 50.0 * 8.364629745483398
Epoch 1460, val loss: 0.6071711778640747
Epoch 1470, training loss: 418.7887878417969 = 0.582910418510437 + 50.0 * 8.364117622375488
Epoch 1470, val loss: 0.6041350364685059
Epoch 1480, training loss: 418.8148498535156 = 0.5796454548835754 + 50.0 * 8.364704132080078
Epoch 1480, val loss: 0.6011075973510742
Epoch 1490, training loss: 418.7618103027344 = 0.5763201117515564 + 50.0 * 8.363709449768066
Epoch 1490, val loss: 0.5978528261184692
Epoch 1500, training loss: 418.7399597167969 = 0.5730389952659607 + 50.0 * 8.363338470458984
Epoch 1500, val loss: 0.5949528813362122
Epoch 1510, training loss: 418.68670654296875 = 0.5699583292007446 + 50.0 * 8.362335205078125
Epoch 1510, val loss: 0.5920013189315796
Epoch 1520, training loss: 418.6606140136719 = 0.5669138431549072 + 50.0 * 8.361873626708984
Epoch 1520, val loss: 0.5891507267951965
Epoch 1530, training loss: 418.6370544433594 = 0.5638629198074341 + 50.0 * 8.36146354675293
Epoch 1530, val loss: 0.5863426327705383
Epoch 1540, training loss: 418.6861877441406 = 0.5608794689178467 + 50.0 * 8.362505912780762
Epoch 1540, val loss: 0.583507239818573
Epoch 1550, training loss: 418.7289123535156 = 0.557749330997467 + 50.0 * 8.363423347473145
Epoch 1550, val loss: 0.5805199146270752
Epoch 1560, training loss: 418.6187438964844 = 0.5546290874481201 + 50.0 * 8.361282348632812
Epoch 1560, val loss: 0.5778145790100098
Epoch 1570, training loss: 418.54217529296875 = 0.5518054962158203 + 50.0 * 8.359807014465332
Epoch 1570, val loss: 0.5751069188117981
Epoch 1580, training loss: 418.51910400390625 = 0.5489972233772278 + 50.0 * 8.35940170288086
Epoch 1580, val loss: 0.5725135803222656
Epoch 1590, training loss: 418.4896240234375 = 0.5462201833724976 + 50.0 * 8.358867645263672
Epoch 1590, val loss: 0.5699458718299866
Epoch 1600, training loss: 418.47735595703125 = 0.543462872505188 + 50.0 * 8.358677864074707
Epoch 1600, val loss: 0.567436695098877
Epoch 1610, training loss: 418.55389404296875 = 0.5406196713447571 + 50.0 * 8.360265731811523
Epoch 1610, val loss: 0.5648409128189087
Epoch 1620, training loss: 418.4606628417969 = 0.5379269123077393 + 50.0 * 8.358454704284668
Epoch 1620, val loss: 0.562256932258606
Epoch 1630, training loss: 418.493896484375 = 0.5352334380149841 + 50.0 * 8.359172821044922
Epoch 1630, val loss: 0.5596973896026611
Epoch 1640, training loss: 418.40850830078125 = 0.532550036907196 + 50.0 * 8.357519149780273
Epoch 1640, val loss: 0.5573394894599915
Epoch 1650, training loss: 418.3631591796875 = 0.5300161242485046 + 50.0 * 8.35666275024414
Epoch 1650, val loss: 0.5549639463424683
Epoch 1660, training loss: 418.3450622558594 = 0.5275245308876038 + 50.0 * 8.356350898742676
Epoch 1660, val loss: 0.5526843070983887
Epoch 1670, training loss: 418.34649658203125 = 0.5250701904296875 + 50.0 * 8.356429100036621
Epoch 1670, val loss: 0.5503994822502136
Epoch 1680, training loss: 418.38934326171875 = 0.5225805640220642 + 50.0 * 8.357335090637207
Epoch 1680, val loss: 0.5480897426605225
Epoch 1690, training loss: 418.3204650878906 = 0.5200931429862976 + 50.0 * 8.35600757598877
Epoch 1690, val loss: 0.545844316482544
Epoch 1700, training loss: 418.27734375 = 0.5177019834518433 + 50.0 * 8.355193138122559
Epoch 1700, val loss: 0.5436993837356567
Epoch 1710, training loss: 418.25347900390625 = 0.5153954029083252 + 50.0 * 8.354762077331543
Epoch 1710, val loss: 0.5415852069854736
Epoch 1720, training loss: 418.2740173339844 = 0.5130525827407837 + 50.0 * 8.355218887329102
Epoch 1720, val loss: 0.5394723415374756
Epoch 1730, training loss: 418.2577819824219 = 0.510696530342102 + 50.0 * 8.354941368103027
Epoch 1730, val loss: 0.5372856855392456
Epoch 1740, training loss: 418.2366638183594 = 0.5084750056266785 + 50.0 * 8.35456371307373
Epoch 1740, val loss: 0.5351986289024353
Epoch 1750, training loss: 418.1952209472656 = 0.5061917901039124 + 50.0 * 8.353780746459961
Epoch 1750, val loss: 0.5331211090087891
Epoch 1760, training loss: 418.1549377441406 = 0.503990113735199 + 50.0 * 8.353018760681152
Epoch 1760, val loss: 0.5311792492866516
Epoch 1770, training loss: 418.1346435546875 = 0.5018748044967651 + 50.0 * 8.352655410766602
Epoch 1770, val loss: 0.5292257070541382
Epoch 1780, training loss: 418.11944580078125 = 0.49979233741760254 + 50.0 * 8.35239315032959
Epoch 1780, val loss: 0.5272924900054932
Epoch 1790, training loss: 418.22509765625 = 0.4977099597454071 + 50.0 * 8.354547500610352
Epoch 1790, val loss: 0.5253180265426636
Epoch 1800, training loss: 418.1133117675781 = 0.49542441964149475 + 50.0 * 8.352357864379883
Epoch 1800, val loss: 0.5234302878379822
Epoch 1810, training loss: 418.0774841308594 = 0.49344778060913086 + 50.0 * 8.351680755615234
Epoch 1810, val loss: 0.5215160250663757
Epoch 1820, training loss: 418.0438537597656 = 0.49143141508102417 + 50.0 * 8.351048469543457
Epoch 1820, val loss: 0.519775927066803
Epoch 1830, training loss: 418.0334167480469 = 0.48945945501327515 + 50.0 * 8.350878715515137
Epoch 1830, val loss: 0.5179901719093323
Epoch 1840, training loss: 418.2394104003906 = 0.48741891980171204 + 50.0 * 8.355039596557617
Epoch 1840, val loss: 0.5162199139595032
Epoch 1850, training loss: 418.0595703125 = 0.4854925870895386 + 50.0 * 8.351481437683105
Epoch 1850, val loss: 0.5143174529075623
Epoch 1860, training loss: 417.98199462890625 = 0.483541876077652 + 50.0 * 8.349968910217285
Epoch 1860, val loss: 0.5126980543136597
Epoch 1870, training loss: 417.952880859375 = 0.48175325989723206 + 50.0 * 8.349422454833984
Epoch 1870, val loss: 0.5110495090484619
Epoch 1880, training loss: 417.93341064453125 = 0.47992822527885437 + 50.0 * 8.349069595336914
Epoch 1880, val loss: 0.5094488859176636
Epoch 1890, training loss: 417.9701843261719 = 0.478118896484375 + 50.0 * 8.349841117858887
Epoch 1890, val loss: 0.5078724026679993
Epoch 1900, training loss: 417.9223937988281 = 0.4762505292892456 + 50.0 * 8.348922729492188
Epoch 1900, val loss: 0.5061067938804626
Epoch 1910, training loss: 417.923583984375 = 0.474408358335495 + 50.0 * 8.348983764648438
Epoch 1910, val loss: 0.5045886635780334
Epoch 1920, training loss: 417.8669128417969 = 0.4727129340171814 + 50.0 * 8.347884178161621
Epoch 1920, val loss: 0.503031849861145
Epoch 1930, training loss: 417.85369873046875 = 0.47103315591812134 + 50.0 * 8.34765338897705
Epoch 1930, val loss: 0.5015443563461304
Epoch 1940, training loss: 417.8348693847656 = 0.46936875581741333 + 50.0 * 8.347310066223145
Epoch 1940, val loss: 0.5000767707824707
Epoch 1950, training loss: 417.93682861328125 = 0.4677293598651886 + 50.0 * 8.349381446838379
Epoch 1950, val loss: 0.4985847473144531
Epoch 1960, training loss: 417.8398742675781 = 0.46586892008781433 + 50.0 * 8.347479820251465
Epoch 1960, val loss: 0.4969252347946167
Epoch 1970, training loss: 417.8337097167969 = 0.4642506241798401 + 50.0 * 8.347389221191406
Epoch 1970, val loss: 0.49559617042541504
Epoch 1980, training loss: 417.77801513671875 = 0.4626292586326599 + 50.0 * 8.346307754516602
Epoch 1980, val loss: 0.49414193630218506
Epoch 1990, training loss: 417.755615234375 = 0.46106651425361633 + 50.0 * 8.345890998840332
Epoch 1990, val loss: 0.4927835464477539
Epoch 2000, training loss: 417.7500915527344 = 0.45951223373413086 + 50.0 * 8.34581184387207
Epoch 2000, val loss: 0.4914599359035492
Epoch 2010, training loss: 417.8346252441406 = 0.45792385935783386 + 50.0 * 8.3475341796875
Epoch 2010, val loss: 0.4900818169116974
Epoch 2020, training loss: 417.7625732421875 = 0.456413596868515 + 50.0 * 8.346122741699219
Epoch 2020, val loss: 0.48864665627479553
Epoch 2030, training loss: 417.7087097167969 = 0.4548467695713043 + 50.0 * 8.345077514648438
Epoch 2030, val loss: 0.4873484969139099
Epoch 2040, training loss: 417.68548583984375 = 0.4533717632293701 + 50.0 * 8.344642639160156
Epoch 2040, val loss: 0.4860421419143677
Epoch 2050, training loss: 417.7049255371094 = 0.4519094228744507 + 50.0 * 8.345060348510742
Epoch 2050, val loss: 0.4847603142261505
Epoch 2060, training loss: 417.7618408203125 = 0.4503936767578125 + 50.0 * 8.34622859954834
Epoch 2060, val loss: 0.4834173619747162
Epoch 2070, training loss: 417.6808166503906 = 0.4488777816295624 + 50.0 * 8.34463882446289
Epoch 2070, val loss: 0.4822045862674713
Epoch 2080, training loss: 417.63677978515625 = 0.447494238615036 + 50.0 * 8.343786239624023
Epoch 2080, val loss: 0.48096325993537903
Epoch 2090, training loss: 417.6147766113281 = 0.4460967481136322 + 50.0 * 8.34337329864502
Epoch 2090, val loss: 0.47979074716567993
Epoch 2100, training loss: 417.60986328125 = 0.44472286105155945 + 50.0 * 8.343302726745605
Epoch 2100, val loss: 0.47862082719802856
Epoch 2110, training loss: 417.7301330566406 = 0.44329240918159485 + 50.0 * 8.345736503601074
Epoch 2110, val loss: 0.47744932770729065
Epoch 2120, training loss: 417.6070556640625 = 0.4419306218624115 + 50.0 * 8.343302726745605
Epoch 2120, val loss: 0.4761714041233063
Epoch 2130, training loss: 417.5616149902344 = 0.4405630826950073 + 50.0 * 8.342421531677246
Epoch 2130, val loss: 0.47503581643104553
Epoch 2140, training loss: 417.5506896972656 = 0.43925029039382935 + 50.0 * 8.342228889465332
Epoch 2140, val loss: 0.473946213722229
Epoch 2150, training loss: 417.6307373046875 = 0.43796464800834656 + 50.0 * 8.343855857849121
Epoch 2150, val loss: 0.47282591462135315
Epoch 2160, training loss: 417.5367126464844 = 0.43654704093933105 + 50.0 * 8.342002868652344
Epoch 2160, val loss: 0.4715954661369324
Epoch 2170, training loss: 417.5173034667969 = 0.4352499544620514 + 50.0 * 8.341641426086426
Epoch 2170, val loss: 0.47058796882629395
Epoch 2180, training loss: 417.5021057128906 = 0.4340013265609741 + 50.0 * 8.341361999511719
Epoch 2180, val loss: 0.4694922864437103
Epoch 2190, training loss: 417.4886474609375 = 0.43279018998146057 + 50.0 * 8.341116905212402
Epoch 2190, val loss: 0.4684716463088989
Epoch 2200, training loss: 417.48834228515625 = 0.4315847158432007 + 50.0 * 8.341135025024414
Epoch 2200, val loss: 0.46742987632751465
Epoch 2210, training loss: 417.5893859863281 = 0.43035829067230225 + 50.0 * 8.343180656433105
Epoch 2210, val loss: 0.4663970172405243
Epoch 2220, training loss: 417.4914245605469 = 0.4290527403354645 + 50.0 * 8.34124755859375
Epoch 2220, val loss: 0.46541959047317505
Epoch 2230, training loss: 417.44049072265625 = 0.4279002547264099 + 50.0 * 8.340251922607422
Epoch 2230, val loss: 0.46439123153686523
Epoch 2240, training loss: 417.4343566894531 = 0.42675715684890747 + 50.0 * 8.3401517868042
Epoch 2240, val loss: 0.4634377360343933
Epoch 2250, training loss: 417.44873046875 = 0.4256105422973633 + 50.0 * 8.340462684631348
Epoch 2250, val loss: 0.4624902307987213
Epoch 2260, training loss: 417.51507568359375 = 0.424462229013443 + 50.0 * 8.341812133789062
Epoch 2260, val loss: 0.46150529384613037
Epoch 2270, training loss: 417.4212951660156 = 0.4232518672943115 + 50.0 * 8.339961051940918
Epoch 2270, val loss: 0.4606257379055023
Epoch 2280, training loss: 417.3942565917969 = 0.42216038703918457 + 50.0 * 8.339442253112793
Epoch 2280, val loss: 0.45965275168418884
Epoch 2290, training loss: 417.40087890625 = 0.4210662543773651 + 50.0 * 8.33959674835205
Epoch 2290, val loss: 0.45879292488098145
Epoch 2300, training loss: 417.42620849609375 = 0.4199575185775757 + 50.0 * 8.34012508392334
Epoch 2300, val loss: 0.457891583442688
Epoch 2310, training loss: 417.3752136230469 = 0.41889438033103943 + 50.0 * 8.339126586914062
Epoch 2310, val loss: 0.45702245831489563
Epoch 2320, training loss: 417.3454895019531 = 0.4178190529346466 + 50.0 * 8.338553428649902
Epoch 2320, val loss: 0.4561675190925598
Epoch 2330, training loss: 417.33642578125 = 0.4167913496494293 + 50.0 * 8.338393211364746
Epoch 2330, val loss: 0.45532330870628357
Epoch 2340, training loss: 417.4385986328125 = 0.41574132442474365 + 50.0 * 8.34045696258545
Epoch 2340, val loss: 0.45448538661003113
Epoch 2350, training loss: 417.3316345214844 = 0.4146467447280884 + 50.0 * 8.338339805603027
Epoch 2350, val loss: 0.4536675214767456
Epoch 2360, training loss: 417.3061828613281 = 0.4136635959148407 + 50.0 * 8.337850570678711
Epoch 2360, val loss: 0.45280030369758606
Epoch 2370, training loss: 417.29296875 = 0.4126705229282379 + 50.0 * 8.337606430053711
Epoch 2370, val loss: 0.4520748257637024
Epoch 2380, training loss: 417.292236328125 = 0.411708801984787 + 50.0 * 8.337610244750977
Epoch 2380, val loss: 0.4512759745121002
Epoch 2390, training loss: 417.32525634765625 = 0.41072267293930054 + 50.0 * 8.338290214538574
Epoch 2390, val loss: 0.45051535964012146
Epoch 2400, training loss: 417.2691345214844 = 0.4097733795642853 + 50.0 * 8.337187767028809
Epoch 2400, val loss: 0.44977930188179016
Epoch 2410, training loss: 417.28228759765625 = 0.4088312089443207 + 50.0 * 8.337469100952148
Epoch 2410, val loss: 0.4490221440792084
Epoch 2420, training loss: 417.29522705078125 = 0.40785032510757446 + 50.0 * 8.337747573852539
Epoch 2420, val loss: 0.4483042359352112
Epoch 2430, training loss: 417.2540283203125 = 0.40693947672843933 + 50.0 * 8.336941719055176
Epoch 2430, val loss: 0.44747814536094666
Epoch 2440, training loss: 417.2641296386719 = 0.4060315489768982 + 50.0 * 8.337162017822266
Epoch 2440, val loss: 0.4468218982219696
Epoch 2450, training loss: 417.2077941894531 = 0.4051000773906708 + 50.0 * 8.336053848266602
Epoch 2450, val loss: 0.44610396027565
Epoch 2460, training loss: 417.1985168457031 = 0.40420806407928467 + 50.0 * 8.335886001586914
Epoch 2460, val loss: 0.44542989134788513
Epoch 2470, training loss: 417.2011413574219 = 0.40335163474082947 + 50.0 * 8.335955619812012
Epoch 2470, val loss: 0.4447571337223053
Epoch 2480, training loss: 417.4661865234375 = 0.4024573862552643 + 50.0 * 8.34127426147461
Epoch 2480, val loss: 0.44403427839279175
Epoch 2490, training loss: 417.2632751464844 = 0.40144580602645874 + 50.0 * 8.337236404418945
Epoch 2490, val loss: 0.4433409869670868
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.830542871638762
0.8643048612620445
=== training gcn model ===
Epoch 0, training loss: 530.2250366210938 = 1.1125091314315796 + 50.0 * 10.58225154876709
Epoch 0, val loss: 1.1129812002182007
Epoch 10, training loss: 530.2003784179688 = 1.1076784133911133 + 50.0 * 10.581854820251465
Epoch 10, val loss: 1.1081504821777344
Epoch 20, training loss: 530.1193237304688 = 1.1025629043579102 + 50.0 * 10.58033561706543
Epoch 20, val loss: 1.1030470132827759
Epoch 30, training loss: 529.8043823242188 = 1.097033143043518 + 50.0 * 10.574146270751953
Epoch 30, val loss: 1.0975041389465332
Epoch 40, training loss: 528.60009765625 = 1.0907750129699707 + 50.0 * 10.550187110900879
Epoch 40, val loss: 1.0912038087844849
Epoch 50, training loss: 524.4985961914062 = 1.0836530923843384 + 50.0 * 10.468297958374023
Epoch 50, val loss: 1.0840548276901245
Epoch 60, training loss: 511.57623291015625 = 1.075982928276062 + 50.0 * 10.210004806518555
Epoch 60, val loss: 1.0763808488845825
Epoch 70, training loss: 483.76202392578125 = 1.0676801204681396 + 50.0 * 9.653886795043945
Epoch 70, val loss: 1.0680336952209473
Epoch 80, training loss: 470.0937805175781 = 1.060154676437378 + 50.0 * 9.380672454833984
Epoch 80, val loss: 1.0607694387435913
Epoch 90, training loss: 465.28436279296875 = 1.054497480392456 + 50.0 * 9.284597396850586
Epoch 90, val loss: 1.0552901029586792
Epoch 100, training loss: 462.9699401855469 = 1.0496066808700562 + 50.0 * 9.23840618133545
Epoch 100, val loss: 1.0505411624908447
Epoch 110, training loss: 461.6248779296875 = 1.0455906391143799 + 50.0 * 9.211585998535156
Epoch 110, val loss: 1.046659231185913
Epoch 120, training loss: 459.72100830078125 = 1.042649269104004 + 50.0 * 9.173566818237305
Epoch 120, val loss: 1.0438048839569092
Epoch 130, training loss: 457.019287109375 = 1.040545105934143 + 50.0 * 9.119574546813965
Epoch 130, val loss: 1.0417629480361938
Epoch 140, training loss: 453.28912353515625 = 1.039202332496643 + 50.0 * 9.044998168945312
Epoch 140, val loss: 1.0404667854309082
Epoch 150, training loss: 449.0670166015625 = 1.0384061336517334 + 50.0 * 8.960572242736816
Epoch 150, val loss: 1.0397005081176758
Epoch 160, training loss: 445.4617004394531 = 1.0377802848815918 + 50.0 * 8.88847827911377
Epoch 160, val loss: 1.0390496253967285
Epoch 170, training loss: 442.9040222167969 = 1.0368832349777222 + 50.0 * 8.837342262268066
Epoch 170, val loss: 1.0381182432174683
Epoch 180, training loss: 441.27886962890625 = 1.0359419584274292 + 50.0 * 8.804858207702637
Epoch 180, val loss: 1.0371732711791992
Epoch 190, training loss: 439.71453857421875 = 1.0351358652114868 + 50.0 * 8.773588180541992
Epoch 190, val loss: 1.0363914966583252
Epoch 200, training loss: 438.0488586425781 = 1.034696102142334 + 50.0 * 8.740283012390137
Epoch 200, val loss: 1.0359787940979004
Epoch 210, training loss: 436.7880554199219 = 1.0345975160598755 + 50.0 * 8.715068817138672
Epoch 210, val loss: 1.0358800888061523
Epoch 220, training loss: 435.7491149902344 = 1.0344692468643188 + 50.0 * 8.694293022155762
Epoch 220, val loss: 1.0356768369674683
Epoch 230, training loss: 434.5950622558594 = 1.0340335369110107 + 50.0 * 8.671220779418945
Epoch 230, val loss: 1.0352554321289062
Epoch 240, training loss: 433.6592102050781 = 1.0335606336593628 + 50.0 * 8.652512550354004
Epoch 240, val loss: 1.0347851514816284
Epoch 250, training loss: 432.861572265625 = 1.0329933166503906 + 50.0 * 8.636571884155273
Epoch 250, val loss: 1.0342079401016235
Epoch 260, training loss: 432.06732177734375 = 1.0323277711868286 + 50.0 * 8.620699882507324
Epoch 260, val loss: 1.033570647239685
Epoch 270, training loss: 431.2890625 = 1.031790852546692 + 50.0 * 8.605145454406738
Epoch 270, val loss: 1.0330893993377686
Epoch 280, training loss: 430.37353515625 = 1.0313963890075684 + 50.0 * 8.58684253692627
Epoch 280, val loss: 1.0327011346817017
Epoch 290, training loss: 429.6435852050781 = 1.0309993028640747 + 50.0 * 8.572251319885254
Epoch 290, val loss: 1.0322904586791992
Epoch 300, training loss: 429.0149841308594 = 1.030429482460022 + 50.0 * 8.559691429138184
Epoch 300, val loss: 1.0317318439483643
Epoch 310, training loss: 428.5327453613281 = 1.0296920537948608 + 50.0 * 8.550061225891113
Epoch 310, val loss: 1.031009554862976
Epoch 320, training loss: 428.093017578125 = 1.0288617610931396 + 50.0 * 8.541282653808594
Epoch 320, val loss: 1.0301967859268188
Epoch 330, training loss: 427.6380310058594 = 1.0280466079711914 + 50.0 * 8.53219985961914
Epoch 330, val loss: 1.0294172763824463
Epoch 340, training loss: 427.2376403808594 = 1.0272822380065918 + 50.0 * 8.52420711517334
Epoch 340, val loss: 1.0286657810211182
Epoch 350, training loss: 426.84674072265625 = 1.0265196561813354 + 50.0 * 8.516404151916504
Epoch 350, val loss: 1.0279195308685303
Epoch 360, training loss: 426.51348876953125 = 1.0257307291030884 + 50.0 * 8.50975513458252
Epoch 360, val loss: 1.0271415710449219
Epoch 370, training loss: 426.2212219238281 = 1.0248678922653198 + 50.0 * 8.503927230834961
Epoch 370, val loss: 1.026282787322998
Epoch 380, training loss: 425.91754150390625 = 1.0239906311035156 + 50.0 * 8.497871398925781
Epoch 380, val loss: 1.0254428386688232
Epoch 390, training loss: 425.6299133300781 = 1.0231163501739502 + 50.0 * 8.492136001586914
Epoch 390, val loss: 1.0245903730392456
Epoch 400, training loss: 425.3945617675781 = 1.0221927165985107 + 50.0 * 8.487447738647461
Epoch 400, val loss: 1.0236979722976685
Epoch 410, training loss: 425.2334289550781 = 1.0212010145187378 + 50.0 * 8.484244346618652
Epoch 410, val loss: 1.0227056741714478
Epoch 420, training loss: 424.9690856933594 = 1.0201295614242554 + 50.0 * 8.478979110717773
Epoch 420, val loss: 1.0216906070709229
Epoch 430, training loss: 424.7544250488281 = 1.0190550088882446 + 50.0 * 8.47470760345459
Epoch 430, val loss: 1.0206421613693237
Epoch 440, training loss: 424.6063232421875 = 1.0179282426834106 + 50.0 * 8.471768379211426
Epoch 440, val loss: 1.019538164138794
Epoch 450, training loss: 424.4533386230469 = 1.0166929960250854 + 50.0 * 8.468732833862305
Epoch 450, val loss: 1.0183744430541992
Epoch 460, training loss: 424.3099365234375 = 1.0154310464859009 + 50.0 * 8.465889930725098
Epoch 460, val loss: 1.017133355140686
Epoch 470, training loss: 424.1568603515625 = 1.0141181945800781 + 50.0 * 8.462854385375977
Epoch 470, val loss: 1.0158747434616089
Epoch 480, training loss: 424.0287780761719 = 1.0127794742584229 + 50.0 * 8.460319519042969
Epoch 480, val loss: 1.0145758390426636
Epoch 490, training loss: 423.9651184082031 = 1.0114084482192993 + 50.0 * 8.459074020385742
Epoch 490, val loss: 1.0132659673690796
Epoch 500, training loss: 423.80401611328125 = 1.009913444519043 + 50.0 * 8.45588207244873
Epoch 500, val loss: 1.0117952823638916
Epoch 510, training loss: 423.6908874511719 = 1.008439302444458 + 50.0 * 8.453648567199707
Epoch 510, val loss: 1.010385274887085
Epoch 520, training loss: 423.5247802734375 = 1.0070207118988037 + 50.0 * 8.450355529785156
Epoch 520, val loss: 1.0090057849884033
Epoch 530, training loss: 423.3974304199219 = 1.0055729150772095 + 50.0 * 8.447836875915527
Epoch 530, val loss: 1.0076026916503906
Epoch 540, training loss: 423.2828674316406 = 1.0040929317474365 + 50.0 * 8.445575714111328
Epoch 540, val loss: 1.0061742067337036
Epoch 550, training loss: 423.2531433105469 = 1.002495527267456 + 50.0 * 8.445013046264648
Epoch 550, val loss: 1.0045745372772217
Epoch 560, training loss: 423.0897521972656 = 1.0008407831192017 + 50.0 * 8.441778182983398
Epoch 560, val loss: 1.002987265586853
Epoch 570, training loss: 422.9206237792969 = 0.9992236495018005 + 50.0 * 8.438427925109863
Epoch 570, val loss: 1.0014315843582153
Epoch 580, training loss: 422.82049560546875 = 0.9975726008415222 + 50.0 * 8.436458587646484
Epoch 580, val loss: 0.9998205900192261
Epoch 590, training loss: 422.73785400390625 = 0.9958617091178894 + 50.0 * 8.434840202331543
Epoch 590, val loss: 0.9981722235679626
Epoch 600, training loss: 422.6988220214844 = 0.9940515756607056 + 50.0 * 8.43409538269043
Epoch 600, val loss: 0.9963394999504089
Epoch 610, training loss: 422.54803466796875 = 0.9921896457672119 + 50.0 * 8.431117057800293
Epoch 610, val loss: 0.9945625066757202
Epoch 620, training loss: 422.4334716796875 = 0.990356981754303 + 50.0 * 8.428862571716309
Epoch 620, val loss: 0.9927876591682434
Epoch 630, training loss: 422.332763671875 = 0.9884855151176453 + 50.0 * 8.426885604858398
Epoch 630, val loss: 0.9909606575965881
Epoch 640, training loss: 422.2362365722656 = 0.9865692853927612 + 50.0 * 8.424993515014648
Epoch 640, val loss: 0.9891074299812317
Epoch 650, training loss: 422.1434631347656 = 0.9846168160438538 + 50.0 * 8.423176765441895
Epoch 650, val loss: 0.9872148633003235
Epoch 660, training loss: 422.1047058105469 = 0.9825608134269714 + 50.0 * 8.422442436218262
Epoch 660, val loss: 0.9851950407028198
Epoch 670, training loss: 421.9972229003906 = 0.9804313778877258 + 50.0 * 8.42033576965332
Epoch 670, val loss: 0.9831154942512512
Epoch 680, training loss: 421.9057922363281 = 0.9783046245574951 + 50.0 * 8.418549537658691
Epoch 680, val loss: 0.9810313582420349
Epoch 690, training loss: 421.7726745605469 = 0.9761207699775696 + 50.0 * 8.41593074798584
Epoch 690, val loss: 0.9789541959762573
Epoch 700, training loss: 421.73779296875 = 0.9739007353782654 + 50.0 * 8.415277481079102
Epoch 700, val loss: 0.9768187403678894
Epoch 710, training loss: 421.58062744140625 = 0.9715924859046936 + 50.0 * 8.41218090057373
Epoch 710, val loss: 0.9745290279388428
Epoch 720, training loss: 421.5278015136719 = 0.9692538976669312 + 50.0 * 8.411170959472656
Epoch 720, val loss: 0.972230851650238
Epoch 730, training loss: 421.4508056640625 = 0.9668052196502686 + 50.0 * 8.409680366516113
Epoch 730, val loss: 0.9698688387870789
Epoch 740, training loss: 421.376953125 = 0.9642300009727478 + 50.0 * 8.408254623413086
Epoch 740, val loss: 0.9673833250999451
Epoch 750, training loss: 421.2881774902344 = 0.9615933299064636 + 50.0 * 8.406532287597656
Epoch 750, val loss: 0.9647825956344604
Epoch 760, training loss: 421.1893005371094 = 0.9589130878448486 + 50.0 * 8.404607772827148
Epoch 760, val loss: 0.9621654748916626
Epoch 770, training loss: 421.1156311035156 = 0.956134557723999 + 50.0 * 8.403189659118652
Epoch 770, val loss: 0.9594695568084717
Epoch 780, training loss: 421.0528869628906 = 0.9532948136329651 + 50.0 * 8.401991844177246
Epoch 780, val loss: 0.9567059278488159
Epoch 790, training loss: 421.00152587890625 = 0.9503298997879028 + 50.0 * 8.401023864746094
Epoch 790, val loss: 0.9537978768348694
Epoch 800, training loss: 420.8836975097656 = 0.9472495913505554 + 50.0 * 8.39872932434082
Epoch 800, val loss: 0.9508512020111084
Epoch 810, training loss: 420.8154602050781 = 0.9441723227500916 + 50.0 * 8.397425651550293
Epoch 810, val loss: 0.9478663802146912
Epoch 820, training loss: 420.7623291015625 = 0.9410063624382019 + 50.0 * 8.3964262008667
Epoch 820, val loss: 0.944797933101654
Epoch 830, training loss: 420.7936706542969 = 0.9376400709152222 + 50.0 * 8.397120475769043
Epoch 830, val loss: 0.9415378570556641
Epoch 840, training loss: 420.630859375 = 0.9342281222343445 + 50.0 * 8.393932342529297
Epoch 840, val loss: 0.938179612159729
Epoch 850, training loss: 420.5758972167969 = 0.9308266043663025 + 50.0 * 8.392901420593262
Epoch 850, val loss: 0.9348599910736084
Epoch 860, training loss: 420.4943542480469 = 0.9273407459259033 + 50.0 * 8.391340255737305
Epoch 860, val loss: 0.931476354598999
Epoch 870, training loss: 420.54486083984375 = 0.9237794876098633 + 50.0 * 8.39242172241211
Epoch 870, val loss: 0.927996814250946
Epoch 880, training loss: 420.4656066894531 = 0.919955313205719 + 50.0 * 8.390913009643555
Epoch 880, val loss: 0.9243041276931763
Epoch 890, training loss: 420.3787536621094 = 0.9161542057991028 + 50.0 * 8.389251708984375
Epoch 890, val loss: 0.9205856919288635
Epoch 900, training loss: 420.297607421875 = 0.9123473167419434 + 50.0 * 8.387704849243164
Epoch 900, val loss: 0.9168840050697327
Epoch 910, training loss: 420.2590026855469 = 0.9084745049476624 + 50.0 * 8.38701057434082
Epoch 910, val loss: 0.9130941033363342
Epoch 920, training loss: 420.20892333984375 = 0.904500424861908 + 50.0 * 8.386088371276855
Epoch 920, val loss: 0.9092527627944946
Epoch 930, training loss: 420.16229248046875 = 0.9004536867141724 + 50.0 * 8.385236740112305
Epoch 930, val loss: 0.9053924679756165
Epoch 940, training loss: 420.0833435058594 = 0.8963474035263062 + 50.0 * 8.383739471435547
Epoch 940, val loss: 0.9013844728469849
Epoch 950, training loss: 420.04144287109375 = 0.892244815826416 + 50.0 * 8.382984161376953
Epoch 950, val loss: 0.8973598480224609
Epoch 960, training loss: 420.0223388671875 = 0.888049304485321 + 50.0 * 8.382685661315918
Epoch 960, val loss: 0.8932694792747498
Epoch 970, training loss: 420.0563659667969 = 0.8837108612060547 + 50.0 * 8.383453369140625
Epoch 970, val loss: 0.889132559299469
Epoch 980, training loss: 419.9493103027344 = 0.8792096376419067 + 50.0 * 8.381402015686035
Epoch 980, val loss: 0.8847619891166687
Epoch 990, training loss: 419.8743896484375 = 0.8748083114624023 + 50.0 * 8.37999153137207
Epoch 990, val loss: 0.8805336356163025
Epoch 1000, training loss: 419.82489013671875 = 0.8704215288162231 + 50.0 * 8.37908935546875
Epoch 1000, val loss: 0.8762284517288208
Epoch 1010, training loss: 419.7774353027344 = 0.8659476041793823 + 50.0 * 8.378230094909668
Epoch 1010, val loss: 0.8719602227210999
Epoch 1020, training loss: 419.7454528808594 = 0.8614438772201538 + 50.0 * 8.377679824829102
Epoch 1020, val loss: 0.8675790429115295
Epoch 1030, training loss: 419.801025390625 = 0.8567647337913513 + 50.0 * 8.378885269165039
Epoch 1030, val loss: 0.8630991578102112
Epoch 1040, training loss: 419.6918029785156 = 0.8520335555076599 + 50.0 * 8.376795768737793
Epoch 1040, val loss: 0.858465850353241
Epoch 1050, training loss: 419.6224365234375 = 0.8473140597343445 + 50.0 * 8.375502586364746
Epoch 1050, val loss: 0.8539255261421204
Epoch 1060, training loss: 419.5859375 = 0.8425696492195129 + 50.0 * 8.37486743927002
Epoch 1060, val loss: 0.8493245840072632
Epoch 1070, training loss: 419.65789794921875 = 0.8377600908279419 + 50.0 * 8.376402854919434
Epoch 1070, val loss: 0.8445991277694702
Epoch 1080, training loss: 419.5603942871094 = 0.8325883150100708 + 50.0 * 8.374556541442871
Epoch 1080, val loss: 0.8397701978683472
Epoch 1090, training loss: 419.5252685546875 = 0.8276596665382385 + 50.0 * 8.37395191192627
Epoch 1090, val loss: 0.8348900675773621
Epoch 1100, training loss: 419.4536437988281 = 0.8226750493049622 + 50.0 * 8.37261962890625
Epoch 1100, val loss: 0.8301669955253601
Epoch 1110, training loss: 419.4252014160156 = 0.8176736831665039 + 50.0 * 8.372150421142578
Epoch 1110, val loss: 0.8253455758094788
Epoch 1120, training loss: 419.45648193359375 = 0.812578022480011 + 50.0 * 8.372878074645996
Epoch 1120, val loss: 0.8204165697097778
Epoch 1130, training loss: 419.3692932128906 = 0.8073954582214355 + 50.0 * 8.371237754821777
Epoch 1130, val loss: 0.8154492378234863
Epoch 1140, training loss: 419.32989501953125 = 0.8022477626800537 + 50.0 * 8.370553016662598
Epoch 1140, val loss: 0.8104720115661621
Epoch 1150, training loss: 419.3242492675781 = 0.7970210909843445 + 50.0 * 8.37054443359375
Epoch 1150, val loss: 0.80550616979599
Epoch 1160, training loss: 419.2938232421875 = 0.7916747331619263 + 50.0 * 8.37004280090332
Epoch 1160, val loss: 0.8002750873565674
Epoch 1170, training loss: 419.26422119140625 = 0.7863037586212158 + 50.0 * 8.369558334350586
Epoch 1170, val loss: 0.7951959371566772
Epoch 1180, training loss: 419.21636962890625 = 0.7810243368148804 + 50.0 * 8.368706703186035
Epoch 1180, val loss: 0.7900890707969666
Epoch 1190, training loss: 419.1861572265625 = 0.7757230997085571 + 50.0 * 8.368208885192871
Epoch 1190, val loss: 0.7849922776222229
Epoch 1200, training loss: 419.2834777832031 = 0.7704135179519653 + 50.0 * 8.370261192321777
Epoch 1200, val loss: 0.7798324823379517
Epoch 1210, training loss: 419.21148681640625 = 0.7648527026176453 + 50.0 * 8.368932723999023
Epoch 1210, val loss: 0.7745487689971924
Epoch 1220, training loss: 419.1117248535156 = 0.7594661116600037 + 50.0 * 8.367045402526855
Epoch 1220, val loss: 0.7693892121315002
Epoch 1230, training loss: 419.0711975097656 = 0.754117488861084 + 50.0 * 8.366341590881348
Epoch 1230, val loss: 0.7643174529075623
Epoch 1240, training loss: 419.06829833984375 = 0.7487741112709045 + 50.0 * 8.366390228271484
Epoch 1240, val loss: 0.7592344284057617
Epoch 1250, training loss: 419.1289367675781 = 0.7432963848114014 + 50.0 * 8.36771297454834
Epoch 1250, val loss: 0.7539952993392944
Epoch 1260, training loss: 419.02728271484375 = 0.737910807132721 + 50.0 * 8.365787506103516
Epoch 1260, val loss: 0.7487300038337708
Epoch 1270, training loss: 418.9730224609375 = 0.7325658202171326 + 50.0 * 8.364809036254883
Epoch 1270, val loss: 0.7437114119529724
Epoch 1280, training loss: 418.93505859375 = 0.7273103594779968 + 50.0 * 8.364154815673828
Epoch 1280, val loss: 0.7386330962181091
Epoch 1290, training loss: 418.9156494140625 = 0.7220221161842346 + 50.0 * 8.363872528076172
Epoch 1290, val loss: 0.7336103916168213
Epoch 1300, training loss: 419.0028991699219 = 0.7165996432304382 + 50.0 * 8.36572551727295
Epoch 1300, val loss: 0.7284055948257446
Epoch 1310, training loss: 418.871337890625 = 0.711159348487854 + 50.0 * 8.363204002380371
Epoch 1310, val loss: 0.7232871651649475
Epoch 1320, training loss: 418.831787109375 = 0.7059187293052673 + 50.0 * 8.362517356872559
Epoch 1320, val loss: 0.7182566523551941
Epoch 1330, training loss: 418.8036804199219 = 0.7007193565368652 + 50.0 * 8.362059593200684
Epoch 1330, val loss: 0.7132967114448547
Epoch 1340, training loss: 418.80035400390625 = 0.6955632567405701 + 50.0 * 8.362095832824707
Epoch 1340, val loss: 0.708358645439148
Epoch 1350, training loss: 418.8821105957031 = 0.6902468800544739 + 50.0 * 8.363837242126465
Epoch 1350, val loss: 0.7032434940338135
Epoch 1360, training loss: 418.7258605957031 = 0.6848893165588379 + 50.0 * 8.360819816589355
Epoch 1360, val loss: 0.698259711265564
Epoch 1370, training loss: 418.7068786621094 = 0.6798230409622192 + 50.0 * 8.360541343688965
Epoch 1370, val loss: 0.693504273891449
Epoch 1380, training loss: 418.6792907714844 = 0.6748455166816711 + 50.0 * 8.360089302062988
Epoch 1380, val loss: 0.6887118816375732
Epoch 1390, training loss: 418.6492919921875 = 0.6698534488677979 + 50.0 * 8.359588623046875
Epoch 1390, val loss: 0.6840264797210693
Epoch 1400, training loss: 418.6277160644531 = 0.6649150848388672 + 50.0 * 8.35925579071045
Epoch 1400, val loss: 0.6793069243431091
Epoch 1410, training loss: 418.8843994140625 = 0.6599072217941284 + 50.0 * 8.364489555358887
Epoch 1410, val loss: 0.6745132803916931
Epoch 1420, training loss: 418.5873107910156 = 0.6547585725784302 + 50.0 * 8.358651161193848
Epoch 1420, val loss: 0.6696934103965759
Epoch 1430, training loss: 418.5677795410156 = 0.6498852968215942 + 50.0 * 8.358358383178711
Epoch 1430, val loss: 0.6651139855384827
Epoch 1440, training loss: 418.53997802734375 = 0.6451473832130432 + 50.0 * 8.35789680480957
Epoch 1440, val loss: 0.6606379747390747
Epoch 1450, training loss: 418.5083312988281 = 0.6404539346694946 + 50.0 * 8.3573579788208
Epoch 1450, val loss: 0.6562113165855408
Epoch 1460, training loss: 418.52587890625 = 0.6357685923576355 + 50.0 * 8.357802391052246
Epoch 1460, val loss: 0.6518341302871704
Epoch 1470, training loss: 418.478271484375 = 0.6310129165649414 + 50.0 * 8.356945037841797
Epoch 1470, val loss: 0.6472965478897095
Epoch 1480, training loss: 418.4619140625 = 0.6263360381126404 + 50.0 * 8.356711387634277
Epoch 1480, val loss: 0.6429256200790405
Epoch 1490, training loss: 418.47674560546875 = 0.6217685341835022 + 50.0 * 8.357099533081055
Epoch 1490, val loss: 0.6385963559150696
Epoch 1500, training loss: 418.4253234863281 = 0.6172125935554504 + 50.0 * 8.356162071228027
Epoch 1500, val loss: 0.6343140006065369
Epoch 1510, training loss: 418.3813171386719 = 0.6128117442131042 + 50.0 * 8.355369567871094
Epoch 1510, val loss: 0.6301077604293823
Epoch 1520, training loss: 418.3857116699219 = 0.6084581613540649 + 50.0 * 8.355545043945312
Epoch 1520, val loss: 0.6259675025939941
Epoch 1530, training loss: 418.4234313964844 = 0.6040652990341187 + 50.0 * 8.3563871383667
Epoch 1530, val loss: 0.6217930316925049
Epoch 1540, training loss: 418.3247375488281 = 0.5996501445770264 + 50.0 * 8.354501724243164
Epoch 1540, val loss: 0.6177694797515869
Epoch 1550, training loss: 418.3089599609375 = 0.5954107046127319 + 50.0 * 8.354270935058594
Epoch 1550, val loss: 0.6137778162956238
Epoch 1560, training loss: 418.28143310546875 = 0.59125155210495 + 50.0 * 8.353803634643555
Epoch 1560, val loss: 0.6098447442054749
Epoch 1570, training loss: 418.3772277832031 = 0.5870989561080933 + 50.0 * 8.355802536010742
Epoch 1570, val loss: 0.6059755086898804
Epoch 1580, training loss: 418.29345703125 = 0.5828574299812317 + 50.0 * 8.354211807250977
Epoch 1580, val loss: 0.6019435524940491
Epoch 1590, training loss: 418.2705078125 = 0.5787696838378906 + 50.0 * 8.353835105895996
Epoch 1590, val loss: 0.5982128977775574
Epoch 1600, training loss: 418.2542419433594 = 0.5747266411781311 + 50.0 * 8.35359001159668
Epoch 1600, val loss: 0.5944131016731262
Epoch 1610, training loss: 418.21685791015625 = 0.5708264112472534 + 50.0 * 8.352920532226562
Epoch 1610, val loss: 0.5906417369842529
Epoch 1620, training loss: 418.2937316894531 = 0.566910982131958 + 50.0 * 8.354536056518555
Epoch 1620, val loss: 0.5870057344436646
Epoch 1630, training loss: 418.1847839355469 = 0.562977135181427 + 50.0 * 8.352436065673828
Epoch 1630, val loss: 0.5833755731582642
Epoch 1640, training loss: 418.1372985839844 = 0.5592797994613647 + 50.0 * 8.351560592651367
Epoch 1640, val loss: 0.5798741579055786
Epoch 1650, training loss: 418.1103210449219 = 0.5556092858314514 + 50.0 * 8.351094245910645
Epoch 1650, val loss: 0.5764552354812622
Epoch 1660, training loss: 418.09234619140625 = 0.5520071983337402 + 50.0 * 8.350807189941406
Epoch 1660, val loss: 0.5730810165405273
Epoch 1670, training loss: 418.111572265625 = 0.5484359860420227 + 50.0 * 8.351263046264648
Epoch 1670, val loss: 0.5696682929992676
Epoch 1680, training loss: 418.1476745605469 = 0.5447829365730286 + 50.0 * 8.352058410644531
Epoch 1680, val loss: 0.5662541389465332
Epoch 1690, training loss: 418.05206298828125 = 0.5410885810852051 + 50.0 * 8.3502197265625
Epoch 1690, val loss: 0.5628550052642822
Epoch 1700, training loss: 418.04498291015625 = 0.5376396775245667 + 50.0 * 8.350147247314453
Epoch 1700, val loss: 0.55973881483078
Epoch 1710, training loss: 418.0038146972656 = 0.5343092083930969 + 50.0 * 8.349390029907227
Epoch 1710, val loss: 0.5565692782402039
Epoch 1720, training loss: 417.9849548339844 = 0.5310302972793579 + 50.0 * 8.349078178405762
Epoch 1720, val loss: 0.553499162197113
Epoch 1730, training loss: 417.99884033203125 = 0.527792751789093 + 50.0 * 8.349420547485352
Epoch 1730, val loss: 0.5504497289657593
Epoch 1740, training loss: 418.1247253417969 = 0.5244678854942322 + 50.0 * 8.352005004882812
Epoch 1740, val loss: 0.5472856163978577
Epoch 1750, training loss: 417.9993896484375 = 0.5211029052734375 + 50.0 * 8.349565505981445
Epoch 1750, val loss: 0.5443008542060852
Epoch 1760, training loss: 417.9192810058594 = 0.517996609210968 + 50.0 * 8.34802532196045
Epoch 1760, val loss: 0.5413781404495239
Epoch 1770, training loss: 417.9023742675781 = 0.5149776935577393 + 50.0 * 8.347747802734375
Epoch 1770, val loss: 0.5385481715202332
Epoch 1780, training loss: 417.88519287109375 = 0.5120091438293457 + 50.0 * 8.347463607788086
Epoch 1780, val loss: 0.5358144044876099
Epoch 1790, training loss: 417.887451171875 = 0.5090673565864563 + 50.0 * 8.347567558288574
Epoch 1790, val loss: 0.5330569744110107
Epoch 1800, training loss: 417.9548645019531 = 0.5060834884643555 + 50.0 * 8.348976135253906
Epoch 1800, val loss: 0.5302746891975403
Epoch 1810, training loss: 417.9125061035156 = 0.5031665563583374 + 50.0 * 8.348186492919922
Epoch 1810, val loss: 0.5275757908821106
Epoch 1820, training loss: 417.83221435546875 = 0.5002633333206177 + 50.0 * 8.346638679504395
Epoch 1820, val loss: 0.524881899356842
Epoch 1830, training loss: 417.8138427734375 = 0.4974876940250397 + 50.0 * 8.34632682800293
Epoch 1830, val loss: 0.5223735570907593
Epoch 1840, training loss: 417.80120849609375 = 0.49475908279418945 + 50.0 * 8.346129417419434
Epoch 1840, val loss: 0.5198328495025635
Epoch 1850, training loss: 417.8257751464844 = 0.4920510947704315 + 50.0 * 8.346673965454102
Epoch 1850, val loss: 0.517370879650116
Epoch 1860, training loss: 417.82568359375 = 0.4893094003200531 + 50.0 * 8.34672737121582
Epoch 1860, val loss: 0.5148366093635559
Epoch 1870, training loss: 417.8067626953125 = 0.4867047965526581 + 50.0 * 8.34640121459961
Epoch 1870, val loss: 0.5123237371444702
Epoch 1880, training loss: 417.7626037597656 = 0.484051376581192 + 50.0 * 8.345571517944336
Epoch 1880, val loss: 0.5099374651908875
Epoch 1890, training loss: 417.7178649902344 = 0.4815341532230377 + 50.0 * 8.3447265625
Epoch 1890, val loss: 0.5076228380203247
Epoch 1900, training loss: 417.70501708984375 = 0.4790842533111572 + 50.0 * 8.344518661499023
Epoch 1900, val loss: 0.5053279399871826
Epoch 1910, training loss: 417.7063903808594 = 0.4766556918621063 + 50.0 * 8.344594955444336
Epoch 1910, val loss: 0.5030635595321655
Epoch 1920, training loss: 417.81195068359375 = 0.47423243522644043 + 50.0 * 8.34675407409668
Epoch 1920, val loss: 0.5007980465888977
Epoch 1930, training loss: 417.73406982421875 = 0.47171881794929504 + 50.0 * 8.345247268676758
Epoch 1930, val loss: 0.4985508918762207
Epoch 1940, training loss: 417.671630859375 = 0.469356894493103 + 50.0 * 8.344045639038086
Epoch 1940, val loss: 0.49633994698524475
Epoch 1950, training loss: 417.6453552246094 = 0.4670873284339905 + 50.0 * 8.343564987182617
Epoch 1950, val loss: 0.4942326545715332
Epoch 1960, training loss: 417.6295166015625 = 0.4648520350456238 + 50.0 * 8.343293190002441
Epoch 1960, val loss: 0.4922124445438385
Epoch 1970, training loss: 417.6571350097656 = 0.4626670181751251 + 50.0 * 8.343889236450195
Epoch 1970, val loss: 0.49017834663391113
Epoch 1980, training loss: 417.6492919921875 = 0.460438996553421 + 50.0 * 8.34377670288086
Epoch 1980, val loss: 0.4881831407546997
Epoch 1990, training loss: 417.62225341796875 = 0.4582619071006775 + 50.0 * 8.343279838562012
Epoch 1990, val loss: 0.48628395795822144
Epoch 2000, training loss: 417.59503173828125 = 0.4561752676963806 + 50.0 * 8.342777252197266
Epoch 2000, val loss: 0.48430052399635315
Epoch 2010, training loss: 417.5581970214844 = 0.4541287422180176 + 50.0 * 8.342081069946289
Epoch 2010, val loss: 0.4824121296405792
Epoch 2020, training loss: 417.5456237792969 = 0.45212841033935547 + 50.0 * 8.341870307922363
Epoch 2020, val loss: 0.4806080162525177
Epoch 2030, training loss: 417.5751037597656 = 0.4501573443412781 + 50.0 * 8.342498779296875
Epoch 2030, val loss: 0.47875964641571045
Epoch 2040, training loss: 417.5545959472656 = 0.44813939929008484 + 50.0 * 8.34212875366211
Epoch 2040, val loss: 0.47700920701026917
Epoch 2050, training loss: 417.5386047363281 = 0.4461997151374817 + 50.0 * 8.341848373413086
Epoch 2050, val loss: 0.475344181060791
Epoch 2060, training loss: 417.5960388183594 = 0.4442579746246338 + 50.0 * 8.343035697937012
Epoch 2060, val loss: 0.47354722023010254
Epoch 2070, training loss: 417.48431396484375 = 0.4424344301223755 + 50.0 * 8.340837478637695
Epoch 2070, val loss: 0.47183525562286377
Epoch 2080, training loss: 417.4515686035156 = 0.4406394362449646 + 50.0 * 8.340218544006348
Epoch 2080, val loss: 0.470191091299057
Epoch 2090, training loss: 417.4349670410156 = 0.4388846158981323 + 50.0 * 8.339921951293945
Epoch 2090, val loss: 0.46868252754211426
Epoch 2100, training loss: 417.4492492675781 = 0.4371516704559326 + 50.0 * 8.340241432189941
Epoch 2100, val loss: 0.46714550256729126
Epoch 2110, training loss: 417.4930114746094 = 0.43539273738861084 + 50.0 * 8.34115219116211
Epoch 2110, val loss: 0.46554040908813477
Epoch 2120, training loss: 417.5103454589844 = 0.4336385130882263 + 50.0 * 8.341534614562988
Epoch 2120, val loss: 0.46392107009887695
Epoch 2130, training loss: 417.4107360839844 = 0.4319440424442291 + 50.0 * 8.33957576751709
Epoch 2130, val loss: 0.4623876214027405
Epoch 2140, training loss: 417.43377685546875 = 0.43031495809555054 + 50.0 * 8.340068817138672
Epoch 2140, val loss: 0.46086952090263367
Epoch 2150, training loss: 417.37109375 = 0.4286538064479828 + 50.0 * 8.338849067687988
Epoch 2150, val loss: 0.45945990085601807
Epoch 2160, training loss: 417.34625244140625 = 0.42705726623535156 + 50.0 * 8.338383674621582
Epoch 2160, val loss: 0.45807650685310364
Epoch 2170, training loss: 417.32708740234375 = 0.42553576827049255 + 50.0 * 8.338030815124512
Epoch 2170, val loss: 0.45671361684799194
Epoch 2180, training loss: 417.31787109375 = 0.42402976751327515 + 50.0 * 8.337876319885254
Epoch 2180, val loss: 0.4553477168083191
Epoch 2190, training loss: 417.3643493652344 = 0.4225079417228699 + 50.0 * 8.338836669921875
Epoch 2190, val loss: 0.45400676131248474
Epoch 2200, training loss: 417.3467102050781 = 0.4209557771682739 + 50.0 * 8.338515281677246
Epoch 2200, val loss: 0.4527587294578552
Epoch 2210, training loss: 417.28778076171875 = 0.41946572065353394 + 50.0 * 8.337366104125977
Epoch 2210, val loss: 0.4512926936149597
Epoch 2220, training loss: 417.27703857421875 = 0.4180185794830322 + 50.0 * 8.337180137634277
Epoch 2220, val loss: 0.44998592138290405
Epoch 2230, training loss: 417.2593078613281 = 0.4166019558906555 + 50.0 * 8.336853981018066
Epoch 2230, val loss: 0.4488157629966736
Epoch 2240, training loss: 417.24822998046875 = 0.4152120053768158 + 50.0 * 8.336660385131836
Epoch 2240, val loss: 0.44755110144615173
Epoch 2250, training loss: 417.23614501953125 = 0.4138336181640625 + 50.0 * 8.336446762084961
Epoch 2250, val loss: 0.4464367628097534
Epoch 2260, training loss: 417.3445129394531 = 0.4124322533607483 + 50.0 * 8.338641166687012
Epoch 2260, val loss: 0.4451756179332733
Epoch 2270, training loss: 417.21923828125 = 0.41108018159866333 + 50.0 * 8.336163520812988
Epoch 2270, val loss: 0.4439152479171753
Epoch 2280, training loss: 417.1712951660156 = 0.40978527069091797 + 50.0 * 8.335229873657227
Epoch 2280, val loss: 0.4428340196609497
Epoch 2290, training loss: 417.1751708984375 = 0.40854138135910034 + 50.0 * 8.335332870483398
Epoch 2290, val loss: 0.44169631600379944
Epoch 2300, training loss: 417.3695373535156 = 0.40724653005599976 + 50.0 * 8.339245796203613
Epoch 2300, val loss: 0.44052693247795105
Epoch 2310, training loss: 417.1756896972656 = 0.4058595895767212 + 50.0 * 8.335396766662598
Epoch 2310, val loss: 0.43943002820014954
Epoch 2320, training loss: 417.1074523925781 = 0.40466246008872986 + 50.0 * 8.33405590057373
Epoch 2320, val loss: 0.4383547604084015
Epoch 2330, training loss: 417.09173583984375 = 0.4034876525402069 + 50.0 * 8.333765029907227
Epoch 2330, val loss: 0.4373796880245209
Epoch 2340, training loss: 417.0773010253906 = 0.40232786536216736 + 50.0 * 8.333499908447266
Epoch 2340, val loss: 0.43637776374816895
Epoch 2350, training loss: 417.09466552734375 = 0.4011782109737396 + 50.0 * 8.333869934082031
Epoch 2350, val loss: 0.4354234039783478
Epoch 2360, training loss: 417.16119384765625 = 0.39989036321640015 + 50.0 * 8.335226058959961
Epoch 2360, val loss: 0.4343995749950409
Epoch 2370, training loss: 417.0866394042969 = 0.3986984193325043 + 50.0 * 8.333758354187012
Epoch 2370, val loss: 0.4332309365272522
Epoch 2380, training loss: 417.03765869140625 = 0.397588312625885 + 50.0 * 8.332801818847656
Epoch 2380, val loss: 0.4323592185974121
Epoch 2390, training loss: 417.01715087890625 = 0.39652717113494873 + 50.0 * 8.332412719726562
Epoch 2390, val loss: 0.43145522475242615
Epoch 2400, training loss: 417.0111389160156 = 0.3954717516899109 + 50.0 * 8.332313537597656
Epoch 2400, val loss: 0.430581271648407
Epoch 2410, training loss: 417.0919189453125 = 0.39442238211631775 + 50.0 * 8.33395004272461
Epoch 2410, val loss: 0.4296959638595581
Epoch 2420, training loss: 417.10614013671875 = 0.3932812511920929 + 50.0 * 8.334257125854492
Epoch 2420, val loss: 0.4286087453365326
Epoch 2430, training loss: 416.9850158691406 = 0.3921748697757721 + 50.0 * 8.331856727600098
Epoch 2430, val loss: 0.42787450551986694
Epoch 2440, training loss: 416.96014404296875 = 0.3911561071872711 + 50.0 * 8.331379890441895
Epoch 2440, val loss: 0.42694777250289917
Epoch 2450, training loss: 416.9440002441406 = 0.3901820480823517 + 50.0 * 8.331076622009277
Epoch 2450, val loss: 0.42618176341056824
Epoch 2460, training loss: 416.93292236328125 = 0.38920313119888306 + 50.0 * 8.3308744430542
Epoch 2460, val loss: 0.42533472180366516
Epoch 2470, training loss: 416.957763671875 = 0.3882375955581665 + 50.0 * 8.331390380859375
Epoch 2470, val loss: 0.42451685667037964
Epoch 2480, training loss: 417.0337219238281 = 0.3872132897377014 + 50.0 * 8.332930564880371
Epoch 2480, val loss: 0.4236571788787842
Epoch 2490, training loss: 416.9413757324219 = 0.38617533445358276 + 50.0 * 8.331104278564453
Epoch 2490, val loss: 0.42297378182411194
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8376458650431253
0.8637977251322178
The final CL Acc:0.83511, 0.00324, The final GNN Acc:0.86472, 0.00096
Begin epxeriment: noisy_level: 0.2 cont_weight: 50 epoch: der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106170])
remove edge: torch.Size([2, 71076])
updated graph: torch.Size([2, 88598])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 530.2098388671875 = 1.0952085256576538 + 50.0 * 10.582292556762695
Epoch 0, val loss: 1.0946606397628784
Epoch 10, training loss: 530.1895751953125 = 1.091187834739685 + 50.0 * 10.5819673538208
Epoch 10, val loss: 1.0905855894088745
Epoch 20, training loss: 530.1096801757812 = 1.0866758823394775 + 50.0 * 10.580460548400879
Epoch 20, val loss: 1.0859944820404053
Epoch 30, training loss: 529.7433471679688 = 1.081605315208435 + 50.0 * 10.573234558105469
Epoch 30, val loss: 1.080849528312683
Epoch 40, training loss: 528.2676391601562 = 1.075979471206665 + 50.0 * 10.543832778930664
Epoch 40, val loss: 1.0751519203186035
Epoch 50, training loss: 523.9342041015625 = 1.0698988437652588 + 50.0 * 10.457286834716797
Epoch 50, val loss: 1.069104552268982
Epoch 60, training loss: 514.3363647460938 = 1.064759612083435 + 50.0 * 10.265432357788086
Epoch 60, val loss: 1.0641398429870605
Epoch 70, training loss: 503.8139953613281 = 1.0601483583450317 + 50.0 * 10.055076599121094
Epoch 70, val loss: 1.0597407817840576
Epoch 80, training loss: 491.2256774902344 = 1.0557466745376587 + 50.0 * 9.803398132324219
Epoch 80, val loss: 1.0555788278579712
Epoch 90, training loss: 473.1370544433594 = 1.0520864725112915 + 50.0 * 9.441699028015137
Epoch 90, val loss: 1.0522130727767944
Epoch 100, training loss: 465.7008056640625 = 1.0482287406921387 + 50.0 * 9.293051719665527
Epoch 100, val loss: 1.048520565032959
Epoch 110, training loss: 461.9736328125 = 1.0439977645874023 + 50.0 * 9.218592643737793
Epoch 110, val loss: 1.044359803199768
Epoch 120, training loss: 456.64630126953125 = 1.0399409532546997 + 50.0 * 9.112127304077148
Epoch 120, val loss: 1.040371298789978
Epoch 130, training loss: 452.57806396484375 = 1.0367472171783447 + 50.0 * 9.030826568603516
Epoch 130, val loss: 1.0373142957687378
Epoch 140, training loss: 449.8134460449219 = 1.0340747833251953 + 50.0 * 8.975586891174316
Epoch 140, val loss: 1.0346869230270386
Epoch 150, training loss: 447.17181396484375 = 1.0309423208236694 + 50.0 * 8.92281723022461
Epoch 150, val loss: 1.031637191772461
Epoch 160, training loss: 445.1666564941406 = 1.0276432037353516 + 50.0 * 8.882780075073242
Epoch 160, val loss: 1.0284664630889893
Epoch 170, training loss: 443.44012451171875 = 1.0244053602218628 + 50.0 * 8.84831428527832
Epoch 170, val loss: 1.0253324508666992
Epoch 180, training loss: 442.01617431640625 = 1.0209825038909912 + 50.0 * 8.819903373718262
Epoch 180, val loss: 1.0219632387161255
Epoch 190, training loss: 440.8344421386719 = 1.0172406435012817 + 50.0 * 8.796343803405762
Epoch 190, val loss: 1.018258810043335
Epoch 200, training loss: 439.79095458984375 = 1.0132935047149658 + 50.0 * 8.775552749633789
Epoch 200, val loss: 1.0143539905548096
Epoch 210, training loss: 438.92547607421875 = 1.009084939956665 + 50.0 * 8.75832748413086
Epoch 210, val loss: 1.0102282762527466
Epoch 220, training loss: 438.2033386230469 = 1.0045499801635742 + 50.0 * 8.743975639343262
Epoch 220, val loss: 1.0057355165481567
Epoch 230, training loss: 437.4591369628906 = 0.9996863603591919 + 50.0 * 8.729188919067383
Epoch 230, val loss: 1.0009558200836182
Epoch 240, training loss: 436.7195739746094 = 0.9945340156555176 + 50.0 * 8.714500427246094
Epoch 240, val loss: 0.9959040880203247
Epoch 250, training loss: 436.1561584472656 = 0.9890799522399902 + 50.0 * 8.703341484069824
Epoch 250, val loss: 0.9905664324760437
Epoch 260, training loss: 435.5303955078125 = 0.9832278490066528 + 50.0 * 8.690943717956543
Epoch 260, val loss: 0.9847906827926636
Epoch 270, training loss: 434.98492431640625 = 0.9768882393836975 + 50.0 * 8.680160522460938
Epoch 270, val loss: 0.9785624742507935
Epoch 280, training loss: 434.4484558105469 = 0.9700884819030762 + 50.0 * 8.669567108154297
Epoch 280, val loss: 0.9718989133834839
Epoch 290, training loss: 433.9907531738281 = 0.9628334045410156 + 50.0 * 8.660558700561523
Epoch 290, val loss: 0.9648125171661377
Epoch 300, training loss: 433.5812072753906 = 0.9551334381103516 + 50.0 * 8.652521133422852
Epoch 300, val loss: 0.9572227001190186
Epoch 310, training loss: 433.06707763671875 = 0.9469719529151917 + 50.0 * 8.642402648925781
Epoch 310, val loss: 0.9492773413658142
Epoch 320, training loss: 432.69195556640625 = 0.9383313059806824 + 50.0 * 8.635072708129883
Epoch 320, val loss: 0.940868079662323
Epoch 330, training loss: 432.67095947265625 = 0.9292932152748108 + 50.0 * 8.634833335876465
Epoch 330, val loss: 0.9320593476295471
Epoch 340, training loss: 432.1408386230469 = 0.9197263121604919 + 50.0 * 8.624422073364258
Epoch 340, val loss: 0.9227631092071533
Epoch 350, training loss: 431.8016052246094 = 0.9100192189216614 + 50.0 * 8.617831230163574
Epoch 350, val loss: 0.9133263826370239
Epoch 360, training loss: 431.51409912109375 = 0.9000628590583801 + 50.0 * 8.61228084564209
Epoch 360, val loss: 0.9037013053894043
Epoch 370, training loss: 431.2660827636719 = 0.8898815512657166 + 50.0 * 8.607523918151855
Epoch 370, val loss: 0.8938347697257996
Epoch 380, training loss: 431.273681640625 = 0.879515528678894 + 50.0 * 8.60788345336914
Epoch 380, val loss: 0.8838790655136108
Epoch 390, training loss: 430.7637939453125 = 0.8691070675849915 + 50.0 * 8.597893714904785
Epoch 390, val loss: 0.8738065361976624
Epoch 400, training loss: 430.5462341308594 = 0.8587281703948975 + 50.0 * 8.59375
Epoch 400, val loss: 0.8638197779655457
Epoch 410, training loss: 430.2635192871094 = 0.8483960628509521 + 50.0 * 8.588302612304688
Epoch 410, val loss: 0.8538914918899536
Epoch 420, training loss: 430.020751953125 = 0.8381407856941223 + 50.0 * 8.58365249633789
Epoch 420, val loss: 0.8440225720405579
Epoch 430, training loss: 429.8003845214844 = 0.8279221057891846 + 50.0 * 8.579449653625488
Epoch 430, val loss: 0.8342490196228027
Epoch 440, training loss: 429.56036376953125 = 0.8178073167800903 + 50.0 * 8.574851036071777
Epoch 440, val loss: 0.8245393633842468
Epoch 450, training loss: 429.3246765136719 = 0.8078677654266357 + 50.0 * 8.57033634185791
Epoch 450, val loss: 0.815057635307312
Epoch 460, training loss: 429.0984802246094 = 0.7981024384498596 + 50.0 * 8.566007614135742
Epoch 460, val loss: 0.8056853413581848
Epoch 470, training loss: 428.9865417480469 = 0.7884790897369385 + 50.0 * 8.563961029052734
Epoch 470, val loss: 0.7964507937431335
Epoch 480, training loss: 428.9285888671875 = 0.7789162397384644 + 50.0 * 8.562993049621582
Epoch 480, val loss: 0.7874156832695007
Epoch 490, training loss: 428.591064453125 = 0.7695509195327759 + 50.0 * 8.556429862976074
Epoch 490, val loss: 0.7783886194229126
Epoch 500, training loss: 428.3875732421875 = 0.7603744864463806 + 50.0 * 8.552543640136719
Epoch 500, val loss: 0.7696408629417419
Epoch 510, training loss: 428.2564392089844 = 0.7513742446899414 + 50.0 * 8.550101280212402
Epoch 510, val loss: 0.7610630989074707
Epoch 520, training loss: 428.21636962890625 = 0.7425535917282104 + 50.0 * 8.549476623535156
Epoch 520, val loss: 0.7526430487632751
Epoch 530, training loss: 428.0049133300781 = 0.7339271903038025 + 50.0 * 8.545419692993164
Epoch 530, val loss: 0.7443109154701233
Epoch 540, training loss: 427.8636169433594 = 0.7254969477653503 + 50.0 * 8.542762756347656
Epoch 540, val loss: 0.7363041639328003
Epoch 550, training loss: 427.7522277832031 = 0.7173119187355042 + 50.0 * 8.540698051452637
Epoch 550, val loss: 0.7284390330314636
Epoch 560, training loss: 427.87408447265625 = 0.7093509435653687 + 50.0 * 8.543294906616211
Epoch 560, val loss: 0.7207739949226379
Epoch 570, training loss: 427.619384765625 = 0.7015457153320312 + 50.0 * 8.53835678100586
Epoch 570, val loss: 0.7133768200874329
Epoch 580, training loss: 427.4659729003906 = 0.6940627098083496 + 50.0 * 8.535438537597656
Epoch 580, val loss: 0.706219494342804
Epoch 590, training loss: 427.360595703125 = 0.6868531107902527 + 50.0 * 8.533474922180176
Epoch 590, val loss: 0.6993170380592346
Epoch 600, training loss: 427.37615966796875 = 0.6799099445343018 + 50.0 * 8.53392505645752
Epoch 600, val loss: 0.6926072835922241
Epoch 610, training loss: 427.28070068359375 = 0.6731346845626831 + 50.0 * 8.532151222229004
Epoch 610, val loss: 0.6862878799438477
Epoch 620, training loss: 427.11279296875 = 0.6666811108589172 + 50.0 * 8.528922080993652
Epoch 620, val loss: 0.680050253868103
Epoch 630, training loss: 427.0447692871094 = 0.6605269908905029 + 50.0 * 8.527685165405273
Epoch 630, val loss: 0.6741572618484497
Epoch 640, training loss: 426.9613037109375 = 0.654627799987793 + 50.0 * 8.52613353729248
Epoch 640, val loss: 0.6685856580734253
Epoch 650, training loss: 426.9522705078125 = 0.6489956378936768 + 50.0 * 8.526065826416016
Epoch 650, val loss: 0.663186252117157
Epoch 660, training loss: 426.9837646484375 = 0.6435450315475464 + 50.0 * 8.526803970336914
Epoch 660, val loss: 0.6580402255058289
Epoch 670, training loss: 426.8164978027344 = 0.6383531093597412 + 50.0 * 8.52356243133545
Epoch 670, val loss: 0.6531376838684082
Epoch 680, training loss: 426.7103576660156 = 0.6334333419799805 + 50.0 * 8.521538734436035
Epoch 680, val loss: 0.6485031247138977
Epoch 690, training loss: 426.6376037597656 = 0.6287631392478943 + 50.0 * 8.520176887512207
Epoch 690, val loss: 0.6440995335578918
Epoch 700, training loss: 427.00006103515625 = 0.6243220567703247 + 50.0 * 8.527514457702637
Epoch 700, val loss: 0.6398372054100037
Epoch 710, training loss: 426.5411376953125 = 0.6199882626533508 + 50.0 * 8.518423080444336
Epoch 710, val loss: 0.6358938813209534
Epoch 720, training loss: 426.50067138671875 = 0.6159504652023315 + 50.0 * 8.517694473266602
Epoch 720, val loss: 0.6321396827697754
Epoch 730, training loss: 426.42913818359375 = 0.6121319532394409 + 50.0 * 8.516340255737305
Epoch 730, val loss: 0.6286136507987976
Epoch 740, training loss: 426.35614013671875 = 0.6085106134414673 + 50.0 * 8.514952659606934
Epoch 740, val loss: 0.6252851486206055
Epoch 750, training loss: 426.31817626953125 = 0.6050657629966736 + 50.0 * 8.514262199401855
Epoch 750, val loss: 0.6221574544906616
Epoch 760, training loss: 426.46636962890625 = 0.6017648577690125 + 50.0 * 8.517292022705078
Epoch 760, val loss: 0.6192066073417664
Epoch 770, training loss: 426.21270751953125 = 0.5986098051071167 + 50.0 * 8.512282371520996
Epoch 770, val loss: 0.6163169145584106
Epoch 780, training loss: 426.1749572753906 = 0.5956377387046814 + 50.0 * 8.51158618927002
Epoch 780, val loss: 0.613615095615387
Epoch 790, training loss: 426.1021423339844 = 0.5928193926811218 + 50.0 * 8.510186195373535
Epoch 790, val loss: 0.6111445426940918
Epoch 800, training loss: 426.04620361328125 = 0.5901420712471008 + 50.0 * 8.50912094116211
Epoch 800, val loss: 0.608806848526001
Epoch 810, training loss: 426.00958251953125 = 0.5875949263572693 + 50.0 * 8.508440017700195
Epoch 810, val loss: 0.6066204309463501
Epoch 820, training loss: 426.0191955566406 = 0.5851448774337769 + 50.0 * 8.508681297302246
Epoch 820, val loss: 0.6044500470161438
Epoch 830, training loss: 425.9924621582031 = 0.5827770233154297 + 50.0 * 8.508193969726562
Epoch 830, val loss: 0.6024799346923828
Epoch 840, training loss: 425.88336181640625 = 0.5805344581604004 + 50.0 * 8.506056785583496
Epoch 840, val loss: 0.6006864309310913
Epoch 850, training loss: 425.806396484375 = 0.5784270167350769 + 50.0 * 8.504559516906738
Epoch 850, val loss: 0.5988919138908386
Epoch 860, training loss: 425.7423095703125 = 0.5764275193214417 + 50.0 * 8.503317832946777
Epoch 860, val loss: 0.5972526669502258
Epoch 870, training loss: 425.8213806152344 = 0.574510395526886 + 50.0 * 8.504937171936035
Epoch 870, val loss: 0.5957568287849426
Epoch 880, training loss: 425.825927734375 = 0.5726178884506226 + 50.0 * 8.50506591796875
Epoch 880, val loss: 0.5942203998565674
Epoch 890, training loss: 425.59783935546875 = 0.5708122849464417 + 50.0 * 8.500540733337402
Epoch 890, val loss: 0.592782199382782
Epoch 900, training loss: 425.5641174316406 = 0.5691258907318115 + 50.0 * 8.499899864196777
Epoch 900, val loss: 0.5914890766143799
Epoch 910, training loss: 425.49676513671875 = 0.5675255656242371 + 50.0 * 8.498584747314453
Epoch 910, val loss: 0.5902872681617737
Epoch 920, training loss: 425.5201416015625 = 0.5659868121147156 + 50.0 * 8.499083518981934
Epoch 920, val loss: 0.589214026927948
Epoch 930, training loss: 425.4820556640625 = 0.5644675493240356 + 50.0 * 8.49835205078125
Epoch 930, val loss: 0.5880085825920105
Epoch 940, training loss: 425.3925476074219 = 0.5630232691764832 + 50.0 * 8.496590614318848
Epoch 940, val loss: 0.5869556069374084
Epoch 950, training loss: 425.3235168457031 = 0.5616475939750671 + 50.0 * 8.495237350463867
Epoch 950, val loss: 0.5859665274620056
Epoch 960, training loss: 425.26812744140625 = 0.5603364706039429 + 50.0 * 8.494155883789062
Epoch 960, val loss: 0.5850921273231506
Epoch 970, training loss: 425.23699951171875 = 0.5590635538101196 + 50.0 * 8.493558883666992
Epoch 970, val loss: 0.5842236280441284
Epoch 980, training loss: 425.48297119140625 = 0.5578206777572632 + 50.0 * 8.498502731323242
Epoch 980, val loss: 0.5834693312644958
Epoch 990, training loss: 425.2355651855469 = 0.5565831065177917 + 50.0 * 8.493579864501953
Epoch 990, val loss: 0.5824899077415466
Epoch 1000, training loss: 425.1076354980469 = 0.555419921875 + 50.0 * 8.491044044494629
Epoch 1000, val loss: 0.5817582607269287
Epoch 1010, training loss: 425.0622863769531 = 0.5543125867843628 + 50.0 * 8.490159034729004
Epoch 1010, val loss: 0.5810638666152954
Epoch 1020, training loss: 425.0409240722656 = 0.5532433986663818 + 50.0 * 8.489753723144531
Epoch 1020, val loss: 0.5803711414337158
Epoch 1030, training loss: 425.14215087890625 = 0.55218905210495 + 50.0 * 8.491799354553223
Epoch 1030, val loss: 0.5796723961830139
Epoch 1040, training loss: 425.023681640625 = 0.5511404275894165 + 50.0 * 8.489450454711914
Epoch 1040, val loss: 0.5791155099868774
Epoch 1050, training loss: 424.9442138671875 = 0.5501407384872437 + 50.0 * 8.487881660461426
Epoch 1050, val loss: 0.5785655379295349
Epoch 1060, training loss: 424.9858093261719 = 0.5491810441017151 + 50.0 * 8.48873233795166
Epoch 1060, val loss: 0.5780156254768372
Epoch 1070, training loss: 424.9314880371094 = 0.5482251644134521 + 50.0 * 8.487665176391602
Epoch 1070, val loss: 0.5774367451667786
Epoch 1080, training loss: 424.8526916503906 = 0.5473072528839111 + 50.0 * 8.48610782623291
Epoch 1080, val loss: 0.5769103765487671
Epoch 1090, training loss: 424.8060302734375 = 0.5464202761650085 + 50.0 * 8.48519229888916
Epoch 1090, val loss: 0.5764318704605103
Epoch 1100, training loss: 424.83154296875 = 0.5455581545829773 + 50.0 * 8.485719680786133
Epoch 1100, val loss: 0.575914740562439
Epoch 1110, training loss: 424.8048095703125 = 0.5446872711181641 + 50.0 * 8.48520278930664
Epoch 1110, val loss: 0.5754852890968323
Epoch 1120, training loss: 424.72991943359375 = 0.5438350439071655 + 50.0 * 8.483721733093262
Epoch 1120, val loss: 0.5750528573989868
Epoch 1130, training loss: 424.7097473144531 = 0.5430358648300171 + 50.0 * 8.4833345413208
Epoch 1130, val loss: 0.5747149586677551
Epoch 1140, training loss: 424.67279052734375 = 0.5422616600990295 + 50.0 * 8.482610702514648
Epoch 1140, val loss: 0.5743168592453003
Epoch 1150, training loss: 424.7257385253906 = 0.5415098071098328 + 50.0 * 8.483684539794922
Epoch 1150, val loss: 0.574025571346283
Epoch 1160, training loss: 424.6189270019531 = 0.5407050848007202 + 50.0 * 8.48156452178955
Epoch 1160, val loss: 0.5735330581665039
Epoch 1170, training loss: 424.7023010253906 = 0.5399271249771118 + 50.0 * 8.483247756958008
Epoch 1170, val loss: 0.5732327103614807
Epoch 1180, training loss: 424.5696716308594 = 0.5392020344734192 + 50.0 * 8.480608940124512
Epoch 1180, val loss: 0.5728673338890076
Epoch 1190, training loss: 424.5582275390625 = 0.5385071635246277 + 50.0 * 8.48039436340332
Epoch 1190, val loss: 0.5725661516189575
Epoch 1200, training loss: 424.5205383300781 = 0.5378226041793823 + 50.0 * 8.479654312133789
Epoch 1200, val loss: 0.5722863078117371
Epoch 1210, training loss: 424.49664306640625 = 0.537147045135498 + 50.0 * 8.4791898727417
Epoch 1210, val loss: 0.5720245242118835
Epoch 1220, training loss: 424.5431213378906 = 0.5364776253700256 + 50.0 * 8.480133056640625
Epoch 1220, val loss: 0.5717735886573792
Epoch 1230, training loss: 424.5695495605469 = 0.5357710719108582 + 50.0 * 8.48067569732666
Epoch 1230, val loss: 0.5714877843856812
Epoch 1240, training loss: 424.4404296875 = 0.5350738167762756 + 50.0 * 8.478107452392578
Epoch 1240, val loss: 0.5711334347724915
Epoch 1250, training loss: 424.4524230957031 = 0.5344175696372986 + 50.0 * 8.478360176086426
Epoch 1250, val loss: 0.5708736777305603
Epoch 1260, training loss: 424.4926452636719 = 0.5337705612182617 + 50.0 * 8.479177474975586
Epoch 1260, val loss: 0.5706478953361511
Epoch 1270, training loss: 424.42529296875 = 0.5331331491470337 + 50.0 * 8.477843284606934
Epoch 1270, val loss: 0.5704727172851562
Epoch 1280, training loss: 424.3651123046875 = 0.5324972867965698 + 50.0 * 8.476652145385742
Epoch 1280, val loss: 0.5701898336410522
Epoch 1290, training loss: 424.3630065917969 = 0.5318769216537476 + 50.0 * 8.476622581481934
Epoch 1290, val loss: 0.5699894428253174
Epoch 1300, training loss: 424.4545593261719 = 0.531262218952179 + 50.0 * 8.478466033935547
Epoch 1300, val loss: 0.5698258876800537
Epoch 1310, training loss: 424.2987365722656 = 0.5306218862533569 + 50.0 * 8.475362777709961
Epoch 1310, val loss: 0.5694211721420288
Epoch 1320, training loss: 424.2882385253906 = 0.5300149321556091 + 50.0 * 8.475164413452148
Epoch 1320, val loss: 0.5692048668861389
Epoch 1330, training loss: 424.30682373046875 = 0.5294226408004761 + 50.0 * 8.475547790527344
Epoch 1330, val loss: 0.5690789222717285
Epoch 1340, training loss: 424.3638610839844 = 0.5288054943084717 + 50.0 * 8.476700782775879
Epoch 1340, val loss: 0.5687490701675415
Epoch 1350, training loss: 424.2495422363281 = 0.5281828045845032 + 50.0 * 8.474427223205566
Epoch 1350, val loss: 0.5685423016548157
Epoch 1360, training loss: 424.2151794433594 = 0.5275900363922119 + 50.0 * 8.47375202178955
Epoch 1360, val loss: 0.5683552026748657
Epoch 1370, training loss: 424.2077941894531 = 0.5270047187805176 + 50.0 * 8.473615646362305
Epoch 1370, val loss: 0.5681828856468201
Epoch 1380, training loss: 424.3677062988281 = 0.5264137983322144 + 50.0 * 8.476825714111328
Epoch 1380, val loss: 0.5680229663848877
Epoch 1390, training loss: 424.21710205078125 = 0.5257974863052368 + 50.0 * 8.47382640838623
Epoch 1390, val loss: 0.5676947236061096
Epoch 1400, training loss: 424.2449035644531 = 0.5252045392990112 + 50.0 * 8.474393844604492
Epoch 1400, val loss: 0.5675410032272339
Epoch 1410, training loss: 424.1819152832031 = 0.5246089696884155 + 50.0 * 8.473146438598633
Epoch 1410, val loss: 0.567324697971344
Epoch 1420, training loss: 424.1201171875 = 0.5240156054496765 + 50.0 * 8.471921920776367
Epoch 1420, val loss: 0.5670373439788818
Epoch 1430, training loss: 424.1114196777344 = 0.5234426259994507 + 50.0 * 8.471759796142578
Epoch 1430, val loss: 0.5668532848358154
Epoch 1440, training loss: 424.1107177734375 = 0.522866427898407 + 50.0 * 8.471756935119629
Epoch 1440, val loss: 0.5666372776031494
Epoch 1450, training loss: 424.1939392089844 = 0.5222826600074768 + 50.0 * 8.473433494567871
Epoch 1450, val loss: 0.566393256187439
Epoch 1460, training loss: 424.0508728027344 = 0.5216817259788513 + 50.0 * 8.47058391571045
Epoch 1460, val loss: 0.5662806034088135
Epoch 1470, training loss: 424.1006164550781 = 0.5211032032966614 + 50.0 * 8.471590042114258
Epoch 1470, val loss: 0.566122829914093
Epoch 1480, training loss: 424.2095031738281 = 0.5205014944076538 + 50.0 * 8.473779678344727
Epoch 1480, val loss: 0.5658473372459412
Epoch 1490, training loss: 424.0609130859375 = 0.5198888778686523 + 50.0 * 8.470820426940918
Epoch 1490, val loss: 0.5655900835990906
Epoch 1500, training loss: 424.01800537109375 = 0.5193064212799072 + 50.0 * 8.46997356414795
Epoch 1500, val loss: 0.5654743909835815
Epoch 1510, training loss: 423.9777526855469 = 0.518726110458374 + 50.0 * 8.4691801071167
Epoch 1510, val loss: 0.5652059316635132
Epoch 1520, training loss: 423.9972839355469 = 0.518144965171814 + 50.0 * 8.469582557678223
Epoch 1520, val loss: 0.5650306344032288
Epoch 1530, training loss: 424.0169372558594 = 0.5175395011901855 + 50.0 * 8.469987869262695
Epoch 1530, val loss: 0.5648326277732849
Epoch 1540, training loss: 423.9429931640625 = 0.5169277787208557 + 50.0 * 8.468521118164062
Epoch 1540, val loss: 0.564650297164917
Epoch 1550, training loss: 424.13262939453125 = 0.5163277983665466 + 50.0 * 8.472326278686523
Epoch 1550, val loss: 0.5645118951797485
Epoch 1560, training loss: 423.93511962890625 = 0.5156785845756531 + 50.0 * 8.468388557434082
Epoch 1560, val loss: 0.5640979409217834
Epoch 1570, training loss: 423.8964538574219 = 0.5150652527809143 + 50.0 * 8.46762752532959
Epoch 1570, val loss: 0.5639420747756958
Epoch 1580, training loss: 423.8671875 = 0.514471709728241 + 50.0 * 8.46705436706543
Epoch 1580, val loss: 0.5636902451515198
Epoch 1590, training loss: 423.8587341308594 = 0.5138778686523438 + 50.0 * 8.466897010803223
Epoch 1590, val loss: 0.5634920597076416
Epoch 1600, training loss: 424.0210266113281 = 0.5132690072059631 + 50.0 * 8.470154762268066
Epoch 1600, val loss: 0.5632237792015076
Epoch 1610, training loss: 423.9212951660156 = 0.5126278400421143 + 50.0 * 8.468173027038574
Epoch 1610, val loss: 0.5630384087562561
Epoch 1620, training loss: 423.8797302246094 = 0.5119855403900146 + 50.0 * 8.467354774475098
Epoch 1620, val loss: 0.5628569722175598
Epoch 1630, training loss: 423.8043212890625 = 0.5113628506660461 + 50.0 * 8.465859413146973
Epoch 1630, val loss: 0.5625748634338379
Epoch 1640, training loss: 423.80572509765625 = 0.5107437968254089 + 50.0 * 8.465899467468262
Epoch 1640, val loss: 0.5623255372047424
Epoch 1650, training loss: 423.8715515136719 = 0.5101116299629211 + 50.0 * 8.467228889465332
Epoch 1650, val loss: 0.5621201395988464
Epoch 1660, training loss: 423.919677734375 = 0.5094620585441589 + 50.0 * 8.468204498291016
Epoch 1660, val loss: 0.5619889497756958
Epoch 1670, training loss: 423.7542724609375 = 0.508787214756012 + 50.0 * 8.464909553527832
Epoch 1670, val loss: 0.5615345239639282
Epoch 1680, training loss: 423.7325439453125 = 0.5081490874290466 + 50.0 * 8.46448802947998
Epoch 1680, val loss: 0.5613206624984741
Epoch 1690, training loss: 423.7129821777344 = 0.5075119733810425 + 50.0 * 8.464109420776367
Epoch 1690, val loss: 0.5611245036125183
Epoch 1700, training loss: 423.8573303222656 = 0.5068697333335876 + 50.0 * 8.467009544372559
Epoch 1700, val loss: 0.560942530632019
Epoch 1710, training loss: 423.7706298828125 = 0.5061736106872559 + 50.0 * 8.465289115905762
Epoch 1710, val loss: 0.5605754256248474
Epoch 1720, training loss: 423.7235412597656 = 0.5054833292961121 + 50.0 * 8.464361190795898
Epoch 1720, val loss: 0.5603083968162537
Epoch 1730, training loss: 423.64190673828125 = 0.5048101544380188 + 50.0 * 8.46274185180664
Epoch 1730, val loss: 0.5599952340126038
Epoch 1740, training loss: 423.63543701171875 = 0.5041464567184448 + 50.0 * 8.462625503540039
Epoch 1740, val loss: 0.5596967935562134
Epoch 1750, training loss: 423.6981201171875 = 0.5034785270690918 + 50.0 * 8.463892936706543
Epoch 1750, val loss: 0.5593623518943787
Epoch 1760, training loss: 423.70465087890625 = 0.5027539134025574 + 50.0 * 8.464037895202637
Epoch 1760, val loss: 0.5590276122093201
Epoch 1770, training loss: 423.6077880859375 = 0.5020204186439514 + 50.0 * 8.462115287780762
Epoch 1770, val loss: 0.5587780475616455
Epoch 1780, training loss: 423.574462890625 = 0.5013128519058228 + 50.0 * 8.46146297454834
Epoch 1780, val loss: 0.5584425330162048
Epoch 1790, training loss: 423.562255859375 = 0.5006105899810791 + 50.0 * 8.461233139038086
Epoch 1790, val loss: 0.5581486821174622
Epoch 1800, training loss: 423.6176452636719 = 0.499897301197052 + 50.0 * 8.46235466003418
Epoch 1800, val loss: 0.5577773451805115
Epoch 1810, training loss: 423.5639953613281 = 0.49914276599884033 + 50.0 * 8.461297035217285
Epoch 1810, val loss: 0.5574204921722412
Epoch 1820, training loss: 423.5604248046875 = 0.4983777105808258 + 50.0 * 8.461240768432617
Epoch 1820, val loss: 0.5569697022438049
Epoch 1830, training loss: 423.59259033203125 = 0.4976043105125427 + 50.0 * 8.461899757385254
Epoch 1830, val loss: 0.5566326379776001
Epoch 1840, training loss: 423.4930114746094 = 0.49682533740997314 + 50.0 * 8.45992374420166
Epoch 1840, val loss: 0.5562713742256165
Epoch 1850, training loss: 423.46697998046875 = 0.49605557322502136 + 50.0 * 8.459418296813965
Epoch 1850, val loss: 0.5559937357902527
Epoch 1860, training loss: 423.4908447265625 = 0.49528345465660095 + 50.0 * 8.459911346435547
Epoch 1860, val loss: 0.5556603074073792
Epoch 1870, training loss: 423.5992736816406 = 0.4944755434989929 + 50.0 * 8.462096214294434
Epoch 1870, val loss: 0.5552814602851868
Epoch 1880, training loss: 423.53948974609375 = 0.4936320185661316 + 50.0 * 8.460917472839355
Epoch 1880, val loss: 0.554661214351654
Epoch 1890, training loss: 423.5054016113281 = 0.4927860498428345 + 50.0 * 8.460251808166504
Epoch 1890, val loss: 0.5543816089630127
Epoch 1900, training loss: 423.41632080078125 = 0.49193722009658813 + 50.0 * 8.458487510681152
Epoch 1900, val loss: 0.5539388656616211
Epoch 1910, training loss: 423.38079833984375 = 0.49109819531440735 + 50.0 * 8.457794189453125
Epoch 1910, val loss: 0.5535314083099365
Epoch 1920, training loss: 423.394775390625 = 0.4902454912662506 + 50.0 * 8.458090782165527
Epoch 1920, val loss: 0.5531315803527832
Epoch 1930, training loss: 423.6092224121094 = 0.4893617033958435 + 50.0 * 8.462397575378418
Epoch 1930, val loss: 0.5526432991027832
Epoch 1940, training loss: 423.4075622558594 = 0.4884279668331146 + 50.0 * 8.458382606506348
Epoch 1940, val loss: 0.5523147583007812
Epoch 1950, training loss: 423.3310852050781 = 0.4875219166278839 + 50.0 * 8.456871032714844
Epoch 1950, val loss: 0.5518448352813721
Epoch 1960, training loss: 423.3067932128906 = 0.48661500215530396 + 50.0 * 8.456403732299805
Epoch 1960, val loss: 0.5514729619026184
Epoch 1970, training loss: 423.4328918457031 = 0.4856988489627838 + 50.0 * 8.458944320678711
Epoch 1970, val loss: 0.551137387752533
Epoch 1980, training loss: 423.3275451660156 = 0.4847116768360138 + 50.0 * 8.456856727600098
Epoch 1980, val loss: 0.5504698753356934
Epoch 1990, training loss: 423.27984619140625 = 0.483733594417572 + 50.0 * 8.45592212677002
Epoch 1990, val loss: 0.5500004291534424
Epoch 2000, training loss: 423.2536315917969 = 0.48277002573013306 + 50.0 * 8.45541763305664
Epoch 2000, val loss: 0.5494885444641113
Epoch 2010, training loss: 423.2314147949219 = 0.48180848360061646 + 50.0 * 8.454992294311523
Epoch 2010, val loss: 0.549045979976654
Epoch 2020, training loss: 423.2449035644531 = 0.48083317279815674 + 50.0 * 8.455281257629395
Epoch 2020, val loss: 0.5485660433769226
Epoch 2030, training loss: 423.4478759765625 = 0.47981777787208557 + 50.0 * 8.45936107635498
Epoch 2030, val loss: 0.548121988773346
Epoch 2040, training loss: 423.3721008300781 = 0.47874724864959717 + 50.0 * 8.457866668701172
Epoch 2040, val loss: 0.5474376678466797
Epoch 2050, training loss: 423.20135498046875 = 0.47769051790237427 + 50.0 * 8.454473495483398
Epoch 2050, val loss: 0.5468651652336121
Epoch 2060, training loss: 423.1759033203125 = 0.4766557216644287 + 50.0 * 8.453985214233398
Epoch 2060, val loss: 0.5463127493858337
Epoch 2070, training loss: 423.1425476074219 = 0.4756116569042206 + 50.0 * 8.453338623046875
Epoch 2070, val loss: 0.5457907915115356
Epoch 2080, training loss: 423.1467590332031 = 0.4745548665523529 + 50.0 * 8.453444480895996
Epoch 2080, val loss: 0.545258641242981
Epoch 2090, training loss: 423.35919189453125 = 0.4734668731689453 + 50.0 * 8.457714080810547
Epoch 2090, val loss: 0.544694721698761
Epoch 2100, training loss: 423.1802062988281 = 0.47232842445373535 + 50.0 * 8.454157829284668
Epoch 2100, val loss: 0.5440882444381714
Epoch 2110, training loss: 423.241455078125 = 0.47118687629699707 + 50.0 * 8.455405235290527
Epoch 2110, val loss: 0.543545663356781
Epoch 2120, training loss: 423.091796875 = 0.4700291156768799 + 50.0 * 8.452435493469238
Epoch 2120, val loss: 0.5427700877189636
Epoch 2130, training loss: 423.0984191894531 = 0.468883216381073 + 50.0 * 8.452590942382812
Epoch 2130, val loss: 0.5421580672264099
Epoch 2140, training loss: 423.0704040527344 = 0.4677176773548126 + 50.0 * 8.452054023742676
Epoch 2140, val loss: 0.5415103435516357
Epoch 2150, training loss: 423.0750427246094 = 0.466529905796051 + 50.0 * 8.452170372009277
Epoch 2150, val loss: 0.5408443212509155
Epoch 2160, training loss: 423.2078552246094 = 0.46530479192733765 + 50.0 * 8.454851150512695
Epoch 2160, val loss: 0.5401302576065063
Epoch 2170, training loss: 423.0613708496094 = 0.4640216529369354 + 50.0 * 8.451947212219238
Epoch 2170, val loss: 0.5396718382835388
Epoch 2180, training loss: 423.0453796386719 = 0.4627452790737152 + 50.0 * 8.451652526855469
Epoch 2180, val loss: 0.538852334022522
Epoch 2190, training loss: 423.01226806640625 = 0.46146562695503235 + 50.0 * 8.451016426086426
Epoch 2190, val loss: 0.5381441116333008
Epoch 2200, training loss: 423.0070495605469 = 0.46018195152282715 + 50.0 * 8.450937271118164
Epoch 2200, val loss: 0.537499725818634
Epoch 2210, training loss: 423.2228698730469 = 0.4588727056980133 + 50.0 * 8.455280303955078
Epoch 2210, val loss: 0.5366470217704773
Epoch 2220, training loss: 423.0354919433594 = 0.457498162984848 + 50.0 * 8.451560020446777
Epoch 2220, val loss: 0.5361278653144836
Epoch 2230, training loss: 422.99560546875 = 0.4561401903629303 + 50.0 * 8.450789451599121
Epoch 2230, val loss: 0.5352725982666016
Epoch 2240, training loss: 423.05364990234375 = 0.45476946234703064 + 50.0 * 8.451977729797363
Epoch 2240, val loss: 0.5346958637237549
Epoch 2250, training loss: 422.9913635253906 = 0.4533616602420807 + 50.0 * 8.450759887695312
Epoch 2250, val loss: 0.5338069796562195
Epoch 2260, training loss: 422.94427490234375 = 0.45195668935775757 + 50.0 * 8.449846267700195
Epoch 2260, val loss: 0.5329551100730896
Epoch 2270, training loss: 422.9128723144531 = 0.4505520164966583 + 50.0 * 8.449246406555176
Epoch 2270, val loss: 0.5321947336196899
Epoch 2280, training loss: 422.9307556152344 = 0.44912782311439514 + 50.0 * 8.44963264465332
Epoch 2280, val loss: 0.5315254330635071
Epoch 2290, training loss: 423.0066223144531 = 0.4476732015609741 + 50.0 * 8.451179504394531
Epoch 2290, val loss: 0.5307663679122925
Epoch 2300, training loss: 422.96453857421875 = 0.44619739055633545 + 50.0 * 8.450366973876953
Epoch 2300, val loss: 0.5300423502922058
Epoch 2310, training loss: 422.8817138671875 = 0.444707989692688 + 50.0 * 8.448740005493164
Epoch 2310, val loss: 0.5292047262191772
Epoch 2320, training loss: 422.8445129394531 = 0.4432268738746643 + 50.0 * 8.448025703430176
Epoch 2320, val loss: 0.5283962488174438
Epoch 2330, training loss: 422.8288269042969 = 0.4417409300804138 + 50.0 * 8.447741508483887
Epoch 2330, val loss: 0.5276544690132141
Epoch 2340, training loss: 422.8226013183594 = 0.4402455687522888 + 50.0 * 8.447647094726562
Epoch 2340, val loss: 0.5268128514289856
Epoch 2350, training loss: 423.3208923339844 = 0.43874427676200867 + 50.0 * 8.457642555236816
Epoch 2350, val loss: 0.5258200168609619
Epoch 2360, training loss: 423.1157531738281 = 0.4371081292629242 + 50.0 * 8.453573226928711
Epoch 2360, val loss: 0.5252378582954407
Epoch 2370, training loss: 422.8250732421875 = 0.435528427362442 + 50.0 * 8.44779109954834
Epoch 2370, val loss: 0.524336040019989
Epoch 2380, training loss: 422.8013610839844 = 0.4339805245399475 + 50.0 * 8.447347640991211
Epoch 2380, val loss: 0.5235174894332886
Epoch 2390, training loss: 422.7794189453125 = 0.4324371814727783 + 50.0 * 8.446939468383789
Epoch 2390, val loss: 0.5228579640388489
Epoch 2400, training loss: 422.7669677734375 = 0.43088382482528687 + 50.0 * 8.446722030639648
Epoch 2400, val loss: 0.5220602750778198
Epoch 2410, training loss: 422.8813171386719 = 0.4293038547039032 + 50.0 * 8.449040412902832
Epoch 2410, val loss: 0.5213586091995239
Epoch 2420, training loss: 422.7432861328125 = 0.42768457531929016 + 50.0 * 8.446311950683594
Epoch 2420, val loss: 0.5204154253005981
Epoch 2430, training loss: 422.75927734375 = 0.42607489228248596 + 50.0 * 8.446663856506348
Epoch 2430, val loss: 0.5195695161819458
Epoch 2440, training loss: 422.7821350097656 = 0.4244581162929535 + 50.0 * 8.447153091430664
Epoch 2440, val loss: 0.5188135504722595
Epoch 2450, training loss: 422.8267822265625 = 0.4228207767009735 + 50.0 * 8.448079109191895
Epoch 2450, val loss: 0.5179258584976196
Epoch 2460, training loss: 422.7306213378906 = 0.42116060853004456 + 50.0 * 8.446188926696777
Epoch 2460, val loss: 0.5171784162521362
Epoch 2470, training loss: 422.7042236328125 = 0.4195171892642975 + 50.0 * 8.445693969726562
Epoch 2470, val loss: 0.516435980796814
Epoch 2480, training loss: 422.7299499511719 = 0.41786882281303406 + 50.0 * 8.44624137878418
Epoch 2480, val loss: 0.5156662464141846
Epoch 2490, training loss: 422.70233154296875 = 0.41620317101478577 + 50.0 * 8.445722579956055
Epoch 2490, val loss: 0.5148172378540039
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7919837645865042
0.8150402086502935
=== training gcn model ===
Epoch 0, training loss: 530.2273559570312 = 1.113296389579773 + 50.0 * 10.582282066345215
Epoch 0, val loss: 1.1118255853652954
Epoch 10, training loss: 530.2039184570312 = 1.108103632926941 + 50.0 * 10.581916809082031
Epoch 10, val loss: 1.1066515445709229
Epoch 20, training loss: 530.11962890625 = 1.1023658514022827 + 50.0 * 10.580345153808594
Epoch 20, val loss: 1.1009281873703003
Epoch 30, training loss: 529.7600708007812 = 1.0959781408309937 + 50.0 * 10.573281288146973
Epoch 30, val loss: 1.094611406326294
Epoch 40, training loss: 528.31396484375 = 1.0890940427780151 + 50.0 * 10.544496536254883
Epoch 40, val loss: 1.0878472328186035
Epoch 50, training loss: 523.6383056640625 = 1.0817331075668335 + 50.0 * 10.451131820678711
Epoch 50, val loss: 1.080721378326416
Epoch 60, training loss: 512.5579833984375 = 1.0750120878219604 + 50.0 * 10.229660034179688
Epoch 60, val loss: 1.0743898153305054
Epoch 70, training loss: 499.5853271484375 = 1.068476676940918 + 50.0 * 9.9703369140625
Epoch 70, val loss: 1.068231225013733
Epoch 80, training loss: 494.04248046875 = 1.0617667436599731 + 50.0 * 9.859614372253418
Epoch 80, val loss: 1.0618300437927246
Epoch 90, training loss: 487.7956848144531 = 1.0555808544158936 + 50.0 * 9.73480224609375
Epoch 90, val loss: 1.0559190511703491
Epoch 100, training loss: 479.77069091796875 = 1.051440715789795 + 50.0 * 9.574384689331055
Epoch 100, val loss: 1.0519790649414062
Epoch 110, training loss: 471.0572814941406 = 1.0490998029708862 + 50.0 * 9.400163650512695
Epoch 110, val loss: 1.0496386289596558
Epoch 120, training loss: 466.00994873046875 = 1.046891450881958 + 50.0 * 9.299261093139648
Epoch 120, val loss: 1.047363042831421
Epoch 130, training loss: 463.90167236328125 = 1.0438545942306519 + 50.0 * 9.257156372070312
Epoch 130, val loss: 1.0445761680603027
Epoch 140, training loss: 460.40887451171875 = 1.0417832136154175 + 50.0 * 9.187341690063477
Epoch 140, val loss: 1.042745590209961
Epoch 150, training loss: 455.2456970214844 = 1.0403668880462646 + 50.0 * 9.0841064453125
Epoch 150, val loss: 1.0414286851882935
Epoch 160, training loss: 451.09368896484375 = 1.039343237876892 + 50.0 * 9.001087188720703
Epoch 160, val loss: 1.04037606716156
Epoch 170, training loss: 448.255615234375 = 1.0372333526611328 + 50.0 * 8.944367408752441
Epoch 170, val loss: 1.0382329225540161
Epoch 180, training loss: 445.79022216796875 = 1.0346091985702515 + 50.0 * 8.895112037658691
Epoch 180, val loss: 1.0356568098068237
Epoch 190, training loss: 444.0850524902344 = 1.03203547000885 + 50.0 * 8.86106014251709
Epoch 190, val loss: 1.033138632774353
Epoch 200, training loss: 442.325927734375 = 1.0295518636703491 + 50.0 * 8.825927734375
Epoch 200, val loss: 1.0307526588439941
Epoch 210, training loss: 440.76416015625 = 1.0272674560546875 + 50.0 * 8.794737815856934
Epoch 210, val loss: 1.02852201461792
Epoch 220, training loss: 439.5435485839844 = 1.0248838663101196 + 50.0 * 8.770373344421387
Epoch 220, val loss: 1.0261589288711548
Epoch 230, training loss: 438.5462646484375 = 1.0221747159957886 + 50.0 * 8.750481605529785
Epoch 230, val loss: 1.023465633392334
Epoch 240, training loss: 437.6986083984375 = 1.0191807746887207 + 50.0 * 8.733588218688965
Epoch 240, val loss: 1.0205190181732178
Epoch 250, training loss: 436.9462890625 = 1.015900731086731 + 50.0 * 8.718607902526855
Epoch 250, val loss: 1.0173012018203735
Epoch 260, training loss: 436.2095947265625 = 1.0123838186264038 + 50.0 * 8.703944206237793
Epoch 260, val loss: 1.0138754844665527
Epoch 270, training loss: 435.5062561035156 = 1.0086592435836792 + 50.0 * 8.68995189666748
Epoch 270, val loss: 1.0102150440216064
Epoch 280, training loss: 434.9700622558594 = 1.004609227180481 + 50.0 * 8.679308891296387
Epoch 280, val loss: 1.006242036819458
Epoch 290, training loss: 434.332275390625 = 1.0002070665359497 + 50.0 * 8.666641235351562
Epoch 290, val loss: 1.0019440650939941
Epoch 300, training loss: 433.93084716796875 = 0.9955213069915771 + 50.0 * 8.658706665039062
Epoch 300, val loss: 0.9973541498184204
Epoch 310, training loss: 433.3621826171875 = 0.9905562400817871 + 50.0 * 8.647432327270508
Epoch 310, val loss: 0.9925273060798645
Epoch 320, training loss: 432.901123046875 = 0.9853419065475464 + 50.0 * 8.638315200805664
Epoch 320, val loss: 0.987444281578064
Epoch 330, training loss: 432.49249267578125 = 0.9798097014427185 + 50.0 * 8.630253791809082
Epoch 330, val loss: 0.9820420145988464
Epoch 340, training loss: 432.1559143066406 = 0.9739640355110168 + 50.0 * 8.623639106750488
Epoch 340, val loss: 0.976321816444397
Epoch 350, training loss: 431.94805908203125 = 0.9677916765213013 + 50.0 * 8.61960506439209
Epoch 350, val loss: 0.970268964767456
Epoch 360, training loss: 431.57415771484375 = 0.9612970948219299 + 50.0 * 8.61225700378418
Epoch 360, val loss: 0.9639103412628174
Epoch 370, training loss: 431.2951354980469 = 0.9545670747756958 + 50.0 * 8.6068115234375
Epoch 370, val loss: 0.957298994064331
Epoch 380, training loss: 431.0258483886719 = 0.9475702047348022 + 50.0 * 8.60156536102295
Epoch 380, val loss: 0.9504289627075195
Epoch 390, training loss: 431.0050964355469 = 0.9403759241104126 + 50.0 * 8.60129451751709
Epoch 390, val loss: 0.9433145523071289
Epoch 400, training loss: 430.64556884765625 = 0.9328396916389465 + 50.0 * 8.594254493713379
Epoch 400, val loss: 0.9359280467033386
Epoch 410, training loss: 430.30810546875 = 0.9252136945724487 + 50.0 * 8.587657928466797
Epoch 410, val loss: 0.9284336566925049
Epoch 420, training loss: 430.0394287109375 = 0.9174622893333435 + 50.0 * 8.582439422607422
Epoch 420, val loss: 0.9207842946052551
Epoch 430, training loss: 429.8338623046875 = 0.9095616936683655 + 50.0 * 8.578485488891602
Epoch 430, val loss: 0.9130012392997742
Epoch 440, training loss: 429.5996398925781 = 0.9015120267868042 + 50.0 * 8.573962211608887
Epoch 440, val loss: 0.9050557017326355
Epoch 450, training loss: 429.39215087890625 = 0.893376350402832 + 50.0 * 8.569975852966309
Epoch 450, val loss: 0.8970274925231934
Epoch 460, training loss: 429.19049072265625 = 0.8852413892745972 + 50.0 * 8.566104888916016
Epoch 460, val loss: 0.8889873623847961
Epoch 470, training loss: 429.0064697265625 = 0.8770816326141357 + 50.0 * 8.56258773803711
Epoch 470, val loss: 0.8809139132499695
Epoch 480, training loss: 429.11712646484375 = 0.8688891530036926 + 50.0 * 8.564964294433594
Epoch 480, val loss: 0.8727814555168152
Epoch 490, training loss: 428.695068359375 = 0.8605959415435791 + 50.0 * 8.556689262390137
Epoch 490, val loss: 0.8646294474601746
Epoch 500, training loss: 428.5198059082031 = 0.8524073362350464 + 50.0 * 8.55334758758545
Epoch 500, val loss: 0.8565619587898254
Epoch 510, training loss: 428.36376953125 = 0.8442638516426086 + 50.0 * 8.550390243530273
Epoch 510, val loss: 0.8485478758811951
Epoch 520, training loss: 428.2886962890625 = 0.8361683487892151 + 50.0 * 8.549050331115723
Epoch 520, val loss: 0.8406044840812683
Epoch 530, training loss: 428.179931640625 = 0.8281185626983643 + 50.0 * 8.547036170959473
Epoch 530, val loss: 0.8326731324195862
Epoch 540, training loss: 428.0054626464844 = 0.8201165199279785 + 50.0 * 8.543706893920898
Epoch 540, val loss: 0.8248383402824402
Epoch 550, training loss: 427.84857177734375 = 0.8122166395187378 + 50.0 * 8.540726661682129
Epoch 550, val loss: 0.8171257376670837
Epoch 560, training loss: 428.1031188964844 = 0.8044329285621643 + 50.0 * 8.545973777770996
Epoch 560, val loss: 0.8095158338546753
Epoch 570, training loss: 427.60791015625 = 0.7966182827949524 + 50.0 * 8.536225318908691
Epoch 570, val loss: 0.8019219636917114
Epoch 580, training loss: 427.51995849609375 = 0.789002001285553 + 50.0 * 8.534619331359863
Epoch 580, val loss: 0.7945153117179871
Epoch 590, training loss: 427.3495178222656 = 0.7815032601356506 + 50.0 * 8.531360626220703
Epoch 590, val loss: 0.7872288823127747
Epoch 600, training loss: 427.41302490234375 = 0.7741259336471558 + 50.0 * 8.532777786254883
Epoch 600, val loss: 0.7800549864768982
Epoch 610, training loss: 427.125732421875 = 0.7667677998542786 + 50.0 * 8.527179718017578
Epoch 610, val loss: 0.7729636430740356
Epoch 620, training loss: 427.0074768066406 = 0.7595788240432739 + 50.0 * 8.524957656860352
Epoch 620, val loss: 0.7660259008407593
Epoch 630, training loss: 426.8876037597656 = 0.7525044679641724 + 50.0 * 8.52270221710205
Epoch 630, val loss: 0.7592191696166992
Epoch 640, training loss: 427.0623474121094 = 0.7455289959907532 + 50.0 * 8.526336669921875
Epoch 640, val loss: 0.7524843811988831
Epoch 650, training loss: 426.7091979980469 = 0.738572359085083 + 50.0 * 8.51941204071045
Epoch 650, val loss: 0.7458378672599792
Epoch 660, training loss: 426.57635498046875 = 0.731833279132843 + 50.0 * 8.516890525817871
Epoch 660, val loss: 0.739391028881073
Epoch 670, training loss: 426.48907470703125 = 0.7252205610275269 + 50.0 * 8.515276908874512
Epoch 670, val loss: 0.7330917716026306
Epoch 680, training loss: 426.6147766113281 = 0.7187084555625916 + 50.0 * 8.517921447753906
Epoch 680, val loss: 0.7268878817558289
Epoch 690, training loss: 426.316162109375 = 0.7122732996940613 + 50.0 * 8.512077331542969
Epoch 690, val loss: 0.7207871079444885
Epoch 700, training loss: 426.19775390625 = 0.7060666084289551 + 50.0 * 8.509834289550781
Epoch 700, val loss: 0.7149119973182678
Epoch 710, training loss: 426.1142578125 = 0.699995756149292 + 50.0 * 8.508285522460938
Epoch 710, val loss: 0.7092032432556152
Epoch 720, training loss: 426.13543701171875 = 0.6940996050834656 + 50.0 * 8.508827209472656
Epoch 720, val loss: 0.7036629319190979
Epoch 730, training loss: 425.9875793457031 = 0.6883261203765869 + 50.0 * 8.505985260009766
Epoch 730, val loss: 0.6982185244560242
Epoch 740, training loss: 425.9015808105469 = 0.6827684044837952 + 50.0 * 8.504376411437988
Epoch 740, val loss: 0.6930578947067261
Epoch 750, training loss: 425.9091491699219 = 0.6773933172225952 + 50.0 * 8.504634857177734
Epoch 750, val loss: 0.6880759596824646
Epoch 760, training loss: 425.7489929199219 = 0.6721819043159485 + 50.0 * 8.50153636932373
Epoch 760, val loss: 0.6832174062728882
Epoch 770, training loss: 425.6755676269531 = 0.6671692132949829 + 50.0 * 8.500167846679688
Epoch 770, val loss: 0.6786490082740784
Epoch 780, training loss: 425.5581970214844 = 0.662375807762146 + 50.0 * 8.497916221618652
Epoch 780, val loss: 0.6742536425590515
Epoch 790, training loss: 425.4926452636719 = 0.6577664613723755 + 50.0 * 8.496697425842285
Epoch 790, val loss: 0.6700847148895264
Epoch 800, training loss: 425.9337463378906 = 0.6533195972442627 + 50.0 * 8.505608558654785
Epoch 800, val loss: 0.6660788655281067
Epoch 810, training loss: 425.3511657714844 = 0.6489567160606384 + 50.0 * 8.494044303894043
Epoch 810, val loss: 0.6621585488319397
Epoch 820, training loss: 425.3160095214844 = 0.6448853611946106 + 50.0 * 8.493422508239746
Epoch 820, val loss: 0.6585429906845093
Epoch 830, training loss: 425.22393798828125 = 0.6410197019577026 + 50.0 * 8.491658210754395
Epoch 830, val loss: 0.6551383137702942
Epoch 840, training loss: 425.1579284667969 = 0.6373330950737 + 50.0 * 8.490411758422852
Epoch 840, val loss: 0.6519497632980347
Epoch 850, training loss: 425.2833557128906 = 0.633790910243988 + 50.0 * 8.49299144744873
Epoch 850, val loss: 0.6489228010177612
Epoch 860, training loss: 425.07293701171875 = 0.630386471748352 + 50.0 * 8.488850593566895
Epoch 860, val loss: 0.6459379196166992
Epoch 870, training loss: 424.98779296875 = 0.627150297164917 + 50.0 * 8.487213134765625
Epoch 870, val loss: 0.6432020664215088
Epoch 880, training loss: 424.93768310546875 = 0.6240711808204651 + 50.0 * 8.486271858215332
Epoch 880, val loss: 0.6406314373016357
Epoch 890, training loss: 425.1042175292969 = 0.621150553226471 + 50.0 * 8.48966121673584
Epoch 890, val loss: 0.6381877660751343
Epoch 900, training loss: 424.9507751464844 = 0.6182683110237122 + 50.0 * 8.486650466918945
Epoch 900, val loss: 0.6358277797698975
Epoch 910, training loss: 424.84716796875 = 0.6155917048454285 + 50.0 * 8.484631538391113
Epoch 910, val loss: 0.6336786150932312
Epoch 920, training loss: 424.7591552734375 = 0.6130394339561462 + 50.0 * 8.482922554016113
Epoch 920, val loss: 0.6316794157028198
Epoch 930, training loss: 424.7017822265625 = 0.610629141330719 + 50.0 * 8.481822967529297
Epoch 930, val loss: 0.6297860741615295
Epoch 940, training loss: 424.7580261230469 = 0.6083135604858398 + 50.0 * 8.482994079589844
Epoch 940, val loss: 0.6279939413070679
Epoch 950, training loss: 424.6255798339844 = 0.6060553789138794 + 50.0 * 8.480390548706055
Epoch 950, val loss: 0.6263076663017273
Epoch 960, training loss: 424.58319091796875 = 0.6039116978645325 + 50.0 * 8.479585647583008
Epoch 960, val loss: 0.6247120499610901
Epoch 970, training loss: 424.5379638671875 = 0.6018741726875305 + 50.0 * 8.478721618652344
Epoch 970, val loss: 0.623228132724762
Epoch 980, training loss: 424.5057067871094 = 0.5999304056167603 + 50.0 * 8.47811508178711
Epoch 980, val loss: 0.6218196153640747
Epoch 990, training loss: 424.4923095703125 = 0.598054826259613 + 50.0 * 8.477885246276855
Epoch 990, val loss: 0.6204971075057983
Epoch 1000, training loss: 424.5914611816406 = 0.596240222454071 + 50.0 * 8.479904174804688
Epoch 1000, val loss: 0.6192108988761902
Epoch 1010, training loss: 424.4581604003906 = 0.5944491624832153 + 50.0 * 8.477273941040039
Epoch 1010, val loss: 0.6180555820465088
Epoch 1020, training loss: 424.3811340332031 = 0.5927804708480835 + 50.0 * 8.475767135620117
Epoch 1020, val loss: 0.6169443130493164
Epoch 1030, training loss: 424.35357666015625 = 0.59116131067276 + 50.0 * 8.475248336791992
Epoch 1030, val loss: 0.6159151196479797
Epoch 1040, training loss: 424.5326843261719 = 0.5896121263504028 + 50.0 * 8.478861808776855
Epoch 1040, val loss: 0.6149627566337585
Epoch 1050, training loss: 424.3271484375 = 0.5880473852157593 + 50.0 * 8.47478199005127
Epoch 1050, val loss: 0.6139082908630371
Epoch 1060, training loss: 424.25872802734375 = 0.5865968465805054 + 50.0 * 8.473442077636719
Epoch 1060, val loss: 0.6130690574645996
Epoch 1070, training loss: 424.2234191894531 = 0.5851818323135376 + 50.0 * 8.47276496887207
Epoch 1070, val loss: 0.6122353672981262
Epoch 1080, training loss: 424.1876525878906 = 0.5838281512260437 + 50.0 * 8.472076416015625
Epoch 1080, val loss: 0.6114152073860168
Epoch 1090, training loss: 424.3533935546875 = 0.5824937224388123 + 50.0 * 8.475418090820312
Epoch 1090, val loss: 0.6106327772140503
Epoch 1100, training loss: 424.22650146484375 = 0.5811578631401062 + 50.0 * 8.472907066345215
Epoch 1100, val loss: 0.609847366809845
Epoch 1110, training loss: 424.1118469238281 = 0.5798881649971008 + 50.0 * 8.4706392288208
Epoch 1110, val loss: 0.6091645956039429
Epoch 1120, training loss: 424.08740234375 = 0.5786649584770203 + 50.0 * 8.470174789428711
Epoch 1120, val loss: 0.6085332632064819
Epoch 1130, training loss: 424.1058654785156 = 0.5774872303009033 + 50.0 * 8.47056770324707
Epoch 1130, val loss: 0.6079023480415344
Epoch 1140, training loss: 424.1355895996094 = 0.5763071179389954 + 50.0 * 8.471185684204102
Epoch 1140, val loss: 0.6072715520858765
Epoch 1150, training loss: 424.02764892578125 = 0.5751567482948303 + 50.0 * 8.469049453735352
Epoch 1150, val loss: 0.606576144695282
Epoch 1160, training loss: 423.97442626953125 = 0.574055552482605 + 50.0 * 8.46800708770752
Epoch 1160, val loss: 0.6060783863067627
Epoch 1170, training loss: 423.9629211425781 = 0.572973906993866 + 50.0 * 8.467799186706543
Epoch 1170, val loss: 0.6055107116699219
Epoch 1180, training loss: 424.0594177246094 = 0.5719015002250671 + 50.0 * 8.46975040435791
Epoch 1180, val loss: 0.6049370765686035
Epoch 1190, training loss: 424.0164489746094 = 0.5708433985710144 + 50.0 * 8.468912124633789
Epoch 1190, val loss: 0.6043954491615295
Epoch 1200, training loss: 423.898681640625 = 0.5697944164276123 + 50.0 * 8.466577529907227
Epoch 1200, val loss: 0.6039162278175354
Epoch 1210, training loss: 423.84722900390625 = 0.5687984824180603 + 50.0 * 8.465568542480469
Epoch 1210, val loss: 0.6034523248672485
Epoch 1220, training loss: 423.82476806640625 = 0.567830502986908 + 50.0 * 8.46513843536377
Epoch 1220, val loss: 0.6029730439186096
Epoch 1230, training loss: 423.85284423828125 = 0.5668849349021912 + 50.0 * 8.465719223022461
Epoch 1230, val loss: 0.6025487184524536
Epoch 1240, training loss: 423.8419189453125 = 0.5659308433532715 + 50.0 * 8.465519905090332
Epoch 1240, val loss: 0.6020450592041016
Epoch 1250, training loss: 423.8453063964844 = 0.5649620294570923 + 50.0 * 8.465606689453125
Epoch 1250, val loss: 0.6015810966491699
Epoch 1260, training loss: 423.7917175292969 = 0.5640205144882202 + 50.0 * 8.464553833007812
Epoch 1260, val loss: 0.601176381111145
Epoch 1270, training loss: 423.708984375 = 0.5631253123283386 + 50.0 * 8.46291732788086
Epoch 1270, val loss: 0.6007806062698364
Epoch 1280, training loss: 423.67864990234375 = 0.5622618198394775 + 50.0 * 8.46232795715332
Epoch 1280, val loss: 0.6003702878952026
Epoch 1290, training loss: 423.6543273925781 = 0.5614076256752014 + 50.0 * 8.461858749389648
Epoch 1290, val loss: 0.6000389456748962
Epoch 1300, training loss: 423.6666564941406 = 0.5605601668357849 + 50.0 * 8.462121963500977
Epoch 1300, val loss: 0.5996996760368347
Epoch 1310, training loss: 423.8603515625 = 0.5596851706504822 + 50.0 * 8.466012954711914
Epoch 1310, val loss: 0.599317193031311
Epoch 1320, training loss: 423.6073303222656 = 0.5587906241416931 + 50.0 * 8.460970878601074
Epoch 1320, val loss: 0.5988408327102661
Epoch 1330, training loss: 423.59429931640625 = 0.5579376220703125 + 50.0 * 8.46072769165039
Epoch 1330, val loss: 0.5984898209571838
Epoch 1340, training loss: 423.5495910644531 = 0.5571181774139404 + 50.0 * 8.45984935760498
Epoch 1340, val loss: 0.5981425642967224
Epoch 1350, training loss: 423.5316467285156 = 0.5563076138496399 + 50.0 * 8.45950698852539
Epoch 1350, val loss: 0.597813606262207
Epoch 1360, training loss: 423.6771545410156 = 0.5555000901222229 + 50.0 * 8.462432861328125
Epoch 1360, val loss: 0.5974785685539246
Epoch 1370, training loss: 423.5425720214844 = 0.5546255707740784 + 50.0 * 8.459758758544922
Epoch 1370, val loss: 0.5970346331596375
Epoch 1380, training loss: 423.5033874511719 = 0.5538061261177063 + 50.0 * 8.458992004394531
Epoch 1380, val loss: 0.5966526865959167
Epoch 1390, training loss: 423.4599609375 = 0.5529932379722595 + 50.0 * 8.458139419555664
Epoch 1390, val loss: 0.5963379144668579
Epoch 1400, training loss: 423.4378662109375 = 0.552209198474884 + 50.0 * 8.45771312713623
Epoch 1400, val loss: 0.5960039496421814
Epoch 1410, training loss: 423.4513244628906 = 0.5514199733734131 + 50.0 * 8.457998275756836
Epoch 1410, val loss: 0.5956863760948181
Epoch 1420, training loss: 423.4844055175781 = 0.5506159067153931 + 50.0 * 8.458675384521484
Epoch 1420, val loss: 0.5952131152153015
Epoch 1430, training loss: 423.4709167480469 = 0.5497708320617676 + 50.0 * 8.458422660827637
Epoch 1430, val loss: 0.594884991645813
Epoch 1440, training loss: 423.37261962890625 = 0.5489506721496582 + 50.0 * 8.456473350524902
Epoch 1440, val loss: 0.5944659113883972
Epoch 1450, training loss: 423.3500671386719 = 0.5481632351875305 + 50.0 * 8.456038475036621
Epoch 1450, val loss: 0.5941837430000305
Epoch 1460, training loss: 423.3207702636719 = 0.5473954081535339 + 50.0 * 8.455467224121094
Epoch 1460, val loss: 0.5938060283660889
Epoch 1470, training loss: 423.2928161621094 = 0.5466378927230835 + 50.0 * 8.454923629760742
Epoch 1470, val loss: 0.5935215353965759
Epoch 1480, training loss: 423.2704162597656 = 0.5458763837814331 + 50.0 * 8.454490661621094
Epoch 1480, val loss: 0.5931758880615234
Epoch 1490, training loss: 423.2635803222656 = 0.545112669467926 + 50.0 * 8.45436954498291
Epoch 1490, val loss: 0.5928530097007751
Epoch 1500, training loss: 423.5248718261719 = 0.5443239212036133 + 50.0 * 8.459610939025879
Epoch 1500, val loss: 0.5924199223518372
Epoch 1510, training loss: 423.44818115234375 = 0.5434914827346802 + 50.0 * 8.458093643188477
Epoch 1510, val loss: 0.5920657515525818
Epoch 1520, training loss: 423.2305908203125 = 0.5426761507987976 + 50.0 * 8.453758239746094
Epoch 1520, val loss: 0.5917305946350098
Epoch 1530, training loss: 423.1909484863281 = 0.5419082045555115 + 50.0 * 8.452980995178223
Epoch 1530, val loss: 0.5913808941841125
Epoch 1540, training loss: 423.1673889160156 = 0.5411546230316162 + 50.0 * 8.452524185180664
Epoch 1540, val loss: 0.5910604596138
Epoch 1550, training loss: 423.1755676269531 = 0.5404053330421448 + 50.0 * 8.452703475952148
Epoch 1550, val loss: 0.5907279849052429
Epoch 1560, training loss: 423.3674011230469 = 0.5396400094032288 + 50.0 * 8.456555366516113
Epoch 1560, val loss: 0.5903593897819519
Epoch 1570, training loss: 423.17474365234375 = 0.538850724697113 + 50.0 * 8.452717781066895
Epoch 1570, val loss: 0.5900496244430542
Epoch 1580, training loss: 423.1033630371094 = 0.5380657911300659 + 50.0 * 8.451306343078613
Epoch 1580, val loss: 0.5896967053413391
Epoch 1590, training loss: 423.0800476074219 = 0.5373135209083557 + 50.0 * 8.450854301452637
Epoch 1590, val loss: 0.5893841981887817
Epoch 1600, training loss: 423.0809020996094 = 0.5365689396858215 + 50.0 * 8.450886726379395
Epoch 1600, val loss: 0.5890968441963196
Epoch 1610, training loss: 423.244873046875 = 0.5358054637908936 + 50.0 * 8.454181671142578
Epoch 1610, val loss: 0.5888147354125977
Epoch 1620, training loss: 423.05816650390625 = 0.5350103974342346 + 50.0 * 8.45046329498291
Epoch 1620, val loss: 0.5884128212928772
Epoch 1630, training loss: 423.038818359375 = 0.5342204570770264 + 50.0 * 8.450092315673828
Epoch 1630, val loss: 0.5880786776542664
Epoch 1640, training loss: 423.0099182128906 = 0.5334632992744446 + 50.0 * 8.449528694152832
Epoch 1640, val loss: 0.5877811312675476
Epoch 1650, training loss: 423.0133972167969 = 0.5327014327049255 + 50.0 * 8.449613571166992
Epoch 1650, val loss: 0.5874224901199341
Epoch 1660, training loss: 423.2150573730469 = 0.5319256782531738 + 50.0 * 8.453662872314453
Epoch 1660, val loss: 0.5870423316955566
Epoch 1670, training loss: 423.0465087890625 = 0.5311253666877747 + 50.0 * 8.450307846069336
Epoch 1670, val loss: 0.5868208408355713
Epoch 1680, training loss: 423.01812744140625 = 0.530336320400238 + 50.0 * 8.449755668640137
Epoch 1680, val loss: 0.5864133834838867
Epoch 1690, training loss: 422.9324951171875 = 0.5295531749725342 + 50.0 * 8.44805908203125
Epoch 1690, val loss: 0.5861172080039978
Epoch 1700, training loss: 422.9127502441406 = 0.5287936329841614 + 50.0 * 8.447678565979004
Epoch 1700, val loss: 0.5857972502708435
Epoch 1710, training loss: 422.890380859375 = 0.5280243754386902 + 50.0 * 8.447247505187988
Epoch 1710, val loss: 0.5854682922363281
Epoch 1720, training loss: 422.9532775878906 = 0.5272460579872131 + 50.0 * 8.44852066040039
Epoch 1720, val loss: 0.5850985646247864
Epoch 1730, training loss: 422.9776306152344 = 0.5264283418655396 + 50.0 * 8.449024200439453
Epoch 1730, val loss: 0.5847374200820923
Epoch 1740, training loss: 422.8845520019531 = 0.5256110429763794 + 50.0 * 8.447178840637207
Epoch 1740, val loss: 0.5844257473945618
Epoch 1750, training loss: 422.8373718261719 = 0.5248061418533325 + 50.0 * 8.446250915527344
Epoch 1750, val loss: 0.5840592980384827
Epoch 1760, training loss: 422.8236999511719 = 0.5240263342857361 + 50.0 * 8.445993423461914
Epoch 1760, val loss: 0.5837277770042419
Epoch 1770, training loss: 422.9084777832031 = 0.5232378244400024 + 50.0 * 8.447705268859863
Epoch 1770, val loss: 0.5834969878196716
Epoch 1780, training loss: 422.81201171875 = 0.5224016308784485 + 50.0 * 8.445792198181152
Epoch 1780, val loss: 0.5829591155052185
Epoch 1790, training loss: 422.8397521972656 = 0.521586537361145 + 50.0 * 8.44636344909668
Epoch 1790, val loss: 0.5827393531799316
Epoch 1800, training loss: 422.81585693359375 = 0.5207616686820984 + 50.0 * 8.445901870727539
Epoch 1800, val loss: 0.5822846293449402
Epoch 1810, training loss: 422.74945068359375 = 0.5199492573738098 + 50.0 * 8.444589614868164
Epoch 1810, val loss: 0.5819822549819946
Epoch 1820, training loss: 422.7271423339844 = 0.5191390514373779 + 50.0 * 8.444160461425781
Epoch 1820, val loss: 0.5816030502319336
Epoch 1830, training loss: 422.7596130371094 = 0.5183281898498535 + 50.0 * 8.444825172424316
Epoch 1830, val loss: 0.581306517124176
Epoch 1840, training loss: 423.0007629394531 = 0.517471194267273 + 50.0 * 8.449666023254395
Epoch 1840, val loss: 0.580899715423584
Epoch 1850, training loss: 422.716064453125 = 0.5165807008743286 + 50.0 * 8.443989753723145
Epoch 1850, val loss: 0.580441415309906
Epoch 1860, training loss: 422.6753845214844 = 0.5157285332679749 + 50.0 * 8.443193435668945
Epoch 1860, val loss: 0.5800729393959045
Epoch 1870, training loss: 422.6698913574219 = 0.5148971080780029 + 50.0 * 8.443099975585938
Epoch 1870, val loss: 0.5796891450881958
Epoch 1880, training loss: 422.66278076171875 = 0.514072060585022 + 50.0 * 8.442974090576172
Epoch 1880, val loss: 0.5793536305427551
Epoch 1890, training loss: 422.8983459472656 = 0.5132288336753845 + 50.0 * 8.447702407836914
Epoch 1890, val loss: 0.5789262652397156
Epoch 1900, training loss: 422.7027893066406 = 0.5123540163040161 + 50.0 * 8.443808555603027
Epoch 1900, val loss: 0.5785959959030151
Epoch 1910, training loss: 422.6335754394531 = 0.5114820003509521 + 50.0 * 8.442441940307617
Epoch 1910, val loss: 0.578147292137146
Epoch 1920, training loss: 422.6053466796875 = 0.5106349587440491 + 50.0 * 8.44189453125
Epoch 1920, val loss: 0.5777683258056641
Epoch 1930, training loss: 422.6030578613281 = 0.5097836852073669 + 50.0 * 8.441864967346191
Epoch 1930, val loss: 0.57741779088974
Epoch 1940, training loss: 422.7147216796875 = 0.5089274644851685 + 50.0 * 8.44411563873291
Epoch 1940, val loss: 0.5769864916801453
Epoch 1950, training loss: 422.5807800292969 = 0.5080282092094421 + 50.0 * 8.441454887390137
Epoch 1950, val loss: 0.5766420364379883
Epoch 1960, training loss: 422.55572509765625 = 0.5071488618850708 + 50.0 * 8.440971374511719
Epoch 1960, val loss: 0.5761970281600952
Epoch 1970, training loss: 422.55279541015625 = 0.5062718987464905 + 50.0 * 8.440930366516113
Epoch 1970, val loss: 0.5758107900619507
Epoch 1980, training loss: 422.6361389160156 = 0.5053949356079102 + 50.0 * 8.442614555358887
Epoch 1980, val loss: 0.5754704475402832
Epoch 1990, training loss: 422.658447265625 = 0.5044724345207214 + 50.0 * 8.443078994750977
Epoch 1990, val loss: 0.5749700665473938
Epoch 2000, training loss: 422.5671691894531 = 0.5035398602485657 + 50.0 * 8.441272735595703
Epoch 2000, val loss: 0.5744844675064087
Epoch 2010, training loss: 422.51702880859375 = 0.5026451945304871 + 50.0 * 8.440287590026855
Epoch 2010, val loss: 0.5741175413131714
Epoch 2020, training loss: 422.4854431152344 = 0.5017459392547607 + 50.0 * 8.439674377441406
Epoch 2020, val loss: 0.5736548900604248
Epoch 2030, training loss: 422.47674560546875 = 0.5008481740951538 + 50.0 * 8.439517974853516
Epoch 2030, val loss: 0.5732923150062561
Epoch 2040, training loss: 422.6106872558594 = 0.49993300437927246 + 50.0 * 8.442214965820312
Epoch 2040, val loss: 0.5729430317878723
Epoch 2050, training loss: 422.495361328125 = 0.4989664852619171 + 50.0 * 8.43992805480957
Epoch 2050, val loss: 0.5723431706428528
Epoch 2060, training loss: 422.442626953125 = 0.49802032113075256 + 50.0 * 8.438892364501953
Epoch 2060, val loss: 0.5719815492630005
Epoch 2070, training loss: 422.4415283203125 = 0.4970857799053192 + 50.0 * 8.438888549804688
Epoch 2070, val loss: 0.5714908838272095
Epoch 2080, training loss: 422.5382995605469 = 0.4961385428905487 + 50.0 * 8.44084358215332
Epoch 2080, val loss: 0.5710523724555969
Epoch 2090, training loss: 422.5826721191406 = 0.49515658617019653 + 50.0 * 8.441750526428223
Epoch 2090, val loss: 0.570638120174408
Epoch 2100, training loss: 422.4123840332031 = 0.4941570460796356 + 50.0 * 8.438364028930664
Epoch 2100, val loss: 0.5701500773429871
Epoch 2110, training loss: 422.4161376953125 = 0.49317726492881775 + 50.0 * 8.438459396362305
Epoch 2110, val loss: 0.5697939395904541
Epoch 2120, training loss: 422.36383056640625 = 0.4922151267528534 + 50.0 * 8.437432289123535
Epoch 2120, val loss: 0.569311261177063
Epoch 2130, training loss: 422.3598937988281 = 0.491249144077301 + 50.0 * 8.437373161315918
Epoch 2130, val loss: 0.568878710269928
Epoch 2140, training loss: 422.4102783203125 = 0.49027350544929504 + 50.0 * 8.438400268554688
Epoch 2140, val loss: 0.5684388875961304
Epoch 2150, training loss: 422.4000549316406 = 0.4892647862434387 + 50.0 * 8.438216209411621
Epoch 2150, val loss: 0.5679607391357422
Epoch 2160, training loss: 422.3819274902344 = 0.4882476031780243 + 50.0 * 8.437873840332031
Epoch 2160, val loss: 0.5675843954086304
Epoch 2170, training loss: 422.4997863769531 = 0.4872080981731415 + 50.0 * 8.440251350402832
Epoch 2170, val loss: 0.567046582698822
Epoch 2180, training loss: 422.3455810546875 = 0.486162394285202 + 50.0 * 8.437188148498535
Epoch 2180, val loss: 0.5665777325630188
Epoch 2190, training loss: 422.29510498046875 = 0.48513704538345337 + 50.0 * 8.436199188232422
Epoch 2190, val loss: 0.5660788416862488
Epoch 2200, training loss: 422.2760314941406 = 0.4841318130493164 + 50.0 * 8.435837745666504
Epoch 2200, val loss: 0.5656679272651672
Epoch 2210, training loss: 422.25494384765625 = 0.4831185042858124 + 50.0 * 8.435436248779297
Epoch 2210, val loss: 0.5651934146881104
Epoch 2220, training loss: 422.25213623046875 = 0.48210078477859497 + 50.0 * 8.43540096282959
Epoch 2220, val loss: 0.5647673010826111
Epoch 2230, training loss: 422.5048522949219 = 0.48107248544692993 + 50.0 * 8.440475463867188
Epoch 2230, val loss: 0.5642982721328735
Epoch 2240, training loss: 422.33563232421875 = 0.47996464371681213 + 50.0 * 8.437113761901855
Epoch 2240, val loss: 0.5638684630393982
Epoch 2250, training loss: 422.2605285644531 = 0.47887757420539856 + 50.0 * 8.435632705688477
Epoch 2250, val loss: 0.5632069706916809
Epoch 2260, training loss: 422.22412109375 = 0.477811723947525 + 50.0 * 8.43492603302002
Epoch 2260, val loss: 0.5627859234809875
Epoch 2270, training loss: 422.1990661621094 = 0.4767586588859558 + 50.0 * 8.434446334838867
Epoch 2270, val loss: 0.5623354911804199
Epoch 2280, training loss: 422.2223815917969 = 0.47569358348846436 + 50.0 * 8.43493366241455
Epoch 2280, val loss: 0.561810314655304
Epoch 2290, training loss: 422.39959716796875 = 0.474606454372406 + 50.0 * 8.438499450683594
Epoch 2290, val loss: 0.561281144618988
Epoch 2300, training loss: 422.2505798339844 = 0.4734920859336853 + 50.0 * 8.435542106628418
Epoch 2300, val loss: 0.5609725713729858
Epoch 2310, training loss: 422.16900634765625 = 0.4723827540874481 + 50.0 * 8.433932304382324
Epoch 2310, val loss: 0.5603082180023193
Epoch 2320, training loss: 422.1382141113281 = 0.471301794052124 + 50.0 * 8.433338165283203
Epoch 2320, val loss: 0.5599291920661926
Epoch 2330, training loss: 422.13555908203125 = 0.47022339701652527 + 50.0 * 8.433306694030762
Epoch 2330, val loss: 0.559439480304718
Epoch 2340, training loss: 422.2437744140625 = 0.4691314697265625 + 50.0 * 8.435493469238281
Epoch 2340, val loss: 0.5589600205421448
Epoch 2350, training loss: 422.15960693359375 = 0.4679945111274719 + 50.0 * 8.433832168579102
Epoch 2350, val loss: 0.5584743618965149
Epoch 2360, training loss: 422.1106262207031 = 0.4668522775173187 + 50.0 * 8.432875633239746
Epoch 2360, val loss: 0.5579142570495605
Epoch 2370, training loss: 422.09454345703125 = 0.46573352813720703 + 50.0 * 8.432576179504395
Epoch 2370, val loss: 0.55744469165802
Epoch 2380, training loss: 422.1019592285156 = 0.4646209478378296 + 50.0 * 8.432746887207031
Epoch 2380, val loss: 0.5570176839828491
Epoch 2390, training loss: 422.19915771484375 = 0.4634937345981598 + 50.0 * 8.434713363647461
Epoch 2390, val loss: 0.5564887523651123
Epoch 2400, training loss: 422.14349365234375 = 0.4623641073703766 + 50.0 * 8.433622360229492
Epoch 2400, val loss: 0.5560621619224548
Epoch 2410, training loss: 422.1720275878906 = 0.46121224761009216 + 50.0 * 8.434216499328613
Epoch 2410, val loss: 0.5555853247642517
Epoch 2420, training loss: 422.0523986816406 = 0.4600577652454376 + 50.0 * 8.431846618652344
Epoch 2420, val loss: 0.5550234913825989
Epoch 2430, training loss: 422.04071044921875 = 0.45891883969306946 + 50.0 * 8.431635856628418
Epoch 2430, val loss: 0.5545153617858887
Epoch 2440, training loss: 422.07501220703125 = 0.45778992772102356 + 50.0 * 8.432344436645508
Epoch 2440, val loss: 0.5540109276771545
Epoch 2450, training loss: 422.1119689941406 = 0.456636518239975 + 50.0 * 8.433106422424316
Epoch 2450, val loss: 0.5535642504692078
Epoch 2460, training loss: 422.0006103515625 = 0.4554769992828369 + 50.0 * 8.430902481079102
Epoch 2460, val loss: 0.5532124042510986
Epoch 2470, training loss: 422.07843017578125 = 0.45434024930000305 + 50.0 * 8.43248176574707
Epoch 2470, val loss: 0.5528908967971802
Epoch 2480, training loss: 422.0975646972656 = 0.45315712690353394 + 50.0 * 8.43288803100586
Epoch 2480, val loss: 0.5522221922874451
Epoch 2490, training loss: 421.9918518066406 = 0.4519708752632141 + 50.0 * 8.430797576904297
Epoch 2490, val loss: 0.5518734455108643
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7803145611364789
0.8138810403535464
=== training gcn model ===
Epoch 0, training loss: 530.2091674804688 = 1.095466136932373 + 50.0 * 10.582274436950684
Epoch 0, val loss: 1.0935956239700317
Epoch 10, training loss: 530.1868896484375 = 1.0916478633880615 + 50.0 * 10.581904411315918
Epoch 10, val loss: 1.0897984504699707
Epoch 20, training loss: 530.10693359375 = 1.087388277053833 + 50.0 * 10.580390930175781
Epoch 20, val loss: 1.0855751037597656
Epoch 30, training loss: 529.7849731445312 = 1.082646131515503 + 50.0 * 10.57404613494873
Epoch 30, val loss: 1.0809035301208496
Epoch 40, training loss: 528.4994506835938 = 1.0773760080337524 + 50.0 * 10.548440933227539
Epoch 40, val loss: 1.0757195949554443
Epoch 50, training loss: 523.9368286132812 = 1.0714291334152222 + 50.0 * 10.457308769226074
Epoch 50, val loss: 1.0699087381362915
Epoch 60, training loss: 509.6646728515625 = 1.0649828910827637 + 50.0 * 10.17199420928955
Epoch 60, val loss: 1.0637216567993164
Epoch 70, training loss: 487.26019287109375 = 1.0582172870635986 + 50.0 * 9.724039077758789
Epoch 70, val loss: 1.0572766065597534
Epoch 80, training loss: 483.5447998046875 = 1.0524977445602417 + 50.0 * 9.649846076965332
Epoch 80, val loss: 1.0519520044326782
Epoch 90, training loss: 480.2641906738281 = 1.0481897592544556 + 50.0 * 9.584320068359375
Epoch 90, val loss: 1.0479683876037598
Epoch 100, training loss: 474.64556884765625 = 1.0448071956634521 + 50.0 * 9.472015380859375
Epoch 100, val loss: 1.044806957244873
Epoch 110, training loss: 467.341064453125 = 1.0416737794876099 + 50.0 * 9.325987815856934
Epoch 110, val loss: 1.0417879819869995
Epoch 120, training loss: 462.1395568847656 = 1.0382986068725586 + 50.0 * 9.222024917602539
Epoch 120, val loss: 1.0385088920593262
Epoch 130, training loss: 458.2848205566406 = 1.0351197719573975 + 50.0 * 9.144993782043457
Epoch 130, val loss: 1.0354937314987183
Epoch 140, training loss: 452.78790283203125 = 1.0328986644744873 + 50.0 * 9.035099983215332
Epoch 140, val loss: 1.0334784984588623
Epoch 150, training loss: 447.7443542480469 = 1.0316214561462402 + 50.0 * 8.93425464630127
Epoch 150, val loss: 1.032294511795044
Epoch 160, training loss: 445.0564270019531 = 1.0299179553985596 + 50.0 * 8.88053035736084
Epoch 160, val loss: 1.0305304527282715
Epoch 170, training loss: 443.0618896484375 = 1.0268131494522095 + 50.0 * 8.84070110321045
Epoch 170, val loss: 1.0274218320846558
Epoch 180, training loss: 441.4959716796875 = 1.0230382680892944 + 50.0 * 8.80945873260498
Epoch 180, val loss: 1.0237934589385986
Epoch 190, training loss: 440.2997131347656 = 1.0192537307739258 + 50.0 * 8.785609245300293
Epoch 190, val loss: 1.0201640129089355
Epoch 200, training loss: 439.0155334472656 = 1.0156633853912354 + 50.0 * 8.759997367858887
Epoch 200, val loss: 1.0167521238327026
Epoch 210, training loss: 437.66802978515625 = 1.0122193098068237 + 50.0 * 8.733116149902344
Epoch 210, val loss: 1.013498067855835
Epoch 220, training loss: 436.3787841796875 = 1.0086758136749268 + 50.0 * 8.707402229309082
Epoch 220, val loss: 1.0101332664489746
Epoch 230, training loss: 435.39520263671875 = 1.0047768354415894 + 50.0 * 8.6878080368042
Epoch 230, val loss: 1.0063753128051758
Epoch 240, training loss: 434.5786437988281 = 1.0003855228424072 + 50.0 * 8.671565055847168
Epoch 240, val loss: 1.0021523237228394
Epoch 250, training loss: 433.90911865234375 = 0.9955740571022034 + 50.0 * 8.658270835876465
Epoch 250, val loss: 0.9975431561470032
Epoch 260, training loss: 433.39312744140625 = 0.9904229640960693 + 50.0 * 8.648054122924805
Epoch 260, val loss: 0.9925921559333801
Epoch 270, training loss: 433.0498962402344 = 0.9849125146865845 + 50.0 * 8.6412992477417
Epoch 270, val loss: 0.9872844815254211
Epoch 280, training loss: 432.57891845703125 = 0.9789857864379883 + 50.0 * 8.631999015808105
Epoch 280, val loss: 0.9816038012504578
Epoch 290, training loss: 432.18865966796875 = 0.9727287292480469 + 50.0 * 8.624319076538086
Epoch 290, val loss: 0.9756165146827698
Epoch 300, training loss: 431.7846984863281 = 0.9661778807640076 + 50.0 * 8.61637020111084
Epoch 300, val loss: 0.9693449139595032
Epoch 310, training loss: 431.4244079589844 = 0.9593062996864319 + 50.0 * 8.609301567077637
Epoch 310, val loss: 0.962786078453064
Epoch 320, training loss: 431.1585693359375 = 0.9520828127861023 + 50.0 * 8.604129791259766
Epoch 320, val loss: 0.9559099078178406
Epoch 330, training loss: 430.7683410644531 = 0.9445043802261353 + 50.0 * 8.596476554870605
Epoch 330, val loss: 0.9486878514289856
Epoch 340, training loss: 430.4825134277344 = 0.9366446137428284 + 50.0 * 8.590917587280273
Epoch 340, val loss: 0.9412100911140442
Epoch 350, training loss: 430.1932373046875 = 0.928509533405304 + 50.0 * 8.585294723510742
Epoch 350, val loss: 0.933498740196228
Epoch 360, training loss: 429.9609680175781 = 0.9201094508171082 + 50.0 * 8.580817222595215
Epoch 360, val loss: 0.9255198836326599
Epoch 370, training loss: 429.7171325683594 = 0.9114362001419067 + 50.0 * 8.5761137008667
Epoch 370, val loss: 0.917317807674408
Epoch 380, training loss: 429.4700012207031 = 0.9025635123252869 + 50.0 * 8.571349143981934
Epoch 380, val loss: 0.9089667201042175
Epoch 390, training loss: 429.41168212890625 = 0.8935251235961914 + 50.0 * 8.57036304473877
Epoch 390, val loss: 0.9004628658294678
Epoch 400, training loss: 429.05157470703125 = 0.884326159954071 + 50.0 * 8.563344955444336
Epoch 400, val loss: 0.8917772173881531
Epoch 410, training loss: 428.86627197265625 = 0.875042200088501 + 50.0 * 8.55982494354248
Epoch 410, val loss: 0.8830707669258118
Epoch 420, training loss: 428.68682861328125 = 0.8657149076461792 + 50.0 * 8.556422233581543
Epoch 420, val loss: 0.8743379712104797
Epoch 430, training loss: 428.68310546875 = 0.8563323020935059 + 50.0 * 8.556535720825195
Epoch 430, val loss: 0.8655853867530823
Epoch 440, training loss: 428.33953857421875 = 0.8469284772872925 + 50.0 * 8.54985237121582
Epoch 440, val loss: 0.8568052649497986
Epoch 450, training loss: 428.182373046875 = 0.8375915884971619 + 50.0 * 8.546895980834961
Epoch 450, val loss: 0.848111093044281
Epoch 460, training loss: 428.0292663574219 = 0.8283288478851318 + 50.0 * 8.544018745422363
Epoch 460, val loss: 0.839523196220398
Epoch 470, training loss: 427.96429443359375 = 0.8191466331481934 + 50.0 * 8.542902946472168
Epoch 470, val loss: 0.8310331702232361
Epoch 480, training loss: 427.757568359375 = 0.8100458979606628 + 50.0 * 8.538949966430664
Epoch 480, val loss: 0.8226691484451294
Epoch 490, training loss: 427.7455749511719 = 0.801116943359375 + 50.0 * 8.538888931274414
Epoch 490, val loss: 0.8144582509994507
Epoch 500, training loss: 427.5055236816406 = 0.792314350605011 + 50.0 * 8.534263610839844
Epoch 500, val loss: 0.8064374923706055
Epoch 510, training loss: 427.3077392578125 = 0.7837613224983215 + 50.0 * 8.530479431152344
Epoch 510, val loss: 0.798645555973053
Epoch 520, training loss: 427.1729431152344 = 0.7754347324371338 + 50.0 * 8.527950286865234
Epoch 520, val loss: 0.7910984754562378
Epoch 530, training loss: 427.0367736816406 = 0.7673389911651611 + 50.0 * 8.525388717651367
Epoch 530, val loss: 0.7838050127029419
Epoch 540, training loss: 427.07366943359375 = 0.7594528198242188 + 50.0 * 8.526284217834473
Epoch 540, val loss: 0.7767252922058105
Epoch 550, training loss: 426.8510437011719 = 0.7517889738082886 + 50.0 * 8.521985054016113
Epoch 550, val loss: 0.7698811888694763
Epoch 560, training loss: 426.6832580566406 = 0.7444024085998535 + 50.0 * 8.518776893615723
Epoch 560, val loss: 0.7633224725723267
Epoch 570, training loss: 426.54913330078125 = 0.7372808456420898 + 50.0 * 8.516237258911133
Epoch 570, val loss: 0.7570494413375854
Epoch 580, training loss: 426.59930419921875 = 0.7304027080535889 + 50.0 * 8.517377853393555
Epoch 580, val loss: 0.7510254383087158
Epoch 590, training loss: 426.3548583984375 = 0.7237178087234497 + 50.0 * 8.512622833251953
Epoch 590, val loss: 0.7452205419540405
Epoch 600, training loss: 426.22906494140625 = 0.7173032760620117 + 50.0 * 8.510234832763672
Epoch 600, val loss: 0.7396853566169739
Epoch 610, training loss: 426.2117919921875 = 0.7111241221427917 + 50.0 * 8.510013580322266
Epoch 610, val loss: 0.734377920627594
Epoch 620, training loss: 426.0596618652344 = 0.7051585912704468 + 50.0 * 8.507089614868164
Epoch 620, val loss: 0.729304850101471
Epoch 630, training loss: 425.9652099609375 = 0.6994324326515198 + 50.0 * 8.505315780639648
Epoch 630, val loss: 0.7244709134101868
Epoch 640, training loss: 425.98968505859375 = 0.6939257383346558 + 50.0 * 8.505914688110352
Epoch 640, val loss: 0.7198590040206909
Epoch 650, training loss: 425.8326110839844 = 0.6886085867881775 + 50.0 * 8.502880096435547
Epoch 650, val loss: 0.7154463529586792
Epoch 660, training loss: 425.7371826171875 = 0.683530867099762 + 50.0 * 8.501072883605957
Epoch 660, val loss: 0.711279571056366
Epoch 670, training loss: 425.69085693359375 = 0.6786569952964783 + 50.0 * 8.500244140625
Epoch 670, val loss: 0.7073186039924622
Epoch 680, training loss: 425.7102966308594 = 0.673949658870697 + 50.0 * 8.500726699829102
Epoch 680, val loss: 0.7035287022590637
Epoch 690, training loss: 425.56610107421875 = 0.669420063495636 + 50.0 * 8.497933387756348
Epoch 690, val loss: 0.6999088525772095
Epoch 700, training loss: 425.4912414550781 = 0.6650979518890381 + 50.0 * 8.496522903442383
Epoch 700, val loss: 0.696536660194397
Epoch 710, training loss: 425.4347839355469 = 0.660964846611023 + 50.0 * 8.495476722717285
Epoch 710, val loss: 0.6933450102806091
Epoch 720, training loss: 425.4089660644531 = 0.6570026874542236 + 50.0 * 8.495038986206055
Epoch 720, val loss: 0.6903288960456848
Epoch 730, training loss: 425.3575744628906 = 0.6531548500061035 + 50.0 * 8.494088172912598
Epoch 730, val loss: 0.6874228715896606
Epoch 740, training loss: 425.3529052734375 = 0.6494594216346741 + 50.0 * 8.49406909942627
Epoch 740, val loss: 0.6847090721130371
Epoch 750, training loss: 425.2443542480469 = 0.6459373235702515 + 50.0 * 8.491968154907227
Epoch 750, val loss: 0.6821401119232178
Epoch 760, training loss: 425.1824645996094 = 0.6425856947898865 + 50.0 * 8.490797996520996
Epoch 760, val loss: 0.6797558069229126
Epoch 770, training loss: 425.166259765625 = 0.6393858790397644 + 50.0 * 8.490537643432617
Epoch 770, val loss: 0.6775240898132324
Epoch 780, training loss: 425.0851745605469 = 0.6362950205802917 + 50.0 * 8.488977432250977
Epoch 780, val loss: 0.6753917932510376
Epoch 790, training loss: 425.0282287597656 = 0.6333454847335815 + 50.0 * 8.487897872924805
Epoch 790, val loss: 0.6734155416488647
Epoch 800, training loss: 425.04998779296875 = 0.6305283308029175 + 50.0 * 8.488389015197754
Epoch 800, val loss: 0.671556830406189
Epoch 810, training loss: 424.9739685058594 = 0.6278089284896851 + 50.0 * 8.486923217773438
Epoch 810, val loss: 0.6698302030563354
Epoch 820, training loss: 424.86016845703125 = 0.6252100467681885 + 50.0 * 8.484699249267578
Epoch 820, val loss: 0.668185293674469
Epoch 830, training loss: 424.7991943359375 = 0.6227470636367798 + 50.0 * 8.483529090881348
Epoch 830, val loss: 0.6666813492774963
Epoch 840, training loss: 424.78350830078125 = 0.6203932762145996 + 50.0 * 8.483262062072754
Epoch 840, val loss: 0.6652836203575134
Epoch 850, training loss: 424.75836181640625 = 0.6180907487869263 + 50.0 * 8.482805252075195
Epoch 850, val loss: 0.6639611124992371
Epoch 860, training loss: 424.6932067871094 = 0.6158648729324341 + 50.0 * 8.481546401977539
Epoch 860, val loss: 0.662656843662262
Epoch 870, training loss: 424.5922546386719 = 0.6137694716453552 + 50.0 * 8.479569435119629
Epoch 870, val loss: 0.6615129113197327
Epoch 880, training loss: 424.5436096191406 = 0.6117732524871826 + 50.0 * 8.478636741638184
Epoch 880, val loss: 0.6604865789413452
Epoch 890, training loss: 424.48858642578125 = 0.6098523139953613 + 50.0 * 8.477574348449707
Epoch 890, val loss: 0.6594993472099304
Epoch 900, training loss: 424.9305114746094 = 0.6079912781715393 + 50.0 * 8.4864501953125
Epoch 900, val loss: 0.6585876941680908
Epoch 910, training loss: 424.4065856933594 = 0.6060873866081238 + 50.0 * 8.4760103225708
Epoch 910, val loss: 0.6575955748558044
Epoch 920, training loss: 424.3436584472656 = 0.6043224930763245 + 50.0 * 8.474786758422852
Epoch 920, val loss: 0.6567473411560059
Epoch 930, training loss: 424.2972106933594 = 0.6026618480682373 + 50.0 * 8.473891258239746
Epoch 930, val loss: 0.6559996604919434
Epoch 940, training loss: 424.25244140625 = 0.6010667085647583 + 50.0 * 8.473027229309082
Epoch 940, val loss: 0.6553042531013489
Epoch 950, training loss: 424.20263671875 = 0.5995175838470459 + 50.0 * 8.472062110900879
Epoch 950, val loss: 0.654658854007721
Epoch 960, training loss: 424.6508483886719 = 0.5979843139648438 + 50.0 * 8.481057167053223
Epoch 960, val loss: 0.6539996862411499
Epoch 970, training loss: 424.22406005859375 = 0.5964101552963257 + 50.0 * 8.472553253173828
Epoch 970, val loss: 0.6532978415489197
Epoch 980, training loss: 424.1219787597656 = 0.5949419140815735 + 50.0 * 8.470541000366211
Epoch 980, val loss: 0.652650773525238
Epoch 990, training loss: 424.05047607421875 = 0.5935559272766113 + 50.0 * 8.469138145446777
Epoch 990, val loss: 0.6520752310752869
Epoch 1000, training loss: 423.998291015625 = 0.5922127962112427 + 50.0 * 8.468121528625488
Epoch 1000, val loss: 0.651549756526947
Epoch 1010, training loss: 423.9544677734375 = 0.5908972024917603 + 50.0 * 8.467270851135254
Epoch 1010, val loss: 0.6510152816772461
Epoch 1020, training loss: 423.9156188964844 = 0.5896033644676208 + 50.0 * 8.466520309448242
Epoch 1020, val loss: 0.6504997611045837
Epoch 1030, training loss: 424.11431884765625 = 0.5883301496505737 + 50.0 * 8.47052001953125
Epoch 1030, val loss: 0.6499727368354797
Epoch 1040, training loss: 423.9841003417969 = 0.5869790315628052 + 50.0 * 8.467942237854004
Epoch 1040, val loss: 0.649340033531189
Epoch 1050, training loss: 423.8499450683594 = 0.5857017636299133 + 50.0 * 8.465285301208496
Epoch 1050, val loss: 0.6488300561904907
Epoch 1060, training loss: 423.7893981933594 = 0.5844898223876953 + 50.0 * 8.46409797668457
Epoch 1060, val loss: 0.6483438611030579
Epoch 1070, training loss: 423.74444580078125 = 0.5833126902580261 + 50.0 * 8.46322250366211
Epoch 1070, val loss: 0.6478887796401978
Epoch 1080, training loss: 423.7143249511719 = 0.5821532011032104 + 50.0 * 8.46264362335205
Epoch 1080, val loss: 0.6474127173423767
Epoch 1090, training loss: 423.8794860839844 = 0.5809937715530396 + 50.0 * 8.465970039367676
Epoch 1090, val loss: 0.6469317674636841
Epoch 1100, training loss: 423.7249450683594 = 0.5798073410987854 + 50.0 * 8.462903022766113
Epoch 1100, val loss: 0.646415650844574
Epoch 1110, training loss: 423.6289978027344 = 0.5786563754081726 + 50.0 * 8.461007118225098
Epoch 1110, val loss: 0.6459068059921265
Epoch 1120, training loss: 423.5813293457031 = 0.5775479078292847 + 50.0 * 8.460075378417969
Epoch 1120, val loss: 0.6454501748085022
Epoch 1130, training loss: 423.5498962402344 = 0.5764558911323547 + 50.0 * 8.459468841552734
Epoch 1130, val loss: 0.644997775554657
Epoch 1140, training loss: 423.51995849609375 = 0.575371503829956 + 50.0 * 8.458891868591309
Epoch 1140, val loss: 0.6445302963256836
Epoch 1150, training loss: 423.65338134765625 = 0.5742910504341125 + 50.0 * 8.46158218383789
Epoch 1150, val loss: 0.6440800428390503
Epoch 1160, training loss: 423.6604919433594 = 0.5731221437454224 + 50.0 * 8.461747169494629
Epoch 1160, val loss: 0.6434678435325623
Epoch 1170, training loss: 423.4917907714844 = 0.5719954967498779 + 50.0 * 8.458395957946777
Epoch 1170, val loss: 0.6429088115692139
Epoch 1180, training loss: 423.4164733886719 = 0.570931077003479 + 50.0 * 8.456911087036133
Epoch 1180, val loss: 0.642431378364563
Epoch 1190, training loss: 423.38507080078125 = 0.569890022277832 + 50.0 * 8.456303596496582
Epoch 1190, val loss: 0.6419476270675659
Epoch 1200, training loss: 423.36456298828125 = 0.5688557028770447 + 50.0 * 8.455914497375488
Epoch 1200, val loss: 0.6414576768875122
Epoch 1210, training loss: 423.4850769042969 = 0.5678117275238037 + 50.0 * 8.458345413208008
Epoch 1210, val loss: 0.6409415602684021
Epoch 1220, training loss: 423.3477478027344 = 0.5667317509651184 + 50.0 * 8.455619812011719
Epoch 1220, val loss: 0.6403605341911316
Epoch 1230, training loss: 423.29693603515625 = 0.5656761527061462 + 50.0 * 8.454625129699707
Epoch 1230, val loss: 0.6398362517356873
Epoch 1240, training loss: 423.2700500488281 = 0.564636766910553 + 50.0 * 8.454108238220215
Epoch 1240, val loss: 0.639267086982727
Epoch 1250, training loss: 423.4811096191406 = 0.5635916590690613 + 50.0 * 8.45835018157959
Epoch 1250, val loss: 0.6386924982070923
Epoch 1260, training loss: 423.28631591796875 = 0.5625048279762268 + 50.0 * 8.454476356506348
Epoch 1260, val loss: 0.6381493210792542
Epoch 1270, training loss: 423.18328857421875 = 0.561448335647583 + 50.0 * 8.452436447143555
Epoch 1270, val loss: 0.637557864189148
Epoch 1280, training loss: 423.16595458984375 = 0.5604123473167419 + 50.0 * 8.452110290527344
Epoch 1280, val loss: 0.6369609832763672
Epoch 1290, training loss: 423.1441955566406 = 0.5593799948692322 + 50.0 * 8.451696395874023
Epoch 1290, val loss: 0.6364302635192871
Epoch 1300, training loss: 423.6604919433594 = 0.5583204627037048 + 50.0 * 8.462043762207031
Epoch 1300, val loss: 0.6358090043067932
Epoch 1310, training loss: 423.14630126953125 = 0.557176411151886 + 50.0 * 8.4517822265625
Epoch 1310, val loss: 0.6351063847541809
Epoch 1320, training loss: 423.0851135253906 = 0.5560908317565918 + 50.0 * 8.450580596923828
Epoch 1320, val loss: 0.6344888806343079
Epoch 1330, training loss: 423.05560302734375 = 0.5550426840782166 + 50.0 * 8.450011253356934
Epoch 1330, val loss: 0.633902370929718
Epoch 1340, training loss: 423.0315246582031 = 0.5539998412132263 + 50.0 * 8.44955062866211
Epoch 1340, val loss: 0.6333160400390625
Epoch 1350, training loss: 423.0099182128906 = 0.5529512763023376 + 50.0 * 8.449139595031738
Epoch 1350, val loss: 0.6327027678489685
Epoch 1360, training loss: 422.9899597167969 = 0.551895260810852 + 50.0 * 8.448760986328125
Epoch 1360, val loss: 0.6320815682411194
Epoch 1370, training loss: 422.9892578125 = 0.5508322715759277 + 50.0 * 8.448768615722656
Epoch 1370, val loss: 0.6314684748649597
Epoch 1380, training loss: 423.1664733886719 = 0.5497130155563354 + 50.0 * 8.452335357666016
Epoch 1380, val loss: 0.6307268142700195
Epoch 1390, training loss: 423.0099182128906 = 0.5485470294952393 + 50.0 * 8.449227333068848
Epoch 1390, val loss: 0.6300305128097534
Epoch 1400, training loss: 422.91412353515625 = 0.5474345684051514 + 50.0 * 8.447334289550781
Epoch 1400, val loss: 0.6293016076087952
Epoch 1410, training loss: 422.9015197753906 = 0.5463472008705139 + 50.0 * 8.447103500366211
Epoch 1410, val loss: 0.6285943984985352
Epoch 1420, training loss: 422.8740539550781 = 0.5452663898468018 + 50.0 * 8.446576118469238
Epoch 1420, val loss: 0.627935528755188
Epoch 1430, training loss: 422.8526916503906 = 0.54417884349823 + 50.0 * 8.44616985321045
Epoch 1430, val loss: 0.6272366046905518
Epoch 1440, training loss: 422.8359375 = 0.5430829524993896 + 50.0 * 8.445857048034668
Epoch 1440, val loss: 0.6265338659286499
Epoch 1450, training loss: 423.0741271972656 = 0.5419735312461853 + 50.0 * 8.450643539428711
Epoch 1450, val loss: 0.6258253455162048
Epoch 1460, training loss: 422.8885803222656 = 0.540793776512146 + 50.0 * 8.446955680847168
Epoch 1460, val loss: 0.6249809861183167
Epoch 1470, training loss: 422.8346252441406 = 0.5396426916122437 + 50.0 * 8.445899963378906
Epoch 1470, val loss: 0.6242794990539551
Epoch 1480, training loss: 422.76922607421875 = 0.5385103225708008 + 50.0 * 8.44461441040039
Epoch 1480, val loss: 0.6235312819480896
Epoch 1490, training loss: 422.74267578125 = 0.537386953830719 + 50.0 * 8.444106101989746
Epoch 1490, val loss: 0.6227755546569824
Epoch 1500, training loss: 422.7205810546875 = 0.5362685918807983 + 50.0 * 8.443686485290527
Epoch 1500, val loss: 0.6220555305480957
Epoch 1510, training loss: 422.8756103515625 = 0.535142183303833 + 50.0 * 8.446808815002441
Epoch 1510, val loss: 0.6213344931602478
Epoch 1520, training loss: 422.7509765625 = 0.5339391231536865 + 50.0 * 8.444340705871582
Epoch 1520, val loss: 0.6204110383987427
Epoch 1530, training loss: 422.71624755859375 = 0.5327528119087219 + 50.0 * 8.443670272827148
Epoch 1530, val loss: 0.6196393370628357
Epoch 1540, training loss: 422.6459045410156 = 0.5315989851951599 + 50.0 * 8.442286491394043
Epoch 1540, val loss: 0.6188255548477173
Epoch 1550, training loss: 422.6264343261719 = 0.5304512977600098 + 50.0 * 8.441919326782227
Epoch 1550, val loss: 0.6180429458618164
Epoch 1560, training loss: 422.60589599609375 = 0.5293026566505432 + 50.0 * 8.441532135009766
Epoch 1560, val loss: 0.6172572374343872
Epoch 1570, training loss: 422.6013488769531 = 0.5281442403793335 + 50.0 * 8.4414644241333
Epoch 1570, val loss: 0.6164659857749939
Epoch 1580, training loss: 422.7620544433594 = 0.5269524455070496 + 50.0 * 8.4447021484375
Epoch 1580, val loss: 0.6156210899353027
Epoch 1590, training loss: 422.587890625 = 0.5257075428962708 + 50.0 * 8.441244125366211
Epoch 1590, val loss: 0.6147293448448181
Epoch 1600, training loss: 422.5447082519531 = 0.5244966745376587 + 50.0 * 8.440403938293457
Epoch 1600, val loss: 0.6138346791267395
Epoch 1610, training loss: 422.5159606933594 = 0.5232990980148315 + 50.0 * 8.43985366821289
Epoch 1610, val loss: 0.6129591464996338
Epoch 1620, training loss: 422.4976501464844 = 0.5221011638641357 + 50.0 * 8.4395112991333
Epoch 1620, val loss: 0.6121426224708557
Epoch 1630, training loss: 422.7300720214844 = 0.5208921432495117 + 50.0 * 8.444183349609375
Epoch 1630, val loss: 0.6112626791000366
Epoch 1640, training loss: 422.59185791015625 = 0.519599437713623 + 50.0 * 8.441445350646973
Epoch 1640, val loss: 0.6103082895278931
Epoch 1650, training loss: 422.44879150390625 = 0.518330454826355 + 50.0 * 8.43860912322998
Epoch 1650, val loss: 0.6093767285346985
Epoch 1660, training loss: 422.43475341796875 = 0.5170847177505493 + 50.0 * 8.438353538513184
Epoch 1660, val loss: 0.6084636449813843
Epoch 1670, training loss: 422.4254150390625 = 0.5158366560935974 + 50.0 * 8.438191413879395
Epoch 1670, val loss: 0.6075426936149597
Epoch 1680, training loss: 422.47222900390625 = 0.5145674347877502 + 50.0 * 8.439153671264648
Epoch 1680, val loss: 0.6066200733184814
Epoch 1690, training loss: 422.4566955566406 = 0.5132725238800049 + 50.0 * 8.438868522644043
Epoch 1690, val loss: 0.6057018637657166
Epoch 1700, training loss: 422.34228515625 = 0.511947751045227 + 50.0 * 8.436606407165527
Epoch 1700, val loss: 0.6046316623687744
Epoch 1710, training loss: 422.3309631347656 = 0.5106399059295654 + 50.0 * 8.436406135559082
Epoch 1710, val loss: 0.6036832332611084
Epoch 1720, training loss: 422.3149719238281 = 0.5093353390693665 + 50.0 * 8.436112403869629
Epoch 1720, val loss: 0.6027119755744934
Epoch 1730, training loss: 422.3357238769531 = 0.5080208778381348 + 50.0 * 8.436553955078125
Epoch 1730, val loss: 0.6017032265663147
Epoch 1740, training loss: 422.41583251953125 = 0.5066735148429871 + 50.0 * 8.438182830810547
Epoch 1740, val loss: 0.600669264793396
Epoch 1750, training loss: 422.3180236816406 = 0.5052984952926636 + 50.0 * 8.436254501342773
Epoch 1750, val loss: 0.5996561050415039
Epoch 1760, training loss: 422.2691345214844 = 0.503945529460907 + 50.0 * 8.435303688049316
Epoch 1760, val loss: 0.5986248850822449
Epoch 1770, training loss: 422.26806640625 = 0.5025875568389893 + 50.0 * 8.435309410095215
Epoch 1770, val loss: 0.5976079702377319
Epoch 1780, training loss: 422.2034606933594 = 0.5012181401252747 + 50.0 * 8.43404483795166
Epoch 1780, val loss: 0.5965933203697205
Epoch 1790, training loss: 422.1939392089844 = 0.499850869178772 + 50.0 * 8.433881759643555
Epoch 1790, val loss: 0.595561146736145
Epoch 1800, training loss: 422.1950988769531 = 0.4984745681285858 + 50.0 * 8.433932304382324
Epoch 1800, val loss: 0.5945271849632263
Epoch 1810, training loss: 422.3467102050781 = 0.49706724286079407 + 50.0 * 8.436992645263672
Epoch 1810, val loss: 0.5934144854545593
Epoch 1820, training loss: 422.21160888671875 = 0.49563100934028625 + 50.0 * 8.434319496154785
Epoch 1820, val loss: 0.5923767685890198
Epoch 1830, training loss: 422.1180419921875 = 0.4942041337490082 + 50.0 * 8.432476997375488
Epoch 1830, val loss: 0.5913290977478027
Epoch 1840, training loss: 422.119140625 = 0.4927935302257538 + 50.0 * 8.432526588439941
Epoch 1840, val loss: 0.5902548432350159
Epoch 1850, training loss: 422.17498779296875 = 0.49138039350509644 + 50.0 * 8.433671951293945
Epoch 1850, val loss: 0.5892607569694519
Epoch 1860, training loss: 422.13946533203125 = 0.48992350697517395 + 50.0 * 8.432991027832031
Epoch 1860, val loss: 0.5881412029266357
Epoch 1870, training loss: 422.0921936035156 = 0.48845523595809937 + 50.0 * 8.432074546813965
Epoch 1870, val loss: 0.5870810747146606
Epoch 1880, training loss: 422.055908203125 = 0.4869932234287262 + 50.0 * 8.431378364562988
Epoch 1880, val loss: 0.5860592722892761
Epoch 1890, training loss: 422.0256652832031 = 0.4855407774448395 + 50.0 * 8.430802345275879
Epoch 1890, val loss: 0.5850089192390442
Epoch 1900, training loss: 422.0065002441406 = 0.48408448696136475 + 50.0 * 8.430448532104492
Epoch 1900, val loss: 0.5839601159095764
Epoch 1910, training loss: 422.0076599121094 = 0.4826197326183319 + 50.0 * 8.430500984191895
Epoch 1910, val loss: 0.582890510559082
Epoch 1920, training loss: 422.2842712402344 = 0.48113515973091125 + 50.0 * 8.436062812805176
Epoch 1920, val loss: 0.5817840695381165
Epoch 1930, training loss: 422.07177734375 = 0.4795781373977661 + 50.0 * 8.431843757629395
Epoch 1930, val loss: 0.5807157754898071
Epoch 1940, training loss: 421.9914245605469 = 0.4780510663986206 + 50.0 * 8.430267333984375
Epoch 1940, val loss: 0.579534649848938
Epoch 1950, training loss: 421.9496154785156 = 0.4765387177467346 + 50.0 * 8.429461479187012
Epoch 1950, val loss: 0.5784958004951477
Epoch 1960, training loss: 421.9547424316406 = 0.47503069043159485 + 50.0 * 8.429594039916992
Epoch 1960, val loss: 0.5774232745170593
Epoch 1970, training loss: 422.0953674316406 = 0.47349074482917786 + 50.0 * 8.432437896728516
Epoch 1970, val loss: 0.5763285160064697
Epoch 1980, training loss: 421.9136657714844 = 0.4719223380088806 + 50.0 * 8.428834915161133
Epoch 1980, val loss: 0.5752557516098022
Epoch 1990, training loss: 421.9002990722656 = 0.47038155794143677 + 50.0 * 8.428598403930664
Epoch 1990, val loss: 0.5741581916809082
Epoch 2000, training loss: 421.88287353515625 = 0.46885016560554504 + 50.0 * 8.4282808303833
Epoch 2000, val loss: 0.5730917453765869
Epoch 2010, training loss: 421.8865966796875 = 0.4673217535018921 + 50.0 * 8.428385734558105
Epoch 2010, val loss: 0.5720593929290771
Epoch 2020, training loss: 421.98052978515625 = 0.46577972173690796 + 50.0 * 8.43029499053955
Epoch 2020, val loss: 0.5710473656654358
Epoch 2030, training loss: 421.8876953125 = 0.4641977548599243 + 50.0 * 8.42846965789795
Epoch 2030, val loss: 0.5699295997619629
Epoch 2040, training loss: 421.8447570800781 = 0.4626282751560211 + 50.0 * 8.427642822265625
Epoch 2040, val loss: 0.5689151287078857
Epoch 2050, training loss: 421.8254089355469 = 0.4610728919506073 + 50.0 * 8.427287101745605
Epoch 2050, val loss: 0.5679315328598022
Epoch 2060, training loss: 421.85247802734375 = 0.45951420068740845 + 50.0 * 8.42785930633545
Epoch 2060, val loss: 0.5669304728507996
Epoch 2070, training loss: 421.8531188964844 = 0.45793500542640686 + 50.0 * 8.427903175354004
Epoch 2070, val loss: 0.5659079551696777
Epoch 2080, training loss: 421.8172302246094 = 0.45634928345680237 + 50.0 * 8.427217483520508
Epoch 2080, val loss: 0.5649060606956482
Epoch 2090, training loss: 421.8175964355469 = 0.45477208495140076 + 50.0 * 8.42725658416748
Epoch 2090, val loss: 0.5639485716819763
Epoch 2100, training loss: 421.7843017578125 = 0.45318466424942017 + 50.0 * 8.42662239074707
Epoch 2100, val loss: 0.5629533529281616
Epoch 2110, training loss: 421.7691345214844 = 0.45159849524497986 + 50.0 * 8.426350593566895
Epoch 2110, val loss: 0.5619894862174988
Epoch 2120, training loss: 421.77093505859375 = 0.4500187635421753 + 50.0 * 8.42641830444336
Epoch 2120, val loss: 0.561076283454895
Epoch 2130, training loss: 421.8619079589844 = 0.4484253525733948 + 50.0 * 8.428269386291504
Epoch 2130, val loss: 0.5601121187210083
Epoch 2140, training loss: 421.7718200683594 = 0.4468068480491638 + 50.0 * 8.42650032043457
Epoch 2140, val loss: 0.5590519905090332
Epoch 2150, training loss: 421.7325134277344 = 0.4452177584171295 + 50.0 * 8.425745964050293
Epoch 2150, val loss: 0.5582040548324585
Epoch 2160, training loss: 421.7112731933594 = 0.4436224400997162 + 50.0 * 8.425353050231934
Epoch 2160, val loss: 0.55726158618927
Epoch 2170, training loss: 421.6960144042969 = 0.44203513860702515 + 50.0 * 8.425079345703125
Epoch 2170, val loss: 0.5563955307006836
Epoch 2180, training loss: 421.701904296875 = 0.4404509663581848 + 50.0 * 8.4252290725708
Epoch 2180, val loss: 0.5555597543716431
Epoch 2190, training loss: 421.7841796875 = 0.43885529041290283 + 50.0 * 8.42690658569336
Epoch 2190, val loss: 0.5546861290931702
Epoch 2200, training loss: 421.7682189941406 = 0.4372439980506897 + 50.0 * 8.426619529724121
Epoch 2200, val loss: 0.5538074970245361
Epoch 2210, training loss: 421.65875244140625 = 0.4356500208377838 + 50.0 * 8.42446231842041
Epoch 2210, val loss: 0.5529822707176208
Epoch 2220, training loss: 421.63104248046875 = 0.43406543135643005 + 50.0 * 8.42393970489502
Epoch 2220, val loss: 0.5521851778030396
Epoch 2230, training loss: 421.622802734375 = 0.43250373005867004 + 50.0 * 8.423806190490723
Epoch 2230, val loss: 0.5514459609985352
Epoch 2240, training loss: 421.60784912109375 = 0.43094077706336975 + 50.0 * 8.423538208007812
Epoch 2240, val loss: 0.5506719350814819
Epoch 2250, training loss: 421.6106872558594 = 0.42937901616096497 + 50.0 * 8.423625946044922
Epoch 2250, val loss: 0.5499162077903748
Epoch 2260, training loss: 421.9482116699219 = 0.4278111159801483 + 50.0 * 8.430407524108887
Epoch 2260, val loss: 0.5492231845855713
Epoch 2270, training loss: 421.6312561035156 = 0.4261843264102936 + 50.0 * 8.424101829528809
Epoch 2270, val loss: 0.548383355140686
Epoch 2280, training loss: 421.6048278808594 = 0.4246062934398651 + 50.0 * 8.423604965209961
Epoch 2280, val loss: 0.5477002859115601
Epoch 2290, training loss: 421.5637512207031 = 0.4230351746082306 + 50.0 * 8.42281436920166
Epoch 2290, val loss: 0.5470165610313416
Epoch 2300, training loss: 421.56201171875 = 0.421476274728775 + 50.0 * 8.422810554504395
Epoch 2300, val loss: 0.5463464856147766
Epoch 2310, training loss: 421.62750244140625 = 0.41992315649986267 + 50.0 * 8.424151420593262
Epoch 2310, val loss: 0.5456733703613281
Epoch 2320, training loss: 421.5703125 = 0.41834306716918945 + 50.0 * 8.423039436340332
Epoch 2320, val loss: 0.5449653267860413
Epoch 2330, training loss: 421.55511474609375 = 0.4167815148830414 + 50.0 * 8.42276668548584
Epoch 2330, val loss: 0.5443155169487
Epoch 2340, training loss: 421.5350036621094 = 0.4152320325374603 + 50.0 * 8.422395706176758
Epoch 2340, val loss: 0.5436573028564453
Epoch 2350, training loss: 421.54925537109375 = 0.4136888384819031 + 50.0 * 8.422711372375488
Epoch 2350, val loss: 0.5430530905723572
Epoch 2360, training loss: 421.5738525390625 = 0.41215193271636963 + 50.0 * 8.423233985900879
Epoch 2360, val loss: 0.5425048470497131
Epoch 2370, training loss: 421.48785400390625 = 0.4106138348579407 + 50.0 * 8.421545028686523
Epoch 2370, val loss: 0.5419424176216125
Epoch 2380, training loss: 421.492919921875 = 0.4090959429740906 + 50.0 * 8.421676635742188
Epoch 2380, val loss: 0.5414031744003296
Epoch 2390, training loss: 421.4796142578125 = 0.4075843095779419 + 50.0 * 8.421440124511719
Epoch 2390, val loss: 0.5409111976623535
Epoch 2400, training loss: 421.8611755371094 = 0.40607038140296936 + 50.0 * 8.429101943969727
Epoch 2400, val loss: 0.5404950380325317
Epoch 2410, training loss: 421.493408203125 = 0.4045233726501465 + 50.0 * 8.421777725219727
Epoch 2410, val loss: 0.5399539470672607
Epoch 2420, training loss: 421.4713439941406 = 0.40300488471984863 + 50.0 * 8.421366691589355
Epoch 2420, val loss: 0.5394163727760315
Epoch 2430, training loss: 421.4396667480469 = 0.4015156328678131 + 50.0 * 8.42076301574707
Epoch 2430, val loss: 0.5390127301216125
Epoch 2440, training loss: 421.4272766113281 = 0.400040864944458 + 50.0 * 8.420544624328613
Epoch 2440, val loss: 0.538628339767456
Epoch 2450, training loss: 421.422607421875 = 0.39857280254364014 + 50.0 * 8.420480728149414
Epoch 2450, val loss: 0.5382272005081177
Epoch 2460, training loss: 421.57110595703125 = 0.3971022367477417 + 50.0 * 8.423480033874512
Epoch 2460, val loss: 0.5377895832061768
Epoch 2470, training loss: 421.4244079589844 = 0.3956104815006256 + 50.0 * 8.420576095581055
Epoch 2470, val loss: 0.5375056266784668
Epoch 2480, training loss: 421.4200744628906 = 0.39412468671798706 + 50.0 * 8.42051887512207
Epoch 2480, val loss: 0.537078320980072
Epoch 2490, training loss: 421.3996887207031 = 0.3926788866519928 + 50.0 * 8.420140266418457
Epoch 2490, val loss: 0.536841630935669
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7894469812278031
0.8131565601680795
The final CL Acc:0.78725, 0.00501, The final GNN Acc:0.81403, 0.00078
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110716])
remove edge: torch.Size([2, 66846])
updated graph: torch.Size([2, 88914])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 530.2096557617188 = 1.095473051071167 + 50.0 * 10.582283973693848
Epoch 0, val loss: 1.0943328142166138
Epoch 10, training loss: 530.1864624023438 = 1.0912704467773438 + 50.0 * 10.581904411315918
Epoch 10, val loss: 1.0901356935501099
Epoch 20, training loss: 530.0993041992188 = 1.0867820978164673 + 50.0 * 10.580249786376953
Epoch 20, val loss: 1.0856508016586304
Epoch 30, training loss: 529.7108154296875 = 1.0820043087005615 + 50.0 * 10.572575569152832
Epoch 30, val loss: 1.080891489982605
Epoch 40, training loss: 527.9793090820312 = 1.0768342018127441 + 50.0 * 10.53804874420166
Epoch 40, val loss: 1.0757501125335693
Epoch 50, training loss: 521.4913940429688 = 1.0715092420578003 + 50.0 * 10.40839672088623
Epoch 50, val loss: 1.0705476999282837
Epoch 60, training loss: 502.0590515136719 = 1.0664633512496948 + 50.0 * 10.019851684570312
Epoch 60, val loss: 1.065611481666565
Epoch 70, training loss: 482.45306396484375 = 1.0613007545471191 + 50.0 * 9.627835273742676
Epoch 70, val loss: 1.0605735778808594
Epoch 80, training loss: 474.98199462890625 = 1.056731939315796 + 50.0 * 9.47850513458252
Epoch 80, val loss: 1.0562148094177246
Epoch 90, training loss: 466.7807922363281 = 1.0531208515167236 + 50.0 * 9.314553260803223
Epoch 90, val loss: 1.052815556526184
Epoch 100, training loss: 460.524169921875 = 1.0498329401016235 + 50.0 * 9.189486503601074
Epoch 100, val loss: 1.0497064590454102
Epoch 110, training loss: 456.2991638183594 = 1.0465142726898193 + 50.0 * 9.105052947998047
Epoch 110, val loss: 1.046575903892517
Epoch 120, training loss: 451.01544189453125 = 1.0432292222976685 + 50.0 * 8.999444007873535
Epoch 120, val loss: 1.04348623752594
Epoch 130, training loss: 446.7096862792969 = 1.040316104888916 + 50.0 * 8.913387298583984
Epoch 130, val loss: 1.0407265424728394
Epoch 140, training loss: 443.9001770019531 = 1.0374819040298462 + 50.0 * 8.857254028320312
Epoch 140, val loss: 1.0379666090011597
Epoch 150, training loss: 441.90338134765625 = 1.0344212055206299 + 50.0 * 8.817378997802734
Epoch 150, val loss: 1.0349841117858887
Epoch 160, training loss: 440.7793884277344 = 1.031166911125183 + 50.0 * 8.794964790344238
Epoch 160, val loss: 1.0318514108657837
Epoch 170, training loss: 440.0809020996094 = 1.0276540517807007 + 50.0 * 8.781064987182617
Epoch 170, val loss: 1.0284498929977417
Epoch 180, training loss: 439.4685363769531 = 1.0238206386566162 + 50.0 * 8.76889419555664
Epoch 180, val loss: 1.0247468948364258
Epoch 190, training loss: 438.8948974609375 = 1.019713282585144 + 50.0 * 8.757503509521484
Epoch 190, val loss: 1.0207957029342651
Epoch 200, training loss: 438.29534912109375 = 1.01542329788208 + 50.0 * 8.745598793029785
Epoch 200, val loss: 1.0166865587234497
Epoch 210, training loss: 437.604736328125 = 1.0110015869140625 + 50.0 * 8.731874465942383
Epoch 210, val loss: 1.0124469995498657
Epoch 220, training loss: 436.8580017089844 = 1.0063107013702393 + 50.0 * 8.717033386230469
Epoch 220, val loss: 1.007944941520691
Epoch 230, training loss: 435.8766174316406 = 1.0013142824172974 + 50.0 * 8.697505950927734
Epoch 230, val loss: 1.0031174421310425
Epoch 240, training loss: 434.8241271972656 = 0.9960986971855164 + 50.0 * 8.676560401916504
Epoch 240, val loss: 0.9980925917625427
Epoch 250, training loss: 433.86322021484375 = 0.9905526041984558 + 50.0 * 8.657453536987305
Epoch 250, val loss: 0.9927472472190857
Epoch 260, training loss: 433.0858459472656 = 0.9846004843711853 + 50.0 * 8.642024993896484
Epoch 260, val loss: 0.9870032668113708
Epoch 270, training loss: 432.4779968261719 = 0.9782383441925049 + 50.0 * 8.629995346069336
Epoch 270, val loss: 0.9808675050735474
Epoch 280, training loss: 431.9346008300781 = 0.9714004993438721 + 50.0 * 8.619263648986816
Epoch 280, val loss: 0.9742951393127441
Epoch 290, training loss: 431.4777526855469 = 0.9641642570495605 + 50.0 * 8.610271453857422
Epoch 290, val loss: 0.9673087000846863
Epoch 300, training loss: 430.9734191894531 = 0.9566041231155396 + 50.0 * 8.600336074829102
Epoch 300, val loss: 0.9600696563720703
Epoch 310, training loss: 430.4476013183594 = 0.9487994909286499 + 50.0 * 8.58997631072998
Epoch 310, val loss: 0.9525616765022278
Epoch 320, training loss: 429.9434509277344 = 0.9407158493995667 + 50.0 * 8.580055236816406
Epoch 320, val loss: 0.9447749257087708
Epoch 330, training loss: 429.39056396484375 = 0.9323492646217346 + 50.0 * 8.569164276123047
Epoch 330, val loss: 0.9367472529411316
Epoch 340, training loss: 428.9718322753906 = 0.9237253665924072 + 50.0 * 8.560961723327637
Epoch 340, val loss: 0.9284634590148926
Epoch 350, training loss: 428.48699951171875 = 0.9147798418998718 + 50.0 * 8.551444053649902
Epoch 350, val loss: 0.9198967814445496
Epoch 360, training loss: 428.1761169433594 = 0.9055611491203308 + 50.0 * 8.545411109924316
Epoch 360, val loss: 0.9110722541809082
Epoch 370, training loss: 427.79559326171875 = 0.8960555195808411 + 50.0 * 8.53799057006836
Epoch 370, val loss: 0.9019702076911926
Epoch 380, training loss: 427.4915466308594 = 0.8863203525543213 + 50.0 * 8.5321044921875
Epoch 380, val loss: 0.8926753997802734
Epoch 390, training loss: 427.28515625 = 0.8763492107391357 + 50.0 * 8.528176307678223
Epoch 390, val loss: 0.883166491985321
Epoch 400, training loss: 427.03131103515625 = 0.8661620020866394 + 50.0 * 8.523303031921387
Epoch 400, val loss: 0.8734888434410095
Epoch 410, training loss: 426.7925109863281 = 0.855856716632843 + 50.0 * 8.518733024597168
Epoch 410, val loss: 0.8637234568595886
Epoch 420, training loss: 426.6780700683594 = 0.8454504609107971 + 50.0 * 8.51665210723877
Epoch 420, val loss: 0.8538914918899536
Epoch 430, training loss: 426.40301513671875 = 0.8349494934082031 + 50.0 * 8.511361122131348
Epoch 430, val loss: 0.8439520001411438
Epoch 440, training loss: 426.1958923339844 = 0.8244166970252991 + 50.0 * 8.507430076599121
Epoch 440, val loss: 0.8340201377868652
Epoch 450, training loss: 426.0179443359375 = 0.8138975501060486 + 50.0 * 8.504080772399902
Epoch 450, val loss: 0.8241354823112488
Epoch 460, training loss: 425.9869689941406 = 0.8033483028411865 + 50.0 * 8.50367259979248
Epoch 460, val loss: 0.8142483234405518
Epoch 470, training loss: 425.7458190917969 = 0.7927776575088501 + 50.0 * 8.49906063079834
Epoch 470, val loss: 0.8043400645256042
Epoch 480, training loss: 425.5611877441406 = 0.7822577357292175 + 50.0 * 8.49557876586914
Epoch 480, val loss: 0.7945206761360168
Epoch 490, training loss: 425.4154052734375 = 0.7717892527580261 + 50.0 * 8.49287223815918
Epoch 490, val loss: 0.7847956418991089
Epoch 500, training loss: 425.37060546875 = 0.7613682746887207 + 50.0 * 8.49218463897705
Epoch 500, val loss: 0.775163471698761
Epoch 510, training loss: 425.1934814453125 = 0.7509928941726685 + 50.0 * 8.488849639892578
Epoch 510, val loss: 0.7655029892921448
Epoch 520, training loss: 425.0135192871094 = 0.7407388091087341 + 50.0 * 8.485455513000488
Epoch 520, val loss: 0.7560471892356873
Epoch 530, training loss: 424.8795471191406 = 0.7306188941001892 + 50.0 * 8.482978820800781
Epoch 530, val loss: 0.7467493414878845
Epoch 540, training loss: 424.89495849609375 = 0.7206067442893982 + 50.0 * 8.483487129211426
Epoch 540, val loss: 0.7375124096870422
Epoch 550, training loss: 424.6701354980469 = 0.7106308937072754 + 50.0 * 8.4791898727417
Epoch 550, val loss: 0.7283781170845032
Epoch 560, training loss: 424.51336669921875 = 0.7008760571479797 + 50.0 * 8.476249694824219
Epoch 560, val loss: 0.7194507122039795
Epoch 570, training loss: 424.3720397949219 = 0.6913096308708191 + 50.0 * 8.473614692687988
Epoch 570, val loss: 0.7107264399528503
Epoch 580, training loss: 424.33966064453125 = 0.6819281578063965 + 50.0 * 8.47315502166748
Epoch 580, val loss: 0.7021624445915222
Epoch 590, training loss: 424.24066162109375 = 0.6726882457733154 + 50.0 * 8.471359252929688
Epoch 590, val loss: 0.6937719583511353
Epoch 600, training loss: 424.02471923828125 = 0.6636087894439697 + 50.0 * 8.467222213745117
Epoch 600, val loss: 0.6855798363685608
Epoch 610, training loss: 423.89801025390625 = 0.6548137664794922 + 50.0 * 8.464863777160645
Epoch 610, val loss: 0.6776083111763
Epoch 620, training loss: 423.88897705078125 = 0.6462827920913696 + 50.0 * 8.46485424041748
Epoch 620, val loss: 0.6698957681655884
Epoch 630, training loss: 423.8274841308594 = 0.6378580331802368 + 50.0 * 8.46379280090332
Epoch 630, val loss: 0.6624451279640198
Epoch 640, training loss: 423.5716857910156 = 0.6297442317008972 + 50.0 * 8.458839416503906
Epoch 640, val loss: 0.6552202701568604
Epoch 650, training loss: 423.4327087402344 = 0.6219695210456848 + 50.0 * 8.456214904785156
Epoch 650, val loss: 0.648311197757721
Epoch 660, training loss: 423.3121032714844 = 0.6144770979881287 + 50.0 * 8.45395278930664
Epoch 660, val loss: 0.6416720151901245
Epoch 670, training loss: 423.42742919921875 = 0.6072250008583069 + 50.0 * 8.456403732299805
Epoch 670, val loss: 0.6352828741073608
Epoch 680, training loss: 423.1396789550781 = 0.6001481413841248 + 50.0 * 8.450790405273438
Epoch 680, val loss: 0.6291097402572632
Epoch 690, training loss: 422.9805908203125 = 0.5933767557144165 + 50.0 * 8.447744369506836
Epoch 690, val loss: 0.62324059009552
Epoch 700, training loss: 423.01312255859375 = 0.5868598818778992 + 50.0 * 8.448525428771973
Epoch 700, val loss: 0.6175804734230042
Epoch 710, training loss: 422.8487854003906 = 0.5805444717407227 + 50.0 * 8.445364952087402
Epoch 710, val loss: 0.6121863722801208
Epoch 720, training loss: 422.6852111816406 = 0.5744762420654297 + 50.0 * 8.442214965820312
Epoch 720, val loss: 0.6070103645324707
Epoch 730, training loss: 422.5976867675781 = 0.568672776222229 + 50.0 * 8.440580368041992
Epoch 730, val loss: 0.602110743522644
Epoch 740, training loss: 422.5323486328125 = 0.5631004571914673 + 50.0 * 8.439384460449219
Epoch 740, val loss: 0.5974056124687195
Epoch 750, training loss: 422.5168762207031 = 0.5577099323272705 + 50.0 * 8.439183235168457
Epoch 750, val loss: 0.5929005146026611
Epoch 760, training loss: 422.4256286621094 = 0.5525125861167908 + 50.0 * 8.437461853027344
Epoch 760, val loss: 0.5885578393936157
Epoch 770, training loss: 422.3089294433594 = 0.5475373864173889 + 50.0 * 8.435227394104004
Epoch 770, val loss: 0.5844855308532715
Epoch 780, training loss: 422.3105163574219 = 0.5428174138069153 + 50.0 * 8.435354232788086
Epoch 780, val loss: 0.5806658267974854
Epoch 790, training loss: 422.19964599609375 = 0.5382587909698486 + 50.0 * 8.4332275390625
Epoch 790, val loss: 0.5768944621086121
Epoch 800, training loss: 422.24139404296875 = 0.5338908433914185 + 50.0 * 8.434149742126465
Epoch 800, val loss: 0.5734027624130249
Epoch 810, training loss: 422.0461120605469 = 0.5296953320503235 + 50.0 * 8.430328369140625
Epoch 810, val loss: 0.5700643062591553
Epoch 820, training loss: 422.00946044921875 = 0.5257047414779663 + 50.0 * 8.429675102233887
Epoch 820, val loss: 0.5668798685073853
Epoch 830, training loss: 421.9741516113281 = 0.5218995213508606 + 50.0 * 8.429044723510742
Epoch 830, val loss: 0.563895046710968
Epoch 840, training loss: 421.91033935546875 = 0.5182194709777832 + 50.0 * 8.427842140197754
Epoch 840, val loss: 0.5610257387161255
Epoch 850, training loss: 421.8492736816406 = 0.5146830677986145 + 50.0 * 8.426692008972168
Epoch 850, val loss: 0.5583520531654358
Epoch 860, training loss: 422.1012268066406 = 0.5112938284873962 + 50.0 * 8.431798934936523
Epoch 860, val loss: 0.5557623505592346
Epoch 870, training loss: 421.7879638671875 = 0.5079952478408813 + 50.0 * 8.425599098205566
Epoch 870, val loss: 0.5532065629959106
Epoch 880, training loss: 421.69085693359375 = 0.5048651695251465 + 50.0 * 8.423720359802246
Epoch 880, val loss: 0.5509142279624939
Epoch 890, training loss: 421.6307373046875 = 0.5018852949142456 + 50.0 * 8.422576904296875
Epoch 890, val loss: 0.548678994178772
Epoch 900, training loss: 421.5737609863281 = 0.49900582432746887 + 50.0 * 8.42149543762207
Epoch 900, val loss: 0.5466023087501526
Epoch 910, training loss: 421.8645324707031 = 0.49620166420936584 + 50.0 * 8.427366256713867
Epoch 910, val loss: 0.5446329116821289
Epoch 920, training loss: 421.51458740234375 = 0.49345457553863525 + 50.0 * 8.420422554016113
Epoch 920, val loss: 0.5425669550895691
Epoch 930, training loss: 421.4458923339844 = 0.4908393323421478 + 50.0 * 8.419100761413574
Epoch 930, val loss: 0.5407139658927917
Epoch 940, training loss: 421.39697265625 = 0.48833170533180237 + 50.0 * 8.418172836303711
Epoch 940, val loss: 0.5389660596847534
Epoch 950, training loss: 421.340576171875 = 0.4859062433242798 + 50.0 * 8.417093276977539
Epoch 950, val loss: 0.537295401096344
Epoch 960, training loss: 421.63531494140625 = 0.4835200905799866 + 50.0 * 8.423035621643066
Epoch 960, val loss: 0.5357413291931152
Epoch 970, training loss: 421.34783935546875 = 0.48115989565849304 + 50.0 * 8.417333602905273
Epoch 970, val loss: 0.5340374112129211
Epoch 980, training loss: 421.22918701171875 = 0.4789106547832489 + 50.0 * 8.415005683898926
Epoch 980, val loss: 0.532435953617096
Epoch 990, training loss: 421.1725769042969 = 0.4767482280731201 + 50.0 * 8.41391658782959
Epoch 990, val loss: 0.5310409069061279
Epoch 1000, training loss: 421.1512451171875 = 0.4746594727039337 + 50.0 * 8.413531303405762
Epoch 1000, val loss: 0.529672384262085
Epoch 1010, training loss: 421.1319580078125 = 0.4725841283798218 + 50.0 * 8.413187026977539
Epoch 1010, val loss: 0.5282832980155945
Epoch 1020, training loss: 421.04364013671875 = 0.4705502390861511 + 50.0 * 8.41146183013916
Epoch 1020, val loss: 0.5270180106163025
Epoch 1030, training loss: 420.998291015625 = 0.46860018372535706 + 50.0 * 8.41059398651123
Epoch 1030, val loss: 0.5257418155670166
Epoch 1040, training loss: 420.9759826660156 = 0.46669110655784607 + 50.0 * 8.410185813903809
Epoch 1040, val loss: 0.5245442390441895
Epoch 1050, training loss: 421.10394287109375 = 0.4648136496543884 + 50.0 * 8.412782669067383
Epoch 1050, val loss: 0.5233919620513916
Epoch 1060, training loss: 420.9052429199219 = 0.46294108033180237 + 50.0 * 8.408845901489258
Epoch 1060, val loss: 0.5221565961837769
Epoch 1070, training loss: 420.8340148925781 = 0.46115824580192566 + 50.0 * 8.40745735168457
Epoch 1070, val loss: 0.5210703015327454
Epoch 1080, training loss: 421.2789306640625 = 0.45940276980400085 + 50.0 * 8.416390419006348
Epoch 1080, val loss: 0.519926130771637
Epoch 1090, training loss: 420.823974609375 = 0.45757827162742615 + 50.0 * 8.407327651977539
Epoch 1090, val loss: 0.5188512206077576
Epoch 1100, training loss: 420.76031494140625 = 0.455870121717453 + 50.0 * 8.406088829040527
Epoch 1100, val loss: 0.5178181529045105
Epoch 1110, training loss: 420.7094421386719 = 0.45423614978790283 + 50.0 * 8.405104637145996
Epoch 1110, val loss: 0.5168537497520447
Epoch 1120, training loss: 420.6639404296875 = 0.45263367891311646 + 50.0 * 8.404226303100586
Epoch 1120, val loss: 0.5159245133399963
Epoch 1130, training loss: 420.62164306640625 = 0.4510497450828552 + 50.0 * 8.403411865234375
Epoch 1130, val loss: 0.5149681568145752
Epoch 1140, training loss: 420.588134765625 = 0.44948810338974 + 50.0 * 8.402772903442383
Epoch 1140, val loss: 0.514069139957428
Epoch 1150, training loss: 420.59454345703125 = 0.44794511795043945 + 50.0 * 8.402932167053223
Epoch 1150, val loss: 0.5131950974464417
Epoch 1160, training loss: 420.6038513183594 = 0.4463869333267212 + 50.0 * 8.403149604797363
Epoch 1160, val loss: 0.512230634689331
Epoch 1170, training loss: 420.5953063964844 = 0.44482654333114624 + 50.0 * 8.403009414672852
Epoch 1170, val loss: 0.5113071799278259
Epoch 1180, training loss: 420.4927062988281 = 0.44336479902267456 + 50.0 * 8.400986671447754
Epoch 1180, val loss: 0.5104483962059021
Epoch 1190, training loss: 420.4388732910156 = 0.4419449269771576 + 50.0 * 8.399938583374023
Epoch 1190, val loss: 0.5096448063850403
Epoch 1200, training loss: 420.4098815917969 = 0.4405406713485718 + 50.0 * 8.399386405944824
Epoch 1200, val loss: 0.5088011026382446
Epoch 1210, training loss: 420.3938293457031 = 0.439146488904953 + 50.0 * 8.399093627929688
Epoch 1210, val loss: 0.5080165266990662
Epoch 1220, training loss: 420.56146240234375 = 0.4377538561820984 + 50.0 * 8.402474403381348
Epoch 1220, val loss: 0.5072065591812134
Epoch 1230, training loss: 420.3982849121094 = 0.4363204836845398 + 50.0 * 8.399239540100098
Epoch 1230, val loss: 0.506425142288208
Epoch 1240, training loss: 420.32037353515625 = 0.43494415283203125 + 50.0 * 8.397708892822266
Epoch 1240, val loss: 0.5056458115577698
Epoch 1250, training loss: 420.29364013671875 = 0.4336301386356354 + 50.0 * 8.397200584411621
Epoch 1250, val loss: 0.5048601627349854
Epoch 1260, training loss: 420.2442626953125 = 0.43233829736709595 + 50.0 * 8.396238327026367
Epoch 1260, val loss: 0.5041704773902893
Epoch 1270, training loss: 420.21990966796875 = 0.4310619831085205 + 50.0 * 8.395776748657227
Epoch 1270, val loss: 0.5034406781196594
Epoch 1280, training loss: 420.192138671875 = 0.4297977089881897 + 50.0 * 8.395246505737305
Epoch 1280, val loss: 0.5027393102645874
Epoch 1290, training loss: 420.1872863769531 = 0.42854246497154236 + 50.0 * 8.395174980163574
Epoch 1290, val loss: 0.5020491480827332
Epoch 1300, training loss: 420.4896240234375 = 0.4272737503051758 + 50.0 * 8.401247024536133
Epoch 1300, val loss: 0.5013217329978943
Epoch 1310, training loss: 420.1473083496094 = 0.42599037289619446 + 50.0 * 8.394426345825195
Epoch 1310, val loss: 0.5005828738212585
Epoch 1320, training loss: 420.114013671875 = 0.424771249294281 + 50.0 * 8.393784523010254
Epoch 1320, val loss: 0.49988770484924316
Epoch 1330, training loss: 420.0878601074219 = 0.42358583211898804 + 50.0 * 8.393285751342773
Epoch 1330, val loss: 0.49925005435943604
Epoch 1340, training loss: 420.05792236328125 = 0.4224216043949127 + 50.0 * 8.392709732055664
Epoch 1340, val loss: 0.4985907971858978
Epoch 1350, training loss: 420.16192626953125 = 0.421261727809906 + 50.0 * 8.394813537597656
Epoch 1350, val loss: 0.497950941324234
Epoch 1360, training loss: 420.0205993652344 = 0.4200702905654907 + 50.0 * 8.392010688781738
Epoch 1360, val loss: 0.4973427355289459
Epoch 1370, training loss: 420.03729248046875 = 0.4189133942127228 + 50.0 * 8.392367362976074
Epoch 1370, val loss: 0.4967098832130432
Epoch 1380, training loss: 420.0268859863281 = 0.4177664518356323 + 50.0 * 8.392182350158691
Epoch 1380, val loss: 0.4960913360118866
Epoch 1390, training loss: 419.9476623535156 = 0.41664984822273254 + 50.0 * 8.390620231628418
Epoch 1390, val loss: 0.4953813850879669
Epoch 1400, training loss: 419.94708251953125 = 0.4155457019805908 + 50.0 * 8.390630722045898
Epoch 1400, val loss: 0.494748055934906
Epoch 1410, training loss: 419.96588134765625 = 0.4144410490989685 + 50.0 * 8.391029357910156
Epoch 1410, val loss: 0.4940955936908722
Epoch 1420, training loss: 419.931884765625 = 0.4133254289627075 + 50.0 * 8.390371322631836
Epoch 1420, val loss: 0.49353301525115967
Epoch 1430, training loss: 419.91082763671875 = 0.4122275412082672 + 50.0 * 8.389971733093262
Epoch 1430, val loss: 0.49296241998672485
Epoch 1440, training loss: 419.8591613769531 = 0.41116198897361755 + 50.0 * 8.388959884643555
Epoch 1440, val loss: 0.4922935664653778
Epoch 1450, training loss: 419.8543701171875 = 0.41009652614593506 + 50.0 * 8.388885498046875
Epoch 1450, val loss: 0.4917362928390503
Epoch 1460, training loss: 419.91790771484375 = 0.4090140759944916 + 50.0 * 8.390177726745605
Epoch 1460, val loss: 0.4911401569843292
Epoch 1470, training loss: 419.7947082519531 = 0.40793243050575256 + 50.0 * 8.387735366821289
Epoch 1470, val loss: 0.49052324891090393
Epoch 1480, training loss: 419.7791748046875 = 0.4069010019302368 + 50.0 * 8.387445449829102
Epoch 1480, val loss: 0.48992666602134705
Epoch 1490, training loss: 419.72900390625 = 0.4058762788772583 + 50.0 * 8.386462211608887
Epoch 1490, val loss: 0.4893503487110138
Epoch 1500, training loss: 419.71453857421875 = 0.4048630893230438 + 50.0 * 8.38619327545166
Epoch 1500, val loss: 0.48878347873687744
Epoch 1510, training loss: 420.0861511230469 = 0.40384209156036377 + 50.0 * 8.393646240234375
Epoch 1510, val loss: 0.4880530536174774
Epoch 1520, training loss: 419.767333984375 = 0.40276169776916504 + 50.0 * 8.387290954589844
Epoch 1520, val loss: 0.4876703917980194
Epoch 1530, training loss: 419.6871643066406 = 0.401751846075058 + 50.0 * 8.38570785522461
Epoch 1530, val loss: 0.48715221881866455
Epoch 1540, training loss: 419.6374206542969 = 0.40075692534446716 + 50.0 * 8.384733200073242
Epoch 1540, val loss: 0.48652392625808716
Epoch 1550, training loss: 419.6397705078125 = 0.3997780382633209 + 50.0 * 8.38479995727539
Epoch 1550, val loss: 0.48596712946891785
Epoch 1560, training loss: 419.80126953125 = 0.3987906873226166 + 50.0 * 8.388049125671387
Epoch 1560, val loss: 0.4854590594768524
Epoch 1570, training loss: 419.62103271484375 = 0.39777082204818726 + 50.0 * 8.384465217590332
Epoch 1570, val loss: 0.484890341758728
Epoch 1580, training loss: 419.5562438964844 = 0.3967915177345276 + 50.0 * 8.38318920135498
Epoch 1580, val loss: 0.4843613803386688
Epoch 1590, training loss: 419.5340881347656 = 0.3958231210708618 + 50.0 * 8.382765769958496
Epoch 1590, val loss: 0.4838421642780304
Epoch 1600, training loss: 419.61474609375 = 0.39486008882522583 + 50.0 * 8.384397506713867
Epoch 1600, val loss: 0.4833543002605438
Epoch 1610, training loss: 419.5013427734375 = 0.3938736617565155 + 50.0 * 8.382149696350098
Epoch 1610, val loss: 0.482773095369339
Epoch 1620, training loss: 419.51171875 = 0.3928958773612976 + 50.0 * 8.382376670837402
Epoch 1620, val loss: 0.48228052258491516
Epoch 1630, training loss: 419.47662353515625 = 0.39195144176483154 + 50.0 * 8.381693840026855
Epoch 1630, val loss: 0.4817873239517212
Epoch 1640, training loss: 419.4440612792969 = 0.3910098969936371 + 50.0 * 8.381060600280762
Epoch 1640, val loss: 0.4812842607498169
Epoch 1650, training loss: 419.4854431152344 = 0.3900657594203949 + 50.0 * 8.38190746307373
Epoch 1650, val loss: 0.4808362126350403
Epoch 1660, training loss: 419.5328369140625 = 0.38908761739730835 + 50.0 * 8.382874488830566
Epoch 1660, val loss: 0.48019957542419434
Epoch 1670, training loss: 419.4511413574219 = 0.38809457421302795 + 50.0 * 8.381260871887207
Epoch 1670, val loss: 0.47977203130722046
Epoch 1680, training loss: 419.3952941894531 = 0.38715967535972595 + 50.0 * 8.380163192749023
Epoch 1680, val loss: 0.47924739122390747
Epoch 1690, training loss: 419.35162353515625 = 0.3862331509590149 + 50.0 * 8.379307746887207
Epoch 1690, val loss: 0.4787893295288086
Epoch 1700, training loss: 419.3384094238281 = 0.3853147625923157 + 50.0 * 8.379061698913574
Epoch 1700, val loss: 0.47833535075187683
Epoch 1710, training loss: 419.3519287109375 = 0.3843948543071747 + 50.0 * 8.379350662231445
Epoch 1710, val loss: 0.4778340756893158
Epoch 1720, training loss: 419.55645751953125 = 0.38344866037368774 + 50.0 * 8.38346004486084
Epoch 1720, val loss: 0.4772856831550598
Epoch 1730, training loss: 419.32269287109375 = 0.3824780583381653 + 50.0 * 8.378804206848145
Epoch 1730, val loss: 0.47690677642822266
Epoch 1740, training loss: 419.2667541503906 = 0.38154667615890503 + 50.0 * 8.377703666687012
Epoch 1740, val loss: 0.47646069526672363
Epoch 1750, training loss: 419.25390625 = 0.38063451647758484 + 50.0 * 8.37746524810791
Epoch 1750, val loss: 0.47597065567970276
Epoch 1760, training loss: 419.23577880859375 = 0.3797244727611542 + 50.0 * 8.377120971679688
Epoch 1760, val loss: 0.475573867559433
Epoch 1770, training loss: 419.2341613769531 = 0.37881481647491455 + 50.0 * 8.377106666564941
Epoch 1770, val loss: 0.4751107692718506
Epoch 1780, training loss: 419.52886962890625 = 0.37789058685302734 + 50.0 * 8.38301944732666
Epoch 1780, val loss: 0.4745921194553375
Epoch 1790, training loss: 419.3830261230469 = 0.3769165277481079 + 50.0 * 8.380122184753418
Epoch 1790, val loss: 0.4742558002471924
Epoch 1800, training loss: 419.19525146484375 = 0.3759807348251343 + 50.0 * 8.376385688781738
Epoch 1800, val loss: 0.4737675189971924
Epoch 1810, training loss: 419.1831359863281 = 0.37507230043411255 + 50.0 * 8.376161575317383
Epoch 1810, val loss: 0.47328028082847595
Epoch 1820, training loss: 419.13885498046875 = 0.3741784393787384 + 50.0 * 8.375293731689453
Epoch 1820, val loss: 0.4728889763355255
Epoch 1830, training loss: 419.1268615722656 = 0.3732863962650299 + 50.0 * 8.37507152557373
Epoch 1830, val loss: 0.4724947512149811
Epoch 1840, training loss: 419.1220397949219 = 0.372392863035202 + 50.0 * 8.374992370605469
Epoch 1840, val loss: 0.47206559777259827
Epoch 1850, training loss: 419.283935546875 = 0.3714921772480011 + 50.0 * 8.378249168395996
Epoch 1850, val loss: 0.47163817286491394
Epoch 1860, training loss: 419.2305603027344 = 0.3705570697784424 + 50.0 * 8.37720012664795
Epoch 1860, val loss: 0.47120723128318787
Epoch 1870, training loss: 419.1441345214844 = 0.3696235418319702 + 50.0 * 8.375490188598633
Epoch 1870, val loss: 0.4707662761211395
Epoch 1880, training loss: 419.08172607421875 = 0.36871346831321716 + 50.0 * 8.374259948730469
Epoch 1880, val loss: 0.4703391492366791
Epoch 1890, training loss: 419.0478515625 = 0.36781156063079834 + 50.0 * 8.373600959777832
Epoch 1890, val loss: 0.4699254333972931
Epoch 1900, training loss: 419.0372009277344 = 0.36691173911094666 + 50.0 * 8.373405456542969
Epoch 1900, val loss: 0.46955135464668274
Epoch 1910, training loss: 419.20477294921875 = 0.3660100996494293 + 50.0 * 8.376775741577148
Epoch 1910, val loss: 0.46920374035835266
Epoch 1920, training loss: 419.05731201171875 = 0.3650646507740021 + 50.0 * 8.373845100402832
Epoch 1920, val loss: 0.4686501920223236
Epoch 1930, training loss: 419.0457763671875 = 0.3641512393951416 + 50.0 * 8.373632431030273
Epoch 1930, val loss: 0.46834784746170044
Epoch 1940, training loss: 418.9880676269531 = 0.3632397949695587 + 50.0 * 8.372496604919434
Epoch 1940, val loss: 0.4678560495376587
Epoch 1950, training loss: 418.9635009765625 = 0.36234450340270996 + 50.0 * 8.372023582458496
Epoch 1950, val loss: 0.4675089716911316
Epoch 1960, training loss: 419.25250244140625 = 0.36144617199897766 + 50.0 * 8.37782096862793
Epoch 1960, val loss: 0.4670645296573639
Epoch 1970, training loss: 419.07183837890625 = 0.3604808449745178 + 50.0 * 8.374227523803711
Epoch 1970, val loss: 0.4666920304298401
Epoch 1980, training loss: 418.9580078125 = 0.3595614731311798 + 50.0 * 8.371969223022461
Epoch 1980, val loss: 0.4663011431694031
Epoch 1990, training loss: 418.90838623046875 = 0.358652800321579 + 50.0 * 8.370994567871094
Epoch 1990, val loss: 0.46588754653930664
Epoch 2000, training loss: 418.8971862792969 = 0.357760488986969 + 50.0 * 8.37078857421875
Epoch 2000, val loss: 0.4655849039554596
Epoch 2010, training loss: 418.8798522949219 = 0.356858491897583 + 50.0 * 8.37045955657959
Epoch 2010, val loss: 0.4652031660079956
Epoch 2020, training loss: 418.9527587890625 = 0.35595235228538513 + 50.0 * 8.371935844421387
Epoch 2020, val loss: 0.464765727519989
Epoch 2030, training loss: 418.8751220703125 = 0.3549957573413849 + 50.0 * 8.370402336120605
Epoch 2030, val loss: 0.4644744098186493
Epoch 2040, training loss: 418.8875732421875 = 0.35406070947647095 + 50.0 * 8.370670318603516
Epoch 2040, val loss: 0.4640747010707855
Epoch 2050, training loss: 418.8495788574219 = 0.3531305193901062 + 50.0 * 8.369929313659668
Epoch 2050, val loss: 0.46371161937713623
Epoch 2060, training loss: 418.83056640625 = 0.35222554206848145 + 50.0 * 8.369566917419434
Epoch 2060, val loss: 0.4634224772453308
Epoch 2070, training loss: 419.0521545410156 = 0.3513089120388031 + 50.0 * 8.374016761779785
Epoch 2070, val loss: 0.4630430042743683
Epoch 2080, training loss: 418.86126708984375 = 0.35035333037376404 + 50.0 * 8.370218276977539
Epoch 2080, val loss: 0.46266207098960876
Epoch 2090, training loss: 418.805419921875 = 0.34942424297332764 + 50.0 * 8.369119644165039
Epoch 2090, val loss: 0.4623112082481384
Epoch 2100, training loss: 418.7757263183594 = 0.34850192070007324 + 50.0 * 8.368544578552246
Epoch 2100, val loss: 0.46202313899993896
Epoch 2110, training loss: 418.7587585449219 = 0.34757548570632935 + 50.0 * 8.368224143981934
Epoch 2110, val loss: 0.46163976192474365
Epoch 2120, training loss: 418.7853698730469 = 0.34664782881736755 + 50.0 * 8.3687744140625
Epoch 2120, val loss: 0.46131959557533264
Epoch 2130, training loss: 418.99859619140625 = 0.34567761421203613 + 50.0 * 8.373058319091797
Epoch 2130, val loss: 0.4609465003013611
Epoch 2140, training loss: 418.73779296875 = 0.3447015583515167 + 50.0 * 8.3678617477417
Epoch 2140, val loss: 0.4606066942214966
Epoch 2150, training loss: 418.756103515625 = 0.34375444054603577 + 50.0 * 8.368247032165527
Epoch 2150, val loss: 0.46021372079849243
Epoch 2160, training loss: 418.70263671875 = 0.3428247272968292 + 50.0 * 8.367196083068848
Epoch 2160, val loss: 0.4599883258342743
Epoch 2170, training loss: 418.68768310546875 = 0.3418981730937958 + 50.0 * 8.366915702819824
Epoch 2170, val loss: 0.4596317708492279
Epoch 2180, training loss: 418.6799011230469 = 0.3409633934497833 + 50.0 * 8.366778373718262
Epoch 2180, val loss: 0.45926541090011597
Epoch 2190, training loss: 418.9253845214844 = 0.340021550655365 + 50.0 * 8.37170696258545
Epoch 2190, val loss: 0.4588364362716675
Epoch 2200, training loss: 418.7680358886719 = 0.3390643000602722 + 50.0 * 8.368579864501953
Epoch 2200, val loss: 0.4587085247039795
Epoch 2210, training loss: 418.6636657714844 = 0.3380979597568512 + 50.0 * 8.366511344909668
Epoch 2210, val loss: 0.4583562910556793
Epoch 2220, training loss: 418.6458435058594 = 0.33716678619384766 + 50.0 * 8.36617374420166
Epoch 2220, val loss: 0.45808175206184387
Epoch 2230, training loss: 418.6300048828125 = 0.3362365663051605 + 50.0 * 8.365875244140625
Epoch 2230, val loss: 0.4577897787094116
Epoch 2240, training loss: 418.6917724609375 = 0.3352971076965332 + 50.0 * 8.3671293258667
Epoch 2240, val loss: 0.4575566351413727
Epoch 2250, training loss: 418.6510314941406 = 0.3343260586261749 + 50.0 * 8.366333961486816
Epoch 2250, val loss: 0.4572518467903137
Epoch 2260, training loss: 418.6045837402344 = 0.33336344361305237 + 50.0 * 8.365424156188965
Epoch 2260, val loss: 0.45693591237068176
Epoch 2270, training loss: 418.59124755859375 = 0.3324124813079834 + 50.0 * 8.365177154541016
Epoch 2270, val loss: 0.45664098858833313
Epoch 2280, training loss: 418.6531066894531 = 0.3314659595489502 + 50.0 * 8.366433143615723
Epoch 2280, val loss: 0.4564462900161743
Epoch 2290, training loss: 418.58782958984375 = 0.3305053114891052 + 50.0 * 8.36514663696289
Epoch 2290, val loss: 0.4561518728733063
Epoch 2300, training loss: 418.5714416503906 = 0.32955124974250793 + 50.0 * 8.364837646484375
Epoch 2300, val loss: 0.4558272957801819
Epoch 2310, training loss: 418.5375061035156 = 0.32859793305397034 + 50.0 * 8.364178657531738
Epoch 2310, val loss: 0.4556295871734619
Epoch 2320, training loss: 418.5844421386719 = 0.3276423215866089 + 50.0 * 8.36513614654541
Epoch 2320, val loss: 0.45534026622772217
Epoch 2330, training loss: 418.6420593261719 = 0.3266593813896179 + 50.0 * 8.366308212280273
Epoch 2330, val loss: 0.45503830909729004
Epoch 2340, training loss: 418.5212707519531 = 0.3256755471229553 + 50.0 * 8.363911628723145
Epoch 2340, val loss: 0.4548477530479431
Epoch 2350, training loss: 418.52166748046875 = 0.3247087597846985 + 50.0 * 8.36393928527832
Epoch 2350, val loss: 0.45467036962509155
Epoch 2360, training loss: 418.4933166503906 = 0.32374826073646545 + 50.0 * 8.363390922546387
Epoch 2360, val loss: 0.45438382029533386
Epoch 2370, training loss: 418.49066162109375 = 0.32278281450271606 + 50.0 * 8.363357543945312
Epoch 2370, val loss: 0.45414385199546814
Epoch 2380, training loss: 418.67340087890625 = 0.3218158185482025 + 50.0 * 8.367032051086426
Epoch 2380, val loss: 0.45394182205200195
Epoch 2390, training loss: 418.484375 = 0.32081517577171326 + 50.0 * 8.363271713256836
Epoch 2390, val loss: 0.45378682017326355
Epoch 2400, training loss: 418.4354248046875 = 0.3198252022266388 + 50.0 * 8.362312316894531
Epoch 2400, val loss: 0.45344817638397217
Epoch 2410, training loss: 418.4345703125 = 0.3188474178314209 + 50.0 * 8.362314224243164
Epoch 2410, val loss: 0.45318183302879333
Epoch 2420, training loss: 418.4447937011719 = 0.31787022948265076 + 50.0 * 8.36253833770752
Epoch 2420, val loss: 0.4529878497123718
Epoch 2430, training loss: 418.60479736328125 = 0.31688156723976135 + 50.0 * 8.365757942199707
Epoch 2430, val loss: 0.45268580317497253
Epoch 2440, training loss: 418.4082336425781 = 0.3158814013004303 + 50.0 * 8.361846923828125
Epoch 2440, val loss: 0.4526425898075104
Epoch 2450, training loss: 418.4655456542969 = 0.3148920238018036 + 50.0 * 8.36301326751709
Epoch 2450, val loss: 0.45246246457099915
Epoch 2460, training loss: 418.42254638671875 = 0.31388866901397705 + 50.0 * 8.362173080444336
Epoch 2460, val loss: 0.4522247016429901
Epoch 2470, training loss: 418.3764953613281 = 0.3129023313522339 + 50.0 * 8.361271858215332
Epoch 2470, val loss: 0.45203277468681335
Epoch 2480, training loss: 418.35528564453125 = 0.3119226098060608 + 50.0 * 8.360867500305176
Epoch 2480, val loss: 0.45182889699935913
Epoch 2490, training loss: 418.33843994140625 = 0.31095001101493835 + 50.0 * 8.360549926757812
Epoch 2490, val loss: 0.45172807574272156
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8234398782343988
0.8643773092805912
=== training gcn model ===
Epoch 0, training loss: 530.207275390625 = 1.0927728414535522 + 50.0 * 10.582290649414062
Epoch 0, val loss: 1.0912271738052368
Epoch 10, training loss: 530.1865844726562 = 1.0889955759048462 + 50.0 * 10.581952095031738
Epoch 10, val loss: 1.0875074863433838
Epoch 20, training loss: 530.1060180664062 = 1.084913730621338 + 50.0 * 10.580422401428223
Epoch 20, val loss: 1.0834755897521973
Epoch 30, training loss: 529.74853515625 = 1.0804229974746704 + 50.0 * 10.57336139678955
Epoch 30, val loss: 1.0790530443191528
Epoch 40, training loss: 528.308349609375 = 1.0755457878112793 + 50.0 * 10.544655799865723
Epoch 40, val loss: 1.0742465257644653
Epoch 50, training loss: 524.0086669921875 = 1.0700188875198364 + 50.0 * 10.458773612976074
Epoch 50, val loss: 1.06879460811615
Epoch 60, training loss: 514.865234375 = 1.0638214349746704 + 50.0 * 10.27602767944336
Epoch 60, val loss: 1.0627306699752808
Epoch 70, training loss: 503.29302978515625 = 1.056943416595459 + 50.0 * 10.044721603393555
Epoch 70, val loss: 1.0559335947036743
Epoch 80, training loss: 483.5331726074219 = 1.0493298768997192 + 50.0 * 9.649677276611328
Epoch 80, val loss: 1.0485867261886597
Epoch 90, training loss: 472.43756103515625 = 1.042637825012207 + 50.0 * 9.427898406982422
Epoch 90, val loss: 1.0422192811965942
Epoch 100, training loss: 466.6452941894531 = 1.0366041660308838 + 50.0 * 9.312173843383789
Epoch 100, val loss: 1.0365822315216064
Epoch 110, training loss: 463.8359375 = 1.0319483280181885 + 50.0 * 9.25607967376709
Epoch 110, val loss: 1.0322381258010864
Epoch 120, training loss: 462.15655517578125 = 1.0278407335281372 + 50.0 * 9.222574234008789
Epoch 120, val loss: 1.028279185295105
Epoch 130, training loss: 460.4045104980469 = 1.0233038663864136 + 50.0 * 9.187623977661133
Epoch 130, val loss: 1.023850917816162
Epoch 140, training loss: 458.18603515625 = 1.0181317329406738 + 50.0 * 9.14335823059082
Epoch 140, val loss: 1.018823266029358
Epoch 150, training loss: 455.22625732421875 = 1.0129709243774414 + 50.0 * 9.08426570892334
Epoch 150, val loss: 1.0139315128326416
Epoch 160, training loss: 451.9573974609375 = 1.0085833072662354 + 50.0 * 9.018976211547852
Epoch 160, val loss: 1.0098094940185547
Epoch 170, training loss: 449.65155029296875 = 1.0046744346618652 + 50.0 * 8.97293758392334
Epoch 170, val loss: 1.0059888362884521
Epoch 180, training loss: 447.60833740234375 = 0.999790370464325 + 50.0 * 8.932170867919922
Epoch 180, val loss: 1.0011134147644043
Epoch 190, training loss: 445.5147705078125 = 0.9939722418785095 + 50.0 * 8.890416145324707
Epoch 190, val loss: 0.9954755902290344
Epoch 200, training loss: 444.3500671386719 = 0.9881415367126465 + 50.0 * 8.867238998413086
Epoch 200, val loss: 0.989899754524231
Epoch 210, training loss: 443.52667236328125 = 0.9813520312309265 + 50.0 * 8.850906372070312
Epoch 210, val loss: 0.9833243489265442
Epoch 220, training loss: 442.643798828125 = 0.9736709594726562 + 50.0 * 8.833402633666992
Epoch 220, val loss: 0.9759799242019653
Epoch 230, training loss: 441.53106689453125 = 0.9665188193321228 + 50.0 * 8.811290740966797
Epoch 230, val loss: 0.9692235589027405
Epoch 240, training loss: 440.1359558105469 = 0.9596028327941895 + 50.0 * 8.783527374267578
Epoch 240, val loss: 0.9626402854919434
Epoch 250, training loss: 438.7191162109375 = 0.9522828459739685 + 50.0 * 8.75533676147461
Epoch 250, val loss: 0.9556467533111572
Epoch 260, training loss: 437.4765319824219 = 0.944314181804657 + 50.0 * 8.730644226074219
Epoch 260, val loss: 0.9480342864990234
Epoch 270, training loss: 436.3306884765625 = 0.9355695247650146 + 50.0 * 8.707901954650879
Epoch 270, val loss: 0.93972247838974
Epoch 280, training loss: 435.274169921875 = 0.9265655875205994 + 50.0 * 8.686951637268066
Epoch 280, val loss: 0.9312548637390137
Epoch 290, training loss: 434.3774108886719 = 0.9172570705413818 + 50.0 * 8.66920280456543
Epoch 290, val loss: 0.9223947525024414
Epoch 300, training loss: 433.7606506347656 = 0.9074559211730957 + 50.0 * 8.657063484191895
Epoch 300, val loss: 0.9130791425704956
Epoch 310, training loss: 433.0165710449219 = 0.8972607254981995 + 50.0 * 8.642386436462402
Epoch 310, val loss: 0.9033477902412415
Epoch 320, training loss: 432.4035949707031 = 0.8868100047111511 + 50.0 * 8.630335807800293
Epoch 320, val loss: 0.89347904920578
Epoch 330, training loss: 432.1955871582031 = 0.8760547637939453 + 50.0 * 8.62639045715332
Epoch 330, val loss: 0.8832090497016907
Epoch 340, training loss: 431.4345397949219 = 0.8648271560668945 + 50.0 * 8.611393928527832
Epoch 340, val loss: 0.8726642727851868
Epoch 350, training loss: 430.9052734375 = 0.8536072373390198 + 50.0 * 8.601033210754395
Epoch 350, val loss: 0.8620127439498901
Epoch 360, training loss: 430.4327087402344 = 0.8421899676322937 + 50.0 * 8.59181022644043
Epoch 360, val loss: 0.8511893153190613
Epoch 370, training loss: 429.9893798828125 = 0.8305475115776062 + 50.0 * 8.583176612854004
Epoch 370, val loss: 0.8401417136192322
Epoch 380, training loss: 429.5090637207031 = 0.8187744617462158 + 50.0 * 8.573805809020996
Epoch 380, val loss: 0.8289670944213867
Epoch 390, training loss: 429.12152099609375 = 0.8068786263465881 + 50.0 * 8.566292762756348
Epoch 390, val loss: 0.817659854888916
Epoch 400, training loss: 428.8115234375 = 0.7948127388954163 + 50.0 * 8.560334205627441
Epoch 400, val loss: 0.8061614632606506
Epoch 410, training loss: 428.3925476074219 = 0.7827786207199097 + 50.0 * 8.55219554901123
Epoch 410, val loss: 0.7946677803993225
Epoch 420, training loss: 428.07000732421875 = 0.7706631422042847 + 50.0 * 8.545987129211426
Epoch 420, val loss: 0.7831398844718933
Epoch 430, training loss: 427.8652038574219 = 0.7584589123725891 + 50.0 * 8.542135238647461
Epoch 430, val loss: 0.7715449333190918
Epoch 440, training loss: 427.6003723144531 = 0.7460691928863525 + 50.0 * 8.537086486816406
Epoch 440, val loss: 0.7596466541290283
Epoch 450, training loss: 427.2940979003906 = 0.7338835000991821 + 50.0 * 8.531204223632812
Epoch 450, val loss: 0.7480685710906982
Epoch 460, training loss: 427.0441589355469 = 0.7217360734939575 + 50.0 * 8.526448249816895
Epoch 460, val loss: 0.736453115940094
Epoch 470, training loss: 426.8268737792969 = 0.7096683382987976 + 50.0 * 8.522344589233398
Epoch 470, val loss: 0.7249301671981812
Epoch 480, training loss: 426.77276611328125 = 0.6976452469825745 + 50.0 * 8.521502494812012
Epoch 480, val loss: 0.7134483456611633
Epoch 490, training loss: 426.4853515625 = 0.6857413649559021 + 50.0 * 8.515992164611816
Epoch 490, val loss: 0.7021735906600952
Epoch 500, training loss: 426.26531982421875 = 0.6741044521331787 + 50.0 * 8.511824607849121
Epoch 500, val loss: 0.6911269426345825
Epoch 510, training loss: 426.0787658691406 = 0.662684977054596 + 50.0 * 8.508321762084961
Epoch 510, val loss: 0.6803070306777954
Epoch 520, training loss: 425.90667724609375 = 0.6515269875526428 + 50.0 * 8.50510311126709
Epoch 520, val loss: 0.6697399616241455
Epoch 530, training loss: 425.7606506347656 = 0.6406732201576233 + 50.0 * 8.502399444580078
Epoch 530, val loss: 0.6595004796981812
Epoch 540, training loss: 425.9361572265625 = 0.6300630569458008 + 50.0 * 8.506121635437012
Epoch 540, val loss: 0.6494103670120239
Epoch 550, training loss: 425.5535583496094 = 0.6197484135627747 + 50.0 * 8.498676300048828
Epoch 550, val loss: 0.639824628829956
Epoch 560, training loss: 425.3392639160156 = 0.6099392771720886 + 50.0 * 8.494586944580078
Epoch 560, val loss: 0.630648672580719
Epoch 570, training loss: 425.19464111328125 = 0.6004361510276794 + 50.0 * 8.491884231567383
Epoch 570, val loss: 0.6217573285102844
Epoch 580, training loss: 425.0628356933594 = 0.5913193821907043 + 50.0 * 8.48943042755127
Epoch 580, val loss: 0.6133248805999756
Epoch 590, training loss: 424.9360656738281 = 0.5825424194335938 + 50.0 * 8.487070083618164
Epoch 590, val loss: 0.605177640914917
Epoch 600, training loss: 424.89520263671875 = 0.5741162896156311 + 50.0 * 8.486421585083008
Epoch 600, val loss: 0.5974730849266052
Epoch 610, training loss: 424.8487243652344 = 0.5659425854682922 + 50.0 * 8.485655784606934
Epoch 610, val loss: 0.5898194909095764
Epoch 620, training loss: 424.63116455078125 = 0.558284342288971 + 50.0 * 8.481457710266113
Epoch 620, val loss: 0.5829218626022339
Epoch 630, training loss: 424.4850158691406 = 0.5509461760520935 + 50.0 * 8.478681564331055
Epoch 630, val loss: 0.5762323141098022
Epoch 640, training loss: 424.6123046875 = 0.543902575969696 + 50.0 * 8.481368064880371
Epoch 640, val loss: 0.5698676705360413
Epoch 650, training loss: 424.26422119140625 = 0.537172794342041 + 50.0 * 8.474540710449219
Epoch 650, val loss: 0.5639042854309082
Epoch 660, training loss: 424.1728820800781 = 0.5308627486228943 + 50.0 * 8.472840309143066
Epoch 660, val loss: 0.5582185387611389
Epoch 670, training loss: 424.0526428222656 = 0.5247839689254761 + 50.0 * 8.47055721282959
Epoch 670, val loss: 0.5528119206428528
Epoch 680, training loss: 423.95025634765625 = 0.5190194249153137 + 50.0 * 8.46862506866455
Epoch 680, val loss: 0.5477433800697327
Epoch 690, training loss: 424.0770263671875 = 0.5134597420692444 + 50.0 * 8.471271514892578
Epoch 690, val loss: 0.5428968071937561
Epoch 700, training loss: 423.7646789550781 = 0.5081894397735596 + 50.0 * 8.465129852294922
Epoch 700, val loss: 0.5383000373840332
Epoch 710, training loss: 423.68280029296875 = 0.5032374858856201 + 50.0 * 8.463591575622559
Epoch 710, val loss: 0.53396075963974
Epoch 720, training loss: 423.6502990722656 = 0.4984655976295471 + 50.0 * 8.46303653717041
Epoch 720, val loss: 0.5298235416412354
Epoch 730, training loss: 423.5352783203125 = 0.49384963512420654 + 50.0 * 8.46082878112793
Epoch 730, val loss: 0.5259348154067993
Epoch 740, training loss: 423.4643859863281 = 0.48955145478248596 + 50.0 * 8.45949649810791
Epoch 740, val loss: 0.5223084092140198
Epoch 750, training loss: 423.3643798828125 = 0.4854269027709961 + 50.0 * 8.457579612731934
Epoch 750, val loss: 0.5188186168670654
Epoch 760, training loss: 423.288330078125 = 0.4815061092376709 + 50.0 * 8.456136703491211
Epoch 760, val loss: 0.5155802369117737
Epoch 770, training loss: 423.2566833496094 = 0.4777681231498718 + 50.0 * 8.455577850341797
Epoch 770, val loss: 0.5124539136886597
Epoch 780, training loss: 423.2308044433594 = 0.4740906357765198 + 50.0 * 8.455134391784668
Epoch 780, val loss: 0.5095078349113464
Epoch 790, training loss: 423.09429931640625 = 0.47067686915397644 + 50.0 * 8.452472686767578
Epoch 790, val loss: 0.5067635774612427
Epoch 800, training loss: 423.0415344238281 = 0.4674062728881836 + 50.0 * 8.451482772827148
Epoch 800, val loss: 0.5040991902351379
Epoch 810, training loss: 423.07025146484375 = 0.4642704427242279 + 50.0 * 8.452119827270508
Epoch 810, val loss: 0.5016305446624756
Epoch 820, training loss: 422.92095947265625 = 0.4612272381782532 + 50.0 * 8.44919490814209
Epoch 820, val loss: 0.49926042556762695
Epoch 830, training loss: 422.8623352050781 = 0.4583791494369507 + 50.0 * 8.448079109191895
Epoch 830, val loss: 0.4970404803752899
Epoch 840, training loss: 422.7952575683594 = 0.4556264877319336 + 50.0 * 8.446792602539062
Epoch 840, val loss: 0.49490922689437866
Epoch 850, training loss: 422.7530212402344 = 0.4530019760131836 + 50.0 * 8.446000099182129
Epoch 850, val loss: 0.4929156005382538
Epoch 860, training loss: 422.8115234375 = 0.45045286417007446 + 50.0 * 8.447221755981445
Epoch 860, val loss: 0.49100616574287415
Epoch 870, training loss: 422.6658020019531 = 0.447986364364624 + 50.0 * 8.444355964660645
Epoch 870, val loss: 0.4891953468322754
Epoch 880, training loss: 422.59295654296875 = 0.445665180683136 + 50.0 * 8.44294548034668
Epoch 880, val loss: 0.4874724745750427
Epoch 890, training loss: 422.5810852050781 = 0.4433967173099518 + 50.0 * 8.442753791809082
Epoch 890, val loss: 0.485821008682251
Epoch 900, training loss: 422.5908203125 = 0.4412085711956024 + 50.0 * 8.442992210388184
Epoch 900, val loss: 0.4842763841152191
Epoch 910, training loss: 422.4363708496094 = 0.43910738825798035 + 50.0 * 8.439945220947266
Epoch 910, val loss: 0.48274683952331543
Epoch 920, training loss: 422.3774719238281 = 0.43708890676498413 + 50.0 * 8.438807487487793
Epoch 920, val loss: 0.48136812448501587
Epoch 930, training loss: 422.33013916015625 = 0.43514391779899597 + 50.0 * 8.437899589538574
Epoch 930, val loss: 0.47999107837677
Epoch 940, training loss: 422.4120178222656 = 0.43324434757232666 + 50.0 * 8.4395751953125
Epoch 940, val loss: 0.4786689877510071
Epoch 950, training loss: 422.2745361328125 = 0.43141359090805054 + 50.0 * 8.436861991882324
Epoch 950, val loss: 0.4774969816207886
Epoch 960, training loss: 422.2312316894531 = 0.4296516180038452 + 50.0 * 8.436031341552734
Epoch 960, val loss: 0.4762527048587799
Epoch 970, training loss: 422.14031982421875 = 0.4279322922229767 + 50.0 * 8.434247970581055
Epoch 970, val loss: 0.4751940667629242
Epoch 980, training loss: 422.07147216796875 = 0.42628243565559387 + 50.0 * 8.432904243469238
Epoch 980, val loss: 0.4741394817829132
Epoch 990, training loss: 422.1214904785156 = 0.42466604709625244 + 50.0 * 8.433937072753906
Epoch 990, val loss: 0.4731248617172241
Epoch 1000, training loss: 422.09954833984375 = 0.4230539798736572 + 50.0 * 8.4335298538208
Epoch 1000, val loss: 0.47199803590774536
Epoch 1010, training loss: 421.96124267578125 = 0.4215204119682312 + 50.0 * 8.430794715881348
Epoch 1010, val loss: 0.4712037444114685
Epoch 1020, training loss: 421.9010009765625 = 0.4200460910797119 + 50.0 * 8.429618835449219
Epoch 1020, val loss: 0.4702512323856354
Epoch 1030, training loss: 421.8533935546875 = 0.41861873865127563 + 50.0 * 8.428695678710938
Epoch 1030, val loss: 0.4693819582462311
Epoch 1040, training loss: 421.8258361816406 = 0.4172230660915375 + 50.0 * 8.42817211151123
Epoch 1040, val loss: 0.46856507658958435
Epoch 1050, training loss: 421.87469482421875 = 0.4158431589603424 + 50.0 * 8.429177284240723
Epoch 1050, val loss: 0.4677160978317261
Epoch 1060, training loss: 421.8247985839844 = 0.41448307037353516 + 50.0 * 8.428206443786621
Epoch 1060, val loss: 0.4670097827911377
Epoch 1070, training loss: 421.7317199707031 = 0.4131603538990021 + 50.0 * 8.426371574401855
Epoch 1070, val loss: 0.46623167395591736
Epoch 1080, training loss: 421.68194580078125 = 0.41189199686050415 + 50.0 * 8.425400733947754
Epoch 1080, val loss: 0.46559983491897583
Epoch 1090, training loss: 421.6228332519531 = 0.4106537401676178 + 50.0 * 8.424243927001953
Epoch 1090, val loss: 0.4648832380771637
Epoch 1100, training loss: 421.5936584472656 = 0.40944796800613403 + 50.0 * 8.423684120178223
Epoch 1100, val loss: 0.4642470180988312
Epoch 1110, training loss: 421.6414489746094 = 0.40826088190078735 + 50.0 * 8.424663543701172
Epoch 1110, val loss: 0.46360161900520325
Epoch 1120, training loss: 421.55303955078125 = 0.40707772970199585 + 50.0 * 8.422919273376465
Epoch 1120, val loss: 0.4629729688167572
Epoch 1130, training loss: 421.47747802734375 = 0.40594202280044556 + 50.0 * 8.421430587768555
Epoch 1130, val loss: 0.46243083477020264
Epoch 1140, training loss: 421.4438171386719 = 0.40483903884887695 + 50.0 * 8.42077922821045
Epoch 1140, val loss: 0.4618151783943176
Epoch 1150, training loss: 421.42095947265625 = 0.40376630425453186 + 50.0 * 8.420343399047852
Epoch 1150, val loss: 0.4612901508808136
Epoch 1160, training loss: 421.62615966796875 = 0.40270519256591797 + 50.0 * 8.424468994140625
Epoch 1160, val loss: 0.4606646001338959
Epoch 1170, training loss: 421.42657470703125 = 0.4016351103782654 + 50.0 * 8.420498847961426
Epoch 1170, val loss: 0.46028992533683777
Epoch 1180, training loss: 421.3114318847656 = 0.400625616312027 + 50.0 * 8.41821575164795
Epoch 1180, val loss: 0.45974865555763245
Epoch 1190, training loss: 421.31170654296875 = 0.3996245265007019 + 50.0 * 8.418241500854492
Epoch 1190, val loss: 0.45923665165901184
Epoch 1200, training loss: 421.3664855957031 = 0.3986274003982544 + 50.0 * 8.419357299804688
Epoch 1200, val loss: 0.4588159918785095
Epoch 1210, training loss: 421.2808837890625 = 0.39765581488609314 + 50.0 * 8.417664527893066
Epoch 1210, val loss: 0.45833998918533325
Epoch 1220, training loss: 421.19537353515625 = 0.39670681953430176 + 50.0 * 8.415973663330078
Epoch 1220, val loss: 0.457908570766449
Epoch 1230, training loss: 421.171630859375 = 0.3957709074020386 + 50.0 * 8.41551685333252
Epoch 1230, val loss: 0.45751890540122986
Epoch 1240, training loss: 421.270751953125 = 0.39485687017440796 + 50.0 * 8.41751766204834
Epoch 1240, val loss: 0.45705336332321167
Epoch 1250, training loss: 421.17919921875 = 0.39391544461250305 + 50.0 * 8.415705680847168
Epoch 1250, val loss: 0.4566870927810669
Epoch 1260, training loss: 421.1233825683594 = 0.39303094148635864 + 50.0 * 8.414607048034668
Epoch 1260, val loss: 0.45624420046806335
Epoch 1270, training loss: 421.04559326171875 = 0.3921479284763336 + 50.0 * 8.413068771362305
Epoch 1270, val loss: 0.455806165933609
Epoch 1280, training loss: 421.0243835449219 = 0.3912906348705292 + 50.0 * 8.4126615524292
Epoch 1280, val loss: 0.45544421672821045
Epoch 1290, training loss: 421.2308044433594 = 0.39043083786964417 + 50.0 * 8.416807174682617
Epoch 1290, val loss: 0.455003559589386
Epoch 1300, training loss: 421.04986572265625 = 0.3895677626132965 + 50.0 * 8.413206100463867
Epoch 1300, val loss: 0.45478275418281555
Epoch 1310, training loss: 420.9555358886719 = 0.388741135597229 + 50.0 * 8.411335945129395
Epoch 1310, val loss: 0.45430880784988403
Epoch 1320, training loss: 420.9104919433594 = 0.387923926115036 + 50.0 * 8.410451889038086
Epoch 1320, val loss: 0.45398667454719543
Epoch 1330, training loss: 420.9098205566406 = 0.3871190845966339 + 50.0 * 8.410453796386719
Epoch 1330, val loss: 0.4536709189414978
Epoch 1340, training loss: 421.0199279785156 = 0.3863099217414856 + 50.0 * 8.41267204284668
Epoch 1340, val loss: 0.4532827138900757
Epoch 1350, training loss: 420.88604736328125 = 0.3855115473270416 + 50.0 * 8.410011291503906
Epoch 1350, val loss: 0.45304539799690247
Epoch 1360, training loss: 420.83050537109375 = 0.38473543524742126 + 50.0 * 8.408915519714355
Epoch 1360, val loss: 0.4526476562023163
Epoch 1370, training loss: 420.8711242675781 = 0.38397395610809326 + 50.0 * 8.409743309020996
Epoch 1370, val loss: 0.4523419439792633
Epoch 1380, training loss: 420.8553771972656 = 0.38318073749542236 + 50.0 * 8.409443855285645
Epoch 1380, val loss: 0.4521297812461853
Epoch 1390, training loss: 420.7864990234375 = 0.3824334740638733 + 50.0 * 8.4080810546875
Epoch 1390, val loss: 0.45176199078559875
Epoch 1400, training loss: 420.7226867675781 = 0.38168662786483765 + 50.0 * 8.406820297241211
Epoch 1400, val loss: 0.45144861936569214
Epoch 1410, training loss: 420.6961364746094 = 0.38095924258232117 + 50.0 * 8.406303405761719
Epoch 1410, val loss: 0.4512416422367096
Epoch 1420, training loss: 420.6644592285156 = 0.38024190068244934 + 50.0 * 8.405684471130371
Epoch 1420, val loss: 0.4509586989879608
Epoch 1430, training loss: 420.6516418457031 = 0.3795280456542969 + 50.0 * 8.405442237854004
Epoch 1430, val loss: 0.45069822669029236
Epoch 1440, training loss: 421.214599609375 = 0.37879326939582825 + 50.0 * 8.416716575622559
Epoch 1440, val loss: 0.45041558146476746
Epoch 1450, training loss: 420.6351013183594 = 0.37804660201072693 + 50.0 * 8.40514087677002
Epoch 1450, val loss: 0.45020273327827454
Epoch 1460, training loss: 420.63958740234375 = 0.3773582875728607 + 50.0 * 8.405244827270508
Epoch 1460, val loss: 0.449804425239563
Epoch 1470, training loss: 420.5700378417969 = 0.3766613304615021 + 50.0 * 8.403867721557617
Epoch 1470, val loss: 0.44958919286727905
Epoch 1480, training loss: 420.54071044921875 = 0.3759811520576477 + 50.0 * 8.403294563293457
Epoch 1480, val loss: 0.4493793547153473
Epoch 1490, training loss: 420.51849365234375 = 0.3753076195716858 + 50.0 * 8.402863502502441
Epoch 1490, val loss: 0.4490995407104492
Epoch 1500, training loss: 420.5002136230469 = 0.3746371865272522 + 50.0 * 8.402511596679688
Epoch 1500, val loss: 0.44888728857040405
Epoch 1510, training loss: 420.6397705078125 = 0.37396690249443054 + 50.0 * 8.405316352844238
Epoch 1510, val loss: 0.44854024052619934
Epoch 1520, training loss: 420.61065673828125 = 0.3732661008834839 + 50.0 * 8.40474796295166
Epoch 1520, val loss: 0.4484504163265228
Epoch 1530, training loss: 420.4745178222656 = 0.3725990653038025 + 50.0 * 8.40203857421875
Epoch 1530, val loss: 0.4481160640716553
Epoch 1540, training loss: 420.4364929199219 = 0.37194404006004333 + 50.0 * 8.401290893554688
Epoch 1540, val loss: 0.4478583335876465
Epoch 1550, training loss: 420.4584655761719 = 0.3712974190711975 + 50.0 * 8.401742935180664
Epoch 1550, val loss: 0.4476836919784546
Epoch 1560, training loss: 420.39202880859375 = 0.3706466257572174 + 50.0 * 8.40042781829834
Epoch 1560, val loss: 0.4474482536315918
Epoch 1570, training loss: 420.4225158691406 = 0.3700135350227356 + 50.0 * 8.401049613952637
Epoch 1570, val loss: 0.44721758365631104
Epoch 1580, training loss: 420.4433898925781 = 0.3693625330924988 + 50.0 * 8.401480674743652
Epoch 1580, val loss: 0.44698962569236755
Epoch 1590, training loss: 420.36614990234375 = 0.36873283982276917 + 50.0 * 8.399948120117188
Epoch 1590, val loss: 0.4467753469944
Epoch 1600, training loss: 420.3228759765625 = 0.3681109547615051 + 50.0 * 8.39909553527832
Epoch 1600, val loss: 0.446541965007782
Epoch 1610, training loss: 420.2918395996094 = 0.3674977719783783 + 50.0 * 8.398487091064453
Epoch 1610, val loss: 0.44637802243232727
Epoch 1620, training loss: 420.270263671875 = 0.36688709259033203 + 50.0 * 8.398067474365234
Epoch 1620, val loss: 0.44616881012916565
Epoch 1630, training loss: 420.2679748535156 = 0.3662789762020111 + 50.0 * 8.39803409576416
Epoch 1630, val loss: 0.44595909118652344
Epoch 1640, training loss: 420.52447509765625 = 0.36566248536109924 + 50.0 * 8.403176307678223
Epoch 1640, val loss: 0.4457290768623352
Epoch 1650, training loss: 420.30926513671875 = 0.3650340437889099 + 50.0 * 8.398884773254395
Epoch 1650, val loss: 0.4456084370613098
Epoch 1660, training loss: 420.21453857421875 = 0.3644309341907501 + 50.0 * 8.397002220153809
Epoch 1660, val loss: 0.44533607363700867
Epoch 1670, training loss: 420.1818542480469 = 0.36383330821990967 + 50.0 * 8.396360397338867
Epoch 1670, val loss: 0.4451783001422882
Epoch 1680, training loss: 420.1695861816406 = 0.3632426857948303 + 50.0 * 8.396126747131348
Epoch 1680, val loss: 0.4450061023235321
Epoch 1690, training loss: 420.3079833984375 = 0.3626518249511719 + 50.0 * 8.398906707763672
Epoch 1690, val loss: 0.44482192397117615
Epoch 1700, training loss: 420.1430969238281 = 0.36203137040138245 + 50.0 * 8.395621299743652
Epoch 1700, val loss: 0.4446122646331787
Epoch 1710, training loss: 420.14788818359375 = 0.3614383041858673 + 50.0 * 8.395729064941406
Epoch 1710, val loss: 0.4444149136543274
Epoch 1720, training loss: 420.26983642578125 = 0.3608337938785553 + 50.0 * 8.39818000793457
Epoch 1720, val loss: 0.4442707896232605
Epoch 1730, training loss: 420.10235595703125 = 0.36024487018585205 + 50.0 * 8.394842147827148
Epoch 1730, val loss: 0.4440324306488037
Epoch 1740, training loss: 420.07244873046875 = 0.3596663475036621 + 50.0 * 8.394255638122559
Epoch 1740, val loss: 0.44378966093063354
Epoch 1750, training loss: 420.05328369140625 = 0.3590896725654602 + 50.0 * 8.39388370513916
Epoch 1750, val loss: 0.4437350332736969
Epoch 1760, training loss: 420.031494140625 = 0.35852131247520447 + 50.0 * 8.39345932006836
Epoch 1760, val loss: 0.4435136318206787
Epoch 1770, training loss: 420.04669189453125 = 0.3579469621181488 + 50.0 * 8.39377498626709
Epoch 1770, val loss: 0.44341370463371277
Epoch 1780, training loss: 420.1596374511719 = 0.35736170411109924 + 50.0 * 8.396045684814453
Epoch 1780, val loss: 0.44323134422302246
Epoch 1790, training loss: 420.0677490234375 = 0.3567878305912018 + 50.0 * 8.394219398498535
Epoch 1790, val loss: 0.4429953098297119
Epoch 1800, training loss: 420.02386474609375 = 0.35620737075805664 + 50.0 * 8.393353462219238
Epoch 1800, val loss: 0.44286423921585083
Epoch 1810, training loss: 419.9908447265625 = 0.3556419610977173 + 50.0 * 8.392704010009766
Epoch 1810, val loss: 0.44268468022346497
Epoch 1820, training loss: 419.9510192871094 = 0.35507699847221375 + 50.0 * 8.391919136047363
Epoch 1820, val loss: 0.44255197048187256
Epoch 1830, training loss: 419.9528503417969 = 0.3545168340206146 + 50.0 * 8.391966819763184
Epoch 1830, val loss: 0.4423748850822449
Epoch 1840, training loss: 420.0201416015625 = 0.35395222902297974 + 50.0 * 8.39332389831543
Epoch 1840, val loss: 0.44218963384628296
Epoch 1850, training loss: 419.96710205078125 = 0.35338330268859863 + 50.0 * 8.392273902893066
Epoch 1850, val loss: 0.44212207198143005
Epoch 1860, training loss: 419.94696044921875 = 0.3528168499469757 + 50.0 * 8.39188289642334
Epoch 1860, val loss: 0.44190773367881775
Epoch 1870, training loss: 419.8712463378906 = 0.35225844383239746 + 50.0 * 8.390379905700684
Epoch 1870, val loss: 0.441791296005249
Epoch 1880, training loss: 419.87701416015625 = 0.35170894861221313 + 50.0 * 8.39050579071045
Epoch 1880, val loss: 0.441623717546463
Epoch 1890, training loss: 419.87530517578125 = 0.3511570394039154 + 50.0 * 8.390482902526855
Epoch 1890, val loss: 0.4415340721607208
Epoch 1900, training loss: 419.89306640625 = 0.3506079912185669 + 50.0 * 8.390849113464355
Epoch 1900, val loss: 0.44139036536216736
Epoch 1910, training loss: 419.8006896972656 = 0.35005295276641846 + 50.0 * 8.389012336730957
Epoch 1910, val loss: 0.4412636458873749
Epoch 1920, training loss: 419.7887878417969 = 0.34950926899909973 + 50.0 * 8.388785362243652
Epoch 1920, val loss: 0.44116395711898804
Epoch 1930, training loss: 419.7891845703125 = 0.3489685356616974 + 50.0 * 8.38880443572998
Epoch 1930, val loss: 0.4410616457462311
Epoch 1940, training loss: 419.9444580078125 = 0.348423570394516 + 50.0 * 8.391921043395996
Epoch 1940, val loss: 0.4409305453300476
Epoch 1950, training loss: 419.86309814453125 = 0.34786146879196167 + 50.0 * 8.390304565429688
Epoch 1950, val loss: 0.44069766998291016
Epoch 1960, training loss: 419.7460632324219 = 0.3473130464553833 + 50.0 * 8.387974739074707
Epoch 1960, val loss: 0.4406009912490845
Epoch 1970, training loss: 419.70892333984375 = 0.34677737951278687 + 50.0 * 8.387243270874023
Epoch 1970, val loss: 0.44051098823547363
Epoch 1980, training loss: 419.72222900390625 = 0.3462544083595276 + 50.0 * 8.387519836425781
Epoch 1980, val loss: 0.4403523802757263
Epoch 1990, training loss: 419.7544250488281 = 0.34572145342826843 + 50.0 * 8.388174057006836
Epoch 1990, val loss: 0.44023701548576355
Epoch 2000, training loss: 419.6980895996094 = 0.34518468379974365 + 50.0 * 8.38705825805664
Epoch 2000, val loss: 0.44019922614097595
Epoch 2010, training loss: 419.7034912109375 = 0.3446597754955292 + 50.0 * 8.387176513671875
Epoch 2010, val loss: 0.44003549218177795
Epoch 2020, training loss: 419.8329162597656 = 0.34412434697151184 + 50.0 * 8.389776229858398
Epoch 2020, val loss: 0.4399350881576538
Epoch 2030, training loss: 419.68035888671875 = 0.34357380867004395 + 50.0 * 8.386735916137695
Epoch 2030, val loss: 0.43981555104255676
Epoch 2040, training loss: 419.6090393066406 = 0.3430400490760803 + 50.0 * 8.385319709777832
Epoch 2040, val loss: 0.43969738483428955
Epoch 2050, training loss: 419.601806640625 = 0.3425152003765106 + 50.0 * 8.385185241699219
Epoch 2050, val loss: 0.43957215547561646
Epoch 2060, training loss: 419.5804443359375 = 0.34198832511901855 + 50.0 * 8.384769439697266
Epoch 2060, val loss: 0.4394800364971161
Epoch 2070, training loss: 419.8086242675781 = 0.34146028757095337 + 50.0 * 8.38934326171875
Epoch 2070, val loss: 0.4393097758293152
Epoch 2080, training loss: 419.63995361328125 = 0.34091120958328247 + 50.0 * 8.385980606079102
Epoch 2080, val loss: 0.43934470415115356
Epoch 2090, training loss: 419.6003723144531 = 0.3403809368610382 + 50.0 * 8.385199546813965
Epoch 2090, val loss: 0.4391478896141052
Epoch 2100, training loss: 419.5845642089844 = 0.339858740568161 + 50.0 * 8.384894371032715
Epoch 2100, val loss: 0.43911078572273254
Epoch 2110, training loss: 419.57818603515625 = 0.3393370509147644 + 50.0 * 8.384777069091797
Epoch 2110, val loss: 0.43900352716445923
Epoch 2120, training loss: 419.580810546875 = 0.33881816267967224 + 50.0 * 8.38484001159668
Epoch 2120, val loss: 0.4388681650161743
Epoch 2130, training loss: 419.4945983886719 = 0.33829858899116516 + 50.0 * 8.383126258850098
Epoch 2130, val loss: 0.43879833817481995
Epoch 2140, training loss: 419.492431640625 = 0.3377850353717804 + 50.0 * 8.383092880249023
Epoch 2140, val loss: 0.4386987090110779
Epoch 2150, training loss: 419.4844970703125 = 0.33727288246154785 + 50.0 * 8.382944107055664
Epoch 2150, val loss: 0.43859055638313293
Epoch 2160, training loss: 419.7196350097656 = 0.33676010370254517 + 50.0 * 8.387657165527344
Epoch 2160, val loss: 0.4384657144546509
Epoch 2170, training loss: 419.5444641113281 = 0.33620932698249817 + 50.0 * 8.384164810180664
Epoch 2170, val loss: 0.4385347366333008
Epoch 2180, training loss: 419.4731750488281 = 0.3356926739215851 + 50.0 * 8.382749557495117
Epoch 2180, val loss: 0.43824607133865356
Epoch 2190, training loss: 419.4381408691406 = 0.33517518639564514 + 50.0 * 8.382059097290039
Epoch 2190, val loss: 0.438269704580307
Epoch 2200, training loss: 419.4308776855469 = 0.33467066287994385 + 50.0 * 8.381924629211426
Epoch 2200, val loss: 0.43815112113952637
Epoch 2210, training loss: 419.5196533203125 = 0.33415669202804565 + 50.0 * 8.383709907531738
Epoch 2210, val loss: 0.438103586435318
Epoch 2220, training loss: 419.468994140625 = 0.33364078402519226 + 50.0 * 8.382706642150879
Epoch 2220, val loss: 0.4379987418651581
Epoch 2230, training loss: 419.4487609863281 = 0.33311665058135986 + 50.0 * 8.382312774658203
Epoch 2230, val loss: 0.43789729475975037
Epoch 2240, training loss: 419.4014587402344 = 0.3325999975204468 + 50.0 * 8.381377220153809
Epoch 2240, val loss: 0.43776363134384155
Epoch 2250, training loss: 419.44110107421875 = 0.33208465576171875 + 50.0 * 8.382180213928223
Epoch 2250, val loss: 0.4376766085624695
Epoch 2260, training loss: 419.4061584472656 = 0.3315613865852356 + 50.0 * 8.381491661071777
Epoch 2260, val loss: 0.4376114308834076
Epoch 2270, training loss: 419.36602783203125 = 0.3310437500476837 + 50.0 * 8.380699157714844
Epoch 2270, val loss: 0.43756550550460815
Epoch 2280, training loss: 419.3320617675781 = 0.3305300772190094 + 50.0 * 8.380030632019043
Epoch 2280, val loss: 0.4374525845050812
Epoch 2290, training loss: 419.3238830566406 = 0.3300165832042694 + 50.0 * 8.379877090454102
Epoch 2290, val loss: 0.4373968541622162
Epoch 2300, training loss: 419.4014587402344 = 0.3295021653175354 + 50.0 * 8.381439208984375
Epoch 2300, val loss: 0.43731117248535156
Epoch 2310, training loss: 419.31982421875 = 0.3289644420146942 + 50.0 * 8.379817008972168
Epoch 2310, val loss: 0.43724778294563293
Epoch 2320, training loss: 419.2986755371094 = 0.3284328877925873 + 50.0 * 8.37940502166748
Epoch 2320, val loss: 0.4371545910835266
Epoch 2330, training loss: 419.3450012207031 = 0.3279065191745758 + 50.0 * 8.380341529846191
Epoch 2330, val loss: 0.437055766582489
Epoch 2340, training loss: 419.31549072265625 = 0.3273696303367615 + 50.0 * 8.379762649536133
Epoch 2340, val loss: 0.43696391582489014
Epoch 2350, training loss: 419.35723876953125 = 0.3268316686153412 + 50.0 * 8.380607604980469
Epoch 2350, val loss: 0.43684297800064087
Epoch 2360, training loss: 419.25787353515625 = 0.3262874484062195 + 50.0 * 8.378631591796875
Epoch 2360, val loss: 0.436781108379364
Epoch 2370, training loss: 419.24395751953125 = 0.32574906945228577 + 50.0 * 8.378364562988281
Epoch 2370, val loss: 0.43672385811805725
Epoch 2380, training loss: 419.2314758300781 = 0.3252182602882385 + 50.0 * 8.378125190734863
Epoch 2380, val loss: 0.4366108179092407
Epoch 2390, training loss: 419.21124267578125 = 0.324690580368042 + 50.0 * 8.377731323242188
Epoch 2390, val loss: 0.4365580976009369
Epoch 2400, training loss: 419.2044372558594 = 0.3241589665412903 + 50.0 * 8.377605438232422
Epoch 2400, val loss: 0.436509370803833
Epoch 2410, training loss: 419.4518737792969 = 0.3236266076564789 + 50.0 * 8.382564544677734
Epoch 2410, val loss: 0.4364113509654999
Epoch 2420, training loss: 419.35906982421875 = 0.32305341958999634 + 50.0 * 8.380720138549805
Epoch 2420, val loss: 0.4364098012447357
Epoch 2430, training loss: 419.2000732421875 = 0.3224978744983673 + 50.0 * 8.377551078796387
Epoch 2430, val loss: 0.43617382645606995
Epoch 2440, training loss: 419.18524169921875 = 0.3219521939754486 + 50.0 * 8.377265930175781
Epoch 2440, val loss: 0.43620458245277405
Epoch 2450, training loss: 419.1566162109375 = 0.32141992449760437 + 50.0 * 8.376704216003418
Epoch 2450, val loss: 0.43611690402030945
Epoch 2460, training loss: 419.14202880859375 = 0.3208869695663452 + 50.0 * 8.376422882080078
Epoch 2460, val loss: 0.43607285618782043
Epoch 2470, training loss: 419.13232421875 = 0.32034918665885925 + 50.0 * 8.376239776611328
Epoch 2470, val loss: 0.4360368549823761
Epoch 2480, training loss: 419.1667175292969 = 0.3198099136352539 + 50.0 * 8.376937866210938
Epoch 2480, val loss: 0.43589848279953003
Epoch 2490, training loss: 419.2095642089844 = 0.3192499876022339 + 50.0 * 8.377806663513184
Epoch 2490, val loss: 0.4358592629432678
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8300355149670218
0.8658987176700718
=== training gcn model ===
Epoch 0, training loss: 530.2017211914062 = 1.0873920917510986 + 50.0 * 10.582286834716797
Epoch 0, val loss: 1.0859392881393433
Epoch 10, training loss: 530.1810302734375 = 1.0843071937561035 + 50.0 * 10.581934928894043
Epoch 10, val loss: 1.0828644037246704
Epoch 20, training loss: 530.1038208007812 = 1.0809780359268188 + 50.0 * 10.580456733703613
Epoch 20, val loss: 1.0795611143112183
Epoch 30, training loss: 529.764892578125 = 1.0772918462753296 + 50.0 * 10.573752403259277
Epoch 30, val loss: 1.0759063959121704
Epoch 40, training loss: 528.2886962890625 = 1.0730222463607788 + 50.0 * 10.54431438446045
Epoch 40, val loss: 1.0716601610183716
Epoch 50, training loss: 522.9002685546875 = 1.0680886507034302 + 50.0 * 10.436643600463867
Epoch 50, val loss: 1.066757082939148
Epoch 60, training loss: 505.72869873046875 = 1.0623868703842163 + 50.0 * 10.093326568603516
Epoch 60, val loss: 1.061061978340149
Epoch 70, training loss: 482.570556640625 = 1.0557301044464111 + 50.0 * 9.63029670715332
Epoch 70, val loss: 1.0545480251312256
Epoch 80, training loss: 476.0172119140625 = 1.0505176782608032 + 50.0 * 9.499334335327148
Epoch 80, val loss: 1.0497275590896606
Epoch 90, training loss: 468.51361083984375 = 1.046773076057434 + 50.0 * 9.349336624145508
Epoch 90, val loss: 1.046280026435852
Epoch 100, training loss: 461.600341796875 = 1.0438157320022583 + 50.0 * 9.211130142211914
Epoch 100, val loss: 1.04350745677948
Epoch 110, training loss: 457.4400939941406 = 1.0407887697219849 + 50.0 * 9.127985954284668
Epoch 110, val loss: 1.0405501127243042
Epoch 120, training loss: 454.16259765625 = 1.0373613834381104 + 50.0 * 9.062504768371582
Epoch 120, val loss: 1.0372238159179688
Epoch 130, training loss: 451.44287109375 = 1.0340834856033325 + 50.0 * 9.00817584991455
Epoch 130, val loss: 1.0341500043869019
Epoch 140, training loss: 449.069580078125 = 1.0312411785125732 + 50.0 * 8.960766792297363
Epoch 140, val loss: 1.0314934253692627
Epoch 150, training loss: 446.7091979980469 = 1.028293490409851 + 50.0 * 8.913618087768555
Epoch 150, val loss: 1.0286424160003662
Epoch 160, training loss: 444.896728515625 = 1.0249407291412354 + 50.0 * 8.877435684204102
Epoch 160, val loss: 1.0253562927246094
Epoch 170, training loss: 443.6861572265625 = 1.0212334394454956 + 50.0 * 8.85329818725586
Epoch 170, val loss: 1.0217161178588867
Epoch 180, training loss: 442.4033203125 = 1.0171747207641602 + 50.0 * 8.827722549438477
Epoch 180, val loss: 1.0177494287490845
Epoch 190, training loss: 440.89410400390625 = 1.0131558179855347 + 50.0 * 8.797618865966797
Epoch 190, val loss: 1.013868808746338
Epoch 200, training loss: 439.346923828125 = 1.0092215538024902 + 50.0 * 8.766754150390625
Epoch 200, val loss: 1.0100488662719727
Epoch 210, training loss: 438.26763916015625 = 1.0050575733184814 + 50.0 * 8.745251655578613
Epoch 210, val loss: 1.0059465169906616
Epoch 220, training loss: 436.99627685546875 = 1.000462532043457 + 50.0 * 8.719916343688965
Epoch 220, val loss: 1.001389980316162
Epoch 230, training loss: 435.85955810546875 = 0.9956961274147034 + 50.0 * 8.697277069091797
Epoch 230, val loss: 0.9966806173324585
Epoch 240, training loss: 434.6843566894531 = 0.9908185601234436 + 50.0 * 8.673871040344238
Epoch 240, val loss: 0.9918984174728394
Epoch 250, training loss: 433.6833190917969 = 0.9857367873191833 + 50.0 * 8.653951644897461
Epoch 250, val loss: 0.986903190612793
Epoch 260, training loss: 432.74530029296875 = 0.9801543354988098 + 50.0 * 8.635302543640137
Epoch 260, val loss: 0.9813940525054932
Epoch 270, training loss: 431.955078125 = 0.9739960432052612 + 50.0 * 8.619621276855469
Epoch 270, val loss: 0.9753420352935791
Epoch 280, training loss: 431.32843017578125 = 0.9672055244445801 + 50.0 * 8.607224464416504
Epoch 280, val loss: 0.9686712026596069
Epoch 290, training loss: 430.77960205078125 = 0.9597168564796448 + 50.0 * 8.596397399902344
Epoch 290, val loss: 0.9613586068153381
Epoch 300, training loss: 430.2718811035156 = 0.951675534248352 + 50.0 * 8.586403846740723
Epoch 300, val loss: 0.9535176157951355
Epoch 310, training loss: 429.81292724609375 = 0.943122923374176 + 50.0 * 8.577396392822266
Epoch 310, val loss: 0.9451982975006104
Epoch 320, training loss: 429.6255798339844 = 0.9340980052947998 + 50.0 * 8.573829650878906
Epoch 320, val loss: 0.9364231824874878
Epoch 330, training loss: 429.0115051269531 = 0.9245973825454712 + 50.0 * 8.561738014221191
Epoch 330, val loss: 0.9271223545074463
Epoch 340, training loss: 428.5927429199219 = 0.9147676825523376 + 50.0 * 8.553559303283691
Epoch 340, val loss: 0.9176189303398132
Epoch 350, training loss: 428.20684814453125 = 0.9046563506126404 + 50.0 * 8.546043395996094
Epoch 350, val loss: 0.9077699780464172
Epoch 360, training loss: 427.8695373535156 = 0.8942122459411621 + 50.0 * 8.539505958557129
Epoch 360, val loss: 0.8976193070411682
Epoch 370, training loss: 427.6025390625 = 0.8833891749382019 + 50.0 * 8.534382820129395
Epoch 370, val loss: 0.8871076703071594
Epoch 380, training loss: 427.35382080078125 = 0.8722876906394958 + 50.0 * 8.529630661010742
Epoch 380, val loss: 0.8763365149497986
Epoch 390, training loss: 427.02886962890625 = 0.8610566258430481 + 50.0 * 8.523356437683105
Epoch 390, val loss: 0.8654417395591736
Epoch 400, training loss: 426.7912902832031 = 0.8496827483177185 + 50.0 * 8.518832206726074
Epoch 400, val loss: 0.8544047474861145
Epoch 410, training loss: 426.8754577636719 = 0.8381975889205933 + 50.0 * 8.520745277404785
Epoch 410, val loss: 0.8432338833808899
Epoch 420, training loss: 426.4339294433594 = 0.8265653848648071 + 50.0 * 8.512146949768066
Epoch 420, val loss: 0.8320019841194153
Epoch 430, training loss: 426.16534423828125 = 0.8149973750114441 + 50.0 * 8.507006645202637
Epoch 430, val loss: 0.8208374381065369
Epoch 440, training loss: 425.9584045410156 = 0.8034714460372925 + 50.0 * 8.503098487854004
Epoch 440, val loss: 0.8096943497657776
Epoch 450, training loss: 425.7679443359375 = 0.792027473449707 + 50.0 * 8.499518394470215
Epoch 450, val loss: 0.7986443638801575
Epoch 460, training loss: 425.62310791015625 = 0.7806534171104431 + 50.0 * 8.496849060058594
Epoch 460, val loss: 0.7876647710800171
Epoch 470, training loss: 425.5042419433594 = 0.7693126797676086 + 50.0 * 8.494698524475098
Epoch 470, val loss: 0.7767933011054993
Epoch 480, training loss: 425.3019104003906 = 0.7581363320350647 + 50.0 * 8.490875244140625
Epoch 480, val loss: 0.766072154045105
Epoch 490, training loss: 425.16204833984375 = 0.7471345663070679 + 50.0 * 8.488298416137695
Epoch 490, val loss: 0.7555195093154907
Epoch 500, training loss: 425.0255432128906 = 0.7362224459648132 + 50.0 * 8.485786437988281
Epoch 500, val loss: 0.7450329661369324
Epoch 510, training loss: 424.9022521972656 = 0.725511372089386 + 50.0 * 8.483534812927246
Epoch 510, val loss: 0.7348344326019287
Epoch 520, training loss: 424.7616271972656 = 0.7150145769119263 + 50.0 * 8.480932235717773
Epoch 520, val loss: 0.7248277068138123
Epoch 530, training loss: 424.6337585449219 = 0.7047587037086487 + 50.0 * 8.4785795211792
Epoch 530, val loss: 0.7150911688804626
Epoch 540, training loss: 424.72003173828125 = 0.6946889758110046 + 50.0 * 8.480506896972656
Epoch 540, val loss: 0.7055187225341797
Epoch 550, training loss: 424.4523010253906 = 0.6847896575927734 + 50.0 * 8.475350379943848
Epoch 550, val loss: 0.6961950659751892
Epoch 560, training loss: 424.29205322265625 = 0.6752049326896667 + 50.0 * 8.472336769104004
Epoch 560, val loss: 0.6871817111968994
Epoch 570, training loss: 424.1592712402344 = 0.6659212112426758 + 50.0 * 8.469866752624512
Epoch 570, val loss: 0.6784646511077881
Epoch 580, training loss: 424.1564636230469 = 0.6569192409515381 + 50.0 * 8.469990730285645
Epoch 580, val loss: 0.6700121760368347
Epoch 590, training loss: 424.0138854980469 = 0.6481124758720398 + 50.0 * 8.467315673828125
Epoch 590, val loss: 0.6618145108222961
Epoch 600, training loss: 423.8442687988281 = 0.6396138668060303 + 50.0 * 8.464093208312988
Epoch 600, val loss: 0.6539458632469177
Epoch 610, training loss: 423.7114562988281 = 0.6314305067062378 + 50.0 * 8.461600303649902
Epoch 610, val loss: 0.6463897824287415
Epoch 620, training loss: 423.6186218261719 = 0.62353515625 + 50.0 * 8.459901809692383
Epoch 620, val loss: 0.6391474604606628
Epoch 630, training loss: 423.59112548828125 = 0.6158639788627625 + 50.0 * 8.459505081176758
Epoch 630, val loss: 0.6320590376853943
Epoch 640, training loss: 423.4560241699219 = 0.6084331274032593 + 50.0 * 8.456952095031738
Epoch 640, val loss: 0.6253795623779297
Epoch 650, training loss: 423.298828125 = 0.6013233661651611 + 50.0 * 8.453949928283691
Epoch 650, val loss: 0.6189027428627014
Epoch 660, training loss: 423.2157287597656 = 0.5944904088973999 + 50.0 * 8.452425003051758
Epoch 660, val loss: 0.6126973628997803
Epoch 670, training loss: 423.1720886230469 = 0.5879091024398804 + 50.0 * 8.451683044433594
Epoch 670, val loss: 0.6067871451377869
Epoch 680, training loss: 423.0890197753906 = 0.5814867615699768 + 50.0 * 8.450150489807129
Epoch 680, val loss: 0.6011130213737488
Epoch 690, training loss: 423.0149230957031 = 0.5753417015075684 + 50.0 * 8.44879150390625
Epoch 690, val loss: 0.5956594347953796
Epoch 700, training loss: 422.91571044921875 = 0.5693942904472351 + 50.0 * 8.44692611694336
Epoch 700, val loss: 0.5903592109680176
Epoch 710, training loss: 422.84429931640625 = 0.5636903047561646 + 50.0 * 8.445611953735352
Epoch 710, val loss: 0.5854047536849976
Epoch 720, training loss: 422.7631530761719 = 0.5582170486450195 + 50.0 * 8.444098472595215
Epoch 720, val loss: 0.5806207060813904
Epoch 730, training loss: 422.70294189453125 = 0.5529607534408569 + 50.0 * 8.442999839782715
Epoch 730, val loss: 0.5761083364486694
Epoch 740, training loss: 422.8159484863281 = 0.5478578805923462 + 50.0 * 8.445362091064453
Epoch 740, val loss: 0.5716729164123535
Epoch 750, training loss: 422.5861511230469 = 0.5429407954216003 + 50.0 * 8.440864562988281
Epoch 750, val loss: 0.567532479763031
Epoch 760, training loss: 422.52899169921875 = 0.538264274597168 + 50.0 * 8.439814567565918
Epoch 760, val loss: 0.5636085867881775
Epoch 770, training loss: 422.45892333984375 = 0.5337610244750977 + 50.0 * 8.43850326538086
Epoch 770, val loss: 0.559845507144928
Epoch 780, training loss: 422.505615234375 = 0.5294232964515686 + 50.0 * 8.439523696899414
Epoch 780, val loss: 0.5562617778778076
Epoch 790, training loss: 422.3780212402344 = 0.5251979827880859 + 50.0 * 8.437056541442871
Epoch 790, val loss: 0.5527439117431641
Epoch 800, training loss: 422.3330383300781 = 0.5211659669876099 + 50.0 * 8.436237335205078
Epoch 800, val loss: 0.549509584903717
Epoch 810, training loss: 422.2510986328125 = 0.5172930359840393 + 50.0 * 8.434676170349121
Epoch 810, val loss: 0.546364426612854
Epoch 820, training loss: 422.20172119140625 = 0.5135735869407654 + 50.0 * 8.433762550354004
Epoch 820, val loss: 0.5434210300445557
Epoch 830, training loss: 422.2699890136719 = 0.5099596977233887 + 50.0 * 8.435200691223145
Epoch 830, val loss: 0.5405901670455933
Epoch 840, training loss: 422.1786804199219 = 0.5064612030982971 + 50.0 * 8.433444023132324
Epoch 840, val loss: 0.5378462076187134
Epoch 850, training loss: 422.07379150390625 = 0.5030979514122009 + 50.0 * 8.431413650512695
Epoch 850, val loss: 0.5351668000221252
Epoch 860, training loss: 421.9976501464844 = 0.49988842010498047 + 50.0 * 8.42995548248291
Epoch 860, val loss: 0.5327448844909668
Epoch 870, training loss: 421.9412536621094 = 0.49680134654045105 + 50.0 * 8.428889274597168
Epoch 870, val loss: 0.5304136872291565
Epoch 880, training loss: 422.0632629394531 = 0.49381595849990845 + 50.0 * 8.431388854980469
Epoch 880, val loss: 0.5281328558921814
Epoch 890, training loss: 421.9471435546875 = 0.49084851145744324 + 50.0 * 8.429125785827637
Epoch 890, val loss: 0.5260012149810791
Epoch 900, training loss: 421.80743408203125 = 0.48804906010627747 + 50.0 * 8.426387786865234
Epoch 900, val loss: 0.5239508748054504
Epoch 910, training loss: 421.7515563964844 = 0.48537129163742065 + 50.0 * 8.425323486328125
Epoch 910, val loss: 0.5219793319702148
Epoch 920, training loss: 421.7216796875 = 0.48278915882110596 + 50.0 * 8.42477798461914
Epoch 920, val loss: 0.5201930403709412
Epoch 930, training loss: 421.7459411621094 = 0.4802502691745758 + 50.0 * 8.425313949584961
Epoch 930, val loss: 0.5183483958244324
Epoch 940, training loss: 421.6189270019531 = 0.47777268290519714 + 50.0 * 8.422822952270508
Epoch 940, val loss: 0.5166840553283691
Epoch 950, training loss: 421.5834045410156 = 0.4754202961921692 + 50.0 * 8.422159194946289
Epoch 950, val loss: 0.5149560570716858
Epoch 960, training loss: 421.5264587402344 = 0.47314217686653137 + 50.0 * 8.421066284179688
Epoch 960, val loss: 0.5134764909744263
Epoch 970, training loss: 421.5571594238281 = 0.4709361791610718 + 50.0 * 8.421724319458008
Epoch 970, val loss: 0.5119646191596985
Epoch 980, training loss: 421.7273254394531 = 0.4686947762966156 + 50.0 * 8.425172805786133
Epoch 980, val loss: 0.510448694229126
Epoch 990, training loss: 421.471435546875 = 0.4665469527244568 + 50.0 * 8.420097351074219
Epoch 990, val loss: 0.5090578198432922
Epoch 1000, training loss: 421.3816833496094 = 0.4645204544067383 + 50.0 * 8.418343544006348
Epoch 1000, val loss: 0.507695734500885
Epoch 1010, training loss: 421.31549072265625 = 0.46256235241889954 + 50.0 * 8.417058944702148
Epoch 1010, val loss: 0.5064184069633484
Epoch 1020, training loss: 421.2680358886719 = 0.4606676399707794 + 50.0 * 8.416147232055664
Epoch 1020, val loss: 0.5052180290222168
Epoch 1030, training loss: 421.32232666015625 = 0.45881739258766174 + 50.0 * 8.417269706726074
Epoch 1030, val loss: 0.5040610432624817
Epoch 1040, training loss: 421.2044677734375 = 0.456941157579422 + 50.0 * 8.414950370788574
Epoch 1040, val loss: 0.5028345584869385
Epoch 1050, training loss: 421.2048645019531 = 0.45514994859695435 + 50.0 * 8.414994239807129
Epoch 1050, val loss: 0.5017634630203247
Epoch 1060, training loss: 421.1099548339844 = 0.4534299671649933 + 50.0 * 8.413130760192871
Epoch 1060, val loss: 0.500663161277771
Epoch 1070, training loss: 421.05841064453125 = 0.4517606794834137 + 50.0 * 8.41213321685791
Epoch 1070, val loss: 0.4996746778488159
Epoch 1080, training loss: 421.0288391113281 = 0.45012366771698 + 50.0 * 8.411574363708496
Epoch 1080, val loss: 0.49865737557411194
Epoch 1090, training loss: 421.34222412109375 = 0.4484645426273346 + 50.0 * 8.417875289916992
Epoch 1090, val loss: 0.4976239502429962
Epoch 1100, training loss: 421.0850830078125 = 0.44681254029273987 + 50.0 * 8.412765502929688
Epoch 1100, val loss: 0.49662744998931885
Epoch 1110, training loss: 420.9579162597656 = 0.4452536106109619 + 50.0 * 8.410253524780273
Epoch 1110, val loss: 0.49576494097709656
Epoch 1120, training loss: 420.88543701171875 = 0.44375815987586975 + 50.0 * 8.408833503723145
Epoch 1120, val loss: 0.49489834904670715
Epoch 1130, training loss: 420.8359069824219 = 0.4422951340675354 + 50.0 * 8.407872200012207
Epoch 1130, val loss: 0.4940781593322754
Epoch 1140, training loss: 420.7972717285156 = 0.4408594071865082 + 50.0 * 8.40712833404541
Epoch 1140, val loss: 0.4932829439640045
Epoch 1150, training loss: 420.7760314941406 = 0.4394414722919464 + 50.0 * 8.406731605529785
Epoch 1150, val loss: 0.4925220012664795
Epoch 1160, training loss: 420.87158203125 = 0.437966912984848 + 50.0 * 8.408672332763672
Epoch 1160, val loss: 0.491567462682724
Epoch 1170, training loss: 420.73626708984375 = 0.4365001618862152 + 50.0 * 8.40599536895752
Epoch 1170, val loss: 0.4907763600349426
Epoch 1180, training loss: 420.7264709472656 = 0.43514567613601685 + 50.0 * 8.405826568603516
Epoch 1180, val loss: 0.49003762006759644
Epoch 1190, training loss: 420.6529235839844 = 0.4338397979736328 + 50.0 * 8.40438175201416
Epoch 1190, val loss: 0.48933571577072144
Epoch 1200, training loss: 420.6104431152344 = 0.4325743317604065 + 50.0 * 8.403556823730469
Epoch 1200, val loss: 0.4886896312236786
Epoch 1210, training loss: 420.58148193359375 = 0.43130871653556824 + 50.0 * 8.403003692626953
Epoch 1210, val loss: 0.4880267083644867
Epoch 1220, training loss: 420.5537109375 = 0.43006056547164917 + 50.0 * 8.402473449707031
Epoch 1220, val loss: 0.48737832903862
Epoch 1230, training loss: 420.6767272949219 = 0.4288129210472107 + 50.0 * 8.404958724975586
Epoch 1230, val loss: 0.48676422238349915
Epoch 1240, training loss: 420.64202880859375 = 0.4275544285774231 + 50.0 * 8.404289245605469
Epoch 1240, val loss: 0.485979825258255
Epoch 1250, training loss: 420.47589111328125 = 0.4263271391391754 + 50.0 * 8.400991439819336
Epoch 1250, val loss: 0.48537060618400574
Epoch 1260, training loss: 420.46661376953125 = 0.4251338541507721 + 50.0 * 8.400829315185547
Epoch 1260, val loss: 0.48479610681533813
Epoch 1270, training loss: 420.42724609375 = 0.4239865243434906 + 50.0 * 8.400065422058105
Epoch 1270, val loss: 0.4842256009578705
Epoch 1280, training loss: 420.404296875 = 0.4228523075580597 + 50.0 * 8.399628639221191
Epoch 1280, val loss: 0.4836314916610718
Epoch 1290, training loss: 420.451171875 = 0.42172667384147644 + 50.0 * 8.400588989257812
Epoch 1290, val loss: 0.48309096693992615
Epoch 1300, training loss: 420.383056640625 = 0.42056357860565186 + 50.0 * 8.399250030517578
Epoch 1300, val loss: 0.48244786262512207
Epoch 1310, training loss: 420.330810546875 = 0.41944560408592224 + 50.0 * 8.39822769165039
Epoch 1310, val loss: 0.4819135069847107
Epoch 1320, training loss: 420.30487060546875 = 0.4183589220046997 + 50.0 * 8.397729873657227
Epoch 1320, val loss: 0.4813269376754761
Epoch 1330, training loss: 420.2850036621094 = 0.41729971766471863 + 50.0 * 8.397354125976562
Epoch 1330, val loss: 0.4808192849159241
Epoch 1340, training loss: 420.2684326171875 = 0.4162565767765045 + 50.0 * 8.397043228149414
Epoch 1340, val loss: 0.48028111457824707
Epoch 1350, training loss: 420.39990234375 = 0.4152146279811859 + 50.0 * 8.399693489074707
Epoch 1350, val loss: 0.47963523864746094
Epoch 1360, training loss: 420.31036376953125 = 0.4141407012939453 + 50.0 * 8.397924423217773
Epoch 1360, val loss: 0.47933870553970337
Epoch 1370, training loss: 420.23095703125 = 0.4131148159503937 + 50.0 * 8.396356582641602
Epoch 1370, val loss: 0.4786530137062073
Epoch 1380, training loss: 420.1790466308594 = 0.41211622953414917 + 50.0 * 8.395339012145996
Epoch 1380, val loss: 0.4781962037086487
Epoch 1390, training loss: 420.1627502441406 = 0.41114893555641174 + 50.0 * 8.395031929016113
Epoch 1390, val loss: 0.47775882482528687
Epoch 1400, training loss: 420.25372314453125 = 0.41017913818359375 + 50.0 * 8.396870613098145
Epoch 1400, val loss: 0.47722846269607544
Epoch 1410, training loss: 420.1590576171875 = 0.4091932773590088 + 50.0 * 8.394997596740723
Epoch 1410, val loss: 0.4767981767654419
Epoch 1420, training loss: 420.3058776855469 = 0.4082190990447998 + 50.0 * 8.397953033447266
Epoch 1420, val loss: 0.47624847292900085
Epoch 1430, training loss: 420.1000671386719 = 0.40725600719451904 + 50.0 * 8.393856048583984
Epoch 1430, val loss: 0.4759258031845093
Epoch 1440, training loss: 420.06561279296875 = 0.40633633732795715 + 50.0 * 8.39318561553955
Epoch 1440, val loss: 0.4753784239292145
Epoch 1450, training loss: 420.04443359375 = 0.40542685985565186 + 50.0 * 8.392780303955078
Epoch 1450, val loss: 0.4750056266784668
Epoch 1460, training loss: 420.0173645019531 = 0.4045393168926239 + 50.0 * 8.392256736755371
Epoch 1460, val loss: 0.4745856821537018
Epoch 1470, training loss: 420.0229187011719 = 0.4036538600921631 + 50.0 * 8.392385482788086
Epoch 1470, val loss: 0.4742155373096466
Epoch 1480, training loss: 420.13641357421875 = 0.4027434289455414 + 50.0 * 8.394673347473145
Epoch 1480, val loss: 0.4737927317619324
Epoch 1490, training loss: 419.9931335449219 = 0.40185144543647766 + 50.0 * 8.391825675964355
Epoch 1490, val loss: 0.4732930362224579
Epoch 1500, training loss: 420.0536804199219 = 0.4009709358215332 + 50.0 * 8.393054008483887
Epoch 1500, val loss: 0.4729052186012268
Epoch 1510, training loss: 419.9436950683594 = 0.4000769257545471 + 50.0 * 8.39087200164795
Epoch 1510, val loss: 0.4724680483341217
Epoch 1520, training loss: 419.91864013671875 = 0.3992290794849396 + 50.0 * 8.390388488769531
Epoch 1520, val loss: 0.4720812737941742
Epoch 1530, training loss: 419.8970947265625 = 0.39838990569114685 + 50.0 * 8.389974594116211
Epoch 1530, val loss: 0.4716661274433136
Epoch 1540, training loss: 419.8746032714844 = 0.39757445454597473 + 50.0 * 8.389540672302246
Epoch 1540, val loss: 0.4713284373283386
Epoch 1550, training loss: 419.85546875 = 0.39675772190093994 + 50.0 * 8.389174461364746
Epoch 1550, val loss: 0.4709862172603607
Epoch 1560, training loss: 419.96112060546875 = 0.39593979716300964 + 50.0 * 8.391304016113281
Epoch 1560, val loss: 0.4707588851451874
Epoch 1570, training loss: 419.9471130371094 = 0.3950835168361664 + 50.0 * 8.391040802001953
Epoch 1570, val loss: 0.4700389504432678
Epoch 1580, training loss: 419.86309814453125 = 0.39424219727516174 + 50.0 * 8.389376640319824
Epoch 1580, val loss: 0.4698265492916107
Epoch 1590, training loss: 419.7867736816406 = 0.3934430181980133 + 50.0 * 8.387866973876953
Epoch 1590, val loss: 0.46944135427474976
Epoch 1600, training loss: 419.7828369140625 = 0.3926701247692108 + 50.0 * 8.387803077697754
Epoch 1600, val loss: 0.468988299369812
Epoch 1610, training loss: 419.8865051269531 = 0.39188867807388306 + 50.0 * 8.389892578125
Epoch 1610, val loss: 0.46868494153022766
Epoch 1620, training loss: 419.73345947265625 = 0.39107465744018555 + 50.0 * 8.386847496032715
Epoch 1620, val loss: 0.4683220386505127
Epoch 1630, training loss: 419.7291259765625 = 0.3902926445007324 + 50.0 * 8.3867769241333
Epoch 1630, val loss: 0.46791574358940125
Epoch 1640, training loss: 419.7152099609375 = 0.38953062891960144 + 50.0 * 8.386513710021973
Epoch 1640, val loss: 0.46760258078575134
Epoch 1650, training loss: 419.68756103515625 = 0.38877391815185547 + 50.0 * 8.38597583770752
Epoch 1650, val loss: 0.46725720167160034
Epoch 1660, training loss: 419.6753845214844 = 0.3880216181278229 + 50.0 * 8.385746955871582
Epoch 1660, val loss: 0.4669284522533417
Epoch 1670, training loss: 419.7559509277344 = 0.38726553320884705 + 50.0 * 8.387373924255371
Epoch 1670, val loss: 0.46661198139190674
Epoch 1680, training loss: 419.7931213378906 = 0.38647356629371643 + 50.0 * 8.38813304901123
Epoch 1680, val loss: 0.46612343192100525
Epoch 1690, training loss: 419.6927795410156 = 0.38569289445877075 + 50.0 * 8.386141777038574
Epoch 1690, val loss: 0.4658268690109253
Epoch 1700, training loss: 419.6065368652344 = 0.38493812084198 + 50.0 * 8.384431838989258
Epoch 1700, val loss: 0.46541765332221985
Epoch 1710, training loss: 419.6038513183594 = 0.38420605659484863 + 50.0 * 8.384392738342285
Epoch 1710, val loss: 0.4650723338127136
Epoch 1720, training loss: 419.7157897949219 = 0.3834728002548218 + 50.0 * 8.386646270751953
Epoch 1720, val loss: 0.46468570828437805
Epoch 1730, training loss: 419.5672912597656 = 0.3827010989189148 + 50.0 * 8.383691787719727
Epoch 1730, val loss: 0.464372843503952
Epoch 1740, training loss: 419.5594482421875 = 0.3819580674171448 + 50.0 * 8.383549690246582
Epoch 1740, val loss: 0.46393755078315735
Epoch 1750, training loss: 419.538818359375 = 0.38122987747192383 + 50.0 * 8.38315200805664
Epoch 1750, val loss: 0.4636244475841522
Epoch 1760, training loss: 419.5158996582031 = 0.380513995885849 + 50.0 * 8.382707595825195
Epoch 1760, val loss: 0.46326547861099243
Epoch 1770, training loss: 419.50897216796875 = 0.3797992765903473 + 50.0 * 8.382583618164062
Epoch 1770, val loss: 0.4629575312137604
Epoch 1780, training loss: 419.8310546875 = 0.3790733218193054 + 50.0 * 8.389039993286133
Epoch 1780, val loss: 0.4626292586326599
Epoch 1790, training loss: 419.6009521484375 = 0.3783090114593506 + 50.0 * 8.384452819824219
Epoch 1790, val loss: 0.46214962005615234
Epoch 1800, training loss: 419.4793701171875 = 0.377581924200058 + 50.0 * 8.382035255432129
Epoch 1800, val loss: 0.4618360102176666
Epoch 1810, training loss: 419.43707275390625 = 0.3768795132637024 + 50.0 * 8.381203651428223
Epoch 1810, val loss: 0.4614849090576172
Epoch 1820, training loss: 419.4263610839844 = 0.37618982791900635 + 50.0 * 8.381003379821777
Epoch 1820, val loss: 0.4611489474773407
Epoch 1830, training loss: 419.4955139160156 = 0.375495970249176 + 50.0 * 8.382400512695312
Epoch 1830, val loss: 0.4608304798603058
Epoch 1840, training loss: 419.4098815917969 = 0.3747731149196625 + 50.0 * 8.380702018737793
Epoch 1840, val loss: 0.460507869720459
Epoch 1850, training loss: 419.4297180175781 = 0.3740672171115875 + 50.0 * 8.381113052368164
Epoch 1850, val loss: 0.46024543046951294
Epoch 1860, training loss: 419.41607666015625 = 0.37335851788520813 + 50.0 * 8.380854606628418
Epoch 1860, val loss: 0.45985063910484314
Epoch 1870, training loss: 419.4371337890625 = 0.3726574778556824 + 50.0 * 8.3812894821167
Epoch 1870, val loss: 0.45941177010536194
Epoch 1880, training loss: 419.3637390136719 = 0.37195897102355957 + 50.0 * 8.379836082458496
Epoch 1880, val loss: 0.4592013657093048
Epoch 1890, training loss: 419.31988525390625 = 0.3712693452835083 + 50.0 * 8.378972053527832
Epoch 1890, val loss: 0.45882514119148254
Epoch 1900, training loss: 419.3114013671875 = 0.37058988213539124 + 50.0 * 8.378816604614258
Epoch 1900, val loss: 0.4584837555885315
Epoch 1910, training loss: 419.3960266113281 = 0.3699049651622772 + 50.0 * 8.380522727966309
Epoch 1910, val loss: 0.4581510126590729
Epoch 1920, training loss: 419.3835754394531 = 0.36916765570640564 + 50.0 * 8.380288124084473
Epoch 1920, val loss: 0.45795580744743347
Epoch 1930, training loss: 419.31884765625 = 0.3684557378292084 + 50.0 * 8.379007339477539
Epoch 1930, val loss: 0.45735469460487366
Epoch 1940, training loss: 419.284912109375 = 0.36776408553123474 + 50.0 * 8.378342628479004
Epoch 1940, val loss: 0.45722532272338867
Epoch 1950, training loss: 419.2420349121094 = 0.36709532141685486 + 50.0 * 8.377498626708984
Epoch 1950, val loss: 0.45682743191719055
Epoch 1960, training loss: 419.2275390625 = 0.36643144488334656 + 50.0 * 8.377222061157227
Epoch 1960, val loss: 0.4565747082233429
Epoch 1970, training loss: 419.3751525878906 = 0.365755558013916 + 50.0 * 8.38018798828125
Epoch 1970, val loss: 0.4562990665435791
Epoch 1980, training loss: 419.2471008300781 = 0.36506369709968567 + 50.0 * 8.377640724182129
Epoch 1980, val loss: 0.45586857199668884
Epoch 1990, training loss: 419.2062072753906 = 0.3643685579299927 + 50.0 * 8.376836776733398
Epoch 1990, val loss: 0.4556328356266022
Epoch 2000, training loss: 419.1772766113281 = 0.3637060821056366 + 50.0 * 8.37627124786377
Epoch 2000, val loss: 0.45530006289482117
Epoch 2010, training loss: 419.1715087890625 = 0.3630371689796448 + 50.0 * 8.376169204711914
Epoch 2010, val loss: 0.4550669491291046
Epoch 2020, training loss: 419.37835693359375 = 0.36236417293548584 + 50.0 * 8.380319595336914
Epoch 2020, val loss: 0.4548419415950775
Epoch 2030, training loss: 419.3031311035156 = 0.36164718866348267 + 50.0 * 8.378829956054688
Epoch 2030, val loss: 0.45442652702331543
Epoch 2040, training loss: 419.1333312988281 = 0.360952764749527 + 50.0 * 8.375447273254395
Epoch 2040, val loss: 0.45410647988319397
Epoch 2050, training loss: 419.11676025390625 = 0.360287606716156 + 50.0 * 8.375129699707031
Epoch 2050, val loss: 0.45383837819099426
Epoch 2060, training loss: 419.08819580078125 = 0.35963189601898193 + 50.0 * 8.374571800231934
Epoch 2060, val loss: 0.4535476863384247
Epoch 2070, training loss: 419.0826110839844 = 0.35898029804229736 + 50.0 * 8.374472618103027
Epoch 2070, val loss: 0.45329195261001587
Epoch 2080, training loss: 419.4388122558594 = 0.35830432176589966 + 50.0 * 8.381609916687012
Epoch 2080, val loss: 0.45295244455337524
Epoch 2090, training loss: 419.13555908203125 = 0.3576093912124634 + 50.0 * 8.375558853149414
Epoch 2090, val loss: 0.45279818773269653
Epoch 2100, training loss: 419.0400390625 = 0.356924831867218 + 50.0 * 8.373661994934082
Epoch 2100, val loss: 0.4523828625679016
Epoch 2110, training loss: 419.029541015625 = 0.35626617074012756 + 50.0 * 8.373465538024902
Epoch 2110, val loss: 0.45220786333084106
Epoch 2120, training loss: 419.0337219238281 = 0.3556155264377594 + 50.0 * 8.37356185913086
Epoch 2120, val loss: 0.45189446210861206
Epoch 2130, training loss: 419.2208251953125 = 0.354946494102478 + 50.0 * 8.377317428588867
Epoch 2130, val loss: 0.45158347487449646
Epoch 2140, training loss: 419.0323181152344 = 0.35426729917526245 + 50.0 * 8.373560905456543
Epoch 2140, val loss: 0.45136094093322754
Epoch 2150, training loss: 418.9745178222656 = 0.353596955537796 + 50.0 * 8.372418403625488
Epoch 2150, val loss: 0.45102962851524353
Epoch 2160, training loss: 418.9507141113281 = 0.35294100642204285 + 50.0 * 8.371955871582031
Epoch 2160, val loss: 0.45081666111946106
Epoch 2170, training loss: 418.9759826660156 = 0.3522871732711792 + 50.0 * 8.37247371673584
Epoch 2170, val loss: 0.45059019327163696
Epoch 2180, training loss: 419.15380859375 = 0.35160329937934875 + 50.0 * 8.376044273376465
Epoch 2180, val loss: 0.45030996203422546
Epoch 2190, training loss: 418.9693603515625 = 0.35091838240623474 + 50.0 * 8.372368812561035
Epoch 2190, val loss: 0.44985702633857727
Epoch 2200, training loss: 418.9092712402344 = 0.35023921728134155 + 50.0 * 8.371180534362793
Epoch 2200, val loss: 0.44963857531547546
Epoch 2210, training loss: 418.9162902832031 = 0.349577397108078 + 50.0 * 8.371334075927734
Epoch 2210, val loss: 0.44935372471809387
Epoch 2220, training loss: 419.04388427734375 = 0.3489075005054474 + 50.0 * 8.373899459838867
Epoch 2220, val loss: 0.44898468255996704
Epoch 2230, training loss: 418.89935302734375 = 0.34822413325309753 + 50.0 * 8.37102222442627
Epoch 2230, val loss: 0.4488810896873474
Epoch 2240, training loss: 418.8542785644531 = 0.3475547134876251 + 50.0 * 8.370134353637695
Epoch 2240, val loss: 0.44851890206336975
Epoch 2250, training loss: 418.87786865234375 = 0.34689217805862427 + 50.0 * 8.370619773864746
Epoch 2250, val loss: 0.4483424127101898
Epoch 2260, training loss: 419.0471496582031 = 0.3462193012237549 + 50.0 * 8.374018669128418
Epoch 2260, val loss: 0.44806045293807983
Epoch 2270, training loss: 418.89141845703125 = 0.34552475810050964 + 50.0 * 8.370918273925781
Epoch 2270, val loss: 0.44765666127204895
Epoch 2280, training loss: 418.8238220214844 = 0.34485217928886414 + 50.0 * 8.369579315185547
Epoch 2280, val loss: 0.44746848940849304
Epoch 2290, training loss: 418.7917175292969 = 0.3441886901855469 + 50.0 * 8.368950843811035
Epoch 2290, val loss: 0.44716161489486694
Epoch 2300, training loss: 418.7873229980469 = 0.3435291647911072 + 50.0 * 8.368875503540039
Epoch 2300, val loss: 0.44696569442749023
Epoch 2310, training loss: 419.0546569824219 = 0.3428618609905243 + 50.0 * 8.374236106872559
Epoch 2310, val loss: 0.44674062728881836
Epoch 2320, training loss: 418.89520263671875 = 0.34214508533477783 + 50.0 * 8.371061325073242
Epoch 2320, val loss: 0.4463631510734558
Epoch 2330, training loss: 418.77490234375 = 0.34145587682724 + 50.0 * 8.368668556213379
Epoch 2330, val loss: 0.446150004863739
Epoch 2340, training loss: 418.7393493652344 = 0.3407786786556244 + 50.0 * 8.367971420288086
Epoch 2340, val loss: 0.4458998441696167
Epoch 2350, training loss: 418.741943359375 = 0.3401122987270355 + 50.0 * 8.368036270141602
Epoch 2350, val loss: 0.44567328691482544
Epoch 2360, training loss: 418.8437194824219 = 0.3394342362880707 + 50.0 * 8.370085716247559
Epoch 2360, val loss: 0.44544482231140137
Epoch 2370, training loss: 418.698974609375 = 0.33874019980430603 + 50.0 * 8.367204666137695
Epoch 2370, val loss: 0.44515836238861084
Epoch 2380, training loss: 418.7176818847656 = 0.3380553126335144 + 50.0 * 8.367592811584473
Epoch 2380, val loss: 0.44492727518081665
Epoch 2390, training loss: 418.8122253417969 = 0.3373720943927765 + 50.0 * 8.369497299194336
Epoch 2390, val loss: 0.444729208946228
Epoch 2400, training loss: 418.6944274902344 = 0.3366611897945404 + 50.0 * 8.367155075073242
Epoch 2400, val loss: 0.4444442093372345
Epoch 2410, training loss: 418.68255615234375 = 0.3359641432762146 + 50.0 * 8.366931915283203
Epoch 2410, val loss: 0.4441867172718048
Epoch 2420, training loss: 418.6602478027344 = 0.33527445793151855 + 50.0 * 8.366499900817871
Epoch 2420, val loss: 0.443918913602829
Epoch 2430, training loss: 418.7204895019531 = 0.33458757400512695 + 50.0 * 8.367717742919922
Epoch 2430, val loss: 0.4436226189136505
Epoch 2440, training loss: 418.65777587890625 = 0.3338744640350342 + 50.0 * 8.366477966308594
Epoch 2440, val loss: 0.4434594213962555
Epoch 2450, training loss: 418.6587829589844 = 0.333168625831604 + 50.0 * 8.366512298583984
Epoch 2450, val loss: 0.44331687688827515
Epoch 2460, training loss: 418.6489562988281 = 0.33246320486068726 + 50.0 * 8.36633014678955
Epoch 2460, val loss: 0.4430159032344818
Epoch 2470, training loss: 418.6782531738281 = 0.3317524790763855 + 50.0 * 8.36693000793457
Epoch 2470, val loss: 0.4428075850009918
Epoch 2480, training loss: 418.69464111328125 = 0.33103179931640625 + 50.0 * 8.36727237701416
Epoch 2480, val loss: 0.442489892244339
Epoch 2490, training loss: 418.5906677246094 = 0.33030933141708374 + 50.0 * 8.365206718444824
Epoch 2490, val loss: 0.44230446219444275
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8386605783866057
0.8619865246685504
The final CL Acc:0.83071, 0.00623, The final GNN Acc:0.86409, 0.00161
Begin epxeriment: noisy_level: 0.2 cont_weight: 50 epoch: der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106450])
remove edge: torch.Size([2, 70950])
updated graph: torch.Size([2, 88752])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 530.205322265625 = 1.093077540397644 + 50.0 * 10.582244873046875
Epoch 0, val loss: 1.0940043926239014
Epoch 10, training loss: 530.176025390625 = 1.0889949798583984 + 50.0 * 10.581741333007812
Epoch 10, val loss: 1.0899035930633545
Epoch 20, training loss: 530.0560302734375 = 1.084509015083313 + 50.0 * 10.579429626464844
Epoch 20, val loss: 1.0853638648986816
Epoch 30, training loss: 529.5067749023438 = 1.079527735710144 + 50.0 * 10.5685453414917
Epoch 30, val loss: 1.0802850723266602
Epoch 40, training loss: 527.358154296875 = 1.0739359855651855 + 50.0 * 10.525684356689453
Epoch 40, val loss: 1.0745422840118408
Epoch 50, training loss: 521.0346069335938 = 1.0679129362106323 + 50.0 * 10.399333000183105
Epoch 50, val loss: 1.0683891773223877
Epoch 60, training loss: 507.01171875 = 1.0618908405303955 + 50.0 * 10.118996620178223
Epoch 60, val loss: 1.0621620416641235
Epoch 70, training loss: 491.7401428222656 = 1.0560617446899414 + 50.0 * 9.813681602478027
Epoch 70, val loss: 1.0562405586242676
Epoch 80, training loss: 481.8442077636719 = 1.051601767539978 + 50.0 * 9.615852355957031
Epoch 80, val loss: 1.0519453287124634
Epoch 90, training loss: 474.65484619140625 = 1.048079490661621 + 50.0 * 9.472135543823242
Epoch 90, val loss: 1.048615574836731
Epoch 100, training loss: 465.1459655761719 = 1.045662522315979 + 50.0 * 9.28200626373291
Epoch 100, val loss: 1.0463263988494873
Epoch 110, training loss: 458.62530517578125 = 1.0448939800262451 + 50.0 * 9.15160846710205
Epoch 110, val loss: 1.0455890893936157
Epoch 120, training loss: 453.1885986328125 = 1.0442571640014648 + 50.0 * 9.042886734008789
Epoch 120, val loss: 1.0448248386383057
Epoch 130, training loss: 449.0074768066406 = 1.0431596040725708 + 50.0 * 8.9592866897583
Epoch 130, val loss: 1.0436009168624878
Epoch 140, training loss: 446.75567626953125 = 1.0416368246078491 + 50.0 * 8.914280891418457
Epoch 140, val loss: 1.042044758796692
Epoch 150, training loss: 444.9949035644531 = 1.0399341583251953 + 50.0 * 8.879098892211914
Epoch 150, val loss: 1.0403653383255005
Epoch 160, training loss: 443.3633117675781 = 1.038307547569275 + 50.0 * 8.846500396728516
Epoch 160, val loss: 1.0388531684875488
Epoch 170, training loss: 441.9249572753906 = 1.0368393659591675 + 50.0 * 8.81776237487793
Epoch 170, val loss: 1.0374765396118164
Epoch 180, training loss: 440.64813232421875 = 1.035294532775879 + 50.0 * 8.792256355285645
Epoch 180, val loss: 1.0359811782836914
Epoch 190, training loss: 439.62823486328125 = 1.0335971117019653 + 50.0 * 8.771892547607422
Epoch 190, val loss: 1.034332513809204
Epoch 200, training loss: 438.77484130859375 = 1.0317965745925903 + 50.0 * 8.754860877990723
Epoch 200, val loss: 1.032621145248413
Epoch 210, training loss: 438.0798645019531 = 1.0299028158187866 + 50.0 * 8.740999221801758
Epoch 210, val loss: 1.0308254957199097
Epoch 220, training loss: 437.48028564453125 = 1.0278562307357788 + 50.0 * 8.729048728942871
Epoch 220, val loss: 1.0288803577423096
Epoch 230, training loss: 436.8144836425781 = 1.0256868600845337 + 50.0 * 8.715775489807129
Epoch 230, val loss: 1.026798129081726
Epoch 240, training loss: 436.1686096191406 = 1.0233995914459229 + 50.0 * 8.702903747558594
Epoch 240, val loss: 1.0246288776397705
Epoch 250, training loss: 435.6155700683594 = 1.0209912061691284 + 50.0 * 8.69189167022705
Epoch 250, val loss: 1.0223206281661987
Epoch 260, training loss: 434.9967041015625 = 1.0183862447738647 + 50.0 * 8.679566383361816
Epoch 260, val loss: 1.0198700428009033
Epoch 270, training loss: 434.48541259765625 = 1.0156270265579224 + 50.0 * 8.669395446777344
Epoch 270, val loss: 1.0172549486160278
Epoch 280, training loss: 434.0316162109375 = 1.012689232826233 + 50.0 * 8.660378456115723
Epoch 280, val loss: 1.0144834518432617
Epoch 290, training loss: 433.54583740234375 = 1.009529948234558 + 50.0 * 8.650726318359375
Epoch 290, val loss: 1.0115113258361816
Epoch 300, training loss: 433.1104736328125 = 1.0061708688735962 + 50.0 * 8.642086029052734
Epoch 300, val loss: 1.0083690881729126
Epoch 310, training loss: 432.7137451171875 = 1.0025655031204224 + 50.0 * 8.634223937988281
Epoch 310, val loss: 1.0049777030944824
Epoch 320, training loss: 432.2945861816406 = 0.9987112283706665 + 50.0 * 8.625917434692383
Epoch 320, val loss: 1.0013667345046997
Epoch 330, training loss: 431.9146728515625 = 0.9946319460868835 + 50.0 * 8.618400573730469
Epoch 330, val loss: 0.9975334405899048
Epoch 340, training loss: 431.56231689453125 = 0.9903106689453125 + 50.0 * 8.61143970489502
Epoch 340, val loss: 0.9934958219528198
Epoch 350, training loss: 431.23406982421875 = 0.9857594966888428 + 50.0 * 8.604966163635254
Epoch 350, val loss: 0.9892407655715942
Epoch 360, training loss: 431.02020263671875 = 0.9810341596603394 + 50.0 * 8.600783348083496
Epoch 360, val loss: 0.9848031997680664
Epoch 370, training loss: 430.6589660644531 = 0.9760507941246033 + 50.0 * 8.593658447265625
Epoch 370, val loss: 0.980177640914917
Epoch 380, training loss: 430.3505554199219 = 0.9709085822105408 + 50.0 * 8.587593078613281
Epoch 380, val loss: 0.9753618836402893
Epoch 390, training loss: 430.10076904296875 = 0.9655691385269165 + 50.0 * 8.582703590393066
Epoch 390, val loss: 0.970409095287323
Epoch 400, training loss: 429.89886474609375 = 0.9600527286529541 + 50.0 * 8.578776359558105
Epoch 400, val loss: 0.9652402997016907
Epoch 410, training loss: 429.6424255371094 = 0.9543249607086182 + 50.0 * 8.573761940002441
Epoch 410, val loss: 0.9599565863609314
Epoch 420, training loss: 429.3740539550781 = 0.9485344886779785 + 50.0 * 8.568510055541992
Epoch 420, val loss: 0.9545525312423706
Epoch 430, training loss: 429.13018798828125 = 0.9426801204681396 + 50.0 * 8.563750267028809
Epoch 430, val loss: 0.9490967392921448
Epoch 440, training loss: 429.0267333984375 = 0.936733603477478 + 50.0 * 8.561800003051758
Epoch 440, val loss: 0.9435772895812988
Epoch 450, training loss: 428.7538757324219 = 0.9306787848472595 + 50.0 * 8.556464195251465
Epoch 450, val loss: 0.9379070997238159
Epoch 460, training loss: 428.57598876953125 = 0.9245812296867371 + 50.0 * 8.553028106689453
Epoch 460, val loss: 0.9322272539138794
Epoch 470, training loss: 428.3497314453125 = 0.9184359908103943 + 50.0 * 8.548625946044922
Epoch 470, val loss: 0.9265276193618774
Epoch 480, training loss: 428.212890625 = 0.9122918844223022 + 50.0 * 8.546011924743652
Epoch 480, val loss: 0.9208328723907471
Epoch 490, training loss: 428.1033935546875 = 0.9061396718025208 + 50.0 * 8.5439453125
Epoch 490, val loss: 0.9151275157928467
Epoch 500, training loss: 427.9230041503906 = 0.9000272154808044 + 50.0 * 8.540459632873535
Epoch 500, val loss: 0.909403383731842
Epoch 510, training loss: 427.7599182128906 = 0.8939347267150879 + 50.0 * 8.537320137023926
Epoch 510, val loss: 0.9037901759147644
Epoch 520, training loss: 427.6798095703125 = 0.8879191875457764 + 50.0 * 8.53583812713623
Epoch 520, val loss: 0.8981837630271912
Epoch 530, training loss: 427.541748046875 = 0.8819316029548645 + 50.0 * 8.533196449279785
Epoch 530, val loss: 0.8926355838775635
Epoch 540, training loss: 427.39959716796875 = 0.8760098814964294 + 50.0 * 8.530471801757812
Epoch 540, val loss: 0.8871167302131653
Epoch 550, training loss: 427.2702941894531 = 0.870151162147522 + 50.0 * 8.528002738952637
Epoch 550, val loss: 0.8816673755645752
Epoch 560, training loss: 427.13861083984375 = 0.8643600344657898 + 50.0 * 8.525485038757324
Epoch 560, val loss: 0.8762729167938232
Epoch 570, training loss: 427.0717468261719 = 0.8586052656173706 + 50.0 * 8.524262428283691
Epoch 570, val loss: 0.8709021806716919
Epoch 580, training loss: 426.9638366699219 = 0.8528392910957336 + 50.0 * 8.52221965789795
Epoch 580, val loss: 0.8655146360397339
Epoch 590, training loss: 426.8730163574219 = 0.8471330404281616 + 50.0 * 8.520517349243164
Epoch 590, val loss: 0.8601729273796082
Epoch 600, training loss: 426.7579345703125 = 0.8415061235427856 + 50.0 * 8.518328666687012
Epoch 600, val loss: 0.8548949956893921
Epoch 610, training loss: 426.6751708984375 = 0.8359256982803345 + 50.0 * 8.51678466796875
Epoch 610, val loss: 0.8496301174163818
Epoch 620, training loss: 426.6142883300781 = 0.8303292989730835 + 50.0 * 8.515679359436035
Epoch 620, val loss: 0.844382107257843
Epoch 630, training loss: 426.4886169433594 = 0.8247657418251038 + 50.0 * 8.513277053833008
Epoch 630, val loss: 0.8391440510749817
Epoch 640, training loss: 426.3908996582031 = 0.8192793726921082 + 50.0 * 8.511432647705078
Epoch 640, val loss: 0.8340127468109131
Epoch 650, training loss: 426.3028259277344 = 0.8138551712036133 + 50.0 * 8.509779930114746
Epoch 650, val loss: 0.8288947939872742
Epoch 660, training loss: 426.25592041015625 = 0.808413028717041 + 50.0 * 8.508950233459473
Epoch 660, val loss: 0.8237757682800293
Epoch 670, training loss: 426.1756896972656 = 0.8030071258544922 + 50.0 * 8.507453918457031
Epoch 670, val loss: 0.8186970949172974
Epoch 680, training loss: 426.0552062988281 = 0.7976915836334229 + 50.0 * 8.505149841308594
Epoch 680, val loss: 0.8137195110321045
Epoch 690, training loss: 425.9550476074219 = 0.792442262172699 + 50.0 * 8.503252029418945
Epoch 690, val loss: 0.8087813258171082
Epoch 700, training loss: 426.0515441894531 = 0.7872390151023865 + 50.0 * 8.50528621673584
Epoch 700, val loss: 0.8038872480392456
Epoch 710, training loss: 425.8140869140625 = 0.7820307016372681 + 50.0 * 8.500640869140625
Epoch 710, val loss: 0.7989934086799622
Epoch 720, training loss: 425.732177734375 = 0.7769331336021423 + 50.0 * 8.499104499816895
Epoch 720, val loss: 0.7942166924476624
Epoch 730, training loss: 425.6422119140625 = 0.7719603180885315 + 50.0 * 8.497405052185059
Epoch 730, val loss: 0.789584755897522
Epoch 740, training loss: 425.56524658203125 = 0.7670896053314209 + 50.0 * 8.495963096618652
Epoch 740, val loss: 0.7850273251533508
Epoch 750, training loss: 425.4933776855469 = 0.7623082399368286 + 50.0 * 8.494621276855469
Epoch 750, val loss: 0.7805743217468262
Epoch 760, training loss: 425.61737060546875 = 0.7576055526733398 + 50.0 * 8.49719524383545
Epoch 760, val loss: 0.7762587070465088
Epoch 770, training loss: 425.4928894042969 = 0.752896785736084 + 50.0 * 8.494799613952637
Epoch 770, val loss: 0.7717993855476379
Epoch 780, training loss: 425.2939758300781 = 0.7483537197113037 + 50.0 * 8.490912437438965
Epoch 780, val loss: 0.7676243782043457
Epoch 790, training loss: 425.2312927246094 = 0.7439795136451721 + 50.0 * 8.48974609375
Epoch 790, val loss: 0.7636305093765259
Epoch 800, training loss: 425.2279357910156 = 0.7397223711013794 + 50.0 * 8.489764213562012
Epoch 800, val loss: 0.7597149014472961
Epoch 810, training loss: 425.10302734375 = 0.735514223575592 + 50.0 * 8.487350463867188
Epoch 810, val loss: 0.7558656930923462
Epoch 820, training loss: 425.0503234863281 = 0.7314613461494446 + 50.0 * 8.486376762390137
Epoch 820, val loss: 0.7521932721138
Epoch 830, training loss: 425.0091247558594 = 0.727532148361206 + 50.0 * 8.485631942749023
Epoch 830, val loss: 0.7486404180526733
Epoch 840, training loss: 424.99981689453125 = 0.7237072587013245 + 50.0 * 8.485522270202637
Epoch 840, val loss: 0.7451928853988647
Epoch 850, training loss: 424.886474609375 = 0.7199816107749939 + 50.0 * 8.483329772949219
Epoch 850, val loss: 0.7418735027313232
Epoch 860, training loss: 424.8333435058594 = 0.7164026498794556 + 50.0 * 8.482338905334473
Epoch 860, val loss: 0.738709032535553
Epoch 870, training loss: 424.787109375 = 0.7129452228546143 + 50.0 * 8.481483459472656
Epoch 870, val loss: 0.735654890537262
Epoch 880, training loss: 424.8215026855469 = 0.7096208930015564 + 50.0 * 8.482237815856934
Epoch 880, val loss: 0.732766330242157
Epoch 890, training loss: 424.7055358886719 = 0.7063258290290833 + 50.0 * 8.479984283447266
Epoch 890, val loss: 0.7298228144645691
Epoch 900, training loss: 424.6831970214844 = 0.703167200088501 + 50.0 * 8.47960090637207
Epoch 900, val loss: 0.7271410226821899
Epoch 910, training loss: 424.6780700683594 = 0.7001585960388184 + 50.0 * 8.479557991027832
Epoch 910, val loss: 0.7245339751243591
Epoch 920, training loss: 424.5743408203125 = 0.6972501873970032 + 50.0 * 8.47754192352295
Epoch 920, val loss: 0.7220392227172852
Epoch 930, training loss: 424.5369873046875 = 0.694458544254303 + 50.0 * 8.476850509643555
Epoch 930, val loss: 0.7196919918060303
Epoch 940, training loss: 424.496826171875 = 0.6917788982391357 + 50.0 * 8.47610092163086
Epoch 940, val loss: 0.7174336314201355
Epoch 950, training loss: 424.57208251953125 = 0.6891965270042419 + 50.0 * 8.477657318115234
Epoch 950, val loss: 0.7153358459472656
Epoch 960, training loss: 424.4606628417969 = 0.6866106390953064 + 50.0 * 8.475481033325195
Epoch 960, val loss: 0.7130769491195679
Epoch 970, training loss: 424.402099609375 = 0.6841927170753479 + 50.0 * 8.474357604980469
Epoch 970, val loss: 0.7111619710922241
Epoch 980, training loss: 424.41259765625 = 0.6818476915359497 + 50.0 * 8.474615097045898
Epoch 980, val loss: 0.7092413902282715
Epoch 990, training loss: 424.3294982910156 = 0.6795470714569092 + 50.0 * 8.472999572753906
Epoch 990, val loss: 0.7073638439178467
Epoch 1000, training loss: 424.2967834472656 = 0.6773461103439331 + 50.0 * 8.472389221191406
Epoch 1000, val loss: 0.7056146860122681
Epoch 1010, training loss: 424.2703857421875 = 0.6752289533615112 + 50.0 * 8.471902847290039
Epoch 1010, val loss: 0.703945517539978
Epoch 1020, training loss: 424.2928466796875 = 0.6731814742088318 + 50.0 * 8.472393035888672
Epoch 1020, val loss: 0.7023396492004395
Epoch 1030, training loss: 424.20025634765625 = 0.6711912155151367 + 50.0 * 8.4705810546875
Epoch 1030, val loss: 0.7007997632026672
Epoch 1040, training loss: 424.15618896484375 = 0.669278085231781 + 50.0 * 8.469738006591797
Epoch 1040, val loss: 0.6993298530578613
Epoch 1050, training loss: 424.2442932128906 = 0.6674200296401978 + 50.0 * 8.471537590026855
Epoch 1050, val loss: 0.6978889107704163
Epoch 1060, training loss: 424.1334228515625 = 0.6655513644218445 + 50.0 * 8.46935749053955
Epoch 1060, val loss: 0.6965487003326416
Epoch 1070, training loss: 424.0960998535156 = 0.6637638211250305 + 50.0 * 8.468647003173828
Epoch 1070, val loss: 0.6951799988746643
Epoch 1080, training loss: 424.0535583496094 = 0.6620770692825317 + 50.0 * 8.467829704284668
Epoch 1080, val loss: 0.693995475769043
Epoch 1090, training loss: 424.0259704589844 = 0.6604344248771667 + 50.0 * 8.467310905456543
Epoch 1090, val loss: 0.6927796602249146
Epoch 1100, training loss: 424.15228271484375 = 0.6588416695594788 + 50.0 * 8.469868659973145
Epoch 1100, val loss: 0.6917259693145752
Epoch 1110, training loss: 424.0543212890625 = 0.6571928858757019 + 50.0 * 8.467942237854004
Epoch 1110, val loss: 0.6904422640800476
Epoch 1120, training loss: 423.981689453125 = 0.6556419730186462 + 50.0 * 8.466521263122559
Epoch 1120, val loss: 0.6893990635871887
Epoch 1130, training loss: 423.95294189453125 = 0.6541479825973511 + 50.0 * 8.465975761413574
Epoch 1130, val loss: 0.688363790512085
Epoch 1140, training loss: 423.9030456542969 = 0.6527103781700134 + 50.0 * 8.465006828308105
Epoch 1140, val loss: 0.6873835921287537
Epoch 1150, training loss: 423.8910217285156 = 0.651318371295929 + 50.0 * 8.464794158935547
Epoch 1150, val loss: 0.6864694356918335
Epoch 1160, training loss: 423.9740295410156 = 0.6499360799789429 + 50.0 * 8.466482162475586
Epoch 1160, val loss: 0.6855685710906982
Epoch 1170, training loss: 423.92681884765625 = 0.6485298871994019 + 50.0 * 8.46556568145752
Epoch 1170, val loss: 0.6846176385879517
Epoch 1180, training loss: 423.83892822265625 = 0.6471608877182007 + 50.0 * 8.463835716247559
Epoch 1180, val loss: 0.6837312579154968
Epoch 1190, training loss: 423.814697265625 = 0.6458573937416077 + 50.0 * 8.463376998901367
Epoch 1190, val loss: 0.6829036474227905
Epoch 1200, training loss: 423.78973388671875 = 0.6446043252944946 + 50.0 * 8.462903022766113
Epoch 1200, val loss: 0.6821006536483765
Epoch 1210, training loss: 423.9188537597656 = 0.6433459520339966 + 50.0 * 8.465510368347168
Epoch 1210, val loss: 0.6812403798103333
Epoch 1220, training loss: 423.777587890625 = 0.6421096920967102 + 50.0 * 8.462709426879883
Epoch 1220, val loss: 0.6806240677833557
Epoch 1230, training loss: 423.72125244140625 = 0.6408985257148743 + 50.0 * 8.461606979370117
Epoch 1230, val loss: 0.6797971129417419
Epoch 1240, training loss: 423.70648193359375 = 0.6397554874420166 + 50.0 * 8.461334228515625
Epoch 1240, val loss: 0.6791566014289856
Epoch 1250, training loss: 423.9245910644531 = 0.6385909914970398 + 50.0 * 8.465720176696777
Epoch 1250, val loss: 0.6784186959266663
Epoch 1260, training loss: 423.681396484375 = 0.6373853087425232 + 50.0 * 8.460880279541016
Epoch 1260, val loss: 0.6776609420776367
Epoch 1270, training loss: 423.62176513671875 = 0.6362733840942383 + 50.0 * 8.459710121154785
Epoch 1270, val loss: 0.6770445108413696
Epoch 1280, training loss: 423.60595703125 = 0.6352112889289856 + 50.0 * 8.4594144821167
Epoch 1280, val loss: 0.6763944625854492
Epoch 1290, training loss: 423.5812072753906 = 0.634179413318634 + 50.0 * 8.458940505981445
Epoch 1290, val loss: 0.6758508086204529
Epoch 1300, training loss: 423.6065979003906 = 0.6331523060798645 + 50.0 * 8.459468841552734
Epoch 1300, val loss: 0.6752603054046631
Epoch 1310, training loss: 423.5592346191406 = 0.6320825815200806 + 50.0 * 8.458542823791504
Epoch 1310, val loss: 0.6746687889099121
Epoch 1320, training loss: 423.6657409667969 = 0.6310152411460876 + 50.0 * 8.460694313049316
Epoch 1320, val loss: 0.6740379929542542
Epoch 1330, training loss: 423.5121765136719 = 0.6299373507499695 + 50.0 * 8.45764446258545
Epoch 1330, val loss: 0.6733569502830505
Epoch 1340, training loss: 423.4771728515625 = 0.6289451122283936 + 50.0 * 8.456964492797852
Epoch 1340, val loss: 0.6728379130363464
Epoch 1350, training loss: 423.4465026855469 = 0.6279878616333008 + 50.0 * 8.45637035369873
Epoch 1350, val loss: 0.6722908616065979
Epoch 1360, training loss: 423.4417419433594 = 0.6270471811294556 + 50.0 * 8.456294059753418
Epoch 1360, val loss: 0.6717411279678345
Epoch 1370, training loss: 423.6437683105469 = 0.6260543465614319 + 50.0 * 8.46035385131836
Epoch 1370, val loss: 0.6711278557777405
Epoch 1380, training loss: 423.3888244628906 = 0.6250509023666382 + 50.0 * 8.455275535583496
Epoch 1380, val loss: 0.6706193089485168
Epoch 1390, training loss: 423.3668518066406 = 0.6241061687469482 + 50.0 * 8.454854965209961
Epoch 1390, val loss: 0.6701700687408447
Epoch 1400, training loss: 423.3372497558594 = 0.6232010722160339 + 50.0 * 8.454280853271484
Epoch 1400, val loss: 0.6696030497550964
Epoch 1410, training loss: 423.31011962890625 = 0.6223297715187073 + 50.0 * 8.453755378723145
Epoch 1410, val loss: 0.6691833734512329
Epoch 1420, training loss: 423.5874938964844 = 0.6214284896850586 + 50.0 * 8.459321022033691
Epoch 1420, val loss: 0.6686353087425232
Epoch 1430, training loss: 423.3929138183594 = 0.6204378008842468 + 50.0 * 8.455449104309082
Epoch 1430, val loss: 0.6680949330329895
Epoch 1440, training loss: 423.2396545410156 = 0.6195105910301208 + 50.0 * 8.45240306854248
Epoch 1440, val loss: 0.667532205581665
Epoch 1450, training loss: 423.2666931152344 = 0.6186549663543701 + 50.0 * 8.452960968017578
Epoch 1450, val loss: 0.6671000719070435
Epoch 1460, training loss: 423.2516174316406 = 0.6177517175674438 + 50.0 * 8.452677726745605
Epoch 1460, val loss: 0.6665435433387756
Epoch 1470, training loss: 423.17510986328125 = 0.6168608665466309 + 50.0 * 8.451165199279785
Epoch 1470, val loss: 0.6660578846931458
Epoch 1480, training loss: 423.1425476074219 = 0.6160097122192383 + 50.0 * 8.450531005859375
Epoch 1480, val loss: 0.6655422449111938
Epoch 1490, training loss: 423.1151123046875 = 0.615182638168335 + 50.0 * 8.44999885559082
Epoch 1490, val loss: 0.6651284694671631
Epoch 1500, training loss: 423.0916442871094 = 0.6143564581871033 + 50.0 * 8.449545860290527
Epoch 1500, val loss: 0.6646531224250793
Epoch 1510, training loss: 423.0772705078125 = 0.6135221719741821 + 50.0 * 8.449275016784668
Epoch 1510, val loss: 0.6641778349876404
Epoch 1520, training loss: 423.32061767578125 = 0.6126329302787781 + 50.0 * 8.4541597366333
Epoch 1520, val loss: 0.6636385917663574
Epoch 1530, training loss: 423.1004943847656 = 0.6116684675216675 + 50.0 * 8.449776649475098
Epoch 1530, val loss: 0.6630610823631287
Epoch 1540, training loss: 423.01959228515625 = 0.6107802391052246 + 50.0 * 8.448176383972168
Epoch 1540, val loss: 0.6624979972839355
Epoch 1550, training loss: 422.9820556640625 = 0.6099482178688049 + 50.0 * 8.447442054748535
Epoch 1550, val loss: 0.6620050072669983
Epoch 1560, training loss: 423.10675048828125 = 0.6091123819351196 + 50.0 * 8.449953079223633
Epoch 1560, val loss: 0.6614460349082947
Epoch 1570, training loss: 422.998779296875 = 0.6081830859184265 + 50.0 * 8.4478120803833
Epoch 1570, val loss: 0.6610116362571716
Epoch 1580, training loss: 422.97003173828125 = 0.6072516441345215 + 50.0 * 8.447256088256836
Epoch 1580, val loss: 0.6603248715400696
Epoch 1590, training loss: 422.9049072265625 = 0.6064038276672363 + 50.0 * 8.445969581604004
Epoch 1590, val loss: 0.6598210334777832
Epoch 1600, training loss: 422.8885192871094 = 0.60557621717453 + 50.0 * 8.445658683776855
Epoch 1600, val loss: 0.6593299508094788
Epoch 1610, training loss: 423.06365966796875 = 0.6047256588935852 + 50.0 * 8.449178695678711
Epoch 1610, val loss: 0.6587470173835754
Epoch 1620, training loss: 422.890625 = 0.6037984490394592 + 50.0 * 8.4457368850708
Epoch 1620, val loss: 0.6581834554672241
Epoch 1630, training loss: 422.8354797363281 = 0.6029125452041626 + 50.0 * 8.44465160369873
Epoch 1630, val loss: 0.6575755476951599
Epoch 1640, training loss: 422.79766845703125 = 0.6020514965057373 + 50.0 * 8.443912506103516
Epoch 1640, val loss: 0.6570290327072144
Epoch 1650, training loss: 422.77435302734375 = 0.6011923551559448 + 50.0 * 8.443463325500488
Epoch 1650, val loss: 0.6564337015151978
Epoch 1660, training loss: 423.0833435058594 = 0.600294291973114 + 50.0 * 8.449661254882812
Epoch 1660, val loss: 0.6557099223136902
Epoch 1670, training loss: 422.8079833984375 = 0.5993199944496155 + 50.0 * 8.444172859191895
Epoch 1670, val loss: 0.6552127599716187
Epoch 1680, training loss: 422.7210693359375 = 0.598381757736206 + 50.0 * 8.442453384399414
Epoch 1680, val loss: 0.6545907258987427
Epoch 1690, training loss: 422.70428466796875 = 0.5974944233894348 + 50.0 * 8.44213581085205
Epoch 1690, val loss: 0.6539332866668701
Epoch 1700, training loss: 422.68499755859375 = 0.5966252088546753 + 50.0 * 8.441767692565918
Epoch 1700, val loss: 0.653346598148346
Epoch 1710, training loss: 422.99578857421875 = 0.595710039138794 + 50.0 * 8.448001861572266
Epoch 1710, val loss: 0.6526678800582886
Epoch 1720, training loss: 422.747802734375 = 0.5947208404541016 + 50.0 * 8.443061828613281
Epoch 1720, val loss: 0.6520318984985352
Epoch 1730, training loss: 422.6382141113281 = 0.5937747955322266 + 50.0 * 8.440888404846191
Epoch 1730, val loss: 0.6513075828552246
Epoch 1740, training loss: 422.60198974609375 = 0.5928803086280823 + 50.0 * 8.440181732177734
Epoch 1740, val loss: 0.6507180333137512
Epoch 1750, training loss: 422.5850524902344 = 0.5919854044914246 + 50.0 * 8.439861297607422
Epoch 1750, val loss: 0.6500473618507385
Epoch 1760, training loss: 422.78521728515625 = 0.5910792946815491 + 50.0 * 8.443882942199707
Epoch 1760, val loss: 0.6493573188781738
Epoch 1770, training loss: 422.6815185546875 = 0.5900357365608215 + 50.0 * 8.441829681396484
Epoch 1770, val loss: 0.6486431956291199
Epoch 1780, training loss: 422.55926513671875 = 0.5890380144119263 + 50.0 * 8.439404487609863
Epoch 1780, val loss: 0.6478484272956848
Epoch 1790, training loss: 422.5191345214844 = 0.5880928635597229 + 50.0 * 8.438620567321777
Epoch 1790, val loss: 0.6471505165100098
Epoch 1800, training loss: 422.5041198730469 = 0.5871699452400208 + 50.0 * 8.438339233398438
Epoch 1800, val loss: 0.646504819393158
Epoch 1810, training loss: 422.48199462890625 = 0.5862396955490112 + 50.0 * 8.437914848327637
Epoch 1810, val loss: 0.6458061933517456
Epoch 1820, training loss: 422.50360107421875 = 0.5852882266044617 + 50.0 * 8.438365936279297
Epoch 1820, val loss: 0.64506596326828
Epoch 1830, training loss: 422.615234375 = 0.5842519998550415 + 50.0 * 8.440619468688965
Epoch 1830, val loss: 0.6442112326622009
Epoch 1840, training loss: 422.4567565917969 = 0.5831855535507202 + 50.0 * 8.437471389770508
Epoch 1840, val loss: 0.6434822082519531
Epoch 1850, training loss: 422.4398193359375 = 0.5821743011474609 + 50.0 * 8.437152862548828
Epoch 1850, val loss: 0.6427205204963684
Epoch 1860, training loss: 422.40216064453125 = 0.5811887383460999 + 50.0 * 8.436419486999512
Epoch 1860, val loss: 0.6419572234153748
Epoch 1870, training loss: 422.38995361328125 = 0.5802114009857178 + 50.0 * 8.436195373535156
Epoch 1870, val loss: 0.6412225365638733
Epoch 1880, training loss: 422.4347229003906 = 0.5792190432548523 + 50.0 * 8.43710994720459
Epoch 1880, val loss: 0.6405094265937805
Epoch 1890, training loss: 422.4604797363281 = 0.5781323909759521 + 50.0 * 8.437646865844727
Epoch 1890, val loss: 0.6396098136901855
Epoch 1900, training loss: 422.3590087890625 = 0.5770239233970642 + 50.0 * 8.435639381408691
Epoch 1900, val loss: 0.6387222409248352
Epoch 1910, training loss: 422.3370666503906 = 0.5759778618812561 + 50.0 * 8.435221672058105
Epoch 1910, val loss: 0.6378973722457886
Epoch 1920, training loss: 422.3184509277344 = 0.5749616622924805 + 50.0 * 8.434869766235352
Epoch 1920, val loss: 0.637139081954956
Epoch 1930, training loss: 422.3425598144531 = 0.573937714099884 + 50.0 * 8.435372352600098
Epoch 1930, val loss: 0.6363683342933655
Epoch 1940, training loss: 422.51165771484375 = 0.572839617729187 + 50.0 * 8.438776016235352
Epoch 1940, val loss: 0.6355165243148804
Epoch 1950, training loss: 422.36065673828125 = 0.5716380476951599 + 50.0 * 8.43578052520752
Epoch 1950, val loss: 0.6344388723373413
Epoch 1960, training loss: 422.2582092285156 = 0.5705485939979553 + 50.0 * 8.43375301361084
Epoch 1960, val loss: 0.6335744261741638
Epoch 1970, training loss: 422.245849609375 = 0.5694958567619324 + 50.0 * 8.433526992797852
Epoch 1970, val loss: 0.6327569484710693
Epoch 1980, training loss: 422.2272033691406 = 0.5684516429901123 + 50.0 * 8.433175086975098
Epoch 1980, val loss: 0.6319507360458374
Epoch 1990, training loss: 422.417724609375 = 0.5673806667327881 + 50.0 * 8.437006950378418
Epoch 1990, val loss: 0.6311572790145874
Epoch 2000, training loss: 422.2816467285156 = 0.5661752820014954 + 50.0 * 8.434309959411621
Epoch 2000, val loss: 0.6300044655799866
Epoch 2010, training loss: 422.2317199707031 = 0.5650173425674438 + 50.0 * 8.433334350585938
Epoch 2010, val loss: 0.6291720271110535
Epoch 2020, training loss: 422.16949462890625 = 0.5639030933380127 + 50.0 * 8.432111740112305
Epoch 2020, val loss: 0.6282549500465393
Epoch 2030, training loss: 422.153076171875 = 0.5628050565719604 + 50.0 * 8.431805610656738
Epoch 2030, val loss: 0.6273587346076965
Epoch 2040, training loss: 422.14642333984375 = 0.5616951584815979 + 50.0 * 8.431694030761719
Epoch 2040, val loss: 0.6264998912811279
Epoch 2050, training loss: 422.616943359375 = 0.5605367422103882 + 50.0 * 8.44112777709961
Epoch 2050, val loss: 0.6255196332931519
Epoch 2060, training loss: 422.22833251953125 = 0.5592336654663086 + 50.0 * 8.433382034301758
Epoch 2060, val loss: 0.6244761347770691
Epoch 2070, training loss: 422.11578369140625 = 0.558021068572998 + 50.0 * 8.43115520477295
Epoch 2070, val loss: 0.6235118508338928
Epoch 2080, training loss: 422.0996398925781 = 0.5568665266036987 + 50.0 * 8.430855751037598
Epoch 2080, val loss: 0.622625470161438
Epoch 2090, training loss: 422.06890869140625 = 0.5557251572608948 + 50.0 * 8.43026351928711
Epoch 2090, val loss: 0.6217392086982727
Epoch 2100, training loss: 422.05682373046875 = 0.5545682311058044 + 50.0 * 8.430045127868652
Epoch 2100, val loss: 0.6208416223526001
Epoch 2110, training loss: 422.0489196777344 = 0.553389847278595 + 50.0 * 8.429910659790039
Epoch 2110, val loss: 0.6199546456336975
Epoch 2120, training loss: 422.24591064453125 = 0.5522004961967468 + 50.0 * 8.433874130249023
Epoch 2120, val loss: 0.6191999316215515
Epoch 2130, training loss: 422.1233825683594 = 0.55083829164505 + 50.0 * 8.431450843811035
Epoch 2130, val loss: 0.6177639365196228
Epoch 2140, training loss: 422.0509948730469 = 0.5495752096176147 + 50.0 * 8.430028915405273
Epoch 2140, val loss: 0.6169540286064148
Epoch 2150, training loss: 421.99560546875 = 0.5483294129371643 + 50.0 * 8.428945541381836
Epoch 2150, val loss: 0.6159362196922302
Epoch 2160, training loss: 421.97723388671875 = 0.547113299369812 + 50.0 * 8.42860221862793
Epoch 2160, val loss: 0.615009069442749
Epoch 2170, training loss: 421.99676513671875 = 0.545882523059845 + 50.0 * 8.429018020629883
Epoch 2170, val loss: 0.6140835881233215
Epoch 2180, training loss: 422.2339782714844 = 0.5445656776428223 + 50.0 * 8.433788299560547
Epoch 2180, val loss: 0.6130158305168152
Epoch 2190, training loss: 421.9640808105469 = 0.5431828498840332 + 50.0 * 8.428418159484863
Epoch 2190, val loss: 0.6119625568389893
Epoch 2200, training loss: 421.95281982421875 = 0.5418742299079895 + 50.0 * 8.428218841552734
Epoch 2200, val loss: 0.6109268069267273
Epoch 2210, training loss: 421.92877197265625 = 0.5406189560890198 + 50.0 * 8.427762985229492
Epoch 2210, val loss: 0.6099913120269775
Epoch 2220, training loss: 421.906494140625 = 0.5393636226654053 + 50.0 * 8.427342414855957
Epoch 2220, val loss: 0.6090431809425354
Epoch 2230, training loss: 421.9423522949219 = 0.538094162940979 + 50.0 * 8.428085327148438
Epoch 2230, val loss: 0.6080962419509888
Epoch 2240, training loss: 422.0147705078125 = 0.5367453098297119 + 50.0 * 8.429560661315918
Epoch 2240, val loss: 0.6070592403411865
Epoch 2250, training loss: 421.90655517578125 = 0.535365104675293 + 50.0 * 8.427423477172852
Epoch 2250, val loss: 0.6059191823005676
Epoch 2260, training loss: 421.8641662597656 = 0.5340487957000732 + 50.0 * 8.426602363586426
Epoch 2260, val loss: 0.6049706935882568
Epoch 2270, training loss: 421.84552001953125 = 0.5327470898628235 + 50.0 * 8.426255226135254
Epoch 2270, val loss: 0.6039496660232544
Epoch 2280, training loss: 421.8436279296875 = 0.5314440727233887 + 50.0 * 8.426243782043457
Epoch 2280, val loss: 0.6029319763183594
Epoch 2290, training loss: 422.0901184082031 = 0.5301116704940796 + 50.0 * 8.43120002746582
Epoch 2290, val loss: 0.601766049861908
Epoch 2300, training loss: 421.9137268066406 = 0.528691828250885 + 50.0 * 8.427700996398926
Epoch 2300, val loss: 0.6010046601295471
Epoch 2310, training loss: 421.8323974609375 = 0.5272980332374573 + 50.0 * 8.426101684570312
Epoch 2310, val loss: 0.5997588634490967
Epoch 2320, training loss: 421.79095458984375 = 0.5259675979614258 + 50.0 * 8.425299644470215
Epoch 2320, val loss: 0.5988548994064331
Epoch 2330, training loss: 421.8094787597656 = 0.5246433019638062 + 50.0 * 8.42569637298584
Epoch 2330, val loss: 0.597837507724762
Epoch 2340, training loss: 421.96942138671875 = 0.5232636332511902 + 50.0 * 8.428923606872559
Epoch 2340, val loss: 0.5967586636543274
Epoch 2350, training loss: 421.80853271484375 = 0.5218380093574524 + 50.0 * 8.42573356628418
Epoch 2350, val loss: 0.5957406163215637
Epoch 2360, training loss: 421.7588195800781 = 0.5204698443412781 + 50.0 * 8.424766540527344
Epoch 2360, val loss: 0.5947638750076294
Epoch 2370, training loss: 421.7322998046875 = 0.5191176533699036 + 50.0 * 8.424263954162598
Epoch 2370, val loss: 0.5937758088111877
Epoch 2380, training loss: 421.7230529785156 = 0.5177695155143738 + 50.0 * 8.424105644226074
Epoch 2380, val loss: 0.5927672982215881
Epoch 2390, training loss: 421.72589111328125 = 0.5164153575897217 + 50.0 * 8.424189567565918
Epoch 2390, val loss: 0.5918322205543518
Epoch 2400, training loss: 421.9908752441406 = 0.5150315761566162 + 50.0 * 8.429516792297363
Epoch 2400, val loss: 0.5909139513969421
Epoch 2410, training loss: 421.8126525878906 = 0.5135104060173035 + 50.0 * 8.425982475280762
Epoch 2410, val loss: 0.5895717740058899
Epoch 2420, training loss: 421.7137756347656 = 0.5120552182197571 + 50.0 * 8.424034118652344
Epoch 2420, val loss: 0.5884604454040527
Epoch 2430, training loss: 421.700439453125 = 0.5106587409973145 + 50.0 * 8.423795700073242
Epoch 2430, val loss: 0.5876086354255676
Epoch 2440, training loss: 421.6549987792969 = 0.509281575679779 + 50.0 * 8.422914505004883
Epoch 2440, val loss: 0.5866169333457947
Epoch 2450, training loss: 421.66015625 = 0.5079153776168823 + 50.0 * 8.42304515838623
Epoch 2450, val loss: 0.5856465101242065
Epoch 2460, training loss: 421.82452392578125 = 0.5065297484397888 + 50.0 * 8.426360130310059
Epoch 2460, val loss: 0.5847451686859131
Epoch 2470, training loss: 421.6546325683594 = 0.5050326585769653 + 50.0 * 8.422991752624512
Epoch 2470, val loss: 0.5834894776344299
Epoch 2480, training loss: 421.61962890625 = 0.5035935640335083 + 50.0 * 8.422320365905762
Epoch 2480, val loss: 0.5826078057289124
Epoch 2490, training loss: 421.6070556640625 = 0.5021889209747314 + 50.0 * 8.422097206115723
Epoch 2490, val loss: 0.5816173553466797
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7671232876712328
0.8143881764833732
=== training gcn model ===
Epoch 0, training loss: 530.2244262695312 = 1.1101012229919434 + 50.0 * 10.58228588104248
Epoch 0, val loss: 1.1089264154434204
Epoch 10, training loss: 530.2046508789062 = 1.1053704023361206 + 50.0 * 10.581986427307129
Epoch 10, val loss: 1.104223370552063
Epoch 20, training loss: 530.1387939453125 = 1.1002305746078491 + 50.0 * 10.580771446228027
Epoch 20, val loss: 1.0990960597991943
Epoch 30, training loss: 529.859375 = 1.0944559574127197 + 50.0 * 10.575297355651855
Epoch 30, val loss: 1.0933293104171753
Epoch 40, training loss: 528.6920166015625 = 1.0879768133163452 + 50.0 * 10.552081108093262
Epoch 40, val loss: 1.086855173110962
Epoch 50, training loss: 524.49267578125 = 1.0807907581329346 + 50.0 * 10.46823787689209
Epoch 50, val loss: 1.079752802848816
Epoch 60, training loss: 510.8763427734375 = 1.0745196342468262 + 50.0 * 10.196036338806152
Epoch 60, val loss: 1.073623776435852
Epoch 70, training loss: 490.2187194824219 = 1.0685703754425049 + 50.0 * 9.783002853393555
Epoch 70, val loss: 1.0678426027297974
Epoch 80, training loss: 478.3878479003906 = 1.0643103122711182 + 50.0 * 9.546470642089844
Epoch 80, val loss: 1.0638312101364136
Epoch 90, training loss: 470.2285461425781 = 1.060742735862732 + 50.0 * 9.383356094360352
Epoch 90, val loss: 1.0604376792907715
Epoch 100, training loss: 464.8609313964844 = 1.0573136806488037 + 50.0 * 9.27607250213623
Epoch 100, val loss: 1.0571300983428955
Epoch 110, training loss: 461.2709655761719 = 1.0540493726730347 + 50.0 * 9.204338073730469
Epoch 110, val loss: 1.053991436958313
Epoch 120, training loss: 456.56671142578125 = 1.0515927076339722 + 50.0 * 9.110301971435547
Epoch 120, val loss: 1.0516432523727417
Epoch 130, training loss: 451.4186706542969 = 1.0497888326644897 + 50.0 * 9.007377624511719
Epoch 130, val loss: 1.0498583316802979
Epoch 140, training loss: 448.6958312988281 = 1.0481301546096802 + 50.0 * 8.952954292297363
Epoch 140, val loss: 1.048153281211853
Epoch 150, training loss: 447.1374206542969 = 1.0461018085479736 + 50.0 * 8.921826362609863
Epoch 150, val loss: 1.0460429191589355
Epoch 160, training loss: 445.34527587890625 = 1.0438261032104492 + 50.0 * 8.886029243469238
Epoch 160, val loss: 1.0437427759170532
Epoch 170, training loss: 443.680908203125 = 1.0416799783706665 + 50.0 * 8.852784156799316
Epoch 170, val loss: 1.041635274887085
Epoch 180, training loss: 442.6178283691406 = 1.0396521091461182 + 50.0 * 8.831563949584961
Epoch 180, val loss: 1.0396648645401
Epoch 190, training loss: 441.9085998535156 = 1.0375869274139404 + 50.0 * 8.81742000579834
Epoch 190, val loss: 1.0376496315002441
Epoch 200, training loss: 441.1608581542969 = 1.035417079925537 + 50.0 * 8.802508354187012
Epoch 200, val loss: 1.0355396270751953
Epoch 210, training loss: 440.343505859375 = 1.033241868019104 + 50.0 * 8.786205291748047
Epoch 210, val loss: 1.0334159135818481
Epoch 220, training loss: 439.3501281738281 = 1.031089425086975 + 50.0 * 8.766380310058594
Epoch 220, val loss: 1.0313222408294678
Epoch 230, training loss: 438.20904541015625 = 1.028950810432434 + 50.0 * 8.74360179901123
Epoch 230, val loss: 1.0292446613311768
Epoch 240, training loss: 437.082275390625 = 1.0267746448516846 + 50.0 * 8.721110343933105
Epoch 240, val loss: 1.0271140336990356
Epoch 250, training loss: 436.1175231933594 = 1.0244160890579224 + 50.0 * 8.701862335205078
Epoch 250, val loss: 1.0248003005981445
Epoch 260, training loss: 435.2591552734375 = 1.0218110084533691 + 50.0 * 8.684746742248535
Epoch 260, val loss: 1.0222270488739014
Epoch 270, training loss: 434.6405029296875 = 1.0189225673675537 + 50.0 * 8.672431945800781
Epoch 270, val loss: 1.0193884372711182
Epoch 280, training loss: 433.9695739746094 = 1.0157746076583862 + 50.0 * 8.659075736999512
Epoch 280, val loss: 1.0162594318389893
Epoch 290, training loss: 433.3902893066406 = 1.0124061107635498 + 50.0 * 8.647557258605957
Epoch 290, val loss: 1.012939214706421
Epoch 300, training loss: 432.9320373535156 = 1.0088430643081665 + 50.0 * 8.638463973999023
Epoch 300, val loss: 1.0094326734542847
Epoch 310, training loss: 432.6009521484375 = 1.0050560235977173 + 50.0 * 8.631917953491211
Epoch 310, val loss: 1.005723237991333
Epoch 320, training loss: 432.2562561035156 = 1.0010219812393188 + 50.0 * 8.625104904174805
Epoch 320, val loss: 1.0017855167388916
Epoch 330, training loss: 431.9635009765625 = 0.9967331886291504 + 50.0 * 8.619335174560547
Epoch 330, val loss: 0.9976102113723755
Epoch 340, training loss: 431.69573974609375 = 0.9921886920928955 + 50.0 * 8.614070892333984
Epoch 340, val loss: 0.9932025074958801
Epoch 350, training loss: 431.4817810058594 = 0.9873733520507812 + 50.0 * 8.609888076782227
Epoch 350, val loss: 0.9885414242744446
Epoch 360, training loss: 431.20709228515625 = 0.9823048114776611 + 50.0 * 8.604496002197266
Epoch 360, val loss: 0.9836285710334778
Epoch 370, training loss: 430.9543151855469 = 0.9769958853721619 + 50.0 * 8.599546432495117
Epoch 370, val loss: 0.9785032868385315
Epoch 380, training loss: 430.7216796875 = 0.971439003944397 + 50.0 * 8.59500503540039
Epoch 380, val loss: 0.973135769367218
Epoch 390, training loss: 430.56964111328125 = 0.9655669927597046 + 50.0 * 8.592081069946289
Epoch 390, val loss: 0.9674974083900452
Epoch 400, training loss: 430.2757873535156 = 0.9594366550445557 + 50.0 * 8.586326599121094
Epoch 400, val loss: 0.9616032838821411
Epoch 410, training loss: 430.03607177734375 = 0.9530778527259827 + 50.0 * 8.581660270690918
Epoch 410, val loss: 0.9554982781410217
Epoch 420, training loss: 429.84771728515625 = 0.9464684128761292 + 50.0 * 8.578024864196777
Epoch 420, val loss: 0.9491612315177917
Epoch 430, training loss: 429.7029724121094 = 0.9395848512649536 + 50.0 * 8.575267791748047
Epoch 430, val loss: 0.9425699710845947
Epoch 440, training loss: 429.4823303222656 = 0.9324586391448975 + 50.0 * 8.57099723815918
Epoch 440, val loss: 0.9357834458351135
Epoch 450, training loss: 429.2987365722656 = 0.9251558780670166 + 50.0 * 8.567471504211426
Epoch 450, val loss: 0.9288294911384583
Epoch 460, training loss: 429.13531494140625 = 0.9176729321479797 + 50.0 * 8.564352989196777
Epoch 460, val loss: 0.9217168688774109
Epoch 470, training loss: 429.0063781738281 = 0.9100016355514526 + 50.0 * 8.561927795410156
Epoch 470, val loss: 0.9144397974014282
Epoch 480, training loss: 428.78521728515625 = 0.9021744132041931 + 50.0 * 8.557661056518555
Epoch 480, val loss: 0.9070272445678711
Epoch 490, training loss: 428.6366271972656 = 0.894254744052887 + 50.0 * 8.554847717285156
Epoch 490, val loss: 0.8995465636253357
Epoch 500, training loss: 428.6689453125 = 0.8862330317497253 + 50.0 * 8.555654525756836
Epoch 500, val loss: 0.8919655680656433
Epoch 510, training loss: 428.3488464355469 = 0.8781192898750305 + 50.0 * 8.54941463470459
Epoch 510, val loss: 0.8843294382095337
Epoch 520, training loss: 428.1525573730469 = 0.8699806928634644 + 50.0 * 8.54565143585205
Epoch 520, val loss: 0.876696765422821
Epoch 530, training loss: 428.0252380371094 = 0.8618313074111938 + 50.0 * 8.543268203735352
Epoch 530, val loss: 0.8690586686134338
Epoch 540, training loss: 427.9432678222656 = 0.8536475300788879 + 50.0 * 8.541792869567871
Epoch 540, val loss: 0.8614081144332886
Epoch 550, training loss: 427.7341613769531 = 0.8454737663269043 + 50.0 * 8.537774085998535
Epoch 550, val loss: 0.8538095355033875
Epoch 560, training loss: 427.5552673339844 = 0.8373714685440063 + 50.0 * 8.534358024597168
Epoch 560, val loss: 0.846291184425354
Epoch 570, training loss: 427.5267639160156 = 0.8293076157569885 + 50.0 * 8.53394889831543
Epoch 570, val loss: 0.8388386368751526
Epoch 580, training loss: 427.4223937988281 = 0.8212671279907227 + 50.0 * 8.532022476196289
Epoch 580, val loss: 0.8313739895820618
Epoch 590, training loss: 427.17156982421875 = 0.8132883906364441 + 50.0 * 8.527165412902832
Epoch 590, val loss: 0.8240392208099365
Epoch 600, training loss: 427.0111389160156 = 0.8054643273353577 + 50.0 * 8.524113655090332
Epoch 600, val loss: 0.8168579339981079
Epoch 610, training loss: 426.876220703125 = 0.7977473139762878 + 50.0 * 8.52156925201416
Epoch 610, val loss: 0.8098092675209045
Epoch 620, training loss: 426.77789306640625 = 0.790163516998291 + 50.0 * 8.519754409790039
Epoch 620, val loss: 0.8029248714447021
Epoch 630, training loss: 426.698974609375 = 0.7826811075210571 + 50.0 * 8.518325805664062
Epoch 630, val loss: 0.7961632013320923
Epoch 640, training loss: 426.54010009765625 = 0.7753584980964661 + 50.0 * 8.515295028686523
Epoch 640, val loss: 0.7895802855491638
Epoch 650, training loss: 426.447998046875 = 0.7682263851165771 + 50.0 * 8.513595581054688
Epoch 650, val loss: 0.7831959128379822
Epoch 660, training loss: 426.55084228515625 = 0.761259913444519 + 50.0 * 8.515791893005371
Epoch 660, val loss: 0.7769789099693298
Epoch 670, training loss: 426.28668212890625 = 0.7544365525245667 + 50.0 * 8.510644912719727
Epoch 670, val loss: 0.7709631323814392
Epoch 680, training loss: 426.1615905761719 = 0.7478418946266174 + 50.0 * 8.508275032043457
Epoch 680, val loss: 0.7651851177215576
Epoch 690, training loss: 426.0763244628906 = 0.7414652109146118 + 50.0 * 8.506697654724121
Epoch 690, val loss: 0.7596355676651001
Epoch 700, training loss: 425.99188232421875 = 0.7352864146232605 + 50.0 * 8.505131721496582
Epoch 700, val loss: 0.7542983293533325
Epoch 710, training loss: 426.2093505859375 = 0.7292718887329102 + 50.0 * 8.509601593017578
Epoch 710, val loss: 0.7491127252578735
Epoch 720, training loss: 425.9267578125 = 0.7233906984329224 + 50.0 * 8.504067420959473
Epoch 720, val loss: 0.7441359162330627
Epoch 730, training loss: 425.7894592285156 = 0.7177538275718689 + 50.0 * 8.501434326171875
Epoch 730, val loss: 0.7393662333488464
Epoch 740, training loss: 425.6958312988281 = 0.7123322486877441 + 50.0 * 8.499670028686523
Epoch 740, val loss: 0.7348230481147766
Epoch 750, training loss: 425.6180725097656 = 0.7071058750152588 + 50.0 * 8.49821949005127
Epoch 750, val loss: 0.7305101752281189
Epoch 760, training loss: 425.5542907714844 = 0.702063798904419 + 50.0 * 8.497044563293457
Epoch 760, val loss: 0.7263848185539246
Epoch 770, training loss: 425.84271240234375 = 0.6971776485443115 + 50.0 * 8.502910614013672
Epoch 770, val loss: 0.7224043607711792
Epoch 780, training loss: 425.449462890625 = 0.6924038529396057 + 50.0 * 8.49514102935791
Epoch 780, val loss: 0.718580424785614
Epoch 790, training loss: 425.38482666015625 = 0.6878677010536194 + 50.0 * 8.493939399719238
Epoch 790, val loss: 0.7149670124053955
Epoch 800, training loss: 425.3045349121094 = 0.6835305094718933 + 50.0 * 8.492420196533203
Epoch 800, val loss: 0.711558997631073
Epoch 810, training loss: 425.2481994628906 = 0.6793601512908936 + 50.0 * 8.491376876831055
Epoch 810, val loss: 0.7083529829978943
Epoch 820, training loss: 425.4534606933594 = 0.6753231883049011 + 50.0 * 8.495562553405762
Epoch 820, val loss: 0.7052831053733826
Epoch 830, training loss: 425.1618957519531 = 0.6713877320289612 + 50.0 * 8.4898099899292
Epoch 830, val loss: 0.702297568321228
Epoch 840, training loss: 425.07830810546875 = 0.6676505208015442 + 50.0 * 8.488212585449219
Epoch 840, val loss: 0.6994770765304565
Epoch 850, training loss: 425.0109558105469 = 0.6640937924385071 + 50.0 * 8.486937522888184
Epoch 850, val loss: 0.6968603730201721
Epoch 860, training loss: 425.031494140625 = 0.6606749296188354 + 50.0 * 8.48741626739502
Epoch 860, val loss: 0.6943838596343994
Epoch 870, training loss: 424.90521240234375 = 0.657326340675354 + 50.0 * 8.484957695007324
Epoch 870, val loss: 0.6919844746589661
Epoch 880, training loss: 424.8580322265625 = 0.654121994972229 + 50.0 * 8.484078407287598
Epoch 880, val loss: 0.689744234085083
Epoch 890, training loss: 424.7888488769531 = 0.6510509252548218 + 50.0 * 8.482755661010742
Epoch 890, val loss: 0.6876195669174194
Epoch 900, training loss: 424.7740173339844 = 0.6480976343154907 + 50.0 * 8.482518196105957
Epoch 900, val loss: 0.6855894923210144
Epoch 910, training loss: 424.9256286621094 = 0.645178496837616 + 50.0 * 8.48560905456543
Epoch 910, val loss: 0.6837083697319031
Epoch 920, training loss: 424.7166442871094 = 0.6423430442810059 + 50.0 * 8.481486320495605
Epoch 920, val loss: 0.6818044185638428
Epoch 930, training loss: 424.5907287597656 = 0.6396723389625549 + 50.0 * 8.479021072387695
Epoch 930, val loss: 0.6800255179405212
Epoch 940, training loss: 424.52545166015625 = 0.6371179223060608 + 50.0 * 8.477766990661621
Epoch 940, val loss: 0.6783761382102966
Epoch 950, training loss: 424.47979736328125 = 0.6346520185470581 + 50.0 * 8.476902961730957
Epoch 950, val loss: 0.6768118739128113
Epoch 960, training loss: 424.4353942871094 = 0.632253885269165 + 50.0 * 8.476062774658203
Epoch 960, val loss: 0.6753154993057251
Epoch 970, training loss: 424.4043884277344 = 0.6299222111701965 + 50.0 * 8.475489616394043
Epoch 970, val loss: 0.6738819479942322
Epoch 980, training loss: 424.6320495605469 = 0.627633273601532 + 50.0 * 8.480088233947754
Epoch 980, val loss: 0.6724910736083984
Epoch 990, training loss: 424.36962890625 = 0.6253509521484375 + 50.0 * 8.474885940551758
Epoch 990, val loss: 0.6710765957832336
Epoch 1000, training loss: 424.2768859863281 = 0.6231870055198669 + 50.0 * 8.473073959350586
Epoch 1000, val loss: 0.6698102951049805
Epoch 1010, training loss: 424.2406921386719 = 0.6211097836494446 + 50.0 * 8.472391128540039
Epoch 1010, val loss: 0.6686168909072876
Epoch 1020, training loss: 424.2112731933594 = 0.6190925240516663 + 50.0 * 8.471843719482422
Epoch 1020, val loss: 0.6674621105194092
Epoch 1030, training loss: 424.3844909667969 = 0.6171126961708069 + 50.0 * 8.475347518920898
Epoch 1030, val loss: 0.6663669943809509
Epoch 1040, training loss: 424.20660400390625 = 0.6151453852653503 + 50.0 * 8.471829414367676
Epoch 1040, val loss: 0.6652316451072693
Epoch 1050, training loss: 424.1028137207031 = 0.6132526397705078 + 50.0 * 8.469791412353516
Epoch 1050, val loss: 0.6641978621482849
Epoch 1060, training loss: 424.0699462890625 = 0.6114197969436646 + 50.0 * 8.469170570373535
Epoch 1060, val loss: 0.6632217168807983
Epoch 1070, training loss: 424.1752624511719 = 0.6096354126930237 + 50.0 * 8.471312522888184
Epoch 1070, val loss: 0.6622625589370728
Epoch 1080, training loss: 424.0681457519531 = 0.6078236699104309 + 50.0 * 8.469206809997559
Epoch 1080, val loss: 0.6613008975982666
Epoch 1090, training loss: 424.0088195800781 = 0.6060832142829895 + 50.0 * 8.46805477142334
Epoch 1090, val loss: 0.6603525876998901
Epoch 1100, training loss: 423.9389953613281 = 0.604414701461792 + 50.0 * 8.466691970825195
Epoch 1100, val loss: 0.6594942808151245
Epoch 1110, training loss: 423.914306640625 = 0.6027942895889282 + 50.0 * 8.466230392456055
Epoch 1110, val loss: 0.6586880087852478
Epoch 1120, training loss: 423.8852844238281 = 0.6012041568756104 + 50.0 * 8.465682029724121
Epoch 1120, val loss: 0.6578746438026428
Epoch 1130, training loss: 424.081298828125 = 0.5996260046958923 + 50.0 * 8.469633102416992
Epoch 1130, val loss: 0.6570912599563599
Epoch 1140, training loss: 423.84930419921875 = 0.5980340242385864 + 50.0 * 8.465025901794434
Epoch 1140, val loss: 0.6562709212303162
Epoch 1150, training loss: 423.8181457519531 = 0.5965062975883484 + 50.0 * 8.464432716369629
Epoch 1150, val loss: 0.655499279499054
Epoch 1160, training loss: 423.8916320800781 = 0.5950181484222412 + 50.0 * 8.46593189239502
Epoch 1160, val loss: 0.6547616124153137
Epoch 1170, training loss: 423.7825622558594 = 0.5935155153274536 + 50.0 * 8.463781356811523
Epoch 1170, val loss: 0.6540257334709167
Epoch 1180, training loss: 423.7514343261719 = 0.592062771320343 + 50.0 * 8.463187217712402
Epoch 1180, val loss: 0.6532891988754272
Epoch 1190, training loss: 423.6999206542969 = 0.5906633734703064 + 50.0 * 8.46218490600586
Epoch 1190, val loss: 0.6526334881782532
Epoch 1200, training loss: 423.6693115234375 = 0.5892899632453918 + 50.0 * 8.461600303649902
Epoch 1200, val loss: 0.6519523859024048
Epoch 1210, training loss: 423.9515075683594 = 0.5879298448562622 + 50.0 * 8.46727180480957
Epoch 1210, val loss: 0.6512235999107361
Epoch 1220, training loss: 423.6880798339844 = 0.5865272283554077 + 50.0 * 8.462031364440918
Epoch 1220, val loss: 0.6505991816520691
Epoch 1230, training loss: 423.62994384765625 = 0.5851829648017883 + 50.0 * 8.460895538330078
Epoch 1230, val loss: 0.6499358415603638
Epoch 1240, training loss: 423.5804748535156 = 0.583886444568634 + 50.0 * 8.459931373596191
Epoch 1240, val loss: 0.6492803692817688
Epoch 1250, training loss: 423.6921081542969 = 0.5826029181480408 + 50.0 * 8.462189674377441
Epoch 1250, val loss: 0.6486109495162964
Epoch 1260, training loss: 423.56787109375 = 0.5813275575637817 + 50.0 * 8.459731101989746
Epoch 1260, val loss: 0.6481315493583679
Epoch 1270, training loss: 423.55889892578125 = 0.5800713896751404 + 50.0 * 8.459576606750488
Epoch 1270, val loss: 0.647460401058197
Epoch 1280, training loss: 423.565673828125 = 0.5788379907608032 + 50.0 * 8.459736824035645
Epoch 1280, val loss: 0.6469207406044006
Epoch 1290, training loss: 423.49951171875 = 0.5776040554046631 + 50.0 * 8.4584379196167
Epoch 1290, val loss: 0.6462896466255188
Epoch 1300, training loss: 423.4400329589844 = 0.5764116644859314 + 50.0 * 8.45727252960205
Epoch 1300, val loss: 0.6457169055938721
Epoch 1310, training loss: 423.4114685058594 = 0.5752429366111755 + 50.0 * 8.456724166870117
Epoch 1310, val loss: 0.6451904773712158
Epoch 1320, training loss: 423.4136047363281 = 0.5740863084793091 + 50.0 * 8.45678997039795
Epoch 1320, val loss: 0.6446456909179688
Epoch 1330, training loss: 423.5114440917969 = 0.5729178190231323 + 50.0 * 8.458770751953125
Epoch 1330, val loss: 0.6440661549568176
Epoch 1340, training loss: 423.4838562011719 = 0.5717365741729736 + 50.0 * 8.458242416381836
Epoch 1340, val loss: 0.6434957385063171
Epoch 1350, training loss: 423.3810729980469 = 0.5705549120903015 + 50.0 * 8.456210136413574
Epoch 1350, val loss: 0.6428834795951843
Epoch 1360, training loss: 423.3277893066406 = 0.5694093704223633 + 50.0 * 8.455167770385742
Epoch 1360, val loss: 0.6423774361610413
Epoch 1370, training loss: 423.2901611328125 = 0.5682905912399292 + 50.0 * 8.454437255859375
Epoch 1370, val loss: 0.6418101787567139
Epoch 1380, training loss: 423.258056640625 = 0.5671947002410889 + 50.0 * 8.453817367553711
Epoch 1380, val loss: 0.6412842273712158
Epoch 1390, training loss: 423.2410888671875 = 0.5661024451255798 + 50.0 * 8.453499794006348
Epoch 1390, val loss: 0.6407386660575867
Epoch 1400, training loss: 423.37750244140625 = 0.5650055408477783 + 50.0 * 8.456250190734863
Epoch 1400, val loss: 0.6401709318161011
Epoch 1410, training loss: 423.3539123535156 = 0.563866138458252 + 50.0 * 8.455801010131836
Epoch 1410, val loss: 0.6396830081939697
Epoch 1420, training loss: 423.23089599609375 = 0.5627275705337524 + 50.0 * 8.453363418579102
Epoch 1420, val loss: 0.6390297412872314
Epoch 1430, training loss: 423.15771484375 = 0.5616550445556641 + 50.0 * 8.451921463012695
Epoch 1430, val loss: 0.6384827494621277
Epoch 1440, training loss: 423.13946533203125 = 0.5606053471565247 + 50.0 * 8.451577186584473
Epoch 1440, val loss: 0.638008177280426
Epoch 1450, training loss: 423.1122131347656 = 0.5595515966415405 + 50.0 * 8.451053619384766
Epoch 1450, val loss: 0.6374621987342834
Epoch 1460, training loss: 423.3948059082031 = 0.5584966540336609 + 50.0 * 8.45672607421875
Epoch 1460, val loss: 0.63694828748703
Epoch 1470, training loss: 423.2128601074219 = 0.5573782920837402 + 50.0 * 8.453109741210938
Epoch 1470, val loss: 0.6363272666931152
Epoch 1480, training loss: 423.06903076171875 = 0.556297242641449 + 50.0 * 8.450254440307617
Epoch 1480, val loss: 0.6357839107513428
Epoch 1490, training loss: 423.0305480957031 = 0.5552555322647095 + 50.0 * 8.449505805969238
Epoch 1490, val loss: 0.6352616548538208
Epoch 1500, training loss: 423.0216064453125 = 0.5542312860488892 + 50.0 * 8.449347496032715
Epoch 1500, val loss: 0.6347814798355103
Epoch 1510, training loss: 423.32415771484375 = 0.5531986951828003 + 50.0 * 8.455419540405273
Epoch 1510, val loss: 0.6342788338661194
Epoch 1520, training loss: 423.06915283203125 = 0.552099883556366 + 50.0 * 8.45034122467041
Epoch 1520, val loss: 0.6336944699287415
Epoch 1530, training loss: 422.9493713378906 = 0.551057755947113 + 50.0 * 8.447966575622559
Epoch 1530, val loss: 0.6332079768180847
Epoch 1540, training loss: 422.9291687011719 = 0.5500325560569763 + 50.0 * 8.447583198547363
Epoch 1540, val loss: 0.632668673992157
Epoch 1550, training loss: 422.93890380859375 = 0.5490190982818604 + 50.0 * 8.447797775268555
Epoch 1550, val loss: 0.632210910320282
Epoch 1560, training loss: 422.9988098144531 = 0.5479828119277954 + 50.0 * 8.449016571044922
Epoch 1560, val loss: 0.631659984588623
Epoch 1570, training loss: 422.89459228515625 = 0.5469258427619934 + 50.0 * 8.446952819824219
Epoch 1570, val loss: 0.6311171054840088
Epoch 1580, training loss: 422.8638916015625 = 0.5459012389183044 + 50.0 * 8.446359634399414
Epoch 1580, val loss: 0.6305871605873108
Epoch 1590, training loss: 422.939208984375 = 0.5448805689811707 + 50.0 * 8.44788646697998
Epoch 1590, val loss: 0.630075216293335
Epoch 1600, training loss: 422.8163146972656 = 0.5438367128372192 + 50.0 * 8.445449829101562
Epoch 1600, val loss: 0.629510760307312
Epoch 1610, training loss: 422.8031005859375 = 0.542811930179596 + 50.0 * 8.445205688476562
Epoch 1610, val loss: 0.6289774179458618
Epoch 1620, training loss: 422.78765869140625 = 0.5417968034744263 + 50.0 * 8.444916725158691
Epoch 1620, val loss: 0.6284547448158264
Epoch 1630, training loss: 422.8806457519531 = 0.5407800078392029 + 50.0 * 8.446797370910645
Epoch 1630, val loss: 0.6278852820396423
Epoch 1640, training loss: 422.810546875 = 0.5397329926490784 + 50.0 * 8.445416450500488
Epoch 1640, val loss: 0.6274793744087219
Epoch 1650, training loss: 422.76947021484375 = 0.5386775135993958 + 50.0 * 8.444616317749023
Epoch 1650, val loss: 0.6268393993377686
Epoch 1660, training loss: 422.71502685546875 = 0.5376664996147156 + 50.0 * 8.443547248840332
Epoch 1660, val loss: 0.6264037489891052
Epoch 1670, training loss: 422.7132568359375 = 0.5366592407226562 + 50.0 * 8.44353199005127
Epoch 1670, val loss: 0.6258972883224487
Epoch 1680, training loss: 422.86737060546875 = 0.5356394648551941 + 50.0 * 8.446634292602539
Epoch 1680, val loss: 0.62535160779953
Epoch 1690, training loss: 422.6961975097656 = 0.5345824360847473 + 50.0 * 8.443232536315918
Epoch 1690, val loss: 0.6247810125350952
Epoch 1700, training loss: 422.64703369140625 = 0.5335542559623718 + 50.0 * 8.442269325256348
Epoch 1700, val loss: 0.6242734789848328
Epoch 1710, training loss: 422.635986328125 = 0.532536506652832 + 50.0 * 8.442069053649902
Epoch 1710, val loss: 0.6237303018569946
Epoch 1720, training loss: 422.7669982910156 = 0.531510591506958 + 50.0 * 8.444709777832031
Epoch 1720, val loss: 0.6231645941734314
Epoch 1730, training loss: 422.65020751953125 = 0.5304453372955322 + 50.0 * 8.442395210266113
Epoch 1730, val loss: 0.6226230263710022
Epoch 1740, training loss: 422.6478271484375 = 0.5293800830841064 + 50.0 * 8.442368507385254
Epoch 1740, val loss: 0.6220071315765381
Epoch 1750, training loss: 422.6065979003906 = 0.5283353924751282 + 50.0 * 8.44156551361084
Epoch 1750, val loss: 0.621475100517273
Epoch 1760, training loss: 422.5689392089844 = 0.5272911787033081 + 50.0 * 8.44083309173584
Epoch 1760, val loss: 0.6209104061126709
Epoch 1770, training loss: 422.5816650390625 = 0.5262451767921448 + 50.0 * 8.441108703613281
Epoch 1770, val loss: 0.6203762292861938
Epoch 1780, training loss: 422.6932373046875 = 0.5251713991165161 + 50.0 * 8.443361282348633
Epoch 1780, val loss: 0.6197788119316101
Epoch 1790, training loss: 422.561279296875 = 0.524060845375061 + 50.0 * 8.440744400024414
Epoch 1790, val loss: 0.6191577911376953
Epoch 1800, training loss: 422.51727294921875 = 0.522978663444519 + 50.0 * 8.439886093139648
Epoch 1800, val loss: 0.618571937084198
Epoch 1810, training loss: 422.5088806152344 = 0.5218971371650696 + 50.0 * 8.439739227294922
Epoch 1810, val loss: 0.61798495054245
Epoch 1820, training loss: 422.5946350097656 = 0.5207954049110413 + 50.0 * 8.441476821899414
Epoch 1820, val loss: 0.6173396110534668
Epoch 1830, training loss: 422.52081298828125 = 0.5196619033813477 + 50.0 * 8.440023422241211
Epoch 1830, val loss: 0.6167513728141785
Epoch 1840, training loss: 422.4729309082031 = 0.5185302495956421 + 50.0 * 8.439087867736816
Epoch 1840, val loss: 0.6160989999771118
Epoch 1850, training loss: 422.43658447265625 = 0.5173946619033813 + 50.0 * 8.438384056091309
Epoch 1850, val loss: 0.6154857873916626
Epoch 1860, training loss: 422.4532165527344 = 0.516248881816864 + 50.0 * 8.438739776611328
Epoch 1860, val loss: 0.6148326992988586
Epoch 1870, training loss: 422.4854431152344 = 0.5150736570358276 + 50.0 * 8.439407348632812
Epoch 1870, val loss: 0.6140799522399902
Epoch 1880, training loss: 422.4723205566406 = 0.5138756036758423 + 50.0 * 8.439168930053711
Epoch 1880, val loss: 0.6133549213409424
Epoch 1890, training loss: 422.38031005859375 = 0.5126619935035706 + 50.0 * 8.437353134155273
Epoch 1890, val loss: 0.6126989126205444
Epoch 1900, training loss: 422.3556823730469 = 0.5114568471908569 + 50.0 * 8.436884880065918
Epoch 1900, val loss: 0.6119783520698547
Epoch 1910, training loss: 422.3236083984375 = 0.5102445483207703 + 50.0 * 8.436266899108887
Epoch 1910, val loss: 0.6112574934959412
Epoch 1920, training loss: 422.3101806640625 = 0.5090341567993164 + 50.0 * 8.436022758483887
Epoch 1920, val loss: 0.6105722188949585
Epoch 1930, training loss: 422.42724609375 = 0.5078065991401672 + 50.0 * 8.43838882446289
Epoch 1930, val loss: 0.6098034381866455
Epoch 1940, training loss: 422.30126953125 = 0.5065351724624634 + 50.0 * 8.435894966125488
Epoch 1940, val loss: 0.6090680360794067
Epoch 1950, training loss: 422.29486083984375 = 0.5052635669708252 + 50.0 * 8.435791969299316
Epoch 1950, val loss: 0.608310878276825
Epoch 1960, training loss: 422.2654724121094 = 0.5040077567100525 + 50.0 * 8.435229301452637
Epoch 1960, val loss: 0.6076084971427917
Epoch 1970, training loss: 422.2455749511719 = 0.5027437210083008 + 50.0 * 8.434856414794922
Epoch 1970, val loss: 0.6068611145019531
Epoch 1980, training loss: 422.2879638671875 = 0.5014763474464417 + 50.0 * 8.43572998046875
Epoch 1980, val loss: 0.6061310768127441
Epoch 1990, training loss: 422.2483825683594 = 0.5001777410507202 + 50.0 * 8.434964179992676
Epoch 1990, val loss: 0.6054275631904602
Epoch 2000, training loss: 422.2652587890625 = 0.49888089299201965 + 50.0 * 8.435327529907227
Epoch 2000, val loss: 0.6047043204307556
Epoch 2010, training loss: 422.1885986328125 = 0.4975666403770447 + 50.0 * 8.433820724487305
Epoch 2010, val loss: 0.6038486957550049
Epoch 2020, training loss: 422.1772155761719 = 0.49626439809799194 + 50.0 * 8.433618545532227
Epoch 2020, val loss: 0.6031255125999451
Epoch 2030, training loss: 422.27142333984375 = 0.4949435591697693 + 50.0 * 8.435529708862305
Epoch 2030, val loss: 0.6023319363594055
Epoch 2040, training loss: 422.1585693359375 = 0.4936088025569916 + 50.0 * 8.43329906463623
Epoch 2040, val loss: 0.6015345454216003
Epoch 2050, training loss: 422.1473693847656 = 0.4922672212123871 + 50.0 * 8.433101654052734
Epoch 2050, val loss: 0.6006923317909241
Epoch 2060, training loss: 422.24365234375 = 0.4909169673919678 + 50.0 * 8.435054779052734
Epoch 2060, val loss: 0.5999139547348022
Epoch 2070, training loss: 422.11309814453125 = 0.48953840136528015 + 50.0 * 8.43247127532959
Epoch 2070, val loss: 0.599064290523529
Epoch 2080, training loss: 422.10162353515625 = 0.4881603419780731 + 50.0 * 8.432269096374512
Epoch 2080, val loss: 0.5982844829559326
Epoch 2090, training loss: 422.1365661621094 = 0.4867885112762451 + 50.0 * 8.432995796203613
Epoch 2090, val loss: 0.597547173500061
Epoch 2100, training loss: 422.09912109375 = 0.48537978529930115 + 50.0 * 8.43227481842041
Epoch 2100, val loss: 0.5965800881385803
Epoch 2110, training loss: 422.09271240234375 = 0.4839678406715393 + 50.0 * 8.432174682617188
Epoch 2110, val loss: 0.5957019329071045
Epoch 2120, training loss: 422.0884094238281 = 0.4825533330440521 + 50.0 * 8.432117462158203
Epoch 2120, val loss: 0.5949186086654663
Epoch 2130, training loss: 422.06103515625 = 0.4811270833015442 + 50.0 * 8.431597709655762
Epoch 2130, val loss: 0.5940179228782654
Epoch 2140, training loss: 422.0658874511719 = 0.4797028601169586 + 50.0 * 8.431723594665527
Epoch 2140, val loss: 0.5932464599609375
Epoch 2150, training loss: 422.0718994140625 = 0.4782651364803314 + 50.0 * 8.431872367858887
Epoch 2150, val loss: 0.5924766063690186
Epoch 2160, training loss: 422.0472106933594 = 0.47682130336761475 + 50.0 * 8.431407928466797
Epoch 2160, val loss: 0.59166419506073
Epoch 2170, training loss: 422.00299072265625 = 0.4753645658493042 + 50.0 * 8.43055248260498
Epoch 2170, val loss: 0.590773344039917
Epoch 2180, training loss: 422.1197509765625 = 0.4739018976688385 + 50.0 * 8.432916641235352
Epoch 2180, val loss: 0.5898589491844177
Epoch 2190, training loss: 422.0312194824219 = 0.47239863872528076 + 50.0 * 8.43117618560791
Epoch 2190, val loss: 0.5890916585922241
Epoch 2200, training loss: 421.986572265625 = 0.47090888023376465 + 50.0 * 8.430313110351562
Epoch 2200, val loss: 0.5881324410438538
Epoch 2210, training loss: 421.95562744140625 = 0.46943703293800354 + 50.0 * 8.429723739624023
Epoch 2210, val loss: 0.5874125361442566
Epoch 2220, training loss: 421.9346008300781 = 0.4679647982120514 + 50.0 * 8.429332733154297
Epoch 2220, val loss: 0.5865126848220825
Epoch 2230, training loss: 421.9206237792969 = 0.4664947986602783 + 50.0 * 8.429082870483398
Epoch 2230, val loss: 0.5857058167457581
Epoch 2240, training loss: 421.92498779296875 = 0.4650135338306427 + 50.0 * 8.42919921875
Epoch 2240, val loss: 0.5848454833030701
Epoch 2250, training loss: 422.2132263183594 = 0.4634997546672821 + 50.0 * 8.4349946975708
Epoch 2250, val loss: 0.5839183926582336
Epoch 2260, training loss: 422.0030822753906 = 0.4619736075401306 + 50.0 * 8.430822372436523
Epoch 2260, val loss: 0.5832509994506836
Epoch 2270, training loss: 421.89434814453125 = 0.4604315459728241 + 50.0 * 8.428678512573242
Epoch 2270, val loss: 0.5822465419769287
Epoch 2280, training loss: 421.8707275390625 = 0.45892831683158875 + 50.0 * 8.42823600769043
Epoch 2280, val loss: 0.5814380645751953
Epoch 2290, training loss: 421.86212158203125 = 0.45742642879486084 + 50.0 * 8.428093910217285
Epoch 2290, val loss: 0.5806658267974854
Epoch 2300, training loss: 421.9035339355469 = 0.4559197723865509 + 50.0 * 8.42895221710205
Epoch 2300, val loss: 0.5798196196556091
Epoch 2310, training loss: 421.9411315917969 = 0.4543614983558655 + 50.0 * 8.42973518371582
Epoch 2310, val loss: 0.5789949893951416
Epoch 2320, training loss: 421.8529968261719 = 0.45281943678855896 + 50.0 * 8.428003311157227
Epoch 2320, val loss: 0.5781007409095764
Epoch 2330, training loss: 421.8792419433594 = 0.45126983523368835 + 50.0 * 8.428559303283691
Epoch 2330, val loss: 0.577312707901001
Epoch 2340, training loss: 421.87298583984375 = 0.44973668456077576 + 50.0 * 8.428464889526367
Epoch 2340, val loss: 0.5764766931533813
Epoch 2350, training loss: 421.8124084472656 = 0.4482094645500183 + 50.0 * 8.427284240722656
Epoch 2350, val loss: 0.5757520794868469
Epoch 2360, training loss: 421.8365173339844 = 0.44667744636535645 + 50.0 * 8.427796363830566
Epoch 2360, val loss: 0.5749968886375427
Epoch 2370, training loss: 421.84820556640625 = 0.44513025879859924 + 50.0 * 8.428061485290527
Epoch 2370, val loss: 0.5741395354270935
Epoch 2380, training loss: 421.8494567871094 = 0.4435720443725586 + 50.0 * 8.428117752075195
Epoch 2380, val loss: 0.5733281970024109
Epoch 2390, training loss: 421.7771911621094 = 0.4420200288295746 + 50.0 * 8.426703453063965
Epoch 2390, val loss: 0.5725740790367126
Epoch 2400, training loss: 421.7690734863281 = 0.4404759705066681 + 50.0 * 8.4265718460083
Epoch 2400, val loss: 0.5718122124671936
Epoch 2410, training loss: 421.77496337890625 = 0.4389246106147766 + 50.0 * 8.42672061920166
Epoch 2410, val loss: 0.5710182189941406
Epoch 2420, training loss: 421.86553955078125 = 0.4373634457588196 + 50.0 * 8.428563117980957
Epoch 2420, val loss: 0.5702068209648132
Epoch 2430, training loss: 421.8096008300781 = 0.43580037355422974 + 50.0 * 8.427475929260254
Epoch 2430, val loss: 0.5695868134498596
Epoch 2440, training loss: 421.76220703125 = 0.43423739075660706 + 50.0 * 8.426559448242188
Epoch 2440, val loss: 0.5688034296035767
Epoch 2450, training loss: 421.76123046875 = 0.4326840341091156 + 50.0 * 8.426570892333984
Epoch 2450, val loss: 0.5682024955749512
Epoch 2460, training loss: 421.73114013671875 = 0.43112698197364807 + 50.0 * 8.426000595092773
Epoch 2460, val loss: 0.5674566030502319
Epoch 2470, training loss: 421.7317810058594 = 0.4295717775821686 + 50.0 * 8.426044464111328
Epoch 2470, val loss: 0.5667855739593506
Epoch 2480, training loss: 421.6893310546875 = 0.4280242621898651 + 50.0 * 8.425226211547852
Epoch 2480, val loss: 0.5660951137542725
Epoch 2490, training loss: 421.697998046875 = 0.4264776110649109 + 50.0 * 8.425430297851562
Epoch 2490, val loss: 0.5654273629188538
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7747336377473364
0.8163442729841339
=== training gcn model ===
Epoch 0, training loss: 530.229248046875 = 1.1153461933135986 + 50.0 * 10.58227825164795
Epoch 0, val loss: 1.1128946542739868
Epoch 10, training loss: 530.2078857421875 = 1.1106492280960083 + 50.0 * 10.581944465637207
Epoch 10, val loss: 1.1082606315612793
Epoch 20, training loss: 530.1356811523438 = 1.1052888631820679 + 50.0 * 10.580607414245605
Epoch 20, val loss: 1.1029683351516724
Epoch 30, training loss: 529.8350830078125 = 1.0991253852844238 + 50.0 * 10.574719429016113
Epoch 30, val loss: 1.0968921184539795
Epoch 40, training loss: 528.6746215820312 = 1.0922495126724243 + 50.0 * 10.551648139953613
Epoch 40, val loss: 1.0901381969451904
Epoch 50, training loss: 525.1568603515625 = 1.0847047567367554 + 50.0 * 10.48144245147705
Epoch 50, val loss: 1.0827877521514893
Epoch 60, training loss: 516.2017211914062 = 1.0776532888412476 + 50.0 * 10.302481651306152
Epoch 60, val loss: 1.0760623216629028
Epoch 70, training loss: 498.9895324707031 = 1.070759892463684 + 50.0 * 9.958374977111816
Epoch 70, val loss: 1.0693739652633667
Epoch 80, training loss: 483.67059326171875 = 1.0649267435073853 + 50.0 * 9.65211296081543
Epoch 80, val loss: 1.064056396484375
Epoch 90, training loss: 474.3843078613281 = 1.0605818033218384 + 50.0 * 9.466474533081055
Epoch 90, val loss: 1.0599544048309326
Epoch 100, training loss: 467.6602478027344 = 1.0563980340957642 + 50.0 * 9.332077026367188
Epoch 100, val loss: 1.0559839010238647
Epoch 110, training loss: 461.9220886230469 = 1.0523923635482788 + 50.0 * 9.21739387512207
Epoch 110, val loss: 1.0522103309631348
Epoch 120, training loss: 457.0287780761719 = 1.0491161346435547 + 50.0 * 9.119593620300293
Epoch 120, val loss: 1.04916512966156
Epoch 130, training loss: 453.1097717285156 = 1.0467017889022827 + 50.0 * 9.041261672973633
Epoch 130, val loss: 1.046898365020752
Epoch 140, training loss: 450.4379577636719 = 1.0443978309631348 + 50.0 * 8.987871170043945
Epoch 140, val loss: 1.0447198152542114
Epoch 150, training loss: 447.8940734863281 = 1.042199969291687 + 50.0 * 8.937037467956543
Epoch 150, val loss: 1.0427042245864868
Epoch 160, training loss: 446.3424377441406 = 1.0402462482452393 + 50.0 * 8.906044006347656
Epoch 160, val loss: 1.0408837795257568
Epoch 170, training loss: 444.9933776855469 = 1.0381320714950562 + 50.0 * 8.879104614257812
Epoch 170, val loss: 1.0389162302017212
Epoch 180, training loss: 443.3379211425781 = 1.0362224578857422 + 50.0 * 8.846034049987793
Epoch 180, val loss: 1.0371723175048828
Epoch 190, training loss: 441.626708984375 = 1.0346819162368774 + 50.0 * 8.811841011047363
Epoch 190, val loss: 1.0357686281204224
Epoch 200, training loss: 440.20086669921875 = 1.033133625984192 + 50.0 * 8.783354759216309
Epoch 200, val loss: 1.0343812704086304
Epoch 210, training loss: 439.0719299316406 = 1.0314826965332031 + 50.0 * 8.760808944702148
Epoch 210, val loss: 1.0328562259674072
Epoch 220, training loss: 438.15838623046875 = 1.0296580791473389 + 50.0 * 8.742574691772461
Epoch 220, val loss: 1.0312323570251465
Epoch 230, training loss: 437.31915283203125 = 1.0277165174484253 + 50.0 * 8.725829124450684
Epoch 230, val loss: 1.0294955968856812
Epoch 240, training loss: 436.59185791015625 = 1.0256460905075073 + 50.0 * 8.711324691772461
Epoch 240, val loss: 1.0276180505752563
Epoch 250, training loss: 436.00439453125 = 1.0233545303344727 + 50.0 * 8.699621200561523
Epoch 250, val loss: 1.025551438331604
Epoch 260, training loss: 435.6796569824219 = 1.0207775831222534 + 50.0 * 8.693177223205566
Epoch 260, val loss: 1.0232042074203491
Epoch 270, training loss: 435.142333984375 = 1.0179136991500854 + 50.0 * 8.682488441467285
Epoch 270, val loss: 1.020629644393921
Epoch 280, training loss: 434.6885070800781 = 1.0149049758911133 + 50.0 * 8.67347240447998
Epoch 280, val loss: 1.0179508924484253
Epoch 290, training loss: 434.2338562011719 = 1.0117969512939453 + 50.0 * 8.664441108703613
Epoch 290, val loss: 1.0151565074920654
Epoch 300, training loss: 433.8094482421875 = 1.0085787773132324 + 50.0 * 8.656017303466797
Epoch 300, val loss: 1.0122604370117188
Epoch 310, training loss: 433.31121826171875 = 1.0051568746566772 + 50.0 * 8.64612102508545
Epoch 310, val loss: 1.009231448173523
Epoch 320, training loss: 432.816650390625 = 1.0015907287597656 + 50.0 * 8.636301040649414
Epoch 320, val loss: 1.0060418844223022
Epoch 330, training loss: 432.3666076660156 = 0.9978254437446594 + 50.0 * 8.627375602722168
Epoch 330, val loss: 1.0026860237121582
Epoch 340, training loss: 431.9803161621094 = 0.9938128590583801 + 50.0 * 8.619729995727539
Epoch 340, val loss: 0.9990883469581604
Epoch 350, training loss: 431.6385803222656 = 0.9895079731941223 + 50.0 * 8.612981796264648
Epoch 350, val loss: 0.9952375292778015
Epoch 360, training loss: 431.2799072265625 = 0.9849384427070618 + 50.0 * 8.605899810791016
Epoch 360, val loss: 0.9911751747131348
Epoch 370, training loss: 430.999267578125 = 0.9801186323165894 + 50.0 * 8.600382804870605
Epoch 370, val loss: 0.9868936538696289
Epoch 380, training loss: 430.7440490722656 = 0.9750552177429199 + 50.0 * 8.595379829406738
Epoch 380, val loss: 0.9823967218399048
Epoch 390, training loss: 430.5745849609375 = 0.9697130918502808 + 50.0 * 8.592097282409668
Epoch 390, val loss: 0.9776632189750671
Epoch 400, training loss: 430.3431091308594 = 0.9641595482826233 + 50.0 * 8.587578773498535
Epoch 400, val loss: 0.972743809223175
Epoch 410, training loss: 430.1805725097656 = 0.9584089517593384 + 50.0 * 8.584443092346191
Epoch 410, val loss: 0.9676707983016968
Epoch 420, training loss: 429.9593811035156 = 0.9524755477905273 + 50.0 * 8.580138206481934
Epoch 420, val loss: 0.9624420404434204
Epoch 430, training loss: 429.7306213378906 = 0.9464418292045593 + 50.0 * 8.57568359375
Epoch 430, val loss: 0.9571214914321899
Epoch 440, training loss: 429.52532958984375 = 0.9402753114700317 + 50.0 * 8.571701049804688
Epoch 440, val loss: 0.9516888856887817
Epoch 450, training loss: 429.31781005859375 = 0.9339972138404846 + 50.0 * 8.567676544189453
Epoch 450, val loss: 0.9461967349052429
Epoch 460, training loss: 429.1046142578125 = 0.9276372790336609 + 50.0 * 8.563539505004883
Epoch 460, val loss: 0.9406308531761169
Epoch 470, training loss: 428.96453857421875 = 0.9212336540222168 + 50.0 * 8.560866355895996
Epoch 470, val loss: 0.9350042939186096
Epoch 480, training loss: 428.7780456542969 = 0.9145885109901428 + 50.0 * 8.557269096374512
Epoch 480, val loss: 0.9292609095573425
Epoch 490, training loss: 428.5989074707031 = 0.9080275893211365 + 50.0 * 8.553817749023438
Epoch 490, val loss: 0.9235407710075378
Epoch 500, training loss: 428.3322448730469 = 0.9014711976051331 + 50.0 * 8.548615455627441
Epoch 500, val loss: 0.9179041981697083
Epoch 510, training loss: 428.1626892089844 = 0.8948934674263 + 50.0 * 8.545355796813965
Epoch 510, val loss: 0.9121994376182556
Epoch 520, training loss: 427.991455078125 = 0.8883178234100342 + 50.0 * 8.542062759399414
Epoch 520, val loss: 0.9065172076225281
Epoch 530, training loss: 427.83978271484375 = 0.8817156553268433 + 50.0 * 8.539161682128906
Epoch 530, val loss: 0.9008409380912781
Epoch 540, training loss: 427.7465515136719 = 0.875058650970459 + 50.0 * 8.537429809570312
Epoch 540, val loss: 0.8950786590576172
Epoch 550, training loss: 427.5676574707031 = 0.8683443069458008 + 50.0 * 8.53398609161377
Epoch 550, val loss: 0.8893153071403503
Epoch 560, training loss: 427.43658447265625 = 0.8617008328437805 + 50.0 * 8.531497955322266
Epoch 560, val loss: 0.8836166858673096
Epoch 570, training loss: 427.3560791015625 = 0.8550540208816528 + 50.0 * 8.530020713806152
Epoch 570, val loss: 0.8779143691062927
Epoch 580, training loss: 427.19549560546875 = 0.8483952879905701 + 50.0 * 8.526942253112793
Epoch 580, val loss: 0.872129499912262
Epoch 590, training loss: 427.0851745605469 = 0.8417585492134094 + 50.0 * 8.52486801147461
Epoch 590, val loss: 0.8664603233337402
Epoch 600, training loss: 426.9722900390625 = 0.83517986536026 + 50.0 * 8.52274227142334
Epoch 600, val loss: 0.8607872724533081
Epoch 610, training loss: 426.96429443359375 = 0.8286187648773193 + 50.0 * 8.522713661193848
Epoch 610, val loss: 0.855121910572052
Epoch 620, training loss: 426.832763671875 = 0.8220332264900208 + 50.0 * 8.520215034484863
Epoch 620, val loss: 0.8494582772254944
Epoch 630, training loss: 426.6600341796875 = 0.815527617931366 + 50.0 * 8.516890525817871
Epoch 630, val loss: 0.843855082988739
Epoch 640, training loss: 426.5565185546875 = 0.8090730905532837 + 50.0 * 8.514948844909668
Epoch 640, val loss: 0.8383492827415466
Epoch 650, training loss: 426.71478271484375 = 0.8026713132858276 + 50.0 * 8.518241882324219
Epoch 650, val loss: 0.8329232931137085
Epoch 660, training loss: 426.3959045410156 = 0.796314001083374 + 50.0 * 8.511991500854492
Epoch 660, val loss: 0.827301025390625
Epoch 670, training loss: 426.27484130859375 = 0.790039598941803 + 50.0 * 8.509696006774902
Epoch 670, val loss: 0.8219515681266785
Epoch 680, training loss: 426.1712951660156 = 0.7839154601097107 + 50.0 * 8.507747650146484
Epoch 680, val loss: 0.8166624307632446
Epoch 690, training loss: 426.0775451660156 = 0.7778939604759216 + 50.0 * 8.505992889404297
Epoch 690, val loss: 0.8115040063858032
Epoch 700, training loss: 426.05645751953125 = 0.7719435691833496 + 50.0 * 8.505690574645996
Epoch 700, val loss: 0.806412398815155
Epoch 710, training loss: 426.06903076171875 = 0.766048789024353 + 50.0 * 8.506059646606445
Epoch 710, val loss: 0.801247775554657
Epoch 720, training loss: 425.8572082519531 = 0.760244607925415 + 50.0 * 8.501938819885254
Epoch 720, val loss: 0.7963721752166748
Epoch 730, training loss: 425.7425231933594 = 0.754640519618988 + 50.0 * 8.499757766723633
Epoch 730, val loss: 0.791564404964447
Epoch 740, training loss: 425.6604919433594 = 0.749191164970398 + 50.0 * 8.498226165771484
Epoch 740, val loss: 0.7869331240653992
Epoch 750, training loss: 426.01702880859375 = 0.7438048124313354 + 50.0 * 8.505464553833008
Epoch 750, val loss: 0.7824908494949341
Epoch 760, training loss: 425.527587890625 = 0.7385327219963074 + 50.0 * 8.495780944824219
Epoch 760, val loss: 0.7778802514076233
Epoch 770, training loss: 425.43695068359375 = 0.7334944009780884 + 50.0 * 8.49406909942627
Epoch 770, val loss: 0.773585855960846
Epoch 780, training loss: 425.36260986328125 = 0.7286357283592224 + 50.0 * 8.492679595947266
Epoch 780, val loss: 0.7694423794746399
Epoch 790, training loss: 425.29449462890625 = 0.7239506244659424 + 50.0 * 8.491411209106445
Epoch 790, val loss: 0.7654982805252075
Epoch 800, training loss: 425.2216491699219 = 0.7194201350212097 + 50.0 * 8.490044593811035
Epoch 800, val loss: 0.7616977691650391
Epoch 810, training loss: 425.1650085449219 = 0.7150319814682007 + 50.0 * 8.488999366760254
Epoch 810, val loss: 0.7580638527870178
Epoch 820, training loss: 425.1607360839844 = 0.7107455730438232 + 50.0 * 8.488999366760254
Epoch 820, val loss: 0.7544575929641724
Epoch 830, training loss: 425.09521484375 = 0.7065860033035278 + 50.0 * 8.487772941589355
Epoch 830, val loss: 0.7510741949081421
Epoch 840, training loss: 424.98193359375 = 0.7026621699333191 + 50.0 * 8.48558521270752
Epoch 840, val loss: 0.7478475570678711
Epoch 850, training loss: 424.9018859863281 = 0.6989094614982605 + 50.0 * 8.48405933380127
Epoch 850, val loss: 0.7447621822357178
Epoch 860, training loss: 425.23785400390625 = 0.6952557563781738 + 50.0 * 8.490852355957031
Epoch 860, val loss: 0.7418674230575562
Epoch 870, training loss: 424.84893798828125 = 0.6917039155960083 + 50.0 * 8.483144760131836
Epoch 870, val loss: 0.7388267517089844
Epoch 880, training loss: 424.7368469238281 = 0.6883105635643005 + 50.0 * 8.48097038269043
Epoch 880, val loss: 0.7361238598823547
Epoch 890, training loss: 424.6801452636719 = 0.6850811839103699 + 50.0 * 8.479901313781738
Epoch 890, val loss: 0.7335984110832214
Epoch 900, training loss: 424.6159362792969 = 0.682012140750885 + 50.0 * 8.478678703308105
Epoch 900, val loss: 0.731166660785675
Epoch 910, training loss: 424.5614013671875 = 0.6790334582328796 + 50.0 * 8.477646827697754
Epoch 910, val loss: 0.7288738489151001
Epoch 920, training loss: 424.6569519042969 = 0.6761678457260132 + 50.0 * 8.479615211486816
Epoch 920, val loss: 0.7265703082084656
Epoch 930, training loss: 424.57098388671875 = 0.6732727885246277 + 50.0 * 8.477953910827637
Epoch 930, val loss: 0.7244187593460083
Epoch 940, training loss: 424.4327392578125 = 0.6705670952796936 + 50.0 * 8.47524356842041
Epoch 940, val loss: 0.7223215103149414
Epoch 950, training loss: 424.35882568359375 = 0.6679674386978149 + 50.0 * 8.473816871643066
Epoch 950, val loss: 0.7203276753425598
Epoch 960, training loss: 424.3609313964844 = 0.6654730439186096 + 50.0 * 8.473909378051758
Epoch 960, val loss: 0.7184798121452332
Epoch 970, training loss: 424.371826171875 = 0.6629904508590698 + 50.0 * 8.474176406860352
Epoch 970, val loss: 0.716504693031311
Epoch 980, training loss: 424.2615661621094 = 0.6605450510978699 + 50.0 * 8.472020149230957
Epoch 980, val loss: 0.7147718071937561
Epoch 990, training loss: 424.2084655761719 = 0.658242404460907 + 50.0 * 8.471004486083984
Epoch 990, val loss: 0.7130729556083679
Epoch 1000, training loss: 424.145263671875 = 0.6560558676719666 + 50.0 * 8.469783782958984
Epoch 1000, val loss: 0.7114394903182983
Epoch 1010, training loss: 424.1038818359375 = 0.6539339423179626 + 50.0 * 8.468998908996582
Epoch 1010, val loss: 0.7098995447158813
Epoch 1020, training loss: 424.3582763671875 = 0.65188068151474 + 50.0 * 8.474127769470215
Epoch 1020, val loss: 0.7082680463790894
Epoch 1030, training loss: 424.1574401855469 = 0.6496666073799133 + 50.0 * 8.470155715942383
Epoch 1030, val loss: 0.7068867683410645
Epoch 1040, training loss: 423.9839782714844 = 0.6476845741271973 + 50.0 * 8.466726303100586
Epoch 1040, val loss: 0.7053833603858948
Epoch 1050, training loss: 423.9584045410156 = 0.6457704901695251 + 50.0 * 8.466252326965332
Epoch 1050, val loss: 0.7039610743522644
Epoch 1060, training loss: 423.9139099121094 = 0.6439016461372375 + 50.0 * 8.465400695800781
Epoch 1060, val loss: 0.702708899974823
Epoch 1070, training loss: 423.87322998046875 = 0.642098069190979 + 50.0 * 8.464622497558594
Epoch 1070, val loss: 0.7013814449310303
Epoch 1080, training loss: 423.8415832519531 = 0.6403073668479919 + 50.0 * 8.464025497436523
Epoch 1080, val loss: 0.7001290917396545
Epoch 1090, training loss: 424.0304260253906 = 0.6385457515716553 + 50.0 * 8.4678373336792
Epoch 1090, val loss: 0.6987917423248291
Epoch 1100, training loss: 423.77825927734375 = 0.6367323994636536 + 50.0 * 8.462830543518066
Epoch 1100, val loss: 0.697594165802002
Epoch 1110, training loss: 423.7728576660156 = 0.6350182294845581 + 50.0 * 8.462757110595703
Epoch 1110, val loss: 0.696391761302948
Epoch 1120, training loss: 423.7254943847656 = 0.6333572268486023 + 50.0 * 8.46184253692627
Epoch 1120, val loss: 0.6952294707298279
Epoch 1130, training loss: 423.7464599609375 = 0.6317375898361206 + 50.0 * 8.462294578552246
Epoch 1130, val loss: 0.6940609812736511
Epoch 1140, training loss: 423.73974609375 = 0.6301390528678894 + 50.0 * 8.46219253540039
Epoch 1140, val loss: 0.6927796602249146
Epoch 1150, training loss: 423.6434020996094 = 0.6285086870193481 + 50.0 * 8.460297584533691
Epoch 1150, val loss: 0.6917414665222168
Epoch 1160, training loss: 423.59033203125 = 0.6269521713256836 + 50.0 * 8.459267616271973
Epoch 1160, val loss: 0.6905854344367981
Epoch 1170, training loss: 423.5607604980469 = 0.625438928604126 + 50.0 * 8.458706855773926
Epoch 1170, val loss: 0.6895308494567871
Epoch 1180, training loss: 423.61083984375 = 0.6239559054374695 + 50.0 * 8.459737777709961
Epoch 1180, val loss: 0.6884381771087646
Epoch 1190, training loss: 423.5633850097656 = 0.6224053502082825 + 50.0 * 8.458819389343262
Epoch 1190, val loss: 0.6872639656066895
Epoch 1200, training loss: 423.5031433105469 = 0.6208674311637878 + 50.0 * 8.457645416259766
Epoch 1200, val loss: 0.6862285137176514
Epoch 1210, training loss: 423.47833251953125 = 0.6194244623184204 + 50.0 * 8.457178115844727
Epoch 1210, val loss: 0.6851850748062134
Epoch 1220, training loss: 423.437744140625 = 0.618035078048706 + 50.0 * 8.45639419555664
Epoch 1220, val loss: 0.6841638088226318
Epoch 1230, training loss: 423.4111022949219 = 0.6166564226150513 + 50.0 * 8.455888748168945
Epoch 1230, val loss: 0.6831803917884827
Epoch 1240, training loss: 423.4662780761719 = 0.6152984499931335 + 50.0 * 8.457019805908203
Epoch 1240, val loss: 0.682146430015564
Epoch 1250, training loss: 423.3786315917969 = 0.6138706207275391 + 50.0 * 8.45529556274414
Epoch 1250, val loss: 0.6811977028846741
Epoch 1260, training loss: 423.3707275390625 = 0.6125227808952332 + 50.0 * 8.455163955688477
Epoch 1260, val loss: 0.6800951957702637
Epoch 1270, training loss: 423.3260192871094 = 0.6111785769462585 + 50.0 * 8.454297065734863
Epoch 1270, val loss: 0.6791725158691406
Epoch 1280, training loss: 423.29681396484375 = 0.6098828315734863 + 50.0 * 8.45373821258545
Epoch 1280, val loss: 0.6781935691833496
Epoch 1290, training loss: 423.3011169433594 = 0.6085829138755798 + 50.0 * 8.453850746154785
Epoch 1290, val loss: 0.6773044466972351
Epoch 1300, training loss: 423.37322998046875 = 0.607198178768158 + 50.0 * 8.455320358276367
Epoch 1300, val loss: 0.6762527227401733
Epoch 1310, training loss: 423.33294677734375 = 0.6058374047279358 + 50.0 * 8.45454216003418
Epoch 1310, val loss: 0.6752294898033142
Epoch 1320, training loss: 423.2273254394531 = 0.6045539975166321 + 50.0 * 8.452455520629883
Epoch 1320, val loss: 0.674152135848999
Epoch 1330, training loss: 423.2046203613281 = 0.6033159494400024 + 50.0 * 8.4520263671875
Epoch 1330, val loss: 0.6732383370399475
Epoch 1340, training loss: 423.1898193359375 = 0.6020897626876831 + 50.0 * 8.451754570007324
Epoch 1340, val loss: 0.6722802519798279
Epoch 1350, training loss: 423.34503173828125 = 0.6008629202842712 + 50.0 * 8.454883575439453
Epoch 1350, val loss: 0.6713168025016785
Epoch 1360, training loss: 423.2019348144531 = 0.5995200872421265 + 50.0 * 8.452048301696777
Epoch 1360, val loss: 0.6703528165817261
Epoch 1370, training loss: 423.1700744628906 = 0.5982815623283386 + 50.0 * 8.451436042785645
Epoch 1370, val loss: 0.6692770719528198
Epoch 1380, training loss: 423.1271667480469 = 0.5970364809036255 + 50.0 * 8.450602531433105
Epoch 1380, val loss: 0.6684240102767944
Epoch 1390, training loss: 423.09942626953125 = 0.5958557724952698 + 50.0 * 8.450071334838867
Epoch 1390, val loss: 0.6674367785453796
Epoch 1400, training loss: 423.080078125 = 0.594649612903595 + 50.0 * 8.449708938598633
Epoch 1400, val loss: 0.6665294170379639
Epoch 1410, training loss: 423.11279296875 = 0.5934455394744873 + 50.0 * 8.450387001037598
Epoch 1410, val loss: 0.6656143069267273
Epoch 1420, training loss: 423.0679016113281 = 0.5922226309776306 + 50.0 * 8.44951343536377
Epoch 1420, val loss: 0.6645192503929138
Epoch 1430, training loss: 423.05157470703125 = 0.5909773111343384 + 50.0 * 8.449212074279785
Epoch 1430, val loss: 0.6636477708816528
Epoch 1440, training loss: 423.01849365234375 = 0.589796245098114 + 50.0 * 8.44857406616211
Epoch 1440, val loss: 0.6626200079917908
Epoch 1450, training loss: 423.00091552734375 = 0.5886173248291016 + 50.0 * 8.448246002197266
Epoch 1450, val loss: 0.6617199182510376
Epoch 1460, training loss: 423.1139221191406 = 0.5874399542808533 + 50.0 * 8.450530052185059
Epoch 1460, val loss: 0.6607243418693542
Epoch 1470, training loss: 423.0168151855469 = 0.586201548576355 + 50.0 * 8.448612213134766
Epoch 1470, val loss: 0.6597587466239929
Epoch 1480, training loss: 422.98760986328125 = 0.5849851369857788 + 50.0 * 8.448052406311035
Epoch 1480, val loss: 0.6587887406349182
Epoch 1490, training loss: 423.03167724609375 = 0.583793044090271 + 50.0 * 8.448957443237305
Epoch 1490, val loss: 0.6579263210296631
Epoch 1500, training loss: 422.9328918457031 = 0.5825831294059753 + 50.0 * 8.447006225585938
Epoch 1500, val loss: 0.6568132042884827
Epoch 1510, training loss: 422.9117126464844 = 0.5814054608345032 + 50.0 * 8.446606636047363
Epoch 1510, val loss: 0.6558178067207336
Epoch 1520, training loss: 422.8958435058594 = 0.580228328704834 + 50.0 * 8.446311950683594
Epoch 1520, val loss: 0.654928982257843
Epoch 1530, training loss: 422.87567138671875 = 0.5790913105010986 + 50.0 * 8.445931434631348
Epoch 1530, val loss: 0.6539275646209717
Epoch 1540, training loss: 422.8622131347656 = 0.5779264569282532 + 50.0 * 8.445686340332031
Epoch 1540, val loss: 0.6530225276947021
Epoch 1550, training loss: 423.0632629394531 = 0.5767273306846619 + 50.0 * 8.44973087310791
Epoch 1550, val loss: 0.6520957350730896
Epoch 1560, training loss: 422.8872375488281 = 0.575513482093811 + 50.0 * 8.446234703063965
Epoch 1560, val loss: 0.6509441137313843
Epoch 1570, training loss: 422.8138427734375 = 0.5743019580841064 + 50.0 * 8.444790840148926
Epoch 1570, val loss: 0.6499496102333069
Epoch 1580, training loss: 422.8320617675781 = 0.5731131434440613 + 50.0 * 8.445178985595703
Epoch 1580, val loss: 0.6490775346755981
Epoch 1590, training loss: 422.9471130371094 = 0.5719055533409119 + 50.0 * 8.447504043579102
Epoch 1590, val loss: 0.6480522751808167
Epoch 1600, training loss: 422.8145446777344 = 0.5707126259803772 + 50.0 * 8.444876670837402
Epoch 1600, val loss: 0.6469210386276245
Epoch 1610, training loss: 422.75830078125 = 0.569503128528595 + 50.0 * 8.44377613067627
Epoch 1610, val loss: 0.645986795425415
Epoch 1620, training loss: 422.73052978515625 = 0.5683438777923584 + 50.0 * 8.443243980407715
Epoch 1620, val loss: 0.6449710130691528
Epoch 1630, training loss: 422.8235778808594 = 0.5671588182449341 + 50.0 * 8.445128440856934
Epoch 1630, val loss: 0.6440879106521606
Epoch 1640, training loss: 422.7864685058594 = 0.5659207105636597 + 50.0 * 8.444411277770996
Epoch 1640, val loss: 0.6429481506347656
Epoch 1650, training loss: 422.7298889160156 = 0.5647006034851074 + 50.0 * 8.443304061889648
Epoch 1650, val loss: 0.6418453454971313
Epoch 1660, training loss: 422.6697998046875 = 0.5635245442390442 + 50.0 * 8.44212532043457
Epoch 1660, val loss: 0.6408459544181824
Epoch 1670, training loss: 422.6512451171875 = 0.5623522400856018 + 50.0 * 8.441778182983398
Epoch 1670, val loss: 0.6398950219154358
Epoch 1680, training loss: 422.6402893066406 = 0.5611864328384399 + 50.0 * 8.441581726074219
Epoch 1680, val loss: 0.6389045119285583
Epoch 1690, training loss: 422.75091552734375 = 0.5599926710128784 + 50.0 * 8.443818092346191
Epoch 1690, val loss: 0.6379547715187073
Epoch 1700, training loss: 422.602294921875 = 0.5587528347969055 + 50.0 * 8.440871238708496
Epoch 1700, val loss: 0.6367527842521667
Epoch 1710, training loss: 422.5864562988281 = 0.5575347542762756 + 50.0 * 8.44057846069336
Epoch 1710, val loss: 0.6356747150421143
Epoch 1720, training loss: 422.5848693847656 = 0.5563597679138184 + 50.0 * 8.440569877624512
Epoch 1720, val loss: 0.6346229910850525
Epoch 1730, training loss: 422.69158935546875 = 0.5551853179931641 + 50.0 * 8.442728042602539
Epoch 1730, val loss: 0.6335015892982483
Epoch 1740, training loss: 422.5773620605469 = 0.553939700126648 + 50.0 * 8.440468788146973
Epoch 1740, val loss: 0.6326414346694946
Epoch 1750, training loss: 422.5348205566406 = 0.5527531504631042 + 50.0 * 8.439640998840332
Epoch 1750, val loss: 0.6314908862113953
Epoch 1760, training loss: 422.53143310546875 = 0.5515770316123962 + 50.0 * 8.439597129821777
Epoch 1760, val loss: 0.6304853558540344
Epoch 1770, training loss: 422.6509094238281 = 0.550345242023468 + 50.0 * 8.442010879516602
Epoch 1770, val loss: 0.6294166445732117
Epoch 1780, training loss: 422.50372314453125 = 0.5491019487380981 + 50.0 * 8.439092636108398
Epoch 1780, val loss: 0.6282771825790405
Epoch 1790, training loss: 422.4695739746094 = 0.5478757619857788 + 50.0 * 8.438433647155762
Epoch 1790, val loss: 0.6272247433662415
Epoch 1800, training loss: 422.45135498046875 = 0.5466981530189514 + 50.0 * 8.438093185424805
Epoch 1800, val loss: 0.6261201500892639
Epoch 1810, training loss: 422.50384521484375 = 0.5455092191696167 + 50.0 * 8.439167022705078
Epoch 1810, val loss: 0.6249982714653015
Epoch 1820, training loss: 422.4685363769531 = 0.5442297458648682 + 50.0 * 8.438486099243164
Epoch 1820, val loss: 0.6239698529243469
Epoch 1830, training loss: 422.4361267089844 = 0.5429607629776001 + 50.0 * 8.43786334991455
Epoch 1830, val loss: 0.6227434277534485
Epoch 1840, training loss: 422.39849853515625 = 0.5417054295539856 + 50.0 * 8.437135696411133
Epoch 1840, val loss: 0.6218351125717163
Epoch 1850, training loss: 422.374267578125 = 0.5405185222625732 + 50.0 * 8.436675071716309
Epoch 1850, val loss: 0.6206729412078857
Epoch 1860, training loss: 422.39404296875 = 0.5393059253692627 + 50.0 * 8.437094688415527
Epoch 1860, val loss: 0.619717538356781
Epoch 1870, training loss: 422.4842529296875 = 0.5380475521087646 + 50.0 * 8.438923835754395
Epoch 1870, val loss: 0.6186044812202454
Epoch 1880, training loss: 422.3699951171875 = 0.5367981195449829 + 50.0 * 8.436663627624512
Epoch 1880, val loss: 0.6173670887947083
Epoch 1890, training loss: 422.31585693359375 = 0.5355423092842102 + 50.0 * 8.435606002807617
Epoch 1890, val loss: 0.6164137125015259
Epoch 1900, training loss: 422.2977600097656 = 0.534338653087616 + 50.0 * 8.43526840209961
Epoch 1900, val loss: 0.6152942776679993
Epoch 1910, training loss: 422.5113525390625 = 0.5330989956855774 + 50.0 * 8.43956470489502
Epoch 1910, val loss: 0.6144316792488098
Epoch 1920, training loss: 422.3612976074219 = 0.5318278670310974 + 50.0 * 8.436589241027832
Epoch 1920, val loss: 0.6129220724105835
Epoch 1930, training loss: 422.25360107421875 = 0.530531644821167 + 50.0 * 8.43446159362793
Epoch 1930, val loss: 0.6119605898857117
Epoch 1940, training loss: 422.2301940917969 = 0.5293022394180298 + 50.0 * 8.4340181350708
Epoch 1940, val loss: 0.6109045743942261
Epoch 1950, training loss: 422.2178649902344 = 0.5280899405479431 + 50.0 * 8.433795928955078
Epoch 1950, val loss: 0.6097897887229919
Epoch 1960, training loss: 422.2333068847656 = 0.5268541574478149 + 50.0 * 8.434128761291504
Epoch 1960, val loss: 0.6088113784790039
Epoch 1970, training loss: 422.39801025390625 = 0.525542676448822 + 50.0 * 8.43744945526123
Epoch 1970, val loss: 0.6076450347900391
Epoch 1980, training loss: 422.1983337402344 = 0.5242170691490173 + 50.0 * 8.43348217010498
Epoch 1980, val loss: 0.6063743829727173
Epoch 1990, training loss: 422.1852722167969 = 0.5229419469833374 + 50.0 * 8.433246612548828
Epoch 1990, val loss: 0.6053019165992737
Epoch 2000, training loss: 422.14581298828125 = 0.5217140913009644 + 50.0 * 8.43248176574707
Epoch 2000, val loss: 0.6042006015777588
Epoch 2010, training loss: 422.1326904296875 = 0.5204854011535645 + 50.0 * 8.432244300842285
Epoch 2010, val loss: 0.6031805276870728
Epoch 2020, training loss: 422.1360168457031 = 0.5192593932151794 + 50.0 * 8.432334899902344
Epoch 2020, val loss: 0.6021159291267395
Epoch 2030, training loss: 422.4106750488281 = 0.5179696679115295 + 50.0 * 8.437853813171387
Epoch 2030, val loss: 0.6010768413543701
Epoch 2040, training loss: 422.1668395996094 = 0.516650915145874 + 50.0 * 8.433003425598145
Epoch 2040, val loss: 0.5997553467750549
Epoch 2050, training loss: 422.0859375 = 0.5153588056564331 + 50.0 * 8.431411743164062
Epoch 2050, val loss: 0.5987805128097534
Epoch 2060, training loss: 422.0625305175781 = 0.5141202211380005 + 50.0 * 8.430968284606934
Epoch 2060, val loss: 0.597640335559845
Epoch 2070, training loss: 422.0459289550781 = 0.5128865838050842 + 50.0 * 8.43066120147705
Epoch 2070, val loss: 0.5966156721115112
Epoch 2080, training loss: 422.0358581542969 = 0.5116561651229858 + 50.0 * 8.4304838180542
Epoch 2080, val loss: 0.5955407023429871
Epoch 2090, training loss: 422.2567443847656 = 0.5104202032089233 + 50.0 * 8.434926986694336
Epoch 2090, val loss: 0.5943905711174011
Epoch 2100, training loss: 422.14959716796875 = 0.5090495944023132 + 50.0 * 8.43281078338623
Epoch 2100, val loss: 0.5934998989105225
Epoch 2110, training loss: 422.0172424316406 = 0.5077626705169678 + 50.0 * 8.430190086364746
Epoch 2110, val loss: 0.5922346711158752
Epoch 2120, training loss: 422.00128173828125 = 0.5065106153488159 + 50.0 * 8.429895401000977
Epoch 2120, val loss: 0.5911397933959961
Epoch 2130, training loss: 421.9755859375 = 0.5052828788757324 + 50.0 * 8.42940616607666
Epoch 2130, val loss: 0.5901581048965454
Epoch 2140, training loss: 422.02734375 = 0.5040472149848938 + 50.0 * 8.430465698242188
Epoch 2140, val loss: 0.5891091227531433
Epoch 2150, training loss: 421.9765625 = 0.5027651786804199 + 50.0 * 8.429475784301758
Epoch 2150, val loss: 0.5880813598632812
Epoch 2160, training loss: 421.96051025390625 = 0.5014835596084595 + 50.0 * 8.429180145263672
Epoch 2160, val loss: 0.5871080756187439
Epoch 2170, training loss: 421.9804382324219 = 0.5002187490463257 + 50.0 * 8.429604530334473
Epoch 2170, val loss: 0.586086630821228
Epoch 2180, training loss: 421.9462890625 = 0.4989590048789978 + 50.0 * 8.428946495056152
Epoch 2180, val loss: 0.5849816203117371
Epoch 2190, training loss: 421.9186096191406 = 0.49771401286125183 + 50.0 * 8.428418159484863
Epoch 2190, val loss: 0.5838636755943298
Epoch 2200, training loss: 421.9074401855469 = 0.4964771866798401 + 50.0 * 8.428218841552734
Epoch 2200, val loss: 0.5828433632850647
Epoch 2210, training loss: 421.88568115234375 = 0.49523240327835083 + 50.0 * 8.42780876159668
Epoch 2210, val loss: 0.5818748474121094
Epoch 2220, training loss: 421.9966125488281 = 0.4939815104007721 + 50.0 * 8.430052757263184
Epoch 2220, val loss: 0.5810014009475708
Epoch 2230, training loss: 421.9054260253906 = 0.49266937375068665 + 50.0 * 8.428255081176758
Epoch 2230, val loss: 0.579814076423645
Epoch 2240, training loss: 421.8446350097656 = 0.4913909435272217 + 50.0 * 8.427064895629883
Epoch 2240, val loss: 0.578732430934906
Epoch 2250, training loss: 421.8289794921875 = 0.4901531934738159 + 50.0 * 8.426776885986328
Epoch 2250, val loss: 0.5776838064193726
Epoch 2260, training loss: 421.8135681152344 = 0.48893049359321594 + 50.0 * 8.426492691040039
Epoch 2260, val loss: 0.5767987966537476
Epoch 2270, training loss: 421.8046569824219 = 0.48771533370018005 + 50.0 * 8.426339149475098
Epoch 2270, val loss: 0.5758333206176758
Epoch 2280, training loss: 421.8459777832031 = 0.486495703458786 + 50.0 * 8.427189826965332
Epoch 2280, val loss: 0.5749237537384033
Epoch 2290, training loss: 421.8962707519531 = 0.4852313697338104 + 50.0 * 8.428220748901367
Epoch 2290, val loss: 0.5738376379013062
Epoch 2300, training loss: 421.87060546875 = 0.4839705228805542 + 50.0 * 8.427732467651367
Epoch 2300, val loss: 0.5726668834686279
Epoch 2310, training loss: 421.7813415527344 = 0.4826856851577759 + 50.0 * 8.425972938537598
Epoch 2310, val loss: 0.5718164443969727
Epoch 2320, training loss: 421.75494384765625 = 0.48146307468414307 + 50.0 * 8.425469398498535
Epoch 2320, val loss: 0.5709297060966492
Epoch 2330, training loss: 421.7434387207031 = 0.4802633225917816 + 50.0 * 8.425263404846191
Epoch 2330, val loss: 0.5699539184570312
Epoch 2340, training loss: 421.769287109375 = 0.4790436029434204 + 50.0 * 8.42580509185791
Epoch 2340, val loss: 0.5691248774528503
Epoch 2350, training loss: 421.82977294921875 = 0.47779425978660583 + 50.0 * 8.427040100097656
Epoch 2350, val loss: 0.5681153535842896
Epoch 2360, training loss: 421.7572326660156 = 0.4765525460243225 + 50.0 * 8.425613403320312
Epoch 2360, val loss: 0.5669617652893066
Epoch 2370, training loss: 421.70599365234375 = 0.4752911627292633 + 50.0 * 8.424613952636719
Epoch 2370, val loss: 0.5660863518714905
Epoch 2380, training loss: 421.7159729003906 = 0.47406554222106934 + 50.0 * 8.424838066101074
Epoch 2380, val loss: 0.565144419670105
Epoch 2390, training loss: 421.77349853515625 = 0.4728245437145233 + 50.0 * 8.426012992858887
Epoch 2390, val loss: 0.5640974044799805
Epoch 2400, training loss: 421.6835021972656 = 0.4715762734413147 + 50.0 * 8.424238204956055
Epoch 2400, val loss: 0.5630089640617371
Epoch 2410, training loss: 421.68792724609375 = 0.47033265233039856 + 50.0 * 8.424351692199707
Epoch 2410, val loss: 0.562134861946106
Epoch 2420, training loss: 421.8435363769531 = 0.4690782427787781 + 50.0 * 8.427489280700684
Epoch 2420, val loss: 0.5612806081771851
Epoch 2430, training loss: 421.6811828613281 = 0.4678097665309906 + 50.0 * 8.424267768859863
Epoch 2430, val loss: 0.5600146055221558
Epoch 2440, training loss: 421.6395263671875 = 0.4665736258029938 + 50.0 * 8.42345905303955
Epoch 2440, val loss: 0.559127926826477
Epoch 2450, training loss: 421.62200927734375 = 0.4653692841529846 + 50.0 * 8.42313289642334
Epoch 2450, val loss: 0.5581885576248169
Epoch 2460, training loss: 421.66693115234375 = 0.4641715884208679 + 50.0 * 8.424055099487305
Epoch 2460, val loss: 0.5573079586029053
Epoch 2470, training loss: 421.696533203125 = 0.46293097734451294 + 50.0 * 8.42467212677002
Epoch 2470, val loss: 0.55632084608078
Epoch 2480, training loss: 421.6283874511719 = 0.4616765081882477 + 50.0 * 8.423334121704102
Epoch 2480, val loss: 0.5554421544075012
Epoch 2490, training loss: 421.5916748046875 = 0.46046507358551025 + 50.0 * 8.422623634338379
Epoch 2490, val loss: 0.5546953082084656
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7737189244038558
0.8130116641309861
The final CL Acc:0.77186, 0.00337, The final GNN Acc:0.81458, 0.00137
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110340])
remove edge: torch.Size([2, 66264])
updated graph: torch.Size([2, 87956])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 530.197265625 = 1.0828408002853394 + 50.0 * 10.58228874206543
Epoch 0, val loss: 1.0831103324890137
Epoch 10, training loss: 530.1761474609375 = 1.079265832901001 + 50.0 * 10.581937789916992
Epoch 10, val loss: 1.079507827758789
Epoch 20, training loss: 530.0914306640625 = 1.0753191709518433 + 50.0 * 10.580322265625
Epoch 20, val loss: 1.075495719909668
Epoch 30, training loss: 529.7097778320312 = 1.070754051208496 + 50.0 * 10.57278060913086
Epoch 30, val loss: 1.0708242654800415
Epoch 40, training loss: 528.2598266601562 = 1.0654994249343872 + 50.0 * 10.5438871383667
Epoch 40, val loss: 1.0654475688934326
Epoch 50, training loss: 524.4957275390625 = 1.0595407485961914 + 50.0 * 10.46872329711914
Epoch 50, val loss: 1.0593230724334717
Epoch 60, training loss: 517.0271606445312 = 1.0529268980026245 + 50.0 * 10.31948471069336
Epoch 60, val loss: 1.0525811910629272
Epoch 70, training loss: 505.9709167480469 = 1.0451873540878296 + 50.0 * 10.098514556884766
Epoch 70, val loss: 1.0445454120635986
Epoch 80, training loss: 489.39422607421875 = 1.0364229679107666 + 50.0 * 9.767155647277832
Epoch 80, val loss: 1.03587806224823
Epoch 90, training loss: 477.0052185058594 = 1.0287989377975464 + 50.0 * 9.51952838897705
Epoch 90, val loss: 1.0287123918533325
Epoch 100, training loss: 468.9730529785156 = 1.0226725339889526 + 50.0 * 9.359007835388184
Epoch 100, val loss: 1.0230982303619385
Epoch 110, training loss: 465.53399658203125 = 1.0177308320999146 + 50.0 * 9.290325164794922
Epoch 110, val loss: 1.0184401273727417
Epoch 120, training loss: 463.3311462402344 = 1.0129183530807495 + 50.0 * 9.24636459350586
Epoch 120, val loss: 1.013746976852417
Epoch 130, training loss: 461.8166809082031 = 1.0086393356323242 + 50.0 * 9.216160774230957
Epoch 130, val loss: 1.009550929069519
Epoch 140, training loss: 460.173095703125 = 1.0049247741699219 + 50.0 * 9.183363914489746
Epoch 140, val loss: 1.005793809890747
Epoch 150, training loss: 457.92950439453125 = 1.0011613368988037 + 50.0 * 9.138566970825195
Epoch 150, val loss: 1.0019360780715942
Epoch 160, training loss: 454.88055419921875 = 0.9977970719337463 + 50.0 * 9.077654838562012
Epoch 160, val loss: 0.9985896944999695
Epoch 170, training loss: 451.926025390625 = 0.995339035987854 + 50.0 * 9.018613815307617
Epoch 170, val loss: 0.9960730075836182
Epoch 180, training loss: 450.60699462890625 = 0.9916606545448303 + 50.0 * 8.99230670928955
Epoch 180, val loss: 0.9919997453689575
Epoch 190, training loss: 449.36285400390625 = 0.9856249690055847 + 50.0 * 8.967544555664062
Epoch 190, val loss: 0.9858153462409973
Epoch 200, training loss: 447.6256103515625 = 0.9805833697319031 + 50.0 * 8.932900428771973
Epoch 200, val loss: 0.9810558557510376
Epoch 210, training loss: 445.223876953125 = 0.9765940308570862 + 50.0 * 8.8849458694458
Epoch 210, val loss: 0.9772555828094482
Epoch 220, training loss: 442.7601318359375 = 0.9726834893226624 + 50.0 * 8.835748672485352
Epoch 220, val loss: 0.973463237285614
Epoch 230, training loss: 441.03546142578125 = 0.9682469964027405 + 50.0 * 8.80134391784668
Epoch 230, val loss: 0.9689584374427795
Epoch 240, training loss: 439.4050598144531 = 0.9629316926002502 + 50.0 * 8.768842697143555
Epoch 240, val loss: 0.9635658860206604
Epoch 250, training loss: 437.91119384765625 = 0.9576044082641602 + 50.0 * 8.7390718460083
Epoch 250, val loss: 0.9581621885299683
Epoch 260, training loss: 436.5622863769531 = 0.9520833492279053 + 50.0 * 8.712203979492188
Epoch 260, val loss: 0.9525986313819885
Epoch 270, training loss: 435.4144592285156 = 0.9463790059089661 + 50.0 * 8.689361572265625
Epoch 270, val loss: 0.946727454662323
Epoch 280, training loss: 434.35504150390625 = 0.9399805665016174 + 50.0 * 8.668301582336426
Epoch 280, val loss: 0.9403229355812073
Epoch 290, training loss: 433.5205078125 = 0.9329857230186462 + 50.0 * 8.651750564575195
Epoch 290, val loss: 0.9333893656730652
Epoch 300, training loss: 432.9472961425781 = 0.9257280230522156 + 50.0 * 8.64043140411377
Epoch 300, val loss: 0.9261682629585266
Epoch 310, training loss: 432.0779113769531 = 0.9180039763450623 + 50.0 * 8.623198509216309
Epoch 310, val loss: 0.9184191226959229
Epoch 320, training loss: 431.4615173339844 = 0.9102150201797485 + 50.0 * 8.6110258102417
Epoch 320, val loss: 0.9108259081840515
Epoch 330, training loss: 430.8417663574219 = 0.9022652506828308 + 50.0 * 8.598790168762207
Epoch 330, val loss: 0.9029189944267273
Epoch 340, training loss: 430.2618408203125 = 0.8940902352333069 + 50.0 * 8.58735466003418
Epoch 340, val loss: 0.8948447704315186
Epoch 350, training loss: 429.7290344238281 = 0.885681688785553 + 50.0 * 8.57686710357666
Epoch 350, val loss: 0.8865312337875366
Epoch 360, training loss: 429.3520202636719 = 0.8768277168273926 + 50.0 * 8.569503784179688
Epoch 360, val loss: 0.8777955770492554
Epoch 370, training loss: 428.9307861328125 = 0.8676829934120178 + 50.0 * 8.561262130737305
Epoch 370, val loss: 0.8687959313392639
Epoch 380, training loss: 428.5960693359375 = 0.8583934903144836 + 50.0 * 8.554753303527832
Epoch 380, val loss: 0.8596721291542053
Epoch 390, training loss: 428.3065185546875 = 0.8489376306533813 + 50.0 * 8.549151420593262
Epoch 390, val loss: 0.8503985404968262
Epoch 400, training loss: 428.0384826660156 = 0.8394086956977844 + 50.0 * 8.543981552124023
Epoch 400, val loss: 0.8410592079162598
Epoch 410, training loss: 427.8638610839844 = 0.8299248814582825 + 50.0 * 8.540678977966309
Epoch 410, val loss: 0.8317086696624756
Epoch 420, training loss: 427.5411376953125 = 0.8202958703041077 + 50.0 * 8.534417152404785
Epoch 420, val loss: 0.8223889470100403
Epoch 430, training loss: 427.28131103515625 = 0.8108006715774536 + 50.0 * 8.529410362243652
Epoch 430, val loss: 0.8131737112998962
Epoch 440, training loss: 427.0114440917969 = 0.801338255405426 + 50.0 * 8.524202346801758
Epoch 440, val loss: 0.8039855360984802
Epoch 450, training loss: 426.7675476074219 = 0.7918990850448608 + 50.0 * 8.519513130187988
Epoch 450, val loss: 0.7947917580604553
Epoch 460, training loss: 426.49114990234375 = 0.7824312448501587 + 50.0 * 8.514174461364746
Epoch 460, val loss: 0.7856053113937378
Epoch 470, training loss: 426.2718505859375 = 0.7730603218078613 + 50.0 * 8.50997543334961
Epoch 470, val loss: 0.7764984965324402
Epoch 480, training loss: 426.0020751953125 = 0.7637119293212891 + 50.0 * 8.504767417907715
Epoch 480, val loss: 0.7674350738525391
Epoch 490, training loss: 425.7696838378906 = 0.7544786930084229 + 50.0 * 8.500304222106934
Epoch 490, val loss: 0.7585153579711914
Epoch 500, training loss: 425.7026672363281 = 0.7453031539916992 + 50.0 * 8.499147415161133
Epoch 500, val loss: 0.7495975494384766
Epoch 510, training loss: 425.34808349609375 = 0.7361425757408142 + 50.0 * 8.492238998413086
Epoch 510, val loss: 0.7407069802284241
Epoch 520, training loss: 425.1499938964844 = 0.7271394729614258 + 50.0 * 8.488456726074219
Epoch 520, val loss: 0.7320458292961121
Epoch 530, training loss: 424.9975891113281 = 0.7182404398918152 + 50.0 * 8.485587120056152
Epoch 530, val loss: 0.7234466671943665
Epoch 540, training loss: 424.8667907714844 = 0.709308922290802 + 50.0 * 8.483149528503418
Epoch 540, val loss: 0.714888334274292
Epoch 550, training loss: 424.7080078125 = 0.7005151510238647 + 50.0 * 8.48015022277832
Epoch 550, val loss: 0.7063826322555542
Epoch 560, training loss: 424.5668029785156 = 0.6918758749961853 + 50.0 * 8.477499008178711
Epoch 560, val loss: 0.6981436014175415
Epoch 570, training loss: 424.4285888671875 = 0.6833928823471069 + 50.0 * 8.47490406036377
Epoch 570, val loss: 0.6899638772010803
Epoch 580, training loss: 424.38690185546875 = 0.6750955581665039 + 50.0 * 8.474235534667969
Epoch 580, val loss: 0.6819560527801514
Epoch 590, training loss: 424.2199401855469 = 0.6669699549674988 + 50.0 * 8.471059799194336
Epoch 590, val loss: 0.6742033362388611
Epoch 600, training loss: 424.0818176269531 = 0.6590622663497925 + 50.0 * 8.46845531463623
Epoch 600, val loss: 0.6666145920753479
Epoch 610, training loss: 423.96661376953125 = 0.6513369083404541 + 50.0 * 8.46630573272705
Epoch 610, val loss: 0.659203827381134
Epoch 620, training loss: 423.920166015625 = 0.6437507271766663 + 50.0 * 8.46552848815918
Epoch 620, val loss: 0.6519119739532471
Epoch 630, training loss: 423.7880554199219 = 0.6363867521286011 + 50.0 * 8.463033676147461
Epoch 630, val loss: 0.6449437141418457
Epoch 640, training loss: 423.6716613769531 = 0.6293509006500244 + 50.0 * 8.460845947265625
Epoch 640, val loss: 0.6382337212562561
Epoch 650, training loss: 423.60711669921875 = 0.6225429177284241 + 50.0 * 8.459692001342773
Epoch 650, val loss: 0.6316976547241211
Epoch 660, training loss: 423.5065612792969 = 0.6159365177154541 + 50.0 * 8.457812309265137
Epoch 660, val loss: 0.625385582447052
Epoch 670, training loss: 423.3913879394531 = 0.609593391418457 + 50.0 * 8.455636024475098
Epoch 670, val loss: 0.6193311810493469
Epoch 680, training loss: 423.30230712890625 = 0.6034742593765259 + 50.0 * 8.45397663116455
Epoch 680, val loss: 0.613439679145813
Epoch 690, training loss: 423.32391357421875 = 0.5975490212440491 + 50.0 * 8.454527854919434
Epoch 690, val loss: 0.6078150868415833
Epoch 700, training loss: 423.1543273925781 = 0.5918493270874023 + 50.0 * 8.451249122619629
Epoch 700, val loss: 0.6023300886154175
Epoch 710, training loss: 423.0511169433594 = 0.5864208340644836 + 50.0 * 8.449294090270996
Epoch 710, val loss: 0.5972303152084351
Epoch 720, training loss: 422.95037841796875 = 0.581152617931366 + 50.0 * 8.44738483428955
Epoch 720, val loss: 0.5921643972396851
Epoch 730, training loss: 422.86578369140625 = 0.5761585235595703 + 50.0 * 8.445792198181152
Epoch 730, val loss: 0.5874123573303223
Epoch 740, training loss: 422.8255920410156 = 0.5713186264038086 + 50.0 * 8.445085525512695
Epoch 740, val loss: 0.5828439593315125
Epoch 750, training loss: 422.7743225097656 = 0.5666252374649048 + 50.0 * 8.444153785705566
Epoch 750, val loss: 0.5784186720848083
Epoch 760, training loss: 422.655029296875 = 0.5621814727783203 + 50.0 * 8.441856384277344
Epoch 760, val loss: 0.5741432905197144
Epoch 770, training loss: 422.570068359375 = 0.5578842163085938 + 50.0 * 8.4402437210083
Epoch 770, val loss: 0.57015061378479
Epoch 780, training loss: 422.52288818359375 = 0.5537919998168945 + 50.0 * 8.43938159942627
Epoch 780, val loss: 0.566307544708252
Epoch 790, training loss: 422.5083312988281 = 0.5497837662696838 + 50.0 * 8.439170837402344
Epoch 790, val loss: 0.5625681281089783
Epoch 800, training loss: 422.4349060058594 = 0.5459384322166443 + 50.0 * 8.437779426574707
Epoch 800, val loss: 0.5589237213134766
Epoch 810, training loss: 422.32843017578125 = 0.5422840714454651 + 50.0 * 8.435722351074219
Epoch 810, val loss: 0.5554855465888977
Epoch 820, training loss: 422.2570495605469 = 0.5387769937515259 + 50.0 * 8.434365272521973
Epoch 820, val loss: 0.5522456169128418
Epoch 830, training loss: 422.208740234375 = 0.5354115962982178 + 50.0 * 8.433466911315918
Epoch 830, val loss: 0.5491651296615601
Epoch 840, training loss: 422.23699951171875 = 0.5321351885795593 + 50.0 * 8.434097290039062
Epoch 840, val loss: 0.546148955821991
Epoch 850, training loss: 422.1064758300781 = 0.5290160179138184 + 50.0 * 8.431549072265625
Epoch 850, val loss: 0.5432215332984924
Epoch 860, training loss: 422.0142822265625 = 0.5260075330734253 + 50.0 * 8.429765701293945
Epoch 860, val loss: 0.5404879450798035
Epoch 870, training loss: 421.97509765625 = 0.5231176018714905 + 50.0 * 8.429039001464844
Epoch 870, val loss: 0.537834107875824
Epoch 880, training loss: 421.948486328125 = 0.520298957824707 + 50.0 * 8.428564071655273
Epoch 880, val loss: 0.5352553725242615
Epoch 890, training loss: 421.97430419921875 = 0.517572820186615 + 50.0 * 8.429134368896484
Epoch 890, val loss: 0.5327847003936768
Epoch 900, training loss: 421.8689880371094 = 0.5149018168449402 + 50.0 * 8.427082061767578
Epoch 900, val loss: 0.530523419380188
Epoch 910, training loss: 421.7711181640625 = 0.5123834013938904 + 50.0 * 8.425174713134766
Epoch 910, val loss: 0.5282323956489563
Epoch 920, training loss: 421.7058410644531 = 0.5099409222602844 + 50.0 * 8.423917770385742
Epoch 920, val loss: 0.5260844230651855
Epoch 930, training loss: 421.6468811035156 = 0.5076074004173279 + 50.0 * 8.422785758972168
Epoch 930, val loss: 0.5239806175231934
Epoch 940, training loss: 421.6043395996094 = 0.5053342580795288 + 50.0 * 8.421979904174805
Epoch 940, val loss: 0.5219608545303345
Epoch 950, training loss: 421.7185974121094 = 0.5031159520149231 + 50.0 * 8.424309730529785
Epoch 950, val loss: 0.5199296474456787
Epoch 960, training loss: 421.57440185546875 = 0.5009139776229858 + 50.0 * 8.421469688415527
Epoch 960, val loss: 0.5182202458381653
Epoch 970, training loss: 421.487060546875 = 0.4988599121570587 + 50.0 * 8.419763565063477
Epoch 970, val loss: 0.5163416266441345
Epoch 980, training loss: 421.4685974121094 = 0.4968286454677582 + 50.0 * 8.419435501098633
Epoch 980, val loss: 0.5146321654319763
Epoch 990, training loss: 421.369873046875 = 0.49489250779151917 + 50.0 * 8.417499542236328
Epoch 990, val loss: 0.5128830075263977
Epoch 1000, training loss: 421.3204040527344 = 0.49300074577331543 + 50.0 * 8.416547775268555
Epoch 1000, val loss: 0.5113031268119812
Epoch 1010, training loss: 421.3048095703125 = 0.4911617338657379 + 50.0 * 8.41627311706543
Epoch 1010, val loss: 0.5096903443336487
Epoch 1020, training loss: 421.2835388183594 = 0.4893741011619568 + 50.0 * 8.41588306427002
Epoch 1020, val loss: 0.5081390738487244
Epoch 1030, training loss: 421.1865234375 = 0.4876248240470886 + 50.0 * 8.413978576660156
Epoch 1030, val loss: 0.5068435668945312
Epoch 1040, training loss: 421.1600646972656 = 0.48594385385513306 + 50.0 * 8.413482666015625
Epoch 1040, val loss: 0.5053776502609253
Epoch 1050, training loss: 421.0859069824219 = 0.4842996895313263 + 50.0 * 8.412032127380371
Epoch 1050, val loss: 0.5040003657341003
Epoch 1060, training loss: 421.139892578125 = 0.4827044606208801 + 50.0 * 8.4131441116333
Epoch 1060, val loss: 0.5026470422744751
Epoch 1070, training loss: 421.04302978515625 = 0.4811245799064636 + 50.0 * 8.411238670349121
Epoch 1070, val loss: 0.5013936758041382
Epoch 1080, training loss: 420.9668884277344 = 0.4796174466609955 + 50.0 * 8.409745216369629
Epoch 1080, val loss: 0.5001816749572754
Epoch 1090, training loss: 420.92388916015625 = 0.47815650701522827 + 50.0 * 8.408914566040039
Epoch 1090, val loss: 0.4989701509475708
Epoch 1100, training loss: 421.0932922363281 = 0.47672027349472046 + 50.0 * 8.412331581115723
Epoch 1100, val loss: 0.497748464345932
Epoch 1110, training loss: 420.879150390625 = 0.4752795994281769 + 50.0 * 8.408077239990234
Epoch 1110, val loss: 0.49678000807762146
Epoch 1120, training loss: 420.79144287109375 = 0.4739266335964203 + 50.0 * 8.406350135803223
Epoch 1120, val loss: 0.49563509225845337
Epoch 1130, training loss: 420.7628173828125 = 0.4726030230522156 + 50.0 * 8.405804634094238
Epoch 1130, val loss: 0.49458810687065125
Epoch 1140, training loss: 420.728271484375 = 0.4713152348995209 + 50.0 * 8.405138969421387
Epoch 1140, val loss: 0.49358564615249634
Epoch 1150, training loss: 421.02288818359375 = 0.4699956476688385 + 50.0 * 8.411057472229004
Epoch 1150, val loss: 0.4924417734146118
Epoch 1160, training loss: 420.70361328125 = 0.46869543194770813 + 50.0 * 8.404698371887207
Epoch 1160, val loss: 0.49162188172340393
Epoch 1170, training loss: 420.628662109375 = 0.46747809648513794 + 50.0 * 8.403223991394043
Epoch 1170, val loss: 0.49061453342437744
Epoch 1180, training loss: 420.5889892578125 = 0.4663008451461792 + 50.0 * 8.402453422546387
Epoch 1180, val loss: 0.48971644043922424
Epoch 1190, training loss: 420.5560607910156 = 0.4651466906070709 + 50.0 * 8.40181827545166
Epoch 1190, val loss: 0.48884809017181396
Epoch 1200, training loss: 420.5411071777344 = 0.46400541067123413 + 50.0 * 8.401541709899902
Epoch 1200, val loss: 0.48799434304237366
Epoch 1210, training loss: 420.5221252441406 = 0.46284782886505127 + 50.0 * 8.401185035705566
Epoch 1210, val loss: 0.4871502220630646
Epoch 1220, training loss: 420.65679931640625 = 0.46172136068344116 + 50.0 * 8.403901100158691
Epoch 1220, val loss: 0.48643961548805237
Epoch 1230, training loss: 420.47808837890625 = 0.4605874717235565 + 50.0 * 8.400349617004395
Epoch 1230, val loss: 0.4853753447532654
Epoch 1240, training loss: 420.42877197265625 = 0.45953014492988586 + 50.0 * 8.399384498596191
Epoch 1240, val loss: 0.48475685715675354
Epoch 1250, training loss: 420.38104248046875 = 0.4585082232952118 + 50.0 * 8.39845085144043
Epoch 1250, val loss: 0.48389768600463867
Epoch 1260, training loss: 420.33782958984375 = 0.4575136601924896 + 50.0 * 8.397605895996094
Epoch 1260, val loss: 0.4832383692264557
Epoch 1270, training loss: 420.30810546875 = 0.4565252661705017 + 50.0 * 8.397031784057617
Epoch 1270, val loss: 0.482525497674942
Epoch 1280, training loss: 420.3012390136719 = 0.455553263425827 + 50.0 * 8.396913528442383
Epoch 1280, val loss: 0.48182567954063416
Epoch 1290, training loss: 420.32318115234375 = 0.45454055070877075 + 50.0 * 8.39737319946289
Epoch 1290, val loss: 0.48108044266700745
Epoch 1300, training loss: 420.23248291015625 = 0.45356643199920654 + 50.0 * 8.395578384399414
Epoch 1300, val loss: 0.4803876578807831
Epoch 1310, training loss: 420.20452880859375 = 0.4526166617870331 + 50.0 * 8.395038604736328
Epoch 1310, val loss: 0.4797030985355377
Epoch 1320, training loss: 420.2196960449219 = 0.4516841471195221 + 50.0 * 8.395359992980957
Epoch 1320, val loss: 0.47908005118370056
Epoch 1330, training loss: 420.15606689453125 = 0.45071056485176086 + 50.0 * 8.3941068649292
Epoch 1330, val loss: 0.47829771041870117
Epoch 1340, training loss: 420.0978698730469 = 0.4497992992401123 + 50.0 * 8.392961502075195
Epoch 1340, val loss: 0.4777132272720337
Epoch 1350, training loss: 420.0743408203125 = 0.4489036500453949 + 50.0 * 8.392508506774902
Epoch 1350, val loss: 0.4769573211669922
Epoch 1360, training loss: 420.04217529296875 = 0.44803643226623535 + 50.0 * 8.39188289642334
Epoch 1360, val loss: 0.47643402218818665
Epoch 1370, training loss: 420.0248107910156 = 0.4471769332885742 + 50.0 * 8.391552925109863
Epoch 1370, val loss: 0.4757613241672516
Epoch 1380, training loss: 420.1230163574219 = 0.44628816843032837 + 50.0 * 8.393534660339355
Epoch 1380, val loss: 0.47510066628456116
Epoch 1390, training loss: 419.9854736328125 = 0.44540339708328247 + 50.0 * 8.390801429748535
Epoch 1390, val loss: 0.4745371639728546
Epoch 1400, training loss: 419.94036865234375 = 0.4445457458496094 + 50.0 * 8.38991641998291
Epoch 1400, val loss: 0.47395142912864685
Epoch 1410, training loss: 419.9126281738281 = 0.4437088072299957 + 50.0 * 8.389378547668457
Epoch 1410, val loss: 0.4733922779560089
Epoch 1420, training loss: 419.94146728515625 = 0.44288069009780884 + 50.0 * 8.389971733093262
Epoch 1420, val loss: 0.47280293703079224
Epoch 1430, training loss: 419.99359130859375 = 0.4420020580291748 + 50.0 * 8.391031265258789
Epoch 1430, val loss: 0.4722617566585541
Epoch 1440, training loss: 419.8851013183594 = 0.4411506950855255 + 50.0 * 8.38887882232666
Epoch 1440, val loss: 0.4715850055217743
Epoch 1450, training loss: 419.80450439453125 = 0.4403262436389923 + 50.0 * 8.387283325195312
Epoch 1450, val loss: 0.47100454568862915
Epoch 1460, training loss: 419.786376953125 = 0.43952229619026184 + 50.0 * 8.386937141418457
Epoch 1460, val loss: 0.4705062508583069
Epoch 1470, training loss: 419.7556457519531 = 0.438734769821167 + 50.0 * 8.386338233947754
Epoch 1470, val loss: 0.4699309170246124
Epoch 1480, training loss: 419.7676086425781 = 0.4379409849643707 + 50.0 * 8.38659381866455
Epoch 1480, val loss: 0.4694446921348572
Epoch 1490, training loss: 419.8534240722656 = 0.43711549043655396 + 50.0 * 8.388326644897461
Epoch 1490, val loss: 0.46891868114471436
Epoch 1500, training loss: 419.6928405761719 = 0.43629708886146545 + 50.0 * 8.385130882263184
Epoch 1500, val loss: 0.46828433871269226
Epoch 1510, training loss: 419.6553649902344 = 0.43549948930740356 + 50.0 * 8.384397506713867
Epoch 1510, val loss: 0.467718243598938
Epoch 1520, training loss: 419.6352233886719 = 0.43472820520401 + 50.0 * 8.384010314941406
Epoch 1520, val loss: 0.46722522377967834
Epoch 1530, training loss: 419.6132507324219 = 0.4339649975299835 + 50.0 * 8.383585929870605
Epoch 1530, val loss: 0.46672534942626953
Epoch 1540, training loss: 419.6835021972656 = 0.43319305777549744 + 50.0 * 8.385005950927734
Epoch 1540, val loss: 0.4661819040775299
Epoch 1550, training loss: 419.5863342285156 = 0.432392418384552 + 50.0 * 8.383078575134277
Epoch 1550, val loss: 0.4656873941421509
Epoch 1560, training loss: 419.6953430175781 = 0.4316115975379944 + 50.0 * 8.385274887084961
Epoch 1560, val loss: 0.46496808528900146
Epoch 1570, training loss: 419.5436706542969 = 0.43078961968421936 + 50.0 * 8.382257461547852
Epoch 1570, val loss: 0.4647118151187897
Epoch 1580, training loss: 419.5163269042969 = 0.4300096333026886 + 50.0 * 8.381726264953613
Epoch 1580, val loss: 0.46403536200523376
Epoch 1590, training loss: 419.49029541015625 = 0.42926254868507385 + 50.0 * 8.381220817565918
Epoch 1590, val loss: 0.46362873911857605
Epoch 1600, training loss: 419.4604187011719 = 0.42851176857948303 + 50.0 * 8.380638122558594
Epoch 1600, val loss: 0.4631139636039734
Epoch 1610, training loss: 419.43878173828125 = 0.4277668297290802 + 50.0 * 8.380220413208008
Epoch 1610, val loss: 0.4626641571521759
Epoch 1620, training loss: 419.44097900390625 = 0.42700904607772827 + 50.0 * 8.380279541015625
Epoch 1620, val loss: 0.46219855546951294
Epoch 1630, training loss: 419.5311584472656 = 0.42622119188308716 + 50.0 * 8.382099151611328
Epoch 1630, val loss: 0.46173402667045593
Epoch 1640, training loss: 419.3921203613281 = 0.4254387319087982 + 50.0 * 8.37933349609375
Epoch 1640, val loss: 0.4611934423446655
Epoch 1650, training loss: 419.3785400390625 = 0.42467328906059265 + 50.0 * 8.379076957702637
Epoch 1650, val loss: 0.4605785608291626
Epoch 1660, training loss: 419.3635559082031 = 0.42391398549079895 + 50.0 * 8.378792762756348
Epoch 1660, val loss: 0.46016332507133484
Epoch 1670, training loss: 419.40045166015625 = 0.4231574833393097 + 50.0 * 8.379546165466309
Epoch 1670, val loss: 0.4596107602119446
Epoch 1680, training loss: 419.3420715332031 = 0.4223649501800537 + 50.0 * 8.37839412689209
Epoch 1680, val loss: 0.45914426445961
Epoch 1690, training loss: 419.3424377441406 = 0.42159393429756165 + 50.0 * 8.378417015075684
Epoch 1690, val loss: 0.45866337418556213
Epoch 1700, training loss: 419.3055419921875 = 0.42082685232162476 + 50.0 * 8.377694129943848
Epoch 1700, val loss: 0.45813366770744324
Epoch 1710, training loss: 419.3244934082031 = 0.4200468957424164 + 50.0 * 8.37808895111084
Epoch 1710, val loss: 0.4575996398925781
Epoch 1720, training loss: 419.2696838378906 = 0.4192614257335663 + 50.0 * 8.377008438110352
Epoch 1720, val loss: 0.45715710520744324
Epoch 1730, training loss: 419.3354797363281 = 0.4184706211090088 + 50.0 * 8.378340721130371
Epoch 1730, val loss: 0.45662474632263184
Epoch 1740, training loss: 419.236083984375 = 0.41769346594810486 + 50.0 * 8.376367568969727
Epoch 1740, val loss: 0.4562489092350006
Epoch 1750, training loss: 419.2016296386719 = 0.41691362857818604 + 50.0 * 8.375694274902344
Epoch 1750, val loss: 0.4556735157966614
Epoch 1760, training loss: 419.1852722167969 = 0.41614896059036255 + 50.0 * 8.375382423400879
Epoch 1760, val loss: 0.45520660281181335
Epoch 1770, training loss: 419.236572265625 = 0.4153805375099182 + 50.0 * 8.376423835754395
Epoch 1770, val loss: 0.45465803146362305
Epoch 1780, training loss: 419.2640686035156 = 0.4145444333553314 + 50.0 * 8.37699031829834
Epoch 1780, val loss: 0.45405539870262146
Epoch 1790, training loss: 419.1549377441406 = 0.41373899579048157 + 50.0 * 8.374824523925781
Epoch 1790, val loss: 0.45370104908943176
Epoch 1800, training loss: 419.1486511230469 = 0.41295164823532104 + 50.0 * 8.374713897705078
Epoch 1800, val loss: 0.45314332842826843
Epoch 1810, training loss: 419.11505126953125 = 0.4121913015842438 + 50.0 * 8.374056816101074
Epoch 1810, val loss: 0.45268714427948
Epoch 1820, training loss: 419.0914001464844 = 0.411434143781662 + 50.0 * 8.3735990524292
Epoch 1820, val loss: 0.45220401883125305
Epoch 1830, training loss: 419.07470703125 = 0.410670667886734 + 50.0 * 8.37328052520752
Epoch 1830, val loss: 0.4517323970794678
Epoch 1840, training loss: 419.0630187988281 = 0.4098997116088867 + 50.0 * 8.373062133789062
Epoch 1840, val loss: 0.45124080777168274
Epoch 1850, training loss: 419.2616882324219 = 0.4091113805770874 + 50.0 * 8.37705135345459
Epoch 1850, val loss: 0.45075222849845886
Epoch 1860, training loss: 419.1253356933594 = 0.408290296792984 + 50.0 * 8.374341011047363
Epoch 1860, val loss: 0.45020589232444763
Epoch 1870, training loss: 419.0215759277344 = 0.40748584270477295 + 50.0 * 8.372282028198242
Epoch 1870, val loss: 0.4497189223766327
Epoch 1880, training loss: 419.08514404296875 = 0.40670332312583923 + 50.0 * 8.373568534851074
Epoch 1880, val loss: 0.4493215084075928
Epoch 1890, training loss: 419.02825927734375 = 0.4058637022972107 + 50.0 * 8.372447967529297
Epoch 1890, val loss: 0.44864341616630554
Epoch 1900, training loss: 419.0155334472656 = 0.4050615727901459 + 50.0 * 8.372209548950195
Epoch 1900, val loss: 0.4481922388076782
Epoch 1910, training loss: 418.97344970703125 = 0.4042728841304779 + 50.0 * 8.371383666992188
Epoch 1910, val loss: 0.44765302538871765
Epoch 1920, training loss: 418.9436340332031 = 0.4034954905509949 + 50.0 * 8.370802879333496
Epoch 1920, val loss: 0.44719189405441284
Epoch 1930, training loss: 418.9263916015625 = 0.4027129113674164 + 50.0 * 8.370473861694336
Epoch 1930, val loss: 0.4467080533504486
Epoch 1940, training loss: 419.0486755371094 = 0.4019279181957245 + 50.0 * 8.37293529510498
Epoch 1940, val loss: 0.4462756812572479
Epoch 1950, training loss: 418.9900207519531 = 0.40108463168144226 + 50.0 * 8.37177848815918
Epoch 1950, val loss: 0.4457324147224426
Epoch 1960, training loss: 418.9097595214844 = 0.40027910470962524 + 50.0 * 8.370189666748047
Epoch 1960, val loss: 0.4451614022254944
Epoch 1970, training loss: 418.88116455078125 = 0.39948585629463196 + 50.0 * 8.369633674621582
Epoch 1970, val loss: 0.4447444677352905
Epoch 1980, training loss: 418.9336242675781 = 0.3986910879611969 + 50.0 * 8.370698928833008
Epoch 1980, val loss: 0.44426700472831726
Epoch 1990, training loss: 418.879638671875 = 0.39785853028297424 + 50.0 * 8.369635581970215
Epoch 1990, val loss: 0.4437132775783539
Epoch 2000, training loss: 418.8568420410156 = 0.39704349637031555 + 50.0 * 8.369195938110352
Epoch 2000, val loss: 0.4430726170539856
Epoch 2010, training loss: 418.81378173828125 = 0.3962445855140686 + 50.0 * 8.368350982666016
Epoch 2010, val loss: 0.4427218437194824
Epoch 2020, training loss: 418.8059387207031 = 0.3954516053199768 + 50.0 * 8.368209838867188
Epoch 2020, val loss: 0.4422041177749634
Epoch 2030, training loss: 418.8367004394531 = 0.39465218782424927 + 50.0 * 8.368841171264648
Epoch 2030, val loss: 0.4416392147541046
Epoch 2040, training loss: 418.8580322265625 = 0.3938136100769043 + 50.0 * 8.369284629821777
Epoch 2040, val loss: 0.4410696029663086
Epoch 2050, training loss: 418.7793273925781 = 0.3929895758628845 + 50.0 * 8.367727279663086
Epoch 2050, val loss: 0.4406653344631195
Epoch 2060, training loss: 418.74603271484375 = 0.3921786844730377 + 50.0 * 8.367076873779297
Epoch 2060, val loss: 0.44015541672706604
Epoch 2070, training loss: 418.72088623046875 = 0.3913716673851013 + 50.0 * 8.36659049987793
Epoch 2070, val loss: 0.4395546019077301
Epoch 2080, training loss: 418.7731628417969 = 0.39056918025016785 + 50.0 * 8.36765193939209
Epoch 2080, val loss: 0.4390285909175873
Epoch 2090, training loss: 418.7425842285156 = 0.3897341787815094 + 50.0 * 8.367056846618652
Epoch 2090, val loss: 0.43872779607772827
Epoch 2100, training loss: 418.72442626953125 = 0.3888861835002899 + 50.0 * 8.366710662841797
Epoch 2100, val loss: 0.438040554523468
Epoch 2110, training loss: 418.67529296875 = 0.38806241750717163 + 50.0 * 8.365744590759277
Epoch 2110, val loss: 0.4375406503677368
Epoch 2120, training loss: 418.66436767578125 = 0.3872504234313965 + 50.0 * 8.3655424118042
Epoch 2120, val loss: 0.4369790554046631
Epoch 2130, training loss: 418.6739501953125 = 0.38642948865890503 + 50.0 * 8.365750312805176
Epoch 2130, val loss: 0.4364706575870514
Epoch 2140, training loss: 418.67138671875 = 0.3855898678302765 + 50.0 * 8.365715980529785
Epoch 2140, val loss: 0.4359290301799774
Epoch 2150, training loss: 418.602783203125 = 0.3847469091415405 + 50.0 * 8.364360809326172
Epoch 2150, val loss: 0.43546536564826965
Epoch 2160, training loss: 418.6679992675781 = 0.38390636444091797 + 50.0 * 8.365681648254395
Epoch 2160, val loss: 0.4348779320716858
Epoch 2170, training loss: 418.5873718261719 = 0.3830532133579254 + 50.0 * 8.364086151123047
Epoch 2170, val loss: 0.43442365527153015
Epoch 2180, training loss: 418.5997314453125 = 0.38220125436782837 + 50.0 * 8.364350318908691
Epoch 2180, val loss: 0.43386995792388916
Epoch 2190, training loss: 418.5926208496094 = 0.3813483417034149 + 50.0 * 8.364225387573242
Epoch 2190, val loss: 0.4333227574825287
Epoch 2200, training loss: 418.61895751953125 = 0.3804866671562195 + 50.0 * 8.364768981933594
Epoch 2200, val loss: 0.4326689839363098
Epoch 2210, training loss: 418.5386047363281 = 0.3796110451221466 + 50.0 * 8.363180160522461
Epoch 2210, val loss: 0.4323086142539978
Epoch 2220, training loss: 418.51593017578125 = 0.37875866889953613 + 50.0 * 8.362743377685547
Epoch 2220, val loss: 0.43173748254776
Epoch 2230, training loss: 418.48223876953125 = 0.3779156804084778 + 50.0 * 8.362086296081543
Epoch 2230, val loss: 0.43120265007019043
Epoch 2240, training loss: 418.46807861328125 = 0.37708377838134766 + 50.0 * 8.361820220947266
Epoch 2240, val loss: 0.430742472410202
Epoch 2250, training loss: 418.4888610839844 = 0.3762422502040863 + 50.0 * 8.362252235412598
Epoch 2250, val loss: 0.43015772104263306
Epoch 2260, training loss: 418.6153564453125 = 0.3753640055656433 + 50.0 * 8.364799499511719
Epoch 2260, val loss: 0.4295612573623657
Epoch 2270, training loss: 418.4960632324219 = 0.37447768449783325 + 50.0 * 8.362431526184082
Epoch 2270, val loss: 0.42913344502449036
Epoch 2280, training loss: 418.4517517089844 = 0.37361255288124084 + 50.0 * 8.361562728881836
Epoch 2280, val loss: 0.4285423457622528
Epoch 2290, training loss: 418.4442443847656 = 0.37274834513664246 + 50.0 * 8.361430168151855
Epoch 2290, val loss: 0.4280420243740082
Epoch 2300, training loss: 418.3887939453125 = 0.3718853294849396 + 50.0 * 8.36033821105957
Epoch 2300, val loss: 0.42752060294151306
Epoch 2310, training loss: 418.4213562011719 = 0.37102189660072327 + 50.0 * 8.361006736755371
Epoch 2310, val loss: 0.4269847571849823
Epoch 2320, training loss: 418.4346923828125 = 0.3701338469982147 + 50.0 * 8.36129093170166
Epoch 2320, val loss: 0.42641475796699524
Epoch 2330, training loss: 418.3873596191406 = 0.36925646662712097 + 50.0 * 8.36036205291748
Epoch 2330, val loss: 0.4259587526321411
Epoch 2340, training loss: 418.38232421875 = 0.3683711886405945 + 50.0 * 8.360279083251953
Epoch 2340, val loss: 0.42553043365478516
Epoch 2350, training loss: 418.3357238769531 = 0.3674882650375366 + 50.0 * 8.35936450958252
Epoch 2350, val loss: 0.42488718032836914
Epoch 2360, training loss: 418.31781005859375 = 0.36660322546958923 + 50.0 * 8.359024047851562
Epoch 2360, val loss: 0.42430880665779114
Epoch 2370, training loss: 418.33203125 = 0.36572447419166565 + 50.0 * 8.359326362609863
Epoch 2370, val loss: 0.4238452911376953
Epoch 2380, training loss: 418.47955322265625 = 0.36483484506607056 + 50.0 * 8.36229419708252
Epoch 2380, val loss: 0.4234723746776581
Epoch 2390, training loss: 418.3321533203125 = 0.36389100551605225 + 50.0 * 8.359365463256836
Epoch 2390, val loss: 0.4225962460041046
Epoch 2400, training loss: 418.27325439453125 = 0.3629855513572693 + 50.0 * 8.358205795288086
Epoch 2400, val loss: 0.4222524166107178
Epoch 2410, training loss: 418.2410888671875 = 0.3620958626270294 + 50.0 * 8.357580184936523
Epoch 2410, val loss: 0.4216296374797821
Epoch 2420, training loss: 418.2234191894531 = 0.3612159192562103 + 50.0 * 8.357244491577148
Epoch 2420, val loss: 0.42119115591049194
Epoch 2430, training loss: 418.238037109375 = 0.3603358566761017 + 50.0 * 8.35755443572998
Epoch 2430, val loss: 0.42067694664001465
Epoch 2440, training loss: 418.30810546875 = 0.35942891240119934 + 50.0 * 8.358973503112793
Epoch 2440, val loss: 0.4201456606388092
Epoch 2450, training loss: 418.24603271484375 = 0.35851436853408813 + 50.0 * 8.357749938964844
Epoch 2450, val loss: 0.4196809232234955
Epoch 2460, training loss: 418.2699890136719 = 0.35760343074798584 + 50.0 * 8.358247756958008
Epoch 2460, val loss: 0.4192090332508087
Epoch 2470, training loss: 418.1625061035156 = 0.35668566823005676 + 50.0 * 8.35611629486084
Epoch 2470, val loss: 0.41864317655563354
Epoch 2480, training loss: 418.1639404296875 = 0.3557860553264618 + 50.0 * 8.356163024902344
Epoch 2480, val loss: 0.4181024432182312
Epoch 2490, training loss: 418.1578063964844 = 0.35488829016685486 + 50.0 * 8.356058120727539
Epoch 2490, val loss: 0.4175877571105957
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.830542871638762
0.8632905890023909
=== training gcn model ===
Epoch 0, training loss: 530.2010498046875 = 1.088681936264038 + 50.0 * 10.582246780395508
Epoch 0, val loss: 1.0877642631530762
Epoch 10, training loss: 530.1687622070312 = 1.0852938890457153 + 50.0 * 10.581669807434082
Epoch 10, val loss: 1.08440363407135
Epoch 20, training loss: 530.0336303710938 = 1.0816057920455933 + 50.0 * 10.57904052734375
Epoch 20, val loss: 1.0807334184646606
Epoch 30, training loss: 529.4364013671875 = 1.077629566192627 + 50.0 * 10.567174911499023
Epoch 30, val loss: 1.0767765045166016
Epoch 40, training loss: 527.1284790039062 = 1.0733022689819336 + 50.0 * 10.521102905273438
Epoch 40, val loss: 1.0724296569824219
Epoch 50, training loss: 520.4188842773438 = 1.0684516429901123 + 50.0 * 10.387007713317871
Epoch 50, val loss: 1.0675420761108398
Epoch 60, training loss: 507.65423583984375 = 1.0632878541946411 + 50.0 * 10.131818771362305
Epoch 60, val loss: 1.0623327493667603
Epoch 70, training loss: 491.0248718261719 = 1.057861566543579 + 50.0 * 9.79934024810791
Epoch 70, val loss: 1.0569789409637451
Epoch 80, training loss: 476.6993713378906 = 1.0525883436203003 + 50.0 * 9.512935638427734
Epoch 80, val loss: 1.051901936531067
Epoch 90, training loss: 465.35198974609375 = 1.0476468801498413 + 50.0 * 9.286087036132812
Epoch 90, val loss: 1.0472537279129028
Epoch 100, training loss: 460.0051574707031 = 1.0435901880264282 + 50.0 * 9.179231643676758
Epoch 100, val loss: 1.0434073209762573
Epoch 110, training loss: 455.8208923339844 = 1.0405007600784302 + 50.0 * 9.09560775756836
Epoch 110, val loss: 1.0403980016708374
Epoch 120, training loss: 452.23828125 = 1.037609577178955 + 50.0 * 9.02401351928711
Epoch 120, val loss: 1.0374925136566162
Epoch 130, training loss: 449.8761901855469 = 1.0350396633148193 + 50.0 * 8.976822853088379
Epoch 130, val loss: 1.0348820686340332
Epoch 140, training loss: 447.4075012207031 = 1.0322946310043335 + 50.0 * 8.927504539489746
Epoch 140, val loss: 1.032076120376587
Epoch 150, training loss: 444.8206787109375 = 1.0292822122573853 + 50.0 * 8.87582778930664
Epoch 150, val loss: 1.0290567874908447
Epoch 160, training loss: 443.30548095703125 = 1.0260709524154663 + 50.0 * 8.845588684082031
Epoch 160, val loss: 1.0258283615112305
Epoch 170, training loss: 442.0003967285156 = 1.0224765539169312 + 50.0 * 8.819558143615723
Epoch 170, val loss: 1.02228844165802
Epoch 180, training loss: 440.50390625 = 1.019220232963562 + 50.0 * 8.789693832397461
Epoch 180, val loss: 1.0191441774368286
Epoch 190, training loss: 439.2480163574219 = 1.0164120197296143 + 50.0 * 8.764632225036621
Epoch 190, val loss: 1.016319990158081
Epoch 200, training loss: 438.4004211425781 = 1.013058066368103 + 50.0 * 8.747747421264648
Epoch 200, val loss: 1.0129177570343018
Epoch 210, training loss: 437.6702575683594 = 1.0089884996414185 + 50.0 * 8.733224868774414
Epoch 210, val loss: 1.0088694095611572
Epoch 220, training loss: 436.84521484375 = 1.004850149154663 + 50.0 * 8.71680736541748
Epoch 220, val loss: 1.0047485828399658
Epoch 230, training loss: 435.9429931640625 = 1.0007266998291016 + 50.0 * 8.698844909667969
Epoch 230, val loss: 1.000718355178833
Epoch 240, training loss: 434.84423828125 = 0.9966132044792175 + 50.0 * 8.676952362060547
Epoch 240, val loss: 0.9965837001800537
Epoch 250, training loss: 433.7966003417969 = 0.9922066330909729 + 50.0 * 8.656087875366211
Epoch 250, val loss: 0.9922498464584351
Epoch 260, training loss: 432.9102783203125 = 0.9873117804527283 + 50.0 * 8.638459205627441
Epoch 260, val loss: 0.9874448776245117
Epoch 270, training loss: 432.10992431640625 = 0.98207026720047 + 50.0 * 8.622557640075684
Epoch 270, val loss: 0.9822701811790466
Epoch 280, training loss: 431.4131164550781 = 0.9765656590461731 + 50.0 * 8.608731269836426
Epoch 280, val loss: 0.9768693447113037
Epoch 290, training loss: 430.8119201660156 = 0.9706878066062927 + 50.0 * 8.596824645996094
Epoch 290, val loss: 0.9710985422134399
Epoch 300, training loss: 430.21728515625 = 0.9644944071769714 + 50.0 * 8.585055351257324
Epoch 300, val loss: 0.9650623202323914
Epoch 310, training loss: 429.7056884765625 = 0.957971453666687 + 50.0 * 8.57495403289795
Epoch 310, val loss: 0.958645761013031
Epoch 320, training loss: 429.5025634765625 = 0.9510775804519653 + 50.0 * 8.571029663085938
Epoch 320, val loss: 0.9517992734909058
Epoch 330, training loss: 428.82086181640625 = 0.9435864686965942 + 50.0 * 8.55754566192627
Epoch 330, val loss: 0.9446013569831848
Epoch 340, training loss: 428.4254455566406 = 0.9359145164489746 + 50.0 * 8.549790382385254
Epoch 340, val loss: 0.9371252059936523
Epoch 350, training loss: 428.0289611816406 = 0.9279780983924866 + 50.0 * 8.542019844055176
Epoch 350, val loss: 0.9293469190597534
Epoch 360, training loss: 427.7017822265625 = 0.9197313785552979 + 50.0 * 8.535640716552734
Epoch 360, val loss: 0.9213083386421204
Epoch 370, training loss: 427.3597106933594 = 0.911069393157959 + 50.0 * 8.528972625732422
Epoch 370, val loss: 0.9128315448760986
Epoch 380, training loss: 427.070556640625 = 0.9021613001823425 + 50.0 * 8.523367881774902
Epoch 380, val loss: 0.9041111469268799
Epoch 390, training loss: 426.79180908203125 = 0.8929881453514099 + 50.0 * 8.517976760864258
Epoch 390, val loss: 0.8951806426048279
Epoch 400, training loss: 426.5260925292969 = 0.8835446238517761 + 50.0 * 8.512850761413574
Epoch 400, val loss: 0.8859822154045105
Epoch 410, training loss: 426.4128112792969 = 0.8739255666732788 + 50.0 * 8.510777473449707
Epoch 410, val loss: 0.8765374422073364
Epoch 420, training loss: 426.0560607910156 = 0.8639836311340332 + 50.0 * 8.503841400146484
Epoch 420, val loss: 0.8669416904449463
Epoch 430, training loss: 425.8274230957031 = 0.8541346192359924 + 50.0 * 8.499465942382812
Epoch 430, val loss: 0.8573725819587708
Epoch 440, training loss: 425.5982360839844 = 0.8442600965499878 + 50.0 * 8.495079040527344
Epoch 440, val loss: 0.8478096723556519
Epoch 450, training loss: 425.3836669921875 = 0.8342970609664917 + 50.0 * 8.490987777709961
Epoch 450, val loss: 0.838143527507782
Epoch 460, training loss: 425.3258361816406 = 0.8241265416145325 + 50.0 * 8.490034103393555
Epoch 460, val loss: 0.8283701539039612
Epoch 470, training loss: 424.990478515625 = 0.8140940070152283 + 50.0 * 8.483528137207031
Epoch 470, val loss: 0.8186609148979187
Epoch 480, training loss: 424.7669677734375 = 0.8041513562202454 + 50.0 * 8.479256629943848
Epoch 480, val loss: 0.8090298175811768
Epoch 490, training loss: 424.5748291015625 = 0.7941868305206299 + 50.0 * 8.47561264038086
Epoch 490, val loss: 0.7993873953819275
Epoch 500, training loss: 424.3841552734375 = 0.7843034267425537 + 50.0 * 8.471997261047363
Epoch 500, val loss: 0.7898293733596802
Epoch 510, training loss: 424.2876281738281 = 0.7744371294975281 + 50.0 * 8.470263481140137
Epoch 510, val loss: 0.7802326083183289
Epoch 520, training loss: 424.0951232910156 = 0.7645123600959778 + 50.0 * 8.466611862182617
Epoch 520, val loss: 0.7707422375679016
Epoch 530, training loss: 423.9117431640625 = 0.7548617720603943 + 50.0 * 8.46313762664795
Epoch 530, val loss: 0.7614030241966248
Epoch 540, training loss: 423.79052734375 = 0.7452799081802368 + 50.0 * 8.460905075073242
Epoch 540, val loss: 0.7521098256111145
Epoch 550, training loss: 423.6218566894531 = 0.7357171177864075 + 50.0 * 8.457722663879395
Epoch 550, val loss: 0.7429752349853516
Epoch 560, training loss: 423.498779296875 = 0.7263031005859375 + 50.0 * 8.455450057983398
Epoch 560, val loss: 0.7339762449264526
Epoch 570, training loss: 423.41192626953125 = 0.7170373797416687 + 50.0 * 8.453897476196289
Epoch 570, val loss: 0.725070595741272
Epoch 580, training loss: 423.26708984375 = 0.7079140543937683 + 50.0 * 8.451183319091797
Epoch 580, val loss: 0.7163621783256531
Epoch 590, training loss: 423.43621826171875 = 0.6989458799362183 + 50.0 * 8.454745292663574
Epoch 590, val loss: 0.707878828048706
Epoch 600, training loss: 423.0732421875 = 0.6901740431785583 + 50.0 * 8.447661399841309
Epoch 600, val loss: 0.6994781494140625
Epoch 610, training loss: 422.9773254394531 = 0.6817139387130737 + 50.0 * 8.44591236114502
Epoch 610, val loss: 0.6914159655570984
Epoch 620, training loss: 422.881103515625 = 0.6734542846679688 + 50.0 * 8.44415283203125
Epoch 620, val loss: 0.6835851073265076
Epoch 630, training loss: 422.7760314941406 = 0.6654374003410339 + 50.0 * 8.442212104797363
Epoch 630, val loss: 0.6760419011116028
Epoch 640, training loss: 422.69549560546875 = 0.6576600670814514 + 50.0 * 8.440756797790527
Epoch 640, val loss: 0.6687135696411133
Epoch 650, training loss: 422.6508483886719 = 0.6500466465950012 + 50.0 * 8.44001579284668
Epoch 650, val loss: 0.6615904569625854
Epoch 660, training loss: 422.5657043457031 = 0.6427315473556519 + 50.0 * 8.438459396362305
Epoch 660, val loss: 0.6547070741653442
Epoch 670, training loss: 422.45965576171875 = 0.6357216238975525 + 50.0 * 8.436478614807129
Epoch 670, val loss: 0.6481939554214478
Epoch 680, training loss: 422.3907775878906 = 0.6289317607879639 + 50.0 * 8.435236930847168
Epoch 680, val loss: 0.6419191956520081
Epoch 690, training loss: 422.3034362792969 = 0.6224128007888794 + 50.0 * 8.43362045288086
Epoch 690, val loss: 0.6359331011772156
Epoch 700, training loss: 422.2936096191406 = 0.616122841835022 + 50.0 * 8.433549880981445
Epoch 700, val loss: 0.6301443576812744
Epoch 710, training loss: 422.13787841796875 = 0.6100921630859375 + 50.0 * 8.430556297302246
Epoch 710, val loss: 0.6245855093002319
Epoch 720, training loss: 422.0736083984375 = 0.604324460029602 + 50.0 * 8.4293851852417
Epoch 720, val loss: 0.619285523891449
Epoch 730, training loss: 422.01507568359375 = 0.5987668037414551 + 50.0 * 8.428326606750488
Epoch 730, val loss: 0.6142640709877014
Epoch 740, training loss: 422.0375671386719 = 0.5934306979179382 + 50.0 * 8.428882598876953
Epoch 740, val loss: 0.609388530254364
Epoch 750, training loss: 421.9427185058594 = 0.5882323980331421 + 50.0 * 8.42708969116211
Epoch 750, val loss: 0.6048003435134888
Epoch 760, training loss: 421.8266906738281 = 0.5833098292350769 + 50.0 * 8.424867630004883
Epoch 760, val loss: 0.6004382967948914
Epoch 770, training loss: 421.76385498046875 = 0.5786205530166626 + 50.0 * 8.423705101013184
Epoch 770, val loss: 0.5962721109390259
Epoch 780, training loss: 421.7036437988281 = 0.5741159319877625 + 50.0 * 8.422590255737305
Epoch 780, val loss: 0.5923406481742859
Epoch 790, training loss: 421.6604309082031 = 0.5698027610778809 + 50.0 * 8.421813011169434
Epoch 790, val loss: 0.5886222124099731
Epoch 800, training loss: 421.7453918457031 = 0.5656243562698364 + 50.0 * 8.423595428466797
Epoch 800, val loss: 0.584929883480072
Epoch 810, training loss: 421.62921142578125 = 0.5615717768669128 + 50.0 * 8.42135238647461
Epoch 810, val loss: 0.5815466642379761
Epoch 820, training loss: 421.5325927734375 = 0.5577811002731323 + 50.0 * 8.419496536254883
Epoch 820, val loss: 0.5783380270004272
Epoch 830, training loss: 421.48712158203125 = 0.5541151165962219 + 50.0 * 8.418660163879395
Epoch 830, val loss: 0.5752605199813843
Epoch 840, training loss: 421.4045104980469 = 0.5506109595298767 + 50.0 * 8.417078018188477
Epoch 840, val loss: 0.5723674893379211
Epoch 850, training loss: 421.3551940917969 = 0.547259509563446 + 50.0 * 8.416158676147461
Epoch 850, val loss: 0.5696403980255127
Epoch 860, training loss: 421.4361877441406 = 0.5440233945846558 + 50.0 * 8.417842864990234
Epoch 860, val loss: 0.5670563578605652
Epoch 870, training loss: 421.3699951171875 = 0.5408996343612671 + 50.0 * 8.416582107543945
Epoch 870, val loss: 0.5644291043281555
Epoch 880, training loss: 421.2427673339844 = 0.5379232168197632 + 50.0 * 8.41409683227539
Epoch 880, val loss: 0.5621389746665955
Epoch 890, training loss: 421.18914794921875 = 0.5350906848907471 + 50.0 * 8.413081169128418
Epoch 890, val loss: 0.5599048733711243
Epoch 900, training loss: 421.1375732421875 = 0.5323693156242371 + 50.0 * 8.412103652954102
Epoch 900, val loss: 0.5577824711799622
Epoch 910, training loss: 421.1550598144531 = 0.5297528505325317 + 50.0 * 8.412506103515625
Epoch 910, val loss: 0.5557513236999512
Epoch 920, training loss: 421.0785217285156 = 0.5271655917167664 + 50.0 * 8.411026954650879
Epoch 920, val loss: 0.5538331270217896
Epoch 930, training loss: 421.0436706542969 = 0.5247382521629333 + 50.0 * 8.410378456115723
Epoch 930, val loss: 0.5519989728927612
Epoch 940, training loss: 420.9778137207031 = 0.5223944783210754 + 50.0 * 8.40910816192627
Epoch 940, val loss: 0.5502634644508362
Epoch 950, training loss: 420.9391174316406 = 0.5201419591903687 + 50.0 * 8.408379554748535
Epoch 950, val loss: 0.5486477017402649
Epoch 960, training loss: 421.0757751464844 = 0.5179734230041504 + 50.0 * 8.411155700683594
Epoch 960, val loss: 0.5469318628311157
Epoch 970, training loss: 420.9524230957031 = 0.5157949924468994 + 50.0 * 8.408732414245605
Epoch 970, val loss: 0.5454779267311096
Epoch 980, training loss: 420.86785888671875 = 0.5137292146682739 + 50.0 * 8.407082557678223
Epoch 980, val loss: 0.5441144108772278
Epoch 990, training loss: 420.8009338378906 = 0.5117591619491577 + 50.0 * 8.405783653259277
Epoch 990, val loss: 0.5427225232124329
Epoch 1000, training loss: 420.7749938964844 = 0.5098630785942078 + 50.0 * 8.405303001403809
Epoch 1000, val loss: 0.5414244532585144
Epoch 1010, training loss: 420.8826904296875 = 0.5079658031463623 + 50.0 * 8.40749454498291
Epoch 1010, val loss: 0.5401448011398315
Epoch 1020, training loss: 420.7016296386719 = 0.5061907768249512 + 50.0 * 8.403908729553223
Epoch 1020, val loss: 0.5388743281364441
Epoch 1030, training loss: 420.6407775878906 = 0.5044301152229309 + 50.0 * 8.402727127075195
Epoch 1030, val loss: 0.537733793258667
Epoch 1040, training loss: 420.6042175292969 = 0.502730667591095 + 50.0 * 8.402029991149902
Epoch 1040, val loss: 0.5366459488868713
Epoch 1050, training loss: 420.5931396484375 = 0.5010771155357361 + 50.0 * 8.401841163635254
Epoch 1050, val loss: 0.5355895757675171
Epoch 1060, training loss: 420.6709289550781 = 0.4994332194328308 + 50.0 * 8.403429985046387
Epoch 1060, val loss: 0.5345594882965088
Epoch 1070, training loss: 420.5560607910156 = 0.49785444140434265 + 50.0 * 8.401164054870605
Epoch 1070, val loss: 0.5335028767585754
Epoch 1080, training loss: 420.46978759765625 = 0.4963070750236511 + 50.0 * 8.399469375610352
Epoch 1080, val loss: 0.5325360894203186
Epoch 1090, training loss: 420.43194580078125 = 0.49482256174087524 + 50.0 * 8.39874267578125
Epoch 1090, val loss: 0.5316059589385986
Epoch 1100, training loss: 420.47296142578125 = 0.4933719336986542 + 50.0 * 8.399591445922852
Epoch 1100, val loss: 0.5306919813156128
Epoch 1110, training loss: 420.5113830566406 = 0.491855651140213 + 50.0 * 8.400390625
Epoch 1110, val loss: 0.529774010181427
Epoch 1120, training loss: 420.3739013671875 = 0.49042242765426636 + 50.0 * 8.397669792175293
Epoch 1120, val loss: 0.5289036631584167
Epoch 1130, training loss: 420.3226013183594 = 0.48903778195381165 + 50.0 * 8.396671295166016
Epoch 1130, val loss: 0.5280705094337463
Epoch 1140, training loss: 420.2603454589844 = 0.4876813590526581 + 50.0 * 8.395453453063965
Epoch 1140, val loss: 0.5272839665412903
Epoch 1150, training loss: 420.2281188964844 = 0.4863642454147339 + 50.0 * 8.394835472106934
Epoch 1150, val loss: 0.5265101790428162
Epoch 1160, training loss: 420.1934814453125 = 0.4850696325302124 + 50.0 * 8.39416790008545
Epoch 1160, val loss: 0.5257695317268372
Epoch 1170, training loss: 420.30792236328125 = 0.483783096075058 + 50.0 * 8.396482467651367
Epoch 1170, val loss: 0.5250449776649475
Epoch 1180, training loss: 420.29986572265625 = 0.48246121406555176 + 50.0 * 8.396347999572754
Epoch 1180, val loss: 0.5242230892181396
Epoch 1190, training loss: 420.11865234375 = 0.4812060594558716 + 50.0 * 8.392748832702637
Epoch 1190, val loss: 0.5234909653663635
Epoch 1200, training loss: 420.0777587890625 = 0.4799950122833252 + 50.0 * 8.391955375671387
Epoch 1200, val loss: 0.522854208946228
Epoch 1210, training loss: 420.05853271484375 = 0.478813111782074 + 50.0 * 8.391593933105469
Epoch 1210, val loss: 0.5221397876739502
Epoch 1220, training loss: 420.2405700683594 = 0.4776443839073181 + 50.0 * 8.395258903503418
Epoch 1220, val loss: 0.5214961767196655
Epoch 1230, training loss: 420.0584411621094 = 0.4764227569103241 + 50.0 * 8.391640663146973
Epoch 1230, val loss: 0.5208284854888916
Epoch 1240, training loss: 419.9893493652344 = 0.47529950737953186 + 50.0 * 8.390280723571777
Epoch 1240, val loss: 0.520153284072876
Epoch 1250, training loss: 419.9406433105469 = 0.4741555154323578 + 50.0 * 8.38932991027832
Epoch 1250, val loss: 0.5195778608322144
Epoch 1260, training loss: 419.90582275390625 = 0.4730634093284607 + 50.0 * 8.388655662536621
Epoch 1260, val loss: 0.5189412832260132
Epoch 1270, training loss: 420.1165466308594 = 0.4719616174697876 + 50.0 * 8.392891883850098
Epoch 1270, val loss: 0.5183471441268921
Epoch 1280, training loss: 419.9439392089844 = 0.4708154797554016 + 50.0 * 8.3894624710083
Epoch 1280, val loss: 0.5177342295646667
Epoch 1290, training loss: 419.8374938964844 = 0.4697560667991638 + 50.0 * 8.387354850769043
Epoch 1290, val loss: 0.5171331167221069
Epoch 1300, training loss: 419.80859375 = 0.4686994254589081 + 50.0 * 8.386797904968262
Epoch 1300, val loss: 0.5165508985519409
Epoch 1310, training loss: 419.7792053222656 = 0.46767181158065796 + 50.0 * 8.38623046875
Epoch 1310, val loss: 0.5159763097763062
Epoch 1320, training loss: 419.8451843261719 = 0.4666629135608673 + 50.0 * 8.38757038116455
Epoch 1320, val loss: 0.5154227614402771
Epoch 1330, training loss: 419.7579345703125 = 0.46558335423469543 + 50.0 * 8.385847091674805
Epoch 1330, val loss: 0.5148380398750305
Epoch 1340, training loss: 419.7456970214844 = 0.46458670496940613 + 50.0 * 8.385622024536133
Epoch 1340, val loss: 0.5142434239387512
Epoch 1350, training loss: 419.7089538574219 = 0.4635764956474304 + 50.0 * 8.384907722473145
Epoch 1350, val loss: 0.5137704610824585
Epoch 1360, training loss: 419.66888427734375 = 0.4626321792602539 + 50.0 * 8.384124755859375
Epoch 1360, val loss: 0.5131967663764954
Epoch 1370, training loss: 419.6431579589844 = 0.4616793096065521 + 50.0 * 8.38362979888916
Epoch 1370, val loss: 0.5127052664756775
Epoch 1380, training loss: 419.6302185058594 = 0.46074265241622925 + 50.0 * 8.383389472961426
Epoch 1380, val loss: 0.5121920704841614
Epoch 1390, training loss: 419.8095703125 = 0.45979657769203186 + 50.0 * 8.386995315551758
Epoch 1390, val loss: 0.5116146802902222
Epoch 1400, training loss: 419.6871032714844 = 0.4588296115398407 + 50.0 * 8.384565353393555
Epoch 1400, val loss: 0.5111529231071472
Epoch 1410, training loss: 419.5859375 = 0.45787501335144043 + 50.0 * 8.382560729980469
Epoch 1410, val loss: 0.5106050968170166
Epoch 1420, training loss: 419.5556945800781 = 0.4569694399833679 + 50.0 * 8.381974220275879
Epoch 1420, val loss: 0.5100278854370117
Epoch 1430, training loss: 419.52496337890625 = 0.45606744289398193 + 50.0 * 8.381378173828125
Epoch 1430, val loss: 0.5096153616905212
Epoch 1440, training loss: 419.4938659667969 = 0.45519280433654785 + 50.0 * 8.380773544311523
Epoch 1440, val loss: 0.5091040730476379
Epoch 1450, training loss: 419.4753112792969 = 0.45432209968566895 + 50.0 * 8.380419731140137
Epoch 1450, val loss: 0.5086263418197632
Epoch 1460, training loss: 419.4734802246094 = 0.4534497559070587 + 50.0 * 8.380400657653809
Epoch 1460, val loss: 0.5081638097763062
Epoch 1470, training loss: 419.69097900390625 = 0.4525473117828369 + 50.0 * 8.38476848602295
Epoch 1470, val loss: 0.5076829791069031
Epoch 1480, training loss: 419.52996826171875 = 0.4516504108905792 + 50.0 * 8.381566047668457
Epoch 1480, val loss: 0.5071564316749573
Epoch 1490, training loss: 419.4109802246094 = 0.45078185200691223 + 50.0 * 8.379203796386719
Epoch 1490, val loss: 0.5066400766372681
Epoch 1500, training loss: 419.37554931640625 = 0.44992223381996155 + 50.0 * 8.378512382507324
Epoch 1500, val loss: 0.5061953067779541
Epoch 1510, training loss: 419.3581237792969 = 0.449089914560318 + 50.0 * 8.378180503845215
Epoch 1510, val loss: 0.5057052969932556
Epoch 1520, training loss: 419.3681335449219 = 0.4482511579990387 + 50.0 * 8.378397941589355
Epoch 1520, val loss: 0.5052801966667175
Epoch 1530, training loss: 419.4986877441406 = 0.44738540053367615 + 50.0 * 8.381026268005371
Epoch 1530, val loss: 0.5047556757926941
Epoch 1540, training loss: 419.3745422363281 = 0.44654569029808044 + 50.0 * 8.378560066223145
Epoch 1540, val loss: 0.5043143033981323
Epoch 1550, training loss: 419.2943115234375 = 0.44569504261016846 + 50.0 * 8.376972198486328
Epoch 1550, val loss: 0.5037884712219238
Epoch 1560, training loss: 419.2632141113281 = 0.44488048553466797 + 50.0 * 8.37636661529541
Epoch 1560, val loss: 0.5033817887306213
Epoch 1570, training loss: 419.2526550292969 = 0.4440710246562958 + 50.0 * 8.376172065734863
Epoch 1570, val loss: 0.5029312372207642
Epoch 1580, training loss: 419.3661804199219 = 0.4432530105113983 + 50.0 * 8.378458023071289
Epoch 1580, val loss: 0.5024539828300476
Epoch 1590, training loss: 419.2572021484375 = 0.4424298405647278 + 50.0 * 8.37629508972168
Epoch 1590, val loss: 0.5020034909248352
Epoch 1600, training loss: 419.2622375488281 = 0.4416130483150482 + 50.0 * 8.376412391662598
Epoch 1600, val loss: 0.5015009641647339
Epoch 1610, training loss: 419.1954345703125 = 0.4408014714717865 + 50.0 * 8.375092506408691
Epoch 1610, val loss: 0.5011183619499207
Epoch 1620, training loss: 419.17938232421875 = 0.44000235199928284 + 50.0 * 8.374787330627441
Epoch 1620, val loss: 0.5006142854690552
Epoch 1630, training loss: 419.2028503417969 = 0.4392109215259552 + 50.0 * 8.375272750854492
Epoch 1630, val loss: 0.5001654624938965
Epoch 1640, training loss: 419.1983947753906 = 0.4383977949619293 + 50.0 * 8.375200271606445
Epoch 1640, val loss: 0.49974533915519714
Epoch 1650, training loss: 419.1433410644531 = 0.43757665157318115 + 50.0 * 8.374114990234375
Epoch 1650, val loss: 0.49926337599754333
Epoch 1660, training loss: 419.1166687011719 = 0.4367799758911133 + 50.0 * 8.373598098754883
Epoch 1660, val loss: 0.49879956245422363
Epoch 1670, training loss: 419.0837707519531 = 0.4359932243824005 + 50.0 * 8.372955322265625
Epoch 1670, val loss: 0.4983975291252136
Epoch 1680, training loss: 419.0627136230469 = 0.4352179765701294 + 50.0 * 8.372550010681152
Epoch 1680, val loss: 0.49791741371154785
Epoch 1690, training loss: 419.1220397949219 = 0.4344305396080017 + 50.0 * 8.37375259399414
Epoch 1690, val loss: 0.49748796224594116
Epoch 1700, training loss: 419.1555480957031 = 0.43359410762786865 + 50.0 * 8.374439239501953
Epoch 1700, val loss: 0.49703866243362427
Epoch 1710, training loss: 419.04595947265625 = 0.4327964186668396 + 50.0 * 8.372262954711914
Epoch 1710, val loss: 0.49648523330688477
Epoch 1720, training loss: 419.0080261230469 = 0.43198996782302856 + 50.0 * 8.37152099609375
Epoch 1720, val loss: 0.4960988461971283
Epoch 1730, training loss: 418.98065185546875 = 0.43121522665023804 + 50.0 * 8.370988845825195
Epoch 1730, val loss: 0.4956037104129791
Epoch 1740, training loss: 418.9718322753906 = 0.43043848872184753 + 50.0 * 8.370827674865723
Epoch 1740, val loss: 0.4951663613319397
Epoch 1750, training loss: 419.1341552734375 = 0.42965027689933777 + 50.0 * 8.374090194702148
Epoch 1750, val loss: 0.49463212490081787
Epoch 1760, training loss: 418.99755859375 = 0.42881667613983154 + 50.0 * 8.37137508392334
Epoch 1760, val loss: 0.494279146194458
Epoch 1770, training loss: 418.9939880371094 = 0.42802730202674866 + 50.0 * 8.371318817138672
Epoch 1770, val loss: 0.49375012516975403
Epoch 1780, training loss: 418.96124267578125 = 0.4272194802761078 + 50.0 * 8.370680809020996
Epoch 1780, val loss: 0.4932665228843689
Epoch 1790, training loss: 418.9222717285156 = 0.4264228045940399 + 50.0 * 8.369916915893555
Epoch 1790, val loss: 0.4928472936153412
Epoch 1800, training loss: 418.8910827636719 = 0.42564594745635986 + 50.0 * 8.369308471679688
Epoch 1800, val loss: 0.49239590764045715
Epoch 1810, training loss: 418.87884521484375 = 0.42487213015556335 + 50.0 * 8.36907958984375
Epoch 1810, val loss: 0.49195823073387146
Epoch 1820, training loss: 418.9590148925781 = 0.4240865409374237 + 50.0 * 8.370698928833008
Epoch 1820, val loss: 0.49151062965393066
Epoch 1830, training loss: 418.8882141113281 = 0.4232792258262634 + 50.0 * 8.369298934936523
Epoch 1830, val loss: 0.4910535514354706
Epoch 1840, training loss: 418.8258361816406 = 0.42248937487602234 + 50.0 * 8.368066787719727
Epoch 1840, val loss: 0.4905589520931244
Epoch 1850, training loss: 418.8200988769531 = 0.4217117428779602 + 50.0 * 8.36796760559082
Epoch 1850, val loss: 0.4901149570941925
Epoch 1860, training loss: 418.80810546875 = 0.42094165086746216 + 50.0 * 8.367743492126465
Epoch 1860, val loss: 0.4896591007709503
Epoch 1870, training loss: 418.8907470703125 = 0.4201616644859314 + 50.0 * 8.36941146850586
Epoch 1870, val loss: 0.4892190098762512
Epoch 1880, training loss: 418.7980651855469 = 0.41934892535209656 + 50.0 * 8.367574691772461
Epoch 1880, val loss: 0.4887458086013794
Epoch 1890, training loss: 418.8084411621094 = 0.41855108737945557 + 50.0 * 8.3677978515625
Epoch 1890, val loss: 0.48826900124549866
Epoch 1900, training loss: 418.7546691894531 = 0.4177488088607788 + 50.0 * 8.366738319396973
Epoch 1900, val loss: 0.4877776801586151
Epoch 1910, training loss: 418.728515625 = 0.41696032881736755 + 50.0 * 8.366230964660645
Epoch 1910, val loss: 0.48734304308891296
Epoch 1920, training loss: 418.7127380371094 = 0.4161773920059204 + 50.0 * 8.365931510925293
Epoch 1920, val loss: 0.4868789315223694
Epoch 1930, training loss: 418.76434326171875 = 0.41539010405540466 + 50.0 * 8.366978645324707
Epoch 1930, val loss: 0.4864358603954315
Epoch 1940, training loss: 418.7423095703125 = 0.41456249356269836 + 50.0 * 8.366555213928223
Epoch 1940, val loss: 0.4859400689601898
Epoch 1950, training loss: 418.71490478515625 = 0.4137629270553589 + 50.0 * 8.366023063659668
Epoch 1950, val loss: 0.4855024814605713
Epoch 1960, training loss: 418.7140197753906 = 0.4129554033279419 + 50.0 * 8.366021156311035
Epoch 1960, val loss: 0.4850119352340698
Epoch 1970, training loss: 418.6642150878906 = 0.4121550917625427 + 50.0 * 8.365041732788086
Epoch 1970, val loss: 0.48453864455223083
Epoch 1980, training loss: 418.6251525878906 = 0.41135624051094055 + 50.0 * 8.364275932312012
Epoch 1980, val loss: 0.4840584397315979
Epoch 1990, training loss: 418.6683044433594 = 0.4105646014213562 + 50.0 * 8.365155220031738
Epoch 1990, val loss: 0.48358434438705444
Epoch 2000, training loss: 418.6458435058594 = 0.40973520278930664 + 50.0 * 8.36472225189209
Epoch 2000, val loss: 0.4831503629684448
Epoch 2010, training loss: 418.6049499511719 = 0.4089185297489166 + 50.0 * 8.363921165466309
Epoch 2010, val loss: 0.4825836718082428
Epoch 2020, training loss: 418.5689697265625 = 0.40810418128967285 + 50.0 * 8.3632173538208
Epoch 2020, val loss: 0.48215922713279724
Epoch 2030, training loss: 418.55975341796875 = 0.4073066711425781 + 50.0 * 8.363048553466797
Epoch 2030, val loss: 0.48168453574180603
Epoch 2040, training loss: 418.6236267089844 = 0.4065020680427551 + 50.0 * 8.36434268951416
Epoch 2040, val loss: 0.4811917245388031
Epoch 2050, training loss: 418.61328125 = 0.4056597948074341 + 50.0 * 8.364151954650879
Epoch 2050, val loss: 0.48077598214149475
Epoch 2060, training loss: 418.5181579589844 = 0.4048239290714264 + 50.0 * 8.362266540527344
Epoch 2060, val loss: 0.480202317237854
Epoch 2070, training loss: 418.52398681640625 = 0.40400412678718567 + 50.0 * 8.36240005493164
Epoch 2070, val loss: 0.47972750663757324
Epoch 2080, training loss: 418.5638427734375 = 0.4031844437122345 + 50.0 * 8.363212585449219
Epoch 2080, val loss: 0.4792529344558716
Epoch 2090, training loss: 418.4953918457031 = 0.40235063433647156 + 50.0 * 8.361861228942871
Epoch 2090, val loss: 0.4787709712982178
Epoch 2100, training loss: 418.487060546875 = 0.40153446793556213 + 50.0 * 8.361710548400879
Epoch 2100, val loss: 0.4783012866973877
Epoch 2110, training loss: 418.46478271484375 = 0.40071529150009155 + 50.0 * 8.361281394958496
Epoch 2110, val loss: 0.4778304100036621
Epoch 2120, training loss: 418.5704040527344 = 0.3998875319957733 + 50.0 * 8.363409996032715
Epoch 2120, val loss: 0.47737985849380493
Epoch 2130, training loss: 418.4471740722656 = 0.3990258574485779 + 50.0 * 8.360962867736816
Epoch 2130, val loss: 0.47689446806907654
Epoch 2140, training loss: 418.4243469238281 = 0.39818429946899414 + 50.0 * 8.360523223876953
Epoch 2140, val loss: 0.47644296288490295
Epoch 2150, training loss: 418.4090881347656 = 0.39735648036003113 + 50.0 * 8.360234260559082
Epoch 2150, val loss: 0.4759112000465393
Epoch 2160, training loss: 418.3951110839844 = 0.3965415358543396 + 50.0 * 8.359971046447754
Epoch 2160, val loss: 0.47552087903022766
Epoch 2170, training loss: 418.4161071777344 = 0.39571934938430786 + 50.0 * 8.360407829284668
Epoch 2170, val loss: 0.47506022453308105
Epoch 2180, training loss: 418.47357177734375 = 0.39486122131347656 + 50.0 * 8.361574172973633
Epoch 2180, val loss: 0.4745592176914215
Epoch 2190, training loss: 418.36761474609375 = 0.39400675892829895 + 50.0 * 8.359472274780273
Epoch 2190, val loss: 0.47403398156166077
Epoch 2200, training loss: 418.3674011230469 = 0.39316022396087646 + 50.0 * 8.359484672546387
Epoch 2200, val loss: 0.47359567880630493
Epoch 2210, training loss: 418.47625732421875 = 0.39230892062187195 + 50.0 * 8.361679077148438
Epoch 2210, val loss: 0.47316697239875793
Epoch 2220, training loss: 418.3666687011719 = 0.39145371317863464 + 50.0 * 8.359504699707031
Epoch 2220, val loss: 0.4726090431213379
Epoch 2230, training loss: 418.31842041015625 = 0.3906080424785614 + 50.0 * 8.358556747436523
Epoch 2230, val loss: 0.4722341299057007
Epoch 2240, training loss: 418.2987365722656 = 0.3897739052772522 + 50.0 * 8.358179092407227
Epoch 2240, val loss: 0.4717733561992645
Epoch 2250, training loss: 418.34539794921875 = 0.3889385163784027 + 50.0 * 8.359128952026367
Epoch 2250, val loss: 0.4713172912597656
Epoch 2260, training loss: 418.3981018066406 = 0.3880631625652313 + 50.0 * 8.360200881958008
Epoch 2260, val loss: 0.47081393003463745
Epoch 2270, training loss: 418.31610107421875 = 0.38718584179878235 + 50.0 * 8.3585786819458
Epoch 2270, val loss: 0.4703558683395386
Epoch 2280, training loss: 418.26824951171875 = 0.38632771372795105 + 50.0 * 8.357638359069824
Epoch 2280, val loss: 0.46988847851753235
Epoch 2290, training loss: 418.2471618652344 = 0.38548025488853455 + 50.0 * 8.357234001159668
Epoch 2290, val loss: 0.46947312355041504
Epoch 2300, training loss: 418.2475280761719 = 0.3846355080604553 + 50.0 * 8.357257843017578
Epoch 2300, val loss: 0.4689673185348511
Epoch 2310, training loss: 418.3861999511719 = 0.383774995803833 + 50.0 * 8.360048294067383
Epoch 2310, val loss: 0.4684756398200989
Epoch 2320, training loss: 418.2582702636719 = 0.3828829824924469 + 50.0 * 8.357507705688477
Epoch 2320, val loss: 0.46817561984062195
Epoch 2330, training loss: 418.2033386230469 = 0.3820187747478485 + 50.0 * 8.356426239013672
Epoch 2330, val loss: 0.46764516830444336
Epoch 2340, training loss: 418.1869201660156 = 0.38116806745529175 + 50.0 * 8.356115341186523
Epoch 2340, val loss: 0.46725404262542725
Epoch 2350, training loss: 418.18157958984375 = 0.38032111525535583 + 50.0 * 8.356025695800781
Epoch 2350, val loss: 0.46677884459495544
Epoch 2360, training loss: 418.2729187011719 = 0.37946459650993347 + 50.0 * 8.357869148254395
Epoch 2360, val loss: 0.4663071036338806
Epoch 2370, training loss: 418.3072814941406 = 0.37856778502464294 + 50.0 * 8.358573913574219
Epoch 2370, val loss: 0.4659506678581238
Epoch 2380, training loss: 418.16839599609375 = 0.3776509165763855 + 50.0 * 8.355814933776855
Epoch 2380, val loss: 0.4653691053390503
Epoch 2390, training loss: 418.1610412597656 = 0.376766562461853 + 50.0 * 8.355685234069824
Epoch 2390, val loss: 0.4650217294692993
Epoch 2400, training loss: 418.13250732421875 = 0.37590929865837097 + 50.0 * 8.355132102966309
Epoch 2400, val loss: 0.4644719958305359
Epoch 2410, training loss: 418.1145935058594 = 0.3750571012496948 + 50.0 * 8.354790687561035
Epoch 2410, val loss: 0.4640907943248749
Epoch 2420, training loss: 418.1016845703125 = 0.3742031753063202 + 50.0 * 8.354549407958984
Epoch 2420, val loss: 0.4636646807193756
Epoch 2430, training loss: 418.148681640625 = 0.37334367632865906 + 50.0 * 8.355506896972656
Epoch 2430, val loss: 0.4632498323917389
Epoch 2440, training loss: 418.1969299316406 = 0.3724364638328552 + 50.0 * 8.356490135192871
Epoch 2440, val loss: 0.46277010440826416
Epoch 2450, training loss: 418.10107421875 = 0.3715299367904663 + 50.0 * 8.354591369628906
Epoch 2450, val loss: 0.462321937084198
Epoch 2460, training loss: 418.0694885253906 = 0.37065020203590393 + 50.0 * 8.35397720336914
Epoch 2460, val loss: 0.4617658257484436
Epoch 2470, training loss: 418.0548095703125 = 0.36978277564048767 + 50.0 * 8.353700637817383
Epoch 2470, val loss: 0.461462140083313
Epoch 2480, training loss: 418.0687561035156 = 0.36891788244247437 + 50.0 * 8.353996276855469
Epoch 2480, val loss: 0.4609881341457367
Epoch 2490, training loss: 418.18487548828125 = 0.36802199482917786 + 50.0 * 8.356337547302246
Epoch 2490, val loss: 0.4605623483657837
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8214104515474379
0.8640150691878578
=== training gcn model ===
Epoch 0, training loss: 530.1913452148438 = 1.0790066719055176 + 50.0 * 10.582246780395508
Epoch 0, val loss: 1.0797641277313232
Epoch 10, training loss: 530.1588745117188 = 1.0757144689559937 + 50.0 * 10.581663131713867
Epoch 10, val loss: 1.0764307975769043
Epoch 20, training loss: 530.0152587890625 = 1.0722483396530151 + 50.0 * 10.57886028289795
Epoch 20, val loss: 1.072898030281067
Epoch 30, training loss: 529.3831176757812 = 1.0684337615966797 + 50.0 * 10.566293716430664
Epoch 30, val loss: 1.0689996480941772
Epoch 40, training loss: 527.1201171875 = 1.0641987323760986 + 50.0 * 10.5211181640625
Epoch 40, val loss: 1.0646600723266602
Epoch 50, training loss: 521.2229614257812 = 1.0594199895858765 + 50.0 * 10.40326976776123
Epoch 50, val loss: 1.0597668886184692
Epoch 60, training loss: 510.68536376953125 = 1.0542619228363037 + 50.0 * 10.192622184753418
Epoch 60, val loss: 1.0544909238815308
Epoch 70, training loss: 496.9494934082031 = 1.0480037927627563 + 50.0 * 9.91802978515625
Epoch 70, val loss: 1.0481791496276855
Epoch 80, training loss: 484.77178955078125 = 1.0419543981552124 + 50.0 * 9.674596786499023
Epoch 80, val loss: 1.0424261093139648
Epoch 90, training loss: 474.2084655761719 = 1.0368274450302124 + 50.0 * 9.463432312011719
Epoch 90, val loss: 1.0375845432281494
Epoch 100, training loss: 466.4960021972656 = 1.0325955152511597 + 50.0 * 9.3092679977417
Epoch 100, val loss: 1.033650279045105
Epoch 110, training loss: 463.7798156738281 = 1.0293747186660767 + 50.0 * 9.255008697509766
Epoch 110, val loss: 1.0305854082107544
Epoch 120, training loss: 462.4012451171875 = 1.0264006853103638 + 50.0 * 9.227497100830078
Epoch 120, val loss: 1.0275694131851196
Epoch 130, training loss: 460.34588623046875 = 1.023118257522583 + 50.0 * 9.186454772949219
Epoch 130, val loss: 1.02418851852417
Epoch 140, training loss: 456.96502685546875 = 1.0202686786651611 + 50.0 * 9.118895530700684
Epoch 140, val loss: 1.0213565826416016
Epoch 150, training loss: 451.8568115234375 = 1.0190376043319702 + 50.0 * 9.016755104064941
Epoch 150, val loss: 1.0202542543411255
Epoch 160, training loss: 447.5694274902344 = 1.0189346075057983 + 50.0 * 8.931010246276855
Epoch 160, val loss: 1.020090937614441
Epoch 170, training loss: 445.3841552734375 = 1.0172022581100464 + 50.0 * 8.887338638305664
Epoch 170, val loss: 1.01803719997406
Epoch 180, training loss: 444.36029052734375 = 1.013300895690918 + 50.0 * 8.866939544677734
Epoch 180, val loss: 1.0139777660369873
Epoch 190, training loss: 443.3670349121094 = 1.0090627670288086 + 50.0 * 8.847159385681152
Epoch 190, val loss: 1.0098299980163574
Epoch 200, training loss: 442.3172302246094 = 1.006110429763794 + 50.0 * 8.82622241973877
Epoch 200, val loss: 1.0070834159851074
Epoch 210, training loss: 441.0569763183594 = 1.0040091276168823 + 50.0 * 8.80105972290039
Epoch 210, val loss: 1.0050086975097656
Epoch 220, training loss: 439.6432189941406 = 1.001755714416504 + 50.0 * 8.772829055786133
Epoch 220, val loss: 1.0027282238006592
Epoch 230, training loss: 438.27484130859375 = 0.9994791746139526 + 50.0 * 8.74550724029541
Epoch 230, val loss: 1.0004171133041382
Epoch 240, training loss: 437.052001953125 = 0.9969266653060913 + 50.0 * 8.721101760864258
Epoch 240, val loss: 0.9978286027908325
Epoch 250, training loss: 435.97637939453125 = 0.9939008355140686 + 50.0 * 8.699649810791016
Epoch 250, val loss: 0.9947751760482788
Epoch 260, training loss: 434.8232421875 = 0.9905152916908264 + 50.0 * 8.676654815673828
Epoch 260, val loss: 0.991485595703125
Epoch 270, training loss: 433.8345947265625 = 0.9870033860206604 + 50.0 * 8.656951904296875
Epoch 270, val loss: 0.9881095290184021
Epoch 280, training loss: 432.8711242675781 = 0.9831405878067017 + 50.0 * 8.6377592086792
Epoch 280, val loss: 0.9842582941055298
Epoch 290, training loss: 432.13018798828125 = 0.9787740111351013 + 50.0 * 8.623028755187988
Epoch 290, val loss: 0.979935348033905
Epoch 300, training loss: 431.3283386230469 = 0.9738773107528687 + 50.0 * 8.607089042663574
Epoch 300, val loss: 0.9752135276794434
Epoch 310, training loss: 430.643798828125 = 0.9685541987419128 + 50.0 * 8.593504905700684
Epoch 310, val loss: 0.9700629711151123
Epoch 320, training loss: 430.1551818847656 = 0.9628182053565979 + 50.0 * 8.583847045898438
Epoch 320, val loss: 0.9644505381584167
Epoch 330, training loss: 429.5755310058594 = 0.9566658735275269 + 50.0 * 8.57237720489502
Epoch 330, val loss: 0.9584876894950867
Epoch 340, training loss: 429.0795593261719 = 0.95032799243927 + 50.0 * 8.56258487701416
Epoch 340, val loss: 0.9523119330406189
Epoch 350, training loss: 428.6275634765625 = 0.9437369108200073 + 50.0 * 8.55367660522461
Epoch 350, val loss: 0.9459841251373291
Epoch 360, training loss: 428.1912841796875 = 0.9368804693222046 + 50.0 * 8.545087814331055
Epoch 360, val loss: 0.9392885565757751
Epoch 370, training loss: 427.69342041015625 = 0.9296970963478088 + 50.0 * 8.535274505615234
Epoch 370, val loss: 0.9324007034301758
Epoch 380, training loss: 427.2564392089844 = 0.9223194718360901 + 50.0 * 8.526681900024414
Epoch 380, val loss: 0.9252617955207825
Epoch 390, training loss: 426.96807861328125 = 0.9145642518997192 + 50.0 * 8.52107048034668
Epoch 390, val loss: 0.917753279209137
Epoch 400, training loss: 426.58544921875 = 0.9063870310783386 + 50.0 * 8.513581275939941
Epoch 400, val loss: 0.9098420143127441
Epoch 410, training loss: 426.23687744140625 = 0.8979716897010803 + 50.0 * 8.5067777633667
Epoch 410, val loss: 0.9017497301101685
Epoch 420, training loss: 426.0975646972656 = 0.8892789483070374 + 50.0 * 8.504165649414062
Epoch 420, val loss: 0.8933365345001221
Epoch 430, training loss: 425.734619140625 = 0.8801779747009277 + 50.0 * 8.497088432312012
Epoch 430, val loss: 0.8847306370735168
Epoch 440, training loss: 425.46795654296875 = 0.8711174726486206 + 50.0 * 8.491936683654785
Epoch 440, val loss: 0.8760519027709961
Epoch 450, training loss: 425.2453918457031 = 0.8619023561477661 + 50.0 * 8.487669944763184
Epoch 450, val loss: 0.8672376871109009
Epoch 460, training loss: 425.05841064453125 = 0.8525405526161194 + 50.0 * 8.48411750793457
Epoch 460, val loss: 0.8582879900932312
Epoch 470, training loss: 424.9581298828125 = 0.8430429100990295 + 50.0 * 8.482301712036133
Epoch 470, val loss: 0.8492748737335205
Epoch 480, training loss: 424.7932434082031 = 0.8333739042282104 + 50.0 * 8.47919750213623
Epoch 480, val loss: 0.8401268124580383
Epoch 490, training loss: 424.597412109375 = 0.8237794041633606 + 50.0 * 8.475472450256348
Epoch 490, val loss: 0.8310710787773132
Epoch 500, training loss: 424.4453430175781 = 0.8142909407615662 + 50.0 * 8.472620964050293
Epoch 500, val loss: 0.8220835328102112
Epoch 510, training loss: 424.30621337890625 = 0.8048239946365356 + 50.0 * 8.470027923583984
Epoch 510, val loss: 0.8131698369979858
Epoch 520, training loss: 424.1690673828125 = 0.7954794764518738 + 50.0 * 8.467472076416016
Epoch 520, val loss: 0.8043801188468933
Epoch 530, training loss: 424.08428955078125 = 0.7862340211868286 + 50.0 * 8.465961456298828
Epoch 530, val loss: 0.7957026958465576
Epoch 540, training loss: 424.0398254394531 = 0.7770363688468933 + 50.0 * 8.465255737304688
Epoch 540, val loss: 0.7870578169822693
Epoch 550, training loss: 424.02191162109375 = 0.7680983543395996 + 50.0 * 8.465076446533203
Epoch 550, val loss: 0.7786436676979065
Epoch 560, training loss: 423.73809814453125 = 0.7592806220054626 + 50.0 * 8.459576606750488
Epoch 560, val loss: 0.7705485820770264
Epoch 570, training loss: 423.5999755859375 = 0.7508524060249329 + 50.0 * 8.456982612609863
Epoch 570, val loss: 0.7627159953117371
Epoch 580, training loss: 423.48321533203125 = 0.7426843643188477 + 50.0 * 8.454811096191406
Epoch 580, val loss: 0.7551160454750061
Epoch 590, training loss: 423.4013977050781 = 0.7347031235694885 + 50.0 * 8.453333854675293
Epoch 590, val loss: 0.7477633357048035
Epoch 600, training loss: 423.2650146484375 = 0.7268644571304321 + 50.0 * 8.450762748718262
Epoch 600, val loss: 0.7404317855834961
Epoch 610, training loss: 423.18804931640625 = 0.7193246483802795 + 50.0 * 8.449374198913574
Epoch 610, val loss: 0.7335221171379089
Epoch 620, training loss: 423.0533142089844 = 0.7121550440788269 + 50.0 * 8.446823120117188
Epoch 620, val loss: 0.7268431782722473
Epoch 630, training loss: 423.0992736816406 = 0.7051357626914978 + 50.0 * 8.447882652282715
Epoch 630, val loss: 0.7203381061553955
Epoch 640, training loss: 422.87335205078125 = 0.6982371211051941 + 50.0 * 8.443502426147461
Epoch 640, val loss: 0.7140695452690125
Epoch 650, training loss: 422.74859619140625 = 0.6917344927787781 + 50.0 * 8.441137313842773
Epoch 650, val loss: 0.7080546617507935
Epoch 660, training loss: 422.6425476074219 = 0.6854934692382812 + 50.0 * 8.439141273498535
Epoch 660, val loss: 0.7022800445556641
Epoch 670, training loss: 422.63751220703125 = 0.6794299483299255 + 50.0 * 8.43916130065918
Epoch 670, val loss: 0.696648895740509
Epoch 680, training loss: 422.5126037597656 = 0.673467755317688 + 50.0 * 8.436782836914062
Epoch 680, val loss: 0.6912165880203247
Epoch 690, training loss: 422.373779296875 = 0.6677806377410889 + 50.0 * 8.434120178222656
Epoch 690, val loss: 0.6859984397888184
Epoch 700, training loss: 422.2828369140625 = 0.6622806191444397 + 50.0 * 8.432411193847656
Epoch 700, val loss: 0.6809831857681274
Epoch 710, training loss: 422.54205322265625 = 0.6568896174430847 + 50.0 * 8.437703132629395
Epoch 710, val loss: 0.6760830283164978
Epoch 720, training loss: 422.1205749511719 = 0.6516531109809875 + 50.0 * 8.429378509521484
Epoch 720, val loss: 0.6712124347686768
Epoch 730, training loss: 422.0767822265625 = 0.6467680931091309 + 50.0 * 8.428600311279297
Epoch 730, val loss: 0.6666688919067383
Epoch 740, training loss: 421.972412109375 = 0.6419920325279236 + 50.0 * 8.426608085632324
Epoch 740, val loss: 0.662299633026123
Epoch 750, training loss: 421.90704345703125 = 0.6373850703239441 + 50.0 * 8.425393104553223
Epoch 750, val loss: 0.658118486404419
Epoch 760, training loss: 421.85247802734375 = 0.6329303979873657 + 50.0 * 8.42439079284668
Epoch 760, val loss: 0.6540282368659973
Epoch 770, training loss: 421.9311218261719 = 0.6285159587860107 + 50.0 * 8.42605209350586
Epoch 770, val loss: 0.6500067710876465
Epoch 780, training loss: 421.7494201660156 = 0.6242121458053589 + 50.0 * 8.422504425048828
Epoch 780, val loss: 0.6461101770401001
Epoch 790, training loss: 421.6673583984375 = 0.6202071905136108 + 50.0 * 8.420943260192871
Epoch 790, val loss: 0.6424407362937927
Epoch 800, training loss: 421.6019592285156 = 0.6163046956062317 + 50.0 * 8.419713020324707
Epoch 800, val loss: 0.638922929763794
Epoch 810, training loss: 421.5438537597656 = 0.6125278472900391 + 50.0 * 8.41862678527832
Epoch 810, val loss: 0.6354953646659851
Epoch 820, training loss: 421.4981689453125 = 0.6088771820068359 + 50.0 * 8.41778564453125
Epoch 820, val loss: 0.6321768760681152
Epoch 830, training loss: 421.63665771484375 = 0.6053199768066406 + 50.0 * 8.420626640319824
Epoch 830, val loss: 0.6288811564445496
Epoch 840, training loss: 421.5321044921875 = 0.6017763614654541 + 50.0 * 8.418606758117676
Epoch 840, val loss: 0.6257693767547607
Epoch 850, training loss: 421.3409729003906 = 0.5984064936637878 + 50.0 * 8.414851188659668
Epoch 850, val loss: 0.6227748394012451
Epoch 860, training loss: 421.3222351074219 = 0.5951760411262512 + 50.0 * 8.414541244506836
Epoch 860, val loss: 0.6198676824569702
Epoch 870, training loss: 421.26361083984375 = 0.5920244455337524 + 50.0 * 8.413432121276855
Epoch 870, val loss: 0.617068350315094
Epoch 880, training loss: 421.3003845214844 = 0.5889549851417542 + 50.0 * 8.414228439331055
Epoch 880, val loss: 0.6143767833709717
Epoch 890, training loss: 421.1595153808594 = 0.5859460830688477 + 50.0 * 8.411471366882324
Epoch 890, val loss: 0.6116483807563782
Epoch 900, training loss: 421.1343078613281 = 0.5830618739128113 + 50.0 * 8.411025047302246
Epoch 900, val loss: 0.6091148257255554
Epoch 910, training loss: 421.1362609863281 = 0.5802441835403442 + 50.0 * 8.411120414733887
Epoch 910, val loss: 0.6066996455192566
Epoch 920, training loss: 421.0595703125 = 0.5774632096290588 + 50.0 * 8.409642219543457
Epoch 920, val loss: 0.6042431592941284
Epoch 930, training loss: 421.0078125 = 0.5748201608657837 + 50.0 * 8.408659934997559
Epoch 930, val loss: 0.6019247174263
Epoch 940, training loss: 420.9578857421875 = 0.5722379088401794 + 50.0 * 8.407712936401367
Epoch 940, val loss: 0.5997301936149597
Epoch 950, training loss: 420.9164123535156 = 0.5697358846664429 + 50.0 * 8.406933784484863
Epoch 950, val loss: 0.5975526571273804
Epoch 960, training loss: 421.1087951660156 = 0.5672681331634521 + 50.0 * 8.4108304977417
Epoch 960, val loss: 0.5955153703689575
Epoch 970, training loss: 421.0021667480469 = 0.5647071003913879 + 50.0 * 8.4087495803833
Epoch 970, val loss: 0.5932522416114807
Epoch 980, training loss: 420.8332824707031 = 0.5623527765274048 + 50.0 * 8.405418395996094
Epoch 980, val loss: 0.591288685798645
Epoch 990, training loss: 420.7636413574219 = 0.5600758194923401 + 50.0 * 8.404070854187012
Epoch 990, val loss: 0.58939129114151
Epoch 1000, training loss: 420.734375 = 0.557867169380188 + 50.0 * 8.40353012084961
Epoch 1000, val loss: 0.5875497460365295
Epoch 1010, training loss: 420.6931457519531 = 0.5556799173355103 + 50.0 * 8.402749061584473
Epoch 1010, val loss: 0.5857353210449219
Epoch 1020, training loss: 420.7386779785156 = 0.5535405874252319 + 50.0 * 8.403702735900879
Epoch 1020, val loss: 0.5839627385139465
Epoch 1030, training loss: 420.637939453125 = 0.5513281226158142 + 50.0 * 8.401732444763184
Epoch 1030, val loss: 0.5821942090988159
Epoch 1040, training loss: 420.6347351074219 = 0.5492379665374756 + 50.0 * 8.401710510253906
Epoch 1040, val loss: 0.5804703235626221
Epoch 1050, training loss: 420.5697937011719 = 0.5472054481506348 + 50.0 * 8.40045166015625
Epoch 1050, val loss: 0.5788527727127075
Epoch 1060, training loss: 420.59771728515625 = 0.545219898223877 + 50.0 * 8.401049613952637
Epoch 1060, val loss: 0.5772097706794739
Epoch 1070, training loss: 420.5148620605469 = 0.5432013273239136 + 50.0 * 8.399433135986328
Epoch 1070, val loss: 0.5756222605705261
Epoch 1080, training loss: 420.4786682128906 = 0.5412684082984924 + 50.0 * 8.398748397827148
Epoch 1080, val loss: 0.5741400718688965
Epoch 1090, training loss: 420.4486999511719 = 0.539385199546814 + 50.0 * 8.398185729980469
Epoch 1090, val loss: 0.5726134777069092
Epoch 1100, training loss: 420.4266357421875 = 0.5375213027000427 + 50.0 * 8.397782325744629
Epoch 1100, val loss: 0.5711953043937683
Epoch 1110, training loss: 420.4911193847656 = 0.5356809496879578 + 50.0 * 8.39910888671875
Epoch 1110, val loss: 0.569794237613678
Epoch 1120, training loss: 420.39373779296875 = 0.5338190197944641 + 50.0 * 8.397198677062988
Epoch 1120, val loss: 0.5682339668273926
Epoch 1130, training loss: 420.34869384765625 = 0.5320389270782471 + 50.0 * 8.396332740783691
Epoch 1130, val loss: 0.5668507218360901
Epoch 1140, training loss: 420.305908203125 = 0.5302860140800476 + 50.0 * 8.395512580871582
Epoch 1140, val loss: 0.565453827381134
Epoch 1150, training loss: 420.32843017578125 = 0.5285502076148987 + 50.0 * 8.395997047424316
Epoch 1150, val loss: 0.5640866756439209
Epoch 1160, training loss: 420.32916259765625 = 0.5267882943153381 + 50.0 * 8.396047592163086
Epoch 1160, val loss: 0.5627394318580627
Epoch 1170, training loss: 420.2250671386719 = 0.5250482559204102 + 50.0 * 8.394000053405762
Epoch 1170, val loss: 0.5614383220672607
Epoch 1180, training loss: 420.198974609375 = 0.523371696472168 + 50.0 * 8.393511772155762
Epoch 1180, val loss: 0.560130774974823
Epoch 1190, training loss: 420.4532470703125 = 0.5217100381851196 + 50.0 * 8.39863109588623
Epoch 1190, val loss: 0.5587495565414429
Epoch 1200, training loss: 420.2646789550781 = 0.5199518799781799 + 50.0 * 8.39489459991455
Epoch 1200, val loss: 0.5575949549674988
Epoch 1210, training loss: 420.1310729980469 = 0.5183262825012207 + 50.0 * 8.392254829406738
Epoch 1210, val loss: 0.5562539100646973
Epoch 1220, training loss: 420.0933837890625 = 0.5167222619056702 + 50.0 * 8.391532897949219
Epoch 1220, val loss: 0.5550636053085327
Epoch 1230, training loss: 420.062255859375 = 0.5151563882827759 + 50.0 * 8.390941619873047
Epoch 1230, val loss: 0.5538955330848694
Epoch 1240, training loss: 420.0392761230469 = 0.5135995149612427 + 50.0 * 8.39051342010498
Epoch 1240, val loss: 0.5527348518371582
Epoch 1250, training loss: 420.13653564453125 = 0.5120440125465393 + 50.0 * 8.392489433288574
Epoch 1250, val loss: 0.551623523235321
Epoch 1260, training loss: 420.0334167480469 = 0.5104562640190125 + 50.0 * 8.390459060668945
Epoch 1260, val loss: 0.5502996444702148
Epoch 1270, training loss: 420.03619384765625 = 0.5089124441146851 + 50.0 * 8.390545845031738
Epoch 1270, val loss: 0.5491440296173096
Epoch 1280, training loss: 419.9825134277344 = 0.5073708295822144 + 50.0 * 8.38950252532959
Epoch 1280, val loss: 0.5479819774627686
Epoch 1290, training loss: 419.9456481933594 = 0.5058790445327759 + 50.0 * 8.388794898986816
Epoch 1290, val loss: 0.5468642711639404
Epoch 1300, training loss: 419.90289306640625 = 0.504398763179779 + 50.0 * 8.387969970703125
Epoch 1300, val loss: 0.5457814931869507
Epoch 1310, training loss: 419.9213562011719 = 0.5029494762420654 + 50.0 * 8.388367652893066
Epoch 1310, val loss: 0.544685959815979
Epoch 1320, training loss: 419.97637939453125 = 0.5014453530311584 + 50.0 * 8.389498710632324
Epoch 1320, val loss: 0.5435866117477417
Epoch 1330, training loss: 419.8570251464844 = 0.49996379017829895 + 50.0 * 8.387141227722168
Epoch 1330, val loss: 0.5423214435577393
Epoch 1340, training loss: 419.82159423828125 = 0.49852487444877625 + 50.0 * 8.38646125793457
Epoch 1340, val loss: 0.5412903428077698
Epoch 1350, training loss: 419.7896423339844 = 0.49710795283317566 + 50.0 * 8.38585090637207
Epoch 1350, val loss: 0.5401884913444519
Epoch 1360, training loss: 419.763916015625 = 0.495708703994751 + 50.0 * 8.385364532470703
Epoch 1360, val loss: 0.539130449295044
Epoch 1370, training loss: 419.756103515625 = 0.49431273341178894 + 50.0 * 8.385235786437988
Epoch 1370, val loss: 0.5380421876907349
Epoch 1380, training loss: 420.0662536621094 = 0.492897093296051 + 50.0 * 8.391467094421387
Epoch 1380, val loss: 0.5368306636810303
Epoch 1390, training loss: 419.7776184082031 = 0.4913433790206909 + 50.0 * 8.385725975036621
Epoch 1390, val loss: 0.5357780456542969
Epoch 1400, training loss: 419.73211669921875 = 0.48994290828704834 + 50.0 * 8.384843826293945
Epoch 1400, val loss: 0.5347503423690796
Epoch 1410, training loss: 419.6628723144531 = 0.4885736107826233 + 50.0 * 8.383485794067383
Epoch 1410, val loss: 0.533663272857666
Epoch 1420, training loss: 419.6496887207031 = 0.487220823764801 + 50.0 * 8.383249282836914
Epoch 1420, val loss: 0.5325638055801392
Epoch 1430, training loss: 419.6269836425781 = 0.48587924242019653 + 50.0 * 8.382822036743164
Epoch 1430, val loss: 0.5315308570861816
Epoch 1440, training loss: 419.79766845703125 = 0.4845191538333893 + 50.0 * 8.386262893676758
Epoch 1440, val loss: 0.5303568243980408
Epoch 1450, training loss: 419.652099609375 = 0.48307275772094727 + 50.0 * 8.383380889892578
Epoch 1450, val loss: 0.5294014811515808
Epoch 1460, training loss: 419.6107482910156 = 0.4817141890525818 + 50.0 * 8.382580757141113
Epoch 1460, val loss: 0.5281691551208496
Epoch 1470, training loss: 419.5602111816406 = 0.48036128282546997 + 50.0 * 8.381597518920898
Epoch 1470, val loss: 0.527207612991333
Epoch 1480, training loss: 419.5335998535156 = 0.4790308475494385 + 50.0 * 8.381091117858887
Epoch 1480, val loss: 0.526096522808075
Epoch 1490, training loss: 419.5325927734375 = 0.4777000844478607 + 50.0 * 8.381097793579102
Epoch 1490, val loss: 0.5250871181488037
Epoch 1500, training loss: 419.7068176269531 = 0.47633126378059387 + 50.0 * 8.384610176086426
Epoch 1500, val loss: 0.5239855051040649
Epoch 1510, training loss: 419.5108642578125 = 0.47492966055870056 + 50.0 * 8.380719184875488
Epoch 1510, val loss: 0.5228255391120911
Epoch 1520, training loss: 419.50439453125 = 0.47358670830726624 + 50.0 * 8.380616188049316
Epoch 1520, val loss: 0.5217190384864807
Epoch 1530, training loss: 419.4460754394531 = 0.4722701907157898 + 50.0 * 8.379476547241211
Epoch 1530, val loss: 0.5206784009933472
Epoch 1540, training loss: 419.4314880371094 = 0.4709664583206177 + 50.0 * 8.379210472106934
Epoch 1540, val loss: 0.5196451544761658
Epoch 1550, training loss: 419.41363525390625 = 0.46965643763542175 + 50.0 * 8.37887954711914
Epoch 1550, val loss: 0.5185859799385071
Epoch 1560, training loss: 419.48797607421875 = 0.46833595633506775 + 50.0 * 8.380393028259277
Epoch 1560, val loss: 0.5175718665122986
Epoch 1570, training loss: 419.3999938964844 = 0.46693432331085205 + 50.0 * 8.378661155700684
Epoch 1570, val loss: 0.5164006352424622
Epoch 1580, training loss: 419.365478515625 = 0.4655803143978119 + 50.0 * 8.377998352050781
Epoch 1580, val loss: 0.5152446627616882
Epoch 1590, training loss: 419.3437805175781 = 0.464244544506073 + 50.0 * 8.377591133117676
Epoch 1590, val loss: 0.5142139792442322
Epoch 1600, training loss: 419.3288879394531 = 0.4629180133342743 + 50.0 * 8.3773193359375
Epoch 1600, val loss: 0.5131179094314575
Epoch 1610, training loss: 419.3125305175781 = 0.4615914821624756 + 50.0 * 8.377018928527832
Epoch 1610, val loss: 0.5120624899864197
Epoch 1620, training loss: 419.6008605957031 = 0.4602368474006653 + 50.0 * 8.3828125
Epoch 1620, val loss: 0.5109705924987793
Epoch 1630, training loss: 419.3555603027344 = 0.45879971981048584 + 50.0 * 8.377935409545898
Epoch 1630, val loss: 0.5096986889839172
Epoch 1640, training loss: 419.305908203125 = 0.457428514957428 + 50.0 * 8.376969337463379
Epoch 1640, val loss: 0.508680522441864
Epoch 1650, training loss: 419.24957275390625 = 0.45608964562416077 + 50.0 * 8.375869750976562
Epoch 1650, val loss: 0.5075801610946655
Epoch 1660, training loss: 419.232666015625 = 0.45476943254470825 + 50.0 * 8.375557899475098
Epoch 1660, val loss: 0.5065162181854248
Epoch 1670, training loss: 419.3046569824219 = 0.45344045758247375 + 50.0 * 8.37702465057373
Epoch 1670, val loss: 0.5052809715270996
Epoch 1680, training loss: 419.2063903808594 = 0.45199277997016907 + 50.0 * 8.37508773803711
Epoch 1680, val loss: 0.5042577981948853
Epoch 1690, training loss: 419.2130432128906 = 0.4506063163280487 + 50.0 * 8.375248908996582
Epoch 1690, val loss: 0.5029690265655518
Epoch 1700, training loss: 419.1961364746094 = 0.44924643635749817 + 50.0 * 8.374938011169434
Epoch 1700, val loss: 0.5020356178283691
Epoch 1710, training loss: 419.1577453613281 = 0.447914183139801 + 50.0 * 8.374197006225586
Epoch 1710, val loss: 0.5008283257484436
Epoch 1720, training loss: 419.13885498046875 = 0.4465731978416443 + 50.0 * 8.373846054077148
Epoch 1720, val loss: 0.4998147785663605
Epoch 1730, training loss: 419.219970703125 = 0.4452188313007355 + 50.0 * 8.375494956970215
Epoch 1730, val loss: 0.4987156093120575
Epoch 1740, training loss: 419.114013671875 = 0.44378599524497986 + 50.0 * 8.373404502868652
Epoch 1740, val loss: 0.4975021183490753
Epoch 1750, training loss: 419.1208801269531 = 0.4424022436141968 + 50.0 * 8.37356948852539
Epoch 1750, val loss: 0.4964476227760315
Epoch 1760, training loss: 419.09124755859375 = 0.4410319924354553 + 50.0 * 8.373003959655762
Epoch 1760, val loss: 0.49528953433036804
Epoch 1770, training loss: 419.0647277832031 = 0.43968111276626587 + 50.0 * 8.372501373291016
Epoch 1770, val loss: 0.4942261874675751
Epoch 1780, training loss: 419.052001953125 = 0.43832555413246155 + 50.0 * 8.372273445129395
Epoch 1780, val loss: 0.493122935295105
Epoch 1790, training loss: 419.1636047363281 = 0.4369684159755707 + 50.0 * 8.374532699584961
Epoch 1790, val loss: 0.49197110533714294
Epoch 1800, training loss: 419.1217041015625 = 0.43552395701408386 + 50.0 * 8.373723983764648
Epoch 1800, val loss: 0.4908541440963745
Epoch 1810, training loss: 419.0572814941406 = 0.43412351608276367 + 50.0 * 8.37246322631836
Epoch 1810, val loss: 0.4897838830947876
Epoch 1820, training loss: 418.9975280761719 = 0.4327518343925476 + 50.0 * 8.371295928955078
Epoch 1820, val loss: 0.48869165778160095
Epoch 1830, training loss: 419.00628662109375 = 0.4314027428627014 + 50.0 * 8.371498107910156
Epoch 1830, val loss: 0.48767247796058655
Epoch 1840, training loss: 419.1878356933594 = 0.4300144612789154 + 50.0 * 8.37515640258789
Epoch 1840, val loss: 0.48664751648902893
Epoch 1850, training loss: 419.0113525390625 = 0.4285968244075775 + 50.0 * 8.371655464172363
Epoch 1850, val loss: 0.4853282868862152
Epoch 1860, training loss: 418.95916748046875 = 0.4272245466709137 + 50.0 * 8.370638847351074
Epoch 1860, val loss: 0.4843408465385437
Epoch 1870, training loss: 418.92767333984375 = 0.4258657693862915 + 50.0 * 8.370036125183105
Epoch 1870, val loss: 0.483227014541626
Epoch 1880, training loss: 418.9098205566406 = 0.4245133399963379 + 50.0 * 8.369706153869629
Epoch 1880, val loss: 0.4822291135787964
Epoch 1890, training loss: 418.9453125 = 0.42315351963043213 + 50.0 * 8.370443344116211
Epoch 1890, val loss: 0.48120659589767456
Epoch 1900, training loss: 418.9732360839844 = 0.4217490255832672 + 50.0 * 8.3710298538208
Epoch 1900, val loss: 0.48004239797592163
Epoch 1910, training loss: 418.92071533203125 = 0.42035239934921265 + 50.0 * 8.370007514953613
Epoch 1910, val loss: 0.47878775000572205
Epoch 1920, training loss: 418.86602783203125 = 0.41896939277648926 + 50.0 * 8.368941307067871
Epoch 1920, val loss: 0.47781988978385925
Epoch 1930, training loss: 418.85052490234375 = 0.41762152314186096 + 50.0 * 8.368658065795898
Epoch 1930, val loss: 0.4767085909843445
Epoch 1940, training loss: 418.9217834472656 = 0.41625839471817017 + 50.0 * 8.370110511779785
Epoch 1940, val loss: 0.47567349672317505
Epoch 1950, training loss: 418.8665771484375 = 0.4148388206958771 + 50.0 * 8.369034767150879
Epoch 1950, val loss: 0.474528044462204
Epoch 1960, training loss: 418.8343505859375 = 0.4134516417980194 + 50.0 * 8.368417739868164
Epoch 1960, val loss: 0.4735611081123352
Epoch 1970, training loss: 418.8023986816406 = 0.412080854177475 + 50.0 * 8.367806434631348
Epoch 1970, val loss: 0.47242599725723267
Epoch 1980, training loss: 418.7960510253906 = 0.41072526574134827 + 50.0 * 8.367706298828125
Epoch 1980, val loss: 0.4714447557926178
Epoch 1990, training loss: 418.81915283203125 = 0.4093649685382843 + 50.0 * 8.368195533752441
Epoch 1990, val loss: 0.4704244136810303
Epoch 2000, training loss: 418.7547302246094 = 0.407995343208313 + 50.0 * 8.366934776306152
Epoch 2000, val loss: 0.4694226384162903
Epoch 2010, training loss: 418.7494201660156 = 0.4066389203071594 + 50.0 * 8.36685562133789
Epoch 2010, val loss: 0.46850210428237915
Epoch 2020, training loss: 418.8822326660156 = 0.40527987480163574 + 50.0 * 8.369539260864258
Epoch 2020, val loss: 0.46761375665664673
Epoch 2030, training loss: 418.8162841796875 = 0.4038580060005188 + 50.0 * 8.368247985839844
Epoch 2030, val loss: 0.466404527425766
Epoch 2040, training loss: 418.7335510253906 = 0.4024840295314789 + 50.0 * 8.366621017456055
Epoch 2040, val loss: 0.46541574597358704
Epoch 2050, training loss: 418.69146728515625 = 0.4011290669441223 + 50.0 * 8.365806579589844
Epoch 2050, val loss: 0.4643842279911041
Epoch 2060, training loss: 418.67864990234375 = 0.3997972905635834 + 50.0 * 8.36557674407959
Epoch 2060, val loss: 0.463443785905838
Epoch 2070, training loss: 418.7729187011719 = 0.3984529674053192 + 50.0 * 8.367488861083984
Epoch 2070, val loss: 0.46236327290534973
Epoch 2080, training loss: 418.7055969238281 = 0.39705824851989746 + 50.0 * 8.366170883178711
Epoch 2080, val loss: 0.46146997809410095
Epoch 2090, training loss: 418.63641357421875 = 0.3956928551197052 + 50.0 * 8.364814758300781
Epoch 2090, val loss: 0.4605512320995331
Epoch 2100, training loss: 418.6234130859375 = 0.3943515419960022 + 50.0 * 8.364581108093262
Epoch 2100, val loss: 0.4595881998538971
Epoch 2110, training loss: 418.61773681640625 = 0.39301884174346924 + 50.0 * 8.364494323730469
Epoch 2110, val loss: 0.45866602659225464
Epoch 2120, training loss: 418.6625671386719 = 0.3916793167591095 + 50.0 * 8.36541748046875
Epoch 2120, val loss: 0.4577879309654236
Epoch 2130, training loss: 418.6768493652344 = 0.39029985666275024 + 50.0 * 8.365731239318848
Epoch 2130, val loss: 0.4568621516227722
Epoch 2140, training loss: 418.6156311035156 = 0.38894104957580566 + 50.0 * 8.364533424377441
Epoch 2140, val loss: 0.45574215054512024
Epoch 2150, training loss: 418.6081848144531 = 0.38759294152259827 + 50.0 * 8.364411354064941
Epoch 2150, val loss: 0.45493775606155396
Epoch 2160, training loss: 418.67108154296875 = 0.3862437903881073 + 50.0 * 8.365696907043457
Epoch 2160, val loss: 0.4540092349052429
Epoch 2170, training loss: 418.5521545410156 = 0.38490259647369385 + 50.0 * 8.3633451461792
Epoch 2170, val loss: 0.45309892296791077
Epoch 2180, training loss: 418.5309143066406 = 0.38357725739479065 + 50.0 * 8.362946510314941
Epoch 2180, val loss: 0.45227673649787903
Epoch 2190, training loss: 418.5152893066406 = 0.38226622343063354 + 50.0 * 8.36266040802002
Epoch 2190, val loss: 0.4514239430427551
Epoch 2200, training loss: 418.6146240234375 = 0.38094887137413025 + 50.0 * 8.364673614501953
Epoch 2200, val loss: 0.45063352584838867
Epoch 2210, training loss: 418.5672607421875 = 0.37961024045944214 + 50.0 * 8.363753318786621
Epoch 2210, val loss: 0.44969409704208374
Epoch 2220, training loss: 418.49578857421875 = 0.3782718777656555 + 50.0 * 8.362350463867188
Epoch 2220, val loss: 0.4487873613834381
Epoch 2230, training loss: 418.4751892089844 = 0.3769741356372833 + 50.0 * 8.361964225769043
Epoch 2230, val loss: 0.4479241967201233
Epoch 2240, training loss: 418.51080322265625 = 0.37567558884620667 + 50.0 * 8.362702369689941
Epoch 2240, val loss: 0.4471008777618408
Epoch 2250, training loss: 418.5183410644531 = 0.3743560016155243 + 50.0 * 8.362879753112793
Epoch 2250, val loss: 0.44605502486228943
Epoch 2260, training loss: 418.4685363769531 = 0.37303420901298523 + 50.0 * 8.361909866333008
Epoch 2260, val loss: 0.44521352648735046
Epoch 2270, training loss: 418.4182434082031 = 0.3717329204082489 + 50.0 * 8.360930442810059
Epoch 2270, val loss: 0.44447872042655945
Epoch 2280, training loss: 418.4122619628906 = 0.3704501986503601 + 50.0 * 8.360836029052734
Epoch 2280, val loss: 0.44372087717056274
Epoch 2290, training loss: 418.4033508300781 = 0.3691706657409668 + 50.0 * 8.36068344116211
Epoch 2290, val loss: 0.4429188668727875
Epoch 2300, training loss: 418.5003356933594 = 0.36788210272789 + 50.0 * 8.362648963928223
Epoch 2300, val loss: 0.4422115087509155
Epoch 2310, training loss: 418.3868103027344 = 0.36654555797576904 + 50.0 * 8.360404968261719
Epoch 2310, val loss: 0.4411401152610779
Epoch 2320, training loss: 418.44622802734375 = 0.3652380108833313 + 50.0 * 8.36161994934082
Epoch 2320, val loss: 0.44029757380485535
Epoch 2330, training loss: 418.3590087890625 = 0.3639417588710785 + 50.0 * 8.359901428222656
Epoch 2330, val loss: 0.4395847022533417
Epoch 2340, training loss: 418.334716796875 = 0.36267584562301636 + 50.0 * 8.359440803527832
Epoch 2340, val loss: 0.4387921392917633
Epoch 2350, training loss: 418.332275390625 = 0.3614259362220764 + 50.0 * 8.359416961669922
Epoch 2350, val loss: 0.43808361887931824
Epoch 2360, training loss: 418.38714599609375 = 0.36017054319381714 + 50.0 * 8.360539436340332
Epoch 2360, val loss: 0.43738269805908203
Epoch 2370, training loss: 418.3669738769531 = 0.3588981628417969 + 50.0 * 8.360161781311035
Epoch 2370, val loss: 0.43661969900131226
Epoch 2380, training loss: 418.3729248046875 = 0.35763487219810486 + 50.0 * 8.360305786132812
Epoch 2380, val loss: 0.43575310707092285
Epoch 2390, training loss: 418.2806091308594 = 0.35636189579963684 + 50.0 * 8.358485221862793
Epoch 2390, val loss: 0.4351482689380646
Epoch 2400, training loss: 418.271728515625 = 0.3551226556301117 + 50.0 * 8.358331680297852
Epoch 2400, val loss: 0.4343593120574951
Epoch 2410, training loss: 418.2934875488281 = 0.3538925051689148 + 50.0 * 8.358792304992676
Epoch 2410, val loss: 0.4336009919643402
Epoch 2420, training loss: 418.3807373046875 = 0.3526484966278076 + 50.0 * 8.36056137084961
Epoch 2420, val loss: 0.43280109763145447
Epoch 2430, training loss: 418.2503967285156 = 0.3514014482498169 + 50.0 * 8.357979774475098
Epoch 2430, val loss: 0.43240994215011597
Epoch 2440, training loss: 418.2220764160156 = 0.3501726984977722 + 50.0 * 8.357438087463379
Epoch 2440, val loss: 0.4316616952419281
Epoch 2450, training loss: 418.2254943847656 = 0.34896835684776306 + 50.0 * 8.35753059387207
Epoch 2450, val loss: 0.43094825744628906
Epoch 2460, training loss: 418.33056640625 = 0.3477489650249481 + 50.0 * 8.35965633392334
Epoch 2460, val loss: 0.4303893446922302
Epoch 2470, training loss: 418.24908447265625 = 0.3465137481689453 + 50.0 * 8.358051300048828
Epoch 2470, val loss: 0.4299831986427307
Epoch 2480, training loss: 418.18817138671875 = 0.3453006446361542 + 50.0 * 8.356857299804688
Epoch 2480, val loss: 0.42915645241737366
Epoch 2490, training loss: 418.1996765136719 = 0.3440978229045868 + 50.0 * 8.357111930847168
Epoch 2490, val loss: 0.42856207489967346
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8356164383561644
0.8639426211693111
The final CL Acc:0.82919, 0.00588, The final GNN Acc:0.86375, 0.00033
Begin epxeriment: noisy_level: 0.2 cont_weight: 50 epoch: der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106358])
remove edge: torch.Size([2, 70738])
updated graph: torch.Size([2, 88448])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 530.212646484375 = 1.096678614616394 + 50.0 * 10.582319259643555
Epoch 0, val loss: 1.0984551906585693
Epoch 10, training loss: 530.1984252929688 = 1.0922001600265503 + 50.0 * 10.582123756408691
Epoch 10, val loss: 1.093937873840332
Epoch 20, training loss: 530.1521606445312 = 1.087320327758789 + 50.0 * 10.581296920776367
Epoch 20, val loss: 1.0889867544174194
Epoch 30, training loss: 529.9514770507812 = 1.0819860696792603 + 50.0 * 10.577390670776367
Epoch 30, val loss: 1.0835808515548706
Epoch 40, training loss: 529.1190795898438 = 1.0761483907699585 + 50.0 * 10.560857772827148
Epoch 40, val loss: 1.0776340961456299
Epoch 50, training loss: 526.4629516601562 = 1.0695499181747437 + 50.0 * 10.507867813110352
Epoch 50, val loss: 1.0708805322647095
Epoch 60, training loss: 520.3391723632812 = 1.0625356435775757 + 50.0 * 10.38553237915039
Epoch 60, val loss: 1.0638025999069214
Epoch 70, training loss: 510.6380920410156 = 1.0559488534927368 + 50.0 * 10.191642761230469
Epoch 70, val loss: 1.0570602416992188
Epoch 80, training loss: 504.9446716308594 = 1.0502136945724487 + 50.0 * 10.077889442443848
Epoch 80, val loss: 1.0513094663619995
Epoch 90, training loss: 494.7431640625 = 1.046046495437622 + 50.0 * 9.873942375183105
Epoch 90, val loss: 1.0471339225769043
Epoch 100, training loss: 479.9849853515625 = 1.0432839393615723 + 50.0 * 9.578834533691406
Epoch 100, val loss: 1.0445098876953125
Epoch 110, training loss: 465.9453430175781 = 1.0419018268585205 + 50.0 * 9.29806900024414
Epoch 110, val loss: 1.0432385206222534
Epoch 120, training loss: 458.1737976074219 = 1.0411362648010254 + 50.0 * 9.142653465270996
Epoch 120, val loss: 1.042380452156067
Epoch 130, training loss: 456.7278137207031 = 1.037989854812622 + 50.0 * 9.11379623413086
Epoch 130, val loss: 1.0390230417251587
Epoch 140, training loss: 455.1413879394531 = 1.0341023206710815 + 50.0 * 9.082145690917969
Epoch 140, val loss: 1.0351310968399048
Epoch 150, training loss: 453.7533874511719 = 1.0310256481170654 + 50.0 * 9.054447174072266
Epoch 150, val loss: 1.0320621728897095
Epoch 160, training loss: 452.1220703125 = 1.0283671617507935 + 50.0 * 9.021873474121094
Epoch 160, val loss: 1.0294382572174072
Epoch 170, training loss: 449.9247131347656 = 1.026034951210022 + 50.0 * 8.977973937988281
Epoch 170, val loss: 1.0271955728530884
Epoch 180, training loss: 447.3612976074219 = 1.023848533630371 + 50.0 * 8.926749229431152
Epoch 180, val loss: 1.0250533819198608
Epoch 190, training loss: 445.2711486816406 = 1.0214183330535889 + 50.0 * 8.884994506835938
Epoch 190, val loss: 1.0226914882659912
Epoch 200, training loss: 443.42889404296875 = 1.0186275243759155 + 50.0 * 8.84820556640625
Epoch 200, val loss: 1.0199497938156128
Epoch 210, training loss: 441.7716979980469 = 1.015522837638855 + 50.0 * 8.815123558044434
Epoch 210, val loss: 1.0169192552566528
Epoch 220, training loss: 440.5197448730469 = 1.0119441747665405 + 50.0 * 8.790156364440918
Epoch 220, val loss: 1.0134015083312988
Epoch 230, training loss: 439.5781555175781 = 1.0077629089355469 + 50.0 * 8.771408081054688
Epoch 230, val loss: 1.0093051195144653
Epoch 240, training loss: 438.91778564453125 = 1.0031858682632446 + 50.0 * 8.758292198181152
Epoch 240, val loss: 1.0048240423202515
Epoch 250, training loss: 438.3759765625 = 0.9984027743339539 + 50.0 * 8.747550964355469
Epoch 250, val loss: 1.0001543760299683
Epoch 260, training loss: 437.98126220703125 = 0.9934805631637573 + 50.0 * 8.739755630493164
Epoch 260, val loss: 0.9953381419181824
Epoch 270, training loss: 437.3267517089844 = 0.9882928729057312 + 50.0 * 8.72676944732666
Epoch 270, val loss: 0.9902987480163574
Epoch 280, training loss: 436.73834228515625 = 0.98284912109375 + 50.0 * 8.715109825134277
Epoch 280, val loss: 0.9850322604179382
Epoch 290, training loss: 436.143798828125 = 0.9771072268486023 + 50.0 * 8.703333854675293
Epoch 290, val loss: 0.9794570207595825
Epoch 300, training loss: 435.59521484375 = 0.9710116386413574 + 50.0 * 8.692483901977539
Epoch 300, val loss: 0.973561704158783
Epoch 310, training loss: 435.1510925292969 = 0.9645388722419739 + 50.0 * 8.683731079101562
Epoch 310, val loss: 0.9673166275024414
Epoch 320, training loss: 434.6766052246094 = 0.9576748013496399 + 50.0 * 8.674378395080566
Epoch 320, val loss: 0.9606728553771973
Epoch 330, training loss: 434.29339599609375 = 0.9504210352897644 + 50.0 * 8.66685962677002
Epoch 330, val loss: 0.9537102580070496
Epoch 340, training loss: 433.9093933105469 = 0.9428262114524841 + 50.0 * 8.659331321716309
Epoch 340, val loss: 0.9463896751403809
Epoch 350, training loss: 433.569091796875 = 0.9349498748779297 + 50.0 * 8.65268325805664
Epoch 350, val loss: 0.9388414025306702
Epoch 360, training loss: 433.30157470703125 = 0.9267455339431763 + 50.0 * 8.647496223449707
Epoch 360, val loss: 0.93099045753479
Epoch 370, training loss: 432.9053649902344 = 0.9182159900665283 + 50.0 * 8.639742851257324
Epoch 370, val loss: 0.9228166341781616
Epoch 380, training loss: 432.61102294921875 = 0.9094655513763428 + 50.0 * 8.634031295776367
Epoch 380, val loss: 0.9144414663314819
Epoch 390, training loss: 432.2946472167969 = 0.9004668593406677 + 50.0 * 8.627883911132812
Epoch 390, val loss: 0.9058964252471924
Epoch 400, training loss: 431.9502258300781 = 0.891311764717102 + 50.0 * 8.621177673339844
Epoch 400, val loss: 0.8971733450889587
Epoch 410, training loss: 431.6851806640625 = 0.8820154070854187 + 50.0 * 8.616063117980957
Epoch 410, val loss: 0.8883503079414368
Epoch 420, training loss: 431.4547119140625 = 0.8725483417510986 + 50.0 * 8.611642837524414
Epoch 420, val loss: 0.8793795108795166
Epoch 430, training loss: 431.18231201171875 = 0.8630099892616272 + 50.0 * 8.606386184692383
Epoch 430, val loss: 0.8703798055648804
Epoch 440, training loss: 430.9616394042969 = 0.8534277081489563 + 50.0 * 8.602164268493652
Epoch 440, val loss: 0.8613518476486206
Epoch 450, training loss: 430.9347229003906 = 0.8438205718994141 + 50.0 * 8.601818084716797
Epoch 450, val loss: 0.8523479700088501
Epoch 460, training loss: 430.6415100097656 = 0.8342194557189941 + 50.0 * 8.596145629882812
Epoch 460, val loss: 0.843325674533844
Epoch 470, training loss: 430.4839172363281 = 0.82472163438797 + 50.0 * 8.593184471130371
Epoch 470, val loss: 0.83445143699646
Epoch 480, training loss: 430.3316650390625 = 0.8153406977653503 + 50.0 * 8.590326309204102
Epoch 480, val loss: 0.8257365822792053
Epoch 490, training loss: 430.2437438964844 = 0.8061093091964722 + 50.0 * 8.588752746582031
Epoch 490, val loss: 0.8172006011009216
Epoch 500, training loss: 430.07342529296875 = 0.7970448136329651 + 50.0 * 8.585527420043945
Epoch 500, val loss: 0.8088003993034363
Epoch 510, training loss: 429.9249572753906 = 0.7882176637649536 + 50.0 * 8.582735061645508
Epoch 510, val loss: 0.8007017970085144
Epoch 520, training loss: 429.7844543457031 = 0.7796303033828735 + 50.0 * 8.580096244812012
Epoch 520, val loss: 0.792809247970581
Epoch 530, training loss: 429.66656494140625 = 0.7712691426277161 + 50.0 * 8.577905654907227
Epoch 530, val loss: 0.785184919834137
Epoch 540, training loss: 429.54132080078125 = 0.7631499171257019 + 50.0 * 8.575563430786133
Epoch 540, val loss: 0.7777915596961975
Epoch 550, training loss: 429.39739990234375 = 0.7553240656852722 + 50.0 * 8.57284164428711
Epoch 550, val loss: 0.7707087397575378
Epoch 560, training loss: 429.2730712890625 = 0.7477632164955139 + 50.0 * 8.57050609588623
Epoch 560, val loss: 0.7638968825340271
Epoch 570, training loss: 429.31500244140625 = 0.7404455542564392 + 50.0 * 8.571491241455078
Epoch 570, val loss: 0.7573658227920532
Epoch 580, training loss: 429.1179504394531 = 0.7333121299743652 + 50.0 * 8.567692756652832
Epoch 580, val loss: 0.7509407997131348
Epoch 590, training loss: 428.995361328125 = 0.7264912128448486 + 50.0 * 8.565377235412598
Epoch 590, val loss: 0.7448826432228088
Epoch 600, training loss: 428.8813171386719 = 0.7199466228485107 + 50.0 * 8.563227653503418
Epoch 600, val loss: 0.739077627658844
Epoch 610, training loss: 428.7763366699219 = 0.7136557102203369 + 50.0 * 8.561253547668457
Epoch 610, val loss: 0.7335349917411804
Epoch 620, training loss: 428.79473876953125 = 0.7075981497764587 + 50.0 * 8.561742782592773
Epoch 620, val loss: 0.7282542586326599
Epoch 630, training loss: 428.6051330566406 = 0.7017561197280884 + 50.0 * 8.558067321777344
Epoch 630, val loss: 0.7230834364891052
Epoch 640, training loss: 428.48345947265625 = 0.6961880922317505 + 50.0 * 8.555745124816895
Epoch 640, val loss: 0.7182565331459045
Epoch 650, training loss: 428.4177551269531 = 0.6908719539642334 + 50.0 * 8.554537773132324
Epoch 650, val loss: 0.7136570811271667
Epoch 660, training loss: 428.33123779296875 = 0.6857269406318665 + 50.0 * 8.552909851074219
Epoch 660, val loss: 0.7092432975769043
Epoch 670, training loss: 428.1558532714844 = 0.680828332901001 + 50.0 * 8.549500465393066
Epoch 670, val loss: 0.7050938010215759
Epoch 680, training loss: 428.05218505859375 = 0.6761429309844971 + 50.0 * 8.547520637512207
Epoch 680, val loss: 0.7011354565620422
Epoch 690, training loss: 428.0732727050781 = 0.6716421842575073 + 50.0 * 8.548032760620117
Epoch 690, val loss: 0.697357177734375
Epoch 700, training loss: 427.8725280761719 = 0.6672860980033875 + 50.0 * 8.54410457611084
Epoch 700, val loss: 0.6936243772506714
Epoch 710, training loss: 427.7524108886719 = 0.6631219983100891 + 50.0 * 8.541786193847656
Epoch 710, val loss: 0.6902110576629639
Epoch 720, training loss: 427.66790771484375 = 0.6591012477874756 + 50.0 * 8.540176391601562
Epoch 720, val loss: 0.6868374347686768
Epoch 730, training loss: 427.5992736816406 = 0.655214250087738 + 50.0 * 8.538881301879883
Epoch 730, val loss: 0.6836473941802979
Epoch 740, training loss: 427.4535827636719 = 0.6514574885368347 + 50.0 * 8.536042213439941
Epoch 740, val loss: 0.6806156039237976
Epoch 750, training loss: 427.3579406738281 = 0.6478440165519714 + 50.0 * 8.534201622009277
Epoch 750, val loss: 0.6776537299156189
Epoch 760, training loss: 427.5107421875 = 0.644353985786438 + 50.0 * 8.537327766418457
Epoch 760, val loss: 0.6748003363609314
Epoch 770, training loss: 427.2215881347656 = 0.6409409642219543 + 50.0 * 8.53161334991455
Epoch 770, val loss: 0.6721113324165344
Epoch 780, training loss: 427.13720703125 = 0.6376950144767761 + 50.0 * 8.529990196228027
Epoch 780, val loss: 0.6695395708084106
Epoch 790, training loss: 427.0566101074219 = 0.6345751285552979 + 50.0 * 8.528440475463867
Epoch 790, val loss: 0.6670381426811218
Epoch 800, training loss: 426.9867858886719 = 0.6315680742263794 + 50.0 * 8.527104377746582
Epoch 800, val loss: 0.6646532416343689
Epoch 810, training loss: 426.97900390625 = 0.628668487071991 + 50.0 * 8.527007102966309
Epoch 810, val loss: 0.6624354124069214
Epoch 820, training loss: 426.9820251464844 = 0.6257957816123962 + 50.0 * 8.527124404907227
Epoch 820, val loss: 0.6600152254104614
Epoch 830, training loss: 426.8252868652344 = 0.6230500936508179 + 50.0 * 8.52404499053955
Epoch 830, val loss: 0.6579554080963135
Epoch 840, training loss: 426.7491149902344 = 0.6204382181167603 + 50.0 * 8.522573471069336
Epoch 840, val loss: 0.6559489965438843
Epoch 850, training loss: 426.68121337890625 = 0.6179295778274536 + 50.0 * 8.521265983581543
Epoch 850, val loss: 0.654019832611084
Epoch 860, training loss: 426.6154479980469 = 0.615506649017334 + 50.0 * 8.519998550415039
Epoch 860, val loss: 0.6521831154823303
Epoch 870, training loss: 426.6138000488281 = 0.6131629347801208 + 50.0 * 8.520012855529785
Epoch 870, val loss: 0.6503786444664001
Epoch 880, training loss: 426.574951171875 = 0.610861599445343 + 50.0 * 8.519281387329102
Epoch 880, val loss: 0.6486372351646423
Epoch 890, training loss: 426.4320983886719 = 0.608644425868988 + 50.0 * 8.51646900177002
Epoch 890, val loss: 0.6469964981079102
Epoch 900, training loss: 426.3717956542969 = 0.6065154075622559 + 50.0 * 8.515305519104004
Epoch 900, val loss: 0.6454009413719177
Epoch 910, training loss: 426.3768005371094 = 0.6044629216194153 + 50.0 * 8.515446662902832
Epoch 910, val loss: 0.6439346671104431
Epoch 920, training loss: 426.3002624511719 = 0.6024243235588074 + 50.0 * 8.513957023620605
Epoch 920, val loss: 0.6423301696777344
Epoch 930, training loss: 426.19757080078125 = 0.6004598736763 + 50.0 * 8.511941909790039
Epoch 930, val loss: 0.640895664691925
Epoch 940, training loss: 426.1216735839844 = 0.5985690951347351 + 50.0 * 8.510461807250977
Epoch 940, val loss: 0.6395174860954285
Epoch 950, training loss: 426.0619812011719 = 0.5967321395874023 + 50.0 * 8.509305000305176
Epoch 950, val loss: 0.6382007598876953
Epoch 960, training loss: 426.28472900390625 = 0.5949450731277466 + 50.0 * 8.513795852661133
Epoch 960, val loss: 0.6370249390602112
Epoch 970, training loss: 425.97320556640625 = 0.5931044220924377 + 50.0 * 8.507601737976074
Epoch 970, val loss: 0.6355571150779724
Epoch 980, training loss: 425.9072265625 = 0.5913663506507874 + 50.0 * 8.506317138671875
Epoch 980, val loss: 0.634304940700531
Epoch 990, training loss: 425.8404235839844 = 0.5896911025047302 + 50.0 * 8.505014419555664
Epoch 990, val loss: 0.6330578923225403
Epoch 1000, training loss: 425.87213134765625 = 0.5880470275878906 + 50.0 * 8.505681991577148
Epoch 1000, val loss: 0.6318466663360596
Epoch 1010, training loss: 425.76336669921875 = 0.5864118337631226 + 50.0 * 8.503539085388184
Epoch 1010, val loss: 0.6307620406150818
Epoch 1020, training loss: 425.7032165527344 = 0.5848102569580078 + 50.0 * 8.502367973327637
Epoch 1020, val loss: 0.629586398601532
Epoch 1030, training loss: 425.6517639160156 = 0.5832594037055969 + 50.0 * 8.501370429992676
Epoch 1030, val loss: 0.6285399794578552
Epoch 1040, training loss: 425.7056579589844 = 0.5817193388938904 + 50.0 * 8.50247859954834
Epoch 1040, val loss: 0.6274666786193848
Epoch 1050, training loss: 425.56256103515625 = 0.5801759958267212 + 50.0 * 8.499648094177246
Epoch 1050, val loss: 0.6263477802276611
Epoch 1060, training loss: 425.4859924316406 = 0.5787091255187988 + 50.0 * 8.498146057128906
Epoch 1060, val loss: 0.625318706035614
Epoch 1070, training loss: 425.44342041015625 = 0.5772769451141357 + 50.0 * 8.497323036193848
Epoch 1070, val loss: 0.6243643164634705
Epoch 1080, training loss: 425.4222412109375 = 0.5758636593818665 + 50.0 * 8.496927261352539
Epoch 1080, val loss: 0.6234480738639832
Epoch 1090, training loss: 425.3875732421875 = 0.5744467973709106 + 50.0 * 8.496262550354004
Epoch 1090, val loss: 0.6224425435066223
Epoch 1100, training loss: 425.3157653808594 = 0.573041558265686 + 50.0 * 8.494854927062988
Epoch 1100, val loss: 0.621479332447052
Epoch 1110, training loss: 425.2731018066406 = 0.5716827511787415 + 50.0 * 8.494028091430664
Epoch 1110, val loss: 0.6205107569694519
Epoch 1120, training loss: 425.223876953125 = 0.5703475475311279 + 50.0 * 8.493070602416992
Epoch 1120, val loss: 0.6196481585502625
Epoch 1130, training loss: 425.31707763671875 = 0.5690187215805054 + 50.0 * 8.49496078491211
Epoch 1130, val loss: 0.6186603307723999
Epoch 1140, training loss: 425.306640625 = 0.5676541328430176 + 50.0 * 8.494779586791992
Epoch 1140, val loss: 0.6178125143051147
Epoch 1150, training loss: 425.1562805175781 = 0.5663172006607056 + 50.0 * 8.491799354553223
Epoch 1150, val loss: 0.6169321537017822
Epoch 1160, training loss: 425.0692138671875 = 0.5650308132171631 + 50.0 * 8.490083694458008
Epoch 1160, val loss: 0.6160444021224976
Epoch 1170, training loss: 425.01605224609375 = 0.5637674927711487 + 50.0 * 8.489045143127441
Epoch 1170, val loss: 0.615222692489624
Epoch 1180, training loss: 424.984375 = 0.5625142455101013 + 50.0 * 8.48843765258789
Epoch 1180, val loss: 0.6143651604652405
Epoch 1190, training loss: 425.3110046386719 = 0.5612549185752869 + 50.0 * 8.4949951171875
Epoch 1190, val loss: 0.613408088684082
Epoch 1200, training loss: 424.9193420410156 = 0.5599532723426819 + 50.0 * 8.487187385559082
Epoch 1200, val loss: 0.6127240657806396
Epoch 1210, training loss: 424.9076843261719 = 0.558703601360321 + 50.0 * 8.486979484558105
Epoch 1210, val loss: 0.6119292974472046
Epoch 1220, training loss: 424.8352355957031 = 0.5574820637702942 + 50.0 * 8.485554695129395
Epoch 1220, val loss: 0.6111125946044922
Epoch 1230, training loss: 424.80426025390625 = 0.5562745928764343 + 50.0 * 8.484959602355957
Epoch 1230, val loss: 0.6103314161300659
Epoch 1240, training loss: 424.7737731933594 = 0.5550705790519714 + 50.0 * 8.484374046325684
Epoch 1240, val loss: 0.6096015572547913
Epoch 1250, training loss: 424.9263000488281 = 0.5538759827613831 + 50.0 * 8.487448692321777
Epoch 1250, val loss: 0.6089480519294739
Epoch 1260, training loss: 424.8092346191406 = 0.5526046752929688 + 50.0 * 8.485132217407227
Epoch 1260, val loss: 0.6079317331314087
Epoch 1270, training loss: 424.6930847167969 = 0.5513949990272522 + 50.0 * 8.482833862304688
Epoch 1270, val loss: 0.607187807559967
Epoch 1280, training loss: 424.6671447753906 = 0.5502113103866577 + 50.0 * 8.482338905334473
Epoch 1280, val loss: 0.6064471006393433
Epoch 1290, training loss: 424.6390380859375 = 0.5490303039550781 + 50.0 * 8.481800079345703
Epoch 1290, val loss: 0.6056647896766663
Epoch 1300, training loss: 424.78387451171875 = 0.5478488802909851 + 50.0 * 8.484720230102539
Epoch 1300, val loss: 0.6048188209533691
Epoch 1310, training loss: 424.63720703125 = 0.5466407537460327 + 50.0 * 8.4818115234375
Epoch 1310, val loss: 0.6042153835296631
Epoch 1320, training loss: 424.5645751953125 = 0.5454694628715515 + 50.0 * 8.480381965637207
Epoch 1320, val loss: 0.6034387946128845
Epoch 1330, training loss: 424.53814697265625 = 0.5443181991577148 + 50.0 * 8.479876518249512
Epoch 1330, val loss: 0.6027185916900635
Epoch 1340, training loss: 424.57171630859375 = 0.5431808233261108 + 50.0 * 8.480570793151855
Epoch 1340, val loss: 0.6021168828010559
Epoch 1350, training loss: 424.516357421875 = 0.5420122146606445 + 50.0 * 8.479486465454102
Epoch 1350, val loss: 0.6012646555900574
Epoch 1360, training loss: 424.4786071777344 = 0.5408524870872498 + 50.0 * 8.478754997253418
Epoch 1360, val loss: 0.6005904078483582
Epoch 1370, training loss: 424.46490478515625 = 0.539733350276947 + 50.0 * 8.478503227233887
Epoch 1370, val loss: 0.5998021364212036
Epoch 1380, training loss: 424.43548583984375 = 0.538626492023468 + 50.0 * 8.477936744689941
Epoch 1380, val loss: 0.5991865396499634
Epoch 1390, training loss: 424.51019287109375 = 0.5375233888626099 + 50.0 * 8.479453086853027
Epoch 1390, val loss: 0.5983912944793701
Epoch 1400, training loss: 424.41204833984375 = 0.5363730788230896 + 50.0 * 8.477513313293457
Epoch 1400, val loss: 0.5978565812110901
Epoch 1410, training loss: 424.4223937988281 = 0.535243809223175 + 50.0 * 8.477743148803711
Epoch 1410, val loss: 0.5970462560653687
Epoch 1420, training loss: 424.3459167480469 = 0.5341495871543884 + 50.0 * 8.476235389709473
Epoch 1420, val loss: 0.5964438915252686
Epoch 1430, training loss: 424.3297424316406 = 0.5330707430839539 + 50.0 * 8.475933074951172
Epoch 1430, val loss: 0.5958167910575867
Epoch 1440, training loss: 424.3061218261719 = 0.5319934487342834 + 50.0 * 8.475482940673828
Epoch 1440, val loss: 0.5951683521270752
Epoch 1450, training loss: 424.4272766113281 = 0.5309251546859741 + 50.0 * 8.477927207946777
Epoch 1450, val loss: 0.5946611762046814
Epoch 1460, training loss: 424.3444519042969 = 0.5298027396202087 + 50.0 * 8.476292610168457
Epoch 1460, val loss: 0.5938400626182556
Epoch 1470, training loss: 424.2535095214844 = 0.5287260413169861 + 50.0 * 8.474495887756348
Epoch 1470, val loss: 0.5932378768920898
Epoch 1480, training loss: 424.2239990234375 = 0.5276768803596497 + 50.0 * 8.473926544189453
Epoch 1480, val loss: 0.5926468968391418
Epoch 1490, training loss: 424.2048034667969 = 0.526631772518158 + 50.0 * 8.473563194274902
Epoch 1490, val loss: 0.5920249223709106
Epoch 1500, training loss: 424.33221435546875 = 0.525592029094696 + 50.0 * 8.4761323928833
Epoch 1500, val loss: 0.5914345383644104
Epoch 1510, training loss: 424.2460021972656 = 0.5245097875595093 + 50.0 * 8.474430084228516
Epoch 1510, val loss: 0.5908525586128235
Epoch 1520, training loss: 424.14593505859375 = 0.5234502553939819 + 50.0 * 8.472450256347656
Epoch 1520, val loss: 0.5901769995689392
Epoch 1530, training loss: 424.1310119628906 = 0.5224180221557617 + 50.0 * 8.472171783447266
Epoch 1530, val loss: 0.5895659327507019
Epoch 1540, training loss: 424.2325439453125 = 0.521395742893219 + 50.0 * 8.474223136901855
Epoch 1540, val loss: 0.5888475775718689
Epoch 1550, training loss: 424.0928039550781 = 0.5203226208686829 + 50.0 * 8.471449851989746
Epoch 1550, val loss: 0.5885010957717896
Epoch 1560, training loss: 424.0710754394531 = 0.5192745923995972 + 50.0 * 8.471035957336426
Epoch 1560, val loss: 0.5877906084060669
Epoch 1570, training loss: 424.0435791015625 = 0.5182502865791321 + 50.0 * 8.47050666809082
Epoch 1570, val loss: 0.5872992277145386
Epoch 1580, training loss: 424.0125732421875 = 0.517234742641449 + 50.0 * 8.4699068069458
Epoch 1580, val loss: 0.5867329239845276
Epoch 1590, training loss: 423.9934387207031 = 0.5162168741226196 + 50.0 * 8.469544410705566
Epoch 1590, val loss: 0.5862375497817993
Epoch 1600, training loss: 424.17529296875 = 0.5151958465576172 + 50.0 * 8.473201751708984
Epoch 1600, val loss: 0.5857762098312378
Epoch 1610, training loss: 424.1059875488281 = 0.5141240954399109 + 50.0 * 8.471837043762207
Epoch 1610, val loss: 0.5849815011024475
Epoch 1620, training loss: 423.9395751953125 = 0.5130785703659058 + 50.0 * 8.46852970123291
Epoch 1620, val loss: 0.5845240950584412
Epoch 1630, training loss: 423.92510986328125 = 0.5120720863342285 + 50.0 * 8.468260765075684
Epoch 1630, val loss: 0.5839922428131104
Epoch 1640, training loss: 423.891357421875 = 0.511077344417572 + 50.0 * 8.467605590820312
Epoch 1640, val loss: 0.5834941267967224
Epoch 1650, training loss: 423.8714599609375 = 0.5100820660591125 + 50.0 * 8.467227935791016
Epoch 1650, val loss: 0.5829638242721558
Epoch 1660, training loss: 423.86407470703125 = 0.5090755820274353 + 50.0 * 8.467100143432617
Epoch 1660, val loss: 0.5823917388916016
Epoch 1670, training loss: 424.1418151855469 = 0.5080623626708984 + 50.0 * 8.472675323486328
Epoch 1670, val loss: 0.5816391110420227
Epoch 1680, training loss: 423.8707580566406 = 0.5069844722747803 + 50.0 * 8.467275619506836
Epoch 1680, val loss: 0.5813368558883667
Epoch 1690, training loss: 423.8102722167969 = 0.5059592127799988 + 50.0 * 8.466086387634277
Epoch 1690, val loss: 0.5808067321777344
Epoch 1700, training loss: 423.7916564941406 = 0.5049582719802856 + 50.0 * 8.465734481811523
Epoch 1700, val loss: 0.5801898241043091
Epoch 1710, training loss: 423.7652893066406 = 0.5039611458778381 + 50.0 * 8.465226173400879
Epoch 1710, val loss: 0.5797146558761597
Epoch 1720, training loss: 423.864501953125 = 0.5029535293579102 + 50.0 * 8.467230796813965
Epoch 1720, val loss: 0.5792443752288818
Epoch 1730, training loss: 423.7327880859375 = 0.5019049048423767 + 50.0 * 8.464617729187012
Epoch 1730, val loss: 0.5785298943519592
Epoch 1740, training loss: 423.7456359863281 = 0.5008761286735535 + 50.0 * 8.464895248413086
Epoch 1740, val loss: 0.578069806098938
Epoch 1750, training loss: 423.79248046875 = 0.49983835220336914 + 50.0 * 8.465852737426758
Epoch 1750, val loss: 0.5774555802345276
Epoch 1760, training loss: 423.70379638671875 = 0.4988001883029938 + 50.0 * 8.464099884033203
Epoch 1760, val loss: 0.576848566532135
Epoch 1770, training loss: 423.6597595214844 = 0.4977778196334839 + 50.0 * 8.463239669799805
Epoch 1770, val loss: 0.5763263702392578
Epoch 1780, training loss: 423.63482666015625 = 0.49676379561424255 + 50.0 * 8.462760925292969
Epoch 1780, val loss: 0.5758395195007324
Epoch 1790, training loss: 423.6195373535156 = 0.4957451820373535 + 50.0 * 8.462475776672363
Epoch 1790, val loss: 0.5753614902496338
Epoch 1800, training loss: 423.7315673828125 = 0.49472731351852417 + 50.0 * 8.464736938476562
Epoch 1800, val loss: 0.5749698877334595
Epoch 1810, training loss: 423.68487548828125 = 0.49366503953933716 + 50.0 * 8.463824272155762
Epoch 1810, val loss: 0.5742736458778381
Epoch 1820, training loss: 423.5951232910156 = 0.49262043833732605 + 50.0 * 8.462050437927246
Epoch 1820, val loss: 0.5737218856811523
Epoch 1830, training loss: 423.5506286621094 = 0.4915991723537445 + 50.0 * 8.461180686950684
Epoch 1830, val loss: 0.5732618570327759
Epoch 1840, training loss: 423.5362548828125 = 0.49058371782302856 + 50.0 * 8.46091365814209
Epoch 1840, val loss: 0.5727571249008179
Epoch 1850, training loss: 423.55194091796875 = 0.48956528306007385 + 50.0 * 8.461247444152832
Epoch 1850, val loss: 0.5722101330757141
Epoch 1860, training loss: 423.672119140625 = 0.4885205328464508 + 50.0 * 8.463671684265137
Epoch 1860, val loss: 0.5716699957847595
Epoch 1870, training loss: 423.5008544921875 = 0.4874482750892639 + 50.0 * 8.460268020629883
Epoch 1870, val loss: 0.5713486075401306
Epoch 1880, training loss: 423.5115966796875 = 0.4864112138748169 + 50.0 * 8.460503578186035
Epoch 1880, val loss: 0.5709100365638733
Epoch 1890, training loss: 423.4862365722656 = 0.48538076877593994 + 50.0 * 8.460017204284668
Epoch 1890, val loss: 0.5703912973403931
Epoch 1900, training loss: 423.43841552734375 = 0.4843527674674988 + 50.0 * 8.459081649780273
Epoch 1900, val loss: 0.569966733455658
Epoch 1910, training loss: 423.4342346191406 = 0.4833255708217621 + 50.0 * 8.459017753601074
Epoch 1910, val loss: 0.5695756077766418
Epoch 1920, training loss: 423.5619201660156 = 0.48229140043258667 + 50.0 * 8.461592674255371
Epoch 1920, val loss: 0.569193959236145
Epoch 1930, training loss: 423.49847412109375 = 0.4812159836292267 + 50.0 * 8.460345268249512
Epoch 1930, val loss: 0.5685166716575623
Epoch 1940, training loss: 423.4198913574219 = 0.4801534414291382 + 50.0 * 8.458794593811035
Epoch 1940, val loss: 0.5681574940681458
Epoch 1950, training loss: 423.38421630859375 = 0.47910428047180176 + 50.0 * 8.458102226257324
Epoch 1950, val loss: 0.5676091313362122
Epoch 1960, training loss: 423.3475646972656 = 0.4780633747577667 + 50.0 * 8.457389831542969
Epoch 1960, val loss: 0.5672323703765869
Epoch 1970, training loss: 423.35980224609375 = 0.477020800113678 + 50.0 * 8.457655906677246
Epoch 1970, val loss: 0.5667009353637695
Epoch 1980, training loss: 423.4309387207031 = 0.47595059871673584 + 50.0 * 8.459099769592285
Epoch 1980, val loss: 0.5662153959274292
Epoch 1990, training loss: 423.3316650390625 = 0.47486674785614014 + 50.0 * 8.457136154174805
Epoch 1990, val loss: 0.5659478306770325
Epoch 2000, training loss: 423.29425048828125 = 0.47379449009895325 + 50.0 * 8.456409454345703
Epoch 2000, val loss: 0.5654296278953552
Epoch 2010, training loss: 423.3484191894531 = 0.4727286398410797 + 50.0 * 8.457513809204102
Epoch 2010, val loss: 0.5649847984313965
Epoch 2020, training loss: 423.2920837402344 = 0.4716314971446991 + 50.0 * 8.456409454345703
Epoch 2020, val loss: 0.5645473599433899
Epoch 2030, training loss: 423.2605895996094 = 0.4705384075641632 + 50.0 * 8.455801010131836
Epoch 2030, val loss: 0.5641189217567444
Epoch 2040, training loss: 423.2357482910156 = 0.46945735812187195 + 50.0 * 8.455326080322266
Epoch 2040, val loss: 0.5636661648750305
Epoch 2050, training loss: 423.22271728515625 = 0.4683767259120941 + 50.0 * 8.455086708068848
Epoch 2050, val loss: 0.5632608532905579
Epoch 2060, training loss: 423.218994140625 = 0.4672832190990448 + 50.0 * 8.455034255981445
Epoch 2060, val loss: 0.5628620982170105
Epoch 2070, training loss: 423.5835876464844 = 0.46618980169296265 + 50.0 * 8.462347984313965
Epoch 2070, val loss: 0.5626452565193176
Epoch 2080, training loss: 423.2403259277344 = 0.464995801448822 + 50.0 * 8.455506324768066
Epoch 2080, val loss: 0.5617392659187317
Epoch 2090, training loss: 423.1968994140625 = 0.46386072039604187 + 50.0 * 8.454660415649414
Epoch 2090, val loss: 0.5613307356834412
Epoch 2100, training loss: 423.16925048828125 = 0.46274933218955994 + 50.0 * 8.454130172729492
Epoch 2100, val loss: 0.5609497427940369
Epoch 2110, training loss: 423.1442565917969 = 0.46163514256477356 + 50.0 * 8.453652381896973
Epoch 2110, val loss: 0.5605321526527405
Epoch 2120, training loss: 423.1542053222656 = 0.4605129659175873 + 50.0 * 8.453873634338379
Epoch 2120, val loss: 0.5600882172584534
Epoch 2130, training loss: 423.2965087890625 = 0.4593675136566162 + 50.0 * 8.456742286682129
Epoch 2130, val loss: 0.5597129464149475
Epoch 2140, training loss: 423.13836669921875 = 0.45818158984184265 + 50.0 * 8.453603744506836
Epoch 2140, val loss: 0.5591429471969604
Epoch 2150, training loss: 423.0982666015625 = 0.45702412724494934 + 50.0 * 8.452824592590332
Epoch 2150, val loss: 0.5587029457092285
Epoch 2160, training loss: 423.0909423828125 = 0.4558675289154053 + 50.0 * 8.452701568603516
Epoch 2160, val loss: 0.558314859867096
Epoch 2170, training loss: 423.0816345214844 = 0.45470118522644043 + 50.0 * 8.45253849029541
Epoch 2170, val loss: 0.5578676462173462
Epoch 2180, training loss: 423.3035888671875 = 0.4535377025604248 + 50.0 * 8.457000732421875
Epoch 2180, val loss: 0.5576674342155457
Epoch 2190, training loss: 423.1642150878906 = 0.4522824287414551 + 50.0 * 8.454238891601562
Epoch 2190, val loss: 0.5567955374717712
Epoch 2200, training loss: 423.0859375 = 0.4510546624660492 + 50.0 * 8.45269775390625
Epoch 2200, val loss: 0.5564985275268555
Epoch 2210, training loss: 423.03778076171875 = 0.4498480558395386 + 50.0 * 8.45175838470459
Epoch 2210, val loss: 0.5560440421104431
Epoch 2220, training loss: 423.0307312011719 = 0.4486443102359772 + 50.0 * 8.451642036437988
Epoch 2220, val loss: 0.5555819869041443
Epoch 2230, training loss: 423.0155944824219 = 0.44742727279663086 + 50.0 * 8.451363563537598
Epoch 2230, val loss: 0.5551572442054749
Epoch 2240, training loss: 423.05303955078125 = 0.446194052696228 + 50.0 * 8.452136993408203
Epoch 2240, val loss: 0.5547531247138977
Epoch 2250, training loss: 423.05914306640625 = 0.4449196457862854 + 50.0 * 8.452284812927246
Epoch 2250, val loss: 0.5542649626731873
Epoch 2260, training loss: 422.993408203125 = 0.44362661242485046 + 50.0 * 8.450995445251465
Epoch 2260, val loss: 0.5537701845169067
Epoch 2270, training loss: 422.9977111816406 = 0.4423505961894989 + 50.0 * 8.451107025146484
Epoch 2270, val loss: 0.5532780885696411
Epoch 2280, training loss: 422.96710205078125 = 0.44107407331466675 + 50.0 * 8.450520515441895
Epoch 2280, val loss: 0.5529098510742188
Epoch 2290, training loss: 422.9527893066406 = 0.4397851228713989 + 50.0 * 8.450260162353516
Epoch 2290, val loss: 0.5524868369102478
Epoch 2300, training loss: 423.0049743652344 = 0.43848517537117004 + 50.0 * 8.451330184936523
Epoch 2300, val loss: 0.5519841313362122
Epoch 2310, training loss: 422.9998474121094 = 0.4371523857116699 + 50.0 * 8.451253890991211
Epoch 2310, val loss: 0.5514397621154785
Epoch 2320, training loss: 422.94903564453125 = 0.4357982277870178 + 50.0 * 8.450264930725098
Epoch 2320, val loss: 0.5510813593864441
Epoch 2330, training loss: 422.927490234375 = 0.4344632029533386 + 50.0 * 8.449860572814941
Epoch 2330, val loss: 0.5507401823997498
Epoch 2340, training loss: 422.9154968261719 = 0.4331378936767578 + 50.0 * 8.449646949768066
Epoch 2340, val loss: 0.5503475069999695
Epoch 2350, training loss: 422.9449462890625 = 0.4318077564239502 + 50.0 * 8.450263023376465
Epoch 2350, val loss: 0.5500223636627197
Epoch 2360, training loss: 422.9596862792969 = 0.4304485619068146 + 50.0 * 8.450584411621094
Epoch 2360, val loss: 0.5495855212211609
Epoch 2370, training loss: 422.8961181640625 = 0.42907798290252686 + 50.0 * 8.4493408203125
Epoch 2370, val loss: 0.5489317178726196
Epoch 2380, training loss: 422.8877868652344 = 0.4277117848396301 + 50.0 * 8.449201583862305
Epoch 2380, val loss: 0.5485869646072388
Epoch 2390, training loss: 422.9220275878906 = 0.4263552129268646 + 50.0 * 8.449913024902344
Epoch 2390, val loss: 0.5483303070068359
Epoch 2400, training loss: 422.86529541015625 = 0.4249551594257355 + 50.0 * 8.448806762695312
Epoch 2400, val loss: 0.5477608442306519
Epoch 2410, training loss: 422.90313720703125 = 0.42356595396995544 + 50.0 * 8.449591636657715
Epoch 2410, val loss: 0.5472142696380615
Epoch 2420, training loss: 422.8291015625 = 0.42215701937675476 + 50.0 * 8.448139190673828
Epoch 2420, val loss: 0.5469409227371216
Epoch 2430, training loss: 422.8273620605469 = 0.4207461178302765 + 50.0 * 8.448132514953613
Epoch 2430, val loss: 0.5465770959854126
Epoch 2440, training loss: 422.85009765625 = 0.41932544112205505 + 50.0 * 8.448616027832031
Epoch 2440, val loss: 0.546101450920105
Epoch 2450, training loss: 422.8329772949219 = 0.41789132356643677 + 50.0 * 8.448302268981934
Epoch 2450, val loss: 0.5456621646881104
Epoch 2460, training loss: 422.8118591308594 = 0.41644513607025146 + 50.0 * 8.447908401489258
Epoch 2460, val loss: 0.5452360510826111
Epoch 2470, training loss: 422.8921203613281 = 0.4150064289569855 + 50.0 * 8.449542045593262
Epoch 2470, val loss: 0.5446937084197998
Epoch 2480, training loss: 422.80645751953125 = 0.4135020673274994 + 50.0 * 8.447858810424805
Epoch 2480, val loss: 0.5444406270980835
Epoch 2490, training loss: 422.7562255859375 = 0.41201284527778625 + 50.0 * 8.446884155273438
Epoch 2490, val loss: 0.5438687801361084
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7706747843734145
0.8146055205390134
=== training gcn model ===
Epoch 0, training loss: 530.2228393554688 = 1.106998324394226 + 50.0 * 10.582316398620605
Epoch 0, val loss: 1.106198787689209
Epoch 10, training loss: 530.2080078125 = 1.1021885871887207 + 50.0 * 10.582117080688477
Epoch 10, val loss: 1.1014370918273926
Epoch 20, training loss: 530.1572265625 = 1.0970463752746582 + 50.0 * 10.58120346069336
Epoch 20, val loss: 1.0963410139083862
Epoch 30, training loss: 529.9321899414062 = 1.0914512872695923 + 50.0 * 10.576814651489258
Epoch 30, val loss: 1.0908085107803345
Epoch 40, training loss: 529.0109252929688 = 1.0852757692337036 + 50.0 * 10.558513641357422
Epoch 40, val loss: 1.084682583808899
Epoch 50, training loss: 526.2527465820312 = 1.0786802768707275 + 50.0 * 10.5034818649292
Epoch 50, val loss: 1.0781885385513306
Epoch 60, training loss: 520.2619018554688 = 1.0727579593658447 + 50.0 * 10.383783340454102
Epoch 60, val loss: 1.0724623203277588
Epoch 70, training loss: 511.5090637207031 = 1.0680972337722778 + 50.0 * 10.208819389343262
Epoch 70, val loss: 1.0678805112838745
Epoch 80, training loss: 507.32171630859375 = 1.0645118951797485 + 50.0 * 10.125144004821777
Epoch 80, val loss: 1.0643826723098755
Epoch 90, training loss: 500.0711364746094 = 1.0618706941604614 + 50.0 * 9.980185508728027
Epoch 90, val loss: 1.0617657899856567
Epoch 100, training loss: 484.5159912109375 = 1.0591440200805664 + 50.0 * 9.669137001037598
Epoch 100, val loss: 1.0590258836746216
Epoch 110, training loss: 471.6570129394531 = 1.0552732944488525 + 50.0 * 9.41203498840332
Epoch 110, val loss: 1.0551810264587402
Epoch 120, training loss: 467.9110412597656 = 1.0515578985214233 + 50.0 * 9.337189674377441
Epoch 120, val loss: 1.0516585111618042
Epoch 130, training loss: 465.7017822265625 = 1.0486079454421997 + 50.0 * 9.293063163757324
Epoch 130, val loss: 1.0488054752349854
Epoch 140, training loss: 462.2419738769531 = 1.0463166236877441 + 50.0 * 9.223913192749023
Epoch 140, val loss: 1.0465548038482666
Epoch 150, training loss: 457.7677001953125 = 1.0442999601364136 + 50.0 * 9.134468078613281
Epoch 150, val loss: 1.0445390939712524
Epoch 160, training loss: 454.6891784667969 = 1.0423284769058228 + 50.0 * 9.07293701171875
Epoch 160, val loss: 1.0425593852996826
Epoch 170, training loss: 452.75750732421875 = 1.040065050125122 + 50.0 * 9.034348487854004
Epoch 170, val loss: 1.0402849912643433
Epoch 180, training loss: 449.9494323730469 = 1.0374648571014404 + 50.0 * 8.978239059448242
Epoch 180, val loss: 1.037718415260315
Epoch 190, training loss: 446.92205810546875 = 1.034887671470642 + 50.0 * 8.917743682861328
Epoch 190, val loss: 1.0352030992507935
Epoch 200, training loss: 444.6369934082031 = 1.0322314500808716 + 50.0 * 8.872095108032227
Epoch 200, val loss: 1.0325939655303955
Epoch 210, training loss: 443.3312072753906 = 1.0291850566864014 + 50.0 * 8.846040725708008
Epoch 210, val loss: 1.0296214818954468
Epoch 220, training loss: 442.33917236328125 = 1.0258127450942993 + 50.0 * 8.82626724243164
Epoch 220, val loss: 1.0263237953186035
Epoch 230, training loss: 441.43359375 = 1.0222160816192627 + 50.0 * 8.8082275390625
Epoch 230, val loss: 1.0228081941604614
Epoch 240, training loss: 440.5335693359375 = 1.0183980464935303 + 50.0 * 8.790303230285645
Epoch 240, val loss: 1.0190818309783936
Epoch 250, training loss: 439.5495910644531 = 1.014367938041687 + 50.0 * 8.77070426940918
Epoch 250, val loss: 1.0151455402374268
Epoch 260, training loss: 438.6119079589844 = 1.010102391242981 + 50.0 * 8.752036094665527
Epoch 260, val loss: 1.0109895467758179
Epoch 270, training loss: 437.9441833496094 = 1.005561113357544 + 50.0 * 8.73877239227295
Epoch 270, val loss: 1.0065449476242065
Epoch 280, training loss: 437.100341796875 = 1.000609040260315 + 50.0 * 8.721994400024414
Epoch 280, val loss: 1.0017457008361816
Epoch 290, training loss: 436.486572265625 = 0.9952854514122009 + 50.0 * 8.70982551574707
Epoch 290, val loss: 0.9965496063232422
Epoch 300, training loss: 435.94580078125 = 0.9895792007446289 + 50.0 * 8.699124336242676
Epoch 300, val loss: 0.9910284280776978
Epoch 310, training loss: 435.4380798339844 = 0.9834793210029602 + 50.0 * 8.689091682434082
Epoch 310, val loss: 0.9850950241088867
Epoch 320, training loss: 434.9590759277344 = 0.9769261479377747 + 50.0 * 8.679642677307129
Epoch 320, val loss: 0.9787480235099792
Epoch 330, training loss: 434.4976806640625 = 0.9699578881263733 + 50.0 * 8.670554161071777
Epoch 330, val loss: 0.9719992876052856
Epoch 340, training loss: 434.2488098144531 = 0.9625757336616516 + 50.0 * 8.665724754333496
Epoch 340, val loss: 0.9648399353027344
Epoch 350, training loss: 433.7944030761719 = 0.9546940326690674 + 50.0 * 8.656794548034668
Epoch 350, val loss: 0.9572340846061707
Epoch 360, training loss: 433.3601379394531 = 0.9463700652122498 + 50.0 * 8.648275375366211
Epoch 360, val loss: 0.9492325782775879
Epoch 370, training loss: 433.02392578125 = 0.9376730918884277 + 50.0 * 8.641724586486816
Epoch 370, val loss: 0.9408180117607117
Epoch 380, training loss: 432.78375244140625 = 0.928579568862915 + 50.0 * 8.637103080749512
Epoch 380, val loss: 0.9320451617240906
Epoch 390, training loss: 432.5362243652344 = 0.9191027879714966 + 50.0 * 8.632342338562012
Epoch 390, val loss: 0.9229618310928345
Epoch 400, training loss: 432.240966796875 = 0.9093475341796875 + 50.0 * 8.626632690429688
Epoch 400, val loss: 0.9135985374450684
Epoch 410, training loss: 431.9739685058594 = 0.8993611931800842 + 50.0 * 8.621492385864258
Epoch 410, val loss: 0.9040334820747375
Epoch 420, training loss: 431.746337890625 = 0.8891589045524597 + 50.0 * 8.617143630981445
Epoch 420, val loss: 0.894285261631012
Epoch 430, training loss: 431.5119934082031 = 0.8787720203399658 + 50.0 * 8.612664222717285
Epoch 430, val loss: 0.8844009637832642
Epoch 440, training loss: 431.40252685546875 = 0.8682546615600586 + 50.0 * 8.610685348510742
Epoch 440, val loss: 0.87440025806427
Epoch 450, training loss: 431.10137939453125 = 0.8576239347457886 + 50.0 * 8.604874610900879
Epoch 450, val loss: 0.8643401861190796
Epoch 460, training loss: 430.97247314453125 = 0.8469942212104797 + 50.0 * 8.602509498596191
Epoch 460, val loss: 0.8542724251747131
Epoch 470, training loss: 430.6924743652344 = 0.8363560438156128 + 50.0 * 8.597122192382812
Epoch 470, val loss: 0.8442608714103699
Epoch 480, training loss: 430.4699401855469 = 0.8257892727851868 + 50.0 * 8.592883110046387
Epoch 480, val loss: 0.8343253135681152
Epoch 490, training loss: 430.3652038574219 = 0.8152920603752136 + 50.0 * 8.590998649597168
Epoch 490, val loss: 0.8244652152061462
Epoch 500, training loss: 430.197265625 = 0.8048499226570129 + 50.0 * 8.587848663330078
Epoch 500, val loss: 0.8147355318069458
Epoch 510, training loss: 429.9200439453125 = 0.7945565581321716 + 50.0 * 8.582509994506836
Epoch 510, val loss: 0.8051411509513855
Epoch 520, training loss: 429.758056640625 = 0.7844697833061218 + 50.0 * 8.579471588134766
Epoch 520, val loss: 0.7957559823989868
Epoch 530, training loss: 429.58038330078125 = 0.7745711803436279 + 50.0 * 8.576116561889648
Epoch 530, val loss: 0.7866010069847107
Epoch 540, training loss: 429.6981201171875 = 0.7648870944976807 + 50.0 * 8.578664779663086
Epoch 540, val loss: 0.7776197195053101
Epoch 550, training loss: 429.3096923828125 = 0.7553498148918152 + 50.0 * 8.571086883544922
Epoch 550, val loss: 0.7688822746276855
Epoch 560, training loss: 429.0926208496094 = 0.7461209893226624 + 50.0 * 8.566929817199707
Epoch 560, val loss: 0.7604643702507019
Epoch 570, training loss: 428.94073486328125 = 0.7371982336044312 + 50.0 * 8.564070701599121
Epoch 570, val loss: 0.7523283958435059
Epoch 580, training loss: 428.8070983886719 = 0.728561282157898 + 50.0 * 8.56157112121582
Epoch 580, val loss: 0.7444725036621094
Epoch 590, training loss: 428.71197509765625 = 0.7201462984085083 + 50.0 * 8.559836387634277
Epoch 590, val loss: 0.7369081974029541
Epoch 600, training loss: 428.561767578125 = 0.7120389342308044 + 50.0 * 8.556994438171387
Epoch 600, val loss: 0.7296061515808105
Epoch 610, training loss: 428.4122314453125 = 0.7043277621269226 + 50.0 * 8.554158210754395
Epoch 610, val loss: 0.7227012515068054
Epoch 620, training loss: 428.26202392578125 = 0.6969678997993469 + 50.0 * 8.551301002502441
Epoch 620, val loss: 0.7161517143249512
Epoch 630, training loss: 428.1317443847656 = 0.6899201273918152 + 50.0 * 8.548836708068848
Epoch 630, val loss: 0.7099298238754272
Epoch 640, training loss: 428.09429931640625 = 0.6831706762313843 + 50.0 * 8.548222541809082
Epoch 640, val loss: 0.7039926052093506
Epoch 650, training loss: 427.91912841796875 = 0.6767002940177917 + 50.0 * 8.544848442077637
Epoch 650, val loss: 0.6983604431152344
Epoch 660, training loss: 427.8038024902344 = 0.6705589890480042 + 50.0 * 8.542664527893066
Epoch 660, val loss: 0.6930248141288757
Epoch 670, training loss: 427.7041320800781 = 0.6647178530693054 + 50.0 * 8.540788650512695
Epoch 670, val loss: 0.6879918575286865
Epoch 680, training loss: 427.79046630859375 = 0.6591499447822571 + 50.0 * 8.54262638092041
Epoch 680, val loss: 0.6832063794136047
Epoch 690, training loss: 427.5536193847656 = 0.6538366675376892 + 50.0 * 8.537995338439941
Epoch 690, val loss: 0.6787511110305786
Epoch 700, training loss: 427.4408264160156 = 0.6488191485404968 + 50.0 * 8.535840034484863
Epoch 700, val loss: 0.6745368838310242
Epoch 710, training loss: 427.3603515625 = 0.6440916061401367 + 50.0 * 8.534324645996094
Epoch 710, val loss: 0.6705837845802307
Epoch 720, training loss: 427.33209228515625 = 0.639612078666687 + 50.0 * 8.533849716186523
Epoch 720, val loss: 0.6668494939804077
Epoch 730, training loss: 427.2744445800781 = 0.6353251934051514 + 50.0 * 8.532782554626465
Epoch 730, val loss: 0.6633842587471008
Epoch 740, training loss: 427.152587890625 = 0.6312832236289978 + 50.0 * 8.530426025390625
Epoch 740, val loss: 0.6600942015647888
Epoch 750, training loss: 427.08135986328125 = 0.6274588704109192 + 50.0 * 8.529077529907227
Epoch 750, val loss: 0.6570209860801697
Epoch 760, training loss: 427.0353698730469 = 0.6238516569137573 + 50.0 * 8.528230667114258
Epoch 760, val loss: 0.6541547775268555
Epoch 770, training loss: 427.0343933105469 = 0.6204146146774292 + 50.0 * 8.528279304504395
Epoch 770, val loss: 0.6514438986778259
Epoch 780, training loss: 426.92431640625 = 0.6171286702156067 + 50.0 * 8.526144027709961
Epoch 780, val loss: 0.648868203163147
Epoch 790, training loss: 426.8182067871094 = 0.6140528321266174 + 50.0 * 8.524083137512207
Epoch 790, val loss: 0.6465142965316772
Epoch 800, training loss: 426.7686767578125 = 0.6111419796943665 + 50.0 * 8.523150444030762
Epoch 800, val loss: 0.6442953944206238
Epoch 810, training loss: 426.82269287109375 = 0.6083659529685974 + 50.0 * 8.524286270141602
Epoch 810, val loss: 0.6422456502914429
Epoch 820, training loss: 426.6974182128906 = 0.6056599020957947 + 50.0 * 8.521835327148438
Epoch 820, val loss: 0.6401762366294861
Epoch 830, training loss: 426.65155029296875 = 0.6031131744384766 + 50.0 * 8.520968437194824
Epoch 830, val loss: 0.6383360624313354
Epoch 840, training loss: 426.5562744140625 = 0.6007015109062195 + 50.0 * 8.519111633300781
Epoch 840, val loss: 0.6365613341331482
Epoch 850, training loss: 426.5237731933594 = 0.5983973145484924 + 50.0 * 8.518507957458496
Epoch 850, val loss: 0.6348986029624939
Epoch 860, training loss: 426.47802734375 = 0.5961713790893555 + 50.0 * 8.517637252807617
Epoch 860, val loss: 0.6333377957344055
Epoch 870, training loss: 426.40521240234375 = 0.5940200090408325 + 50.0 * 8.516223907470703
Epoch 870, val loss: 0.6318256855010986
Epoch 880, training loss: 426.3635559082031 = 0.5919784307479858 + 50.0 * 8.51543140411377
Epoch 880, val loss: 0.6304339170455933
Epoch 890, training loss: 426.3243408203125 = 0.590029239654541 + 50.0 * 8.514686584472656
Epoch 890, val loss: 0.6290976405143738
Epoch 900, training loss: 426.5523986816406 = 0.588134765625 + 50.0 * 8.519285202026367
Epoch 900, val loss: 0.6278552412986755
Epoch 910, training loss: 426.25970458984375 = 0.586276650428772 + 50.0 * 8.513468742370605
Epoch 910, val loss: 0.6265760064125061
Epoch 920, training loss: 426.23101806640625 = 0.5845196843147278 + 50.0 * 8.512929916381836
Epoch 920, val loss: 0.6254237294197083
Epoch 930, training loss: 426.2889099121094 = 0.5828438997268677 + 50.0 * 8.514121055603027
Epoch 930, val loss: 0.6243781447410583
Epoch 940, training loss: 426.1492919921875 = 0.5811871886253357 + 50.0 * 8.511362075805664
Epoch 940, val loss: 0.6232665777206421
Epoch 950, training loss: 426.0979919433594 = 0.5796167850494385 + 50.0 * 8.510367393493652
Epoch 950, val loss: 0.6223083138465881
Epoch 960, training loss: 426.0521545410156 = 0.5781058669090271 + 50.0 * 8.509481430053711
Epoch 960, val loss: 0.6213547587394714
Epoch 970, training loss: 426.0294189453125 = 0.5766475200653076 + 50.0 * 8.509055137634277
Epoch 970, val loss: 0.6204723715782166
Epoch 980, training loss: 426.0244445800781 = 0.5752273797988892 + 50.0 * 8.508984565734863
Epoch 980, val loss: 0.6196088194847107
Epoch 990, training loss: 426.02423095703125 = 0.5738351941108704 + 50.0 * 8.509008407592773
Epoch 990, val loss: 0.6187660694122314
Epoch 1000, training loss: 425.9851379394531 = 0.572485625743866 + 50.0 * 8.50825309753418
Epoch 1000, val loss: 0.6180490255355835
Epoch 1010, training loss: 425.9201354980469 = 0.5711827874183655 + 50.0 * 8.506978988647461
Epoch 1010, val loss: 0.6172569394111633
Epoch 1020, training loss: 426.03662109375 = 0.5699344277381897 + 50.0 * 8.509333610534668
Epoch 1020, val loss: 0.6165689826011658
Epoch 1030, training loss: 425.89605712890625 = 0.5686583518981934 + 50.0 * 8.506547927856445
Epoch 1030, val loss: 0.6158762574195862
Epoch 1040, training loss: 425.811767578125 = 0.5674660801887512 + 50.0 * 8.50488567352295
Epoch 1040, val loss: 0.6152392029762268
Epoch 1050, training loss: 425.7608947753906 = 0.5663321614265442 + 50.0 * 8.503890991210938
Epoch 1050, val loss: 0.6146022081375122
Epoch 1060, training loss: 425.7215881347656 = 0.5652339458465576 + 50.0 * 8.503127098083496
Epoch 1060, val loss: 0.614051342010498
Epoch 1070, training loss: 425.6863098144531 = 0.5641644597053528 + 50.0 * 8.502442359924316
Epoch 1070, val loss: 0.6135145425796509
Epoch 1080, training loss: 425.74798583984375 = 0.5631150007247925 + 50.0 * 8.503697395324707
Epoch 1080, val loss: 0.6129545569419861
Epoch 1090, training loss: 425.658447265625 = 0.5620409250259399 + 50.0 * 8.501928329467773
Epoch 1090, val loss: 0.6124225854873657
Epoch 1100, training loss: 425.6408996582031 = 0.5610236525535583 + 50.0 * 8.50159740447998
Epoch 1100, val loss: 0.6119226217269897
Epoch 1110, training loss: 425.5429382324219 = 0.560054361820221 + 50.0 * 8.49965763092041
Epoch 1110, val loss: 0.6114595532417297
Epoch 1120, training loss: 425.5048828125 = 0.5591229796409607 + 50.0 * 8.498915672302246
Epoch 1120, val loss: 0.6110314726829529
Epoch 1130, training loss: 425.47857666015625 = 0.5582091808319092 + 50.0 * 8.498407363891602
Epoch 1130, val loss: 0.6105911731719971
Epoch 1140, training loss: 425.8358459472656 = 0.5572776794433594 + 50.0 * 8.505571365356445
Epoch 1140, val loss: 0.6101561188697815
Epoch 1150, training loss: 425.4654541015625 = 0.5563331842422485 + 50.0 * 8.49818229675293
Epoch 1150, val loss: 0.6097014546394348
Epoch 1160, training loss: 425.3747863769531 = 0.5554653406143188 + 50.0 * 8.496386528015137
Epoch 1160, val loss: 0.6093093752861023
Epoch 1170, training loss: 425.3254089355469 = 0.55463707447052 + 50.0 * 8.495415687561035
Epoch 1170, val loss: 0.6089805364608765
Epoch 1180, training loss: 425.2930603027344 = 0.5538251399993896 + 50.0 * 8.494784355163574
Epoch 1180, val loss: 0.6086553931236267
Epoch 1190, training loss: 425.26336669921875 = 0.5530264973640442 + 50.0 * 8.494206428527832
Epoch 1190, val loss: 0.6083155870437622
Epoch 1200, training loss: 425.5229797363281 = 0.5522186756134033 + 50.0 * 8.499415397644043
Epoch 1200, val loss: 0.6079850196838379
Epoch 1210, training loss: 425.196533203125 = 0.5513885617256165 + 50.0 * 8.492902755737305
Epoch 1210, val loss: 0.6075730323791504
Epoch 1220, training loss: 425.1895751953125 = 0.55061936378479 + 50.0 * 8.492778778076172
Epoch 1220, val loss: 0.607255756855011
Epoch 1230, training loss: 425.126953125 = 0.5498822331428528 + 50.0 * 8.491540908813477
Epoch 1230, val loss: 0.6069623827934265
Epoch 1240, training loss: 425.21405029296875 = 0.5491542816162109 + 50.0 * 8.493297576904297
Epoch 1240, val loss: 0.6066760420799255
Epoch 1250, training loss: 425.06268310546875 = 0.5484013557434082 + 50.0 * 8.490285873413086
Epoch 1250, val loss: 0.6063461303710938
Epoch 1260, training loss: 425.02911376953125 = 0.5476735830307007 + 50.0 * 8.489628791809082
Epoch 1260, val loss: 0.6060839295387268
Epoch 1270, training loss: 424.99993896484375 = 0.5469760298728943 + 50.0 * 8.489059448242188
Epoch 1270, val loss: 0.6057832837104797
Epoch 1280, training loss: 424.9911193847656 = 0.5462909936904907 + 50.0 * 8.488896369934082
Epoch 1280, val loss: 0.6055552363395691
Epoch 1290, training loss: 425.1123352050781 = 0.5455897450447083 + 50.0 * 8.491334915161133
Epoch 1290, val loss: 0.6053109765052795
Epoch 1300, training loss: 424.9294738769531 = 0.5448822975158691 + 50.0 * 8.487691879272461
Epoch 1300, val loss: 0.6049782037734985
Epoch 1310, training loss: 424.9095153808594 = 0.5442119240760803 + 50.0 * 8.487305641174316
Epoch 1310, val loss: 0.6047690510749817
Epoch 1320, training loss: 424.8627624511719 = 0.5435596704483032 + 50.0 * 8.486384391784668
Epoch 1320, val loss: 0.6045387983322144
Epoch 1330, training loss: 425.01251220703125 = 0.5429099798202515 + 50.0 * 8.489392280578613
Epoch 1330, val loss: 0.6042803525924683
Epoch 1340, training loss: 424.8296203613281 = 0.5422155261039734 + 50.0 * 8.485748291015625
Epoch 1340, val loss: 0.6040387749671936
Epoch 1350, training loss: 424.815185546875 = 0.5415567755699158 + 50.0 * 8.485472679138184
Epoch 1350, val loss: 0.6037967205047607
Epoch 1360, training loss: 424.77703857421875 = 0.5409318208694458 + 50.0 * 8.484722137451172
Epoch 1360, val loss: 0.6035809516906738
Epoch 1370, training loss: 424.7389831542969 = 0.5403186082839966 + 50.0 * 8.483973503112793
Epoch 1370, val loss: 0.6033885478973389
Epoch 1380, training loss: 424.7076721191406 = 0.5397078394889832 + 50.0 * 8.483359336853027
Epoch 1380, val loss: 0.6031763553619385
Epoch 1390, training loss: 424.8332824707031 = 0.5391023755073547 + 50.0 * 8.485883712768555
Epoch 1390, val loss: 0.6029247641563416
Epoch 1400, training loss: 424.8060607910156 = 0.5384233593940735 + 50.0 * 8.485352516174316
Epoch 1400, val loss: 0.60272616147995
Epoch 1410, training loss: 424.6619873046875 = 0.5377897620201111 + 50.0 * 8.482483863830566
Epoch 1410, val loss: 0.6024805307388306
Epoch 1420, training loss: 424.63763427734375 = 0.5371965765953064 + 50.0 * 8.482008934020996
Epoch 1420, val loss: 0.6022586822509766
Epoch 1430, training loss: 424.59381103515625 = 0.536611020565033 + 50.0 * 8.481143951416016
Epoch 1430, val loss: 0.6020836234092712
Epoch 1440, training loss: 424.586181640625 = 0.5360315442085266 + 50.0 * 8.481002807617188
Epoch 1440, val loss: 0.601902425289154
Epoch 1450, training loss: 424.74755859375 = 0.5354353785514832 + 50.0 * 8.48424243927002
Epoch 1450, val loss: 0.6016783714294434
Epoch 1460, training loss: 424.5357666015625 = 0.5347779989242554 + 50.0 * 8.480019569396973
Epoch 1460, val loss: 0.601420521736145
Epoch 1470, training loss: 424.5322570800781 = 0.5341701507568359 + 50.0 * 8.479961395263672
Epoch 1470, val loss: 0.6011370420455933
Epoch 1480, training loss: 424.48675537109375 = 0.5335994362831116 + 50.0 * 8.479063034057617
Epoch 1480, val loss: 0.6009365916252136
Epoch 1490, training loss: 424.4604187011719 = 0.5330368280410767 + 50.0 * 8.478547096252441
Epoch 1490, val loss: 0.600772500038147
Epoch 1500, training loss: 424.4354248046875 = 0.5324743986129761 + 50.0 * 8.478058815002441
Epoch 1500, val loss: 0.6005752086639404
Epoch 1510, training loss: 424.4902648925781 = 0.5319072008132935 + 50.0 * 8.479166984558105
Epoch 1510, val loss: 0.600338876247406
Epoch 1520, training loss: 424.51416015625 = 0.5313019156455994 + 50.0 * 8.479657173156738
Epoch 1520, val loss: 0.6001001596450806
Epoch 1530, training loss: 424.39996337890625 = 0.530701220035553 + 50.0 * 8.477385520935059
Epoch 1530, val loss: 0.5998618006706238
Epoch 1540, training loss: 424.3774108886719 = 0.5301336646080017 + 50.0 * 8.476945877075195
Epoch 1540, val loss: 0.5996731519699097
Epoch 1550, training loss: 424.3207092285156 = 0.5295851826667786 + 50.0 * 8.475822448730469
Epoch 1550, val loss: 0.5994633436203003
Epoch 1560, training loss: 424.3058776855469 = 0.5290437936782837 + 50.0 * 8.475536346435547
Epoch 1560, val loss: 0.5992565751075745
Epoch 1570, training loss: 424.5155029296875 = 0.528490424156189 + 50.0 * 8.479740142822266
Epoch 1570, val loss: 0.5990214347839355
Epoch 1580, training loss: 424.31488037109375 = 0.5278839468955994 + 50.0 * 8.475739479064941
Epoch 1580, val loss: 0.5988188982009888
Epoch 1590, training loss: 424.2862854003906 = 0.5273090600967407 + 50.0 * 8.475179672241211
Epoch 1590, val loss: 0.5985541939735413
Epoch 1600, training loss: 424.2406005859375 = 0.5267724394798279 + 50.0 * 8.474276542663574
Epoch 1600, val loss: 0.5983739495277405
Epoch 1610, training loss: 424.206298828125 = 0.5262410044670105 + 50.0 * 8.473601341247559
Epoch 1610, val loss: 0.598156750202179
Epoch 1620, training loss: 424.3525390625 = 0.5257070660591125 + 50.0 * 8.476536750793457
Epoch 1620, val loss: 0.5979243516921997
Epoch 1630, training loss: 424.2740478515625 = 0.5251052379608154 + 50.0 * 8.47497844696045
Epoch 1630, val loss: 0.5976910591125488
Epoch 1640, training loss: 424.1647033691406 = 0.5245378017425537 + 50.0 * 8.472803115844727
Epoch 1640, val loss: 0.5974355340003967
Epoch 1650, training loss: 424.1302795410156 = 0.5239990949630737 + 50.0 * 8.472126007080078
Epoch 1650, val loss: 0.5972087979316711
Epoch 1660, training loss: 424.1039733886719 = 0.5234684348106384 + 50.0 * 8.471610069274902
Epoch 1660, val loss: 0.5969861149787903
Epoch 1670, training loss: 424.07940673828125 = 0.5229349136352539 + 50.0 * 8.471129417419434
Epoch 1670, val loss: 0.5967592597007751
Epoch 1680, training loss: 424.0877380371094 = 0.5223951935768127 + 50.0 * 8.471306800842285
Epoch 1680, val loss: 0.5965405702590942
Epoch 1690, training loss: 424.3765563964844 = 0.5218208432197571 + 50.0 * 8.477094650268555
Epoch 1690, val loss: 0.5962783098220825
Epoch 1700, training loss: 424.0666809082031 = 0.5212041735649109 + 50.0 * 8.470909118652344
Epoch 1700, val loss: 0.5959988832473755
Epoch 1710, training loss: 424.0452880859375 = 0.5206471681594849 + 50.0 * 8.470492362976074
Epoch 1710, val loss: 0.5956498980522156
Epoch 1720, training loss: 423.9933776855469 = 0.5201140642166138 + 50.0 * 8.469465255737305
Epoch 1720, val loss: 0.5954867601394653
Epoch 1730, training loss: 423.9766845703125 = 0.5195849537849426 + 50.0 * 8.469141960144043
Epoch 1730, val loss: 0.5952582955360413
Epoch 1740, training loss: 424.1153869628906 = 0.5190463066101074 + 50.0 * 8.47192668914795
Epoch 1740, val loss: 0.5950214862823486
Epoch 1750, training loss: 424.0123291015625 = 0.5184515714645386 + 50.0 * 8.469877243041992
Epoch 1750, val loss: 0.594691812992096
Epoch 1760, training loss: 423.94927978515625 = 0.5178855657577515 + 50.0 * 8.4686279296875
Epoch 1760, val loss: 0.5944960713386536
Epoch 1770, training loss: 423.9071960449219 = 0.5173325538635254 + 50.0 * 8.46779727935791
Epoch 1770, val loss: 0.5942392945289612
Epoch 1780, training loss: 423.9230651855469 = 0.5167840123176575 + 50.0 * 8.468125343322754
Epoch 1780, val loss: 0.5940229892730713
Epoch 1790, training loss: 424.23004150390625 = 0.5162012577056885 + 50.0 * 8.474276542663574
Epoch 1790, val loss: 0.5937193036079407
Epoch 1800, training loss: 424.0009765625 = 0.5155556797981262 + 50.0 * 8.469708442687988
Epoch 1800, val loss: 0.5934068560600281
Epoch 1810, training loss: 423.87103271484375 = 0.5149767994880676 + 50.0 * 8.467121124267578
Epoch 1810, val loss: 0.5930978655815125
Epoch 1820, training loss: 423.8381652832031 = 0.5144295692443848 + 50.0 * 8.466474533081055
Epoch 1820, val loss: 0.5928615927696228
Epoch 1830, training loss: 423.8066101074219 = 0.5138852000236511 + 50.0 * 8.46585464477539
Epoch 1830, val loss: 0.5926321148872375
Epoch 1840, training loss: 423.78704833984375 = 0.5133264660835266 + 50.0 * 8.465474128723145
Epoch 1840, val loss: 0.5923818945884705
Epoch 1850, training loss: 423.7897033691406 = 0.5127607583999634 + 50.0 * 8.46553897857666
Epoch 1850, val loss: 0.5921264886856079
Epoch 1860, training loss: 424.0285949707031 = 0.5121682286262512 + 50.0 * 8.470328330993652
Epoch 1860, val loss: 0.5918581485748291
Epoch 1870, training loss: 423.7952575683594 = 0.5115439891815186 + 50.0 * 8.46567440032959
Epoch 1870, val loss: 0.5915368795394897
Epoch 1880, training loss: 423.725341796875 = 0.5109522938728333 + 50.0 * 8.464287757873535
Epoch 1880, val loss: 0.5912317633628845
Epoch 1890, training loss: 423.7178955078125 = 0.5103751420974731 + 50.0 * 8.464150428771973
Epoch 1890, val loss: 0.5909899473190308
Epoch 1900, training loss: 423.9209289550781 = 0.5097901821136475 + 50.0 * 8.468222618103027
Epoch 1900, val loss: 0.5907111763954163
Epoch 1910, training loss: 423.7057800292969 = 0.5091285109519958 + 50.0 * 8.463932991027832
Epoch 1910, val loss: 0.5903387069702148
Epoch 1920, training loss: 423.69171142578125 = 0.5085116624832153 + 50.0 * 8.463664054870605
Epoch 1920, val loss: 0.59004807472229
Epoch 1930, training loss: 423.6648864746094 = 0.507918655872345 + 50.0 * 8.463139533996582
Epoch 1930, val loss: 0.5897713899612427
Epoch 1940, training loss: 423.63861083984375 = 0.5073270797729492 + 50.0 * 8.462625503540039
Epoch 1940, val loss: 0.5894924998283386
Epoch 1950, training loss: 423.62164306640625 = 0.5067263841629028 + 50.0 * 8.462298393249512
Epoch 1950, val loss: 0.5891914963722229
Epoch 1960, training loss: 423.6197509765625 = 0.5061157941818237 + 50.0 * 8.462272644042969
Epoch 1960, val loss: 0.5888925194740295
Epoch 1970, training loss: 423.82464599609375 = 0.5054838061332703 + 50.0 * 8.46638298034668
Epoch 1970, val loss: 0.5885360240936279
Epoch 1980, training loss: 423.6660461425781 = 0.504820704460144 + 50.0 * 8.463224411010742
Epoch 1980, val loss: 0.5882540345191956
Epoch 1990, training loss: 423.56988525390625 = 0.5041720867156982 + 50.0 * 8.46131420135498
Epoch 1990, val loss: 0.5879054665565491
Epoch 2000, training loss: 423.571533203125 = 0.5035457611083984 + 50.0 * 8.461359977722168
Epoch 2000, val loss: 0.5875697731971741
Epoch 2010, training loss: 423.6895446777344 = 0.5029116272926331 + 50.0 * 8.463732719421387
Epoch 2010, val loss: 0.58726966381073
Epoch 2020, training loss: 423.6588134765625 = 0.502224326133728 + 50.0 * 8.46313190460205
Epoch 2020, val loss: 0.5869394540786743
Epoch 2030, training loss: 423.552734375 = 0.5015347003936768 + 50.0 * 8.461024284362793
Epoch 2030, val loss: 0.5865294337272644
Epoch 2040, training loss: 423.5179443359375 = 0.5008853077888489 + 50.0 * 8.460341453552246
Epoch 2040, val loss: 0.586266815662384
Epoch 2050, training loss: 423.498779296875 = 0.500240683555603 + 50.0 * 8.459970474243164
Epoch 2050, val loss: 0.5859269499778748
Epoch 2060, training loss: 423.4874572753906 = 0.49958816170692444 + 50.0 * 8.459756851196289
Epoch 2060, val loss: 0.585602343082428
Epoch 2070, training loss: 423.8104553222656 = 0.4989114999771118 + 50.0 * 8.466231346130371
Epoch 2070, val loss: 0.5852615833282471
Epoch 2080, training loss: 423.5855407714844 = 0.49817848205566406 + 50.0 * 8.461747169494629
Epoch 2080, val loss: 0.5848666429519653
Epoch 2090, training loss: 423.4461669921875 = 0.4974709749221802 + 50.0 * 8.45897388458252
Epoch 2090, val loss: 0.5845093131065369
Epoch 2100, training loss: 423.4436340332031 = 0.4967907965183258 + 50.0 * 8.45893669128418
Epoch 2100, val loss: 0.5841825604438782
Epoch 2110, training loss: 423.45306396484375 = 0.49611029028892517 + 50.0 * 8.459138870239258
Epoch 2110, val loss: 0.5838428735733032
Epoch 2120, training loss: 423.8074035644531 = 0.495388925075531 + 50.0 * 8.466239929199219
Epoch 2120, val loss: 0.5834524631500244
Epoch 2130, training loss: 423.5067443847656 = 0.49461638927459717 + 50.0 * 8.46024227142334
Epoch 2130, val loss: 0.5830737352371216
Epoch 2140, training loss: 423.4010925292969 = 0.4938943088054657 + 50.0 * 8.458144187927246
Epoch 2140, val loss: 0.5827334523200989
Epoch 2150, training loss: 423.3750915527344 = 0.4931921660900116 + 50.0 * 8.457637786865234
Epoch 2150, val loss: 0.5823543071746826
Epoch 2160, training loss: 423.3624267578125 = 0.4924805760383606 + 50.0 * 8.457398414611816
Epoch 2160, val loss: 0.5820103287696838
Epoch 2170, training loss: 423.3524169921875 = 0.49176162481307983 + 50.0 * 8.457213401794434
Epoch 2170, val loss: 0.5816735625267029
Epoch 2180, training loss: 423.3398132324219 = 0.49103087186813354 + 50.0 * 8.456975936889648
Epoch 2180, val loss: 0.5813220739364624
Epoch 2190, training loss: 423.3946838378906 = 0.4902886152267456 + 50.0 * 8.458087921142578
Epoch 2190, val loss: 0.5809915065765381
Epoch 2200, training loss: 423.3472595214844 = 0.4895060062408447 + 50.0 * 8.457155227661133
Epoch 2200, val loss: 0.5805636644363403
Epoch 2210, training loss: 423.4363098144531 = 0.488719642162323 + 50.0 * 8.458951950073242
Epoch 2210, val loss: 0.5802497267723083
Epoch 2220, training loss: 423.3407897949219 = 0.48793137073516846 + 50.0 * 8.457056999206543
Epoch 2220, val loss: 0.5798292756080627
Epoch 2230, training loss: 423.30377197265625 = 0.48715850710868835 + 50.0 * 8.456332206726074
Epoch 2230, val loss: 0.579491376876831
Epoch 2240, training loss: 423.2854919433594 = 0.4863939881324768 + 50.0 * 8.455982208251953
Epoch 2240, val loss: 0.5791578888893127
Epoch 2250, training loss: 423.2710876464844 = 0.4856274127960205 + 50.0 * 8.455709457397461
Epoch 2250, val loss: 0.5788105726242065
Epoch 2260, training loss: 423.5455322265625 = 0.4848465621471405 + 50.0 * 8.461214065551758
Epoch 2260, val loss: 0.5784699320793152
Epoch 2270, training loss: 423.3045349121094 = 0.48397374153137207 + 50.0 * 8.456411361694336
Epoch 2270, val loss: 0.578036904335022
Epoch 2280, training loss: 423.2720947265625 = 0.48314547538757324 + 50.0 * 8.455779075622559
Epoch 2280, val loss: 0.5776097774505615
Epoch 2290, training loss: 423.23651123046875 = 0.48234492540359497 + 50.0 * 8.455083847045898
Epoch 2290, val loss: 0.5772619843482971
Epoch 2300, training loss: 423.2109680175781 = 0.4815416634082794 + 50.0 * 8.454588890075684
Epoch 2300, val loss: 0.5768722891807556
Epoch 2310, training loss: 423.1974182128906 = 0.4807251989841461 + 50.0 * 8.454334259033203
Epoch 2310, val loss: 0.5765171051025391
Epoch 2320, training loss: 423.19854736328125 = 0.47989746928215027 + 50.0 * 8.454373359680176
Epoch 2320, val loss: 0.5761183500289917
Epoch 2330, training loss: 423.5147399902344 = 0.47904273867607117 + 50.0 * 8.460714340209961
Epoch 2330, val loss: 0.5756738781929016
Epoch 2340, training loss: 423.25714111328125 = 0.4781185984611511 + 50.0 * 8.455580711364746
Epoch 2340, val loss: 0.5753182768821716
Epoch 2350, training loss: 423.1795959472656 = 0.47723299264907837 + 50.0 * 8.454047203063965
Epoch 2350, val loss: 0.5749382972717285
Epoch 2360, training loss: 423.1612854003906 = 0.47638165950775146 + 50.0 * 8.45369815826416
Epoch 2360, val loss: 0.5745149850845337
Epoch 2370, training loss: 423.1366271972656 = 0.47553005814552307 + 50.0 * 8.453222274780273
Epoch 2370, val loss: 0.5741580724716187
Epoch 2380, training loss: 423.1765441894531 = 0.4746687710285187 + 50.0 * 8.4540376663208
Epoch 2380, val loss: 0.5738120675086975
Epoch 2390, training loss: 423.2702331542969 = 0.473764568567276 + 50.0 * 8.45592975616455
Epoch 2390, val loss: 0.5734093189239502
Epoch 2400, training loss: 423.1705627441406 = 0.4728444218635559 + 50.0 * 8.453954696655273
Epoch 2400, val loss: 0.5730001330375671
Epoch 2410, training loss: 423.1465759277344 = 0.4719495475292206 + 50.0 * 8.453492164611816
Epoch 2410, val loss: 0.5726585984230042
Epoch 2420, training loss: 423.21197509765625 = 0.47104546427726746 + 50.0 * 8.454818725585938
Epoch 2420, val loss: 0.5723080039024353
Epoch 2430, training loss: 423.1309509277344 = 0.4701160490512848 + 50.0 * 8.453216552734375
Epoch 2430, val loss: 0.5720075964927673
Epoch 2440, training loss: 423.0635681152344 = 0.46918854117393494 + 50.0 * 8.451888084411621
Epoch 2440, val loss: 0.5716638565063477
Epoch 2450, training loss: 423.0487976074219 = 0.4682692289352417 + 50.0 * 8.451610565185547
Epoch 2450, val loss: 0.5713384747505188
Epoch 2460, training loss: 423.054443359375 = 0.467341810464859 + 50.0 * 8.451742172241211
Epoch 2460, val loss: 0.5709986090660095
Epoch 2470, training loss: 423.4441833496094 = 0.46638819575309753 + 50.0 * 8.459555625915527
Epoch 2470, val loss: 0.5706232190132141
Epoch 2480, training loss: 423.1349792480469 = 0.4653608500957489 + 50.0 * 8.453392028808594
Epoch 2480, val loss: 0.5703126192092896
Epoch 2490, training loss: 423.02374267578125 = 0.4643859565258026 + 50.0 * 8.451187133789062
Epoch 2490, val loss: 0.5699535608291626
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7554540842212074
0.8161269289284939
=== training gcn model ===
Epoch 0, training loss: 530.2012939453125 = 1.086808204650879 + 50.0 * 10.582290649414062
Epoch 0, val loss: 1.0875763893127441
Epoch 10, training loss: 530.1824340820312 = 1.0828747749328613 + 50.0 * 10.581991195678711
Epoch 10, val loss: 1.0836267471313477
Epoch 20, training loss: 530.1160278320312 = 1.0787087678909302 + 50.0 * 10.580745697021484
Epoch 20, val loss: 1.0794514417648315
Epoch 30, training loss: 529.8223876953125 = 1.074248194694519 + 50.0 * 10.574963569641113
Epoch 30, val loss: 1.0749753713607788
Epoch 40, training loss: 528.6045532226562 = 1.0695315599441528 + 50.0 * 10.550701141357422
Epoch 40, val loss: 1.0702400207519531
Epoch 50, training loss: 524.4779052734375 = 1.0647927522659302 + 50.0 * 10.46826171875
Epoch 50, val loss: 1.0655333995819092
Epoch 60, training loss: 514.0692749023438 = 1.0610562562942505 + 50.0 * 10.260164260864258
Epoch 60, val loss: 1.0618847608566284
Epoch 70, training loss: 502.935302734375 = 1.0583269596099854 + 50.0 * 10.0375394821167
Epoch 70, val loss: 1.0591539144515991
Epoch 80, training loss: 494.86517333984375 = 1.055658221244812 + 50.0 * 9.876190185546875
Epoch 80, val loss: 1.0563479661941528
Epoch 90, training loss: 486.17486572265625 = 1.0521029233932495 + 50.0 * 9.702455520629883
Epoch 90, val loss: 1.0525827407836914
Epoch 100, training loss: 479.9532470703125 = 1.048452615737915 + 50.0 * 9.578095436096191
Epoch 100, val loss: 1.0488263368606567
Epoch 110, training loss: 471.90911865234375 = 1.045209288597107 + 50.0 * 9.417278289794922
Epoch 110, val loss: 1.0455882549285889
Epoch 120, training loss: 461.9647521972656 = 1.0424106121063232 + 50.0 * 9.218446731567383
Epoch 120, val loss: 1.0429176092147827
Epoch 130, training loss: 456.5189514160156 = 1.0399425029754639 + 50.0 * 9.109580039978027
Epoch 130, val loss: 1.040687918663025
Epoch 140, training loss: 450.98565673828125 = 1.0378090143203735 + 50.0 * 8.998956680297852
Epoch 140, val loss: 1.038777232170105
Epoch 150, training loss: 447.3665771484375 = 1.0357369184494019 + 50.0 * 8.926616668701172
Epoch 150, val loss: 1.0368331670761108
Epoch 160, training loss: 445.0556640625 = 1.0333913564682007 + 50.0 * 8.88044548034668
Epoch 160, val loss: 1.0345864295959473
Epoch 170, training loss: 443.4054870605469 = 1.0307337045669556 + 50.0 * 8.847495079040527
Epoch 170, val loss: 1.0320324897766113
Epoch 180, training loss: 442.0987548828125 = 1.0277695655822754 + 50.0 * 8.821419715881348
Epoch 180, val loss: 1.0291774272918701
Epoch 190, training loss: 440.95611572265625 = 1.0245674848556519 + 50.0 * 8.798630714416504
Epoch 190, val loss: 1.0261043310165405
Epoch 200, training loss: 439.75006103515625 = 1.021196722984314 + 50.0 * 8.774577140808105
Epoch 200, val loss: 1.022868037223816
Epoch 210, training loss: 438.47601318359375 = 1.017699122428894 + 50.0 * 8.749166488647461
Epoch 210, val loss: 1.0195101499557495
Epoch 220, training loss: 437.3159484863281 = 1.0139927864074707 + 50.0 * 8.726038932800293
Epoch 220, val loss: 1.0159590244293213
Epoch 230, training loss: 436.341064453125 = 1.0098578929901123 + 50.0 * 8.706624031066895
Epoch 230, val loss: 1.0119668245315552
Epoch 240, training loss: 435.52655029296875 = 1.0052530765533447 + 50.0 * 8.690425872802734
Epoch 240, val loss: 1.0075215101242065
Epoch 250, training loss: 434.949462890625 = 1.0001294612884521 + 50.0 * 8.678986549377441
Epoch 250, val loss: 1.002592921257019
Epoch 260, training loss: 434.5003967285156 = 0.9944770932197571 + 50.0 * 8.67011833190918
Epoch 260, val loss: 0.9971586465835571
Epoch 270, training loss: 434.115478515625 = 0.9882758259773254 + 50.0 * 8.662544250488281
Epoch 270, val loss: 0.9912132024765015
Epoch 280, training loss: 433.7779235839844 = 0.9815495610237122 + 50.0 * 8.655927658081055
Epoch 280, val loss: 0.9847697615623474
Epoch 290, training loss: 433.470703125 = 0.9743024706840515 + 50.0 * 8.649928092956543
Epoch 290, val loss: 0.9778397083282471
Epoch 300, training loss: 433.0875549316406 = 0.9665440917015076 + 50.0 * 8.642419815063477
Epoch 300, val loss: 0.9704371690750122
Epoch 310, training loss: 432.736083984375 = 0.9583027362823486 + 50.0 * 8.635555267333984
Epoch 310, val loss: 0.9625853896141052
Epoch 320, training loss: 432.4420166015625 = 0.9495691061019897 + 50.0 * 8.629849433898926
Epoch 320, val loss: 0.9542791843414307
Epoch 330, training loss: 432.0254821777344 = 0.9403642416000366 + 50.0 * 8.621702194213867
Epoch 330, val loss: 0.9455307722091675
Epoch 340, training loss: 431.67236328125 = 0.9307220578193665 + 50.0 * 8.614832878112793
Epoch 340, val loss: 0.9363827705383301
Epoch 350, training loss: 431.334716796875 = 0.9206356406211853 + 50.0 * 8.608282089233398
Epoch 350, val loss: 0.926829993724823
Epoch 360, training loss: 431.0250549316406 = 0.9101414084434509 + 50.0 * 8.60229778289795
Epoch 360, val loss: 0.9169080853462219
Epoch 370, training loss: 430.72406005859375 = 0.8992933034896851 + 50.0 * 8.596495628356934
Epoch 370, val loss: 0.9066898226737976
Epoch 380, training loss: 430.4656066894531 = 0.8881198763847351 + 50.0 * 8.59154987335205
Epoch 380, val loss: 0.8961997032165527
Epoch 390, training loss: 430.2030944824219 = 0.8766607642173767 + 50.0 * 8.586528778076172
Epoch 390, val loss: 0.8854301571846008
Epoch 400, training loss: 429.96771240234375 = 0.8649459481239319 + 50.0 * 8.58205509185791
Epoch 400, val loss: 0.8744992613792419
Epoch 410, training loss: 429.7298278808594 = 0.8530821800231934 + 50.0 * 8.577534675598145
Epoch 410, val loss: 0.8634619116783142
Epoch 420, training loss: 429.6132507324219 = 0.8411281108856201 + 50.0 * 8.57544231414795
Epoch 420, val loss: 0.8523566126823425
Epoch 430, training loss: 429.3106384277344 = 0.8291043639183044 + 50.0 * 8.56963062286377
Epoch 430, val loss: 0.8412677645683289
Epoch 440, training loss: 429.0821533203125 = 0.8171371221542358 + 50.0 * 8.565299987792969
Epoch 440, val loss: 0.8302667737007141
Epoch 450, training loss: 428.979248046875 = 0.8052651882171631 + 50.0 * 8.56347942352295
Epoch 450, val loss: 0.819411039352417
Epoch 460, training loss: 428.72802734375 = 0.7935122847557068 + 50.0 * 8.558690071105957
Epoch 460, val loss: 0.8086664080619812
Epoch 470, training loss: 428.5088195800781 = 0.7819277048110962 + 50.0 * 8.554537773132324
Epoch 470, val loss: 0.798184871673584
Epoch 480, training loss: 428.29388427734375 = 0.7705993056297302 + 50.0 * 8.55046558380127
Epoch 480, val loss: 0.7879507541656494
Epoch 490, training loss: 428.1084289550781 = 0.759520411491394 + 50.0 * 8.546977996826172
Epoch 490, val loss: 0.7779983282089233
Epoch 500, training loss: 428.0255432128906 = 0.7486962676048279 + 50.0 * 8.545536994934082
Epoch 500, val loss: 0.768348217010498
Epoch 510, training loss: 427.9307556152344 = 0.7381604909896851 + 50.0 * 8.543851852416992
Epoch 510, val loss: 0.7589232325553894
Epoch 520, training loss: 427.6874084472656 = 0.7279242873191833 + 50.0 * 8.539189338684082
Epoch 520, val loss: 0.749896228313446
Epoch 530, training loss: 427.5284423828125 = 0.7180672883987427 + 50.0 * 8.53620719909668
Epoch 530, val loss: 0.7412367463111877
Epoch 540, training loss: 427.39654541015625 = 0.7085754871368408 + 50.0 * 8.533759117126465
Epoch 540, val loss: 0.7329530119895935
Epoch 550, training loss: 427.27685546875 = 0.6994536519050598 + 50.0 * 8.531547546386719
Epoch 550, val loss: 0.7250438332557678
Epoch 560, training loss: 427.2203674316406 = 0.6906813383102417 + 50.0 * 8.530593872070312
Epoch 560, val loss: 0.7174997925758362
Epoch 570, training loss: 427.2686767578125 = 0.682260274887085 + 50.0 * 8.531728744506836
Epoch 570, val loss: 0.71026211977005
Epoch 580, training loss: 426.98858642578125 = 0.6741836071014404 + 50.0 * 8.526288032531738
Epoch 580, val loss: 0.7034301161766052
Epoch 590, training loss: 426.88104248046875 = 0.6665357947349548 + 50.0 * 8.524290084838867
Epoch 590, val loss: 0.6970129609107971
Epoch 600, training loss: 426.78106689453125 = 0.6592745780944824 + 50.0 * 8.522436141967773
Epoch 600, val loss: 0.6909675598144531
Epoch 610, training loss: 426.6895446777344 = 0.6523599028587341 + 50.0 * 8.520743370056152
Epoch 610, val loss: 0.6852623820304871
Epoch 620, training loss: 426.601806640625 = 0.645784318447113 + 50.0 * 8.519120216369629
Epoch 620, val loss: 0.6798945665359497
Epoch 630, training loss: 426.51904296875 = 0.6395464539527893 + 50.0 * 8.517589569091797
Epoch 630, val loss: 0.674856960773468
Epoch 640, training loss: 426.5685729980469 = 0.6336359977722168 + 50.0 * 8.518698692321777
Epoch 640, val loss: 0.6701378226280212
Epoch 650, training loss: 426.4884033203125 = 0.6279895901679993 + 50.0 * 8.517208099365234
Epoch 650, val loss: 0.6656643152236938
Epoch 660, training loss: 426.3355712890625 = 0.6226616501808167 + 50.0 * 8.51425838470459
Epoch 660, val loss: 0.6615167856216431
Epoch 670, training loss: 426.2358703613281 = 0.6176594495773315 + 50.0 * 8.512364387512207
Epoch 670, val loss: 0.6576564311981201
Epoch 680, training loss: 426.1637878417969 = 0.6129394173622131 + 50.0 * 8.511016845703125
Epoch 680, val loss: 0.6540712118148804
Epoch 690, training loss: 426.1033020019531 = 0.608478307723999 + 50.0 * 8.509896278381348
Epoch 690, val loss: 0.6507260203361511
Epoch 700, training loss: 426.328125 = 0.60425865650177 + 50.0 * 8.514477729797363
Epoch 700, val loss: 0.6475905179977417
Epoch 710, training loss: 426.0618591308594 = 0.6002272367477417 + 50.0 * 8.509232521057129
Epoch 710, val loss: 0.6446611285209656
Epoch 720, training loss: 425.9386291503906 = 0.5964524149894714 + 50.0 * 8.506843566894531
Epoch 720, val loss: 0.6419525742530823
Epoch 730, training loss: 425.87664794921875 = 0.5929002165794373 + 50.0 * 8.505675315856934
Epoch 730, val loss: 0.6394366025924683
Epoch 740, training loss: 425.826416015625 = 0.5895500183105469 + 50.0 * 8.504737854003906
Epoch 740, val loss: 0.6371064782142639
Epoch 750, training loss: 425.7738037109375 = 0.5863762497901917 + 50.0 * 8.503748893737793
Epoch 750, val loss: 0.6349281668663025
Epoch 760, training loss: 425.73162841796875 = 0.5833678245544434 + 50.0 * 8.502964973449707
Epoch 760, val loss: 0.6328882575035095
Epoch 770, training loss: 426.0791320800781 = 0.5805066227912903 + 50.0 * 8.50997257232666
Epoch 770, val loss: 0.6309736371040344
Epoch 780, training loss: 425.6543273925781 = 0.577754557132721 + 50.0 * 8.501531600952148
Epoch 780, val loss: 0.6291426420211792
Epoch 790, training loss: 425.6175842285156 = 0.5751808881759644 + 50.0 * 8.500847816467285
Epoch 790, val loss: 0.627471923828125
Epoch 800, training loss: 425.5628356933594 = 0.5727573037147522 + 50.0 * 8.499801635742188
Epoch 800, val loss: 0.6259308457374573
Epoch 810, training loss: 425.50738525390625 = 0.5704576373100281 + 50.0 * 8.498738288879395
Epoch 810, val loss: 0.6244852542877197
Epoch 820, training loss: 425.464599609375 = 0.5682649612426758 + 50.0 * 8.497926712036133
Epoch 820, val loss: 0.6231216788291931
Epoch 830, training loss: 425.5726623535156 = 0.5661876201629639 + 50.0 * 8.500129699707031
Epoch 830, val loss: 0.6219162940979004
Epoch 840, training loss: 425.4541015625 = 0.5641354918479919 + 50.0 * 8.497798919677734
Epoch 840, val loss: 0.6205753087997437
Epoch 850, training loss: 425.36724853515625 = 0.562212347984314 + 50.0 * 8.496100425720215
Epoch 850, val loss: 0.6194117665290833
Epoch 860, training loss: 425.3017578125 = 0.560394823551178 + 50.0 * 8.494827270507812
Epoch 860, val loss: 0.6183592677116394
Epoch 870, training loss: 425.25830078125 = 0.5586593151092529 + 50.0 * 8.493992805480957
Epoch 870, val loss: 0.6173678636550903
Epoch 880, training loss: 425.4373779296875 = 0.556976854801178 + 50.0 * 8.497608184814453
Epoch 880, val loss: 0.6164022088050842
Epoch 890, training loss: 425.2029113769531 = 0.5553390383720398 + 50.0 * 8.492951393127441
Epoch 890, val loss: 0.6154631972312927
Epoch 900, training loss: 425.1177062988281 = 0.5537850260734558 + 50.0 * 8.491278648376465
Epoch 900, val loss: 0.6145806908607483
Epoch 910, training loss: 425.0763854980469 = 0.5523056387901306 + 50.0 * 8.49048137664795
Epoch 910, val loss: 0.6137616634368896
Epoch 920, training loss: 425.0645446777344 = 0.550874650478363 + 50.0 * 8.490273475646973
Epoch 920, val loss: 0.6129685640335083
Epoch 930, training loss: 425.0373840332031 = 0.5494768619537354 + 50.0 * 8.489758491516113
Epoch 930, val loss: 0.6122300028800964
Epoch 940, training loss: 424.9564514160156 = 0.5481186509132385 + 50.0 * 8.488166809082031
Epoch 940, val loss: 0.6114676594734192
Epoch 950, training loss: 424.9066162109375 = 0.5468232035636902 + 50.0 * 8.48719596862793
Epoch 950, val loss: 0.6107675433158875
Epoch 960, training loss: 424.9490966796875 = 0.5455665588378906 + 50.0 * 8.488070487976074
Epoch 960, val loss: 0.6101185083389282
Epoch 970, training loss: 424.7929992675781 = 0.5443205237388611 + 50.0 * 8.484973907470703
Epoch 970, val loss: 0.6094605922698975
Epoch 980, training loss: 424.7843322753906 = 0.5431214570999146 + 50.0 * 8.484824180603027
Epoch 980, val loss: 0.6088272333145142
Epoch 990, training loss: 424.71563720703125 = 0.5419571995735168 + 50.0 * 8.483473777770996
Epoch 990, val loss: 0.6082371473312378
Epoch 1000, training loss: 424.8992614746094 = 0.540809690952301 + 50.0 * 8.48716926574707
Epoch 1000, val loss: 0.6076286435127258
Epoch 1010, training loss: 424.6568603515625 = 0.5396879315376282 + 50.0 * 8.482343673706055
Epoch 1010, val loss: 0.6070634722709656
Epoch 1020, training loss: 424.6979064941406 = 0.5385922789573669 + 50.0 * 8.483185768127441
Epoch 1020, val loss: 0.6064878106117249
Epoch 1030, training loss: 424.5561218261719 = 0.5375081300735474 + 50.0 * 8.480372428894043
Epoch 1030, val loss: 0.6059393286705017
Epoch 1040, training loss: 424.53619384765625 = 0.5364601016044617 + 50.0 * 8.479994773864746
Epoch 1040, val loss: 0.6054167151451111
Epoch 1050, training loss: 424.4834899902344 = 0.5354446172714233 + 50.0 * 8.478960990905762
Epoch 1050, val loss: 0.6049022674560547
Epoch 1060, training loss: 424.46575927734375 = 0.5344457030296326 + 50.0 * 8.478626251220703
Epoch 1060, val loss: 0.604411780834198
Epoch 1070, training loss: 424.47552490234375 = 0.533455491065979 + 50.0 * 8.478841781616211
Epoch 1070, val loss: 0.6039175391197205
Epoch 1080, training loss: 424.3914794921875 = 0.5324702858924866 + 50.0 * 8.477180480957031
Epoch 1080, val loss: 0.6034170389175415
Epoch 1090, training loss: 424.6679382324219 = 0.5314990282058716 + 50.0 * 8.482728958129883
Epoch 1090, val loss: 0.6029753684997559
Epoch 1100, training loss: 424.4282531738281 = 0.5305214524269104 + 50.0 * 8.477954864501953
Epoch 1100, val loss: 0.6023831963539124
Epoch 1110, training loss: 424.2975769042969 = 0.5295761823654175 + 50.0 * 8.475359916687012
Epoch 1110, val loss: 0.601876974105835
Epoch 1120, training loss: 424.2593078613281 = 0.5286736488342285 + 50.0 * 8.47461223602295
Epoch 1120, val loss: 0.601473867893219
Epoch 1130, training loss: 424.2176208496094 = 0.5277822017669678 + 50.0 * 8.473796844482422
Epoch 1130, val loss: 0.6010211706161499
Epoch 1140, training loss: 424.18792724609375 = 0.526899516582489 + 50.0 * 8.473220825195312
Epoch 1140, val loss: 0.6005702018737793
Epoch 1150, training loss: 424.1600036621094 = 0.5260199904441833 + 50.0 * 8.472679138183594
Epoch 1150, val loss: 0.6001275777816772
Epoch 1160, training loss: 424.1900939941406 = 0.5251533389091492 + 50.0 * 8.473299026489258
Epoch 1160, val loss: 0.5996959209442139
Epoch 1170, training loss: 424.1413269042969 = 0.524258017539978 + 50.0 * 8.472341537475586
Epoch 1170, val loss: 0.5991555452346802
Epoch 1180, training loss: 424.15625 = 0.5233741402626038 + 50.0 * 8.472657203674316
Epoch 1180, val loss: 0.5987306237220764
Epoch 1190, training loss: 424.06982421875 = 0.5225315093994141 + 50.0 * 8.470946311950684
Epoch 1190, val loss: 0.5982980728149414
Epoch 1200, training loss: 424.0351257324219 = 0.521710991859436 + 50.0 * 8.470268249511719
Epoch 1200, val loss: 0.5978315472602844
Epoch 1210, training loss: 424.0099182128906 = 0.5209047198295593 + 50.0 * 8.469779968261719
Epoch 1210, val loss: 0.597413957118988
Epoch 1220, training loss: 423.9907531738281 = 0.5201040506362915 + 50.0 * 8.469412803649902
Epoch 1220, val loss: 0.5970109105110168
Epoch 1230, training loss: 424.3374328613281 = 0.5192970633506775 + 50.0 * 8.476363182067871
Epoch 1230, val loss: 0.5965749621391296
Epoch 1240, training loss: 423.9493408203125 = 0.5184659957885742 + 50.0 * 8.46861743927002
Epoch 1240, val loss: 0.5960646867752075
Epoch 1250, training loss: 423.9290771484375 = 0.5176717042922974 + 50.0 * 8.468228340148926
Epoch 1250, val loss: 0.5956859588623047
Epoch 1260, training loss: 423.9064025878906 = 0.5169015526771545 + 50.0 * 8.467789649963379
Epoch 1260, val loss: 0.5952574610710144
Epoch 1270, training loss: 423.8869323730469 = 0.5161418318748474 + 50.0 * 8.467415809631348
Epoch 1270, val loss: 0.5948522686958313
Epoch 1280, training loss: 424.11199951171875 = 0.5153887271881104 + 50.0 * 8.471932411193848
Epoch 1280, val loss: 0.5944327116012573
Epoch 1290, training loss: 423.9259033203125 = 0.5145788192749023 + 50.0 * 8.468226432800293
Epoch 1290, val loss: 0.5939803719520569
Epoch 1300, training loss: 423.8397521972656 = 0.5138237476348877 + 50.0 * 8.46651840209961
Epoch 1300, val loss: 0.5935544967651367
Epoch 1310, training loss: 423.7957763671875 = 0.5130795836448669 + 50.0 * 8.465653419494629
Epoch 1310, val loss: 0.5931394100189209
Epoch 1320, training loss: 423.7700500488281 = 0.512349009513855 + 50.0 * 8.465153694152832
Epoch 1320, val loss: 0.5927473902702332
Epoch 1330, training loss: 423.75262451171875 = 0.511620819568634 + 50.0 * 8.46481990814209
Epoch 1330, val loss: 0.5923660397529602
Epoch 1340, training loss: 423.85675048828125 = 0.5108984708786011 + 50.0 * 8.466917037963867
Epoch 1340, val loss: 0.5920363664627075
Epoch 1350, training loss: 423.811767578125 = 0.5101259350776672 + 50.0 * 8.466032981872559
Epoch 1350, val loss: 0.5914674997329712
Epoch 1360, training loss: 423.72918701171875 = 0.5093931555747986 + 50.0 * 8.464395523071289
Epoch 1360, val loss: 0.5911145210266113
Epoch 1370, training loss: 423.68414306640625 = 0.5086753964424133 + 50.0 * 8.463509559631348
Epoch 1370, val loss: 0.5907098054885864
Epoch 1380, training loss: 423.66790771484375 = 0.5079709887504578 + 50.0 * 8.4631986618042
Epoch 1380, val loss: 0.5902702808380127
Epoch 1390, training loss: 423.7718505859375 = 0.5072731971740723 + 50.0 * 8.465291976928711
Epoch 1390, val loss: 0.589837908744812
Epoch 1400, training loss: 423.6390075683594 = 0.5065364837646484 + 50.0 * 8.46264934539795
Epoch 1400, val loss: 0.5894842147827148
Epoch 1410, training loss: 423.62579345703125 = 0.5058181881904602 + 50.0 * 8.46239948272705
Epoch 1410, val loss: 0.589016318321228
Epoch 1420, training loss: 423.5810852050781 = 0.5051209330558777 + 50.0 * 8.461519241333008
Epoch 1420, val loss: 0.5886364579200745
Epoch 1430, training loss: 423.5736083984375 = 0.5044271945953369 + 50.0 * 8.461383819580078
Epoch 1430, val loss: 0.5882412791252136
Epoch 1440, training loss: 423.61279296875 = 0.5037317276000977 + 50.0 * 8.462181091308594
Epoch 1440, val loss: 0.5878066420555115
Epoch 1450, training loss: 423.6031494140625 = 0.5030167102813721 + 50.0 * 8.462002754211426
Epoch 1450, val loss: 0.587382435798645
Epoch 1460, training loss: 423.76312255859375 = 0.502291202545166 + 50.0 * 8.465216636657715
Epoch 1460, val loss: 0.5869122743606567
Epoch 1470, training loss: 423.54168701171875 = 0.5015790462493896 + 50.0 * 8.46080207824707
Epoch 1470, val loss: 0.5865486860275269
Epoch 1480, training loss: 423.4698791503906 = 0.5008837580680847 + 50.0 * 8.459380149841309
Epoch 1480, val loss: 0.5861164331436157
Epoch 1490, training loss: 423.4548645019531 = 0.5002031326293945 + 50.0 * 8.45909309387207
Epoch 1490, val loss: 0.5857154130935669
Epoch 1500, training loss: 423.4358215332031 = 0.4995203912258148 + 50.0 * 8.458725929260254
Epoch 1500, val loss: 0.5853264927864075
Epoch 1510, training loss: 423.42608642578125 = 0.4988362789154053 + 50.0 * 8.458544731140137
Epoch 1510, val loss: 0.5849080085754395
Epoch 1520, training loss: 423.888671875 = 0.4981519877910614 + 50.0 * 8.46781063079834
Epoch 1520, val loss: 0.5844154357910156
Epoch 1530, training loss: 423.504150390625 = 0.4973863363265991 + 50.0 * 8.460135459899902
Epoch 1530, val loss: 0.5840312838554382
Epoch 1540, training loss: 423.3883361816406 = 0.49668648838996887 + 50.0 * 8.457833290100098
Epoch 1540, val loss: 0.5836543440818787
Epoch 1550, training loss: 423.3689880371094 = 0.4960010051727295 + 50.0 * 8.457459449768066
Epoch 1550, val loss: 0.5831460952758789
Epoch 1560, training loss: 423.3334045410156 = 0.4953223466873169 + 50.0 * 8.456761360168457
Epoch 1560, val loss: 0.5827853679656982
Epoch 1570, training loss: 423.35943603515625 = 0.4946381747722626 + 50.0 * 8.457296371459961
Epoch 1570, val loss: 0.5823939442634583
Epoch 1580, training loss: 423.3082580566406 = 0.4939276874065399 + 50.0 * 8.456286430358887
Epoch 1580, val loss: 0.581936776638031
Epoch 1590, training loss: 423.339111328125 = 0.49322009086608887 + 50.0 * 8.456917762756348
Epoch 1590, val loss: 0.5815138816833496
Epoch 1600, training loss: 423.2686462402344 = 0.4925236403942108 + 50.0 * 8.455522537231445
Epoch 1600, val loss: 0.5810796618461609
Epoch 1610, training loss: 423.289306640625 = 0.491835355758667 + 50.0 * 8.455949783325195
Epoch 1610, val loss: 0.5806914567947388
Epoch 1620, training loss: 423.3583068847656 = 0.4911365807056427 + 50.0 * 8.457343101501465
Epoch 1620, val loss: 0.5802345275878906
Epoch 1630, training loss: 423.2464599609375 = 0.4904271066188812 + 50.0 * 8.455121040344238
Epoch 1630, val loss: 0.5797942876815796
Epoch 1640, training loss: 423.20538330078125 = 0.48972851037979126 + 50.0 * 8.454313278198242
Epoch 1640, val loss: 0.5793482065200806
Epoch 1650, training loss: 423.1908264160156 = 0.48904135823249817 + 50.0 * 8.454035758972168
Epoch 1650, val loss: 0.5789756774902344
Epoch 1660, training loss: 423.3147888183594 = 0.4883500039577484 + 50.0 * 8.456528663635254
Epoch 1660, val loss: 0.5785558223724365
Epoch 1670, training loss: 423.19000244140625 = 0.4876251518726349 + 50.0 * 8.454047203063965
Epoch 1670, val loss: 0.5780776739120483
Epoch 1680, training loss: 423.1833190917969 = 0.4869129955768585 + 50.0 * 8.453927993774414
Epoch 1680, val loss: 0.5776209831237793
Epoch 1690, training loss: 423.15130615234375 = 0.48622554540634155 + 50.0 * 8.453301429748535
Epoch 1690, val loss: 0.5772109031677246
Epoch 1700, training loss: 423.11474609375 = 0.4855412542819977 + 50.0 * 8.452584266662598
Epoch 1700, val loss: 0.5767878890037537
Epoch 1710, training loss: 423.1026306152344 = 0.48486092686653137 + 50.0 * 8.45235538482666
Epoch 1710, val loss: 0.5763875246047974
Epoch 1720, training loss: 423.3888244628906 = 0.48417720198631287 + 50.0 * 8.45809268951416
Epoch 1720, val loss: 0.5759504437446594
Epoch 1730, training loss: 423.156982421875 = 0.48341885209083557 + 50.0 * 8.453471183776855
Epoch 1730, val loss: 0.5755314230918884
Epoch 1740, training loss: 423.0714111328125 = 0.48272058367729187 + 50.0 * 8.451773643493652
Epoch 1740, val loss: 0.5751477479934692
Epoch 1750, training loss: 423.0399169921875 = 0.4820203185081482 + 50.0 * 8.451157569885254
Epoch 1750, val loss: 0.5747019052505493
Epoch 1760, training loss: 423.0213623046875 = 0.4813289940357208 + 50.0 * 8.450800895690918
Epoch 1760, val loss: 0.5743292570114136
Epoch 1770, training loss: 423.0137939453125 = 0.4806358516216278 + 50.0 * 8.450663566589355
Epoch 1770, val loss: 0.5738953351974487
Epoch 1780, training loss: 423.19024658203125 = 0.4799323081970215 + 50.0 * 8.454206466674805
Epoch 1780, val loss: 0.5733979344367981
Epoch 1790, training loss: 423.03961181640625 = 0.47920915484428406 + 50.0 * 8.451208114624023
Epoch 1790, val loss: 0.5731863975524902
Epoch 1800, training loss: 422.9867248535156 = 0.4784907102584839 + 50.0 * 8.450164794921875
Epoch 1800, val loss: 0.5726255774497986
Epoch 1810, training loss: 422.9564208984375 = 0.47779422998428345 + 50.0 * 8.449572563171387
Epoch 1810, val loss: 0.572315514087677
Epoch 1820, training loss: 423.0137023925781 = 0.4770870506763458 + 50.0 * 8.450732231140137
Epoch 1820, val loss: 0.5719017386436462
Epoch 1830, training loss: 423.0092468261719 = 0.47635623812675476 + 50.0 * 8.450657844543457
Epoch 1830, val loss: 0.5714873671531677
Epoch 1840, training loss: 422.9065246582031 = 0.47562381625175476 + 50.0 * 8.448617935180664
Epoch 1840, val loss: 0.5710424780845642
Epoch 1850, training loss: 422.89093017578125 = 0.4749065339565277 + 50.0 * 8.448320388793945
Epoch 1850, val loss: 0.5706406235694885
Epoch 1860, training loss: 422.89630126953125 = 0.4741918742656708 + 50.0 * 8.448442459106445
Epoch 1860, val loss: 0.5702966451644897
Epoch 1870, training loss: 422.9347839355469 = 0.4734724164009094 + 50.0 * 8.449226379394531
Epoch 1870, val loss: 0.5698732137680054
Epoch 1880, training loss: 422.8829040527344 = 0.47274070978164673 + 50.0 * 8.448203086853027
Epoch 1880, val loss: 0.569456160068512
Epoch 1890, training loss: 422.84130859375 = 0.47200730443000793 + 50.0 * 8.447385787963867
Epoch 1890, val loss: 0.5690857768058777
Epoch 1900, training loss: 422.89727783203125 = 0.4712734818458557 + 50.0 * 8.448519706726074
Epoch 1900, val loss: 0.5687031149864197
Epoch 1910, training loss: 422.86041259765625 = 0.47052648663520813 + 50.0 * 8.447797775268555
Epoch 1910, val loss: 0.5683320760726929
Epoch 1920, training loss: 422.8301086425781 = 0.4697777032852173 + 50.0 * 8.447206497192383
Epoch 1920, val loss: 0.5679425001144409
Epoch 1930, training loss: 422.79205322265625 = 0.4690425395965576 + 50.0 * 8.446459770202637
Epoch 1930, val loss: 0.5675584077835083
Epoch 1940, training loss: 422.77471923828125 = 0.46830981969833374 + 50.0 * 8.446127891540527
Epoch 1940, val loss: 0.5672250390052795
Epoch 1950, training loss: 422.8168640136719 = 0.4675689935684204 + 50.0 * 8.446986198425293
Epoch 1950, val loss: 0.5668736696243286
Epoch 1960, training loss: 422.79339599609375 = 0.46681350469589233 + 50.0 * 8.446531295776367
Epoch 1960, val loss: 0.5664715766906738
Epoch 1970, training loss: 422.7361145019531 = 0.46606799960136414 + 50.0 * 8.445401191711426
Epoch 1970, val loss: 0.5660933256149292
Epoch 1980, training loss: 422.7312927246094 = 0.465316504240036 + 50.0 * 8.445320129394531
Epoch 1980, val loss: 0.5657095313072205
Epoch 1990, training loss: 422.78387451171875 = 0.46455439925193787 + 50.0 * 8.446386337280273
Epoch 1990, val loss: 0.565343976020813
Epoch 2000, training loss: 422.6839599609375 = 0.4637880325317383 + 50.0 * 8.444403648376465
Epoch 2000, val loss: 0.5650630593299866
Epoch 2010, training loss: 422.71954345703125 = 0.4630182981491089 + 50.0 * 8.445130348205566
Epoch 2010, val loss: 0.5647203326225281
Epoch 2020, training loss: 422.85772705078125 = 0.46222442388534546 + 50.0 * 8.44791030883789
Epoch 2020, val loss: 0.564272403717041
Epoch 2030, training loss: 422.66705322265625 = 0.4614281952381134 + 50.0 * 8.444112777709961
Epoch 2030, val loss: 0.5639020204544067
Epoch 2040, training loss: 422.6337585449219 = 0.46064648032188416 + 50.0 * 8.443462371826172
Epoch 2040, val loss: 0.5635237097740173
Epoch 2050, training loss: 422.6200256347656 = 0.4598741829395294 + 50.0 * 8.44320297241211
Epoch 2050, val loss: 0.5632032155990601
Epoch 2060, training loss: 422.6066589355469 = 0.45910152792930603 + 50.0 * 8.442951202392578
Epoch 2060, val loss: 0.5628663301467896
Epoch 2070, training loss: 422.6944580078125 = 0.45831209421157837 + 50.0 * 8.444723129272461
Epoch 2070, val loss: 0.5625579953193665
Epoch 2080, training loss: 422.6043701171875 = 0.4574979245662689 + 50.0 * 8.442937850952148
Epoch 2080, val loss: 0.5621297955513
Epoch 2090, training loss: 422.56805419921875 = 0.45668551325798035 + 50.0 * 8.442227363586426
Epoch 2090, val loss: 0.5617709755897522
Epoch 2100, training loss: 422.5567932128906 = 0.45588165521621704 + 50.0 * 8.442018508911133
Epoch 2100, val loss: 0.5614087581634521
Epoch 2110, training loss: 422.5537109375 = 0.4550783634185791 + 50.0 * 8.441972732543945
Epoch 2110, val loss: 0.5610573291778564
Epoch 2120, training loss: 422.7042541503906 = 0.45426714420318604 + 50.0 * 8.444999694824219
Epoch 2120, val loss: 0.5606803297996521
Epoch 2130, training loss: 422.53741455078125 = 0.45342475175857544 + 50.0 * 8.441679954528809
Epoch 2130, val loss: 0.560316801071167
Epoch 2140, training loss: 422.52862548828125 = 0.45258811116218567 + 50.0 * 8.441520690917969
Epoch 2140, val loss: 0.5599881410598755
Epoch 2150, training loss: 422.51910400390625 = 0.4517654478549957 + 50.0 * 8.441347122192383
Epoch 2150, val loss: 0.5596200227737427
Epoch 2160, training loss: 422.5274963378906 = 0.4509288966655731 + 50.0 * 8.44153118133545
Epoch 2160, val loss: 0.5592685341835022
Epoch 2170, training loss: 422.50115966796875 = 0.4500790238380432 + 50.0 * 8.441021919250488
Epoch 2170, val loss: 0.5588797330856323
Epoch 2180, training loss: 422.7080383300781 = 0.4492228031158447 + 50.0 * 8.445176124572754
Epoch 2180, val loss: 0.5583364963531494
Epoch 2190, training loss: 422.5384216308594 = 0.44833797216415405 + 50.0 * 8.441802024841309
Epoch 2190, val loss: 0.55832839012146
Epoch 2200, training loss: 422.4554443359375 = 0.4474472403526306 + 50.0 * 8.440159797668457
Epoch 2200, val loss: 0.5578221082687378
Epoch 2210, training loss: 422.4667663574219 = 0.446571946144104 + 50.0 * 8.440403938293457
Epoch 2210, val loss: 0.557523787021637
Epoch 2220, training loss: 422.76904296875 = 0.44567185640335083 + 50.0 * 8.446467399597168
Epoch 2220, val loss: 0.5572544932365417
Epoch 2230, training loss: 422.4974365234375 = 0.44475504755973816 + 50.0 * 8.44105339050293
Epoch 2230, val loss: 0.556729257106781
Epoch 2240, training loss: 422.41632080078125 = 0.4438374936580658 + 50.0 * 8.439449310302734
Epoch 2240, val loss: 0.5563409924507141
Epoch 2250, training loss: 422.3924255371094 = 0.44294026494026184 + 50.0 * 8.438989639282227
Epoch 2250, val loss: 0.5560439229011536
Epoch 2260, training loss: 422.3798522949219 = 0.4420338273048401 + 50.0 * 8.438755989074707
Epoch 2260, val loss: 0.555681049823761
Epoch 2270, training loss: 422.3782653808594 = 0.44112399220466614 + 50.0 * 8.438742637634277
Epoch 2270, val loss: 0.5553468465805054
Epoch 2280, training loss: 422.7289733886719 = 0.4401966333389282 + 50.0 * 8.445775985717773
Epoch 2280, val loss: 0.5550721883773804
Epoch 2290, training loss: 422.4400329589844 = 0.4392416775226593 + 50.0 * 8.44001579284668
Epoch 2290, val loss: 0.5545850396156311
Epoch 2300, training loss: 422.3528747558594 = 0.4382997155189514 + 50.0 * 8.438291549682617
Epoch 2300, val loss: 0.5542839169502258
Epoch 2310, training loss: 422.34423828125 = 0.43736809492111206 + 50.0 * 8.43813705444336
Epoch 2310, val loss: 0.5539172291755676
Epoch 2320, training loss: 422.3426208496094 = 0.43643468618392944 + 50.0 * 8.43812370300293
Epoch 2320, val loss: 0.5535908937454224
Epoch 2330, training loss: 422.5142517089844 = 0.4354867935180664 + 50.0 * 8.441575050354004
Epoch 2330, val loss: 0.553241491317749
Epoch 2340, training loss: 422.3206481933594 = 0.4344930946826935 + 50.0 * 8.437723159790039
Epoch 2340, val loss: 0.5529567003250122
Epoch 2350, training loss: 422.28973388671875 = 0.43351849913597107 + 50.0 * 8.437124252319336
Epoch 2350, val loss: 0.5526348948478699
Epoch 2360, training loss: 422.2914733886719 = 0.4325454533100128 + 50.0 * 8.437178611755371
Epoch 2360, val loss: 0.5523274540901184
Epoch 2370, training loss: 422.33148193359375 = 0.43155917525291443 + 50.0 * 8.43799877166748
Epoch 2370, val loss: 0.5520397424697876
Epoch 2380, training loss: 422.30987548828125 = 0.4305513799190521 + 50.0 * 8.437586784362793
Epoch 2380, val loss: 0.5516864061355591
Epoch 2390, training loss: 422.28070068359375 = 0.42953720688819885 + 50.0 * 8.437023162841797
Epoch 2390, val loss: 0.551385760307312
Epoch 2400, training loss: 422.3051452636719 = 0.42852669954299927 + 50.0 * 8.437532424926758
Epoch 2400, val loss: 0.5510740280151367
Epoch 2410, training loss: 422.260009765625 = 0.4275050163269043 + 50.0 * 8.436650276184082
Epoch 2410, val loss: 0.5507974624633789
Epoch 2420, training loss: 422.22833251953125 = 0.42648816108703613 + 50.0 * 8.436037063598633
Epoch 2420, val loss: 0.5505146384239197
Epoch 2430, training loss: 422.24407958984375 = 0.42546582221984863 + 50.0 * 8.436371803283691
Epoch 2430, val loss: 0.5502068400382996
Epoch 2440, training loss: 422.4205017089844 = 0.4244326651096344 + 50.0 * 8.439921379089355
Epoch 2440, val loss: 0.5498766899108887
Epoch 2450, training loss: 422.22479248046875 = 0.42336711287498474 + 50.0 * 8.436028480529785
Epoch 2450, val loss: 0.5496795177459717
Epoch 2460, training loss: 422.1842956542969 = 0.422313392162323 + 50.0 * 8.435239791870117
Epoch 2460, val loss: 0.5492963790893555
Epoch 2470, training loss: 422.1854553222656 = 0.4212627708911896 + 50.0 * 8.435283660888672
Epoch 2470, val loss: 0.548997163772583
Epoch 2480, training loss: 422.2354431152344 = 0.42020246386528015 + 50.0 * 8.436305046081543
Epoch 2480, val loss: 0.5487048625946045
Epoch 2490, training loss: 422.20989990234375 = 0.4191119968891144 + 50.0 * 8.435815811157227
Epoch 2490, val loss: 0.5483680963516235
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7792998477929984
0.8140259363906398
The final CL Acc:0.76848, 0.00986, The final GNN Acc:0.81492, 0.00089
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110826])
remove edge: torch.Size([2, 66198])
updated graph: torch.Size([2, 88376])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 530.2184448242188 = 1.1025493144989014 + 50.0 * 10.582318305969238
Epoch 0, val loss: 1.1003410816192627
Epoch 10, training loss: 530.2003173828125 = 1.0982773303985596 + 50.0 * 10.582040786743164
Epoch 10, val loss: 1.0961097478866577
Epoch 20, training loss: 530.1340942382812 = 1.0937941074371338 + 50.0 * 10.580805778503418
Epoch 20, val loss: 1.091654896736145
Epoch 30, training loss: 529.8594970703125 = 1.0888947248458862 + 50.0 * 10.57541275024414
Epoch 30, val loss: 1.0867936611175537
Epoch 40, training loss: 528.7833251953125 = 1.0833698511123657 + 50.0 * 10.553998947143555
Epoch 40, val loss: 1.0813261270523071
Epoch 50, training loss: 525.5304565429688 = 1.0771735906600952 + 50.0 * 10.489066123962402
Epoch 50, val loss: 1.0752259492874146
Epoch 60, training loss: 518.077880859375 = 1.0706701278686523 + 50.0 * 10.340144157409668
Epoch 60, val loss: 1.0689266920089722
Epoch 70, training loss: 504.1114196777344 = 1.0640679597854614 + 50.0 * 10.06094741821289
Epoch 70, val loss: 1.0624586343765259
Epoch 80, training loss: 485.03955078125 = 1.0561003684997559 + 50.0 * 9.679669380187988
Epoch 80, val loss: 1.05460786819458
Epoch 90, training loss: 476.8199157714844 = 1.0484195947647095 + 50.0 * 9.515429496765137
Epoch 90, val loss: 1.0472195148468018
Epoch 100, training loss: 469.5740661621094 = 1.0421286821365356 + 50.0 * 9.370638847351074
Epoch 100, val loss: 1.041237235069275
Epoch 110, training loss: 463.6493225097656 = 1.0368931293487549 + 50.0 * 9.252248764038086
Epoch 110, val loss: 1.0362298488616943
Epoch 120, training loss: 460.9779052734375 = 1.0316108465194702 + 50.0 * 9.198925971984863
Epoch 120, val loss: 1.0310667753219604
Epoch 130, training loss: 458.9684143066406 = 1.0254576206207275 + 50.0 * 9.158859252929688
Epoch 130, val loss: 1.0249550342559814
Epoch 140, training loss: 456.7120056152344 = 1.0190478563308716 + 50.0 * 9.113859176635742
Epoch 140, val loss: 1.01868736743927
Epoch 150, training loss: 454.6495361328125 = 1.013291835784912 + 50.0 * 9.072724342346191
Epoch 150, val loss: 1.0131057500839233
Epoch 160, training loss: 453.158203125 = 1.0078903436660767 + 50.0 * 9.04300594329834
Epoch 160, val loss: 1.0077913999557495
Epoch 170, training loss: 452.3450622558594 = 1.0020480155944824 + 50.0 * 9.026860237121582
Epoch 170, val loss: 1.0019140243530273
Epoch 180, training loss: 451.8333435058594 = 0.9950198531150818 + 50.0 * 9.016766548156738
Epoch 180, val loss: 0.9948365688323975
Epoch 190, training loss: 451.335693359375 = 0.9872086644172668 + 50.0 * 9.006969451904297
Epoch 190, val loss: 0.9871704578399658
Epoch 200, training loss: 450.7494201660156 = 0.9792569875717163 + 50.0 * 8.995403289794922
Epoch 200, val loss: 0.9795442819595337
Epoch 210, training loss: 449.93621826171875 = 0.9713805913925171 + 50.0 * 8.979296684265137
Epoch 210, val loss: 0.9720346331596375
Epoch 220, training loss: 448.6659240722656 = 0.9635383486747742 + 50.0 * 8.954048156738281
Epoch 220, val loss: 0.9645541906356812
Epoch 230, training loss: 446.8092956542969 = 0.9560160636901855 + 50.0 * 8.917065620422363
Epoch 230, val loss: 0.957434356212616
Epoch 240, training loss: 445.06512451171875 = 0.9486574530601501 + 50.0 * 8.882328987121582
Epoch 240, val loss: 0.9503944516181946
Epoch 250, training loss: 444.17333984375 = 0.9401038289070129 + 50.0 * 8.864665031433105
Epoch 250, val loss: 0.9420375823974609
Epoch 260, training loss: 443.5238037109375 = 0.930162250995636 + 50.0 * 8.851872444152832
Epoch 260, val loss: 0.9324551224708557
Epoch 270, training loss: 442.6621398925781 = 0.9204662442207336 + 50.0 * 8.834833145141602
Epoch 270, val loss: 0.9233636260032654
Epoch 280, training loss: 441.64208984375 = 0.9114689230918884 + 50.0 * 8.81461238861084
Epoch 280, val loss: 0.9148784875869751
Epoch 290, training loss: 440.3933410644531 = 0.9028215408325195 + 50.0 * 8.789810180664062
Epoch 290, val loss: 0.9066901803016663
Epoch 300, training loss: 439.31158447265625 = 0.894030749797821 + 50.0 * 8.768350601196289
Epoch 300, val loss: 0.8982635736465454
Epoch 310, training loss: 438.77325439453125 = 0.8838225603103638 + 50.0 * 8.75778865814209
Epoch 310, val loss: 0.8883306980133057
Epoch 320, training loss: 438.2712097167969 = 0.8722792863845825 + 50.0 * 8.747978210449219
Epoch 320, val loss: 0.8773677349090576
Epoch 330, training loss: 437.75396728515625 = 0.8607931137084961 + 50.0 * 8.737863540649414
Epoch 330, val loss: 0.8665147423744202
Epoch 340, training loss: 437.15576171875 = 0.8495983481407166 + 50.0 * 8.726122856140137
Epoch 340, val loss: 0.8560780882835388
Epoch 350, training loss: 436.4540710449219 = 0.838899552822113 + 50.0 * 8.712303161621094
Epoch 350, val loss: 0.8460848331451416
Epoch 360, training loss: 435.769775390625 = 0.8283042907714844 + 50.0 * 8.698829650878906
Epoch 360, val loss: 0.8360690474510193
Epoch 370, training loss: 434.95556640625 = 0.8175972104072571 + 50.0 * 8.682759284973145
Epoch 370, val loss: 0.8260732889175415
Epoch 380, training loss: 434.2835693359375 = 0.8066442608833313 + 50.0 * 8.669538497924805
Epoch 380, val loss: 0.8158560395240784
Epoch 390, training loss: 433.7406311035156 = 0.7952842116355896 + 50.0 * 8.658906936645508
Epoch 390, val loss: 0.8052268624305725
Epoch 400, training loss: 433.3047180175781 = 0.7835090160369873 + 50.0 * 8.650424003601074
Epoch 400, val loss: 0.7941629886627197
Epoch 410, training loss: 432.9540710449219 = 0.7715710997581482 + 50.0 * 8.64365005493164
Epoch 410, val loss: 0.7829535603523254
Epoch 420, training loss: 432.64080810546875 = 0.7597672939300537 + 50.0 * 8.63762092590332
Epoch 420, val loss: 0.7718597054481506
Epoch 430, training loss: 432.3519287109375 = 0.7480807900428772 + 50.0 * 8.63207721710205
Epoch 430, val loss: 0.760883629322052
Epoch 440, training loss: 432.0675964355469 = 0.7365871071815491 + 50.0 * 8.626620292663574
Epoch 440, val loss: 0.7501285076141357
Epoch 450, training loss: 431.8556823730469 = 0.7253143191337585 + 50.0 * 8.622607231140137
Epoch 450, val loss: 0.7397620677947998
Epoch 460, training loss: 431.5003356933594 = 0.7142122983932495 + 50.0 * 8.61572265625
Epoch 460, val loss: 0.7293094992637634
Epoch 470, training loss: 431.1469421386719 = 0.7033796906471252 + 50.0 * 8.608871459960938
Epoch 470, val loss: 0.7191475033760071
Epoch 480, training loss: 430.78363037109375 = 0.6928081512451172 + 50.0 * 8.601816177368164
Epoch 480, val loss: 0.7092716097831726
Epoch 490, training loss: 430.41436767578125 = 0.6824634075164795 + 50.0 * 8.594637870788574
Epoch 490, val loss: 0.6996679902076721
Epoch 500, training loss: 430.1835632324219 = 0.672161340713501 + 50.0 * 8.590228080749512
Epoch 500, val loss: 0.6901440620422363
Epoch 510, training loss: 429.7117004394531 = 0.6619530916213989 + 50.0 * 8.580994606018066
Epoch 510, val loss: 0.680675745010376
Epoch 520, training loss: 429.4013366699219 = 0.6518723368644714 + 50.0 * 8.574989318847656
Epoch 520, val loss: 0.6713796257972717
Epoch 530, training loss: 429.0927734375 = 0.6419239640235901 + 50.0 * 8.569016456604004
Epoch 530, val loss: 0.6621406078338623
Epoch 540, training loss: 428.8161926269531 = 0.6321832537651062 + 50.0 * 8.563680648803711
Epoch 540, val loss: 0.6531001329421997
Epoch 550, training loss: 428.60052490234375 = 0.6226027011871338 + 50.0 * 8.559558868408203
Epoch 550, val loss: 0.6442746520042419
Epoch 560, training loss: 428.4226379394531 = 0.6132069826126099 + 50.0 * 8.556188583374023
Epoch 560, val loss: 0.6356640458106995
Epoch 570, training loss: 428.1623840332031 = 0.6042466163635254 + 50.0 * 8.551162719726562
Epoch 570, val loss: 0.6273819208145142
Epoch 580, training loss: 427.9744567871094 = 0.5955997705459595 + 50.0 * 8.547576904296875
Epoch 580, val loss: 0.6194575428962708
Epoch 590, training loss: 427.77606201171875 = 0.5872803926467896 + 50.0 * 8.54377555847168
Epoch 590, val loss: 0.6118801236152649
Epoch 600, training loss: 427.6884460449219 = 0.5793256759643555 + 50.0 * 8.542182922363281
Epoch 600, val loss: 0.6046683192253113
Epoch 610, training loss: 427.42730712890625 = 0.5716640949249268 + 50.0 * 8.537113189697266
Epoch 610, val loss: 0.5976880788803101
Epoch 620, training loss: 427.2316589355469 = 0.5644403100013733 + 50.0 * 8.533344268798828
Epoch 620, val loss: 0.5911599397659302
Epoch 630, training loss: 427.061767578125 = 0.5575918555259705 + 50.0 * 8.530083656311035
Epoch 630, val loss: 0.5848661661148071
Epoch 640, training loss: 426.85693359375 = 0.5509427189826965 + 50.0 * 8.52612018585205
Epoch 640, val loss: 0.5791676640510559
Epoch 650, training loss: 426.6523742675781 = 0.5446882247924805 + 50.0 * 8.522153854370117
Epoch 650, val loss: 0.5734809637069702
Epoch 660, training loss: 426.4783935546875 = 0.5386810898780823 + 50.0 * 8.518794059753418
Epoch 660, val loss: 0.5681596994400024
Epoch 670, training loss: 426.4639587402344 = 0.5329816341400146 + 50.0 * 8.518619537353516
Epoch 670, val loss: 0.5629734992980957
Epoch 680, training loss: 426.1796569824219 = 0.527435839176178 + 50.0 * 8.513044357299805
Epoch 680, val loss: 0.5583515763282776
Epoch 690, training loss: 426.0433044433594 = 0.5222785472869873 + 50.0 * 8.510420799255371
Epoch 690, val loss: 0.5537978410720825
Epoch 700, training loss: 425.90374755859375 = 0.5173771381378174 + 50.0 * 8.50772762298584
Epoch 700, val loss: 0.5494629144668579
Epoch 710, training loss: 425.7953186035156 = 0.5126858353614807 + 50.0 * 8.50565242767334
Epoch 710, val loss: 0.5455813407897949
Epoch 720, training loss: 425.6459655761719 = 0.5082777142524719 + 50.0 * 8.502754211425781
Epoch 720, val loss: 0.5417319536209106
Epoch 730, training loss: 425.51300048828125 = 0.5041135549545288 + 50.0 * 8.500177383422852
Epoch 730, val loss: 0.5381699800491333
Epoch 740, training loss: 425.3772277832031 = 0.5001354217529297 + 50.0 * 8.497542381286621
Epoch 740, val loss: 0.534755289554596
Epoch 750, training loss: 425.2520446777344 = 0.4963836073875427 + 50.0 * 8.495113372802734
Epoch 750, val loss: 0.5316500067710876
Epoch 760, training loss: 425.127197265625 = 0.49288108944892883 + 50.0 * 8.49268627166748
Epoch 760, val loss: 0.5287502408027649
Epoch 770, training loss: 424.9924011230469 = 0.48951563239097595 + 50.0 * 8.490057945251465
Epoch 770, val loss: 0.5260133147239685
Epoch 780, training loss: 424.9036560058594 = 0.48627644777297974 + 50.0 * 8.488348007202148
Epoch 780, val loss: 0.5234543681144714
Epoch 790, training loss: 424.7733459472656 = 0.4831528663635254 + 50.0 * 8.485803604125977
Epoch 790, val loss: 0.5206771492958069
Epoch 800, training loss: 424.676513671875 = 0.4801645278930664 + 50.0 * 8.483926773071289
Epoch 800, val loss: 0.5184935927391052
Epoch 810, training loss: 424.56243896484375 = 0.47734880447387695 + 50.0 * 8.481701850891113
Epoch 810, val loss: 0.5162789821624756
Epoch 820, training loss: 424.4541931152344 = 0.4746554493904114 + 50.0 * 8.47959041595459
Epoch 820, val loss: 0.5141057968139648
Epoch 830, training loss: 424.36090087890625 = 0.47207897901535034 + 50.0 * 8.477776527404785
Epoch 830, val loss: 0.5121403932571411
Epoch 840, training loss: 424.3148498535156 = 0.4696149528026581 + 50.0 * 8.47690486907959
Epoch 840, val loss: 0.5102527737617493
Epoch 850, training loss: 424.27825927734375 = 0.46718546748161316 + 50.0 * 8.476221084594727
Epoch 850, val loss: 0.5084632635116577
Epoch 860, training loss: 424.1363525390625 = 0.4649118185043335 + 50.0 * 8.473428726196289
Epoch 860, val loss: 0.5067845582962036
Epoch 870, training loss: 424.0438537597656 = 0.46274372935295105 + 50.0 * 8.471622467041016
Epoch 870, val loss: 0.5051121711730957
Epoch 880, training loss: 423.96038818359375 = 0.46065258979797363 + 50.0 * 8.46999454498291
Epoch 880, val loss: 0.503564178943634
Epoch 890, training loss: 423.9845886230469 = 0.4586343765258789 + 50.0 * 8.470519065856934
Epoch 890, val loss: 0.5021882653236389
Epoch 900, training loss: 423.9910888671875 = 0.4566226601600647 + 50.0 * 8.470688819885254
Epoch 900, val loss: 0.5004870891571045
Epoch 910, training loss: 423.7865905761719 = 0.4547017812728882 + 50.0 * 8.46663761138916
Epoch 910, val loss: 0.4992316663265228
Epoch 920, training loss: 423.7323303222656 = 0.4529009759426117 + 50.0 * 8.465588569641113
Epoch 920, val loss: 0.49792829155921936
Epoch 930, training loss: 423.6477355957031 = 0.45114976167678833 + 50.0 * 8.463932037353516
Epoch 930, val loss: 0.4966711699962616
Epoch 940, training loss: 423.5824890136719 = 0.44945910573005676 + 50.0 * 8.462660789489746
Epoch 940, val loss: 0.49552786350250244
Epoch 950, training loss: 423.5211486816406 = 0.4478165805339813 + 50.0 * 8.461466789245605
Epoch 950, val loss: 0.49438413977622986
Epoch 960, training loss: 423.4596862792969 = 0.4462145268917084 + 50.0 * 8.4602689743042
Epoch 960, val loss: 0.4932715594768524
Epoch 970, training loss: 423.39752197265625 = 0.4446602463722229 + 50.0 * 8.459056854248047
Epoch 970, val loss: 0.492210328578949
Epoch 980, training loss: 423.6679992675781 = 0.44318726658821106 + 50.0 * 8.464496612548828
Epoch 980, val loss: 0.49085524678230286
Epoch 990, training loss: 423.3553771972656 = 0.44160300493240356 + 50.0 * 8.45827579498291
Epoch 990, val loss: 0.4900045096874237
Epoch 1000, training loss: 423.2730712890625 = 0.44019198417663574 + 50.0 * 8.456657409667969
Epoch 1000, val loss: 0.48914647102355957
Epoch 1010, training loss: 423.1575012207031 = 0.43878352642059326 + 50.0 * 8.454374313354492
Epoch 1010, val loss: 0.48818647861480713
Epoch 1020, training loss: 423.0985107421875 = 0.437441885471344 + 50.0 * 8.453221321105957
Epoch 1020, val loss: 0.4873810112476349
Epoch 1030, training loss: 423.02593994140625 = 0.4361223876476288 + 50.0 * 8.451796531677246
Epoch 1030, val loss: 0.48649686574935913
Epoch 1040, training loss: 422.960693359375 = 0.4348337948322296 + 50.0 * 8.450516700744629
Epoch 1040, val loss: 0.4856359660625458
Epoch 1050, training loss: 422.9450378417969 = 0.43356892466545105 + 50.0 * 8.45022964477539
Epoch 1050, val loss: 0.484735369682312
Epoch 1060, training loss: 422.8458557128906 = 0.43226954340934753 + 50.0 * 8.448271751403809
Epoch 1060, val loss: 0.48408806324005127
Epoch 1070, training loss: 422.8102722167969 = 0.4310363829135895 + 50.0 * 8.447585105895996
Epoch 1070, val loss: 0.48309236764907837
Epoch 1080, training loss: 422.7160339355469 = 0.42982375621795654 + 50.0 * 8.445724487304688
Epoch 1080, val loss: 0.4823545813560486
Epoch 1090, training loss: 422.6460266113281 = 0.42864617705345154 + 50.0 * 8.444347381591797
Epoch 1090, val loss: 0.4816645383834839
Epoch 1100, training loss: 422.62530517578125 = 0.4274997115135193 + 50.0 * 8.44395637512207
Epoch 1100, val loss: 0.4808831810951233
Epoch 1110, training loss: 422.6239318847656 = 0.42631179094314575 + 50.0 * 8.443952560424805
Epoch 1110, val loss: 0.4802480638027191
Epoch 1120, training loss: 422.4734191894531 = 0.4251858592033386 + 50.0 * 8.440964698791504
Epoch 1120, val loss: 0.4795081913471222
Epoch 1130, training loss: 422.40118408203125 = 0.4240925908088684 + 50.0 * 8.439541816711426
Epoch 1130, val loss: 0.47879382967948914
Epoch 1140, training loss: 422.3382873535156 = 0.4230230450630188 + 50.0 * 8.438304901123047
Epoch 1140, val loss: 0.478131502866745
Epoch 1150, training loss: 422.3132629394531 = 0.421968549489975 + 50.0 * 8.437826156616211
Epoch 1150, val loss: 0.47753387689590454
Epoch 1160, training loss: 422.2866516113281 = 0.4208926558494568 + 50.0 * 8.437314987182617
Epoch 1160, val loss: 0.47682690620422363
Epoch 1170, training loss: 422.21612548828125 = 0.4198571443557739 + 50.0 * 8.435925483703613
Epoch 1170, val loss: 0.47619298100471497
Epoch 1180, training loss: 422.1280517578125 = 0.4188559353351593 + 50.0 * 8.434184074401855
Epoch 1180, val loss: 0.4754846692085266
Epoch 1190, training loss: 422.0694274902344 = 0.4178687036037445 + 50.0 * 8.43303108215332
Epoch 1190, val loss: 0.4749123454093933
Epoch 1200, training loss: 422.0647277832031 = 0.4168877899646759 + 50.0 * 8.43295669555664
Epoch 1200, val loss: 0.4744262397289276
Epoch 1210, training loss: 421.97589111328125 = 0.41590821743011475 + 50.0 * 8.43120002746582
Epoch 1210, val loss: 0.47369733452796936
Epoch 1220, training loss: 421.9239196777344 = 0.41494783759117126 + 50.0 * 8.430179595947266
Epoch 1220, val loss: 0.47310495376586914
Epoch 1230, training loss: 421.87359619140625 = 0.41400936245918274 + 50.0 * 8.429191589355469
Epoch 1230, val loss: 0.47252511978149414
Epoch 1240, training loss: 421.8290710449219 = 0.41309019923210144 + 50.0 * 8.428319931030273
Epoch 1240, val loss: 0.4719725549221039
Epoch 1250, training loss: 421.852783203125 = 0.4121708273887634 + 50.0 * 8.428812026977539
Epoch 1250, val loss: 0.47148725390434265
Epoch 1260, training loss: 421.810302734375 = 0.41123610734939575 + 50.0 * 8.42798137664795
Epoch 1260, val loss: 0.4709645211696625
Epoch 1270, training loss: 421.7243347167969 = 0.41033706068992615 + 50.0 * 8.42628002166748
Epoch 1270, val loss: 0.47032630443573
Epoch 1280, training loss: 421.6688232421875 = 0.4094496965408325 + 50.0 * 8.425187110900879
Epoch 1280, val loss: 0.4697486460208893
Epoch 1290, training loss: 421.64337158203125 = 0.40858256816864014 + 50.0 * 8.42469596862793
Epoch 1290, val loss: 0.4692029654979706
Epoch 1300, training loss: 421.7166748046875 = 0.4077190160751343 + 50.0 * 8.426178932189941
Epoch 1300, val loss: 0.46855786442756653
Epoch 1310, training loss: 421.59619140625 = 0.40681973099708557 + 50.0 * 8.423787117004395
Epoch 1310, val loss: 0.4683035910129547
Epoch 1320, training loss: 421.5217590332031 = 0.40598684549331665 + 50.0 * 8.42231559753418
Epoch 1320, val loss: 0.4677128791809082
Epoch 1330, training loss: 421.4896545410156 = 0.40514931082725525 + 50.0 * 8.421689987182617
Epoch 1330, val loss: 0.46728795766830444
Epoch 1340, training loss: 421.54180908203125 = 0.40431949496269226 + 50.0 * 8.422749519348145
Epoch 1340, val loss: 0.4669145345687866
Epoch 1350, training loss: 421.4599304199219 = 0.40347859263420105 + 50.0 * 8.42112922668457
Epoch 1350, val loss: 0.46623945236206055
Epoch 1360, training loss: 421.43255615234375 = 0.40266212821006775 + 50.0 * 8.420598030090332
Epoch 1360, val loss: 0.46581053733825684
Epoch 1370, training loss: 421.35894775390625 = 0.40184885263442993 + 50.0 * 8.41914176940918
Epoch 1370, val loss: 0.465343713760376
Epoch 1380, training loss: 421.31890869140625 = 0.40106141567230225 + 50.0 * 8.418356895446777
Epoch 1380, val loss: 0.4648817479610443
Epoch 1390, training loss: 421.3122863769531 = 0.40027934312820435 + 50.0 * 8.418240547180176
Epoch 1390, val loss: 0.46438026428222656
Epoch 1400, training loss: 421.3266296386719 = 0.3994888961315155 + 50.0 * 8.418542861938477
Epoch 1400, val loss: 0.4639391005039215
Epoch 1410, training loss: 421.24859619140625 = 0.39870762825012207 + 50.0 * 8.416997909545898
Epoch 1410, val loss: 0.4635465145111084
Epoch 1420, training loss: 421.2714538574219 = 0.39794668555259705 + 50.0 * 8.41746997833252
Epoch 1420, val loss: 0.4629138708114624
Epoch 1430, training loss: 421.1877746582031 = 0.39714667201042175 + 50.0 * 8.415812492370605
Epoch 1430, val loss: 0.462703138589859
Epoch 1440, training loss: 421.1561584472656 = 0.39639344811439514 + 50.0 * 8.41519546508789
Epoch 1440, val loss: 0.4621719419956207
Epoch 1450, training loss: 421.1182556152344 = 0.39564189314842224 + 50.0 * 8.41445255279541
Epoch 1450, val loss: 0.4618256986141205
Epoch 1460, training loss: 421.0843811035156 = 0.39490947127342224 + 50.0 * 8.413789749145508
Epoch 1460, val loss: 0.46143120527267456
Epoch 1470, training loss: 421.07135009765625 = 0.39417746663093567 + 50.0 * 8.413543701171875
Epoch 1470, val loss: 0.4610331058502197
Epoch 1480, training loss: 421.2599792480469 = 0.3934306800365448 + 50.0 * 8.417330741882324
Epoch 1480, val loss: 0.4606899321079254
Epoch 1490, training loss: 421.0343322753906 = 0.3926834464073181 + 50.0 * 8.412833213806152
Epoch 1490, val loss: 0.4601898193359375
Epoch 1500, training loss: 421.0119323730469 = 0.39195823669433594 + 50.0 * 8.412399291992188
Epoch 1500, val loss: 0.45971742272377014
Epoch 1510, training loss: 420.9598388671875 = 0.39124149084091187 + 50.0 * 8.411372184753418
Epoch 1510, val loss: 0.4594663679599762
Epoch 1520, training loss: 420.9386291503906 = 0.390535831451416 + 50.0 * 8.410962104797363
Epoch 1520, val loss: 0.45903679728507996
Epoch 1530, training loss: 420.95166015625 = 0.3898260295391083 + 50.0 * 8.411236763000488
Epoch 1530, val loss: 0.4587472677230835
Epoch 1540, training loss: 420.9555969238281 = 0.38909563422203064 + 50.0 * 8.411330223083496
Epoch 1540, val loss: 0.458375483751297
Epoch 1550, training loss: 420.9360656738281 = 0.38838401436805725 + 50.0 * 8.410953521728516
Epoch 1550, val loss: 0.45785561203956604
Epoch 1560, training loss: 420.85894775390625 = 0.38766905665397644 + 50.0 * 8.409425735473633
Epoch 1560, val loss: 0.4575138986110687
Epoch 1570, training loss: 420.84075927734375 = 0.3869698941707611 + 50.0 * 8.409075736999512
Epoch 1570, val loss: 0.45718732476234436
Epoch 1580, training loss: 420.8369140625 = 0.38626646995544434 + 50.0 * 8.409012794494629
Epoch 1580, val loss: 0.456790566444397
Epoch 1590, training loss: 420.79730224609375 = 0.38556820154190063 + 50.0 * 8.408234596252441
Epoch 1590, val loss: 0.4564956724643707
Epoch 1600, training loss: 420.77349853515625 = 0.38487765192985535 + 50.0 * 8.407772064208984
Epoch 1600, val loss: 0.45605483651161194
Epoch 1610, training loss: 420.8040771484375 = 0.38418805599212646 + 50.0 * 8.408397674560547
Epoch 1610, val loss: 0.45573899149894714
Epoch 1620, training loss: 420.7250061035156 = 0.383502334356308 + 50.0 * 8.406829833984375
Epoch 1620, val loss: 0.45535919070243835
Epoch 1630, training loss: 420.81365966796875 = 0.38281771540641785 + 50.0 * 8.40861701965332
Epoch 1630, val loss: 0.45516523718833923
Epoch 1640, training loss: 420.69708251953125 = 0.38211727142333984 + 50.0 * 8.406299591064453
Epoch 1640, val loss: 0.45454275608062744
Epoch 1650, training loss: 420.6680908203125 = 0.38143736124038696 + 50.0 * 8.405733108520508
Epoch 1650, val loss: 0.4542368948459625
Epoch 1660, training loss: 420.64453125 = 0.3807792365550995 + 50.0 * 8.405275344848633
Epoch 1660, val loss: 0.453823447227478
Epoch 1670, training loss: 420.63958740234375 = 0.3801213800907135 + 50.0 * 8.405189514160156
Epoch 1670, val loss: 0.4535038471221924
Epoch 1680, training loss: 420.8363037109375 = 0.3794540464878082 + 50.0 * 8.409136772155762
Epoch 1680, val loss: 0.45307350158691406
Epoch 1690, training loss: 420.63031005859375 = 0.37877488136291504 + 50.0 * 8.405030250549316
Epoch 1690, val loss: 0.4529132544994354
Epoch 1700, training loss: 420.5736083984375 = 0.3781222999095917 + 50.0 * 8.403909683227539
Epoch 1700, val loss: 0.45233315229415894
Epoch 1710, training loss: 420.5513000488281 = 0.3774730861186981 + 50.0 * 8.40347671508789
Epoch 1710, val loss: 0.4521825611591339
Epoch 1720, training loss: 420.5314025878906 = 0.37682706117630005 + 50.0 * 8.403091430664062
Epoch 1720, val loss: 0.45178863406181335
Epoch 1730, training loss: 420.5118408203125 = 0.37617653608322144 + 50.0 * 8.40271282196045
Epoch 1730, val loss: 0.4515222907066345
Epoch 1740, training loss: 420.5277099609375 = 0.3755248486995697 + 50.0 * 8.403043746948242
Epoch 1740, val loss: 0.45123374462127686
Epoch 1750, training loss: 420.5898742675781 = 0.37485745549201965 + 50.0 * 8.404300689697266
Epoch 1750, val loss: 0.4509502053260803
Epoch 1760, training loss: 420.4734191894531 = 0.37419405579566956 + 50.0 * 8.401984214782715
Epoch 1760, val loss: 0.4505089819431305
Epoch 1770, training loss: 420.4629211425781 = 0.37354448437690735 + 50.0 * 8.401787757873535
Epoch 1770, val loss: 0.45015817880630493
Epoch 1780, training loss: 420.44879150390625 = 0.37289825081825256 + 50.0 * 8.401517868041992
Epoch 1780, val loss: 0.44992226362228394
Epoch 1790, training loss: 420.41485595703125 = 0.37226319313049316 + 50.0 * 8.40085220336914
Epoch 1790, val loss: 0.4495602250099182
Epoch 1800, training loss: 420.39166259765625 = 0.37163060903549194 + 50.0 * 8.400400161743164
Epoch 1800, val loss: 0.44923990964889526
Epoch 1810, training loss: 420.4227600097656 = 0.37100017070770264 + 50.0 * 8.40103530883789
Epoch 1810, val loss: 0.4489624798297882
Epoch 1820, training loss: 420.38226318359375 = 0.37034812569618225 + 50.0 * 8.400238037109375
Epoch 1820, val loss: 0.44862404465675354
Epoch 1830, training loss: 420.38995361328125 = 0.36971190571784973 + 50.0 * 8.400404930114746
Epoch 1830, val loss: 0.44815564155578613
Epoch 1840, training loss: 420.40435791015625 = 0.36908096075057983 + 50.0 * 8.400705337524414
Epoch 1840, val loss: 0.4478569030761719
Epoch 1850, training loss: 420.3166198730469 = 0.36843639612197876 + 50.0 * 8.398963928222656
Epoch 1850, val loss: 0.4477088749408722
Epoch 1860, training loss: 420.28216552734375 = 0.3678307831287384 + 50.0 * 8.398286819458008
Epoch 1860, val loss: 0.44742050766944885
Epoch 1870, training loss: 420.24774169921875 = 0.367221474647522 + 50.0 * 8.397610664367676
Epoch 1870, val loss: 0.4471606910228729
Epoch 1880, training loss: 420.2519226074219 = 0.3666115701198578 + 50.0 * 8.397706031799316
Epoch 1880, val loss: 0.446917325258255
Epoch 1890, training loss: 420.4688720703125 = 0.3659822344779968 + 50.0 * 8.402057647705078
Epoch 1890, val loss: 0.4467165172100067
Epoch 1900, training loss: 420.2049255371094 = 0.3653532564640045 + 50.0 * 8.396791458129883
Epoch 1900, val loss: 0.4461963474750519
Epoch 1910, training loss: 420.18963623046875 = 0.36474937200546265 + 50.0 * 8.39649772644043
Epoch 1910, val loss: 0.4458257853984833
Epoch 1920, training loss: 420.1601257324219 = 0.36414819955825806 + 50.0 * 8.395919799804688
Epoch 1920, val loss: 0.44569000601768494
Epoch 1930, training loss: 420.1312255859375 = 0.3635578751564026 + 50.0 * 8.395353317260742
Epoch 1930, val loss: 0.44539329409599304
Epoch 1940, training loss: 420.14410400390625 = 0.3629589080810547 + 50.0 * 8.395623207092285
Epoch 1940, val loss: 0.44522562623023987
Epoch 1950, training loss: 420.2057800292969 = 0.36233267188072205 + 50.0 * 8.396868705749512
Epoch 1950, val loss: 0.4449957609176636
Epoch 1960, training loss: 420.0994567871094 = 0.36172017455101013 + 50.0 * 8.394754409790039
Epoch 1960, val loss: 0.44455796480178833
Epoch 1970, training loss: 420.0814208984375 = 0.3611190915107727 + 50.0 * 8.39440631866455
Epoch 1970, val loss: 0.44429582357406616
Epoch 1980, training loss: 420.1242370605469 = 0.3605278432369232 + 50.0 * 8.39527416229248
Epoch 1980, val loss: 0.44406354427337646
Epoch 1990, training loss: 420.0291748046875 = 0.3599214255809784 + 50.0 * 8.39338493347168
Epoch 1990, val loss: 0.4438772201538086
Epoch 2000, training loss: 419.99822998046875 = 0.35932570695877075 + 50.0 * 8.392778396606445
Epoch 2000, val loss: 0.4435510039329529
Epoch 2010, training loss: 419.99127197265625 = 0.3587357997894287 + 50.0 * 8.392650604248047
Epoch 2010, val loss: 0.4433073103427887
Epoch 2020, training loss: 420.0388488769531 = 0.35813993215560913 + 50.0 * 8.393613815307617
Epoch 2020, val loss: 0.4430229365825653
Epoch 2030, training loss: 419.9593200683594 = 0.3575303256511688 + 50.0 * 8.392035484313965
Epoch 2030, val loss: 0.4427677094936371
Epoch 2040, training loss: 420.0343933105469 = 0.35693082213401794 + 50.0 * 8.393548965454102
Epoch 2040, val loss: 0.4424407482147217
Epoch 2050, training loss: 419.9346923828125 = 0.3563184142112732 + 50.0 * 8.39156723022461
Epoch 2050, val loss: 0.4423411190509796
Epoch 2060, training loss: 419.89813232421875 = 0.35572463274002075 + 50.0 * 8.390848159790039
Epoch 2060, val loss: 0.4420558214187622
Epoch 2070, training loss: 419.8706359863281 = 0.3551309108734131 + 50.0 * 8.390310287475586
Epoch 2070, val loss: 0.4418969452381134
Epoch 2080, training loss: 419.89898681640625 = 0.35453394055366516 + 50.0 * 8.390889167785645
Epoch 2080, val loss: 0.4417797923088074
Epoch 2090, training loss: 419.9431457519531 = 0.3539210855960846 + 50.0 * 8.39178466796875
Epoch 2090, val loss: 0.4414377212524414
Epoch 2100, training loss: 419.8759765625 = 0.3533068597316742 + 50.0 * 8.390453338623047
Epoch 2100, val loss: 0.4410480856895447
Epoch 2110, training loss: 419.801025390625 = 0.35269808769226074 + 50.0 * 8.38896656036377
Epoch 2110, val loss: 0.44083425402641296
Epoch 2120, training loss: 419.7756042480469 = 0.3521069884300232 + 50.0 * 8.388469696044922
Epoch 2120, val loss: 0.44056063890457153
Epoch 2130, training loss: 419.7716064453125 = 0.35151535272598267 + 50.0 * 8.388401985168457
Epoch 2130, val loss: 0.4402734339237213
Epoch 2140, training loss: 419.8531799316406 = 0.3509173095226288 + 50.0 * 8.390045166015625
Epoch 2140, val loss: 0.4400034546852112
Epoch 2150, training loss: 419.7353820800781 = 0.35029137134552 + 50.0 * 8.387701988220215
Epoch 2150, val loss: 0.43994760513305664
Epoch 2160, training loss: 419.7384338378906 = 0.3496876060962677 + 50.0 * 8.387774467468262
Epoch 2160, val loss: 0.4397198259830475
Epoch 2170, training loss: 419.8148498535156 = 0.34906744956970215 + 50.0 * 8.389315605163574
Epoch 2170, val loss: 0.4394896328449249
Epoch 2180, training loss: 419.6986389160156 = 0.348451167345047 + 50.0 * 8.387003898620605
Epoch 2180, val loss: 0.43899163603782654
Epoch 2190, training loss: 419.660888671875 = 0.3478369116783142 + 50.0 * 8.386260986328125
Epoch 2190, val loss: 0.4389306902885437
Epoch 2200, training loss: 419.6368713378906 = 0.3472338616847992 + 50.0 * 8.38579273223877
Epoch 2200, val loss: 0.4385375678539276
Epoch 2210, training loss: 419.6278991699219 = 0.3466261327266693 + 50.0 * 8.385625839233398
Epoch 2210, val loss: 0.43831831216812134
Epoch 2220, training loss: 419.79840087890625 = 0.34601879119873047 + 50.0 * 8.389047622680664
Epoch 2220, val loss: 0.4378938376903534
Epoch 2230, training loss: 419.6632385253906 = 0.3453630208969116 + 50.0 * 8.386357307434082
Epoch 2230, val loss: 0.4379270374774933
Epoch 2240, training loss: 419.5957336425781 = 0.3447352945804596 + 50.0 * 8.38502025604248
Epoch 2240, val loss: 0.4376046359539032
Epoch 2250, training loss: 419.558837890625 = 0.34410756826400757 + 50.0 * 8.384294509887695
Epoch 2250, val loss: 0.43733644485473633
Epoch 2260, training loss: 419.56878662109375 = 0.3434816598892212 + 50.0 * 8.384506225585938
Epoch 2260, val loss: 0.43711426854133606
Epoch 2270, training loss: 419.6490173339844 = 0.3428399860858917 + 50.0 * 8.386123657226562
Epoch 2270, val loss: 0.43687042593955994
Epoch 2280, training loss: 419.58819580078125 = 0.34219327569007874 + 50.0 * 8.384920120239258
Epoch 2280, val loss: 0.43668362498283386
Epoch 2290, training loss: 419.595458984375 = 0.3415393531322479 + 50.0 * 8.385078430175781
Epoch 2290, val loss: 0.4364972710609436
Epoch 2300, training loss: 419.5065002441406 = 0.3408944010734558 + 50.0 * 8.383312225341797
Epoch 2300, val loss: 0.436063677072525
Epoch 2310, training loss: 419.4675598144531 = 0.3402523100376129 + 50.0 * 8.382546424865723
Epoch 2310, val loss: 0.43587973713874817
Epoch 2320, training loss: 419.4593811035156 = 0.33961012959480286 + 50.0 * 8.38239574432373
Epoch 2320, val loss: 0.4356853663921356
Epoch 2330, training loss: 419.46490478515625 = 0.33896735310554504 + 50.0 * 8.382518768310547
Epoch 2330, val loss: 0.43537330627441406
Epoch 2340, training loss: 419.7114562988281 = 0.33832159638404846 + 50.0 * 8.387462615966797
Epoch 2340, val loss: 0.4348843991756439
Epoch 2350, training loss: 419.42645263671875 = 0.33761757612228394 + 50.0 * 8.381776809692383
Epoch 2350, val loss: 0.4349655508995056
Epoch 2360, training loss: 419.420166015625 = 0.3369589149951935 + 50.0 * 8.381664276123047
Epoch 2360, val loss: 0.4347690939903259
Epoch 2370, training loss: 419.39886474609375 = 0.3363122344017029 + 50.0 * 8.381251335144043
Epoch 2370, val loss: 0.43435055017471313
Epoch 2380, training loss: 419.37530517578125 = 0.33565789461135864 + 50.0 * 8.380792617797852
Epoch 2380, val loss: 0.4342632591724396
Epoch 2390, training loss: 419.3655700683594 = 0.33500152826309204 + 50.0 * 8.380611419677734
Epoch 2390, val loss: 0.43402981758117676
Epoch 2400, training loss: 419.37823486328125 = 0.33433881402015686 + 50.0 * 8.380877494812012
Epoch 2400, val loss: 0.4338330626487732
Epoch 2410, training loss: 419.5126953125 = 0.33366379141807556 + 50.0 * 8.383581161499023
Epoch 2410, val loss: 0.4336851239204407
Epoch 2420, training loss: 419.3680419921875 = 0.33297768235206604 + 50.0 * 8.380701065063477
Epoch 2420, val loss: 0.43306562304496765
Epoch 2430, training loss: 419.3197326660156 = 0.3322896361351013 + 50.0 * 8.379749298095703
Epoch 2430, val loss: 0.4330384433269501
Epoch 2440, training loss: 419.32275390625 = 0.33161503076553345 + 50.0 * 8.379822731018066
Epoch 2440, val loss: 0.4326496124267578
Epoch 2450, training loss: 419.3876953125 = 0.33092978596687317 + 50.0 * 8.381134986877441
Epoch 2450, val loss: 0.43238428235054016
Epoch 2460, training loss: 419.30291748046875 = 0.3302365839481354 + 50.0 * 8.379453659057617
Epoch 2460, val loss: 0.43220099806785583
Epoch 2470, training loss: 419.2828063964844 = 0.32954105734825134 + 50.0 * 8.37906551361084
Epoch 2470, val loss: 0.43190205097198486
Epoch 2480, training loss: 419.27459716796875 = 0.32884693145751953 + 50.0 * 8.378914833068848
Epoch 2480, val loss: 0.43168652057647705
Epoch 2490, training loss: 419.365966796875 = 0.3281431794166565 + 50.0 * 8.380756378173828
Epoch 2490, val loss: 0.4314873218536377
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8244545915778791
0.8632181409838442
=== training gcn model ===
Epoch 0, training loss: 530.2205810546875 = 1.1053862571716309 + 50.0 * 10.582304000854492
Epoch 0, val loss: 1.1054883003234863
Epoch 10, training loss: 530.2017211914062 = 1.1009793281555176 + 50.0 * 10.582015037536621
Epoch 10, val loss: 1.1010503768920898
Epoch 20, training loss: 530.13525390625 = 1.096113920211792 + 50.0 * 10.580781936645508
Epoch 20, val loss: 1.096137285232544
Epoch 30, training loss: 529.8521118164062 = 1.0908260345458984 + 50.0 * 10.575225830078125
Epoch 30, val loss: 1.090793251991272
Epoch 40, training loss: 528.6943969726562 = 1.084983229637146 + 50.0 * 10.552188873291016
Epoch 40, val loss: 1.084869146347046
Epoch 50, training loss: 524.9901123046875 = 1.0782620906829834 + 50.0 * 10.47823715209961
Epoch 50, val loss: 1.0780233144760132
Epoch 60, training loss: 515.9908447265625 = 1.0711201429367065 + 50.0 * 10.298394203186035
Epoch 60, val loss: 1.0708391666412354
Epoch 70, training loss: 499.96807861328125 = 1.0637435913085938 + 50.0 * 9.978086471557617
Epoch 70, val loss: 1.0633230209350586
Epoch 80, training loss: 484.5330505371094 = 1.05532705783844 + 50.0 * 9.669554710388184
Epoch 80, val loss: 1.0548948049545288
Epoch 90, training loss: 476.5177307128906 = 1.0476950407028198 + 50.0 * 9.509400367736816
Epoch 90, val loss: 1.0473146438598633
Epoch 100, training loss: 468.6668701171875 = 1.0415351390838623 + 50.0 * 9.352506637573242
Epoch 100, val loss: 1.0413086414337158
Epoch 110, training loss: 463.4187316894531 = 1.037178635597229 + 50.0 * 9.247631072998047
Epoch 110, val loss: 1.037066102027893
Epoch 120, training loss: 461.4779052734375 = 1.0332084894180298 + 50.0 * 9.208893775939941
Epoch 120, val loss: 1.03310227394104
Epoch 130, training loss: 459.2784118652344 = 1.0283557176589966 + 50.0 * 9.165000915527344
Epoch 130, val loss: 1.028219223022461
Epoch 140, training loss: 456.1112976074219 = 1.0235357284545898 + 50.0 * 9.101755142211914
Epoch 140, val loss: 1.023503303527832
Epoch 150, training loss: 452.182373046875 = 1.0197449922561646 + 50.0 * 9.023252487182617
Epoch 150, val loss: 1.0198410749435425
Epoch 160, training loss: 449.3131408691406 = 1.0161231756210327 + 50.0 * 8.965940475463867
Epoch 160, val loss: 1.0162360668182373
Epoch 170, training loss: 446.4653625488281 = 1.0115597248077393 + 50.0 * 8.909075736999512
Epoch 170, val loss: 1.011751413345337
Epoch 180, training loss: 443.5421447753906 = 1.0069977045059204 + 50.0 * 8.850703239440918
Epoch 180, val loss: 1.0073397159576416
Epoch 190, training loss: 441.7124328613281 = 1.0023138523101807 + 50.0 * 8.814202308654785
Epoch 190, val loss: 1.0026686191558838
Epoch 200, training loss: 440.25372314453125 = 0.9965221285820007 + 50.0 * 8.785143852233887
Epoch 200, val loss: 0.9967640042304993
Epoch 210, training loss: 439.23553466796875 = 0.9898740649223328 + 50.0 * 8.764913558959961
Epoch 210, val loss: 0.9900712966918945
Epoch 220, training loss: 438.2625427246094 = 0.9827548861503601 + 50.0 * 8.745595932006836
Epoch 220, val loss: 0.9830360412597656
Epoch 230, training loss: 437.2597351074219 = 0.9754443764686584 + 50.0 * 8.725686073303223
Epoch 230, val loss: 0.9758707284927368
Epoch 240, training loss: 436.3634338378906 = 0.967891275882721 + 50.0 * 8.707910537719727
Epoch 240, val loss: 0.9684653282165527
Epoch 250, training loss: 435.6236572265625 = 0.9599014520645142 + 50.0 * 8.693275451660156
Epoch 250, val loss: 0.9605902433395386
Epoch 260, training loss: 434.9765319824219 = 0.9513124823570251 + 50.0 * 8.680503845214844
Epoch 260, val loss: 0.9521076083183289
Epoch 270, training loss: 434.3410949707031 = 0.9421543478965759 + 50.0 * 8.667978286743164
Epoch 270, val loss: 0.9430801868438721
Epoch 280, training loss: 433.689697265625 = 0.9326033592224121 + 50.0 * 8.655141830444336
Epoch 280, val loss: 0.9337431192398071
Epoch 290, training loss: 433.25909423828125 = 0.9227370619773865 + 50.0 * 8.646727561950684
Epoch 290, val loss: 0.9240071177482605
Epoch 300, training loss: 432.6385192871094 = 0.9122745990753174 + 50.0 * 8.634525299072266
Epoch 300, val loss: 0.9138428568840027
Epoch 310, training loss: 432.1574401855469 = 0.9014329314231873 + 50.0 * 8.625120162963867
Epoch 310, val loss: 0.9031646847724915
Epoch 320, training loss: 431.7092590332031 = 0.8901761770248413 + 50.0 * 8.616381645202637
Epoch 320, val loss: 0.8922054767608643
Epoch 330, training loss: 431.33966064453125 = 0.8786165118217468 + 50.0 * 8.609220504760742
Epoch 330, val loss: 0.8809492588043213
Epoch 340, training loss: 430.8371276855469 = 0.8668658137321472 + 50.0 * 8.599405288696289
Epoch 340, val loss: 0.869516134262085
Epoch 350, training loss: 430.4039611816406 = 0.8549020886421204 + 50.0 * 8.590981483459473
Epoch 350, val loss: 0.8579200506210327
Epoch 360, training loss: 430.0542907714844 = 0.8427590727806091 + 50.0 * 8.584230422973633
Epoch 360, val loss: 0.8461140990257263
Epoch 370, training loss: 429.6673583984375 = 0.8303747773170471 + 50.0 * 8.576739311218262
Epoch 370, val loss: 0.834094226360321
Epoch 380, training loss: 429.3634033203125 = 0.8178418874740601 + 50.0 * 8.570911407470703
Epoch 380, val loss: 0.8219425082206726
Epoch 390, training loss: 429.056396484375 = 0.8051866888999939 + 50.0 * 8.565024375915527
Epoch 390, val loss: 0.8097220659255981
Epoch 400, training loss: 428.80010986328125 = 0.7924288511276245 + 50.0 * 8.56015396118164
Epoch 400, val loss: 0.7974083423614502
Epoch 410, training loss: 428.55352783203125 = 0.7796298265457153 + 50.0 * 8.5554780960083
Epoch 410, val loss: 0.7850582003593445
Epoch 420, training loss: 428.3006286621094 = 0.7668585777282715 + 50.0 * 8.550675392150879
Epoch 420, val loss: 0.7727814316749573
Epoch 430, training loss: 428.10467529296875 = 0.7541424036026001 + 50.0 * 8.54701042175293
Epoch 430, val loss: 0.7605783343315125
Epoch 440, training loss: 427.8688659667969 = 0.7414506077766418 + 50.0 * 8.542548179626465
Epoch 440, val loss: 0.7484115958213806
Epoch 450, training loss: 427.6224060058594 = 0.7288886308670044 + 50.0 * 8.537870407104492
Epoch 450, val loss: 0.7363530993461609
Epoch 460, training loss: 427.40576171875 = 0.7164583206176758 + 50.0 * 8.533785820007324
Epoch 460, val loss: 0.7244850397109985
Epoch 470, training loss: 427.269775390625 = 0.7041594982147217 + 50.0 * 8.531311988830566
Epoch 470, val loss: 0.7127541899681091
Epoch 480, training loss: 427.0794372558594 = 0.6919726729393005 + 50.0 * 8.527749061584473
Epoch 480, val loss: 0.7011407613754272
Epoch 490, training loss: 426.87158203125 = 0.6800166368484497 + 50.0 * 8.523831367492676
Epoch 490, val loss: 0.689827561378479
Epoch 500, training loss: 426.68585205078125 = 0.6683524250984192 + 50.0 * 8.520349502563477
Epoch 500, val loss: 0.6787407398223877
Epoch 510, training loss: 426.5508117675781 = 0.6569710969924927 + 50.0 * 8.517876625061035
Epoch 510, val loss: 0.6679190993309021
Epoch 520, training loss: 426.3972473144531 = 0.645826518535614 + 50.0 * 8.515028953552246
Epoch 520, val loss: 0.657474935054779
Epoch 530, training loss: 426.178466796875 = 0.6350764036178589 + 50.0 * 8.510868072509766
Epoch 530, val loss: 0.6473034024238586
Epoch 540, training loss: 426.0166931152344 = 0.6246846914291382 + 50.0 * 8.507840156555176
Epoch 540, val loss: 0.6374872922897339
Epoch 550, training loss: 425.8437805175781 = 0.6146544814109802 + 50.0 * 8.504582405090332
Epoch 550, val loss: 0.6280579566955566
Epoch 560, training loss: 425.7007751464844 = 0.6049788594245911 + 50.0 * 8.50191593170166
Epoch 560, val loss: 0.6190184354782104
Epoch 570, training loss: 425.52862548828125 = 0.5956829190254211 + 50.0 * 8.498659133911133
Epoch 570, val loss: 0.6102970242500305
Epoch 580, training loss: 425.3722229003906 = 0.5867775678634644 + 50.0 * 8.495708465576172
Epoch 580, val loss: 0.6019776463508606
Epoch 590, training loss: 425.251708984375 = 0.5782551169395447 + 50.0 * 8.49346923828125
Epoch 590, val loss: 0.5940301418304443
Epoch 600, training loss: 425.1970520019531 = 0.5700700879096985 + 50.0 * 8.492539405822754
Epoch 600, val loss: 0.586328387260437
Epoch 610, training loss: 424.991943359375 = 0.5622408390045166 + 50.0 * 8.488594055175781
Epoch 610, val loss: 0.5791094899177551
Epoch 620, training loss: 424.84136962890625 = 0.5548090934753418 + 50.0 * 8.48573112487793
Epoch 620, val loss: 0.5722386837005615
Epoch 630, training loss: 424.7218933105469 = 0.5477197766304016 + 50.0 * 8.48348331451416
Epoch 630, val loss: 0.5656988620758057
Epoch 640, training loss: 424.8273620605469 = 0.5409712195396423 + 50.0 * 8.485727310180664
Epoch 640, val loss: 0.5595466494560242
Epoch 650, training loss: 424.54705810546875 = 0.5344857573509216 + 50.0 * 8.48025131225586
Epoch 650, val loss: 0.553581953048706
Epoch 660, training loss: 424.44049072265625 = 0.5283778309822083 + 50.0 * 8.478241920471191
Epoch 660, val loss: 0.5479941964149475
Epoch 670, training loss: 424.3187255859375 = 0.5225847363471985 + 50.0 * 8.475922584533691
Epoch 670, val loss: 0.5427108407020569
Epoch 680, training loss: 424.2306213378906 = 0.5170921087265015 + 50.0 * 8.474270820617676
Epoch 680, val loss: 0.5377270579338074
Epoch 690, training loss: 424.1573791503906 = 0.5118457078933716 + 50.0 * 8.47291088104248
Epoch 690, val loss: 0.5330191254615784
Epoch 700, training loss: 424.0815124511719 = 0.5068707466125488 + 50.0 * 8.471492767333984
Epoch 700, val loss: 0.5285651683807373
Epoch 710, training loss: 423.95855712890625 = 0.5021566152572632 + 50.0 * 8.469127655029297
Epoch 710, val loss: 0.5244008302688599
Epoch 720, training loss: 423.8627624511719 = 0.4976692795753479 + 50.0 * 8.467301368713379
Epoch 720, val loss: 0.5204564929008484
Epoch 730, training loss: 423.7855224609375 = 0.4934096932411194 + 50.0 * 8.465842247009277
Epoch 730, val loss: 0.5167294144630432
Epoch 740, training loss: 423.7259216308594 = 0.48936402797698975 + 50.0 * 8.464731216430664
Epoch 740, val loss: 0.5132200121879578
Epoch 750, training loss: 423.66864013671875 = 0.48548105359077454 + 50.0 * 8.463663101196289
Epoch 750, val loss: 0.5098485946655273
Epoch 760, training loss: 423.5989990234375 = 0.48178377747535706 + 50.0 * 8.4623441696167
Epoch 760, val loss: 0.5067407488822937
Epoch 770, training loss: 423.5075378417969 = 0.4782925844192505 + 50.0 * 8.46058464050293
Epoch 770, val loss: 0.5037963390350342
Epoch 780, training loss: 423.4374694824219 = 0.4749567210674286 + 50.0 * 8.459250450134277
Epoch 780, val loss: 0.5009676814079285
Epoch 790, training loss: 423.504150390625 = 0.4717746675014496 + 50.0 * 8.460647583007812
Epoch 790, val loss: 0.4982532262802124
Epoch 800, training loss: 423.3710021972656 = 0.4687199890613556 + 50.0 * 8.458045959472656
Epoch 800, val loss: 0.4958755671977997
Epoch 810, training loss: 423.2559814453125 = 0.46582165360450745 + 50.0 * 8.455802917480469
Epoch 810, val loss: 0.4935295581817627
Epoch 820, training loss: 423.18206787109375 = 0.4630601108074188 + 50.0 * 8.45438003540039
Epoch 820, val loss: 0.49126994609832764
Epoch 830, training loss: 423.1748046875 = 0.46042364835739136 + 50.0 * 8.4542875289917
Epoch 830, val loss: 0.48915937542915344
Epoch 840, training loss: 423.0640563964844 = 0.4578811824321747 + 50.0 * 8.452123641967773
Epoch 840, val loss: 0.4872395992279053
Epoch 850, training loss: 423.0246276855469 = 0.4554497301578522 + 50.0 * 8.451383590698242
Epoch 850, val loss: 0.48536738753318787
Epoch 860, training loss: 422.93609619140625 = 0.45312759280204773 + 50.0 * 8.44965934753418
Epoch 860, val loss: 0.48357516527175903
Epoch 870, training loss: 422.94744873046875 = 0.4508926272392273 + 50.0 * 8.449931144714355
Epoch 870, val loss: 0.48193976283073425
Epoch 880, training loss: 422.814697265625 = 0.4487370252609253 + 50.0 * 8.447319030761719
Epoch 880, val loss: 0.4802474081516266
Epoch 890, training loss: 422.7434387207031 = 0.4466719627380371 + 50.0 * 8.445935249328613
Epoch 890, val loss: 0.4788290560245514
Epoch 900, training loss: 422.7275085449219 = 0.44468268752098083 + 50.0 * 8.445656776428223
Epoch 900, val loss: 0.47732144594192505
Epoch 910, training loss: 422.6143798828125 = 0.4427476227283478 + 50.0 * 8.443432807922363
Epoch 910, val loss: 0.47603684663772583
Epoch 920, training loss: 422.5663146972656 = 0.44088852405548096 + 50.0 * 8.442508697509766
Epoch 920, val loss: 0.4746599495410919
Epoch 930, training loss: 422.4827880859375 = 0.4390926957130432 + 50.0 * 8.440874099731445
Epoch 930, val loss: 0.47354111075401306
Epoch 940, training loss: 422.6286926269531 = 0.43735766410827637 + 50.0 * 8.443826675415039
Epoch 940, val loss: 0.47238290309906006
Epoch 950, training loss: 422.3736267089844 = 0.4356447160243988 + 50.0 * 8.438759803771973
Epoch 950, val loss: 0.47114062309265137
Epoch 960, training loss: 422.31304931640625 = 0.43400245904922485 + 50.0 * 8.437581062316895
Epoch 960, val loss: 0.47009557485580444
Epoch 970, training loss: 422.24969482421875 = 0.43241918087005615 + 50.0 * 8.436345100402832
Epoch 970, val loss: 0.46910014748573303
Epoch 980, training loss: 422.3013610839844 = 0.4308728277683258 + 50.0 * 8.437409400939941
Epoch 980, val loss: 0.4681403934955597
Epoch 990, training loss: 422.14691162109375 = 0.4293558895587921 + 50.0 * 8.434350967407227
Epoch 990, val loss: 0.46713781356811523
Epoch 1000, training loss: 422.0650329589844 = 0.42789679765701294 + 50.0 * 8.432743072509766
Epoch 1000, val loss: 0.46629828214645386
Epoch 1010, training loss: 422.1097106933594 = 0.4264635741710663 + 50.0 * 8.43366527557373
Epoch 1010, val loss: 0.46536603569984436
Epoch 1020, training loss: 421.9901428222656 = 0.4250675141811371 + 50.0 * 8.43130111694336
Epoch 1020, val loss: 0.46463099122047424
Epoch 1030, training loss: 421.91510009765625 = 0.4236961305141449 + 50.0 * 8.429827690124512
Epoch 1030, val loss: 0.46378588676452637
Epoch 1040, training loss: 421.9455871582031 = 0.4223507344722748 + 50.0 * 8.430464744567871
Epoch 1040, val loss: 0.4629570245742798
Epoch 1050, training loss: 421.83740234375 = 0.4210266172885895 + 50.0 * 8.428327560424805
Epoch 1050, val loss: 0.46227768063545227
Epoch 1060, training loss: 421.7485656738281 = 0.4197370707988739 + 50.0 * 8.426576614379883
Epoch 1060, val loss: 0.4615374505519867
Epoch 1070, training loss: 421.71856689453125 = 0.41847389936447144 + 50.0 * 8.42600154876709
Epoch 1070, val loss: 0.460925817489624
Epoch 1080, training loss: 421.7193298339844 = 0.41722631454467773 + 50.0 * 8.426041603088379
Epoch 1080, val loss: 0.460212379693985
Epoch 1090, training loss: 421.6971740722656 = 0.4159889817237854 + 50.0 * 8.425623893737793
Epoch 1090, val loss: 0.4594779312610626
Epoch 1100, training loss: 421.6159973144531 = 0.41476085782051086 + 50.0 * 8.42402458190918
Epoch 1100, val loss: 0.4588962197303772
Epoch 1110, training loss: 421.5431213378906 = 0.41356587409973145 + 50.0 * 8.422591209411621
Epoch 1110, val loss: 0.45825621485710144
Epoch 1120, training loss: 421.4981689453125 = 0.41240766644477844 + 50.0 * 8.421714782714844
Epoch 1120, val loss: 0.45769378542900085
Epoch 1130, training loss: 421.4521179199219 = 0.4112603962421417 + 50.0 * 8.420817375183105
Epoch 1130, val loss: 0.4571322500705719
Epoch 1140, training loss: 421.4123840332031 = 0.4101339280605316 + 50.0 * 8.420044898986816
Epoch 1140, val loss: 0.45657965540885925
Epoch 1150, training loss: 421.4245300292969 = 0.40902236104011536 + 50.0 * 8.420310020446777
Epoch 1150, val loss: 0.456136018037796
Epoch 1160, training loss: 421.3771057128906 = 0.4079066216945648 + 50.0 * 8.419384002685547
Epoch 1160, val loss: 0.455462247133255
Epoch 1170, training loss: 421.4688720703125 = 0.4068053960800171 + 50.0 * 8.421241760253906
Epoch 1170, val loss: 0.45500490069389343
Epoch 1180, training loss: 421.29559326171875 = 0.4057154059410095 + 50.0 * 8.417798042297363
Epoch 1180, val loss: 0.45449572801589966
Epoch 1190, training loss: 421.25689697265625 = 0.4046621024608612 + 50.0 * 8.417044639587402
Epoch 1190, val loss: 0.453901469707489
Epoch 1200, training loss: 421.20263671875 = 0.4036308825016022 + 50.0 * 8.415980339050293
Epoch 1200, val loss: 0.45347219705581665
Epoch 1210, training loss: 421.1669616699219 = 0.4026161730289459 + 50.0 * 8.415287017822266
Epoch 1210, val loss: 0.45303139090538025
Epoch 1220, training loss: 421.1359558105469 = 0.4016128480434418 + 50.0 * 8.414687156677246
Epoch 1220, val loss: 0.45257478952407837
Epoch 1230, training loss: 421.3887023925781 = 0.4006146788597107 + 50.0 * 8.419761657714844
Epoch 1230, val loss: 0.4521372616291046
Epoch 1240, training loss: 421.1854553222656 = 0.39960429072380066 + 50.0 * 8.415717124938965
Epoch 1240, val loss: 0.45169562101364136
Epoch 1250, training loss: 421.068603515625 = 0.39862391352653503 + 50.0 * 8.413399696350098
Epoch 1250, val loss: 0.4512903094291687
Epoch 1260, training loss: 421.01507568359375 = 0.3976626396179199 + 50.0 * 8.412347793579102
Epoch 1260, val loss: 0.45079848170280457
Epoch 1270, training loss: 420.98651123046875 = 0.3967239260673523 + 50.0 * 8.411795616149902
Epoch 1270, val loss: 0.45035630464553833
Epoch 1280, training loss: 420.97088623046875 = 0.39579102396965027 + 50.0 * 8.41150188446045
Epoch 1280, val loss: 0.4499638080596924
Epoch 1290, training loss: 420.9112548828125 = 0.39486566185951233 + 50.0 * 8.410327911376953
Epoch 1290, val loss: 0.44956034421920776
Epoch 1300, training loss: 421.1103210449219 = 0.39395228028297424 + 50.0 * 8.414327621459961
Epoch 1300, val loss: 0.4492603838443756
Epoch 1310, training loss: 420.92669677734375 = 0.393017441034317 + 50.0 * 8.410674095153809
Epoch 1310, val loss: 0.4487375319004059
Epoch 1320, training loss: 420.8404846191406 = 0.39211803674697876 + 50.0 * 8.408967018127441
Epoch 1320, val loss: 0.4483189880847931
Epoch 1330, training loss: 420.78485107421875 = 0.3912360370159149 + 50.0 * 8.407872200012207
Epoch 1330, val loss: 0.4479829967021942
Epoch 1340, training loss: 420.7572021484375 = 0.39036479592323303 + 50.0 * 8.407337188720703
Epoch 1340, val loss: 0.4475861191749573
Epoch 1350, training loss: 420.7638854980469 = 0.38950127363204956 + 50.0 * 8.407487869262695
Epoch 1350, val loss: 0.44726282358169556
Epoch 1360, training loss: 420.7911682128906 = 0.3886350393295288 + 50.0 * 8.408050537109375
Epoch 1360, val loss: 0.44687697291374207
Epoch 1370, training loss: 420.6839904785156 = 0.38776862621307373 + 50.0 * 8.405924797058105
Epoch 1370, val loss: 0.4464329779148102
Epoch 1380, training loss: 420.6709289550781 = 0.38692665100097656 + 50.0 * 8.405679702758789
Epoch 1380, val loss: 0.44608771800994873
Epoch 1390, training loss: 420.62164306640625 = 0.3861033022403717 + 50.0 * 8.40471076965332
Epoch 1390, val loss: 0.44576576352119446
Epoch 1400, training loss: 420.6195373535156 = 0.38529324531555176 + 50.0 * 8.404685020446777
Epoch 1400, val loss: 0.44548308849334717
Epoch 1410, training loss: 420.6837463378906 = 0.38446852564811707 + 50.0 * 8.405985832214355
Epoch 1410, val loss: 0.44499990344047546
Epoch 1420, training loss: 420.5718688964844 = 0.3836425244808197 + 50.0 * 8.403764724731445
Epoch 1420, val loss: 0.44474396109580994
Epoch 1430, training loss: 420.5264892578125 = 0.38284164667129517 + 50.0 * 8.402873039245605
Epoch 1430, val loss: 0.44431373476982117
Epoch 1440, training loss: 420.4913330078125 = 0.38205522298812866 + 50.0 * 8.402185440063477
Epoch 1440, val loss: 0.4439423978328705
Epoch 1450, training loss: 420.4637451171875 = 0.38127660751342773 + 50.0 * 8.401649475097656
Epoch 1450, val loss: 0.443664014339447
Epoch 1460, training loss: 420.5636291503906 = 0.38050171732902527 + 50.0 * 8.40366268157959
Epoch 1460, val loss: 0.44333022832870483
Epoch 1470, training loss: 420.4793395996094 = 0.37969493865966797 + 50.0 * 8.401992797851562
Epoch 1470, val loss: 0.44291824102401733
Epoch 1480, training loss: 420.42596435546875 = 0.3789108693599701 + 50.0 * 8.400940895080566
Epoch 1480, val loss: 0.44256624579429626
Epoch 1490, training loss: 420.37469482421875 = 0.3781445324420929 + 50.0 * 8.399930953979492
Epoch 1490, val loss: 0.4422204792499542
Epoch 1500, training loss: 420.34368896484375 = 0.37739309668540955 + 50.0 * 8.39932632446289
Epoch 1500, val loss: 0.44188591837882996
Epoch 1510, training loss: 420.3187255859375 = 0.37664520740509033 + 50.0 * 8.398841857910156
Epoch 1510, val loss: 0.44154128432273865
Epoch 1520, training loss: 420.4458312988281 = 0.3758946359157562 + 50.0 * 8.401398658752441
Epoch 1520, val loss: 0.441160649061203
Epoch 1530, training loss: 420.36590576171875 = 0.375125527381897 + 50.0 * 8.399815559387207
Epoch 1530, val loss: 0.4409283697605133
Epoch 1540, training loss: 420.2641296386719 = 0.374367356300354 + 50.0 * 8.397795677185059
Epoch 1540, val loss: 0.440472811460495
Epoch 1550, training loss: 420.2414855957031 = 0.37362951040267944 + 50.0 * 8.397356986999512
Epoch 1550, val loss: 0.44016313552856445
Epoch 1560, training loss: 420.20697021484375 = 0.3729020357131958 + 50.0 * 8.396681785583496
Epoch 1560, val loss: 0.4398192763328552
Epoch 1570, training loss: 420.1864013671875 = 0.37217429280281067 + 50.0 * 8.396285057067871
Epoch 1570, val loss: 0.43951550126075745
Epoch 1580, training loss: 420.4189453125 = 0.37144535779953003 + 50.0 * 8.400949478149414
Epoch 1580, val loss: 0.4393162131309509
Epoch 1590, training loss: 420.26263427734375 = 0.3706825077533722 + 50.0 * 8.397838592529297
Epoch 1590, val loss: 0.43869057297706604
Epoch 1600, training loss: 420.1258544921875 = 0.369945764541626 + 50.0 * 8.395118713378906
Epoch 1600, val loss: 0.43839746713638306
Epoch 1610, training loss: 420.1097106933594 = 0.3692263066768646 + 50.0 * 8.39480972290039
Epoch 1610, val loss: 0.4380543828010559
Epoch 1620, training loss: 420.0758361816406 = 0.36851638555526733 + 50.0 * 8.394145965576172
Epoch 1620, val loss: 0.4376881718635559
Epoch 1630, training loss: 420.0757141113281 = 0.36780813336372375 + 50.0 * 8.394158363342285
Epoch 1630, val loss: 0.4373558759689331
Epoch 1640, training loss: 420.34149169921875 = 0.3670744299888611 + 50.0 * 8.39948844909668
Epoch 1640, val loss: 0.4369809031486511
Epoch 1650, training loss: 420.0768127441406 = 0.36632272601127625 + 50.0 * 8.394209861755371
Epoch 1650, val loss: 0.4365520775318146
Epoch 1660, training loss: 420.0014343261719 = 0.36560186743736267 + 50.0 * 8.392716407775879
Epoch 1660, val loss: 0.4362756311893463
Epoch 1670, training loss: 419.99169921875 = 0.3649006485939026 + 50.0 * 8.392536163330078
Epoch 1670, val loss: 0.4359235465526581
Epoch 1680, training loss: 419.951904296875 = 0.3642059862613678 + 50.0 * 8.391754150390625
Epoch 1680, val loss: 0.4355951249599457
Epoch 1690, training loss: 419.9364929199219 = 0.3635111153125763 + 50.0 * 8.391459465026855
Epoch 1690, val loss: 0.4352659583091736
Epoch 1700, training loss: 420.0038757324219 = 0.3628138303756714 + 50.0 * 8.392821311950684
Epoch 1700, val loss: 0.4350203275680542
Epoch 1710, training loss: 419.910400390625 = 0.3620900809764862 + 50.0 * 8.390966415405273
Epoch 1710, val loss: 0.43453100323677063
Epoch 1720, training loss: 419.90826416015625 = 0.36137932538986206 + 50.0 * 8.390937805175781
Epoch 1720, val loss: 0.4342537522315979
Epoch 1730, training loss: 419.8697204589844 = 0.3606771230697632 + 50.0 * 8.390180587768555
Epoch 1730, val loss: 0.43386298418045044
Epoch 1740, training loss: 419.8953857421875 = 0.3599846065044403 + 50.0 * 8.390707969665527
Epoch 1740, val loss: 0.4334973096847534
Epoch 1750, training loss: 419.83087158203125 = 0.3592848777770996 + 50.0 * 8.389431953430176
Epoch 1750, val loss: 0.4332234263420105
Epoch 1760, training loss: 419.8192138671875 = 0.35859066247940063 + 50.0 * 8.389212608337402
Epoch 1760, val loss: 0.4329279363155365
Epoch 1770, training loss: 419.8106689453125 = 0.35790252685546875 + 50.0 * 8.389055252075195
Epoch 1770, val loss: 0.43260180950164795
Epoch 1780, training loss: 419.84197998046875 = 0.357209712266922 + 50.0 * 8.389695167541504
Epoch 1780, val loss: 0.43236249685287476
Epoch 1790, training loss: 419.7922058105469 = 0.35651305317878723 + 50.0 * 8.388713836669922
Epoch 1790, val loss: 0.43196576833724976
Epoch 1800, training loss: 419.8357849121094 = 0.3558250665664673 + 50.0 * 8.389598846435547
Epoch 1800, val loss: 0.4317810535430908
Epoch 1810, training loss: 419.75201416015625 = 0.3551211357116699 + 50.0 * 8.387937545776367
Epoch 1810, val loss: 0.43121999502182007
Epoch 1820, training loss: 419.78961181640625 = 0.3544330298900604 + 50.0 * 8.388703346252441
Epoch 1820, val loss: 0.430959016084671
Epoch 1830, training loss: 419.7044982910156 = 0.35373395681381226 + 50.0 * 8.387015342712402
Epoch 1830, val loss: 0.4306528568267822
Epoch 1840, training loss: 419.68310546875 = 0.35304006934165955 + 50.0 * 8.386601448059082
Epoch 1840, val loss: 0.43032997846603394
Epoch 1850, training loss: 419.6499938964844 = 0.35234853625297546 + 50.0 * 8.385952949523926
Epoch 1850, val loss: 0.4299636781215668
Epoch 1860, training loss: 419.6589660644531 = 0.3516576588153839 + 50.0 * 8.386146545410156
Epoch 1860, val loss: 0.4296402633190155
Epoch 1870, training loss: 419.7811279296875 = 0.3509526550769806 + 50.0 * 8.388603210449219
Epoch 1870, val loss: 0.42929890751838684
Epoch 1880, training loss: 419.64141845703125 = 0.35024362802505493 + 50.0 * 8.385823249816895
Epoch 1880, val loss: 0.4290548861026764
Epoch 1890, training loss: 419.592041015625 = 0.34954240918159485 + 50.0 * 8.384849548339844
Epoch 1890, val loss: 0.42869243025779724
Epoch 1900, training loss: 419.56890869140625 = 0.34885281324386597 + 50.0 * 8.384401321411133
Epoch 1900, val loss: 0.4284488558769226
Epoch 1910, training loss: 419.6349182128906 = 0.34816595911979675 + 50.0 * 8.385734558105469
Epoch 1910, val loss: 0.42825618386268616
Epoch 1920, training loss: 419.6700134277344 = 0.3474484980106354 + 50.0 * 8.386451721191406
Epoch 1920, val loss: 0.4277285039424896
Epoch 1930, training loss: 419.5740661621094 = 0.34673142433166504 + 50.0 * 8.384546279907227
Epoch 1930, val loss: 0.42745450139045715
Epoch 1940, training loss: 419.50811767578125 = 0.34602826833724976 + 50.0 * 8.383241653442383
Epoch 1940, val loss: 0.42712005972862244
Epoch 1950, training loss: 419.4922180175781 = 0.3453327417373657 + 50.0 * 8.38293743133545
Epoch 1950, val loss: 0.42677223682403564
Epoch 1960, training loss: 419.5443420410156 = 0.34463560581207275 + 50.0 * 8.383994102478027
Epoch 1960, val loss: 0.42646104097366333
Epoch 1970, training loss: 419.50897216796875 = 0.3439229130744934 + 50.0 * 8.38330078125
Epoch 1970, val loss: 0.42618492245674133
Epoch 1980, training loss: 419.47613525390625 = 0.34321242570877075 + 50.0 * 8.382658958435059
Epoch 1980, val loss: 0.42587852478027344
Epoch 1990, training loss: 419.4339599609375 = 0.3425115942955017 + 50.0 * 8.381829261779785
Epoch 1990, val loss: 0.42555394768714905
Epoch 2000, training loss: 419.41448974609375 = 0.3418160676956177 + 50.0 * 8.381453514099121
Epoch 2000, val loss: 0.4253164231777191
Epoch 2010, training loss: 419.4585876464844 = 0.34112027287483215 + 50.0 * 8.382349014282227
Epoch 2010, val loss: 0.42509838938713074
Epoch 2020, training loss: 419.5718078613281 = 0.34040626883506775 + 50.0 * 8.384628295898438
Epoch 2020, val loss: 0.424718976020813
Epoch 2030, training loss: 419.4107360839844 = 0.3396797776222229 + 50.0 * 8.381421089172363
Epoch 2030, val loss: 0.4243873059749603
Epoch 2040, training loss: 419.3650207519531 = 0.33897411823272705 + 50.0 * 8.380520820617676
Epoch 2040, val loss: 0.42406371235847473
Epoch 2050, training loss: 419.34075927734375 = 0.3382778465747833 + 50.0 * 8.380049705505371
Epoch 2050, val loss: 0.42382127046585083
Epoch 2060, training loss: 419.3560791015625 = 0.33757922053337097 + 50.0 * 8.380370140075684
Epoch 2060, val loss: 0.42353883385658264
Epoch 2070, training loss: 419.5560607910156 = 0.3368630111217499 + 50.0 * 8.384384155273438
Epoch 2070, val loss: 0.42324066162109375
Epoch 2080, training loss: 419.34600830078125 = 0.33613112568855286 + 50.0 * 8.380197525024414
Epoch 2080, val loss: 0.4229471981525421
Epoch 2090, training loss: 419.2848205566406 = 0.3354228734970093 + 50.0 * 8.378988265991211
Epoch 2090, val loss: 0.42273589968681335
Epoch 2100, training loss: 419.2662048339844 = 0.334719717502594 + 50.0 * 8.378629684448242
Epoch 2100, val loss: 0.4224616289138794
Epoch 2110, training loss: 419.2542419433594 = 0.33401772379875183 + 50.0 * 8.37840461730957
Epoch 2110, val loss: 0.42223724722862244
Epoch 2120, training loss: 419.3328552246094 = 0.3333132863044739 + 50.0 * 8.379990577697754
Epoch 2120, val loss: 0.4220907986164093
Epoch 2130, training loss: 419.2265319824219 = 0.3325757384300232 + 50.0 * 8.37787914276123
Epoch 2130, val loss: 0.4216259717941284
Epoch 2140, training loss: 419.22283935546875 = 0.3318507969379425 + 50.0 * 8.377820014953613
Epoch 2140, val loss: 0.42148807644844055
Epoch 2150, training loss: 419.2079772949219 = 0.3311331868171692 + 50.0 * 8.37753677368164
Epoch 2150, val loss: 0.4211076498031616
Epoch 2160, training loss: 419.1964416503906 = 0.33041974902153015 + 50.0 * 8.377320289611816
Epoch 2160, val loss: 0.4209093153476715
Epoch 2170, training loss: 419.3580322265625 = 0.3296983540058136 + 50.0 * 8.380566596984863
Epoch 2170, val loss: 0.42059236764907837
Epoch 2180, training loss: 419.2207946777344 = 0.32896193861961365 + 50.0 * 8.377837181091309
Epoch 2180, val loss: 0.42039644718170166
Epoch 2190, training loss: 419.1628112792969 = 0.3282305896282196 + 50.0 * 8.376691818237305
Epoch 2190, val loss: 0.4201015532016754
Epoch 2200, training loss: 419.15472412109375 = 0.3275076150894165 + 50.0 * 8.376543998718262
Epoch 2200, val loss: 0.4198837876319885
Epoch 2210, training loss: 419.2658996582031 = 0.3267788290977478 + 50.0 * 8.378782272338867
Epoch 2210, val loss: 0.41963571310043335
Epoch 2220, training loss: 419.2029113769531 = 0.3260301351547241 + 50.0 * 8.377537727355957
Epoch 2220, val loss: 0.419185072183609
Epoch 2230, training loss: 419.1188659667969 = 0.3252975344657898 + 50.0 * 8.375871658325195
Epoch 2230, val loss: 0.41915395855903625
Epoch 2240, training loss: 419.09405517578125 = 0.32456091046333313 + 50.0 * 8.37539005279541
Epoch 2240, val loss: 0.4187535047531128
Epoch 2250, training loss: 419.0693054199219 = 0.3238372504711151 + 50.0 * 8.374909400939941
Epoch 2250, val loss: 0.4186154305934906
Epoch 2260, training loss: 419.0631408691406 = 0.323108434677124 + 50.0 * 8.374800682067871
Epoch 2260, val loss: 0.41834816336631775
Epoch 2270, training loss: 419.30316162109375 = 0.32237952947616577 + 50.0 * 8.379615783691406
Epoch 2270, val loss: 0.4182274341583252
Epoch 2280, training loss: 419.1430358886719 = 0.3216063976287842 + 50.0 * 8.376428604125977
Epoch 2280, val loss: 0.41769129037857056
Epoch 2290, training loss: 419.09765625 = 0.3208611309528351 + 50.0 * 8.37553596496582
Epoch 2290, val loss: 0.41765183210372925
Epoch 2300, training loss: 419.026123046875 = 0.3201152980327606 + 50.0 * 8.374119758605957
Epoch 2300, val loss: 0.4173326790332794
Epoch 2310, training loss: 418.9950866699219 = 0.3193807303905487 + 50.0 * 8.373514175415039
Epoch 2310, val loss: 0.41708871722221375
Epoch 2320, training loss: 418.9844665527344 = 0.31864815950393677 + 50.0 * 8.373316764831543
Epoch 2320, val loss: 0.4169212281703949
Epoch 2330, training loss: 419.0661926269531 = 0.3179108500480652 + 50.0 * 8.37496566772461
Epoch 2330, val loss: 0.416700154542923
Epoch 2340, training loss: 418.9857482910156 = 0.3171501159667969 + 50.0 * 8.373372077941895
Epoch 2340, val loss: 0.41643279790878296
Epoch 2350, training loss: 418.9595031738281 = 0.31638723611831665 + 50.0 * 8.372862815856934
Epoch 2350, val loss: 0.4161364436149597
Epoch 2360, training loss: 418.9415588378906 = 0.3156380355358124 + 50.0 * 8.372518539428711
Epoch 2360, val loss: 0.415945440530777
Epoch 2370, training loss: 418.927978515625 = 0.31488925218582153 + 50.0 * 8.372262001037598
Epoch 2370, val loss: 0.41572999954223633
Epoch 2380, training loss: 418.9698181152344 = 0.3141401708126068 + 50.0 * 8.373113632202148
Epoch 2380, val loss: 0.41558656096458435
Epoch 2390, training loss: 418.9374084472656 = 0.3133658468723297 + 50.0 * 8.372481346130371
Epoch 2390, val loss: 0.415250688791275
Epoch 2400, training loss: 418.9156799316406 = 0.31259411573410034 + 50.0 * 8.372061729431152
Epoch 2400, val loss: 0.4150339961051941
Epoch 2410, training loss: 418.9289245605469 = 0.3118281066417694 + 50.0 * 8.372342109680176
Epoch 2410, val loss: 0.41477909684181213
Epoch 2420, training loss: 418.9564514160156 = 0.31106656789779663 + 50.0 * 8.372907638549805
Epoch 2420, val loss: 0.41470786929130554
Epoch 2430, training loss: 418.93011474609375 = 0.31029823422431946 + 50.0 * 8.372396469116211
Epoch 2430, val loss: 0.4145309329032898
Epoch 2440, training loss: 418.8548583984375 = 0.30952417850494385 + 50.0 * 8.370906829833984
Epoch 2440, val loss: 0.4142007827758789
Epoch 2450, training loss: 418.8487548828125 = 0.30875805020332336 + 50.0 * 8.370800018310547
Epoch 2450, val loss: 0.41396859288215637
Epoch 2460, training loss: 418.835693359375 = 0.3079957962036133 + 50.0 * 8.370553970336914
Epoch 2460, val loss: 0.41384220123291016
Epoch 2470, training loss: 418.9325256347656 = 0.3072254955768585 + 50.0 * 8.372506141662598
Epoch 2470, val loss: 0.4136022627353668
Epoch 2480, training loss: 418.8397521972656 = 0.30643320083618164 + 50.0 * 8.37066650390625
Epoch 2480, val loss: 0.4133787453174591
Epoch 2490, training loss: 418.81976318359375 = 0.3056497573852539 + 50.0 * 8.370282173156738
Epoch 2490, val loss: 0.41328123211860657
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8330796549974632
0.8648844454104181
=== training gcn model ===
Epoch 0, training loss: 530.2211303710938 = 1.1053346395492554 + 50.0 * 10.582315444946289
Epoch 0, val loss: 1.1032843589782715
Epoch 10, training loss: 530.2061157226562 = 1.100754976272583 + 50.0 * 10.582107543945312
Epoch 10, val loss: 1.0986952781677246
Epoch 20, training loss: 530.1575317382812 = 1.0958648920059204 + 50.0 * 10.581233024597168
Epoch 20, val loss: 1.0938072204589844
Epoch 30, training loss: 529.9605712890625 = 1.0905017852783203 + 50.0 * 10.577401161193848
Epoch 30, val loss: 1.088444709777832
Epoch 40, training loss: 529.178466796875 = 1.0845426321029663 + 50.0 * 10.56187915802002
Epoch 40, val loss: 1.0824942588806152
Epoch 50, training loss: 526.6954956054688 = 1.077724814414978 + 50.0 * 10.51235580444336
Epoch 50, val loss: 1.0756771564483643
Epoch 60, training loss: 520.952392578125 = 1.0700205564498901 + 50.0 * 10.397647857666016
Epoch 60, val loss: 1.068089485168457
Epoch 70, training loss: 510.9116516113281 = 1.0620453357696533 + 50.0 * 10.196991920471191
Epoch 70, val loss: 1.0602511167526245
Epoch 80, training loss: 499.370361328125 = 1.053586721420288 + 50.0 * 9.96633529663086
Epoch 80, val loss: 1.0520496368408203
Epoch 90, training loss: 481.26629638671875 = 1.044893503189087 + 50.0 * 9.6044282913208
Epoch 90, val loss: 1.0437164306640625
Epoch 100, training loss: 472.4960632324219 = 1.0377757549285889 + 50.0 * 9.429165840148926
Epoch 100, val loss: 1.0369137525558472
Epoch 110, training loss: 465.53717041015625 = 1.031395435333252 + 50.0 * 9.290115356445312
Epoch 110, val loss: 1.0308277606964111
Epoch 120, training loss: 461.1434020996094 = 1.0254805088043213 + 50.0 * 9.20235824584961
Epoch 120, val loss: 1.0251060724258423
Epoch 130, training loss: 458.6811218261719 = 1.019522786140442 + 50.0 * 9.153231620788574
Epoch 130, val loss: 1.0192968845367432
Epoch 140, training loss: 456.5982666015625 = 1.0134871006011963 + 50.0 * 9.111695289611816
Epoch 140, val loss: 1.0134307146072388
Epoch 150, training loss: 454.64080810546875 = 1.007546305656433 + 50.0 * 9.072665214538574
Epoch 150, val loss: 1.0077012777328491
Epoch 160, training loss: 453.1910705566406 = 1.001715064048767 + 50.0 * 9.043787002563477
Epoch 160, val loss: 1.0020675659179688
Epoch 170, training loss: 452.2823486328125 = 0.9956170916557312 + 50.0 * 9.025734901428223
Epoch 170, val loss: 0.9961395859718323
Epoch 180, training loss: 451.61810302734375 = 0.9888760447502136 + 50.0 * 9.012584686279297
Epoch 180, val loss: 0.9895442128181458
Epoch 190, training loss: 450.9166564941406 = 0.9815211892127991 + 50.0 * 8.998703002929688
Epoch 190, val loss: 0.9823894500732422
Epoch 200, training loss: 450.0462646484375 = 0.973854124546051 + 50.0 * 8.98144817352295
Epoch 200, val loss: 0.9749663472175598
Epoch 210, training loss: 448.8800048828125 = 0.9663019776344299 + 50.0 * 8.958273887634277
Epoch 210, val loss: 0.9676806330680847
Epoch 220, training loss: 447.22515869140625 = 0.9590359926223755 + 50.0 * 8.925322532653809
Epoch 220, val loss: 0.960686445236206
Epoch 230, training loss: 445.0403137207031 = 0.9523404836654663 + 50.0 * 8.881759643554688
Epoch 230, val loss: 0.9542521834373474
Epoch 240, training loss: 443.00189208984375 = 0.9457204341888428 + 50.0 * 8.841123580932617
Epoch 240, val loss: 0.9477812647819519
Epoch 250, training loss: 441.58526611328125 = 0.9380728602409363 + 50.0 * 8.812943458557129
Epoch 250, val loss: 0.9402100443840027
Epoch 260, training loss: 440.4322814941406 = 0.9294265508651733 + 50.0 * 8.790057182312012
Epoch 260, val loss: 0.9317871928215027
Epoch 270, training loss: 439.45068359375 = 0.9205756187438965 + 50.0 * 8.770602226257324
Epoch 270, val loss: 0.9232273101806641
Epoch 280, training loss: 438.5425720214844 = 0.9113451242446899 + 50.0 * 8.75262451171875
Epoch 280, val loss: 0.914320170879364
Epoch 290, training loss: 437.6728515625 = 0.9017176032066345 + 50.0 * 8.73542308807373
Epoch 290, val loss: 0.9050439596176147
Epoch 300, training loss: 436.6791687011719 = 0.8918278813362122 + 50.0 * 8.715746879577637
Epoch 300, val loss: 0.8956083655357361
Epoch 310, training loss: 435.82244873046875 = 0.8818582892417908 + 50.0 * 8.698811531066895
Epoch 310, val loss: 0.8860493302345276
Epoch 320, training loss: 435.1021423339844 = 0.8713946342468262 + 50.0 * 8.684615135192871
Epoch 320, val loss: 0.8759885430335999
Epoch 330, training loss: 434.4222717285156 = 0.8605766892433167 + 50.0 * 8.671234130859375
Epoch 330, val loss: 0.865658164024353
Epoch 340, training loss: 433.7466735839844 = 0.8498184084892273 + 50.0 * 8.657937049865723
Epoch 340, val loss: 0.8554224967956543
Epoch 350, training loss: 433.19134521484375 = 0.8390716910362244 + 50.0 * 8.647045135498047
Epoch 350, val loss: 0.8452472686767578
Epoch 360, training loss: 432.7567138671875 = 0.8282244205474854 + 50.0 * 8.638569831848145
Epoch 360, val loss: 0.8349876403808594
Epoch 370, training loss: 432.37884521484375 = 0.8174173831939697 + 50.0 * 8.63122844696045
Epoch 370, val loss: 0.8247964978218079
Epoch 380, training loss: 431.987548828125 = 0.8067185878753662 + 50.0 * 8.623616218566895
Epoch 380, val loss: 0.8147227764129639
Epoch 390, training loss: 431.60931396484375 = 0.7961302995681763 + 50.0 * 8.616263389587402
Epoch 390, val loss: 0.8047628998756409
Epoch 400, training loss: 431.3313293457031 = 0.7856008410453796 + 50.0 * 8.61091423034668
Epoch 400, val loss: 0.7948745489120483
Epoch 410, training loss: 431.00421142578125 = 0.7751064896583557 + 50.0 * 8.604581832885742
Epoch 410, val loss: 0.7850648164749146
Epoch 420, training loss: 430.7247619628906 = 0.7646947503089905 + 50.0 * 8.599201202392578
Epoch 420, val loss: 0.77534419298172
Epoch 430, training loss: 430.47705078125 = 0.7543076276779175 + 50.0 * 8.594454765319824
Epoch 430, val loss: 0.7656406760215759
Epoch 440, training loss: 430.2495422363281 = 0.7439730763435364 + 50.0 * 8.590110778808594
Epoch 440, val loss: 0.7560222148895264
Epoch 450, training loss: 430.0665283203125 = 0.7337256669998169 + 50.0 * 8.586655616760254
Epoch 450, val loss: 0.7464622855186462
Epoch 460, training loss: 429.89947509765625 = 0.7235230207443237 + 50.0 * 8.583518981933594
Epoch 460, val loss: 0.7369303703308105
Epoch 470, training loss: 429.55889892578125 = 0.7134400010108948 + 50.0 * 8.576909065246582
Epoch 470, val loss: 0.7275930047035217
Epoch 480, training loss: 429.29541015625 = 0.703575074672699 + 50.0 * 8.571836471557617
Epoch 480, val loss: 0.7184416055679321
Epoch 490, training loss: 429.0108337402344 = 0.693911075592041 + 50.0 * 8.566338539123535
Epoch 490, val loss: 0.709477961063385
Epoch 500, training loss: 428.82232666015625 = 0.6842780709266663 + 50.0 * 8.562761306762695
Epoch 500, val loss: 0.7005985975265503
Epoch 510, training loss: 428.4095153808594 = 0.6748781800270081 + 50.0 * 8.554693222045898
Epoch 510, val loss: 0.6918862462043762
Epoch 520, training loss: 428.07000732421875 = 0.6657134890556335 + 50.0 * 8.548086166381836
Epoch 520, val loss: 0.6834323406219482
Epoch 530, training loss: 427.7269287109375 = 0.6566674709320068 + 50.0 * 8.541404724121094
Epoch 530, val loss: 0.6751205325126648
Epoch 540, training loss: 427.4017028808594 = 0.6477710008621216 + 50.0 * 8.535079002380371
Epoch 540, val loss: 0.666955292224884
Epoch 550, training loss: 427.15228271484375 = 0.6388752460479736 + 50.0 * 8.530267715454102
Epoch 550, val loss: 0.6587933897972107
Epoch 560, training loss: 426.8783874511719 = 0.629958987236023 + 50.0 * 8.524969100952148
Epoch 560, val loss: 0.6505835652351379
Epoch 570, training loss: 426.65093994140625 = 0.6212460398674011 + 50.0 * 8.520593643188477
Epoch 570, val loss: 0.6426651477813721
Epoch 580, training loss: 426.4498291015625 = 0.6126478910446167 + 50.0 * 8.516743659973145
Epoch 580, val loss: 0.6348025798797607
Epoch 590, training loss: 426.2654113769531 = 0.6041776537895203 + 50.0 * 8.513224601745605
Epoch 590, val loss: 0.6271328330039978
Epoch 600, training loss: 426.1026306152344 = 0.5959001183509827 + 50.0 * 8.51013469696045
Epoch 600, val loss: 0.6196390986442566
Epoch 610, training loss: 426.1641845703125 = 0.5878382325172424 + 50.0 * 8.511527061462402
Epoch 610, val loss: 0.6123454570770264
Epoch 620, training loss: 425.851318359375 = 0.5798860192298889 + 50.0 * 8.505428314208984
Epoch 620, val loss: 0.6052290201187134
Epoch 630, training loss: 425.71124267578125 = 0.5722455978393555 + 50.0 * 8.502779960632324
Epoch 630, val loss: 0.5984098315238953
Epoch 640, training loss: 425.5765686035156 = 0.5648645162582397 + 50.0 * 8.500234603881836
Epoch 640, val loss: 0.5918651223182678
Epoch 650, training loss: 425.4494934082031 = 0.5577451586723328 + 50.0 * 8.497835159301758
Epoch 650, val loss: 0.5855845808982849
Epoch 660, training loss: 425.40313720703125 = 0.55088871717453 + 50.0 * 8.497044563293457
Epoch 660, val loss: 0.579554557800293
Epoch 670, training loss: 425.32952880859375 = 0.5441990494728088 + 50.0 * 8.495706558227539
Epoch 670, val loss: 0.573668897151947
Epoch 680, training loss: 425.13128662109375 = 0.537863552570343 + 50.0 * 8.491868019104004
Epoch 680, val loss: 0.5681911706924438
Epoch 690, training loss: 424.9996032714844 = 0.5317961573600769 + 50.0 * 8.48935604095459
Epoch 690, val loss: 0.5629456043243408
Epoch 700, training loss: 424.8873291015625 = 0.5259696841239929 + 50.0 * 8.487227439880371
Epoch 700, val loss: 0.5579358339309692
Epoch 710, training loss: 424.7728576660156 = 0.5203812122344971 + 50.0 * 8.4850492477417
Epoch 710, val loss: 0.5531913638114929
Epoch 720, training loss: 424.6697082519531 = 0.5150332450866699 + 50.0 * 8.48309326171875
Epoch 720, val loss: 0.5486778020858765
Epoch 730, training loss: 424.743896484375 = 0.5098924040794373 + 50.0 * 8.48468017578125
Epoch 730, val loss: 0.5443257689476013
Epoch 740, training loss: 424.516845703125 = 0.5048982501029968 + 50.0 * 8.480238914489746
Epoch 740, val loss: 0.5402072072029114
Epoch 750, training loss: 424.3772888183594 = 0.5001738667488098 + 50.0 * 8.47754192352295
Epoch 750, val loss: 0.536320149898529
Epoch 760, training loss: 424.2770080566406 = 0.49565497040748596 + 50.0 * 8.475626945495605
Epoch 760, val loss: 0.5326409339904785
Epoch 770, training loss: 424.19317626953125 = 0.4913526475429535 + 50.0 * 8.47403621673584
Epoch 770, val loss: 0.5291644334793091
Epoch 780, training loss: 424.3055725097656 = 0.48721280694007874 + 50.0 * 8.476366996765137
Epoch 780, val loss: 0.5258422493934631
Epoch 790, training loss: 424.02716064453125 = 0.4832284152507782 + 50.0 * 8.470878601074219
Epoch 790, val loss: 0.5226902365684509
Epoch 800, training loss: 423.94744873046875 = 0.47943979501724243 + 50.0 * 8.4693603515625
Epoch 800, val loss: 0.5196937918663025
Epoch 810, training loss: 423.85009765625 = 0.4758254885673523 + 50.0 * 8.467485427856445
Epoch 810, val loss: 0.5168793797492981
Epoch 820, training loss: 423.77178955078125 = 0.4723782539367676 + 50.0 * 8.465988159179688
Epoch 820, val loss: 0.5142537951469421
Epoch 830, training loss: 423.8609619140625 = 0.4690723717212677 + 50.0 * 8.4678373336792
Epoch 830, val loss: 0.5117174386978149
Epoch 840, training loss: 423.6865539550781 = 0.4658227264881134 + 50.0 * 8.464414596557617
Epoch 840, val loss: 0.5092710256576538
Epoch 850, training loss: 423.5647888183594 = 0.46280747652053833 + 50.0 * 8.462039947509766
Epoch 850, val loss: 0.5070485472679138
Epoch 860, training loss: 423.48602294921875 = 0.45990800857543945 + 50.0 * 8.460522651672363
Epoch 860, val loss: 0.5049296617507935
Epoch 870, training loss: 423.4105529785156 = 0.45712798833847046 + 50.0 * 8.459068298339844
Epoch 870, val loss: 0.5029066205024719
Epoch 880, training loss: 423.4931640625 = 0.45444732904434204 + 50.0 * 8.460774421691895
Epoch 880, val loss: 0.5009601712226868
Epoch 890, training loss: 423.2755432128906 = 0.4518609046936035 + 50.0 * 8.456473350524902
Epoch 890, val loss: 0.4991720914840698
Epoch 900, training loss: 423.2134704589844 = 0.44939902424812317 + 50.0 * 8.455281257629395
Epoch 900, val loss: 0.49745073914527893
Epoch 910, training loss: 423.13922119140625 = 0.44703617691993713 + 50.0 * 8.45384407043457
Epoch 910, val loss: 0.4958169460296631
Epoch 920, training loss: 423.0606384277344 = 0.4447702467441559 + 50.0 * 8.452317237854004
Epoch 920, val loss: 0.4942830502986908
Epoch 930, training loss: 423.0365295410156 = 0.4425908327102661 + 50.0 * 8.451878547668457
Epoch 930, val loss: 0.4928343594074249
Epoch 940, training loss: 423.0115051269531 = 0.44045859575271606 + 50.0 * 8.451420783996582
Epoch 940, val loss: 0.4914391040802002
Epoch 950, training loss: 422.9196472167969 = 0.4383868873119354 + 50.0 * 8.449625015258789
Epoch 950, val loss: 0.4900413453578949
Epoch 960, training loss: 422.8300476074219 = 0.43643882870674133 + 50.0 * 8.447872161865234
Epoch 960, val loss: 0.48881664872169495
Epoch 970, training loss: 422.74560546875 = 0.4345422089099884 + 50.0 * 8.446221351623535
Epoch 970, val loss: 0.4875924289226532
Epoch 980, training loss: 422.6820373535156 = 0.4327123463153839 + 50.0 * 8.444986343383789
Epoch 980, val loss: 0.4864582121372223
Epoch 990, training loss: 422.8681640625 = 0.4309264123439789 + 50.0 * 8.448744773864746
Epoch 990, val loss: 0.48529165983200073
Epoch 1000, training loss: 422.62774658203125 = 0.4291818141937256 + 50.0 * 8.443971633911133
Epoch 1000, val loss: 0.48432472348213196
Epoch 1010, training loss: 422.51824951171875 = 0.4275122582912445 + 50.0 * 8.441814422607422
Epoch 1010, val loss: 0.4833269715309143
Epoch 1020, training loss: 422.4416198730469 = 0.42588070034980774 + 50.0 * 8.440315246582031
Epoch 1020, val loss: 0.48232030868530273
Epoch 1030, training loss: 422.37841796875 = 0.42431479692459106 + 50.0 * 8.439082145690918
Epoch 1030, val loss: 0.48139816522598267
Epoch 1040, training loss: 422.3255920410156 = 0.4227884113788605 + 50.0 * 8.438055992126465
Epoch 1040, val loss: 0.4805181920528412
Epoch 1050, training loss: 422.5382385253906 = 0.4212816655635834 + 50.0 * 8.442338943481445
Epoch 1050, val loss: 0.4796556532382965
Epoch 1060, training loss: 422.2315368652344 = 0.41979843378067017 + 50.0 * 8.436234474182129
Epoch 1060, val loss: 0.4787592589855194
Epoch 1070, training loss: 422.1923828125 = 0.41837987303733826 + 50.0 * 8.435480117797852
Epoch 1070, val loss: 0.47798678278923035
Epoch 1080, training loss: 422.1742248535156 = 0.4169890284538269 + 50.0 * 8.435144424438477
Epoch 1080, val loss: 0.47718921303749084
Epoch 1090, training loss: 422.0674133300781 = 0.41563090682029724 + 50.0 * 8.433035850524902
Epoch 1090, val loss: 0.47646424174308777
Epoch 1100, training loss: 422.0208435058594 = 0.41430965065956116 + 50.0 * 8.432130813598633
Epoch 1100, val loss: 0.4757035970687866
Epoch 1110, training loss: 421.96600341796875 = 0.4130190908908844 + 50.0 * 8.431059837341309
Epoch 1110, val loss: 0.47499558329582214
Epoch 1120, training loss: 421.9891052246094 = 0.4117540717124939 + 50.0 * 8.431547164916992
Epoch 1120, val loss: 0.474265992641449
Epoch 1130, training loss: 421.93548583984375 = 0.41049689054489136 + 50.0 * 8.430500030517578
Epoch 1130, val loss: 0.47362834215164185
Epoch 1140, training loss: 421.85296630859375 = 0.4092599153518677 + 50.0 * 8.428874015808105
Epoch 1140, val loss: 0.4729629158973694
Epoch 1150, training loss: 421.79638671875 = 0.4080623388290405 + 50.0 * 8.427766799926758
Epoch 1150, val loss: 0.47231385111808777
Epoch 1160, training loss: 421.7673034667969 = 0.40689176321029663 + 50.0 * 8.427207946777344
Epoch 1160, val loss: 0.4716983735561371
Epoch 1170, training loss: 421.9169006347656 = 0.40572717785835266 + 50.0 * 8.43022346496582
Epoch 1170, val loss: 0.47109103202819824
Epoch 1180, training loss: 421.6953430175781 = 0.404570996761322 + 50.0 * 8.42581558227539
Epoch 1180, val loss: 0.4704509377479553
Epoch 1190, training loss: 421.6197509765625 = 0.40345701575279236 + 50.0 * 8.424325942993164
Epoch 1190, val loss: 0.46987923979759216
Epoch 1200, training loss: 421.5824890136719 = 0.4023725092411041 + 50.0 * 8.423602104187012
Epoch 1200, val loss: 0.46931254863739014
Epoch 1210, training loss: 421.5464172363281 = 0.40131062269210815 + 50.0 * 8.42290210723877
Epoch 1210, val loss: 0.4688117802143097
Epoch 1220, training loss: 421.8047790527344 = 0.4002746343612671 + 50.0 * 8.42809009552002
Epoch 1220, val loss: 0.46837249398231506
Epoch 1230, training loss: 421.52227783203125 = 0.3991844356060028 + 50.0 * 8.422462463378906
Epoch 1230, val loss: 0.4676883816719055
Epoch 1240, training loss: 421.4601135253906 = 0.39815473556518555 + 50.0 * 8.421238899230957
Epoch 1240, val loss: 0.46718496084213257
Epoch 1250, training loss: 421.4003601074219 = 0.3971526622772217 + 50.0 * 8.420063972473145
Epoch 1250, val loss: 0.4667098820209503
Epoch 1260, training loss: 421.3693542480469 = 0.39616337418556213 + 50.0 * 8.419464111328125
Epoch 1260, val loss: 0.4662567973136902
Epoch 1270, training loss: 421.60650634765625 = 0.39518338441848755 + 50.0 * 8.424226760864258
Epoch 1270, val loss: 0.4656863212585449
Epoch 1280, training loss: 421.34625244140625 = 0.39417800307273865 + 50.0 * 8.419041633605957
Epoch 1280, val loss: 0.4653007984161377
Epoch 1290, training loss: 421.2796936035156 = 0.39320796728134155 + 50.0 * 8.417729377746582
Epoch 1290, val loss: 0.4648112952709198
Epoch 1300, training loss: 421.2350158691406 = 0.39226293563842773 + 50.0 * 8.416854858398438
Epoch 1300, val loss: 0.46434441208839417
Epoch 1310, training loss: 421.2010192871094 = 0.39133092761039734 + 50.0 * 8.416193962097168
Epoch 1310, val loss: 0.4639219641685486
Epoch 1320, training loss: 421.17620849609375 = 0.3904103636741638 + 50.0 * 8.415716171264648
Epoch 1320, val loss: 0.4634840488433838
Epoch 1330, training loss: 421.4239807128906 = 0.38948753476142883 + 50.0 * 8.420689582824707
Epoch 1330, val loss: 0.4630349576473236
Epoch 1340, training loss: 421.1667175292969 = 0.3885452151298523 + 50.0 * 8.415563583374023
Epoch 1340, val loss: 0.4626351296901703
Epoch 1350, training loss: 421.1038818359375 = 0.3876360356807709 + 50.0 * 8.414324760437012
Epoch 1350, val loss: 0.4621926248073578
Epoch 1360, training loss: 421.05706787109375 = 0.3867410719394684 + 50.0 * 8.413406372070312
Epoch 1360, val loss: 0.46176114678382874
Epoch 1370, training loss: 421.01788330078125 = 0.38586297631263733 + 50.0 * 8.412640571594238
Epoch 1370, val loss: 0.46136674284935
Epoch 1380, training loss: 420.9985656738281 = 0.3849863111972809 + 50.0 * 8.412271499633789
Epoch 1380, val loss: 0.4609336853027344
Epoch 1390, training loss: 421.1450500488281 = 0.38409915566444397 + 50.0 * 8.4152193069458
Epoch 1390, val loss: 0.460447758436203
Epoch 1400, training loss: 420.9462890625 = 0.3832181692123413 + 50.0 * 8.411261558532715
Epoch 1400, val loss: 0.4601321220397949
Epoch 1410, training loss: 420.9188232421875 = 0.38235098123550415 + 50.0 * 8.41072940826416
Epoch 1410, val loss: 0.4597530663013458
Epoch 1420, training loss: 420.89642333984375 = 0.38149380683898926 + 50.0 * 8.410298347473145
Epoch 1420, val loss: 0.45935478806495667
Epoch 1430, training loss: 421.1127014160156 = 0.3806399703025818 + 50.0 * 8.414641380310059
Epoch 1430, val loss: 0.45898371934890747
Epoch 1440, training loss: 420.9285888671875 = 0.3797723352909088 + 50.0 * 8.41097640991211
Epoch 1440, val loss: 0.45851489901542664
Epoch 1450, training loss: 420.8317565917969 = 0.3789244294166565 + 50.0 * 8.409056663513184
Epoch 1450, val loss: 0.45817750692367554
Epoch 1460, training loss: 420.78375244140625 = 0.37809908390045166 + 50.0 * 8.408112525939941
Epoch 1460, val loss: 0.4577637314796448
Epoch 1470, training loss: 420.7554626464844 = 0.37728357315063477 + 50.0 * 8.407563209533691
Epoch 1470, val loss: 0.457428902387619
Epoch 1480, training loss: 420.7891845703125 = 0.3764731287956238 + 50.0 * 8.408254623413086
Epoch 1480, val loss: 0.4570707678794861
Epoch 1490, training loss: 420.7190246582031 = 0.3756444752216339 + 50.0 * 8.406867980957031
Epoch 1490, val loss: 0.4566534459590912
Epoch 1500, training loss: 420.69195556640625 = 0.3748244047164917 + 50.0 * 8.406342506408691
Epoch 1500, val loss: 0.45627689361572266
Epoch 1510, training loss: 420.659423828125 = 0.3740272521972656 + 50.0 * 8.405708312988281
Epoch 1510, val loss: 0.45591938495635986
Epoch 1520, training loss: 420.6336669921875 = 0.37323620915412903 + 50.0 * 8.405208587646484
Epoch 1520, val loss: 0.45559123158454895
Epoch 1530, training loss: 420.6448669433594 = 0.3724555969238281 + 50.0 * 8.405447959899902
Epoch 1530, val loss: 0.45525068044662476
Epoch 1540, training loss: 420.7895812988281 = 0.3716496527194977 + 50.0 * 8.408358573913574
Epoch 1540, val loss: 0.45476439595222473
Epoch 1550, training loss: 420.66455078125 = 0.3708396553993225 + 50.0 * 8.405874252319336
Epoch 1550, val loss: 0.45455873012542725
Epoch 1560, training loss: 420.5730285644531 = 0.3700503408908844 + 50.0 * 8.404059410095215
Epoch 1560, val loss: 0.4541500508785248
Epoch 1570, training loss: 420.5229187011719 = 0.36928591132164 + 50.0 * 8.403072357177734
Epoch 1570, val loss: 0.45382261276245117
Epoch 1580, training loss: 420.498291015625 = 0.3685268461704254 + 50.0 * 8.402595520019531
Epoch 1580, val loss: 0.45348742604255676
Epoch 1590, training loss: 420.47662353515625 = 0.3677730858325958 + 50.0 * 8.402176856994629
Epoch 1590, val loss: 0.4531579613685608
Epoch 1600, training loss: 420.4580078125 = 0.3670159876346588 + 50.0 * 8.401820182800293
Epoch 1600, val loss: 0.4528321921825409
Epoch 1610, training loss: 420.550048828125 = 0.3662549555301666 + 50.0 * 8.40367603302002
Epoch 1610, val loss: 0.45243436098098755
Epoch 1620, training loss: 420.4313659667969 = 0.36547794938087463 + 50.0 * 8.401317596435547
Epoch 1620, val loss: 0.45225098729133606
Epoch 1630, training loss: 420.43402099609375 = 0.3647020161151886 + 50.0 * 8.401386260986328
Epoch 1630, val loss: 0.4518286883831024
Epoch 1640, training loss: 420.41265869140625 = 0.363951712846756 + 50.0 * 8.40097427368164
Epoch 1640, val loss: 0.45161008834838867
Epoch 1650, training loss: 420.36297607421875 = 0.3631998598575592 + 50.0 * 8.399995803833008
Epoch 1650, val loss: 0.4512120187282562
Epoch 1660, training loss: 420.33935546875 = 0.3624637722969055 + 50.0 * 8.399538040161133
Epoch 1660, val loss: 0.4509558379650116
Epoch 1670, training loss: 420.45849609375 = 0.3617268204689026 + 50.0 * 8.401935577392578
Epoch 1670, val loss: 0.4506457448005676
Epoch 1680, training loss: 420.3271789550781 = 0.36095839738845825 + 50.0 * 8.399324417114258
Epoch 1680, val loss: 0.45024269819259644
Epoch 1690, training loss: 420.31634521484375 = 0.36021703481674194 + 50.0 * 8.39912223815918
Epoch 1690, val loss: 0.4499889016151428
Epoch 1700, training loss: 420.2724304199219 = 0.3594787120819092 + 50.0 * 8.398259162902832
Epoch 1700, val loss: 0.4496445059776306
Epoch 1710, training loss: 420.2464599609375 = 0.3587576746940613 + 50.0 * 8.397753715515137
Epoch 1710, val loss: 0.44937554001808167
Epoch 1720, training loss: 420.29705810546875 = 0.3580341935157776 + 50.0 * 8.398780822753906
Epoch 1720, val loss: 0.44903650879859924
Epoch 1730, training loss: 420.2037353515625 = 0.3573041260242462 + 50.0 * 8.396928787231445
Epoch 1730, val loss: 0.44875895977020264
Epoch 1740, training loss: 420.1883544921875 = 0.3565852642059326 + 50.0 * 8.396635055541992
Epoch 1740, val loss: 0.44852012395858765
Epoch 1750, training loss: 420.1779479980469 = 0.3558678925037384 + 50.0 * 8.396441459655762
Epoch 1750, val loss: 0.4481840133666992
Epoch 1760, training loss: 420.33306884765625 = 0.3551509976387024 + 50.0 * 8.399558067321777
Epoch 1760, val loss: 0.4478452503681183
Epoch 1770, training loss: 420.1825256347656 = 0.35442280769348145 + 50.0 * 8.396561622619629
Epoch 1770, val loss: 0.44768980145454407
Epoch 1780, training loss: 420.12860107421875 = 0.35370635986328125 + 50.0 * 8.395498275756836
Epoch 1780, val loss: 0.44733166694641113
Epoch 1790, training loss: 420.0951843261719 = 0.35300567746162415 + 50.0 * 8.394844055175781
Epoch 1790, val loss: 0.44712042808532715
Epoch 1800, training loss: 420.0714111328125 = 0.35230985283851624 + 50.0 * 8.39438247680664
Epoch 1800, val loss: 0.4468369483947754
Epoch 1810, training loss: 420.05413818359375 = 0.3516163229942322 + 50.0 * 8.394050598144531
Epoch 1810, val loss: 0.44659194350242615
Epoch 1820, training loss: 420.1546936035156 = 0.3509182929992676 + 50.0 * 8.396075248718262
Epoch 1820, val loss: 0.44621723890304565
Epoch 1830, training loss: 420.063720703125 = 0.3502020239830017 + 50.0 * 8.394270896911621
Epoch 1830, val loss: 0.44611164927482605
Epoch 1840, training loss: 420.0463562011719 = 0.3494863212108612 + 50.0 * 8.393937110900879
Epoch 1840, val loss: 0.44574642181396484
Epoch 1850, training loss: 420.0923156738281 = 0.3487907946109772 + 50.0 * 8.39487075805664
Epoch 1850, val loss: 0.44553136825561523
Epoch 1860, training loss: 420.0303039550781 = 0.34807920455932617 + 50.0 * 8.393644332885742
Epoch 1860, val loss: 0.44527357816696167
Epoch 1870, training loss: 419.96246337890625 = 0.3473806381225586 + 50.0 * 8.392301559448242
Epoch 1870, val loss: 0.4450457990169525
Epoch 1880, training loss: 419.9458923339844 = 0.3466891944408417 + 50.0 * 8.391983985900879
Epoch 1880, val loss: 0.444802463054657
Epoch 1890, training loss: 419.9360656738281 = 0.34599974751472473 + 50.0 * 8.391800880432129
Epoch 1890, val loss: 0.4445531964302063
Epoch 1900, training loss: 420.0069580078125 = 0.34530776739120483 + 50.0 * 8.393233299255371
Epoch 1900, val loss: 0.4443255662918091
Epoch 1910, training loss: 419.931884765625 = 0.34460386633872986 + 50.0 * 8.391745567321777
Epoch 1910, val loss: 0.4441230595111847
Epoch 1920, training loss: 419.9595947265625 = 0.34390178322792053 + 50.0 * 8.392313957214355
Epoch 1920, val loss: 0.4438707232475281
Epoch 1930, training loss: 419.8797302246094 = 0.3431972563266754 + 50.0 * 8.390730857849121
Epoch 1930, val loss: 0.4435741603374481
Epoch 1940, training loss: 419.8594970703125 = 0.34251129627227783 + 50.0 * 8.390339851379395
Epoch 1940, val loss: 0.44334903359413147
Epoch 1950, training loss: 419.84881591796875 = 0.34182682633399963 + 50.0 * 8.39013957977295
Epoch 1950, val loss: 0.4431374669075012
Epoch 1960, training loss: 419.8872985839844 = 0.3411406874656677 + 50.0 * 8.390923500061035
Epoch 1960, val loss: 0.44287529587745667
Epoch 1970, training loss: 419.89508056640625 = 0.34043648838996887 + 50.0 * 8.391093254089355
Epoch 1970, val loss: 0.442582905292511
Epoch 1980, training loss: 419.8376770019531 = 0.33973029255867004 + 50.0 * 8.389959335327148
Epoch 1980, val loss: 0.4424136281013489
Epoch 1990, training loss: 419.7861022949219 = 0.3390379250049591 + 50.0 * 8.388940811157227
Epoch 1990, val loss: 0.4421636164188385
Epoch 2000, training loss: 419.7754211425781 = 0.3383490741252899 + 50.0 * 8.388741493225098
Epoch 2000, val loss: 0.4419752061367035
Epoch 2010, training loss: 419.81036376953125 = 0.33766627311706543 + 50.0 * 8.389453887939453
Epoch 2010, val loss: 0.44185784459114075
Epoch 2020, training loss: 419.81964111328125 = 0.3369646668434143 + 50.0 * 8.389653205871582
Epoch 2020, val loss: 0.4415695369243622
Epoch 2030, training loss: 419.7369689941406 = 0.33625587821006775 + 50.0 * 8.388014793395996
Epoch 2030, val loss: 0.4413292706012726
Epoch 2040, training loss: 419.7135314941406 = 0.3355697691440582 + 50.0 * 8.387558937072754
Epoch 2040, val loss: 0.4411170482635498
Epoch 2050, training loss: 419.6970520019531 = 0.3348890244960785 + 50.0 * 8.387243270874023
Epoch 2050, val loss: 0.4409381151199341
Epoch 2060, training loss: 419.7161560058594 = 0.33421096205711365 + 50.0 * 8.387639045715332
Epoch 2060, val loss: 0.4407823979854584
Epoch 2070, training loss: 419.90802001953125 = 0.33352354168891907 + 50.0 * 8.39148998260498
Epoch 2070, val loss: 0.44062668085098267
Epoch 2080, training loss: 419.6814270019531 = 0.33280786871910095 + 50.0 * 8.386972427368164
Epoch 2080, val loss: 0.44033047556877136
Epoch 2090, training loss: 419.6728820800781 = 0.3321116268634796 + 50.0 * 8.386815071105957
Epoch 2090, val loss: 0.4400934875011444
Epoch 2100, training loss: 419.63787841796875 = 0.33143436908721924 + 50.0 * 8.386129379272461
Epoch 2100, val loss: 0.43995773792266846
Epoch 2110, training loss: 419.6168518066406 = 0.3307581841945648 + 50.0 * 8.385722160339355
Epoch 2110, val loss: 0.43977707624435425
Epoch 2120, training loss: 419.6052551269531 = 0.3300829827785492 + 50.0 * 8.385503768920898
Epoch 2120, val loss: 0.43958863615989685
Epoch 2130, training loss: 419.61279296875 = 0.32940900325775146 + 50.0 * 8.38566780090332
Epoch 2130, val loss: 0.4393807351589203
Epoch 2140, training loss: 419.8232116699219 = 0.3287287652492523 + 50.0 * 8.38988971710205
Epoch 2140, val loss: 0.4391157329082489
Epoch 2150, training loss: 419.662109375 = 0.32803621888160706 + 50.0 * 8.38668155670166
Epoch 2150, val loss: 0.43915843963623047
Epoch 2160, training loss: 419.5810241699219 = 0.3273438513278961 + 50.0 * 8.3850736618042
Epoch 2160, val loss: 0.43880876898765564
Epoch 2170, training loss: 419.55364990234375 = 0.3266703486442566 + 50.0 * 8.384539604187012
Epoch 2170, val loss: 0.4387330114841461
Epoch 2180, training loss: 419.57305908203125 = 0.3259928822517395 + 50.0 * 8.384941101074219
Epoch 2180, val loss: 0.43853047490119934
Epoch 2190, training loss: 419.6408996582031 = 0.32530471682548523 + 50.0 * 8.386311531066895
Epoch 2190, val loss: 0.4383968710899353
Epoch 2200, training loss: 419.56500244140625 = 0.32461345195770264 + 50.0 * 8.384807586669922
Epoch 2200, val loss: 0.4381617605686188
Epoch 2210, training loss: 419.50067138671875 = 0.3239213824272156 + 50.0 * 8.383535385131836
Epoch 2210, val loss: 0.4380166828632355
Epoch 2220, training loss: 419.48333740234375 = 0.32324016094207764 + 50.0 * 8.383201599121094
Epoch 2220, val loss: 0.4378829002380371
Epoch 2230, training loss: 419.48455810546875 = 0.3225608170032501 + 50.0 * 8.38323974609375
Epoch 2230, val loss: 0.43766626715660095
Epoch 2240, training loss: 419.5878601074219 = 0.3218746483325958 + 50.0 * 8.385319709777832
Epoch 2240, val loss: 0.4374377429485321
Epoch 2250, training loss: 419.485107421875 = 0.32117941975593567 + 50.0 * 8.383278846740723
Epoch 2250, val loss: 0.4374714493751526
Epoch 2260, training loss: 419.4477233886719 = 0.32048293948173523 + 50.0 * 8.38254451751709
Epoch 2260, val loss: 0.43724116683006287
Epoch 2270, training loss: 419.4479675292969 = 0.3197989761829376 + 50.0 * 8.382563591003418
Epoch 2270, val loss: 0.43714359402656555
Epoch 2280, training loss: 419.44683837890625 = 0.3191141188144684 + 50.0 * 8.382554054260254
Epoch 2280, val loss: 0.4369538426399231
Epoch 2290, training loss: 419.47894287109375 = 0.3184314966201782 + 50.0 * 8.383210182189941
Epoch 2290, val loss: 0.436790406703949
Epoch 2300, training loss: 419.4312438964844 = 0.3177286982536316 + 50.0 * 8.382270812988281
Epoch 2300, val loss: 0.43660300970077515
Epoch 2310, training loss: 419.38958740234375 = 0.31703588366508484 + 50.0 * 8.381450653076172
Epoch 2310, val loss: 0.4365968108177185
Epoch 2320, training loss: 419.3480529785156 = 0.31634780764579773 + 50.0 * 8.380634307861328
Epoch 2320, val loss: 0.4363708198070526
Epoch 2330, training loss: 419.34051513671875 = 0.31566643714904785 + 50.0 * 8.380496978759766
Epoch 2330, val loss: 0.4362351894378662
Epoch 2340, training loss: 419.3385925292969 = 0.31498315930366516 + 50.0 * 8.380472183227539
Epoch 2340, val loss: 0.43611058592796326
Epoch 2350, training loss: 419.4721374511719 = 0.3142889142036438 + 50.0 * 8.383156776428223
Epoch 2350, val loss: 0.4358997344970703
Epoch 2360, training loss: 419.4725646972656 = 0.31358078122138977 + 50.0 * 8.383179664611816
Epoch 2360, val loss: 0.43597251176834106
Epoch 2370, training loss: 419.3373718261719 = 0.3128703236579895 + 50.0 * 8.38049030303955
Epoch 2370, val loss: 0.43570512533187866
Epoch 2380, training loss: 419.273681640625 = 0.3121780455112457 + 50.0 * 8.379230499267578
Epoch 2380, val loss: 0.4355657398700714
Epoch 2390, training loss: 419.26953125 = 0.3114936947822571 + 50.0 * 8.37916088104248
Epoch 2390, val loss: 0.43541184067726135
Epoch 2400, training loss: 419.32647705078125 = 0.3108093738555908 + 50.0 * 8.3803129196167
Epoch 2400, val loss: 0.43521636724472046
Epoch 2410, training loss: 419.2995300292969 = 0.3101080358028412 + 50.0 * 8.379788398742676
Epoch 2410, val loss: 0.43515563011169434
Epoch 2420, training loss: 419.2643737792969 = 0.30940717458724976 + 50.0 * 8.379098892211914
Epoch 2420, val loss: 0.43509674072265625
Epoch 2430, training loss: 419.2186584472656 = 0.30871066451072693 + 50.0 * 8.378198623657227
Epoch 2430, val loss: 0.4348587393760681
Epoch 2440, training loss: 419.2049255371094 = 0.3080167770385742 + 50.0 * 8.377938270568848
Epoch 2440, val loss: 0.43475431203842163
Epoch 2450, training loss: 419.3271484375 = 0.30732110142707825 + 50.0 * 8.380396842956543
Epoch 2450, val loss: 0.43462076783180237
Epoch 2460, training loss: 419.2396545410156 = 0.30660906434059143 + 50.0 * 8.378661155700684
Epoch 2460, val loss: 0.4345904588699341
Epoch 2470, training loss: 419.2025146484375 = 0.30589187145233154 + 50.0 * 8.37793254852295
Epoch 2470, val loss: 0.43432965874671936
Epoch 2480, training loss: 419.17535400390625 = 0.3051891624927521 + 50.0 * 8.377403259277344
Epoch 2480, val loss: 0.4342576265335083
Epoch 2490, training loss: 419.1965637207031 = 0.30448904633522034 + 50.0 * 8.37784194946289
Epoch 2490, val loss: 0.43410468101501465
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8325722983257229
0.8632905890023909
The final CL Acc:0.83004, 0.00395, The final GNN Acc:0.86380, 0.00077
Begin epxeriment: noisy_level: 0.2 cont_weight: 50 epoch: der1:0.3 der2:0.3 dfr1:0.4 dfr2:0
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.0, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106404])
remove edge: torch.Size([2, 70822])
updated graph: torch.Size([2, 88578])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 504.52191162109375 = 1.096075177192688 + 50.0 * 10.068516731262207
Epoch 0, val loss: 1.0945086479187012
Epoch 10, training loss: 480.0206604003906 = 1.0928752422332764 + 50.0 * 9.578556060791016
Epoch 10, val loss: 1.091391682624817
Epoch 20, training loss: 471.90435791015625 = 1.0899053812026978 + 50.0 * 9.416289329528809
Epoch 20, val loss: 1.0884308815002441
Epoch 30, training loss: 466.4049377441406 = 1.0869969129562378 + 50.0 * 9.306358337402344
Epoch 30, val loss: 1.085569143295288
Epoch 40, training loss: 462.1214904785156 = 1.0842397212982178 + 50.0 * 9.220745086669922
Epoch 40, val loss: 1.08286714553833
Epoch 50, training loss: 458.64373779296875 = 1.0816497802734375 + 50.0 * 9.151241302490234
Epoch 50, val loss: 1.0803364515304565
Epoch 60, training loss: 455.6463317871094 = 1.0791953802108765 + 50.0 * 9.09134292602539
Epoch 60, val loss: 1.0779500007629395
Epoch 70, training loss: 452.9952087402344 = 1.0768938064575195 + 50.0 * 9.038366317749023
Epoch 70, val loss: 1.075719952583313
Epoch 80, training loss: 450.7023620605469 = 1.074723482131958 + 50.0 * 8.992552757263184
Epoch 80, val loss: 1.0736281871795654
Epoch 90, training loss: 448.72216796875 = 1.0726784467697144 + 50.0 * 8.95298957824707
Epoch 90, val loss: 1.071668028831482
Epoch 100, training loss: 447.02972412109375 = 1.0707670450210571 + 50.0 * 8.91917896270752
Epoch 100, val loss: 1.0698466300964355
Epoch 110, training loss: 445.55865478515625 = 1.0689846277236938 + 50.0 * 8.889793395996094
Epoch 110, val loss: 1.0681582689285278
Epoch 120, training loss: 444.2147521972656 = 1.0673304796218872 + 50.0 * 8.862948417663574
Epoch 120, val loss: 1.066602110862732
Epoch 130, training loss: 443.0422058105469 = 1.0657991170883179 + 50.0 * 8.83952808380127
Epoch 130, val loss: 1.065171480178833
Epoch 140, training loss: 441.986083984375 = 1.0643932819366455 + 50.0 * 8.81843376159668
Epoch 140, val loss: 1.0638718605041504
Epoch 150, training loss: 441.0205383300781 = 1.0631158351898193 + 50.0 * 8.799148559570312
Epoch 150, val loss: 1.0626978874206543
Epoch 160, training loss: 440.24786376953125 = 1.061958909034729 + 50.0 * 8.78371810913086
Epoch 160, val loss: 1.061644434928894
Epoch 170, training loss: 439.4806823730469 = 1.0609227418899536 + 50.0 * 8.76839542388916
Epoch 170, val loss: 1.0607081651687622
Epoch 180, training loss: 438.8217468261719 = 1.0600069761276245 + 50.0 * 8.755234718322754
Epoch 180, val loss: 1.059888243675232
Epoch 190, training loss: 438.33709716796875 = 1.0591987371444702 + 50.0 * 8.74555778503418
Epoch 190, val loss: 1.0591779947280884
Epoch 200, training loss: 437.79150390625 = 1.0584936141967773 + 50.0 * 8.734660148620605
Epoch 200, val loss: 1.0585579872131348
Epoch 210, training loss: 437.32818603515625 = 1.0579026937484741 + 50.0 * 8.7254056930542
Epoch 210, val loss: 1.05804443359375
Epoch 220, training loss: 436.95819091796875 = 1.057393193244934 + 50.0 * 8.718015670776367
Epoch 220, val loss: 1.057606816291809
Epoch 230, training loss: 436.6462097167969 = 1.0569666624069214 + 50.0 * 8.711784362792969
Epoch 230, val loss: 1.0572500228881836
Epoch 240, training loss: 436.3988342285156 = 1.056614637374878 + 50.0 * 8.706844329833984
Epoch 240, val loss: 1.056959629058838
Epoch 250, training loss: 436.0598449707031 = 1.0563244819641113 + 50.0 * 8.70007038116455
Epoch 250, val loss: 1.0567268133163452
Epoch 260, training loss: 435.7738952636719 = 1.0560885667800903 + 50.0 * 8.694355964660645
Epoch 260, val loss: 1.0565409660339355
Epoch 270, training loss: 435.50701904296875 = 1.0558888912200928 + 50.0 * 8.6890230178833
Epoch 270, val loss: 1.0563912391662598
Epoch 280, training loss: 435.5337829589844 = 1.0557332038879395 + 50.0 * 8.689560890197754
Epoch 280, val loss: 1.056281566619873
Epoch 290, training loss: 435.2708435058594 = 1.0555846691131592 + 50.0 * 8.684305191040039
Epoch 290, val loss: 1.0561556816101074
Epoch 300, training loss: 435.4300231933594 = 1.0555135011672974 + 50.0 * 8.687490463256836
Epoch 300, val loss: 1.056117057800293
Epoch 310, training loss: 435.00335693359375 = 1.0554307699203491 + 50.0 * 8.678958892822266
Epoch 310, val loss: 1.0560650825500488
Epoch 320, training loss: 434.9176025390625 = 1.0553568601608276 + 50.0 * 8.677245140075684
Epoch 320, val loss: 1.056015133857727
Epoch 330, training loss: 434.8778991699219 = 1.055328607559204 + 50.0 * 8.676451683044434
Epoch 330, val loss: 1.0560040473937988
Epoch 340, training loss: 434.7042236328125 = 1.0552854537963867 + 50.0 * 8.672978401184082
Epoch 340, val loss: 1.0559825897216797
Epoch 350, training loss: 434.7256774902344 = 1.0552659034729004 + 50.0 * 8.673408508300781
Epoch 350, val loss: 1.0559784173965454
Epoch 360, training loss: 434.6749267578125 = 1.0552351474761963 + 50.0 * 8.672393798828125
Epoch 360, val loss: 1.0559537410736084
Epoch 370, training loss: 434.2814025878906 = 1.0551319122314453 + 50.0 * 8.664525032043457
Epoch 370, val loss: 1.055875301361084
Epoch 380, training loss: 434.6497802734375 = 1.0551698207855225 + 50.0 * 8.671892166137695
Epoch 380, val loss: 1.055899739265442
Epoch 390, training loss: 434.6311950683594 = 1.0551602840423584 + 50.0 * 8.671521186828613
Epoch 390, val loss: 1.0559074878692627
Epoch 400, training loss: 434.5640563964844 = 1.0551389455795288 + 50.0 * 8.670178413391113
Epoch 400, val loss: 1.0558977127075195
Epoch 410, training loss: 434.64666748046875 = 1.0551310777664185 + 50.0 * 8.671830177307129
Epoch 410, val loss: 1.0558950901031494
Epoch 420, training loss: 434.7756042480469 = 1.0551148653030396 + 50.0 * 8.674409866333008
Epoch 420, val loss: 1.0558849573135376
Epoch 430, training loss: 434.7242126464844 = 1.0550870895385742 + 50.0 * 8.673382759094238
Epoch 430, val loss: 1.0558644533157349
Epoch 440, training loss: 434.765625 = 1.055076241493225 + 50.0 * 8.674210548400879
Epoch 440, val loss: 1.0558489561080933
Epoch 450, training loss: 434.9653625488281 = 1.0550637245178223 + 50.0 * 8.678206443786621
Epoch 450, val loss: 1.0558419227600098
Epoch 460, training loss: 434.8281555175781 = 1.0550165176391602 + 50.0 * 8.67546272277832
Epoch 460, val loss: 1.0558089017868042
Epoch 470, training loss: 434.9057312011719 = 1.055018663406372 + 50.0 * 8.677014350891113
Epoch 470, val loss: 1.055806040763855
Epoch 480, training loss: 434.9461975097656 = 1.0550053119659424 + 50.0 * 8.677824020385742
Epoch 480, val loss: 1.05579674243927
Epoch 490, training loss: 435.0581970214844 = 1.0549895763397217 + 50.0 * 8.68006420135498
Epoch 490, val loss: 1.0557873249053955
Epoch 500, training loss: 435.120361328125 = 1.0549710988998413 + 50.0 * 8.681307792663574
Epoch 500, val loss: 1.0557717084884644
Epoch 510, training loss: 435.1409606933594 = 1.0549556016921997 + 50.0 * 8.681719779968262
Epoch 510, val loss: 1.0557589530944824
Epoch 520, training loss: 435.3111572265625 = 1.0549445152282715 + 50.0 * 8.685124397277832
Epoch 520, val loss: 1.0557503700256348
Epoch 530, training loss: 435.38519287109375 = 1.0549278259277344 + 50.0 * 8.686605453491211
Epoch 530, val loss: 1.055734395980835
Epoch 540, training loss: 435.488037109375 = 1.0549112558364868 + 50.0 * 8.6886625289917
Epoch 540, val loss: 1.055716872215271
Epoch 550, training loss: 435.3706970214844 = 1.054837703704834 + 50.0 * 8.686317443847656
Epoch 550, val loss: 1.0556405782699585
Epoch 560, training loss: 435.4590759277344 = 1.0548464059829712 + 50.0 * 8.688084602355957
Epoch 560, val loss: 1.0556399822235107
Epoch 570, training loss: 435.4755554199219 = 1.0548433065414429 + 50.0 * 8.688414573669434
Epoch 570, val loss: 1.0556590557098389
Epoch 580, training loss: 435.4941101074219 = 1.0548306703567505 + 50.0 * 8.688785552978516
Epoch 580, val loss: 1.0556483268737793
Epoch 590, training loss: 435.71002197265625 = 1.0547882318496704 + 50.0 * 8.69310474395752
Epoch 590, val loss: 1.0556193590164185
Epoch 600, training loss: 435.60888671875 = 1.054782748222351 + 50.0 * 8.691082000732422
Epoch 600, val loss: 1.05560302734375
Epoch 610, training loss: 435.78460693359375 = 1.0547822713851929 + 50.0 * 8.694596290588379
Epoch 610, val loss: 1.055606722831726
Epoch 620, training loss: 435.9442443847656 = 1.0547723770141602 + 50.0 * 8.697789192199707
Epoch 620, val loss: 1.055590271949768
Epoch 630, training loss: 435.88946533203125 = 1.0547422170639038 + 50.0 * 8.696694374084473
Epoch 630, val loss: 1.0555665493011475
Epoch 640, training loss: 436.0105895996094 = 1.0547246932983398 + 50.0 * 8.699117660522461
Epoch 640, val loss: 1.055549144744873
Epoch 650, training loss: 436.02471923828125 = 1.0547046661376953 + 50.0 * 8.699399948120117
Epoch 650, val loss: 1.055534839630127
Epoch 660, training loss: 436.2176513671875 = 1.0546932220458984 + 50.0 * 8.703259468078613
Epoch 660, val loss: 1.0555249452590942
Epoch 670, training loss: 436.308837890625 = 1.0546735525131226 + 50.0 * 8.705082893371582
Epoch 670, val loss: 1.055509328842163
Epoch 680, training loss: 436.4272155761719 = 1.0546600818634033 + 50.0 * 8.707450866699219
Epoch 680, val loss: 1.0554933547973633
Epoch 690, training loss: 436.35845947265625 = 1.0546313524246216 + 50.0 * 8.706076622009277
Epoch 690, val loss: 1.0554728507995605
Epoch 700, training loss: 436.3951416015625 = 1.054602861404419 + 50.0 * 8.70681095123291
Epoch 700, val loss: 1.0554475784301758
Epoch 710, training loss: 436.5384216308594 = 1.0545943975448608 + 50.0 * 8.709676742553711
Epoch 710, val loss: 1.0554475784301758
Epoch 720, training loss: 436.67205810546875 = 1.0545581579208374 + 50.0 * 8.712349891662598
Epoch 720, val loss: 1.055415391921997
Epoch 730, training loss: 436.7868347167969 = 1.054563045501709 + 50.0 * 8.714645385742188
Epoch 730, val loss: 1.055408239364624
Epoch 740, training loss: 436.6887512207031 = 1.0545233488082886 + 50.0 * 8.712684631347656
Epoch 740, val loss: 1.0553754568099976
Epoch 750, training loss: 436.7079162597656 = 1.0544995069503784 + 50.0 * 8.713068008422852
Epoch 750, val loss: 1.0553622245788574
Epoch 760, training loss: 436.87255859375 = 1.0544993877410889 + 50.0 * 8.716361045837402
Epoch 760, val loss: 1.0553522109985352
Epoch 770, training loss: 436.4482116699219 = 1.0543599128723145 + 50.0 * 8.707877159118652
Epoch 770, val loss: 1.0552486181259155
Epoch 780, training loss: 436.7496643066406 = 1.054417610168457 + 50.0 * 8.713905334472656
Epoch 780, val loss: 1.055279016494751
Epoch 790, training loss: 436.7538146972656 = 1.054402232170105 + 50.0 * 8.713988304138184
Epoch 790, val loss: 1.0552600622177124
Epoch 800, training loss: 436.947998046875 = 1.0544039011001587 + 50.0 * 8.71787166595459
Epoch 800, val loss: 1.0552769899368286
Epoch 810, training loss: 437.1783752441406 = 1.0543957948684692 + 50.0 * 8.722479820251465
Epoch 810, val loss: 1.0552672147750854
Epoch 820, training loss: 437.2049255371094 = 1.0543687343597412 + 50.0 * 8.723011016845703
Epoch 820, val loss: 1.0552374124526978
Epoch 830, training loss: 437.29150390625 = 1.0543491840362549 + 50.0 * 8.724742889404297
Epoch 830, val loss: 1.055227518081665
Epoch 840, training loss: 437.32666015625 = 1.0543190240859985 + 50.0 * 8.725446701049805
Epoch 840, val loss: 1.0551975965499878
Epoch 850, training loss: 437.4523620605469 = 1.0542916059494019 + 50.0 * 8.727961540222168
Epoch 850, val loss: 1.0551787614822388
Epoch 860, training loss: 437.46038818359375 = 1.0542681217193604 + 50.0 * 8.72812271118164
Epoch 860, val loss: 1.0551596879959106
Epoch 870, training loss: 437.5229187011719 = 1.0542446374893188 + 50.0 * 8.729373931884766
Epoch 870, val loss: 1.055147409439087
Epoch 880, training loss: 437.7571105957031 = 1.054235816001892 + 50.0 * 8.734057426452637
Epoch 880, val loss: 1.0551315546035767
Epoch 890, training loss: 437.7268981933594 = 1.0541971921920776 + 50.0 * 8.733453750610352
Epoch 890, val loss: 1.0551016330718994
Epoch 900, training loss: 437.79046630859375 = 1.0541614294052124 + 50.0 * 8.734725952148438
Epoch 900, val loss: 1.0550705194473267
Epoch 910, training loss: 438.0041198730469 = 1.0541365146636963 + 50.0 * 8.738999366760254
Epoch 910, val loss: 1.0550466775894165
Epoch 920, training loss: 438.00469970703125 = 1.054122805595398 + 50.0 * 8.739011764526367
Epoch 920, val loss: 1.0550326108932495
Epoch 930, training loss: 438.2947998046875 = 1.054111361503601 + 50.0 * 8.744813919067383
Epoch 930, val loss: 1.0550159215927124
Epoch 940, training loss: 438.19696044921875 = 1.054074764251709 + 50.0 * 8.742857933044434
Epoch 940, val loss: 1.0549851655960083
Epoch 950, training loss: 438.06976318359375 = 1.0539640188217163 + 50.0 * 8.740316390991211
Epoch 950, val loss: 1.0548949241638184
Epoch 960, training loss: 437.7825622558594 = 1.053950309753418 + 50.0 * 8.734572410583496
Epoch 960, val loss: 1.054857850074768
Epoch 970, training loss: 438.0824279785156 = 1.0539381504058838 + 50.0 * 8.740570068359375
Epoch 970, val loss: 1.0548597574234009
Epoch 980, training loss: 438.08685302734375 = 1.0539249181747437 + 50.0 * 8.7406587600708
Epoch 980, val loss: 1.0548375844955444
Epoch 990, training loss: 438.4078063964844 = 1.0539132356643677 + 50.0 * 8.747077941894531
Epoch 990, val loss: 1.0548423528671265
Epoch 1000, training loss: 438.63568115234375 = 1.0539079904556274 + 50.0 * 8.751635551452637
Epoch 1000, val loss: 1.0548378229141235
Epoch 1010, training loss: 438.75750732421875 = 1.0538941621780396 + 50.0 * 8.754072189331055
Epoch 1010, val loss: 1.0548391342163086
Epoch 1020, training loss: 438.9242858886719 = 1.0538709163665771 + 50.0 * 8.757408142089844
Epoch 1020, val loss: 1.0548135042190552
Epoch 1030, training loss: 438.5419616699219 = 1.053809404373169 + 50.0 * 8.749763488769531
Epoch 1030, val loss: 1.0547583103179932
Epoch 1040, training loss: 438.7869873046875 = 1.0537970066070557 + 50.0 * 8.754663467407227
Epoch 1040, val loss: 1.0547534227371216
Epoch 1050, training loss: 439.1051025390625 = 1.053776502609253 + 50.0 * 8.761026382446289
Epoch 1050, val loss: 1.0547332763671875
Epoch 1060, training loss: 439.2025146484375 = 1.0537569522857666 + 50.0 * 8.762974739074707
Epoch 1060, val loss: 1.0547131299972534
Epoch 1070, training loss: 440.015625 = 1.053470492362976 + 50.0 * 8.779243469238281
Epoch 1070, val loss: 1.0543605089187622
Epoch 1080, training loss: 439.3014221191406 = 1.0535905361175537 + 50.0 * 8.7649564743042
Epoch 1080, val loss: 1.0546149015426636
Epoch 1090, training loss: 438.16900634765625 = 1.0534414052963257 + 50.0 * 8.742311477661133
Epoch 1090, val loss: 1.0543925762176514
Epoch 1100, training loss: 439.29736328125 = 1.0535986423492432 + 50.0 * 8.764875411987305
Epoch 1100, val loss: 1.0545440912246704
Epoch 1110, training loss: 439.26416015625 = 1.0535762310028076 + 50.0 * 8.764211654663086
Epoch 1110, val loss: 1.0545486211776733
Epoch 1120, training loss: 439.52093505859375 = 1.0535838603973389 + 50.0 * 8.769347190856934
Epoch 1120, val loss: 1.0545481443405151
Epoch 1130, training loss: 440.0695495605469 = 1.0535964965820312 + 50.0 * 8.780319213867188
Epoch 1130, val loss: 1.054559350013733
Epoch 1140, training loss: 440.1098937988281 = 1.0535638332366943 + 50.0 * 8.781126976013184
Epoch 1140, val loss: 1.0545308589935303
Epoch 1150, training loss: 440.3600158691406 = 1.0535457134246826 + 50.0 * 8.786128997802734
Epoch 1150, val loss: 1.0545141696929932
Epoch 1160, training loss: 440.47784423828125 = 1.053518295288086 + 50.0 * 8.78848648071289
Epoch 1160, val loss: 1.0544871091842651
Epoch 1170, training loss: 440.60552978515625 = 1.0534851551055908 + 50.0 * 8.791040420532227
Epoch 1170, val loss: 1.0544506311416626
Epoch 1180, training loss: 440.6128845214844 = 1.0534507036209106 + 50.0 * 8.791189193725586
Epoch 1180, val loss: 1.054417371749878
Epoch 1190, training loss: 440.602294921875 = 1.0534108877182007 + 50.0 * 8.790977478027344
Epoch 1190, val loss: 1.054381012916565
Epoch 1200, training loss: 440.8761291503906 = 1.0533851385116577 + 50.0 * 8.796455383300781
Epoch 1200, val loss: 1.0543687343597412
Epoch 1210, training loss: 441.15399169921875 = 1.0533721446990967 + 50.0 * 8.80201244354248
Epoch 1210, val loss: 1.0543476343154907
Epoch 1220, training loss: 441.13739013671875 = 1.0533289909362793 + 50.0 * 8.801681518554688
Epoch 1220, val loss: 1.054298758506775
Epoch 1230, training loss: 441.0708923339844 = 1.0532808303833008 + 50.0 * 8.800352096557617
Epoch 1230, val loss: 1.0542232990264893
Epoch 1240, training loss: 441.0823974609375 = 1.053100347518921 + 50.0 * 8.800585746765137
Epoch 1240, val loss: 1.0541390180587769
Epoch 1250, training loss: 441.1506652832031 = 1.053102731704712 + 50.0 * 8.80195140838623
Epoch 1250, val loss: 1.0541101694107056
Epoch 1260, training loss: 441.41876220703125 = 1.0530749559402466 + 50.0 * 8.807313919067383
Epoch 1260, val loss: 1.054090142250061
Epoch 1270, training loss: 441.8641662597656 = 1.0530961751937866 + 50.0 * 8.816221237182617
Epoch 1270, val loss: 1.0541026592254639
Epoch 1280, training loss: 442.1893310546875 = 1.053104043006897 + 50.0 * 8.822724342346191
Epoch 1280, val loss: 1.054115653038025
Epoch 1290, training loss: 442.5350341796875 = 1.053098201751709 + 50.0 * 8.829638481140137
Epoch 1290, val loss: 1.0541142225265503
Epoch 1300, training loss: 442.5951232910156 = 1.0530619621276855 + 50.0 * 8.830841064453125
Epoch 1300, val loss: 1.0540751218795776
Epoch 1310, training loss: 442.756591796875 = 1.0530411005020142 + 50.0 * 8.834071159362793
Epoch 1310, val loss: 1.0540499687194824
Epoch 1320, training loss: 442.8694763183594 = 1.0530116558074951 + 50.0 * 8.836329460144043
Epoch 1320, val loss: 1.0540289878845215
Epoch 1330, training loss: 443.0342102050781 = 1.0529841184616089 + 50.0 * 8.839624404907227
Epoch 1330, val loss: 1.0539886951446533
Epoch 1340, training loss: 443.1645812988281 = 1.052939534187317 + 50.0 * 8.842232704162598
Epoch 1340, val loss: 1.0539673566818237
Epoch 1350, training loss: 443.2704162597656 = 1.0528947114944458 + 50.0 * 8.844350814819336
Epoch 1350, val loss: 1.0539274215698242
Epoch 1360, training loss: 443.4479675292969 = 1.0528720617294312 + 50.0 * 8.847901344299316
Epoch 1360, val loss: 1.0538946390151978
Epoch 1370, training loss: 443.511474609375 = 1.05284583568573 + 50.0 * 8.849172592163086
Epoch 1370, val loss: 1.0538709163665771
Epoch 1380, training loss: 443.3614196777344 = 1.0527849197387695 + 50.0 * 8.846172332763672
Epoch 1380, val loss: 1.0538381338119507
Epoch 1390, training loss: 443.5591735839844 = 1.0527585744857788 + 50.0 * 8.850128173828125
Epoch 1390, val loss: 1.053794264793396
Epoch 1400, training loss: 443.6814270019531 = 1.0527105331420898 + 50.0 * 8.852574348449707
Epoch 1400, val loss: 1.0537619590759277
Epoch 1410, training loss: 443.6409606933594 = 1.0526469945907593 + 50.0 * 8.851766586303711
Epoch 1410, val loss: 1.0537033081054688
Epoch 1420, training loss: 443.77850341796875 = 1.052639365196228 + 50.0 * 8.854516983032227
Epoch 1420, val loss: 1.0537010431289673
Epoch 1430, training loss: 443.9209289550781 = 1.052599549293518 + 50.0 * 8.857366561889648
Epoch 1430, val loss: 1.0536601543426514
Epoch 1440, training loss: 443.6126708984375 = 1.0524826049804688 + 50.0 * 8.851203918457031
Epoch 1440, val loss: 1.053572177886963
Epoch 1450, training loss: 445.4844665527344 = 1.052051305770874 + 50.0 * 8.88864803314209
Epoch 1450, val loss: 1.0527966022491455
Epoch 1460, training loss: 439.59527587890625 = 1.0512312650680542 + 50.0 * 8.770880699157715
Epoch 1460, val loss: 1.0525622367858887
Epoch 1470, training loss: 443.607177734375 = 1.0522116422653198 + 50.0 * 8.851099014282227
Epoch 1470, val loss: 1.0531996488571167
Epoch 1480, training loss: 440.9215393066406 = 1.05186128616333 + 50.0 * 8.797393798828125
Epoch 1480, val loss: 1.0530040264129639
Epoch 1490, training loss: 441.74652099609375 = 1.0520068407058716 + 50.0 * 8.81389045715332
Epoch 1490, val loss: 1.0530701875686646
Epoch 1500, training loss: 442.35186767578125 = 1.0520823001861572 + 50.0 * 8.825995445251465
Epoch 1500, val loss: 1.0531920194625854
Epoch 1510, training loss: 442.7486267089844 = 1.0521094799041748 + 50.0 * 8.833930015563965
Epoch 1510, val loss: 1.053226113319397
Epoch 1520, training loss: 443.0592041015625 = 1.052135705947876 + 50.0 * 8.840141296386719
Epoch 1520, val loss: 1.053244948387146
Epoch 1530, training loss: 443.50390625 = 1.0521490573883057 + 50.0 * 8.849035263061523
Epoch 1530, val loss: 1.0532509088516235
Epoch 1540, training loss: 443.66644287109375 = 1.0521268844604492 + 50.0 * 8.852286338806152
Epoch 1540, val loss: 1.0532306432724
Epoch 1550, training loss: 443.8212890625 = 1.0520936250686646 + 50.0 * 8.85538387298584
Epoch 1550, val loss: 1.053204894065857
Epoch 1560, training loss: 444.0125732421875 = 1.052061676979065 + 50.0 * 8.859210014343262
Epoch 1560, val loss: 1.0531699657440186
Epoch 1570, training loss: 444.1759033203125 = 1.0520367622375488 + 50.0 * 8.86247730255127
Epoch 1570, val loss: 1.053153395652771
Epoch 1580, training loss: 444.1842346191406 = 1.051979899406433 + 50.0 * 8.862645149230957
Epoch 1580, val loss: 1.0531076192855835
Epoch 1590, training loss: 444.3426513671875 = 1.0519380569458008 + 50.0 * 8.865814208984375
Epoch 1590, val loss: 1.0530611276626587
Epoch 1600, training loss: 444.39178466796875 = 1.0518985986709595 + 50.0 * 8.86679744720459
Epoch 1600, val loss: 1.0530186891555786
Epoch 1610, training loss: 444.5362854003906 = 1.0518739223480225 + 50.0 * 8.869688034057617
Epoch 1610, val loss: 1.053003191947937
Epoch 1620, training loss: 444.6678771972656 = 1.0518192052841187 + 50.0 * 8.872321128845215
Epoch 1620, val loss: 1.0529487133026123
Epoch 1630, training loss: 444.78533935546875 = 1.0517877340316772 + 50.0 * 8.87467098236084
Epoch 1630, val loss: 1.052927851676941
Epoch 1640, training loss: 444.663818359375 = 1.0517158508300781 + 50.0 * 8.872241973876953
Epoch 1640, val loss: 1.0528512001037598
Epoch 1650, training loss: 444.7796630859375 = 1.051681637763977 + 50.0 * 8.87455940246582
Epoch 1650, val loss: 1.0528210401535034
Epoch 1660, training loss: 445.0849609375 = 1.0516599416732788 + 50.0 * 8.88066577911377
Epoch 1660, val loss: 1.052799940109253
Epoch 1670, training loss: 444.94586181640625 = 1.0515803098678589 + 50.0 * 8.877885818481445
Epoch 1670, val loss: 1.0527400970458984
Epoch 1680, training loss: 445.2205505371094 = 1.0515458583831787 + 50.0 * 8.883379936218262
Epoch 1680, val loss: 1.0527162551879883
Epoch 1690, training loss: 445.31427001953125 = 1.0515146255493164 + 50.0 * 8.885254859924316
Epoch 1690, val loss: 1.0526877641677856
Epoch 1700, training loss: 445.42987060546875 = 1.0514625310897827 + 50.0 * 8.887568473815918
Epoch 1700, val loss: 1.052636742591858
Epoch 1710, training loss: 445.5220947265625 = 1.0514044761657715 + 50.0 * 8.889413833618164
Epoch 1710, val loss: 1.0525842905044556
Epoch 1720, training loss: 445.6976318359375 = 1.0513739585876465 + 50.0 * 8.892925262451172
Epoch 1720, val loss: 1.0525411367416382
Epoch 1730, training loss: 445.5836486816406 = 1.0513112545013428 + 50.0 * 8.890646934509277
Epoch 1730, val loss: 1.052494764328003
Epoch 1740, training loss: 445.6573791503906 = 1.0512558221817017 + 50.0 * 8.892122268676758
Epoch 1740, val loss: 1.0524473190307617
Epoch 1750, training loss: 445.8874816894531 = 1.0512162446975708 + 50.0 * 8.89672565460205
Epoch 1750, val loss: 1.0524169206619263
Epoch 1760, training loss: 445.5246276855469 = 1.0511225461959839 + 50.0 * 8.889470100402832
Epoch 1760, val loss: 1.05234956741333
Epoch 1770, training loss: 445.880615234375 = 1.051107406616211 + 50.0 * 8.896590232849121
Epoch 1770, val loss: 1.0523245334625244
Epoch 1780, training loss: 445.7188415527344 = 1.0510237216949463 + 50.0 * 8.893356323242188
Epoch 1780, val loss: 1.052233338356018
Epoch 1790, training loss: 445.6407165527344 = 1.0509564876556396 + 50.0 * 8.89179515838623
Epoch 1790, val loss: 1.0521754026412964
Epoch 1800, training loss: 445.7633972167969 = 1.0509059429168701 + 50.0 * 8.89424991607666
Epoch 1800, val loss: 1.0521413087844849
Epoch 1810, training loss: 445.9700012207031 = 1.0508966445922852 + 50.0 * 8.898382186889648
Epoch 1810, val loss: 1.0521137714385986
Epoch 1820, training loss: 446.1619567871094 = 1.0508663654327393 + 50.0 * 8.9022216796875
Epoch 1820, val loss: 1.0520927906036377
Epoch 1830, training loss: 446.0660095214844 = 1.0507991313934326 + 50.0 * 8.900303840637207
Epoch 1830, val loss: 1.0520304441452026
Epoch 1840, training loss: 446.3638916015625 = 1.050768494606018 + 50.0 * 8.906262397766113
Epoch 1840, val loss: 1.0520012378692627
Epoch 1850, training loss: 446.356201171875 = 1.0507110357284546 + 50.0 * 8.906109809875488
Epoch 1850, val loss: 1.0519475936889648
Epoch 1860, training loss: 446.49029541015625 = 1.050653338432312 + 50.0 * 8.908792495727539
Epoch 1860, val loss: 1.0519025325775146
Epoch 1870, training loss: 446.5905456542969 = 1.0505841970443726 + 50.0 * 8.910799026489258
Epoch 1870, val loss: 1.0518529415130615
Epoch 1880, training loss: 446.6173400878906 = 1.0505391359329224 + 50.0 * 8.911335945129395
Epoch 1880, val loss: 1.0517932176589966
Epoch 1890, training loss: 446.7089538574219 = 1.0505033731460571 + 50.0 * 8.913168907165527
Epoch 1890, val loss: 1.0517737865447998
Epoch 1900, training loss: 446.8975524902344 = 1.0504635572433472 + 50.0 * 8.91694164276123
Epoch 1900, val loss: 1.0517311096191406
Epoch 1910, training loss: 446.74072265625 = 1.0503792762756348 + 50.0 * 8.913806915283203
Epoch 1910, val loss: 1.05165696144104
Epoch 1920, training loss: 446.9149169921875 = 1.0503227710723877 + 50.0 * 8.917291641235352
Epoch 1920, val loss: 1.051613211631775
Epoch 1930, training loss: 447.1936340332031 = 1.050300121307373 + 50.0 * 8.922866821289062
Epoch 1930, val loss: 1.0515824556350708
Epoch 1940, training loss: 447.00750732421875 = 1.0502229928970337 + 50.0 * 8.919145584106445
Epoch 1940, val loss: 1.0515049695968628
Epoch 1950, training loss: 447.0322265625 = 1.0501493215560913 + 50.0 * 8.919641494750977
Epoch 1950, val loss: 1.0514624118804932
Epoch 1960, training loss: 447.2469177246094 = 1.0501238107681274 + 50.0 * 8.923935890197754
Epoch 1960, val loss: 1.0514253377914429
Epoch 1970, training loss: 447.3325500488281 = 1.0500662326812744 + 50.0 * 8.925649642944336
Epoch 1970, val loss: 1.0513696670532227
Epoch 1980, training loss: 447.11431884765625 = 1.0499252080917358 + 50.0 * 8.921287536621094
Epoch 1980, val loss: 1.0512351989746094
Epoch 1990, training loss: 447.24322509765625 = 1.0499207973480225 + 50.0 * 8.923866271972656
Epoch 1990, val loss: 1.0512397289276123
Epoch 2000, training loss: 447.4367980957031 = 1.0498135089874268 + 50.0 * 8.927740097045898
Epoch 2000, val loss: 1.0511406660079956
Epoch 2010, training loss: 447.3030090332031 = 1.0497260093688965 + 50.0 * 8.925065994262695
Epoch 2010, val loss: 1.0510703325271606
Epoch 2020, training loss: 447.56683349609375 = 1.0497158765792847 + 50.0 * 8.930342674255371
Epoch 2020, val loss: 1.051069974899292
Epoch 2030, training loss: 447.63873291015625 = 1.0496782064437866 + 50.0 * 8.931780815124512
Epoch 2030, val loss: 1.0510284900665283
Epoch 2040, training loss: 447.8670349121094 = 1.049645185470581 + 50.0 * 8.936347961425781
Epoch 2040, val loss: 1.0509862899780273
Epoch 2050, training loss: 447.81878662109375 = 1.049554705619812 + 50.0 * 8.935384750366211
Epoch 2050, val loss: 1.0509110689163208
Epoch 2060, training loss: 447.8528747558594 = 1.049505591392517 + 50.0 * 8.936067581176758
Epoch 2060, val loss: 1.0508583784103394
Epoch 2070, training loss: 448.0458679199219 = 1.0494669675827026 + 50.0 * 8.93992805480957
Epoch 2070, val loss: 1.0508352518081665
Epoch 2080, training loss: 448.04315185546875 = 1.049395203590393 + 50.0 * 8.939874649047852
Epoch 2080, val loss: 1.0507539510726929
Epoch 2090, training loss: 447.91192626953125 = 1.0493017435073853 + 50.0 * 8.937252044677734
Epoch 2090, val loss: 1.0506898164749146
Epoch 2100, training loss: 447.6091003417969 = 1.0490155220031738 + 50.0 * 8.931201934814453
Epoch 2100, val loss: 1.0503233671188354
Epoch 2110, training loss: 447.3929748535156 = 1.0489951372146606 + 50.0 * 8.9268798828125
Epoch 2110, val loss: 1.0503733158111572
Epoch 2120, training loss: 446.65283203125 = 1.0487924814224243 + 50.0 * 8.912080764770508
Epoch 2120, val loss: 1.0502574443817139
Epoch 2130, training loss: 447.2135009765625 = 1.0488108396530151 + 50.0 * 8.923294067382812
Epoch 2130, val loss: 1.0502861738204956
Epoch 2140, training loss: 447.74212646484375 = 1.0488663911819458 + 50.0 * 8.933865547180176
Epoch 2140, val loss: 1.050359845161438
Epoch 2150, training loss: 448.06396484375 = 1.048867106437683 + 50.0 * 8.940301895141602
Epoch 2150, val loss: 1.0503679513931274
Epoch 2160, training loss: 448.109375 = 1.0487983226776123 + 50.0 * 8.941211700439453
Epoch 2160, val loss: 1.0502856969833374
Epoch 2170, training loss: 448.2466735839844 = 1.0487581491470337 + 50.0 * 8.943958282470703
Epoch 2170, val loss: 1.0502654314041138
Epoch 2180, training loss: 448.5070495605469 = 1.0487487316131592 + 50.0 * 8.949166297912598
Epoch 2180, val loss: 1.050234079360962
Epoch 2190, training loss: 448.4417724609375 = 1.048653244972229 + 50.0 * 8.94786262512207
Epoch 2190, val loss: 1.0501786470413208
Epoch 2200, training loss: 448.4156188964844 = 1.0485910177230835 + 50.0 * 8.947340965270996
Epoch 2200, val loss: 1.0501091480255127
Epoch 2210, training loss: 448.5640563964844 = 1.0485516786575317 + 50.0 * 8.950309753417969
Epoch 2210, val loss: 1.0500681400299072
Epoch 2220, training loss: 448.5855407714844 = 1.0484654903411865 + 50.0 * 8.9507417678833
Epoch 2220, val loss: 1.0499762296676636
Epoch 2230, training loss: 448.6346435546875 = 1.048393726348877 + 50.0 * 8.951725006103516
Epoch 2230, val loss: 1.0499193668365479
Epoch 2240, training loss: 448.75738525390625 = 1.0483415126800537 + 50.0 * 8.954180717468262
Epoch 2240, val loss: 1.0498913526535034
Epoch 2250, training loss: 448.9219055175781 = 1.048298954963684 + 50.0 * 8.95747184753418
Epoch 2250, val loss: 1.0498336553573608
Epoch 2260, training loss: 448.7377624511719 = 1.0482014417648315 + 50.0 * 8.953791618347168
Epoch 2260, val loss: 1.0497506856918335
Epoch 2270, training loss: 448.5703125 = 1.0479987859725952 + 50.0 * 8.950446128845215
Epoch 2270, val loss: 1.049466848373413
Epoch 2280, training loss: 448.95733642578125 = 1.0480713844299316 + 50.0 * 8.958185195922852
Epoch 2280, val loss: 1.0495685338974
Epoch 2290, training loss: 449.0549621582031 = 1.0480058193206787 + 50.0 * 8.960139274597168
Epoch 2290, val loss: 1.0495728254318237
Epoch 2300, training loss: 449.2491149902344 = 1.0479846000671387 + 50.0 * 8.964022636413574
Epoch 2300, val loss: 1.0495654344558716
Epoch 2310, training loss: 449.46234130859375 = 1.0479729175567627 + 50.0 * 8.968287467956543
Epoch 2310, val loss: 1.0495706796646118
Epoch 2320, training loss: 449.77374267578125 = 1.0479919910430908 + 50.0 * 8.974514961242676
Epoch 2320, val loss: 1.0495799779891968
Epoch 2330, training loss: 449.6026916503906 = 1.047853946685791 + 50.0 * 8.971096992492676
Epoch 2330, val loss: 1.0494557619094849
Epoch 2340, training loss: 449.6724853515625 = 1.04781973361969 + 50.0 * 8.972493171691895
Epoch 2340, val loss: 1.049436330795288
Epoch 2350, training loss: 449.79705810546875 = 1.047792673110962 + 50.0 * 8.974985122680664
Epoch 2350, val loss: 1.049405813217163
Epoch 2360, training loss: 449.8722839355469 = 1.0477216243743896 + 50.0 * 8.97649097442627
Epoch 2360, val loss: 1.0493417978286743
Epoch 2370, training loss: 449.92962646484375 = 1.0476726293563843 + 50.0 * 8.977639198303223
Epoch 2370, val loss: 1.049302339553833
Epoch 2380, training loss: 449.77587890625 = 1.0475497245788574 + 50.0 * 8.974566459655762
Epoch 2380, val loss: 1.0492329597473145
Epoch 2390, training loss: 449.7798767089844 = 1.0474520921707153 + 50.0 * 8.974648475646973
Epoch 2390, val loss: 1.0491265058517456
Epoch 2400, training loss: 450.0278625488281 = 1.0474281311035156 + 50.0 * 8.979608535766602
Epoch 2400, val loss: 1.0490868091583252
Epoch 2410, training loss: 450.29339599609375 = 1.0474259853363037 + 50.0 * 8.984919548034668
Epoch 2410, val loss: 1.0490827560424805
Epoch 2420, training loss: 450.1480407714844 = 1.0472900867462158 + 50.0 * 8.982014656066895
Epoch 2420, val loss: 1.048938274383545
Epoch 2430, training loss: 450.235107421875 = 1.0472184419631958 + 50.0 * 8.983757972717285
Epoch 2430, val loss: 1.0489091873168945
Epoch 2440, training loss: 450.3384094238281 = 1.0471795797348022 + 50.0 * 8.985824584960938
Epoch 2440, val loss: 1.0488665103912354
Epoch 2450, training loss: 450.42218017578125 = 1.0471209287643433 + 50.0 * 8.98750114440918
Epoch 2450, val loss: 1.0488181114196777
Epoch 2460, training loss: 450.30670166015625 = 1.046986699104309 + 50.0 * 8.985194206237793
Epoch 2460, val loss: 1.0487089157104492
Epoch 2470, training loss: 450.4367370605469 = 1.046939730644226 + 50.0 * 8.98779582977295
Epoch 2470, val loss: 1.048663854598999
Epoch 2480, training loss: 450.6030578613281 = 1.0469156503677368 + 50.0 * 8.99112319946289
Epoch 2480, val loss: 1.0486506223678589
Epoch 2490, training loss: 450.61126708984375 = 1.0468411445617676 + 50.0 * 8.991288185119629
Epoch 2490, val loss: 1.048564076423645
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.42666666666666664
0.8138810403535464
=== training gcn model ===
Epoch 0, training loss: 503.2944641113281 = 1.1232959032058716 + 50.0 * 10.043423652648926
Epoch 0, val loss: 1.1228026151657104
Epoch 10, training loss: 480.4909362792969 = 1.1179324388504028 + 50.0 * 9.5874605178833
Epoch 10, val loss: 1.117519497871399
Epoch 20, training loss: 473.5516662597656 = 1.112906813621521 + 50.0 * 9.448775291442871
Epoch 20, val loss: 1.1125751733779907
Epoch 30, training loss: 467.96435546875 = 1.1081408262252808 + 50.0 * 9.33712387084961
Epoch 30, val loss: 1.1078814268112183
Epoch 40, training loss: 463.5851745605469 = 1.103667974472046 + 50.0 * 9.249629974365234
Epoch 40, val loss: 1.1034847497940063
Epoch 50, training loss: 459.8966979980469 = 1.0994725227355957 + 50.0 * 9.175944328308105
Epoch 50, val loss: 1.0993620157241821
Epoch 60, training loss: 456.7469787597656 = 1.0955591201782227 + 50.0 * 9.113028526306152
Epoch 60, val loss: 1.0955153703689575
Epoch 70, training loss: 453.9941101074219 = 1.091894507408142 + 50.0 * 9.05804443359375
Epoch 70, val loss: 1.0919171571731567
Epoch 80, training loss: 451.60162353515625 = 1.088487982749939 + 50.0 * 9.010262489318848
Epoch 80, val loss: 1.0885727405548096
Epoch 90, training loss: 449.51177978515625 = 1.0853071212768555 + 50.0 * 8.96852970123291
Epoch 90, val loss: 1.0854498147964478
Epoch 100, training loss: 447.6702575683594 = 1.0823525190353394 + 50.0 * 8.931757926940918
Epoch 100, val loss: 1.0825538635253906
Epoch 110, training loss: 446.0618896484375 = 1.0796037912368774 + 50.0 * 8.899645805358887
Epoch 110, val loss: 1.0798615217208862
Epoch 120, training loss: 444.59222412109375 = 1.077050805091858 + 50.0 * 8.8703031539917
Epoch 120, val loss: 1.0773605108261108
Epoch 130, training loss: 443.2769470214844 = 1.074703574180603 + 50.0 * 8.84404468536377
Epoch 130, val loss: 1.0750603675842285
Epoch 140, training loss: 442.14251708984375 = 1.0725325345993042 + 50.0 * 8.821399688720703
Epoch 140, val loss: 1.0729354619979858
Epoch 150, training loss: 441.1932678222656 = 1.070542335510254 + 50.0 * 8.802453994750977
Epoch 150, val loss: 1.0709874629974365
Epoch 160, training loss: 440.3005065917969 = 1.0687165260314941 + 50.0 * 8.784635543823242
Epoch 160, val loss: 1.0692006349563599
Epoch 170, training loss: 439.6083679199219 = 1.0670497417449951 + 50.0 * 8.77082633972168
Epoch 170, val loss: 1.0675686597824097
Epoch 180, training loss: 438.9262390136719 = 1.0655487775802612 + 50.0 * 8.757213592529297
Epoch 180, val loss: 1.0660938024520874
Epoch 190, training loss: 438.35369873046875 = 1.0641752481460571 + 50.0 * 8.745790481567383
Epoch 190, val loss: 1.0647579431533813
Epoch 200, training loss: 437.8968200683594 = 1.0629624128341675 + 50.0 * 8.736677169799805
Epoch 200, val loss: 1.0635708570480347
Epoch 210, training loss: 437.5244445800781 = 1.0618653297424316 + 50.0 * 8.729251861572266
Epoch 210, val loss: 1.062498927116394
Epoch 220, training loss: 437.169677734375 = 1.0609010457992554 + 50.0 * 8.722175598144531
Epoch 220, val loss: 1.0615510940551758
Epoch 230, training loss: 436.92547607421875 = 1.060037612915039 + 50.0 * 8.71730899810791
Epoch 230, val loss: 1.0607075691223145
Epoch 240, training loss: 436.78375244140625 = 1.059269666671753 + 50.0 * 8.714489936828613
Epoch 240, val loss: 1.0599628686904907
Epoch 250, training loss: 436.68359375 = 1.0586169958114624 + 50.0 * 8.712499618530273
Epoch 250, val loss: 1.0593020915985107
Epoch 260, training loss: 436.40869140625 = 1.0580590963363647 + 50.0 * 8.707013130187988
Epoch 260, val loss: 1.0587730407714844
Epoch 270, training loss: 436.1258850097656 = 1.0575698614120483 + 50.0 * 8.701366424560547
Epoch 270, val loss: 1.0582937002182007
Epoch 280, training loss: 435.8526306152344 = 1.0571459531784058 + 50.0 * 8.69590950012207
Epoch 280, val loss: 1.0578792095184326
Epoch 290, training loss: 435.8557434082031 = 1.0568053722381592 + 50.0 * 8.695979118347168
Epoch 290, val loss: 1.05753755569458
Epoch 300, training loss: 435.8153076171875 = 1.0564969778060913 + 50.0 * 8.695176124572754
Epoch 300, val loss: 1.0572341680526733
Epoch 310, training loss: 435.7605895996094 = 1.0562578439712524 + 50.0 * 8.694087028503418
Epoch 310, val loss: 1.0569936037063599
Epoch 320, training loss: 435.5695495605469 = 1.0560394525527954 + 50.0 * 8.69027042388916
Epoch 320, val loss: 1.0567787885665894
Epoch 330, training loss: 435.50677490234375 = 1.0558624267578125 + 50.0 * 8.689018249511719
Epoch 330, val loss: 1.0566065311431885
Epoch 340, training loss: 435.38519287109375 = 1.05570387840271 + 50.0 * 8.686590194702148
Epoch 340, val loss: 1.0564533472061157
Epoch 350, training loss: 435.52276611328125 = 1.055587887763977 + 50.0 * 8.689343452453613
Epoch 350, val loss: 1.0563468933105469
Epoch 360, training loss: 435.396728515625 = 1.0554838180541992 + 50.0 * 8.686824798583984
Epoch 360, val loss: 1.0562440156936646
Epoch 370, training loss: 435.392333984375 = 1.0553948879241943 + 50.0 * 8.686738967895508
Epoch 370, val loss: 1.0561529397964478
Epoch 380, training loss: 435.30078125 = 1.0553271770477295 + 50.0 * 8.684908866882324
Epoch 380, val loss: 1.0560662746429443
Epoch 390, training loss: 435.6063537597656 = 1.0552836656570435 + 50.0 * 8.691020965576172
Epoch 390, val loss: 1.0560411214828491
Epoch 400, training loss: 435.41815185546875 = 1.0552526712417603 + 50.0 * 8.687257766723633
Epoch 400, val loss: 1.0560002326965332
Epoch 410, training loss: 435.52716064453125 = 1.0552150011062622 + 50.0 * 8.689438819885254
Epoch 410, val loss: 1.0559662580490112
Epoch 420, training loss: 435.546630859375 = 1.055185079574585 + 50.0 * 8.689828872680664
Epoch 420, val loss: 1.0559371709823608
Epoch 430, training loss: 435.70574951171875 = 1.0551706552505493 + 50.0 * 8.693011283874512
Epoch 430, val loss: 1.0559159517288208
Epoch 440, training loss: 435.64215087890625 = 1.0551425218582153 + 50.0 * 8.691740036010742
Epoch 440, val loss: 1.0558902025222778
Epoch 450, training loss: 435.6266784667969 = 1.0551170110702515 + 50.0 * 8.691431045532227
Epoch 450, val loss: 1.05586576461792
Epoch 460, training loss: 435.60968017578125 = 1.0551031827926636 + 50.0 * 8.691091537475586
Epoch 460, val loss: 1.055855631828308
Epoch 470, training loss: 435.7467956542969 = 1.0550919771194458 + 50.0 * 8.69383430480957
Epoch 470, val loss: 1.0558449029922485
Epoch 480, training loss: 435.9048156738281 = 1.0550861358642578 + 50.0 * 8.69699478149414
Epoch 480, val loss: 1.055837631225586
Epoch 490, training loss: 435.87579345703125 = 1.0550694465637207 + 50.0 * 8.69641399383545
Epoch 490, val loss: 1.0558110475540161
Epoch 500, training loss: 435.7856140136719 = 1.0550209283828735 + 50.0 * 8.694611549377441
Epoch 500, val loss: 1.05576491355896
Epoch 510, training loss: 436.23040771484375 = 1.0550298690795898 + 50.0 * 8.703507423400879
Epoch 510, val loss: 1.055783987045288
Epoch 520, training loss: 435.7464599609375 = 1.0550270080566406 + 50.0 * 8.693828582763672
Epoch 520, val loss: 1.0557804107666016
Epoch 530, training loss: 435.690185546875 = 1.0550312995910645 + 50.0 * 8.692703247070312
Epoch 530, val loss: 1.0557827949523926
Epoch 540, training loss: 435.89862060546875 = 1.0550352334976196 + 50.0 * 8.696871757507324
Epoch 540, val loss: 1.0557889938354492
Epoch 550, training loss: 436.1378479003906 = 1.0550265312194824 + 50.0 * 8.701656341552734
Epoch 550, val loss: 1.055780053138733
Epoch 560, training loss: 436.267578125 = 1.0550287961959839 + 50.0 * 8.704251289367676
Epoch 560, val loss: 1.0557812452316284
Epoch 570, training loss: 436.2840881347656 = 1.055022120475769 + 50.0 * 8.704581260681152
Epoch 570, val loss: 1.055780053138733
Epoch 580, training loss: 436.35552978515625 = 1.0549979209899902 + 50.0 * 8.706010818481445
Epoch 580, val loss: 1.055750846862793
Epoch 590, training loss: 436.3861389160156 = 1.0549970865249634 + 50.0 * 8.706623077392578
Epoch 590, val loss: 1.055753231048584
Epoch 600, training loss: 436.305908203125 = 1.0549856424331665 + 50.0 * 8.705018043518066
Epoch 600, val loss: 1.0557433366775513
Epoch 610, training loss: 436.5200500488281 = 1.0549874305725098 + 50.0 * 8.709300994873047
Epoch 610, val loss: 1.0557533502578735
Epoch 620, training loss: 436.67291259765625 = 1.0549836158752441 + 50.0 * 8.712358474731445
Epoch 620, val loss: 1.0557434558868408
Epoch 630, training loss: 436.6279296875 = 1.0549741983413696 + 50.0 * 8.711459159851074
Epoch 630, val loss: 1.0557336807250977
Epoch 640, training loss: 436.7067565917969 = 1.0549708604812622 + 50.0 * 8.713035583496094
Epoch 640, val loss: 1.0557351112365723
Epoch 650, training loss: 436.6538391113281 = 1.0549437999725342 + 50.0 * 8.7119779586792
Epoch 650, val loss: 1.0557150840759277
Epoch 660, training loss: 436.72235107421875 = 1.054945707321167 + 50.0 * 8.713348388671875
Epoch 660, val loss: 1.0557109117507935
Epoch 670, training loss: 436.796630859375 = 1.0549399852752686 + 50.0 * 8.714834213256836
Epoch 670, val loss: 1.0557130575180054
Epoch 680, training loss: 437.00091552734375 = 1.0549390316009521 + 50.0 * 8.71891975402832
Epoch 680, val loss: 1.0557063817977905
Epoch 690, training loss: 436.9866638183594 = 1.0549297332763672 + 50.0 * 8.718634605407715
Epoch 690, val loss: 1.0556894540786743
Epoch 700, training loss: 437.0179138183594 = 1.0549153089523315 + 50.0 * 8.719260215759277
Epoch 700, val loss: 1.0556844472885132
Epoch 710, training loss: 437.1080322265625 = 1.054916262626648 + 50.0 * 8.721062660217285
Epoch 710, val loss: 1.0556881427764893
Epoch 720, training loss: 437.2369079589844 = 1.0549168586730957 + 50.0 * 8.723639488220215
Epoch 720, val loss: 1.055690050125122
Epoch 730, training loss: 437.22882080078125 = 1.0548979043960571 + 50.0 * 8.723478317260742
Epoch 730, val loss: 1.055672526359558
Epoch 740, training loss: 437.2384338378906 = 1.0548943281173706 + 50.0 * 8.723670959472656
Epoch 740, val loss: 1.0556731224060059
Epoch 750, training loss: 437.2286376953125 = 1.0548913478851318 + 50.0 * 8.723474502563477
Epoch 750, val loss: 1.0556613206863403
Epoch 760, training loss: 437.3096923828125 = 1.054887056350708 + 50.0 * 8.725095748901367
Epoch 760, val loss: 1.055666208267212
Epoch 770, training loss: 437.3603820800781 = 1.0548710823059082 + 50.0 * 8.726110458374023
Epoch 770, val loss: 1.0556532144546509
Epoch 780, training loss: 437.205810546875 = 1.0548455715179443 + 50.0 * 8.72301959991455
Epoch 780, val loss: 1.0556262731552124
Epoch 790, training loss: 437.4365539550781 = 1.054857611656189 + 50.0 * 8.727633476257324
Epoch 790, val loss: 1.0556375980377197
Epoch 800, training loss: 437.68572998046875 = 1.0548683404922485 + 50.0 * 8.732617378234863
Epoch 800, val loss: 1.0556495189666748
Epoch 810, training loss: 437.6255187988281 = 1.0548509359359741 + 50.0 * 8.731413841247559
Epoch 810, val loss: 1.0556223392486572
Epoch 820, training loss: 437.8787841796875 = 1.0548502206802368 + 50.0 * 8.736478805541992
Epoch 820, val loss: 1.0556353330612183
Epoch 830, training loss: 438.0188903808594 = 1.0548526048660278 + 50.0 * 8.739280700683594
Epoch 830, val loss: 1.0556362867355347
Epoch 840, training loss: 438.1376647949219 = 1.0548396110534668 + 50.0 * 8.741656303405762
Epoch 840, val loss: 1.055619716644287
Epoch 850, training loss: 438.14251708984375 = 1.0548352003097534 + 50.0 * 8.741753578186035
Epoch 850, val loss: 1.055615782737732
Epoch 860, training loss: 438.3368835449219 = 1.0548280477523804 + 50.0 * 8.745640754699707
Epoch 860, val loss: 1.0556120872497559
Epoch 870, training loss: 438.3408508300781 = 1.0548145771026611 + 50.0 * 8.745720863342285
Epoch 870, val loss: 1.0555970668792725
Epoch 880, training loss: 437.795654296875 = 1.0547280311584473 + 50.0 * 8.734818458557129
Epoch 880, val loss: 1.055533766746521
Epoch 890, training loss: 437.95574951171875 = 1.0547313690185547 + 50.0 * 8.738020896911621
Epoch 890, val loss: 1.0555205345153809
Epoch 900, training loss: 438.27630615234375 = 1.0547701120376587 + 50.0 * 8.744430541992188
Epoch 900, val loss: 1.0555528402328491
Epoch 910, training loss: 438.5323791503906 = 1.0547786951065063 + 50.0 * 8.749551773071289
Epoch 910, val loss: 1.055562973022461
Epoch 920, training loss: 438.765869140625 = 1.0547922849655151 + 50.0 * 8.75422191619873
Epoch 920, val loss: 1.0555760860443115
Epoch 930, training loss: 438.84893798828125 = 1.0547873973846436 + 50.0 * 8.75588321685791
Epoch 930, val loss: 1.0555708408355713
Epoch 940, training loss: 438.47119140625 = 1.0547412633895874 + 50.0 * 8.748329162597656
Epoch 940, val loss: 1.0555305480957031
Epoch 950, training loss: 438.8005065917969 = 1.0547534227371216 + 50.0 * 8.754915237426758
Epoch 950, val loss: 1.0555453300476074
Epoch 960, training loss: 439.05078125 = 1.0547692775726318 + 50.0 * 8.759920120239258
Epoch 960, val loss: 1.0555471181869507
Epoch 970, training loss: 439.0405578613281 = 1.0547536611557007 + 50.0 * 8.759716033935547
Epoch 970, val loss: 1.0555412769317627
Epoch 980, training loss: 439.2382507324219 = 1.0547491312026978 + 50.0 * 8.763669967651367
Epoch 980, val loss: 1.0555428266525269
Epoch 990, training loss: 439.0660705566406 = 1.0547363758087158 + 50.0 * 8.760226249694824
Epoch 990, val loss: 1.0555216073989868
Epoch 1000, training loss: 439.280029296875 = 1.0547360181808472 + 50.0 * 8.764505386352539
Epoch 1000, val loss: 1.0555274486541748
Epoch 1010, training loss: 439.446533203125 = 1.0547292232513428 + 50.0 * 8.767836570739746
Epoch 1010, val loss: 1.0555294752120972
Epoch 1020, training loss: 439.5730895996094 = 1.0547271966934204 + 50.0 * 8.770367622375488
Epoch 1020, val loss: 1.0555223226547241
Epoch 1030, training loss: 439.750244140625 = 1.054724931716919 + 50.0 * 8.773910522460938
Epoch 1030, val loss: 1.055524468421936
Epoch 1040, training loss: 439.656982421875 = 1.054694414138794 + 50.0 * 8.772046089172363
Epoch 1040, val loss: 1.055484414100647
Epoch 1050, training loss: 439.5631103515625 = 1.0546927452087402 + 50.0 * 8.77016830444336
Epoch 1050, val loss: 1.055480718612671
Epoch 1060, training loss: 439.83966064453125 = 1.0546914339065552 + 50.0 * 8.775699615478516
Epoch 1060, val loss: 1.0554940700531006
Epoch 1070, training loss: 440.03460693359375 = 1.0546802282333374 + 50.0 * 8.779598236083984
Epoch 1070, val loss: 1.0554866790771484
Epoch 1080, training loss: 440.0638732910156 = 1.0546741485595703 + 50.0 * 8.780183792114258
Epoch 1080, val loss: 1.0554736852645874
Epoch 1090, training loss: 440.2474670410156 = 1.054684042930603 + 50.0 * 8.783855438232422
Epoch 1090, val loss: 1.0554871559143066
Epoch 1100, training loss: 440.3565979003906 = 1.0546786785125732 + 50.0 * 8.786038398742676
Epoch 1100, val loss: 1.0554717779159546
Epoch 1110, training loss: 440.31219482421875 = 1.0546621084213257 + 50.0 * 8.785150527954102
Epoch 1110, val loss: 1.055463433265686
Epoch 1120, training loss: 440.5005798339844 = 1.0546549558639526 + 50.0 * 8.788918495178223
Epoch 1120, val loss: 1.0554609298706055
Epoch 1130, training loss: 440.37298583984375 = 1.054646611213684 + 50.0 * 8.78636646270752
Epoch 1130, val loss: 1.0554442405700684
Epoch 1140, training loss: 440.5253601074219 = 1.0546406507492065 + 50.0 * 8.789414405822754
Epoch 1140, val loss: 1.055442452430725
Epoch 1150, training loss: 440.7245178222656 = 1.0546437501907349 + 50.0 * 8.793396949768066
Epoch 1150, val loss: 1.05544114112854
Epoch 1160, training loss: 440.853515625 = 1.0546410083770752 + 50.0 * 8.795977592468262
Epoch 1160, val loss: 1.0554434061050415
Epoch 1170, training loss: 440.7311096191406 = 1.0546108484268188 + 50.0 * 8.793530464172363
Epoch 1170, val loss: 1.055412769317627
Epoch 1180, training loss: 441.004150390625 = 1.0546232461929321 + 50.0 * 8.798990249633789
Epoch 1180, val loss: 1.0554263591766357
Epoch 1190, training loss: 441.1140441894531 = 1.0546129941940308 + 50.0 * 8.801188468933105
Epoch 1190, val loss: 1.055423617362976
Epoch 1200, training loss: 441.0827941894531 = 1.054601788520813 + 50.0 * 8.80056381225586
Epoch 1200, val loss: 1.0554167032241821
Epoch 1210, training loss: 441.2440490722656 = 1.0546045303344727 + 50.0 * 8.803789138793945
Epoch 1210, val loss: 1.0554125308990479
Epoch 1220, training loss: 441.37542724609375 = 1.0545964241027832 + 50.0 * 8.806416511535645
Epoch 1220, val loss: 1.0554028749465942
Epoch 1230, training loss: 441.1505126953125 = 1.0545724630355835 + 50.0 * 8.801918983459473
Epoch 1230, val loss: 1.055384874343872
Epoch 1240, training loss: 441.3154602050781 = 1.0545588731765747 + 50.0 * 8.805217742919922
Epoch 1240, val loss: 1.0553765296936035
Epoch 1250, training loss: 441.54730224609375 = 1.0545653104782104 + 50.0 * 8.809854507446289
Epoch 1250, val loss: 1.055385708808899
Epoch 1260, training loss: 441.5721740722656 = 1.0545622110366821 + 50.0 * 8.810352325439453
Epoch 1260, val loss: 1.0553802251815796
Epoch 1270, training loss: 441.64898681640625 = 1.0545567274093628 + 50.0 * 8.811888694763184
Epoch 1270, val loss: 1.0553789138793945
Epoch 1280, training loss: 441.86785888671875 = 1.0545552968978882 + 50.0 * 8.816266059875488
Epoch 1280, val loss: 1.055373191833496
Epoch 1290, training loss: 441.7787780761719 = 1.05453622341156 + 50.0 * 8.814484596252441
Epoch 1290, val loss: 1.055360198020935
Epoch 1300, training loss: 441.9396057128906 = 1.054532527923584 + 50.0 * 8.81770133972168
Epoch 1300, val loss: 1.0553624629974365
Epoch 1310, training loss: 442.037841796875 = 1.0545203685760498 + 50.0 * 8.819665908813477
Epoch 1310, val loss: 1.0553452968597412
Epoch 1320, training loss: 441.75970458984375 = 1.0544987916946411 + 50.0 * 8.814104080200195
Epoch 1320, val loss: 1.0553083419799805
Epoch 1330, training loss: 441.8713684082031 = 1.05448579788208 + 50.0 * 8.816337585449219
Epoch 1330, val loss: 1.0553141832351685
Epoch 1340, training loss: 442.10089111328125 = 1.0544917583465576 + 50.0 * 8.820927619934082
Epoch 1340, val loss: 1.0553193092346191
Epoch 1350, training loss: 442.1656494140625 = 1.0544815063476562 + 50.0 * 8.822223663330078
Epoch 1350, val loss: 1.0553123950958252
Epoch 1360, training loss: 442.27984619140625 = 1.05447256565094 + 50.0 * 8.824507713317871
Epoch 1360, val loss: 1.055304765701294
Epoch 1370, training loss: 442.4256896972656 = 1.054477572441101 + 50.0 * 8.827424049377441
Epoch 1370, val loss: 1.05530846118927
Epoch 1380, training loss: 442.5985412597656 = 1.0544685125350952 + 50.0 * 8.830881118774414
Epoch 1380, val loss: 1.055302619934082
Epoch 1390, training loss: 442.6565856933594 = 1.0544464588165283 + 50.0 * 8.832042694091797
Epoch 1390, val loss: 1.0552854537963867
Epoch 1400, training loss: 442.64178466796875 = 1.0544408559799194 + 50.0 * 8.831747055053711
Epoch 1400, val loss: 1.055272102355957
Epoch 1410, training loss: 442.7337341308594 = 1.0544308423995972 + 50.0 * 8.833585739135742
Epoch 1410, val loss: 1.0552690029144287
Epoch 1420, training loss: 442.91790771484375 = 1.0544196367263794 + 50.0 * 8.83726978302002
Epoch 1420, val loss: 1.055261492729187
Epoch 1430, training loss: 442.7760009765625 = 1.054397463798523 + 50.0 * 8.834432601928711
Epoch 1430, val loss: 1.0552363395690918
Epoch 1440, training loss: 442.9245300292969 = 1.054400086402893 + 50.0 * 8.83740234375
Epoch 1440, val loss: 1.0552400350570679
Epoch 1450, training loss: 443.0450439453125 = 1.0543925762176514 + 50.0 * 8.839813232421875
Epoch 1450, val loss: 1.0552334785461426
Epoch 1460, training loss: 443.0372314453125 = 1.0543670654296875 + 50.0 * 8.8396577835083
Epoch 1460, val loss: 1.0552033185958862
Epoch 1470, training loss: 443.2256164550781 = 1.0543678998947144 + 50.0 * 8.843424797058105
Epoch 1470, val loss: 1.0552077293395996
Epoch 1480, training loss: 443.3191833496094 = 1.0543705224990845 + 50.0 * 8.845295906066895
Epoch 1480, val loss: 1.05520761013031
Epoch 1490, training loss: 443.3513488769531 = 1.0543527603149414 + 50.0 * 8.845939636230469
Epoch 1490, val loss: 1.0551954507827759
Epoch 1500, training loss: 443.3303527832031 = 1.0543372631072998 + 50.0 * 8.84552001953125
Epoch 1500, val loss: 1.0551825761795044
Epoch 1510, training loss: 443.32012939453125 = 1.054319977760315 + 50.0 * 8.845315933227539
Epoch 1510, val loss: 1.0551586151123047
Epoch 1520, training loss: 443.5036926269531 = 1.0543123483657837 + 50.0 * 8.848987579345703
Epoch 1520, val loss: 1.0551592111587524
Epoch 1530, training loss: 443.6238098144531 = 1.0543173551559448 + 50.0 * 8.85138988494873
Epoch 1530, val loss: 1.0551644563674927
Epoch 1540, training loss: 443.6865539550781 = 1.054258942604065 + 50.0 * 8.852645874023438
Epoch 1540, val loss: 1.0550786256790161
Epoch 1550, training loss: 443.2301330566406 = 1.0542194843292236 + 50.0 * 8.843518257141113
Epoch 1550, val loss: 1.0550768375396729
Epoch 1560, training loss: 443.3555908203125 = 1.0542198419570923 + 50.0 * 8.846027374267578
Epoch 1560, val loss: 1.055083155632019
Epoch 1570, training loss: 443.5311279296875 = 1.0542250871658325 + 50.0 * 8.84953784942627
Epoch 1570, val loss: 1.0550862550735474
Epoch 1580, training loss: 443.80731201171875 = 1.0542384386062622 + 50.0 * 8.855061531066895
Epoch 1580, val loss: 1.0550990104675293
Epoch 1590, training loss: 443.7991638183594 = 1.054234266281128 + 50.0 * 8.854898452758789
Epoch 1590, val loss: 1.0550880432128906
Epoch 1600, training loss: 443.75018310546875 = 1.054195761680603 + 50.0 * 8.853919982910156
Epoch 1600, val loss: 1.0550626516342163
Epoch 1610, training loss: 443.84600830078125 = 1.0541905164718628 + 50.0 * 8.855835914611816
Epoch 1610, val loss: 1.0550538301467896
Epoch 1620, training loss: 444.0917053222656 = 1.0541998147964478 + 50.0 * 8.860750198364258
Epoch 1620, val loss: 1.0550594329833984
Epoch 1630, training loss: 444.23675537109375 = 1.0541929006576538 + 50.0 * 8.863651275634766
Epoch 1630, val loss: 1.0550427436828613
Epoch 1640, training loss: 444.18023681640625 = 1.0541670322418213 + 50.0 * 8.862521171569824
Epoch 1640, val loss: 1.055029034614563
Epoch 1650, training loss: 444.3909912109375 = 1.054157018661499 + 50.0 * 8.86673641204834
Epoch 1650, val loss: 1.055019736289978
Epoch 1660, training loss: 444.2701721191406 = 1.0541486740112305 + 50.0 * 8.864320755004883
Epoch 1660, val loss: 1.0550119876861572
Epoch 1670, training loss: 444.5019226074219 = 1.0541419982910156 + 50.0 * 8.868955612182617
Epoch 1670, val loss: 1.0550041198730469
Epoch 1680, training loss: 444.60882568359375 = 1.0541235208511353 + 50.0 * 8.87109375
Epoch 1680, val loss: 1.0549838542938232
Epoch 1690, training loss: 444.58258056640625 = 1.0541054010391235 + 50.0 * 8.870569229125977
Epoch 1690, val loss: 1.0549737215042114
Epoch 1700, training loss: 444.401123046875 = 1.0540616512298584 + 50.0 * 8.866941452026367
Epoch 1700, val loss: 1.054929256439209
Epoch 1710, training loss: 444.5705261230469 = 1.054063320159912 + 50.0 * 8.870328903198242
Epoch 1710, val loss: 1.0549360513687134
Epoch 1720, training loss: 444.6579895019531 = 1.0540530681610107 + 50.0 * 8.872078895568848
Epoch 1720, val loss: 1.0549148321151733
Epoch 1730, training loss: 444.4786071777344 = 1.0540326833724976 + 50.0 * 8.868491172790527
Epoch 1730, val loss: 1.0549066066741943
Epoch 1740, training loss: 444.8727111816406 = 1.0540354251861572 + 50.0 * 8.876373291015625
Epoch 1740, val loss: 1.0549088716506958
Epoch 1750, training loss: 444.8861389160156 = 1.0540237426757812 + 50.0 * 8.876642227172852
Epoch 1750, val loss: 1.0548946857452393
Epoch 1760, training loss: 444.9747314453125 = 1.054009199142456 + 50.0 * 8.878414154052734
Epoch 1760, val loss: 1.0548837184906006
Epoch 1770, training loss: 445.02191162109375 = 1.053984522819519 + 50.0 * 8.879358291625977
Epoch 1770, val loss: 1.0548702478408813
Epoch 1780, training loss: 445.090087890625 = 1.053978681564331 + 50.0 * 8.880722045898438
Epoch 1780, val loss: 1.0548648834228516
Epoch 1790, training loss: 445.1294860839844 = 1.053966999053955 + 50.0 * 8.881510734558105
Epoch 1790, val loss: 1.0548436641693115
Epoch 1800, training loss: 445.1755676269531 = 1.0539436340332031 + 50.0 * 8.882431983947754
Epoch 1800, val loss: 1.0548282861709595
Epoch 1810, training loss: 445.2015075683594 = 1.0539237260818481 + 50.0 * 8.882951736450195
Epoch 1810, val loss: 1.054810881614685
Epoch 1820, training loss: 445.289306640625 = 1.0539096593856812 + 50.0 * 8.8847074508667
Epoch 1820, val loss: 1.0548070669174194
Epoch 1830, training loss: 445.4788818359375 = 1.053910732269287 + 50.0 * 8.88849925994873
Epoch 1830, val loss: 1.0547947883605957
Epoch 1840, training loss: 445.4273986816406 = 1.0538820028305054 + 50.0 * 8.887470245361328
Epoch 1840, val loss: 1.0547748804092407
Epoch 1850, training loss: 445.3509826660156 = 1.0538524389266968 + 50.0 * 8.885942459106445
Epoch 1850, val loss: 1.054748773574829
Epoch 1860, training loss: 445.4001770019531 = 1.0538384914398193 + 50.0 * 8.886926651000977
Epoch 1860, val loss: 1.0547345876693726
Epoch 1870, training loss: 445.5875549316406 = 1.0538355112075806 + 50.0 * 8.890674591064453
Epoch 1870, val loss: 1.0547295808792114
Epoch 1880, training loss: 445.62554931640625 = 1.0538082122802734 + 50.0 * 8.891434669494629
Epoch 1880, val loss: 1.0547208786010742
Epoch 1890, training loss: 445.6218566894531 = 1.0537885427474976 + 50.0 * 8.891361236572266
Epoch 1890, val loss: 1.0546938180923462
Epoch 1900, training loss: 445.59124755859375 = 1.0537456274032593 + 50.0 * 8.89074993133545
Epoch 1900, val loss: 1.0546483993530273
Epoch 1910, training loss: 445.318359375 = 1.053662896156311 + 50.0 * 8.885293960571289
Epoch 1910, val loss: 1.0545653104782104
Epoch 1920, training loss: 445.096435546875 = 1.0536329746246338 + 50.0 * 8.88085651397705
Epoch 1920, val loss: 1.0545300245285034
Epoch 1930, training loss: 444.8454284667969 = 1.0536079406738281 + 50.0 * 8.875836372375488
Epoch 1930, val loss: 1.0545222759246826
Epoch 1940, training loss: 444.9217834472656 = 1.0536009073257446 + 50.0 * 8.877364158630371
Epoch 1940, val loss: 1.0545246601104736
Epoch 1950, training loss: 445.06793212890625 = 1.0536069869995117 + 50.0 * 8.88028621673584
Epoch 1950, val loss: 1.0545380115509033
Epoch 1960, training loss: 445.4837951660156 = 1.053631067276001 + 50.0 * 8.888603210449219
Epoch 1960, val loss: 1.054561734199524
Epoch 1970, training loss: 445.83514404296875 = 1.0536466836929321 + 50.0 * 8.8956298828125
Epoch 1970, val loss: 1.05457603931427
Epoch 1980, training loss: 445.8353576660156 = 1.0536245107650757 + 50.0 * 8.895634651184082
Epoch 1980, val loss: 1.0545448064804077
Epoch 1990, training loss: 445.795654296875 = 1.0535963773727417 + 50.0 * 8.894841194152832
Epoch 1990, val loss: 1.0545283555984497
Epoch 2000, training loss: 446.05914306640625 = 1.053591012954712 + 50.0 * 8.900111198425293
Epoch 2000, val loss: 1.0545164346694946
Epoch 2010, training loss: 446.1036376953125 = 1.053580641746521 + 50.0 * 8.9010009765625
Epoch 2010, val loss: 1.05451238155365
Epoch 2020, training loss: 446.1391296386719 = 1.053552508354187 + 50.0 * 8.901711463928223
Epoch 2020, val loss: 1.0544782876968384
Epoch 2030, training loss: 446.1669616699219 = 1.0535317659378052 + 50.0 * 8.902268409729004
Epoch 2030, val loss: 1.0544617176055908
Epoch 2040, training loss: 446.279296875 = 1.053523302078247 + 50.0 * 8.904515266418457
Epoch 2040, val loss: 1.0544524192810059
Epoch 2050, training loss: 446.12744140625 = 1.0534720420837402 + 50.0 * 8.901479721069336
Epoch 2050, val loss: 1.0544025897979736
Epoch 2060, training loss: 446.0744323730469 = 1.0534549951553345 + 50.0 * 8.900419235229492
Epoch 2060, val loss: 1.0543911457061768
Epoch 2070, training loss: 446.3143310546875 = 1.0534533262252808 + 50.0 * 8.905217170715332
Epoch 2070, val loss: 1.0543889999389648
Epoch 2080, training loss: 446.48358154296875 = 1.0534377098083496 + 50.0 * 8.908602714538574
Epoch 2080, val loss: 1.0543785095214844
Epoch 2090, training loss: 446.3949890136719 = 1.0534042119979858 + 50.0 * 8.906831741333008
Epoch 2090, val loss: 1.054343819618225
Epoch 2100, training loss: 446.63360595703125 = 1.0533928871154785 + 50.0 * 8.911603927612305
Epoch 2100, val loss: 1.0543369054794312
Epoch 2110, training loss: 446.7679443359375 = 1.0533807277679443 + 50.0 * 8.914291381835938
Epoch 2110, val loss: 1.0543218851089478
Epoch 2120, training loss: 446.74237060546875 = 1.0533411502838135 + 50.0 * 8.913780212402344
Epoch 2120, val loss: 1.0542854070663452
Epoch 2130, training loss: 446.6736755371094 = 1.0533065795898438 + 50.0 * 8.912406921386719
Epoch 2130, val loss: 1.0542532205581665
Epoch 2140, training loss: 446.4447326660156 = 1.0532082319259644 + 50.0 * 8.907830238342285
Epoch 2140, val loss: 1.054149866104126
Epoch 2150, training loss: 446.220703125 = 1.0531717538833618 + 50.0 * 8.903350830078125
Epoch 2150, val loss: 1.0541292428970337
Epoch 2160, training loss: 446.0951843261719 = 1.053146243095398 + 50.0 * 8.900840759277344
Epoch 2160, val loss: 1.0541261434555054
Epoch 2170, training loss: 446.3880615234375 = 1.0531703233718872 + 50.0 * 8.906698226928711
Epoch 2170, val loss: 1.0541417598724365
Epoch 2180, training loss: 446.6612854003906 = 1.053179144859314 + 50.0 * 8.912161827087402
Epoch 2180, val loss: 1.0541489124298096
Epoch 2190, training loss: 446.8834533691406 = 1.053181529045105 + 50.0 * 8.916604995727539
Epoch 2190, val loss: 1.0541374683380127
Epoch 2200, training loss: 446.8016052246094 = 1.053126335144043 + 50.0 * 8.914969444274902
Epoch 2200, val loss: 1.0540995597839355
Epoch 2210, training loss: 446.92779541015625 = 1.0531103610992432 + 50.0 * 8.91749382019043
Epoch 2210, val loss: 1.054081916809082
Epoch 2220, training loss: 447.0538635253906 = 1.0530999898910522 + 50.0 * 8.920015335083008
Epoch 2220, val loss: 1.0540777444839478
Epoch 2230, training loss: 447.1238708496094 = 1.0530778169631958 + 50.0 * 8.921416282653809
Epoch 2230, val loss: 1.0540540218353271
Epoch 2240, training loss: 447.0682067871094 = 1.053045392036438 + 50.0 * 8.920303344726562
Epoch 2240, val loss: 1.054020881652832
Epoch 2250, training loss: 447.2179260253906 = 1.0530253648757935 + 50.0 * 8.923297882080078
Epoch 2250, val loss: 1.0540140867233276
Epoch 2260, training loss: 447.0773010253906 = 1.052977204322815 + 50.0 * 8.920486450195312
Epoch 2260, val loss: 1.0539753437042236
Epoch 2270, training loss: 447.1098937988281 = 1.0529570579528809 + 50.0 * 8.921138763427734
Epoch 2270, val loss: 1.0539480447769165
Epoch 2280, training loss: 447.3353271484375 = 1.0529510974884033 + 50.0 * 8.925647735595703
Epoch 2280, val loss: 1.0539449453353882
Epoch 2290, training loss: 447.4464111328125 = 1.0529295206069946 + 50.0 * 8.92786979675293
Epoch 2290, val loss: 1.0539354085922241
Epoch 2300, training loss: 447.3730773925781 = 1.052882194519043 + 50.0 * 8.926403999328613
Epoch 2300, val loss: 1.053882360458374
Epoch 2310, training loss: 447.2984313964844 = 1.0528515577316284 + 50.0 * 8.924911499023438
Epoch 2310, val loss: 1.0538541078567505
Epoch 2320, training loss: 447.4382629394531 = 1.052839994430542 + 50.0 * 8.927708625793457
Epoch 2320, val loss: 1.0538498163223267
Epoch 2330, training loss: 447.6717529296875 = 1.0528335571289062 + 50.0 * 8.932378768920898
Epoch 2330, val loss: 1.0538420677185059
Epoch 2340, training loss: 447.5676574707031 = 1.0527795553207397 + 50.0 * 8.9302978515625
Epoch 2340, val loss: 1.0537800788879395
Epoch 2350, training loss: 447.20611572265625 = 1.0527173280715942 + 50.0 * 8.923068046569824
Epoch 2350, val loss: 1.0537371635437012
Epoch 2360, training loss: 447.56011962890625 = 1.0527082681655884 + 50.0 * 8.930148124694824
Epoch 2360, val loss: 1.0537229776382446
Epoch 2370, training loss: 447.7161865234375 = 1.0527082681655884 + 50.0 * 8.933269500732422
Epoch 2370, val loss: 1.053733229637146
Epoch 2380, training loss: 447.8525085449219 = 1.052694320678711 + 50.0 * 8.935996055603027
Epoch 2380, val loss: 1.053717017173767
Epoch 2390, training loss: 447.6341857910156 = 1.0526405572891235 + 50.0 * 8.931631088256836
Epoch 2390, val loss: 1.0535314083099365
Epoch 2400, training loss: 443.4737243652344 = 1.0513429641723633 + 50.0 * 8.848447799682617
Epoch 2400, val loss: 1.052527904510498
Epoch 2410, training loss: 445.9319763183594 = 1.0519012212753296 + 50.0 * 8.897601127624512
Epoch 2410, val loss: 1.052899718284607
Epoch 2420, training loss: 444.79876708984375 = 1.0519013404846191 + 50.0 * 8.874937057495117
Epoch 2420, val loss: 1.0529799461364746
Epoch 2430, training loss: 444.9162902832031 = 1.0519872903823853 + 50.0 * 8.877285957336426
Epoch 2430, val loss: 1.0530380010604858
Epoch 2440, training loss: 445.6690673828125 = 1.052123785018921 + 50.0 * 8.892338752746582
Epoch 2440, val loss: 1.053159475326538
Epoch 2450, training loss: 446.06170654296875 = 1.0521684885025024 + 50.0 * 8.900191307067871
Epoch 2450, val loss: 1.0532171726226807
Epoch 2460, training loss: 446.4039306640625 = 1.0521897077560425 + 50.0 * 8.907034873962402
Epoch 2460, val loss: 1.0532363653182983
Epoch 2470, training loss: 446.6435241699219 = 1.0522010326385498 + 50.0 * 8.911826133728027
Epoch 2470, val loss: 1.0532461404800415
Epoch 2480, training loss: 446.848876953125 = 1.0521914958953857 + 50.0 * 8.915933609008789
Epoch 2480, val loss: 1.0532439947128296
Epoch 2490, training loss: 447.15264892578125 = 1.052188754081726 + 50.0 * 8.922009468078613
Epoch 2490, val loss: 1.053242802619934
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.3982608695652174
0.8120698398898791
=== training gcn model ===
Epoch 0, training loss: 501.4682312011719 = 1.0912361145019531 + 50.0 * 10.007539749145508
Epoch 0, val loss: 1.0925393104553223
Epoch 10, training loss: 479.24462890625 = 1.088152527809143 + 50.0 * 9.563129425048828
Epoch 10, val loss: 1.0894603729248047
Epoch 20, training loss: 471.7680358886719 = 1.0853605270385742 + 50.0 * 9.413653373718262
Epoch 20, val loss: 1.0866693258285522
Epoch 30, training loss: 466.02374267578125 = 1.082705020904541 + 50.0 * 9.298820495605469
Epoch 30, val loss: 1.0840120315551758
Epoch 40, training loss: 461.5767822265625 = 1.0802127122879028 + 50.0 * 9.209931373596191
Epoch 40, val loss: 1.081517219543457
Epoch 50, training loss: 457.9369812011719 = 1.0778695344924927 + 50.0 * 9.137182235717773
Epoch 50, val loss: 1.0791640281677246
Epoch 60, training loss: 454.8688659667969 = 1.075670599937439 + 50.0 * 9.0758638381958
Epoch 60, val loss: 1.076951503753662
Epoch 70, training loss: 452.2705078125 = 1.0735962390899658 + 50.0 * 9.023938179016113
Epoch 70, val loss: 1.074858546257019
Epoch 80, training loss: 450.0856018066406 = 1.0716551542282104 + 50.0 * 8.980278968811035
Epoch 80, val loss: 1.0728974342346191
Epoch 90, training loss: 448.1861572265625 = 1.0698373317718506 + 50.0 * 8.942326545715332
Epoch 90, val loss: 1.0710543394088745
Epoch 100, training loss: 446.5062561035156 = 1.0681536197662354 + 50.0 * 8.908761978149414
Epoch 100, val loss: 1.069344162940979
Epoch 110, training loss: 445.0644836425781 = 1.0665841102600098 + 50.0 * 8.879958152770996
Epoch 110, val loss: 1.0677452087402344
Epoch 120, training loss: 443.80828857421875 = 1.0651319026947021 + 50.0 * 8.854863166809082
Epoch 120, val loss: 1.0662660598754883
Epoch 130, training loss: 442.6903076171875 = 1.06380295753479 + 50.0 * 8.83253002166748
Epoch 130, val loss: 1.0649067163467407
Epoch 140, training loss: 441.67327880859375 = 1.062592625617981 + 50.0 * 8.812213897705078
Epoch 140, val loss: 1.0636664628982544
Epoch 150, training loss: 440.855224609375 = 1.0615005493164062 + 50.0 * 8.79587459564209
Epoch 150, val loss: 1.0625438690185547
Epoch 160, training loss: 440.4201354980469 = 1.0605312585830688 + 50.0 * 8.787192344665527
Epoch 160, val loss: 1.061547875404358
Epoch 170, training loss: 439.58319091796875 = 1.0596619844436646 + 50.0 * 8.77047061920166
Epoch 170, val loss: 1.0606638193130493
Epoch 180, training loss: 438.9425964355469 = 1.0589063167572021 + 50.0 * 8.757674217224121
Epoch 180, val loss: 1.0598728656768799
Epoch 190, training loss: 438.5314636230469 = 1.058266043663025 + 50.0 * 8.74946403503418
Epoch 190, val loss: 1.0592100620269775
Epoch 200, training loss: 438.0195617675781 = 1.0577075481414795 + 50.0 * 8.739236831665039
Epoch 200, val loss: 1.0586268901824951
Epoch 210, training loss: 437.7225646972656 = 1.0572165250778198 + 50.0 * 8.733306884765625
Epoch 210, val loss: 1.0581024885177612
Epoch 220, training loss: 437.5874328613281 = 1.0568474531173706 + 50.0 * 8.730611801147461
Epoch 220, val loss: 1.0577248334884644
Epoch 230, training loss: 437.0549621582031 = 1.0565080642700195 + 50.0 * 8.719968795776367
Epoch 230, val loss: 1.057374119758606
Epoch 240, training loss: 436.84185791015625 = 1.056260108947754 + 50.0 * 8.71571159362793
Epoch 240, val loss: 1.0571011304855347
Epoch 250, training loss: 436.5924377441406 = 1.056050419807434 + 50.0 * 8.71072769165039
Epoch 250, val loss: 1.0568716526031494
Epoch 260, training loss: 436.4009704589844 = 1.0558793544769287 + 50.0 * 8.706901550292969
Epoch 260, val loss: 1.0566918849945068
Epoch 270, training loss: 436.2086181640625 = 1.0557410717010498 + 50.0 * 8.703057289123535
Epoch 270, val loss: 1.0565381050109863
Epoch 280, training loss: 436.0348205566406 = 1.0556418895721436 + 50.0 * 8.699584007263184
Epoch 280, val loss: 1.0564310550689697
Epoch 290, training loss: 436.01898193359375 = 1.0555676221847534 + 50.0 * 8.699268341064453
Epoch 290, val loss: 1.0563597679138184
Epoch 300, training loss: 435.67437744140625 = 1.0554945468902588 + 50.0 * 8.692378044128418
Epoch 300, val loss: 1.0562691688537598
Epoch 310, training loss: 435.6429748535156 = 1.055452823638916 + 50.0 * 8.691750526428223
Epoch 310, val loss: 1.0562193393707275
Epoch 320, training loss: 435.6292724609375 = 1.055414080619812 + 50.0 * 8.691476821899414
Epoch 320, val loss: 1.0561769008636475
Epoch 330, training loss: 435.7787780761719 = 1.0553929805755615 + 50.0 * 8.694467544555664
Epoch 330, val loss: 1.056146502494812
Epoch 340, training loss: 435.6645202636719 = 1.0553535223007202 + 50.0 * 8.692183494567871
Epoch 340, val loss: 1.056102991104126
Epoch 350, training loss: 435.57208251953125 = 1.055328369140625 + 50.0 * 8.690335273742676
Epoch 350, val loss: 1.0560795068740845
Epoch 360, training loss: 435.4910888671875 = 1.0552952289581299 + 50.0 * 8.688715934753418
Epoch 360, val loss: 1.0560494661331177
Epoch 370, training loss: 435.4016418457031 = 1.0552765130996704 + 50.0 * 8.686927795410156
Epoch 370, val loss: 1.0560275316238403
Epoch 380, training loss: 435.5758056640625 = 1.0552717447280884 + 50.0 * 8.690410614013672
Epoch 380, val loss: 1.0560184717178345
Epoch 390, training loss: 435.4726257324219 = 1.055243730545044 + 50.0 * 8.688347816467285
Epoch 390, val loss: 1.0559943914413452
Epoch 400, training loss: 435.4365234375 = 1.0552293062210083 + 50.0 * 8.687625885009766
Epoch 400, val loss: 1.0559742450714111
Epoch 410, training loss: 435.5263671875 = 1.055222988128662 + 50.0 * 8.689422607421875
Epoch 410, val loss: 1.0559650659561157
Epoch 420, training loss: 435.55609130859375 = 1.0552141666412354 + 50.0 * 8.690017700195312
Epoch 420, val loss: 1.0559505224227905
Epoch 430, training loss: 435.3965148925781 = 1.055182933807373 + 50.0 * 8.686826705932617
Epoch 430, val loss: 1.055933952331543
Epoch 440, training loss: 435.89276123046875 = 1.0551741123199463 + 50.0 * 8.696751594543457
Epoch 440, val loss: 1.0559254884719849
Epoch 450, training loss: 435.43109130859375 = 1.0551520586013794 + 50.0 * 8.687519073486328
Epoch 450, val loss: 1.0558910369873047
Epoch 460, training loss: 435.6022033691406 = 1.055151104927063 + 50.0 * 8.690940856933594
Epoch 460, val loss: 1.0558937788009644
Epoch 470, training loss: 435.598876953125 = 1.0551213026046753 + 50.0 * 8.690875053405762
Epoch 470, val loss: 1.0558604001998901
Epoch 480, training loss: 435.70867919921875 = 1.0551235675811768 + 50.0 * 8.693071365356445
Epoch 480, val loss: 1.0558611154556274
Epoch 490, training loss: 435.7379150390625 = 1.055104374885559 + 50.0 * 8.693655967712402
Epoch 490, val loss: 1.0558466911315918
Epoch 500, training loss: 435.73248291015625 = 1.0550882816314697 + 50.0 * 8.693548202514648
Epoch 500, val loss: 1.0558316707611084
Epoch 510, training loss: 435.8261413574219 = 1.055071234703064 + 50.0 * 8.69542121887207
Epoch 510, val loss: 1.0558189153671265
Epoch 520, training loss: 435.84075927734375 = 1.0550661087036133 + 50.0 * 8.695713996887207
Epoch 520, val loss: 1.0558102130889893
Epoch 530, training loss: 436.1507263183594 = 1.0550315380096436 + 50.0 * 8.701913833618164
Epoch 530, val loss: 1.0557630062103271
Epoch 540, training loss: 435.74273681640625 = 1.0550200939178467 + 50.0 * 8.693754196166992
Epoch 540, val loss: 1.0557643175125122
Epoch 550, training loss: 435.8596496582031 = 1.0550148487091064 + 50.0 * 8.69609260559082
Epoch 550, val loss: 1.0557578802108765
Epoch 560, training loss: 436.1192932128906 = 1.0550122261047363 + 50.0 * 8.701285362243652
Epoch 560, val loss: 1.0557591915130615
Epoch 570, training loss: 436.1822509765625 = 1.0549967288970947 + 50.0 * 8.702545166015625
Epoch 570, val loss: 1.0557482242584229
Epoch 580, training loss: 436.1537170410156 = 1.0549765825271606 + 50.0 * 8.701974868774414
Epoch 580, val loss: 1.0557256937026978
Epoch 590, training loss: 436.13067626953125 = 1.0549715757369995 + 50.0 * 8.70151424407959
Epoch 590, val loss: 1.0557234287261963
Epoch 600, training loss: 436.2205505371094 = 1.0549554824829102 + 50.0 * 8.703311920166016
Epoch 600, val loss: 1.0557093620300293
Epoch 610, training loss: 436.30267333984375 = 1.0549418926239014 + 50.0 * 8.704955101013184
Epoch 610, val loss: 1.0556970834732056
Epoch 620, training loss: 436.4993896484375 = 1.0549342632293701 + 50.0 * 8.70888900756836
Epoch 620, val loss: 1.0556931495666504
Epoch 630, training loss: 436.5990295410156 = 1.054931879043579 + 50.0 * 8.710882186889648
Epoch 630, val loss: 1.0556803941726685
Epoch 640, training loss: 436.448486328125 = 1.0549019575119019 + 50.0 * 8.707871437072754
Epoch 640, val loss: 1.0556594133377075
Epoch 650, training loss: 436.5366516113281 = 1.0548919439315796 + 50.0 * 8.709634780883789
Epoch 650, val loss: 1.0556522607803345
Epoch 660, training loss: 436.6282653808594 = 1.0548841953277588 + 50.0 * 8.711467742919922
Epoch 660, val loss: 1.055646300315857
Epoch 670, training loss: 436.4452209472656 = 1.0548462867736816 + 50.0 * 8.707807540893555
Epoch 670, val loss: 1.0556045770645142
Epoch 680, training loss: 436.5708312988281 = 1.0548360347747803 + 50.0 * 8.710319519042969
Epoch 680, val loss: 1.0555994510650635
Epoch 690, training loss: 436.79840087890625 = 1.0548319816589355 + 50.0 * 8.714871406555176
Epoch 690, val loss: 1.0555983781814575
Epoch 700, training loss: 436.92559814453125 = 1.0548224449157715 + 50.0 * 8.717415809631348
Epoch 700, val loss: 1.055586814880371
Epoch 710, training loss: 437.0186767578125 = 1.0548110008239746 + 50.0 * 8.719277381896973
Epoch 710, val loss: 1.0555850267410278
Epoch 720, training loss: 437.2352600097656 = 1.0548070669174194 + 50.0 * 8.72360897064209
Epoch 720, val loss: 1.0555812120437622
Epoch 730, training loss: 437.3198547363281 = 1.0547919273376465 + 50.0 * 8.725301742553711
Epoch 730, val loss: 1.0555715560913086
Epoch 740, training loss: 437.29571533203125 = 1.0547531843185425 + 50.0 * 8.72481918334961
Epoch 740, val loss: 1.0555367469787598
Epoch 750, training loss: 437.1034851074219 = 1.0547384023666382 + 50.0 * 8.720974922180176
Epoch 750, val loss: 1.0555161237716675
Epoch 760, training loss: 437.0804443359375 = 1.0546104907989502 + 50.0 * 8.7205171585083
Epoch 760, val loss: 1.0553659200668335
Epoch 770, training loss: 438.0721130371094 = 1.05470871925354 + 50.0 * 8.740347862243652
Epoch 770, val loss: 1.0554914474487305
Epoch 780, training loss: 437.1805419921875 = 1.0546634197235107 + 50.0 * 8.722517967224121
Epoch 780, val loss: 1.055449366569519
Epoch 790, training loss: 437.16644287109375 = 1.0546684265136719 + 50.0 * 8.722235679626465
Epoch 790, val loss: 1.0554507970809937
Epoch 800, training loss: 437.6549987792969 = 1.0546867847442627 + 50.0 * 8.732006072998047
Epoch 800, val loss: 1.055467963218689
Epoch 810, training loss: 437.9400939941406 = 1.0546876192092896 + 50.0 * 8.73770809173584
Epoch 810, val loss: 1.0554713010787964
Epoch 820, training loss: 437.8586730957031 = 1.0546542406082153 + 50.0 * 8.736080169677734
Epoch 820, val loss: 1.0554462671279907
Epoch 830, training loss: 437.9659118652344 = 1.0546528100967407 + 50.0 * 8.738224983215332
Epoch 830, val loss: 1.055440068244934
Epoch 840, training loss: 438.1919250488281 = 1.0546456575393677 + 50.0 * 8.742745399475098
Epoch 840, val loss: 1.055440902709961
Epoch 850, training loss: 438.4244079589844 = 1.0546066761016846 + 50.0 * 8.747396469116211
Epoch 850, val loss: 1.0553865432739258
Epoch 860, training loss: 438.5751647949219 = 1.0546208620071411 + 50.0 * 8.750411033630371
Epoch 860, val loss: 1.055410385131836
Epoch 870, training loss: 438.4217529296875 = 1.0545833110809326 + 50.0 * 8.747343063354492
Epoch 870, val loss: 1.0553712844848633
Epoch 880, training loss: 438.60430908203125 = 1.0545737743377686 + 50.0 * 8.750994682312012
Epoch 880, val loss: 1.0553689002990723
Epoch 890, training loss: 438.6560363769531 = 1.054561734199524 + 50.0 * 8.752029418945312
Epoch 890, val loss: 1.0553538799285889
Epoch 900, training loss: 438.7031555175781 = 1.054548978805542 + 50.0 * 8.752972602844238
Epoch 900, val loss: 1.0553516149520874
Epoch 910, training loss: 438.8556213378906 = 1.0545330047607422 + 50.0 * 8.756021499633789
Epoch 910, val loss: 1.0553399324417114
Epoch 920, training loss: 439.12152099609375 = 1.054519534111023 + 50.0 * 8.761340141296387
Epoch 920, val loss: 1.0553148984909058
Epoch 930, training loss: 439.0570983886719 = 1.0545066595077515 + 50.0 * 8.760051727294922
Epoch 930, val loss: 1.0553098917007446
Epoch 940, training loss: 439.1739196777344 = 1.0544929504394531 + 50.0 * 8.762388229370117
Epoch 940, val loss: 1.0553009510040283
Epoch 950, training loss: 439.3604736328125 = 1.054480791091919 + 50.0 * 8.766119956970215
Epoch 950, val loss: 1.0552866458892822
Epoch 960, training loss: 439.3367919921875 = 1.0544580221176147 + 50.0 * 8.765646934509277
Epoch 960, val loss: 1.0552634000778198
Epoch 970, training loss: 439.4371032714844 = 1.0544359683990479 + 50.0 * 8.767653465270996
Epoch 970, val loss: 1.0552536249160767
Epoch 980, training loss: 439.4734802246094 = 1.0544188022613525 + 50.0 * 8.768381118774414
Epoch 980, val loss: 1.0552444458007812
Epoch 990, training loss: 439.5675048828125 = 1.0543982982635498 + 50.0 * 8.770261764526367
Epoch 990, val loss: 1.0552165508270264
Epoch 1000, training loss: 439.7248840332031 = 1.05439031124115 + 50.0 * 8.773409843444824
Epoch 1000, val loss: 1.0552074909210205
Epoch 1010, training loss: 439.81939697265625 = 1.0543769598007202 + 50.0 * 8.775300025939941
Epoch 1010, val loss: 1.0551962852478027
Epoch 1020, training loss: 439.2240295410156 = 1.0541077852249146 + 50.0 * 8.763398170471191
Epoch 1020, val loss: 1.0548884868621826
Epoch 1030, training loss: 440.49261474609375 = 1.054317593574524 + 50.0 * 8.788765907287598
Epoch 1030, val loss: 1.0551682710647583
Epoch 1040, training loss: 438.1410217285156 = 1.054119348526001 + 50.0 * 8.741738319396973
Epoch 1040, val loss: 1.0549606084823608
Epoch 1050, training loss: 438.50604248046875 = 1.0541603565216064 + 50.0 * 8.749037742614746
Epoch 1050, val loss: 1.0549895763397217
Epoch 1060, training loss: 438.9963684082031 = 1.054201602935791 + 50.0 * 8.758843421936035
Epoch 1060, val loss: 1.0550414323806763
Epoch 1070, training loss: 439.1463928222656 = 1.0541975498199463 + 50.0 * 8.76184368133545
Epoch 1070, val loss: 1.0550447702407837
Epoch 1080, training loss: 439.5829772949219 = 1.0542012453079224 + 50.0 * 8.770575523376465
Epoch 1080, val loss: 1.0550487041473389
Epoch 1090, training loss: 439.8599853515625 = 1.054192066192627 + 50.0 * 8.776115417480469
Epoch 1090, val loss: 1.0550408363342285
Epoch 1100, training loss: 439.96533203125 = 1.0541749000549316 + 50.0 * 8.778223037719727
Epoch 1100, val loss: 1.0550217628479004
Epoch 1110, training loss: 440.0042419433594 = 1.0541516542434692 + 50.0 * 8.77900218963623
Epoch 1110, val loss: 1.0549991130828857
Epoch 1120, training loss: 440.05865478515625 = 1.054134488105774 + 50.0 * 8.78009033203125
Epoch 1120, val loss: 1.0549845695495605
Epoch 1130, training loss: 440.2283630371094 = 1.0541216135025024 + 50.0 * 8.783485412597656
Epoch 1130, val loss: 1.0549721717834473
Epoch 1140, training loss: 440.37567138671875 = 1.0540993213653564 + 50.0 * 8.786431312561035
Epoch 1140, val loss: 1.0549596548080444
Epoch 1150, training loss: 440.2361755371094 = 1.054073452949524 + 50.0 * 8.783641815185547
Epoch 1150, val loss: 1.0549262762069702
Epoch 1160, training loss: 440.4836120605469 = 1.054060697555542 + 50.0 * 8.788591384887695
Epoch 1160, val loss: 1.0549170970916748
Epoch 1170, training loss: 440.7568359375 = 1.054047703742981 + 50.0 * 8.794055938720703
Epoch 1170, val loss: 1.0549061298370361
Epoch 1180, training loss: 440.7425537109375 = 1.0540179014205933 + 50.0 * 8.793770790100098
Epoch 1180, val loss: 1.0548763275146484
Epoch 1190, training loss: 440.334716796875 = 1.053778052330017 + 50.0 * 8.785618782043457
Epoch 1190, val loss: 1.054565191268921
Epoch 1200, training loss: 440.4547119140625 = 1.053879737854004 + 50.0 * 8.788016319274902
Epoch 1200, val loss: 1.054730772972107
Epoch 1210, training loss: 439.1622619628906 = 1.0537126064300537 + 50.0 * 8.762170791625977
Epoch 1210, val loss: 1.0545648336410522
Epoch 1220, training loss: 439.6258239746094 = 1.0537440776824951 + 50.0 * 8.771441459655762
Epoch 1220, val loss: 1.0545834302902222
Epoch 1230, training loss: 440.03253173828125 = 1.0537846088409424 + 50.0 * 8.77957534790039
Epoch 1230, val loss: 1.054636001586914
Epoch 1240, training loss: 440.24481201171875 = 1.05377995967865 + 50.0 * 8.783821105957031
Epoch 1240, val loss: 1.0546456575393677
Epoch 1250, training loss: 440.5877380371094 = 1.0537958145141602 + 50.0 * 8.790678977966309
Epoch 1250, val loss: 1.0546624660491943
Epoch 1260, training loss: 440.9231262207031 = 1.0537919998168945 + 50.0 * 8.797386169433594
Epoch 1260, val loss: 1.0546574592590332
Epoch 1270, training loss: 441.06878662109375 = 1.0537747144699097 + 50.0 * 8.800300598144531
Epoch 1270, val loss: 1.05464506149292
Epoch 1280, training loss: 441.2956237792969 = 1.05376136302948 + 50.0 * 8.804837226867676
Epoch 1280, val loss: 1.0546367168426514
Epoch 1290, training loss: 441.0756530761719 = 1.0537127256393433 + 50.0 * 8.80043888092041
Epoch 1290, val loss: 1.0545862913131714
Epoch 1300, training loss: 441.30279541015625 = 1.0536881685256958 + 50.0 * 8.80498218536377
Epoch 1300, val loss: 1.0545703172683716
Epoch 1310, training loss: 441.5025939941406 = 1.0536861419677734 + 50.0 * 8.808978080749512
Epoch 1310, val loss: 1.0545662641525269
Epoch 1320, training loss: 441.5065612792969 = 1.0536432266235352 + 50.0 * 8.80905818939209
Epoch 1320, val loss: 1.0545333623886108
Epoch 1330, training loss: 441.7059020996094 = 1.053623914718628 + 50.0 * 8.813045501708984
Epoch 1330, val loss: 1.054516315460205
Epoch 1340, training loss: 441.37646484375 = 1.0535424947738647 + 50.0 * 8.806458473205566
Epoch 1340, val loss: 1.0544415712356567
Epoch 1350, training loss: 441.5852966308594 = 1.0535459518432617 + 50.0 * 8.81063461303711
Epoch 1350, val loss: 1.0544425249099731
Epoch 1360, training loss: 441.8626403808594 = 1.0535355806350708 + 50.0 * 8.816182136535645
Epoch 1360, val loss: 1.0544381141662598
Epoch 1370, training loss: 442.2218017578125 = 1.053524136543274 + 50.0 * 8.823365211486816
Epoch 1370, val loss: 1.054426670074463
Epoch 1380, training loss: 442.10418701171875 = 1.0534799098968506 + 50.0 * 8.821014404296875
Epoch 1380, val loss: 1.0543830394744873
Epoch 1390, training loss: 442.3390197753906 = 1.053464651107788 + 50.0 * 8.825711250305176
Epoch 1390, val loss: 1.0543791055679321
Epoch 1400, training loss: 442.2868347167969 = 1.0534284114837646 + 50.0 * 8.824667930603027
Epoch 1400, val loss: 1.0543476343154907
Epoch 1410, training loss: 442.46051025390625 = 1.0534065961837769 + 50.0 * 8.828142166137695
Epoch 1410, val loss: 1.0543254613876343
Epoch 1420, training loss: 442.5315246582031 = 1.0533822774887085 + 50.0 * 8.82956314086914
Epoch 1420, val loss: 1.0543029308319092
Epoch 1430, training loss: 442.7423400878906 = 1.0533612966537476 + 50.0 * 8.833779335021973
Epoch 1430, val loss: 1.054290771484375
Epoch 1440, training loss: 442.6802978515625 = 1.053309440612793 + 50.0 * 8.832539558410645
Epoch 1440, val loss: 1.0542336702346802
Epoch 1450, training loss: 442.54840087890625 = 1.053270697593689 + 50.0 * 8.829902648925781
Epoch 1450, val loss: 1.0542069673538208
Epoch 1460, training loss: 442.73382568359375 = 1.0532609224319458 + 50.0 * 8.833611488342285
Epoch 1460, val loss: 1.0541974306106567
Epoch 1470, training loss: 442.9739074707031 = 1.0532476902008057 + 50.0 * 8.83841323852539
Epoch 1470, val loss: 1.054192066192627
Epoch 1480, training loss: 443.0613708496094 = 1.0532112121582031 + 50.0 * 8.840163230895996
Epoch 1480, val loss: 1.054151177406311
Epoch 1490, training loss: 443.0950012207031 = 1.0531705617904663 + 50.0 * 8.840836524963379
Epoch 1490, val loss: 1.0541208982467651
Epoch 1500, training loss: 443.1319274902344 = 1.0531394481658936 + 50.0 * 8.841575622558594
Epoch 1500, val loss: 1.054090142250061
Epoch 1510, training loss: 442.5753173828125 = 1.053040862083435 + 50.0 * 8.830445289611816
Epoch 1510, val loss: 1.0539908409118652
Epoch 1520, training loss: 442.54241943359375 = 1.0529968738555908 + 50.0 * 8.829788208007812
Epoch 1520, val loss: 1.0539714097976685
Epoch 1530, training loss: 442.4086608886719 = 1.0529165267944336 + 50.0 * 8.827115058898926
Epoch 1530, val loss: 1.0538707971572876
Epoch 1540, training loss: 442.6307373046875 = 1.0529122352600098 + 50.0 * 8.83155632019043
Epoch 1540, val loss: 1.0538800954818726
Epoch 1550, training loss: 442.8829040527344 = 1.0529019832611084 + 50.0 * 8.836600303649902
Epoch 1550, val loss: 1.0538884401321411
Epoch 1560, training loss: 443.1240234375 = 1.0529133081436157 + 50.0 * 8.841422080993652
Epoch 1560, val loss: 1.0538930892944336
Epoch 1570, training loss: 443.46392822265625 = 1.0528992414474487 + 50.0 * 8.848220825195312
Epoch 1570, val loss: 1.0538877248764038
Epoch 1580, training loss: 443.4920654296875 = 1.0528616905212402 + 50.0 * 8.848784446716309
Epoch 1580, val loss: 1.0538533926010132
Epoch 1590, training loss: 443.5592346191406 = 1.0528302192687988 + 50.0 * 8.850128173828125
Epoch 1590, val loss: 1.0538253784179688
Epoch 1600, training loss: 443.7099304199219 = 1.0527952909469604 + 50.0 * 8.853142738342285
Epoch 1600, val loss: 1.053798794746399
Epoch 1610, training loss: 443.82391357421875 = 1.0527606010437012 + 50.0 * 8.855422973632812
Epoch 1610, val loss: 1.0537704229354858
Epoch 1620, training loss: 443.9596252441406 = 1.0527386665344238 + 50.0 * 8.858138084411621
Epoch 1620, val loss: 1.053744912147522
Epoch 1630, training loss: 443.8011169433594 = 1.0526719093322754 + 50.0 * 8.854969024658203
Epoch 1630, val loss: 1.0536819696426392
Epoch 1640, training loss: 443.8713684082031 = 1.0526365041732788 + 50.0 * 8.856374740600586
Epoch 1640, val loss: 1.0536566972732544
Epoch 1650, training loss: 444.0594482421875 = 1.0526165962219238 + 50.0 * 8.860136985778809
Epoch 1650, val loss: 1.0536384582519531
Epoch 1660, training loss: 444.1087951660156 = 1.0525832176208496 + 50.0 * 8.861124038696289
Epoch 1660, val loss: 1.0536147356033325
Epoch 1670, training loss: 444.16796875 = 1.052541732788086 + 50.0 * 8.862308502197266
Epoch 1670, val loss: 1.0535778999328613
Epoch 1680, training loss: 444.15643310546875 = 1.0524908304214478 + 50.0 * 8.862078666687012
Epoch 1680, val loss: 1.053529977798462
Epoch 1690, training loss: 444.32623291015625 = 1.0524731874465942 + 50.0 * 8.86547565460205
Epoch 1690, val loss: 1.0535211563110352
Epoch 1700, training loss: 444.5506286621094 = 1.0524470806121826 + 50.0 * 8.869963645935059
Epoch 1700, val loss: 1.0534926652908325
Epoch 1710, training loss: 444.3685302734375 = 1.0523704290390015 + 50.0 * 8.866323471069336
Epoch 1710, val loss: 1.0534377098083496
Epoch 1720, training loss: 444.32568359375 = 1.0523321628570557 + 50.0 * 8.865467071533203
Epoch 1720, val loss: 1.0534001588821411
Epoch 1730, training loss: 444.578369140625 = 1.0523021221160889 + 50.0 * 8.870521545410156
Epoch 1730, val loss: 1.0533690452575684
Epoch 1740, training loss: 444.8150939941406 = 1.0522810220718384 + 50.0 * 8.875256538391113
Epoch 1740, val loss: 1.0533421039581299
Epoch 1750, training loss: 444.6034851074219 = 1.0522180795669556 + 50.0 * 8.871025085449219
Epoch 1750, val loss: 1.0532972812652588
Epoch 1760, training loss: 444.77716064453125 = 1.0521868467330933 + 50.0 * 8.874499320983887
Epoch 1760, val loss: 1.0532643795013428
Epoch 1770, training loss: 444.8973388671875 = 1.0521479845046997 + 50.0 * 8.876903533935547
Epoch 1770, val loss: 1.0532293319702148
Epoch 1780, training loss: 444.81329345703125 = 1.052095651626587 + 50.0 * 8.875224113464355
Epoch 1780, val loss: 1.053188681602478
Epoch 1790, training loss: 444.9723815917969 = 1.0520577430725098 + 50.0 * 8.878406524658203
Epoch 1790, val loss: 1.0531587600708008
Epoch 1800, training loss: 445.0065612792969 = 1.0520126819610596 + 50.0 * 8.879091262817383
Epoch 1800, val loss: 1.0531280040740967
Epoch 1810, training loss: 445.05206298828125 = 1.0519719123840332 + 50.0 * 8.88000202178955
Epoch 1810, val loss: 1.0530887842178345
Epoch 1820, training loss: 445.26165771484375 = 1.0519438982009888 + 50.0 * 8.884194374084473
Epoch 1820, val loss: 1.0530614852905273
Epoch 1830, training loss: 445.4119873046875 = 1.0518959760665894 + 50.0 * 8.887201309204102
Epoch 1830, val loss: 1.0530023574829102
Epoch 1840, training loss: 445.2579650878906 = 1.051814317703247 + 50.0 * 8.884122848510742
Epoch 1840, val loss: 1.0529370307922363
Epoch 1850, training loss: 445.1999816894531 = 1.051782488822937 + 50.0 * 8.882964134216309
Epoch 1850, val loss: 1.0529180765151978
Epoch 1860, training loss: 445.5323791503906 = 1.0517683029174805 + 50.0 * 8.889612197875977
Epoch 1860, val loss: 1.052890419960022
Epoch 1870, training loss: 445.10382080078125 = 1.0516268014907837 + 50.0 * 8.881043434143066
Epoch 1870, val loss: 1.0527704954147339
Epoch 1880, training loss: 445.12506103515625 = 1.0515729188919067 + 50.0 * 8.8814697265625
Epoch 1880, val loss: 1.0527139902114868
Epoch 1890, training loss: 444.7629699707031 = 1.0514814853668213 + 50.0 * 8.874229431152344
Epoch 1890, val loss: 1.0526384115219116
Epoch 1900, training loss: 445.0578308105469 = 1.0514715909957886 + 50.0 * 8.880126953125
Epoch 1900, val loss: 1.0526416301727295
Epoch 1910, training loss: 445.28515625 = 1.0514452457427979 + 50.0 * 8.884674072265625
Epoch 1910, val loss: 1.0526156425476074
Epoch 1920, training loss: 445.5838623046875 = 1.051439642906189 + 50.0 * 8.890647888183594
Epoch 1920, val loss: 1.0526208877563477
Epoch 1930, training loss: 445.7959289550781 = 1.051418662071228 + 50.0 * 8.894889831542969
Epoch 1930, val loss: 1.0525994300842285
Epoch 1940, training loss: 445.7369689941406 = 1.0513719320297241 + 50.0 * 8.893712043762207
Epoch 1940, val loss: 1.0525559186935425
Epoch 1950, training loss: 445.8466491699219 = 1.0513255596160889 + 50.0 * 8.895906448364258
Epoch 1950, val loss: 1.0525243282318115
Epoch 1960, training loss: 445.86328125 = 1.0512757301330566 + 50.0 * 8.896240234375
Epoch 1960, val loss: 1.0524866580963135
Epoch 1970, training loss: 446.06695556640625 = 1.051241159439087 + 50.0 * 8.900314331054688
Epoch 1970, val loss: 1.052439570426941
Epoch 1980, training loss: 446.16168212890625 = 1.0511938333511353 + 50.0 * 8.902209281921387
Epoch 1980, val loss: 1.0524095296859741
Epoch 1990, training loss: 446.17254638671875 = 1.0511457920074463 + 50.0 * 8.902427673339844
Epoch 1990, val loss: 1.052361249923706
Epoch 2000, training loss: 446.0990905761719 = 1.0510811805725098 + 50.0 * 8.900959968566895
Epoch 2000, val loss: 1.0523080825805664
Epoch 2010, training loss: 446.0462341308594 = 1.0510128736495972 + 50.0 * 8.899904251098633
Epoch 2010, val loss: 1.0522297620773315
Epoch 2020, training loss: 446.2274169921875 = 1.0509639978408813 + 50.0 * 8.903529167175293
Epoch 2020, val loss: 1.052206039428711
Epoch 2030, training loss: 446.3359069824219 = 1.050941824913025 + 50.0 * 8.905699729919434
Epoch 2030, val loss: 1.0521886348724365
Epoch 2040, training loss: 446.4696044921875 = 1.050907015800476 + 50.0 * 8.908373832702637
Epoch 2040, val loss: 1.0521565675735474
Epoch 2050, training loss: 446.3339538574219 = 1.0508121252059937 + 50.0 * 8.905662536621094
Epoch 2050, val loss: 1.052068829536438
Epoch 2060, training loss: 446.1700134277344 = 1.0507327318191528 + 50.0 * 8.902385711669922
Epoch 2060, val loss: 1.052003264427185
Epoch 2070, training loss: 446.12640380859375 = 1.0506783723831177 + 50.0 * 8.901514053344727
Epoch 2070, val loss: 1.0519577264785767
Epoch 2080, training loss: 446.4495544433594 = 1.0506550073623657 + 50.0 * 8.907978057861328
Epoch 2080, val loss: 1.051949143409729
Epoch 2090, training loss: 446.5534973144531 = 1.0506224632263184 + 50.0 * 8.910057067871094
Epoch 2090, val loss: 1.0519099235534668
Epoch 2100, training loss: 446.6435546875 = 1.0505720376968384 + 50.0 * 8.911859512329102
Epoch 2100, val loss: 1.0518540143966675
Epoch 2110, training loss: 446.60882568359375 = 1.0504993200302124 + 50.0 * 8.911166191101074
Epoch 2110, val loss: 1.0517914295196533
Epoch 2120, training loss: 446.6777038574219 = 1.0504344701766968 + 50.0 * 8.912545204162598
Epoch 2120, val loss: 1.0517383813858032
Epoch 2130, training loss: 446.8356628417969 = 1.0504090785980225 + 50.0 * 8.915704727172852
Epoch 2130, val loss: 1.0517175197601318
Epoch 2140, training loss: 447.00390625 = 1.0503785610198975 + 50.0 * 8.91907024383545
Epoch 2140, val loss: 1.051698088645935
Epoch 2150, training loss: 446.8388671875 = 1.0502827167510986 + 50.0 * 8.915771484375
Epoch 2150, val loss: 1.0515823364257812
Epoch 2160, training loss: 446.6684875488281 = 1.0501996278762817 + 50.0 * 8.912365913391113
Epoch 2160, val loss: 1.0515239238739014
Epoch 2170, training loss: 446.86090087890625 = 1.0501493215560913 + 50.0 * 8.916214942932129
Epoch 2170, val loss: 1.0514802932739258
Epoch 2180, training loss: 447.0443420410156 = 1.05012047290802 + 50.0 * 8.91988468170166
Epoch 2180, val loss: 1.0514640808105469
Epoch 2190, training loss: 447.35711669921875 = 1.0500987768173218 + 50.0 * 8.926139831542969
Epoch 2190, val loss: 1.0514408349990845
Epoch 2200, training loss: 447.0501708984375 = 1.0499777793884277 + 50.0 * 8.920003890991211
Epoch 2200, val loss: 1.0513463020324707
Epoch 2210, training loss: 447.1499938964844 = 1.049943447113037 + 50.0 * 8.922000885009766
Epoch 2210, val loss: 1.051304578781128
Epoch 2220, training loss: 447.3282165527344 = 1.0499001741409302 + 50.0 * 8.925566673278809
Epoch 2220, val loss: 1.0512560606002808
Epoch 2230, training loss: 447.2942810058594 = 1.0498260259628296 + 50.0 * 8.924888610839844
Epoch 2230, val loss: 1.0512365102767944
Epoch 2240, training loss: 447.0915832519531 = 1.049712896347046 + 50.0 * 8.92083740234375
Epoch 2240, val loss: 1.0511027574539185
Epoch 2250, training loss: 447.3009338378906 = 1.04967200756073 + 50.0 * 8.92502498626709
Epoch 2250, val loss: 1.0510591268539429
Epoch 2260, training loss: 447.4151306152344 = 1.0496302843093872 + 50.0 * 8.9273099899292
Epoch 2260, val loss: 1.0510278940200806
Epoch 2270, training loss: 447.5272216796875 = 1.0495750904083252 + 50.0 * 8.929553031921387
Epoch 2270, val loss: 1.0509874820709229
Epoch 2280, training loss: 447.2089538574219 = 1.0494440793991089 + 50.0 * 8.923190116882324
Epoch 2280, val loss: 1.0508623123168945
Epoch 2290, training loss: 446.75439453125 = 1.0492233037948608 + 50.0 * 8.914103507995605
Epoch 2290, val loss: 1.050654411315918
Epoch 2300, training loss: 447.0596618652344 = 1.0491636991500854 + 50.0 * 8.920209884643555
Epoch 2300, val loss: 1.0505437850952148
Epoch 2310, training loss: 447.306396484375 = 1.0491434335708618 + 50.0 * 8.925145149230957
Epoch 2310, val loss: 1.050585150718689
Epoch 2320, training loss: 447.2633056640625 = 1.0491050481796265 + 50.0 * 8.924283981323242
Epoch 2320, val loss: 1.0505380630493164
Epoch 2330, training loss: 447.6500244140625 = 1.0490957498550415 + 50.0 * 8.932018280029297
Epoch 2330, val loss: 1.0505362749099731
Epoch 2340, training loss: 448.1106872558594 = 1.0491029024124146 + 50.0 * 8.941231727600098
Epoch 2340, val loss: 1.0505462884902954
Epoch 2350, training loss: 448.3258972167969 = 1.0490877628326416 + 50.0 * 8.945535659790039
Epoch 2350, val loss: 1.0505374670028687
Epoch 2360, training loss: 448.0992126464844 = 1.04898202419281 + 50.0 * 8.941004753112793
Epoch 2360, val loss: 1.0504398345947266
Epoch 2370, training loss: 448.254638671875 = 1.0489470958709717 + 50.0 * 8.944113731384277
Epoch 2370, val loss: 1.0504084825515747
Epoch 2380, training loss: 448.5215759277344 = 1.0489217042922974 + 50.0 * 8.949453353881836
Epoch 2380, val loss: 1.050382137298584
Epoch 2390, training loss: 448.57073974609375 = 1.0488662719726562 + 50.0 * 8.950437545776367
Epoch 2390, val loss: 1.0503157377243042
Epoch 2400, training loss: 448.4452209472656 = 1.048742413520813 + 50.0 * 8.947929382324219
Epoch 2400, val loss: 1.0502253770828247
Epoch 2410, training loss: 448.5014953613281 = 1.0486950874328613 + 50.0 * 8.949055671691895
Epoch 2410, val loss: 1.0501739978790283
Epoch 2420, training loss: 448.6825256347656 = 1.0486518144607544 + 50.0 * 8.952677726745605
Epoch 2420, val loss: 1.0501453876495361
Epoch 2430, training loss: 448.5539855957031 = 1.0485512018203735 + 50.0 * 8.950108528137207
Epoch 2430, val loss: 1.0500580072402954
Epoch 2440, training loss: 448.5249328613281 = 1.0484877824783325 + 50.0 * 8.949528694152832
Epoch 2440, val loss: 1.0500035285949707
Epoch 2450, training loss: 448.68035888671875 = 1.048443078994751 + 50.0 * 8.952638626098633
Epoch 2450, val loss: 1.0499742031097412
Epoch 2460, training loss: 448.91961669921875 = 1.0484108924865723 + 50.0 * 8.95742416381836
Epoch 2460, val loss: 1.0499414205551147
Epoch 2470, training loss: 448.591552734375 = 1.048268437385559 + 50.0 * 8.950865745544434
Epoch 2470, val loss: 1.0498241186141968
Epoch 2480, training loss: 448.738525390625 = 1.0482275485992432 + 50.0 * 8.953805923461914
Epoch 2480, val loss: 1.0497723817825317
Epoch 2490, training loss: 449.045654296875 = 1.048200249671936 + 50.0 * 8.959949493408203
Epoch 2490, val loss: 1.0497548580169678
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.39956521739130435
0.8140983844091865
The final CL Acc:0.40816, 0.01309, The final GNN Acc:0.81335, 0.00091
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 111054])
remove edge: torch.Size([2, 66404])
updated graph: torch.Size([2, 88810])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 500.1907653808594 = 1.1181609630584717 + 50.0 * 9.981451988220215
Epoch 0, val loss: 1.116963267326355
Epoch 10, training loss: 475.736328125 = 1.1133136749267578 + 50.0 * 9.492460250854492
Epoch 10, val loss: 1.1122220754623413
Epoch 20, training loss: 467.7292785644531 = 1.108876347541809 + 50.0 * 9.33240795135498
Epoch 20, val loss: 1.1078256368637085
Epoch 30, training loss: 461.5635986328125 = 1.1046040058135986 + 50.0 * 9.209179878234863
Epoch 30, val loss: 1.1035999059677124
Epoch 40, training loss: 456.68572998046875 = 1.100576400756836 + 50.0 * 9.111702919006348
Epoch 40, val loss: 1.0996246337890625
Epoch 50, training loss: 452.7176208496094 = 1.0967851877212524 + 50.0 * 9.032417297363281
Epoch 50, val loss: 1.095881700515747
Epoch 60, training loss: 449.39495849609375 = 1.0932080745697021 + 50.0 * 8.966034889221191
Epoch 60, val loss: 1.0923488140106201
Epoch 70, training loss: 446.6561279296875 = 1.0898239612579346 + 50.0 * 8.91132640838623
Epoch 70, val loss: 1.0890120267868042
Epoch 80, training loss: 444.3541259765625 = 1.0866271257400513 + 50.0 * 8.865349769592285
Epoch 80, val loss: 1.0858638286590576
Epoch 90, training loss: 442.4102783203125 = 1.0836176872253418 + 50.0 * 8.826533317565918
Epoch 90, val loss: 1.0829010009765625
Epoch 100, training loss: 440.7729797363281 = 1.0808038711547852 + 50.0 * 8.793843269348145
Epoch 100, val loss: 1.0801422595977783
Epoch 110, training loss: 439.3095703125 = 1.078166127204895 + 50.0 * 8.764628410339355
Epoch 110, val loss: 1.0775552988052368
Epoch 120, training loss: 438.0387878417969 = 1.0757064819335938 + 50.0 * 8.739261627197266
Epoch 120, val loss: 1.0751521587371826
Epoch 130, training loss: 436.9196472167969 = 1.0734281539916992 + 50.0 * 8.716924667358398
Epoch 130, val loss: 1.0729286670684814
Epoch 140, training loss: 435.9421691894531 = 1.071319580078125 + 50.0 * 8.697417259216309
Epoch 140, val loss: 1.0708768367767334
Epoch 150, training loss: 435.13421630859375 = 1.0693836212158203 + 50.0 * 8.681296348571777
Epoch 150, val loss: 1.0690016746520996
Epoch 160, training loss: 434.40374755859375 = 1.0676027536392212 + 50.0 * 8.666723251342773
Epoch 160, val loss: 1.0672844648361206
Epoch 170, training loss: 433.8703308105469 = 1.0659934282302856 + 50.0 * 8.656086921691895
Epoch 170, val loss: 1.0657342672348022
Epoch 180, training loss: 433.29486083984375 = 1.0645325183868408 + 50.0 * 8.644606590270996
Epoch 180, val loss: 1.064339280128479
Epoch 190, training loss: 432.8194580078125 = 1.0632224082946777 + 50.0 * 8.635124206542969
Epoch 190, val loss: 1.0630924701690674
Epoch 200, training loss: 432.3773193359375 = 1.0620425939559937 + 50.0 * 8.62630558013916
Epoch 200, val loss: 1.0619767904281616
Epoch 210, training loss: 432.09429931640625 = 1.060984492301941 + 50.0 * 8.62066650390625
Epoch 210, val loss: 1.0609915256500244
Epoch 220, training loss: 431.7356262207031 = 1.0600720643997192 + 50.0 * 8.613511085510254
Epoch 220, val loss: 1.0601356029510498
Epoch 230, training loss: 431.30255126953125 = 1.0592848062515259 + 50.0 * 8.604865074157715
Epoch 230, val loss: 1.0594135522842407
Epoch 240, training loss: 431.02093505859375 = 1.0585887432098389 + 50.0 * 8.599246978759766
Epoch 240, val loss: 1.0587769746780396
Epoch 250, training loss: 430.72393798828125 = 1.0579887628555298 + 50.0 * 8.593318939208984
Epoch 250, val loss: 1.0582365989685059
Epoch 260, training loss: 430.5762634277344 = 1.057477593421936 + 50.0 * 8.590375900268555
Epoch 260, val loss: 1.057784080505371
Epoch 270, training loss: 430.4534606933594 = 1.0570513010025024 + 50.0 * 8.587928771972656
Epoch 270, val loss: 1.0574102401733398
Epoch 280, training loss: 430.39892578125 = 1.0566924810409546 + 50.0 * 8.586844444274902
Epoch 280, val loss: 1.0570992231369019
Epoch 290, training loss: 430.3930358886719 = 1.056386113166809 + 50.0 * 8.586732864379883
Epoch 290, val loss: 1.0568405389785767
Epoch 300, training loss: 430.3185729980469 = 1.056130051612854 + 50.0 * 8.585248947143555
Epoch 300, val loss: 1.0566279888153076
Epoch 310, training loss: 430.24163818359375 = 1.0559163093566895 + 50.0 * 8.583714485168457
Epoch 310, val loss: 1.0564501285552979
Epoch 320, training loss: 430.2809753417969 = 1.0557494163513184 + 50.0 * 8.584504127502441
Epoch 320, val loss: 1.0563240051269531
Epoch 330, training loss: 430.1823425292969 = 1.055609107017517 + 50.0 * 8.582534790039062
Epoch 330, val loss: 1.0562108755111694
Epoch 340, training loss: 430.2192687988281 = 1.0554859638214111 + 50.0 * 8.58327579498291
Epoch 340, val loss: 1.0561180114746094
Epoch 350, training loss: 430.1663818359375 = 1.0553853511810303 + 50.0 * 8.582220077514648
Epoch 350, val loss: 1.0560458898544312
Epoch 360, training loss: 429.99615478515625 = 1.055310845375061 + 50.0 * 8.578817367553711
Epoch 360, val loss: 1.0559958219528198
Epoch 370, training loss: 430.0751647949219 = 1.0552494525909424 + 50.0 * 8.580398559570312
Epoch 370, val loss: 1.0559552907943726
Epoch 380, training loss: 430.0331726074219 = 1.055199384689331 + 50.0 * 8.579559326171875
Epoch 380, val loss: 1.0559176206588745
Epoch 390, training loss: 430.0798034667969 = 1.0551456212997437 + 50.0 * 8.580492973327637
Epoch 390, val loss: 1.0558862686157227
Epoch 400, training loss: 430.12652587890625 = 1.055119276046753 + 50.0 * 8.581428527832031
Epoch 400, val loss: 1.0558757781982422
Epoch 410, training loss: 430.1176452636719 = 1.0550894737243652 + 50.0 * 8.58125114440918
Epoch 410, val loss: 1.0558582544326782
Epoch 420, training loss: 430.04656982421875 = 1.0550540685653687 + 50.0 * 8.579830169677734
Epoch 420, val loss: 1.0558338165283203
Epoch 430, training loss: 430.0434265136719 = 1.0550296306610107 + 50.0 * 8.579768180847168
Epoch 430, val loss: 1.0558292865753174
Epoch 440, training loss: 430.1540832519531 = 1.054998755455017 + 50.0 * 8.581981658935547
Epoch 440, val loss: 1.0557971000671387
Epoch 450, training loss: 430.2679138183594 = 1.054902195930481 + 50.0 * 8.584259986877441
Epoch 450, val loss: 1.0557047128677368
Epoch 460, training loss: 429.1808166503906 = 1.054726481437683 + 50.0 * 8.562521934509277
Epoch 460, val loss: 1.055565357208252
Epoch 470, training loss: 430.9969482421875 = 1.0549488067626953 + 50.0 * 8.59883975982666
Epoch 470, val loss: 1.0557770729064941
Epoch 480, training loss: 429.3519287109375 = 1.0548609495162964 + 50.0 * 8.565940856933594
Epoch 480, val loss: 1.055681824684143
Epoch 490, training loss: 429.438232421875 = 1.0548566579818726 + 50.0 * 8.567667007446289
Epoch 490, val loss: 1.055694341659546
Epoch 500, training loss: 429.7919921875 = 1.0548721551895142 + 50.0 * 8.574742317199707
Epoch 500, val loss: 1.055702805519104
Epoch 510, training loss: 429.9493408203125 = 1.0548655986785889 + 50.0 * 8.577889442443848
Epoch 510, val loss: 1.055701494216919
Epoch 520, training loss: 430.1485290527344 = 1.0548568964004517 + 50.0 * 8.581872940063477
Epoch 520, val loss: 1.0556914806365967
Epoch 530, training loss: 430.23895263671875 = 1.0548467636108398 + 50.0 * 8.5836820602417
Epoch 530, val loss: 1.0556875467300415
Epoch 540, training loss: 430.36883544921875 = 1.0548291206359863 + 50.0 * 8.58627986907959
Epoch 540, val loss: 1.0556684732437134
Epoch 550, training loss: 430.3620300292969 = 1.0548125505447388 + 50.0 * 8.58614444732666
Epoch 550, val loss: 1.0556553602218628
Epoch 560, training loss: 430.5924072265625 = 1.0548017024993896 + 50.0 * 8.590751647949219
Epoch 560, val loss: 1.0556443929672241
Epoch 570, training loss: 430.6090393066406 = 1.0547906160354614 + 50.0 * 8.591085433959961
Epoch 570, val loss: 1.0556367635726929
Epoch 580, training loss: 430.72406005859375 = 1.0547723770141602 + 50.0 * 8.593385696411133
Epoch 580, val loss: 1.0556188821792603
Epoch 590, training loss: 430.632568359375 = 1.0547537803649902 + 50.0 * 8.591556549072266
Epoch 590, val loss: 1.0556129217147827
Epoch 600, training loss: 430.69952392578125 = 1.0547336339950562 + 50.0 * 8.5928955078125
Epoch 600, val loss: 1.0555921792984009
Epoch 610, training loss: 430.9324951171875 = 1.054735541343689 + 50.0 * 8.597555160522461
Epoch 610, val loss: 1.055590033531189
Epoch 620, training loss: 430.95904541015625 = 1.054732084274292 + 50.0 * 8.5980863571167
Epoch 620, val loss: 1.0555803775787354
Epoch 630, training loss: 431.1585998535156 = 1.0547215938568115 + 50.0 * 8.60207748413086
Epoch 630, val loss: 1.0555752515792847
Epoch 640, training loss: 431.25592041015625 = 1.0547016859054565 + 50.0 * 8.604024887084961
Epoch 640, val loss: 1.0555598735809326
Epoch 650, training loss: 431.3370056152344 = 1.054699182510376 + 50.0 * 8.605646133422852
Epoch 650, val loss: 1.0555485486984253
Epoch 660, training loss: 431.4224548339844 = 1.0546969175338745 + 50.0 * 8.607355117797852
Epoch 660, val loss: 1.0555484294891357
Epoch 670, training loss: 431.5743713378906 = 1.0546875 + 50.0 * 8.610393524169922
Epoch 670, val loss: 1.055538296699524
Epoch 680, training loss: 431.53790283203125 = 1.0546561479568481 + 50.0 * 8.609664916992188
Epoch 680, val loss: 1.055509328842163
Epoch 690, training loss: 431.67132568359375 = 1.054660439491272 + 50.0 * 8.612333297729492
Epoch 690, val loss: 1.0555133819580078
Epoch 700, training loss: 431.855224609375 = 1.05465567111969 + 50.0 * 8.616011619567871
Epoch 700, val loss: 1.055511236190796
Epoch 710, training loss: 432.0718994140625 = 1.0546499490737915 + 50.0 * 8.620345115661621
Epoch 710, val loss: 1.0555061101913452
Epoch 720, training loss: 431.93780517578125 = 1.0546289682388306 + 50.0 * 8.617663383483887
Epoch 720, val loss: 1.0554825067520142
Epoch 730, training loss: 432.14251708984375 = 1.0546258687973022 + 50.0 * 8.621757507324219
Epoch 730, val loss: 1.0554884672164917
Epoch 740, training loss: 432.111083984375 = 1.054612398147583 + 50.0 * 8.621129035949707
Epoch 740, val loss: 1.0554780960083008
Epoch 750, training loss: 432.3114013671875 = 1.0546069145202637 + 50.0 * 8.625136375427246
Epoch 750, val loss: 1.0554686784744263
Epoch 760, training loss: 432.36724853515625 = 1.0545846223831177 + 50.0 * 8.626253128051758
Epoch 760, val loss: 1.0554527044296265
Epoch 770, training loss: 432.4180603027344 = 1.0545731782913208 + 50.0 * 8.627269744873047
Epoch 770, val loss: 1.0554437637329102
Epoch 780, training loss: 432.6334533691406 = 1.0545687675476074 + 50.0 * 8.631577491760254
Epoch 780, val loss: 1.05543851852417
Epoch 790, training loss: 432.462890625 = 1.054535150527954 + 50.0 * 8.628167152404785
Epoch 790, val loss: 1.0554172992706299
Epoch 800, training loss: 432.58349609375 = 1.0545324087142944 + 50.0 * 8.630578994750977
Epoch 800, val loss: 1.0554155111312866
Epoch 810, training loss: 432.8473815917969 = 1.0545408725738525 + 50.0 * 8.635856628417969
Epoch 810, val loss: 1.0554139614105225
Epoch 820, training loss: 432.8326416015625 = 1.0545196533203125 + 50.0 * 8.6355619430542
Epoch 820, val loss: 1.0554007291793823
Epoch 830, training loss: 432.9016418457031 = 1.054504632949829 + 50.0 * 8.636942863464355
Epoch 830, val loss: 1.0553807020187378
Epoch 840, training loss: 432.8760986328125 = 1.054479718208313 + 50.0 * 8.636432647705078
Epoch 840, val loss: 1.0553585290908813
Epoch 850, training loss: 432.99713134765625 = 1.0544662475585938 + 50.0 * 8.638853073120117
Epoch 850, val loss: 1.055352807044983
Epoch 860, training loss: 433.2712097167969 = 1.0544674396514893 + 50.0 * 8.64433479309082
Epoch 860, val loss: 1.0553497076034546
Epoch 870, training loss: 433.5217590332031 = 1.054458498954773 + 50.0 * 8.649346351623535
Epoch 870, val loss: 1.0553216934204102
Epoch 880, training loss: 433.0964660644531 = 1.0543946027755737 + 50.0 * 8.640841484069824
Epoch 880, val loss: 1.0552715063095093
Epoch 890, training loss: 433.14447021484375 = 1.0543962717056274 + 50.0 * 8.641801834106445
Epoch 890, val loss: 1.0552780628204346
Epoch 900, training loss: 433.3785400390625 = 1.054397463798523 + 50.0 * 8.646483421325684
Epoch 900, val loss: 1.0552891492843628
Epoch 910, training loss: 433.640380859375 = 1.0543971061706543 + 50.0 * 8.65172004699707
Epoch 910, val loss: 1.0552836656570435
Epoch 920, training loss: 434.0254821777344 = 1.0543748140335083 + 50.0 * 8.659421920776367
Epoch 920, val loss: 1.055245280265808
Epoch 930, training loss: 434.19915771484375 = 1.0543566942214966 + 50.0 * 8.662896156311035
Epoch 930, val loss: 1.0552550554275513
Epoch 940, training loss: 433.8879699707031 = 1.0543034076690674 + 50.0 * 8.656673431396484
Epoch 940, val loss: 1.0551996231079102
Epoch 950, training loss: 433.7845458984375 = 1.0542984008789062 + 50.0 * 8.6546049118042
Epoch 950, val loss: 1.0551944971084595
Epoch 960, training loss: 433.874267578125 = 1.054280400276184 + 50.0 * 8.656399726867676
Epoch 960, val loss: 1.0551716089248657
Epoch 970, training loss: 433.8508605957031 = 1.0542831420898438 + 50.0 * 8.65593147277832
Epoch 970, val loss: 1.0551749467849731
Epoch 980, training loss: 434.1183776855469 = 1.054290533065796 + 50.0 * 8.66128158569336
Epoch 980, val loss: 1.055181622505188
Epoch 990, training loss: 434.2497253417969 = 1.0542815923690796 + 50.0 * 8.663908958435059
Epoch 990, val loss: 1.0551851987838745
Epoch 1000, training loss: 434.4471435546875 = 1.0542813539505005 + 50.0 * 8.66785717010498
Epoch 1000, val loss: 1.0551793575286865
Epoch 1010, training loss: 434.5853576660156 = 1.0542727708816528 + 50.0 * 8.670621871948242
Epoch 1010, val loss: 1.055167317390442
Epoch 1020, training loss: 434.5071716308594 = 1.0542421340942383 + 50.0 * 8.669058799743652
Epoch 1020, val loss: 1.0551506280899048
Epoch 1030, training loss: 434.72332763671875 = 1.054242730140686 + 50.0 * 8.673381805419922
Epoch 1030, val loss: 1.0551443099975586
Epoch 1040, training loss: 434.7670593261719 = 1.0542254447937012 + 50.0 * 8.674256324768066
Epoch 1040, val loss: 1.0551310777664185
Epoch 1050, training loss: 434.9170837402344 = 1.054213047027588 + 50.0 * 8.677257537841797
Epoch 1050, val loss: 1.0551260709762573
Epoch 1060, training loss: 434.9911804199219 = 1.0541913509368896 + 50.0 * 8.678739547729492
Epoch 1060, val loss: 1.055108666419983
Epoch 1070, training loss: 435.0713806152344 = 1.0541490316390991 + 50.0 * 8.680344581604004
Epoch 1070, val loss: 1.0550402402877808
Epoch 1080, training loss: 435.0119934082031 = 1.0541467666625977 + 50.0 * 8.679157257080078
Epoch 1080, val loss: 1.0550541877746582
Epoch 1090, training loss: 435.121337890625 = 1.0541417598724365 + 50.0 * 8.681344032287598
Epoch 1090, val loss: 1.0550646781921387
Epoch 1100, training loss: 435.2926330566406 = 1.0541365146636963 + 50.0 * 8.684769630432129
Epoch 1100, val loss: 1.0550565719604492
Epoch 1110, training loss: 435.05792236328125 = 1.0540884733200073 + 50.0 * 8.680076599121094
Epoch 1110, val loss: 1.0550179481506348
Epoch 1120, training loss: 435.1556701660156 = 1.054079294204712 + 50.0 * 8.682031631469727
Epoch 1120, val loss: 1.055010437965393
Epoch 1130, training loss: 435.5185852050781 = 1.0541378259658813 + 50.0 * 8.689289093017578
Epoch 1130, val loss: 1.0550816059112549
Epoch 1140, training loss: 435.61767578125 = 1.0541249513626099 + 50.0 * 8.69127082824707
Epoch 1140, val loss: 1.0550596714019775
Epoch 1150, training loss: 435.7158508300781 = 1.054132103919983 + 50.0 * 8.69323444366455
Epoch 1150, val loss: 1.0550661087036133
Epoch 1160, training loss: 436.0140380859375 = 1.0541267395019531 + 50.0 * 8.699197769165039
Epoch 1160, val loss: 1.055059790611267
Epoch 1170, training loss: 436.16510009765625 = 1.054121971130371 + 50.0 * 8.70221996307373
Epoch 1170, val loss: 1.0550590753555298
Epoch 1180, training loss: 436.11895751953125 = 1.0540971755981445 + 50.0 * 8.70129680633545
Epoch 1180, val loss: 1.055031657218933
Epoch 1190, training loss: 435.9371337890625 = 1.0540626049041748 + 50.0 * 8.697661399841309
Epoch 1190, val loss: 1.0550111532211304
Epoch 1200, training loss: 436.2271728515625 = 1.0540560483932495 + 50.0 * 8.703462600708008
Epoch 1200, val loss: 1.0550023317337036
Epoch 1210, training loss: 436.3002624511719 = 1.054064393043518 + 50.0 * 8.704923629760742
Epoch 1210, val loss: 1.055007815361023
Epoch 1220, training loss: 436.4854736328125 = 1.0540467500686646 + 50.0 * 8.70862865447998
Epoch 1220, val loss: 1.054998517036438
Epoch 1230, training loss: 436.4937438964844 = 1.0540287494659424 + 50.0 * 8.708794593811035
Epoch 1230, val loss: 1.054980993270874
Epoch 1240, training loss: 436.6171875 = 1.0540146827697754 + 50.0 * 8.711263656616211
Epoch 1240, val loss: 1.054970383644104
Epoch 1250, training loss: 436.73284912109375 = 1.0539954900741577 + 50.0 * 8.713577270507812
Epoch 1250, val loss: 1.0549545288085938
Epoch 1260, training loss: 436.7561340332031 = 1.0539857149124146 + 50.0 * 8.714042663574219
Epoch 1260, val loss: 1.0549432039260864
Epoch 1270, training loss: 437.0101623535156 = 1.053971290588379 + 50.0 * 8.719123840332031
Epoch 1270, val loss: 1.0549317598342896
Epoch 1280, training loss: 436.8551330566406 = 1.0539321899414062 + 50.0 * 8.716024398803711
Epoch 1280, val loss: 1.054898977279663
Epoch 1290, training loss: 436.6143798828125 = 1.0538787841796875 + 50.0 * 8.711210250854492
Epoch 1290, val loss: 1.0548505783081055
Epoch 1300, training loss: 436.6292419433594 = 1.053863763809204 + 50.0 * 8.711507797241211
Epoch 1300, val loss: 1.0548409223556519
Epoch 1310, training loss: 436.8755798339844 = 1.0538558959960938 + 50.0 * 8.716434478759766
Epoch 1310, val loss: 1.0548337697982788
Epoch 1320, training loss: 436.9393615722656 = 1.0538235902786255 + 50.0 * 8.717710494995117
Epoch 1320, val loss: 1.054800271987915
Epoch 1330, training loss: 437.0147705078125 = 1.0538145303726196 + 50.0 * 8.719219207763672
Epoch 1330, val loss: 1.054807424545288
Epoch 1340, training loss: 437.22528076171875 = 1.0538266897201538 + 50.0 * 8.723428726196289
Epoch 1340, val loss: 1.054814100265503
Epoch 1350, training loss: 437.4452819824219 = 1.053824543952942 + 50.0 * 8.727828979492188
Epoch 1350, val loss: 1.0548137426376343
Epoch 1360, training loss: 437.4721984863281 = 1.053807020187378 + 50.0 * 8.728367805480957
Epoch 1360, val loss: 1.0547934770584106
Epoch 1370, training loss: 437.54022216796875 = 1.0537811517715454 + 50.0 * 8.729728698730469
Epoch 1370, val loss: 1.0547714233398438
Epoch 1380, training loss: 437.5142822265625 = 1.0537620782852173 + 50.0 * 8.729209899902344
Epoch 1380, val loss: 1.0547608137130737
Epoch 1390, training loss: 437.72576904296875 = 1.0537500381469727 + 50.0 * 8.733440399169922
Epoch 1390, val loss: 1.054753065109253
Epoch 1400, training loss: 437.5986633300781 = 1.0537177324295044 + 50.0 * 8.7308988571167
Epoch 1400, val loss: 1.054726004600525
Epoch 1410, training loss: 437.869873046875 = 1.0537136793136597 + 50.0 * 8.736323356628418
Epoch 1410, val loss: 1.054725170135498
Epoch 1420, training loss: 437.7276611328125 = 1.0536603927612305 + 50.0 * 8.733480453491211
Epoch 1420, val loss: 1.0546587705612183
Epoch 1430, training loss: 437.3399658203125 = 1.05362069606781 + 50.0 * 8.725727081298828
Epoch 1430, val loss: 1.0546114444732666
Epoch 1440, training loss: 437.149169921875 = 1.0535728931427002 + 50.0 * 8.721912384033203
Epoch 1440, val loss: 1.0545904636383057
Epoch 1450, training loss: 437.37060546875 = 1.053574800491333 + 50.0 * 8.726340293884277
Epoch 1450, val loss: 1.0546090602874756
Epoch 1460, training loss: 437.7399597167969 = 1.0535786151885986 + 50.0 * 8.73372745513916
Epoch 1460, val loss: 1.0546144247055054
Epoch 1470, training loss: 438.001708984375 = 1.0535788536071777 + 50.0 * 8.738962173461914
Epoch 1470, val loss: 1.0546215772628784
Epoch 1480, training loss: 438.1740417480469 = 1.0535714626312256 + 50.0 * 8.742409706115723
Epoch 1480, val loss: 1.0546119213104248
Epoch 1490, training loss: 438.1947021484375 = 1.0535446405410767 + 50.0 * 8.742822647094727
Epoch 1490, val loss: 1.0545897483825684
Epoch 1500, training loss: 438.2465515136719 = 1.0535248517990112 + 50.0 * 8.743860244750977
Epoch 1500, val loss: 1.0545799732208252
Epoch 1510, training loss: 438.4082946777344 = 1.0535157918930054 + 50.0 * 8.747095108032227
Epoch 1510, val loss: 1.0545680522918701
Epoch 1520, training loss: 438.5781555175781 = 1.0534968376159668 + 50.0 * 8.750493049621582
Epoch 1520, val loss: 1.0545551776885986
Epoch 1530, training loss: 438.5834045410156 = 1.0534687042236328 + 50.0 * 8.750598907470703
Epoch 1530, val loss: 1.0545356273651123
Epoch 1540, training loss: 438.2249755859375 = 1.0534156560897827 + 50.0 * 8.743431091308594
Epoch 1540, val loss: 1.0544859170913696
Epoch 1550, training loss: 437.8167419433594 = 1.0533195734024048 + 50.0 * 8.735268592834473
Epoch 1550, val loss: 1.0543957948684692
Epoch 1560, training loss: 438.136474609375 = 1.0533194541931152 + 50.0 * 8.741662979125977
Epoch 1560, val loss: 1.0544072389602661
Epoch 1570, training loss: 438.41851806640625 = 1.053324818611145 + 50.0 * 8.74730396270752
Epoch 1570, val loss: 1.054418683052063
Epoch 1580, training loss: 438.65545654296875 = 1.0533294677734375 + 50.0 * 8.752042770385742
Epoch 1580, val loss: 1.0544229745864868
Epoch 1590, training loss: 438.5318908691406 = 1.0533043146133423 + 50.0 * 8.749571800231934
Epoch 1590, val loss: 1.0544098615646362
Epoch 1600, training loss: 438.67047119140625 = 1.0533032417297363 + 50.0 * 8.75234317779541
Epoch 1600, val loss: 1.05440354347229
Epoch 1610, training loss: 438.8430480957031 = 1.053291916847229 + 50.0 * 8.7557954788208
Epoch 1610, val loss: 1.0543932914733887
Epoch 1620, training loss: 438.7302551269531 = 1.053243637084961 + 50.0 * 8.7535400390625
Epoch 1620, val loss: 1.0543678998947144
Epoch 1630, training loss: 438.9193115234375 = 1.0532411336898804 + 50.0 * 8.75732135772705
Epoch 1630, val loss: 1.054351806640625
Epoch 1640, training loss: 438.9225769042969 = 1.053213119506836 + 50.0 * 8.757387161254883
Epoch 1640, val loss: 1.054328441619873
Epoch 1650, training loss: 438.9166564941406 = 1.0531647205352783 + 50.0 * 8.757269859313965
Epoch 1650, val loss: 1.054284930229187
Epoch 1660, training loss: 439.0122375488281 = 1.0531458854675293 + 50.0 * 8.75918197631836
Epoch 1660, val loss: 1.0542761087417603
Epoch 1670, training loss: 439.0967712402344 = 1.0531293153762817 + 50.0 * 8.760872840881348
Epoch 1670, val loss: 1.05426824092865
Epoch 1680, training loss: 437.19830322265625 = 1.0527535676956177 + 50.0 * 8.72291088104248
Epoch 1680, val loss: 1.0538842678070068
Epoch 1690, training loss: 438.9387512207031 = 1.0529859066009521 + 50.0 * 8.757715225219727
Epoch 1690, val loss: 1.0540934801101685
Epoch 1700, training loss: 437.9041442871094 = 1.0528147220611572 + 50.0 * 8.73702621459961
Epoch 1700, val loss: 1.0539984703063965
Epoch 1710, training loss: 438.14288330078125 = 1.0528411865234375 + 50.0 * 8.741801261901855
Epoch 1710, val loss: 1.0540212392807007
Epoch 1720, training loss: 438.541259765625 = 1.0528757572174072 + 50.0 * 8.749767303466797
Epoch 1720, val loss: 1.0540375709533691
Epoch 1730, training loss: 438.9062194824219 = 1.0529001951217651 + 50.0 * 8.75706672668457
Epoch 1730, val loss: 1.0540636777877808
Epoch 1740, training loss: 439.0350646972656 = 1.0528936386108398 + 50.0 * 8.7596435546875
Epoch 1740, val loss: 1.0540639162063599
Epoch 1750, training loss: 439.1877136230469 = 1.0528883934020996 + 50.0 * 8.762696266174316
Epoch 1750, val loss: 1.0540571212768555
Epoch 1760, training loss: 439.2611389160156 = 1.0528652667999268 + 50.0 * 8.764165878295898
Epoch 1760, val loss: 1.0540356636047363
Epoch 1770, training loss: 439.37078857421875 = 1.0528323650360107 + 50.0 * 8.766359329223633
Epoch 1770, val loss: 1.0540050268173218
Epoch 1780, training loss: 439.3865661621094 = 1.0528136491775513 + 50.0 * 8.766674995422363
Epoch 1780, val loss: 1.0539945363998413
Epoch 1790, training loss: 439.53082275390625 = 1.0527886152267456 + 50.0 * 8.769560813903809
Epoch 1790, val loss: 1.0539753437042236
Epoch 1800, training loss: 439.6397705078125 = 1.052767038345337 + 50.0 * 8.771739959716797
Epoch 1800, val loss: 1.0539588928222656
Epoch 1810, training loss: 439.7156982421875 = 1.052742838859558 + 50.0 * 8.773259162902832
Epoch 1810, val loss: 1.0539368391036987
Epoch 1820, training loss: 439.6983642578125 = 1.0527039766311646 + 50.0 * 8.772912979125977
Epoch 1820, val loss: 1.053899884223938
Epoch 1830, training loss: 439.7809753417969 = 1.0526490211486816 + 50.0 * 8.774566650390625
Epoch 1830, val loss: 1.0538568496704102
Epoch 1840, training loss: 439.7709655761719 = 1.0526297092437744 + 50.0 * 8.77436637878418
Epoch 1840, val loss: 1.053846001625061
Epoch 1850, training loss: 440.1191711425781 = 1.0526221990585327 + 50.0 * 8.781331062316895
Epoch 1850, val loss: 1.0538486242294312
Epoch 1860, training loss: 440.11669921875 = 1.0525965690612793 + 50.0 * 8.781282424926758
Epoch 1860, val loss: 1.0538218021392822
Epoch 1870, training loss: 440.1587829589844 = 1.0525596141815186 + 50.0 * 8.782124519348145
Epoch 1870, val loss: 1.0537950992584229
Epoch 1880, training loss: 440.2740783691406 = 1.0525286197662354 + 50.0 * 8.784431457519531
Epoch 1880, val loss: 1.0537718534469604
Epoch 1890, training loss: 440.27716064453125 = 1.0524979829788208 + 50.0 * 8.784493446350098
Epoch 1890, val loss: 1.0537374019622803
Epoch 1900, training loss: 440.5447998046875 = 1.0524873733520508 + 50.0 * 8.789846420288086
Epoch 1900, val loss: 1.0537306070327759
Epoch 1910, training loss: 440.61810302734375 = 1.052441954612732 + 50.0 * 8.791313171386719
Epoch 1910, val loss: 1.053694248199463
Epoch 1920, training loss: 440.4381408691406 = 1.0523868799209595 + 50.0 * 8.787714958190918
Epoch 1920, val loss: 1.05364191532135
Epoch 1930, training loss: 440.6119689941406 = 1.0523724555969238 + 50.0 * 8.791192054748535
Epoch 1930, val loss: 1.0536344051361084
Epoch 1940, training loss: 440.97259521484375 = 1.052357792854309 + 50.0 * 8.798404693603516
Epoch 1940, val loss: 1.0536264181137085
Epoch 1950, training loss: 440.78253173828125 = 1.0522960424423218 + 50.0 * 8.794604301452637
Epoch 1950, val loss: 1.0535688400268555
Epoch 1960, training loss: 440.9668884277344 = 1.0522830486297607 + 50.0 * 8.79829216003418
Epoch 1960, val loss: 1.0535621643066406
Epoch 1970, training loss: 441.2353515625 = 1.0522791147232056 + 50.0 * 8.803661346435547
Epoch 1970, val loss: 1.0535598993301392
Epoch 1980, training loss: 441.2259521484375 = 1.0522358417510986 + 50.0 * 8.803474426269531
Epoch 1980, val loss: 1.0535168647766113
Epoch 1990, training loss: 441.13360595703125 = 1.0521905422210693 + 50.0 * 8.801628112792969
Epoch 1990, val loss: 1.0534781217575073
Epoch 2000, training loss: 441.15411376953125 = 1.052156925201416 + 50.0 * 8.80203914642334
Epoch 2000, val loss: 1.0534613132476807
Epoch 2010, training loss: 441.3334045410156 = 1.0521398782730103 + 50.0 * 8.805624961853027
Epoch 2010, val loss: 1.0534435510635376
Epoch 2020, training loss: 441.1183776855469 = 1.0520453453063965 + 50.0 * 8.801326751708984
Epoch 2020, val loss: 1.053382396697998
Epoch 2030, training loss: 441.3283996582031 = 1.0520554780960083 + 50.0 * 8.805526733398438
Epoch 2030, val loss: 1.0533643960952759
Epoch 2040, training loss: 441.5531921386719 = 1.05203115940094 + 50.0 * 8.810023307800293
Epoch 2040, val loss: 1.0533660650253296
Epoch 2050, training loss: 441.46954345703125 = 1.0519909858703613 + 50.0 * 8.808350563049316
Epoch 2050, val loss: 1.053321123123169
Epoch 2060, training loss: 441.48114013671875 = 1.0519437789916992 + 50.0 * 8.808584213256836
Epoch 2060, val loss: 1.053283452987671
Epoch 2070, training loss: 441.52484130859375 = 1.0519095659255981 + 50.0 * 8.80945873260498
Epoch 2070, val loss: 1.0532523393630981
Epoch 2080, training loss: 441.62896728515625 = 1.0518871545791626 + 50.0 * 8.811541557312012
Epoch 2080, val loss: 1.0532374382019043
Epoch 2090, training loss: 441.792236328125 = 1.0518628358840942 + 50.0 * 8.814807891845703
Epoch 2090, val loss: 1.0532171726226807
Epoch 2100, training loss: 441.8260803222656 = 1.05182683467865 + 50.0 * 8.815485000610352
Epoch 2100, val loss: 1.0531795024871826
Epoch 2110, training loss: 441.8035888671875 = 1.051766276359558 + 50.0 * 8.81503677368164
Epoch 2110, val loss: 1.0531296730041504
Epoch 2120, training loss: 441.64398193359375 = 1.0516908168792725 + 50.0 * 8.811845779418945
Epoch 2120, val loss: 1.0530617237091064
Epoch 2130, training loss: 441.05950927734375 = 1.0515331029891968 + 50.0 * 8.800159454345703
Epoch 2130, val loss: 1.0529334545135498
Epoch 2140, training loss: 440.68865966796875 = 1.0513899326324463 + 50.0 * 8.792745590209961
Epoch 2140, val loss: 1.0528124570846558
Epoch 2150, training loss: 440.700927734375 = 1.0513144731521606 + 50.0 * 8.79299259185791
Epoch 2150, val loss: 1.0527470111846924
Epoch 2160, training loss: 440.46893310546875 = 1.0512791872024536 + 50.0 * 8.788352966308594
Epoch 2160, val loss: 1.0527364015579224
Epoch 2170, training loss: 440.4271240234375 = 1.0512114763259888 + 50.0 * 8.787518501281738
Epoch 2170, val loss: 1.0526864528656006
Epoch 2180, training loss: 441.2067565917969 = 1.0513619184494019 + 50.0 * 8.803108215332031
Epoch 2180, val loss: 1.0527920722961426
Epoch 2190, training loss: 441.4234313964844 = 1.0513646602630615 + 50.0 * 8.807441711425781
Epoch 2190, val loss: 1.0528159141540527
Epoch 2200, training loss: 441.65142822265625 = 1.0513545274734497 + 50.0 * 8.81200122833252
Epoch 2200, val loss: 1.0528048276901245
Epoch 2210, training loss: 441.8659362792969 = 1.0513533353805542 + 50.0 * 8.816291809082031
Epoch 2210, val loss: 1.0528017282485962
Epoch 2220, training loss: 442.0546875 = 1.0513361692428589 + 50.0 * 8.820067405700684
Epoch 2220, val loss: 1.0527881383895874
Epoch 2230, training loss: 442.03924560546875 = 1.0512830018997192 + 50.0 * 8.819759368896484
Epoch 2230, val loss: 1.052734613418579
Epoch 2240, training loss: 442.0117492675781 = 1.051237940788269 + 50.0 * 8.819210052490234
Epoch 2240, val loss: 1.0527068376541138
Epoch 2250, training loss: 442.20269775390625 = 1.0512219667434692 + 50.0 * 8.823029518127441
Epoch 2250, val loss: 1.052685022354126
Epoch 2260, training loss: 442.2367858886719 = 1.0511895418167114 + 50.0 * 8.823712348937988
Epoch 2260, val loss: 1.0526492595672607
Epoch 2270, training loss: 442.41357421875 = 1.0511575937271118 + 50.0 * 8.827248573303223
Epoch 2270, val loss: 1.0526314973831177
Epoch 2280, training loss: 442.27276611328125 = 1.0510789155960083 + 50.0 * 8.824433326721191
Epoch 2280, val loss: 1.0525474548339844
Epoch 2290, training loss: 442.31024169921875 = 1.0510406494140625 + 50.0 * 8.825183868408203
Epoch 2290, val loss: 1.0525288581848145
Epoch 2300, training loss: 442.4628601074219 = 1.0510174036026 + 50.0 * 8.82823657989502
Epoch 2300, val loss: 1.0525039434432983
Epoch 2310, training loss: 442.61865234375 = 1.0509872436523438 + 50.0 * 8.831353187561035
Epoch 2310, val loss: 1.0524758100509644
Epoch 2320, training loss: 442.5851135253906 = 1.0509343147277832 + 50.0 * 8.830683708190918
Epoch 2320, val loss: 1.0524357557296753
Epoch 2330, training loss: 442.63153076171875 = 1.050890326499939 + 50.0 * 8.831612586975098
Epoch 2330, val loss: 1.0523351430892944
Epoch 2340, training loss: 442.23052978515625 = 1.0507253408432007 + 50.0 * 8.823596000671387
Epoch 2340, val loss: 1.0522748231887817
Epoch 2350, training loss: 442.48883056640625 = 1.0507088899612427 + 50.0 * 8.82876205444336
Epoch 2350, val loss: 1.0522606372833252
Epoch 2360, training loss: 442.7178955078125 = 1.050724744796753 + 50.0 * 8.833343505859375
Epoch 2360, val loss: 1.0522689819335938
Epoch 2370, training loss: 443.0496826171875 = 1.0507261753082275 + 50.0 * 8.83997917175293
Epoch 2370, val loss: 1.0522710084915161
Epoch 2380, training loss: 443.2106628417969 = 1.0507023334503174 + 50.0 * 8.843199729919434
Epoch 2380, val loss: 1.0522607564926147
Epoch 2390, training loss: 443.10980224609375 = 1.0506258010864258 + 50.0 * 8.84118366241455
Epoch 2390, val loss: 1.052191972732544
Epoch 2400, training loss: 443.0849914550781 = 1.050585150718689 + 50.0 * 8.84068775177002
Epoch 2400, val loss: 1.05216646194458
Epoch 2410, training loss: 443.3336181640625 = 1.0505701303482056 + 50.0 * 8.845661163330078
Epoch 2410, val loss: 1.0521410703659058
Epoch 2420, training loss: 443.4243469238281 = 1.0505383014678955 + 50.0 * 8.8474760055542
Epoch 2420, val loss: 1.05210542678833
Epoch 2430, training loss: 443.32501220703125 = 1.0504661798477173 + 50.0 * 8.845490455627441
Epoch 2430, val loss: 1.0520488023757935
Epoch 2440, training loss: 443.4476623535156 = 1.0504347085952759 + 50.0 * 8.847944259643555
Epoch 2440, val loss: 1.0520317554473877
Epoch 2450, training loss: 443.4509582519531 = 1.050365686416626 + 50.0 * 8.84801197052002
Epoch 2450, val loss: 1.0519682168960571
Epoch 2460, training loss: 443.594970703125 = 1.0503278970718384 + 50.0 * 8.850893020629883
Epoch 2460, val loss: 1.0519399642944336
Epoch 2470, training loss: 443.85089111328125 = 1.0503103733062744 + 50.0 * 8.856011390686035
Epoch 2470, val loss: 1.0519170761108398
Epoch 2480, training loss: 443.65704345703125 = 1.0502125024795532 + 50.0 * 8.852136611938477
Epoch 2480, val loss: 1.0518267154693604
Epoch 2490, training loss: 443.70611572265625 = 1.0501718521118164 + 50.0 * 8.853118896484375
Epoch 2490, val loss: 1.0517946481704712
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.4011594202898551
0.8649568934289648
=== training gcn model ===
Epoch 0, training loss: 503.5218505859375 = 1.1140978336334229 + 50.0 * 10.048154830932617
Epoch 0, val loss: 1.1140782833099365
Epoch 10, training loss: 481.04168701171875 = 1.10981285572052 + 50.0 * 9.598637580871582
Epoch 10, val loss: 1.1098130941390991
Epoch 20, training loss: 471.15814208984375 = 1.105748176574707 + 50.0 * 9.401047706604004
Epoch 20, val loss: 1.1057660579681396
Epoch 30, training loss: 464.4974365234375 = 1.101887583732605 + 50.0 * 9.267910957336426
Epoch 30, val loss: 1.101919412612915
Epoch 40, training loss: 459.4541015625 = 1.0982123613357544 + 50.0 * 9.167118072509766
Epoch 40, val loss: 1.0982612371444702
Epoch 50, training loss: 455.3800048828125 = 1.0947465896606445 + 50.0 * 9.085704803466797
Epoch 50, val loss: 1.0948134660720825
Epoch 60, training loss: 452.0117492675781 = 1.0914589166641235 + 50.0 * 9.01840591430664
Epoch 60, val loss: 1.0915464162826538
Epoch 70, training loss: 449.1309814453125 = 1.0883593559265137 + 50.0 * 8.96085262298584
Epoch 70, val loss: 1.0884672403335571
Epoch 80, training loss: 446.61688232421875 = 1.0854145288467407 + 50.0 * 8.910629272460938
Epoch 80, val loss: 1.0855408906936646
Epoch 90, training loss: 444.421142578125 = 1.0826236009597778 + 50.0 * 8.86677074432373
Epoch 90, val loss: 1.082769513130188
Epoch 100, training loss: 442.5347900390625 = 1.0799837112426758 + 50.0 * 8.829095840454102
Epoch 100, val loss: 1.0801522731781006
Epoch 110, training loss: 440.93609619140625 = 1.0774999856948853 + 50.0 * 8.797171592712402
Epoch 110, val loss: 1.0776907205581665
Epoch 120, training loss: 439.4825439453125 = 1.0751655101776123 + 50.0 * 8.768147468566895
Epoch 120, val loss: 1.0753791332244873
Epoch 130, training loss: 438.2431335449219 = 1.0729771852493286 + 50.0 * 8.743403434753418
Epoch 130, val loss: 1.07321298122406
Epoch 140, training loss: 437.2024841308594 = 1.0709515810012817 + 50.0 * 8.722630500793457
Epoch 140, val loss: 1.071214199066162
Epoch 150, training loss: 436.2590026855469 = 1.0690778493881226 + 50.0 * 8.703798294067383
Epoch 150, val loss: 1.069366693496704
Epoch 160, training loss: 435.6136779785156 = 1.067341923713684 + 50.0 * 8.690926551818848
Epoch 160, val loss: 1.0676525831222534
Epoch 170, training loss: 434.7508850097656 = 1.0657612085342407 + 50.0 * 8.673702239990234
Epoch 170, val loss: 1.0660972595214844
Epoch 180, training loss: 434.00091552734375 = 1.0643188953399658 + 50.0 * 8.658731460571289
Epoch 180, val loss: 1.064691185951233
Epoch 190, training loss: 433.441650390625 = 1.063025712966919 + 50.0 * 8.64757251739502
Epoch 190, val loss: 1.0634229183197021
Epoch 200, training loss: 432.9784240722656 = 1.061866044998169 + 50.0 * 8.638331413269043
Epoch 200, val loss: 1.062288761138916
Epoch 210, training loss: 432.5945739746094 = 1.0608406066894531 + 50.0 * 8.630674362182617
Epoch 210, val loss: 1.061292290687561
Epoch 220, training loss: 432.349609375 = 1.059932827949524 + 50.0 * 8.62579345703125
Epoch 220, val loss: 1.0603852272033691
Epoch 230, training loss: 432.12054443359375 = 1.0591468811035156 + 50.0 * 8.621228218078613
Epoch 230, val loss: 1.0596562623977661
Epoch 240, training loss: 431.7118225097656 = 1.058437466621399 + 50.0 * 8.613067626953125
Epoch 240, val loss: 1.0589629411697388
Epoch 250, training loss: 431.48687744140625 = 1.0578709840774536 + 50.0 * 8.608580589294434
Epoch 250, val loss: 1.058419942855835
Epoch 260, training loss: 431.20501708984375 = 1.0573610067367554 + 50.0 * 8.60295295715332
Epoch 260, val loss: 1.057936429977417
Epoch 270, training loss: 431.1904296875 = 1.0569558143615723 + 50.0 * 8.602669715881348
Epoch 270, val loss: 1.0575491189956665
Epoch 280, training loss: 431.2952880859375 = 1.0566154718399048 + 50.0 * 8.60477352142334
Epoch 280, val loss: 1.0572254657745361
Epoch 290, training loss: 430.9853210449219 = 1.0563093423843384 + 50.0 * 8.598580360412598
Epoch 290, val loss: 1.056939721107483
Epoch 300, training loss: 430.72906494140625 = 1.056046724319458 + 50.0 * 8.593460083007812
Epoch 300, val loss: 1.056687831878662
Epoch 310, training loss: 430.6264343261719 = 1.0558581352233887 + 50.0 * 8.591411590576172
Epoch 310, val loss: 1.0565146207809448
Epoch 320, training loss: 430.5820617675781 = 1.0557020902633667 + 50.0 * 8.590527534484863
Epoch 320, val loss: 1.0563699007034302
Epoch 330, training loss: 430.52349853515625 = 1.055582880973816 + 50.0 * 8.58935832977295
Epoch 330, val loss: 1.0562655925750732
Epoch 340, training loss: 430.4867248535156 = 1.055469274520874 + 50.0 * 8.588624954223633
Epoch 340, val loss: 1.0561575889587402
Epoch 350, training loss: 430.4287109375 = 1.055387258529663 + 50.0 * 8.5874662399292
Epoch 350, val loss: 1.0560916662216187
Epoch 360, training loss: 430.49700927734375 = 1.055323600769043 + 50.0 * 8.588833808898926
Epoch 360, val loss: 1.0560249090194702
Epoch 370, training loss: 430.3915710449219 = 1.0552557706832886 + 50.0 * 8.586726188659668
Epoch 370, val loss: 1.0559802055358887
Epoch 380, training loss: 430.4808349609375 = 1.0552211999893188 + 50.0 * 8.588512420654297
Epoch 380, val loss: 1.055943489074707
Epoch 390, training loss: 430.55255126953125 = 1.0551890134811401 + 50.0 * 8.589947700500488
Epoch 390, val loss: 1.0559206008911133
Epoch 400, training loss: 430.6299743652344 = 1.0551464557647705 + 50.0 * 8.591496467590332
Epoch 400, val loss: 1.0558860301971436
Epoch 410, training loss: 430.62115478515625 = 1.0551425218582153 + 50.0 * 8.591320037841797
Epoch 410, val loss: 1.0558806657791138
Epoch 420, training loss: 430.6219787597656 = 1.0551261901855469 + 50.0 * 8.591337203979492
Epoch 420, val loss: 1.0558762550354004
Epoch 430, training loss: 430.5618591308594 = 1.0551091432571411 + 50.0 * 8.590134620666504
Epoch 430, val loss: 1.0558626651763916
Epoch 440, training loss: 430.6656188964844 = 1.0550998449325562 + 50.0 * 8.592209815979004
Epoch 440, val loss: 1.0558613538742065
Epoch 450, training loss: 430.6336975097656 = 1.0550622940063477 + 50.0 * 8.591572761535645
Epoch 450, val loss: 1.0558209419250488
Epoch 460, training loss: 430.7145690917969 = 1.055067539215088 + 50.0 * 8.59319019317627
Epoch 460, val loss: 1.055834174156189
Epoch 470, training loss: 430.6158447265625 = 1.0550538301467896 + 50.0 * 8.591216087341309
Epoch 470, val loss: 1.0558208227157593
Epoch 480, training loss: 430.6748962402344 = 1.0550587177276611 + 50.0 * 8.59239673614502
Epoch 480, val loss: 1.0558207035064697
Epoch 490, training loss: 430.70611572265625 = 1.0550577640533447 + 50.0 * 8.593021392822266
Epoch 490, val loss: 1.0558228492736816
Epoch 500, training loss: 430.9771728515625 = 1.0550251007080078 + 50.0 * 8.598443031311035
Epoch 500, val loss: 1.0557574033737183
Epoch 510, training loss: 431.08917236328125 = 1.0550484657287598 + 50.0 * 8.600682258605957
Epoch 510, val loss: 1.0558198690414429
Epoch 520, training loss: 430.633056640625 = 1.05500066280365 + 50.0 * 8.591561317443848
Epoch 520, val loss: 1.055781364440918
Epoch 530, training loss: 430.717529296875 = 1.0550087690353394 + 50.0 * 8.593250274658203
Epoch 530, val loss: 1.0557841062545776
Epoch 540, training loss: 430.8807678222656 = 1.05501389503479 + 50.0 * 8.596514701843262
Epoch 540, val loss: 1.0557894706726074
Epoch 550, training loss: 431.0201721191406 = 1.055013656616211 + 50.0 * 8.599303245544434
Epoch 550, val loss: 1.0557864904403687
Epoch 560, training loss: 431.0592041015625 = 1.0550111532211304 + 50.0 * 8.600083351135254
Epoch 560, val loss: 1.0557827949523926
Epoch 570, training loss: 431.0586853027344 = 1.0549979209899902 + 50.0 * 8.60007381439209
Epoch 570, val loss: 1.0557703971862793
Epoch 580, training loss: 431.18804931640625 = 1.0549877882003784 + 50.0 * 8.6026611328125
Epoch 580, val loss: 1.0557667016983032
Epoch 590, training loss: 431.24542236328125 = 1.0549880266189575 + 50.0 * 8.603808403015137
Epoch 590, val loss: 1.0557712316513062
Epoch 600, training loss: 431.3366394042969 = 1.0549767017364502 + 50.0 * 8.605633735656738
Epoch 600, val loss: 1.0557589530944824
Epoch 610, training loss: 431.409423828125 = 1.0549736022949219 + 50.0 * 8.607089042663574
Epoch 610, val loss: 1.055753469467163
Epoch 620, training loss: 431.5116882324219 = 1.05495285987854 + 50.0 * 8.609134674072266
Epoch 620, val loss: 1.055740237236023
Epoch 630, training loss: 431.5098876953125 = 1.0549570322036743 + 50.0 * 8.609098434448242
Epoch 630, val loss: 1.0557363033294678
Epoch 640, training loss: 431.6780090332031 = 1.0549547672271729 + 50.0 * 8.61246109008789
Epoch 640, val loss: 1.055740475654602
Epoch 650, training loss: 431.70367431640625 = 1.0549403429031372 + 50.0 * 8.612975120544434
Epoch 650, val loss: 1.055720329284668
Epoch 660, training loss: 431.79827880859375 = 1.0549410581588745 + 50.0 * 8.614867210388184
Epoch 660, val loss: 1.0557208061218262
Epoch 670, training loss: 431.87115478515625 = 1.0549321174621582 + 50.0 * 8.616324424743652
Epoch 670, val loss: 1.055708408355713
Epoch 680, training loss: 431.8507080078125 = 1.0549088716506958 + 50.0 * 8.61591625213623
Epoch 680, val loss: 1.055690050125122
Epoch 690, training loss: 432.14862060546875 = 1.0549191236495972 + 50.0 * 8.62187385559082
Epoch 690, val loss: 1.0556952953338623
Epoch 700, training loss: 431.5682373046875 = 1.0548148155212402 + 50.0 * 8.610268592834473
Epoch 700, val loss: 1.055580973625183
Epoch 710, training loss: 432.2132568359375 = 1.0548826456069946 + 50.0 * 8.623167991638184
Epoch 710, val loss: 1.0556707382202148
Epoch 720, training loss: 431.8293762207031 = 1.0548501014709473 + 50.0 * 8.615490913391113
Epoch 720, val loss: 1.055627703666687
Epoch 730, training loss: 431.8203125 = 1.0548641681671143 + 50.0 * 8.61530876159668
Epoch 730, val loss: 1.0556457042694092
Epoch 740, training loss: 432.0542907714844 = 1.054874062538147 + 50.0 * 8.619988441467285
Epoch 740, val loss: 1.055635929107666
Epoch 750, training loss: 432.2864990234375 = 1.054874300956726 + 50.0 * 8.624632835388184
Epoch 750, val loss: 1.0556449890136719
Epoch 760, training loss: 432.3214111328125 = 1.0548704862594604 + 50.0 * 8.625330924987793
Epoch 760, val loss: 1.0556421279907227
Epoch 770, training loss: 432.5717468261719 = 1.0548686981201172 + 50.0 * 8.630337715148926
Epoch 770, val loss: 1.055647611618042
Epoch 780, training loss: 432.89495849609375 = 1.054869294166565 + 50.0 * 8.636801719665527
Epoch 780, val loss: 1.0556440353393555
Epoch 790, training loss: 432.90032958984375 = 1.0548591613769531 + 50.0 * 8.636909484863281
Epoch 790, val loss: 1.0556395053863525
Epoch 800, training loss: 432.9482116699219 = 1.054857850074768 + 50.0 * 8.637866973876953
Epoch 800, val loss: 1.055637240409851
Epoch 810, training loss: 433.0801696777344 = 1.054847240447998 + 50.0 * 8.640506744384766
Epoch 810, val loss: 1.0556296110153198
Epoch 820, training loss: 433.1788024902344 = 1.0548450946807861 + 50.0 * 8.642478942871094
Epoch 820, val loss: 1.0556341409683228
Epoch 830, training loss: 433.35540771484375 = 1.0548423528671265 + 50.0 * 8.646011352539062
Epoch 830, val loss: 1.0556304454803467
Epoch 840, training loss: 433.363525390625 = 1.054830551147461 + 50.0 * 8.646173477172852
Epoch 840, val loss: 1.055610179901123
Epoch 850, training loss: 433.3453674316406 = 1.054821252822876 + 50.0 * 8.645811080932617
Epoch 850, val loss: 1.0555981397628784
Epoch 860, training loss: 433.6045227050781 = 1.0548113584518433 + 50.0 * 8.650994300842285
Epoch 860, val loss: 1.05559504032135
Epoch 870, training loss: 433.8325500488281 = 1.0548228025436401 + 50.0 * 8.65555477142334
Epoch 870, val loss: 1.0556011199951172
Epoch 880, training loss: 433.718017578125 = 1.0548063516616821 + 50.0 * 8.653264045715332
Epoch 880, val loss: 1.0555821657180786
Epoch 890, training loss: 433.6706848144531 = 1.0547734498977661 + 50.0 * 8.652318000793457
Epoch 890, val loss: 1.0555634498596191
Epoch 900, training loss: 433.8273620605469 = 1.0547800064086914 + 50.0 * 8.655451774597168
Epoch 900, val loss: 1.0555633306503296
Epoch 910, training loss: 434.1238098144531 = 1.0547904968261719 + 50.0 * 8.661380767822266
Epoch 910, val loss: 1.0555763244628906
Epoch 920, training loss: 434.230712890625 = 1.054779052734375 + 50.0 * 8.663518905639648
Epoch 920, val loss: 1.0555634498596191
Epoch 930, training loss: 434.1944885253906 = 1.0547654628753662 + 50.0 * 8.66279411315918
Epoch 930, val loss: 1.055552363395691
Epoch 940, training loss: 434.2666015625 = 1.054760456085205 + 50.0 * 8.664237022399902
Epoch 940, val loss: 1.0555378198623657
Epoch 950, training loss: 434.4355773925781 = 1.0547510385513306 + 50.0 * 8.667616844177246
Epoch 950, val loss: 1.0555442571640015
Epoch 960, training loss: 434.5050048828125 = 1.0547552108764648 + 50.0 * 8.669005393981934
Epoch 960, val loss: 1.055543065071106
Epoch 970, training loss: 434.63079833984375 = 1.0547475814819336 + 50.0 * 8.671521186828613
Epoch 970, val loss: 1.0555351972579956
Epoch 980, training loss: 434.62176513671875 = 1.0547317266464233 + 50.0 * 8.671340942382812
Epoch 980, val loss: 1.055523157119751
Epoch 990, training loss: 434.5941162109375 = 1.0546927452087402 + 50.0 * 8.670788764953613
Epoch 990, val loss: 1.055492639541626
Epoch 1000, training loss: 434.4329528808594 = 1.0546841621398926 + 50.0 * 8.66756534576416
Epoch 1000, val loss: 1.0554813146591187
Epoch 1010, training loss: 434.60894775390625 = 1.0546852350234985 + 50.0 * 8.671085357666016
Epoch 1010, val loss: 1.0554847717285156
Epoch 1020, training loss: 434.7732849121094 = 1.0546971559524536 + 50.0 * 8.674371719360352
Epoch 1020, val loss: 1.0554908514022827
Epoch 1030, training loss: 434.9688720703125 = 1.0546982288360596 + 50.0 * 8.67828369140625
Epoch 1030, val loss: 1.0554962158203125
Epoch 1040, training loss: 435.08880615234375 = 1.0546934604644775 + 50.0 * 8.680682182312012
Epoch 1040, val loss: 1.0554903745651245
Epoch 1050, training loss: 435.2772521972656 = 1.0546826124191284 + 50.0 * 8.68445110321045
Epoch 1050, val loss: 1.0554789304733276
Epoch 1060, training loss: 435.19915771484375 = 1.0546627044677734 + 50.0 * 8.682889938354492
Epoch 1060, val loss: 1.0554646253585815
Epoch 1070, training loss: 435.3465881347656 = 1.0546619892120361 + 50.0 * 8.68583869934082
Epoch 1070, val loss: 1.0554616451263428
Epoch 1080, training loss: 435.4508361816406 = 1.0546566247940063 + 50.0 * 8.687923431396484
Epoch 1080, val loss: 1.0554544925689697
Epoch 1090, training loss: 435.190673828125 = 1.0546133518218994 + 50.0 * 8.682721138000488
Epoch 1090, val loss: 1.055409550666809
Epoch 1100, training loss: 435.1201171875 = 1.054589867591858 + 50.0 * 8.681310653686523
Epoch 1100, val loss: 1.0553889274597168
Epoch 1110, training loss: 435.19866943359375 = 1.0545930862426758 + 50.0 * 8.682881355285645
Epoch 1110, val loss: 1.0553981065750122
Epoch 1120, training loss: 435.49884033203125 = 1.0546014308929443 + 50.0 * 8.688884735107422
Epoch 1120, val loss: 1.055413842201233
Epoch 1130, training loss: 435.7358093261719 = 1.0545998811721802 + 50.0 * 8.693624496459961
Epoch 1130, val loss: 1.0554128885269165
Epoch 1140, training loss: 435.8736877441406 = 1.0545958280563354 + 50.0 * 8.696381568908691
Epoch 1140, val loss: 1.0554050207138062
Epoch 1150, training loss: 435.9989013671875 = 1.0545856952667236 + 50.0 * 8.698885917663574
Epoch 1150, val loss: 1.0554020404815674
Epoch 1160, training loss: 435.99365234375 = 1.0545746088027954 + 50.0 * 8.698781967163086
Epoch 1160, val loss: 1.0553860664367676
Epoch 1170, training loss: 435.9122009277344 = 1.0545284748077393 + 50.0 * 8.697153091430664
Epoch 1170, val loss: 1.0553219318389893
Epoch 1180, training loss: 435.8387756347656 = 1.0544915199279785 + 50.0 * 8.695685386657715
Epoch 1180, val loss: 1.0553202629089355
Epoch 1190, training loss: 435.5843505859375 = 1.0544618368148804 + 50.0 * 8.690597534179688
Epoch 1190, val loss: 1.0552858114242554
Epoch 1200, training loss: 435.76544189453125 = 1.054492712020874 + 50.0 * 8.694218635559082
Epoch 1200, val loss: 1.0553139448165894
Epoch 1210, training loss: 436.26348876953125 = 1.0545105934143066 + 50.0 * 8.704179763793945
Epoch 1210, val loss: 1.0553317070007324
Epoch 1220, training loss: 436.29833984375 = 1.0545021295547485 + 50.0 * 8.704876899719238
Epoch 1220, val loss: 1.055325984954834
Epoch 1230, training loss: 436.3688049316406 = 1.0544840097427368 + 50.0 * 8.706286430358887
Epoch 1230, val loss: 1.0553040504455566
Epoch 1240, training loss: 436.2198486328125 = 1.0544558763504028 + 50.0 * 8.70330810546875
Epoch 1240, val loss: 1.0552845001220703
Epoch 1250, training loss: 436.388671875 = 1.0544605255126953 + 50.0 * 8.706684112548828
Epoch 1250, val loss: 1.0552952289581299
Epoch 1260, training loss: 436.79852294921875 = 1.0544698238372803 + 50.0 * 8.71488094329834
Epoch 1260, val loss: 1.0553014278411865
Epoch 1270, training loss: 436.6615905761719 = 1.054450511932373 + 50.0 * 8.712142944335938
Epoch 1270, val loss: 1.0552810430526733
Epoch 1280, training loss: 435.906982421875 = 1.0541949272155762 + 50.0 * 8.69705581665039
Epoch 1280, val loss: 1.0549252033233643
Epoch 1290, training loss: 435.1890563964844 = 1.0541397333145142 + 50.0 * 8.682698249816895
Epoch 1290, val loss: 1.0550057888031006
Epoch 1300, training loss: 435.08013916015625 = 1.0540740489959717 + 50.0 * 8.680521011352539
Epoch 1300, val loss: 1.0549213886260986
Epoch 1310, training loss: 434.9373474121094 = 1.0541598796844482 + 50.0 * 8.677663803100586
Epoch 1310, val loss: 1.0549852848052979
Epoch 1320, training loss: 435.3931884765625 = 1.054232120513916 + 50.0 * 8.686779022216797
Epoch 1320, val loss: 1.0550659894943237
Epoch 1330, training loss: 435.83563232421875 = 1.0542588233947754 + 50.0 * 8.695627212524414
Epoch 1330, val loss: 1.0551068782806396
Epoch 1340, training loss: 436.10943603515625 = 1.0542702674865723 + 50.0 * 8.701103210449219
Epoch 1340, val loss: 1.0551204681396484
Epoch 1350, training loss: 436.414306640625 = 1.0542762279510498 + 50.0 * 8.707200050354004
Epoch 1350, val loss: 1.0551272630691528
Epoch 1360, training loss: 436.6665954589844 = 1.0542834997177124 + 50.0 * 8.71224594116211
Epoch 1360, val loss: 1.0551332235336304
Epoch 1370, training loss: 436.8800354003906 = 1.0542820692062378 + 50.0 * 8.716514587402344
Epoch 1370, val loss: 1.0551302433013916
Epoch 1380, training loss: 436.9671936035156 = 1.054271936416626 + 50.0 * 8.71825885772705
Epoch 1380, val loss: 1.055118441581726
Epoch 1390, training loss: 437.03411865234375 = 1.054260492324829 + 50.0 * 8.719596862792969
Epoch 1390, val loss: 1.0551081895828247
Epoch 1400, training loss: 437.1431884765625 = 1.0542501211166382 + 50.0 * 8.721778869628906
Epoch 1400, val loss: 1.0551061630249023
Epoch 1410, training loss: 437.2525634765625 = 1.054240345954895 + 50.0 * 8.723966598510742
Epoch 1410, val loss: 1.05509352684021
Epoch 1420, training loss: 437.3359680175781 = 1.0542340278625488 + 50.0 * 8.725634574890137
Epoch 1420, val loss: 1.0550813674926758
Epoch 1430, training loss: 437.38470458984375 = 1.054222583770752 + 50.0 * 8.726609230041504
Epoch 1430, val loss: 1.0550729036331177
Epoch 1440, training loss: 437.4825439453125 = 1.0542147159576416 + 50.0 * 8.72856616973877
Epoch 1440, val loss: 1.0550658702850342
Epoch 1450, training loss: 437.3846435546875 = 1.0541828870773315 + 50.0 * 8.726609230041504
Epoch 1450, val loss: 1.055034875869751
Epoch 1460, training loss: 437.5095520019531 = 1.0541735887527466 + 50.0 * 8.729107856750488
Epoch 1460, val loss: 1.0550295114517212
Epoch 1470, training loss: 437.3908996582031 = 1.0541439056396484 + 50.0 * 8.72673511505127
Epoch 1470, val loss: 1.05500328540802
Epoch 1480, training loss: 436.95947265625 = 1.0540815591812134 + 50.0 * 8.718108177185059
Epoch 1480, val loss: 1.0549676418304443
Epoch 1490, training loss: 436.5420227050781 = 1.053963303565979 + 50.0 * 8.709761619567871
Epoch 1490, val loss: 1.0548255443572998
Epoch 1500, training loss: 437.1286315917969 = 1.054033875465393 + 50.0 * 8.721491813659668
Epoch 1500, val loss: 1.0548882484436035
Epoch 1510, training loss: 437.3146667480469 = 1.0540354251861572 + 50.0 * 8.725212097167969
Epoch 1510, val loss: 1.0548985004425049
Epoch 1520, training loss: 437.7525939941406 = 1.0540519952774048 + 50.0 * 8.733970642089844
Epoch 1520, val loss: 1.0549101829528809
Epoch 1530, training loss: 438.0003356933594 = 1.054065465927124 + 50.0 * 8.738924980163574
Epoch 1530, val loss: 1.0549249649047852
Epoch 1540, training loss: 438.09619140625 = 1.0540533065795898 + 50.0 * 8.740842819213867
Epoch 1540, val loss: 1.0549159049987793
Epoch 1550, training loss: 438.2274169921875 = 1.0540423393249512 + 50.0 * 8.743467330932617
Epoch 1550, val loss: 1.0549002885818481
Epoch 1560, training loss: 438.2348327636719 = 1.0540302991867065 + 50.0 * 8.743616104125977
Epoch 1560, val loss: 1.054893970489502
Epoch 1570, training loss: 438.3546447753906 = 1.054015040397644 + 50.0 * 8.746012687683105
Epoch 1570, val loss: 1.054880976676941
Epoch 1580, training loss: 438.42718505859375 = 1.053995966911316 + 50.0 * 8.747464179992676
Epoch 1580, val loss: 1.0548654794692993
Epoch 1590, training loss: 438.37298583984375 = 1.0539696216583252 + 50.0 * 8.746380805969238
Epoch 1590, val loss: 1.0548359155654907
Epoch 1600, training loss: 438.5109558105469 = 1.053962230682373 + 50.0 * 8.749139785766602
Epoch 1600, val loss: 1.0548279285430908
Epoch 1610, training loss: 438.65460205078125 = 1.0539475679397583 + 50.0 * 8.752013206481934
Epoch 1610, val loss: 1.0548208951950073
Epoch 1620, training loss: 438.6651611328125 = 1.0539300441741943 + 50.0 * 8.752224922180176
Epoch 1620, val loss: 1.054801344871521
Epoch 1630, training loss: 438.6334228515625 = 1.0538960695266724 + 50.0 * 8.751590728759766
Epoch 1630, val loss: 1.0547791719436646
Epoch 1640, training loss: 438.7719421386719 = 1.053889513015747 + 50.0 * 8.754361152648926
Epoch 1640, val loss: 1.0547648668289185
Epoch 1650, training loss: 438.80816650390625 = 1.0538702011108398 + 50.0 * 8.755085945129395
Epoch 1650, val loss: 1.0547473430633545
Epoch 1660, training loss: 438.8987731933594 = 1.0538614988327026 + 50.0 * 8.756897926330566
Epoch 1660, val loss: 1.0547442436218262
Epoch 1670, training loss: 439.0531005859375 = 1.0538491010665894 + 50.0 * 8.759984970092773
Epoch 1670, val loss: 1.05471932888031
Epoch 1680, training loss: 438.8765869140625 = 1.053805947303772 + 50.0 * 8.756455421447754
Epoch 1680, val loss: 1.0546892881393433
Epoch 1690, training loss: 439.1280517578125 = 1.0538030862808228 + 50.0 * 8.76148509979248
Epoch 1690, val loss: 1.054689884185791
Epoch 1700, training loss: 439.3265686035156 = 1.05379319190979 + 50.0 * 8.76545524597168
Epoch 1700, val loss: 1.0546770095825195
Epoch 1710, training loss: 439.3201904296875 = 1.0537700653076172 + 50.0 * 8.765328407287598
Epoch 1710, val loss: 1.0546611547470093
Epoch 1720, training loss: 439.0590515136719 = 1.0537046194076538 + 50.0 * 8.760107040405273
Epoch 1720, val loss: 1.0545897483825684
Epoch 1730, training loss: 438.73828125 = 1.0536493062973022 + 50.0 * 8.753692626953125
Epoch 1730, val loss: 1.0545331239700317
Epoch 1740, training loss: 438.84930419921875 = 1.0536458492279053 + 50.0 * 8.755912780761719
Epoch 1740, val loss: 1.054537057876587
Epoch 1750, training loss: 439.2171630859375 = 1.053663730621338 + 50.0 * 8.763270378112793
Epoch 1750, val loss: 1.0545549392700195
Epoch 1760, training loss: 439.54693603515625 = 1.0536599159240723 + 50.0 * 8.769865989685059
Epoch 1760, val loss: 1.0545541048049927
Epoch 1770, training loss: 439.771484375 = 1.0536597967147827 + 50.0 * 8.774356842041016
Epoch 1770, val loss: 1.0545542240142822
Epoch 1780, training loss: 439.5127258300781 = 1.0536099672317505 + 50.0 * 8.769182205200195
Epoch 1780, val loss: 1.054512619972229
Epoch 1790, training loss: 439.7324523925781 = 1.0536054372787476 + 50.0 * 8.773576736450195
Epoch 1790, val loss: 1.0545077323913574
Epoch 1800, training loss: 439.9134826660156 = 1.0535959005355835 + 50.0 * 8.77719783782959
Epoch 1800, val loss: 1.0544991493225098
Epoch 1810, training loss: 439.9588928222656 = 1.0535707473754883 + 50.0 * 8.778106689453125
Epoch 1810, val loss: 1.054480791091919
Epoch 1820, training loss: 439.939208984375 = 1.0535446405410767 + 50.0 * 8.77771282196045
Epoch 1820, val loss: 1.0544520616531372
Epoch 1830, training loss: 439.95184326171875 = 1.0535187721252441 + 50.0 * 8.777966499328613
Epoch 1830, val loss: 1.0544228553771973
Epoch 1840, training loss: 440.02191162109375 = 1.053489327430725 + 50.0 * 8.77936840057373
Epoch 1840, val loss: 1.0543919801712036
Epoch 1850, training loss: 440.0526428222656 = 1.0534632205963135 + 50.0 * 8.779983520507812
Epoch 1850, val loss: 1.054370403289795
Epoch 1860, training loss: 440.1596984863281 = 1.0534465312957764 + 50.0 * 8.782125473022461
Epoch 1860, val loss: 1.0543599128723145
Epoch 1870, training loss: 440.31597900390625 = 1.0534306764602661 + 50.0 * 8.785250663757324
Epoch 1870, val loss: 1.0543514490127563
Epoch 1880, training loss: 440.3633728027344 = 1.0534100532531738 + 50.0 * 8.786199569702148
Epoch 1880, val loss: 1.05433988571167
Epoch 1890, training loss: 440.2916259765625 = 1.0533771514892578 + 50.0 * 8.784765243530273
Epoch 1890, val loss: 1.054307222366333
Epoch 1900, training loss: 440.4302673339844 = 1.0533535480499268 + 50.0 * 8.787538528442383
Epoch 1900, val loss: 1.054283857345581
Epoch 1910, training loss: 440.5856628417969 = 1.053341031074524 + 50.0 * 8.79064655303955
Epoch 1910, val loss: 1.0542711019515991
Epoch 1920, training loss: 440.5245056152344 = 1.0533149242401123 + 50.0 * 8.789423942565918
Epoch 1920, val loss: 1.0542393922805786
Epoch 1930, training loss: 440.4881896972656 = 1.0532729625701904 + 50.0 * 8.788698196411133
Epoch 1930, val loss: 1.054208755493164
Epoch 1940, training loss: 440.6575012207031 = 1.0532621145248413 + 50.0 * 8.792084693908691
Epoch 1940, val loss: 1.0541950464248657
Epoch 1950, training loss: 440.7570495605469 = 1.0532454252243042 + 50.0 * 8.794075965881348
Epoch 1950, val loss: 1.0541791915893555
Epoch 1960, training loss: 440.92401123046875 = 1.0532277822494507 + 50.0 * 8.797415733337402
Epoch 1960, val loss: 1.0541613101959229
Epoch 1970, training loss: 440.7799987792969 = 1.0531730651855469 + 50.0 * 8.794536590576172
Epoch 1970, val loss: 1.054118037223816
Epoch 1980, training loss: 439.9942932128906 = 1.0530476570129395 + 50.0 * 8.778824806213379
Epoch 1980, val loss: 1.053985357284546
Epoch 1990, training loss: 440.0196228027344 = 1.0530239343643188 + 50.0 * 8.779332160949707
Epoch 1990, val loss: 1.0539624691009521
Epoch 2000, training loss: 440.2012023925781 = 1.0530099868774414 + 50.0 * 8.782963752746582
Epoch 2000, val loss: 1.0539597272872925
Epoch 2010, training loss: 440.60955810546875 = 1.0530368089675903 + 50.0 * 8.791130065917969
Epoch 2010, val loss: 1.053978443145752
Epoch 2020, training loss: 440.8707580566406 = 1.0530354976654053 + 50.0 * 8.796354293823242
Epoch 2020, val loss: 1.053984522819519
Epoch 2030, training loss: 441.0054626464844 = 1.0530205965042114 + 50.0 * 8.799049377441406
Epoch 2030, val loss: 1.0539759397506714
Epoch 2040, training loss: 440.917236328125 = 1.0529769659042358 + 50.0 * 8.797285079956055
Epoch 2040, val loss: 1.0539371967315674
Epoch 2050, training loss: 440.9730529785156 = 1.0529450178146362 + 50.0 * 8.798401832580566
Epoch 2050, val loss: 1.0539124011993408
Epoch 2060, training loss: 441.16168212890625 = 1.0529372692108154 + 50.0 * 8.80217456817627
Epoch 2060, val loss: 1.0539041757583618
Epoch 2070, training loss: 441.2078552246094 = 1.0529139041900635 + 50.0 * 8.803098678588867
Epoch 2070, val loss: 1.0538785457611084
Epoch 2080, training loss: 441.2771911621094 = 1.0528813600540161 + 50.0 * 8.804486274719238
Epoch 2080, val loss: 1.053848147392273
Epoch 2090, training loss: 441.3573303222656 = 1.0528573989868164 + 50.0 * 8.806089401245117
Epoch 2090, val loss: 1.0538270473480225
Epoch 2100, training loss: 441.41851806640625 = 1.0528229475021362 + 50.0 * 8.807313919067383
Epoch 2100, val loss: 1.0537989139556885
Epoch 2110, training loss: 441.2774963378906 = 1.0527656078338623 + 50.0 * 8.804494857788086
Epoch 2110, val loss: 1.053697109222412
Epoch 2120, training loss: 441.307861328125 = 1.052703857421875 + 50.0 * 8.805103302001953
Epoch 2120, val loss: 1.0536452531814575
Epoch 2130, training loss: 440.8428955078125 = 1.0526199340820312 + 50.0 * 8.795805931091309
Epoch 2130, val loss: 1.0536091327667236
Epoch 2140, training loss: 441.0054626464844 = 1.0525996685028076 + 50.0 * 8.799057006835938
Epoch 2140, val loss: 1.0535982847213745
Epoch 2150, training loss: 441.197998046875 = 1.0526001453399658 + 50.0 * 8.802907943725586
Epoch 2150, val loss: 1.053594946861267
Epoch 2160, training loss: 441.470947265625 = 1.052599549293518 + 50.0 * 8.808366775512695
Epoch 2160, val loss: 1.0535959005355835
Epoch 2170, training loss: 441.6414794921875 = 1.052586317062378 + 50.0 * 8.81177806854248
Epoch 2170, val loss: 1.0535850524902344
Epoch 2180, training loss: 441.3673400878906 = 1.0525147914886475 + 50.0 * 8.806296348571777
Epoch 2180, val loss: 1.0535203218460083
Epoch 2190, training loss: 441.4410400390625 = 1.052488923072815 + 50.0 * 8.807770729064941
Epoch 2190, val loss: 1.0534919500350952
Epoch 2200, training loss: 441.63751220703125 = 1.0524731874465942 + 50.0 * 8.811700820922852
Epoch 2200, val loss: 1.053477168083191
Epoch 2210, training loss: 441.77020263671875 = 1.0524547100067139 + 50.0 * 8.81435489654541
Epoch 2210, val loss: 1.053465723991394
Epoch 2220, training loss: 441.74066162109375 = 1.052412509918213 + 50.0 * 8.813765525817871
Epoch 2220, val loss: 1.0534273386001587
Epoch 2230, training loss: 441.81768798828125 = 1.0523804426193237 + 50.0 * 8.815306663513184
Epoch 2230, val loss: 1.0533918142318726
Epoch 2240, training loss: 441.66290283203125 = 1.0523130893707275 + 50.0 * 8.812211990356445
Epoch 2240, val loss: 1.0533298254013062
Epoch 2250, training loss: 441.77825927734375 = 1.052290439605713 + 50.0 * 8.814519882202148
Epoch 2250, val loss: 1.0533030033111572
Epoch 2260, training loss: 441.8664245605469 = 1.0522624254226685 + 50.0 * 8.816283226013184
Epoch 2260, val loss: 1.0532814264297485
Epoch 2270, training loss: 441.9984436035156 = 1.0522346496582031 + 50.0 * 8.818923950195312
Epoch 2270, val loss: 1.0532608032226562
Epoch 2280, training loss: 442.0332336425781 = 1.0521990060806274 + 50.0 * 8.819621086120605
Epoch 2280, val loss: 1.0532293319702148
Epoch 2290, training loss: 442.04693603515625 = 1.052162528038025 + 50.0 * 8.81989574432373
Epoch 2290, val loss: 1.0531896352767944
Epoch 2300, training loss: 441.99871826171875 = 1.052109956741333 + 50.0 * 8.818931579589844
Epoch 2300, val loss: 1.0531398057937622
Epoch 2310, training loss: 441.911376953125 = 1.0520623922348022 + 50.0 * 8.81718635559082
Epoch 2310, val loss: 1.053098201751709
Epoch 2320, training loss: 441.9693908691406 = 1.0520163774490356 + 50.0 * 8.818347930908203
Epoch 2320, val loss: 1.0530647039413452
Epoch 2330, training loss: 442.1614685058594 = 1.0520014762878418 + 50.0 * 8.822189331054688
Epoch 2330, val loss: 1.053044319152832
Epoch 2340, training loss: 442.2347412109375 = 1.0519676208496094 + 50.0 * 8.823655128479004
Epoch 2340, val loss: 1.0530210733413696
Epoch 2350, training loss: 442.31304931640625 = 1.0519269704818726 + 50.0 * 8.82522201538086
Epoch 2350, val loss: 1.0529801845550537
Epoch 2360, training loss: 442.31060791015625 = 1.0518862009048462 + 50.0 * 8.825174331665039
Epoch 2360, val loss: 1.0529416799545288
Epoch 2370, training loss: 442.31939697265625 = 1.0518378019332886 + 50.0 * 8.825350761413574
Epoch 2370, val loss: 1.0528953075408936
Epoch 2380, training loss: 442.4378356933594 = 1.0517995357513428 + 50.0 * 8.827720642089844
Epoch 2380, val loss: 1.0528632402420044
Epoch 2390, training loss: 442.4617614746094 = 1.0517630577087402 + 50.0 * 8.828200340270996
Epoch 2390, val loss: 1.0528295040130615
Epoch 2400, training loss: 442.56207275390625 = 1.0517361164093018 + 50.0 * 8.830206871032715
Epoch 2400, val loss: 1.0528029203414917
Epoch 2410, training loss: 441.90716552734375 = 1.0515775680541992 + 50.0 * 8.81711196899414
Epoch 2410, val loss: 1.052688717842102
Epoch 2420, training loss: 441.3026428222656 = 1.0513418912887573 + 50.0 * 8.805026054382324
Epoch 2420, val loss: 1.0524756908416748
Epoch 2430, training loss: 441.4476318359375 = 1.0513464212417603 + 50.0 * 8.8079252243042
Epoch 2430, val loss: 1.052461862564087
Epoch 2440, training loss: 441.5689392089844 = 1.0513354539871216 + 50.0 * 8.810352325439453
Epoch 2440, val loss: 1.0524345636367798
Epoch 2450, training loss: 441.9603576660156 = 1.0513567924499512 + 50.0 * 8.818180084228516
Epoch 2450, val loss: 1.0524661540985107
Epoch 2460, training loss: 442.2842712402344 = 1.0513646602630615 + 50.0 * 8.824658393859863
Epoch 2460, val loss: 1.0524712800979614
Epoch 2470, training loss: 442.46380615234375 = 1.0513488054275513 + 50.0 * 8.828248977661133
Epoch 2470, val loss: 1.052459955215454
Epoch 2480, training loss: 442.6150817871094 = 1.0513144731521606 + 50.0 * 8.831275939941406
Epoch 2480, val loss: 1.0524256229400635
Epoch 2490, training loss: 442.5188293457031 = 1.0512574911117554 + 50.0 * 8.829351425170898
Epoch 2490, val loss: 1.0523676872253418
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.39855072463768115
0.8641599652249512
=== training gcn model ===
Epoch 0, training loss: 507.26666259765625 = 1.0756866931915283 + 50.0 * 10.123819351196289
Epoch 0, val loss: 1.075524091720581
Epoch 10, training loss: 480.4396667480469 = 1.0733587741851807 + 50.0 * 9.587326049804688
Epoch 10, val loss: 1.0732285976409912
Epoch 20, training loss: 470.10638427734375 = 1.0712754726409912 + 50.0 * 9.380702018737793
Epoch 20, val loss: 1.0711843967437744
Epoch 30, training loss: 463.7807922363281 = 1.0692813396453857 + 50.0 * 9.254230499267578
Epoch 30, val loss: 1.069227933883667
Epoch 40, training loss: 458.79400634765625 = 1.0674419403076172 + 50.0 * 9.154531478881836
Epoch 40, val loss: 1.0674268007278442
Epoch 50, training loss: 454.8408508300781 = 1.0657756328582764 + 50.0 * 9.075501441955566
Epoch 50, val loss: 1.0658080577850342
Epoch 60, training loss: 451.5430603027344 = 1.064266324043274 + 50.0 * 9.009575843811035
Epoch 60, val loss: 1.0643446445465088
Epoch 70, training loss: 448.7112731933594 = 1.0629066228866577 + 50.0 * 8.952967643737793
Epoch 70, val loss: 1.0630319118499756
Epoch 80, training loss: 446.3389587402344 = 1.0617088079452515 + 50.0 * 8.905545234680176
Epoch 80, val loss: 1.0618764162063599
Epoch 90, training loss: 444.3408508300781 = 1.0606496334075928 + 50.0 * 8.865604400634766
Epoch 90, val loss: 1.0608644485473633
Epoch 100, training loss: 442.57501220703125 = 1.059719204902649 + 50.0 * 8.830306053161621
Epoch 100, val loss: 1.0599786043167114
Epoch 110, training loss: 441.06512451171875 = 1.058925747871399 + 50.0 * 8.800124168395996
Epoch 110, val loss: 1.0592290163040161
Epoch 120, training loss: 439.722412109375 = 1.058239221572876 + 50.0 * 8.773283958435059
Epoch 120, val loss: 1.0585883855819702
Epoch 130, training loss: 438.6910095214844 = 1.0576543807983398 + 50.0 * 8.752667427062988
Epoch 130, val loss: 1.0580412149429321
Epoch 140, training loss: 437.69061279296875 = 1.0571746826171875 + 50.0 * 8.73266887664795
Epoch 140, val loss: 1.0576008558273315
Epoch 150, training loss: 436.8709411621094 = 1.0567773580551147 + 50.0 * 8.716283798217773
Epoch 150, val loss: 1.0572458505630493
Epoch 160, training loss: 436.08489990234375 = 1.056450605392456 + 50.0 * 8.700569152832031
Epoch 160, val loss: 1.0569562911987305
Epoch 170, training loss: 435.5257568359375 = 1.0562024116516113 + 50.0 * 8.689391136169434
Epoch 170, val loss: 1.0567396879196167
Epoch 180, training loss: 434.9541015625 = 1.0559861660003662 + 50.0 * 8.677962303161621
Epoch 180, val loss: 1.056554913520813
Epoch 190, training loss: 434.5057373046875 = 1.0558439493179321 + 50.0 * 8.668997764587402
Epoch 190, val loss: 1.0564348697662354
Epoch 200, training loss: 434.0049743652344 = 1.0557124614715576 + 50.0 * 8.658985137939453
Epoch 200, val loss: 1.0563291311264038
Epoch 210, training loss: 433.7615966796875 = 1.0555821657180786 + 50.0 * 8.654120445251465
Epoch 210, val loss: 1.0562012195587158
Epoch 220, training loss: 433.4395446777344 = 1.0555330514907837 + 50.0 * 8.647680282592773
Epoch 220, val loss: 1.05618417263031
Epoch 230, training loss: 432.9236755371094 = 1.0554180145263672 + 50.0 * 8.637365341186523
Epoch 230, val loss: 1.0560977458953857
Epoch 240, training loss: 432.9588317871094 = 1.0554171800613403 + 50.0 * 8.638068199157715
Epoch 240, val loss: 1.056104302406311
Epoch 250, training loss: 432.56353759765625 = 1.055338978767395 + 50.0 * 8.63016414642334
Epoch 250, val loss: 1.0560411214828491
Epoch 260, training loss: 432.33404541015625 = 1.055310606956482 + 50.0 * 8.625575065612793
Epoch 260, val loss: 1.0560258626937866
Epoch 270, training loss: 432.1239929199219 = 1.0552836656570435 + 50.0 * 8.621374130249023
Epoch 270, val loss: 1.0560028553009033
Epoch 280, training loss: 431.88323974609375 = 1.0552375316619873 + 50.0 * 8.616559982299805
Epoch 280, val loss: 1.0559698343276978
Epoch 290, training loss: 431.7861633300781 = 1.055203914642334 + 50.0 * 8.614619255065918
Epoch 290, val loss: 1.0559377670288086
Epoch 300, training loss: 431.633544921875 = 1.0551667213439941 + 50.0 * 8.611567497253418
Epoch 300, val loss: 1.055907130241394
Epoch 310, training loss: 431.4500732421875 = 1.05513334274292 + 50.0 * 8.607898712158203
Epoch 310, val loss: 1.0558770895004272
Epoch 320, training loss: 431.37738037109375 = 1.0551118850708008 + 50.0 * 8.6064453125
Epoch 320, val loss: 1.0558643341064453
Epoch 330, training loss: 431.2109375 = 1.0550786256790161 + 50.0 * 8.603116989135742
Epoch 330, val loss: 1.0558375120162964
Epoch 340, training loss: 431.154052734375 = 1.0550472736358643 + 50.0 * 8.601980209350586
Epoch 340, val loss: 1.055802345275879
Epoch 350, training loss: 431.1210021972656 = 1.0550163984298706 + 50.0 * 8.601319313049316
Epoch 350, val loss: 1.0557806491851807
Epoch 360, training loss: 431.0384521484375 = 1.055005669593811 + 50.0 * 8.599669456481934
Epoch 360, val loss: 1.0557693243026733
Epoch 370, training loss: 431.0207824707031 = 1.0549691915512085 + 50.0 * 8.599316596984863
Epoch 370, val loss: 1.055734634399414
Epoch 380, training loss: 430.94488525390625 = 1.0549253225326538 + 50.0 * 8.597799301147461
Epoch 380, val loss: 1.0557000637054443
Epoch 390, training loss: 430.83538818359375 = 1.0548940896987915 + 50.0 * 8.595609664916992
Epoch 390, val loss: 1.0556706190109253
Epoch 400, training loss: 430.8719177246094 = 1.0548804998397827 + 50.0 * 8.596341133117676
Epoch 400, val loss: 1.0556459426879883
Epoch 410, training loss: 430.8293762207031 = 1.0548540353775024 + 50.0 * 8.595490455627441
Epoch 410, val loss: 1.0556213855743408
Epoch 420, training loss: 430.8994140625 = 1.0548328161239624 + 50.0 * 8.596891403198242
Epoch 420, val loss: 1.0556071996688843
Epoch 430, training loss: 430.83062744140625 = 1.0548001527786255 + 50.0 * 8.595516204833984
Epoch 430, val loss: 1.0555719137191772
Epoch 440, training loss: 430.78704833984375 = 1.0547637939453125 + 50.0 * 8.594645500183105
Epoch 440, val loss: 1.055534839630127
Epoch 450, training loss: 430.79913330078125 = 1.0547502040863037 + 50.0 * 8.594887733459473
Epoch 450, val loss: 1.0555219650268555
Epoch 460, training loss: 430.86212158203125 = 1.0546948909759521 + 50.0 * 8.596148490905762
Epoch 460, val loss: 1.0554695129394531
Epoch 470, training loss: 430.5546569824219 = 1.0546435117721558 + 50.0 * 8.59000015258789
Epoch 470, val loss: 1.0554157495498657
Epoch 480, training loss: 430.7806396484375 = 1.05466628074646 + 50.0 * 8.59451961517334
Epoch 480, val loss: 1.0554392337799072
Epoch 490, training loss: 430.9253234863281 = 1.0546550750732422 + 50.0 * 8.597413063049316
Epoch 490, val loss: 1.055430293083191
Epoch 500, training loss: 430.91363525390625 = 1.0546233654022217 + 50.0 * 8.597180366516113
Epoch 500, val loss: 1.0553969144821167
Epoch 510, training loss: 431.0342102050781 = 1.0545943975448608 + 50.0 * 8.599592208862305
Epoch 510, val loss: 1.0553712844848633
Epoch 520, training loss: 430.8701477050781 = 1.0545347929000854 + 50.0 * 8.596312522888184
Epoch 520, val loss: 1.0553096532821655
Epoch 530, training loss: 431.00421142578125 = 1.0545099973678589 + 50.0 * 8.598994255065918
Epoch 530, val loss: 1.0552932024002075
Epoch 540, training loss: 431.05462646484375 = 1.0545001029968262 + 50.0 * 8.60000228881836
Epoch 540, val loss: 1.0552860498428345
Epoch 550, training loss: 431.1202087402344 = 1.0544745922088623 + 50.0 * 8.601314544677734
Epoch 550, val loss: 1.0552524328231812
Epoch 560, training loss: 431.20849609375 = 1.054451584815979 + 50.0 * 8.603080749511719
Epoch 560, val loss: 1.055228352546692
Epoch 570, training loss: 431.2887878417969 = 1.054420828819275 + 50.0 * 8.604687690734863
Epoch 570, val loss: 1.0551929473876953
Epoch 580, training loss: 431.1614685058594 = 1.054366946220398 + 50.0 * 8.602142333984375
Epoch 580, val loss: 1.0551464557647705
Epoch 590, training loss: 431.1142883300781 = 1.0543181896209717 + 50.0 * 8.60119915008545
Epoch 590, val loss: 1.0551048517227173
Epoch 600, training loss: 431.1400146484375 = 1.0543073415756226 + 50.0 * 8.601714134216309
Epoch 600, val loss: 1.055098295211792
Epoch 610, training loss: 431.31646728515625 = 1.054291844367981 + 50.0 * 8.605243682861328
Epoch 610, val loss: 1.055076241493225
Epoch 620, training loss: 431.33984375 = 1.0542556047439575 + 50.0 * 8.605711936950684
Epoch 620, val loss: 1.0550402402877808
Epoch 630, training loss: 431.4606628417969 = 1.0542296171188354 + 50.0 * 8.608128547668457
Epoch 630, val loss: 1.055011510848999
Epoch 640, training loss: 431.4689636230469 = 1.0541951656341553 + 50.0 * 8.608295440673828
Epoch 640, val loss: 1.0549795627593994
Epoch 650, training loss: 431.70916748046875 = 1.0541640520095825 + 50.0 * 8.613100051879883
Epoch 650, val loss: 1.054962158203125
Epoch 660, training loss: 431.578857421875 = 1.054124116897583 + 50.0 * 8.610494613647461
Epoch 660, val loss: 1.0548979043960571
Epoch 670, training loss: 431.50067138671875 = 1.054045557975769 + 50.0 * 8.608932495117188
Epoch 670, val loss: 1.0548374652862549
Epoch 680, training loss: 431.4092102050781 = 1.0539978742599487 + 50.0 * 8.607104301452637
Epoch 680, val loss: 1.0548138618469238
Epoch 690, training loss: 431.5905456542969 = 1.0539777278900146 + 50.0 * 8.61073112487793
Epoch 690, val loss: 1.054791808128357
Epoch 700, training loss: 431.46331787109375 = 1.0539257526397705 + 50.0 * 8.608187675476074
Epoch 700, val loss: 1.054723858833313
Epoch 710, training loss: 431.7284240722656 = 1.0539535284042358 + 50.0 * 8.613489151000977
Epoch 710, val loss: 1.054742455482483
Epoch 720, training loss: 431.90020751953125 = 1.0539425611495972 + 50.0 * 8.616925239562988
Epoch 720, val loss: 1.0547308921813965
Epoch 730, training loss: 432.0049133300781 = 1.0539075136184692 + 50.0 * 8.619020462036133
Epoch 730, val loss: 1.054686188697815
Epoch 740, training loss: 432.1491394042969 = 1.0538771152496338 + 50.0 * 8.621905326843262
Epoch 740, val loss: 1.0546690225601196
Epoch 750, training loss: 432.03466796875 = 1.0538153648376465 + 50.0 * 8.619617462158203
Epoch 750, val loss: 1.0545984506607056
Epoch 760, training loss: 432.3731384277344 = 1.0538034439086914 + 50.0 * 8.626386642456055
Epoch 760, val loss: 1.0545873641967773
Epoch 770, training loss: 432.42144775390625 = 1.0537728071212769 + 50.0 * 8.62735366821289
Epoch 770, val loss: 1.0545647144317627
Epoch 780, training loss: 432.5912170410156 = 1.0537439584732056 + 50.0 * 8.630749702453613
Epoch 780, val loss: 1.0545377731323242
Epoch 790, training loss: 432.5669860839844 = 1.053704023361206 + 50.0 * 8.630265235900879
Epoch 790, val loss: 1.05449378490448
Epoch 800, training loss: 432.5890808105469 = 1.0536552667617798 + 50.0 * 8.630708694458008
Epoch 800, val loss: 1.0544496774673462
Epoch 810, training loss: 432.7055358886719 = 1.0536247491836548 + 50.0 * 8.633038520812988
Epoch 810, val loss: 1.054429292678833
Epoch 820, training loss: 432.82763671875 = 1.053602695465088 + 50.0 * 8.635480880737305
Epoch 820, val loss: 1.0543954372406006
Epoch 830, training loss: 432.87518310546875 = 1.0535587072372437 + 50.0 * 8.636432647705078
Epoch 830, val loss: 1.0543535947799683
Epoch 840, training loss: 432.75439453125 = 1.0534999370574951 + 50.0 * 8.634017944335938
Epoch 840, val loss: 1.0542916059494019
Epoch 850, training loss: 432.95062255859375 = 1.053483009338379 + 50.0 * 8.63794231414795
Epoch 850, val loss: 1.0542713403701782
Epoch 860, training loss: 432.8632507324219 = 1.0533510446548462 + 50.0 * 8.636198043823242
Epoch 860, val loss: 1.0541949272155762
Epoch 870, training loss: 432.483154296875 = 1.0532571077346802 + 50.0 * 8.6285982131958
Epoch 870, val loss: 1.0540651082992554
Epoch 880, training loss: 432.63092041015625 = 1.053279161453247 + 50.0 * 8.631552696228027
Epoch 880, val loss: 1.0540738105773926
Epoch 890, training loss: 432.76739501953125 = 1.053275227546692 + 50.0 * 8.634282112121582
Epoch 890, val loss: 1.0540682077407837
Epoch 900, training loss: 432.83135986328125 = 1.0532459020614624 + 50.0 * 8.6355619430542
Epoch 900, val loss: 1.0540449619293213
Epoch 910, training loss: 433.0035095214844 = 1.0532344579696655 + 50.0 * 8.639005661010742
Epoch 910, val loss: 1.054029107093811
Epoch 920, training loss: 433.1040344238281 = 1.0531957149505615 + 50.0 * 8.641016960144043
Epoch 920, val loss: 1.0539861917495728
Epoch 930, training loss: 433.1817932128906 = 1.0531461238861084 + 50.0 * 8.642573356628418
Epoch 930, val loss: 1.0539427995681763
Epoch 940, training loss: 433.240234375 = 1.0531021356582642 + 50.0 * 8.643742561340332
Epoch 940, val loss: 1.0539005994796753
Epoch 950, training loss: 433.4383850097656 = 1.0530779361724854 + 50.0 * 8.647706031799316
Epoch 950, val loss: 1.0538643598556519
Epoch 960, training loss: 433.3501281738281 = 1.0530041456222534 + 50.0 * 8.645942687988281
Epoch 960, val loss: 1.053806185722351
Epoch 970, training loss: 433.5654602050781 = 1.052984595298767 + 50.0 * 8.650249481201172
Epoch 970, val loss: 1.0537772178649902
Epoch 980, training loss: 433.6707458496094 = 1.0529438257217407 + 50.0 * 8.652356147766113
Epoch 980, val loss: 1.0537422895431519
Epoch 990, training loss: 433.6689758300781 = 1.0528944730758667 + 50.0 * 8.652321815490723
Epoch 990, val loss: 1.0536900758743286
Epoch 1000, training loss: 433.7290954589844 = 1.052839756011963 + 50.0 * 8.653525352478027
Epoch 1000, val loss: 1.0536171197891235
Epoch 1010, training loss: 433.9473571777344 = 1.0528101921081543 + 50.0 * 8.657891273498535
Epoch 1010, val loss: 1.0535908937454224
Epoch 1020, training loss: 434.0633544921875 = 1.0527589321136475 + 50.0 * 8.660211563110352
Epoch 1020, val loss: 1.0535383224487305
Epoch 1030, training loss: 433.9908142089844 = 1.052695631980896 + 50.0 * 8.658761978149414
Epoch 1030, val loss: 1.0534802675247192
Epoch 1040, training loss: 434.2184753417969 = 1.0526663064956665 + 50.0 * 8.663315773010254
Epoch 1040, val loss: 1.0534653663635254
Epoch 1050, training loss: 434.2466735839844 = 1.052610158920288 + 50.0 * 8.663881301879883
Epoch 1050, val loss: 1.0533958673477173
Epoch 1060, training loss: 434.1613464355469 = 1.05254065990448 + 50.0 * 8.662176132202148
Epoch 1060, val loss: 1.0533242225646973
Epoch 1070, training loss: 434.3572998046875 = 1.0525206327438354 + 50.0 * 8.666095733642578
Epoch 1070, val loss: 1.0533051490783691
Epoch 1080, training loss: 434.4691467285156 = 1.0524762868881226 + 50.0 * 8.668333053588867
Epoch 1080, val loss: 1.0532605648040771
Epoch 1090, training loss: 434.6549072265625 = 1.0524380207061768 + 50.0 * 8.672049522399902
Epoch 1090, val loss: 1.053234338760376
Epoch 1100, training loss: 434.65203857421875 = 1.052376627922058 + 50.0 * 8.671993255615234
Epoch 1100, val loss: 1.0531370639801025
Epoch 1110, training loss: 434.6080017089844 = 1.0522956848144531 + 50.0 * 8.671113967895508
Epoch 1110, val loss: 1.053081750869751
Epoch 1120, training loss: 434.8028259277344 = 1.0522470474243164 + 50.0 * 8.67501163482666
Epoch 1120, val loss: 1.0530352592468262
Epoch 1130, training loss: 434.87994384765625 = 1.052203893661499 + 50.0 * 8.676554679870605
Epoch 1130, val loss: 1.053002953529358
Epoch 1140, training loss: 435.1286315917969 = 1.0521798133850098 + 50.0 * 8.68152904510498
Epoch 1140, val loss: 1.052977442741394
Epoch 1150, training loss: 435.1457824707031 = 1.0521156787872314 + 50.0 * 8.681873321533203
Epoch 1150, val loss: 1.0529143810272217
Epoch 1160, training loss: 435.1830749511719 = 1.0520488023757935 + 50.0 * 8.68262004852295
Epoch 1160, val loss: 1.0528355836868286
Epoch 1170, training loss: 434.618408203125 = 1.0518419742584229 + 50.0 * 8.671331405639648
Epoch 1170, val loss: 1.0526763200759888
Epoch 1180, training loss: 434.65472412109375 = 1.0518094301223755 + 50.0 * 8.67205810546875
Epoch 1180, val loss: 1.0526303052902222
Epoch 1190, training loss: 434.87823486328125 = 1.0517762899398804 + 50.0 * 8.676528930664062
Epoch 1190, val loss: 1.0525856018066406
Epoch 1200, training loss: 435.1550598144531 = 1.0517597198486328 + 50.0 * 8.682065963745117
Epoch 1200, val loss: 1.0525636672973633
Epoch 1210, training loss: 435.4190673828125 = 1.0517573356628418 + 50.0 * 8.687346458435059
Epoch 1210, val loss: 1.0525634288787842
Epoch 1220, training loss: 435.4609069824219 = 1.051712155342102 + 50.0 * 8.688183784484863
Epoch 1220, val loss: 1.0525165796279907
Epoch 1230, training loss: 435.48626708984375 = 1.0516518354415894 + 50.0 * 8.688692092895508
Epoch 1230, val loss: 1.052451491355896
Epoch 1240, training loss: 435.6332092285156 = 1.05160653591156 + 50.0 * 8.691632270812988
Epoch 1240, val loss: 1.052412748336792
Epoch 1250, training loss: 435.7872619628906 = 1.0515700578689575 + 50.0 * 8.694713592529297
Epoch 1250, val loss: 1.052370548248291
Epoch 1260, training loss: 435.8457946777344 = 1.051511526107788 + 50.0 * 8.69588565826416
Epoch 1260, val loss: 1.052312970161438
Epoch 1270, training loss: 435.90008544921875 = 1.0514485836029053 + 50.0 * 8.696972846984863
Epoch 1270, val loss: 1.0522407293319702
Epoch 1280, training loss: 435.7093505859375 = 1.0513017177581787 + 50.0 * 8.693161010742188
Epoch 1280, val loss: 1.05210280418396
Epoch 1290, training loss: 435.9246826171875 = 1.0512521266937256 + 50.0 * 8.697468757629395
Epoch 1290, val loss: 1.0520509481430054
Epoch 1300, training loss: 435.8058166503906 = 1.0512018203735352 + 50.0 * 8.69509220123291
Epoch 1300, val loss: 1.0520063638687134
Epoch 1310, training loss: 436.09942626953125 = 1.0511760711669922 + 50.0 * 8.70096492767334
Epoch 1310, val loss: 1.0519776344299316
Epoch 1320, training loss: 435.94708251953125 = 1.0510838031768799 + 50.0 * 8.697919845581055
Epoch 1320, val loss: 1.0518893003463745
Epoch 1330, training loss: 436.1136779785156 = 1.0510284900665283 + 50.0 * 8.701252937316895
Epoch 1330, val loss: 1.051840901374817
Epoch 1340, training loss: 436.41766357421875 = 1.0510064363479614 + 50.0 * 8.7073335647583
Epoch 1340, val loss: 1.0518232583999634
Epoch 1350, training loss: 436.4825134277344 = 1.0509378910064697 + 50.0 * 8.70863151550293
Epoch 1350, val loss: 1.0517473220825195
Epoch 1360, training loss: 436.4389343261719 = 1.0508508682250977 + 50.0 * 8.707761764526367
Epoch 1360, val loss: 1.0516602993011475
Epoch 1370, training loss: 436.5137939453125 = 1.0507913827896118 + 50.0 * 8.709259986877441
Epoch 1370, val loss: 1.0515936613082886
Epoch 1380, training loss: 436.7013244628906 = 1.0507503747940063 + 50.0 * 8.713011741638184
Epoch 1380, val loss: 1.051552176475525
Epoch 1390, training loss: 436.5940246582031 = 1.050614833831787 + 50.0 * 8.710867881774902
Epoch 1390, val loss: 1.0514018535614014
Epoch 1400, training loss: 436.23095703125 = 1.050478219985962 + 50.0 * 8.703609466552734
Epoch 1400, val loss: 1.0512969493865967
Epoch 1410, training loss: 436.1338806152344 = 1.0502636432647705 + 50.0 * 8.701672554016113
Epoch 1410, val loss: 1.0511524677276611
Epoch 1420, training loss: 436.5413513183594 = 1.0503133535385132 + 50.0 * 8.709820747375488
Epoch 1420, val loss: 1.0511932373046875
Epoch 1430, training loss: 436.20001220703125 = 1.0500980615615845 + 50.0 * 8.702998161315918
Epoch 1430, val loss: 1.0509657859802246
Epoch 1440, training loss: 436.5469055175781 = 1.0501335859298706 + 50.0 * 8.709935188293457
Epoch 1440, val loss: 1.050980806350708
Epoch 1450, training loss: 436.8014221191406 = 1.050126314163208 + 50.0 * 8.715025901794434
Epoch 1450, val loss: 1.0509862899780273
Epoch 1460, training loss: 437.1024475097656 = 1.0501307249069214 + 50.0 * 8.721046447753906
Epoch 1460, val loss: 1.0509898662567139
Epoch 1470, training loss: 437.2198486328125 = 1.0500876903533936 + 50.0 * 8.723395347595215
Epoch 1470, val loss: 1.050918459892273
Epoch 1480, training loss: 437.2485046386719 = 1.0500237941741943 + 50.0 * 8.723969459533691
Epoch 1480, val loss: 1.0508739948272705
Epoch 1490, training loss: 437.41351318359375 = 1.0499670505523682 + 50.0 * 8.72727108001709
Epoch 1490, val loss: 1.0508168935775757
Epoch 1500, training loss: 437.40435791015625 = 1.0498727560043335 + 50.0 * 8.727089881896973
Epoch 1500, val loss: 1.0506983995437622
Epoch 1510, training loss: 437.4203796386719 = 1.0498111248016357 + 50.0 * 8.727411270141602
Epoch 1510, val loss: 1.0506527423858643
Epoch 1520, training loss: 437.6910705566406 = 1.0497685670852661 + 50.0 * 8.732826232910156
Epoch 1520, val loss: 1.0506207942962646
Epoch 1530, training loss: 437.7706298828125 = 1.0496890544891357 + 50.0 * 8.734418869018555
Epoch 1530, val loss: 1.0505212545394897
Epoch 1540, training loss: 437.7554931640625 = 1.0496156215667725 + 50.0 * 8.73411750793457
Epoch 1540, val loss: 1.0504447221755981
Epoch 1550, training loss: 437.7855529785156 = 1.0495216846466064 + 50.0 * 8.734720230102539
Epoch 1550, val loss: 1.0503634214401245
Epoch 1560, training loss: 437.8327941894531 = 1.0494343042373657 + 50.0 * 8.73566722869873
Epoch 1560, val loss: 1.0502545833587646
Epoch 1570, training loss: 437.8566589355469 = 1.0493475198745728 + 50.0 * 8.736145973205566
Epoch 1570, val loss: 1.0501989126205444
Epoch 1580, training loss: 437.61212158203125 = 1.0491819381713867 + 50.0 * 8.731258392333984
Epoch 1580, val loss: 1.0500125885009766
Epoch 1590, training loss: 437.5765075683594 = 1.0491052865982056 + 50.0 * 8.730547904968262
Epoch 1590, val loss: 1.04994797706604
Epoch 1600, training loss: 437.8751525878906 = 1.0490885972976685 + 50.0 * 8.736520767211914
Epoch 1600, val loss: 1.0499615669250488
Epoch 1610, training loss: 438.0566101074219 = 1.0490552186965942 + 50.0 * 8.740151405334473
Epoch 1610, val loss: 1.0499287843704224
Epoch 1620, training loss: 438.046875 = 1.048943281173706 + 50.0 * 8.739958763122559
Epoch 1620, val loss: 1.0498288869857788
Epoch 1630, training loss: 438.3017883300781 = 1.048905611038208 + 50.0 * 8.745057106018066
Epoch 1630, val loss: 1.0497853755950928
Epoch 1640, training loss: 438.2383728027344 = 1.048783302307129 + 50.0 * 8.743791580200195
Epoch 1640, val loss: 1.0496654510498047
Epoch 1650, training loss: 438.03216552734375 = 1.0485917329788208 + 50.0 * 8.73967170715332
Epoch 1650, val loss: 1.0494948625564575
Epoch 1660, training loss: 437.9120178222656 = 1.0484744310379028 + 50.0 * 8.737271308898926
Epoch 1660, val loss: 1.0494000911712646
Epoch 1670, training loss: 438.1423034667969 = 1.0484551191329956 + 50.0 * 8.741876602172852
Epoch 1670, val loss: 1.0493193864822388
Epoch 1680, training loss: 438.2622375488281 = 1.0484087467193604 + 50.0 * 8.744277000427246
Epoch 1680, val loss: 1.0492961406707764
Epoch 1690, training loss: 438.4828186035156 = 1.0483680963516235 + 50.0 * 8.748688697814941
Epoch 1690, val loss: 1.0492448806762695
Epoch 1700, training loss: 438.56756591796875 = 1.0482994318008423 + 50.0 * 8.750385284423828
Epoch 1700, val loss: 1.0492007732391357
Epoch 1710, training loss: 438.7389221191406 = 1.0482442378997803 + 50.0 * 8.753813743591309
Epoch 1710, val loss: 1.0491324663162231
Epoch 1720, training loss: 438.56768798828125 = 1.0481046438217163 + 50.0 * 8.750391960144043
Epoch 1720, val loss: 1.0489777326583862
Epoch 1730, training loss: 438.65093994140625 = 1.048014521598816 + 50.0 * 8.752058982849121
Epoch 1730, val loss: 1.0489174127578735
Epoch 1740, training loss: 438.8125 = 1.0479522943496704 + 50.0 * 8.755290985107422
Epoch 1740, val loss: 1.0488343238830566
Epoch 1750, training loss: 438.9341125488281 = 1.0478870868682861 + 50.0 * 8.75772476196289
Epoch 1750, val loss: 1.0487817525863647
Epoch 1760, training loss: 438.92730712890625 = 1.0477937459945679 + 50.0 * 8.757590293884277
Epoch 1760, val loss: 1.0486810207366943
Epoch 1770, training loss: 438.88726806640625 = 1.0476733446121216 + 50.0 * 8.756792068481445
Epoch 1770, val loss: 1.0485655069351196
Epoch 1780, training loss: 438.8818054199219 = 1.0475592613220215 + 50.0 * 8.756685256958008
Epoch 1780, val loss: 1.0484570264816284
Epoch 1790, training loss: 439.0375671386719 = 1.0475026369094849 + 50.0 * 8.759800910949707
Epoch 1790, val loss: 1.048405647277832
Epoch 1800, training loss: 439.0747375488281 = 1.0473973751068115 + 50.0 * 8.760546684265137
Epoch 1800, val loss: 1.0482760667800903
Epoch 1810, training loss: 438.7783508300781 = 1.047085165977478 + 50.0 * 8.75462532043457
Epoch 1810, val loss: 1.047908902168274
Epoch 1820, training loss: 438.7869567871094 = 1.0469813346862793 + 50.0 * 8.754799842834473
Epoch 1820, val loss: 1.0478525161743164
Epoch 1830, training loss: 438.82879638671875 = 1.0469849109649658 + 50.0 * 8.755636215209961
Epoch 1830, val loss: 1.0478719472885132
Epoch 1840, training loss: 439.1034240722656 = 1.0469615459442139 + 50.0 * 8.761129379272461
Epoch 1840, val loss: 1.047850489616394
Epoch 1850, training loss: 439.5577697753906 = 1.0462603569030762 + 50.0 * 8.770230293273926
Epoch 1850, val loss: 1.0471605062484741
Epoch 1860, training loss: 439.63092041015625 = 1.0462758541107178 + 50.0 * 8.771693229675293
Epoch 1860, val loss: 1.047294020652771
Epoch 1870, training loss: 438.5625 = 1.0458678007125854 + 50.0 * 8.750332832336426
Epoch 1870, val loss: 1.0468991994857788
Epoch 1880, training loss: 438.8570556640625 = 1.0460214614868164 + 50.0 * 8.756220817565918
Epoch 1880, val loss: 1.0470079183578491
Epoch 1890, training loss: 439.3090515136719 = 1.0461612939834595 + 50.0 * 8.765257835388184
Epoch 1890, val loss: 1.0471113920211792
Epoch 1900, training loss: 439.68267822265625 = 1.0461626052856445 + 50.0 * 8.772729873657227
Epoch 1900, val loss: 1.0471220016479492
Epoch 1910, training loss: 440.0251770019531 = 1.046165943145752 + 50.0 * 8.779580116271973
Epoch 1910, val loss: 1.0471051931381226
Epoch 1920, training loss: 440.1905822753906 = 1.046107292175293 + 50.0 * 8.782889366149902
Epoch 1920, val loss: 1.047065258026123
Epoch 1930, training loss: 440.3796081542969 = 1.046081304550171 + 50.0 * 8.786670684814453
Epoch 1930, val loss: 1.047034502029419
Epoch 1940, training loss: 440.4438781738281 = 1.0459632873535156 + 50.0 * 8.787958145141602
Epoch 1940, val loss: 1.0469163656234741
Epoch 1950, training loss: 440.5433349609375 = 1.045866847038269 + 50.0 * 8.789949417114258
Epoch 1950, val loss: 1.0468329191207886
Epoch 1960, training loss: 440.6155700683594 = 1.0457730293273926 + 50.0 * 8.791396141052246
Epoch 1960, val loss: 1.0467380285263062
Epoch 1970, training loss: 440.6705322265625 = 1.0456616878509521 + 50.0 * 8.792497634887695
Epoch 1970, val loss: 1.0466331243515015
Epoch 1980, training loss: 440.83740234375 = 1.045601725578308 + 50.0 * 8.795836448669434
Epoch 1980, val loss: 1.0465646982192993
Epoch 1990, training loss: 440.78228759765625 = 1.0454403162002563 + 50.0 * 8.794736862182617
Epoch 1990, val loss: 1.0464286804199219
Epoch 2000, training loss: 440.9769592285156 = 1.0453723669052124 + 50.0 * 8.79863166809082
Epoch 2000, val loss: 1.0463509559631348
Epoch 2010, training loss: 440.9792175292969 = 1.045248031616211 + 50.0 * 8.79867935180664
Epoch 2010, val loss: 1.0462545156478882
Epoch 2020, training loss: 441.0028381347656 = 1.0451369285583496 + 50.0 * 8.799154281616211
Epoch 2020, val loss: 1.0461349487304688
Epoch 2030, training loss: 441.134521484375 = 1.0450553894042969 + 50.0 * 8.801789283752441
Epoch 2030, val loss: 1.0460578203201294
Epoch 2040, training loss: 441.1930236816406 = 1.0449477434158325 + 50.0 * 8.802961349487305
Epoch 2040, val loss: 1.0459449291229248
Epoch 2050, training loss: 441.2712707519531 = 1.044851541519165 + 50.0 * 8.80452823638916
Epoch 2050, val loss: 1.045832872390747
Epoch 2060, training loss: 440.86029052734375 = 1.0445398092269897 + 50.0 * 8.79631519317627
Epoch 2060, val loss: 1.0455538034439087
Epoch 2070, training loss: 441.01116943359375 = 1.0444482564926147 + 50.0 * 8.799334526062012
Epoch 2070, val loss: 1.0454902648925781
Epoch 2080, training loss: 441.28375244140625 = 1.0444376468658447 + 50.0 * 8.804786682128906
Epoch 2080, val loss: 1.0454672574996948
Epoch 2090, training loss: 441.3570861816406 = 1.044358730316162 + 50.0 * 8.806254386901855
Epoch 2090, val loss: 1.045386791229248
Epoch 2100, training loss: 441.2842712402344 = 1.0441735982894897 + 50.0 * 8.804801940917969
Epoch 2100, val loss: 1.0451875925064087
Epoch 2110, training loss: 441.20086669921875 = 1.0439751148223877 + 50.0 * 8.80313777923584
Epoch 2110, val loss: 1.0449732542037964
Epoch 2120, training loss: 441.4242248535156 = 1.0439454317092896 + 50.0 * 8.807605743408203
Epoch 2120, val loss: 1.044960379600525
Epoch 2130, training loss: 441.4794006347656 = 1.0438525676727295 + 50.0 * 8.808711051940918
Epoch 2130, val loss: 1.0448760986328125
Epoch 2140, training loss: 441.58636474609375 = 1.0437718629837036 + 50.0 * 8.81085205078125
Epoch 2140, val loss: 1.0447806119918823
Epoch 2150, training loss: 441.49432373046875 = 1.0435333251953125 + 50.0 * 8.809015274047852
Epoch 2150, val loss: 1.0445919036865234
Epoch 2160, training loss: 441.5789489746094 = 1.0434786081314087 + 50.0 * 8.810708999633789
Epoch 2160, val loss: 1.04453444480896
Epoch 2170, training loss: 441.7618713378906 = 1.0434187650680542 + 50.0 * 8.814369201660156
Epoch 2170, val loss: 1.0444583892822266
Epoch 2180, training loss: 441.81396484375 = 1.0433025360107422 + 50.0 * 8.815413475036621
Epoch 2180, val loss: 1.0443553924560547
Epoch 2190, training loss: 441.8277282714844 = 1.0431276559829712 + 50.0 * 8.815691947937012
Epoch 2190, val loss: 1.044185996055603
Epoch 2200, training loss: 441.8316650390625 = 1.0430254936218262 + 50.0 * 8.815773010253906
Epoch 2200, val loss: 1.0441069602966309
Epoch 2210, training loss: 441.8745422363281 = 1.042883276939392 + 50.0 * 8.816633224487305
Epoch 2210, val loss: 1.0439870357513428
Epoch 2220, training loss: 441.22967529296875 = 1.0423500537872314 + 50.0 * 8.803746223449707
Epoch 2220, val loss: 1.0434597730636597
Epoch 2230, training loss: 441.58563232421875 = 1.0422707796096802 + 50.0 * 8.810867309570312
Epoch 2230, val loss: 1.0434049367904663
Epoch 2240, training loss: 441.76531982421875 = 1.04230535030365 + 50.0 * 8.814460754394531
Epoch 2240, val loss: 1.0434170961380005
Epoch 2250, training loss: 442.0173645019531 = 1.0422816276550293 + 50.0 * 8.819501876831055
Epoch 2250, val loss: 1.0433902740478516
Epoch 2260, training loss: 442.10791015625 = 1.0422011613845825 + 50.0 * 8.821313858032227
Epoch 2260, val loss: 1.0432895421981812
Epoch 2270, training loss: 441.9674072265625 = 1.0419819355010986 + 50.0 * 8.81850814819336
Epoch 2270, val loss: 1.0431044101715088
Epoch 2280, training loss: 442.08355712890625 = 1.0419121980667114 + 50.0 * 8.820833206176758
Epoch 2280, val loss: 1.0430525541305542
Epoch 2290, training loss: 442.2768249511719 = 1.0418291091918945 + 50.0 * 8.824699401855469
Epoch 2290, val loss: 1.042958378791809
Epoch 2300, training loss: 442.3301086425781 = 1.0417319536209106 + 50.0 * 8.825767517089844
Epoch 2300, val loss: 1.042860507965088
Epoch 2310, training loss: 442.30084228515625 = 1.0415672063827515 + 50.0 * 8.825185775756836
Epoch 2310, val loss: 1.0426983833312988
Epoch 2320, training loss: 442.1005859375 = 1.0413180589675903 + 50.0 * 8.821185111999512
Epoch 2320, val loss: 1.0424519777297974
Epoch 2330, training loss: 442.25396728515625 = 1.0411626100540161 + 50.0 * 8.82425594329834
Epoch 2330, val loss: 1.0423285961151123
Epoch 2340, training loss: 442.48504638671875 = 1.0411367416381836 + 50.0 * 8.828878402709961
Epoch 2340, val loss: 1.0423096418380737
Epoch 2350, training loss: 442.64422607421875 = 1.0410574674606323 + 50.0 * 8.832063674926758
Epoch 2350, val loss: 1.042182207107544
Epoch 2360, training loss: 442.4830627441406 = 1.040802240371704 + 50.0 * 8.828845024108887
Epoch 2360, val loss: 1.041933298110962
Epoch 2370, training loss: 442.5670166015625 = 1.0407053232192993 + 50.0 * 8.830526351928711
Epoch 2370, val loss: 1.0417996644973755
Epoch 2380, training loss: 442.2812805175781 = 1.0403155088424683 + 50.0 * 8.824819564819336
Epoch 2380, val loss: 1.0414494276046753
Epoch 2390, training loss: 442.13983154296875 = 1.040023684501648 + 50.0 * 8.821996688842773
Epoch 2390, val loss: 1.0412567853927612
Epoch 2400, training loss: 442.3480529785156 = 1.0399473905563354 + 50.0 * 8.826162338256836
Epoch 2400, val loss: 1.0411874055862427
Epoch 2410, training loss: 442.5149841308594 = 1.0399248600006104 + 50.0 * 8.829501152038574
Epoch 2410, val loss: 1.0411592721939087
Epoch 2420, training loss: 442.8121032714844 = 1.0399144887924194 + 50.0 * 8.835443496704102
Epoch 2420, val loss: 1.0411392450332642
Epoch 2430, training loss: 442.95062255859375 = 1.0398026704788208 + 50.0 * 8.838216781616211
Epoch 2430, val loss: 1.0410209894180298
Epoch 2440, training loss: 442.8140869140625 = 1.0396108627319336 + 50.0 * 8.835489273071289
Epoch 2440, val loss: 1.040863275527954
Epoch 2450, training loss: 442.89691162109375 = 1.03948175907135 + 50.0 * 8.837148666381836
Epoch 2450, val loss: 1.0407414436340332
Epoch 2460, training loss: 442.8912353515625 = 1.039323329925537 + 50.0 * 8.837038040161133
Epoch 2460, val loss: 1.0405772924423218
Epoch 2470, training loss: 442.9700012207031 = 1.0391948223114014 + 50.0 * 8.838616371154785
Epoch 2470, val loss: 1.040445327758789
Epoch 2480, training loss: 443.0187683105469 = 1.0390264987945557 + 50.0 * 8.839594841003418
Epoch 2480, val loss: 1.0403127670288086
Epoch 2490, training loss: 443.066162109375 = 1.0388927459716797 + 50.0 * 8.840545654296875
Epoch 2490, val loss: 1.0401949882507324
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.4479710144927536
0.8642324132434979
The final CL Acc:0.41589, 0.02271, The final GNN Acc:0.86445, 0.00036
Begin epxeriment: noisy_level: 0.2 cont_weight: 50 epoch: der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106472])
remove edge: torch.Size([2, 71072])
updated graph: torch.Size([2, 88896])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 518.0608520507812 = 1.0927928686141968 + 50.0 * 10.339361190795898
Epoch 0, val loss: 1.092606782913208
Epoch 10, training loss: 497.10565185546875 = 1.0890650749206543 + 50.0 * 9.920331954956055
Epoch 10, val loss: 1.0889561176300049
Epoch 20, training loss: 486.9788513183594 = 1.0857428312301636 + 50.0 * 9.717862129211426
Epoch 20, val loss: 1.085669755935669
Epoch 30, training loss: 479.8707580566406 = 1.0825084447860718 + 50.0 * 9.575764656066895
Epoch 30, val loss: 1.082472324371338
Epoch 40, training loss: 474.056884765625 = 1.0794907808303833 + 50.0 * 9.459547996520996
Epoch 40, val loss: 1.0794939994812012
Epoch 50, training loss: 469.35205078125 = 1.0766644477844238 + 50.0 * 9.365508079528809
Epoch 50, val loss: 1.0767111778259277
Epoch 60, training loss: 465.36651611328125 = 1.0740272998809814 + 50.0 * 9.285849571228027
Epoch 60, val loss: 1.0741174221038818
Epoch 70, training loss: 461.9325866699219 = 1.0715625286102295 + 50.0 * 9.217220306396484
Epoch 70, val loss: 1.0716899633407593
Epoch 80, training loss: 458.96270751953125 = 1.069238305091858 + 50.0 * 9.157869338989258
Epoch 80, val loss: 1.0694085359573364
Epoch 90, training loss: 456.3590393066406 = 1.0670450925827026 + 50.0 * 9.105839729309082
Epoch 90, val loss: 1.0672566890716553
Epoch 100, training loss: 454.1753234863281 = 1.0649868249893188 + 50.0 * 9.062207221984863
Epoch 100, val loss: 1.0652436017990112
Epoch 110, training loss: 452.2167663574219 = 1.0630412101745605 + 50.0 * 9.02307415008545
Epoch 110, val loss: 1.063342809677124
Epoch 120, training loss: 450.4917297363281 = 1.0612156391143799 + 50.0 * 8.98861026763916
Epoch 120, val loss: 1.0615577697753906
Epoch 130, training loss: 449.1206359863281 = 1.059500813484192 + 50.0 * 8.961222648620605
Epoch 130, val loss: 1.0598849058151245
Epoch 140, training loss: 447.9756774902344 = 1.0578515529632568 + 50.0 * 8.938356399536133
Epoch 140, val loss: 1.0582798719406128
Epoch 150, training loss: 446.87359619140625 = 1.0562691688537598 + 50.0 * 8.916346549987793
Epoch 150, val loss: 1.0567575693130493
Epoch 160, training loss: 446.0587463378906 = 1.0548015832901 + 50.0 * 8.900078773498535
Epoch 160, val loss: 1.0553377866744995
Epoch 170, training loss: 445.25762939453125 = 1.0534052848815918 + 50.0 * 8.884084701538086
Epoch 170, val loss: 1.0539841651916504
Epoch 180, training loss: 444.5810852050781 = 1.052001714706421 + 50.0 * 8.87058162689209
Epoch 180, val loss: 1.0526461601257324
Epoch 190, training loss: 444.0139465332031 = 1.050696849822998 + 50.0 * 8.859265327453613
Epoch 190, val loss: 1.051403522491455
Epoch 200, training loss: 443.5517578125 = 1.0493932962417603 + 50.0 * 8.85004711151123
Epoch 200, val loss: 1.050154685974121
Epoch 210, training loss: 443.1954650878906 = 1.0480728149414062 + 50.0 * 8.842947959899902
Epoch 210, val loss: 1.0489205121994019
Epoch 220, training loss: 442.982421875 = 1.0467572212219238 + 50.0 * 8.838713645935059
Epoch 220, val loss: 1.0476679801940918
Epoch 230, training loss: 442.6166687011719 = 1.0453367233276367 + 50.0 * 8.831426620483398
Epoch 230, val loss: 1.0463125705718994
Epoch 240, training loss: 442.3496398925781 = 1.0439410209655762 + 50.0 * 8.8261137008667
Epoch 240, val loss: 1.0450186729431152
Epoch 250, training loss: 441.9900817871094 = 1.0424641370773315 + 50.0 * 8.818952560424805
Epoch 250, val loss: 1.043627381324768
Epoch 260, training loss: 441.82037353515625 = 1.0408841371536255 + 50.0 * 8.815589904785156
Epoch 260, val loss: 1.0421377420425415
Epoch 270, training loss: 441.8288269042969 = 1.0392612218856812 + 50.0 * 8.815791130065918
Epoch 270, val loss: 1.040598750114441
Epoch 280, training loss: 441.72930908203125 = 1.0375012159347534 + 50.0 * 8.813836097717285
Epoch 280, val loss: 1.0389297008514404
Epoch 290, training loss: 442.2790222167969 = 1.035557508468628 + 50.0 * 8.824869155883789
Epoch 290, val loss: 1.0370919704437256
Epoch 300, training loss: 441.7622985839844 = 1.0335521697998047 + 50.0 * 8.8145751953125
Epoch 300, val loss: 1.0352354049682617
Epoch 310, training loss: 441.29693603515625 = 1.031333327293396 + 50.0 * 8.805312156677246
Epoch 310, val loss: 1.0331472158432007
Epoch 320, training loss: 441.3067321777344 = 1.029116153717041 + 50.0 * 8.80555248260498
Epoch 320, val loss: 1.0310559272766113
Epoch 330, training loss: 441.1318054199219 = 1.026613712310791 + 50.0 * 8.802103996276855
Epoch 330, val loss: 1.0287158489227295
Epoch 340, training loss: 444.83355712890625 = 1.0239554643630981 + 50.0 * 8.876192092895508
Epoch 340, val loss: 1.0261067152023315
Epoch 350, training loss: 442.6231384277344 = 1.0187078714370728 + 50.0 * 8.832088470458984
Epoch 350, val loss: 1.0213236808776855
Epoch 360, training loss: 441.8489074707031 = 1.0171395540237427 + 50.0 * 8.816635131835938
Epoch 360, val loss: 1.0199599266052246
Epoch 370, training loss: 442.8389587402344 = 1.0146251916885376 + 50.0 * 8.83648681640625
Epoch 370, val loss: 1.0173375606536865
Epoch 380, training loss: 441.593017578125 = 1.0108134746551514 + 50.0 * 8.811644554138184
Epoch 380, val loss: 1.0138280391693115
Epoch 390, training loss: 441.7704162597656 = 1.0071851015090942 + 50.0 * 8.815264701843262
Epoch 390, val loss: 1.0104080438613892
Epoch 400, training loss: 441.4460144042969 = 1.003114104270935 + 50.0 * 8.808857917785645
Epoch 400, val loss: 1.0066412687301636
Epoch 410, training loss: 441.6589660644531 = 0.9989368319511414 + 50.0 * 8.813200950622559
Epoch 410, val loss: 1.0027111768722534
Epoch 420, training loss: 441.67205810546875 = 0.9944100975990295 + 50.0 * 8.813552856445312
Epoch 420, val loss: 0.9984886646270752
Epoch 430, training loss: 441.7052307128906 = 0.9896194338798523 + 50.0 * 8.814311981201172
Epoch 430, val loss: 0.9940192699432373
Epoch 440, training loss: 441.69482421875 = 0.9846071004867554 + 50.0 * 8.814204216003418
Epoch 440, val loss: 0.9893429279327393
Epoch 450, training loss: 441.7325439453125 = 0.9792945981025696 + 50.0 * 8.815064430236816
Epoch 450, val loss: 0.984401524066925
Epoch 460, training loss: 441.7423400878906 = 0.9737294316291809 + 50.0 * 8.815372467041016
Epoch 460, val loss: 0.9792470335960388
Epoch 470, training loss: 441.74066162109375 = 0.967902660369873 + 50.0 * 8.815455436706543
Epoch 470, val loss: 0.9738286733627319
Epoch 480, training loss: 441.9039611816406 = 0.9618082642555237 + 50.0 * 8.818842887878418
Epoch 480, val loss: 0.9681846499443054
Epoch 490, training loss: 441.88922119140625 = 0.9553952217102051 + 50.0 * 8.818676948547363
Epoch 490, val loss: 0.9622606635093689
Epoch 500, training loss: 441.96929931640625 = 0.948844850063324 + 50.0 * 8.820408821105957
Epoch 500, val loss: 0.9561229944229126
Epoch 510, training loss: 442.2347717285156 = 0.9420598149299622 + 50.0 * 8.825854301452637
Epoch 510, val loss: 0.9498938918113708
Epoch 520, training loss: 442.1671142578125 = 0.9349195957183838 + 50.0 * 8.824644088745117
Epoch 520, val loss: 0.9433693885803223
Epoch 530, training loss: 442.1647644042969 = 0.927561342716217 + 50.0 * 8.82474422454834
Epoch 530, val loss: 0.936653733253479
Epoch 540, training loss: 442.2892761230469 = 0.9200818538665771 + 50.0 * 8.827383995056152
Epoch 540, val loss: 0.9297357201576233
Epoch 550, training loss: 442.2563171386719 = 0.9122629761695862 + 50.0 * 8.826881408691406
Epoch 550, val loss: 0.9225846529006958
Epoch 560, training loss: 442.32122802734375 = 0.9043731689453125 + 50.0 * 8.828336715698242
Epoch 560, val loss: 0.9153699278831482
Epoch 570, training loss: 442.423828125 = 0.8963533639907837 + 50.0 * 8.830549240112305
Epoch 570, val loss: 0.9080731272697449
Epoch 580, training loss: 442.6202697753906 = 0.8881505727767944 + 50.0 * 8.83464241027832
Epoch 580, val loss: 0.9007151126861572
Epoch 590, training loss: 442.5625915527344 = 0.8798262476921082 + 50.0 * 8.83365535736084
Epoch 590, val loss: 0.8930295705795288
Epoch 600, training loss: 442.6170959472656 = 0.8714243173599243 + 50.0 * 8.83491325378418
Epoch 600, val loss: 0.8854385614395142
Epoch 610, training loss: 442.7660217285156 = 0.8629050254821777 + 50.0 * 8.838062286376953
Epoch 610, val loss: 0.8777297735214233
Epoch 620, training loss: 442.908447265625 = 0.8543866872787476 + 50.0 * 8.841080665588379
Epoch 620, val loss: 0.8700375556945801
Epoch 630, training loss: 443.1214599609375 = 0.8457818031311035 + 50.0 * 8.845513343811035
Epoch 630, val loss: 0.8622493743896484
Epoch 640, training loss: 443.1855163574219 = 0.8371136784553528 + 50.0 * 8.846967697143555
Epoch 640, val loss: 0.8544812202453613
Epoch 650, training loss: 443.1271057128906 = 0.8283957839012146 + 50.0 * 8.84597396850586
Epoch 650, val loss: 0.846649169921875
Epoch 660, training loss: 443.24420166015625 = 0.8198496699333191 + 50.0 * 8.84848690032959
Epoch 660, val loss: 0.8390558958053589
Epoch 670, training loss: 443.4661560058594 = 0.8113644123077393 + 50.0 * 8.853096008300781
Epoch 670, val loss: 0.8314113020896912
Epoch 680, training loss: 443.59478759765625 = 0.8028525710105896 + 50.0 * 8.855838775634766
Epoch 680, val loss: 0.8238583207130432
Epoch 690, training loss: 443.7982177734375 = 0.794553816318512 + 50.0 * 8.86007308959961
Epoch 690, val loss: 0.816581666469574
Epoch 700, training loss: 443.57476806640625 = 0.7861351370811462 + 50.0 * 8.855772972106934
Epoch 700, val loss: 0.8091545104980469
Epoch 710, training loss: 443.5304870605469 = 0.7778908610343933 + 50.0 * 8.85505199432373
Epoch 710, val loss: 0.8018848299980164
Epoch 720, training loss: 444.0699462890625 = 0.7701455950737 + 50.0 * 8.865996360778809
Epoch 720, val loss: 0.7949861288070679
Epoch 730, training loss: 443.79345703125 = 0.7622283697128296 + 50.0 * 8.860624313354492
Epoch 730, val loss: 0.7881069779396057
Epoch 740, training loss: 444.03326416015625 = 0.754662811756134 + 50.0 * 8.865571975708008
Epoch 740, val loss: 0.7815045714378357
Epoch 750, training loss: 444.00811767578125 = 0.7470766305923462 + 50.0 * 8.86522102355957
Epoch 750, val loss: 0.7748985886573792
Epoch 760, training loss: 444.24615478515625 = 0.7399720549583435 + 50.0 * 8.870123863220215
Epoch 760, val loss: 0.7687691450119019
Epoch 770, training loss: 444.4611511230469 = 0.7330808639526367 + 50.0 * 8.874561309814453
Epoch 770, val loss: 0.7628906965255737
Epoch 780, training loss: 444.3281555175781 = 0.7260958552360535 + 50.0 * 8.872040748596191
Epoch 780, val loss: 0.7569481134414673
Epoch 790, training loss: 444.4753723144531 = 0.7195064425468445 + 50.0 * 8.875117301940918
Epoch 790, val loss: 0.7514083385467529
Epoch 800, training loss: 444.2728271484375 = 0.7128533720970154 + 50.0 * 8.871199607849121
Epoch 800, val loss: 0.7457466721534729
Epoch 810, training loss: 444.2467956542969 = 0.7066685557365417 + 50.0 * 8.870802879333496
Epoch 810, val loss: 0.7405980825424194
Epoch 820, training loss: 444.54852294921875 = 0.7007293105125427 + 50.0 * 8.87695598602295
Epoch 820, val loss: 0.7356212735176086
Epoch 830, training loss: 444.57427978515625 = 0.6949430108070374 + 50.0 * 8.877586364746094
Epoch 830, val loss: 0.7308187484741211
Epoch 840, training loss: 444.56549072265625 = 0.6894349455833435 + 50.0 * 8.877521514892578
Epoch 840, val loss: 0.726206362247467
Epoch 850, training loss: 444.53021240234375 = 0.6839230060577393 + 50.0 * 8.876925468444824
Epoch 850, val loss: 0.7218170762062073
Epoch 860, training loss: 444.4881591796875 = 0.6787271499633789 + 50.0 * 8.876188278198242
Epoch 860, val loss: 0.7177260518074036
Epoch 870, training loss: 444.71826171875 = 0.6740341782569885 + 50.0 * 8.880884170532227
Epoch 870, val loss: 0.7139027714729309
Epoch 880, training loss: 444.9362487792969 = 0.6694178581237793 + 50.0 * 8.885336875915527
Epoch 880, val loss: 0.7103350162506104
Epoch 890, training loss: 444.7831726074219 = 0.6644904613494873 + 50.0 * 8.882373809814453
Epoch 890, val loss: 0.706584632396698
Epoch 900, training loss: 445.107666015625 = 0.6606743931770325 + 50.0 * 8.88893985748291
Epoch 900, val loss: 0.7033809423446655
Epoch 910, training loss: 445.1125183105469 = 0.6563582420349121 + 50.0 * 8.88912296295166
Epoch 910, val loss: 0.7003008723258972
Epoch 920, training loss: 445.3809814453125 = 0.652513861656189 + 50.0 * 8.894569396972656
Epoch 920, val loss: 0.6974481344223022
Epoch 930, training loss: 445.6023254394531 = 0.6487674117088318 + 50.0 * 8.899070739746094
Epoch 930, val loss: 0.6946865320205688
Epoch 940, training loss: 445.5751037597656 = 0.6450109481811523 + 50.0 * 8.898601531982422
Epoch 940, val loss: 0.6919903755187988
Epoch 950, training loss: 445.4438781738281 = 0.6413750052452087 + 50.0 * 8.896049499511719
Epoch 950, val loss: 0.689222514629364
Epoch 960, training loss: 445.7066650390625 = 0.638023853302002 + 50.0 * 8.901372909545898
Epoch 960, val loss: 0.6869236826896667
Epoch 970, training loss: 445.7610778808594 = 0.6347830891609192 + 50.0 * 8.902525901794434
Epoch 970, val loss: 0.6846908330917358
Epoch 980, training loss: 445.90008544921875 = 0.631739616394043 + 50.0 * 8.905366897583008
Epoch 980, val loss: 0.6825939416885376
Epoch 990, training loss: 446.0704040527344 = 0.6288074254989624 + 50.0 * 8.908831596374512
Epoch 990, val loss: 0.6806657314300537
Epoch 1000, training loss: 445.73583984375 = 0.6257022619247437 + 50.0 * 8.902202606201172
Epoch 1000, val loss: 0.6785131692886353
Epoch 1010, training loss: 445.53594970703125 = 0.6229649782180786 + 50.0 * 8.898260116577148
Epoch 1010, val loss: 0.6767549514770508
Epoch 1020, training loss: 446.00531005859375 = 0.6203913688659668 + 50.0 * 8.907698631286621
Epoch 1020, val loss: 0.6751395463943481
Epoch 1030, training loss: 446.3503723144531 = 0.6180039048194885 + 50.0 * 8.914647102355957
Epoch 1030, val loss: 0.6736651062965393
Epoch 1040, training loss: 446.5394287109375 = 0.6155283451080322 + 50.0 * 8.918478012084961
Epoch 1040, val loss: 0.6720848679542542
Epoch 1050, training loss: 446.51220703125 = 0.6130623817443848 + 50.0 * 8.917983055114746
Epoch 1050, val loss: 0.6705307960510254
Epoch 1060, training loss: 446.5968933105469 = 0.6107125282287598 + 50.0 * 8.919723510742188
Epoch 1060, val loss: 0.6691513061523438
Epoch 1070, training loss: 446.6832580566406 = 0.6083940863609314 + 50.0 * 8.921497344970703
Epoch 1070, val loss: 0.6676976084709167
Epoch 1080, training loss: 446.7456970214844 = 0.6061677932739258 + 50.0 * 8.92279052734375
Epoch 1080, val loss: 0.6664450168609619
Epoch 1090, training loss: 446.8348388671875 = 0.6040343642234802 + 50.0 * 8.924615859985352
Epoch 1090, val loss: 0.6652795076370239
Epoch 1100, training loss: 447.0561828613281 = 0.6019816398620605 + 50.0 * 8.929083824157715
Epoch 1100, val loss: 0.6640986204147339
Epoch 1110, training loss: 446.98095703125 = 0.5999225974082947 + 50.0 * 8.927620887756348
Epoch 1110, val loss: 0.6629571914672852
Epoch 1120, training loss: 446.8929138183594 = 0.5979062914848328 + 50.0 * 8.92590045928955
Epoch 1120, val loss: 0.6618139147758484
Epoch 1130, training loss: 447.26068115234375 = 0.5960052013397217 + 50.0 * 8.933293342590332
Epoch 1130, val loss: 0.6610250473022461
Epoch 1140, training loss: 447.3982849121094 = 0.5941362380981445 + 50.0 * 8.93608283996582
Epoch 1140, val loss: 0.6599431037902832
Epoch 1150, training loss: 447.4185485839844 = 0.5923197865486145 + 50.0 * 8.936524391174316
Epoch 1150, val loss: 0.6591523289680481
Epoch 1160, training loss: 447.5297546386719 = 0.5905627608299255 + 50.0 * 8.938783645629883
Epoch 1160, val loss: 0.6582393646240234
Epoch 1170, training loss: 447.5716552734375 = 0.5887925624847412 + 50.0 * 8.939657211303711
Epoch 1170, val loss: 0.6573764085769653
Epoch 1180, training loss: 447.5611267089844 = 0.5870329737663269 + 50.0 * 8.939481735229492
Epoch 1180, val loss: 0.6564974784851074
Epoch 1190, training loss: 447.747802734375 = 0.5853580236434937 + 50.0 * 8.943248748779297
Epoch 1190, val loss: 0.6557614207267761
Epoch 1200, training loss: 447.7631530761719 = 0.5837314128875732 + 50.0 * 8.943588256835938
Epoch 1200, val loss: 0.6550949811935425
Epoch 1210, training loss: 447.78045654296875 = 0.5820488929748535 + 50.0 * 8.943967819213867
Epoch 1210, val loss: 0.6542920470237732
Epoch 1220, training loss: 447.9488220214844 = 0.5804526209831238 + 50.0 * 8.947367668151855
Epoch 1220, val loss: 0.6535667777061462
Epoch 1230, training loss: 448.2799987792969 = 0.5789176225662231 + 50.0 * 8.954021453857422
Epoch 1230, val loss: 0.6529345512390137
Epoch 1240, training loss: 448.40325927734375 = 0.5773875117301941 + 50.0 * 8.956517219543457
Epoch 1240, val loss: 0.6521964073181152
Epoch 1250, training loss: 448.3452453613281 = 0.5758181810379028 + 50.0 * 8.955389022827148
Epoch 1250, val loss: 0.6516578793525696
Epoch 1260, training loss: 448.40240478515625 = 0.5743111371994019 + 50.0 * 8.956562042236328
Epoch 1260, val loss: 0.6509644389152527
Epoch 1270, training loss: 448.50006103515625 = 0.5728647112846375 + 50.0 * 8.95854377746582
Epoch 1270, val loss: 0.6503662467002869
Epoch 1280, training loss: 448.6898498535156 = 0.5714141130447388 + 50.0 * 8.962368965148926
Epoch 1280, val loss: 0.6497505307197571
Epoch 1290, training loss: 448.55718994140625 = 0.5699837803840637 + 50.0 * 8.959744453430176
Epoch 1290, val loss: 0.6492056250572205
Epoch 1300, training loss: 448.6463317871094 = 0.568596601486206 + 50.0 * 8.961554527282715
Epoch 1300, val loss: 0.6488396525382996
Epoch 1310, training loss: 448.8799743652344 = 0.5672507882118225 + 50.0 * 8.966254234313965
Epoch 1310, val loss: 0.648354172706604
Epoch 1320, training loss: 448.94183349609375 = 0.5658442974090576 + 50.0 * 8.967519760131836
Epoch 1320, val loss: 0.647732138633728
Epoch 1330, training loss: 449.16485595703125 = 0.5645050406455994 + 50.0 * 8.972006797790527
Epoch 1330, val loss: 0.6471935510635376
Epoch 1340, training loss: 448.9712219238281 = 0.5630927085876465 + 50.0 * 8.968162536621094
Epoch 1340, val loss: 0.6466606259346008
Epoch 1350, training loss: 448.986083984375 = 0.5616928339004517 + 50.0 * 8.968487739562988
Epoch 1350, val loss: 0.6460952162742615
Epoch 1360, training loss: 449.216552734375 = 0.5604135990142822 + 50.0 * 8.973122596740723
Epoch 1360, val loss: 0.6455904841423035
Epoch 1370, training loss: 449.39910888671875 = 0.5591084361076355 + 50.0 * 8.976799964904785
Epoch 1370, val loss: 0.6450737118721008
Epoch 1380, training loss: 449.4657897949219 = 0.5577885508537292 + 50.0 * 8.97815990447998
Epoch 1380, val loss: 0.6446115970611572
Epoch 1390, training loss: 449.61053466796875 = 0.5565178394317627 + 50.0 * 8.981080055236816
Epoch 1390, val loss: 0.644150972366333
Epoch 1400, training loss: 449.24615478515625 = 0.5551081299781799 + 50.0 * 8.973820686340332
Epoch 1400, val loss: 0.6432797908782959
Epoch 1410, training loss: 449.0723876953125 = 0.5538354516029358 + 50.0 * 8.97037124633789
Epoch 1410, val loss: 0.6430769562721252
Epoch 1420, training loss: 449.4360656738281 = 0.5526682138442993 + 50.0 * 8.977667808532715
Epoch 1420, val loss: 0.6424566507339478
Epoch 1430, training loss: 449.8172912597656 = 0.5512508749961853 + 50.0 * 8.985321044921875
Epoch 1430, val loss: 0.6417531371116638
Epoch 1440, training loss: 449.6131591796875 = 0.5499902963638306 + 50.0 * 8.981263160705566
Epoch 1440, val loss: 0.6415858268737793
Epoch 1450, training loss: 449.7443542480469 = 0.5488643646240234 + 50.0 * 8.983909606933594
Epoch 1450, val loss: 0.6412127017974854
Epoch 1460, training loss: 448.0153503417969 = 0.547379732131958 + 50.0 * 8.949358940124512
Epoch 1460, val loss: 0.6411263942718506
Epoch 1470, training loss: 449.25689697265625 = 0.5464516878128052 + 50.0 * 8.97420883178711
Epoch 1470, val loss: 0.639552116394043
Epoch 1480, training loss: 449.2449035644531 = 0.5453947186470032 + 50.0 * 8.973990440368652
Epoch 1480, val loss: 0.6402319669723511
Epoch 1490, training loss: 449.5064697265625 = 0.5442097783088684 + 50.0 * 8.97924518585205
Epoch 1490, val loss: 0.6391453146934509
Epoch 1500, training loss: 449.77044677734375 = 0.5430362820625305 + 50.0 * 8.984548568725586
Epoch 1500, val loss: 0.6390916109085083
Epoch 1510, training loss: 450.0623779296875 = 0.5417807102203369 + 50.0 * 8.990411758422852
Epoch 1510, val loss: 0.6384281516075134
Epoch 1520, training loss: 450.32598876953125 = 0.5405446887016296 + 50.0 * 8.995708465576172
Epoch 1520, val loss: 0.6380344033241272
Epoch 1530, training loss: 450.20513916015625 = 0.5393001437187195 + 50.0 * 8.993316650390625
Epoch 1530, val loss: 0.6375762224197388
Epoch 1540, training loss: 450.3981018066406 = 0.538116455078125 + 50.0 * 8.997200012207031
Epoch 1540, val loss: 0.6371400356292725
Epoch 1550, training loss: 450.6253967285156 = 0.5368839502334595 + 50.0 * 9.00177001953125
Epoch 1550, val loss: 0.6367462277412415
Epoch 1560, training loss: 450.7616882324219 = 0.535663366317749 + 50.0 * 9.004520416259766
Epoch 1560, val loss: 0.6362000703811646
Epoch 1570, training loss: 450.8257141113281 = 0.5344169735908508 + 50.0 * 9.005825996398926
Epoch 1570, val loss: 0.6358222961425781
Epoch 1580, training loss: 450.8104553222656 = 0.5331859588623047 + 50.0 * 9.005545616149902
Epoch 1580, val loss: 0.6353310346603394
Epoch 1590, training loss: 450.92425537109375 = 0.5319603681564331 + 50.0 * 9.007845878601074
Epoch 1590, val loss: 0.634753406047821
Epoch 1600, training loss: 451.2253723144531 = 0.5307232141494751 + 50.0 * 9.013893127441406
Epoch 1600, val loss: 0.6342652440071106
Epoch 1610, training loss: 450.720458984375 = 0.5292465686798096 + 50.0 * 9.003824234008789
Epoch 1610, val loss: 0.6335519552230835
Epoch 1620, training loss: 451.0501403808594 = 0.5281272530555725 + 50.0 * 9.0104398727417
Epoch 1620, val loss: 0.6332820653915405
Epoch 1630, training loss: 451.1737365722656 = 0.5269109010696411 + 50.0 * 9.01293659210205
Epoch 1630, val loss: 0.6327682137489319
Epoch 1640, training loss: 451.3691101074219 = 0.5256462693214417 + 50.0 * 9.01686954498291
Epoch 1640, val loss: 0.6321597099304199
Epoch 1650, training loss: 451.59576416015625 = 0.524414598941803 + 50.0 * 9.021427154541016
Epoch 1650, val loss: 0.6317664980888367
Epoch 1660, training loss: 451.721923828125 = 0.5231531262397766 + 50.0 * 9.023975372314453
Epoch 1660, val loss: 0.6312704682350159
Epoch 1670, training loss: 451.5712585449219 = 0.5218720436096191 + 50.0 * 9.020987510681152
Epoch 1670, val loss: 0.6307384371757507
Epoch 1680, training loss: 451.9100036621094 = 0.5206814408302307 + 50.0 * 9.027786254882812
Epoch 1680, val loss: 0.6303120255470276
Epoch 1690, training loss: 452.1360778808594 = 0.519481360912323 + 50.0 * 9.032332420349121
Epoch 1690, val loss: 0.629789412021637
Epoch 1700, training loss: 452.0264587402344 = 0.5182121992111206 + 50.0 * 9.03016471862793
Epoch 1700, val loss: 0.6292529106140137
Epoch 1710, training loss: 452.1551818847656 = 0.5169689059257507 + 50.0 * 9.032764434814453
Epoch 1710, val loss: 0.6288266777992249
Epoch 1720, training loss: 452.3442077636719 = 0.5157314538955688 + 50.0 * 9.036569595336914
Epoch 1720, val loss: 0.6281238198280334
Epoch 1730, training loss: 452.22674560546875 = 0.5144739747047424 + 50.0 * 9.034245491027832
Epoch 1730, val loss: 0.6277511715888977
Epoch 1740, training loss: 452.7143249511719 = 0.513245165348053 + 50.0 * 9.044021606445312
Epoch 1740, val loss: 0.6269532442092896
Epoch 1750, training loss: 452.7217712402344 = 0.5119719505310059 + 50.0 * 9.044196128845215
Epoch 1750, val loss: 0.6267072558403015
Epoch 1760, training loss: 452.69537353515625 = 0.5108205080032349 + 50.0 * 9.04369068145752
Epoch 1760, val loss: 0.6262335777282715
Epoch 1770, training loss: 452.8004455566406 = 0.5095164775848389 + 50.0 * 9.045818328857422
Epoch 1770, val loss: 0.6257041692733765
Epoch 1780, training loss: 452.97381591796875 = 0.5082331895828247 + 50.0 * 9.049311637878418
Epoch 1780, val loss: 0.6251427531242371
Epoch 1790, training loss: 452.6009826660156 = 0.5069473385810852 + 50.0 * 9.04188060760498
Epoch 1790, val loss: 0.6246862411499023
Epoch 1800, training loss: 452.88323974609375 = 0.5056907534599304 + 50.0 * 9.047551155090332
Epoch 1800, val loss: 0.6240312457084656
Epoch 1810, training loss: 453.2276306152344 = 0.504425048828125 + 50.0 * 9.054464340209961
Epoch 1810, val loss: 0.6235395073890686
Epoch 1820, training loss: 453.1226806640625 = 0.5031157732009888 + 50.0 * 9.052391052246094
Epoch 1820, val loss: 0.6230823397636414
Epoch 1830, training loss: 453.29949951171875 = 0.5017967820167542 + 50.0 * 9.055953979492188
Epoch 1830, val loss: 0.6224730610847473
Epoch 1840, training loss: 453.67584228515625 = 0.5004903674125671 + 50.0 * 9.063507080078125
Epoch 1840, val loss: 0.6219751834869385
Epoch 1850, training loss: 453.5434265136719 = 0.4991394281387329 + 50.0 * 9.060885429382324
Epoch 1850, val loss: 0.6214215755462646
Epoch 1860, training loss: 453.5379333496094 = 0.49777382612228394 + 50.0 * 9.060803413391113
Epoch 1860, val loss: 0.6207265853881836
Epoch 1870, training loss: 453.8923645019531 = 0.4964666962623596 + 50.0 * 9.067917823791504
Epoch 1870, val loss: 0.6203321814537048
Epoch 1880, training loss: 453.8961486816406 = 0.495126336812973 + 50.0 * 9.068020820617676
Epoch 1880, val loss: 0.6197285652160645
Epoch 1890, training loss: 453.85589599609375 = 0.4937187433242798 + 50.0 * 9.067243576049805
Epoch 1890, val loss: 0.6192065477371216
Epoch 1900, training loss: 453.94122314453125 = 0.4923735558986664 + 50.0 * 9.068977355957031
Epoch 1900, val loss: 0.6185898780822754
Epoch 1910, training loss: 454.09295654296875 = 0.49103009700775146 + 50.0 * 9.072038650512695
Epoch 1910, val loss: 0.6180151104927063
Epoch 1920, training loss: 454.1276550292969 = 0.489675372838974 + 50.0 * 9.072759628295898
Epoch 1920, val loss: 0.6175520420074463
Epoch 1930, training loss: 454.0328063964844 = 0.48826608061790466 + 50.0 * 9.070890426635742
Epoch 1930, val loss: 0.6168802976608276
Epoch 1940, training loss: 454.1313171386719 = 0.48685869574546814 + 50.0 * 9.07288932800293
Epoch 1940, val loss: 0.6163674592971802
Epoch 1950, training loss: 454.21630859375 = 0.485482782125473 + 50.0 * 9.074616432189941
Epoch 1950, val loss: 0.615845263004303
Epoch 1960, training loss: 454.42071533203125 = 0.484096497297287 + 50.0 * 9.07873249053955
Epoch 1960, val loss: 0.6153097748756409
Epoch 1970, training loss: 454.41351318359375 = 0.48266077041625977 + 50.0 * 9.078617095947266
Epoch 1970, val loss: 0.614780604839325
Epoch 1980, training loss: 454.4820861816406 = 0.4812460243701935 + 50.0 * 9.08001708984375
Epoch 1980, val loss: 0.6141653060913086
Epoch 1990, training loss: 454.548095703125 = 0.4798312485218048 + 50.0 * 9.081365585327148
Epoch 1990, val loss: 0.6136823296546936
Epoch 2000, training loss: 454.6968078613281 = 0.4784122109413147 + 50.0 * 9.084367752075195
Epoch 2000, val loss: 0.6129571199417114
Epoch 2010, training loss: 454.6031494140625 = 0.4769326448440552 + 50.0 * 9.082524299621582
Epoch 2010, val loss: 0.6122817397117615
Epoch 2020, training loss: 454.68731689453125 = 0.4754745066165924 + 50.0 * 9.084237098693848
Epoch 2020, val loss: 0.6117916107177734
Epoch 2030, training loss: 454.8064270019531 = 0.4740171730518341 + 50.0 * 9.086647987365723
Epoch 2030, val loss: 0.611115038394928
Epoch 2040, training loss: 454.7710876464844 = 0.47254034876823425 + 50.0 * 9.085970878601074
Epoch 2040, val loss: 0.6104342937469482
Epoch 2050, training loss: 454.99993896484375 = 0.47106292843818665 + 50.0 * 9.090577125549316
Epoch 2050, val loss: 0.6097378730773926
Epoch 2060, training loss: 454.9671325683594 = 0.4695354998111725 + 50.0 * 9.089951515197754
Epoch 2060, val loss: 0.6090234518051147
Epoch 2070, training loss: 454.8998107910156 = 0.4679810404777527 + 50.0 * 9.08863639831543
Epoch 2070, val loss: 0.6083919405937195
Epoch 2080, training loss: 454.9420166015625 = 0.4664555490016937 + 50.0 * 9.089510917663574
Epoch 2080, val loss: 0.607354998588562
Epoch 2090, training loss: 455.0883483886719 = 0.46493759751319885 + 50.0 * 9.09246826171875
Epoch 2090, val loss: 0.6068195104598999
Epoch 2100, training loss: 455.16796875 = 0.46341633796691895 + 50.0 * 9.094091415405273
Epoch 2100, val loss: 0.6063032746315002
Epoch 2110, training loss: 455.3494567871094 = 0.46188974380493164 + 50.0 * 9.09775161743164
Epoch 2110, val loss: 0.6055517792701721
Epoch 2120, training loss: 454.7294006347656 = 0.46035417914390564 + 50.0 * 9.085380554199219
Epoch 2120, val loss: 0.6047504544258118
Epoch 2130, training loss: 454.7425231933594 = 0.4588540494441986 + 50.0 * 9.085673332214355
Epoch 2130, val loss: 0.604370653629303
Epoch 2140, training loss: 455.0729675292969 = 0.45729032158851624 + 50.0 * 9.092313766479492
Epoch 2140, val loss: 0.6036567091941833
Epoch 2150, training loss: 455.526611328125 = 0.45573899149894714 + 50.0 * 9.101417541503906
Epoch 2150, val loss: 0.6028597950935364
Epoch 2160, training loss: 455.5156555175781 = 0.45415884256362915 + 50.0 * 9.101229667663574
Epoch 2160, val loss: 0.6022921204566956
Epoch 2170, training loss: 454.8831481933594 = 0.4524923861026764 + 50.0 * 9.088613510131836
Epoch 2170, val loss: 0.6011085510253906
Epoch 2180, training loss: 454.4429626464844 = 0.45076072216033936 + 50.0 * 9.079843521118164
Epoch 2180, val loss: 0.6007412672042847
Epoch 2190, training loss: 454.9667053222656 = 0.44933274388313293 + 50.0 * 9.090347290039062
Epoch 2190, val loss: 0.599854588508606
Epoch 2200, training loss: 452.7099609375 = 0.4485251307487488 + 50.0 * 9.045228958129883
Epoch 2200, val loss: 0.6003075242042542
Epoch 2210, training loss: 450.8961181640625 = 0.4480225145816803 + 50.0 * 9.00896167755127
Epoch 2210, val loss: 0.6005608439445496
Epoch 2220, training loss: 452.445068359375 = 0.44615715742111206 + 50.0 * 9.03997802734375
Epoch 2220, val loss: 0.5989402532577515
Epoch 2230, training loss: 452.3556213378906 = 0.44451719522476196 + 50.0 * 9.038222312927246
Epoch 2230, val loss: 0.5971459150314331
Epoch 2240, training loss: 451.820556640625 = 0.44276535511016846 + 50.0 * 9.027555465698242
Epoch 2240, val loss: 0.5982077717781067
Epoch 2250, training loss: 452.3980407714844 = 0.4410415291786194 + 50.0 * 9.039139747619629
Epoch 2250, val loss: 0.5965357422828674
Epoch 2260, training loss: 452.97607421875 = 0.4393842816352844 + 50.0 * 9.05073356628418
Epoch 2260, val loss: 0.5961143374443054
Epoch 2270, training loss: 453.36846923828125 = 0.4377345144748688 + 50.0 * 9.058614730834961
Epoch 2270, val loss: 0.5951389074325562
Epoch 2280, training loss: 453.8330993652344 = 0.4361189305782318 + 50.0 * 9.067939758300781
Epoch 2280, val loss: 0.5944024324417114
Epoch 2290, training loss: 454.0859375 = 0.4344947934150696 + 50.0 * 9.073028564453125
Epoch 2290, val loss: 0.5940744280815125
Epoch 2300, training loss: 454.3959045410156 = 0.43286070227622986 + 50.0 * 9.07926082611084
Epoch 2300, val loss: 0.5933639407157898
Epoch 2310, training loss: 454.5172424316406 = 0.4312145709991455 + 50.0 * 9.081720352172852
Epoch 2310, val loss: 0.5928512811660767
Epoch 2320, training loss: 454.6767883300781 = 0.42957454919815063 + 50.0 * 9.084944725036621
Epoch 2320, val loss: 0.592237114906311
Epoch 2330, training loss: 454.8544616699219 = 0.4279387593269348 + 50.0 * 9.088530540466309
Epoch 2330, val loss: 0.5917280316352844
Epoch 2340, training loss: 454.9515380859375 = 0.4262920618057251 + 50.0 * 9.09050464630127
Epoch 2340, val loss: 0.591188371181488
Epoch 2350, training loss: 455.0013427734375 = 0.42465320229530334 + 50.0 * 9.091533660888672
Epoch 2350, val loss: 0.5906000733375549
Epoch 2360, training loss: 455.2635192871094 = 0.42300763726234436 + 50.0 * 9.096810340881348
Epoch 2360, val loss: 0.5899679660797119
Epoch 2370, training loss: 455.1007995605469 = 0.42134568095207214 + 50.0 * 9.093588829040527
Epoch 2370, val loss: 0.5892975330352783
Epoch 2380, training loss: 455.1979675292969 = 0.41974374651908875 + 50.0 * 9.095564842224121
Epoch 2380, val loss: 0.5888095498085022
Epoch 2390, training loss: 455.53131103515625 = 0.41809484362602234 + 50.0 * 9.102264404296875
Epoch 2390, val loss: 0.5881190299987793
Epoch 2400, training loss: 455.0557556152344 = 0.41639718413352966 + 50.0 * 9.09278678894043
Epoch 2400, val loss: 0.5876331329345703
Epoch 2410, training loss: 455.5570068359375 = 0.4147140681743622 + 50.0 * 9.102846145629883
Epoch 2410, val loss: 0.5869052410125732
Epoch 2420, training loss: 455.821533203125 = 0.4130418300628662 + 50.0 * 9.108169555664062
Epoch 2420, val loss: 0.5862205624580383
Epoch 2430, training loss: 455.8108825683594 = 0.4113132655620575 + 50.0 * 9.107991218566895
Epoch 2430, val loss: 0.5857070684432983
Epoch 2440, training loss: 455.78857421875 = 0.40958866477012634 + 50.0 * 9.107580184936523
Epoch 2440, val loss: 0.5849868059158325
Epoch 2450, training loss: 456.1153869628906 = 0.40789276361465454 + 50.0 * 9.114150047302246
Epoch 2450, val loss: 0.5843715667724609
Epoch 2460, training loss: 456.19818115234375 = 0.40618565678596497 + 50.0 * 9.115839958190918
Epoch 2460, val loss: 0.5837913155555725
Epoch 2470, training loss: 456.2110290527344 = 0.4044507145881653 + 50.0 * 9.116131782531738
Epoch 2470, val loss: 0.5831612348556519
Epoch 2480, training loss: 456.2265319824219 = 0.40274083614349365 + 50.0 * 9.116476058959961
Epoch 2480, val loss: 0.582538366317749
Epoch 2490, training loss: 456.3721618652344 = 0.40102800726890564 + 50.0 * 9.119422912597656
Epoch 2490, val loss: 0.5819538235664368
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7711594202898551
0.8177207853365211
=== training gcn model ===
Epoch 0, training loss: 516.6936645507812 = 1.119088888168335 + 50.0 * 10.311491966247559
Epoch 0, val loss: 1.1168075799942017
Epoch 10, training loss: 496.24981689453125 = 1.1150106191635132 + 50.0 * 9.902695655822754
Epoch 10, val loss: 1.112802505493164
Epoch 20, training loss: 485.6545104980469 = 1.1110683679580688 + 50.0 * 9.690869331359863
Epoch 20, val loss: 1.1089204549789429
Epoch 30, training loss: 478.3927001953125 = 1.1073012351989746 + 50.0 * 9.545707702636719
Epoch 30, val loss: 1.1052168607711792
Epoch 40, training loss: 473.1645812988281 = 1.10368812084198 + 50.0 * 9.441217422485352
Epoch 40, val loss: 1.1016720533370972
Epoch 50, training loss: 468.7384338378906 = 1.100224256515503 + 50.0 * 9.352764129638672
Epoch 50, val loss: 1.0982658863067627
Epoch 60, training loss: 464.98675537109375 = 1.096855640411377 + 50.0 * 9.27779769897461
Epoch 60, val loss: 1.094967007637024
Epoch 70, training loss: 461.7660217285156 = 1.0935845375061035 + 50.0 * 9.213448524475098
Epoch 70, val loss: 1.091761827468872
Epoch 80, training loss: 459.035888671875 = 1.0904005765914917 + 50.0 * 9.158909797668457
Epoch 80, val loss: 1.088661789894104
Epoch 90, training loss: 456.7037658691406 = 1.087289810180664 + 50.0 * 9.112329483032227
Epoch 90, val loss: 1.0856364965438843
Epoch 100, training loss: 454.67950439453125 = 1.0842726230621338 + 50.0 * 9.071905136108398
Epoch 100, val loss: 1.0827081203460693
Epoch 110, training loss: 452.91192626953125 = 1.0813065767288208 + 50.0 * 9.036612510681152
Epoch 110, val loss: 1.0798313617706299
Epoch 120, training loss: 451.38397216796875 = 1.0784296989440918 + 50.0 * 9.006111145019531
Epoch 120, val loss: 1.0770586729049683
Epoch 130, training loss: 450.09527587890625 = 1.0755890607833862 + 50.0 * 8.980393409729004
Epoch 130, val loss: 1.0743352174758911
Epoch 140, training loss: 448.8536682128906 = 1.0728126764297485 + 50.0 * 8.95561695098877
Epoch 140, val loss: 1.0716735124588013
Epoch 150, training loss: 447.8426208496094 = 1.0701104402542114 + 50.0 * 8.935450553894043
Epoch 150, val loss: 1.0690810680389404
Epoch 160, training loss: 446.9984436035156 = 1.0674470663070679 + 50.0 * 8.918620109558105
Epoch 160, val loss: 1.066548466682434
Epoch 170, training loss: 446.1675720214844 = 1.0648205280303955 + 50.0 * 8.902054786682129
Epoch 170, val loss: 1.0640676021575928
Epoch 180, training loss: 445.5909423828125 = 1.0622618198394775 + 50.0 * 8.890573501586914
Epoch 180, val loss: 1.0616228580474854
Epoch 190, training loss: 444.9026184082031 = 1.0597178936004639 + 50.0 * 8.87685775756836
Epoch 190, val loss: 1.0592385530471802
Epoch 200, training loss: 444.5973205566406 = 1.0572108030319214 + 50.0 * 8.87080192565918
Epoch 200, val loss: 1.0568311214447021
Epoch 210, training loss: 443.9748840332031 = 1.0546849966049194 + 50.0 * 8.858404159545898
Epoch 210, val loss: 1.0544649362564087
Epoch 220, training loss: 443.4885559082031 = 1.052172303199768 + 50.0 * 8.848727226257324
Epoch 220, val loss: 1.0520803928375244
Epoch 230, training loss: 443.23919677734375 = 1.0496665239334106 + 50.0 * 8.843791007995605
Epoch 230, val loss: 1.0497034788131714
Epoch 240, training loss: 442.9407958984375 = 1.0471056699752808 + 50.0 * 8.837873458862305
Epoch 240, val loss: 1.047267198562622
Epoch 250, training loss: 442.6487731933594 = 1.0444936752319336 + 50.0 * 8.832085609436035
Epoch 250, val loss: 1.0447719097137451
Epoch 260, training loss: 442.4358215332031 = 1.0418546199798584 + 50.0 * 8.827879905700684
Epoch 260, val loss: 1.0422582626342773
Epoch 270, training loss: 442.34393310546875 = 1.0391606092453003 + 50.0 * 8.826095581054688
Epoch 270, val loss: 1.0396807193756104
Epoch 280, training loss: 442.11895751953125 = 1.0363656282424927 + 50.0 * 8.821651458740234
Epoch 280, val loss: 1.036999225616455
Epoch 290, training loss: 442.01885986328125 = 1.033390998840332 + 50.0 * 8.819709777832031
Epoch 290, val loss: 1.034162998199463
Epoch 300, training loss: 441.94989013671875 = 1.0304008722305298 + 50.0 * 8.818389892578125
Epoch 300, val loss: 1.0313152074813843
Epoch 310, training loss: 441.7946472167969 = 1.0272337198257446 + 50.0 * 8.815348625183105
Epoch 310, val loss: 1.0282320976257324
Epoch 320, training loss: 441.91497802734375 = 1.0239781141281128 + 50.0 * 8.817819595336914
Epoch 320, val loss: 1.0250718593597412
Epoch 330, training loss: 441.7323913574219 = 1.020369052886963 + 50.0 * 8.814240455627441
Epoch 330, val loss: 1.0216140747070312
Epoch 340, training loss: 441.68182373046875 = 1.0168274641036987 + 50.0 * 8.813300132751465
Epoch 340, val loss: 1.0182071924209595
Epoch 350, training loss: 441.72021484375 = 1.0129731893539429 + 50.0 * 8.8141450881958
Epoch 350, val loss: 1.0144633054733276
Epoch 360, training loss: 441.6986083984375 = 1.0089271068572998 + 50.0 * 8.813793182373047
Epoch 360, val loss: 1.0105730295181274
Epoch 370, training loss: 441.5982971191406 = 1.0045713186264038 + 50.0 * 8.811874389648438
Epoch 370, val loss: 1.00638747215271
Epoch 380, training loss: 441.65789794921875 = 0.9998104572296143 + 50.0 * 8.813161849975586
Epoch 380, val loss: 1.0016794204711914
Epoch 390, training loss: 442.1507873535156 = 0.9955375790596008 + 50.0 * 8.823104858398438
Epoch 390, val loss: 0.997647762298584
Epoch 400, training loss: 441.3679504394531 = 0.9902186393737793 + 50.0 * 8.807555198669434
Epoch 400, val loss: 0.9926042556762695
Epoch 410, training loss: 441.794921875 = 0.9851891398429871 + 50.0 * 8.816194534301758
Epoch 410, val loss: 0.9876998066902161
Epoch 420, training loss: 441.65618896484375 = 0.9796470999717712 + 50.0 * 8.813530921936035
Epoch 420, val loss: 0.9824819564819336
Epoch 430, training loss: 441.89739990234375 = 0.9740773439407349 + 50.0 * 8.818466186523438
Epoch 430, val loss: 0.9771308898925781
Epoch 440, training loss: 441.9975891113281 = 0.968174159526825 + 50.0 * 8.820588111877441
Epoch 440, val loss: 0.9714846611022949
Epoch 450, training loss: 442.2569274902344 = 0.9619834423065186 + 50.0 * 8.825899124145508
Epoch 450, val loss: 0.9655725359916687
Epoch 460, training loss: 442.4427795410156 = 0.9555142521858215 + 50.0 * 8.829745292663574
Epoch 460, val loss: 0.9593809843063354
Epoch 470, training loss: 442.4643249511719 = 0.9489055871963501 + 50.0 * 8.830307960510254
Epoch 470, val loss: 0.9530804753303528
Epoch 480, training loss: 442.5603332519531 = 0.9419777989387512 + 50.0 * 8.832366943359375
Epoch 480, val loss: 0.9464282989501953
Epoch 490, training loss: 442.9002990722656 = 0.9349044561386108 + 50.0 * 8.83930778503418
Epoch 490, val loss: 0.9397209882736206
Epoch 500, training loss: 443.1470947265625 = 0.9276176691055298 + 50.0 * 8.844389915466309
Epoch 500, val loss: 0.9328374862670898
Epoch 510, training loss: 443.0416564941406 = 0.9201123714447021 + 50.0 * 8.84243106842041
Epoch 510, val loss: 0.925749659538269
Epoch 520, training loss: 444.1121520996094 = 0.9123983383178711 + 50.0 * 8.863995552062988
Epoch 520, val loss: 0.9181927442550659
Epoch 530, training loss: 443.64666748046875 = 0.9039314389228821 + 50.0 * 8.854854583740234
Epoch 530, val loss: 0.9104436039924622
Epoch 540, training loss: 444.3142395019531 = 0.8966026902198792 + 50.0 * 8.868352890014648
Epoch 540, val loss: 0.9033198952674866
Epoch 550, training loss: 442.7257080078125 = 0.888014018535614 + 50.0 * 8.836753845214844
Epoch 550, val loss: 0.8954336047172546
Epoch 560, training loss: 443.4204406738281 = 0.8802716135978699 + 50.0 * 8.85080337524414
Epoch 560, val loss: 0.8881717920303345
Epoch 570, training loss: 443.7000427246094 = 0.8721539378166199 + 50.0 * 8.856557846069336
Epoch 570, val loss: 0.8806248307228088
Epoch 580, training loss: 443.6659851074219 = 0.8637433052062988 + 50.0 * 8.85604476928711
Epoch 580, val loss: 0.8727936148643494
Epoch 590, training loss: 444.03839111328125 = 0.8555124402046204 + 50.0 * 8.86365795135498
Epoch 590, val loss: 0.8650646805763245
Epoch 600, training loss: 444.2635498046875 = 0.8470946550369263 + 50.0 * 8.868329048156738
Epoch 600, val loss: 0.8572934865951538
Epoch 610, training loss: 444.4546813964844 = 0.8386925458908081 + 50.0 * 8.872320175170898
Epoch 610, val loss: 0.849486231803894
Epoch 620, training loss: 444.7535400390625 = 0.8303042054176331 + 50.0 * 8.878464698791504
Epoch 620, val loss: 0.8417614102363586
Epoch 630, training loss: 444.6674499511719 = 0.8216863870620728 + 50.0 * 8.876914978027344
Epoch 630, val loss: 0.8337429165840149
Epoch 640, training loss: 445.01416015625 = 0.8133648037910461 + 50.0 * 8.884016036987305
Epoch 640, val loss: 0.8262072801589966
Epoch 650, training loss: 445.1535339355469 = 0.8049677014350891 + 50.0 * 8.886971473693848
Epoch 650, val loss: 0.8183450102806091
Epoch 660, training loss: 445.20623779296875 = 0.7964723706245422 + 50.0 * 8.888195037841797
Epoch 660, val loss: 0.8107598423957825
Epoch 670, training loss: 445.15625 = 0.7881237268447876 + 50.0 * 8.887362480163574
Epoch 670, val loss: 0.8031268119812012
Epoch 680, training loss: 445.17669677734375 = 0.779632568359375 + 50.0 * 8.887941360473633
Epoch 680, val loss: 0.7953653931617737
Epoch 690, training loss: 445.5519714355469 = 0.771399199962616 + 50.0 * 8.895611763000488
Epoch 690, val loss: 0.7881346344947815
Epoch 700, training loss: 445.8060302734375 = 0.7632318735122681 + 50.0 * 8.900856018066406
Epoch 700, val loss: 0.7806996703147888
Epoch 710, training loss: 445.90234375 = 0.7548679113388062 + 50.0 * 8.902949333190918
Epoch 710, val loss: 0.7731797695159912
Epoch 720, training loss: 445.55828857421875 = 0.7463723421096802 + 50.0 * 8.896238327026367
Epoch 720, val loss: 0.7657824754714966
Epoch 730, training loss: 445.54815673828125 = 0.7376839518547058 + 50.0 * 8.896209716796875
Epoch 730, val loss: 0.7575927376747131
Epoch 740, training loss: 445.7543640136719 = 0.7290041446685791 + 50.0 * 8.900506973266602
Epoch 740, val loss: 0.7499061226844788
Epoch 750, training loss: 446.186279296875 = 0.7207493782043457 + 50.0 * 8.909310340881348
Epoch 750, val loss: 0.7424930930137634
Epoch 760, training loss: 446.26580810546875 = 0.7123907804489136 + 50.0 * 8.911067962646484
Epoch 760, val loss: 0.7349286079406738
Epoch 770, training loss: 446.37506103515625 = 0.7043439745903015 + 50.0 * 8.913414001464844
Epoch 770, val loss: 0.7278110384941101
Epoch 780, training loss: 446.33953857421875 = 0.696593165397644 + 50.0 * 8.912858963012695
Epoch 780, val loss: 0.7208817601203918
Epoch 790, training loss: 446.7868957519531 = 0.6891416907310486 + 50.0 * 8.921955108642578
Epoch 790, val loss: 0.7142238020896912
Epoch 800, training loss: 446.5334167480469 = 0.6816580891609192 + 50.0 * 8.917035102844238
Epoch 800, val loss: 0.7076618671417236
Epoch 810, training loss: 446.957275390625 = 0.6747964024543762 + 50.0 * 8.925649642944336
Epoch 810, val loss: 0.7015178799629211
Epoch 820, training loss: 447.3890075683594 = 0.6681166887283325 + 50.0 * 8.934417724609375
Epoch 820, val loss: 0.6956474781036377
Epoch 830, training loss: 447.5754089355469 = 0.6615733504295349 + 50.0 * 8.938277244567871
Epoch 830, val loss: 0.6899717450141907
Epoch 840, training loss: 447.5700988769531 = 0.6552383303642273 + 50.0 * 8.938297271728516
Epoch 840, val loss: 0.6843767762184143
Epoch 850, training loss: 447.81280517578125 = 0.6491839289665222 + 50.0 * 8.943272590637207
Epoch 850, val loss: 0.6791849136352539
Epoch 860, training loss: 447.9043884277344 = 0.6433874368667603 + 50.0 * 8.945219993591309
Epoch 860, val loss: 0.6740936040878296
Epoch 870, training loss: 447.8728942871094 = 0.6377798318862915 + 50.0 * 8.9447021484375
Epoch 870, val loss: 0.6693081259727478
Epoch 880, training loss: 448.1455383300781 = 0.6325520873069763 + 50.0 * 8.950260162353516
Epoch 880, val loss: 0.6648435592651367
Epoch 890, training loss: 448.281982421875 = 0.6275030970573425 + 50.0 * 8.953089714050293
Epoch 890, val loss: 0.6604363918304443
Epoch 900, training loss: 448.3713073730469 = 0.622672975063324 + 50.0 * 8.954972267150879
Epoch 900, val loss: 0.6564282178878784
Epoch 910, training loss: 448.41851806640625 = 0.6180858016014099 + 50.0 * 8.956008911132812
Epoch 910, val loss: 0.652396023273468
Epoch 920, training loss: 448.41595458984375 = 0.6137363910675049 + 50.0 * 8.95604419708252
Epoch 920, val loss: 0.6486688852310181
Epoch 930, training loss: 448.84820556640625 = 0.6096926331520081 + 50.0 * 8.964770317077637
Epoch 930, val loss: 0.6453802585601807
Epoch 940, training loss: 448.6864013671875 = 0.6057240962982178 + 50.0 * 8.961613655090332
Epoch 940, val loss: 0.6419603824615479
Epoch 950, training loss: 448.6051940917969 = 0.6018218398094177 + 50.0 * 8.960067749023438
Epoch 950, val loss: 0.6389091610908508
Epoch 960, training loss: 448.6094665527344 = 0.5982765555381775 + 50.0 * 8.960224151611328
Epoch 960, val loss: 0.6358504295349121
Epoch 970, training loss: 448.67620849609375 = 0.5944024920463562 + 50.0 * 8.961636543273926
Epoch 970, val loss: 0.6323286890983582
Epoch 980, training loss: 445.5264587402344 = 0.5910470485687256 + 50.0 * 8.89870834350586
Epoch 980, val loss: 0.6301720142364502
Epoch 990, training loss: 448.7445983886719 = 0.5896652340888977 + 50.0 * 8.963098526000977
Epoch 990, val loss: 0.6291202306747437
Epoch 1000, training loss: 446.274169921875 = 0.5858666896820068 + 50.0 * 8.913765907287598
Epoch 1000, val loss: 0.6257618069648743
Epoch 1010, training loss: 447.2232360839844 = 0.5833115577697754 + 50.0 * 8.932798385620117
Epoch 1010, val loss: 0.6238857507705688
Epoch 1020, training loss: 447.8459167480469 = 0.5809759497642517 + 50.0 * 8.94529914855957
Epoch 1020, val loss: 0.6220924854278564
Epoch 1030, training loss: 448.13427734375 = 0.5784661769866943 + 50.0 * 8.951116561889648
Epoch 1030, val loss: 0.6203441023826599
Epoch 1040, training loss: 448.6167297363281 = 0.576111376285553 + 50.0 * 8.96081256866455
Epoch 1040, val loss: 0.6185076832771301
Epoch 1050, training loss: 448.65234375 = 0.5738058686256409 + 50.0 * 8.961570739746094
Epoch 1050, val loss: 0.6167868375778198
Epoch 1060, training loss: 448.89910888671875 = 0.5716356039047241 + 50.0 * 8.96654987335205
Epoch 1060, val loss: 0.6151207089424133
Epoch 1070, training loss: 449.3175048828125 = 0.5695682764053345 + 50.0 * 8.974958419799805
Epoch 1070, val loss: 0.6136624813079834
Epoch 1080, training loss: 449.4406433105469 = 0.5676019787788391 + 50.0 * 8.977460861206055
Epoch 1080, val loss: 0.6121969223022461
Epoch 1090, training loss: 449.4649963378906 = 0.5656372308731079 + 50.0 * 8.977987289428711
Epoch 1090, val loss: 0.6107947826385498
Epoch 1100, training loss: 449.6384582519531 = 0.5638247132301331 + 50.0 * 8.98149299621582
Epoch 1100, val loss: 0.6095060706138611
Epoch 1110, training loss: 449.929443359375 = 0.5620637536048889 + 50.0 * 8.987347602844238
Epoch 1110, val loss: 0.6082455515861511
Epoch 1120, training loss: 449.74798583984375 = 0.5602596998214722 + 50.0 * 8.98375415802002
Epoch 1120, val loss: 0.6070147752761841
Epoch 1130, training loss: 449.50640869140625 = 0.5585101246833801 + 50.0 * 8.978958129882812
Epoch 1130, val loss: 0.6056528687477112
Epoch 1140, training loss: 449.8624267578125 = 0.5570650696754456 + 50.0 * 8.986106872558594
Epoch 1140, val loss: 0.604893684387207
Epoch 1150, training loss: 450.08245849609375 = 0.5555799603462219 + 50.0 * 8.990537643432617
Epoch 1150, val loss: 0.6038274168968201
Epoch 1160, training loss: 450.4869079589844 = 0.5542292594909668 + 50.0 * 8.998653411865234
Epoch 1160, val loss: 0.6031209230422974
Epoch 1170, training loss: 450.4369201660156 = 0.552699089050293 + 50.0 * 8.997684478759766
Epoch 1170, val loss: 0.6020001173019409
Epoch 1180, training loss: 450.6578063964844 = 0.5513455867767334 + 50.0 * 9.002129554748535
Epoch 1180, val loss: 0.6012988686561584
Epoch 1190, training loss: 450.9340515136719 = 0.5500351786613464 + 50.0 * 9.00767993927002
Epoch 1190, val loss: 0.6003938317298889
Epoch 1200, training loss: 450.5962829589844 = 0.5485683679580688 + 50.0 * 9.000954627990723
Epoch 1200, val loss: 0.599369466304779
Epoch 1210, training loss: 450.7396240234375 = 0.5472016930580139 + 50.0 * 9.0038480758667
Epoch 1210, val loss: 0.5986944437026978
Epoch 1220, training loss: 451.0125427246094 = 0.5460038781166077 + 50.0 * 9.009330749511719
Epoch 1220, val loss: 0.5978952646255493
Epoch 1230, training loss: 451.25604248046875 = 0.5448033809661865 + 50.0 * 9.014225006103516
Epoch 1230, val loss: 0.5972793102264404
Epoch 1240, training loss: 451.63311767578125 = 0.543648362159729 + 50.0 * 9.02178955078125
Epoch 1240, val loss: 0.5965702533721924
Epoch 1250, training loss: 451.3683776855469 = 0.5424526333808899 + 50.0 * 9.016518592834473
Epoch 1250, val loss: 0.5959246754646301
Epoch 1260, training loss: 451.76800537109375 = 0.5413597822189331 + 50.0 * 9.02453327178955
Epoch 1260, val loss: 0.5952887535095215
Epoch 1270, training loss: 451.7124328613281 = 0.5402134656906128 + 50.0 * 9.023444175720215
Epoch 1270, val loss: 0.5946872234344482
Epoch 1280, training loss: 451.6216735839844 = 0.5390511155128479 + 50.0 * 9.021652221679688
Epoch 1280, val loss: 0.5941620469093323
Epoch 1290, training loss: 451.7151794433594 = 0.5378941893577576 + 50.0 * 9.023545265197754
Epoch 1290, val loss: 0.5934682488441467
Epoch 1300, training loss: 451.9808654785156 = 0.5369013547897339 + 50.0 * 9.028879165649414
Epoch 1300, val loss: 0.5927577018737793
Epoch 1310, training loss: 452.0986633300781 = 0.5358787178993225 + 50.0 * 9.031255722045898
Epoch 1310, val loss: 0.5923346877098083
Epoch 1320, training loss: 452.1979064941406 = 0.5348515510559082 + 50.0 * 9.0332612991333
Epoch 1320, val loss: 0.5919459462165833
Epoch 1330, training loss: 452.6162109375 = 0.5338991284370422 + 50.0 * 9.041646003723145
Epoch 1330, val loss: 0.5913479924201965
Epoch 1340, training loss: 452.5633239746094 = 0.5328809022903442 + 50.0 * 9.040609359741211
Epoch 1340, val loss: 0.5908329486846924
Epoch 1350, training loss: 452.1312561035156 = 0.531806230545044 + 50.0 * 9.031989097595215
Epoch 1350, val loss: 0.5901508927345276
Epoch 1360, training loss: 451.6484069824219 = 0.5307777523994446 + 50.0 * 9.02235221862793
Epoch 1360, val loss: 0.5894172191619873
Epoch 1370, training loss: 451.97991943359375 = 0.5299856066703796 + 50.0 * 9.028998374938965
Epoch 1370, val loss: 0.5892302989959717
Epoch 1380, training loss: 452.2170715332031 = 0.5290189981460571 + 50.0 * 9.033761024475098
Epoch 1380, val loss: 0.5887239575386047
Epoch 1390, training loss: 452.6064147949219 = 0.528081476688385 + 50.0 * 9.041566848754883
Epoch 1390, val loss: 0.5883554816246033
Epoch 1400, training loss: 453.06585693359375 = 0.527155339717865 + 50.0 * 9.050773620605469
Epoch 1400, val loss: 0.5878715515136719
Epoch 1410, training loss: 452.9966735839844 = 0.5261709690093994 + 50.0 * 9.049409866333008
Epoch 1410, val loss: 0.5872848033905029
Epoch 1420, training loss: 453.0081481933594 = 0.5252168774604797 + 50.0 * 9.04965877532959
Epoch 1420, val loss: 0.5869396328926086
Epoch 1430, training loss: 453.3705749511719 = 0.5243275761604309 + 50.0 * 9.056924819946289
Epoch 1430, val loss: 0.5865482091903687
Epoch 1440, training loss: 453.3779602050781 = 0.5234679579734802 + 50.0 * 9.057089805603027
Epoch 1440, val loss: 0.5862675905227661
Epoch 1450, training loss: 453.4691467285156 = 0.522674024105072 + 50.0 * 9.058929443359375
Epoch 1450, val loss: 0.5856623649597168
Epoch 1460, training loss: 453.6672058105469 = 0.52174973487854 + 50.0 * 9.062909126281738
Epoch 1460, val loss: 0.5850462913513184
Epoch 1470, training loss: 453.5614318847656 = 0.5207716226577759 + 50.0 * 9.060812950134277
Epoch 1470, val loss: 0.5846752524375916
Epoch 1480, training loss: 453.726806640625 = 0.5197802186012268 + 50.0 * 9.064140319824219
Epoch 1480, val loss: 0.5842233300209045
Epoch 1490, training loss: 453.9226989746094 = 0.5188248157501221 + 50.0 * 9.068077087402344
Epoch 1490, val loss: 0.5838196277618408
Epoch 1500, training loss: 453.86199951171875 = 0.5178915858268738 + 50.0 * 9.066882133483887
Epoch 1500, val loss: 0.5835654139518738
Epoch 1510, training loss: 454.0626525878906 = 0.5170089602470398 + 50.0 * 9.070913314819336
Epoch 1510, val loss: 0.5830699801445007
Epoch 1520, training loss: 454.25445556640625 = 0.5161035656929016 + 50.0 * 9.074767112731934
Epoch 1520, val loss: 0.5826115608215332
Epoch 1530, training loss: 453.86688232421875 = 0.5151336193084717 + 50.0 * 9.067034721374512
Epoch 1530, val loss: 0.5819454193115234
Epoch 1540, training loss: 453.9186706542969 = 0.5141851902008057 + 50.0 * 9.068089485168457
Epoch 1540, val loss: 0.581728994846344
Epoch 1550, training loss: 454.2946472167969 = 0.5132784247398376 + 50.0 * 9.075627326965332
Epoch 1550, val loss: 0.5813103318214417
Epoch 1560, training loss: 454.36004638671875 = 0.5123919248580933 + 50.0 * 9.076952934265137
Epoch 1560, val loss: 0.5808900594711304
Epoch 1570, training loss: 454.22711181640625 = 0.5114951133728027 + 50.0 * 9.074312210083008
Epoch 1570, val loss: 0.5805849432945251
Epoch 1580, training loss: 454.4415283203125 = 0.5105821490287781 + 50.0 * 9.078619003295898
Epoch 1580, val loss: 0.5802293419837952
Epoch 1590, training loss: 454.8820495605469 = 0.5097087025642395 + 50.0 * 9.087447166442871
Epoch 1590, val loss: 0.5799123048782349
Epoch 1600, training loss: 454.7301330566406 = 0.5087241530418396 + 50.0 * 9.084427833557129
Epoch 1600, val loss: 0.5794945359230042
Epoch 1610, training loss: 454.91485595703125 = 0.5077776312828064 + 50.0 * 9.088141441345215
Epoch 1610, val loss: 0.5790673494338989
Epoch 1620, training loss: 454.9757080078125 = 0.5068432688713074 + 50.0 * 9.089377403259277
Epoch 1620, val loss: 0.5786687731742859
Epoch 1630, training loss: 454.9031066894531 = 0.5058996081352234 + 50.0 * 9.087944030761719
Epoch 1630, val loss: 0.5783858299255371
Epoch 1640, training loss: 454.2196350097656 = 0.5049346089363098 + 50.0 * 9.074294090270996
Epoch 1640, val loss: 0.5778090953826904
Epoch 1650, training loss: 453.3346252441406 = 0.5039446949958801 + 50.0 * 9.05661392211914
Epoch 1650, val loss: 0.5767064690589905
Epoch 1660, training loss: 454.1788024902344 = 0.5030588507652283 + 50.0 * 9.073514938354492
Epoch 1660, val loss: 0.5770923495292664
Epoch 1670, training loss: 454.6354675292969 = 0.5022248029708862 + 50.0 * 9.082664489746094
Epoch 1670, val loss: 0.5766852498054504
Epoch 1680, training loss: 454.9150085449219 = 0.501334547996521 + 50.0 * 9.088273048400879
Epoch 1680, val loss: 0.5762606859207153
Epoch 1690, training loss: 455.2610778808594 = 0.5004309415817261 + 50.0 * 9.095212936401367
Epoch 1690, val loss: 0.5759614109992981
Epoch 1700, training loss: 455.0937194824219 = 0.4994777739048004 + 50.0 * 9.09188461303711
Epoch 1700, val loss: 0.5755295753479004
Epoch 1710, training loss: 455.42034912109375 = 0.49856916069984436 + 50.0 * 9.098435401916504
Epoch 1710, val loss: 0.5752249360084534
Epoch 1720, training loss: 455.8395080566406 = 0.49765801429748535 + 50.0 * 9.106837272644043
Epoch 1720, val loss: 0.5748260617256165
Epoch 1730, training loss: 455.71112060546875 = 0.4966959059238434 + 50.0 * 9.104288101196289
Epoch 1730, val loss: 0.5744274854660034
Epoch 1740, training loss: 455.7117004394531 = 0.49574506282806396 + 50.0 * 9.104318618774414
Epoch 1740, val loss: 0.5741761326789856
Epoch 1750, training loss: 455.7040100097656 = 0.4948170781135559 + 50.0 * 9.1041841506958
Epoch 1750, val loss: 0.5737783312797546
Epoch 1760, training loss: 455.90679931640625 = 0.4938858449459076 + 50.0 * 9.108258247375488
Epoch 1760, val loss: 0.5734475255012512
Epoch 1770, training loss: 456.2466735839844 = 0.49294576048851013 + 50.0 * 9.115074157714844
Epoch 1770, val loss: 0.573049783706665
Epoch 1780, training loss: 455.9651794433594 = 0.49193882942199707 + 50.0 * 9.109464645385742
Epoch 1780, val loss: 0.5726977586746216
Epoch 1790, training loss: 456.2113037109375 = 0.4909762740135193 + 50.0 * 9.11440658569336
Epoch 1790, val loss: 0.5723298788070679
Epoch 1800, training loss: 455.7920227050781 = 0.4899313151836395 + 50.0 * 9.10604190826416
Epoch 1800, val loss: 0.5719395279884338
Epoch 1810, training loss: 455.2923583984375 = 0.489075243473053 + 50.0 * 9.096065521240234
Epoch 1810, val loss: 0.5715292096138
Epoch 1820, training loss: 455.6582946777344 = 0.48819854855537415 + 50.0 * 9.103402137756348
Epoch 1820, val loss: 0.5712069272994995
Epoch 1830, training loss: 456.2552490234375 = 0.48725345730781555 + 50.0 * 9.115360260009766
Epoch 1830, val loss: 0.5709633231163025
Epoch 1840, training loss: 456.5207824707031 = 0.4862730801105499 + 50.0 * 9.12069034576416
Epoch 1840, val loss: 0.5706038475036621
Epoch 1850, training loss: 456.7159729003906 = 0.48529869318008423 + 50.0 * 9.124613761901855
Epoch 1850, val loss: 0.5703404545783997
Epoch 1860, training loss: 456.61041259765625 = 0.48426553606987 + 50.0 * 9.122523307800293
Epoch 1860, val loss: 0.5698649287223816
Epoch 1870, training loss: 456.6083068847656 = 0.4832768142223358 + 50.0 * 9.1225004196167
Epoch 1870, val loss: 0.5694445967674255
Epoch 1880, training loss: 456.93414306640625 = 0.4823150336742401 + 50.0 * 9.129036903381348
Epoch 1880, val loss: 0.5692293643951416
Epoch 1890, training loss: 457.0475158691406 = 0.4813179671764374 + 50.0 * 9.13132381439209
Epoch 1890, val loss: 0.5687289237976074
Epoch 1900, training loss: 456.9233703613281 = 0.48027780652046204 + 50.0 * 9.128861427307129
Epoch 1900, val loss: 0.5683741569519043
Epoch 1910, training loss: 456.9940490722656 = 0.4792519509792328 + 50.0 * 9.130295753479004
Epoch 1910, val loss: 0.5678917765617371
Epoch 1920, training loss: 457.0125427246094 = 0.47823622822761536 + 50.0 * 9.130685806274414
Epoch 1920, val loss: 0.5675166249275208
Epoch 1930, training loss: 457.0121765136719 = 0.4772176742553711 + 50.0 * 9.130699157714844
Epoch 1930, val loss: 0.5671176910400391
Epoch 1940, training loss: 457.2852783203125 = 0.47622692584991455 + 50.0 * 9.136180877685547
Epoch 1940, val loss: 0.5667648911476135
Epoch 1950, training loss: 456.6358642578125 = 0.4752819836139679 + 50.0 * 9.123211860656738
Epoch 1950, val loss: 0.5669746398925781
Epoch 1960, training loss: 456.7314758300781 = 0.4750264286994934 + 50.0 * 9.125128746032715
Epoch 1960, val loss: 0.5661089420318604
Epoch 1970, training loss: 455.9679870605469 = 0.47404640913009644 + 50.0 * 9.109878540039062
Epoch 1970, val loss: 0.5653348565101624
Epoch 1980, training loss: 456.0531005859375 = 0.47295501828193665 + 50.0 * 9.111602783203125
Epoch 1980, val loss: 0.5654881596565247
Epoch 1990, training loss: 456.6864013671875 = 0.471870094537735 + 50.0 * 9.124290466308594
Epoch 1990, val loss: 0.5646001100540161
Epoch 2000, training loss: 457.2497863769531 = 0.47078636288642883 + 50.0 * 9.135580062866211
Epoch 2000, val loss: 0.5646814107894897
Epoch 2010, training loss: 457.4469909667969 = 0.46965572237968445 + 50.0 * 9.139546394348145
Epoch 2010, val loss: 0.5641505122184753
Epoch 2020, training loss: 457.37530517578125 = 0.4685162305831909 + 50.0 * 9.13813591003418
Epoch 2020, val loss: 0.5638861656188965
Epoch 2030, training loss: 457.5751953125 = 0.46740925312042236 + 50.0 * 9.142155647277832
Epoch 2030, val loss: 0.5633278489112854
Epoch 2040, training loss: 457.5291442871094 = 0.4662795066833496 + 50.0 * 9.141257286071777
Epoch 2040, val loss: 0.5630748867988586
Epoch 2050, training loss: 457.73779296875 = 0.46517306566238403 + 50.0 * 9.145452499389648
Epoch 2050, val loss: 0.5625799298286438
Epoch 2060, training loss: 457.92657470703125 = 0.4640706479549408 + 50.0 * 9.149250030517578
Epoch 2060, val loss: 0.5621154308319092
Epoch 2070, training loss: 457.89056396484375 = 0.4629456698894501 + 50.0 * 9.148551940917969
Epoch 2070, val loss: 0.5617415904998779
Epoch 2080, training loss: 457.90972900390625 = 0.4618270993232727 + 50.0 * 9.148958206176758
Epoch 2080, val loss: 0.5612476468086243
Epoch 2090, training loss: 458.0224914550781 = 0.4607204496860504 + 50.0 * 9.151235580444336
Epoch 2090, val loss: 0.5609169006347656
Epoch 2100, training loss: 458.04718017578125 = 0.45959317684173584 + 50.0 * 9.151751518249512
Epoch 2100, val loss: 0.5603952407836914
Epoch 2110, training loss: 457.93707275390625 = 0.45848017930984497 + 50.0 * 9.149572372436523
Epoch 2110, val loss: 0.5601082444190979
Epoch 2120, training loss: 457.4851379394531 = 0.4573172926902771 + 50.0 * 9.140556335449219
Epoch 2120, val loss: 0.5595207810401917
Epoch 2130, training loss: 457.8905944824219 = 0.4561900496482849 + 50.0 * 9.148688316345215
Epoch 2130, val loss: 0.5591498613357544
Epoch 2140, training loss: 458.3143310546875 = 0.4550522565841675 + 50.0 * 9.157185554504395
Epoch 2140, val loss: 0.5586755871772766
Epoch 2150, training loss: 458.2436828613281 = 0.45389094948768616 + 50.0 * 9.15579605102539
Epoch 2150, val loss: 0.5582545399665833
Epoch 2160, training loss: 458.295654296875 = 0.45271843671798706 + 50.0 * 9.156858444213867
Epoch 2160, val loss: 0.5579192638397217
Epoch 2170, training loss: 458.4432678222656 = 0.45155009627342224 + 50.0 * 9.159834861755371
Epoch 2170, val loss: 0.5574513077735901
Epoch 2180, training loss: 458.46160888671875 = 0.45040515065193176 + 50.0 * 9.160223960876465
Epoch 2180, val loss: 0.5571231245994568
Epoch 2190, training loss: 458.54754638671875 = 0.449227511882782 + 50.0 * 9.161966323852539
Epoch 2190, val loss: 0.5567103624343872
Epoch 2200, training loss: 458.64239501953125 = 0.44800853729248047 + 50.0 * 9.163887977600098
Epoch 2200, val loss: 0.5562330484390259
Epoch 2210, training loss: 458.635498046875 = 0.44678211212158203 + 50.0 * 9.163774490356445
Epoch 2210, val loss: 0.555766761302948
Epoch 2220, training loss: 458.8722229003906 = 0.4455527365207672 + 50.0 * 9.168533325195312
Epoch 2220, val loss: 0.5551739931106567
Epoch 2230, training loss: 458.85107421875 = 0.44430971145629883 + 50.0 * 9.168135643005371
Epoch 2230, val loss: 0.554861843585968
Epoch 2240, training loss: 458.8169250488281 = 0.44306838512420654 + 50.0 * 9.16747760772705
Epoch 2240, val loss: 0.5544238686561584
Epoch 2250, training loss: 458.813720703125 = 0.4417984187602997 + 50.0 * 9.167438507080078
Epoch 2250, val loss: 0.5540206432342529
Epoch 2260, training loss: 458.67779541015625 = 0.4405142068862915 + 50.0 * 9.164745330810547
Epoch 2260, val loss: 0.5534681081771851
Epoch 2270, training loss: 458.93634033203125 = 0.43924951553344727 + 50.0 * 9.169941902160645
Epoch 2270, val loss: 0.5530557632446289
Epoch 2280, training loss: 459.1767578125 = 0.437968373298645 + 50.0 * 9.174776077270508
Epoch 2280, val loss: 0.5525460839271545
Epoch 2290, training loss: 459.28704833984375 = 0.436681866645813 + 50.0 * 9.177007675170898
Epoch 2290, val loss: 0.5519911646842957
Epoch 2300, training loss: 459.1373291015625 = 0.43538254499435425 + 50.0 * 9.174038887023926
Epoch 2300, val loss: 0.551550567150116
Epoch 2310, training loss: 458.9749450683594 = 0.43406838178634644 + 50.0 * 9.170817375183105
Epoch 2310, val loss: 0.5509491562843323
Epoch 2320, training loss: 459.13348388671875 = 0.4327661991119385 + 50.0 * 9.1740140914917
Epoch 2320, val loss: 0.5506989359855652
Epoch 2330, training loss: 459.2336730957031 = 0.4314608573913574 + 50.0 * 9.176044464111328
Epoch 2330, val loss: 0.5501469969749451
Epoch 2340, training loss: 459.3822021484375 = 0.4301522672176361 + 50.0 * 9.179040908813477
Epoch 2340, val loss: 0.5497692227363586
Epoch 2350, training loss: 459.4050598144531 = 0.42881324887275696 + 50.0 * 9.179525375366211
Epoch 2350, val loss: 0.549334704875946
Epoch 2360, training loss: 458.8476867675781 = 0.4274531602859497 + 50.0 * 9.168404579162598
Epoch 2360, val loss: 0.5488321185112
Epoch 2370, training loss: 458.3066101074219 = 0.42625537514686584 + 50.0 * 9.157607078552246
Epoch 2370, val loss: 0.5484607219696045
Epoch 2380, training loss: 458.2004089355469 = 0.4251472055912018 + 50.0 * 9.155505180358887
Epoch 2380, val loss: 0.5485284924507141
Epoch 2390, training loss: 458.619140625 = 0.4236982762813568 + 50.0 * 9.163908958435059
Epoch 2390, val loss: 0.5477437973022461
Epoch 2400, training loss: 458.3979187011719 = 0.4223385155200958 + 50.0 * 9.15951156616211
Epoch 2400, val loss: 0.5475903749465942
Epoch 2410, training loss: 458.7689514160156 = 0.420966774225235 + 50.0 * 9.166959762573242
Epoch 2410, val loss: 0.5462308526039124
Epoch 2420, training loss: 459.28619384765625 = 0.41959840059280396 + 50.0 * 9.177331924438477
Epoch 2420, val loss: 0.5458438396453857
Epoch 2430, training loss: 459.5648193359375 = 0.4182211756706238 + 50.0 * 9.182931900024414
Epoch 2430, val loss: 0.5452564358711243
Epoch 2440, training loss: 459.5746765136719 = 0.4168281555175781 + 50.0 * 9.183156967163086
Epoch 2440, val loss: 0.5447484850883484
Epoch 2450, training loss: 459.7457580566406 = 0.41543570160865784 + 50.0 * 9.186606407165527
Epoch 2450, val loss: 0.544266939163208
Epoch 2460, training loss: 459.823974609375 = 0.4140288829803467 + 50.0 * 9.188199043273926
Epoch 2460, val loss: 0.5436885356903076
Epoch 2470, training loss: 459.8794250488281 = 0.41261211037635803 + 50.0 * 9.189336776733398
Epoch 2470, val loss: 0.543141782283783
Epoch 2480, training loss: 459.8276062011719 = 0.41117537021636963 + 50.0 * 9.188328742980957
Epoch 2480, val loss: 0.5426852107048035
Epoch 2490, training loss: 459.8851318359375 = 0.4097459614276886 + 50.0 * 9.189507484436035
Epoch 2490, val loss: 0.5422077178955078
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7797101449275362
0.8167789610954141
=== training gcn model ===
Epoch 0, training loss: 511.5279846191406 = 1.0916353464126587 + 50.0 * 10.20872688293457
Epoch 0, val loss: 1.0933283567428589
Epoch 10, training loss: 492.8035583496094 = 1.0883055925369263 + 50.0 * 9.834304809570312
Epoch 10, val loss: 1.0900715589523315
Epoch 20, training loss: 483.637451171875 = 1.0852710008621216 + 50.0 * 9.651043891906738
Epoch 20, val loss: 1.0870392322540283
Epoch 30, training loss: 476.7119445800781 = 1.0823251008987427 + 50.0 * 9.512592315673828
Epoch 30, val loss: 1.0841220617294312
Epoch 40, training loss: 471.400634765625 = 1.0795707702636719 + 50.0 * 9.406421661376953
Epoch 40, val loss: 1.0813812017440796
Epoch 50, training loss: 467.1884765625 = 1.0769765377044678 + 50.0 * 9.322230339050293
Epoch 50, val loss: 1.078790307044983
Epoch 60, training loss: 463.6888122558594 = 1.074520468711853 + 50.0 * 9.252285957336426
Epoch 60, val loss: 1.076330542564392
Epoch 70, training loss: 460.70196533203125 = 1.0721932649612427 + 50.0 * 9.192595481872559
Epoch 70, val loss: 1.0739887952804565
Epoch 80, training loss: 458.1500549316406 = 1.0699670314788818 + 50.0 * 9.1416015625
Epoch 80, val loss: 1.0717402696609497
Epoch 90, training loss: 455.8664855957031 = 1.0678569078445435 + 50.0 * 9.095972061157227
Epoch 90, val loss: 1.069593906402588
Epoch 100, training loss: 453.8575744628906 = 1.0657994747161865 + 50.0 * 9.055835723876953
Epoch 100, val loss: 1.0675026178359985
Epoch 110, training loss: 452.192626953125 = 1.0638487339019775 + 50.0 * 9.022575378417969
Epoch 110, val loss: 1.0655086040496826
Epoch 120, training loss: 450.68792724609375 = 1.061956763267517 + 50.0 * 8.99251937866211
Epoch 120, val loss: 1.0635724067687988
Epoch 130, training loss: 449.4178771972656 = 1.0600944757461548 + 50.0 * 8.967155456542969
Epoch 130, val loss: 1.0616689920425415
Epoch 140, training loss: 448.3629455566406 = 1.0583369731903076 + 50.0 * 8.946091651916504
Epoch 140, val loss: 1.0598561763763428
Epoch 150, training loss: 447.3538818359375 = 1.0565913915634155 + 50.0 * 8.925946235656738
Epoch 150, val loss: 1.0580915212631226
Epoch 160, training loss: 446.45367431640625 = 1.0548595190048218 + 50.0 * 8.907976150512695
Epoch 160, val loss: 1.0563194751739502
Epoch 170, training loss: 445.7333679199219 = 1.0531576871871948 + 50.0 * 8.893604278564453
Epoch 170, val loss: 1.05458402633667
Epoch 180, training loss: 444.9519348144531 = 1.0515385866165161 + 50.0 * 8.878007888793945
Epoch 180, val loss: 1.052930235862732
Epoch 190, training loss: 444.81939697265625 = 1.049890160560608 + 50.0 * 8.87539005279541
Epoch 190, val loss: 1.0513019561767578
Epoch 200, training loss: 443.8179626464844 = 1.0481764078140259 + 50.0 * 8.855395317077637
Epoch 200, val loss: 1.0495514869689941
Epoch 210, training loss: 443.3807678222656 = 1.0465257167816162 + 50.0 * 8.846684455871582
Epoch 210, val loss: 1.0478854179382324
Epoch 220, training loss: 443.1268310546875 = 1.0448765754699707 + 50.0 * 8.841638565063477
Epoch 220, val loss: 1.046226978302002
Epoch 230, training loss: 442.7907409667969 = 1.0431318283081055 + 50.0 * 8.834952354431152
Epoch 230, val loss: 1.0444960594177246
Epoch 240, training loss: 442.48175048828125 = 1.041250467300415 + 50.0 * 8.82880973815918
Epoch 240, val loss: 1.042663335800171
Epoch 250, training loss: 442.2240295410156 = 1.0393743515014648 + 50.0 * 8.82369327545166
Epoch 250, val loss: 1.040777325630188
Epoch 260, training loss: 442.1468811035156 = 1.037461757659912 + 50.0 * 8.822188377380371
Epoch 260, val loss: 1.0389429330825806
Epoch 270, training loss: 441.94940185546875 = 1.0353914499282837 + 50.0 * 8.818280220031738
Epoch 270, val loss: 1.036927580833435
Epoch 280, training loss: 441.7306823730469 = 1.0331412553787231 + 50.0 * 8.813950538635254
Epoch 280, val loss: 1.0346845388412476
Epoch 290, training loss: 441.74420166015625 = 1.0308924913406372 + 50.0 * 8.814266204833984
Epoch 290, val loss: 1.0324982404708862
Epoch 300, training loss: 441.2842712402344 = 1.0280216932296753 + 50.0 * 8.80512523651123
Epoch 300, val loss: 1.0297012329101562
Epoch 310, training loss: 442.0285339355469 = 1.025965690612793 + 50.0 * 8.820051193237305
Epoch 310, val loss: 1.0276809930801392
Epoch 320, training loss: 441.1180114746094 = 1.0228174924850464 + 50.0 * 8.80190372467041
Epoch 320, val loss: 1.0246608257293701
Epoch 330, training loss: 441.3529357910156 = 1.0200788974761963 + 50.0 * 8.806656837463379
Epoch 330, val loss: 1.0220003128051758
Epoch 340, training loss: 441.19354248046875 = 1.0168914794921875 + 50.0 * 8.803533554077148
Epoch 340, val loss: 1.0189772844314575
Epoch 350, training loss: 441.1759948730469 = 1.013625144958496 + 50.0 * 8.803247451782227
Epoch 350, val loss: 1.0158203840255737
Epoch 360, training loss: 441.0814514160156 = 1.0099854469299316 + 50.0 * 8.801429748535156
Epoch 360, val loss: 1.0122826099395752
Epoch 370, training loss: 441.18707275390625 = 1.0062836408615112 + 50.0 * 8.80361557006836
Epoch 370, val loss: 1.0087440013885498
Epoch 380, training loss: 441.4649963378906 = 1.0024298429489136 + 50.0 * 8.809250831604004
Epoch 380, val loss: 1.0049951076507568
Epoch 390, training loss: 441.4196472167969 = 0.9981669187545776 + 50.0 * 8.808429718017578
Epoch 390, val loss: 1.000909447669983
Epoch 400, training loss: 441.2566833496094 = 0.9937337636947632 + 50.0 * 8.805258750915527
Epoch 400, val loss: 0.9966796040534973
Epoch 410, training loss: 441.398193359375 = 0.989202618598938 + 50.0 * 8.80817985534668
Epoch 410, val loss: 0.9923065304756165
Epoch 420, training loss: 441.4808349609375 = 0.9843563437461853 + 50.0 * 8.809929847717285
Epoch 420, val loss: 0.9876139760017395
Epoch 430, training loss: 441.4488830566406 = 0.9791964888572693 + 50.0 * 8.809393882751465
Epoch 430, val loss: 0.9826352596282959
Epoch 440, training loss: 441.63983154296875 = 0.9738660454750061 + 50.0 * 8.813319206237793
Epoch 440, val loss: 0.9774993062019348
Epoch 450, training loss: 441.64984130859375 = 0.9682827591896057 + 50.0 * 8.813631057739258
Epoch 450, val loss: 0.9721089601516724
Epoch 460, training loss: 441.6999206542969 = 0.9624884724617004 + 50.0 * 8.814748764038086
Epoch 460, val loss: 0.9665390849113464
Epoch 470, training loss: 441.8233642578125 = 0.9565771818161011 + 50.0 * 8.817336082458496
Epoch 470, val loss: 0.9608244895935059
Epoch 480, training loss: 441.7862243652344 = 0.95038241147995 + 50.0 * 8.816717147827148
Epoch 480, val loss: 0.9548371434211731
Epoch 490, training loss: 441.93353271484375 = 0.9439955353736877 + 50.0 * 8.819790840148926
Epoch 490, val loss: 0.9487804770469666
Epoch 500, training loss: 442.1995849609375 = 0.9375801086425781 + 50.0 * 8.825240135192871
Epoch 500, val loss: 0.9425655007362366
Epoch 510, training loss: 442.18157958984375 = 0.9309200644493103 + 50.0 * 8.825013160705566
Epoch 510, val loss: 0.9361563920974731
Epoch 520, training loss: 441.93109130859375 = 0.9238047003746033 + 50.0 * 8.820145606994629
Epoch 520, val loss: 0.9293766617774963
Epoch 530, training loss: 442.16168212890625 = 0.916752278804779 + 50.0 * 8.824898719787598
Epoch 530, val loss: 0.9226402640342712
Epoch 540, training loss: 442.6260070800781 = 0.90985506772995 + 50.0 * 8.834322929382324
Epoch 540, val loss: 0.9160171747207642
Epoch 550, training loss: 442.3968505859375 = 0.9022626876831055 + 50.0 * 8.8298921585083
Epoch 550, val loss: 0.9088051915168762
Epoch 560, training loss: 442.4592590332031 = 0.8950246572494507 + 50.0 * 8.831284523010254
Epoch 560, val loss: 0.9018393754959106
Epoch 570, training loss: 442.4718933105469 = 0.8874532580375671 + 50.0 * 8.83168888092041
Epoch 570, val loss: 0.894687294960022
Epoch 580, training loss: 442.5715637207031 = 0.8801174759864807 + 50.0 * 8.833828926086426
Epoch 580, val loss: 0.8876750469207764
Epoch 590, training loss: 442.77984619140625 = 0.8726657629013062 + 50.0 * 8.838143348693848
Epoch 590, val loss: 0.8805655837059021
Epoch 600, training loss: 442.9825134277344 = 0.8651473522186279 + 50.0 * 8.842347145080566
Epoch 600, val loss: 0.8733310103416443
Epoch 610, training loss: 442.93243408203125 = 0.8574774861335754 + 50.0 * 8.841499328613281
Epoch 610, val loss: 0.8662051558494568
Epoch 620, training loss: 442.8835144042969 = 0.8498448729515076 + 50.0 * 8.840673446655273
Epoch 620, val loss: 0.8589277863502502
Epoch 630, training loss: 443.0065002441406 = 0.8422452211380005 + 50.0 * 8.843284606933594
Epoch 630, val loss: 0.8518046140670776
Epoch 640, training loss: 443.07086181640625 = 0.8346335291862488 + 50.0 * 8.844724655151367
Epoch 640, val loss: 0.8446758389472961
Epoch 650, training loss: 443.2610778808594 = 0.8274181485176086 + 50.0 * 8.848672866821289
Epoch 650, val loss: 0.8379690647125244
Epoch 660, training loss: 443.5584716796875 = 0.8199916481971741 + 50.0 * 8.854769706726074
Epoch 660, val loss: 0.8309760689735413
Epoch 670, training loss: 443.60107421875 = 0.8124575614929199 + 50.0 * 8.855772018432617
Epoch 670, val loss: 0.8239661455154419
Epoch 680, training loss: 443.9618225097656 = 0.8051058650016785 + 50.0 * 8.863134384155273
Epoch 680, val loss: 0.8171190619468689
Epoch 690, training loss: 443.9311828613281 = 0.7977826595306396 + 50.0 * 8.86266803741455
Epoch 690, val loss: 0.8102278113365173
Epoch 700, training loss: 444.0262451171875 = 0.7903619408607483 + 50.0 * 8.864717483520508
Epoch 700, val loss: 0.8034018874168396
Epoch 710, training loss: 444.03912353515625 = 0.7833436131477356 + 50.0 * 8.86511516571045
Epoch 710, val loss: 0.7968851327896118
Epoch 720, training loss: 444.4065246582031 = 0.7764567136764526 + 50.0 * 8.872601509094238
Epoch 720, val loss: 0.790546715259552
Epoch 730, training loss: 444.4095764160156 = 0.7695455551147461 + 50.0 * 8.872800827026367
Epoch 730, val loss: 0.7841758728027344
Epoch 740, training loss: 444.3192443847656 = 0.7626148462295532 + 50.0 * 8.871132850646973
Epoch 740, val loss: 0.7778805494308472
Epoch 750, training loss: 444.46990966796875 = 0.7560045123100281 + 50.0 * 8.87427806854248
Epoch 750, val loss: 0.7717771530151367
Epoch 760, training loss: 444.5773620605469 = 0.7494856715202332 + 50.0 * 8.876557350158691
Epoch 760, val loss: 0.7658457159996033
Epoch 770, training loss: 444.7946472167969 = 0.743218183517456 + 50.0 * 8.881028175354004
Epoch 770, val loss: 0.7601128816604614
Epoch 780, training loss: 444.8526611328125 = 0.7370602488517761 + 50.0 * 8.882311820983887
Epoch 780, val loss: 0.7545502185821533
Epoch 790, training loss: 444.8263244628906 = 0.7309490442276001 + 50.0 * 8.88190746307373
Epoch 790, val loss: 0.7490964531898499
Epoch 800, training loss: 445.12396240234375 = 0.7251225113868713 + 50.0 * 8.88797664642334
Epoch 800, val loss: 0.743816614151001
Epoch 810, training loss: 445.1581726074219 = 0.7193235754966736 + 50.0 * 8.888776779174805
Epoch 810, val loss: 0.7386835217475891
Epoch 820, training loss: 445.16143798828125 = 0.7137457728385925 + 50.0 * 8.888954162597656
Epoch 820, val loss: 0.7336392402648926
Epoch 830, training loss: 445.1986999511719 = 0.7083151936531067 + 50.0 * 8.88980770111084
Epoch 830, val loss: 0.7289905548095703
Epoch 840, training loss: 445.997802734375 = 0.7031908631324768 + 50.0 * 8.905892372131348
Epoch 840, val loss: 0.7244508862495422
Epoch 850, training loss: 444.11468505859375 = 0.697067379951477 + 50.0 * 8.868351936340332
Epoch 850, val loss: 0.7190725207328796
Epoch 860, training loss: 445.3190002441406 = 0.6933847665786743 + 50.0 * 8.892512321472168
Epoch 860, val loss: 0.7160882949829102
Epoch 870, training loss: 445.2896423339844 = 0.6886380910873413 + 50.0 * 8.892020225524902
Epoch 870, val loss: 0.7120127081871033
Epoch 880, training loss: 444.9383239746094 = 0.683594286441803 + 50.0 * 8.88509464263916
Epoch 880, val loss: 0.7077987194061279
Epoch 890, training loss: 445.2353820800781 = 0.6791108846664429 + 50.0 * 8.891125679016113
Epoch 890, val loss: 0.7040745615959167
Epoch 900, training loss: 445.7161865234375 = 0.6747207045555115 + 50.0 * 8.900829315185547
Epoch 900, val loss: 0.700281023979187
Epoch 910, training loss: 445.732666015625 = 0.6703369617462158 + 50.0 * 8.901246070861816
Epoch 910, val loss: 0.6966065764427185
Epoch 920, training loss: 446.0763854980469 = 0.6661675572395325 + 50.0 * 8.908204078674316
Epoch 920, val loss: 0.6931961178779602
Epoch 930, training loss: 445.99847412109375 = 0.6620743870735168 + 50.0 * 8.90672779083252
Epoch 930, val loss: 0.6898120045661926
Epoch 940, training loss: 445.75164794921875 = 0.6579400897026062 + 50.0 * 8.901874542236328
Epoch 940, val loss: 0.6864833235740662
Epoch 950, training loss: 446.1064453125 = 0.6541522145271301 + 50.0 * 8.909046173095703
Epoch 950, val loss: 0.683459460735321
Epoch 960, training loss: 446.272216796875 = 0.6505623459815979 + 50.0 * 8.912432670593262
Epoch 960, val loss: 0.6805391311645508
Epoch 970, training loss: 446.41033935546875 = 0.6470027565956116 + 50.0 * 8.915266990661621
Epoch 970, val loss: 0.6776912212371826
Epoch 980, training loss: 446.49945068359375 = 0.6435473561286926 + 50.0 * 8.917118072509766
Epoch 980, val loss: 0.6750056147575378
Epoch 990, training loss: 446.8094787597656 = 0.6402572989463806 + 50.0 * 8.923384666442871
Epoch 990, val loss: 0.6723892092704773
Epoch 1000, training loss: 446.7543029785156 = 0.6369943022727966 + 50.0 * 8.922346115112305
Epoch 1000, val loss: 0.6698591709136963
Epoch 1010, training loss: 446.9343566894531 = 0.633947491645813 + 50.0 * 8.926008224487305
Epoch 1010, val loss: 0.6675023436546326
Epoch 1020, training loss: 446.9541015625 = 0.6309614181518555 + 50.0 * 8.92646312713623
Epoch 1020, val loss: 0.6653860211372375
Epoch 1030, training loss: 447.26800537109375 = 0.6281645894050598 + 50.0 * 8.932796478271484
Epoch 1030, val loss: 0.6632065773010254
Epoch 1040, training loss: 447.19732666015625 = 0.6252803802490234 + 50.0 * 8.931441307067871
Epoch 1040, val loss: 0.6610386967658997
Epoch 1050, training loss: 447.07598876953125 = 0.6225053071975708 + 50.0 * 8.929069519042969
Epoch 1050, val loss: 0.6591735482215881
Epoch 1060, training loss: 447.23101806640625 = 0.6199457049369812 + 50.0 * 8.932221412658691
Epoch 1060, val loss: 0.6573712229728699
Epoch 1070, training loss: 447.10028076171875 = 0.6173767447471619 + 50.0 * 8.929657936096191
Epoch 1070, val loss: 0.6554558277130127
Epoch 1080, training loss: 447.3262939453125 = 0.6149803996086121 + 50.0 * 8.934226036071777
Epoch 1080, val loss: 0.6537805199623108
Epoch 1090, training loss: 447.18035888671875 = 0.612586498260498 + 50.0 * 8.931355476379395
Epoch 1090, val loss: 0.6521394848823547
Epoch 1100, training loss: 447.4078674316406 = 0.6102567315101624 + 50.0 * 8.935952186584473
Epoch 1100, val loss: 0.6505204439163208
Epoch 1110, training loss: 447.42083740234375 = 0.6078695058822632 + 50.0 * 8.936259269714355
Epoch 1110, val loss: 0.6488268971443176
Epoch 1120, training loss: 447.34820556640625 = 0.605635941028595 + 50.0 * 8.93485164642334
Epoch 1120, val loss: 0.6473382711410522
Epoch 1130, training loss: 447.6571960449219 = 0.603522539138794 + 50.0 * 8.941073417663574
Epoch 1130, val loss: 0.645940363407135
Epoch 1140, training loss: 447.85162353515625 = 0.6014169454574585 + 50.0 * 8.9450044631958
Epoch 1140, val loss: 0.6446561217308044
Epoch 1150, training loss: 448.1014404296875 = 0.5993665456771851 + 50.0 * 8.950041770935059
Epoch 1150, val loss: 0.6432351469993591
Epoch 1160, training loss: 448.0819396972656 = 0.5972899198532104 + 50.0 * 8.949692726135254
Epoch 1160, val loss: 0.6419126391410828
Epoch 1170, training loss: 448.01348876953125 = 0.5952813029289246 + 50.0 * 8.9483642578125
Epoch 1170, val loss: 0.6408118009567261
Epoch 1180, training loss: 448.1384582519531 = 0.5934045910835266 + 50.0 * 8.95090103149414
Epoch 1180, val loss: 0.6396161913871765
Epoch 1190, training loss: 448.376953125 = 0.5915170907974243 + 50.0 * 8.955708503723145
Epoch 1190, val loss: 0.6384443044662476
Epoch 1200, training loss: 448.4483947753906 = 0.5896270275115967 + 50.0 * 8.957175254821777
Epoch 1200, val loss: 0.6373009085655212
Epoch 1210, training loss: 448.566162109375 = 0.5877208113670349 + 50.0 * 8.959568977355957
Epoch 1210, val loss: 0.6361081004142761
Epoch 1220, training loss: 448.7676086425781 = 0.5858373045921326 + 50.0 * 8.963635444641113
Epoch 1220, val loss: 0.6350582838058472
Epoch 1230, training loss: 449.00115966796875 = 0.5840222835540771 + 50.0 * 8.968342781066895
Epoch 1230, val loss: 0.6339121460914612
Epoch 1240, training loss: 448.7078857421875 = 0.5821570754051208 + 50.0 * 8.962514877319336
Epoch 1240, val loss: 0.6327939033508301
Epoch 1250, training loss: 448.8058776855469 = 0.580407977104187 + 50.0 * 8.964509010314941
Epoch 1250, val loss: 0.6318419575691223
Epoch 1260, training loss: 449.1167297363281 = 0.5786796808242798 + 50.0 * 8.9707612991333
Epoch 1260, val loss: 0.6308693885803223
Epoch 1270, training loss: 449.4417419433594 = 0.5769878625869751 + 50.0 * 8.977294921875
Epoch 1270, val loss: 0.6299392580986023
Epoch 1280, training loss: 449.160400390625 = 0.5752605199813843 + 50.0 * 8.971702575683594
Epoch 1280, val loss: 0.6289680004119873
Epoch 1290, training loss: 449.2903137207031 = 0.5736386775970459 + 50.0 * 8.974333763122559
Epoch 1290, val loss: 0.6281108260154724
Epoch 1300, training loss: 449.39874267578125 = 0.5719960927963257 + 50.0 * 8.976534843444824
Epoch 1300, val loss: 0.6273148059844971
Epoch 1310, training loss: 449.7660827636719 = 0.5703845620155334 + 50.0 * 8.983914375305176
Epoch 1310, val loss: 0.6263315081596375
Epoch 1320, training loss: 449.72601318359375 = 0.5687214136123657 + 50.0 * 8.983145713806152
Epoch 1320, val loss: 0.6253576278686523
Epoch 1330, training loss: 449.65313720703125 = 0.5670866966247559 + 50.0 * 8.981720924377441
Epoch 1330, val loss: 0.6244996190071106
Epoch 1340, training loss: 449.9039611816406 = 0.5655151605606079 + 50.0 * 8.98676872253418
Epoch 1340, val loss: 0.6237518787384033
Epoch 1350, training loss: 449.862060546875 = 0.563880980014801 + 50.0 * 8.985963821411133
Epoch 1350, val loss: 0.6229212880134583
Epoch 1360, training loss: 449.8289794921875 = 0.5622567534446716 + 50.0 * 8.985334396362305
Epoch 1360, val loss: 0.622076690196991
Epoch 1370, training loss: 449.9604797363281 = 0.5606350898742676 + 50.0 * 8.987997055053711
Epoch 1370, val loss: 0.6210852265357971
Epoch 1380, training loss: 449.9797058105469 = 0.5589522123336792 + 50.0 * 8.988414764404297
Epoch 1380, val loss: 0.6202754378318787
Epoch 1390, training loss: 450.0912780761719 = 0.5574799180030823 + 50.0 * 8.990675926208496
Epoch 1390, val loss: 0.6195530891418457
Epoch 1400, training loss: 450.4013977050781 = 0.5559927821159363 + 50.0 * 8.996908187866211
Epoch 1400, val loss: 0.6188526153564453
Epoch 1410, training loss: 450.0945129394531 = 0.5543928146362305 + 50.0 * 8.990802764892578
Epoch 1410, val loss: 0.618068277835846
Epoch 1420, training loss: 450.41571044921875 = 0.552885115146637 + 50.0 * 8.9972562789917
Epoch 1420, val loss: 0.6173290610313416
Epoch 1430, training loss: 447.39697265625 = 0.5508636236190796 + 50.0 * 8.936922073364258
Epoch 1430, val loss: 0.6162630319595337
Epoch 1440, training loss: 449.5694885253906 = 0.5495789647102356 + 50.0 * 8.980398178100586
Epoch 1440, val loss: 0.6155722141265869
Epoch 1450, training loss: 449.6114501953125 = 0.5482001900672913 + 50.0 * 8.9812650680542
Epoch 1450, val loss: 0.6150075793266296
Epoch 1460, training loss: 449.8409729003906 = 0.5466094017028809 + 50.0 * 8.98588752746582
Epoch 1460, val loss: 0.6143708825111389
Epoch 1470, training loss: 450.1123046875 = 0.545086145401001 + 50.0 * 8.991344451904297
Epoch 1470, val loss: 0.6133719682693481
Epoch 1480, training loss: 450.2997741699219 = 0.5435546636581421 + 50.0 * 8.995124816894531
Epoch 1480, val loss: 0.612720251083374
Epoch 1490, training loss: 450.4727478027344 = 0.5419889092445374 + 50.0 * 8.998615264892578
Epoch 1490, val loss: 0.6118651032447815
Epoch 1500, training loss: 450.74700927734375 = 0.5404127240180969 + 50.0 * 9.004132270812988
Epoch 1500, val loss: 0.6111516356468201
Epoch 1510, training loss: 450.5108947753906 = 0.5387775301933289 + 50.0 * 8.999442100524902
Epoch 1510, val loss: 0.6103074550628662
Epoch 1520, training loss: 450.8305969238281 = 0.5372706055641174 + 50.0 * 9.005867004394531
Epoch 1520, val loss: 0.6096518039703369
Epoch 1530, training loss: 451.1176452636719 = 0.5357016921043396 + 50.0 * 9.011638641357422
Epoch 1530, val loss: 0.6088390350341797
Epoch 1540, training loss: 450.8921813964844 = 0.5340784192085266 + 50.0 * 9.007162094116211
Epoch 1540, val loss: 0.6080629825592041
Epoch 1550, training loss: 451.19775390625 = 0.5325621962547302 + 50.0 * 9.013303756713867
Epoch 1550, val loss: 0.6073218584060669
Epoch 1560, training loss: 451.31732177734375 = 0.5309984087944031 + 50.0 * 9.015726089477539
Epoch 1560, val loss: 0.6066133379936218
Epoch 1570, training loss: 451.1884765625 = 0.5293983817100525 + 50.0 * 9.013181686401367
Epoch 1570, val loss: 0.6059094071388245
Epoch 1580, training loss: 451.538818359375 = 0.5278492569923401 + 50.0 * 9.020218849182129
Epoch 1580, val loss: 0.6051603555679321
Epoch 1590, training loss: 451.479736328125 = 0.5262517333030701 + 50.0 * 9.01906967163086
Epoch 1590, val loss: 0.6044761538505554
Epoch 1600, training loss: 451.6186218261719 = 0.5247498154640198 + 50.0 * 9.02187728881836
Epoch 1600, val loss: 0.6036944389343262
Epoch 1610, training loss: 451.8713684082031 = 0.5231850743293762 + 50.0 * 9.026963233947754
Epoch 1610, val loss: 0.6029150485992432
Epoch 1620, training loss: 451.8013000488281 = 0.5216047167778015 + 50.0 * 9.025593757629395
Epoch 1620, val loss: 0.60224449634552
Epoch 1630, training loss: 451.4742431640625 = 0.5199539065361023 + 50.0 * 9.019085884094238
Epoch 1630, val loss: 0.601579487323761
Epoch 1640, training loss: 451.732421875 = 0.5183992981910706 + 50.0 * 9.024280548095703
Epoch 1640, val loss: 0.6007683277130127
Epoch 1650, training loss: 452.0992736816406 = 0.5168550610542297 + 50.0 * 9.031648635864258
Epoch 1650, val loss: 0.6001389622688293
Epoch 1660, training loss: 452.2029113769531 = 0.5152764320373535 + 50.0 * 9.03375244140625
Epoch 1660, val loss: 0.5993086099624634
Epoch 1670, training loss: 452.264892578125 = 0.5136582255363464 + 50.0 * 9.035024642944336
Epoch 1670, val loss: 0.598580539226532
Epoch 1680, training loss: 452.01470947265625 = 0.5120619535446167 + 50.0 * 9.03005313873291
Epoch 1680, val loss: 0.5980269312858582
Epoch 1690, training loss: 451.9765930175781 = 0.5105016231536865 + 50.0 * 9.029321670532227
Epoch 1690, val loss: 0.5973420739173889
Epoch 1700, training loss: 452.291015625 = 0.5089848637580872 + 50.0 * 9.035640716552734
Epoch 1700, val loss: 0.596692681312561
Epoch 1710, training loss: 452.40081787109375 = 0.5073907971382141 + 50.0 * 9.03786849975586
Epoch 1710, val loss: 0.5960568785667419
Epoch 1720, training loss: 452.6962890625 = 0.5058263540267944 + 50.0 * 9.043808937072754
Epoch 1720, val loss: 0.5953431129455566
Epoch 1730, training loss: 452.69708251953125 = 0.5042545199394226 + 50.0 * 9.043856620788574
Epoch 1730, val loss: 0.5946773290634155
Epoch 1740, training loss: 452.7354736328125 = 0.5026751160621643 + 50.0 * 9.044655799865723
Epoch 1740, val loss: 0.5940098166465759
Epoch 1750, training loss: 452.9064025878906 = 0.5010879039764404 + 50.0 * 9.04810619354248
Epoch 1750, val loss: 0.5933172702789307
Epoch 1760, training loss: 452.7884826660156 = 0.4994988739490509 + 50.0 * 9.04577922821045
Epoch 1760, val loss: 0.5925974249839783
Epoch 1770, training loss: 450.848876953125 = 0.4982759952545166 + 50.0 * 9.007011413574219
Epoch 1770, val loss: 0.5918959975242615
Epoch 1780, training loss: 452.8095397949219 = 0.4968148469924927 + 50.0 * 9.04625415802002
Epoch 1780, val loss: 0.5915053486824036
Epoch 1790, training loss: 452.33935546875 = 0.4951488971710205 + 50.0 * 9.036884307861328
Epoch 1790, val loss: 0.5905359983444214
Epoch 1800, training loss: 452.1445007324219 = 0.4935133159160614 + 50.0 * 9.03302001953125
Epoch 1800, val loss: 0.5904995203018188
Epoch 1810, training loss: 452.9914855957031 = 0.4919474720954895 + 50.0 * 9.0499906539917
Epoch 1810, val loss: 0.5894730091094971
Epoch 1820, training loss: 453.5913391113281 = 0.49042680859565735 + 50.0 * 9.062018394470215
Epoch 1820, val loss: 0.5892230272293091
Epoch 1830, training loss: 453.8172912597656 = 0.48886188864707947 + 50.0 * 9.066568374633789
Epoch 1830, val loss: 0.5883259773254395
Epoch 1840, training loss: 454.0352478027344 = 0.4872549772262573 + 50.0 * 9.07096004486084
Epoch 1840, val loss: 0.5876924991607666
Epoch 1850, training loss: 454.3083801269531 = 0.4856407344341278 + 50.0 * 9.076455116271973
Epoch 1850, val loss: 0.5869935750961304
Epoch 1860, training loss: 454.1241760253906 = 0.48402395844459534 + 50.0 * 9.072803497314453
Epoch 1860, val loss: 0.5864090323448181
Epoch 1870, training loss: 454.1733093261719 = 0.48239949345588684 + 50.0 * 9.07381820678711
Epoch 1870, val loss: 0.585639238357544
Epoch 1880, training loss: 454.5378723144531 = 0.4808080494403839 + 50.0 * 9.081141471862793
Epoch 1880, val loss: 0.5849384069442749
Epoch 1890, training loss: 454.5646667480469 = 0.479199081659317 + 50.0 * 9.081709861755371
Epoch 1890, val loss: 0.5843669176101685
Epoch 1900, training loss: 454.7742004394531 = 0.4776049256324768 + 50.0 * 9.085931777954102
Epoch 1900, val loss: 0.5837680101394653
Epoch 1910, training loss: 454.8973388671875 = 0.4760063588619232 + 50.0 * 9.08842658996582
Epoch 1910, val loss: 0.5830819606781006
Epoch 1920, training loss: 454.4744567871094 = 0.47435489296913147 + 50.0 * 9.080001831054688
Epoch 1920, val loss: 0.582417368888855
Epoch 1930, training loss: 454.68115234375 = 0.4727892577648163 + 50.0 * 9.08416748046875
Epoch 1930, val loss: 0.5818238258361816
Epoch 1940, training loss: 454.8323974609375 = 0.4711863696575165 + 50.0 * 9.087224006652832
Epoch 1940, val loss: 0.5811797380447388
Epoch 1950, training loss: 454.9511413574219 = 0.46955594420433044 + 50.0 * 9.089632034301758
Epoch 1950, val loss: 0.5806249976158142
Epoch 1960, training loss: 455.2349853515625 = 0.46795785427093506 + 50.0 * 9.095340728759766
Epoch 1960, val loss: 0.579929769039154
Epoch 1970, training loss: 455.16973876953125 = 0.46631738543510437 + 50.0 * 9.09406852722168
Epoch 1970, val loss: 0.5793372392654419
Epoch 1980, training loss: 455.43316650390625 = 0.4647088050842285 + 50.0 * 9.099369049072266
Epoch 1980, val loss: 0.5787357687950134
Epoch 1990, training loss: 455.46527099609375 = 0.46307384967803955 + 50.0 * 9.100044250488281
Epoch 1990, val loss: 0.5781431794166565
Epoch 2000, training loss: 455.5451965332031 = 0.4614626169204712 + 50.0 * 9.101675033569336
Epoch 2000, val loss: 0.5773878693580627
Epoch 2010, training loss: 455.3456115722656 = 0.459844708442688 + 50.0 * 9.097715377807617
Epoch 2010, val loss: 0.5769060850143433
Epoch 2020, training loss: 454.401611328125 = 0.4581523835659027 + 50.0 * 9.078868865966797
Epoch 2020, val loss: 0.5764302611351013
Epoch 2030, training loss: 454.73065185546875 = 0.4566129744052887 + 50.0 * 9.085480690002441
Epoch 2030, val loss: 0.5754305124282837
Epoch 2040, training loss: 455.00390625 = 0.45508167147636414 + 50.0 * 9.09097671508789
Epoch 2040, val loss: 0.5752713680267334
Epoch 2050, training loss: 455.4317932128906 = 0.453531950712204 + 50.0 * 9.099565505981445
Epoch 2050, val loss: 0.5746732354164124
Epoch 2060, training loss: 455.84405517578125 = 0.4519398510456085 + 50.0 * 9.107842445373535
Epoch 2060, val loss: 0.5739189386367798
Epoch 2070, training loss: 456.0093994140625 = 0.45034247636795044 + 50.0 * 9.111181259155273
Epoch 2070, val loss: 0.573376476764679
Epoch 2080, training loss: 455.9886779785156 = 0.44873911142349243 + 50.0 * 9.110798835754395
Epoch 2080, val loss: 0.5728152394294739
Epoch 2090, training loss: 456.0827331542969 = 0.4471378028392792 + 50.0 * 9.112711906433105
Epoch 2090, val loss: 0.5719833374023438
Epoch 2100, training loss: 456.065673828125 = 0.4455316960811615 + 50.0 * 9.11240291595459
Epoch 2100, val loss: 0.5714766383171082
Epoch 2110, training loss: 456.2834167480469 = 0.44396287202835083 + 50.0 * 9.116788864135742
Epoch 2110, val loss: 0.5711071491241455
Epoch 2120, training loss: 455.9444274902344 = 0.4423552453517914 + 50.0 * 9.110041618347168
Epoch 2120, val loss: 0.57057124376297
Epoch 2130, training loss: 455.9012451171875 = 0.44078823924064636 + 50.0 * 9.109209060668945
Epoch 2130, val loss: 0.569955587387085
Epoch 2140, training loss: 456.14678955078125 = 0.4391573965549469 + 50.0 * 9.114152908325195
Epoch 2140, val loss: 0.5694814920425415
Epoch 2150, training loss: 456.59161376953125 = 0.437591016292572 + 50.0 * 9.123080253601074
Epoch 2150, val loss: 0.5688273906707764
Epoch 2160, training loss: 456.8721923828125 = 0.43594661355018616 + 50.0 * 9.128725051879883
Epoch 2160, val loss: 0.5683121085166931
Epoch 2170, training loss: 456.37176513671875 = 0.4343098998069763 + 50.0 * 9.118749618530273
Epoch 2170, val loss: 0.5678780674934387
Epoch 2180, training loss: 456.00030517578125 = 0.4326286315917969 + 50.0 * 9.111353874206543
Epoch 2180, val loss: 0.5673103332519531
Epoch 2190, training loss: 456.43499755859375 = 0.4310287833213806 + 50.0 * 9.120079040527344
Epoch 2190, val loss: 0.5668030977249146
Epoch 2200, training loss: 456.8899230957031 = 0.4294056296348572 + 50.0 * 9.129210472106934
Epoch 2200, val loss: 0.5663535594940186
Epoch 2210, training loss: 457.14154052734375 = 0.4277417063713074 + 50.0 * 9.134276390075684
Epoch 2210, val loss: 0.5656707882881165
Epoch 2220, training loss: 456.83880615234375 = 0.42605990171432495 + 50.0 * 9.128254890441895
Epoch 2220, val loss: 0.5651103258132935
Epoch 2230, training loss: 457.01788330078125 = 0.42439353466033936 + 50.0 * 9.131869316101074
Epoch 2230, val loss: 0.5647754073143005
Epoch 2240, training loss: 457.2263488769531 = 0.42273348569869995 + 50.0 * 9.136072158813477
Epoch 2240, val loss: 0.5641887784004211
Epoch 2250, training loss: 457.2717590332031 = 0.421051561832428 + 50.0 * 9.137014389038086
Epoch 2250, val loss: 0.5637388229370117
Epoch 2260, training loss: 457.00762939453125 = 0.41936376690864563 + 50.0 * 9.131765365600586
Epoch 2260, val loss: 0.5632644295692444
Epoch 2270, training loss: 455.7493591308594 = 0.4177676737308502 + 50.0 * 9.106632232666016
Epoch 2270, val loss: 0.5628715753555298
Epoch 2280, training loss: 456.33404541015625 = 0.4162047207355499 + 50.0 * 9.118356704711914
Epoch 2280, val loss: 0.5627380609512329
Epoch 2290, training loss: 454.75390625 = 0.4146793484687805 + 50.0 * 9.086784362792969
Epoch 2290, val loss: 0.5610612034797668
Epoch 2300, training loss: 455.1687927246094 = 0.4131329655647278 + 50.0 * 9.095112800598145
Epoch 2300, val loss: 0.5612977743148804
Epoch 2310, training loss: 455.99359130859375 = 0.4114173948764801 + 50.0 * 9.11164379119873
Epoch 2310, val loss: 0.5613341331481934
Epoch 2320, training loss: 456.1148986816406 = 0.4097159206867218 + 50.0 * 9.114103317260742
Epoch 2320, val loss: 0.5606455206871033
Epoch 2330, training loss: 456.6031494140625 = 0.40814054012298584 + 50.0 * 9.123900413513184
Epoch 2330, val loss: 0.5601226091384888
Epoch 2340, training loss: 456.85894775390625 = 0.40649500489234924 + 50.0 * 9.129049301147461
Epoch 2340, val loss: 0.5597717761993408
Epoch 2350, training loss: 457.0418701171875 = 0.4048246741294861 + 50.0 * 9.13274097442627
Epoch 2350, val loss: 0.5594985485076904
Epoch 2360, training loss: 457.08880615234375 = 0.4031803607940674 + 50.0 * 9.133712768554688
Epoch 2360, val loss: 0.559022068977356
Epoch 2370, training loss: 457.22100830078125 = 0.4015289545059204 + 50.0 * 9.13638973236084
Epoch 2370, val loss: 0.5585146546363831
Epoch 2380, training loss: 457.3040466308594 = 0.39987701177597046 + 50.0 * 9.138083457946777
Epoch 2380, val loss: 0.5580518245697021
Epoch 2390, training loss: 457.1846923828125 = 0.3981931507587433 + 50.0 * 9.135729789733887
Epoch 2390, val loss: 0.5575966238975525
Epoch 2400, training loss: 457.47650146484375 = 0.39656415581703186 + 50.0 * 9.14159870147705
Epoch 2400, val loss: 0.5572503805160522
Epoch 2410, training loss: 457.55633544921875 = 0.3949068486690521 + 50.0 * 9.143228530883789
Epoch 2410, val loss: 0.5567782521247864
Epoch 2420, training loss: 457.6101379394531 = 0.39325186610221863 + 50.0 * 9.14433765411377
Epoch 2420, val loss: 0.5564936399459839
Epoch 2430, training loss: 457.63385009765625 = 0.39159953594207764 + 50.0 * 9.144845008850098
Epoch 2430, val loss: 0.5561285018920898
Epoch 2440, training loss: 457.51629638671875 = 0.3899511992931366 + 50.0 * 9.142526626586914
Epoch 2440, val loss: 0.5556493997573853
Epoch 2450, training loss: 457.48956298828125 = 0.3883104622364044 + 50.0 * 9.142024993896484
Epoch 2450, val loss: 0.5553184747695923
Epoch 2460, training loss: 457.62432861328125 = 0.38666781783103943 + 50.0 * 9.144753456115723
Epoch 2460, val loss: 0.5548498034477234
Epoch 2470, training loss: 457.8578796386719 = 0.3850364089012146 + 50.0 * 9.149456977844238
Epoch 2470, val loss: 0.5544224381446838
Epoch 2480, training loss: 457.9012756347656 = 0.3834111988544464 + 50.0 * 9.150357246398926
Epoch 2480, val loss: 0.5541166663169861
Epoch 2490, training loss: 457.99481201171875 = 0.38176673650741577 + 50.0 * 9.152260780334473
Epoch 2490, val loss: 0.5538462400436401
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7840579710144927
0.8176483373179744
The final CL Acc:0.77831, 0.00536, The final GNN Acc:0.81738, 0.00043
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110760])
remove edge: torch.Size([2, 66186])
updated graph: torch.Size([2, 88298])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 509.2823486328125 = 1.1310553550720215 + 50.0 * 10.163025856018066
Epoch 0, val loss: 1.1313281059265137
Epoch 10, training loss: 490.7170104980469 = 1.1258081197738647 + 50.0 * 9.791824340820312
Epoch 10, val loss: 1.1261332035064697
Epoch 20, training loss: 480.3936462402344 = 1.120923399925232 + 50.0 * 9.585454940795898
Epoch 20, val loss: 1.1212749481201172
Epoch 30, training loss: 472.8939208984375 = 1.1162222623825073 + 50.0 * 9.435554504394531
Epoch 30, val loss: 1.1166021823883057
Epoch 40, training loss: 466.95477294921875 = 1.1117244958877563 + 50.0 * 9.316861152648926
Epoch 40, val loss: 1.1121306419372559
Epoch 50, training loss: 462.0201416015625 = 1.1074185371398926 + 50.0 * 9.218254089355469
Epoch 50, val loss: 1.107852578163147
Epoch 60, training loss: 457.83343505859375 = 1.1032921075820923 + 50.0 * 9.134602546691895
Epoch 60, val loss: 1.1037489175796509
Epoch 70, training loss: 454.3681335449219 = 1.0993235111236572 + 50.0 * 9.065376281738281
Epoch 70, val loss: 1.0998015403747559
Epoch 80, training loss: 451.49346923828125 = 1.095502257347107 + 50.0 * 9.007959365844727
Epoch 80, val loss: 1.0960030555725098
Epoch 90, training loss: 449.0638122558594 = 1.0917983055114746 + 50.0 * 8.959440231323242
Epoch 90, val loss: 1.0923200845718384
Epoch 100, training loss: 447.0337829589844 = 1.0882155895233154 + 50.0 * 8.91891098022461
Epoch 100, val loss: 1.0887662172317505
Epoch 110, training loss: 445.31884765625 = 1.0847636461257935 + 50.0 * 8.884681701660156
Epoch 110, val loss: 1.085337519645691
Epoch 120, training loss: 443.8359069824219 = 1.081404685974121 + 50.0 * 8.855090141296387
Epoch 120, val loss: 1.0820122957229614
Epoch 130, training loss: 442.6080322265625 = 1.078178882598877 + 50.0 * 8.830596923828125
Epoch 130, val loss: 1.0788135528564453
Epoch 140, training loss: 441.6258850097656 = 1.0750305652618408 + 50.0 * 8.811017036437988
Epoch 140, val loss: 1.0756927728652954
Epoch 150, training loss: 440.73760986328125 = 1.0719561576843262 + 50.0 * 8.793313026428223
Epoch 150, val loss: 1.0726394653320312
Epoch 160, training loss: 440.0978698730469 = 1.0690124034881592 + 50.0 * 8.780577659606934
Epoch 160, val loss: 1.0697365999221802
Epoch 170, training loss: 439.41650390625 = 1.066101312637329 + 50.0 * 8.767007827758789
Epoch 170, val loss: 1.0668816566467285
Epoch 180, training loss: 438.8612365722656 = 1.0632719993591309 + 50.0 * 8.755959510803223
Epoch 180, val loss: 1.0641001462936401
Epoch 190, training loss: 438.3235778808594 = 1.0605214834213257 + 50.0 * 8.745261192321777
Epoch 190, val loss: 1.0613749027252197
Epoch 200, training loss: 438.0730895996094 = 1.057887077331543 + 50.0 * 8.740303993225098
Epoch 200, val loss: 1.0587849617004395
Epoch 210, training loss: 437.7237854003906 = 1.0552966594696045 + 50.0 * 8.733369827270508
Epoch 210, val loss: 1.0562641620635986
Epoch 220, training loss: 437.4106750488281 = 1.052786946296692 + 50.0 * 8.727157592773438
Epoch 220, val loss: 1.0538115501403809
Epoch 230, training loss: 437.1690979003906 = 1.050250768661499 + 50.0 * 8.722376823425293
Epoch 230, val loss: 1.0513650178909302
Epoch 240, training loss: 436.90631103515625 = 1.0478023290634155 + 50.0 * 8.717170715332031
Epoch 240, val loss: 1.0489567518234253
Epoch 250, training loss: 436.7473449707031 = 1.0454466342926025 + 50.0 * 8.714037895202637
Epoch 250, val loss: 1.0466487407684326
Epoch 260, training loss: 436.7945861816406 = 1.043095588684082 + 50.0 * 8.7150297164917
Epoch 260, val loss: 1.0443916320800781
Epoch 270, training loss: 436.9460754394531 = 1.040771245956421 + 50.0 * 8.718106269836426
Epoch 270, val loss: 1.0421160459518433
Epoch 280, training loss: 436.9898681640625 = 1.0383974313735962 + 50.0 * 8.719029426574707
Epoch 280, val loss: 1.0398058891296387
Epoch 290, training loss: 436.8372497558594 = 1.0359848737716675 + 50.0 * 8.716025352478027
Epoch 290, val loss: 1.0374972820281982
Epoch 300, training loss: 436.999755859375 = 1.033622145652771 + 50.0 * 8.719322204589844
Epoch 300, val loss: 1.0352216958999634
Epoch 310, training loss: 436.8269958496094 = 1.031166672706604 + 50.0 * 8.715916633605957
Epoch 310, val loss: 1.0329029560089111
Epoch 320, training loss: 436.9502868652344 = 1.0286661386489868 + 50.0 * 8.718432426452637
Epoch 320, val loss: 1.0304877758026123
Epoch 330, training loss: 437.0 = 1.0261099338531494 + 50.0 * 8.719477653503418
Epoch 330, val loss: 1.0280133485794067
Epoch 340, training loss: 437.0893859863281 = 1.0234251022338867 + 50.0 * 8.721319198608398
Epoch 340, val loss: 1.0254470109939575
Epoch 350, training loss: 437.15130615234375 = 1.0205883979797363 + 50.0 * 8.722614288330078
Epoch 350, val loss: 1.0227735042572021
Epoch 360, training loss: 437.0699157714844 = 1.0177125930786133 + 50.0 * 8.721044540405273
Epoch 360, val loss: 1.0199899673461914
Epoch 370, training loss: 437.1920471191406 = 1.0147535800933838 + 50.0 * 8.723546028137207
Epoch 370, val loss: 1.0171504020690918
Epoch 380, training loss: 437.23388671875 = 1.0115728378295898 + 50.0 * 8.724446296691895
Epoch 380, val loss: 1.014173984527588
Epoch 390, training loss: 436.86175537109375 = 1.008336067199707 + 50.0 * 8.717068672180176
Epoch 390, val loss: 1.0109943151474
Epoch 400, training loss: 437.474853515625 = 1.0050510168075562 + 50.0 * 8.729395866394043
Epoch 400, val loss: 1.0079082250595093
Epoch 410, training loss: 437.2044677734375 = 1.001315951347351 + 50.0 * 8.7240629196167
Epoch 410, val loss: 1.0044102668762207
Epoch 420, training loss: 437.4134521484375 = 0.9976456165313721 + 50.0 * 8.728316307067871
Epoch 420, val loss: 1.0009061098098755
Epoch 430, training loss: 437.1257629394531 = 0.9937710165977478 + 50.0 * 8.722640037536621
Epoch 430, val loss: 0.9971961379051208
Epoch 440, training loss: 437.149169921875 = 0.9895974397659302 + 50.0 * 8.723191261291504
Epoch 440, val loss: 0.993294894695282
Epoch 450, training loss: 437.0334167480469 = 0.9853443503379822 + 50.0 * 8.720961570739746
Epoch 450, val loss: 0.9892303943634033
Epoch 460, training loss: 437.58331298828125 = 0.9811131358146667 + 50.0 * 8.732044219970703
Epoch 460, val loss: 0.9851764440536499
Epoch 470, training loss: 437.86590576171875 = 0.9765401482582092 + 50.0 * 8.737787246704102
Epoch 470, val loss: 0.9809311628341675
Epoch 480, training loss: 438.00726318359375 = 0.9717428088188171 + 50.0 * 8.740710258483887
Epoch 480, val loss: 0.9763803482055664
Epoch 490, training loss: 438.31707763671875 = 0.9667779803276062 + 50.0 * 8.7470064163208
Epoch 490, val loss: 0.9716809391975403
Epoch 500, training loss: 438.1620788574219 = 0.9614012837409973 + 50.0 * 8.744013786315918
Epoch 500, val loss: 0.9666820168495178
Epoch 510, training loss: 438.3898010253906 = 0.9560351371765137 + 50.0 * 8.748675346374512
Epoch 510, val loss: 0.9614902138710022
Epoch 520, training loss: 438.4981384277344 = 0.9503488540649414 + 50.0 * 8.750955581665039
Epoch 520, val loss: 0.9561343193054199
Epoch 530, training loss: 438.6259460449219 = 0.944551944732666 + 50.0 * 8.75362777709961
Epoch 530, val loss: 0.9506719708442688
Epoch 540, training loss: 438.6512145996094 = 0.9384590983390808 + 50.0 * 8.754255294799805
Epoch 540, val loss: 0.9448811411857605
Epoch 550, training loss: 438.87261962890625 = 0.9322113394737244 + 50.0 * 8.758808135986328
Epoch 550, val loss: 0.9389966726303101
Epoch 560, training loss: 439.0035095214844 = 0.9257060289382935 + 50.0 * 8.761555671691895
Epoch 560, val loss: 0.9328599572181702
Epoch 570, training loss: 439.0594482421875 = 0.918941855430603 + 50.0 * 8.762809753417969
Epoch 570, val loss: 0.9264873266220093
Epoch 580, training loss: 439.2144470214844 = 0.9119241237640381 + 50.0 * 8.766050338745117
Epoch 580, val loss: 0.9198331236839294
Epoch 590, training loss: 438.80267333984375 = 0.9045087099075317 + 50.0 * 8.757963180541992
Epoch 590, val loss: 0.9129698872566223
Epoch 600, training loss: 439.1217346191406 = 0.8971961736679077 + 50.0 * 8.764491081237793
Epoch 600, val loss: 0.9060236811637878
Epoch 610, training loss: 439.24261474609375 = 0.8889908194541931 + 50.0 * 8.767072677612305
Epoch 610, val loss: 0.8982840180397034
Epoch 620, training loss: 439.9383850097656 = 0.8817166090011597 + 50.0 * 8.781133651733398
Epoch 620, val loss: 0.8916099071502686
Epoch 630, training loss: 439.0355224609375 = 0.8732406497001648 + 50.0 * 8.763245582580566
Epoch 630, val loss: 0.8837801814079285
Epoch 640, training loss: 439.4874572753906 = 0.8653400540351868 + 50.0 * 8.772442817687988
Epoch 640, val loss: 0.8763212561607361
Epoch 650, training loss: 439.7917175292969 = 0.8569782376289368 + 50.0 * 8.778695106506348
Epoch 650, val loss: 0.8685844540596008
Epoch 660, training loss: 439.8227233886719 = 0.8483737111091614 + 50.0 * 8.779486656188965
Epoch 660, val loss: 0.8605708479881287
Epoch 670, training loss: 439.95428466796875 = 0.8394701480865479 + 50.0 * 8.782296180725098
Epoch 670, val loss: 0.8523370623588562
Epoch 680, training loss: 440.12139892578125 = 0.8306387662887573 + 50.0 * 8.785815238952637
Epoch 680, val loss: 0.8441696166992188
Epoch 690, training loss: 439.9291687011719 = 0.8214848041534424 + 50.0 * 8.782154083251953
Epoch 690, val loss: 0.8357204794883728
Epoch 700, training loss: 440.0485534667969 = 0.8124794363975525 + 50.0 * 8.784721374511719
Epoch 700, val loss: 0.8274341821670532
Epoch 710, training loss: 440.4046325683594 = 0.8036402463912964 + 50.0 * 8.792019844055176
Epoch 710, val loss: 0.8192949891090393
Epoch 720, training loss: 440.7619323730469 = 0.7947565913200378 + 50.0 * 8.79934310913086
Epoch 720, val loss: 0.8112283945083618
Epoch 730, training loss: 440.8148498535156 = 0.7857769727706909 + 50.0 * 8.800581932067871
Epoch 730, val loss: 0.8029432892799377
Epoch 740, training loss: 440.9088439941406 = 0.7768815159797668 + 50.0 * 8.80263900756836
Epoch 740, val loss: 0.7947991490364075
Epoch 750, training loss: 441.09326171875 = 0.7680343389511108 + 50.0 * 8.806504249572754
Epoch 750, val loss: 0.7867528796195984
Epoch 760, training loss: 441.0745544433594 = 0.7590641379356384 + 50.0 * 8.806309700012207
Epoch 760, val loss: 0.7786170244216919
Epoch 770, training loss: 441.4364929199219 = 0.7503973841667175 + 50.0 * 8.813721656799316
Epoch 770, val loss: 0.7708054780960083
Epoch 780, training loss: 441.6925964355469 = 0.7418286800384521 + 50.0 * 8.819015502929688
Epoch 780, val loss: 0.7630517482757568
Epoch 790, training loss: 441.2592468261719 = 0.7330129742622375 + 50.0 * 8.810524940490723
Epoch 790, val loss: 0.7552112936973572
Epoch 800, training loss: 441.54571533203125 = 0.724651575088501 + 50.0 * 8.816421508789062
Epoch 800, val loss: 0.7477073669433594
Epoch 810, training loss: 441.90460205078125 = 0.7165594696998596 + 50.0 * 8.823760986328125
Epoch 810, val loss: 0.7404578924179077
Epoch 820, training loss: 442.0822448730469 = 0.7086476683616638 + 50.0 * 8.827471733093262
Epoch 820, val loss: 0.7334288954734802
Epoch 830, training loss: 442.119384765625 = 0.7007883191108704 + 50.0 * 8.82837200164795
Epoch 830, val loss: 0.7265361547470093
Epoch 840, training loss: 442.2970886230469 = 0.6932498812675476 + 50.0 * 8.832077026367188
Epoch 840, val loss: 0.7198861241340637
Epoch 850, training loss: 442.3696594238281 = 0.6858828067779541 + 50.0 * 8.833675384521484
Epoch 850, val loss: 0.7134085297584534
Epoch 860, training loss: 442.34857177734375 = 0.6784958243370056 + 50.0 * 8.833401679992676
Epoch 860, val loss: 0.7070040702819824
Epoch 870, training loss: 442.4360656738281 = 0.6715081930160522 + 50.0 * 8.835290908813477
Epoch 870, val loss: 0.7009117603302002
Epoch 880, training loss: 442.64178466796875 = 0.6646808981895447 + 50.0 * 8.839542388916016
Epoch 880, val loss: 0.6950871348381042
Epoch 890, training loss: 442.808349609375 = 0.6580643057823181 + 50.0 * 8.843006134033203
Epoch 890, val loss: 0.6894111633300781
Epoch 900, training loss: 443.0090026855469 = 0.6517066955566406 + 50.0 * 8.847146034240723
Epoch 900, val loss: 0.6839591264724731
Epoch 910, training loss: 442.6177673339844 = 0.6451864838600159 + 50.0 * 8.839451789855957
Epoch 910, val loss: 0.6784202456474304
Epoch 920, training loss: 442.9100646972656 = 0.6393409371376038 + 50.0 * 8.845414161682129
Epoch 920, val loss: 0.6735156178474426
Epoch 930, training loss: 443.12774658203125 = 0.6335549354553223 + 50.0 * 8.849884033203125
Epoch 930, val loss: 0.6686754822731018
Epoch 940, training loss: 442.49981689453125 = 0.6279445290565491 + 50.0 * 8.837437629699707
Epoch 940, val loss: 0.6640281677246094
Epoch 950, training loss: 442.7857666015625 = 0.6224877238273621 + 50.0 * 8.843265533447266
Epoch 950, val loss: 0.6594993472099304
Epoch 960, training loss: 443.1223449707031 = 0.6172217130661011 + 50.0 * 8.850102424621582
Epoch 960, val loss: 0.6551141738891602
Epoch 970, training loss: 443.489501953125 = 0.6121563911437988 + 50.0 * 8.85754680633545
Epoch 970, val loss: 0.6509812474250793
Epoch 980, training loss: 443.2535705566406 = 0.6071407794952393 + 50.0 * 8.852928161621094
Epoch 980, val loss: 0.6468725800514221
Epoch 990, training loss: 443.4458923339844 = 0.6024096012115479 + 50.0 * 8.8568696975708
Epoch 990, val loss: 0.6429932713508606
Epoch 1000, training loss: 443.84552001953125 = 0.597808301448822 + 50.0 * 8.864953994750977
Epoch 1000, val loss: 0.6392402648925781
Epoch 1010, training loss: 443.9001770019531 = 0.5933806896209717 + 50.0 * 8.866135597229004
Epoch 1010, val loss: 0.6356726288795471
Epoch 1020, training loss: 443.7403869628906 = 0.5891193151473999 + 50.0 * 8.863025665283203
Epoch 1020, val loss: 0.6322318315505981
Epoch 1030, training loss: 443.93402099609375 = 0.5849705338478088 + 50.0 * 8.866981506347656
Epoch 1030, val loss: 0.62894606590271
Epoch 1040, training loss: 444.155029296875 = 0.5809385776519775 + 50.0 * 8.871481895446777
Epoch 1040, val loss: 0.6256695985794067
Epoch 1050, training loss: 444.33453369140625 = 0.5771626830101013 + 50.0 * 8.875147819519043
Epoch 1050, val loss: 0.6227489113807678
Epoch 1060, training loss: 443.5210876464844 = 0.5734421610832214 + 50.0 * 8.858952522277832
Epoch 1060, val loss: 0.619827389717102
Epoch 1070, training loss: 442.0103759765625 = 0.5695407390594482 + 50.0 * 8.828816413879395
Epoch 1070, val loss: 0.6167003512382507
Epoch 1080, training loss: 443.54327392578125 = 0.5666480660438538 + 50.0 * 8.859532356262207
Epoch 1080, val loss: 0.6148188710212708
Epoch 1090, training loss: 442.33941650390625 = 0.5633004903793335 + 50.0 * 8.835522651672363
Epoch 1090, val loss: 0.6121891140937805
Epoch 1100, training loss: 442.82843017578125 = 0.5603091716766357 + 50.0 * 8.845362663269043
Epoch 1100, val loss: 0.6097443103790283
Epoch 1110, training loss: 440.566162109375 = 0.5562514662742615 + 50.0 * 8.800198554992676
Epoch 1110, val loss: 0.6064258217811584
Epoch 1120, training loss: 443.5809631347656 = 0.5536166429519653 + 50.0 * 8.860547065734863
Epoch 1120, val loss: 0.6040304899215698
Epoch 1130, training loss: 442.90875244140625 = 0.5511206388473511 + 50.0 * 8.847152709960938
Epoch 1130, val loss: 0.6021796464920044
Epoch 1140, training loss: 442.6485900878906 = 0.548210084438324 + 50.0 * 8.842007637023926
Epoch 1140, val loss: 0.6003744602203369
Epoch 1150, training loss: 442.9163818359375 = 0.5454904437065125 + 50.0 * 8.847417831420898
Epoch 1150, val loss: 0.5978224277496338
Epoch 1160, training loss: 443.3018493652344 = 0.5431504845619202 + 50.0 * 8.85517406463623
Epoch 1160, val loss: 0.5962870717048645
Epoch 1170, training loss: 443.5542907714844 = 0.5407131910324097 + 50.0 * 8.860271453857422
Epoch 1170, val loss: 0.5946049094200134
Epoch 1180, training loss: 444.1808166503906 = 0.5384678840637207 + 50.0 * 8.872846603393555
Epoch 1180, val loss: 0.5928166508674622
Epoch 1190, training loss: 444.4835205078125 = 0.536155104637146 + 50.0 * 8.878947257995605
Epoch 1190, val loss: 0.5910266637802124
Epoch 1200, training loss: 444.463134765625 = 0.5338565111160278 + 50.0 * 8.878585815429688
Epoch 1200, val loss: 0.5893097519874573
Epoch 1210, training loss: 444.75823974609375 = 0.5316615104675293 + 50.0 * 8.88453197479248
Epoch 1210, val loss: 0.5877137780189514
Epoch 1220, training loss: 445.0198974609375 = 0.5294780731201172 + 50.0 * 8.889808654785156
Epoch 1220, val loss: 0.5860380530357361
Epoch 1230, training loss: 444.927001953125 = 0.5273091793060303 + 50.0 * 8.887993812561035
Epoch 1230, val loss: 0.5843680500984192
Epoch 1240, training loss: 445.0957946777344 = 0.5252459645271301 + 50.0 * 8.891410827636719
Epoch 1240, val loss: 0.5829267501831055
Epoch 1250, training loss: 445.3551330566406 = 0.523228645324707 + 50.0 * 8.896637916564941
Epoch 1250, val loss: 0.5814349055290222
Epoch 1260, training loss: 445.3995666503906 = 0.5212331414222717 + 50.0 * 8.897566795349121
Epoch 1260, val loss: 0.5799617767333984
Epoch 1270, training loss: 445.585205078125 = 0.5193123817443848 + 50.0 * 8.901317596435547
Epoch 1270, val loss: 0.5785559415817261
Epoch 1280, training loss: 445.7648010253906 = 0.5174193382263184 + 50.0 * 8.904947280883789
Epoch 1280, val loss: 0.5771345496177673
Epoch 1290, training loss: 445.8653564453125 = 0.5155215859413147 + 50.0 * 8.906996726989746
Epoch 1290, val loss: 0.5756771564483643
Epoch 1300, training loss: 445.931640625 = 0.5136670470237732 + 50.0 * 8.90835952758789
Epoch 1300, val loss: 0.5743483901023865
Epoch 1310, training loss: 446.066650390625 = 0.5118690729141235 + 50.0 * 8.91109561920166
Epoch 1310, val loss: 0.5730773210525513
Epoch 1320, training loss: 446.0242614746094 = 0.5100675225257874 + 50.0 * 8.910284042358398
Epoch 1320, val loss: 0.5717219114303589
Epoch 1330, training loss: 446.1421203613281 = 0.5082846283912659 + 50.0 * 8.912676811218262
Epoch 1330, val loss: 0.570485532283783
Epoch 1340, training loss: 446.59783935546875 = 0.5065610408782959 + 50.0 * 8.921825408935547
Epoch 1340, val loss: 0.5691720843315125
Epoch 1350, training loss: 446.33795166015625 = 0.5047770738601685 + 50.0 * 8.91666316986084
Epoch 1350, val loss: 0.5678834319114685
Epoch 1360, training loss: 445.4090881347656 = 0.5029853582382202 + 50.0 * 8.89812183380127
Epoch 1360, val loss: 0.5664567351341248
Epoch 1370, training loss: 445.1280212402344 = 0.5013089179992676 + 50.0 * 8.892534255981445
Epoch 1370, val loss: 0.5651222467422485
Epoch 1380, training loss: 446.0344543457031 = 0.49977147579193115 + 50.0 * 8.910693168640137
Epoch 1380, val loss: 0.5641741156578064
Epoch 1390, training loss: 446.2037353515625 = 0.49831241369247437 + 50.0 * 8.914108276367188
Epoch 1390, val loss: 0.5629259943962097
Epoch 1400, training loss: 446.5811462402344 = 0.4968397617340088 + 50.0 * 8.921686172485352
Epoch 1400, val loss: 0.5619901418685913
Epoch 1410, training loss: 446.8712158203125 = 0.49536019563674927 + 50.0 * 8.92751693725586
Epoch 1410, val loss: 0.5609773993492126
Epoch 1420, training loss: 446.93829345703125 = 0.4938580393791199 + 50.0 * 8.928888320922852
Epoch 1420, val loss: 0.55979984998703
Epoch 1430, training loss: 447.0998840332031 = 0.4924021065235138 + 50.0 * 8.932149887084961
Epoch 1430, val loss: 0.5587338805198669
Epoch 1440, training loss: 446.9905700683594 = 0.4909416139125824 + 50.0 * 8.92999267578125
Epoch 1440, val loss: 0.5576682090759277
Epoch 1450, training loss: 447.2619323730469 = 0.4895237386226654 + 50.0 * 8.935447692871094
Epoch 1450, val loss: 0.5567341446876526
Epoch 1460, training loss: 447.5975341796875 = 0.4881325960159302 + 50.0 * 8.942188262939453
Epoch 1460, val loss: 0.5556982159614563
Epoch 1470, training loss: 447.4143371582031 = 0.4866984486579895 + 50.0 * 8.938552856445312
Epoch 1470, val loss: 0.5547069907188416
Epoch 1480, training loss: 447.2944030761719 = 0.4853573739528656 + 50.0 * 8.93618106842041
Epoch 1480, val loss: 0.5537899732589722
Epoch 1490, training loss: 447.13531494140625 = 0.48399701714515686 + 50.0 * 8.933026313781738
Epoch 1490, val loss: 0.5528308153152466
Epoch 1500, training loss: 447.4216613769531 = 0.48271408677101135 + 50.0 * 8.9387788772583
Epoch 1500, val loss: 0.5519871711730957
Epoch 1510, training loss: 447.5730285644531 = 0.4814174473285675 + 50.0 * 8.941832542419434
Epoch 1510, val loss: 0.5510572195053101
Epoch 1520, training loss: 447.5921325683594 = 0.48009952902793884 + 50.0 * 8.942240715026855
Epoch 1520, val loss: 0.5501348972320557
Epoch 1530, training loss: 447.4739990234375 = 0.47877752780914307 + 50.0 * 8.93990421295166
Epoch 1530, val loss: 0.5493056774139404
Epoch 1540, training loss: 447.8116149902344 = 0.4774858057498932 + 50.0 * 8.946682929992676
Epoch 1540, val loss: 0.5483933687210083
Epoch 1550, training loss: 448.0346374511719 = 0.47619855403900146 + 50.0 * 8.95116901397705
Epoch 1550, val loss: 0.5475443601608276
Epoch 1560, training loss: 448.0405578613281 = 0.4749040901660919 + 50.0 * 8.951313018798828
Epoch 1560, val loss: 0.5466936230659485
Epoch 1570, training loss: 447.92138671875 = 0.47360724210739136 + 50.0 * 8.948955535888672
Epoch 1570, val loss: 0.545786440372467
Epoch 1580, training loss: 447.97772216796875 = 0.4723495841026306 + 50.0 * 8.95010757446289
Epoch 1580, val loss: 0.5450127720832825
Epoch 1590, training loss: 448.03802490234375 = 0.4710426330566406 + 50.0 * 8.951339721679688
Epoch 1590, val loss: 0.5440623164176941
Epoch 1600, training loss: 447.6126403808594 = 0.4697759747505188 + 50.0 * 8.942856788635254
Epoch 1600, val loss: 0.5432677268981934
Epoch 1610, training loss: 447.65118408203125 = 0.4685770273208618 + 50.0 * 8.943652153015137
Epoch 1610, val loss: 0.5426217913627625
Epoch 1620, training loss: 447.1056213378906 = 0.4673239290714264 + 50.0 * 8.93276596069336
Epoch 1620, val loss: 0.5418463349342346
Epoch 1630, training loss: 446.2069396972656 = 0.4661565124988556 + 50.0 * 8.914815902709961
Epoch 1630, val loss: 0.5413169860839844
Epoch 1640, training loss: 445.5436096191406 = 0.4648173153400421 + 50.0 * 8.901576042175293
Epoch 1640, val loss: 0.5405805706977844
Epoch 1650, training loss: 446.45721435546875 = 0.46359482407569885 + 50.0 * 8.919872283935547
Epoch 1650, val loss: 0.5396056771278381
Epoch 1660, training loss: 447.0809631347656 = 0.4623664617538452 + 50.0 * 8.932372093200684
Epoch 1660, val loss: 0.5385059714317322
Epoch 1670, training loss: 447.4049377441406 = 0.46110963821411133 + 50.0 * 8.938876152038574
Epoch 1670, val loss: 0.5378724336624146
Epoch 1680, training loss: 447.4267578125 = 0.4601196050643921 + 50.0 * 8.939332962036133
Epoch 1680, val loss: 0.5369237661361694
Epoch 1690, training loss: 447.4145202636719 = 0.4589138329029083 + 50.0 * 8.939111709594727
Epoch 1690, val loss: 0.5361253619194031
Epoch 1700, training loss: 447.2158203125 = 0.4578009247779846 + 50.0 * 8.935160636901855
Epoch 1700, val loss: 0.5361451506614685
Epoch 1710, training loss: 447.2633056640625 = 0.4565568268299103 + 50.0 * 8.936135292053223
Epoch 1710, val loss: 0.5350406765937805
Epoch 1720, training loss: 447.82464599609375 = 0.4552709460258484 + 50.0 * 8.9473876953125
Epoch 1720, val loss: 0.5341149568557739
Epoch 1730, training loss: 448.1457214355469 = 0.4539959132671356 + 50.0 * 8.953834533691406
Epoch 1730, val loss: 0.5332557559013367
Epoch 1740, training loss: 448.3458557128906 = 0.4527323544025421 + 50.0 * 8.957862854003906
Epoch 1740, val loss: 0.5323706865310669
Epoch 1750, training loss: 448.5260314941406 = 0.4514581561088562 + 50.0 * 8.961491584777832
Epoch 1750, val loss: 0.5314544439315796
Epoch 1760, training loss: 448.33392333984375 = 0.4501764476299286 + 50.0 * 8.957674980163574
Epoch 1760, val loss: 0.530631422996521
Epoch 1770, training loss: 448.5677185058594 = 0.4489368498325348 + 50.0 * 8.96237564086914
Epoch 1770, val loss: 0.5297521948814392
Epoch 1780, training loss: 448.7763366699219 = 0.4476827383041382 + 50.0 * 8.966572761535645
Epoch 1780, val loss: 0.5289567708969116
Epoch 1790, training loss: 449.00506591796875 = 0.44641759991645813 + 50.0 * 8.971173286437988
Epoch 1790, val loss: 0.5281534790992737
Epoch 1800, training loss: 448.95867919921875 = 0.4451446831226349 + 50.0 * 8.970270156860352
Epoch 1800, val loss: 0.527256965637207
Epoch 1810, training loss: 449.1541442871094 = 0.44387272000312805 + 50.0 * 8.974205017089844
Epoch 1810, val loss: 0.5264430642127991
Epoch 1820, training loss: 449.1397705078125 = 0.4425860047340393 + 50.0 * 8.973943710327148
Epoch 1820, val loss: 0.5255680084228516
Epoch 1830, training loss: 449.3031921386719 = 0.4413052797317505 + 50.0 * 8.977237701416016
Epoch 1830, val loss: 0.5247376561164856
Epoch 1840, training loss: 449.4123840332031 = 0.4400106966495514 + 50.0 * 8.979447364807129
Epoch 1840, val loss: 0.5238150358200073
Epoch 1850, training loss: 449.4162902832031 = 0.43872395157814026 + 50.0 * 8.979551315307617
Epoch 1850, val loss: 0.5230156779289246
Epoch 1860, training loss: 448.92822265625 = 0.43745848536491394 + 50.0 * 8.969815254211426
Epoch 1860, val loss: 0.5222107172012329
Epoch 1870, training loss: 448.13421630859375 = 0.4361073970794678 + 50.0 * 8.953962326049805
Epoch 1870, val loss: 0.5211654901504517
Epoch 1880, training loss: 448.30706787109375 = 0.43501248955726624 + 50.0 * 8.957441329956055
Epoch 1880, val loss: 0.5203839540481567
Epoch 1890, training loss: 448.6376647949219 = 0.4336934983730316 + 50.0 * 8.964079856872559
Epoch 1890, val loss: 0.5195599794387817
Epoch 1900, training loss: 448.2259826660156 = 0.4323558509349823 + 50.0 * 8.955872535705566
Epoch 1900, val loss: 0.5185354351997375
Epoch 1910, training loss: 448.4027099609375 = 0.4310796558856964 + 50.0 * 8.959432601928711
Epoch 1910, val loss: 0.5179765224456787
Epoch 1920, training loss: 448.89605712890625 = 0.429831862449646 + 50.0 * 8.969324111938477
Epoch 1920, val loss: 0.5169531106948853
Epoch 1930, training loss: 449.3352355957031 = 0.42853406071662903 + 50.0 * 8.978134155273438
Epoch 1930, val loss: 0.5162616968154907
Epoch 1940, training loss: 449.6876525878906 = 0.42719709873199463 + 50.0 * 8.985209465026855
Epoch 1940, val loss: 0.5152895450592041
Epoch 1950, training loss: 449.9141540527344 = 0.4258376657962799 + 50.0 * 8.989766120910645
Epoch 1950, val loss: 0.5143432021141052
Epoch 1960, training loss: 449.8946228027344 = 0.4244714081287384 + 50.0 * 8.989402770996094
Epoch 1960, val loss: 0.5134441256523132
Epoch 1970, training loss: 450.0580749511719 = 0.4231235682964325 + 50.0 * 8.992698669433594
Epoch 1970, val loss: 0.5124803781509399
Epoch 1980, training loss: 450.08966064453125 = 0.4217703640460968 + 50.0 * 8.99335765838623
Epoch 1980, val loss: 0.511488676071167
Epoch 1990, training loss: 449.9720764160156 = 0.4204007685184479 + 50.0 * 8.991033554077148
Epoch 1990, val loss: 0.5106743574142456
Epoch 2000, training loss: 450.1796569824219 = 0.4190690517425537 + 50.0 * 8.995211601257324
Epoch 2000, val loss: 0.5097095966339111
Epoch 2010, training loss: 450.3522033691406 = 0.41771140694618225 + 50.0 * 8.998689651489258
Epoch 2010, val loss: 0.5088560581207275
Epoch 2020, training loss: 450.2847595214844 = 0.4163358211517334 + 50.0 * 8.997368812561035
Epoch 2020, val loss: 0.5078731775283813
Epoch 2030, training loss: 450.2682189941406 = 0.414984792470932 + 50.0 * 8.997064590454102
Epoch 2030, val loss: 0.5069402456283569
Epoch 2040, training loss: 450.328369140625 = 0.41362011432647705 + 50.0 * 8.998294830322266
Epoch 2040, val loss: 0.5060359835624695
Epoch 2050, training loss: 450.4947814941406 = 0.4122302234172821 + 50.0 * 9.0016508102417
Epoch 2050, val loss: 0.5050908923149109
Epoch 2060, training loss: 450.3577880859375 = 0.4108385145664215 + 50.0 * 8.998939514160156
Epoch 2060, val loss: 0.5041180849075317
Epoch 2070, training loss: 450.50738525390625 = 0.4094540476799011 + 50.0 * 9.001958847045898
Epoch 2070, val loss: 0.503296971321106
Epoch 2080, training loss: 450.6866760253906 = 0.4080582559108734 + 50.0 * 9.005572319030762
Epoch 2080, val loss: 0.5022781491279602
Epoch 2090, training loss: 450.26129150390625 = 0.40663033723831177 + 50.0 * 8.997093200683594
Epoch 2090, val loss: 0.5010790228843689
Epoch 2100, training loss: 449.684326171875 = 0.40521207451820374 + 50.0 * 8.98558235168457
Epoch 2100, val loss: 0.5002624988555908
Epoch 2110, training loss: 449.9464416503906 = 0.40386903285980225 + 50.0 * 8.990851402282715
Epoch 2110, val loss: 0.4997975826263428
Epoch 2120, training loss: 450.22314453125 = 0.4025075137615204 + 50.0 * 8.996413230895996
Epoch 2120, val loss: 0.49859005212783813
Epoch 2130, training loss: 450.50128173828125 = 0.4010927975177765 + 50.0 * 9.00200366973877
Epoch 2130, val loss: 0.49771973490715027
Epoch 2140, training loss: 450.6613464355469 = 0.3996478021144867 + 50.0 * 9.005233764648438
Epoch 2140, val loss: 0.4965733587741852
Epoch 2150, training loss: 450.5837097167969 = 0.39817771315574646 + 50.0 * 9.003710746765137
Epoch 2150, val loss: 0.49578914046287537
Epoch 2160, training loss: 450.78131103515625 = 0.39673224091529846 + 50.0 * 9.007691383361816
Epoch 2160, val loss: 0.494751900434494
Epoch 2170, training loss: 450.882080078125 = 0.3952666223049164 + 50.0 * 9.009736061096191
Epoch 2170, val loss: 0.49382278323173523
Epoch 2180, training loss: 451.05517578125 = 0.39380308985710144 + 50.0 * 9.013227462768555
Epoch 2180, val loss: 0.4928801655769348
Epoch 2190, training loss: 450.911865234375 = 0.39231735467910767 + 50.0 * 9.010391235351562
Epoch 2190, val loss: 0.49203068017959595
Epoch 2200, training loss: 450.9390869140625 = 0.39086681604385376 + 50.0 * 9.010964393615723
Epoch 2200, val loss: 0.4910813570022583
Epoch 2210, training loss: 451.0563659667969 = 0.3893778622150421 + 50.0 * 9.01333999633789
Epoch 2210, val loss: 0.49020230770111084
Epoch 2220, training loss: 451.12615966796875 = 0.3878900110721588 + 50.0 * 9.014765739440918
Epoch 2220, val loss: 0.48927170038223267
Epoch 2230, training loss: 451.31549072265625 = 0.38639888167381287 + 50.0 * 9.01858139038086
Epoch 2230, val loss: 0.4884069561958313
Epoch 2240, training loss: 451.21844482421875 = 0.38487890362739563 + 50.0 * 9.016671180725098
Epoch 2240, val loss: 0.4874056577682495
Epoch 2250, training loss: 450.161865234375 = 0.3834504187107086 + 50.0 * 8.99556827545166
Epoch 2250, val loss: 0.4868764877319336
Epoch 2260, training loss: 449.6802978515625 = 0.38213297724723816 + 50.0 * 8.985962867736816
Epoch 2260, val loss: 0.4859510064125061
Epoch 2270, training loss: 449.9908142089844 = 0.38075199723243713 + 50.0 * 8.992201805114746
Epoch 2270, val loss: 0.48534834384918213
Epoch 2280, training loss: 450.33465576171875 = 0.37927162647247314 + 50.0 * 8.999107360839844
Epoch 2280, val loss: 0.4846213459968567
Epoch 2290, training loss: 450.8046569824219 = 0.3777416944503784 + 50.0 * 9.008538246154785
Epoch 2290, val loss: 0.48330581188201904
Epoch 2300, training loss: 450.9864807128906 = 0.3761880099773407 + 50.0 * 9.012206077575684
Epoch 2300, val loss: 0.4825054109096527
Epoch 2310, training loss: 451.2787780761719 = 0.3746054470539093 + 50.0 * 9.018083572387695
Epoch 2310, val loss: 0.4815538227558136
Epoch 2320, training loss: 451.4155578613281 = 0.3730098307132721 + 50.0 * 9.020851135253906
Epoch 2320, val loss: 0.4805484414100647
Epoch 2330, training loss: 451.37872314453125 = 0.3714068830013275 + 50.0 * 9.020146369934082
Epoch 2330, val loss: 0.4797343313694
Epoch 2340, training loss: 451.4618225097656 = 0.3698117733001709 + 50.0 * 9.02184009552002
Epoch 2340, val loss: 0.47881245613098145
Epoch 2350, training loss: 451.6551208496094 = 0.36821475625038147 + 50.0 * 9.025737762451172
Epoch 2350, val loss: 0.4779559373855591
Epoch 2360, training loss: 451.7034606933594 = 0.3666253089904785 + 50.0 * 9.02673625946045
Epoch 2360, val loss: 0.477084755897522
Epoch 2370, training loss: 451.7093200683594 = 0.36503273248672485 + 50.0 * 9.026885986328125
Epoch 2370, val loss: 0.47626954317092896
Epoch 2380, training loss: 451.4167785644531 = 0.36343368887901306 + 50.0 * 9.021066665649414
Epoch 2380, val loss: 0.4754753112792969
Epoch 2390, training loss: 451.5611267089844 = 0.36185958981513977 + 50.0 * 9.023985862731934
Epoch 2390, val loss: 0.47468045353889465
Epoch 2400, training loss: 451.6440124511719 = 0.3602614402770996 + 50.0 * 9.025674819946289
Epoch 2400, val loss: 0.47379013895988464
Epoch 2410, training loss: 451.6877136230469 = 0.35867297649383545 + 50.0 * 9.026580810546875
Epoch 2410, val loss: 0.47293150424957275
Epoch 2420, training loss: 451.9364318847656 = 0.35706421732902527 + 50.0 * 9.031587600708008
Epoch 2420, val loss: 0.47220903635025024
Epoch 2430, training loss: 452.04736328125 = 0.35544824600219727 + 50.0 * 9.033838272094727
Epoch 2430, val loss: 0.4713384807109833
Epoch 2440, training loss: 451.82440185546875 = 0.3538181185722351 + 50.0 * 9.029411315917969
Epoch 2440, val loss: 0.47047531604766846
Epoch 2450, training loss: 451.9918518066406 = 0.35222098231315613 + 50.0 * 9.032792091369629
Epoch 2450, val loss: 0.4697504937648773
Epoch 2460, training loss: 452.26727294921875 = 0.3506063222885132 + 50.0 * 9.03833293914795
Epoch 2460, val loss: 0.46886178851127625
Epoch 2470, training loss: 452.256591796875 = 0.34897580742836 + 50.0 * 9.038152694702148
Epoch 2470, val loss: 0.46812883019447327
Epoch 2480, training loss: 452.0588684082031 = 0.3473608195781708 + 50.0 * 9.03423023223877
Epoch 2480, val loss: 0.46742063760757446
Epoch 2490, training loss: 452.1991882324219 = 0.3457528352737427 + 50.0 * 9.037068367004395
Epoch 2490, val loss: 0.46664494276046753
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8272463768115942
0.8623487647612839
=== training gcn model ===
Epoch 0, training loss: 510.16241455078125 = 1.126225233078003 + 50.0 * 10.180724143981934
Epoch 0, val loss: 1.1256438493728638
Epoch 10, training loss: 488.4676208496094 = 1.1210298538208008 + 50.0 * 9.746932029724121
Epoch 10, val loss: 1.1204915046691895
Epoch 20, training loss: 477.9328918457031 = 1.116134762763977 + 50.0 * 9.536334991455078
Epoch 20, val loss: 1.1156128644943237
Epoch 30, training loss: 470.0592041015625 = 1.1114542484283447 + 50.0 * 9.378954887390137
Epoch 30, val loss: 1.1109580993652344
Epoch 40, training loss: 464.1798095703125 = 1.1069910526275635 + 50.0 * 9.261456489562988
Epoch 40, val loss: 1.1065200567245483
Epoch 50, training loss: 459.5382080078125 = 1.1027679443359375 + 50.0 * 9.168708801269531
Epoch 50, val loss: 1.102317214012146
Epoch 60, training loss: 455.6756591796875 = 1.0987380743026733 + 50.0 * 9.091538429260254
Epoch 60, val loss: 1.0983091592788696
Epoch 70, training loss: 452.4735412597656 = 1.0948964357376099 + 50.0 * 9.027572631835938
Epoch 70, val loss: 1.09449303150177
Epoch 80, training loss: 449.8146667480469 = 1.091247320175171 + 50.0 * 8.974468231201172
Epoch 80, val loss: 1.0908702611923218
Epoch 90, training loss: 447.5946960449219 = 1.0877605676651 + 50.0 * 8.93013858795166
Epoch 90, val loss: 1.0874054431915283
Epoch 100, training loss: 445.6708984375 = 1.0844323635101318 + 50.0 * 8.891729354858398
Epoch 100, val loss: 1.0841056108474731
Epoch 110, training loss: 444.0121765136719 = 1.0812493562698364 + 50.0 * 8.85861873626709
Epoch 110, val loss: 1.0809502601623535
Epoch 120, training loss: 442.6808776855469 = 1.078208327293396 + 50.0 * 8.832053184509277
Epoch 120, val loss: 1.0779430866241455
Epoch 130, training loss: 441.5005798339844 = 1.0753071308135986 + 50.0 * 8.808505058288574
Epoch 130, val loss: 1.0750741958618164
Epoch 140, training loss: 440.49920654296875 = 1.0725308656692505 + 50.0 * 8.788533210754395
Epoch 140, val loss: 1.0723398923873901
Epoch 150, training loss: 439.6802062988281 = 1.06987726688385 + 50.0 * 8.77220630645752
Epoch 150, val loss: 1.0697435140609741
Epoch 160, training loss: 439.4444580078125 = 1.067150354385376 + 50.0 * 8.767546653747559
Epoch 160, val loss: 1.0670385360717773
Epoch 170, training loss: 439.2216491699219 = 1.0648573637008667 + 50.0 * 8.76313591003418
Epoch 170, val loss: 1.0648119449615479
Epoch 180, training loss: 438.1934509277344 = 1.06245756149292 + 50.0 * 8.742619514465332
Epoch 180, val loss: 1.062442421913147
Epoch 190, training loss: 437.5833435058594 = 1.0601545572280884 + 50.0 * 8.730463981628418
Epoch 190, val loss: 1.0601893663406372
Epoch 200, training loss: 437.1484375 = 1.0579122304916382 + 50.0 * 8.721810340881348
Epoch 200, val loss: 1.0580042600631714
Epoch 210, training loss: 436.83770751953125 = 1.0557653903961182 + 50.0 * 8.715639114379883
Epoch 210, val loss: 1.0559155941009521
Epoch 220, training loss: 436.5625305175781 = 1.053621530532837 + 50.0 * 8.71017837524414
Epoch 220, val loss: 1.0538314580917358
Epoch 230, training loss: 436.2879943847656 = 1.0514756441116333 + 50.0 * 8.704730033874512
Epoch 230, val loss: 1.051760196685791
Epoch 240, training loss: 436.1387939453125 = 1.049392819404602 + 50.0 * 8.701787948608398
Epoch 240, val loss: 1.0497350692749023
Epoch 250, training loss: 436.0699462890625 = 1.0473849773406982 + 50.0 * 8.700450897216797
Epoch 250, val loss: 1.0477776527404785
Epoch 260, training loss: 435.70562744140625 = 1.045204758644104 + 50.0 * 8.693208694458008
Epoch 260, val loss: 1.045683741569519
Epoch 270, training loss: 435.8069152832031 = 1.043147087097168 + 50.0 * 8.69527530670166
Epoch 270, val loss: 1.0436818599700928
Epoch 280, training loss: 435.728271484375 = 1.0409785509109497 + 50.0 * 8.693745613098145
Epoch 280, val loss: 1.0415934324264526
Epoch 290, training loss: 435.5321350097656 = 1.0387076139450073 + 50.0 * 8.689868927001953
Epoch 290, val loss: 1.0394257307052612
Epoch 300, training loss: 435.5878601074219 = 1.0364638566970825 + 50.0 * 8.691027641296387
Epoch 300, val loss: 1.037237286567688
Epoch 310, training loss: 435.5609436035156 = 1.0341150760650635 + 50.0 * 8.690536499023438
Epoch 310, val loss: 1.034981608390808
Epoch 320, training loss: 435.478271484375 = 1.031657338142395 + 50.0 * 8.688932418823242
Epoch 320, val loss: 1.0326240062713623
Epoch 330, training loss: 435.3775329589844 = 1.0290859937667847 + 50.0 * 8.686968803405762
Epoch 330, val loss: 1.0301451683044434
Epoch 340, training loss: 435.3787841796875 = 1.0264043807983398 + 50.0 * 8.687047958374023
Epoch 340, val loss: 1.0275397300720215
Epoch 350, training loss: 435.6017150878906 = 1.0236858129501343 + 50.0 * 8.691560745239258
Epoch 350, val loss: 1.02492356300354
Epoch 360, training loss: 435.6111755371094 = 1.0207931995391846 + 50.0 * 8.691807746887207
Epoch 360, val loss: 1.0221515893936157
Epoch 370, training loss: 435.6791076660156 = 1.0177539587020874 + 50.0 * 8.69322681427002
Epoch 370, val loss: 1.0191724300384521
Epoch 380, training loss: 435.5376281738281 = 1.0144448280334473 + 50.0 * 8.69046401977539
Epoch 380, val loss: 1.0159729719161987
Epoch 390, training loss: 435.64593505859375 = 1.011167287826538 + 50.0 * 8.692695617675781
Epoch 390, val loss: 1.0127843618392944
Epoch 400, training loss: 435.7386474609375 = 1.0076823234558105 + 50.0 * 8.694619178771973
Epoch 400, val loss: 1.0094302892684937
Epoch 410, training loss: 435.88763427734375 = 1.003940463066101 + 50.0 * 8.697673797607422
Epoch 410, val loss: 1.0057910680770874
Epoch 420, training loss: 435.9695739746094 = 0.9999996423721313 + 50.0 * 8.69939136505127
Epoch 420, val loss: 1.001952886581421
Epoch 430, training loss: 436.1376953125 = 0.9960249066352844 + 50.0 * 8.70283317565918
Epoch 430, val loss: 0.9980971217155457
Epoch 440, training loss: 436.052734375 = 0.9915210604667664 + 50.0 * 8.701224327087402
Epoch 440, val loss: 0.9938510656356812
Epoch 450, training loss: 435.90997314453125 = 0.9870373606681824 + 50.0 * 8.698458671569824
Epoch 450, val loss: 0.9894624948501587
Epoch 460, training loss: 436.3431396484375 = 0.9821001887321472 + 50.0 * 8.707221031188965
Epoch 460, val loss: 0.9846505522727966
Epoch 470, training loss: 436.2843017578125 = 0.977421224117279 + 50.0 * 8.706137657165527
Epoch 470, val loss: 0.9802846312522888
Epoch 480, training loss: 435.9132080078125 = 0.9717271327972412 + 50.0 * 8.698829650878906
Epoch 480, val loss: 0.9747349619865417
Epoch 490, training loss: 436.0479431152344 = 0.9664623737335205 + 50.0 * 8.701629638671875
Epoch 490, val loss: 0.9696059823036194
Epoch 500, training loss: 436.25689697265625 = 0.9608615040779114 + 50.0 * 8.705920219421387
Epoch 500, val loss: 0.9642038941383362
Epoch 510, training loss: 436.2877502441406 = 0.9548736214637756 + 50.0 * 8.706657409667969
Epoch 510, val loss: 0.9584386944770813
Epoch 520, training loss: 436.44580078125 = 0.9486936330795288 + 50.0 * 8.709941864013672
Epoch 520, val loss: 0.9524862766265869
Epoch 530, training loss: 436.7174987792969 = 0.9422574043273926 + 50.0 * 8.71550464630127
Epoch 530, val loss: 0.9462371468544006
Epoch 540, training loss: 435.95513916015625 = 0.9349493384361267 + 50.0 * 8.700404167175293
Epoch 540, val loss: 0.9395697712898254
Epoch 550, training loss: 436.2454528808594 = 0.9281346797943115 + 50.0 * 8.70634651184082
Epoch 550, val loss: 0.9327618479728699
Epoch 560, training loss: 436.5343322753906 = 0.9211723804473877 + 50.0 * 8.712263107299805
Epoch 560, val loss: 0.9260811805725098
Epoch 570, training loss: 436.3345947265625 = 0.9135420322418213 + 50.0 * 8.708420753479004
Epoch 570, val loss: 0.9186939597129822
Epoch 580, training loss: 436.6573486328125 = 0.9059555530548096 + 50.0 * 8.715027809143066
Epoch 580, val loss: 0.9114170670509338
Epoch 590, training loss: 437.0576171875 = 0.8981753587722778 + 50.0 * 8.723189353942871
Epoch 590, val loss: 0.9039512276649475
Epoch 600, training loss: 437.0734558105469 = 0.8899123668670654 + 50.0 * 8.723670959472656
Epoch 600, val loss: 0.8960193991661072
Epoch 610, training loss: 437.1719665527344 = 0.8814836144447327 + 50.0 * 8.725810050964355
Epoch 610, val loss: 0.8880042433738708
Epoch 620, training loss: 437.431396484375 = 0.8732975125312805 + 50.0 * 8.731162071228027
Epoch 620, val loss: 0.880190908908844
Epoch 630, training loss: 437.2353515625 = 0.8641228079795837 + 50.0 * 8.727424621582031
Epoch 630, val loss: 0.8713831305503845
Epoch 640, training loss: 437.32684326171875 = 0.8546784520149231 + 50.0 * 8.729443550109863
Epoch 640, val loss: 0.8623701930046082
Epoch 650, training loss: 437.31756591796875 = 0.8452925086021423 + 50.0 * 8.729445457458496
Epoch 650, val loss: 0.8534293174743652
Epoch 660, training loss: 437.54766845703125 = 0.8358309268951416 + 50.0 * 8.734236717224121
Epoch 660, val loss: 0.8444927930831909
Epoch 670, training loss: 437.6988830566406 = 0.8261037468910217 + 50.0 * 8.737455368041992
Epoch 670, val loss: 0.8353376388549805
Epoch 680, training loss: 437.7994689941406 = 0.8162298798561096 + 50.0 * 8.739665031433105
Epoch 680, val loss: 0.8259265422821045
Epoch 690, training loss: 437.7380676269531 = 0.8062183856964111 + 50.0 * 8.73863697052002
Epoch 690, val loss: 0.8165780901908875
Epoch 700, training loss: 437.88323974609375 = 0.7960697412490845 + 50.0 * 8.741743087768555
Epoch 700, val loss: 0.8070238828659058
Epoch 710, training loss: 437.93603515625 = 0.785828709602356 + 50.0 * 8.743003845214844
Epoch 710, val loss: 0.7973472476005554
Epoch 720, training loss: 438.0317687988281 = 0.7758404612541199 + 50.0 * 8.745118141174316
Epoch 720, val loss: 0.7878932356834412
Epoch 730, training loss: 438.19720458984375 = 0.7656862139701843 + 50.0 * 8.74863052368164
Epoch 730, val loss: 0.7783536314964294
Epoch 740, training loss: 438.1338806152344 = 0.7555821537971497 + 50.0 * 8.747566223144531
Epoch 740, val loss: 0.7688896059989929
Epoch 750, training loss: 438.32684326171875 = 0.7457309365272522 + 50.0 * 8.751622200012207
Epoch 750, val loss: 0.7596325874328613
Epoch 760, training loss: 438.54754638671875 = 0.7357017397880554 + 50.0 * 8.756237030029297
Epoch 760, val loss: 0.7502874135971069
Epoch 770, training loss: 438.59771728515625 = 0.725777268409729 + 50.0 * 8.757438659667969
Epoch 770, val loss: 0.7409849762916565
Epoch 780, training loss: 438.52813720703125 = 0.7159141898155212 + 50.0 * 8.756244659423828
Epoch 780, val loss: 0.7317360639572144
Epoch 790, training loss: 438.66485595703125 = 0.7062524557113647 + 50.0 * 8.759172439575195
Epoch 790, val loss: 0.7226214408874512
Epoch 800, training loss: 439.0089111328125 = 0.6970425248146057 + 50.0 * 8.766237258911133
Epoch 800, val loss: 0.7142656445503235
Epoch 810, training loss: 438.9881896972656 = 0.6877037882804871 + 50.0 * 8.766009330749512
Epoch 810, val loss: 0.7054693102836609
Epoch 820, training loss: 439.0809326171875 = 0.6786147952079773 + 50.0 * 8.768046379089355
Epoch 820, val loss: 0.6969582438468933
Epoch 830, training loss: 439.3855285644531 = 0.6697668433189392 + 50.0 * 8.774314880371094
Epoch 830, val loss: 0.6887279748916626
Epoch 840, training loss: 439.2142028808594 = 0.6609201431274414 + 50.0 * 8.771065711975098
Epoch 840, val loss: 0.6806171536445618
Epoch 850, training loss: 439.59539794921875 = 0.6522802114486694 + 50.0 * 8.778861999511719
Epoch 850, val loss: 0.6728387475013733
Epoch 860, training loss: 436.04388427734375 = 0.6406956315040588 + 50.0 * 8.708064079284668
Epoch 860, val loss: 0.66138756275177
Epoch 870, training loss: 437.6634826660156 = 0.6356614232063293 + 50.0 * 8.740556716918945
Epoch 870, val loss: 0.6575806736946106
Epoch 880, training loss: 436.8569030761719 = 0.6270530819892883 + 50.0 * 8.724596977233887
Epoch 880, val loss: 0.6492979526519775
Epoch 890, training loss: 437.8844299316406 = 0.621009111404419 + 50.0 * 8.745268821716309
Epoch 890, val loss: 0.6438565254211426
Epoch 900, training loss: 437.7622985839844 = 0.6134213209152222 + 50.0 * 8.742977142333984
Epoch 900, val loss: 0.637133777141571
Epoch 910, training loss: 438.31268310546875 = 0.6067400574684143 + 50.0 * 8.754118919372559
Epoch 910, val loss: 0.6308935880661011
Epoch 920, training loss: 438.5821228027344 = 0.5998044013977051 + 50.0 * 8.75964641571045
Epoch 920, val loss: 0.6245912909507751
Epoch 930, training loss: 438.8277587890625 = 0.5931159257888794 + 50.0 * 8.764693260192871
Epoch 930, val loss: 0.6185228824615479
Epoch 940, training loss: 439.22271728515625 = 0.5866438746452332 + 50.0 * 8.772721290588379
Epoch 940, val loss: 0.6126455664634705
Epoch 950, training loss: 439.3307800292969 = 0.5802709460258484 + 50.0 * 8.775010108947754
Epoch 950, val loss: 0.6069799065589905
Epoch 960, training loss: 439.4440612792969 = 0.574087381362915 + 50.0 * 8.777399063110352
Epoch 960, val loss: 0.6013836860656738
Epoch 970, training loss: 439.659423828125 = 0.5680849552154541 + 50.0 * 8.781826972961426
Epoch 970, val loss: 0.5960339307785034
Epoch 980, training loss: 439.72149658203125 = 0.5622425675392151 + 50.0 * 8.783185005187988
Epoch 980, val loss: 0.5907816290855408
Epoch 990, training loss: 439.8392028808594 = 0.556555449962616 + 50.0 * 8.785653114318848
Epoch 990, val loss: 0.5856953263282776
Epoch 1000, training loss: 440.09234619140625 = 0.5512480735778809 + 50.0 * 8.79082202911377
Epoch 1000, val loss: 0.5809916257858276
Epoch 1010, training loss: 440.1288757324219 = 0.5460495948791504 + 50.0 * 8.791656494140625
Epoch 1010, val loss: 0.5763813853263855
Epoch 1020, training loss: 440.06109619140625 = 0.5409292578697205 + 50.0 * 8.790403366088867
Epoch 1020, val loss: 0.5720481872558594
Epoch 1030, training loss: 440.45654296875 = 0.5362108945846558 + 50.0 * 8.798406600952148
Epoch 1030, val loss: 0.5679225325584412
Epoch 1040, training loss: 440.6642761230469 = 0.5316116213798523 + 50.0 * 8.802653312683105
Epoch 1040, val loss: 0.5638520121574402
Epoch 1050, training loss: 440.3064270019531 = 0.5269981622695923 + 50.0 * 8.795588493347168
Epoch 1050, val loss: 0.559878408908844
Epoch 1060, training loss: 440.4290466308594 = 0.5226362347602844 + 50.0 * 8.798128128051758
Epoch 1060, val loss: 0.5562865734100342
Epoch 1070, training loss: 440.7352294921875 = 0.5184898376464844 + 50.0 * 8.80433464050293
Epoch 1070, val loss: 0.5527926087379456
Epoch 1080, training loss: 440.9427795410156 = 0.5144683122634888 + 50.0 * 8.808566093444824
Epoch 1080, val loss: 0.5493537187576294
Epoch 1090, training loss: 440.9998474121094 = 0.5105957984924316 + 50.0 * 8.809784889221191
Epoch 1090, val loss: 0.5461471676826477
Epoch 1100, training loss: 441.1443786621094 = 0.5068297386169434 + 50.0 * 8.812750816345215
Epoch 1100, val loss: 0.5430354475975037
Epoch 1110, training loss: 441.28875732421875 = 0.5032098889350891 + 50.0 * 8.81571102142334
Epoch 1110, val loss: 0.5400857925415039
Epoch 1120, training loss: 441.2340087890625 = 0.4996553957462311 + 50.0 * 8.81468677520752
Epoch 1120, val loss: 0.5372252464294434
Epoch 1130, training loss: 441.4338684082031 = 0.49625536799430847 + 50.0 * 8.81875228881836
Epoch 1130, val loss: 0.5344361066818237
Epoch 1140, training loss: 441.4358215332031 = 0.4929649233818054 + 50.0 * 8.818857192993164
Epoch 1140, val loss: 0.5318570137023926
Epoch 1150, training loss: 441.6944274902344 = 0.48985588550567627 + 50.0 * 8.824090957641602
Epoch 1150, val loss: 0.5293217301368713
Epoch 1160, training loss: 441.6972961425781 = 0.48673808574676514 + 50.0 * 8.824211120605469
Epoch 1160, val loss: 0.5269750952720642
Epoch 1170, training loss: 441.79852294921875 = 0.4837779104709625 + 50.0 * 8.826294898986816
Epoch 1170, val loss: 0.5246918797492981
Epoch 1180, training loss: 442.0044860839844 = 0.4809037446975708 + 50.0 * 8.830471992492676
Epoch 1180, val loss: 0.5226074457168579
Epoch 1190, training loss: 441.99212646484375 = 0.47815483808517456 + 50.0 * 8.830279350280762
Epoch 1190, val loss: 0.5204026699066162
Epoch 1200, training loss: 442.1582946777344 = 0.475528746843338 + 50.0 * 8.83365535736084
Epoch 1200, val loss: 0.5184335708618164
Epoch 1210, training loss: 441.7583923339844 = 0.4729192554950714 + 50.0 * 8.825709342956543
Epoch 1210, val loss: 0.5163663625717163
Epoch 1220, training loss: 441.7882080078125 = 0.4705324172973633 + 50.0 * 8.826354026794434
Epoch 1220, val loss: 0.5146961808204651
Epoch 1230, training loss: 442.1277770996094 = 0.46823057532310486 + 50.0 * 8.83319091796875
Epoch 1230, val loss: 0.5130148530006409
Epoch 1240, training loss: 442.37432861328125 = 0.4659067988395691 + 50.0 * 8.838168144226074
Epoch 1240, val loss: 0.5114783048629761
Epoch 1250, training loss: 442.51971435546875 = 0.46360310912132263 + 50.0 * 8.8411226272583
Epoch 1250, val loss: 0.5098469257354736
Epoch 1260, training loss: 438.8681335449219 = 0.4601578116416931 + 50.0 * 8.768159866333008
Epoch 1260, val loss: 0.5060368776321411
Epoch 1270, training loss: 440.38995361328125 = 0.4582756459712982 + 50.0 * 8.798633575439453
Epoch 1270, val loss: 0.5070101618766785
Epoch 1280, training loss: 440.2445373535156 = 0.4571608304977417 + 50.0 * 8.795747756958008
Epoch 1280, val loss: 0.5044618248939514
Epoch 1290, training loss: 440.1673583984375 = 0.45500725507736206 + 50.0 * 8.794246673583984
Epoch 1290, val loss: 0.5043838620185852
Epoch 1300, training loss: 440.9208068847656 = 0.45290690660476685 + 50.0 * 8.809357643127441
Epoch 1300, val loss: 0.5024329423904419
Epoch 1310, training loss: 441.582763671875 = 0.45110219717025757 + 50.0 * 8.822632789611816
Epoch 1310, val loss: 0.5010889768600464
Epoch 1320, training loss: 441.9176940917969 = 0.4492543041706085 + 50.0 * 8.829368591308594
Epoch 1320, val loss: 0.5001852512359619
Epoch 1330, training loss: 442.2565002441406 = 0.44742974638938904 + 50.0 * 8.836181640625
Epoch 1330, val loss: 0.49899259209632874
Epoch 1340, training loss: 442.5495300292969 = 0.4455792307853699 + 50.0 * 8.842079162597656
Epoch 1340, val loss: 0.4978455901145935
Epoch 1350, training loss: 442.461181640625 = 0.4437357485294342 + 50.0 * 8.840349197387695
Epoch 1350, val loss: 0.4966054856777191
Epoch 1360, training loss: 442.4071044921875 = 0.44190430641174316 + 50.0 * 8.839303970336914
Epoch 1360, val loss: 0.4956091344356537
Epoch 1370, training loss: 442.4084777832031 = 0.44020605087280273 + 50.0 * 8.839365005493164
Epoch 1370, val loss: 0.4944739043712616
Epoch 1380, training loss: 442.8507385253906 = 0.43857088685035706 + 50.0 * 8.848243713378906
Epoch 1380, val loss: 0.4934958815574646
Epoch 1390, training loss: 442.9975280761719 = 0.4368799328804016 + 50.0 * 8.851212501525879
Epoch 1390, val loss: 0.4925857186317444
Epoch 1400, training loss: 443.0547790527344 = 0.43520471453666687 + 50.0 * 8.852391242980957
Epoch 1400, val loss: 0.49148064851760864
Epoch 1410, training loss: 443.24273681640625 = 0.4336286187171936 + 50.0 * 8.856182098388672
Epoch 1410, val loss: 0.490659236907959
Epoch 1420, training loss: 443.49798583984375 = 0.4321017563343048 + 50.0 * 8.86131763458252
Epoch 1420, val loss: 0.4897739291191101
Epoch 1430, training loss: 443.4223937988281 = 0.43051427602767944 + 50.0 * 8.859837532043457
Epoch 1430, val loss: 0.4888904392719269
Epoch 1440, training loss: 443.1086730957031 = 0.4288790822029114 + 50.0 * 8.853595733642578
Epoch 1440, val loss: 0.48789411783218384
Epoch 1450, training loss: 443.4383850097656 = 0.42743968963623047 + 50.0 * 8.86021900177002
Epoch 1450, val loss: 0.4869709610939026
Epoch 1460, training loss: 443.7496337890625 = 0.4259788990020752 + 50.0 * 8.866473197937012
Epoch 1460, val loss: 0.48626771569252014
Epoch 1470, training loss: 443.8152770996094 = 0.42454347014427185 + 50.0 * 8.867815017700195
Epoch 1470, val loss: 0.48548945784568787
Epoch 1480, training loss: 443.9250793457031 = 0.4231133759021759 + 50.0 * 8.870038986206055
Epoch 1480, val loss: 0.4847222566604614
Epoch 1490, training loss: 444.06903076171875 = 0.42169833183288574 + 50.0 * 8.872946739196777
Epoch 1490, val loss: 0.4840378761291504
Epoch 1500, training loss: 444.0023498535156 = 0.4202527105808258 + 50.0 * 8.871642112731934
Epoch 1500, val loss: 0.48323318362236023
Epoch 1510, training loss: 444.1718444824219 = 0.4188530445098877 + 50.0 * 8.875060081481934
Epoch 1510, val loss: 0.48243802785873413
Epoch 1520, training loss: 444.2246398925781 = 0.4175175130367279 + 50.0 * 8.876142501831055
Epoch 1520, val loss: 0.48172980546951294
Epoch 1530, training loss: 444.4848937988281 = 0.4162234961986542 + 50.0 * 8.881373405456543
Epoch 1530, val loss: 0.481030136346817
Epoch 1540, training loss: 444.26123046875 = 0.41485658288002014 + 50.0 * 8.876927375793457
Epoch 1540, val loss: 0.480304092168808
Epoch 1550, training loss: 444.4749450683594 = 0.4135827124118805 + 50.0 * 8.881227493286133
Epoch 1550, val loss: 0.47962382435798645
Epoch 1560, training loss: 444.4377746582031 = 0.4122600853443146 + 50.0 * 8.880510330200195
Epoch 1560, val loss: 0.47902432084083557
Epoch 1570, training loss: 444.66033935546875 = 0.4110322594642639 + 50.0 * 8.88498592376709
Epoch 1570, val loss: 0.4783046841621399
Epoch 1580, training loss: 444.1975402832031 = 0.40962308645248413 + 50.0 * 8.875758171081543
Epoch 1580, val loss: 0.4774514436721802
Epoch 1590, training loss: 444.3540954589844 = 0.40845298767089844 + 50.0 * 8.878912925720215
Epoch 1590, val loss: 0.4768768548965454
Epoch 1600, training loss: 444.7848205566406 = 0.40726548433303833 + 50.0 * 8.887551307678223
Epoch 1600, val loss: 0.4763825237751007
Epoch 1610, training loss: 444.8591003417969 = 0.4060273766517639 + 50.0 * 8.889060974121094
Epoch 1610, val loss: 0.4755655527114868
Epoch 1620, training loss: 444.88629150390625 = 0.40482181310653687 + 50.0 * 8.889629364013672
Epoch 1620, val loss: 0.47487834095954895
Epoch 1630, training loss: 444.8627014160156 = 0.40349626541137695 + 50.0 * 8.88918399810791
Epoch 1630, val loss: 0.4746047556400299
Epoch 1640, training loss: 444.9278564453125 = 0.40234363079071045 + 50.0 * 8.890510559082031
Epoch 1640, val loss: 0.4729660451412201
Epoch 1650, training loss: 444.7621765136719 = 0.4011361002922058 + 50.0 * 8.887221336364746
Epoch 1650, val loss: 0.4728737473487854
Epoch 1660, training loss: 445.3648986816406 = 0.4001272916793823 + 50.0 * 8.899295806884766
Epoch 1660, val loss: 0.47224825620651245
Epoch 1670, training loss: 445.8507995605469 = 0.3990093171596527 + 50.0 * 8.909035682678223
Epoch 1670, val loss: 0.4717824459075928
Epoch 1680, training loss: 446.2383728027344 = 0.39785444736480713 + 50.0 * 8.916810035705566
Epoch 1680, val loss: 0.4710991084575653
Epoch 1690, training loss: 446.2867736816406 = 0.3966626226902008 + 50.0 * 8.917801856994629
Epoch 1690, val loss: 0.4703685939311981
Epoch 1700, training loss: 446.3004455566406 = 0.3954865634441376 + 50.0 * 8.918099403381348
Epoch 1700, val loss: 0.46983838081359863
Epoch 1710, training loss: 446.4970397949219 = 0.3943679928779602 + 50.0 * 8.922053337097168
Epoch 1710, val loss: 0.4692527651786804
Epoch 1720, training loss: 446.7763977050781 = 0.39323630928993225 + 50.0 * 8.92766284942627
Epoch 1720, val loss: 0.46866273880004883
Epoch 1730, training loss: 446.5312194824219 = 0.39206475019454956 + 50.0 * 8.922782897949219
Epoch 1730, val loss: 0.46792489290237427
Epoch 1740, training loss: 446.7624816894531 = 0.3909502625465393 + 50.0 * 8.927430152893066
Epoch 1740, val loss: 0.46741896867752075
Epoch 1750, training loss: 446.8080139160156 = 0.38987186551094055 + 50.0 * 8.928362846374512
Epoch 1750, val loss: 0.46691128611564636
Epoch 1760, training loss: 446.71612548828125 = 0.3887437880039215 + 50.0 * 8.92654800415039
Epoch 1760, val loss: 0.46636512875556946
Epoch 1770, training loss: 444.79473876953125 = 0.38779520988464355 + 50.0 * 8.888138771057129
Epoch 1770, val loss: 0.465707927942276
Epoch 1780, training loss: 446.1380310058594 = 0.3871907591819763 + 50.0 * 8.915017127990723
Epoch 1780, val loss: 0.4661042094230652
Epoch 1790, training loss: 446.17877197265625 = 0.38619884848594666 + 50.0 * 8.915851593017578
Epoch 1790, val loss: 0.4649619162082672
Epoch 1800, training loss: 447.17144775390625 = 0.3851974308490753 + 50.0 * 8.935725212097168
Epoch 1800, val loss: 0.4645654261112213
Epoch 1810, training loss: 447.57470703125 = 0.3840923607349396 + 50.0 * 8.943812370300293
Epoch 1810, val loss: 0.4640427827835083
Epoch 1820, training loss: 447.87744140625 = 0.3830176889896393 + 50.0 * 8.949888229370117
Epoch 1820, val loss: 0.4636629521846771
Epoch 1830, training loss: 447.9946594238281 = 0.38194364309310913 + 50.0 * 8.952254295349121
Epoch 1830, val loss: 0.4630451798439026
Epoch 1840, training loss: 447.8373718261719 = 0.3808418810367584 + 50.0 * 8.94913101196289
Epoch 1840, val loss: 0.4626578986644745
Epoch 1850, training loss: 448.299560546875 = 0.3797667920589447 + 50.0 * 8.958395957946777
Epoch 1850, val loss: 0.4620124101638794
Epoch 1860, training loss: 448.60186767578125 = 0.3786681890487671 + 50.0 * 8.96446418762207
Epoch 1860, val loss: 0.4615245461463928
Epoch 1870, training loss: 448.3658142089844 = 0.3775438368320465 + 50.0 * 8.959765434265137
Epoch 1870, val loss: 0.46095404028892517
Epoch 1880, training loss: 448.5929870605469 = 0.3764846920967102 + 50.0 * 8.964329719543457
Epoch 1880, val loss: 0.46044301986694336
Epoch 1890, training loss: 448.458251953125 = 0.37537330389022827 + 50.0 * 8.961657524108887
Epoch 1890, val loss: 0.45986229181289673
Epoch 1900, training loss: 448.7420959472656 = 0.37430086731910706 + 50.0 * 8.967355728149414
Epoch 1900, val loss: 0.45929545164108276
Epoch 1910, training loss: 449.1764831542969 = 0.37327009439468384 + 50.0 * 8.976064682006836
Epoch 1910, val loss: 0.45873820781707764
Epoch 1920, training loss: 449.1202697753906 = 0.3722075819969177 + 50.0 * 8.974961280822754
Epoch 1920, val loss: 0.4583221673965454
Epoch 1930, training loss: 449.2281799316406 = 0.3711543381214142 + 50.0 * 8.977140426635742
Epoch 1930, val loss: 0.4577823579311371
Epoch 1940, training loss: 449.201416015625 = 0.37016722559928894 + 50.0 * 8.976624488830566
Epoch 1940, val loss: 0.45711177587509155
Epoch 1950, training loss: 449.3800354003906 = 0.3691413104534149 + 50.0 * 8.980217933654785
Epoch 1950, val loss: 0.45679476857185364
Epoch 1960, training loss: 449.50384521484375 = 0.3680969178676605 + 50.0 * 8.982714653015137
Epoch 1960, val loss: 0.45623958110809326
Epoch 1970, training loss: 449.49713134765625 = 0.3670528531074524 + 50.0 * 8.982601165771484
Epoch 1970, val loss: 0.45566070079803467
Epoch 1980, training loss: 449.7507629394531 = 0.36602553725242615 + 50.0 * 8.98769474029541
Epoch 1980, val loss: 0.4551643133163452
Epoch 1990, training loss: 449.4640197753906 = 0.36498555541038513 + 50.0 * 8.981980323791504
Epoch 1990, val loss: 0.4546670615673065
Epoch 2000, training loss: 449.6997375488281 = 0.3639778792858124 + 50.0 * 8.986715316772461
Epoch 2000, val loss: 0.4540996551513672
Epoch 2010, training loss: 450.0274658203125 = 0.3629681169986725 + 50.0 * 8.993289947509766
Epoch 2010, val loss: 0.4536864161491394
Epoch 2020, training loss: 449.9742736816406 = 0.3619159162044525 + 50.0 * 8.992247581481934
Epoch 2020, val loss: 0.45312532782554626
Epoch 2030, training loss: 450.05535888671875 = 0.36090928316116333 + 50.0 * 8.993888854980469
Epoch 2030, val loss: 0.4526953101158142
Epoch 2040, training loss: 450.15472412109375 = 0.3599068522453308 + 50.0 * 8.995896339416504
Epoch 2040, val loss: 0.4521784484386444
Epoch 2050, training loss: 450.41265869140625 = 0.3588975965976715 + 50.0 * 9.001075744628906
Epoch 2050, val loss: 0.45177796483039856
Epoch 2060, training loss: 450.63287353515625 = 0.35788694024086 + 50.0 * 9.005499839782715
Epoch 2060, val loss: 0.4511580765247345
Epoch 2070, training loss: 450.2765197753906 = 0.35686516761779785 + 50.0 * 8.998393058776855
Epoch 2070, val loss: 0.45065972208976746
Epoch 2080, training loss: 450.36566162109375 = 0.3558935523033142 + 50.0 * 9.000195503234863
Epoch 2080, val loss: 0.45027491450309753
Epoch 2090, training loss: 450.5547180175781 = 0.354905366897583 + 50.0 * 9.003995895385742
Epoch 2090, val loss: 0.44979098439216614
Epoch 2100, training loss: 450.3411560058594 = 0.35389044880867004 + 50.0 * 8.99974536895752
Epoch 2100, val loss: 0.44937339425086975
Epoch 2110, training loss: 450.5049133300781 = 0.35292768478393555 + 50.0 * 9.003039360046387
Epoch 2110, val loss: 0.44883131980895996
Epoch 2120, training loss: 450.7426452636719 = 0.35196709632873535 + 50.0 * 9.007813453674316
Epoch 2120, val loss: 0.44856691360473633
Epoch 2130, training loss: 450.90216064453125 = 0.3509661555290222 + 50.0 * 9.011024475097656
Epoch 2130, val loss: 0.44805803894996643
Epoch 2140, training loss: 450.8370361328125 = 0.34995776414871216 + 50.0 * 9.00974178314209
Epoch 2140, val loss: 0.44756582379341125
Epoch 2150, training loss: 450.7708435058594 = 0.3489886522293091 + 50.0 * 9.008437156677246
Epoch 2150, val loss: 0.44719940423965454
Epoch 2160, training loss: 451.064697265625 = 0.3480073809623718 + 50.0 * 9.014333724975586
Epoch 2160, val loss: 0.4467780590057373
Epoch 2170, training loss: 451.2615966796875 = 0.3470149338245392 + 50.0 * 9.018291473388672
Epoch 2170, val loss: 0.44625192880630493
Epoch 2180, training loss: 450.8788146972656 = 0.3459985852241516 + 50.0 * 9.010656356811523
Epoch 2180, val loss: 0.44575148820877075
Epoch 2190, training loss: 451.1387023925781 = 0.3450542688369751 + 50.0 * 9.015872955322266
Epoch 2190, val loss: 0.4453469216823578
Epoch 2200, training loss: 451.3515319824219 = 0.34409698843955994 + 50.0 * 9.020149230957031
Epoch 2200, val loss: 0.4450070858001709
Epoch 2210, training loss: 451.40374755859375 = 0.34311795234680176 + 50.0 * 9.021212577819824
Epoch 2210, val loss: 0.4444919526576996
Epoch 2220, training loss: 451.59478759765625 = 0.3421146273612976 + 50.0 * 9.025053977966309
Epoch 2220, val loss: 0.4440458416938782
Epoch 2230, training loss: 451.4739074707031 = 0.34111228585243225 + 50.0 * 9.022655487060547
Epoch 2230, val loss: 0.4434894323348999
Epoch 2240, training loss: 451.5118408203125 = 0.3401470482349396 + 50.0 * 9.023433685302734
Epoch 2240, val loss: 0.4431785047054291
Epoch 2250, training loss: 451.4926452636719 = 0.33918502926826477 + 50.0 * 9.023069381713867
Epoch 2250, val loss: 0.44280242919921875
Epoch 2260, training loss: 451.77130126953125 = 0.3382458984851837 + 50.0 * 9.028660774230957
Epoch 2260, val loss: 0.44226303696632385
Epoch 2270, training loss: 450.3796081542969 = 0.33738669753074646 + 50.0 * 9.000844955444336
Epoch 2270, val loss: 0.44141075015068054
Epoch 2280, training loss: 450.0439758300781 = 0.33646392822265625 + 50.0 * 8.994150161743164
Epoch 2280, val loss: 0.44137412309646606
Epoch 2290, training loss: 449.8341979980469 = 0.3356409966945648 + 50.0 * 8.989971160888672
Epoch 2290, val loss: 0.4410058557987213
Epoch 2300, training loss: 450.1490783691406 = 0.33473336696624756 + 50.0 * 8.996286392211914
Epoch 2300, val loss: 0.4410421848297119
Epoch 2310, training loss: 450.61322021484375 = 0.3338126838207245 + 50.0 * 9.00558853149414
Epoch 2310, val loss: 0.4406745731830597
Epoch 2320, training loss: 450.99609375 = 0.33281686902046204 + 50.0 * 9.013265609741211
Epoch 2320, val loss: 0.44020766019821167
Epoch 2330, training loss: 451.3006286621094 = 0.3318406641483307 + 50.0 * 9.019375801086426
Epoch 2330, val loss: 0.43994590640068054
Epoch 2340, training loss: 451.12115478515625 = 0.3308280110359192 + 50.0 * 9.015806198120117
Epoch 2340, val loss: 0.4395465552806854
Epoch 2350, training loss: 451.3681640625 = 0.3298541009426117 + 50.0 * 9.020766258239746
Epoch 2350, val loss: 0.43938004970550537
Epoch 2360, training loss: 451.4820556640625 = 0.3288651406764984 + 50.0 * 9.023063659667969
Epoch 2360, val loss: 0.43886879086494446
Epoch 2370, training loss: 451.66033935546875 = 0.32789385318756104 + 50.0 * 9.026649475097656
Epoch 2370, val loss: 0.43850916624069214
Epoch 2380, training loss: 451.7988586425781 = 0.32691648602485657 + 50.0 * 9.029438972473145
Epoch 2380, val loss: 0.43834492564201355
Epoch 2390, training loss: 451.90338134765625 = 0.32596340775489807 + 50.0 * 9.031548500061035
Epoch 2390, val loss: 0.43788036704063416
Epoch 2400, training loss: 451.7554016113281 = 0.3249928057193756 + 50.0 * 9.028608322143555
Epoch 2400, val loss: 0.43768370151519775
Epoch 2410, training loss: 451.89154052734375 = 0.32403576374053955 + 50.0 * 9.031350135803223
Epoch 2410, val loss: 0.4373660683631897
Epoch 2420, training loss: 452.0876159667969 = 0.32308849692344666 + 50.0 * 9.035290718078613
Epoch 2420, val loss: 0.43706294894218445
Epoch 2430, training loss: 451.8819580078125 = 0.32217487692832947 + 50.0 * 9.031195640563965
Epoch 2430, val loss: 0.4364214539527893
Epoch 2440, training loss: 451.9344787597656 = 0.3212644159793854 + 50.0 * 9.032264709472656
Epoch 2440, val loss: 0.4363223612308502
Epoch 2450, training loss: 452.2227783203125 = 0.32033276557922363 + 50.0 * 9.03804874420166
Epoch 2450, val loss: 0.4360922574996948
Epoch 2460, training loss: 452.3709411621094 = 0.3193853497505188 + 50.0 * 9.041030883789062
Epoch 2460, val loss: 0.4357925355434418
Epoch 2470, training loss: 452.2518615722656 = 0.3184073865413666 + 50.0 * 9.038668632507324
Epoch 2470, val loss: 0.4354396164417267
Epoch 2480, training loss: 452.2609558105469 = 0.317461758852005 + 50.0 * 9.038869857788086
Epoch 2480, val loss: 0.43514829874038696
Epoch 2490, training loss: 452.3349914550781 = 0.3165210485458374 + 50.0 * 9.040369033813477
Epoch 2490, val loss: 0.4347839951515198
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8410144927536232
0.8645222053176846
=== training gcn model ===
Epoch 0, training loss: 512.3849487304688 = 1.1177371740341187 + 50.0 * 10.225343704223633
Epoch 0, val loss: 1.119358777999878
Epoch 10, training loss: 489.8489685058594 = 1.1130872964859009 + 50.0 * 9.774717330932617
Epoch 10, val loss: 1.1147410869598389
Epoch 20, training loss: 477.86505126953125 = 1.1087106466293335 + 50.0 * 9.535126686096191
Epoch 20, val loss: 1.1103699207305908
Epoch 30, training loss: 469.9600524902344 = 1.1045819520950317 + 50.0 * 9.37710952758789
Epoch 30, val loss: 1.1062372922897339
Epoch 40, training loss: 464.1130676269531 = 1.1006463766098022 + 50.0 * 9.260248184204102
Epoch 40, val loss: 1.1023024320602417
Epoch 50, training loss: 459.6219482421875 = 1.0969254970550537 + 50.0 * 9.170500755310059
Epoch 50, val loss: 1.0985795259475708
Epoch 60, training loss: 456.03240966796875 = 1.0933932065963745 + 50.0 * 9.098780632019043
Epoch 60, val loss: 1.0950440168380737
Epoch 70, training loss: 453.0953674316406 = 1.0900259017944336 + 50.0 * 9.040106773376465
Epoch 70, val loss: 1.091671109199524
Epoch 80, training loss: 450.61865234375 = 1.0868127346038818 + 50.0 * 8.990636825561523
Epoch 80, val loss: 1.0884504318237305
Epoch 90, training loss: 448.4975280761719 = 1.0837105512619019 + 50.0 * 8.94827651977539
Epoch 90, val loss: 1.085343360900879
Epoch 100, training loss: 446.5976257324219 = 1.080751895904541 + 50.0 * 8.910337448120117
Epoch 100, val loss: 1.0823849439620972
Epoch 110, training loss: 445.0399475097656 = 1.077898383140564 + 50.0 * 8.879240989685059
Epoch 110, val loss: 1.0795148611068726
Epoch 120, training loss: 443.6205139160156 = 1.0751285552978516 + 50.0 * 8.850907325744629
Epoch 120, val loss: 1.0767388343811035
Epoch 130, training loss: 442.38323974609375 = 1.0724574327468872 + 50.0 * 8.826215744018555
Epoch 130, val loss: 1.074051856994629
Epoch 140, training loss: 441.3041076660156 = 1.0698810815811157 + 50.0 * 8.80468463897705
Epoch 140, val loss: 1.0714668035507202
Epoch 150, training loss: 440.4366455078125 = 1.0673702955245972 + 50.0 * 8.787384986877441
Epoch 150, val loss: 1.0689464807510376
Epoch 160, training loss: 439.60235595703125 = 1.0648969411849976 + 50.0 * 8.77074909210205
Epoch 160, val loss: 1.0664629936218262
Epoch 170, training loss: 438.8743896484375 = 1.0625195503234863 + 50.0 * 8.756237030029297
Epoch 170, val loss: 1.0640783309936523
Epoch 180, training loss: 438.2987365722656 = 1.0602394342422485 + 50.0 * 8.744770050048828
Epoch 180, val loss: 1.0617971420288086
Epoch 190, training loss: 437.6026916503906 = 1.05795419216156 + 50.0 * 8.730895042419434
Epoch 190, val loss: 1.0594899654388428
Epoch 200, training loss: 437.1396789550781 = 1.0557526350021362 + 50.0 * 8.721678733825684
Epoch 200, val loss: 1.0572699308395386
Epoch 210, training loss: 436.73828125 = 1.0535591840744019 + 50.0 * 8.71369457244873
Epoch 210, val loss: 1.0550826787948608
Epoch 220, training loss: 436.462646484375 = 1.0514206886291504 + 50.0 * 8.708224296569824
Epoch 220, val loss: 1.052918791770935
Epoch 230, training loss: 436.02130126953125 = 1.0492486953735352 + 50.0 * 8.699440956115723
Epoch 230, val loss: 1.0507712364196777
Epoch 240, training loss: 435.6585388183594 = 1.0470929145812988 + 50.0 * 8.692229270935059
Epoch 240, val loss: 1.048594355583191
Epoch 250, training loss: 435.4479675292969 = 1.0449131727218628 + 50.0 * 8.688060760498047
Epoch 250, val loss: 1.046398639678955
Epoch 260, training loss: 435.2752685546875 = 1.0426599979400635 + 50.0 * 8.684652328491211
Epoch 260, val loss: 1.0441572666168213
Epoch 270, training loss: 435.27227783203125 = 1.0405724048614502 + 50.0 * 8.6846342086792
Epoch 270, val loss: 1.042047142982483
Epoch 280, training loss: 434.8273010253906 = 1.038226842880249 + 50.0 * 8.67578125
Epoch 280, val loss: 1.0397249460220337
Epoch 290, training loss: 434.873291015625 = 1.035817265510559 + 50.0 * 8.676749229431152
Epoch 290, val loss: 1.0373376607894897
Epoch 300, training loss: 434.4272766113281 = 1.0331802368164062 + 50.0 * 8.667881965637207
Epoch 300, val loss: 1.034753680229187
Epoch 310, training loss: 434.6356201171875 = 1.0307023525238037 + 50.0 * 8.672098159790039
Epoch 310, val loss: 1.0322201251983643
Epoch 320, training loss: 434.4223937988281 = 1.0279160737991333 + 50.0 * 8.667889595031738
Epoch 320, val loss: 1.02949059009552
Epoch 330, training loss: 434.42877197265625 = 1.0250414609909058 + 50.0 * 8.668074607849121
Epoch 330, val loss: 1.0266309976577759
Epoch 340, training loss: 434.32061767578125 = 1.0218583345413208 + 50.0 * 8.665975570678711
Epoch 340, val loss: 1.0235040187835693
Epoch 350, training loss: 434.3614196777344 = 1.0185742378234863 + 50.0 * 8.66685676574707
Epoch 350, val loss: 1.0202542543411255
Epoch 360, training loss: 434.441650390625 = 1.015137791633606 + 50.0 * 8.668530464172363
Epoch 360, val loss: 1.0168530941009521
Epoch 370, training loss: 434.533935546875 = 1.0113816261291504 + 50.0 * 8.670451164245605
Epoch 370, val loss: 1.0131809711456299
Epoch 380, training loss: 434.5349426269531 = 1.007419466972351 + 50.0 * 8.670550346374512
Epoch 380, val loss: 1.0093072652816772
Epoch 390, training loss: 434.56658935546875 = 1.0031901597976685 + 50.0 * 8.67126750946045
Epoch 390, val loss: 1.0051496028900146
Epoch 400, training loss: 434.46484375 = 0.9986470937728882 + 50.0 * 8.669323921203613
Epoch 400, val loss: 1.0007362365722656
Epoch 410, training loss: 434.3802795410156 = 0.9938020706176758 + 50.0 * 8.667729377746582
Epoch 410, val loss: 0.9959635734558105
Epoch 420, training loss: 434.4107360839844 = 0.9888836741447449 + 50.0 * 8.668437004089355
Epoch 420, val loss: 0.991133451461792
Epoch 430, training loss: 434.6380615234375 = 0.9837649464607239 + 50.0 * 8.673086166381836
Epoch 430, val loss: 0.9860594868659973
Epoch 440, training loss: 434.4913635253906 = 0.9781318306922913 + 50.0 * 8.670265197753906
Epoch 440, val loss: 0.9806514382362366
Epoch 450, training loss: 434.68304443359375 = 0.972385585308075 + 50.0 * 8.674213409423828
Epoch 450, val loss: 0.9749364256858826
Epoch 460, training loss: 434.83587646484375 = 0.9663175344467163 + 50.0 * 8.677391052246094
Epoch 460, val loss: 0.9690422415733337
Epoch 470, training loss: 434.8419189453125 = 0.9599595069885254 + 50.0 * 8.67763900756836
Epoch 470, val loss: 0.9627739191055298
Epoch 480, training loss: 434.8741760253906 = 0.9533048272132874 + 50.0 * 8.678417205810547
Epoch 480, val loss: 0.9562976360321045
Epoch 490, training loss: 434.9896545410156 = 0.9462464451789856 + 50.0 * 8.680868148803711
Epoch 490, val loss: 0.9494563341140747
Epoch 500, training loss: 435.03955078125 = 0.9391042590141296 + 50.0 * 8.682008743286133
Epoch 500, val loss: 0.9423524141311646
Epoch 510, training loss: 435.05804443359375 = 0.9319993257522583 + 50.0 * 8.682520866394043
Epoch 510, val loss: 0.9352803230285645
Epoch 520, training loss: 435.005126953125 = 0.9245347380638123 + 50.0 * 8.681612014770508
Epoch 520, val loss: 0.9279959201812744
Epoch 530, training loss: 435.0805969238281 = 0.9168637990951538 + 50.0 * 8.683274269104004
Epoch 530, val loss: 0.920461893081665
Epoch 540, training loss: 435.2459411621094 = 0.9089921116828918 + 50.0 * 8.686738967895508
Epoch 540, val loss: 0.9127760529518127
Epoch 550, training loss: 435.3236999511719 = 0.901037871837616 + 50.0 * 8.688453674316406
Epoch 550, val loss: 0.9049720168113708
Epoch 560, training loss: 435.44293212890625 = 0.8928200006484985 + 50.0 * 8.691001892089844
Epoch 560, val loss: 0.8968508839607239
Epoch 570, training loss: 435.64666748046875 = 0.8844490051269531 + 50.0 * 8.695243835449219
Epoch 570, val loss: 0.8887118101119995
Epoch 580, training loss: 435.774658203125 = 0.8761351704597473 + 50.0 * 8.697970390319824
Epoch 580, val loss: 0.8805586099624634
Epoch 590, training loss: 435.3397521972656 = 0.8670238256454468 + 50.0 * 8.689454078674316
Epoch 590, val loss: 0.8714615702629089
Epoch 600, training loss: 435.2942199707031 = 0.858506441116333 + 50.0 * 8.688714027404785
Epoch 600, val loss: 0.8629800081253052
Epoch 610, training loss: 440.7254943847656 = 0.8505102396011353 + 50.0 * 8.797499656677246
Epoch 610, val loss: 0.8545430898666382
Epoch 620, training loss: 434.34173583984375 = 0.8366841673851013 + 50.0 * 8.670101165771484
Epoch 620, val loss: 0.8420860767364502
Epoch 630, training loss: 435.6308288574219 = 0.8316856622695923 + 50.0 * 8.695982933044434
Epoch 630, val loss: 0.8370605111122131
Epoch 640, training loss: 435.21136474609375 = 0.82379549741745 + 50.0 * 8.687751770019531
Epoch 640, val loss: 0.8289013504981995
Epoch 650, training loss: 435.48046875 = 0.8165565133094788 + 50.0 * 8.693278312683105
Epoch 650, val loss: 0.8218600153923035
Epoch 660, training loss: 433.7528076171875 = 0.8067026138305664 + 50.0 * 8.65892219543457
Epoch 660, val loss: 0.8119660019874573
Epoch 670, training loss: 435.6249084472656 = 0.8000354766845703 + 50.0 * 8.696496963500977
Epoch 670, val loss: 0.8056858777999878
Epoch 680, training loss: 434.8918762207031 = 0.7907065153121948 + 50.0 * 8.682023048400879
Epoch 680, val loss: 0.7964870929718018
Epoch 690, training loss: 435.6708984375 = 0.7828694581985474 + 50.0 * 8.697760581970215
Epoch 690, val loss: 0.7888274192810059
Epoch 700, training loss: 435.5923767089844 = 0.774295449256897 + 50.0 * 8.696361541748047
Epoch 700, val loss: 0.780544102191925
Epoch 710, training loss: 435.8465881347656 = 0.7659380435943604 + 50.0 * 8.701613426208496
Epoch 710, val loss: 0.7724429965019226
Epoch 720, training loss: 435.93585205078125 = 0.7575798630714417 + 50.0 * 8.70356559753418
Epoch 720, val loss: 0.7643003463745117
Epoch 730, training loss: 435.8677062988281 = 0.7492320537567139 + 50.0 * 8.702369689941406
Epoch 730, val loss: 0.7562236785888672
Epoch 740, training loss: 436.287353515625 = 0.7412967681884766 + 50.0 * 8.710921287536621
Epoch 740, val loss: 0.7485135197639465
Epoch 750, training loss: 436.5431823730469 = 0.7332338094711304 + 50.0 * 8.716198921203613
Epoch 750, val loss: 0.7407427430152893
Epoch 760, training loss: 436.6059265136719 = 0.725223958492279 + 50.0 * 8.71761417388916
Epoch 760, val loss: 0.7330247759819031
Epoch 770, training loss: 436.7383117675781 = 0.7172305583953857 + 50.0 * 8.72042179107666
Epoch 770, val loss: 0.7253665328025818
Epoch 780, training loss: 436.8329162597656 = 0.7094043493270874 + 50.0 * 8.7224702835083
Epoch 780, val loss: 0.7178073525428772
Epoch 790, training loss: 436.9562072753906 = 0.7016741037368774 + 50.0 * 8.725090980529785
Epoch 790, val loss: 0.7104593515396118
Epoch 800, training loss: 436.9108581542969 = 0.6939269304275513 + 50.0 * 8.72433853149414
Epoch 800, val loss: 0.7031381726264954
Epoch 810, training loss: 437.14056396484375 = 0.6865407824516296 + 50.0 * 8.729080200195312
Epoch 810, val loss: 0.6960461735725403
Epoch 820, training loss: 437.31610107421875 = 0.6791715025901794 + 50.0 * 8.732738494873047
Epoch 820, val loss: 0.6890566349029541
Epoch 830, training loss: 437.5268249511719 = 0.6719921827316284 + 50.0 * 8.737096786499023
Epoch 830, val loss: 0.6823055148124695
Epoch 840, training loss: 437.6875305175781 = 0.6648826599121094 + 50.0 * 8.740452766418457
Epoch 840, val loss: 0.6755889058113098
Epoch 850, training loss: 437.505615234375 = 0.6574304103851318 + 50.0 * 8.736963272094727
Epoch 850, val loss: 0.6686148047447205
Epoch 860, training loss: 437.9468994140625 = 0.6500802040100098 + 50.0 * 8.745936393737793
Epoch 860, val loss: 0.6617812514305115
Epoch 870, training loss: 437.9228210449219 = 0.6439144611358643 + 50.0 * 8.745577812194824
Epoch 870, val loss: 0.6561185717582703
Epoch 880, training loss: 437.5848693847656 = 0.6375985741615295 + 50.0 * 8.738945007324219
Epoch 880, val loss: 0.6502039432525635
Epoch 890, training loss: 438.0071716308594 = 0.6312626004219055 + 50.0 * 8.747518539428711
Epoch 890, val loss: 0.644274890422821
Epoch 900, training loss: 438.0287170410156 = 0.624923050403595 + 50.0 * 8.748076438903809
Epoch 900, val loss: 0.6385605335235596
Epoch 910, training loss: 438.137451171875 = 0.6187763214111328 + 50.0 * 8.750373840332031
Epoch 910, val loss: 0.632893443107605
Epoch 920, training loss: 438.309814453125 = 0.6128032803535461 + 50.0 * 8.75394058227539
Epoch 920, val loss: 0.6274687051773071
Epoch 930, training loss: 438.360595703125 = 0.6067988276481628 + 50.0 * 8.755075454711914
Epoch 930, val loss: 0.6220470666885376
Epoch 940, training loss: 438.12554931640625 = 0.6008989810943604 + 50.0 * 8.750493049621582
Epoch 940, val loss: 0.6166692972183228
Epoch 950, training loss: 438.252197265625 = 0.5952283143997192 + 50.0 * 8.75313949584961
Epoch 950, val loss: 0.6117119193077087
Epoch 960, training loss: 438.4439697265625 = 0.5898615121841431 + 50.0 * 8.757081985473633
Epoch 960, val loss: 0.6068951487541199
Epoch 970, training loss: 438.5548095703125 = 0.5842202305793762 + 50.0 * 8.759411811828613
Epoch 970, val loss: 0.6020432710647583
Epoch 980, training loss: 438.6251525878906 = 0.5788813829421997 + 50.0 * 8.76092529296875
Epoch 980, val loss: 0.5973027944564819
Epoch 990, training loss: 438.6651611328125 = 0.5737093091011047 + 50.0 * 8.761829376220703
Epoch 990, val loss: 0.5928006172180176
Epoch 1000, training loss: 438.8751525878906 = 0.5686640739440918 + 50.0 * 8.766129493713379
Epoch 1000, val loss: 0.5884521007537842
Epoch 1010, training loss: 438.99395751953125 = 0.5636765360832214 + 50.0 * 8.76860523223877
Epoch 1010, val loss: 0.5842200517654419
Epoch 1020, training loss: 439.0333251953125 = 0.5588672757148743 + 50.0 * 8.769489288330078
Epoch 1020, val loss: 0.5801718831062317
Epoch 1030, training loss: 439.1299133300781 = 0.5538990497589111 + 50.0 * 8.771520614624023
Epoch 1030, val loss: 0.5760185122489929
Epoch 1040, training loss: 438.9495544433594 = 0.5494369864463806 + 50.0 * 8.7680025100708
Epoch 1040, val loss: 0.5725218057632446
Epoch 1050, training loss: 439.18426513671875 = 0.545106053352356 + 50.0 * 8.772783279418945
Epoch 1050, val loss: 0.5687402486801147
Epoch 1060, training loss: 439.4037170410156 = 0.5407654047012329 + 50.0 * 8.77725887298584
Epoch 1060, val loss: 0.5653195381164551
Epoch 1070, training loss: 439.4731750488281 = 0.5366308689117432 + 50.0 * 8.778731346130371
Epoch 1070, val loss: 0.5618512034416199
Epoch 1080, training loss: 439.571533203125 = 0.5325085520744324 + 50.0 * 8.780780792236328
Epoch 1080, val loss: 0.5586114525794983
Epoch 1090, training loss: 439.7621154785156 = 0.528552234172821 + 50.0 * 8.78467082977295
Epoch 1090, val loss: 0.5554541945457458
Epoch 1100, training loss: 439.8170471191406 = 0.5247107744216919 + 50.0 * 8.785846710205078
Epoch 1100, val loss: 0.5525404810905457
Epoch 1110, training loss: 439.5968322753906 = 0.5206819772720337 + 50.0 * 8.781522750854492
Epoch 1110, val loss: 0.5493485331535339
Epoch 1120, training loss: 439.7650146484375 = 0.5170246958732605 + 50.0 * 8.78495979309082
Epoch 1120, val loss: 0.5465920567512512
Epoch 1130, training loss: 439.96337890625 = 0.5135519504547119 + 50.0 * 8.788996696472168
Epoch 1130, val loss: 0.5439981818199158
Epoch 1140, training loss: 439.9966735839844 = 0.5101620554924011 + 50.0 * 8.789730072021484
Epoch 1140, val loss: 0.5415661334991455
Epoch 1150, training loss: 440.04931640625 = 0.5069289207458496 + 50.0 * 8.790847778320312
Epoch 1150, val loss: 0.5391074419021606
Epoch 1160, training loss: 440.14324951171875 = 0.5037278532981873 + 50.0 * 8.792790412902832
Epoch 1160, val loss: 0.5367844700813293
Epoch 1170, training loss: 440.3755798339844 = 0.5007176995277405 + 50.0 * 8.797496795654297
Epoch 1170, val loss: 0.5345757007598877
Epoch 1180, training loss: 440.3014221191406 = 0.49763214588165283 + 50.0 * 8.796075820922852
Epoch 1180, val loss: 0.5323330760002136
Epoch 1190, training loss: 440.4820251464844 = 0.49475276470184326 + 50.0 * 8.799745559692383
Epoch 1190, val loss: 0.5304033756256104
Epoch 1200, training loss: 440.4604187011719 = 0.4918944537639618 + 50.0 * 8.799370765686035
Epoch 1200, val loss: 0.5282485485076904
Epoch 1210, training loss: 439.2607116699219 = 0.4883895516395569 + 50.0 * 8.775445938110352
Epoch 1210, val loss: 0.5258471369743347
Epoch 1220, training loss: 440.3411560058594 = 0.48646679520606995 + 50.0 * 8.797093391418457
Epoch 1220, val loss: 0.5245271325111389
Epoch 1230, training loss: 439.4087829589844 = 0.48352277278900146 + 50.0 * 8.778505325317383
Epoch 1230, val loss: 0.5223894715309143
Epoch 1240, training loss: 439.7618103027344 = 0.4812731146812439 + 50.0 * 8.785611152648926
Epoch 1240, val loss: 0.5211154222488403
Epoch 1250, training loss: 440.11029052734375 = 0.47898802161216736 + 50.0 * 8.79262638092041
Epoch 1250, val loss: 0.5194215774536133
Epoch 1260, training loss: 440.57623291015625 = 0.4767204523086548 + 50.0 * 8.801990509033203
Epoch 1260, val loss: 0.5177739262580872
Epoch 1270, training loss: 440.5888977050781 = 0.47429001331329346 + 50.0 * 8.802291870117188
Epoch 1270, val loss: 0.516214907169342
Epoch 1280, training loss: 440.8202819824219 = 0.47207969427108765 + 50.0 * 8.806963920593262
Epoch 1280, val loss: 0.5148226022720337
Epoch 1290, training loss: 441.05419921875 = 0.46990880370140076 + 50.0 * 8.811685562133789
Epoch 1290, val loss: 0.5133433938026428
Epoch 1300, training loss: 441.1083679199219 = 0.46772798895835876 + 50.0 * 8.812812805175781
Epoch 1300, val loss: 0.5118885040283203
Epoch 1310, training loss: 441.2407531738281 = 0.46560585498809814 + 50.0 * 8.815503120422363
Epoch 1310, val loss: 0.5105628967285156
Epoch 1320, training loss: 441.3176574707031 = 0.4635157883167267 + 50.0 * 8.817083358764648
Epoch 1320, val loss: 0.509141206741333
Epoch 1330, training loss: 441.4759521484375 = 0.4615715444087982 + 50.0 * 8.820287704467773
Epoch 1330, val loss: 0.5078951716423035
Epoch 1340, training loss: 441.5975646972656 = 0.4596121907234192 + 50.0 * 8.822758674621582
Epoch 1340, val loss: 0.5066750645637512
Epoch 1350, training loss: 441.48388671875 = 0.4576798975467682 + 50.0 * 8.820524215698242
Epoch 1350, val loss: 0.5053907632827759
Epoch 1360, training loss: 441.48370361328125 = 0.4558141529560089 + 50.0 * 8.820557594299316
Epoch 1360, val loss: 0.5040649175643921
Epoch 1370, training loss: 441.7062683105469 = 0.45399191975593567 + 50.0 * 8.825045585632324
Epoch 1370, val loss: 0.5030298233032227
Epoch 1380, training loss: 441.02496337890625 = 0.45184946060180664 + 50.0 * 8.81146240234375
Epoch 1380, val loss: 0.5018374919891357
Epoch 1390, training loss: 441.4571228027344 = 0.4502745270729065 + 50.0 * 8.820137023925781
Epoch 1390, val loss: 0.5008013248443604
Epoch 1400, training loss: 441.83160400390625 = 0.44860634207725525 + 50.0 * 8.827659606933594
Epoch 1400, val loss: 0.499688059091568
Epoch 1410, training loss: 441.9761657714844 = 0.4469207525253296 + 50.0 * 8.830584526062012
Epoch 1410, val loss: 0.4987435042858124
Epoch 1420, training loss: 442.1745300292969 = 0.4452214539051056 + 50.0 * 8.834586143493652
Epoch 1420, val loss: 0.4975931942462921
Epoch 1430, training loss: 442.1689147949219 = 0.4435838758945465 + 50.0 * 8.83450698852539
Epoch 1430, val loss: 0.49657535552978516
Epoch 1440, training loss: 442.1983337402344 = 0.4419690668582916 + 50.0 * 8.835127830505371
Epoch 1440, val loss: 0.49572044610977173
Epoch 1450, training loss: 442.2570495605469 = 0.4403518736362457 + 50.0 * 8.836334228515625
Epoch 1450, val loss: 0.4946882128715515
Epoch 1460, training loss: 442.3623352050781 = 0.4388127028942108 + 50.0 * 8.838470458984375
Epoch 1460, val loss: 0.493792325258255
Epoch 1470, training loss: 442.3486633300781 = 0.43724945187568665 + 50.0 * 8.838228225708008
Epoch 1470, val loss: 0.49284350872039795
Epoch 1480, training loss: 442.5879211425781 = 0.43570050597190857 + 50.0 * 8.84304428100586
Epoch 1480, val loss: 0.49202826619148254
Epoch 1490, training loss: 442.6806945800781 = 0.43417951464653015 + 50.0 * 8.844930648803711
Epoch 1490, val loss: 0.49119922518730164
Epoch 1500, training loss: 442.7251892089844 = 0.4326844811439514 + 50.0 * 8.845849990844727
Epoch 1500, val loss: 0.4902748763561249
Epoch 1510, training loss: 442.9073791503906 = 0.4312284588813782 + 50.0 * 8.849523544311523
Epoch 1510, val loss: 0.4894815981388092
Epoch 1520, training loss: 442.93890380859375 = 0.4298055171966553 + 50.0 * 8.850181579589844
Epoch 1520, val loss: 0.4886936545372009
Epoch 1530, training loss: 443.04278564453125 = 0.4283883571624756 + 50.0 * 8.852288246154785
Epoch 1530, val loss: 0.4878663122653961
Epoch 1540, training loss: 443.010009765625 = 0.4269292652606964 + 50.0 * 8.851661682128906
Epoch 1540, val loss: 0.4871934652328491
Epoch 1550, training loss: 443.1248474121094 = 0.4255005419254303 + 50.0 * 8.853986740112305
Epoch 1550, val loss: 0.4862929880619049
Epoch 1560, training loss: 443.10540771484375 = 0.4240308403968811 + 50.0 * 8.85362720489502
Epoch 1560, val loss: 0.48557040095329285
Epoch 1570, training loss: 442.23516845703125 = 0.42242443561553955 + 50.0 * 8.836255073547363
Epoch 1570, val loss: 0.4848937690258026
Epoch 1580, training loss: 442.6884765625 = 0.4211421608924866 + 50.0 * 8.845346450805664
Epoch 1580, val loss: 0.48400256037712097
Epoch 1590, training loss: 442.7455139160156 = 0.41973578929901123 + 50.0 * 8.846515655517578
Epoch 1590, val loss: 0.4831141233444214
Epoch 1600, training loss: 443.1019592285156 = 0.41831105947494507 + 50.0 * 8.853672981262207
Epoch 1600, val loss: 0.48232415318489075
Epoch 1610, training loss: 443.3279724121094 = 0.41696736216545105 + 50.0 * 8.858220100402832
Epoch 1610, val loss: 0.48170673847198486
Epoch 1620, training loss: 443.1649475097656 = 0.4154413640499115 + 50.0 * 8.854990005493164
Epoch 1620, val loss: 0.4809228479862213
Epoch 1630, training loss: 443.2015075683594 = 0.4140720069408417 + 50.0 * 8.855749130249023
Epoch 1630, val loss: 0.4802878499031067
Epoch 1640, training loss: 443.5728759765625 = 0.412873238325119 + 50.0 * 8.863200187683105
Epoch 1640, val loss: 0.4792211353778839
Epoch 1650, training loss: 443.2294616699219 = 0.41148918867111206 + 50.0 * 8.856359481811523
Epoch 1650, val loss: 0.47887206077575684
Epoch 1660, training loss: 443.4754333496094 = 0.4101393222808838 + 50.0 * 8.861306190490723
Epoch 1660, val loss: 0.4781888425350189
Epoch 1670, training loss: 443.6441345214844 = 0.40879321098327637 + 50.0 * 8.864706993103027
Epoch 1670, val loss: 0.47755980491638184
Epoch 1680, training loss: 443.9532775878906 = 0.407487154006958 + 50.0 * 8.870915412902832
Epoch 1680, val loss: 0.4766985774040222
Epoch 1690, training loss: 443.81689453125 = 0.40614524483680725 + 50.0 * 8.86821460723877
Epoch 1690, val loss: 0.4759230315685272
Epoch 1700, training loss: 443.6097412109375 = 0.40481284260749817 + 50.0 * 8.86409854888916
Epoch 1700, val loss: 0.4750105142593384
Epoch 1710, training loss: 443.75811767578125 = 0.4035690128803253 + 50.0 * 8.867091178894043
Epoch 1710, val loss: 0.47469422221183777
Epoch 1720, training loss: 443.96630859375 = 0.4023863971233368 + 50.0 * 8.871278762817383
Epoch 1720, val loss: 0.47385987639427185
Epoch 1730, training loss: 444.23516845703125 = 0.4011397659778595 + 50.0 * 8.876680374145508
Epoch 1730, val loss: 0.47347918152809143
Epoch 1740, training loss: 444.2383117675781 = 0.39988216757774353 + 50.0 * 8.876769065856934
Epoch 1740, val loss: 0.4725768268108368
Epoch 1750, training loss: 444.17181396484375 = 0.3985608220100403 + 50.0 * 8.875465393066406
Epoch 1750, val loss: 0.4719279110431671
Epoch 1760, training loss: 443.8326416015625 = 0.3973191976547241 + 50.0 * 8.868706703186035
Epoch 1760, val loss: 0.47121521830558777
Epoch 1770, training loss: 444.3470764160156 = 0.3962843716144562 + 50.0 * 8.879015922546387
Epoch 1770, val loss: 0.47044721245765686
Epoch 1780, training loss: 443.8406982421875 = 0.39500463008880615 + 50.0 * 8.868913650512695
Epoch 1780, val loss: 0.4701182544231415
Epoch 1790, training loss: 443.4173583984375 = 0.3938330113887787 + 50.0 * 8.86047077178955
Epoch 1790, val loss: 0.46930623054504395
Epoch 1800, training loss: 443.11248779296875 = 0.3927208185195923 + 50.0 * 8.854394912719727
Epoch 1800, val loss: 0.46917104721069336
Epoch 1810, training loss: 443.2362365722656 = 0.3914478123188019 + 50.0 * 8.856895446777344
Epoch 1810, val loss: 0.4682306945323944
Epoch 1820, training loss: 443.5115661621094 = 0.39038315415382385 + 50.0 * 8.86242389678955
Epoch 1820, val loss: 0.46802693605422974
Epoch 1830, training loss: 444.1103820800781 = 0.3891828656196594 + 50.0 * 8.87442398071289
Epoch 1830, val loss: 0.46720340847969055
Epoch 1840, training loss: 444.24957275390625 = 0.38799694180488586 + 50.0 * 8.87723159790039
Epoch 1840, val loss: 0.4665710926055908
Epoch 1850, training loss: 444.5140686035156 = 0.38684847950935364 + 50.0 * 8.88254451751709
Epoch 1850, val loss: 0.46603646874427795
Epoch 1860, training loss: 444.6585693359375 = 0.3856661915779114 + 50.0 * 8.885457992553711
Epoch 1860, val loss: 0.46549081802368164
Epoch 1870, training loss: 444.5792236328125 = 0.3844413459300995 + 50.0 * 8.883895874023438
Epoch 1870, val loss: 0.46481752395629883
Epoch 1880, training loss: 444.5798034667969 = 0.3832489550113678 + 50.0 * 8.883931159973145
Epoch 1880, val loss: 0.46425801515579224
Epoch 1890, training loss: 444.8083801269531 = 0.38207775354385376 + 50.0 * 8.88852596282959
Epoch 1890, val loss: 0.4636320173740387
Epoch 1900, training loss: 445.0069274902344 = 0.3809114694595337 + 50.0 * 8.8925199508667
Epoch 1900, val loss: 0.4630650281906128
Epoch 1910, training loss: 444.99365234375 = 0.3797452449798584 + 50.0 * 8.892278671264648
Epoch 1910, val loss: 0.46237748861312866
Epoch 1920, training loss: 445.0481262207031 = 0.37854400277137756 + 50.0 * 8.893391609191895
Epoch 1920, val loss: 0.4618350863456726
Epoch 1930, training loss: 445.0137634277344 = 0.3773863613605499 + 50.0 * 8.892727851867676
Epoch 1930, val loss: 0.4611820876598358
Epoch 1940, training loss: 445.0708312988281 = 0.37625688314437866 + 50.0 * 8.893891334533691
Epoch 1940, val loss: 0.4608462154865265
Epoch 1950, training loss: 445.131591796875 = 0.37511566281318665 + 50.0 * 8.895129203796387
Epoch 1950, val loss: 0.4602786898612976
Epoch 1960, training loss: 445.26983642578125 = 0.3739734888076782 + 50.0 * 8.897917747497559
Epoch 1960, val loss: 0.45971786975860596
Epoch 1970, training loss: 445.4722900390625 = 0.37282219529151917 + 50.0 * 8.901988983154297
Epoch 1970, val loss: 0.45921483635902405
Epoch 1980, training loss: 445.55474853515625 = 0.37168580293655396 + 50.0 * 8.903661727905273
Epoch 1980, val loss: 0.45862144231796265
Epoch 1990, training loss: 445.3222351074219 = 0.37052658200263977 + 50.0 * 8.89903450012207
Epoch 1990, val loss: 0.4582217037677765
Epoch 2000, training loss: 445.3410949707031 = 0.36936724185943604 + 50.0 * 8.899435043334961
Epoch 2000, val loss: 0.4574843645095825
Epoch 2010, training loss: 445.314697265625 = 0.3682103753089905 + 50.0 * 8.898929595947266
Epoch 2010, val loss: 0.45739033818244934
Epoch 2020, training loss: 445.0775146484375 = 0.3670446276664734 + 50.0 * 8.894209861755371
Epoch 2020, val loss: 0.4561748802661896
Epoch 2030, training loss: 445.16973876953125 = 0.36598119139671326 + 50.0 * 8.896075248718262
Epoch 2030, val loss: 0.4558565318584442
Epoch 2040, training loss: 445.5148010253906 = 0.36489376425743103 + 50.0 * 8.902997970581055
Epoch 2040, val loss: 0.45553529262542725
Epoch 2050, training loss: 445.8332214355469 = 0.3637411594390869 + 50.0 * 8.90938949584961
Epoch 2050, val loss: 0.45482951402664185
Epoch 2060, training loss: 445.9098205566406 = 0.3626002371311188 + 50.0 * 8.910943984985352
Epoch 2060, val loss: 0.4542793929576874
Epoch 2070, training loss: 445.8434753417969 = 0.36143651604652405 + 50.0 * 8.909640312194824
Epoch 2070, val loss: 0.4538840353488922
Epoch 2080, training loss: 445.80755615234375 = 0.36026206612586975 + 50.0 * 8.90894603729248
Epoch 2080, val loss: 0.45342856645584106
Epoch 2090, training loss: 446.09136962890625 = 0.3591707944869995 + 50.0 * 8.914644241333008
Epoch 2090, val loss: 0.45272335410118103
Epoch 2100, training loss: 445.6458435058594 = 0.35803866386413574 + 50.0 * 8.905755996704102
Epoch 2100, val loss: 0.45230644941329956
Epoch 2110, training loss: 445.4791259765625 = 0.3568831980228424 + 50.0 * 8.902444839477539
Epoch 2110, val loss: 0.45172202587127686
Epoch 2120, training loss: 445.7442932128906 = 0.3557796776294708 + 50.0 * 8.907770156860352
Epoch 2120, val loss: 0.45137789845466614
Epoch 2130, training loss: 446.0354309082031 = 0.3546811044216156 + 50.0 * 8.913615226745605
Epoch 2130, val loss: 0.45098233222961426
Epoch 2140, training loss: 446.22808837890625 = 0.3535674214363098 + 50.0 * 8.917490005493164
Epoch 2140, val loss: 0.4502517879009247
Epoch 2150, training loss: 446.2649841308594 = 0.3524399399757385 + 50.0 * 8.918251037597656
Epoch 2150, val loss: 0.44986316561698914
Epoch 2160, training loss: 446.259033203125 = 0.35129648447036743 + 50.0 * 8.9181547164917
Epoch 2160, val loss: 0.44937512278556824
Epoch 2170, training loss: 446.409423828125 = 0.3501691222190857 + 50.0 * 8.921185493469238
Epoch 2170, val loss: 0.4487941265106201
Epoch 2180, training loss: 446.55126953125 = 0.349031925201416 + 50.0 * 8.924044609069824
Epoch 2180, val loss: 0.4483056962490082
Epoch 2190, training loss: 446.5357666015625 = 0.34789809584617615 + 50.0 * 8.923757553100586
Epoch 2190, val loss: 0.4478241503238678
Epoch 2200, training loss: 446.5693664550781 = 0.3467671573162079 + 50.0 * 8.92445182800293
Epoch 2200, val loss: 0.44740384817123413
Epoch 2210, training loss: 446.7216796875 = 0.34563639760017395 + 50.0 * 8.927520751953125
Epoch 2210, val loss: 0.44698378443717957
Epoch 2220, training loss: 446.7042236328125 = 0.34448763728141785 + 50.0 * 8.927194595336914
Epoch 2220, val loss: 0.4463697373867035
Epoch 2230, training loss: 446.6739807128906 = 0.34335198998451233 + 50.0 * 8.926612854003906
Epoch 2230, val loss: 0.4460776746273041
Epoch 2240, training loss: 446.8717346191406 = 0.34225261211395264 + 50.0 * 8.93058967590332
Epoch 2240, val loss: 0.4454801082611084
Epoch 2250, training loss: 446.7905578613281 = 0.3410815894603729 + 50.0 * 8.92898941040039
Epoch 2250, val loss: 0.4451943635940552
Epoch 2260, training loss: 446.1080627441406 = 0.3399140238761902 + 50.0 * 8.915363311767578
Epoch 2260, val loss: 0.4447902739048004
Epoch 2270, training loss: 446.0437316894531 = 0.3389015793800354 + 50.0 * 8.91409683227539
Epoch 2270, val loss: 0.44405698776245117
Epoch 2280, training loss: 446.4779968261719 = 0.33781254291534424 + 50.0 * 8.92280387878418
Epoch 2280, val loss: 0.4440787434577942
Epoch 2290, training loss: 445.3982849121094 = 0.336635023355484 + 50.0 * 8.901232719421387
Epoch 2290, val loss: 0.44372862577438354
Epoch 2300, training loss: 444.6387023925781 = 0.3354929983615875 + 50.0 * 8.886064529418945
Epoch 2300, val loss: 0.44245246052742004
Epoch 2310, training loss: 444.9568786621094 = 0.3344891369342804 + 50.0 * 8.892447471618652
Epoch 2310, val loss: 0.44304221868515015
Epoch 2320, training loss: 445.7682800292969 = 0.33344408869743347 + 50.0 * 8.908697128295898
Epoch 2320, val loss: 0.44280844926834106
Epoch 2330, training loss: 446.15228271484375 = 0.3323696255683899 + 50.0 * 8.916398048400879
Epoch 2330, val loss: 0.4419022798538208
Epoch 2340, training loss: 446.4379577636719 = 0.3312458097934723 + 50.0 * 8.922134399414062
Epoch 2340, val loss: 0.4422028064727783
Epoch 2350, training loss: 446.7205810546875 = 0.3301291763782501 + 50.0 * 8.92780876159668
Epoch 2350, val loss: 0.4414442777633667
Epoch 2360, training loss: 446.7639465332031 = 0.32900747656822205 + 50.0 * 8.928698539733887
Epoch 2360, val loss: 0.4413272440433502
Epoch 2370, training loss: 446.87139892578125 = 0.3278774321079254 + 50.0 * 8.930870056152344
Epoch 2370, val loss: 0.440883606672287
Epoch 2380, training loss: 446.9129943847656 = 0.3267655670642853 + 50.0 * 8.931724548339844
Epoch 2380, val loss: 0.4404594898223877
Epoch 2390, training loss: 446.88507080078125 = 0.32564276456832886 + 50.0 * 8.931188583374023
Epoch 2390, val loss: 0.44023197889328003
Epoch 2400, training loss: 446.8768615722656 = 0.3245229423046112 + 50.0 * 8.931046485900879
Epoch 2400, val loss: 0.43996015191078186
Epoch 2410, training loss: 447.0709228515625 = 0.32342904806137085 + 50.0 * 8.93494987487793
Epoch 2410, val loss: 0.4396369159221649
Epoch 2420, training loss: 447.081787109375 = 0.3223136067390442 + 50.0 * 8.935189247131348
Epoch 2420, val loss: 0.4394153654575348
Epoch 2430, training loss: 447.2441101074219 = 0.32121893763542175 + 50.0 * 8.938457489013672
Epoch 2430, val loss: 0.4390946328639984
Epoch 2440, training loss: 447.2796630859375 = 0.32012268900871277 + 50.0 * 8.939190864562988
Epoch 2440, val loss: 0.43874841928482056
Epoch 2450, training loss: 447.2829895019531 = 0.31900647282600403 + 50.0 * 8.939279556274414
Epoch 2450, val loss: 0.43849948048591614
Epoch 2460, training loss: 447.4189453125 = 0.31792548298835754 + 50.0 * 8.942020416259766
Epoch 2460, val loss: 0.43818017840385437
Epoch 2470, training loss: 445.5486145019531 = 0.31669872999191284 + 50.0 * 8.904638290405273
Epoch 2470, val loss: 0.43848198652267456
Epoch 2480, training loss: 445.7853698730469 = 0.31593868136405945 + 50.0 * 8.909388542175293
Epoch 2480, val loss: 0.4382154047489166
Epoch 2490, training loss: 445.4707946777344 = 0.3149297833442688 + 50.0 * 8.903117179870605
Epoch 2490, val loss: 0.4379722774028778
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8363768115942029
0.865391581540245
The final CL Acc:0.83488, 0.00572, The final GNN Acc:0.86409, 0.00128
Begin epxeriment: noisy_level: 0.2 cont_weight: 50 epoch: der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106286])
remove edge: torch.Size([2, 70798])
updated graph: torch.Size([2, 88436])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 509.569580078125 = 1.0916974544525146 + 50.0 * 10.169557571411133
Epoch 0, val loss: 1.0905600786209106
Epoch 10, training loss: 484.76300048828125 = 1.0884766578674316 + 50.0 * 9.673490524291992
Epoch 10, val loss: 1.0873913764953613
Epoch 20, training loss: 476.7197570800781 = 1.0854988098144531 + 50.0 * 9.51268482208252
Epoch 20, val loss: 1.0844480991363525
Epoch 30, training loss: 470.7998046875 = 1.0826596021652222 + 50.0 * 9.394342422485352
Epoch 30, val loss: 1.0816497802734375
Epoch 40, training loss: 466.09979248046875 = 1.079998254776001 + 50.0 * 9.300395965576172
Epoch 40, val loss: 1.0790390968322754
Epoch 50, training loss: 462.2156677246094 = 1.077510952949524 + 50.0 * 9.222763061523438
Epoch 50, val loss: 1.0766041278839111
Epoch 60, training loss: 458.8950500488281 = 1.0751688480377197 + 50.0 * 9.156397819519043
Epoch 60, val loss: 1.0743157863616943
Epoch 70, training loss: 456.03277587890625 = 1.07296621799469 + 50.0 * 9.099196434020996
Epoch 70, val loss: 1.0721724033355713
Epoch 80, training loss: 453.5646667480469 = 1.0708767175674438 + 50.0 * 9.04987621307373
Epoch 80, val loss: 1.0701448917388916
Epoch 90, training loss: 451.5089416503906 = 1.0688841342926025 + 50.0 * 9.008801460266113
Epoch 90, val loss: 1.068224549293518
Epoch 100, training loss: 449.697998046875 = 1.066968560218811 + 50.0 * 8.972620964050293
Epoch 100, val loss: 1.0663851499557495
Epoch 110, training loss: 448.0766296386719 = 1.0651427507400513 + 50.0 * 8.940229415893555
Epoch 110, val loss: 1.0646376609802246
Epoch 120, training loss: 446.72412109375 = 1.0633786916732788 + 50.0 * 8.913214683532715
Epoch 120, val loss: 1.062963604927063
Epoch 130, training loss: 445.537841796875 = 1.0616687536239624 + 50.0 * 8.88952350616455
Epoch 130, val loss: 1.0613446235656738
Epoch 140, training loss: 444.490966796875 = 1.0600224733352661 + 50.0 * 8.868618965148926
Epoch 140, val loss: 1.0597941875457764
Epoch 150, training loss: 443.5552062988281 = 1.0583899021148682 + 50.0 * 8.849936485290527
Epoch 150, val loss: 1.0582524538040161
Epoch 160, training loss: 442.79193115234375 = 1.0568000078201294 + 50.0 * 8.834702491760254
Epoch 160, val loss: 1.056768536567688
Epoch 170, training loss: 441.9903259277344 = 1.055205225944519 + 50.0 * 8.818702697753906
Epoch 170, val loss: 1.0552895069122314
Epoch 180, training loss: 441.3133850097656 = 1.0536611080169678 + 50.0 * 8.805194854736328
Epoch 180, val loss: 1.0538341999053955
Epoch 190, training loss: 440.7361755371094 = 1.0521243810653687 + 50.0 * 8.793681144714355
Epoch 190, val loss: 1.05239999294281
Epoch 200, training loss: 440.27337646484375 = 1.0505640506744385 + 50.0 * 8.784456253051758
Epoch 200, val loss: 1.0509092807769775
Epoch 210, training loss: 439.841796875 = 1.0490223169326782 + 50.0 * 8.775856018066406
Epoch 210, val loss: 1.0494900941848755
Epoch 220, training loss: 439.37847900390625 = 1.047398328781128 + 50.0 * 8.766621589660645
Epoch 220, val loss: 1.0479706525802612
Epoch 230, training loss: 438.9901428222656 = 1.0457347631454468 + 50.0 * 8.758888244628906
Epoch 230, val loss: 1.0463707447052002
Epoch 240, training loss: 438.59759521484375 = 1.0439366102218628 + 50.0 * 8.751072883605957
Epoch 240, val loss: 1.0446914434432983
Epoch 250, training loss: 438.3617248535156 = 1.0421350002288818 + 50.0 * 8.746391296386719
Epoch 250, val loss: 1.0429651737213135
Epoch 260, training loss: 438.2682800292969 = 1.0402158498764038 + 50.0 * 8.744561195373535
Epoch 260, val loss: 1.0411714315414429
Epoch 270, training loss: 438.0875549316406 = 1.0381628274917603 + 50.0 * 8.740987777709961
Epoch 270, val loss: 1.0392180681228638
Epoch 280, training loss: 437.804443359375 = 1.0359516143798828 + 50.0 * 8.735369682312012
Epoch 280, val loss: 1.0371071100234985
Epoch 290, training loss: 437.5329284667969 = 1.0334733724594116 + 50.0 * 8.729989051818848
Epoch 290, val loss: 1.0347155332565308
Epoch 300, training loss: 437.4986572265625 = 1.0310063362121582 + 50.0 * 8.729352951049805
Epoch 300, val loss: 1.0323162078857422
Epoch 310, training loss: 437.3525695800781 = 1.0284003019332886 + 50.0 * 8.726483345031738
Epoch 310, val loss: 1.0298219919204712
Epoch 320, training loss: 437.1772766113281 = 1.0254933834075928 + 50.0 * 8.72303581237793
Epoch 320, val loss: 1.0270421504974365
Epoch 330, training loss: 437.08270263671875 = 1.0223764181137085 + 50.0 * 8.721206665039062
Epoch 330, val loss: 1.0240188837051392
Epoch 340, training loss: 437.0362243652344 = 1.019042730331421 + 50.0 * 8.720343589782715
Epoch 340, val loss: 1.0208474397659302
Epoch 350, training loss: 437.16595458984375 = 1.015501856803894 + 50.0 * 8.72300910949707
Epoch 350, val loss: 1.017414927482605
Epoch 360, training loss: 437.16851806640625 = 1.0117151737213135 + 50.0 * 8.723135948181152
Epoch 360, val loss: 1.013700246810913
Epoch 370, training loss: 437.1961364746094 = 1.007860779762268 + 50.0 * 8.72376537322998
Epoch 370, val loss: 1.010034441947937
Epoch 380, training loss: 436.8626403808594 = 1.003478765487671 + 50.0 * 8.717183113098145
Epoch 380, val loss: 1.0058525800704956
Epoch 390, training loss: 436.858154296875 = 0.9991676807403564 + 50.0 * 8.717179298400879
Epoch 390, val loss: 1.0016539096832275
Epoch 400, training loss: 437.01092529296875 = 0.9946631789207458 + 50.0 * 8.720325469970703
Epoch 400, val loss: 0.9973269701004028
Epoch 410, training loss: 437.0447998046875 = 0.9898679852485657 + 50.0 * 8.721098899841309
Epoch 410, val loss: 0.9927017092704773
Epoch 420, training loss: 437.0384521484375 = 0.9848268032073975 + 50.0 * 8.72107219696045
Epoch 420, val loss: 0.9879035353660583
Epoch 430, training loss: 437.01409912109375 = 0.9796119332313538 + 50.0 * 8.72068977355957
Epoch 430, val loss: 0.9828005433082581
Epoch 440, training loss: 437.1831359863281 = 0.9742759466171265 + 50.0 * 8.724177360534668
Epoch 440, val loss: 0.9777408838272095
Epoch 450, training loss: 437.13232421875 = 0.9687055945396423 + 50.0 * 8.723272323608398
Epoch 450, val loss: 0.9724262356758118
Epoch 460, training loss: 437.1418762207031 = 0.9630395174026489 + 50.0 * 8.723576545715332
Epoch 460, val loss: 0.9669918417930603
Epoch 470, training loss: 437.119384765625 = 0.9572188258171082 + 50.0 * 8.723243713378906
Epoch 470, val loss: 0.9613677263259888
Epoch 480, training loss: 437.04095458984375 = 0.9511559009552002 + 50.0 * 8.721796035766602
Epoch 480, val loss: 0.9557236433029175
Epoch 490, training loss: 436.9942932128906 = 0.9451528191566467 + 50.0 * 8.720982551574707
Epoch 490, val loss: 0.9501005411148071
Epoch 500, training loss: 437.1790466308594 = 0.9392317533493042 + 50.0 * 8.724796295166016
Epoch 500, val loss: 0.9443929195404053
Epoch 510, training loss: 437.0817565917969 = 0.9331932663917542 + 50.0 * 8.722970962524414
Epoch 510, val loss: 0.938689649105072
Epoch 520, training loss: 437.6451110839844 = 0.926681637763977 + 50.0 * 8.734368324279785
Epoch 520, val loss: 0.9329046607017517
Epoch 530, training loss: 437.6160583496094 = 0.9209509491920471 + 50.0 * 8.733901977539062
Epoch 530, val loss: 0.9273267984390259
Epoch 540, training loss: 437.6333312988281 = 0.9151120185852051 + 50.0 * 8.73436450958252
Epoch 540, val loss: 0.9217385053634644
Epoch 550, training loss: 437.6829528808594 = 0.9092008471488953 + 50.0 * 8.735474586486816
Epoch 550, val loss: 0.9162532091140747
Epoch 560, training loss: 437.67950439453125 = 0.9033752083778381 + 50.0 * 8.735522270202637
Epoch 560, val loss: 0.910913348197937
Epoch 570, training loss: 437.9879150390625 = 0.8979385495185852 + 50.0 * 8.741799354553223
Epoch 570, val loss: 0.9058799743652344
Epoch 580, training loss: 438.15325927734375 = 0.8923109769821167 + 50.0 * 8.745219230651855
Epoch 580, val loss: 0.9007203578948975
Epoch 590, training loss: 438.24041748046875 = 0.8868285417556763 + 50.0 * 8.747071266174316
Epoch 590, val loss: 0.8956758379936218
Epoch 600, training loss: 438.3687744140625 = 0.8814992904663086 + 50.0 * 8.74974536895752
Epoch 600, val loss: 0.8908073306083679
Epoch 610, training loss: 438.3827209472656 = 0.876261830329895 + 50.0 * 8.750129699707031
Epoch 610, val loss: 0.8860065937042236
Epoch 620, training loss: 438.3825988769531 = 0.8712212443351746 + 50.0 * 8.750227928161621
Epoch 620, val loss: 0.8813897371292114
Epoch 630, training loss: 438.5653076171875 = 0.8663256764411926 + 50.0 * 8.753979682922363
Epoch 630, val loss: 0.8770161867141724
Epoch 640, training loss: 438.69061279296875 = 0.8613937497138977 + 50.0 * 8.756584167480469
Epoch 640, val loss: 0.872530460357666
Epoch 650, training loss: 438.6347961425781 = 0.8569418787956238 + 50.0 * 8.7555570602417
Epoch 650, val loss: 0.8685709238052368
Epoch 660, training loss: 438.7027893066406 = 0.8525038361549377 + 50.0 * 8.75700569152832
Epoch 660, val loss: 0.864613950252533
Epoch 670, training loss: 438.8439025878906 = 0.8481297492980957 + 50.0 * 8.759915351867676
Epoch 670, val loss: 0.8607615828514099
Epoch 680, training loss: 438.874755859375 = 0.8439104557037354 + 50.0 * 8.76061725616455
Epoch 680, val loss: 0.8570204377174377
Epoch 690, training loss: 439.0218200683594 = 0.839802622795105 + 50.0 * 8.763640403747559
Epoch 690, val loss: 0.8533822298049927
Epoch 700, training loss: 439.0787353515625 = 0.8357025384902954 + 50.0 * 8.764861106872559
Epoch 700, val loss: 0.8497551679611206
Epoch 710, training loss: 439.2261047363281 = 0.8316914439201355 + 50.0 * 8.767888069152832
Epoch 710, val loss: 0.8462076783180237
Epoch 720, training loss: 439.2366027832031 = 0.8278378248214722 + 50.0 * 8.76817512512207
Epoch 720, val loss: 0.8427783846855164
Epoch 730, training loss: 439.40478515625 = 0.8240963220596313 + 50.0 * 8.771614074707031
Epoch 730, val loss: 0.8395038843154907
Epoch 740, training loss: 439.39739990234375 = 0.8203186988830566 + 50.0 * 8.771541595458984
Epoch 740, val loss: 0.8361456990242004
Epoch 750, training loss: 439.3575744628906 = 0.8166854977607727 + 50.0 * 8.770817756652832
Epoch 750, val loss: 0.8329519033432007
Epoch 760, training loss: 439.4744873046875 = 0.813047468662262 + 50.0 * 8.773228645324707
Epoch 760, val loss: 0.8297616243362427
Epoch 770, training loss: 439.5090026855469 = 0.8093973994255066 + 50.0 * 8.773992538452148
Epoch 770, val loss: 0.8265573978424072
Epoch 780, training loss: 439.5234375 = 0.8058443665504456 + 50.0 * 8.774352073669434
Epoch 780, val loss: 0.8234780430793762
Epoch 790, training loss: 439.6720886230469 = 0.8023487329483032 + 50.0 * 8.777395248413086
Epoch 790, val loss: 0.8203331828117371
Epoch 800, training loss: 439.748046875 = 0.7987827062606812 + 50.0 * 8.778985023498535
Epoch 800, val loss: 0.8171857595443726
Epoch 810, training loss: 439.8506164550781 = 0.7952203154563904 + 50.0 * 8.781107902526855
Epoch 810, val loss: 0.814041256904602
Epoch 820, training loss: 439.909423828125 = 0.7916373014450073 + 50.0 * 8.782356262207031
Epoch 820, val loss: 0.8109066486358643
Epoch 830, training loss: 439.7760925292969 = 0.7879976034164429 + 50.0 * 8.779762268066406
Epoch 830, val loss: 0.8077254891395569
Epoch 840, training loss: 440.3934326171875 = 0.7844322919845581 + 50.0 * 8.792180061340332
Epoch 840, val loss: 0.8045353293418884
Epoch 850, training loss: 439.8204345703125 = 0.7807369232177734 + 50.0 * 8.780794143676758
Epoch 850, val loss: 0.8013026714324951
Epoch 860, training loss: 440.0172119140625 = 0.7771965265274048 + 50.0 * 8.78480052947998
Epoch 860, val loss: 0.7981146574020386
Epoch 870, training loss: 440.1910400390625 = 0.7736266255378723 + 50.0 * 8.788348197937012
Epoch 870, val loss: 0.7948572635650635
Epoch 880, training loss: 440.4374084472656 = 0.769968569278717 + 50.0 * 8.793349266052246
Epoch 880, val loss: 0.7915377020835876
Epoch 890, training loss: 440.40740966796875 = 0.766094446182251 + 50.0 * 8.792826652526855
Epoch 890, val loss: 0.7880433797836304
Epoch 900, training loss: 440.4754638671875 = 0.7622728943824768 + 50.0 * 8.79426383972168
Epoch 900, val loss: 0.7846572399139404
Epoch 910, training loss: 440.6275634765625 = 0.7583113312721252 + 50.0 * 8.797385215759277
Epoch 910, val loss: 0.7810674905776978
Epoch 920, training loss: 440.8653564453125 = 0.7544795870780945 + 50.0 * 8.802217483520508
Epoch 920, val loss: 0.7776371836662292
Epoch 930, training loss: 440.85369873046875 = 0.7505406141281128 + 50.0 * 8.80206298828125
Epoch 930, val loss: 0.7739441990852356
Epoch 940, training loss: 440.58843994140625 = 0.7462021112442017 + 50.0 * 8.796844482421875
Epoch 940, val loss: 0.7702966332435608
Epoch 950, training loss: 440.8721618652344 = 0.7421243786811829 + 50.0 * 8.802600860595703
Epoch 950, val loss: 0.7661668658256531
Epoch 960, training loss: 440.8443298339844 = 0.7382160425186157 + 50.0 * 8.802122116088867
Epoch 960, val loss: 0.763217568397522
Epoch 970, training loss: 441.00665283203125 = 0.7341837286949158 + 50.0 * 8.805449485778809
Epoch 970, val loss: 0.7592979669570923
Epoch 980, training loss: 441.30401611328125 = 0.7301270365715027 + 50.0 * 8.811477661132812
Epoch 980, val loss: 0.7555355429649353
Epoch 990, training loss: 441.37420654296875 = 0.7259895205497742 + 50.0 * 8.81296443939209
Epoch 990, val loss: 0.7518768310546875
Epoch 1000, training loss: 441.2608947753906 = 0.7217199802398682 + 50.0 * 8.810783386230469
Epoch 1000, val loss: 0.7479854822158813
Epoch 1010, training loss: 441.4957580566406 = 0.7175658345222473 + 50.0 * 8.815564155578613
Epoch 1010, val loss: 0.7443839311599731
Epoch 1020, training loss: 441.6054382324219 = 0.7134767770767212 + 50.0 * 8.817839622497559
Epoch 1020, val loss: 0.7406690120697021
Epoch 1030, training loss: 441.73992919921875 = 0.709281325340271 + 50.0 * 8.820612907409668
Epoch 1030, val loss: 0.736843466758728
Epoch 1040, training loss: 441.7540283203125 = 0.7051204442977905 + 50.0 * 8.820978164672852
Epoch 1040, val loss: 0.7331477403640747
Epoch 1050, training loss: 442.0176696777344 = 0.7010270357131958 + 50.0 * 8.826333045959473
Epoch 1050, val loss: 0.7295186519622803
Epoch 1060, training loss: 442.0322570800781 = 0.6968575119972229 + 50.0 * 8.82670783996582
Epoch 1060, val loss: 0.7257893085479736
Epoch 1070, training loss: 441.9396667480469 = 0.6928113698959351 + 50.0 * 8.824936866760254
Epoch 1070, val loss: 0.7222045063972473
Epoch 1080, training loss: 442.1015319824219 = 0.6887587904930115 + 50.0 * 8.828255653381348
Epoch 1080, val loss: 0.7187183499336243
Epoch 1090, training loss: 442.177734375 = 0.6847620606422424 + 50.0 * 8.829859733581543
Epoch 1090, val loss: 0.7153248190879822
Epoch 1100, training loss: 442.2467041015625 = 0.6808445453643799 + 50.0 * 8.831316947937012
Epoch 1100, val loss: 0.7118934392929077
Epoch 1110, training loss: 442.4752197265625 = 0.6768836975097656 + 50.0 * 8.835967063903809
Epoch 1110, val loss: 0.7085899114608765
Epoch 1120, training loss: 442.4695739746094 = 0.673064649105072 + 50.0 * 8.835929870605469
Epoch 1120, val loss: 0.7052498459815979
Epoch 1130, training loss: 442.49578857421875 = 0.6692412495613098 + 50.0 * 8.836530685424805
Epoch 1130, val loss: 0.701999306678772
Epoch 1140, training loss: 442.5964660644531 = 0.6655626893043518 + 50.0 * 8.838618278503418
Epoch 1140, val loss: 0.6989007592201233
Epoch 1150, training loss: 442.7802734375 = 0.6619287133216858 + 50.0 * 8.842367172241211
Epoch 1150, val loss: 0.6959720849990845
Epoch 1160, training loss: 442.69378662109375 = 0.658214271068573 + 50.0 * 8.84071159362793
Epoch 1160, val loss: 0.6928452253341675
Epoch 1170, training loss: 442.7903137207031 = 0.6547549962997437 + 50.0 * 8.842711448669434
Epoch 1170, val loss: 0.6900895237922668
Epoch 1180, training loss: 442.98883056640625 = 0.6514159440994263 + 50.0 * 8.846748352050781
Epoch 1180, val loss: 0.6874212026596069
Epoch 1190, training loss: 443.0650939941406 = 0.6479621529579163 + 50.0 * 8.848342895507812
Epoch 1190, val loss: 0.68443363904953
Epoch 1200, training loss: 443.0799560546875 = 0.6447718739509583 + 50.0 * 8.848703384399414
Epoch 1200, val loss: 0.6820871233940125
Epoch 1210, training loss: 443.232666015625 = 0.641628086566925 + 50.0 * 8.851820945739746
Epoch 1210, val loss: 0.6795178055763245
Epoch 1220, training loss: 443.2659912109375 = 0.6384705305099487 + 50.0 * 8.852550506591797
Epoch 1220, val loss: 0.677259087562561
Epoch 1230, training loss: 442.8358459472656 = 0.6353024244308472 + 50.0 * 8.844010353088379
Epoch 1230, val loss: 0.6745450496673584
Epoch 1240, training loss: 443.04132080078125 = 0.6325728297233582 + 50.0 * 8.848175048828125
Epoch 1240, val loss: 0.672505259513855
Epoch 1250, training loss: 443.33978271484375 = 0.6298351287841797 + 50.0 * 8.854199409484863
Epoch 1250, val loss: 0.6706018447875977
Epoch 1260, training loss: 443.7330322265625 = 0.6271295547485352 + 50.0 * 8.862117767333984
Epoch 1260, val loss: 0.6685990691184998
Epoch 1270, training loss: 443.5106506347656 = 0.6243579983711243 + 50.0 * 8.857726097106934
Epoch 1270, val loss: 0.6664519906044006
Epoch 1280, training loss: 443.3749694824219 = 0.621650218963623 + 50.0 * 8.855066299438477
Epoch 1280, val loss: 0.6643257141113281
Epoch 1290, training loss: 443.3421936035156 = 0.6191951632499695 + 50.0 * 8.854459762573242
Epoch 1290, val loss: 0.6630926728248596
Epoch 1300, training loss: 443.7303771972656 = 0.6168376803398132 + 50.0 * 8.86227035522461
Epoch 1300, val loss: 0.6611540913581848
Epoch 1310, training loss: 443.70855712890625 = 0.6143986582756042 + 50.0 * 8.861883163452148
Epoch 1310, val loss: 0.6595247387886047
Epoch 1320, training loss: 443.9561767578125 = 0.6120136380195618 + 50.0 * 8.866883277893066
Epoch 1320, val loss: 0.6579790115356445
Epoch 1330, training loss: 444.2281188964844 = 0.609776496887207 + 50.0 * 8.872366905212402
Epoch 1330, val loss: 0.6564539670944214
Epoch 1340, training loss: 444.2584228515625 = 0.6075329780578613 + 50.0 * 8.873017311096191
Epoch 1340, val loss: 0.6550547480583191
Epoch 1350, training loss: 444.2494812011719 = 0.6053725481033325 + 50.0 * 8.872881889343262
Epoch 1350, val loss: 0.6536835432052612
Epoch 1360, training loss: 444.4240417480469 = 0.6033612489700317 + 50.0 * 8.876413345336914
Epoch 1360, val loss: 0.6523436307907104
Epoch 1370, training loss: 444.4936828613281 = 0.6012639999389648 + 50.0 * 8.877848625183105
Epoch 1370, val loss: 0.6510883569717407
Epoch 1380, training loss: 444.5531005859375 = 0.5993313789367676 + 50.0 * 8.879075050354004
Epoch 1380, val loss: 0.6498362421989441
Epoch 1390, training loss: 444.65228271484375 = 0.59737229347229 + 50.0 * 8.881097793579102
Epoch 1390, val loss: 0.6486292481422424
Epoch 1400, training loss: 444.5088195800781 = 0.5955492258071899 + 50.0 * 8.878265380859375
Epoch 1400, val loss: 0.6476328372955322
Epoch 1410, training loss: 444.5633850097656 = 0.5937467217445374 + 50.0 * 8.879392623901367
Epoch 1410, val loss: 0.6466357111930847
Epoch 1420, training loss: 444.7906188964844 = 0.5919960141181946 + 50.0 * 8.88397216796875
Epoch 1420, val loss: 0.6456899642944336
Epoch 1430, training loss: 444.99908447265625 = 0.5902771949768066 + 50.0 * 8.888175964355469
Epoch 1430, val loss: 0.6447048783302307
Epoch 1440, training loss: 444.8962707519531 = 0.5885135531425476 + 50.0 * 8.886155128479004
Epoch 1440, val loss: 0.643525242805481
Epoch 1450, training loss: 444.8584289550781 = 0.5868374109268188 + 50.0 * 8.885432243347168
Epoch 1450, val loss: 0.6428892016410828
Epoch 1460, training loss: 445.088134765625 = 0.5852841734886169 + 50.0 * 8.890056610107422
Epoch 1460, val loss: 0.6419876217842102
Epoch 1470, training loss: 445.11334228515625 = 0.5837226510047913 + 50.0 * 8.890592575073242
Epoch 1470, val loss: 0.6411325931549072
Epoch 1480, training loss: 445.39630126953125 = 0.5822644233703613 + 50.0 * 8.896280288696289
Epoch 1480, val loss: 0.6405375599861145
Epoch 1490, training loss: 445.4264831542969 = 0.5807645916938782 + 50.0 * 8.8969144821167
Epoch 1490, val loss: 0.6396446228027344
Epoch 1500, training loss: 445.58380126953125 = 0.5792898535728455 + 50.0 * 8.900090217590332
Epoch 1500, val loss: 0.6390370726585388
Epoch 1510, training loss: 445.281494140625 = 0.5777738690376282 + 50.0 * 8.894074440002441
Epoch 1510, val loss: 0.6383447051048279
Epoch 1520, training loss: 445.67059326171875 = 0.5764853954315186 + 50.0 * 8.90188217163086
Epoch 1520, val loss: 0.6377347111701965
Epoch 1530, training loss: 445.6301574707031 = 0.5749627351760864 + 50.0 * 8.901103973388672
Epoch 1530, val loss: 0.6367707848548889
Epoch 1540, training loss: 445.7495422363281 = 0.5736678838729858 + 50.0 * 8.903517723083496
Epoch 1540, val loss: 0.6366431713104248
Epoch 1550, training loss: 445.8407287597656 = 0.5723044276237488 + 50.0 * 8.90536880493164
Epoch 1550, val loss: 0.6357772350311279
Epoch 1560, training loss: 445.9888000488281 = 0.5709283351898193 + 50.0 * 8.908357620239258
Epoch 1560, val loss: 0.6353287100791931
Epoch 1570, training loss: 445.8956298828125 = 0.5695763826370239 + 50.0 * 8.90652084350586
Epoch 1570, val loss: 0.6345897912979126
Epoch 1580, training loss: 446.1124572753906 = 0.5683103799819946 + 50.0 * 8.910882949829102
Epoch 1580, val loss: 0.6341548562049866
Epoch 1590, training loss: 446.0701599121094 = 0.5670189261436462 + 50.0 * 8.910062789916992
Epoch 1590, val loss: 0.6336530447006226
Epoch 1600, training loss: 446.2561950683594 = 0.5657727718353271 + 50.0 * 8.913808822631836
Epoch 1600, val loss: 0.6333226561546326
Epoch 1610, training loss: 445.9007263183594 = 0.5642437934875488 + 50.0 * 8.906729698181152
Epoch 1610, val loss: 0.6322736144065857
Epoch 1620, training loss: 445.8425598144531 = 0.5632095336914062 + 50.0 * 8.905587196350098
Epoch 1620, val loss: 0.632183849811554
Epoch 1630, training loss: 445.8924560546875 = 0.5619794130325317 + 50.0 * 8.906609535217285
Epoch 1630, val loss: 0.6317603588104248
Epoch 1640, training loss: 446.283203125 = 0.5608140230178833 + 50.0 * 8.914447784423828
Epoch 1640, val loss: 0.6313092708587646
Epoch 1650, training loss: 446.6551513671875 = 0.5597353577613831 + 50.0 * 8.921908378601074
Epoch 1650, val loss: 0.6310392022132874
Epoch 1660, training loss: 447.0141296386719 = 0.5585829019546509 + 50.0 * 8.929110527038574
Epoch 1660, val loss: 0.6306208372116089
Epoch 1670, training loss: 446.6273193359375 = 0.5573843121528625 + 50.0 * 8.921399116516113
Epoch 1670, val loss: 0.6303027272224426
Epoch 1680, training loss: 446.784423828125 = 0.5562102198600769 + 50.0 * 8.924564361572266
Epoch 1680, val loss: 0.6299552917480469
Epoch 1690, training loss: 447.01446533203125 = 0.5551150441169739 + 50.0 * 8.929186820983887
Epoch 1690, val loss: 0.6295444369316101
Epoch 1700, training loss: 447.155517578125 = 0.5539888143539429 + 50.0 * 8.93203067779541
Epoch 1700, val loss: 0.6292400360107422
Epoch 1710, training loss: 447.18994140625 = 0.5528154969215393 + 50.0 * 8.93274211883545
Epoch 1710, val loss: 0.6289367079734802
Epoch 1720, training loss: 447.2677917480469 = 0.5516675114631653 + 50.0 * 8.934322357177734
Epoch 1720, val loss: 0.6285713315010071
Epoch 1730, training loss: 447.1014709472656 = 0.5504862070083618 + 50.0 * 8.93101978302002
Epoch 1730, val loss: 0.6283050179481506
Epoch 1740, training loss: 447.07928466796875 = 0.5493061542510986 + 50.0 * 8.930599212646484
Epoch 1740, val loss: 0.6279394030570984
Epoch 1750, training loss: 447.1645812988281 = 0.5482210516929626 + 50.0 * 8.932327270507812
Epoch 1750, val loss: 0.6275608539581299
Epoch 1760, training loss: 447.3440246582031 = 0.547093391418457 + 50.0 * 8.935938835144043
Epoch 1760, val loss: 0.6270976066589355
Epoch 1770, training loss: 447.7006530761719 = 0.5459803938865662 + 50.0 * 8.943093299865723
Epoch 1770, val loss: 0.6266366243362427
Epoch 1780, training loss: 447.73455810546875 = 0.5448118448257446 + 50.0 * 8.943795204162598
Epoch 1780, val loss: 0.6263051629066467
Epoch 1790, training loss: 447.5743408203125 = 0.5436928868293762 + 50.0 * 8.94061279296875
Epoch 1790, val loss: 0.6257254481315613
Epoch 1800, training loss: 447.7469482421875 = 0.5426099300384521 + 50.0 * 8.944087028503418
Epoch 1800, val loss: 0.625815749168396
Epoch 1810, training loss: 447.7855529785156 = 0.5414579510688782 + 50.0 * 8.9448823928833
Epoch 1810, val loss: 0.6253585815429688
Epoch 1820, training loss: 447.876220703125 = 0.5403185486793518 + 50.0 * 8.946718215942383
Epoch 1820, val loss: 0.624969482421875
Epoch 1830, training loss: 447.9535217285156 = 0.5392042994499207 + 50.0 * 8.948286056518555
Epoch 1830, val loss: 0.6246496438980103
Epoch 1840, training loss: 448.2017822265625 = 0.5380977392196655 + 50.0 * 8.95327377319336
Epoch 1840, val loss: 0.6240962147712708
Epoch 1850, training loss: 448.04278564453125 = 0.5369069576263428 + 50.0 * 8.950118064880371
Epoch 1850, val loss: 0.6238421201705933
Epoch 1860, training loss: 448.2039489746094 = 0.5357747673988342 + 50.0 * 8.953363418579102
Epoch 1860, val loss: 0.6235132217407227
Epoch 1870, training loss: 448.2856140136719 = 0.534669816493988 + 50.0 * 8.955018997192383
Epoch 1870, val loss: 0.623115062713623
Epoch 1880, training loss: 448.33056640625 = 0.5335171818733215 + 50.0 * 8.955941200256348
Epoch 1880, val loss: 0.622725248336792
Epoch 1890, training loss: 448.7627258300781 = 0.532509446144104 + 50.0 * 8.964604377746582
Epoch 1890, val loss: 0.6226849555969238
Epoch 1900, training loss: 448.7546691894531 = 0.5313646793365479 + 50.0 * 8.964466094970703
Epoch 1900, val loss: 0.62193763256073
Epoch 1910, training loss: 448.9131164550781 = 0.5302662253379822 + 50.0 * 8.967657089233398
Epoch 1910, val loss: 0.621701180934906
Epoch 1920, training loss: 449.0455322265625 = 0.5291035771369934 + 50.0 * 8.970328330993652
Epoch 1920, val loss: 0.6213197708129883
Epoch 1930, training loss: 449.12591552734375 = 0.5279361009597778 + 50.0 * 8.971960067749023
Epoch 1930, val loss: 0.6208972334861755
Epoch 1940, training loss: 448.9819641113281 = 0.5267332196235657 + 50.0 * 8.969104766845703
Epoch 1940, val loss: 0.6205242276191711
Epoch 1950, training loss: 448.98199462890625 = 0.5255261063575745 + 50.0 * 8.96912956237793
Epoch 1950, val loss: 0.6201810836791992
Epoch 1960, training loss: 448.76397705078125 = 0.5243219137191772 + 50.0 * 8.96479320526123
Epoch 1960, val loss: 0.6198400259017944
Epoch 1970, training loss: 448.82122802734375 = 0.5232033729553223 + 50.0 * 8.965960502624512
Epoch 1970, val loss: 0.6195157170295715
Epoch 1980, training loss: 449.2215881347656 = 0.5220529437065125 + 50.0 * 8.973990440368652
Epoch 1980, val loss: 0.6190951466560364
Epoch 1990, training loss: 449.50140380859375 = 0.5208843946456909 + 50.0 * 8.979610443115234
Epoch 1990, val loss: 0.618606448173523
Epoch 2000, training loss: 449.4032897949219 = 0.5196159482002258 + 50.0 * 8.977673530578613
Epoch 2000, val loss: 0.6181356310844421
Epoch 2010, training loss: 449.3538818359375 = 0.5184120535850525 + 50.0 * 8.976709365844727
Epoch 2010, val loss: 0.6178287267684937
Epoch 2020, training loss: 449.615234375 = 0.5172597169876099 + 50.0 * 8.981959342956543
Epoch 2020, val loss: 0.6175302267074585
Epoch 2030, training loss: 449.61212158203125 = 0.516029953956604 + 50.0 * 8.981922149658203
Epoch 2030, val loss: 0.6171483993530273
Epoch 2040, training loss: 448.98040771484375 = 0.5147374272346497 + 50.0 * 8.969313621520996
Epoch 2040, val loss: 0.6168765425682068
Epoch 2050, training loss: 448.8242492675781 = 0.5135839581489563 + 50.0 * 8.96621322631836
Epoch 2050, val loss: 0.6166278719902039
Epoch 2060, training loss: 449.3359375 = 0.5124387741088867 + 50.0 * 8.976469993591309
Epoch 2060, val loss: 0.6164963841438293
Epoch 2070, training loss: 449.65985107421875 = 0.5112226009368896 + 50.0 * 8.982972145080566
Epoch 2070, val loss: 0.6156971454620361
Epoch 2080, training loss: 449.94378662109375 = 0.5099734663963318 + 50.0 * 8.988676071166992
Epoch 2080, val loss: 0.6154577136039734
Epoch 2090, training loss: 449.88385009765625 = 0.5086875557899475 + 50.0 * 8.987503051757812
Epoch 2090, val loss: 0.6150093674659729
Epoch 2100, training loss: 450.0474853515625 = 0.5074620246887207 + 50.0 * 8.990799903869629
Epoch 2100, val loss: 0.6146396398544312
Epoch 2110, training loss: 450.2892150878906 = 0.5062218308448792 + 50.0 * 8.995659828186035
Epoch 2110, val loss: 0.6143074631690979
Epoch 2120, training loss: 450.1238098144531 = 0.5049557685852051 + 50.0 * 8.992377281188965
Epoch 2120, val loss: 0.6140798926353455
Epoch 2130, training loss: 449.7393798828125 = 0.5036140084266663 + 50.0 * 8.984715461730957
Epoch 2130, val loss: 0.6138020753860474
Epoch 2140, training loss: 449.9722595214844 = 0.5024094581604004 + 50.0 * 8.989397048950195
Epoch 2140, val loss: 0.6133341789245605
Epoch 2150, training loss: 450.2842712402344 = 0.5011800527572632 + 50.0 * 8.995661735534668
Epoch 2150, val loss: 0.6129867434501648
Epoch 2160, training loss: 450.53472900390625 = 0.4998749792575836 + 50.0 * 9.000697135925293
Epoch 2160, val loss: 0.6127115488052368
Epoch 2170, training loss: 450.5429382324219 = 0.4985378682613373 + 50.0 * 9.000887870788574
Epoch 2170, val loss: 0.6122973561286926
Epoch 2180, training loss: 450.28948974609375 = 0.4972534775733948 + 50.0 * 8.995844841003418
Epoch 2180, val loss: 0.6119869351387024
Epoch 2190, training loss: 450.19171142578125 = 0.49597394466400146 + 50.0 * 8.993914604187012
Epoch 2190, val loss: 0.6117965579032898
Epoch 2200, training loss: 450.4694519042969 = 0.4947698712348938 + 50.0 * 8.999493598937988
Epoch 2200, val loss: 0.6111078858375549
Epoch 2210, training loss: 450.58404541015625 = 0.4934226870536804 + 50.0 * 9.001812934875488
Epoch 2210, val loss: 0.6107742190361023
Epoch 2220, training loss: 450.8640441894531 = 0.4920758008956909 + 50.0 * 9.007439613342285
Epoch 2220, val loss: 0.6107680201530457
Epoch 2230, training loss: 450.5720520019531 = 0.49066129326820374 + 50.0 * 9.001627922058105
Epoch 2230, val loss: 0.6105703115463257
Epoch 2240, training loss: 450.6146545410156 = 0.48929527401924133 + 50.0 * 9.002507209777832
Epoch 2240, val loss: 0.6099981665611267
Epoch 2250, training loss: 450.9637756347656 = 0.4879557490348816 + 50.0 * 9.009516716003418
Epoch 2250, val loss: 0.6096588969230652
Epoch 2260, training loss: 451.1868896484375 = 0.48659905791282654 + 50.0 * 9.014005661010742
Epoch 2260, val loss: 0.6092670559883118
Epoch 2270, training loss: 450.94158935546875 = 0.4851849675178528 + 50.0 * 9.009127616882324
Epoch 2270, val loss: 0.6088733673095703
Epoch 2280, training loss: 450.6786193847656 = 0.4838036000728607 + 50.0 * 9.003896713256836
Epoch 2280, val loss: 0.6086776852607727
Epoch 2290, training loss: 451.06988525390625 = 0.48252707719802856 + 50.0 * 9.011747360229492
Epoch 2290, val loss: 0.608464777469635
Epoch 2300, training loss: 451.2079772949219 = 0.4811767637729645 + 50.0 * 9.014535903930664
Epoch 2300, val loss: 0.6083369255065918
Epoch 2310, training loss: 451.25006103515625 = 0.47991740703582764 + 50.0 * 9.015402793884277
Epoch 2310, val loss: 0.6078898310661316
Epoch 2320, training loss: 451.4356689453125 = 0.47856757044792175 + 50.0 * 9.019142150878906
Epoch 2320, val loss: 0.6074660420417786
Epoch 2330, training loss: 451.3125305175781 = 0.4771265387535095 + 50.0 * 9.016708374023438
Epoch 2330, val loss: 0.6072266697883606
Epoch 2340, training loss: 451.3319091796875 = 0.4757460653781891 + 50.0 * 9.017123222351074
Epoch 2340, val loss: 0.6068660020828247
Epoch 2350, training loss: 451.4770202636719 = 0.47434505820274353 + 50.0 * 9.02005386352539
Epoch 2350, val loss: 0.6066452264785767
Epoch 2360, training loss: 451.6620178222656 = 0.47292882204055786 + 50.0 * 9.023781776428223
Epoch 2360, val loss: 0.6062915325164795
Epoch 2370, training loss: 451.51898193359375 = 0.47140541672706604 + 50.0 * 9.020951271057129
Epoch 2370, val loss: 0.6058990955352783
Epoch 2380, training loss: 451.55633544921875 = 0.4699707627296448 + 50.0 * 9.021727561950684
Epoch 2380, val loss: 0.6055602431297302
Epoch 2390, training loss: 451.7037353515625 = 0.46852636337280273 + 50.0 * 9.024703979492188
Epoch 2390, val loss: 0.6051741242408752
Epoch 2400, training loss: 451.6230773925781 = 0.4670352637767792 + 50.0 * 9.023120880126953
Epoch 2400, val loss: 0.6047226786613464
Epoch 2410, training loss: 451.62060546875 = 0.4655647575855255 + 50.0 * 9.023100852966309
Epoch 2410, val loss: 0.6045182943344116
Epoch 2420, training loss: 451.8023986816406 = 0.4641411304473877 + 50.0 * 9.026764869689941
Epoch 2420, val loss: 0.6041947603225708
Epoch 2430, training loss: 451.8515319824219 = 0.4626586139202118 + 50.0 * 9.027777671813965
Epoch 2430, val loss: 0.6039812564849854
Epoch 2440, training loss: 451.9168701171875 = 0.4611865282058716 + 50.0 * 9.02911376953125
Epoch 2440, val loss: 0.603486955165863
Epoch 2450, training loss: 451.10845947265625 = 0.45967164635658264 + 50.0 * 9.012975692749023
Epoch 2450, val loss: 0.6038061380386353
Epoch 2460, training loss: 451.0667724609375 = 0.4584655463695526 + 50.0 * 9.012166023254395
Epoch 2460, val loss: 0.6031684279441833
Epoch 2470, training loss: 450.5671691894531 = 0.4572400450706482 + 50.0 * 9.002198219299316
Epoch 2470, val loss: 0.6039899587631226
Epoch 2480, training loss: 450.6005859375 = 0.45576930046081543 + 50.0 * 9.002896308898926
Epoch 2480, val loss: 0.6022606492042542
Epoch 2490, training loss: 450.79498291015625 = 0.4542100429534912 + 50.0 * 9.006814956665039
Epoch 2490, val loss: 0.6023088097572327
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7542028985507246
0.8164891690212274
=== training gcn model ===
Epoch 0, training loss: 513.187744140625 = 1.1055201292037964 + 50.0 * 10.241644859313965
Epoch 0, val loss: 1.103452205657959
Epoch 10, training loss: 493.70068359375 = 1.1018122434616089 + 50.0 * 9.851977348327637
Epoch 10, val loss: 1.0998259782791138
Epoch 20, training loss: 484.6136169433594 = 1.098309874534607 + 50.0 * 9.670306205749512
Epoch 20, val loss: 1.0963635444641113
Epoch 30, training loss: 477.6141662597656 = 1.0949592590332031 + 50.0 * 9.530384063720703
Epoch 30, val loss: 1.0930697917938232
Epoch 40, training loss: 471.9531555175781 = 1.091780662536621 + 50.0 * 9.417227745056152
Epoch 40, val loss: 1.0899624824523926
Epoch 50, training loss: 467.2544250488281 = 1.0887579917907715 + 50.0 * 9.32331371307373
Epoch 50, val loss: 1.087013840675354
Epoch 60, training loss: 463.2789001464844 = 1.0858681201934814 + 50.0 * 9.243860244750977
Epoch 60, val loss: 1.0842097997665405
Epoch 70, training loss: 459.90179443359375 = 1.0831326246261597 + 50.0 * 9.176373481750488
Epoch 70, val loss: 1.0815625190734863
Epoch 80, training loss: 456.97003173828125 = 1.0805188417434692 + 50.0 * 9.117790222167969
Epoch 80, val loss: 1.0790438652038574
Epoch 90, training loss: 454.478515625 = 1.0780242681503296 + 50.0 * 9.068009376525879
Epoch 90, val loss: 1.0766493082046509
Epoch 100, training loss: 452.2798156738281 = 1.0756419897079468 + 50.0 * 9.024083137512207
Epoch 100, val loss: 1.0743709802627563
Epoch 110, training loss: 450.4496154785156 = 1.0733683109283447 + 50.0 * 8.98752498626709
Epoch 110, val loss: 1.0722062587738037
Epoch 120, training loss: 448.8327941894531 = 1.0711936950683594 + 50.0 * 8.955231666564941
Epoch 120, val loss: 1.0701402425765991
Epoch 130, training loss: 447.543212890625 = 1.0690860748291016 + 50.0 * 8.929482460021973
Epoch 130, val loss: 1.068151831626892
Epoch 140, training loss: 446.4155578613281 = 1.0671241283416748 + 50.0 * 8.906968116760254
Epoch 140, val loss: 1.066287636756897
Epoch 150, training loss: 445.4056091308594 = 1.065260648727417 + 50.0 * 8.886807441711426
Epoch 150, val loss: 1.0645438432693481
Epoch 160, training loss: 444.4986877441406 = 1.0634490251541138 + 50.0 * 8.868704795837402
Epoch 160, val loss: 1.0628457069396973
Epoch 170, training loss: 443.6835021972656 = 1.0617343187332153 + 50.0 * 8.852435111999512
Epoch 170, val loss: 1.0612432956695557
Epoch 180, training loss: 443.0226745605469 = 1.0600682497024536 + 50.0 * 8.839252471923828
Epoch 180, val loss: 1.059691309928894
Epoch 190, training loss: 442.5431823730469 = 1.0585070848464966 + 50.0 * 8.829693794250488
Epoch 190, val loss: 1.058233380317688
Epoch 200, training loss: 442.0281066894531 = 1.0570025444030762 + 50.0 * 8.819421768188477
Epoch 200, val loss: 1.0568243265151978
Epoch 210, training loss: 441.6821594238281 = 1.0555673837661743 + 50.0 * 8.812531471252441
Epoch 210, val loss: 1.0554853677749634
Epoch 220, training loss: 441.45672607421875 = 1.0541481971740723 + 50.0 * 8.808052062988281
Epoch 220, val loss: 1.0541751384735107
Epoch 230, training loss: 440.7433776855469 = 1.0527409315109253 + 50.0 * 8.79381275177002
Epoch 230, val loss: 1.052833914756775
Epoch 240, training loss: 440.91192626953125 = 1.0514744520187378 + 50.0 * 8.797208786010742
Epoch 240, val loss: 1.0516358613967896
Epoch 250, training loss: 440.45501708984375 = 1.0501230955123901 + 50.0 * 8.788098335266113
Epoch 250, val loss: 1.0503897666931152
Epoch 260, training loss: 440.13189697265625 = 1.0487825870513916 + 50.0 * 8.781661987304688
Epoch 260, val loss: 1.049144983291626
Epoch 270, training loss: 440.0827331542969 = 1.0474673509597778 + 50.0 * 8.780705451965332
Epoch 270, val loss: 1.0478838682174683
Epoch 280, training loss: 439.8871154785156 = 1.0461050271987915 + 50.0 * 8.776820182800293
Epoch 280, val loss: 1.046593189239502
Epoch 290, training loss: 439.8087463378906 = 1.0447217226028442 + 50.0 * 8.775280952453613
Epoch 290, val loss: 1.0452932119369507
Epoch 300, training loss: 439.5819091796875 = 1.043292760848999 + 50.0 * 8.770771980285645
Epoch 300, val loss: 1.0439505577087402
Epoch 310, training loss: 439.57720947265625 = 1.0417509078979492 + 50.0 * 8.770709037780762
Epoch 310, val loss: 1.04245924949646
Epoch 320, training loss: 439.5522766113281 = 1.0402634143829346 + 50.0 * 8.770240783691406
Epoch 320, val loss: 1.0410608053207397
Epoch 330, training loss: 439.4830017089844 = 1.0386390686035156 + 50.0 * 8.768887519836426
Epoch 330, val loss: 1.0394853353500366
Epoch 340, training loss: 439.48883056640625 = 1.0369216203689575 + 50.0 * 8.769038200378418
Epoch 340, val loss: 1.0378421545028687
Epoch 350, training loss: 439.48419189453125 = 1.0350414514541626 + 50.0 * 8.768982887268066
Epoch 350, val loss: 1.0360088348388672
Epoch 360, training loss: 439.34063720703125 = 1.03309965133667 + 50.0 * 8.76615047454834
Epoch 360, val loss: 1.034173607826233
Epoch 370, training loss: 439.3611755371094 = 1.0310416221618652 + 50.0 * 8.766602516174316
Epoch 370, val loss: 1.0321851968765259
Epoch 380, training loss: 439.3350524902344 = 1.0288728475570679 + 50.0 * 8.76612377166748
Epoch 380, val loss: 1.030125617980957
Epoch 390, training loss: 439.3038330078125 = 1.0266348123550415 + 50.0 * 8.765543937683105
Epoch 390, val loss: 1.0279322862625122
Epoch 400, training loss: 439.32452392578125 = 1.024208664894104 + 50.0 * 8.766006469726562
Epoch 400, val loss: 1.0255844593048096
Epoch 410, training loss: 439.33251953125 = 1.0216158628463745 + 50.0 * 8.766218185424805
Epoch 410, val loss: 1.023132562637329
Epoch 420, training loss: 439.45733642578125 = 1.0186330080032349 + 50.0 * 8.768774032592773
Epoch 420, val loss: 1.0201414823532104
Epoch 430, training loss: 440.0255126953125 = 1.0163404941558838 + 50.0 * 8.780183792114258
Epoch 430, val loss: 1.0179884433746338
Epoch 440, training loss: 439.877685546875 = 1.0133591890335083 + 50.0 * 8.777286529541016
Epoch 440, val loss: 1.0152496099472046
Epoch 450, training loss: 439.5476989746094 = 1.010340929031372 + 50.0 * 8.770747184753418
Epoch 450, val loss: 1.0124282836914062
Epoch 460, training loss: 439.3680725097656 = 1.0071953535079956 + 50.0 * 8.767217636108398
Epoch 460, val loss: 1.0094411373138428
Epoch 470, training loss: 439.43560791015625 = 1.003899335861206 + 50.0 * 8.768633842468262
Epoch 470, val loss: 1.0063010454177856
Epoch 480, training loss: 439.72930908203125 = 1.000624179840088 + 50.0 * 8.774574279785156
Epoch 480, val loss: 1.0032284259796143
Epoch 490, training loss: 439.8302917480469 = 0.9970536828041077 + 50.0 * 8.776664733886719
Epoch 490, val loss: 0.9999067783355713
Epoch 500, training loss: 440.2482604980469 = 0.9935111999511719 + 50.0 * 8.78509521484375
Epoch 500, val loss: 0.996593713760376
Epoch 510, training loss: 440.3016662597656 = 0.9898228645324707 + 50.0 * 8.786236763000488
Epoch 510, val loss: 0.993046760559082
Epoch 520, training loss: 440.29766845703125 = 0.9860846996307373 + 50.0 * 8.786231994628906
Epoch 520, val loss: 0.9896508455276489
Epoch 530, training loss: 440.2537841796875 = 0.9821550250053406 + 50.0 * 8.785432815551758
Epoch 530, val loss: 0.98599773645401
Epoch 540, training loss: 440.3629150390625 = 0.978175163269043 + 50.0 * 8.787694931030273
Epoch 540, val loss: 0.9823142290115356
Epoch 550, training loss: 440.67877197265625 = 0.9741343259811401 + 50.0 * 8.794093132019043
Epoch 550, val loss: 0.9784862399101257
Epoch 560, training loss: 440.4835510253906 = 0.9699719548225403 + 50.0 * 8.790271759033203
Epoch 560, val loss: 0.9746639132499695
Epoch 570, training loss: 440.95751953125 = 0.9659360647201538 + 50.0 * 8.79983139038086
Epoch 570, val loss: 0.9709407091140747
Epoch 580, training loss: 440.963134765625 = 0.9617074728012085 + 50.0 * 8.800028800964355
Epoch 580, val loss: 0.9670459032058716
Epoch 590, training loss: 441.2125244140625 = 0.957496702671051 + 50.0 * 8.805100440979004
Epoch 590, val loss: 0.9632025957107544
Epoch 600, training loss: 441.3608093261719 = 0.9532011151313782 + 50.0 * 8.808152198791504
Epoch 600, val loss: 0.9593089818954468
Epoch 610, training loss: 441.38836669921875 = 0.9488574266433716 + 50.0 * 8.80879020690918
Epoch 610, val loss: 0.9552973508834839
Epoch 620, training loss: 441.05621337890625 = 0.9440939426422119 + 50.0 * 8.802242279052734
Epoch 620, val loss: 0.9512027502059937
Epoch 630, training loss: 441.29437255859375 = 0.940049946308136 + 50.0 * 8.807085990905762
Epoch 630, val loss: 0.947312593460083
Epoch 640, training loss: 441.1969909667969 = 0.9355778694152832 + 50.0 * 8.805228233337402
Epoch 640, val loss: 0.9432632327079773
Epoch 650, training loss: 441.7270202636719 = 0.9316237568855286 + 50.0 * 8.815908432006836
Epoch 650, val loss: 0.939782440662384
Epoch 660, training loss: 441.86163330078125 = 0.9274106025695801 + 50.0 * 8.818684577941895
Epoch 660, val loss: 0.9360318779945374
Epoch 670, training loss: 442.03948974609375 = 0.923241913318634 + 50.0 * 8.822324752807617
Epoch 670, val loss: 0.9323030710220337
Epoch 680, training loss: 442.2354736328125 = 0.9189708828926086 + 50.0 * 8.826330184936523
Epoch 680, val loss: 0.9284820556640625
Epoch 690, training loss: 442.2264099121094 = 0.9147496223449707 + 50.0 * 8.82623291015625
Epoch 690, val loss: 0.9247428774833679
Epoch 700, training loss: 442.22283935546875 = 0.9105377197265625 + 50.0 * 8.82624626159668
Epoch 700, val loss: 0.9209474325180054
Epoch 710, training loss: 442.3081359863281 = 0.9062852263450623 + 50.0 * 8.82803726196289
Epoch 710, val loss: 0.917195737361908
Epoch 720, training loss: 442.4412536621094 = 0.9021674990653992 + 50.0 * 8.830781936645508
Epoch 720, val loss: 0.9136313199996948
Epoch 730, training loss: 442.740966796875 = 0.8981456756591797 + 50.0 * 8.836856842041016
Epoch 730, val loss: 0.9101422429084778
Epoch 740, training loss: 442.7214050292969 = 0.894281268119812 + 50.0 * 8.836542129516602
Epoch 740, val loss: 0.906770646572113
Epoch 750, training loss: 442.73199462890625 = 0.8900940418243408 + 50.0 * 8.836837768554688
Epoch 750, val loss: 0.9030914306640625
Epoch 760, training loss: 442.8874206542969 = 0.8862684369087219 + 50.0 * 8.840023040771484
Epoch 760, val loss: 0.8997488617897034
Epoch 770, training loss: 443.1531982421875 = 0.8823801279067993 + 50.0 * 8.845416069030762
Epoch 770, val loss: 0.8963421583175659
Epoch 780, training loss: 443.24237060546875 = 0.8783562183380127 + 50.0 * 8.847280502319336
Epoch 780, val loss: 0.8929263949394226
Epoch 790, training loss: 443.0173645019531 = 0.874438464641571 + 50.0 * 8.84285831451416
Epoch 790, val loss: 0.8893972635269165
Epoch 800, training loss: 442.9635009765625 = 0.8704525232315063 + 50.0 * 8.8418607711792
Epoch 800, val loss: 0.8860708475112915
Epoch 810, training loss: 443.2336120605469 = 0.866655170917511 + 50.0 * 8.847338676452637
Epoch 810, val loss: 0.8828519582748413
Epoch 820, training loss: 443.51409912109375 = 0.8628035187721252 + 50.0 * 8.853026390075684
Epoch 820, val loss: 0.8794572949409485
Epoch 830, training loss: 443.57696533203125 = 0.8589417338371277 + 50.0 * 8.854360580444336
Epoch 830, val loss: 0.8762046098709106
Epoch 840, training loss: 443.73822021484375 = 0.8551785945892334 + 50.0 * 8.857661247253418
Epoch 840, val loss: 0.8730130791664124
Epoch 850, training loss: 443.811767578125 = 0.8514214158058167 + 50.0 * 8.859207153320312
Epoch 850, val loss: 0.8698890805244446
Epoch 860, training loss: 443.85546875 = 0.8476126790046692 + 50.0 * 8.860157012939453
Epoch 860, val loss: 0.8667178750038147
Epoch 870, training loss: 443.81524658203125 = 0.8438657522201538 + 50.0 * 8.859427452087402
Epoch 870, val loss: 0.8635727763175964
Epoch 880, training loss: 444.073974609375 = 0.8402273654937744 + 50.0 * 8.86467456817627
Epoch 880, val loss: 0.8605719804763794
Epoch 890, training loss: 444.17938232421875 = 0.8367165327072144 + 50.0 * 8.866852760314941
Epoch 890, val loss: 0.8576201796531677
Epoch 900, training loss: 444.3443298339844 = 0.8331922888755798 + 50.0 * 8.870223045349121
Epoch 900, val loss: 0.8547070622444153
Epoch 910, training loss: 444.4650573730469 = 0.8296509385108948 + 50.0 * 8.872708320617676
Epoch 910, val loss: 0.8516907095909119
Epoch 920, training loss: 444.6195068359375 = 0.8261610865592957 + 50.0 * 8.875866889953613
Epoch 920, val loss: 0.8488296270370483
Epoch 930, training loss: 444.7308349609375 = 0.8226667642593384 + 50.0 * 8.87816333770752
Epoch 930, val loss: 0.8458927869796753
Epoch 940, training loss: 444.9341125488281 = 0.8192493319511414 + 50.0 * 8.88229751586914
Epoch 940, val loss: 0.8430766463279724
Epoch 950, training loss: 444.88494873046875 = 0.8158617615699768 + 50.0 * 8.88138198852539
Epoch 950, val loss: 0.8403273224830627
Epoch 960, training loss: 445.05938720703125 = 0.8125175833702087 + 50.0 * 8.884937286376953
Epoch 960, val loss: 0.8375864028930664
Epoch 970, training loss: 445.1188659667969 = 0.8091983795166016 + 50.0 * 8.88619327545166
Epoch 970, val loss: 0.8349298238754272
Epoch 980, training loss: 445.3785705566406 = 0.805973470211029 + 50.0 * 8.891451835632324
Epoch 980, val loss: 0.8322168588638306
Epoch 990, training loss: 445.3757629394531 = 0.8026044964790344 + 50.0 * 8.891463279724121
Epoch 990, val loss: 0.8295411467552185
Epoch 1000, training loss: 445.49798583984375 = 0.799406111240387 + 50.0 * 8.89397144317627
Epoch 1000, val loss: 0.8269206285476685
Epoch 1010, training loss: 444.8214111328125 = 0.7958080768585205 + 50.0 * 8.880512237548828
Epoch 1010, val loss: 0.8240297436714172
Epoch 1020, training loss: 445.11346435546875 = 0.7921654582023621 + 50.0 * 8.886425971984863
Epoch 1020, val loss: 0.8207520842552185
Epoch 1030, training loss: 445.0390319824219 = 0.7889775633811951 + 50.0 * 8.885001182556152
Epoch 1030, val loss: 0.8183478116989136
Epoch 1040, training loss: 445.1145324707031 = 0.786047637462616 + 50.0 * 8.88656997680664
Epoch 1040, val loss: 0.8160534501075745
Epoch 1050, training loss: 445.1784362792969 = 0.7830386757850647 + 50.0 * 8.887907981872559
Epoch 1050, val loss: 0.8136181831359863
Epoch 1060, training loss: 445.3974609375 = 0.7798938751220703 + 50.0 * 8.892351150512695
Epoch 1060, val loss: 0.8111096620559692
Epoch 1070, training loss: 445.6917419433594 = 0.7768363952636719 + 50.0 * 8.898298263549805
Epoch 1070, val loss: 0.8086045980453491
Epoch 1080, training loss: 445.86322021484375 = 0.7736768126487732 + 50.0 * 8.901790618896484
Epoch 1080, val loss: 0.8061462044715881
Epoch 1090, training loss: 445.9459533691406 = 0.770452618598938 + 50.0 * 8.903510093688965
Epoch 1090, val loss: 0.8036239147186279
Epoch 1100, training loss: 445.7296447753906 = 0.7671807408332825 + 50.0 * 8.899249076843262
Epoch 1100, val loss: 0.8008957505226135
Epoch 1110, training loss: 445.9599304199219 = 0.7640879154205322 + 50.0 * 8.903916358947754
Epoch 1110, val loss: 0.798498272895813
Epoch 1120, training loss: 446.18560791015625 = 0.7610465884208679 + 50.0 * 8.908491134643555
Epoch 1120, val loss: 0.7961471080780029
Epoch 1130, training loss: 446.3956604003906 = 0.7579801082611084 + 50.0 * 8.91275405883789
Epoch 1130, val loss: 0.7936217784881592
Epoch 1140, training loss: 446.220947265625 = 0.754619836807251 + 50.0 * 8.909326553344727
Epoch 1140, val loss: 0.7911850810050964
Epoch 1150, training loss: 446.036376953125 = 0.7513346076011658 + 50.0 * 8.90570068359375
Epoch 1150, val loss: 0.7884194850921631
Epoch 1160, training loss: 446.2953186035156 = 0.7485017776489258 + 50.0 * 8.91093635559082
Epoch 1160, val loss: 0.7861189842224121
Epoch 1170, training loss: 446.56378173828125 = 0.7455286979675293 + 50.0 * 8.916365623474121
Epoch 1170, val loss: 0.7838324308395386
Epoch 1180, training loss: 446.54925537109375 = 0.742420494556427 + 50.0 * 8.916136741638184
Epoch 1180, val loss: 0.781487226486206
Epoch 1190, training loss: 446.6708068847656 = 0.7393825054168701 + 50.0 * 8.918628692626953
Epoch 1190, val loss: 0.779046356678009
Epoch 1200, training loss: 446.8392639160156 = 0.7364636063575745 + 50.0 * 8.922056198120117
Epoch 1200, val loss: 0.7768316864967346
Epoch 1210, training loss: 447.0290222167969 = 0.7335972189903259 + 50.0 * 8.925908088684082
Epoch 1210, val loss: 0.7746506929397583
Epoch 1220, training loss: 447.11669921875 = 0.7305838465690613 + 50.0 * 8.927721977233887
Epoch 1220, val loss: 0.77226322889328
Epoch 1230, training loss: 447.25897216796875 = 0.7276306748390198 + 50.0 * 8.93062686920166
Epoch 1230, val loss: 0.7698551416397095
Epoch 1240, training loss: 446.9085693359375 = 0.7245245575904846 + 50.0 * 8.923681259155273
Epoch 1240, val loss: 0.7677054405212402
Epoch 1250, training loss: 446.9901123046875 = 0.7216496467590332 + 50.0 * 8.925369262695312
Epoch 1250, val loss: 0.7656905055046082
Epoch 1260, training loss: 447.4315490722656 = 0.7189015746116638 + 50.0 * 8.934252738952637
Epoch 1260, val loss: 0.7635533809661865
Epoch 1270, training loss: 447.2204895019531 = 0.7159320116043091 + 50.0 * 8.93009090423584
Epoch 1270, val loss: 0.7612356543540955
Epoch 1280, training loss: 447.427978515625 = 0.7131494879722595 + 50.0 * 8.934296607971191
Epoch 1280, val loss: 0.7591823935508728
Epoch 1290, training loss: 447.6815185546875 = 0.7103813290596008 + 50.0 * 8.939422607421875
Epoch 1290, val loss: 0.757160484790802
Epoch 1300, training loss: 447.7913513183594 = 0.7074575424194336 + 50.0 * 8.941678047180176
Epoch 1300, val loss: 0.7549833059310913
Epoch 1310, training loss: 447.7504577636719 = 0.7045100927352905 + 50.0 * 8.940918922424316
Epoch 1310, val loss: 0.7528120875358582
Epoch 1320, training loss: 447.9209289550781 = 0.7016065120697021 + 50.0 * 8.94438648223877
Epoch 1320, val loss: 0.7507563233375549
Epoch 1330, training loss: 448.0270690917969 = 0.6988010406494141 + 50.0 * 8.946565628051758
Epoch 1330, val loss: 0.7486923336982727
Epoch 1340, training loss: 448.14654541015625 = 0.6959558725357056 + 50.0 * 8.94901180267334
Epoch 1340, val loss: 0.7466614246368408
Epoch 1350, training loss: 447.9589538574219 = 0.6929875016212463 + 50.0 * 8.945319175720215
Epoch 1350, val loss: 0.744533896446228
Epoch 1360, training loss: 448.05474853515625 = 0.6901817917823792 + 50.0 * 8.947291374206543
Epoch 1360, val loss: 0.742542028427124
Epoch 1370, training loss: 448.37445068359375 = 0.687579870223999 + 50.0 * 8.953737258911133
Epoch 1370, val loss: 0.7405170798301697
Epoch 1380, training loss: 448.39776611328125 = 0.6847105026245117 + 50.0 * 8.95426082611084
Epoch 1380, val loss: 0.7385151982307434
Epoch 1390, training loss: 448.5798645019531 = 0.681959331035614 + 50.0 * 8.957958221435547
Epoch 1390, val loss: 0.7365519404411316
Epoch 1400, training loss: 448.7231140136719 = 0.6791386604309082 + 50.0 * 8.9608793258667
Epoch 1400, val loss: 0.7344797253608704
Epoch 1410, training loss: 448.4097900390625 = 0.6760573983192444 + 50.0 * 8.95467472076416
Epoch 1410, val loss: 0.7324214577674866
Epoch 1420, training loss: 447.29522705078125 = 0.6734333038330078 + 50.0 * 8.932435989379883
Epoch 1420, val loss: 0.7301096320152283
Epoch 1430, training loss: 447.18023681640625 = 0.6706087589263916 + 50.0 * 8.930191993713379
Epoch 1430, val loss: 0.7287600636482239
Epoch 1440, training loss: 447.43487548828125 = 0.6679757833480835 + 50.0 * 8.935338020324707
Epoch 1440, val loss: 0.7261694669723511
Epoch 1450, training loss: 447.18609619140625 = 0.665335476398468 + 50.0 * 8.930415153503418
Epoch 1450, val loss: 0.7245644330978394
Epoch 1460, training loss: 447.73101806640625 = 0.6629916429519653 + 50.0 * 8.941360473632812
Epoch 1460, val loss: 0.7227733731269836
Epoch 1470, training loss: 448.1404724121094 = 0.660514771938324 + 50.0 * 8.949599266052246
Epoch 1470, val loss: 0.7211137413978577
Epoch 1480, training loss: 448.4463806152344 = 0.6579542756080627 + 50.0 * 8.955768585205078
Epoch 1480, val loss: 0.719312846660614
Epoch 1490, training loss: 448.4958801269531 = 0.6552579402923584 + 50.0 * 8.956812858581543
Epoch 1490, val loss: 0.7175123691558838
Epoch 1500, training loss: 448.8874206542969 = 0.6527050733566284 + 50.0 * 8.964694023132324
Epoch 1500, val loss: 0.7156701683998108
Epoch 1510, training loss: 448.9794921875 = 0.6500788331031799 + 50.0 * 8.966588020324707
Epoch 1510, val loss: 0.7138431072235107
Epoch 1520, training loss: 448.77850341796875 = 0.6474177837371826 + 50.0 * 8.962621688842773
Epoch 1520, val loss: 0.7120103240013123
Epoch 1530, training loss: 449.0655212402344 = 0.6449597477912903 + 50.0 * 8.968411445617676
Epoch 1530, val loss: 0.7102790474891663
Epoch 1540, training loss: 449.3212890625 = 0.6424547433853149 + 50.0 * 8.973576545715332
Epoch 1540, val loss: 0.7086734175682068
Epoch 1550, training loss: 449.01416015625 = 0.6398163437843323 + 50.0 * 8.967486381530762
Epoch 1550, val loss: 0.7069053053855896
Epoch 1560, training loss: 449.2283630371094 = 0.6374130249023438 + 50.0 * 8.971818923950195
Epoch 1560, val loss: 0.7051876783370972
Epoch 1570, training loss: 449.5941162109375 = 0.6350240111351013 + 50.0 * 8.979182243347168
Epoch 1570, val loss: 0.7035328149795532
Epoch 1580, training loss: 449.38916015625 = 0.6325017213821411 + 50.0 * 8.975132942199707
Epoch 1580, val loss: 0.7017951607704163
Epoch 1590, training loss: 449.69085693359375 = 0.6301398873329163 + 50.0 * 8.98121452331543
Epoch 1590, val loss: 0.7003702521324158
Epoch 1600, training loss: 449.7880859375 = 0.6278132796287537 + 50.0 * 8.983205795288086
Epoch 1600, val loss: 0.698857843875885
Epoch 1610, training loss: 449.95220947265625 = 0.6254808306694031 + 50.0 * 8.986534118652344
Epoch 1610, val loss: 0.6972636580467224
Epoch 1620, training loss: 449.71826171875 = 0.6230936646461487 + 50.0 * 8.981903076171875
Epoch 1620, val loss: 0.6956488490104675
Epoch 1630, training loss: 449.49163818359375 = 0.6206496953964233 + 50.0 * 8.97741985321045
Epoch 1630, val loss: 0.6942673921585083
Epoch 1640, training loss: 449.8336486816406 = 0.6185188293457031 + 50.0 * 8.984302520751953
Epoch 1640, val loss: 0.6926491260528564
Epoch 1650, training loss: 450.1734313964844 = 0.616269052028656 + 50.0 * 8.991143226623535
Epoch 1650, val loss: 0.6913982033729553
Epoch 1660, training loss: 450.11907958984375 = 0.613983690738678 + 50.0 * 8.99010181427002
Epoch 1660, val loss: 0.6897302865982056
Epoch 1670, training loss: 449.973388671875 = 0.6117362380027771 + 50.0 * 8.98723316192627
Epoch 1670, val loss: 0.6883543729782104
Epoch 1680, training loss: 450.43060302734375 = 0.6096140146255493 + 50.0 * 8.996419906616211
Epoch 1680, val loss: 0.6868529319763184
Epoch 1690, training loss: 450.1893310546875 = 0.6073616147041321 + 50.0 * 8.991639137268066
Epoch 1690, val loss: 0.6855882406234741
Epoch 1700, training loss: 449.6920471191406 = 0.6052165627479553 + 50.0 * 8.981736183166504
Epoch 1700, val loss: 0.6837396025657654
Epoch 1710, training loss: 448.96563720703125 = 0.6029585599899292 + 50.0 * 8.967253684997559
Epoch 1710, val loss: 0.682478666305542
Epoch 1720, training loss: 449.4601745605469 = 0.6010029315948486 + 50.0 * 8.97718334197998
Epoch 1720, val loss: 0.681275486946106
Epoch 1730, training loss: 450.1136169433594 = 0.5991263389587402 + 50.0 * 8.990289688110352
Epoch 1730, val loss: 0.6800779104232788
Epoch 1740, training loss: 450.5897216796875 = 0.5970916748046875 + 50.0 * 8.999853134155273
Epoch 1740, val loss: 0.6788125038146973
Epoch 1750, training loss: 450.6880187988281 = 0.5950087308883667 + 50.0 * 9.001860618591309
Epoch 1750, val loss: 0.6775403022766113
Epoch 1760, training loss: 450.83038330078125 = 0.5929444432258606 + 50.0 * 9.004748344421387
Epoch 1760, val loss: 0.6762589812278748
Epoch 1770, training loss: 450.71966552734375 = 0.5908987522125244 + 50.0 * 9.002574920654297
Epoch 1770, val loss: 0.674891471862793
Epoch 1780, training loss: 450.8922119140625 = 0.5888508558273315 + 50.0 * 9.006067276000977
Epoch 1780, val loss: 0.6735674738883972
Epoch 1790, training loss: 450.9134826660156 = 0.5868784189224243 + 50.0 * 9.006531715393066
Epoch 1790, val loss: 0.672236979007721
Epoch 1800, training loss: 451.0751953125 = 0.5848701000213623 + 50.0 * 9.009806632995605
Epoch 1800, val loss: 0.6710759997367859
Epoch 1810, training loss: 451.097412109375 = 0.5829126834869385 + 50.0 * 9.010290145874023
Epoch 1810, val loss: 0.6697421073913574
Epoch 1820, training loss: 451.2714538574219 = 0.5809445977210999 + 50.0 * 9.013810157775879
Epoch 1820, val loss: 0.6685840487480164
Epoch 1830, training loss: 451.3033447265625 = 0.5790471434593201 + 50.0 * 9.014486312866211
Epoch 1830, val loss: 0.6674214005470276
Epoch 1840, training loss: 451.239990234375 = 0.5771695971488953 + 50.0 * 9.013256072998047
Epoch 1840, val loss: 0.6661484241485596
Epoch 1850, training loss: 451.4096374511719 = 0.5752685070037842 + 50.0 * 9.016687393188477
Epoch 1850, val loss: 0.6649699211120605
Epoch 1860, training loss: 451.35955810546875 = 0.5733016133308411 + 50.0 * 9.015725135803223
Epoch 1860, val loss: 0.6637067794799805
Epoch 1870, training loss: 451.3357849121094 = 0.5714068412780762 + 50.0 * 9.015287399291992
Epoch 1870, val loss: 0.6623315215110779
Epoch 1880, training loss: 451.5655212402344 = 0.5695439577102661 + 50.0 * 9.019919395446777
Epoch 1880, val loss: 0.6612468361854553
Epoch 1890, training loss: 451.73583984375 = 0.567726731300354 + 50.0 * 9.023362159729004
Epoch 1890, val loss: 0.6599934697151184
Epoch 1900, training loss: 451.6892395019531 = 0.565851092338562 + 50.0 * 9.022467613220215
Epoch 1900, val loss: 0.6586201786994934
Epoch 1910, training loss: 451.69183349609375 = 0.563938558101654 + 50.0 * 9.022558212280273
Epoch 1910, val loss: 0.6576170325279236
Epoch 1920, training loss: 451.7589416503906 = 0.5622915029525757 + 50.0 * 9.023933410644531
Epoch 1920, val loss: 0.6566774845123291
Epoch 1930, training loss: 452.0794677734375 = 0.5606380701065063 + 50.0 * 9.030376434326172
Epoch 1930, val loss: 0.6553421020507812
Epoch 1940, training loss: 452.3251953125 = 0.558902382850647 + 50.0 * 9.03532600402832
Epoch 1940, val loss: 0.6542880535125732
Epoch 1950, training loss: 452.46527099609375 = 0.5570845603942871 + 50.0 * 9.038163185119629
Epoch 1950, val loss: 0.6529514193534851
Epoch 1960, training loss: 452.20135498046875 = 0.5552006363868713 + 50.0 * 9.032922744750977
Epoch 1960, val loss: 0.6517452001571655
Epoch 1970, training loss: 452.406494140625 = 0.5534891486167908 + 50.0 * 9.037059783935547
Epoch 1970, val loss: 0.6504949927330017
Epoch 1980, training loss: 452.6917419433594 = 0.5517503619194031 + 50.0 * 9.042799949645996
Epoch 1980, val loss: 0.6493918299674988
Epoch 1990, training loss: 452.7603454589844 = 0.5500195622444153 + 50.0 * 9.044206619262695
Epoch 1990, val loss: 0.6482775211334229
Epoch 2000, training loss: 452.737060546875 = 0.5482638478279114 + 50.0 * 9.04377555847168
Epoch 2000, val loss: 0.6471185684204102
Epoch 2010, training loss: 452.721923828125 = 0.5465521216392517 + 50.0 * 9.04350757598877
Epoch 2010, val loss: 0.6459685564041138
Epoch 2020, training loss: 452.893798828125 = 0.5448657870292664 + 50.0 * 9.046978950500488
Epoch 2020, val loss: 0.6449044942855835
Epoch 2030, training loss: 453.0686340332031 = 0.5432100892066956 + 50.0 * 9.050508499145508
Epoch 2030, val loss: 0.6439321041107178
Epoch 2040, training loss: 453.0490417480469 = 0.5414533019065857 + 50.0 * 9.050151824951172
Epoch 2040, val loss: 0.6427323818206787
Epoch 2050, training loss: 452.7486877441406 = 0.5397466421127319 + 50.0 * 9.04417896270752
Epoch 2050, val loss: 0.6417297720909119
Epoch 2060, training loss: 452.9352111816406 = 0.5381359457969666 + 50.0 * 9.047941207885742
Epoch 2060, val loss: 0.6407268643379211
Epoch 2070, training loss: 453.22357177734375 = 0.5365033149719238 + 50.0 * 9.053741455078125
Epoch 2070, val loss: 0.6396291255950928
Epoch 2080, training loss: 453.2720031738281 = 0.5347548723220825 + 50.0 * 9.054744720458984
Epoch 2080, val loss: 0.6384397149085999
Epoch 2090, training loss: 453.18231201171875 = 0.5330504179000854 + 50.0 * 9.052985191345215
Epoch 2090, val loss: 0.6375948190689087
Epoch 2100, training loss: 453.35443115234375 = 0.531399130821228 + 50.0 * 9.0564603805542
Epoch 2100, val loss: 0.636482298374176
Epoch 2110, training loss: 453.3419494628906 = 0.5297049880027771 + 50.0 * 9.056244850158691
Epoch 2110, val loss: 0.6353068351745605
Epoch 2120, training loss: 453.412841796875 = 0.5280283093452454 + 50.0 * 9.057696342468262
Epoch 2120, val loss: 0.6343502402305603
Epoch 2130, training loss: 453.2137756347656 = 0.5264617800712585 + 50.0 * 9.053746223449707
Epoch 2130, val loss: 0.6334916949272156
Epoch 2140, training loss: 453.07342529296875 = 0.5249693393707275 + 50.0 * 9.050969123840332
Epoch 2140, val loss: 0.6323796510696411
Epoch 2150, training loss: 453.32720947265625 = 0.523351788520813 + 50.0 * 9.056077003479004
Epoch 2150, val loss: 0.6313830018043518
Epoch 2160, training loss: 453.6890869140625 = 0.5217273831367493 + 50.0 * 9.063346862792969
Epoch 2160, val loss: 0.6304988861083984
Epoch 2170, training loss: 453.48876953125 = 0.5200198292732239 + 50.0 * 9.059374809265137
Epoch 2170, val loss: 0.629410982131958
Epoch 2180, training loss: 453.6182861328125 = 0.518400251865387 + 50.0 * 9.061997413635254
Epoch 2180, val loss: 0.6285089254379272
Epoch 2190, training loss: 453.9347229003906 = 0.5168005228042603 + 50.0 * 9.068358421325684
Epoch 2190, val loss: 0.627557635307312
Epoch 2200, training loss: 453.0652770996094 = 0.5150205492973328 + 50.0 * 9.051005363464355
Epoch 2200, val loss: 0.6259439587593079
Epoch 2210, training loss: 453.0452575683594 = 0.5134029388427734 + 50.0 * 9.050637245178223
Epoch 2210, val loss: 0.625192403793335
Epoch 2220, training loss: 452.8762512207031 = 0.5120444297790527 + 50.0 * 9.047284126281738
Epoch 2220, val loss: 0.6247736215591431
Epoch 2230, training loss: 453.0238037109375 = 0.5105947256088257 + 50.0 * 9.050264358520508
Epoch 2230, val loss: 0.6238340735435486
Epoch 2240, training loss: 453.526123046875 = 0.5088948607444763 + 50.0 * 9.060344696044922
Epoch 2240, val loss: 0.6230058670043945
Epoch 2250, training loss: 453.8280944824219 = 0.5072691440582275 + 50.0 * 9.06641674041748
Epoch 2250, val loss: 0.6219830513000488
Epoch 2260, training loss: 453.7495422363281 = 0.5055845975875854 + 50.0 * 9.064879417419434
Epoch 2260, val loss: 0.6209773421287537
Epoch 2270, training loss: 453.8397521972656 = 0.5039408206939697 + 50.0 * 9.066716194152832
Epoch 2270, val loss: 0.6200571060180664
Epoch 2280, training loss: 454.1539611816406 = 0.5022878050804138 + 50.0 * 9.073033332824707
Epoch 2280, val loss: 0.6190109848976135
Epoch 2290, training loss: 454.208740234375 = 0.5006190538406372 + 50.0 * 9.074162483215332
Epoch 2290, val loss: 0.6181719899177551
Epoch 2300, training loss: 454.25244140625 = 0.4989736080169678 + 50.0 * 9.075069427490234
Epoch 2300, val loss: 0.6174161434173584
Epoch 2310, training loss: 454.2819519042969 = 0.497293084859848 + 50.0 * 9.075693130493164
Epoch 2310, val loss: 0.6163315176963806
Epoch 2320, training loss: 454.2148132324219 = 0.49566030502319336 + 50.0 * 9.074382781982422
Epoch 2320, val loss: 0.6153993606567383
Epoch 2330, training loss: 454.2286376953125 = 0.49397534132003784 + 50.0 * 9.074692726135254
Epoch 2330, val loss: 0.6144492030143738
Epoch 2340, training loss: 454.3856506347656 = 0.49235883355140686 + 50.0 * 9.077865600585938
Epoch 2340, val loss: 0.6136341094970703
Epoch 2350, training loss: 454.4476318359375 = 0.49070078134536743 + 50.0 * 9.07913875579834
Epoch 2350, val loss: 0.6126668453216553
Epoch 2360, training loss: 454.37786865234375 = 0.48906460404396057 + 50.0 * 9.077775955200195
Epoch 2360, val loss: 0.6117646098136902
Epoch 2370, training loss: 454.24139404296875 = 0.4873890280723572 + 50.0 * 9.075079917907715
Epoch 2370, val loss: 0.6108241677284241
Epoch 2380, training loss: 454.2105407714844 = 0.48580706119537354 + 50.0 * 9.074494361877441
Epoch 2380, val loss: 0.6098422408103943
Epoch 2390, training loss: 454.25146484375 = 0.4842553436756134 + 50.0 * 9.07534408569336
Epoch 2390, val loss: 0.6094618439674377
Epoch 2400, training loss: 454.5373840332031 = 0.48264628648757935 + 50.0 * 9.081094741821289
Epoch 2400, val loss: 0.6086087822914124
Epoch 2410, training loss: 454.7507019042969 = 0.4810396730899811 + 50.0 * 9.085392951965332
Epoch 2410, val loss: 0.607570230960846
Epoch 2420, training loss: 454.51104736328125 = 0.4793180525302887 + 50.0 * 9.080635070800781
Epoch 2420, val loss: 0.6064653396606445
Epoch 2430, training loss: 454.56787109375 = 0.4777442514896393 + 50.0 * 9.081802368164062
Epoch 2430, val loss: 0.605854868888855
Epoch 2440, training loss: 454.7880554199219 = 0.47616106271743774 + 50.0 * 9.086237907409668
Epoch 2440, val loss: 0.604951024055481
Epoch 2450, training loss: 454.9009094238281 = 0.47451111674308777 + 50.0 * 9.08852767944336
Epoch 2450, val loss: 0.6040268540382385
Epoch 2460, training loss: 454.8148193359375 = 0.4728683829307556 + 50.0 * 9.086838722229004
Epoch 2460, val loss: 0.603118360042572
Epoch 2470, training loss: 454.9347229003906 = 0.47123730182647705 + 50.0 * 9.089269638061523
Epoch 2470, val loss: 0.6022922992706299
Epoch 2480, training loss: 455.01690673828125 = 0.46961715817451477 + 50.0 * 9.090946197509766
Epoch 2480, val loss: 0.6015865206718445
Epoch 2490, training loss: 454.8051452636719 = 0.46798065304756165 + 50.0 * 9.086743354797363
Epoch 2490, val loss: 0.6008536219596863
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7639130434782608
0.8146055205390134
=== training gcn model ===
Epoch 0, training loss: 512.1878051757812 = 1.0897502899169922 + 50.0 * 10.22196102142334
Epoch 0, val loss: 1.0880882740020752
Epoch 10, training loss: 491.7434387207031 = 1.0868861675262451 + 50.0 * 9.813131332397461
Epoch 10, val loss: 1.0852937698364258
Epoch 20, training loss: 482.3354187011719 = 1.0841459035873413 + 50.0 * 9.625025749206543
Epoch 20, val loss: 1.082629919052124
Epoch 30, training loss: 475.57366943359375 = 1.0815463066101074 + 50.0 * 9.489842414855957
Epoch 30, val loss: 1.08010733127594
Epoch 40, training loss: 470.27447509765625 = 1.07908034324646 + 50.0 * 9.38390827178955
Epoch 40, val loss: 1.0777242183685303
Epoch 50, training loss: 465.8149108886719 = 1.0767335891723633 + 50.0 * 9.294763565063477
Epoch 50, val loss: 1.0754680633544922
Epoch 60, training loss: 462.0438537597656 = 1.0744980573654175 + 50.0 * 9.21938705444336
Epoch 60, val loss: 1.0733264684677124
Epoch 70, training loss: 458.8414306640625 = 1.0723600387573242 + 50.0 * 9.155381202697754
Epoch 70, val loss: 1.0712895393371582
Epoch 80, training loss: 456.0489807128906 = 1.070319414138794 + 50.0 * 9.099573135375977
Epoch 80, val loss: 1.0693527460098267
Epoch 90, training loss: 453.62371826171875 = 1.0683575868606567 + 50.0 * 9.051107406616211
Epoch 90, val loss: 1.0675034523010254
Epoch 100, training loss: 451.5944519042969 = 1.0664845705032349 + 50.0 * 9.01055908203125
Epoch 100, val loss: 1.0657447576522827
Epoch 110, training loss: 449.78643798828125 = 1.0646870136260986 + 50.0 * 8.974434852600098
Epoch 110, val loss: 1.0640639066696167
Epoch 120, training loss: 448.2400207519531 = 1.0629712343215942 + 50.0 * 8.943541526794434
Epoch 120, val loss: 1.0624662637710571
Epoch 130, training loss: 446.8129577636719 = 1.061281681060791 + 50.0 * 8.915033340454102
Epoch 130, val loss: 1.0609021186828613
Epoch 140, training loss: 445.6441955566406 = 1.0596997737884521 + 50.0 * 8.891690254211426
Epoch 140, val loss: 1.0594252347946167
Epoch 150, training loss: 444.6104736328125 = 1.058142900466919 + 50.0 * 8.871047019958496
Epoch 150, val loss: 1.057978868484497
Epoch 160, training loss: 443.8551940917969 = 1.0566433668136597 + 50.0 * 8.855971336364746
Epoch 160, val loss: 1.0565800666809082
Epoch 170, training loss: 443.1312255859375 = 1.0551193952560425 + 50.0 * 8.841522216796875
Epoch 170, val loss: 1.0551471710205078
Epoch 180, training loss: 442.6048889160156 = 1.0536243915557861 + 50.0 * 8.831025123596191
Epoch 180, val loss: 1.0537546873092651
Epoch 190, training loss: 441.99700927734375 = 1.052150011062622 + 50.0 * 8.818897247314453
Epoch 190, val loss: 1.0524003505706787
Epoch 200, training loss: 441.63336181640625 = 1.0506070852279663 + 50.0 * 8.811655044555664
Epoch 200, val loss: 1.0509740114212036
Epoch 210, training loss: 441.15728759765625 = 1.0491479635238647 + 50.0 * 8.802163124084473
Epoch 210, val loss: 1.049565076828003
Epoch 220, training loss: 440.7522277832031 = 1.0475878715515137 + 50.0 * 8.794093132019043
Epoch 220, val loss: 1.0480989217758179
Epoch 230, training loss: 440.38848876953125 = 1.0459848642349243 + 50.0 * 8.786849975585938
Epoch 230, val loss: 1.0465755462646484
Epoch 240, training loss: 440.16339111328125 = 1.0442774295806885 + 50.0 * 8.782382011413574
Epoch 240, val loss: 1.044979214668274
Epoch 250, training loss: 439.8769226074219 = 1.042479395866394 + 50.0 * 8.776688575744629
Epoch 250, val loss: 1.0432630777359009
Epoch 260, training loss: 439.557373046875 = 1.0405528545379639 + 50.0 * 8.770336151123047
Epoch 260, val loss: 1.0414034128189087
Epoch 270, training loss: 439.6882019042969 = 1.0386494398117065 + 50.0 * 8.772991180419922
Epoch 270, val loss: 1.039549708366394
Epoch 280, training loss: 439.41656494140625 = 1.0364586114883423 + 50.0 * 8.76760196685791
Epoch 280, val loss: 1.0374737977981567
Epoch 290, training loss: 439.3016052246094 = 1.03414785861969 + 50.0 * 8.765349388122559
Epoch 290, val loss: 1.0352284908294678
Epoch 300, training loss: 439.2686767578125 = 1.0317575931549072 + 50.0 * 8.764738082885742
Epoch 300, val loss: 1.032910943031311
Epoch 310, training loss: 439.2290954589844 = 1.0290806293487549 + 50.0 * 8.763999938964844
Epoch 310, val loss: 1.0303657054901123
Epoch 320, training loss: 439.1565856933594 = 1.0263421535491943 + 50.0 * 8.762604713439941
Epoch 320, val loss: 1.0276986360549927
Epoch 330, training loss: 439.2276916503906 = 1.0233315229415894 + 50.0 * 8.764086723327637
Epoch 330, val loss: 1.0248103141784668
Epoch 340, training loss: 438.969970703125 = 1.0201915502548218 + 50.0 * 8.758995056152344
Epoch 340, val loss: 1.0217989683151245
Epoch 350, training loss: 438.8286437988281 = 1.0167874097824097 + 50.0 * 8.756237030029297
Epoch 350, val loss: 1.018498420715332
Epoch 360, training loss: 438.80352783203125 = 1.0131906270980835 + 50.0 * 8.755806922912598
Epoch 360, val loss: 1.0149976015090942
Epoch 370, training loss: 438.85943603515625 = 1.009421467781067 + 50.0 * 8.756999969482422
Epoch 370, val loss: 1.0114257335662842
Epoch 380, training loss: 438.7072448730469 = 1.0053060054779053 + 50.0 * 8.75403881072998
Epoch 380, val loss: 1.007528305053711
Epoch 390, training loss: 438.6804504394531 = 1.0010838508605957 + 50.0 * 8.753586769104004
Epoch 390, val loss: 1.00343656539917
Epoch 400, training loss: 438.8612976074219 = 0.9967182874679565 + 50.0 * 8.757291793823242
Epoch 400, val loss: 0.999213457107544
Epoch 410, training loss: 439.1829833984375 = 0.9920833706855774 + 50.0 * 8.76381778717041
Epoch 410, val loss: 0.9947922825813293
Epoch 420, training loss: 439.20562744140625 = 0.9871861934661865 + 50.0 * 8.764369010925293
Epoch 420, val loss: 0.9901199340820312
Epoch 430, training loss: 439.17327880859375 = 0.9820025563240051 + 50.0 * 8.763825416564941
Epoch 430, val loss: 0.9850967526435852
Epoch 440, training loss: 439.2923889160156 = 0.9767331480979919 + 50.0 * 8.766312599182129
Epoch 440, val loss: 0.9801672697067261
Epoch 450, training loss: 439.3131103515625 = 0.9709981679916382 + 50.0 * 8.766841888427734
Epoch 450, val loss: 0.9746088981628418
Epoch 460, training loss: 439.4093322753906 = 0.9652323722839355 + 50.0 * 8.768881797790527
Epoch 460, val loss: 0.969202995300293
Epoch 470, training loss: 439.218505859375 = 0.9590977430343628 + 50.0 * 8.765188217163086
Epoch 470, val loss: 0.9634371995925903
Epoch 480, training loss: 439.3298034667969 = 0.9529040455818176 + 50.0 * 8.767538070678711
Epoch 480, val loss: 0.9575207829475403
Epoch 490, training loss: 439.3117980957031 = 0.9464730620384216 + 50.0 * 8.767306327819824
Epoch 490, val loss: 0.9514279961585999
Epoch 500, training loss: 439.5515441894531 = 0.9399371147155762 + 50.0 * 8.772232055664062
Epoch 500, val loss: 0.9452599287033081
Epoch 510, training loss: 439.5439147949219 = 0.9330815076828003 + 50.0 * 8.772216796875
Epoch 510, val loss: 0.9387891292572021
Epoch 520, training loss: 439.5794372558594 = 0.9261437654495239 + 50.0 * 8.773065567016602
Epoch 520, val loss: 0.9323264956474304
Epoch 530, training loss: 439.8364562988281 = 0.9191039204597473 + 50.0 * 8.77834701538086
Epoch 530, val loss: 0.9259276986122131
Epoch 540, training loss: 439.6606140136719 = 0.9121060967445374 + 50.0 * 8.774970054626465
Epoch 540, val loss: 0.9193003177642822
Epoch 550, training loss: 439.634765625 = 0.904964029788971 + 50.0 * 8.774596214294434
Epoch 550, val loss: 0.9125791192054749
Epoch 560, training loss: 439.8084716796875 = 0.8977183103561401 + 50.0 * 8.778215408325195
Epoch 560, val loss: 0.9058167934417725
Epoch 570, training loss: 439.6235656738281 = 0.8903166651725769 + 50.0 * 8.774664878845215
Epoch 570, val loss: 0.8989847302436829
Epoch 580, training loss: 439.82427978515625 = 0.8830351829528809 + 50.0 * 8.778824806213379
Epoch 580, val loss: 0.8923172950744629
Epoch 590, training loss: 439.91845703125 = 0.8758975267410278 + 50.0 * 8.780851364135742
Epoch 590, val loss: 0.885740339756012
Epoch 600, training loss: 439.9186096191406 = 0.8685728311538696 + 50.0 * 8.781001091003418
Epoch 600, val loss: 0.8788821697235107
Epoch 610, training loss: 440.09283447265625 = 0.8612435460090637 + 50.0 * 8.784631729125977
Epoch 610, val loss: 0.8723475337028503
Epoch 620, training loss: 440.28912353515625 = 0.854152500629425 + 50.0 * 8.78869915008545
Epoch 620, val loss: 0.8658468723297119
Epoch 630, training loss: 440.35107421875 = 0.8468444347381592 + 50.0 * 8.790084838867188
Epoch 630, val loss: 0.8591558933258057
Epoch 640, training loss: 440.4898681640625 = 0.8400709629058838 + 50.0 * 8.792996406555176
Epoch 640, val loss: 0.8530800938606262
Epoch 650, training loss: 440.4032897949219 = 0.8331946730613708 + 50.0 * 8.791401863098145
Epoch 650, val loss: 0.8469406962394714
Epoch 660, training loss: 440.4902038574219 = 0.8263121843338013 + 50.0 * 8.793277740478516
Epoch 660, val loss: 0.8408495783805847
Epoch 670, training loss: 440.54864501953125 = 0.8195806741714478 + 50.0 * 8.794581413269043
Epoch 670, val loss: 0.8348286747932434
Epoch 680, training loss: 440.5978698730469 = 0.8130276203155518 + 50.0 * 8.795697212219238
Epoch 680, val loss: 0.8289156556129456
Epoch 690, training loss: 440.8305969238281 = 0.8065269589424133 + 50.0 * 8.800481796264648
Epoch 690, val loss: 0.8231062293052673
Epoch 700, training loss: 440.5876770019531 = 0.7998030185699463 + 50.0 * 8.795757293701172
Epoch 700, val loss: 0.8171917200088501
Epoch 710, training loss: 440.7682189941406 = 0.7937622666358948 + 50.0 * 8.79948902130127
Epoch 710, val loss: 0.8118715286254883
Epoch 720, training loss: 441.0336608886719 = 0.7877563238143921 + 50.0 * 8.80491828918457
Epoch 720, val loss: 0.8066518902778625
Epoch 730, training loss: 441.1054382324219 = 0.7816945314407349 + 50.0 * 8.806474685668945
Epoch 730, val loss: 0.8013468980789185
Epoch 740, training loss: 441.21478271484375 = 0.7758596539497375 + 50.0 * 8.808778762817383
Epoch 740, val loss: 0.7960854768753052
Epoch 750, training loss: 441.2055969238281 = 0.7700377702713013 + 50.0 * 8.808711051940918
Epoch 750, val loss: 0.7912190556526184
Epoch 760, training loss: 441.2208251953125 = 0.7644922137260437 + 50.0 * 8.809126853942871
Epoch 760, val loss: 0.7865254282951355
Epoch 770, training loss: 441.3372497558594 = 0.7589730024337769 + 50.0 * 8.811565399169922
Epoch 770, val loss: 0.782007098197937
Epoch 780, training loss: 440.2762451171875 = 0.7521032094955444 + 50.0 * 8.790482521057129
Epoch 780, val loss: 0.7760342955589294
Epoch 790, training loss: 441.07720947265625 = 0.7479605674743652 + 50.0 * 8.806585311889648
Epoch 790, val loss: 0.7721936106681824
Epoch 800, training loss: 441.3210144042969 = 0.7429749369621277 + 50.0 * 8.81156063079834
Epoch 800, val loss: 0.76833176612854
Epoch 810, training loss: 441.15203857421875 = 0.7380881905555725 + 50.0 * 8.808279037475586
Epoch 810, val loss: 0.7640095949172974
Epoch 820, training loss: 441.40142822265625 = 0.7333611845970154 + 50.0 * 8.813361167907715
Epoch 820, val loss: 0.7599818706512451
Epoch 830, training loss: 441.57904052734375 = 0.7283830046653748 + 50.0 * 8.817012786865234
Epoch 830, val loss: 0.7556407451629639
Epoch 840, training loss: 442.0865173339844 = 0.7241570353507996 + 50.0 * 8.827247619628906
Epoch 840, val loss: 0.752360463142395
Epoch 850, training loss: 441.5108337402344 = 0.7194187045097351 + 50.0 * 8.815828323364258
Epoch 850, val loss: 0.7484456300735474
Epoch 860, training loss: 442.0496520996094 = 0.7150277495384216 + 50.0 * 8.826692581176758
Epoch 860, val loss: 0.744763195514679
Epoch 870, training loss: 442.1619567871094 = 0.7107987403869629 + 50.0 * 8.829023361206055
Epoch 870, val loss: 0.7414314150810242
Epoch 880, training loss: 442.5478210449219 = 0.7064990401268005 + 50.0 * 8.83682632446289
Epoch 880, val loss: 0.7378540635108948
Epoch 890, training loss: 442.49432373046875 = 0.70228511095047 + 50.0 * 8.835841178894043
Epoch 890, val loss: 0.7344984412193298
Epoch 900, training loss: 442.5959167480469 = 0.698257565498352 + 50.0 * 8.837952613830566
Epoch 900, val loss: 0.7312003374099731
Epoch 910, training loss: 442.7740783691406 = 0.6942566633224487 + 50.0 * 8.841596603393555
Epoch 910, val loss: 0.728169322013855
Epoch 920, training loss: 442.7711486816406 = 0.6903988718986511 + 50.0 * 8.841614723205566
Epoch 920, val loss: 0.725080668926239
Epoch 930, training loss: 442.92169189453125 = 0.686653733253479 + 50.0 * 8.844700813293457
Epoch 930, val loss: 0.7221705317497253
Epoch 940, training loss: 443.2342529296875 = 0.6830081343650818 + 50.0 * 8.851024627685547
Epoch 940, val loss: 0.7192224264144897
Epoch 950, training loss: 443.2327575683594 = 0.6793751120567322 + 50.0 * 8.851067543029785
Epoch 950, val loss: 0.7165108323097229
Epoch 960, training loss: 443.3211364746094 = 0.6759300827980042 + 50.0 * 8.852904319763184
Epoch 960, val loss: 0.7138974070549011
Epoch 970, training loss: 443.3598937988281 = 0.6725296378135681 + 50.0 * 8.853747367858887
Epoch 970, val loss: 0.711229145526886
Epoch 980, training loss: 443.568359375 = 0.6692169308662415 + 50.0 * 8.857982635498047
Epoch 980, val loss: 0.70862877368927
Epoch 990, training loss: 443.2702331542969 = 0.6658002138137817 + 50.0 * 8.852088928222656
Epoch 990, val loss: 0.7061337828636169
Epoch 1000, training loss: 443.31121826171875 = 0.6627503037452698 + 50.0 * 8.8529691696167
Epoch 1000, val loss: 0.7038699388504028
Epoch 1010, training loss: 443.99163818359375 = 0.6596758365631104 + 50.0 * 8.866639137268066
Epoch 1010, val loss: 0.7015022039413452
Epoch 1020, training loss: 443.79156494140625 = 0.6566600799560547 + 50.0 * 8.862698554992676
Epoch 1020, val loss: 0.6992664337158203
Epoch 1030, training loss: 444.0764465332031 = 0.6538109183311462 + 50.0 * 8.868453025817871
Epoch 1030, val loss: 0.6972548365592957
Epoch 1040, training loss: 444.38311767578125 = 0.6510915756225586 + 50.0 * 8.874640464782715
Epoch 1040, val loss: 0.6952574253082275
Epoch 1050, training loss: 444.4537658691406 = 0.6483350396156311 + 50.0 * 8.876108169555664
Epoch 1050, val loss: 0.6932060718536377
Epoch 1060, training loss: 444.3096008300781 = 0.6454687714576721 + 50.0 * 8.873282432556152
Epoch 1060, val loss: 0.6911907196044922
Epoch 1070, training loss: 444.56524658203125 = 0.642918586730957 + 50.0 * 8.878446578979492
Epoch 1070, val loss: 0.6894672513008118
Epoch 1080, training loss: 444.706787109375 = 0.640407383441925 + 50.0 * 8.881327629089355
Epoch 1080, val loss: 0.687776505947113
Epoch 1090, training loss: 444.5118408203125 = 0.638680100440979 + 50.0 * 8.877463340759277
Epoch 1090, val loss: 0.6865124106407166
Epoch 1100, training loss: 444.1383056640625 = 0.6361836194992065 + 50.0 * 8.87004280090332
Epoch 1100, val loss: 0.6849477291107178
Epoch 1110, training loss: 444.6858215332031 = 0.6340309381484985 + 50.0 * 8.881035804748535
Epoch 1110, val loss: 0.6832848191261292
Epoch 1120, training loss: 444.99310302734375 = 0.6317927837371826 + 50.0 * 8.887226104736328
Epoch 1120, val loss: 0.6817833781242371
Epoch 1130, training loss: 445.2936706542969 = 0.6295030117034912 + 50.0 * 8.893282890319824
Epoch 1130, val loss: 0.6802734136581421
Epoch 1140, training loss: 445.36602783203125 = 0.6272624135017395 + 50.0 * 8.894775390625
Epoch 1140, val loss: 0.6787760853767395
Epoch 1150, training loss: 445.4861755371094 = 0.6250825524330139 + 50.0 * 8.897221565246582
Epoch 1150, val loss: 0.6772885918617249
Epoch 1160, training loss: 445.3207702636719 = 0.6227214336395264 + 50.0 * 8.893960952758789
Epoch 1160, val loss: 0.6756834387779236
Epoch 1170, training loss: 445.37762451171875 = 0.620734691619873 + 50.0 * 8.895137786865234
Epoch 1170, val loss: 0.6745465397834778
Epoch 1180, training loss: 445.7806091308594 = 0.6188207864761353 + 50.0 * 8.90323543548584
Epoch 1180, val loss: 0.6732618808746338
Epoch 1190, training loss: 445.95703125 = 0.6169894337654114 + 50.0 * 8.906800270080566
Epoch 1190, val loss: 0.6720519661903381
Epoch 1200, training loss: 445.7396545410156 = 0.6151191592216492 + 50.0 * 8.902490615844727
Epoch 1200, val loss: 0.670914888381958
Epoch 1210, training loss: 445.9136657714844 = 0.613365888595581 + 50.0 * 8.906005859375
Epoch 1210, val loss: 0.6699394583702087
Epoch 1220, training loss: 446.3299560546875 = 0.6116554141044617 + 50.0 * 8.914365768432617
Epoch 1220, val loss: 0.6688615083694458
Epoch 1230, training loss: 446.26824951171875 = 0.6099718809127808 + 50.0 * 8.913165092468262
Epoch 1230, val loss: 0.6677752137184143
Epoch 1240, training loss: 446.3910827636719 = 0.6083372831344604 + 50.0 * 8.915655136108398
Epoch 1240, val loss: 0.6667949557304382
Epoch 1250, training loss: 446.21551513671875 = 0.6065669059753418 + 50.0 * 8.912178993225098
Epoch 1250, val loss: 0.6656821966171265
Epoch 1260, training loss: 446.2120056152344 = 0.60495525598526 + 50.0 * 8.912140846252441
Epoch 1260, val loss: 0.6647709012031555
Epoch 1270, training loss: 446.60821533203125 = 0.6034965515136719 + 50.0 * 8.92009449005127
Epoch 1270, val loss: 0.6638555526733398
Epoch 1280, training loss: 446.593994140625 = 0.6019764542579651 + 50.0 * 8.919839859008789
Epoch 1280, val loss: 0.6628834009170532
Epoch 1290, training loss: 446.64617919921875 = 0.600565493106842 + 50.0 * 8.920912742614746
Epoch 1290, val loss: 0.6621911525726318
Epoch 1300, training loss: 446.8952941894531 = 0.5991865992546082 + 50.0 * 8.925922393798828
Epoch 1300, val loss: 0.6612915396690369
Epoch 1310, training loss: 446.9907531738281 = 0.5977619886398315 + 50.0 * 8.927860260009766
Epoch 1310, val loss: 0.6605489253997803
Epoch 1320, training loss: 446.9471435546875 = 0.5963414907455444 + 50.0 * 8.927016258239746
Epoch 1320, val loss: 0.6596794724464417
Epoch 1330, training loss: 447.2303161621094 = 0.5950629115104675 + 50.0 * 8.93270492553711
Epoch 1330, val loss: 0.6589694023132324
Epoch 1340, training loss: 447.2247009277344 = 0.593683660030365 + 50.0 * 8.93262004852295
Epoch 1340, val loss: 0.6581570506095886
Epoch 1350, training loss: 447.1985168457031 = 0.5924203395843506 + 50.0 * 8.932122230529785
Epoch 1350, val loss: 0.6575170159339905
Epoch 1360, training loss: 447.54949951171875 = 0.591141402721405 + 50.0 * 8.939167022705078
Epoch 1360, val loss: 0.6567423939704895
Epoch 1370, training loss: 447.515380859375 = 0.5898852348327637 + 50.0 * 8.938509941101074
Epoch 1370, val loss: 0.6560092568397522
Epoch 1380, training loss: 447.5341796875 = 0.5886097550392151 + 50.0 * 8.938911437988281
Epoch 1380, val loss: 0.6552778482437134
Epoch 1390, training loss: 447.7439880371094 = 0.587449848651886 + 50.0 * 8.943130493164062
Epoch 1390, val loss: 0.6546468734741211
Epoch 1400, training loss: 447.841552734375 = 0.5862563848495483 + 50.0 * 8.945106506347656
Epoch 1400, val loss: 0.6540175080299377
Epoch 1410, training loss: 447.9002685546875 = 0.5850405693054199 + 50.0 * 8.946304321289062
Epoch 1410, val loss: 0.6532883644104004
Epoch 1420, training loss: 447.9255676269531 = 0.5839017629623413 + 50.0 * 8.946833610534668
Epoch 1420, val loss: 0.6526514887809753
Epoch 1430, training loss: 447.9516296386719 = 0.5827721357345581 + 50.0 * 8.94737720489502
Epoch 1430, val loss: 0.6517835259437561
Epoch 1440, training loss: 447.1387023925781 = 0.5818336606025696 + 50.0 * 8.931137084960938
Epoch 1440, val loss: 0.6522865295410156
Epoch 1450, training loss: 447.28375244140625 = 0.5806653499603271 + 50.0 * 8.934062004089355
Epoch 1450, val loss: 0.6510612368583679
Epoch 1460, training loss: 447.56103515625 = 0.5794958472251892 + 50.0 * 8.939630508422852
Epoch 1460, val loss: 0.6497781276702881
Epoch 1470, training loss: 447.5970153808594 = 0.5784066915512085 + 50.0 * 8.940372467041016
Epoch 1470, val loss: 0.6497198343276978
Epoch 1480, training loss: 447.4905700683594 = 0.5771734714508057 + 50.0 * 8.938267707824707
Epoch 1480, val loss: 0.6484167575836182
Epoch 1490, training loss: 447.84344482421875 = 0.5762038230895996 + 50.0 * 8.945344924926758
Epoch 1490, val loss: 0.648290753364563
Epoch 1500, training loss: 448.1006164550781 = 0.5752300024032593 + 50.0 * 8.950508117675781
Epoch 1500, val loss: 0.6478040218353271
Epoch 1510, training loss: 448.3039855957031 = 0.5742149353027344 + 50.0 * 8.954595565795898
Epoch 1510, val loss: 0.6470885276794434
Epoch 1520, training loss: 448.3378601074219 = 0.5732098817825317 + 50.0 * 8.955292701721191
Epoch 1520, val loss: 0.646618664264679
Epoch 1530, training loss: 448.26666259765625 = 0.5721859335899353 + 50.0 * 8.953889846801758
Epoch 1530, val loss: 0.6460570693016052
Epoch 1540, training loss: 448.4566345214844 = 0.5712182521820068 + 50.0 * 8.957708358764648
Epoch 1540, val loss: 0.645518958568573
Epoch 1550, training loss: 448.8040466308594 = 0.5701791048049927 + 50.0 * 8.964676856994629
Epoch 1550, val loss: 0.6455832123756409
Epoch 1560, training loss: 448.359130859375 = 0.5691007971763611 + 50.0 * 8.955801010131836
Epoch 1560, val loss: 0.6428113579750061
Epoch 1570, training loss: 447.76226806640625 = 0.5678409934043884 + 50.0 * 8.943888664245605
Epoch 1570, val loss: 0.6442546844482422
Epoch 1580, training loss: 447.9148254394531 = 0.5668101906776428 + 50.0 * 8.94696044921875
Epoch 1580, val loss: 0.6427921056747437
Epoch 1590, training loss: 448.16473388671875 = 0.5659738183021545 + 50.0 * 8.951974868774414
Epoch 1590, val loss: 0.6423933506011963
Epoch 1600, training loss: 448.5935974121094 = 0.5651726722717285 + 50.0 * 8.96056842803955
Epoch 1600, val loss: 0.6422090530395508
Epoch 1610, training loss: 448.9503479003906 = 0.5642946362495422 + 50.0 * 8.967720985412598
Epoch 1610, val loss: 0.6414974927902222
Epoch 1620, training loss: 448.42803955078125 = 0.563361406326294 + 50.0 * 8.957293510437012
Epoch 1620, val loss: 0.6413165926933289
Epoch 1630, training loss: 448.5978088378906 = 0.5624316334724426 + 50.0 * 8.960707664489746
Epoch 1630, val loss: 0.6405954360961914
Epoch 1640, training loss: 448.9695129394531 = 0.5615085363388062 + 50.0 * 8.968159675598145
Epoch 1640, val loss: 0.6400482654571533
Epoch 1650, training loss: 449.18218994140625 = 0.5605480670928955 + 50.0 * 8.972433090209961
Epoch 1650, val loss: 0.6395028233528137
Epoch 1660, training loss: 449.2330322265625 = 0.5595535039901733 + 50.0 * 8.973469734191895
Epoch 1660, val loss: 0.6390270590782166
Epoch 1670, training loss: 449.08489990234375 = 0.5585704445838928 + 50.0 * 8.970526695251465
Epoch 1670, val loss: 0.6383147835731506
Epoch 1680, training loss: 449.2154846191406 = 0.5576789975166321 + 50.0 * 8.973155975341797
Epoch 1680, val loss: 0.637912929058075
Epoch 1690, training loss: 449.44873046875 = 0.5567741990089417 + 50.0 * 8.977839469909668
Epoch 1690, val loss: 0.6375302076339722
Epoch 1700, training loss: 449.5260925292969 = 0.5558115243911743 + 50.0 * 8.979405403137207
Epoch 1700, val loss: 0.6369684934616089
Epoch 1710, training loss: 449.5740051269531 = 0.5548774600028992 + 50.0 * 8.980382919311523
Epoch 1710, val loss: 0.6365252137184143
Epoch 1720, training loss: 449.8279113769531 = 0.5539458990097046 + 50.0 * 8.985479354858398
Epoch 1720, val loss: 0.6359018683433533
Epoch 1730, training loss: 449.6565856933594 = 0.5529564619064331 + 50.0 * 8.982072830200195
Epoch 1730, val loss: 0.6352465748786926
Epoch 1740, training loss: 449.52294921875 = 0.5519684553146362 + 50.0 * 8.979419708251953
Epoch 1740, val loss: 0.6349666118621826
Epoch 1750, training loss: 449.8252868652344 = 0.5510843396186829 + 50.0 * 8.98548412322998
Epoch 1750, val loss: 0.634365439414978
Epoch 1760, training loss: 450.1040954589844 = 0.55014568567276 + 50.0 * 8.991079330444336
Epoch 1760, val loss: 0.6339731216430664
Epoch 1770, training loss: 449.70172119140625 = 0.5491706132888794 + 50.0 * 8.983051300048828
Epoch 1770, val loss: 0.6335068941116333
Epoch 1780, training loss: 449.8672180175781 = 0.5482366681098938 + 50.0 * 8.986379623413086
Epoch 1780, val loss: 0.6330181956291199
Epoch 1790, training loss: 450.2514953613281 = 0.5473220944404602 + 50.0 * 8.994083404541016
Epoch 1790, val loss: 0.6324054002761841
Epoch 1800, training loss: 450.3376770019531 = 0.5463590025901794 + 50.0 * 8.995826721191406
Epoch 1800, val loss: 0.6318942904472351
Epoch 1810, training loss: 450.2271728515625 = 0.5453180074691772 + 50.0 * 8.993637084960938
Epoch 1810, val loss: 0.631336510181427
Epoch 1820, training loss: 450.23114013671875 = 0.5443862676620483 + 50.0 * 8.993735313415527
Epoch 1820, val loss: 0.6306979060173035
Epoch 1830, training loss: 450.5105285644531 = 0.5434231162071228 + 50.0 * 8.99934196472168
Epoch 1830, val loss: 0.6302094459533691
Epoch 1840, training loss: 450.57611083984375 = 0.5424485206604004 + 50.0 * 9.000673294067383
Epoch 1840, val loss: 0.6296276450157166
Epoch 1850, training loss: 450.3924255371094 = 0.5414391160011292 + 50.0 * 8.99701976776123
Epoch 1850, val loss: 0.6291429996490479
Epoch 1860, training loss: 450.65802001953125 = 0.5404607057571411 + 50.0 * 9.002350807189941
Epoch 1860, val loss: 0.628509521484375
Epoch 1870, training loss: 450.7868347167969 = 0.5394902229309082 + 50.0 * 9.0049467086792
Epoch 1870, val loss: 0.6280150413513184
Epoch 1880, training loss: 450.6940612792969 = 0.5384681820869446 + 50.0 * 9.003111839294434
Epoch 1880, val loss: 0.6273607611656189
Epoch 1890, training loss: 450.76690673828125 = 0.5375126004219055 + 50.0 * 9.00458812713623
Epoch 1890, val loss: 0.6269750595092773
Epoch 1900, training loss: 451.0340576171875 = 0.5365396738052368 + 50.0 * 9.009950637817383
Epoch 1900, val loss: 0.6262837648391724
Epoch 1910, training loss: 451.0126647949219 = 0.5355522036552429 + 50.0 * 9.009542465209961
Epoch 1910, val loss: 0.6257491707801819
Epoch 1920, training loss: 450.98834228515625 = 0.5345805883407593 + 50.0 * 9.009075164794922
Epoch 1920, val loss: 0.6251571774482727
Epoch 1930, training loss: 450.73529052734375 = 0.5336167216300964 + 50.0 * 9.004033088684082
Epoch 1930, val loss: 0.6245925426483154
Epoch 1940, training loss: 450.3132019042969 = 0.5326448678970337 + 50.0 * 8.995611190795898
Epoch 1940, val loss: 0.6242124438285828
Epoch 1950, training loss: 450.33404541015625 = 0.5317334532737732 + 50.0 * 8.99604606628418
Epoch 1950, val loss: 0.623566210269928
Epoch 1960, training loss: 450.6488037109375 = 0.5308048725128174 + 50.0 * 9.002360343933105
Epoch 1960, val loss: 0.6231522560119629
Epoch 1970, training loss: 451.02691650390625 = 0.5298214554786682 + 50.0 * 9.009942054748535
Epoch 1970, val loss: 0.6225026845932007
Epoch 1980, training loss: 451.08636474609375 = 0.5288131237030029 + 50.0 * 9.011151313781738
Epoch 1980, val loss: 0.6219503879547119
Epoch 1990, training loss: 451.186279296875 = 0.5278109908103943 + 50.0 * 9.013169288635254
Epoch 1990, val loss: 0.6214653253555298
Epoch 2000, training loss: 451.41546630859375 = 0.5267986059188843 + 50.0 * 9.017773628234863
Epoch 2000, val loss: 0.6208435297012329
Epoch 2010, training loss: 451.42547607421875 = 0.525783360004425 + 50.0 * 9.017993927001953
Epoch 2010, val loss: 0.6202477812767029
Epoch 2020, training loss: 451.15716552734375 = 0.5247020125389099 + 50.0 * 9.012649536132812
Epoch 2020, val loss: 0.6197586059570312
Epoch 2030, training loss: 451.2709045410156 = 0.5237195491790771 + 50.0 * 9.014944076538086
Epoch 2030, val loss: 0.619448721408844
Epoch 2040, training loss: 451.51898193359375 = 0.5227426290512085 + 50.0 * 9.019925117492676
Epoch 2040, val loss: 0.6187856793403625
Epoch 2050, training loss: 451.361083984375 = 0.5217008590698242 + 50.0 * 9.0167875289917
Epoch 2050, val loss: 0.6183831691741943
Epoch 2060, training loss: 451.5633850097656 = 0.5207246541976929 + 50.0 * 9.020853042602539
Epoch 2060, val loss: 0.6177785396575928
Epoch 2070, training loss: 451.37139892578125 = 0.5196976661682129 + 50.0 * 9.017034530639648
Epoch 2070, val loss: 0.6174536943435669
Epoch 2080, training loss: 451.5772399902344 = 0.5187113881111145 + 50.0 * 9.021170616149902
Epoch 2080, val loss: 0.6167502999305725
Epoch 2090, training loss: 451.8010559082031 = 0.5177288055419922 + 50.0 * 9.025666236877441
Epoch 2090, val loss: 0.6162137389183044
Epoch 2100, training loss: 451.8444519042969 = 0.5166741013526917 + 50.0 * 9.026556015014648
Epoch 2100, val loss: 0.61549311876297
Epoch 2110, training loss: 451.5367431640625 = 0.5155004262924194 + 50.0 * 9.020424842834473
Epoch 2110, val loss: 0.6145607829093933
Epoch 2120, training loss: 451.1083984375 = 0.5144150257110596 + 50.0 * 9.011879920959473
Epoch 2120, val loss: 0.6140512824058533
Epoch 2130, training loss: 451.1775817871094 = 0.5135853886604309 + 50.0 * 9.013279914855957
Epoch 2130, val loss: 0.6139633655548096
Epoch 2140, training loss: 451.0983581542969 = 0.5126808285713196 + 50.0 * 9.011713027954102
Epoch 2140, val loss: 0.6136872172355652
Epoch 2150, training loss: 451.427978515625 = 0.5116936564445496 + 50.0 * 9.018325805664062
Epoch 2150, val loss: 0.6129153370857239
Epoch 2160, training loss: 451.7570495605469 = 0.5106517672538757 + 50.0 * 9.024928092956543
Epoch 2160, val loss: 0.612267792224884
Epoch 2170, training loss: 451.796875 = 0.5095270276069641 + 50.0 * 9.025747299194336
Epoch 2170, val loss: 0.611522912979126
Epoch 2180, training loss: 451.69781494140625 = 0.5084506273269653 + 50.0 * 9.023787498474121
Epoch 2180, val loss: 0.6109902858734131
Epoch 2190, training loss: 451.915771484375 = 0.5073374509811401 + 50.0 * 9.028168678283691
Epoch 2190, val loss: 0.6104028224945068
Epoch 2200, training loss: 452.0438537597656 = 0.506273627281189 + 50.0 * 9.03075122833252
Epoch 2200, val loss: 0.6097579002380371
Epoch 2210, training loss: 452.2194519042969 = 0.5051751732826233 + 50.0 * 9.034285545349121
Epoch 2210, val loss: 0.609111487865448
Epoch 2220, training loss: 452.0732421875 = 0.5040367841720581 + 50.0 * 9.031384468078613
Epoch 2220, val loss: 0.6085190773010254
Epoch 2230, training loss: 451.9070739746094 = 0.5030077695846558 + 50.0 * 9.028080940246582
Epoch 2230, val loss: 0.6080371141433716
Epoch 2240, training loss: 452.1400146484375 = 0.5019721984863281 + 50.0 * 9.032760620117188
Epoch 2240, val loss: 0.6075572967529297
Epoch 2250, training loss: 452.3603820800781 = 0.5009324550628662 + 50.0 * 9.037188529968262
Epoch 2250, val loss: 0.6069202423095703
Epoch 2260, training loss: 452.2930908203125 = 0.49976882338523865 + 50.0 * 9.035866737365723
Epoch 2260, val loss: 0.6063268780708313
Epoch 2270, training loss: 452.165283203125 = 0.49865561723709106 + 50.0 * 9.033332824707031
Epoch 2270, val loss: 0.6057490110397339
Epoch 2280, training loss: 452.3961181640625 = 0.4975716769695282 + 50.0 * 9.037971496582031
Epoch 2280, val loss: 0.6051768064498901
Epoch 2290, training loss: 452.6100769042969 = 0.4964933395385742 + 50.0 * 9.042271614074707
Epoch 2290, val loss: 0.604458212852478
Epoch 2300, training loss: 452.7015686035156 = 0.49533799290657043 + 50.0 * 9.044124603271484
Epoch 2300, val loss: 0.6038011908531189
Epoch 2310, training loss: 452.5160827636719 = 0.49421021342277527 + 50.0 * 9.040437698364258
Epoch 2310, val loss: 0.6031564474105835
Epoch 2320, training loss: 452.6368103027344 = 0.49311715364456177 + 50.0 * 9.042874336242676
Epoch 2320, val loss: 0.6026227474212646
Epoch 2330, training loss: 452.77471923828125 = 0.49199771881103516 + 50.0 * 9.045654296875
Epoch 2330, val loss: 0.6020755171775818
Epoch 2340, training loss: 452.70965576171875 = 0.4908522665500641 + 50.0 * 9.044376373291016
Epoch 2340, val loss: 0.6011982560157776
Epoch 2350, training loss: 452.81488037109375 = 0.4897061288356781 + 50.0 * 9.046503067016602
Epoch 2350, val loss: 0.6006525754928589
Epoch 2360, training loss: 452.9402160644531 = 0.4885539412498474 + 50.0 * 9.049033164978027
Epoch 2360, val loss: 0.6000676155090332
Epoch 2370, training loss: 452.9316711425781 = 0.48740604519844055 + 50.0 * 9.048885345458984
Epoch 2370, val loss: 0.5994306802749634
Epoch 2380, training loss: 452.9933776855469 = 0.4862633943557739 + 50.0 * 9.050142288208008
Epoch 2380, val loss: 0.5986717343330383
Epoch 2390, training loss: 453.22344970703125 = 0.48512202501296997 + 50.0 * 9.054766654968262
Epoch 2390, val loss: 0.598008394241333
Epoch 2400, training loss: 453.150390625 = 0.48393169045448303 + 50.0 * 9.053329467773438
Epoch 2400, val loss: 0.5975109338760376
Epoch 2410, training loss: 452.9459228515625 = 0.48277905583381653 + 50.0 * 9.049263000488281
Epoch 2410, val loss: 0.5967270731925964
Epoch 2420, training loss: 453.1051025390625 = 0.4816262423992157 + 50.0 * 9.052469253540039
Epoch 2420, val loss: 0.5961299538612366
Epoch 2430, training loss: 453.3184814453125 = 0.4804930090904236 + 50.0 * 9.05675983428955
Epoch 2430, val loss: 0.5955004096031189
Epoch 2440, training loss: 453.43341064453125 = 0.47931012511253357 + 50.0 * 9.05908203125
Epoch 2440, val loss: 0.5948792099952698
Epoch 2450, training loss: 453.3294982910156 = 0.478121817111969 + 50.0 * 9.057027816772461
Epoch 2450, val loss: 0.5944903492927551
Epoch 2460, training loss: 453.2201843261719 = 0.47691357135772705 + 50.0 * 9.054864883422852
Epoch 2460, val loss: 0.593601405620575
Epoch 2470, training loss: 453.4580383300781 = 0.4757758677005768 + 50.0 * 9.059645652770996
Epoch 2470, val loss: 0.5928701162338257
Epoch 2480, training loss: 453.6788635253906 = 0.47458022832870483 + 50.0 * 9.064085960388184
Epoch 2480, val loss: 0.5922702550888062
Epoch 2490, training loss: 453.67327880859375 = 0.4733470380306244 + 50.0 * 9.063998222351074
Epoch 2490, val loss: 0.5915420055389404
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7585507246376811
0.8159820328914005
The final CL Acc:0.75889, 0.00397, The final GNN Acc:0.81569, 0.00080
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110934])
remove edge: torch.Size([2, 66872])
updated graph: torch.Size([2, 89158])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 506.9686279296875 = 1.0954511165618896 + 50.0 * 10.117463111877441
Epoch 0, val loss: 1.0966414213180542
Epoch 10, training loss: 484.4017028808594 = 1.0921008586883545 + 50.0 * 9.666192054748535
Epoch 10, val loss: 1.0933020114898682
Epoch 20, training loss: 474.6934509277344 = 1.0889157056808472 + 50.0 * 9.472090721130371
Epoch 20, val loss: 1.0901247262954712
Epoch 30, training loss: 467.75543212890625 = 1.085881233215332 + 50.0 * 9.333391189575195
Epoch 30, val loss: 1.08709716796875
Epoch 40, training loss: 462.3844299316406 = 1.083010196685791 + 50.0 * 9.226028442382812
Epoch 40, val loss: 1.084238886833191
Epoch 50, training loss: 458.0687561035156 = 1.0803111791610718 + 50.0 * 9.139768600463867
Epoch 50, val loss: 1.0815520286560059
Epoch 60, training loss: 454.5336608886719 = 1.0777400732040405 + 50.0 * 9.06911849975586
Epoch 60, val loss: 1.0789906978607178
Epoch 70, training loss: 451.5513000488281 = 1.075319766998291 + 50.0 * 9.009519577026367
Epoch 70, val loss: 1.076582670211792
Epoch 80, training loss: 448.99090576171875 = 1.0730160474777222 + 50.0 * 8.958357810974121
Epoch 80, val loss: 1.0742841958999634
Epoch 90, training loss: 446.8134765625 = 1.0708521604537964 + 50.0 * 8.914852142333984
Epoch 90, val loss: 1.0721226930618286
Epoch 100, training loss: 444.98699951171875 = 1.0688027143478394 + 50.0 * 8.878363609313965
Epoch 100, val loss: 1.0700680017471313
Epoch 110, training loss: 443.39178466796875 = 1.0668590068817139 + 50.0 * 8.846498489379883
Epoch 110, val loss: 1.068124771118164
Epoch 120, training loss: 441.94061279296875 = 1.0650207996368408 + 50.0 * 8.817511558532715
Epoch 120, val loss: 1.0662720203399658
Epoch 130, training loss: 440.78411865234375 = 1.0632908344268799 + 50.0 * 8.794416427612305
Epoch 130, val loss: 1.0645365715026855
Epoch 140, training loss: 439.768310546875 = 1.0616179704666138 + 50.0 * 8.774133682250977
Epoch 140, val loss: 1.0628498792648315
Epoch 150, training loss: 438.9723815917969 = 1.060038447380066 + 50.0 * 8.758247375488281
Epoch 150, val loss: 1.061250925064087
Epoch 160, training loss: 438.1191711425781 = 1.0585306882858276 + 50.0 * 8.741212844848633
Epoch 160, val loss: 1.0597320795059204
Epoch 170, training loss: 437.4862365722656 = 1.0570764541625977 + 50.0 * 8.728583335876465
Epoch 170, val loss: 1.0582561492919922
Epoch 180, training loss: 436.9711608886719 = 1.0556979179382324 + 50.0 * 8.71830940246582
Epoch 180, val loss: 1.0568568706512451
Epoch 190, training loss: 436.4544677734375 = 1.0543205738067627 + 50.0 * 8.708003044128418
Epoch 190, val loss: 1.0554600954055786
Epoch 200, training loss: 436.0009765625 = 1.0530043840408325 + 50.0 * 8.698959350585938
Epoch 200, val loss: 1.0541250705718994
Epoch 210, training loss: 435.553955078125 = 1.051688313484192 + 50.0 * 8.690045356750488
Epoch 210, val loss: 1.0527998208999634
Epoch 220, training loss: 435.251708984375 = 1.0503718852996826 + 50.0 * 8.684026718139648
Epoch 220, val loss: 1.0514674186706543
Epoch 230, training loss: 434.90325927734375 = 1.0490788221359253 + 50.0 * 8.677083969116211
Epoch 230, val loss: 1.0501782894134521
Epoch 240, training loss: 434.727294921875 = 1.0477352142333984 + 50.0 * 8.673591613769531
Epoch 240, val loss: 1.0488100051879883
Epoch 250, training loss: 434.62493896484375 = 1.0463656187057495 + 50.0 * 8.671571731567383
Epoch 250, val loss: 1.0474339723587036
Epoch 260, training loss: 434.2764892578125 = 1.044752597808838 + 50.0 * 8.664634704589844
Epoch 260, val loss: 1.0457792282104492
Epoch 270, training loss: 434.4080810546875 = 1.0434569120407104 + 50.0 * 8.667292594909668
Epoch 270, val loss: 1.0445330142974854
Epoch 280, training loss: 433.9935607910156 = 1.0418274402618408 + 50.0 * 8.659034729003906
Epoch 280, val loss: 1.0429271459579468
Epoch 290, training loss: 433.8769836425781 = 1.040168046951294 + 50.0 * 8.656736373901367
Epoch 290, val loss: 1.0412729978561401
Epoch 300, training loss: 433.7833251953125 = 1.038437843322754 + 50.0 * 8.654897689819336
Epoch 300, val loss: 1.0395606756210327
Epoch 310, training loss: 433.80133056640625 = 1.0366166830062866 + 50.0 * 8.655294418334961
Epoch 310, val loss: 1.0377317667007446
Epoch 320, training loss: 433.75225830078125 = 1.0346146821975708 + 50.0 * 8.654353141784668
Epoch 320, val loss: 1.0357484817504883
Epoch 330, training loss: 433.6041564941406 = 1.032456874847412 + 50.0 * 8.651433944702148
Epoch 330, val loss: 1.0336158275604248
Epoch 340, training loss: 433.6444091796875 = 1.030179500579834 + 50.0 * 8.652284622192383
Epoch 340, val loss: 1.03134286403656
Epoch 350, training loss: 433.4521179199219 = 1.0276942253112793 + 50.0 * 8.648488998413086
Epoch 350, val loss: 1.0289548635482788
Epoch 360, training loss: 433.5543212890625 = 1.02512526512146 + 50.0 * 8.65058422088623
Epoch 360, val loss: 1.0264121294021606
Epoch 370, training loss: 433.7194519042969 = 1.0223897695541382 + 50.0 * 8.65394115447998
Epoch 370, val loss: 1.023736596107483
Epoch 380, training loss: 433.64434814453125 = 1.0194249153137207 + 50.0 * 8.652498245239258
Epoch 380, val loss: 1.020889163017273
Epoch 390, training loss: 433.6878967285156 = 1.01620352268219 + 50.0 * 8.653433799743652
Epoch 390, val loss: 1.0176748037338257
Epoch 400, training loss: 433.79351806640625 = 1.0128021240234375 + 50.0 * 8.655613899230957
Epoch 400, val loss: 1.0143438577651978
Epoch 410, training loss: 433.7856140136719 = 1.0091354846954346 + 50.0 * 8.655529975891113
Epoch 410, val loss: 1.0107851028442383
Epoch 420, training loss: 433.91619873046875 = 1.005277156829834 + 50.0 * 8.658218383789062
Epoch 420, val loss: 1.0070314407348633
Epoch 430, training loss: 433.94927978515625 = 1.0011438131332397 + 50.0 * 8.658963203430176
Epoch 430, val loss: 1.0029246807098389
Epoch 440, training loss: 433.6946105957031 = 0.9967122077941895 + 50.0 * 8.653958320617676
Epoch 440, val loss: 0.9986839890480042
Epoch 450, training loss: 433.7502746582031 = 0.9922701120376587 + 50.0 * 8.655159950256348
Epoch 450, val loss: 0.9942550659179688
Epoch 460, training loss: 434.1539306640625 = 0.9872865676879883 + 50.0 * 8.66333293914795
Epoch 460, val loss: 0.9894346594810486
Epoch 470, training loss: 433.8945007324219 = 0.9820650219917297 + 50.0 * 8.658248901367188
Epoch 470, val loss: 0.9843127727508545
Epoch 480, training loss: 434.22845458984375 = 0.9770321846008301 + 50.0 * 8.66502857208252
Epoch 480, val loss: 0.9794661402702332
Epoch 490, training loss: 434.23333740234375 = 0.9713157415390015 + 50.0 * 8.665240287780762
Epoch 490, val loss: 0.9739358425140381
Epoch 500, training loss: 434.32879638671875 = 0.9653148651123047 + 50.0 * 8.667269706726074
Epoch 500, val loss: 0.9681373238563538
Epoch 510, training loss: 434.3915710449219 = 0.9592123031616211 + 50.0 * 8.668647766113281
Epoch 510, val loss: 0.962211549282074
Epoch 520, training loss: 434.5219421386719 = 0.9526546001434326 + 50.0 * 8.671385765075684
Epoch 520, val loss: 0.9557809829711914
Epoch 530, training loss: 434.6185607910156 = 0.9460364580154419 + 50.0 * 8.673450469970703
Epoch 530, val loss: 0.9493687152862549
Epoch 540, training loss: 434.7229919433594 = 0.9392790198326111 + 50.0 * 8.675674438476562
Epoch 540, val loss: 0.9427583813667297
Epoch 550, training loss: 434.82666015625 = 0.9321356415748596 + 50.0 * 8.67789077758789
Epoch 550, val loss: 0.9358074069023132
Epoch 560, training loss: 434.8462829589844 = 0.9247920513153076 + 50.0 * 8.67842960357666
Epoch 560, val loss: 0.9287011027336121
Epoch 570, training loss: 434.9456481933594 = 0.9171602129936218 + 50.0 * 8.680569648742676
Epoch 570, val loss: 0.9212333559989929
Epoch 580, training loss: 434.9002380371094 = 0.9091823697090149 + 50.0 * 8.679821014404297
Epoch 580, val loss: 0.9135202765464783
Epoch 590, training loss: 435.0025329589844 = 0.9013643264770508 + 50.0 * 8.682023048400879
Epoch 590, val loss: 0.9057943820953369
Epoch 600, training loss: 434.948974609375 = 0.8930391669273376 + 50.0 * 8.681118965148926
Epoch 600, val loss: 0.8978000283241272
Epoch 610, training loss: 435.2073059082031 = 0.8846782445907593 + 50.0 * 8.686452865600586
Epoch 610, val loss: 0.8897212147712708
Epoch 620, training loss: 435.4139099121094 = 0.8761436343193054 + 50.0 * 8.690755844116211
Epoch 620, val loss: 0.8814722299575806
Epoch 630, training loss: 435.5107421875 = 0.8674632906913757 + 50.0 * 8.692865371704102
Epoch 630, val loss: 0.8730174899101257
Epoch 640, training loss: 435.6109313964844 = 0.8584458231925964 + 50.0 * 8.695049285888672
Epoch 640, val loss: 0.8643448352813721
Epoch 650, training loss: 435.5658264160156 = 0.8494589328765869 + 50.0 * 8.694327354431152
Epoch 650, val loss: 0.8556190133094788
Epoch 660, training loss: 435.5783996582031 = 0.8403084874153137 + 50.0 * 8.694762229919434
Epoch 660, val loss: 0.8466092348098755
Epoch 670, training loss: 435.5595397949219 = 0.8307830095291138 + 50.0 * 8.694575309753418
Epoch 670, val loss: 0.8374451994895935
Epoch 680, training loss: 435.8114318847656 = 0.8218572735786438 + 50.0 * 8.699790954589844
Epoch 680, val loss: 0.8287965655326843
Epoch 690, training loss: 435.9664611816406 = 0.8126417398452759 + 50.0 * 8.703076362609863
Epoch 690, val loss: 0.8198530077934265
Epoch 700, training loss: 436.115478515625 = 0.8033921718597412 + 50.0 * 8.706241607666016
Epoch 700, val loss: 0.8106927871704102
Epoch 710, training loss: 436.3321228027344 = 0.7939060926437378 + 50.0 * 8.710763931274414
Epoch 710, val loss: 0.8016834855079651
Epoch 720, training loss: 436.4657287597656 = 0.7844263911247253 + 50.0 * 8.71362590789795
Epoch 720, val loss: 0.7924671173095703
Epoch 730, training loss: 436.429931640625 = 0.775246798992157 + 50.0 * 8.713093757629395
Epoch 730, val loss: 0.7835522294044495
Epoch 740, training loss: 435.82489013671875 = 0.7650134563446045 + 50.0 * 8.701197624206543
Epoch 740, val loss: 0.7740669250488281
Epoch 750, training loss: 435.9408874511719 = 0.7558813095092773 + 50.0 * 8.703700065612793
Epoch 750, val loss: 0.7649451494216919
Epoch 760, training loss: 436.3918151855469 = 0.7472042441368103 + 50.0 * 8.712892532348633
Epoch 760, val loss: 0.7564721703529358
Epoch 770, training loss: 436.3061218261719 = 0.7377318739891052 + 50.0 * 8.7113676071167
Epoch 770, val loss: 0.7474331259727478
Epoch 780, training loss: 436.6838684082031 = 0.729101300239563 + 50.0 * 8.719095230102539
Epoch 780, val loss: 0.7390228509902954
Epoch 790, training loss: 436.9072265625 = 0.720161497592926 + 50.0 * 8.72374153137207
Epoch 790, val loss: 0.7305392026901245
Epoch 800, training loss: 437.019287109375 = 0.7115190625190735 + 50.0 * 8.726155281066895
Epoch 800, val loss: 0.7221609950065613
Epoch 810, training loss: 437.2519836425781 = 0.7030279040336609 + 50.0 * 8.730978965759277
Epoch 810, val loss: 0.7139391303062439
Epoch 820, training loss: 437.3671569824219 = 0.6946883201599121 + 50.0 * 8.73344898223877
Epoch 820, val loss: 0.705941379070282
Epoch 830, training loss: 437.4717102050781 = 0.6864163875579834 + 50.0 * 8.735706329345703
Epoch 830, val loss: 0.6980908513069153
Epoch 840, training loss: 437.6508483886719 = 0.6784622073173523 + 50.0 * 8.739447593688965
Epoch 840, val loss: 0.6902498602867126
Epoch 850, training loss: 437.5465087890625 = 0.6705024838447571 + 50.0 * 8.737520217895508
Epoch 850, val loss: 0.6827878952026367
Epoch 860, training loss: 437.80511474609375 = 0.6629341244697571 + 50.0 * 8.742843627929688
Epoch 860, val loss: 0.6755713820457458
Epoch 870, training loss: 437.73553466796875 = 0.6554300785064697 + 50.0 * 8.741601943969727
Epoch 870, val loss: 0.668361485004425
Epoch 880, training loss: 437.60504150390625 = 0.648108184337616 + 50.0 * 8.73913860321045
Epoch 880, val loss: 0.661398708820343
Epoch 890, training loss: 438.1877746582031 = 0.6413118839263916 + 50.0 * 8.75092887878418
Epoch 890, val loss: 0.6548342108726501
Epoch 900, training loss: 438.1230773925781 = 0.6346800923347473 + 50.0 * 8.749768257141113
Epoch 900, val loss: 0.6485562324523926
Epoch 910, training loss: 438.2546691894531 = 0.6281718611717224 + 50.0 * 8.752530097961426
Epoch 910, val loss: 0.6423097252845764
Epoch 920, training loss: 438.5272216796875 = 0.6218161582946777 + 50.0 * 8.758108139038086
Epoch 920, val loss: 0.6363962292671204
Epoch 930, training loss: 438.34912109375 = 0.615329384803772 + 50.0 * 8.75467586517334
Epoch 930, val loss: 0.6297320127487183
Epoch 940, training loss: 438.62335205078125 = 0.6096472144126892 + 50.0 * 8.760273933410645
Epoch 940, val loss: 0.6246752738952637
Epoch 950, training loss: 438.1107177734375 = 0.6034567952156067 + 50.0 * 8.750144958496094
Epoch 950, val loss: 0.618869423866272
Epoch 960, training loss: 438.6510009765625 = 0.598140299320221 + 50.0 * 8.761056900024414
Epoch 960, val loss: 0.6137690544128418
Epoch 970, training loss: 438.8841247558594 = 0.5927646160125732 + 50.0 * 8.765827178955078
Epoch 970, val loss: 0.6086907386779785
Epoch 980, training loss: 438.8340759277344 = 0.587487518787384 + 50.0 * 8.764931678771973
Epoch 980, val loss: 0.6036250591278076
Epoch 990, training loss: 438.9611511230469 = 0.582501232624054 + 50.0 * 8.767573356628418
Epoch 990, val loss: 0.5990263819694519
Epoch 1000, training loss: 439.12432861328125 = 0.5775725245475769 + 50.0 * 8.77093505859375
Epoch 1000, val loss: 0.5943794846534729
Epoch 1010, training loss: 439.2303161621094 = 0.5727476477622986 + 50.0 * 8.773151397705078
Epoch 1010, val loss: 0.5898749828338623
Epoch 1020, training loss: 439.3555908203125 = 0.5680969953536987 + 50.0 * 8.775750160217285
Epoch 1020, val loss: 0.5856115818023682
Epoch 1030, training loss: 439.54754638671875 = 0.5636283755302429 + 50.0 * 8.779678344726562
Epoch 1030, val loss: 0.5813923478126526
Epoch 1040, training loss: 439.4204406738281 = 0.5592900514602661 + 50.0 * 8.777222633361816
Epoch 1040, val loss: 0.577502965927124
Epoch 1050, training loss: 439.58465576171875 = 0.5549516677856445 + 50.0 * 8.780593872070312
Epoch 1050, val loss: 0.5735190510749817
Epoch 1060, training loss: 439.7662048339844 = 0.5508396029472351 + 50.0 * 8.784307479858398
Epoch 1060, val loss: 0.5697320699691772
Epoch 1070, training loss: 439.8544616699219 = 0.5468162894248962 + 50.0 * 8.786152839660645
Epoch 1070, val loss: 0.5660056471824646
Epoch 1080, training loss: 440.0735778808594 = 0.5430088043212891 + 50.0 * 8.790611267089844
Epoch 1080, val loss: 0.5625160932540894
Epoch 1090, training loss: 439.6463317871094 = 0.5390637516975403 + 50.0 * 8.782145500183105
Epoch 1090, val loss: 0.5589292645454407
Epoch 1100, training loss: 440.0176086425781 = 0.5355494022369385 + 50.0 * 8.789641380310059
Epoch 1100, val loss: 0.5557143092155457
Epoch 1110, training loss: 440.1527099609375 = 0.5319350957870483 + 50.0 * 8.792415618896484
Epoch 1110, val loss: 0.5525315403938293
Epoch 1120, training loss: 440.43634033203125 = 0.5284525156021118 + 50.0 * 8.798157691955566
Epoch 1120, val loss: 0.5494063496589661
Epoch 1130, training loss: 440.351318359375 = 0.5250201225280762 + 50.0 * 8.796525955200195
Epoch 1130, val loss: 0.5463124513626099
Epoch 1140, training loss: 440.4945983886719 = 0.5217182636260986 + 50.0 * 8.799457550048828
Epoch 1140, val loss: 0.5432913303375244
Epoch 1150, training loss: 440.6015930175781 = 0.5184516906738281 + 50.0 * 8.80166244506836
Epoch 1150, val loss: 0.5404651165008545
Epoch 1160, training loss: 440.6064453125 = 0.5153369903564453 + 50.0 * 8.8018217086792
Epoch 1160, val loss: 0.5376688838005066
Epoch 1170, training loss: 440.7593078613281 = 0.5123101472854614 + 50.0 * 8.804940223693848
Epoch 1170, val loss: 0.5349640846252441
Epoch 1180, training loss: 440.75537109375 = 0.5093187093734741 + 50.0 * 8.80492115020752
Epoch 1180, val loss: 0.5324026942253113
Epoch 1190, training loss: 440.9440612792969 = 0.5064645409584045 + 50.0 * 8.808752059936523
Epoch 1190, val loss: 0.5298659801483154
Epoch 1200, training loss: 441.1628112792969 = 0.5035485029220581 + 50.0 * 8.813185691833496
Epoch 1200, val loss: 0.5273258090019226
Epoch 1210, training loss: 441.2079772949219 = 0.5006718039512634 + 50.0 * 8.814146041870117
Epoch 1210, val loss: 0.5247755646705627
Epoch 1220, training loss: 441.1209411621094 = 0.49789413809776306 + 50.0 * 8.812460899353027
Epoch 1220, val loss: 0.5224302411079407
Epoch 1230, training loss: 441.34210205078125 = 0.495232492685318 + 50.0 * 8.816937446594238
Epoch 1230, val loss: 0.5201277136802673
Epoch 1240, training loss: 441.4668884277344 = 0.49254730343818665 + 50.0 * 8.819486618041992
Epoch 1240, val loss: 0.5177808403968811
Epoch 1250, training loss: 441.50909423828125 = 0.4899035096168518 + 50.0 * 8.82038402557373
Epoch 1250, val loss: 0.5156182050704956
Epoch 1260, training loss: 441.591796875 = 0.487433522939682 + 50.0 * 8.822087287902832
Epoch 1260, val loss: 0.5134332180023193
Epoch 1270, training loss: 441.8511962890625 = 0.4849850833415985 + 50.0 * 8.827323913574219
Epoch 1270, val loss: 0.5113610029220581
Epoch 1280, training loss: 441.807373046875 = 0.4825020432472229 + 50.0 * 8.826497077941895
Epoch 1280, val loss: 0.5092228651046753
Epoch 1290, training loss: 441.28509521484375 = 0.47999119758605957 + 50.0 * 8.816102027893066
Epoch 1290, val loss: 0.507196843624115
Epoch 1300, training loss: 441.9850158691406 = 0.47805845737457275 + 50.0 * 8.83013916015625
Epoch 1300, val loss: 0.5054453611373901
Epoch 1310, training loss: 440.8672180175781 = 0.47543758153915405 + 50.0 * 8.807835578918457
Epoch 1310, val loss: 0.5031209588050842
Epoch 1320, training loss: 441.13262939453125 = 0.4732227623462677 + 50.0 * 8.813187599182129
Epoch 1320, val loss: 0.501800537109375
Epoch 1330, training loss: 441.08953857421875 = 0.4711315333843231 + 50.0 * 8.812368392944336
Epoch 1330, val loss: 0.49970489740371704
Epoch 1340, training loss: 441.5606689453125 = 0.4690949618816376 + 50.0 * 8.821831703186035
Epoch 1340, val loss: 0.49827882647514343
Epoch 1350, training loss: 441.9179992675781 = 0.46707087755203247 + 50.0 * 8.829018592834473
Epoch 1350, val loss: 0.4966578185558319
Epoch 1360, training loss: 442.1889343261719 = 0.4649491310119629 + 50.0 * 8.834480285644531
Epoch 1360, val loss: 0.4949226975440979
Epoch 1370, training loss: 442.3929443359375 = 0.46281716227531433 + 50.0 * 8.838602066040039
Epoch 1370, val loss: 0.49330976605415344
Epoch 1380, training loss: 442.5053405761719 = 0.46076592803001404 + 50.0 * 8.84089183807373
Epoch 1380, val loss: 0.4915328025817871
Epoch 1390, training loss: 442.3374328613281 = 0.4586935043334961 + 50.0 * 8.83757495880127
Epoch 1390, val loss: 0.4900164306163788
Epoch 1400, training loss: 442.56707763671875 = 0.45673757791519165 + 50.0 * 8.842206954956055
Epoch 1400, val loss: 0.488491028547287
Epoch 1410, training loss: 442.75994873046875 = 0.45478346943855286 + 50.0 * 8.84610366821289
Epoch 1410, val loss: 0.4868867099285126
Epoch 1420, training loss: 442.9359130859375 = 0.45283105969429016 + 50.0 * 8.849661827087402
Epoch 1420, val loss: 0.48536592721939087
Epoch 1430, training loss: 443.0312194824219 = 0.45094555616378784 + 50.0 * 8.851605415344238
Epoch 1430, val loss: 0.4839369058609009
Epoch 1440, training loss: 443.0986328125 = 0.4490332007408142 + 50.0 * 8.852992057800293
Epoch 1440, val loss: 0.48250728845596313
Epoch 1450, training loss: 443.1798400878906 = 0.4472234547138214 + 50.0 * 8.854652404785156
Epoch 1450, val loss: 0.48115622997283936
Epoch 1460, training loss: 443.2077941894531 = 0.44536179304122925 + 50.0 * 8.85524845123291
Epoch 1460, val loss: 0.47968676686286926
Epoch 1470, training loss: 443.3139953613281 = 0.44359534978866577 + 50.0 * 8.857407569885254
Epoch 1470, val loss: 0.47829726338386536
Epoch 1480, training loss: 443.5705871582031 = 0.44186675548553467 + 50.0 * 8.862574577331543
Epoch 1480, val loss: 0.47704431414604187
Epoch 1490, training loss: 443.13018798828125 = 0.4401649832725525 + 50.0 * 8.853800773620605
Epoch 1490, val loss: 0.47558730840682983
Epoch 1500, training loss: 443.02716064453125 = 0.4385323226451874 + 50.0 * 8.85177230834961
Epoch 1500, val loss: 0.4745413661003113
Epoch 1510, training loss: 442.88372802734375 = 0.43681955337524414 + 50.0 * 8.84893798828125
Epoch 1510, val loss: 0.4732697606086731
Epoch 1520, training loss: 442.8360290527344 = 0.4352407455444336 + 50.0 * 8.848015785217285
Epoch 1520, val loss: 0.4721643328666687
Epoch 1530, training loss: 443.2181396484375 = 0.43353286385536194 + 50.0 * 8.855691909790039
Epoch 1530, val loss: 0.47086265683174133
Epoch 1540, training loss: 443.244384765625 = 0.43194663524627686 + 50.0 * 8.85624885559082
Epoch 1540, val loss: 0.4696480631828308
Epoch 1550, training loss: 443.6964416503906 = 0.4303928017616272 + 50.0 * 8.865321159362793
Epoch 1550, val loss: 0.46853378415107727
Epoch 1560, training loss: 443.94390869140625 = 0.42884740233421326 + 50.0 * 8.870301246643066
Epoch 1560, val loss: 0.4674018621444702
Epoch 1570, training loss: 444.1095886230469 = 0.4273034632205963 + 50.0 * 8.873645782470703
Epoch 1570, val loss: 0.4663808345794678
Epoch 1580, training loss: 444.1149597167969 = 0.42576560378074646 + 50.0 * 8.873784065246582
Epoch 1580, val loss: 0.4653071463108063
Epoch 1590, training loss: 444.3121643066406 = 0.4242401421070099 + 50.0 * 8.877758026123047
Epoch 1590, val loss: 0.4642132520675659
Epoch 1600, training loss: 444.2192687988281 = 0.42266902327537537 + 50.0 * 8.875931739807129
Epoch 1600, val loss: 0.46318039298057556
Epoch 1610, training loss: 444.2517395019531 = 0.4212113916873932 + 50.0 * 8.87661075592041
Epoch 1610, val loss: 0.46218883991241455
Epoch 1620, training loss: 444.3469543457031 = 0.41979777812957764 + 50.0 * 8.87854290008545
Epoch 1620, val loss: 0.46121054887771606
Epoch 1630, training loss: 444.60357666015625 = 0.4183507263660431 + 50.0 * 8.88370418548584
Epoch 1630, val loss: 0.4601858854293823
Epoch 1640, training loss: 444.7070007324219 = 0.4169418513774872 + 50.0 * 8.885801315307617
Epoch 1640, val loss: 0.4591440260410309
Epoch 1650, training loss: 444.38262939453125 = 0.41545626521110535 + 50.0 * 8.879343032836914
Epoch 1650, val loss: 0.45832112431526184
Epoch 1660, training loss: 444.25811767578125 = 0.4141142666339874 + 50.0 * 8.876879692077637
Epoch 1660, val loss: 0.457192063331604
Epoch 1670, training loss: 444.09771728515625 = 0.41270777583122253 + 50.0 * 8.873700141906738
Epoch 1670, val loss: 0.45642903447151184
Epoch 1680, training loss: 444.2914123535156 = 0.41137492656707764 + 50.0 * 8.87760066986084
Epoch 1680, val loss: 0.45564839243888855
Epoch 1690, training loss: 444.6695861816406 = 0.41004395484924316 + 50.0 * 8.885190963745117
Epoch 1690, val loss: 0.45471933484077454
Epoch 1700, training loss: 444.92437744140625 = 0.40866619348526 + 50.0 * 8.890314102172852
Epoch 1700, val loss: 0.45377132296562195
Epoch 1710, training loss: 445.1496276855469 = 0.40731891989707947 + 50.0 * 8.894845962524414
Epoch 1710, val loss: 0.45285874605178833
Epoch 1720, training loss: 444.9242858886719 = 0.4059329628944397 + 50.0 * 8.890366554260254
Epoch 1720, val loss: 0.4518505334854126
Epoch 1730, training loss: 444.6802062988281 = 0.40452778339385986 + 50.0 * 8.885513305664062
Epoch 1730, val loss: 0.4511435031890869
Epoch 1740, training loss: 444.69561767578125 = 0.40330836176872253 + 50.0 * 8.885846138000488
Epoch 1740, val loss: 0.4503578245639801
Epoch 1750, training loss: 445.0641174316406 = 0.402059406042099 + 50.0 * 8.893240928649902
Epoch 1750, val loss: 0.4495140016078949
Epoch 1760, training loss: 445.2766418457031 = 0.4007737934589386 + 50.0 * 8.897517204284668
Epoch 1760, val loss: 0.4486894905567169
Epoch 1770, training loss: 445.2658386230469 = 0.39946249127388 + 50.0 * 8.897327423095703
Epoch 1770, val loss: 0.44771578907966614
Epoch 1780, training loss: 445.124755859375 = 0.39809539914131165 + 50.0 * 8.894533157348633
Epoch 1780, val loss: 0.4470636248588562
Epoch 1790, training loss: 445.332275390625 = 0.39684176445007324 + 50.0 * 8.89870834350586
Epoch 1790, val loss: 0.4461918771266937
Epoch 1800, training loss: 445.48297119140625 = 0.39554646611213684 + 50.0 * 8.901748657226562
Epoch 1800, val loss: 0.4453600347042084
Epoch 1810, training loss: 445.6020812988281 = 0.3942491114139557 + 50.0 * 8.904156684875488
Epoch 1810, val loss: 0.4444705843925476
Epoch 1820, training loss: 445.0914001464844 = 0.39282605051994324 + 50.0 * 8.89397144317627
Epoch 1820, val loss: 0.44344693422317505
Epoch 1830, training loss: 445.8598327636719 = 0.3916858732700348 + 50.0 * 8.90936279296875
Epoch 1830, val loss: 0.44321882724761963
Epoch 1840, training loss: 444.7541198730469 = 0.390291303396225 + 50.0 * 8.887276649475098
Epoch 1840, val loss: 0.4414187967777252
Epoch 1850, training loss: 444.8177490234375 = 0.38910654187202454 + 50.0 * 8.888572692871094
Epoch 1850, val loss: 0.4414275884628296
Epoch 1860, training loss: 445.2654724121094 = 0.38794976472854614 + 50.0 * 8.897550582885742
Epoch 1860, val loss: 0.44088849425315857
Epoch 1870, training loss: 445.7137756347656 = 0.38676708936691284 + 50.0 * 8.906539916992188
Epoch 1870, val loss: 0.43999746441841125
Epoch 1880, training loss: 445.8891906738281 = 0.3855435252189636 + 50.0 * 8.910073280334473
Epoch 1880, val loss: 0.43929705023765564
Epoch 1890, training loss: 446.018798828125 = 0.3843425512313843 + 50.0 * 8.912689208984375
Epoch 1890, val loss: 0.43868014216423035
Epoch 1900, training loss: 446.1151428222656 = 0.3831251561641693 + 50.0 * 8.914640426635742
Epoch 1900, val loss: 0.4377536475658417
Epoch 1910, training loss: 446.244140625 = 0.3819015324115753 + 50.0 * 8.917244911193848
Epoch 1910, val loss: 0.43705904483795166
Epoch 1920, training loss: 446.12548828125 = 0.3806331157684326 + 50.0 * 8.914896965026855
Epoch 1920, val loss: 0.43632927536964417
Epoch 1930, training loss: 445.09429931640625 = 0.37933820486068726 + 50.0 * 8.894299507141113
Epoch 1930, val loss: 0.43543076515197754
Epoch 1940, training loss: 445.34466552734375 = 0.37818294763565063 + 50.0 * 8.899330139160156
Epoch 1940, val loss: 0.4346822202205658
Epoch 1950, training loss: 445.94891357421875 = 0.37711063027381897 + 50.0 * 8.911436080932617
Epoch 1950, val loss: 0.43423205614089966
Epoch 1960, training loss: 446.2109375 = 0.37605664134025574 + 50.0 * 8.91669750213623
Epoch 1960, val loss: 0.4336617588996887
Epoch 1970, training loss: 446.4703674316406 = 0.3749064803123474 + 50.0 * 8.92190933227539
Epoch 1970, val loss: 0.4330825209617615
Epoch 1980, training loss: 446.65203857421875 = 0.37374067306518555 + 50.0 * 8.925565719604492
Epoch 1980, val loss: 0.4324595034122467
Epoch 1990, training loss: 446.3700256347656 = 0.3725558817386627 + 50.0 * 8.919949531555176
Epoch 1990, val loss: 0.4317449927330017
Epoch 2000, training loss: 446.5681457519531 = 0.37140557169914246 + 50.0 * 8.923934936523438
Epoch 2000, val loss: 0.43116044998168945
Epoch 2010, training loss: 446.755126953125 = 0.3702748119831085 + 50.0 * 8.92769718170166
Epoch 2010, val loss: 0.43045294284820557
Epoch 2020, training loss: 446.5668029785156 = 0.36910420656204224 + 50.0 * 8.923954010009766
Epoch 2020, val loss: 0.4297860562801361
Epoch 2030, training loss: 446.89715576171875 = 0.3679537773132324 + 50.0 * 8.930583953857422
Epoch 2030, val loss: 0.42923349142074585
Epoch 2040, training loss: 446.6067810058594 = 0.36677971482276917 + 50.0 * 8.924799919128418
Epoch 2040, val loss: 0.4286034405231476
Epoch 2050, training loss: 446.72454833984375 = 0.36563923954963684 + 50.0 * 8.927178382873535
Epoch 2050, val loss: 0.4279881417751312
Epoch 2060, training loss: 446.9965515136719 = 0.3645055890083313 + 50.0 * 8.93264102935791
Epoch 2060, val loss: 0.42737677693367004
Epoch 2070, training loss: 447.0057067871094 = 0.36334094405174255 + 50.0 * 8.932847023010254
Epoch 2070, val loss: 0.4266818165779114
Epoch 2080, training loss: 446.875 = 0.3622082769870758 + 50.0 * 8.930255889892578
Epoch 2080, val loss: 0.4260585904121399
Epoch 2090, training loss: 447.09625244140625 = 0.3611205816268921 + 50.0 * 8.93470287322998
Epoch 2090, val loss: 0.4256458282470703
Epoch 2100, training loss: 447.2951354980469 = 0.3600439131259918 + 50.0 * 8.938701629638672
Epoch 2100, val loss: 0.42493146657943726
Epoch 2110, training loss: 447.50750732421875 = 0.35893765091896057 + 50.0 * 8.942971229553223
Epoch 2110, val loss: 0.4244740903377533
Epoch 2120, training loss: 447.1573486328125 = 0.35780295729637146 + 50.0 * 8.935991287231445
Epoch 2120, val loss: 0.42385703325271606
Epoch 2130, training loss: 447.23541259765625 = 0.3566927909851074 + 50.0 * 8.93757438659668
Epoch 2130, val loss: 0.4232654869556427
Epoch 2140, training loss: 447.3811950683594 = 0.35561197996139526 + 50.0 * 8.940511703491211
Epoch 2140, val loss: 0.42293140292167664
Epoch 2150, training loss: 447.6214904785156 = 0.3545265793800354 + 50.0 * 8.94533920288086
Epoch 2150, val loss: 0.4222393333911896
Epoch 2160, training loss: 447.7539978027344 = 0.3534039258956909 + 50.0 * 8.948012351989746
Epoch 2160, val loss: 0.42179158329963684
Epoch 2170, training loss: 447.7851867675781 = 0.35231441259384155 + 50.0 * 8.948657035827637
Epoch 2170, val loss: 0.42121100425720215
Epoch 2180, training loss: 447.60638427734375 = 0.3511734902858734 + 50.0 * 8.945104598999023
Epoch 2180, val loss: 0.4207044541835785
Epoch 2190, training loss: 447.8137512207031 = 0.3500930666923523 + 50.0 * 8.949273109436035
Epoch 2190, val loss: 0.4201570749282837
Epoch 2200, training loss: 448.0148620605469 = 0.34902068972587585 + 50.0 * 8.953316688537598
Epoch 2200, val loss: 0.4196518361568451
Epoch 2210, training loss: 448.0095520019531 = 0.3479154407978058 + 50.0 * 8.953232765197754
Epoch 2210, val loss: 0.4191970229148865
Epoch 2220, training loss: 446.5079650878906 = 0.3468096852302551 + 50.0 * 8.923223495483398
Epoch 2220, val loss: 0.41884753108024597
Epoch 2230, training loss: 446.1580810546875 = 0.3459227979183197 + 50.0 * 8.916243553161621
Epoch 2230, val loss: 0.4175286591053009
Epoch 2240, training loss: 445.62872314453125 = 0.3450551927089691 + 50.0 * 8.905673027038574
Epoch 2240, val loss: 0.4179491698741913
Epoch 2250, training loss: 445.62420654296875 = 0.3439834415912628 + 50.0 * 8.905604362487793
Epoch 2250, val loss: 0.41749754548072815
Epoch 2260, training loss: 446.09014892578125 = 0.3429347276687622 + 50.0 * 8.914944648742676
Epoch 2260, val loss: 0.4168858826160431
Epoch 2270, training loss: 446.30413818359375 = 0.3419434428215027 + 50.0 * 8.919243812561035
Epoch 2270, val loss: 0.41609278321266174
Epoch 2280, training loss: 446.75177001953125 = 0.3409087657928467 + 50.0 * 8.928216934204102
Epoch 2280, val loss: 0.4164760112762451
Epoch 2290, training loss: 447.11859130859375 = 0.3398740887641907 + 50.0 * 8.935574531555176
Epoch 2290, val loss: 0.4156532287597656
Epoch 2300, training loss: 447.3069152832031 = 0.33879026770591736 + 50.0 * 8.939362525939941
Epoch 2300, val loss: 0.4153728187084198
Epoch 2310, training loss: 447.49224853515625 = 0.33772361278533936 + 50.0 * 8.943090438842773
Epoch 2310, val loss: 0.4148082435131073
Epoch 2320, training loss: 447.66717529296875 = 0.33663907647132874 + 50.0 * 8.946610450744629
Epoch 2320, val loss: 0.4144522547721863
Epoch 2330, training loss: 447.6783447265625 = 0.3355608582496643 + 50.0 * 8.946855545043945
Epoch 2330, val loss: 0.4139406979084015
Epoch 2340, training loss: 447.8283386230469 = 0.3344939947128296 + 50.0 * 8.94987678527832
Epoch 2340, val loss: 0.4134388864040375
Epoch 2350, training loss: 447.96917724609375 = 0.33343610167503357 + 50.0 * 8.952714920043945
Epoch 2350, val loss: 0.4130067229270935
Epoch 2360, training loss: 447.96893310546875 = 0.3323700726032257 + 50.0 * 8.952731132507324
Epoch 2360, val loss: 0.412569135427475
Epoch 2370, training loss: 448.1275329589844 = 0.3313209116458893 + 50.0 * 8.955924034118652
Epoch 2370, val loss: 0.4121442437171936
Epoch 2380, training loss: 448.1418151855469 = 0.3302656412124634 + 50.0 * 8.956231117248535
Epoch 2380, val loss: 0.4116900563240051
Epoch 2390, training loss: 448.0704345703125 = 0.3292345702648163 + 50.0 * 8.954824447631836
Epoch 2390, val loss: 0.411098837852478
Epoch 2400, training loss: 448.0733947753906 = 0.3282211124897003 + 50.0 * 8.954903602600098
Epoch 2400, val loss: 0.4109134078025818
Epoch 2410, training loss: 448.14288330078125 = 0.3271801173686981 + 50.0 * 8.956314086914062
Epoch 2410, val loss: 0.4105001986026764
Epoch 2420, training loss: 448.40167236328125 = 0.3261474668979645 + 50.0 * 8.96151065826416
Epoch 2420, val loss: 0.41013890504837036
Epoch 2430, training loss: 448.4541320800781 = 0.3250913918018341 + 50.0 * 8.962580680847168
Epoch 2430, val loss: 0.40977632999420166
Epoch 2440, training loss: 448.33905029296875 = 0.3240322768688202 + 50.0 * 8.96030044555664
Epoch 2440, val loss: 0.40935221314430237
Epoch 2450, training loss: 448.3793029785156 = 0.3230038285255432 + 50.0 * 8.961126327514648
Epoch 2450, val loss: 0.40893790125846863
Epoch 2460, training loss: 448.4809875488281 = 0.32196423411369324 + 50.0 * 8.963180541992188
Epoch 2460, val loss: 0.4086270332336426
Epoch 2470, training loss: 448.45697021484375 = 0.3209274411201477 + 50.0 * 8.96272087097168
Epoch 2470, val loss: 0.4080902338027954
Epoch 2480, training loss: 448.41015625 = 0.3198970854282379 + 50.0 * 8.96180534362793
Epoch 2480, val loss: 0.4078730642795563
Epoch 2490, training loss: 448.67327880859375 = 0.31885668635368347 + 50.0 * 8.96708869934082
Epoch 2490, val loss: 0.40741899609565735
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8485507246376811
0.8632905890023909
=== training gcn model ===
Epoch 0, training loss: 516.7676391601562 = 1.092605710029602 + 50.0 * 10.313501358032227
Epoch 0, val loss: 1.0930243730545044
Epoch 10, training loss: 494.8896484375 = 1.0892066955566406 + 50.0 * 9.876008987426758
Epoch 10, val loss: 1.0896679162979126
Epoch 20, training loss: 482.0800476074219 = 1.0861287117004395 + 50.0 * 9.619878768920898
Epoch 20, val loss: 1.0866285562515259
Epoch 30, training loss: 473.9366760253906 = 1.0832278728485107 + 50.0 * 9.457069396972656
Epoch 30, val loss: 1.0837526321411133
Epoch 40, training loss: 467.7284851074219 = 1.0804777145385742 + 50.0 * 9.33296012878418
Epoch 40, val loss: 1.0810343027114868
Epoch 50, training loss: 462.74713134765625 = 1.077907681465149 + 50.0 * 9.233384132385254
Epoch 50, val loss: 1.0784891843795776
Epoch 60, training loss: 458.5832824707031 = 1.0754833221435547 + 50.0 * 9.150156021118164
Epoch 60, val loss: 1.0760830640792847
Epoch 70, training loss: 455.03472900390625 = 1.0731912851333618 + 50.0 * 9.079231262207031
Epoch 70, val loss: 1.07381010055542
Epoch 80, training loss: 452.12078857421875 = 1.0710407495498657 + 50.0 * 9.020995140075684
Epoch 80, val loss: 1.071677327156067
Epoch 90, training loss: 449.5801696777344 = 1.0689982175827026 + 50.0 * 8.970223426818848
Epoch 90, val loss: 1.069654107093811
Epoch 100, training loss: 447.5065612792969 = 1.0671032667160034 + 50.0 * 8.928789138793945
Epoch 100, val loss: 1.0677707195281982
Epoch 110, training loss: 445.69049072265625 = 1.0653034448623657 + 50.0 * 8.89250373840332
Epoch 110, val loss: 1.0659894943237305
Epoch 120, training loss: 444.15509033203125 = 1.0636032819747925 + 50.0 * 8.86182975769043
Epoch 120, val loss: 1.0643041133880615
Epoch 130, training loss: 442.90447998046875 = 1.0620064735412598 + 50.0 * 8.836849212646484
Epoch 130, val loss: 1.0627238750457764
Epoch 140, training loss: 441.7212829589844 = 1.060495138168335 + 50.0 * 8.813216209411621
Epoch 140, val loss: 1.0612326860427856
Epoch 150, training loss: 440.785888671875 = 1.0590571165084839 + 50.0 * 8.794536590576172
Epoch 150, val loss: 1.059826374053955
Epoch 160, training loss: 439.91986083984375 = 1.0576716661453247 + 50.0 * 8.777243614196777
Epoch 160, val loss: 1.058473825454712
Epoch 170, training loss: 439.1969909667969 = 1.0563676357269287 + 50.0 * 8.762812614440918
Epoch 170, val loss: 1.0571941137313843
Epoch 180, training loss: 438.6668701171875 = 1.0551304817199707 + 50.0 * 8.75223445892334
Epoch 180, val loss: 1.0559884309768677
Epoch 190, training loss: 438.1082763671875 = 1.0538897514343262 + 50.0 * 8.741087913513184
Epoch 190, val loss: 1.054777979850769
Epoch 200, training loss: 437.57098388671875 = 1.0527141094207764 + 50.0 * 8.730365753173828
Epoch 200, val loss: 1.0536412000656128
Epoch 210, training loss: 437.2564697265625 = 1.051511287689209 + 50.0 * 8.724099159240723
Epoch 210, val loss: 1.0524829626083374
Epoch 220, training loss: 436.8117980957031 = 1.0502959489822388 + 50.0 * 8.715229988098145
Epoch 220, val loss: 1.051316261291504
Epoch 230, training loss: 436.5732727050781 = 1.0490566492080688 + 50.0 * 8.710484504699707
Epoch 230, val loss: 1.0501140356063843
Epoch 240, training loss: 436.379638671875 = 1.0478098392486572 + 50.0 * 8.706636428833008
Epoch 240, val loss: 1.0489025115966797
Epoch 250, training loss: 436.1629638671875 = 1.0463850498199463 + 50.0 * 8.70233154296875
Epoch 250, val loss: 1.0475434064865112
Epoch 260, training loss: 436.17413330078125 = 1.0452057123184204 + 50.0 * 8.7025785446167
Epoch 260, val loss: 1.0463634729385376
Epoch 270, training loss: 435.8283386230469 = 1.0437023639678955 + 50.0 * 8.695693016052246
Epoch 270, val loss: 1.044945478439331
Epoch 280, training loss: 435.5894470214844 = 1.0421044826507568 + 50.0 * 8.690946578979492
Epoch 280, val loss: 1.0433933734893799
Epoch 290, training loss: 435.5960998535156 = 1.0404424667358398 + 50.0 * 8.691113471984863
Epoch 290, val loss: 1.0417985916137695
Epoch 300, training loss: 435.53314208984375 = 1.038690209388733 + 50.0 * 8.689888954162598
Epoch 300, val loss: 1.0400933027267456
Epoch 310, training loss: 435.5707702636719 = 1.0368140935897827 + 50.0 * 8.690679550170898
Epoch 310, val loss: 1.0383422374725342
Epoch 320, training loss: 435.2389831542969 = 1.0346955060958862 + 50.0 * 8.684085845947266
Epoch 320, val loss: 1.0363247394561768
Epoch 330, training loss: 435.31341552734375 = 1.0325043201446533 + 50.0 * 8.68561840057373
Epoch 330, val loss: 1.034195899963379
Epoch 340, training loss: 435.6815490722656 = 1.0302869081497192 + 50.0 * 8.693025588989258
Epoch 340, val loss: 1.0320556163787842
Epoch 350, training loss: 435.53497314453125 = 1.027718186378479 + 50.0 * 8.690145492553711
Epoch 350, val loss: 1.029600739479065
Epoch 360, training loss: 435.4772644042969 = 1.0249333381652832 + 50.0 * 8.689046859741211
Epoch 360, val loss: 1.026943564414978
Epoch 370, training loss: 435.5568542480469 = 1.0220638513565063 + 50.0 * 8.690695762634277
Epoch 370, val loss: 1.0241936445236206
Epoch 380, training loss: 435.53485107421875 = 1.0189485549926758 + 50.0 * 8.69031810760498
Epoch 380, val loss: 1.0212397575378418
Epoch 390, training loss: 435.6285095214844 = 1.0156127214431763 + 50.0 * 8.69225788116455
Epoch 390, val loss: 1.0180495977401733
Epoch 400, training loss: 435.77960205078125 = 1.0120928287506104 + 50.0 * 8.695350646972656
Epoch 400, val loss: 1.0146478414535522
Epoch 410, training loss: 435.8575744628906 = 1.0082730054855347 + 50.0 * 8.696986198425293
Epoch 410, val loss: 1.0109707117080688
Epoch 420, training loss: 435.9191589355469 = 1.0042427778244019 + 50.0 * 8.698298454284668
Epoch 420, val loss: 1.0071073770523071
Epoch 430, training loss: 435.94482421875 = 0.9997351765632629 + 50.0 * 8.698902130126953
Epoch 430, val loss: 1.0028011798858643
Epoch 440, training loss: 435.87890625 = 0.9952166080474854 + 50.0 * 8.697673797607422
Epoch 440, val loss: 0.998494029045105
Epoch 450, training loss: 436.0766906738281 = 0.9904894828796387 + 50.0 * 8.7017240524292
Epoch 450, val loss: 0.9939701557159424
Epoch 460, training loss: 436.1570739746094 = 0.985424816608429 + 50.0 * 8.7034330368042
Epoch 460, val loss: 0.9891712665557861
Epoch 470, training loss: 436.1988830566406 = 0.9800708293914795 + 50.0 * 8.704376220703125
Epoch 470, val loss: 0.9840819835662842
Epoch 480, training loss: 436.2979431152344 = 0.9744308590888977 + 50.0 * 8.706470489501953
Epoch 480, val loss: 0.9787344932556152
Epoch 490, training loss: 436.2608947753906 = 0.9685083627700806 + 50.0 * 8.70584774017334
Epoch 490, val loss: 0.9731258153915405
Epoch 500, training loss: 436.2564697265625 = 0.9623770713806152 + 50.0 * 8.70588207244873
Epoch 500, val loss: 0.967262864112854
Epoch 510, training loss: 436.5906677246094 = 0.9561058282852173 + 50.0 * 8.712691307067871
Epoch 510, val loss: 0.9611597061157227
Epoch 520, training loss: 436.81207275390625 = 0.9495114088058472 + 50.0 * 8.71725082397461
Epoch 520, val loss: 0.9548993706703186
Epoch 530, training loss: 436.4301452636719 = 0.9425503015518188 + 50.0 * 8.709752082824707
Epoch 530, val loss: 0.948376476764679
Epoch 540, training loss: 436.34197998046875 = 0.9355409145355225 + 50.0 * 8.708128929138184
Epoch 540, val loss: 0.941716730594635
Epoch 550, training loss: 436.8882141113281 = 0.9283197522163391 + 50.0 * 8.719198226928711
Epoch 550, val loss: 0.934833288192749
Epoch 560, training loss: 436.81103515625 = 0.9208881855010986 + 50.0 * 8.717803001403809
Epoch 560, val loss: 0.9278010725975037
Epoch 570, training loss: 436.800537109375 = 0.9132022857666016 + 50.0 * 8.71774673461914
Epoch 570, val loss: 0.920474648475647
Epoch 580, training loss: 436.9763488769531 = 0.905320942401886 + 50.0 * 8.721420288085938
Epoch 580, val loss: 0.9129958748817444
Epoch 590, training loss: 437.2532653808594 = 0.8971922397613525 + 50.0 * 8.727121353149414
Epoch 590, val loss: 0.9053326845169067
Epoch 600, training loss: 437.3357849121094 = 0.8889875411987305 + 50.0 * 8.728936195373535
Epoch 600, val loss: 0.897617757320404
Epoch 610, training loss: 437.3847351074219 = 0.880618691444397 + 50.0 * 8.730082511901855
Epoch 610, val loss: 0.8895334005355835
Epoch 620, training loss: 437.66845703125 = 0.8722149133682251 + 50.0 * 8.73592472076416
Epoch 620, val loss: 0.881546676158905
Epoch 630, training loss: 437.7972106933594 = 0.8635808229446411 + 50.0 * 8.738672256469727
Epoch 630, val loss: 0.8734206557273865
Epoch 640, training loss: 437.6648254394531 = 0.8548955917358398 + 50.0 * 8.736198425292969
Epoch 640, val loss: 0.8650593757629395
Epoch 650, training loss: 437.7727966308594 = 0.8460451364517212 + 50.0 * 8.738534927368164
Epoch 650, val loss: 0.8566645979881287
Epoch 660, training loss: 437.4385070800781 = 0.8369735479354858 + 50.0 * 8.732030868530273
Epoch 660, val loss: 0.848077118396759
Epoch 670, training loss: 437.6151428222656 = 0.8283409476280212 + 50.0 * 8.735735893249512
Epoch 670, val loss: 0.8399002552032471
Epoch 680, training loss: 437.927001953125 = 0.819576621055603 + 50.0 * 8.742148399353027
Epoch 680, val loss: 0.8315312266349792
Epoch 690, training loss: 437.983642578125 = 0.8110895752906799 + 50.0 * 8.743451118469238
Epoch 690, val loss: 0.8235265016555786
Epoch 700, training loss: 437.7345275878906 = 0.8020978569984436 + 50.0 * 8.738648414611816
Epoch 700, val loss: 0.8149188756942749
Epoch 710, training loss: 437.8508605957031 = 0.7937021255493164 + 50.0 * 8.741143226623535
Epoch 710, val loss: 0.8070135116577148
Epoch 720, training loss: 438.16107177734375 = 0.7853569984436035 + 50.0 * 8.747513771057129
Epoch 720, val loss: 0.7991384267807007
Epoch 730, training loss: 438.2974548339844 = 0.7769607901573181 + 50.0 * 8.750410079956055
Epoch 730, val loss: 0.7911783456802368
Epoch 740, training loss: 438.2702331542969 = 0.7685195803642273 + 50.0 * 8.75003433227539
Epoch 740, val loss: 0.7831020951271057
Epoch 750, training loss: 438.4281005859375 = 0.7602471113204956 + 50.0 * 8.75335693359375
Epoch 750, val loss: 0.7753230333328247
Epoch 760, training loss: 438.61773681640625 = 0.7521341443061829 + 50.0 * 8.757311820983887
Epoch 760, val loss: 0.7676292061805725
Epoch 770, training loss: 438.8081359863281 = 0.7441534996032715 + 50.0 * 8.761280059814453
Epoch 770, val loss: 0.7600576877593994
Epoch 780, training loss: 438.896240234375 = 0.7361924052238464 + 50.0 * 8.763200759887695
Epoch 780, val loss: 0.7525086402893066
Epoch 790, training loss: 438.79400634765625 = 0.7282444834709167 + 50.0 * 8.76131534576416
Epoch 790, val loss: 0.7450725436210632
Epoch 800, training loss: 439.17919921875 = 0.7208819389343262 + 50.0 * 8.769165992736816
Epoch 800, val loss: 0.7381877303123474
Epoch 810, training loss: 438.88360595703125 = 0.7131667137145996 + 50.0 * 8.763408660888672
Epoch 810, val loss: 0.7309617400169373
Epoch 820, training loss: 439.0922546386719 = 0.7058120965957642 + 50.0 * 8.767728805541992
Epoch 820, val loss: 0.7239077091217041
Epoch 830, training loss: 439.2369079589844 = 0.6986960172653198 + 50.0 * 8.770764350891113
Epoch 830, val loss: 0.7173175811767578
Epoch 840, training loss: 439.1630859375 = 0.6916983723640442 + 50.0 * 8.769427299499512
Epoch 840, val loss: 0.710698127746582
Epoch 850, training loss: 439.45892333984375 = 0.6849735379219055 + 50.0 * 8.775479316711426
Epoch 850, val loss: 0.7043533325195312
Epoch 860, training loss: 439.5392150878906 = 0.6782049536705017 + 50.0 * 8.777220726013184
Epoch 860, val loss: 0.698061466217041
Epoch 870, training loss: 439.53741455078125 = 0.6716781854629517 + 50.0 * 8.777314186096191
Epoch 870, val loss: 0.6919964551925659
Epoch 880, training loss: 439.2762756347656 = 0.6651145815849304 + 50.0 * 8.772223472595215
Epoch 880, val loss: 0.6858453750610352
Epoch 890, training loss: 439.2930908203125 = 0.6589930057525635 + 50.0 * 8.772682189941406
Epoch 890, val loss: 0.6801726222038269
Epoch 900, training loss: 439.2551574707031 = 0.6531178951263428 + 50.0 * 8.772041320800781
Epoch 900, val loss: 0.674765408039093
Epoch 910, training loss: 439.5294494628906 = 0.6473618745803833 + 50.0 * 8.777641296386719
Epoch 910, val loss: 0.669418215751648
Epoch 920, training loss: 439.7863464355469 = 0.6417805552482605 + 50.0 * 8.782891273498535
Epoch 920, val loss: 0.6642668843269348
Epoch 930, training loss: 439.95989990234375 = 0.6363009214401245 + 50.0 * 8.78647232055664
Epoch 930, val loss: 0.6591537594795227
Epoch 940, training loss: 440.0054016113281 = 0.6307560205459595 + 50.0 * 8.787492752075195
Epoch 940, val loss: 0.6541013717651367
Epoch 950, training loss: 440.1611633300781 = 0.6255370378494263 + 50.0 * 8.790712356567383
Epoch 950, val loss: 0.6493290066719055
Epoch 960, training loss: 440.31207275390625 = 0.6204147338867188 + 50.0 * 8.793832778930664
Epoch 960, val loss: 0.644609808921814
Epoch 970, training loss: 439.1764221191406 = 0.614923894405365 + 50.0 * 8.77122974395752
Epoch 970, val loss: 0.6399643421173096
Epoch 980, training loss: 439.8638000488281 = 0.6104638576507568 + 50.0 * 8.785066604614258
Epoch 980, val loss: 0.6356918811798096
Epoch 990, training loss: 439.95184326171875 = 0.6059690713882446 + 50.0 * 8.786917686462402
Epoch 990, val loss: 0.631604015827179
Epoch 1000, training loss: 439.5727233886719 = 0.6014450788497925 + 50.0 * 8.779425621032715
Epoch 1000, val loss: 0.6274546980857849
Epoch 1010, training loss: 439.9158935546875 = 0.5969690084457397 + 50.0 * 8.786378860473633
Epoch 1010, val loss: 0.6234892010688782
Epoch 1020, training loss: 440.35284423828125 = 0.5928573608398438 + 50.0 * 8.795199394226074
Epoch 1020, val loss: 0.6198135018348694
Epoch 1030, training loss: 440.5883483886719 = 0.5886301398277283 + 50.0 * 8.799994468688965
Epoch 1030, val loss: 0.6160274147987366
Epoch 1040, training loss: 440.68646240234375 = 0.5845159292221069 + 50.0 * 8.80203914642334
Epoch 1040, val loss: 0.6122681498527527
Epoch 1050, training loss: 440.71820068359375 = 0.5804727673530579 + 50.0 * 8.802754402160645
Epoch 1050, val loss: 0.6087388396263123
Epoch 1060, training loss: 440.9877624511719 = 0.5765448808670044 + 50.0 * 8.80822467803955
Epoch 1060, val loss: 0.6053113341331482
Epoch 1070, training loss: 441.1228942871094 = 0.5727120637893677 + 50.0 * 8.811003684997559
Epoch 1070, val loss: 0.6019081473350525
Epoch 1080, training loss: 441.0932922363281 = 0.5689496397972107 + 50.0 * 8.810486793518066
Epoch 1080, val loss: 0.59865403175354
Epoch 1090, training loss: 441.32293701171875 = 0.5652906894683838 + 50.0 * 8.815153121948242
Epoch 1090, val loss: 0.5955086946487427
Epoch 1100, training loss: 441.2511901855469 = 0.5615857839584351 + 50.0 * 8.81379222869873
Epoch 1100, val loss: 0.5923577547073364
Epoch 1110, training loss: 441.2688903808594 = 0.5582060217857361 + 50.0 * 8.814213752746582
Epoch 1110, val loss: 0.5895444750785828
Epoch 1120, training loss: 441.4344787597656 = 0.5548911690711975 + 50.0 * 8.817591667175293
Epoch 1120, val loss: 0.5867241024971008
Epoch 1130, training loss: 441.6932067871094 = 0.5516244173049927 + 50.0 * 8.822831153869629
Epoch 1130, val loss: 0.5839790105819702
Epoch 1140, training loss: 441.70452880859375 = 0.5483604073524475 + 50.0 * 8.82312297821045
Epoch 1140, val loss: 0.5812896490097046
Epoch 1150, training loss: 441.7525634765625 = 0.5451526641845703 + 50.0 * 8.824148178100586
Epoch 1150, val loss: 0.578534722328186
Epoch 1160, training loss: 441.2537536621094 = 0.5418754816055298 + 50.0 * 8.814237594604492
Epoch 1160, val loss: 0.5757960677146912
Epoch 1170, training loss: 441.2810974121094 = 0.5386757850646973 + 50.0 * 8.814848899841309
Epoch 1170, val loss: 0.5732325911521912
Epoch 1180, training loss: 441.89703369140625 = 0.5361188054084778 + 50.0 * 8.827218055725098
Epoch 1180, val loss: 0.5712870359420776
Epoch 1190, training loss: 441.54864501953125 = 0.5334558486938477 + 50.0 * 8.820303916931152
Epoch 1190, val loss: 0.5688987970352173
Epoch 1200, training loss: 441.815673828125 = 0.5304796695709229 + 50.0 * 8.825703620910645
Epoch 1200, val loss: 0.5667355060577393
Epoch 1210, training loss: 442.4364013671875 = 0.5279917120933533 + 50.0 * 8.838168144226074
Epoch 1210, val loss: 0.5646709203720093
Epoch 1220, training loss: 442.3022155761719 = 0.5252357721328735 + 50.0 * 8.835539817810059
Epoch 1220, val loss: 0.5624381303787231
Epoch 1230, training loss: 442.6554260253906 = 0.5226074457168579 + 50.0 * 8.842656135559082
Epoch 1230, val loss: 0.5604076385498047
Epoch 1240, training loss: 442.69586181640625 = 0.5200894474983215 + 50.0 * 8.843515396118164
Epoch 1240, val loss: 0.5584745407104492
Epoch 1250, training loss: 442.90106201171875 = 0.5175386071205139 + 50.0 * 8.847670555114746
Epoch 1250, val loss: 0.5565117001533508
Epoch 1260, training loss: 442.96807861328125 = 0.5150075554847717 + 50.0 * 8.849061012268066
Epoch 1260, val loss: 0.5546032786369324
Epoch 1270, training loss: 443.10296630859375 = 0.512549877166748 + 50.0 * 8.851808547973633
Epoch 1270, val loss: 0.552727460861206
Epoch 1280, training loss: 442.87945556640625 = 0.5101076364517212 + 50.0 * 8.847387313842773
Epoch 1280, val loss: 0.5508444905281067
Epoch 1290, training loss: 443.24517822265625 = 0.5077608227729797 + 50.0 * 8.854748725891113
Epoch 1290, val loss: 0.5490880608558655
Epoch 1300, training loss: 443.3963317871094 = 0.5054540038108826 + 50.0 * 8.857817649841309
Epoch 1300, val loss: 0.5473740100860596
Epoch 1310, training loss: 443.41265869140625 = 0.5032333135604858 + 50.0 * 8.85818862915039
Epoch 1310, val loss: 0.5456942319869995
Epoch 1320, training loss: 443.3616943359375 = 0.501033365726471 + 50.0 * 8.857213020324707
Epoch 1320, val loss: 0.5443087220191956
Epoch 1330, training loss: 443.27215576171875 = 0.49878156185150146 + 50.0 * 8.855467796325684
Epoch 1330, val loss: 0.5424872636795044
Epoch 1340, training loss: 443.51312255859375 = 0.4965660572052002 + 50.0 * 8.860331535339355
Epoch 1340, val loss: 0.5409980416297913
Epoch 1350, training loss: 443.72503662109375 = 0.49453824758529663 + 50.0 * 8.864609718322754
Epoch 1350, val loss: 0.5395301580429077
Epoch 1360, training loss: 443.80322265625 = 0.49240389466285706 + 50.0 * 8.866216659545898
Epoch 1360, val loss: 0.5380459427833557
Epoch 1370, training loss: 443.9127197265625 = 0.49026837944984436 + 50.0 * 8.868449211120605
Epoch 1370, val loss: 0.5366603136062622
Epoch 1380, training loss: 443.9763488769531 = 0.4882163107395172 + 50.0 * 8.869762420654297
Epoch 1380, val loss: 0.5352071523666382
Epoch 1390, training loss: 443.9197082519531 = 0.4862290024757385 + 50.0 * 8.868669509887695
Epoch 1390, val loss: 0.5338249802589417
Epoch 1400, training loss: 444.1413269042969 = 0.4842658042907715 + 50.0 * 8.873141288757324
Epoch 1400, val loss: 0.5325092077255249
Epoch 1410, training loss: 444.2111511230469 = 0.48222362995147705 + 50.0 * 8.874578475952148
Epoch 1410, val loss: 0.531138002872467
Epoch 1420, training loss: 442.4315490722656 = 0.4802750051021576 + 50.0 * 8.839025497436523
Epoch 1420, val loss: 0.5292569994926453
Epoch 1430, training loss: 444.0635681152344 = 0.4791719317436218 + 50.0 * 8.871687889099121
Epoch 1430, val loss: 0.5297126770019531
Epoch 1440, training loss: 443.1615905761719 = 0.47700434923171997 + 50.0 * 8.853692054748535
Epoch 1440, val loss: 0.5275158286094666
Epoch 1450, training loss: 443.0292663574219 = 0.4750322699546814 + 50.0 * 8.85108470916748
Epoch 1450, val loss: 0.5260885953903198
Epoch 1460, training loss: 443.158935546875 = 0.4734692871570587 + 50.0 * 8.85370922088623
Epoch 1460, val loss: 0.5252960920333862
Epoch 1470, training loss: 443.33917236328125 = 0.4716162085533142 + 50.0 * 8.857351303100586
Epoch 1470, val loss: 0.5243337154388428
Epoch 1480, training loss: 443.72259521484375 = 0.4697754681110382 + 50.0 * 8.865056037902832
Epoch 1480, val loss: 0.5228721499443054
Epoch 1490, training loss: 444.1812744140625 = 0.46783334016799927 + 50.0 * 8.874268531799316
Epoch 1490, val loss: 0.5216434597969055
Epoch 1500, training loss: 444.1505126953125 = 0.4660411477088928 + 50.0 * 8.873689651489258
Epoch 1500, val loss: 0.5204387307167053
Epoch 1510, training loss: 444.36767578125 = 0.4642552137374878 + 50.0 * 8.878067970275879
Epoch 1510, val loss: 0.5192103981971741
Epoch 1520, training loss: 444.4822998046875 = 0.4624592363834381 + 50.0 * 8.880396842956543
Epoch 1520, val loss: 0.517998218536377
Epoch 1530, training loss: 444.4502258300781 = 0.46067187190055847 + 50.0 * 8.879791259765625
Epoch 1530, val loss: 0.5167564153671265
Epoch 1540, training loss: 444.6562805175781 = 0.4589630365371704 + 50.0 * 8.883946418762207
Epoch 1540, val loss: 0.5157139897346497
Epoch 1550, training loss: 444.7921447753906 = 0.45721474289894104 + 50.0 * 8.886698722839355
Epoch 1550, val loss: 0.51457279920578
Epoch 1560, training loss: 444.8847351074219 = 0.45552244782447815 + 50.0 * 8.88858413696289
Epoch 1560, val loss: 0.5134649872779846
Epoch 1570, training loss: 444.7871398925781 = 0.4538368582725525 + 50.0 * 8.886666297912598
Epoch 1570, val loss: 0.5124126672744751
Epoch 1580, training loss: 445.00592041015625 = 0.45222237706184387 + 50.0 * 8.891074180603027
Epoch 1580, val loss: 0.5113828778266907
Epoch 1590, training loss: 445.14593505859375 = 0.45058751106262207 + 50.0 * 8.893906593322754
Epoch 1590, val loss: 0.5102877020835876
Epoch 1600, training loss: 444.9271545410156 = 0.4489692747592926 + 50.0 * 8.88956356048584
Epoch 1600, val loss: 0.5093933343887329
Epoch 1610, training loss: 445.0215759277344 = 0.4473673701286316 + 50.0 * 8.891484260559082
Epoch 1610, val loss: 0.5082927942276001
Epoch 1620, training loss: 445.2007751464844 = 0.4457333981990814 + 50.0 * 8.895100593566895
Epoch 1620, val loss: 0.5072895884513855
Epoch 1630, training loss: 445.2971496582031 = 0.4441194534301758 + 50.0 * 8.89706039428711
Epoch 1630, val loss: 0.5062990784645081
Epoch 1640, training loss: 445.3446044921875 = 0.4425140917301178 + 50.0 * 8.898041725158691
Epoch 1640, val loss: 0.5053002238273621
Epoch 1650, training loss: 445.5296325683594 = 0.4410087764263153 + 50.0 * 8.901772499084473
Epoch 1650, val loss: 0.5043944120407104
Epoch 1660, training loss: 445.67132568359375 = 0.43945521116256714 + 50.0 * 8.904637336730957
Epoch 1660, val loss: 0.5033951997756958
Epoch 1670, training loss: 445.667236328125 = 0.43780937790870667 + 50.0 * 8.90458869934082
Epoch 1670, val loss: 0.5024310350418091
Epoch 1680, training loss: 445.75885009765625 = 0.43628087639808655 + 50.0 * 8.906451225280762
Epoch 1680, val loss: 0.5014716982841492
Epoch 1690, training loss: 444.99078369140625 = 0.4347875118255615 + 50.0 * 8.891119956970215
Epoch 1690, val loss: 0.5003389716148376
Epoch 1700, training loss: 444.69183349609375 = 0.43312230706214905 + 50.0 * 8.885173797607422
Epoch 1700, val loss: 0.4997641444206238
Epoch 1710, training loss: 444.88525390625 = 0.4320133924484253 + 50.0 * 8.88906478881836
Epoch 1710, val loss: 0.4991883933544159
Epoch 1720, training loss: 445.25872802734375 = 0.4305051267147064 + 50.0 * 8.896564483642578
Epoch 1720, val loss: 0.4983803629875183
Epoch 1730, training loss: 445.4794921875 = 0.4289683997631073 + 50.0 * 8.901010513305664
Epoch 1730, val loss: 0.49726784229278564
Epoch 1740, training loss: 445.85101318359375 = 0.42747482657432556 + 50.0 * 8.90847110748291
Epoch 1740, val loss: 0.4964635670185089
Epoch 1750, training loss: 445.99969482421875 = 0.4260267913341522 + 50.0 * 8.911473274230957
Epoch 1750, val loss: 0.4955631196498871
Epoch 1760, training loss: 446.1030578613281 = 0.4246060848236084 + 50.0 * 8.913569450378418
Epoch 1760, val loss: 0.4945943355560303
Epoch 1770, training loss: 446.2720642089844 = 0.42320454120635986 + 50.0 * 8.916976928710938
Epoch 1770, val loss: 0.4938144087791443
Epoch 1780, training loss: 446.34942626953125 = 0.42171233892440796 + 50.0 * 8.918554306030273
Epoch 1780, val loss: 0.4928733706474304
Epoch 1790, training loss: 446.46136474609375 = 0.42020753026008606 + 50.0 * 8.920823097229004
Epoch 1790, val loss: 0.4919102191925049
Epoch 1800, training loss: 446.3510437011719 = 0.4186783730983734 + 50.0 * 8.918647766113281
Epoch 1800, val loss: 0.49095627665519714
Epoch 1810, training loss: 446.50091552734375 = 0.41723403334617615 + 50.0 * 8.921673774719238
Epoch 1810, val loss: 0.4900023341178894
Epoch 1820, training loss: 446.7079772949219 = 0.41578638553619385 + 50.0 * 8.925844192504883
Epoch 1820, val loss: 0.48915135860443115
Epoch 1830, training loss: 446.69482421875 = 0.4142478406429291 + 50.0 * 8.92561149597168
Epoch 1830, val loss: 0.48818516731262207
Epoch 1840, training loss: 447.1133728027344 = 0.41302499175071716 + 50.0 * 8.934006690979004
Epoch 1840, val loss: 0.48756369948387146
Epoch 1850, training loss: 447.2093200683594 = 0.4116649329662323 + 50.0 * 8.935953140258789
Epoch 1850, val loss: 0.4866389334201813
Epoch 1860, training loss: 447.4216003417969 = 0.41027140617370605 + 50.0 * 8.940226554870605
Epoch 1860, val loss: 0.4856727123260498
Epoch 1870, training loss: 447.4866027832031 = 0.4087989628314972 + 50.0 * 8.941555976867676
Epoch 1870, val loss: 0.4849034547805786
Epoch 1880, training loss: 447.45867919921875 = 0.4073863923549652 + 50.0 * 8.941025733947754
Epoch 1880, val loss: 0.4840574562549591
Epoch 1890, training loss: 447.58050537109375 = 0.40590330958366394 + 50.0 * 8.94349193572998
Epoch 1890, val loss: 0.48283863067626953
Epoch 1900, training loss: 447.6487731933594 = 0.40445855259895325 + 50.0 * 8.944886207580566
Epoch 1900, val loss: 0.4826612174510956
Epoch 1910, training loss: 447.78448486328125 = 0.4031614065170288 + 50.0 * 8.947626113891602
Epoch 1910, val loss: 0.48177850246429443
Epoch 1920, training loss: 447.89788818359375 = 0.4017890393733978 + 50.0 * 8.949921607971191
Epoch 1920, val loss: 0.4809134602546692
Epoch 1930, training loss: 447.83575439453125 = 0.4003962278366089 + 50.0 * 8.948707580566406
Epoch 1930, val loss: 0.4801807701587677
Epoch 1940, training loss: 447.9504089355469 = 0.3990077078342438 + 50.0 * 8.951027870178223
Epoch 1940, val loss: 0.4794018864631653
Epoch 1950, training loss: 447.86810302734375 = 0.3975874185562134 + 50.0 * 8.949410438537598
Epoch 1950, val loss: 0.47866091132164
Epoch 1960, training loss: 448.1219787597656 = 0.39619895815849304 + 50.0 * 8.95451545715332
Epoch 1960, val loss: 0.4778282046318054
Epoch 1970, training loss: 447.9465026855469 = 0.39487209916114807 + 50.0 * 8.951032638549805
Epoch 1970, val loss: 0.4772391617298126
Epoch 1980, training loss: 448.08477783203125 = 0.3935212194919586 + 50.0 * 8.953824996948242
Epoch 1980, val loss: 0.4765051007270813
Epoch 1990, training loss: 448.314208984375 = 0.3921319246292114 + 50.0 * 8.958441734313965
Epoch 1990, val loss: 0.4757474362850189
Epoch 2000, training loss: 448.1728210449219 = 0.3906937539577484 + 50.0 * 8.955642700195312
Epoch 2000, val loss: 0.47499170899391174
Epoch 2010, training loss: 447.35516357421875 = 0.3890429437160492 + 50.0 * 8.939322471618652
Epoch 2010, val loss: 0.47472140192985535
Epoch 2020, training loss: 446.57568359375 = 0.3890082836151123 + 50.0 * 8.923733711242676
Epoch 2020, val loss: 0.47417229413986206
Epoch 2030, training loss: 446.7370300292969 = 0.3878658711910248 + 50.0 * 8.926982879638672
Epoch 2030, val loss: 0.4744240939617157
Epoch 2040, training loss: 447.0889892578125 = 0.38649776577949524 + 50.0 * 8.934049606323242
Epoch 2040, val loss: 0.4727540612220764
Epoch 2050, training loss: 447.6686706542969 = 0.3852320909500122 + 50.0 * 8.945669174194336
Epoch 2050, val loss: 0.4722621738910675
Epoch 2060, training loss: 448.11669921875 = 0.38380947709083557 + 50.0 * 8.954657554626465
Epoch 2060, val loss: 0.4710651636123657
Epoch 2070, training loss: 448.449951171875 = 0.38241496682167053 + 50.0 * 8.961350440979004
Epoch 2070, val loss: 0.4703781008720398
Epoch 2080, training loss: 448.5266418457031 = 0.3810133635997772 + 50.0 * 8.962912559509277
Epoch 2080, val loss: 0.46962592005729675
Epoch 2090, training loss: 448.4741516113281 = 0.3796525001525879 + 50.0 * 8.96189022064209
Epoch 2090, val loss: 0.46898147463798523
Epoch 2100, training loss: 444.0223388671875 = 0.38001230359077454 + 50.0 * 8.872846603393555
Epoch 2100, val loss: 0.4677675664424896
Epoch 2110, training loss: 443.1796875 = 0.3800288140773773 + 50.0 * 8.855993270874023
Epoch 2110, val loss: 0.47225162386894226
Epoch 2120, training loss: 444.2642517089844 = 0.3781154751777649 + 50.0 * 8.87772274017334
Epoch 2120, val loss: 0.46872204542160034
Epoch 2130, training loss: 445.14764404296875 = 0.37716975808143616 + 50.0 * 8.89540958404541
Epoch 2130, val loss: 0.46773940324783325
Epoch 2140, training loss: 444.9866638183594 = 0.37590885162353516 + 50.0 * 8.89221477508545
Epoch 2140, val loss: 0.4680473208427429
Epoch 2150, training loss: 445.60992431640625 = 0.37457871437072754 + 50.0 * 8.904706954956055
Epoch 2150, val loss: 0.46675121784210205
Epoch 2160, training loss: 446.29595947265625 = 0.3731900453567505 + 50.0 * 8.918455123901367
Epoch 2160, val loss: 0.46629464626312256
Epoch 2170, training loss: 446.7961120605469 = 0.3718167841434479 + 50.0 * 8.928485870361328
Epoch 2170, val loss: 0.4657001197338104
Epoch 2180, training loss: 447.2078552246094 = 0.3704783022403717 + 50.0 * 8.936747550964355
Epoch 2180, val loss: 0.4649844765663147
Epoch 2190, training loss: 447.4908142089844 = 0.36909982562065125 + 50.0 * 8.942434310913086
Epoch 2190, val loss: 0.46441376209259033
Epoch 2200, training loss: 447.6133728027344 = 0.367735892534256 + 50.0 * 8.944912910461426
Epoch 2200, val loss: 0.4637826085090637
Epoch 2210, training loss: 447.42333984375 = 0.3663332164287567 + 50.0 * 8.941140174865723
Epoch 2210, val loss: 0.4631458520889282
Epoch 2220, training loss: 447.6170654296875 = 0.3650006651878357 + 50.0 * 8.94504165649414
Epoch 2220, val loss: 0.4626351296901703
Epoch 2230, training loss: 447.88189697265625 = 0.3636743724346161 + 50.0 * 8.950364112854004
Epoch 2230, val loss: 0.462034672498703
Epoch 2240, training loss: 448.2002868652344 = 0.36236006021499634 + 50.0 * 8.956758499145508
Epoch 2240, val loss: 0.4613759517669678
Epoch 2250, training loss: 448.1583251953125 = 0.36102235317230225 + 50.0 * 8.95594596862793
Epoch 2250, val loss: 0.4606997072696686
Epoch 2260, training loss: 447.9378967285156 = 0.3596058487892151 + 50.0 * 8.951565742492676
Epoch 2260, val loss: 0.4601004421710968
Epoch 2270, training loss: 445.4691162109375 = 0.35837388038635254 + 50.0 * 8.902215003967285
Epoch 2270, val loss: 0.4583490192890167
Epoch 2280, training loss: 446.1156921386719 = 0.3572787344455719 + 50.0 * 8.915168762207031
Epoch 2280, val loss: 0.46008747816085815
Epoch 2290, training loss: 447.24664306640625 = 0.35621967911720276 + 50.0 * 8.9378080368042
Epoch 2290, val loss: 0.4582584798336029
Epoch 2300, training loss: 447.5284118652344 = 0.35484009981155396 + 50.0 * 8.943471908569336
Epoch 2300, val loss: 0.45797526836395264
Epoch 2310, training loss: 447.5570068359375 = 0.3535082936286926 + 50.0 * 8.944069862365723
Epoch 2310, val loss: 0.45699626207351685
Epoch 2320, training loss: 447.82244873046875 = 0.3521925210952759 + 50.0 * 8.9494047164917
Epoch 2320, val loss: 0.45713451504707336
Epoch 2330, training loss: 448.0046691894531 = 0.3508587181568146 + 50.0 * 8.953076362609863
Epoch 2330, val loss: 0.4564160406589508
Epoch 2340, training loss: 448.3539123535156 = 0.34943142533302307 + 50.0 * 8.960089683532715
Epoch 2340, val loss: 0.456074059009552
Epoch 2350, training loss: 448.48846435546875 = 0.3479086756706238 + 50.0 * 8.962811470031738
Epoch 2350, val loss: 0.45552563667297363
Epoch 2360, training loss: 448.60675048828125 = 0.3464251160621643 + 50.0 * 8.965206146240234
Epoch 2360, val loss: 0.4550887644290924
Epoch 2370, training loss: 448.87738037109375 = 0.34502798318862915 + 50.0 * 8.970646858215332
Epoch 2370, val loss: 0.4544304609298706
Epoch 2380, training loss: 448.8548889160156 = 0.34363365173339844 + 50.0 * 8.97022533416748
Epoch 2380, val loss: 0.45399805903434753
Epoch 2390, training loss: 448.8420715332031 = 0.34220489859580994 + 50.0 * 8.96999740600586
Epoch 2390, val loss: 0.45341983437538147
Epoch 2400, training loss: 448.9886474609375 = 0.34082064032554626 + 50.0 * 8.972956657409668
Epoch 2400, val loss: 0.4529029130935669
Epoch 2410, training loss: 449.0967712402344 = 0.339431494474411 + 50.0 * 8.975147247314453
Epoch 2410, val loss: 0.45241647958755493
Epoch 2420, training loss: 449.099609375 = 0.33803609013557434 + 50.0 * 8.975231170654297
Epoch 2420, val loss: 0.4516982138156891
Epoch 2430, training loss: 449.248779296875 = 0.33663031458854675 + 50.0 * 8.978242874145508
Epoch 2430, val loss: 0.45131003856658936
Epoch 2440, training loss: 449.3816833496094 = 0.33526599407196045 + 50.0 * 8.980928421020508
Epoch 2440, val loss: 0.45079511404037476
Epoch 2450, training loss: 449.4519348144531 = 0.3338949680328369 + 50.0 * 8.98236083984375
Epoch 2450, val loss: 0.45032334327697754
Epoch 2460, training loss: 449.2754821777344 = 0.33250126242637634 + 50.0 * 8.978859901428223
Epoch 2460, val loss: 0.44970792531967163
Epoch 2470, training loss: 449.35968017578125 = 0.3311464786529541 + 50.0 * 8.980570793151855
Epoch 2470, val loss: 0.44937989115715027
Epoch 2480, training loss: 449.4838562011719 = 0.3297850489616394 + 50.0 * 8.983081817626953
Epoch 2480, val loss: 0.4489676058292389
Epoch 2490, training loss: 449.580810546875 = 0.3284154236316681 + 50.0 * 8.985047340393066
Epoch 2490, val loss: 0.44855165481567383
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8318840579710145
0.8658262696515251
=== training gcn model ===
Epoch 0, training loss: 509.3053283691406 = 1.1084977388381958 + 50.0 * 10.163936614990234
Epoch 0, val loss: 1.1078153848648071
Epoch 10, training loss: 486.23040771484375 = 1.1041933298110962 + 50.0 * 9.702524185180664
Epoch 10, val loss: 1.103619933128357
Epoch 20, training loss: 474.719482421875 = 1.1003416776657104 + 50.0 * 9.472382545471191
Epoch 20, val loss: 1.0998356342315674
Epoch 30, training loss: 467.5411682128906 = 1.0966155529022217 + 50.0 * 9.328890800476074
Epoch 30, val loss: 1.096168041229248
Epoch 40, training loss: 462.21405029296875 = 1.0930753946304321 + 50.0 * 9.222419738769531
Epoch 40, val loss: 1.0926872491836548
Epoch 50, training loss: 458.05596923828125 = 1.0897088050842285 + 50.0 * 9.139325141906738
Epoch 50, val loss: 1.089378833770752
Epoch 60, training loss: 454.6202087402344 = 1.0865137577056885 + 50.0 * 9.070673942565918
Epoch 60, val loss: 1.0862350463867188
Epoch 70, training loss: 451.63397216796875 = 1.0834531784057617 + 50.0 * 9.01101016998291
Epoch 70, val loss: 1.0832260847091675
Epoch 80, training loss: 449.3591613769531 = 1.0805332660675049 + 50.0 * 8.965572357177734
Epoch 80, val loss: 1.0803529024124146
Epoch 90, training loss: 447.1304626464844 = 1.0777443647384644 + 50.0 * 8.921053886413574
Epoch 90, val loss: 1.0776172876358032
Epoch 100, training loss: 445.2456359863281 = 1.0750898122787476 + 50.0 * 8.883410453796387
Epoch 100, val loss: 1.0750056505203247
Epoch 110, training loss: 443.6536560058594 = 1.0725319385528564 + 50.0 * 8.851622581481934
Epoch 110, val loss: 1.0724995136260986
Epoch 120, training loss: 442.2964172363281 = 1.0700600147247314 + 50.0 * 8.8245267868042
Epoch 120, val loss: 1.0700839757919312
Epoch 130, training loss: 441.1325378417969 = 1.067700982093811 + 50.0 * 8.801297187805176
Epoch 130, val loss: 1.0677778720855713
Epoch 140, training loss: 440.04254150390625 = 1.065418004989624 + 50.0 * 8.779541969299316
Epoch 140, val loss: 1.0655511617660522
Epoch 150, training loss: 439.20672607421875 = 1.0632327795028687 + 50.0 * 8.762869834899902
Epoch 150, val loss: 1.0634201765060425
Epoch 160, training loss: 438.36920166015625 = 1.0611166954040527 + 50.0 * 8.746161460876465
Epoch 160, val loss: 1.0613834857940674
Epoch 170, training loss: 437.60418701171875 = 1.0590314865112305 + 50.0 * 8.730903625488281
Epoch 170, val loss: 1.0593671798706055
Epoch 180, training loss: 436.981201171875 = 1.0570040941238403 + 50.0 * 8.718483924865723
Epoch 180, val loss: 1.057399868965149
Epoch 190, training loss: 436.60113525390625 = 1.0550367832183838 + 50.0 * 8.710922241210938
Epoch 190, val loss: 1.0555105209350586
Epoch 200, training loss: 436.1360168457031 = 1.0530927181243896 + 50.0 * 8.701658248901367
Epoch 200, val loss: 1.0536059141159058
Epoch 210, training loss: 435.7870178222656 = 1.051069736480713 + 50.0 * 8.694719314575195
Epoch 210, val loss: 1.0517044067382812
Epoch 220, training loss: 438.6920471191406 = 1.0488688945770264 + 50.0 * 8.752863883972168
Epoch 220, val loss: 1.049450159072876
Epoch 230, training loss: 435.7752990722656 = 1.045798420906067 + 50.0 * 8.694589614868164
Epoch 230, val loss: 1.0466803312301636
Epoch 240, training loss: 435.6803283691406 = 1.0450612306594849 + 50.0 * 8.692705154418945
Epoch 240, val loss: 1.0460282564163208
Epoch 250, training loss: 435.001953125 = 1.0429185628890991 + 50.0 * 8.679181098937988
Epoch 250, val loss: 1.043776512145996
Epoch 260, training loss: 434.1302795410156 = 1.040488600730896 + 50.0 * 8.661795616149902
Epoch 260, val loss: 1.041568398475647
Epoch 270, training loss: 434.1593933105469 = 1.0385196208953857 + 50.0 * 8.6624174118042
Epoch 270, val loss: 1.039625644683838
Epoch 280, training loss: 434.0912780761719 = 1.0362414121627808 + 50.0 * 8.661100387573242
Epoch 280, val loss: 1.0374144315719604
Epoch 290, training loss: 433.9347229003906 = 1.0337873697280884 + 50.0 * 8.658019065856934
Epoch 290, val loss: 1.0350661277770996
Epoch 300, training loss: 433.8172302246094 = 1.0311994552612305 + 50.0 * 8.655720710754395
Epoch 300, val loss: 1.03257155418396
Epoch 310, training loss: 433.80560302734375 = 1.0285511016845703 + 50.0 * 8.655540466308594
Epoch 310, val loss: 1.0299861431121826
Epoch 320, training loss: 433.6875305175781 = 1.0257341861724854 + 50.0 * 8.653236389160156
Epoch 320, val loss: 1.0272365808486938
Epoch 330, training loss: 433.576171875 = 1.0227439403533936 + 50.0 * 8.651068687438965
Epoch 330, val loss: 1.024351954460144
Epoch 340, training loss: 433.6004943847656 = 1.0196435451507568 + 50.0 * 8.651617050170898
Epoch 340, val loss: 1.02133309841156
Epoch 350, training loss: 433.51171875 = 1.0162720680236816 + 50.0 * 8.649909019470215
Epoch 350, val loss: 1.0180469751358032
Epoch 360, training loss: 433.4708557128906 = 1.0127296447753906 + 50.0 * 8.649162292480469
Epoch 360, val loss: 1.0146093368530273
Epoch 370, training loss: 433.4733581542969 = 1.0090287923812866 + 50.0 * 8.649286270141602
Epoch 370, val loss: 1.0109962224960327
Epoch 380, training loss: 433.3586120605469 = 1.0050510168075562 + 50.0 * 8.64707088470459
Epoch 380, val loss: 1.007142186164856
Epoch 390, training loss: 433.3272705078125 = 1.0009329319000244 + 50.0 * 8.646526336669922
Epoch 390, val loss: 1.0031226873397827
Epoch 400, training loss: 433.29803466796875 = 0.9965256452560425 + 50.0 * 8.64603042602539
Epoch 400, val loss: 0.9988470077514648
Epoch 410, training loss: 433.22979736328125 = 0.9918668270111084 + 50.0 * 8.644759178161621
Epoch 410, val loss: 0.9943059086799622
Epoch 420, training loss: 433.3181457519531 = 0.9869605302810669 + 50.0 * 8.646623611450195
Epoch 420, val loss: 0.9895308017730713
Epoch 430, training loss: 433.2582092285156 = 0.9818121194839478 + 50.0 * 8.645527839660645
Epoch 430, val loss: 0.9844996333122253
Epoch 440, training loss: 433.2684631347656 = 0.9764259457588196 + 50.0 * 8.645840644836426
Epoch 440, val loss: 0.9792473912239075
Epoch 450, training loss: 433.3086242675781 = 0.9707403779029846 + 50.0 * 8.646758079528809
Epoch 450, val loss: 0.9737085103988647
Epoch 460, training loss: 433.34747314453125 = 0.9645609259605408 + 50.0 * 8.647658348083496
Epoch 460, val loss: 0.9676404595375061
Epoch 470, training loss: 433.33612060546875 = 0.9585097432136536 + 50.0 * 8.647552490234375
Epoch 470, val loss: 0.9618375897407532
Epoch 480, training loss: 433.3782958984375 = 0.9521036148071289 + 50.0 * 8.648523330688477
Epoch 480, val loss: 0.9555314779281616
Epoch 490, training loss: 433.5037841796875 = 0.945256233215332 + 50.0 * 8.65117073059082
Epoch 490, val loss: 0.9488402009010315
Epoch 500, training loss: 433.38812255859375 = 0.9382565021514893 + 50.0 * 8.64899730682373
Epoch 500, val loss: 0.9419279098510742
Epoch 510, training loss: 433.4109191894531 = 0.9311172366142273 + 50.0 * 8.649596214294434
Epoch 510, val loss: 0.9350281953811646
Epoch 520, training loss: 433.4456787109375 = 0.9236207604408264 + 50.0 * 8.65044116973877
Epoch 520, val loss: 0.9277491569519043
Epoch 530, training loss: 433.388427734375 = 0.9159601926803589 + 50.0 * 8.649449348449707
Epoch 530, val loss: 0.920268714427948
Epoch 540, training loss: 433.5013122558594 = 0.9079302549362183 + 50.0 * 8.651867866516113
Epoch 540, val loss: 0.9124664664268494
Epoch 550, training loss: 433.69873046875 = 0.8997326493263245 + 50.0 * 8.655980110168457
Epoch 550, val loss: 0.9044609069824219
Epoch 560, training loss: 433.7159729003906 = 0.8912672996520996 + 50.0 * 8.656494140625
Epoch 560, val loss: 0.8962217569351196
Epoch 570, training loss: 433.82354736328125 = 0.8825845122337341 + 50.0 * 8.658819198608398
Epoch 570, val loss: 0.8878586888313293
Epoch 580, training loss: 433.8908386230469 = 0.8737813234329224 + 50.0 * 8.660341262817383
Epoch 580, val loss: 0.8793681263923645
Epoch 590, training loss: 433.9609069824219 = 0.8647727966308594 + 50.0 * 8.661922454833984
Epoch 590, val loss: 0.8706420063972473
Epoch 600, training loss: 433.9738464355469 = 0.8555487990379333 + 50.0 * 8.662365913391113
Epoch 600, val loss: 0.8617024421691895
Epoch 610, training loss: 433.93304443359375 = 0.8463069200515747 + 50.0 * 8.661734580993652
Epoch 610, val loss: 0.8526870012283325
Epoch 620, training loss: 433.9127197265625 = 0.8369472622871399 + 50.0 * 8.661515235900879
Epoch 620, val loss: 0.8437513709068298
Epoch 630, training loss: 433.99053955078125 = 0.8276137709617615 + 50.0 * 8.66325855255127
Epoch 630, val loss: 0.8348003029823303
Epoch 640, training loss: 434.15325927734375 = 0.8183107972145081 + 50.0 * 8.666699409484863
Epoch 640, val loss: 0.8258423209190369
Epoch 650, training loss: 434.0018310546875 = 0.8088654279708862 + 50.0 * 8.663859367370605
Epoch 650, val loss: 0.81693035364151
Epoch 660, training loss: 434.1868591308594 = 0.799700915813446 + 50.0 * 8.667742729187012
Epoch 660, val loss: 0.8080651164054871
Epoch 670, training loss: 434.10699462890625 = 0.7903389930725098 + 50.0 * 8.666333198547363
Epoch 670, val loss: 0.7990416288375854
Epoch 680, training loss: 434.346435546875 = 0.781396210193634 + 50.0 * 8.671300888061523
Epoch 680, val loss: 0.7904970645904541
Epoch 690, training loss: 434.4621276855469 = 0.7722517251968384 + 50.0 * 8.673797607421875
Epoch 690, val loss: 0.7817721366882324
Epoch 700, training loss: 434.2605895996094 = 0.7628137469291687 + 50.0 * 8.669955253601074
Epoch 700, val loss: 0.7728700041770935
Epoch 710, training loss: 434.4830322265625 = 0.753956139087677 + 50.0 * 8.674581527709961
Epoch 710, val loss: 0.7645100355148315
Epoch 720, training loss: 434.6616516113281 = 0.7453644871711731 + 50.0 * 8.678325653076172
Epoch 720, val loss: 0.7563645839691162
Epoch 730, training loss: 434.8277587890625 = 0.7367244958877563 + 50.0 * 8.6818208694458
Epoch 730, val loss: 0.7482637763023376
Epoch 740, training loss: 434.9859619140625 = 0.7281753420829773 + 50.0 * 8.685155868530273
Epoch 740, val loss: 0.7402271032333374
Epoch 750, training loss: 435.0483093261719 = 0.7198465466499329 + 50.0 * 8.686569213867188
Epoch 750, val loss: 0.7323557734489441
Epoch 760, training loss: 435.0482482910156 = 0.7115418910980225 + 50.0 * 8.686734199523926
Epoch 760, val loss: 0.7247170805931091
Epoch 770, training loss: 435.23345947265625 = 0.7035840153694153 + 50.0 * 8.690597534179688
Epoch 770, val loss: 0.7172583341598511
Epoch 780, training loss: 435.1998596191406 = 0.6956806182861328 + 50.0 * 8.690083503723145
Epoch 780, val loss: 0.7098182439804077
Epoch 790, training loss: 435.3348388671875 = 0.6878510117530823 + 50.0 * 8.692939758300781
Epoch 790, val loss: 0.7026769518852234
Epoch 800, training loss: 435.4023742675781 = 0.6803285479545593 + 50.0 * 8.694440841674805
Epoch 800, val loss: 0.6957160830497742
Epoch 810, training loss: 435.47857666015625 = 0.6730942130088806 + 50.0 * 8.696109771728516
Epoch 810, val loss: 0.6889287829399109
Epoch 820, training loss: 435.55596923828125 = 0.6659083962440491 + 50.0 * 8.69780158996582
Epoch 820, val loss: 0.6823087334632874
Epoch 830, training loss: 435.5331115722656 = 0.6589034199714661 + 50.0 * 8.697484016418457
Epoch 830, val loss: 0.6759522557258606
Epoch 840, training loss: 435.7795715332031 = 0.6521393060684204 + 50.0 * 8.70254898071289
Epoch 840, val loss: 0.6696712970733643
Epoch 850, training loss: 435.7347412109375 = 0.645473301410675 + 50.0 * 8.70178508758545
Epoch 850, val loss: 0.663511335849762
Epoch 860, training loss: 435.5011291503906 = 0.6387635469436646 + 50.0 * 8.697247505187988
Epoch 860, val loss: 0.6570508480072021
Epoch 870, training loss: 435.82086181640625 = 0.6328534483909607 + 50.0 * 8.703760147094727
Epoch 870, val loss: 0.6522112488746643
Epoch 880, training loss: 435.59857177734375 = 0.6269007325172424 + 50.0 * 8.699433326721191
Epoch 880, val loss: 0.6467011570930481
Epoch 890, training loss: 435.557373046875 = 0.6209100484848022 + 50.0 * 8.698729515075684
Epoch 890, val loss: 0.6413763761520386
Epoch 900, training loss: 435.78179931640625 = 0.6151196360588074 + 50.0 * 8.703333854675293
Epoch 900, val loss: 0.6360419392585754
Epoch 910, training loss: 435.95257568359375 = 0.6095696687698364 + 50.0 * 8.706860542297363
Epoch 910, val loss: 0.631054162979126
Epoch 920, training loss: 436.0099182128906 = 0.6040050387382507 + 50.0 * 8.708118438720703
Epoch 920, val loss: 0.6260805130004883
Epoch 930, training loss: 436.234619140625 = 0.5987198948860168 + 50.0 * 8.71271800994873
Epoch 930, val loss: 0.6213565468788147
Epoch 940, training loss: 436.3157958984375 = 0.5935454964637756 + 50.0 * 8.714445114135742
Epoch 940, val loss: 0.6168001294136047
Epoch 950, training loss: 436.462646484375 = 0.5884650945663452 + 50.0 * 8.717483520507812
Epoch 950, val loss: 0.6122894883155823
Epoch 960, training loss: 436.53375244140625 = 0.5834420919418335 + 50.0 * 8.719006538391113
Epoch 960, val loss: 0.6079303622245789
Epoch 970, training loss: 436.5335693359375 = 0.5785950422286987 + 50.0 * 8.719099998474121
Epoch 970, val loss: 0.6036123037338257
Epoch 980, training loss: 436.676513671875 = 0.5738508105278015 + 50.0 * 8.722053527832031
Epoch 980, val loss: 0.5994467735290527
Epoch 990, training loss: 436.63128662109375 = 0.568821132183075 + 50.0 * 8.7212495803833
Epoch 990, val loss: 0.5950378179550171
Epoch 1000, training loss: 436.44921875 = 0.5645444393157959 + 50.0 * 8.717693328857422
Epoch 1000, val loss: 0.5914254784584045
Epoch 1010, training loss: 435.9934997558594 = 0.5599069595336914 + 50.0 * 8.708671569824219
Epoch 1010, val loss: 0.5872156023979187
Epoch 1020, training loss: 436.302978515625 = 0.5558788776397705 + 50.0 * 8.71494197845459
Epoch 1020, val loss: 0.583926260471344
Epoch 1030, training loss: 436.4300537109375 = 0.5517024993896484 + 50.0 * 8.717567443847656
Epoch 1030, val loss: 0.5803784728050232
Epoch 1040, training loss: 436.6017150878906 = 0.5477748513221741 + 50.0 * 8.721078872680664
Epoch 1040, val loss: 0.5770860314369202
Epoch 1050, training loss: 436.8671875 = 0.543899416923523 + 50.0 * 8.726466178894043
Epoch 1050, val loss: 0.5738167762756348
Epoch 1060, training loss: 437.0174255371094 = 0.5400028824806213 + 50.0 * 8.729548454284668
Epoch 1060, val loss: 0.570502758026123
Epoch 1070, training loss: 437.18096923828125 = 0.536161482334137 + 50.0 * 8.732895851135254
Epoch 1070, val loss: 0.5673396587371826
Epoch 1080, training loss: 437.36260986328125 = 0.5324597954750061 + 50.0 * 8.736602783203125
Epoch 1080, val loss: 0.5642214417457581
Epoch 1090, training loss: 437.33160400390625 = 0.5287330746650696 + 50.0 * 8.73605728149414
Epoch 1090, val loss: 0.5612112283706665
Epoch 1100, training loss: 437.3942565917969 = 0.5251443386077881 + 50.0 * 8.737381935119629
Epoch 1100, val loss: 0.5583521723747253
Epoch 1110, training loss: 437.6310119628906 = 0.5217295289039612 + 50.0 * 8.742185592651367
Epoch 1110, val loss: 0.555577278137207
Epoch 1120, training loss: 437.3869934082031 = 0.5183427333831787 + 50.0 * 8.737373352050781
Epoch 1120, val loss: 0.5528585910797119
Epoch 1130, training loss: 437.5015869140625 = 0.5150378942489624 + 50.0 * 8.739730834960938
Epoch 1130, val loss: 0.550267219543457
Epoch 1140, training loss: 437.720458984375 = 0.5118930339813232 + 50.0 * 8.744171142578125
Epoch 1140, val loss: 0.5477754473686218
Epoch 1150, training loss: 438.03662109375 = 0.508786141872406 + 50.0 * 8.750556945800781
Epoch 1150, val loss: 0.5453339219093323
Epoch 1160, training loss: 438.0021057128906 = 0.5056664943695068 + 50.0 * 8.74992847442627
Epoch 1160, val loss: 0.5429081320762634
Epoch 1170, training loss: 437.5778503417969 = 0.5024740695953369 + 50.0 * 8.741507530212402
Epoch 1170, val loss: 0.5403944253921509
Epoch 1180, training loss: 438.0171203613281 = 0.4999019503593445 + 50.0 * 8.750344276428223
Epoch 1180, val loss: 0.5385092496871948
Epoch 1190, training loss: 437.87841796875 = 0.4970719516277313 + 50.0 * 8.747627258300781
Epoch 1190, val loss: 0.5363237261772156
Epoch 1200, training loss: 437.8564453125 = 0.4941338896751404 + 50.0 * 8.747245788574219
Epoch 1200, val loss: 0.5340723395347595
Epoch 1210, training loss: 438.0185546875 = 0.49160218238830566 + 50.0 * 8.75053882598877
Epoch 1210, val loss: 0.5321922898292542
Epoch 1220, training loss: 438.2825622558594 = 0.4890267848968506 + 50.0 * 8.755870819091797
Epoch 1220, val loss: 0.530308187007904
Epoch 1230, training loss: 438.4998474121094 = 0.48646992444992065 + 50.0 * 8.76026725769043
Epoch 1230, val loss: 0.5283598303794861
Epoch 1240, training loss: 438.6339111328125 = 0.48397156596183777 + 50.0 * 8.762998580932617
Epoch 1240, val loss: 0.5265438556671143
Epoch 1250, training loss: 438.6297607421875 = 0.48150303959846497 + 50.0 * 8.762965202331543
Epoch 1250, val loss: 0.5248143076896667
Epoch 1260, training loss: 438.8968200683594 = 0.4791596531867981 + 50.0 * 8.768353462219238
Epoch 1260, val loss: 0.523094117641449
Epoch 1270, training loss: 439.00323486328125 = 0.47680360078811646 + 50.0 * 8.770528793334961
Epoch 1270, val loss: 0.5214270949363708
Epoch 1280, training loss: 438.9559020996094 = 0.4744739234447479 + 50.0 * 8.769628524780273
Epoch 1280, val loss: 0.5197939276695251
Epoch 1290, training loss: 439.1911926269531 = 0.4723018407821655 + 50.0 * 8.774377822875977
Epoch 1290, val loss: 0.5182639360427856
Epoch 1300, training loss: 439.05755615234375 = 0.4700312614440918 + 50.0 * 8.771750450134277
Epoch 1300, val loss: 0.5166498422622681
Epoch 1310, training loss: 439.35101318359375 = 0.46797287464141846 + 50.0 * 8.777660369873047
Epoch 1310, val loss: 0.5152324438095093
Epoch 1320, training loss: 439.3675842285156 = 0.4658760726451874 + 50.0 * 8.778034210205078
Epoch 1320, val loss: 0.51381915807724
Epoch 1330, training loss: 439.505859375 = 0.46382033824920654 + 50.0 * 8.780840873718262
Epoch 1330, val loss: 0.5124262571334839
Epoch 1340, training loss: 439.4698791503906 = 0.46177929639816284 + 50.0 * 8.78016185760498
Epoch 1340, val loss: 0.5110189318656921
Epoch 1350, training loss: 439.50299072265625 = 0.45979321002960205 + 50.0 * 8.780863761901855
Epoch 1350, val loss: 0.5096060037612915
Epoch 1360, training loss: 439.3175048828125 = 0.4578460156917572 + 50.0 * 8.777193069458008
Epoch 1360, val loss: 0.5082947015762329
Epoch 1370, training loss: 439.4044189453125 = 0.4558728039264679 + 50.0 * 8.778970718383789
Epoch 1370, val loss: 0.5069581866264343
Epoch 1380, training loss: 439.29669189453125 = 0.4540241062641144 + 50.0 * 8.776853561401367
Epoch 1380, val loss: 0.5058987140655518
Epoch 1390, training loss: 439.5206604003906 = 0.45229047536849976 + 50.0 * 8.781367301940918
Epoch 1390, val loss: 0.5047339200973511
Epoch 1400, training loss: 439.6906433105469 = 0.4505600035190582 + 50.0 * 8.784801483154297
Epoch 1400, val loss: 0.5034891366958618
Epoch 1410, training loss: 439.8879699707031 = 0.44884946942329407 + 50.0 * 8.788782119750977
Epoch 1410, val loss: 0.5024334192276001
Epoch 1420, training loss: 439.8721008300781 = 0.44711264967918396 + 50.0 * 8.78849983215332
Epoch 1420, val loss: 0.5013980865478516
Epoch 1430, training loss: 440.0169982910156 = 0.44543224573135376 + 50.0 * 8.791431427001953
Epoch 1430, val loss: 0.5003706216812134
Epoch 1440, training loss: 439.7982177734375 = 0.4437055289745331 + 50.0 * 8.787090301513672
Epoch 1440, val loss: 0.4993216097354889
Epoch 1450, training loss: 440.0901794433594 = 0.4420786499977112 + 50.0 * 8.792962074279785
Epoch 1450, val loss: 0.4983796179294586
Epoch 1460, training loss: 440.139404296875 = 0.44037723541259766 + 50.0 * 8.793980598449707
Epoch 1460, val loss: 0.4972463548183441
Epoch 1470, training loss: 439.6750793457031 = 0.438730925321579 + 50.0 * 8.784727096557617
Epoch 1470, val loss: 0.49632805585861206
Epoch 1480, training loss: 439.5533447265625 = 0.4372498393058777 + 50.0 * 8.78232192993164
Epoch 1480, val loss: 0.4955810606479645
Epoch 1490, training loss: 439.7258605957031 = 0.4357598125934601 + 50.0 * 8.785801887512207
Epoch 1490, val loss: 0.4943900406360626
Epoch 1500, training loss: 439.9909362792969 = 0.43423885107040405 + 50.0 * 8.791133880615234
Epoch 1500, val loss: 0.493661493062973
Epoch 1510, training loss: 440.29595947265625 = 0.4327496886253357 + 50.0 * 8.797264099121094
Epoch 1510, val loss: 0.4927581250667572
Epoch 1520, training loss: 440.45562744140625 = 0.4312523901462555 + 50.0 * 8.800487518310547
Epoch 1520, val loss: 0.4918287396430969
Epoch 1530, training loss: 440.4192199707031 = 0.42975708842277527 + 50.0 * 8.799789428710938
Epoch 1530, val loss: 0.49101749062538147
Epoch 1540, training loss: 440.57342529296875 = 0.4283130168914795 + 50.0 * 8.802902221679688
Epoch 1540, val loss: 0.4901731312274933
Epoch 1550, training loss: 440.7603454589844 = 0.42687544226646423 + 50.0 * 8.806669235229492
Epoch 1550, val loss: 0.4893275201320648
Epoch 1560, training loss: 440.6694641113281 = 0.4254346489906311 + 50.0 * 8.804880142211914
Epoch 1560, val loss: 0.48852428793907166
Epoch 1570, training loss: 440.8531494140625 = 0.42404159903526306 + 50.0 * 8.808582305908203
Epoch 1570, val loss: 0.48774662613868713
Epoch 1580, training loss: 440.9468078613281 = 0.4226568341255188 + 50.0 * 8.8104829788208
Epoch 1580, val loss: 0.4869445860385895
Epoch 1590, training loss: 440.8931884765625 = 0.4212893545627594 + 50.0 * 8.80943775177002
Epoch 1590, val loss: 0.48614054918289185
Epoch 1600, training loss: 440.97259521484375 = 0.4199232757091522 + 50.0 * 8.811053276062012
Epoch 1600, val loss: 0.48546263575553894
Epoch 1610, training loss: 441.11090087890625 = 0.4185972213745117 + 50.0 * 8.81384563446045
Epoch 1610, val loss: 0.48473280668258667
Epoch 1620, training loss: 441.25665283203125 = 0.4173208475112915 + 50.0 * 8.816786766052246
Epoch 1620, val loss: 0.48406222462654114
Epoch 1630, training loss: 441.3956298828125 = 0.4160385727882385 + 50.0 * 8.819591522216797
Epoch 1630, val loss: 0.4833310544490814
Epoch 1640, training loss: 441.22845458984375 = 0.41475898027420044 + 50.0 * 8.81627368927002
Epoch 1640, val loss: 0.48274385929107666
Epoch 1650, training loss: 441.39776611328125 = 0.4135016202926636 + 50.0 * 8.819684982299805
Epoch 1650, val loss: 0.4820178747177124
Epoch 1660, training loss: 437.6959228515625 = 0.4116891324520111 + 50.0 * 8.745684623718262
Epoch 1660, val loss: 0.4810691773891449
Epoch 1670, training loss: 439.6321716308594 = 0.4108264148235321 + 50.0 * 8.78442668914795
Epoch 1670, val loss: 0.48011377453804016
Epoch 1680, training loss: 439.5732421875 = 0.4098389744758606 + 50.0 * 8.783267974853516
Epoch 1680, val loss: 0.4798126220703125
Epoch 1690, training loss: 439.408447265625 = 0.4087003767490387 + 50.0 * 8.77999496459961
Epoch 1690, val loss: 0.47954341769218445
Epoch 1700, training loss: 439.8897399902344 = 0.4076770842075348 + 50.0 * 8.789641380310059
Epoch 1700, val loss: 0.47902458906173706
Epoch 1710, training loss: 440.4376525878906 = 0.4065681993961334 + 50.0 * 8.80062198638916
Epoch 1710, val loss: 0.4784572124481201
Epoch 1720, training loss: 440.8330383300781 = 0.40548327565193176 + 50.0 * 8.808550834655762
Epoch 1720, val loss: 0.4779842495918274
Epoch 1730, training loss: 440.963623046875 = 0.4043358862400055 + 50.0 * 8.811185836791992
Epoch 1730, val loss: 0.4774335026741028
Epoch 1740, training loss: 441.2068176269531 = 0.4031996428966522 + 50.0 * 8.816072463989258
Epoch 1740, val loss: 0.4768529534339905
Epoch 1750, training loss: 441.2207946777344 = 0.40201008319854736 + 50.0 * 8.816375732421875
Epoch 1750, val loss: 0.4762709140777588
Epoch 1760, training loss: 441.3989562988281 = 0.4008828103542328 + 50.0 * 8.819961547851562
Epoch 1760, val loss: 0.47571995854377747
Epoch 1770, training loss: 441.3446350097656 = 0.3997301161289215 + 50.0 * 8.81889820098877
Epoch 1770, val loss: 0.47519421577453613
Epoch 1780, training loss: 441.48919677734375 = 0.3986102044582367 + 50.0 * 8.82181167602539
Epoch 1780, val loss: 0.4746689796447754
Epoch 1790, training loss: 441.6343688964844 = 0.39748385548591614 + 50.0 * 8.824737548828125
Epoch 1790, val loss: 0.47408199310302734
Epoch 1800, training loss: 441.455078125 = 0.3963528275489807 + 50.0 * 8.821174621582031
Epoch 1800, val loss: 0.473562091588974
Epoch 1810, training loss: 441.5086975097656 = 0.3952649235725403 + 50.0 * 8.82226848602295
Epoch 1810, val loss: 0.47308802604675293
Epoch 1820, training loss: 441.603271484375 = 0.39421260356903076 + 50.0 * 8.824180603027344
Epoch 1820, val loss: 0.4725673496723175
Epoch 1830, training loss: 441.7763671875 = 0.39315783977508545 + 50.0 * 8.827664375305176
Epoch 1830, val loss: 0.472145140171051
Epoch 1840, training loss: 441.8830871582031 = 0.3920646905899048 + 50.0 * 8.82982063293457
Epoch 1840, val loss: 0.4716308116912842
Epoch 1850, training loss: 442.01611328125 = 0.3909797668457031 + 50.0 * 8.832502365112305
Epoch 1850, val loss: 0.4711281359195709
Epoch 1860, training loss: 442.09576416015625 = 0.38992738723754883 + 50.0 * 8.83411693572998
Epoch 1860, val loss: 0.4705716371536255
Epoch 1870, training loss: 442.18798828125 = 0.3888722360134125 + 50.0 * 8.835982322692871
Epoch 1870, val loss: 0.4701630473136902
Epoch 1880, training loss: 442.1341552734375 = 0.38780516386032104 + 50.0 * 8.83492660522461
Epoch 1880, val loss: 0.4696848392486572
Epoch 1890, training loss: 442.1720275878906 = 0.3867720365524292 + 50.0 * 8.835704803466797
Epoch 1890, val loss: 0.46921873092651367
Epoch 1900, training loss: 442.29974365234375 = 0.3857068717479706 + 50.0 * 8.83828067779541
Epoch 1900, val loss: 0.46874216198921204
Epoch 1910, training loss: 442.49365234375 = 0.38468119502067566 + 50.0 * 8.842179298400879
Epoch 1910, val loss: 0.4682824909687042
Epoch 1920, training loss: 442.3852233886719 = 0.38361018896102905 + 50.0 * 8.840032577514648
Epoch 1920, val loss: 0.46787866950035095
Epoch 1930, training loss: 442.4610290527344 = 0.3825817108154297 + 50.0 * 8.841568946838379
Epoch 1930, val loss: 0.46744102239608765
Epoch 1940, training loss: 442.5367126464844 = 0.3815458118915558 + 50.0 * 8.843103408813477
Epoch 1940, val loss: 0.46692803502082825
Epoch 1950, training loss: 442.6417541503906 = 0.38050577044487 + 50.0 * 8.84522533416748
Epoch 1950, val loss: 0.46653059124946594
Epoch 1960, training loss: 442.7503662109375 = 0.37948909401893616 + 50.0 * 8.847417831420898
Epoch 1960, val loss: 0.46607059240341187
Epoch 1970, training loss: 442.8439025878906 = 0.37844714522361755 + 50.0 * 8.849308967590332
Epoch 1970, val loss: 0.465654581785202
Epoch 1980, training loss: 442.75830078125 = 0.3774408996105194 + 50.0 * 8.847617149353027
Epoch 1980, val loss: 0.46518197655677795
Epoch 1990, training loss: 442.8651428222656 = 0.3764430582523346 + 50.0 * 8.849774360656738
Epoch 1990, val loss: 0.46480628848075867
Epoch 2000, training loss: 442.9593200683594 = 0.37541574239730835 + 50.0 * 8.851677894592285
Epoch 2000, val loss: 0.4643232822418213
Epoch 2010, training loss: 442.93902587890625 = 0.37438347935676575 + 50.0 * 8.851292610168457
Epoch 2010, val loss: 0.4638376235961914
Epoch 2020, training loss: 442.7403564453125 = 0.3733327388763428 + 50.0 * 8.84734058380127
Epoch 2020, val loss: 0.46346205472946167
Epoch 2030, training loss: 442.6364440917969 = 0.37230172753334045 + 50.0 * 8.845282554626465
Epoch 2030, val loss: 0.4629250168800354
Epoch 2040, training loss: 442.8611755371094 = 0.37136200070381165 + 50.0 * 8.849796295166016
Epoch 2040, val loss: 0.4623831808567047
Epoch 2050, training loss: 441.556640625 = 0.37029582262039185 + 50.0 * 8.823726654052734
Epoch 2050, val loss: 0.46210265159606934
Epoch 2060, training loss: 442.13873291015625 = 0.36950960755348206 + 50.0 * 8.835384368896484
Epoch 2060, val loss: 0.4617765545845032
Epoch 2070, training loss: 441.9667053222656 = 0.3683432340621948 + 50.0 * 8.8319673538208
Epoch 2070, val loss: 0.461078941822052
Epoch 2080, training loss: 442.1180419921875 = 0.36742860078811646 + 50.0 * 8.835012435913086
Epoch 2080, val loss: 0.46085476875305176
Epoch 2090, training loss: 442.5741271972656 = 0.36651599407196045 + 50.0 * 8.844152450561523
Epoch 2090, val loss: 0.4604593813419342
Epoch 2100, training loss: 442.85028076171875 = 0.3655058741569519 + 50.0 * 8.849695205688477
Epoch 2100, val loss: 0.4599759578704834
Epoch 2110, training loss: 443.18975830078125 = 0.3644964396953583 + 50.0 * 8.856505393981934
Epoch 2110, val loss: 0.4596058130264282
Epoch 2120, training loss: 443.3943176269531 = 0.36348956823349 + 50.0 * 8.860616683959961
Epoch 2120, val loss: 0.45914551615715027
Epoch 2130, training loss: 443.41400146484375 = 0.3624742925167084 + 50.0 * 8.861030578613281
Epoch 2130, val loss: 0.45880523324012756
Epoch 2140, training loss: 443.4444274902344 = 0.36146852374076843 + 50.0 * 8.861659049987793
Epoch 2140, val loss: 0.45839086174964905
Epoch 2150, training loss: 443.6332092285156 = 0.3604739010334015 + 50.0 * 8.86545467376709
Epoch 2150, val loss: 0.4580427408218384
Epoch 2160, training loss: 443.58221435546875 = 0.3594709038734436 + 50.0 * 8.864455223083496
Epoch 2160, val loss: 0.45764973759651184
Epoch 2170, training loss: 443.6574401855469 = 0.3584865629673004 + 50.0 * 8.865979194641113
Epoch 2170, val loss: 0.4573132395744324
Epoch 2180, training loss: 443.68853759765625 = 0.3574763834476471 + 50.0 * 8.866621017456055
Epoch 2180, val loss: 0.456877201795578
Epoch 2190, training loss: 443.7247619628906 = 0.3565010130405426 + 50.0 * 8.867364883422852
Epoch 2190, val loss: 0.45652851462364197
Epoch 2200, training loss: 443.9295959472656 = 0.3555140793323517 + 50.0 * 8.871481895446777
Epoch 2200, val loss: 0.4560469686985016
Epoch 2210, training loss: 444.0137939453125 = 0.35452035069465637 + 50.0 * 8.873185157775879
Epoch 2210, val loss: 0.45570677518844604
Epoch 2220, training loss: 444.00616455078125 = 0.3535326421260834 + 50.0 * 8.873052597045898
Epoch 2220, val loss: 0.4553171694278717
Epoch 2230, training loss: 444.0989685058594 = 0.35253041982650757 + 50.0 * 8.87492847442627
Epoch 2230, val loss: 0.45500028133392334
Epoch 2240, training loss: 444.1459655761719 = 0.35154059529304504 + 50.0 * 8.87588882446289
Epoch 2240, val loss: 0.4545207917690277
Epoch 2250, training loss: 444.08282470703125 = 0.35054805874824524 + 50.0 * 8.874645233154297
Epoch 2250, val loss: 0.45420894026756287
Epoch 2260, training loss: 444.0677185058594 = 0.3495774269104004 + 50.0 * 8.87436294555664
Epoch 2260, val loss: 0.45384326577186584
Epoch 2270, training loss: 443.4455871582031 = 0.34848061203956604 + 50.0 * 8.861942291259766
Epoch 2270, val loss: 0.4532786011695862
Epoch 2280, training loss: 443.345458984375 = 0.34757599234580994 + 50.0 * 8.859957695007324
Epoch 2280, val loss: 0.45305076241493225
Epoch 2290, training loss: 443.54022216796875 = 0.34668266773223877 + 50.0 * 8.863870620727539
Epoch 2290, val loss: 0.45264801383018494
Epoch 2300, training loss: 443.78863525390625 = 0.3457103371620178 + 50.0 * 8.868858337402344
Epoch 2300, val loss: 0.45226892828941345
Epoch 2310, training loss: 444.1168518066406 = 0.34472009539604187 + 50.0 * 8.875442504882812
Epoch 2310, val loss: 0.45188793540000916
Epoch 2320, training loss: 444.1560974121094 = 0.34372058510780334 + 50.0 * 8.87624740600586
Epoch 2320, val loss: 0.45139849185943604
Epoch 2330, training loss: 444.27960205078125 = 0.34273993968963623 + 50.0 * 8.878737449645996
Epoch 2330, val loss: 0.45111283659935
Epoch 2340, training loss: 444.4207763671875 = 0.34175747632980347 + 50.0 * 8.881580352783203
Epoch 2340, val loss: 0.4507671892642975
Epoch 2350, training loss: 444.51800537109375 = 0.3407554626464844 + 50.0 * 8.883544921875
Epoch 2350, val loss: 0.45039063692092896
Epoch 2360, training loss: 444.44866943359375 = 0.3397645056247711 + 50.0 * 8.88217830657959
Epoch 2360, val loss: 0.45012399554252625
Epoch 2370, training loss: 444.4890441894531 = 0.3387744426727295 + 50.0 * 8.883005142211914
Epoch 2370, val loss: 0.449653685092926
Epoch 2380, training loss: 444.5833435058594 = 0.33780667185783386 + 50.0 * 8.884910583496094
Epoch 2380, val loss: 0.4494367837905884
Epoch 2390, training loss: 444.808349609375 = 0.33680459856987 + 50.0 * 8.88943099975586
Epoch 2390, val loss: 0.44901320338249207
Epoch 2400, training loss: 444.8260192871094 = 0.33579760789871216 + 50.0 * 8.88980484008789
Epoch 2400, val loss: 0.4487155079841614
Epoch 2410, training loss: 444.9043884277344 = 0.3348234295845032 + 50.0 * 8.89139175415039
Epoch 2410, val loss: 0.4482918679714203
Epoch 2420, training loss: 444.8114013671875 = 0.33381810784339905 + 50.0 * 8.889551162719727
Epoch 2420, val loss: 0.4479014575481415
Epoch 2430, training loss: 444.952392578125 = 0.3328202962875366 + 50.0 * 8.892391204833984
Epoch 2430, val loss: 0.4475860893726349
Epoch 2440, training loss: 444.96978759765625 = 0.3318295478820801 + 50.0 * 8.892759323120117
Epoch 2440, val loss: 0.4472215473651886
Epoch 2450, training loss: 445.09222412109375 = 0.3308342397212982 + 50.0 * 8.895227432250977
Epoch 2450, val loss: 0.4469192624092102
Epoch 2460, training loss: 445.11859130859375 = 0.32983389496803284 + 50.0 * 8.895774841308594
Epoch 2460, val loss: 0.4464760422706604
Epoch 2470, training loss: 445.12847900390625 = 0.3288429081439972 + 50.0 * 8.895992279052734
Epoch 2470, val loss: 0.4462099075317383
Epoch 2480, training loss: 445.2398986816406 = 0.3278781473636627 + 50.0 * 8.898240089416504
Epoch 2480, val loss: 0.44589731097221375
Epoch 2490, training loss: 445.28765869140625 = 0.3268817961215973 + 50.0 * 8.899215698242188
Epoch 2490, val loss: 0.4454638957977295
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8314492753623188
0.8640875172064045
The final CL Acc:0.83729, 0.00796, The final GNN Acc:0.86440, 0.00106
Begin epxeriment: noisy_level: 0.2 cont_weight: 50 epoch: der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106268])
remove edge: torch.Size([2, 70960])
updated graph: torch.Size([2, 88580])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:227: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 519.7924194335938 = 1.103524088859558 + 50.0 * 10.373778343200684
Epoch 0, val loss: 1.1023906469345093
Epoch 10, training loss: 500.9362487792969 = 1.1033798456192017 + 50.0 * 9.996657371520996
Epoch 10, val loss: 1.1022354364395142
Epoch 20, training loss: 491.16552734375 = 1.1030843257904053 + 50.0 * 9.801248550415039
Epoch 20, val loss: 1.101905345916748
Epoch 30, training loss: 483.7683410644531 = 1.1026731729507446 + 50.0 * 9.653313636779785
Epoch 30, val loss: 1.1014703512191772
Epoch 40, training loss: 477.7713623046875 = 1.1022306680679321 + 50.0 * 9.533382415771484
Epoch 40, val loss: 1.1010068655014038
Epoch 50, training loss: 472.8977355957031 = 1.101757287979126 + 50.0 * 9.435919761657715
Epoch 50, val loss: 1.1005171537399292
Epoch 60, training loss: 468.8697509765625 = 1.1012637615203857 + 50.0 * 9.355369567871094
Epoch 60, val loss: 1.10000479221344
Epoch 70, training loss: 465.46923828125 = 1.1007459163665771 + 50.0 * 9.287369728088379
Epoch 70, val loss: 1.0994691848754883
Epoch 80, training loss: 462.5146179199219 = 1.1001954078674316 + 50.0 * 9.228288650512695
Epoch 80, val loss: 1.0989044904708862
Epoch 90, training loss: 459.96685791015625 = 1.09961998462677 + 50.0 * 9.177345275878906
Epoch 90, val loss: 1.0983127355575562
Epoch 100, training loss: 457.72674560546875 = 1.0990161895751953 + 50.0 * 9.132554054260254
Epoch 100, val loss: 1.0976914167404175
Epoch 110, training loss: 455.78851318359375 = 1.0983877182006836 + 50.0 * 9.093802452087402
Epoch 110, val loss: 1.0970485210418701
Epoch 120, training loss: 454.12628173828125 = 1.0977551937103271 + 50.0 * 9.06057071685791
Epoch 120, val loss: 1.0963913202285767
Epoch 130, training loss: 452.5792236328125 = 1.097069263458252 + 50.0 * 9.029643058776855
Epoch 130, val loss: 1.0956889390945435
Epoch 140, training loss: 451.38446044921875 = 1.09639310836792 + 50.0 * 9.00576114654541
Epoch 140, val loss: 1.09501051902771
Epoch 150, training loss: 450.2164001464844 = 1.0956698656082153 + 50.0 * 8.982414245605469
Epoch 150, val loss: 1.0942704677581787
Epoch 160, training loss: 449.3021545410156 = 1.094986915588379 + 50.0 * 8.964142799377441
Epoch 160, val loss: 1.0935533046722412
Epoch 170, training loss: 448.4410705566406 = 1.0942533016204834 + 50.0 * 8.94693660736084
Epoch 170, val loss: 1.0928102731704712
Epoch 180, training loss: 447.7504577636719 = 1.093501329421997 + 50.0 * 8.933138847351074
Epoch 180, val loss: 1.0920528173446655
Epoch 190, training loss: 447.02288818359375 = 1.092748761177063 + 50.0 * 8.91860294342041
Epoch 190, val loss: 1.0912812948226929
Epoch 200, training loss: 446.5018310546875 = 1.0920054912567139 + 50.0 * 8.908196449279785
Epoch 200, val loss: 1.0905253887176514
Epoch 210, training loss: 446.02508544921875 = 1.091261863708496 + 50.0 * 8.898676872253418
Epoch 210, val loss: 1.0897630453109741
Epoch 220, training loss: 445.8711853027344 = 1.0905554294586182 + 50.0 * 8.895612716674805
Epoch 220, val loss: 1.0890483856201172
Epoch 230, training loss: 445.4141845703125 = 1.0898640155792236 + 50.0 * 8.886486053466797
Epoch 230, val loss: 1.0883554220199585
Epoch 240, training loss: 445.0804748535156 = 1.0892196893692017 + 50.0 * 8.8798246383667
Epoch 240, val loss: 1.0877037048339844
Epoch 250, training loss: 444.8243408203125 = 1.08860445022583 + 50.0 * 8.874714851379395
Epoch 250, val loss: 1.087066888809204
Epoch 260, training loss: 444.7353210449219 = 1.0880683660507202 + 50.0 * 8.872944831848145
Epoch 260, val loss: 1.0865117311477661
Epoch 270, training loss: 444.7078552246094 = 1.0875543355941772 + 50.0 * 8.872406005859375
Epoch 270, val loss: 1.0859966278076172
Epoch 280, training loss: 444.45477294921875 = 1.0870611667633057 + 50.0 * 8.867354393005371
Epoch 280, val loss: 1.0854928493499756
Epoch 290, training loss: 444.2794189453125 = 1.086594581604004 + 50.0 * 8.863856315612793
Epoch 290, val loss: 1.085017442703247
Epoch 300, training loss: 444.0978088378906 = 1.0861542224884033 + 50.0 * 8.860233306884766
Epoch 300, val loss: 1.0845692157745361
Epoch 310, training loss: 444.015625 = 1.085707426071167 + 50.0 * 8.858598709106445
Epoch 310, val loss: 1.0841147899627686
Epoch 320, training loss: 443.85003662109375 = 1.0852642059326172 + 50.0 * 8.855295181274414
Epoch 320, val loss: 1.0836710929870605
Epoch 330, training loss: 443.7314453125 = 1.0848122835159302 + 50.0 * 8.852932929992676
Epoch 330, val loss: 1.0832139253616333
Epoch 340, training loss: 443.74267578125 = 1.084370493888855 + 50.0 * 8.853165626525879
Epoch 340, val loss: 1.0827665328979492
Epoch 350, training loss: 443.97137451171875 = 1.083919882774353 + 50.0 * 8.857748985290527
Epoch 350, val loss: 1.082308292388916
Epoch 360, training loss: 443.6865539550781 = 1.0834380388259888 + 50.0 * 8.852062225341797
Epoch 360, val loss: 1.081833839416504
Epoch 370, training loss: 443.7533264160156 = 1.0829640626907349 + 50.0 * 8.85340690612793
Epoch 370, val loss: 1.081353783607483
Epoch 380, training loss: 444.0323486328125 = 1.0825022459030151 + 50.0 * 8.858997344970703
Epoch 380, val loss: 1.0809003114700317
Epoch 390, training loss: 443.7115783691406 = 1.0820062160491943 + 50.0 * 8.852591514587402
Epoch 390, val loss: 1.0803968906402588
Epoch 400, training loss: 443.7779235839844 = 1.0814927816390991 + 50.0 * 8.853928565979004
Epoch 400, val loss: 1.0798746347427368
Epoch 410, training loss: 443.7582092285156 = 1.0810004472732544 + 50.0 * 8.853544235229492
Epoch 410, val loss: 1.0793824195861816
Epoch 420, training loss: 444.1388244628906 = 1.0804749727249146 + 50.0 * 8.861166954040527
Epoch 420, val loss: 1.0788609981536865
Epoch 430, training loss: 444.4317626953125 = 1.0799349546432495 + 50.0 * 8.867036819458008
Epoch 430, val loss: 1.0783191919326782
Epoch 440, training loss: 443.8525085449219 = 1.079419732093811 + 50.0 * 8.855462074279785
Epoch 440, val loss: 1.07780122756958
Epoch 450, training loss: 444.14996337890625 = 1.0788779258728027 + 50.0 * 8.861421585083008
Epoch 450, val loss: 1.0772606134414673
Epoch 460, training loss: 444.39556884765625 = 1.0783350467681885 + 50.0 * 8.866344451904297
Epoch 460, val loss: 1.0767199993133545
Epoch 470, training loss: 444.64483642578125 = 1.0777581930160522 + 50.0 * 8.871341705322266
Epoch 470, val loss: 1.076155662536621
Epoch 480, training loss: 444.7889709472656 = 1.0771832466125488 + 50.0 * 8.874236106872559
Epoch 480, val loss: 1.0755748748779297
Epoch 490, training loss: 444.83843994140625 = 1.0765796899795532 + 50.0 * 8.875237464904785
Epoch 490, val loss: 1.0749857425689697
Epoch 500, training loss: 445.0031433105469 = 1.07598876953125 + 50.0 * 8.87854290008545
Epoch 500, val loss: 1.0743952989578247
Epoch 510, training loss: 444.8861389160156 = 1.0753662586212158 + 50.0 * 8.876214981079102
Epoch 510, val loss: 1.0737687349319458
Epoch 520, training loss: 445.158203125 = 1.074741005897522 + 50.0 * 8.881669044494629
Epoch 520, val loss: 1.0731570720672607
Epoch 530, training loss: 445.1162414550781 = 1.0741112232208252 + 50.0 * 8.880843162536621
Epoch 530, val loss: 1.0725411176681519
Epoch 540, training loss: 445.22552490234375 = 1.0734565258026123 + 50.0 * 8.883041381835938
Epoch 540, val loss: 1.0718783140182495
Epoch 550, training loss: 445.206787109375 = 1.0727901458740234 + 50.0 * 8.88267993927002
Epoch 550, val loss: 1.0712331533432007
Epoch 560, training loss: 445.29522705078125 = 1.0721256732940674 + 50.0 * 8.884462356567383
Epoch 560, val loss: 1.0705753564834595
Epoch 570, training loss: 445.5404052734375 = 1.0714589357376099 + 50.0 * 8.889378547668457
Epoch 570, val loss: 1.0699188709259033
Epoch 580, training loss: 445.6116638183594 = 1.07078218460083 + 50.0 * 8.890817642211914
Epoch 580, val loss: 1.0692483186721802
Epoch 590, training loss: 445.7518005371094 = 1.0700887441635132 + 50.0 * 8.893633842468262
Epoch 590, val loss: 1.068568229675293
Epoch 600, training loss: 446.1521911621094 = 1.0693968534469604 + 50.0 * 8.901656150817871
Epoch 600, val loss: 1.0678871870040894
Epoch 610, training loss: 446.2322692871094 = 1.0686821937561035 + 50.0 * 8.903271675109863
Epoch 610, val loss: 1.0671966075897217
Epoch 620, training loss: 446.2910461425781 = 1.0679563283920288 + 50.0 * 8.904461860656738
Epoch 620, val loss: 1.0664565563201904
Epoch 630, training loss: 446.3255920410156 = 1.0672082901000977 + 50.0 * 8.905167579650879
Epoch 630, val loss: 1.065734624862671
Epoch 640, training loss: 446.0691833496094 = 1.0664278268814087 + 50.0 * 8.900054931640625
Epoch 640, val loss: 1.0649852752685547
Epoch 650, training loss: 446.21209716796875 = 1.0656694173812866 + 50.0 * 8.902928352355957
Epoch 650, val loss: 1.0642117261886597
Epoch 660, training loss: 446.68072509765625 = 1.0649296045303345 + 50.0 * 8.912315368652344
Epoch 660, val loss: 1.0635088682174683
Epoch 670, training loss: 446.8497619628906 = 1.064141869544983 + 50.0 * 8.915712356567383
Epoch 670, val loss: 1.0627219676971436
Epoch 680, training loss: 447.0960693359375 = 1.063299298286438 + 50.0 * 8.920655250549316
Epoch 680, val loss: 1.061922311782837
Epoch 690, training loss: 446.7613525390625 = 1.0625441074371338 + 50.0 * 8.913976669311523
Epoch 690, val loss: 1.061181902885437
Epoch 700, training loss: 446.8069763183594 = 1.0617311000823975 + 50.0 * 8.914904594421387
Epoch 700, val loss: 1.06036376953125
Epoch 710, training loss: 447.3805847167969 = 1.0609378814697266 + 50.0 * 8.926392555236816
Epoch 710, val loss: 1.0595790147781372
Epoch 720, training loss: 447.576904296875 = 1.0601173639297485 + 50.0 * 8.930335998535156
Epoch 720, val loss: 1.0587594509124756
Epoch 730, training loss: 447.7718505859375 = 1.05928373336792 + 50.0 * 8.934250831604004
Epoch 730, val loss: 1.0579380989074707
Epoch 740, training loss: 447.55364990234375 = 1.058400273323059 + 50.0 * 8.92990493774414
Epoch 740, val loss: 1.0570772886276245
Epoch 750, training loss: 447.7499694824219 = 1.0575506687164307 + 50.0 * 8.93384838104248
Epoch 750, val loss: 1.056236982345581
Epoch 760, training loss: 447.7868957519531 = 1.05667245388031 + 50.0 * 8.93460464477539
Epoch 760, val loss: 1.0553209781646729
Epoch 770, training loss: 448.1060791015625 = 1.055837869644165 + 50.0 * 8.941004753112793
Epoch 770, val loss: 1.054509162902832
Epoch 780, training loss: 448.16119384765625 = 1.0549367666244507 + 50.0 * 8.94212532043457
Epoch 780, val loss: 1.0536389350891113
Epoch 790, training loss: 448.2168884277344 = 1.0541024208068848 + 50.0 * 8.943255424499512
Epoch 790, val loss: 1.052811622619629
Epoch 800, training loss: 448.466064453125 = 1.053239345550537 + 50.0 * 8.948256492614746
Epoch 800, val loss: 1.0519541501998901
Epoch 810, training loss: 448.70556640625 = 1.0523667335510254 + 50.0 * 8.95306396484375
Epoch 810, val loss: 1.051089882850647
Epoch 820, training loss: 448.8336181640625 = 1.0514768362045288 + 50.0 * 8.955642700195312
Epoch 820, val loss: 1.0502163171768188
Epoch 830, training loss: 448.8589782714844 = 1.050560712814331 + 50.0 * 8.956168174743652
Epoch 830, val loss: 1.0493156909942627
Epoch 840, training loss: 449.55438232421875 = 1.0496536493301392 + 50.0 * 8.970094680786133
Epoch 840, val loss: 1.0484036207199097
Epoch 850, training loss: 448.4397888183594 = 1.0486952066421509 + 50.0 * 8.947821617126465
Epoch 850, val loss: 1.0474658012390137
Epoch 860, training loss: 449.12945556640625 = 1.0478376150131226 + 50.0 * 8.961631774902344
Epoch 860, val loss: 1.0466163158416748
Epoch 870, training loss: 449.40496826171875 = 1.0469690561294556 + 50.0 * 8.96716022491455
Epoch 870, val loss: 1.045752763748169
Epoch 880, training loss: 449.7225341796875 = 1.0460964441299438 + 50.0 * 8.973528861999512
Epoch 880, val loss: 1.0448869466781616
Epoch 890, training loss: 449.7417907714844 = 1.0451732873916626 + 50.0 * 8.973932266235352
Epoch 890, val loss: 1.0439647436141968
Epoch 900, training loss: 449.77374267578125 = 1.0442314147949219 + 50.0 * 8.974590301513672
Epoch 900, val loss: 1.0430335998535156
Epoch 910, training loss: 450.19903564453125 = 1.0433318614959717 + 50.0 * 8.983114242553711
Epoch 910, val loss: 1.0421316623687744
Epoch 920, training loss: 450.0337829589844 = 1.0423994064331055 + 50.0 * 8.979827880859375
Epoch 920, val loss: 1.0412142276763916
Epoch 930, training loss: 450.2923889160156 = 1.0414893627166748 + 50.0 * 8.985017776489258
Epoch 930, val loss: 1.0403062105178833
Epoch 940, training loss: 450.8926086425781 = 1.040553331375122 + 50.0 * 8.997040748596191
Epoch 940, val loss: 1.0393736362457275
Epoch 950, training loss: 450.40631103515625 = 1.0396020412445068 + 50.0 * 8.987334251403809
Epoch 950, val loss: 1.038413166999817
Epoch 960, training loss: 449.4300842285156 = 1.0385059118270874 + 50.0 * 8.9678316116333
Epoch 960, val loss: 1.0373083353042603
Epoch 970, training loss: 450.5885314941406 = 1.0376663208007812 + 50.0 * 8.99101734161377
Epoch 970, val loss: 1.0364665985107422
Epoch 980, training loss: 450.26348876953125 = 1.0366321802139282 + 50.0 * 8.984537124633789
Epoch 980, val loss: 1.0354807376861572
Epoch 990, training loss: 450.3118896484375 = 1.035671591758728 + 50.0 * 8.98552417755127
Epoch 990, val loss: 1.034548282623291
Epoch 1000, training loss: 450.4925231933594 = 1.034721851348877 + 50.0 * 8.989155769348145
Epoch 1000, val loss: 1.033593773841858
Epoch 1010, training loss: 451.1804504394531 = 1.0337897539138794 + 50.0 * 9.002933502197266
Epoch 1010, val loss: 1.03267502784729
Epoch 1020, training loss: 450.5061950683594 = 1.032728910446167 + 50.0 * 8.989469528198242
Epoch 1020, val loss: 1.0315697193145752
Epoch 1030, training loss: 450.9247741699219 = 1.0317498445510864 + 50.0 * 8.9978609085083
Epoch 1030, val loss: 1.030639886856079
Epoch 1040, training loss: 451.7727966308594 = 1.03079354763031 + 50.0 * 9.014840126037598
Epoch 1040, val loss: 1.0297279357910156
Epoch 1050, training loss: 451.7259216308594 = 1.0297777652740479 + 50.0 * 9.013922691345215
Epoch 1050, val loss: 1.0287256240844727
Epoch 1060, training loss: 452.12139892578125 = 1.0287965536117554 + 50.0 * 9.021851539611816
Epoch 1060, val loss: 1.0277366638183594
Epoch 1070, training loss: 452.221923828125 = 1.027757167816162 + 50.0 * 9.023882865905762
Epoch 1070, val loss: 1.0267386436462402
Epoch 1080, training loss: 452.24798583984375 = 1.0267056226730347 + 50.0 * 9.024425506591797
Epoch 1080, val loss: 1.0257099866867065
Epoch 1090, training loss: 452.4944152832031 = 1.0256648063659668 + 50.0 * 9.029375076293945
Epoch 1090, val loss: 1.0246999263763428
Epoch 1100, training loss: 452.8179931640625 = 1.0246630907058716 + 50.0 * 9.035866737365723
Epoch 1100, val loss: 1.0236889123916626
Epoch 1110, training loss: 452.6567077636719 = 1.0235881805419922 + 50.0 * 9.032662391662598
Epoch 1110, val loss: 1.0226266384124756
Epoch 1120, training loss: 452.810791015625 = 1.0225309133529663 + 50.0 * 9.035765647888184
Epoch 1120, val loss: 1.0215884447097778
Epoch 1130, training loss: 452.9366455078125 = 1.0214717388153076 + 50.0 * 9.03830337524414
Epoch 1130, val loss: 1.02054762840271
Epoch 1140, training loss: 452.7417297363281 = 1.0203876495361328 + 50.0 * 9.03442668914795
Epoch 1140, val loss: 1.019459843635559
Epoch 1150, training loss: 452.74749755859375 = 1.019271969795227 + 50.0 * 9.034564018249512
Epoch 1150, val loss: 1.0183919668197632
Epoch 1160, training loss: 452.1557922363281 = 1.0180940628051758 + 50.0 * 9.022753715515137
Epoch 1160, val loss: 1.0172232389450073
Epoch 1170, training loss: 452.7431335449219 = 1.0171071290969849 + 50.0 * 9.034520149230957
Epoch 1170, val loss: 1.0162731409072876
Epoch 1180, training loss: 453.3299865722656 = 1.0160714387893677 + 50.0 * 9.04627799987793
Epoch 1180, val loss: 1.0152702331542969
Epoch 1190, training loss: 453.5235900878906 = 1.0149725675582886 + 50.0 * 9.050171852111816
Epoch 1190, val loss: 1.0141959190368652
Epoch 1200, training loss: 453.7825012207031 = 1.013899564743042 + 50.0 * 9.05537223815918
Epoch 1200, val loss: 1.0131670236587524
Epoch 1210, training loss: 453.88494873046875 = 1.012831449508667 + 50.0 * 9.057442665100098
Epoch 1210, val loss: 1.0121090412139893
Epoch 1220, training loss: 453.7057189941406 = 1.0117177963256836 + 50.0 * 9.053879737854004
Epoch 1220, val loss: 1.0110260248184204
Epoch 1230, training loss: 453.8406677246094 = 1.0106579065322876 + 50.0 * 9.056600570678711
Epoch 1230, val loss: 1.0099953413009644
Epoch 1240, training loss: 453.93218994140625 = 1.009554386138916 + 50.0 * 9.058452606201172
Epoch 1240, val loss: 1.0089409351348877
Epoch 1250, training loss: 454.1864929199219 = 1.0084530115127563 + 50.0 * 9.063560485839844
Epoch 1250, val loss: 1.0078517198562622
Epoch 1260, training loss: 454.2336120605469 = 1.0074305534362793 + 50.0 * 9.064523696899414
Epoch 1260, val loss: 1.006842851638794
Epoch 1270, training loss: 454.1741638183594 = 1.0063366889953613 + 50.0 * 9.063356399536133
Epoch 1270, val loss: 1.0058071613311768
Epoch 1280, training loss: 454.4318542480469 = 1.005250334739685 + 50.0 * 9.06853199005127
Epoch 1280, val loss: 1.0047491788864136
Epoch 1290, training loss: 454.46875 = 1.0041766166687012 + 50.0 * 9.069291114807129
Epoch 1290, val loss: 1.0037039518356323
Epoch 1300, training loss: 454.6904602050781 = 1.003103494644165 + 50.0 * 9.073746681213379
Epoch 1300, val loss: 1.0026699304580688
Epoch 1310, training loss: 454.7667541503906 = 1.001991868019104 + 50.0 * 9.075295448303223
Epoch 1310, val loss: 1.0015990734100342
Epoch 1320, training loss: 455.0185852050781 = 1.0008569955825806 + 50.0 * 9.080354690551758
Epoch 1320, val loss: 1.0004973411560059
Epoch 1330, training loss: 454.75836181640625 = 0.9997668862342834 + 50.0 * 9.075172424316406
Epoch 1330, val loss: 0.9994796514511108
Epoch 1340, training loss: 454.9994201660156 = 0.9986493587493896 + 50.0 * 9.080015182495117
Epoch 1340, val loss: 0.9983925819396973
Epoch 1350, training loss: 455.2144775390625 = 0.9975696206092834 + 50.0 * 9.084338188171387
Epoch 1350, val loss: 0.997374951839447
Epoch 1360, training loss: 455.35107421875 = 0.9964644312858582 + 50.0 * 9.087092399597168
Epoch 1360, val loss: 0.9962937831878662
Epoch 1370, training loss: 454.90216064453125 = 0.9953002333641052 + 50.0 * 9.078137397766113
Epoch 1370, val loss: 0.9952008128166199
Epoch 1380, training loss: 455.0184326171875 = 0.9942015409469604 + 50.0 * 9.080484390258789
Epoch 1380, val loss: 0.9941549897193909
Epoch 1390, training loss: 455.3373107910156 = 0.9931337833404541 + 50.0 * 9.086883544921875
Epoch 1390, val loss: 0.9931128025054932
Epoch 1400, training loss: 455.722900390625 = 0.9920582175254822 + 50.0 * 9.094616889953613
Epoch 1400, val loss: 0.9920911192893982
Epoch 1410, training loss: 455.8079833984375 = 0.9909507632255554 + 50.0 * 9.096341133117676
Epoch 1410, val loss: 0.9910378456115723
Epoch 1420, training loss: 455.7189636230469 = 0.9898463487625122 + 50.0 * 9.094582557678223
Epoch 1420, val loss: 0.9899699091911316
Epoch 1430, training loss: 455.92022705078125 = 0.9887600541114807 + 50.0 * 9.098628997802734
Epoch 1430, val loss: 0.9889336824417114
Epoch 1440, training loss: 456.0126037597656 = 0.987648069858551 + 50.0 * 9.100499153137207
Epoch 1440, val loss: 0.9878628849983215
Epoch 1450, training loss: 455.8253479003906 = 0.9865218997001648 + 50.0 * 9.096776962280273
Epoch 1450, val loss: 0.9867672920227051
Epoch 1460, training loss: 456.1153259277344 = 0.9854253530502319 + 50.0 * 9.102598190307617
Epoch 1460, val loss: 0.9857296943664551
Epoch 1470, training loss: 456.379150390625 = 0.98432856798172 + 50.0 * 9.10789680480957
Epoch 1470, val loss: 0.9846807718276978
Epoch 1480, training loss: 456.1895446777344 = 0.9831937551498413 + 50.0 * 9.104126930236816
Epoch 1480, val loss: 0.9835761189460754
Epoch 1490, training loss: 455.6097106933594 = 0.9820774793624878 + 50.0 * 9.092552185058594
Epoch 1490, val loss: 0.9825034141540527
Epoch 1500, training loss: 455.8871154785156 = 0.9809777736663818 + 50.0 * 9.098122596740723
Epoch 1500, val loss: 0.981487512588501
Epoch 1510, training loss: 456.2550964355469 = 0.9798946380615234 + 50.0 * 9.105504035949707
Epoch 1510, val loss: 0.980454683303833
Epoch 1520, training loss: 456.66143798828125 = 0.9788137078285217 + 50.0 * 9.113652229309082
Epoch 1520, val loss: 0.9794154167175293
Epoch 1530, training loss: 456.7542724609375 = 0.9776729345321655 + 50.0 * 9.115531921386719
Epoch 1530, val loss: 0.9783353805541992
Epoch 1540, training loss: 456.79168701171875 = 0.97655189037323 + 50.0 * 9.116302490234375
Epoch 1540, val loss: 0.9772606492042542
Epoch 1550, training loss: 456.9269714355469 = 0.9754369854927063 + 50.0 * 9.119030952453613
Epoch 1550, val loss: 0.976192057132721
Epoch 1560, training loss: 457.14599609375 = 0.9743222594261169 + 50.0 * 9.123433113098145
Epoch 1560, val loss: 0.975124180316925
Epoch 1570, training loss: 457.2315368652344 = 0.973211407661438 + 50.0 * 9.125166893005371
Epoch 1570, val loss: 0.9740462899208069
Epoch 1580, training loss: 456.9428405761719 = 0.972082257270813 + 50.0 * 9.119415283203125
Epoch 1580, val loss: 0.9729776978492737
Epoch 1590, training loss: 457.1272888183594 = 0.970958411693573 + 50.0 * 9.123126983642578
Epoch 1590, val loss: 0.9719042181968689
Epoch 1600, training loss: 457.16705322265625 = 0.9698581099510193 + 50.0 * 9.123944282531738
Epoch 1600, val loss: 0.9708816409111023
Epoch 1610, training loss: 457.40673828125 = 0.9687986373901367 + 50.0 * 9.128758430480957
Epoch 1610, val loss: 0.9698528051376343
Epoch 1620, training loss: 457.3547668457031 = 0.967683732509613 + 50.0 * 9.127741813659668
Epoch 1620, val loss: 0.9688004851341248
Epoch 1630, training loss: 457.4605712890625 = 0.9665713310241699 + 50.0 * 9.12987995147705
Epoch 1630, val loss: 0.9677291512489319
Epoch 1640, training loss: 457.4190979003906 = 0.9654467105865479 + 50.0 * 9.129073143005371
Epoch 1640, val loss: 0.9666672945022583
Epoch 1650, training loss: 457.3351135253906 = 0.9643185138702393 + 50.0 * 9.127415657043457
Epoch 1650, val loss: 0.9655819535255432
Epoch 1660, training loss: 457.8061828613281 = 0.9631943702697754 + 50.0 * 9.136859893798828
Epoch 1660, val loss: 0.9645267724990845
Epoch 1670, training loss: 457.8222351074219 = 0.9621407389640808 + 50.0 * 9.137202262878418
Epoch 1670, val loss: 0.9635209441184998
Epoch 1680, training loss: 457.8936462402344 = 0.9610769152641296 + 50.0 * 9.138650894165039
Epoch 1680, val loss: 0.9625036120414734
Epoch 1690, training loss: 457.53515625 = 0.9599797129631042 + 50.0 * 9.131503105163574
Epoch 1690, val loss: 0.9614850878715515
Epoch 1700, training loss: 457.8904724121094 = 0.9589273929595947 + 50.0 * 9.138630867004395
Epoch 1700, val loss: 0.9604761004447937
Epoch 1710, training loss: 457.7882995605469 = 0.9578034281730652 + 50.0 * 9.13661003112793
Epoch 1710, val loss: 0.9594220519065857
Epoch 1720, training loss: 458.002197265625 = 0.9567022323608398 + 50.0 * 9.140910148620605
Epoch 1720, val loss: 0.9583784341812134
Epoch 1730, training loss: 458.18975830078125 = 0.9556277394294739 + 50.0 * 9.144682884216309
Epoch 1730, val loss: 0.9573503732681274
Epoch 1740, training loss: 457.8419494628906 = 0.9545072913169861 + 50.0 * 9.137748718261719
Epoch 1740, val loss: 0.9562970995903015
Epoch 1750, training loss: 457.9522399902344 = 0.9534001350402832 + 50.0 * 9.139976501464844
Epoch 1750, val loss: 0.9552347660064697
Epoch 1760, training loss: 458.4585266113281 = 0.9523897171020508 + 50.0 * 9.15012264251709
Epoch 1760, val loss: 0.9542853236198425
Epoch 1770, training loss: 457.5876770019531 = 0.9512587189674377 + 50.0 * 9.132728576660156
Epoch 1770, val loss: 0.9532430171966553
Epoch 1780, training loss: 457.8818054199219 = 0.9502232670783997 + 50.0 * 9.138631820678711
Epoch 1780, val loss: 0.9522618651390076
Epoch 1790, training loss: 458.38055419921875 = 0.9492829442024231 + 50.0 * 9.148625373840332
Epoch 1790, val loss: 0.9513931274414062
Epoch 1800, training loss: 458.6146545410156 = 0.9482855200767517 + 50.0 * 9.153327941894531
Epoch 1800, val loss: 0.9504604339599609
Epoch 1810, training loss: 458.12457275390625 = 0.94719398021698 + 50.0 * 9.143547058105469
Epoch 1810, val loss: 0.9494373202323914
Epoch 1820, training loss: 458.5700378417969 = 0.9461422562599182 + 50.0 * 9.152478218078613
Epoch 1820, val loss: 0.9484465718269348
Epoch 1830, training loss: 458.7958984375 = 0.9450773000717163 + 50.0 * 9.15701675415039
Epoch 1830, val loss: 0.9474492073059082
Epoch 1840, training loss: 458.9932861328125 = 0.944001317024231 + 50.0 * 9.160985946655273
Epoch 1840, val loss: 0.946435809135437
Epoch 1850, training loss: 459.1053466796875 = 0.9428936243057251 + 50.0 * 9.163249015808105
Epoch 1850, val loss: 0.9453949332237244
Epoch 1860, training loss: 458.71832275390625 = 0.9417799115180969 + 50.0 * 9.15553092956543
Epoch 1860, val loss: 0.9443395733833313
Epoch 1870, training loss: 459.0643615722656 = 0.9407440423965454 + 50.0 * 9.16247272491455
Epoch 1870, val loss: 0.9433730840682983
Epoch 1880, training loss: 459.46844482421875 = 0.9397304654121399 + 50.0 * 9.170574188232422
Epoch 1880, val loss: 0.9424182772636414
Epoch 1890, training loss: 459.508544921875 = 0.9386865496635437 + 50.0 * 9.17139720916748
Epoch 1890, val loss: 0.9414313435554504
Epoch 1900, training loss: 459.3069152832031 = 0.9376094341278076 + 50.0 * 9.167386054992676
Epoch 1900, val loss: 0.9403994679450989
Epoch 1910, training loss: 459.4550476074219 = 0.9366347789764404 + 50.0 * 9.170368194580078
Epoch 1910, val loss: 0.9394957423210144
Epoch 1920, training loss: 459.5105285644531 = 0.9356088042259216 + 50.0 * 9.17149829864502
Epoch 1920, val loss: 0.9385459423065186
Epoch 1930, training loss: 459.75372314453125 = 0.9345734119415283 + 50.0 * 9.176383018493652
Epoch 1930, val loss: 0.9375846982002258
Epoch 1940, training loss: 459.9674377441406 = 0.9335663914680481 + 50.0 * 9.18067741394043
Epoch 1940, val loss: 0.9366396069526672
Epoch 1950, training loss: 459.8430480957031 = 0.9325242042541504 + 50.0 * 9.178210258483887
Epoch 1950, val loss: 0.9356702566146851
Epoch 1960, training loss: 459.5544738769531 = 0.9314844608306885 + 50.0 * 9.172459602355957
Epoch 1960, val loss: 0.9346954822540283
Epoch 1970, training loss: 459.8209533691406 = 0.930487871170044 + 50.0 * 9.177809715270996
Epoch 1970, val loss: 0.9337723851203918
Epoch 1980, training loss: 460.1075134277344 = 0.9294947981834412 + 50.0 * 9.183560371398926
Epoch 1980, val loss: 0.9328435659408569
Epoch 1990, training loss: 460.17822265625 = 0.9284820556640625 + 50.0 * 9.1849946975708
Epoch 1990, val loss: 0.9318972826004028
Epoch 2000, training loss: 460.39373779296875 = 0.9274427890777588 + 50.0 * 9.189326286315918
Epoch 2000, val loss: 0.9308865070343018
Epoch 2010, training loss: 459.22296142578125 = 0.9266575574874878 + 50.0 * 9.165925979614258
Epoch 2010, val loss: 0.930294394493103
Epoch 2020, training loss: 458.8786315917969 = 0.9258137345314026 + 50.0 * 9.159056663513184
Epoch 2020, val loss: 0.929520308971405
Epoch 2030, training loss: 459.04107666015625 = 0.924863338470459 + 50.0 * 9.162323951721191
Epoch 2030, val loss: 0.9286031126976013
Epoch 2040, training loss: 459.20867919921875 = 0.9239247441291809 + 50.0 * 9.165695190429688
Epoch 2040, val loss: 0.9277089238166809
Epoch 2050, training loss: 459.6552734375 = 0.9229466915130615 + 50.0 * 9.174646377563477
Epoch 2050, val loss: 0.926814615726471
Epoch 2060, training loss: 460.06915283203125 = 0.9219949841499329 + 50.0 * 9.182943344116211
Epoch 2060, val loss: 0.9259325861930847
Epoch 2070, training loss: 460.38470458984375 = 0.9210464954376221 + 50.0 * 9.1892728805542
Epoch 2070, val loss: 0.9250588417053223
Epoch 2080, training loss: 460.27301025390625 = 0.9200745224952698 + 50.0 * 9.187058448791504
Epoch 2080, val loss: 0.9241611361503601
Epoch 2090, training loss: 460.4132995605469 = 0.9191164374351501 + 50.0 * 9.1898832321167
Epoch 2090, val loss: 0.9232731461524963
Epoch 2100, training loss: 460.65606689453125 = 0.9181681275367737 + 50.0 * 9.194757461547852
Epoch 2100, val loss: 0.9224056005477905
Epoch 2110, training loss: 460.7898254394531 = 0.9171949625015259 + 50.0 * 9.197452545166016
Epoch 2110, val loss: 0.9215067625045776
Epoch 2120, training loss: 460.8002624511719 = 0.9162435531616211 + 50.0 * 9.197680473327637
Epoch 2120, val loss: 0.920635461807251
Epoch 2130, training loss: 460.9831237792969 = 0.9152947664260864 + 50.0 * 9.201356887817383
Epoch 2130, val loss: 0.9197569489479065
Epoch 2140, training loss: 461.09112548828125 = 0.9143306612968445 + 50.0 * 9.203536033630371
Epoch 2140, val loss: 0.9188638925552368
Epoch 2150, training loss: 460.6102600097656 = 0.9133720397949219 + 50.0 * 9.193938255310059
Epoch 2150, val loss: 0.9179903268814087
Epoch 2160, training loss: 460.9649963378906 = 0.9124501347541809 + 50.0 * 9.201050758361816
Epoch 2160, val loss: 0.9171401858329773
Epoch 2170, training loss: 461.4234924316406 = 0.9115192890167236 + 50.0 * 9.21023941040039
Epoch 2170, val loss: 0.9162992238998413
Epoch 2180, training loss: 461.4929504394531 = 0.9105672240257263 + 50.0 * 9.211647987365723
Epoch 2180, val loss: 0.9154425859451294
Epoch 2190, training loss: 461.4782409667969 = 0.9096382856369019 + 50.0 * 9.211372375488281
Epoch 2190, val loss: 0.9145899415016174
Epoch 2200, training loss: 461.62628173828125 = 0.9087018966674805 + 50.0 * 9.214351654052734
Epoch 2200, val loss: 0.9137327075004578
Epoch 2210, training loss: 461.0644836425781 = 0.9077296853065491 + 50.0 * 9.20313549041748
Epoch 2210, val loss: 0.9128870964050293
Epoch 2220, training loss: 460.3273010253906 = 0.9067752957344055 + 50.0 * 9.188410758972168
Epoch 2220, val loss: 0.9120067954063416
Epoch 2230, training loss: 460.6716003417969 = 0.905910074710846 + 50.0 * 9.195313453674316
Epoch 2230, val loss: 0.9112424254417419
Epoch 2240, training loss: 461.3172302246094 = 0.9050624966621399 + 50.0 * 9.208243370056152
Epoch 2240, val loss: 0.910461962223053
Epoch 2250, training loss: 461.84027099609375 = 0.9041565656661987 + 50.0 * 9.218722343444824
Epoch 2250, val loss: 0.9096398949623108
Epoch 2260, training loss: 462.164306640625 = 0.9032683968544006 + 50.0 * 9.225220680236816
Epoch 2260, val loss: 0.9088424444198608
Epoch 2270, training loss: 462.1093444824219 = 0.9023754000663757 + 50.0 * 9.224139213562012
Epoch 2270, val loss: 0.9080297350883484
Epoch 2280, training loss: 462.1700439453125 = 0.9014866352081299 + 50.0 * 9.225371360778809
Epoch 2280, val loss: 0.9072359204292297
Epoch 2290, training loss: 462.5184631347656 = 0.9006097912788391 + 50.0 * 9.232357025146484
Epoch 2290, val loss: 0.9064525365829468
Epoch 2300, training loss: 462.16693115234375 = 0.8996906280517578 + 50.0 * 9.22534465789795
Epoch 2300, val loss: 0.9056268334388733
Epoch 2310, training loss: 462.41241455078125 = 0.898838460445404 + 50.0 * 9.230271339416504
Epoch 2310, val loss: 0.904853343963623
Epoch 2320, training loss: 462.6199035644531 = 0.8979615569114685 + 50.0 * 9.2344388961792
Epoch 2320, val loss: 0.9040713310241699
Epoch 2330, training loss: 462.7522277832031 = 0.8970978260040283 + 50.0 * 9.237102508544922
Epoch 2330, val loss: 0.9033030867576599
Epoch 2340, training loss: 462.6167907714844 = 0.8962278962135315 + 50.0 * 9.234411239624023
Epoch 2340, val loss: 0.9025210738182068
Epoch 2350, training loss: 462.78070068359375 = 0.8953803777694702 + 50.0 * 9.237706184387207
Epoch 2350, val loss: 0.9017672538757324
Epoch 2360, training loss: 462.961181640625 = 0.8945436477661133 + 50.0 * 9.2413330078125
Epoch 2360, val loss: 0.90102618932724
Epoch 2370, training loss: 462.9551696777344 = 0.8936883211135864 + 50.0 * 9.241230010986328
Epoch 2370, val loss: 0.900268018245697
Epoch 2380, training loss: 462.96246337890625 = 0.8928543925285339 + 50.0 * 9.241392135620117
Epoch 2380, val loss: 0.8995201587677002
Epoch 2390, training loss: 463.1994323730469 = 0.8920183777809143 + 50.0 * 9.246148109436035
Epoch 2390, val loss: 0.8987892866134644
Epoch 2400, training loss: 462.7108459472656 = 0.8911469578742981 + 50.0 * 9.236393928527832
Epoch 2400, val loss: 0.8980326652526855
Epoch 2410, training loss: 462.8669738769531 = 0.8903519511222839 + 50.0 * 9.239532470703125
Epoch 2410, val loss: 0.8973397612571716
Epoch 2420, training loss: 463.6231384277344 = 0.8895792365074158 + 50.0 * 9.254671096801758
Epoch 2420, val loss: 0.8966439366340637
Epoch 2430, training loss: 463.37445068359375 = 0.888760507106781 + 50.0 * 9.249713897705078
Epoch 2430, val loss: 0.8959208726882935
Epoch 2440, training loss: 463.3956604003906 = 0.8879580497741699 + 50.0 * 9.250153541564941
Epoch 2440, val loss: 0.8952270150184631
Epoch 2450, training loss: 463.7428894042969 = 0.8871821761131287 + 50.0 * 9.25711441040039
Epoch 2450, val loss: 0.8945600986480713
Epoch 2460, training loss: 463.9089050292969 = 0.8863818645477295 + 50.0 * 9.26045036315918
Epoch 2460, val loss: 0.8938578963279724
Epoch 2470, training loss: 463.8132629394531 = 0.885589599609375 + 50.0 * 9.258553504943848
Epoch 2470, val loss: 0.8931635618209839
Epoch 2480, training loss: 464.0567626953125 = 0.8848095536231995 + 50.0 * 9.263439178466797
Epoch 2480, val loss: 0.8924859762191772
Epoch 2490, training loss: 464.2194519042969 = 0.8840385675430298 + 50.0 * 9.266708374023438
Epoch 2490, val loss: 0.8918229937553406
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.5389855072463768
0.8135188002608129
=== training gcn model ===
Epoch 0, training loss: 518.982666015625 = 1.0986112356185913 + 50.0 * 10.357680320739746
Epoch 0, val loss: 1.0986124277114868
Epoch 10, training loss: 501.4363098144531 = 1.0985251665115356 + 50.0 * 10.006755828857422
Epoch 10, val loss: 1.0985125303268433
Epoch 20, training loss: 491.81756591796875 = 1.0983966588974 + 50.0 * 9.814383506774902
Epoch 20, val loss: 1.09837007522583
Epoch 30, training loss: 484.41259765625 = 1.0982295274734497 + 50.0 * 9.666287422180176
Epoch 30, val loss: 1.0981886386871338
Epoch 40, training loss: 478.5767517089844 = 1.0980545282363892 + 50.0 * 9.54957389831543
Epoch 40, val loss: 1.0979975461959839
Epoch 50, training loss: 473.62188720703125 = 1.0978739261627197 + 50.0 * 9.450480461120605
Epoch 50, val loss: 1.0977978706359863
Epoch 60, training loss: 469.3868713378906 = 1.0976821184158325 + 50.0 * 9.36578369140625
Epoch 60, val loss: 1.0975888967514038
Epoch 70, training loss: 465.7746276855469 = 1.0974735021591187 + 50.0 * 9.293542861938477
Epoch 70, val loss: 1.097360372543335
Epoch 80, training loss: 462.6898498535156 = 1.0972660779953003 + 50.0 * 9.231851577758789
Epoch 80, val loss: 1.0971328020095825
Epoch 90, training loss: 460.0440979003906 = 1.0970278978347778 + 50.0 * 9.17894172668457
Epoch 90, val loss: 1.0968800783157349
Epoch 100, training loss: 457.7651062011719 = 1.0967822074890137 + 50.0 * 9.133366584777832
Epoch 100, val loss: 1.0966172218322754
Epoch 110, training loss: 455.7764587402344 = 1.09653902053833 + 50.0 * 9.093598365783691
Epoch 110, val loss: 1.0963603258132935
Epoch 120, training loss: 454.0770263671875 = 1.0962868928909302 + 50.0 * 9.059615135192871
Epoch 120, val loss: 1.096091628074646
Epoch 130, training loss: 452.5693359375 = 1.0960254669189453 + 50.0 * 9.029465675354004
Epoch 130, val loss: 1.0958186388015747
Epoch 140, training loss: 451.3267517089844 = 1.0957521200180054 + 50.0 * 9.004619598388672
Epoch 140, val loss: 1.095535397529602
Epoch 150, training loss: 450.19183349609375 = 1.095490574836731 + 50.0 * 8.981926918029785
Epoch 150, val loss: 1.0952531099319458
Epoch 160, training loss: 449.17449951171875 = 1.095212459564209 + 50.0 * 8.961585998535156
Epoch 160, val loss: 1.0949703454971313
Epoch 170, training loss: 448.34564208984375 = 1.0949424505233765 + 50.0 * 8.945013999938965
Epoch 170, val loss: 1.0946816205978394
Epoch 180, training loss: 447.6755065917969 = 1.0946683883666992 + 50.0 * 8.93161678314209
Epoch 180, val loss: 1.0943987369537354
Epoch 190, training loss: 447.119140625 = 1.0943925380706787 + 50.0 * 8.92049503326416
Epoch 190, val loss: 1.094103217124939
Epoch 200, training loss: 446.55419921875 = 1.0941038131713867 + 50.0 * 8.909201622009277
Epoch 200, val loss: 1.0938071012496948
Epoch 210, training loss: 446.24993896484375 = 1.0938307046890259 + 50.0 * 8.903121948242188
Epoch 210, val loss: 1.093515396118164
Epoch 220, training loss: 445.8702697753906 = 1.0935254096984863 + 50.0 * 8.89553451538086
Epoch 220, val loss: 1.0932143926620483
Epoch 230, training loss: 445.4985656738281 = 1.0932036638259888 + 50.0 * 8.888107299804688
Epoch 230, val loss: 1.0928709506988525
Epoch 240, training loss: 445.1905517578125 = 1.0929100513458252 + 50.0 * 8.881953239440918
Epoch 240, val loss: 1.0925586223602295
Epoch 250, training loss: 445.3701477050781 = 1.0926182270050049 + 50.0 * 8.885550498962402
Epoch 250, val loss: 1.0922656059265137
Epoch 260, training loss: 444.71820068359375 = 1.092281460762024 + 50.0 * 8.872518539428711
Epoch 260, val loss: 1.0919231176376343
Epoch 270, training loss: 444.503662109375 = 1.0919692516326904 + 50.0 * 8.868233680725098
Epoch 270, val loss: 1.0916017293930054
Epoch 280, training loss: 444.4173583984375 = 1.0916396379470825 + 50.0 * 8.866514205932617
Epoch 280, val loss: 1.0912563800811768
Epoch 290, training loss: 444.4417724609375 = 1.091312050819397 + 50.0 * 8.867009162902832
Epoch 290, val loss: 1.0909162759780884
Epoch 300, training loss: 444.5572509765625 = 1.0909634828567505 + 50.0 * 8.869325637817383
Epoch 300, val loss: 1.090560793876648
Epoch 310, training loss: 444.4992370605469 = 1.0906152725219727 + 50.0 * 8.868172645568848
Epoch 310, val loss: 1.0902093648910522
Epoch 320, training loss: 444.46453857421875 = 1.0902596712112427 + 50.0 * 8.867485046386719
Epoch 320, val loss: 1.0898488759994507
Epoch 330, training loss: 444.491943359375 = 1.0899077653884888 + 50.0 * 8.868041038513184
Epoch 330, val loss: 1.0894898176193237
Epoch 340, training loss: 444.6406555175781 = 1.0895428657531738 + 50.0 * 8.87102222442627
Epoch 340, val loss: 1.0891098976135254
Epoch 350, training loss: 444.5228576660156 = 1.089154601097107 + 50.0 * 8.868674278259277
Epoch 350, val loss: 1.0887259244918823
Epoch 360, training loss: 444.540283203125 = 1.088766098022461 + 50.0 * 8.869029998779297
Epoch 360, val loss: 1.088333249092102
Epoch 370, training loss: 444.83441162109375 = 1.0883703231811523 + 50.0 * 8.874920845031738
Epoch 370, val loss: 1.087929368019104
Epoch 380, training loss: 444.6794128417969 = 1.087961196899414 + 50.0 * 8.87182903289795
Epoch 380, val loss: 1.0875191688537598
Epoch 390, training loss: 444.7435302734375 = 1.0875343084335327 + 50.0 * 8.873120307922363
Epoch 390, val loss: 1.0870975255966187
Epoch 400, training loss: 444.9607849121094 = 1.087109923362732 + 50.0 * 8.877473831176758
Epoch 400, val loss: 1.0866779088974
Epoch 410, training loss: 445.1162414550781 = 1.086674690246582 + 50.0 * 8.88059139251709
Epoch 410, val loss: 1.0862430334091187
Epoch 420, training loss: 444.9393005371094 = 1.0862188339233398 + 50.0 * 8.87706184387207
Epoch 420, val loss: 1.0857969522476196
Epoch 430, training loss: 445.1292419433594 = 1.0857608318328857 + 50.0 * 8.88086986541748
Epoch 430, val loss: 1.0853337049484253
Epoch 440, training loss: 445.3792724609375 = 1.0853102207183838 + 50.0 * 8.885879516601562
Epoch 440, val loss: 1.0848883390426636
Epoch 450, training loss: 445.59857177734375 = 1.084858775138855 + 50.0 * 8.890274047851562
Epoch 450, val loss: 1.084444522857666
Epoch 460, training loss: 445.772216796875 = 1.0843955278396606 + 50.0 * 8.893756866455078
Epoch 460, val loss: 1.0839842557907104
Epoch 470, training loss: 445.84735107421875 = 1.0839056968688965 + 50.0 * 8.895269393920898
Epoch 470, val loss: 1.0834852457046509
Epoch 480, training loss: 445.80926513671875 = 1.083409309387207 + 50.0 * 8.894516944885254
Epoch 480, val loss: 1.0830087661743164
Epoch 490, training loss: 445.48614501953125 = 1.0829273462295532 + 50.0 * 8.88806438446045
Epoch 490, val loss: 1.0825481414794922
Epoch 500, training loss: 445.48828125 = 1.0824205875396729 + 50.0 * 8.888116836547852
Epoch 500, val loss: 1.082037329673767
Epoch 510, training loss: 445.85809326171875 = 1.0819443464279175 + 50.0 * 8.895523071289062
Epoch 510, val loss: 1.0815680027008057
Epoch 520, training loss: 446.181396484375 = 1.0814732313156128 + 50.0 * 8.901998519897461
Epoch 520, val loss: 1.0810978412628174
Epoch 530, training loss: 446.24969482421875 = 1.0809677839279175 + 50.0 * 8.903374671936035
Epoch 530, val loss: 1.08059561252594
Epoch 540, training loss: 446.3670349121094 = 1.0804755687713623 + 50.0 * 8.905731201171875
Epoch 540, val loss: 1.0801119804382324
Epoch 550, training loss: 446.6387023925781 = 1.0799695253372192 + 50.0 * 8.911174774169922
Epoch 550, val loss: 1.0796014070510864
Epoch 560, training loss: 446.7984313964844 = 1.079455018043518 + 50.0 * 8.914379119873047
Epoch 560, val loss: 1.0790915489196777
Epoch 570, training loss: 446.71978759765625 = 1.0788944959640503 + 50.0 * 8.91281795501709
Epoch 570, val loss: 1.078545331954956
Epoch 580, training loss: 446.83770751953125 = 1.0784103870391846 + 50.0 * 8.915185928344727
Epoch 580, val loss: 1.0780643224716187
Epoch 590, training loss: 447.12451171875 = 1.0778863430023193 + 50.0 * 8.92093276977539
Epoch 590, val loss: 1.0775219202041626
Epoch 600, training loss: 447.2554626464844 = 1.0772827863693237 + 50.0 * 8.923563957214355
Epoch 600, val loss: 1.0769236087799072
Epoch 610, training loss: 447.4109191894531 = 1.0766735076904297 + 50.0 * 8.926685333251953
Epoch 610, val loss: 1.0763295888900757
Epoch 620, training loss: 447.41937255859375 = 1.0760672092437744 + 50.0 * 8.926865577697754
Epoch 620, val loss: 1.0757182836532593
Epoch 630, training loss: 447.66864013671875 = 1.075454592704773 + 50.0 * 8.931863784790039
Epoch 630, val loss: 1.0751113891601562
Epoch 640, training loss: 447.7871398925781 = 1.0748374462127686 + 50.0 * 8.934246063232422
Epoch 640, val loss: 1.0744949579238892
Epoch 650, training loss: 447.7989196777344 = 1.0742110013961792 + 50.0 * 8.934494018554688
Epoch 650, val loss: 1.0738639831542969
Epoch 660, training loss: 447.9997863769531 = 1.0735690593719482 + 50.0 * 8.93852424621582
Epoch 660, val loss: 1.0732289552688599
Epoch 670, training loss: 456.2017822265625 = 1.0731993913650513 + 50.0 * 9.102571487426758
Epoch 670, val loss: 1.072804570198059
Epoch 680, training loss: 449.37451171875 = 1.0718145370483398 + 50.0 * 8.96605396270752
Epoch 680, val loss: 1.0713791847229004
Epoch 690, training loss: 446.9970397949219 = 1.0713707208633423 + 50.0 * 8.918513298034668
Epoch 690, val loss: 1.0710537433624268
Epoch 700, training loss: 445.0250244140625 = 1.0709059238433838 + 50.0 * 8.879082679748535
Epoch 700, val loss: 1.0705616474151611
Epoch 710, training loss: 443.9840087890625 = 1.0703818798065186 + 50.0 * 8.858272552490234
Epoch 710, val loss: 1.0700265169143677
Epoch 720, training loss: 443.32574462890625 = 1.0698647499084473 + 50.0 * 8.845117568969727
Epoch 720, val loss: 1.069536805152893
Epoch 730, training loss: 443.7903137207031 = 1.0693427324295044 + 50.0 * 8.854419708251953
Epoch 730, val loss: 1.0690116882324219
Epoch 740, training loss: 444.2454833984375 = 1.0687395334243774 + 50.0 * 8.863534927368164
Epoch 740, val loss: 1.0684006214141846
Epoch 750, training loss: 444.646728515625 = 1.068110466003418 + 50.0 * 8.871572494506836
Epoch 750, val loss: 1.0677776336669922
Epoch 760, training loss: 445.1524658203125 = 1.0674693584442139 + 50.0 * 8.881699562072754
Epoch 760, val loss: 1.067138671875
Epoch 770, training loss: 445.7488708496094 = 1.0668128728866577 + 50.0 * 8.893641471862793
Epoch 770, val loss: 1.0664831399917603
Epoch 780, training loss: 446.2041320800781 = 1.0661102533340454 + 50.0 * 8.90276050567627
Epoch 780, val loss: 1.0657907724380493
Epoch 790, training loss: 446.55572509765625 = 1.065413475036621 + 50.0 * 8.909806251525879
Epoch 790, val loss: 1.0650923252105713
Epoch 800, training loss: 447.06719970703125 = 1.06471848487854 + 50.0 * 8.920049667358398
Epoch 800, val loss: 1.0644112825393677
Epoch 810, training loss: 447.2266845703125 = 1.0640008449554443 + 50.0 * 8.923254013061523
Epoch 810, val loss: 1.0637015104293823
Epoch 820, training loss: 447.6110534667969 = 1.0632734298706055 + 50.0 * 8.93095588684082
Epoch 820, val loss: 1.0629886388778687
Epoch 830, training loss: 447.8649597167969 = 1.0625340938568115 + 50.0 * 8.93604850769043
Epoch 830, val loss: 1.062255620956421
Epoch 840, training loss: 447.96063232421875 = 1.061784029006958 + 50.0 * 8.937976837158203
Epoch 840, val loss: 1.061519742012024
Epoch 850, training loss: 448.22467041015625 = 1.0610415935516357 + 50.0 * 8.943272590637207
Epoch 850, val loss: 1.060788869857788
Epoch 860, training loss: 448.51580810546875 = 1.060302972793579 + 50.0 * 8.94911003112793
Epoch 860, val loss: 1.0600550174713135
Epoch 870, training loss: 448.8482971191406 = 1.0595544576644897 + 50.0 * 8.955775260925293
Epoch 870, val loss: 1.0593136548995972
Epoch 880, training loss: 449.1190490722656 = 1.0587835311889648 + 50.0 * 8.96120548248291
Epoch 880, val loss: 1.0585449934005737
Epoch 890, training loss: 449.1047058105469 = 1.057982325553894 + 50.0 * 8.96093463897705
Epoch 890, val loss: 1.0577625036239624
Epoch 900, training loss: 449.1546936035156 = 1.0571832656860352 + 50.0 * 8.961950302124023
Epoch 900, val loss: 1.0569896697998047
Epoch 910, training loss: 449.3875732421875 = 1.0563870668411255 + 50.0 * 8.966623306274414
Epoch 910, val loss: 1.056187391281128
Epoch 920, training loss: 449.7597961425781 = 1.0555955171585083 + 50.0 * 8.97408390045166
Epoch 920, val loss: 1.0554096698760986
Epoch 930, training loss: 449.7340087890625 = 1.0547430515289307 + 50.0 * 8.97358512878418
Epoch 930, val loss: 1.054556965827942
Epoch 940, training loss: 449.95849609375 = 1.0539110898971558 + 50.0 * 8.9780912399292
Epoch 940, val loss: 1.0537434816360474
Epoch 950, training loss: 450.2287902832031 = 1.0530545711517334 + 50.0 * 8.983514785766602
Epoch 950, val loss: 1.0529080629348755
Epoch 960, training loss: 450.3966979980469 = 1.0522044897079468 + 50.0 * 8.986889839172363
Epoch 960, val loss: 1.0520673990249634
Epoch 970, training loss: 450.4532470703125 = 1.051332712173462 + 50.0 * 8.988038063049316
Epoch 970, val loss: 1.0512181520462036
Epoch 980, training loss: 450.6873779296875 = 1.0504727363586426 + 50.0 * 8.992737770080566
Epoch 980, val loss: 1.0503679513931274
Epoch 990, training loss: 449.32977294921875 = 1.049487829208374 + 50.0 * 8.965605735778809
Epoch 990, val loss: 1.0494121313095093
Epoch 1000, training loss: 450.60870361328125 = 1.0487258434295654 + 50.0 * 8.991199493408203
Epoch 1000, val loss: 1.0486606359481812
Epoch 1010, training loss: 450.02386474609375 = 1.0478671789169312 + 50.0 * 8.979519844055176
Epoch 1010, val loss: 1.047799825668335
Epoch 1020, training loss: 450.7208251953125 = 1.0470855236053467 + 50.0 * 8.993474960327148
Epoch 1020, val loss: 1.0470341444015503
Epoch 1030, training loss: 450.9631652832031 = 1.0462329387664795 + 50.0 * 8.99833869934082
Epoch 1030, val loss: 1.0461993217468262
Epoch 1040, training loss: 451.2339172363281 = 1.0453673601150513 + 50.0 * 9.00377082824707
Epoch 1040, val loss: 1.0453588962554932
Epoch 1050, training loss: 451.3571472167969 = 1.044521689414978 + 50.0 * 9.00625228881836
Epoch 1050, val loss: 1.0445122718811035
Epoch 1060, training loss: 451.4325866699219 = 1.04364013671875 + 50.0 * 9.007779121398926
Epoch 1060, val loss: 1.0436664819717407
Epoch 1070, training loss: 451.6425476074219 = 1.0427831411361694 + 50.0 * 9.011995315551758
Epoch 1070, val loss: 1.042830228805542
Epoch 1080, training loss: 451.6484680175781 = 1.0419188737869263 + 50.0 * 9.012130737304688
Epoch 1080, val loss: 1.0419824123382568
Epoch 1090, training loss: 451.9800720214844 = 1.0410796403884888 + 50.0 * 9.018779754638672
Epoch 1090, val loss: 1.0411442518234253
Epoch 1100, training loss: 452.19140625 = 1.0402147769927979 + 50.0 * 9.02302360534668
Epoch 1100, val loss: 1.0403002500534058
Epoch 1110, training loss: 452.12554931640625 = 1.0393214225769043 + 50.0 * 9.021724700927734
Epoch 1110, val loss: 1.0394302606582642
Epoch 1120, training loss: 451.8450622558594 = 1.038478136062622 + 50.0 * 9.016131401062012
Epoch 1120, val loss: 1.0386009216308594
Epoch 1130, training loss: 452.0798645019531 = 1.037544846534729 + 50.0 * 9.020846366882324
Epoch 1130, val loss: 1.037706732749939
Epoch 1140, training loss: 452.49847412109375 = 1.0366686582565308 + 50.0 * 9.02923583984375
Epoch 1140, val loss: 1.0368484258651733
Epoch 1150, training loss: 452.5735168457031 = 1.035797119140625 + 50.0 * 9.030754089355469
Epoch 1150, val loss: 1.0359936952590942
Epoch 1160, training loss: 452.6243896484375 = 1.034913182258606 + 50.0 * 9.031789779663086
Epoch 1160, val loss: 1.0351316928863525
Epoch 1170, training loss: 452.7174987792969 = 1.0340240001678467 + 50.0 * 9.033669471740723
Epoch 1170, val loss: 1.0342662334442139
Epoch 1180, training loss: 452.8529357910156 = 1.0331313610076904 + 50.0 * 9.036396026611328
Epoch 1180, val loss: 1.0333904027938843
Epoch 1190, training loss: 453.1225280761719 = 1.032235860824585 + 50.0 * 9.0418062210083
Epoch 1190, val loss: 1.0325289964675903
Epoch 1200, training loss: 452.6736145019531 = 1.0312625169754028 + 50.0 * 9.03284740447998
Epoch 1200, val loss: 1.031583547592163
Epoch 1210, training loss: 452.6955261230469 = 1.0303641557693481 + 50.0 * 9.033303260803223
Epoch 1210, val loss: 1.0306875705718994
Epoch 1220, training loss: 452.94378662109375 = 1.0295144319534302 + 50.0 * 9.038285255432129
Epoch 1220, val loss: 1.029868721961975
Epoch 1230, training loss: 453.295654296875 = 1.0286246538162231 + 50.0 * 9.045340538024902
Epoch 1230, val loss: 1.0290043354034424
Epoch 1240, training loss: 453.4764099121094 = 1.027716875076294 + 50.0 * 9.04897403717041
Epoch 1240, val loss: 1.0281355381011963
Epoch 1250, training loss: 453.6657409667969 = 1.0268081426620483 + 50.0 * 9.052779197692871
Epoch 1250, val loss: 1.0272432565689087
Epoch 1260, training loss: 453.62420654296875 = 1.0258643627166748 + 50.0 * 9.051966667175293
Epoch 1260, val loss: 1.0263417959213257
Epoch 1270, training loss: 453.84991455078125 = 1.0249425172805786 + 50.0 * 9.056499481201172
Epoch 1270, val loss: 1.0254437923431396
Epoch 1280, training loss: 453.9846496582031 = 1.0240161418914795 + 50.0 * 9.059212684631348
Epoch 1280, val loss: 1.0245335102081299
Epoch 1290, training loss: 454.11505126953125 = 1.023091435432434 + 50.0 * 9.06183910369873
Epoch 1290, val loss: 1.0236283540725708
Epoch 1300, training loss: 454.1646423339844 = 1.0221519470214844 + 50.0 * 9.062849998474121
Epoch 1300, val loss: 1.022725224494934
Epoch 1310, training loss: 453.9874267578125 = 1.0211679935455322 + 50.0 * 9.059325218200684
Epoch 1310, val loss: 1.0217812061309814
Epoch 1320, training loss: 454.0603332519531 = 1.020215392112732 + 50.0 * 9.060802459716797
Epoch 1320, val loss: 1.0208498239517212
Epoch 1330, training loss: 454.46685791015625 = 1.019277811050415 + 50.0 * 9.068951606750488
Epoch 1330, val loss: 1.0199466943740845
Epoch 1340, training loss: 454.6484069824219 = 1.0183541774749756 + 50.0 * 9.072601318359375
Epoch 1340, val loss: 1.0190438032150269
Epoch 1350, training loss: 454.6946716308594 = 1.0173898935317993 + 50.0 * 9.073545455932617
Epoch 1350, val loss: 1.0181093215942383
Epoch 1360, training loss: 454.86212158203125 = 1.0164217948913574 + 50.0 * 9.076913833618164
Epoch 1360, val loss: 1.0171719789505005
Epoch 1370, training loss: 454.98736572265625 = 1.0154305696487427 + 50.0 * 9.079438209533691
Epoch 1370, val loss: 1.0162255764007568
Epoch 1380, training loss: 454.74591064453125 = 1.0144236087799072 + 50.0 * 9.074629783630371
Epoch 1380, val loss: 1.015254259109497
Epoch 1390, training loss: 454.9922790527344 = 1.0134731531143188 + 50.0 * 9.07957649230957
Epoch 1390, val loss: 1.0143288373947144
Epoch 1400, training loss: 455.1669921875 = 1.0125051736831665 + 50.0 * 9.083089828491211
Epoch 1400, val loss: 1.0134022235870361
Epoch 1410, training loss: 455.0946960449219 = 1.011511206626892 + 50.0 * 9.081664085388184
Epoch 1410, val loss: 1.012450098991394
Epoch 1420, training loss: 455.2838134765625 = 1.0105429887771606 + 50.0 * 9.085465431213379
Epoch 1420, val loss: 1.0115097761154175
Epoch 1430, training loss: 455.4298400878906 = 1.0095590353012085 + 50.0 * 9.08840560913086
Epoch 1430, val loss: 1.0105482339859009
Epoch 1440, training loss: 455.27691650390625 = 1.0085549354553223 + 50.0 * 9.085367202758789
Epoch 1440, val loss: 1.0095843076705933
Epoch 1450, training loss: 455.51324462890625 = 1.007585048675537 + 50.0 * 9.090112686157227
Epoch 1450, val loss: 1.008647084236145
Epoch 1460, training loss: 455.4246826171875 = 1.0065743923187256 + 50.0 * 9.088362693786621
Epoch 1460, val loss: 1.0076695680618286
Epoch 1470, training loss: 455.3624572753906 = 1.0055586099624634 + 50.0 * 9.087138175964355
Epoch 1470, val loss: 1.0066994428634644
Epoch 1480, training loss: 455.5531921386719 = 1.0046030282974243 + 50.0 * 9.090971946716309
Epoch 1480, val loss: 1.0057766437530518
Epoch 1490, training loss: 455.69866943359375 = 1.0036232471466064 + 50.0 * 9.093900680541992
Epoch 1490, val loss: 1.0048271417617798
Epoch 1500, training loss: 455.5025939941406 = 1.0026029348373413 + 50.0 * 9.09000015258789
Epoch 1500, val loss: 1.0038517713546753
Epoch 1510, training loss: 455.7246398925781 = 1.0016220808029175 + 50.0 * 9.094460487365723
Epoch 1510, val loss: 1.0029162168502808
Epoch 1520, training loss: 455.8124694824219 = 1.0006170272827148 + 50.0 * 9.096237182617188
Epoch 1520, val loss: 1.0019513368606567
Epoch 1530, training loss: 455.7306823730469 = 0.9996230006217957 + 50.0 * 9.094620704650879
Epoch 1530, val loss: 1.0009902715682983
Epoch 1540, training loss: 455.803955078125 = 0.998618483543396 + 50.0 * 9.09610652923584
Epoch 1540, val loss: 1.0000478029251099
Epoch 1550, training loss: 455.08416748046875 = 0.9975217580795288 + 50.0 * 9.081732749938965
Epoch 1550, val loss: 0.9989778995513916
Epoch 1560, training loss: 455.7535095214844 = 0.9966831803321838 + 50.0 * 9.095136642456055
Epoch 1560, val loss: 0.9981890320777893
Epoch 1570, training loss: 454.94415283203125 = 0.9955956935882568 + 50.0 * 9.078970909118652
Epoch 1570, val loss: 0.9971620440483093
Epoch 1580, training loss: 454.7283630371094 = 0.994601845741272 + 50.0 * 9.074675559997559
Epoch 1580, val loss: 0.9961952567100525
Epoch 1590, training loss: 455.4903259277344 = 0.9936544299125671 + 50.0 * 9.089933395385742
Epoch 1590, val loss: 0.9952978491783142
Epoch 1600, training loss: 455.7288818359375 = 0.9926361441612244 + 50.0 * 9.094724655151367
Epoch 1600, val loss: 0.9943419694900513
Epoch 1610, training loss: 456.0527038574219 = 0.9916456341743469 + 50.0 * 9.101221084594727
Epoch 1610, val loss: 0.9933860898017883
Epoch 1620, training loss: 456.1470947265625 = 0.9906173944473267 + 50.0 * 9.103129386901855
Epoch 1620, val loss: 0.9923867583274841
Epoch 1630, training loss: 456.32373046875 = 0.9895837306976318 + 50.0 * 9.106682777404785
Epoch 1630, val loss: 0.9914036989212036
Epoch 1640, training loss: 456.5544738769531 = 0.9885544180870056 + 50.0 * 9.111318588256836
Epoch 1640, val loss: 0.990418553352356
Epoch 1650, training loss: 456.6224060058594 = 0.9875291585922241 + 50.0 * 9.11269760131836
Epoch 1650, val loss: 0.9894365072250366
Epoch 1660, training loss: 456.4176025390625 = 0.9865072965621948 + 50.0 * 9.108621597290039
Epoch 1660, val loss: 0.988463282585144
Epoch 1670, training loss: 456.5655212402344 = 0.9854878187179565 + 50.0 * 9.111600875854492
Epoch 1670, val loss: 0.9874939918518066
Epoch 1680, training loss: 456.8170471191406 = 0.9844812154769897 + 50.0 * 9.11665153503418
Epoch 1680, val loss: 0.9865220785140991
Epoch 1690, training loss: 456.8547668457031 = 0.9834514260292053 + 50.0 * 9.117425918579102
Epoch 1690, val loss: 0.9855363965034485
Epoch 1700, training loss: 456.6820373535156 = 0.9823876023292542 + 50.0 * 9.113992691040039
Epoch 1700, val loss: 0.9845293164253235
Epoch 1710, training loss: 456.85845947265625 = 0.9813811779022217 + 50.0 * 9.117541313171387
Epoch 1710, val loss: 0.9835586547851562
Epoch 1720, training loss: 457.2262878417969 = 0.9803733229637146 + 50.0 * 9.124917984008789
Epoch 1720, val loss: 0.9826038479804993
Epoch 1730, training loss: 457.12066650390625 = 0.9792925715446472 + 50.0 * 9.122827529907227
Epoch 1730, val loss: 0.9815806150436401
Epoch 1740, training loss: 457.029541015625 = 0.9782638549804688 + 50.0 * 9.121025085449219
Epoch 1740, val loss: 0.9806036353111267
Epoch 1750, training loss: 457.22174072265625 = 0.9772276282310486 + 50.0 * 9.124890327453613
Epoch 1750, val loss: 0.9796261787414551
Epoch 1760, training loss: 457.406005859375 = 0.97623211145401 + 50.0 * 9.128595352172852
Epoch 1760, val loss: 0.9786679744720459
Epoch 1770, training loss: 457.21270751953125 = 0.9751641154289246 + 50.0 * 9.124751091003418
Epoch 1770, val loss: 0.9776363372802734
Epoch 1780, training loss: 457.0477600097656 = 0.9741446375846863 + 50.0 * 9.121472358703613
Epoch 1780, val loss: 0.9766958951950073
Epoch 1790, training loss: 457.32183837890625 = 0.9730973243713379 + 50.0 * 9.126975059509277
Epoch 1790, val loss: 0.9757068157196045
Epoch 1800, training loss: 457.70977783203125 = 0.9721069931983948 + 50.0 * 9.134753227233887
Epoch 1800, val loss: 0.9747700691223145
Epoch 1810, training loss: 457.6098327636719 = 0.9710742235183716 + 50.0 * 9.13277530670166
Epoch 1810, val loss: 0.9737997055053711
Epoch 1820, training loss: 457.8167724609375 = 0.9700618982315063 + 50.0 * 9.136934280395508
Epoch 1820, val loss: 0.97284996509552
Epoch 1830, training loss: 457.83953857421875 = 0.9690324664115906 + 50.0 * 9.137410163879395
Epoch 1830, val loss: 0.9718723893165588
Epoch 1840, training loss: 457.9223327636719 = 0.9679931402206421 + 50.0 * 9.139086723327637
Epoch 1840, val loss: 0.970893919467926
Epoch 1850, training loss: 457.7850646972656 = 0.966949462890625 + 50.0 * 9.136362075805664
Epoch 1850, val loss: 0.9699100852012634
Epoch 1860, training loss: 458.0372314453125 = 0.9659351706504822 + 50.0 * 9.141426086425781
Epoch 1860, val loss: 0.968957781791687
Epoch 1870, training loss: 458.2581481933594 = 0.9649268388748169 + 50.0 * 9.145864486694336
Epoch 1870, val loss: 0.9680078625679016
Epoch 1880, training loss: 458.2912292480469 = 0.9638978242874146 + 50.0 * 9.146546363830566
Epoch 1880, val loss: 0.967052698135376
Epoch 1890, training loss: 458.41033935546875 = 0.9628772735595703 + 50.0 * 9.148948669433594
Epoch 1890, val loss: 0.9660837650299072
Epoch 1900, training loss: 458.50189208984375 = 0.9618515372276306 + 50.0 * 9.150800704956055
Epoch 1900, val loss: 0.9651162028312683
Epoch 1910, training loss: 458.535888671875 = 0.9608168005943298 + 50.0 * 9.151501655578613
Epoch 1910, val loss: 0.9641551375389099
Epoch 1920, training loss: 458.707763671875 = 0.9597864151000977 + 50.0 * 9.154959678649902
Epoch 1920, val loss: 0.9631917476654053
Epoch 1930, training loss: 458.6329040527344 = 0.9587570428848267 + 50.0 * 9.153482437133789
Epoch 1930, val loss: 0.962213933467865
Epoch 1940, training loss: 458.7919616699219 = 0.957735002040863 + 50.0 * 9.156684875488281
Epoch 1940, val loss: 0.9612575173377991
Epoch 1950, training loss: 458.8236389160156 = 0.9567022919654846 + 50.0 * 9.157339096069336
Epoch 1950, val loss: 0.9602853059768677
Epoch 1960, training loss: 458.91650390625 = 0.9556708931922913 + 50.0 * 9.15921688079834
Epoch 1960, val loss: 0.959309995174408
Epoch 1970, training loss: 458.9322204589844 = 0.954648494720459 + 50.0 * 9.159551620483398
Epoch 1970, val loss: 0.9583346247673035
Epoch 1980, training loss: 459.1026916503906 = 0.9536535143852234 + 50.0 * 9.162981033325195
Epoch 1980, val loss: 0.9573628902435303
Epoch 1990, training loss: 458.6798095703125 = 0.9525758028030396 + 50.0 * 9.154544830322266
Epoch 1990, val loss: 0.9563890099525452
Epoch 2000, training loss: 458.78912353515625 = 0.9515867233276367 + 50.0 * 9.156750679016113
Epoch 2000, val loss: 0.9554305672645569
Epoch 2010, training loss: 459.23486328125 = 0.9505930542945862 + 50.0 * 9.165685653686523
Epoch 2010, val loss: 0.9545044898986816
Epoch 2020, training loss: 459.396484375 = 0.9495927691459656 + 50.0 * 9.168937683105469
Epoch 2020, val loss: 0.953559935092926
Epoch 2030, training loss: 459.38543701171875 = 0.9485667943954468 + 50.0 * 9.168737411499023
Epoch 2030, val loss: 0.9525876045227051
Epoch 2040, training loss: 459.4812316894531 = 0.9475736021995544 + 50.0 * 9.170673370361328
Epoch 2040, val loss: 0.9516531229019165
Epoch 2050, training loss: 459.4172058105469 = 0.9465543031692505 + 50.0 * 9.169412612915039
Epoch 2050, val loss: 0.950653612613678
Epoch 2060, training loss: 459.18902587890625 = 0.9455381035804749 + 50.0 * 9.164870262145996
Epoch 2060, val loss: 0.9497029781341553
Epoch 2070, training loss: 459.29693603515625 = 0.9445399641990662 + 50.0 * 9.167047500610352
Epoch 2070, val loss: 0.9487637877464294
Epoch 2080, training loss: 459.6075439453125 = 0.9435462355613708 + 50.0 * 9.173279762268066
Epoch 2080, val loss: 0.9478304386138916
Epoch 2090, training loss: 459.6927795410156 = 0.9425504207611084 + 50.0 * 9.175004959106445
Epoch 2090, val loss: 0.9469029307365417
Epoch 2100, training loss: 459.895263671875 = 0.941550612449646 + 50.0 * 9.17907428741455
Epoch 2100, val loss: 0.9459695816040039
Epoch 2110, training loss: 459.9974060058594 = 0.9405519366264343 + 50.0 * 9.181137084960938
Epoch 2110, val loss: 0.945023238658905
Epoch 2120, training loss: 459.79595947265625 = 0.9395201802253723 + 50.0 * 9.177128791809082
Epoch 2120, val loss: 0.9440587759017944
Epoch 2130, training loss: 459.8065490722656 = 0.9385094046592712 + 50.0 * 9.177360534667969
Epoch 2130, val loss: 0.9431332945823669
Epoch 2140, training loss: 459.9262390136719 = 0.9375410079956055 + 50.0 * 9.179774284362793
Epoch 2140, val loss: 0.9422267079353333
Epoch 2150, training loss: 460.2264404296875 = 0.9365503191947937 + 50.0 * 9.185797691345215
Epoch 2150, val loss: 0.9413148760795593
Epoch 2160, training loss: 460.338134765625 = 0.9355818033218384 + 50.0 * 9.188051223754883
Epoch 2160, val loss: 0.9404152631759644
Epoch 2170, training loss: 460.0854797363281 = 0.9345888495445251 + 50.0 * 9.18301773071289
Epoch 2170, val loss: 0.9394888877868652
Epoch 2180, training loss: 460.2066345214844 = 0.9336147904396057 + 50.0 * 9.185460090637207
Epoch 2180, val loss: 0.9385891556739807
Epoch 2190, training loss: 460.2646484375 = 0.9326348304748535 + 50.0 * 9.186639785766602
Epoch 2190, val loss: 0.9376943707466125
Epoch 2200, training loss: 460.3968505859375 = 0.9316856861114502 + 50.0 * 9.189303398132324
Epoch 2200, val loss: 0.9368099570274353
Epoch 2210, training loss: 460.48681640625 = 0.9307188987731934 + 50.0 * 9.191122055053711
Epoch 2210, val loss: 0.9359211325645447
Epoch 2220, training loss: 460.70294189453125 = 0.929770827293396 + 50.0 * 9.195463180541992
Epoch 2220, val loss: 0.9350438714027405
Epoch 2230, training loss: 460.67376708984375 = 0.928808331489563 + 50.0 * 9.194899559020996
Epoch 2230, val loss: 0.9341577291488647
Epoch 2240, training loss: 460.4056701660156 = 0.9278295040130615 + 50.0 * 9.189557075500488
Epoch 2240, val loss: 0.9332598447799683
Epoch 2250, training loss: 460.6695556640625 = 0.9269030094146729 + 50.0 * 9.194852828979492
Epoch 2250, val loss: 0.9324210286140442
Epoch 2260, training loss: 460.6573181152344 = 0.9259652495384216 + 50.0 * 9.194626808166504
Epoch 2260, val loss: 0.9315623641014099
Epoch 2270, training loss: 460.64190673828125 = 0.9250309467315674 + 50.0 * 9.194337844848633
Epoch 2270, val loss: 0.9307147860527039
Epoch 2280, training loss: 460.7418518066406 = 0.9240970611572266 + 50.0 * 9.196354866027832
Epoch 2280, val loss: 0.9298771023750305
Epoch 2290, training loss: 460.9734802246094 = 0.9231758117675781 + 50.0 * 9.201005935668945
Epoch 2290, val loss: 0.929025411605835
Epoch 2300, training loss: 460.7680358886719 = 0.9222368001937866 + 50.0 * 9.196915626525879
Epoch 2300, val loss: 0.9281715750694275
Epoch 2310, training loss: 460.8810729980469 = 0.9213147163391113 + 50.0 * 9.19919490814209
Epoch 2310, val loss: 0.9273448586463928
Epoch 2320, training loss: 461.1210021972656 = 0.9204086065292358 + 50.0 * 9.204011917114258
Epoch 2320, val loss: 0.9265206456184387
Epoch 2330, training loss: 461.1076354980469 = 0.9194876551628113 + 50.0 * 9.203763008117676
Epoch 2330, val loss: 0.9256913065910339
Epoch 2340, training loss: 461.1874694824219 = 0.9185768961906433 + 50.0 * 9.205377578735352
Epoch 2340, val loss: 0.9248635768890381
Epoch 2350, training loss: 461.23980712890625 = 0.9176651239395142 + 50.0 * 9.206442832946777
Epoch 2350, val loss: 0.9240515232086182
Epoch 2360, training loss: 461.38006591796875 = 0.9167675971984863 + 50.0 * 9.20926570892334
Epoch 2360, val loss: 0.9232473373413086
Epoch 2370, training loss: 461.2337646484375 = 0.9158667922019958 + 50.0 * 9.206357955932617
Epoch 2370, val loss: 0.9224425554275513
Epoch 2380, training loss: 461.288818359375 = 0.9149768352508545 + 50.0 * 9.207476615905762
Epoch 2380, val loss: 0.9216330051422119
Epoch 2390, training loss: 461.3927917480469 = 0.9140975475311279 + 50.0 * 9.209573745727539
Epoch 2390, val loss: 0.920841634273529
Epoch 2400, training loss: 461.1837463378906 = 0.9132007360458374 + 50.0 * 9.205410957336426
Epoch 2400, val loss: 0.9200525283813477
Epoch 2410, training loss: 461.4381103515625 = 0.9123296141624451 + 50.0 * 9.210515975952148
Epoch 2410, val loss: 0.9192641973495483
Epoch 2420, training loss: 461.36444091796875 = 0.9114553332328796 + 50.0 * 9.209059715270996
Epoch 2420, val loss: 0.9184795618057251
Epoch 2430, training loss: 461.4545593261719 = 0.9105855822563171 + 50.0 * 9.2108793258667
Epoch 2430, val loss: 0.9177055358886719
Epoch 2440, training loss: 461.6877136230469 = 0.9097263216972351 + 50.0 * 9.215559959411621
Epoch 2440, val loss: 0.9169492721557617
Epoch 2450, training loss: 461.7522888183594 = 0.9088805913925171 + 50.0 * 9.21686840057373
Epoch 2450, val loss: 0.9162030816078186
Epoch 2460, training loss: 461.4643859863281 = 0.9080212116241455 + 50.0 * 9.211127281188965
Epoch 2460, val loss: 0.9154399037361145
Epoch 2470, training loss: 461.61181640625 = 0.9071887731552124 + 50.0 * 9.214092254638672
Epoch 2470, val loss: 0.9147005081176758
Epoch 2480, training loss: 461.8482971191406 = 0.906356930732727 + 50.0 * 9.218838691711426
Epoch 2480, val loss: 0.9139590859413147
Epoch 2490, training loss: 461.7118225097656 = 0.9054977893829346 + 50.0 * 9.216126441955566
Epoch 2490, val loss: 0.9131935834884644
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.5269565217391304
0.8149677606317468
=== training gcn model ===
Epoch 0, training loss: 515.6782836914062 = 1.0956145524978638 + 50.0 * 10.29165267944336
Epoch 0, val loss: 1.094897985458374
Epoch 10, training loss: 496.2195739746094 = 1.094951868057251 + 50.0 * 9.90249252319336
Epoch 10, val loss: 1.0942665338516235
Epoch 20, training loss: 487.13916015625 = 1.0944331884384155 + 50.0 * 9.720894813537598
Epoch 20, val loss: 1.0937539339065552
Epoch 30, training loss: 480.246826171875 = 1.0938150882720947 + 50.0 * 9.583060264587402
Epoch 30, val loss: 1.0931406021118164
Epoch 40, training loss: 474.5657958984375 = 1.0931898355484009 + 50.0 * 9.469451904296875
Epoch 40, val loss: 1.092523455619812
Epoch 50, training loss: 469.9111328125 = 1.0925482511520386 + 50.0 * 9.376371383666992
Epoch 50, val loss: 1.091882586479187
Epoch 60, training loss: 466.0082092285156 = 1.0918852090835571 + 50.0 * 9.29832649230957
Epoch 60, val loss: 1.0912281274795532
Epoch 70, training loss: 462.667236328125 = 1.091213345527649 + 50.0 * 9.231520652770996
Epoch 70, val loss: 1.0905605554580688
Epoch 80, training loss: 459.7463073730469 = 1.0905137062072754 + 50.0 * 9.173115730285645
Epoch 80, val loss: 1.0898653268814087
Epoch 90, training loss: 457.250244140625 = 1.0897935628890991 + 50.0 * 9.123208999633789
Epoch 90, val loss: 1.0891555547714233
Epoch 100, training loss: 455.0118408203125 = 1.0890599489212036 + 50.0 * 9.078455924987793
Epoch 100, val loss: 1.0884212255477905
Epoch 110, training loss: 453.11572265625 = 1.0883145332336426 + 50.0 * 9.040548324584961
Epoch 110, val loss: 1.0876822471618652
Epoch 120, training loss: 451.4681091308594 = 1.0875455141067505 + 50.0 * 9.007611274719238
Epoch 120, val loss: 1.0869227647781372
Epoch 130, training loss: 449.9825134277344 = 1.0868167877197266 + 50.0 * 8.977913856506348
Epoch 130, val loss: 1.0861965417861938
Epoch 140, training loss: 448.69439697265625 = 1.086115837097168 + 50.0 * 8.952165603637695
Epoch 140, val loss: 1.0855106115341187
Epoch 150, training loss: 447.57733154296875 = 1.0854943990707397 + 50.0 * 8.929837226867676
Epoch 150, val loss: 1.084913730621338
Epoch 160, training loss: 446.6762390136719 = 1.084938883781433 + 50.0 * 8.911826133728027
Epoch 160, val loss: 1.0843764543533325
Epoch 170, training loss: 445.7625427246094 = 1.084433674812317 + 50.0 * 8.893562316894531
Epoch 170, val loss: 1.0838747024536133
Epoch 180, training loss: 444.9503173828125 = 1.0839192867279053 + 50.0 * 8.877327919006348
Epoch 180, val loss: 1.0833678245544434
Epoch 190, training loss: 444.29339599609375 = 1.0833826065063477 + 50.0 * 8.864200592041016
Epoch 190, val loss: 1.0828415155410767
Epoch 200, training loss: 443.6544189453125 = 1.0828388929367065 + 50.0 * 8.851431846618652
Epoch 200, val loss: 1.0823068618774414
Epoch 210, training loss: 443.1354064941406 = 1.0822759866714478 + 50.0 * 8.841062545776367
Epoch 210, val loss: 1.0817521810531616
Epoch 220, training loss: 442.64691162109375 = 1.0817033052444458 + 50.0 * 8.831304550170898
Epoch 220, val loss: 1.081188678741455
Epoch 230, training loss: 442.4134521484375 = 1.0811246633529663 + 50.0 * 8.82664680480957
Epoch 230, val loss: 1.080606460571289
Epoch 240, training loss: 441.99676513671875 = 1.08051335811615 + 50.0 * 8.81832504272461
Epoch 240, val loss: 1.0799938440322876
Epoch 250, training loss: 441.6054382324219 = 1.0798897743225098 + 50.0 * 8.810510635375977
Epoch 250, val loss: 1.0793768167495728
Epoch 260, training loss: 441.4757080078125 = 1.0792564153671265 + 50.0 * 8.807929039001465
Epoch 260, val loss: 1.0787546634674072
Epoch 270, training loss: 441.3544921875 = 1.0785884857177734 + 50.0 * 8.80551815032959
Epoch 270, val loss: 1.0781042575836182
Epoch 280, training loss: 441.103759765625 = 1.0779298543930054 + 50.0 * 8.800516128540039
Epoch 280, val loss: 1.077449917793274
Epoch 290, training loss: 441.2057189941406 = 1.0772145986557007 + 50.0 * 8.802570343017578
Epoch 290, val loss: 1.0767422914505005
Epoch 300, training loss: 440.8757629394531 = 1.0765063762664795 + 50.0 * 8.795985221862793
Epoch 300, val loss: 1.0760512351989746
Epoch 310, training loss: 441.4300842285156 = 1.075786828994751 + 50.0 * 8.807085990905762
Epoch 310, val loss: 1.0753355026245117
Epoch 320, training loss: 440.6809997558594 = 1.0750330686569214 + 50.0 * 8.792119026184082
Epoch 320, val loss: 1.0745775699615479
Epoch 330, training loss: 440.68701171875 = 1.074243187904358 + 50.0 * 8.792255401611328
Epoch 330, val loss: 1.0737993717193604
Epoch 340, training loss: 441.1260070800781 = 1.0731266736984253 + 50.0 * 8.801057815551758
Epoch 340, val loss: 1.0726854801177979
Epoch 350, training loss: 440.6942138671875 = 1.0726217031478882 + 50.0 * 8.792431831359863
Epoch 350, val loss: 1.072243094444275
Epoch 360, training loss: 440.8880310058594 = 1.0718042850494385 + 50.0 * 8.796324729919434
Epoch 360, val loss: 1.071372628211975
Epoch 370, training loss: 440.2372741699219 = 1.0709342956542969 + 50.0 * 8.783327102661133
Epoch 370, val loss: 1.0705292224884033
Epoch 380, training loss: 440.19842529296875 = 1.0700569152832031 + 50.0 * 8.782567024230957
Epoch 380, val loss: 1.069652795791626
Epoch 390, training loss: 440.480224609375 = 1.0692397356033325 + 50.0 * 8.788219451904297
Epoch 390, val loss: 1.0688419342041016
Epoch 400, training loss: 440.25689697265625 = 1.0683544874191284 + 50.0 * 8.783770561218262
Epoch 400, val loss: 1.0679669380187988
Epoch 410, training loss: 440.4585266113281 = 1.067446231842041 + 50.0 * 8.787821769714355
Epoch 410, val loss: 1.0670650005340576
Epoch 420, training loss: 440.6560363769531 = 1.066536784172058 + 50.0 * 8.791790008544922
Epoch 420, val loss: 1.0661540031433105
Epoch 430, training loss: 440.6194152832031 = 1.0655771493911743 + 50.0 * 8.79107666015625
Epoch 430, val loss: 1.0652176141738892
Epoch 440, training loss: 440.75897216796875 = 1.0646017789840698 + 50.0 * 8.7938871383667
Epoch 440, val loss: 1.0642406940460205
Epoch 450, training loss: 440.6915283203125 = 1.0636091232299805 + 50.0 * 8.792558670043945
Epoch 450, val loss: 1.0632696151733398
Epoch 460, training loss: 440.7988586425781 = 1.0626063346862793 + 50.0 * 8.79472541809082
Epoch 460, val loss: 1.0622797012329102
Epoch 470, training loss: 440.76593017578125 = 1.0616059303283691 + 50.0 * 8.794086456298828
Epoch 470, val loss: 1.0612832307815552
Epoch 480, training loss: 440.88519287109375 = 1.0605883598327637 + 50.0 * 8.796492576599121
Epoch 480, val loss: 1.060273289680481
Epoch 490, training loss: 441.2038269042969 = 1.0595767498016357 + 50.0 * 8.802885055541992
Epoch 490, val loss: 1.0592554807662964
Epoch 500, training loss: 441.9173889160156 = 1.058430790901184 + 50.0 * 8.817178726196289
Epoch 500, val loss: 1.058086633682251
Epoch 510, training loss: 440.843994140625 = 1.0572974681854248 + 50.0 * 8.795733451843262
Epoch 510, val loss: 1.0570496320724487
Epoch 520, training loss: 440.85736083984375 = 1.0563470125198364 + 50.0 * 8.7960205078125
Epoch 520, val loss: 1.0560474395751953
Epoch 530, training loss: 440.7974853515625 = 1.055304765701294 + 50.0 * 8.794843673706055
Epoch 530, val loss: 1.0550415515899658
Epoch 540, training loss: 440.44830322265625 = 1.0541846752166748 + 50.0 * 8.787881851196289
Epoch 540, val loss: 1.0539238452911377
Epoch 550, training loss: 441.0242004394531 = 1.0531433820724487 + 50.0 * 8.799421310424805
Epoch 550, val loss: 1.0528931617736816
Epoch 560, training loss: 440.81561279296875 = 1.0520364046096802 + 50.0 * 8.795271873474121
Epoch 560, val loss: 1.0517958402633667
Epoch 570, training loss: 440.8873596191406 = 1.0508838891983032 + 50.0 * 8.796730041503906
Epoch 570, val loss: 1.0506646633148193
Epoch 580, training loss: 441.3039855957031 = 1.0497686862945557 + 50.0 * 8.805084228515625
Epoch 580, val loss: 1.049550175666809
Epoch 590, training loss: 441.5231628417969 = 1.0486499071121216 + 50.0 * 8.809490203857422
Epoch 590, val loss: 1.0484424829483032
Epoch 600, training loss: 441.6270751953125 = 1.0474858283996582 + 50.0 * 8.811592102050781
Epoch 600, val loss: 1.0472880601882935
Epoch 610, training loss: 441.64715576171875 = 1.0463228225708008 + 50.0 * 8.812016487121582
Epoch 610, val loss: 1.0461435317993164
Epoch 620, training loss: 441.6832275390625 = 1.0451507568359375 + 50.0 * 8.812761306762695
Epoch 620, val loss: 1.0449825525283813
Epoch 630, training loss: 441.9069519042969 = 1.04396653175354 + 50.0 * 8.817259788513184
Epoch 630, val loss: 1.0438097715377808
Epoch 640, training loss: 441.9022216796875 = 1.0427576303482056 + 50.0 * 8.81718921661377
Epoch 640, val loss: 1.0426262617111206
Epoch 650, training loss: 442.02301025390625 = 1.041579008102417 + 50.0 * 8.819628715515137
Epoch 650, val loss: 1.0414499044418335
Epoch 660, training loss: 442.2104187011719 = 1.0403528213500977 + 50.0 * 8.82340145111084
Epoch 660, val loss: 1.0402377843856812
Epoch 670, training loss: 442.131591796875 = 1.039102554321289 + 50.0 * 8.821849822998047
Epoch 670, val loss: 1.0389915704727173
Epoch 680, training loss: 442.3995361328125 = 1.0378822088241577 + 50.0 * 8.82723331451416
Epoch 680, val loss: 1.0377836227416992
Epoch 690, training loss: 442.24847412109375 = 1.0366095304489136 + 50.0 * 8.824236869812012
Epoch 690, val loss: 1.036525011062622
Epoch 700, training loss: 442.2364196777344 = 1.0353379249572754 + 50.0 * 8.824021339416504
Epoch 700, val loss: 1.0352786779403687
Epoch 710, training loss: 441.76904296875 = 1.0339860916137695 + 50.0 * 8.814701080322266
Epoch 710, val loss: 1.033949375152588
Epoch 720, training loss: 443.1650695800781 = 1.032880187034607 + 50.0 * 8.842643737792969
Epoch 720, val loss: 1.0327770709991455
Epoch 730, training loss: 442.55377197265625 = 1.0315660238265991 + 50.0 * 8.8304443359375
Epoch 730, val loss: 1.0315663814544678
Epoch 740, training loss: 442.5802307128906 = 1.0302700996398926 + 50.0 * 8.830999374389648
Epoch 740, val loss: 1.0302494764328003
Epoch 750, training loss: 442.63836669921875 = 1.0290114879608154 + 50.0 * 8.832186698913574
Epoch 750, val loss: 1.028997778892517
Epoch 760, training loss: 442.8386535644531 = 1.0276976823806763 + 50.0 * 8.83621883392334
Epoch 760, val loss: 1.0277129411697388
Epoch 770, training loss: 443.07635498046875 = 1.0263702869415283 + 50.0 * 8.840999603271484
Epoch 770, val loss: 1.0263882875442505
Epoch 780, training loss: 443.1786804199219 = 1.0250338315963745 + 50.0 * 8.843072891235352
Epoch 780, val loss: 1.025055170059204
Epoch 790, training loss: 443.1912841796875 = 1.0236765146255493 + 50.0 * 8.843352317810059
Epoch 790, val loss: 1.0237295627593994
Epoch 800, training loss: 443.2933654785156 = 1.0223051309585571 + 50.0 * 8.845420837402344
Epoch 800, val loss: 1.0223733186721802
Epoch 810, training loss: 443.6009216308594 = 1.020926594734192 + 50.0 * 8.85159969329834
Epoch 810, val loss: 1.0209991931915283
Epoch 820, training loss: 443.7321472167969 = 1.019553542137146 + 50.0 * 8.854251861572266
Epoch 820, val loss: 1.0196179151535034
Epoch 830, training loss: 443.8551025390625 = 1.018143653869629 + 50.0 * 8.856739044189453
Epoch 830, val loss: 1.0182313919067383
Epoch 840, training loss: 443.6760559082031 = 1.0167350769042969 + 50.0 * 8.85318660736084
Epoch 840, val loss: 1.0168193578720093
Epoch 850, training loss: 444.00018310546875 = 1.0153217315673828 + 50.0 * 8.859697341918945
Epoch 850, val loss: 1.0154329538345337
Epoch 860, training loss: 444.0632629394531 = 1.01387619972229 + 50.0 * 8.860987663269043
Epoch 860, val loss: 1.0139977931976318
Epoch 870, training loss: 444.1501159667969 = 1.0124269723892212 + 50.0 * 8.862753868103027
Epoch 870, val loss: 1.0125709772109985
Epoch 880, training loss: 444.2315368652344 = 1.0109586715698242 + 50.0 * 8.864411354064941
Epoch 880, val loss: 1.0110803842544556
Epoch 890, training loss: 444.0021057128906 = 1.0094493627548218 + 50.0 * 8.85985279083252
Epoch 890, val loss: 1.0095791816711426
Epoch 900, training loss: 444.1716613769531 = 1.0079749822616577 + 50.0 * 8.863273620605469
Epoch 900, val loss: 1.0081201791763306
Epoch 910, training loss: 444.43798828125 = 1.0065207481384277 + 50.0 * 8.868629455566406
Epoch 910, val loss: 1.0066897869110107
Epoch 920, training loss: 444.5608215332031 = 1.004988431930542 + 50.0 * 8.871116638183594
Epoch 920, val loss: 1.005168080329895
Epoch 930, training loss: 444.4992980957031 = 1.0035181045532227 + 50.0 * 8.869915962219238
Epoch 930, val loss: 1.003702163696289
Epoch 940, training loss: 444.6794738769531 = 1.002000331878662 + 50.0 * 8.873549461364746
Epoch 940, val loss: 1.0022016763687134
Epoch 950, training loss: 444.8570556640625 = 1.000474214553833 + 50.0 * 8.877131462097168
Epoch 950, val loss: 1.0006837844848633
Epoch 960, training loss: 444.8172302246094 = 0.9989216923713684 + 50.0 * 8.876365661621094
Epoch 960, val loss: 0.9991344213485718
Epoch 970, training loss: 444.8466796875 = 0.9973397254943848 + 50.0 * 8.876986503601074
Epoch 970, val loss: 0.9975559115409851
Epoch 980, training loss: 444.9930725097656 = 0.99581378698349 + 50.0 * 8.879944801330566
Epoch 980, val loss: 0.996036171913147
Epoch 990, training loss: 445.23370361328125 = 0.9942457675933838 + 50.0 * 8.88478946685791
Epoch 990, val loss: 0.9944732785224915
Epoch 1000, training loss: 445.28985595703125 = 0.9926474094390869 + 50.0 * 8.885944366455078
Epoch 1000, val loss: 0.9928857088088989
Epoch 1010, training loss: 445.3222961425781 = 0.9910194277763367 + 50.0 * 8.886625289916992
Epoch 1010, val loss: 0.9912785291671753
Epoch 1020, training loss: 445.3994445800781 = 0.989424467086792 + 50.0 * 8.888200759887695
Epoch 1020, val loss: 0.9896875023841858
Epoch 1030, training loss: 445.2019958496094 = 0.9877601265907288 + 50.0 * 8.884284973144531
Epoch 1030, val loss: 0.9880120158195496
Epoch 1040, training loss: 445.3546142578125 = 0.9861129522323608 + 50.0 * 8.887370109558105
Epoch 1040, val loss: 0.9863611459732056
Epoch 1050, training loss: 445.57684326171875 = 0.9844963550567627 + 50.0 * 8.891846656799316
Epoch 1050, val loss: 0.9847515821456909
Epoch 1060, training loss: 445.6966857910156 = 0.9828134179115295 + 50.0 * 8.894277572631836
Epoch 1060, val loss: 0.9830746650695801
Epoch 1070, training loss: 445.743408203125 = 0.9810921549797058 + 50.0 * 8.895246505737305
Epoch 1070, val loss: 0.9813753962516785
Epoch 1080, training loss: 445.7525939941406 = 0.9793530702590942 + 50.0 * 8.895464897155762
Epoch 1080, val loss: 0.9796280264854431
Epoch 1090, training loss: 445.9324645996094 = 0.9776236414909363 + 50.0 * 8.899096488952637
Epoch 1090, val loss: 0.9778793454170227
Epoch 1100, training loss: 446.0591125488281 = 0.9758567810058594 + 50.0 * 8.901664733886719
Epoch 1100, val loss: 0.9761183261871338
Epoch 1110, training loss: 446.2264404296875 = 0.9740201234817505 + 50.0 * 8.905048370361328
Epoch 1110, val loss: 0.9742828607559204
Epoch 1120, training loss: 445.9808044433594 = 0.972128689289093 + 50.0 * 8.90017318725586
Epoch 1120, val loss: 0.9724499583244324
Epoch 1130, training loss: 446.20001220703125 = 0.9702349305152893 + 50.0 * 8.904595375061035
Epoch 1130, val loss: 0.970513641834259
Epoch 1140, training loss: 446.62628173828125 = 0.9685209393501282 + 50.0 * 8.913155555725098
Epoch 1140, val loss: 0.9687736630439758
Epoch 1150, training loss: 445.8526306152344 = 0.9665214419364929 + 50.0 * 8.897722244262695
Epoch 1150, val loss: 0.9667601585388184
Epoch 1160, training loss: 446.6324157714844 = 0.9646445512771606 + 50.0 * 8.913355827331543
Epoch 1160, val loss: 0.9648528099060059
Epoch 1170, training loss: 446.6698303222656 = 0.9627066850662231 + 50.0 * 8.914142608642578
Epoch 1170, val loss: 0.9629844427108765
Epoch 1180, training loss: 446.7314147949219 = 0.9607743620872498 + 50.0 * 8.915412902832031
Epoch 1180, val loss: 0.961047887802124
Epoch 1190, training loss: 446.8338317871094 = 0.9588480591773987 + 50.0 * 8.917499542236328
Epoch 1190, val loss: 0.9591150879859924
Epoch 1200, training loss: 446.9952697753906 = 0.9569427967071533 + 50.0 * 8.920766830444336
Epoch 1200, val loss: 0.9572224617004395
Epoch 1210, training loss: 447.1729736328125 = 0.9550068974494934 + 50.0 * 8.924359321594238
Epoch 1210, val loss: 0.9552502036094666
Epoch 1220, training loss: 446.5558166503906 = 0.9528753757476807 + 50.0 * 8.91205883026123
Epoch 1220, val loss: 0.9531553387641907
Epoch 1230, training loss: 447.0990905761719 = 0.9509283900260925 + 50.0 * 8.92296314239502
Epoch 1230, val loss: 0.9512123465538025
Epoch 1240, training loss: 447.3128662109375 = 0.9489462971687317 + 50.0 * 8.927278518676758
Epoch 1240, val loss: 0.9492334723472595
Epoch 1250, training loss: 447.43951416015625 = 0.9469160437583923 + 50.0 * 8.929851531982422
Epoch 1250, val loss: 0.9472147226333618
Epoch 1260, training loss: 447.4706115722656 = 0.9448769688606262 + 50.0 * 8.930514335632324
Epoch 1260, val loss: 0.9451889991760254
Epoch 1270, training loss: 447.5262451171875 = 0.9428462982177734 + 50.0 * 8.931668281555176
Epoch 1270, val loss: 0.9431473016738892
Epoch 1280, training loss: 447.2615051269531 = 0.9407497048377991 + 50.0 * 8.92641544342041
Epoch 1280, val loss: 0.9410538673400879
Epoch 1290, training loss: 447.1053161621094 = 0.9385931491851807 + 50.0 * 8.923334121704102
Epoch 1290, val loss: 0.9389038681983948
Epoch 1300, training loss: 447.3958740234375 = 0.9365769624710083 + 50.0 * 8.92918586730957
Epoch 1300, val loss: 0.9369012117385864
Epoch 1310, training loss: 447.59320068359375 = 0.9345032572746277 + 50.0 * 8.933174133300781
Epoch 1310, val loss: 0.9348082542419434
Epoch 1320, training loss: 447.9571533203125 = 0.9324395060539246 + 50.0 * 8.940494537353516
Epoch 1320, val loss: 0.9327694773674011
Epoch 1330, training loss: 448.3235168457031 = 0.930332601070404 + 50.0 * 8.947863578796387
Epoch 1330, val loss: 0.9306573867797852
Epoch 1340, training loss: 448.3475646972656 = 0.9281958341598511 + 50.0 * 8.948387145996094
Epoch 1340, val loss: 0.9285351037979126
Epoch 1350, training loss: 448.121826171875 = 0.9259538650512695 + 50.0 * 8.943917274475098
Epoch 1350, val loss: 0.9263138771057129
Epoch 1360, training loss: 448.1293029785156 = 0.9237795472145081 + 50.0 * 8.944110870361328
Epoch 1360, val loss: 0.9241663217544556
Epoch 1370, training loss: 448.423095703125 = 0.9216356873512268 + 50.0 * 8.950029373168945
Epoch 1370, val loss: 0.922027051448822
Epoch 1380, training loss: 448.6543884277344 = 0.9194772839546204 + 50.0 * 8.95469856262207
Epoch 1380, val loss: 0.9198472499847412
Epoch 1390, training loss: 448.77294921875 = 0.9172839522361755 + 50.0 * 8.957113265991211
Epoch 1390, val loss: 0.9176769852638245
Epoch 1400, training loss: 448.86566162109375 = 0.9150753021240234 + 50.0 * 8.959012031555176
Epoch 1400, val loss: 0.9154717922210693
Epoch 1410, training loss: 448.6924743652344 = 0.912786066532135 + 50.0 * 8.955594062805176
Epoch 1410, val loss: 0.9132232069969177
Epoch 1420, training loss: 448.93499755859375 = 0.9105938673019409 + 50.0 * 8.960488319396973
Epoch 1420, val loss: 0.9110342860221863
Epoch 1430, training loss: 449.2414245605469 = 0.9083528518676758 + 50.0 * 8.96666145324707
Epoch 1430, val loss: 0.9088186025619507
Epoch 1440, training loss: 449.4865417480469 = 0.9059722423553467 + 50.0 * 8.971611022949219
Epoch 1440, val loss: 0.9063801169395447
Epoch 1450, training loss: 449.0354919433594 = 0.9037507176399231 + 50.0 * 8.962635040283203
Epoch 1450, val loss: 0.9042165875434875
Epoch 1460, training loss: 448.6000061035156 = 0.9014596343040466 + 50.0 * 8.953970909118652
Epoch 1460, val loss: 0.901962399482727
Epoch 1470, training loss: 449.0002746582031 = 0.8991565108299255 + 50.0 * 8.962021827697754
Epoch 1470, val loss: 0.8997095823287964
Epoch 1480, training loss: 449.3824157714844 = 0.8969429135322571 + 50.0 * 8.969709396362305
Epoch 1480, val loss: 0.8975363969802856
Epoch 1490, training loss: 449.55792236328125 = 0.8946431279182434 + 50.0 * 8.973265647888184
Epoch 1490, val loss: 0.8952611088752747
Epoch 1500, training loss: 449.6169738769531 = 0.8923527002334595 + 50.0 * 8.974492073059082
Epoch 1500, val loss: 0.8929883241653442
Epoch 1510, training loss: 449.5549621582031 = 0.8900220394134521 + 50.0 * 8.973299026489258
Epoch 1510, val loss: 0.8907200694084167
Epoch 1520, training loss: 449.6655578613281 = 0.8877278566360474 + 50.0 * 8.975556373596191
Epoch 1520, val loss: 0.8884398937225342
Epoch 1530, training loss: 449.742919921875 = 0.8854367733001709 + 50.0 * 8.977149963378906
Epoch 1530, val loss: 0.8861397504806519
Epoch 1540, training loss: 450.0393981933594 = 0.8831093311309814 + 50.0 * 8.983125686645508
Epoch 1540, val loss: 0.8838630318641663
Epoch 1550, training loss: 449.9354248046875 = 0.880778431892395 + 50.0 * 8.981093406677246
Epoch 1550, val loss: 0.881567120552063
Epoch 1560, training loss: 450.164794921875 = 0.878487765789032 + 50.0 * 8.985726356506348
Epoch 1560, val loss: 0.8793213367462158
Epoch 1570, training loss: 450.2733459472656 = 0.8761472702026367 + 50.0 * 8.987943649291992
Epoch 1570, val loss: 0.877021849155426
Epoch 1580, training loss: 450.27093505859375 = 0.873851478099823 + 50.0 * 8.98794174194336
Epoch 1580, val loss: 0.8747310638427734
Epoch 1590, training loss: 450.1608581542969 = 0.8715575933456421 + 50.0 * 8.985786437988281
Epoch 1590, val loss: 0.8724770545959473
Epoch 1600, training loss: 450.3362121582031 = 0.8692569136619568 + 50.0 * 8.989338874816895
Epoch 1600, val loss: 0.8701902627944946
Epoch 1610, training loss: 450.34112548828125 = 0.8669342398643494 + 50.0 * 8.989483833312988
Epoch 1610, val loss: 0.8678960204124451
Epoch 1620, training loss: 450.3547058105469 = 0.8646103143692017 + 50.0 * 8.989801406860352
Epoch 1620, val loss: 0.8656078577041626
Epoch 1630, training loss: 450.6471862792969 = 0.8623207807540894 + 50.0 * 8.995697021484375
Epoch 1630, val loss: 0.8633517026901245
Epoch 1640, training loss: 450.64910888671875 = 0.8600311875343323 + 50.0 * 8.995780944824219
Epoch 1640, val loss: 0.8610804080963135
Epoch 1650, training loss: 450.6596984863281 = 0.8577260971069336 + 50.0 * 8.996039390563965
Epoch 1650, val loss: 0.8588194251060486
Epoch 1660, training loss: 450.6404113769531 = 0.8554003834724426 + 50.0 * 8.995699882507324
Epoch 1660, val loss: 0.8565375804901123
Epoch 1670, training loss: 450.729248046875 = 0.8530976176261902 + 50.0 * 8.997523307800293
Epoch 1670, val loss: 0.8542999625205994
Epoch 1680, training loss: 450.9861145019531 = 0.8508257865905762 + 50.0 * 9.002705574035645
Epoch 1680, val loss: 0.8521056771278381
Epoch 1690, training loss: 451.0158386230469 = 0.8486985564231873 + 50.0 * 9.003342628479004
Epoch 1690, val loss: 0.8499806523323059
Epoch 1700, training loss: 450.9294738769531 = 0.8464722633361816 + 50.0 * 9.001660346984863
Epoch 1700, val loss: 0.8477977514266968
Epoch 1710, training loss: 451.3089904785156 = 0.8442021012306213 + 50.0 * 9.009295463562012
Epoch 1710, val loss: 0.8455988764762878
Epoch 1720, training loss: 451.2425842285156 = 0.8418903350830078 + 50.0 * 9.008013725280762
Epoch 1720, val loss: 0.843332827091217
Epoch 1730, training loss: 450.9822998046875 = 0.8396252989768982 + 50.0 * 9.002853393554688
Epoch 1730, val loss: 0.8411085605621338
Epoch 1740, training loss: 450.89727783203125 = 0.8372752666473389 + 50.0 * 9.001199722290039
Epoch 1740, val loss: 0.8388200402259827
Epoch 1750, training loss: 447.16558837890625 = 0.8343695402145386 + 50.0 * 8.926624298095703
Epoch 1750, val loss: 0.8359697461128235
Epoch 1760, training loss: 449.7981262207031 = 0.8325269222259521 + 50.0 * 8.9793119430542
Epoch 1760, val loss: 0.8341789841651917
Epoch 1770, training loss: 449.2782897949219 = 0.8302781581878662 + 50.0 * 8.96895980834961
Epoch 1770, val loss: 0.8320109844207764
Epoch 1780, training loss: 449.67108154296875 = 0.8281782865524292 + 50.0 * 8.976858139038086
Epoch 1780, val loss: 0.8298856616020203
Epoch 1790, training loss: 448.89996337890625 = 0.8259497880935669 + 50.0 * 8.961480140686035
Epoch 1790, val loss: 0.8277761340141296
Epoch 1800, training loss: 449.57452392578125 = 0.8237292170524597 + 50.0 * 8.975015640258789
Epoch 1800, val loss: 0.8255901336669922
Epoch 1810, training loss: 449.5262145996094 = 0.8215168714523315 + 50.0 * 8.97409439086914
Epoch 1810, val loss: 0.8234350085258484
Epoch 1820, training loss: 449.9263000488281 = 0.8193132877349854 + 50.0 * 8.982139587402344
Epoch 1820, val loss: 0.8212843537330627
Epoch 1830, training loss: 450.4708557128906 = 0.817136824131012 + 50.0 * 8.993074417114258
Epoch 1830, val loss: 0.8191774487495422
Epoch 1840, training loss: 450.858642578125 = 0.8149605393409729 + 50.0 * 9.000873565673828
Epoch 1840, val loss: 0.8170599937438965
Epoch 1850, training loss: 450.76953125 = 0.8127339482307434 + 50.0 * 8.999135971069336
Epoch 1850, val loss: 0.8149164915084839
Epoch 1860, training loss: 450.8173522949219 = 0.8105589747428894 + 50.0 * 9.000136375427246
Epoch 1860, val loss: 0.8127897381782532
Epoch 1870, training loss: 451.27880859375 = 0.8083964586257935 + 50.0 * 9.009407997131348
Epoch 1870, val loss: 0.810703456401825
Epoch 1880, training loss: 451.54388427734375 = 0.806217610836029 + 50.0 * 9.014753341674805
Epoch 1880, val loss: 0.8085864782333374
Epoch 1890, training loss: 451.6485290527344 = 0.8040489554405212 + 50.0 * 9.016889572143555
Epoch 1890, val loss: 0.8064818382263184
Epoch 1900, training loss: 451.5126647949219 = 0.8018558621406555 + 50.0 * 9.014216423034668
Epoch 1900, val loss: 0.8043883442878723
Epoch 1910, training loss: 451.5169372558594 = 0.7996863126754761 + 50.0 * 9.014345169067383
Epoch 1910, val loss: 0.8022796511650085
Epoch 1920, training loss: 451.73736572265625 = 0.7975177764892578 + 50.0 * 9.018796920776367
Epoch 1920, val loss: 0.8002092838287354
Epoch 1930, training loss: 451.82208251953125 = 0.7953529357910156 + 50.0 * 9.02053451538086
Epoch 1930, val loss: 0.7981353998184204
Epoch 1940, training loss: 451.9287109375 = 0.7931976318359375 + 50.0 * 9.022710800170898
Epoch 1940, val loss: 0.7960439920425415
Epoch 1950, training loss: 452.0101013183594 = 0.7910621166229248 + 50.0 * 9.024380683898926
Epoch 1950, val loss: 0.7939916849136353
Epoch 1960, training loss: 452.1957092285156 = 0.7889373898506165 + 50.0 * 9.028135299682617
Epoch 1960, val loss: 0.7919417023658752
Epoch 1970, training loss: 452.27166748046875 = 0.7868313789367676 + 50.0 * 9.029696464538574
Epoch 1970, val loss: 0.7899192571640015
Epoch 1980, training loss: 452.1200866699219 = 0.784709095954895 + 50.0 * 9.026707649230957
Epoch 1980, val loss: 0.7878556251525879
Epoch 1990, training loss: 451.9833984375 = 0.7826024889945984 + 50.0 * 9.024016380310059
Epoch 1990, val loss: 0.7858492732048035
Epoch 2000, training loss: 452.3114013671875 = 0.7805237770080566 + 50.0 * 9.030617713928223
Epoch 2000, val loss: 0.7838716506958008
Epoch 2010, training loss: 452.7111511230469 = 0.7784550786018372 + 50.0 * 9.038654327392578
Epoch 2010, val loss: 0.7819105386734009
Epoch 2020, training loss: 452.5595397949219 = 0.7763545513153076 + 50.0 * 9.035663604736328
Epoch 2020, val loss: 0.7798910737037659
Epoch 2030, training loss: 452.45867919921875 = 0.7742940187454224 + 50.0 * 9.033687591552734
Epoch 2030, val loss: 0.7779365181922913
Epoch 2040, training loss: 452.69482421875 = 0.7722743153572083 + 50.0 * 9.038451194763184
Epoch 2040, val loss: 0.7759889364242554
Epoch 2050, training loss: 452.8493957519531 = 0.7702142596244812 + 50.0 * 9.041584014892578
Epoch 2050, val loss: 0.7740445137023926
Epoch 2060, training loss: 452.773681640625 = 0.7681835293769836 + 50.0 * 9.040109634399414
Epoch 2060, val loss: 0.7721191048622131
Epoch 2070, training loss: 452.9696960449219 = 0.7661626935005188 + 50.0 * 9.04407024383545
Epoch 2070, val loss: 0.7701786160469055
Epoch 2080, training loss: 452.6569519042969 = 0.764101505279541 + 50.0 * 9.037857055664062
Epoch 2080, val loss: 0.7682294249534607
Epoch 2090, training loss: 452.6884460449219 = 0.7621069550514221 + 50.0 * 9.03852653503418
Epoch 2090, val loss: 0.766330361366272
Epoch 2100, training loss: 452.7538146972656 = 0.7601603865623474 + 50.0 * 9.039873123168945
Epoch 2100, val loss: 0.7644951343536377
Epoch 2110, training loss: 452.8635559082031 = 0.7581276893615723 + 50.0 * 9.042108535766602
Epoch 2110, val loss: 0.7625899314880371
Epoch 2120, training loss: 453.1338806152344 = 0.7562013268470764 + 50.0 * 9.047554016113281
Epoch 2120, val loss: 0.7607543468475342
Epoch 2130, training loss: 453.4261169433594 = 0.7542305588722229 + 50.0 * 9.053437232971191
Epoch 2130, val loss: 0.7589018940925598
Epoch 2140, training loss: 453.0959167480469 = 0.7522305250167847 + 50.0 * 9.046874046325684
Epoch 2140, val loss: 0.7570476531982422
Epoch 2150, training loss: 453.2635498046875 = 0.7503257989883423 + 50.0 * 9.050264358520508
Epoch 2150, val loss: 0.7552606463432312
Epoch 2160, training loss: 453.4482727050781 = 0.7484103441238403 + 50.0 * 9.053997039794922
Epoch 2160, val loss: 0.7534258365631104
Epoch 2170, training loss: 453.3830261230469 = 0.7464783191680908 + 50.0 * 9.052730560302734
Epoch 2170, val loss: 0.7516549825668335
Epoch 2180, training loss: 453.4869384765625 = 0.7445669174194336 + 50.0 * 9.054847717285156
Epoch 2180, val loss: 0.749881386756897
Epoch 2190, training loss: 453.4923400878906 = 0.7426522374153137 + 50.0 * 9.054993629455566
Epoch 2190, val loss: 0.7480873465538025
Epoch 2200, training loss: 453.5645446777344 = 0.740745484828949 + 50.0 * 9.056475639343262
Epoch 2200, val loss: 0.7463197708129883
Epoch 2210, training loss: 453.75341796875 = 0.7388771772384644 + 50.0 * 9.060290336608887
Epoch 2210, val loss: 0.7445970177650452
Epoch 2220, training loss: 453.6226501464844 = 0.736963152885437 + 50.0 * 9.057713508605957
Epoch 2220, val loss: 0.7428359389305115
Epoch 2230, training loss: 453.18603515625 = 0.7351526021957397 + 50.0 * 9.049017906188965
Epoch 2230, val loss: 0.7411483526229858
Epoch 2240, training loss: 453.48089599609375 = 0.7333744168281555 + 50.0 * 9.054950714111328
Epoch 2240, val loss: 0.7394819259643555
Epoch 2250, training loss: 453.2896728515625 = 0.7315229177474976 + 50.0 * 9.051162719726562
Epoch 2250, val loss: 0.737807035446167
Epoch 2260, training loss: 453.6102294921875 = 0.7297065258026123 + 50.0 * 9.057610511779785
Epoch 2260, val loss: 0.7361574769020081
Epoch 2270, training loss: 453.891845703125 = 0.7279314398765564 + 50.0 * 9.063278198242188
Epoch 2270, val loss: 0.7345203161239624
Epoch 2280, training loss: 454.0113220214844 = 0.726118266582489 + 50.0 * 9.065704345703125
Epoch 2280, val loss: 0.7328667044639587
Epoch 2290, training loss: 453.80352783203125 = 0.724309504032135 + 50.0 * 9.06158447265625
Epoch 2290, val loss: 0.7312076091766357
Epoch 2300, training loss: 453.9450378417969 = 0.7225197553634644 + 50.0 * 9.06445026397705
Epoch 2300, val loss: 0.7295830845832825
Epoch 2310, training loss: 454.21221923828125 = 0.7207548022270203 + 50.0 * 9.069828987121582
Epoch 2310, val loss: 0.7279843688011169
Epoch 2320, training loss: 453.53961181640625 = 0.7189233303070068 + 50.0 * 9.056413650512695
Epoch 2320, val loss: 0.7263288497924805
Epoch 2330, training loss: 453.74310302734375 = 0.7172091603279114 + 50.0 * 9.060517311096191
Epoch 2330, val loss: 0.7247846126556396
Epoch 2340, training loss: 454.1634826660156 = 0.7155147790908813 + 50.0 * 9.06895923614502
Epoch 2340, val loss: 0.7232348918914795
Epoch 2350, training loss: 454.3190002441406 = 0.7138216495513916 + 50.0 * 9.072103500366211
Epoch 2350, val loss: 0.721704363822937
Epoch 2360, training loss: 454.41796875 = 0.712088942527771 + 50.0 * 9.074117660522461
Epoch 2360, val loss: 0.7201658487319946
Epoch 2370, training loss: 454.4608154296875 = 0.7104007601737976 + 50.0 * 9.075008392333984
Epoch 2370, val loss: 0.7186262607574463
Epoch 2380, training loss: 454.5271301269531 = 0.7087163329124451 + 50.0 * 9.07636833190918
Epoch 2380, val loss: 0.7171347141265869
Epoch 2390, training loss: 454.5780334472656 = 0.7070425748825073 + 50.0 * 9.077420234680176
Epoch 2390, val loss: 0.7156237959861755
Epoch 2400, training loss: 454.56744384765625 = 0.7053579092025757 + 50.0 * 9.077241897583008
Epoch 2400, val loss: 0.7141344547271729
Epoch 2410, training loss: 454.5379333496094 = 0.7037264704704285 + 50.0 * 9.07668399810791
Epoch 2410, val loss: 0.7126789093017578
Epoch 2420, training loss: 454.9541015625 = 0.7020976543426514 + 50.0 * 9.085040092468262
Epoch 2420, val loss: 0.7112370729446411
Epoch 2430, training loss: 454.6207275390625 = 0.700466513633728 + 50.0 * 9.078405380249023
Epoch 2430, val loss: 0.709803581237793
Epoch 2440, training loss: 454.6396179199219 = 0.698851466178894 + 50.0 * 9.078815460205078
Epoch 2440, val loss: 0.7084029912948608
Epoch 2450, training loss: 454.67584228515625 = 0.6972681283950806 + 50.0 * 9.079571723937988
Epoch 2450, val loss: 0.7069993019104004
Epoch 2460, training loss: 454.8363952636719 = 0.695686399936676 + 50.0 * 9.08281421661377
Epoch 2460, val loss: 0.7056215405464172
Epoch 2470, training loss: 454.9939880371094 = 0.6941283345222473 + 50.0 * 9.085997581481934
Epoch 2470, val loss: 0.7042475342750549
Epoch 2480, training loss: 455.1096496582031 = 0.6925634145736694 + 50.0 * 9.08834171295166
Epoch 2480, val loss: 0.7028710842132568
Epoch 2490, training loss: 454.3804626464844 = 0.6909903883934021 + 50.0 * 9.073789596557617
Epoch 2490, val loss: 0.7014731764793396
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7469565217391304
0.8133014562051729
The final CL Acc:0.60430, 0.10099, The final GNN Acc:0.81393, 0.00074
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110624])
remove edge: torch.Size([2, 66562])
updated graph: torch.Size([2, 88538])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 510.869384765625 = 1.09575617313385 + 50.0 * 10.195472717285156
Epoch 0, val loss: 1.095159649848938
Epoch 10, training loss: 488.9246826171875 = 1.0956050157546997 + 50.0 * 9.75658130645752
Epoch 10, val loss: 1.0949907302856445
Epoch 20, training loss: 478.7106628417969 = 1.095545768737793 + 50.0 * 9.552302360534668
Epoch 20, val loss: 1.0948964357376099
Epoch 30, training loss: 471.44659423828125 = 1.0954285860061646 + 50.0 * 9.407023429870605
Epoch 30, val loss: 1.09475839138031
Epoch 40, training loss: 465.7601623535156 = 1.0953145027160645 + 50.0 * 9.293296813964844
Epoch 40, val loss: 1.0946305990219116
Epoch 50, training loss: 461.1840515136719 = 1.0951852798461914 + 50.0 * 9.201777458190918
Epoch 50, val loss: 1.094486117362976
Epoch 60, training loss: 457.40313720703125 = 1.0950485467910767 + 50.0 * 9.126161575317383
Epoch 60, val loss: 1.0943360328674316
Epoch 70, training loss: 454.2353210449219 = 1.0948967933654785 + 50.0 * 9.0628080368042
Epoch 70, val loss: 1.0941725969314575
Epoch 80, training loss: 451.4957275390625 = 1.09474515914917 + 50.0 * 9.00801944732666
Epoch 80, val loss: 1.0940073728561401
Epoch 90, training loss: 449.2330017089844 = 1.0945793390274048 + 50.0 * 8.9627685546875
Epoch 90, val loss: 1.0938316583633423
Epoch 100, training loss: 447.23016357421875 = 1.0944032669067383 + 50.0 * 8.922715187072754
Epoch 100, val loss: 1.0936448574066162
Epoch 110, training loss: 445.59393310546875 = 1.0942288637161255 + 50.0 * 8.889993667602539
Epoch 110, val loss: 1.0934579372406006
Epoch 120, training loss: 444.1065368652344 = 1.094042420387268 + 50.0 * 8.860249519348145
Epoch 120, val loss: 1.093262791633606
Epoch 130, training loss: 442.8973083496094 = 1.0938525199890137 + 50.0 * 8.836069107055664
Epoch 130, val loss: 1.0930596590042114
Epoch 140, training loss: 441.889892578125 = 1.0936529636383057 + 50.0 * 8.815924644470215
Epoch 140, val loss: 1.0928528308868408
Epoch 150, training loss: 441.0212707519531 = 1.0934572219848633 + 50.0 * 8.798556327819824
Epoch 150, val loss: 1.0926446914672852
Epoch 160, training loss: 440.3687438964844 = 1.0932447910308838 + 50.0 * 8.785510063171387
Epoch 160, val loss: 1.0924286842346191
Epoch 170, training loss: 439.67938232421875 = 1.0930229425430298 + 50.0 * 8.771727561950684
Epoch 170, val loss: 1.092197299003601
Epoch 180, training loss: 439.19342041015625 = 1.0928125381469727 + 50.0 * 8.762012481689453
Epoch 180, val loss: 1.0919764041900635
Epoch 190, training loss: 438.73626708984375 = 1.0925884246826172 + 50.0 * 8.752873420715332
Epoch 190, val loss: 1.0917447805404663
Epoch 200, training loss: 438.42327880859375 = 1.0923632383346558 + 50.0 * 8.746618270874023
Epoch 200, val loss: 1.0915052890777588
Epoch 210, training loss: 438.0880126953125 = 1.0921300649642944 + 50.0 * 8.739917755126953
Epoch 210, val loss: 1.0912624597549438
Epoch 220, training loss: 437.7520446777344 = 1.0918848514556885 + 50.0 * 8.733202934265137
Epoch 220, val loss: 1.0910124778747559
Epoch 230, training loss: 437.44580078125 = 1.0916508436203003 + 50.0 * 8.727083206176758
Epoch 230, val loss: 1.0907753705978394
Epoch 240, training loss: 437.317138671875 = 1.0914026498794556 + 50.0 * 8.724514961242676
Epoch 240, val loss: 1.0905146598815918
Epoch 250, training loss: 437.20599365234375 = 1.091148018836975 + 50.0 * 8.722296714782715
Epoch 250, val loss: 1.0902528762817383
Epoch 260, training loss: 437.0343933105469 = 1.0908901691436768 + 50.0 * 8.718870162963867
Epoch 260, val loss: 1.0899850130081177
Epoch 270, training loss: 436.78192138671875 = 1.0906212329864502 + 50.0 * 8.713826179504395
Epoch 270, val loss: 1.0897133350372314
Epoch 280, training loss: 436.92791748046875 = 1.090368628501892 + 50.0 * 8.716751098632812
Epoch 280, val loss: 1.0894525051116943
Epoch 290, training loss: 436.86993408203125 = 1.0900863409042358 + 50.0 * 8.715597152709961
Epoch 290, val loss: 1.0891649723052979
Epoch 300, training loss: 436.72003173828125 = 1.0898019075393677 + 50.0 * 8.712604522705078
Epoch 300, val loss: 1.088877558708191
Epoch 310, training loss: 436.7013854980469 = 1.089510440826416 + 50.0 * 8.712237358093262
Epoch 310, val loss: 1.088587760925293
Epoch 320, training loss: 436.568359375 = 1.0892109870910645 + 50.0 * 8.709583282470703
Epoch 320, val loss: 1.0882729291915894
Epoch 330, training loss: 436.64959716796875 = 1.0888935327529907 + 50.0 * 8.711214065551758
Epoch 330, val loss: 1.0879732370376587
Epoch 340, training loss: 436.6926574707031 = 1.0885601043701172 + 50.0 * 8.712081909179688
Epoch 340, val loss: 1.0876256227493286
Epoch 350, training loss: 436.3424072265625 = 1.0882577896118164 + 50.0 * 8.705082893371582
Epoch 350, val loss: 1.0873243808746338
Epoch 360, training loss: 436.521728515625 = 1.0879114866256714 + 50.0 * 8.7086763381958
Epoch 360, val loss: 1.086978554725647
Epoch 370, training loss: 436.8074035644531 = 1.087578296661377 + 50.0 * 8.714396476745605
Epoch 370, val loss: 1.086637258529663
Epoch 380, training loss: 436.9751892089844 = 1.0872306823730469 + 50.0 * 8.717759132385254
Epoch 380, val loss: 1.0862892866134644
Epoch 390, training loss: 437.1294860839844 = 1.0868748426437378 + 50.0 * 8.72085189819336
Epoch 390, val loss: 1.085930347442627
Epoch 400, training loss: 437.17608642578125 = 1.0865083932876587 + 50.0 * 8.72179126739502
Epoch 400, val loss: 1.085563063621521
Epoch 410, training loss: 437.2811584472656 = 1.086138129234314 + 50.0 * 8.723899841308594
Epoch 410, val loss: 1.0851861238479614
Epoch 420, training loss: 437.51318359375 = 1.0857069492340088 + 50.0 * 8.72854995727539
Epoch 420, val loss: 1.0847352743148804
Epoch 430, training loss: 436.11785888671875 = 1.0852805376052856 + 50.0 * 8.700652122497559
Epoch 430, val loss: 1.0843366384506226
Epoch 440, training loss: 437.07672119140625 = 1.0849778652191162 + 50.0 * 8.719834327697754
Epoch 440, val loss: 1.0840312242507935
Epoch 450, training loss: 436.780517578125 = 1.0845212936401367 + 50.0 * 8.713919639587402
Epoch 450, val loss: 1.0835682153701782
Epoch 460, training loss: 437.2379455566406 = 1.0841888189315796 + 50.0 * 8.723074913024902
Epoch 460, val loss: 1.0832425355911255
Epoch 470, training loss: 436.759765625 = 1.0837180614471436 + 50.0 * 8.713521003723145
Epoch 470, val loss: 1.0827713012695312
Epoch 480, training loss: 437.1789245605469 = 1.0833077430725098 + 50.0 * 8.721912384033203
Epoch 480, val loss: 1.0823700428009033
Epoch 490, training loss: 437.30975341796875 = 1.082858681678772 + 50.0 * 8.72453784942627
Epoch 490, val loss: 1.0819264650344849
Epoch 500, training loss: 437.45697021484375 = 1.082404375076294 + 50.0 * 8.72749137878418
Epoch 500, val loss: 1.081483006477356
Epoch 510, training loss: 437.7087097167969 = 1.0819445848464966 + 50.0 * 8.732535362243652
Epoch 510, val loss: 1.0810275077819824
Epoch 520, training loss: 437.750244140625 = 1.0814729928970337 + 50.0 * 8.733375549316406
Epoch 520, val loss: 1.0805597305297852
Epoch 530, training loss: 437.8779602050781 = 1.08097243309021 + 50.0 * 8.735939979553223
Epoch 530, val loss: 1.0800752639770508
Epoch 540, training loss: 437.5655822753906 = 1.0804662704467773 + 50.0 * 8.72970199584961
Epoch 540, val loss: 1.0795789957046509
Epoch 550, training loss: 438.459716796875 = 1.0799407958984375 + 50.0 * 8.74759578704834
Epoch 550, val loss: 1.0790592432022095
Epoch 560, training loss: 437.70263671875 = 1.0793930292129517 + 50.0 * 8.732464790344238
Epoch 560, val loss: 1.078521728515625
Epoch 570, training loss: 438.1568603515625 = 1.0789076089859009 + 50.0 * 8.741559028625488
Epoch 570, val loss: 1.0780514478683472
Epoch 580, training loss: 437.085693359375 = 1.0783759355545044 + 50.0 * 8.720146179199219
Epoch 580, val loss: 1.0775171518325806
Epoch 590, training loss: 437.7533264160156 = 1.0778915882110596 + 50.0 * 8.733509063720703
Epoch 590, val loss: 1.077059030532837
Epoch 600, training loss: 438.0215759277344 = 1.0773522853851318 + 50.0 * 8.738883972167969
Epoch 600, val loss: 1.076529860496521
Epoch 610, training loss: 438.3511962890625 = 1.0768307447433472 + 50.0 * 8.745487213134766
Epoch 610, val loss: 1.0760161876678467
Epoch 620, training loss: 438.6564636230469 = 1.0762916803359985 + 50.0 * 8.751603126525879
Epoch 620, val loss: 1.0754904747009277
Epoch 630, training loss: 438.8204650878906 = 1.0757251977920532 + 50.0 * 8.754895210266113
Epoch 630, val loss: 1.07494056224823
Epoch 640, training loss: 438.923828125 = 1.0751676559448242 + 50.0 * 8.756973266601562
Epoch 640, val loss: 1.0743937492370605
Epoch 650, training loss: 439.1064453125 = 1.07460355758667 + 50.0 * 8.760636329650879
Epoch 650, val loss: 1.0738450288772583
Epoch 660, training loss: 439.2726135253906 = 1.074029564857483 + 50.0 * 8.763971328735352
Epoch 660, val loss: 1.073286771774292
Epoch 670, training loss: 439.57550048828125 = 1.0734401941299438 + 50.0 * 8.770041465759277
Epoch 670, val loss: 1.0727157592773438
Epoch 680, training loss: 439.642578125 = 1.072861909866333 + 50.0 * 8.771393775939941
Epoch 680, val loss: 1.0721532106399536
Epoch 690, training loss: 439.8874206542969 = 1.0722711086273193 + 50.0 * 8.7763032913208
Epoch 690, val loss: 1.0715689659118652
Epoch 700, training loss: 439.99591064453125 = 1.071674108505249 + 50.0 * 8.778484344482422
Epoch 700, val loss: 1.0709946155548096
Epoch 710, training loss: 440.0029602050781 = 1.0710618495941162 + 50.0 * 8.778637886047363
Epoch 710, val loss: 1.070405125617981
Epoch 720, training loss: 440.28765869140625 = 1.070460319519043 + 50.0 * 8.784343719482422
Epoch 720, val loss: 1.0698213577270508
Epoch 730, training loss: 440.55126953125 = 1.0698546171188354 + 50.0 * 8.789628028869629
Epoch 730, val loss: 1.0692368745803833
Epoch 740, training loss: 440.2281799316406 = 1.0692193508148193 + 50.0 * 8.78317928314209
Epoch 740, val loss: 1.068616271018982
Epoch 750, training loss: 441.1756896972656 = 1.0686614513397217 + 50.0 * 8.802140235900879
Epoch 750, val loss: 1.0680625438690186
Epoch 760, training loss: 439.19488525390625 = 1.0678867101669312 + 50.0 * 8.762539863586426
Epoch 760, val loss: 1.0673189163208008
Epoch 770, training loss: 440.3057556152344 = 1.067286729812622 + 50.0 * 8.784769058227539
Epoch 770, val loss: 1.0667260885238647
Epoch 780, training loss: 439.7690734863281 = 1.0666182041168213 + 50.0 * 8.774048805236816
Epoch 780, val loss: 1.0660953521728516
Epoch 790, training loss: 440.2358093261719 = 1.0660196542739868 + 50.0 * 8.783395767211914
Epoch 790, val loss: 1.0654972791671753
Epoch 800, training loss: 440.5895690917969 = 1.0653858184814453 + 50.0 * 8.790483474731445
Epoch 800, val loss: 1.0648707151412964
Epoch 810, training loss: 440.8941955566406 = 1.0647698640823364 + 50.0 * 8.796588897705078
Epoch 810, val loss: 1.06427001953125
Epoch 820, training loss: 440.9609375 = 1.0641521215438843 + 50.0 * 8.797935485839844
Epoch 820, val loss: 1.0636615753173828
Epoch 830, training loss: 441.1092529296875 = 1.063520908355713 + 50.0 * 8.800914764404297
Epoch 830, val loss: 1.0630462169647217
Epoch 840, training loss: 441.30670166015625 = 1.0628876686096191 + 50.0 * 8.804876327514648
Epoch 840, val loss: 1.0624237060546875
Epoch 850, training loss: 441.5423889160156 = 1.0622435808181763 + 50.0 * 8.809602737426758
Epoch 850, val loss: 1.061793327331543
Epoch 860, training loss: 441.75848388671875 = 1.061600923538208 + 50.0 * 8.813937187194824
Epoch 860, val loss: 1.0611507892608643
Epoch 870, training loss: 441.9214172363281 = 1.0609440803527832 + 50.0 * 8.817209243774414
Epoch 870, val loss: 1.0605145692825317
Epoch 880, training loss: 441.6291198730469 = 1.0602885484695435 + 50.0 * 8.811376571655273
Epoch 880, val loss: 1.0598708391189575
Epoch 890, training loss: 441.9681701660156 = 1.0596461296081543 + 50.0 * 8.818170547485352
Epoch 890, val loss: 1.0592288970947266
Epoch 900, training loss: 442.3728332519531 = 1.0589940547943115 + 50.0 * 8.826276779174805
Epoch 900, val loss: 1.0585914850234985
Epoch 910, training loss: 442.5693054199219 = 1.0583480596542358 + 50.0 * 8.830219268798828
Epoch 910, val loss: 1.057943344116211
Epoch 920, training loss: 442.5440979003906 = 1.0576742887496948 + 50.0 * 8.829728126525879
Epoch 920, val loss: 1.0572779178619385
Epoch 930, training loss: 442.4637451171875 = 1.056982398033142 + 50.0 * 8.82813549041748
Epoch 930, val loss: 1.0566009283065796
Epoch 940, training loss: 442.5672607421875 = 1.056301236152649 + 50.0 * 8.830219268798828
Epoch 940, val loss: 1.0559073686599731
Epoch 950, training loss: 442.6059875488281 = 1.0556224584579468 + 50.0 * 8.83100700378418
Epoch 950, val loss: 1.0552231073379517
Epoch 960, training loss: 442.93719482421875 = 1.0549546480178833 + 50.0 * 8.837644577026367
Epoch 960, val loss: 1.0545721054077148
Epoch 970, training loss: 443.0486755371094 = 1.054279088973999 + 50.0 * 8.839887619018555
Epoch 970, val loss: 1.0538952350616455
Epoch 980, training loss: 443.1716003417969 = 1.053591012954712 + 50.0 * 8.842360496520996
Epoch 980, val loss: 1.0532151460647583
Epoch 990, training loss: 443.4233093261719 = 1.0528984069824219 + 50.0 * 8.847408294677734
Epoch 990, val loss: 1.0525269508361816
Epoch 1000, training loss: 443.200439453125 = 1.0521925687789917 + 50.0 * 8.842965126037598
Epoch 1000, val loss: 1.0518302917480469
Epoch 1010, training loss: 443.5961608886719 = 1.0515210628509521 + 50.0 * 8.850893020629883
Epoch 1010, val loss: 1.051155686378479
Epoch 1020, training loss: 443.73541259765625 = 1.0508097410202026 + 50.0 * 8.853692054748535
Epoch 1020, val loss: 1.0504482984542847
Epoch 1030, training loss: 443.9041748046875 = 1.0500984191894531 + 50.0 * 8.857081413269043
Epoch 1030, val loss: 1.0497348308563232
Epoch 1040, training loss: 443.75 = 1.0493662357330322 + 50.0 * 8.854012489318848
Epoch 1040, val loss: 1.049017310142517
Epoch 1050, training loss: 443.7867431640625 = 1.0486301183700562 + 50.0 * 8.854762077331543
Epoch 1050, val loss: 1.0482864379882812
Epoch 1060, training loss: 444.2353210449219 = 1.0479326248168945 + 50.0 * 8.863747596740723
Epoch 1060, val loss: 1.0475894212722778
Epoch 1070, training loss: 444.1181945800781 = 1.0473551750183105 + 50.0 * 8.861416816711426
Epoch 1070, val loss: 1.04702889919281
Epoch 1080, training loss: 444.20721435546875 = 1.0468037128448486 + 50.0 * 8.863207817077637
Epoch 1080, val loss: 1.0464671850204468
Epoch 1090, training loss: 444.582763671875 = 1.0461467504501343 + 50.0 * 8.870732307434082
Epoch 1090, val loss: 1.0458146333694458
Epoch 1100, training loss: 444.71746826171875 = 1.0454374551773071 + 50.0 * 8.873440742492676
Epoch 1100, val loss: 1.0450994968414307
Epoch 1110, training loss: 444.8946533203125 = 1.0447155237197876 + 50.0 * 8.876998901367188
Epoch 1110, val loss: 1.0443928241729736
Epoch 1120, training loss: 444.6914978027344 = 1.0439622402191162 + 50.0 * 8.872950553894043
Epoch 1120, val loss: 1.043631911277771
Epoch 1130, training loss: 444.86322021484375 = 1.0432195663452148 + 50.0 * 8.876399993896484
Epoch 1130, val loss: 1.0429086685180664
Epoch 1140, training loss: 445.1353759765625 = 1.0424857139587402 + 50.0 * 8.881857872009277
Epoch 1140, val loss: 1.042178750038147
Epoch 1150, training loss: 445.1180114746094 = 1.0417401790618896 + 50.0 * 8.881525039672852
Epoch 1150, val loss: 1.0414366722106934
Epoch 1160, training loss: 445.2744140625 = 1.0409871339797974 + 50.0 * 8.884668350219727
Epoch 1160, val loss: 1.040691614151001
Epoch 1170, training loss: 445.4521789550781 = 1.0402348041534424 + 50.0 * 8.888238906860352
Epoch 1170, val loss: 1.0399432182312012
Epoch 1180, training loss: 445.5067138671875 = 1.0394643545150757 + 50.0 * 8.889345169067383
Epoch 1180, val loss: 1.039184331893921
Epoch 1190, training loss: 445.4927978515625 = 1.0387070178985596 + 50.0 * 8.889081954956055
Epoch 1190, val loss: 1.0384140014648438
Epoch 1200, training loss: 445.58966064453125 = 1.0379103422164917 + 50.0 * 8.891035079956055
Epoch 1200, val loss: 1.037649393081665
Epoch 1210, training loss: 445.8692626953125 = 1.0371462106704712 + 50.0 * 8.896642684936523
Epoch 1210, val loss: 1.0368891954421997
Epoch 1220, training loss: 445.90386962890625 = 1.0363492965698242 + 50.0 * 8.897350311279297
Epoch 1220, val loss: 1.0360926389694214
Epoch 1230, training loss: 445.9783020019531 = 1.0355497598648071 + 50.0 * 8.898855209350586
Epoch 1230, val loss: 1.035292148590088
Epoch 1240, training loss: 445.90185546875 = 1.0347342491149902 + 50.0 * 8.897342681884766
Epoch 1240, val loss: 1.034468650817871
Epoch 1250, training loss: 446.18865966796875 = 1.033920407295227 + 50.0 * 8.903094291687012
Epoch 1250, val loss: 1.0336567163467407
Epoch 1260, training loss: 446.34234619140625 = 1.0330908298492432 + 50.0 * 8.906185150146484
Epoch 1260, val loss: 1.0328311920166016
Epoch 1270, training loss: 446.2137756347656 = 1.0322198867797852 + 50.0 * 8.903631210327148
Epoch 1270, val loss: 1.0319700241088867
Epoch 1280, training loss: 446.3607482910156 = 1.0313611030578613 + 50.0 * 8.906587600708008
Epoch 1280, val loss: 1.0311044454574585
Epoch 1290, training loss: 446.4030456542969 = 1.030462622642517 + 50.0 * 8.907451629638672
Epoch 1290, val loss: 1.0302237272262573
Epoch 1300, training loss: 446.575439453125 = 1.029585599899292 + 50.0 * 8.910917282104492
Epoch 1300, val loss: 1.0293511152267456
Epoch 1310, training loss: 445.3959655761719 = 1.0286593437194824 + 50.0 * 8.887346267700195
Epoch 1310, val loss: 1.0284229516983032
Epoch 1320, training loss: 443.6895446777344 = 1.027494192123413 + 50.0 * 8.853240966796875
Epoch 1320, val loss: 1.0272883176803589
Epoch 1330, training loss: 444.9997253417969 = 1.026787519454956 + 50.0 * 8.8794584274292
Epoch 1330, val loss: 1.0265363454818726
Epoch 1340, training loss: 443.8542175292969 = 1.025851845741272 + 50.0 * 8.8565673828125
Epoch 1340, val loss: 1.0256394147872925
Epoch 1350, training loss: 444.79559326171875 = 1.0249372720718384 + 50.0 * 8.875412940979004
Epoch 1350, val loss: 1.0247060060501099
Epoch 1360, training loss: 445.439208984375 = 1.0240252017974854 + 50.0 * 8.888303756713867
Epoch 1360, val loss: 1.0237950086593628
Epoch 1370, training loss: 445.86376953125 = 1.0231059789657593 + 50.0 * 8.89681339263916
Epoch 1370, val loss: 1.022878646850586
Epoch 1380, training loss: 446.19879150390625 = 1.022188425064087 + 50.0 * 8.903532028198242
Epoch 1380, val loss: 1.0219708681106567
Epoch 1390, training loss: 446.4216613769531 = 1.0212639570236206 + 50.0 * 8.908007621765137
Epoch 1390, val loss: 1.0210511684417725
Epoch 1400, training loss: 446.2149353027344 = 1.0202839374542236 + 50.0 * 8.903892517089844
Epoch 1400, val loss: 1.0200787782669067
Epoch 1410, training loss: 446.5404052734375 = 1.0193450450897217 + 50.0 * 8.910421371459961
Epoch 1410, val loss: 1.019142746925354
Epoch 1420, training loss: 446.92041015625 = 1.0184029340744019 + 50.0 * 8.91804027557373
Epoch 1420, val loss: 1.0182116031646729
Epoch 1430, training loss: 446.8258361816406 = 1.017425775527954 + 50.0 * 8.916168212890625
Epoch 1430, val loss: 1.0172412395477295
Epoch 1440, training loss: 447.0189208984375 = 1.0164591073989868 + 50.0 * 8.920049667358398
Epoch 1440, val loss: 1.0162813663482666
Epoch 1450, training loss: 446.8392028808594 = 1.015465497970581 + 50.0 * 8.916474342346191
Epoch 1450, val loss: 1.0152732133865356
Epoch 1460, training loss: 447.1278381347656 = 1.0144952535629272 + 50.0 * 8.922266960144043
Epoch 1460, val loss: 1.0143115520477295
Epoch 1470, training loss: 447.4320373535156 = 1.0135221481323242 + 50.0 * 8.928370475769043
Epoch 1470, val loss: 1.0133371353149414
Epoch 1480, training loss: 447.2464904785156 = 1.0125175714492798 + 50.0 * 8.92467975616455
Epoch 1480, val loss: 1.0123438835144043
Epoch 1490, training loss: 447.6827392578125 = 1.0115500688552856 + 50.0 * 8.93342399597168
Epoch 1490, val loss: 1.0113778114318848
Epoch 1500, training loss: 447.5978698730469 = 1.0105504989624023 + 50.0 * 8.931746482849121
Epoch 1500, val loss: 1.0103875398635864
Epoch 1510, training loss: 446.9198913574219 = 1.009494423866272 + 50.0 * 8.918208122253418
Epoch 1510, val loss: 1.0092700719833374
Epoch 1520, training loss: 446.7906494140625 = 1.008521318435669 + 50.0 * 8.915642738342285
Epoch 1520, val loss: 1.008359670639038
Epoch 1530, training loss: 446.6200866699219 = 1.0075145959854126 + 50.0 * 8.912251472473145
Epoch 1530, val loss: 1.007365107536316
Epoch 1540, training loss: 447.3044738769531 = 1.0065423250198364 + 50.0 * 8.925958633422852
Epoch 1540, val loss: 1.0064024925231934
Epoch 1550, training loss: 447.69110107421875 = 1.0055660009384155 + 50.0 * 8.933711051940918
Epoch 1550, val loss: 1.0054397583007812
Epoch 1560, training loss: 447.9998474121094 = 1.00456702709198 + 50.0 * 8.939905166625977
Epoch 1560, val loss: 1.0044435262680054
Epoch 1570, training loss: 447.756103515625 = 1.0035451650619507 + 50.0 * 8.935050964355469
Epoch 1570, val loss: 1.0034273862838745
Epoch 1580, training loss: 448.0675048828125 = 1.0025384426116943 + 50.0 * 8.941299438476562
Epoch 1580, val loss: 1.0024287700653076
Epoch 1590, training loss: 448.375 = 1.0015357732772827 + 50.0 * 8.947469711303711
Epoch 1590, val loss: 1.001425862312317
Epoch 1600, training loss: 448.0314025878906 = 1.000510334968567 + 50.0 * 8.940617561340332
Epoch 1600, val loss: 1.000409722328186
Epoch 1610, training loss: 448.2809753417969 = 0.9994984865188599 + 50.0 * 8.945629119873047
Epoch 1610, val loss: 0.9993919730186462
Epoch 1620, training loss: 448.6025085449219 = 0.99849933385849 + 50.0 * 8.952079772949219
Epoch 1620, val loss: 0.998401403427124
Epoch 1630, training loss: 448.2581787109375 = 0.9974697232246399 + 50.0 * 8.94521427154541
Epoch 1630, val loss: 0.9973734617233276
Epoch 1640, training loss: 448.3786315917969 = 0.9964454174041748 + 50.0 * 8.947643280029297
Epoch 1640, val loss: 0.9963526129722595
Epoch 1650, training loss: 448.50469970703125 = 0.9954336285591125 + 50.0 * 8.950185775756836
Epoch 1650, val loss: 0.9953381419181824
Epoch 1660, training loss: 448.13226318359375 = 0.9943965077400208 + 50.0 * 8.942757606506348
Epoch 1660, val loss: 0.994307816028595
Epoch 1670, training loss: 448.22430419921875 = 0.9933580756187439 + 50.0 * 8.944619178771973
Epoch 1670, val loss: 0.993276834487915
Epoch 1680, training loss: 448.6785583496094 = 0.9923722147941589 + 50.0 * 8.953723907470703
Epoch 1680, val loss: 0.9922903180122375
Epoch 1690, training loss: 448.4972839355469 = 0.9913380146026611 + 50.0 * 8.950119018554688
Epoch 1690, val loss: 0.9912583231925964
Epoch 1700, training loss: 448.68389892578125 = 0.9903423190116882 + 50.0 * 8.95387077331543
Epoch 1700, val loss: 0.9902682304382324
Epoch 1710, training loss: 449.0035400390625 = 0.9893655776977539 + 50.0 * 8.960283279418945
Epoch 1710, val loss: 0.9892930388450623
Epoch 1720, training loss: 449.1139831542969 = 0.9883646965026855 + 50.0 * 8.962512016296387
Epoch 1720, val loss: 0.988290011882782
Epoch 1730, training loss: 449.19268798828125 = 0.9873626232147217 + 50.0 * 8.964106559753418
Epoch 1730, val loss: 0.9872922301292419
Epoch 1740, training loss: 449.22430419921875 = 0.9863618016242981 + 50.0 * 8.96475887298584
Epoch 1740, val loss: 0.9862903356552124
Epoch 1750, training loss: 449.1475830078125 = 0.9853405356407166 + 50.0 * 8.963244438171387
Epoch 1750, val loss: 0.9852707386016846
Epoch 1760, training loss: 449.32794189453125 = 0.9843401908874512 + 50.0 * 8.966872215270996
Epoch 1760, val loss: 0.9842813014984131
Epoch 1770, training loss: 449.6380615234375 = 0.9833539724349976 + 50.0 * 8.97309398651123
Epoch 1770, val loss: 0.9832951426506042
Epoch 1780, training loss: 449.4175109863281 = 0.9823505282402039 + 50.0 * 8.968703269958496
Epoch 1780, val loss: 0.9823002815246582
Epoch 1790, training loss: 449.6881103515625 = 0.9813545346260071 + 50.0 * 8.974135398864746
Epoch 1790, val loss: 0.9813173413276672
Epoch 1800, training loss: 449.8024597167969 = 0.9803619980812073 + 50.0 * 8.976441383361816
Epoch 1800, val loss: 0.9803324341773987
Epoch 1810, training loss: 449.8173522949219 = 0.9793651700019836 + 50.0 * 8.976759910583496
Epoch 1810, val loss: 0.9793514013290405
Epoch 1820, training loss: 449.7792053222656 = 0.9783786535263062 + 50.0 * 8.9760160446167
Epoch 1820, val loss: 0.978366494178772
Epoch 1830, training loss: 449.856201171875 = 0.9773941040039062 + 50.0 * 8.97757625579834
Epoch 1830, val loss: 0.9773866534233093
Epoch 1840, training loss: 450.05267333984375 = 0.9764208197593689 + 50.0 * 8.981525421142578
Epoch 1840, val loss: 0.9764205813407898
Epoch 1850, training loss: 450.1675720214844 = 0.975430965423584 + 50.0 * 8.983842849731445
Epoch 1850, val loss: 0.9754412770271301
Epoch 1860, training loss: 450.1184387207031 = 0.9744433164596558 + 50.0 * 8.982879638671875
Epoch 1860, val loss: 0.974471390247345
Epoch 1870, training loss: 450.1513366699219 = 0.9734593033790588 + 50.0 * 8.98355770111084
Epoch 1870, val loss: 0.9734981656074524
Epoch 1880, training loss: 450.2659606933594 = 0.9724892377853394 + 50.0 * 8.985869407653809
Epoch 1880, val loss: 0.9725332856178284
Epoch 1890, training loss: 450.4129638671875 = 0.9715220928192139 + 50.0 * 8.988828659057617
Epoch 1890, val loss: 0.9715731143951416
Epoch 1900, training loss: 450.5155029296875 = 0.9705377221107483 + 50.0 * 8.990899085998535
Epoch 1900, val loss: 0.9706058502197266
Epoch 1910, training loss: 450.5851135253906 = 0.9695632457733154 + 50.0 * 8.992310523986816
Epoch 1910, val loss: 0.9696252346038818
Epoch 1920, training loss: 450.2959289550781 = 0.9685752391815186 + 50.0 * 8.986547470092773
Epoch 1920, val loss: 0.9686471819877625
Epoch 1930, training loss: 449.544921875 = 0.9675701260566711 + 50.0 * 8.97154712677002
Epoch 1930, val loss: 0.9676477909088135
Epoch 1940, training loss: 449.4754638671875 = 0.9666030406951904 + 50.0 * 8.970176696777344
Epoch 1940, val loss: 0.9666913151741028
Epoch 1950, training loss: 449.90252685546875 = 0.9656946063041687 + 50.0 * 8.978736877441406
Epoch 1950, val loss: 0.9657898545265198
Epoch 1960, training loss: 450.3141174316406 = 0.9647363424301147 + 50.0 * 8.986988067626953
Epoch 1960, val loss: 0.9648520946502686
Epoch 1970, training loss: 450.7604675292969 = 0.963775634765625 + 50.0 * 8.995933532714844
Epoch 1970, val loss: 0.9638996124267578
Epoch 1980, training loss: 450.95440673828125 = 0.962810218334198 + 50.0 * 8.999832153320312
Epoch 1980, val loss: 0.9629397392272949
Epoch 1990, training loss: 451.0015869140625 = 0.961846649646759 + 50.0 * 9.000794410705566
Epoch 1990, val loss: 0.9619861245155334
Epoch 2000, training loss: 450.9030456542969 = 0.9608908891677856 + 50.0 * 8.9988431930542
Epoch 2000, val loss: 0.9610433578491211
Epoch 2010, training loss: 451.15362548828125 = 0.9599379897117615 + 50.0 * 9.003873825073242
Epoch 2010, val loss: 0.9601010084152222
Epoch 2020, training loss: 451.2325439453125 = 0.9589914083480835 + 50.0 * 9.005471229553223
Epoch 2020, val loss: 0.9591622948646545
Epoch 2030, training loss: 450.8825988769531 = 0.9580262303352356 + 50.0 * 8.998491287231445
Epoch 2030, val loss: 0.9582073092460632
Epoch 2040, training loss: 450.77227783203125 = 0.9570760726928711 + 50.0 * 8.996304512023926
Epoch 2040, val loss: 0.9572873711585999
Epoch 2050, training loss: 451.0064392089844 = 0.9561641812324524 + 50.0 * 9.001005172729492
Epoch 2050, val loss: 0.956386923789978
Epoch 2060, training loss: 451.3111267089844 = 0.9552388787269592 + 50.0 * 9.007118225097656
Epoch 2060, val loss: 0.9554699659347534
Epoch 2070, training loss: 451.30816650390625 = 0.9542999267578125 + 50.0 * 9.00707721710205
Epoch 2070, val loss: 0.9545416235923767
Epoch 2080, training loss: 451.23699951171875 = 0.9533563852310181 + 50.0 * 9.005672454833984
Epoch 2080, val loss: 0.9536100625991821
Epoch 2090, training loss: 451.3761291503906 = 0.9524329304695129 + 50.0 * 9.008474349975586
Epoch 2090, val loss: 0.9527027606964111
Epoch 2100, training loss: 451.429931640625 = 0.9515062570571899 + 50.0 * 9.009568214416504
Epoch 2100, val loss: 0.9517885446548462
Epoch 2110, training loss: 450.4017333984375 = 0.9505322575569153 + 50.0 * 8.98902416229248
Epoch 2110, val loss: 0.9508025050163269
Epoch 2120, training loss: 450.9981689453125 = 0.949816107749939 + 50.0 * 9.000967025756836
Epoch 2120, val loss: 0.9500948190689087
Epoch 2130, training loss: 451.21514892578125 = 0.9489297270774841 + 50.0 * 9.005324363708496
Epoch 2130, val loss: 0.9492508769035339
Epoch 2140, training loss: 451.60382080078125 = 0.9480471014976501 + 50.0 * 9.013114929199219
Epoch 2140, val loss: 0.948380172252655
Epoch 2150, training loss: 451.9866638183594 = 0.9471765160560608 + 50.0 * 9.020790100097656
Epoch 2150, val loss: 0.9475084543228149
Epoch 2160, training loss: 452.2975769042969 = 0.946267306804657 + 50.0 * 9.027026176452637
Epoch 2160, val loss: 0.9466130137443542
Epoch 2170, training loss: 452.08917236328125 = 0.9453321695327759 + 50.0 * 9.022876739501953
Epoch 2170, val loss: 0.9457085728645325
Epoch 2180, training loss: 452.2676086425781 = 0.9444369077682495 + 50.0 * 9.026463508605957
Epoch 2180, val loss: 0.9448330402374268
Epoch 2190, training loss: 452.51513671875 = 0.9435412883758545 + 50.0 * 9.031432151794434
Epoch 2190, val loss: 0.9439603686332703
Epoch 2200, training loss: 452.36480712890625 = 0.9426419138908386 + 50.0 * 9.028443336486816
Epoch 2200, val loss: 0.94307941198349
Epoch 2210, training loss: 452.4826354980469 = 0.9417489171028137 + 50.0 * 9.030817985534668
Epoch 2210, val loss: 0.9422180652618408
Epoch 2220, training loss: 452.5622863769531 = 0.9408676624298096 + 50.0 * 9.032428741455078
Epoch 2220, val loss: 0.9413524866104126
Epoch 2230, training loss: 452.6476745605469 = 0.9399840831756592 + 50.0 * 9.034153938293457
Epoch 2230, val loss: 0.9404923915863037
Epoch 2240, training loss: 452.75128173828125 = 0.9391158223152161 + 50.0 * 9.036243438720703
Epoch 2240, val loss: 0.9396284222602844
Epoch 2250, training loss: 452.6255798339844 = 0.938225507736206 + 50.0 * 9.033746719360352
Epoch 2250, val loss: 0.9387836456298828
Epoch 2260, training loss: 450.8038024902344 = 0.937262237071991 + 50.0 * 8.997330665588379
Epoch 2260, val loss: 0.9378602504730225
Epoch 2270, training loss: 450.8239440917969 = 0.9364897608757019 + 50.0 * 8.997749328613281
Epoch 2270, val loss: 0.9371004700660706
Epoch 2280, training loss: 450.8600158691406 = 0.9357825517654419 + 50.0 * 8.99848461151123
Epoch 2280, val loss: 0.93643718957901
Epoch 2290, training loss: 450.55035400390625 = 0.9350356459617615 + 50.0 * 8.99230670928955
Epoch 2290, val loss: 0.9356977343559265
Epoch 2300, training loss: 450.7575988769531 = 0.9342755675315857 + 50.0 * 8.996466636657715
Epoch 2300, val loss: 0.9349523782730103
Epoch 2310, training loss: 451.4994812011719 = 0.9334592223167419 + 50.0 * 9.011320114135742
Epoch 2310, val loss: 0.9341611862182617
Epoch 2320, training loss: 451.9079895019531 = 0.9326082468032837 + 50.0 * 9.01950740814209
Epoch 2320, val loss: 0.9333304166793823
Epoch 2330, training loss: 452.2597961425781 = 0.931769847869873 + 50.0 * 9.02656078338623
Epoch 2330, val loss: 0.9325124025344849
Epoch 2340, training loss: 452.6360168457031 = 0.930934488773346 + 50.0 * 9.034101486206055
Epoch 2340, val loss: 0.9317014217376709
Epoch 2350, training loss: 452.9433288574219 = 0.9301014542579651 + 50.0 * 9.040264129638672
Epoch 2350, val loss: 0.9308903813362122
Epoch 2360, training loss: 452.5144348144531 = 0.9293009638786316 + 50.0 * 9.031702995300293
Epoch 2360, val loss: 0.930095374584198
Epoch 2370, training loss: 452.6402587890625 = 0.9285039305686951 + 50.0 * 9.034235000610352
Epoch 2370, val loss: 0.9293310642242432
Epoch 2380, training loss: 452.8179016113281 = 0.9277178645133972 + 50.0 * 9.037803649902344
Epoch 2380, val loss: 0.9285832047462463
Epoch 2390, training loss: 453.0923767089844 = 0.9269252419471741 + 50.0 * 9.043309211730957
Epoch 2390, val loss: 0.9278169870376587
Epoch 2400, training loss: 453.2883605957031 = 0.9261272549629211 + 50.0 * 9.047245025634766
Epoch 2400, val loss: 0.9270414710044861
Epoch 2410, training loss: 453.2106628417969 = 0.925324022769928 + 50.0 * 9.045706748962402
Epoch 2410, val loss: 0.9262579083442688
Epoch 2420, training loss: 453.16180419921875 = 0.924537181854248 + 50.0 * 9.044745445251465
Epoch 2420, val loss: 0.9254891276359558
Epoch 2430, training loss: 453.25604248046875 = 0.9237459897994995 + 50.0 * 9.046646118164062
Epoch 2430, val loss: 0.9247239232063293
Epoch 2440, training loss: 453.43353271484375 = 0.9229724407196045 + 50.0 * 9.050210952758789
Epoch 2440, val loss: 0.9239713549613953
Epoch 2450, training loss: 453.4501647949219 = 0.9221978187561035 + 50.0 * 9.050559043884277
Epoch 2450, val loss: 0.9232212901115417
Epoch 2460, training loss: 453.5731201171875 = 0.921431839466095 + 50.0 * 9.053033828735352
Epoch 2460, val loss: 0.9224744439125061
Epoch 2470, training loss: 453.4974670410156 = 0.9206706285476685 + 50.0 * 9.051535606384277
Epoch 2470, val loss: 0.9217379093170166
Epoch 2480, training loss: 453.61151123046875 = 0.9199363589286804 + 50.0 * 9.053832054138184
Epoch 2480, val loss: 0.9210276007652283
Epoch 2490, training loss: 453.82598876953125 = 0.919203519821167 + 50.0 * 9.058135986328125
Epoch 2490, val loss: 0.9203163385391235
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.5418840579710145
0.8628559008911107
=== training gcn model ===
Epoch 0, training loss: 510.34783935546875 = 1.1077864170074463 + 50.0 * 10.18480110168457
Epoch 0, val loss: 1.1077518463134766
Epoch 10, training loss: 491.1243896484375 = 1.1073074340820312 + 50.0 * 9.800341606140137
Epoch 10, val loss: 1.1072510480880737
Epoch 20, training loss: 480.84478759765625 = 1.10677170753479 + 50.0 * 9.594759941101074
Epoch 20, val loss: 1.1066933870315552
Epoch 30, training loss: 473.80828857421875 = 1.1062577962875366 + 50.0 * 9.45404052734375
Epoch 30, val loss: 1.106154203414917
Epoch 40, training loss: 468.41845703125 = 1.1057602167129517 + 50.0 * 9.346253395080566
Epoch 40, val loss: 1.1056358814239502
Epoch 50, training loss: 463.9981689453125 = 1.1052594184875488 + 50.0 * 9.257858276367188
Epoch 50, val loss: 1.1051172018051147
Epoch 60, training loss: 460.1827087402344 = 1.1047604084014893 + 50.0 * 9.181558609008789
Epoch 60, val loss: 1.104600429534912
Epoch 70, training loss: 456.8963928222656 = 1.1042530536651611 + 50.0 * 9.115842819213867
Epoch 70, val loss: 1.1040716171264648
Epoch 80, training loss: 454.05682373046875 = 1.1037393808364868 + 50.0 * 9.059062004089355
Epoch 80, val loss: 1.1035436391830444
Epoch 90, training loss: 451.63226318359375 = 1.1032103300094604 + 50.0 * 9.010581016540527
Epoch 90, val loss: 1.1029956340789795
Epoch 100, training loss: 449.6135559082031 = 1.1026746034622192 + 50.0 * 8.97021770477295
Epoch 100, val loss: 1.1024340391159058
Epoch 110, training loss: 447.9113464355469 = 1.1021182537078857 + 50.0 * 8.936184883117676
Epoch 110, val loss: 1.1018682718276978
Epoch 120, training loss: 446.3731384277344 = 1.1015568971633911 + 50.0 * 8.905431747436523
Epoch 120, val loss: 1.1012910604476929
Epoch 130, training loss: 445.0552062988281 = 1.1009740829467773 + 50.0 * 8.879084587097168
Epoch 130, val loss: 1.1006883382797241
Epoch 140, training loss: 443.9139099121094 = 1.1003721952438354 + 50.0 * 8.856270790100098
Epoch 140, val loss: 1.100070595741272
Epoch 150, training loss: 442.86798095703125 = 1.0997370481491089 + 50.0 * 8.835365295410156
Epoch 150, val loss: 1.0994231700897217
Epoch 160, training loss: 442.1119384765625 = 1.0990961790084839 + 50.0 * 8.820257186889648
Epoch 160, val loss: 1.0987694263458252
Epoch 170, training loss: 441.2535400390625 = 1.0984406471252441 + 50.0 * 8.803101539611816
Epoch 170, val loss: 1.0981048345565796
Epoch 180, training loss: 440.6318359375 = 1.0977672338485718 + 50.0 * 8.790680885314941
Epoch 180, val loss: 1.0974085330963135
Epoch 190, training loss: 439.93182373046875 = 1.0970542430877686 + 50.0 * 8.776695251464844
Epoch 190, val loss: 1.0966901779174805
Epoch 200, training loss: 439.4298095703125 = 1.0963343381881714 + 50.0 * 8.766669273376465
Epoch 200, val loss: 1.0959618091583252
Epoch 210, training loss: 438.91851806640625 = 1.0956169366836548 + 50.0 * 8.756458282470703
Epoch 210, val loss: 1.095228910446167
Epoch 220, training loss: 438.4488830566406 = 1.0948734283447266 + 50.0 * 8.747079849243164
Epoch 220, val loss: 1.0944836139678955
Epoch 230, training loss: 438.171875 = 1.0941060781478882 + 50.0 * 8.741555213928223
Epoch 230, val loss: 1.0937082767486572
Epoch 240, training loss: 438.00732421875 = 1.0929161310195923 + 50.0 * 8.738287925720215
Epoch 240, val loss: 1.0925018787384033
Epoch 250, training loss: 437.7989196777344 = 1.091567039489746 + 50.0 * 8.734147071838379
Epoch 250, val loss: 1.0912233591079712
Epoch 260, training loss: 437.5482482910156 = 1.0903254747390747 + 50.0 * 8.729158401489258
Epoch 260, val loss: 1.090057611465454
Epoch 270, training loss: 437.3516845703125 = 1.0891788005828857 + 50.0 * 8.725250244140625
Epoch 270, val loss: 1.088958740234375
Epoch 280, training loss: 437.19921875 = 1.08812415599823 + 50.0 * 8.722221374511719
Epoch 280, val loss: 1.087961196899414
Epoch 290, training loss: 437.0989685058594 = 1.0871227979660034 + 50.0 * 8.720236778259277
Epoch 290, val loss: 1.0870306491851807
Epoch 300, training loss: 437.0971984863281 = 1.0861774682998657 + 50.0 * 8.720220565795898
Epoch 300, val loss: 1.0861355066299438
Epoch 310, training loss: 437.0724792480469 = 1.0852234363555908 + 50.0 * 8.719744682312012
Epoch 310, val loss: 1.085222840309143
Epoch 320, training loss: 436.8728942871094 = 1.0842623710632324 + 50.0 * 8.71577262878418
Epoch 320, val loss: 1.0843193531036377
Epoch 330, training loss: 436.843994140625 = 1.0833138227462769 + 50.0 * 8.715213775634766
Epoch 330, val loss: 1.083417296409607
Epoch 340, training loss: 436.7127990722656 = 1.0823595523834229 + 50.0 * 8.712608337402344
Epoch 340, val loss: 1.0825059413909912
Epoch 350, training loss: 436.7276306152344 = 1.0814093351364136 + 50.0 * 8.712924003601074
Epoch 350, val loss: 1.0815974473953247
Epoch 360, training loss: 436.8475341796875 = 1.080434799194336 + 50.0 * 8.715341567993164
Epoch 360, val loss: 1.0806728601455688
Epoch 370, training loss: 436.6565856933594 = 1.0794583559036255 + 50.0 * 8.711542129516602
Epoch 370, val loss: 1.0797371864318848
Epoch 380, training loss: 436.703125 = 1.0784403085708618 + 50.0 * 8.712493896484375
Epoch 380, val loss: 1.0787651538848877
Epoch 390, training loss: 436.4752502441406 = 1.0774341821670532 + 50.0 * 8.707956314086914
Epoch 390, val loss: 1.0778107643127441
Epoch 400, training loss: 436.71624755859375 = 1.076472520828247 + 50.0 * 8.71279525756836
Epoch 400, val loss: 1.076886534690857
Epoch 410, training loss: 436.678955078125 = 1.0754395723342896 + 50.0 * 8.71207046508789
Epoch 410, val loss: 1.0758957862854004
Epoch 420, training loss: 436.9532165527344 = 1.074415683746338 + 50.0 * 8.717576026916504
Epoch 420, val loss: 1.0749239921569824
Epoch 430, training loss: 436.972900390625 = 1.0733771324157715 + 50.0 * 8.71799087524414
Epoch 430, val loss: 1.0739216804504395
Epoch 440, training loss: 437.15057373046875 = 1.0723276138305664 + 50.0 * 8.721565246582031
Epoch 440, val loss: 1.0729255676269531
Epoch 450, training loss: 437.53155517578125 = 1.0712541341781616 + 50.0 * 8.729206085205078
Epoch 450, val loss: 1.0719304084777832
Epoch 460, training loss: 437.6120910644531 = 1.0701686143875122 + 50.0 * 8.730838775634766
Epoch 460, val loss: 1.0708831548690796
Epoch 470, training loss: 437.0858459472656 = 1.0690490007400513 + 50.0 * 8.720335960388184
Epoch 470, val loss: 1.0697944164276123
Epoch 480, training loss: 437.7217712402344 = 1.0679519176483154 + 50.0 * 8.733076095581055
Epoch 480, val loss: 1.0687482357025146
Epoch 490, training loss: 437.5945129394531 = 1.0668692588806152 + 50.0 * 8.730552673339844
Epoch 490, val loss: 1.0677177906036377
Epoch 500, training loss: 437.2414855957031 = 1.0657047033309937 + 50.0 * 8.723515510559082
Epoch 500, val loss: 1.0665853023529053
Epoch 510, training loss: 437.7398376464844 = 1.064588189125061 + 50.0 * 8.733505249023438
Epoch 510, val loss: 1.0654970407485962
Epoch 520, training loss: 437.7909851074219 = 1.0634292364120483 + 50.0 * 8.734551429748535
Epoch 520, val loss: 1.0644055604934692
Epoch 530, training loss: 437.86822509765625 = 1.062245488166809 + 50.0 * 8.736119270324707
Epoch 530, val loss: 1.0632777214050293
Epoch 540, training loss: 438.151123046875 = 1.0610575675964355 + 50.0 * 8.741801261901855
Epoch 540, val loss: 1.0620979070663452
Epoch 550, training loss: 438.6445007324219 = 1.0598863363265991 + 50.0 * 8.751692771911621
Epoch 550, val loss: 1.0610215663909912
Epoch 560, training loss: 437.7953796386719 = 1.0586402416229248 + 50.0 * 8.734734535217285
Epoch 560, val loss: 1.059827208518982
Epoch 570, training loss: 437.9019470214844 = 1.0573904514312744 + 50.0 * 8.73689079284668
Epoch 570, val loss: 1.0586061477661133
Epoch 580, training loss: 438.4289855957031 = 1.0561654567718506 + 50.0 * 8.747456550598145
Epoch 580, val loss: 1.05743408203125
Epoch 590, training loss: 438.38580322265625 = 1.0549290180206299 + 50.0 * 8.746617317199707
Epoch 590, val loss: 1.0562604665756226
Epoch 600, training loss: 438.4991760253906 = 1.0536799430847168 + 50.0 * 8.748909950256348
Epoch 600, val loss: 1.055064082145691
Epoch 610, training loss: 438.77197265625 = 1.0524083375930786 + 50.0 * 8.75439167022705
Epoch 610, val loss: 1.0538476705551147
Epoch 620, training loss: 439.03082275390625 = 1.0510802268981934 + 50.0 * 8.759594917297363
Epoch 620, val loss: 1.052581548690796
Epoch 630, training loss: 439.04052734375 = 1.049759864807129 + 50.0 * 8.759815216064453
Epoch 630, val loss: 1.0512971878051758
Epoch 640, training loss: 439.4243469238281 = 1.0483955144882202 + 50.0 * 8.767518997192383
Epoch 640, val loss: 1.0499974489212036
Epoch 650, training loss: 439.1358337402344 = 1.047015905380249 + 50.0 * 8.761775970458984
Epoch 650, val loss: 1.0486961603164673
Epoch 660, training loss: 439.53533935546875 = 1.045615315437317 + 50.0 * 8.769794464111328
Epoch 660, val loss: 1.047360897064209
Epoch 670, training loss: 439.9163818359375 = 1.0442036390304565 + 50.0 * 8.777443885803223
Epoch 670, val loss: 1.0460070371627808
Epoch 680, training loss: 439.75128173828125 = 1.0426909923553467 + 50.0 * 8.774171829223633
Epoch 680, val loss: 1.0445454120635986
Epoch 690, training loss: 439.6455078125 = 1.041113257408142 + 50.0 * 8.772088050842285
Epoch 690, val loss: 1.0430476665496826
Epoch 700, training loss: 440.0296936035156 = 1.0395334959030151 + 50.0 * 8.779803276062012
Epoch 700, val loss: 1.0415314435958862
Epoch 710, training loss: 440.2893371582031 = 1.0379533767700195 + 50.0 * 8.785027503967285
Epoch 710, val loss: 1.0399961471557617
Epoch 720, training loss: 440.5489196777344 = 1.036283016204834 + 50.0 * 8.790252685546875
Epoch 720, val loss: 1.0384103059768677
Epoch 730, training loss: 440.9057922363281 = 1.0345728397369385 + 50.0 * 8.79742431640625
Epoch 730, val loss: 1.0367587804794312
Epoch 740, training loss: 440.5303955078125 = 1.032751202583313 + 50.0 * 8.789953231811523
Epoch 740, val loss: 1.0350244045257568
Epoch 750, training loss: 440.6392822265625 = 1.0310176610946655 + 50.0 * 8.792165756225586
Epoch 750, val loss: 1.0333558320999146
Epoch 760, training loss: 441.0743408203125 = 1.0292640924453735 + 50.0 * 8.800901412963867
Epoch 760, val loss: 1.0316838026046753
Epoch 770, training loss: 441.2613525390625 = 1.027492642402649 + 50.0 * 8.80467700958252
Epoch 770, val loss: 1.029990792274475
Epoch 780, training loss: 441.08984375 = 1.0256683826446533 + 50.0 * 8.801283836364746
Epoch 780, val loss: 1.0282578468322754
Epoch 790, training loss: 441.3528137207031 = 1.0239059925079346 + 50.0 * 8.806578636169434
Epoch 790, val loss: 1.0265787839889526
Epoch 800, training loss: 441.5319519042969 = 1.0220975875854492 + 50.0 * 8.810196876525879
Epoch 800, val loss: 1.024855613708496
Epoch 810, training loss: 441.60479736328125 = 1.0202858448028564 + 50.0 * 8.811690330505371
Epoch 810, val loss: 1.023105263710022
Epoch 820, training loss: 441.80731201171875 = 1.0184451341629028 + 50.0 * 8.815777778625488
Epoch 820, val loss: 1.0213770866394043
Epoch 830, training loss: 441.8850402832031 = 1.0165975093841553 + 50.0 * 8.817368507385254
Epoch 830, val loss: 1.0195813179016113
Epoch 840, training loss: 441.73382568359375 = 1.014702558517456 + 50.0 * 8.814382553100586
Epoch 840, val loss: 1.0178003311157227
Epoch 850, training loss: 441.85101318359375 = 1.012744426727295 + 50.0 * 8.816764831542969
Epoch 850, val loss: 1.015905499458313
Epoch 860, training loss: 442.0126953125 = 1.0108648538589478 + 50.0 * 8.820036888122559
Epoch 860, val loss: 1.0141249895095825
Epoch 870, training loss: 442.2881774902344 = 1.0089547634124756 + 50.0 * 8.825584411621094
Epoch 870, val loss: 1.0122904777526855
Epoch 880, training loss: 442.46978759765625 = 1.007012963294983 + 50.0 * 8.829255104064941
Epoch 880, val loss: 1.0104293823242188
Epoch 890, training loss: 442.59808349609375 = 1.0050022602081299 + 50.0 * 8.83186149597168
Epoch 890, val loss: 1.0085422992706299
Epoch 900, training loss: 442.6346130371094 = 1.0030207633972168 + 50.0 * 8.832632064819336
Epoch 900, val loss: 1.0066622495651245
Epoch 910, training loss: 441.63372802734375 = 1.0008609294891357 + 50.0 * 8.812657356262207
Epoch 910, val loss: 1.0046017169952393
Epoch 920, training loss: 442.4213562011719 = 0.9990698099136353 + 50.0 * 8.828445434570312
Epoch 920, val loss: 1.0028616189956665
Epoch 930, training loss: 442.4080505371094 = 0.9969525337219238 + 50.0 * 8.828222274780273
Epoch 930, val loss: 1.0008337497711182
Epoch 940, training loss: 442.6482849121094 = 0.9948571920394897 + 50.0 * 8.83306884765625
Epoch 940, val loss: 0.9988552331924438
Epoch 950, training loss: 442.875244140625 = 0.9927330017089844 + 50.0 * 8.837650299072266
Epoch 950, val loss: 0.9968239068984985
Epoch 960, training loss: 443.18603515625 = 0.9905542731285095 + 50.0 * 8.843910217285156
Epoch 960, val loss: 0.9947347044944763
Epoch 970, training loss: 443.0937194824219 = 0.9882723093032837 + 50.0 * 8.842108726501465
Epoch 970, val loss: 0.9925872087478638
Epoch 980, training loss: 443.06329345703125 = 0.9860357642173767 + 50.0 * 8.841545104980469
Epoch 980, val loss: 0.9904804825782776
Epoch 990, training loss: 443.4518737792969 = 0.9837702512741089 + 50.0 * 8.84936237335205
Epoch 990, val loss: 0.9883061051368713
Epoch 1000, training loss: 443.3558349609375 = 0.9814162254333496 + 50.0 * 8.847488403320312
Epoch 1000, val loss: 0.9860718250274658
Epoch 1010, training loss: 443.6294860839844 = 0.9791477918624878 + 50.0 * 8.853006362915039
Epoch 1010, val loss: 0.9839257597923279
Epoch 1020, training loss: 443.6070556640625 = 0.9766130447387695 + 50.0 * 8.852608680725098
Epoch 1020, val loss: 0.9815404415130615
Epoch 1030, training loss: 443.709228515625 = 0.9742134213447571 + 50.0 * 8.854700088500977
Epoch 1030, val loss: 0.9792444705963135
Epoch 1040, training loss: 444.00152587890625 = 0.9718171954154968 + 50.0 * 8.860593795776367
Epoch 1040, val loss: 0.9769511818885803
Epoch 1050, training loss: 443.8846130371094 = 0.9693294167518616 + 50.0 * 8.858305931091309
Epoch 1050, val loss: 0.9745997190475464
Epoch 1060, training loss: 444.1320495605469 = 0.9668880105018616 + 50.0 * 8.863303184509277
Epoch 1060, val loss: 0.9722745418548584
Epoch 1070, training loss: 444.29901123046875 = 0.964331865310669 + 50.0 * 8.866693496704102
Epoch 1070, val loss: 0.9698725342750549
Epoch 1080, training loss: 444.3686828613281 = 0.9617692828178406 + 50.0 * 8.868138313293457
Epoch 1080, val loss: 0.9674012660980225
Epoch 1090, training loss: 444.4967041015625 = 0.9592296481132507 + 50.0 * 8.870749473571777
Epoch 1090, val loss: 0.9650059938430786
Epoch 1100, training loss: 444.22344970703125 = 0.9565709233283997 + 50.0 * 8.865337371826172
Epoch 1100, val loss: 0.9624780416488647
Epoch 1110, training loss: 443.7877197265625 = 0.9540998935699463 + 50.0 * 8.856672286987305
Epoch 1110, val loss: 0.9602051377296448
Epoch 1120, training loss: 443.7187805175781 = 0.9512752294540405 + 50.0 * 8.855350494384766
Epoch 1120, val loss: 0.9574044942855835
Epoch 1130, training loss: 444.4575500488281 = 0.9487113952636719 + 50.0 * 8.870177268981934
Epoch 1130, val loss: 0.9550002217292786
Epoch 1140, training loss: 444.68023681640625 = 0.946002185344696 + 50.0 * 8.87468433380127
Epoch 1140, val loss: 0.9524600505828857
Epoch 1150, training loss: 444.9937744140625 = 0.9433552026748657 + 50.0 * 8.88100814819336
Epoch 1150, val loss: 0.9499397277832031
Epoch 1160, training loss: 445.1429138183594 = 0.9406627416610718 + 50.0 * 8.884044647216797
Epoch 1160, val loss: 0.9473739862442017
Epoch 1170, training loss: 444.5877990722656 = 0.937812089920044 + 50.0 * 8.873000144958496
Epoch 1170, val loss: 0.9447007179260254
Epoch 1180, training loss: 444.59002685546875 = 0.9350571036338806 + 50.0 * 8.873099327087402
Epoch 1180, val loss: 0.9420667290687561
Epoch 1190, training loss: 444.9511413574219 = 0.9323447942733765 + 50.0 * 8.880375862121582
Epoch 1190, val loss: 0.9394547939300537
Epoch 1200, training loss: 445.29168701171875 = 0.9295434355735779 + 50.0 * 8.887243270874023
Epoch 1200, val loss: 0.936805009841919
Epoch 1210, training loss: 445.35113525390625 = 0.9267071485519409 + 50.0 * 8.88848876953125
Epoch 1210, val loss: 0.9341020584106445
Epoch 1220, training loss: 445.4407653808594 = 0.9238549470901489 + 50.0 * 8.890337944030762
Epoch 1220, val loss: 0.9314428567886353
Epoch 1230, training loss: 445.6591796875 = 0.9211239218711853 + 50.0 * 8.894761085510254
Epoch 1230, val loss: 0.9288092255592346
Epoch 1240, training loss: 445.7805480957031 = 0.918256938457489 + 50.0 * 8.897246360778809
Epoch 1240, val loss: 0.926085889339447
Epoch 1250, training loss: 445.7243957519531 = 0.9153358340263367 + 50.0 * 8.896181106567383
Epoch 1250, val loss: 0.9233306050300598
Epoch 1260, training loss: 445.8141174316406 = 0.9123586416244507 + 50.0 * 8.898035049438477
Epoch 1260, val loss: 0.9205406904220581
Epoch 1270, training loss: 446.0041198730469 = 0.9094417691230774 + 50.0 * 8.901893615722656
Epoch 1270, val loss: 0.9177277684211731
Epoch 1280, training loss: 446.3276062011719 = 0.9064371585845947 + 50.0 * 8.90842342376709
Epoch 1280, val loss: 0.9149033427238464
Epoch 1290, training loss: 446.2386169433594 = 0.9034201502799988 + 50.0 * 8.90670394897461
Epoch 1290, val loss: 0.9120132327079773
Epoch 1300, training loss: 446.32916259765625 = 0.9004218578338623 + 50.0 * 8.908575057983398
Epoch 1300, val loss: 0.9091393947601318
Epoch 1310, training loss: 445.9082946777344 = 0.8972499966621399 + 50.0 * 8.90022087097168
Epoch 1310, val loss: 0.9062483906745911
Epoch 1320, training loss: 445.7148132324219 = 0.89441978931427 + 50.0 * 8.896408081054688
Epoch 1320, val loss: 0.9034677147865295
Epoch 1330, training loss: 445.1034240722656 = 0.8911800384521484 + 50.0 * 8.884244918823242
Epoch 1330, val loss: 0.9004704356193542
Epoch 1340, training loss: 445.7043151855469 = 0.8883249759674072 + 50.0 * 8.896319389343262
Epoch 1340, val loss: 0.897740364074707
Epoch 1350, training loss: 445.0058898925781 = 0.8852588534355164 + 50.0 * 8.882412910461426
Epoch 1350, val loss: 0.8948012590408325
Epoch 1360, training loss: 445.67669677734375 = 0.8821751475334167 + 50.0 * 8.895890235900879
Epoch 1360, val loss: 0.8919070363044739
Epoch 1370, training loss: 445.8739318847656 = 0.8790850043296814 + 50.0 * 8.899896621704102
Epoch 1370, val loss: 0.8889694809913635
Epoch 1380, training loss: 446.1405334472656 = 0.8759151697158813 + 50.0 * 8.905292510986328
Epoch 1380, val loss: 0.8859859704971313
Epoch 1390, training loss: 446.49298095703125 = 0.8727595806121826 + 50.0 * 8.91240406036377
Epoch 1390, val loss: 0.8829989433288574
Epoch 1400, training loss: 446.5808410644531 = 0.8695558905601501 + 50.0 * 8.914225578308105
Epoch 1400, val loss: 0.8799717426300049
Epoch 1410, training loss: 446.6807861328125 = 0.8663203716278076 + 50.0 * 8.916289329528809
Epoch 1410, val loss: 0.8768633008003235
Epoch 1420, training loss: 446.5979309082031 = 0.8630340099334717 + 50.0 * 8.914697647094727
Epoch 1420, val loss: 0.8738051056861877
Epoch 1430, training loss: 446.8584289550781 = 0.8598095178604126 + 50.0 * 8.91997241973877
Epoch 1430, val loss: 0.8707664608955383
Epoch 1440, training loss: 447.0313415527344 = 0.8565611839294434 + 50.0 * 8.923495292663574
Epoch 1440, val loss: 0.867662787437439
Epoch 1450, training loss: 446.94818115234375 = 0.853255033493042 + 50.0 * 8.92189884185791
Epoch 1450, val loss: 0.8645764589309692
Epoch 1460, training loss: 447.0768127441406 = 0.8499717712402344 + 50.0 * 8.92453670501709
Epoch 1460, val loss: 0.8614702224731445
Epoch 1470, training loss: 447.19586181640625 = 0.8466756343841553 + 50.0 * 8.926983833312988
Epoch 1470, val loss: 0.8583757281303406
Epoch 1480, training loss: 447.516845703125 = 0.8433544635772705 + 50.0 * 8.933469772338867
Epoch 1480, val loss: 0.855236291885376
Epoch 1490, training loss: 447.5030517578125 = 0.8399853706359863 + 50.0 * 8.933260917663574
Epoch 1490, val loss: 0.852056622505188
Epoch 1500, training loss: 447.3059997558594 = 0.8366246223449707 + 50.0 * 8.929387092590332
Epoch 1500, val loss: 0.8489010334014893
Epoch 1510, training loss: 446.7052001953125 = 0.8329849243164062 + 50.0 * 8.917444229125977
Epoch 1510, val loss: 0.8454669713973999
Epoch 1520, training loss: 446.1180419921875 = 0.8295243382453918 + 50.0 * 8.905770301818848
Epoch 1520, val loss: 0.8421337008476257
Epoch 1530, training loss: 446.9353332519531 = 0.8262186646461487 + 50.0 * 8.922182083129883
Epoch 1530, val loss: 0.8389855623245239
Epoch 1540, training loss: 447.1868896484375 = 0.8227881789207458 + 50.0 * 8.927282333374023
Epoch 1540, val loss: 0.8357832431793213
Epoch 1550, training loss: 447.5282287597656 = 0.8192983865737915 + 50.0 * 8.934178352355957
Epoch 1550, val loss: 0.8325121998786926
Epoch 1560, training loss: 447.74517822265625 = 0.8157801032066345 + 50.0 * 8.93858814239502
Epoch 1560, val loss: 0.8292061686515808
Epoch 1570, training loss: 448.0193176269531 = 0.8122503161430359 + 50.0 * 8.944141387939453
Epoch 1570, val loss: 0.8258823156356812
Epoch 1580, training loss: 447.96795654296875 = 0.8087103962898254 + 50.0 * 8.943184852600098
Epoch 1580, val loss: 0.8225662112236023
Epoch 1590, training loss: 447.9339294433594 = 0.8051310181617737 + 50.0 * 8.942575454711914
Epoch 1590, val loss: 0.8192446827888489
Epoch 1600, training loss: 448.0574951171875 = 0.8016132116317749 + 50.0 * 8.945117950439453
Epoch 1600, val loss: 0.8159523010253906
Epoch 1610, training loss: 448.27178955078125 = 0.7981311678886414 + 50.0 * 8.94947338104248
Epoch 1610, val loss: 0.8127039074897766
Epoch 1620, training loss: 447.8852233886719 = 0.7945767641067505 + 50.0 * 8.941812515258789
Epoch 1620, val loss: 0.8093765377998352
Epoch 1630, training loss: 448.24395751953125 = 0.7910831570625305 + 50.0 * 8.949057579040527
Epoch 1630, val loss: 0.8061267733573914
Epoch 1640, training loss: 448.2132263183594 = 0.7876151204109192 + 50.0 * 8.948512077331543
Epoch 1640, val loss: 0.8028496503829956
Epoch 1650, training loss: 447.8479309082031 = 0.7840893864631653 + 50.0 * 8.941276550292969
Epoch 1650, val loss: 0.7995250225067139
Epoch 1660, training loss: 446.92083740234375 = 0.7806369066238403 + 50.0 * 8.92280387878418
Epoch 1660, val loss: 0.796352744102478
Epoch 1670, training loss: 445.93829345703125 = 0.7770472764968872 + 50.0 * 8.90322494506836
Epoch 1670, val loss: 0.7930007576942444
Epoch 1680, training loss: 446.8586730957031 = 0.7740098834037781 + 50.0 * 8.921692848205566
Epoch 1680, val loss: 0.7900990843772888
Epoch 1690, training loss: 447.1885070800781 = 0.7709184288978577 + 50.0 * 8.928352355957031
Epoch 1690, val loss: 0.7872945070266724
Epoch 1700, training loss: 446.44378662109375 = 0.7675353288650513 + 50.0 * 8.913524627685547
Epoch 1700, val loss: 0.7842126488685608
Epoch 1710, training loss: 446.6011047363281 = 0.7641289234161377 + 50.0 * 8.916739463806152
Epoch 1710, val loss: 0.7810162305831909
Epoch 1720, training loss: 447.60595703125 = 0.760792076587677 + 50.0 * 8.93690299987793
Epoch 1720, val loss: 0.7779296636581421
Epoch 1730, training loss: 447.94293212890625 = 0.7574055790901184 + 50.0 * 8.943710327148438
Epoch 1730, val loss: 0.7747890949249268
Epoch 1740, training loss: 448.2091064453125 = 0.7540377974510193 + 50.0 * 8.949101448059082
Epoch 1740, val loss: 0.7716482281684875
Epoch 1750, training loss: 448.3822021484375 = 0.7506516575813293 + 50.0 * 8.952630996704102
Epoch 1750, val loss: 0.7685096263885498
Epoch 1760, training loss: 448.4304504394531 = 0.7473036050796509 + 50.0 * 8.953662872314453
Epoch 1760, val loss: 0.765390932559967
Epoch 1770, training loss: 448.6047668457031 = 0.7439629435539246 + 50.0 * 8.957216262817383
Epoch 1770, val loss: 0.7622946500778198
Epoch 1780, training loss: 448.6739807128906 = 0.7406282424926758 + 50.0 * 8.958666801452637
Epoch 1780, val loss: 0.7592097520828247
Epoch 1790, training loss: 448.71966552734375 = 0.7372902631759644 + 50.0 * 8.959647178649902
Epoch 1790, val loss: 0.7561081051826477
Epoch 1800, training loss: 448.6932067871094 = 0.7339962124824524 + 50.0 * 8.959183692932129
Epoch 1800, val loss: 0.7530587315559387
Epoch 1810, training loss: 448.8758544921875 = 0.7307016253471375 + 50.0 * 8.962903022766113
Epoch 1810, val loss: 0.7500209212303162
Epoch 1820, training loss: 448.998779296875 = 0.7274342179298401 + 50.0 * 8.965426445007324
Epoch 1820, val loss: 0.747009813785553
Epoch 1830, training loss: 449.1367492675781 = 0.7241901755332947 + 50.0 * 8.96825122833252
Epoch 1830, val loss: 0.7439873218536377
Epoch 1840, training loss: 449.1412353515625 = 0.7209369540214539 + 50.0 * 8.968405723571777
Epoch 1840, val loss: 0.740979790687561
Epoch 1850, training loss: 449.341064453125 = 0.7177339792251587 + 50.0 * 8.972466468811035
Epoch 1850, val loss: 0.7380135655403137
Epoch 1860, training loss: 449.435791015625 = 0.7145367860794067 + 50.0 * 8.974425315856934
Epoch 1860, val loss: 0.7350713014602661
Epoch 1870, training loss: 449.3577575683594 = 0.7113118171691895 + 50.0 * 8.972929000854492
Epoch 1870, val loss: 0.7321161031723022
Epoch 1880, training loss: 449.4233093261719 = 0.7081514596939087 + 50.0 * 8.974303245544434
Epoch 1880, val loss: 0.7291938066482544
Epoch 1890, training loss: 449.580810546875 = 0.7050266265869141 + 50.0 * 8.977516174316406
Epoch 1890, val loss: 0.7263162732124329
Epoch 1900, training loss: 449.6767883300781 = 0.7019031643867493 + 50.0 * 8.979497909545898
Epoch 1900, val loss: 0.7234563231468201
Epoch 1910, training loss: 449.7481994628906 = 0.6988145709037781 + 50.0 * 8.980987548828125
Epoch 1910, val loss: 0.7205982804298401
Epoch 1920, training loss: 449.7093200683594 = 0.6957175731658936 + 50.0 * 8.98027229309082
Epoch 1920, val loss: 0.7177476286888123
Epoch 1930, training loss: 449.7198181152344 = 0.6926323175430298 + 50.0 * 8.980544090270996
Epoch 1930, val loss: 0.7149161696434021
Epoch 1940, training loss: 449.90924072265625 = 0.6895783543586731 + 50.0 * 8.984393119812012
Epoch 1940, val loss: 0.7121074199676514
Epoch 1950, training loss: 449.88690185546875 = 0.6865339875221252 + 50.0 * 8.984007835388184
Epoch 1950, val loss: 0.7093011140823364
Epoch 1960, training loss: 449.8610534667969 = 0.6835560202598572 + 50.0 * 8.983550071716309
Epoch 1960, val loss: 0.7065965533256531
Epoch 1970, training loss: 450.0486145019531 = 0.6805747151374817 + 50.0 * 8.987360954284668
Epoch 1970, val loss: 0.703853964805603
Epoch 1980, training loss: 450.3015441894531 = 0.6775951981544495 + 50.0 * 8.99247932434082
Epoch 1980, val loss: 0.7011005282402039
Epoch 1990, training loss: 450.2716064453125 = 0.6746270060539246 + 50.0 * 8.991939544677734
Epoch 1990, val loss: 0.6983857154846191
Epoch 2000, training loss: 450.3177490234375 = 0.6716727614402771 + 50.0 * 8.992921829223633
Epoch 2000, val loss: 0.6956877708435059
Epoch 2010, training loss: 449.7896728515625 = 0.6687650680541992 + 50.0 * 8.982418060302734
Epoch 2010, val loss: 0.6929965615272522
Epoch 2020, training loss: 444.7410888671875 = 0.665469765663147 + 50.0 * 8.881512641906738
Epoch 2020, val loss: 0.6897187829017639
Epoch 2030, training loss: 443.3448181152344 = 0.6634456515312195 + 50.0 * 8.85362720489502
Epoch 2030, val loss: 0.6883513927459717
Epoch 2040, training loss: 445.013671875 = 0.6608771085739136 + 50.0 * 8.887055397033691
Epoch 2040, val loss: 0.685796856880188
Epoch 2050, training loss: 445.5552673339844 = 0.6585811376571655 + 50.0 * 8.897933959960938
Epoch 2050, val loss: 0.6837798357009888
Epoch 2060, training loss: 444.7693176269531 = 0.6556398272514343 + 50.0 * 8.88227367401123
Epoch 2060, val loss: 0.6809060573577881
Epoch 2070, training loss: 446.70379638671875 = 0.6532648205757141 + 50.0 * 8.921010971069336
Epoch 2070, val loss: 0.678932249546051
Epoch 2080, training loss: 445.74945068359375 = 0.6505880355834961 + 50.0 * 8.9019775390625
Epoch 2080, val loss: 0.6764251589775085
Epoch 2090, training loss: 446.4916687011719 = 0.6481014490127563 + 50.0 * 8.916871070861816
Epoch 2090, val loss: 0.6742189526557922
Epoch 2100, training loss: 446.1758117675781 = 0.6452962160110474 + 50.0 * 8.91061019897461
Epoch 2100, val loss: 0.6717075705528259
Epoch 2110, training loss: 447.05926513671875 = 0.6427817940711975 + 50.0 * 8.928329467773438
Epoch 2110, val loss: 0.6694002747535706
Epoch 2120, training loss: 447.59063720703125 = 0.6402946710586548 + 50.0 * 8.939006805419922
Epoch 2120, val loss: 0.667161226272583
Epoch 2130, training loss: 447.8716735839844 = 0.637920081615448 + 50.0 * 8.94467544555664
Epoch 2130, val loss: 0.6650930047035217
Epoch 2140, training loss: 448.357177734375 = 0.6354027390480042 + 50.0 * 8.954435348510742
Epoch 2140, val loss: 0.6627653241157532
Epoch 2150, training loss: 448.78155517578125 = 0.6327993869781494 + 50.0 * 8.962974548339844
Epoch 2150, val loss: 0.6603653430938721
Epoch 2160, training loss: 448.9803466796875 = 0.6301611065864563 + 50.0 * 8.96700382232666
Epoch 2160, val loss: 0.6579672694206238
Epoch 2170, training loss: 449.28289794921875 = 0.6275634169578552 + 50.0 * 8.973106384277344
Epoch 2170, val loss: 0.6556123495101929
Epoch 2180, training loss: 449.4671325683594 = 0.6250349283218384 + 50.0 * 8.976841926574707
Epoch 2180, val loss: 0.6533262729644775
Epoch 2190, training loss: 449.4607238769531 = 0.6225234866142273 + 50.0 * 8.976763725280762
Epoch 2190, val loss: 0.6510494351387024
Epoch 2200, training loss: 449.5497131347656 = 0.6200456619262695 + 50.0 * 8.978592872619629
Epoch 2200, val loss: 0.6488152146339417
Epoch 2210, training loss: 449.8107604980469 = 0.6175990104675293 + 50.0 * 8.983863830566406
Epoch 2210, val loss: 0.6465997099876404
Epoch 2220, training loss: 449.812744140625 = 0.6151712536811829 + 50.0 * 8.983951568603516
Epoch 2220, val loss: 0.6444129943847656
Epoch 2230, training loss: 449.94012451171875 = 0.6127731800079346 + 50.0 * 8.986547470092773
Epoch 2230, val loss: 0.6422543525695801
Epoch 2240, training loss: 450.0041809082031 = 0.610390305519104 + 50.0 * 8.987875938415527
Epoch 2240, val loss: 0.6401127576828003
Epoch 2250, training loss: 449.1749267578125 = 0.6079944968223572 + 50.0 * 8.971338272094727
Epoch 2250, val loss: 0.6379490494728088
Epoch 2260, training loss: 449.2095947265625 = 0.6056724786758423 + 50.0 * 8.972078323364258
Epoch 2260, val loss: 0.6358945965766907
Epoch 2270, training loss: 449.64312744140625 = 0.6033905148506165 + 50.0 * 8.980794906616211
Epoch 2270, val loss: 0.6338638663291931
Epoch 2280, training loss: 449.98321533203125 = 0.6011369228363037 + 50.0 * 8.987641334533691
Epoch 2280, val loss: 0.6318579316139221
Epoch 2290, training loss: 450.2170104980469 = 0.5988926291465759 + 50.0 * 8.992362022399902
Epoch 2290, val loss: 0.6298611760139465
Epoch 2300, training loss: 450.07666015625 = 0.5966371893882751 + 50.0 * 8.98960018157959
Epoch 2300, val loss: 0.6278615593910217
Epoch 2310, training loss: 450.2005615234375 = 0.5944227576255798 + 50.0 * 8.992122650146484
Epoch 2310, val loss: 0.6258920431137085
Epoch 2320, training loss: 450.21136474609375 = 0.5922300219535828 + 50.0 * 8.992383003234863
Epoch 2320, val loss: 0.6239478588104248
Epoch 2330, training loss: 450.2823486328125 = 0.5900714993476868 + 50.0 * 8.99384593963623
Epoch 2330, val loss: 0.6220178604125977
Epoch 2340, training loss: 450.4437255859375 = 0.5879316329956055 + 50.0 * 8.997116088867188
Epoch 2340, val loss: 0.620140552520752
Epoch 2350, training loss: 450.32275390625 = 0.5858076214790344 + 50.0 * 8.994738578796387
Epoch 2350, val loss: 0.6182627081871033
Epoch 2360, training loss: 450.4956970214844 = 0.5837231278419495 + 50.0 * 8.998239517211914
Epoch 2360, val loss: 0.6164213418960571
Epoch 2370, training loss: 450.6288757324219 = 0.58164381980896 + 50.0 * 9.000945091247559
Epoch 2370, val loss: 0.614590048789978
Epoch 2380, training loss: 450.424560546875 = 0.5796231627464294 + 50.0 * 8.996898651123047
Epoch 2380, val loss: 0.6128426790237427
Epoch 2390, training loss: 450.6048889160156 = 0.5776669383049011 + 50.0 * 9.000544548034668
Epoch 2390, val loss: 0.6111074090003967
Epoch 2400, training loss: 450.6578674316406 = 0.5756608247756958 + 50.0 * 9.001644134521484
Epoch 2400, val loss: 0.6093587875366211
Epoch 2410, training loss: 450.8552551269531 = 0.5736742615699768 + 50.0 * 9.005631446838379
Epoch 2410, val loss: 0.6076019406318665
Epoch 2420, training loss: 450.8629455566406 = 0.5716860890388489 + 50.0 * 9.00582504272461
Epoch 2420, val loss: 0.6058715581893921
Epoch 2430, training loss: 450.67840576171875 = 0.5697384476661682 + 50.0 * 9.00217342376709
Epoch 2430, val loss: 0.604185163974762
Epoch 2440, training loss: 450.7693176269531 = 0.5678212642669678 + 50.0 * 9.004030227661133
Epoch 2440, val loss: 0.6025071144104004
Epoch 2450, training loss: 450.94989013671875 = 0.5659337639808655 + 50.0 * 9.007678985595703
Epoch 2450, val loss: 0.6008632183074951
Epoch 2460, training loss: 451.0970764160156 = 0.5640482902526855 + 50.0 * 9.010660171508789
Epoch 2460, val loss: 0.5992440581321716
Epoch 2470, training loss: 450.9837646484375 = 0.562181293964386 + 50.0 * 9.008431434631348
Epoch 2470, val loss: 0.5976316332817078
Epoch 2480, training loss: 450.1810302734375 = 0.5603758096694946 + 50.0 * 8.992413520812988
Epoch 2480, val loss: 0.5960884690284729
Epoch 2490, training loss: 450.06396484375 = 0.5586098432540894 + 50.0 * 8.990106582641602
Epoch 2490, val loss: 0.5945619344711304
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8021739130434783
0.8643048612620445
=== training gcn model ===
Epoch 0, training loss: 515.8701782226562 = 1.0986114740371704 + 50.0 * 10.295431137084961
Epoch 0, val loss: 1.098612904548645
Epoch 10, training loss: 494.6317443847656 = 1.0986114740371704 + 50.0 * 9.870662689208984
Epoch 10, val loss: 1.098612904548645
Epoch 20, training loss: 483.43426513671875 = 1.0986114740371704 + 50.0 * 9.646713256835938
Epoch 20, val loss: 1.098612904548645
Epoch 30, training loss: 475.5567626953125 = 1.0986114740371704 + 50.0 * 9.489163398742676
Epoch 30, val loss: 1.098612904548645
Epoch 40, training loss: 469.40386962890625 = 1.0986114740371704 + 50.0 * 9.366105079650879
Epoch 40, val loss: 1.098612904548645
Epoch 50, training loss: 464.52105712890625 = 1.0986114740371704 + 50.0 * 9.268448829650879
Epoch 50, val loss: 1.098612904548645
Epoch 60, training loss: 460.5187683105469 = 1.0986114740371704 + 50.0 * 9.188403129577637
Epoch 60, val loss: 1.098612904548645
Epoch 70, training loss: 457.1705017089844 = 1.0986114740371704 + 50.0 * 9.121438026428223
Epoch 70, val loss: 1.098612904548645
Epoch 80, training loss: 454.3979187011719 = 1.0986114740371704 + 50.0 * 9.065986633300781
Epoch 80, val loss: 1.098612904548645
Epoch 90, training loss: 452.0120849609375 = 1.0986114740371704 + 50.0 * 9.018269538879395
Epoch 90, val loss: 1.098612904548645
Epoch 100, training loss: 450.0001220703125 = 1.0986114740371704 + 50.0 * 8.97803020477295
Epoch 100, val loss: 1.098612904548645
Epoch 110, training loss: 448.2460632324219 = 1.0986114740371704 + 50.0 * 8.942949295043945
Epoch 110, val loss: 1.098612904548645
Epoch 120, training loss: 446.72186279296875 = 1.0986114740371704 + 50.0 * 8.91246509552002
Epoch 120, val loss: 1.098612904548645
Epoch 130, training loss: 445.3582458496094 = 1.0986114740371704 + 50.0 * 8.88519287109375
Epoch 130, val loss: 1.098612904548645
Epoch 140, training loss: 444.2362365722656 = 1.0986114740371704 + 50.0 * 8.862752914428711
Epoch 140, val loss: 1.098612904548645
Epoch 150, training loss: 443.15802001953125 = 1.0986114740371704 + 50.0 * 8.841188430786133
Epoch 150, val loss: 1.098612904548645
Epoch 160, training loss: 442.3019104003906 = 1.0986114740371704 + 50.0 * 8.824066162109375
Epoch 160, val loss: 1.098612904548645
Epoch 170, training loss: 441.516357421875 = 1.0986114740371704 + 50.0 * 8.808355331420898
Epoch 170, val loss: 1.098612904548645
Epoch 180, training loss: 440.8152770996094 = 1.0986114740371704 + 50.0 * 8.794333457946777
Epoch 180, val loss: 1.098612904548645
Epoch 190, training loss: 440.1898193359375 = 1.0986114740371704 + 50.0 * 8.781824111938477
Epoch 190, val loss: 1.098612904548645
Epoch 200, training loss: 439.8367919921875 = 1.0986114740371704 + 50.0 * 8.774764060974121
Epoch 200, val loss: 1.098612904548645
Epoch 210, training loss: 439.1402282714844 = 1.0986114740371704 + 50.0 * 8.760832786560059
Epoch 210, val loss: 1.098612904548645
Epoch 220, training loss: 438.928955078125 = 1.0986114740371704 + 50.0 * 8.756607055664062
Epoch 220, val loss: 1.098612904548645
Epoch 230, training loss: 438.3492126464844 = 1.0986114740371704 + 50.0 * 8.745012283325195
Epoch 230, val loss: 1.098612904548645
Epoch 240, training loss: 438.23748779296875 = 1.0986114740371704 + 50.0 * 8.742777824401855
Epoch 240, val loss: 1.098612904548645
Epoch 250, training loss: 437.92840576171875 = 1.0986114740371704 + 50.0 * 8.73659610748291
Epoch 250, val loss: 1.098612904548645
Epoch 260, training loss: 437.80218505859375 = 1.0986114740371704 + 50.0 * 8.734071731567383
Epoch 260, val loss: 1.098612904548645
Epoch 270, training loss: 437.6142578125 = 1.0986114740371704 + 50.0 * 8.730313301086426
Epoch 270, val loss: 1.098612904548645
Epoch 280, training loss: 437.3800354003906 = 1.0986114740371704 + 50.0 * 8.725628852844238
Epoch 280, val loss: 1.098612904548645
Epoch 290, training loss: 437.0787658691406 = 1.0986114740371704 + 50.0 * 8.719603538513184
Epoch 290, val loss: 1.098612904548645
Epoch 300, training loss: 437.0113525390625 = 1.0986108779907227 + 50.0 * 8.718255043029785
Epoch 300, val loss: 1.0986112356185913
Epoch 310, training loss: 437.0001525878906 = 1.0986106395721436 + 50.0 * 8.71803092956543
Epoch 310, val loss: 1.0986120700836182
Epoch 320, training loss: 437.0333251953125 = 1.098610520362854 + 50.0 * 8.718694686889648
Epoch 320, val loss: 1.0986119508743286
Epoch 330, training loss: 436.97125244140625 = 1.0986075401306152 + 50.0 * 8.717453002929688
Epoch 330, val loss: 1.0986071825027466
Epoch 340, training loss: 436.8857727050781 = 1.098565697669983 + 50.0 * 8.715744018554688
Epoch 340, val loss: 1.0985528230667114
Epoch 350, training loss: 436.9056091308594 = 1.0984888076782227 + 50.0 * 8.716142654418945
Epoch 350, val loss: 1.0984749794006348
Epoch 360, training loss: 436.9018859863281 = 1.09840726852417 + 50.0 * 8.716069221496582
Epoch 360, val loss: 1.0983848571777344
Epoch 370, training loss: 436.74737548828125 = 1.098319411277771 + 50.0 * 8.712981224060059
Epoch 370, val loss: 1.0982944965362549
Epoch 380, training loss: 436.6527404785156 = 1.0982260704040527 + 50.0 * 8.711090087890625
Epoch 380, val loss: 1.0982012748718262
Epoch 390, training loss: 436.7489929199219 = 1.098126769065857 + 50.0 * 8.713017463684082
Epoch 390, val loss: 1.098101019859314
Epoch 400, training loss: 436.7090759277344 = 1.0980207920074463 + 50.0 * 8.712221145629883
Epoch 400, val loss: 1.0979949235916138
Epoch 410, training loss: 436.83831787109375 = 1.097901463508606 + 50.0 * 8.714808464050293
Epoch 410, val loss: 1.0978801250457764
Epoch 420, training loss: 436.8741455078125 = 1.097777247428894 + 50.0 * 8.715527534484863
Epoch 420, val loss: 1.0977569818496704
Epoch 430, training loss: 437.0526123046875 = 1.0976431369781494 + 50.0 * 8.719099044799805
Epoch 430, val loss: 1.0976263284683228
Epoch 440, training loss: 437.0744934082031 = 1.097503423690796 + 50.0 * 8.719539642333984
Epoch 440, val loss: 1.0974903106689453
Epoch 450, training loss: 436.93377685546875 = 1.0973466634750366 + 50.0 * 8.716728210449219
Epoch 450, val loss: 1.0973347425460815
Epoch 460, training loss: 437.0472412109375 = 1.0971872806549072 + 50.0 * 8.719000816345215
Epoch 460, val loss: 1.0971788167953491
Epoch 470, training loss: 437.2450866699219 = 1.097019910812378 + 50.0 * 8.72296142578125
Epoch 470, val loss: 1.0970168113708496
Epoch 480, training loss: 437.2715148925781 = 1.0968462228775024 + 50.0 * 8.723493576049805
Epoch 480, val loss: 1.096846580505371
Epoch 490, training loss: 437.4082946777344 = 1.0966670513153076 + 50.0 * 8.726232528686523
Epoch 490, val loss: 1.0966745615005493
Epoch 500, training loss: 437.5712890625 = 1.096482515335083 + 50.0 * 8.729496002197266
Epoch 500, val loss: 1.0964939594268799
Epoch 510, training loss: 437.6366882324219 = 1.0962936878204346 + 50.0 * 8.73080825805664
Epoch 510, val loss: 1.0963056087493896
Epoch 520, training loss: 437.7862854003906 = 1.0960930585861206 + 50.0 * 8.733803749084473
Epoch 520, val loss: 1.0961109399795532
Epoch 530, training loss: 437.53131103515625 = 1.0958882570266724 + 50.0 * 8.728708267211914
Epoch 530, val loss: 1.0959078073501587
Epoch 540, training loss: 437.2578125 = 1.095652461051941 + 50.0 * 8.723243713378906
Epoch 540, val loss: 1.0956838130950928
Epoch 550, training loss: 437.591064453125 = 1.0954383611679077 + 50.0 * 8.729912757873535
Epoch 550, val loss: 1.0954737663269043
Epoch 560, training loss: 437.6211853027344 = 1.0952054262161255 + 50.0 * 8.73051929473877
Epoch 560, val loss: 1.0952372550964355
Epoch 570, training loss: 437.5747375488281 = 1.0949827432632446 + 50.0 * 8.729595184326172
Epoch 570, val loss: 1.095015048980713
Epoch 580, training loss: 437.95184326171875 = 1.094746708869934 + 50.0 * 8.737141609191895
Epoch 580, val loss: 1.0947831869125366
Epoch 590, training loss: 438.03680419921875 = 1.094504475593567 + 50.0 * 8.738845825195312
Epoch 590, val loss: 1.0945463180541992
Epoch 600, training loss: 438.3694152832031 = 1.094253420829773 + 50.0 * 8.745503425598145
Epoch 600, val loss: 1.0942968130111694
Epoch 610, training loss: 438.4589538574219 = 1.0939947366714478 + 50.0 * 8.747299194335938
Epoch 610, val loss: 1.0940396785736084
Epoch 620, training loss: 438.5763244628906 = 1.0937303304672241 + 50.0 * 8.749651908874512
Epoch 620, val loss: 1.0937858819961548
Epoch 630, training loss: 438.7103576660156 = 1.0934616327285767 + 50.0 * 8.752337455749512
Epoch 630, val loss: 1.0935189723968506
Epoch 640, training loss: 438.93450927734375 = 1.0931892395019531 + 50.0 * 8.756826400756836
Epoch 640, val loss: 1.0932492017745972
Epoch 650, training loss: 439.0728454589844 = 1.0929040908813477 + 50.0 * 8.759598731994629
Epoch 650, val loss: 1.092976689338684
Epoch 660, training loss: 438.9310607910156 = 1.0926038026809692 + 50.0 * 8.756769180297852
Epoch 660, val loss: 1.0926860570907593
Epoch 670, training loss: 438.9331970214844 = 1.092318058013916 + 50.0 * 8.756817817687988
Epoch 670, val loss: 1.0924023389816284
Epoch 680, training loss: 438.89886474609375 = 1.092024803161621 + 50.0 * 8.756136894226074
Epoch 680, val loss: 1.0921194553375244
Epoch 690, training loss: 439.3360290527344 = 1.091732382774353 + 50.0 * 8.764885902404785
Epoch 690, val loss: 1.0918307304382324
Epoch 700, training loss: 439.58941650390625 = 1.0914380550384521 + 50.0 * 8.769959449768066
Epoch 700, val loss: 1.0915398597717285
Epoch 710, training loss: 439.77825927734375 = 1.091133952140808 + 50.0 * 8.77374267578125
Epoch 710, val loss: 1.0912445783615112
Epoch 720, training loss: 439.88018798828125 = 1.0908256769180298 + 50.0 * 8.775787353515625
Epoch 720, val loss: 1.090942621231079
Epoch 730, training loss: 439.4677734375 = 1.0904844999313354 + 50.0 * 8.767545700073242
Epoch 730, val loss: 1.09060800075531
Epoch 740, training loss: 442.859130859375 = 1.0902130603790283 + 50.0 * 8.835378646850586
Epoch 740, val loss: 1.0903047323226929
Epoch 750, training loss: 437.0646667480469 = 1.0897061824798584 + 50.0 * 8.719499588012695
Epoch 750, val loss: 1.089875340461731
Epoch 760, training loss: 439.19952392578125 = 1.089592695236206 + 50.0 * 8.762198448181152
Epoch 760, val loss: 1.0897469520568848
Epoch 770, training loss: 439.21514892578125 = 1.0892611742019653 + 50.0 * 8.762517929077148
Epoch 770, val loss: 1.0894113779067993
Epoch 780, training loss: 438.98822021484375 = 1.0889452695846558 + 50.0 * 8.75798511505127
Epoch 780, val loss: 1.0891146659851074
Epoch 790, training loss: 439.0946350097656 = 1.0886396169662476 + 50.0 * 8.760119438171387
Epoch 790, val loss: 1.0888121128082275
Epoch 800, training loss: 439.7284851074219 = 1.088315725326538 + 50.0 * 8.77280330657959
Epoch 800, val loss: 1.0884941816329956
Epoch 810, training loss: 440.05419921875 = 1.087990164756775 + 50.0 * 8.779324531555176
Epoch 810, val loss: 1.0881788730621338
Epoch 820, training loss: 440.5661315917969 = 1.0876609086990356 + 50.0 * 8.789569854736328
Epoch 820, val loss: 1.0878572463989258
Epoch 830, training loss: 440.7441101074219 = 1.087328314781189 + 50.0 * 8.793135643005371
Epoch 830, val loss: 1.0875349044799805
Epoch 840, training loss: 441.02264404296875 = 1.0869903564453125 + 50.0 * 8.798712730407715
Epoch 840, val loss: 1.0872057676315308
Epoch 850, training loss: 441.1220703125 = 1.086641788482666 + 50.0 * 8.800708770751953
Epoch 850, val loss: 1.086866021156311
Epoch 860, training loss: 441.5569152832031 = 1.0863049030303955 + 50.0 * 8.809412002563477
Epoch 860, val loss: 1.0865395069122314
Epoch 870, training loss: 441.75946044921875 = 1.0859624147415161 + 50.0 * 8.813469886779785
Epoch 870, val loss: 1.0861977338790894
Epoch 880, training loss: 441.8544616699219 = 1.0856059789657593 + 50.0 * 8.815377235412598
Epoch 880, val loss: 1.085858941078186
Epoch 890, training loss: 442.10888671875 = 1.0852618217468262 + 50.0 * 8.820472717285156
Epoch 890, val loss: 1.085522174835205
Epoch 900, training loss: 442.40618896484375 = 1.0849086046218872 + 50.0 * 8.826425552368164
Epoch 900, val loss: 1.0851762294769287
Epoch 910, training loss: 442.36358642578125 = 1.084549903869629 + 50.0 * 8.825580596923828
Epoch 910, val loss: 1.0848337411880493
Epoch 920, training loss: 442.6866760253906 = 1.0841976404190063 + 50.0 * 8.832049369812012
Epoch 920, val loss: 1.084486484527588
Epoch 930, training loss: 442.885009765625 = 1.0838406085968018 + 50.0 * 8.836023330688477
Epoch 930, val loss: 1.0841364860534668
Epoch 940, training loss: 442.9090576171875 = 1.083475112915039 + 50.0 * 8.836511611938477
Epoch 940, val loss: 1.0837808847427368
Epoch 950, training loss: 443.0038146972656 = 1.0831553936004639 + 50.0 * 8.83841323852539
Epoch 950, val loss: 1.0834704637527466
Epoch 960, training loss: 443.13873291015625 = 1.0828070640563965 + 50.0 * 8.841118812561035
Epoch 960, val loss: 1.0831284523010254
Epoch 970, training loss: 443.5336608886719 = 1.0824313163757324 + 50.0 * 8.849024772644043
Epoch 970, val loss: 1.0827643871307373
Epoch 980, training loss: 443.7831115722656 = 1.0820618867874146 + 50.0 * 8.854021072387695
Epoch 980, val loss: 1.0824007987976074
Epoch 990, training loss: 443.6566467285156 = 1.0816500186920166 + 50.0 * 8.851499557495117
Epoch 990, val loss: 1.081998586654663
Epoch 1000, training loss: 443.614501953125 = 1.0812524557113647 + 50.0 * 8.850665092468262
Epoch 1000, val loss: 1.0816155672073364
Epoch 1010, training loss: 443.88519287109375 = 1.0808881521224976 + 50.0 * 8.856085777282715
Epoch 1010, val loss: 1.081254005432129
Epoch 1020, training loss: 443.9423828125 = 1.0805037021636963 + 50.0 * 8.857237815856934
Epoch 1020, val loss: 1.080876111984253
Epoch 1030, training loss: 444.472900390625 = 1.080133318901062 + 50.0 * 8.867855072021484
Epoch 1030, val loss: 1.080513834953308
Epoch 1040, training loss: 444.2780456542969 = 1.0797367095947266 + 50.0 * 8.86396598815918
Epoch 1040, val loss: 1.0801327228546143
Epoch 1050, training loss: 444.5652160644531 = 1.0793581008911133 + 50.0 * 8.869717597961426
Epoch 1050, val loss: 1.0797611474990845
Epoch 1060, training loss: 444.6204528808594 = 1.0789684057235718 + 50.0 * 8.870829582214355
Epoch 1060, val loss: 1.0793792009353638
Epoch 1070, training loss: 445.0623779296875 = 1.0785771608352661 + 50.0 * 8.87967586517334
Epoch 1070, val loss: 1.0789945125579834
Epoch 1080, training loss: 445.1036682128906 = 1.0781620740890503 + 50.0 * 8.880510330200195
Epoch 1080, val loss: 1.0785824060440063
Epoch 1090, training loss: 445.1114501953125 = 1.0777403116226196 + 50.0 * 8.880674362182617
Epoch 1090, val loss: 1.078162431716919
Epoch 1100, training loss: 444.92156982421875 = 1.0772963762283325 + 50.0 * 8.876885414123535
Epoch 1100, val loss: 1.077723503112793
Epoch 1110, training loss: 445.3576965332031 = 1.076852560043335 + 50.0 * 8.88561725616455
Epoch 1110, val loss: 1.0772783756256104
Epoch 1120, training loss: 445.6347351074219 = 1.076396107673645 + 50.0 * 8.891166687011719
Epoch 1120, val loss: 1.0768382549285889
Epoch 1130, training loss: 445.4650573730469 = 1.0759702920913696 + 50.0 * 8.887782096862793
Epoch 1130, val loss: 1.0764245986938477
Epoch 1140, training loss: 445.4976806640625 = 1.0755157470703125 + 50.0 * 8.888442993164062
Epoch 1140, val loss: 1.07597815990448
Epoch 1150, training loss: 445.4513244628906 = 1.0750384330749512 + 50.0 * 8.88752555847168
Epoch 1150, val loss: 1.0755029916763306
Epoch 1160, training loss: 445.1751403808594 = 1.074544906616211 + 50.0 * 8.882011413574219
Epoch 1160, val loss: 1.0750041007995605
Epoch 1170, training loss: 446.01568603515625 = 1.0740729570388794 + 50.0 * 8.898832321166992
Epoch 1170, val loss: 1.0745456218719482
Epoch 1180, training loss: 445.78173828125 = 1.073585867881775 + 50.0 * 8.894163131713867
Epoch 1180, val loss: 1.074076533317566
Epoch 1190, training loss: 446.2516784667969 = 1.0731114149093628 + 50.0 * 8.903571128845215
Epoch 1190, val loss: 1.0736169815063477
Epoch 1200, training loss: 446.4227294921875 = 1.0726474523544312 + 50.0 * 8.907001495361328
Epoch 1200, val loss: 1.0731667280197144
Epoch 1210, training loss: 446.484375 = 1.0721728801727295 + 50.0 * 8.908244132995605
Epoch 1210, val loss: 1.0727102756500244
Epoch 1220, training loss: 446.38214111328125 = 1.0716995000839233 + 50.0 * 8.906208992004395
Epoch 1220, val loss: 1.0722475051879883
Epoch 1230, training loss: 446.270263671875 = 1.0712517499923706 + 50.0 * 8.903980255126953
Epoch 1230, val loss: 1.0718108415603638
Epoch 1240, training loss: 446.5466003417969 = 1.070766806602478 + 50.0 * 8.909516334533691
Epoch 1240, val loss: 1.0713471174240112
Epoch 1250, training loss: 446.6177673339844 = 1.0702760219573975 + 50.0 * 8.91094970703125
Epoch 1250, val loss: 1.0708632469177246
Epoch 1260, training loss: 446.8330383300781 = 1.069800615310669 + 50.0 * 8.915265083312988
Epoch 1260, val loss: 1.0703961849212646
Epoch 1270, training loss: 447.1172790527344 = 1.0693124532699585 + 50.0 * 8.92095947265625
Epoch 1270, val loss: 1.069919228553772
Epoch 1280, training loss: 447.2795715332031 = 1.068824052810669 + 50.0 * 8.924215316772461
Epoch 1280, val loss: 1.069446325302124
Epoch 1290, training loss: 447.56439208984375 = 1.0683428049087524 + 50.0 * 8.92992115020752
Epoch 1290, val loss: 1.0689764022827148
Epoch 1300, training loss: 447.6003112792969 = 1.0678565502166748 + 50.0 * 8.930648803710938
Epoch 1300, val loss: 1.0684983730316162
Epoch 1310, training loss: 447.5824890136719 = 1.067363977432251 + 50.0 * 8.930302619934082
Epoch 1310, val loss: 1.0680265426635742
Epoch 1320, training loss: 447.8141784667969 = 1.0668718814849854 + 50.0 * 8.934946060180664
Epoch 1320, val loss: 1.0675467252731323
Epoch 1330, training loss: 447.8404235839844 = 1.0663793087005615 + 50.0 * 8.935481071472168
Epoch 1330, val loss: 1.0670655965805054
Epoch 1340, training loss: 447.91021728515625 = 1.0658857822418213 + 50.0 * 8.93688678741455
Epoch 1340, val loss: 1.0665843486785889
Epoch 1350, training loss: 447.9162292480469 = 1.0653938055038452 + 50.0 * 8.937016487121582
Epoch 1350, val loss: 1.0661064386367798
Epoch 1360, training loss: 448.0857238769531 = 1.0648999214172363 + 50.0 * 8.94041633605957
Epoch 1360, val loss: 1.0656217336654663
Epoch 1370, training loss: 448.252197265625 = 1.0644029378890991 + 50.0 * 8.943756103515625
Epoch 1370, val loss: 1.065142273902893
Epoch 1380, training loss: 448.26275634765625 = 1.0639034509658813 + 50.0 * 8.943977355957031
Epoch 1380, val loss: 1.0646535158157349
Epoch 1390, training loss: 448.1919860839844 = 1.0634021759033203 + 50.0 * 8.942571640014648
Epoch 1390, val loss: 1.0641590356826782
Epoch 1400, training loss: 448.24847412109375 = 1.0629013776779175 + 50.0 * 8.943711280822754
Epoch 1400, val loss: 1.0636833906173706
Epoch 1410, training loss: 448.4470520019531 = 1.0624061822891235 + 50.0 * 8.94769287109375
Epoch 1410, val loss: 1.0631992816925049
Epoch 1420, training loss: 448.5193176269531 = 1.06190824508667 + 50.0 * 8.949148178100586
Epoch 1420, val loss: 1.062713384628296
Epoch 1430, training loss: 448.5455627441406 = 1.0614020824432373 + 50.0 * 8.94968318939209
Epoch 1430, val loss: 1.062216877937317
Epoch 1440, training loss: 448.80731201171875 = 1.0608958005905151 + 50.0 * 8.954928398132324
Epoch 1440, val loss: 1.0617189407348633
Epoch 1450, training loss: 448.5226135253906 = 1.0603797435760498 + 50.0 * 8.949244499206543
Epoch 1450, val loss: 1.061218023300171
Epoch 1460, training loss: 449.2878723144531 = 1.059891939163208 + 50.0 * 8.964559555053711
Epoch 1460, val loss: 1.0607489347457886
Epoch 1470, training loss: 448.6932678222656 = 1.0593726634979248 + 50.0 * 8.952677726745605
Epoch 1470, val loss: 1.0602301359176636
Epoch 1480, training loss: 448.7963562011719 = 1.0588773488998413 + 50.0 * 8.954750061035156
Epoch 1480, val loss: 1.059747338294983
Epoch 1490, training loss: 448.84674072265625 = 1.058351755142212 + 50.0 * 8.955767631530762
Epoch 1490, val loss: 1.0592464208602905
Epoch 1500, training loss: 449.0684509277344 = 1.0578579902648926 + 50.0 * 8.960211753845215
Epoch 1500, val loss: 1.0587573051452637
Epoch 1510, training loss: 449.1501770019531 = 1.0573623180389404 + 50.0 * 8.9618558883667
Epoch 1510, val loss: 1.0582771301269531
Epoch 1520, training loss: 449.44366455078125 = 1.056867003440857 + 50.0 * 8.96773624420166
Epoch 1520, val loss: 1.0577902793884277
Epoch 1530, training loss: 449.5354309082031 = 1.0563545227050781 + 50.0 * 8.969581604003906
Epoch 1530, val loss: 1.0572938919067383
Epoch 1540, training loss: 449.4949035644531 = 1.055831789970398 + 50.0 * 8.968781471252441
Epoch 1540, val loss: 1.0567803382873535
Epoch 1550, training loss: 449.8130798339844 = 1.0553233623504639 + 50.0 * 8.975154876708984
Epoch 1550, val loss: 1.056287169456482
Epoch 1560, training loss: 449.802490234375 = 1.054805040359497 + 50.0 * 8.974953651428223
Epoch 1560, val loss: 1.0557752847671509
Epoch 1570, training loss: 449.81768798828125 = 1.0542707443237305 + 50.0 * 8.975268363952637
Epoch 1570, val loss: 1.0552573204040527
Epoch 1580, training loss: 449.879638671875 = 1.0537467002868652 + 50.0 * 8.976517677307129
Epoch 1580, val loss: 1.0547518730163574
Epoch 1590, training loss: 450.064697265625 = 1.0532279014587402 + 50.0 * 8.980229377746582
Epoch 1590, val loss: 1.0542398691177368
Epoch 1600, training loss: 450.1496276855469 = 1.0526912212371826 + 50.0 * 8.981938362121582
Epoch 1600, val loss: 1.053724765777588
Epoch 1610, training loss: 450.2336120605469 = 1.0521628856658936 + 50.0 * 8.98362922668457
Epoch 1610, val loss: 1.0532130002975464
Epoch 1620, training loss: 450.2646484375 = 1.0516256093978882 + 50.0 * 8.984260559082031
Epoch 1620, val loss: 1.0526916980743408
Epoch 1630, training loss: 450.4036865234375 = 1.0510889291763306 + 50.0 * 8.987051963806152
Epoch 1630, val loss: 1.0521730184555054
Epoch 1640, training loss: 450.3857421875 = 1.0505454540252686 + 50.0 * 8.986703872680664
Epoch 1640, val loss: 1.0516510009765625
Epoch 1650, training loss: 450.67822265625 = 1.0500034093856812 + 50.0 * 8.99256420135498
Epoch 1650, val loss: 1.0511260032653809
Epoch 1660, training loss: 450.81988525390625 = 1.0494507551193237 + 50.0 * 8.99540901184082
Epoch 1660, val loss: 1.0506031513214111
Epoch 1670, training loss: 450.9264831542969 = 1.048913598060608 + 50.0 * 8.997550964355469
Epoch 1670, val loss: 1.0500738620758057
Epoch 1680, training loss: 450.1108703613281 = 1.0483362674713135 + 50.0 * 8.981250762939453
Epoch 1680, val loss: 1.0495339632034302
Epoch 1690, training loss: 449.86395263671875 = 1.047792911529541 + 50.0 * 8.976323127746582
Epoch 1690, val loss: 1.0489903688430786
Epoch 1700, training loss: 449.8133239746094 = 1.0472356081008911 + 50.0 * 8.975321769714355
Epoch 1700, val loss: 1.0484610795974731
Epoch 1710, training loss: 450.37237548828125 = 1.0467147827148438 + 50.0 * 8.986513137817383
Epoch 1710, val loss: 1.0479573011398315
Epoch 1720, training loss: 450.7610168457031 = 1.0461835861206055 + 50.0 * 8.99429702758789
Epoch 1720, val loss: 1.0474475622177124
Epoch 1730, training loss: 450.90374755859375 = 1.0456492900848389 + 50.0 * 8.997161865234375
Epoch 1730, val loss: 1.0469334125518799
Epoch 1740, training loss: 451.0234680175781 = 1.0451078414916992 + 50.0 * 8.999567031860352
Epoch 1740, val loss: 1.0464216470718384
Epoch 1750, training loss: 451.20166015625 = 1.0445715188980103 + 50.0 * 9.003141403198242
Epoch 1750, val loss: 1.045906662940979
Epoch 1760, training loss: 451.3038024902344 = 1.0440373420715332 + 50.0 * 9.005195617675781
Epoch 1760, val loss: 1.0453968048095703
Epoch 1770, training loss: 451.3350830078125 = 1.0435007810592651 + 50.0 * 9.005831718444824
Epoch 1770, val loss: 1.0448813438415527
Epoch 1780, training loss: 451.3028869628906 = 1.0429598093032837 + 50.0 * 9.00519847869873
Epoch 1780, val loss: 1.04436194896698
Epoch 1790, training loss: 451.4539794921875 = 1.0424314737319946 + 50.0 * 9.008231163024902
Epoch 1790, val loss: 1.0438525676727295
Epoch 1800, training loss: 451.5172119140625 = 1.0418930053710938 + 50.0 * 9.009506225585938
Epoch 1800, val loss: 1.0433269739151
Epoch 1810, training loss: 451.4707336425781 = 1.0413599014282227 + 50.0 * 9.008587837219238
Epoch 1810, val loss: 1.042816400527954
Epoch 1820, training loss: 451.6436462402344 = 1.0408273935317993 + 50.0 * 9.012056350708008
Epoch 1820, val loss: 1.0423120260238647
Epoch 1830, training loss: 451.95458984375 = 1.0403075218200684 + 50.0 * 9.018285751342773
Epoch 1830, val loss: 1.0418033599853516
Epoch 1840, training loss: 449.9917907714844 = 1.0397320985794067 + 50.0 * 8.97904109954834
Epoch 1840, val loss: 1.0412856340408325
Epoch 1850, training loss: 450.6062927246094 = 1.0392173528671265 + 50.0 * 8.991341590881348
Epoch 1850, val loss: 1.0407226085662842
Epoch 1860, training loss: 450.2294921875 = 1.038689374923706 + 50.0 * 8.983816146850586
Epoch 1860, val loss: 1.0402495861053467
Epoch 1870, training loss: 449.7649230957031 = 1.0381855964660645 + 50.0 * 8.97453498840332
Epoch 1870, val loss: 1.0397779941558838
Epoch 1880, training loss: 449.84954833984375 = 1.0376957654953003 + 50.0 * 8.976237297058105
Epoch 1880, val loss: 1.0392893552780151
Epoch 1890, training loss: 449.558349609375 = 1.0371792316436768 + 50.0 * 8.970423698425293
Epoch 1890, val loss: 1.0388011932373047
Epoch 1900, training loss: 450.2464599609375 = 1.0366909503936768 + 50.0 * 8.984195709228516
Epoch 1900, val loss: 1.038328766822815
Epoch 1910, training loss: 450.8229675292969 = 1.0361607074737549 + 50.0 * 8.995736122131348
Epoch 1910, val loss: 1.0378400087356567
Epoch 1920, training loss: 451.0770263671875 = 1.0356483459472656 + 50.0 * 9.00082778930664
Epoch 1920, val loss: 1.0373469591140747
Epoch 1930, training loss: 451.5586853027344 = 1.0351481437683105 + 50.0 * 9.010470390319824
Epoch 1930, val loss: 1.0368719100952148
Epoch 1940, training loss: 451.8285827636719 = 1.034637451171875 + 50.0 * 9.015878677368164
Epoch 1940, val loss: 1.0363855361938477
Epoch 1950, training loss: 452.0074462890625 = 1.0341224670410156 + 50.0 * 9.019466400146484
Epoch 1950, val loss: 1.0358998775482178
Epoch 1960, training loss: 452.1233215332031 = 1.0336143970489502 + 50.0 * 9.021794319152832
Epoch 1960, val loss: 1.0354188680648804
Epoch 1970, training loss: 452.1365661621094 = 1.0330997705459595 + 50.0 * 9.022068977355957
Epoch 1970, val loss: 1.0349318981170654
Epoch 1980, training loss: 452.29364013671875 = 1.032602310180664 + 50.0 * 9.02522087097168
Epoch 1980, val loss: 1.0344576835632324
Epoch 1990, training loss: 452.3447265625 = 1.032096028327942 + 50.0 * 9.026252746582031
Epoch 1990, val loss: 1.0339758396148682
Epoch 2000, training loss: 452.33502197265625 = 1.0315895080566406 + 50.0 * 9.026068687438965
Epoch 2000, val loss: 1.0334899425506592
Epoch 2010, training loss: 452.53851318359375 = 1.0310909748077393 + 50.0 * 9.03014850616455
Epoch 2010, val loss: 1.0330257415771484
Epoch 2020, training loss: 452.71881103515625 = 1.0306024551391602 + 50.0 * 9.033763885498047
Epoch 2020, val loss: 1.0325579643249512
Epoch 2030, training loss: 452.1783142089844 = 1.0301036834716797 + 50.0 * 9.022964477539062
Epoch 2030, val loss: 1.0320974588394165
Epoch 2040, training loss: 452.49212646484375 = 1.0296216011047363 + 50.0 * 9.029250144958496
Epoch 2040, val loss: 1.0316308736801147
Epoch 2050, training loss: 452.8162841796875 = 1.029125690460205 + 50.0 * 9.035743713378906
Epoch 2050, val loss: 1.0311683416366577
Epoch 2060, training loss: 452.96575927734375 = 1.0286431312561035 + 50.0 * 9.038742065429688
Epoch 2060, val loss: 1.0307087898254395
Epoch 2070, training loss: 452.8730163574219 = 1.0281461477279663 + 50.0 * 9.036897659301758
Epoch 2070, val loss: 1.0302410125732422
Epoch 2080, training loss: 452.97039794921875 = 1.0276683568954468 + 50.0 * 9.038854598999023
Epoch 2080, val loss: 1.0297924280166626
Epoch 2090, training loss: 453.0802001953125 = 1.0271885395050049 + 50.0 * 9.041060447692871
Epoch 2090, val loss: 1.029339075088501
Epoch 2100, training loss: 453.3092346191406 = 1.0267126560211182 + 50.0 * 9.045650482177734
Epoch 2100, val loss: 1.028892993927002
Epoch 2110, training loss: 453.313720703125 = 1.026230812072754 + 50.0 * 9.04574966430664
Epoch 2110, val loss: 1.0284430980682373
Epoch 2120, training loss: 453.24920654296875 = 1.0257574319839478 + 50.0 * 9.044468879699707
Epoch 2120, val loss: 1.028003215789795
Epoch 2130, training loss: 453.3190612792969 = 1.025287389755249 + 50.0 * 9.045875549316406
Epoch 2130, val loss: 1.0275630950927734
Epoch 2140, training loss: 453.5145568847656 = 1.024823784828186 + 50.0 * 9.049795150756836
Epoch 2140, val loss: 1.0271339416503906
Epoch 2150, training loss: 452.768310546875 = 1.0243198871612549 + 50.0 * 9.034879684448242
Epoch 2150, val loss: 1.0266419649124146
Epoch 2160, training loss: 444.4963684082031 = 1.0237047672271729 + 50.0 * 8.869453430175781
Epoch 2160, val loss: 1.026056170463562
Epoch 2170, training loss: 449.16424560546875 = 1.0236717462539673 + 50.0 * 8.962811470031738
Epoch 2170, val loss: 1.026052713394165
Epoch 2180, training loss: 448.8345642089844 = 1.023259162902832 + 50.0 * 8.956226348876953
Epoch 2180, val loss: 1.0256811380386353
Epoch 2190, training loss: 450.3372497558594 = 1.0228939056396484 + 50.0 * 8.986287117004395
Epoch 2190, val loss: 1.0253279209136963
Epoch 2200, training loss: 450.8345642089844 = 1.022444248199463 + 50.0 * 8.99624252319336
Epoch 2200, val loss: 1.0249298810958862
Epoch 2210, training loss: 451.2433166503906 = 1.021970510482788 + 50.0 * 9.004426956176758
Epoch 2210, val loss: 1.0245060920715332
Epoch 2220, training loss: 451.9530029296875 = 1.0215222835540771 + 50.0 * 9.018630027770996
Epoch 2220, val loss: 1.0240912437438965
Epoch 2230, training loss: 452.4579162597656 = 1.0210660696029663 + 50.0 * 9.02873706817627
Epoch 2230, val loss: 1.0236889123916626
Epoch 2240, training loss: 452.7265930175781 = 1.0206575393676758 + 50.0 * 9.03411865234375
Epoch 2240, val loss: 1.023318886756897
Epoch 2250, training loss: 452.95758056640625 = 1.0202383995056152 + 50.0 * 9.03874683380127
Epoch 2250, val loss: 1.0229361057281494
Epoch 2260, training loss: 453.07012939453125 = 1.0198273658752441 + 50.0 * 9.041006088256836
Epoch 2260, val loss: 1.0225746631622314
Epoch 2270, training loss: 453.2298278808594 = 1.0194164514541626 + 50.0 * 9.044208526611328
Epoch 2270, val loss: 1.022204041481018
Epoch 2280, training loss: 453.4449157714844 = 1.0190119743347168 + 50.0 * 9.048518180847168
Epoch 2280, val loss: 1.0218383073806763
Epoch 2290, training loss: 453.4710693359375 = 1.0186034440994263 + 50.0 * 9.049049377441406
Epoch 2290, val loss: 1.021477222442627
Epoch 2300, training loss: 453.532470703125 = 1.0181971788406372 + 50.0 * 9.050285339355469
Epoch 2300, val loss: 1.0211185216903687
Epoch 2310, training loss: 453.7750549316406 = 1.017798900604248 + 50.0 * 9.055145263671875
Epoch 2310, val loss: 1.0207589864730835
Epoch 2320, training loss: 454.0612487792969 = 1.017405390739441 + 50.0 * 9.060876846313477
Epoch 2320, val loss: 1.0204054117202759
Epoch 2330, training loss: 454.1149597167969 = 1.0170083045959473 + 50.0 * 9.061959266662598
Epoch 2330, val loss: 1.020054578781128
Epoch 2340, training loss: 453.8321228027344 = 1.0166046619415283 + 50.0 * 9.056310653686523
Epoch 2340, val loss: 1.0196973085403442
Epoch 2350, training loss: 454.01080322265625 = 1.0162197351455688 + 50.0 * 9.059891700744629
Epoch 2350, val loss: 1.0193525552749634
Epoch 2360, training loss: 454.3055419921875 = 1.0158361196517944 + 50.0 * 9.065793991088867
Epoch 2360, val loss: 1.019013524055481
Epoch 2370, training loss: 454.4635925292969 = 1.0154579877853394 + 50.0 * 9.068962097167969
Epoch 2370, val loss: 1.018674373626709
Epoch 2380, training loss: 454.4568786621094 = 1.0150717496871948 + 50.0 * 9.068836212158203
Epoch 2380, val loss: 1.0183357000350952
Epoch 2390, training loss: 454.5572814941406 = 1.0146933794021606 + 50.0 * 9.070852279663086
Epoch 2390, val loss: 1.0180001258850098
Epoch 2400, training loss: 454.74285888671875 = 1.014321208000183 + 50.0 * 9.074570655822754
Epoch 2400, val loss: 1.0176702737808228
Epoch 2410, training loss: 454.7476806640625 = 1.0139509439468384 + 50.0 * 9.074674606323242
Epoch 2410, val loss: 1.017343521118164
Epoch 2420, training loss: 454.9639892578125 = 1.013584852218628 + 50.0 * 9.079008102416992
Epoch 2420, val loss: 1.01701819896698
Epoch 2430, training loss: 454.9026184082031 = 1.0132174491882324 + 50.0 * 9.077788352966309
Epoch 2430, val loss: 1.01669442653656
Epoch 2440, training loss: 454.901123046875 = 1.0128564834594727 + 50.0 * 9.077765464782715
Epoch 2440, val loss: 1.0163745880126953
Epoch 2450, training loss: 455.10150146484375 = 1.0125025510787964 + 50.0 * 9.081779479980469
Epoch 2450, val loss: 1.0160635709762573
Epoch 2460, training loss: 455.2098388671875 = 1.0121474266052246 + 50.0 * 9.083953857421875
Epoch 2460, val loss: 1.0157500505447388
Epoch 2470, training loss: 455.2503356933594 = 1.0117939710617065 + 50.0 * 9.084771156311035
Epoch 2470, val loss: 1.015438199043274
Epoch 2480, training loss: 455.3175354003906 = 1.0114455223083496 + 50.0 * 9.086121559143066
Epoch 2480, val loss: 1.015130639076233
Epoch 2490, training loss: 455.3176574707031 = 1.0111006498336792 + 50.0 * 9.08613109588623
Epoch 2490, val loss: 1.0148301124572754
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.20594202898550723
0.8633630370209375
The final CL Acc:0.51667, 0.24406, The final GNN Acc:0.86351, 0.00060
Begin epxeriment: noisy_level: 0.2 cont_weight: 50 epoch: der1:0.3 der2:0.4 dfr1:0.4 dfr2:0
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.0, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106400])
remove edge: torch.Size([2, 70812])
updated graph: torch.Size([2, 88564])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:227: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 509.072021484375 = 1.0948233604431152 + 50.0 * 10.159543991088867
Epoch 0, val loss: 1.093865990638733
Epoch 10, training loss: 485.4550476074219 = 1.094772458076477 + 50.0 * 9.68720531463623
Epoch 10, val loss: 1.0938163995742798
Epoch 20, training loss: 477.2209777832031 = 1.0946972370147705 + 50.0 * 9.522525787353516
Epoch 20, val loss: 1.0937334299087524
Epoch 30, training loss: 471.309814453125 = 1.094626545906067 + 50.0 * 9.404303550720215
Epoch 30, val loss: 1.093642234802246
Epoch 40, training loss: 466.594482421875 = 1.0945569276809692 + 50.0 * 9.309998512268066
Epoch 40, val loss: 1.0935540199279785
Epoch 50, training loss: 462.6694641113281 = 1.0944972038269043 + 50.0 * 9.231499671936035
Epoch 50, val loss: 1.0934797525405884
Epoch 60, training loss: 459.3526916503906 = 1.094437837600708 + 50.0 * 9.165164947509766
Epoch 60, val loss: 1.093403935432434
Epoch 70, training loss: 456.5435485839844 = 1.0943796634674072 + 50.0 * 9.108983039855957
Epoch 70, val loss: 1.093328595161438
Epoch 80, training loss: 454.1182556152344 = 1.0943251848220825 + 50.0 * 9.060478210449219
Epoch 80, val loss: 1.0932564735412598
Epoch 90, training loss: 451.9942932128906 = 1.0942747592926025 + 50.0 * 9.018000602722168
Epoch 90, val loss: 1.0931895971298218
Epoch 100, training loss: 450.1103210449219 = 1.0942232608795166 + 50.0 * 8.980321884155273
Epoch 100, val loss: 1.0931214094161987
Epoch 110, training loss: 448.4885559082031 = 1.0941720008850098 + 50.0 * 8.947887420654297
Epoch 110, val loss: 1.093054175376892
Epoch 120, training loss: 446.9743347167969 = 1.0941226482391357 + 50.0 * 8.917604446411133
Epoch 120, val loss: 1.0929886102676392
Epoch 130, training loss: 445.6233825683594 = 1.094074010848999 + 50.0 * 8.890585899353027
Epoch 130, val loss: 1.092925786972046
Epoch 140, training loss: 444.4790954589844 = 1.0940313339233398 + 50.0 * 8.867701530456543
Epoch 140, val loss: 1.0928668975830078
Epoch 150, training loss: 443.44647216796875 = 1.0939881801605225 + 50.0 * 8.847049713134766
Epoch 150, val loss: 1.0928109884262085
Epoch 160, training loss: 442.52520751953125 = 1.0939486026763916 + 50.0 * 8.828624725341797
Epoch 160, val loss: 1.0927553176879883
Epoch 170, training loss: 441.6795959472656 = 1.0939092636108398 + 50.0 * 8.811714172363281
Epoch 170, val loss: 1.0926990509033203
Epoch 180, training loss: 440.97052001953125 = 1.0938705205917358 + 50.0 * 8.79753303527832
Epoch 180, val loss: 1.0926446914672852
Epoch 190, training loss: 440.2936706542969 = 1.0938316583633423 + 50.0 * 8.78399658203125
Epoch 190, val loss: 1.0925929546356201
Epoch 200, training loss: 439.7090148925781 = 1.093794822692871 + 50.0 * 8.77230453491211
Epoch 200, val loss: 1.0925426483154297
Epoch 210, training loss: 439.21209716796875 = 1.093759298324585 + 50.0 * 8.762367248535156
Epoch 210, val loss: 1.0924923419952393
Epoch 220, training loss: 438.74658203125 = 1.093725562095642 + 50.0 * 8.753057479858398
Epoch 220, val loss: 1.0924444198608398
Epoch 230, training loss: 438.3127746582031 = 1.0936915874481201 + 50.0 * 8.74438190460205
Epoch 230, val loss: 1.0923969745635986
Epoch 240, training loss: 437.99908447265625 = 1.0936585664749146 + 50.0 * 8.73810863494873
Epoch 240, val loss: 1.0923503637313843
Epoch 250, training loss: 437.7547302246094 = 1.0936284065246582 + 50.0 * 8.733222007751465
Epoch 250, val loss: 1.092305302619934
Epoch 260, training loss: 437.5012512207031 = 1.0935966968536377 + 50.0 * 8.728153228759766
Epoch 260, val loss: 1.0922610759735107
Epoch 270, training loss: 437.2757873535156 = 1.0935652256011963 + 50.0 * 8.723644256591797
Epoch 270, val loss: 1.0922154188156128
Epoch 280, training loss: 437.06341552734375 = 1.0935389995574951 + 50.0 * 8.71939754486084
Epoch 280, val loss: 1.0921761989593506
Epoch 290, training loss: 436.81689453125 = 1.0935121774673462 + 50.0 * 8.714468002319336
Epoch 290, val loss: 1.0921363830566406
Epoch 300, training loss: 436.64263916015625 = 1.0934828519821167 + 50.0 * 8.710983276367188
Epoch 300, val loss: 1.0920934677124023
Epoch 310, training loss: 436.3982238769531 = 1.0934587717056274 + 50.0 * 8.706095695495605
Epoch 310, val loss: 1.0920556783676147
Epoch 320, training loss: 436.23248291015625 = 1.093432903289795 + 50.0 * 8.702780723571777
Epoch 320, val loss: 1.0920168161392212
Epoch 330, training loss: 436.1669921875 = 1.0934100151062012 + 50.0 * 8.701471328735352
Epoch 330, val loss: 1.091982126235962
Epoch 340, training loss: 436.1028137207031 = 1.0933876037597656 + 50.0 * 8.700188636779785
Epoch 340, val loss: 1.0919466018676758
Epoch 350, training loss: 436.0159912109375 = 1.093366265296936 + 50.0 * 8.698452949523926
Epoch 350, val loss: 1.0919125080108643
Epoch 360, training loss: 436.0050964355469 = 1.0933443307876587 + 50.0 * 8.698234558105469
Epoch 360, val loss: 1.0918782949447632
Epoch 370, training loss: 435.9134521484375 = 1.0933221578598022 + 50.0 * 8.696402549743652
Epoch 370, val loss: 1.0918456315994263
Epoch 380, training loss: 435.7544860839844 = 1.0933021306991577 + 50.0 * 8.69322395324707
Epoch 380, val loss: 1.0918129682540894
Epoch 390, training loss: 435.5702819824219 = 1.0932832956314087 + 50.0 * 8.689539909362793
Epoch 390, val loss: 1.0917819738388062
Epoch 400, training loss: 435.8171081542969 = 1.0932680368423462 + 50.0 * 8.694477081298828
Epoch 400, val loss: 1.091754674911499
Epoch 410, training loss: 435.8473815917969 = 1.0932508707046509 + 50.0 * 8.695082664489746
Epoch 410, val loss: 1.091726303100586
Epoch 420, training loss: 435.75482177734375 = 1.0932341814041138 + 50.0 * 8.693231582641602
Epoch 420, val loss: 1.0916967391967773
Epoch 430, training loss: 435.72747802734375 = 1.0932176113128662 + 50.0 * 8.6926851272583
Epoch 430, val loss: 1.0916703939437866
Epoch 440, training loss: 435.81585693359375 = 1.0932024717330933 + 50.0 * 8.694453239440918
Epoch 440, val loss: 1.0916435718536377
Epoch 450, training loss: 435.8857727050781 = 1.0931895971298218 + 50.0 * 8.69585132598877
Epoch 450, val loss: 1.0916199684143066
Epoch 460, training loss: 435.8481750488281 = 1.0931748151779175 + 50.0 * 8.695099830627441
Epoch 460, val loss: 1.0915942192077637
Epoch 470, training loss: 435.9394836425781 = 1.093162178993225 + 50.0 * 8.69692611694336
Epoch 470, val loss: 1.0915709733963013
Epoch 480, training loss: 435.6917419433594 = 1.0931453704833984 + 50.0 * 8.691971778869629
Epoch 480, val loss: 1.0915439128875732
Epoch 490, training loss: 435.84564208984375 = 1.0931355953216553 + 50.0 * 8.695050239562988
Epoch 490, val loss: 1.0915229320526123
Epoch 500, training loss: 435.8780822753906 = 1.093124508857727 + 50.0 * 8.695698738098145
Epoch 500, val loss: 1.0915021896362305
Epoch 510, training loss: 435.9394226074219 = 1.0931137800216675 + 50.0 * 8.69692611694336
Epoch 510, val loss: 1.0914808511734009
Epoch 520, training loss: 435.94580078125 = 1.093102216720581 + 50.0 * 8.697053909301758
Epoch 520, val loss: 1.091461420059204
Epoch 530, training loss: 436.1943054199219 = 1.0930920839309692 + 50.0 * 8.702024459838867
Epoch 530, val loss: 1.0914407968521118
Epoch 540, training loss: 436.1788024902344 = 1.0930829048156738 + 50.0 * 8.701714515686035
Epoch 540, val loss: 1.0914225578308105
Epoch 550, training loss: 436.34051513671875 = 1.0930739641189575 + 50.0 * 8.704948425292969
Epoch 550, val loss: 1.0914031267166138
Epoch 560, training loss: 436.3952941894531 = 1.093063473701477 + 50.0 * 8.70604419708252
Epoch 560, val loss: 1.0913832187652588
Epoch 570, training loss: 436.4017028808594 = 1.0930564403533936 + 50.0 * 8.706172943115234
Epoch 570, val loss: 1.0913667678833008
Epoch 580, training loss: 436.5970458984375 = 1.093047857284546 + 50.0 * 8.71008014678955
Epoch 580, val loss: 1.0913515090942383
Epoch 590, training loss: 436.52410888671875 = 1.09303879737854 + 50.0 * 8.70862102508545
Epoch 590, val loss: 1.0913327932357788
Epoch 600, training loss: 436.6122741699219 = 1.093031406402588 + 50.0 * 8.7103853225708
Epoch 600, val loss: 1.0913171768188477
Epoch 610, training loss: 436.4673767089844 = 1.0930216312408447 + 50.0 * 8.707487106323242
Epoch 610, val loss: 1.091299057006836
Epoch 620, training loss: 436.6079406738281 = 1.093015193939209 + 50.0 * 8.710298538208008
Epoch 620, val loss: 1.0912833213806152
Epoch 630, training loss: 436.6683654785156 = 1.0930085182189941 + 50.0 * 8.711506843566895
Epoch 630, val loss: 1.0912704467773438
Epoch 640, training loss: 436.6892395019531 = 1.0930044651031494 + 50.0 * 8.71192455291748
Epoch 640, val loss: 1.0912590026855469
Epoch 650, training loss: 436.49505615234375 = 1.0929944515228271 + 50.0 * 8.708041191101074
Epoch 650, val loss: 1.0912412405014038
Epoch 660, training loss: 436.9205627441406 = 1.0929927825927734 + 50.0 * 8.716551780700684
Epoch 660, val loss: 1.0912315845489502
Epoch 670, training loss: 436.7494812011719 = 1.0929874181747437 + 50.0 * 8.713129997253418
Epoch 670, val loss: 1.09121835231781
Epoch 680, training loss: 436.90325927734375 = 1.0929811000823975 + 50.0 * 8.716205596923828
Epoch 680, val loss: 1.091205358505249
Epoch 690, training loss: 436.8619384765625 = 1.092973232269287 + 50.0 * 8.715378761291504
Epoch 690, val loss: 1.0911918878555298
Epoch 700, training loss: 437.03790283203125 = 1.0929689407348633 + 50.0 * 8.71889877319336
Epoch 700, val loss: 1.0911808013916016
Epoch 710, training loss: 437.2923889160156 = 1.0929667949676514 + 50.0 * 8.72398853302002
Epoch 710, val loss: 1.091171145439148
Epoch 720, training loss: 437.47735595703125 = 1.0929627418518066 + 50.0 * 8.72768783569336
Epoch 720, val loss: 1.0911612510681152
Epoch 730, training loss: 437.139892578125 = 1.0929499864578247 + 50.0 * 8.720938682556152
Epoch 730, val loss: 1.0911433696746826
Epoch 740, training loss: 437.26324462890625 = 1.092950463294983 + 50.0 * 8.723405838012695
Epoch 740, val loss: 1.0911372900009155
Epoch 750, training loss: 437.4207458496094 = 1.092947006225586 + 50.0 * 8.726555824279785
Epoch 750, val loss: 1.0911272764205933
Epoch 760, training loss: 437.53875732421875 = 1.0929440259933472 + 50.0 * 8.72891616821289
Epoch 760, val loss: 1.0911186933517456
Epoch 770, training loss: 437.8411865234375 = 1.0929428339004517 + 50.0 * 8.734964370727539
Epoch 770, val loss: 1.0911117792129517
Epoch 780, training loss: 437.8335266113281 = 1.0929388999938965 + 50.0 * 8.734811782836914
Epoch 780, val loss: 1.0911027193069458
Epoch 790, training loss: 437.9859619140625 = 1.0929373502731323 + 50.0 * 8.737860679626465
Epoch 790, val loss: 1.091095209121704
Epoch 800, training loss: 438.269287109375 = 1.0929352045059204 + 50.0 * 8.74352741241455
Epoch 800, val loss: 1.091086745262146
Epoch 810, training loss: 438.1341247558594 = 1.092931866645813 + 50.0 * 8.740823745727539
Epoch 810, val loss: 1.0910776853561401
Epoch 820, training loss: 438.1087341308594 = 1.09292733669281 + 50.0 * 8.740316390991211
Epoch 820, val loss: 1.0910708904266357
Epoch 830, training loss: 438.4676513671875 = 1.0929268598556519 + 50.0 * 8.7474946975708
Epoch 830, val loss: 1.091064214706421
Epoch 840, training loss: 438.5423278808594 = 1.0929243564605713 + 50.0 * 8.748988151550293
Epoch 840, val loss: 1.091057300567627
Epoch 850, training loss: 438.47296142578125 = 1.0929216146469116 + 50.0 * 8.747600555419922
Epoch 850, val loss: 1.0910500288009644
Epoch 860, training loss: 438.6925354003906 = 1.0929205417633057 + 50.0 * 8.751992225646973
Epoch 860, val loss: 1.0910437107086182
Epoch 870, training loss: 438.80780029296875 = 1.0929189920425415 + 50.0 * 8.754297256469727
Epoch 870, val loss: 1.0910378694534302
Epoch 880, training loss: 438.87591552734375 = 1.0929173231124878 + 50.0 * 8.755660057067871
Epoch 880, val loss: 1.091031789779663
Epoch 890, training loss: 438.9831237792969 = 1.0929148197174072 + 50.0 * 8.757803916931152
Epoch 890, val loss: 1.0910210609436035
Epoch 900, training loss: 438.92059326171875 = 1.0929104089736938 + 50.0 * 8.756553649902344
Epoch 900, val loss: 1.0910168886184692
Epoch 910, training loss: 439.05419921875 = 1.0929088592529297 + 50.0 * 8.759225845336914
Epoch 910, val loss: 1.091011643409729
Epoch 920, training loss: 439.2739562988281 = 1.0929077863693237 + 50.0 * 8.76362133026123
Epoch 920, val loss: 1.0910078287124634
Epoch 930, training loss: 439.13214111328125 = 1.0929055213928223 + 50.0 * 8.760785102844238
Epoch 930, val loss: 1.0910013914108276
Epoch 940, training loss: 439.2764892578125 = 1.0929054021835327 + 50.0 * 8.763671875
Epoch 940, val loss: 1.0909969806671143
Epoch 950, training loss: 439.49664306640625 = 1.0929051637649536 + 50.0 * 8.768074989318848
Epoch 950, val loss: 1.0909936428070068
Epoch 960, training loss: 439.400146484375 = 1.092902421951294 + 50.0 * 8.766144752502441
Epoch 960, val loss: 1.0909879207611084
Epoch 970, training loss: 439.4619445800781 = 1.092901349067688 + 50.0 * 8.767380714416504
Epoch 970, val loss: 1.0909838676452637
Epoch 980, training loss: 439.7898864746094 = 1.0929025411605835 + 50.0 * 8.773940086364746
Epoch 980, val loss: 1.0909814834594727
Epoch 990, training loss: 439.80767822265625 = 1.0929008722305298 + 50.0 * 8.774295806884766
Epoch 990, val loss: 1.090976357460022
Epoch 1000, training loss: 439.85980224609375 = 1.0928997993469238 + 50.0 * 8.775338172912598
Epoch 1000, val loss: 1.090972900390625
Epoch 1010, training loss: 439.98553466796875 = 1.0928996801376343 + 50.0 * 8.777853012084961
Epoch 1010, val loss: 1.090969204902649
Epoch 1020, training loss: 439.9894104003906 = 1.0928974151611328 + 50.0 * 8.77793025970459
Epoch 1020, val loss: 1.0909661054611206
Epoch 1030, training loss: 440.16961669921875 = 1.092897891998291 + 50.0 * 8.781534194946289
Epoch 1030, val loss: 1.090963363647461
Epoch 1040, training loss: 440.3453674316406 = 1.0928970575332642 + 50.0 * 8.785049438476562
Epoch 1040, val loss: 1.0909593105316162
Epoch 1050, training loss: 440.2543029785156 = 1.092894196510315 + 50.0 * 8.783227920532227
Epoch 1050, val loss: 1.0909557342529297
Epoch 1060, training loss: 440.33245849609375 = 1.0928946733474731 + 50.0 * 8.784790992736816
Epoch 1060, val loss: 1.0909521579742432
Epoch 1070, training loss: 440.5223693847656 = 1.092894434928894 + 50.0 * 8.788589477539062
Epoch 1070, val loss: 1.0909496545791626
Epoch 1080, training loss: 440.5583190917969 = 1.0928940773010254 + 50.0 * 8.789308547973633
Epoch 1080, val loss: 1.0909472703933716
Epoch 1090, training loss: 440.5716857910156 = 1.092891812324524 + 50.0 * 8.789575576782227
Epoch 1090, val loss: 1.0909439325332642
Epoch 1100, training loss: 440.58831787109375 = 1.0928915739059448 + 50.0 * 8.789908409118652
Epoch 1100, val loss: 1.090941309928894
Epoch 1110, training loss: 440.8238220214844 = 1.0928916931152344 + 50.0 * 8.794618606567383
Epoch 1110, val loss: 1.0909392833709717
Epoch 1120, training loss: 440.8550109863281 = 1.0928906202316284 + 50.0 * 8.795242309570312
Epoch 1120, val loss: 1.0909371376037598
Epoch 1130, training loss: 440.9049377441406 = 1.0928891897201538 + 50.0 * 8.79624080657959
Epoch 1130, val loss: 1.0909333229064941
Epoch 1140, training loss: 441.0469970703125 = 1.092889666557312 + 50.0 * 8.799081802368164
Epoch 1140, val loss: 1.0909323692321777
Epoch 1150, training loss: 441.12109375 = 1.0928897857666016 + 50.0 * 8.80056381225586
Epoch 1150, val loss: 1.0909308195114136
Epoch 1160, training loss: 441.1976623535156 = 1.0928891897201538 + 50.0 * 8.802095413208008
Epoch 1160, val loss: 1.0909284353256226
Epoch 1170, training loss: 441.1484680175781 = 1.0928887128829956 + 50.0 * 8.801111221313477
Epoch 1170, val loss: 1.0909262895584106
Epoch 1180, training loss: 441.24334716796875 = 1.0928865671157837 + 50.0 * 8.803009033203125
Epoch 1180, val loss: 1.090923547744751
Epoch 1190, training loss: 441.2820739746094 = 1.0928877592086792 + 50.0 * 8.803783416748047
Epoch 1190, val loss: 1.0909227132797241
Epoch 1200, training loss: 441.52886962890625 = 1.0928882360458374 + 50.0 * 8.808719635009766
Epoch 1200, val loss: 1.090922474861145
Epoch 1210, training loss: 441.6584777832031 = 1.0928871631622314 + 50.0 * 8.811311721801758
Epoch 1210, val loss: 1.0909192562103271
Epoch 1220, training loss: 441.51806640625 = 1.0928857326507568 + 50.0 * 8.808503150939941
Epoch 1220, val loss: 1.0909171104431152
Epoch 1230, training loss: 441.68121337890625 = 1.0928865671157837 + 50.0 * 8.811766624450684
Epoch 1230, val loss: 1.090916633605957
Epoch 1240, training loss: 442.0005187988281 = 1.0928874015808105 + 50.0 * 8.81815242767334
Epoch 1240, val loss: 1.0909157991409302
Epoch 1250, training loss: 441.8352355957031 = 1.09288489818573 + 50.0 * 8.814846992492676
Epoch 1250, val loss: 1.090912938117981
Epoch 1260, training loss: 441.7217102050781 = 1.092882752418518 + 50.0 * 8.812576293945312
Epoch 1260, val loss: 1.0909110307693481
Epoch 1270, training loss: 442.1609802246094 = 1.0928832292556763 + 50.0 * 8.821361541748047
Epoch 1270, val loss: 1.09091055393219
Epoch 1280, training loss: 442.11370849609375 = 1.0928843021392822 + 50.0 * 8.820416450500488
Epoch 1280, val loss: 1.090909719467163
Epoch 1290, training loss: 442.2685546875 = 1.0928840637207031 + 50.0 * 8.82351303100586
Epoch 1290, val loss: 1.0909096002578735
Epoch 1300, training loss: 442.3056945800781 = 1.092882513999939 + 50.0 * 8.82425594329834
Epoch 1300, val loss: 1.0909079313278198
Epoch 1310, training loss: 442.3701477050781 = 1.092881679534912 + 50.0 * 8.825545310974121
Epoch 1310, val loss: 1.090904951095581
Epoch 1320, training loss: 442.29559326171875 = 1.0928791761398315 + 50.0 * 8.824054718017578
Epoch 1320, val loss: 1.0909029245376587
Epoch 1330, training loss: 442.49456787109375 = 1.0928815603256226 + 50.0 * 8.828033447265625
Epoch 1330, val loss: 1.0909048318862915
Epoch 1340, training loss: 442.82806396484375 = 1.0928828716278076 + 50.0 * 8.83470344543457
Epoch 1340, val loss: 1.0909054279327393
Epoch 1350, training loss: 442.9107971191406 = 1.0928821563720703 + 50.0 * 8.836358070373535
Epoch 1350, val loss: 1.0909039974212646
Epoch 1360, training loss: 443.0126953125 = 1.0928822755813599 + 50.0 * 8.838396072387695
Epoch 1360, val loss: 1.0909029245376587
Epoch 1370, training loss: 443.1195068359375 = 1.0928823947906494 + 50.0 * 8.840532302856445
Epoch 1370, val loss: 1.0909030437469482
Epoch 1380, training loss: 443.1993408203125 = 1.0928817987442017 + 50.0 * 8.84212875366211
Epoch 1380, val loss: 1.0909020900726318
Epoch 1390, training loss: 443.2065734863281 = 1.092881202697754 + 50.0 * 8.842273712158203
Epoch 1390, val loss: 1.09089994430542
Epoch 1400, training loss: 443.4313659667969 = 1.092881202697754 + 50.0 * 8.846769332885742
Epoch 1400, val loss: 1.0909000635147095
Epoch 1410, training loss: 443.61456298828125 = 1.0928807258605957 + 50.0 * 8.850433349609375
Epoch 1410, val loss: 1.0908993482589722
Epoch 1420, training loss: 443.5662536621094 = 1.0928808450698853 + 50.0 * 8.849467277526855
Epoch 1420, val loss: 1.0908987522125244
Epoch 1430, training loss: 443.65771484375 = 1.0928802490234375 + 50.0 * 8.851296424865723
Epoch 1430, val loss: 1.090897798538208
Epoch 1440, training loss: 443.46112060546875 = 1.0928763151168823 + 50.0 * 8.847365379333496
Epoch 1440, val loss: 1.0908927917480469
Epoch 1450, training loss: 443.4322814941406 = 1.0928751230239868 + 50.0 * 8.84678840637207
Epoch 1450, val loss: 1.0908927917480469
Epoch 1460, training loss: 443.6145324707031 = 1.0928763151168823 + 50.0 * 8.850433349609375
Epoch 1460, val loss: 1.0908931493759155
Epoch 1470, training loss: 443.8918151855469 = 1.0928782224655151 + 50.0 * 8.855978965759277
Epoch 1470, val loss: 1.0908938646316528
Epoch 1480, training loss: 444.0701599121094 = 1.0928785800933838 + 50.0 * 8.859545707702637
Epoch 1480, val loss: 1.0908944606781006
Epoch 1490, training loss: 443.92242431640625 = 1.0928778648376465 + 50.0 * 8.85659122467041
Epoch 1490, val loss: 1.0908929109573364
Epoch 1500, training loss: 444.16680908203125 = 1.0928778648376465 + 50.0 * 8.861478805541992
Epoch 1500, val loss: 1.0908933877944946
Epoch 1510, training loss: 444.35137939453125 = 1.092877984046936 + 50.0 * 8.8651704788208
Epoch 1510, val loss: 1.090893268585205
Epoch 1520, training loss: 444.2548828125 = 1.0928765535354614 + 50.0 * 8.863240242004395
Epoch 1520, val loss: 1.09089195728302
Epoch 1530, training loss: 444.4123229980469 = 1.0928759574890137 + 50.0 * 8.866389274597168
Epoch 1530, val loss: 1.0908911228179932
Epoch 1540, training loss: 444.2959289550781 = 1.092873454093933 + 50.0 * 8.86406135559082
Epoch 1540, val loss: 1.0908886194229126
Epoch 1550, training loss: 444.390869140625 = 1.0928741693496704 + 50.0 * 8.865960121154785
Epoch 1550, val loss: 1.0908890962600708
Epoch 1560, training loss: 444.6726379394531 = 1.0928748846054077 + 50.0 * 8.87159538269043
Epoch 1560, val loss: 1.0908896923065186
Epoch 1570, training loss: 444.92901611328125 = 1.092875599861145 + 50.0 * 8.876723289489746
Epoch 1570, val loss: 1.0908887386322021
Epoch 1580, training loss: 444.6676330566406 = 1.0928735733032227 + 50.0 * 8.871495246887207
Epoch 1580, val loss: 1.090887427330017
Epoch 1590, training loss: 444.5736389160156 = 1.0928709506988525 + 50.0 * 8.86961555480957
Epoch 1590, val loss: 1.090886116027832
Epoch 1600, training loss: 444.853271484375 = 1.0928726196289062 + 50.0 * 8.875207901000977
Epoch 1600, val loss: 1.0908870697021484
Epoch 1610, training loss: 445.0564270019531 = 1.092873454093933 + 50.0 * 8.879271507263184
Epoch 1610, val loss: 1.0908875465393066
Epoch 1620, training loss: 445.1054992675781 = 1.0928736925125122 + 50.0 * 8.880252838134766
Epoch 1620, val loss: 1.0908879041671753
Epoch 1630, training loss: 445.1737976074219 = 1.092872977256775 + 50.0 * 8.88161849975586
Epoch 1630, val loss: 1.090887188911438
Epoch 1640, training loss: 445.1956787109375 = 1.092872977256775 + 50.0 * 8.88205623626709
Epoch 1640, val loss: 1.090886116027832
Epoch 1650, training loss: 445.14276123046875 = 1.0928707122802734 + 50.0 * 8.880997657775879
Epoch 1650, val loss: 1.0908854007720947
Epoch 1660, training loss: 445.3239440917969 = 1.0928715467453003 + 50.0 * 8.884621620178223
Epoch 1660, val loss: 1.0908859968185425
Epoch 1670, training loss: 445.45269775390625 = 1.0928715467453003 + 50.0 * 8.88719654083252
Epoch 1670, val loss: 1.0908859968185425
Epoch 1680, training loss: 445.575439453125 = 1.092871069908142 + 50.0 * 8.88965129852295
Epoch 1680, val loss: 1.0908851623535156
Epoch 1690, training loss: 445.62921142578125 = 1.0928713083267212 + 50.0 * 8.890727043151855
Epoch 1690, val loss: 1.090884804725647
Epoch 1700, training loss: 445.75958251953125 = 1.0928714275360107 + 50.0 * 8.89333438873291
Epoch 1700, val loss: 1.0908851623535156
Epoch 1710, training loss: 445.86767578125 = 1.0928702354431152 + 50.0 * 8.895496368408203
Epoch 1710, val loss: 1.090883731842041
Epoch 1720, training loss: 449.62255859375 = 1.092847228050232 + 50.0 * 8.97059440612793
Epoch 1720, val loss: 1.0908491611480713
Epoch 1730, training loss: 438.4197692871094 = 1.09267258644104 + 50.0 * 8.746541976928711
Epoch 1730, val loss: 1.090704083442688
Epoch 1740, training loss: 439.9497985839844 = 1.092775821685791 + 50.0 * 8.777140617370605
Epoch 1740, val loss: 1.0908058881759644
Epoch 1750, training loss: 441.1964416503906 = 1.0928162336349487 + 50.0 * 8.802072525024414
Epoch 1750, val loss: 1.0908327102661133
Epoch 1760, training loss: 441.8818054199219 = 1.092829704284668 + 50.0 * 8.815779685974121
Epoch 1760, val loss: 1.0908474922180176
Epoch 1770, training loss: 442.3114013671875 = 1.0928348302841187 + 50.0 * 8.824371337890625
Epoch 1770, val loss: 1.0908516645431519
Epoch 1780, training loss: 442.9049072265625 = 1.0928432941436768 + 50.0 * 8.836241722106934
Epoch 1780, val loss: 1.090859293937683
Epoch 1790, training loss: 443.60693359375 = 1.0928475856781006 + 50.0 * 8.850281715393066
Epoch 1790, val loss: 1.0908632278442383
Epoch 1800, training loss: 444.1612854003906 = 1.0928521156311035 + 50.0 * 8.861368179321289
Epoch 1800, val loss: 1.0908666849136353
Epoch 1810, training loss: 444.5926208496094 = 1.0928535461425781 + 50.0 * 8.8699951171875
Epoch 1810, val loss: 1.0908682346343994
Epoch 1820, training loss: 444.7744445800781 = 1.0928534269332886 + 50.0 * 8.873631477355957
Epoch 1820, val loss: 1.0908684730529785
Epoch 1830, training loss: 444.8125915527344 = 1.092854380607605 + 50.0 * 8.874394416809082
Epoch 1830, val loss: 1.0908691883087158
Epoch 1840, training loss: 445.21600341796875 = 1.0928541421890259 + 50.0 * 8.882462501525879
Epoch 1840, val loss: 1.0908681154251099
Epoch 1850, training loss: 445.3756408691406 = 1.0928542613983154 + 50.0 * 8.885655403137207
Epoch 1850, val loss: 1.0908689498901367
Epoch 1860, training loss: 445.5460205078125 = 1.092855453491211 + 50.0 * 8.889062881469727
Epoch 1860, val loss: 1.0908699035644531
Epoch 1870, training loss: 445.7689208984375 = 1.0928561687469482 + 50.0 * 8.893521308898926
Epoch 1870, val loss: 1.0908704996109009
Epoch 1880, training loss: 445.8822326660156 = 1.0928559303283691 + 50.0 * 8.895787239074707
Epoch 1880, val loss: 1.0908702611923218
Epoch 1890, training loss: 446.0480651855469 = 1.0928558111190796 + 50.0 * 8.899104118347168
Epoch 1890, val loss: 1.0908702611923218
Epoch 1900, training loss: 446.0779113769531 = 1.0928552150726318 + 50.0 * 8.899701118469238
Epoch 1900, val loss: 1.0908702611923218
Epoch 1910, training loss: 446.2724609375 = 1.0928558111190796 + 50.0 * 8.903592109680176
Epoch 1910, val loss: 1.0908706188201904
Epoch 1920, training loss: 446.3094787597656 = 1.092854619026184 + 50.0 * 8.904332160949707
Epoch 1920, val loss: 1.090869665145874
Epoch 1930, training loss: 446.73590087890625 = 1.09285569190979 + 50.0 * 8.912860870361328
Epoch 1930, val loss: 1.0908702611923218
Epoch 1940, training loss: 445.8110046386719 = 1.0928468704223633 + 50.0 * 8.894363403320312
Epoch 1940, val loss: 1.0908616781234741
Epoch 1950, training loss: 446.0518493652344 = 1.0928479433059692 + 50.0 * 8.89918041229248
Epoch 1950, val loss: 1.0908634662628174
Epoch 1960, training loss: 446.6192626953125 = 1.0928512811660767 + 50.0 * 8.910528182983398
Epoch 1960, val loss: 1.0908669233322144
Epoch 1970, training loss: 446.5625915527344 = 1.0928508043289185 + 50.0 * 8.909394264221191
Epoch 1970, val loss: 1.0908666849136353
Epoch 1980, training loss: 446.8481140136719 = 1.0928524732589722 + 50.0 * 8.915104866027832
Epoch 1980, val loss: 1.0908678770065308
Epoch 1990, training loss: 447.1589050292969 = 1.0928541421890259 + 50.0 * 8.921320915222168
Epoch 1990, val loss: 1.0908693075180054
Epoch 2000, training loss: 446.8114929199219 = 1.0928514003753662 + 50.0 * 8.914372444152832
Epoch 2000, val loss: 1.0908665657043457
Epoch 2010, training loss: 446.87109375 = 1.0928510427474976 + 50.0 * 8.91556453704834
Epoch 2010, val loss: 1.0908658504486084
Epoch 2020, training loss: 447.1368713378906 = 1.0928518772125244 + 50.0 * 8.920880317687988
Epoch 2020, val loss: 1.0908679962158203
Epoch 2030, training loss: 447.5192565917969 = 1.0928537845611572 + 50.0 * 8.92852783203125
Epoch 2030, val loss: 1.0908690690994263
Epoch 2040, training loss: 447.47930908203125 = 1.0928529500961304 + 50.0 * 8.927728652954102
Epoch 2040, val loss: 1.0908678770065308
Epoch 2050, training loss: 447.4004821777344 = 1.0928514003753662 + 50.0 * 8.926152229309082
Epoch 2050, val loss: 1.090867280960083
Epoch 2060, training loss: 447.6191101074219 = 1.0928531885147095 + 50.0 * 8.930524826049805
Epoch 2060, val loss: 1.0908687114715576
Epoch 2070, training loss: 447.8133850097656 = 1.09285306930542 + 50.0 * 8.934410095214844
Epoch 2070, val loss: 1.0908679962158203
Epoch 2080, training loss: 447.6629333496094 = 1.092852234840393 + 50.0 * 8.931401252746582
Epoch 2080, val loss: 1.0908669233322144
Epoch 2090, training loss: 447.8719177246094 = 1.0928514003753662 + 50.0 * 8.93558120727539
Epoch 2090, val loss: 1.0908676385879517
Epoch 2100, training loss: 448.03466796875 = 1.0928518772125244 + 50.0 * 8.938836097717285
Epoch 2100, val loss: 1.0908679962158203
Epoch 2110, training loss: 447.81707763671875 = 1.092849850654602 + 50.0 * 8.934484481811523
Epoch 2110, val loss: 1.0908658504486084
Epoch 2120, training loss: 447.9963684082031 = 1.0928504467010498 + 50.0 * 8.938070297241211
Epoch 2120, val loss: 1.0908664464950562
Epoch 2130, training loss: 447.9533386230469 = 1.0928490161895752 + 50.0 * 8.937210083007812
Epoch 2130, val loss: 1.090865135192871
Epoch 2140, training loss: 447.8506774902344 = 1.0928475856781006 + 50.0 * 8.93515682220459
Epoch 2140, val loss: 1.0908645391464233
Epoch 2150, training loss: 448.2144470214844 = 1.0928486585617065 + 50.0 * 8.942432403564453
Epoch 2150, val loss: 1.0908650159835815
Epoch 2160, training loss: 448.5030212402344 = 1.0928499698638916 + 50.0 * 8.948203086853027
Epoch 2160, val loss: 1.0908666849136353
Epoch 2170, training loss: 448.50054931640625 = 1.0928490161895752 + 50.0 * 8.94815444946289
Epoch 2170, val loss: 1.0908658504486084
Epoch 2180, training loss: 448.3568420410156 = 1.0928469896316528 + 50.0 * 8.945280075073242
Epoch 2180, val loss: 1.0908641815185547
Epoch 2190, training loss: 447.84576416015625 = 1.0928415060043335 + 50.0 * 8.93505859375
Epoch 2190, val loss: 1.0908597707748413
Epoch 2200, training loss: 448.01995849609375 = 1.0928438901901245 + 50.0 * 8.938542366027832
Epoch 2200, val loss: 1.0908604860305786
Epoch 2210, training loss: 448.42901611328125 = 1.0928446054458618 + 50.0 * 8.946723937988281
Epoch 2210, val loss: 1.0908619165420532
Epoch 2220, training loss: 448.8418273925781 = 1.0928480625152588 + 50.0 * 8.95497989654541
Epoch 2220, val loss: 1.0908647775650024
Epoch 2230, training loss: 449.2043151855469 = 1.092849612236023 + 50.0 * 8.96222972869873
Epoch 2230, val loss: 1.0908664464950562
Epoch 2240, training loss: 449.428955078125 = 1.0928500890731812 + 50.0 * 8.966721534729004
Epoch 2240, val loss: 1.0908665657043457
Epoch 2250, training loss: 449.5872497558594 = 1.0928503274917603 + 50.0 * 8.969887733459473
Epoch 2250, val loss: 1.0908669233322144
Epoch 2260, training loss: 449.6630554199219 = 1.0928503274917603 + 50.0 * 8.971404075622559
Epoch 2260, val loss: 1.0908664464950562
Epoch 2270, training loss: 449.6246032714844 = 1.0928490161895752 + 50.0 * 8.970635414123535
Epoch 2270, val loss: 1.090865969657898
Epoch 2280, training loss: 449.543212890625 = 1.0928477048873901 + 50.0 * 8.96900749206543
Epoch 2280, val loss: 1.090864896774292
Epoch 2290, training loss: 449.6003723144531 = 1.0928468704223633 + 50.0 * 8.9701509475708
Epoch 2290, val loss: 1.0908639430999756
Epoch 2300, training loss: 449.31390380859375 = 1.092844009399414 + 50.0 * 8.964421272277832
Epoch 2300, val loss: 1.0908619165420532
Epoch 2310, training loss: 449.48431396484375 = 1.0928441286087036 + 50.0 * 8.967829704284668
Epoch 2310, val loss: 1.0908622741699219
Epoch 2320, training loss: 449.84271240234375 = 1.0928467512130737 + 50.0 * 8.974997520446777
Epoch 2320, val loss: 1.0908645391464233
Epoch 2330, training loss: 450.1026306152344 = 1.0928475856781006 + 50.0 * 8.980195999145508
Epoch 2330, val loss: 1.0908653736114502
Epoch 2340, training loss: 450.02825927734375 = 1.0928467512130737 + 50.0 * 8.978708267211914
Epoch 2340, val loss: 1.0908650159835815
Epoch 2350, training loss: 450.1150817871094 = 1.0928468704223633 + 50.0 * 8.98044490814209
Epoch 2350, val loss: 1.0908645391464233
Epoch 2360, training loss: 450.31072998046875 = 1.0928469896316528 + 50.0 * 8.984357833862305
Epoch 2360, val loss: 1.0908647775650024
Epoch 2370, training loss: 450.29132080078125 = 1.0928465127944946 + 50.0 * 8.983969688415527
Epoch 2370, val loss: 1.0908645391464233
Epoch 2380, training loss: 450.4281311035156 = 1.092846155166626 + 50.0 * 8.986705780029297
Epoch 2380, val loss: 1.0908641815185547
Epoch 2390, training loss: 450.3961181640625 = 1.0928460359573364 + 50.0 * 8.986065864562988
Epoch 2390, val loss: 1.0908632278442383
Epoch 2400, training loss: 450.3712463378906 = 1.0928447246551514 + 50.0 * 8.985568046569824
Epoch 2400, val loss: 1.0908626317977905
Epoch 2410, training loss: 450.5578308105469 = 1.092844843864441 + 50.0 * 8.989299774169922
Epoch 2410, val loss: 1.0908634662628174
Epoch 2420, training loss: 450.80328369140625 = 1.0928454399108887 + 50.0 * 8.994209289550781
Epoch 2420, val loss: 1.0908639430999756
Epoch 2430, training loss: 450.61102294921875 = 1.0928435325622559 + 50.0 * 8.990364074707031
Epoch 2430, val loss: 1.0908617973327637
Epoch 2440, training loss: 450.62982177734375 = 1.0928435325622559 + 50.0 * 8.990739822387695
Epoch 2440, val loss: 1.0908619165420532
Epoch 2450, training loss: 450.69921875 = 1.092842936515808 + 50.0 * 8.992127418518066
Epoch 2450, val loss: 1.0908616781234741
Epoch 2460, training loss: 450.8699951171875 = 1.0928434133529663 + 50.0 * 8.995543479919434
Epoch 2460, val loss: 1.0908620357513428
Epoch 2470, training loss: 451.10052490234375 = 1.092844009399414 + 50.0 * 9.000153541564941
Epoch 2470, val loss: 1.0908623933792114
Epoch 2480, training loss: 450.99407958984375 = 1.0928430557250977 + 50.0 * 8.998024940490723
Epoch 2480, val loss: 1.0908615589141846
Epoch 2490, training loss: 451.0205383300781 = 1.0928412675857544 + 50.0 * 8.998554229736328
Epoch 2490, val loss: 1.0908602476119995
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.3973913043478261
0.8204738100412955
=== training gcn model ===
Epoch 0, training loss: 504.94061279296875 = 1.0986114740371704 + 50.0 * 10.0768404006958
Epoch 0, val loss: 1.098612904548645
Epoch 10, training loss: 483.5730285644531 = 1.0986114740371704 + 50.0 * 9.64948844909668
Epoch 10, val loss: 1.098612904548645
Epoch 20, training loss: 475.6964416503906 = 1.0986114740371704 + 50.0 * 9.49195671081543
Epoch 20, val loss: 1.098612904548645
Epoch 30, training loss: 470.0489196777344 = 1.0986114740371704 + 50.0 * 9.379006385803223
Epoch 30, val loss: 1.098612904548645
Epoch 40, training loss: 465.5231018066406 = 1.0986114740371704 + 50.0 * 9.288490295410156
Epoch 40, val loss: 1.098612904548645
Epoch 50, training loss: 461.6958312988281 = 1.0986114740371704 + 50.0 * 9.211944580078125
Epoch 50, val loss: 1.098612904548645
Epoch 60, training loss: 458.4472961425781 = 1.0986114740371704 + 50.0 * 9.146973609924316
Epoch 60, val loss: 1.098612904548645
Epoch 70, training loss: 455.6806335449219 = 1.0986114740371704 + 50.0 * 9.09164047241211
Epoch 70, val loss: 1.098612904548645
Epoch 80, training loss: 453.2948303222656 = 1.0986114740371704 + 50.0 * 9.043924331665039
Epoch 80, val loss: 1.098612904548645
Epoch 90, training loss: 451.2004699707031 = 1.0986114740371704 + 50.0 * 9.002037048339844
Epoch 90, val loss: 1.098612904548645
Epoch 100, training loss: 449.3397521972656 = 1.0986114740371704 + 50.0 * 8.964822769165039
Epoch 100, val loss: 1.098612904548645
Epoch 110, training loss: 447.69879150390625 = 1.0986114740371704 + 50.0 * 8.93200397491455
Epoch 110, val loss: 1.098612904548645
Epoch 120, training loss: 446.2174072265625 = 1.0986114740371704 + 50.0 * 8.902376174926758
Epoch 120, val loss: 1.098612904548645
Epoch 130, training loss: 444.8991394042969 = 1.0986114740371704 + 50.0 * 8.87601089477539
Epoch 130, val loss: 1.098612904548645
Epoch 140, training loss: 443.7356262207031 = 1.0986113548278809 + 50.0 * 8.852740287780762
Epoch 140, val loss: 1.098612904548645
Epoch 150, training loss: 442.7607727050781 = 1.098595380783081 + 50.0 * 8.833243370056152
Epoch 150, val loss: 1.0985829830169678
Epoch 160, training loss: 441.7767333984375 = 1.0983301401138306 + 50.0 * 8.813568115234375
Epoch 160, val loss: 1.0982725620269775
Epoch 170, training loss: 440.97589111328125 = 1.098077416419983 + 50.0 * 8.797555923461914
Epoch 170, val loss: 1.0979782342910767
Epoch 180, training loss: 440.2786865234375 = 1.0978504419326782 + 50.0 * 8.78361701965332
Epoch 180, val loss: 1.0977140665054321
Epoch 190, training loss: 439.72503662109375 = 1.097646951675415 + 50.0 * 8.772547721862793
Epoch 190, val loss: 1.0974773168563843
Epoch 200, training loss: 439.33013916015625 = 1.0974656343460083 + 50.0 * 8.764653205871582
Epoch 200, val loss: 1.0972634553909302
Epoch 210, training loss: 438.8823547363281 = 1.0973049402236938 + 50.0 * 8.755701065063477
Epoch 210, val loss: 1.097072720527649
Epoch 220, training loss: 438.4111633300781 = 1.0971497297286987 + 50.0 * 8.746280670166016
Epoch 220, val loss: 1.0968902111053467
Epoch 230, training loss: 437.9352722167969 = 1.097008466720581 + 50.0 * 8.736764907836914
Epoch 230, val loss: 1.0967206954956055
Epoch 240, training loss: 437.6749267578125 = 1.096880555152893 + 50.0 * 8.731560707092285
Epoch 240, val loss: 1.0965662002563477
Epoch 250, training loss: 437.3739929199219 = 1.0967518091201782 + 50.0 * 8.725544929504395
Epoch 250, val loss: 1.0964137315750122
Epoch 260, training loss: 437.2158203125 = 1.096636414527893 + 50.0 * 8.722383499145508
Epoch 260, val loss: 1.0962715148925781
Epoch 270, training loss: 437.0763854980469 = 1.0965179204940796 + 50.0 * 8.719596862792969
Epoch 270, val loss: 1.0961318016052246
Epoch 280, training loss: 436.8251037597656 = 1.096415638923645 + 50.0 * 8.714573860168457
Epoch 280, val loss: 1.0960042476654053
Epoch 290, training loss: 436.51983642578125 = 1.0963095426559448 + 50.0 * 8.708470344543457
Epoch 290, val loss: 1.0958772897720337
Epoch 300, training loss: 436.35260009765625 = 1.0962131023406982 + 50.0 * 8.705127716064453
Epoch 300, val loss: 1.0957586765289307
Epoch 310, training loss: 436.3503723144531 = 1.0961183309555054 + 50.0 * 8.705084800720215
Epoch 310, val loss: 1.0956425666809082
Epoch 320, training loss: 436.18829345703125 = 1.0960255861282349 + 50.0 * 8.701845169067383
Epoch 320, val loss: 1.0955270528793335
Epoch 330, training loss: 436.04058837890625 = 1.0959367752075195 + 50.0 * 8.698892593383789
Epoch 330, val loss: 1.0954179763793945
Epoch 340, training loss: 436.0335693359375 = 1.0956214666366577 + 50.0 * 8.698759078979492
Epoch 340, val loss: 1.095059871673584
Epoch 350, training loss: 435.88629150390625 = 1.0949119329452515 + 50.0 * 8.69582748413086
Epoch 350, val loss: 1.094393014907837
Epoch 360, training loss: 435.8200378417969 = 1.0942307710647583 + 50.0 * 8.6945161819458
Epoch 360, val loss: 1.0937626361846924
Epoch 370, training loss: 435.55902099609375 = 1.0936163663864136 + 50.0 * 8.689308166503906
Epoch 370, val loss: 1.0931944847106934
Epoch 380, training loss: 435.7520446777344 = 1.0930695533752441 + 50.0 * 8.6931791305542
Epoch 380, val loss: 1.0926822423934937
Epoch 390, training loss: 435.5473937988281 = 1.0925624370574951 + 50.0 * 8.689096450805664
Epoch 390, val loss: 1.0922071933746338
Epoch 400, training loss: 435.6435852050781 = 1.0920969247817993 + 50.0 * 8.69102954864502
Epoch 400, val loss: 1.091766119003296
Epoch 410, training loss: 435.5746154785156 = 1.0916577577590942 + 50.0 * 8.689659118652344
Epoch 410, val loss: 1.0913505554199219
Epoch 420, training loss: 435.52972412109375 = 1.0912436246871948 + 50.0 * 8.688769340515137
Epoch 420, val loss: 1.090954065322876
Epoch 430, training loss: 435.6265563964844 = 1.0908483266830444 + 50.0 * 8.690713882446289
Epoch 430, val loss: 1.0905778408050537
Epoch 440, training loss: 435.4637451171875 = 1.090465784072876 + 50.0 * 8.68746566772461
Epoch 440, val loss: 1.0902132987976074
Epoch 450, training loss: 435.3550720214844 = 1.0900986194610596 + 50.0 * 8.68529987335205
Epoch 450, val loss: 1.089860439300537
Epoch 460, training loss: 435.4041442871094 = 1.0897518396377563 + 50.0 * 8.686287879943848
Epoch 460, val loss: 1.089524269104004
Epoch 470, training loss: 435.2872314453125 = 1.08940851688385 + 50.0 * 8.683956146240234
Epoch 470, val loss: 1.0891945362091064
Epoch 480, training loss: 435.3647155761719 = 1.0890796184539795 + 50.0 * 8.68551254272461
Epoch 480, val loss: 1.0888768434524536
Epoch 490, training loss: 435.39154052734375 = 1.0887569189071655 + 50.0 * 8.686056137084961
Epoch 490, val loss: 1.0885632038116455
Epoch 500, training loss: 435.3065490722656 = 1.0884393453598022 + 50.0 * 8.684362411499023
Epoch 500, val loss: 1.0882588624954224
Epoch 510, training loss: 435.44488525390625 = 1.0881304740905762 + 50.0 * 8.687134742736816
Epoch 510, val loss: 1.0879573822021484
Epoch 520, training loss: 435.3795166015625 = 1.0878320932388306 + 50.0 * 8.685833930969238
Epoch 520, val loss: 1.08766770362854
Epoch 530, training loss: 435.486083984375 = 1.087538242340088 + 50.0 * 8.687971115112305
Epoch 530, val loss: 1.0873829126358032
Epoch 540, training loss: 435.5540771484375 = 1.0872522592544556 + 50.0 * 8.689336776733398
Epoch 540, val loss: 1.0871044397354126
Epoch 550, training loss: 435.563720703125 = 1.0869700908660889 + 50.0 * 8.689535140991211
Epoch 550, val loss: 1.0868302583694458
Epoch 560, training loss: 435.5711975097656 = 1.08669114112854 + 50.0 * 8.689689636230469
Epoch 560, val loss: 1.0865586996078491
Epoch 570, training loss: 435.5267639160156 = 1.0864142179489136 + 50.0 * 8.688806533813477
Epoch 570, val loss: 1.086287021636963
Epoch 580, training loss: 435.8287658691406 = 1.0861283540725708 + 50.0 * 8.694852828979492
Epoch 580, val loss: 1.086008071899414
Epoch 590, training loss: 435.93804931640625 = 1.0858869552612305 + 50.0 * 8.697043418884277
Epoch 590, val loss: 1.0857670307159424
Epoch 600, training loss: 435.7312927246094 = 1.085626482963562 + 50.0 * 8.692913055419922
Epoch 600, val loss: 1.0855180025100708
Epoch 610, training loss: 435.8067321777344 = 1.0853703022003174 + 50.0 * 8.694427490234375
Epoch 610, val loss: 1.0852667093276978
Epoch 620, training loss: 435.88031005859375 = 1.0851185321807861 + 50.0 * 8.695903778076172
Epoch 620, val loss: 1.0850207805633545
Epoch 630, training loss: 436.0704040527344 = 1.08487069606781 + 50.0 * 8.699710845947266
Epoch 630, val loss: 1.0847783088684082
Epoch 640, training loss: 436.07666015625 = 1.0846223831176758 + 50.0 * 8.699840545654297
Epoch 640, val loss: 1.0845344066619873
Epoch 650, training loss: 436.09637451171875 = 1.0843799114227295 + 50.0 * 8.700240135192871
Epoch 650, val loss: 1.0842989683151245
Epoch 660, training loss: 436.2275085449219 = 1.0841424465179443 + 50.0 * 8.70286750793457
Epoch 660, val loss: 1.0840647220611572
Epoch 670, training loss: 436.2164001464844 = 1.08390212059021 + 50.0 * 8.70265007019043
Epoch 670, val loss: 1.0838314294815063
Epoch 680, training loss: 436.2505187988281 = 1.0836706161499023 + 50.0 * 8.703336715698242
Epoch 680, val loss: 1.0836029052734375
Epoch 690, training loss: 436.26019287109375 = 1.0834389925003052 + 50.0 * 8.703535079956055
Epoch 690, val loss: 1.0833747386932373
Epoch 700, training loss: 436.31591796875 = 1.0832117795944214 + 50.0 * 8.7046537399292
Epoch 700, val loss: 1.0831538438796997
Epoch 710, training loss: 436.5211486816406 = 1.0829906463623047 + 50.0 * 8.708763122558594
Epoch 710, val loss: 1.0829359292984009
Epoch 720, training loss: 436.5388488769531 = 1.0827670097351074 + 50.0 * 8.709121704101562
Epoch 720, val loss: 1.0827158689498901
Epoch 730, training loss: 436.5687561035156 = 1.08253812789917 + 50.0 * 8.709724426269531
Epoch 730, val loss: 1.082491397857666
Epoch 740, training loss: 436.4143371582031 = 1.0823215246200562 + 50.0 * 8.706640243530273
Epoch 740, val loss: 1.082279086112976
Epoch 750, training loss: 436.7639465332031 = 1.0821099281311035 + 50.0 * 8.71363639831543
Epoch 750, val loss: 1.0820711851119995
Epoch 760, training loss: 436.94775390625 = 1.0818994045257568 + 50.0 * 8.717316627502441
Epoch 760, val loss: 1.081865668296814
Epoch 770, training loss: 436.9592590332031 = 1.0816898345947266 + 50.0 * 8.717551231384277
Epoch 770, val loss: 1.0816577672958374
Epoch 780, training loss: 437.0840148925781 = 1.0814803838729858 + 50.0 * 8.720050811767578
Epoch 780, val loss: 1.08145272731781
Epoch 790, training loss: 437.222900390625 = 1.0812721252441406 + 50.0 * 8.722832679748535
Epoch 790, val loss: 1.0812478065490723
Epoch 800, training loss: 437.2859191894531 = 1.081067681312561 + 50.0 * 8.72409725189209
Epoch 800, val loss: 1.08104407787323
Epoch 810, training loss: 437.350341796875 = 1.0808665752410889 + 50.0 * 8.72538948059082
Epoch 810, val loss: 1.0808472633361816
Epoch 820, training loss: 437.2616271972656 = 1.08065927028656 + 50.0 * 8.72361946105957
Epoch 820, val loss: 1.0806461572647095
Epoch 830, training loss: 437.40045166015625 = 1.0804634094238281 + 50.0 * 8.726399421691895
Epoch 830, val loss: 1.0804529190063477
Epoch 840, training loss: 437.6770324707031 = 1.080270767211914 + 50.0 * 8.731935501098633
Epoch 840, val loss: 1.0802621841430664
Epoch 850, training loss: 437.77020263671875 = 1.0800726413726807 + 50.0 * 8.733802795410156
Epoch 850, val loss: 1.0800693035125732
Epoch 860, training loss: 437.667724609375 = 1.0798763036727905 + 50.0 * 8.731757164001465
Epoch 860, val loss: 1.079876184463501
Epoch 870, training loss: 437.8699645996094 = 1.0796881914138794 + 50.0 * 8.73580551147461
Epoch 870, val loss: 1.0796900987625122
Epoch 880, training loss: 437.9276123046875 = 1.0794973373413086 + 50.0 * 8.73696231842041
Epoch 880, val loss: 1.079501986503601
Epoch 890, training loss: 437.9949645996094 = 1.0793073177337646 + 50.0 * 8.738312721252441
Epoch 890, val loss: 1.0793159008026123
Epoch 900, training loss: 438.10662841796875 = 1.079121708869934 + 50.0 * 8.74055004119873
Epoch 900, val loss: 1.0791332721710205
Epoch 910, training loss: 438.292236328125 = 1.07893705368042 + 50.0 * 8.74426555633545
Epoch 910, val loss: 1.0789512395858765
Epoch 920, training loss: 438.2311096191406 = 1.0787545442581177 + 50.0 * 8.743046760559082
Epoch 920, val loss: 1.078770637512207
Epoch 930, training loss: 438.32952880859375 = 1.0785701274871826 + 50.0 * 8.74501895904541
Epoch 930, val loss: 1.0785902738571167
Epoch 940, training loss: 438.4607238769531 = 1.0783895254135132 + 50.0 * 8.74764633178711
Epoch 940, val loss: 1.0784107446670532
Epoch 950, training loss: 438.41107177734375 = 1.0782082080841064 + 50.0 * 8.746657371520996
Epoch 950, val loss: 1.0782341957092285
Epoch 960, training loss: 438.5852966308594 = 1.078032374382019 + 50.0 * 8.750144958496094
Epoch 960, val loss: 1.0780606269836426
Epoch 970, training loss: 438.8020935058594 = 1.0778563022613525 + 50.0 * 8.754485130310059
Epoch 970, val loss: 1.0778803825378418
Epoch 980, training loss: 439.3562316894531 = 1.0776793956756592 + 50.0 * 8.765571594238281
Epoch 980, val loss: 1.0777121782302856
Epoch 990, training loss: 439.1094665527344 = 1.0775048732757568 + 50.0 * 8.760639190673828
Epoch 990, val loss: 1.0775434970855713
Epoch 1000, training loss: 439.2081298828125 = 1.0773354768753052 + 50.0 * 8.762616157531738
Epoch 1000, val loss: 1.0773756504058838
Epoch 1010, training loss: 439.15899658203125 = 1.077160120010376 + 50.0 * 8.761636734008789
Epoch 1010, val loss: 1.0771996974945068
Epoch 1020, training loss: 439.03253173828125 = 1.076991081237793 + 50.0 * 8.759110450744629
Epoch 1020, val loss: 1.0770370960235596
Epoch 1030, training loss: 439.179443359375 = 1.0768263339996338 + 50.0 * 8.762052536010742
Epoch 1030, val loss: 1.0768743753433228
Epoch 1040, training loss: 439.4630432128906 = 1.0766639709472656 + 50.0 * 8.767727851867676
Epoch 1040, val loss: 1.0767147541046143
Epoch 1050, training loss: 439.7237243652344 = 1.0765011310577393 + 50.0 * 8.772944450378418
Epoch 1050, val loss: 1.0765529870986938
Epoch 1060, training loss: 439.6953125 = 1.0763343572616577 + 50.0 * 8.772379875183105
Epoch 1060, val loss: 1.0763880014419556
Epoch 1070, training loss: 439.6204833984375 = 1.076171875 + 50.0 * 8.770886421203613
Epoch 1070, val loss: 1.0762295722961426
Epoch 1080, training loss: 439.9539489746094 = 1.076014757156372 + 50.0 * 8.777558326721191
Epoch 1080, val loss: 1.0760737657546997
Epoch 1090, training loss: 440.0502014160156 = 1.0758533477783203 + 50.0 * 8.779486656188965
Epoch 1090, val loss: 1.0759145021438599
Epoch 1100, training loss: 439.9254150390625 = 1.0756903886795044 + 50.0 * 8.776994705200195
Epoch 1100, val loss: 1.0757542848587036
Epoch 1110, training loss: 440.13812255859375 = 1.0755343437194824 + 50.0 * 8.781251907348633
Epoch 1110, val loss: 1.075601577758789
Epoch 1120, training loss: 439.65875244140625 = 1.0753674507141113 + 50.0 * 8.77166748046875
Epoch 1120, val loss: 1.0754389762878418
Epoch 1130, training loss: 439.9231262207031 = 1.075207233428955 + 50.0 * 8.776958465576172
Epoch 1130, val loss: 1.075279712677002
Epoch 1140, training loss: 439.14947509765625 = 1.075040340423584 + 50.0 * 8.761488914489746
Epoch 1140, val loss: 1.0751183032989502
Epoch 1150, training loss: 439.61248779296875 = 1.0749024152755737 + 50.0 * 8.770751953125
Epoch 1150, val loss: 1.0749826431274414
Epoch 1160, training loss: 439.84405517578125 = 1.074753999710083 + 50.0 * 8.775385856628418
Epoch 1160, val loss: 1.0748350620269775
Epoch 1170, training loss: 440.1517028808594 = 1.074607491493225 + 50.0 * 8.78154182434082
Epoch 1170, val loss: 1.0746920108795166
Epoch 1180, training loss: 440.53192138671875 = 1.0744638442993164 + 50.0 * 8.789149284362793
Epoch 1180, val loss: 1.074550747871399
Epoch 1190, training loss: 440.5738830566406 = 1.074313759803772 + 50.0 * 8.78999137878418
Epoch 1190, val loss: 1.0744017362594604
Epoch 1200, training loss: 440.722412109375 = 1.074168086051941 + 50.0 * 8.792964935302734
Epoch 1200, val loss: 1.0742591619491577
Epoch 1210, training loss: 440.5479736328125 = 1.074018955230713 + 50.0 * 8.78947925567627
Epoch 1210, val loss: 1.0741132497787476
Epoch 1220, training loss: 440.82794189453125 = 1.0738760232925415 + 50.0 * 8.79508113861084
Epoch 1220, val loss: 1.073972463607788
Epoch 1230, training loss: 441.1744079589844 = 1.0737351179122925 + 50.0 * 8.802013397216797
Epoch 1230, val loss: 1.0738319158554077
Epoch 1240, training loss: 440.9461364746094 = 1.0735855102539062 + 50.0 * 8.79745101928711
Epoch 1240, val loss: 1.0736865997314453
Epoch 1250, training loss: 441.0393371582031 = 1.0734447240829468 + 50.0 * 8.799317359924316
Epoch 1250, val loss: 1.0735477209091187
Epoch 1260, training loss: 441.21893310546875 = 1.0733064413070679 + 50.0 * 8.802912712097168
Epoch 1260, val loss: 1.0734105110168457
Epoch 1270, training loss: 441.2766418457031 = 1.0731655359268188 + 50.0 * 8.804069519042969
Epoch 1270, val loss: 1.0732719898223877
Epoch 1280, training loss: 441.3282775878906 = 1.073026418685913 + 50.0 * 8.805105209350586
Epoch 1280, val loss: 1.0731334686279297
Epoch 1290, training loss: 441.3962707519531 = 1.0728881359100342 + 50.0 * 8.80646800994873
Epoch 1290, val loss: 1.0729985237121582
Epoch 1300, training loss: 441.49981689453125 = 1.0727510452270508 + 50.0 * 8.808541297912598
Epoch 1300, val loss: 1.0728644132614136
Epoch 1310, training loss: 441.4682922363281 = 1.072615146636963 + 50.0 * 8.807913780212402
Epoch 1310, val loss: 1.072730302810669
Epoch 1320, training loss: 441.51275634765625 = 1.0724787712097168 + 50.0 * 8.808805465698242
Epoch 1320, val loss: 1.0725959539413452
Epoch 1330, training loss: 441.7899169921875 = 1.0723460912704468 + 50.0 * 8.814351081848145
Epoch 1330, val loss: 1.0724656581878662
Epoch 1340, training loss: 442.19439697265625 = 1.0721986293792725 + 50.0 * 8.822443962097168
Epoch 1340, val loss: 1.0723063945770264
Epoch 1350, training loss: 441.7586975097656 = 1.0720700025558472 + 50.0 * 8.813732147216797
Epoch 1350, val loss: 1.0721936225891113
Epoch 1360, training loss: 440.7428283691406 = 1.0719189643859863 + 50.0 * 8.793417930603027
Epoch 1360, val loss: 1.0720486640930176
Epoch 1370, training loss: 440.9858093261719 = 1.0717968940734863 + 50.0 * 8.798279762268066
Epoch 1370, val loss: 1.0719274282455444
Epoch 1380, training loss: 441.6040344238281 = 1.0716784000396729 + 50.0 * 8.810647010803223
Epoch 1380, val loss: 1.0718088150024414
Epoch 1390, training loss: 442.0112609863281 = 1.0715581178665161 + 50.0 * 8.818794250488281
Epoch 1390, val loss: 1.071690320968628
Epoch 1400, training loss: 442.1650695800781 = 1.0714340209960938 + 50.0 * 8.82187271118164
Epoch 1400, val loss: 1.0715675354003906
Epoch 1410, training loss: 442.2584228515625 = 1.0713049173355103 + 50.0 * 8.823741912841797
Epoch 1410, val loss: 1.071440577507019
Epoch 1420, training loss: 442.45208740234375 = 1.071181058883667 + 50.0 * 8.827618598937988
Epoch 1420, val loss: 1.0713189840316772
Epoch 1430, training loss: 442.58154296875 = 1.0710574388504028 + 50.0 * 8.830209732055664
Epoch 1430, val loss: 1.0711979866027832
Epoch 1440, training loss: 442.7156982421875 = 1.0709328651428223 + 50.0 * 8.832895278930664
Epoch 1440, val loss: 1.0710749626159668
Epoch 1450, training loss: 442.7945251464844 = 1.0708096027374268 + 50.0 * 8.834474563598633
Epoch 1450, val loss: 1.070953607559204
Epoch 1460, training loss: 442.9557189941406 = 1.0706872940063477 + 50.0 * 8.837700843811035
Epoch 1460, val loss: 1.0708343982696533
Epoch 1470, training loss: 442.871826171875 = 1.0705642700195312 + 50.0 * 8.83602523803711
Epoch 1470, val loss: 1.070711374282837
Epoch 1480, training loss: 442.8591613769531 = 1.0704408884048462 + 50.0 * 8.835774421691895
Epoch 1480, val loss: 1.0705922842025757
Epoch 1490, training loss: 443.1389465332031 = 1.070324182510376 + 50.0 * 8.8413724899292
Epoch 1490, val loss: 1.0704768896102905
Epoch 1500, training loss: 443.3438720703125 = 1.0702065229415894 + 50.0 * 8.845473289489746
Epoch 1500, val loss: 1.070360779762268
Epoch 1510, training loss: 443.3203125 = 1.0700868368148804 + 50.0 * 8.845004081726074
Epoch 1510, val loss: 1.070243000984192
Epoch 1520, training loss: 443.29022216796875 = 1.0699673891067505 + 50.0 * 8.844405174255371
Epoch 1520, val loss: 1.0701251029968262
Epoch 1530, training loss: 443.4744567871094 = 1.0698521137237549 + 50.0 * 8.848092079162598
Epoch 1530, val loss: 1.0700125694274902
Epoch 1540, training loss: 443.72235107421875 = 1.0697357654571533 + 50.0 * 8.853052139282227
Epoch 1540, val loss: 1.0699008703231812
Epoch 1550, training loss: 443.6133117675781 = 1.0696182250976562 + 50.0 * 8.850873947143555
Epoch 1550, val loss: 1.0697823762893677
Epoch 1560, training loss: 443.6227722167969 = 1.0695027112960815 + 50.0 * 8.851065635681152
Epoch 1560, val loss: 1.0696709156036377
Epoch 1570, training loss: 443.8782043457031 = 1.0693910121917725 + 50.0 * 8.856176376342773
Epoch 1570, val loss: 1.069562315940857
Epoch 1580, training loss: 443.99273681640625 = 1.0692806243896484 + 50.0 * 8.858469009399414
Epoch 1580, val loss: 1.0694526433944702
Epoch 1590, training loss: 444.0325927734375 = 1.0691688060760498 + 50.0 * 8.859268188476562
Epoch 1590, val loss: 1.0693442821502686
Epoch 1600, training loss: 444.1962585449219 = 1.0690580606460571 + 50.0 * 8.862544059753418
Epoch 1600, val loss: 1.0692336559295654
Epoch 1610, training loss: 444.27471923828125 = 1.068946361541748 + 50.0 * 8.864115715026855
Epoch 1610, val loss: 1.0691255331039429
Epoch 1620, training loss: 444.4365234375 = 1.0688363313674927 + 50.0 * 8.867353439331055
Epoch 1620, val loss: 1.069018006324768
Epoch 1630, training loss: 444.09246826171875 = 1.068720817565918 + 50.0 * 8.860474586486816
Epoch 1630, val loss: 1.0689054727554321
Epoch 1640, training loss: 444.0487976074219 = 1.0686123371124268 + 50.0 * 8.859603881835938
Epoch 1640, val loss: 1.0687981843948364
Epoch 1650, training loss: 444.45025634765625 = 1.0685081481933594 + 50.0 * 8.867634773254395
Epoch 1650, val loss: 1.0686975717544556
Epoch 1660, training loss: 444.68353271484375 = 1.068405032157898 + 50.0 * 8.872303009033203
Epoch 1660, val loss: 1.0685961246490479
Epoch 1670, training loss: 444.74371337890625 = 1.0682989358901978 + 50.0 * 8.87350845336914
Epoch 1670, val loss: 1.0684912204742432
Epoch 1680, training loss: 444.94281005859375 = 1.0681926012039185 + 50.0 * 8.87749195098877
Epoch 1680, val loss: 1.0683876276016235
Epoch 1690, training loss: 444.90167236328125 = 1.068084955215454 + 50.0 * 8.87667179107666
Epoch 1690, val loss: 1.0682828426361084
Epoch 1700, training loss: 444.86395263671875 = 1.0679820775985718 + 50.0 * 8.875919342041016
Epoch 1700, val loss: 1.0681811571121216
Epoch 1710, training loss: 445.1640625 = 1.0678812265396118 + 50.0 * 8.88192367553711
Epoch 1710, val loss: 1.0680829286575317
Epoch 1720, training loss: 445.0738830566406 = 1.0677742958068848 + 50.0 * 8.880122184753418
Epoch 1720, val loss: 1.0679798126220703
Epoch 1730, training loss: 445.38385009765625 = 1.0676740407943726 + 50.0 * 8.886322975158691
Epoch 1730, val loss: 1.0678815841674805
Epoch 1740, training loss: 445.3983459472656 = 1.0675734281539917 + 50.0 * 8.886615753173828
Epoch 1740, val loss: 1.0677818059921265
Epoch 1750, training loss: 445.6363525390625 = 1.067476511001587 + 50.0 * 8.891377449035645
Epoch 1750, val loss: 1.0676876306533813
Epoch 1760, training loss: 445.6817321777344 = 1.0673736333847046 + 50.0 * 8.892287254333496
Epoch 1760, val loss: 1.067585825920105
Epoch 1770, training loss: 445.7240295410156 = 1.0672765970230103 + 50.0 * 8.893135070800781
Epoch 1770, val loss: 1.067492127418518
Epoch 1780, training loss: 445.98345947265625 = 1.0671815872192383 + 50.0 * 8.89832592010498
Epoch 1780, val loss: 1.067396640777588
Epoch 1790, training loss: 445.82110595703125 = 1.067081093788147 + 50.0 * 8.89508056640625
Epoch 1790, val loss: 1.0672974586486816
Epoch 1800, training loss: 445.69403076171875 = 1.0669810771942139 + 50.0 * 8.89254093170166
Epoch 1800, val loss: 1.0672017335891724
Epoch 1810, training loss: 445.85443115234375 = 1.0668845176696777 + 50.0 * 8.895750999450684
Epoch 1810, val loss: 1.0671101808547974
Epoch 1820, training loss: 446.198486328125 = 1.0667939186096191 + 50.0 * 8.902633666992188
Epoch 1820, val loss: 1.0670188665390015
Epoch 1830, training loss: 446.1249694824219 = 1.066697120666504 + 50.0 * 8.901165008544922
Epoch 1830, val loss: 1.0669240951538086
Epoch 1840, training loss: 446.1965026855469 = 1.0666011571884155 + 50.0 * 8.90259838104248
Epoch 1840, val loss: 1.066829800605774
Epoch 1850, training loss: 446.1152648925781 = 1.0665082931518555 + 50.0 * 8.900975227355957
Epoch 1850, val loss: 1.0667386054992676
Epoch 1860, training loss: 445.8990173339844 = 1.0664122104644775 + 50.0 * 8.896652221679688
Epoch 1860, val loss: 1.066643238067627
Epoch 1870, training loss: 445.9158020019531 = 1.066320538520813 + 50.0 * 8.896989822387695
Epoch 1870, val loss: 1.0665545463562012
Epoch 1880, training loss: 446.15838623046875 = 1.0662275552749634 + 50.0 * 8.901843070983887
Epoch 1880, val loss: 1.0664674043655396
Epoch 1890, training loss: 446.3553771972656 = 1.0661389827728271 + 50.0 * 8.905784606933594
Epoch 1890, val loss: 1.066378116607666
Epoch 1900, training loss: 446.4668884277344 = 1.066049575805664 + 50.0 * 8.9080171585083
Epoch 1900, val loss: 1.0662909746170044
Epoch 1910, training loss: 446.531982421875 = 1.0659594535827637 + 50.0 * 8.909320831298828
Epoch 1910, val loss: 1.066202163696289
Epoch 1920, training loss: 446.3543701171875 = 1.065862774848938 + 50.0 * 8.905770301818848
Epoch 1920, val loss: 1.066111445426941
Epoch 1930, training loss: 446.5275573730469 = 1.0657745599746704 + 50.0 * 8.909235954284668
Epoch 1930, val loss: 1.066027045249939
Epoch 1940, training loss: 446.4144287109375 = 1.0656872987747192 + 50.0 * 8.906974792480469
Epoch 1940, val loss: 1.0659376382827759
Epoch 1950, training loss: 446.59918212890625 = 1.0656026601791382 + 50.0 * 8.91067123413086
Epoch 1950, val loss: 1.0658552646636963
Epoch 1960, training loss: 446.817138671875 = 1.0655195713043213 + 50.0 * 8.915032386779785
Epoch 1960, val loss: 1.06577467918396
Epoch 1970, training loss: 446.9511413574219 = 1.065434217453003 + 50.0 * 8.91771411895752
Epoch 1970, val loss: 1.0656911134719849
Epoch 1980, training loss: 447.002685546875 = 1.0653492212295532 + 50.0 * 8.918746948242188
Epoch 1980, val loss: 1.0656070709228516
Epoch 1990, training loss: 447.0419616699219 = 1.0652637481689453 + 50.0 * 8.919533729553223
Epoch 1990, val loss: 1.0655245780944824
Epoch 2000, training loss: 446.7959289550781 = 1.0651754140853882 + 50.0 * 8.9146146774292
Epoch 2000, val loss: 1.065438151359558
Epoch 2010, training loss: 446.9893798828125 = 1.0650923252105713 + 50.0 * 8.918485641479492
Epoch 2010, val loss: 1.0653576850891113
Epoch 2020, training loss: 447.1747131347656 = 1.0650123357772827 + 50.0 * 8.922194480895996
Epoch 2020, val loss: 1.0652790069580078
Epoch 2030, training loss: 447.2663269042969 = 1.0649299621582031 + 50.0 * 8.924027442932129
Epoch 2030, val loss: 1.0651969909667969
Epoch 2040, training loss: 447.3373718261719 = 1.0648468732833862 + 50.0 * 8.925450325012207
Epoch 2040, val loss: 1.0651168823242188
Epoch 2050, training loss: 447.2483825683594 = 1.064765214920044 + 50.0 * 8.923672676086426
Epoch 2050, val loss: 1.0650384426116943
Epoch 2060, training loss: 447.4537048339844 = 1.0646870136260986 + 50.0 * 8.927780151367188
Epoch 2060, val loss: 1.0649622678756714
Epoch 2070, training loss: 447.5716247558594 = 1.0646075010299683 + 50.0 * 8.930140495300293
Epoch 2070, val loss: 1.0648837089538574
Epoch 2080, training loss: 447.57684326171875 = 1.0645259618759155 + 50.0 * 8.930246353149414
Epoch 2080, val loss: 1.0648053884506226
Epoch 2090, training loss: 447.47576904296875 = 1.0644416809082031 + 50.0 * 8.928226470947266
Epoch 2090, val loss: 1.0647221803665161
Epoch 2100, training loss: 447.2652587890625 = 1.064357042312622 + 50.0 * 8.924017906188965
Epoch 2100, val loss: 1.064639687538147
Epoch 2110, training loss: 446.08740234375 = 1.0642541646957397 + 50.0 * 8.900463104248047
Epoch 2110, val loss: 1.064549207687378
Epoch 2120, training loss: 446.426513671875 = 1.064188003540039 + 50.0 * 8.907246589660645
Epoch 2120, val loss: 1.0644772052764893
Epoch 2130, training loss: 446.6604919433594 = 1.064117670059204 + 50.0 * 8.911927223205566
Epoch 2130, val loss: 1.0644084215164185
Epoch 2140, training loss: 447.0835266113281 = 1.0640482902526855 + 50.0 * 8.920389175415039
Epoch 2140, val loss: 1.0643413066864014
Epoch 2150, training loss: 447.5068664550781 = 1.0639790296554565 + 50.0 * 8.928857803344727
Epoch 2150, val loss: 1.0642718076705933
Epoch 2160, training loss: 447.68475341796875 = 1.0639054775238037 + 50.0 * 8.932416915893555
Epoch 2160, val loss: 1.0642006397247314
Epoch 2170, training loss: 447.6598815917969 = 1.0638295412063599 + 50.0 * 8.931921005249023
Epoch 2170, val loss: 1.0641274452209473
Epoch 2180, training loss: 447.8254699707031 = 1.063759446144104 + 50.0 * 8.935234069824219
Epoch 2180, val loss: 1.0640578269958496
Epoch 2190, training loss: 447.94775390625 = 1.0636860132217407 + 50.0 * 8.937681198120117
Epoch 2190, val loss: 1.063986897468567
Epoch 2200, training loss: 447.9751281738281 = 1.063612699508667 + 50.0 * 8.938230514526367
Epoch 2200, val loss: 1.0639159679412842
Epoch 2210, training loss: 448.0286560058594 = 1.0635396242141724 + 50.0 * 8.939302444458008
Epoch 2210, val loss: 1.0638455152511597
Epoch 2220, training loss: 448.0719909667969 = 1.0634678602218628 + 50.0 * 8.940170288085938
Epoch 2220, val loss: 1.063774824142456
Epoch 2230, training loss: 448.0574035644531 = 1.0633946657180786 + 50.0 * 8.93988037109375
Epoch 2230, val loss: 1.0637047290802002
Epoch 2240, training loss: 448.00616455078125 = 1.0633224248886108 + 50.0 * 8.938857078552246
Epoch 2240, val loss: 1.0636359453201294
Epoch 2250, training loss: 448.2227478027344 = 1.0632553100585938 + 50.0 * 8.94318962097168
Epoch 2250, val loss: 1.06356942653656
Epoch 2260, training loss: 448.30438232421875 = 1.0631859302520752 + 50.0 * 8.94482421875
Epoch 2260, val loss: 1.0635026693344116
Epoch 2270, training loss: 448.2305908203125 = 1.0631136894226074 + 50.0 * 8.943349838256836
Epoch 2270, val loss: 1.063433289527893
Epoch 2280, training loss: 448.3537902832031 = 1.0630464553833008 + 50.0 * 8.945815086364746
Epoch 2280, val loss: 1.0633666515350342
Epoch 2290, training loss: 445.9357604980469 = 1.0629031658172607 + 50.0 * 8.897457122802734
Epoch 2290, val loss: 1.0632377862930298
Epoch 2300, training loss: 446.56268310546875 = 1.0628602504730225 + 50.0 * 8.909996032714844
Epoch 2300, val loss: 1.063180685043335
Epoch 2310, training loss: 446.6182556152344 = 1.0628050565719604 + 50.0 * 8.91110897064209
Epoch 2310, val loss: 1.0631334781646729
Epoch 2320, training loss: 446.69769287109375 = 1.062743067741394 + 50.0 * 8.912698745727539
Epoch 2320, val loss: 1.0630744695663452
Epoch 2330, training loss: 447.0508728027344 = 1.062687873840332 + 50.0 * 8.919763565063477
Epoch 2330, val loss: 1.0630203485488892
Epoch 2340, training loss: 447.3901062011719 = 1.0626261234283447 + 50.0 * 8.926549911499023
Epoch 2340, val loss: 1.0629624128341675
Epoch 2350, training loss: 447.86297607421875 = 1.062569260597229 + 50.0 * 8.93600845336914
Epoch 2350, val loss: 1.062905192375183
Epoch 2360, training loss: 448.22418212890625 = 1.0625077486038208 + 50.0 * 8.943233489990234
Epoch 2360, val loss: 1.0628466606140137
Epoch 2370, training loss: 448.4347229003906 = 1.062448263168335 + 50.0 * 8.9474458694458
Epoch 2370, val loss: 1.0627875328063965
Epoch 2380, training loss: 448.49383544921875 = 1.0623832941055298 + 50.0 * 8.948629379272461
Epoch 2380, val loss: 1.0627262592315674
Epoch 2390, training loss: 448.63873291015625 = 1.0623235702514648 + 50.0 * 8.951528549194336
Epoch 2390, val loss: 1.0626678466796875
Epoch 2400, training loss: 448.80633544921875 = 1.0622611045837402 + 50.0 * 8.95488166809082
Epoch 2400, val loss: 1.0626084804534912
Epoch 2410, training loss: 448.93658447265625 = 1.0622003078460693 + 50.0 * 8.957488059997559
Epoch 2410, val loss: 1.0625481605529785
Epoch 2420, training loss: 448.9104919433594 = 1.0621373653411865 + 50.0 * 8.9569673538208
Epoch 2420, val loss: 1.0624885559082031
Epoch 2430, training loss: 449.0968322753906 = 1.0620784759521484 + 50.0 * 8.960695266723633
Epoch 2430, val loss: 1.0624291896820068
Epoch 2440, training loss: 449.1560363769531 = 1.0620169639587402 + 50.0 * 8.961880683898926
Epoch 2440, val loss: 1.0623692274093628
Epoch 2450, training loss: 449.1290283203125 = 1.0619550943374634 + 50.0 * 8.961341857910156
Epoch 2450, val loss: 1.0623114109039307
Epoch 2460, training loss: 449.2922058105469 = 1.061896562576294 + 50.0 * 8.964606285095215
Epoch 2460, val loss: 1.062255620956421
Epoch 2470, training loss: 449.5081481933594 = 1.0618396997451782 + 50.0 * 8.968926429748535
Epoch 2470, val loss: 1.0621979236602783
Epoch 2480, training loss: 449.15460205078125 = 1.0617719888687134 + 50.0 * 8.961856842041016
Epoch 2480, val loss: 1.0621367692947388
Epoch 2490, training loss: 449.2544860839844 = 1.0617146492004395 + 50.0 * 8.963855743408203
Epoch 2490, val loss: 1.0620800256729126
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.39652173913043476
0.8190972976889083
=== training gcn model ===
Epoch 0, training loss: 505.44342041015625 = 1.1119704246520996 + 50.0 * 10.086628913879395
Epoch 0, val loss: 1.1121957302093506
Epoch 10, training loss: 482.94696044921875 = 1.111520767211914 + 50.0 * 9.636709213256836
Epoch 10, val loss: 1.111770510673523
Epoch 20, training loss: 474.6993103027344 = 1.1110707521438599 + 50.0 * 9.47176456451416
Epoch 20, val loss: 1.1113381385803223
Epoch 30, training loss: 468.9066467285156 = 1.1105835437774658 + 50.0 * 9.355920791625977
Epoch 30, val loss: 1.110871434211731
Epoch 40, training loss: 464.54132080078125 = 1.1100975275039673 + 50.0 * 9.268624305725098
Epoch 40, val loss: 1.1104028224945068
Epoch 50, training loss: 461.0496826171875 = 1.1096071004867554 + 50.0 * 9.198801040649414
Epoch 50, val loss: 1.1099351644515991
Epoch 60, training loss: 458.0953063964844 = 1.1091320514678955 + 50.0 * 9.139723777770996
Epoch 60, val loss: 1.1094778776168823
Epoch 70, training loss: 455.52020263671875 = 1.1086509227752686 + 50.0 * 9.088231086730957
Epoch 70, val loss: 1.1090165376663208
Epoch 80, training loss: 453.2752685546875 = 1.1081820726394653 + 50.0 * 9.043341636657715
Epoch 80, val loss: 1.1085691452026367
Epoch 90, training loss: 451.30194091796875 = 1.1077172756195068 + 50.0 * 9.003884315490723
Epoch 90, val loss: 1.1081234216690063
Epoch 100, training loss: 449.5494079589844 = 1.1072648763656616 + 50.0 * 8.968842506408691
Epoch 100, val loss: 1.1076905727386475
Epoch 110, training loss: 447.89306640625 = 1.1068127155303955 + 50.0 * 8.935725212097168
Epoch 110, val loss: 1.1072570085525513
Epoch 120, training loss: 446.47369384765625 = 1.1063684225082397 + 50.0 * 8.907346725463867
Epoch 120, val loss: 1.1068315505981445
Epoch 130, training loss: 445.2051086425781 = 1.1059309244155884 + 50.0 * 8.881983757019043
Epoch 130, val loss: 1.10641348361969
Epoch 140, training loss: 444.0575256347656 = 1.105495572090149 + 50.0 * 8.859040260314941
Epoch 140, val loss: 1.1059948205947876
Epoch 150, training loss: 443.06793212890625 = 1.1050639152526855 + 50.0 * 8.83925724029541
Epoch 150, val loss: 1.1055805683135986
Epoch 160, training loss: 442.2662658691406 = 1.104636311531067 + 50.0 * 8.823232650756836
Epoch 160, val loss: 1.1051760911941528
Epoch 170, training loss: 441.4447326660156 = 1.1042171716690063 + 50.0 * 8.80681037902832
Epoch 170, val loss: 1.1047723293304443
Epoch 180, training loss: 440.73297119140625 = 1.1037964820861816 + 50.0 * 8.792583465576172
Epoch 180, val loss: 1.1043721437454224
Epoch 190, training loss: 440.2658386230469 = 1.1033819913864136 + 50.0 * 8.783248901367188
Epoch 190, val loss: 1.1039756536483765
Epoch 200, training loss: 439.6694641113281 = 1.10297691822052 + 50.0 * 8.771329879760742
Epoch 200, val loss: 1.1035875082015991
Epoch 210, training loss: 439.1928405761719 = 1.1025716066360474 + 50.0 * 8.761805534362793
Epoch 210, val loss: 1.1031986474990845
Epoch 220, training loss: 438.83087158203125 = 1.1021696329116821 + 50.0 * 8.754573822021484
Epoch 220, val loss: 1.1028159856796265
Epoch 230, training loss: 438.3597412109375 = 1.1017730236053467 + 50.0 * 8.745159149169922
Epoch 230, val loss: 1.1024396419525146
Epoch 240, training loss: 437.9549865722656 = 1.1013799905776978 + 50.0 * 8.737071990966797
Epoch 240, val loss: 1.1020623445510864
Epoch 250, training loss: 437.6627197265625 = 1.1009875535964966 + 50.0 * 8.731234550476074
Epoch 250, val loss: 1.1016876697540283
Epoch 260, training loss: 437.37371826171875 = 1.1005992889404297 + 50.0 * 8.725462913513184
Epoch 260, val loss: 1.101318120956421
Epoch 270, training loss: 437.31939697265625 = 1.100218653678894 + 50.0 * 8.724383354187012
Epoch 270, val loss: 1.1009560823440552
Epoch 280, training loss: 437.0578918457031 = 1.099841833114624 + 50.0 * 8.719161033630371
Epoch 280, val loss: 1.1005926132202148
Epoch 290, training loss: 436.7956237792969 = 1.0994666814804077 + 50.0 * 8.713923454284668
Epoch 290, val loss: 1.1002377271652222
Epoch 300, training loss: 436.59771728515625 = 1.0990952253341675 + 50.0 * 8.709972381591797
Epoch 300, val loss: 1.0998843908309937
Epoch 310, training loss: 436.5939636230469 = 1.0987329483032227 + 50.0 * 8.709904670715332
Epoch 310, val loss: 1.0995373725891113
Epoch 320, training loss: 436.4739074707031 = 1.0983655452728271 + 50.0 * 8.707510948181152
Epoch 320, val loss: 1.099188208580017
Epoch 330, training loss: 436.2615051269531 = 1.0980021953582764 + 50.0 * 8.703269958496094
Epoch 330, val loss: 1.0988446474075317
Epoch 340, training loss: 436.27117919921875 = 1.0976451635360718 + 50.0 * 8.703470230102539
Epoch 340, val loss: 1.0985039472579956
Epoch 350, training loss: 436.1315612792969 = 1.0972954034805298 + 50.0 * 8.700685501098633
Epoch 350, val loss: 1.098170518875122
Epoch 360, training loss: 436.07696533203125 = 1.0969427824020386 + 50.0 * 8.699600219726562
Epoch 360, val loss: 1.097835659980774
Epoch 370, training loss: 436.0223388671875 = 1.096595048904419 + 50.0 * 8.698514938354492
Epoch 370, val loss: 1.0975053310394287
Epoch 380, training loss: 436.1809387207031 = 1.0962525606155396 + 50.0 * 8.701693534851074
Epoch 380, val loss: 1.097177267074585
Epoch 390, training loss: 435.8017883300781 = 1.0958691835403442 + 50.0 * 8.69411849975586
Epoch 390, val loss: 1.0968066453933716
Epoch 400, training loss: 437.373291015625 = 1.0955778360366821 + 50.0 * 8.725554466247559
Epoch 400, val loss: 1.0965323448181152
Epoch 410, training loss: 435.57489013671875 = 1.0952249765396118 + 50.0 * 8.689593315124512
Epoch 410, val loss: 1.0961987972259521
Epoch 420, training loss: 435.4471740722656 = 1.0949034690856934 + 50.0 * 8.687045097351074
Epoch 420, val loss: 1.0958926677703857
Epoch 430, training loss: 435.6506652832031 = 1.094582200050354 + 50.0 * 8.691122055053711
Epoch 430, val loss: 1.0955883264541626
Epoch 440, training loss: 435.5362548828125 = 1.0942610502243042 + 50.0 * 8.68883991241455
Epoch 440, val loss: 1.0952811241149902
Epoch 450, training loss: 435.7289123535156 = 1.0939421653747559 + 50.0 * 8.692699432373047
Epoch 450, val loss: 1.0949774980545044
Epoch 460, training loss: 435.67138671875 = 1.0936232805252075 + 50.0 * 8.69155502319336
Epoch 460, val loss: 1.0946729183197021
Epoch 470, training loss: 435.71697998046875 = 1.0933105945587158 + 50.0 * 8.692473411560059
Epoch 470, val loss: 1.094376802444458
Epoch 480, training loss: 435.7315979003906 = 1.0929973125457764 + 50.0 * 8.692771911621094
Epoch 480, val loss: 1.0940794944763184
Epoch 490, training loss: 435.74029541015625 = 1.0926915407180786 + 50.0 * 8.692952156066895
Epoch 490, val loss: 1.0937877893447876
Epoch 500, training loss: 435.87091064453125 = 1.092384934425354 + 50.0 * 8.695570945739746
Epoch 500, val loss: 1.0934984683990479
Epoch 510, training loss: 435.90179443359375 = 1.0920846462249756 + 50.0 * 8.696194648742676
Epoch 510, val loss: 1.093214750289917
Epoch 520, training loss: 435.9757385253906 = 1.0917867422103882 + 50.0 * 8.697678565979004
Epoch 520, val loss: 1.0929290056228638
Epoch 530, training loss: 435.99969482421875 = 1.0914852619171143 + 50.0 * 8.698163986206055
Epoch 530, val loss: 1.0926437377929688
Epoch 540, training loss: 435.9197082519531 = 1.0911905765533447 + 50.0 * 8.69657039642334
Epoch 540, val loss: 1.0923644304275513
Epoch 550, training loss: 435.876953125 = 1.0908986330032349 + 50.0 * 8.695720672607422
Epoch 550, val loss: 1.0920865535736084
Epoch 560, training loss: 436.0289001464844 = 1.0906155109405518 + 50.0 * 8.698765754699707
Epoch 560, val loss: 1.0918184518814087
Epoch 570, training loss: 436.2530517578125 = 1.0903345346450806 + 50.0 * 8.703254699707031
Epoch 570, val loss: 1.0915518999099731
Epoch 580, training loss: 436.3106689453125 = 1.090055227279663 + 50.0 * 8.704412460327148
Epoch 580, val loss: 1.0912874937057495
Epoch 590, training loss: 436.33917236328125 = 1.089791178703308 + 50.0 * 8.704987525939941
Epoch 590, val loss: 1.091041088104248
Epoch 600, training loss: 436.8869323730469 = 1.0895698070526123 + 50.0 * 8.715947151184082
Epoch 600, val loss: 1.0908451080322266
Epoch 610, training loss: 436.3755187988281 = 1.089483618736267 + 50.0 * 8.705720901489258
Epoch 610, val loss: 1.0907844305038452
Epoch 620, training loss: 436.5511169433594 = 1.0894075632095337 + 50.0 * 8.709234237670898
Epoch 620, val loss: 1.0907212495803833
Epoch 630, training loss: 436.6861877441406 = 1.0893326997756958 + 50.0 * 8.711936950683594
Epoch 630, val loss: 1.0906617641448975
Epoch 640, training loss: 436.7046203613281 = 1.0892611742019653 + 50.0 * 8.71230697631836
Epoch 640, val loss: 1.0906047821044922
Epoch 650, training loss: 436.713623046875 = 1.0891876220703125 + 50.0 * 8.712488174438477
Epoch 650, val loss: 1.090545654296875
Epoch 660, training loss: 436.74346923828125 = 1.0891185998916626 + 50.0 * 8.71308708190918
Epoch 660, val loss: 1.0904897451400757
Epoch 670, training loss: 436.80963134765625 = 1.0890507698059082 + 50.0 * 8.714411735534668
Epoch 670, val loss: 1.0904361009597778
Epoch 680, training loss: 436.8461608886719 = 1.0889796018600464 + 50.0 * 8.715143203735352
Epoch 680, val loss: 1.0903812646865845
Epoch 690, training loss: 436.4306335449219 = 1.0889058113098145 + 50.0 * 8.70683479309082
Epoch 690, val loss: 1.0903202295303345
Epoch 700, training loss: 436.9990234375 = 1.08884596824646 + 50.0 * 8.7182035446167
Epoch 700, val loss: 1.0902734994888306
Epoch 710, training loss: 437.050537109375 = 1.0887795686721802 + 50.0 * 8.71923542022705
Epoch 710, val loss: 1.0902221202850342
Epoch 720, training loss: 437.2123718261719 = 1.0887151956558228 + 50.0 * 8.72247314453125
Epoch 720, val loss: 1.0901689529418945
Epoch 730, training loss: 437.113037109375 = 1.088646650314331 + 50.0 * 8.720487594604492
Epoch 730, val loss: 1.0901153087615967
Epoch 740, training loss: 437.0435791015625 = 1.08858323097229 + 50.0 * 8.719099998474121
Epoch 740, val loss: 1.0900647640228271
Epoch 750, training loss: 437.14947509765625 = 1.0885201692581177 + 50.0 * 8.721219062805176
Epoch 750, val loss: 1.0900156497955322
Epoch 760, training loss: 437.3930969238281 = 1.0884602069854736 + 50.0 * 8.726092338562012
Epoch 760, val loss: 1.0899678468704224
Epoch 770, training loss: 437.3315734863281 = 1.0883948802947998 + 50.0 * 8.724863052368164
Epoch 770, val loss: 1.0899174213409424
Epoch 780, training loss: 437.4278259277344 = 1.088331937789917 + 50.0 * 8.726790428161621
Epoch 780, val loss: 1.0898678302764893
Epoch 790, training loss: 437.27606201171875 = 1.0882694721221924 + 50.0 * 8.723755836486816
Epoch 790, val loss: 1.0898196697235107
Epoch 800, training loss: 437.4517822265625 = 1.0882091522216797 + 50.0 * 8.727272033691406
Epoch 800, val loss: 1.0897740125656128
Epoch 810, training loss: 437.5302429199219 = 1.08815336227417 + 50.0 * 8.728841781616211
Epoch 810, val loss: 1.0897308588027954
Epoch 820, training loss: 437.5736389160156 = 1.0880928039550781 + 50.0 * 8.729710578918457
Epoch 820, val loss: 1.0896821022033691
Epoch 830, training loss: 437.66632080078125 = 1.088034987449646 + 50.0 * 8.731565475463867
Epoch 830, val loss: 1.089638590812683
Epoch 840, training loss: 437.80926513671875 = 1.0879759788513184 + 50.0 * 8.73442554473877
Epoch 840, val loss: 1.0895920991897583
Epoch 850, training loss: 437.87432861328125 = 1.0879216194152832 + 50.0 * 8.73572826385498
Epoch 850, val loss: 1.0895519256591797
Epoch 860, training loss: 437.90496826171875 = 1.087864637374878 + 50.0 * 8.736342430114746
Epoch 860, val loss: 1.0895066261291504
Epoch 870, training loss: 437.8479309082031 = 1.0878069400787354 + 50.0 * 8.73520278930664
Epoch 870, val loss: 1.0894618034362793
Epoch 880, training loss: 438.13128662109375 = 1.087754249572754 + 50.0 * 8.740870475769043
Epoch 880, val loss: 1.0894229412078857
Epoch 890, training loss: 438.2021484375 = 1.08769953250885 + 50.0 * 8.742288589477539
Epoch 890, val loss: 1.0893805027008057
Epoch 900, training loss: 438.1938781738281 = 1.087646245956421 + 50.0 * 8.742124557495117
Epoch 900, val loss: 1.0893415212631226
Epoch 910, training loss: 438.25665283203125 = 1.0875924825668335 + 50.0 * 8.74338150024414
Epoch 910, val loss: 1.089300274848938
Epoch 920, training loss: 438.77197265625 = 1.0875366926193237 + 50.0 * 8.75368881225586
Epoch 920, val loss: 1.0892571210861206
Epoch 930, training loss: 438.9273986816406 = 1.0874873399734497 + 50.0 * 8.756797790527344
Epoch 930, val loss: 1.0892194509506226
Epoch 940, training loss: 438.2711486816406 = 1.087430477142334 + 50.0 * 8.743674278259277
Epoch 940, val loss: 1.0891799926757812
Epoch 950, training loss: 438.1682434082031 = 1.0873773097991943 + 50.0 * 8.741617202758789
Epoch 950, val loss: 1.0891374349594116
Epoch 960, training loss: 438.8256530761719 = 1.0873310565948486 + 50.0 * 8.754766464233398
Epoch 960, val loss: 1.0891022682189941
Epoch 970, training loss: 438.51690673828125 = 1.0872806310653687 + 50.0 * 8.748592376708984
Epoch 970, val loss: 1.0890659093856812
Epoch 980, training loss: 438.7504577636719 = 1.0872347354888916 + 50.0 * 8.753264427185059
Epoch 980, val loss: 1.0890307426452637
Epoch 990, training loss: 439.0610656738281 = 1.0871882438659668 + 50.0 * 8.759477615356445
Epoch 990, val loss: 1.088997483253479
Epoch 1000, training loss: 439.318115234375 = 1.0871423482894897 + 50.0 * 8.764619827270508
Epoch 1000, val loss: 1.0889637470245361
Epoch 1010, training loss: 439.40863037109375 = 1.087095856666565 + 50.0 * 8.766430854797363
Epoch 1010, val loss: 1.0889296531677246
Epoch 1020, training loss: 439.427978515625 = 1.0870476961135864 + 50.0 * 8.76681900024414
Epoch 1020, val loss: 1.0888947248458862
Epoch 1030, training loss: 439.52764892578125 = 1.087003231048584 + 50.0 * 8.768813133239746
Epoch 1030, val loss: 1.0888622999191284
Epoch 1040, training loss: 439.74786376953125 = 1.0869587659835815 + 50.0 * 8.773218154907227
Epoch 1040, val loss: 1.0888291597366333
Epoch 1050, training loss: 439.82684326171875 = 1.0869132280349731 + 50.0 * 8.774798393249512
Epoch 1050, val loss: 1.0887973308563232
Epoch 1060, training loss: 439.9403381347656 = 1.0868706703186035 + 50.0 * 8.777069091796875
Epoch 1060, val loss: 1.0887655019760132
Epoch 1070, training loss: 439.9300231933594 = 1.0868258476257324 + 50.0 * 8.776864051818848
Epoch 1070, val loss: 1.0887316465377808
Epoch 1080, training loss: 439.96124267578125 = 1.0867829322814941 + 50.0 * 8.777488708496094
Epoch 1080, val loss: 1.0887010097503662
Epoch 1090, training loss: 440.08660888671875 = 1.0867414474487305 + 50.0 * 8.779997825622559
Epoch 1090, val loss: 1.0886726379394531
Epoch 1100, training loss: 440.2677307128906 = 1.086700677871704 + 50.0 * 8.783620834350586
Epoch 1100, val loss: 1.0886427164077759
Epoch 1110, training loss: 440.3415832519531 = 1.0866587162017822 + 50.0 * 8.7850980758667
Epoch 1110, val loss: 1.0886094570159912
Epoch 1120, training loss: 440.2167663574219 = 1.0866150856018066 + 50.0 * 8.78260326385498
Epoch 1120, val loss: 1.0885790586471558
Epoch 1130, training loss: 440.30328369140625 = 1.086576223373413 + 50.0 * 8.784334182739258
Epoch 1130, val loss: 1.088553547859192
Epoch 1140, training loss: 440.58062744140625 = 1.0865386724472046 + 50.0 * 8.789881706237793
Epoch 1140, val loss: 1.0885264873504639
Epoch 1150, training loss: 440.5379943847656 = 1.086497187614441 + 50.0 * 8.789030075073242
Epoch 1150, val loss: 1.0884971618652344
Epoch 1160, training loss: 440.5352783203125 = 1.0864601135253906 + 50.0 * 8.788976669311523
Epoch 1160, val loss: 1.0884712934494019
Epoch 1170, training loss: 440.7187805175781 = 1.0864230394363403 + 50.0 * 8.792647361755371
Epoch 1170, val loss: 1.0884456634521484
Epoch 1180, training loss: 440.18585205078125 = 1.0863714218139648 + 50.0 * 8.781990051269531
Epoch 1180, val loss: 1.088411569595337
Epoch 1190, training loss: 440.0463562011719 = 1.0863357782363892 + 50.0 * 8.779200553894043
Epoch 1190, val loss: 1.088379144668579
Epoch 1200, training loss: 440.3849792480469 = 1.086304783821106 + 50.0 * 8.78597354888916
Epoch 1200, val loss: 1.0883601903915405
Epoch 1210, training loss: 440.7977600097656 = 1.0862730741500854 + 50.0 * 8.794229507446289
Epoch 1210, val loss: 1.0883395671844482
Epoch 1220, training loss: 441.09283447265625 = 1.0862350463867188 + 50.0 * 8.800131797790527
Epoch 1220, val loss: 1.0883134603500366
Epoch 1230, training loss: 441.1139831542969 = 1.0862042903900146 + 50.0 * 8.800555229187012
Epoch 1230, val loss: 1.0882927179336548
Epoch 1240, training loss: 441.18695068359375 = 1.0861701965332031 + 50.0 * 8.80201530456543
Epoch 1240, val loss: 1.088270902633667
Epoch 1250, training loss: 441.22998046875 = 1.0861374139785767 + 50.0 * 8.802876472473145
Epoch 1250, val loss: 1.088249683380127
Epoch 1260, training loss: 441.31884765625 = 1.0861051082611084 + 50.0 * 8.804655075073242
Epoch 1260, val loss: 1.0882282257080078
Epoch 1270, training loss: 441.3139953613281 = 1.0860718488693237 + 50.0 * 8.804558753967285
Epoch 1270, val loss: 1.0882045030593872
Epoch 1280, training loss: 441.3396301269531 = 1.0860390663146973 + 50.0 * 8.805071830749512
Epoch 1280, val loss: 1.088184118270874
Epoch 1290, training loss: 441.5513916015625 = 1.0860093832015991 + 50.0 * 8.809308052062988
Epoch 1290, val loss: 1.0881630182266235
Epoch 1300, training loss: 441.4682922363281 = 1.085976004600525 + 50.0 * 8.807646751403809
Epoch 1300, val loss: 1.0881433486938477
Epoch 1310, training loss: 441.72943115234375 = 1.0859475135803223 + 50.0 * 8.812870025634766
Epoch 1310, val loss: 1.08812415599823
Epoch 1320, training loss: 442.0878601074219 = 1.0859192609786987 + 50.0 * 8.820038795471191
Epoch 1320, val loss: 1.0881054401397705
Epoch 1330, training loss: 441.7168884277344 = 1.0858858823776245 + 50.0 * 8.812620162963867
Epoch 1330, val loss: 1.0880831480026245
Epoch 1340, training loss: 441.8232727050781 = 1.0858573913574219 + 50.0 * 8.814748764038086
Epoch 1340, val loss: 1.0880658626556396
Epoch 1350, training loss: 442.0582275390625 = 1.0858302116394043 + 50.0 * 8.819448471069336
Epoch 1350, val loss: 1.0880451202392578
Epoch 1360, training loss: 442.10736083984375 = 1.0858008861541748 + 50.0 * 8.820430755615234
Epoch 1360, val loss: 1.0880292654037476
Epoch 1370, training loss: 442.1142883300781 = 1.0857723951339722 + 50.0 * 8.82056999206543
Epoch 1370, val loss: 1.0880112648010254
Epoch 1380, training loss: 442.2296447753906 = 1.08574640750885 + 50.0 * 8.822877883911133
Epoch 1380, val loss: 1.0879943370819092
Epoch 1390, training loss: 442.37921142578125 = 1.0857198238372803 + 50.0 * 8.8258695602417
Epoch 1390, val loss: 1.087978720664978
Epoch 1400, training loss: 442.4658508300781 = 1.085692048072815 + 50.0 * 8.827603340148926
Epoch 1400, val loss: 1.087962031364441
Epoch 1410, training loss: 442.4842224121094 = 1.0856660604476929 + 50.0 * 8.827971458435059
Epoch 1410, val loss: 1.0879424810409546
Epoch 1420, training loss: 442.4477233886719 = 1.085639238357544 + 50.0 * 8.827241897583008
Epoch 1420, val loss: 1.0879281759262085
Epoch 1430, training loss: 442.6389465332031 = 1.085614562034607 + 50.0 * 8.831067085266113
Epoch 1430, val loss: 1.087913155555725
Epoch 1440, training loss: 442.9367980957031 = 1.0855903625488281 + 50.0 * 8.837023735046387
Epoch 1440, val loss: 1.087897777557373
Epoch 1450, training loss: 442.76739501953125 = 1.0855627059936523 + 50.0 * 8.833636283874512
Epoch 1450, val loss: 1.0878785848617554
Epoch 1460, training loss: 442.81060791015625 = 1.085539698600769 + 50.0 * 8.834501266479492
Epoch 1460, val loss: 1.0878673791885376
Epoch 1470, training loss: 442.9554748535156 = 1.0855168104171753 + 50.0 * 8.83739948272705
Epoch 1470, val loss: 1.0878535509109497
Epoch 1480, training loss: 442.9847717285156 = 1.0854923725128174 + 50.0 * 8.83798599243164
Epoch 1480, val loss: 1.087839961051941
Epoch 1490, training loss: 443.1017150878906 = 1.0854684114456177 + 50.0 * 8.840324401855469
Epoch 1490, val loss: 1.0878260135650635
Epoch 1500, training loss: 443.3403625488281 = 1.0854488611221313 + 50.0 * 8.845098495483398
Epoch 1500, val loss: 1.0878140926361084
Epoch 1510, training loss: 443.2852783203125 = 1.0854257345199585 + 50.0 * 8.84399700164795
Epoch 1510, val loss: 1.0877997875213623
Epoch 1520, training loss: 443.277587890625 = 1.0854038000106812 + 50.0 * 8.843843460083008
Epoch 1520, val loss: 1.0877881050109863
Epoch 1530, training loss: 443.38519287109375 = 1.0853828191757202 + 50.0 * 8.845995903015137
Epoch 1530, val loss: 1.087777018547058
Epoch 1540, training loss: 443.5820007324219 = 1.0853633880615234 + 50.0 * 8.849932670593262
Epoch 1540, val loss: 1.0877658128738403
Epoch 1550, training loss: 443.29315185546875 = 1.085331916809082 + 50.0 * 8.844156265258789
Epoch 1550, val loss: 1.0877443552017212
Epoch 1560, training loss: 443.4896545410156 = 1.0853121280670166 + 50.0 * 8.8480863571167
Epoch 1560, val loss: 1.087729573249817
Epoch 1570, training loss: 443.1094665527344 = 1.0852915048599243 + 50.0 * 8.840483665466309
Epoch 1570, val loss: 1.0877208709716797
Epoch 1580, training loss: 443.21807861328125 = 1.0852750539779663 + 50.0 * 8.842656135559082
Epoch 1580, val loss: 1.087714433670044
Epoch 1590, training loss: 443.5434265136719 = 1.0852597951889038 + 50.0 * 8.849163055419922
Epoch 1590, val loss: 1.0877079963684082
Epoch 1600, training loss: 443.65740966796875 = 1.0852417945861816 + 50.0 * 8.85144329071045
Epoch 1600, val loss: 1.0876978635787964
Epoch 1610, training loss: 443.7919616699219 = 1.0852233171463013 + 50.0 * 8.854134559631348
Epoch 1610, val loss: 1.0876888036727905
Epoch 1620, training loss: 443.8084411621094 = 1.0852056741714478 + 50.0 * 8.854464530944824
Epoch 1620, val loss: 1.087679386138916
Epoch 1630, training loss: 443.72808837890625 = 1.085186243057251 + 50.0 * 8.852858543395996
Epoch 1630, val loss: 1.0876692533493042
Epoch 1640, training loss: 443.88946533203125 = 1.0851702690124512 + 50.0 * 8.856085777282715
Epoch 1640, val loss: 1.0876621007919312
Epoch 1650, training loss: 444.03057861328125 = 1.085153579711914 + 50.0 * 8.858908653259277
Epoch 1650, val loss: 1.0876542329788208
Epoch 1660, training loss: 444.08905029296875 = 1.0851362943649292 + 50.0 * 8.860077857971191
Epoch 1660, val loss: 1.0876436233520508
Epoch 1670, training loss: 444.02520751953125 = 1.085118055343628 + 50.0 * 8.85880184173584
Epoch 1670, val loss: 1.0876351594924927
Epoch 1680, training loss: 444.23077392578125 = 1.085103154182434 + 50.0 * 8.862913131713867
Epoch 1680, val loss: 1.0876270532608032
Epoch 1690, training loss: 444.2054748535156 = 1.0850855112075806 + 50.0 * 8.862407684326172
Epoch 1690, val loss: 1.0876193046569824
Epoch 1700, training loss: 444.26763916015625 = 1.0850696563720703 + 50.0 * 8.863651275634766
Epoch 1700, val loss: 1.087612271308899
Epoch 1710, training loss: 444.3497619628906 = 1.0850549936294556 + 50.0 * 8.865294456481934
Epoch 1710, val loss: 1.087604284286499
Epoch 1720, training loss: 444.47088623046875 = 1.0850403308868408 + 50.0 * 8.867716789245605
Epoch 1720, val loss: 1.0875979661941528
Epoch 1730, training loss: 444.5279541015625 = 1.085016131401062 + 50.0 * 8.868858337402344
Epoch 1730, val loss: 1.0875800848007202
Epoch 1740, training loss: 444.84454345703125 = 1.0850090980529785 + 50.0 * 8.875190734863281
Epoch 1740, val loss: 1.0875808000564575
Epoch 1750, training loss: 444.5788879394531 = 1.0849888324737549 + 50.0 * 8.869877815246582
Epoch 1750, val loss: 1.0875730514526367
Epoch 1760, training loss: 444.66290283203125 = 1.0849740505218506 + 50.0 * 8.871559143066406
Epoch 1760, val loss: 1.0875673294067383
Epoch 1770, training loss: 444.9557800292969 = 1.0849641561508179 + 50.0 * 8.877416610717773
Epoch 1770, val loss: 1.0875651836395264
Epoch 1780, training loss: 445.03448486328125 = 1.0849512815475464 + 50.0 * 8.878990173339844
Epoch 1780, val loss: 1.087557077407837
Epoch 1790, training loss: 444.9425964355469 = 1.0849379301071167 + 50.0 * 8.877153396606445
Epoch 1790, val loss: 1.0875526666641235
Epoch 1800, training loss: 445.1661071777344 = 1.0849262475967407 + 50.0 * 8.881623268127441
Epoch 1800, val loss: 1.0875476598739624
Epoch 1810, training loss: 445.3118896484375 = 1.0849140882492065 + 50.0 * 8.884539604187012
Epoch 1810, val loss: 1.0875440835952759
Epoch 1820, training loss: 445.40399169921875 = 1.0849028825759888 + 50.0 * 8.886382102966309
Epoch 1820, val loss: 1.087541103363037
Epoch 1830, training loss: 445.4325256347656 = 1.0848896503448486 + 50.0 * 8.88695240020752
Epoch 1830, val loss: 1.0875341892242432
Epoch 1840, training loss: 445.4148864746094 = 1.0848771333694458 + 50.0 * 8.886600494384766
Epoch 1840, val loss: 1.0875298976898193
Epoch 1850, training loss: 445.7215576171875 = 1.0848675966262817 + 50.0 * 8.892733573913574
Epoch 1850, val loss: 1.0875272750854492
Epoch 1860, training loss: 445.6407775878906 = 1.0848536491394043 + 50.0 * 8.891119003295898
Epoch 1860, val loss: 1.0875216722488403
Epoch 1870, training loss: 445.79205322265625 = 1.0848376750946045 + 50.0 * 8.894144058227539
Epoch 1870, val loss: 1.0875121355056763
Epoch 1880, training loss: 445.4402160644531 = 1.0848222970962524 + 50.0 * 8.887107849121094
Epoch 1880, val loss: 1.0875048637390137
Epoch 1890, training loss: 445.3829650878906 = 1.0848139524459839 + 50.0 * 8.885963439941406
Epoch 1890, val loss: 1.08750319480896
Epoch 1900, training loss: 445.7361755371094 = 1.084807276725769 + 50.0 * 8.893027305603027
Epoch 1900, val loss: 1.0875020027160645
Epoch 1910, training loss: 446.23828125 = 1.0848007202148438 + 50.0 * 8.903069496154785
Epoch 1910, val loss: 1.0875035524368286
Epoch 1920, training loss: 446.472412109375 = 1.084794044494629 + 50.0 * 8.90775203704834
Epoch 1920, val loss: 1.0875027179718018
Epoch 1930, training loss: 446.65692138671875 = 1.0847851037979126 + 50.0 * 8.911442756652832
Epoch 1930, val loss: 1.0875000953674316
Epoch 1940, training loss: 446.631591796875 = 1.0847752094268799 + 50.0 * 8.91093635559082
Epoch 1940, val loss: 1.0874965190887451
Epoch 1950, training loss: 446.88861083984375 = 1.08476722240448 + 50.0 * 8.91607666015625
Epoch 1950, val loss: 1.0874958038330078
Epoch 1960, training loss: 447.0176696777344 = 1.0847581624984741 + 50.0 * 8.918658256530762
Epoch 1960, val loss: 1.0874937772750854
Epoch 1970, training loss: 446.60162353515625 = 1.084736704826355 + 50.0 * 8.910337448120117
Epoch 1970, val loss: 1.087480902671814
Epoch 1980, training loss: 444.1189880371094 = 1.0846829414367676 + 50.0 * 8.860686302185059
Epoch 1980, val loss: 1.0874242782592773
Epoch 1990, training loss: 445.0928955078125 = 1.084693431854248 + 50.0 * 8.88016414642334
Epoch 1990, val loss: 1.0874598026275635
Epoch 2000, training loss: 444.664794921875 = 1.0846799612045288 + 50.0 * 8.871602058410645
Epoch 2000, val loss: 1.087442398071289
Epoch 2010, training loss: 445.2100524902344 = 1.0846840143203735 + 50.0 * 8.88250732421875
Epoch 2010, val loss: 1.087456464767456
Epoch 2020, training loss: 445.8175354003906 = 1.0846816301345825 + 50.0 * 8.894657135009766
Epoch 2020, val loss: 1.0874576568603516
Epoch 2030, training loss: 445.76666259765625 = 1.0846781730651855 + 50.0 * 8.89363956451416
Epoch 2030, val loss: 1.0874605178833008
Epoch 2040, training loss: 445.9976501464844 = 1.0846736431121826 + 50.0 * 8.898259162902832
Epoch 2040, val loss: 1.0874618291854858
Epoch 2050, training loss: 446.6270751953125 = 1.0846728086471558 + 50.0 * 8.910847663879395
Epoch 2050, val loss: 1.0874660015106201
Epoch 2060, training loss: 446.8984680175781 = 1.0846697092056274 + 50.0 * 8.916275978088379
Epoch 2060, val loss: 1.0874686241149902
Epoch 2070, training loss: 447.08074951171875 = 1.0846621990203857 + 50.0 * 8.919921875
Epoch 2070, val loss: 1.0874677896499634
Epoch 2080, training loss: 447.2469177246094 = 1.0846564769744873 + 50.0 * 8.923245429992676
Epoch 2080, val loss: 1.0874675512313843
Epoch 2090, training loss: 447.3513488769531 = 1.0846511125564575 + 50.0 * 8.925333976745605
Epoch 2090, val loss: 1.0874680280685425
Epoch 2100, training loss: 447.4758605957031 = 1.0846446752548218 + 50.0 * 8.927824020385742
Epoch 2100, val loss: 1.0874677896499634
Epoch 2110, training loss: 447.4286804199219 = 1.084638237953186 + 50.0 * 8.926880836486816
Epoch 2110, val loss: 1.087466835975647
Epoch 2120, training loss: 447.553466796875 = 1.0846322774887085 + 50.0 * 8.929376602172852
Epoch 2120, val loss: 1.0874665975570679
Epoch 2130, training loss: 447.71649169921875 = 1.0846264362335205 + 50.0 * 8.932637214660645
Epoch 2130, val loss: 1.0874665975570679
Epoch 2140, training loss: 447.88055419921875 = 1.0846211910247803 + 50.0 * 8.935918807983398
Epoch 2140, val loss: 1.087466835975647
Epoch 2150, training loss: 447.6177062988281 = 1.084609031677246 + 50.0 * 8.930662155151367
Epoch 2150, val loss: 1.087459921836853
Epoch 2160, training loss: 447.5124206542969 = 1.084604024887085 + 50.0 * 8.928556442260742
Epoch 2160, val loss: 1.087461233139038
Epoch 2170, training loss: 447.6274719238281 = 1.0845980644226074 + 50.0 * 8.93085765838623
Epoch 2170, val loss: 1.0874606370925903
Epoch 2180, training loss: 448.0028991699219 = 1.0845947265625 + 50.0 * 8.938365936279297
Epoch 2180, val loss: 1.08746337890625
Epoch 2190, training loss: 448.43145751953125 = 1.0845922231674194 + 50.0 * 8.946937561035156
Epoch 2190, val loss: 1.087465763092041
Epoch 2200, training loss: 448.3023986816406 = 1.0845855474472046 + 50.0 * 8.944355964660645
Epoch 2200, val loss: 1.0874649286270142
Epoch 2210, training loss: 448.27703857421875 = 1.0845805406570435 + 50.0 * 8.943848609924316
Epoch 2210, val loss: 1.0874645709991455
Epoch 2220, training loss: 448.4875793457031 = 1.08457612991333 + 50.0 * 8.948060035705566
Epoch 2220, val loss: 1.087465763092041
Epoch 2230, training loss: 448.587890625 = 1.0845715999603271 + 50.0 * 8.950066566467285
Epoch 2230, val loss: 1.0874652862548828
Epoch 2240, training loss: 448.7696228027344 = 1.0845675468444824 + 50.0 * 8.95370101928711
Epoch 2240, val loss: 1.087466835975647
Epoch 2250, training loss: 448.8260803222656 = 1.0845627784729004 + 50.0 * 8.954830169677734
Epoch 2250, val loss: 1.0874671936035156
Epoch 2260, training loss: 448.9573059082031 = 1.084559679031372 + 50.0 * 8.957454681396484
Epoch 2260, val loss: 1.0874696969985962
Epoch 2270, training loss: 449.0276184082031 = 1.0845544338226318 + 50.0 * 8.958861351013184
Epoch 2270, val loss: 1.0874691009521484
Epoch 2280, training loss: 448.69830322265625 = 1.0845462083816528 + 50.0 * 8.952275276184082
Epoch 2280, val loss: 1.087465763092041
Epoch 2290, training loss: 448.8516540527344 = 1.084543228149414 + 50.0 * 8.955342292785645
Epoch 2290, val loss: 1.087467908859253
Epoch 2300, training loss: 449.2281188964844 = 1.0845417976379395 + 50.0 * 8.962871551513672
Epoch 2300, val loss: 1.087471604347229
Epoch 2310, training loss: 449.1938171386719 = 1.0845332145690918 + 50.0 * 8.962185859680176
Epoch 2310, val loss: 1.087468147277832
Epoch 2320, training loss: 448.8671569824219 = 1.084527611732483 + 50.0 * 8.955652236938477
Epoch 2320, val loss: 1.0874677896499634
Epoch 2330, training loss: 449.2326965332031 = 1.0845216512680054 + 50.0 * 8.962963104248047
Epoch 2330, val loss: 1.087465524673462
Epoch 2340, training loss: 448.9806823730469 = 1.084514856338501 + 50.0 * 8.957923889160156
Epoch 2340, val loss: 1.0874651670455933
Epoch 2350, training loss: 449.2299499511719 = 1.0845122337341309 + 50.0 * 8.962908744812012
Epoch 2350, val loss: 1.0874671936035156
Epoch 2360, training loss: 449.6564636230469 = 1.0845146179199219 + 50.0 * 8.971439361572266
Epoch 2360, val loss: 1.0874720811843872
Epoch 2370, training loss: 449.9759521484375 = 1.0845143795013428 + 50.0 * 8.977828979492188
Epoch 2370, val loss: 1.0874758958816528
Epoch 2380, training loss: 450.31756591796875 = 1.0845146179199219 + 50.0 * 8.984661102294922
Epoch 2380, val loss: 1.0874794721603394
Epoch 2390, training loss: 450.4393005371094 = 1.084511399269104 + 50.0 * 8.987095832824707
Epoch 2390, val loss: 1.0874806642532349
Epoch 2400, training loss: 450.42242431640625 = 1.0845075845718384 + 50.0 * 8.9867582321167
Epoch 2400, val loss: 1.0874810218811035
Epoch 2410, training loss: 450.5061340332031 = 1.0845047235488892 + 50.0 * 8.988432884216309
Epoch 2410, val loss: 1.0874820947647095
Epoch 2420, training loss: 450.5722961425781 = 1.0845022201538086 + 50.0 * 8.989755630493164
Epoch 2420, val loss: 1.0874829292297363
Epoch 2430, training loss: 448.97882080078125 = 1.0844709873199463 + 50.0 * 8.957886695861816
Epoch 2430, val loss: 1.0874650478363037
Epoch 2440, training loss: 449.5235595703125 = 1.0844768285751343 + 50.0 * 8.968781471252441
Epoch 2440, val loss: 1.0874691009521484
Epoch 2450, training loss: 449.5079650878906 = 1.0844792127609253 + 50.0 * 8.968469619750977
Epoch 2450, val loss: 1.0874747037887573
Epoch 2460, training loss: 449.7816162109375 = 1.0844812393188477 + 50.0 * 8.973942756652832
Epoch 2460, val loss: 1.087481141090393
Epoch 2470, training loss: 450.40460205078125 = 1.0844868421554565 + 50.0 * 8.98640251159668
Epoch 2470, val loss: 1.0874886512756348
Epoch 2480, training loss: 450.85333251953125 = 1.0844898223876953 + 50.0 * 8.995376586914062
Epoch 2480, val loss: 1.0874958038330078
Epoch 2490, training loss: 451.09991455078125 = 1.0844908952713013 + 50.0 * 9.0003080368042
Epoch 2490, val loss: 1.0875004529953003
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.39666666666666667
0.8188799536332682
The final CL Acc:0.39686, 0.00038, The final GNN Acc:0.81948, 0.00071
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110608])
remove edge: torch.Size([2, 66648])
updated graph: torch.Size([2, 88608])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 502.0130920410156 = 1.095957636833191 + 50.0 * 10.018342971801758
Epoch 0, val loss: 1.0954049825668335
Epoch 10, training loss: 478.1014099121094 = 1.0958317518234253 + 50.0 * 9.540111541748047
Epoch 10, val loss: 1.095261812210083
Epoch 20, training loss: 468.3912048339844 = 1.0957821607589722 + 50.0 * 9.345908164978027
Epoch 20, val loss: 1.0951939821243286
Epoch 30, training loss: 461.9775390625 = 1.0956770181655884 + 50.0 * 9.217637062072754
Epoch 30, val loss: 1.09507155418396
Epoch 40, training loss: 457.2177429199219 = 1.0955852270126343 + 50.0 * 9.122443199157715
Epoch 40, val loss: 1.0949625968933105
Epoch 50, training loss: 453.5088195800781 = 1.0954996347427368 + 50.0 * 9.048266410827637
Epoch 50, val loss: 1.0948569774627686
Epoch 60, training loss: 450.4587707519531 = 1.0954139232635498 + 50.0 * 8.987266540527344
Epoch 60, val loss: 1.094752311706543
Epoch 70, training loss: 447.7998046875 = 1.0953327417373657 + 50.0 * 8.934089660644531
Epoch 70, val loss: 1.0946545600891113
Epoch 80, training loss: 445.5481872558594 = 1.0952656269073486 + 50.0 * 8.889058113098145
Epoch 80, val loss: 1.0945688486099243
Epoch 90, training loss: 443.6031799316406 = 1.09519362449646 + 50.0 * 8.850159645080566
Epoch 90, val loss: 1.0944793224334717
Epoch 100, training loss: 441.89727783203125 = 1.0951275825500488 + 50.0 * 8.81604290008545
Epoch 100, val loss: 1.0943955183029175
Epoch 110, training loss: 440.43975830078125 = 1.0950665473937988 + 50.0 * 8.786893844604492
Epoch 110, val loss: 1.0943156480789185
Epoch 120, training loss: 439.2298583984375 = 1.0950045585632324 + 50.0 * 8.762697219848633
Epoch 120, val loss: 1.094237208366394
Epoch 130, training loss: 438.1073303222656 = 1.0949463844299316 + 50.0 * 8.74024772644043
Epoch 130, val loss: 1.0941587686538696
Epoch 140, training loss: 437.09112548828125 = 1.0948866605758667 + 50.0 * 8.719924926757812
Epoch 140, val loss: 1.0940818786621094
Epoch 150, training loss: 436.2116394042969 = 1.0948306322097778 + 50.0 * 8.702336311340332
Epoch 150, val loss: 1.0940074920654297
Epoch 160, training loss: 435.4441833496094 = 1.094773530960083 + 50.0 * 8.68698787689209
Epoch 160, val loss: 1.0939327478408813
Epoch 170, training loss: 434.76739501953125 = 1.0947160720825195 + 50.0 * 8.673453330993652
Epoch 170, val loss: 1.093856930732727
Epoch 180, training loss: 434.1155090332031 = 1.0946598052978516 + 50.0 * 8.660416603088379
Epoch 180, val loss: 1.0937868356704712
Epoch 190, training loss: 433.5458984375 = 1.0946043729782104 + 50.0 * 8.649025917053223
Epoch 190, val loss: 1.093714714050293
Epoch 200, training loss: 433.0731506347656 = 1.0945534706115723 + 50.0 * 8.639572143554688
Epoch 200, val loss: 1.0936473608016968
Epoch 210, training loss: 432.61883544921875 = 1.0945028066635132 + 50.0 * 8.630486488342285
Epoch 210, val loss: 1.093579888343811
Epoch 220, training loss: 432.28436279296875 = 1.094452142715454 + 50.0 * 8.623798370361328
Epoch 220, val loss: 1.0935114622116089
Epoch 230, training loss: 431.9167785644531 = 1.0944020748138428 + 50.0 * 8.616447448730469
Epoch 230, val loss: 1.0934486389160156
Epoch 240, training loss: 431.5157470703125 = 1.094352126121521 + 50.0 * 8.608428001403809
Epoch 240, val loss: 1.0933833122253418
Epoch 250, training loss: 431.2449951171875 = 1.0943052768707275 + 50.0 * 8.60301399230957
Epoch 250, val loss: 1.0933201313018799
Epoch 260, training loss: 430.8850402832031 = 1.0942564010620117 + 50.0 * 8.595815658569336
Epoch 260, val loss: 1.093255877494812
Epoch 270, training loss: 430.7222595214844 = 1.094213843345642 + 50.0 * 8.592560768127441
Epoch 270, val loss: 1.093196153640747
Epoch 280, training loss: 430.5411682128906 = 1.0941721200942993 + 50.0 * 8.588939666748047
Epoch 280, val loss: 1.093140721321106
Epoch 290, training loss: 430.30755615234375 = 1.0941290855407715 + 50.0 * 8.584268569946289
Epoch 290, val loss: 1.0930836200714111
Epoch 300, training loss: 430.1607971191406 = 1.0940868854522705 + 50.0 * 8.581334114074707
Epoch 300, val loss: 1.093024492263794
Epoch 310, training loss: 430.023681640625 = 1.0940463542938232 + 50.0 * 8.578592300415039
Epoch 310, val loss: 1.0929694175720215
Epoch 320, training loss: 429.838134765625 = 1.0940062999725342 + 50.0 * 8.574882507324219
Epoch 320, val loss: 1.092915415763855
Epoch 330, training loss: 429.7153625488281 = 1.0939687490463257 + 50.0 * 8.572427749633789
Epoch 330, val loss: 1.0928641557693481
Epoch 340, training loss: 429.5979309082031 = 1.0939284563064575 + 50.0 * 8.570079803466797
Epoch 340, val loss: 1.0928105115890503
Epoch 350, training loss: 429.591552734375 = 1.0938961505889893 + 50.0 * 8.569952964782715
Epoch 350, val loss: 1.0927613973617554
Epoch 360, training loss: 429.5191345214844 = 1.093859314918518 + 50.0 * 8.56850528717041
Epoch 360, val loss: 1.0927108526229858
Epoch 370, training loss: 429.6208801269531 = 1.0938230752944946 + 50.0 * 8.570541381835938
Epoch 370, val loss: 1.0926611423492432
Epoch 380, training loss: 429.569580078125 = 1.0937882661819458 + 50.0 * 8.5695161819458
Epoch 380, val loss: 1.0926125049591064
Epoch 390, training loss: 429.4790954589844 = 1.0937564373016357 + 50.0 * 8.567707061767578
Epoch 390, val loss: 1.0925674438476562
Epoch 400, training loss: 429.50006103515625 = 1.093723177909851 + 50.0 * 8.568126678466797
Epoch 400, val loss: 1.0925195217132568
Epoch 410, training loss: 429.5870361328125 = 1.093693733215332 + 50.0 * 8.569867134094238
Epoch 410, val loss: 1.0924758911132812
Epoch 420, training loss: 429.5453796386719 = 1.0936663150787354 + 50.0 * 8.569034576416016
Epoch 420, val loss: 1.0924347639083862
Epoch 430, training loss: 429.4696350097656 = 1.0936322212219238 + 50.0 * 8.567520141601562
Epoch 430, val loss: 1.09238862991333
Epoch 440, training loss: 429.4272155761719 = 1.093604326248169 + 50.0 * 8.566672325134277
Epoch 440, val loss: 1.09234619140625
Epoch 450, training loss: 429.5560302734375 = 1.093575358390808 + 50.0 * 8.569249153137207
Epoch 450, val loss: 1.0923017263412476
Epoch 460, training loss: 429.3781433105469 = 1.0935461521148682 + 50.0 * 8.565691947937012
Epoch 460, val loss: 1.0922625064849854
Epoch 470, training loss: 429.2618408203125 = 1.093515157699585 + 50.0 * 8.563366889953613
Epoch 470, val loss: 1.0922216176986694
Epoch 480, training loss: 429.7298583984375 = 1.0935020446777344 + 50.0 * 8.57272720336914
Epoch 480, val loss: 1.0921903848648071
Epoch 490, training loss: 429.39599609375 = 1.093469262123108 + 50.0 * 8.56605052947998
Epoch 490, val loss: 1.0921462774276733
Epoch 500, training loss: 429.52606201171875 = 1.0934463739395142 + 50.0 * 8.568652153015137
Epoch 500, val loss: 1.092111587524414
Epoch 510, training loss: 429.55157470703125 = 1.0934240818023682 + 50.0 * 8.56916332244873
Epoch 510, val loss: 1.0920765399932861
Epoch 520, training loss: 429.6700744628906 = 1.093402624130249 + 50.0 * 8.571533203125
Epoch 520, val loss: 1.0920430421829224
Epoch 530, training loss: 429.76043701171875 = 1.0933793783187866 + 50.0 * 8.573341369628906
Epoch 530, val loss: 1.092008352279663
Epoch 540, training loss: 429.64788818359375 = 1.0933572053909302 + 50.0 * 8.571090698242188
Epoch 540, val loss: 1.091974139213562
Epoch 550, training loss: 429.82281494140625 = 1.0933374166488647 + 50.0 * 8.574589729309082
Epoch 550, val loss: 1.0919421911239624
Epoch 560, training loss: 429.8867492675781 = 1.0933170318603516 + 50.0 * 8.575868606567383
Epoch 560, val loss: 1.0919103622436523
Epoch 570, training loss: 429.9774169921875 = 1.0932966470718384 + 50.0 * 8.577682495117188
Epoch 570, val loss: 1.0918794870376587
Epoch 580, training loss: 430.0973815917969 = 1.09328031539917 + 50.0 * 8.580081939697266
Epoch 580, val loss: 1.0918506383895874
Epoch 590, training loss: 430.1390380859375 = 1.0932639837265015 + 50.0 * 8.580915451049805
Epoch 590, val loss: 1.0918211936950684
Epoch 600, training loss: 429.88031005859375 = 1.0932360887527466 + 50.0 * 8.5757417678833
Epoch 600, val loss: 1.091781497001648
Epoch 610, training loss: 430.1745300292969 = 1.0932226181030273 + 50.0 * 8.581625938415527
Epoch 610, val loss: 1.091758131980896
Epoch 620, training loss: 430.1681213378906 = 1.0932079553604126 + 50.0 * 8.581498146057129
Epoch 620, val loss: 1.0917327404022217
Epoch 630, training loss: 430.3694152832031 = 1.093193769454956 + 50.0 * 8.585524559020996
Epoch 630, val loss: 1.0917086601257324
Epoch 640, training loss: 430.58660888671875 = 1.0931788682937622 + 50.0 * 8.589868545532227
Epoch 640, val loss: 1.091683268547058
Epoch 650, training loss: 430.5101013183594 = 1.0931614637374878 + 50.0 * 8.588338851928711
Epoch 650, val loss: 1.0916565656661987
Epoch 660, training loss: 430.59735107421875 = 1.0931475162506104 + 50.0 * 8.590084075927734
Epoch 660, val loss: 1.0916324853897095
Epoch 670, training loss: 430.7149353027344 = 1.0931355953216553 + 50.0 * 8.592435836791992
Epoch 670, val loss: 1.0916088819503784
Epoch 680, training loss: 430.86383056640625 = 1.0931217670440674 + 50.0 * 8.595414161682129
Epoch 680, val loss: 1.0915839672088623
Epoch 690, training loss: 430.8480224609375 = 1.09310781955719 + 50.0 * 8.595098495483398
Epoch 690, val loss: 1.0915602445602417
Epoch 700, training loss: 430.96075439453125 = 1.093095064163208 + 50.0 * 8.597352981567383
Epoch 700, val loss: 1.0915385484695435
Epoch 710, training loss: 431.14263916015625 = 1.0930795669555664 + 50.0 * 8.600991249084473
Epoch 710, val loss: 1.0915101766586304
Epoch 720, training loss: 430.9989318847656 = 1.0930664539337158 + 50.0 * 8.598116874694824
Epoch 720, val loss: 1.0914902687072754
Epoch 730, training loss: 430.9621887207031 = 1.093053936958313 + 50.0 * 8.597382545471191
Epoch 730, val loss: 1.0914685726165771
Epoch 740, training loss: 430.9614562988281 = 1.0930439233779907 + 50.0 * 8.597368240356445
Epoch 740, val loss: 1.0914504528045654
Epoch 750, training loss: 431.2652893066406 = 1.0930366516113281 + 50.0 * 8.603445053100586
Epoch 750, val loss: 1.091432809829712
Epoch 760, training loss: 431.44317626953125 = 1.0930266380310059 + 50.0 * 8.607003211975098
Epoch 760, val loss: 1.0914140939712524
Epoch 770, training loss: 431.52081298828125 = 1.0930167436599731 + 50.0 * 8.608555793762207
Epoch 770, val loss: 1.0913941860198975
Epoch 780, training loss: 431.6699523925781 = 1.0930068492889404 + 50.0 * 8.611538887023926
Epoch 780, val loss: 1.0913761854171753
Epoch 790, training loss: 431.80364990234375 = 1.0929975509643555 + 50.0 * 8.614212989807129
Epoch 790, val loss: 1.0913593769073486
Epoch 800, training loss: 431.9682312011719 = 1.09298837184906 + 50.0 * 8.617505073547363
Epoch 800, val loss: 1.0913366079330444
Epoch 810, training loss: 431.8745422363281 = 1.0929789543151855 + 50.0 * 8.615631103515625
Epoch 810, val loss: 1.0913225412368774
Epoch 820, training loss: 431.5207824707031 = 1.0929611921310425 + 50.0 * 8.608556747436523
Epoch 820, val loss: 1.0912991762161255
Epoch 830, training loss: 431.8170471191406 = 1.0929583311080933 + 50.0 * 8.614481925964355
Epoch 830, val loss: 1.0912857055664062
Epoch 840, training loss: 431.9138488769531 = 1.0929522514343262 + 50.0 * 8.61641788482666
Epoch 840, val loss: 1.0912717580795288
Epoch 850, training loss: 432.0189514160156 = 1.0929449796676636 + 50.0 * 8.61851978302002
Epoch 850, val loss: 1.0912569761276245
Epoch 860, training loss: 432.1931457519531 = 1.0929384231567383 + 50.0 * 8.622004508972168
Epoch 860, val loss: 1.0912433862686157
Epoch 870, training loss: 432.36181640625 = 1.092932939529419 + 50.0 * 8.625377655029297
Epoch 870, val loss: 1.091230034828186
Epoch 880, training loss: 432.4276123046875 = 1.0929267406463623 + 50.0 * 8.626693725585938
Epoch 880, val loss: 1.0912154912948608
Epoch 890, training loss: 432.4165954589844 = 1.0929174423217773 + 50.0 * 8.626473426818848
Epoch 890, val loss: 1.0911997556686401
Epoch 900, training loss: 432.4171142578125 = 1.0929111242294312 + 50.0 * 8.626483917236328
Epoch 900, val loss: 1.0911868810653687
Epoch 910, training loss: 432.7352294921875 = 1.0929086208343506 + 50.0 * 8.63284683227539
Epoch 910, val loss: 1.0911750793457031
Epoch 920, training loss: 432.8160705566406 = 1.0929011106491089 + 50.0 * 8.6344633102417
Epoch 920, val loss: 1.0911612510681152
Epoch 930, training loss: 432.7935485839844 = 1.0928950309753418 + 50.0 * 8.634013175964355
Epoch 930, val loss: 1.091149091720581
Epoch 940, training loss: 432.99298095703125 = 1.0928906202316284 + 50.0 * 8.638001441955566
Epoch 940, val loss: 1.0911377668380737
Epoch 950, training loss: 433.10302734375 = 1.0928813219070435 + 50.0 * 8.640202522277832
Epoch 950, val loss: 1.0911223888397217
Epoch 960, training loss: 433.27459716796875 = 1.092879056930542 + 50.0 * 8.643634796142578
Epoch 960, val loss: 1.0911122560501099
Epoch 970, training loss: 433.27288818359375 = 1.092875599861145 + 50.0 * 8.643600463867188
Epoch 970, val loss: 1.0911022424697876
Epoch 980, training loss: 433.5077819824219 = 1.0928733348846436 + 50.0 * 8.648298263549805
Epoch 980, val loss: 1.091093897819519
Epoch 990, training loss: 433.7391052246094 = 1.0928715467453003 + 50.0 * 8.652924537658691
Epoch 990, val loss: 1.0910847187042236
Epoch 1000, training loss: 433.809326171875 = 1.0928677320480347 + 50.0 * 8.654329299926758
Epoch 1000, val loss: 1.0910756587982178
Epoch 1010, training loss: 433.95361328125 = 1.0928637981414795 + 50.0 * 8.657215118408203
Epoch 1010, val loss: 1.0910654067993164
Epoch 1020, training loss: 434.1875915527344 = 1.092861533164978 + 50.0 * 8.661894798278809
Epoch 1020, val loss: 1.0910577774047852
Epoch 1030, training loss: 434.3334655761719 = 1.0928585529327393 + 50.0 * 8.664812088012695
Epoch 1030, val loss: 1.0910465717315674
Epoch 1040, training loss: 434.3131408691406 = 1.0928528308868408 + 50.0 * 8.664405822753906
Epoch 1040, val loss: 1.0910372734069824
Epoch 1050, training loss: 434.413818359375 = 1.0928505659103394 + 50.0 * 8.66641902923584
Epoch 1050, val loss: 1.091029405593872
Epoch 1060, training loss: 434.603515625 = 1.0928477048873901 + 50.0 * 8.67021369934082
Epoch 1060, val loss: 1.0910205841064453
Epoch 1070, training loss: 434.6893005371094 = 1.0928447246551514 + 50.0 * 8.671929359436035
Epoch 1070, val loss: 1.0910125970840454
Epoch 1080, training loss: 434.77813720703125 = 1.0928417444229126 + 50.0 * 8.6737060546875
Epoch 1080, val loss: 1.0910038948059082
Epoch 1090, training loss: 434.8712158203125 = 1.0928395986557007 + 50.0 * 8.675567626953125
Epoch 1090, val loss: 1.0909969806671143
Epoch 1100, training loss: 434.7385559082031 = 1.0928343534469604 + 50.0 * 8.672914505004883
Epoch 1100, val loss: 1.0909866094589233
Epoch 1110, training loss: 434.8970031738281 = 1.0928330421447754 + 50.0 * 8.6760835647583
Epoch 1110, val loss: 1.0909801721572876
Epoch 1120, training loss: 435.19970703125 = 1.0928319692611694 + 50.0 * 8.682137489318848
Epoch 1120, val loss: 1.090975046157837
Epoch 1130, training loss: 434.98193359375 = 1.0928256511688232 + 50.0 * 8.67778205871582
Epoch 1130, val loss: 1.090959906578064
Epoch 1140, training loss: 434.7665100097656 = 1.0928211212158203 + 50.0 * 8.673473358154297
Epoch 1140, val loss: 1.0909518003463745
Epoch 1150, training loss: 434.79779052734375 = 1.0928159952163696 + 50.0 * 8.674099922180176
Epoch 1150, val loss: 1.090943694114685
Epoch 1160, training loss: 434.9461975097656 = 1.092817783355713 + 50.0 * 8.677067756652832
Epoch 1160, val loss: 1.0909411907196045
Epoch 1170, training loss: 435.1705017089844 = 1.0928184986114502 + 50.0 * 8.681553840637207
Epoch 1170, val loss: 1.0909382104873657
Epoch 1180, training loss: 435.4744873046875 = 1.0928181409835815 + 50.0 * 8.687633514404297
Epoch 1180, val loss: 1.090934157371521
Epoch 1190, training loss: 435.4698486328125 = 1.0928161144256592 + 50.0 * 8.687541007995605
Epoch 1190, val loss: 1.0909274816513062
Epoch 1200, training loss: 435.5002746582031 = 1.0928151607513428 + 50.0 * 8.688149452209473
Epoch 1200, val loss: 1.090922474861145
Epoch 1210, training loss: 435.6742858886719 = 1.0928138494491577 + 50.0 * 8.691629409790039
Epoch 1210, val loss: 1.0909171104431152
Epoch 1220, training loss: 435.5693359375 = 1.0928089618682861 + 50.0 * 8.689530372619629
Epoch 1220, val loss: 1.0909096002578735
Epoch 1230, training loss: 435.4536437988281 = 1.0928055047988892 + 50.0 * 8.687216758728027
Epoch 1230, val loss: 1.0909030437469482
Epoch 1240, training loss: 435.5455322265625 = 1.092803955078125 + 50.0 * 8.689054489135742
Epoch 1240, val loss: 1.0908974409103394
Epoch 1250, training loss: 435.7771301269531 = 1.0928045511245728 + 50.0 * 8.693686485290527
Epoch 1250, val loss: 1.090893268585205
Epoch 1260, training loss: 436.0639343261719 = 1.0928065776824951 + 50.0 * 8.699422836303711
Epoch 1260, val loss: 1.0908907651901245
Epoch 1270, training loss: 436.1224670410156 = 1.0928044319152832 + 50.0 * 8.700592994689941
Epoch 1270, val loss: 1.090885043144226
Epoch 1280, training loss: 436.1697692871094 = 1.0928035974502563 + 50.0 * 8.701539039611816
Epoch 1280, val loss: 1.0908817052841187
Epoch 1290, training loss: 436.2247009277344 = 1.092801809310913 + 50.0 * 8.702637672424316
Epoch 1290, val loss: 1.0908761024475098
Epoch 1300, training loss: 436.23809814453125 = 1.0928012132644653 + 50.0 * 8.702905654907227
Epoch 1300, val loss: 1.09087073802948
Epoch 1310, training loss: 436.37127685546875 = 1.0927999019622803 + 50.0 * 8.70556926727295
Epoch 1310, val loss: 1.0908682346343994
Epoch 1320, training loss: 436.4669189453125 = 1.0927996635437012 + 50.0 * 8.70748233795166
Epoch 1320, val loss: 1.0908640623092651
Epoch 1330, training loss: 436.0661315917969 = 1.0927934646606445 + 50.0 * 8.699466705322266
Epoch 1330, val loss: 1.0908560752868652
Epoch 1340, training loss: 436.1287536621094 = 1.0927926301956177 + 50.0 * 8.700718879699707
Epoch 1340, val loss: 1.090848684310913
Epoch 1350, training loss: 436.0618896484375 = 1.092788815498352 + 50.0 * 8.699381828308105
Epoch 1350, val loss: 1.0908441543579102
Epoch 1360, training loss: 436.09869384765625 = 1.0927902460098267 + 50.0 * 8.700118064880371
Epoch 1360, val loss: 1.0908443927764893
Epoch 1370, training loss: 436.3295593261719 = 1.0927907228469849 + 50.0 * 8.704734802246094
Epoch 1370, val loss: 1.0908424854278564
Epoch 1380, training loss: 436.4205627441406 = 1.0927890539169312 + 50.0 * 8.706555366516113
Epoch 1380, val loss: 1.0908381938934326
Epoch 1390, training loss: 436.7027587890625 = 1.0927915573120117 + 50.0 * 8.712199211120605
Epoch 1390, val loss: 1.090837001800537
Epoch 1400, training loss: 436.95733642578125 = 1.0927934646606445 + 50.0 * 8.717290878295898
Epoch 1400, val loss: 1.090835690498352
Epoch 1410, training loss: 436.8868408203125 = 1.0927878618240356 + 50.0 * 8.71588134765625
Epoch 1410, val loss: 1.0908246040344238
Epoch 1420, training loss: 436.75 = 1.0927876234054565 + 50.0 * 8.713144302368164
Epoch 1420, val loss: 1.090824842453003
Epoch 1430, training loss: 436.9486999511719 = 1.0927871465682983 + 50.0 * 8.717118263244629
Epoch 1430, val loss: 1.0908229351043701
Epoch 1440, training loss: 437.1177673339844 = 1.0927897691726685 + 50.0 * 8.720499038696289
Epoch 1440, val loss: 1.0908231735229492
Epoch 1450, training loss: 437.24365234375 = 1.092790126800537 + 50.0 * 8.723016738891602
Epoch 1450, val loss: 1.0908211469650269
Epoch 1460, training loss: 437.2777404785156 = 1.0927894115447998 + 50.0 * 8.723698616027832
Epoch 1460, val loss: 1.090819001197815
Epoch 1470, training loss: 437.34661865234375 = 1.092788577079773 + 50.0 * 8.725076675415039
Epoch 1470, val loss: 1.0908156633377075
Epoch 1480, training loss: 437.55950927734375 = 1.0927894115447998 + 50.0 * 8.729333877563477
Epoch 1480, val loss: 1.0908141136169434
Epoch 1490, training loss: 437.7603454589844 = 1.0927895307540894 + 50.0 * 8.73335075378418
Epoch 1490, val loss: 1.0908125638961792
Epoch 1500, training loss: 437.235107421875 = 1.0927811861038208 + 50.0 * 8.722846984863281
Epoch 1500, val loss: 1.0908037424087524
Epoch 1510, training loss: 437.2477111816406 = 1.0927801132202148 + 50.0 * 8.723098754882812
Epoch 1510, val loss: 1.0908020734786987
Epoch 1520, training loss: 437.5114440917969 = 1.0927826166152954 + 50.0 * 8.728373527526855
Epoch 1520, val loss: 1.09080171585083
Epoch 1530, training loss: 437.74639892578125 = 1.0927836894989014 + 50.0 * 8.733072280883789
Epoch 1530, val loss: 1.0908012390136719
Epoch 1540, training loss: 437.9246826171875 = 1.0927858352661133 + 50.0 * 8.736638069152832
Epoch 1540, val loss: 1.0908007621765137
Epoch 1550, training loss: 437.9755554199219 = 1.0927848815917969 + 50.0 * 8.737655639648438
Epoch 1550, val loss: 1.0907975435256958
Epoch 1560, training loss: 437.8086853027344 = 1.0924887657165527 + 50.0 * 8.734323501586914
Epoch 1560, val loss: 1.0904353857040405
Epoch 1570, training loss: 437.4396667480469 = 1.0913262367248535 + 50.0 * 8.726966857910156
Epoch 1570, val loss: 1.0893566608428955
Epoch 1580, training loss: 437.8592529296875 = 1.0901951789855957 + 50.0 * 8.735381126403809
Epoch 1580, val loss: 1.0883368253707886
Epoch 1590, training loss: 438.06097412109375 = 1.0892068147659302 + 50.0 * 8.739435195922852
Epoch 1590, val loss: 1.0874487161636353
Epoch 1600, training loss: 438.2325744628906 = 1.088343858718872 + 50.0 * 8.742884635925293
Epoch 1600, val loss: 1.086670994758606
Epoch 1610, training loss: 438.38836669921875 = 1.087583303451538 + 50.0 * 8.746015548706055
Epoch 1610, val loss: 1.0859829187393188
Epoch 1620, training loss: 438.547607421875 = 1.086900234222412 + 50.0 * 8.749214172363281
Epoch 1620, val loss: 1.0853629112243652
Epoch 1630, training loss: 438.6791687011719 = 1.0862772464752197 + 50.0 * 8.75185775756836
Epoch 1630, val loss: 1.0847947597503662
Epoch 1640, training loss: 438.7253112792969 = 1.0857001543045044 + 50.0 * 8.752792358398438
Epoch 1640, val loss: 1.084268569946289
Epoch 1650, training loss: 438.8141174316406 = 1.0851647853851318 + 50.0 * 8.754578590393066
Epoch 1650, val loss: 1.0837773084640503
Epoch 1660, training loss: 438.8946838378906 = 1.0846614837646484 + 50.0 * 8.756200790405273
Epoch 1660, val loss: 1.0833146572113037
Epoch 1670, training loss: 438.5973815917969 = 1.0841801166534424 + 50.0 * 8.750264167785645
Epoch 1670, val loss: 1.0828742980957031
Epoch 1680, training loss: 438.47808837890625 = 1.0837219953536987 + 50.0 * 8.74788761138916
Epoch 1680, val loss: 1.0824458599090576
Epoch 1690, training loss: 438.6698303222656 = 1.0832953453063965 + 50.0 * 8.751730918884277
Epoch 1690, val loss: 1.082051157951355
Epoch 1700, training loss: 438.6867980957031 = 1.0828857421875 + 50.0 * 8.75207805633545
Epoch 1700, val loss: 1.0816720724105835
Epoch 1710, training loss: 438.9375305175781 = 1.082493543624878 + 50.0 * 8.757101058959961
Epoch 1710, val loss: 1.0813078880310059
Epoch 1720, training loss: 439.28265380859375 = 1.0821175575256348 + 50.0 * 8.764010429382324
Epoch 1720, val loss: 1.0809565782546997
Epoch 1730, training loss: 439.2662048339844 = 1.0817503929138184 + 50.0 * 8.763689041137695
Epoch 1730, val loss: 1.0806125402450562
Epoch 1740, training loss: 439.3605041503906 = 1.081396222114563 + 50.0 * 8.765582084655762
Epoch 1740, val loss: 1.080281138420105
Epoch 1750, training loss: 439.4986267089844 = 1.0810542106628418 + 50.0 * 8.768351554870605
Epoch 1750, val loss: 1.079960584640503
Epoch 1760, training loss: 439.5896911621094 = 1.0807206630706787 + 50.0 * 8.770179748535156
Epoch 1760, val loss: 1.0796477794647217
Epoch 1770, training loss: 439.6758728027344 = 1.0803982019424438 + 50.0 * 8.771909713745117
Epoch 1770, val loss: 1.079343557357788
Epoch 1780, training loss: 439.9027404785156 = 1.0800846815109253 + 50.0 * 8.776453018188477
Epoch 1780, val loss: 1.0790494680404663
Epoch 1790, training loss: 439.9152526855469 = 1.0797795057296753 + 50.0 * 8.77670955657959
Epoch 1790, val loss: 1.0787596702575684
Epoch 1800, training loss: 439.90283203125 = 1.079479455947876 + 50.0 * 8.776467323303223
Epoch 1800, val loss: 1.0784752368927002
Epoch 1810, training loss: 439.73455810546875 = 1.07918381690979 + 50.0 * 8.773107528686523
Epoch 1810, val loss: 1.078197956085205
Epoch 1820, training loss: 439.8824157714844 = 1.078902006149292 + 50.0 * 8.776070594787598
Epoch 1820, val loss: 1.0779317617416382
Epoch 1830, training loss: 440.1122741699219 = 1.0786267518997192 + 50.0 * 8.780673027038574
Epoch 1830, val loss: 1.0776708126068115
Epoch 1840, training loss: 440.2174377441406 = 1.078355073928833 + 50.0 * 8.782781600952148
Epoch 1840, val loss: 1.0774109363555908
Epoch 1850, training loss: 440.2005310058594 = 1.0780879259109497 + 50.0 * 8.782448768615723
Epoch 1850, val loss: 1.0771594047546387
Epoch 1860, training loss: 440.25579833984375 = 1.077828049659729 + 50.0 * 8.783559799194336
Epoch 1860, val loss: 1.0769129991531372
Epoch 1870, training loss: 440.1793212890625 = 1.0775747299194336 + 50.0 * 8.782034873962402
Epoch 1870, val loss: 1.0766690969467163
Epoch 1880, training loss: 440.089111328125 = 1.0773099660873413 + 50.0 * 8.78023624420166
Epoch 1880, val loss: 1.0764224529266357
Epoch 1890, training loss: 436.585205078125 = 1.076911211013794 + 50.0 * 8.710165977478027
Epoch 1890, val loss: 1.076053261756897
Epoch 1900, training loss: 440.0993957519531 = 1.0768016576766968 + 50.0 * 8.780451774597168
Epoch 1900, val loss: 1.0758943557739258
Epoch 1910, training loss: 436.4747009277344 = 1.0764355659484863 + 50.0 * 8.707964897155762
Epoch 1910, val loss: 1.075609564781189
Epoch 1920, training loss: 437.26129150390625 = 1.0762450695037842 + 50.0 * 8.723701477050781
Epoch 1920, val loss: 1.0754071474075317
Epoch 1930, training loss: 437.1220397949219 = 1.0760470628738403 + 50.0 * 8.720919609069824
Epoch 1930, val loss: 1.075221061706543
Epoch 1940, training loss: 436.8321838378906 = 1.0758230686187744 + 50.0 * 8.715126991271973
Epoch 1940, val loss: 1.07500159740448
Epoch 1950, training loss: 437.5653991699219 = 1.0756282806396484 + 50.0 * 8.729795455932617
Epoch 1950, val loss: 1.0748108625411987
Epoch 1960, training loss: 438.093994140625 = 1.075424075126648 + 50.0 * 8.740371704101562
Epoch 1960, val loss: 1.0746201276779175
Epoch 1970, training loss: 438.3136291503906 = 1.0752151012420654 + 50.0 * 8.744768142700195
Epoch 1970, val loss: 1.0744189023971558
Epoch 1980, training loss: 438.7830505371094 = 1.0750131607055664 + 50.0 * 8.75416088104248
Epoch 1980, val loss: 1.074224829673767
Epoch 1990, training loss: 438.951171875 = 1.0748108625411987 + 50.0 * 8.757527351379395
Epoch 1990, val loss: 1.0740313529968262
Epoch 2000, training loss: 438.93414306640625 = 1.0746058225631714 + 50.0 * 8.757190704345703
Epoch 2000, val loss: 1.0738353729248047
Epoch 2010, training loss: 439.3285217285156 = 1.0744109153747559 + 50.0 * 8.765082359313965
Epoch 2010, val loss: 1.07364821434021
Epoch 2020, training loss: 439.501708984375 = 1.0742158889770508 + 50.0 * 8.768549919128418
Epoch 2020, val loss: 1.0734615325927734
Epoch 2030, training loss: 439.6267395019531 = 1.0740211009979248 + 50.0 * 8.7710542678833
Epoch 2030, val loss: 1.0732746124267578
Epoch 2040, training loss: 439.56884765625 = 1.0738277435302734 + 50.0 * 8.76990032196045
Epoch 2040, val loss: 1.0730900764465332
Epoch 2050, training loss: 439.7765197753906 = 1.0736408233642578 + 50.0 * 8.774057388305664
Epoch 2050, val loss: 1.0729106664657593
Epoch 2060, training loss: 440.065673828125 = 1.0734578371047974 + 50.0 * 8.779844284057617
Epoch 2060, val loss: 1.0727359056472778
Epoch 2070, training loss: 439.8995056152344 = 1.0732719898223877 + 50.0 * 8.776524543762207
Epoch 2070, val loss: 1.0725574493408203
Epoch 2080, training loss: 440.1689147949219 = 1.0730925798416138 + 50.0 * 8.781916618347168
Epoch 2080, val loss: 1.0723860263824463
Epoch 2090, training loss: 440.2328186035156 = 1.072914481163025 + 50.0 * 8.783198356628418
Epoch 2090, val loss: 1.0722148418426514
Epoch 2100, training loss: 440.2955017089844 = 1.0727380514144897 + 50.0 * 8.784455299377441
Epoch 2100, val loss: 1.0720458030700684
Epoch 2110, training loss: 440.27069091796875 = 1.0725632905960083 + 50.0 * 8.78396224975586
Epoch 2110, val loss: 1.0718780755996704
Epoch 2120, training loss: 440.44091796875 = 1.0723930597305298 + 50.0 * 8.787370681762695
Epoch 2120, val loss: 1.0717148780822754
Epoch 2130, training loss: 440.6326599121094 = 1.07222580909729 + 50.0 * 8.791208267211914
Epoch 2130, val loss: 1.0715546607971191
Epoch 2140, training loss: 440.487548828125 = 1.0720537900924683 + 50.0 * 8.788310050964355
Epoch 2140, val loss: 1.0713895559310913
Epoch 2150, training loss: 440.5634460449219 = 1.0718889236450195 + 50.0 * 8.789831161499023
Epoch 2150, val loss: 1.0712329149246216
Epoch 2160, training loss: 440.6737976074219 = 1.0717267990112305 + 50.0 * 8.792041778564453
Epoch 2160, val loss: 1.0710771083831787
Epoch 2170, training loss: 440.72314453125 = 1.0715657472610474 + 50.0 * 8.793031692504883
Epoch 2170, val loss: 1.0709210634231567
Epoch 2180, training loss: 440.681640625 = 1.0714048147201538 + 50.0 * 8.792204856872559
Epoch 2180, val loss: 1.0707679986953735
Epoch 2190, training loss: 440.25738525390625 = 1.0712344646453857 + 50.0 * 8.783722877502441
Epoch 2190, val loss: 1.0706058740615845
Epoch 2200, training loss: 440.3999328613281 = 1.0710805654525757 + 50.0 * 8.786577224731445
Epoch 2200, val loss: 1.070459008216858
Epoch 2210, training loss: 440.65838623046875 = 1.0709317922592163 + 50.0 * 8.791749000549316
Epoch 2210, val loss: 1.0703164339065552
Epoch 2220, training loss: 440.8889465332031 = 1.0707837343215942 + 50.0 * 8.796363830566406
Epoch 2220, val loss: 1.0701735019683838
Epoch 2230, training loss: 440.94854736328125 = 1.070633053779602 + 50.0 * 8.797557830810547
Epoch 2230, val loss: 1.0700308084487915
Epoch 2240, training loss: 441.05810546875 = 1.07048499584198 + 50.0 * 8.799752235412598
Epoch 2240, val loss: 1.0698890686035156
Epoch 2250, training loss: 441.1402587890625 = 1.0703387260437012 + 50.0 * 8.801398277282715
Epoch 2250, val loss: 1.0697495937347412
Epoch 2260, training loss: 440.78753662109375 = 1.070181965827942 + 50.0 * 8.794346809387207
Epoch 2260, val loss: 1.0695985555648804
Epoch 2270, training loss: 440.69439697265625 = 1.070034146308899 + 50.0 * 8.792487144470215
Epoch 2270, val loss: 1.0694574117660522
Epoch 2280, training loss: 440.80596923828125 = 1.069892406463623 + 50.0 * 8.794721603393555
Epoch 2280, val loss: 1.069324016571045
Epoch 2290, training loss: 441.0660095214844 = 1.069758415222168 + 50.0 * 8.799924850463867
Epoch 2290, val loss: 1.0691930055618286
Epoch 2300, training loss: 441.19482421875 = 1.0696206092834473 + 50.0 * 8.802504539489746
Epoch 2300, val loss: 1.069062352180481
Epoch 2310, training loss: 441.4146728515625 = 1.069486141204834 + 50.0 * 8.806903839111328
Epoch 2310, val loss: 1.0689337253570557
Epoch 2320, training loss: 441.669189453125 = 1.069352149963379 + 50.0 * 8.811996459960938
Epoch 2320, val loss: 1.0688060522079468
Epoch 2330, training loss: 441.6275634765625 = 1.0692172050476074 + 50.0 * 8.811166763305664
Epoch 2330, val loss: 1.0686769485473633
Epoch 2340, training loss: 441.3570251464844 = 1.0690747499465942 + 50.0 * 8.80575942993164
Epoch 2340, val loss: 1.0685399770736694
Epoch 2350, training loss: 441.51904296875 = 1.0689444541931152 + 50.0 * 8.809001922607422
Epoch 2350, val loss: 1.0684152841567993
Epoch 2360, training loss: 441.44647216796875 = 1.0688140392303467 + 50.0 * 8.8075532913208
Epoch 2360, val loss: 1.0682921409606934
Epoch 2370, training loss: 441.6345520019531 = 1.068686604499817 + 50.0 * 8.811317443847656
Epoch 2370, val loss: 1.0681689977645874
Epoch 2380, training loss: 441.5678405761719 = 1.0685536861419678 + 50.0 * 8.809986114501953
Epoch 2380, val loss: 1.0680451393127441
Epoch 2390, training loss: 441.2322692871094 = 1.0684150457382202 + 50.0 * 8.803277015686035
Epoch 2390, val loss: 1.0679140090942383
Epoch 2400, training loss: 441.0089416503906 = 1.068285346031189 + 50.0 * 8.798812866210938
Epoch 2400, val loss: 1.067791223526001
Epoch 2410, training loss: 441.4337158203125 = 1.0681686401367188 + 50.0 * 8.807311058044434
Epoch 2410, val loss: 1.0676767826080322
Epoch 2420, training loss: 441.832275390625 = 1.0680546760559082 + 50.0 * 8.815284729003906
Epoch 2420, val loss: 1.0675686597824097
Epoch 2430, training loss: 442.0186462402344 = 1.0679380893707275 + 50.0 * 8.819014549255371
Epoch 2430, val loss: 1.0674570798873901
Epoch 2440, training loss: 442.2773132324219 = 1.0678199529647827 + 50.0 * 8.824190139770508
Epoch 2440, val loss: 1.0673460960388184
Epoch 2450, training loss: 442.3744201660156 = 1.067703366279602 + 50.0 * 8.826133728027344
Epoch 2450, val loss: 1.067234992980957
Epoch 2460, training loss: 442.3565368652344 = 1.067583680152893 + 50.0 * 8.82577896118164
Epoch 2460, val loss: 1.067120909690857
Epoch 2470, training loss: 442.3937072753906 = 1.0674664974212646 + 50.0 * 8.82652473449707
Epoch 2470, val loss: 1.0670052766799927
Epoch 2480, training loss: 442.383056640625 = 1.067347526550293 + 50.0 * 8.826313972473145
Epoch 2480, val loss: 1.0668948888778687
Epoch 2490, training loss: 442.5643615722656 = 1.0672359466552734 + 50.0 * 8.82994270324707
Epoch 2490, val loss: 1.0667893886566162
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.39753623188405796
0.8632905890023909
=== training gcn model ===
Epoch 0, training loss: 504.4148254394531 = 1.1114811897277832 + 50.0 * 10.06606674194336
Epoch 0, val loss: 1.1114153861999512
Epoch 10, training loss: 480.76318359375 = 1.1108249425888062 + 50.0 * 9.593047142028809
Epoch 10, val loss: 1.1107932329177856
Epoch 20, training loss: 471.16680908203125 = 1.110511302947998 + 50.0 * 9.40112590789795
Epoch 20, val loss: 1.1104861497879028
Epoch 30, training loss: 464.8200378417969 = 1.1102135181427002 + 50.0 * 9.27419662475586
Epoch 30, val loss: 1.1101864576339722
Epoch 40, training loss: 459.83453369140625 = 1.1098897457122803 + 50.0 * 9.174492835998535
Epoch 40, val loss: 1.1098605394363403
Epoch 50, training loss: 455.7315673828125 = 1.1095885038375854 + 50.0 * 9.092439651489258
Epoch 50, val loss: 1.1095589399337769
Epoch 60, training loss: 452.2277526855469 = 1.109289526939392 + 50.0 * 9.022369384765625
Epoch 60, val loss: 1.1092582941055298
Epoch 70, training loss: 449.277099609375 = 1.1089948415756226 + 50.0 * 8.963361740112305
Epoch 70, val loss: 1.1089636087417603
Epoch 80, training loss: 446.7745056152344 = 1.1086978912353516 + 50.0 * 8.913315773010254
Epoch 80, val loss: 1.1086645126342773
Epoch 90, training loss: 444.6483154296875 = 1.108400583267212 + 50.0 * 8.870798110961914
Epoch 90, val loss: 1.1083698272705078
Epoch 100, training loss: 442.78399658203125 = 1.108107566833496 + 50.0 * 8.833518028259277
Epoch 100, val loss: 1.1080793142318726
Epoch 110, training loss: 441.230712890625 = 1.107817530632019 + 50.0 * 8.802457809448242
Epoch 110, val loss: 1.1077907085418701
Epoch 120, training loss: 439.8927307128906 = 1.1075279712677002 + 50.0 * 8.775704383850098
Epoch 120, val loss: 1.107502818107605
Epoch 130, training loss: 438.8143310546875 = 1.107243299484253 + 50.0 * 8.754141807556152
Epoch 130, val loss: 1.1072174310684204
Epoch 140, training loss: 437.707275390625 = 1.1069539785385132 + 50.0 * 8.732006072998047
Epoch 140, val loss: 1.1069297790527344
Epoch 150, training loss: 436.786376953125 = 1.1066685914993286 + 50.0 * 8.713594436645508
Epoch 150, val loss: 1.1066452264785767
Epoch 160, training loss: 436.07598876953125 = 1.10638427734375 + 50.0 * 8.699392318725586
Epoch 160, val loss: 1.1063618659973145
Epoch 170, training loss: 435.4523620605469 = 1.1060832738876343 + 50.0 * 8.686925888061523
Epoch 170, val loss: 1.1060576438903809
Epoch 180, training loss: 434.7437744140625 = 1.1054446697235107 + 50.0 * 8.67276668548584
Epoch 180, val loss: 1.1054165363311768
Epoch 190, training loss: 434.2477111816406 = 1.1046777963638306 + 50.0 * 8.662860870361328
Epoch 190, val loss: 1.1047062873840332
Epoch 200, training loss: 433.8369140625 = 1.1039565801620483 + 50.0 * 8.654659271240234
Epoch 200, val loss: 1.1040406227111816
Epoch 210, training loss: 433.3987731933594 = 1.1032909154891968 + 50.0 * 8.645909309387207
Epoch 210, val loss: 1.1034249067306519
Epoch 220, training loss: 432.99835205078125 = 1.102670431137085 + 50.0 * 8.637913703918457
Epoch 220, val loss: 1.1028501987457275
Epoch 230, training loss: 432.657958984375 = 1.102084755897522 + 50.0 * 8.631117820739746
Epoch 230, val loss: 1.102306842803955
Epoch 240, training loss: 432.31610107421875 = 1.1015269756317139 + 50.0 * 8.62429141998291
Epoch 240, val loss: 1.1017861366271973
Epoch 250, training loss: 432.0412292480469 = 1.1009914875030518 + 50.0 * 8.618804931640625
Epoch 250, val loss: 1.1012864112854004
Epoch 260, training loss: 431.9200134277344 = 1.100478172302246 + 50.0 * 8.6163911819458
Epoch 260, val loss: 1.10080885887146
Epoch 270, training loss: 431.6730041503906 = 1.0999815464019775 + 50.0 * 8.61146068572998
Epoch 270, val loss: 1.1003443002700806
Epoch 280, training loss: 431.5540771484375 = 1.09949791431427 + 50.0 * 8.609091758728027
Epoch 280, val loss: 1.0998916625976562
Epoch 290, training loss: 431.29388427734375 = 1.0990288257598877 + 50.0 * 8.603897094726562
Epoch 290, val loss: 1.0994513034820557
Epoch 300, training loss: 431.07208251953125 = 1.0985703468322754 + 50.0 * 8.599470138549805
Epoch 300, val loss: 1.0990221500396729
Epoch 310, training loss: 430.87188720703125 = 1.0981203317642212 + 50.0 * 8.595475196838379
Epoch 310, val loss: 1.098602056503296
Epoch 320, training loss: 430.6588134765625 = 1.0976814031600952 + 50.0 * 8.591222763061523
Epoch 320, val loss: 1.098189353942871
Epoch 330, training loss: 430.5155944824219 = 1.0972498655319214 + 50.0 * 8.588366508483887
Epoch 330, val loss: 1.0977847576141357
Epoch 340, training loss: 430.5301208496094 = 1.0968348979949951 + 50.0 * 8.588665962219238
Epoch 340, val loss: 1.0973913669586182
Epoch 350, training loss: 430.34857177734375 = 1.0964213609695435 + 50.0 * 8.585042953491211
Epoch 350, val loss: 1.097007393836975
Epoch 360, training loss: 430.3657531738281 = 1.096015214920044 + 50.0 * 8.585394859313965
Epoch 360, val loss: 1.0966240167617798
Epoch 370, training loss: 430.18896484375 = 1.0956164598464966 + 50.0 * 8.581867218017578
Epoch 370, val loss: 1.0962483882904053
Epoch 380, training loss: 430.0472412109375 = 1.095224142074585 + 50.0 * 8.57904052734375
Epoch 380, val loss: 1.095878005027771
Epoch 390, training loss: 430.0251159667969 = 1.0947917699813843 + 50.0 * 8.578606605529785
Epoch 390, val loss: 1.0954738855361938
Epoch 400, training loss: 430.86480712890625 = 1.0944905281066895 + 50.0 * 8.595406532287598
Epoch 400, val loss: 1.0951908826828003
Epoch 410, training loss: 430.1417541503906 = 1.0940830707550049 + 50.0 * 8.580953598022461
Epoch 410, val loss: 1.0948039293289185
Epoch 420, training loss: 430.03387451171875 = 1.0937200784683228 + 50.0 * 8.578803062438965
Epoch 420, val loss: 1.0944652557373047
Epoch 430, training loss: 429.9076843261719 = 1.0933548212051392 + 50.0 * 8.576286315917969
Epoch 430, val loss: 1.0941212177276611
Epoch 440, training loss: 430.04986572265625 = 1.0930018424987793 + 50.0 * 8.579137802124023
Epoch 440, val loss: 1.0937891006469727
Epoch 450, training loss: 430.0835876464844 = 1.092655062675476 + 50.0 * 8.579818725585938
Epoch 450, val loss: 1.0934641361236572
Epoch 460, training loss: 430.182861328125 = 1.0923277139663696 + 50.0 * 8.58181095123291
Epoch 460, val loss: 1.0931549072265625
Epoch 470, training loss: 430.16864013671875 = 1.092076301574707 + 50.0 * 8.581531524658203
Epoch 470, val loss: 1.0929303169250488
Epoch 480, training loss: 430.27166748046875 = 1.0919547080993652 + 50.0 * 8.58359432220459
Epoch 480, val loss: 1.0928313732147217
Epoch 490, training loss: 430.20355224609375 = 1.091825246810913 + 50.0 * 8.582234382629395
Epoch 490, val loss: 1.092719316482544
Epoch 500, training loss: 430.23565673828125 = 1.091700553894043 + 50.0 * 8.582879066467285
Epoch 500, val loss: 1.0926177501678467
Epoch 510, training loss: 430.1939697265625 = 1.0915824174880981 + 50.0 * 8.582047462463379
Epoch 510, val loss: 1.0925164222717285
Epoch 520, training loss: 430.2135314941406 = 1.091463565826416 + 50.0 * 8.582441329956055
Epoch 520, val loss: 1.0924183130264282
Epoch 530, training loss: 430.3083801269531 = 1.0913512706756592 + 50.0 * 8.584341049194336
Epoch 530, val loss: 1.092323899269104
Epoch 540, training loss: 430.4320373535156 = 1.0912365913391113 + 50.0 * 8.58681583404541
Epoch 540, val loss: 1.0922279357910156
Epoch 550, training loss: 430.32867431640625 = 1.091121792793274 + 50.0 * 8.58475112915039
Epoch 550, val loss: 1.0921298265457153
Epoch 560, training loss: 430.43817138671875 = 1.0910145044326782 + 50.0 * 8.586943626403809
Epoch 560, val loss: 1.0920411348342896
Epoch 570, training loss: 430.7521057128906 = 1.0909074544906616 + 50.0 * 8.593223571777344
Epoch 570, val loss: 1.0919533967971802
Epoch 580, training loss: 430.55584716796875 = 1.0907973051071167 + 50.0 * 8.589301109313965
Epoch 580, val loss: 1.0918614864349365
Epoch 590, training loss: 430.4656677246094 = 1.0906919240951538 + 50.0 * 8.587499618530273
Epoch 590, val loss: 1.091774821281433
Epoch 600, training loss: 430.5377502441406 = 1.0905911922454834 + 50.0 * 8.588943481445312
Epoch 600, val loss: 1.0916911363601685
Epoch 610, training loss: 430.7635498046875 = 1.0904914140701294 + 50.0 * 8.593461036682129
Epoch 610, val loss: 1.0916091203689575
Epoch 620, training loss: 430.8227233886719 = 1.0903924703598022 + 50.0 * 8.594646453857422
Epoch 620, val loss: 1.0915266275405884
Epoch 630, training loss: 430.7682189941406 = 1.090290904045105 + 50.0 * 8.593558311462402
Epoch 630, val loss: 1.091444492340088
Epoch 640, training loss: 430.91943359375 = 1.0901954174041748 + 50.0 * 8.59658432006836
Epoch 640, val loss: 1.0913622379302979
Epoch 650, training loss: 430.8580322265625 = 1.090089201927185 + 50.0 * 8.595358848571777
Epoch 650, val loss: 1.0912789106369019
Epoch 660, training loss: 430.8638916015625 = 1.0899972915649414 + 50.0 * 8.595478057861328
Epoch 660, val loss: 1.091201901435852
Epoch 670, training loss: 430.83642578125 = 1.0899049043655396 + 50.0 * 8.594930648803711
Epoch 670, val loss: 1.0911253690719604
Epoch 680, training loss: 431.07763671875 = 1.089816689491272 + 50.0 * 8.599756240844727
Epoch 680, val loss: 1.0910553932189941
Epoch 690, training loss: 431.0604553222656 = 1.0897284746170044 + 50.0 * 8.599414825439453
Epoch 690, val loss: 1.0909825563430786
Epoch 700, training loss: 431.2254638671875 = 1.0896390676498413 + 50.0 * 8.602716445922852
Epoch 700, val loss: 1.0909096002578735
Epoch 710, training loss: 431.33074951171875 = 1.0895508527755737 + 50.0 * 8.60482406616211
Epoch 710, val loss: 1.09083890914917
Epoch 720, training loss: 431.5707092285156 = 1.0894644260406494 + 50.0 * 8.609624862670898
Epoch 720, val loss: 1.0907691717147827
Epoch 730, training loss: 431.4325256347656 = 1.0893765687942505 + 50.0 * 8.606863021850586
Epoch 730, val loss: 1.0906987190246582
Epoch 740, training loss: 431.4870910644531 = 1.0892925262451172 + 50.0 * 8.607955932617188
Epoch 740, val loss: 1.0906280279159546
Epoch 750, training loss: 431.6457824707031 = 1.0892122983932495 + 50.0 * 8.61113166809082
Epoch 750, val loss: 1.090563178062439
Epoch 760, training loss: 431.8756103515625 = 1.0891313552856445 + 50.0 * 8.615729331970215
Epoch 760, val loss: 1.0904990434646606
Epoch 770, training loss: 432.0579528808594 = 1.0890512466430664 + 50.0 * 8.619378089904785
Epoch 770, val loss: 1.0904334783554077
Epoch 780, training loss: 432.1849060058594 = 1.0889713764190674 + 50.0 * 8.621918678283691
Epoch 780, val loss: 1.0903700590133667
Epoch 790, training loss: 432.2396240234375 = 1.088891625404358 + 50.0 * 8.623014450073242
Epoch 790, val loss: 1.0903065204620361
Epoch 800, training loss: 432.1853942871094 = 1.0888112783432007 + 50.0 * 8.621932029724121
Epoch 800, val loss: 1.0902425050735474
Epoch 810, training loss: 432.49505615234375 = 1.0887377262115479 + 50.0 * 8.62812614440918
Epoch 810, val loss: 1.0901836156845093
Epoch 820, training loss: 432.55755615234375 = 1.0886627435684204 + 50.0 * 8.629378318786621
Epoch 820, val loss: 1.0901241302490234
Epoch 830, training loss: 432.6685485839844 = 1.088588833808899 + 50.0 * 8.631599426269531
Epoch 830, val loss: 1.090064287185669
Epoch 840, training loss: 432.6913146972656 = 1.0885131359100342 + 50.0 * 8.63205623626709
Epoch 840, val loss: 1.0900039672851562
Epoch 850, training loss: 432.90087890625 = 1.088441014289856 + 50.0 * 8.636248588562012
Epoch 850, val loss: 1.0899474620819092
Epoch 860, training loss: 432.8974609375 = 1.0883712768554688 + 50.0 * 8.636181831359863
Epoch 860, val loss: 1.0898919105529785
Epoch 870, training loss: 433.0549621582031 = 1.0883009433746338 + 50.0 * 8.639333724975586
Epoch 870, val loss: 1.0898370742797852
Epoch 880, training loss: 433.1756896972656 = 1.0882326364517212 + 50.0 * 8.641749382019043
Epoch 880, val loss: 1.089782476425171
Epoch 890, training loss: 433.2330017089844 = 1.0881627798080444 + 50.0 * 8.64289665222168
Epoch 890, val loss: 1.0897274017333984
Epoch 900, training loss: 433.3385009765625 = 1.088095784187317 + 50.0 * 8.645008087158203
Epoch 900, val loss: 1.0896755456924438
Epoch 910, training loss: 433.42193603515625 = 1.0880297422409058 + 50.0 * 8.64667797088623
Epoch 910, val loss: 1.0896227359771729
Epoch 920, training loss: 433.32147216796875 = 1.0879594087600708 + 50.0 * 8.644670486450195
Epoch 920, val loss: 1.0895681381225586
Epoch 930, training loss: 433.3167419433594 = 1.0878939628601074 + 50.0 * 8.644577026367188
Epoch 930, val loss: 1.0895169973373413
Epoch 940, training loss: 433.4945983886719 = 1.0878322124481201 + 50.0 * 8.6481351852417
Epoch 940, val loss: 1.089469075202942
Epoch 950, training loss: 433.5540466308594 = 1.0877695083618164 + 50.0 * 8.649325370788574
Epoch 950, val loss: 1.0894219875335693
Epoch 960, training loss: 433.7747497558594 = 1.0877087116241455 + 50.0 * 8.653740882873535
Epoch 960, val loss: 1.0893751382827759
Epoch 970, training loss: 433.8022155761719 = 1.0876487493515015 + 50.0 * 8.654291152954102
Epoch 970, val loss: 1.089328408241272
Epoch 980, training loss: 433.9269714355469 = 1.087589979171753 + 50.0 * 8.656787872314453
Epoch 980, val loss: 1.0892834663391113
Epoch 990, training loss: 434.51153564453125 = 1.087529182434082 + 50.0 * 8.668479919433594
Epoch 990, val loss: 1.0892353057861328
Epoch 1000, training loss: 433.8194580078125 = 1.0874676704406738 + 50.0 * 8.654640197753906
Epoch 1000, val loss: 1.0891894102096558
Epoch 1010, training loss: 433.73712158203125 = 1.0874099731445312 + 50.0 * 8.652994155883789
Epoch 1010, val loss: 1.089146375656128
Epoch 1020, training loss: 434.0664978027344 = 1.0873562097549438 + 50.0 * 8.65958309173584
Epoch 1020, val loss: 1.0891053676605225
Epoch 1030, training loss: 434.1153869628906 = 1.0873013734817505 + 50.0 * 8.660561561584473
Epoch 1030, val loss: 1.0890640020370483
Epoch 1040, training loss: 434.2022399902344 = 1.0872461795806885 + 50.0 * 8.662300109863281
Epoch 1040, val loss: 1.0890227556228638
Epoch 1050, training loss: 434.3060607910156 = 1.087192416191101 + 50.0 * 8.664377212524414
Epoch 1050, val loss: 1.0889830589294434
Epoch 1060, training loss: 434.3692626953125 = 1.0871385335922241 + 50.0 * 8.665642738342285
Epoch 1060, val loss: 1.0889406204223633
Epoch 1070, training loss: 434.395263671875 = 1.0870848894119263 + 50.0 * 8.666163444519043
Epoch 1070, val loss: 1.0889021158218384
Epoch 1080, training loss: 434.5013427734375 = 1.087032675743103 + 50.0 * 8.668286323547363
Epoch 1080, val loss: 1.0888622999191284
Epoch 1090, training loss: 434.6270446777344 = 1.086982011795044 + 50.0 * 8.670801162719727
Epoch 1090, val loss: 1.0888255834579468
Epoch 1100, training loss: 434.73309326171875 = 1.0869332551956177 + 50.0 * 8.67292308807373
Epoch 1100, val loss: 1.088789939880371
Epoch 1110, training loss: 434.7450866699219 = 1.0868810415267944 + 50.0 * 8.673164367675781
Epoch 1110, val loss: 1.088748812675476
Epoch 1120, training loss: 434.6612243652344 = 1.08683180809021 + 50.0 * 8.671487808227539
Epoch 1120, val loss: 1.0887144804000854
Epoch 1130, training loss: 434.8827819824219 = 1.0867862701416016 + 50.0 * 8.675919532775879
Epoch 1130, val loss: 1.0886814594268799
Epoch 1140, training loss: 434.9596252441406 = 1.086738109588623 + 50.0 * 8.677457809448242
Epoch 1140, val loss: 1.0886435508728027
Epoch 1150, training loss: 434.8930358886719 = 1.0866870880126953 + 50.0 * 8.676126480102539
Epoch 1150, val loss: 1.0886074304580688
Epoch 1160, training loss: 434.57440185546875 = 1.0866327285766602 + 50.0 * 8.669754981994629
Epoch 1160, val loss: 1.088564157485962
Epoch 1170, training loss: 434.8216247558594 = 1.0865920782089233 + 50.0 * 8.674700736999512
Epoch 1170, val loss: 1.0885334014892578
Epoch 1180, training loss: 435.029541015625 = 1.086548089981079 + 50.0 * 8.67885971069336
Epoch 1180, val loss: 1.0885056257247925
Epoch 1190, training loss: 434.4811706542969 = 1.0864977836608887 + 50.0 * 8.667893409729004
Epoch 1190, val loss: 1.0884698629379272
Epoch 1200, training loss: 434.7310791015625 = 1.086456298828125 + 50.0 * 8.672892570495605
Epoch 1200, val loss: 1.0884402990341187
Epoch 1210, training loss: 434.83074951171875 = 1.0864167213439941 + 50.0 * 8.674886703491211
Epoch 1210, val loss: 1.0884140729904175
Epoch 1220, training loss: 435.1278991699219 = 1.086378812789917 + 50.0 * 8.680830955505371
Epoch 1220, val loss: 1.0883868932724
Epoch 1230, training loss: 435.3085632324219 = 1.0863405466079712 + 50.0 * 8.684444427490234
Epoch 1230, val loss: 1.0883606672286987
Epoch 1240, training loss: 435.3741455078125 = 1.0862995386123657 + 50.0 * 8.68575668334961
Epoch 1240, val loss: 1.0883320569992065
Epoch 1250, training loss: 435.55096435546875 = 1.0862611532211304 + 50.0 * 8.68929386138916
Epoch 1250, val loss: 1.0883055925369263
Epoch 1260, training loss: 435.5078430175781 = 1.086217999458313 + 50.0 * 8.688432693481445
Epoch 1260, val loss: 1.0882744789123535
Epoch 1270, training loss: 435.447265625 = 1.0861756801605225 + 50.0 * 8.68722152709961
Epoch 1270, val loss: 1.088245153427124
Epoch 1280, training loss: 435.61956787109375 = 1.0861393213272095 + 50.0 * 8.690668106079102
Epoch 1280, val loss: 1.0882209539413452
Epoch 1290, training loss: 435.94073486328125 = 1.0861068964004517 + 50.0 * 8.697092056274414
Epoch 1290, val loss: 1.088199257850647
Epoch 1300, training loss: 436.04486083984375 = 1.0860692262649536 + 50.0 * 8.699175834655762
Epoch 1300, val loss: 1.0881725549697876
Epoch 1310, training loss: 436.5875244140625 = 1.0859266519546509 + 50.0 * 8.710031509399414
Epoch 1310, val loss: 1.088025450706482
Epoch 1320, training loss: 433.9755554199219 = 1.0858596563339233 + 50.0 * 8.657793998718262
Epoch 1320, val loss: 1.0880112648010254
Epoch 1330, training loss: 434.85223388671875 = 1.0858943462371826 + 50.0 * 8.675326347351074
Epoch 1330, val loss: 1.0880284309387207
Epoch 1340, training loss: 434.99420166015625 = 1.0858758687973022 + 50.0 * 8.678166389465332
Epoch 1340, val loss: 1.0880308151245117
Epoch 1350, training loss: 435.10150146484375 = 1.0858535766601562 + 50.0 * 8.680313110351562
Epoch 1350, val loss: 1.0880178213119507
Epoch 1360, training loss: 435.7471008300781 = 1.0858347415924072 + 50.0 * 8.693224906921387
Epoch 1360, val loss: 1.0880087614059448
Epoch 1370, training loss: 436.1179504394531 = 1.0858100652694702 + 50.0 * 8.700642585754395
Epoch 1370, val loss: 1.0879950523376465
Epoch 1380, training loss: 436.250244140625 = 1.0857820510864258 + 50.0 * 8.703289031982422
Epoch 1380, val loss: 1.087977409362793
Epoch 1390, training loss: 436.4430847167969 = 1.085753321647644 + 50.0 * 8.707146644592285
Epoch 1390, val loss: 1.0879604816436768
Epoch 1400, training loss: 436.2833557128906 = 1.0857130289077759 + 50.0 * 8.70395278930664
Epoch 1400, val loss: 1.087929368019104
Epoch 1410, training loss: 435.9517822265625 = 1.0856841802597046 + 50.0 * 8.697321891784668
Epoch 1410, val loss: 1.0879102945327759
Epoch 1420, training loss: 436.4595642089844 = 1.0856473445892334 + 50.0 * 8.707478523254395
Epoch 1420, val loss: 1.0878809690475464
Epoch 1430, training loss: 435.9617614746094 = 1.0856144428253174 + 50.0 * 8.69752311706543
Epoch 1430, val loss: 1.0878676176071167
Epoch 1440, training loss: 436.3563232421875 = 1.085586428642273 + 50.0 * 8.705414772033691
Epoch 1440, val loss: 1.087848424911499
Epoch 1450, training loss: 436.053955078125 = 1.0855591297149658 + 50.0 * 8.69936752319336
Epoch 1450, val loss: 1.087834358215332
Epoch 1460, training loss: 436.4084777832031 = 1.085538387298584 + 50.0 * 8.706459045410156
Epoch 1460, val loss: 1.0878230333328247
Epoch 1470, training loss: 436.8401794433594 = 1.0855164527893066 + 50.0 * 8.715093612670898
Epoch 1470, val loss: 1.0878108739852905
Epoch 1480, training loss: 437.1762390136719 = 1.0854943990707397 + 50.0 * 8.72181510925293
Epoch 1480, val loss: 1.0877983570098877
Epoch 1490, training loss: 437.4385070800781 = 1.0854707956314087 + 50.0 * 8.727060317993164
Epoch 1490, val loss: 1.0877851247787476
Epoch 1500, training loss: 437.6507873535156 = 1.0854462385177612 + 50.0 * 8.731307029724121
Epoch 1500, val loss: 1.0877699851989746
Epoch 1510, training loss: 437.57318115234375 = 1.0854198932647705 + 50.0 * 8.729755401611328
Epoch 1510, val loss: 1.0877546072006226
Epoch 1520, training loss: 437.9230651855469 = 1.0853968858718872 + 50.0 * 8.736753463745117
Epoch 1520, val loss: 1.0877410173416138
Epoch 1530, training loss: 437.9248962402344 = 1.0853708982467651 + 50.0 * 8.736790657043457
Epoch 1530, val loss: 1.0877251625061035
Epoch 1540, training loss: 438.0218200683594 = 1.0853466987609863 + 50.0 * 8.738729476928711
Epoch 1540, val loss: 1.087710976600647
Epoch 1550, training loss: 438.205322265625 = 1.0853230953216553 + 50.0 * 8.742400169372559
Epoch 1550, val loss: 1.0876972675323486
Epoch 1560, training loss: 438.1854248046875 = 1.0852980613708496 + 50.0 * 8.742002487182617
Epoch 1560, val loss: 1.0876818895339966
Epoch 1570, training loss: 438.38970947265625 = 1.0852761268615723 + 50.0 * 8.746088981628418
Epoch 1570, val loss: 1.087669849395752
Epoch 1580, training loss: 438.40472412109375 = 1.0852528810501099 + 50.0 * 8.746389389038086
Epoch 1580, val loss: 1.087655782699585
Epoch 1590, training loss: 438.41949462890625 = 1.0852303504943848 + 50.0 * 8.746685028076172
Epoch 1590, val loss: 1.0876439809799194
Epoch 1600, training loss: 438.6459655761719 = 1.0852092504501343 + 50.0 * 8.751214981079102
Epoch 1600, val loss: 1.0876319408416748
Epoch 1610, training loss: 438.6595153808594 = 1.0851874351501465 + 50.0 * 8.751486778259277
Epoch 1610, val loss: 1.0876197814941406
Epoch 1620, training loss: 438.8677978515625 = 1.0851666927337646 + 50.0 * 8.75565242767334
Epoch 1620, val loss: 1.0876094102859497
Epoch 1630, training loss: 438.90228271484375 = 1.085145115852356 + 50.0 * 8.756342887878418
Epoch 1630, val loss: 1.0875965356826782
Epoch 1640, training loss: 438.9532165527344 = 1.0851236581802368 + 50.0 * 8.757362365722656
Epoch 1640, val loss: 1.087585687637329
Epoch 1650, training loss: 438.93994140625 = 1.085101842880249 + 50.0 * 8.757096290588379
Epoch 1650, val loss: 1.0875725746154785
Epoch 1660, training loss: 437.0911560058594 = 1.0850419998168945 + 50.0 * 8.720122337341309
Epoch 1660, val loss: 1.0875247716903687
Epoch 1670, training loss: 438.35125732421875 = 1.0850468873977661 + 50.0 * 8.74532413482666
Epoch 1670, val loss: 1.0875389575958252
Epoch 1680, training loss: 438.26214599609375 = 1.0850266218185425 + 50.0 * 8.743542671203613
Epoch 1680, val loss: 1.0875287055969238
Epoch 1690, training loss: 438.6500244140625 = 1.0850157737731934 + 50.0 * 8.751299858093262
Epoch 1690, val loss: 1.0875252485275269
Epoch 1700, training loss: 438.8660888671875 = 1.085000991821289 + 50.0 * 8.755621910095215
Epoch 1700, val loss: 1.0875191688537598
Epoch 1710, training loss: 439.26068115234375 = 1.0849876403808594 + 50.0 * 8.763513565063477
Epoch 1710, val loss: 1.0875141620635986
Epoch 1720, training loss: 439.4264221191406 = 1.0849722623825073 + 50.0 * 8.766829490661621
Epoch 1720, val loss: 1.0875060558319092
Epoch 1730, training loss: 439.44146728515625 = 1.084954023361206 + 50.0 * 8.767129898071289
Epoch 1730, val loss: 1.0874968767166138
Epoch 1740, training loss: 439.5497131347656 = 1.084938406944275 + 50.0 * 8.769295692443848
Epoch 1740, val loss: 1.0874900817871094
Epoch 1750, training loss: 439.8060607910156 = 1.0849233865737915 + 50.0 * 8.774422645568848
Epoch 1750, val loss: 1.0874836444854736
Epoch 1760, training loss: 439.8018493652344 = 1.0849074125289917 + 50.0 * 8.774338722229004
Epoch 1760, val loss: 1.087475061416626
Epoch 1770, training loss: 439.8169860839844 = 1.0848909616470337 + 50.0 * 8.774641990661621
Epoch 1770, val loss: 1.0874676704406738
Epoch 1780, training loss: 439.8199768066406 = 1.0848743915557861 + 50.0 * 8.774702072143555
Epoch 1780, val loss: 1.0874594449996948
Epoch 1790, training loss: 439.8916015625 = 1.08486008644104 + 50.0 * 8.776134490966797
Epoch 1790, val loss: 1.0874532461166382
Epoch 1800, training loss: 440.0702209472656 = 1.0848464965820312 + 50.0 * 8.779707908630371
Epoch 1800, val loss: 1.0874477624893188
Epoch 1810, training loss: 439.937255859375 = 1.084826946258545 + 50.0 * 8.777048110961914
Epoch 1810, val loss: 1.0874378681182861
Epoch 1820, training loss: 440.1217041015625 = 1.0848162174224854 + 50.0 * 8.78073787689209
Epoch 1820, val loss: 1.0874344110488892
Epoch 1830, training loss: 440.34735107421875 = 1.0848031044006348 + 50.0 * 8.785250663757324
Epoch 1830, val loss: 1.0874290466308594
Epoch 1840, training loss: 440.32476806640625 = 1.084787368774414 + 50.0 * 8.784799575805664
Epoch 1840, val loss: 1.0874202251434326
Epoch 1850, training loss: 440.340576171875 = 1.0847737789154053 + 50.0 * 8.785116195678711
Epoch 1850, val loss: 1.0874149799346924
Epoch 1860, training loss: 440.4875183105469 = 1.0847601890563965 + 50.0 * 8.788055419921875
Epoch 1860, val loss: 1.0874089002609253
Epoch 1870, training loss: 440.65960693359375 = 1.0847477912902832 + 50.0 * 8.791497230529785
Epoch 1870, val loss: 1.0874037742614746
Epoch 1880, training loss: 440.6220703125 = 1.084733486175537 + 50.0 * 8.790746688842773
Epoch 1880, val loss: 1.087397813796997
Epoch 1890, training loss: 440.6408386230469 = 1.0847201347351074 + 50.0 * 8.791122436523438
Epoch 1890, val loss: 1.0873934030532837
Epoch 1900, training loss: 440.76788330078125 = 1.0847100019454956 + 50.0 * 8.793663024902344
Epoch 1900, val loss: 1.0873901844024658
Epoch 1910, training loss: 440.94183349609375 = 1.0846999883651733 + 50.0 * 8.79714298248291
Epoch 1910, val loss: 1.0873867273330688
Epoch 1920, training loss: 440.88909912109375 = 1.0846856832504272 + 50.0 * 8.796088218688965
Epoch 1920, val loss: 1.0873804092407227
Epoch 1930, training loss: 440.9264831542969 = 1.08467435836792 + 50.0 * 8.796835899353027
Epoch 1930, val loss: 1.0873759984970093
Epoch 1940, training loss: 441.03741455078125 = 1.0846636295318604 + 50.0 * 8.799055099487305
Epoch 1940, val loss: 1.0873733758926392
Epoch 1950, training loss: 441.2037353515625 = 1.0846549272537231 + 50.0 * 8.80238151550293
Epoch 1950, val loss: 1.0873717069625854
Epoch 1960, training loss: 441.1974792480469 = 1.0846421718597412 + 50.0 * 8.80225658416748
Epoch 1960, val loss: 1.0873661041259766
Epoch 1970, training loss: 441.0765380859375 = 1.0846295356750488 + 50.0 * 8.799838066101074
Epoch 1970, val loss: 1.08735990524292
Epoch 1980, training loss: 441.2156677246094 = 1.0846205949783325 + 50.0 * 8.802620887756348
Epoch 1980, val loss: 1.0873584747314453
Epoch 1990, training loss: 441.4085998535156 = 1.0846134424209595 + 50.0 * 8.806479454040527
Epoch 1990, val loss: 1.0873578786849976
Epoch 2000, training loss: 441.33990478515625 = 1.0846021175384521 + 50.0 * 8.805106163024902
Epoch 2000, val loss: 1.0873522758483887
Epoch 2010, training loss: 440.0721740722656 = 1.0845147371292114 + 50.0 * 8.779753684997559
Epoch 2010, val loss: 1.087266206741333
Epoch 2020, training loss: 440.0671081542969 = 1.084528923034668 + 50.0 * 8.779651641845703
Epoch 2020, val loss: 1.0872942209243774
Epoch 2030, training loss: 439.9715881347656 = 1.0845180749893188 + 50.0 * 8.777741432189941
Epoch 2030, val loss: 1.0872957706451416
Epoch 2040, training loss: 439.94110107421875 = 1.0845144987106323 + 50.0 * 8.777132034301758
Epoch 2040, val loss: 1.087297797203064
Epoch 2050, training loss: 440.67431640625 = 1.0845259428024292 + 50.0 * 8.79179573059082
Epoch 2050, val loss: 1.0873136520385742
Epoch 2060, training loss: 441.0240783691406 = 1.0845242738723755 + 50.0 * 8.79879093170166
Epoch 2060, val loss: 1.0873178243637085
Epoch 2070, training loss: 441.5091552734375 = 1.0845223665237427 + 50.0 * 8.808492660522461
Epoch 2070, val loss: 1.0873210430145264
Epoch 2080, training loss: 441.8395690917969 = 1.0845186710357666 + 50.0 * 8.81510066986084
Epoch 2080, val loss: 1.0873233079910278
Epoch 2090, training loss: 441.96099853515625 = 1.0845129489898682 + 50.0 * 8.817529678344727
Epoch 2090, val loss: 1.0873230695724487
Epoch 2100, training loss: 442.1789855957031 = 1.084507703781128 + 50.0 * 8.821889877319336
Epoch 2100, val loss: 1.0873241424560547
Epoch 2110, training loss: 442.38031005859375 = 1.084501028060913 + 50.0 * 8.825916290283203
Epoch 2110, val loss: 1.0873239040374756
Epoch 2120, training loss: 442.4848937988281 = 1.0844956636428833 + 50.0 * 8.828007698059082
Epoch 2120, val loss: 1.0873240232467651
Epoch 2130, training loss: 442.5435791015625 = 1.0844894647598267 + 50.0 * 8.829181671142578
Epoch 2130, val loss: 1.0873233079910278
Epoch 2140, training loss: 442.6192321777344 = 1.0844823122024536 + 50.0 * 8.830695152282715
Epoch 2140, val loss: 1.0873216390609741
Epoch 2150, training loss: 442.6646728515625 = 1.0844751596450806 + 50.0 * 8.83160400390625
Epoch 2150, val loss: 1.0873210430145264
Epoch 2160, training loss: 442.7669982910156 = 1.0844697952270508 + 50.0 * 8.833650588989258
Epoch 2160, val loss: 1.087321400642395
Epoch 2170, training loss: 442.8614196777344 = 1.0844628810882568 + 50.0 * 8.835538864135742
Epoch 2170, val loss: 1.0873205661773682
Epoch 2180, training loss: 442.95123291015625 = 1.0844568014144897 + 50.0 * 8.837335586547852
Epoch 2180, val loss: 1.0873196125030518
Epoch 2190, training loss: 442.9661865234375 = 1.084450125694275 + 50.0 * 8.837635040283203
Epoch 2190, val loss: 1.087319254875183
Epoch 2200, training loss: 443.06207275390625 = 1.0844447612762451 + 50.0 * 8.839552879333496
Epoch 2200, val loss: 1.087319254875183
Epoch 2210, training loss: 443.2064208984375 = 1.0844403505325317 + 50.0 * 8.842439651489258
Epoch 2210, val loss: 1.0873199701309204
Epoch 2220, training loss: 443.1935119628906 = 1.0844335556030273 + 50.0 * 8.842181205749512
Epoch 2220, val loss: 1.087318778038025
Epoch 2230, training loss: 443.27264404296875 = 1.0844272375106812 + 50.0 * 8.843764305114746
Epoch 2230, val loss: 1.087317705154419
Epoch 2240, training loss: 443.4010925292969 = 1.084423303604126 + 50.0 * 8.846333503723145
Epoch 2240, val loss: 1.087318778038025
Epoch 2250, training loss: 443.5177307128906 = 1.0844192504882812 + 50.0 * 8.848666191101074
Epoch 2250, val loss: 1.0873198509216309
Epoch 2260, training loss: 443.54010009765625 = 1.0844125747680664 + 50.0 * 8.849113464355469
Epoch 2260, val loss: 1.0873174667358398
Epoch 2270, training loss: 443.6861877441406 = 1.0844061374664307 + 50.0 * 8.852035522460938
Epoch 2270, val loss: 1.0873169898986816
Epoch 2280, training loss: 440.7261657714844 = 1.0843254327774048 + 50.0 * 8.792837142944336
Epoch 2280, val loss: 1.0872459411621094
Epoch 2290, training loss: 441.5404052734375 = 1.0843480825424194 + 50.0 * 8.809121131896973
Epoch 2290, val loss: 1.0872738361358643
Epoch 2300, training loss: 441.98516845703125 = 1.0843584537506104 + 50.0 * 8.818016052246094
Epoch 2300, val loss: 1.0872893333435059
Epoch 2310, training loss: 442.5419006347656 = 1.0843626260757446 + 50.0 * 8.829151153564453
Epoch 2310, val loss: 1.0872979164123535
Epoch 2320, training loss: 442.81256103515625 = 1.0843641757965088 + 50.0 * 8.834564208984375
Epoch 2320, val loss: 1.0873037576675415
Epoch 2330, training loss: 443.1430969238281 = 1.0843656063079834 + 50.0 * 8.841175079345703
Epoch 2330, val loss: 1.0873091220855713
Epoch 2340, training loss: 443.4283447265625 = 1.084364414215088 + 50.0 * 8.846879959106445
Epoch 2340, val loss: 1.0873132944107056
Epoch 2350, training loss: 443.61474609375 = 1.084362268447876 + 50.0 * 8.850607872009277
Epoch 2350, val loss: 1.0873146057128906
Epoch 2360, training loss: 443.79376220703125 = 1.0843604803085327 + 50.0 * 8.854187965393066
Epoch 2360, val loss: 1.0873173475265503
Epoch 2370, training loss: 443.671630859375 = 1.0843526124954224 + 50.0 * 8.85174560546875
Epoch 2370, val loss: 1.0873136520385742
Epoch 2380, training loss: 443.7742004394531 = 1.0843498706817627 + 50.0 * 8.85379695892334
Epoch 2380, val loss: 1.0873159170150757
Epoch 2390, training loss: 443.93511962890625 = 1.084348201751709 + 50.0 * 8.857015609741211
Epoch 2390, val loss: 1.0873180627822876
Epoch 2400, training loss: 444.11456298828125 = 1.084347128868103 + 50.0 * 8.860604286193848
Epoch 2400, val loss: 1.0873210430145264
Epoch 2410, training loss: 444.0676574707031 = 1.084341287612915 + 50.0 * 8.859665870666504
Epoch 2410, val loss: 1.0873188972473145
Epoch 2420, training loss: 443.9838562011719 = 1.0843358039855957 + 50.0 * 8.857990264892578
Epoch 2420, val loss: 1.087319016456604
Epoch 2430, training loss: 444.16278076171875 = 1.084333062171936 + 50.0 * 8.86156940460205
Epoch 2430, val loss: 1.0873208045959473
Epoch 2440, training loss: 444.298828125 = 1.08433198928833 + 50.0 * 8.864290237426758
Epoch 2440, val loss: 1.0873225927352905
Epoch 2450, training loss: 444.4463195800781 = 1.0843303203582764 + 50.0 * 8.867239952087402
Epoch 2450, val loss: 1.0873247385025024
Epoch 2460, training loss: 444.42401123046875 = 1.0843251943588257 + 50.0 * 8.866793632507324
Epoch 2460, val loss: 1.0873239040374756
Epoch 2470, training loss: 444.5134582519531 = 1.0843236446380615 + 50.0 * 8.868582725524902
Epoch 2470, val loss: 1.0873262882232666
Epoch 2480, training loss: 444.52313232421875 = 1.084320068359375 + 50.0 * 8.868776321411133
Epoch 2480, val loss: 1.0873267650604248
Epoch 2490, training loss: 444.62774658203125 = 1.0843160152435303 + 50.0 * 8.870868682861328
Epoch 2490, val loss: 1.0873266458511353
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.39666666666666667
0.8643048612620445
=== training gcn model ===
Epoch 0, training loss: 500.1874084472656 = 1.089446783065796 + 50.0 * 9.981959342956543
Epoch 0, val loss: 1.0902342796325684
Epoch 10, training loss: 475.02239990234375 = 1.089275598526001 + 50.0 * 9.478662490844727
Epoch 10, val loss: 1.090060830116272
Epoch 20, training loss: 466.4678955078125 = 1.0890812873840332 + 50.0 * 9.307576179504395
Epoch 20, val loss: 1.089862585067749
Epoch 30, training loss: 460.4866943359375 = 1.0888566970825195 + 50.0 * 9.187956809997559
Epoch 30, val loss: 1.0896316766738892
Epoch 40, training loss: 455.8473205566406 = 1.0886138677597046 + 50.0 * 9.095173835754395
Epoch 40, val loss: 1.0893878936767578
Epoch 50, training loss: 452.2161560058594 = 1.0883750915527344 + 50.0 * 9.022555351257324
Epoch 50, val loss: 1.0891510248184204
Epoch 60, training loss: 449.2513427734375 = 1.0881413221359253 + 50.0 * 8.963264465332031
Epoch 60, val loss: 1.0889157056808472
Epoch 70, training loss: 446.7832946777344 = 1.0878993272781372 + 50.0 * 8.913908004760742
Epoch 70, val loss: 1.0886718034744263
Epoch 80, training loss: 444.7000427246094 = 1.087660789489746 + 50.0 * 8.872247695922852
Epoch 80, val loss: 1.0884370803833008
Epoch 90, training loss: 442.870849609375 = 1.0874226093292236 + 50.0 * 8.835668563842773
Epoch 90, val loss: 1.088196873664856
Epoch 100, training loss: 441.3041687011719 = 1.0871870517730713 + 50.0 * 8.804339408874512
Epoch 100, val loss: 1.087963581085205
Epoch 110, training loss: 440.02728271484375 = 1.0869522094726562 + 50.0 * 8.778806686401367
Epoch 110, val loss: 1.0877288579940796
Epoch 120, training loss: 438.7771911621094 = 1.0867234468460083 + 50.0 * 8.753808975219727
Epoch 120, val loss: 1.0875006914138794
Epoch 130, training loss: 437.6962890625 = 1.0864931344985962 + 50.0 * 8.732195854187012
Epoch 130, val loss: 1.0872701406478882
Epoch 140, training loss: 436.7442932128906 = 1.086268424987793 + 50.0 * 8.713160514831543
Epoch 140, val loss: 1.087048053741455
Epoch 150, training loss: 435.9169006347656 = 1.086043119430542 + 50.0 * 8.696617126464844
Epoch 150, val loss: 1.0868213176727295
Epoch 160, training loss: 435.1433410644531 = 1.0858268737792969 + 50.0 * 8.681150436401367
Epoch 160, val loss: 1.0866053104400635
Epoch 170, training loss: 434.4484558105469 = 1.085606336593628 + 50.0 * 8.667257308959961
Epoch 170, val loss: 1.0863840579986572
Epoch 180, training loss: 433.9206848144531 = 1.0853904485702515 + 50.0 * 8.656705856323242
Epoch 180, val loss: 1.086167573928833
Epoch 190, training loss: 433.3504943847656 = 1.0851725339889526 + 50.0 * 8.645306587219238
Epoch 190, val loss: 1.0859488248825073
Epoch 200, training loss: 432.8516540527344 = 1.084957242012024 + 50.0 * 8.635334014892578
Epoch 200, val loss: 1.085734248161316
Epoch 210, training loss: 432.4169921875 = 1.084741234779358 + 50.0 * 8.6266450881958
Epoch 210, val loss: 1.0855172872543335
Epoch 220, training loss: 432.0693664550781 = 1.0845295190811157 + 50.0 * 8.619696617126465
Epoch 220, val loss: 1.0853074789047241
Epoch 230, training loss: 431.6526184082031 = 1.0843214988708496 + 50.0 * 8.611366271972656
Epoch 230, val loss: 1.085099697113037
Epoch 240, training loss: 431.3996887207031 = 1.084115982055664 + 50.0 * 8.606311798095703
Epoch 240, val loss: 1.0848946571350098
Epoch 250, training loss: 431.1739807128906 = 1.0839037895202637 + 50.0 * 8.601801872253418
Epoch 250, val loss: 1.0846827030181885
Epoch 260, training loss: 430.8521423339844 = 1.0837053060531616 + 50.0 * 8.595368385314941
Epoch 260, val loss: 1.0844852924346924
Epoch 270, training loss: 430.6842956542969 = 1.0834956169128418 + 50.0 * 8.592016220092773
Epoch 270, val loss: 1.0842784643173218
Epoch 280, training loss: 430.42767333984375 = 1.0832996368408203 + 50.0 * 8.58688735961914
Epoch 280, val loss: 1.0840798616409302
Epoch 290, training loss: 430.1856994628906 = 1.08309805393219 + 50.0 * 8.582052230834961
Epoch 290, val loss: 1.0838803052902222
Epoch 300, training loss: 430.1320495605469 = 1.082899808883667 + 50.0 * 8.58098316192627
Epoch 300, val loss: 1.0836818218231201
Epoch 310, training loss: 429.93035888671875 = 1.082699179649353 + 50.0 * 8.576952934265137
Epoch 310, val loss: 1.083482265472412
Epoch 320, training loss: 429.739013671875 = 1.0824995040893555 + 50.0 * 8.57313060760498
Epoch 320, val loss: 1.0832844972610474
Epoch 330, training loss: 429.703369140625 = 1.082306981086731 + 50.0 * 8.572421073913574
Epoch 330, val loss: 1.083092212677002
Epoch 340, training loss: 429.5107116699219 = 1.0821088552474976 + 50.0 * 8.568572044372559
Epoch 340, val loss: 1.0828937292099
Epoch 350, training loss: 429.4077453613281 = 1.0819169282913208 + 50.0 * 8.566516876220703
Epoch 350, val loss: 1.0827032327651978
Epoch 360, training loss: 429.3132629394531 = 1.0817248821258545 + 50.0 * 8.564630508422852
Epoch 360, val loss: 1.0825115442276
Epoch 370, training loss: 429.24835205078125 = 1.081534504890442 + 50.0 * 8.563336372375488
Epoch 370, val loss: 1.0823229551315308
Epoch 380, training loss: 429.0629577636719 = 1.0813422203063965 + 50.0 * 8.559632301330566
Epoch 380, val loss: 1.0821324586868286
Epoch 390, training loss: 429.0567626953125 = 1.0811554193496704 + 50.0 * 8.5595121383667
Epoch 390, val loss: 1.0819439888000488
Epoch 400, training loss: 428.9794616699219 = 1.0809648036956787 + 50.0 * 8.55797004699707
Epoch 400, val loss: 1.0817558765411377
Epoch 410, training loss: 428.9446716308594 = 1.08078134059906 + 50.0 * 8.55727767944336
Epoch 410, val loss: 1.0815719366073608
Epoch 420, training loss: 429.05926513671875 = 1.0806010961532593 + 50.0 * 8.55957317352295
Epoch 420, val loss: 1.081392765045166
Epoch 430, training loss: 428.93914794921875 = 1.08041512966156 + 50.0 * 8.557174682617188
Epoch 430, val loss: 1.081206202507019
Epoch 440, training loss: 429.0378112792969 = 1.0802327394485474 + 50.0 * 8.559151649475098
Epoch 440, val loss: 1.0810221433639526
Epoch 450, training loss: 429.0541687011719 = 1.0800554752349854 + 50.0 * 8.55948257446289
Epoch 450, val loss: 1.0808477401733398
Epoch 460, training loss: 429.0448913574219 = 1.0798755884170532 + 50.0 * 8.559300422668457
Epoch 460, val loss: 1.0806688070297241
Epoch 470, training loss: 428.9598388671875 = 1.079689621925354 + 50.0 * 8.557602882385254
Epoch 470, val loss: 1.0804909467697144
Epoch 480, training loss: 428.8739929199219 = 1.0795159339904785 + 50.0 * 8.555889129638672
Epoch 480, val loss: 1.080310344696045
Epoch 490, training loss: 428.9091796875 = 1.0793415307998657 + 50.0 * 8.556596755981445
Epoch 490, val loss: 1.0801372528076172
Epoch 500, training loss: 428.804443359375 = 1.0791444778442383 + 50.0 * 8.554506301879883
Epoch 500, val loss: 1.0799411535263062
Epoch 510, training loss: 428.984619140625 = 1.0789865255355835 + 50.0 * 8.558113098144531
Epoch 510, val loss: 1.0797795057296753
Epoch 520, training loss: 428.9514465332031 = 1.0788160562515259 + 50.0 * 8.557452201843262
Epoch 520, val loss: 1.0796115398406982
Epoch 530, training loss: 428.9615478515625 = 1.0786398649215698 + 50.0 * 8.557658195495605
Epoch 530, val loss: 1.0794363021850586
Epoch 540, training loss: 428.9769592285156 = 1.0784696340560913 + 50.0 * 8.55797004699707
Epoch 540, val loss: 1.079265832901001
Epoch 550, training loss: 429.0799865722656 = 1.0783010721206665 + 50.0 * 8.560033798217773
Epoch 550, val loss: 1.079098105430603
Epoch 560, training loss: 429.01153564453125 = 1.078133225440979 + 50.0 * 8.55866813659668
Epoch 560, val loss: 1.0789313316345215
Epoch 570, training loss: 429.0770263671875 = 1.0779632329940796 + 50.0 * 8.559981346130371
Epoch 570, val loss: 1.0787626504898071
Epoch 580, training loss: 429.1253356933594 = 1.077797770500183 + 50.0 * 8.560951232910156
Epoch 580, val loss: 1.0785913467407227
Epoch 590, training loss: 429.17913818359375 = 1.0776315927505493 + 50.0 * 8.562029838562012
Epoch 590, val loss: 1.0784286260604858
Epoch 600, training loss: 429.20196533203125 = 1.0774683952331543 + 50.0 * 8.562490463256836
Epoch 600, val loss: 1.0782639980316162
Epoch 610, training loss: 429.35150146484375 = 1.0773048400878906 + 50.0 * 8.565484046936035
Epoch 610, val loss: 1.078102469444275
Epoch 620, training loss: 429.3650817871094 = 1.0771422386169434 + 50.0 * 8.56575870513916
Epoch 620, val loss: 1.0779370069503784
Epoch 630, training loss: 429.41845703125 = 1.0769803524017334 + 50.0 * 8.566829681396484
Epoch 630, val loss: 1.077775478363037
Epoch 640, training loss: 429.4443664550781 = 1.0768182277679443 + 50.0 * 8.567351341247559
Epoch 640, val loss: 1.0776156187057495
Epoch 650, training loss: 429.5084228515625 = 1.0766572952270508 + 50.0 * 8.568634986877441
Epoch 650, val loss: 1.0774558782577515
Epoch 660, training loss: 429.6596374511719 = 1.0765000581741333 + 50.0 * 8.571662902832031
Epoch 660, val loss: 1.0772972106933594
Epoch 670, training loss: 429.6951599121094 = 1.0763416290283203 + 50.0 * 8.572376251220703
Epoch 670, val loss: 1.0771392583847046
Epoch 680, training loss: 429.7820129394531 = 1.0761854648590088 + 50.0 * 8.574116706848145
Epoch 680, val loss: 1.0769840478897095
Epoch 690, training loss: 429.7833251953125 = 1.0760307312011719 + 50.0 * 8.574146270751953
Epoch 690, val loss: 1.0768269300460815
Epoch 700, training loss: 429.9139709472656 = 1.0758737325668335 + 50.0 * 8.576762199401855
Epoch 700, val loss: 1.0766745805740356
Epoch 710, training loss: 429.73919677734375 = 1.0757136344909668 + 50.0 * 8.573269844055176
Epoch 710, val loss: 1.076513409614563
Epoch 720, training loss: 429.7115478515625 = 1.075562834739685 + 50.0 * 8.57271957397461
Epoch 720, val loss: 1.0763623714447021
Epoch 730, training loss: 429.9101257324219 = 1.075413465499878 + 50.0 * 8.57669448852539
Epoch 730, val loss: 1.076210379600525
Epoch 740, training loss: 429.9333801269531 = 1.0752637386322021 + 50.0 * 8.577162742614746
Epoch 740, val loss: 1.0760619640350342
Epoch 750, training loss: 430.29425048828125 = 1.0751150846481323 + 50.0 * 8.584383010864258
Epoch 750, val loss: 1.0759150981903076
Epoch 760, training loss: 430.0837707519531 = 1.0749590396881104 + 50.0 * 8.58017635345459
Epoch 760, val loss: 1.0757611989974976
Epoch 770, training loss: 430.1963806152344 = 1.0748142004013062 + 50.0 * 8.582430839538574
Epoch 770, val loss: 1.0756134986877441
Epoch 780, training loss: 430.3078308105469 = 1.0746694803237915 + 50.0 * 8.584663391113281
Epoch 780, val loss: 1.0754687786102295
Epoch 790, training loss: 430.3157958984375 = 1.0745198726654053 + 50.0 * 8.58482551574707
Epoch 790, val loss: 1.0753203630447388
Epoch 800, training loss: 430.5445251464844 = 1.0743796825408936 + 50.0 * 8.58940315246582
Epoch 800, val loss: 1.0751780271530151
Epoch 810, training loss: 430.5926208496094 = 1.0742322206497192 + 50.0 * 8.590368270874023
Epoch 810, val loss: 1.0750313997268677
Epoch 820, training loss: 430.7741394042969 = 1.0740922689437866 + 50.0 * 8.594000816345215
Epoch 820, val loss: 1.0748900175094604
Epoch 830, training loss: 430.80780029296875 = 1.0739479064941406 + 50.0 * 8.594676971435547
Epoch 830, val loss: 1.074747920036316
Epoch 840, training loss: 430.9901428222656 = 1.073808193206787 + 50.0 * 8.598326683044434
Epoch 840, val loss: 1.0746064186096191
Epoch 850, training loss: 431.01776123046875 = 1.0736666917800903 + 50.0 * 8.598881721496582
Epoch 850, val loss: 1.074463129043579
Epoch 860, training loss: 430.9892272949219 = 1.0735238790512085 + 50.0 * 8.59831428527832
Epoch 860, val loss: 1.074321985244751
Epoch 870, training loss: 431.16845703125 = 1.0733860731124878 + 50.0 * 8.601901054382324
Epoch 870, val loss: 1.0741842985153198
Epoch 880, training loss: 431.31524658203125 = 1.0732505321502686 + 50.0 * 8.604840278625488
Epoch 880, val loss: 1.0740455389022827
Epoch 890, training loss: 431.11749267578125 = 1.0731072425842285 + 50.0 * 8.600887298583984
Epoch 890, val loss: 1.0739022493362427
Epoch 900, training loss: 431.2278747558594 = 1.072972059249878 + 50.0 * 8.603097915649414
Epoch 900, val loss: 1.073769211769104
Epoch 910, training loss: 431.38671875 = 1.072838306427002 + 50.0 * 8.606277465820312
Epoch 910, val loss: 1.0736340284347534
Epoch 920, training loss: 431.3764953613281 = 1.0727020502090454 + 50.0 * 8.60607624053955
Epoch 920, val loss: 1.073500156402588
Epoch 930, training loss: 431.33526611328125 = 1.0725624561309814 + 50.0 * 8.605254173278809
Epoch 930, val loss: 1.0733648538589478
Epoch 940, training loss: 431.3549499511719 = 1.072426438331604 + 50.0 * 8.605650901794434
Epoch 940, val loss: 1.0732260942459106
Epoch 950, training loss: 431.470703125 = 1.0722979307174683 + 50.0 * 8.6079683303833
Epoch 950, val loss: 1.0730974674224854
Epoch 960, training loss: 431.6002502441406 = 1.072167158126831 + 50.0 * 8.61056137084961
Epoch 960, val loss: 1.0729670524597168
Epoch 970, training loss: 431.4164733886719 = 1.0720239877700806 + 50.0 * 8.606888771057129
Epoch 970, val loss: 1.0728228092193604
Epoch 980, training loss: 431.8056335449219 = 1.071900725364685 + 50.0 * 8.61467456817627
Epoch 980, val loss: 1.0726959705352783
Epoch 990, training loss: 431.6634826660156 = 1.071758508682251 + 50.0 * 8.611834526062012
Epoch 990, val loss: 1.0725533962249756
Epoch 1000, training loss: 431.7593994140625 = 1.0716394186019897 + 50.0 * 8.613755226135254
Epoch 1000, val loss: 1.0724371671676636
Epoch 1010, training loss: 431.60687255859375 = 1.071512222290039 + 50.0 * 8.61070728302002
Epoch 1010, val loss: 1.0723118782043457
Epoch 1020, training loss: 431.90057373046875 = 1.0713881254196167 + 50.0 * 8.616583824157715
Epoch 1020, val loss: 1.072190284729004
Epoch 1030, training loss: 431.68634033203125 = 1.0712627172470093 + 50.0 * 8.61230182647705
Epoch 1030, val loss: 1.0720651149749756
Epoch 1040, training loss: 431.9485778808594 = 1.0711429119110107 + 50.0 * 8.617548942565918
Epoch 1040, val loss: 1.0719438791275024
Epoch 1050, training loss: 432.12652587890625 = 1.071018099784851 + 50.0 * 8.621109962463379
Epoch 1050, val loss: 1.0718177556991577
Epoch 1060, training loss: 432.29046630859375 = 1.0708975791931152 + 50.0 * 8.624391555786133
Epoch 1060, val loss: 1.0716978311538696
Epoch 1070, training loss: 432.5414123535156 = 1.0707770586013794 + 50.0 * 8.629412651062012
Epoch 1070, val loss: 1.0715762376785278
Epoch 1080, training loss: 432.568603515625 = 1.0706547498703003 + 50.0 * 8.629959106445312
Epoch 1080, val loss: 1.0714508295059204
Epoch 1090, training loss: 432.5246276855469 = 1.0705302953720093 + 50.0 * 8.629081726074219
Epoch 1090, val loss: 1.0713294744491577
Epoch 1100, training loss: 432.6514587402344 = 1.0704118013381958 + 50.0 * 8.631621360778809
Epoch 1100, val loss: 1.0712109804153442
Epoch 1110, training loss: 432.8221435546875 = 1.0702919960021973 + 50.0 * 8.635037422180176
Epoch 1110, val loss: 1.071093201637268
Epoch 1120, training loss: 432.9271240234375 = 1.0701733827590942 + 50.0 * 8.637139320373535
Epoch 1120, val loss: 1.0709717273712158
Epoch 1130, training loss: 433.11358642578125 = 1.0700569152832031 + 50.0 * 8.640870094299316
Epoch 1130, val loss: 1.0708568096160889
Epoch 1140, training loss: 433.0696716308594 = 1.069935917854309 + 50.0 * 8.639994621276855
Epoch 1140, val loss: 1.0707365274429321
Epoch 1150, training loss: 433.2369689941406 = 1.0698227882385254 + 50.0 * 8.643342971801758
Epoch 1150, val loss: 1.0706228017807007
Epoch 1160, training loss: 433.28607177734375 = 1.0697053670883179 + 50.0 * 8.644327163696289
Epoch 1160, val loss: 1.0705063343048096
Epoch 1170, training loss: 433.440185546875 = 1.0695927143096924 + 50.0 * 8.647412300109863
Epoch 1170, val loss: 1.0703915357589722
Epoch 1180, training loss: 433.3560485839844 = 1.069474458694458 + 50.0 * 8.645730972290039
Epoch 1180, val loss: 1.0702753067016602
Epoch 1190, training loss: 433.4869384765625 = 1.0693637132644653 + 50.0 * 8.648351669311523
Epoch 1190, val loss: 1.0701621770858765
Epoch 1200, training loss: 433.6412353515625 = 1.0692517757415771 + 50.0 * 8.651439666748047
Epoch 1200, val loss: 1.0700511932373047
Epoch 1210, training loss: 433.6468200683594 = 1.0691393613815308 + 50.0 * 8.6515531539917
Epoch 1210, val loss: 1.0699397325515747
Epoch 1220, training loss: 433.77081298828125 = 1.0690284967422485 + 50.0 * 8.654035568237305
Epoch 1220, val loss: 1.0698270797729492
Epoch 1230, training loss: 433.7897033691406 = 1.0689177513122559 + 50.0 * 8.65441608428955
Epoch 1230, val loss: 1.069716453552246
Epoch 1240, training loss: 433.6441955566406 = 1.0688037872314453 + 50.0 * 8.651507377624512
Epoch 1240, val loss: 1.069604754447937
Epoch 1250, training loss: 433.81573486328125 = 1.0686969757080078 + 50.0 * 8.654940605163574
Epoch 1250, val loss: 1.0694959163665771
Epoch 1260, training loss: 433.88427734375 = 1.0685869455337524 + 50.0 * 8.6563138961792
Epoch 1260, val loss: 1.0693864822387695
Epoch 1270, training loss: 434.16363525390625 = 1.068481683731079 + 50.0 * 8.661903381347656
Epoch 1270, val loss: 1.069280982017517
Epoch 1280, training loss: 434.2140808105469 = 1.0683741569519043 + 50.0 * 8.662914276123047
Epoch 1280, val loss: 1.0691732168197632
Epoch 1290, training loss: 434.1960144042969 = 1.068265676498413 + 50.0 * 8.662554740905762
Epoch 1290, val loss: 1.069064736366272
Epoch 1300, training loss: 434.2777404785156 = 1.0681601762771606 + 50.0 * 8.664192199707031
Epoch 1300, val loss: 1.0689599514007568
Epoch 1310, training loss: 434.2160949707031 = 1.0680521726608276 + 50.0 * 8.66296100616455
Epoch 1310, val loss: 1.0688525438308716
Epoch 1320, training loss: 434.44317626953125 = 1.067952036857605 + 50.0 * 8.66750431060791
Epoch 1320, val loss: 1.0687525272369385
Epoch 1330, training loss: 434.48486328125 = 1.0678484439849854 + 50.0 * 8.668340682983398
Epoch 1330, val loss: 1.0686479806900024
Epoch 1340, training loss: 434.5494079589844 = 1.0677464008331299 + 50.0 * 8.669632911682129
Epoch 1340, val loss: 1.0685442686080933
Epoch 1350, training loss: 434.5784606933594 = 1.0676394701004028 + 50.0 * 8.67021656036377
Epoch 1350, val loss: 1.0684396028518677
Epoch 1360, training loss: 434.6567687988281 = 1.0675410032272339 + 50.0 * 8.671784400939941
Epoch 1360, val loss: 1.0683389902114868
Epoch 1370, training loss: 434.8191833496094 = 1.0674402713775635 + 50.0 * 8.675034523010254
Epoch 1370, val loss: 1.0682388544082642
Epoch 1380, training loss: 434.8235778808594 = 1.067336916923523 + 50.0 * 8.675125122070312
Epoch 1380, val loss: 1.068135380744934
Epoch 1390, training loss: 434.5731506347656 = 1.06723153591156 + 50.0 * 8.67011833190918
Epoch 1390, val loss: 1.0680325031280518
Epoch 1400, training loss: 434.8536682128906 = 1.0671360492706299 + 50.0 * 8.67573070526123
Epoch 1400, val loss: 1.067938208580017
Epoch 1410, training loss: 435.0197448730469 = 1.0670413970947266 + 50.0 * 8.679054260253906
Epoch 1410, val loss: 1.0678391456604004
Epoch 1420, training loss: 435.0013427734375 = 1.0669410228729248 + 50.0 * 8.678688049316406
Epoch 1420, val loss: 1.0677396059036255
Epoch 1430, training loss: 435.11163330078125 = 1.0668445825576782 + 50.0 * 8.680895805358887
Epoch 1430, val loss: 1.0676435232162476
Epoch 1440, training loss: 435.29254150390625 = 1.0667507648468018 + 50.0 * 8.684515953063965
Epoch 1440, val loss: 1.067548394203186
Epoch 1450, training loss: 435.35296630859375 = 1.066651701927185 + 50.0 * 8.685726165771484
Epoch 1450, val loss: 1.0674493312835693
Epoch 1460, training loss: 435.4882507324219 = 1.0665562152862549 + 50.0 * 8.688433647155762
Epoch 1460, val loss: 1.0673538446426392
Epoch 1470, training loss: 435.5797424316406 = 1.0664639472961426 + 50.0 * 8.690265655517578
Epoch 1470, val loss: 1.067261815071106
Epoch 1480, training loss: 435.5771179199219 = 1.0663686990737915 + 50.0 * 8.690215110778809
Epoch 1480, val loss: 1.0671662092208862
Epoch 1490, training loss: 435.14910888671875 = 1.0662654638290405 + 50.0 * 8.681656837463379
Epoch 1490, val loss: 1.0670636892318726
Epoch 1500, training loss: 435.1908264160156 = 1.0661718845367432 + 50.0 * 8.682493209838867
Epoch 1500, val loss: 1.0669728517532349
Epoch 1510, training loss: 435.3758544921875 = 1.0660858154296875 + 50.0 * 8.686195373535156
Epoch 1510, val loss: 1.0668847560882568
Epoch 1520, training loss: 435.62615966796875 = 1.0659981966018677 + 50.0 * 8.691203117370605
Epoch 1520, val loss: 1.0667957067489624
Epoch 1530, training loss: 435.715576171875 = 1.0659042596817017 + 50.0 * 8.6929931640625
Epoch 1530, val loss: 1.0667024850845337
Epoch 1540, training loss: 435.85772705078125 = 1.0658183097839355 + 50.0 * 8.69583797454834
Epoch 1540, val loss: 1.0666149854660034
Epoch 1550, training loss: 436.0230407714844 = 1.0657293796539307 + 50.0 * 8.699146270751953
Epoch 1550, val loss: 1.0665262937545776
Epoch 1560, training loss: 436.0032043457031 = 1.0656377077102661 + 50.0 * 8.698751449584961
Epoch 1560, val loss: 1.0664327144622803
Epoch 1570, training loss: 436.02386474609375 = 1.065548062324524 + 50.0 * 8.699166297912598
Epoch 1570, val loss: 1.0663456916809082
Epoch 1580, training loss: 436.02545166015625 = 1.0654598474502563 + 50.0 * 8.699199676513672
Epoch 1580, val loss: 1.066257357597351
Epoch 1590, training loss: 436.1673889160156 = 1.0653741359710693 + 50.0 * 8.702040672302246
Epoch 1590, val loss: 1.066172480583191
Epoch 1600, training loss: 436.0986022949219 = 1.0652862787246704 + 50.0 * 8.700666427612305
Epoch 1600, val loss: 1.066084861755371
Epoch 1610, training loss: 436.1799621582031 = 1.0652005672454834 + 50.0 * 8.702295303344727
Epoch 1610, val loss: 1.0660011768341064
Epoch 1620, training loss: 435.8453674316406 = 1.0650982856750488 + 50.0 * 8.695605278015137
Epoch 1620, val loss: 1.0658977031707764
Epoch 1630, training loss: 436.1608581542969 = 1.065021276473999 + 50.0 * 8.701916694641113
Epoch 1630, val loss: 1.0658198595046997
Epoch 1640, training loss: 436.3728332519531 = 1.0649425983428955 + 50.0 * 8.706157684326172
Epoch 1640, val loss: 1.0657401084899902
Epoch 1650, training loss: 436.4773864746094 = 1.064862608909607 + 50.0 * 8.708250999450684
Epoch 1650, val loss: 1.0656615495681763
Epoch 1660, training loss: 436.45440673828125 = 1.0647790431976318 + 50.0 * 8.707792282104492
Epoch 1660, val loss: 1.0655741691589355
Epoch 1670, training loss: 436.48541259765625 = 1.0646954774856567 + 50.0 * 8.708414077758789
Epoch 1670, val loss: 1.0654913187026978
Epoch 1680, training loss: 436.5793762207031 = 1.0646147727966309 + 50.0 * 8.710295677185059
Epoch 1680, val loss: 1.0654104948043823
Epoch 1690, training loss: 436.7591552734375 = 1.0645350217819214 + 50.0 * 8.713891983032227
Epoch 1690, val loss: 1.0653316974639893
Epoch 1700, training loss: 436.6778564453125 = 1.0644519329071045 + 50.0 * 8.712267875671387
Epoch 1700, val loss: 1.0652503967285156
Epoch 1710, training loss: 436.7699279785156 = 1.0643714666366577 + 50.0 * 8.714111328125
Epoch 1710, val loss: 1.0651679039001465
Epoch 1720, training loss: 436.7679748535156 = 1.0642913579940796 + 50.0 * 8.714073181152344
Epoch 1720, val loss: 1.065087080001831
Epoch 1730, training loss: 437.3329772949219 = 1.0642125606536865 + 50.0 * 8.725375175476074
Epoch 1730, val loss: 1.0650078058242798
Epoch 1740, training loss: 436.78900146484375 = 1.0641148090362549 + 50.0 * 8.714497566223145
Epoch 1740, val loss: 1.0649100542068481
Epoch 1750, training loss: 437.15692138671875 = 1.0640454292297363 + 50.0 * 8.721857070922852
Epoch 1750, val loss: 1.064842939376831
Epoch 1760, training loss: 437.2834167480469 = 1.0639762878417969 + 50.0 * 8.72438907623291
Epoch 1760, val loss: 1.064772367477417
Epoch 1770, training loss: 437.6148681640625 = 1.063904881477356 + 50.0 * 8.731019020080566
Epoch 1770, val loss: 1.0647010803222656
Epoch 1780, training loss: 437.7493591308594 = 1.0638307332992554 + 50.0 * 8.733710289001465
Epoch 1780, val loss: 1.064626693725586
Epoch 1790, training loss: 437.8616943359375 = 1.0637562274932861 + 50.0 * 8.73595905303955
Epoch 1790, val loss: 1.0645532608032227
Epoch 1800, training loss: 437.8592529296875 = 1.0636789798736572 + 50.0 * 8.73591136932373
Epoch 1800, val loss: 1.0644761323928833
Epoch 1810, training loss: 437.8122863769531 = 1.0636026859283447 + 50.0 * 8.734973907470703
Epoch 1810, val loss: 1.0643982887268066
Epoch 1820, training loss: 437.9380187988281 = 1.0635303258895874 + 50.0 * 8.737489700317383
Epoch 1820, val loss: 1.0643267631530762
Epoch 1830, training loss: 438.2030029296875 = 1.0634618997573853 + 50.0 * 8.742790222167969
Epoch 1830, val loss: 1.0642573833465576
Epoch 1840, training loss: 438.1661376953125 = 1.0633879899978638 + 50.0 * 8.74205493927002
Epoch 1840, val loss: 1.064178705215454
Epoch 1850, training loss: 438.0915832519531 = 1.0633100271224976 + 50.0 * 8.740565299987793
Epoch 1850, val loss: 1.064105749130249
Epoch 1860, training loss: 438.2928161621094 = 1.06324303150177 + 50.0 * 8.74459171295166
Epoch 1860, val loss: 1.064038634300232
Epoch 1870, training loss: 438.3154602050781 = 1.0631710290908813 + 50.0 * 8.74504566192627
Epoch 1870, val loss: 1.0639668703079224
Epoch 1880, training loss: 438.3691711425781 = 1.0630983114242554 + 50.0 * 8.746121406555176
Epoch 1880, val loss: 1.063893437385559
Epoch 1890, training loss: 438.26336669921875 = 1.0630265474319458 + 50.0 * 8.744007110595703
Epoch 1890, val loss: 1.0638213157653809
Epoch 1900, training loss: 438.4227294921875 = 1.0629490613937378 + 50.0 * 8.74719524383545
Epoch 1900, val loss: 1.063745141029358
Epoch 1910, training loss: 438.4512634277344 = 1.062886118888855 + 50.0 * 8.747767448425293
Epoch 1910, val loss: 1.0636824369430542
Epoch 1920, training loss: 438.567138671875 = 1.0628186464309692 + 50.0 * 8.750086784362793
Epoch 1920, val loss: 1.0636134147644043
Epoch 1930, training loss: 438.59014892578125 = 1.0627498626708984 + 50.0 * 8.750548362731934
Epoch 1930, val loss: 1.06354820728302
Epoch 1940, training loss: 438.7755432128906 = 1.062684416770935 + 50.0 * 8.754257202148438
Epoch 1940, val loss: 1.0634807348251343
Epoch 1950, training loss: 438.72149658203125 = 1.0626158714294434 + 50.0 * 8.753177642822266
Epoch 1950, val loss: 1.063413381576538
Epoch 1960, training loss: 438.7531433105469 = 1.062549352645874 + 50.0 * 8.753811836242676
Epoch 1960, val loss: 1.0633463859558105
Epoch 1970, training loss: 438.9905090332031 = 1.0624871253967285 + 50.0 * 8.758560180664062
Epoch 1970, val loss: 1.063281774520874
Epoch 1980, training loss: 439.0660400390625 = 1.0624207258224487 + 50.0 * 8.760072708129883
Epoch 1980, val loss: 1.0632156133651733
Epoch 1990, training loss: 439.1103210449219 = 1.0623542070388794 + 50.0 * 8.76095962524414
Epoch 1990, val loss: 1.063149333000183
Epoch 2000, training loss: 439.05108642578125 = 1.0622879266738892 + 50.0 * 8.75977611541748
Epoch 2000, val loss: 1.0630841255187988
Epoch 2010, training loss: 439.1623840332031 = 1.0622235536575317 + 50.0 * 8.762002944946289
Epoch 2010, val loss: 1.0630192756652832
Epoch 2020, training loss: 439.1080017089844 = 1.0621602535247803 + 50.0 * 8.760916709899902
Epoch 2020, val loss: 1.0629559755325317
Epoch 2030, training loss: 439.149658203125 = 1.062095284461975 + 50.0 * 8.761751174926758
Epoch 2030, val loss: 1.0628914833068848
Epoch 2040, training loss: 439.25811767578125 = 1.0620336532592773 + 50.0 * 8.763921737670898
Epoch 2040, val loss: 1.062828540802002
Epoch 2050, training loss: 439.3694152832031 = 1.0619723796844482 + 50.0 * 8.766148567199707
Epoch 2050, val loss: 1.0627660751342773
Epoch 2060, training loss: 438.9688720703125 = 1.061894178390503 + 50.0 * 8.758139610290527
Epoch 2060, val loss: 1.0626895427703857
Epoch 2070, training loss: 437.89794921875 = 1.0617989301681519 + 50.0 * 8.736722946166992
Epoch 2070, val loss: 1.0625967979431152
Epoch 2080, training loss: 438.0492248535156 = 1.0617479085922241 + 50.0 * 8.739749908447266
Epoch 2080, val loss: 1.0625394582748413
Epoch 2090, training loss: 438.4423522949219 = 1.0617002248764038 + 50.0 * 8.747612953186035
Epoch 2090, val loss: 1.0624936819076538
Epoch 2100, training loss: 438.7856750488281 = 1.0616483688354492 + 50.0 * 8.754480361938477
Epoch 2100, val loss: 1.0624430179595947
Epoch 2110, training loss: 439.066650390625 = 1.0615960359573364 + 50.0 * 8.760101318359375
Epoch 2110, val loss: 1.062391757965088
Epoch 2120, training loss: 439.2959289550781 = 1.0615428686141968 + 50.0 * 8.764687538146973
Epoch 2120, val loss: 1.0623371601104736
Epoch 2130, training loss: 439.4622497558594 = 1.061486840248108 + 50.0 * 8.768014907836914
Epoch 2130, val loss: 1.0622808933258057
Epoch 2140, training loss: 439.52197265625 = 1.0614278316497803 + 50.0 * 8.769210815429688
Epoch 2140, val loss: 1.0622222423553467
Epoch 2150, training loss: 439.6690673828125 = 1.0613723993301392 + 50.0 * 8.772153854370117
Epoch 2150, val loss: 1.0621650218963623
Epoch 2160, training loss: 439.9063720703125 = 1.061316728591919 + 50.0 * 8.776901245117188
Epoch 2160, val loss: 1.062110424041748
Epoch 2170, training loss: 439.8330078125 = 1.0612587928771973 + 50.0 * 8.775435447692871
Epoch 2170, val loss: 1.062052607536316
Epoch 2180, training loss: 439.8765869140625 = 1.0612027645111084 + 50.0 * 8.776308059692383
Epoch 2180, val loss: 1.0619956254959106
Epoch 2190, training loss: 439.94091796875 = 1.0611467361450195 + 50.0 * 8.777595520019531
Epoch 2190, val loss: 1.0619406700134277
Epoch 2200, training loss: 439.8825988769531 = 1.0610904693603516 + 50.0 * 8.776430130004883
Epoch 2200, val loss: 1.061885952949524
Epoch 2210, training loss: 440.0665588378906 = 1.0610376596450806 + 50.0 * 8.780110359191895
Epoch 2210, val loss: 1.0618314743041992
Epoch 2220, training loss: 440.1501770019531 = 1.0609838962554932 + 50.0 * 8.781784057617188
Epoch 2220, val loss: 1.061777114868164
Epoch 2230, training loss: 440.1028137207031 = 1.0609281063079834 + 50.0 * 8.780838012695312
Epoch 2230, val loss: 1.0617213249206543
Epoch 2240, training loss: 440.369384765625 = 1.060876488685608 + 50.0 * 8.78617000579834
Epoch 2240, val loss: 1.0616580247879028
Epoch 2250, training loss: 439.9490966796875 = 1.0608099699020386 + 50.0 * 8.777765274047852
Epoch 2250, val loss: 1.0615917444229126
Epoch 2260, training loss: 439.467529296875 = 1.060743808746338 + 50.0 * 8.768136024475098
Epoch 2260, val loss: 1.061541199684143
Epoch 2270, training loss: 439.3484191894531 = 1.0606963634490967 + 50.0 * 8.765754699707031
Epoch 2270, val loss: 1.061495304107666
Epoch 2280, training loss: 439.5414123535156 = 1.060649037361145 + 50.0 * 8.769615173339844
Epoch 2280, val loss: 1.0614466667175293
Epoch 2290, training loss: 439.9361877441406 = 1.0606063604354858 + 50.0 * 8.777511596679688
Epoch 2290, val loss: 1.0614012479782104
Epoch 2300, training loss: 440.1763916015625 = 1.0605612993240356 + 50.0 * 8.782317161560059
Epoch 2300, val loss: 1.0613555908203125
Epoch 2310, training loss: 440.3329772949219 = 1.0605134963989258 + 50.0 * 8.785449028015137
Epoch 2310, val loss: 1.0613071918487549
Epoch 2320, training loss: 440.3204650878906 = 1.060461163520813 + 50.0 * 8.785200119018555
Epoch 2320, val loss: 1.0612553358078003
Epoch 2330, training loss: 440.36578369140625 = 1.0604147911071777 + 50.0 * 8.786107063293457
Epoch 2330, val loss: 1.061208724975586
Epoch 2340, training loss: 440.58416748046875 = 1.0603681802749634 + 50.0 * 8.790475845336914
Epoch 2340, val loss: 1.0611616373062134
Epoch 2350, training loss: 440.590576171875 = 1.0603175163269043 + 50.0 * 8.790605545043945
Epoch 2350, val loss: 1.0611101388931274
Epoch 2360, training loss: 440.7564392089844 = 1.0602703094482422 + 50.0 * 8.793923377990723
Epoch 2360, val loss: 1.061063528060913
Epoch 2370, training loss: 440.8931884765625 = 1.0602235794067383 + 50.0 * 8.796659469604492
Epoch 2370, val loss: 1.0610159635543823
Epoch 2380, training loss: 440.8192443847656 = 1.060174584388733 + 50.0 * 8.795181274414062
Epoch 2380, val loss: 1.060965895652771
Epoch 2390, training loss: 440.9320983886719 = 1.0601270198822021 + 50.0 * 8.797439575195312
Epoch 2390, val loss: 1.0609186887741089
Epoch 2400, training loss: 440.96673583984375 = 1.0600802898406982 + 50.0 * 8.79813289642334
Epoch 2400, val loss: 1.06087327003479
Epoch 2410, training loss: 440.9244384765625 = 1.0600343942642212 + 50.0 * 8.797287940979004
Epoch 2410, val loss: 1.0608201026916504
Epoch 2420, training loss: 440.7567138671875 = 1.0599796772003174 + 50.0 * 8.79393482208252
Epoch 2420, val loss: 1.0607744455337524
Epoch 2430, training loss: 440.97576904296875 = 1.0599379539489746 + 50.0 * 8.798316955566406
Epoch 2430, val loss: 1.0607304573059082
Epoch 2440, training loss: 441.21783447265625 = 1.0598969459533691 + 50.0 * 8.8031587600708
Epoch 2440, val loss: 1.0606889724731445
Epoch 2450, training loss: 441.0751953125 = 1.059848427772522 + 50.0 * 8.800307273864746
Epoch 2450, val loss: 1.060640573501587
Epoch 2460, training loss: 441.0737609863281 = 1.0598037242889404 + 50.0 * 8.800278663635254
Epoch 2460, val loss: 1.0605942010879517
Epoch 2470, training loss: 441.1843566894531 = 1.059759259223938 + 50.0 * 8.802492141723633
Epoch 2470, val loss: 1.060551404953003
Epoch 2480, training loss: 441.2812194824219 = 1.0597182512283325 + 50.0 * 8.80443000793457
Epoch 2480, val loss: 1.0605099201202393
Epoch 2490, training loss: 441.4994201660156 = 1.059677004814148 + 50.0 * 8.808794975280762
Epoch 2490, val loss: 1.0604664087295532
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.39666666666666667
0.861624284575817
The final CL Acc:0.39696, 0.00041, The final GNN Acc:0.86307, 0.00111
Begin epxeriment: noisy_level: 0.2 cont_weight: 50 epoch: der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106582])
remove edge: torch.Size([2, 71010])
updated graph: torch.Size([2, 88944])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:227: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 510.64190673828125 = 1.102038860321045 + 50.0 * 10.190796852111816
Epoch 0, val loss: 1.102039098739624
Epoch 10, training loss: 490.9835510253906 = 1.101423740386963 + 50.0 * 9.797642707824707
Epoch 10, val loss: 1.1014103889465332
Epoch 20, training loss: 483.0663757324219 = 1.1007941961288452 + 50.0 * 9.639311790466309
Epoch 20, val loss: 1.1007863283157349
Epoch 30, training loss: 476.8990173339844 = 1.1002297401428223 + 50.0 * 9.515975952148438
Epoch 30, val loss: 1.1002156734466553
Epoch 40, training loss: 471.9665832519531 = 1.099623203277588 + 50.0 * 9.417339324951172
Epoch 40, val loss: 1.0996103286743164
Epoch 50, training loss: 467.76959228515625 = 1.0990307331085205 + 50.0 * 9.33341121673584
Epoch 50, val loss: 1.0990190505981445
Epoch 60, training loss: 464.13604736328125 = 1.0984293222427368 + 50.0 * 9.26075267791748
Epoch 60, val loss: 1.0984190702438354
Epoch 70, training loss: 460.9411926269531 = 1.097813606262207 + 50.0 * 9.196867942810059
Epoch 70, val loss: 1.0978049039840698
Epoch 80, training loss: 458.1513366699219 = 1.097184658050537 + 50.0 * 9.141082763671875
Epoch 80, val loss: 1.0971813201904297
Epoch 90, training loss: 455.72088623046875 = 1.096550703048706 + 50.0 * 9.092486381530762
Epoch 90, val loss: 1.0965518951416016
Epoch 100, training loss: 453.6363220214844 = 1.095903992652893 + 50.0 * 9.05080795288086
Epoch 100, val loss: 1.0959110260009766
Epoch 110, training loss: 451.8124084472656 = 1.0952463150024414 + 50.0 * 9.01434326171875
Epoch 110, val loss: 1.0952584743499756
Epoch 120, training loss: 450.35699462890625 = 1.0945773124694824 + 50.0 * 8.985248565673828
Epoch 120, val loss: 1.0945954322814941
Epoch 130, training loss: 448.9978332519531 = 1.0939161777496338 + 50.0 * 8.958078384399414
Epoch 130, val loss: 1.093941569328308
Epoch 140, training loss: 447.90765380859375 = 1.0932618379592896 + 50.0 * 8.936287879943848
Epoch 140, val loss: 1.0932872295379639
Epoch 150, training loss: 446.8938903808594 = 1.092577338218689 + 50.0 * 8.91602611541748
Epoch 150, val loss: 1.0926015377044678
Epoch 160, training loss: 445.9884033203125 = 1.091871738433838 + 50.0 * 8.897931098937988
Epoch 160, val loss: 1.0919065475463867
Epoch 170, training loss: 445.1685791015625 = 1.0911754369735718 + 50.0 * 8.881547927856445
Epoch 170, val loss: 1.0912237167358398
Epoch 180, training loss: 444.41314697265625 = 1.0904648303985596 + 50.0 * 8.866454124450684
Epoch 180, val loss: 1.0905213356018066
Epoch 190, training loss: 443.8780212402344 = 1.0897417068481445 + 50.0 * 8.855765342712402
Epoch 190, val loss: 1.0898146629333496
Epoch 200, training loss: 443.32354736328125 = 1.0890215635299683 + 50.0 * 8.844690322875977
Epoch 200, val loss: 1.0890969038009644
Epoch 210, training loss: 442.9258728027344 = 1.0882896184921265 + 50.0 * 8.836751937866211
Epoch 210, val loss: 1.0883725881576538
Epoch 220, training loss: 442.8318176269531 = 1.0875258445739746 + 50.0 * 8.834885597229004
Epoch 220, val loss: 1.087618112564087
Epoch 230, training loss: 442.48883056640625 = 1.0867841243743896 + 50.0 * 8.828041076660156
Epoch 230, val loss: 1.0868899822235107
Epoch 240, training loss: 441.86651611328125 = 1.0859671831130981 + 50.0 * 8.815610885620117
Epoch 240, val loss: 1.0860921144485474
Epoch 250, training loss: 441.5874938964844 = 1.0851750373840332 + 50.0 * 8.810046195983887
Epoch 250, val loss: 1.085315465927124
Epoch 260, training loss: 441.412353515625 = 1.0843706130981445 + 50.0 * 8.806559562683105
Epoch 260, val loss: 1.0845190286636353
Epoch 270, training loss: 441.2720947265625 = 1.083580732345581 + 50.0 * 8.803770065307617
Epoch 270, val loss: 1.0837384462356567
Epoch 280, training loss: 441.02227783203125 = 1.0827417373657227 + 50.0 * 8.79879093170166
Epoch 280, val loss: 1.0829229354858398
Epoch 290, training loss: 440.75494384765625 = 1.0819182395935059 + 50.0 * 8.793460845947266
Epoch 290, val loss: 1.0821176767349243
Epoch 300, training loss: 440.63238525390625 = 1.081071376800537 + 50.0 * 8.79102611541748
Epoch 300, val loss: 1.081299901008606
Epoch 310, training loss: 440.8410339355469 = 1.0802136659622192 + 50.0 * 8.79521656036377
Epoch 310, val loss: 1.0804673433303833
Epoch 320, training loss: 440.66253662109375 = 1.07938551902771 + 50.0 * 8.79166316986084
Epoch 320, val loss: 1.079654574394226
Epoch 330, training loss: 440.5787353515625 = 1.0785541534423828 + 50.0 * 8.790003776550293
Epoch 330, val loss: 1.0788547992706299
Epoch 340, training loss: 440.4816589355469 = 1.077720046043396 + 50.0 * 8.788078308105469
Epoch 340, val loss: 1.0780417919158936
Epoch 350, training loss: 440.4759216308594 = 1.076889157295227 + 50.0 * 8.787980079650879
Epoch 350, val loss: 1.077243685722351
Epoch 360, training loss: 440.4775085449219 = 1.0760672092437744 + 50.0 * 8.788028717041016
Epoch 360, val loss: 1.076459527015686
Epoch 370, training loss: 440.7221984863281 = 1.075333595275879 + 50.0 * 8.792937278747559
Epoch 370, val loss: 1.0757558345794678
Epoch 380, training loss: 440.41741943359375 = 1.074518084526062 + 50.0 * 8.786857604980469
Epoch 380, val loss: 1.0749890804290771
Epoch 390, training loss: 440.4496154785156 = 1.073777437210083 + 50.0 * 8.787516593933105
Epoch 390, val loss: 1.0742758512496948
Epoch 400, training loss: 440.5983581542969 = 1.0730316638946533 + 50.0 * 8.790506362915039
Epoch 400, val loss: 1.073565125465393
Epoch 410, training loss: 440.2312316894531 = 1.072325587272644 + 50.0 * 8.783178329467773
Epoch 410, val loss: 1.0728808641433716
Epoch 420, training loss: 440.41583251953125 = 1.07163667678833 + 50.0 * 8.786884307861328
Epoch 420, val loss: 1.0722500085830688
Epoch 430, training loss: 440.4013671875 = 1.0709370374679565 + 50.0 * 8.786608695983887
Epoch 430, val loss: 1.071587324142456
Epoch 440, training loss: 440.4901428222656 = 1.0702550411224365 + 50.0 * 8.788397789001465
Epoch 440, val loss: 1.0709363222122192
Epoch 450, training loss: 440.5053405761719 = 1.0695221424102783 + 50.0 * 8.788716316223145
Epoch 450, val loss: 1.0702509880065918
Epoch 460, training loss: 440.6839599609375 = 1.0688055753707886 + 50.0 * 8.792303085327148
Epoch 460, val loss: 1.0695674419403076
Epoch 470, training loss: 441.184326171875 = 1.0680426359176636 + 50.0 * 8.802325248718262
Epoch 470, val loss: 1.0688560009002686
Epoch 480, training loss: 440.9122009277344 = 1.0673400163650513 + 50.0 * 8.796896934509277
Epoch 480, val loss: 1.0681856870651245
Epoch 490, training loss: 440.8519592285156 = 1.0665693283081055 + 50.0 * 8.795707702636719
Epoch 490, val loss: 1.0674725770950317
Epoch 500, training loss: 440.9330139160156 = 1.06584632396698 + 50.0 * 8.797343254089355
Epoch 500, val loss: 1.0667887926101685
Epoch 510, training loss: 441.10784912109375 = 1.065068006515503 + 50.0 * 8.80085563659668
Epoch 510, val loss: 1.066051721572876
Epoch 520, training loss: 441.0261535644531 = 1.0643290281295776 + 50.0 * 8.799236297607422
Epoch 520, val loss: 1.0653564929962158
Epoch 530, training loss: 440.79656982421875 = 1.0635143518447876 + 50.0 * 8.794661521911621
Epoch 530, val loss: 1.0645880699157715
Epoch 540, training loss: 441.1093444824219 = 1.0627528429031372 + 50.0 * 8.800931930541992
Epoch 540, val loss: 1.0638669729232788
Epoch 550, training loss: 441.01800537109375 = 1.0619208812713623 + 50.0 * 8.799121856689453
Epoch 550, val loss: 1.0631005764007568
Epoch 560, training loss: 441.26068115234375 = 1.0611239671707153 + 50.0 * 8.803991317749023
Epoch 560, val loss: 1.0623447895050049
Epoch 570, training loss: 441.3614501953125 = 1.060299038887024 + 50.0 * 8.806022644042969
Epoch 570, val loss: 1.061571478843689
Epoch 580, training loss: 441.3219909667969 = 1.0594362020492554 + 50.0 * 8.805251121520996
Epoch 580, val loss: 1.0607560873031616
Epoch 590, training loss: 441.5845642089844 = 1.0585756301879883 + 50.0 * 8.81052017211914
Epoch 590, val loss: 1.0599439144134521
Epoch 600, training loss: 441.47882080078125 = 1.0577110052108765 + 50.0 * 8.808422088623047
Epoch 600, val loss: 1.0591552257537842
Epoch 610, training loss: 441.6636962890625 = 1.0568122863769531 + 50.0 * 8.812137603759766
Epoch 610, val loss: 1.0583184957504272
Epoch 620, training loss: 441.8409423828125 = 1.055914044380188 + 50.0 * 8.81570053100586
Epoch 620, val loss: 1.057469129562378
Epoch 630, training loss: 441.91796875 = 1.0549838542938232 + 50.0 * 8.817259788513184
Epoch 630, val loss: 1.056605339050293
Epoch 640, training loss: 442.12689208984375 = 1.0540565252304077 + 50.0 * 8.821456909179688
Epoch 640, val loss: 1.0557429790496826
Epoch 650, training loss: 442.1099548339844 = 1.0531315803527832 + 50.0 * 8.821136474609375
Epoch 650, val loss: 1.0548672676086426
Epoch 660, training loss: 442.24298095703125 = 1.052168607711792 + 50.0 * 8.823816299438477
Epoch 660, val loss: 1.0539653301239014
Epoch 670, training loss: 442.4140319824219 = 1.0512150526046753 + 50.0 * 8.827256202697754
Epoch 670, val loss: 1.0530731678009033
Epoch 680, training loss: 442.3108215332031 = 1.0502208471298218 + 50.0 * 8.825211524963379
Epoch 680, val loss: 1.052137017250061
Epoch 690, training loss: 442.2245788574219 = 1.0492165088653564 + 50.0 * 8.823507308959961
Epoch 690, val loss: 1.051213026046753
Epoch 700, training loss: 442.7598876953125 = 1.0482069253921509 + 50.0 * 8.834233283996582
Epoch 700, val loss: 1.0502172708511353
Epoch 710, training loss: 443.14752197265625 = 1.0471566915512085 + 50.0 * 8.842007637023926
Epoch 710, val loss: 1.0492857694625854
Epoch 720, training loss: 442.626708984375 = 1.046137809753418 + 50.0 * 8.831611633300781
Epoch 720, val loss: 1.0483404397964478
Epoch 730, training loss: 442.8218994140625 = 1.0450900793075562 + 50.0 * 8.835536003112793
Epoch 730, val loss: 1.0473682880401611
Epoch 740, training loss: 443.0508728027344 = 1.0440293550491333 + 50.0 * 8.840136528015137
Epoch 740, val loss: 1.0463733673095703
Epoch 750, training loss: 443.1849060058594 = 1.0429255962371826 + 50.0 * 8.842839241027832
Epoch 750, val loss: 1.0453511476516724
Epoch 760, training loss: 443.1708679199219 = 1.0418248176574707 + 50.0 * 8.842580795288086
Epoch 760, val loss: 1.044319987297058
Epoch 770, training loss: 443.5465087890625 = 1.0406694412231445 + 50.0 * 8.850116729736328
Epoch 770, val loss: 1.043284296989441
Epoch 780, training loss: 443.1135559082031 = 1.039570689201355 + 50.0 * 8.841479301452637
Epoch 780, val loss: 1.0422120094299316
Epoch 790, training loss: 443.338134765625 = 1.038427710533142 + 50.0 * 8.845993995666504
Epoch 790, val loss: 1.0411624908447266
Epoch 800, training loss: 443.4129638671875 = 1.037294626235962 + 50.0 * 8.847513198852539
Epoch 800, val loss: 1.0401047468185425
Epoch 810, training loss: 443.42095947265625 = 1.0362050533294678 + 50.0 * 8.847695350646973
Epoch 810, val loss: 1.0390681028366089
Epoch 820, training loss: 443.74078369140625 = 1.0350584983825684 + 50.0 * 8.854114532470703
Epoch 820, val loss: 1.0379983186721802
Epoch 830, training loss: 443.853271484375 = 1.0339181423187256 + 50.0 * 8.8563871383667
Epoch 830, val loss: 1.0369226932525635
Epoch 840, training loss: 443.88433837890625 = 1.0327600240707397 + 50.0 * 8.85703182220459
Epoch 840, val loss: 1.035843014717102
Epoch 850, training loss: 443.99798583984375 = 1.0315752029418945 + 50.0 * 8.859328269958496
Epoch 850, val loss: 1.0347435474395752
Epoch 860, training loss: 443.9806213378906 = 1.030370831489563 + 50.0 * 8.859004974365234
Epoch 860, val loss: 1.0335941314697266
Epoch 870, training loss: 444.0440673828125 = 1.0291714668273926 + 50.0 * 8.860298156738281
Epoch 870, val loss: 1.0324933528900146
Epoch 880, training loss: 444.31964111328125 = 1.0279958248138428 + 50.0 * 8.865833282470703
Epoch 880, val loss: 1.031409502029419
Epoch 890, training loss: 444.2597961425781 = 1.026741623878479 + 50.0 * 8.86466121673584
Epoch 890, val loss: 1.0302261114120483
Epoch 900, training loss: 444.501953125 = 1.0255179405212402 + 50.0 * 8.869528770446777
Epoch 900, val loss: 1.0290886163711548
Epoch 910, training loss: 444.6917419433594 = 1.0243165493011475 + 50.0 * 8.873348236083984
Epoch 910, val loss: 1.0279549360275269
Epoch 920, training loss: 444.7273864746094 = 1.023064374923706 + 50.0 * 8.874086380004883
Epoch 920, val loss: 1.0267727375030518
Epoch 930, training loss: 444.75421142578125 = 1.0217729806900024 + 50.0 * 8.874649047851562
Epoch 930, val loss: 1.025567889213562
Epoch 940, training loss: 445.06494140625 = 1.0205230712890625 + 50.0 * 8.880887985229492
Epoch 940, val loss: 1.0244090557098389
Epoch 950, training loss: 444.8305358886719 = 1.0192127227783203 + 50.0 * 8.876226425170898
Epoch 950, val loss: 1.0231627225875854
Epoch 960, training loss: 444.66558837890625 = 1.017837405204773 + 50.0 * 8.872955322265625
Epoch 960, val loss: 1.0219074487686157
Epoch 970, training loss: 444.7222900390625 = 1.0166114568710327 + 50.0 * 8.874114036560059
Epoch 970, val loss: 1.0207265615463257
Epoch 980, training loss: 445.14990234375 = 1.0153332948684692 + 50.0 * 8.882691383361816
Epoch 980, val loss: 1.0195170640945435
Epoch 990, training loss: 445.1800537109375 = 1.0139905214309692 + 50.0 * 8.883321762084961
Epoch 990, val loss: 1.018267273902893
Epoch 1000, training loss: 445.3424377441406 = 1.0126599073410034 + 50.0 * 8.886595726013184
Epoch 1000, val loss: 1.0170115232467651
Epoch 1010, training loss: 445.56622314453125 = 1.0113089084625244 + 50.0 * 8.891098022460938
Epoch 1010, val loss: 1.015743613243103
Epoch 1020, training loss: 445.6771545410156 = 1.0099543333053589 + 50.0 * 8.893343925476074
Epoch 1020, val loss: 1.01441490650177
Epoch 1030, training loss: 445.5102233886719 = 1.0085461139678955 + 50.0 * 8.890033721923828
Epoch 1030, val loss: 1.0131337642669678
Epoch 1040, training loss: 445.8362121582031 = 1.0071721076965332 + 50.0 * 8.896580696105957
Epoch 1040, val loss: 1.011855125427246
Epoch 1050, training loss: 445.8371887207031 = 1.0057587623596191 + 50.0 * 8.896628379821777
Epoch 1050, val loss: 1.0105085372924805
Epoch 1060, training loss: 445.8453369140625 = 1.0043318271636963 + 50.0 * 8.896820068359375
Epoch 1060, val loss: 1.0091729164123535
Epoch 1070, training loss: 445.83233642578125 = 1.002871036529541 + 50.0 * 8.896589279174805
Epoch 1070, val loss: 1.0078144073486328
Epoch 1080, training loss: 446.0143127441406 = 1.0014503002166748 + 50.0 * 8.900257110595703
Epoch 1080, val loss: 1.0064773559570312
Epoch 1090, training loss: 446.2408447265625 = 1.0000114440917969 + 50.0 * 8.904816627502441
Epoch 1090, val loss: 1.005125880241394
Epoch 1100, training loss: 446.15972900390625 = 0.9985424876213074 + 50.0 * 8.903223991394043
Epoch 1100, val loss: 1.0037031173706055
Epoch 1110, training loss: 446.15802001953125 = 0.9970159530639648 + 50.0 * 8.903220176696777
Epoch 1110, val loss: 1.0023044347763062
Epoch 1120, training loss: 446.4357604980469 = 0.9955684542655945 + 50.0 * 8.908803939819336
Epoch 1120, val loss: 1.0009363889694214
Epoch 1130, training loss: 446.7159423828125 = 0.9940931797027588 + 50.0 * 8.914437294006348
Epoch 1130, val loss: 0.9995597004890442
Epoch 1140, training loss: 446.766845703125 = 0.9925702214241028 + 50.0 * 8.915485382080078
Epoch 1140, val loss: 0.9981284141540527
Epoch 1150, training loss: 446.7757568359375 = 0.9910528659820557 + 50.0 * 8.915694236755371
Epoch 1150, val loss: 0.9966945648193359
Epoch 1160, training loss: 446.8663635253906 = 0.9894822835922241 + 50.0 * 8.917537689208984
Epoch 1160, val loss: 0.9951932430267334
Epoch 1170, training loss: 446.89959716796875 = 0.9880312085151672 + 50.0 * 8.918231010437012
Epoch 1170, val loss: 0.993890643119812
Epoch 1180, training loss: 446.6981506347656 = 0.9864740371704102 + 50.0 * 8.914233207702637
Epoch 1180, val loss: 0.9923917055130005
Epoch 1190, training loss: 446.9427185058594 = 0.9849210977554321 + 50.0 * 8.919156074523926
Epoch 1190, val loss: 0.9909430742263794
Epoch 1200, training loss: 447.2048034667969 = 0.9833601713180542 + 50.0 * 8.924428939819336
Epoch 1200, val loss: 0.9894607067108154
Epoch 1210, training loss: 447.2356262207031 = 0.9817618131637573 + 50.0 * 8.925077438354492
Epoch 1210, val loss: 0.987960934638977
Epoch 1220, training loss: 447.3942565917969 = 0.9801738858222961 + 50.0 * 8.928281784057617
Epoch 1220, val loss: 0.9864707589149475
Epoch 1230, training loss: 447.61798095703125 = 0.9785645008087158 + 50.0 * 8.932787895202637
Epoch 1230, val loss: 0.9849503636360168
Epoch 1240, training loss: 447.2882080078125 = 0.9768607020378113 + 50.0 * 8.926226615905762
Epoch 1240, val loss: 0.9833672046661377
Epoch 1250, training loss: 447.4998474121094 = 0.9752820134162903 + 50.0 * 8.93049144744873
Epoch 1250, val loss: 0.9818886518478394
Epoch 1260, training loss: 447.788818359375 = 0.9736504554748535 + 50.0 * 8.93630313873291
Epoch 1260, val loss: 0.9803636074066162
Epoch 1270, training loss: 447.9253234863281 = 0.9719724059104919 + 50.0 * 8.939066886901855
Epoch 1270, val loss: 0.9787949919700623
Epoch 1280, training loss: 447.7663269042969 = 0.9702972173690796 + 50.0 * 8.935920715332031
Epoch 1280, val loss: 0.9772518277168274
Epoch 1290, training loss: 447.814453125 = 0.968593180179596 + 50.0 * 8.936917304992676
Epoch 1290, val loss: 0.9756725430488586
Epoch 1300, training loss: 447.85028076171875 = 0.9669402241706848 + 50.0 * 8.937666893005371
Epoch 1300, val loss: 0.9741102457046509
Epoch 1310, training loss: 448.2383728027344 = 0.9652569890022278 + 50.0 * 8.945462226867676
Epoch 1310, val loss: 0.9725617170333862
Epoch 1320, training loss: 448.1143493652344 = 0.9635339975357056 + 50.0 * 8.943016052246094
Epoch 1320, val loss: 0.9709467887878418
Epoch 1330, training loss: 448.7021179199219 = 0.9617615342140198 + 50.0 * 8.95480728149414
Epoch 1330, val loss: 0.9692768454551697
Epoch 1340, training loss: 448.2738342285156 = 0.9600502252578735 + 50.0 * 8.94627571105957
Epoch 1340, val loss: 0.9676825404167175
Epoch 1350, training loss: 448.5377502441406 = 0.9583242535591125 + 50.0 * 8.95158863067627
Epoch 1350, val loss: 0.9660751223564148
Epoch 1360, training loss: 448.69384765625 = 0.9566253423690796 + 50.0 * 8.954744338989258
Epoch 1360, val loss: 0.9644900560379028
Epoch 1370, training loss: 448.6601257324219 = 0.9548739194869995 + 50.0 * 8.954105377197266
Epoch 1370, val loss: 0.9628867506980896
Epoch 1380, training loss: 448.6761474609375 = 0.9531108736991882 + 50.0 * 8.954460144042969
Epoch 1380, val loss: 0.9611867070198059
Epoch 1390, training loss: 448.868896484375 = 0.9513754844665527 + 50.0 * 8.95835018157959
Epoch 1390, val loss: 0.9595852494239807
Epoch 1400, training loss: 449.12213134765625 = 0.9496217966079712 + 50.0 * 8.96345043182373
Epoch 1400, val loss: 0.9579715728759766
Epoch 1410, training loss: 449.1071472167969 = 0.9478347301483154 + 50.0 * 8.963186264038086
Epoch 1410, val loss: 0.9562892913818359
Epoch 1420, training loss: 449.1448974609375 = 0.9460186958312988 + 50.0 * 8.963977813720703
Epoch 1420, val loss: 0.9546354413032532
Epoch 1430, training loss: 448.84576416015625 = 0.94427090883255 + 50.0 * 8.958029747009277
Epoch 1430, val loss: 0.9530150294303894
Epoch 1440, training loss: 449.103515625 = 0.9424571990966797 + 50.0 * 8.963221549987793
Epoch 1440, val loss: 0.9513327479362488
Epoch 1450, training loss: 449.1368408203125 = 0.9406700730323792 + 50.0 * 8.963923454284668
Epoch 1450, val loss: 0.9496239423751831
Epoch 1460, training loss: 449.1997985839844 = 0.9387862682342529 + 50.0 * 8.96522045135498
Epoch 1460, val loss: 0.9479407668113708
Epoch 1470, training loss: 449.1716003417969 = 0.9369495511054993 + 50.0 * 8.964693069458008
Epoch 1470, val loss: 0.9462279081344604
Epoch 1480, training loss: 449.413330078125 = 0.9351133704185486 + 50.0 * 8.969564437866211
Epoch 1480, val loss: 0.944537341594696
Epoch 1490, training loss: 449.4358825683594 = 0.9332565665245056 + 50.0 * 8.970052719116211
Epoch 1490, val loss: 0.9428008198738098
Epoch 1500, training loss: 449.3224182128906 = 0.931380033493042 + 50.0 * 8.96782112121582
Epoch 1500, val loss: 0.9410848021507263
Epoch 1510, training loss: 447.6870422363281 = 0.9293562769889832 + 50.0 * 8.93515396118164
Epoch 1510, val loss: 0.9392269253730774
Epoch 1520, training loss: 448.52008056640625 = 0.9276775121688843 + 50.0 * 8.951848030090332
Epoch 1520, val loss: 0.937673807144165
Epoch 1530, training loss: 449.113037109375 = 0.9259526133537292 + 50.0 * 8.963741302490234
Epoch 1530, val loss: 0.9361059665679932
Epoch 1540, training loss: 449.06195068359375 = 0.9241176843643188 + 50.0 * 8.962757110595703
Epoch 1540, val loss: 0.9344111680984497
Epoch 1550, training loss: 449.2861022949219 = 0.9222725629806519 + 50.0 * 8.967276573181152
Epoch 1550, val loss: 0.9327040314674377
Epoch 1560, training loss: 449.6372375488281 = 0.9204087853431702 + 50.0 * 8.974336624145508
Epoch 1560, val loss: 0.9310021996498108
Epoch 1570, training loss: 449.4263610839844 = 0.9184779524803162 + 50.0 * 8.970157623291016
Epoch 1570, val loss: 0.9292100667953491
Epoch 1580, training loss: 449.7363586425781 = 0.9166162014007568 + 50.0 * 8.976394653320312
Epoch 1580, val loss: 0.9275155663490295
Epoch 1590, training loss: 449.8677978515625 = 0.9147115349769592 + 50.0 * 8.9790620803833
Epoch 1590, val loss: 0.9257632493972778
Epoch 1600, training loss: 449.0864562988281 = 0.9127562642097473 + 50.0 * 8.96347427368164
Epoch 1600, val loss: 0.923997163772583
Epoch 1610, training loss: 449.6061096191406 = 0.9108301997184753 + 50.0 * 8.973905563354492
Epoch 1610, val loss: 0.9221847057342529
Epoch 1620, training loss: 449.7412414550781 = 0.9088626503944397 + 50.0 * 8.97664737701416
Epoch 1620, val loss: 0.9204114675521851
Epoch 1630, training loss: 450.1954650878906 = 0.9070081114768982 + 50.0 * 8.985769271850586
Epoch 1630, val loss: 0.9187150001525879
Epoch 1640, training loss: 450.3586120605469 = 0.9050675630569458 + 50.0 * 8.989070892333984
Epoch 1640, val loss: 0.9169759154319763
Epoch 1650, training loss: 450.2714538574219 = 0.9033242464065552 + 50.0 * 8.9873628616333
Epoch 1650, val loss: 0.9154370427131653
Epoch 1660, training loss: 450.39691162109375 = 0.9016345739364624 + 50.0 * 8.98990535736084
Epoch 1660, val loss: 0.9139224886894226
Epoch 1670, training loss: 450.54864501953125 = 0.8997728228569031 + 50.0 * 8.992977142333984
Epoch 1670, val loss: 0.9122347235679626
Epoch 1680, training loss: 450.46905517578125 = 0.897814929485321 + 50.0 * 8.991424560546875
Epoch 1680, val loss: 0.9104205369949341
Epoch 1690, training loss: 450.7174377441406 = 0.8958600163459778 + 50.0 * 8.996431350708008
Epoch 1690, val loss: 0.9086595177650452
Epoch 1700, training loss: 450.99554443359375 = 0.8939136266708374 + 50.0 * 9.002032279968262
Epoch 1700, val loss: 0.9068640470504761
Epoch 1710, training loss: 450.8681640625 = 0.8918938636779785 + 50.0 * 8.99952507019043
Epoch 1710, val loss: 0.9050393104553223
Epoch 1720, training loss: 450.89727783203125 = 0.8899350762367249 + 50.0 * 9.000146865844727
Epoch 1720, val loss: 0.9032948017120361
Epoch 1730, training loss: 450.88153076171875 = 0.8879904747009277 + 50.0 * 8.999870300292969
Epoch 1730, val loss: 0.9015174508094788
Epoch 1740, training loss: 451.0 = 0.885918378829956 + 50.0 * 9.002281188964844
Epoch 1740, val loss: 0.899645209312439
Epoch 1750, training loss: 451.1519775390625 = 0.8839567303657532 + 50.0 * 9.00536060333252
Epoch 1750, val loss: 0.8978701829910278
Epoch 1760, training loss: 451.3828125 = 0.8819940686225891 + 50.0 * 9.010016441345215
Epoch 1760, val loss: 0.8960774540901184
Epoch 1770, training loss: 451.3919982910156 = 0.8800056576728821 + 50.0 * 9.010239601135254
Epoch 1770, val loss: 0.8942782282829285
Epoch 1780, training loss: 451.2061767578125 = 0.8780252933502197 + 50.0 * 9.006563186645508
Epoch 1780, val loss: 0.8925069570541382
Epoch 1790, training loss: 451.4591064453125 = 0.876059353351593 + 50.0 * 9.0116605758667
Epoch 1790, val loss: 0.8906978368759155
Epoch 1800, training loss: 451.51287841796875 = 0.8740478754043579 + 50.0 * 9.012776374816895
Epoch 1800, val loss: 0.8889174461364746
Epoch 1810, training loss: 451.5772399902344 = 0.8720871210098267 + 50.0 * 9.014102935791016
Epoch 1810, val loss: 0.8871253132820129
Epoch 1820, training loss: 451.7645568847656 = 0.8701221346855164 + 50.0 * 9.017889022827148
Epoch 1820, val loss: 0.8853268623352051
Epoch 1830, training loss: 451.7812805175781 = 0.8681190013885498 + 50.0 * 9.01826286315918
Epoch 1830, val loss: 0.8835192918777466
Epoch 1840, training loss: 451.64801025390625 = 0.8660809993743896 + 50.0 * 9.01563835144043
Epoch 1840, val loss: 0.8816707134246826
Epoch 1850, training loss: 451.8117370605469 = 0.8641179800033569 + 50.0 * 9.018952369689941
Epoch 1850, val loss: 0.8798927068710327
Epoch 1860, training loss: 451.9365234375 = 0.8621647357940674 + 50.0 * 9.02148723602295
Epoch 1860, val loss: 0.8781344890594482
Epoch 1870, training loss: 452.0837707519531 = 0.8601990342140198 + 50.0 * 9.024471282958984
Epoch 1870, val loss: 0.8763772249221802
Epoch 1880, training loss: 450.648681640625 = 0.8580614924430847 + 50.0 * 8.99581241607666
Epoch 1880, val loss: 0.874453604221344
Epoch 1890, training loss: 450.8379211425781 = 0.8562267422676086 + 50.0 * 8.9996337890625
Epoch 1890, val loss: 0.8727316856384277
Epoch 1900, training loss: 451.0498352050781 = 0.8542442917823792 + 50.0 * 9.003911972045898
Epoch 1900, val loss: 0.8709372878074646
Epoch 1910, training loss: 451.2171936035156 = 0.8523653149604797 + 50.0 * 9.007296562194824
Epoch 1910, val loss: 0.8692217469215393
Epoch 1920, training loss: 451.7546691894531 = 0.8503957986831665 + 50.0 * 9.018085479736328
Epoch 1920, val loss: 0.8674684762954712
Epoch 1930, training loss: 452.1286315917969 = 0.8485044240951538 + 50.0 * 9.025602340698242
Epoch 1930, val loss: 0.8657654523849487
Epoch 1940, training loss: 452.1560974121094 = 0.8465617299079895 + 50.0 * 9.026190757751465
Epoch 1940, val loss: 0.8640269041061401
Epoch 1950, training loss: 452.2353210449219 = 0.8446344137191772 + 50.0 * 9.027813911437988
Epoch 1950, val loss: 0.8622754216194153
Epoch 1960, training loss: 452.41259765625 = 0.8426991105079651 + 50.0 * 9.031397819519043
Epoch 1960, val loss: 0.860527753829956
Epoch 1970, training loss: 452.27337646484375 = 0.8408650755882263 + 50.0 * 9.028650283813477
Epoch 1970, val loss: 0.8589223027229309
Epoch 1980, training loss: 452.3221740722656 = 0.838948667049408 + 50.0 * 9.029664039611816
Epoch 1980, val loss: 0.8571739792823792
Epoch 1990, training loss: 451.9704895019531 = 0.8370047211647034 + 50.0 * 9.022669792175293
Epoch 1990, val loss: 0.8553951382637024
Epoch 2000, training loss: 452.1209716796875 = 0.83501136302948 + 50.0 * 9.025718688964844
Epoch 2000, val loss: 0.8535912036895752
Epoch 2010, training loss: 452.411376953125 = 0.8330932855606079 + 50.0 * 9.03156566619873
Epoch 2010, val loss: 0.8518557548522949
Epoch 2020, training loss: 452.7466735839844 = 0.8311939835548401 + 50.0 * 9.038309097290039
Epoch 2020, val loss: 0.8501198887825012
Epoch 2030, training loss: 451.46331787109375 = 0.829150915145874 + 50.0 * 9.012682914733887
Epoch 2030, val loss: 0.8481835126876831
Epoch 2040, training loss: 452.359619140625 = 0.8272160887718201 + 50.0 * 9.030648231506348
Epoch 2040, val loss: 0.8465122580528259
Epoch 2050, training loss: 452.0028076171875 = 0.825487494468689 + 50.0 * 9.02354621887207
Epoch 2050, val loss: 0.8449161052703857
Epoch 2060, training loss: 452.5929260253906 = 0.8236075639724731 + 50.0 * 9.035386085510254
Epoch 2060, val loss: 0.8432977199554443
Epoch 2070, training loss: 453.1000671386719 = 0.8217895030975342 + 50.0 * 9.045565605163574
Epoch 2070, val loss: 0.8416443467140198
Epoch 2080, training loss: 453.380615234375 = 0.8199529051780701 + 50.0 * 9.051213264465332
Epoch 2080, val loss: 0.839981198310852
Epoch 2090, training loss: 453.72900390625 = 0.8181093335151672 + 50.0 * 9.058218002319336
Epoch 2090, val loss: 0.83833909034729
Epoch 2100, training loss: 453.8930969238281 = 0.816254198551178 + 50.0 * 9.06153678894043
Epoch 2100, val loss: 0.8366719484329224
Epoch 2110, training loss: 453.755615234375 = 0.8143842220306396 + 50.0 * 9.05882453918457
Epoch 2110, val loss: 0.8349946141242981
Epoch 2120, training loss: 453.9736022949219 = 0.8125718235969543 + 50.0 * 9.063220977783203
Epoch 2120, val loss: 0.8333320617675781
Epoch 2130, training loss: 454.11749267578125 = 0.810734212398529 + 50.0 * 9.06613540649414
Epoch 2130, val loss: 0.8316921591758728
Epoch 2140, training loss: 454.162841796875 = 0.8088890910148621 + 50.0 * 9.067078590393066
Epoch 2140, val loss: 0.8300167918205261
Epoch 2150, training loss: 454.0904541015625 = 0.8070427179336548 + 50.0 * 9.065668106079102
Epoch 2150, val loss: 0.8283520936965942
Epoch 2160, training loss: 454.30999755859375 = 0.8052428364753723 + 50.0 * 9.07009506225586
Epoch 2160, val loss: 0.8267310857772827
Epoch 2170, training loss: 454.4482116699219 = 0.8034203052520752 + 50.0 * 9.072896003723145
Epoch 2170, val loss: 0.8250963687896729
Epoch 2180, training loss: 454.43377685546875 = 0.8016025424003601 + 50.0 * 9.072643280029297
Epoch 2180, val loss: 0.823458731174469
Epoch 2190, training loss: 454.3827209472656 = 0.7997836470603943 + 50.0 * 9.071659088134766
Epoch 2190, val loss: 0.8218141198158264
Epoch 2200, training loss: 454.5287780761719 = 0.7979947924613953 + 50.0 * 9.074615478515625
Epoch 2200, val loss: 0.8202112317085266
Epoch 2210, training loss: 454.6711120605469 = 0.7961950898170471 + 50.0 * 9.077498435974121
Epoch 2210, val loss: 0.8185974359512329
Epoch 2220, training loss: 454.56732177734375 = 0.7943887114524841 + 50.0 * 9.075458526611328
Epoch 2220, val loss: 0.8169870376586914
Epoch 2230, training loss: 454.47979736328125 = 0.7926005721092224 + 50.0 * 9.07374382019043
Epoch 2230, val loss: 0.8153777718544006
Epoch 2240, training loss: 454.5432434082031 = 0.7908411622047424 + 50.0 * 9.075048446655273
Epoch 2240, val loss: 0.8137904405593872
Epoch 2250, training loss: 454.45013427734375 = 0.7890307307243347 + 50.0 * 9.073222160339355
Epoch 2250, val loss: 0.8121863603591919
Epoch 2260, training loss: 454.614013671875 = 0.7873013019561768 + 50.0 * 9.076534271240234
Epoch 2260, val loss: 0.8106098771095276
Epoch 2270, training loss: 454.63140869140625 = 0.7855769395828247 + 50.0 * 9.076916694641113
Epoch 2270, val loss: 0.8090658783912659
Epoch 2280, training loss: 454.97003173828125 = 0.7838443517684937 + 50.0 * 9.083724021911621
Epoch 2280, val loss: 0.8075147271156311
Epoch 2290, training loss: 455.2043151855469 = 0.7820816040039062 + 50.0 * 9.088444709777832
Epoch 2290, val loss: 0.8059644103050232
Epoch 2300, training loss: 454.8984069824219 = 0.7802942991256714 + 50.0 * 9.082362174987793
Epoch 2300, val loss: 0.8043820261955261
Epoch 2310, training loss: 454.9624328613281 = 0.7786175012588501 + 50.0 * 9.0836763381958
Epoch 2310, val loss: 0.8028319478034973
Epoch 2320, training loss: 455.2201843261719 = 0.7769187688827515 + 50.0 * 9.088865280151367
Epoch 2320, val loss: 0.8013231158256531
Epoch 2330, training loss: 455.0329284667969 = 0.7752255797386169 + 50.0 * 9.085153579711914
Epoch 2330, val loss: 0.7998287677764893
Epoch 2340, training loss: 455.2351379394531 = 0.7735601663589478 + 50.0 * 9.089231491088867
Epoch 2340, val loss: 0.7983407378196716
Epoch 2350, training loss: 455.1744689941406 = 0.7718047499656677 + 50.0 * 9.088053703308105
Epoch 2350, val loss: 0.7968035936355591
Epoch 2360, training loss: 454.8115539550781 = 0.7700717449188232 + 50.0 * 9.080829620361328
Epoch 2360, val loss: 0.7951952219009399
Epoch 2370, training loss: 455.0315246582031 = 0.7683750987052917 + 50.0 * 9.0852632522583
Epoch 2370, val loss: 0.7937189340591431
Epoch 2380, training loss: 455.306884765625 = 0.7667083144187927 + 50.0 * 9.090804100036621
Epoch 2380, val loss: 0.7922704815864563
Epoch 2390, training loss: 455.5205383300781 = 0.7650805711746216 + 50.0 * 9.095108985900879
Epoch 2390, val loss: 0.7908074259757996
Epoch 2400, training loss: 455.4023132324219 = 0.7633908987045288 + 50.0 * 9.092778205871582
Epoch 2400, val loss: 0.7893292307853699
Epoch 2410, training loss: 455.306396484375 = 0.7617461085319519 + 50.0 * 9.090892791748047
Epoch 2410, val loss: 0.7878847718238831
Epoch 2420, training loss: 455.4791564941406 = 0.760109007358551 + 50.0 * 9.094381332397461
Epoch 2420, val loss: 0.7864254117012024
Epoch 2430, training loss: 455.86572265625 = 0.7584686279296875 + 50.0 * 9.102145195007324
Epoch 2430, val loss: 0.7849807739257812
Epoch 2440, training loss: 455.755615234375 = 0.7568243145942688 + 50.0 * 9.0999755859375
Epoch 2440, val loss: 0.7834979295730591
Epoch 2450, training loss: 455.83966064453125 = 0.7551807165145874 + 50.0 * 9.101689338684082
Epoch 2450, val loss: 0.7820819020271301
Epoch 2460, training loss: 455.80743408203125 = 0.753576397895813 + 50.0 * 9.10107707977295
Epoch 2460, val loss: 0.7806363701820374
Epoch 2470, training loss: 455.9492492675781 = 0.7520064115524292 + 50.0 * 9.103944778442383
Epoch 2470, val loss: 0.7792505621910095
Epoch 2480, training loss: 456.06793212890625 = 0.7504194378852844 + 50.0 * 9.10634994506836
Epoch 2480, val loss: 0.7778331637382507
Epoch 2490, training loss: 455.8808288574219 = 0.7488043904304504 + 50.0 * 9.102640151977539
Epoch 2490, val loss: 0.7764205932617188
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7153623188405797
0.8061291023690503
=== training gcn model ===
Epoch 0, training loss: 512.2029418945312 = 1.0906684398651123 + 50.0 * 10.222245216369629
Epoch 0, val loss: 1.0917490720748901
Epoch 10, training loss: 492.8043518066406 = 1.0904755592346191 + 50.0 * 9.834277153015137
Epoch 10, val loss: 1.0916070938110352
Epoch 20, training loss: 484.2149963378906 = 1.0903652906417847 + 50.0 * 9.662492752075195
Epoch 20, val loss: 1.0915157794952393
Epoch 30, training loss: 477.5813903808594 = 1.0902488231658936 + 50.0 * 9.529823303222656
Epoch 30, val loss: 1.0914157629013062
Epoch 40, training loss: 472.2467956542969 = 1.0901151895523071 + 50.0 * 9.423133850097656
Epoch 40, val loss: 1.0913029909133911
Epoch 50, training loss: 467.8377380371094 = 1.0899772644042969 + 50.0 * 9.334955215454102
Epoch 50, val loss: 1.091184377670288
Epoch 60, training loss: 464.1828308105469 = 1.0898391008377075 + 50.0 * 9.261859893798828
Epoch 60, val loss: 1.0910664796829224
Epoch 70, training loss: 461.06707763671875 = 1.0896929502487183 + 50.0 * 9.19954776763916
Epoch 70, val loss: 1.0909403562545776
Epoch 80, training loss: 458.3918151855469 = 1.0895442962646484 + 50.0 * 9.146045684814453
Epoch 80, val loss: 1.090810775756836
Epoch 90, training loss: 455.9866027832031 = 1.089393973350525 + 50.0 * 9.097944259643555
Epoch 90, val loss: 1.0906811952590942
Epoch 100, training loss: 453.9126281738281 = 1.0892333984375 + 50.0 * 9.05646800994873
Epoch 100, val loss: 1.0905407667160034
Epoch 110, training loss: 452.1317443847656 = 1.089068055152893 + 50.0 * 9.020853042602539
Epoch 110, val loss: 1.090396761894226
Epoch 120, training loss: 450.55206298828125 = 1.088904619216919 + 50.0 * 8.989263534545898
Epoch 120, val loss: 1.0902564525604248
Epoch 130, training loss: 449.1605529785156 = 1.0887361764907837 + 50.0 * 8.96143627166748
Epoch 130, val loss: 1.0901086330413818
Epoch 140, training loss: 448.0660095214844 = 1.0885456800460815 + 50.0 * 8.939549446105957
Epoch 140, val loss: 1.0899378061294556
Epoch 150, training loss: 447.1221923828125 = 1.0883691310882568 + 50.0 * 8.920676231384277
Epoch 150, val loss: 1.0897834300994873
Epoch 160, training loss: 446.3238830566406 = 1.0881847143173218 + 50.0 * 8.90471363067627
Epoch 160, val loss: 1.0896177291870117
Epoch 170, training loss: 445.48577880859375 = 1.0879886150360107 + 50.0 * 8.887955665588379
Epoch 170, val loss: 1.0894465446472168
Epoch 180, training loss: 444.8735656738281 = 1.0877937078475952 + 50.0 * 8.875715255737305
Epoch 180, val loss: 1.0892689228057861
Epoch 190, training loss: 444.3417663574219 = 1.0875866413116455 + 50.0 * 8.865083694458008
Epoch 190, val loss: 1.0890847444534302
Epoch 200, training loss: 443.8448791503906 = 1.0873773097991943 + 50.0 * 8.85515022277832
Epoch 200, val loss: 1.0888921022415161
Epoch 210, training loss: 443.425048828125 = 1.087156891822815 + 50.0 * 8.846757888793945
Epoch 210, val loss: 1.088693618774414
Epoch 220, training loss: 443.0950622558594 = 1.0869512557983398 + 50.0 * 8.84016227722168
Epoch 220, val loss: 1.0885076522827148
Epoch 230, training loss: 442.9117736816406 = 1.0867258310317993 + 50.0 * 8.836501121520996
Epoch 230, val loss: 1.088305950164795
Epoch 240, training loss: 442.450439453125 = 1.086491346359253 + 50.0 * 8.827279090881348
Epoch 240, val loss: 1.0880969762802124
Epoch 250, training loss: 442.3072814941406 = 1.086259365081787 + 50.0 * 8.824419975280762
Epoch 250, val loss: 1.087883710861206
Epoch 260, training loss: 442.134521484375 = 1.086017370223999 + 50.0 * 8.820969581604004
Epoch 260, val loss: 1.0876649618148804
Epoch 270, training loss: 441.984619140625 = 1.0857696533203125 + 50.0 * 8.817976951599121
Epoch 270, val loss: 1.087438941001892
Epoch 280, training loss: 441.84564208984375 = 1.0855088233947754 + 50.0 * 8.815202713012695
Epoch 280, val loss: 1.0872023105621338
Epoch 290, training loss: 441.66326904296875 = 1.0852453708648682 + 50.0 * 8.81156063079834
Epoch 290, val loss: 1.0869680643081665
Epoch 300, training loss: 441.3551025390625 = 1.0849756002426147 + 50.0 * 8.805402755737305
Epoch 300, val loss: 1.0867187976837158
Epoch 310, training loss: 441.5240478515625 = 1.0847160816192627 + 50.0 * 8.808786392211914
Epoch 310, val loss: 1.0864830017089844
Epoch 320, training loss: 441.31256103515625 = 1.0844473838806152 + 50.0 * 8.80456256866455
Epoch 320, val loss: 1.0862351655960083
Epoch 330, training loss: 441.4333190917969 = 1.0841597318649292 + 50.0 * 8.80698299407959
Epoch 330, val loss: 1.085970163345337
Epoch 340, training loss: 441.4049072265625 = 1.0838922262191772 + 50.0 * 8.80642032623291
Epoch 340, val loss: 1.0857317447662354
Epoch 350, training loss: 441.224609375 = 1.0835975408554077 + 50.0 * 8.802820205688477
Epoch 350, val loss: 1.0854566097259521
Epoch 360, training loss: 441.2579345703125 = 1.083316445350647 + 50.0 * 8.803492546081543
Epoch 360, val loss: 1.0851987600326538
Epoch 370, training loss: 441.24822998046875 = 1.0830165147781372 + 50.0 * 8.803304672241211
Epoch 370, val loss: 1.0849262475967407
Epoch 380, training loss: 441.1438293457031 = 1.0827282667160034 + 50.0 * 8.80122184753418
Epoch 380, val loss: 1.0846571922302246
Epoch 390, training loss: 441.2470397949219 = 1.0824354887008667 + 50.0 * 8.803292274475098
Epoch 390, val loss: 1.0843900442123413
Epoch 400, training loss: 441.2265625 = 1.0821298360824585 + 50.0 * 8.802888870239258
Epoch 400, val loss: 1.0841073989868164
Epoch 410, training loss: 441.2348937988281 = 1.0818265676498413 + 50.0 * 8.803061485290527
Epoch 410, val loss: 1.0838261842727661
Epoch 420, training loss: 441.387939453125 = 1.0815154314041138 + 50.0 * 8.80612850189209
Epoch 420, val loss: 1.0835405588150024
Epoch 430, training loss: 441.469970703125 = 1.0811954736709595 + 50.0 * 8.807775497436523
Epoch 430, val loss: 1.0832470655441284
Epoch 440, training loss: 441.3667907714844 = 1.0808701515197754 + 50.0 * 8.805718421936035
Epoch 440, val loss: 1.0829441547393799
Epoch 450, training loss: 441.54705810546875 = 1.0804909467697144 + 50.0 * 8.809330940246582
Epoch 450, val loss: 1.0825797319412231
Epoch 460, training loss: 441.9073181152344 = 1.0800923109054565 + 50.0 * 8.816544532775879
Epoch 460, val loss: 1.08220636844635
Epoch 470, training loss: 441.90814208984375 = 1.0796986818313599 + 50.0 * 8.816568374633789
Epoch 470, val loss: 1.0818381309509277
Epoch 480, training loss: 442.2662658691406 = 1.079306960105896 + 50.0 * 8.823739051818848
Epoch 480, val loss: 1.0814766883850098
Epoch 490, training loss: 441.6936340332031 = 1.0788965225219727 + 50.0 * 8.812294960021973
Epoch 490, val loss: 1.0810896158218384
Epoch 500, training loss: 442.0104675292969 = 1.0784924030303955 + 50.0 * 8.818639755249023
Epoch 500, val loss: 1.0807126760482788
Epoch 510, training loss: 442.01202392578125 = 1.0780985355377197 + 50.0 * 8.818678855895996
Epoch 510, val loss: 1.0803351402282715
Epoch 520, training loss: 442.2900695800781 = 1.0776890516281128 + 50.0 * 8.824247360229492
Epoch 520, val loss: 1.0799568891525269
Epoch 530, training loss: 442.337646484375 = 1.0772738456726074 + 50.0 * 8.825207710266113
Epoch 530, val loss: 1.0795609951019287
Epoch 540, training loss: 442.411865234375 = 1.0768510103225708 + 50.0 * 8.826700210571289
Epoch 540, val loss: 1.0791661739349365
Epoch 550, training loss: 442.5824279785156 = 1.0764164924621582 + 50.0 * 8.830120086669922
Epoch 550, val loss: 1.0787525177001953
Epoch 560, training loss: 442.9305114746094 = 1.0759809017181396 + 50.0 * 8.837090492248535
Epoch 560, val loss: 1.0783445835113525
Epoch 570, training loss: 442.3290100097656 = 1.0755122900009155 + 50.0 * 8.82507038116455
Epoch 570, val loss: 1.077888011932373
Epoch 580, training loss: 442.9171447753906 = 1.0751091241836548 + 50.0 * 8.836840629577637
Epoch 580, val loss: 1.0775079727172852
Epoch 590, training loss: 443.255859375 = 1.0746573209762573 + 50.0 * 8.843624114990234
Epoch 590, val loss: 1.0770920515060425
Epoch 600, training loss: 443.1337585449219 = 1.074196219444275 + 50.0 * 8.841191291809082
Epoch 600, val loss: 1.0766574144363403
Epoch 610, training loss: 443.1006774902344 = 1.0737172365188599 + 50.0 * 8.84053897857666
Epoch 610, val loss: 1.0762022733688354
Epoch 620, training loss: 443.3043518066406 = 1.0732619762420654 + 50.0 * 8.844621658325195
Epoch 620, val loss: 1.0757660865783691
Epoch 630, training loss: 443.39532470703125 = 1.0727829933166504 + 50.0 * 8.846450805664062
Epoch 630, val loss: 1.0753134489059448
Epoch 640, training loss: 443.9966125488281 = 1.0722754001617432 + 50.0 * 8.858487129211426
Epoch 640, val loss: 1.0748318433761597
Epoch 650, training loss: 443.647216796875 = 1.0717658996582031 + 50.0 * 8.851509094238281
Epoch 650, val loss: 1.0743544101715088
Epoch 660, training loss: 443.61199951171875 = 1.071252465248108 + 50.0 * 8.850814819335938
Epoch 660, val loss: 1.0738605260849
Epoch 670, training loss: 443.98748779296875 = 1.0707777738571167 + 50.0 * 8.8583345413208
Epoch 670, val loss: 1.0734131336212158
Epoch 680, training loss: 443.8896179199219 = 1.0702530145645142 + 50.0 * 8.8563871383667
Epoch 680, val loss: 1.0729271173477173
Epoch 690, training loss: 443.8525390625 = 1.069716453552246 + 50.0 * 8.855656623840332
Epoch 690, val loss: 1.0724154710769653
Epoch 700, training loss: 443.9169006347656 = 1.069180965423584 + 50.0 * 8.856954574584961
Epoch 700, val loss: 1.0719141960144043
Epoch 710, training loss: 444.28955078125 = 1.0686558485031128 + 50.0 * 8.864418029785156
Epoch 710, val loss: 1.0714175701141357
Epoch 720, training loss: 444.4634094238281 = 1.0681180953979492 + 50.0 * 8.867905616760254
Epoch 720, val loss: 1.070899248123169
Epoch 730, training loss: 444.7209167480469 = 1.0675616264343262 + 50.0 * 8.873066902160645
Epoch 730, val loss: 1.07038414478302
Epoch 740, training loss: 444.73980712890625 = 1.0670078992843628 + 50.0 * 8.873456001281738
Epoch 740, val loss: 1.0698559284210205
Epoch 750, training loss: 444.9949645996094 = 1.066444993019104 + 50.0 * 8.878570556640625
Epoch 750, val loss: 1.0693213939666748
Epoch 760, training loss: 445.1439514160156 = 1.0658727884292603 + 50.0 * 8.881561279296875
Epoch 760, val loss: 1.0687837600708008
Epoch 770, training loss: 445.28973388671875 = 1.0652817487716675 + 50.0 * 8.884489059448242
Epoch 770, val loss: 1.0682297945022583
Epoch 780, training loss: 445.3337707519531 = 1.0646957159042358 + 50.0 * 8.885381698608398
Epoch 780, val loss: 1.0676754713058472
Epoch 790, training loss: 445.2914123535156 = 1.0640947818756104 + 50.0 * 8.884546279907227
Epoch 790, val loss: 1.0671080350875854
Epoch 800, training loss: 445.2152099609375 = 1.0635324716567993 + 50.0 * 8.883033752441406
Epoch 800, val loss: 1.0665761232376099
Epoch 810, training loss: 445.3230895996094 = 1.0629359483718872 + 50.0 * 8.88520336151123
Epoch 810, val loss: 1.066015601158142
Epoch 820, training loss: 445.7760009765625 = 1.0623431205749512 + 50.0 * 8.894272804260254
Epoch 820, val loss: 1.0654572248458862
Epoch 830, training loss: 447.3681335449219 = 1.0617060661315918 + 50.0 * 8.926128387451172
Epoch 830, val loss: 1.0648138523101807
Epoch 840, training loss: 444.0317077636719 = 1.0609169006347656 + 50.0 * 8.859416007995605
Epoch 840, val loss: 1.0641649961471558
Epoch 850, training loss: 446.6410827636719 = 1.0605182647705078 + 50.0 * 8.911611557006836
Epoch 850, val loss: 1.0637404918670654
Epoch 860, training loss: 445.08599853515625 = 1.0598050355911255 + 50.0 * 8.880523681640625
Epoch 860, val loss: 1.0630987882614136
Epoch 870, training loss: 445.5411376953125 = 1.0592916011810303 + 50.0 * 8.889636993408203
Epoch 870, val loss: 1.0625900030136108
Epoch 880, training loss: 445.1151123046875 = 1.0586055517196655 + 50.0 * 8.88113021850586
Epoch 880, val loss: 1.0619587898254395
Epoch 890, training loss: 445.4651184082031 = 1.0579833984375 + 50.0 * 8.888142585754395
Epoch 890, val loss: 1.0613536834716797
Epoch 900, training loss: 445.8948669433594 = 1.05735182762146 + 50.0 * 8.896750450134277
Epoch 900, val loss: 1.0607718229293823
Epoch 910, training loss: 446.1630554199219 = 1.0567116737365723 + 50.0 * 8.902127265930176
Epoch 910, val loss: 1.0601754188537598
Epoch 920, training loss: 446.58673095703125 = 1.0560864210128784 + 50.0 * 8.910613059997559
Epoch 920, val loss: 1.0595742464065552
Epoch 930, training loss: 446.599853515625 = 1.0554344654083252 + 50.0 * 8.910888671875
Epoch 930, val loss: 1.0589607954025269
Epoch 940, training loss: 446.7340087890625 = 1.0547668933868408 + 50.0 * 8.91358470916748
Epoch 940, val loss: 1.0583250522613525
Epoch 950, training loss: 446.48651123046875 = 1.0540728569030762 + 50.0 * 8.908648490905762
Epoch 950, val loss: 1.0576882362365723
Epoch 960, training loss: 446.7655029296875 = 1.0534045696258545 + 50.0 * 8.914241790771484
Epoch 960, val loss: 1.0570595264434814
Epoch 970, training loss: 447.2287902832031 = 1.0527281761169434 + 50.0 * 8.923521041870117
Epoch 970, val loss: 1.0564241409301758
Epoch 980, training loss: 447.41558837890625 = 1.052039623260498 + 50.0 * 8.927270889282227
Epoch 980, val loss: 1.0557897090911865
Epoch 990, training loss: 447.5436096191406 = 1.0513598918914795 + 50.0 * 8.929844856262207
Epoch 990, val loss: 1.0551460981369019
Epoch 1000, training loss: 447.6263427734375 = 1.0506588220596313 + 50.0 * 8.931513786315918
Epoch 1000, val loss: 1.054491400718689
Epoch 1010, training loss: 447.5796203613281 = 1.0499643087387085 + 50.0 * 8.930593490600586
Epoch 1010, val loss: 1.0538148880004883
Epoch 1020, training loss: 447.6712951660156 = 1.0492455959320068 + 50.0 * 8.932440757751465
Epoch 1020, val loss: 1.0531468391418457
Epoch 1030, training loss: 447.7947692871094 = 1.048537015914917 + 50.0 * 8.934925079345703
Epoch 1030, val loss: 1.0524991750717163
Epoch 1040, training loss: 448.0856628417969 = 1.0478301048278809 + 50.0 * 8.940756797790527
Epoch 1040, val loss: 1.0518404245376587
Epoch 1050, training loss: 448.19415283203125 = 1.0471237897872925 + 50.0 * 8.942940711975098
Epoch 1050, val loss: 1.051168441772461
Epoch 1060, training loss: 448.3136901855469 = 1.0463935136795044 + 50.0 * 8.945345878601074
Epoch 1060, val loss: 1.0504896640777588
Epoch 1070, training loss: 448.3928527832031 = 1.0456796884536743 + 50.0 * 8.946943283081055
Epoch 1070, val loss: 1.0498151779174805
Epoch 1080, training loss: 448.5064697265625 = 1.0449509620666504 + 50.0 * 8.949230194091797
Epoch 1080, val loss: 1.049122929573059
Epoch 1090, training loss: 448.76409912109375 = 1.044203758239746 + 50.0 * 8.954398155212402
Epoch 1090, val loss: 1.0484297275543213
Epoch 1100, training loss: 448.729736328125 = 1.0434818267822266 + 50.0 * 8.95372486114502
Epoch 1100, val loss: 1.0477402210235596
Epoch 1110, training loss: 448.339111328125 = 1.0427074432373047 + 50.0 * 8.945928573608398
Epoch 1110, val loss: 1.0469931364059448
Epoch 1120, training loss: 448.7135009765625 = 1.0420010089874268 + 50.0 * 8.95343017578125
Epoch 1120, val loss: 1.0463323593139648
Epoch 1130, training loss: 448.7937316894531 = 1.0412458181381226 + 50.0 * 8.955049514770508
Epoch 1130, val loss: 1.0456410646438599
Epoch 1140, training loss: 449.00177001953125 = 1.040496587753296 + 50.0 * 8.95922565460205
Epoch 1140, val loss: 1.044933795928955
Epoch 1150, training loss: 449.0779724121094 = 1.0397615432739258 + 50.0 * 8.960763931274414
Epoch 1150, val loss: 1.0442358255386353
Epoch 1160, training loss: 449.09613037109375 = 1.0389958620071411 + 50.0 * 8.961142539978027
Epoch 1160, val loss: 1.0435279607772827
Epoch 1170, training loss: 449.3019104003906 = 1.0382577180862427 + 50.0 * 8.965272903442383
Epoch 1170, val loss: 1.0428143739700317
Epoch 1180, training loss: 449.30010986328125 = 1.037480115890503 + 50.0 * 8.965252876281738
Epoch 1180, val loss: 1.0420933961868286
Epoch 1190, training loss: 449.49169921875 = 1.0367032289505005 + 50.0 * 8.969099998474121
Epoch 1190, val loss: 1.0413681268692017
Epoch 1200, training loss: 449.5385437011719 = 1.0359236001968384 + 50.0 * 8.970052719116211
Epoch 1200, val loss: 1.0406345129013062
Epoch 1210, training loss: 449.4353942871094 = 1.0351358652114868 + 50.0 * 8.968005180358887
Epoch 1210, val loss: 1.0398906469345093
Epoch 1220, training loss: 449.51983642578125 = 1.0343700647354126 + 50.0 * 8.969709396362305
Epoch 1220, val loss: 1.0391530990600586
Epoch 1230, training loss: 449.88653564453125 = 1.0336171388626099 + 50.0 * 8.977058410644531
Epoch 1230, val loss: 1.038453221321106
Epoch 1240, training loss: 449.911376953125 = 1.0328110456466675 + 50.0 * 8.977571487426758
Epoch 1240, val loss: 1.0377051830291748
Epoch 1250, training loss: 450.2048645019531 = 1.0320402383804321 + 50.0 * 8.9834566116333
Epoch 1250, val loss: 1.0369704961776733
Epoch 1260, training loss: 450.2964782714844 = 1.031237244606018 + 50.0 * 8.985304832458496
Epoch 1260, val loss: 1.0362093448638916
Epoch 1270, training loss: 450.2055358886719 = 1.0304492712020874 + 50.0 * 8.983501434326172
Epoch 1270, val loss: 1.0354633331298828
Epoch 1280, training loss: 450.2546691894531 = 1.02964186668396 + 50.0 * 8.984500885009766
Epoch 1280, val loss: 1.034704566001892
Epoch 1290, training loss: 450.5146179199219 = 1.0288426876068115 + 50.0 * 8.989715576171875
Epoch 1290, val loss: 1.0339443683624268
Epoch 1300, training loss: 450.3343505859375 = 1.0280061960220337 + 50.0 * 8.986126899719238
Epoch 1300, val loss: 1.033149242401123
Epoch 1310, training loss: 450.3854675292969 = 1.027176856994629 + 50.0 * 8.987165451049805
Epoch 1310, val loss: 1.0323514938354492
Epoch 1320, training loss: 450.5314636230469 = 1.0263612270355225 + 50.0 * 8.99010181427002
Epoch 1320, val loss: 1.0316044092178345
Epoch 1330, training loss: 450.5926818847656 = 1.025517463684082 + 50.0 * 8.99134349822998
Epoch 1330, val loss: 1.0308138132095337
Epoch 1340, training loss: 450.8206787109375 = 1.0247175693511963 + 50.0 * 8.995919227600098
Epoch 1340, val loss: 1.0300538539886475
Epoch 1350, training loss: 451.1293029785156 = 1.0238765478134155 + 50.0 * 9.002108573913574
Epoch 1350, val loss: 1.029272437095642
Epoch 1360, training loss: 450.8909606933594 = 1.0230140686035156 + 50.0 * 8.997359275817871
Epoch 1360, val loss: 1.0284581184387207
Epoch 1370, training loss: 451.1708068847656 = 1.0221842527389526 + 50.0 * 9.002972602844238
Epoch 1370, val loss: 1.0276702642440796
Epoch 1380, training loss: 451.31500244140625 = 1.021355152130127 + 50.0 * 9.00587272644043
Epoch 1380, val loss: 1.0268900394439697
Epoch 1390, training loss: 451.35089111328125 = 1.020487666130066 + 50.0 * 9.006608009338379
Epoch 1390, val loss: 1.0260798931121826
Epoch 1400, training loss: 451.45526123046875 = 1.0196326971054077 + 50.0 * 9.008712768554688
Epoch 1400, val loss: 1.0252670049667358
Epoch 1410, training loss: 451.38592529296875 = 1.0187702178955078 + 50.0 * 9.007343292236328
Epoch 1410, val loss: 1.0244603157043457
Epoch 1420, training loss: 451.65728759765625 = 1.017932653427124 + 50.0 * 9.012786865234375
Epoch 1420, val loss: 1.0236684083938599
Epoch 1430, training loss: 451.7181091308594 = 1.0170674324035645 + 50.0 * 9.014020919799805
Epoch 1430, val loss: 1.022845983505249
Epoch 1440, training loss: 451.6037292480469 = 1.0161705017089844 + 50.0 * 9.011751174926758
Epoch 1440, val loss: 1.0220199823379517
Epoch 1450, training loss: 451.0356140136719 = 1.0152490139007568 + 50.0 * 9.000407218933105
Epoch 1450, val loss: 1.021160364151001
Epoch 1460, training loss: 450.6438293457031 = 1.0143412351608276 + 50.0 * 8.992589950561523
Epoch 1460, val loss: 1.0202895402908325
Epoch 1470, training loss: 451.2860107421875 = 1.0135525465011597 + 50.0 * 9.005449295043945
Epoch 1470, val loss: 1.0195401906967163
Epoch 1480, training loss: 451.8270263671875 = 1.0127140283584595 + 50.0 * 9.01628589630127
Epoch 1480, val loss: 1.0187479257583618
Epoch 1490, training loss: 452.0867614746094 = 1.0118494033813477 + 50.0 * 9.021498680114746
Epoch 1490, val loss: 1.0179393291473389
Epoch 1500, training loss: 452.0262756347656 = 1.0109504461288452 + 50.0 * 9.020306587219238
Epoch 1500, val loss: 1.0170681476593018
Epoch 1510, training loss: 452.0309753417969 = 1.0100622177124023 + 50.0 * 9.020418167114258
Epoch 1510, val loss: 1.0162456035614014
Epoch 1520, training loss: 452.3192443847656 = 1.0092002153396606 + 50.0 * 9.026201248168945
Epoch 1520, val loss: 1.0154223442077637
Epoch 1530, training loss: 452.4992370605469 = 1.008310079574585 + 50.0 * 9.029818534851074
Epoch 1530, val loss: 1.01458740234375
Epoch 1540, training loss: 452.4315490722656 = 1.0074037313461304 + 50.0 * 9.028482437133789
Epoch 1540, val loss: 1.0137207508087158
Epoch 1550, training loss: 452.58453369140625 = 1.006508469581604 + 50.0 * 9.031560897827148
Epoch 1550, val loss: 1.0128941535949707
Epoch 1560, training loss: 452.8160095214844 = 1.005630612373352 + 50.0 * 9.03620719909668
Epoch 1560, val loss: 1.0120526552200317
Epoch 1570, training loss: 452.2619323730469 = 1.0047098398208618 + 50.0 * 9.025144577026367
Epoch 1570, val loss: 1.0111802816390991
Epoch 1580, training loss: 452.48333740234375 = 1.003801703453064 + 50.0 * 9.029590606689453
Epoch 1580, val loss: 1.0103535652160645
Epoch 1590, training loss: 452.73175048828125 = 1.0029253959655762 + 50.0 * 9.034576416015625
Epoch 1590, val loss: 1.0095255374908447
Epoch 1600, training loss: 453.0467224121094 = 1.0020548105239868 + 50.0 * 9.0408935546875
Epoch 1600, val loss: 1.0086913108825684
Epoch 1610, training loss: 452.9554443359375 = 1.0011416673660278 + 50.0 * 9.03908634185791
Epoch 1610, val loss: 1.007818579673767
Epoch 1620, training loss: 452.97027587890625 = 1.000227689743042 + 50.0 * 9.039401054382324
Epoch 1620, val loss: 1.0069756507873535
Epoch 1630, training loss: 453.1187744140625 = 0.9993149638175964 + 50.0 * 9.042388916015625
Epoch 1630, val loss: 1.0061341524124146
Epoch 1640, training loss: 453.1966857910156 = 0.998397707939148 + 50.0 * 9.043966293334961
Epoch 1640, val loss: 1.0052762031555176
Epoch 1650, training loss: 453.30987548828125 = 0.997481107711792 + 50.0 * 9.046248435974121
Epoch 1650, val loss: 1.0044151544570923
Epoch 1660, training loss: 453.50775146484375 = 0.996568500995636 + 50.0 * 9.050223350524902
Epoch 1660, val loss: 1.0035438537597656
Epoch 1670, training loss: 453.338623046875 = 0.9956308007240295 + 50.0 * 9.046859741210938
Epoch 1670, val loss: 1.0026732683181763
Epoch 1680, training loss: 453.6606750488281 = 0.9947182536125183 + 50.0 * 9.053318977355957
Epoch 1680, val loss: 1.0018298625946045
Epoch 1690, training loss: 453.19891357421875 = 0.9937660098075867 + 50.0 * 9.044102668762207
Epoch 1690, val loss: 1.0009632110595703
Epoch 1700, training loss: 453.2585144042969 = 0.992838442325592 + 50.0 * 9.045313835144043
Epoch 1700, val loss: 1.0000648498535156
Epoch 1710, training loss: 453.60858154296875 = 0.991866946220398 + 50.0 * 9.052334785461426
Epoch 1710, val loss: 0.9991748929023743
Epoch 1720, training loss: 453.90740966796875 = 0.9909322261810303 + 50.0 * 9.058329582214355
Epoch 1720, val loss: 0.9983026385307312
Epoch 1730, training loss: 453.8822937011719 = 0.9899900555610657 + 50.0 * 9.057846069335938
Epoch 1730, val loss: 0.9974116683006287
Epoch 1740, training loss: 453.9375915527344 = 0.989011824131012 + 50.0 * 9.058971405029297
Epoch 1740, val loss: 0.9965168833732605
Epoch 1750, training loss: 454.12255859375 = 0.9880772829055786 + 50.0 * 9.062689781188965
Epoch 1750, val loss: 0.9956458210945129
Epoch 1760, training loss: 454.32763671875 = 0.9871392846107483 + 50.0 * 9.06680965423584
Epoch 1760, val loss: 0.9947671890258789
Epoch 1770, training loss: 454.24835205078125 = 0.986182451248169 + 50.0 * 9.0652437210083
Epoch 1770, val loss: 0.9938768744468689
Epoch 1780, training loss: 454.3330078125 = 0.9852216243743896 + 50.0 * 9.06695556640625
Epoch 1780, val loss: 0.9929788112640381
Epoch 1790, training loss: 454.41961669921875 = 0.9842644333839417 + 50.0 * 9.068707466125488
Epoch 1790, val loss: 0.9920862913131714
Epoch 1800, training loss: 454.51995849609375 = 0.9833088517189026 + 50.0 * 9.070733070373535
Epoch 1800, val loss: 0.9912089109420776
Epoch 1810, training loss: 454.4310607910156 = 0.9823419451713562 + 50.0 * 9.068974494934082
Epoch 1810, val loss: 0.99030601978302
Epoch 1820, training loss: 454.5205993652344 = 0.9813783168792725 + 50.0 * 9.070784568786621
Epoch 1820, val loss: 0.989424467086792
Epoch 1830, training loss: 454.7628173828125 = 0.9804205298423767 + 50.0 * 9.075648307800293
Epoch 1830, val loss: 0.9885601997375488
Epoch 1840, training loss: 454.8111877441406 = 0.9794559478759766 + 50.0 * 9.076634407043457
Epoch 1840, val loss: 0.9876654744148254
Epoch 1850, training loss: 454.615478515625 = 0.978489875793457 + 50.0 * 9.072739601135254
Epoch 1850, val loss: 0.9867818355560303
Epoch 1860, training loss: 454.80072021484375 = 0.9775324463844299 + 50.0 * 9.07646369934082
Epoch 1860, val loss: 0.9859000444412231
Epoch 1870, training loss: 454.9282531738281 = 0.9765678644180298 + 50.0 * 9.079033851623535
Epoch 1870, val loss: 0.9850239157676697
Epoch 1880, training loss: 454.7215270996094 = 0.9755832552909851 + 50.0 * 9.074918746948242
Epoch 1880, val loss: 0.9841236472129822
Epoch 1890, training loss: 455.0660400390625 = 0.9746250510215759 + 50.0 * 9.081828117370605
Epoch 1890, val loss: 0.9832408428192139
Epoch 1900, training loss: 455.30682373046875 = 0.9736844301223755 + 50.0 * 9.086662292480469
Epoch 1900, val loss: 0.982383131980896
Epoch 1910, training loss: 455.4933776855469 = 0.9727166891098022 + 50.0 * 9.090413093566895
Epoch 1910, val loss: 0.9814891815185547
Epoch 1920, training loss: 455.0851745605469 = 0.9717524647712708 + 50.0 * 9.082268714904785
Epoch 1920, val loss: 0.9806033968925476
Epoch 1930, training loss: 454.7475891113281 = 0.9707865715026855 + 50.0 * 9.075535774230957
Epoch 1930, val loss: 0.9797267913818359
Epoch 1940, training loss: 454.8154296875 = 0.9698623418807983 + 50.0 * 9.076911926269531
Epoch 1940, val loss: 0.9788670539855957
Epoch 1950, training loss: 454.95428466796875 = 0.9688668847084045 + 50.0 * 9.079708099365234
Epoch 1950, val loss: 0.9779698252677917
Epoch 1960, training loss: 455.1851501464844 = 0.9679427146911621 + 50.0 * 9.084343910217285
Epoch 1960, val loss: 0.9771140217781067
Epoch 1970, training loss: 455.4441833496094 = 0.9669928550720215 + 50.0 * 9.089544296264648
Epoch 1970, val loss: 0.9762474894523621
Epoch 1980, training loss: 455.644775390625 = 0.9660090804100037 + 50.0 * 9.093575477600098
Epoch 1980, val loss: 0.9753456711769104
Epoch 1990, training loss: 455.753173828125 = 0.9650174975395203 + 50.0 * 9.095763206481934
Epoch 1990, val loss: 0.9744318127632141
Epoch 2000, training loss: 455.5951232910156 = 0.9640204310417175 + 50.0 * 9.092621803283691
Epoch 2000, val loss: 0.9735066294670105
Epoch 2010, training loss: 455.60650634765625 = 0.9630172252655029 + 50.0 * 9.092869758605957
Epoch 2010, val loss: 0.9725844860076904
Epoch 2020, training loss: 455.66351318359375 = 0.9620198011398315 + 50.0 * 9.094030380249023
Epoch 2020, val loss: 0.9716801643371582
Epoch 2030, training loss: 455.9323425292969 = 0.9610278606414795 + 50.0 * 9.09942626953125
Epoch 2030, val loss: 0.9707748889923096
Epoch 2040, training loss: 455.9814453125 = 0.9600249528884888 + 50.0 * 9.100428581237793
Epoch 2040, val loss: 0.9698496460914612
Epoch 2050, training loss: 456.0979309082031 = 0.959037184715271 + 50.0 * 9.102777481079102
Epoch 2050, val loss: 0.9689517617225647
Epoch 2060, training loss: 456.1394348144531 = 0.9580326676368713 + 50.0 * 9.103628158569336
Epoch 2060, val loss: 0.9680339097976685
Epoch 2070, training loss: 456.2588195800781 = 0.9570505023002625 + 50.0 * 9.106035232543945
Epoch 2070, val loss: 0.9671247601509094
Epoch 2080, training loss: 456.152587890625 = 0.9560587406158447 + 50.0 * 9.103930473327637
Epoch 2080, val loss: 0.9662442803382874
Epoch 2090, training loss: 456.3873596191406 = 0.955089271068573 + 50.0 * 9.10864543914795
Epoch 2090, val loss: 0.9653684496879578
Epoch 2100, training loss: 456.5960998535156 = 0.9541110396385193 + 50.0 * 9.112839698791504
Epoch 2100, val loss: 0.9644818902015686
Epoch 2110, training loss: 456.5750427246094 = 0.9531247019767761 + 50.0 * 9.112438201904297
Epoch 2110, val loss: 0.9635783433914185
Epoch 2120, training loss: 456.3246765136719 = 0.9521452188491821 + 50.0 * 9.107450485229492
Epoch 2120, val loss: 0.9626922607421875
Epoch 2130, training loss: 456.6192321777344 = 0.9511979818344116 + 50.0 * 9.113360404968262
Epoch 2130, val loss: 0.961837112903595
Epoch 2140, training loss: 456.6229553222656 = 0.9502356648445129 + 50.0 * 9.113454818725586
Epoch 2140, val loss: 0.960966944694519
Epoch 2150, training loss: 456.719482421875 = 0.9493018984794617 + 50.0 * 9.115403175354004
Epoch 2150, val loss: 0.9601113796234131
Epoch 2160, training loss: 456.8110656738281 = 0.9483582377433777 + 50.0 * 9.117254257202148
Epoch 2160, val loss: 0.9592462778091431
Epoch 2170, training loss: 456.22247314453125 = 0.947368323802948 + 50.0 * 9.105502128601074
Epoch 2170, val loss: 0.9583652019500732
Epoch 2180, training loss: 456.2626647949219 = 0.9463545680046082 + 50.0 * 9.10632610321045
Epoch 2180, val loss: 0.9574829936027527
Epoch 2190, training loss: 456.4214782714844 = 0.9454763531684875 + 50.0 * 9.109519958496094
Epoch 2190, val loss: 0.9566788673400879
Epoch 2200, training loss: 456.6778259277344 = 0.9445714950561523 + 50.0 * 9.114665031433105
Epoch 2200, val loss: 0.955859363079071
Epoch 2210, training loss: 457.0093994140625 = 0.9436684846878052 + 50.0 * 9.121315002441406
Epoch 2210, val loss: 0.95504230260849
Epoch 2220, training loss: 457.18048095703125 = 0.9427483677864075 + 50.0 * 9.124754905700684
Epoch 2220, val loss: 0.9542176723480225
Epoch 2230, training loss: 456.92095947265625 = 0.9418149590492249 + 50.0 * 9.119583129882812
Epoch 2230, val loss: 0.9533854126930237
Epoch 2240, training loss: 457.0970764160156 = 0.9409081935882568 + 50.0 * 9.123123168945312
Epoch 2240, val loss: 0.952576220035553
Epoch 2250, training loss: 457.04180908203125 = 0.9399998188018799 + 50.0 * 9.12203598022461
Epoch 2250, val loss: 0.9517672657966614
Epoch 2260, training loss: 457.07861328125 = 0.9390930533409119 + 50.0 * 9.122790336608887
Epoch 2260, val loss: 0.9509549736976624
Epoch 2270, training loss: 457.2533874511719 = 0.9381935596466064 + 50.0 * 9.126303672790527
Epoch 2270, val loss: 0.9501623511314392
Epoch 2280, training loss: 457.29266357421875 = 0.9372947216033936 + 50.0 * 9.127107620239258
Epoch 2280, val loss: 0.9493582248687744
Epoch 2290, training loss: 457.3101501464844 = 0.9363791942596436 + 50.0 * 9.12747573852539
Epoch 2290, val loss: 0.9485354423522949
Epoch 2300, training loss: 456.7057189941406 = 0.9354403614997864 + 50.0 * 9.115405082702637
Epoch 2300, val loss: 0.9476965665817261
Epoch 2310, training loss: 456.7894592285156 = 0.9345583915710449 + 50.0 * 9.117097854614258
Epoch 2310, val loss: 0.9469331502914429
Epoch 2320, training loss: 457.180908203125 = 0.9336816668510437 + 50.0 * 9.124944686889648
Epoch 2320, val loss: 0.9461482167243958
Epoch 2330, training loss: 457.3990173339844 = 0.9327701330184937 + 50.0 * 9.129324913024902
Epoch 2330, val loss: 0.9453628063201904
Epoch 2340, training loss: 456.9261779785156 = 0.93186354637146 + 50.0 * 9.11988639831543
Epoch 2340, val loss: 0.9445222616195679
Epoch 2350, training loss: 456.793212890625 = 0.9309725761413574 + 50.0 * 9.117244720458984
Epoch 2350, val loss: 0.943763017654419
Epoch 2360, training loss: 457.13916015625 = 0.9301092028617859 + 50.0 * 9.124180793762207
Epoch 2360, val loss: 0.9430042505264282
Epoch 2370, training loss: 457.3299560546875 = 0.9292185306549072 + 50.0 * 9.12801456451416
Epoch 2370, val loss: 0.9422163367271423
Epoch 2380, training loss: 457.60723876953125 = 0.9283212423324585 + 50.0 * 9.133578300476074
Epoch 2380, val loss: 0.9414313435554504
Epoch 2390, training loss: 457.3917236328125 = 0.9274023771286011 + 50.0 * 9.129286766052246
Epoch 2390, val loss: 0.9406206011772156
Epoch 2400, training loss: 457.6380310058594 = 0.9265164136886597 + 50.0 * 9.134230613708496
Epoch 2400, val loss: 0.9398570656776428
Epoch 2410, training loss: 457.7711486816406 = 0.9256294369697571 + 50.0 * 9.136910438537598
Epoch 2410, val loss: 0.9390854239463806
Epoch 2420, training loss: 457.78997802734375 = 0.92472243309021 + 50.0 * 9.13730525970459
Epoch 2420, val loss: 0.9382913708686829
Epoch 2430, training loss: 457.8181457519531 = 0.9238463044166565 + 50.0 * 9.137886047363281
Epoch 2430, val loss: 0.937519371509552
Epoch 2440, training loss: 457.8998107910156 = 0.9229661226272583 + 50.0 * 9.13953685760498
Epoch 2440, val loss: 0.9367596507072449
Epoch 2450, training loss: 457.89959716796875 = 0.9220877885818481 + 50.0 * 9.13955020904541
Epoch 2450, val loss: 0.9360100626945496
Epoch 2460, training loss: 458.0694274902344 = 0.9212092161178589 + 50.0 * 9.142964363098145
Epoch 2460, val loss: 0.93525230884552
Epoch 2470, training loss: 458.2483215332031 = 0.9203553795814514 + 50.0 * 9.146559715270996
Epoch 2470, val loss: 0.9345209002494812
Epoch 2480, training loss: 456.26031494140625 = 0.9194150567054749 + 50.0 * 9.106818199157715
Epoch 2480, val loss: 0.9336638450622559
Epoch 2490, training loss: 456.2090148925781 = 0.9186174869537354 + 50.0 * 9.10580825805664
Epoch 2490, val loss: 0.9330129623413086
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.5185507246376811
0.8125769760197059
=== training gcn model ===
Epoch 0, training loss: 513.6273803710938 = 1.0952403545379639 + 50.0 * 10.250643730163574
Epoch 0, val loss: 1.0961767435073853
Epoch 10, training loss: 493.51605224609375 = 1.0950815677642822 + 50.0 * 9.848419189453125
Epoch 10, val loss: 1.0959887504577637
Epoch 20, training loss: 484.7549743652344 = 1.094683051109314 + 50.0 * 9.673205375671387
Epoch 20, val loss: 1.095599889755249
Epoch 30, training loss: 478.0416564941406 = 1.0942655801773071 + 50.0 * 9.538948059082031
Epoch 30, val loss: 1.0951977968215942
Epoch 40, training loss: 472.9385986328125 = 1.0938353538513184 + 50.0 * 9.436895370483398
Epoch 40, val loss: 1.094791293144226
Epoch 50, training loss: 468.7359619140625 = 1.093422532081604 + 50.0 * 9.352850914001465
Epoch 50, val loss: 1.0943996906280518
Epoch 60, training loss: 465.1587219238281 = 1.0930171012878418 + 50.0 * 9.2813138961792
Epoch 60, val loss: 1.0940144062042236
Epoch 70, training loss: 462.0611877441406 = 1.0925827026367188 + 50.0 * 9.219371795654297
Epoch 70, val loss: 1.093602180480957
Epoch 80, training loss: 459.3578796386719 = 1.0921456813812256 + 50.0 * 9.165314674377441
Epoch 80, val loss: 1.0931867361068726
Epoch 90, training loss: 457.0399475097656 = 1.091707706451416 + 50.0 * 9.118965148925781
Epoch 90, val loss: 1.092771291732788
Epoch 100, training loss: 455.0178527832031 = 1.091265082359314 + 50.0 * 9.078531265258789
Epoch 100, val loss: 1.0923489332199097
Epoch 110, training loss: 453.24212646484375 = 1.090823769569397 + 50.0 * 9.043025970458984
Epoch 110, val loss: 1.0919301509857178
Epoch 120, training loss: 451.7518615722656 = 1.0903804302215576 + 50.0 * 9.013229370117188
Epoch 120, val loss: 1.0915156602859497
Epoch 130, training loss: 450.4534606933594 = 1.0900161266326904 + 50.0 * 8.987268447875977
Epoch 130, val loss: 1.0911821126937866
Epoch 140, training loss: 449.27520751953125 = 1.0897505283355713 + 50.0 * 8.963708877563477
Epoch 140, val loss: 1.090944766998291
Epoch 150, training loss: 448.29461669921875 = 1.089551568031311 + 50.0 * 8.944101333618164
Epoch 150, val loss: 1.0907716751098633
Epoch 160, training loss: 447.44134521484375 = 1.0893573760986328 + 50.0 * 8.927040100097656
Epoch 160, val loss: 1.0905938148498535
Epoch 170, training loss: 446.66357421875 = 1.0891551971435547 + 50.0 * 8.91148853302002
Epoch 170, val loss: 1.090417504310608
Epoch 180, training loss: 445.9615173339844 = 1.0889663696289062 + 50.0 * 8.897451400756836
Epoch 180, val loss: 1.0902459621429443
Epoch 190, training loss: 445.2976989746094 = 1.0887662172317505 + 50.0 * 8.884178161621094
Epoch 190, val loss: 1.0900671482086182
Epoch 200, training loss: 444.9052734375 = 1.0885679721832275 + 50.0 * 8.876334190368652
Epoch 200, val loss: 1.0898925065994263
Epoch 210, training loss: 444.35235595703125 = 1.0883623361587524 + 50.0 * 8.865280151367188
Epoch 210, val loss: 1.089704155921936
Epoch 220, training loss: 444.05908203125 = 1.0881474018096924 + 50.0 * 8.859418869018555
Epoch 220, val loss: 1.0895119905471802
Epoch 230, training loss: 443.74847412109375 = 1.0879589319229126 + 50.0 * 8.85321044921875
Epoch 230, val loss: 1.0893560647964478
Epoch 240, training loss: 443.6926574707031 = 1.0877702236175537 + 50.0 * 8.852097511291504
Epoch 240, val loss: 1.089173674583435
Epoch 250, training loss: 443.1046447753906 = 1.0875331163406372 + 50.0 * 8.84034252166748
Epoch 250, val loss: 1.0889590978622437
Epoch 260, training loss: 442.8813781738281 = 1.0872673988342285 + 50.0 * 8.835882186889648
Epoch 260, val loss: 1.0887092351913452
Epoch 270, training loss: 443.46917724609375 = 1.086983323097229 + 50.0 * 8.847643852233887
Epoch 270, val loss: 1.0884425640106201
Epoch 280, training loss: 443.6952819824219 = 1.086731195449829 + 50.0 * 8.852170944213867
Epoch 280, val loss: 1.0882173776626587
Epoch 290, training loss: 442.6798400878906 = 1.0864038467407227 + 50.0 * 8.831869125366211
Epoch 290, val loss: 1.0879042148590088
Epoch 300, training loss: 441.94873046875 = 1.086072564125061 + 50.0 * 8.817253112792969
Epoch 300, val loss: 1.0875966548919678
Epoch 310, training loss: 441.9823913574219 = 1.0857797861099243 + 50.0 * 8.81793212890625
Epoch 310, val loss: 1.0873197317123413
Epoch 320, training loss: 441.896728515625 = 1.08547043800354 + 50.0 * 8.816225051879883
Epoch 320, val loss: 1.087033987045288
Epoch 330, training loss: 441.91619873046875 = 1.0851573944091797 + 50.0 * 8.816620826721191
Epoch 330, val loss: 1.0867397785186768
Epoch 340, training loss: 441.99786376953125 = 1.0848262310028076 + 50.0 * 8.818260192871094
Epoch 340, val loss: 1.0864232778549194
Epoch 350, training loss: 441.7396240234375 = 1.0844429731369019 + 50.0 * 8.813103675842285
Epoch 350, val loss: 1.0860743522644043
Epoch 360, training loss: 442.22955322265625 = 1.0841461420059204 + 50.0 * 8.822908401489258
Epoch 360, val loss: 1.085791826248169
Epoch 370, training loss: 441.4963684082031 = 1.0837424993515015 + 50.0 * 8.808252334594727
Epoch 370, val loss: 1.0854226350784302
Epoch 380, training loss: 441.6874084472656 = 1.0834099054336548 + 50.0 * 8.812080383300781
Epoch 380, val loss: 1.0851041078567505
Epoch 390, training loss: 441.4458312988281 = 1.083014726638794 + 50.0 * 8.807256698608398
Epoch 390, val loss: 1.0847331285476685
Epoch 400, training loss: 441.61175537109375 = 1.0826491117477417 + 50.0 * 8.810582160949707
Epoch 400, val loss: 1.0843908786773682
Epoch 410, training loss: 441.681884765625 = 1.0822534561157227 + 50.0 * 8.811992645263672
Epoch 410, val loss: 1.0840200185775757
Epoch 420, training loss: 441.735595703125 = 1.0818512439727783 + 50.0 * 8.813075065612793
Epoch 420, val loss: 1.0836480855941772
Epoch 430, training loss: 441.72491455078125 = 1.081452488899231 + 50.0 * 8.81286907196045
Epoch 430, val loss: 1.0832723379135132
Epoch 440, training loss: 441.7196044921875 = 1.0810511112213135 + 50.0 * 8.81277084350586
Epoch 440, val loss: 1.0828990936279297
Epoch 450, training loss: 441.79986572265625 = 1.0806207656860352 + 50.0 * 8.814384460449219
Epoch 450, val loss: 1.082496166229248
Epoch 460, training loss: 441.7623291015625 = 1.0801939964294434 + 50.0 * 8.813642501831055
Epoch 460, val loss: 1.082091212272644
Epoch 470, training loss: 441.9447021484375 = 1.0797642469406128 + 50.0 * 8.817298889160156
Epoch 470, val loss: 1.0816816091537476
Epoch 480, training loss: 441.9371643066406 = 1.0793112516403198 + 50.0 * 8.817156791687012
Epoch 480, val loss: 1.0812667608261108
Epoch 490, training loss: 441.93463134765625 = 1.078856110572815 + 50.0 * 8.817115783691406
Epoch 490, val loss: 1.080828309059143
Epoch 500, training loss: 442.06365966796875 = 1.0783971548080444 + 50.0 * 8.81970500946045
Epoch 500, val loss: 1.080404281616211
Epoch 510, training loss: 442.12115478515625 = 1.0779175758361816 + 50.0 * 8.8208646774292
Epoch 510, val loss: 1.079953670501709
Epoch 520, training loss: 442.3202209472656 = 1.0774306058883667 + 50.0 * 8.82485580444336
Epoch 520, val loss: 1.0794986486434937
Epoch 530, training loss: 442.36083984375 = 1.0769412517547607 + 50.0 * 8.825677871704102
Epoch 530, val loss: 1.0790355205535889
Epoch 540, training loss: 442.36199951171875 = 1.0764498710632324 + 50.0 * 8.825711250305176
Epoch 540, val loss: 1.0785624980926514
Epoch 550, training loss: 442.22772216796875 = 1.0759358406066895 + 50.0 * 8.823036193847656
Epoch 550, val loss: 1.0780781507492065
Epoch 560, training loss: 442.4824523925781 = 1.0754443407058716 + 50.0 * 8.828140258789062
Epoch 560, val loss: 1.077623724937439
Epoch 570, training loss: 442.5351257324219 = 1.0749043226242065 + 50.0 * 8.829204559326172
Epoch 570, val loss: 1.0771158933639526
Epoch 580, training loss: 442.65704345703125 = 1.0743801593780518 + 50.0 * 8.831653594970703
Epoch 580, val loss: 1.0766189098358154
Epoch 590, training loss: 442.688232421875 = 1.0738508701324463 + 50.0 * 8.832287788391113
Epoch 590, val loss: 1.0761208534240723
Epoch 600, training loss: 442.4639587402344 = 1.0733121633529663 + 50.0 * 8.827813148498535
Epoch 600, val loss: 1.0756183862686157
Epoch 610, training loss: 442.74945068359375 = 1.0727719068527222 + 50.0 * 8.83353328704834
Epoch 610, val loss: 1.0751111507415771
Epoch 620, training loss: 443.0174255371094 = 1.0722533464431763 + 50.0 * 8.838903427124023
Epoch 620, val loss: 1.0746238231658936
Epoch 630, training loss: 443.0699462890625 = 1.0717028379440308 + 50.0 * 8.839964866638184
Epoch 630, val loss: 1.074104905128479
Epoch 640, training loss: 443.14532470703125 = 1.0711770057678223 + 50.0 * 8.841483116149902
Epoch 640, val loss: 1.0736064910888672
Epoch 650, training loss: 443.3763732910156 = 1.0706164836883545 + 50.0 * 8.846115112304688
Epoch 650, val loss: 1.073087215423584
Epoch 660, training loss: 443.4288330078125 = 1.070076584815979 + 50.0 * 8.847175598144531
Epoch 660, val loss: 1.0725685358047485
Epoch 670, training loss: 443.3695983886719 = 1.069516897201538 + 50.0 * 8.846001625061035
Epoch 670, val loss: 1.0720443725585938
Epoch 680, training loss: 443.45745849609375 = 1.0689657926559448 + 50.0 * 8.847769737243652
Epoch 680, val loss: 1.0715136528015137
Epoch 690, training loss: 443.5713806152344 = 1.0683939456939697 + 50.0 * 8.850059509277344
Epoch 690, val loss: 1.0709834098815918
Epoch 700, training loss: 443.5449523925781 = 1.0678201913833618 + 50.0 * 8.849542617797852
Epoch 700, val loss: 1.0704193115234375
Epoch 710, training loss: 443.0382385253906 = 1.0672156810760498 + 50.0 * 8.839420318603516
Epoch 710, val loss: 1.0698907375335693
Epoch 720, training loss: 443.4709167480469 = 1.066696047782898 + 50.0 * 8.848084449768066
Epoch 720, val loss: 1.0693806409835815
Epoch 730, training loss: 443.78839111328125 = 1.0661323070526123 + 50.0 * 8.854445457458496
Epoch 730, val loss: 1.0688505172729492
Epoch 740, training loss: 443.9281311035156 = 1.0655807256698608 + 50.0 * 8.857251167297363
Epoch 740, val loss: 1.0683283805847168
Epoch 750, training loss: 443.9560852050781 = 1.0650168657302856 + 50.0 * 8.857821464538574
Epoch 750, val loss: 1.0677980184555054
Epoch 760, training loss: 444.1580810546875 = 1.0644558668136597 + 50.0 * 8.861872673034668
Epoch 760, val loss: 1.0672707557678223
Epoch 770, training loss: 444.2193603515625 = 1.0638712644577026 + 50.0 * 8.863109588623047
Epoch 770, val loss: 1.0667189359664917
Epoch 780, training loss: 444.3401184082031 = 1.0632954835891724 + 50.0 * 8.8655366897583
Epoch 780, val loss: 1.0661920309066772
Epoch 790, training loss: 444.62359619140625 = 1.062731146812439 + 50.0 * 8.871216773986816
Epoch 790, val loss: 1.0656521320343018
Epoch 800, training loss: 443.8807373046875 = 1.0619523525238037 + 50.0 * 8.856375694274902
Epoch 800, val loss: 1.0648611783981323
Epoch 810, training loss: 445.7356872558594 = 1.06143319606781 + 50.0 * 8.893485069274902
Epoch 810, val loss: 1.0644716024398804
Epoch 820, training loss: 444.4779357910156 = 1.0608463287353516 + 50.0 * 8.868341445922852
Epoch 820, val loss: 1.0638397932052612
Epoch 830, training loss: 444.99700927734375 = 1.0602754354476929 + 50.0 * 8.878734588623047
Epoch 830, val loss: 1.0633268356323242
Epoch 840, training loss: 444.928466796875 = 1.0596823692321777 + 50.0 * 8.877375602722168
Epoch 840, val loss: 1.0627766847610474
Epoch 850, training loss: 445.1697082519531 = 1.0591106414794922 + 50.0 * 8.882211685180664
Epoch 850, val loss: 1.0622310638427734
Epoch 860, training loss: 445.55059814453125 = 1.0585284233093262 + 50.0 * 8.889841079711914
Epoch 860, val loss: 1.0616768598556519
Epoch 870, training loss: 446.0453796386719 = 1.0579395294189453 + 50.0 * 8.899748802185059
Epoch 870, val loss: 1.0611190795898438
Epoch 880, training loss: 446.1280822753906 = 1.0573302507400513 + 50.0 * 8.90141487121582
Epoch 880, val loss: 1.0605403184890747
Epoch 890, training loss: 446.3717346191406 = 1.0567008256912231 + 50.0 * 8.90630054473877
Epoch 890, val loss: 1.0599517822265625
Epoch 900, training loss: 446.71014404296875 = 1.0560941696166992 + 50.0 * 8.913081169128418
Epoch 900, val loss: 1.0593668222427368
Epoch 910, training loss: 446.66558837890625 = 1.0554485321044922 + 50.0 * 8.912202835083008
Epoch 910, val loss: 1.0587642192840576
Epoch 920, training loss: 446.9400939941406 = 1.0547995567321777 + 50.0 * 8.917705535888672
Epoch 920, val loss: 1.058156132698059
Epoch 930, training loss: 447.1504821777344 = 1.0541634559631348 + 50.0 * 8.921926498413086
Epoch 930, val loss: 1.0575429201126099
Epoch 940, training loss: 446.6243896484375 = 1.0534158945083618 + 50.0 * 8.911419868469238
Epoch 940, val loss: 1.0568549633026123
Epoch 950, training loss: 446.4905700683594 = 1.0526915788650513 + 50.0 * 8.908757209777832
Epoch 950, val loss: 1.0561469793319702
Epoch 960, training loss: 447.3677978515625 = 1.0521987676620483 + 50.0 * 8.926312446594238
Epoch 960, val loss: 1.0556979179382324
Epoch 970, training loss: 446.9721374511719 = 1.0515118837356567 + 50.0 * 8.918412208557129
Epoch 970, val loss: 1.0550779104232788
Epoch 980, training loss: 447.373046875 = 1.0509023666381836 + 50.0 * 8.926443099975586
Epoch 980, val loss: 1.0544846057891846
Epoch 990, training loss: 447.7919006347656 = 1.0502713918685913 + 50.0 * 8.934832572937012
Epoch 990, val loss: 1.0538846254348755
Epoch 1000, training loss: 447.70343017578125 = 1.049588918685913 + 50.0 * 8.933076858520508
Epoch 1000, val loss: 1.0532480478286743
Epoch 1010, training loss: 448.125244140625 = 1.048923373222351 + 50.0 * 8.941526412963867
Epoch 1010, val loss: 1.0526193380355835
Epoch 1020, training loss: 448.29278564453125 = 1.0482444763183594 + 50.0 * 8.944890975952148
Epoch 1020, val loss: 1.0519787073135376
Epoch 1030, training loss: 448.2881774902344 = 1.047541856765747 + 50.0 * 8.944812774658203
Epoch 1030, val loss: 1.0513125658035278
Epoch 1040, training loss: 448.350341796875 = 1.0468472242355347 + 50.0 * 8.946069717407227
Epoch 1040, val loss: 1.0506656169891357
Epoch 1050, training loss: 448.7031555175781 = 1.046141505241394 + 50.0 * 8.953140258789062
Epoch 1050, val loss: 1.0500049591064453
Epoch 1060, training loss: 448.68121337890625 = 1.0454113483428955 + 50.0 * 8.952715873718262
Epoch 1060, val loss: 1.0493230819702148
Epoch 1070, training loss: 448.78070068359375 = 1.044680118560791 + 50.0 * 8.954720497131348
Epoch 1070, val loss: 1.048609733581543
Epoch 1080, training loss: 448.7528076171875 = 1.0439282655715942 + 50.0 * 8.954177856445312
Epoch 1080, val loss: 1.0479168891906738
Epoch 1090, training loss: 448.9178466796875 = 1.0431578159332275 + 50.0 * 8.957493782043457
Epoch 1090, val loss: 1.0471895933151245
Epoch 1100, training loss: 449.18475341796875 = 1.0424401760101318 + 50.0 * 8.962845802307129
Epoch 1100, val loss: 1.0465198755264282
Epoch 1110, training loss: 449.3426818847656 = 1.0416966676712036 + 50.0 * 8.966019630432129
Epoch 1110, val loss: 1.0458147525787354
Epoch 1120, training loss: 449.70074462890625 = 1.040897250175476 + 50.0 * 8.973196983337402
Epoch 1120, val loss: 1.045098066329956
Epoch 1130, training loss: 449.3075866699219 = 1.0400577783584595 + 50.0 * 8.965350151062012
Epoch 1130, val loss: 1.0443223714828491
Epoch 1140, training loss: 448.386962890625 = 1.0392783880233765 + 50.0 * 8.946953773498535
Epoch 1140, val loss: 1.0435658693313599
Epoch 1150, training loss: 449.0862731933594 = 1.0385414361953735 + 50.0 * 8.960954666137695
Epoch 1150, val loss: 1.04287588596344
Epoch 1160, training loss: 449.58599853515625 = 1.0377838611602783 + 50.0 * 8.970964431762695
Epoch 1160, val loss: 1.042176604270935
Epoch 1170, training loss: 449.993896484375 = 1.037013053894043 + 50.0 * 8.979137420654297
Epoch 1170, val loss: 1.041446566581726
Epoch 1180, training loss: 450.04766845703125 = 1.036202311515808 + 50.0 * 8.980229377746582
Epoch 1180, val loss: 1.0407072305679321
Epoch 1190, training loss: 450.0411682128906 = 1.0353785753250122 + 50.0 * 8.98011589050293
Epoch 1190, val loss: 1.0399292707443237
Epoch 1200, training loss: 450.0119323730469 = 1.0345478057861328 + 50.0 * 8.979547500610352
Epoch 1200, val loss: 1.0391454696655273
Epoch 1210, training loss: 450.1048583984375 = 1.0337449312210083 + 50.0 * 8.981422424316406
Epoch 1210, val loss: 1.0383869409561157
Epoch 1220, training loss: 450.4837646484375 = 1.032975673675537 + 50.0 * 8.989015579223633
Epoch 1220, val loss: 1.037672519683838
Epoch 1230, training loss: 450.74169921875 = 1.0321837663650513 + 50.0 * 8.994190216064453
Epoch 1230, val loss: 1.0369234085083008
Epoch 1240, training loss: 450.70550537109375 = 1.0313576459884644 + 50.0 * 8.99348258972168
Epoch 1240, val loss: 1.0361483097076416
Epoch 1250, training loss: 450.8868713378906 = 1.0305465459823608 + 50.0 * 8.997126579284668
Epoch 1250, val loss: 1.0353847742080688
Epoch 1260, training loss: 451.1623840332031 = 1.029736876487732 + 50.0 * 9.002653121948242
Epoch 1260, val loss: 1.0346260070800781
Epoch 1270, training loss: 451.134033203125 = 1.0289099216461182 + 50.0 * 9.002102851867676
Epoch 1270, val loss: 1.0338380336761475
Epoch 1280, training loss: 451.3747863769531 = 1.0280776023864746 + 50.0 * 9.00693416595459
Epoch 1280, val loss: 1.0330451726913452
Epoch 1290, training loss: 450.6602478027344 = 1.0271483659744263 + 50.0 * 8.992661476135254
Epoch 1290, val loss: 1.0322132110595703
Epoch 1300, training loss: 450.64141845703125 = 1.0263527631759644 + 50.0 * 8.992300987243652
Epoch 1300, val loss: 1.0314358472824097
Epoch 1310, training loss: 450.6776123046875 = 1.02556312084198 + 50.0 * 8.993041038513184
Epoch 1310, val loss: 1.0307108163833618
Epoch 1320, training loss: 451.2195739746094 = 1.0247524976730347 + 50.0 * 9.003896713256836
Epoch 1320, val loss: 1.02996027469635
Epoch 1330, training loss: 451.6264953613281 = 1.0239531993865967 + 50.0 * 9.01205062866211
Epoch 1330, val loss: 1.0291978120803833
Epoch 1340, training loss: 451.5281982421875 = 1.0230687856674194 + 50.0 * 9.010102272033691
Epoch 1340, val loss: 1.028369665145874
Epoch 1350, training loss: 451.8830871582031 = 1.0222243070602417 + 50.0 * 9.017217636108398
Epoch 1350, val loss: 1.027580976486206
Epoch 1360, training loss: 452.0285949707031 = 1.021353006362915 + 50.0 * 9.02014446258545
Epoch 1360, val loss: 1.0267785787582397
Epoch 1370, training loss: 451.76104736328125 = 1.0204812288284302 + 50.0 * 9.014811515808105
Epoch 1370, val loss: 1.0259459018707275
Epoch 1380, training loss: 451.87506103515625 = 1.019615888595581 + 50.0 * 9.017108917236328
Epoch 1380, val loss: 1.0251526832580566
Epoch 1390, training loss: 452.28472900390625 = 1.0187815427780151 + 50.0 * 9.02531909942627
Epoch 1390, val loss: 1.0243682861328125
Epoch 1400, training loss: 452.5003967285156 = 1.0179083347320557 + 50.0 * 9.02964973449707
Epoch 1400, val loss: 1.0235477685928345
Epoch 1410, training loss: 452.5635070800781 = 1.0170259475708008 + 50.0 * 9.030929565429688
Epoch 1410, val loss: 1.0227056741714478
Epoch 1420, training loss: 452.48956298828125 = 1.0161432027816772 + 50.0 * 9.029468536376953
Epoch 1420, val loss: 1.0219066143035889
Epoch 1430, training loss: 452.7958984375 = 1.0152812004089355 + 50.0 * 9.035612106323242
Epoch 1430, val loss: 1.021092414855957
Epoch 1440, training loss: 452.9703369140625 = 1.0144128799438477 + 50.0 * 9.039118766784668
Epoch 1440, val loss: 1.0202703475952148
Epoch 1450, training loss: 452.7480773925781 = 1.0135092735290527 + 50.0 * 9.034690856933594
Epoch 1450, val loss: 1.01943039894104
Epoch 1460, training loss: 453.0194396972656 = 1.0126233100891113 + 50.0 * 9.040136337280273
Epoch 1460, val loss: 1.0185883045196533
Epoch 1470, training loss: 452.7494812011719 = 1.0116833448410034 + 50.0 * 9.03475570678711
Epoch 1470, val loss: 1.0177279710769653
Epoch 1480, training loss: 452.9259948730469 = 1.0108482837677002 + 50.0 * 9.03830337524414
Epoch 1480, val loss: 1.0169533491134644
Epoch 1490, training loss: 453.1444396972656 = 1.0099672079086304 + 50.0 * 9.042689323425293
Epoch 1490, val loss: 1.016125202178955
Epoch 1500, training loss: 453.1852722167969 = 1.009090781211853 + 50.0 * 9.043523788452148
Epoch 1500, val loss: 1.0152831077575684
Epoch 1510, training loss: 453.0287780761719 = 1.0081689357757568 + 50.0 * 9.040411949157715
Epoch 1510, val loss: 1.014435887336731
Epoch 1520, training loss: 453.3166198730469 = 1.0072742700576782 + 50.0 * 9.046187400817871
Epoch 1520, val loss: 1.0136216878890991
Epoch 1530, training loss: 453.5104064941406 = 1.0063804388046265 + 50.0 * 9.050080299377441
Epoch 1530, val loss: 1.0127743482589722
Epoch 1540, training loss: 453.46246337890625 = 1.005459189414978 + 50.0 * 9.049139976501465
Epoch 1540, val loss: 1.0119009017944336
Epoch 1550, training loss: 453.53582763671875 = 1.0045479536056519 + 50.0 * 9.050625801086426
Epoch 1550, val loss: 1.011063575744629
Epoch 1560, training loss: 453.714599609375 = 1.003627896308899 + 50.0 * 9.054219245910645
Epoch 1560, val loss: 1.0102121829986572
Epoch 1570, training loss: 453.8531188964844 = 1.0027154684066772 + 50.0 * 9.057007789611816
Epoch 1570, val loss: 1.0093499422073364
Epoch 1580, training loss: 453.8138427734375 = 1.0017859935760498 + 50.0 * 9.056241035461426
Epoch 1580, val loss: 1.0084880590438843
Epoch 1590, training loss: 453.79766845703125 = 1.0008227825164795 + 50.0 * 9.055936813354492
Epoch 1590, val loss: 1.007598638534546
Epoch 1600, training loss: 453.86004638671875 = 0.9998899698257446 + 50.0 * 9.05720329284668
Epoch 1600, val loss: 1.0067318677902222
Epoch 1610, training loss: 454.12847900390625 = 0.9989840388298035 + 50.0 * 9.062589645385742
Epoch 1610, val loss: 1.0058823823928833
Epoch 1620, training loss: 454.0291442871094 = 0.9980548620223999 + 50.0 * 9.060622215270996
Epoch 1620, val loss: 1.0050063133239746
Epoch 1630, training loss: 452.3734130859375 = 0.9968888163566589 + 50.0 * 9.027530670166016
Epoch 1630, val loss: 1.003911018371582
Epoch 1640, training loss: 453.0118103027344 = 0.9960362315177917 + 50.0 * 9.040315628051758
Epoch 1640, val loss: 1.003136157989502
Epoch 1650, training loss: 452.94805908203125 = 0.9951289892196655 + 50.0 * 9.039058685302734
Epoch 1650, val loss: 1.0022693872451782
Epoch 1660, training loss: 453.13629150390625 = 0.9942250847816467 + 50.0 * 9.042840957641602
Epoch 1660, val loss: 1.0014621019363403
Epoch 1670, training loss: 453.5496826171875 = 0.9934201240539551 + 50.0 * 9.051125526428223
Epoch 1670, val loss: 1.000698208808899
Epoch 1680, training loss: 453.9730224609375 = 0.992566704750061 + 50.0 * 9.059609413146973
Epoch 1680, val loss: 0.9999125003814697
Epoch 1690, training loss: 454.2688293457031 = 0.9916783571243286 + 50.0 * 9.065543174743652
Epoch 1690, val loss: 0.9990801215171814
Epoch 1700, training loss: 454.45062255859375 = 0.9907589554786682 + 50.0 * 9.069197654724121
Epoch 1700, val loss: 0.9982290863990784
Epoch 1710, training loss: 454.3210754394531 = 0.9898132681846619 + 50.0 * 9.066625595092773
Epoch 1710, val loss: 0.9973516464233398
Epoch 1720, training loss: 454.40191650390625 = 0.9888737797737122 + 50.0 * 9.06826114654541
Epoch 1720, val loss: 0.9964790344238281
Epoch 1730, training loss: 454.7572021484375 = 0.9879425764083862 + 50.0 * 9.075385093688965
Epoch 1730, val loss: 0.9956176280975342
Epoch 1740, training loss: 454.8202819824219 = 0.9869992733001709 + 50.0 * 9.076665878295898
Epoch 1740, val loss: 0.9947332739830017
Epoch 1750, training loss: 454.8641357421875 = 0.9860413074493408 + 50.0 * 9.077561378479004
Epoch 1750, val loss: 0.9938510060310364
Epoch 1760, training loss: 454.7159118652344 = 0.9850755333900452 + 50.0 * 9.074616432189941
Epoch 1760, val loss: 0.9929243922233582
Epoch 1770, training loss: 454.7622985839844 = 0.9841351509094238 + 50.0 * 9.075563430786133
Epoch 1770, val loss: 0.9920531511306763
Epoch 1780, training loss: 455.059326171875 = 0.9832099080085754 + 50.0 * 9.081521987915039
Epoch 1780, val loss: 0.9911869168281555
Epoch 1790, training loss: 455.0946960449219 = 0.9822619557380676 + 50.0 * 9.08224868774414
Epoch 1790, val loss: 0.9903132915496826
Epoch 1800, training loss: 454.99859619140625 = 0.9813272953033447 + 50.0 * 9.080345153808594
Epoch 1800, val loss: 0.9894323945045471
Epoch 1810, training loss: 455.153076171875 = 0.9803678393363953 + 50.0 * 9.083454132080078
Epoch 1810, val loss: 0.9885460138320923
Epoch 1820, training loss: 455.4771423339844 = 0.9794443249702454 + 50.0 * 9.089954376220703
Epoch 1820, val loss: 0.987705647945404
Epoch 1830, training loss: 455.68701171875 = 0.9785096049308777 + 50.0 * 9.094169616699219
Epoch 1830, val loss: 0.9868342280387878
Epoch 1840, training loss: 455.4801330566406 = 0.977515459060669 + 50.0 * 9.090052604675293
Epoch 1840, val loss: 0.985927164554596
Epoch 1850, training loss: 455.73492431640625 = 0.9765703082084656 + 50.0 * 9.09516716003418
Epoch 1850, val loss: 0.9850293397903442
Epoch 1860, training loss: 455.7698669433594 = 0.9756189584732056 + 50.0 * 9.095885276794434
Epoch 1860, val loss: 0.9841568470001221
Epoch 1870, training loss: 455.84307861328125 = 0.9746782183647156 + 50.0 * 9.097368240356445
Epoch 1870, val loss: 0.9832834005355835
Epoch 1880, training loss: 455.96343994140625 = 0.9737329483032227 + 50.0 * 9.099794387817383
Epoch 1880, val loss: 0.9824154376983643
Epoch 1890, training loss: 455.90020751953125 = 0.9727690815925598 + 50.0 * 9.098548889160156
Epoch 1890, val loss: 0.9815184473991394
Epoch 1900, training loss: 456.09576416015625 = 0.9718475341796875 + 50.0 * 9.10247802734375
Epoch 1900, val loss: 0.9806736707687378
Epoch 1910, training loss: 455.9565734863281 = 0.9708651900291443 + 50.0 * 9.099714279174805
Epoch 1910, val loss: 0.9797780513763428
Epoch 1920, training loss: 454.77191162109375 = 0.9697747230529785 + 50.0 * 9.076042175292969
Epoch 1920, val loss: 0.9788109660148621
Epoch 1930, training loss: 455.1868896484375 = 0.9689227342605591 + 50.0 * 9.084359169006348
Epoch 1930, val loss: 0.9779554009437561
Epoch 1940, training loss: 455.1261901855469 = 0.9680072665214539 + 50.0 * 9.083163261413574
Epoch 1940, val loss: 0.9770914316177368
Epoch 1950, training loss: 455.6949157714844 = 0.9671577215194702 + 50.0 * 9.094554901123047
Epoch 1950, val loss: 0.976341962814331
Epoch 1960, training loss: 456.1390686035156 = 0.9662688374519348 + 50.0 * 9.103455543518066
Epoch 1960, val loss: 0.9755262732505798
Epoch 1970, training loss: 456.479736328125 = 0.965358316898346 + 50.0 * 9.1102876663208
Epoch 1970, val loss: 0.9746711850166321
Epoch 1980, training loss: 456.46856689453125 = 0.964436411857605 + 50.0 * 9.110082626342773
Epoch 1980, val loss: 0.9738364219665527
Epoch 1990, training loss: 456.6849060058594 = 0.9635284543037415 + 50.0 * 9.11442756652832
Epoch 1990, val loss: 0.9729855060577393
Epoch 2000, training loss: 456.8323059082031 = 0.9625959992408752 + 50.0 * 9.11739444732666
Epoch 2000, val loss: 0.9721330404281616
Epoch 2010, training loss: 456.87158203125 = 0.9616646766662598 + 50.0 * 9.11819839477539
Epoch 2010, val loss: 0.971265435218811
Epoch 2020, training loss: 456.988525390625 = 0.9607369303703308 + 50.0 * 9.120555877685547
Epoch 2020, val loss: 0.9704061150550842
Epoch 2030, training loss: 456.9638977050781 = 0.9597983956336975 + 50.0 * 9.120081901550293
Epoch 2030, val loss: 0.969537615776062
Epoch 2040, training loss: 457.1277160644531 = 0.9588690400123596 + 50.0 * 9.123376846313477
Epoch 2040, val loss: 0.968660831451416
Epoch 2050, training loss: 457.0927429199219 = 0.9578856229782104 + 50.0 * 9.122696876525879
Epoch 2050, val loss: 0.9677678346633911
Epoch 2060, training loss: 457.4173889160156 = 0.9569615125656128 + 50.0 * 9.1292085647583
Epoch 2060, val loss: 0.9669215083122253
Epoch 2070, training loss: 457.69561767578125 = 0.9560511708259583 + 50.0 * 9.134791374206543
Epoch 2070, val loss: 0.9660688042640686
Epoch 2080, training loss: 457.8335266113281 = 0.9551162123680115 + 50.0 * 9.137568473815918
Epoch 2080, val loss: 0.9652148485183716
Epoch 2090, training loss: 457.7064208984375 = 0.9541741013526917 + 50.0 * 9.135045051574707
Epoch 2090, val loss: 0.9643316864967346
Epoch 2100, training loss: 457.8252868652344 = 0.9532573819160461 + 50.0 * 9.13744068145752
Epoch 2100, val loss: 0.9634928703308105
Epoch 2110, training loss: 458.0214538574219 = 0.9523370265960693 + 50.0 * 9.141382217407227
Epoch 2110, val loss: 0.9626651406288147
Epoch 2120, training loss: 457.8866882324219 = 0.9514104723930359 + 50.0 * 9.138705253601074
Epoch 2120, val loss: 0.9617487788200378
Epoch 2130, training loss: 456.7256774902344 = 0.9503846168518066 + 50.0 * 9.115506172180176
Epoch 2130, val loss: 0.960793673992157
Epoch 2140, training loss: 456.8099060058594 = 0.9495750665664673 + 50.0 * 9.117206573486328
Epoch 2140, val loss: 0.9600977301597595
Epoch 2150, training loss: 457.24322509765625 = 0.9486907124519348 + 50.0 * 9.125890731811523
Epoch 2150, val loss: 0.9592924118041992
Epoch 2160, training loss: 457.1452941894531 = 0.9478422999382019 + 50.0 * 9.12394905090332
Epoch 2160, val loss: 0.9585269689559937
Epoch 2170, training loss: 457.4934997558594 = 0.9470487833023071 + 50.0 * 9.130928993225098
Epoch 2170, val loss: 0.9578137397766113
Epoch 2180, training loss: 457.90167236328125 = 0.9462107419967651 + 50.0 * 9.13910961151123
Epoch 2180, val loss: 0.9570544362068176
Epoch 2190, training loss: 458.2024230957031 = 0.9453537464141846 + 50.0 * 9.1451416015625
Epoch 2190, val loss: 0.956265389919281
Epoch 2200, training loss: 458.21807861328125 = 0.9444441199302673 + 50.0 * 9.145472526550293
Epoch 2200, val loss: 0.9554190635681152
Epoch 2210, training loss: 458.3817138671875 = 0.9434663653373718 + 50.0 * 9.148764610290527
Epoch 2210, val loss: 0.9545556306838989
Epoch 2220, training loss: 457.91717529296875 = 0.9425414800643921 + 50.0 * 9.139492988586426
Epoch 2220, val loss: 0.9537039399147034
Epoch 2230, training loss: 458.25726318359375 = 0.941717803478241 + 50.0 * 9.146310806274414
Epoch 2230, val loss: 0.9529220461845398
Epoch 2240, training loss: 458.714599609375 = 0.9408576488494873 + 50.0 * 9.155474662780762
Epoch 2240, val loss: 0.9521446228027344
Epoch 2250, training loss: 459.07330322265625 = 0.9399959444999695 + 50.0 * 9.162666320800781
Epoch 2250, val loss: 0.951345682144165
Epoch 2260, training loss: 459.33050537109375 = 0.9391208291053772 + 50.0 * 9.167827606201172
Epoch 2260, val loss: 0.9505454897880554
Epoch 2270, training loss: 459.35980224609375 = 0.9382210373878479 + 50.0 * 9.168431282043457
Epoch 2270, val loss: 0.9497213363647461
Epoch 2280, training loss: 459.5027770996094 = 0.9373423457145691 + 50.0 * 9.171308517456055
Epoch 2280, val loss: 0.948914110660553
Epoch 2290, training loss: 459.69305419921875 = 0.9364654421806335 + 50.0 * 9.175131797790527
Epoch 2290, val loss: 0.9481083154678345
Epoch 2300, training loss: 459.7047424316406 = 0.9355807304382324 + 50.0 * 9.175383567810059
Epoch 2300, val loss: 0.9472925066947937
Epoch 2310, training loss: 459.7945251464844 = 0.9346969723701477 + 50.0 * 9.177196502685547
Epoch 2310, val loss: 0.9464858770370483
Epoch 2320, training loss: 459.8433532714844 = 0.9338253736495972 + 50.0 * 9.178190231323242
Epoch 2320, val loss: 0.945695161819458
Epoch 2330, training loss: 460.0193176269531 = 0.9329540729522705 + 50.0 * 9.181727409362793
Epoch 2330, val loss: 0.9449026584625244
Epoch 2340, training loss: 460.09808349609375 = 0.9320778846740723 + 50.0 * 9.183320045471191
Epoch 2340, val loss: 0.9441038966178894
Epoch 2350, training loss: 460.1083984375 = 0.9311879277229309 + 50.0 * 9.183544158935547
Epoch 2350, val loss: 0.9432966709136963
Epoch 2360, training loss: 460.1577453613281 = 0.9303316473960876 + 50.0 * 9.184548377990723
Epoch 2360, val loss: 0.9425033330917358
Epoch 2370, training loss: 460.3414306640625 = 0.9294474720954895 + 50.0 * 9.188240051269531
Epoch 2370, val loss: 0.9417065382003784
Epoch 2380, training loss: 460.2632141113281 = 0.9285664558410645 + 50.0 * 9.18669319152832
Epoch 2380, val loss: 0.9408888816833496
Epoch 2390, training loss: 460.4078674316406 = 0.9276851415634155 + 50.0 * 9.189603805541992
Epoch 2390, val loss: 0.9400867223739624
Epoch 2400, training loss: 460.4745788574219 = 0.9268117547035217 + 50.0 * 9.19095516204834
Epoch 2400, val loss: 0.9392773509025574
Epoch 2410, training loss: 460.4090881347656 = 0.9259063005447388 + 50.0 * 9.189663887023926
Epoch 2410, val loss: 0.938453197479248
Epoch 2420, training loss: 460.63238525390625 = 0.9250146150588989 + 50.0 * 9.194147109985352
Epoch 2420, val loss: 0.9376373887062073
Epoch 2430, training loss: 460.7332458496094 = 0.9241213202476501 + 50.0 * 9.196182250976562
Epoch 2430, val loss: 0.9368087649345398
Epoch 2440, training loss: 460.5489501953125 = 0.9231886267662048 + 50.0 * 9.19251537322998
Epoch 2440, val loss: 0.9359585642814636
Epoch 2450, training loss: 460.7441101074219 = 0.9222807288169861 + 50.0 * 9.196436882019043
Epoch 2450, val loss: 0.9351372122764587
Epoch 2460, training loss: 460.85064697265625 = 0.9213734865188599 + 50.0 * 9.198585510253906
Epoch 2460, val loss: 0.9343303442001343
Epoch 2470, training loss: 460.92327880859375 = 0.9204680919647217 + 50.0 * 9.200056076049805
Epoch 2470, val loss: 0.9335052967071533
Epoch 2480, training loss: 460.9767761230469 = 0.919567346572876 + 50.0 * 9.201144218444824
Epoch 2480, val loss: 0.9327065944671631
Epoch 2490, training loss: 461.0722351074219 = 0.9186817407608032 + 50.0 * 9.203071594238281
Epoch 2490, val loss: 0.9319046139717102
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.5191304347826087
0.8130116641309861
The final CL Acc:0.58435, 0.09264, The final GNN Acc:0.81057, 0.00315
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110940])
remove edge: torch.Size([2, 66452])
updated graph: torch.Size([2, 88744])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 508.92572021484375 = 1.093916893005371 + 50.0 * 10.156636238098145
Epoch 0, val loss: 1.0940543413162231
Epoch 10, training loss: 488.9243469238281 = 1.0934149026870728 + 50.0 * 9.75661849975586
Epoch 10, val loss: 1.0935496091842651
Epoch 20, training loss: 478.8696594238281 = 1.0928419828414917 + 50.0 * 9.555536270141602
Epoch 20, val loss: 1.0929911136627197
Epoch 30, training loss: 471.3135681152344 = 1.092287302017212 + 50.0 * 9.404425621032715
Epoch 30, val loss: 1.0924358367919922
Epoch 40, training loss: 465.1204833984375 = 1.0917335748672485 + 50.0 * 9.280574798583984
Epoch 40, val loss: 1.0918846130371094
Epoch 50, training loss: 460.12738037109375 = 1.0911808013916016 + 50.0 * 9.180724143981934
Epoch 50, val loss: 1.091333031654358
Epoch 60, training loss: 456.21038818359375 = 1.0906320810317993 + 50.0 * 9.102395057678223
Epoch 60, val loss: 1.090785264968872
Epoch 70, training loss: 453.1394958496094 = 1.0900541543960571 + 50.0 * 9.04098892211914
Epoch 70, val loss: 1.0902098417282104
Epoch 80, training loss: 450.64453125 = 1.08958101272583 + 50.0 * 8.99109935760498
Epoch 80, val loss: 1.0897377729415894
Epoch 90, training loss: 448.43646240234375 = 1.0891990661621094 + 50.0 * 8.946945190429688
Epoch 90, val loss: 1.0893723964691162
Epoch 100, training loss: 446.6000671386719 = 1.0888330936431885 + 50.0 * 8.910224914550781
Epoch 100, val loss: 1.0890036821365356
Epoch 110, training loss: 445.0377502441406 = 1.0884407758712769 + 50.0 * 8.878986358642578
Epoch 110, val loss: 1.0886117219924927
Epoch 120, training loss: 443.6971740722656 = 1.088039755821228 + 50.0 * 8.852182388305664
Epoch 120, val loss: 1.0882154703140259
Epoch 130, training loss: 442.5403747558594 = 1.0876429080963135 + 50.0 * 8.829054832458496
Epoch 130, val loss: 1.087823748588562
Epoch 140, training loss: 441.53765869140625 = 1.0872200727462769 + 50.0 * 8.809008598327637
Epoch 140, val loss: 1.087408185005188
Epoch 150, training loss: 440.63238525390625 = 1.086796760559082 + 50.0 * 8.790911674499512
Epoch 150, val loss: 1.0869883298873901
Epoch 160, training loss: 439.8853759765625 = 1.086366891860962 + 50.0 * 8.775979995727539
Epoch 160, val loss: 1.086561679840088
Epoch 170, training loss: 439.2467956542969 = 1.0859287977218628 + 50.0 * 8.763216972351074
Epoch 170, val loss: 1.0861223936080933
Epoch 180, training loss: 438.66778564453125 = 1.0854741334915161 + 50.0 * 8.751646041870117
Epoch 180, val loss: 1.0856789350509644
Epoch 190, training loss: 438.1087341308594 = 1.0849971771240234 + 50.0 * 8.740474700927734
Epoch 190, val loss: 1.0852022171020508
Epoch 200, training loss: 437.7676696777344 = 1.0845342874526978 + 50.0 * 8.733662605285645
Epoch 200, val loss: 1.084743857383728
Epoch 210, training loss: 437.274658203125 = 1.084044098854065 + 50.0 * 8.723812103271484
Epoch 210, val loss: 1.08426034450531
Epoch 220, training loss: 436.87939453125 = 1.083560824394226 + 50.0 * 8.715916633605957
Epoch 220, val loss: 1.0837805271148682
Epoch 230, training loss: 436.6490783691406 = 1.0830546617507935 + 50.0 * 8.711319923400879
Epoch 230, val loss: 1.0832717418670654
Epoch 240, training loss: 436.25701904296875 = 1.0825531482696533 + 50.0 * 8.703489303588867
Epoch 240, val loss: 1.0827813148498535
Epoch 250, training loss: 436.0294189453125 = 1.082046627998352 + 50.0 * 8.698946952819824
Epoch 250, val loss: 1.0822724103927612
Epoch 260, training loss: 435.890380859375 = 1.081518530845642 + 50.0 * 8.69617748260498
Epoch 260, val loss: 1.0817655324935913
Epoch 270, training loss: 435.6688537597656 = 1.0809868574142456 + 50.0 * 8.691757202148438
Epoch 270, val loss: 1.0812376737594604
Epoch 280, training loss: 435.5189208984375 = 1.0804554224014282 + 50.0 * 8.688769340515137
Epoch 280, val loss: 1.0807108879089355
Epoch 290, training loss: 435.426513671875 = 1.079911708831787 + 50.0 * 8.686931610107422
Epoch 290, val loss: 1.0801607370376587
Epoch 300, training loss: 435.4217834472656 = 1.0793567895889282 + 50.0 * 8.686848640441895
Epoch 300, val loss: 1.0796208381652832
Epoch 310, training loss: 435.2799377441406 = 1.0787917375564575 + 50.0 * 8.684022903442383
Epoch 310, val loss: 1.0790542364120483
Epoch 320, training loss: 435.24273681640625 = 1.078216552734375 + 50.0 * 8.683290481567383
Epoch 320, val loss: 1.0784929990768433
Epoch 330, training loss: 435.29937744140625 = 1.0776265859603882 + 50.0 * 8.68443489074707
Epoch 330, val loss: 1.07790207862854
Epoch 340, training loss: 435.35003662109375 = 1.0770020484924316 + 50.0 * 8.685461044311523
Epoch 340, val loss: 1.0772955417633057
Epoch 350, training loss: 435.0848693847656 = 1.0763477087020874 + 50.0 * 8.680170059204102
Epoch 350, val loss: 1.0766550302505493
Epoch 360, training loss: 435.2904357910156 = 1.0757359266281128 + 50.0 * 8.684293746948242
Epoch 360, val loss: 1.0760295391082764
Epoch 370, training loss: 435.1671142578125 = 1.0750932693481445 + 50.0 * 8.681839942932129
Epoch 370, val loss: 1.0753928422927856
Epoch 380, training loss: 434.9915466308594 = 1.0744085311889648 + 50.0 * 8.678342819213867
Epoch 380, val loss: 1.0747356414794922
Epoch 390, training loss: 435.2391662597656 = 1.0737823247909546 + 50.0 * 8.683307647705078
Epoch 390, val loss: 1.0741019248962402
Epoch 400, training loss: 435.291015625 = 1.07313072681427 + 50.0 * 8.684357643127441
Epoch 400, val loss: 1.0734584331512451
Epoch 410, training loss: 435.232666015625 = 1.0724470615386963 + 50.0 * 8.683204650878906
Epoch 410, val loss: 1.0727899074554443
Epoch 420, training loss: 435.4521179199219 = 1.0717717409133911 + 50.0 * 8.687606811523438
Epoch 420, val loss: 1.072119116783142
Epoch 430, training loss: 435.5738220214844 = 1.0710867643356323 + 50.0 * 8.690054893493652
Epoch 430, val loss: 1.0714372396469116
Epoch 440, training loss: 435.483154296875 = 1.0703794956207275 + 50.0 * 8.688255310058594
Epoch 440, val loss: 1.0707311630249023
Epoch 450, training loss: 435.5103759765625 = 1.0696611404418945 + 50.0 * 8.688814163208008
Epoch 450, val loss: 1.0700315237045288
Epoch 460, training loss: 435.5955810546875 = 1.0689489841461182 + 50.0 * 8.690532684326172
Epoch 460, val loss: 1.0693187713623047
Epoch 470, training loss: 435.69537353515625 = 1.0682309865951538 + 50.0 * 8.692543029785156
Epoch 470, val loss: 1.0686143636703491
Epoch 480, training loss: 435.7309875488281 = 1.0675026178359985 + 50.0 * 8.693269729614258
Epoch 480, val loss: 1.067883014678955
Epoch 490, training loss: 435.7862548828125 = 1.0667650699615479 + 50.0 * 8.694389343261719
Epoch 490, val loss: 1.0671555995941162
Epoch 500, training loss: 435.7687072753906 = 1.0660029649734497 + 50.0 * 8.694053649902344
Epoch 500, val loss: 1.0664091110229492
Epoch 510, training loss: 435.9302673339844 = 1.0652636289596558 + 50.0 * 8.69729995727539
Epoch 510, val loss: 1.0656771659851074
Epoch 520, training loss: 435.8290100097656 = 1.064487338066101 + 50.0 * 8.695290565490723
Epoch 520, val loss: 1.0649051666259766
Epoch 530, training loss: 435.8867492675781 = 1.0637156963348389 + 50.0 * 8.696460723876953
Epoch 530, val loss: 1.0641429424285889
Epoch 540, training loss: 435.9351806640625 = 1.0629308223724365 + 50.0 * 8.697444915771484
Epoch 540, val loss: 1.0633717775344849
Epoch 550, training loss: 436.0508728027344 = 1.0621395111083984 + 50.0 * 8.699774742126465
Epoch 550, val loss: 1.0625876188278198
Epoch 560, training loss: 435.9739685058594 = 1.061329960823059 + 50.0 * 8.69825267791748
Epoch 560, val loss: 1.0617984533309937
Epoch 570, training loss: 436.1370849609375 = 1.0605297088623047 + 50.0 * 8.701531410217285
Epoch 570, val loss: 1.060996651649475
Epoch 580, training loss: 436.0651550292969 = 1.059694528579712 + 50.0 * 8.700109481811523
Epoch 580, val loss: 1.0601695775985718
Epoch 590, training loss: 436.15924072265625 = 1.0588723421096802 + 50.0 * 8.702007293701172
Epoch 590, val loss: 1.0593678951263428
Epoch 600, training loss: 436.2224426269531 = 1.05803644657135 + 50.0 * 8.703288078308105
Epoch 600, val loss: 1.0585280656814575
Epoch 610, training loss: 436.2750244140625 = 1.0572177171707153 + 50.0 * 8.70435619354248
Epoch 610, val loss: 1.057710886001587
Epoch 620, training loss: 436.45654296875 = 1.0563470125198364 + 50.0 * 8.708003997802734
Epoch 620, val loss: 1.0568549633026123
Epoch 630, training loss: 436.67474365234375 = 1.0554865598678589 + 50.0 * 8.712385177612305
Epoch 630, val loss: 1.0560029745101929
Epoch 640, training loss: 436.7425231933594 = 1.0545942783355713 + 50.0 * 8.71375846862793
Epoch 640, val loss: 1.055118441581726
Epoch 650, training loss: 436.8086242675781 = 1.0537109375 + 50.0 * 8.71509838104248
Epoch 650, val loss: 1.054247260093689
Epoch 660, training loss: 436.9736022949219 = 1.0528392791748047 + 50.0 * 8.718415260314941
Epoch 660, val loss: 1.0533727407455444
Epoch 670, training loss: 437.1336669921875 = 1.0519226789474487 + 50.0 * 8.721634864807129
Epoch 670, val loss: 1.0524357557296753
Epoch 680, training loss: 436.8361511230469 = 1.0509624481201172 + 50.0 * 8.715703964233398
Epoch 680, val loss: 1.0515260696411133
Epoch 690, training loss: 437.186279296875 = 1.050076961517334 + 50.0 * 8.722723960876465
Epoch 690, val loss: 1.0506336688995361
Epoch 700, training loss: 437.4478454589844 = 1.0491585731506348 + 50.0 * 8.727973937988281
Epoch 700, val loss: 1.0497289896011353
Epoch 710, training loss: 437.3414001464844 = 1.0481998920440674 + 50.0 * 8.72586441040039
Epoch 710, val loss: 1.0487748384475708
Epoch 720, training loss: 437.49462890625 = 1.0472452640533447 + 50.0 * 8.728947639465332
Epoch 720, val loss: 1.0478335618972778
Epoch 730, training loss: 437.6872863769531 = 1.0463051795959473 + 50.0 * 8.732819557189941
Epoch 730, val loss: 1.0469187498092651
Epoch 740, training loss: 437.7171936035156 = 1.0453208684921265 + 50.0 * 8.733437538146973
Epoch 740, val loss: 1.0459214448928833
Epoch 750, training loss: 437.66168212890625 = 1.0443267822265625 + 50.0 * 8.73234748840332
Epoch 750, val loss: 1.0449707508087158
Epoch 760, training loss: 438.04608154296875 = 1.043345332145691 + 50.0 * 8.740055084228516
Epoch 760, val loss: 1.043993592262268
Epoch 770, training loss: 438.1888427734375 = 1.0423556566238403 + 50.0 * 8.742929458618164
Epoch 770, val loss: 1.0430201292037964
Epoch 780, training loss: 438.20361328125 = 1.0413259267807007 + 50.0 * 8.743246078491211
Epoch 780, val loss: 1.0420145988464355
Epoch 790, training loss: 438.3428649902344 = 1.040323257446289 + 50.0 * 8.746050834655762
Epoch 790, val loss: 1.0410101413726807
Epoch 800, training loss: 438.44293212890625 = 1.0393033027648926 + 50.0 * 8.748072624206543
Epoch 800, val loss: 1.04001784324646
Epoch 810, training loss: 438.314208984375 = 1.0382425785064697 + 50.0 * 8.745519638061523
Epoch 810, val loss: 1.0389701128005981
Epoch 820, training loss: 438.5533447265625 = 1.0371795892715454 + 50.0 * 8.750323295593262
Epoch 820, val loss: 1.0379279851913452
Epoch 830, training loss: 438.6258239746094 = 1.0361437797546387 + 50.0 * 8.75179386138916
Epoch 830, val loss: 1.0369040966033936
Epoch 840, training loss: 438.61163330078125 = 1.0350598096847534 + 50.0 * 8.751531600952148
Epoch 840, val loss: 1.035826563835144
Epoch 850, training loss: 438.6383361816406 = 1.0339670181274414 + 50.0 * 8.752087593078613
Epoch 850, val loss: 1.0347496271133423
Epoch 860, training loss: 438.7409973144531 = 1.032887578010559 + 50.0 * 8.754161834716797
Epoch 860, val loss: 1.0337022542953491
Epoch 870, training loss: 438.9140625 = 1.031783103942871 + 50.0 * 8.757645606994629
Epoch 870, val loss: 1.0326007604599
Epoch 880, training loss: 439.01507568359375 = 1.0306881666183472 + 50.0 * 8.759687423706055
Epoch 880, val loss: 1.031554102897644
Epoch 890, training loss: 439.1169738769531 = 1.0295920372009277 + 50.0 * 8.761747360229492
Epoch 890, val loss: 1.0304750204086304
Epoch 900, training loss: 439.43572998046875 = 1.0285120010375977 + 50.0 * 8.768144607543945
Epoch 900, val loss: 1.0293995141983032
Epoch 910, training loss: 439.346923828125 = 1.0273653268814087 + 50.0 * 8.766390800476074
Epoch 910, val loss: 1.0282881259918213
Epoch 920, training loss: 439.48846435546875 = 1.0262377262115479 + 50.0 * 8.769244194030762
Epoch 920, val loss: 1.0271806716918945
Epoch 930, training loss: 439.658935546875 = 1.0251250267028809 + 50.0 * 8.772676467895508
Epoch 930, val loss: 1.0260775089263916
Epoch 940, training loss: 439.5525817871094 = 1.023944616317749 + 50.0 * 8.770572662353516
Epoch 940, val loss: 1.0249302387237549
Epoch 950, training loss: 439.7491149902344 = 1.0227895975112915 + 50.0 * 8.774526596069336
Epoch 950, val loss: 1.0237880945205688
Epoch 960, training loss: 439.3247375488281 = 1.021609902381897 + 50.0 * 8.76606273651123
Epoch 960, val loss: 1.022642731666565
Epoch 970, training loss: 440.1118469238281 = 1.0205919742584229 + 50.0 * 8.781825065612793
Epoch 970, val loss: 1.0215964317321777
Epoch 980, training loss: 439.54217529296875 = 1.0191631317138672 + 50.0 * 8.77046012878418
Epoch 980, val loss: 1.0202282667160034
Epoch 990, training loss: 439.7916564941406 = 1.018041729927063 + 50.0 * 8.775472640991211
Epoch 990, val loss: 1.0191328525543213
Epoch 1000, training loss: 439.9612121582031 = 1.0168887376785278 + 50.0 * 8.778886795043945
Epoch 1000, val loss: 1.0179986953735352
Epoch 1010, training loss: 440.1349792480469 = 1.0157501697540283 + 50.0 * 8.782384872436523
Epoch 1010, val loss: 1.016884684562683
Epoch 1020, training loss: 440.27813720703125 = 1.014573574066162 + 50.0 * 8.785270690917969
Epoch 1020, val loss: 1.015734314918518
Epoch 1030, training loss: 440.59173583984375 = 1.0134224891662598 + 50.0 * 8.791565895080566
Epoch 1030, val loss: 1.014589786529541
Epoch 1040, training loss: 440.67156982421875 = 1.0122202634811401 + 50.0 * 8.793187141418457
Epoch 1040, val loss: 1.0134209394454956
Epoch 1050, training loss: 440.551025390625 = 1.0109832286834717 + 50.0 * 8.790801048278809
Epoch 1050, val loss: 1.0122005939483643
Epoch 1060, training loss: 440.7264709472656 = 1.0097495317459106 + 50.0 * 8.794334411621094
Epoch 1060, val loss: 1.0109891891479492
Epoch 1070, training loss: 440.767578125 = 1.0085265636444092 + 50.0 * 8.795181274414062
Epoch 1070, val loss: 1.0097895860671997
Epoch 1080, training loss: 441.0079345703125 = 1.0072999000549316 + 50.0 * 8.800012588500977
Epoch 1080, val loss: 1.0085780620574951
Epoch 1090, training loss: 441.3198547363281 = 1.0060523748397827 + 50.0 * 8.806276321411133
Epoch 1090, val loss: 1.0073689222335815
Epoch 1100, training loss: 441.233154296875 = 1.0048062801361084 + 50.0 * 8.804567337036133
Epoch 1100, val loss: 1.006133794784546
Epoch 1110, training loss: 441.3838806152344 = 1.003529667854309 + 50.0 * 8.80760669708252
Epoch 1110, val loss: 1.0049126148223877
Epoch 1120, training loss: 441.57293701171875 = 1.00227952003479 + 50.0 * 8.811412811279297
Epoch 1120, val loss: 1.0036472082138062
Epoch 1130, training loss: 441.5660705566406 = 1.000957727432251 + 50.0 * 8.811302185058594
Epoch 1130, val loss: 1.0023646354675293
Epoch 1140, training loss: 441.2327575683594 = 0.9995707869529724 + 50.0 * 8.80466365814209
Epoch 1140, val loss: 1.0010435581207275
Epoch 1150, training loss: 441.61041259765625 = 0.9983083009719849 + 50.0 * 8.812241554260254
Epoch 1150, val loss: 0.9997866749763489
Epoch 1160, training loss: 441.858154296875 = 0.9970108270645142 + 50.0 * 8.817222595214844
Epoch 1160, val loss: 0.9985160827636719
Epoch 1170, training loss: 442.096923828125 = 0.9956963658332825 + 50.0 * 8.82202434539795
Epoch 1170, val loss: 0.997228741645813
Epoch 1180, training loss: 442.09649658203125 = 0.9943366050720215 + 50.0 * 8.822043418884277
Epoch 1180, val loss: 0.9958797693252563
Epoch 1190, training loss: 441.985595703125 = 0.9929149746894836 + 50.0 * 8.819853782653809
Epoch 1190, val loss: 0.9944709539413452
Epoch 1200, training loss: 441.8868103027344 = 0.9915590286254883 + 50.0 * 8.81790542602539
Epoch 1200, val loss: 0.9931533336639404
Epoch 1210, training loss: 442.2376708984375 = 0.9901859164237976 + 50.0 * 8.824950218200684
Epoch 1210, val loss: 0.9918285608291626
Epoch 1220, training loss: 442.58319091796875 = 0.9888219833374023 + 50.0 * 8.831887245178223
Epoch 1220, val loss: 0.9904637336730957
Epoch 1230, training loss: 442.5231018066406 = 0.9874018430709839 + 50.0 * 8.830714225769043
Epoch 1230, val loss: 0.9890791773796082
Epoch 1240, training loss: 442.0636291503906 = 0.9859406352043152 + 50.0 * 8.821554183959961
Epoch 1240, val loss: 0.9877321124076843
Epoch 1250, training loss: 442.0083923339844 = 0.9844555258750916 + 50.0 * 8.820478439331055
Epoch 1250, val loss: 0.9861607551574707
Epoch 1260, training loss: 442.05609130859375 = 0.9829834699630737 + 50.0 * 8.821462631225586
Epoch 1260, val loss: 0.984730064868927
Epoch 1270, training loss: 442.7157287597656 = 0.98142409324646 + 50.0 * 8.834686279296875
Epoch 1270, val loss: 0.9832310676574707
Epoch 1280, training loss: 442.0035400390625 = 0.9799162149429321 + 50.0 * 8.820472717285156
Epoch 1280, val loss: 0.9817795157432556
Epoch 1290, training loss: 442.18060302734375 = 0.9785319566726685 + 50.0 * 8.824041366577148
Epoch 1290, val loss: 0.9803895354270935
Epoch 1300, training loss: 442.67681884765625 = 0.9770678281784058 + 50.0 * 8.83399486541748
Epoch 1300, val loss: 0.9789531230926514
Epoch 1310, training loss: 443.02642822265625 = 0.975563645362854 + 50.0 * 8.841017723083496
Epoch 1310, val loss: 0.977479100227356
Epoch 1320, training loss: 443.1701965332031 = 0.9739813804626465 + 50.0 * 8.843924522399902
Epoch 1320, val loss: 0.9759544730186462
Epoch 1330, training loss: 443.3013610839844 = 0.9723724126815796 + 50.0 * 8.846579551696777
Epoch 1330, val loss: 0.9743592143058777
Epoch 1340, training loss: 443.5023193359375 = 0.9707277417182922 + 50.0 * 8.850631713867188
Epoch 1340, val loss: 0.9727795124053955
Epoch 1350, training loss: 443.7657165527344 = 0.9690457582473755 + 50.0 * 8.85593318939209
Epoch 1350, val loss: 0.9711326360702515
Epoch 1360, training loss: 443.633544921875 = 0.9672704935073853 + 50.0 * 8.853324890136719
Epoch 1360, val loss: 0.9694347977638245
Epoch 1370, training loss: 443.28143310546875 = 0.9653188586235046 + 50.0 * 8.846322059631348
Epoch 1370, val loss: 0.9674453139305115
Epoch 1380, training loss: 443.2772216796875 = 0.9635570645332336 + 50.0 * 8.846273422241211
Epoch 1380, val loss: 0.9657737016677856
Epoch 1390, training loss: 442.9508972167969 = 0.9616701602935791 + 50.0 * 8.839784622192383
Epoch 1390, val loss: 0.963970422744751
Epoch 1400, training loss: 443.1202087402344 = 0.9598178267478943 + 50.0 * 8.843208312988281
Epoch 1400, val loss: 0.9621453881263733
Epoch 1410, training loss: 443.2672119140625 = 0.9580056071281433 + 50.0 * 8.846183776855469
Epoch 1410, val loss: 0.9603418111801147
Epoch 1420, training loss: 443.6676940917969 = 0.9561998248100281 + 50.0 * 8.854229927062988
Epoch 1420, val loss: 0.9585564136505127
Epoch 1430, training loss: 443.9591064453125 = 0.9543497562408447 + 50.0 * 8.860095024108887
Epoch 1430, val loss: 0.9567472338676453
Epoch 1440, training loss: 444.2015075683594 = 0.9525079131126404 + 50.0 * 8.86497974395752
Epoch 1440, val loss: 0.954933226108551
Epoch 1450, training loss: 444.3169250488281 = 0.9506299495697021 + 50.0 * 8.867325782775879
Epoch 1450, val loss: 0.9530985951423645
Epoch 1460, training loss: 444.4888610839844 = 0.9487472772598267 + 50.0 * 8.87080192565918
Epoch 1460, val loss: 0.9512616395950317
Epoch 1470, training loss: 444.56402587890625 = 0.9468276500701904 + 50.0 * 8.872344017028809
Epoch 1470, val loss: 0.9493708610534668
Epoch 1480, training loss: 444.682373046875 = 0.9449025392532349 + 50.0 * 8.874749183654785
Epoch 1480, val loss: 0.9474835395812988
Epoch 1490, training loss: 444.9226989746094 = 0.9430059790611267 + 50.0 * 8.879593849182129
Epoch 1490, val loss: 0.9456103444099426
Epoch 1500, training loss: 444.9909362792969 = 0.9410359263420105 + 50.0 * 8.880997657775879
Epoch 1500, val loss: 0.9437300562858582
Epoch 1510, training loss: 444.88848876953125 = 0.9390177726745605 + 50.0 * 8.878989219665527
Epoch 1510, val loss: 0.9417500495910645
Epoch 1520, training loss: 444.8616027832031 = 0.9370605945587158 + 50.0 * 8.878490447998047
Epoch 1520, val loss: 0.9398086667060852
Epoch 1530, training loss: 445.0526428222656 = 0.9350506663322449 + 50.0 * 8.882351875305176
Epoch 1530, val loss: 0.9378488659858704
Epoch 1540, training loss: 445.1842041015625 = 0.9330411553382874 + 50.0 * 8.88502311706543
Epoch 1540, val loss: 0.9358788132667542
Epoch 1550, training loss: 445.2544250488281 = 0.9310187697410583 + 50.0 * 8.886467933654785
Epoch 1550, val loss: 0.9339230060577393
Epoch 1560, training loss: 445.3065490722656 = 0.9289922118186951 + 50.0 * 8.887551307678223
Epoch 1560, val loss: 0.9319420456886292
Epoch 1570, training loss: 445.6014709472656 = 0.9269742369651794 + 50.0 * 8.893489837646484
Epoch 1570, val loss: 0.9299600124359131
Epoch 1580, training loss: 445.7856750488281 = 0.9249268174171448 + 50.0 * 8.897214889526367
Epoch 1580, val loss: 0.9279734492301941
Epoch 1590, training loss: 445.8548278808594 = 0.9228569865226746 + 50.0 * 8.898639678955078
Epoch 1590, val loss: 0.9259533882141113
Epoch 1600, training loss: 446.0454406738281 = 0.9207836389541626 + 50.0 * 8.902493476867676
Epoch 1600, val loss: 0.9239044785499573
Epoch 1610, training loss: 445.9676208496094 = 0.918667197227478 + 50.0 * 8.900979042053223
Epoch 1610, val loss: 0.9218573570251465
Epoch 1620, training loss: 446.1427001953125 = 0.9165706634521484 + 50.0 * 8.904522895812988
Epoch 1620, val loss: 0.9197858572006226
Epoch 1630, training loss: 446.3655700683594 = 0.9144507646560669 + 50.0 * 8.909022331237793
Epoch 1630, val loss: 0.9177279472351074
Epoch 1640, training loss: 446.40850830078125 = 0.9123178124427795 + 50.0 * 8.909923553466797
Epoch 1640, val loss: 0.9156339764595032
Epoch 1650, training loss: 446.4655456542969 = 0.9101810455322266 + 50.0 * 8.911107063293457
Epoch 1650, val loss: 0.9135469794273376
Epoch 1660, training loss: 446.662841796875 = 0.9080201983451843 + 50.0 * 8.915096282958984
Epoch 1660, val loss: 0.9114551544189453
Epoch 1670, training loss: 445.8166198730469 = 0.9056590795516968 + 50.0 * 8.898219108581543
Epoch 1670, val loss: 0.908972978591919
Epoch 1680, training loss: 446.1455078125 = 0.9033942222595215 + 50.0 * 8.904842376708984
Epoch 1680, val loss: 0.9068644642829895
Epoch 1690, training loss: 445.7394104003906 = 0.9011698365211487 + 50.0 * 8.896764755249023
Epoch 1690, val loss: 0.9047735929489136
Epoch 1700, training loss: 446.1304931640625 = 0.89911288022995 + 50.0 * 8.904627799987793
Epoch 1700, val loss: 0.9027610421180725
Epoch 1710, training loss: 446.55584716796875 = 0.8969876170158386 + 50.0 * 8.913177490234375
Epoch 1710, val loss: 0.9006888270378113
Epoch 1720, training loss: 446.9166564941406 = 0.894808292388916 + 50.0 * 8.92043685913086
Epoch 1720, val loss: 0.8985620737075806
Epoch 1730, training loss: 447.13818359375 = 0.8925947546958923 + 50.0 * 8.924911499023438
Epoch 1730, val loss: 0.896431565284729
Epoch 1740, training loss: 447.18218994140625 = 0.8903462886810303 + 50.0 * 8.925836563110352
Epoch 1740, val loss: 0.8942192792892456
Epoch 1750, training loss: 447.38671875 = 0.8881118297576904 + 50.0 * 8.929971694946289
Epoch 1750, val loss: 0.892032265663147
Epoch 1760, training loss: 447.4068908691406 = 0.8858426213264465 + 50.0 * 8.930420875549316
Epoch 1760, val loss: 0.8898525834083557
Epoch 1770, training loss: 447.4590148925781 = 0.8835843801498413 + 50.0 * 8.931509017944336
Epoch 1770, val loss: 0.8876214027404785
Epoch 1780, training loss: 447.4916687011719 = 0.881269097328186 + 50.0 * 8.932208061218262
Epoch 1780, val loss: 0.8853745460510254
Epoch 1790, training loss: 447.4866027832031 = 0.8789582848548889 + 50.0 * 8.93215274810791
Epoch 1790, val loss: 0.8831170797348022
Epoch 1800, training loss: 447.7300109863281 = 0.8766452670097351 + 50.0 * 8.937067031860352
Epoch 1800, val loss: 0.8808860182762146
Epoch 1810, training loss: 447.41864013671875 = 0.8742772340774536 + 50.0 * 8.930887222290039
Epoch 1810, val loss: 0.8785150051116943
Epoch 1820, training loss: 447.2669677734375 = 0.8720101118087769 + 50.0 * 8.927899360656738
Epoch 1820, val loss: 0.8763595819473267
Epoch 1830, training loss: 447.6347351074219 = 0.8696726560592651 + 50.0 * 8.935301780700684
Epoch 1830, val loss: 0.8740712404251099
Epoch 1840, training loss: 448.02679443359375 = 0.8673847317695618 + 50.0 * 8.943188667297363
Epoch 1840, val loss: 0.8718542456626892
Epoch 1850, training loss: 448.0790710449219 = 0.8650749325752258 + 50.0 * 8.944279670715332
Epoch 1850, val loss: 0.8695855140686035
Epoch 1860, training loss: 448.0124816894531 = 0.862735390663147 + 50.0 * 8.942995071411133
Epoch 1860, val loss: 0.8672921061515808
Epoch 1870, training loss: 447.5464782714844 = 0.86031574010849 + 50.0 * 8.933723449707031
Epoch 1870, val loss: 0.8649237751960754
Epoch 1880, training loss: 447.247314453125 = 0.8578990697860718 + 50.0 * 8.927787780761719
Epoch 1880, val loss: 0.8626647591590881
Epoch 1890, training loss: 447.56597900390625 = 0.8556023240089417 + 50.0 * 8.934207916259766
Epoch 1890, val loss: 0.8604152798652649
Epoch 1900, training loss: 447.74053955078125 = 0.85332852602005 + 50.0 * 8.937744140625
Epoch 1900, val loss: 0.8581625819206238
Epoch 1910, training loss: 448.1774597167969 = 0.8509690761566162 + 50.0 * 8.946529388427734
Epoch 1910, val loss: 0.8558633327484131
Epoch 1920, training loss: 448.49578857421875 = 0.8486143946647644 + 50.0 * 8.952943801879883
Epoch 1920, val loss: 0.8535532355308533
Epoch 1930, training loss: 448.4443664550781 = 0.8462006449699402 + 50.0 * 8.951963424682617
Epoch 1930, val loss: 0.8511777520179749
Epoch 1940, training loss: 448.5702209472656 = 0.8437913060188293 + 50.0 * 8.95452880859375
Epoch 1940, val loss: 0.8488534688949585
Epoch 1950, training loss: 448.77392578125 = 0.84139484167099 + 50.0 * 8.958650588989258
Epoch 1950, val loss: 0.8465321660041809
Epoch 1960, training loss: 448.7284851074219 = 0.8389581441879272 + 50.0 * 8.95779037475586
Epoch 1960, val loss: 0.8441489934921265
Epoch 1970, training loss: 448.7373962402344 = 0.8365235328674316 + 50.0 * 8.958017349243164
Epoch 1970, val loss: 0.8417909741401672
Epoch 1980, training loss: 448.83624267578125 = 0.834115743637085 + 50.0 * 8.960042953491211
Epoch 1980, val loss: 0.8394491672515869
Epoch 1990, training loss: 448.828369140625 = 0.8316888809204102 + 50.0 * 8.959933280944824
Epoch 1990, val loss: 0.8371001482009888
Epoch 2000, training loss: 448.9075927734375 = 0.8292625546455383 + 50.0 * 8.961566925048828
Epoch 2000, val loss: 0.8347193002700806
Epoch 2010, training loss: 448.95770263671875 = 0.8268153071403503 + 50.0 * 8.962617874145508
Epoch 2010, val loss: 0.8323527574539185
Epoch 2020, training loss: 448.9478454589844 = 0.8243421316146851 + 50.0 * 8.962470054626465
Epoch 2020, val loss: 0.8299373388290405
Epoch 2030, training loss: 448.4069519042969 = 0.8218147158622742 + 50.0 * 8.951703071594238
Epoch 2030, val loss: 0.827484667301178
Epoch 2040, training loss: 448.9076843261719 = 0.8194125294685364 + 50.0 * 8.96176528930664
Epoch 2040, val loss: 0.825177788734436
Epoch 2050, training loss: 449.09722900390625 = 0.8169863224029541 + 50.0 * 8.965604782104492
Epoch 2050, val loss: 0.8228024244308472
Epoch 2060, training loss: 449.2488098144531 = 0.8145430088043213 + 50.0 * 8.968685150146484
Epoch 2060, val loss: 0.8204420208930969
Epoch 2070, training loss: 449.2182922363281 = 0.8120592832565308 + 50.0 * 8.968124389648438
Epoch 2070, val loss: 0.8180270195007324
Epoch 2080, training loss: 449.14215087890625 = 0.8095815777778625 + 50.0 * 8.966651916503906
Epoch 2080, val loss: 0.8156179189682007
Epoch 2090, training loss: 449.1547546386719 = 0.8070892691612244 + 50.0 * 8.96695327758789
Epoch 2090, val loss: 0.8132340312004089
Epoch 2100, training loss: 448.9414978027344 = 0.8045468926429749 + 50.0 * 8.962738990783691
Epoch 2100, val loss: 0.810738742351532
Epoch 2110, training loss: 449.2454833984375 = 0.8020447492599487 + 50.0 * 8.96886920928955
Epoch 2110, val loss: 0.808332622051239
Epoch 2120, training loss: 449.57891845703125 = 0.7996198534965515 + 50.0 * 8.9755859375
Epoch 2120, val loss: 0.8059987425804138
Epoch 2130, training loss: 449.6114807128906 = 0.7971659302711487 + 50.0 * 8.976285934448242
Epoch 2130, val loss: 0.8036176562309265
Epoch 2140, training loss: 449.6060791015625 = 0.7946866154670715 + 50.0 * 8.976227760314941
Epoch 2140, val loss: 0.8012371063232422
Epoch 2150, training loss: 449.70599365234375 = 0.7922436594963074 + 50.0 * 8.978275299072266
Epoch 2150, val loss: 0.7988110184669495
Epoch 2160, training loss: 449.8662109375 = 0.7897946238517761 + 50.0 * 8.981528282165527
Epoch 2160, val loss: 0.7964790463447571
Epoch 2170, training loss: 449.83111572265625 = 0.7873174548149109 + 50.0 * 8.980875968933105
Epoch 2170, val loss: 0.794098973274231
Epoch 2180, training loss: 449.90301513671875 = 0.7848324775695801 + 50.0 * 8.9823637008667
Epoch 2180, val loss: 0.7916755676269531
Epoch 2190, training loss: 451.2250671386719 = 0.7822885513305664 + 50.0 * 9.008855819702148
Epoch 2190, val loss: 0.7892455458641052
Epoch 2200, training loss: 439.45953369140625 = 0.7776709198951721 + 50.0 * 8.773636817932129
Epoch 2200, val loss: 0.7841055393218994
Epoch 2210, training loss: 438.93902587890625 = 0.775432825088501 + 50.0 * 8.763272285461426
Epoch 2210, val loss: 0.782494306564331
Epoch 2220, training loss: 441.9163513183594 = 0.7732246518135071 + 50.0 * 8.82286262512207
Epoch 2220, val loss: 0.7804587483406067
Epoch 2230, training loss: 441.4919738769531 = 0.7712163925170898 + 50.0 * 8.814414978027344
Epoch 2230, val loss: 0.7786290049552917
Epoch 2240, training loss: 443.17254638671875 = 0.7694829702377319 + 50.0 * 8.848061561584473
Epoch 2240, val loss: 0.7769317030906677
Epoch 2250, training loss: 444.18878173828125 = 0.7674657702445984 + 50.0 * 8.868426322937012
Epoch 2250, val loss: 0.7748702168464661
Epoch 2260, training loss: 445.1417541503906 = 0.7653916478157043 + 50.0 * 8.887527465820312
Epoch 2260, val loss: 0.7728689312934875
Epoch 2270, training loss: 445.5800476074219 = 0.763200044631958 + 50.0 * 8.896336555480957
Epoch 2270, val loss: 0.7707469463348389
Epoch 2280, training loss: 446.1401672363281 = 0.761024534702301 + 50.0 * 8.907583236694336
Epoch 2280, val loss: 0.7686587572097778
Epoch 2290, training loss: 446.48443603515625 = 0.7587919235229492 + 50.0 * 8.914512634277344
Epoch 2290, val loss: 0.766537606716156
Epoch 2300, training loss: 446.9391784667969 = 0.7565663456916809 + 50.0 * 8.923652648925781
Epoch 2300, val loss: 0.7643921971321106
Epoch 2310, training loss: 447.1904296875 = 0.7543125748634338 + 50.0 * 8.928722381591797
Epoch 2310, val loss: 0.7622162699699402
Epoch 2320, training loss: 447.1881408691406 = 0.7520024180412292 + 50.0 * 8.928722381591797
Epoch 2320, val loss: 0.7599779367446899
Epoch 2330, training loss: 447.572998046875 = 0.7496806979179382 + 50.0 * 8.936466217041016
Epoch 2330, val loss: 0.7577535510063171
Epoch 2340, training loss: 447.8091125488281 = 0.7473906874656677 + 50.0 * 8.941234588623047
Epoch 2340, val loss: 0.7555403113365173
Epoch 2350, training loss: 447.97589111328125 = 0.7450869083404541 + 50.0 * 8.944616317749023
Epoch 2350, val loss: 0.753341794013977
Epoch 2360, training loss: 448.0050354003906 = 0.7426463961601257 + 50.0 * 8.945247650146484
Epoch 2360, val loss: 0.7509278059005737
Epoch 2370, training loss: 448.4680480957031 = 0.7404605746269226 + 50.0 * 8.954551696777344
Epoch 2370, val loss: 0.7488812208175659
Epoch 2380, training loss: 447.44415283203125 = 0.7382512092590332 + 50.0 * 8.934118270874023
Epoch 2380, val loss: 0.746779203414917
Epoch 2390, training loss: 447.7060241699219 = 0.7359206080436707 + 50.0 * 8.939401626586914
Epoch 2390, val loss: 0.744541347026825
Epoch 2400, training loss: 448.0994873046875 = 0.7338223457336426 + 50.0 * 8.94731330871582
Epoch 2400, val loss: 0.7425114512443542
Epoch 2410, training loss: 448.380615234375 = 0.7315055727958679 + 50.0 * 8.952981948852539
Epoch 2410, val loss: 0.7403128147125244
Epoch 2420, training loss: 448.5798034667969 = 0.7291863560676575 + 50.0 * 8.957012176513672
Epoch 2420, val loss: 0.738111138343811
Epoch 2430, training loss: 448.9491271972656 = 0.726932168006897 + 50.0 * 8.964444160461426
Epoch 2430, val loss: 0.7359557747840881
Epoch 2440, training loss: 449.2195739746094 = 0.7246986627578735 + 50.0 * 8.969897270202637
Epoch 2440, val loss: 0.7338131070137024
Epoch 2450, training loss: 449.1427307128906 = 0.7224414944648743 + 50.0 * 8.968405723571777
Epoch 2450, val loss: 0.7316247224807739
Epoch 2460, training loss: 449.2106018066406 = 0.7201793789863586 + 50.0 * 8.969808578491211
Epoch 2460, val loss: 0.7294808626174927
Epoch 2470, training loss: 449.3942565917969 = 0.7181286215782166 + 50.0 * 8.973522186279297
Epoch 2470, val loss: 0.7275232076644897
Epoch 2480, training loss: 448.7327880859375 = 0.7158405184745789 + 50.0 * 8.960338592529297
Epoch 2480, val loss: 0.7252671718597412
Epoch 2490, training loss: 448.4742126464844 = 0.7135446667671204 + 50.0 * 8.95521354675293
Epoch 2490, val loss: 0.7230144739151001
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7810144927536232
0.8631456929652975
=== training gcn model ===
Epoch 0, training loss: 511.1560363769531 = 1.1106982231140137 + 50.0 * 10.200906753540039
Epoch 0, val loss: 1.1110577583312988
Epoch 10, training loss: 491.60711669921875 = 1.1102135181427002 + 50.0 * 9.809938430786133
Epoch 10, val loss: 1.1106386184692383
Epoch 20, training loss: 480.92987060546875 = 1.1097108125686646 + 50.0 * 9.596403121948242
Epoch 20, val loss: 1.110139012336731
Epoch 30, training loss: 473.5126953125 = 1.1091396808624268 + 50.0 * 9.448071479797363
Epoch 30, val loss: 1.1095913648605347
Epoch 40, training loss: 467.77667236328125 = 1.10857355594635 + 50.0 * 9.333361625671387
Epoch 40, val loss: 1.10905122756958
Epoch 50, training loss: 463.2688903808594 = 1.1079989671707153 + 50.0 * 9.243217468261719
Epoch 50, val loss: 1.1085039377212524
Epoch 60, training loss: 459.5806884765625 = 1.1074233055114746 + 50.0 * 9.169465065002441
Epoch 60, val loss: 1.1079514026641846
Epoch 70, training loss: 456.46343994140625 = 1.1068384647369385 + 50.0 * 9.107131958007812
Epoch 70, val loss: 1.1073888540267944
Epoch 80, training loss: 453.785400390625 = 1.1062337160110474 + 50.0 * 9.053583145141602
Epoch 80, val loss: 1.1068081855773926
Epoch 90, training loss: 451.4825439453125 = 1.1056402921676636 + 50.0 * 9.007537841796875
Epoch 90, val loss: 1.1062347888946533
Epoch 100, training loss: 449.47918701171875 = 1.1050208806991577 + 50.0 * 8.967483520507812
Epoch 100, val loss: 1.1056361198425293
Epoch 110, training loss: 447.7536315917969 = 1.104411244392395 + 50.0 * 8.932984352111816
Epoch 110, val loss: 1.1050381660461426
Epoch 120, training loss: 446.1856384277344 = 1.103804111480713 + 50.0 * 8.901637077331543
Epoch 120, val loss: 1.1044622659683228
Epoch 130, training loss: 444.81817626953125 = 1.1031724214553833 + 50.0 * 8.874300003051758
Epoch 130, val loss: 1.103840947151184
Epoch 140, training loss: 443.63592529296875 = 1.102524995803833 + 50.0 * 8.850667953491211
Epoch 140, val loss: 1.1032148599624634
Epoch 150, training loss: 442.6440734863281 = 1.1018712520599365 + 50.0 * 8.830843925476074
Epoch 150, val loss: 1.1025768518447876
Epoch 160, training loss: 441.76153564453125 = 1.1012110710144043 + 50.0 * 8.813206672668457
Epoch 160, val loss: 1.1019343137741089
Epoch 170, training loss: 441.07794189453125 = 1.1005343198776245 + 50.0 * 8.799548149108887
Epoch 170, val loss: 1.1012800931930542
Epoch 180, training loss: 440.4421691894531 = 1.0998408794403076 + 50.0 * 8.786846160888672
Epoch 180, val loss: 1.1006044149398804
Epoch 190, training loss: 439.8304748535156 = 1.0991379022598267 + 50.0 * 8.774626731872559
Epoch 190, val loss: 1.0999255180358887
Epoch 200, training loss: 439.22674560546875 = 1.098407506942749 + 50.0 * 8.762566566467285
Epoch 200, val loss: 1.099210262298584
Epoch 210, training loss: 438.82232666015625 = 1.0976574420928955 + 50.0 * 8.754493713378906
Epoch 210, val loss: 1.0984840393066406
Epoch 220, training loss: 438.3657531738281 = 1.0969854593276978 + 50.0 * 8.745375633239746
Epoch 220, val loss: 1.0978306531906128
Epoch 230, training loss: 437.94940185546875 = 1.0962203741073608 + 50.0 * 8.73706340789795
Epoch 230, val loss: 1.0970898866653442
Epoch 240, training loss: 437.4397888183594 = 1.095451831817627 + 50.0 * 8.726886749267578
Epoch 240, val loss: 1.0963464975357056
Epoch 250, training loss: 437.3005065917969 = 1.0946974754333496 + 50.0 * 8.724116325378418
Epoch 250, val loss: 1.0956040620803833
Epoch 260, training loss: 437.12957763671875 = 1.0939222574234009 + 50.0 * 8.720712661743164
Epoch 260, val loss: 1.094838261604309
Epoch 270, training loss: 436.93267822265625 = 1.0931223630905151 + 50.0 * 8.716791152954102
Epoch 270, val loss: 1.0940748453140259
Epoch 280, training loss: 436.75909423828125 = 1.0923360586166382 + 50.0 * 8.713335037231445
Epoch 280, val loss: 1.0933091640472412
Epoch 290, training loss: 436.5723571777344 = 1.0915358066558838 + 50.0 * 8.709616661071777
Epoch 290, val loss: 1.0925322771072388
Epoch 300, training loss: 436.4820556640625 = 1.0907269716262817 + 50.0 * 8.707826614379883
Epoch 300, val loss: 1.0917357206344604
Epoch 310, training loss: 436.3327941894531 = 1.0898985862731934 + 50.0 * 8.70485782623291
Epoch 310, val loss: 1.0909334421157837
Epoch 320, training loss: 436.2187805175781 = 1.089079737663269 + 50.0 * 8.702593803405762
Epoch 320, val loss: 1.0901260375976562
Epoch 330, training loss: 436.2691650390625 = 1.088245153427124 + 50.0 * 8.703618049621582
Epoch 330, val loss: 1.0893174409866333
Epoch 340, training loss: 436.1797180175781 = 1.087398648262024 + 50.0 * 8.7018461227417
Epoch 340, val loss: 1.0884877443313599
Epoch 350, training loss: 436.1269836425781 = 1.0866093635559082 + 50.0 * 8.700807571411133
Epoch 350, val loss: 1.0877124071121216
Epoch 360, training loss: 436.1112365722656 = 1.08578622341156 + 50.0 * 8.700509071350098
Epoch 360, val loss: 1.0869262218475342
Epoch 370, training loss: 436.0385437011719 = 1.0850058794021606 + 50.0 * 8.699070930480957
Epoch 370, val loss: 1.0861928462982178
Epoch 380, training loss: 436.4554748535156 = 1.08416748046875 + 50.0 * 8.707426071166992
Epoch 380, val loss: 1.0853902101516724
Epoch 390, training loss: 435.91357421875 = 1.0834428071975708 + 50.0 * 8.696602821350098
Epoch 390, val loss: 1.0846621990203857
Epoch 400, training loss: 435.71405029296875 = 1.0827640295028687 + 50.0 * 8.692625999450684
Epoch 400, val loss: 1.084027647972107
Epoch 410, training loss: 435.5223083496094 = 1.082122802734375 + 50.0 * 8.688803672790527
Epoch 410, val loss: 1.0834001302719116
Epoch 420, training loss: 435.722900390625 = 1.0814688205718994 + 50.0 * 8.692828178405762
Epoch 420, val loss: 1.0827690362930298
Epoch 430, training loss: 435.87017822265625 = 1.0807985067367554 + 50.0 * 8.69578742980957
Epoch 430, val loss: 1.0821260213851929
Epoch 440, training loss: 435.7220764160156 = 1.0802308320999146 + 50.0 * 8.69283676147461
Epoch 440, val loss: 1.0815715789794922
Epoch 450, training loss: 436.14544677734375 = 1.0796562433242798 + 50.0 * 8.701315879821777
Epoch 450, val loss: 1.0810279846191406
Epoch 460, training loss: 435.9887390136719 = 1.07906973361969 + 50.0 * 8.698193550109863
Epoch 460, val loss: 1.0804593563079834
Epoch 470, training loss: 436.2413024902344 = 1.0785123109817505 + 50.0 * 8.703255653381348
Epoch 470, val loss: 1.07991361618042
Epoch 480, training loss: 436.2786560058594 = 1.0779643058776855 + 50.0 * 8.70401382446289
Epoch 480, val loss: 1.0793834924697876
Epoch 490, training loss: 436.5192565917969 = 1.077406406402588 + 50.0 * 8.708837509155273
Epoch 490, val loss: 1.0788469314575195
Epoch 500, training loss: 435.968994140625 = 1.0768684148788452 + 50.0 * 8.697842597961426
Epoch 500, val loss: 1.078351378440857
Epoch 510, training loss: 436.322998046875 = 1.0763086080551147 + 50.0 * 8.704934120178223
Epoch 510, val loss: 1.0777642726898193
Epoch 520, training loss: 436.5682067871094 = 1.0757712125778198 + 50.0 * 8.709848403930664
Epoch 520, val loss: 1.077251672744751
Epoch 530, training loss: 436.3443298339844 = 1.0751748085021973 + 50.0 * 8.70538330078125
Epoch 530, val loss: 1.0766938924789429
Epoch 540, training loss: 436.27392578125 = 1.07459557056427 + 50.0 * 8.703987121582031
Epoch 540, val loss: 1.0761338472366333
Epoch 550, training loss: 436.6881408691406 = 1.074042797088623 + 50.0 * 8.712282180786133
Epoch 550, val loss: 1.0755996704101562
Epoch 560, training loss: 436.989013671875 = 1.0734859704971313 + 50.0 * 8.718310356140137
Epoch 560, val loss: 1.075057029724121
Epoch 570, training loss: 437.14739990234375 = 1.0729080438613892 + 50.0 * 8.721489906311035
Epoch 570, val loss: 1.074501633644104
Epoch 580, training loss: 437.7550964355469 = 1.0722154378890991 + 50.0 * 8.733657836914062
Epoch 580, val loss: 1.0738062858581543
Epoch 590, training loss: 438.6639709472656 = 1.0717111825942993 + 50.0 * 8.751845359802246
Epoch 590, val loss: 1.0733692646026611
Epoch 600, training loss: 437.0405578613281 = 1.0710182189941406 + 50.0 * 8.719390869140625
Epoch 600, val loss: 1.0726581811904907
Epoch 610, training loss: 437.5688171386719 = 1.0704731941223145 + 50.0 * 8.72996711730957
Epoch 610, val loss: 1.0721616744995117
Epoch 620, training loss: 437.24725341796875 = 1.069822907447815 + 50.0 * 8.723548889160156
Epoch 620, val loss: 1.071554183959961
Epoch 630, training loss: 437.4670104980469 = 1.0692100524902344 + 50.0 * 8.72795581817627
Epoch 630, val loss: 1.070962905883789
Epoch 640, training loss: 437.7035827636719 = 1.0685787200927734 + 50.0 * 8.73270034790039
Epoch 640, val loss: 1.070351481437683
Epoch 650, training loss: 437.7296447753906 = 1.0679177045822144 + 50.0 * 8.733234405517578
Epoch 650, val loss: 1.069717288017273
Epoch 660, training loss: 438.09124755859375 = 1.0672677755355835 + 50.0 * 8.740479469299316
Epoch 660, val loss: 1.069094181060791
Epoch 670, training loss: 438.19891357421875 = 1.0665783882141113 + 50.0 * 8.742646217346191
Epoch 670, val loss: 1.06842839717865
Epoch 680, training loss: 438.49774169921875 = 1.0658918619155884 + 50.0 * 8.748637199401855
Epoch 680, val loss: 1.0677546262741089
Epoch 690, training loss: 438.3574523925781 = 1.0651575326919556 + 50.0 * 8.745845794677734
Epoch 690, val loss: 1.0670629739761353
Epoch 700, training loss: 438.5295715332031 = 1.0644711256027222 + 50.0 * 8.74930191040039
Epoch 700, val loss: 1.0664178133010864
Epoch 710, training loss: 438.6363830566406 = 1.0637303590774536 + 50.0 * 8.751453399658203
Epoch 710, val loss: 1.0657002925872803
Epoch 720, training loss: 438.5092468261719 = 1.063002109527588 + 50.0 * 8.74892520904541
Epoch 720, val loss: 1.0650098323822021
Epoch 730, training loss: 438.60577392578125 = 1.0622793436050415 + 50.0 * 8.750869750976562
Epoch 730, val loss: 1.064325213432312
Epoch 740, training loss: 438.67510986328125 = 1.061523675918579 + 50.0 * 8.75227165222168
Epoch 740, val loss: 1.0635977983474731
Epoch 750, training loss: 438.9873046875 = 1.0607643127441406 + 50.0 * 8.758530616760254
Epoch 750, val loss: 1.062868595123291
Epoch 760, training loss: 439.1119384765625 = 1.059998869895935 + 50.0 * 8.761038780212402
Epoch 760, val loss: 1.062131643295288
Epoch 770, training loss: 439.2163391113281 = 1.0592106580734253 + 50.0 * 8.763142585754395
Epoch 770, val loss: 1.0613842010498047
Epoch 780, training loss: 439.30950927734375 = 1.0584139823913574 + 50.0 * 8.765022277832031
Epoch 780, val loss: 1.060621738433838
Epoch 790, training loss: 439.2792053222656 = 1.057613492012024 + 50.0 * 8.764431953430176
Epoch 790, val loss: 1.0598517656326294
Epoch 800, training loss: 439.5012512207031 = 1.0568063259124756 + 50.0 * 8.768889427185059
Epoch 800, val loss: 1.0590846538543701
Epoch 810, training loss: 439.4825134277344 = 1.0559827089309692 + 50.0 * 8.76853084564209
Epoch 810, val loss: 1.058293104171753
Epoch 820, training loss: 439.5022277832031 = 1.0551429986953735 + 50.0 * 8.768941879272461
Epoch 820, val loss: 1.0574907064437866
Epoch 830, training loss: 439.7775573730469 = 1.0543181896209717 + 50.0 * 8.77446460723877
Epoch 830, val loss: 1.0566998720169067
Epoch 840, training loss: 439.908447265625 = 1.0534741878509521 + 50.0 * 8.777099609375
Epoch 840, val loss: 1.0559022426605225
Epoch 850, training loss: 440.00042724609375 = 1.0526231527328491 + 50.0 * 8.778956413269043
Epoch 850, val loss: 1.0550786256790161
Epoch 860, training loss: 440.10223388671875 = 1.0517494678497314 + 50.0 * 8.781009674072266
Epoch 860, val loss: 1.0542343854904175
Epoch 870, training loss: 440.18499755859375 = 1.050845742225647 + 50.0 * 8.782683372497559
Epoch 870, val loss: 1.0533766746520996
Epoch 880, training loss: 440.3423767089844 = 1.0499627590179443 + 50.0 * 8.785848617553711
Epoch 880, val loss: 1.0525305271148682
Epoch 890, training loss: 440.43670654296875 = 1.0490491390228271 + 50.0 * 8.787753105163574
Epoch 890, val loss: 1.051667332649231
Epoch 900, training loss: 440.67333984375 = 1.048129916191101 + 50.0 * 8.79250431060791
Epoch 900, val loss: 1.050781011581421
Epoch 910, training loss: 440.3963317871094 = 1.047203779220581 + 50.0 * 8.786982536315918
Epoch 910, val loss: 1.049904227256775
Epoch 920, training loss: 440.73944091796875 = 1.0462700128555298 + 50.0 * 8.793863296508789
Epoch 920, val loss: 1.0489870309829712
Epoch 930, training loss: 440.8692626953125 = 1.0453033447265625 + 50.0 * 8.796479225158691
Epoch 930, val loss: 1.048079490661621
Epoch 940, training loss: 440.8591613769531 = 1.0443240404129028 + 50.0 * 8.796297073364258
Epoch 940, val loss: 1.0471395254135132
Epoch 950, training loss: 441.1430358886719 = 1.0435073375701904 + 50.0 * 8.801990509033203
Epoch 950, val loss: 1.046373724937439
Epoch 960, training loss: 441.0769958496094 = 1.0426040887832642 + 50.0 * 8.800687789916992
Epoch 960, val loss: 1.0455074310302734
Epoch 970, training loss: 441.3038635253906 = 1.041646957397461 + 50.0 * 8.805244445800781
Epoch 970, val loss: 1.0445940494537354
Epoch 980, training loss: 441.5122375488281 = 1.0406577587127686 + 50.0 * 8.809432029724121
Epoch 980, val loss: 1.0436408519744873
Epoch 990, training loss: 441.60498046875 = 1.0396480560302734 + 50.0 * 8.811306953430176
Epoch 990, val loss: 1.0426688194274902
Epoch 1000, training loss: 441.7027282714844 = 1.0386285781860352 + 50.0 * 8.813282012939453
Epoch 1000, val loss: 1.0416828393936157
Epoch 1010, training loss: 441.79534912109375 = 1.0375971794128418 + 50.0 * 8.815155029296875
Epoch 1010, val loss: 1.040700912475586
Epoch 1020, training loss: 441.9760437011719 = 1.036550760269165 + 50.0 * 8.8187894821167
Epoch 1020, val loss: 1.0396770238876343
Epoch 1030, training loss: 441.817626953125 = 1.0354735851287842 + 50.0 * 8.815643310546875
Epoch 1030, val loss: 1.0386555194854736
Epoch 1040, training loss: 441.95611572265625 = 1.034400224685669 + 50.0 * 8.818434715270996
Epoch 1040, val loss: 1.03761887550354
Epoch 1050, training loss: 442.09588623046875 = 1.033329725265503 + 50.0 * 8.821250915527344
Epoch 1050, val loss: 1.0366053581237793
Epoch 1060, training loss: 442.32696533203125 = 1.0322628021240234 + 50.0 * 8.825894355773926
Epoch 1060, val loss: 1.0355417728424072
Epoch 1070, training loss: 441.7585754394531 = 1.031085729598999 + 50.0 * 8.814549446105957
Epoch 1070, val loss: 1.0344291925430298
Epoch 1080, training loss: 441.9841003417969 = 1.0300003290176392 + 50.0 * 8.819082260131836
Epoch 1080, val loss: 1.033388376235962
Epoch 1090, training loss: 442.37921142578125 = 1.0289173126220703 + 50.0 * 8.827005386352539
Epoch 1090, val loss: 1.0323269367218018
Epoch 1100, training loss: 442.67706298828125 = 1.0278207063674927 + 50.0 * 8.832984924316406
Epoch 1100, val loss: 1.0312707424163818
Epoch 1110, training loss: 442.7024841308594 = 1.026681900024414 + 50.0 * 8.833516120910645
Epoch 1110, val loss: 1.0301555395126343
Epoch 1120, training loss: 442.55853271484375 = 1.0255297422409058 + 50.0 * 8.830659866333008
Epoch 1120, val loss: 1.0290616750717163
Epoch 1130, training loss: 442.37188720703125 = 1.0243911743164062 + 50.0 * 8.826950073242188
Epoch 1130, val loss: 1.0279552936553955
Epoch 1140, training loss: 442.48907470703125 = 1.023144006729126 + 50.0 * 8.82931900024414
Epoch 1140, val loss: 1.026753544807434
Epoch 1150, training loss: 442.3390197753906 = 1.0220212936401367 + 50.0 * 8.826339721679688
Epoch 1150, val loss: 1.0256457328796387
Epoch 1160, training loss: 442.1703796386719 = 1.02083158493042 + 50.0 * 8.822990417480469
Epoch 1160, val loss: 1.0244898796081543
Epoch 1170, training loss: 442.6195068359375 = 1.019727349281311 + 50.0 * 8.831995964050293
Epoch 1170, val loss: 1.023436188697815
Epoch 1180, training loss: 442.8985595703125 = 1.0185930728912354 + 50.0 * 8.837599754333496
Epoch 1180, val loss: 1.022332787513733
Epoch 1190, training loss: 443.0831298828125 = 1.0174270868301392 + 50.0 * 8.841314315795898
Epoch 1190, val loss: 1.0211985111236572
Epoch 1200, training loss: 443.2228698730469 = 1.0162333250045776 + 50.0 * 8.844132423400879
Epoch 1200, val loss: 1.0200437307357788
Epoch 1210, training loss: 443.5613098144531 = 1.0150386095046997 + 50.0 * 8.85092544555664
Epoch 1210, val loss: 1.0188874006271362
Epoch 1220, training loss: 443.5195617675781 = 1.0138243436813354 + 50.0 * 8.850114822387695
Epoch 1220, val loss: 1.0176985263824463
Epoch 1230, training loss: 443.5579833984375 = 1.0125839710235596 + 50.0 * 8.850908279418945
Epoch 1230, val loss: 1.0165263414382935
Epoch 1240, training loss: 443.81396484375 = 1.0113532543182373 + 50.0 * 8.85605239868164
Epoch 1240, val loss: 1.0153260231018066
Epoch 1250, training loss: 443.9530334472656 = 1.0101332664489746 + 50.0 * 8.858858108520508
Epoch 1250, val loss: 1.014143466949463
Epoch 1260, training loss: 444.07342529296875 = 1.0088752508163452 + 50.0 * 8.86129093170166
Epoch 1260, val loss: 1.0129234790802002
Epoch 1270, training loss: 444.1954650878906 = 1.0076117515563965 + 50.0 * 8.863757133483887
Epoch 1270, val loss: 1.0117052793502808
Epoch 1280, training loss: 444.08062744140625 = 1.0063456296920776 + 50.0 * 8.861485481262207
Epoch 1280, val loss: 1.0104764699935913
Epoch 1290, training loss: 443.3512268066406 = 1.0049787759780884 + 50.0 * 8.846924781799316
Epoch 1290, val loss: 1.009151577949524
Epoch 1300, training loss: 443.9798889160156 = 1.003783941268921 + 50.0 * 8.859521865844727
Epoch 1300, val loss: 1.0079824924468994
Epoch 1310, training loss: 443.70782470703125 = 1.002435326576233 + 50.0 * 8.854107856750488
Epoch 1310, val loss: 1.0066946744918823
Epoch 1320, training loss: 443.85076904296875 = 1.0012328624725342 + 50.0 * 8.856990814208984
Epoch 1320, val loss: 1.005520224571228
Epoch 1330, training loss: 444.23687744140625 = 0.9999958276748657 + 50.0 * 8.864737510681152
Epoch 1330, val loss: 1.004314661026001
Epoch 1340, training loss: 444.319580078125 = 0.9986822605133057 + 50.0 * 8.86641788482666
Epoch 1340, val loss: 1.0030478239059448
Epoch 1350, training loss: 444.70941162109375 = 0.9973928332328796 + 50.0 * 8.874239921569824
Epoch 1350, val loss: 1.00180184841156
Epoch 1360, training loss: 444.68377685546875 = 0.996066689491272 + 50.0 * 8.873754501342773
Epoch 1360, val loss: 1.0005173683166504
Epoch 1370, training loss: 444.93572998046875 = 0.9947476387023926 + 50.0 * 8.878819465637207
Epoch 1370, val loss: 0.9992457032203674
Epoch 1380, training loss: 444.9639587402344 = 0.9934003949165344 + 50.0 * 8.879410743713379
Epoch 1380, val loss: 0.9979519248008728
Epoch 1390, training loss: 444.8041076660156 = 0.9920545220375061 + 50.0 * 8.876240730285645
Epoch 1390, val loss: 0.9966415166854858
Epoch 1400, training loss: 445.134765625 = 0.9907373785972595 + 50.0 * 8.882881164550781
Epoch 1400, val loss: 0.995357871055603
Epoch 1410, training loss: 445.5235595703125 = 0.9893550276756287 + 50.0 * 8.890684127807617
Epoch 1410, val loss: 0.9940256476402283
Epoch 1420, training loss: 445.37982177734375 = 0.9879583716392517 + 50.0 * 8.887837409973145
Epoch 1420, val loss: 0.9926847815513611
Epoch 1430, training loss: 445.58660888671875 = 0.9866222143173218 + 50.0 * 8.891999244689941
Epoch 1430, val loss: 0.9913835525512695
Epoch 1440, training loss: 445.9562072753906 = 0.9852321743965149 + 50.0 * 8.899419784545898
Epoch 1440, val loss: 0.9900404214859009
Epoch 1450, training loss: 446.0693359375 = 0.9838448166847229 + 50.0 * 8.90170955657959
Epoch 1450, val loss: 0.9886821508407593
Epoch 1460, training loss: 446.0338439941406 = 0.9824422001838684 + 50.0 * 8.90102767944336
Epoch 1460, val loss: 0.987324595451355
Epoch 1470, training loss: 445.92559814453125 = 0.9810149669647217 + 50.0 * 8.89889144897461
Epoch 1470, val loss: 0.9859346151351929
Epoch 1480, training loss: 446.0082702636719 = 0.9796649217605591 + 50.0 * 8.900571823120117
Epoch 1480, val loss: 0.9846431016921997
Epoch 1490, training loss: 446.16845703125 = 0.9782819151878357 + 50.0 * 8.903803825378418
Epoch 1490, val loss: 0.9832980036735535
Epoch 1500, training loss: 446.5462951660156 = 0.9768780469894409 + 50.0 * 8.911388397216797
Epoch 1500, val loss: 0.9819256663322449
Epoch 1510, training loss: 446.50982666015625 = 0.9754574298858643 + 50.0 * 8.910687446594238
Epoch 1510, val loss: 0.9805532693862915
Epoch 1520, training loss: 446.60614013671875 = 0.9740390777587891 + 50.0 * 8.912642478942871
Epoch 1520, val loss: 0.9791791439056396
Epoch 1530, training loss: 446.65869140625 = 0.972589373588562 + 50.0 * 8.913722038269043
Epoch 1530, val loss: 0.9777910709381104
Epoch 1540, training loss: 446.8554992675781 = 0.9711631536483765 + 50.0 * 8.917686462402344
Epoch 1540, val loss: 0.976402223110199
Epoch 1550, training loss: 446.8286437988281 = 0.9697097539901733 + 50.0 * 8.917179107666016
Epoch 1550, val loss: 0.9749976396560669
Epoch 1560, training loss: 446.6814880371094 = 0.9682878255844116 + 50.0 * 8.914263725280762
Epoch 1560, val loss: 0.9736304879188538
Epoch 1570, training loss: 446.64849853515625 = 0.9668784141540527 + 50.0 * 8.9136323928833
Epoch 1570, val loss: 0.9722450375556946
Epoch 1580, training loss: 446.5467834472656 = 0.9653891921043396 + 50.0 * 8.911627769470215
Epoch 1580, val loss: 0.9708046913146973
Epoch 1590, training loss: 446.8860168457031 = 0.9639319777488708 + 50.0 * 8.918441772460938
Epoch 1590, val loss: 0.9693939685821533
Epoch 1600, training loss: 447.20440673828125 = 0.9624648690223694 + 50.0 * 8.92483901977539
Epoch 1600, val loss: 0.9679785966873169
Epoch 1610, training loss: 447.2824401855469 = 0.9610067009925842 + 50.0 * 8.92642879486084
Epoch 1610, val loss: 0.9665597677230835
Epoch 1620, training loss: 447.24200439453125 = 0.959519624710083 + 50.0 * 8.925649642944336
Epoch 1620, val loss: 0.965121865272522
Epoch 1630, training loss: 447.12353515625 = 0.9580129384994507 + 50.0 * 8.923310279846191
Epoch 1630, val loss: 0.9636725783348083
Epoch 1640, training loss: 447.2644348144531 = 0.9565762877464294 + 50.0 * 8.926156997680664
Epoch 1640, val loss: 0.962268054485321
Epoch 1650, training loss: 447.6073913574219 = 0.9551030993461609 + 50.0 * 8.933045387268066
Epoch 1650, val loss: 0.9608551263809204
Epoch 1660, training loss: 447.21258544921875 = 0.9535983204841614 + 50.0 * 8.925179481506348
Epoch 1660, val loss: 0.9594213962554932
Epoch 1670, training loss: 446.5464782714844 = 0.9521782398223877 + 50.0 * 8.911886215209961
Epoch 1670, val loss: 0.9579949378967285
Epoch 1680, training loss: 446.3736267089844 = 0.9507342576980591 + 50.0 * 8.90845775604248
Epoch 1680, val loss: 0.9565900564193726
Epoch 1690, training loss: 446.7108459472656 = 0.9492413401603699 + 50.0 * 8.915231704711914
Epoch 1690, val loss: 0.9551536440849304
Epoch 1700, training loss: 446.9128723144531 = 0.9477301836013794 + 50.0 * 8.919302940368652
Epoch 1700, val loss: 0.9536792635917664
Epoch 1710, training loss: 447.36669921875 = 0.9462326169013977 + 50.0 * 8.928409576416016
Epoch 1710, val loss: 0.9522254467010498
Epoch 1720, training loss: 447.5303955078125 = 0.9447214603424072 + 50.0 * 8.931713104248047
Epoch 1720, val loss: 0.9507609009742737
Epoch 1730, training loss: 447.5699768066406 = 0.9432047605514526 + 50.0 * 8.932535171508789
Epoch 1730, val loss: 0.9493009448051453
Epoch 1740, training loss: 447.884033203125 = 0.9417075514793396 + 50.0 * 8.938846588134766
Epoch 1740, val loss: 0.9478303790092468
Epoch 1750, training loss: 448.1224060058594 = 0.9401793479919434 + 50.0 * 8.943644523620605
Epoch 1750, val loss: 0.946343719959259
Epoch 1760, training loss: 448.0063171386719 = 0.9386256337165833 + 50.0 * 8.941353797912598
Epoch 1760, val loss: 0.9448540806770325
Epoch 1770, training loss: 447.80377197265625 = 0.9371553659439087 + 50.0 * 8.937332153320312
Epoch 1770, val loss: 0.9434211850166321
Epoch 1780, training loss: 448.1900939941406 = 0.9355762004852295 + 50.0 * 8.945090293884277
Epoch 1780, val loss: 0.9418956637382507
Epoch 1790, training loss: 448.58502197265625 = 0.9340232014656067 + 50.0 * 8.953020095825195
Epoch 1790, val loss: 0.9404042363166809
Epoch 1800, training loss: 448.63287353515625 = 0.9324661493301392 + 50.0 * 8.954008102416992
Epoch 1800, val loss: 0.9389058947563171
Epoch 1810, training loss: 448.5659484863281 = 0.9309067130088806 + 50.0 * 8.9527006149292
Epoch 1810, val loss: 0.9373889565467834
Epoch 1820, training loss: 446.4180908203125 = 0.9291119575500488 + 50.0 * 8.90977954864502
Epoch 1820, val loss: 0.9357115030288696
Epoch 1830, training loss: 447.4801025390625 = 0.9277663230895996 + 50.0 * 8.931046485900879
Epoch 1830, val loss: 0.9343286156654358
Epoch 1840, training loss: 448.40374755859375 = 0.9263569116592407 + 50.0 * 8.94954776763916
Epoch 1840, val loss: 0.9329541325569153
Epoch 1850, training loss: 448.3783874511719 = 0.9248164892196655 + 50.0 * 8.949071884155273
Epoch 1850, val loss: 0.9315044283866882
Epoch 1860, training loss: 448.8226623535156 = 0.9232242703437805 + 50.0 * 8.957988739013672
Epoch 1860, val loss: 0.9299716949462891
Epoch 1870, training loss: 449.315673828125 = 0.9217241406440735 + 50.0 * 8.967879295349121
Epoch 1870, val loss: 0.9285190105438232
Epoch 1880, training loss: 449.538330078125 = 0.9201887845993042 + 50.0 * 8.972362518310547
Epoch 1880, val loss: 0.9270384311676025
Epoch 1890, training loss: 449.7641296386719 = 0.9186425805091858 + 50.0 * 8.976909637451172
Epoch 1890, val loss: 0.9255449771881104
Epoch 1900, training loss: 449.8769226074219 = 0.9170889854431152 + 50.0 * 8.979196548461914
Epoch 1900, val loss: 0.9240496158599854
Epoch 1910, training loss: 450.0340881347656 = 0.9155415296554565 + 50.0 * 8.98237133026123
Epoch 1910, val loss: 0.922553300857544
Epoch 1920, training loss: 450.2138671875 = 0.9139955043792725 + 50.0 * 8.985997200012207
Epoch 1920, val loss: 0.9210651516914368
Epoch 1930, training loss: 450.3402099609375 = 0.9124637842178345 + 50.0 * 8.988554954528809
Epoch 1930, val loss: 0.9195539355278015
Epoch 1940, training loss: 450.16314697265625 = 0.910895586013794 + 50.0 * 8.985045433044434
Epoch 1940, val loss: 0.9180450439453125
Epoch 1950, training loss: 450.23480224609375 = 0.9093405604362488 + 50.0 * 8.986509323120117
Epoch 1950, val loss: 0.916553258895874
Epoch 1960, training loss: 450.5073547363281 = 0.9078088998794556 + 50.0 * 8.99199104309082
Epoch 1960, val loss: 0.9150671362876892
Epoch 1970, training loss: 450.4119567871094 = 0.9062700867652893 + 50.0 * 8.990113258361816
Epoch 1970, val loss: 0.9135724902153015
Epoch 1980, training loss: 450.5699768066406 = 0.9047511219978333 + 50.0 * 8.993304252624512
Epoch 1980, val loss: 0.9120920896530151
Epoch 1990, training loss: 450.8318786621094 = 0.9032347798347473 + 50.0 * 8.998573303222656
Epoch 1990, val loss: 0.9106239080429077
Epoch 2000, training loss: 450.90875244140625 = 0.9016997814178467 + 50.0 * 9.000141143798828
Epoch 2000, val loss: 0.9091377854347229
Epoch 2010, training loss: 450.75897216796875 = 0.9001884460449219 + 50.0 * 8.997176170349121
Epoch 2010, val loss: 0.9076486825942993
Epoch 2020, training loss: 450.6911315917969 = 0.898659884929657 + 50.0 * 8.995849609375
Epoch 2020, val loss: 0.9061816930770874
Epoch 2030, training loss: 451.0902404785156 = 0.8971443176269531 + 50.0 * 9.003861427307129
Epoch 2030, val loss: 0.9047199487686157
Epoch 2040, training loss: 451.190185546875 = 0.8956276178359985 + 50.0 * 9.005890846252441
Epoch 2040, val loss: 0.9032428860664368
Epoch 2050, training loss: 451.2960205078125 = 0.8940895199775696 + 50.0 * 9.008038520812988
Epoch 2050, val loss: 0.9017526507377625
Epoch 2060, training loss: 448.82611083984375 = 0.8925135731697083 + 50.0 * 8.958671569824219
Epoch 2060, val loss: 0.9002368450164795
Epoch 2070, training loss: 449.46685791015625 = 0.8911957740783691 + 50.0 * 8.971512794494629
Epoch 2070, val loss: 0.8989331722259521
Epoch 2080, training loss: 449.35009765625 = 0.8895623087882996 + 50.0 * 8.969210624694824
Epoch 2080, val loss: 0.8973667025566101
Epoch 2090, training loss: 449.910888671875 = 0.8880523443222046 + 50.0 * 8.980456352233887
Epoch 2090, val loss: 0.8959046006202698
Epoch 2100, training loss: 450.27874755859375 = 0.8865517973899841 + 50.0 * 8.98784351348877
Epoch 2100, val loss: 0.8944514393806458
Epoch 2110, training loss: 450.834716796875 = 0.8850621581077576 + 50.0 * 8.998992919921875
Epoch 2110, val loss: 0.8930079936981201
Epoch 2120, training loss: 451.10784912109375 = 0.8835944533348083 + 50.0 * 9.004485130310059
Epoch 2120, val loss: 0.8915838003158569
Epoch 2130, training loss: 451.137451171875 = 0.8821073174476624 + 50.0 * 9.005106925964355
Epoch 2130, val loss: 0.8901607990264893
Epoch 2140, training loss: 451.3734130859375 = 0.8806301355361938 + 50.0 * 9.009856224060059
Epoch 2140, val loss: 0.8887283802032471
Epoch 2150, training loss: 451.4830322265625 = 0.8791449666023254 + 50.0 * 9.012077331542969
Epoch 2150, val loss: 0.8873025178909302
Epoch 2160, training loss: 451.4609375 = 0.8776782155036926 + 50.0 * 9.011665344238281
Epoch 2160, val loss: 0.8858866095542908
Epoch 2170, training loss: 451.6603698730469 = 0.8762110471725464 + 50.0 * 9.0156831741333
Epoch 2170, val loss: 0.8844674229621887
Epoch 2180, training loss: 451.67352294921875 = 0.874736487865448 + 50.0 * 9.015975952148438
Epoch 2180, val loss: 0.8830519318580627
Epoch 2190, training loss: 451.7096252441406 = 0.8732634782791138 + 50.0 * 9.016727447509766
Epoch 2190, val loss: 0.8816483616828918
Epoch 2200, training loss: 451.7734375 = 0.8718106746673584 + 50.0 * 9.018033027648926
Epoch 2200, val loss: 0.8802374601364136
Epoch 2210, training loss: 451.9649963378906 = 0.8704314827919006 + 50.0 * 9.021891593933105
Epoch 2210, val loss: 0.8789001107215881
Epoch 2220, training loss: 451.89654541015625 = 0.8690115213394165 + 50.0 * 9.020550727844238
Epoch 2220, val loss: 0.8775352835655212
Epoch 2230, training loss: 451.8612976074219 = 0.8675801157951355 + 50.0 * 9.019874572753906
Epoch 2230, val loss: 0.8761759996414185
Epoch 2240, training loss: 452.0708312988281 = 0.8661511540412903 + 50.0 * 9.024093627929688
Epoch 2240, val loss: 0.8748167753219604
Epoch 2250, training loss: 452.2277526855469 = 0.8647255897521973 + 50.0 * 9.027260780334473
Epoch 2250, val loss: 0.8734569549560547
Epoch 2260, training loss: 452.1434326171875 = 0.8632848262786865 + 50.0 * 9.025603294372559
Epoch 2260, val loss: 0.8720958232879639
Epoch 2270, training loss: 452.2831726074219 = 0.8618838787078857 + 50.0 * 9.028426170349121
Epoch 2270, val loss: 0.8707466125488281
Epoch 2280, training loss: 452.45758056640625 = 0.860474169254303 + 50.0 * 9.031942367553711
Epoch 2280, val loss: 0.8694158792495728
Epoch 2290, training loss: 452.3841552734375 = 0.8590897917747498 + 50.0 * 9.030501365661621
Epoch 2290, val loss: 0.8680658936500549
Epoch 2300, training loss: 452.3879699707031 = 0.8576862812042236 + 50.0 * 9.03060531616211
Epoch 2300, val loss: 0.8667400479316711
Epoch 2310, training loss: 452.4624938964844 = 0.8563114404678345 + 50.0 * 9.032123565673828
Epoch 2310, val loss: 0.8654289841651917
Epoch 2320, training loss: 452.53289794921875 = 0.8549342751502991 + 50.0 * 9.033559799194336
Epoch 2320, val loss: 0.864119291305542
Epoch 2330, training loss: 452.5968017578125 = 0.8535568714141846 + 50.0 * 9.034865379333496
Epoch 2330, val loss: 0.8628052473068237
Epoch 2340, training loss: 452.5636901855469 = 0.8521865010261536 + 50.0 * 9.03423023223877
Epoch 2340, val loss: 0.8615058064460754
Epoch 2350, training loss: 452.6429138183594 = 0.8508428335189819 + 50.0 * 9.035841941833496
Epoch 2350, val loss: 0.860228419303894
Epoch 2360, training loss: 452.80029296875 = 0.8495275974273682 + 50.0 * 9.039015769958496
Epoch 2360, val loss: 0.8589761257171631
Epoch 2370, training loss: 452.99383544921875 = 0.8481971621513367 + 50.0 * 9.042912483215332
Epoch 2370, val loss: 0.8577167391777039
Epoch 2380, training loss: 452.8063659667969 = 0.8468909859657288 + 50.0 * 9.039189338684082
Epoch 2380, val loss: 0.8564628958702087
Epoch 2390, training loss: 452.8842468261719 = 0.845596969127655 + 50.0 * 9.040772438049316
Epoch 2390, val loss: 0.8552478551864624
Epoch 2400, training loss: 452.9367980957031 = 0.8442951440811157 + 50.0 * 9.041850090026855
Epoch 2400, val loss: 0.8540038466453552
Epoch 2410, training loss: 453.24237060546875 = 0.8428534269332886 + 50.0 * 9.047989845275879
Epoch 2410, val loss: 0.8526259660720825
Epoch 2420, training loss: 442.638916015625 = 0.8399831652641296 + 50.0 * 8.835978507995605
Epoch 2420, val loss: 0.8499919772148132
Epoch 2430, training loss: 439.53753662109375 = 0.8395281434059143 + 50.0 * 8.77396011352539
Epoch 2430, val loss: 0.8495096564292908
Epoch 2440, training loss: 437.9909973144531 = 0.8393314480781555 + 50.0 * 8.743033409118652
Epoch 2440, val loss: 0.8490009307861328
Epoch 2450, training loss: 436.8271789550781 = 0.838157594203949 + 50.0 * 8.719779968261719
Epoch 2450, val loss: 0.8479139804840088
Epoch 2460, training loss: 437.1894226074219 = 0.8371282815933228 + 50.0 * 8.727046012878418
Epoch 2460, val loss: 0.8471042513847351
Epoch 2470, training loss: 438.6869812011719 = 0.8362125754356384 + 50.0 * 8.757015228271484
Epoch 2470, val loss: 0.8463231921195984
Epoch 2480, training loss: 439.6558532714844 = 0.8351207971572876 + 50.0 * 8.77641487121582
Epoch 2480, val loss: 0.8453674912452698
Epoch 2490, training loss: 440.6318359375 = 0.8340396881103516 + 50.0 * 8.795955657958984
Epoch 2490, val loss: 0.844343900680542
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.5342028985507247
0.8645222053176846
=== training gcn model ===
Epoch 0, training loss: 508.0655517578125 = 1.1034680604934692 + 50.0 * 10.139242172241211
Epoch 0, val loss: 1.1027452945709229
Epoch 10, training loss: 488.5670471191406 = 1.10289466381073 + 50.0 * 9.749282836914062
Epoch 10, val loss: 1.1022180318832397
Epoch 20, training loss: 478.8153076171875 = 1.1023616790771484 + 50.0 * 9.554259300231934
Epoch 20, val loss: 1.1016815900802612
Epoch 30, training loss: 471.4493103027344 = 1.101768970489502 + 50.0 * 9.406950950622559
Epoch 30, val loss: 1.1010878086090088
Epoch 40, training loss: 465.7130432128906 = 1.1011462211608887 + 50.0 * 9.292238235473633
Epoch 40, val loss: 1.1004689931869507
Epoch 50, training loss: 461.0361633300781 = 1.1004844903945923 + 50.0 * 9.198713302612305
Epoch 50, val loss: 1.0998092889785767
Epoch 60, training loss: 457.184326171875 = 1.0998015403747559 + 50.0 * 9.12169075012207
Epoch 60, val loss: 1.0991281270980835
Epoch 70, training loss: 453.95166015625 = 1.0991142988204956 + 50.0 * 9.057050704956055
Epoch 70, val loss: 1.0984443426132202
Epoch 80, training loss: 451.3265075683594 = 1.0983742475509644 + 50.0 * 9.004562377929688
Epoch 80, val loss: 1.09770929813385
Epoch 90, training loss: 448.9944152832031 = 1.097681999206543 + 50.0 * 8.957934379577637
Epoch 90, val loss: 1.0970232486724854
Epoch 100, training loss: 446.9811096191406 = 1.0969220399856567 + 50.0 * 8.917683601379395
Epoch 100, val loss: 1.0962731838226318
Epoch 110, training loss: 445.3097229003906 = 1.0961275100708008 + 50.0 * 8.884271621704102
Epoch 110, val loss: 1.0954941511154175
Epoch 120, training loss: 443.86749267578125 = 1.0953184366226196 + 50.0 * 8.855443954467773
Epoch 120, val loss: 1.0946927070617676
Epoch 130, training loss: 442.63824462890625 = 1.0944938659667969 + 50.0 * 8.830875396728516
Epoch 130, val loss: 1.0938838720321655
Epoch 140, training loss: 441.53375244140625 = 1.0936468839645386 + 50.0 * 8.808801651000977
Epoch 140, val loss: 1.0930577516555786
Epoch 150, training loss: 440.6340026855469 = 1.0927956104278564 + 50.0 * 8.790823936462402
Epoch 150, val loss: 1.0922163724899292
Epoch 160, training loss: 439.795654296875 = 1.0919090509414673 + 50.0 * 8.77407455444336
Epoch 160, val loss: 1.0913488864898682
Epoch 170, training loss: 439.07220458984375 = 1.091033697128296 + 50.0 * 8.759623527526855
Epoch 170, val loss: 1.0904875993728638
Epoch 180, training loss: 438.4751892089844 = 1.0901210308074951 + 50.0 * 8.747701644897461
Epoch 180, val loss: 1.0895676612854004
Epoch 190, training loss: 437.95648193359375 = 1.0892027616500854 + 50.0 * 8.737345695495605
Epoch 190, val loss: 1.0886778831481934
Epoch 200, training loss: 437.44415283203125 = 1.0882772207260132 + 50.0 * 8.727117538452148
Epoch 200, val loss: 1.087773084640503
Epoch 210, training loss: 437.1361999511719 = 1.0873234272003174 + 50.0 * 8.720977783203125
Epoch 210, val loss: 1.0868151187896729
Epoch 220, training loss: 436.5576171875 = 1.0863285064697266 + 50.0 * 8.709425926208496
Epoch 220, val loss: 1.0858430862426758
Epoch 230, training loss: 436.2244567871094 = 1.0853571891784668 + 50.0 * 8.702781677246094
Epoch 230, val loss: 1.0848894119262695
Epoch 240, training loss: 435.9729919433594 = 1.0843405723571777 + 50.0 * 8.697772979736328
Epoch 240, val loss: 1.0838922262191772
Epoch 250, training loss: 436.03826904296875 = 1.0833507776260376 + 50.0 * 8.699098587036133
Epoch 250, val loss: 1.0829098224639893
Epoch 260, training loss: 435.61114501953125 = 1.0822983980178833 + 50.0 * 8.690576553344727
Epoch 260, val loss: 1.0818836688995361
Epoch 270, training loss: 435.5498352050781 = 1.0812327861785889 + 50.0 * 8.689372062683105
Epoch 270, val loss: 1.08085298538208
Epoch 280, training loss: 435.3271484375 = 1.0801472663879395 + 50.0 * 8.684940338134766
Epoch 280, val loss: 1.0797920227050781
Epoch 290, training loss: 435.2275390625 = 1.0790759325027466 + 50.0 * 8.682969093322754
Epoch 290, val loss: 1.078739881515503
Epoch 300, training loss: 435.1170349121094 = 1.0779815912246704 + 50.0 * 8.680781364440918
Epoch 300, val loss: 1.077669620513916
Epoch 310, training loss: 435.1394348144531 = 1.0768955945968628 + 50.0 * 8.68125057220459
Epoch 310, val loss: 1.0766074657440186
Epoch 320, training loss: 435.0603942871094 = 1.075778841972351 + 50.0 * 8.679692268371582
Epoch 320, val loss: 1.0755177736282349
Epoch 330, training loss: 435.0426025390625 = 1.0746805667877197 + 50.0 * 8.67935848236084
Epoch 330, val loss: 1.0744630098342896
Epoch 340, training loss: 435.00042724609375 = 1.0736085176467896 + 50.0 * 8.678536415100098
Epoch 340, val loss: 1.0734152793884277
Epoch 350, training loss: 435.1134948730469 = 1.0725749731063843 + 50.0 * 8.680818557739258
Epoch 350, val loss: 1.0724005699157715
Epoch 360, training loss: 434.9326171875 = 1.0714954137802124 + 50.0 * 8.67722225189209
Epoch 360, val loss: 1.0713517665863037
Epoch 370, training loss: 435.2716369628906 = 1.070491075515747 + 50.0 * 8.684022903442383
Epoch 370, val loss: 1.0703707933425903
Epoch 380, training loss: 434.958984375 = 1.0694643259048462 + 50.0 * 8.677790641784668
Epoch 380, val loss: 1.0693711042404175
Epoch 390, training loss: 435.0653991699219 = 1.0684884786605835 + 50.0 * 8.679938316345215
Epoch 390, val loss: 1.0684216022491455
Epoch 400, training loss: 435.2078857421875 = 1.0675383806228638 + 50.0 * 8.682806968688965
Epoch 400, val loss: 1.0674798488616943
Epoch 410, training loss: 435.1980285644531 = 1.066563367843628 + 50.0 * 8.682629585266113
Epoch 410, val loss: 1.0665422677993774
Epoch 420, training loss: 435.2214050292969 = 1.0656360387802124 + 50.0 * 8.683115005493164
Epoch 420, val loss: 1.0656355619430542
Epoch 430, training loss: 435.24945068359375 = 1.064706563949585 + 50.0 * 8.683694839477539
Epoch 430, val loss: 1.064744472503662
Epoch 440, training loss: 435.31634521484375 = 1.0637794733047485 + 50.0 * 8.685050964355469
Epoch 440, val loss: 1.0638571977615356
Epoch 450, training loss: 435.4668884277344 = 1.062883734703064 + 50.0 * 8.688079833984375
Epoch 450, val loss: 1.0629973411560059
Epoch 460, training loss: 435.2065124511719 = 1.0619280338287354 + 50.0 * 8.682891845703125
Epoch 460, val loss: 1.0620836019515991
Epoch 470, training loss: 435.4844970703125 = 1.061033844947815 + 50.0 * 8.688468933105469
Epoch 470, val loss: 1.0612095594406128
Epoch 480, training loss: 435.60333251953125 = 1.0601266622543335 + 50.0 * 8.690864562988281
Epoch 480, val loss: 1.0603408813476562
Epoch 490, training loss: 435.6719665527344 = 1.0591931343078613 + 50.0 * 8.692255020141602
Epoch 490, val loss: 1.059453010559082
Epoch 500, training loss: 435.66046142578125 = 1.0582447052001953 + 50.0 * 8.692044258117676
Epoch 500, val loss: 1.0585415363311768
Epoch 510, training loss: 435.8502197265625 = 1.0572965145111084 + 50.0 * 8.6958589553833
Epoch 510, val loss: 1.0576335191726685
Epoch 520, training loss: 435.9216613769531 = 1.0563548803329468 + 50.0 * 8.697305679321289
Epoch 520, val loss: 1.0567200183868408
Epoch 530, training loss: 435.9200134277344 = 1.0553480386734009 + 50.0 * 8.697293281555176
Epoch 530, val loss: 1.0557667016983032
Epoch 540, training loss: 436.1153869628906 = 1.0544236898422241 + 50.0 * 8.70121955871582
Epoch 540, val loss: 1.054875373840332
Epoch 550, training loss: 436.2110900878906 = 1.053390383720398 + 50.0 * 8.703154563903809
Epoch 550, val loss: 1.0538908243179321
Epoch 560, training loss: 436.0927734375 = 1.0523395538330078 + 50.0 * 8.70080852508545
Epoch 560, val loss: 1.0528934001922607
Epoch 570, training loss: 436.7701110839844 = 1.0513899326324463 + 50.0 * 8.714374542236328
Epoch 570, val loss: 1.0519627332687378
Epoch 580, training loss: 436.5513000488281 = 1.0503528118133545 + 50.0 * 8.7100191116333
Epoch 580, val loss: 1.0509859323501587
Epoch 590, training loss: 436.84710693359375 = 1.0493239164352417 + 50.0 * 8.71595573425293
Epoch 590, val loss: 1.050001621246338
Epoch 600, training loss: 436.99615478515625 = 1.0482631921768188 + 50.0 * 8.718957901000977
Epoch 600, val loss: 1.0489873886108398
Epoch 610, training loss: 436.5395202636719 = 1.0470635890960693 + 50.0 * 8.70984935760498
Epoch 610, val loss: 1.0478769540786743
Epoch 620, training loss: 436.91351318359375 = 1.046050786972046 + 50.0 * 8.7173490524292
Epoch 620, val loss: 1.0468782186508179
Epoch 630, training loss: 437.2306823730469 = 1.0449903011322021 + 50.0 * 8.723713874816895
Epoch 630, val loss: 1.0458672046661377
Epoch 640, training loss: 437.41204833984375 = 1.043906807899475 + 50.0 * 8.727362632751465
Epoch 640, val loss: 1.0448278188705444
Epoch 650, training loss: 437.57928466796875 = 1.0427757501602173 + 50.0 * 8.730730056762695
Epoch 650, val loss: 1.0437554121017456
Epoch 660, training loss: 437.6074523925781 = 1.0416386127471924 + 50.0 * 8.731316566467285
Epoch 660, val loss: 1.0426726341247559
Epoch 670, training loss: 437.71533203125 = 1.0404845476150513 + 50.0 * 8.73349666595459
Epoch 670, val loss: 1.0415822267532349
Epoch 680, training loss: 438.0599670410156 = 1.0393327474594116 + 50.0 * 8.740412712097168
Epoch 680, val loss: 1.0404925346374512
Epoch 690, training loss: 438.1412658691406 = 1.03815758228302 + 50.0 * 8.74206256866455
Epoch 690, val loss: 1.0393458604812622
Epoch 700, training loss: 438.1492004394531 = 1.0369480848312378 + 50.0 * 8.742244720458984
Epoch 700, val loss: 1.0382070541381836
Epoch 710, training loss: 438.2462463378906 = 1.03573739528656 + 50.0 * 8.744210243225098
Epoch 710, val loss: 1.0370521545410156
Epoch 720, training loss: 438.307861328125 = 1.0345031023025513 + 50.0 * 8.745467185974121
Epoch 720, val loss: 1.0358760356903076
Epoch 730, training loss: 438.2979431152344 = 1.0332120656967163 + 50.0 * 8.745294570922852
Epoch 730, val loss: 1.0346704721450806
Epoch 740, training loss: 438.3262023925781 = 1.0319139957427979 + 50.0 * 8.745885848999023
Epoch 740, val loss: 1.0334166288375854
Epoch 750, training loss: 438.5296630859375 = 1.0306681394577026 + 50.0 * 8.749979972839355
Epoch 750, val loss: 1.0322182178497314
Epoch 760, training loss: 438.60003662109375 = 1.0293443202972412 + 50.0 * 8.751413345336914
Epoch 760, val loss: 1.0309699773788452
Epoch 770, training loss: 438.947021484375 = 1.0279972553253174 + 50.0 * 8.758380889892578
Epoch 770, val loss: 1.0296705961227417
Epoch 780, training loss: 438.998779296875 = 1.0265918970108032 + 50.0 * 8.759444236755371
Epoch 780, val loss: 1.0283386707305908
Epoch 790, training loss: 438.8841247558594 = 1.0251996517181396 + 50.0 * 8.75717830657959
Epoch 790, val loss: 1.0270036458969116
Epoch 800, training loss: 439.2110900878906 = 1.0237785577774048 + 50.0 * 8.76374626159668
Epoch 800, val loss: 1.0256580114364624
Epoch 810, training loss: 439.2882995605469 = 1.0222991704940796 + 50.0 * 8.76531982421875
Epoch 810, val loss: 1.0242571830749512
Epoch 820, training loss: 439.37371826171875 = 1.020835041999817 + 50.0 * 8.767057418823242
Epoch 820, val loss: 1.0228594541549683
Epoch 830, training loss: 439.6558532714844 = 1.019330620765686 + 50.0 * 8.772730827331543
Epoch 830, val loss: 1.0214229822158813
Epoch 840, training loss: 439.8437805175781 = 1.0177992582321167 + 50.0 * 8.776519775390625
Epoch 840, val loss: 1.0199916362762451
Epoch 850, training loss: 439.96099853515625 = 1.0162503719329834 + 50.0 * 8.778895378112793
Epoch 850, val loss: 1.0184822082519531
Epoch 860, training loss: 439.96600341796875 = 1.014697551727295 + 50.0 * 8.77902603149414
Epoch 860, val loss: 1.017017126083374
Epoch 870, training loss: 440.20458984375 = 1.013001799583435 + 50.0 * 8.783831596374512
Epoch 870, val loss: 1.0153778791427612
Epoch 880, training loss: 439.2361755371094 = 1.0115373134613037 + 50.0 * 8.764492988586426
Epoch 880, val loss: 1.0140405893325806
Epoch 890, training loss: 439.8124694824219 = 1.0099440813064575 + 50.0 * 8.776050567626953
Epoch 890, val loss: 1.0125257968902588
Epoch 900, training loss: 439.8941650390625 = 1.0083791017532349 + 50.0 * 8.777715682983398
Epoch 900, val loss: 1.0110459327697754
Epoch 910, training loss: 440.0459289550781 = 1.0067962408065796 + 50.0 * 8.780782699584961
Epoch 910, val loss: 1.0095247030258179
Epoch 920, training loss: 440.3119201660156 = 1.0051753520965576 + 50.0 * 8.786134719848633
Epoch 920, val loss: 1.0079998970031738
Epoch 930, training loss: 440.1222839355469 = 1.0034847259521484 + 50.0 * 8.782376289367676
Epoch 930, val loss: 1.0063929557800293
Epoch 940, training loss: 440.53070068359375 = 1.001794695854187 + 50.0 * 8.79057788848877
Epoch 940, val loss: 1.0047911405563354
Epoch 950, training loss: 440.97503662109375 = 1.0001201629638672 + 50.0 * 8.799498558044434
Epoch 950, val loss: 1.0032129287719727
Epoch 960, training loss: 440.895263671875 = 0.9983787536621094 + 50.0 * 8.797937393188477
Epoch 960, val loss: 1.0015413761138916
Epoch 970, training loss: 440.9184875488281 = 0.9966365098953247 + 50.0 * 8.798437118530273
Epoch 970, val loss: 0.9999213218688965
Epoch 980, training loss: 441.083251953125 = 0.9948868155479431 + 50.0 * 8.801767349243164
Epoch 980, val loss: 0.9982672929763794
Epoch 990, training loss: 441.3821716308594 = 0.9931533932685852 + 50.0 * 8.807780265808105
Epoch 990, val loss: 0.9965932369232178
Epoch 1000, training loss: 441.5122375488281 = 0.9913570880889893 + 50.0 * 8.810417175292969
Epoch 1000, val loss: 0.9949116706848145
Epoch 1010, training loss: 440.77984619140625 = 0.9894295334815979 + 50.0 * 8.795807838439941
Epoch 1010, val loss: 0.9930868744850159
Epoch 1020, training loss: 441.3768005371094 = 0.9877877831459045 + 50.0 * 8.807780265808105
Epoch 1020, val loss: 0.991506040096283
Epoch 1030, training loss: 441.3116760253906 = 0.9859450459480286 + 50.0 * 8.806514739990234
Epoch 1030, val loss: 0.9897825121879578
Epoch 1040, training loss: 441.5482482910156 = 0.9841142892837524 + 50.0 * 8.811283111572266
Epoch 1040, val loss: 0.9880397319793701
Epoch 1050, training loss: 441.8124694824219 = 0.9822445511817932 + 50.0 * 8.816604614257812
Epoch 1050, val loss: 0.9862833619117737
Epoch 1060, training loss: 441.9308776855469 = 0.9803899526596069 + 50.0 * 8.819009780883789
Epoch 1060, val loss: 0.9845283031463623
Epoch 1070, training loss: 442.0699462890625 = 0.9785053133964539 + 50.0 * 8.821828842163086
Epoch 1070, val loss: 0.9827235341072083
Epoch 1080, training loss: 442.13201904296875 = 0.9766089916229248 + 50.0 * 8.823107719421387
Epoch 1080, val loss: 0.9809278845787048
Epoch 1090, training loss: 441.5754089355469 = 0.9746485352516174 + 50.0 * 8.812015533447266
Epoch 1090, val loss: 0.9790738224983215
Epoch 1100, training loss: 441.6224365234375 = 0.9727375507354736 + 50.0 * 8.812994003295898
Epoch 1100, val loss: 0.9772432446479797
Epoch 1110, training loss: 441.9975891113281 = 0.9707750082015991 + 50.0 * 8.820536613464355
Epoch 1110, val loss: 0.9754380583763123
Epoch 1120, training loss: 442.29718017578125 = 0.9688171744346619 + 50.0 * 8.826567649841309
Epoch 1120, val loss: 0.9735498428344727
Epoch 1130, training loss: 442.6584167480469 = 0.9668225646018982 + 50.0 * 8.833831787109375
Epoch 1130, val loss: 0.9716595411300659
Epoch 1140, training loss: 442.602783203125 = 0.9646902680397034 + 50.0 * 8.832761764526367
Epoch 1140, val loss: 0.9696361422538757
Epoch 1150, training loss: 442.9658203125 = 0.9627537131309509 + 50.0 * 8.84006118774414
Epoch 1150, val loss: 0.9677690863609314
Epoch 1160, training loss: 442.6531982421875 = 0.9606770277023315 + 50.0 * 8.833850860595703
Epoch 1160, val loss: 0.9657971262931824
Epoch 1170, training loss: 442.80218505859375 = 0.9585273265838623 + 50.0 * 8.836873054504395
Epoch 1170, val loss: 0.963782012462616
Epoch 1180, training loss: 443.1453857421875 = 0.956444501876831 + 50.0 * 8.843778610229492
Epoch 1180, val loss: 0.961790144443512
Epoch 1190, training loss: 443.33709716796875 = 0.954280436038971 + 50.0 * 8.84765625
Epoch 1190, val loss: 0.9597362875938416
Epoch 1200, training loss: 443.5511779785156 = 0.9521311521530151 + 50.0 * 8.851981163024902
Epoch 1200, val loss: 0.9576795697212219
Epoch 1210, training loss: 443.6127624511719 = 0.9499385952949524 + 50.0 * 8.853256225585938
Epoch 1210, val loss: 0.9556083679199219
Epoch 1220, training loss: 443.4500427246094 = 0.9477028250694275 + 50.0 * 8.85004711151123
Epoch 1220, val loss: 0.9534742832183838
Epoch 1230, training loss: 443.27618408203125 = 0.9454938769340515 + 50.0 * 8.846613883972168
Epoch 1230, val loss: 0.9513801336288452
Epoch 1240, training loss: 443.69189453125 = 0.9432432651519775 + 50.0 * 8.854972839355469
Epoch 1240, val loss: 0.9492632746696472
Epoch 1250, training loss: 443.37762451171875 = 0.9409283995628357 + 50.0 * 8.848733901977539
Epoch 1250, val loss: 0.9470692873001099
Epoch 1260, training loss: 443.51483154296875 = 0.9385600090026855 + 50.0 * 8.85152530670166
Epoch 1260, val loss: 0.9448201656341553
Epoch 1270, training loss: 443.86541748046875 = 0.9361714124679565 + 50.0 * 8.858585357666016
Epoch 1270, val loss: 0.9425880312919617
Epoch 1280, training loss: 443.8170166015625 = 0.9338043928146362 + 50.0 * 8.857664108276367
Epoch 1280, val loss: 0.9403378963470459
Epoch 1290, training loss: 444.0360107421875 = 0.9313923716545105 + 50.0 * 8.862092018127441
Epoch 1290, val loss: 0.938021183013916
Epoch 1300, training loss: 444.16119384765625 = 0.9289453625679016 + 50.0 * 8.864645004272461
Epoch 1300, val loss: 0.9357069730758667
Epoch 1310, training loss: 444.2539367675781 = 0.9264655113220215 + 50.0 * 8.866549491882324
Epoch 1310, val loss: 0.9333690404891968
Epoch 1320, training loss: 444.46875 = 0.9239728450775146 + 50.0 * 8.870895385742188
Epoch 1320, val loss: 0.9310017824172974
Epoch 1330, training loss: 444.66455078125 = 0.9214836955070496 + 50.0 * 8.874861717224121
Epoch 1330, val loss: 0.9286263585090637
Epoch 1340, training loss: 444.8041687011719 = 0.9189346432685852 + 50.0 * 8.877704620361328
Epoch 1340, val loss: 0.9261876940727234
Epoch 1350, training loss: 445.0662841796875 = 0.9163324236869812 + 50.0 * 8.882999420166016
Epoch 1350, val loss: 0.9237444996833801
Epoch 1360, training loss: 444.8255920410156 = 0.913711428642273 + 50.0 * 8.8782377243042
Epoch 1360, val loss: 0.9212688207626343
Epoch 1370, training loss: 445.3358154296875 = 0.9111090302467346 + 50.0 * 8.888494491577148
Epoch 1370, val loss: 0.9188160300254822
Epoch 1380, training loss: 444.4323425292969 = 0.9084146618843079 + 50.0 * 8.870478630065918
Epoch 1380, val loss: 0.9162784218788147
Epoch 1390, training loss: 444.62054443359375 = 0.9057413339614868 + 50.0 * 8.874296188354492
Epoch 1390, val loss: 0.9137421250343323
Epoch 1400, training loss: 445.5554504394531 = 0.9029524326324463 + 50.0 * 8.893050193786621
Epoch 1400, val loss: 0.9110477566719055
Epoch 1410, training loss: 445.1609191894531 = 0.9003309607505798 + 50.0 * 8.885211944580078
Epoch 1410, val loss: 0.9086248874664307
Epoch 1420, training loss: 444.8923034667969 = 0.897584080696106 + 50.0 * 8.879894256591797
Epoch 1420, val loss: 0.9060010313987732
Epoch 1430, training loss: 445.08636474609375 = 0.894794225692749 + 50.0 * 8.883831024169922
Epoch 1430, val loss: 0.903367280960083
Epoch 1440, training loss: 445.2751770019531 = 0.8921116590499878 + 50.0 * 8.88766098022461
Epoch 1440, val loss: 0.9008246064186096
Epoch 1450, training loss: 445.55517578125 = 0.8893983364105225 + 50.0 * 8.893315315246582
Epoch 1450, val loss: 0.8982744216918945
Epoch 1460, training loss: 445.8392028808594 = 0.8866147994995117 + 50.0 * 8.899051666259766
Epoch 1460, val loss: 0.8956477046012878
Epoch 1470, training loss: 445.8750305175781 = 0.8837896585464478 + 50.0 * 8.899825096130371
Epoch 1470, val loss: 0.892968475818634
Epoch 1480, training loss: 445.9591369628906 = 0.8809545636177063 + 50.0 * 8.90156364440918
Epoch 1480, val loss: 0.890285313129425
Epoch 1490, training loss: 446.23553466796875 = 0.8781179189682007 + 50.0 * 8.907148361206055
Epoch 1490, val loss: 0.8876049518585205
Epoch 1500, training loss: 446.27020263671875 = 0.8752297759056091 + 50.0 * 8.907898902893066
Epoch 1500, val loss: 0.8848485946655273
Epoch 1510, training loss: 445.93780517578125 = 0.8722608089447021 + 50.0 * 8.901310920715332
Epoch 1510, val loss: 0.8820406794548035
Epoch 1520, training loss: 446.1990051269531 = 0.8694683909416199 + 50.0 * 8.906590461730957
Epoch 1520, val loss: 0.8794506192207336
Epoch 1530, training loss: 446.4973449707031 = 0.8665958642959595 + 50.0 * 8.912614822387695
Epoch 1530, val loss: 0.8767328262329102
Epoch 1540, training loss: 446.7493896484375 = 0.8636850714683533 + 50.0 * 8.91771411895752
Epoch 1540, val loss: 0.8739644289016724
Epoch 1550, training loss: 446.8083801269531 = 0.8607375025749207 + 50.0 * 8.918952941894531
Epoch 1550, val loss: 0.8711917400360107
Epoch 1560, training loss: 446.9200439453125 = 0.8577578663825989 + 50.0 * 8.921245574951172
Epoch 1560, val loss: 0.8683940768241882
Epoch 1570, training loss: 446.9636535644531 = 0.8548031449317932 + 50.0 * 8.9221773147583
Epoch 1570, val loss: 0.8655492067337036
Epoch 1580, training loss: 447.1263427734375 = 0.8517900109291077 + 50.0 * 8.925491333007812
Epoch 1580, val loss: 0.8627360463142395
Epoch 1590, training loss: 447.250244140625 = 0.848800539970398 + 50.0 * 8.92802906036377
Epoch 1590, val loss: 0.8598645925521851
Epoch 1600, training loss: 447.2744140625 = 0.845741331577301 + 50.0 * 8.928573608398438
Epoch 1600, val loss: 0.857006847858429
Epoch 1610, training loss: 447.49676513671875 = 0.8426849246025085 + 50.0 * 8.93308162689209
Epoch 1610, val loss: 0.8541193604469299
Epoch 1620, training loss: 447.640869140625 = 0.83962482213974 + 50.0 * 8.93602466583252
Epoch 1620, val loss: 0.8511982560157776
Epoch 1630, training loss: 448.3800354003906 = 0.8361830711364746 + 50.0 * 8.95087718963623
Epoch 1630, val loss: 0.8477941751480103
Epoch 1640, training loss: 442.9764709472656 = 0.8333258628845215 + 50.0 * 8.842863082885742
Epoch 1640, val loss: 0.8454033136367798
Epoch 1650, training loss: 445.5802307128906 = 0.8312669396400452 + 50.0 * 8.894979476928711
Epoch 1650, val loss: 0.8433212637901306
Epoch 1660, training loss: 443.9989318847656 = 0.8285284638404846 + 50.0 * 8.863408088684082
Epoch 1660, val loss: 0.8408076167106628
Epoch 1670, training loss: 444.78924560546875 = 0.8262354731559753 + 50.0 * 8.879260063171387
Epoch 1670, val loss: 0.8386135697364807
Epoch 1680, training loss: 444.9927673339844 = 0.8234319686889648 + 50.0 * 8.883386611938477
Epoch 1680, val loss: 0.8359895348548889
Epoch 1690, training loss: 445.74749755859375 = 0.8206130862236023 + 50.0 * 8.898537635803223
Epoch 1690, val loss: 0.8333396315574646
Epoch 1700, training loss: 446.3319091796875 = 0.8176539540290833 + 50.0 * 8.910284996032715
Epoch 1700, val loss: 0.8305583000183105
Epoch 1710, training loss: 446.7072448730469 = 0.8145784735679626 + 50.0 * 8.917853355407715
Epoch 1710, val loss: 0.8276382088661194
Epoch 1720, training loss: 447.00335693359375 = 0.8114653825759888 + 50.0 * 8.923837661743164
Epoch 1720, val loss: 0.8247202038764954
Epoch 1730, training loss: 447.23883056640625 = 0.8083477020263672 + 50.0 * 8.928609848022461
Epoch 1730, val loss: 0.8217736482620239
Epoch 1740, training loss: 447.39312744140625 = 0.8052217364311218 + 50.0 * 8.931757926940918
Epoch 1740, val loss: 0.8188143968582153
Epoch 1750, training loss: 447.6995849609375 = 0.8020765781402588 + 50.0 * 8.937950134277344
Epoch 1750, val loss: 0.8158501982688904
Epoch 1760, training loss: 448.0495910644531 = 0.798958957195282 + 50.0 * 8.945013046264648
Epoch 1760, val loss: 0.8129174113273621
Epoch 1770, training loss: 448.088623046875 = 0.7957906723022461 + 50.0 * 8.945857048034668
Epoch 1770, val loss: 0.8099333643913269
Epoch 1780, training loss: 448.4392395019531 = 0.7926443815231323 + 50.0 * 8.952932357788086
Epoch 1780, val loss: 0.8069598078727722
Epoch 1790, training loss: 448.3475646972656 = 0.7894713878631592 + 50.0 * 8.951162338256836
Epoch 1790, val loss: 0.803977906703949
Epoch 1800, training loss: 448.62567138671875 = 0.7863242626190186 + 50.0 * 8.956787109375
Epoch 1800, val loss: 0.800986647605896
Epoch 1810, training loss: 448.57366943359375 = 0.7831418514251709 + 50.0 * 8.955810546875
Epoch 1810, val loss: 0.7979609370231628
Epoch 1820, training loss: 448.65264892578125 = 0.7798784375190735 + 50.0 * 8.9574556350708
Epoch 1820, val loss: 0.7949333786964417
Epoch 1830, training loss: 448.5621032714844 = 0.7766764163970947 + 50.0 * 8.955708503723145
Epoch 1830, val loss: 0.7919389009475708
Epoch 1840, training loss: 448.8707580566406 = 0.7735233306884766 + 50.0 * 8.961944580078125
Epoch 1840, val loss: 0.7889497876167297
Epoch 1850, training loss: 449.041015625 = 0.7704029679298401 + 50.0 * 8.965412139892578
Epoch 1850, val loss: 0.7860084176063538
Epoch 1860, training loss: 448.59344482421875 = 0.7671443223953247 + 50.0 * 8.956525802612305
Epoch 1860, val loss: 0.7829205393791199
Epoch 1870, training loss: 448.90631103515625 = 0.7639841437339783 + 50.0 * 8.962846755981445
Epoch 1870, val loss: 0.779961884021759
Epoch 1880, training loss: 449.0882568359375 = 0.76079261302948 + 50.0 * 8.966548919677734
Epoch 1880, val loss: 0.7769695520401001
Epoch 1890, training loss: 449.380859375 = 0.7576445937156677 + 50.0 * 8.972464561462402
Epoch 1890, val loss: 0.7739897966384888
Epoch 1900, training loss: 448.9515686035156 = 0.7544057369232178 + 50.0 * 8.963943481445312
Epoch 1900, val loss: 0.770971953868866
Epoch 1910, training loss: 449.0276184082031 = 0.7514976859092712 + 50.0 * 8.965522766113281
Epoch 1910, val loss: 0.768216609954834
Epoch 1920, training loss: 449.3995666503906 = 0.7483802437782288 + 50.0 * 8.973023414611816
Epoch 1920, val loss: 0.7653048634529114
Epoch 1930, training loss: 449.6044006347656 = 0.7452718615531921 + 50.0 * 8.977182388305664
Epoch 1930, val loss: 0.7623686790466309
Epoch 1940, training loss: 450.0130615234375 = 0.7421109080314636 + 50.0 * 8.985419273376465
Epoch 1940, val loss: 0.7593879699707031
Epoch 1950, training loss: 449.86376953125 = 0.7389466762542725 + 50.0 * 8.98249626159668
Epoch 1950, val loss: 0.756405770778656
Epoch 1960, training loss: 450.1452941894531 = 0.7358037829399109 + 50.0 * 8.988189697265625
Epoch 1960, val loss: 0.7534899115562439
Epoch 1970, training loss: 450.1947326660156 = 0.7326573729515076 + 50.0 * 8.989241600036621
Epoch 1970, val loss: 0.7505260109901428
Epoch 1980, training loss: 450.31744384765625 = 0.7295247912406921 + 50.0 * 8.991758346557617
Epoch 1980, val loss: 0.7475739121437073
Epoch 1990, training loss: 450.2764892578125 = 0.7263820171356201 + 50.0 * 8.991002082824707
Epoch 1990, val loss: 0.7446351051330566
Epoch 2000, training loss: 450.48822021484375 = 0.7232723236083984 + 50.0 * 8.995299339294434
Epoch 2000, val loss: 0.7417154312133789
Epoch 2010, training loss: 450.44183349609375 = 0.7201439142227173 + 50.0 * 8.994433403015137
Epoch 2010, val loss: 0.7387906908988953
Epoch 2020, training loss: 450.61083984375 = 0.7170588374137878 + 50.0 * 8.997875213623047
Epoch 2020, val loss: 0.7358954548835754
Epoch 2030, training loss: 450.59674072265625 = 0.7139549255371094 + 50.0 * 8.997655868530273
Epoch 2030, val loss: 0.7329792380332947
Epoch 2040, training loss: 450.5366516113281 = 0.7108748555183411 + 50.0 * 8.996515274047852
Epoch 2040, val loss: 0.7300805449485779
Epoch 2050, training loss: 450.7106018066406 = 0.7077398896217346 + 50.0 * 9.000057220458984
Epoch 2050, val loss: 0.7271640300750732
Epoch 2060, training loss: 450.8707580566406 = 0.7046818137168884 + 50.0 * 9.003321647644043
Epoch 2060, val loss: 0.7243117690086365
Epoch 2070, training loss: 450.74542236328125 = 0.7015908360481262 + 50.0 * 9.000876426696777
Epoch 2070, val loss: 0.721420168876648
Epoch 2080, training loss: 450.743408203125 = 0.6985335946083069 + 50.0 * 9.000897407531738
Epoch 2080, val loss: 0.7185676693916321
Epoch 2090, training loss: 450.9358215332031 = 0.6955089569091797 + 50.0 * 9.004806518554688
Epoch 2090, val loss: 0.7157434821128845
Epoch 2100, training loss: 449.90478515625 = 0.6924120187759399 + 50.0 * 8.984247207641602
Epoch 2100, val loss: 0.7127891778945923
Epoch 2110, training loss: 450.0678405761719 = 0.6895507574081421 + 50.0 * 8.987565994262695
Epoch 2110, val loss: 0.7101444602012634
Epoch 2120, training loss: 449.3038024902344 = 0.6868414878845215 + 50.0 * 8.972339630126953
Epoch 2120, val loss: 0.7076123356819153
Epoch 2130, training loss: 448.2553405761719 = 0.684256374835968 + 50.0 * 8.951421737670898
Epoch 2130, val loss: 0.705175518989563
Epoch 2140, training loss: 448.29522705078125 = 0.6813319325447083 + 50.0 * 8.952278137207031
Epoch 2140, val loss: 0.7025178074836731
Epoch 2150, training loss: 448.944580078125 = 0.678485631942749 + 50.0 * 8.96532154083252
Epoch 2150, val loss: 0.6998820304870605
Epoch 2160, training loss: 448.758544921875 = 0.6755572557449341 + 50.0 * 8.96165943145752
Epoch 2160, val loss: 0.6971801519393921
Epoch 2170, training loss: 449.318603515625 = 0.672766923904419 + 50.0 * 8.972916603088379
Epoch 2170, val loss: 0.6945399641990662
Epoch 2180, training loss: 449.8138427734375 = 0.6699151396751404 + 50.0 * 8.982878684997559
Epoch 2180, val loss: 0.6918808221817017
Epoch 2190, training loss: 450.1019592285156 = 0.667016863822937 + 50.0 * 8.988698959350586
Epoch 2190, val loss: 0.6891858577728271
Epoch 2200, training loss: 450.46380615234375 = 0.6641812324523926 + 50.0 * 8.995992660522461
Epoch 2200, val loss: 0.6865606307983398
Epoch 2210, training loss: 450.6488952636719 = 0.6613407135009766 + 50.0 * 8.999751091003418
Epoch 2210, val loss: 0.6839332580566406
Epoch 2220, training loss: 450.7215881347656 = 0.6584955453872681 + 50.0 * 9.001261711120605
Epoch 2220, val loss: 0.681307315826416
Epoch 2230, training loss: 450.8253173828125 = 0.6556568741798401 + 50.0 * 9.003393173217773
Epoch 2230, val loss: 0.6786826848983765
Epoch 2240, training loss: 450.876953125 = 0.652840256690979 + 50.0 * 9.00448226928711
Epoch 2240, val loss: 0.6760663986206055
Epoch 2250, training loss: 450.997802734375 = 0.6500441431999207 + 50.0 * 9.00695514678955
Epoch 2250, val loss: 0.6734732389450073
Epoch 2260, training loss: 451.017333984375 = 0.6472602486610413 + 50.0 * 9.007401466369629
Epoch 2260, val loss: 0.6709009408950806
Epoch 2270, training loss: 450.49359130859375 = 0.6444813013076782 + 50.0 * 8.99698257446289
Epoch 2270, val loss: 0.6683633327484131
Epoch 2280, training loss: 450.7597961425781 = 0.6417834758758545 + 50.0 * 9.002360343933105
Epoch 2280, val loss: 0.6658669710159302
Epoch 2290, training loss: 450.91058349609375 = 0.6390743255615234 + 50.0 * 9.005430221557617
Epoch 2290, val loss: 0.6633697748184204
Epoch 2300, training loss: 451.1599426269531 = 0.6363805532455444 + 50.0 * 9.01047134399414
Epoch 2300, val loss: 0.6608878374099731
Epoch 2310, training loss: 451.169189453125 = 0.6336798667907715 + 50.0 * 9.010710716247559
Epoch 2310, val loss: 0.6583875417709351
Epoch 2320, training loss: 451.2854309082031 = 0.6310216188430786 + 50.0 * 9.01308822631836
Epoch 2320, val loss: 0.6559456586837769
Epoch 2330, training loss: 451.4122009277344 = 0.62839674949646 + 50.0 * 9.015676498413086
Epoch 2330, val loss: 0.6535172462463379
Epoch 2340, training loss: 451.45220947265625 = 0.625754177570343 + 50.0 * 9.016529083251953
Epoch 2340, val loss: 0.6511033773422241
Epoch 2350, training loss: 451.5988464355469 = 0.6231422424316406 + 50.0 * 9.019514083862305
Epoch 2350, val loss: 0.6487003564834595
Epoch 2360, training loss: 451.7477111816406 = 0.6205700039863586 + 50.0 * 9.022542953491211
Epoch 2360, val loss: 0.6463074088096619
Epoch 2370, training loss: 451.63458251953125 = 0.6180037260055542 + 50.0 * 9.020331382751465
Epoch 2370, val loss: 0.6439570188522339
Epoch 2380, training loss: 451.8545837402344 = 0.615489661693573 + 50.0 * 9.024782180786133
Epoch 2380, val loss: 0.6416463851928711
Epoch 2390, training loss: 451.94622802734375 = 0.6129711270332336 + 50.0 * 9.026664733886719
Epoch 2390, val loss: 0.6393213868141174
Epoch 2400, training loss: 451.78143310546875 = 0.6104622483253479 + 50.0 * 9.023419380187988
Epoch 2400, val loss: 0.6370353102684021
Epoch 2410, training loss: 451.01483154296875 = 0.608000636100769 + 50.0 * 9.008136749267578
Epoch 2410, val loss: 0.6347673535346985
Epoch 2420, training loss: 449.7864074707031 = 0.6055738925933838 + 50.0 * 8.983616828918457
Epoch 2420, val loss: 0.6326192021369934
Epoch 2430, training loss: 450.57098388671875 = 0.6034570336341858 + 50.0 * 8.999350547790527
Epoch 2430, val loss: 0.6306044459342957
Epoch 2440, training loss: 450.9604797363281 = 0.6010637283325195 + 50.0 * 9.007187843322754
Epoch 2440, val loss: 0.6284326910972595
Epoch 2450, training loss: 451.2110595703125 = 0.598664402961731 + 50.0 * 9.012248039245605
Epoch 2450, val loss: 0.6262456178665161
Epoch 2460, training loss: 451.5878601074219 = 0.5963273048400879 + 50.0 * 9.019830703735352
Epoch 2460, val loss: 0.6241101026535034
Epoch 2470, training loss: 451.865966796875 = 0.593971312046051 + 50.0 * 9.025440216064453
Epoch 2470, val loss: 0.621955931186676
Epoch 2480, training loss: 451.893798828125 = 0.5916153788566589 + 50.0 * 9.026043891906738
Epoch 2480, val loss: 0.6198074221611023
Epoch 2490, training loss: 452.0441589355469 = 0.5893049240112305 + 50.0 * 9.029097557067871
Epoch 2490, val loss: 0.617705225944519
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8207246376811594
0.8645946533362313
The final CL Acc:0.71198, 0.12675, The final GNN Acc:0.86409, 0.00067
Begin epxeriment: noisy_level: 0.2 cont_weight: 50 epoch: der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106476])
remove edge: torch.Size([2, 70632])
updated graph: torch.Size([2, 88460])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:227: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 513.7422485351562 = 1.104261040687561 + 50.0 * 10.25275993347168
Epoch 0, val loss: 1.1042534112930298
Epoch 10, training loss: 496.4104309082031 = 1.1036635637283325 + 50.0 * 9.906135559082031
Epoch 10, val loss: 1.1036897897720337
Epoch 20, training loss: 488.0657958984375 = 1.1030875444412231 + 50.0 * 9.739253997802734
Epoch 20, val loss: 1.1031146049499512
Epoch 30, training loss: 481.4553527832031 = 1.102502703666687 + 50.0 * 9.607056617736816
Epoch 30, val loss: 1.1025365591049194
Epoch 40, training loss: 476.07623291015625 = 1.101906180381775 + 50.0 * 9.499486923217773
Epoch 40, val loss: 1.1019365787506104
Epoch 50, training loss: 471.4878845214844 = 1.101314663887024 + 50.0 * 9.407731056213379
Epoch 50, val loss: 1.1013474464416504
Epoch 60, training loss: 467.6079406738281 = 1.1007112264633179 + 50.0 * 9.330144882202148
Epoch 60, val loss: 1.1007404327392578
Epoch 70, training loss: 464.3335266113281 = 1.1000850200653076 + 50.0 * 9.264668464660645
Epoch 70, val loss: 1.1001185178756714
Epoch 80, training loss: 461.4806213378906 = 1.0994632244110107 + 50.0 * 9.207623481750488
Epoch 80, val loss: 1.0995001792907715
Epoch 90, training loss: 459.006591796875 = 1.0988229513168335 + 50.0 * 9.15815544128418
Epoch 90, val loss: 1.0988613367080688
Epoch 100, training loss: 456.82501220703125 = 1.0981967449188232 + 50.0 * 9.11453628540039
Epoch 100, val loss: 1.0982424020767212
Epoch 110, training loss: 454.8625793457031 = 1.0975606441497803 + 50.0 * 9.075300216674805
Epoch 110, val loss: 1.0976130962371826
Epoch 120, training loss: 453.4769592285156 = 1.0969529151916504 + 50.0 * 9.047599792480469
Epoch 120, val loss: 1.0970040559768677
Epoch 130, training loss: 451.8959045410156 = 1.0962859392166138 + 50.0 * 9.015992164611816
Epoch 130, val loss: 1.0963565111160278
Epoch 140, training loss: 450.6609191894531 = 1.0956496000289917 + 50.0 * 8.991305351257324
Epoch 140, val loss: 1.0957096815109253
Epoch 150, training loss: 449.5357360839844 = 1.094996452331543 + 50.0 * 8.968814849853516
Epoch 150, val loss: 1.0950675010681152
Epoch 160, training loss: 448.58953857421875 = 1.0943671464920044 + 50.0 * 8.94990348815918
Epoch 160, val loss: 1.0944424867630005
Epoch 170, training loss: 447.6584777832031 = 1.093724250793457 + 50.0 * 8.931295394897461
Epoch 170, val loss: 1.0938093662261963
Epoch 180, training loss: 446.9847106933594 = 1.093051552772522 + 50.0 * 8.91783332824707
Epoch 180, val loss: 1.0931262969970703
Epoch 190, training loss: 446.4596252441406 = 1.0924261808395386 + 50.0 * 8.907343864440918
Epoch 190, val loss: 1.0925205945968628
Epoch 200, training loss: 445.6991271972656 = 1.0917240381240845 + 50.0 * 8.8921480178833
Epoch 200, val loss: 1.091827154159546
Epoch 210, training loss: 445.376708984375 = 1.0910662412643433 + 50.0 * 8.885712623596191
Epoch 210, val loss: 1.0911742448806763
Epoch 220, training loss: 444.8799133300781 = 1.0903894901275635 + 50.0 * 8.8757905960083
Epoch 220, val loss: 1.0905038118362427
Epoch 230, training loss: 444.79693603515625 = 1.089719533920288 + 50.0 * 8.874144554138184
Epoch 230, val loss: 1.0898512601852417
Epoch 240, training loss: 444.39605712890625 = 1.0890556573867798 + 50.0 * 8.866140365600586
Epoch 240, val loss: 1.0891848802566528
Epoch 250, training loss: 443.80462646484375 = 1.088321328163147 + 50.0 * 8.854326248168945
Epoch 250, val loss: 1.0884661674499512
Epoch 260, training loss: 443.61981201171875 = 1.0876364707946777 + 50.0 * 8.850643157958984
Epoch 260, val loss: 1.0877845287322998
Epoch 270, training loss: 443.494140625 = 1.0869203805923462 + 50.0 * 8.84814453125
Epoch 270, val loss: 1.0870978832244873
Epoch 280, training loss: 443.0006103515625 = 1.0862400531768799 + 50.0 * 8.838287353515625
Epoch 280, val loss: 1.0864002704620361
Epoch 290, training loss: 442.8301696777344 = 1.0855556726455688 + 50.0 * 8.834892272949219
Epoch 290, val loss: 1.0857446193695068
Epoch 300, training loss: 442.80902099609375 = 1.0848948955535889 + 50.0 * 8.834482192993164
Epoch 300, val loss: 1.0850833654403687
Epoch 310, training loss: 442.76739501953125 = 1.0842081308364868 + 50.0 * 8.833663940429688
Epoch 310, val loss: 1.0844112634658813
Epoch 320, training loss: 442.6651916503906 = 1.0835679769515991 + 50.0 * 8.831632614135742
Epoch 320, val loss: 1.0837656259536743
Epoch 330, training loss: 442.72943115234375 = 1.0828572511672974 + 50.0 * 8.832931518554688
Epoch 330, val loss: 1.0830472707748413
Epoch 340, training loss: 442.5373840332031 = 1.0822205543518066 + 50.0 * 8.829103469848633
Epoch 340, val loss: 1.0824544429779053
Epoch 350, training loss: 442.10284423828125 = 1.0815917253494263 + 50.0 * 8.820425033569336
Epoch 350, val loss: 1.0818477869033813
Epoch 360, training loss: 442.2170715332031 = 1.0810519456863403 + 50.0 * 8.822720527648926
Epoch 360, val loss: 1.0813004970550537
Epoch 370, training loss: 442.3758850097656 = 1.0805177688598633 + 50.0 * 8.825907707214355
Epoch 370, val loss: 1.0807863473892212
Epoch 380, training loss: 442.20721435546875 = 1.0800139904022217 + 50.0 * 8.82254409790039
Epoch 380, val loss: 1.0802922248840332
Epoch 390, training loss: 442.4314270019531 = 1.0795120000839233 + 50.0 * 8.827038764953613
Epoch 390, val loss: 1.0797820091247559
Epoch 400, training loss: 442.8108215332031 = 1.0790789127349854 + 50.0 * 8.834634780883789
Epoch 400, val loss: 1.0793812274932861
Epoch 410, training loss: 441.89874267578125 = 1.0786280632019043 + 50.0 * 8.816402435302734
Epoch 410, val loss: 1.0789544582366943
Epoch 420, training loss: 442.06768798828125 = 1.0782387256622314 + 50.0 * 8.819788932800293
Epoch 420, val loss: 1.0785678625106812
Epoch 430, training loss: 442.3575744628906 = 1.077832818031311 + 50.0 * 8.825594902038574
Epoch 430, val loss: 1.0781763792037964
Epoch 440, training loss: 442.4395751953125 = 1.0774160623550415 + 50.0 * 8.827242851257324
Epoch 440, val loss: 1.0777751207351685
Epoch 450, training loss: 442.3493347167969 = 1.0769983530044556 + 50.0 * 8.825447082519531
Epoch 450, val loss: 1.0773786306381226
Epoch 460, training loss: 442.1124572753906 = 1.0765650272369385 + 50.0 * 8.820717811584473
Epoch 460, val loss: 1.0769529342651367
Epoch 470, training loss: 442.1461181640625 = 1.0761600732803345 + 50.0 * 8.821398735046387
Epoch 470, val loss: 1.0765577554702759
Epoch 480, training loss: 442.38275146484375 = 1.0757496356964111 + 50.0 * 8.826140403747559
Epoch 480, val loss: 1.0761542320251465
Epoch 490, training loss: 441.99652099609375 = 1.075318455696106 + 50.0 * 8.818424224853516
Epoch 490, val loss: 1.0757451057434082
Epoch 500, training loss: 442.3153076171875 = 1.0749050378799438 + 50.0 * 8.824808120727539
Epoch 500, val loss: 1.0753494501113892
Epoch 510, training loss: 442.4753112792969 = 1.0744878053665161 + 50.0 * 8.82801628112793
Epoch 510, val loss: 1.0749355554580688
Epoch 520, training loss: 442.71356201171875 = 1.0740488767623901 + 50.0 * 8.83279037475586
Epoch 520, val loss: 1.074514389038086
Epoch 530, training loss: 442.6902770996094 = 1.073602318763733 + 50.0 * 8.8323335647583
Epoch 530, val loss: 1.0740834474563599
Epoch 540, training loss: 442.7310791015625 = 1.0731525421142578 + 50.0 * 8.833158493041992
Epoch 540, val loss: 1.0736567974090576
Epoch 550, training loss: 443.1380615234375 = 1.0727258920669556 + 50.0 * 8.841306686401367
Epoch 550, val loss: 1.073229193687439
Epoch 560, training loss: 442.6147766113281 = 1.0722402334213257 + 50.0 * 8.830850601196289
Epoch 560, val loss: 1.0727671384811401
Epoch 570, training loss: 442.7405700683594 = 1.0717742443084717 + 50.0 * 8.833375930786133
Epoch 570, val loss: 1.0723044872283936
Epoch 580, training loss: 442.758056640625 = 1.071311354637146 + 50.0 * 8.833734512329102
Epoch 580, val loss: 1.0718491077423096
Epoch 590, training loss: 442.9243469238281 = 1.070844292640686 + 50.0 * 8.83707046508789
Epoch 590, val loss: 1.0714117288589478
Epoch 600, training loss: 443.04071044921875 = 1.0703705549240112 + 50.0 * 8.839406967163086
Epoch 600, val loss: 1.0709550380706787
Epoch 610, training loss: 443.1955261230469 = 1.069890022277832 + 50.0 * 8.842513084411621
Epoch 610, val loss: 1.0704938173294067
Epoch 620, training loss: 443.2950439453125 = 1.0694035291671753 + 50.0 * 8.844512939453125
Epoch 620, val loss: 1.0700114965438843
Epoch 630, training loss: 443.477294921875 = 1.0689208507537842 + 50.0 * 8.848167419433594
Epoch 630, val loss: 1.0695463418960571
Epoch 640, training loss: 443.3853759765625 = 1.0684257745742798 + 50.0 * 8.846339225769043
Epoch 640, val loss: 1.0690683126449585
Epoch 650, training loss: 443.3509521484375 = 1.0679067373275757 + 50.0 * 8.845661163330078
Epoch 650, val loss: 1.068589687347412
Epoch 660, training loss: 443.21331787109375 = 1.0673891305923462 + 50.0 * 8.842918395996094
Epoch 660, val loss: 1.0680774450302124
Epoch 670, training loss: 443.4204406738281 = 1.0668985843658447 + 50.0 * 8.847070693969727
Epoch 670, val loss: 1.0675981044769287
Epoch 680, training loss: 443.6513366699219 = 1.0664117336273193 + 50.0 * 8.851698875427246
Epoch 680, val loss: 1.067118525505066
Epoch 690, training loss: 443.6431884765625 = 1.0658884048461914 + 50.0 * 8.851546287536621
Epoch 690, val loss: 1.06661057472229
Epoch 700, training loss: 443.5106201171875 = 1.065342664718628 + 50.0 * 8.848905563354492
Epoch 700, val loss: 1.0661081075668335
Epoch 710, training loss: 443.9518737792969 = 1.0648412704467773 + 50.0 * 8.85774040222168
Epoch 710, val loss: 1.0656129121780396
Epoch 720, training loss: 444.0710144042969 = 1.064292550086975 + 50.0 * 8.86013412475586
Epoch 720, val loss: 1.0650781393051147
Epoch 730, training loss: 444.1910400390625 = 1.0637602806091309 + 50.0 * 8.86254596710205
Epoch 730, val loss: 1.0645633935928345
Epoch 740, training loss: 444.4554748535156 = 1.0632202625274658 + 50.0 * 8.867844581604004
Epoch 740, val loss: 1.0640358924865723
Epoch 750, training loss: 444.50201416015625 = 1.0626728534698486 + 50.0 * 8.868786811828613
Epoch 750, val loss: 1.0635221004486084
Epoch 760, training loss: 444.5702209472656 = 1.0621187686920166 + 50.0 * 8.870162010192871
Epoch 760, val loss: 1.0630030632019043
Epoch 770, training loss: 444.67645263671875 = 1.0615694522857666 + 50.0 * 8.872297286987305
Epoch 770, val loss: 1.0624525547027588
Epoch 780, training loss: 444.5766906738281 = 1.0609760284423828 + 50.0 * 8.870314598083496
Epoch 780, val loss: 1.061893343925476
Epoch 790, training loss: 444.79998779296875 = 1.0604205131530762 + 50.0 * 8.874791145324707
Epoch 790, val loss: 1.0613722801208496
Epoch 800, training loss: 445.06292724609375 = 1.0598620176315308 + 50.0 * 8.880061149597168
Epoch 800, val loss: 1.0608270168304443
Epoch 810, training loss: 445.0799560546875 = 1.0592659711837769 + 50.0 * 8.880414009094238
Epoch 810, val loss: 1.0602474212646484
Epoch 820, training loss: 444.974609375 = 1.0586636066436768 + 50.0 * 8.878318786621094
Epoch 820, val loss: 1.0596657991409302
Epoch 830, training loss: 444.5027770996094 = 1.0580471754074097 + 50.0 * 8.868894577026367
Epoch 830, val loss: 1.0590806007385254
Epoch 840, training loss: 445.0990295410156 = 1.057483434677124 + 50.0 * 8.880830764770508
Epoch 840, val loss: 1.0585275888442993
Epoch 850, training loss: 444.90667724609375 = 1.0568851232528687 + 50.0 * 8.876996040344238
Epoch 850, val loss: 1.0579465627670288
Epoch 860, training loss: 445.2417297363281 = 1.0562779903411865 + 50.0 * 8.883708953857422
Epoch 860, val loss: 1.0573654174804688
Epoch 870, training loss: 445.610595703125 = 1.055689811706543 + 50.0 * 8.891098022460938
Epoch 870, val loss: 1.0567994117736816
Epoch 880, training loss: 445.76190185546875 = 1.0550775527954102 + 50.0 * 8.894136428833008
Epoch 880, val loss: 1.0561931133270264
Epoch 890, training loss: 445.89459228515625 = 1.0544394254684448 + 50.0 * 8.89680290222168
Epoch 890, val loss: 1.055602788925171
Epoch 900, training loss: 445.7539978027344 = 1.0538077354431152 + 50.0 * 8.894003868103027
Epoch 900, val loss: 1.0549864768981934
Epoch 910, training loss: 445.7606201171875 = 1.0531820058822632 + 50.0 * 8.894148826599121
Epoch 910, val loss: 1.0543936491012573
Epoch 920, training loss: 446.0772399902344 = 1.0525518655776978 + 50.0 * 8.900493621826172
Epoch 920, val loss: 1.0537813901901245
Epoch 930, training loss: 446.1376647949219 = 1.0518954992294312 + 50.0 * 8.901715278625488
Epoch 930, val loss: 1.0531589984893799
Epoch 940, training loss: 446.12005615234375 = 1.0512170791625977 + 50.0 * 8.901376724243164
Epoch 940, val loss: 1.052503228187561
Epoch 950, training loss: 446.1972351074219 = 1.0505517721176147 + 50.0 * 8.902934074401855
Epoch 950, val loss: 1.0518698692321777
Epoch 960, training loss: 446.4208984375 = 1.0498977899551392 + 50.0 * 8.90742015838623
Epoch 960, val loss: 1.0512397289276123
Epoch 970, training loss: 446.6033020019531 = 1.0492311716079712 + 50.0 * 8.911081314086914
Epoch 970, val loss: 1.050575613975525
Epoch 980, training loss: 446.4618225097656 = 1.0485295057296753 + 50.0 * 8.908266067504883
Epoch 980, val loss: 1.0499199628829956
Epoch 990, training loss: 446.7513427734375 = 1.0478395223617554 + 50.0 * 8.914070129394531
Epoch 990, val loss: 1.0492576360702515
Epoch 1000, training loss: 446.93536376953125 = 1.0471282005310059 + 50.0 * 8.917764663696289
Epoch 1000, val loss: 1.0485841035842896
Epoch 1010, training loss: 447.00787353515625 = 1.0464341640472412 + 50.0 * 8.919228553771973
Epoch 1010, val loss: 1.0479146242141724
Epoch 1020, training loss: 446.73779296875 = 1.0456799268722534 + 50.0 * 8.91384220123291
Epoch 1020, val loss: 1.047215223312378
Epoch 1030, training loss: 446.9735412597656 = 1.0449512004852295 + 50.0 * 8.918571472167969
Epoch 1030, val loss: 1.0464885234832764
Epoch 1040, training loss: 447.2984619140625 = 1.0442157983779907 + 50.0 * 8.925085067749023
Epoch 1040, val loss: 1.045784592628479
Epoch 1050, training loss: 447.4261169433594 = 1.0434620380401611 + 50.0 * 8.927653312683105
Epoch 1050, val loss: 1.0450503826141357
Epoch 1060, training loss: 447.4448547363281 = 1.0427087545394897 + 50.0 * 8.928043365478516
Epoch 1060, val loss: 1.0443072319030762
Epoch 1070, training loss: 447.64862060546875 = 1.041917324066162 + 50.0 * 8.932133674621582
Epoch 1070, val loss: 1.0435633659362793
Epoch 1080, training loss: 447.503173828125 = 1.0410969257354736 + 50.0 * 8.929241180419922
Epoch 1080, val loss: 1.0427778959274292
Epoch 1090, training loss: 447.6900939941406 = 1.0402989387512207 + 50.0 * 8.932995796203613
Epoch 1090, val loss: 1.0420252084732056
Epoch 1100, training loss: 447.7990417480469 = 1.0395009517669678 + 50.0 * 8.93519115447998
Epoch 1100, val loss: 1.0412565469741821
Epoch 1110, training loss: 447.7645568847656 = 1.0386773347854614 + 50.0 * 8.934517860412598
Epoch 1110, val loss: 1.04045569896698
Epoch 1120, training loss: 447.9558410644531 = 1.037886381149292 + 50.0 * 8.938359260559082
Epoch 1120, val loss: 1.0396928787231445
Epoch 1130, training loss: 448.3244934082031 = 1.0370662212371826 + 50.0 * 8.945748329162598
Epoch 1130, val loss: 1.03891921043396
Epoch 1140, training loss: 448.29937744140625 = 1.0362299680709839 + 50.0 * 8.945262908935547
Epoch 1140, val loss: 1.0381168127059937
Epoch 1150, training loss: 448.5174560546875 = 1.0354130268096924 + 50.0 * 8.949641227722168
Epoch 1150, val loss: 1.0373358726501465
Epoch 1160, training loss: 448.3255310058594 = 1.0345505475997925 + 50.0 * 8.945819854736328
Epoch 1160, val loss: 1.0365172624588013
Epoch 1170, training loss: 448.33575439453125 = 1.0336816310882568 + 50.0 * 8.946041107177734
Epoch 1170, val loss: 1.035681128501892
Epoch 1180, training loss: 448.5636901855469 = 1.0328489542007446 + 50.0 * 8.950616836547852
Epoch 1180, val loss: 1.0348718166351318
Epoch 1190, training loss: 448.9286804199219 = 1.032004475593567 + 50.0 * 8.95793342590332
Epoch 1190, val loss: 1.0340555906295776
Epoch 1200, training loss: 448.7308349609375 = 1.031105399131775 + 50.0 * 8.953994750976562
Epoch 1200, val loss: 1.0331988334655762
Epoch 1210, training loss: 448.7467346191406 = 1.0301998853683472 + 50.0 * 8.954330444335938
Epoch 1210, val loss: 1.0323512554168701
Epoch 1220, training loss: 448.86224365234375 = 1.0293220281600952 + 50.0 * 8.956658363342285
Epoch 1220, val loss: 1.0315145254135132
Epoch 1230, training loss: 449.084716796875 = 1.0284327268600464 + 50.0 * 8.961125373840332
Epoch 1230, val loss: 1.030652642250061
Epoch 1240, training loss: 449.35479736328125 = 1.0275278091430664 + 50.0 * 8.966545104980469
Epoch 1240, val loss: 1.0297809839248657
Epoch 1250, training loss: 449.4445495605469 = 1.0266108512878418 + 50.0 * 8.968358993530273
Epoch 1250, val loss: 1.0289145708084106
Epoch 1260, training loss: 449.19189453125 = 1.0256692171096802 + 50.0 * 8.963324546813965
Epoch 1260, val loss: 1.0280303955078125
Epoch 1270, training loss: 449.2004699707031 = 1.0247292518615723 + 50.0 * 8.963515281677246
Epoch 1270, val loss: 1.0271037817001343
Epoch 1280, training loss: 449.3132019042969 = 1.023816466331482 + 50.0 * 8.965787887573242
Epoch 1280, val loss: 1.0262393951416016
Epoch 1290, training loss: 449.5646667480469 = 1.022911787033081 + 50.0 * 8.970834732055664
Epoch 1290, val loss: 1.025373101234436
Epoch 1300, training loss: 449.5504150390625 = 1.0219308137893677 + 50.0 * 8.970569610595703
Epoch 1300, val loss: 1.0244507789611816
Epoch 1310, training loss: 449.4158020019531 = 1.020943522453308 + 50.0 * 8.967897415161133
Epoch 1310, val loss: 1.0235134363174438
Epoch 1320, training loss: 449.5904235839844 = 1.0199778079986572 + 50.0 * 8.97140884399414
Epoch 1320, val loss: 1.022576093673706
Epoch 1330, training loss: 449.902587890625 = 1.0190083980560303 + 50.0 * 8.97767162322998
Epoch 1330, val loss: 1.0216374397277832
Epoch 1340, training loss: 449.86773681640625 = 1.0180139541625977 + 50.0 * 8.976994514465332
Epoch 1340, val loss: 1.0206910371780396
Epoch 1350, training loss: 449.89556884765625 = 1.0170001983642578 + 50.0 * 8.977571487426758
Epoch 1350, val loss: 1.0197081565856934
Epoch 1360, training loss: 449.8514404296875 = 1.0160397291183472 + 50.0 * 8.976707458496094
Epoch 1360, val loss: 1.0187886953353882
Epoch 1370, training loss: 449.98260498046875 = 1.0150177478790283 + 50.0 * 8.979351997375488
Epoch 1370, val loss: 1.0178147554397583
Epoch 1380, training loss: 450.2357482910156 = 1.014015555381775 + 50.0 * 8.984435081481934
Epoch 1380, val loss: 1.0168477296829224
Epoch 1390, training loss: 450.1143493652344 = 1.0129907131195068 + 50.0 * 8.982027053833008
Epoch 1390, val loss: 1.0158847570419312
Epoch 1400, training loss: 450.1195068359375 = 1.0119504928588867 + 50.0 * 8.98215103149414
Epoch 1400, val loss: 1.0148637294769287
Epoch 1410, training loss: 450.2438659667969 = 1.0109004974365234 + 50.0 * 8.984659194946289
Epoch 1410, val loss: 1.0138862133026123
Epoch 1420, training loss: 450.4206848144531 = 1.009857416152954 + 50.0 * 8.988216400146484
Epoch 1420, val loss: 1.01289701461792
Epoch 1430, training loss: 450.0984802246094 = 1.0087465047836304 + 50.0 * 8.981794357299805
Epoch 1430, val loss: 1.0118342638015747
Epoch 1440, training loss: 450.3066711425781 = 1.0076382160186768 + 50.0 * 8.985980987548828
Epoch 1440, val loss: 1.0107816457748413
Epoch 1450, training loss: 450.4291687011719 = 1.0066276788711548 + 50.0 * 8.98845100402832
Epoch 1450, val loss: 1.0098258256912231
Epoch 1460, training loss: 450.8173522949219 = 1.005551815032959 + 50.0 * 8.996235847473145
Epoch 1460, val loss: 1.0088109970092773
Epoch 1470, training loss: 450.90948486328125 = 1.0044548511505127 + 50.0 * 8.998100280761719
Epoch 1470, val loss: 1.0077699422836304
Epoch 1480, training loss: 450.9470520019531 = 1.0033555030822754 + 50.0 * 8.998873710632324
Epoch 1480, val loss: 1.0066673755645752
Epoch 1490, training loss: 450.89971923828125 = 1.0022337436676025 + 50.0 * 8.997949600219727
Epoch 1490, val loss: 1.005624771118164
Epoch 1500, training loss: 450.8448486328125 = 1.001110553741455 + 50.0 * 8.996874809265137
Epoch 1500, val loss: 1.0045762062072754
Epoch 1510, training loss: 451.179443359375 = 1.000015139579773 + 50.0 * 9.003588676452637
Epoch 1510, val loss: 1.0035176277160645
Epoch 1520, training loss: 451.0605773925781 = 0.9988319277763367 + 50.0 * 9.001235008239746
Epoch 1520, val loss: 1.0024042129516602
Epoch 1530, training loss: 451.3126525878906 = 0.9977139830589294 + 50.0 * 9.006299018859863
Epoch 1530, val loss: 1.0013221502304077
Epoch 1540, training loss: 451.3230285644531 = 0.996571958065033 + 50.0 * 9.006528854370117
Epoch 1540, val loss: 1.0001835823059082
Epoch 1550, training loss: 451.151123046875 = 0.9953457713127136 + 50.0 * 9.0031156539917
Epoch 1550, val loss: 0.999046266078949
Epoch 1560, training loss: 450.75213623046875 = 0.9941419959068298 + 50.0 * 8.995160102844238
Epoch 1560, val loss: 0.9979146718978882
Epoch 1570, training loss: 450.7376708984375 = 0.9929785132408142 + 50.0 * 8.994894027709961
Epoch 1570, val loss: 0.9968028664588928
Epoch 1580, training loss: 451.0891418457031 = 0.9918238520622253 + 50.0 * 9.001946449279785
Epoch 1580, val loss: 0.9956884384155273
Epoch 1590, training loss: 451.33953857421875 = 0.9907056093215942 + 50.0 * 9.006977081298828
Epoch 1590, val loss: 0.9946272969245911
Epoch 1600, training loss: 451.3255310058594 = 0.9895108938217163 + 50.0 * 9.006720542907715
Epoch 1600, val loss: 0.9934781193733215
Epoch 1610, training loss: 451.39141845703125 = 0.9883186221122742 + 50.0 * 9.008062362670898
Epoch 1610, val loss: 0.992335855960846
Epoch 1620, training loss: 451.5378112792969 = 0.9871434569358826 + 50.0 * 9.01101303100586
Epoch 1620, val loss: 0.9912127256393433
Epoch 1630, training loss: 451.77764892578125 = 0.985977053642273 + 50.0 * 9.015833854675293
Epoch 1630, val loss: 0.9901106357574463
Epoch 1640, training loss: 451.76519775390625 = 0.9847744703292847 + 50.0 * 9.015608787536621
Epoch 1640, val loss: 0.9889516234397888
Epoch 1650, training loss: 451.9834899902344 = 0.9835447072982788 + 50.0 * 9.019998550415039
Epoch 1650, val loss: 0.9877946376800537
Epoch 1660, training loss: 451.61273193359375 = 0.9822969436645508 + 50.0 * 9.012608528137207
Epoch 1660, val loss: 0.9866003394126892
Epoch 1670, training loss: 451.87017822265625 = 0.9811035990715027 + 50.0 * 9.017781257629395
Epoch 1670, val loss: 0.9854514002799988
Epoch 1680, training loss: 452.1319274902344 = 0.9799438118934631 + 50.0 * 9.023039817810059
Epoch 1680, val loss: 0.9843378067016602
Epoch 1690, training loss: 452.30511474609375 = 0.9787079691886902 + 50.0 * 9.026528358459473
Epoch 1690, val loss: 0.9831526875495911
Epoch 1700, training loss: 452.03656005859375 = 0.9773640632629395 + 50.0 * 9.021183967590332
Epoch 1700, val loss: 0.9819015860557556
Epoch 1710, training loss: 452.4508972167969 = 0.9762009978294373 + 50.0 * 9.029494285583496
Epoch 1710, val loss: 0.9807607531547546
Epoch 1720, training loss: 452.33074951171875 = 0.9749093651771545 + 50.0 * 9.027116775512695
Epoch 1720, val loss: 0.9795452356338501
Epoch 1730, training loss: 452.4818115234375 = 0.9736526012420654 + 50.0 * 9.030162811279297
Epoch 1730, val loss: 0.9783646464347839
Epoch 1740, training loss: 452.4112243652344 = 0.9724225401878357 + 50.0 * 9.028776168823242
Epoch 1740, val loss: 0.9771801829338074
Epoch 1750, training loss: 452.4115295410156 = 0.9711674451828003 + 50.0 * 9.028807640075684
Epoch 1750, val loss: 0.9759982228279114
Epoch 1760, training loss: 452.62255859375 = 0.9699273705482483 + 50.0 * 9.033052444458008
Epoch 1760, val loss: 0.9748075008392334
Epoch 1770, training loss: 452.6649169921875 = 0.9686594605445862 + 50.0 * 9.03392505645752
Epoch 1770, val loss: 0.9735847115516663
Epoch 1780, training loss: 452.66693115234375 = 0.967366635799408 + 50.0 * 9.033990859985352
Epoch 1780, val loss: 0.9723524451255798
Epoch 1790, training loss: 452.6800231933594 = 0.9661058187484741 + 50.0 * 9.034278869628906
Epoch 1790, val loss: 0.9711313247680664
Epoch 1800, training loss: 452.6789245605469 = 0.9647601842880249 + 50.0 * 9.034283638000488
Epoch 1800, val loss: 0.9698664546012878
Epoch 1810, training loss: 452.76971435546875 = 0.9634964466094971 + 50.0 * 9.036124229431152
Epoch 1810, val loss: 0.9686609506607056
Epoch 1820, training loss: 452.8319396972656 = 0.9622237682342529 + 50.0 * 9.037394523620605
Epoch 1820, val loss: 0.967439591884613
Epoch 1830, training loss: 452.63079833984375 = 0.9608771800994873 + 50.0 * 9.033398628234863
Epoch 1830, val loss: 0.966183602809906
Epoch 1840, training loss: 452.9280700683594 = 0.9595934748649597 + 50.0 * 9.039369583129883
Epoch 1840, val loss: 0.9649272561073303
Epoch 1850, training loss: 452.7524719238281 = 0.9582812190055847 + 50.0 * 9.035883903503418
Epoch 1850, val loss: 0.9636784195899963
Epoch 1860, training loss: 453.0936584472656 = 0.9570589065551758 + 50.0 * 9.042732238769531
Epoch 1860, val loss: 0.9625083208084106
Epoch 1870, training loss: 453.4024658203125 = 0.9558093547821045 + 50.0 * 9.048933029174805
Epoch 1870, val loss: 0.9612919688224792
Epoch 1880, training loss: 453.49896240234375 = 0.9544975757598877 + 50.0 * 9.050889015197754
Epoch 1880, val loss: 0.9600445032119751
Epoch 1890, training loss: 453.6544189453125 = 0.9531885385513306 + 50.0 * 9.054024696350098
Epoch 1890, val loss: 0.9587984681129456
Epoch 1900, training loss: 453.6520080566406 = 0.9518764615058899 + 50.0 * 9.05400276184082
Epoch 1900, val loss: 0.9575327038764954
Epoch 1910, training loss: 453.5405578613281 = 0.9505367279052734 + 50.0 * 9.051800727844238
Epoch 1910, val loss: 0.956265389919281
Epoch 1920, training loss: 453.7047119140625 = 0.949232280254364 + 50.0 * 9.055109977722168
Epoch 1920, val loss: 0.954997181892395
Epoch 1930, training loss: 453.90863037109375 = 0.947913646697998 + 50.0 * 9.05921459197998
Epoch 1930, val loss: 0.9537688493728638
Epoch 1940, training loss: 453.84344482421875 = 0.9465550780296326 + 50.0 * 9.057937622070312
Epoch 1940, val loss: 0.952458918094635
Epoch 1950, training loss: 453.677978515625 = 0.9452170133590698 + 50.0 * 9.054655075073242
Epoch 1950, val loss: 0.9511801600456238
Epoch 1960, training loss: 453.9687194824219 = 0.943918764591217 + 50.0 * 9.06049633026123
Epoch 1960, val loss: 0.9499641060829163
Epoch 1970, training loss: 454.1128234863281 = 0.9425668716430664 + 50.0 * 9.06340503692627
Epoch 1970, val loss: 0.9486660361289978
Epoch 1980, training loss: 454.0134582519531 = 0.9412000179290771 + 50.0 * 9.061445236206055
Epoch 1980, val loss: 0.947372555732727
Epoch 1990, training loss: 453.2481689453125 = 0.939661979675293 + 50.0 * 9.046170234680176
Epoch 1990, val loss: 0.9458795189857483
Epoch 2000, training loss: 453.2342529296875 = 0.9384738206863403 + 50.0 * 9.045915603637695
Epoch 2000, val loss: 0.9447788000106812
Epoch 2010, training loss: 453.05535888671875 = 0.9371821880340576 + 50.0 * 9.042363166809082
Epoch 2010, val loss: 0.9435827136039734
Epoch 2020, training loss: 453.3388977050781 = 0.9358834028244019 + 50.0 * 9.048060417175293
Epoch 2020, val loss: 0.9423540830612183
Epoch 2030, training loss: 453.792724609375 = 0.9345729947090149 + 50.0 * 9.05716323852539
Epoch 2030, val loss: 0.9410704374313354
Epoch 2040, training loss: 454.10284423828125 = 0.9331799745559692 + 50.0 * 9.063393592834473
Epoch 2040, val loss: 0.9397164583206177
Epoch 2050, training loss: 454.10595703125 = 0.9317010641098022 + 50.0 * 9.063485145568848
Epoch 2050, val loss: 0.9383255839347839
Epoch 2060, training loss: 454.3504638671875 = 0.9302982091903687 + 50.0 * 9.068403244018555
Epoch 2060, val loss: 0.9369829893112183
Epoch 2070, training loss: 454.3697814941406 = 0.9289118051528931 + 50.0 * 9.068817138671875
Epoch 2070, val loss: 0.9356855154037476
Epoch 2080, training loss: 454.3737487792969 = 0.9274829030036926 + 50.0 * 9.068924903869629
Epoch 2080, val loss: 0.9343401789665222
Epoch 2090, training loss: 454.5207824707031 = 0.9261351227760315 + 50.0 * 9.071892738342285
Epoch 2090, val loss: 0.9330651760101318
Epoch 2100, training loss: 454.6399841308594 = 0.924734890460968 + 50.0 * 9.074304580688477
Epoch 2100, val loss: 0.9317283034324646
Epoch 2110, training loss: 454.4176330566406 = 0.9232923984527588 + 50.0 * 9.069887161254883
Epoch 2110, val loss: 0.9303755164146423
Epoch 2120, training loss: 454.5044250488281 = 0.92192542552948 + 50.0 * 9.071649551391602
Epoch 2120, val loss: 0.9290995597839355
Epoch 2130, training loss: 454.69970703125 = 0.9205323457717896 + 50.0 * 9.075583457946777
Epoch 2130, val loss: 0.9277949929237366
Epoch 2140, training loss: 454.9850769042969 = 0.9191125631332397 + 50.0 * 9.081319808959961
Epoch 2140, val loss: 0.9264325499534607
Epoch 2150, training loss: 454.8772277832031 = 0.9176430702209473 + 50.0 * 9.079192161560059
Epoch 2150, val loss: 0.9250518679618835
Epoch 2160, training loss: 454.954345703125 = 0.9161943793296814 + 50.0 * 9.08076286315918
Epoch 2160, val loss: 0.9236904978752136
Epoch 2170, training loss: 455.1009216308594 = 0.9147229194641113 + 50.0 * 9.083724021911621
Epoch 2170, val loss: 0.9223105311393738
Epoch 2180, training loss: 455.2196044921875 = 0.9132521748542786 + 50.0 * 9.086127281188965
Epoch 2180, val loss: 0.9209243059158325
Epoch 2190, training loss: 455.28485107421875 = 0.9117818474769592 + 50.0 * 9.087461471557617
Epoch 2190, val loss: 0.9195316433906555
Epoch 2200, training loss: 455.27215576171875 = 0.9102931618690491 + 50.0 * 9.087237358093262
Epoch 2200, val loss: 0.9181373119354248
Epoch 2210, training loss: 455.3985290527344 = 0.9088115692138672 + 50.0 * 9.089794158935547
Epoch 2210, val loss: 0.916725754737854
Epoch 2220, training loss: 455.1931457519531 = 0.9073310494422913 + 50.0 * 9.085716247558594
Epoch 2220, val loss: 0.9153361916542053
Epoch 2230, training loss: 455.41204833984375 = 0.9058515429496765 + 50.0 * 9.090124130249023
Epoch 2230, val loss: 0.9139637351036072
Epoch 2240, training loss: 455.7314758300781 = 0.9043508768081665 + 50.0 * 9.096542358398438
Epoch 2240, val loss: 0.9125705361366272
Epoch 2250, training loss: 456.0511474609375 = 0.9028869867324829 + 50.0 * 9.102965354919434
Epoch 2250, val loss: 0.9111721515655518
Epoch 2260, training loss: 455.6327209472656 = 0.9015488624572754 + 50.0 * 9.094623565673828
Epoch 2260, val loss: 0.9099026918411255
Epoch 2270, training loss: 454.8605651855469 = 0.9000667333602905 + 50.0 * 9.07921028137207
Epoch 2270, val loss: 0.9085596799850464
Epoch 2280, training loss: 453.98779296875 = 0.8984867930412292 + 50.0 * 9.061785697937012
Epoch 2280, val loss: 0.9070574641227722
Epoch 2290, training loss: 454.5570373535156 = 0.8970373868942261 + 50.0 * 9.073200225830078
Epoch 2290, val loss: 0.9057172536849976
Epoch 2300, training loss: 455.0125427246094 = 0.8955370187759399 + 50.0 * 9.082340240478516
Epoch 2300, val loss: 0.9043377637863159
Epoch 2310, training loss: 455.4482727050781 = 0.8940911889076233 + 50.0 * 9.091083526611328
Epoch 2310, val loss: 0.9030027389526367
Epoch 2320, training loss: 455.61761474609375 = 0.8925866484642029 + 50.0 * 9.094500541687012
Epoch 2320, val loss: 0.9016121625900269
Epoch 2330, training loss: 455.73431396484375 = 0.8911020159721375 + 50.0 * 9.096863746643066
Epoch 2330, val loss: 0.9002283215522766
Epoch 2340, training loss: 455.6858215332031 = 0.889610767364502 + 50.0 * 9.095924377441406
Epoch 2340, val loss: 0.8988676071166992
Epoch 2350, training loss: 455.9410095214844 = 0.8881478309631348 + 50.0 * 9.101057052612305
Epoch 2350, val loss: 0.8975033760070801
Epoch 2360, training loss: 456.1017761230469 = 0.8866755962371826 + 50.0 * 9.104301452636719
Epoch 2360, val loss: 0.8961372375488281
Epoch 2370, training loss: 456.0530090332031 = 0.8851913213729858 + 50.0 * 9.10335636138916
Epoch 2370, val loss: 0.8947510719299316
Epoch 2380, training loss: 456.0362548828125 = 0.883735179901123 + 50.0 * 9.103050231933594
Epoch 2380, val loss: 0.8933996558189392
Epoch 2390, training loss: 456.19195556640625 = 0.8822725415229797 + 50.0 * 9.106193542480469
Epoch 2390, val loss: 0.8920708894729614
Epoch 2400, training loss: 456.22698974609375 = 0.8807879090309143 + 50.0 * 9.106924057006836
Epoch 2400, val loss: 0.89070725440979
Epoch 2410, training loss: 456.04266357421875 = 0.8793559074401855 + 50.0 * 9.103265762329102
Epoch 2410, val loss: 0.8893467783927917
Epoch 2420, training loss: 456.2000732421875 = 0.8779131174087524 + 50.0 * 9.106443405151367
Epoch 2420, val loss: 0.8880046606063843
Epoch 2430, training loss: 456.5157775878906 = 0.8764854073524475 + 50.0 * 9.112785339355469
Epoch 2430, val loss: 0.8866859674453735
Epoch 2440, training loss: 456.61419677734375 = 0.8750432133674622 + 50.0 * 9.11478328704834
Epoch 2440, val loss: 0.8853408694267273
Epoch 2450, training loss: 456.4363708496094 = 0.8735818862915039 + 50.0 * 9.111255645751953
Epoch 2450, val loss: 0.8839766383171082
Epoch 2460, training loss: 455.9562072753906 = 0.8721490502357483 + 50.0 * 9.101680755615234
Epoch 2460, val loss: 0.8826850056648254
Epoch 2470, training loss: 456.0762939453125 = 0.8707652688026428 + 50.0 * 9.104110717773438
Epoch 2470, val loss: 0.8813880085945129
Epoch 2480, training loss: 456.519775390625 = 0.8693533539772034 + 50.0 * 9.113008499145508
Epoch 2480, val loss: 0.8800859451293945
Epoch 2490, training loss: 456.7838439941406 = 0.8679479360580444 + 50.0 * 9.118317604064941
Epoch 2490, val loss: 0.8787820339202881
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.662463768115942
0.8133739042237196
=== training gcn model ===
Epoch 0, training loss: 513.8563842773438 = 1.098612666130066 + 50.0 * 10.255155563354492
Epoch 0, val loss: 1.0986131429672241
Epoch 10, training loss: 492.8691101074219 = 1.0986117124557495 + 50.0 * 9.835410118103027
Epoch 10, val loss: 1.098612904548645
Epoch 20, training loss: 484.07965087890625 = 1.0986114740371704 + 50.0 * 9.659621238708496
Epoch 20, val loss: 1.098612904548645
Epoch 30, training loss: 477.6221923828125 = 1.0986114740371704 + 50.0 * 9.530471801757812
Epoch 30, val loss: 1.098612904548645
Epoch 40, training loss: 472.4399108886719 = 1.0986114740371704 + 50.0 * 9.426826477050781
Epoch 40, val loss: 1.098612904548645
Epoch 50, training loss: 468.1100158691406 = 1.0986114740371704 + 50.0 * 9.340228080749512
Epoch 50, val loss: 1.098612904548645
Epoch 60, training loss: 464.4081726074219 = 1.0986114740371704 + 50.0 * 9.266191482543945
Epoch 60, val loss: 1.098612904548645
Epoch 70, training loss: 461.23406982421875 = 1.0986114740371704 + 50.0 * 9.202709197998047
Epoch 70, val loss: 1.098612904548645
Epoch 80, training loss: 458.4311218261719 = 1.0986114740371704 + 50.0 * 9.146650314331055
Epoch 80, val loss: 1.098612904548645
Epoch 90, training loss: 456.0015869140625 = 1.0986114740371704 + 50.0 * 9.09805965423584
Epoch 90, val loss: 1.098612904548645
Epoch 100, training loss: 453.95928955078125 = 1.0986114740371704 + 50.0 * 9.05721378326416
Epoch 100, val loss: 1.098612904548645
Epoch 110, training loss: 452.1983947753906 = 1.0986114740371704 + 50.0 * 9.021995544433594
Epoch 110, val loss: 1.098612904548645
Epoch 120, training loss: 450.7099304199219 = 1.0986114740371704 + 50.0 * 8.992226600646973
Epoch 120, val loss: 1.098612904548645
Epoch 130, training loss: 449.3878173828125 = 1.0986114740371704 + 50.0 * 8.965784072875977
Epoch 130, val loss: 1.098612904548645
Epoch 140, training loss: 448.2298583984375 = 1.0986114740371704 + 50.0 * 8.942625045776367
Epoch 140, val loss: 1.098612904548645
Epoch 150, training loss: 447.21142578125 = 1.0986114740371704 + 50.0 * 8.922256469726562
Epoch 150, val loss: 1.098612904548645
Epoch 160, training loss: 446.34259033203125 = 1.0986114740371704 + 50.0 * 8.904879570007324
Epoch 160, val loss: 1.098612904548645
Epoch 170, training loss: 445.5795593261719 = 1.0986114740371704 + 50.0 * 8.889618873596191
Epoch 170, val loss: 1.098612904548645
Epoch 180, training loss: 444.8878479003906 = 1.0986114740371704 + 50.0 * 8.875784873962402
Epoch 180, val loss: 1.098612904548645
Epoch 190, training loss: 444.33978271484375 = 1.0986114740371704 + 50.0 * 8.864823341369629
Epoch 190, val loss: 1.098612904548645
Epoch 200, training loss: 443.81329345703125 = 1.0986114740371704 + 50.0 * 8.854293823242188
Epoch 200, val loss: 1.098612904548645
Epoch 210, training loss: 443.2347106933594 = 1.0986114740371704 + 50.0 * 8.842721939086914
Epoch 210, val loss: 1.098612904548645
Epoch 220, training loss: 442.8631896972656 = 1.0986114740371704 + 50.0 * 8.835291862487793
Epoch 220, val loss: 1.098612904548645
Epoch 230, training loss: 442.5669250488281 = 1.0986114740371704 + 50.0 * 8.829366683959961
Epoch 230, val loss: 1.098612904548645
Epoch 240, training loss: 442.19677734375 = 1.0986114740371704 + 50.0 * 8.8219633102417
Epoch 240, val loss: 1.098612904548645
Epoch 250, training loss: 441.9077453613281 = 1.0986114740371704 + 50.0 * 8.816183090209961
Epoch 250, val loss: 1.098612904548645
Epoch 260, training loss: 441.871337890625 = 1.0986114740371704 + 50.0 * 8.815454483032227
Epoch 260, val loss: 1.098612904548645
Epoch 270, training loss: 442.09619140625 = 1.0986114740371704 + 50.0 * 8.819952011108398
Epoch 270, val loss: 1.098612904548645
Epoch 280, training loss: 440.87921142578125 = 1.0986114740371704 + 50.0 * 8.795612335205078
Epoch 280, val loss: 1.098612904548645
Epoch 290, training loss: 440.8778381347656 = 1.0986114740371704 + 50.0 * 8.795584678649902
Epoch 290, val loss: 1.098612904548645
Epoch 300, training loss: 440.80029296875 = 1.0986114740371704 + 50.0 * 8.794034004211426
Epoch 300, val loss: 1.098612904548645
Epoch 310, training loss: 440.72296142578125 = 1.0986114740371704 + 50.0 * 8.792487144470215
Epoch 310, val loss: 1.098612904548645
Epoch 320, training loss: 440.7264404296875 = 1.0986114740371704 + 50.0 * 8.792556762695312
Epoch 320, val loss: 1.098612904548645
Epoch 330, training loss: 440.65625 = 1.0986114740371704 + 50.0 * 8.791152954101562
Epoch 330, val loss: 1.098612904548645
Epoch 340, training loss: 440.67388916015625 = 1.0986114740371704 + 50.0 * 8.791505813598633
Epoch 340, val loss: 1.098612904548645
Epoch 350, training loss: 440.57061767578125 = 1.0986114740371704 + 50.0 * 8.789440155029297
Epoch 350, val loss: 1.098612904548645
Epoch 360, training loss: 440.54132080078125 = 1.0986114740371704 + 50.0 * 8.788854598999023
Epoch 360, val loss: 1.0986131429672241
Epoch 370, training loss: 440.5390930175781 = 1.0986111164093018 + 50.0 * 8.788809776306152
Epoch 370, val loss: 1.098612904548645
Epoch 380, training loss: 440.3708190917969 = 1.098610281944275 + 50.0 * 8.785444259643555
Epoch 380, val loss: 1.0986125469207764
Epoch 390, training loss: 440.3707275390625 = 1.0986086130142212 + 50.0 * 8.785442352294922
Epoch 390, val loss: 1.09861159324646
Epoch 400, training loss: 440.38177490234375 = 1.098610520362854 + 50.0 * 8.785663604736328
Epoch 400, val loss: 1.098612904548645
Epoch 410, training loss: 440.3180847167969 = 1.098610520362854 + 50.0 * 8.78438949584961
Epoch 410, val loss: 1.098612904548645
Epoch 420, training loss: 440.4540710449219 = 1.098610281944275 + 50.0 * 8.787109375
Epoch 420, val loss: 1.098612904548645
Epoch 430, training loss: 440.25970458984375 = 1.0986096858978271 + 50.0 * 8.783222198486328
Epoch 430, val loss: 1.098612666130066
Epoch 440, training loss: 440.1675720214844 = 1.0986090898513794 + 50.0 * 8.781379699707031
Epoch 440, val loss: 1.098611831665039
Epoch 450, training loss: 441.130126953125 = 1.0986042022705078 + 50.0 * 8.800630569458008
Epoch 450, val loss: 1.098602533340454
Epoch 460, training loss: 441.3673400878906 = 1.0985844135284424 + 50.0 * 8.805375099182129
Epoch 460, val loss: 1.098588228225708
Epoch 470, training loss: 441.273193359375 = 1.0985584259033203 + 50.0 * 8.803492546081543
Epoch 470, val loss: 1.098555326461792
Epoch 480, training loss: 441.55804443359375 = 1.098516583442688 + 50.0 * 8.80919075012207
Epoch 480, val loss: 1.0985140800476074
Epoch 490, training loss: 441.0646057128906 = 1.098462462425232 + 50.0 * 8.799323081970215
Epoch 490, val loss: 1.098462462425232
Epoch 500, training loss: 441.2352600097656 = 1.0984032154083252 + 50.0 * 8.80273723602295
Epoch 500, val loss: 1.0984065532684326
Epoch 510, training loss: 441.54962158203125 = 1.0983374118804932 + 50.0 * 8.809025764465332
Epoch 510, val loss: 1.09834623336792
Epoch 520, training loss: 441.7657470703125 = 1.0982694625854492 + 50.0 * 8.813349723815918
Epoch 520, val loss: 1.0982837677001953
Epoch 530, training loss: 441.7374267578125 = 1.098196268081665 + 50.0 * 8.812784194946289
Epoch 530, val loss: 1.0982165336608887
Epoch 540, training loss: 441.7887268066406 = 1.0981210470199585 + 50.0 * 8.813812255859375
Epoch 540, val loss: 1.0981477499008179
Epoch 550, training loss: 441.8486633300781 = 1.0980393886566162 + 50.0 * 8.815011978149414
Epoch 550, val loss: 1.0980756282806396
Epoch 560, training loss: 442.0665588378906 = 1.0979564189910889 + 50.0 * 8.819372177124023
Epoch 560, val loss: 1.0980000495910645
Epoch 570, training loss: 442.0679626464844 = 1.0978680849075317 + 50.0 * 8.819401741027832
Epoch 570, val loss: 1.097921371459961
Epoch 580, training loss: 442.14703369140625 = 1.0977784395217896 + 50.0 * 8.820984840393066
Epoch 580, val loss: 1.0978387594223022
Epoch 590, training loss: 442.2222595214844 = 1.0976836681365967 + 50.0 * 8.822491645812988
Epoch 590, val loss: 1.0977551937103271
Epoch 600, training loss: 442.3310852050781 = 1.097585678100586 + 50.0 * 8.82466983795166
Epoch 600, val loss: 1.097668170928955
Epoch 610, training loss: 442.2989807128906 = 1.0974838733673096 + 50.0 * 8.824029922485352
Epoch 610, val loss: 1.0975756645202637
Epoch 620, training loss: 442.6332702636719 = 1.0973811149597168 + 50.0 * 8.830718040466309
Epoch 620, val loss: 1.0974817276000977
Epoch 630, training loss: 442.62017822265625 = 1.0972723960876465 + 50.0 * 8.830458641052246
Epoch 630, val loss: 1.0973821878433228
Epoch 640, training loss: 442.7005310058594 = 1.0971554517745972 + 50.0 * 8.832067489624023
Epoch 640, val loss: 1.0972800254821777
Epoch 650, training loss: 442.766845703125 = 1.0970358848571777 + 50.0 * 8.833395957946777
Epoch 650, val loss: 1.0971710681915283
Epoch 660, training loss: 443.08062744140625 = 1.0969222784042358 + 50.0 * 8.83967399597168
Epoch 660, val loss: 1.0970662832260132
Epoch 670, training loss: 443.1815490722656 = 1.0967986583709717 + 50.0 * 8.841694831848145
Epoch 670, val loss: 1.0969575643539429
Epoch 680, training loss: 443.2757873535156 = 1.096676230430603 + 50.0 * 8.843582153320312
Epoch 680, val loss: 1.0968492031097412
Epoch 690, training loss: 443.0340576171875 = 1.0965498685836792 + 50.0 * 8.838749885559082
Epoch 690, val loss: 1.096736192703247
Epoch 700, training loss: 443.2228088378906 = 1.096423864364624 + 50.0 * 8.842527389526367
Epoch 700, val loss: 1.0966243743896484
Epoch 710, training loss: 443.1405334472656 = 1.0962990522384644 + 50.0 * 8.8408842086792
Epoch 710, val loss: 1.0965131521224976
Epoch 720, training loss: 443.6392517089844 = 1.0961699485778809 + 50.0 * 8.850861549377441
Epoch 720, val loss: 1.0963995456695557
Epoch 730, training loss: 443.9607849121094 = 1.0960414409637451 + 50.0 * 8.857295036315918
Epoch 730, val loss: 1.09628427028656
Epoch 740, training loss: 444.0699462890625 = 1.0959069728851318 + 50.0 * 8.859480857849121
Epoch 740, val loss: 1.0961663722991943
Epoch 750, training loss: 444.1330871582031 = 1.0957751274108887 + 50.0 * 8.860746383666992
Epoch 750, val loss: 1.0960469245910645
Epoch 760, training loss: 444.275390625 = 1.0956414937973022 + 50.0 * 8.863595008850098
Epoch 760, val loss: 1.0959330797195435
Epoch 770, training loss: 444.4014892578125 = 1.0955052375793457 + 50.0 * 8.866119384765625
Epoch 770, val loss: 1.0958107709884644
Epoch 780, training loss: 444.39617919921875 = 1.0953657627105713 + 50.0 * 8.866016387939453
Epoch 780, val loss: 1.095689058303833
Epoch 790, training loss: 444.1551513671875 = 1.0952279567718506 + 50.0 * 8.861198425292969
Epoch 790, val loss: 1.0955649614334106
Epoch 800, training loss: 444.3973083496094 = 1.0950874090194702 + 50.0 * 8.866044044494629
Epoch 800, val loss: 1.0954405069351196
Epoch 810, training loss: 444.6138610839844 = 1.0949493646621704 + 50.0 * 8.870378494262695
Epoch 810, val loss: 1.0953172445297241
Epoch 820, training loss: 444.6132507324219 = 1.0948081016540527 + 50.0 * 8.870368957519531
Epoch 820, val loss: 1.0951963663101196
Epoch 830, training loss: 444.7742919921875 = 1.0946673154830933 + 50.0 * 8.873592376708984
Epoch 830, val loss: 1.0950709581375122
Epoch 840, training loss: 444.9112243652344 = 1.0945249795913696 + 50.0 * 8.876334190368652
Epoch 840, val loss: 1.0949442386627197
Epoch 850, training loss: 444.9607849121094 = 1.0943803787231445 + 50.0 * 8.877327919006348
Epoch 850, val loss: 1.094818115234375
Epoch 860, training loss: 445.11346435546875 = 1.0942414999008179 + 50.0 * 8.88038444519043
Epoch 860, val loss: 1.094694972038269
Epoch 870, training loss: 445.2168884277344 = 1.094094157218933 + 50.0 * 8.882455825805664
Epoch 870, val loss: 1.0945647954940796
Epoch 880, training loss: 445.3056335449219 = 1.0939490795135498 + 50.0 * 8.884233474731445
Epoch 880, val loss: 1.0944381952285767
Epoch 890, training loss: 445.5815124511719 = 1.0938102006912231 + 50.0 * 8.889754295349121
Epoch 890, val loss: 1.094314694404602
Epoch 900, training loss: 445.7007751464844 = 1.0936683416366577 + 50.0 * 8.892142295837402
Epoch 900, val loss: 1.0941888093948364
Epoch 910, training loss: 445.68328857421875 = 1.093523621559143 + 50.0 * 8.89179515838623
Epoch 910, val loss: 1.0940618515014648
Epoch 920, training loss: 445.84722900390625 = 1.0933703184127808 + 50.0 * 8.895076751708984
Epoch 920, val loss: 1.0939321517944336
Epoch 930, training loss: 445.9272155761719 = 1.0932221412658691 + 50.0 * 8.896679878234863
Epoch 930, val loss: 1.093802809715271
Epoch 940, training loss: 446.152099609375 = 1.0930787324905396 + 50.0 * 8.901180267333984
Epoch 940, val loss: 1.0936747789382935
Epoch 950, training loss: 446.4198303222656 = 1.0929319858551025 + 50.0 * 8.906538009643555
Epoch 950, val loss: 1.0935485363006592
Epoch 960, training loss: 446.8177490234375 = 1.0927844047546387 + 50.0 * 8.914499282836914
Epoch 960, val loss: 1.0934207439422607
Epoch 970, training loss: 446.52996826171875 = 1.092633605003357 + 50.0 * 8.908746719360352
Epoch 970, val loss: 1.0932916402816772
Epoch 980, training loss: 446.6024169921875 = 1.0924913883209229 + 50.0 * 8.910198211669922
Epoch 980, val loss: 1.093169093132019
Epoch 990, training loss: 447.05877685546875 = 1.0923457145690918 + 50.0 * 8.919328689575195
Epoch 990, val loss: 1.0930439233779907
Epoch 1000, training loss: 447.3194580078125 = 1.092191457748413 + 50.0 * 8.924545288085938
Epoch 1000, val loss: 1.0929096937179565
Epoch 1010, training loss: 447.56903076171875 = 1.0920346975326538 + 50.0 * 8.929539680480957
Epoch 1010, val loss: 1.0927754640579224
Epoch 1020, training loss: 447.7405700683594 = 1.091880440711975 + 50.0 * 8.932973861694336
Epoch 1020, val loss: 1.0926369428634644
Epoch 1030, training loss: 447.65045166015625 = 1.0917209386825562 + 50.0 * 8.931174278259277
Epoch 1030, val loss: 1.09249746799469
Epoch 1040, training loss: 447.81280517578125 = 1.0915647745132446 + 50.0 * 8.934425354003906
Epoch 1040, val loss: 1.0923625230789185
Epoch 1050, training loss: 448.0361022949219 = 1.091417670249939 + 50.0 * 8.93889331817627
Epoch 1050, val loss: 1.0922315120697021
Epoch 1060, training loss: 448.2284240722656 = 1.0912621021270752 + 50.0 * 8.942743301391602
Epoch 1060, val loss: 1.0920979976654053
Epoch 1070, training loss: 448.2873840332031 = 1.091085433959961 + 50.0 * 8.943925857543945
Epoch 1070, val loss: 1.091937780380249
Epoch 1080, training loss: 448.1484375 = 1.0909230709075928 + 50.0 * 8.941150665283203
Epoch 1080, val loss: 1.0917962789535522
Epoch 1090, training loss: 448.30230712890625 = 1.0907741785049438 + 50.0 * 8.944231033325195
Epoch 1090, val loss: 1.0916671752929688
Epoch 1100, training loss: 448.5876159667969 = 1.0906248092651367 + 50.0 * 8.949939727783203
Epoch 1100, val loss: 1.0915355682373047
Epoch 1110, training loss: 448.7046813964844 = 1.0904700756072998 + 50.0 * 8.95228385925293
Epoch 1110, val loss: 1.09140145778656
Epoch 1120, training loss: 449.0023193359375 = 1.0903199911117554 + 50.0 * 8.958239555358887
Epoch 1120, val loss: 1.091270089149475
Epoch 1130, training loss: 448.92718505859375 = 1.0901660919189453 + 50.0 * 8.956740379333496
Epoch 1130, val loss: 1.091133713722229
Epoch 1140, training loss: 449.1483459472656 = 1.090013027191162 + 50.0 * 8.961166381835938
Epoch 1140, val loss: 1.0909974575042725
Epoch 1150, training loss: 449.23291015625 = 1.0898561477661133 + 50.0 * 8.962861061096191
Epoch 1150, val loss: 1.0908591747283936
Epoch 1160, training loss: 449.0912780761719 = 1.0896772146224976 + 50.0 * 8.960031509399414
Epoch 1160, val loss: 1.0906976461410522
Epoch 1170, training loss: 449.16949462890625 = 1.0895248651504517 + 50.0 * 8.961599349975586
Epoch 1170, val loss: 1.0905712842941284
Epoch 1180, training loss: 449.5631408691406 = 1.089378833770752 + 50.0 * 8.969474792480469
Epoch 1180, val loss: 1.090442419052124
Epoch 1190, training loss: 449.8817443847656 = 1.0892271995544434 + 50.0 * 8.975850105285645
Epoch 1190, val loss: 1.0903053283691406
Epoch 1200, training loss: 449.9818420410156 = 1.0890649557113647 + 50.0 * 8.977855682373047
Epoch 1200, val loss: 1.090166687965393
Epoch 1210, training loss: 450.16827392578125 = 1.0889099836349487 + 50.0 * 8.981587409973145
Epoch 1210, val loss: 1.0900328159332275
Epoch 1220, training loss: 450.48822021484375 = 1.0887551307678223 + 50.0 * 8.98798942565918
Epoch 1220, val loss: 1.0898929834365845
Epoch 1230, training loss: 450.2677307128906 = 1.0885883569717407 + 50.0 * 8.983582496643066
Epoch 1230, val loss: 1.0897477865219116
Epoch 1240, training loss: 450.54217529296875 = 1.088426947593689 + 50.0 * 8.98907470703125
Epoch 1240, val loss: 1.089609146118164
Epoch 1250, training loss: 450.8470153808594 = 1.0882670879364014 + 50.0 * 8.9951753616333
Epoch 1250, val loss: 1.0894687175750732
Epoch 1260, training loss: 450.5228576660156 = 1.0881046056747437 + 50.0 * 8.98869514465332
Epoch 1260, val loss: 1.0893265008926392
Epoch 1270, training loss: 450.679443359375 = 1.0879418849945068 + 50.0 * 8.991829872131348
Epoch 1270, val loss: 1.0891865491867065
Epoch 1280, training loss: 450.8368225097656 = 1.0877817869186401 + 50.0 * 8.994980812072754
Epoch 1280, val loss: 1.089045763015747
Epoch 1290, training loss: 451.1527404785156 = 1.0876213312149048 + 50.0 * 9.001302719116211
Epoch 1290, val loss: 1.0889075994491577
Epoch 1300, training loss: 451.219970703125 = 1.0874614715576172 + 50.0 * 9.002650260925293
Epoch 1300, val loss: 1.0887638330459595
Epoch 1310, training loss: 451.20458984375 = 1.0872920751571655 + 50.0 * 9.00234603881836
Epoch 1310, val loss: 1.0886224508285522
Epoch 1320, training loss: 451.3910217285156 = 1.0871399641036987 + 50.0 * 9.006077766418457
Epoch 1320, val loss: 1.088487148284912
Epoch 1330, training loss: 451.82073974609375 = 1.086983323097229 + 50.0 * 9.01467514038086
Epoch 1330, val loss: 1.0883517265319824
Epoch 1340, training loss: 451.77880859375 = 1.0868138074874878 + 50.0 * 9.013839721679688
Epoch 1340, val loss: 1.088211178779602
Epoch 1350, training loss: 451.7319641113281 = 1.0866559743881226 + 50.0 * 9.012906074523926
Epoch 1350, val loss: 1.0880731344223022
Epoch 1360, training loss: 452.1535949707031 = 1.0864956378936768 + 50.0 * 9.021342277526855
Epoch 1360, val loss: 1.0879361629486084
Epoch 1370, training loss: 452.1410827636719 = 1.0863360166549683 + 50.0 * 9.021095275878906
Epoch 1370, val loss: 1.0877954959869385
Epoch 1380, training loss: 451.9281005859375 = 1.086169719696045 + 50.0 * 9.016838073730469
Epoch 1380, val loss: 1.0876489877700806
Epoch 1390, training loss: 451.9369201660156 = 1.0860074758529663 + 50.0 * 9.01701831817627
Epoch 1390, val loss: 1.0875047445297241
Epoch 1400, training loss: 452.2388000488281 = 1.085854411125183 + 50.0 * 9.023058891296387
Epoch 1400, val loss: 1.0873770713806152
Epoch 1410, training loss: 452.47515869140625 = 1.0856951475143433 + 50.0 * 9.027789115905762
Epoch 1410, val loss: 1.0872411727905273
Epoch 1420, training loss: 452.5507507324219 = 1.0855380296707153 + 50.0 * 9.029304504394531
Epoch 1420, val loss: 1.0871020555496216
Epoch 1430, training loss: 452.8414306640625 = 1.0853772163391113 + 50.0 * 9.035120964050293
Epoch 1430, val loss: 1.086965560913086
Epoch 1440, training loss: 452.8489685058594 = 1.0852200984954834 + 50.0 * 9.03527545928955
Epoch 1440, val loss: 1.086830496788025
Epoch 1450, training loss: 453.0130920410156 = 1.0850670337677002 + 50.0 * 9.03856086730957
Epoch 1450, val loss: 1.0866951942443848
Epoch 1460, training loss: 452.94189453125 = 1.0849043130874634 + 50.0 * 9.037139892578125
Epoch 1460, val loss: 1.0865545272827148
Epoch 1470, training loss: 453.1282653808594 = 1.0847415924072266 + 50.0 * 9.040870666503906
Epoch 1470, val loss: 1.08641517162323
Epoch 1480, training loss: 452.09027099609375 = 1.0845539569854736 + 50.0 * 9.020113945007324
Epoch 1480, val loss: 1.086262822151184
Epoch 1490, training loss: 451.3934631347656 = 1.0843846797943115 + 50.0 * 9.006181716918945
Epoch 1490, val loss: 1.0861092805862427
Epoch 1500, training loss: 451.73126220703125 = 1.0842341184616089 + 50.0 * 9.012940406799316
Epoch 1500, val loss: 1.085979700088501
Epoch 1510, training loss: 452.1528015136719 = 1.0840758085250854 + 50.0 * 9.021374702453613
Epoch 1510, val loss: 1.085853934288025
Epoch 1520, training loss: 452.7899475097656 = 1.0839312076568604 + 50.0 * 9.034120559692383
Epoch 1520, val loss: 1.0857341289520264
Epoch 1530, training loss: 452.96197509765625 = 1.0837774276733398 + 50.0 * 9.037564277648926
Epoch 1530, val loss: 1.085605263710022
Epoch 1540, training loss: 453.2193603515625 = 1.0836195945739746 + 50.0 * 9.042715072631836
Epoch 1540, val loss: 1.085471749305725
Epoch 1550, training loss: 453.42291259765625 = 1.0834636688232422 + 50.0 * 9.046789169311523
Epoch 1550, val loss: 1.0853363275527954
Epoch 1560, training loss: 453.3097229003906 = 1.0833030939102173 + 50.0 * 9.044528007507324
Epoch 1560, val loss: 1.0851960182189941
Epoch 1570, training loss: 453.1606750488281 = 1.083142638206482 + 50.0 * 9.041550636291504
Epoch 1570, val loss: 1.0850553512573242
Epoch 1580, training loss: 453.54718017578125 = 1.0829781293869019 + 50.0 * 9.049283981323242
Epoch 1580, val loss: 1.0849179029464722
Epoch 1590, training loss: 453.61578369140625 = 1.0828258991241455 + 50.0 * 9.0506591796875
Epoch 1590, val loss: 1.0847864151000977
Epoch 1600, training loss: 453.8851013183594 = 1.0826677083969116 + 50.0 * 9.056048393249512
Epoch 1600, val loss: 1.0846490859985352
Epoch 1610, training loss: 453.18377685546875 = 1.0825027227401733 + 50.0 * 9.042025566101074
Epoch 1610, val loss: 1.0845030546188354
Epoch 1620, training loss: 453.1933288574219 = 1.0823341608047485 + 50.0 * 9.042220115661621
Epoch 1620, val loss: 1.084362506866455
Epoch 1630, training loss: 453.4320983886719 = 1.0821778774261475 + 50.0 * 9.046998023986816
Epoch 1630, val loss: 1.0842339992523193
Epoch 1640, training loss: 453.4656982421875 = 1.0820144414901733 + 50.0 * 9.047674179077148
Epoch 1640, val loss: 1.08408784866333
Epoch 1650, training loss: 453.8664855957031 = 1.0818549394607544 + 50.0 * 9.055692672729492
Epoch 1650, val loss: 1.083959937095642
Epoch 1660, training loss: 454.26153564453125 = 1.0817075967788696 + 50.0 * 9.063596725463867
Epoch 1660, val loss: 1.0838351249694824
Epoch 1670, training loss: 453.96783447265625 = 1.081540822982788 + 50.0 * 9.05772590637207
Epoch 1670, val loss: 1.0836941003799438
Epoch 1680, training loss: 454.16461181640625 = 1.0813860893249512 + 50.0 * 9.061664581298828
Epoch 1680, val loss: 1.0835620164871216
Epoch 1690, training loss: 454.45343017578125 = 1.0812309980392456 + 50.0 * 9.06744384765625
Epoch 1690, val loss: 1.0834327936172485
Epoch 1700, training loss: 454.62799072265625 = 1.0810739994049072 + 50.0 * 9.070938110351562
Epoch 1700, val loss: 1.0832974910736084
Epoch 1710, training loss: 454.6335144042969 = 1.0809123516082764 + 50.0 * 9.071052551269531
Epoch 1710, val loss: 1.0831634998321533
Epoch 1720, training loss: 454.8204345703125 = 1.0807535648345947 + 50.0 * 9.074793815612793
Epoch 1720, val loss: 1.0830296277999878
Epoch 1730, training loss: 454.7732238769531 = 1.0805917978286743 + 50.0 * 9.0738525390625
Epoch 1730, val loss: 1.082891583442688
Epoch 1740, training loss: 454.97296142578125 = 1.080431580543518 + 50.0 * 9.077850341796875
Epoch 1740, val loss: 1.0827584266662598
Epoch 1750, training loss: 455.3043212890625 = 1.0802756547927856 + 50.0 * 9.084481239318848
Epoch 1750, val loss: 1.082625150680542
Epoch 1760, training loss: 454.9272155761719 = 1.0801067352294922 + 50.0 * 9.076942443847656
Epoch 1760, val loss: 1.0824803113937378
Epoch 1770, training loss: 455.057861328125 = 1.0799448490142822 + 50.0 * 9.079558372497559
Epoch 1770, val loss: 1.0823463201522827
Epoch 1780, training loss: 455.3623962402344 = 1.0797868967056274 + 50.0 * 9.085652351379395
Epoch 1780, val loss: 1.0822125673294067
Epoch 1790, training loss: 455.4714660644531 = 1.0796276330947876 + 50.0 * 9.087837219238281
Epoch 1790, val loss: 1.0820766687393188
Epoch 1800, training loss: 455.4751892089844 = 1.0794706344604492 + 50.0 * 9.08791446685791
Epoch 1800, val loss: 1.0819365978240967
Epoch 1810, training loss: 455.52978515625 = 1.0793036222457886 + 50.0 * 9.089009284973145
Epoch 1810, val loss: 1.081799864768982
Epoch 1820, training loss: 455.7297668457031 = 1.0791410207748413 + 50.0 * 9.093012809753418
Epoch 1820, val loss: 1.0816621780395508
Epoch 1830, training loss: 455.690673828125 = 1.078979730606079 + 50.0 * 9.092233657836914
Epoch 1830, val loss: 1.0815246105194092
Epoch 1840, training loss: 455.7400207519531 = 1.0788182020187378 + 50.0 * 9.093223571777344
Epoch 1840, val loss: 1.0813900232315063
Epoch 1850, training loss: 455.33905029296875 = 1.0786434412002563 + 50.0 * 9.08520793914795
Epoch 1850, val loss: 1.081244707107544
Epoch 1860, training loss: 455.54229736328125 = 1.0784869194030762 + 50.0 * 9.089276313781738
Epoch 1860, val loss: 1.081107258796692
Epoch 1870, training loss: 455.73876953125 = 1.078316569328308 + 50.0 * 9.093209266662598
Epoch 1870, val loss: 1.0809510946273804
Epoch 1880, training loss: 455.9099426269531 = 1.0781972408294678 + 50.0 * 9.096634864807129
Epoch 1880, val loss: 1.0808427333831787
Epoch 1890, training loss: 455.3498229980469 = 1.0779465436935425 + 50.0 * 9.085437774658203
Epoch 1890, val loss: 1.0806399583816528
Epoch 1900, training loss: 455.74951171875 = 1.0777966976165771 + 50.0 * 9.09343433380127
Epoch 1900, val loss: 1.080519676208496
Epoch 1910, training loss: 455.9491882324219 = 1.0776652097702026 + 50.0 * 9.097430229187012
Epoch 1910, val loss: 1.0804051160812378
Epoch 1920, training loss: 456.3584289550781 = 1.0775147676467896 + 50.0 * 9.105618476867676
Epoch 1920, val loss: 1.0802812576293945
Epoch 1930, training loss: 456.57440185546875 = 1.0773650407791138 + 50.0 * 9.109940528869629
Epoch 1930, val loss: 1.08015775680542
Epoch 1940, training loss: 456.5610656738281 = 1.077214002609253 + 50.0 * 9.1096773147583
Epoch 1940, val loss: 1.0800251960754395
Epoch 1950, training loss: 456.68609619140625 = 1.0770525932312012 + 50.0 * 9.112180709838867
Epoch 1950, val loss: 1.0798929929733276
Epoch 1960, training loss: 456.8287658691406 = 1.0768965482711792 + 50.0 * 9.115036964416504
Epoch 1960, val loss: 1.0797606706619263
Epoch 1970, training loss: 456.358154296875 = 1.076721429824829 + 50.0 * 9.105628967285156
Epoch 1970, val loss: 1.079610824584961
Epoch 1980, training loss: 456.03411865234375 = 1.0765529870986938 + 50.0 * 9.099151611328125
Epoch 1980, val loss: 1.0794758796691895
Epoch 1990, training loss: 456.4089660644531 = 1.0764005184173584 + 50.0 * 9.106651306152344
Epoch 1990, val loss: 1.0793477296829224
Epoch 2000, training loss: 456.2940673828125 = 1.0762465000152588 + 50.0 * 9.10435676574707
Epoch 2000, val loss: 1.0792124271392822
Epoch 2010, training loss: 456.5696105957031 = 1.0760899782180786 + 50.0 * 9.109870910644531
Epoch 2010, val loss: 1.0790907144546509
Epoch 2020, training loss: 456.9170837402344 = 1.0759375095367432 + 50.0 * 9.116823196411133
Epoch 2020, val loss: 1.0789599418640137
Epoch 2030, training loss: 457.2614440917969 = 1.075785517692566 + 50.0 * 9.123713493347168
Epoch 2030, val loss: 1.078831672668457
Epoch 2040, training loss: 457.27044677734375 = 1.0756279230117798 + 50.0 * 9.123896598815918
Epoch 2040, val loss: 1.0787001848220825
Epoch 2050, training loss: 457.3798828125 = 1.0754677057266235 + 50.0 * 9.12608814239502
Epoch 2050, val loss: 1.0785623788833618
Epoch 2060, training loss: 457.5444641113281 = 1.0753083229064941 + 50.0 * 9.129383087158203
Epoch 2060, val loss: 1.0784292221069336
Epoch 2070, training loss: 457.7811279296875 = 1.0751495361328125 + 50.0 * 9.134119987487793
Epoch 2070, val loss: 1.0782963037490845
Epoch 2080, training loss: 457.60394287109375 = 1.0749812126159668 + 50.0 * 9.130578994750977
Epoch 2080, val loss: 1.0781511068344116
Epoch 2090, training loss: 457.5401611328125 = 1.0748164653778076 + 50.0 * 9.12930679321289
Epoch 2090, val loss: 1.0780158042907715
Epoch 2100, training loss: 457.7181701660156 = 1.0746561288833618 + 50.0 * 9.1328706741333
Epoch 2100, val loss: 1.0778861045837402
Epoch 2110, training loss: 458.0207824707031 = 1.074499249458313 + 50.0 * 9.138925552368164
Epoch 2110, val loss: 1.0777533054351807
Epoch 2120, training loss: 458.3053283691406 = 1.0743443965911865 + 50.0 * 9.144619941711426
Epoch 2120, val loss: 1.0776243209838867
Epoch 2130, training loss: 458.3779602050781 = 1.0741819143295288 + 50.0 * 9.146075248718262
Epoch 2130, val loss: 1.077490210533142
Epoch 2140, training loss: 458.2842712402344 = 1.074014663696289 + 50.0 * 9.144205093383789
Epoch 2140, val loss: 1.077347993850708
Epoch 2150, training loss: 458.0524597167969 = 1.0738497972488403 + 50.0 * 9.139572143554688
Epoch 2150, val loss: 1.0772252082824707
Epoch 2160, training loss: 456.5424499511719 = 1.0736280679702759 + 50.0 * 9.109375953674316
Epoch 2160, val loss: 1.077012300491333
Epoch 2170, training loss: 456.45361328125 = 1.0734500885009766 + 50.0 * 9.107603073120117
Epoch 2170, val loss: 1.0768576860427856
Epoch 2180, training loss: 456.5499572753906 = 1.0733226537704468 + 50.0 * 9.109532356262207
Epoch 2180, val loss: 1.0767571926116943
Epoch 2190, training loss: 457.2508544921875 = 1.0731887817382812 + 50.0 * 9.123553276062012
Epoch 2190, val loss: 1.0766485929489136
Epoch 2200, training loss: 457.1560363769531 = 1.0730513334274292 + 50.0 * 9.121659278869629
Epoch 2200, val loss: 1.076539397239685
Epoch 2210, training loss: 457.9116516113281 = 1.0729093551635742 + 50.0 * 9.136775016784668
Epoch 2210, val loss: 1.076421856880188
Epoch 2220, training loss: 458.2413024902344 = 1.072769284248352 + 50.0 * 9.143370628356934
Epoch 2220, val loss: 1.0763062238693237
Epoch 2230, training loss: 458.4328918457031 = 1.0726220607757568 + 50.0 * 9.147205352783203
Epoch 2230, val loss: 1.0761845111846924
Epoch 2240, training loss: 458.58349609375 = 1.072463035583496 + 50.0 * 9.15022087097168
Epoch 2240, val loss: 1.076055645942688
Epoch 2250, training loss: 458.7783508300781 = 1.0723117589950562 + 50.0 * 9.154120445251465
Epoch 2250, val loss: 1.075926423072815
Epoch 2260, training loss: 458.90594482421875 = 1.0721553564071655 + 50.0 * 9.156676292419434
Epoch 2260, val loss: 1.075796365737915
Epoch 2270, training loss: 458.83026123046875 = 1.0719953775405884 + 50.0 * 9.155165672302246
Epoch 2270, val loss: 1.0756628513336182
Epoch 2280, training loss: 458.9666748046875 = 1.071834921836853 + 50.0 * 9.157896995544434
Epoch 2280, val loss: 1.0755289793014526
Epoch 2290, training loss: 459.0616760253906 = 1.0716779232025146 + 50.0 * 9.159799575805664
Epoch 2290, val loss: 1.0753947496414185
Epoch 2300, training loss: 459.2978820800781 = 1.0715200901031494 + 50.0 * 9.16452693939209
Epoch 2300, val loss: 1.075262427330017
Epoch 2310, training loss: 459.334228515625 = 1.0713621377944946 + 50.0 * 9.165257453918457
Epoch 2310, val loss: 1.0751277208328247
Epoch 2320, training loss: 459.0655517578125 = 1.0711942911148071 + 50.0 * 9.159887313842773
Epoch 2320, val loss: 1.0749833583831787
Epoch 2330, training loss: 459.2848205566406 = 1.0710268020629883 + 50.0 * 9.164276123046875
Epoch 2330, val loss: 1.0748493671417236
Epoch 2340, training loss: 459.3603515625 = 1.0708681344985962 + 50.0 * 9.165789604187012
Epoch 2340, val loss: 1.0747196674346924
Epoch 2350, training loss: 459.53662109375 = 1.0707118511199951 + 50.0 * 9.169318199157715
Epoch 2350, val loss: 1.0745900869369507
Epoch 2360, training loss: 459.7059326171875 = 1.070548176765442 + 50.0 * 9.172707557678223
Epoch 2360, val loss: 1.0744552612304688
Epoch 2370, training loss: 459.6141052246094 = 1.0703847408294678 + 50.0 * 9.17087459564209
Epoch 2370, val loss: 1.0743119716644287
Epoch 2380, training loss: 459.6831970214844 = 1.0702224969863892 + 50.0 * 9.172259330749512
Epoch 2380, val loss: 1.0741740465164185
Epoch 2390, training loss: 459.91839599609375 = 1.070063829421997 + 50.0 * 9.176966667175293
Epoch 2390, val loss: 1.0740418434143066
Epoch 2400, training loss: 459.802001953125 = 1.0699009895324707 + 50.0 * 9.174641609191895
Epoch 2400, val loss: 1.0738953351974487
Epoch 2410, training loss: 459.7897033691406 = 1.0697333812713623 + 50.0 * 9.174399375915527
Epoch 2410, val loss: 1.0737591981887817
Epoch 2420, training loss: 460.0087585449219 = 1.0695769786834717 + 50.0 * 9.178783416748047
Epoch 2420, val loss: 1.0736262798309326
Epoch 2430, training loss: 460.1357727050781 = 1.0694153308868408 + 50.0 * 9.181326866149902
Epoch 2430, val loss: 1.0734859704971313
Epoch 2440, training loss: 460.1874084472656 = 1.0692548751831055 + 50.0 * 9.182363510131836
Epoch 2440, val loss: 1.0733489990234375
Epoch 2450, training loss: 460.12451171875 = 1.0690819025039673 + 50.0 * 9.181108474731445
Epoch 2450, val loss: 1.0732020139694214
Epoch 2460, training loss: 460.40936279296875 = 1.0689254999160767 + 50.0 * 9.186808586120605
Epoch 2460, val loss: 1.0730692148208618
Epoch 2470, training loss: 460.62371826171875 = 1.0687676668167114 + 50.0 * 9.191099166870117
Epoch 2470, val loss: 1.0729353427886963
Epoch 2480, training loss: 460.35992431640625 = 1.0685991048812866 + 50.0 * 9.185826301574707
Epoch 2480, val loss: 1.0727876424789429
Epoch 2490, training loss: 460.5306701660156 = 1.068434476852417 + 50.0 * 9.189245223999023
Epoch 2490, val loss: 1.0726555585861206
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.20594202898550723
0.8130841121495328
=== training gcn model ===
Epoch 0, training loss: 512.2110595703125 = 1.1086007356643677 + 50.0 * 10.22204875946045
Epoch 0, val loss: 1.1085704565048218
Epoch 10, training loss: 491.9987487792969 = 1.1081777811050415 + 50.0 * 9.817811012268066
Epoch 10, val loss: 1.1081547737121582
Epoch 20, training loss: 483.80828857421875 = 1.107877492904663 + 50.0 * 9.654007911682129
Epoch 20, val loss: 1.1078522205352783
Epoch 30, training loss: 477.6235656738281 = 1.107535719871521 + 50.0 * 9.530320167541504
Epoch 30, val loss: 1.1075081825256348
Epoch 40, training loss: 472.6419982910156 = 1.1072094440460205 + 50.0 * 9.430695533752441
Epoch 40, val loss: 1.1071836948394775
Epoch 50, training loss: 468.4002685546875 = 1.1068726778030396 + 50.0 * 9.345868110656738
Epoch 50, val loss: 1.1068484783172607
Epoch 60, training loss: 464.80963134765625 = 1.1065336465835571 + 50.0 * 9.274062156677246
Epoch 60, val loss: 1.1065094470977783
Epoch 70, training loss: 461.70965576171875 = 1.1061846017837524 + 50.0 * 9.212069511413574
Epoch 70, val loss: 1.106162428855896
Epoch 80, training loss: 458.9768371582031 = 1.1058317422866821 + 50.0 * 9.15742015838623
Epoch 80, val loss: 1.1058093309402466
Epoch 90, training loss: 456.5771789550781 = 1.1054712533950806 + 50.0 * 9.109434127807617
Epoch 90, val loss: 1.105453610420227
Epoch 100, training loss: 454.56732177734375 = 1.1050957441329956 + 50.0 * 9.069244384765625
Epoch 100, val loss: 1.1050795316696167
Epoch 110, training loss: 452.7931823730469 = 1.1047072410583496 + 50.0 * 9.033769607543945
Epoch 110, val loss: 1.104696273803711
Epoch 120, training loss: 451.47027587890625 = 1.104304313659668 + 50.0 * 9.007319450378418
Epoch 120, val loss: 1.1042916774749756
Epoch 130, training loss: 450.0239562988281 = 1.1039140224456787 + 50.0 * 8.978401184082031
Epoch 130, val loss: 1.103910207748413
Epoch 140, training loss: 448.7300109863281 = 1.103516697883606 + 50.0 * 8.952529907226562
Epoch 140, val loss: 1.1035115718841553
Epoch 150, training loss: 447.6674499511719 = 1.10310697555542 + 50.0 * 8.931286811828613
Epoch 150, val loss: 1.1031084060668945
Epoch 160, training loss: 446.7207336425781 = 1.102695107460022 + 50.0 * 8.912361145019531
Epoch 160, val loss: 1.102698564529419
Epoch 170, training loss: 445.83221435546875 = 1.1022732257843018 + 50.0 * 8.894598960876465
Epoch 170, val loss: 1.1022770404815674
Epoch 180, training loss: 445.0599060058594 = 1.1018550395965576 + 50.0 * 8.87916088104248
Epoch 180, val loss: 1.101868987083435
Epoch 190, training loss: 444.5161437988281 = 1.1014153957366943 + 50.0 * 8.868294715881348
Epoch 190, val loss: 1.1014302968978882
Epoch 200, training loss: 443.89825439453125 = 1.1009951829910278 + 50.0 * 8.855945587158203
Epoch 200, val loss: 1.1010171175003052
Epoch 210, training loss: 443.3612365722656 = 1.1005580425262451 + 50.0 * 8.845213890075684
Epoch 210, val loss: 1.1005791425704956
Epoch 220, training loss: 442.8909912109375 = 1.1001185178756714 + 50.0 * 8.835817337036133
Epoch 220, val loss: 1.1001427173614502
Epoch 230, training loss: 442.4624938964844 = 1.0996637344360352 + 50.0 * 8.827256202697754
Epoch 230, val loss: 1.0997015237808228
Epoch 240, training loss: 442.3616943359375 = 1.0992377996444702 + 50.0 * 8.825248718261719
Epoch 240, val loss: 1.099273443222046
Epoch 250, training loss: 441.71929931640625 = 1.0987703800201416 + 50.0 * 8.812410354614258
Epoch 250, val loss: 1.098838210105896
Epoch 260, training loss: 441.4576110839844 = 1.0983905792236328 + 50.0 * 8.807184219360352
Epoch 260, val loss: 1.0984569787979126
Epoch 270, training loss: 441.1842346191406 = 1.098031997680664 + 50.0 * 8.801724433898926
Epoch 270, val loss: 1.0980998277664185
Epoch 280, training loss: 441.06103515625 = 1.0977402925491333 + 50.0 * 8.79926586151123
Epoch 280, val loss: 1.0978049039840698
Epoch 290, training loss: 440.9394836425781 = 1.0975124835968018 + 50.0 * 8.796839714050293
Epoch 290, val loss: 1.0975816249847412
Epoch 300, training loss: 440.7151794433594 = 1.0973365306854248 + 50.0 * 8.792356491088867
Epoch 300, val loss: 1.0974090099334717
Epoch 310, training loss: 440.56695556640625 = 1.0971994400024414 + 50.0 * 8.789395332336426
Epoch 310, val loss: 1.0972720384597778
Epoch 320, training loss: 440.48480224609375 = 1.097091794013977 + 50.0 * 8.78775405883789
Epoch 320, val loss: 1.0971673727035522
Epoch 330, training loss: 440.3282775878906 = 1.0969899892807007 + 50.0 * 8.784626007080078
Epoch 330, val loss: 1.0970622301101685
Epoch 340, training loss: 440.3296203613281 = 1.0968953371047974 + 50.0 * 8.78465461730957
Epoch 340, val loss: 1.096967101097107
Epoch 350, training loss: 440.24298095703125 = 1.0968023538589478 + 50.0 * 8.782923698425293
Epoch 350, val loss: 1.0968761444091797
Epoch 360, training loss: 440.17236328125 = 1.0967075824737549 + 50.0 * 8.781513214111328
Epoch 360, val loss: 1.096784234046936
Epoch 370, training loss: 440.10113525390625 = 1.096609354019165 + 50.0 * 8.78009033203125
Epoch 370, val loss: 1.0966840982437134
Epoch 380, training loss: 440.24725341796875 = 1.096505045890808 + 50.0 * 8.783015251159668
Epoch 380, val loss: 1.096584677696228
Epoch 390, training loss: 440.2277526855469 = 1.0964109897613525 + 50.0 * 8.78262710571289
Epoch 390, val loss: 1.0964916944503784
Epoch 400, training loss: 440.21942138671875 = 1.0963150262832642 + 50.0 * 8.782462120056152
Epoch 400, val loss: 1.0963966846466064
Epoch 410, training loss: 440.1564636230469 = 1.0962046384811401 + 50.0 * 8.781205177307129
Epoch 410, val loss: 1.0962905883789062
Epoch 420, training loss: 440.1579895019531 = 1.096093773841858 + 50.0 * 8.781237602233887
Epoch 420, val loss: 1.0961834192276
Epoch 430, training loss: 440.385986328125 = 1.0959831476211548 + 50.0 * 8.785799980163574
Epoch 430, val loss: 1.0960698127746582
Epoch 440, training loss: 440.27923583984375 = 1.0958658456802368 + 50.0 * 8.78366756439209
Epoch 440, val loss: 1.0959577560424805
Epoch 450, training loss: 440.3291015625 = 1.0957496166229248 + 50.0 * 8.784667015075684
Epoch 450, val loss: 1.0958486795425415
Epoch 460, training loss: 440.3931884765625 = 1.0956335067749023 + 50.0 * 8.785950660705566
Epoch 460, val loss: 1.09573233127594
Epoch 470, training loss: 440.5106506347656 = 1.0955067873001099 + 50.0 * 8.788302421569824
Epoch 470, val loss: 1.095606803894043
Epoch 480, training loss: 440.43243408203125 = 1.0953819751739502 + 50.0 * 8.786741256713867
Epoch 480, val loss: 1.095487356185913
Epoch 490, training loss: 440.7134094238281 = 1.0952544212341309 + 50.0 * 8.792363166809082
Epoch 490, val loss: 1.0953596830368042
Epoch 500, training loss: 440.6006164550781 = 1.0951192378997803 + 50.0 * 8.790109634399414
Epoch 500, val loss: 1.0952301025390625
Epoch 510, training loss: 440.6694030761719 = 1.0949835777282715 + 50.0 * 8.791488647460938
Epoch 510, val loss: 1.0950976610183716
Epoch 520, training loss: 440.70208740234375 = 1.0948511362075806 + 50.0 * 8.792144775390625
Epoch 520, val loss: 1.094961404800415
Epoch 530, training loss: 440.8328552246094 = 1.0947158336639404 + 50.0 * 8.79476261138916
Epoch 530, val loss: 1.0948323011398315
Epoch 540, training loss: 440.8076171875 = 1.0945796966552734 + 50.0 * 8.79426097869873
Epoch 540, val loss: 1.0946967601776123
Epoch 550, training loss: 440.8761291503906 = 1.0944383144378662 + 50.0 * 8.795633316040039
Epoch 550, val loss: 1.0945618152618408
Epoch 560, training loss: 441.01873779296875 = 1.0943034887313843 + 50.0 * 8.79848861694336
Epoch 560, val loss: 1.0944279432296753
Epoch 570, training loss: 441.00335693359375 = 1.094165563583374 + 50.0 * 8.79818344116211
Epoch 570, val loss: 1.0942949056625366
Epoch 580, training loss: 441.0543212890625 = 1.09402596950531 + 50.0 * 8.799205780029297
Epoch 580, val loss: 1.0941569805145264
Epoch 590, training loss: 441.06439208984375 = 1.0938873291015625 + 50.0 * 8.799409866333008
Epoch 590, val loss: 1.094024896621704
Epoch 600, training loss: 441.0402526855469 = 1.0937517881393433 + 50.0 * 8.798930168151855
Epoch 600, val loss: 1.0938867330551147
Epoch 610, training loss: 441.3334045410156 = 1.0936180353164673 + 50.0 * 8.804795265197754
Epoch 610, val loss: 1.0937654972076416
Epoch 620, training loss: 441.2531433105469 = 1.093467354774475 + 50.0 * 8.803193092346191
Epoch 620, val loss: 1.0936200618743896
Epoch 630, training loss: 441.29913330078125 = 1.0933297872543335 + 50.0 * 8.804116249084473
Epoch 630, val loss: 1.0934795141220093
Epoch 640, training loss: 441.3465881347656 = 1.0931885242462158 + 50.0 * 8.805068016052246
Epoch 640, val loss: 1.0933376550674438
Epoch 650, training loss: 441.4362487792969 = 1.0930436849594116 + 50.0 * 8.806863784790039
Epoch 650, val loss: 1.0932058095932007
Epoch 660, training loss: 441.52301025390625 = 1.0929003953933716 + 50.0 * 8.808602333068848
Epoch 660, val loss: 1.0930640697479248
Epoch 670, training loss: 441.6506042480469 = 1.092750906944275 + 50.0 * 8.8111572265625
Epoch 670, val loss: 1.0929198265075684
Epoch 680, training loss: 441.6174011230469 = 1.092594861984253 + 50.0 * 8.81049633026123
Epoch 680, val loss: 1.0927720069885254
Epoch 690, training loss: 441.782470703125 = 1.0924485921859741 + 50.0 * 8.813800811767578
Epoch 690, val loss: 1.0926265716552734
Epoch 700, training loss: 441.79241943359375 = 1.092293381690979 + 50.0 * 8.814002990722656
Epoch 700, val loss: 1.0924806594848633
Epoch 710, training loss: 442.0111999511719 = 1.0921411514282227 + 50.0 * 8.818381309509277
Epoch 710, val loss: 1.0923264026641846
Epoch 720, training loss: 442.2294921875 = 1.0919837951660156 + 50.0 * 8.822750091552734
Epoch 720, val loss: 1.0921741724014282
Epoch 730, training loss: 442.24072265625 = 1.0918172597885132 + 50.0 * 8.822978019714355
Epoch 730, val loss: 1.0920130014419556
Epoch 740, training loss: 442.29302978515625 = 1.091665267944336 + 50.0 * 8.824027061462402
Epoch 740, val loss: 1.091866374015808
Epoch 750, training loss: 442.521240234375 = 1.0915106534957886 + 50.0 * 8.828594207763672
Epoch 750, val loss: 1.0917134284973145
Epoch 760, training loss: 442.5922546386719 = 1.0913431644439697 + 50.0 * 8.830018043518066
Epoch 760, val loss: 1.0915613174438477
Epoch 770, training loss: 442.3332214355469 = 1.09116792678833 + 50.0 * 8.824841499328613
Epoch 770, val loss: 1.0913904905319214
Epoch 780, training loss: 442.3914489746094 = 1.0910160541534424 + 50.0 * 8.826008796691895
Epoch 780, val loss: 1.091238021850586
Epoch 790, training loss: 442.4203796386719 = 1.0908514261245728 + 50.0 * 8.826590538024902
Epoch 790, val loss: 1.0910826921463013
Epoch 800, training loss: 442.5620422363281 = 1.090694785118103 + 50.0 * 8.829426765441895
Epoch 800, val loss: 1.0909264087677002
Epoch 810, training loss: 442.8623962402344 = 1.0905354022979736 + 50.0 * 8.835436820983887
Epoch 810, val loss: 1.090773582458496
Epoch 820, training loss: 442.9554748535156 = 1.0903728008270264 + 50.0 * 8.837302207946777
Epoch 820, val loss: 1.0906121730804443
Epoch 830, training loss: 443.0321044921875 = 1.0902025699615479 + 50.0 * 8.838837623596191
Epoch 830, val loss: 1.0904512405395508
Epoch 840, training loss: 443.1861572265625 = 1.0900325775146484 + 50.0 * 8.841922760009766
Epoch 840, val loss: 1.0902806520462036
Epoch 850, training loss: 443.24560546875 = 1.0898596048355103 + 50.0 * 8.843114852905273
Epoch 850, val loss: 1.0901145935058594
Epoch 860, training loss: 443.22735595703125 = 1.0896847248077393 + 50.0 * 8.842753410339355
Epoch 860, val loss: 1.089946985244751
Epoch 870, training loss: 443.43499755859375 = 1.0895096063613892 + 50.0 * 8.846909523010254
Epoch 870, val loss: 1.0897740125656128
Epoch 880, training loss: 443.4516296386719 = 1.089332938194275 + 50.0 * 8.847246170043945
Epoch 880, val loss: 1.0896084308624268
Epoch 890, training loss: 443.66680908203125 = 1.0891661643981934 + 50.0 * 8.851552963256836
Epoch 890, val loss: 1.0894477367401123
Epoch 900, training loss: 443.5651550292969 = 1.0889933109283447 + 50.0 * 8.849523544311523
Epoch 900, val loss: 1.0892727375030518
Epoch 910, training loss: 443.8213806152344 = 1.0888060331344604 + 50.0 * 8.85465145111084
Epoch 910, val loss: 1.089095115661621
Epoch 920, training loss: 443.8973388671875 = 1.0886279344558716 + 50.0 * 8.85617446899414
Epoch 920, val loss: 1.0889241695404053
Epoch 930, training loss: 444.1175231933594 = 1.0884490013122559 + 50.0 * 8.860581398010254
Epoch 930, val loss: 1.088750958442688
Epoch 940, training loss: 443.6856384277344 = 1.088263988494873 + 50.0 * 8.851947784423828
Epoch 940, val loss: 1.0885626077651978
Epoch 950, training loss: 444.0398864746094 = 1.0880835056304932 + 50.0 * 8.859036445617676
Epoch 950, val loss: 1.088387131690979
Epoch 960, training loss: 444.1973571777344 = 1.0878998041152954 + 50.0 * 8.862189292907715
Epoch 960, val loss: 1.088218331336975
Epoch 970, training loss: 444.394775390625 = 1.0877193212509155 + 50.0 * 8.866141319274902
Epoch 970, val loss: 1.0880430936813354
Epoch 980, training loss: 444.4894714355469 = 1.0875235795974731 + 50.0 * 8.86803913116455
Epoch 980, val loss: 1.0878499746322632
Epoch 990, training loss: 444.486572265625 = 1.0873435735702515 + 50.0 * 8.867984771728516
Epoch 990, val loss: 1.0876718759536743
Epoch 1000, training loss: 444.6809997558594 = 1.0871504545211792 + 50.0 * 8.87187671661377
Epoch 1000, val loss: 1.0874830484390259
Epoch 1010, training loss: 444.6676330566406 = 1.0869593620300293 + 50.0 * 8.871613502502441
Epoch 1010, val loss: 1.087295651435852
Epoch 1020, training loss: 444.5627136230469 = 1.0867515802383423 + 50.0 * 8.869519233703613
Epoch 1020, val loss: 1.0871061086654663
Epoch 1030, training loss: 444.7149963378906 = 1.0865541696548462 + 50.0 * 8.87256908416748
Epoch 1030, val loss: 1.0869052410125732
Epoch 1040, training loss: 444.8227233886719 = 1.0863572359085083 + 50.0 * 8.874727249145508
Epoch 1040, val loss: 1.0867139101028442
Epoch 1050, training loss: 444.9333801269531 = 1.0861501693725586 + 50.0 * 8.876944541931152
Epoch 1050, val loss: 1.08651864528656
Epoch 1060, training loss: 444.7427062988281 = 1.085949182510376 + 50.0 * 8.873135566711426
Epoch 1060, val loss: 1.0863161087036133
Epoch 1070, training loss: 444.8427734375 = 1.0857264995574951 + 50.0 * 8.875141143798828
Epoch 1070, val loss: 1.0861047506332397
Epoch 1080, training loss: 445.00604248046875 = 1.0855212211608887 + 50.0 * 8.878410339355469
Epoch 1080, val loss: 1.0858999490737915
Epoch 1090, training loss: 445.162109375 = 1.085303544998169 + 50.0 * 8.881536483764648
Epoch 1090, val loss: 1.0856947898864746
Epoch 1100, training loss: 445.4345703125 = 1.0850938558578491 + 50.0 * 8.88698959350586
Epoch 1100, val loss: 1.0854898691177368
Epoch 1110, training loss: 445.3000183105469 = 1.0848710536956787 + 50.0 * 8.884303092956543
Epoch 1110, val loss: 1.0852687358856201
Epoch 1120, training loss: 445.5860595703125 = 1.084648847579956 + 50.0 * 8.89002799987793
Epoch 1120, val loss: 1.0850542783737183
Epoch 1130, training loss: 445.5384521484375 = 1.0844192504882812 + 50.0 * 8.889081001281738
Epoch 1130, val loss: 1.0848296880722046
Epoch 1140, training loss: 445.56939697265625 = 1.0841917991638184 + 50.0 * 8.889703750610352
Epoch 1140, val loss: 1.0846104621887207
Epoch 1150, training loss: 445.3485412597656 = 1.0839468240737915 + 50.0 * 8.885292053222656
Epoch 1150, val loss: 1.084370493888855
Epoch 1160, training loss: 445.34320068359375 = 1.083720088005066 + 50.0 * 8.8851900100708
Epoch 1160, val loss: 1.0841587781906128
Epoch 1170, training loss: 445.2398986816406 = 1.0834815502166748 + 50.0 * 8.88312816619873
Epoch 1170, val loss: 1.0839245319366455
Epoch 1180, training loss: 445.620361328125 = 1.0832608938217163 + 50.0 * 8.890742301940918
Epoch 1180, val loss: 1.0837172269821167
Epoch 1190, training loss: 445.7213439941406 = 1.0830296277999878 + 50.0 * 8.892765998840332
Epoch 1190, val loss: 1.0834816694259644
Epoch 1200, training loss: 445.8407897949219 = 1.0828012228012085 + 50.0 * 8.895159721374512
Epoch 1200, val loss: 1.0832600593566895
Epoch 1210, training loss: 446.2362976074219 = 1.0825709104537964 + 50.0 * 8.903074264526367
Epoch 1210, val loss: 1.083038568496704
Epoch 1220, training loss: 446.2134094238281 = 1.0823274850845337 + 50.0 * 8.902621269226074
Epoch 1220, val loss: 1.0828068256378174
Epoch 1230, training loss: 446.382568359375 = 1.0820828676223755 + 50.0 * 8.906009674072266
Epoch 1230, val loss: 1.082567811012268
Epoch 1240, training loss: 446.3661804199219 = 1.0818370580673218 + 50.0 * 8.905686378479004
Epoch 1240, val loss: 1.0823230743408203
Epoch 1250, training loss: 446.5141906738281 = 1.0815848112106323 + 50.0 * 8.908652305603027
Epoch 1250, val loss: 1.0820870399475098
Epoch 1260, training loss: 446.5169372558594 = 1.08133065700531 + 50.0 * 8.908712387084961
Epoch 1260, val loss: 1.0818389654159546
Epoch 1270, training loss: 446.70281982421875 = 1.081075668334961 + 50.0 * 8.912434577941895
Epoch 1270, val loss: 1.0815950632095337
Epoch 1280, training loss: 446.63946533203125 = 1.080817461013794 + 50.0 * 8.911172866821289
Epoch 1280, val loss: 1.0813463926315308
Epoch 1290, training loss: 446.4834289550781 = 1.080536961555481 + 50.0 * 8.908058166503906
Epoch 1290, val loss: 1.0810863971710205
Epoch 1300, training loss: 446.5302734375 = 1.0802565813064575 + 50.0 * 8.909000396728516
Epoch 1300, val loss: 1.0808179378509521
Epoch 1310, training loss: 446.38671875 = 1.079997181892395 + 50.0 * 8.906134605407715
Epoch 1310, val loss: 1.080558180809021
Epoch 1320, training loss: 446.6316223144531 = 1.0797277688980103 + 50.0 * 8.91103744506836
Epoch 1320, val loss: 1.0803004503250122
Epoch 1330, training loss: 447.0169982910156 = 1.0794601440429688 + 50.0 * 8.918750762939453
Epoch 1330, val loss: 1.080039381980896
Epoch 1340, training loss: 446.855712890625 = 1.0791807174682617 + 50.0 * 8.91553020477295
Epoch 1340, val loss: 1.0797700881958008
Epoch 1350, training loss: 446.9319152832031 = 1.0788986682891846 + 50.0 * 8.917060852050781
Epoch 1350, val loss: 1.079498052597046
Epoch 1360, training loss: 447.1584777832031 = 1.0786166191101074 + 50.0 * 8.921597480773926
Epoch 1360, val loss: 1.0792330503463745
Epoch 1370, training loss: 447.3335876464844 = 1.0783309936523438 + 50.0 * 8.925105094909668
Epoch 1370, val loss: 1.078954815864563
Epoch 1380, training loss: 447.3802490234375 = 1.0780385732650757 + 50.0 * 8.926044464111328
Epoch 1380, val loss: 1.0786720514297485
Epoch 1390, training loss: 447.41802978515625 = 1.0777435302734375 + 50.0 * 8.92680549621582
Epoch 1390, val loss: 1.0783921480178833
Epoch 1400, training loss: 447.4039611816406 = 1.0774414539337158 + 50.0 * 8.926529884338379
Epoch 1400, val loss: 1.0781000852584839
Epoch 1410, training loss: 447.1018371582031 = 1.0771267414093018 + 50.0 * 8.920494079589844
Epoch 1410, val loss: 1.0777912139892578
Epoch 1420, training loss: 447.1888122558594 = 1.076819658279419 + 50.0 * 8.922240257263184
Epoch 1420, val loss: 1.0774989128112793
Epoch 1430, training loss: 447.1785888671875 = 1.0765233039855957 + 50.0 * 8.922040939331055
Epoch 1430, val loss: 1.0772149562835693
Epoch 1440, training loss: 447.6174011230469 = 1.0762444734573364 + 50.0 * 8.93082332611084
Epoch 1440, val loss: 1.0769386291503906
Epoch 1450, training loss: 447.8160705566406 = 1.0759544372558594 + 50.0 * 8.934802055358887
Epoch 1450, val loss: 1.0766615867614746
Epoch 1460, training loss: 447.9772033691406 = 1.075659990310669 + 50.0 * 8.938031196594238
Epoch 1460, val loss: 1.0763731002807617
Epoch 1470, training loss: 447.9078674316406 = 1.0753384828567505 + 50.0 * 8.936650276184082
Epoch 1470, val loss: 1.076071858406067
Epoch 1480, training loss: 447.9877014160156 = 1.0750399827957153 + 50.0 * 8.938253402709961
Epoch 1480, val loss: 1.0757858753204346
Epoch 1490, training loss: 448.171142578125 = 1.0747249126434326 + 50.0 * 8.941927909851074
Epoch 1490, val loss: 1.075474500656128
Epoch 1500, training loss: 448.06842041015625 = 1.0743985176086426 + 50.0 * 8.93988037109375
Epoch 1500, val loss: 1.0751703977584839
Epoch 1510, training loss: 448.1930236816406 = 1.074075698852539 + 50.0 * 8.942378997802734
Epoch 1510, val loss: 1.074859857559204
Epoch 1520, training loss: 448.28729248046875 = 1.0737422704696655 + 50.0 * 8.944271087646484
Epoch 1520, val loss: 1.074537992477417
Epoch 1530, training loss: 447.93890380859375 = 1.0733922719955444 + 50.0 * 8.937310218811035
Epoch 1530, val loss: 1.0741939544677734
Epoch 1540, training loss: 448.1689758300781 = 1.0730516910552979 + 50.0 * 8.94191837310791
Epoch 1540, val loss: 1.073872447013855
Epoch 1550, training loss: 449.6971435546875 = 1.0726312398910522 + 50.0 * 8.972490310668945
Epoch 1550, val loss: 1.0734130144119263
Epoch 1560, training loss: 446.1176452636719 = 1.0721269845962524 + 50.0 * 8.900910377502441
Epoch 1560, val loss: 1.0729751586914062
Epoch 1570, training loss: 447.3006896972656 = 1.0718350410461426 + 50.0 * 8.924576759338379
Epoch 1570, val loss: 1.072662353515625
Epoch 1580, training loss: 447.61065673828125 = 1.0714954137802124 + 50.0 * 8.93078327178955
Epoch 1580, val loss: 1.0723495483398438
Epoch 1590, training loss: 447.6556701660156 = 1.0711559057235718 + 50.0 * 8.931690216064453
Epoch 1590, val loss: 1.0720372200012207
Epoch 1600, training loss: 448.3665466308594 = 1.0708245038986206 + 50.0 * 8.945914268493652
Epoch 1600, val loss: 1.071714997291565
Epoch 1610, training loss: 448.8097229003906 = 1.07046377658844 + 50.0 * 8.954785346984863
Epoch 1610, val loss: 1.0713701248168945
Epoch 1620, training loss: 449.0578308105469 = 1.0701090097427368 + 50.0 * 8.959754943847656
Epoch 1620, val loss: 1.071022629737854
Epoch 1630, training loss: 449.104736328125 = 1.0697376728057861 + 50.0 * 8.960700035095215
Epoch 1630, val loss: 1.0706721544265747
Epoch 1640, training loss: 449.4966125488281 = 1.0693705081939697 + 50.0 * 8.968544960021973
Epoch 1640, val loss: 1.070322036743164
Epoch 1650, training loss: 449.498291015625 = 1.0689983367919922 + 50.0 * 8.968585968017578
Epoch 1650, val loss: 1.0699694156646729
Epoch 1660, training loss: 449.837158203125 = 1.06862473487854 + 50.0 * 8.975370407104492
Epoch 1660, val loss: 1.0696097612380981
Epoch 1670, training loss: 450.0252380371094 = 1.0682454109191895 + 50.0 * 8.979140281677246
Epoch 1670, val loss: 1.069251298904419
Epoch 1680, training loss: 449.9015197753906 = 1.0678657293319702 + 50.0 * 8.976673126220703
Epoch 1680, val loss: 1.0688846111297607
Epoch 1690, training loss: 450.1294250488281 = 1.067493200302124 + 50.0 * 8.98123836517334
Epoch 1690, val loss: 1.0685248374938965
Epoch 1700, training loss: 450.3573913574219 = 1.0671159029006958 + 50.0 * 8.98580551147461
Epoch 1700, val loss: 1.0681605339050293
Epoch 1710, training loss: 450.4853210449219 = 1.0667340755462646 + 50.0 * 8.988371849060059
Epoch 1710, val loss: 1.0677940845489502
Epoch 1720, training loss: 450.25042724609375 = 1.0663341283798218 + 50.0 * 8.983681678771973
Epoch 1720, val loss: 1.0674190521240234
Epoch 1730, training loss: 450.4087829589844 = 1.0659470558166504 + 50.0 * 8.986856460571289
Epoch 1730, val loss: 1.0670455694198608
Epoch 1740, training loss: 450.7947998046875 = 1.0655707120895386 + 50.0 * 8.994584083557129
Epoch 1740, val loss: 1.0666828155517578
Epoch 1750, training loss: 450.4634094238281 = 1.0651705265045166 + 50.0 * 8.987964630126953
Epoch 1750, val loss: 1.0662957429885864
Epoch 1760, training loss: 450.7682189941406 = 1.0647730827331543 + 50.0 * 8.99406909942627
Epoch 1760, val loss: 1.065928339958191
Epoch 1770, training loss: 450.82806396484375 = 1.0643807649612427 + 50.0 * 8.99527359008789
Epoch 1770, val loss: 1.0655466318130493
Epoch 1780, training loss: 451.01043701171875 = 1.0639804601669312 + 50.0 * 8.998929023742676
Epoch 1780, val loss: 1.065169095993042
Epoch 1790, training loss: 451.18109130859375 = 1.0635844469070435 + 50.0 * 9.002349853515625
Epoch 1790, val loss: 1.0647902488708496
Epoch 1800, training loss: 451.0679016113281 = 1.0631765127182007 + 50.0 * 9.000094413757324
Epoch 1800, val loss: 1.0644004344940186
Epoch 1810, training loss: 450.50054931640625 = 1.0627645254135132 + 50.0 * 8.988755226135254
Epoch 1810, val loss: 1.0640052556991577
Epoch 1820, training loss: 450.4226379394531 = 1.062353253364563 + 50.0 * 8.987205505371094
Epoch 1820, val loss: 1.0636141300201416
Epoch 1830, training loss: 450.7752380371094 = 1.0619487762451172 + 50.0 * 8.99426555633545
Epoch 1830, val loss: 1.063220500946045
Epoch 1840, training loss: 451.1289978027344 = 1.0615369081497192 + 50.0 * 9.001349449157715
Epoch 1840, val loss: 1.0628317594528198
Epoch 1850, training loss: 451.3175964355469 = 1.0611454248428345 + 50.0 * 9.005128860473633
Epoch 1850, val loss: 1.0624618530273438
Epoch 1860, training loss: 451.574462890625 = 1.060745358467102 + 50.0 * 9.010273933410645
Epoch 1860, val loss: 1.062077522277832
Epoch 1870, training loss: 451.0504150390625 = 1.0603272914886475 + 50.0 * 8.999801635742188
Epoch 1870, val loss: 1.0616741180419922
Epoch 1880, training loss: 451.23046875 = 1.0599119663238525 + 50.0 * 9.003411293029785
Epoch 1880, val loss: 1.0612812042236328
Epoch 1890, training loss: 451.2048034667969 = 1.059515357017517 + 50.0 * 9.00290584564209
Epoch 1890, val loss: 1.060909628868103
Epoch 1900, training loss: 451.7239074707031 = 1.0591166019439697 + 50.0 * 9.013296127319336
Epoch 1900, val loss: 1.0605337619781494
Epoch 1910, training loss: 452.0213623046875 = 1.058718204498291 + 50.0 * 9.01925277709961
Epoch 1910, val loss: 1.0601530075073242
Epoch 1920, training loss: 451.93048095703125 = 1.0582963228225708 + 50.0 * 9.017443656921387
Epoch 1920, val loss: 1.0597540140151978
Epoch 1930, training loss: 452.2281799316406 = 1.0578840970993042 + 50.0 * 9.023406028747559
Epoch 1930, val loss: 1.0593630075454712
Epoch 1940, training loss: 452.34906005859375 = 1.0574647188186646 + 50.0 * 9.025832176208496
Epoch 1940, val loss: 1.0589678287506104
Epoch 1950, training loss: 452.4404296875 = 1.0570482015609741 + 50.0 * 9.027667999267578
Epoch 1950, val loss: 1.0585688352584839
Epoch 1960, training loss: 452.4649353027344 = 1.0566291809082031 + 50.0 * 9.028165817260742
Epoch 1960, val loss: 1.0581783056259155
Epoch 1970, training loss: 452.6174621582031 = 1.056215763092041 + 50.0 * 9.031225204467773
Epoch 1970, val loss: 1.0577915906906128
Epoch 1980, training loss: 452.71002197265625 = 1.0558052062988281 + 50.0 * 9.03308391571045
Epoch 1980, val loss: 1.0573986768722534
Epoch 1990, training loss: 452.7970886230469 = 1.0553914308547974 + 50.0 * 9.034833908081055
Epoch 1990, val loss: 1.0570143461227417
Epoch 2000, training loss: 452.83319091796875 = 1.0549674034118652 + 50.0 * 9.035564422607422
Epoch 2000, val loss: 1.0566149950027466
Epoch 2010, training loss: 452.7197570800781 = 1.054549217224121 + 50.0 * 9.033304214477539
Epoch 2010, val loss: 1.0562227964401245
Epoch 2020, training loss: 452.5428161621094 = 1.0541237592697144 + 50.0 * 9.029773712158203
Epoch 2020, val loss: 1.05581533908844
Epoch 2030, training loss: 453.0218200683594 = 1.0537164211273193 + 50.0 * 9.039361953735352
Epoch 2030, val loss: 1.0554327964782715
Epoch 2040, training loss: 453.264892578125 = 1.053301215171814 + 50.0 * 9.044231414794922
Epoch 2040, val loss: 1.0550400018692017
Epoch 2050, training loss: 453.2010803222656 = 1.0528755187988281 + 50.0 * 9.042963981628418
Epoch 2050, val loss: 1.0546391010284424
Epoch 2060, training loss: 453.1230773925781 = 1.0524541139602661 + 50.0 * 9.041412353515625
Epoch 2060, val loss: 1.054245114326477
Epoch 2070, training loss: 453.42242431640625 = 1.0520399808883667 + 50.0 * 9.047408103942871
Epoch 2070, val loss: 1.053857684135437
Epoch 2080, training loss: 453.3519287109375 = 1.0516185760498047 + 50.0 * 9.046006202697754
Epoch 2080, val loss: 1.0534571409225464
Epoch 2090, training loss: 453.4855651855469 = 1.0511950254440308 + 50.0 * 9.048686981201172
Epoch 2090, val loss: 1.053065299987793
Epoch 2100, training loss: 453.68756103515625 = 1.0507749319076538 + 50.0 * 9.052735328674316
Epoch 2100, val loss: 1.0526742935180664
Epoch 2110, training loss: 453.73150634765625 = 1.0503604412078857 + 50.0 * 9.05362319946289
Epoch 2110, val loss: 1.0522891283035278
Epoch 2120, training loss: 453.9638977050781 = 1.0499504804611206 + 50.0 * 9.058279037475586
Epoch 2120, val loss: 1.0518938302993774
Epoch 2130, training loss: 453.581298828125 = 1.0495177507400513 + 50.0 * 9.05063533782959
Epoch 2130, val loss: 1.0514754056930542
Epoch 2140, training loss: 453.2530517578125 = 1.049085259437561 + 50.0 * 9.044079780578613
Epoch 2140, val loss: 1.0510809421539307
Epoch 2150, training loss: 453.48138427734375 = 1.0486576557159424 + 50.0 * 9.048654556274414
Epoch 2150, val loss: 1.0506854057312012
Epoch 2160, training loss: 453.8218688964844 = 1.0482606887817383 + 50.0 * 9.055472373962402
Epoch 2160, val loss: 1.0503222942352295
Epoch 2170, training loss: 454.1471252441406 = 1.0478699207305908 + 50.0 * 9.06198501586914
Epoch 2170, val loss: 1.0499587059020996
Epoch 2180, training loss: 453.8125915527344 = 1.0474634170532227 + 50.0 * 9.055302619934082
Epoch 2180, val loss: 1.0495773553848267
Epoch 2190, training loss: 454.0165710449219 = 1.0470631122589111 + 50.0 * 9.0593900680542
Epoch 2190, val loss: 1.0492088794708252
Epoch 2200, training loss: 454.3104553222656 = 1.046668291091919 + 50.0 * 9.065276145935059
Epoch 2200, val loss: 1.0488353967666626
Epoch 2210, training loss: 454.40509033203125 = 1.0462548732757568 + 50.0 * 9.067176818847656
Epoch 2210, val loss: 1.048449158668518
Epoch 2220, training loss: 454.2869567871094 = 1.0458455085754395 + 50.0 * 9.06482219696045
Epoch 2220, val loss: 1.048068881034851
Epoch 2230, training loss: 454.1651916503906 = 1.045434594154358 + 50.0 * 9.062395095825195
Epoch 2230, val loss: 1.0476876497268677
Epoch 2240, training loss: 454.18341064453125 = 1.0450212955474854 + 50.0 * 9.06276798248291
Epoch 2240, val loss: 1.0473049879074097
Epoch 2250, training loss: 454.2366943359375 = 1.04462468624115 + 50.0 * 9.063841819763184
Epoch 2250, val loss: 1.0469324588775635
Epoch 2260, training loss: 454.4902648925781 = 1.044235110282898 + 50.0 * 9.068921089172363
Epoch 2260, val loss: 1.0465757846832275
Epoch 2270, training loss: 454.5288391113281 = 1.0438380241394043 + 50.0 * 9.069700241088867
Epoch 2270, val loss: 1.0462151765823364
Epoch 2280, training loss: 454.7878112792969 = 1.0434480905532837 + 50.0 * 9.0748872756958
Epoch 2280, val loss: 1.045851469039917
Epoch 2290, training loss: 454.9292297363281 = 1.043055534362793 + 50.0 * 9.077723503112793
Epoch 2290, val loss: 1.04548978805542
Epoch 2300, training loss: 454.7785949707031 = 1.042657732963562 + 50.0 * 9.074718475341797
Epoch 2300, val loss: 1.0451276302337646
Epoch 2310, training loss: 454.866943359375 = 1.042268991470337 + 50.0 * 9.076493263244629
Epoch 2310, val loss: 1.044771671295166
Epoch 2320, training loss: 454.7644958496094 = 1.0418832302093506 + 50.0 * 9.07445240020752
Epoch 2320, val loss: 1.044416069984436
Epoch 2330, training loss: 454.89019775390625 = 1.0414987802505493 + 50.0 * 9.076973915100098
Epoch 2330, val loss: 1.0440672636032104
Epoch 2340, training loss: 455.1124267578125 = 1.0411137342453003 + 50.0 * 9.081426620483398
Epoch 2340, val loss: 1.0437145233154297
Epoch 2350, training loss: 455.2652893066406 = 1.0407320261001587 + 50.0 * 9.084490776062012
Epoch 2350, val loss: 1.0433647632598877
Epoch 2360, training loss: 455.4019775390625 = 1.0403505563735962 + 50.0 * 9.08723258972168
Epoch 2360, val loss: 1.0430151224136353
Epoch 2370, training loss: 455.214599609375 = 1.0399667024612427 + 50.0 * 9.083492279052734
Epoch 2370, val loss: 1.04266357421875
Epoch 2380, training loss: 454.27545166015625 = 1.0395599603652954 + 50.0 * 9.064718246459961
Epoch 2380, val loss: 1.042292594909668
Epoch 2390, training loss: 454.32440185546875 = 1.0391960144042969 + 50.0 * 9.065704345703125
Epoch 2390, val loss: 1.0419471263885498
Epoch 2400, training loss: 453.8810729980469 = 1.038835048675537 + 50.0 * 9.056844711303711
Epoch 2400, val loss: 1.041628122329712
Epoch 2410, training loss: 454.2184753417969 = 1.0384695529937744 + 50.0 * 9.063599586486816
Epoch 2410, val loss: 1.0413000583648682
Epoch 2420, training loss: 454.58563232421875 = 1.0381065607070923 + 50.0 * 9.070950508117676
Epoch 2420, val loss: 1.0409764051437378
Epoch 2430, training loss: 454.918212890625 = 1.0377522706985474 + 50.0 * 9.077609062194824
Epoch 2430, val loss: 1.0406664609909058
Epoch 2440, training loss: 455.27923583984375 = 1.037398338317871 + 50.0 * 9.084836959838867
Epoch 2440, val loss: 1.0403553247451782
Epoch 2450, training loss: 455.58856201171875 = 1.0370464324951172 + 50.0 * 9.09103012084961
Epoch 2450, val loss: 1.0400407314300537
Epoch 2460, training loss: 455.3898620605469 = 1.036678671836853 + 50.0 * 9.087063789367676
Epoch 2460, val loss: 1.0397186279296875
Epoch 2470, training loss: 455.4654235839844 = 1.036322832107544 + 50.0 * 9.088582038879395
Epoch 2470, val loss: 1.039401888847351
Epoch 2480, training loss: 455.7578125 = 1.0359724760055542 + 50.0 * 9.094436645507812
Epoch 2480, val loss: 1.0390956401824951
Epoch 2490, training loss: 455.9053955078125 = 1.0356218814849854 + 50.0 * 9.097395896911621
Epoch 2490, val loss: 1.038784384727478
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.20594202898550723
0.8146055205390134
The final CL Acc:0.35812, 0.21521, The final GNN Acc:0.81369, 0.00066
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110910])
remove edge: torch.Size([2, 66310])
updated graph: torch.Size([2, 88572])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 508.98193359375 = 1.0937620401382446 + 50.0 * 10.157763481140137
Epoch 0, val loss: 1.093354344367981
Epoch 10, training loss: 487.1232604980469 = 1.093283772468567 + 50.0 * 9.720599174499512
Epoch 10, val loss: 1.0928608179092407
Epoch 20, training loss: 477.235595703125 = 1.092719554901123 + 50.0 * 9.522857666015625
Epoch 20, val loss: 1.0922775268554688
Epoch 30, training loss: 470.2447509765625 = 1.0920963287353516 + 50.0 * 9.383052825927734
Epoch 30, val loss: 1.0916619300842285
Epoch 40, training loss: 464.7101745605469 = 1.0915167331695557 + 50.0 * 9.27237319946289
Epoch 40, val loss: 1.091081142425537
Epoch 50, training loss: 460.1710205078125 = 1.0909409523010254 + 50.0 * 9.181601524353027
Epoch 50, val loss: 1.090503454208374
Epoch 60, training loss: 456.47802734375 = 1.0903466939926147 + 50.0 * 9.10775375366211
Epoch 60, val loss: 1.089905858039856
Epoch 70, training loss: 453.4820556640625 = 1.089735984802246 + 50.0 * 9.047846794128418
Epoch 70, val loss: 1.089292049407959
Epoch 80, training loss: 450.9001159667969 = 1.089128851890564 + 50.0 * 8.996219635009766
Epoch 80, val loss: 1.0886893272399902
Epoch 90, training loss: 448.7669677734375 = 1.0885285139083862 + 50.0 * 8.953568458557129
Epoch 90, val loss: 1.088087558746338
Epoch 100, training loss: 446.8702697753906 = 1.087902545928955 + 50.0 * 8.915647506713867
Epoch 100, val loss: 1.0874652862548828
Epoch 110, training loss: 445.33929443359375 = 1.087280511856079 + 50.0 * 8.885040283203125
Epoch 110, val loss: 1.0868467092514038
Epoch 120, training loss: 444.117431640625 = 1.0866320133209229 + 50.0 * 8.860615730285645
Epoch 120, val loss: 1.0861966609954834
Epoch 130, training loss: 442.992431640625 = 1.0860148668289185 + 50.0 * 8.838128089904785
Epoch 130, val loss: 1.0855892896652222
Epoch 140, training loss: 441.958251953125 = 1.0853676795959473 + 50.0 * 8.817458152770996
Epoch 140, val loss: 1.0849543809890747
Epoch 150, training loss: 441.10296630859375 = 1.0847352743148804 + 50.0 * 8.80036449432373
Epoch 150, val loss: 1.0843194723129272
Epoch 160, training loss: 440.3407897949219 = 1.08408784866333 + 50.0 * 8.785134315490723
Epoch 160, val loss: 1.0836818218231201
Epoch 170, training loss: 439.687255859375 = 1.0834540128707886 + 50.0 * 8.772075653076172
Epoch 170, val loss: 1.0830566883087158
Epoch 180, training loss: 439.0792236328125 = 1.0828204154968262 + 50.0 * 8.759927749633789
Epoch 180, val loss: 1.0824310779571533
Epoch 190, training loss: 438.62213134765625 = 1.0821852684020996 + 50.0 * 8.750799179077148
Epoch 190, val loss: 1.0818119049072266
Epoch 200, training loss: 438.2303466796875 = 1.0816361904144287 + 50.0 * 8.742974281311035
Epoch 200, val loss: 1.0812715291976929
Epoch 210, training loss: 437.7397766113281 = 1.0811183452606201 + 50.0 * 8.733173370361328
Epoch 210, val loss: 1.0807623863220215
Epoch 220, training loss: 437.3822326660156 = 1.0806586742401123 + 50.0 * 8.726031303405762
Epoch 220, val loss: 1.0803046226501465
Epoch 230, training loss: 437.1640319824219 = 1.0802419185638428 + 50.0 * 8.721675872802734
Epoch 230, val loss: 1.07989501953125
Epoch 240, training loss: 436.8136901855469 = 1.079824686050415 + 50.0 * 8.714676856994629
Epoch 240, val loss: 1.0794909000396729
Epoch 250, training loss: 436.6661682128906 = 1.0794243812561035 + 50.0 * 8.711734771728516
Epoch 250, val loss: 1.0790934562683105
Epoch 260, training loss: 436.6584167480469 = 1.0790013074874878 + 50.0 * 8.711587905883789
Epoch 260, val loss: 1.078678011894226
Epoch 270, training loss: 436.48626708984375 = 1.0785751342773438 + 50.0 * 8.70815372467041
Epoch 270, val loss: 1.0782723426818848
Epoch 280, training loss: 436.01031494140625 = 1.0781364440917969 + 50.0 * 8.698643684387207
Epoch 280, val loss: 1.0778393745422363
Epoch 290, training loss: 435.8795166015625 = 1.0777007341384888 + 50.0 * 8.696036338806152
Epoch 290, val loss: 1.07741117477417
Epoch 300, training loss: 435.8868713378906 = 1.077253818511963 + 50.0 * 8.696192741394043
Epoch 300, val loss: 1.0769809484481812
Epoch 310, training loss: 435.924560546875 = 1.076796293258667 + 50.0 * 8.696955680847168
Epoch 310, val loss: 1.076521396636963
Epoch 320, training loss: 435.6912536621094 = 1.0763095617294312 + 50.0 * 8.692298889160156
Epoch 320, val loss: 1.076062798500061
Epoch 330, training loss: 435.5461120605469 = 1.0758484601974487 + 50.0 * 8.68940544128418
Epoch 330, val loss: 1.0756096839904785
Epoch 340, training loss: 435.6156921386719 = 1.0753809213638306 + 50.0 * 8.69080638885498
Epoch 340, val loss: 1.075147271156311
Epoch 350, training loss: 435.62890625 = 1.074892520904541 + 50.0 * 8.691080093383789
Epoch 350, val loss: 1.074676513671875
Epoch 360, training loss: 435.70751953125 = 1.0744030475616455 + 50.0 * 8.692662239074707
Epoch 360, val loss: 1.0741996765136719
Epoch 370, training loss: 435.5777282714844 = 1.0738801956176758 + 50.0 * 8.69007682800293
Epoch 370, val loss: 1.073683500289917
Epoch 380, training loss: 435.68646240234375 = 1.0733637809753418 + 50.0 * 8.692261695861816
Epoch 380, val loss: 1.0731914043426514
Epoch 390, training loss: 435.6379089355469 = 1.0728451013565063 + 50.0 * 8.691301345825195
Epoch 390, val loss: 1.0726752281188965
Epoch 400, training loss: 435.3568420410156 = 1.0722922086715698 + 50.0 * 8.685690879821777
Epoch 400, val loss: 1.0721529722213745
Epoch 410, training loss: 435.53216552734375 = 1.0717779397964478 + 50.0 * 8.689208030700684
Epoch 410, val loss: 1.0716420412063599
Epoch 420, training loss: 435.525634765625 = 1.0712316036224365 + 50.0 * 8.689087867736816
Epoch 420, val loss: 1.071107268333435
Epoch 430, training loss: 435.5571594238281 = 1.0706827640533447 + 50.0 * 8.689729690551758
Epoch 430, val loss: 1.0705679655075073
Epoch 440, training loss: 435.6512756347656 = 1.0701240301132202 + 50.0 * 8.691622734069824
Epoch 440, val loss: 1.0700277090072632
Epoch 450, training loss: 435.8375244140625 = 1.069541335105896 + 50.0 * 8.695359230041504
Epoch 450, val loss: 1.0694462060928345
Epoch 460, training loss: 435.6134033203125 = 1.068949580192566 + 50.0 * 8.690889358520508
Epoch 460, val loss: 1.0688889026641846
Epoch 470, training loss: 435.7223205566406 = 1.0683590173721313 + 50.0 * 8.693078994750977
Epoch 470, val loss: 1.0683048963546753
Epoch 480, training loss: 435.7140197753906 = 1.0677624940872192 + 50.0 * 8.692925453186035
Epoch 480, val loss: 1.0677289962768555
Epoch 490, training loss: 435.7960510253906 = 1.0671430826187134 + 50.0 * 8.694578170776367
Epoch 490, val loss: 1.067131757736206
Epoch 500, training loss: 435.8305358886719 = 1.0665254592895508 + 50.0 * 8.695280075073242
Epoch 500, val loss: 1.0665159225463867
Epoch 510, training loss: 435.7903747558594 = 1.0658866167068481 + 50.0 * 8.694489479064941
Epoch 510, val loss: 1.0659016370773315
Epoch 520, training loss: 435.8229675292969 = 1.065239429473877 + 50.0 * 8.695154190063477
Epoch 520, val loss: 1.0652694702148438
Epoch 530, training loss: 435.889892578125 = 1.0645673274993896 + 50.0 * 8.69650650024414
Epoch 530, val loss: 1.0646077394485474
Epoch 540, training loss: 435.9650573730469 = 1.063908338546753 + 50.0 * 8.698022842407227
Epoch 540, val loss: 1.063977837562561
Epoch 550, training loss: 436.2389831542969 = 1.0632472038269043 + 50.0 * 8.70351505279541
Epoch 550, val loss: 1.0633107423782349
Epoch 560, training loss: 436.3019104003906 = 1.0625585317611694 + 50.0 * 8.704787254333496
Epoch 560, val loss: 1.062645435333252
Epoch 570, training loss: 436.3292236328125 = 1.0618324279785156 + 50.0 * 8.705348014831543
Epoch 570, val loss: 1.06192946434021
Epoch 580, training loss: 436.4709167480469 = 1.0611337423324585 + 50.0 * 8.708195686340332
Epoch 580, val loss: 1.0612506866455078
Epoch 590, training loss: 436.551513671875 = 1.060401439666748 + 50.0 * 8.709822654724121
Epoch 590, val loss: 1.060532808303833
Epoch 600, training loss: 436.5698547363281 = 1.0596550703048706 + 50.0 * 8.710204124450684
Epoch 600, val loss: 1.0598105192184448
Epoch 610, training loss: 436.461181640625 = 1.0589102506637573 + 50.0 * 8.708045959472656
Epoch 610, val loss: 1.0590606927871704
Epoch 620, training loss: 436.9363708496094 = 1.0581088066101074 + 50.0 * 8.717565536499023
Epoch 620, val loss: 1.0583127737045288
Epoch 630, training loss: 436.0968933105469 = 1.057325005531311 + 50.0 * 8.700791358947754
Epoch 630, val loss: 1.0575075149536133
Epoch 640, training loss: 436.98089599609375 = 1.0566012859344482 + 50.0 * 8.718485832214355
Epoch 640, val loss: 1.0568115711212158
Epoch 650, training loss: 436.6835021972656 = 1.0558174848556519 + 50.0 * 8.712553977966309
Epoch 650, val loss: 1.0560681819915771
Epoch 660, training loss: 436.9573059082031 = 1.0550556182861328 + 50.0 * 8.718045234680176
Epoch 660, val loss: 1.055313229560852
Epoch 670, training loss: 436.9777526855469 = 1.054255723953247 + 50.0 * 8.718469619750977
Epoch 670, val loss: 1.0545293092727661
Epoch 680, training loss: 437.2017822265625 = 1.05344820022583 + 50.0 * 8.722967147827148
Epoch 680, val loss: 1.0537389516830444
Epoch 690, training loss: 437.27081298828125 = 1.0525479316711426 + 50.0 * 8.724365234375
Epoch 690, val loss: 1.0528790950775146
Epoch 700, training loss: 437.000732421875 = 1.0517293214797974 + 50.0 * 8.718979835510254
Epoch 700, val loss: 1.0520715713500977
Epoch 710, training loss: 437.3578796386719 = 1.050937533378601 + 50.0 * 8.726139068603516
Epoch 710, val loss: 1.0513012409210205
Epoch 720, training loss: 437.5147705078125 = 1.0501564741134644 + 50.0 * 8.729291915893555
Epoch 720, val loss: 1.050553560256958
Epoch 730, training loss: 437.28033447265625 = 1.0492035150527954 + 50.0 * 8.72462272644043
Epoch 730, val loss: 1.0496095418930054
Epoch 740, training loss: 437.81842041015625 = 1.0484017133712769 + 50.0 * 8.735400199890137
Epoch 740, val loss: 1.0488271713256836
Epoch 750, training loss: 438.0251159667969 = 1.047546625137329 + 50.0 * 8.739551544189453
Epoch 750, val loss: 1.047982096672058
Epoch 760, training loss: 438.0787353515625 = 1.0466644763946533 + 50.0 * 8.740641593933105
Epoch 760, val loss: 1.0471304655075073
Epoch 770, training loss: 438.19854736328125 = 1.0457746982574463 + 50.0 * 8.74305534362793
Epoch 770, val loss: 1.0462899208068848
Epoch 780, training loss: 438.3520202636719 = 1.04487943649292 + 50.0 * 8.746142387390137
Epoch 780, val loss: 1.0454093217849731
Epoch 790, training loss: 438.5263366699219 = 1.0439766645431519 + 50.0 * 8.74964714050293
Epoch 790, val loss: 1.044538140296936
Epoch 800, training loss: 438.66778564453125 = 1.0430339574813843 + 50.0 * 8.752494812011719
Epoch 800, val loss: 1.043614149093628
Epoch 810, training loss: 438.7846984863281 = 1.0421408414840698 + 50.0 * 8.754851341247559
Epoch 810, val loss: 1.042722463607788
Epoch 820, training loss: 438.7664794921875 = 1.0411686897277832 + 50.0 * 8.75450611114502
Epoch 820, val loss: 1.0417956113815308
Epoch 830, training loss: 438.7894287109375 = 1.0403584241867065 + 50.0 * 8.754981994628906
Epoch 830, val loss: 1.041027545928955
Epoch 840, training loss: 439.1113586425781 = 1.0394152402877808 + 50.0 * 8.761438369750977
Epoch 840, val loss: 1.0400619506835938
Epoch 850, training loss: 439.3772277832031 = 1.0384961366653442 + 50.0 * 8.766775131225586
Epoch 850, val loss: 1.0392078161239624
Epoch 860, training loss: 439.3733215332031 = 1.0375174283981323 + 50.0 * 8.766716003417969
Epoch 860, val loss: 1.0382792949676514
Epoch 870, training loss: 439.63531494140625 = 1.0366053581237793 + 50.0 * 8.771974563598633
Epoch 870, val loss: 1.037388801574707
Epoch 880, training loss: 439.7543029785156 = 1.0356491804122925 + 50.0 * 8.774373054504395
Epoch 880, val loss: 1.0364571809768677
Epoch 890, training loss: 440.1151428222656 = 1.0346773862838745 + 50.0 * 8.781609535217285
Epoch 890, val loss: 1.0355093479156494
Epoch 900, training loss: 440.0548095703125 = 1.0336629152297974 + 50.0 * 8.780423164367676
Epoch 900, val loss: 1.03451406955719
Epoch 910, training loss: 440.23248291015625 = 1.0326390266418457 + 50.0 * 8.78399658203125
Epoch 910, val loss: 1.0335302352905273
Epoch 920, training loss: 440.3778381347656 = 1.0315934419631958 + 50.0 * 8.786925315856934
Epoch 920, val loss: 1.032517433166504
Epoch 930, training loss: 440.3920593261719 = 1.0305347442626953 + 50.0 * 8.787230491638184
Epoch 930, val loss: 1.0314983129501343
Epoch 940, training loss: 440.5185852050781 = 1.0295491218566895 + 50.0 * 8.789780616760254
Epoch 940, val loss: 1.0305348634719849
Epoch 950, training loss: 440.5049133300781 = 1.028398036956787 + 50.0 * 8.789529800415039
Epoch 950, val loss: 1.029430866241455
Epoch 960, training loss: 440.8752136230469 = 1.02732515335083 + 50.0 * 8.796957969665527
Epoch 960, val loss: 1.028369665145874
Epoch 970, training loss: 440.9140625 = 1.0262175798416138 + 50.0 * 8.797757148742676
Epoch 970, val loss: 1.0273075103759766
Epoch 980, training loss: 440.89263916015625 = 1.0250439643859863 + 50.0 * 8.797351837158203
Epoch 980, val loss: 1.0261788368225098
Epoch 990, training loss: 441.22491455078125 = 1.0240134000778198 + 50.0 * 8.804018020629883
Epoch 990, val loss: 1.025161623954773
Epoch 1000, training loss: 441.314697265625 = 1.0229500532150269 + 50.0 * 8.805834770202637
Epoch 1000, val loss: 1.0241541862487793
Epoch 1010, training loss: 441.5034484863281 = 1.0217286348342896 + 50.0 * 8.8096342086792
Epoch 1010, val loss: 1.0229251384735107
Epoch 1020, training loss: 441.5282897949219 = 1.0204988718032837 + 50.0 * 8.810155868530273
Epoch 1020, val loss: 1.0217596292495728
Epoch 1030, training loss: 441.696044921875 = 1.01934814453125 + 50.0 * 8.813533782958984
Epoch 1030, val loss: 1.0206483602523804
Epoch 1040, training loss: 441.7425231933594 = 1.018165946006775 + 50.0 * 8.81448745727539
Epoch 1040, val loss: 1.0194846391677856
Epoch 1050, training loss: 442.0915222167969 = 1.017002820968628 + 50.0 * 8.821490287780762
Epoch 1050, val loss: 1.0183535814285278
Epoch 1060, training loss: 442.2919616699219 = 1.0157885551452637 + 50.0 * 8.825523376464844
Epoch 1060, val loss: 1.0171846151351929
Epoch 1070, training loss: 441.0366516113281 = 1.0142329931259155 + 50.0 * 8.800448417663574
Epoch 1070, val loss: 1.0155833959579468
Epoch 1080, training loss: 441.6089172363281 = 1.0129671096801758 + 50.0 * 8.811919212341309
Epoch 1080, val loss: 1.0144615173339844
Epoch 1090, training loss: 441.0030212402344 = 1.0118341445922852 + 50.0 * 8.799823760986328
Epoch 1090, val loss: 1.013344168663025
Epoch 1100, training loss: 441.2007751464844 = 1.0105923414230347 + 50.0 * 8.803803443908691
Epoch 1100, val loss: 1.012147068977356
Epoch 1110, training loss: 441.83282470703125 = 1.0093681812286377 + 50.0 * 8.816469192504883
Epoch 1110, val loss: 1.0109492540359497
Epoch 1120, training loss: 442.11199951171875 = 1.0081181526184082 + 50.0 * 8.822077751159668
Epoch 1120, val loss: 1.0097510814666748
Epoch 1130, training loss: 442.45703125 = 1.0068567991256714 + 50.0 * 8.82900333404541
Epoch 1130, val loss: 1.0085179805755615
Epoch 1140, training loss: 442.7751159667969 = 1.00556480884552 + 50.0 * 8.8353910446167
Epoch 1140, val loss: 1.0072582960128784
Epoch 1150, training loss: 442.8923645019531 = 1.0042201280593872 + 50.0 * 8.837762832641602
Epoch 1150, val loss: 1.0059601068496704
Epoch 1160, training loss: 443.01904296875 = 1.0028820037841797 + 50.0 * 8.840323448181152
Epoch 1160, val loss: 1.0046502351760864
Epoch 1170, training loss: 443.2614440917969 = 1.001532793045044 + 50.0 * 8.845198631286621
Epoch 1170, val loss: 1.0033525228500366
Epoch 1180, training loss: 442.85528564453125 = 1.000014066696167 + 50.0 * 8.837105751037598
Epoch 1180, val loss: 1.0019011497497559
Epoch 1190, training loss: 443.15252685546875 = 0.9986928701400757 + 50.0 * 8.843076705932617
Epoch 1190, val loss: 1.0005815029144287
Epoch 1200, training loss: 443.5097351074219 = 0.9973219037055969 + 50.0 * 8.850248336791992
Epoch 1200, val loss: 0.9992704391479492
Epoch 1210, training loss: 443.552978515625 = 0.9958802461624146 + 50.0 * 8.851141929626465
Epoch 1210, val loss: 0.9978711605072021
Epoch 1220, training loss: 443.70111083984375 = 0.994486391544342 + 50.0 * 8.854132652282715
Epoch 1220, val loss: 0.9964990019798279
Epoch 1230, training loss: 443.8001708984375 = 0.9930267333984375 + 50.0 * 8.8561429977417
Epoch 1230, val loss: 0.9951130151748657
Epoch 1240, training loss: 443.90191650390625 = 0.9915635585784912 + 50.0 * 8.858206748962402
Epoch 1240, val loss: 0.9936755895614624
Epoch 1250, training loss: 443.67919921875 = 0.9900343418121338 + 50.0 * 8.85378360748291
Epoch 1250, val loss: 0.9922005534172058
Epoch 1260, training loss: 443.4492492675781 = 0.9885347485542297 + 50.0 * 8.849214553833008
Epoch 1260, val loss: 0.9907002449035645
Epoch 1270, training loss: 443.74591064453125 = 0.9870318174362183 + 50.0 * 8.855177879333496
Epoch 1270, val loss: 0.9892849326133728
Epoch 1280, training loss: 444.06390380859375 = 0.9855728149414062 + 50.0 * 8.861566543579102
Epoch 1280, val loss: 0.9878752827644348
Epoch 1290, training loss: 444.32476806640625 = 0.9840760827064514 + 50.0 * 8.866813659667969
Epoch 1290, val loss: 0.9864178895950317
Epoch 1300, training loss: 444.4425354003906 = 0.9825553894042969 + 50.0 * 8.869199752807617
Epoch 1300, val loss: 0.9849262833595276
Epoch 1310, training loss: 444.46661376953125 = 0.9810115694999695 + 50.0 * 8.869711875915527
Epoch 1310, val loss: 0.9834347367286682
Epoch 1320, training loss: 444.5587158203125 = 0.9794449210166931 + 50.0 * 8.871585845947266
Epoch 1320, val loss: 0.981943666934967
Epoch 1330, training loss: 444.5691223144531 = 0.9778726100921631 + 50.0 * 8.871825218200684
Epoch 1330, val loss: 0.9804500937461853
Epoch 1340, training loss: 444.8570556640625 = 0.9763559699058533 + 50.0 * 8.87761402130127
Epoch 1340, val loss: 0.9789425134658813
Epoch 1350, training loss: 445.0037536621094 = 0.9747613072395325 + 50.0 * 8.880579948425293
Epoch 1350, val loss: 0.9774105548858643
Epoch 1360, training loss: 445.38446044921875 = 0.973250687122345 + 50.0 * 8.888224601745605
Epoch 1360, val loss: 0.9759037494659424
Epoch 1370, training loss: 445.1671142578125 = 0.9716094136238098 + 50.0 * 8.883910179138184
Epoch 1370, val loss: 0.9743229746818542
Epoch 1380, training loss: 445.258056640625 = 0.9699885845184326 + 50.0 * 8.885761260986328
Epoch 1380, val loss: 0.9727874994277954
Epoch 1390, training loss: 445.5316467285156 = 0.968360185623169 + 50.0 * 8.891265869140625
Epoch 1390, val loss: 0.9711828231811523
Epoch 1400, training loss: 445.26318359375 = 0.9665412306785583 + 50.0 * 8.885932922363281
Epoch 1400, val loss: 0.969429612159729
Epoch 1410, training loss: 445.477294921875 = 0.9649405479431152 + 50.0 * 8.890247344970703
Epoch 1410, val loss: 0.9678822159767151
Epoch 1420, training loss: 445.7843017578125 = 0.9632911086082458 + 50.0 * 8.8964204788208
Epoch 1420, val loss: 0.9662351012229919
Epoch 1430, training loss: 445.19842529296875 = 0.9615167379379272 + 50.0 * 8.884737968444824
Epoch 1430, val loss: 0.9645460844039917
Epoch 1440, training loss: 445.4881286621094 = 0.9598499536514282 + 50.0 * 8.890565872192383
Epoch 1440, val loss: 0.9629200100898743
Epoch 1450, training loss: 445.78076171875 = 0.9581747651100159 + 50.0 * 8.896451950073242
Epoch 1450, val loss: 0.961298942565918
Epoch 1460, training loss: 445.9871826171875 = 0.9564725160598755 + 50.0 * 8.900613784790039
Epoch 1460, val loss: 0.9596472382545471
Epoch 1470, training loss: 446.23248291015625 = 0.954746663570404 + 50.0 * 8.90555477142334
Epoch 1470, val loss: 0.9579684734344482
Epoch 1480, training loss: 446.0100402832031 = 0.9529271125793457 + 50.0 * 8.901142120361328
Epoch 1480, val loss: 0.9562503695487976
Epoch 1490, training loss: 446.01318359375 = 0.9511784315109253 + 50.0 * 8.901240348815918
Epoch 1490, val loss: 0.9545477628707886
Epoch 1500, training loss: 446.0673828125 = 0.9495036602020264 + 50.0 * 8.902358055114746
Epoch 1500, val loss: 0.95289546251297
Epoch 1510, training loss: 446.3291931152344 = 0.9476960301399231 + 50.0 * 8.90762996673584
Epoch 1510, val loss: 0.9511498212814331
Epoch 1520, training loss: 446.6263427734375 = 0.9459283947944641 + 50.0 * 8.91360855102539
Epoch 1520, val loss: 0.9494383931159973
Epoch 1530, training loss: 446.602783203125 = 0.9441068172454834 + 50.0 * 8.91317367553711
Epoch 1530, val loss: 0.9476714134216309
Epoch 1540, training loss: 446.6510009765625 = 0.9422718286514282 + 50.0 * 8.914175033569336
Epoch 1540, val loss: 0.9459064602851868
Epoch 1550, training loss: 446.77362060546875 = 0.9404374361038208 + 50.0 * 8.916664123535156
Epoch 1550, val loss: 0.9441249370574951
Epoch 1560, training loss: 446.5776062011719 = 0.9386212825775146 + 50.0 * 8.912779808044434
Epoch 1560, val loss: 0.9423912763595581
Epoch 1570, training loss: 446.54144287109375 = 0.9367439150810242 + 50.0 * 8.912094116210938
Epoch 1570, val loss: 0.9405035376548767
Epoch 1580, training loss: 446.7488098144531 = 0.9349061250686646 + 50.0 * 8.916277885437012
Epoch 1580, val loss: 0.9387632608413696
Epoch 1590, training loss: 446.755859375 = 0.9330116510391235 + 50.0 * 8.916457176208496
Epoch 1590, val loss: 0.9369639158248901
Epoch 1600, training loss: 447.0152893066406 = 0.9312232136726379 + 50.0 * 8.92168140411377
Epoch 1600, val loss: 0.9351946711540222
Epoch 1610, training loss: 446.7317810058594 = 0.9292897582054138 + 50.0 * 8.91604995727539
Epoch 1610, val loss: 0.9333885908126831
Epoch 1620, training loss: 446.8876953125 = 0.9273613691329956 + 50.0 * 8.919206619262695
Epoch 1620, val loss: 0.9314759969711304
Epoch 1630, training loss: 447.3670654296875 = 0.9254920482635498 + 50.0 * 8.928831100463867
Epoch 1630, val loss: 0.929688572883606
Epoch 1640, training loss: 447.6014099121094 = 0.9236159324645996 + 50.0 * 8.933555603027344
Epoch 1640, val loss: 0.9278754591941833
Epoch 1650, training loss: 447.38720703125 = 0.9216499924659729 + 50.0 * 8.92931079864502
Epoch 1650, val loss: 0.9260072112083435
Epoch 1660, training loss: 447.47894287109375 = 0.9197049140930176 + 50.0 * 8.931184768676758
Epoch 1660, val loss: 0.9241243600845337
Epoch 1670, training loss: 447.6596374511719 = 0.9177627563476562 + 50.0 * 8.934837341308594
Epoch 1670, val loss: 0.9222412109375
Epoch 1680, training loss: 447.8216857910156 = 0.9158245325088501 + 50.0 * 8.938117027282715
Epoch 1680, val loss: 0.920390784740448
Epoch 1690, training loss: 447.8557434082031 = 0.91387939453125 + 50.0 * 8.938837051391602
Epoch 1690, val loss: 0.9185062050819397
Epoch 1700, training loss: 447.8881530761719 = 0.9119253754615784 + 50.0 * 8.93952465057373
Epoch 1700, val loss: 0.9165815711021423
Epoch 1710, training loss: 447.98638916015625 = 0.9099786281585693 + 50.0 * 8.9415283203125
Epoch 1710, val loss: 0.9147364497184753
Epoch 1720, training loss: 448.22357177734375 = 0.9080307483673096 + 50.0 * 8.946310997009277
Epoch 1720, val loss: 0.9128507375717163
Epoch 1730, training loss: 447.9017333984375 = 0.9060301184654236 + 50.0 * 8.939913749694824
Epoch 1730, val loss: 0.910857617855072
Epoch 1740, training loss: 448.0499267578125 = 0.9040229320526123 + 50.0 * 8.942917823791504
Epoch 1740, val loss: 0.9089456796646118
Epoch 1750, training loss: 448.2210388183594 = 0.9020942449569702 + 50.0 * 8.946378707885742
Epoch 1750, val loss: 0.9070620536804199
Epoch 1760, training loss: 448.38824462890625 = 0.9001277089118958 + 50.0 * 8.949762344360352
Epoch 1760, val loss: 0.9051725268363953
Epoch 1770, training loss: 448.49615478515625 = 0.898166835308075 + 50.0 * 8.951959609985352
Epoch 1770, val loss: 0.9032468795776367
Epoch 1780, training loss: 448.70635986328125 = 0.8961729407310486 + 50.0 * 8.95620346069336
Epoch 1780, val loss: 0.9013017416000366
Epoch 1790, training loss: 448.44512939453125 = 0.8940903544425964 + 50.0 * 8.951020240783691
Epoch 1790, val loss: 0.8992508053779602
Epoch 1800, training loss: 448.29437255859375 = 0.8920820355415344 + 50.0 * 8.94804573059082
Epoch 1800, val loss: 0.8972910046577454
Epoch 1810, training loss: 448.4537353515625 = 0.8901453614234924 + 50.0 * 8.951272010803223
Epoch 1810, val loss: 0.8954135179519653
Epoch 1820, training loss: 448.7366027832031 = 0.8881595730781555 + 50.0 * 8.956969261169434
Epoch 1820, val loss: 0.893491804599762
Epoch 1830, training loss: 448.697265625 = 0.8861525654792786 + 50.0 * 8.956222534179688
Epoch 1830, val loss: 0.8915448188781738
Epoch 1840, training loss: 448.9055480957031 = 0.8841867446899414 + 50.0 * 8.960427284240723
Epoch 1840, val loss: 0.889618992805481
Epoch 1850, training loss: 449.0344543457031 = 0.8821467757225037 + 50.0 * 8.963046073913574
Epoch 1850, val loss: 0.8876442909240723
Epoch 1860, training loss: 449.0555419921875 = 0.8801423907279968 + 50.0 * 8.963507652282715
Epoch 1860, val loss: 0.885663628578186
Epoch 1870, training loss: 449.1500549316406 = 0.8781185150146484 + 50.0 * 8.965438842773438
Epoch 1870, val loss: 0.8836857080459595
Epoch 1880, training loss: 448.9844970703125 = 0.8761633038520813 + 50.0 * 8.962166786193848
Epoch 1880, val loss: 0.8818037509918213
Epoch 1890, training loss: 448.8984069824219 = 0.87406986951828 + 50.0 * 8.96048641204834
Epoch 1890, val loss: 0.8797603249549866
Epoch 1900, training loss: 449.1650695800781 = 0.8720921874046326 + 50.0 * 8.965859413146973
Epoch 1900, val loss: 0.8778314590454102
Epoch 1910, training loss: 449.4859619140625 = 0.8700806498527527 + 50.0 * 8.972317695617676
Epoch 1910, val loss: 0.8758745193481445
Epoch 1920, training loss: 449.5097351074219 = 0.8680081367492676 + 50.0 * 8.972834587097168
Epoch 1920, val loss: 0.8738674521446228
Epoch 1930, training loss: 449.5340270996094 = 0.8658534288406372 + 50.0 * 8.973363876342773
Epoch 1930, val loss: 0.8717402815818787
Epoch 1940, training loss: 449.5697326660156 = 0.8635085821151733 + 50.0 * 8.974124908447266
Epoch 1940, val loss: 0.8695130944252014
Epoch 1950, training loss: 449.59228515625 = 0.8612437844276428 + 50.0 * 8.974620819091797
Epoch 1950, val loss: 0.8673162460327148
Epoch 1960, training loss: 449.680419921875 = 0.8589094281196594 + 50.0 * 8.97642993927002
Epoch 1960, val loss: 0.864996075630188
Epoch 1970, training loss: 449.5819396972656 = 0.8565382361412048 + 50.0 * 8.974508285522461
Epoch 1970, val loss: 0.8626922369003296
Epoch 1980, training loss: 449.8260498046875 = 0.8541609644889832 + 50.0 * 8.979437828063965
Epoch 1980, val loss: 0.8603886961936951
Epoch 1990, training loss: 449.919189453125 = 0.8517475128173828 + 50.0 * 8.981348991394043
Epoch 1990, val loss: 0.8580543994903564
Epoch 2000, training loss: 449.4466247558594 = 0.8493323922157288 + 50.0 * 8.971945762634277
Epoch 2000, val loss: 0.8557301163673401
Epoch 2010, training loss: 448.9016418457031 = 0.8470388650894165 + 50.0 * 8.961091995239258
Epoch 2010, val loss: 0.8535394668579102
Epoch 2020, training loss: 449.174072265625 = 0.8448259830474854 + 50.0 * 8.966585159301758
Epoch 2020, val loss: 0.8513319492340088
Epoch 2030, training loss: 449.3978271484375 = 0.8425583243370056 + 50.0 * 8.971105575561523
Epoch 2030, val loss: 0.8491544723510742
Epoch 2040, training loss: 449.6687316894531 = 0.8403404355049133 + 50.0 * 8.976568222045898
Epoch 2040, val loss: 0.8469933867454529
Epoch 2050, training loss: 449.91864013671875 = 0.8380784392356873 + 50.0 * 8.981611251831055
Epoch 2050, val loss: 0.8447679877281189
Epoch 2060, training loss: 449.9302673339844 = 0.8357884287834167 + 50.0 * 8.981889724731445
Epoch 2060, val loss: 0.84253990650177
Epoch 2070, training loss: 449.9985046386719 = 0.8334829211235046 + 50.0 * 8.98330020904541
Epoch 2070, val loss: 0.8403364419937134
Epoch 2080, training loss: 450.1916809082031 = 0.8312082886695862 + 50.0 * 8.98720932006836
Epoch 2080, val loss: 0.8381180167198181
Epoch 2090, training loss: 450.11474609375 = 0.8289084434509277 + 50.0 * 8.985716819763184
Epoch 2090, val loss: 0.8358482122421265
Epoch 2100, training loss: 450.101806640625 = 0.8265840411186218 + 50.0 * 8.985504150390625
Epoch 2100, val loss: 0.8336423635482788
Epoch 2110, training loss: 450.2921142578125 = 0.8242921233177185 + 50.0 * 8.989356994628906
Epoch 2110, val loss: 0.8314046263694763
Epoch 2120, training loss: 450.44451904296875 = 0.821999728679657 + 50.0 * 8.992450714111328
Epoch 2120, val loss: 0.8291703462600708
Epoch 2130, training loss: 450.381103515625 = 0.819616436958313 + 50.0 * 8.991230010986328
Epoch 2130, val loss: 0.8269051909446716
Epoch 2140, training loss: 449.701171875 = 0.8171117901802063 + 50.0 * 8.977681159973145
Epoch 2140, val loss: 0.8244556188583374
Epoch 2150, training loss: 449.4833984375 = 0.8148692846298218 + 50.0 * 8.973370552062988
Epoch 2150, val loss: 0.8223148584365845
Epoch 2160, training loss: 449.40020751953125 = 0.8126813173294067 + 50.0 * 8.971750259399414
Epoch 2160, val loss: 0.8202027678489685
Epoch 2170, training loss: 449.92169189453125 = 0.8104693293571472 + 50.0 * 8.982224464416504
Epoch 2170, val loss: 0.8180550932884216
Epoch 2180, training loss: 450.27008056640625 = 0.8082425594329834 + 50.0 * 8.989236831665039
Epoch 2180, val loss: 0.8159160017967224
Epoch 2190, training loss: 450.6344909667969 = 0.8059152960777283 + 50.0 * 8.99657154083252
Epoch 2190, val loss: 0.8136604428291321
Epoch 2200, training loss: 450.80010986328125 = 0.8035956025123596 + 50.0 * 8.999930381774902
Epoch 2200, val loss: 0.811413586139679
Epoch 2210, training loss: 450.9056396484375 = 0.8012790679931641 + 50.0 * 9.002087593078613
Epoch 2210, val loss: 0.809202253818512
Epoch 2220, training loss: 451.04791259765625 = 0.7989595532417297 + 50.0 * 9.004979133605957
Epoch 2220, val loss: 0.8069368004798889
Epoch 2230, training loss: 450.9329833984375 = 0.7966490387916565 + 50.0 * 9.002726554870605
Epoch 2230, val loss: 0.8047117590904236
Epoch 2240, training loss: 450.99700927734375 = 0.7943047285079956 + 50.0 * 9.004054069519043
Epoch 2240, val loss: 0.8024746179580688
Epoch 2250, training loss: 451.0092468261719 = 0.7919852137565613 + 50.0 * 9.004344940185547
Epoch 2250, val loss: 0.8002434372901917
Epoch 2260, training loss: 451.206787109375 = 0.7896967530250549 + 50.0 * 9.008341789245605
Epoch 2260, val loss: 0.7980092167854309
Epoch 2270, training loss: 449.5154113769531 = 0.7868947982788086 + 50.0 * 8.974570274353027
Epoch 2270, val loss: 0.7954755425453186
Epoch 2280, training loss: 448.8650207519531 = 0.7854282259941101 + 50.0 * 8.961591720581055
Epoch 2280, val loss: 0.7939708828926086
Epoch 2290, training loss: 447.4917297363281 = 0.7832412123680115 + 50.0 * 8.93416976928711
Epoch 2290, val loss: 0.7918991446495056
Epoch 2300, training loss: 448.4711608886719 = 0.7812003493309021 + 50.0 * 8.9537992477417
Epoch 2300, val loss: 0.7898568511009216
Epoch 2310, training loss: 448.8678283691406 = 0.7790782451629639 + 50.0 * 8.961774826049805
Epoch 2310, val loss: 0.7878482341766357
Epoch 2320, training loss: 449.506103515625 = 0.7768241763114929 + 50.0 * 8.97458553314209
Epoch 2320, val loss: 0.7856912612915039
Epoch 2330, training loss: 449.80224609375 = 0.7745914459228516 + 50.0 * 8.980552673339844
Epoch 2330, val loss: 0.7835725545883179
Epoch 2340, training loss: 450.1500244140625 = 0.7723363041877747 + 50.0 * 8.987553596496582
Epoch 2340, val loss: 0.7814186811447144
Epoch 2350, training loss: 450.332275390625 = 0.7701137065887451 + 50.0 * 8.991243362426758
Epoch 2350, val loss: 0.7792713642120361
Epoch 2360, training loss: 450.4543762207031 = 0.7678417563438416 + 50.0 * 8.993730545043945
Epoch 2360, val loss: 0.7771012783050537
Epoch 2370, training loss: 450.6292724609375 = 0.765576958656311 + 50.0 * 8.997274398803711
Epoch 2370, val loss: 0.7749399542808533
Epoch 2380, training loss: 450.80078125 = 0.7633134126663208 + 50.0 * 9.000749588012695
Epoch 2380, val loss: 0.7727727890014648
Epoch 2390, training loss: 450.6474914550781 = 0.761003315448761 + 50.0 * 8.997729301452637
Epoch 2390, val loss: 0.7706049680709839
Epoch 2400, training loss: 450.7510986328125 = 0.7587752342224121 + 50.0 * 8.999846458435059
Epoch 2400, val loss: 0.7684312462806702
Epoch 2410, training loss: 450.894287109375 = 0.7565210461616516 + 50.0 * 9.002755165100098
Epoch 2410, val loss: 0.7662822008132935
Epoch 2420, training loss: 451.1081848144531 = 0.7542722821235657 + 50.0 * 9.007078170776367
Epoch 2420, val loss: 0.7641373872756958
Epoch 2430, training loss: 451.1143493652344 = 0.7520372271537781 + 50.0 * 9.007246017456055
Epoch 2430, val loss: 0.7619554996490479
Epoch 2440, training loss: 451.17034912109375 = 0.7497950196266174 + 50.0 * 9.008411407470703
Epoch 2440, val loss: 0.7598281502723694
Epoch 2450, training loss: 451.257568359375 = 0.7475635409355164 + 50.0 * 9.010200500488281
Epoch 2450, val loss: 0.7577128410339355
Epoch 2460, training loss: 451.2205810546875 = 0.7453418970108032 + 50.0 * 9.009505271911621
Epoch 2460, val loss: 0.7555844187736511
Epoch 2470, training loss: 451.2254638671875 = 0.7431445121765137 + 50.0 * 9.00964641571045
Epoch 2470, val loss: 0.7534376978874207
Epoch 2480, training loss: 451.24639892578125 = 0.7409549951553345 + 50.0 * 9.010108947753906
Epoch 2480, val loss: 0.7513617277145386
Epoch 2490, training loss: 451.40155029296875 = 0.7387524247169495 + 50.0 * 9.013256072998047
Epoch 2490, val loss: 0.7492510080337524
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.78
0.8640150691878578
=== training gcn model ===
Epoch 0, training loss: 510.50689697265625 = 1.097988486289978 + 50.0 * 10.188178062438965
Epoch 0, val loss: 1.0987484455108643
Epoch 10, training loss: 490.1476135253906 = 1.0975946187973022 + 50.0 * 9.781000137329102
Epoch 10, val loss: 1.0983740091323853
Epoch 20, training loss: 480.32763671875 = 1.0972086191177368 + 50.0 * 9.584609031677246
Epoch 20, val loss: 1.0979992151260376
Epoch 30, training loss: 472.74273681640625 = 1.0967644453048706 + 50.0 * 9.4329195022583
Epoch 30, val loss: 1.0975782871246338
Epoch 40, training loss: 466.7486267089844 = 1.096328854560852 + 50.0 * 9.313045501708984
Epoch 40, val loss: 1.0971676111221313
Epoch 50, training loss: 461.9146423339844 = 1.0958892107009888 + 50.0 * 9.216375350952148
Epoch 50, val loss: 1.096745491027832
Epoch 60, training loss: 457.88739013671875 = 1.0954567193984985 + 50.0 * 9.135838508605957
Epoch 60, val loss: 1.0963371992111206
Epoch 70, training loss: 454.5000305175781 = 1.0950318574905396 + 50.0 * 9.068099975585938
Epoch 70, val loss: 1.0959324836730957
Epoch 80, training loss: 451.6741943359375 = 1.0945993661880493 + 50.0 * 9.011591911315918
Epoch 80, val loss: 1.0955229997634888
Epoch 90, training loss: 449.2922058105469 = 1.0941702127456665 + 50.0 * 8.963960647583008
Epoch 90, val loss: 1.0951145887374878
Epoch 100, training loss: 447.30767822265625 = 1.0937386751174927 + 50.0 * 8.924278259277344
Epoch 100, val loss: 1.0947035551071167
Epoch 110, training loss: 445.63226318359375 = 1.0933051109313965 + 50.0 * 8.890779495239258
Epoch 110, val loss: 1.0942918062210083
Epoch 120, training loss: 444.126220703125 = 1.0928727388381958 + 50.0 * 8.86066722869873
Epoch 120, val loss: 1.0938827991485596
Epoch 130, training loss: 442.78656005859375 = 1.092430591583252 + 50.0 * 8.833882331848145
Epoch 130, val loss: 1.093459963798523
Epoch 140, training loss: 441.6596374511719 = 1.0920079946517944 + 50.0 * 8.811352729797363
Epoch 140, val loss: 1.0930602550506592
Epoch 150, training loss: 440.6384582519531 = 1.0915734767913818 + 50.0 * 8.790937423706055
Epoch 150, val loss: 1.0926461219787598
Epoch 160, training loss: 439.7130126953125 = 1.091132402420044 + 50.0 * 8.772438049316406
Epoch 160, val loss: 1.0922259092330933
Epoch 170, training loss: 438.91265869140625 = 1.0906904935836792 + 50.0 * 8.756439208984375
Epoch 170, val loss: 1.0918058156967163
Epoch 180, training loss: 438.1872253417969 = 1.0902403593063354 + 50.0 * 8.741939544677734
Epoch 180, val loss: 1.0913864374160767
Epoch 190, training loss: 437.5369873046875 = 1.0898455381393433 + 50.0 * 8.72894287109375
Epoch 190, val loss: 1.0910136699676514
Epoch 200, training loss: 436.97357177734375 = 1.0894834995269775 + 50.0 * 8.717681884765625
Epoch 200, val loss: 1.0906771421432495
Epoch 210, training loss: 436.4736022949219 = 1.0892187356948853 + 50.0 * 8.707687377929688
Epoch 210, val loss: 1.090436577796936
Epoch 220, training loss: 436.0788879394531 = 1.0890299081802368 + 50.0 * 8.699797630310059
Epoch 220, val loss: 1.090267539024353
Epoch 230, training loss: 435.65313720703125 = 1.0888382196426392 + 50.0 * 8.691286087036133
Epoch 230, val loss: 1.090092420578003
Epoch 240, training loss: 435.40228271484375 = 1.088645100593567 + 50.0 * 8.686272621154785
Epoch 240, val loss: 1.0899195671081543
Epoch 250, training loss: 435.1080627441406 = 1.0884438753128052 + 50.0 * 8.680392265319824
Epoch 250, val loss: 1.089739203453064
Epoch 260, training loss: 434.8654479980469 = 1.0882384777069092 + 50.0 * 8.675544738769531
Epoch 260, val loss: 1.089561939239502
Epoch 270, training loss: 434.5989685058594 = 1.0880366563796997 + 50.0 * 8.670218467712402
Epoch 270, val loss: 1.089382529258728
Epoch 280, training loss: 434.3682556152344 = 1.0878289937973022 + 50.0 * 8.665608406066895
Epoch 280, val loss: 1.0891963243484497
Epoch 290, training loss: 434.24871826171875 = 1.087616205215454 + 50.0 * 8.663222312927246
Epoch 290, val loss: 1.0890111923217773
Epoch 300, training loss: 434.1050720214844 = 1.0873875617980957 + 50.0 * 8.660353660583496
Epoch 300, val loss: 1.0888004302978516
Epoch 310, training loss: 434.0030822753906 = 1.0871788263320923 + 50.0 * 8.658317565917969
Epoch 310, val loss: 1.0886262655258179
Epoch 320, training loss: 434.2066955566406 = 1.0869650840759277 + 50.0 * 8.662394523620605
Epoch 320, val loss: 1.0884263515472412
Epoch 330, training loss: 433.6251525878906 = 1.0867254734039307 + 50.0 * 8.650768280029297
Epoch 330, val loss: 1.0882164239883423
Epoch 340, training loss: 433.583984375 = 1.0865154266357422 + 50.0 * 8.649949073791504
Epoch 340, val loss: 1.0880225896835327
Epoch 350, training loss: 433.5909118652344 = 1.0862821340560913 + 50.0 * 8.650093078613281
Epoch 350, val loss: 1.0878111124038696
Epoch 360, training loss: 433.4828796386719 = 1.0860388278961182 + 50.0 * 8.647936820983887
Epoch 360, val loss: 1.087598204612732
Epoch 370, training loss: 433.500732421875 = 1.0858001708984375 + 50.0 * 8.648298263549805
Epoch 370, val loss: 1.0873783826828003
Epoch 380, training loss: 433.6343994140625 = 1.085554599761963 + 50.0 * 8.65097713470459
Epoch 380, val loss: 1.0871543884277344
Epoch 390, training loss: 433.6109313964844 = 1.0852941274642944 + 50.0 * 8.6505126953125
Epoch 390, val loss: 1.0869245529174805
Epoch 400, training loss: 433.6250915527344 = 1.0850415229797363 + 50.0 * 8.650800704956055
Epoch 400, val loss: 1.0866948366165161
Epoch 410, training loss: 433.556640625 = 1.0847675800323486 + 50.0 * 8.649436950683594
Epoch 410, val loss: 1.0864497423171997
Epoch 420, training loss: 433.7237548828125 = 1.08450448513031 + 50.0 * 8.652785301208496
Epoch 420, val loss: 1.0862120389938354
Epoch 430, training loss: 433.82147216796875 = 1.0842257738113403 + 50.0 * 8.654745101928711
Epoch 430, val loss: 1.085961937904358
Epoch 440, training loss: 434.03411865234375 = 1.083956003189087 + 50.0 * 8.659003257751465
Epoch 440, val loss: 1.0857092142105103
Epoch 450, training loss: 434.617919921875 = 1.0836209058761597 + 50.0 * 8.670685768127441
Epoch 450, val loss: 1.0854179859161377
Epoch 460, training loss: 433.1829833984375 = 1.0833147764205933 + 50.0 * 8.641993522644043
Epoch 460, val loss: 1.0851342678070068
Epoch 470, training loss: 433.4873046875 = 1.08305025100708 + 50.0 * 8.648085594177246
Epoch 470, val loss: 1.0848898887634277
Epoch 480, training loss: 433.5810852050781 = 1.082749605178833 + 50.0 * 8.6499662399292
Epoch 480, val loss: 1.0846269130706787
Epoch 490, training loss: 433.7256164550781 = 1.0824395418167114 + 50.0 * 8.652863502502441
Epoch 490, val loss: 1.0843480825424194
Epoch 500, training loss: 433.79876708984375 = 1.0821248292922974 + 50.0 * 8.654333114624023
Epoch 500, val loss: 1.084061861038208
Epoch 510, training loss: 433.7994384765625 = 1.0817941427230835 + 50.0 * 8.654353141784668
Epoch 510, val loss: 1.0837676525115967
Epoch 520, training loss: 433.9105224609375 = 1.081468105316162 + 50.0 * 8.656580924987793
Epoch 520, val loss: 1.083450436592102
Epoch 530, training loss: 434.63714599609375 = 1.0811035633087158 + 50.0 * 8.671120643615723
Epoch 530, val loss: 1.0831433534622192
Epoch 540, training loss: 433.79827880859375 = 1.0807491540908813 + 50.0 * 8.654350280761719
Epoch 540, val loss: 1.0828299522399902
Epoch 550, training loss: 433.8917236328125 = 1.080407738685608 + 50.0 * 8.65622615814209
Epoch 550, val loss: 1.0825157165527344
Epoch 560, training loss: 434.03692626953125 = 1.080051302909851 + 50.0 * 8.659137725830078
Epoch 560, val loss: 1.082187294960022
Epoch 570, training loss: 434.3096008300781 = 1.0796972513198853 + 50.0 * 8.664597511291504
Epoch 570, val loss: 1.0818673372268677
Epoch 580, training loss: 434.2698059082031 = 1.0793168544769287 + 50.0 * 8.663809776306152
Epoch 580, val loss: 1.0815333127975464
Epoch 590, training loss: 434.5154724121094 = 1.078945517539978 + 50.0 * 8.668730735778809
Epoch 590, val loss: 1.0811856985092163
Epoch 600, training loss: 434.6436462402344 = 1.0785614252090454 + 50.0 * 8.67130184173584
Epoch 600, val loss: 1.080839991569519
Epoch 610, training loss: 434.464111328125 = 1.0781452655792236 + 50.0 * 8.667718887329102
Epoch 610, val loss: 1.0804511308670044
Epoch 620, training loss: 434.67645263671875 = 1.0777584314346313 + 50.0 * 8.671974182128906
Epoch 620, val loss: 1.0801057815551758
Epoch 630, training loss: 434.9198913574219 = 1.0773345232009888 + 50.0 * 8.676851272583008
Epoch 630, val loss: 1.0797312259674072
Epoch 640, training loss: 435.0464172363281 = 1.0769230127334595 + 50.0 * 8.679389953613281
Epoch 640, val loss: 1.0793532133102417
Epoch 650, training loss: 435.2296447753906 = 1.0765089988708496 + 50.0 * 8.683062553405762
Epoch 650, val loss: 1.0789769887924194
Epoch 660, training loss: 435.25494384765625 = 1.0760793685913086 + 50.0 * 8.683577537536621
Epoch 660, val loss: 1.078568696975708
Epoch 670, training loss: 435.3578186035156 = 1.0756301879882812 + 50.0 * 8.685644149780273
Epoch 670, val loss: 1.0781643390655518
Epoch 680, training loss: 435.418212890625 = 1.0751887559890747 + 50.0 * 8.686860084533691
Epoch 680, val loss: 1.0777513980865479
Epoch 690, training loss: 435.5477294921875 = 1.0748116970062256 + 50.0 * 8.689458847045898
Epoch 690, val loss: 1.0774061679840088
Epoch 700, training loss: 435.610107421875 = 1.0743087530136108 + 50.0 * 8.690715789794922
Epoch 700, val loss: 1.0769644975662231
Epoch 710, training loss: 435.7398681640625 = 1.0738482475280762 + 50.0 * 8.693320274353027
Epoch 710, val loss: 1.0765494108200073
Epoch 720, training loss: 435.9369812011719 = 1.073377251625061 + 50.0 * 8.697272300720215
Epoch 720, val loss: 1.0761151313781738
Epoch 730, training loss: 436.06817626953125 = 1.0729056596755981 + 50.0 * 8.699905395507812
Epoch 730, val loss: 1.0756657123565674
Epoch 740, training loss: 435.90704345703125 = 1.0724053382873535 + 50.0 * 8.69669246673584
Epoch 740, val loss: 1.0752111673355103
Epoch 750, training loss: 436.26470947265625 = 1.0718846321105957 + 50.0 * 8.703856468200684
Epoch 750, val loss: 1.074730634689331
Epoch 760, training loss: 436.20037841796875 = 1.0714017152786255 + 50.0 * 8.702579498291016
Epoch 760, val loss: 1.0742971897125244
Epoch 770, training loss: 436.4002990722656 = 1.0709110498428345 + 50.0 * 8.706587791442871
Epoch 770, val loss: 1.073848009109497
Epoch 780, training loss: 436.4297790527344 = 1.070406198501587 + 50.0 * 8.70718765258789
Epoch 780, val loss: 1.073380470275879
Epoch 790, training loss: 436.55828857421875 = 1.0698935985565186 + 50.0 * 8.709768295288086
Epoch 790, val loss: 1.0729084014892578
Epoch 800, training loss: 436.8523254394531 = 1.069380760192871 + 50.0 * 8.715659141540527
Epoch 800, val loss: 1.072442889213562
Epoch 810, training loss: 437.00390625 = 1.0688565969467163 + 50.0 * 8.718701362609863
Epoch 810, val loss: 1.0719623565673828
Epoch 820, training loss: 437.1543884277344 = 1.0683085918426514 + 50.0 * 8.721721649169922
Epoch 820, val loss: 1.071452021598816
Epoch 830, training loss: 437.2383728027344 = 1.0677704811096191 + 50.0 * 8.723411560058594
Epoch 830, val loss: 1.0709521770477295
Epoch 840, training loss: 437.1461181640625 = 1.0672197341918945 + 50.0 * 8.721577644348145
Epoch 840, val loss: 1.0704503059387207
Epoch 850, training loss: 437.3849792480469 = 1.0666710138320923 + 50.0 * 8.72636604309082
Epoch 850, val loss: 1.0699571371078491
Epoch 860, training loss: 437.5381774902344 = 1.0661128759384155 + 50.0 * 8.72944164276123
Epoch 860, val loss: 1.0694410800933838
Epoch 870, training loss: 437.6080017089844 = 1.0655604600906372 + 50.0 * 8.730849266052246
Epoch 870, val loss: 1.0689283609390259
Epoch 880, training loss: 437.8524169921875 = 1.0650018453598022 + 50.0 * 8.735748291015625
Epoch 880, val loss: 1.068420171737671
Epoch 890, training loss: 437.9832458496094 = 1.064437747001648 + 50.0 * 8.73837661743164
Epoch 890, val loss: 1.0678969621658325
Epoch 900, training loss: 437.9045104980469 = 1.0638378858566284 + 50.0 * 8.73681354522705
Epoch 900, val loss: 1.067348837852478
Epoch 910, training loss: 438.0357971191406 = 1.0632661581039429 + 50.0 * 8.739450454711914
Epoch 910, val loss: 1.066820740699768
Epoch 920, training loss: 438.224365234375 = 1.0626904964447021 + 50.0 * 8.743233680725098
Epoch 920, val loss: 1.0662974119186401
Epoch 930, training loss: 438.4070739746094 = 1.062100887298584 + 50.0 * 8.746899604797363
Epoch 930, val loss: 1.0657449960708618
Epoch 940, training loss: 438.3980712890625 = 1.0614888668060303 + 50.0 * 8.746731758117676
Epoch 940, val loss: 1.0651949644088745
Epoch 950, training loss: 438.4706726074219 = 1.060882806777954 + 50.0 * 8.74819564819336
Epoch 950, val loss: 1.0646291971206665
Epoch 960, training loss: 438.7292175292969 = 1.0602797269821167 + 50.0 * 8.753378868103027
Epoch 960, val loss: 1.0640616416931152
Epoch 970, training loss: 438.57293701171875 = 1.0596195459365845 + 50.0 * 8.750266075134277
Epoch 970, val loss: 1.0634629726409912
Epoch 980, training loss: 438.9555969238281 = 1.0590176582336426 + 50.0 * 8.75793170928955
Epoch 980, val loss: 1.062915325164795
Epoch 990, training loss: 439.0113525390625 = 1.058396816253662 + 50.0 * 8.759058952331543
Epoch 990, val loss: 1.06233811378479
Epoch 1000, training loss: 439.0171813964844 = 1.0577521324157715 + 50.0 * 8.759188652038574
Epoch 1000, val loss: 1.0617555379867554
Epoch 1010, training loss: 439.1282958984375 = 1.0571010112762451 + 50.0 * 8.76142406463623
Epoch 1010, val loss: 1.061163067817688
Epoch 1020, training loss: 439.3343200683594 = 1.0564557313919067 + 50.0 * 8.765557289123535
Epoch 1020, val loss: 1.0605602264404297
Epoch 1030, training loss: 439.1590881347656 = 1.0557892322540283 + 50.0 * 8.762065887451172
Epoch 1030, val loss: 1.0599249601364136
Epoch 1040, training loss: 438.9961853027344 = 1.0551308393478394 + 50.0 * 8.758820533752441
Epoch 1040, val loss: 1.0593266487121582
Epoch 1050, training loss: 439.02386474609375 = 1.0545145273208618 + 50.0 * 8.759387016296387
Epoch 1050, val loss: 1.0588082075119019
Epoch 1060, training loss: 439.2518615722656 = 1.053855061531067 + 50.0 * 8.763959884643555
Epoch 1060, val loss: 1.0581398010253906
Epoch 1070, training loss: 439.4301452636719 = 1.0531917810440063 + 50.0 * 8.767539024353027
Epoch 1070, val loss: 1.0575417280197144
Epoch 1080, training loss: 439.1415710449219 = 1.0525169372558594 + 50.0 * 8.761780738830566
Epoch 1080, val loss: 1.0569268465042114
Epoch 1090, training loss: 439.56024169921875 = 1.0518752336502075 + 50.0 * 8.770167350769043
Epoch 1090, val loss: 1.0563393831253052
Epoch 1100, training loss: 439.79669189453125 = 1.0512113571166992 + 50.0 * 8.774909973144531
Epoch 1100, val loss: 1.0557314157485962
Epoch 1110, training loss: 440.0444641113281 = 1.0505452156066895 + 50.0 * 8.779878616333008
Epoch 1110, val loss: 1.0551090240478516
Epoch 1120, training loss: 440.0715026855469 = 1.0498706102371216 + 50.0 * 8.78043270111084
Epoch 1120, val loss: 1.0544880628585815
Epoch 1130, training loss: 440.0649719238281 = 1.0491631031036377 + 50.0 * 8.780316352844238
Epoch 1130, val loss: 1.0538500547409058
Epoch 1140, training loss: 440.27056884765625 = 1.0484756231307983 + 50.0 * 8.784441947937012
Epoch 1140, val loss: 1.0532180070877075
Epoch 1150, training loss: 440.48101806640625 = 1.0477803945541382 + 50.0 * 8.788664817810059
Epoch 1150, val loss: 1.0525861978530884
Epoch 1160, training loss: 440.49468994140625 = 1.0470733642578125 + 50.0 * 8.788951873779297
Epoch 1160, val loss: 1.0519260168075562
Epoch 1170, training loss: 440.6250915527344 = 1.0463547706604004 + 50.0 * 8.791574478149414
Epoch 1170, val loss: 1.0512771606445312
Epoch 1180, training loss: 440.6269226074219 = 1.0456236600875854 + 50.0 * 8.7916259765625
Epoch 1180, val loss: 1.0505975484848022
Epoch 1190, training loss: 440.3870544433594 = 1.04487943649292 + 50.0 * 8.786843299865723
Epoch 1190, val loss: 1.0499210357666016
Epoch 1200, training loss: 440.3016662597656 = 1.0441358089447021 + 50.0 * 8.785150527954102
Epoch 1200, val loss: 1.049231767654419
Epoch 1210, training loss: 440.6618957519531 = 1.0433976650238037 + 50.0 * 8.792369842529297
Epoch 1210, val loss: 1.0485687255859375
Epoch 1220, training loss: 441.0346984863281 = 1.0426673889160156 + 50.0 * 8.799840927124023
Epoch 1220, val loss: 1.047898769378662
Epoch 1230, training loss: 441.14666748046875 = 1.0419286489486694 + 50.0 * 8.802094459533691
Epoch 1230, val loss: 1.0472187995910645
Epoch 1240, training loss: 441.1600646972656 = 1.0411607027053833 + 50.0 * 8.802377700805664
Epoch 1240, val loss: 1.0465052127838135
Epoch 1250, training loss: 441.3538513183594 = 1.0404187440872192 + 50.0 * 8.806268692016602
Epoch 1250, val loss: 1.0458229780197144
Epoch 1260, training loss: 441.46044921875 = 1.03966224193573 + 50.0 * 8.808415412902832
Epoch 1260, val loss: 1.045133113861084
Epoch 1270, training loss: 441.4673767089844 = 1.0388908386230469 + 50.0 * 8.80856990814209
Epoch 1270, val loss: 1.0444004535675049
Epoch 1280, training loss: 441.5804748535156 = 1.0381070375442505 + 50.0 * 8.810847282409668
Epoch 1280, val loss: 1.0436816215515137
Epoch 1290, training loss: 441.6184997558594 = 1.0373286008834839 + 50.0 * 8.811623573303223
Epoch 1290, val loss: 1.0429693460464478
Epoch 1300, training loss: 441.7244873046875 = 1.0365694761276245 + 50.0 * 8.813758850097656
Epoch 1300, val loss: 1.0422543287277222
Epoch 1310, training loss: 441.7489318847656 = 1.035787582397461 + 50.0 * 8.814262390136719
Epoch 1310, val loss: 1.041542649269104
Epoch 1320, training loss: 441.7984313964844 = 1.0350054502487183 + 50.0 * 8.815268516540527
Epoch 1320, val loss: 1.0408297777175903
Epoch 1330, training loss: 442.0041809082031 = 1.0342458486557007 + 50.0 * 8.819398880004883
Epoch 1330, val loss: 1.0401185750961304
Epoch 1340, training loss: 442.0162658691406 = 1.0334399938583374 + 50.0 * 8.819656372070312
Epoch 1340, val loss: 1.0393770933151245
Epoch 1350, training loss: 441.8119201660156 = 1.0326321125030518 + 50.0 * 8.81558609008789
Epoch 1350, val loss: 1.0386353731155396
Epoch 1360, training loss: 442.1396484375 = 1.0318506956100464 + 50.0 * 8.822155952453613
Epoch 1360, val loss: 1.0379180908203125
Epoch 1370, training loss: 442.29345703125 = 1.0310633182525635 + 50.0 * 8.825247764587402
Epoch 1370, val loss: 1.0371863842010498
Epoch 1380, training loss: 442.3771667480469 = 1.030266284942627 + 50.0 * 8.826937675476074
Epoch 1380, val loss: 1.0364631414413452
Epoch 1390, training loss: 442.3955078125 = 1.0294610261917114 + 50.0 * 8.82732105255127
Epoch 1390, val loss: 1.035714030265808
Epoch 1400, training loss: 442.4582824707031 = 1.0286273956298828 + 50.0 * 8.828593254089355
Epoch 1400, val loss: 1.0349431037902832
Epoch 1410, training loss: 442.40972900390625 = 1.0278059244155884 + 50.0 * 8.827638626098633
Epoch 1410, val loss: 1.0341861248016357
Epoch 1420, training loss: 442.5192565917969 = 1.0269837379455566 + 50.0 * 8.829845428466797
Epoch 1420, val loss: 1.0334450006484985
Epoch 1430, training loss: 442.745849609375 = 1.0261775255203247 + 50.0 * 8.834393501281738
Epoch 1430, val loss: 1.032692790031433
Epoch 1440, training loss: 442.6954650878906 = 1.0253400802612305 + 50.0 * 8.833402633666992
Epoch 1440, val loss: 1.031929850578308
Epoch 1450, training loss: 442.59967041015625 = 1.024480938911438 + 50.0 * 8.831503868103027
Epoch 1450, val loss: 1.031129240989685
Epoch 1460, training loss: 442.5700378417969 = 1.0236468315124512 + 50.0 * 8.830927848815918
Epoch 1460, val loss: 1.0303391218185425
Epoch 1470, training loss: 442.7216796875 = 1.0227850675582886 + 50.0 * 8.833977699279785
Epoch 1470, val loss: 1.0295647382736206
Epoch 1480, training loss: 442.9759826660156 = 1.0219535827636719 + 50.0 * 8.839080810546875
Epoch 1480, val loss: 1.0288034677505493
Epoch 1490, training loss: 443.052001953125 = 1.0210763216018677 + 50.0 * 8.840618133544922
Epoch 1490, val loss: 1.0279905796051025
Epoch 1500, training loss: 442.92974853515625 = 1.020200252532959 + 50.0 * 8.838191032409668
Epoch 1500, val loss: 1.0271737575531006
Epoch 1510, training loss: 443.20263671875 = 1.0193438529968262 + 50.0 * 8.843666076660156
Epoch 1510, val loss: 1.0263972282409668
Epoch 1520, training loss: 443.3470153808594 = 1.0184639692306519 + 50.0 * 8.84657096862793
Epoch 1520, val loss: 1.025589942932129
Epoch 1530, training loss: 443.3231201171875 = 1.0175578594207764 + 50.0 * 8.846111297607422
Epoch 1530, val loss: 1.0247523784637451
Epoch 1540, training loss: 443.4833068847656 = 1.0166774988174438 + 50.0 * 8.849332809448242
Epoch 1540, val loss: 1.0239384174346924
Epoch 1550, training loss: 443.6548156738281 = 1.01579749584198 + 50.0 * 8.85278034210205
Epoch 1550, val loss: 1.0231059789657593
Epoch 1560, training loss: 443.33892822265625 = 1.0148754119873047 + 50.0 * 8.846481323242188
Epoch 1560, val loss: 1.0222632884979248
Epoch 1570, training loss: 443.6592712402344 = 1.0139440298080444 + 50.0 * 8.852906227111816
Epoch 1570, val loss: 1.0214025974273682
Epoch 1580, training loss: 443.0338134765625 = 1.012985348701477 + 50.0 * 8.840415954589844
Epoch 1580, val loss: 1.0205271244049072
Epoch 1590, training loss: 443.4194641113281 = 1.012226939201355 + 50.0 * 8.84814453125
Epoch 1590, val loss: 1.0198276042938232
Epoch 1600, training loss: 443.58990478515625 = 1.0113558769226074 + 50.0 * 8.851571083068848
Epoch 1600, val loss: 1.0189993381500244
Epoch 1610, training loss: 443.3652038574219 = 1.0102773904800415 + 50.0 * 8.847098350524902
Epoch 1610, val loss: 1.0180174112319946
Epoch 1620, training loss: 443.4659729003906 = 1.0093413591384888 + 50.0 * 8.849132537841797
Epoch 1620, val loss: 1.0171787738800049
Epoch 1630, training loss: 443.7054443359375 = 1.0084644556045532 + 50.0 * 8.8539400100708
Epoch 1630, val loss: 1.016357660293579
Epoch 1640, training loss: 443.7716369628906 = 1.007542371749878 + 50.0 * 8.855281829833984
Epoch 1640, val loss: 1.0155142545700073
Epoch 1650, training loss: 443.7910461425781 = 1.006585955619812 + 50.0 * 8.85568904876709
Epoch 1650, val loss: 1.0146359205245972
Epoch 1660, training loss: 443.9105529785156 = 1.0056434869766235 + 50.0 * 8.858098030090332
Epoch 1660, val loss: 1.0137698650360107
Epoch 1670, training loss: 444.1389465332031 = 1.0047056674957275 + 50.0 * 8.862685203552246
Epoch 1670, val loss: 1.012897253036499
Epoch 1680, training loss: 444.05096435546875 = 1.0037254095077515 + 50.0 * 8.860944747924805
Epoch 1680, val loss: 1.0120203495025635
Epoch 1690, training loss: 444.1986999511719 = 1.0027835369110107 + 50.0 * 8.86391830444336
Epoch 1690, val loss: 1.0111420154571533
Epoch 1700, training loss: 444.14825439453125 = 1.0018107891082764 + 50.0 * 8.862929344177246
Epoch 1700, val loss: 1.0102404356002808
Epoch 1710, training loss: 444.16107177734375 = 1.0008214712142944 + 50.0 * 8.863204956054688
Epoch 1710, val loss: 1.0093507766723633
Epoch 1720, training loss: 444.4510498046875 = 0.9998803734779358 + 50.0 * 8.869023323059082
Epoch 1720, val loss: 1.0084643363952637
Epoch 1730, training loss: 444.2287292480469 = 0.9988684058189392 + 50.0 * 8.86459732055664
Epoch 1730, val loss: 1.0075311660766602
Epoch 1740, training loss: 444.3209228515625 = 0.9979060888290405 + 50.0 * 8.866460800170898
Epoch 1740, val loss: 1.0066604614257812
Epoch 1750, training loss: 444.5520324707031 = 0.9969176054000854 + 50.0 * 8.871102333068848
Epoch 1750, val loss: 1.0057448148727417
Epoch 1760, training loss: 444.52593994140625 = 0.9959240555763245 + 50.0 * 8.870600700378418
Epoch 1760, val loss: 1.0048547983169556
Epoch 1770, training loss: 444.33856201171875 = 0.9949541687965393 + 50.0 * 8.86687183380127
Epoch 1770, val loss: 1.0039271116256714
Epoch 1780, training loss: 444.58135986328125 = 0.9939675331115723 + 50.0 * 8.871747970581055
Epoch 1780, val loss: 1.003033995628357
Epoch 1790, training loss: 444.8141784667969 = 0.9929862022399902 + 50.0 * 8.876423835754395
Epoch 1790, val loss: 1.0021312236785889
Epoch 1800, training loss: 444.7759094238281 = 0.9918641448020935 + 50.0 * 8.875680923461914
Epoch 1800, val loss: 1.000889778137207
Epoch 1810, training loss: 439.82513427734375 = 0.990483283996582 + 50.0 * 8.776693344116211
Epoch 1810, val loss: 0.999972403049469
Epoch 1820, training loss: 441.4620056152344 = 0.9898779988288879 + 50.0 * 8.809442520141602
Epoch 1820, val loss: 0.9992738962173462
Epoch 1830, training loss: 442.7092590332031 = 0.989165186882019 + 50.0 * 8.834402084350586
Epoch 1830, val loss: 0.9985636472702026
Epoch 1840, training loss: 442.38787841796875 = 0.9880986213684082 + 50.0 * 8.827995300292969
Epoch 1840, val loss: 0.9976279735565186
Epoch 1850, training loss: 442.3469543457031 = 0.9870943427085876 + 50.0 * 8.827197074890137
Epoch 1850, val loss: 0.9967079758644104
Epoch 1860, training loss: 443.0808410644531 = 0.986172616481781 + 50.0 * 8.841893196105957
Epoch 1860, val loss: 0.9958559274673462
Epoch 1870, training loss: 443.48046875 = 0.985234797000885 + 50.0 * 8.849905014038086
Epoch 1870, val loss: 0.9949912428855896
Epoch 1880, training loss: 443.92364501953125 = 0.9842426180839539 + 50.0 * 8.858787536621094
Epoch 1880, val loss: 0.9940866231918335
Epoch 1890, training loss: 444.0491943359375 = 0.9832060933113098 + 50.0 * 8.861319541931152
Epoch 1890, val loss: 0.9931374192237854
Epoch 1900, training loss: 444.3027038574219 = 0.9821770191192627 + 50.0 * 8.866410255432129
Epoch 1900, val loss: 0.9921985864639282
Epoch 1910, training loss: 444.474365234375 = 0.9811474084854126 + 50.0 * 8.869864463806152
Epoch 1910, val loss: 0.9912535548210144
Epoch 1920, training loss: 444.5774230957031 = 0.9801099896430969 + 50.0 * 8.871946334838867
Epoch 1920, val loss: 0.990303635597229
Epoch 1930, training loss: 444.65301513671875 = 0.9790581464767456 + 50.0 * 8.873478889465332
Epoch 1930, val loss: 0.9893264770507812
Epoch 1940, training loss: 444.6025390625 = 0.9780008792877197 + 50.0 * 8.872490882873535
Epoch 1940, val loss: 0.9883655309677124
Epoch 1950, training loss: 444.8562927246094 = 0.9769507646560669 + 50.0 * 8.877586364746094
Epoch 1950, val loss: 0.9874182343482971
Epoch 1960, training loss: 444.88177490234375 = 0.9758959412574768 + 50.0 * 8.878117561340332
Epoch 1960, val loss: 0.9864422082901001
Epoch 1970, training loss: 444.8358459472656 = 0.9748108983039856 + 50.0 * 8.877220153808594
Epoch 1970, val loss: 0.9854641556739807
Epoch 1980, training loss: 444.9709777832031 = 0.9737760424613953 + 50.0 * 8.87994384765625
Epoch 1980, val loss: 0.9844984412193298
Epoch 1990, training loss: 445.2210998535156 = 0.9727113842964172 + 50.0 * 8.884967803955078
Epoch 1990, val loss: 0.9835271835327148
Epoch 2000, training loss: 445.1357421875 = 0.9716193079948425 + 50.0 * 8.883282661437988
Epoch 2000, val loss: 0.9825275540351868
Epoch 2010, training loss: 445.10894775390625 = 0.9705384373664856 + 50.0 * 8.882767677307129
Epoch 2010, val loss: 0.981498658657074
Epoch 2020, training loss: 444.789794921875 = 0.9694206714630127 + 50.0 * 8.876407623291016
Epoch 2020, val loss: 0.9804823398590088
Epoch 2030, training loss: 444.83502197265625 = 0.9684455990791321 + 50.0 * 8.877331733703613
Epoch 2030, val loss: 0.9795800447463989
Epoch 2040, training loss: 445.19091796875 = 0.9673788547515869 + 50.0 * 8.88447093963623
Epoch 2040, val loss: 0.9786189198493958
Epoch 2050, training loss: 445.49462890625 = 0.966332733631134 + 50.0 * 8.890565872192383
Epoch 2050, val loss: 0.9776654839515686
Epoch 2060, training loss: 445.6127014160156 = 0.9652543067932129 + 50.0 * 8.892949104309082
Epoch 2060, val loss: 0.9766745567321777
Epoch 2070, training loss: 445.5010681152344 = 0.964152455329895 + 50.0 * 8.890738487243652
Epoch 2070, val loss: 0.9756761789321899
Epoch 2080, training loss: 445.75244140625 = 0.9630739092826843 + 50.0 * 8.895787239074707
Epoch 2080, val loss: 0.9746936559677124
Epoch 2090, training loss: 445.9200134277344 = 0.9619912505149841 + 50.0 * 8.899160385131836
Epoch 2090, val loss: 0.9737036824226379
Epoch 2100, training loss: 445.54290771484375 = 0.960872232913971 + 50.0 * 8.891640663146973
Epoch 2100, val loss: 0.9726629853248596
Epoch 2110, training loss: 445.14794921875 = 0.9596713781356812 + 50.0 * 8.88376522064209
Epoch 2110, val loss: 0.9715897440910339
Epoch 2120, training loss: 445.7546081542969 = 0.9587892889976501 + 50.0 * 8.895915985107422
Epoch 2120, val loss: 0.9707419276237488
Epoch 2130, training loss: 442.6817626953125 = 0.9573766589164734 + 50.0 * 8.834487915039062
Epoch 2130, val loss: 0.969497799873352
Epoch 2140, training loss: 444.0712890625 = 0.9565471410751343 + 50.0 * 8.862295150756836
Epoch 2140, val loss: 0.9687314033508301
Epoch 2150, training loss: 444.751708984375 = 0.955669105052948 + 50.0 * 8.875921249389648
Epoch 2150, val loss: 0.9679412245750427
Epoch 2160, training loss: 444.0931396484375 = 0.9545405507087708 + 50.0 * 8.862771987915039
Epoch 2160, val loss: 0.9669351577758789
Epoch 2170, training loss: 444.60076904296875 = 0.9535260200500488 + 50.0 * 8.872944831848145
Epoch 2170, val loss: 0.9660019874572754
Epoch 2180, training loss: 445.2030334472656 = 0.9524943232536316 + 50.0 * 8.885010719299316
Epoch 2180, val loss: 0.965054988861084
Epoch 2190, training loss: 445.6030578613281 = 0.9514521360397339 + 50.0 * 8.89303207397461
Epoch 2190, val loss: 0.9641109704971313
Epoch 2200, training loss: 445.8627624511719 = 0.9503731727600098 + 50.0 * 8.898247718811035
Epoch 2200, val loss: 0.9631311893463135
Epoch 2210, training loss: 445.8906555175781 = 0.9492756724357605 + 50.0 * 8.89882755279541
Epoch 2210, val loss: 0.9621240496635437
Epoch 2220, training loss: 445.9376220703125 = 0.9481703639030457 + 50.0 * 8.899788856506348
Epoch 2220, val loss: 0.961123526096344
Epoch 2230, training loss: 446.0511474609375 = 0.9470744132995605 + 50.0 * 8.902081489562988
Epoch 2230, val loss: 0.960131049156189
Epoch 2240, training loss: 446.2591857910156 = 0.94597327709198 + 50.0 * 8.906264305114746
Epoch 2240, val loss: 0.9591357111930847
Epoch 2250, training loss: 445.16253662109375 = 0.9447651505470276 + 50.0 * 8.884355545043945
Epoch 2250, val loss: 0.9580444693565369
Epoch 2260, training loss: 445.6070556640625 = 0.9437366127967834 + 50.0 * 8.893266677856445
Epoch 2260, val loss: 0.9570940732955933
Epoch 2270, training loss: 446.0872802734375 = 0.9426871538162231 + 50.0 * 8.902892112731934
Epoch 2270, val loss: 0.9561340808868408
Epoch 2280, training loss: 446.3609924316406 = 0.941591203212738 + 50.0 * 8.908388137817383
Epoch 2280, val loss: 0.9551339149475098
Epoch 2290, training loss: 446.4781494140625 = 0.9404869079589844 + 50.0 * 8.91075325012207
Epoch 2290, val loss: 0.954129159450531
Epoch 2300, training loss: 446.4674987792969 = 0.9393747448921204 + 50.0 * 8.910562515258789
Epoch 2300, val loss: 0.95311039686203
Epoch 2310, training loss: 446.52679443359375 = 0.938265323638916 + 50.0 * 8.911770820617676
Epoch 2310, val loss: 0.9521133303642273
Epoch 2320, training loss: 446.6524963378906 = 0.9371509552001953 + 50.0 * 8.914306640625
Epoch 2320, val loss: 0.9511087536811829
Epoch 2330, training loss: 446.793701171875 = 0.9360451698303223 + 50.0 * 8.917153358459473
Epoch 2330, val loss: 0.950096845626831
Epoch 2340, training loss: 446.86212158203125 = 0.934927225112915 + 50.0 * 8.918543815612793
Epoch 2340, val loss: 0.9490926861763
Epoch 2350, training loss: 446.68804931640625 = 0.9338107109069824 + 50.0 * 8.915084838867188
Epoch 2350, val loss: 0.9480629563331604
Epoch 2360, training loss: 446.6468505859375 = 0.9326959848403931 + 50.0 * 8.91428279876709
Epoch 2360, val loss: 0.9470568299293518
Epoch 2370, training loss: 446.8608093261719 = 0.9315806031227112 + 50.0 * 8.918584823608398
Epoch 2370, val loss: 0.9460484385490417
Epoch 2380, training loss: 447.06561279296875 = 0.930483341217041 + 50.0 * 8.92270278930664
Epoch 2380, val loss: 0.9450496435165405
Epoch 2390, training loss: 446.8546447753906 = 0.9293254613876343 + 50.0 * 8.918506622314453
Epoch 2390, val loss: 0.9440010786056519
Epoch 2400, training loss: 446.97247314453125 = 0.9282408952713013 + 50.0 * 8.920884132385254
Epoch 2400, val loss: 0.9430126547813416
Epoch 2410, training loss: 447.14007568359375 = 0.9271312355995178 + 50.0 * 8.924259185791016
Epoch 2410, val loss: 0.9420163035392761
Epoch 2420, training loss: 447.28631591796875 = 0.9260129332542419 + 50.0 * 8.927206039428711
Epoch 2420, val loss: 0.9410086274147034
Epoch 2430, training loss: 447.3353576660156 = 0.9248971939086914 + 50.0 * 8.92820930480957
Epoch 2430, val loss: 0.9399814605712891
Epoch 2440, training loss: 447.2428283691406 = 0.923768162727356 + 50.0 * 8.92638111114502
Epoch 2440, val loss: 0.9389523863792419
Epoch 2450, training loss: 447.1805725097656 = 0.9226429462432861 + 50.0 * 8.925158500671387
Epoch 2450, val loss: 0.93796306848526
Epoch 2460, training loss: 447.363037109375 = 0.9215527772903442 + 50.0 * 8.92883014678955
Epoch 2460, val loss: 0.9369612336158752
Epoch 2470, training loss: 447.49017333984375 = 0.9204416871070862 + 50.0 * 8.931394577026367
Epoch 2470, val loss: 0.9359535574913025
Epoch 2480, training loss: 447.70526123046875 = 0.9193299412727356 + 50.0 * 8.935718536376953
Epoch 2480, val loss: 0.934939980506897
Epoch 2490, training loss: 447.6500549316406 = 0.9182171821594238 + 50.0 * 8.934637069702148
Epoch 2490, val loss: 0.9339231848716736
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.513768115942029
0.8635803810765776
=== training gcn model ===
Epoch 0, training loss: 511.49298095703125 = 1.1139018535614014 + 50.0 * 10.207581520080566
Epoch 0, val loss: 1.1140440702438354
Epoch 10, training loss: 490.4188232421875 = 1.1130363941192627 + 50.0 * 9.786115646362305
Epoch 10, val loss: 1.1131625175476074
Epoch 20, training loss: 480.64288330078125 = 1.1122782230377197 + 50.0 * 9.590612411499023
Epoch 20, val loss: 1.1124118566513062
Epoch 30, training loss: 473.61614990234375 = 1.1115370988845825 + 50.0 * 9.450092315673828
Epoch 30, val loss: 1.1116796731948853
Epoch 40, training loss: 467.8471374511719 = 1.1108167171478271 + 50.0 * 9.334726333618164
Epoch 40, val loss: 1.110963225364685
Epoch 50, training loss: 462.9148254394531 = 1.1101222038269043 + 50.0 * 9.23609447479248
Epoch 50, val loss: 1.1102650165557861
Epoch 60, training loss: 458.7772216796875 = 1.1094231605529785 + 50.0 * 9.153355598449707
Epoch 60, val loss: 1.1095670461654663
Epoch 70, training loss: 455.2959289550781 = 1.1087242364883423 + 50.0 * 9.083744049072266
Epoch 70, val loss: 1.1088662147521973
Epoch 80, training loss: 452.3061828613281 = 1.108026385307312 + 50.0 * 9.02396297454834
Epoch 80, val loss: 1.1081713438034058
Epoch 90, training loss: 449.81524658203125 = 1.1073336601257324 + 50.0 * 8.97415828704834
Epoch 90, val loss: 1.107480764389038
Epoch 100, training loss: 447.75054931640625 = 1.1066222190856934 + 50.0 * 8.932878494262695
Epoch 100, val loss: 1.1067731380462646
Epoch 110, training loss: 445.9790344238281 = 1.1059263944625854 + 50.0 * 8.897461891174316
Epoch 110, val loss: 1.106076717376709
Epoch 120, training loss: 444.3860778808594 = 1.1052272319793701 + 50.0 * 8.865616798400879
Epoch 120, val loss: 1.105380892753601
Epoch 130, training loss: 443.0875549316406 = 1.1045295000076294 + 50.0 * 8.83966064453125
Epoch 130, val loss: 1.1046936511993408
Epoch 140, training loss: 441.90228271484375 = 1.1038341522216797 + 50.0 * 8.815969467163086
Epoch 140, val loss: 1.1040005683898926
Epoch 150, training loss: 440.94091796875 = 1.1031272411346436 + 50.0 * 8.79675579071045
Epoch 150, val loss: 1.103303074836731
Epoch 160, training loss: 440.0373229980469 = 1.1024196147918701 + 50.0 * 8.778697967529297
Epoch 160, val loss: 1.102593183517456
Epoch 170, training loss: 439.4017639160156 = 1.101688265800476 + 50.0 * 8.76600170135498
Epoch 170, val loss: 1.1018778085708618
Epoch 180, training loss: 438.79144287109375 = 1.1009787321090698 + 50.0 * 8.753808975219727
Epoch 180, val loss: 1.101180076599121
Epoch 190, training loss: 438.0823974609375 = 1.1002577543258667 + 50.0 * 8.739643096923828
Epoch 190, val loss: 1.1004605293273926
Epoch 200, training loss: 437.6554260253906 = 1.0995268821716309 + 50.0 * 8.731118202209473
Epoch 200, val loss: 1.099743127822876
Epoch 210, training loss: 437.20904541015625 = 1.0987927913665771 + 50.0 * 8.72220516204834
Epoch 210, val loss: 1.0990190505981445
Epoch 220, training loss: 436.8822326660156 = 1.098029375076294 + 50.0 * 8.715683937072754
Epoch 220, val loss: 1.098262071609497
Epoch 230, training loss: 436.5447998046875 = 1.0972568988800049 + 50.0 * 8.708950996398926
Epoch 230, val loss: 1.09750497341156
Epoch 240, training loss: 436.2248229980469 = 1.096472144126892 + 50.0 * 8.702567100524902
Epoch 240, val loss: 1.0967397689819336
Epoch 250, training loss: 435.96575927734375 = 1.0956672430038452 + 50.0 * 8.697402000427246
Epoch 250, val loss: 1.095954179763794
Epoch 260, training loss: 435.7685241699219 = 1.0948821306228638 + 50.0 * 8.693472862243652
Epoch 260, val loss: 1.0951812267303467
Epoch 270, training loss: 435.7812194824219 = 1.0940748453140259 + 50.0 * 8.693742752075195
Epoch 270, val loss: 1.0943946838378906
Epoch 280, training loss: 435.33868408203125 = 1.0932245254516602 + 50.0 * 8.684908866882324
Epoch 280, val loss: 1.0935503244400024
Epoch 290, training loss: 435.27667236328125 = 1.092454195022583 + 50.0 * 8.683684349060059
Epoch 290, val loss: 1.0927826166152954
Epoch 300, training loss: 435.1495056152344 = 1.0915355682373047 + 50.0 * 8.681159973144531
Epoch 300, val loss: 1.0918868780136108
Epoch 310, training loss: 436.3512268066406 = 1.0907268524169922 + 50.0 * 8.705209732055664
Epoch 310, val loss: 1.0910217761993408
Epoch 320, training loss: 434.43304443359375 = 1.089435338973999 + 50.0 * 8.666872024536133
Epoch 320, val loss: 1.0898727178573608
Epoch 330, training loss: 436.1489562988281 = 1.089108943939209 + 50.0 * 8.701196670532227
Epoch 330, val loss: 1.0895057916641235
Epoch 340, training loss: 434.4076232910156 = 1.0880985260009766 + 50.0 * 8.666390419006348
Epoch 340, val loss: 1.0885119438171387
Epoch 350, training loss: 434.70208740234375 = 1.087254285812378 + 50.0 * 8.672296524047852
Epoch 350, val loss: 1.0877021551132202
Epoch 360, training loss: 434.6031799316406 = 1.0863802433013916 + 50.0 * 8.67033576965332
Epoch 360, val loss: 1.0868346691131592
Epoch 370, training loss: 434.7332763671875 = 1.085497498512268 + 50.0 * 8.672955513000488
Epoch 370, val loss: 1.0859839916229248
Epoch 380, training loss: 434.7403259277344 = 1.084587812423706 + 50.0 * 8.673114776611328
Epoch 380, val loss: 1.085105061531067
Epoch 390, training loss: 434.6272888183594 = 1.0836591720581055 + 50.0 * 8.670872688293457
Epoch 390, val loss: 1.084209680557251
Epoch 400, training loss: 434.8602600097656 = 1.0828028917312622 + 50.0 * 8.675549507141113
Epoch 400, val loss: 1.0833631753921509
Epoch 410, training loss: 434.8843994140625 = 1.08186674118042 + 50.0 * 8.676050186157227
Epoch 410, val loss: 1.0824604034423828
Epoch 420, training loss: 434.8447570800781 = 1.080939531326294 + 50.0 * 8.675276756286621
Epoch 420, val loss: 1.0815558433532715
Epoch 430, training loss: 434.8684387207031 = 1.0800349712371826 + 50.0 * 8.67576789855957
Epoch 430, val loss: 1.080666422843933
Epoch 440, training loss: 435.0238952636719 = 1.079118251800537 + 50.0 * 8.678894996643066
Epoch 440, val loss: 1.079787254333496
Epoch 450, training loss: 435.1219787597656 = 1.078253984451294 + 50.0 * 8.680874824523926
Epoch 450, val loss: 1.07895827293396
Epoch 460, training loss: 435.2853698730469 = 1.0773676633834839 + 50.0 * 8.684160232543945
Epoch 460, val loss: 1.0780922174453735
Epoch 470, training loss: 435.42669677734375 = 1.0764849185943604 + 50.0 * 8.687004089355469
Epoch 470, val loss: 1.077243685722351
Epoch 480, training loss: 435.5514831542969 = 1.0756354331970215 + 50.0 * 8.6895170211792
Epoch 480, val loss: 1.0764212608337402
Epoch 490, training loss: 435.5709228515625 = 1.074772834777832 + 50.0 * 8.689923286437988
Epoch 490, val loss: 1.0755852460861206
Epoch 500, training loss: 435.56427001953125 = 1.0739072561264038 + 50.0 * 8.689806938171387
Epoch 500, val loss: 1.0747575759887695
Epoch 510, training loss: 435.7868347167969 = 1.0731595754623413 + 50.0 * 8.694273948669434
Epoch 510, val loss: 1.074025273323059
Epoch 520, training loss: 435.8711853027344 = 1.0723689794540405 + 50.0 * 8.695976257324219
Epoch 520, val loss: 1.0732781887054443
Epoch 530, training loss: 435.9908142089844 = 1.0716012716293335 + 50.0 * 8.698384284973145
Epoch 530, val loss: 1.0725370645523071
Epoch 540, training loss: 436.07952880859375 = 1.0708588361740112 + 50.0 * 8.700173377990723
Epoch 540, val loss: 1.0718276500701904
Epoch 550, training loss: 436.133056640625 = 1.0701488256454468 + 50.0 * 8.701257705688477
Epoch 550, val loss: 1.0711458921432495
Epoch 560, training loss: 436.2357482910156 = 1.06942880153656 + 50.0 * 8.703326225280762
Epoch 560, val loss: 1.0704584121704102
Epoch 570, training loss: 436.3752136230469 = 1.0687445402145386 + 50.0 * 8.70612907409668
Epoch 570, val loss: 1.0697956085205078
Epoch 580, training loss: 436.4144592285156 = 1.0680445432662964 + 50.0 * 8.706928253173828
Epoch 580, val loss: 1.069140076637268
Epoch 590, training loss: 436.5603332519531 = 1.067366361618042 + 50.0 * 8.709859848022461
Epoch 590, val loss: 1.0685017108917236
Epoch 600, training loss: 436.42889404296875 = 1.0666532516479492 + 50.0 * 8.707244873046875
Epoch 600, val loss: 1.0678272247314453
Epoch 610, training loss: 436.5989074707031 = 1.0659635066986084 + 50.0 * 8.71065902709961
Epoch 610, val loss: 1.0671578645706177
Epoch 620, training loss: 436.7449035644531 = 1.065267562866211 + 50.0 * 8.713592529296875
Epoch 620, val loss: 1.066507339477539
Epoch 630, training loss: 436.36468505859375 = 1.0644690990447998 + 50.0 * 8.70600414276123
Epoch 630, val loss: 1.0657168626785278
Epoch 640, training loss: 437.0339050292969 = 1.0638526678085327 + 50.0 * 8.719401359558105
Epoch 640, val loss: 1.0651434659957886
Epoch 650, training loss: 436.9167175292969 = 1.063145399093628 + 50.0 * 8.717071533203125
Epoch 650, val loss: 1.0644776821136475
Epoch 660, training loss: 437.1046447753906 = 1.0624667406082153 + 50.0 * 8.720843315124512
Epoch 660, val loss: 1.0638384819030762
Epoch 670, training loss: 437.2975769042969 = 1.0617671012878418 + 50.0 * 8.724716186523438
Epoch 670, val loss: 1.0631649494171143
Epoch 680, training loss: 437.42327880859375 = 1.0610631704330444 + 50.0 * 8.72724437713623
Epoch 680, val loss: 1.0624868869781494
Epoch 690, training loss: 437.50775146484375 = 1.0603522062301636 + 50.0 * 8.728947639465332
Epoch 690, val loss: 1.0618107318878174
Epoch 700, training loss: 437.7421875 = 1.0595835447311401 + 50.0 * 8.733652114868164
Epoch 700, val loss: 1.0611048936843872
Epoch 710, training loss: 437.6894836425781 = 1.0588939189910889 + 50.0 * 8.732611656188965
Epoch 710, val loss: 1.0604212284088135
Epoch 720, training loss: 437.8172912597656 = 1.0581780672073364 + 50.0 * 8.735182762145996
Epoch 720, val loss: 1.0597553253173828
Epoch 730, training loss: 437.83331298828125 = 1.0574421882629395 + 50.0 * 8.735517501831055
Epoch 730, val loss: 1.0590459108352661
Epoch 740, training loss: 437.92681884765625 = 1.056718349456787 + 50.0 * 8.737401962280273
Epoch 740, val loss: 1.0583659410476685
Epoch 750, training loss: 438.1432189941406 = 1.0559650659561157 + 50.0 * 8.741744995117188
Epoch 750, val loss: 1.0576543807983398
Epoch 760, training loss: 438.0264587402344 = 1.0551880598068237 + 50.0 * 8.739425659179688
Epoch 760, val loss: 1.0569164752960205
Epoch 770, training loss: 438.0469970703125 = 1.054409146308899 + 50.0 * 8.739851951599121
Epoch 770, val loss: 1.0561763048171997
Epoch 780, training loss: 437.734130859375 = 1.053612232208252 + 50.0 * 8.733610153198242
Epoch 780, val loss: 1.0553816556930542
Epoch 790, training loss: 437.9400939941406 = 1.0528807640075684 + 50.0 * 8.737744331359863
Epoch 790, val loss: 1.0547025203704834
Epoch 800, training loss: 438.3992004394531 = 1.0521494150161743 + 50.0 * 8.746940612792969
Epoch 800, val loss: 1.054032325744629
Epoch 810, training loss: 438.52410888671875 = 1.0514028072357178 + 50.0 * 8.749454498291016
Epoch 810, val loss: 1.0533244609832764
Epoch 820, training loss: 438.63641357421875 = 1.0506147146224976 + 50.0 * 8.751715660095215
Epoch 820, val loss: 1.0525864362716675
Epoch 830, training loss: 438.8074645996094 = 1.0498276948928833 + 50.0 * 8.755152702331543
Epoch 830, val loss: 1.0518368482589722
Epoch 840, training loss: 438.695068359375 = 1.0490086078643799 + 50.0 * 8.752921104431152
Epoch 840, val loss: 1.051037311553955
Epoch 850, training loss: 438.83087158203125 = 1.0482062101364136 + 50.0 * 8.755653381347656
Epoch 850, val loss: 1.0502930879592896
Epoch 860, training loss: 439.0658874511719 = 1.0474451780319214 + 50.0 * 8.760368347167969
Epoch 860, val loss: 1.0495582818984985
Epoch 870, training loss: 438.9780578613281 = 1.0466151237487793 + 50.0 * 8.758628845214844
Epoch 870, val loss: 1.048782229423523
Epoch 880, training loss: 439.2777404785156 = 1.0458197593688965 + 50.0 * 8.764638900756836
Epoch 880, val loss: 1.048030138015747
Epoch 890, training loss: 439.4336242675781 = 1.0449734926223755 + 50.0 * 8.767772674560547
Epoch 890, val loss: 1.0472639799118042
Epoch 900, training loss: 439.6850280761719 = 1.0441474914550781 + 50.0 * 8.772817611694336
Epoch 900, val loss: 1.0464301109313965
Epoch 910, training loss: 439.5778503417969 = 1.0433015823364258 + 50.0 * 8.77069091796875
Epoch 910, val loss: 1.0456448793411255
Epoch 920, training loss: 439.62255859375 = 1.0424822568893433 + 50.0 * 8.771601676940918
Epoch 920, val loss: 1.044870376586914
Epoch 930, training loss: 439.6588439941406 = 1.0415292978286743 + 50.0 * 8.772346496582031
Epoch 930, val loss: 1.043938159942627
Epoch 940, training loss: 439.7644958496094 = 1.0407921075820923 + 50.0 * 8.774474143981934
Epoch 940, val loss: 1.0432847738265991
Epoch 950, training loss: 439.9041442871094 = 1.0399178266525269 + 50.0 * 8.777284622192383
Epoch 950, val loss: 1.042432188987732
Epoch 960, training loss: 440.0865783691406 = 1.039057970046997 + 50.0 * 8.780950546264648
Epoch 960, val loss: 1.041644811630249
Epoch 970, training loss: 440.2417907714844 = 1.0382144451141357 + 50.0 * 8.784071922302246
Epoch 970, val loss: 1.0408246517181396
Epoch 980, training loss: 440.4099426269531 = 1.0373584032058716 + 50.0 * 8.78745174407959
Epoch 980, val loss: 1.040036678314209
Epoch 990, training loss: 440.57904052734375 = 1.0365079641342163 + 50.0 * 8.790850639343262
Epoch 990, val loss: 1.0392155647277832
Epoch 1000, training loss: 440.81170654296875 = 1.0356189012527466 + 50.0 * 8.79552173614502
Epoch 1000, val loss: 1.0383846759796143
Epoch 1010, training loss: 440.9517822265625 = 1.0347107648849487 + 50.0 * 8.798341751098633
Epoch 1010, val loss: 1.0375062227249146
Epoch 1020, training loss: 440.7336730957031 = 1.033792495727539 + 50.0 * 8.793997764587402
Epoch 1020, val loss: 1.0366559028625488
Epoch 1030, training loss: 440.8760986328125 = 1.0329184532165527 + 50.0 * 8.796863555908203
Epoch 1030, val loss: 1.0358434915542603
Epoch 1040, training loss: 441.2105712890625 = 1.0320512056350708 + 50.0 * 8.803570747375488
Epoch 1040, val loss: 1.0350176095962524
Epoch 1050, training loss: 441.2370300292969 = 1.0311484336853027 + 50.0 * 8.804117202758789
Epoch 1050, val loss: 1.034152865409851
Epoch 1060, training loss: 441.3583984375 = 1.0301742553710938 + 50.0 * 8.806564331054688
Epoch 1060, val loss: 1.033227562904358
Epoch 1070, training loss: 441.3565979003906 = 1.0292296409606934 + 50.0 * 8.806547164916992
Epoch 1070, val loss: 1.032346487045288
Epoch 1080, training loss: 441.5025329589844 = 1.0283229351043701 + 50.0 * 8.809484481811523
Epoch 1080, val loss: 1.0314892530441284
Epoch 1090, training loss: 441.6905517578125 = 1.0274091958999634 + 50.0 * 8.813262939453125
Epoch 1090, val loss: 1.0306329727172852
Epoch 1100, training loss: 441.7873840332031 = 1.0264805555343628 + 50.0 * 8.815217971801758
Epoch 1100, val loss: 1.0297613143920898
Epoch 1110, training loss: 441.83819580078125 = 1.025514006614685 + 50.0 * 8.816253662109375
Epoch 1110, val loss: 1.0288602113723755
Epoch 1120, training loss: 441.0787048339844 = 1.0244613885879517 + 50.0 * 8.801084518432617
Epoch 1120, val loss: 1.0278280973434448
Epoch 1130, training loss: 442.0625915527344 = 1.0234671831130981 + 50.0 * 8.820782661437988
Epoch 1130, val loss: 1.0269492864608765
Epoch 1140, training loss: 442.1621398925781 = 1.0224965810775757 + 50.0 * 8.822793006896973
Epoch 1140, val loss: 1.026009202003479
Epoch 1150, training loss: 441.0906982421875 = 1.0213613510131836 + 50.0 * 8.801386833190918
Epoch 1150, val loss: 1.0249249935150146
Epoch 1160, training loss: 441.6450500488281 = 1.0205354690551758 + 50.0 * 8.812490463256836
Epoch 1160, val loss: 1.0241401195526123
Epoch 1170, training loss: 441.8246154785156 = 1.0195716619491577 + 50.0 * 8.81610107421875
Epoch 1170, val loss: 1.0232359170913696
Epoch 1180, training loss: 442.1630859375 = 1.0186229944229126 + 50.0 * 8.82288932800293
Epoch 1180, val loss: 1.0223573446273804
Epoch 1190, training loss: 442.419189453125 = 1.0176571607589722 + 50.0 * 8.828030586242676
Epoch 1190, val loss: 1.0214160680770874
Epoch 1200, training loss: 442.586669921875 = 1.0166724920272827 + 50.0 * 8.831399917602539
Epoch 1200, val loss: 1.0205000638961792
Epoch 1210, training loss: 442.7886962890625 = 1.0156646966934204 + 50.0 * 8.835460662841797
Epoch 1210, val loss: 1.0195420980453491
Epoch 1220, training loss: 442.842529296875 = 1.0146278142929077 + 50.0 * 8.83655834197998
Epoch 1220, val loss: 1.0185810327529907
Epoch 1230, training loss: 443.0294189453125 = 1.0135852098464966 + 50.0 * 8.840316772460938
Epoch 1230, val loss: 1.017586350440979
Epoch 1240, training loss: 443.17462158203125 = 1.0125483274459839 + 50.0 * 8.843241691589355
Epoch 1240, val loss: 1.0166174173355103
Epoch 1250, training loss: 443.1887512207031 = 1.0114383697509766 + 50.0 * 8.843545913696289
Epoch 1250, val loss: 1.0155599117279053
Epoch 1260, training loss: 443.1390686035156 = 1.0103713274002075 + 50.0 * 8.842574119567871
Epoch 1260, val loss: 1.014578938484192
Epoch 1270, training loss: 443.30401611328125 = 1.0093427896499634 + 50.0 * 8.845893859863281
Epoch 1270, val loss: 1.013597846031189
Epoch 1280, training loss: 443.56060791015625 = 1.0082520246505737 + 50.0 * 8.85104751586914
Epoch 1280, val loss: 1.0125720500946045
Epoch 1290, training loss: 443.5333251953125 = 1.0071520805358887 + 50.0 * 8.850523948669434
Epoch 1290, val loss: 1.0115410089492798
Epoch 1300, training loss: 443.6217041015625 = 1.0060312747955322 + 50.0 * 8.852313041687012
Epoch 1300, val loss: 1.0104738473892212
Epoch 1310, training loss: 443.79736328125 = 1.0049190521240234 + 50.0 * 8.855849266052246
Epoch 1310, val loss: 1.0094375610351562
Epoch 1320, training loss: 443.8812561035156 = 1.003807783126831 + 50.0 * 8.857548713684082
Epoch 1320, val loss: 1.0083847045898438
Epoch 1330, training loss: 443.9610900878906 = 1.002678632736206 + 50.0 * 8.85916805267334
Epoch 1330, val loss: 1.0073362588882446
Epoch 1340, training loss: 444.2089538574219 = 1.0015498399734497 + 50.0 * 8.864148139953613
Epoch 1340, val loss: 1.0062386989593506
Epoch 1350, training loss: 444.1731872558594 = 1.0003851652145386 + 50.0 * 8.863455772399902
Epoch 1350, val loss: 1.0051705837249756
Epoch 1360, training loss: 443.81231689453125 = 0.9991565346717834 + 50.0 * 8.856263160705566
Epoch 1360, val loss: 1.0040546655654907
Epoch 1370, training loss: 444.1261291503906 = 0.9980764389038086 + 50.0 * 8.862561225891113
Epoch 1370, val loss: 1.0030113458633423
Epoch 1380, training loss: 444.2407531738281 = 0.9969241619110107 + 50.0 * 8.864876747131348
Epoch 1380, val loss: 1.0019216537475586
Epoch 1390, training loss: 444.57159423828125 = 0.9957770705223083 + 50.0 * 8.871516227722168
Epoch 1390, val loss: 1.0008596181869507
Epoch 1400, training loss: 444.83306884765625 = 0.99461430311203 + 50.0 * 8.876769065856934
Epoch 1400, val loss: 0.9997511506080627
Epoch 1410, training loss: 444.7939453125 = 0.993411660194397 + 50.0 * 8.87601089477539
Epoch 1410, val loss: 0.9986292123794556
Epoch 1420, training loss: 444.82891845703125 = 0.9921959042549133 + 50.0 * 8.876734733581543
Epoch 1420, val loss: 0.9975027441978455
Epoch 1430, training loss: 444.9835205078125 = 0.9909772872924805 + 50.0 * 8.879851341247559
Epoch 1430, val loss: 0.9963531494140625
Epoch 1440, training loss: 445.190673828125 = 0.9897745847702026 + 50.0 * 8.884017944335938
Epoch 1440, val loss: 0.9952380657196045
Epoch 1450, training loss: 445.05780029296875 = 0.988524854183197 + 50.0 * 8.881385803222656
Epoch 1450, val loss: 0.9940354228019714
Epoch 1460, training loss: 445.2264709472656 = 0.9872798919677734 + 50.0 * 8.884783744812012
Epoch 1460, val loss: 0.9928805828094482
Epoch 1470, training loss: 445.3550109863281 = 0.9860219955444336 + 50.0 * 8.88737964630127
Epoch 1470, val loss: 0.9917176365852356
Epoch 1480, training loss: 445.5342712402344 = 0.9847591519355774 + 50.0 * 8.890990257263184
Epoch 1480, val loss: 0.9905229210853577
Epoch 1490, training loss: 445.5832214355469 = 0.9834588766098022 + 50.0 * 8.891995429992676
Epoch 1490, val loss: 0.9892956018447876
Epoch 1500, training loss: 445.7033996582031 = 0.982175886631012 + 50.0 * 8.894424438476562
Epoch 1500, val loss: 0.98811936378479
Epoch 1510, training loss: 445.68218994140625 = 0.9808677434921265 + 50.0 * 8.894026756286621
Epoch 1510, val loss: 0.9868941903114319
Epoch 1520, training loss: 445.6862487792969 = 0.9795153737068176 + 50.0 * 8.894134521484375
Epoch 1520, val loss: 0.9856477975845337
Epoch 1530, training loss: 445.90576171875 = 0.9782232046127319 + 50.0 * 8.898550987243652
Epoch 1530, val loss: 0.9844220876693726
Epoch 1540, training loss: 446.041015625 = 0.9768856763839722 + 50.0 * 8.90128231048584
Epoch 1540, val loss: 0.9831504225730896
Epoch 1550, training loss: 446.060546875 = 0.9754878878593445 + 50.0 * 8.901700973510742
Epoch 1550, val loss: 0.9818921685218811
Epoch 1560, training loss: 446.173095703125 = 0.9741144776344299 + 50.0 * 8.903979301452637
Epoch 1560, val loss: 0.9806050062179565
Epoch 1570, training loss: 446.0940856933594 = 0.972737193107605 + 50.0 * 8.902426719665527
Epoch 1570, val loss: 0.9793277382850647
Epoch 1580, training loss: 446.13116455078125 = 0.9713393449783325 + 50.0 * 8.903196334838867
Epoch 1580, val loss: 0.9780198931694031
Epoch 1590, training loss: 446.2735900878906 = 0.9699566960334778 + 50.0 * 8.906072616577148
Epoch 1590, val loss: 0.976707398891449
Epoch 1600, training loss: 446.45556640625 = 0.9685307145118713 + 50.0 * 8.909740447998047
Epoch 1600, val loss: 0.9754047393798828
Epoch 1610, training loss: 446.4676208496094 = 0.9670642018318176 + 50.0 * 8.910011291503906
Epoch 1610, val loss: 0.9740511178970337
Epoch 1620, training loss: 446.5101623535156 = 0.9656326174736023 + 50.0 * 8.910890579223633
Epoch 1620, val loss: 0.9726956486701965
Epoch 1630, training loss: 446.6737060546875 = 0.9641920328140259 + 50.0 * 8.914190292358398
Epoch 1630, val loss: 0.9713590741157532
Epoch 1640, training loss: 446.7022705078125 = 0.9627116322517395 + 50.0 * 8.914791107177734
Epoch 1640, val loss: 0.9699591398239136
Epoch 1650, training loss: 446.68408203125 = 0.9611913561820984 + 50.0 * 8.914458274841309
Epoch 1650, val loss: 0.9685465097427368
Epoch 1660, training loss: 446.5875549316406 = 0.9596318602561951 + 50.0 * 8.912558555603027
Epoch 1660, val loss: 0.9671065807342529
Epoch 1670, training loss: 446.8288879394531 = 0.9580950140953064 + 50.0 * 8.917415618896484
Epoch 1670, val loss: 0.9656617045402527
Epoch 1680, training loss: 446.9891052246094 = 0.956643283367157 + 50.0 * 8.920649528503418
Epoch 1680, val loss: 0.9643104076385498
Epoch 1690, training loss: 446.2488708496094 = 0.9549164175987244 + 50.0 * 8.905879020690918
Epoch 1690, val loss: 0.9627049565315247
Epoch 1700, training loss: 446.38629150390625 = 0.9534096717834473 + 50.0 * 8.908658027648926
Epoch 1700, val loss: 0.9613227248191833
Epoch 1710, training loss: 446.6466369628906 = 0.9519041776657104 + 50.0 * 8.913894653320312
Epoch 1710, val loss: 0.9599024057388306
Epoch 1720, training loss: 446.8985290527344 = 0.950410783290863 + 50.0 * 8.918962478637695
Epoch 1720, val loss: 0.9585110545158386
Epoch 1730, training loss: 447.1487731933594 = 0.948911726474762 + 50.0 * 8.923996925354004
Epoch 1730, val loss: 0.9571250081062317
Epoch 1740, training loss: 446.9022216796875 = 0.947338879108429 + 50.0 * 8.919097900390625
Epoch 1740, val loss: 0.9556459784507751
Epoch 1750, training loss: 447.1737060546875 = 0.9457805752754211 + 50.0 * 8.924558639526367
Epoch 1750, val loss: 0.9542109370231628
Epoch 1760, training loss: 447.28692626953125 = 0.9442052245140076 + 50.0 * 8.926854133605957
Epoch 1760, val loss: 0.9527420997619629
Epoch 1770, training loss: 447.2547302246094 = 0.942608118057251 + 50.0 * 8.92624282836914
Epoch 1770, val loss: 0.9512270092964172
Epoch 1780, training loss: 447.1134033203125 = 0.9409724473953247 + 50.0 * 8.92344856262207
Epoch 1780, val loss: 0.949703574180603
Epoch 1790, training loss: 447.3262634277344 = 0.9393728971481323 + 50.0 * 8.927738189697266
Epoch 1790, val loss: 0.9482486248016357
Epoch 1800, training loss: 447.5313720703125 = 0.937813401222229 + 50.0 * 8.93187141418457
Epoch 1800, val loss: 0.9467690587043762
Epoch 1810, training loss: 447.4058837890625 = 0.9361467361450195 + 50.0 * 8.929394721984863
Epoch 1810, val loss: 0.9452078342437744
Epoch 1820, training loss: 447.4922180175781 = 0.9344971179962158 + 50.0 * 8.931154251098633
Epoch 1820, val loss: 0.943670928478241
Epoch 1830, training loss: 447.6965637207031 = 0.9329047799110413 + 50.0 * 8.935273170471191
Epoch 1830, val loss: 0.9421501755714417
Epoch 1840, training loss: 447.8299865722656 = 0.931261420249939 + 50.0 * 8.937973976135254
Epoch 1840, val loss: 0.9406349062919617
Epoch 1850, training loss: 447.7348937988281 = 0.9296019077301025 + 50.0 * 8.936105728149414
Epoch 1850, val loss: 0.9390532374382019
Epoch 1860, training loss: 447.83074951171875 = 0.9279448986053467 + 50.0 * 8.938055992126465
Epoch 1860, val loss: 0.9375019073486328
Epoch 1870, training loss: 448.0253601074219 = 0.9263125061988831 + 50.0 * 8.941981315612793
Epoch 1870, val loss: 0.9359891414642334
Epoch 1880, training loss: 448.01861572265625 = 0.9246423840522766 + 50.0 * 8.941879272460938
Epoch 1880, val loss: 0.9344303011894226
Epoch 1890, training loss: 448.12152099609375 = 0.9229735732078552 + 50.0 * 8.943970680236816
Epoch 1890, val loss: 0.9328304529190063
Epoch 1900, training loss: 448.2222900390625 = 0.921284556388855 + 50.0 * 8.946020126342773
Epoch 1900, val loss: 0.93126380443573
Epoch 1910, training loss: 448.326904296875 = 0.9196032881736755 + 50.0 * 8.948145866394043
Epoch 1910, val loss: 0.9297175407409668
Epoch 1920, training loss: 448.224365234375 = 0.9178921580314636 + 50.0 * 8.94612979888916
Epoch 1920, val loss: 0.9280865788459778
Epoch 1930, training loss: 447.2010192871094 = 0.9160555601119995 + 50.0 * 8.925699234008789
Epoch 1930, val loss: 0.9262642860412598
Epoch 1940, training loss: 446.5917663574219 = 0.9142550826072693 + 50.0 * 8.91355037689209
Epoch 1940, val loss: 0.9247686266899109
Epoch 1950, training loss: 446.96478271484375 = 0.9126705527305603 + 50.0 * 8.921042442321777
Epoch 1950, val loss: 0.923262894153595
Epoch 1960, training loss: 447.06878662109375 = 0.9109595417976379 + 50.0 * 8.92315673828125
Epoch 1960, val loss: 0.9216482043266296
Epoch 1970, training loss: 447.42767333984375 = 0.9092962741851807 + 50.0 * 8.930367469787598
Epoch 1970, val loss: 0.9200826287269592
Epoch 1980, training loss: 447.8688049316406 = 0.9076410531997681 + 50.0 * 8.939223289489746
Epoch 1980, val loss: 0.9185348153114319
Epoch 1990, training loss: 448.2254638671875 = 0.9059557318687439 + 50.0 * 8.946390151977539
Epoch 1990, val loss: 0.9169670939445496
Epoch 2000, training loss: 448.27471923828125 = 0.9041877388954163 + 50.0 * 8.947410583496094
Epoch 2000, val loss: 0.9153301119804382
Epoch 2010, training loss: 448.43115234375 = 0.9024777412414551 + 50.0 * 8.950573921203613
Epoch 2010, val loss: 0.9137033224105835
Epoch 2020, training loss: 448.4929504394531 = 0.900695264339447 + 50.0 * 8.951845169067383
Epoch 2020, val loss: 0.9120528697967529
Epoch 2030, training loss: 448.51568603515625 = 0.8989142775535583 + 50.0 * 8.952335357666016
Epoch 2030, val loss: 0.9103766679763794
Epoch 2040, training loss: 448.66876220703125 = 0.8971357345581055 + 50.0 * 8.955432891845703
Epoch 2040, val loss: 0.9087306261062622
Epoch 2050, training loss: 448.7135314941406 = 0.8953452706336975 + 50.0 * 8.956363677978516
Epoch 2050, val loss: 0.9070426225662231
Epoch 2060, training loss: 448.76715087890625 = 0.8935126066207886 + 50.0 * 8.957472801208496
Epoch 2060, val loss: 0.9053466320037842
Epoch 2070, training loss: 448.8505554199219 = 0.8916871547698975 + 50.0 * 8.959177017211914
Epoch 2070, val loss: 0.9036303162574768
Epoch 2080, training loss: 448.8974914550781 = 0.8898118734359741 + 50.0 * 8.960153579711914
Epoch 2080, val loss: 0.9018955230712891
Epoch 2090, training loss: 448.76617431640625 = 0.8879192471504211 + 50.0 * 8.957565307617188
Epoch 2090, val loss: 0.900094747543335
Epoch 2100, training loss: 448.9928894042969 = 0.8860093355178833 + 50.0 * 8.962137222290039
Epoch 2100, val loss: 0.8983152508735657
Epoch 2110, training loss: 449.036376953125 = 0.8840386271476746 + 50.0 * 8.96304702758789
Epoch 2110, val loss: 0.8964822292327881
Epoch 2120, training loss: 449.0665588378906 = 0.8820852637290955 + 50.0 * 8.963689804077148
Epoch 2120, val loss: 0.8946626782417297
Epoch 2130, training loss: 449.2001953125 = 0.8801369667053223 + 50.0 * 8.966401100158691
Epoch 2130, val loss: 0.8928175568580627
Epoch 2140, training loss: 446.3721923828125 = 0.8779733777046204 + 50.0 * 8.909884452819824
Epoch 2140, val loss: 0.8909863829612732
Epoch 2150, training loss: 447.04931640625 = 0.8768168091773987 + 50.0 * 8.923449516296387
Epoch 2150, val loss: 0.8897256851196289
Epoch 2160, training loss: 448.0798645019531 = 0.875114917755127 + 50.0 * 8.94409465789795
Epoch 2160, val loss: 0.8881564140319824
Epoch 2170, training loss: 447.7115783691406 = 0.8730981349945068 + 50.0 * 8.936769485473633
Epoch 2170, val loss: 0.8862600922584534
Epoch 2180, training loss: 448.4345703125 = 0.8712418675422668 + 50.0 * 8.951266288757324
Epoch 2180, val loss: 0.8845646977424622
Epoch 2190, training loss: 448.66888427734375 = 0.8693906664848328 + 50.0 * 8.955989837646484
Epoch 2190, val loss: 0.8828286528587341
Epoch 2200, training loss: 449.07659912109375 = 0.8675588965415955 + 50.0 * 8.964180946350098
Epoch 2200, val loss: 0.8811124563217163
Epoch 2210, training loss: 449.2571105957031 = 0.8656565546989441 + 50.0 * 8.967828750610352
Epoch 2210, val loss: 0.8793442249298096
Epoch 2220, training loss: 449.4040832519531 = 0.8637374639511108 + 50.0 * 8.970807075500488
Epoch 2220, val loss: 0.8775338530540466
Epoch 2230, training loss: 449.4789123535156 = 0.8617891669273376 + 50.0 * 8.972342491149902
Epoch 2230, val loss: 0.8757101893424988
Epoch 2240, training loss: 449.5399169921875 = 0.85984867811203 + 50.0 * 8.973601341247559
Epoch 2240, val loss: 0.8738895654678345
Epoch 2250, training loss: 449.601806640625 = 0.8578447699546814 + 50.0 * 8.974879264831543
Epoch 2250, val loss: 0.872016966342926
Epoch 2260, training loss: 449.6771240234375 = 0.8558770418167114 + 50.0 * 8.976425170898438
Epoch 2260, val loss: 0.8701715469360352
Epoch 2270, training loss: 449.7369079589844 = 0.8539005517959595 + 50.0 * 8.977660179138184
Epoch 2270, val loss: 0.8683179616928101
Epoch 2280, training loss: 449.6891174316406 = 0.8518986105918884 + 50.0 * 8.976744651794434
Epoch 2280, val loss: 0.8664586544036865
Epoch 2290, training loss: 449.8271484375 = 0.8499061465263367 + 50.0 * 8.979544639587402
Epoch 2290, val loss: 0.8646164536476135
Epoch 2300, training loss: 449.9905700683594 = 0.8479476571083069 + 50.0 * 8.9828519821167
Epoch 2300, val loss: 0.8627537488937378
Epoch 2310, training loss: 450.0134582519531 = 0.8459208607673645 + 50.0 * 8.98335075378418
Epoch 2310, val loss: 0.8608835935592651
Epoch 2320, training loss: 450.0105285644531 = 0.8439146876335144 + 50.0 * 8.983332633972168
Epoch 2320, val loss: 0.8590033054351807
Epoch 2330, training loss: 449.30426025390625 = 0.8420049548149109 + 50.0 * 8.969244956970215
Epoch 2330, val loss: 0.8571667075157166
Epoch 2340, training loss: 448.47772216796875 = 0.8398639559745789 + 50.0 * 8.952756881713867
Epoch 2340, val loss: 0.8551671504974365
Epoch 2350, training loss: 449.2536315917969 = 0.8378259539604187 + 50.0 * 8.968316078186035
Epoch 2350, val loss: 0.8532655835151672
Epoch 2360, training loss: 449.5140075683594 = 0.8358599543571472 + 50.0 * 8.973563194274902
Epoch 2360, val loss: 0.8514739871025085
Epoch 2370, training loss: 449.03521728515625 = 0.8337182998657227 + 50.0 * 8.964030265808105
Epoch 2370, val loss: 0.849506676197052
Epoch 2380, training loss: 449.4441833496094 = 0.8317785859107971 + 50.0 * 8.972248077392578
Epoch 2380, val loss: 0.8477031588554382
Epoch 2390, training loss: 449.8607482910156 = 0.8298238515853882 + 50.0 * 8.980618476867676
Epoch 2390, val loss: 0.8458849191665649
Epoch 2400, training loss: 450.1654052734375 = 0.8278298377990723 + 50.0 * 8.986751556396484
Epoch 2400, val loss: 0.8440279364585876
Epoch 2410, training loss: 450.3009033203125 = 0.8258191347122192 + 50.0 * 8.989501953125
Epoch 2410, val loss: 0.8421531915664673
Epoch 2420, training loss: 450.30670166015625 = 0.8237816691398621 + 50.0 * 8.98965835571289
Epoch 2420, val loss: 0.8402588963508606
Epoch 2430, training loss: 450.4776916503906 = 0.8217742443084717 + 50.0 * 8.993118286132812
Epoch 2430, val loss: 0.8383854627609253
Epoch 2440, training loss: 450.5552673339844 = 0.8197524547576904 + 50.0 * 8.994709968566895
Epoch 2440, val loss: 0.8365031480789185
Epoch 2450, training loss: 450.723388671875 = 0.8177319765090942 + 50.0 * 8.998113632202148
Epoch 2450, val loss: 0.8346123695373535
Epoch 2460, training loss: 450.8179931640625 = 0.815692126750946 + 50.0 * 9.000045776367188
Epoch 2460, val loss: 0.8327429890632629
Epoch 2470, training loss: 450.58734130859375 = 0.8137491345405579 + 50.0 * 8.995471954345703
Epoch 2470, val loss: 0.8308761119842529
Epoch 2480, training loss: 450.1072998046875 = 0.8116824626922607 + 50.0 * 8.985912322998047
Epoch 2480, val loss: 0.8290208578109741
Epoch 2490, training loss: 450.2095947265625 = 0.809689462184906 + 50.0 * 8.987998008728027
Epoch 2490, val loss: 0.8271703124046326
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7711594202898551
0.8636528290951243
The final CL Acc:0.68831, 0.12347, The final GNN Acc:0.86375, 0.00019
Begin epxeriment: noisy_level: 0.2 cont_weight: 50 epoch: der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106306])
remove edge: torch.Size([2, 70736])
updated graph: torch.Size([2, 88394])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:227: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 516.012939453125 = 1.0986114740371704 + 50.0 * 10.298286437988281
Epoch 0, val loss: 1.098612904548645
Epoch 10, training loss: 497.8580627441406 = 1.0986114740371704 + 50.0 * 9.935189247131348
Epoch 10, val loss: 1.098612904548645
Epoch 20, training loss: 488.9534606933594 = 1.0986114740371704 + 50.0 * 9.757097244262695
Epoch 20, val loss: 1.098612904548645
Epoch 30, training loss: 482.08929443359375 = 1.0986114740371704 + 50.0 * 9.619813919067383
Epoch 30, val loss: 1.098612904548645
Epoch 40, training loss: 476.7105407714844 = 1.0986114740371704 + 50.0 * 9.512238502502441
Epoch 40, val loss: 1.098612904548645
Epoch 50, training loss: 472.2547607421875 = 1.0986114740371704 + 50.0 * 9.423123359680176
Epoch 50, val loss: 1.098612904548645
Epoch 60, training loss: 468.4681701660156 = 1.0986114740371704 + 50.0 * 9.347391128540039
Epoch 60, val loss: 1.098612904548645
Epoch 70, training loss: 465.2372131347656 = 1.0986114740371704 + 50.0 * 9.282772064208984
Epoch 70, val loss: 1.098612904548645
Epoch 80, training loss: 462.3866882324219 = 1.0986114740371704 + 50.0 * 9.225761413574219
Epoch 80, val loss: 1.098612904548645
Epoch 90, training loss: 459.8746032714844 = 1.0986114740371704 + 50.0 * 9.175519943237305
Epoch 90, val loss: 1.098612904548645
Epoch 100, training loss: 457.6717529296875 = 1.0986114740371704 + 50.0 * 9.131463050842285
Epoch 100, val loss: 1.098612904548645
Epoch 110, training loss: 455.7141418457031 = 1.0986114740371704 + 50.0 * 9.092310905456543
Epoch 110, val loss: 1.098612904548645
Epoch 120, training loss: 454.7702941894531 = 1.0986114740371704 + 50.0 * 9.073433876037598
Epoch 120, val loss: 1.098612904548645
Epoch 130, training loss: 452.9943542480469 = 1.0986114740371704 + 50.0 * 9.037915229797363
Epoch 130, val loss: 1.098612904548645
Epoch 140, training loss: 451.6372375488281 = 1.0986114740371704 + 50.0 * 9.010772705078125
Epoch 140, val loss: 1.098612904548645
Epoch 150, training loss: 450.5085754394531 = 1.0986114740371704 + 50.0 * 8.988199234008789
Epoch 150, val loss: 1.098612904548645
Epoch 160, training loss: 449.5950927734375 = 1.0986114740371704 + 50.0 * 8.969929695129395
Epoch 160, val loss: 1.098612904548645
Epoch 170, training loss: 449.4292297363281 = 1.0986114740371704 + 50.0 * 8.966612815856934
Epoch 170, val loss: 1.098612904548645
Epoch 180, training loss: 448.26214599609375 = 1.0986114740371704 + 50.0 * 8.943270683288574
Epoch 180, val loss: 1.098612904548645
Epoch 190, training loss: 447.6248474121094 = 1.0986114740371704 + 50.0 * 8.930524826049805
Epoch 190, val loss: 1.098612904548645
Epoch 200, training loss: 447.042724609375 = 1.0986114740371704 + 50.0 * 8.918882369995117
Epoch 200, val loss: 1.098612904548645
Epoch 210, training loss: 446.7281494140625 = 1.0986114740371704 + 50.0 * 8.912590980529785
Epoch 210, val loss: 1.098612904548645
Epoch 220, training loss: 446.2533264160156 = 1.0986114740371704 + 50.0 * 8.903094291687012
Epoch 220, val loss: 1.098612904548645
Epoch 230, training loss: 446.0762634277344 = 1.0986114740371704 + 50.0 * 8.899553298950195
Epoch 230, val loss: 1.098612904548645
Epoch 240, training loss: 445.72528076171875 = 1.0986114740371704 + 50.0 * 8.892533302307129
Epoch 240, val loss: 1.098612904548645
Epoch 250, training loss: 445.37518310546875 = 1.0986114740371704 + 50.0 * 8.885531425476074
Epoch 250, val loss: 1.098612904548645
Epoch 260, training loss: 445.15521240234375 = 1.0986114740371704 + 50.0 * 8.881132125854492
Epoch 260, val loss: 1.098612904548645
Epoch 270, training loss: 445.0697937011719 = 1.0986114740371704 + 50.0 * 8.879424095153809
Epoch 270, val loss: 1.098612904548645
Epoch 280, training loss: 444.9181823730469 = 1.0986114740371704 + 50.0 * 8.876391410827637
Epoch 280, val loss: 1.098612904548645
Epoch 290, training loss: 444.9096374511719 = 1.0986114740371704 + 50.0 * 8.876220703125
Epoch 290, val loss: 1.098612904548645
Epoch 300, training loss: 444.4371337890625 = 1.0986114740371704 + 50.0 * 8.86677074432373
Epoch 300, val loss: 1.098612904548645
Epoch 310, training loss: 444.493408203125 = 1.0986114740371704 + 50.0 * 8.86789608001709
Epoch 310, val loss: 1.098612904548645
Epoch 320, training loss: 444.5771484375 = 1.0986114740371704 + 50.0 * 8.8695707321167
Epoch 320, val loss: 1.098612904548645
Epoch 330, training loss: 444.8828430175781 = 1.0986114740371704 + 50.0 * 8.87568473815918
Epoch 330, val loss: 1.098612904548645
Epoch 340, training loss: 444.1272277832031 = 1.0986114740371704 + 50.0 * 8.860572814941406
Epoch 340, val loss: 1.098612904548645
Epoch 350, training loss: 444.60540771484375 = 1.0986114740371704 + 50.0 * 8.870136260986328
Epoch 350, val loss: 1.098612904548645
Epoch 360, training loss: 444.4819030761719 = 1.0986114740371704 + 50.0 * 8.867666244506836
Epoch 360, val loss: 1.098612904548645
Epoch 370, training loss: 444.2470703125 = 1.0986114740371704 + 50.0 * 8.862969398498535
Epoch 370, val loss: 1.098612904548645
Epoch 380, training loss: 444.35760498046875 = 1.0986114740371704 + 50.0 * 8.865180015563965
Epoch 380, val loss: 1.0986127853393555
Epoch 390, training loss: 444.3439636230469 = 1.0986114740371704 + 50.0 * 8.864907264709473
Epoch 390, val loss: 1.098612666130066
Epoch 400, training loss: 444.38946533203125 = 1.0986114740371704 + 50.0 * 8.865817070007324
Epoch 400, val loss: 1.098612904548645
Epoch 410, training loss: 444.4390869140625 = 1.0986114740371704 + 50.0 * 8.866809844970703
Epoch 410, val loss: 1.098612904548645
Epoch 420, training loss: 444.3031005859375 = 1.0986114740371704 + 50.0 * 8.864089965820312
Epoch 420, val loss: 1.098612904548645
Epoch 430, training loss: 444.5539245605469 = 1.0986114740371704 + 50.0 * 8.86910629272461
Epoch 430, val loss: 1.098612904548645
Epoch 440, training loss: 444.7193908691406 = 1.0986114740371704 + 50.0 * 8.872415542602539
Epoch 440, val loss: 1.098612904548645
Epoch 450, training loss: 444.7888488769531 = 1.0986114740371704 + 50.0 * 8.873805046081543
Epoch 450, val loss: 1.098612904548645
Epoch 460, training loss: 444.77044677734375 = 1.0986114740371704 + 50.0 * 8.87343692779541
Epoch 460, val loss: 1.098612904548645
Epoch 470, training loss: 444.8595275878906 = 1.0986114740371704 + 50.0 * 8.875218391418457
Epoch 470, val loss: 1.098612904548645
Epoch 480, training loss: 444.920166015625 = 1.0986114740371704 + 50.0 * 8.876431465148926
Epoch 480, val loss: 1.098612904548645
Epoch 490, training loss: 445.110107421875 = 1.0986114740371704 + 50.0 * 8.880229949951172
Epoch 490, val loss: 1.098612904548645
Epoch 500, training loss: 445.00042724609375 = 1.0986114740371704 + 50.0 * 8.878036499023438
Epoch 500, val loss: 1.098612904548645
Epoch 510, training loss: 445.26153564453125 = 1.0986114740371704 + 50.0 * 8.883258819580078
Epoch 510, val loss: 1.098612904548645
Epoch 520, training loss: 445.14337158203125 = 1.0986114740371704 + 50.0 * 8.880895614624023
Epoch 520, val loss: 1.098612904548645
Epoch 530, training loss: 445.189453125 = 1.0986114740371704 + 50.0 * 8.881816864013672
Epoch 530, val loss: 1.098612904548645
Epoch 540, training loss: 445.29180908203125 = 1.0986114740371704 + 50.0 * 8.883864402770996
Epoch 540, val loss: 1.098612904548645
Epoch 550, training loss: 445.4691467285156 = 1.0986114740371704 + 50.0 * 8.887411117553711
Epoch 550, val loss: 1.098612904548645
Epoch 560, training loss: 445.5279541015625 = 1.0986114740371704 + 50.0 * 8.88858699798584
Epoch 560, val loss: 1.098612904548645
Epoch 570, training loss: 445.5907897949219 = 1.0986114740371704 + 50.0 * 8.889843940734863
Epoch 570, val loss: 1.098612904548645
Epoch 580, training loss: 445.6532287597656 = 1.0986114740371704 + 50.0 * 8.891092300415039
Epoch 580, val loss: 1.098612904548645
Epoch 590, training loss: 445.8353576660156 = 1.0986114740371704 + 50.0 * 8.894735336303711
Epoch 590, val loss: 1.098612904548645
Epoch 600, training loss: 445.85125732421875 = 1.0986114740371704 + 50.0 * 8.895052909851074
Epoch 600, val loss: 1.098612904548645
Epoch 610, training loss: 446.1966552734375 = 1.0986114740371704 + 50.0 * 8.901961326599121
Epoch 610, val loss: 1.098612904548645
Epoch 620, training loss: 446.8533630371094 = 1.0986114740371704 + 50.0 * 8.915095329284668
Epoch 620, val loss: 1.098612904548645
Epoch 630, training loss: 445.7911682128906 = 1.0986114740371704 + 50.0 * 8.893851280212402
Epoch 630, val loss: 1.098612904548645
Epoch 640, training loss: 446.2154235839844 = 1.0986114740371704 + 50.0 * 8.902336120605469
Epoch 640, val loss: 1.098612904548645
Epoch 650, training loss: 445.9954833984375 = 1.0986114740371704 + 50.0 * 8.897937774658203
Epoch 650, val loss: 1.098612904548645
Epoch 660, training loss: 445.69061279296875 = 1.0986114740371704 + 50.0 * 8.891839981079102
Epoch 660, val loss: 1.098612904548645
Epoch 670, training loss: 446.23394775390625 = 1.0986114740371704 + 50.0 * 8.90270709991455
Epoch 670, val loss: 1.098612904548645
Epoch 680, training loss: 446.6070861816406 = 1.0986114740371704 + 50.0 * 8.91016960144043
Epoch 680, val loss: 1.098612904548645
Epoch 690, training loss: 446.7208251953125 = 1.0984177589416504 + 50.0 * 8.912447929382324
Epoch 690, val loss: 1.098332166671753
Epoch 700, training loss: 446.9486999511719 = 1.0979381799697876 + 50.0 * 8.917015075683594
Epoch 700, val loss: 1.0977660417556763
Epoch 710, training loss: 446.79803466796875 = 1.0974310636520386 + 50.0 * 8.91401195526123
Epoch 710, val loss: 1.0971863269805908
Epoch 720, training loss: 447.0323486328125 = 1.0969374179840088 + 50.0 * 8.918708801269531
Epoch 720, val loss: 1.09662926197052
Epoch 730, training loss: 447.0006103515625 = 1.0964499711990356 + 50.0 * 8.918083190917969
Epoch 730, val loss: 1.096085548400879
Epoch 740, training loss: 446.6844482421875 = 1.09595787525177 + 50.0 * 8.91176986694336
Epoch 740, val loss: 1.095542311668396
Epoch 750, training loss: 446.887451171875 = 1.0954500436782837 + 50.0 * 8.915840148925781
Epoch 750, val loss: 1.094984531402588
Epoch 760, training loss: 447.3452453613281 = 1.09491765499115 + 50.0 * 8.925006866455078
Epoch 760, val loss: 1.09441077709198
Epoch 770, training loss: 447.4674987792969 = 1.094366431236267 + 50.0 * 8.927462577819824
Epoch 770, val loss: 1.0938252210617065
Epoch 780, training loss: 447.73992919921875 = 1.0938115119934082 + 50.0 * 8.93292236328125
Epoch 780, val loss: 1.0932420492172241
Epoch 790, training loss: 447.7867126464844 = 1.0932420492172241 + 50.0 * 8.933869361877441
Epoch 790, val loss: 1.092646598815918
Epoch 800, training loss: 447.78802490234375 = 1.0926532745361328 + 50.0 * 8.933907508850098
Epoch 800, val loss: 1.0920366048812866
Epoch 810, training loss: 447.9433898925781 = 1.0920459032058716 + 50.0 * 8.937026977539062
Epoch 810, val loss: 1.091410517692566
Epoch 820, training loss: 448.1700439453125 = 1.0914241075515747 + 50.0 * 8.941572189331055
Epoch 820, val loss: 1.090774655342102
Epoch 830, training loss: 448.50958251953125 = 1.0907930135726929 + 50.0 * 8.948375701904297
Epoch 830, val loss: 1.0901367664337158
Epoch 840, training loss: 448.55712890625 = 1.0901424884796143 + 50.0 * 8.949339866638184
Epoch 840, val loss: 1.0894832611083984
Epoch 850, training loss: 448.7450256347656 = 1.0894739627838135 + 50.0 * 8.953110694885254
Epoch 850, val loss: 1.0888171195983887
Epoch 860, training loss: 449.0835876464844 = 1.0888237953186035 + 50.0 * 8.959895133972168
Epoch 860, val loss: 1.0881694555282593
Epoch 870, training loss: 449.0489807128906 = 1.0881414413452148 + 50.0 * 8.959217071533203
Epoch 870, val loss: 1.087498426437378
Epoch 880, training loss: 448.91546630859375 = 1.0874625444412231 + 50.0 * 8.956560134887695
Epoch 880, val loss: 1.0868198871612549
Epoch 890, training loss: 448.97967529296875 = 1.086774230003357 + 50.0 * 8.957858085632324
Epoch 890, val loss: 1.0861424207687378
Epoch 900, training loss: 449.3623962402344 = 1.086074709892273 + 50.0 * 8.965526580810547
Epoch 900, val loss: 1.0854527950286865
Epoch 910, training loss: 449.2845764160156 = 1.0853707790374756 + 50.0 * 8.963984489440918
Epoch 910, val loss: 1.084769368171692
Epoch 920, training loss: 449.5059814453125 = 1.0846621990203857 + 50.0 * 8.968426704406738
Epoch 920, val loss: 1.0840767621994019
Epoch 930, training loss: 449.7002258300781 = 1.0839507579803467 + 50.0 * 8.972325325012207
Epoch 930, val loss: 1.083369493484497
Epoch 940, training loss: 449.484375 = 1.083214282989502 + 50.0 * 8.968023300170898
Epoch 940, val loss: 1.0826548337936401
Epoch 950, training loss: 449.7313232421875 = 1.0824930667877197 + 50.0 * 8.972976684570312
Epoch 950, val loss: 1.0819469690322876
Epoch 960, training loss: 450.0530700683594 = 1.0817627906799316 + 50.0 * 8.979426383972168
Epoch 960, val loss: 1.081237554550171
Epoch 970, training loss: 450.1036071777344 = 1.0810329914093018 + 50.0 * 8.980451583862305
Epoch 970, val loss: 1.080528736114502
Epoch 980, training loss: 450.00030517578125 = 1.0803070068359375 + 50.0 * 8.978400230407715
Epoch 980, val loss: 1.0798269510269165
Epoch 990, training loss: 450.2239074707031 = 1.0795780420303345 + 50.0 * 8.98288631439209
Epoch 990, val loss: 1.0791255235671997
Epoch 1000, training loss: 450.39837646484375 = 1.0788544416427612 + 50.0 * 8.986390113830566
Epoch 1000, val loss: 1.0784190893173218
Epoch 1010, training loss: 450.3767395019531 = 1.078125238418579 + 50.0 * 8.98597240447998
Epoch 1010, val loss: 1.0777218341827393
Epoch 1020, training loss: 450.68353271484375 = 1.0774034261703491 + 50.0 * 8.992122650146484
Epoch 1020, val loss: 1.0770334005355835
Epoch 1030, training loss: 450.80419921875 = 1.0766711235046387 + 50.0 * 8.994550704956055
Epoch 1030, val loss: 1.076330542564392
Epoch 1040, training loss: 450.8199157714844 = 1.0759363174438477 + 50.0 * 8.994879722595215
Epoch 1040, val loss: 1.0756208896636963
Epoch 1050, training loss: 450.9827880859375 = 1.0752053260803223 + 50.0 * 8.998151779174805
Epoch 1050, val loss: 1.0749222040176392
Epoch 1060, training loss: 451.0751647949219 = 1.0744609832763672 + 50.0 * 9.000014305114746
Epoch 1060, val loss: 1.0742055177688599
Epoch 1070, training loss: 451.1718444824219 = 1.0737122297286987 + 50.0 * 9.001962661743164
Epoch 1070, val loss: 1.0734978914260864
Epoch 1080, training loss: 451.3616027832031 = 1.072950005531311 + 50.0 * 9.005773544311523
Epoch 1080, val loss: 1.072765827178955
Epoch 1090, training loss: 450.81658935546875 = 1.0721830129623413 + 50.0 * 8.994888305664062
Epoch 1090, val loss: 1.0720282793045044
Epoch 1100, training loss: 451.07550048828125 = 1.0714333057403564 + 50.0 * 9.000081062316895
Epoch 1100, val loss: 1.0713170766830444
Epoch 1110, training loss: 451.1955261230469 = 1.0706604719161987 + 50.0 * 9.002497673034668
Epoch 1110, val loss: 1.0705897808074951
Epoch 1120, training loss: 451.4387512207031 = 1.0699048042297363 + 50.0 * 9.007376670837402
Epoch 1120, val loss: 1.06987464427948
Epoch 1130, training loss: 451.7290344238281 = 1.069148063659668 + 50.0 * 9.013197898864746
Epoch 1130, val loss: 1.0691412687301636
Epoch 1140, training loss: 451.4603271484375 = 1.068352222442627 + 50.0 * 9.00783920288086
Epoch 1140, val loss: 1.0683928728103638
Epoch 1150, training loss: 451.21282958984375 = 1.0675830841064453 + 50.0 * 9.002904891967773
Epoch 1150, val loss: 1.0676355361938477
Epoch 1160, training loss: 451.4361572265625 = 1.0668201446533203 + 50.0 * 9.007386207580566
Epoch 1160, val loss: 1.0669142007827759
Epoch 1170, training loss: 451.8013916015625 = 1.0660698413848877 + 50.0 * 9.0147066116333
Epoch 1170, val loss: 1.066196322441101
Epoch 1180, training loss: 451.6583557128906 = 1.0652889013290405 + 50.0 * 9.011861801147461
Epoch 1180, val loss: 1.0654441118240356
Epoch 1190, training loss: 451.76953125 = 1.0644989013671875 + 50.0 * 9.014100074768066
Epoch 1190, val loss: 1.064690351486206
Epoch 1200, training loss: 451.8735656738281 = 1.0637452602386475 + 50.0 * 9.016196250915527
Epoch 1200, val loss: 1.0639787912368774
Epoch 1210, training loss: 452.2333984375 = 1.0629878044128418 + 50.0 * 9.023407936096191
Epoch 1210, val loss: 1.0632579326629639
Epoch 1220, training loss: 452.26837158203125 = 1.0622121095657349 + 50.0 * 9.024123191833496
Epoch 1220, val loss: 1.062519907951355
Epoch 1230, training loss: 452.4355773925781 = 1.061450719833374 + 50.0 * 9.027482032775879
Epoch 1230, val loss: 1.0617884397506714
Epoch 1240, training loss: 452.52044677734375 = 1.0606688261032104 + 50.0 * 9.029195785522461
Epoch 1240, val loss: 1.0610488653182983
Epoch 1250, training loss: 452.8717956542969 = 1.0599167346954346 + 50.0 * 9.036237716674805
Epoch 1250, val loss: 1.060319185256958
Epoch 1260, training loss: 452.8290710449219 = 1.0591390132904053 + 50.0 * 9.035398483276367
Epoch 1260, val loss: 1.059588074684143
Epoch 1270, training loss: 452.8583679199219 = 1.0583540201187134 + 50.0 * 9.03600025177002
Epoch 1270, val loss: 1.0588475465774536
Epoch 1280, training loss: 452.94329833984375 = 1.057586431503296 + 50.0 * 9.037714004516602
Epoch 1280, val loss: 1.0581097602844238
Epoch 1290, training loss: 453.015869140625 = 1.0568045377731323 + 50.0 * 9.03918170928955
Epoch 1290, val loss: 1.0573675632476807
Epoch 1300, training loss: 453.0794372558594 = 1.0560336112976074 + 50.0 * 9.040468215942383
Epoch 1300, val loss: 1.056641936302185
Epoch 1310, training loss: 452.98211669921875 = 1.0552624464035034 + 50.0 * 9.03853702545166
Epoch 1310, val loss: 1.0559161901474
Epoch 1320, training loss: 453.2066650390625 = 1.0545036792755127 + 50.0 * 9.04304313659668
Epoch 1320, val loss: 1.0552068948745728
Epoch 1330, training loss: 453.397216796875 = 1.0537375211715698 + 50.0 * 9.046869277954102
Epoch 1330, val loss: 1.0544668436050415
Epoch 1340, training loss: 453.44573974609375 = 1.0529643297195435 + 50.0 * 9.047855377197266
Epoch 1340, val loss: 1.05375075340271
Epoch 1350, training loss: 453.2179870605469 = 1.0521721839904785 + 50.0 * 9.043315887451172
Epoch 1350, val loss: 1.0530081987380981
Epoch 1360, training loss: 452.216552734375 = 1.051147699356079 + 50.0 * 9.023307800292969
Epoch 1360, val loss: 1.0520656108856201
Epoch 1370, training loss: 452.545654296875 = 1.0504900217056274 + 50.0 * 9.029903411865234
Epoch 1370, val loss: 1.0514484643936157
Epoch 1380, training loss: 452.4638366699219 = 1.0497852563858032 + 50.0 * 9.028281211853027
Epoch 1380, val loss: 1.0507628917694092
Epoch 1390, training loss: 451.8152160644531 = 1.0489649772644043 + 50.0 * 9.015325546264648
Epoch 1390, val loss: 1.050007700920105
Epoch 1400, training loss: 452.4889831542969 = 1.0482301712036133 + 50.0 * 9.028815269470215
Epoch 1400, val loss: 1.0493125915527344
Epoch 1410, training loss: 452.88092041015625 = 1.0475010871887207 + 50.0 * 9.036667823791504
Epoch 1410, val loss: 1.0486259460449219
Epoch 1420, training loss: 453.0892639160156 = 1.0467255115509033 + 50.0 * 9.040850639343262
Epoch 1420, val loss: 1.0479174852371216
Epoch 1430, training loss: 453.3421325683594 = 1.0459483861923218 + 50.0 * 9.045923233032227
Epoch 1430, val loss: 1.0471748113632202
Epoch 1440, training loss: 453.52386474609375 = 1.0451570749282837 + 50.0 * 9.04957389831543
Epoch 1440, val loss: 1.0464214086532593
Epoch 1450, training loss: 453.65850830078125 = 1.04434072971344 + 50.0 * 9.05228328704834
Epoch 1450, val loss: 1.0456551313400269
Epoch 1460, training loss: 453.61456298828125 = 1.0435044765472412 + 50.0 * 9.051421165466309
Epoch 1460, val loss: 1.0448676347732544
Epoch 1470, training loss: 453.7071228027344 = 1.0426527261734009 + 50.0 * 9.053289413452148
Epoch 1470, val loss: 1.0440579652786255
Epoch 1480, training loss: 453.9942626953125 = 1.0417852401733398 + 50.0 * 9.059049606323242
Epoch 1480, val loss: 1.0432469844818115
Epoch 1490, training loss: 454.4062805175781 = 1.040898084640503 + 50.0 * 9.067307472229004
Epoch 1490, val loss: 1.0424201488494873
Epoch 1500, training loss: 454.1476745605469 = 1.0400259494781494 + 50.0 * 9.062152862548828
Epoch 1500, val loss: 1.0416043996810913
Epoch 1510, training loss: 454.09228515625 = 1.0391392707824707 + 50.0 * 9.061062812805176
Epoch 1510, val loss: 1.0407675504684448
Epoch 1520, training loss: 454.21728515625 = 1.038264513015747 + 50.0 * 9.063580513000488
Epoch 1520, val loss: 1.0399590730667114
Epoch 1530, training loss: 454.5245361328125 = 1.0374177694320679 + 50.0 * 9.069742202758789
Epoch 1530, val loss: 1.0391814708709717
Epoch 1540, training loss: 454.7909851074219 = 1.0366151332855225 + 50.0 * 9.075087547302246
Epoch 1540, val loss: 1.0384314060211182
Epoch 1550, training loss: 454.72979736328125 = 1.0357906818389893 + 50.0 * 9.073880195617676
Epoch 1550, val loss: 1.0376601219177246
Epoch 1560, training loss: 454.9516906738281 = 1.0349992513656616 + 50.0 * 9.078333854675293
Epoch 1560, val loss: 1.0369278192520142
Epoch 1570, training loss: 455.1896057128906 = 1.0342003107070923 + 50.0 * 9.083107948303223
Epoch 1570, val loss: 1.036181926727295
Epoch 1580, training loss: 454.559326171875 = 1.033347487449646 + 50.0 * 9.07051944732666
Epoch 1580, val loss: 1.0353859663009644
Epoch 1590, training loss: 454.851318359375 = 1.0325576066970825 + 50.0 * 9.076375007629395
Epoch 1590, val loss: 1.0346640348434448
Epoch 1600, training loss: 454.92236328125 = 1.0317325592041016 + 50.0 * 9.077812194824219
Epoch 1600, val loss: 1.033921241760254
Epoch 1610, training loss: 455.2955627441406 = 1.0309836864471436 + 50.0 * 9.085291862487793
Epoch 1610, val loss: 1.0331896543502808
Epoch 1620, training loss: 455.5917053222656 = 1.0301774740219116 + 50.0 * 9.091230392456055
Epoch 1620, val loss: 1.0324586629867554
Epoch 1630, training loss: 455.9273681640625 = 1.0293855667114258 + 50.0 * 9.097959518432617
Epoch 1630, val loss: 1.031727910041809
Epoch 1640, training loss: 456.4333801269531 = 1.0286245346069336 + 50.0 * 9.108095169067383
Epoch 1640, val loss: 1.03101646900177
Epoch 1650, training loss: 456.641357421875 = 1.027827262878418 + 50.0 * 9.11227035522461
Epoch 1650, val loss: 1.0302776098251343
Epoch 1660, training loss: 456.8385314941406 = 1.027038812637329 + 50.0 * 9.116230010986328
Epoch 1660, val loss: 1.0295518636703491
Epoch 1670, training loss: 456.91986083984375 = 1.0262384414672852 + 50.0 * 9.11787223815918
Epoch 1670, val loss: 1.028807282447815
Epoch 1680, training loss: 457.09259033203125 = 1.0254364013671875 + 50.0 * 9.121343612670898
Epoch 1680, val loss: 1.028083086013794
Epoch 1690, training loss: 457.25634765625 = 1.0246466398239136 + 50.0 * 9.1246337890625
Epoch 1690, val loss: 1.027357578277588
Epoch 1700, training loss: 457.22186279296875 = 1.0238341093063354 + 50.0 * 9.123960494995117
Epoch 1700, val loss: 1.0266152620315552
Epoch 1710, training loss: 457.1452941894531 = 1.0230319499969482 + 50.0 * 9.122445106506348
Epoch 1710, val loss: 1.0258746147155762
Epoch 1720, training loss: 457.28619384765625 = 1.022212266921997 + 50.0 * 9.125279426574707
Epoch 1720, val loss: 1.0251340866088867
Epoch 1730, training loss: 457.0666809082031 = 1.0214108228683472 + 50.0 * 9.120904922485352
Epoch 1730, val loss: 1.024389624595642
Epoch 1740, training loss: 457.31622314453125 = 1.0205934047698975 + 50.0 * 9.1259126663208
Epoch 1740, val loss: 1.023636817932129
Epoch 1750, training loss: 457.4871520996094 = 1.01979660987854 + 50.0 * 9.12934684753418
Epoch 1750, val loss: 1.0229203701019287
Epoch 1760, training loss: 457.8687744140625 = 1.0190175771713257 + 50.0 * 9.136995315551758
Epoch 1760, val loss: 1.0222080945968628
Epoch 1770, training loss: 457.8291320800781 = 1.0182124376296997 + 50.0 * 9.136218070983887
Epoch 1770, val loss: 1.0214643478393555
Epoch 1780, training loss: 458.01287841796875 = 1.0174156427383423 + 50.0 * 9.139908790588379
Epoch 1780, val loss: 1.0207475423812866
Epoch 1790, training loss: 458.2135009765625 = 1.0166364908218384 + 50.0 * 9.143937110900879
Epoch 1790, val loss: 1.0200371742248535
Epoch 1800, training loss: 458.2171630859375 = 1.015824794769287 + 50.0 * 9.144026756286621
Epoch 1800, val loss: 1.0193029642105103
Epoch 1810, training loss: 458.09881591796875 = 1.0150223970413208 + 50.0 * 9.14167594909668
Epoch 1810, val loss: 1.0185672044754028
Epoch 1820, training loss: 458.30560302734375 = 1.0142455101013184 + 50.0 * 9.145827293395996
Epoch 1820, val loss: 1.0178624391555786
Epoch 1830, training loss: 458.4513244628906 = 1.0134384632110596 + 50.0 * 9.148757934570312
Epoch 1830, val loss: 1.0171204805374146
Epoch 1840, training loss: 458.3309631347656 = 1.012653112411499 + 50.0 * 9.146366119384766
Epoch 1840, val loss: 1.0164122581481934
Epoch 1850, training loss: 458.6589050292969 = 1.0118650197982788 + 50.0 * 9.15294075012207
Epoch 1850, val loss: 1.0157109498977661
Epoch 1860, training loss: 458.90081787109375 = 1.0110880136489868 + 50.0 * 9.157794952392578
Epoch 1860, val loss: 1.014997959136963
Epoch 1870, training loss: 458.77606201171875 = 1.0102812051773071 + 50.0 * 9.155315399169922
Epoch 1870, val loss: 1.014268159866333
Epoch 1880, training loss: 458.8374328613281 = 1.009508490562439 + 50.0 * 9.1565580368042
Epoch 1880, val loss: 1.0135655403137207
Epoch 1890, training loss: 459.02301025390625 = 1.0087311267852783 + 50.0 * 9.160285949707031
Epoch 1890, val loss: 1.0128757953643799
Epoch 1900, training loss: 459.0173034667969 = 1.0079617500305176 + 50.0 * 9.160186767578125
Epoch 1900, val loss: 1.0121850967407227
Epoch 1910, training loss: 459.010986328125 = 1.007180094718933 + 50.0 * 9.160076141357422
Epoch 1910, val loss: 1.0114927291870117
Epoch 1920, training loss: 459.3139343261719 = 1.0064266920089722 + 50.0 * 9.166150093078613
Epoch 1920, val loss: 1.0108146667480469
Epoch 1930, training loss: 459.19622802734375 = 1.005654215812683 + 50.0 * 9.163811683654785
Epoch 1930, val loss: 1.010114073753357
Epoch 1940, training loss: 459.3481140136719 = 1.0048822164535522 + 50.0 * 9.166864395141602
Epoch 1940, val loss: 1.0094428062438965
Epoch 1950, training loss: 459.5521240234375 = 1.0041353702545166 + 50.0 * 9.17095947265625
Epoch 1950, val loss: 1.0087671279907227
Epoch 1960, training loss: 459.5390319824219 = 1.0033645629882812 + 50.0 * 9.170713424682617
Epoch 1960, val loss: 1.0080833435058594
Epoch 1970, training loss: 459.43408203125 = 1.0025827884674072 + 50.0 * 9.16862964630127
Epoch 1970, val loss: 1.0073833465576172
Epoch 1980, training loss: 459.5703430175781 = 1.001818299293518 + 50.0 * 9.171370506286621
Epoch 1980, val loss: 1.0067124366760254
Epoch 1990, training loss: 459.73724365234375 = 1.0010658502578735 + 50.0 * 9.174723625183105
Epoch 1990, val loss: 1.0060473680496216
Epoch 2000, training loss: 459.9666748046875 = 1.0003211498260498 + 50.0 * 9.179327011108398
Epoch 2000, val loss: 1.0053786039352417
Epoch 2010, training loss: 457.3478088378906 = 0.9995806217193604 + 50.0 * 9.126964569091797
Epoch 2010, val loss: 1.0046420097351074
Epoch 2020, training loss: 454.76898193359375 = 0.9990552663803101 + 50.0 * 9.075398445129395
Epoch 2020, val loss: 1.004297137260437
Epoch 2030, training loss: 457.8908996582031 = 0.9985995888710022 + 50.0 * 9.137845993041992
Epoch 2030, val loss: 1.0038304328918457
Epoch 2040, training loss: 457.66680908203125 = 0.9978814125061035 + 50.0 * 9.133378028869629
Epoch 2040, val loss: 1.0032198429107666
Epoch 2050, training loss: 457.8628845214844 = 0.9971826672554016 + 50.0 * 9.137313842773438
Epoch 2050, val loss: 1.0025885105133057
Epoch 2060, training loss: 458.75042724609375 = 0.9964766502380371 + 50.0 * 9.155078887939453
Epoch 2060, val loss: 1.0019644498825073
Epoch 2070, training loss: 459.347900390625 = 0.9957645535469055 + 50.0 * 9.16704273223877
Epoch 2070, val loss: 1.0013383626937866
Epoch 2080, training loss: 459.9262390136719 = 0.9950823783874512 + 50.0 * 9.17862319946289
Epoch 2080, val loss: 1.0007381439208984
Epoch 2090, training loss: 460.1470031738281 = 0.9943684935569763 + 50.0 * 9.183053016662598
Epoch 2090, val loss: 1.000099539756775
Epoch 2100, training loss: 460.4202575683594 = 0.9936566352844238 + 50.0 * 9.188531875610352
Epoch 2100, val loss: 0.999477744102478
Epoch 2110, training loss: 460.4052429199219 = 0.992931604385376 + 50.0 * 9.188246726989746
Epoch 2110, val loss: 0.9988354444503784
Epoch 2120, training loss: 460.4541320800781 = 0.9922305941581726 + 50.0 * 9.189238548278809
Epoch 2120, val loss: 0.998221755027771
Epoch 2130, training loss: 460.7444763183594 = 0.9915426969528198 + 50.0 * 9.195058822631836
Epoch 2130, val loss: 0.9976201057434082
Epoch 2140, training loss: 461.02349853515625 = 0.9908470511436462 + 50.0 * 9.200653076171875
Epoch 2140, val loss: 0.9970065355300903
Epoch 2150, training loss: 460.9439697265625 = 0.9901349544525146 + 50.0 * 9.199076652526855
Epoch 2150, val loss: 0.9963886141777039
Epoch 2160, training loss: 460.8194885253906 = 0.9894382953643799 + 50.0 * 9.196600914001465
Epoch 2160, val loss: 0.9957776069641113
Epoch 2170, training loss: 461.28411865234375 = 0.9887697100639343 + 50.0 * 9.205906867980957
Epoch 2170, val loss: 0.9951905012130737
Epoch 2180, training loss: 461.2473449707031 = 0.9880837202072144 + 50.0 * 9.205184936523438
Epoch 2180, val loss: 0.9945726990699768
Epoch 2190, training loss: 461.18011474609375 = 0.9873862266540527 + 50.0 * 9.20385456085205
Epoch 2190, val loss: 0.9939693212509155
Epoch 2200, training loss: 461.51617431640625 = 0.9867094159126282 + 50.0 * 9.210589408874512
Epoch 2200, val loss: 0.9933896064758301
Epoch 2210, training loss: 461.8004455566406 = 0.986038863658905 + 50.0 * 9.216287612915039
Epoch 2210, val loss: 0.9928018450737
Epoch 2220, training loss: 461.7505798339844 = 0.9853441119194031 + 50.0 * 9.215304374694824
Epoch 2220, val loss: 0.99220871925354
Epoch 2230, training loss: 461.953857421875 = 0.9846727848052979 + 50.0 * 9.219383239746094
Epoch 2230, val loss: 0.9916276931762695
Epoch 2240, training loss: 462.1485900878906 = 0.9840121269226074 + 50.0 * 9.223291397094727
Epoch 2240, val loss: 0.9910471439361572
Epoch 2250, training loss: 462.2507019042969 = 0.9833455681800842 + 50.0 * 9.225347518920898
Epoch 2250, val loss: 0.9904643893241882
Epoch 2260, training loss: 462.0564880371094 = 0.9826678037643433 + 50.0 * 9.221476554870605
Epoch 2260, val loss: 0.9898821711540222
Epoch 2270, training loss: 462.3385314941406 = 0.9820041656494141 + 50.0 * 9.227130889892578
Epoch 2270, val loss: 0.9893030524253845
Epoch 2280, training loss: 462.6373596191406 = 0.9813477396965027 + 50.0 * 9.23311996459961
Epoch 2280, val loss: 0.9887328147888184
Epoch 2290, training loss: 462.6070251464844 = 0.9806836843490601 + 50.0 * 9.232526779174805
Epoch 2290, val loss: 0.9881620407104492
Epoch 2300, training loss: 462.5589904785156 = 0.9800229668617249 + 50.0 * 9.231579780578613
Epoch 2300, val loss: 0.9875954389572144
Epoch 2310, training loss: 462.5743713378906 = 0.9793773889541626 + 50.0 * 9.231900215148926
Epoch 2310, val loss: 0.9870519042015076
Epoch 2320, training loss: 462.6885070800781 = 0.9787344336509705 + 50.0 * 9.234195709228516
Epoch 2320, val loss: 0.9865040183067322
Epoch 2330, training loss: 463.0114440917969 = 0.9781107306480408 + 50.0 * 9.240666389465332
Epoch 2330, val loss: 0.9859730005264282
Epoch 2340, training loss: 462.8974914550781 = 0.9774676561355591 + 50.0 * 9.23840045928955
Epoch 2340, val loss: 0.9854210019111633
Epoch 2350, training loss: 462.997802734375 = 0.9768497943878174 + 50.0 * 9.240419387817383
Epoch 2350, val loss: 0.9848938584327698
Epoch 2360, training loss: 463.1420593261719 = 0.9762256741523743 + 50.0 * 9.243316650390625
Epoch 2360, val loss: 0.9843665957450867
Epoch 2370, training loss: 462.87548828125 = 0.9755957722663879 + 50.0 * 9.237998008728027
Epoch 2370, val loss: 0.9838254451751709
Epoch 2380, training loss: 463.0098571777344 = 0.9749787449836731 + 50.0 * 9.240697860717773
Epoch 2380, val loss: 0.9833049774169922
Epoch 2390, training loss: 463.2629089355469 = 0.9743806719779968 + 50.0 * 9.245770454406738
Epoch 2390, val loss: 0.9827951788902283
Epoch 2400, training loss: 463.5910949707031 = 0.9737833142280579 + 50.0 * 9.25234603881836
Epoch 2400, val loss: 0.9822800755500793
Epoch 2410, training loss: 463.3983154296875 = 0.9731696248054504 + 50.0 * 9.248502731323242
Epoch 2410, val loss: 0.9817480444908142
Epoch 2420, training loss: 463.4770812988281 = 0.9725672006607056 + 50.0 * 9.250090599060059
Epoch 2420, val loss: 0.9812479019165039
Epoch 2430, training loss: 463.6691589355469 = 0.9719846844673157 + 50.0 * 9.25394344329834
Epoch 2430, val loss: 0.980756402015686
Epoch 2440, training loss: 463.3186950683594 = 0.9713783264160156 + 50.0 * 9.246946334838867
Epoch 2440, val loss: 0.9802430272102356
Epoch 2450, training loss: 463.4949645996094 = 0.9708065986633301 + 50.0 * 9.250483512878418
Epoch 2450, val loss: 0.9797729253768921
Epoch 2460, training loss: 463.6537780761719 = 0.9702546000480652 + 50.0 * 9.253670692443848
Epoch 2460, val loss: 0.9793057441711426
Epoch 2470, training loss: 463.84674072265625 = 0.9696882963180542 + 50.0 * 9.257540702819824
Epoch 2470, val loss: 0.9788342118263245
Epoch 2480, training loss: 463.9423828125 = 0.969119668006897 + 50.0 * 9.259465217590332
Epoch 2480, val loss: 0.9783449172973633
Epoch 2490, training loss: 464.0025329589844 = 0.9685570597648621 + 50.0 * 9.260679244995117
Epoch 2490, val loss: 0.9778753519058228
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.4781159420289855
0.8148953126132001
=== training gcn model ===
Epoch 0, training loss: 515.5408935546875 = 1.1118959188461304 + 50.0 * 10.288580894470215
Epoch 0, val loss: 1.1118139028549194
Epoch 10, training loss: 496.8377380371094 = 1.1113845109939575 + 50.0 * 9.91452693939209
Epoch 10, val loss: 1.1113425493240356
Epoch 20, training loss: 488.272705078125 = 1.1110354661941528 + 50.0 * 9.743233680725098
Epoch 20, val loss: 1.1109954118728638
Epoch 30, training loss: 481.6005859375 = 1.1106234788894653 + 50.0 * 9.6097993850708
Epoch 30, val loss: 1.110589623451233
Epoch 40, training loss: 476.30670166015625 = 1.1102179288864136 + 50.0 * 9.503929138183594
Epoch 40, val loss: 1.1101889610290527
Epoch 50, training loss: 471.9714660644531 = 1.109806776046753 + 50.0 * 9.41723346710205
Epoch 50, val loss: 1.1097815036773682
Epoch 60, training loss: 468.3031311035156 = 1.1093813180923462 + 50.0 * 9.34387493133545
Epoch 60, val loss: 1.1093595027923584
Epoch 70, training loss: 465.1131286621094 = 1.1089526414871216 + 50.0 * 9.280083656311035
Epoch 70, val loss: 1.108932375907898
Epoch 80, training loss: 462.31982421875 = 1.1085129976272583 + 50.0 * 9.224225997924805
Epoch 80, val loss: 1.1084961891174316
Epoch 90, training loss: 459.8805847167969 = 1.1080715656280518 + 50.0 * 9.175450325012207
Epoch 90, val loss: 1.1080551147460938
Epoch 100, training loss: 457.6990661621094 = 1.1076139211654663 + 50.0 * 9.131829261779785
Epoch 100, val loss: 1.1076022386550903
Epoch 110, training loss: 455.8002624511719 = 1.1071382761001587 + 50.0 * 9.093862533569336
Epoch 110, val loss: 1.107134222984314
Epoch 120, training loss: 454.1872253417969 = 1.106663465499878 + 50.0 * 9.06161117553711
Epoch 120, val loss: 1.1066612005233765
Epoch 130, training loss: 452.7750549316406 = 1.1061763763427734 + 50.0 * 9.033377647399902
Epoch 130, val loss: 1.1061773300170898
Epoch 140, training loss: 451.5462341308594 = 1.1056828498840332 + 50.0 * 9.008810997009277
Epoch 140, val loss: 1.105691909790039
Epoch 150, training loss: 450.3275451660156 = 1.105168104171753 + 50.0 * 8.984447479248047
Epoch 150, val loss: 1.1051855087280273
Epoch 160, training loss: 449.3186340332031 = 1.1045984029769897 + 50.0 * 8.96428108215332
Epoch 160, val loss: 1.1045862436294556
Epoch 170, training loss: 448.4521789550781 = 1.1036274433135986 + 50.0 * 8.94697093963623
Epoch 170, val loss: 1.1035743951797485
Epoch 180, training loss: 447.6380920410156 = 1.1027005910873413 + 50.0 * 8.930707931518555
Epoch 180, val loss: 1.1025975942611694
Epoch 190, training loss: 446.9984130859375 = 1.101782202720642 + 50.0 * 8.917932510375977
Epoch 190, val loss: 1.1016477346420288
Epoch 200, training loss: 446.35089111328125 = 1.1008896827697754 + 50.0 * 8.904999732971191
Epoch 200, val loss: 1.100710391998291
Epoch 210, training loss: 445.7462158203125 = 1.0999959707260132 + 50.0 * 8.892924308776855
Epoch 210, val loss: 1.099809169769287
Epoch 220, training loss: 445.2875061035156 = 1.0991460084915161 + 50.0 * 8.883767127990723
Epoch 220, val loss: 1.0989196300506592
Epoch 230, training loss: 444.8981018066406 = 1.0982518196105957 + 50.0 * 8.875996589660645
Epoch 230, val loss: 1.0980199575424194
Epoch 240, training loss: 444.50555419921875 = 1.0974024534225464 + 50.0 * 8.868163108825684
Epoch 240, val loss: 1.0971263647079468
Epoch 250, training loss: 444.1615905761719 = 1.0965347290039062 + 50.0 * 8.86130142211914
Epoch 250, val loss: 1.0962483882904053
Epoch 260, training loss: 444.0263977050781 = 1.0957343578338623 + 50.0 * 8.858613014221191
Epoch 260, val loss: 1.095441460609436
Epoch 270, training loss: 443.7445373535156 = 1.0949516296386719 + 50.0 * 8.852992057800293
Epoch 270, val loss: 1.0946446657180786
Epoch 280, training loss: 443.6880798339844 = 1.0942131280899048 + 50.0 * 8.851877212524414
Epoch 280, val loss: 1.0938776731491089
Epoch 290, training loss: 443.4703674316406 = 1.0935028791427612 + 50.0 * 8.84753704071045
Epoch 290, val loss: 1.0931658744812012
Epoch 300, training loss: 443.4096984863281 = 1.0928359031677246 + 50.0 * 8.84633731842041
Epoch 300, val loss: 1.0924937725067139
Epoch 310, training loss: 443.3942565917969 = 1.092221975326538 + 50.0 * 8.846040725708008
Epoch 310, val loss: 1.091860055923462
Epoch 320, training loss: 443.47454833984375 = 1.0916732549667358 + 50.0 * 8.847657203674316
Epoch 320, val loss: 1.091312050819397
Epoch 330, training loss: 443.320556640625 = 1.0911436080932617 + 50.0 * 8.844588279724121
Epoch 330, val loss: 1.0907701253890991
Epoch 340, training loss: 443.2457275390625 = 1.090630054473877 + 50.0 * 8.843101501464844
Epoch 340, val loss: 1.0902409553527832
Epoch 350, training loss: 443.0686950683594 = 1.0901424884796143 + 50.0 * 8.839570999145508
Epoch 350, val loss: 1.0897367000579834
Epoch 360, training loss: 443.19952392578125 = 1.0896990299224854 + 50.0 * 8.842196464538574
Epoch 360, val loss: 1.0892928838729858
Epoch 370, training loss: 443.2237243652344 = 1.0892493724822998 + 50.0 * 8.842689514160156
Epoch 370, val loss: 1.0888265371322632
Epoch 380, training loss: 443.3096008300781 = 1.0888020992279053 + 50.0 * 8.844415664672852
Epoch 380, val loss: 1.088368535041809
Epoch 390, training loss: 443.3231506347656 = 1.0883466005325317 + 50.0 * 8.844696044921875
Epoch 390, val loss: 1.0879087448120117
Epoch 400, training loss: 443.4153137207031 = 1.0879149436950684 + 50.0 * 8.846548080444336
Epoch 400, val loss: 1.0874606370925903
Epoch 410, training loss: 443.4267578125 = 1.0874443054199219 + 50.0 * 8.846786499023438
Epoch 410, val loss: 1.0869907140731812
Epoch 420, training loss: 443.50201416015625 = 1.0870072841644287 + 50.0 * 8.848299980163574
Epoch 420, val loss: 1.0865346193313599
Epoch 430, training loss: 443.4666442871094 = 1.086553692817688 + 50.0 * 8.847601890563965
Epoch 430, val loss: 1.0860692262649536
Epoch 440, training loss: 443.5423278808594 = 1.086076021194458 + 50.0 * 8.849124908447266
Epoch 440, val loss: 1.0855910778045654
Epoch 450, training loss: 443.417236328125 = 1.0856012105941772 + 50.0 * 8.846632957458496
Epoch 450, val loss: 1.085103154182434
Epoch 460, training loss: 443.474365234375 = 1.0851203203201294 + 50.0 * 8.847784996032715
Epoch 460, val loss: 1.084630012512207
Epoch 470, training loss: 443.5207824707031 = 1.0846307277679443 + 50.0 * 8.848723411560059
Epoch 470, val loss: 1.0841388702392578
Epoch 480, training loss: 443.58843994140625 = 1.0841326713562012 + 50.0 * 8.850086212158203
Epoch 480, val loss: 1.083632230758667
Epoch 490, training loss: 443.6509704589844 = 1.0836275815963745 + 50.0 * 8.851346969604492
Epoch 490, val loss: 1.0831307172775269
Epoch 500, training loss: 443.8388671875 = 1.0831354856491089 + 50.0 * 8.855114936828613
Epoch 500, val loss: 1.0826293230056763
Epoch 510, training loss: 443.9010009765625 = 1.0826210975646973 + 50.0 * 8.856368064880371
Epoch 510, val loss: 1.082109808921814
Epoch 520, training loss: 443.7740783691406 = 1.0821070671081543 + 50.0 * 8.853839874267578
Epoch 520, val loss: 1.0816094875335693
Epoch 530, training loss: 443.7983703613281 = 1.0815672874450684 + 50.0 * 8.85433578491211
Epoch 530, val loss: 1.081078290939331
Epoch 540, training loss: 443.8412780761719 = 1.0810561180114746 + 50.0 * 8.855204582214355
Epoch 540, val loss: 1.080556035041809
Epoch 550, training loss: 444.0713195800781 = 1.0805119276046753 + 50.0 * 8.859816551208496
Epoch 550, val loss: 1.0800206661224365
Epoch 560, training loss: 444.1092529296875 = 1.0799634456634521 + 50.0 * 8.860586166381836
Epoch 560, val loss: 1.0794674158096313
Epoch 570, training loss: 444.11175537109375 = 1.0794188976287842 + 50.0 * 8.860647201538086
Epoch 570, val loss: 1.0789291858673096
Epoch 580, training loss: 444.093017578125 = 1.0788483619689941 + 50.0 * 8.860282897949219
Epoch 580, val loss: 1.0783734321594238
Epoch 590, training loss: 443.9392395019531 = 1.078227162361145 + 50.0 * 8.857220649719238
Epoch 590, val loss: 1.0777755975723267
Epoch 600, training loss: 444.1133117675781 = 1.0776959657669067 + 50.0 * 8.860712051391602
Epoch 600, val loss: 1.077246069908142
Epoch 610, training loss: 444.48651123046875 = 1.077124834060669 + 50.0 * 8.86818790435791
Epoch 610, val loss: 1.0766810178756714
Epoch 620, training loss: 444.4763488769531 = 1.0765268802642822 + 50.0 * 8.867996215820312
Epoch 620, val loss: 1.076094627380371
Epoch 630, training loss: 444.4917907714844 = 1.0759105682373047 + 50.0 * 8.868317604064941
Epoch 630, val loss: 1.0754989385604858
Epoch 640, training loss: 444.73138427734375 = 1.0753084421157837 + 50.0 * 8.87312126159668
Epoch 640, val loss: 1.0748919248580933
Epoch 650, training loss: 445.0166015625 = 1.074697732925415 + 50.0 * 8.878837585449219
Epoch 650, val loss: 1.0742851495742798
Epoch 660, training loss: 444.8335266113281 = 1.0740376710891724 + 50.0 * 8.875189781188965
Epoch 660, val loss: 1.0736476182937622
Epoch 670, training loss: 445.2022399902344 = 1.0734074115753174 + 50.0 * 8.882576942443848
Epoch 670, val loss: 1.073026418685913
Epoch 680, training loss: 445.22235107421875 = 1.0727527141571045 + 50.0 * 8.882991790771484
Epoch 680, val loss: 1.0723837614059448
Epoch 690, training loss: 445.4653015136719 = 1.0720958709716797 + 50.0 * 8.887864112854004
Epoch 690, val loss: 1.0717458724975586
Epoch 700, training loss: 445.9480895996094 = 1.0714064836502075 + 50.0 * 8.897533416748047
Epoch 700, val loss: 1.0710328817367554
Epoch 710, training loss: 446.1519775390625 = 1.0706571340560913 + 50.0 * 8.901626586914062
Epoch 710, val loss: 1.070332407951355
Epoch 720, training loss: 445.95587158203125 = 1.069940447807312 + 50.0 * 8.89771842956543
Epoch 720, val loss: 1.0696005821228027
Epoch 730, training loss: 446.3924255371094 = 1.0692342519760132 + 50.0 * 8.906463623046875
Epoch 730, val loss: 1.0689435005187988
Epoch 740, training loss: 446.58819580078125 = 1.0685372352600098 + 50.0 * 8.910392761230469
Epoch 740, val loss: 1.0682685375213623
Epoch 750, training loss: 446.94219970703125 = 1.067839503288269 + 50.0 * 8.917487144470215
Epoch 750, val loss: 1.0675852298736572
Epoch 760, training loss: 447.03558349609375 = 1.067101240158081 + 50.0 * 8.9193696975708
Epoch 760, val loss: 1.0668729543685913
Epoch 770, training loss: 447.30230712890625 = 1.066362977027893 + 50.0 * 8.924718856811523
Epoch 770, val loss: 1.0661396980285645
Epoch 780, training loss: 446.64593505859375 = 1.0655864477157593 + 50.0 * 8.911606788635254
Epoch 780, val loss: 1.065401554107666
Epoch 790, training loss: 447.0387878417969 = 1.0648068189620972 + 50.0 * 8.919479370117188
Epoch 790, val loss: 1.0646463632583618
Epoch 800, training loss: 447.0893249511719 = 1.0640654563903809 + 50.0 * 8.92050552368164
Epoch 800, val loss: 1.0639166831970215
Epoch 810, training loss: 447.190673828125 = 1.0632933378219604 + 50.0 * 8.922547340393066
Epoch 810, val loss: 1.0631706714630127
Epoch 820, training loss: 447.5205993652344 = 1.0625083446502686 + 50.0 * 8.92916202545166
Epoch 820, val loss: 1.0624172687530518
Epoch 830, training loss: 447.9263000488281 = 1.0617406368255615 + 50.0 * 8.937291145324707
Epoch 830, val loss: 1.0616683959960938
Epoch 840, training loss: 448.00335693359375 = 1.0609380006790161 + 50.0 * 8.938848495483398
Epoch 840, val loss: 1.0608887672424316
Epoch 850, training loss: 448.3305969238281 = 1.060150384902954 + 50.0 * 8.945408821105957
Epoch 850, val loss: 1.0601309537887573
Epoch 860, training loss: 448.38299560546875 = 1.0593302249908447 + 50.0 * 8.946473121643066
Epoch 860, val loss: 1.0593258142471313
Epoch 870, training loss: 448.65423583984375 = 1.0585383176803589 + 50.0 * 8.951913833618164
Epoch 870, val loss: 1.0585644245147705
Epoch 880, training loss: 448.668701171875 = 1.0577069520950317 + 50.0 * 8.95221996307373
Epoch 880, val loss: 1.057748556137085
Epoch 890, training loss: 448.7171936035156 = 1.0568779706954956 + 50.0 * 8.953206062316895
Epoch 890, val loss: 1.0569630861282349
Epoch 900, training loss: 448.7297058105469 = 1.0560661554336548 + 50.0 * 8.953473091125488
Epoch 900, val loss: 1.0561569929122925
Epoch 910, training loss: 448.99969482421875 = 1.0552144050598145 + 50.0 * 8.958889961242676
Epoch 910, val loss: 1.0553505420684814
Epoch 920, training loss: 449.2373352050781 = 1.0543913841247559 + 50.0 * 8.963659286499023
Epoch 920, val loss: 1.0545505285263062
Epoch 930, training loss: 449.5069885253906 = 1.053528904914856 + 50.0 * 8.969069480895996
Epoch 930, val loss: 1.053726315498352
Epoch 940, training loss: 449.4220886230469 = 1.0526368618011475 + 50.0 * 8.967389106750488
Epoch 940, val loss: 1.052843451499939
Epoch 950, training loss: 449.52569580078125 = 1.051733136177063 + 50.0 * 8.96947956085205
Epoch 950, val loss: 1.0519630908966064
Epoch 960, training loss: 449.8288879394531 = 1.0507735013961792 + 50.0 * 8.97556209564209
Epoch 960, val loss: 1.0510380268096924
Epoch 970, training loss: 449.99200439453125 = 1.049766182899475 + 50.0 * 8.97884464263916
Epoch 970, val loss: 1.050052285194397
Epoch 980, training loss: 449.73028564453125 = 1.04863703250885 + 50.0 * 8.9736328125
Epoch 980, val loss: 1.0489548444747925
Epoch 990, training loss: 449.8644104003906 = 1.0475698709487915 + 50.0 * 8.976336479187012
Epoch 990, val loss: 1.0479471683502197
Epoch 1000, training loss: 450.0397033691406 = 1.0464959144592285 + 50.0 * 8.979864120483398
Epoch 1000, val loss: 1.0468846559524536
Epoch 1010, training loss: 450.1885681152344 = 1.0454007387161255 + 50.0 * 8.982863426208496
Epoch 1010, val loss: 1.045825481414795
Epoch 1020, training loss: 449.9524230957031 = 1.0442787408828735 + 50.0 * 8.97816276550293
Epoch 1020, val loss: 1.0447373390197754
Epoch 1030, training loss: 450.07745361328125 = 1.0431852340698242 + 50.0 * 8.980685234069824
Epoch 1030, val loss: 1.0436639785766602
Epoch 1040, training loss: 450.1729736328125 = 1.0420401096343994 + 50.0 * 8.98261833190918
Epoch 1040, val loss: 1.042531967163086
Epoch 1050, training loss: 450.4677734375 = 1.040917992591858 + 50.0 * 8.988536834716797
Epoch 1050, val loss: 1.0414646863937378
Epoch 1060, training loss: 450.6150207519531 = 1.0397862195968628 + 50.0 * 8.991504669189453
Epoch 1060, val loss: 1.0403627157211304
Epoch 1070, training loss: 450.81561279296875 = 1.0386263132095337 + 50.0 * 8.995539665222168
Epoch 1070, val loss: 1.0392245054244995
Epoch 1080, training loss: 451.0348815917969 = 1.0374500751495361 + 50.0 * 8.999948501586914
Epoch 1080, val loss: 1.0380914211273193
Epoch 1090, training loss: 451.0938720703125 = 1.0362575054168701 + 50.0 * 9.001152038574219
Epoch 1090, val loss: 1.0369153022766113
Epoch 1100, training loss: 450.86346435546875 = 1.035040020942688 + 50.0 * 8.99656867980957
Epoch 1100, val loss: 1.0357482433319092
Epoch 1110, training loss: 451.2213439941406 = 1.033848762512207 + 50.0 * 9.00374984741211
Epoch 1110, val loss: 1.034567952156067
Epoch 1120, training loss: 451.686767578125 = 1.032647967338562 + 50.0 * 9.013082504272461
Epoch 1120, val loss: 1.0334092378616333
Epoch 1130, training loss: 451.2527770996094 = 1.0313308238983154 + 50.0 * 9.00442886352539
Epoch 1130, val loss: 1.032132863998413
Epoch 1140, training loss: 450.972900390625 = 1.0301227569580078 + 50.0 * 8.998855590820312
Epoch 1140, val loss: 1.030948519706726
Epoch 1150, training loss: 451.0259704589844 = 1.028835654258728 + 50.0 * 8.999942779541016
Epoch 1150, val loss: 1.0296939611434937
Epoch 1160, training loss: 451.279296875 = 1.0275852680206299 + 50.0 * 9.005034446716309
Epoch 1160, val loss: 1.0284881591796875
Epoch 1170, training loss: 451.6093444824219 = 1.0263646841049194 + 50.0 * 9.011659622192383
Epoch 1170, val loss: 1.0272986888885498
Epoch 1180, training loss: 451.9574890136719 = 1.0251091718673706 + 50.0 * 9.018647193908691
Epoch 1180, val loss: 1.026075839996338
Epoch 1190, training loss: 451.9752502441406 = 1.02379310131073 + 50.0 * 9.019028663635254
Epoch 1190, val loss: 1.0247968435287476
Epoch 1200, training loss: 451.8482360839844 = 1.022480845451355 + 50.0 * 9.016514778137207
Epoch 1200, val loss: 1.0235083103179932
Epoch 1210, training loss: 452.1287841796875 = 1.0211853981018066 + 50.0 * 9.022151947021484
Epoch 1210, val loss: 1.0222731828689575
Epoch 1220, training loss: 452.2414245605469 = 1.0198936462402344 + 50.0 * 9.024430274963379
Epoch 1220, val loss: 1.0210046768188477
Epoch 1230, training loss: 452.4697265625 = 1.0185576677322388 + 50.0 * 9.029023170471191
Epoch 1230, val loss: 1.0196970701217651
Epoch 1240, training loss: 452.59002685546875 = 1.0172061920166016 + 50.0 * 9.031455993652344
Epoch 1240, val loss: 1.0183848142623901
Epoch 1250, training loss: 452.6542053222656 = 1.0158509016036987 + 50.0 * 9.032767295837402
Epoch 1250, val loss: 1.0170743465423584
Epoch 1260, training loss: 452.72576904296875 = 1.0144990682601929 + 50.0 * 9.034225463867188
Epoch 1260, val loss: 1.0157618522644043
Epoch 1270, training loss: 452.5970458984375 = 1.0131253004074097 + 50.0 * 9.031678199768066
Epoch 1270, val loss: 1.0144352912902832
Epoch 1280, training loss: 451.878662109375 = 1.0117127895355225 + 50.0 * 9.017338752746582
Epoch 1280, val loss: 1.0130406618118286
Epoch 1290, training loss: 451.9523010253906 = 1.010318636894226 + 50.0 * 9.018839836120605
Epoch 1290, val loss: 1.0117210149765015
Epoch 1300, training loss: 452.34906005859375 = 1.0089836120605469 + 50.0 * 9.026802062988281
Epoch 1300, val loss: 1.0104217529296875
Epoch 1310, training loss: 452.7059326171875 = 1.0076406002044678 + 50.0 * 9.033966064453125
Epoch 1310, val loss: 1.0091180801391602
Epoch 1320, training loss: 452.82720947265625 = 1.0062638521194458 + 50.0 * 9.036418914794922
Epoch 1320, val loss: 1.0077861547470093
Epoch 1330, training loss: 453.15869140625 = 1.0049121379852295 + 50.0 * 9.043075561523438
Epoch 1330, val loss: 1.0064685344696045
Epoch 1340, training loss: 452.9623107910156 = 1.0035126209259033 + 50.0 * 9.039175987243652
Epoch 1340, val loss: 1.0051195621490479
Epoch 1350, training loss: 453.1960754394531 = 1.0021278858184814 + 50.0 * 9.043878555297852
Epoch 1350, val loss: 1.0037695169448853
Epoch 1360, training loss: 453.36993408203125 = 1.0007492303848267 + 50.0 * 9.047383308410645
Epoch 1360, val loss: 1.0024313926696777
Epoch 1370, training loss: 453.40142822265625 = 0.9993396997451782 + 50.0 * 9.048042297363281
Epoch 1370, val loss: 1.0010615587234497
Epoch 1380, training loss: 453.2096862792969 = 0.9979326128959656 + 50.0 * 9.044235229492188
Epoch 1380, val loss: 0.9997167587280273
Epoch 1390, training loss: 453.42645263671875 = 0.9965450167655945 + 50.0 * 9.048598289489746
Epoch 1390, val loss: 0.9983494877815247
Epoch 1400, training loss: 453.61614990234375 = 0.9951637387275696 + 50.0 * 9.052419662475586
Epoch 1400, val loss: 0.9970080852508545
Epoch 1410, training loss: 453.6173400878906 = 0.9937483668327332 + 50.0 * 9.052472114562988
Epoch 1410, val loss: 0.9956488609313965
Epoch 1420, training loss: 454.0718688964844 = 0.9924077391624451 + 50.0 * 9.061589241027832
Epoch 1420, val loss: 0.994321346282959
Epoch 1430, training loss: 454.1242980957031 = 0.9909623861312866 + 50.0 * 9.062666893005371
Epoch 1430, val loss: 0.9929559826850891
Epoch 1440, training loss: 454.2929992675781 = 0.9896209239959717 + 50.0 * 9.066067695617676
Epoch 1440, val loss: 0.991649329662323
Epoch 1450, training loss: 454.258544921875 = 0.9882107973098755 + 50.0 * 9.065406799316406
Epoch 1450, val loss: 0.9902802109718323
Epoch 1460, training loss: 454.48931884765625 = 0.9868373870849609 + 50.0 * 9.070049285888672
Epoch 1460, val loss: 0.9889460206031799
Epoch 1470, training loss: 454.40032958984375 = 0.9854100942611694 + 50.0 * 9.06829833984375
Epoch 1470, val loss: 0.9875509142875671
Epoch 1480, training loss: 454.34869384765625 = 0.9839645028114319 + 50.0 * 9.067294120788574
Epoch 1480, val loss: 0.9861838221549988
Epoch 1490, training loss: 454.79547119140625 = 0.9825801253318787 + 50.0 * 9.076257705688477
Epoch 1490, val loss: 0.9848528504371643
Epoch 1500, training loss: 455.1049499511719 = 0.9812081456184387 + 50.0 * 9.082474708557129
Epoch 1500, val loss: 0.9835363626480103
Epoch 1510, training loss: 454.9262390136719 = 0.9798053503036499 + 50.0 * 9.07892894744873
Epoch 1510, val loss: 0.9821910262107849
Epoch 1520, training loss: 455.23956298828125 = 0.9784064292907715 + 50.0 * 9.085223197937012
Epoch 1520, val loss: 0.9808411002159119
Epoch 1530, training loss: 455.3253173828125 = 0.9770017862319946 + 50.0 * 9.086966514587402
Epoch 1530, val loss: 0.9794930815696716
Epoch 1540, training loss: 455.097900390625 = 0.9755730032920837 + 50.0 * 9.082446098327637
Epoch 1540, val loss: 0.9781379103660583
Epoch 1550, training loss: 454.9828186035156 = 0.9741479754447937 + 50.0 * 9.08017349243164
Epoch 1550, val loss: 0.9767425060272217
Epoch 1560, training loss: 455.222412109375 = 0.9727460741996765 + 50.0 * 9.084993362426758
Epoch 1560, val loss: 0.9754074811935425
Epoch 1570, training loss: 455.5867919921875 = 0.9713603854179382 + 50.0 * 9.092308044433594
Epoch 1570, val loss: 0.9740665555000305
Epoch 1580, training loss: 455.6742248535156 = 0.969941258430481 + 50.0 * 9.094085693359375
Epoch 1580, val loss: 0.9727031588554382
Epoch 1590, training loss: 455.7282409667969 = 0.968525230884552 + 50.0 * 9.095193862915039
Epoch 1590, val loss: 0.9713706970214844
Epoch 1600, training loss: 455.9064636230469 = 0.9671225547790527 + 50.0 * 9.098786354064941
Epoch 1600, val loss: 0.9700236916542053
Epoch 1610, training loss: 455.9042663574219 = 0.9657114148139954 + 50.0 * 9.098771095275879
Epoch 1610, val loss: 0.9686897993087769
Epoch 1620, training loss: 456.0296325683594 = 0.9643005132675171 + 50.0 * 9.101306915283203
Epoch 1620, val loss: 0.967367947101593
Epoch 1630, training loss: 456.04901123046875 = 0.96290522813797 + 50.0 * 9.101722717285156
Epoch 1630, val loss: 0.9660172462463379
Epoch 1640, training loss: 456.18304443359375 = 0.9614893198013306 + 50.0 * 9.10443115234375
Epoch 1640, val loss: 0.9646928310394287
Epoch 1650, training loss: 456.2554016113281 = 0.9600839614868164 + 50.0 * 9.10590648651123
Epoch 1650, val loss: 0.9633620381355286
Epoch 1660, training loss: 456.31964111328125 = 0.958706259727478 + 50.0 * 9.107218742370605
Epoch 1660, val loss: 0.9620393514633179
Epoch 1670, training loss: 456.42803955078125 = 0.9573225378990173 + 50.0 * 9.109414100646973
Epoch 1670, val loss: 0.9607349038124084
Epoch 1680, training loss: 456.518798828125 = 0.9559239149093628 + 50.0 * 9.111257553100586
Epoch 1680, val loss: 0.9593888521194458
Epoch 1690, training loss: 456.5202331542969 = 0.9545153975486755 + 50.0 * 9.111313819885254
Epoch 1690, val loss: 0.9580563306808472
Epoch 1700, training loss: 456.6058654785156 = 0.9531537890434265 + 50.0 * 9.113054275512695
Epoch 1700, val loss: 0.9567687511444092
Epoch 1710, training loss: 456.8836975097656 = 0.9517936110496521 + 50.0 * 9.118638038635254
Epoch 1710, val loss: 0.9554843306541443
Epoch 1720, training loss: 457.09735107421875 = 0.9504467248916626 + 50.0 * 9.12293815612793
Epoch 1720, val loss: 0.9542005062103271
Epoch 1730, training loss: 456.8659973144531 = 0.9490521550178528 + 50.0 * 9.118338584899902
Epoch 1730, val loss: 0.9528877139091492
Epoch 1740, training loss: 456.8348083496094 = 0.9476917386054993 + 50.0 * 9.117742538452148
Epoch 1740, val loss: 0.9515915513038635
Epoch 1750, training loss: 456.9989013671875 = 0.9463399648666382 + 50.0 * 9.121050834655762
Epoch 1750, val loss: 0.9502977132797241
Epoch 1760, training loss: 457.2829895019531 = 0.9449899792671204 + 50.0 * 9.126760482788086
Epoch 1760, val loss: 0.9490432739257812
Epoch 1770, training loss: 457.21331787109375 = 0.9436319470405579 + 50.0 * 9.125393867492676
Epoch 1770, val loss: 0.9477630257606506
Epoch 1780, training loss: 457.4141540527344 = 0.9423145055770874 + 50.0 * 9.129436492919922
Epoch 1780, val loss: 0.9465077519416809
Epoch 1790, training loss: 457.40118408203125 = 0.9409807324409485 + 50.0 * 9.129203796386719
Epoch 1790, val loss: 0.9452394843101501
Epoch 1800, training loss: 457.485595703125 = 0.939655065536499 + 50.0 * 9.130918502807617
Epoch 1800, val loss: 0.9440040588378906
Epoch 1810, training loss: 457.63970947265625 = 0.9383214712142944 + 50.0 * 9.134027481079102
Epoch 1810, val loss: 0.9427506923675537
Epoch 1820, training loss: 457.5832214355469 = 0.9369994401931763 + 50.0 * 9.13292407989502
Epoch 1820, val loss: 0.9414960741996765
Epoch 1830, training loss: 457.8176574707031 = 0.9356909394264221 + 50.0 * 9.137639045715332
Epoch 1830, val loss: 0.9402757883071899
Epoch 1840, training loss: 457.7433776855469 = 0.934373140335083 + 50.0 * 9.13617992401123
Epoch 1840, val loss: 0.9390254020690918
Epoch 1850, training loss: 457.9134826660156 = 0.9330822229385376 + 50.0 * 9.139608383178711
Epoch 1850, val loss: 0.9378255605697632
Epoch 1860, training loss: 457.9581298828125 = 0.9318101406097412 + 50.0 * 9.140525817871094
Epoch 1860, val loss: 0.9366103410720825
Epoch 1870, training loss: 457.82220458984375 = 0.9304888844490051 + 50.0 * 9.137834548950195
Epoch 1870, val loss: 0.9353939890861511
Epoch 1880, training loss: 455.62738037109375 = 0.9290200471878052 + 50.0 * 9.09396743774414
Epoch 1880, val loss: 0.9339677691459656
Epoch 1890, training loss: 456.4302062988281 = 0.9277346134185791 + 50.0 * 9.1100492477417
Epoch 1890, val loss: 0.9327846765518188
Epoch 1900, training loss: 456.45458984375 = 0.9265153408050537 + 50.0 * 9.11056137084961
Epoch 1900, val loss: 0.9316621422767639
Epoch 1910, training loss: 456.92529296875 = 0.9253038763999939 + 50.0 * 9.119999885559082
Epoch 1910, val loss: 0.930570662021637
Epoch 1920, training loss: 456.1090393066406 = 0.9240178465843201 + 50.0 * 9.103700637817383
Epoch 1920, val loss: 0.9293396472930908
Epoch 1930, training loss: 456.50360107421875 = 0.9228968620300293 + 50.0 * 9.111614227294922
Epoch 1930, val loss: 0.928324282169342
Epoch 1940, training loss: 456.92242431640625 = 0.9217348694801331 + 50.0 * 9.120014190673828
Epoch 1940, val loss: 0.927232027053833
Epoch 1950, training loss: 457.4422912597656 = 0.9205940365791321 + 50.0 * 9.130434036254883
Epoch 1950, val loss: 0.9261713027954102
Epoch 1960, training loss: 457.7575378417969 = 0.919402539730072 + 50.0 * 9.136762619018555
Epoch 1960, val loss: 0.9250690340995789
Epoch 1970, training loss: 457.786865234375 = 0.9181960821151733 + 50.0 * 9.137373924255371
Epoch 1970, val loss: 0.923941433429718
Epoch 1980, training loss: 457.858154296875 = 0.9170123934745789 + 50.0 * 9.138822555541992
Epoch 1980, val loss: 0.9228351712226868
Epoch 1990, training loss: 457.8483581542969 = 0.9158115386962891 + 50.0 * 9.138650894165039
Epoch 1990, val loss: 0.9217277765274048
Epoch 2000, training loss: 457.93707275390625 = 0.9146255254745483 + 50.0 * 9.140449523925781
Epoch 2000, val loss: 0.9206320643424988
Epoch 2010, training loss: 458.07421875 = 0.9134454131126404 + 50.0 * 9.14321517944336
Epoch 2010, val loss: 0.9195379614830017
Epoch 2020, training loss: 458.009765625 = 0.9122412204742432 + 50.0 * 9.141950607299805
Epoch 2020, val loss: 0.9184393882751465
Epoch 2030, training loss: 458.08636474609375 = 0.9110731482505798 + 50.0 * 9.143506050109863
Epoch 2030, val loss: 0.9173708558082581
Epoch 2040, training loss: 458.3631286621094 = 0.9099212288856506 + 50.0 * 9.149064064025879
Epoch 2040, val loss: 0.9162917733192444
Epoch 2050, training loss: 458.4660949707031 = 0.9087509512901306 + 50.0 * 9.15114688873291
Epoch 2050, val loss: 0.9152221083641052
Epoch 2060, training loss: 457.8591613769531 = 0.907508134841919 + 50.0 * 9.139033317565918
Epoch 2060, val loss: 0.9140061736106873
Epoch 2070, training loss: 457.7078857421875 = 0.9062735438346863 + 50.0 * 9.136032104492188
Epoch 2070, val loss: 0.9129378199577332
Epoch 2080, training loss: 457.82818603515625 = 0.9051218628883362 + 50.0 * 9.138461112976074
Epoch 2080, val loss: 0.9118679165840149
Epoch 2090, training loss: 458.2678527832031 = 0.9040545225143433 + 50.0 * 9.147275924682617
Epoch 2090, val loss: 0.9108962416648865
Epoch 2100, training loss: 458.595458984375 = 0.9029716849327087 + 50.0 * 9.153849601745605
Epoch 2100, val loss: 0.9099007248878479
Epoch 2110, training loss: 458.5062561035156 = 0.9018447995185852 + 50.0 * 9.152088165283203
Epoch 2110, val loss: 0.9088761806488037
Epoch 2120, training loss: 458.5855712890625 = 0.9007352590560913 + 50.0 * 9.15369701385498
Epoch 2120, val loss: 0.9078498482704163
Epoch 2130, training loss: 458.4126281738281 = 0.8995290398597717 + 50.0 * 9.150261878967285
Epoch 2130, val loss: 0.9067347049713135
Epoch 2140, training loss: 458.1503601074219 = 0.8983907699584961 + 50.0 * 9.145039558410645
Epoch 2140, val loss: 0.9057039618492126
Epoch 2150, training loss: 458.2730712890625 = 0.8973031044006348 + 50.0 * 9.147515296936035
Epoch 2150, val loss: 0.9047411680221558
Epoch 2160, training loss: 458.6044921875 = 0.8962844610214233 + 50.0 * 9.15416431427002
Epoch 2160, val loss: 0.9038318395614624
Epoch 2170, training loss: 458.8949890136719 = 0.8952403664588928 + 50.0 * 9.159995079040527
Epoch 2170, val loss: 0.9028834104537964
Epoch 2180, training loss: 458.8504638671875 = 0.8941642642021179 + 50.0 * 9.159126281738281
Epoch 2180, val loss: 0.9019020199775696
Epoch 2190, training loss: 459.7807312011719 = 0.8931067585945129 + 50.0 * 9.177752494812012
Epoch 2190, val loss: 0.9009493589401245
Epoch 2200, training loss: 459.0097351074219 = 0.8920655250549316 + 50.0 * 9.162353515625
Epoch 2200, val loss: 0.9000004529953003
Epoch 2210, training loss: 459.103515625 = 0.8910757303237915 + 50.0 * 9.1642484664917
Epoch 2210, val loss: 0.8990920782089233
Epoch 2220, training loss: 459.7334899902344 = 0.8900853395462036 + 50.0 * 9.176868438720703
Epoch 2220, val loss: 0.8982118368148804
Epoch 2230, training loss: 460.1233825683594 = 0.8890925645828247 + 50.0 * 9.184685707092285
Epoch 2230, val loss: 0.8973148465156555
Epoch 2240, training loss: 460.5209045410156 = 0.8881029486656189 + 50.0 * 9.192656517028809
Epoch 2240, val loss: 0.8964225649833679
Epoch 2250, training loss: 460.4369812011719 = 0.8870849013328552 + 50.0 * 9.190998077392578
Epoch 2250, val loss: 0.8955012559890747
Epoch 2260, training loss: 460.537109375 = 0.8860907554626465 + 50.0 * 9.193020820617676
Epoch 2260, val loss: 0.894610583782196
Epoch 2270, training loss: 460.6649475097656 = 0.885084331035614 + 50.0 * 9.195597648620605
Epoch 2270, val loss: 0.8937198519706726
Epoch 2280, training loss: 460.77960205078125 = 0.8840983510017395 + 50.0 * 9.19791030883789
Epoch 2280, val loss: 0.8928331136703491
Epoch 2290, training loss: 460.7952880859375 = 0.8831050992012024 + 50.0 * 9.198243141174316
Epoch 2290, val loss: 0.8919508457183838
Epoch 2300, training loss: 460.81463623046875 = 0.8821246027946472 + 50.0 * 9.198650360107422
Epoch 2300, val loss: 0.8910772204399109
Epoch 2310, training loss: 461.0657043457031 = 0.8811590671539307 + 50.0 * 9.203690528869629
Epoch 2310, val loss: 0.8902199864387512
Epoch 2320, training loss: 461.0777587890625 = 0.8801658749580383 + 50.0 * 9.203951835632324
Epoch 2320, val loss: 0.8893447518348694
Epoch 2330, training loss: 461.1954345703125 = 0.8792115449905396 + 50.0 * 9.206324577331543
Epoch 2330, val loss: 0.8884900212287903
Epoch 2340, training loss: 461.3139953613281 = 0.878259003162384 + 50.0 * 9.208714485168457
Epoch 2340, val loss: 0.8876365423202515
Epoch 2350, training loss: 461.3116760253906 = 0.8773007392883301 + 50.0 * 9.208687782287598
Epoch 2350, val loss: 0.8867858648300171
Epoch 2360, training loss: 461.3082580566406 = 0.8763384819030762 + 50.0 * 9.208638191223145
Epoch 2360, val loss: 0.8859383463859558
Epoch 2370, training loss: 460.5541687011719 = 0.8753290176391602 + 50.0 * 9.19357681274414
Epoch 2370, val loss: 0.8850575685501099
Epoch 2380, training loss: 460.64495849609375 = 0.8743997812271118 + 50.0 * 9.195411682128906
Epoch 2380, val loss: 0.8842401504516602
Epoch 2390, training loss: 460.81024169921875 = 0.8734663724899292 + 50.0 * 9.198735237121582
Epoch 2390, val loss: 0.8834020495414734
Epoch 2400, training loss: 461.1601867675781 = 0.8725576400756836 + 50.0 * 9.2057523727417
Epoch 2400, val loss: 0.8826037645339966
Epoch 2410, training loss: 461.5148620605469 = 0.8716723918914795 + 50.0 * 9.21286392211914
Epoch 2410, val loss: 0.8818212151527405
Epoch 2420, training loss: 461.75823974609375 = 0.8707724213600159 + 50.0 * 9.21774959564209
Epoch 2420, val loss: 0.8810332417488098
Epoch 2430, training loss: 461.49176025390625 = 0.8698412179946899 + 50.0 * 9.212438583374023
Epoch 2430, val loss: 0.8802050352096558
Epoch 2440, training loss: 461.5124206542969 = 0.8689393997192383 + 50.0 * 9.212869644165039
Epoch 2440, val loss: 0.8794257044792175
Epoch 2450, training loss: 461.8788757324219 = 0.8680654764175415 + 50.0 * 9.220215797424316
Epoch 2450, val loss: 0.8786552548408508
Epoch 2460, training loss: 461.85333251953125 = 0.8671600818634033 + 50.0 * 9.21972370147705
Epoch 2460, val loss: 0.8778674006462097
Epoch 2470, training loss: 461.8032531738281 = 0.8662717938423157 + 50.0 * 9.21873950958252
Epoch 2470, val loss: 0.8770954012870789
Epoch 2480, training loss: 461.83544921875 = 0.865394651889801 + 50.0 * 9.219401359558105
Epoch 2480, val loss: 0.8763168454170227
Epoch 2490, training loss: 461.6083984375 = 0.8645026683807373 + 50.0 * 9.21487808227539
Epoch 2490, val loss: 0.8755387663841248
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.5392753623188405
0.8159095848728538
=== training gcn model ===
Epoch 0, training loss: 515.415771484375 = 1.1095666885375977 + 50.0 * 10.286124229431152
Epoch 0, val loss: 1.1086986064910889
Epoch 10, training loss: 496.1497497558594 = 1.1089950799942017 + 50.0 * 9.9008150100708
Epoch 10, val loss: 1.1081295013427734
Epoch 20, training loss: 487.2866516113281 = 1.1085824966430664 + 50.0 * 9.72356128692627
Epoch 20, val loss: 1.1077076196670532
Epoch 30, training loss: 481.09735107421875 = 1.1080851554870605 + 50.0 * 9.599784851074219
Epoch 30, val loss: 1.107201337814331
Epoch 40, training loss: 476.20849609375 = 1.1075915098190308 + 50.0 * 9.502017974853516
Epoch 40, val loss: 1.1066933870315552
Epoch 50, training loss: 472.1326599121094 = 1.1070958375930786 + 50.0 * 9.420511245727539
Epoch 50, val loss: 1.1061890125274658
Epoch 60, training loss: 468.56939697265625 = 1.1065768003463745 + 50.0 * 9.34925651550293
Epoch 60, val loss: 1.1056569814682007
Epoch 70, training loss: 465.4181823730469 = 1.1060453653335571 + 50.0 * 9.286242485046387
Epoch 70, val loss: 1.105108380317688
Epoch 80, training loss: 462.65032958984375 = 1.1054797172546387 + 50.0 * 9.230896949768066
Epoch 80, val loss: 1.1045281887054443
Epoch 90, training loss: 460.2049865722656 = 1.1048873662948608 + 50.0 * 9.182002067565918
Epoch 90, val loss: 1.1039170026779175
Epoch 100, training loss: 458.01007080078125 = 1.1042805910110474 + 50.0 * 9.138115882873535
Epoch 100, val loss: 1.1032954454421997
Epoch 110, training loss: 456.0804748535156 = 1.103644609451294 + 50.0 * 9.099536895751953
Epoch 110, val loss: 1.1026471853256226
Epoch 120, training loss: 454.348388671875 = 1.1029964685440063 + 50.0 * 9.064908027648926
Epoch 120, val loss: 1.101993441581726
Epoch 130, training loss: 453.0064392089844 = 1.1022921800613403 + 50.0 * 9.03808307647705
Epoch 130, val loss: 1.1012738943099976
Epoch 140, training loss: 451.72198486328125 = 1.1016285419464111 + 50.0 * 9.012407302856445
Epoch 140, val loss: 1.1006051301956177
Epoch 150, training loss: 450.3623352050781 = 1.1008650064468384 + 50.0 * 8.9852294921875
Epoch 150, val loss: 1.0998347997665405
Epoch 160, training loss: 449.3326721191406 = 1.1001383066177368 + 50.0 * 8.964651107788086
Epoch 160, val loss: 1.0990993976593018
Epoch 170, training loss: 448.451904296875 = 1.0993715524673462 + 50.0 * 8.947051048278809
Epoch 170, val loss: 1.0983279943466187
Epoch 180, training loss: 447.61041259765625 = 1.098583698272705 + 50.0 * 8.93023681640625
Epoch 180, val loss: 1.0975395441055298
Epoch 190, training loss: 446.8838806152344 = 1.0977740287780762 + 50.0 * 8.915721893310547
Epoch 190, val loss: 1.0967305898666382
Epoch 200, training loss: 446.30194091796875 = 1.0969455242156982 + 50.0 * 8.904099464416504
Epoch 200, val loss: 1.095900297164917
Epoch 210, training loss: 445.7018127441406 = 1.0960806608200073 + 50.0 * 8.892114639282227
Epoch 210, val loss: 1.095054030418396
Epoch 220, training loss: 445.06732177734375 = 1.0952191352844238 + 50.0 * 8.87944221496582
Epoch 220, val loss: 1.094190239906311
Epoch 230, training loss: 444.7418518066406 = 1.0943524837493896 + 50.0 * 8.872949600219727
Epoch 230, val loss: 1.093327283859253
Epoch 240, training loss: 444.3813781738281 = 1.09347403049469 + 50.0 * 8.865757942199707
Epoch 240, val loss: 1.0924620628356934
Epoch 250, training loss: 444.2396545410156 = 1.0926058292388916 + 50.0 * 8.862940788269043
Epoch 250, val loss: 1.0915954113006592
Epoch 260, training loss: 443.8857116699219 = 1.0916895866394043 + 50.0 * 8.855880737304688
Epoch 260, val loss: 1.0906859636306763
Epoch 270, training loss: 443.65032958984375 = 1.0907840728759766 + 50.0 * 8.851190567016602
Epoch 270, val loss: 1.089786410331726
Epoch 280, training loss: 443.6689147949219 = 1.0898313522338867 + 50.0 * 8.851581573486328
Epoch 280, val loss: 1.0888676643371582
Epoch 290, training loss: 443.18280029296875 = 1.0889127254486084 + 50.0 * 8.841877937316895
Epoch 290, val loss: 1.0879780054092407
Epoch 300, training loss: 443.1109924316406 = 1.0880461931228638 + 50.0 * 8.840458869934082
Epoch 300, val loss: 1.087110996246338
Epoch 310, training loss: 442.8465576171875 = 1.0871729850769043 + 50.0 * 8.835187911987305
Epoch 310, val loss: 1.0862727165222168
Epoch 320, training loss: 442.8028259277344 = 1.0863478183746338 + 50.0 * 8.834329605102539
Epoch 320, val loss: 1.0854755640029907
Epoch 330, training loss: 442.58056640625 = 1.0855450630187988 + 50.0 * 8.829900741577148
Epoch 330, val loss: 1.084694266319275
Epoch 340, training loss: 442.678466796875 = 1.0847952365875244 + 50.0 * 8.831872940063477
Epoch 340, val loss: 1.0839604139328003
Epoch 350, training loss: 442.7250671386719 = 1.0840871334075928 + 50.0 * 8.832819938659668
Epoch 350, val loss: 1.0832713842391968
Epoch 360, training loss: 442.6396789550781 = 1.083387851715088 + 50.0 * 8.83112621307373
Epoch 360, val loss: 1.0825998783111572
Epoch 370, training loss: 442.49267578125 = 1.082749843597412 + 50.0 * 8.828198432922363
Epoch 370, val loss: 1.0819836854934692
Epoch 380, training loss: 442.6032409667969 = 1.0821294784545898 + 50.0 * 8.830422401428223
Epoch 380, val loss: 1.0813801288604736
Epoch 390, training loss: 442.6172180175781 = 1.0815186500549316 + 50.0 * 8.830714225769043
Epoch 390, val loss: 1.0807955265045166
Epoch 400, training loss: 442.39239501953125 = 1.080914855003357 + 50.0 * 8.8262300491333
Epoch 400, val loss: 1.0802390575408936
Epoch 410, training loss: 442.3426513671875 = 1.080355167388916 + 50.0 * 8.82524585723877
Epoch 410, val loss: 1.0796993970870972
Epoch 420, training loss: 442.4515380859375 = 1.0797960758209229 + 50.0 * 8.827434539794922
Epoch 420, val loss: 1.079163908958435
Epoch 430, training loss: 442.5624694824219 = 1.0792343616485596 + 50.0 * 8.829665184020996
Epoch 430, val loss: 1.0786328315734863
Epoch 440, training loss: 442.53460693359375 = 1.0786917209625244 + 50.0 * 8.829117774963379
Epoch 440, val loss: 1.0781158208847046
Epoch 450, training loss: 442.43243408203125 = 1.0781129598617554 + 50.0 * 8.827086448669434
Epoch 450, val loss: 1.0775790214538574
Epoch 460, training loss: 442.61505126953125 = 1.0775749683380127 + 50.0 * 8.83074951171875
Epoch 460, val loss: 1.0770717859268188
Epoch 470, training loss: 442.55810546875 = 1.0770288705825806 + 50.0 * 8.829621315002441
Epoch 470, val loss: 1.076564073562622
Epoch 480, training loss: 442.5576477050781 = 1.0764658451080322 + 50.0 * 8.829623222351074
Epoch 480, val loss: 1.0760349035263062
Epoch 490, training loss: 442.7453308105469 = 1.0759230852127075 + 50.0 * 8.833388328552246
Epoch 490, val loss: 1.075521469116211
Epoch 500, training loss: 442.7457580566406 = 1.075361967086792 + 50.0 * 8.83340835571289
Epoch 500, val loss: 1.0750036239624023
Epoch 510, training loss: 442.7325439453125 = 1.074796199798584 + 50.0 * 8.833154678344727
Epoch 510, val loss: 1.0744894742965698
Epoch 520, training loss: 442.9021911621094 = 1.0742295980453491 + 50.0 * 8.836559295654297
Epoch 520, val loss: 1.0739573240280151
Epoch 530, training loss: 443.0072326660156 = 1.0736926794052124 + 50.0 * 8.83867073059082
Epoch 530, val loss: 1.073461651802063
Epoch 540, training loss: 443.1519470214844 = 1.0731312036514282 + 50.0 * 8.84157657623291
Epoch 540, val loss: 1.0729297399520874
Epoch 550, training loss: 443.2364501953125 = 1.0725518465042114 + 50.0 * 8.843277931213379
Epoch 550, val loss: 1.072399377822876
Epoch 560, training loss: 443.25372314453125 = 1.0719878673553467 + 50.0 * 8.843634605407715
Epoch 560, val loss: 1.071864128112793
Epoch 570, training loss: 443.5155029296875 = 1.0714054107666016 + 50.0 * 8.848881721496582
Epoch 570, val loss: 1.071329951286316
Epoch 580, training loss: 443.591796875 = 1.0708240270614624 + 50.0 * 8.850419044494629
Epoch 580, val loss: 1.0707944631576538
Epoch 590, training loss: 443.89947509765625 = 1.070261836051941 + 50.0 * 8.856584548950195
Epoch 590, val loss: 1.0702788829803467
Epoch 600, training loss: 443.4091796875 = 1.0696046352386475 + 50.0 * 8.84679126739502
Epoch 600, val loss: 1.069671869277954
Epoch 610, training loss: 443.576904296875 = 1.0690196752548218 + 50.0 * 8.850157737731934
Epoch 610, val loss: 1.0691360235214233
Epoch 620, training loss: 443.6948547363281 = 1.0684314966201782 + 50.0 * 8.85252857208252
Epoch 620, val loss: 1.0685827732086182
Epoch 630, training loss: 443.8049011230469 = 1.0678244829177856 + 50.0 * 8.854742050170898
Epoch 630, val loss: 1.0680243968963623
Epoch 640, training loss: 443.747314453125 = 1.0672115087509155 + 50.0 * 8.853602409362793
Epoch 640, val loss: 1.0674654245376587
Epoch 650, training loss: 443.8656921386719 = 1.0665892362594604 + 50.0 * 8.855981826782227
Epoch 650, val loss: 1.0669057369232178
Epoch 660, training loss: 444.14154052734375 = 1.0659745931625366 + 50.0 * 8.86151123046875
Epoch 660, val loss: 1.06633460521698
Epoch 670, training loss: 444.06182861328125 = 1.065334439277649 + 50.0 * 8.859930038452148
Epoch 670, val loss: 1.065744161605835
Epoch 680, training loss: 444.1973571777344 = 1.0647368431091309 + 50.0 * 8.862652778625488
Epoch 680, val loss: 1.0651856660842896
Epoch 690, training loss: 444.43865966796875 = 1.0641050338745117 + 50.0 * 8.867490768432617
Epoch 690, val loss: 1.064603567123413
Epoch 700, training loss: 444.51849365234375 = 1.0634710788726807 + 50.0 * 8.869100570678711
Epoch 700, val loss: 1.064018964767456
Epoch 710, training loss: 444.0661926269531 = 1.0627646446228027 + 50.0 * 8.860068321228027
Epoch 710, val loss: 1.0633429288864136
Epoch 720, training loss: 444.2745056152344 = 1.062111496925354 + 50.0 * 8.864248275756836
Epoch 720, val loss: 1.0627567768096924
Epoch 730, training loss: 444.45123291015625 = 1.061487078666687 + 50.0 * 8.86779499053955
Epoch 730, val loss: 1.0621696710586548
Epoch 740, training loss: 444.6917419433594 = 1.0608503818511963 + 50.0 * 8.872617721557617
Epoch 740, val loss: 1.0615795850753784
Epoch 750, training loss: 444.76934814453125 = 1.0601788759231567 + 50.0 * 8.874183654785156
Epoch 750, val loss: 1.06096613407135
Epoch 760, training loss: 444.86029052734375 = 1.0595015287399292 + 50.0 * 8.876015663146973
Epoch 760, val loss: 1.060349464416504
Epoch 770, training loss: 444.92596435546875 = 1.058821678161621 + 50.0 * 8.87734317779541
Epoch 770, val loss: 1.059726595878601
Epoch 780, training loss: 445.0790710449219 = 1.0581352710723877 + 50.0 * 8.88041877746582
Epoch 780, val loss: 1.0590800046920776
Epoch 790, training loss: 445.1586608886719 = 1.0574321746826172 + 50.0 * 8.882024765014648
Epoch 790, val loss: 1.0584359169006348
Epoch 800, training loss: 445.3226623535156 = 1.0567604303359985 + 50.0 * 8.8853178024292
Epoch 800, val loss: 1.0578091144561768
Epoch 810, training loss: 445.6454772949219 = 1.056071162223816 + 50.0 * 8.891788482666016
Epoch 810, val loss: 1.0571833848953247
Epoch 820, training loss: 445.59613037109375 = 1.055350422859192 + 50.0 * 8.890815734863281
Epoch 820, val loss: 1.0565147399902344
Epoch 830, training loss: 445.6550598144531 = 1.05464768409729 + 50.0 * 8.892007827758789
Epoch 830, val loss: 1.0558570623397827
Epoch 840, training loss: 445.81536865234375 = 1.0539402961730957 + 50.0 * 8.895228385925293
Epoch 840, val loss: 1.055199384689331
Epoch 850, training loss: 445.9610900878906 = 1.0532244443893433 + 50.0 * 8.898157119750977
Epoch 850, val loss: 1.0545638799667358
Epoch 860, training loss: 445.9363098144531 = 1.0525108575820923 + 50.0 * 8.897675514221191
Epoch 860, val loss: 1.0538678169250488
Epoch 870, training loss: 445.9900817871094 = 1.0517665147781372 + 50.0 * 8.89876651763916
Epoch 870, val loss: 1.0531606674194336
Epoch 880, training loss: 446.1236267089844 = 1.05105459690094 + 50.0 * 8.901451110839844
Epoch 880, val loss: 1.052528738975525
Epoch 890, training loss: 446.2550048828125 = 1.0503418445587158 + 50.0 * 8.904092788696289
Epoch 890, val loss: 1.0518752336502075
Epoch 900, training loss: 446.3969421386719 = 1.0496214628219604 + 50.0 * 8.906946182250977
Epoch 900, val loss: 1.0512022972106934
Epoch 910, training loss: 446.4461669921875 = 1.04887855052948 + 50.0 * 8.90794563293457
Epoch 910, val loss: 1.0505338907241821
Epoch 920, training loss: 446.4969482421875 = 1.0481388568878174 + 50.0 * 8.908976554870605
Epoch 920, val loss: 1.0498530864715576
Epoch 930, training loss: 446.5476379394531 = 1.0473929643630981 + 50.0 * 8.910004615783691
Epoch 930, val loss: 1.0491679906845093
Epoch 940, training loss: 446.9363098144531 = 1.0466747283935547 + 50.0 * 8.917793273925781
Epoch 940, val loss: 1.0485121011734009
Epoch 950, training loss: 446.9942321777344 = 1.0459412336349487 + 50.0 * 8.918966293334961
Epoch 950, val loss: 1.047812581062317
Epoch 960, training loss: 446.98809814453125 = 1.0451728105545044 + 50.0 * 8.918858528137207
Epoch 960, val loss: 1.0471103191375732
Epoch 970, training loss: 446.86163330078125 = 1.0444214344024658 + 50.0 * 8.916343688964844
Epoch 970, val loss: 1.0464110374450684
Epoch 980, training loss: 447.141357421875 = 1.0436933040618896 + 50.0 * 8.921953201293945
Epoch 980, val loss: 1.0457290410995483
Epoch 990, training loss: 447.0397644042969 = 1.0429199934005737 + 50.0 * 8.919937133789062
Epoch 990, val loss: 1.0450351238250732
Epoch 1000, training loss: 447.2536926269531 = 1.0422042608261108 + 50.0 * 8.924229621887207
Epoch 1000, val loss: 1.0443536043167114
Epoch 1010, training loss: 447.57879638671875 = 1.0414514541625977 + 50.0 * 8.930747032165527
Epoch 1010, val loss: 1.0436389446258545
Epoch 1020, training loss: 447.5484924316406 = 1.0406544208526611 + 50.0 * 8.930156707763672
Epoch 1020, val loss: 1.0429192781448364
Epoch 1030, training loss: 447.571533203125 = 1.0399119853973389 + 50.0 * 8.930632591247559
Epoch 1030, val loss: 1.0422202348709106
Epoch 1040, training loss: 447.7850646972656 = 1.0391533374786377 + 50.0 * 8.934918403625488
Epoch 1040, val loss: 1.0415058135986328
Epoch 1050, training loss: 447.5992431640625 = 1.0383375883102417 + 50.0 * 8.931218147277832
Epoch 1050, val loss: 1.040763258934021
Epoch 1060, training loss: 446.8268737792969 = 1.037451148033142 + 50.0 * 8.915788650512695
Epoch 1060, val loss: 1.0399638414382935
Epoch 1070, training loss: 446.777099609375 = 1.036693811416626 + 50.0 * 8.91480827331543
Epoch 1070, val loss: 1.039241909980774
Epoch 1080, training loss: 447.6228332519531 = 1.036029577255249 + 50.0 * 8.93173599243164
Epoch 1080, val loss: 1.038633942604065
Epoch 1090, training loss: 447.6961669921875 = 1.035271406173706 + 50.0 * 8.933218002319336
Epoch 1090, val loss: 1.0379558801651
Epoch 1100, training loss: 447.7942199707031 = 1.0344866514205933 + 50.0 * 8.935194969177246
Epoch 1100, val loss: 1.037233829498291
Epoch 1110, training loss: 448.0179443359375 = 1.033737301826477 + 50.0 * 8.93968391418457
Epoch 1110, val loss: 1.0365417003631592
Epoch 1120, training loss: 448.1941223144531 = 1.0329643487930298 + 50.0 * 8.943222999572754
Epoch 1120, val loss: 1.0358151197433472
Epoch 1130, training loss: 448.3334045410156 = 1.0321816205978394 + 50.0 * 8.946023941040039
Epoch 1130, val loss: 1.0350768566131592
Epoch 1140, training loss: 448.5697326660156 = 1.031389832496643 + 50.0 * 8.950766563415527
Epoch 1140, val loss: 1.0343595743179321
Epoch 1150, training loss: 448.29510498046875 = 1.0305284261703491 + 50.0 * 8.945291519165039
Epoch 1150, val loss: 1.033565640449524
Epoch 1160, training loss: 448.0530700683594 = 1.0297311544418335 + 50.0 * 8.94046688079834
Epoch 1160, val loss: 1.0328196287155151
Epoch 1170, training loss: 448.4403076171875 = 1.028960108757019 + 50.0 * 8.948226928710938
Epoch 1170, val loss: 1.03213369846344
Epoch 1180, training loss: 448.9192810058594 = 1.0281850099563599 + 50.0 * 8.9578218460083
Epoch 1180, val loss: 1.0314058065414429
Epoch 1190, training loss: 448.8506164550781 = 1.0273534059524536 + 50.0 * 8.956465721130371
Epoch 1190, val loss: 1.030632734298706
Epoch 1200, training loss: 449.0071105957031 = 1.0265452861785889 + 50.0 * 8.959610939025879
Epoch 1200, val loss: 1.029897689819336
Epoch 1210, training loss: 449.1310119628906 = 1.0257229804992676 + 50.0 * 8.962105751037598
Epoch 1210, val loss: 1.0291483402252197
Epoch 1220, training loss: 449.3316650390625 = 1.0248979330062866 + 50.0 * 8.966135025024414
Epoch 1220, val loss: 1.0284044742584229
Epoch 1230, training loss: 448.94921875 = 1.0239369869232178 + 50.0 * 8.958505630493164
Epoch 1230, val loss: 1.0275259017944336
Epoch 1240, training loss: 448.07794189453125 = 1.0231692790985107 + 50.0 * 8.941095352172852
Epoch 1240, val loss: 1.0267747640609741
Epoch 1250, training loss: 448.3590087890625 = 1.0222870111465454 + 50.0 * 8.946734428405762
Epoch 1250, val loss: 1.0259780883789062
Epoch 1260, training loss: 448.3603515625 = 1.021480679512024 + 50.0 * 8.94677734375
Epoch 1260, val loss: 1.0252820253372192
Epoch 1270, training loss: 448.6932678222656 = 1.0207256078720093 + 50.0 * 8.953451156616211
Epoch 1270, val loss: 1.0245660543441772
Epoch 1280, training loss: 449.2996826171875 = 1.0199511051177979 + 50.0 * 8.965594291687012
Epoch 1280, val loss: 1.023858666419983
Epoch 1290, training loss: 449.8650817871094 = 1.0191395282745361 + 50.0 * 8.976919174194336
Epoch 1290, val loss: 1.023112177848816
Epoch 1300, training loss: 450.0515441894531 = 1.0182819366455078 + 50.0 * 8.98066520690918
Epoch 1300, val loss: 1.0223174095153809
Epoch 1310, training loss: 450.10162353515625 = 1.0174301862716675 + 50.0 * 8.981683731079102
Epoch 1310, val loss: 1.0215429067611694
Epoch 1320, training loss: 450.1187438964844 = 1.016575813293457 + 50.0 * 8.982043266296387
Epoch 1320, val loss: 1.0207704305648804
Epoch 1330, training loss: 450.47601318359375 = 1.015749454498291 + 50.0 * 8.989205360412598
Epoch 1330, val loss: 1.0200176239013672
Epoch 1340, training loss: 450.7433776855469 = 1.0148907899856567 + 50.0 * 8.994569778442383
Epoch 1340, val loss: 1.0192395448684692
Epoch 1350, training loss: 450.9167175292969 = 1.0140339136123657 + 50.0 * 8.998053550720215
Epoch 1350, val loss: 1.0184420347213745
Epoch 1360, training loss: 450.838623046875 = 1.0131436586380005 + 50.0 * 8.996509552001953
Epoch 1360, val loss: 1.0176628828048706
Epoch 1370, training loss: 450.94488525390625 = 1.0122911930084229 + 50.0 * 8.998651504516602
Epoch 1370, val loss: 1.0168876647949219
Epoch 1380, training loss: 451.213623046875 = 1.0114542245864868 + 50.0 * 9.004043579101562
Epoch 1380, val loss: 1.0161057710647583
Epoch 1390, training loss: 450.9063415527344 = 1.0105373859405518 + 50.0 * 8.997916221618652
Epoch 1390, val loss: 1.0152703523635864
Epoch 1400, training loss: 451.0791931152344 = 1.0096677541732788 + 50.0 * 9.00139045715332
Epoch 1400, val loss: 1.014480471611023
Epoch 1410, training loss: 451.0897521972656 = 1.0088123083114624 + 50.0 * 9.001618385314941
Epoch 1410, val loss: 1.0136762857437134
Epoch 1420, training loss: 451.28363037109375 = 1.0079246759414673 + 50.0 * 9.005514144897461
Epoch 1420, val loss: 1.0129142999649048
Epoch 1430, training loss: 451.3163146972656 = 1.00705087184906 + 50.0 * 9.006185531616211
Epoch 1430, val loss: 1.0121264457702637
Epoch 1440, training loss: 451.58270263671875 = 1.0061792135238647 + 50.0 * 9.011530876159668
Epoch 1440, val loss: 1.0113388299942017
Epoch 1450, training loss: 451.7724609375 = 1.00529146194458 + 50.0 * 9.01534366607666
Epoch 1450, val loss: 1.0105496644973755
Epoch 1460, training loss: 451.9065856933594 = 1.004356861114502 + 50.0 * 9.018044471740723
Epoch 1460, val loss: 1.0097014904022217
Epoch 1470, training loss: 451.7253723144531 = 1.003476858139038 + 50.0 * 9.014437675476074
Epoch 1470, val loss: 1.0088931322097778
Epoch 1480, training loss: 451.8984069824219 = 1.0025479793548584 + 50.0 * 9.01791763305664
Epoch 1480, val loss: 1.0080571174621582
Epoch 1490, training loss: 451.5041198730469 = 1.0016226768493652 + 50.0 * 9.010049819946289
Epoch 1490, val loss: 1.0071978569030762
Epoch 1500, training loss: 451.75860595703125 = 1.0007288455963135 + 50.0 * 9.015157699584961
Epoch 1500, val loss: 1.0064016580581665
Epoch 1510, training loss: 451.7162780761719 = 0.9998042583465576 + 50.0 * 9.014328956604004
Epoch 1510, val loss: 1.005584716796875
Epoch 1520, training loss: 452.1260681152344 = 0.9989354610443115 + 50.0 * 9.022542953491211
Epoch 1520, val loss: 1.0048034191131592
Epoch 1530, training loss: 452.4776611328125 = 0.9980488419532776 + 50.0 * 9.029592514038086
Epoch 1530, val loss: 1.004020094871521
Epoch 1540, training loss: 452.4870910644531 = 0.9971328973770142 + 50.0 * 9.029799461364746
Epoch 1540, val loss: 1.0032013654708862
Epoch 1550, training loss: 452.5646057128906 = 0.9962315559387207 + 50.0 * 9.031367301940918
Epoch 1550, val loss: 1.0024006366729736
Epoch 1560, training loss: 452.92303466796875 = 0.9953453540802002 + 50.0 * 9.038554191589355
Epoch 1560, val loss: 1.0015865564346313
Epoch 1570, training loss: 452.91729736328125 = 0.9944072365760803 + 50.0 * 9.038457870483398
Epoch 1570, val loss: 1.000744342803955
Epoch 1580, training loss: 452.8669738769531 = 0.9934589862823486 + 50.0 * 9.037469863891602
Epoch 1580, val loss: 0.9998961091041565
Epoch 1590, training loss: 453.05712890625 = 0.9925534129142761 + 50.0 * 9.041291236877441
Epoch 1590, val loss: 0.9990977644920349
Epoch 1600, training loss: 453.0281066894531 = 0.9916285872459412 + 50.0 * 9.040729522705078
Epoch 1600, val loss: 0.9982779026031494
Epoch 1610, training loss: 453.16796875 = 0.9907152652740479 + 50.0 * 9.04354476928711
Epoch 1610, val loss: 0.9974619746208191
Epoch 1620, training loss: 453.4605407714844 = 0.9898046255111694 + 50.0 * 9.04941463470459
Epoch 1620, val loss: 0.99665367603302
Epoch 1630, training loss: 453.4010314941406 = 0.9888811111450195 + 50.0 * 9.048242568969727
Epoch 1630, val loss: 0.9958441853523254
Epoch 1640, training loss: 453.04583740234375 = 0.9879122376441956 + 50.0 * 9.041158676147461
Epoch 1640, val loss: 0.9949710965156555
Epoch 1650, training loss: 452.7167053222656 = 0.9869402647018433 + 50.0 * 9.034595489501953
Epoch 1650, val loss: 0.994102954864502
Epoch 1660, training loss: 452.9075927734375 = 0.9860317707061768 + 50.0 * 9.038431167602539
Epoch 1660, val loss: 0.9932997226715088
Epoch 1670, training loss: 453.2268371582031 = 0.9851238131523132 + 50.0 * 9.04483413696289
Epoch 1670, val loss: 0.9925277829170227
Epoch 1680, training loss: 453.63458251953125 = 0.9842695593833923 + 50.0 * 9.053006172180176
Epoch 1680, val loss: 0.9917528629302979
Epoch 1690, training loss: 453.78411865234375 = 0.9833826422691345 + 50.0 * 9.056015014648438
Epoch 1690, val loss: 0.9909365773200989
Epoch 1700, training loss: 453.2966003417969 = 0.9824599027633667 + 50.0 * 9.046282768249512
Epoch 1700, val loss: 0.9901806712150574
Epoch 1710, training loss: 453.15411376953125 = 0.9815454483032227 + 50.0 * 9.043451309204102
Epoch 1710, val loss: 0.9893263578414917
Epoch 1720, training loss: 453.63177490234375 = 0.9807953834533691 + 50.0 * 9.053019523620605
Epoch 1720, val loss: 0.9886953234672546
Epoch 1730, training loss: 453.9432678222656 = 0.9799123406410217 + 50.0 * 9.059267044067383
Epoch 1730, val loss: 0.9879207611083984
Epoch 1740, training loss: 454.24652099609375 = 0.9790053963661194 + 50.0 * 9.065350532531738
Epoch 1740, val loss: 0.9871142506599426
Epoch 1750, training loss: 453.16412353515625 = 0.9780257940292358 + 50.0 * 9.043722152709961
Epoch 1750, val loss: 0.9862886667251587
Epoch 1760, training loss: 452.7681884765625 = 0.9771341681480408 + 50.0 * 9.035820960998535
Epoch 1760, val loss: 0.9854812026023865
Epoch 1770, training loss: 453.13916015625 = 0.9762917757034302 + 50.0 * 9.043257713317871
Epoch 1770, val loss: 0.9847218990325928
Epoch 1780, training loss: 453.66729736328125 = 0.9754738807678223 + 50.0 * 9.053836822509766
Epoch 1780, val loss: 0.9840028285980225
Epoch 1790, training loss: 454.1934814453125 = 0.9746299982070923 + 50.0 * 9.064376831054688
Epoch 1790, val loss: 0.9832732081413269
Epoch 1800, training loss: 454.33087158203125 = 0.973764955997467 + 50.0 * 9.067142486572266
Epoch 1800, val loss: 0.9825087189674377
Epoch 1810, training loss: 454.4115295410156 = 0.9728857278823853 + 50.0 * 9.068772315979004
Epoch 1810, val loss: 0.9817402362823486
Epoch 1820, training loss: 454.5625915527344 = 0.9720115661621094 + 50.0 * 9.07181167602539
Epoch 1820, val loss: 0.9809738397598267
Epoch 1830, training loss: 454.578369140625 = 0.9711229205131531 + 50.0 * 9.072144508361816
Epoch 1830, val loss: 0.9801945686340332
Epoch 1840, training loss: 454.4095153808594 = 0.9702436327934265 + 50.0 * 9.068785667419434
Epoch 1840, val loss: 0.9793907999992371
Epoch 1850, training loss: 454.48797607421875 = 0.9693569540977478 + 50.0 * 9.070372581481934
Epoch 1850, val loss: 0.9786442518234253
Epoch 1860, training loss: 454.7862548828125 = 0.9685037732124329 + 50.0 * 9.07635498046875
Epoch 1860, val loss: 0.9778991937637329
Epoch 1870, training loss: 455.1004943847656 = 0.9676548838615417 + 50.0 * 9.082656860351562
Epoch 1870, val loss: 0.9771525859832764
Epoch 1880, training loss: 455.0842590332031 = 0.9667783975601196 + 50.0 * 9.08234977722168
Epoch 1880, val loss: 0.976392388343811
Epoch 1890, training loss: 454.98016357421875 = 0.9659032821655273 + 50.0 * 9.08028507232666
Epoch 1890, val loss: 0.9756222367286682
Epoch 1900, training loss: 455.30224609375 = 0.9650267958641052 + 50.0 * 9.08674430847168
Epoch 1900, val loss: 0.974856436252594
Epoch 1910, training loss: 455.314697265625 = 0.9641625881195068 + 50.0 * 9.087010383605957
Epoch 1910, val loss: 0.974124550819397
Epoch 1920, training loss: 455.5339660644531 = 0.9633010029792786 + 50.0 * 9.091413497924805
Epoch 1920, val loss: 0.9733754992485046
Epoch 1930, training loss: 455.283203125 = 0.9624307751655579 + 50.0 * 9.08641529083252
Epoch 1930, val loss: 0.972607433795929
Epoch 1940, training loss: 455.3442077636719 = 0.961556613445282 + 50.0 * 9.087653160095215
Epoch 1940, val loss: 0.9718498587608337
Epoch 1950, training loss: 455.3223571777344 = 0.960679292678833 + 50.0 * 9.087233543395996
Epoch 1950, val loss: 0.9711005687713623
Epoch 1960, training loss: 455.5316162109375 = 0.9598301649093628 + 50.0 * 9.091435432434082
Epoch 1960, val loss: 0.9703734517097473
Epoch 1970, training loss: 455.64129638671875 = 0.9589674472808838 + 50.0 * 9.093647003173828
Epoch 1970, val loss: 0.9696359038352966
Epoch 1980, training loss: 455.59344482421875 = 0.9580839276313782 + 50.0 * 9.092707633972168
Epoch 1980, val loss: 0.968878448009491
Epoch 1990, training loss: 455.8736877441406 = 0.9572556614875793 + 50.0 * 9.098328590393066
Epoch 1990, val loss: 0.9681509137153625
Epoch 2000, training loss: 456.1404113769531 = 0.956423282623291 + 50.0 * 9.103679656982422
Epoch 2000, val loss: 0.967438280582428
Epoch 2010, training loss: 455.9293212890625 = 0.9555573463439941 + 50.0 * 9.099474906921387
Epoch 2010, val loss: 0.9666805267333984
Epoch 2020, training loss: 455.5424499511719 = 0.9546980261802673 + 50.0 * 9.091754913330078
Epoch 2020, val loss: 0.9659438729286194
Epoch 2030, training loss: 455.99530029296875 = 0.9538878202438354 + 50.0 * 9.100828170776367
Epoch 2030, val loss: 0.9652525186538696
Epoch 2040, training loss: 456.2684020996094 = 0.953078031539917 + 50.0 * 9.106307029724121
Epoch 2040, val loss: 0.9645679593086243
Epoch 2050, training loss: 456.56298828125 = 0.9522584080696106 + 50.0 * 9.112214088439941
Epoch 2050, val loss: 0.963844358921051
Epoch 2060, training loss: 456.4244079589844 = 0.9514169692993164 + 50.0 * 9.10945987701416
Epoch 2060, val loss: 0.9631384611129761
Epoch 2070, training loss: 456.514892578125 = 0.9505946636199951 + 50.0 * 9.111286163330078
Epoch 2070, val loss: 0.962431788444519
Epoch 2080, training loss: 456.48699951171875 = 0.9497562646865845 + 50.0 * 9.11074447631836
Epoch 2080, val loss: 0.9617254734039307
Epoch 2090, training loss: 456.3088073730469 = 0.9489027857780457 + 50.0 * 9.107197761535645
Epoch 2090, val loss: 0.9609907865524292
Epoch 2100, training loss: 456.47796630859375 = 0.9481019973754883 + 50.0 * 9.110597610473633
Epoch 2100, val loss: 0.9603255987167358
Epoch 2110, training loss: 456.7577819824219 = 0.94728684425354 + 50.0 * 9.116209983825684
Epoch 2110, val loss: 0.9596293568611145
Epoch 2120, training loss: 456.8904724121094 = 0.9464747905731201 + 50.0 * 9.118880271911621
Epoch 2120, val loss: 0.9589136838912964
Epoch 2130, training loss: 456.80670166015625 = 0.9456325769424438 + 50.0 * 9.11722183227539
Epoch 2130, val loss: 0.9582198858261108
Epoch 2140, training loss: 457.0008850097656 = 0.9448245167732239 + 50.0 * 9.121121406555176
Epoch 2140, val loss: 0.9575404524803162
Epoch 2150, training loss: 457.1700744628906 = 0.9440113306045532 + 50.0 * 9.124521255493164
Epoch 2150, val loss: 0.956842303276062
Epoch 2160, training loss: 457.1082763671875 = 0.9431925415992737 + 50.0 * 9.12330150604248
Epoch 2160, val loss: 0.9561483263969421
Epoch 2170, training loss: 457.57159423828125 = 0.9424842596054077 + 50.0 * 9.132582664489746
Epoch 2170, val loss: 0.9555485248565674
Epoch 2180, training loss: 457.3056945800781 = 0.9416162371635437 + 50.0 * 9.127281188964844
Epoch 2180, val loss: 0.954797625541687
Epoch 2190, training loss: 457.01251220703125 = 0.9407677054405212 + 50.0 * 9.121435165405273
Epoch 2190, val loss: 0.9541029334068298
Epoch 2200, training loss: 456.03375244140625 = 0.93987637758255 + 50.0 * 9.101877212524414
Epoch 2200, val loss: 0.9532833695411682
Epoch 2210, training loss: 455.723876953125 = 0.9390870332717896 + 50.0 * 9.095695495605469
Epoch 2210, val loss: 0.9526270031929016
Epoch 2220, training loss: 456.0992431640625 = 0.9383413195610046 + 50.0 * 9.103218078613281
Epoch 2220, val loss: 0.9520143866539001
Epoch 2230, training loss: 456.41754150390625 = 0.9375916719436646 + 50.0 * 9.109599113464355
Epoch 2230, val loss: 0.9514164924621582
Epoch 2240, training loss: 456.84173583984375 = 0.9368455410003662 + 50.0 * 9.118097305297852
Epoch 2240, val loss: 0.950786292552948
Epoch 2250, training loss: 457.1295471191406 = 0.936083197593689 + 50.0 * 9.123868942260742
Epoch 2250, val loss: 0.9501453042030334
Epoch 2260, training loss: 457.4151916503906 = 0.9353174567222595 + 50.0 * 9.129597663879395
Epoch 2260, val loss: 0.9495074152946472
Epoch 2270, training loss: 456.18060302734375 = 0.9344354271888733 + 50.0 * 9.104923248291016
Epoch 2270, val loss: 0.9487545490264893
Epoch 2280, training loss: 456.7767333984375 = 0.9336639642715454 + 50.0 * 9.116861343383789
Epoch 2280, val loss: 0.9481004476547241
Epoch 2290, training loss: 457.13104248046875 = 0.9329261183738708 + 50.0 * 9.12396240234375
Epoch 2290, val loss: 0.9474958777427673
Epoch 2300, training loss: 457.4638671875 = 0.9321907162666321 + 50.0 * 9.130633354187012
Epoch 2300, val loss: 0.9468883275985718
Epoch 2310, training loss: 457.5845947265625 = 0.9314242601394653 + 50.0 * 9.133063316345215
Epoch 2310, val loss: 0.9462394118309021
Epoch 2320, training loss: 457.5923156738281 = 0.9306515455245972 + 50.0 * 9.133233070373535
Epoch 2320, val loss: 0.9456047415733337
Epoch 2330, training loss: 457.7552490234375 = 0.9298923015594482 + 50.0 * 9.136507034301758
Epoch 2330, val loss: 0.9449830055236816
Epoch 2340, training loss: 458.02117919921875 = 0.9291528463363647 + 50.0 * 9.141840934753418
Epoch 2340, val loss: 0.9443591833114624
Epoch 2350, training loss: 457.89947509765625 = 0.9283932447433472 + 50.0 * 9.139421463012695
Epoch 2350, val loss: 0.9437335133552551
Epoch 2360, training loss: 457.8672790527344 = 0.9276595711708069 + 50.0 * 9.138792037963867
Epoch 2360, val loss: 0.9431217312812805
Epoch 2370, training loss: 458.066162109375 = 0.9269139766693115 + 50.0 * 9.14278507232666
Epoch 2370, val loss: 0.9425046443939209
Epoch 2380, training loss: 457.9576110839844 = 0.9261267781257629 + 50.0 * 9.140629768371582
Epoch 2380, val loss: 0.9418198466300964
Epoch 2390, training loss: 458.24652099609375 = 0.9254014492034912 + 50.0 * 9.146422386169434
Epoch 2390, val loss: 0.9412069916725159
Epoch 2400, training loss: 458.3294677734375 = 0.9246108531951904 + 50.0 * 9.148097038269043
Epoch 2400, val loss: 0.9405878186225891
Epoch 2410, training loss: 458.5335998535156 = 0.9238565564155579 + 50.0 * 9.15219497680664
Epoch 2410, val loss: 0.9399582147598267
Epoch 2420, training loss: 458.3543701171875 = 0.9231067299842834 + 50.0 * 9.148625373840332
Epoch 2420, val loss: 0.9393488168716431
Epoch 2430, training loss: 458.3212585449219 = 0.9223688244819641 + 50.0 * 9.147977828979492
Epoch 2430, val loss: 0.9387349486351013
Epoch 2440, training loss: 458.4518737792969 = 0.9216561913490295 + 50.0 * 9.150604248046875
Epoch 2440, val loss: 0.9381455779075623
Epoch 2450, training loss: 458.5483703613281 = 0.9209285974502563 + 50.0 * 9.152548789978027
Epoch 2450, val loss: 0.9375441670417786
Epoch 2460, training loss: 458.65777587890625 = 0.9202084541320801 + 50.0 * 9.154751777648926
Epoch 2460, val loss: 0.936941385269165
Epoch 2470, training loss: 458.6952209472656 = 0.9194855690002441 + 50.0 * 9.15551471710205
Epoch 2470, val loss: 0.9363487362861633
Epoch 2480, training loss: 458.5077209472656 = 0.9187571406364441 + 50.0 * 9.151779174804688
Epoch 2480, val loss: 0.9357433915138245
Epoch 2490, training loss: 458.7611389160156 = 0.9180444478988647 + 50.0 * 9.156862258911133
Epoch 2490, val loss: 0.9351584315299988
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.5292753623188405
0.8148953126132001
The final CL Acc:0.51556, 0.02679, The final GNN Acc:0.81523, 0.00048
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110862])
remove edge: torch.Size([2, 66464])
updated graph: torch.Size([2, 88678])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 513.638427734375 = 1.1080180406570435 + 50.0 * 10.25060749053955
Epoch 0, val loss: 1.10768461227417
Epoch 10, training loss: 493.7689514160156 = 1.1075998544692993 + 50.0 * 9.853226661682129
Epoch 10, val loss: 1.1072548627853394
Epoch 20, training loss: 483.49591064453125 = 1.10712468624115 + 50.0 * 9.647775650024414
Epoch 20, val loss: 1.1067705154418945
Epoch 30, training loss: 475.904052734375 = 1.106642723083496 + 50.0 * 9.495948791503906
Epoch 30, val loss: 1.1062686443328857
Epoch 40, training loss: 469.9933166503906 = 1.1061259508132935 + 50.0 * 9.3777437210083
Epoch 40, val loss: 1.1057401895523071
Epoch 50, training loss: 465.24310302734375 = 1.1055907011032104 + 50.0 * 9.282750129699707
Epoch 50, val loss: 1.1051863431930542
Epoch 60, training loss: 461.2437438964844 = 1.1050326824188232 + 50.0 * 9.202774047851562
Epoch 60, val loss: 1.1046066284179688
Epoch 70, training loss: 457.8829345703125 = 1.1044678688049316 + 50.0 * 9.13556957244873
Epoch 70, val loss: 1.1040246486663818
Epoch 80, training loss: 455.01739501953125 = 1.1038765907287598 + 50.0 * 9.078269958496094
Epoch 80, val loss: 1.1034153699874878
Epoch 90, training loss: 452.5936279296875 = 1.103285789489746 + 50.0 * 9.029807090759277
Epoch 90, val loss: 1.102800726890564
Epoch 100, training loss: 450.49444580078125 = 1.1026685237884521 + 50.0 * 8.987835884094238
Epoch 100, val loss: 1.1021803617477417
Epoch 110, training loss: 448.72760009765625 = 1.102064847946167 + 50.0 * 8.952510833740234
Epoch 110, val loss: 1.1015520095825195
Epoch 120, training loss: 447.12237548828125 = 1.1014351844787598 + 50.0 * 8.920418739318848
Epoch 120, val loss: 1.100908875465393
Epoch 130, training loss: 445.8131103515625 = 1.1007939577102661 + 50.0 * 8.894246101379395
Epoch 130, val loss: 1.1002516746520996
Epoch 140, training loss: 444.7121276855469 = 1.1001359224319458 + 50.0 * 8.87224006652832
Epoch 140, val loss: 1.0995827913284302
Epoch 150, training loss: 443.7238464355469 = 1.099434733390808 + 50.0 * 8.85248851776123
Epoch 150, val loss: 1.0988832712173462
Epoch 160, training loss: 442.9255676269531 = 1.098776936531067 + 50.0 * 8.836535453796387
Epoch 160, val loss: 1.0982052087783813
Epoch 170, training loss: 442.2604675292969 = 1.0980945825576782 + 50.0 * 8.823247909545898
Epoch 170, val loss: 1.0974998474121094
Epoch 180, training loss: 441.6455993652344 = 1.097372055053711 + 50.0 * 8.810964584350586
Epoch 180, val loss: 1.0967726707458496
Epoch 190, training loss: 441.0666198730469 = 1.0966355800628662 + 50.0 * 8.799399375915527
Epoch 190, val loss: 1.096034049987793
Epoch 200, training loss: 440.6001281738281 = 1.0958929061889648 + 50.0 * 8.790084838867188
Epoch 200, val loss: 1.0952856540679932
Epoch 210, training loss: 440.1832580566406 = 1.0951566696166992 + 50.0 * 8.78176212310791
Epoch 210, val loss: 1.0945461988449097
Epoch 220, training loss: 439.82568359375 = 1.0943738222122192 + 50.0 * 8.774626731872559
Epoch 220, val loss: 1.0937713384628296
Epoch 230, training loss: 439.5557861328125 = 1.0936341285705566 + 50.0 * 8.769243240356445
Epoch 230, val loss: 1.0930238962173462
Epoch 240, training loss: 439.19781494140625 = 1.0928982496261597 + 50.0 * 8.76209831237793
Epoch 240, val loss: 1.0922812223434448
Epoch 250, training loss: 439.0513000488281 = 1.0921918153762817 + 50.0 * 8.75918197631836
Epoch 250, val loss: 1.0915682315826416
Epoch 260, training loss: 438.7893981933594 = 1.0915111303329468 + 50.0 * 8.753957748413086
Epoch 260, val loss: 1.090875506401062
Epoch 270, training loss: 438.6329040527344 = 1.0908622741699219 + 50.0 * 8.75084114074707
Epoch 270, val loss: 1.0902196168899536
Epoch 280, training loss: 438.7918395996094 = 1.0902533531188965 + 50.0 * 8.754032135009766
Epoch 280, val loss: 1.0896220207214355
Epoch 290, training loss: 438.22210693359375 = 1.0896542072296143 + 50.0 * 8.74264907836914
Epoch 290, val loss: 1.0890098810195923
Epoch 300, training loss: 438.0905456542969 = 1.0890880823135376 + 50.0 * 8.740029335021973
Epoch 300, val loss: 1.0884636640548706
Epoch 310, training loss: 437.8611755371094 = 1.0885919332504272 + 50.0 * 8.735451698303223
Epoch 310, val loss: 1.087960958480835
Epoch 320, training loss: 437.9346923828125 = 1.0881197452545166 + 50.0 * 8.736930847167969
Epoch 320, val loss: 1.0874855518341064
Epoch 330, training loss: 437.80523681640625 = 1.0876493453979492 + 50.0 * 8.734352111816406
Epoch 330, val loss: 1.087021827697754
Epoch 340, training loss: 437.9123840332031 = 1.0871825218200684 + 50.0 * 8.736503601074219
Epoch 340, val loss: 1.0865542888641357
Epoch 350, training loss: 437.799560546875 = 1.0867282152175903 + 50.0 * 8.734256744384766
Epoch 350, val loss: 1.086087942123413
Epoch 360, training loss: 437.9077453613281 = 1.0862728357315063 + 50.0 * 8.736429214477539
Epoch 360, val loss: 1.0856350660324097
Epoch 370, training loss: 438.0836486816406 = 1.0858126878738403 + 50.0 * 8.739956855773926
Epoch 370, val loss: 1.0851755142211914
Epoch 380, training loss: 437.8569641113281 = 1.0853315591812134 + 50.0 * 8.735432624816895
Epoch 380, val loss: 1.0846996307373047
Epoch 390, training loss: 438.1892395019531 = 1.0848585367202759 + 50.0 * 8.742087364196777
Epoch 390, val loss: 1.0842305421829224
Epoch 400, training loss: 438.0767517089844 = 1.084376335144043 + 50.0 * 8.739847183227539
Epoch 400, val loss: 1.0837475061416626
Epoch 410, training loss: 438.0867614746094 = 1.0838756561279297 + 50.0 * 8.740057945251465
Epoch 410, val loss: 1.0832451581954956
Epoch 420, training loss: 438.2115783691406 = 1.083404779434204 + 50.0 * 8.742563247680664
Epoch 420, val loss: 1.0827791690826416
Epoch 430, training loss: 437.75482177734375 = 1.082848310470581 + 50.0 * 8.733439445495605
Epoch 430, val loss: 1.0822186470031738
Epoch 440, training loss: 438.2496643066406 = 1.0823761224746704 + 50.0 * 8.743346214294434
Epoch 440, val loss: 1.0817562341690063
Epoch 450, training loss: 437.1965637207031 = 1.0818008184432983 + 50.0 * 8.722295761108398
Epoch 450, val loss: 1.0811901092529297
Epoch 460, training loss: 437.8178405761719 = 1.081312656402588 + 50.0 * 8.73473072052002
Epoch 460, val loss: 1.0806879997253418
Epoch 470, training loss: 438.0317687988281 = 1.0807414054870605 + 50.0 * 8.739020347595215
Epoch 470, val loss: 1.0801517963409424
Epoch 480, training loss: 438.0722351074219 = 1.0801960229873657 + 50.0 * 8.739840507507324
Epoch 480, val loss: 1.0796074867248535
Epoch 490, training loss: 438.2085876464844 = 1.0796364545822144 + 50.0 * 8.742578506469727
Epoch 490, val loss: 1.0790609121322632
Epoch 500, training loss: 438.36663818359375 = 1.0790786743164062 + 50.0 * 8.74575138092041
Epoch 500, val loss: 1.0785045623779297
Epoch 510, training loss: 438.65985107421875 = 1.078485369682312 + 50.0 * 8.751626968383789
Epoch 510, val loss: 1.0779238939285278
Epoch 520, training loss: 438.7578430175781 = 1.0778756141662598 + 50.0 * 8.753599166870117
Epoch 520, val loss: 1.077327847480774
Epoch 530, training loss: 438.8868408203125 = 1.0772651433944702 + 50.0 * 8.75619125366211
Epoch 530, val loss: 1.0767326354980469
Epoch 540, training loss: 439.0611572265625 = 1.076640248298645 + 50.0 * 8.759690284729004
Epoch 540, val loss: 1.0761233568191528
Epoch 550, training loss: 439.1591491699219 = 1.0759977102279663 + 50.0 * 8.761663436889648
Epoch 550, val loss: 1.0754942893981934
Epoch 560, training loss: 439.36468505859375 = 1.0753322839736938 + 50.0 * 8.765787124633789
Epoch 560, val loss: 1.0748414993286133
Epoch 570, training loss: 439.3377685546875 = 1.0746536254882812 + 50.0 * 8.765262603759766
Epoch 570, val loss: 1.0741726160049438
Epoch 580, training loss: 439.1943054199219 = 1.0739353895187378 + 50.0 * 8.762407302856445
Epoch 580, val loss: 1.0734833478927612
Epoch 590, training loss: 439.6599426269531 = 1.073257327079773 + 50.0 * 8.771734237670898
Epoch 590, val loss: 1.07280695438385
Epoch 600, training loss: 439.84783935546875 = 1.0725252628326416 + 50.0 * 8.775506019592285
Epoch 600, val loss: 1.072095274925232
Epoch 610, training loss: 439.6064453125 = 1.0717979669570923 + 50.0 * 8.770692825317383
Epoch 610, val loss: 1.071381688117981
Epoch 620, training loss: 440.03106689453125 = 1.0710748434066772 + 50.0 * 8.779199600219727
Epoch 620, val loss: 1.0706796646118164
Epoch 630, training loss: 440.1686096191406 = 1.0703186988830566 + 50.0 * 8.781966209411621
Epoch 630, val loss: 1.0699445009231567
Epoch 640, training loss: 440.4053649902344 = 1.0695604085922241 + 50.0 * 8.78671646118164
Epoch 640, val loss: 1.0692107677459717
Epoch 650, training loss: 440.44195556640625 = 1.068779706954956 + 50.0 * 8.787463188171387
Epoch 650, val loss: 1.0684417486190796
Epoch 660, training loss: 440.44940185546875 = 1.0680011510849 + 50.0 * 8.787628173828125
Epoch 660, val loss: 1.0676952600479126
Epoch 670, training loss: 440.5480041503906 = 1.0672177076339722 + 50.0 * 8.789615631103516
Epoch 670, val loss: 1.06692636013031
Epoch 680, training loss: 440.67047119140625 = 1.0664563179016113 + 50.0 * 8.79207992553711
Epoch 680, val loss: 1.066173791885376
Epoch 690, training loss: 440.9301452636719 = 1.0656522512435913 + 50.0 * 8.797289848327637
Epoch 690, val loss: 1.065405249595642
Epoch 700, training loss: 441.0351867675781 = 1.06484854221344 + 50.0 * 8.799407005310059
Epoch 700, val loss: 1.0646268129348755
Epoch 710, training loss: 440.9405212402344 = 1.0640273094177246 + 50.0 * 8.797530174255371
Epoch 710, val loss: 1.0638240575790405
Epoch 720, training loss: 441.2103271484375 = 1.0631994009017944 + 50.0 * 8.802942276000977
Epoch 720, val loss: 1.063002109527588
Epoch 730, training loss: 441.3103942871094 = 1.062372088432312 + 50.0 * 8.804960250854492
Epoch 730, val loss: 1.0622011423110962
Epoch 740, training loss: 441.31939697265625 = 1.0615142583847046 + 50.0 * 8.805157661437988
Epoch 740, val loss: 1.0613828897476196
Epoch 750, training loss: 441.4781494140625 = 1.0606681108474731 + 50.0 * 8.808349609375
Epoch 750, val loss: 1.060552954673767
Epoch 760, training loss: 441.4584655761719 = 1.0597903728485107 + 50.0 * 8.807973861694336
Epoch 760, val loss: 1.0597063302993774
Epoch 770, training loss: 441.6910705566406 = 1.0589197874069214 + 50.0 * 8.812643051147461
Epoch 770, val loss: 1.0588462352752686
Epoch 780, training loss: 441.6805114746094 = 1.0580389499664307 + 50.0 * 8.81244945526123
Epoch 780, val loss: 1.0579994916915894
Epoch 790, training loss: 441.7828674316406 = 1.0571446418762207 + 50.0 * 8.81451416015625
Epoch 790, val loss: 1.057134985923767
Epoch 800, training loss: 441.8853759765625 = 1.056246280670166 + 50.0 * 8.816582679748535
Epoch 800, val loss: 1.0562671422958374
Epoch 810, training loss: 441.7456359863281 = 1.055310606956482 + 50.0 * 8.813806533813477
Epoch 810, val loss: 1.0553520917892456
Epoch 820, training loss: 441.8935852050781 = 1.0544319152832031 + 50.0 * 8.81678295135498
Epoch 820, val loss: 1.0545063018798828
Epoch 830, training loss: 442.2713928222656 = 1.053510069847107 + 50.0 * 8.824357986450195
Epoch 830, val loss: 1.053605079650879
Epoch 840, training loss: 442.5032653808594 = 1.052597999572754 + 50.0 * 8.829012870788574
Epoch 840, val loss: 1.0527184009552002
Epoch 850, training loss: 442.702392578125 = 1.0516396760940552 + 50.0 * 8.833015441894531
Epoch 850, val loss: 1.0517945289611816
Epoch 860, training loss: 443.1652526855469 = 1.0506482124328613 + 50.0 * 8.842291831970215
Epoch 860, val loss: 1.0508415699005127
Epoch 870, training loss: 441.5237731933594 = 1.0496536493301392 + 50.0 * 8.80948257446289
Epoch 870, val loss: 1.0498627424240112
Epoch 880, training loss: 442.03302001953125 = 1.0487608909606934 + 50.0 * 8.819684982299805
Epoch 880, val loss: 1.0490086078643799
Epoch 890, training loss: 442.1590270996094 = 1.0477646589279175 + 50.0 * 8.822225570678711
Epoch 890, val loss: 1.0480477809906006
Epoch 900, training loss: 442.464599609375 = 1.0468075275421143 + 50.0 * 8.82835578918457
Epoch 900, val loss: 1.0471299886703491
Epoch 910, training loss: 442.7449951171875 = 1.0458476543426514 + 50.0 * 8.833983421325684
Epoch 910, val loss: 1.0461950302124023
Epoch 920, training loss: 443.1766357421875 = 1.0448874235153198 + 50.0 * 8.842635154724121
Epoch 920, val loss: 1.04525887966156
Epoch 930, training loss: 442.8660888671875 = 1.0438790321350098 + 50.0 * 8.836443901062012
Epoch 930, val loss: 1.0442839860916138
Epoch 940, training loss: 443.3363037109375 = 1.0429069995880127 + 50.0 * 8.845868110656738
Epoch 940, val loss: 1.0433416366577148
Epoch 950, training loss: 443.6632385253906 = 1.0418999195098877 + 50.0 * 8.852426528930664
Epoch 950, val loss: 1.0423601865768433
Epoch 960, training loss: 443.5592956542969 = 1.0408754348754883 + 50.0 * 8.85036849975586
Epoch 960, val loss: 1.041368007659912
Epoch 970, training loss: 443.8675537109375 = 1.0398578643798828 + 50.0 * 8.85655403137207
Epoch 970, val loss: 1.040381908416748
Epoch 980, training loss: 444.1177978515625 = 1.0388293266296387 + 50.0 * 8.861579895019531
Epoch 980, val loss: 1.0393823385238647
Epoch 990, training loss: 444.2882080078125 = 1.0377720594406128 + 50.0 * 8.865008354187012
Epoch 990, val loss: 1.038342833518982
Epoch 1000, training loss: 444.31378173828125 = 1.0367159843444824 + 50.0 * 8.865541458129883
Epoch 1000, val loss: 1.0373278856277466
Epoch 1010, training loss: 444.5954284667969 = 1.035667896270752 + 50.0 * 8.871194839477539
Epoch 1010, val loss: 1.036309838294983
Epoch 1020, training loss: 444.6653747558594 = 1.034583330154419 + 50.0 * 8.872615814208984
Epoch 1020, val loss: 1.0352530479431152
Epoch 1030, training loss: 445.1502685546875 = 1.0334815979003906 + 50.0 * 8.882335662841797
Epoch 1030, val loss: 1.0341901779174805
Epoch 1040, training loss: 444.7283935546875 = 1.0323829650878906 + 50.0 * 8.873920440673828
Epoch 1040, val loss: 1.0331131219863892
Epoch 1050, training loss: 444.71148681640625 = 1.0312525033950806 + 50.0 * 8.873604774475098
Epoch 1050, val loss: 1.0320096015930176
Epoch 1060, training loss: 445.0692443847656 = 1.0301657915115356 + 50.0 * 8.880782127380371
Epoch 1060, val loss: 1.030961036682129
Epoch 1070, training loss: 445.3092041015625 = 1.0290549993515015 + 50.0 * 8.885602951049805
Epoch 1070, val loss: 1.029897689819336
Epoch 1080, training loss: 445.2987060546875 = 1.027938961982727 + 50.0 * 8.885415077209473
Epoch 1080, val loss: 1.0288105010986328
Epoch 1090, training loss: 445.5271911621094 = 1.0268380641937256 + 50.0 * 8.890007019042969
Epoch 1090, val loss: 1.0277498960494995
Epoch 1100, training loss: 445.6463928222656 = 1.0257033109664917 + 50.0 * 8.892414093017578
Epoch 1100, val loss: 1.0266562700271606
Epoch 1110, training loss: 445.8243713378906 = 1.0245651006698608 + 50.0 * 8.89599609375
Epoch 1110, val loss: 1.0255569219589233
Epoch 1120, training loss: 445.7372741699219 = 1.0233988761901855 + 50.0 * 8.894277572631836
Epoch 1120, val loss: 1.0244410037994385
Epoch 1130, training loss: 445.68243408203125 = 1.0222488641738892 + 50.0 * 8.893203735351562
Epoch 1130, val loss: 1.0233194828033447
Epoch 1140, training loss: 445.91229248046875 = 1.0211026668548584 + 50.0 * 8.89782428741455
Epoch 1140, val loss: 1.0222196578979492
Epoch 1150, training loss: 445.13604736328125 = 1.0198497772216797 + 50.0 * 8.88232421875
Epoch 1150, val loss: 1.0210256576538086
Epoch 1160, training loss: 445.68341064453125 = 1.0187203884124756 + 50.0 * 8.893294334411621
Epoch 1160, val loss: 1.0199062824249268
Epoch 1170, training loss: 445.8539123535156 = 1.0175845623016357 + 50.0 * 8.896726608276367
Epoch 1170, val loss: 1.0187933444976807
Epoch 1180, training loss: 446.11993408203125 = 1.0163847208023071 + 50.0 * 8.902070999145508
Epoch 1180, val loss: 1.0176416635513306
Epoch 1190, training loss: 446.228515625 = 1.015207052230835 + 50.0 * 8.904266357421875
Epoch 1190, val loss: 1.0164977312088013
Epoch 1200, training loss: 446.5869445800781 = 1.0140289068222046 + 50.0 * 8.911458015441895
Epoch 1200, val loss: 1.0153547525405884
Epoch 1210, training loss: 446.4697570800781 = 1.0128283500671387 + 50.0 * 8.909138679504395
Epoch 1210, val loss: 1.0142059326171875
Epoch 1220, training loss: 446.86029052734375 = 1.0116403102874756 + 50.0 * 8.916973114013672
Epoch 1220, val loss: 1.013052225112915
Epoch 1230, training loss: 446.8578186035156 = 1.0104029178619385 + 50.0 * 8.916948318481445
Epoch 1230, val loss: 1.0118639469146729
Epoch 1240, training loss: 446.9230651855469 = 1.0091962814331055 + 50.0 * 8.918277740478516
Epoch 1240, val loss: 1.0106947422027588
Epoch 1250, training loss: 446.8846130371094 = 1.0079708099365234 + 50.0 * 8.917532920837402
Epoch 1250, val loss: 1.0095207691192627
Epoch 1260, training loss: 447.14703369140625 = 1.0067498683929443 + 50.0 * 8.922805786132812
Epoch 1260, val loss: 1.008346676826477
Epoch 1270, training loss: 447.24322509765625 = 1.0054738521575928 + 50.0 * 8.924755096435547
Epoch 1270, val loss: 1.00710928440094
Epoch 1280, training loss: 445.7228088378906 = 1.004136562347412 + 50.0 * 8.894372940063477
Epoch 1280, val loss: 1.0058083534240723
Epoch 1290, training loss: 443.8352355957031 = 1.0025562047958374 + 50.0 * 8.856653213500977
Epoch 1290, val loss: 1.004093050956726
Epoch 1300, training loss: 438.59039306640625 = 1.000464677810669 + 50.0 * 8.751798629760742
Epoch 1300, val loss: 1.002303957939148
Epoch 1310, training loss: 440.44525146484375 = 0.9999108910560608 + 50.0 * 8.788907051086426
Epoch 1310, val loss: 1.0017703771591187
Epoch 1320, training loss: 442.2832946777344 = 0.9991005659103394 + 50.0 * 8.82568359375
Epoch 1320, val loss: 1.0009592771530151
Epoch 1330, training loss: 442.47784423828125 = 0.9980967044830322 + 50.0 * 8.829594612121582
Epoch 1330, val loss: 0.9999768733978271
Epoch 1340, training loss: 444.1077575683594 = 0.9970034956932068 + 50.0 * 8.862215042114258
Epoch 1340, val loss: 0.9989060163497925
Epoch 1350, training loss: 444.29156494140625 = 0.9956685304641724 + 50.0 * 8.865918159484863
Epoch 1350, val loss: 0.9976444840431213
Epoch 1360, training loss: 445.90301513671875 = 0.9945101737976074 + 50.0 * 8.898170471191406
Epoch 1360, val loss: 0.996527373790741
Epoch 1370, training loss: 445.98797607421875 = 0.9932326078414917 + 50.0 * 8.899894714355469
Epoch 1370, val loss: 0.9952868223190308
Epoch 1380, training loss: 446.0722961425781 = 0.9920099377632141 + 50.0 * 8.901605606079102
Epoch 1380, val loss: 0.9941309690475464
Epoch 1390, training loss: 446.3639221191406 = 0.9908079504966736 + 50.0 * 8.907462120056152
Epoch 1390, val loss: 0.9929715991020203
Epoch 1400, training loss: 447.0985107421875 = 0.9895966649055481 + 50.0 * 8.922178268432617
Epoch 1400, val loss: 0.991794228553772
Epoch 1410, training loss: 447.2362365722656 = 0.9883263111114502 + 50.0 * 8.924958229064941
Epoch 1410, val loss: 0.9905673265457153
Epoch 1420, training loss: 447.6397705078125 = 0.9870729446411133 + 50.0 * 8.933053970336914
Epoch 1420, val loss: 0.9893659353256226
Epoch 1430, training loss: 447.7019348144531 = 0.9857887625694275 + 50.0 * 8.93432331085205
Epoch 1430, val loss: 0.9881211519241333
Epoch 1440, training loss: 448.0457458496094 = 0.9845269322395325 + 50.0 * 8.941224098205566
Epoch 1440, val loss: 0.9869077205657959
Epoch 1450, training loss: 448.408935546875 = 0.983252227306366 + 50.0 * 8.948513984680176
Epoch 1450, val loss: 0.9856772422790527
Epoch 1460, training loss: 448.2613220214844 = 0.9819602966308594 + 50.0 * 8.945587158203125
Epoch 1460, val loss: 0.9844333529472351
Epoch 1470, training loss: 448.63555908203125 = 0.9806677103042603 + 50.0 * 8.953097343444824
Epoch 1470, val loss: 0.9831938743591309
Epoch 1480, training loss: 448.84765625 = 0.9793822765350342 + 50.0 * 8.957365989685059
Epoch 1480, val loss: 0.9819595813751221
Epoch 1490, training loss: 448.7711181640625 = 0.9780443906784058 + 50.0 * 8.95586109161377
Epoch 1490, val loss: 0.9806751608848572
Epoch 1500, training loss: 447.6842346191406 = 0.9766644239425659 + 50.0 * 8.934151649475098
Epoch 1500, val loss: 0.979328989982605
Epoch 1510, training loss: 448.0195617675781 = 0.9753502607345581 + 50.0 * 8.940884590148926
Epoch 1510, val loss: 0.9780735373497009
Epoch 1520, training loss: 448.7277526855469 = 0.9740942716598511 + 50.0 * 8.955073356628418
Epoch 1520, val loss: 0.9768736958503723
Epoch 1530, training loss: 449.11785888671875 = 0.9728033542633057 + 50.0 * 8.96290111541748
Epoch 1530, val loss: 0.9756388664245605
Epoch 1540, training loss: 449.3275451660156 = 0.9715007543563843 + 50.0 * 8.967121124267578
Epoch 1540, val loss: 0.9743920564651489
Epoch 1550, training loss: 449.5224914550781 = 0.9702144265174866 + 50.0 * 8.97104549407959
Epoch 1550, val loss: 0.9731431007385254
Epoch 1560, training loss: 449.643798828125 = 0.9689189791679382 + 50.0 * 8.97349739074707
Epoch 1560, val loss: 0.9719046950340271
Epoch 1570, training loss: 449.7917785644531 = 0.9676300883293152 + 50.0 * 8.976483345031738
Epoch 1570, val loss: 0.9706621766090393
Epoch 1580, training loss: 449.85638427734375 = 0.9663197994232178 + 50.0 * 8.977801322937012
Epoch 1580, val loss: 0.9694144129753113
Epoch 1590, training loss: 449.8795166015625 = 0.965021550655365 + 50.0 * 8.978289604187012
Epoch 1590, val loss: 0.9681700468063354
Epoch 1600, training loss: 450.1085510253906 = 0.9637271165847778 + 50.0 * 8.98289680480957
Epoch 1600, val loss: 0.9669338464736938
Epoch 1610, training loss: 450.1324157714844 = 0.9624440670013428 + 50.0 * 8.983399391174316
Epoch 1610, val loss: 0.9657052755355835
Epoch 1620, training loss: 450.3016052246094 = 0.9611542820930481 + 50.0 * 8.986808776855469
Epoch 1620, val loss: 0.9644643068313599
Epoch 1630, training loss: 450.1937561035156 = 0.9598315954208374 + 50.0 * 8.984678268432617
Epoch 1630, val loss: 0.963212251663208
Epoch 1640, training loss: 450.4315185546875 = 0.9585524797439575 + 50.0 * 8.989459037780762
Epoch 1640, val loss: 0.9619788527488708
Epoch 1650, training loss: 450.4097900390625 = 0.9572517275810242 + 50.0 * 8.98905086517334
Epoch 1650, val loss: 0.9607393145561218
Epoch 1660, training loss: 450.5177917480469 = 0.9559569358825684 + 50.0 * 8.991236686706543
Epoch 1660, val loss: 0.9595044255256653
Epoch 1670, training loss: 450.7701110839844 = 0.9546853303909302 + 50.0 * 8.996308326721191
Epoch 1670, val loss: 0.9582794904708862
Epoch 1680, training loss: 450.8499450683594 = 0.9533708691596985 + 50.0 * 8.997931480407715
Epoch 1680, val loss: 0.9570156931877136
Epoch 1690, training loss: 450.9251403808594 = 0.9520724415779114 + 50.0 * 8.99946117401123
Epoch 1690, val loss: 0.9557775855064392
Epoch 1700, training loss: 450.7113342285156 = 0.950750470161438 + 50.0 * 8.995211601257324
Epoch 1700, val loss: 0.9545336365699768
Epoch 1710, training loss: 451.0242919921875 = 0.9494549036026001 + 50.0 * 9.001496315002441
Epoch 1710, val loss: 0.953292727470398
Epoch 1720, training loss: 451.0937194824219 = 0.9481626152992249 + 50.0 * 9.002911567687988
Epoch 1720, val loss: 0.9520628452301025
Epoch 1730, training loss: 450.8973083496094 = 0.9468348026275635 + 50.0 * 8.999009132385254
Epoch 1730, val loss: 0.9508029222488403
Epoch 1740, training loss: 451.0934753417969 = 0.945560872554779 + 50.0 * 9.002958297729492
Epoch 1740, val loss: 0.9495863318443298
Epoch 1750, training loss: 451.31573486328125 = 0.9442499876022339 + 50.0 * 9.007430076599121
Epoch 1750, val loss: 0.948336660861969
Epoch 1760, training loss: 451.3842468261719 = 0.9429370760917664 + 50.0 * 9.00882625579834
Epoch 1760, val loss: 0.9470834732055664
Epoch 1770, training loss: 451.4996032714844 = 0.9416221380233765 + 50.0 * 9.011159896850586
Epoch 1770, val loss: 0.9458281397819519
Epoch 1780, training loss: 451.3144836425781 = 0.9402769207954407 + 50.0 * 9.007484436035156
Epoch 1780, val loss: 0.9445503950119019
Epoch 1790, training loss: 451.6639099121094 = 0.9389568567276001 + 50.0 * 9.014498710632324
Epoch 1790, val loss: 0.9432851076126099
Epoch 1800, training loss: 451.3952941894531 = 0.9376161694526672 + 50.0 * 9.009153366088867
Epoch 1800, val loss: 0.9420222043991089
Epoch 1810, training loss: 451.68328857421875 = 0.9362996816635132 + 50.0 * 9.014939308166504
Epoch 1810, val loss: 0.9407677054405212
Epoch 1820, training loss: 451.79193115234375 = 0.9349549412727356 + 50.0 * 9.017139434814453
Epoch 1820, val loss: 0.939480721950531
Epoch 1830, training loss: 451.44775390625 = 0.9336005449295044 + 50.0 * 9.010283470153809
Epoch 1830, val loss: 0.9381672143936157
Epoch 1840, training loss: 451.11297607421875 = 0.9323156476020813 + 50.0 * 9.003613471984863
Epoch 1840, val loss: 0.9369709491729736
Epoch 1850, training loss: 451.2725524902344 = 0.930991530418396 + 50.0 * 9.006831169128418
Epoch 1850, val loss: 0.935700535774231
Epoch 1860, training loss: 451.52685546875 = 0.9296717643737793 + 50.0 * 9.011943817138672
Epoch 1860, val loss: 0.9344516396522522
Epoch 1870, training loss: 451.7975158691406 = 0.9283377528190613 + 50.0 * 9.017383575439453
Epoch 1870, val loss: 0.9331825971603394
Epoch 1880, training loss: 451.88885498046875 = 0.9270271062850952 + 50.0 * 9.01923656463623
Epoch 1880, val loss: 0.9319434762001038
Epoch 1890, training loss: 452.05206298828125 = 0.9257227182388306 + 50.0 * 9.022526741027832
Epoch 1890, val loss: 0.9307057857513428
Epoch 1900, training loss: 452.19561767578125 = 0.9244176745414734 + 50.0 * 9.025424003601074
Epoch 1900, val loss: 0.929467499256134
Epoch 1910, training loss: 451.98724365234375 = 0.9231255054473877 + 50.0 * 9.021282196044922
Epoch 1910, val loss: 0.9282391667366028
Epoch 1920, training loss: 452.1865539550781 = 0.921848714351654 + 50.0 * 9.025294303894043
Epoch 1920, val loss: 0.9270262122154236
Epoch 1930, training loss: 452.5141906738281 = 0.9205778241157532 + 50.0 * 9.031872749328613
Epoch 1930, val loss: 0.92581707239151
Epoch 1940, training loss: 452.2998962402344 = 0.9193097352981567 + 50.0 * 9.02761173248291
Epoch 1940, val loss: 0.9246281385421753
Epoch 1950, training loss: 452.4483947753906 = 0.9180404543876648 + 50.0 * 9.030607223510742
Epoch 1950, val loss: 0.9234247803688049
Epoch 1960, training loss: 452.40216064453125 = 0.9167694449424744 + 50.0 * 9.029707908630371
Epoch 1960, val loss: 0.9222189784049988
Epoch 1970, training loss: 452.4383239746094 = 0.9155033230781555 + 50.0 * 9.03045654296875
Epoch 1970, val loss: 0.9210203886032104
Epoch 1980, training loss: 451.4608459472656 = 0.9142155647277832 + 50.0 * 9.010932922363281
Epoch 1980, val loss: 0.9198206067085266
Epoch 1990, training loss: 451.4009704589844 = 0.9130309224128723 + 50.0 * 9.009758949279785
Epoch 1990, val loss: 0.918700098991394
Epoch 2000, training loss: 451.8408203125 = 0.9118014574050903 + 50.0 * 9.018580436706543
Epoch 2000, val loss: 0.9175183773040771
Epoch 2010, training loss: 451.6612854003906 = 0.9105554819107056 + 50.0 * 9.0150146484375
Epoch 2010, val loss: 0.916353702545166
Epoch 2020, training loss: 451.53009033203125 = 0.9093303084373474 + 50.0 * 9.012414932250977
Epoch 2020, val loss: 0.9151964783668518
Epoch 2030, training loss: 452.08447265625 = 0.9080992937088013 + 50.0 * 9.023527145385742
Epoch 2030, val loss: 0.9140491485595703
Epoch 2040, training loss: 452.3213806152344 = 0.9068791270256042 + 50.0 * 9.028289794921875
Epoch 2040, val loss: 0.9128948450088501
Epoch 2050, training loss: 452.6635437011719 = 0.9056575298309326 + 50.0 * 9.035157203674316
Epoch 2050, val loss: 0.9117546677589417
Epoch 2060, training loss: 452.61163330078125 = 0.9044196605682373 + 50.0 * 9.034144401550293
Epoch 2060, val loss: 0.910603404045105
Epoch 2070, training loss: 452.63995361328125 = 0.9032285213470459 + 50.0 * 9.034734725952148
Epoch 2070, val loss: 0.9095024466514587
Epoch 2080, training loss: 453.0158386230469 = 0.9020225405693054 + 50.0 * 9.042276382446289
Epoch 2080, val loss: 0.908368706703186
Epoch 2090, training loss: 453.24578857421875 = 0.9007979035377502 + 50.0 * 9.046899795532227
Epoch 2090, val loss: 0.9072281122207642
Epoch 2100, training loss: 452.6546936035156 = 0.8995772004127502 + 50.0 * 9.035102844238281
Epoch 2100, val loss: 0.9061250686645508
Epoch 2110, training loss: 452.836181640625 = 0.8984166979789734 + 50.0 * 9.038755416870117
Epoch 2110, val loss: 0.9050585627555847
Epoch 2120, training loss: 453.03076171875 = 0.8972064852714539 + 50.0 * 9.042671203613281
Epoch 2120, val loss: 0.9039227366447449
Epoch 2130, training loss: 453.36383056640625 = 0.8959895968437195 + 50.0 * 9.049356460571289
Epoch 2130, val loss: 0.9027957916259766
Epoch 2140, training loss: 453.2841796875 = 0.894757866859436 + 50.0 * 9.047788619995117
Epoch 2140, val loss: 0.9016602635383606
Epoch 2150, training loss: 453.3732604980469 = 0.8935695290565491 + 50.0 * 9.049593925476074
Epoch 2150, val loss: 0.9005908370018005
Epoch 2160, training loss: 453.36090087890625 = 0.8923696875572205 + 50.0 * 9.049370765686035
Epoch 2160, val loss: 0.8994981050491333
Epoch 2170, training loss: 452.73126220703125 = 0.891204297542572 + 50.0 * 9.0368013381958
Epoch 2170, val loss: 0.8983975648880005
Epoch 2180, training loss: 452.9858703613281 = 0.8900656700134277 + 50.0 * 9.041915893554688
Epoch 2180, val loss: 0.8973459601402283
Epoch 2190, training loss: 453.2933044433594 = 0.888896107673645 + 50.0 * 9.048088073730469
Epoch 2190, val loss: 0.896260678768158
Epoch 2200, training loss: 453.46441650390625 = 0.8877397775650024 + 50.0 * 9.051533699035645
Epoch 2200, val loss: 0.895188570022583
Epoch 2210, training loss: 453.5077819824219 = 0.8865947127342224 + 50.0 * 9.052423477172852
Epoch 2210, val loss: 0.894130527973175
Epoch 2220, training loss: 453.44842529296875 = 0.8854318857192993 + 50.0 * 9.051259994506836
Epoch 2220, val loss: 0.8930716514587402
Epoch 2230, training loss: 453.6516418457031 = 0.8843092322349548 + 50.0 * 9.055346488952637
Epoch 2230, val loss: 0.8920303583145142
Epoch 2240, training loss: 453.50384521484375 = 0.8831567168235779 + 50.0 * 9.052413940429688
Epoch 2240, val loss: 0.8909574747085571
Epoch 2250, training loss: 453.64483642578125 = 0.8820344805717468 + 50.0 * 9.055255889892578
Epoch 2250, val loss: 0.8899540305137634
Epoch 2260, training loss: 453.69873046875 = 0.8808609843254089 + 50.0 * 9.056357383728027
Epoch 2260, val loss: 0.8888617753982544
Epoch 2270, training loss: 452.84765625 = 0.8796477317810059 + 50.0 * 9.039360046386719
Epoch 2270, val loss: 0.8877430558204651
Epoch 2280, training loss: 452.9465026855469 = 0.8784974217414856 + 50.0 * 9.041359901428223
Epoch 2280, val loss: 0.8867034316062927
Epoch 2290, training loss: 453.2583312988281 = 0.8773804306983948 + 50.0 * 9.047618865966797
Epoch 2290, val loss: 0.8857521414756775
Epoch 2300, training loss: 453.63372802734375 = 0.876196026802063 + 50.0 * 9.055150985717773
Epoch 2300, val loss: 0.8846654295921326
Epoch 2310, training loss: 453.89361572265625 = 0.8749476075172424 + 50.0 * 9.060373306274414
Epoch 2310, val loss: 0.8835170269012451
Epoch 2320, training loss: 453.6822204589844 = 0.8736593127250671 + 50.0 * 9.056171417236328
Epoch 2320, val loss: 0.8823455572128296
Epoch 2330, training loss: 453.8878479003906 = 0.8723891973495483 + 50.0 * 9.060309410095215
Epoch 2330, val loss: 0.8812028765678406
Epoch 2340, training loss: 454.0530090332031 = 0.8711346387863159 + 50.0 * 9.063637733459473
Epoch 2340, val loss: 0.8800740838050842
Epoch 2350, training loss: 453.9025573730469 = 0.8698878288269043 + 50.0 * 9.060653686523438
Epoch 2350, val loss: 0.8789694309234619
Epoch 2360, training loss: 454.06756591796875 = 0.8687014579772949 + 50.0 * 9.063977241516113
Epoch 2360, val loss: 0.8779168128967285
Epoch 2370, training loss: 454.21832275390625 = 0.8675402998924255 + 50.0 * 9.067015647888184
Epoch 2370, val loss: 0.8768860697746277
Epoch 2380, training loss: 454.00311279296875 = 0.8663842678070068 + 50.0 * 9.062734603881836
Epoch 2380, val loss: 0.875850260257721
Epoch 2390, training loss: 452.4023742675781 = 0.8652849793434143 + 50.0 * 9.030741691589355
Epoch 2390, val loss: 0.8749165534973145
Epoch 2400, training loss: 451.0328674316406 = 0.8641622066497803 + 50.0 * 9.003374099731445
Epoch 2400, val loss: 0.8739163279533386
Epoch 2410, training loss: 451.5523986816406 = 0.8631221055984497 + 50.0 * 9.013785362243652
Epoch 2410, val loss: 0.872965931892395
Epoch 2420, training loss: 452.419677734375 = 0.8620121479034424 + 50.0 * 9.031153678894043
Epoch 2420, val loss: 0.8719582557678223
Epoch 2430, training loss: 453.2718811035156 = 0.8609331250190735 + 50.0 * 9.048218727111816
Epoch 2430, val loss: 0.8710117340087891
Epoch 2440, training loss: 453.4762268066406 = 0.8598182201385498 + 50.0 * 9.052328109741211
Epoch 2440, val loss: 0.8700346350669861
Epoch 2450, training loss: 453.6982727050781 = 0.8587061166763306 + 50.0 * 9.056791305541992
Epoch 2450, val loss: 0.8690592646598816
Epoch 2460, training loss: 453.72955322265625 = 0.8575859665870667 + 50.0 * 9.057439804077148
Epoch 2460, val loss: 0.8680757284164429
Epoch 2470, training loss: 453.8296203613281 = 0.8564999103546143 + 50.0 * 9.059462547302246
Epoch 2470, val loss: 0.8671174049377441
Epoch 2480, training loss: 453.9658508300781 = 0.85543292760849 + 50.0 * 9.06220817565918
Epoch 2480, val loss: 0.8661799430847168
Epoch 2490, training loss: 453.8442687988281 = 0.8543767333030701 + 50.0 * 9.059798240661621
Epoch 2490, val loss: 0.865256130695343
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.5455072463768116
0.8631456929652975
=== training gcn model ===
Epoch 0, training loss: 516.303955078125 = 1.1048073768615723 + 50.0 * 10.303983688354492
Epoch 0, val loss: 1.1038312911987305
Epoch 10, training loss: 495.6206359863281 = 1.104230523109436 + 50.0 * 9.890328407287598
Epoch 10, val loss: 1.1032860279083252
Epoch 20, training loss: 484.92523193359375 = 1.1039717197418213 + 50.0 * 9.676424980163574
Epoch 20, val loss: 1.1029882431030273
Epoch 30, training loss: 477.3835144042969 = 1.1035608053207397 + 50.0 * 9.525599479675293
Epoch 30, val loss: 1.1025527715682983
Epoch 40, training loss: 471.46527099609375 = 1.1030961275100708 + 50.0 * 9.407243728637695
Epoch 40, val loss: 1.1020658016204834
Epoch 50, training loss: 466.53045654296875 = 1.1026549339294434 + 50.0 * 9.308555603027344
Epoch 50, val loss: 1.1016093492507935
Epoch 60, training loss: 462.3251037597656 = 1.1021699905395508 + 50.0 * 9.224458694458008
Epoch 60, val loss: 1.1011054515838623
Epoch 70, training loss: 458.71923828125 = 1.1016836166381836 + 50.0 * 9.152351379394531
Epoch 70, val loss: 1.1006063222885132
Epoch 80, training loss: 455.6219177246094 = 1.1011724472045898 + 50.0 * 9.090415000915527
Epoch 80, val loss: 1.1000843048095703
Epoch 90, training loss: 452.92193603515625 = 1.100649118423462 + 50.0 * 9.036425590515137
Epoch 90, val loss: 1.0995458364486694
Epoch 100, training loss: 450.630615234375 = 1.1000946760177612 + 50.0 * 8.990610122680664
Epoch 100, val loss: 1.098978042602539
Epoch 110, training loss: 448.7411193847656 = 1.0995299816131592 + 50.0 * 8.952832221984863
Epoch 110, val loss: 1.0983985662460327
Epoch 120, training loss: 447.1092529296875 = 1.0989359617233276 + 50.0 * 8.920206069946289
Epoch 120, val loss: 1.0977915525436401
Epoch 130, training loss: 445.7373352050781 = 1.0983363389968872 + 50.0 * 8.892780303955078
Epoch 130, val loss: 1.0971763134002686
Epoch 140, training loss: 444.80126953125 = 1.097713828086853 + 50.0 * 8.87407112121582
Epoch 140, val loss: 1.0965451002120972
Epoch 150, training loss: 444.0619201660156 = 1.0970386266708374 + 50.0 * 8.859297752380371
Epoch 150, val loss: 1.0958616733551025
Epoch 160, training loss: 443.0500793457031 = 1.096386432647705 + 50.0 * 8.83907413482666
Epoch 160, val loss: 1.09519362449646
Epoch 170, training loss: 441.9775390625 = 1.095714807510376 + 50.0 * 8.817636489868164
Epoch 170, val loss: 1.0945175886154175
Epoch 180, training loss: 441.2697448730469 = 1.095024585723877 + 50.0 * 8.803494453430176
Epoch 180, val loss: 1.0938154458999634
Epoch 190, training loss: 440.6426696777344 = 1.094322681427002 + 50.0 * 8.790966987609863
Epoch 190, val loss: 1.0931061506271362
Epoch 200, training loss: 440.0603332519531 = 1.0936139822006226 + 50.0 * 8.77933406829834
Epoch 200, val loss: 1.0923863649368286
Epoch 210, training loss: 439.5338439941406 = 1.0928938388824463 + 50.0 * 8.768818855285645
Epoch 210, val loss: 1.091658115386963
Epoch 220, training loss: 439.35809326171875 = 1.0921738147735596 + 50.0 * 8.765318870544434
Epoch 220, val loss: 1.0909315347671509
Epoch 230, training loss: 439.2127685546875 = 1.0914504528045654 + 50.0 * 8.762426376342773
Epoch 230, val loss: 1.0902104377746582
Epoch 240, training loss: 438.4426574707031 = 1.09066903591156 + 50.0 * 8.747039794921875
Epoch 240, val loss: 1.0894213914871216
Epoch 250, training loss: 438.08282470703125 = 1.0900377035140991 + 50.0 * 8.739855766296387
Epoch 250, val loss: 1.088783860206604
Epoch 260, training loss: 438.06549072265625 = 1.089389443397522 + 50.0 * 8.739521980285645
Epoch 260, val loss: 1.088140845298767
Epoch 270, training loss: 437.7703857421875 = 1.0887389183044434 + 50.0 * 8.733633041381836
Epoch 270, val loss: 1.0875017642974854
Epoch 280, training loss: 437.5494689941406 = 1.0881656408309937 + 50.0 * 8.729226112365723
Epoch 280, val loss: 1.0869275331497192
Epoch 290, training loss: 437.3193054199219 = 1.0876514911651611 + 50.0 * 8.72463321685791
Epoch 290, val loss: 1.0864052772521973
Epoch 300, training loss: 437.29486083984375 = 1.0871855020523071 + 50.0 * 8.724153518676758
Epoch 300, val loss: 1.0859397649765015
Epoch 310, training loss: 437.1933898925781 = 1.0867518186569214 + 50.0 * 8.722132682800293
Epoch 310, val loss: 1.0855003595352173
Epoch 320, training loss: 436.8864440917969 = 1.0863292217254639 + 50.0 * 8.716002464294434
Epoch 320, val loss: 1.0850635766983032
Epoch 330, training loss: 436.8538818359375 = 1.0859200954437256 + 50.0 * 8.715359687805176
Epoch 330, val loss: 1.0846506357192993
Epoch 340, training loss: 436.78594970703125 = 1.0855436325073242 + 50.0 * 8.714008331298828
Epoch 340, val loss: 1.0842740535736084
Epoch 350, training loss: 436.6749267578125 = 1.0851551294326782 + 50.0 * 8.711795806884766
Epoch 350, val loss: 1.0838768482208252
Epoch 360, training loss: 436.68182373046875 = 1.0847585201263428 + 50.0 * 8.711941719055176
Epoch 360, val loss: 1.083482027053833
Epoch 370, training loss: 436.5991516113281 = 1.0843778848648071 + 50.0 * 8.710295677185059
Epoch 370, val loss: 1.0830923318862915
Epoch 380, training loss: 436.67401123046875 = 1.0839793682098389 + 50.0 * 8.711800575256348
Epoch 380, val loss: 1.0826914310455322
Epoch 390, training loss: 436.5194091796875 = 1.083577275276184 + 50.0 * 8.70871639251709
Epoch 390, val loss: 1.0822858810424805
Epoch 400, training loss: 436.4945373535156 = 1.083191990852356 + 50.0 * 8.708227157592773
Epoch 400, val loss: 1.0818883180618286
Epoch 410, training loss: 436.5904846191406 = 1.0827736854553223 + 50.0 * 8.71015453338623
Epoch 410, val loss: 1.081470251083374
Epoch 420, training loss: 436.61761474609375 = 1.0823506116867065 + 50.0 * 8.710705757141113
Epoch 420, val loss: 1.0810480117797852
Epoch 430, training loss: 436.46221923828125 = 1.08193039894104 + 50.0 * 8.707605361938477
Epoch 430, val loss: 1.080611228942871
Epoch 440, training loss: 436.4573059082031 = 1.081496238708496 + 50.0 * 8.70751667022705
Epoch 440, val loss: 1.0801817178726196
Epoch 450, training loss: 436.69219970703125 = 1.0810596942901611 + 50.0 * 8.712223052978516
Epoch 450, val loss: 1.0797332525253296
Epoch 460, training loss: 436.794189453125 = 1.080617070198059 + 50.0 * 8.714271545410156
Epoch 460, val loss: 1.0793037414550781
Epoch 470, training loss: 436.843994140625 = 1.0801546573638916 + 50.0 * 8.715276718139648
Epoch 470, val loss: 1.0788350105285645
Epoch 480, training loss: 436.9894714355469 = 1.0796868801116943 + 50.0 * 8.718195915222168
Epoch 480, val loss: 1.0783586502075195
Epoch 490, training loss: 437.0361328125 = 1.0792160034179688 + 50.0 * 8.719138145446777
Epoch 490, val loss: 1.077878713607788
Epoch 500, training loss: 437.13922119140625 = 1.0787216424942017 + 50.0 * 8.721209526062012
Epoch 500, val loss: 1.0773903131484985
Epoch 510, training loss: 437.1415100097656 = 1.0782227516174316 + 50.0 * 8.72126579284668
Epoch 510, val loss: 1.0768882036209106
Epoch 520, training loss: 437.0740051269531 = 1.0777087211608887 + 50.0 * 8.719925880432129
Epoch 520, val loss: 1.0763832330703735
Epoch 530, training loss: 437.1665344238281 = 1.0771981477737427 + 50.0 * 8.721786499023438
Epoch 530, val loss: 1.0758616924285889
Epoch 540, training loss: 437.27728271484375 = 1.0766503810882568 + 50.0 * 8.72401237487793
Epoch 540, val loss: 1.0753179788589478
Epoch 550, training loss: 437.3951721191406 = 1.0761139392852783 + 50.0 * 8.726381301879883
Epoch 550, val loss: 1.0747894048690796
Epoch 560, training loss: 437.0086975097656 = 1.0755391120910645 + 50.0 * 8.718663215637207
Epoch 560, val loss: 1.0742201805114746
Epoch 570, training loss: 437.22357177734375 = 1.0750248432159424 + 50.0 * 8.722970962524414
Epoch 570, val loss: 1.0736889839172363
Epoch 580, training loss: 437.731201171875 = 1.074478268623352 + 50.0 * 8.733134269714355
Epoch 580, val loss: 1.0731401443481445
Epoch 590, training loss: 437.52935791015625 = 1.0738816261291504 + 50.0 * 8.729109764099121
Epoch 590, val loss: 1.072553277015686
Epoch 600, training loss: 437.7799987792969 = 1.073305368423462 + 50.0 * 8.73413372039795
Epoch 600, val loss: 1.0719746351242065
Epoch 610, training loss: 438.0221252441406 = 1.072691559791565 + 50.0 * 8.738988876342773
Epoch 610, val loss: 1.0713682174682617
Epoch 620, training loss: 438.2511291503906 = 1.072096347808838 + 50.0 * 8.74358081817627
Epoch 620, val loss: 1.0707645416259766
Epoch 630, training loss: 438.2001953125 = 1.07148277759552 + 50.0 * 8.742574691772461
Epoch 630, val loss: 1.070149302482605
Epoch 640, training loss: 438.3606872558594 = 1.070881724357605 + 50.0 * 8.745796203613281
Epoch 640, val loss: 1.0695303678512573
Epoch 650, training loss: 438.6973876953125 = 1.0702550411224365 + 50.0 * 8.752542495727539
Epoch 650, val loss: 1.0689167976379395
Epoch 660, training loss: 438.77239990234375 = 1.0696367025375366 + 50.0 * 8.75405502319336
Epoch 660, val loss: 1.0682916641235352
Epoch 670, training loss: 438.91259765625 = 1.0690101385116577 + 50.0 * 8.756872177124023
Epoch 670, val loss: 1.067655086517334
Epoch 680, training loss: 439.0918273925781 = 1.0683716535568237 + 50.0 * 8.760469436645508
Epoch 680, val loss: 1.0670028924942017
Epoch 690, training loss: 439.1575622558594 = 1.0677217245101929 + 50.0 * 8.761796951293945
Epoch 690, val loss: 1.066360354423523
Epoch 700, training loss: 439.1965637207031 = 1.067071795463562 + 50.0 * 8.762589454650879
Epoch 700, val loss: 1.0657035112380981
Epoch 710, training loss: 439.3681945800781 = 1.0664225816726685 + 50.0 * 8.766035079956055
Epoch 710, val loss: 1.0650489330291748
Epoch 720, training loss: 439.7139892578125 = 1.0657788515090942 + 50.0 * 8.772964477539062
Epoch 720, val loss: 1.0644081830978394
Epoch 730, training loss: 439.8761291503906 = 1.0651193857192993 + 50.0 * 8.776220321655273
Epoch 730, val loss: 1.063722848892212
Epoch 740, training loss: 439.86871337890625 = 1.0644373893737793 + 50.0 * 8.77608585357666
Epoch 740, val loss: 1.063043236732483
Epoch 750, training loss: 439.7863464355469 = 1.063772201538086 + 50.0 * 8.77445125579834
Epoch 750, val loss: 1.0623658895492554
Epoch 760, training loss: 439.9015197753906 = 1.0630663633346558 + 50.0 * 8.776768684387207
Epoch 760, val loss: 1.0616735219955444
Epoch 770, training loss: 440.1592102050781 = 1.0623830556869507 + 50.0 * 8.781936645507812
Epoch 770, val loss: 1.0609776973724365
Epoch 780, training loss: 440.3073425292969 = 1.0616847276687622 + 50.0 * 8.784913063049316
Epoch 780, val loss: 1.0602786540985107
Epoch 790, training loss: 440.57171630859375 = 1.0609800815582275 + 50.0 * 8.790214538574219
Epoch 790, val loss: 1.0595632791519165
Epoch 800, training loss: 440.6016540527344 = 1.0602737665176392 + 50.0 * 8.790827751159668
Epoch 800, val loss: 1.0588510036468506
Epoch 810, training loss: 440.7526550292969 = 1.0595440864562988 + 50.0 * 8.793862342834473
Epoch 810, val loss: 1.0581337213516235
Epoch 820, training loss: 440.8731994628906 = 1.05881667137146 + 50.0 * 8.796287536621094
Epoch 820, val loss: 1.0574110746383667
Epoch 830, training loss: 440.83135986328125 = 1.0580711364746094 + 50.0 * 8.795465469360352
Epoch 830, val loss: 1.0566579103469849
Epoch 840, training loss: 440.99713134765625 = 1.057329773902893 + 50.0 * 8.798795700073242
Epoch 840, val loss: 1.0559104681015015
Epoch 850, training loss: 441.17156982421875 = 1.056580901145935 + 50.0 * 8.802299499511719
Epoch 850, val loss: 1.0551403760910034
Epoch 860, training loss: 441.2139892578125 = 1.055812954902649 + 50.0 * 8.803163528442383
Epoch 860, val loss: 1.054385781288147
Epoch 870, training loss: 441.2588195800781 = 1.0550487041473389 + 50.0 * 8.804075241088867
Epoch 870, val loss: 1.0536017417907715
Epoch 880, training loss: 441.5903015136719 = 1.0543094873428345 + 50.0 * 8.81071949005127
Epoch 880, val loss: 1.052847146987915
Epoch 890, training loss: 441.7725524902344 = 1.0535236597061157 + 50.0 * 8.814380645751953
Epoch 890, val loss: 1.052071213722229
Epoch 900, training loss: 441.4425354003906 = 1.052716612815857 + 50.0 * 8.807796478271484
Epoch 900, val loss: 1.051243543624878
Epoch 910, training loss: 441.6700439453125 = 1.0519022941589355 + 50.0 * 8.812362670898438
Epoch 910, val loss: 1.0504494905471802
Epoch 920, training loss: 441.9878845214844 = 1.0510810613632202 + 50.0 * 8.81873607635498
Epoch 920, val loss: 1.0496107339859009
Epoch 930, training loss: 441.833740234375 = 1.0503344535827637 + 50.0 * 8.815668106079102
Epoch 930, val loss: 1.0488460063934326
Epoch 940, training loss: 441.8453674316406 = 1.0495259761810303 + 50.0 * 8.815917015075684
Epoch 940, val loss: 1.0480446815490723
Epoch 950, training loss: 442.0735778808594 = 1.048710823059082 + 50.0 * 8.820497512817383
Epoch 950, val loss: 1.0472300052642822
Epoch 960, training loss: 442.32525634765625 = 1.0478870868682861 + 50.0 * 8.825547218322754
Epoch 960, val loss: 1.0463895797729492
Epoch 970, training loss: 442.21728515625 = 1.0470283031463623 + 50.0 * 8.823405265808105
Epoch 970, val loss: 1.0455495119094849
Epoch 980, training loss: 442.43634033203125 = 1.0461984872817993 + 50.0 * 8.827802658081055
Epoch 980, val loss: 1.0446909666061401
Epoch 990, training loss: 442.5784912109375 = 1.0453590154647827 + 50.0 * 8.830662727355957
Epoch 990, val loss: 1.043847918510437
Epoch 1000, training loss: 442.48583984375 = 1.0444869995117188 + 50.0 * 8.828826904296875
Epoch 1000, val loss: 1.0429986715316772
Epoch 1010, training loss: 442.7802429199219 = 1.0435925722122192 + 50.0 * 8.834733009338379
Epoch 1010, val loss: 1.042093276977539
Epoch 1020, training loss: 442.85137939453125 = 1.0427395105361938 + 50.0 * 8.836173057556152
Epoch 1020, val loss: 1.0412133932113647
Epoch 1030, training loss: 442.9527893066406 = 1.0418422222137451 + 50.0 * 8.838218688964844
Epoch 1030, val loss: 1.040330171585083
Epoch 1040, training loss: 443.2192077636719 = 1.0410041809082031 + 50.0 * 8.8435640335083
Epoch 1040, val loss: 1.0394926071166992
Epoch 1050, training loss: 443.1176452636719 = 1.0400750637054443 + 50.0 * 8.841551780700684
Epoch 1050, val loss: 1.038575291633606
Epoch 1060, training loss: 443.4295959472656 = 1.0391640663146973 + 50.0 * 8.847808837890625
Epoch 1060, val loss: 1.0376619100570679
Epoch 1070, training loss: 443.54022216796875 = 1.0382403135299683 + 50.0 * 8.8500394821167
Epoch 1070, val loss: 1.0367488861083984
Epoch 1080, training loss: 443.57562255859375 = 1.0373194217681885 + 50.0 * 8.8507661819458
Epoch 1080, val loss: 1.035813331604004
Epoch 1090, training loss: 443.4694519042969 = 1.0363659858703613 + 50.0 * 8.848661422729492
Epoch 1090, val loss: 1.0348583459854126
Epoch 1100, training loss: 444.1811828613281 = 1.0353405475616455 + 50.0 * 8.862916946411133
Epoch 1100, val loss: 1.0338698625564575
Epoch 1110, training loss: 443.3167724609375 = 1.0344003438949585 + 50.0 * 8.845647811889648
Epoch 1110, val loss: 1.032920241355896
Epoch 1120, training loss: 443.48443603515625 = 1.0334351062774658 + 50.0 * 8.849020004272461
Epoch 1120, val loss: 1.031939148902893
Epoch 1130, training loss: 444.0103454589844 = 1.032448649406433 + 50.0 * 8.85955810546875
Epoch 1130, val loss: 1.030960202217102
Epoch 1140, training loss: 444.22845458984375 = 1.0314913988113403 + 50.0 * 8.86393928527832
Epoch 1140, val loss: 1.0300202369689941
Epoch 1150, training loss: 444.4174499511719 = 1.0305047035217285 + 50.0 * 8.867738723754883
Epoch 1150, val loss: 1.029025673866272
Epoch 1160, training loss: 444.51409912109375 = 1.0294994115829468 + 50.0 * 8.869691848754883
Epoch 1160, val loss: 1.0280356407165527
Epoch 1170, training loss: 444.5440673828125 = 1.0284602642059326 + 50.0 * 8.870311737060547
Epoch 1170, val loss: 1.027004599571228
Epoch 1180, training loss: 444.5442199707031 = 1.0274454355239868 + 50.0 * 8.870335578918457
Epoch 1180, val loss: 1.0259920358657837
Epoch 1190, training loss: 444.7456359863281 = 1.0264167785644531 + 50.0 * 8.874383926391602
Epoch 1190, val loss: 1.0249712467193604
Epoch 1200, training loss: 444.8753662109375 = 1.0253705978393555 + 50.0 * 8.876999855041504
Epoch 1200, val loss: 1.0239222049713135
Epoch 1210, training loss: 444.128662109375 = 1.0242040157318115 + 50.0 * 8.862089157104492
Epoch 1210, val loss: 1.0227855443954468
Epoch 1220, training loss: 443.84814453125 = 1.0232394933700562 + 50.0 * 8.856497764587402
Epoch 1220, val loss: 1.0218234062194824
Epoch 1230, training loss: 444.61175537109375 = 1.0222082138061523 + 50.0 * 8.871790885925293
Epoch 1230, val loss: 1.0207465887069702
Epoch 1240, training loss: 444.9010314941406 = 1.0211520195007324 + 50.0 * 8.87759780883789
Epoch 1240, val loss: 1.0197392702102661
Epoch 1250, training loss: 445.345458984375 = 1.0200644731521606 + 50.0 * 8.886507987976074
Epoch 1250, val loss: 1.018662691116333
Epoch 1260, training loss: 445.6143798828125 = 1.0189485549926758 + 50.0 * 8.891908645629883
Epoch 1260, val loss: 1.0175646543502808
Epoch 1270, training loss: 445.7536315917969 = 1.0178498029708862 + 50.0 * 8.894715309143066
Epoch 1270, val loss: 1.0164759159088135
Epoch 1280, training loss: 445.778564453125 = 1.0167267322540283 + 50.0 * 8.89523696899414
Epoch 1280, val loss: 1.015363335609436
Epoch 1290, training loss: 446.69818115234375 = 1.0156006813049316 + 50.0 * 8.913651466369629
Epoch 1290, val loss: 1.0142697095870972
Epoch 1300, training loss: 445.62713623046875 = 1.0144054889678955 + 50.0 * 8.892254829406738
Epoch 1300, val loss: 1.0130490064620972
Epoch 1310, training loss: 445.7081298828125 = 1.01327645778656 + 50.0 * 8.89389705657959
Epoch 1310, val loss: 1.0119420289993286
Epoch 1320, training loss: 445.551025390625 = 1.012087106704712 + 50.0 * 8.890778541564941
Epoch 1320, val loss: 1.0107710361480713
Epoch 1330, training loss: 445.9310302734375 = 1.0109200477600098 + 50.0 * 8.898402214050293
Epoch 1330, val loss: 1.0096135139465332
Epoch 1340, training loss: 446.47894287109375 = 1.009777545928955 + 50.0 * 8.909383773803711
Epoch 1340, val loss: 1.008476972579956
Epoch 1350, training loss: 446.5226135253906 = 1.0086184740066528 + 50.0 * 8.910280227661133
Epoch 1350, val loss: 1.007324457168579
Epoch 1360, training loss: 446.6715087890625 = 1.0074522495269775 + 50.0 * 8.913281440734863
Epoch 1360, val loss: 1.0061646699905396
Epoch 1370, training loss: 446.92291259765625 = 1.0062354803085327 + 50.0 * 8.918334007263184
Epoch 1370, val loss: 1.004915714263916
Epoch 1380, training loss: 447.5218200683594 = 1.0050345659255981 + 50.0 * 8.930335998535156
Epoch 1380, val loss: 1.003732681274414
Epoch 1390, training loss: 447.5022277832031 = 1.003771185874939 + 50.0 * 8.92996883392334
Epoch 1390, val loss: 1.00248384475708
Epoch 1400, training loss: 447.6816101074219 = 1.0026156902313232 + 50.0 * 8.933579444885254
Epoch 1400, val loss: 1.0013427734375
Epoch 1410, training loss: 447.9875183105469 = 1.0014173984527588 + 50.0 * 8.939722061157227
Epoch 1410, val loss: 1.0001530647277832
Epoch 1420, training loss: 448.0032653808594 = 1.0001887083053589 + 50.0 * 8.940061569213867
Epoch 1420, val loss: 0.9989263415336609
Epoch 1430, training loss: 448.2160949707031 = 0.9989548325538635 + 50.0 * 8.944342613220215
Epoch 1430, val loss: 0.9977015256881714
Epoch 1440, training loss: 448.19830322265625 = 0.9976885914802551 + 50.0 * 8.944012641906738
Epoch 1440, val loss: 0.9964297413825989
Epoch 1450, training loss: 448.4125671386719 = 0.9964609742164612 + 50.0 * 8.948322296142578
Epoch 1450, val loss: 0.9952134490013123
Epoch 1460, training loss: 448.8100891113281 = 0.9952402710914612 + 50.0 * 8.956296920776367
Epoch 1460, val loss: 0.9940094947814941
Epoch 1470, training loss: 448.7409362792969 = 0.9939892888069153 + 50.0 * 8.954938888549805
Epoch 1470, val loss: 0.9927679896354675
Epoch 1480, training loss: 448.8882141113281 = 0.9927425384521484 + 50.0 * 8.95790958404541
Epoch 1480, val loss: 0.9915210008621216
Epoch 1490, training loss: 449.0194396972656 = 0.9914947748184204 + 50.0 * 8.960558891296387
Epoch 1490, val loss: 0.9902848601341248
Epoch 1500, training loss: 448.993896484375 = 0.9902582764625549 + 50.0 * 8.96007251739502
Epoch 1500, val loss: 0.9890544414520264
Epoch 1510, training loss: 448.9472961425781 = 0.9889758825302124 + 50.0 * 8.959166526794434
Epoch 1510, val loss: 0.9877881407737732
Epoch 1520, training loss: 449.0894775390625 = 0.9877298474311829 + 50.0 * 8.962035179138184
Epoch 1520, val loss: 0.9865491390228271
Epoch 1530, training loss: 449.3660888671875 = 0.9864989519119263 + 50.0 * 8.967591285705566
Epoch 1530, val loss: 0.985352098941803
Epoch 1540, training loss: 449.53125 = 0.9852274060249329 + 50.0 * 8.97092056274414
Epoch 1540, val loss: 0.9840880632400513
Epoch 1550, training loss: 449.5870056152344 = 0.9839555025100708 + 50.0 * 8.972061157226562
Epoch 1550, val loss: 0.9828230738639832
Epoch 1560, training loss: 449.6466064453125 = 0.9826790690422058 + 50.0 * 8.973278999328613
Epoch 1560, val loss: 0.9815717935562134
Epoch 1570, training loss: 450.038330078125 = 0.9814250469207764 + 50.0 * 8.981138229370117
Epoch 1570, val loss: 0.9803112149238586
Epoch 1580, training loss: 450.010498046875 = 0.980126678943634 + 50.0 * 8.980607032775879
Epoch 1580, val loss: 0.9790382385253906
Epoch 1590, training loss: 450.01806640625 = 0.9788641333580017 + 50.0 * 8.98078441619873
Epoch 1590, val loss: 0.9777945280075073
Epoch 1600, training loss: 450.1228942871094 = 0.9775658845901489 + 50.0 * 8.982906341552734
Epoch 1600, val loss: 0.976524293422699
Epoch 1610, training loss: 450.13580322265625 = 0.9762862324714661 + 50.0 * 8.983190536499023
Epoch 1610, val loss: 0.97524094581604
Epoch 1620, training loss: 450.32098388671875 = 0.974997878074646 + 50.0 * 8.986919403076172
Epoch 1620, val loss: 0.973980188369751
Epoch 1630, training loss: 449.6230163574219 = 0.9736842513084412 + 50.0 * 8.972986221313477
Epoch 1630, val loss: 0.9727328419685364
Epoch 1640, training loss: 449.6655578613281 = 0.9723989963531494 + 50.0 * 8.973862648010254
Epoch 1640, val loss: 0.9714570045471191
Epoch 1650, training loss: 450.1169128417969 = 0.9711328744888306 + 50.0 * 8.982915878295898
Epoch 1650, val loss: 0.97019362449646
Epoch 1660, training loss: 450.545654296875 = 0.9698389768600464 + 50.0 * 8.99151611328125
Epoch 1660, val loss: 0.9689233899116516
Epoch 1670, training loss: 450.8390808105469 = 0.9685268998146057 + 50.0 * 8.997410774230957
Epoch 1670, val loss: 0.9676250219345093
Epoch 1680, training loss: 450.81402587890625 = 0.9671986699104309 + 50.0 * 8.996936798095703
Epoch 1680, val loss: 0.9663193225860596
Epoch 1690, training loss: 451.04547119140625 = 0.9658888578414917 + 50.0 * 9.001591682434082
Epoch 1690, val loss: 0.9650193452835083
Epoch 1700, training loss: 451.1640319824219 = 0.9645590782165527 + 50.0 * 9.003989219665527
Epoch 1700, val loss: 0.963719367980957
Epoch 1710, training loss: 451.0328063964844 = 0.9632151126861572 + 50.0 * 9.001391410827637
Epoch 1710, val loss: 0.9624048471450806
Epoch 1720, training loss: 450.58001708984375 = 0.9618499875068665 + 50.0 * 8.992362976074219
Epoch 1720, val loss: 0.9611027836799622
Epoch 1730, training loss: 450.8277282714844 = 0.9605538845062256 + 50.0 * 8.997344017028809
Epoch 1730, val loss: 0.9597679972648621
Epoch 1740, training loss: 451.1602783203125 = 0.95920729637146 + 50.0 * 9.004021644592285
Epoch 1740, val loss: 0.9584682583808899
Epoch 1750, training loss: 451.3773498535156 = 0.9578722715377808 + 50.0 * 9.008389472961426
Epoch 1750, val loss: 0.9571524858474731
Epoch 1760, training loss: 451.3260498046875 = 0.9565345644950867 + 50.0 * 9.007390022277832
Epoch 1760, val loss: 0.9558333158493042
Epoch 1770, training loss: 451.4884338378906 = 0.9552446603775024 + 50.0 * 9.010663986206055
Epoch 1770, val loss: 0.9545666575431824
Epoch 1780, training loss: 451.3122863769531 = 0.9539636373519897 + 50.0 * 9.007166862487793
Epoch 1780, val loss: 0.9532933235168457
Epoch 1790, training loss: 451.3651123046875 = 0.9527007341384888 + 50.0 * 9.008248329162598
Epoch 1790, val loss: 0.9520300030708313
Epoch 1800, training loss: 450.7054138183594 = 0.9514161944389343 + 50.0 * 8.99507999420166
Epoch 1800, val loss: 0.9508029222488403
Epoch 1810, training loss: 451.3226623535156 = 0.9501168727874756 + 50.0 * 9.007451057434082
Epoch 1810, val loss: 0.9494574666023254
Epoch 1820, training loss: 449.7630920410156 = 0.9488590359687805 + 50.0 * 8.976284980773926
Epoch 1820, val loss: 0.9482932090759277
Epoch 1830, training loss: 448.7261657714844 = 0.9474180936813354 + 50.0 * 8.955574989318848
Epoch 1830, val loss: 0.9468480348587036
Epoch 1840, training loss: 450.0418701171875 = 0.9461225867271423 + 50.0 * 8.981914520263672
Epoch 1840, val loss: 0.9455835819244385
Epoch 1850, training loss: 450.58013916015625 = 0.9448187947273254 + 50.0 * 8.992706298828125
Epoch 1850, val loss: 0.9443144798278809
Epoch 1860, training loss: 451.16314697265625 = 0.9434712529182434 + 50.0 * 9.004393577575684
Epoch 1860, val loss: 0.9430043697357178
Epoch 1870, training loss: 451.85089111328125 = 0.9421568512916565 + 50.0 * 9.018174171447754
Epoch 1870, val loss: 0.9417216181755066
Epoch 1880, training loss: 452.1488952636719 = 0.9408175945281982 + 50.0 * 9.024161338806152
Epoch 1880, val loss: 0.9404037594795227
Epoch 1890, training loss: 452.6771545410156 = 0.9394466876983643 + 50.0 * 9.034753799438477
Epoch 1890, val loss: 0.9390610456466675
Epoch 1900, training loss: 452.9578552246094 = 0.9380767345428467 + 50.0 * 9.040395736694336
Epoch 1900, val loss: 0.9377307295799255
Epoch 1910, training loss: 452.9268798828125 = 0.9367049932479858 + 50.0 * 9.039803504943848
Epoch 1910, val loss: 0.9363963603973389
Epoch 1920, training loss: 453.0481872558594 = 0.9353522658348083 + 50.0 * 9.042256355285645
Epoch 1920, val loss: 0.9350969195365906
Epoch 1930, training loss: 453.44732666015625 = 0.934011697769165 + 50.0 * 9.05026626586914
Epoch 1930, val loss: 0.9338029623031616
Epoch 1940, training loss: 453.5047912597656 = 0.9326696991920471 + 50.0 * 9.05144214630127
Epoch 1940, val loss: 0.9324889779090881
Epoch 1950, training loss: 453.37725830078125 = 0.9313228726387024 + 50.0 * 9.048918724060059
Epoch 1950, val loss: 0.931195855140686
Epoch 1960, training loss: 453.5009460449219 = 0.9299988150596619 + 50.0 * 9.051419258117676
Epoch 1960, val loss: 0.9299137592315674
Epoch 1970, training loss: 453.8028259277344 = 0.9286752343177795 + 50.0 * 9.057482719421387
Epoch 1970, val loss: 0.9286275506019592
Epoch 1980, training loss: 453.81591796875 = 0.9273484349250793 + 50.0 * 9.057771682739258
Epoch 1980, val loss: 0.9273366332054138
Epoch 1990, training loss: 453.80572509765625 = 0.9260193109512329 + 50.0 * 9.057594299316406
Epoch 1990, val loss: 0.9260592460632324
Epoch 2000, training loss: 454.000732421875 = 0.9247204065322876 + 50.0 * 9.06152057647705
Epoch 2000, val loss: 0.924797773361206
Epoch 2010, training loss: 454.03778076171875 = 0.9234034419059753 + 50.0 * 9.062287330627441
Epoch 2010, val loss: 0.9235332012176514
Epoch 2020, training loss: 454.2841796875 = 0.9220966100692749 + 50.0 * 9.067241668701172
Epoch 2020, val loss: 0.9222665429115295
Epoch 2030, training loss: 454.49615478515625 = 0.9207793474197388 + 50.0 * 9.071507453918457
Epoch 2030, val loss: 0.9209886789321899
Epoch 2040, training loss: 454.3157043457031 = 0.9194396734237671 + 50.0 * 9.067925453186035
Epoch 2040, val loss: 0.9197105169296265
Epoch 2050, training loss: 454.4609680175781 = 0.9181116223335266 + 50.0 * 9.070857048034668
Epoch 2050, val loss: 0.9184292554855347
Epoch 2060, training loss: 454.6465148925781 = 0.9167792201042175 + 50.0 * 9.074594497680664
Epoch 2060, val loss: 0.9171497821807861
Epoch 2070, training loss: 454.76617431640625 = 0.9154486060142517 + 50.0 * 9.077014923095703
Epoch 2070, val loss: 0.9158650636672974
Epoch 2080, training loss: 454.6596984863281 = 0.9141046404838562 + 50.0 * 9.074912071228027
Epoch 2080, val loss: 0.9145824313163757
Epoch 2090, training loss: 454.8341369628906 = 0.9127719402313232 + 50.0 * 9.0784273147583
Epoch 2090, val loss: 0.9132944941520691
Epoch 2100, training loss: 454.7885437011719 = 0.9114593863487244 + 50.0 * 9.07754135131836
Epoch 2100, val loss: 0.912030816078186
Epoch 2110, training loss: 454.84722900390625 = 0.9101274609565735 + 50.0 * 9.078742027282715
Epoch 2110, val loss: 0.9107546210289001
Epoch 2120, training loss: 454.73504638671875 = 0.9088127017021179 + 50.0 * 9.07652473449707
Epoch 2120, val loss: 0.9094944596290588
Epoch 2130, training loss: 454.28485107421875 = 0.9074696898460388 + 50.0 * 9.067547798156738
Epoch 2130, val loss: 0.9081404209136963
Epoch 2140, training loss: 452.9388427734375 = 0.9060903787612915 + 50.0 * 9.040655136108398
Epoch 2140, val loss: 0.9068625569343567
Epoch 2150, training loss: 451.66448974609375 = 0.9047502279281616 + 50.0 * 9.0151948928833
Epoch 2150, val loss: 0.9055331349372864
Epoch 2160, training loss: 452.58837890625 = 0.9036197662353516 + 50.0 * 9.033695220947266
Epoch 2160, val loss: 0.9045358300209045
Epoch 2170, training loss: 452.8283386230469 = 0.902355432510376 + 50.0 * 9.038519859313965
Epoch 2170, val loss: 0.9033238887786865
Epoch 2180, training loss: 453.6106262207031 = 0.9010971188545227 + 50.0 * 9.054190635681152
Epoch 2180, val loss: 0.9021310806274414
Epoch 2190, training loss: 454.0029602050781 = 0.8998421430587769 + 50.0 * 9.06206226348877
Epoch 2190, val loss: 0.9009238481521606
Epoch 2200, training loss: 454.4197082519531 = 0.8985567688941956 + 50.0 * 9.070423126220703
Epoch 2200, val loss: 0.8996966481208801
Epoch 2210, training loss: 454.7908020019531 = 0.8972871899604797 + 50.0 * 9.07787036895752
Epoch 2210, val loss: 0.8984753489494324
Epoch 2220, training loss: 454.7381591796875 = 0.8960101008415222 + 50.0 * 9.07684326171875
Epoch 2220, val loss: 0.8972618579864502
Epoch 2230, training loss: 454.8291320800781 = 0.8947376012802124 + 50.0 * 9.07868766784668
Epoch 2230, val loss: 0.8960572481155396
Epoch 2240, training loss: 454.92315673828125 = 0.8934705853462219 + 50.0 * 9.080594062805176
Epoch 2240, val loss: 0.8948462605476379
Epoch 2250, training loss: 455.02545166015625 = 0.8922053575515747 + 50.0 * 9.082664489746094
Epoch 2250, val loss: 0.8936479091644287
Epoch 2260, training loss: 455.0224609375 = 0.8909516334533691 + 50.0 * 9.082630157470703
Epoch 2260, val loss: 0.8924603462219238
Epoch 2270, training loss: 455.16949462890625 = 0.8896940350532532 + 50.0 * 9.085596084594727
Epoch 2270, val loss: 0.8912598490715027
Epoch 2280, training loss: 455.2314453125 = 0.8884649276733398 + 50.0 * 9.086859703063965
Epoch 2280, val loss: 0.8900781273841858
Epoch 2290, training loss: 455.27947998046875 = 0.8872330188751221 + 50.0 * 9.087844848632812
Epoch 2290, val loss: 0.8889153599739075
Epoch 2300, training loss: 455.2785339355469 = 0.8860102295875549 + 50.0 * 9.087850570678711
Epoch 2300, val loss: 0.8877458572387695
Epoch 2310, training loss: 455.388916015625 = 0.8847923874855042 + 50.0 * 9.090082168579102
Epoch 2310, val loss: 0.886593222618103
Epoch 2320, training loss: 454.8809814453125 = 0.8835468292236328 + 50.0 * 9.079948425292969
Epoch 2320, val loss: 0.8854241967201233
Epoch 2330, training loss: 454.87799072265625 = 0.8823705315589905 + 50.0 * 9.079912185668945
Epoch 2330, val loss: 0.8843180537223816
Epoch 2340, training loss: 455.1634521484375 = 0.8811632394790649 + 50.0 * 9.08564567565918
Epoch 2340, val loss: 0.8831789493560791
Epoch 2350, training loss: 455.3973388671875 = 0.8799789547920227 + 50.0 * 9.090347290039062
Epoch 2350, val loss: 0.8820706605911255
Epoch 2360, training loss: 455.7084655761719 = 0.8787920475006104 + 50.0 * 9.096593856811523
Epoch 2360, val loss: 0.8809393048286438
Epoch 2370, training loss: 455.7026672363281 = 0.8776043653488159 + 50.0 * 9.096501350402832
Epoch 2370, val loss: 0.8798103928565979
Epoch 2380, training loss: 455.43560791015625 = 0.8764334917068481 + 50.0 * 9.09118366241455
Epoch 2380, val loss: 0.8786994814872742
Epoch 2390, training loss: 455.6815185546875 = 0.8752846121788025 + 50.0 * 9.096124649047852
Epoch 2390, val loss: 0.8776201009750366
Epoch 2400, training loss: 455.8522033691406 = 0.8741252422332764 + 50.0 * 9.09956169128418
Epoch 2400, val loss: 0.876525342464447
Epoch 2410, training loss: 455.9599609375 = 0.872957706451416 + 50.0 * 9.101739883422852
Epoch 2410, val loss: 0.875451385974884
Epoch 2420, training loss: 455.8387756347656 = 0.8717904090881348 + 50.0 * 9.099339485168457
Epoch 2420, val loss: 0.8743864297866821
Epoch 2430, training loss: 455.82867431640625 = 0.870619535446167 + 50.0 * 9.099161148071289
Epoch 2430, val loss: 0.8733161687850952
Epoch 2440, training loss: 456.0395202636719 = 0.86947101354599 + 50.0 * 9.103401184082031
Epoch 2440, val loss: 0.872251570224762
Epoch 2450, training loss: 456.0940246582031 = 0.8683261275291443 + 50.0 * 9.104514122009277
Epoch 2450, val loss: 0.871191680431366
Epoch 2460, training loss: 456.20562744140625 = 0.8671891689300537 + 50.0 * 9.106768608093262
Epoch 2460, val loss: 0.8701300024986267
Epoch 2470, training loss: 456.1614990234375 = 0.8660416603088379 + 50.0 * 9.10590934753418
Epoch 2470, val loss: 0.869030773639679
Epoch 2480, training loss: 455.0477294921875 = 0.8649712204933167 + 50.0 * 9.08365535736084
Epoch 2480, val loss: 0.8680710792541504
Epoch 2490, training loss: 454.1376647949219 = 0.8638813495635986 + 50.0 * 9.065475463867188
Epoch 2490, val loss: 0.8670835494995117
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.5492753623188406
0.863725277113671
=== training gcn model ===
Epoch 0, training loss: 512.6864013671875 = 1.1072205305099487 + 50.0 * 10.231583595275879
Epoch 0, val loss: 1.1072202920913696
Epoch 10, training loss: 492.1128234863281 = 1.1072062253952026 + 50.0 * 9.820112228393555
Epoch 10, val loss: 1.107190728187561
Epoch 20, training loss: 482.2944030761719 = 1.1069133281707764 + 50.0 * 9.623749732971191
Epoch 20, val loss: 1.1068857908248901
Epoch 30, training loss: 475.0877685546875 = 1.1065391302108765 + 50.0 * 9.47962474822998
Epoch 30, val loss: 1.1065126657485962
Epoch 40, training loss: 469.41351318359375 = 1.1061638593673706 + 50.0 * 9.3661470413208
Epoch 40, val loss: 1.1061419248580933
Epoch 50, training loss: 464.7193908691406 = 1.1057888269424438 + 50.0 * 9.272272109985352
Epoch 50, val loss: 1.1057720184326172
Epoch 60, training loss: 460.7373046875 = 1.105424165725708 + 50.0 * 9.19263744354248
Epoch 60, val loss: 1.1054081916809082
Epoch 70, training loss: 457.3780822753906 = 1.1050434112548828 + 50.0 * 9.125460624694824
Epoch 70, val loss: 1.1050299406051636
Epoch 80, training loss: 454.4808044433594 = 1.1046439409255981 + 50.0 * 9.067523002624512
Epoch 80, val loss: 1.1046390533447266
Epoch 90, training loss: 451.9596862792969 = 1.1042537689208984 + 50.0 * 9.017108917236328
Epoch 90, val loss: 1.104249358177185
Epoch 100, training loss: 449.83856201171875 = 1.1038480997085571 + 50.0 * 8.97469425201416
Epoch 100, val loss: 1.1038461923599243
Epoch 110, training loss: 447.9803161621094 = 1.1034212112426758 + 50.0 * 8.937538146972656
Epoch 110, val loss: 1.103424072265625
Epoch 120, training loss: 446.3739013671875 = 1.1029707193374634 + 50.0 * 8.905418395996094
Epoch 120, val loss: 1.1029739379882812
Epoch 130, training loss: 445.0606384277344 = 1.1023571491241455 + 50.0 * 8.879165649414062
Epoch 130, val loss: 1.1022953987121582
Epoch 140, training loss: 443.84552001953125 = 1.1015669107437134 + 50.0 * 8.854879379272461
Epoch 140, val loss: 1.1014608144760132
Epoch 150, training loss: 442.84051513671875 = 1.1007784605026245 + 50.0 * 8.834794998168945
Epoch 150, val loss: 1.1006447076797485
Epoch 160, training loss: 441.8699645996094 = 1.0999938249588013 + 50.0 * 8.815399169921875
Epoch 160, val loss: 1.0998271703720093
Epoch 170, training loss: 441.1004638671875 = 1.0992258787155151 + 50.0 * 8.80002498626709
Epoch 170, val loss: 1.0990351438522339
Epoch 180, training loss: 440.5382080078125 = 1.0984675884246826 + 50.0 * 8.78879451751709
Epoch 180, val loss: 1.098254919052124
Epoch 190, training loss: 439.89739990234375 = 1.0977177619934082 + 50.0 * 8.775993347167969
Epoch 190, val loss: 1.0974864959716797
Epoch 200, training loss: 439.42071533203125 = 1.09697687625885 + 50.0 * 8.766474723815918
Epoch 200, val loss: 1.0967366695404053
Epoch 210, training loss: 438.95611572265625 = 1.0962493419647217 + 50.0 * 8.757197380065918
Epoch 210, val loss: 1.095994472503662
Epoch 220, training loss: 438.6315612792969 = 1.0955615043640137 + 50.0 * 8.750720024108887
Epoch 220, val loss: 1.0952924489974976
Epoch 230, training loss: 438.7057800292969 = 1.0949041843414307 + 50.0 * 8.752217292785645
Epoch 230, val loss: 1.0946283340454102
Epoch 240, training loss: 438.3350524902344 = 1.0942950248718262 + 50.0 * 8.7448148727417
Epoch 240, val loss: 1.094001054763794
Epoch 250, training loss: 438.06610107421875 = 1.0937284231185913 + 50.0 * 8.739447593688965
Epoch 250, val loss: 1.0934311151504517
Epoch 260, training loss: 437.8959045410156 = 1.0932304859161377 + 50.0 * 8.736053466796875
Epoch 260, val loss: 1.0929310321807861
Epoch 270, training loss: 438.0478515625 = 1.0927660465240479 + 50.0 * 8.73910140991211
Epoch 270, val loss: 1.09245765209198
Epoch 280, training loss: 437.4985656738281 = 1.092349886894226 + 50.0 * 8.728124618530273
Epoch 280, val loss: 1.0920158624649048
Epoch 290, training loss: 437.7149658203125 = 1.0919125080108643 + 50.0 * 8.732460975646973
Epoch 290, val loss: 1.0915851593017578
Epoch 300, training loss: 437.5751647949219 = 1.091483473777771 + 50.0 * 8.729673385620117
Epoch 300, val loss: 1.0911465883255005
Epoch 310, training loss: 437.6156005859375 = 1.0910532474517822 + 50.0 * 8.730490684509277
Epoch 310, val loss: 1.0907279253005981
Epoch 320, training loss: 437.49737548828125 = 1.0906203985214233 + 50.0 * 8.728135108947754
Epoch 320, val loss: 1.0902889966964722
Epoch 330, training loss: 437.6293640136719 = 1.0901851654052734 + 50.0 * 8.730783462524414
Epoch 330, val loss: 1.0898520946502686
Epoch 340, training loss: 437.6095275878906 = 1.0897361040115356 + 50.0 * 8.730396270751953
Epoch 340, val loss: 1.0894017219543457
Epoch 350, training loss: 437.7818298339844 = 1.0892640352249146 + 50.0 * 8.733851432800293
Epoch 350, val loss: 1.0889264345169067
Epoch 360, training loss: 438.0661926269531 = 1.088809609413147 + 50.0 * 8.739547729492188
Epoch 360, val loss: 1.0884723663330078
Epoch 370, training loss: 437.9593811035156 = 1.088327407836914 + 50.0 * 8.737421035766602
Epoch 370, val loss: 1.0879921913146973
Epoch 380, training loss: 437.63134765625 = 1.0878173112869263 + 50.0 * 8.730870246887207
Epoch 380, val loss: 1.0874871015548706
Epoch 390, training loss: 437.66583251953125 = 1.0873055458068848 + 50.0 * 8.73157024383545
Epoch 390, val loss: 1.0869789123535156
Epoch 400, training loss: 438.0799560546875 = 1.0867940187454224 + 50.0 * 8.739863395690918
Epoch 400, val loss: 1.0864734649658203
Epoch 410, training loss: 438.15216064453125 = 1.0862821340560913 + 50.0 * 8.741317749023438
Epoch 410, val loss: 1.0859692096710205
Epoch 420, training loss: 438.35382080078125 = 1.0857439041137695 + 50.0 * 8.745361328125
Epoch 420, val loss: 1.085428237915039
Epoch 430, training loss: 438.4872131347656 = 1.0852023363113403 + 50.0 * 8.748040199279785
Epoch 430, val loss: 1.0848926305770874
Epoch 440, training loss: 438.5904235839844 = 1.084646463394165 + 50.0 * 8.750115394592285
Epoch 440, val loss: 1.084336519241333
Epoch 450, training loss: 439.36102294921875 = 1.0840712785720825 + 50.0 * 8.765539169311523
Epoch 450, val loss: 1.083763599395752
Epoch 460, training loss: 439.6017150878906 = 1.0834484100341797 + 50.0 * 8.770365715026855
Epoch 460, val loss: 1.0831767320632935
Epoch 470, training loss: 438.1163330078125 = 1.082823395729065 + 50.0 * 8.740670204162598
Epoch 470, val loss: 1.0825453996658325
Epoch 480, training loss: 438.7564697265625 = 1.0822780132293701 + 50.0 * 8.753483772277832
Epoch 480, val loss: 1.0820063352584839
Epoch 490, training loss: 438.85369873046875 = 1.0816667079925537 + 50.0 * 8.755440711975098
Epoch 490, val loss: 1.0814192295074463
Epoch 500, training loss: 439.0854187011719 = 1.0810253620147705 + 50.0 * 8.760087966918945
Epoch 500, val loss: 1.0807777643203735
Epoch 510, training loss: 439.3916320800781 = 1.0803775787353516 + 50.0 * 8.76622486114502
Epoch 510, val loss: 1.0801483392715454
Epoch 520, training loss: 439.5888366699219 = 1.079723834991455 + 50.0 * 8.770182609558105
Epoch 520, val loss: 1.0795022249221802
Epoch 530, training loss: 439.54962158203125 = 1.0790226459503174 + 50.0 * 8.76941204071045
Epoch 530, val loss: 1.0788134336471558
Epoch 540, training loss: 439.7039489746094 = 1.0783255100250244 + 50.0 * 8.772512435913086
Epoch 540, val loss: 1.0781347751617432
Epoch 550, training loss: 440.0964050292969 = 1.0776344537734985 + 50.0 * 8.780375480651855
Epoch 550, val loss: 1.0774494409561157
Epoch 560, training loss: 440.10577392578125 = 1.0769011974334717 + 50.0 * 8.780577659606934
Epoch 560, val loss: 1.0767502784729004
Epoch 570, training loss: 440.4039306640625 = 1.0761741399765015 + 50.0 * 8.786555290222168
Epoch 570, val loss: 1.0760345458984375
Epoch 580, training loss: 440.5270080566406 = 1.0754190683364868 + 50.0 * 8.789031982421875
Epoch 580, val loss: 1.0752997398376465
Epoch 590, training loss: 440.5978698730469 = 1.0746155977249146 + 50.0 * 8.790465354919434
Epoch 590, val loss: 1.0745025873184204
Epoch 600, training loss: 440.7393798828125 = 1.0738152265548706 + 50.0 * 8.79331111907959
Epoch 600, val loss: 1.0737310647964478
Epoch 610, training loss: 441.0378112792969 = 1.0729233026504517 + 50.0 * 8.799297332763672
Epoch 610, val loss: 1.0728946924209595
Epoch 620, training loss: 440.4823303222656 = 1.0720893144607544 + 50.0 * 8.78820514678955
Epoch 620, val loss: 1.0720629692077637
Epoch 630, training loss: 441.0190124511719 = 1.0712196826934814 + 50.0 * 8.798955917358398
Epoch 630, val loss: 1.07121741771698
Epoch 640, training loss: 441.3141174316406 = 1.0702980756759644 + 50.0 * 8.804876327514648
Epoch 640, val loss: 1.0703204870224
Epoch 650, training loss: 441.6358642578125 = 1.0693475008010864 + 50.0 * 8.811330795288086
Epoch 650, val loss: 1.0693981647491455
Epoch 660, training loss: 441.45697021484375 = 1.068336009979248 + 50.0 * 8.807772636413574
Epoch 660, val loss: 1.068413496017456
Epoch 670, training loss: 441.9400634765625 = 1.0673658847808838 + 50.0 * 8.81745433807373
Epoch 670, val loss: 1.0674620866775513
Epoch 680, training loss: 442.1045837402344 = 1.0663362741470337 + 50.0 * 8.820764541625977
Epoch 680, val loss: 1.0664740800857544
Epoch 690, training loss: 442.2508239746094 = 1.0652927160263062 + 50.0 * 8.823710441589355
Epoch 690, val loss: 1.0654528141021729
Epoch 700, training loss: 442.4354553222656 = 1.0642313957214355 + 50.0 * 8.827424049377441
Epoch 700, val loss: 1.0644315481185913
Epoch 710, training loss: 442.6448974609375 = 1.0631608963012695 + 50.0 * 8.831634521484375
Epoch 710, val loss: 1.063382863998413
Epoch 720, training loss: 442.74322509765625 = 1.0620561838150024 + 50.0 * 8.833623886108398
Epoch 720, val loss: 1.0623183250427246
Epoch 730, training loss: 442.8594970703125 = 1.0609468221664429 + 50.0 * 8.835970878601074
Epoch 730, val loss: 1.0612339973449707
Epoch 740, training loss: 443.0914306640625 = 1.0598232746124268 + 50.0 * 8.840632438659668
Epoch 740, val loss: 1.060133695602417
Epoch 750, training loss: 443.12347412109375 = 1.0586698055267334 + 50.0 * 8.841296195983887
Epoch 750, val loss: 1.0590249300003052
Epoch 760, training loss: 443.2440185546875 = 1.057515025138855 + 50.0 * 8.843729972839355
Epoch 760, val loss: 1.0578901767730713
Epoch 770, training loss: 443.3501281738281 = 1.056333303451538 + 50.0 * 8.84587574005127
Epoch 770, val loss: 1.056742548942566
Epoch 780, training loss: 442.7167663574219 = 1.0550800561904907 + 50.0 * 8.833233833312988
Epoch 780, val loss: 1.0555213689804077
Epoch 790, training loss: 442.6492004394531 = 1.0538827180862427 + 50.0 * 8.83190631866455
Epoch 790, val loss: 1.0543569326400757
Epoch 800, training loss: 442.6228942871094 = 1.052658200263977 + 50.0 * 8.831404685974121
Epoch 800, val loss: 1.0531632900238037
Epoch 810, training loss: 442.9293212890625 = 1.0514880418777466 + 50.0 * 8.837556838989258
Epoch 810, val loss: 1.0520212650299072
Epoch 820, training loss: 443.24053955078125 = 1.0502525568008423 + 50.0 * 8.843805313110352
Epoch 820, val loss: 1.0508227348327637
Epoch 830, training loss: 443.4603576660156 = 1.0489774942398071 + 50.0 * 8.848227500915527
Epoch 830, val loss: 1.049588680267334
Epoch 840, training loss: 443.73797607421875 = 1.047688603401184 + 50.0 * 8.853805541992188
Epoch 840, val loss: 1.0483465194702148
Epoch 850, training loss: 443.7107849121094 = 1.0463650226593018 + 50.0 * 8.853288650512695
Epoch 850, val loss: 1.047062635421753
Epoch 860, training loss: 443.855712890625 = 1.0450471639633179 + 50.0 * 8.856213569641113
Epoch 860, val loss: 1.045780062675476
Epoch 870, training loss: 444.08074951171875 = 1.0437028408050537 + 50.0 * 8.860740661621094
Epoch 870, val loss: 1.044474720954895
Epoch 880, training loss: 444.2921447753906 = 1.0423476696014404 + 50.0 * 8.864995956420898
Epoch 880, val loss: 1.043148159980774
Epoch 890, training loss: 444.1955871582031 = 1.040945053100586 + 50.0 * 8.863092422485352
Epoch 890, val loss: 1.041786789894104
Epoch 900, training loss: 444.39984130859375 = 1.0395307540893555 + 50.0 * 8.867206573486328
Epoch 900, val loss: 1.0404295921325684
Epoch 910, training loss: 444.7723388671875 = 1.0381269454956055 + 50.0 * 8.87468433380127
Epoch 910, val loss: 1.0390774011611938
Epoch 920, training loss: 444.4901123046875 = 1.0366578102111816 + 50.0 * 8.86906909942627
Epoch 920, val loss: 1.0376521348953247
Epoch 930, training loss: 444.8202209472656 = 1.0351793766021729 + 50.0 * 8.875700950622559
Epoch 930, val loss: 1.036211371421814
Epoch 940, training loss: 444.32916259765625 = 1.0336294174194336 + 50.0 * 8.865910530090332
Epoch 940, val loss: 1.0347298383712769
Epoch 950, training loss: 444.5580749511719 = 1.0321011543273926 + 50.0 * 8.870519638061523
Epoch 950, val loss: 1.0332486629486084
Epoch 960, training loss: 445.0266418457031 = 1.0306190252304077 + 50.0 * 8.879920959472656
Epoch 960, val loss: 1.0318187475204468
Epoch 970, training loss: 445.2673034667969 = 1.029097318649292 + 50.0 * 8.884764671325684
Epoch 970, val loss: 1.0303436517715454
Epoch 980, training loss: 445.2456359863281 = 1.027520775794983 + 50.0 * 8.88436222076416
Epoch 980, val loss: 1.0288219451904297
Epoch 990, training loss: 445.3961181640625 = 1.0259490013122559 + 50.0 * 8.88740348815918
Epoch 990, val loss: 1.0272856950759888
Epoch 1000, training loss: 445.6211242675781 = 1.024346947669983 + 50.0 * 8.891935348510742
Epoch 1000, val loss: 1.0257337093353271
Epoch 1010, training loss: 445.611328125 = 1.0227046012878418 + 50.0 * 8.891772270202637
Epoch 1010, val loss: 1.0241514444351196
Epoch 1020, training loss: 445.8004455566406 = 1.0210793018341064 + 50.0 * 8.895586967468262
Epoch 1020, val loss: 1.022584319114685
Epoch 1030, training loss: 445.9764404296875 = 1.0194216966629028 + 50.0 * 8.899140357971191
Epoch 1030, val loss: 1.0209795236587524
Epoch 1040, training loss: 445.831298828125 = 1.0177083015441895 + 50.0 * 8.896271705627441
Epoch 1040, val loss: 1.0193254947662354
Epoch 1050, training loss: 445.2481689453125 = 1.0160431861877441 + 50.0 * 8.884642601013184
Epoch 1050, val loss: 1.0177022218704224
Epoch 1060, training loss: 445.38616943359375 = 1.0143215656280518 + 50.0 * 8.887436866760254
Epoch 1060, val loss: 1.016059160232544
Epoch 1070, training loss: 445.7124328613281 = 1.0126582384109497 + 50.0 * 8.89399528503418
Epoch 1070, val loss: 1.0144463777542114
Epoch 1080, training loss: 445.9109191894531 = 1.0108952522277832 + 50.0 * 8.898000717163086
Epoch 1080, val loss: 1.0127167701721191
Epoch 1090, training loss: 446.3680419921875 = 1.0091944932937622 + 50.0 * 8.907176971435547
Epoch 1090, val loss: 1.0110806226730347
Epoch 1100, training loss: 446.6381530761719 = 1.0074489116668701 + 50.0 * 8.912613868713379
Epoch 1100, val loss: 1.0094012022018433
Epoch 1110, training loss: 446.742919921875 = 1.0056560039520264 + 50.0 * 8.914745330810547
Epoch 1110, val loss: 1.00766122341156
Epoch 1120, training loss: 446.75103759765625 = 1.0038508176803589 + 50.0 * 8.91494369506836
Epoch 1120, val loss: 1.0059242248535156
Epoch 1130, training loss: 447.017822265625 = 1.0020573139190674 + 50.0 * 8.920315742492676
Epoch 1130, val loss: 1.0041791200637817
Epoch 1140, training loss: 446.5780334472656 = 1.0001916885375977 + 50.0 * 8.9115571975708
Epoch 1140, val loss: 1.0023925304412842
Epoch 1150, training loss: 446.4637451171875 = 0.9983821511268616 + 50.0 * 8.909307479858398
Epoch 1150, val loss: 1.0006641149520874
Epoch 1160, training loss: 446.8021240234375 = 0.9965178370475769 + 50.0 * 8.916111946105957
Epoch 1160, val loss: 0.9988424777984619
Epoch 1170, training loss: 447.0882263183594 = 0.9946743249893188 + 50.0 * 8.921871185302734
Epoch 1170, val loss: 0.9970424175262451
Epoch 1180, training loss: 447.133544921875 = 0.9927672743797302 + 50.0 * 8.922815322875977
Epoch 1180, val loss: 0.9952116012573242
Epoch 1190, training loss: 447.3125305175781 = 0.9909175634384155 + 50.0 * 8.926432609558105
Epoch 1190, val loss: 0.993416428565979
Epoch 1200, training loss: 447.357177734375 = 0.989007294178009 + 50.0 * 8.927363395690918
Epoch 1200, val loss: 0.9915734529495239
Epoch 1210, training loss: 446.407958984375 = 0.98702073097229 + 50.0 * 8.908418655395508
Epoch 1210, val loss: 0.9896915555000305
Epoch 1220, training loss: 446.17315673828125 = 0.9851628541946411 + 50.0 * 8.903759956359863
Epoch 1220, val loss: 0.9878736734390259
Epoch 1230, training loss: 446.55303955078125 = 0.9832454323768616 + 50.0 * 8.911396026611328
Epoch 1230, val loss: 0.986005961894989
Epoch 1240, training loss: 446.92303466796875 = 0.981380820274353 + 50.0 * 8.918832778930664
Epoch 1240, val loss: 0.9842049479484558
Epoch 1250, training loss: 447.3140869140625 = 0.9794875383377075 + 50.0 * 8.926692008972168
Epoch 1250, val loss: 0.9823948740959167
Epoch 1260, training loss: 447.3406982421875 = 0.9775452017784119 + 50.0 * 8.927263259887695
Epoch 1260, val loss: 0.9805370569229126
Epoch 1270, training loss: 447.7610168457031 = 0.975610613822937 + 50.0 * 8.935708045959473
Epoch 1270, val loss: 0.9786672592163086
Epoch 1280, training loss: 447.9997863769531 = 0.9736550450325012 + 50.0 * 8.940522193908691
Epoch 1280, val loss: 0.9767866134643555
Epoch 1290, training loss: 448.1120300292969 = 0.9716950058937073 + 50.0 * 8.942806243896484
Epoch 1290, val loss: 0.9749069213867188
Epoch 1300, training loss: 448.03912353515625 = 0.9697204828262329 + 50.0 * 8.941388130187988
Epoch 1300, val loss: 0.9730015993118286
Epoch 1310, training loss: 445.8985290527344 = 0.9675987958908081 + 50.0 * 8.898618698120117
Epoch 1310, val loss: 0.9710027575492859
Epoch 1320, training loss: 448.0672302246094 = 0.9661427736282349 + 50.0 * 8.942021369934082
Epoch 1320, val loss: 0.969560444355011
Epoch 1330, training loss: 447.29595947265625 = 0.9640328288078308 + 50.0 * 8.92663860321045
Epoch 1330, val loss: 0.9675670862197876
Epoch 1340, training loss: 447.4612121582031 = 0.9620158076286316 + 50.0 * 8.929984092712402
Epoch 1340, val loss: 0.965627908706665
Epoch 1350, training loss: 448.0265197753906 = 0.9600768089294434 + 50.0 * 8.941329002380371
Epoch 1350, val loss: 0.9637753963470459
Epoch 1360, training loss: 448.48858642578125 = 0.9581248760223389 + 50.0 * 8.95060920715332
Epoch 1360, val loss: 0.9619045257568359
Epoch 1370, training loss: 448.8394775390625 = 0.9561384916305542 + 50.0 * 8.957666397094727
Epoch 1370, val loss: 0.9600223898887634
Epoch 1380, training loss: 448.8849792480469 = 0.9541434049606323 + 50.0 * 8.958617210388184
Epoch 1380, val loss: 0.958109974861145
Epoch 1390, training loss: 449.264892578125 = 0.9521816372871399 + 50.0 * 8.966254234313965
Epoch 1390, val loss: 0.9562391042709351
Epoch 1400, training loss: 449.194580078125 = 0.9502062201499939 + 50.0 * 8.964887619018555
Epoch 1400, val loss: 0.9543684720993042
Epoch 1410, training loss: 449.4027099609375 = 0.9482485055923462 + 50.0 * 8.96908950805664
Epoch 1410, val loss: 0.9524819254875183
Epoch 1420, training loss: 446.9481201171875 = 0.9462615251541138 + 50.0 * 8.920037269592285
Epoch 1420, val loss: 0.9506053328514099
Epoch 1430, training loss: 449.08245849609375 = 0.9446163177490234 + 50.0 * 8.962757110595703
Epoch 1430, val loss: 0.9489853382110596
Epoch 1440, training loss: 448.7135314941406 = 0.9426484107971191 + 50.0 * 8.95541763305664
Epoch 1440, val loss: 0.9471539258956909
Epoch 1450, training loss: 448.9898986816406 = 0.9406881332397461 + 50.0 * 8.960984230041504
Epoch 1450, val loss: 0.9452520608901978
Epoch 1460, training loss: 449.4738464355469 = 0.9387807250022888 + 50.0 * 8.970701217651367
Epoch 1460, val loss: 0.9434250593185425
Epoch 1470, training loss: 450.2682189941406 = 0.9368945956230164 + 50.0 * 8.986626625061035
Epoch 1470, val loss: 0.9416224956512451
Epoch 1480, training loss: 450.7691650390625 = 0.934969961643219 + 50.0 * 8.996684074401855
Epoch 1480, val loss: 0.9397941827774048
Epoch 1490, training loss: 451.01898193359375 = 0.9330394864082336 + 50.0 * 9.001718521118164
Epoch 1490, val loss: 0.9379575848579407
Epoch 1500, training loss: 451.1859436035156 = 0.9311281442642212 + 50.0 * 9.005096435546875
Epoch 1500, val loss: 0.9361474514007568
Epoch 1510, training loss: 451.402587890625 = 0.9292117357254028 + 50.0 * 9.009468078613281
Epoch 1510, val loss: 0.9343320727348328
Epoch 1520, training loss: 451.2904968261719 = 0.927282989025116 + 50.0 * 9.007264137268066
Epoch 1520, val loss: 0.9325128197669983
Epoch 1530, training loss: 451.6030578613281 = 0.9253798127174377 + 50.0 * 9.013553619384766
Epoch 1530, val loss: 0.9307048320770264
Epoch 1540, training loss: 451.7911071777344 = 0.9234893321990967 + 50.0 * 9.017352104187012
Epoch 1540, val loss: 0.9289356470108032
Epoch 1550, training loss: 451.8631286621094 = 0.9216004610061646 + 50.0 * 9.018830299377441
Epoch 1550, val loss: 0.927154541015625
Epoch 1560, training loss: 451.9360046386719 = 0.9197188019752502 + 50.0 * 9.020325660705566
Epoch 1560, val loss: 0.9253812432289124
Epoch 1570, training loss: 451.8367919921875 = 0.9178462624549866 + 50.0 * 9.018379211425781
Epoch 1570, val loss: 0.9236406087875366
Epoch 1580, training loss: 452.0575256347656 = 0.9159873127937317 + 50.0 * 9.022830963134766
Epoch 1580, val loss: 0.921881914138794
Epoch 1590, training loss: 452.1434020996094 = 0.9141278862953186 + 50.0 * 9.024585723876953
Epoch 1590, val loss: 0.9201611876487732
Epoch 1600, training loss: 452.09722900390625 = 0.9122867584228516 + 50.0 * 9.023698806762695
Epoch 1600, val loss: 0.9184306859970093
Epoch 1610, training loss: 452.19854736328125 = 0.9104548692703247 + 50.0 * 9.025761604309082
Epoch 1610, val loss: 0.9167349338531494
Epoch 1620, training loss: 452.2426452636719 = 0.9086498022079468 + 50.0 * 9.026679992675781
Epoch 1620, val loss: 0.9150506854057312
Epoch 1630, training loss: 452.4476623535156 = 0.9068610072135925 + 50.0 * 9.030816078186035
Epoch 1630, val loss: 0.9133793711662292
Epoch 1640, training loss: 452.58563232421875 = 0.9050673842430115 + 50.0 * 9.033611297607422
Epoch 1640, val loss: 0.9117057919502258
Epoch 1650, training loss: 452.6181945800781 = 0.9032886028289795 + 50.0 * 9.034297943115234
Epoch 1650, val loss: 0.9100369215011597
Epoch 1660, training loss: 452.6007995605469 = 0.9015857577323914 + 50.0 * 9.033984184265137
Epoch 1660, val loss: 0.9084612131118774
Epoch 1670, training loss: 452.60882568359375 = 0.8999202251434326 + 50.0 * 9.034177780151367
Epoch 1670, val loss: 0.9069017171859741
Epoch 1680, training loss: 452.7257080078125 = 0.8981905579566956 + 50.0 * 9.036550521850586
Epoch 1680, val loss: 0.9052753448486328
Epoch 1690, training loss: 452.8363342285156 = 0.8964424133300781 + 50.0 * 9.038797378540039
Epoch 1690, val loss: 0.9036408066749573
Epoch 1700, training loss: 452.947021484375 = 0.8947087526321411 + 50.0 * 9.041046142578125
Epoch 1700, val loss: 0.9020161628723145
Epoch 1710, training loss: 453.00653076171875 = 0.8930319547653198 + 50.0 * 9.042269706726074
Epoch 1710, val loss: 0.9004786610603333
Epoch 1720, training loss: 453.0898132324219 = 0.8913577795028687 + 50.0 * 9.04396915435791
Epoch 1720, val loss: 0.8987951874732971
Epoch 1730, training loss: 453.20513916015625 = 0.8872342109680176 + 50.0 * 9.046358108520508
Epoch 1730, val loss: 0.8947622179985046
Epoch 1740, training loss: 453.1561584472656 = 0.8825015425682068 + 50.0 * 9.045473098754883
Epoch 1740, val loss: 0.8903180956840515
Epoch 1750, training loss: 453.31500244140625 = 0.8780222535133362 + 50.0 * 9.048739433288574
Epoch 1750, val loss: 0.8861090540885925
Epoch 1760, training loss: 453.0914001464844 = 0.8738054037094116 + 50.0 * 9.044351577758789
Epoch 1760, val loss: 0.8821664452552795
Epoch 1770, training loss: 453.3578796386719 = 0.8698357343673706 + 50.0 * 9.049760818481445
Epoch 1770, val loss: 0.878451406955719
Epoch 1780, training loss: 453.5885314941406 = 0.866029679775238 + 50.0 * 9.054450035095215
Epoch 1780, val loss: 0.8748794198036194
Epoch 1790, training loss: 453.48992919921875 = 0.8623617887496948 + 50.0 * 9.05255126953125
Epoch 1790, val loss: 0.8714444041252136
Epoch 1800, training loss: 453.28973388671875 = 0.858811616897583 + 50.0 * 9.04861831665039
Epoch 1800, val loss: 0.8680980205535889
Epoch 1810, training loss: 453.21630859375 = 0.8552842140197754 + 50.0 * 9.047220230102539
Epoch 1810, val loss: 0.8648076057434082
Epoch 1820, training loss: 453.5837097167969 = 0.8518646955490112 + 50.0 * 9.05463695526123
Epoch 1820, val loss: 0.8615885376930237
Epoch 1830, training loss: 453.7367858886719 = 0.8484853506088257 + 50.0 * 9.05776596069336
Epoch 1830, val loss: 0.8584111928939819
Epoch 1840, training loss: 453.9654846191406 = 0.8451501131057739 + 50.0 * 9.062406539916992
Epoch 1840, val loss: 0.8552997708320618
Epoch 1850, training loss: 453.9497375488281 = 0.8418492078781128 + 50.0 * 9.06215763092041
Epoch 1850, val loss: 0.8521935939788818
Epoch 1860, training loss: 453.83587646484375 = 0.8385741710662842 + 50.0 * 9.059946060180664
Epoch 1860, val loss: 0.8491194844245911
Epoch 1870, training loss: 453.8259582519531 = 0.8353105187416077 + 50.0 * 9.059813499450684
Epoch 1870, val loss: 0.846070408821106
Epoch 1880, training loss: 453.9680480957031 = 0.832112729549408 + 50.0 * 9.062718391418457
Epoch 1880, val loss: 0.8430570960044861
Epoch 1890, training loss: 454.1299133300781 = 0.8289576768875122 + 50.0 * 9.066019058227539
Epoch 1890, val loss: 0.8400733470916748
Epoch 1900, training loss: 454.3614196777344 = 0.8257847428321838 + 50.0 * 9.07071304321289
Epoch 1900, val loss: 0.8370852470397949
Epoch 1910, training loss: 454.44403076171875 = 0.8226280212402344 + 50.0 * 9.072427749633789
Epoch 1910, val loss: 0.834115207195282
Epoch 1920, training loss: 454.41571044921875 = 0.8194700479507446 + 50.0 * 9.071925163269043
Epoch 1920, val loss: 0.8311518430709839
Epoch 1930, training loss: 454.515869140625 = 0.8163058757781982 + 50.0 * 9.073990821838379
Epoch 1930, val loss: 0.8281785845756531
Epoch 1940, training loss: 454.4808044433594 = 0.8131726980209351 + 50.0 * 9.073352813720703
Epoch 1940, val loss: 0.8252410888671875
Epoch 1950, training loss: 453.40167236328125 = 0.8100609183311462 + 50.0 * 9.05183219909668
Epoch 1950, val loss: 0.8223357200622559
Epoch 1960, training loss: 453.3363952636719 = 0.8070714473724365 + 50.0 * 9.050586700439453
Epoch 1960, val loss: 0.819564163684845
Epoch 1970, training loss: 453.4720153808594 = 0.8039191365242004 + 50.0 * 9.053361892700195
Epoch 1970, val loss: 0.8165320158004761
Epoch 1980, training loss: 454.2624206542969 = 0.8007862567901611 + 50.0 * 9.069232940673828
Epoch 1980, val loss: 0.8135524392127991
Epoch 1990, training loss: 454.4353332519531 = 0.7975788116455078 + 50.0 * 9.072754859924316
Epoch 1990, val loss: 0.8105105757713318
Epoch 2000, training loss: 454.6338195800781 = 0.7944403886795044 + 50.0 * 9.076787948608398
Epoch 2000, val loss: 0.8075669407844543
Epoch 2010, training loss: 454.74822998046875 = 0.7912763357162476 + 50.0 * 9.07913875579834
Epoch 2010, val loss: 0.8045734167098999
Epoch 2020, training loss: 454.8009948730469 = 0.788115918636322 + 50.0 * 9.080257415771484
Epoch 2020, val loss: 0.8015986680984497
Epoch 2030, training loss: 454.8058166503906 = 0.7849544882774353 + 50.0 * 9.08041763305664
Epoch 2030, val loss: 0.7986266016960144
Epoch 2040, training loss: 454.91473388671875 = 0.7818058133125305 + 50.0 * 9.082658767700195
Epoch 2040, val loss: 0.7956417202949524
Epoch 2050, training loss: 454.50634765625 = 0.7787867784500122 + 50.0 * 9.074551582336426
Epoch 2050, val loss: 0.7929002642631531
Epoch 2060, training loss: 454.65313720703125 = 0.7757969498634338 + 50.0 * 9.077547073364258
Epoch 2060, val loss: 0.790063202381134
Epoch 2070, training loss: 455.0374755859375 = 0.7727196216583252 + 50.0 * 9.085295677185059
Epoch 2070, val loss: 0.7871395349502563
Epoch 2080, training loss: 455.2082824707031 = 0.7695999145507812 + 50.0 * 9.088773727416992
Epoch 2080, val loss: 0.7841816544532776
Epoch 2090, training loss: 455.1084899902344 = 0.7664453983306885 + 50.0 * 9.086840629577637
Epoch 2090, val loss: 0.7812132835388184
Epoch 2100, training loss: 454.8786315917969 = 0.7632664442062378 + 50.0 * 9.082306861877441
Epoch 2100, val loss: 0.7781979441642761
Epoch 2110, training loss: 455.1134033203125 = 0.7601401209831238 + 50.0 * 9.087065696716309
Epoch 2110, val loss: 0.7752365469932556
Epoch 2120, training loss: 455.3848876953125 = 0.7569975852966309 + 50.0 * 9.092557907104492
Epoch 2120, val loss: 0.7722728252410889
Epoch 2130, training loss: 455.386962890625 = 0.7538061738014221 + 50.0 * 9.092662811279297
Epoch 2130, val loss: 0.7692750096321106
Epoch 2140, training loss: 455.412109375 = 0.7506195306777954 + 50.0 * 9.093230247497559
Epoch 2140, val loss: 0.7662476897239685
Epoch 2150, training loss: 455.2862243652344 = 0.7474390268325806 + 50.0 * 9.090775489807129
Epoch 2150, val loss: 0.7632730603218079
Epoch 2160, training loss: 455.4277038574219 = 0.7442545294761658 + 50.0 * 9.093668937683105
Epoch 2160, val loss: 0.7602241039276123
Epoch 2170, training loss: 455.51617431640625 = 0.7409847974777222 + 50.0 * 9.095503807067871
Epoch 2170, val loss: 0.7571107149124146
Epoch 2180, training loss: 455.5882568359375 = 0.7377443909645081 + 50.0 * 9.097010612487793
Epoch 2180, val loss: 0.7540621161460876
Epoch 2190, training loss: 455.7831115722656 = 0.7345012426376343 + 50.0 * 9.100972175598145
Epoch 2190, val loss: 0.7510000467300415
Epoch 2200, training loss: 455.717041015625 = 0.7312255501747131 + 50.0 * 9.099716186523438
Epoch 2200, val loss: 0.7478850483894348
Epoch 2210, training loss: 455.6531066894531 = 0.7279207110404968 + 50.0 * 9.098503112792969
Epoch 2210, val loss: 0.7447640895843506
Epoch 2220, training loss: 455.83538818359375 = 0.7247229814529419 + 50.0 * 9.102212905883789
Epoch 2220, val loss: 0.7417172193527222
Epoch 2230, training loss: 455.8050842285156 = 0.721443772315979 + 50.0 * 9.101673126220703
Epoch 2230, val loss: 0.7386094927787781
Epoch 2240, training loss: 455.93621826171875 = 0.7181269526481628 + 50.0 * 9.104361534118652
Epoch 2240, val loss: 0.7354804277420044
Epoch 2250, training loss: 455.96234130859375 = 0.7147637009620667 + 50.0 * 9.104951858520508
Epoch 2250, val loss: 0.7323148250579834
Epoch 2260, training loss: 455.8762512207031 = 0.7114308476448059 + 50.0 * 9.103296279907227
Epoch 2260, val loss: 0.7291406393051147
Epoch 2270, training loss: 456.039794921875 = 0.7081080675125122 + 50.0 * 9.106634140014648
Epoch 2270, val loss: 0.7259504199028015
Epoch 2280, training loss: 456.16448974609375 = 0.7047417163848877 + 50.0 * 9.1091947555542
Epoch 2280, val loss: 0.7227768898010254
Epoch 2290, training loss: 456.3138122558594 = 0.7013600468635559 + 50.0 * 9.112249374389648
Epoch 2290, val loss: 0.7195495367050171
Epoch 2300, training loss: 456.4319763183594 = 0.6979748606681824 + 50.0 * 9.114680290222168
Epoch 2300, val loss: 0.7163284420967102
Epoch 2310, training loss: 456.3507385253906 = 0.6945794224739075 + 50.0 * 9.113122940063477
Epoch 2310, val loss: 0.7131308913230896
Epoch 2320, training loss: 456.0824279785156 = 0.6911417841911316 + 50.0 * 9.107826232910156
Epoch 2320, val loss: 0.7098376154899597
Epoch 2330, training loss: 456.3214416503906 = 0.6877920031547546 + 50.0 * 9.112672805786133
Epoch 2330, val loss: 0.7067180275917053
Epoch 2340, training loss: 456.23675537109375 = 0.6844030618667603 + 50.0 * 9.11104679107666
Epoch 2340, val loss: 0.7035413384437561
Epoch 2350, training loss: 456.284912109375 = 0.6809501647949219 + 50.0 * 9.112079620361328
Epoch 2350, val loss: 0.7002710700035095
Epoch 2360, training loss: 456.490234375 = 0.6774859428405762 + 50.0 * 9.116254806518555
Epoch 2360, val loss: 0.6969923973083496
Epoch 2370, training loss: 456.4238586425781 = 0.67397540807724 + 50.0 * 9.114997863769531
Epoch 2370, val loss: 0.6936771273612976
Epoch 2380, training loss: 452.59747314453125 = 0.670300304889679 + 50.0 * 9.038543701171875
Epoch 2380, val loss: 0.6901869177818298
Epoch 2390, training loss: 453.8293762207031 = 0.6672369241714478 + 50.0 * 9.06324291229248
Epoch 2390, val loss: 0.6871565580368042
Epoch 2400, training loss: 454.290283203125 = 0.6635488271713257 + 50.0 * 9.072534561157227
Epoch 2400, val loss: 0.6837204098701477
Epoch 2410, training loss: 454.93603515625 = 0.6597095727920532 + 50.0 * 9.085526466369629
Epoch 2410, val loss: 0.6801204681396484
Epoch 2420, training loss: 455.2840270996094 = 0.656026303768158 + 50.0 * 9.092559814453125
Epoch 2420, val loss: 0.6766325831413269
Epoch 2430, training loss: 455.69561767578125 = 0.6523028612136841 + 50.0 * 9.100866317749023
Epoch 2430, val loss: 0.6731039881706238
Epoch 2440, training loss: 456.0927734375 = 0.6486120223999023 + 50.0 * 9.108882904052734
Epoch 2440, val loss: 0.6695984601974487
Epoch 2450, training loss: 456.0859069824219 = 0.644892156124115 + 50.0 * 9.108819961547852
Epoch 2450, val loss: 0.6660853028297424
Epoch 2460, training loss: 456.15997314453125 = 0.6412413120269775 + 50.0 * 9.110374450683594
Epoch 2460, val loss: 0.6626357436180115
Epoch 2470, training loss: 456.3629455566406 = 0.6376103758811951 + 50.0 * 9.114506721496582
Epoch 2470, val loss: 0.6592125296592712
Epoch 2480, training loss: 456.5590515136719 = 0.6339886784553528 + 50.0 * 9.118500709533691
Epoch 2480, val loss: 0.6558030843734741
Epoch 2490, training loss: 456.5722351074219 = 0.6303865313529968 + 50.0 * 9.118836402893066
Epoch 2490, val loss: 0.6524172425270081
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8121739130434782
0.8647395493733248
The final CL Acc:0.63565, 0.12483, The final GNN Acc:0.86387, 0.00066
Begin epxeriment: noisy_level: 0.2 cont_weight: 50 epoch: der1:0.4 der2:0 dfr1:0.4 dfr2:0
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.0, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106644])
remove edge: torch.Size([2, 70882])
updated graph: torch.Size([2, 88878])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:227: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 531.287109375 = 2.1743404865264893 + 50.0 * 10.582256317138672
Epoch 0, val loss: 2.174811363220215
Epoch 10, training loss: 531.2639770507812 = 2.1662484407424927 + 50.0 * 10.581954002380371
Epoch 10, val loss: 2.1670172214508057
Epoch 20, training loss: 531.2102661132812 = 2.1608108282089233 + 50.0 * 10.580988883972168
Epoch 20, val loss: 2.161930561065674
Epoch 30, training loss: 531.0167846679688 = 2.158845543861389 + 50.0 * 10.577157974243164
Epoch 30, val loss: 2.1602110862731934
Epoch 40, training loss: 530.24267578125 = 2.159477472305298 + 50.0 * 10.561663627624512
Epoch 40, val loss: 2.1609678268432617
Epoch 50, training loss: 527.6174926757812 = 2.161710023880005 + 50.0 * 10.509116172790527
Epoch 50, val loss: 2.1631288528442383
Epoch 60, training loss: 520.1815795898438 = 2.1644195318222046 + 50.0 * 10.360342979431152
Epoch 60, val loss: 2.165506362915039
Epoch 70, training loss: 500.7516784667969 = 2.166683793067932 + 50.0 * 9.971699714660645
Epoch 70, val loss: 2.1675162315368652
Epoch 80, training loss: 479.2897033691406 = 2.1701581478118896 + 50.0 * 9.542390823364258
Epoch 80, val loss: 2.170854091644287
Epoch 90, training loss: 472.33648681640625 = 2.171063780784607 + 50.0 * 9.403308868408203
Epoch 90, val loss: 2.1714601516723633
Epoch 100, training loss: 467.72064208984375 = 2.1679532527923584 + 50.0 * 9.311054229736328
Epoch 100, val loss: 2.168409824371338
Epoch 110, training loss: 466.2431640625 = 2.164386749267578 + 50.0 * 9.281575202941895
Epoch 110, val loss: 2.164966106414795
Epoch 120, training loss: 464.3985595703125 = 2.162056565284729 + 50.0 * 9.244729995727539
Epoch 120, val loss: 2.1627416610717773
Epoch 130, training loss: 461.6481628417969 = 2.160606622695923 + 50.0 * 9.189750671386719
Epoch 130, val loss: 2.1613805294036865
Epoch 140, training loss: 457.859130859375 = 2.159480571746826 + 50.0 * 9.113992691040039
Epoch 140, val loss: 2.1603140830993652
Epoch 150, training loss: 453.9750061035156 = 2.158494472503662 + 50.0 * 9.036330223083496
Epoch 150, val loss: 2.1593446731567383
Epoch 160, training loss: 451.51385498046875 = 2.158028721809387 + 50.0 * 8.987116813659668
Epoch 160, val loss: 2.1588518619537354
Epoch 170, training loss: 449.2501220703125 = 2.157920002937317 + 50.0 * 8.94184398651123
Epoch 170, val loss: 2.1587347984313965
Epoch 180, training loss: 447.17974853515625 = 2.1581881046295166 + 50.0 * 8.900430679321289
Epoch 180, val loss: 2.15899395942688
Epoch 190, training loss: 446.06646728515625 = 2.158353090286255 + 50.0 * 8.878162384033203
Epoch 190, val loss: 2.159135341644287
Epoch 200, training loss: 444.8342590332031 = 2.158205509185791 + 50.0 * 8.853521347045898
Epoch 200, val loss: 2.158951759338379
Epoch 210, training loss: 443.27642822265625 = 2.1579668521881104 + 50.0 * 8.822369575500488
Epoch 210, val loss: 2.1587045192718506
Epoch 220, training loss: 441.6164855957031 = 2.157928466796875 + 50.0 * 8.78917121887207
Epoch 220, val loss: 2.1586685180664062
Epoch 230, training loss: 440.120361328125 = 2.158033847808838 + 50.0 * 8.759246826171875
Epoch 230, val loss: 2.1587460041046143
Epoch 240, training loss: 438.8356018066406 = 2.157991051673889 + 50.0 * 8.733551979064941
Epoch 240, val loss: 2.1586761474609375
Epoch 250, training loss: 437.8389892578125 = 2.15776264667511 + 50.0 * 8.713624000549316
Epoch 250, val loss: 2.1584324836730957
Epoch 260, training loss: 437.10443115234375 = 2.157370090484619 + 50.0 * 8.698941230773926
Epoch 260, val loss: 2.15804123878479
Epoch 270, training loss: 436.53924560546875 = 2.156962752342224 + 50.0 * 8.68764591217041
Epoch 270, val loss: 2.157637119293213
Epoch 280, training loss: 436.1644287109375 = 2.1565998792648315 + 50.0 * 8.680156707763672
Epoch 280, val loss: 2.1572418212890625
Epoch 290, training loss: 435.74517822265625 = 2.156161665916443 + 50.0 * 8.671780586242676
Epoch 290, val loss: 2.1568286418914795
Epoch 300, training loss: 435.35626220703125 = 2.155775308609009 + 50.0 * 8.664010047912598
Epoch 300, val loss: 2.1564340591430664
Epoch 310, training loss: 434.8997802734375 = 2.155417323112488 + 50.0 * 8.654887199401855
Epoch 310, val loss: 2.1560850143432617
Epoch 320, training loss: 434.3927917480469 = 2.155121326446533 + 50.0 * 8.644753456115723
Epoch 320, val loss: 2.1557939052581787
Epoch 330, training loss: 433.9747314453125 = 2.154813051223755 + 50.0 * 8.636398315429688
Epoch 330, val loss: 2.1554975509643555
Epoch 340, training loss: 433.40478515625 = 2.154542565345764 + 50.0 * 8.625004768371582
Epoch 340, val loss: 2.155264377593994
Epoch 350, training loss: 432.9942321777344 = 2.1543368101119995 + 50.0 * 8.616798400878906
Epoch 350, val loss: 2.155010938644409
Epoch 360, training loss: 432.5974426269531 = 2.1540344953536987 + 50.0 * 8.608868598937988
Epoch 360, val loss: 2.1547393798828125
Epoch 370, training loss: 432.2687683105469 = 2.153771758079529 + 50.0 * 8.602299690246582
Epoch 370, val loss: 2.1544833183288574
Epoch 380, training loss: 431.9842529296875 = 2.1534746885299683 + 50.0 * 8.5966157913208
Epoch 380, val loss: 2.1542015075683594
Epoch 390, training loss: 431.7479553222656 = 2.1531896591186523 + 50.0 * 8.59189510345459
Epoch 390, val loss: 2.153897762298584
Epoch 400, training loss: 431.5299987792969 = 2.152852416038513 + 50.0 * 8.587542533874512
Epoch 400, val loss: 2.1535816192626953
Epoch 410, training loss: 431.2851257324219 = 2.1525192260742188 + 50.0 * 8.58265209197998
Epoch 410, val loss: 2.1532721519470215
Epoch 420, training loss: 431.0545654296875 = 2.152207851409912 + 50.0 * 8.578046798706055
Epoch 420, val loss: 2.152966022491455
Epoch 430, training loss: 430.8258972167969 = 2.151909112930298 + 50.0 * 8.573479652404785
Epoch 430, val loss: 2.152672052383423
Epoch 440, training loss: 430.69268798828125 = 2.151626467704773 + 50.0 * 8.570821762084961
Epoch 440, val loss: 2.1523776054382324
Epoch 450, training loss: 430.4710693359375 = 2.151274085044861 + 50.0 * 8.56639575958252
Epoch 450, val loss: 2.1520566940307617
Epoch 460, training loss: 430.2437438964844 = 2.150951623916626 + 50.0 * 8.561856269836426
Epoch 460, val loss: 2.151740074157715
Epoch 470, training loss: 430.0462646484375 = 2.1506296396255493 + 50.0 * 8.557912826538086
Epoch 470, val loss: 2.1514244079589844
Epoch 480, training loss: 429.8648986816406 = 2.150312304496765 + 50.0 * 8.554291725158691
Epoch 480, val loss: 2.1511080265045166
Epoch 490, training loss: 429.6699523925781 = 2.1500115394592285 + 50.0 * 8.550398826599121
Epoch 490, val loss: 2.15079927444458
Epoch 500, training loss: 429.70916748046875 = 2.1497429609298706 + 50.0 * 8.551188468933105
Epoch 500, val loss: 2.1504805088043213
Epoch 510, training loss: 429.3432312011719 = 2.1493865251541138 + 50.0 * 8.543876647949219
Epoch 510, val loss: 2.150156021118164
Epoch 520, training loss: 429.1165466308594 = 2.149076819419861 + 50.0 * 8.539349555969238
Epoch 520, val loss: 2.149850368499756
Epoch 530, training loss: 428.916748046875 = 2.1487690210342407 + 50.0 * 8.535359382629395
Epoch 530, val loss: 2.1495518684387207
Epoch 540, training loss: 428.742431640625 = 2.1484665870666504 + 50.0 * 8.531879425048828
Epoch 540, val loss: 2.149261474609375
Epoch 550, training loss: 428.58282470703125 = 2.148157477378845 + 50.0 * 8.528693199157715
Epoch 550, val loss: 2.148942470550537
Epoch 560, training loss: 428.4833679199219 = 2.1478593349456787 + 50.0 * 8.526710510253906
Epoch 560, val loss: 2.1486287117004395
Epoch 570, training loss: 428.2782897949219 = 2.147546172142029 + 50.0 * 8.522614479064941
Epoch 570, val loss: 2.148332118988037
Epoch 580, training loss: 428.12298583984375 = 2.1472396850585938 + 50.0 * 8.519515037536621
Epoch 580, val loss: 2.1480331420898438
Epoch 590, training loss: 427.9770202636719 = 2.146951675415039 + 50.0 * 8.5166015625
Epoch 590, val loss: 2.1477441787719727
Epoch 600, training loss: 427.83331298828125 = 2.146667242050171 + 50.0 * 8.51373291015625
Epoch 600, val loss: 2.1474618911743164
Epoch 610, training loss: 427.6980895996094 = 2.1463881731033325 + 50.0 * 8.51103401184082
Epoch 610, val loss: 2.1471831798553467
Epoch 620, training loss: 427.7614440917969 = 2.146094799041748 + 50.0 * 8.512307167053223
Epoch 620, val loss: 2.146920680999756
Epoch 630, training loss: 427.5795593261719 = 2.1458317041397095 + 50.0 * 8.508674621582031
Epoch 630, val loss: 2.1465988159179688
Epoch 640, training loss: 427.3782043457031 = 2.1455068588256836 + 50.0 * 8.504653930664062
Epoch 640, val loss: 2.14630126953125
Epoch 650, training loss: 427.2724304199219 = 2.1452001333236694 + 50.0 * 8.502544403076172
Epoch 650, val loss: 2.1460139751434326
Epoch 660, training loss: 427.159912109375 = 2.1449252367019653 + 50.0 * 8.500299453735352
Epoch 660, val loss: 2.145728588104248
Epoch 670, training loss: 427.0627746582031 = 2.144639492034912 + 50.0 * 8.49836254119873
Epoch 670, val loss: 2.145446300506592
Epoch 680, training loss: 426.9648742675781 = 2.1443543434143066 + 50.0 * 8.496410369873047
Epoch 680, val loss: 2.1451659202575684
Epoch 690, training loss: 426.8926696777344 = 2.1440593004226685 + 50.0 * 8.494972229003906
Epoch 690, val loss: 2.144890546798706
Epoch 700, training loss: 426.85107421875 = 2.1437909603118896 + 50.0 * 8.494145393371582
Epoch 700, val loss: 2.144573926925659
Epoch 710, training loss: 426.7403259277344 = 2.1434810161590576 + 50.0 * 8.491936683654785
Epoch 710, val loss: 2.144279718399048
Epoch 720, training loss: 426.63751220703125 = 2.1431933641433716 + 50.0 * 8.489886283874512
Epoch 720, val loss: 2.1440024375915527
Epoch 730, training loss: 426.53436279296875 = 2.1429072618484497 + 50.0 * 8.487829208374023
Epoch 730, val loss: 2.143725872039795
Epoch 740, training loss: 426.44903564453125 = 2.1426308155059814 + 50.0 * 8.486127853393555
Epoch 740, val loss: 2.1434507369995117
Epoch 750, training loss: 426.37872314453125 = 2.1423457860946655 + 50.0 * 8.48472785949707
Epoch 750, val loss: 2.1431679725646973
Epoch 760, training loss: 426.28009033203125 = 2.142046332359314 + 50.0 * 8.482760429382324
Epoch 760, val loss: 2.142878532409668
Epoch 770, training loss: 426.2050476074219 = 2.1417675018310547 + 50.0 * 8.481266021728516
Epoch 770, val loss: 2.142604351043701
Epoch 780, training loss: 426.1186828613281 = 2.141499400138855 + 50.0 * 8.479543685913086
Epoch 780, val loss: 2.1423380374908447
Epoch 790, training loss: 426.0286865234375 = 2.1412376165390015 + 50.0 * 8.47774887084961
Epoch 790, val loss: 2.142077922821045
Epoch 800, training loss: 425.94256591796875 = 2.1409767866134644 + 50.0 * 8.476031303405762
Epoch 800, val loss: 2.1418216228485107
Epoch 810, training loss: 425.860595703125 = 2.1407196521759033 + 50.0 * 8.474397659301758
Epoch 810, val loss: 2.141568183898926
Epoch 820, training loss: 425.9359130859375 = 2.1404287815093994 + 50.0 * 8.475909233093262
Epoch 820, val loss: 2.141315460205078
Epoch 830, training loss: 425.77471923828125 = 2.140151858329773 + 50.0 * 8.472691535949707
Epoch 830, val loss: 2.1410279273986816
Epoch 840, training loss: 425.6494445800781 = 2.1399024724960327 + 50.0 * 8.47019100189209
Epoch 840, val loss: 2.140760898590088
Epoch 850, training loss: 425.5750427246094 = 2.13963520526886 + 50.0 * 8.468708038330078
Epoch 850, val loss: 2.140493631362915
Epoch 860, training loss: 425.50872802734375 = 2.1393699645996094 + 50.0 * 8.467387199401855
Epoch 860, val loss: 2.1402249336242676
Epoch 870, training loss: 425.4869079589844 = 2.139106035232544 + 50.0 * 8.46695613861084
Epoch 870, val loss: 2.1399502754211426
Epoch 880, training loss: 425.4108581542969 = 2.1388096809387207 + 50.0 * 8.46544075012207
Epoch 880, val loss: 2.1396687030792236
Epoch 890, training loss: 425.3392028808594 = 2.1385037899017334 + 50.0 * 8.464014053344727
Epoch 890, val loss: 2.1393747329711914
Epoch 900, training loss: 425.2874755859375 = 2.138218641281128 + 50.0 * 8.462985038757324
Epoch 900, val loss: 2.139092206954956
Epoch 910, training loss: 425.2520751953125 = 2.1379237174987793 + 50.0 * 8.46228313446045
Epoch 910, val loss: 2.1388044357299805
Epoch 920, training loss: 425.3672790527344 = 2.137647032737732 + 50.0 * 8.464592933654785
Epoch 920, val loss: 2.1385090351104736
Epoch 930, training loss: 425.1476745605469 = 2.137320637702942 + 50.0 * 8.460206985473633
Epoch 930, val loss: 2.1382064819335938
Epoch 940, training loss: 425.1181335449219 = 2.137015223503113 + 50.0 * 8.459622383117676
Epoch 940, val loss: 2.137920379638672
Epoch 950, training loss: 425.0577697753906 = 2.136732578277588 + 50.0 * 8.458420753479004
Epoch 950, val loss: 2.1376352310180664
Epoch 960, training loss: 425.02325439453125 = 2.1364399194717407 + 50.0 * 8.457736015319824
Epoch 960, val loss: 2.1373515129089355
Epoch 970, training loss: 425.3009948730469 = 2.1361026763916016 + 50.0 * 8.463297843933105
Epoch 970, val loss: 2.137059211730957
Epoch 980, training loss: 424.94775390625 = 2.135818600654602 + 50.0 * 8.456238746643066
Epoch 980, val loss: 2.13675594329834
Epoch 990, training loss: 424.9022216796875 = 2.1355397701263428 + 50.0 * 8.455333709716797
Epoch 990, val loss: 2.1364660263061523
Epoch 1000, training loss: 424.86688232421875 = 2.1352591514587402 + 50.0 * 8.454632759094238
Epoch 1000, val loss: 2.1361827850341797
Epoch 1010, training loss: 424.816650390625 = 2.1349704265594482 + 50.0 * 8.453633308410645
Epoch 1010, val loss: 2.13590407371521
Epoch 1020, training loss: 424.83612060546875 = 2.1346765756607056 + 50.0 * 8.454029083251953
Epoch 1020, val loss: 2.1356329917907715
Epoch 1030, training loss: 424.7837829589844 = 2.134396195411682 + 50.0 * 8.452987670898438
Epoch 1030, val loss: 2.135340452194214
Epoch 1040, training loss: 424.7071228027344 = 2.1341058015823364 + 50.0 * 8.451460838317871
Epoch 1040, val loss: 2.1350674629211426
Epoch 1050, training loss: 424.6725769042969 = 2.1338268518447876 + 50.0 * 8.450775146484375
Epoch 1050, val loss: 2.134793758392334
Epoch 1060, training loss: 424.62628173828125 = 2.133551001548767 + 50.0 * 8.449854850769043
Epoch 1060, val loss: 2.1345226764678955
Epoch 1070, training loss: 424.5918273925781 = 2.1332781314849854 + 50.0 * 8.44917106628418
Epoch 1070, val loss: 2.1342568397521973
Epoch 1080, training loss: 424.8011169433594 = 2.133001923561096 + 50.0 * 8.453362464904785
Epoch 1080, val loss: 2.1339855194091797
Epoch 1090, training loss: 424.55145263671875 = 2.1327134370803833 + 50.0 * 8.44837474822998
Epoch 1090, val loss: 2.1337039470672607
Epoch 1100, training loss: 424.50592041015625 = 2.1324366331100464 + 50.0 * 8.447469711303711
Epoch 1100, val loss: 2.133434295654297
Epoch 1110, training loss: 424.4527893066406 = 2.132170557975769 + 50.0 * 8.446412086486816
Epoch 1110, val loss: 2.1331710815429688
Epoch 1120, training loss: 424.43365478515625 = 2.131893277168274 + 50.0 * 8.446035385131836
Epoch 1120, val loss: 2.1329214572906494
Epoch 1130, training loss: 424.5161437988281 = 2.1316065788269043 + 50.0 * 8.447690963745117
Epoch 1130, val loss: 2.13265323638916
Epoch 1140, training loss: 424.3664855957031 = 2.1313669681549072 + 50.0 * 8.4447021484375
Epoch 1140, val loss: 2.1323862075805664
Epoch 1150, training loss: 424.3222961425781 = 2.1311068534851074 + 50.0 * 8.44382381439209
Epoch 1150, val loss: 2.132124185562134
Epoch 1160, training loss: 424.28936767578125 = 2.130833625793457 + 50.0 * 8.443170547485352
Epoch 1160, val loss: 2.1318769454956055
Epoch 1170, training loss: 424.25531005859375 = 2.130591034889221 + 50.0 * 8.44249439239502
Epoch 1170, val loss: 2.1316261291503906
Epoch 1180, training loss: 424.2275390625 = 2.13033127784729 + 50.0 * 8.441944122314453
Epoch 1180, val loss: 2.131378173828125
Epoch 1190, training loss: 424.30572509765625 = 2.1300711631774902 + 50.0 * 8.443512916564941
Epoch 1190, val loss: 2.1311287879943848
Epoch 1200, training loss: 424.3459167480469 = 2.129819631576538 + 50.0 * 8.444321632385254
Epoch 1200, val loss: 2.1308529376983643
Epoch 1210, training loss: 424.1706848144531 = 2.1295201778411865 + 50.0 * 8.440823554992676
Epoch 1210, val loss: 2.1305999755859375
Epoch 1220, training loss: 424.1338806152344 = 2.1292792558670044 + 50.0 * 8.440092086791992
Epoch 1220, val loss: 2.1303486824035645
Epoch 1230, training loss: 424.0928649902344 = 2.129026412963867 + 50.0 * 8.439276695251465
Epoch 1230, val loss: 2.130103349685669
Epoch 1240, training loss: 424.0687255859375 = 2.12877094745636 + 50.0 * 8.438798904418945
Epoch 1240, val loss: 2.1298611164093018
Epoch 1250, training loss: 424.0484313964844 = 2.128520965576172 + 50.0 * 8.438398361206055
Epoch 1250, val loss: 2.129619836807251
Epoch 1260, training loss: 424.1965026855469 = 2.1282477378845215 + 50.0 * 8.441365242004395
Epoch 1260, val loss: 2.1293764114379883
Epoch 1270, training loss: 424.09100341796875 = 2.1280336380004883 + 50.0 * 8.43925952911377
Epoch 1270, val loss: 2.129105567932129
Epoch 1280, training loss: 424.00701904296875 = 2.1277313232421875 + 50.0 * 8.437585830688477
Epoch 1280, val loss: 2.128859281539917
Epoch 1290, training loss: 423.9507751464844 = 2.1275120973587036 + 50.0 * 8.4364652633667
Epoch 1290, val loss: 2.128619432449341
Epoch 1300, training loss: 423.9367980957031 = 2.1272623538970947 + 50.0 * 8.436190605163574
Epoch 1300, val loss: 2.1283822059631348
Epoch 1310, training loss: 424.0221252441406 = 2.1270135641098022 + 50.0 * 8.437902450561523
Epoch 1310, val loss: 2.1281447410583496
Epoch 1320, training loss: 423.9166564941406 = 2.1267688274383545 + 50.0 * 8.435797691345215
Epoch 1320, val loss: 2.1278958320617676
Epoch 1330, training loss: 423.8703918457031 = 2.1265244483947754 + 50.0 * 8.434877395629883
Epoch 1330, val loss: 2.127662181854248
Epoch 1340, training loss: 423.8424072265625 = 2.1262818574905396 + 50.0 * 8.434322357177734
Epoch 1340, val loss: 2.127429962158203
Epoch 1350, training loss: 423.8830261230469 = 2.126052141189575 + 50.0 * 8.435139656066895
Epoch 1350, val loss: 2.127202033996582
Epoch 1360, training loss: 423.8490295410156 = 2.1257882118225098 + 50.0 * 8.434464454650879
Epoch 1360, val loss: 2.1269590854644775
Epoch 1370, training loss: 423.80078125 = 2.125582218170166 + 50.0 * 8.433504104614258
Epoch 1370, val loss: 2.1267173290252686
Epoch 1380, training loss: 423.7565002441406 = 2.125316619873047 + 50.0 * 8.432623863220215
Epoch 1380, val loss: 2.1264986991882324
Epoch 1390, training loss: 423.7243347167969 = 2.125103235244751 + 50.0 * 8.431984901428223
Epoch 1390, val loss: 2.1262736320495605
Epoch 1400, training loss: 423.750244140625 = 2.1248691082000732 + 50.0 * 8.432507514953613
Epoch 1400, val loss: 2.126058340072632
Epoch 1410, training loss: 423.6968078613281 = 2.124648332595825 + 50.0 * 8.431443214416504
Epoch 1410, val loss: 2.125812292098999
Epoch 1420, training loss: 423.701416015625 = 2.124385118484497 + 50.0 * 8.431540489196777
Epoch 1420, val loss: 2.1255862712860107
Epoch 1430, training loss: 423.641357421875 = 2.124168634414673 + 50.0 * 8.430343627929688
Epoch 1430, val loss: 2.125369071960449
Epoch 1440, training loss: 423.6089172363281 = 2.123962879180908 + 50.0 * 8.429698944091797
Epoch 1440, val loss: 2.1251566410064697
Epoch 1450, training loss: 423.653076171875 = 2.1237637996673584 + 50.0 * 8.430586814880371
Epoch 1450, val loss: 2.1249423027038574
Epoch 1460, training loss: 423.57110595703125 = 2.123500347137451 + 50.0 * 8.42895221710205
Epoch 1460, val loss: 2.1247200965881348
Epoch 1470, training loss: 423.62139892578125 = 2.12328040599823 + 50.0 * 8.429962158203125
Epoch 1470, val loss: 2.1244845390319824
Epoch 1480, training loss: 423.52020263671875 = 2.123053193092346 + 50.0 * 8.427943229675293
Epoch 1480, val loss: 2.124272584915161
Epoch 1490, training loss: 423.5038757324219 = 2.122838020324707 + 50.0 * 8.427620887756348
Epoch 1490, val loss: 2.1240742206573486
Epoch 1500, training loss: 423.4714660644531 = 2.1226390600204468 + 50.0 * 8.426976203918457
Epoch 1500, val loss: 2.123875617980957
Epoch 1510, training loss: 423.44970703125 = 2.1224331855773926 + 50.0 * 8.426545143127441
Epoch 1510, val loss: 2.1236743927001953
Epoch 1520, training loss: 423.44281005859375 = 2.1222211122512817 + 50.0 * 8.426411628723145
Epoch 1520, val loss: 2.1234750747680664
Epoch 1530, training loss: 423.4916076660156 = 2.1219987869262695 + 50.0 * 8.42739200592041
Epoch 1530, val loss: 2.1232614517211914
Epoch 1540, training loss: 423.4575500488281 = 2.1217589378356934 + 50.0 * 8.426715850830078
Epoch 1540, val loss: 2.1230335235595703
Epoch 1550, training loss: 423.3920593261719 = 2.1215533018112183 + 50.0 * 8.425410270690918
Epoch 1550, val loss: 2.1228251457214355
Epoch 1560, training loss: 423.355712890625 = 2.121351480484009 + 50.0 * 8.424687385559082
Epoch 1560, val loss: 2.1226277351379395
Epoch 1570, training loss: 423.3284912109375 = 2.121152639389038 + 50.0 * 8.42414665222168
Epoch 1570, val loss: 2.122432231903076
Epoch 1580, training loss: 423.30718994140625 = 2.1209510564804077 + 50.0 * 8.423725128173828
Epoch 1580, val loss: 2.12223482131958
Epoch 1590, training loss: 423.288330078125 = 2.1207481622695923 + 50.0 * 8.423351287841797
Epoch 1590, val loss: 2.122037172317505
Epoch 1600, training loss: 423.2778015136719 = 2.1205437183380127 + 50.0 * 8.423145294189453
Epoch 1600, val loss: 2.121839761734009
Epoch 1610, training loss: 423.6379089355469 = 2.1203237771987915 + 50.0 * 8.430351257324219
Epoch 1610, val loss: 2.1216330528259277
Epoch 1620, training loss: 423.2772216796875 = 2.12007737159729 + 50.0 * 8.423142433166504
Epoch 1620, val loss: 2.121399402618408
Epoch 1630, training loss: 423.2584533691406 = 2.1198757886886597 + 50.0 * 8.422771453857422
Epoch 1630, val loss: 2.1211907863616943
Epoch 1640, training loss: 423.2131652832031 = 2.119673013687134 + 50.0 * 8.421870231628418
Epoch 1640, val loss: 2.1209940910339355
Epoch 1650, training loss: 423.1957702636719 = 2.119477868080139 + 50.0 * 8.421525955200195
Epoch 1650, val loss: 2.120797634124756
Epoch 1660, training loss: 423.22491455078125 = 2.1192904710769653 + 50.0 * 8.422112464904785
Epoch 1660, val loss: 2.120598793029785
Epoch 1670, training loss: 423.2182312011719 = 2.119069457054138 + 50.0 * 8.421982765197754
Epoch 1670, val loss: 2.1203956604003906
Epoch 1680, training loss: 423.1616516113281 = 2.1188281774520874 + 50.0 * 8.420856475830078
Epoch 1680, val loss: 2.1201846599578857
Epoch 1690, training loss: 423.13519287109375 = 2.1186389923095703 + 50.0 * 8.420331001281738
Epoch 1690, val loss: 2.1199870109558105
Epoch 1700, training loss: 423.1211242675781 = 2.118425488471985 + 50.0 * 8.420053482055664
Epoch 1700, val loss: 2.119793176651001
Epoch 1710, training loss: 423.10760498046875 = 2.118237257003784 + 50.0 * 8.419787406921387
Epoch 1710, val loss: 2.119598150253296
Epoch 1720, training loss: 423.1174011230469 = 2.118026375770569 + 50.0 * 8.419987678527832
Epoch 1720, val loss: 2.1194050312042236
Epoch 1730, training loss: 423.12139892578125 = 2.1178187131881714 + 50.0 * 8.420071601867676
Epoch 1730, val loss: 2.1191954612731934
Epoch 1740, training loss: 423.06689453125 = 2.117609739303589 + 50.0 * 8.418985366821289
Epoch 1740, val loss: 2.1189959049224854
Epoch 1750, training loss: 423.05517578125 = 2.11740779876709 + 50.0 * 8.418755531311035
Epoch 1750, val loss: 2.1187961101531982
Epoch 1760, training loss: 423.0519714355469 = 2.1171953678131104 + 50.0 * 8.418695449829102
Epoch 1760, val loss: 2.1186094284057617
Epoch 1770, training loss: 423.228759765625 = 2.116975426673889 + 50.0 * 8.422235488891602
Epoch 1770, val loss: 2.118413209915161
Epoch 1780, training loss: 423.0669250488281 = 2.1167980432510376 + 50.0 * 8.419002532958984
Epoch 1780, val loss: 2.118190288543701
Epoch 1790, training loss: 423.0212097167969 = 2.116568088531494 + 50.0 * 8.418092727661133
Epoch 1790, val loss: 2.1179983615875244
Epoch 1800, training loss: 422.99371337890625 = 2.116388201713562 + 50.0 * 8.417546272277832
Epoch 1800, val loss: 2.1178061962127686
Epoch 1810, training loss: 422.990966796875 = 2.116185784339905 + 50.0 * 8.417495727539062
Epoch 1810, val loss: 2.1176223754882812
Epoch 1820, training loss: 423.1427307128906 = 2.1159781217575073 + 50.0 * 8.42053508758545
Epoch 1820, val loss: 2.117424488067627
Epoch 1830, training loss: 422.98419189453125 = 2.115782380104065 + 50.0 * 8.417367935180664
Epoch 1830, val loss: 2.1172194480895996
Epoch 1840, training loss: 422.94000244140625 = 2.115577459335327 + 50.0 * 8.416488647460938
Epoch 1840, val loss: 2.1170289516448975
Epoch 1850, training loss: 422.9240417480469 = 2.1153937578201294 + 50.0 * 8.416172981262207
Epoch 1850, val loss: 2.1168458461761475
Epoch 1860, training loss: 422.91375732421875 = 2.1152095794677734 + 50.0 * 8.415970802307129
Epoch 1860, val loss: 2.1166629791259766
Epoch 1870, training loss: 422.971923828125 = 2.1150330305099487 + 50.0 * 8.41713809967041
Epoch 1870, val loss: 2.116476058959961
Epoch 1880, training loss: 422.91827392578125 = 2.114819645881653 + 50.0 * 8.416069030761719
Epoch 1880, val loss: 2.1162772178649902
Epoch 1890, training loss: 422.87603759765625 = 2.1146045923233032 + 50.0 * 8.415228843688965
Epoch 1890, val loss: 2.1160831451416016
Epoch 1900, training loss: 422.8600769042969 = 2.114416718482971 + 50.0 * 8.414913177490234
Epoch 1900, val loss: 2.115902900695801
Epoch 1910, training loss: 422.8469543457031 = 2.1142356395721436 + 50.0 * 8.414654731750488
Epoch 1910, val loss: 2.115727424621582
Epoch 1920, training loss: 422.8473205566406 = 2.1140540838241577 + 50.0 * 8.414665222167969
Epoch 1920, val loss: 2.115553855895996
Epoch 1930, training loss: 423.012451171875 = 2.1138416528701782 + 50.0 * 8.417972564697266
Epoch 1930, val loss: 2.115373134613037
Epoch 1940, training loss: 422.8752136230469 = 2.1136897802352905 + 50.0 * 8.415230751037598
Epoch 1940, val loss: 2.1151700019836426
Epoch 1950, training loss: 422.8121643066406 = 2.11347234249115 + 50.0 * 8.413973808288574
Epoch 1950, val loss: 2.1149909496307373
Epoch 1960, training loss: 422.79827880859375 = 2.1133021116256714 + 50.0 * 8.41369915008545
Epoch 1960, val loss: 2.114820718765259
Epoch 1970, training loss: 422.9333190917969 = 2.113115906715393 + 50.0 * 8.416403770446777
Epoch 1970, val loss: 2.1146421432495117
Epoch 1980, training loss: 422.80291748046875 = 2.1129342317581177 + 50.0 * 8.413799285888672
Epoch 1980, val loss: 2.114452838897705
Epoch 1990, training loss: 422.76568603515625 = 2.1127458810806274 + 50.0 * 8.41305923461914
Epoch 1990, val loss: 2.114278793334961
Epoch 2000, training loss: 422.7442626953125 = 2.112572193145752 + 50.0 * 8.412633895874023
Epoch 2000, val loss: 2.114109754562378
Epoch 2010, training loss: 422.8349914550781 = 2.1123905181884766 + 50.0 * 8.414451599121094
Epoch 2010, val loss: 2.113945245742798
Epoch 2020, training loss: 422.7201843261719 = 2.112194776535034 + 50.0 * 8.41215991973877
Epoch 2020, val loss: 2.1137452125549316
Epoch 2030, training loss: 422.7115478515625 = 2.1120153665542603 + 50.0 * 8.41199016571045
Epoch 2030, val loss: 2.1135671138763428
Epoch 2040, training loss: 422.6983337402344 = 2.1118472814559937 + 50.0 * 8.41172981262207
Epoch 2040, val loss: 2.113396167755127
Epoch 2050, training loss: 422.6798095703125 = 2.1116747856140137 + 50.0 * 8.411362648010254
Epoch 2050, val loss: 2.113236427307129
Epoch 2060, training loss: 422.6656799316406 = 2.1115095615386963 + 50.0 * 8.411083221435547
Epoch 2060, val loss: 2.113074779510498
Epoch 2070, training loss: 422.65850830078125 = 2.111345887184143 + 50.0 * 8.410943031311035
Epoch 2070, val loss: 2.1129112243652344
Epoch 2080, training loss: 422.9147644042969 = 2.111205816268921 + 50.0 * 8.416070938110352
Epoch 2080, val loss: 2.1127305030822754
Epoch 2090, training loss: 422.68609619140625 = 2.1109583377838135 + 50.0 * 8.411502838134766
Epoch 2090, val loss: 2.112553596496582
Epoch 2100, training loss: 422.6470031738281 = 2.110783815383911 + 50.0 * 8.410724639892578
Epoch 2100, val loss: 2.1123757362365723
Epoch 2110, training loss: 422.6197204589844 = 2.1106311082839966 + 50.0 * 8.410181999206543
Epoch 2110, val loss: 2.1122119426727295
Epoch 2120, training loss: 422.6153259277344 = 2.110469102859497 + 50.0 * 8.410097122192383
Epoch 2120, val loss: 2.1120543479919434
Epoch 2130, training loss: 422.7302551269531 = 2.110297441482544 + 50.0 * 8.412399291992188
Epoch 2130, val loss: 2.111891269683838
Epoch 2140, training loss: 422.6116638183594 = 2.1100951433181763 + 50.0 * 8.41003131866455
Epoch 2140, val loss: 2.1117172241210938
Epoch 2150, training loss: 422.57757568359375 = 2.1099354028701782 + 50.0 * 8.409353256225586
Epoch 2150, val loss: 2.111546516418457
Epoch 2160, training loss: 422.5703430175781 = 2.1097660064697266 + 50.0 * 8.409211158752441
Epoch 2160, val loss: 2.1113901138305664
Epoch 2170, training loss: 422.6435852050781 = 2.1095868349075317 + 50.0 * 8.410679817199707
Epoch 2170, val loss: 2.1112332344055176
Epoch 2180, training loss: 422.55767822265625 = 2.109427332878113 + 50.0 * 8.408965110778809
Epoch 2180, val loss: 2.1110525131225586
Epoch 2190, training loss: 422.5494384765625 = 2.109269618988037 + 50.0 * 8.40880298614502
Epoch 2190, val loss: 2.1108903884887695
Epoch 2200, training loss: 422.530029296875 = 2.1090948581695557 + 50.0 * 8.408418655395508
Epoch 2200, val loss: 2.1107358932495117
Epoch 2210, training loss: 422.51300048828125 = 2.108942985534668 + 50.0 * 8.4080810546875
Epoch 2210, val loss: 2.1105809211730957
Epoch 2220, training loss: 422.5256652832031 = 2.1087883710861206 + 50.0 * 8.408337593078613
Epoch 2220, val loss: 2.110426187515259
Epoch 2230, training loss: 422.63287353515625 = 2.1086193323135376 + 50.0 * 8.41048526763916
Epoch 2230, val loss: 2.110261917114258
Epoch 2240, training loss: 422.5690002441406 = 2.1084171533584595 + 50.0 * 8.409211158752441
Epoch 2240, val loss: 2.1100945472717285
Epoch 2250, training loss: 422.6365966796875 = 2.1082483530044556 + 50.0 * 8.410567283630371
Epoch 2250, val loss: 2.1099209785461426
Epoch 2260, training loss: 422.5119934082031 = 2.1080926656723022 + 50.0 * 8.40807819366455
Epoch 2260, val loss: 2.109745502471924
Epoch 2270, training loss: 422.4658203125 = 2.107919931411743 + 50.0 * 8.407157897949219
Epoch 2270, val loss: 2.1095941066741943
Epoch 2280, training loss: 422.45867919921875 = 2.107764720916748 + 50.0 * 8.407018661499023
Epoch 2280, val loss: 2.1094448566436768
Epoch 2290, training loss: 422.4489440917969 = 2.107615113258362 + 50.0 * 8.406826972961426
Epoch 2290, val loss: 2.10929536819458
Epoch 2300, training loss: 422.4690246582031 = 2.1074520349502563 + 50.0 * 8.407231330871582
Epoch 2300, val loss: 2.1091461181640625
Epoch 2310, training loss: 422.5073547363281 = 2.107273817062378 + 50.0 * 8.408001899719238
Epoch 2310, val loss: 2.1089835166931152
Epoch 2320, training loss: 422.5113220214844 = 2.1071114540100098 + 50.0 * 8.40808391571045
Epoch 2320, val loss: 2.1088171005249023
Epoch 2330, training loss: 422.42547607421875 = 2.106947422027588 + 50.0 * 8.406371116638184
Epoch 2330, val loss: 2.1086502075195312
Epoch 2340, training loss: 422.4237060546875 = 2.106808066368103 + 50.0 * 8.40633773803711
Epoch 2340, val loss: 2.108499050140381
Epoch 2350, training loss: 422.4147644042969 = 2.1066466569900513 + 50.0 * 8.40616226196289
Epoch 2350, val loss: 2.108351707458496
Epoch 2360, training loss: 422.4339294433594 = 2.106498956680298 + 50.0 * 8.406548500061035
Epoch 2360, val loss: 2.1081995964050293
Epoch 2370, training loss: 422.4694519042969 = 2.106333374977112 + 50.0 * 8.407262802124023
Epoch 2370, val loss: 2.108041286468506
Epoch 2380, training loss: 422.397705078125 = 2.1061524152755737 + 50.0 * 8.405831336975098
Epoch 2380, val loss: 2.107884407043457
Epoch 2390, training loss: 422.4798889160156 = 2.105980157852173 + 50.0 * 8.407478332519531
Epoch 2390, val loss: 2.107731819152832
Epoch 2400, training loss: 422.3905944824219 = 2.1058255434036255 + 50.0 * 8.405694961547852
Epoch 2400, val loss: 2.1075658798217773
Epoch 2410, training loss: 422.37103271484375 = 2.10568368434906 + 50.0 * 8.405306816101074
Epoch 2410, val loss: 2.1074135303497314
Epoch 2420, training loss: 422.3562927246094 = 2.1055212020874023 + 50.0 * 8.405014991760254
Epoch 2420, val loss: 2.107273578643799
Epoch 2430, training loss: 422.343017578125 = 2.1053799390792847 + 50.0 * 8.404752731323242
Epoch 2430, val loss: 2.1071319580078125
Epoch 2440, training loss: 422.33966064453125 = 2.1052379608154297 + 50.0 * 8.404688835144043
Epoch 2440, val loss: 2.106989860534668
Epoch 2450, training loss: 422.4927062988281 = 2.1050816774368286 + 50.0 * 8.407752990722656
Epoch 2450, val loss: 2.106846332550049
Epoch 2460, training loss: 422.4742736816406 = 2.104884147644043 + 50.0 * 8.407387733459473
Epoch 2460, val loss: 2.1066696643829346
Epoch 2470, training loss: 422.36383056640625 = 2.104735255241394 + 50.0 * 8.405181884765625
Epoch 2470, val loss: 2.106498956680298
Epoch 2480, training loss: 422.3095703125 = 2.1045817136764526 + 50.0 * 8.404099464416504
Epoch 2480, val loss: 2.106351852416992
Epoch 2490, training loss: 422.31048583984375 = 2.1044387817382812 + 50.0 * 8.404121398925781
Epoch 2490, val loss: 2.1062207221984863
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.3989855072463768
0.8153300007244803
=== training gcn model ===
Epoch 0, training loss: 531.3226928710938 = 2.2091487646102905 + 50.0 * 10.582270622253418
Epoch 0, val loss: 2.2075023651123047
Epoch 10, training loss: 531.298095703125 = 2.197222948074341 + 50.0 * 10.58201789855957
Epoch 10, val loss: 2.19722580909729
Epoch 20, training loss: 531.2556762695312 = 2.197222948074341 + 50.0 * 10.581169128417969
Epoch 20, val loss: 2.19722580909729
Epoch 30, training loss: 531.07666015625 = 2.197222948074341 + 50.0 * 10.57758903503418
Epoch 30, val loss: 2.19722580909729
Epoch 40, training loss: 530.327392578125 = 2.197222948074341 + 50.0 * 10.562603950500488
Epoch 40, val loss: 2.19722580909729
Epoch 50, training loss: 527.832275390625 = 2.197265863418579 + 50.0 * 10.512700080871582
Epoch 50, val loss: 2.197483539581299
Epoch 60, training loss: 521.4721069335938 = 2.201218008995056 + 50.0 * 10.385417938232422
Epoch 60, val loss: 2.201620101928711
Epoch 70, training loss: 507.1903381347656 = 2.206052541732788 + 50.0 * 10.099685668945312
Epoch 70, val loss: 2.2064168453216553
Epoch 80, training loss: 490.1842346191406 = 2.20884370803833 + 50.0 * 9.75950813293457
Epoch 80, val loss: 2.209017038345337
Epoch 90, training loss: 483.0121765136719 = 2.208060145378113 + 50.0 * 9.616082191467285
Epoch 90, val loss: 2.2082326412200928
Epoch 100, training loss: 477.5585021972656 = 2.203930974006653 + 50.0 * 9.507091522216797
Epoch 100, val loss: 2.2040321826934814
Epoch 110, training loss: 471.96539306640625 = 2.200320601463318 + 50.0 * 9.395301818847656
Epoch 110, val loss: 2.2005510330200195
Epoch 120, training loss: 466.2083740234375 = 2.1966001987457275 + 50.0 * 9.280235290527344
Epoch 120, val loss: 2.1968393325805664
Epoch 130, training loss: 461.6939697265625 = 2.19378924369812 + 50.0 * 9.190003395080566
Epoch 130, val loss: 2.1942055225372314
Epoch 140, training loss: 458.0497741699219 = 2.192586064338684 + 50.0 * 9.117143630981445
Epoch 140, val loss: 2.1930999755859375
Epoch 150, training loss: 454.9088439941406 = 2.192119836807251 + 50.0 * 9.05433464050293
Epoch 150, val loss: 2.1926398277282715
Epoch 160, training loss: 453.2193603515625 = 2.1915310621261597 + 50.0 * 9.020556449890137
Epoch 160, val loss: 2.1920981407165527
Epoch 170, training loss: 451.4606018066406 = 2.190590977668762 + 50.0 * 8.985400199890137
Epoch 170, val loss: 2.1913022994995117
Epoch 180, training loss: 448.75439453125 = 2.1900603771209717 + 50.0 * 8.931286811828613
Epoch 180, val loss: 2.1908721923828125
Epoch 190, training loss: 445.6944885253906 = 2.1899174451828003 + 50.0 * 8.870091438293457
Epoch 190, val loss: 2.1908178329467773
Epoch 200, training loss: 443.53253173828125 = 2.1899287700653076 + 50.0 * 8.826851844787598
Epoch 200, val loss: 2.1908979415893555
Epoch 210, training loss: 442.490234375 = 2.189542293548584 + 50.0 * 8.806014060974121
Epoch 210, val loss: 2.1904237270355225
Epoch 220, training loss: 441.772705078125 = 2.188736081123352 + 50.0 * 8.791679382324219
Epoch 220, val loss: 2.1896603107452393
Epoch 230, training loss: 440.9644470214844 = 2.18781054019928 + 50.0 * 8.775532722473145
Epoch 230, val loss: 2.1888108253479004
Epoch 240, training loss: 440.0412292480469 = 2.1870487928390503 + 50.0 * 8.757083892822266
Epoch 240, val loss: 2.188103675842285
Epoch 250, training loss: 438.9621276855469 = 2.1864970922470093 + 50.0 * 8.735512733459473
Epoch 250, val loss: 2.1876070499420166
Epoch 260, training loss: 437.8959045410156 = 2.186000347137451 + 50.0 * 8.714198112487793
Epoch 260, val loss: 2.1871261596679688
Epoch 270, training loss: 436.9967346191406 = 2.1855050325393677 + 50.0 * 8.696224212646484
Epoch 270, val loss: 2.1866283416748047
Epoch 280, training loss: 436.240234375 = 2.18499755859375 + 50.0 * 8.68110466003418
Epoch 280, val loss: 2.1861300468444824
Epoch 290, training loss: 435.48394775390625 = 2.1844310760498047 + 50.0 * 8.665990829467773
Epoch 290, val loss: 2.185631275177002
Epoch 300, training loss: 434.8753356933594 = 2.183980941772461 + 50.0 * 8.653826713562012
Epoch 300, val loss: 2.1852049827575684
Epoch 310, training loss: 434.3044128417969 = 2.1834754943847656 + 50.0 * 8.64241886138916
Epoch 310, val loss: 2.184830665588379
Epoch 320, training loss: 433.8901672363281 = 2.18317973613739 + 50.0 * 8.634140014648438
Epoch 320, val loss: 2.184473991394043
Epoch 330, training loss: 433.40185546875 = 2.182852029800415 + 50.0 * 8.624380111694336
Epoch 330, val loss: 2.184175491333008
Epoch 340, training loss: 432.9827575683594 = 2.1825551986694336 + 50.0 * 8.61600399017334
Epoch 340, val loss: 2.1839375495910645
Epoch 350, training loss: 432.5621337890625 = 2.1823254823684692 + 50.0 * 8.607596397399902
Epoch 350, val loss: 2.1837403774261475
Epoch 360, training loss: 432.1472473144531 = 2.182138681411743 + 50.0 * 8.599302291870117
Epoch 360, val loss: 2.183588743209839
Epoch 370, training loss: 431.8002014160156 = 2.1819846630096436 + 50.0 * 8.592364311218262
Epoch 370, val loss: 2.1834425926208496
Epoch 380, training loss: 431.4098815917969 = 2.1817619800567627 + 50.0 * 8.584562301635742
Epoch 380, val loss: 2.1832985877990723
Epoch 390, training loss: 431.11407470703125 = 2.1815634965896606 + 50.0 * 8.57865047454834
Epoch 390, val loss: 2.183138847351074
Epoch 400, training loss: 430.8711853027344 = 2.1813626289367676 + 50.0 * 8.573796272277832
Epoch 400, val loss: 2.1829633712768555
Epoch 410, training loss: 430.6726989746094 = 2.1811347007751465 + 50.0 * 8.569831848144531
Epoch 410, val loss: 2.182774305343628
Epoch 420, training loss: 430.6551208496094 = 2.180868983268738 + 50.0 * 8.56948471069336
Epoch 420, val loss: 2.1825735569000244
Epoch 430, training loss: 430.3599548339844 = 2.1806360483169556 + 50.0 * 8.563586235046387
Epoch 430, val loss: 2.1823630332946777
Epoch 440, training loss: 430.19464111328125 = 2.1804099082946777 + 50.0 * 8.560284614562988
Epoch 440, val loss: 2.182156562805176
Epoch 450, training loss: 430.0362548828125 = 2.180176258087158 + 50.0 * 8.557121276855469
Epoch 450, val loss: 2.181962251663208
Epoch 460, training loss: 429.8970031738281 = 2.1799381971359253 + 50.0 * 8.554341316223145
Epoch 460, val loss: 2.1817798614501953
Epoch 470, training loss: 429.8612365722656 = 2.1797410249710083 + 50.0 * 8.553629875183105
Epoch 470, val loss: 2.1815757751464844
Epoch 480, training loss: 429.60931396484375 = 2.179494619369507 + 50.0 * 8.548596382141113
Epoch 480, val loss: 2.1814000606536865
Epoch 490, training loss: 429.479248046875 = 2.1792696714401245 + 50.0 * 8.545999526977539
Epoch 490, val loss: 2.181220293045044
Epoch 500, training loss: 429.34100341796875 = 2.179064393043518 + 50.0 * 8.543238639831543
Epoch 500, val loss: 2.181044578552246
Epoch 510, training loss: 429.2355651855469 = 2.17886745929718 + 50.0 * 8.541133880615234
Epoch 510, val loss: 2.180870532989502
Epoch 520, training loss: 429.1881408691406 = 2.17863392829895 + 50.0 * 8.540190696716309
Epoch 520, val loss: 2.1806981563568115
Epoch 530, training loss: 428.9989013671875 = 2.178440570831299 + 50.0 * 8.536409378051758
Epoch 530, val loss: 2.1805222034454346
Epoch 540, training loss: 428.85894775390625 = 2.1782532930374146 + 50.0 * 8.533614158630371
Epoch 540, val loss: 2.1803627014160156
Epoch 550, training loss: 428.7242736816406 = 2.178067207336426 + 50.0 * 8.530923843383789
Epoch 550, val loss: 2.180211067199707
Epoch 560, training loss: 428.6447448730469 = 2.177889585494995 + 50.0 * 8.529336929321289
Epoch 560, val loss: 2.1800665855407715
Epoch 570, training loss: 428.55987548828125 = 2.1777069568634033 + 50.0 * 8.527643203735352
Epoch 570, val loss: 2.179903030395508
Epoch 580, training loss: 428.32269287109375 = 2.177531123161316 + 50.0 * 8.522903442382812
Epoch 580, val loss: 2.1797595024108887
Epoch 590, training loss: 428.18353271484375 = 2.177369236946106 + 50.0 * 8.520123481750488
Epoch 590, val loss: 2.1796295642852783
Epoch 600, training loss: 428.1512451171875 = 2.1772258281707764 + 50.0 * 8.51948070526123
Epoch 600, val loss: 2.1795005798339844
Epoch 610, training loss: 427.9776306152344 = 2.177040457725525 + 50.0 * 8.516012191772461
Epoch 610, val loss: 2.1793689727783203
Epoch 620, training loss: 427.8143615722656 = 2.176865816116333 + 50.0 * 8.512749671936035
Epoch 620, val loss: 2.179248094558716
Epoch 630, training loss: 427.66015625 = 2.1767220497131348 + 50.0 * 8.509668350219727
Epoch 630, val loss: 2.179123878479004
Epoch 640, training loss: 427.5544738769531 = 2.176582455635071 + 50.0 * 8.50755786895752
Epoch 640, val loss: 2.179003953933716
Epoch 650, training loss: 427.59521484375 = 2.176413059234619 + 50.0 * 8.508376121520996
Epoch 650, val loss: 2.178893566131592
Epoch 660, training loss: 427.40185546875 = 2.176276922225952 + 50.0 * 8.504511833190918
Epoch 660, val loss: 2.1787421703338623
Epoch 670, training loss: 427.2480773925781 = 2.176112413406372 + 50.0 * 8.501439094543457
Epoch 670, val loss: 2.178612232208252
Epoch 680, training loss: 427.13885498046875 = 2.175963044166565 + 50.0 * 8.499258041381836
Epoch 680, val loss: 2.178496837615967
Epoch 690, training loss: 427.0357971191406 = 2.175820231437683 + 50.0 * 8.497200012207031
Epoch 690, val loss: 2.1783790588378906
Epoch 700, training loss: 426.9386901855469 = 2.1756755113601685 + 50.0 * 8.495260238647461
Epoch 700, val loss: 2.178262710571289
Epoch 710, training loss: 426.8929748535156 = 2.1755329370498657 + 50.0 * 8.494348526000977
Epoch 710, val loss: 2.1781458854675293
Epoch 720, training loss: 426.87451171875 = 2.175379991531372 + 50.0 * 8.493982315063477
Epoch 720, val loss: 2.1780223846435547
Epoch 730, training loss: 426.7151184082031 = 2.175227165222168 + 50.0 * 8.490797996520996
Epoch 730, val loss: 2.177901268005371
Epoch 740, training loss: 426.6222229003906 = 2.175105929374695 + 50.0 * 8.48894214630127
Epoch 740, val loss: 2.1777825355529785
Epoch 750, training loss: 426.5199890136719 = 2.1749508380889893 + 50.0 * 8.486900329589844
Epoch 750, val loss: 2.177673816680908
Epoch 760, training loss: 426.45721435546875 = 2.1748125553131104 + 50.0 * 8.485648155212402
Epoch 760, val loss: 2.177550792694092
Epoch 770, training loss: 426.3388366699219 = 2.1746582984924316 + 50.0 * 8.483283996582031
Epoch 770, val loss: 2.177440643310547
Epoch 780, training loss: 426.3533935546875 = 2.174509048461914 + 50.0 * 8.483577728271484
Epoch 780, val loss: 2.1773324012756348
Epoch 790, training loss: 426.2343444824219 = 2.174386739730835 + 50.0 * 8.481199264526367
Epoch 790, val loss: 2.177197217941284
Epoch 800, training loss: 426.1356201171875 = 2.1742230653762817 + 50.0 * 8.479228019714355
Epoch 800, val loss: 2.177083969116211
Epoch 810, training loss: 426.0829162597656 = 2.17409348487854 + 50.0 * 8.47817611694336
Epoch 810, val loss: 2.176962375640869
Epoch 820, training loss: 425.97100830078125 = 2.173932194709778 + 50.0 * 8.47594165802002
Epoch 820, val loss: 2.1768462657928467
Epoch 830, training loss: 425.9073791503906 = 2.1737887859344482 + 50.0 * 8.474671363830566
Epoch 830, val loss: 2.1767258644104004
Epoch 840, training loss: 425.91217041015625 = 2.173653483390808 + 50.0 * 8.474770545959473
Epoch 840, val loss: 2.1766037940979004
Epoch 850, training loss: 425.8310852050781 = 2.1735178232192993 + 50.0 * 8.473151206970215
Epoch 850, val loss: 2.1764822006225586
Epoch 860, training loss: 425.7345275878906 = 2.1733380556106567 + 50.0 * 8.471223831176758
Epoch 860, val loss: 2.176363229751587
Epoch 870, training loss: 425.7894592285156 = 2.173194646835327 + 50.0 * 8.472325325012207
Epoch 870, val loss: 2.1762523651123047
Epoch 880, training loss: 425.63134765625 = 2.1730657815933228 + 50.0 * 8.469165802001953
Epoch 880, val loss: 2.1761159896850586
Epoch 890, training loss: 425.5536804199219 = 2.172904849052429 + 50.0 * 8.467615127563477
Epoch 890, val loss: 2.1760082244873047
Epoch 900, training loss: 425.4755859375 = 2.1727795600891113 + 50.0 * 8.466055870056152
Epoch 900, val loss: 2.175893783569336
Epoch 910, training loss: 425.4359436035156 = 2.1726458072662354 + 50.0 * 8.465266227722168
Epoch 910, val loss: 2.175781011581421
Epoch 920, training loss: 425.4048156738281 = 2.1724843978881836 + 50.0 * 8.464646339416504
Epoch 920, val loss: 2.175675392150879
Epoch 930, training loss: 425.30548095703125 = 2.172344923019409 + 50.0 * 8.462662696838379
Epoch 930, val loss: 2.1755504608154297
Epoch 940, training loss: 425.2539367675781 = 2.172205686569214 + 50.0 * 8.461634635925293
Epoch 940, val loss: 2.1754391193389893
Epoch 950, training loss: 425.2023620605469 = 2.1720763444900513 + 50.0 * 8.46060562133789
Epoch 950, val loss: 2.1753320693969727
Epoch 960, training loss: 425.1809997558594 = 2.1719428300857544 + 50.0 * 8.46018123626709
Epoch 960, val loss: 2.175227403640747
Epoch 970, training loss: 425.10943603515625 = 2.1718097925186157 + 50.0 * 8.458752632141113
Epoch 970, val loss: 2.1751084327697754
Epoch 980, training loss: 425.1149597167969 = 2.171673536300659 + 50.0 * 8.458866119384766
Epoch 980, val loss: 2.1749961376190186
Epoch 990, training loss: 425.30706787109375 = 2.1715469360351562 + 50.0 * 8.4627103805542
Epoch 990, val loss: 2.174882411956787
Epoch 1000, training loss: 424.980712890625 = 2.1713863611221313 + 50.0 * 8.456186294555664
Epoch 1000, val loss: 2.174773693084717
Epoch 1010, training loss: 424.940673828125 = 2.1712616682052612 + 50.0 * 8.455388069152832
Epoch 1010, val loss: 2.174670696258545
Epoch 1020, training loss: 424.87469482421875 = 2.171133875846863 + 50.0 * 8.454071044921875
Epoch 1020, val loss: 2.1745688915252686
Epoch 1030, training loss: 424.823974609375 = 2.171011209487915 + 50.0 * 8.453059196472168
Epoch 1030, val loss: 2.174469470977783
Epoch 1040, training loss: 424.7766418457031 = 2.170884370803833 + 50.0 * 8.452115058898926
Epoch 1040, val loss: 2.174372673034668
Epoch 1050, training loss: 424.7459716796875 = 2.170755386352539 + 50.0 * 8.451504707336426
Epoch 1050, val loss: 2.1742777824401855
Epoch 1060, training loss: 424.7459411621094 = 2.1706273555755615 + 50.0 * 8.451506614685059
Epoch 1060, val loss: 2.174163818359375
Epoch 1070, training loss: 424.697021484375 = 2.1704846620559692 + 50.0 * 8.450531005859375
Epoch 1070, val loss: 2.174056053161621
Epoch 1080, training loss: 424.61956787109375 = 2.170363664627075 + 50.0 * 8.448984146118164
Epoch 1080, val loss: 2.173959970474243
Epoch 1090, training loss: 424.5699768066406 = 2.1702462434768677 + 50.0 * 8.447994232177734
Epoch 1090, val loss: 2.173870086669922
Epoch 1100, training loss: 424.5547790527344 = 2.170128583908081 + 50.0 * 8.44769287109375
Epoch 1100, val loss: 2.173783302307129
Epoch 1110, training loss: 424.56085205078125 = 2.1700013875961304 + 50.0 * 8.447816848754883
Epoch 1110, val loss: 2.1736788749694824
Epoch 1120, training loss: 424.4728088378906 = 2.169879198074341 + 50.0 * 8.44605827331543
Epoch 1120, val loss: 2.1735825538635254
Epoch 1130, training loss: 424.4327697753906 = 2.169774889945984 + 50.0 * 8.445260047912598
Epoch 1130, val loss: 2.1734871864318848
Epoch 1140, training loss: 424.37506103515625 = 2.169657826423645 + 50.0 * 8.444108009338379
Epoch 1140, val loss: 2.173400402069092
Epoch 1150, training loss: 424.34228515625 = 2.169544816017151 + 50.0 * 8.44345474243164
Epoch 1150, val loss: 2.173316478729248
Epoch 1160, training loss: 424.4091796875 = 2.1694228649139404 + 50.0 * 8.444794654846191
Epoch 1160, val loss: 2.1732311248779297
Epoch 1170, training loss: 424.38177490234375 = 2.16931688785553 + 50.0 * 8.444249153137207
Epoch 1170, val loss: 2.1731321811676025
Epoch 1180, training loss: 424.2478332519531 = 2.169196128845215 + 50.0 * 8.441573143005371
Epoch 1180, val loss: 2.1730403900146484
Epoch 1190, training loss: 424.22906494140625 = 2.169083595275879 + 50.0 * 8.44119930267334
Epoch 1190, val loss: 2.1729555130004883
Epoch 1200, training loss: 424.2465515136719 = 2.1689711809158325 + 50.0 * 8.441551208496094
Epoch 1200, val loss: 2.17287540435791
Epoch 1210, training loss: 424.1992492675781 = 2.168861150741577 + 50.0 * 8.440608024597168
Epoch 1210, val loss: 2.1727776527404785
Epoch 1220, training loss: 424.1827697753906 = 2.1687408685684204 + 50.0 * 8.44028091430664
Epoch 1220, val loss: 2.1726903915405273
Epoch 1230, training loss: 424.1090087890625 = 2.1686359643936157 + 50.0 * 8.438807487487793
Epoch 1230, val loss: 2.1726088523864746
Epoch 1240, training loss: 424.0922546386719 = 2.1685298681259155 + 50.0 * 8.438474655151367
Epoch 1240, val loss: 2.172529697418213
Epoch 1250, training loss: 424.2542419433594 = 2.168408155441284 + 50.0 * 8.441717147827148
Epoch 1250, val loss: 2.1724536418914795
Epoch 1260, training loss: 424.0985412597656 = 2.168324112892151 + 50.0 * 8.438604354858398
Epoch 1260, val loss: 2.1723551750183105
Epoch 1270, training loss: 424.021240234375 = 2.1682040691375732 + 50.0 * 8.437060356140137
Epoch 1270, val loss: 2.1722822189331055
Epoch 1280, training loss: 423.97625732421875 = 2.1681134700775146 + 50.0 * 8.436162948608398
Epoch 1280, val loss: 2.172203540802002
Epoch 1290, training loss: 423.9610900878906 = 2.168005585670471 + 50.0 * 8.435861587524414
Epoch 1290, val loss: 2.172135591506958
Epoch 1300, training loss: 424.0191955566406 = 2.167899966239929 + 50.0 * 8.437026023864746
Epoch 1300, val loss: 2.172050714492798
Epoch 1310, training loss: 423.9102783203125 = 2.1677950620651245 + 50.0 * 8.434849739074707
Epoch 1310, val loss: 2.171971559524536
Epoch 1320, training loss: 423.8769836425781 = 2.1677087545394897 + 50.0 * 8.434185981750488
Epoch 1320, val loss: 2.1718931198120117
Epoch 1330, training loss: 423.8308410644531 = 2.167608857154846 + 50.0 * 8.43326473236084
Epoch 1330, val loss: 2.1718244552612305
Epoch 1340, training loss: 423.8277282714844 = 2.167514681816101 + 50.0 * 8.433204650878906
Epoch 1340, val loss: 2.171757221221924
Epoch 1350, training loss: 423.86676025390625 = 2.1674129962921143 + 50.0 * 8.43398666381836
Epoch 1350, val loss: 2.1716833114624023
Epoch 1360, training loss: 423.79742431640625 = 2.1673166751861572 + 50.0 * 8.432601928710938
Epoch 1360, val loss: 2.171603202819824
Epoch 1370, training loss: 423.7327575683594 = 2.167224168777466 + 50.0 * 8.431310653686523
Epoch 1370, val loss: 2.1715359687805176
Epoch 1380, training loss: 423.7106628417969 = 2.167134165763855 + 50.0 * 8.430870056152344
Epoch 1380, val loss: 2.1714727878570557
Epoch 1390, training loss: 423.70660400390625 = 2.1670531034469604 + 50.0 * 8.430790901184082
Epoch 1390, val loss: 2.1714072227478027
Epoch 1400, training loss: 423.78289794921875 = 2.166960835456848 + 50.0 * 8.432318687438965
Epoch 1400, val loss: 2.171332359313965
Epoch 1410, training loss: 423.71142578125 = 2.1668522357940674 + 50.0 * 8.430891990661621
Epoch 1410, val loss: 2.171252965927124
Epoch 1420, training loss: 423.6081237792969 = 2.166761040687561 + 50.0 * 8.428827285766602
Epoch 1420, val loss: 2.1711888313293457
Epoch 1430, training loss: 423.59771728515625 = 2.166679620742798 + 50.0 * 8.428620338439941
Epoch 1430, val loss: 2.1711316108703613
Epoch 1440, training loss: 423.5598449707031 = 2.1665995121002197 + 50.0 * 8.427865028381348
Epoch 1440, val loss: 2.171074390411377
Epoch 1450, training loss: 423.54571533203125 = 2.1665234565734863 + 50.0 * 8.427583694458008
Epoch 1450, val loss: 2.1710143089294434
Epoch 1460, training loss: 423.8101806640625 = 2.166457772254944 + 50.0 * 8.43287467956543
Epoch 1460, val loss: 2.170947790145874
Epoch 1470, training loss: 423.5215759277344 = 2.166329860687256 + 50.0 * 8.427104949951172
Epoch 1470, val loss: 2.170881748199463
Epoch 1480, training loss: 423.50604248046875 = 2.1662509441375732 + 50.0 * 8.426795959472656
Epoch 1480, val loss: 2.170820713043213
Epoch 1490, training loss: 423.45440673828125 = 2.1661739349365234 + 50.0 * 8.425765037536621
Epoch 1490, val loss: 2.170766830444336
Epoch 1500, training loss: 423.4833068847656 = 2.166104316711426 + 50.0 * 8.42634391784668
Epoch 1500, val loss: 2.170710802078247
Epoch 1510, training loss: 423.4263610839844 = 2.16601300239563 + 50.0 * 8.425207138061523
Epoch 1510, val loss: 2.170649290084839
Epoch 1520, training loss: 423.4164733886719 = 2.165933132171631 + 50.0 * 8.425010681152344
Epoch 1520, val loss: 2.170588970184326
Epoch 1530, training loss: 423.3727722167969 = 2.165860652923584 + 50.0 * 8.424138069152832
Epoch 1530, val loss: 2.1705331802368164
Epoch 1540, training loss: 423.3497009277344 = 2.1657849550247192 + 50.0 * 8.423678398132324
Epoch 1540, val loss: 2.170483350753784
Epoch 1550, training loss: 423.3633117675781 = 2.1657174825668335 + 50.0 * 8.423952102661133
Epoch 1550, val loss: 2.1704325675964355
Epoch 1560, training loss: 423.3677978515625 = 2.165632128715515 + 50.0 * 8.424043655395508
Epoch 1560, val loss: 2.1703734397888184
Epoch 1570, training loss: 423.34112548828125 = 2.1655502319335938 + 50.0 * 8.423511505126953
Epoch 1570, val loss: 2.17031192779541
Epoch 1580, training loss: 423.309814453125 = 2.16546630859375 + 50.0 * 8.422886848449707
Epoch 1580, val loss: 2.1702606678009033
Epoch 1590, training loss: 423.35687255859375 = 2.1653906106948853 + 50.0 * 8.423829078674316
Epoch 1590, val loss: 2.1702094078063965
Epoch 1600, training loss: 423.24560546875 = 2.1653308868408203 + 50.0 * 8.421605110168457
Epoch 1600, val loss: 2.170151472091675
Epoch 1610, training loss: 423.23248291015625 = 2.1652597188949585 + 50.0 * 8.421344757080078
Epoch 1610, val loss: 2.170105457305908
Epoch 1620, training loss: 423.2033386230469 = 2.165190577507019 + 50.0 * 8.42076301574707
Epoch 1620, val loss: 2.170060157775879
Epoch 1630, training loss: 423.2133483886719 = 2.165127992630005 + 50.0 * 8.420964241027832
Epoch 1630, val loss: 2.170013427734375
Epoch 1640, training loss: 423.2502746582031 = 2.165061116218567 + 50.0 * 8.421704292297363
Epoch 1640, val loss: 2.1699607372283936
Epoch 1650, training loss: 423.19403076171875 = 2.164980888366699 + 50.0 * 8.420580863952637
Epoch 1650, val loss: 2.169909954071045
Epoch 1660, training loss: 423.29833984375 = 2.1649231910705566 + 50.0 * 8.42266845703125
Epoch 1660, val loss: 2.1698570251464844
Epoch 1670, training loss: 423.1476745605469 = 2.1648281812667847 + 50.0 * 8.419656753540039
Epoch 1670, val loss: 2.1698122024536133
Epoch 1680, training loss: 423.0976257324219 = 2.164778232574463 + 50.0 * 8.418657302856445
Epoch 1680, val loss: 2.169761896133423
Epoch 1690, training loss: 423.06689453125 = 2.164711833000183 + 50.0 * 8.418044090270996
Epoch 1690, val loss: 2.1697239875793457
Epoch 1700, training loss: 423.05419921875 = 2.1646519899368286 + 50.0 * 8.417791366577148
Epoch 1700, val loss: 2.169684886932373
Epoch 1710, training loss: 423.0494689941406 = 2.164592742919922 + 50.0 * 8.41769790649414
Epoch 1710, val loss: 2.1696438789367676
Epoch 1720, training loss: 423.2835998535156 = 2.164534568786621 + 50.0 * 8.422381401062012
Epoch 1720, val loss: 2.1695938110351562
Epoch 1730, training loss: 423.02545166015625 = 2.1644517183303833 + 50.0 * 8.417220115661621
Epoch 1730, val loss: 2.1695427894592285
Epoch 1740, training loss: 422.98974609375 = 2.1643869876861572 + 50.0 * 8.41650676727295
Epoch 1740, val loss: 2.169501304626465
Epoch 1750, training loss: 422.9740295410156 = 2.1643364429473877 + 50.0 * 8.416193962097168
Epoch 1750, val loss: 2.169462203979492
Epoch 1760, training loss: 422.9822692871094 = 2.164274573326111 + 50.0 * 8.416359901428223
Epoch 1760, val loss: 2.1694278717041016
Epoch 1770, training loss: 422.99688720703125 = 2.1642067432403564 + 50.0 * 8.416653633117676
Epoch 1770, val loss: 2.1693806648254395
Epoch 1780, training loss: 422.9664611816406 = 2.1641407012939453 + 50.0 * 8.416046142578125
Epoch 1780, val loss: 2.1693310737609863
Epoch 1790, training loss: 422.9133605957031 = 2.164086937904358 + 50.0 * 8.414985656738281
Epoch 1790, val loss: 2.16929030418396
Epoch 1800, training loss: 422.9586181640625 = 2.1640241146087646 + 50.0 * 8.415891647338867
Epoch 1800, val loss: 2.169257879257202
Epoch 1810, training loss: 422.90948486328125 = 2.163961887359619 + 50.0 * 8.414910316467285
Epoch 1810, val loss: 2.1692140102386475
Epoch 1820, training loss: 422.8854064941406 = 2.1639145612716675 + 50.0 * 8.414429664611816
Epoch 1820, val loss: 2.1691696643829346
Epoch 1830, training loss: 422.8528747558594 = 2.1638556718826294 + 50.0 * 8.413780212402344
Epoch 1830, val loss: 2.169137477874756
Epoch 1840, training loss: 422.8323669433594 = 2.1638050079345703 + 50.0 * 8.413371086120605
Epoch 1840, val loss: 2.1691031455993652
Epoch 1850, training loss: 423.0392150878906 = 2.163745164871216 + 50.0 * 8.417509078979492
Epoch 1850, val loss: 2.1690657138824463
Epoch 1860, training loss: 422.8681945800781 = 2.1636860370635986 + 50.0 * 8.414090156555176
Epoch 1860, val loss: 2.169017791748047
Epoch 1870, training loss: 422.7998046875 = 2.1636310815811157 + 50.0 * 8.412723541259766
Epoch 1870, val loss: 2.1689791679382324
Epoch 1880, training loss: 422.7685241699219 = 2.163580298423767 + 50.0 * 8.41209888458252
Epoch 1880, val loss: 2.168945789337158
Epoch 1890, training loss: 422.7564392089844 = 2.1635314226150513 + 50.0 * 8.411857604980469
Epoch 1890, val loss: 2.1689133644104004
Epoch 1900, training loss: 422.91033935546875 = 2.1634888648986816 + 50.0 * 8.414937019348145
Epoch 1900, val loss: 2.1688756942749023
Epoch 1910, training loss: 422.7812194824219 = 2.1634039878845215 + 50.0 * 8.41235637664795
Epoch 1910, val loss: 2.1688320636749268
Epoch 1920, training loss: 422.7542419433594 = 2.163360357284546 + 50.0 * 8.41181755065918
Epoch 1920, val loss: 2.168788433074951
Epoch 1930, training loss: 422.7603759765625 = 2.163300037384033 + 50.0 * 8.411941528320312
Epoch 1930, val loss: 2.168757200241089
Epoch 1940, training loss: 422.73638916015625 = 2.1632449626922607 + 50.0 * 8.411462783813477
Epoch 1940, val loss: 2.168717861175537
Epoch 1950, training loss: 422.7017822265625 = 2.163199305534363 + 50.0 * 8.410771369934082
Epoch 1950, val loss: 2.1686770915985107
Epoch 1960, training loss: 422.6717834472656 = 2.1631444692611694 + 50.0 * 8.410172462463379
Epoch 1960, val loss: 2.1686484813690186
Epoch 1970, training loss: 422.6570129394531 = 2.163097858428955 + 50.0 * 8.409878730773926
Epoch 1970, val loss: 2.1686160564422607
Epoch 1980, training loss: 422.6898498535156 = 2.163049817085266 + 50.0 * 8.41053581237793
Epoch 1980, val loss: 2.1685829162597656
Epoch 1990, training loss: 422.6981201171875 = 2.1629942655563354 + 50.0 * 8.4107027053833
Epoch 1990, val loss: 2.16854190826416
Epoch 2000, training loss: 422.6640319824219 = 2.1629432439804077 + 50.0 * 8.410021781921387
Epoch 2000, val loss: 2.168501138687134
Epoch 2010, training loss: 422.6534423828125 = 2.162890911102295 + 50.0 * 8.409811019897461
Epoch 2010, val loss: 2.1684679985046387
Epoch 2020, training loss: 422.67279052734375 = 2.162843108177185 + 50.0 * 8.410199165344238
Epoch 2020, val loss: 2.1684322357177734
Epoch 2030, training loss: 422.65576171875 = 2.162780284881592 + 50.0 * 8.409859657287598
Epoch 2030, val loss: 2.168397903442383
Epoch 2040, training loss: 422.6115417480469 = 2.1627328395843506 + 50.0 * 8.408976554870605
Epoch 2040, val loss: 2.1683573722839355
Epoch 2050, training loss: 422.5865173339844 = 2.1626839637756348 + 50.0 * 8.408476829528809
Epoch 2050, val loss: 2.168321132659912
Epoch 2060, training loss: 422.5658264160156 = 2.1626381874084473 + 50.0 * 8.408063888549805
Epoch 2060, val loss: 2.168290853500366
Epoch 2070, training loss: 422.6334533691406 = 2.1625866889953613 + 50.0 * 8.409417152404785
Epoch 2070, val loss: 2.1682581901550293
Epoch 2080, training loss: 422.56658935546875 = 2.162533402442932 + 50.0 * 8.4080810546875
Epoch 2080, val loss: 2.168217420578003
Epoch 2090, training loss: 422.52703857421875 = 2.162484645843506 + 50.0 * 8.407291412353516
Epoch 2090, val loss: 2.1681814193725586
Epoch 2100, training loss: 422.51904296875 = 2.1624367237091064 + 50.0 * 8.407132148742676
Epoch 2100, val loss: 2.1681509017944336
Epoch 2110, training loss: 422.5219421386719 = 2.1623928546905518 + 50.0 * 8.407191276550293
Epoch 2110, val loss: 2.1681206226348877
Epoch 2120, training loss: 422.6400146484375 = 2.162338376045227 + 50.0 * 8.409553527832031
Epoch 2120, val loss: 2.168088436126709
Epoch 2130, training loss: 422.6484069824219 = 2.1622893810272217 + 50.0 * 8.409722328186035
Epoch 2130, val loss: 2.168039321899414
Epoch 2140, training loss: 422.516845703125 = 2.162229537963867 + 50.0 * 8.407092094421387
Epoch 2140, val loss: 2.167998790740967
Epoch 2150, training loss: 422.5545349121094 = 2.16218364238739 + 50.0 * 8.40784740447998
Epoch 2150, val loss: 2.1679601669311523
Epoch 2160, training loss: 422.4665222167969 = 2.162127733230591 + 50.0 * 8.406087875366211
Epoch 2160, val loss: 2.16792368888855
Epoch 2170, training loss: 422.46771240234375 = 2.1620835065841675 + 50.0 * 8.406112670898438
Epoch 2170, val loss: 2.1678924560546875
Epoch 2180, training loss: 422.4426574707031 = 2.1620410680770874 + 50.0 * 8.405611991882324
Epoch 2180, val loss: 2.1678626537323
Epoch 2190, training loss: 422.4303283691406 = 2.1619995832443237 + 50.0 * 8.405366897583008
Epoch 2190, val loss: 2.167835235595703
Epoch 2200, training loss: 422.42620849609375 = 2.161958694458008 + 50.0 * 8.405284881591797
Epoch 2200, val loss: 2.1678056716918945
Epoch 2210, training loss: 422.49932861328125 = 2.1619168519973755 + 50.0 * 8.406747817993164
Epoch 2210, val loss: 2.167773962020874
Epoch 2220, training loss: 422.4592590332031 = 2.1618582010269165 + 50.0 * 8.4059476852417
Epoch 2220, val loss: 2.1677310466766357
Epoch 2230, training loss: 422.4916687011719 = 2.16180419921875 + 50.0 * 8.406597137451172
Epoch 2230, val loss: 2.1676907539367676
Epoch 2240, training loss: 422.407470703125 = 2.1617525815963745 + 50.0 * 8.404914855957031
Epoch 2240, val loss: 2.167651653289795
Epoch 2250, training loss: 422.3804626464844 = 2.1617108583450317 + 50.0 * 8.404375076293945
Epoch 2250, val loss: 2.16762375831604
Epoch 2260, training loss: 422.3766174316406 = 2.1616722345352173 + 50.0 * 8.404298782348633
Epoch 2260, val loss: 2.167595863342285
Epoch 2270, training loss: 422.4768371582031 = 2.1616307497024536 + 50.0 * 8.406304359436035
Epoch 2270, val loss: 2.1675682067871094
Epoch 2280, training loss: 422.3683776855469 = 2.1615777015686035 + 50.0 * 8.404135704040527
Epoch 2280, val loss: 2.1675262451171875
Epoch 2290, training loss: 422.3456726074219 = 2.1615307331085205 + 50.0 * 8.403682708740234
Epoch 2290, val loss: 2.1674938201904297
Epoch 2300, training loss: 422.33148193359375 = 2.1614919900894165 + 50.0 * 8.403399467468262
Epoch 2300, val loss: 2.1674656867980957
Epoch 2310, training loss: 422.35089111328125 = 2.1614545583724976 + 50.0 * 8.403788566589355
Epoch 2310, val loss: 2.1674375534057617
Epoch 2320, training loss: 422.5052795410156 = 2.1614086627960205 + 50.0 * 8.406877517700195
Epoch 2320, val loss: 2.1674020290374756
Epoch 2330, training loss: 422.401123046875 = 2.161350131034851 + 50.0 * 8.40479564666748
Epoch 2330, val loss: 2.1673641204833984
Epoch 2340, training loss: 422.3121643066406 = 2.161306381225586 + 50.0 * 8.403017044067383
Epoch 2340, val loss: 2.1673240661621094
Epoch 2350, training loss: 422.2952880859375 = 2.1612660884857178 + 50.0 * 8.402680397033691
Epoch 2350, val loss: 2.1672983169555664
Epoch 2360, training loss: 422.2947692871094 = 2.1612300872802734 + 50.0 * 8.402670860290527
Epoch 2360, val loss: 2.1672732830047607
Epoch 2370, training loss: 422.36053466796875 = 2.1611921787261963 + 50.0 * 8.403986930847168
Epoch 2370, val loss: 2.167245388031006
Epoch 2380, training loss: 422.2910461425781 = 2.161144733428955 + 50.0 * 8.40259838104248
Epoch 2380, val loss: 2.1672096252441406
Epoch 2390, training loss: 422.3891906738281 = 2.1611011028289795 + 50.0 * 8.404561996459961
Epoch 2390, val loss: 2.1671743392944336
Epoch 2400, training loss: 422.2838134765625 = 2.1610541343688965 + 50.0 * 8.40245532989502
Epoch 2400, val loss: 2.167141914367676
Epoch 2410, training loss: 422.2547302246094 = 2.1610158681869507 + 50.0 * 8.401874542236328
Epoch 2410, val loss: 2.167114734649658
Epoch 2420, training loss: 422.23486328125 = 2.160979986190796 + 50.0 * 8.401477813720703
Epoch 2420, val loss: 2.1670894622802734
Epoch 2430, training loss: 422.2935791015625 = 2.160942554473877 + 50.0 * 8.402652740478516
Epoch 2430, val loss: 2.1670637130737305
Epoch 2440, training loss: 422.2939147949219 = 2.1608924865722656 + 50.0 * 8.402660369873047
Epoch 2440, val loss: 2.1670215129852295
Epoch 2450, training loss: 422.2216491699219 = 2.1608402729034424 + 50.0 * 8.401216506958008
Epoch 2450, val loss: 2.166980743408203
Epoch 2460, training loss: 422.2125549316406 = 2.1608049869537354 + 50.0 * 8.40103530883789
Epoch 2460, val loss: 2.16695499420166
Epoch 2470, training loss: 422.1991882324219 = 2.160769820213318 + 50.0 * 8.400768280029297
Epoch 2470, val loss: 2.1669328212738037
Epoch 2480, training loss: 422.22314453125 = 2.160738229751587 + 50.0 * 8.40124797821045
Epoch 2480, val loss: 2.16690993309021
Epoch 2490, training loss: 422.2893371582031 = 2.160694718360901 + 50.0 * 8.402572631835938
Epoch 2490, val loss: 2.1668787002563477
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.39666666666666667
0.8162718249655873
=== training gcn model ===
Epoch 0, training loss: 531.3261108398438 = 2.2130881547927856 + 50.0 * 10.582260131835938
Epoch 0, val loss: 2.2115955352783203
Epoch 10, training loss: 531.2936401367188 = 2.1976239681243896 + 50.0 * 10.581920623779297
Epoch 10, val loss: 2.1968140602111816
Epoch 20, training loss: 531.232421875 = 2.1885435581207275 + 50.0 * 10.580878257751465
Epoch 20, val loss: 2.189222812652588
Epoch 30, training loss: 531.0142822265625 = 2.185677647590637 + 50.0 * 10.57657241821289
Epoch 30, val loss: 2.1868393421173096
Epoch 40, training loss: 530.0992431640625 = 2.18478524684906 + 50.0 * 10.55828857421875
Epoch 40, val loss: 2.1863157749176025
Epoch 50, training loss: 526.9608154296875 = 2.1861990690231323 + 50.0 * 10.495491981506348
Epoch 50, val loss: 2.1877222061157227
Epoch 60, training loss: 518.5631713867188 = 2.188216805458069 + 50.0 * 10.327498435974121
Epoch 60, val loss: 2.1899542808532715
Epoch 70, training loss: 502.5433349609375 = 2.192526936531067 + 50.0 * 10.0070161819458
Epoch 70, val loss: 2.1946520805358887
Epoch 80, training loss: 491.349609375 = 2.1977739334106445 + 50.0 * 9.783036231994629
Epoch 80, val loss: 2.1996188163757324
Epoch 90, training loss: 482.77581787109375 = 2.1988234519958496 + 50.0 * 9.611539840698242
Epoch 90, val loss: 2.200287342071533
Epoch 100, training loss: 471.9310302734375 = 2.1953145265579224 + 50.0 * 9.39471435546875
Epoch 100, val loss: 2.1970725059509277
Epoch 110, training loss: 466.1658020019531 = 2.194259285926819 + 50.0 * 9.279431343078613
Epoch 110, val loss: 2.196209669113159
Epoch 120, training loss: 464.3648376464844 = 2.1924726963043213 + 50.0 * 9.243447303771973
Epoch 120, val loss: 2.194397211074829
Epoch 130, training loss: 461.1152648925781 = 2.1898162364959717 + 50.0 * 9.178508758544922
Epoch 130, val loss: 2.191652774810791
Epoch 140, training loss: 457.031494140625 = 2.1876606941223145 + 50.0 * 9.096877098083496
Epoch 140, val loss: 2.189404010772705
Epoch 150, training loss: 454.1368408203125 = 2.186098337173462 + 50.0 * 9.03901481628418
Epoch 150, val loss: 2.1877408027648926
Epoch 160, training loss: 452.4493103027344 = 2.1848225593566895 + 50.0 * 9.005290031433105
Epoch 160, val loss: 2.1864209175109863
Epoch 170, training loss: 449.89837646484375 = 2.183614134788513 + 50.0 * 8.95429515838623
Epoch 170, val loss: 2.1851959228515625
Epoch 180, training loss: 446.8520812988281 = 2.1829774379730225 + 50.0 * 8.89338207244873
Epoch 180, val loss: 2.1845335960388184
Epoch 190, training loss: 445.00640869140625 = 2.182685613632202 + 50.0 * 8.856474876403809
Epoch 190, val loss: 2.1842682361602783
Epoch 200, training loss: 443.3078308105469 = 2.182013988494873 + 50.0 * 8.822516441345215
Epoch 200, val loss: 2.1836190223693848
Epoch 210, training loss: 441.49969482421875 = 2.18133807182312 + 50.0 * 8.786367416381836
Epoch 210, val loss: 2.1829094886779785
Epoch 220, training loss: 440.0670471191406 = 2.1807303428649902 + 50.0 * 8.757726669311523
Epoch 220, val loss: 2.1823134422302246
Epoch 230, training loss: 439.0558776855469 = 2.1800084114074707 + 50.0 * 8.737517356872559
Epoch 230, val loss: 2.181581497192383
Epoch 240, training loss: 438.2579040527344 = 2.1791099309921265 + 50.0 * 8.721575736999512
Epoch 240, val loss: 2.180680751800537
Epoch 250, training loss: 437.5195617675781 = 2.1781805753707886 + 50.0 * 8.706827163696289
Epoch 250, val loss: 2.179746150970459
Epoch 260, training loss: 436.7901306152344 = 2.1772350072860718 + 50.0 * 8.69225788116455
Epoch 260, val loss: 2.1788430213928223
Epoch 270, training loss: 436.1226501464844 = 2.1764087677001953 + 50.0 * 8.678924560546875
Epoch 270, val loss: 2.17802095413208
Epoch 280, training loss: 435.61224365234375 = 2.1756078004837036 + 50.0 * 8.668732643127441
Epoch 280, val loss: 2.1772713661193848
Epoch 290, training loss: 435.096435546875 = 2.1747599840164185 + 50.0 * 8.658432960510254
Epoch 290, val loss: 2.176426410675049
Epoch 300, training loss: 434.5704040527344 = 2.1739696264266968 + 50.0 * 8.647928237915039
Epoch 300, val loss: 2.1756577491760254
Epoch 310, training loss: 434.0215759277344 = 2.1732592582702637 + 50.0 * 8.636966705322266
Epoch 310, val loss: 2.1749582290649414
Epoch 320, training loss: 433.5561828613281 = 2.1724921464920044 + 50.0 * 8.627674102783203
Epoch 320, val loss: 2.174264430999756
Epoch 330, training loss: 433.026123046875 = 2.1716983318328857 + 50.0 * 8.617088317871094
Epoch 330, val loss: 2.1734941005706787
Epoch 340, training loss: 432.604736328125 = 2.170895576477051 + 50.0 * 8.60867691040039
Epoch 340, val loss: 2.1726880073547363
Epoch 350, training loss: 432.2152404785156 = 2.1701008081436157 + 50.0 * 8.600902557373047
Epoch 350, val loss: 2.1718764305114746
Epoch 360, training loss: 432.0237121582031 = 2.169318914413452 + 50.0 * 8.597087860107422
Epoch 360, val loss: 2.1710851192474365
Epoch 370, training loss: 431.6421813964844 = 2.1684529781341553 + 50.0 * 8.58947467803955
Epoch 370, val loss: 2.1702237129211426
Epoch 380, training loss: 431.2972412109375 = 2.167629837989807 + 50.0 * 8.582592010498047
Epoch 380, val loss: 2.1694447994232178
Epoch 390, training loss: 430.99493408203125 = 2.166851043701172 + 50.0 * 8.57656192779541
Epoch 390, val loss: 2.1686596870422363
Epoch 400, training loss: 430.68670654296875 = 2.1660890579223633 + 50.0 * 8.570412635803223
Epoch 400, val loss: 2.167924642562866
Epoch 410, training loss: 430.438232421875 = 2.16537344455719 + 50.0 * 8.565457344055176
Epoch 410, val loss: 2.167201042175293
Epoch 420, training loss: 430.1717224121094 = 2.1646281480789185 + 50.0 * 8.560141563415527
Epoch 420, val loss: 2.1664724349975586
Epoch 430, training loss: 429.8152160644531 = 2.163937568664551 + 50.0 * 8.553025245666504
Epoch 430, val loss: 2.1657896041870117
Epoch 440, training loss: 429.5038146972656 = 2.163256287574768 + 50.0 * 8.5468111038208
Epoch 440, val loss: 2.165109157562256
Epoch 450, training loss: 429.2873229980469 = 2.162589430809021 + 50.0 * 8.542494773864746
Epoch 450, val loss: 2.1644229888916016
Epoch 460, training loss: 429.122314453125 = 2.161819815635681 + 50.0 * 8.539209365844727
Epoch 460, val loss: 2.163705587387085
Epoch 470, training loss: 428.837158203125 = 2.161164164543152 + 50.0 * 8.533519744873047
Epoch 470, val loss: 2.1630144119262695
Epoch 480, training loss: 428.6588439941406 = 2.160485625267029 + 50.0 * 8.529967308044434
Epoch 480, val loss: 2.1623282432556152
Epoch 490, training loss: 428.4855041503906 = 2.1597777605056763 + 50.0 * 8.526514053344727
Epoch 490, val loss: 2.1616358757019043
Epoch 500, training loss: 428.4450988769531 = 2.159060835838318 + 50.0 * 8.525720596313477
Epoch 500, val loss: 2.1609506607055664
Epoch 510, training loss: 428.2382507324219 = 2.158415198326111 + 50.0 * 8.521596908569336
Epoch 510, val loss: 2.160299301147461
Epoch 520, training loss: 428.0531311035156 = 2.1578177213668823 + 50.0 * 8.517906188964844
Epoch 520, val loss: 2.1597118377685547
Epoch 530, training loss: 427.97100830078125 = 2.157279372215271 + 50.0 * 8.516274452209473
Epoch 530, val loss: 2.1591930389404297
Epoch 540, training loss: 427.8107604980469 = 2.1568278074264526 + 50.0 * 8.513078689575195
Epoch 540, val loss: 2.1587276458740234
Epoch 550, training loss: 427.6995544433594 = 2.1563955545425415 + 50.0 * 8.510863304138184
Epoch 550, val loss: 2.158320903778076
Epoch 560, training loss: 427.5384521484375 = 2.1560312509536743 + 50.0 * 8.507648468017578
Epoch 560, val loss: 2.1579630374908447
Epoch 570, training loss: 427.4525451660156 = 2.155661940574646 + 50.0 * 8.505937576293945
Epoch 570, val loss: 2.1576225757598877
Epoch 580, training loss: 427.3879699707031 = 2.15536892414093 + 50.0 * 8.50465202331543
Epoch 580, val loss: 2.1572680473327637
Epoch 590, training loss: 427.2034912109375 = 2.1549988985061646 + 50.0 * 8.500969886779785
Epoch 590, val loss: 2.156923770904541
Epoch 600, training loss: 427.09869384765625 = 2.154652953147888 + 50.0 * 8.498880386352539
Epoch 600, val loss: 2.156588077545166
Epoch 610, training loss: 427.0167236328125 = 2.1543015241622925 + 50.0 * 8.497248649597168
Epoch 610, val loss: 2.156238317489624
Epoch 620, training loss: 426.8965148925781 = 2.1539597511291504 + 50.0 * 8.494851112365723
Epoch 620, val loss: 2.155895709991455
Epoch 630, training loss: 426.7864074707031 = 2.1536329984664917 + 50.0 * 8.492655754089355
Epoch 630, val loss: 2.155562400817871
Epoch 640, training loss: 426.6669616699219 = 2.1532981395721436 + 50.0 * 8.490273475646973
Epoch 640, val loss: 2.155228614807129
Epoch 650, training loss: 426.583251953125 = 2.1529818773269653 + 50.0 * 8.488605499267578
Epoch 650, val loss: 2.154897689819336
Epoch 660, training loss: 426.50897216796875 = 2.1526546478271484 + 50.0 * 8.487126350402832
Epoch 660, val loss: 2.154571056365967
Epoch 670, training loss: 426.3929443359375 = 2.152302384376526 + 50.0 * 8.48481273651123
Epoch 670, val loss: 2.1542229652404785
Epoch 680, training loss: 426.29095458984375 = 2.1519819498062134 + 50.0 * 8.482779502868652
Epoch 680, val loss: 2.153897285461426
Epoch 690, training loss: 426.2844543457031 = 2.151653528213501 + 50.0 * 8.482656478881836
Epoch 690, val loss: 2.1535754203796387
Epoch 700, training loss: 426.21484375 = 2.151316523551941 + 50.0 * 8.481270790100098
Epoch 700, val loss: 2.15323543548584
Epoch 710, training loss: 426.04876708984375 = 2.150983452796936 + 50.0 * 8.47795581817627
Epoch 710, val loss: 2.1529059410095215
Epoch 720, training loss: 425.9308166503906 = 2.150671124458313 + 50.0 * 8.475603103637695
Epoch 720, val loss: 2.1525864601135254
Epoch 730, training loss: 425.855712890625 = 2.1503536701202393 + 50.0 * 8.474106788635254
Epoch 730, val loss: 2.1522634029388428
Epoch 740, training loss: 425.91497802734375 = 2.1499961614608765 + 50.0 * 8.475299835205078
Epoch 740, val loss: 2.151946544647217
Epoch 750, training loss: 425.7718505859375 = 2.1497069597244263 + 50.0 * 8.472442626953125
Epoch 750, val loss: 2.1516051292419434
Epoch 760, training loss: 425.6538391113281 = 2.149383068084717 + 50.0 * 8.470088958740234
Epoch 760, val loss: 2.1512703895568848
Epoch 770, training loss: 425.5615539550781 = 2.1490484476089478 + 50.0 * 8.468250274658203
Epoch 770, val loss: 2.150944471359253
Epoch 780, training loss: 425.5026550292969 = 2.1487327814102173 + 50.0 * 8.46707820892334
Epoch 780, val loss: 2.150623321533203
Epoch 790, training loss: 425.6650085449219 = 2.1484295129776 + 50.0 * 8.470331192016602
Epoch 790, val loss: 2.150301456451416
Epoch 800, training loss: 425.470947265625 = 2.148054838180542 + 50.0 * 8.466458320617676
Epoch 800, val loss: 2.149942398071289
Epoch 810, training loss: 425.32159423828125 = 2.1477279663085938 + 50.0 * 8.46347713470459
Epoch 810, val loss: 2.1496126651763916
Epoch 820, training loss: 425.2416687011719 = 2.1474016904830933 + 50.0 * 8.461885452270508
Epoch 820, val loss: 2.1492819786071777
Epoch 830, training loss: 425.1839599609375 = 2.1470720767974854 + 50.0 * 8.460738182067871
Epoch 830, val loss: 2.148956298828125
Epoch 840, training loss: 425.13421630859375 = 2.146744132041931 + 50.0 * 8.459749221801758
Epoch 840, val loss: 2.148629665374756
Epoch 850, training loss: 425.22772216796875 = 2.1464176177978516 + 50.0 * 8.461626052856445
Epoch 850, val loss: 2.148301601409912
Epoch 860, training loss: 425.2215576171875 = 2.146109104156494 + 50.0 * 8.461508750915527
Epoch 860, val loss: 2.147958517074585
Epoch 870, training loss: 425.01947021484375 = 2.1457366943359375 + 50.0 * 8.457474708557129
Epoch 870, val loss: 2.1476149559020996
Epoch 880, training loss: 424.9483642578125 = 2.145396113395691 + 50.0 * 8.456059455871582
Epoch 880, val loss: 2.1472864151000977
Epoch 890, training loss: 424.89715576171875 = 2.1450995206832886 + 50.0 * 8.45504093170166
Epoch 890, val loss: 2.146960735321045
Epoch 900, training loss: 424.89862060546875 = 2.1447906494140625 + 50.0 * 8.455077171325684
Epoch 900, val loss: 2.1466331481933594
Epoch 910, training loss: 424.9260559082031 = 2.1444376707077026 + 50.0 * 8.455632209777832
Epoch 910, val loss: 2.146307945251465
Epoch 920, training loss: 424.7914733886719 = 2.1440898180007935 + 50.0 * 8.452947616577148
Epoch 920, val loss: 2.1459600925445557
Epoch 930, training loss: 424.7236633300781 = 2.14377760887146 + 50.0 * 8.451598167419434
Epoch 930, val loss: 2.1456384658813477
Epoch 940, training loss: 424.66754150390625 = 2.1434519290924072 + 50.0 * 8.450481414794922
Epoch 940, val loss: 2.1453166007995605
Epoch 950, training loss: 424.6335754394531 = 2.143137216567993 + 50.0 * 8.449809074401855
Epoch 950, val loss: 2.144998073577881
Epoch 960, training loss: 424.7837829589844 = 2.142791271209717 + 50.0 * 8.45281982421875
Epoch 960, val loss: 2.1446776390075684
Epoch 970, training loss: 424.7006530761719 = 2.142478108406067 + 50.0 * 8.451163291931152
Epoch 970, val loss: 2.1443369388580322
Epoch 980, training loss: 424.51593017578125 = 2.1421449184417725 + 50.0 * 8.44747543334961
Epoch 980, val loss: 2.144007682800293
Epoch 990, training loss: 424.4862976074219 = 2.1418395042419434 + 50.0 * 8.44688892364502
Epoch 990, val loss: 2.143691062927246
Epoch 1000, training loss: 424.43096923828125 = 2.1415241956710815 + 50.0 * 8.445789337158203
Epoch 1000, val loss: 2.1433849334716797
Epoch 1010, training loss: 424.3863220214844 = 2.141218900680542 + 50.0 * 8.444902420043945
Epoch 1010, val loss: 2.1430740356445312
Epoch 1020, training loss: 424.3513488769531 = 2.1409116983413696 + 50.0 * 8.444209098815918
Epoch 1020, val loss: 2.1427669525146484
Epoch 1030, training loss: 424.54498291015625 = 2.14058518409729 + 50.0 * 8.448087692260742
Epoch 1030, val loss: 2.142453193664551
Epoch 1040, training loss: 424.31842041015625 = 2.1402982473373413 + 50.0 * 8.443562507629395
Epoch 1040, val loss: 2.142129898071289
Epoch 1050, training loss: 424.4038391113281 = 2.13997483253479 + 50.0 * 8.445277214050293
Epoch 1050, val loss: 2.141819477081299
Epoch 1060, training loss: 424.2480163574219 = 2.1396368741989136 + 50.0 * 8.442167282104492
Epoch 1060, val loss: 2.141500473022461
Epoch 1070, training loss: 424.1899719238281 = 2.1393502950668335 + 50.0 * 8.441012382507324
Epoch 1070, val loss: 2.141195297241211
Epoch 1080, training loss: 424.14764404296875 = 2.1390455961227417 + 50.0 * 8.44017219543457
Epoch 1080, val loss: 2.140897274017334
Epoch 1090, training loss: 424.1258850097656 = 2.1387429237365723 + 50.0 * 8.439743041992188
Epoch 1090, val loss: 2.140601396560669
Epoch 1100, training loss: 424.4372253417969 = 2.138406276702881 + 50.0 * 8.445976257324219
Epoch 1100, val loss: 2.1402969360351562
Epoch 1110, training loss: 424.1405029296875 = 2.138147711753845 + 50.0 * 8.440047264099121
Epoch 1110, val loss: 2.139986991882324
Epoch 1120, training loss: 424.0765380859375 = 2.137832522392273 + 50.0 * 8.438774108886719
Epoch 1120, val loss: 2.1396751403808594
Epoch 1130, training loss: 424.0013732910156 = 2.1375292539596558 + 50.0 * 8.437276840209961
Epoch 1130, val loss: 2.1393799781799316
Epoch 1140, training loss: 423.9779968261719 = 2.137236475944519 + 50.0 * 8.43681526184082
Epoch 1140, val loss: 2.139091730117798
Epoch 1150, training loss: 424.03948974609375 = 2.1369457244873047 + 50.0 * 8.438051223754883
Epoch 1150, val loss: 2.138803482055664
Epoch 1160, training loss: 423.93280029296875 = 2.1366406679153442 + 50.0 * 8.43592357635498
Epoch 1160, val loss: 2.1385016441345215
Epoch 1170, training loss: 423.92828369140625 = 2.1363528966903687 + 50.0 * 8.43583869934082
Epoch 1170, val loss: 2.1382055282592773
Epoch 1180, training loss: 423.8741149902344 = 2.1360632181167603 + 50.0 * 8.434761047363281
Epoch 1180, val loss: 2.1379165649414062
Epoch 1190, training loss: 423.8382873535156 = 2.1357752084732056 + 50.0 * 8.434050559997559
Epoch 1190, val loss: 2.1376354694366455
Epoch 1200, training loss: 423.82122802734375 = 2.1354856491088867 + 50.0 * 8.433714866638184
Epoch 1200, val loss: 2.1373517513275146
Epoch 1210, training loss: 424.10931396484375 = 2.135193347930908 + 50.0 * 8.439482688903809
Epoch 1210, val loss: 2.1370580196380615
Epoch 1220, training loss: 423.7830505371094 = 2.1349011659622192 + 50.0 * 8.432963371276855
Epoch 1220, val loss: 2.1367664337158203
Epoch 1230, training loss: 423.77587890625 = 2.134619355201721 + 50.0 * 8.432825088500977
Epoch 1230, val loss: 2.136477470397949
Epoch 1240, training loss: 423.71722412109375 = 2.134336829185486 + 50.0 * 8.431657791137695
Epoch 1240, val loss: 2.1361937522888184
Epoch 1250, training loss: 423.7008361816406 = 2.1340543031692505 + 50.0 * 8.43133544921875
Epoch 1250, val loss: 2.1359190940856934
Epoch 1260, training loss: 423.8277893066406 = 2.1337931156158447 + 50.0 * 8.433879852294922
Epoch 1260, val loss: 2.135641098022461
Epoch 1270, training loss: 423.72271728515625 = 2.1334824562072754 + 50.0 * 8.431784629821777
Epoch 1270, val loss: 2.135344982147217
Epoch 1280, training loss: 423.6588439941406 = 2.1331933736801147 + 50.0 * 8.430513381958008
Epoch 1280, val loss: 2.1350624561309814
Epoch 1290, training loss: 423.6201477050781 = 2.132912755012512 + 50.0 * 8.429744720458984
Epoch 1290, val loss: 2.134779930114746
Epoch 1300, training loss: 423.60693359375 = 2.13262939453125 + 50.0 * 8.429486274719238
Epoch 1300, val loss: 2.134512186050415
Epoch 1310, training loss: 423.78759765625 = 2.1323273181915283 + 50.0 * 8.43310546875
Epoch 1310, val loss: 2.134225606918335
Epoch 1320, training loss: 423.6129150390625 = 2.1320886611938477 + 50.0 * 8.429616928100586
Epoch 1320, val loss: 2.133944511413574
Epoch 1330, training loss: 423.5447082519531 = 2.1317840814590454 + 50.0 * 8.428258895874023
Epoch 1330, val loss: 2.1336631774902344
Epoch 1340, training loss: 423.5184326171875 = 2.1315295696258545 + 50.0 * 8.427738189697266
Epoch 1340, val loss: 2.1333930492401123
Epoch 1350, training loss: 423.69293212890625 = 2.1312509775161743 + 50.0 * 8.431233406066895
Epoch 1350, val loss: 2.133119821548462
Epoch 1360, training loss: 423.5130920410156 = 2.130956530570984 + 50.0 * 8.427642822265625
Epoch 1360, val loss: 2.132838249206543
Epoch 1370, training loss: 423.4621276855469 = 2.130700469017029 + 50.0 * 8.426628112792969
Epoch 1370, val loss: 2.1325597763061523
Epoch 1380, training loss: 423.4368591308594 = 2.1304173469543457 + 50.0 * 8.426128387451172
Epoch 1380, val loss: 2.132296085357666
Epoch 1390, training loss: 423.4168395996094 = 2.1301558017730713 + 50.0 * 8.42573356628418
Epoch 1390, val loss: 2.1320295333862305
Epoch 1400, training loss: 423.4053039550781 = 2.1298896074295044 + 50.0 * 8.425508499145508
Epoch 1400, val loss: 2.131765127182007
Epoch 1410, training loss: 423.88299560546875 = 2.129648804664612 + 50.0 * 8.435067176818848
Epoch 1410, val loss: 2.1314945220947266
Epoch 1420, training loss: 423.4278564453125 = 2.1293059587478638 + 50.0 * 8.425971031188965
Epoch 1420, val loss: 2.1311864852905273
Epoch 1430, training loss: 423.40765380859375 = 2.1290401220321655 + 50.0 * 8.425572395324707
Epoch 1430, val loss: 2.130910634994507
Epoch 1440, training loss: 423.3309631347656 = 2.1287803649902344 + 50.0 * 8.424043655395508
Epoch 1440, val loss: 2.1306545734405518
Epoch 1450, training loss: 423.3183288574219 = 2.1285243034362793 + 50.0 * 8.423796653747559
Epoch 1450, val loss: 2.1303980350494385
Epoch 1460, training loss: 423.2928161621094 = 2.128266930580139 + 50.0 * 8.423291206359863
Epoch 1460, val loss: 2.130140542984009
Epoch 1470, training loss: 423.27642822265625 = 2.128011107444763 + 50.0 * 8.422967910766602
Epoch 1470, val loss: 2.1298842430114746
Epoch 1480, training loss: 423.26593017578125 = 2.1277525424957275 + 50.0 * 8.42276382446289
Epoch 1480, val loss: 2.129627227783203
Epoch 1490, training loss: 423.7888488769531 = 2.1274616718292236 + 50.0 * 8.4332275390625
Epoch 1490, val loss: 2.129359722137451
Epoch 1500, training loss: 423.2811584472656 = 2.127209424972534 + 50.0 * 8.423079490661621
Epoch 1500, val loss: 2.1290674209594727
Epoch 1510, training loss: 423.21728515625 = 2.1269367933273315 + 50.0 * 8.421807289123535
Epoch 1510, val loss: 2.128802537918091
Epoch 1520, training loss: 423.1946716308594 = 2.1266844272613525 + 50.0 * 8.42136001586914
Epoch 1520, val loss: 2.1285533905029297
Epoch 1530, training loss: 423.1795654296875 = 2.126437544822693 + 50.0 * 8.421062469482422
Epoch 1530, val loss: 2.128309726715088
Epoch 1540, training loss: 423.163818359375 = 2.1261911392211914 + 50.0 * 8.42075252532959
Epoch 1540, val loss: 2.128065347671509
Epoch 1550, training loss: 423.15283203125 = 2.1259472370147705 + 50.0 * 8.420537948608398
Epoch 1550, val loss: 2.1278207302093506
Epoch 1560, training loss: 423.4307861328125 = 2.125716805458069 + 50.0 * 8.426101684570312
Epoch 1560, val loss: 2.1275722980499268
Epoch 1570, training loss: 423.2289123535156 = 2.1254231929779053 + 50.0 * 8.422069549560547
Epoch 1570, val loss: 2.1272974014282227
Epoch 1580, training loss: 423.10406494140625 = 2.125172972679138 + 50.0 * 8.419577598571777
Epoch 1580, val loss: 2.127042293548584
Epoch 1590, training loss: 423.0974426269531 = 2.1249276399612427 + 50.0 * 8.419449806213379
Epoch 1590, val loss: 2.1268045902252197
Epoch 1600, training loss: 423.07159423828125 = 2.124687433242798 + 50.0 * 8.418937683105469
Epoch 1600, val loss: 2.126565456390381
Epoch 1610, training loss: 423.0755920410156 = 2.1244423389434814 + 50.0 * 8.419022560119629
Epoch 1610, val loss: 2.126328945159912
Epoch 1620, training loss: 423.1451721191406 = 2.1241769790649414 + 50.0 * 8.420419692993164
Epoch 1620, val loss: 2.1260790824890137
Epoch 1630, training loss: 423.0547180175781 = 2.1239800453186035 + 50.0 * 8.418614387512207
Epoch 1630, val loss: 2.1258368492126465
Epoch 1640, training loss: 423.01666259765625 = 2.1237170696258545 + 50.0 * 8.417859077453613
Epoch 1640, val loss: 2.125596761703491
Epoch 1650, training loss: 423.0137939453125 = 2.1235028505325317 + 50.0 * 8.417805671691895
Epoch 1650, val loss: 2.1253652572631836
Epoch 1660, training loss: 423.3485107421875 = 2.123260021209717 + 50.0 * 8.424505233764648
Epoch 1660, val loss: 2.125124931335449
Epoch 1670, training loss: 422.9967346191406 = 2.1229690313339233 + 50.0 * 8.417475700378418
Epoch 1670, val loss: 2.124849796295166
Epoch 1680, training loss: 422.9683837890625 = 2.122738838195801 + 50.0 * 8.416913032531738
Epoch 1680, val loss: 2.1246113777160645
Epoch 1690, training loss: 422.9501647949219 = 2.1225152015686035 + 50.0 * 8.416552543640137
Epoch 1690, val loss: 2.1243863105773926
Epoch 1700, training loss: 422.9190979003906 = 2.1222907304763794 + 50.0 * 8.415936470031738
Epoch 1700, val loss: 2.1241636276245117
Epoch 1710, training loss: 422.9068298339844 = 2.122068166732788 + 50.0 * 8.415695190429688
Epoch 1710, val loss: 2.1239426136016846
Epoch 1720, training loss: 422.8924865722656 = 2.1218502521514893 + 50.0 * 8.415412902832031
Epoch 1720, val loss: 2.12371826171875
Epoch 1730, training loss: 422.9560852050781 = 2.1216450929641724 + 50.0 * 8.416688919067383
Epoch 1730, val loss: 2.1234889030456543
Epoch 1740, training loss: 422.8945007324219 = 2.121372938156128 + 50.0 * 8.415462493896484
Epoch 1740, val loss: 2.1232619285583496
Epoch 1750, training loss: 422.9026794433594 = 2.121154308319092 + 50.0 * 8.415630340576172
Epoch 1750, val loss: 2.1230149269104004
Epoch 1760, training loss: 422.8383483886719 = 2.120923161506653 + 50.0 * 8.414348602294922
Epoch 1760, val loss: 2.1227867603302
Epoch 1770, training loss: 422.8318786621094 = 2.1207010746002197 + 50.0 * 8.414223670959473
Epoch 1770, val loss: 2.1225779056549072
Epoch 1780, training loss: 422.8185119628906 = 2.120498299598694 + 50.0 * 8.413960456848145
Epoch 1780, val loss: 2.1223607063293457
Epoch 1790, training loss: 422.9405517578125 = 2.120291590690613 + 50.0 * 8.416404724121094
Epoch 1790, val loss: 2.1221399307250977
Epoch 1800, training loss: 422.8434143066406 = 2.120024561882019 + 50.0 * 8.414467811584473
Epoch 1800, val loss: 2.1219091415405273
Epoch 1810, training loss: 422.9433898925781 = 2.1197952032089233 + 50.0 * 8.416472434997559
Epoch 1810, val loss: 2.121673107147217
Epoch 1820, training loss: 422.7915954589844 = 2.119586944580078 + 50.0 * 8.413439750671387
Epoch 1820, val loss: 2.1214489936828613
Epoch 1830, training loss: 422.7492980957031 = 2.1193630695343018 + 50.0 * 8.412598609924316
Epoch 1830, val loss: 2.1212387084960938
Epoch 1840, training loss: 422.7425537109375 = 2.119153618812561 + 50.0 * 8.412467956542969
Epoch 1840, val loss: 2.1210246086120605
Epoch 1850, training loss: 422.7283020019531 = 2.118946075439453 + 50.0 * 8.412186622619629
Epoch 1850, val loss: 2.120816707611084
Epoch 1860, training loss: 422.7810974121094 = 2.1187409162521362 + 50.0 * 8.413247108459473
Epoch 1860, val loss: 2.1206040382385254
Epoch 1870, training loss: 422.7850036621094 = 2.1184957027435303 + 50.0 * 8.413330078125
Epoch 1870, val loss: 2.1203842163085938
Epoch 1880, training loss: 422.69775390625 = 2.118285894393921 + 50.0 * 8.411589622497559
Epoch 1880, val loss: 2.1201510429382324
Epoch 1890, training loss: 422.6983642578125 = 2.1180777549743652 + 50.0 * 8.411605834960938
Epoch 1890, val loss: 2.119948625564575
Epoch 1900, training loss: 422.68341064453125 = 2.1178722381591797 + 50.0 * 8.411311149597168
Epoch 1900, val loss: 2.119739055633545
Epoch 1910, training loss: 422.82843017578125 = 2.1176671981811523 + 50.0 * 8.414215087890625
Epoch 1910, val loss: 2.1195311546325684
Epoch 1920, training loss: 422.6888427734375 = 2.117440700531006 + 50.0 * 8.411428451538086
Epoch 1920, val loss: 2.119312286376953
Epoch 1930, training loss: 422.6457214355469 = 2.117234945297241 + 50.0 * 8.410569190979004
Epoch 1930, val loss: 2.1191017627716064
Epoch 1940, training loss: 422.6383361816406 = 2.117038130760193 + 50.0 * 8.410426139831543
Epoch 1940, val loss: 2.118901491165161
Epoch 1950, training loss: 422.8627014160156 = 2.116844058036804 + 50.0 * 8.4149169921875
Epoch 1950, val loss: 2.1186952590942383
Epoch 1960, training loss: 422.65496826171875 = 2.1165871620178223 + 50.0 * 8.410767555236816
Epoch 1960, val loss: 2.118467330932617
Epoch 1970, training loss: 422.60589599609375 = 2.1163910627365112 + 50.0 * 8.4097900390625
Epoch 1970, val loss: 2.118258476257324
Epoch 1980, training loss: 422.5814208984375 = 2.1161848306655884 + 50.0 * 8.40930461883545
Epoch 1980, val loss: 2.118061065673828
Epoch 1990, training loss: 422.5733947753906 = 2.115986466407776 + 50.0 * 8.409148216247559
Epoch 1990, val loss: 2.117863655090332
Epoch 2000, training loss: 422.7666931152344 = 2.1157785654067993 + 50.0 * 8.413018226623535
Epoch 2000, val loss: 2.1176657676696777
Epoch 2010, training loss: 422.6401062011719 = 2.115570068359375 + 50.0 * 8.410490989685059
Epoch 2010, val loss: 2.1174416542053223
Epoch 2020, training loss: 422.5580749511719 = 2.1153619289398193 + 50.0 * 8.408854484558105
Epoch 2020, val loss: 2.1172404289245605
Epoch 2030, training loss: 422.5270080566406 = 2.1151657104492188 + 50.0 * 8.408236503601074
Epoch 2030, val loss: 2.11704158782959
Epoch 2040, training loss: 422.5209045410156 = 2.114973306655884 + 50.0 * 8.408119201660156
Epoch 2040, val loss: 2.1168508529663086
Epoch 2050, training loss: 422.549072265625 = 2.1147825717926025 + 50.0 * 8.408685684204102
Epoch 2050, val loss: 2.1166582107543945
Epoch 2060, training loss: 422.6838684082031 = 2.114579677581787 + 50.0 * 8.411385536193848
Epoch 2060, val loss: 2.116453170776367
Epoch 2070, training loss: 422.5255126953125 = 2.114362120628357 + 50.0 * 8.408223152160645
Epoch 2070, val loss: 2.1162304878234863
Epoch 2080, training loss: 422.4906311035156 = 2.1141330003738403 + 50.0 * 8.407529830932617
Epoch 2080, val loss: 2.116030216217041
Epoch 2090, training loss: 422.48248291015625 = 2.1139551401138306 + 50.0 * 8.407370567321777
Epoch 2090, val loss: 2.1158390045166016
Epoch 2100, training loss: 422.4775390625 = 2.113765239715576 + 50.0 * 8.407275199890137
Epoch 2100, val loss: 2.1156535148620605
Epoch 2110, training loss: 422.5380859375 = 2.113572597503662 + 50.0 * 8.408490180969238
Epoch 2110, val loss: 2.115464210510254
Epoch 2120, training loss: 422.47900390625 = 2.1133581399917603 + 50.0 * 8.407312393188477
Epoch 2120, val loss: 2.115262985229492
Epoch 2130, training loss: 422.5131530761719 = 2.113155245780945 + 50.0 * 8.407999992370605
Epoch 2130, val loss: 2.1150598526000977
Epoch 2140, training loss: 422.43218994140625 = 2.112982988357544 + 50.0 * 8.406384468078613
Epoch 2140, val loss: 2.1148669719696045
Epoch 2150, training loss: 422.43243408203125 = 2.112787127494812 + 50.0 * 8.406393051147461
Epoch 2150, val loss: 2.1146838665008545
Epoch 2160, training loss: 422.4267272949219 = 2.1126071214675903 + 50.0 * 8.406282424926758
Epoch 2160, val loss: 2.114500045776367
Epoch 2170, training loss: 422.4684753417969 = 2.112415313720703 + 50.0 * 8.407120704650879
Epoch 2170, val loss: 2.114315986633301
Epoch 2180, training loss: 422.42413330078125 = 2.1122143268585205 + 50.0 * 8.406238555908203
Epoch 2180, val loss: 2.1141185760498047
Epoch 2190, training loss: 422.39483642578125 = 2.112030506134033 + 50.0 * 8.405655860900879
Epoch 2190, val loss: 2.113926410675049
Epoch 2200, training loss: 422.42578125 = 2.1118600368499756 + 50.0 * 8.406278610229492
Epoch 2200, val loss: 2.113740921020508
Epoch 2210, training loss: 422.49407958984375 = 2.1116673946380615 + 50.0 * 8.407648086547852
Epoch 2210, val loss: 2.1135544776916504
Epoch 2220, training loss: 422.3629150390625 = 2.1114357709884644 + 50.0 * 8.405029296875
Epoch 2220, val loss: 2.1133382320404053
Epoch 2230, training loss: 422.35498046875 = 2.111235499382019 + 50.0 * 8.404874801635742
Epoch 2230, val loss: 2.1131629943847656
Epoch 2240, training loss: 422.33575439453125 = 2.111075282096863 + 50.0 * 8.40449333190918
Epoch 2240, val loss: 2.1129817962646484
Epoch 2250, training loss: 422.32989501953125 = 2.110888957977295 + 50.0 * 8.404379844665527
Epoch 2250, val loss: 2.112813949584961
Epoch 2260, training loss: 422.3993835449219 = 2.1107057332992554 + 50.0 * 8.405773162841797
Epoch 2260, val loss: 2.112638473510742
Epoch 2270, training loss: 422.4078369140625 = 2.1105239391326904 + 50.0 * 8.405945777893066
Epoch 2270, val loss: 2.1124377250671387
Epoch 2280, training loss: 422.4313659667969 = 2.1103203296661377 + 50.0 * 8.406420707702637
Epoch 2280, val loss: 2.112248420715332
Epoch 2290, training loss: 422.3832092285156 = 2.110109806060791 + 50.0 * 8.405462265014648
Epoch 2290, val loss: 2.1120543479919434
Epoch 2300, training loss: 422.322998046875 = 2.1099406480789185 + 50.0 * 8.404260635375977
Epoch 2300, val loss: 2.111863136291504
Epoch 2310, training loss: 422.28216552734375 = 2.109764337539673 + 50.0 * 8.403448104858398
Epoch 2310, val loss: 2.111696243286133
Epoch 2320, training loss: 422.2782897949219 = 2.1096009016036987 + 50.0 * 8.403373718261719
Epoch 2320, val loss: 2.1115267276763916
Epoch 2330, training loss: 422.3684387207031 = 2.1094402074813843 + 50.0 * 8.405179977416992
Epoch 2330, val loss: 2.1113553047180176
Epoch 2340, training loss: 422.2779541015625 = 2.1092315912246704 + 50.0 * 8.403374671936035
Epoch 2340, val loss: 2.1111669540405273
Epoch 2350, training loss: 422.27587890625 = 2.1090341806411743 + 50.0 * 8.403336524963379
Epoch 2350, val loss: 2.110976219177246
Epoch 2360, training loss: 422.24737548828125 = 2.108876347541809 + 50.0 * 8.402770042419434
Epoch 2360, val loss: 2.110809803009033
Epoch 2370, training loss: 422.2310791015625 = 2.1086989641189575 + 50.0 * 8.402447700500488
Epoch 2370, val loss: 2.1106419563293457
Epoch 2380, training loss: 422.3149719238281 = 2.108521580696106 + 50.0 * 8.404129028320312
Epoch 2380, val loss: 2.1104698181152344
Epoch 2390, training loss: 422.23492431640625 = 2.1083301305770874 + 50.0 * 8.402531623840332
Epoch 2390, val loss: 2.1102828979492188
Epoch 2400, training loss: 422.2032775878906 = 2.1081559658050537 + 50.0 * 8.401902198791504
Epoch 2400, val loss: 2.1100997924804688
Epoch 2410, training loss: 422.206298828125 = 2.1079864501953125 + 50.0 * 8.401966094970703
Epoch 2410, val loss: 2.1099348068237305
Epoch 2420, training loss: 422.3709716796875 = 2.107821822166443 + 50.0 * 8.40526294708252
Epoch 2420, val loss: 2.1097683906555176
Epoch 2430, training loss: 422.2238464355469 = 2.1076016426086426 + 50.0 * 8.402324676513672
Epoch 2430, val loss: 2.1095709800720215
Epoch 2440, training loss: 422.2064208984375 = 2.107442855834961 + 50.0 * 8.401979446411133
Epoch 2440, val loss: 2.109391212463379
Epoch 2450, training loss: 422.1763916015625 = 2.1072587966918945 + 50.0 * 8.401382446289062
Epoch 2450, val loss: 2.109226703643799
Epoch 2460, training loss: 422.1593933105469 = 2.107107996940613 + 50.0 * 8.401045799255371
Epoch 2460, val loss: 2.1090712547302246
Epoch 2470, training loss: 422.2078552246094 = 2.106930375099182 + 50.0 * 8.402018547058105
Epoch 2470, val loss: 2.108910083770752
Epoch 2480, training loss: 422.27032470703125 = 2.1067416667938232 + 50.0 * 8.403271675109863
Epoch 2480, val loss: 2.108712911605835
Epoch 2490, training loss: 422.14935302734375 = 2.1065657138824463 + 50.0 * 8.400856018066406
Epoch 2490, val loss: 2.1085381507873535
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.39594202898550723
0.8130116641309861
The final CL Acc:0.39720, 0.00130, The final GNN Acc:0.81487, 0.00137
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110802])
remove edge: torch.Size([2, 66906])
updated graph: torch.Size([2, 89060])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 531.30224609375 = 2.1910911798477173 + 50.0 * 10.582222938537598
Epoch 0, val loss: 2.19126558303833
Epoch 10, training loss: 531.2718505859375 = 2.1855045557022095 + 50.0 * 10.581727027893066
Epoch 10, val loss: 2.186713695526123
Epoch 20, training loss: 531.1630249023438 = 2.1828798055648804 + 50.0 * 10.57960319519043
Epoch 20, val loss: 2.184741973876953
Epoch 30, training loss: 530.68017578125 = 2.1825720071792603 + 50.0 * 10.569952964782715
Epoch 30, val loss: 2.1846141815185547
Epoch 40, training loss: 528.8713989257812 = 2.182731032371521 + 50.0 * 10.533773422241211
Epoch 40, val loss: 2.1844935417175293
Epoch 50, training loss: 523.98193359375 = 2.181168556213379 + 50.0 * 10.436016082763672
Epoch 50, val loss: 2.1827657222747803
Epoch 60, training loss: 513.312744140625 = 2.1803476810455322 + 50.0 * 10.222647666931152
Epoch 60, val loss: 2.1819329261779785
Epoch 70, training loss: 497.1331481933594 = 2.177536368370056 + 50.0 * 9.8991117477417
Epoch 70, val loss: 2.1789369583129883
Epoch 80, training loss: 484.39312744140625 = 2.1781927347183228 + 50.0 * 9.644298553466797
Epoch 80, val loss: 2.1795406341552734
Epoch 90, training loss: 472.8450622558594 = 2.1793196201324463 + 50.0 * 9.413314819335938
Epoch 90, val loss: 2.180682897567749
Epoch 100, training loss: 466.09063720703125 = 2.1793869733810425 + 50.0 * 9.27822494506836
Epoch 100, val loss: 2.180661201477051
Epoch 110, training loss: 464.0855712890625 = 2.1792867183685303 + 50.0 * 9.238125801086426
Epoch 110, val loss: 2.1804492473602295
Epoch 120, training loss: 462.7421875 = 2.1783329248428345 + 50.0 * 9.21127700805664
Epoch 120, val loss: 2.1795411109924316
Epoch 130, training loss: 460.99493408203125 = 2.177353024482727 + 50.0 * 9.176351547241211
Epoch 130, val loss: 2.1786065101623535
Epoch 140, training loss: 458.71343994140625 = 2.1770131587982178 + 50.0 * 9.130728721618652
Epoch 140, val loss: 2.178314685821533
Epoch 150, training loss: 455.8046875 = 2.1773524284362793 + 50.0 * 9.07254695892334
Epoch 150, val loss: 2.178643226623535
Epoch 160, training loss: 452.886962890625 = 2.1780025959014893 + 50.0 * 9.014179229736328
Epoch 160, val loss: 2.17928409576416
Epoch 170, training loss: 451.1725158691406 = 2.1783019304275513 + 50.0 * 8.979884147644043
Epoch 170, val loss: 2.1795830726623535
Epoch 180, training loss: 449.5220947265625 = 2.1782102584838867 + 50.0 * 8.946877479553223
Epoch 180, val loss: 2.179476737976074
Epoch 190, training loss: 446.94024658203125 = 2.1784310340881348 + 50.0 * 8.895236015319824
Epoch 190, val loss: 2.179671287536621
Epoch 200, training loss: 443.6724548339844 = 2.1786094903945923 + 50.0 * 8.829876899719238
Epoch 200, val loss: 2.1798806190490723
Epoch 210, training loss: 441.223388671875 = 2.178780198097229 + 50.0 * 8.780892372131348
Epoch 210, val loss: 2.1800801753997803
Epoch 220, training loss: 439.4498291015625 = 2.178899049758911 + 50.0 * 8.745418548583984
Epoch 220, val loss: 2.1802005767822266
Epoch 230, training loss: 437.9223327636719 = 2.178719639778137 + 50.0 * 8.714872360229492
Epoch 230, val loss: 2.1800155639648438
Epoch 240, training loss: 436.38531494140625 = 2.1782361268997192 + 50.0 * 8.684142112731934
Epoch 240, val loss: 2.179513454437256
Epoch 250, training loss: 435.04388427734375 = 2.1777831315994263 + 50.0 * 8.65732192993164
Epoch 250, val loss: 2.1790506839752197
Epoch 260, training loss: 433.8825988769531 = 2.177396774291992 + 50.0 * 8.634103775024414
Epoch 260, val loss: 2.1786317825317383
Epoch 270, training loss: 432.87457275390625 = 2.176931619644165 + 50.0 * 8.61395263671875
Epoch 270, val loss: 2.1781768798828125
Epoch 280, training loss: 431.9993896484375 = 2.176501750946045 + 50.0 * 8.596457481384277
Epoch 280, val loss: 2.1777634620666504
Epoch 290, training loss: 431.4026184082031 = 2.1760716438293457 + 50.0 * 8.5845308303833
Epoch 290, val loss: 2.1773123741149902
Epoch 300, training loss: 430.67572021484375 = 2.1755951642990112 + 50.0 * 8.570002555847168
Epoch 300, val loss: 2.176846504211426
Epoch 310, training loss: 430.1248474121094 = 2.17513644695282 + 50.0 * 8.55899429321289
Epoch 310, val loss: 2.1763803958892822
Epoch 320, training loss: 429.75018310546875 = 2.1746878623962402 + 50.0 * 8.551509857177734
Epoch 320, val loss: 2.1759350299835205
Epoch 330, training loss: 429.2582702636719 = 2.1742563247680664 + 50.0 * 8.541680335998535
Epoch 330, val loss: 2.1755008697509766
Epoch 340, training loss: 428.7621154785156 = 2.173833966255188 + 50.0 * 8.531765937805176
Epoch 340, val loss: 2.175095558166504
Epoch 350, training loss: 428.41064453125 = 2.1734254360198975 + 50.0 * 8.524744033813477
Epoch 350, val loss: 2.174697160720825
Epoch 360, training loss: 428.0399169921875 = 2.173039197921753 + 50.0 * 8.517337799072266
Epoch 360, val loss: 2.1742911338806152
Epoch 370, training loss: 427.685546875 = 2.172654390335083 + 50.0 * 8.510257720947266
Epoch 370, val loss: 2.17390775680542
Epoch 380, training loss: 427.4815673828125 = 2.1722337007522583 + 50.0 * 8.506186485290527
Epoch 380, val loss: 2.173506498336792
Epoch 390, training loss: 427.09735107421875 = 2.1718454360961914 + 50.0 * 8.498510360717773
Epoch 390, val loss: 2.1731133460998535
Epoch 400, training loss: 426.79083251953125 = 2.1714507341384888 + 50.0 * 8.492387771606445
Epoch 400, val loss: 2.172727346420288
Epoch 410, training loss: 426.5142517089844 = 2.1710710525512695 + 50.0 * 8.486863136291504
Epoch 410, val loss: 2.1723456382751465
Epoch 420, training loss: 426.2673645019531 = 2.1706771850585938 + 50.0 * 8.48193359375
Epoch 420, val loss: 2.1719627380371094
Epoch 430, training loss: 426.3306884765625 = 2.170281410217285 + 50.0 * 8.483207702636719
Epoch 430, val loss: 2.171574592590332
Epoch 440, training loss: 425.89984130859375 = 2.1698803901672363 + 50.0 * 8.47459888458252
Epoch 440, val loss: 2.171184539794922
Epoch 450, training loss: 425.68902587890625 = 2.169492721557617 + 50.0 * 8.470390319824219
Epoch 450, val loss: 2.1708059310913086
Epoch 460, training loss: 425.49468994140625 = 2.169113278388977 + 50.0 * 8.466511726379395
Epoch 460, val loss: 2.170427083969116
Epoch 470, training loss: 425.3743591308594 = 2.168731689453125 + 50.0 * 8.464112281799316
Epoch 470, val loss: 2.1700496673583984
Epoch 480, training loss: 425.2618408203125 = 2.168329358100891 + 50.0 * 8.461870193481445
Epoch 480, val loss: 2.1696596145629883
Epoch 490, training loss: 425.0410461425781 = 2.1679447889328003 + 50.0 * 8.457462310791016
Epoch 490, val loss: 2.1692819595336914
Epoch 500, training loss: 424.88720703125 = 2.167559504508972 + 50.0 * 8.454392433166504
Epoch 500, val loss: 2.1689085960388184
Epoch 510, training loss: 424.7564697265625 = 2.1671717166900635 + 50.0 * 8.451786041259766
Epoch 510, val loss: 2.1685338020324707
Epoch 520, training loss: 424.65118408203125 = 2.1667873859405518 + 50.0 * 8.449687957763672
Epoch 520, val loss: 2.168138265609741
Epoch 530, training loss: 424.5927429199219 = 2.1664031744003296 + 50.0 * 8.448526382446289
Epoch 530, val loss: 2.1677560806274414
Epoch 540, training loss: 424.39093017578125 = 2.166002035140991 + 50.0 * 8.444498062133789
Epoch 540, val loss: 2.167386531829834
Epoch 550, training loss: 424.28924560546875 = 2.165604591369629 + 50.0 * 8.442472457885742
Epoch 550, val loss: 2.1670007705688477
Epoch 560, training loss: 424.1690673828125 = 2.1652209758758545 + 50.0 * 8.44007682800293
Epoch 560, val loss: 2.1666183471679688
Epoch 570, training loss: 424.0595703125 = 2.1648380756378174 + 50.0 * 8.437894821166992
Epoch 570, val loss: 2.1662402153015137
Epoch 580, training loss: 423.9566345214844 = 2.164455533027649 + 50.0 * 8.435843467712402
Epoch 580, val loss: 2.1658620834350586
Epoch 590, training loss: 424.3389892578125 = 2.1640849113464355 + 50.0 * 8.443497657775879
Epoch 590, val loss: 2.1654880046844482
Epoch 600, training loss: 423.7940673828125 = 2.1636897325515747 + 50.0 * 8.432607650756836
Epoch 600, val loss: 2.165092945098877
Epoch 610, training loss: 423.72119140625 = 2.163297176361084 + 50.0 * 8.431158065795898
Epoch 610, val loss: 2.164721965789795
Epoch 620, training loss: 423.6123962402344 = 2.1629230976104736 + 50.0 * 8.42898941040039
Epoch 620, val loss: 2.164346218109131
Epoch 630, training loss: 423.5251159667969 = 2.1625512838363647 + 50.0 * 8.427251815795898
Epoch 630, val loss: 2.163975238800049
Epoch 640, training loss: 423.53533935546875 = 2.1621803045272827 + 50.0 * 8.42746353149414
Epoch 640, val loss: 2.1636078357696533
Epoch 650, training loss: 423.5360412597656 = 2.161795496940613 + 50.0 * 8.427484512329102
Epoch 650, val loss: 2.163231134414673
Epoch 660, training loss: 423.33447265625 = 2.1614335775375366 + 50.0 * 8.423460960388184
Epoch 660, val loss: 2.1628670692443848
Epoch 670, training loss: 423.2416076660156 = 2.161073923110962 + 50.0 * 8.421610832214355
Epoch 670, val loss: 2.162506580352783
Epoch 680, training loss: 423.1795959472656 = 2.160712718963623 + 50.0 * 8.420377731323242
Epoch 680, val loss: 2.162144422531128
Epoch 690, training loss: 423.1114196777344 = 2.1603493690490723 + 50.0 * 8.419021606445312
Epoch 690, val loss: 2.161787986755371
Epoch 700, training loss: 423.2947692871094 = 2.1599851846694946 + 50.0 * 8.422696113586426
Epoch 700, val loss: 2.1614322662353516
Epoch 710, training loss: 423.015625 = 2.159627676010132 + 50.0 * 8.417119979858398
Epoch 710, val loss: 2.161067485809326
Epoch 720, training loss: 422.9580383300781 = 2.159274935722351 + 50.0 * 8.415975570678711
Epoch 720, val loss: 2.1607141494750977
Epoch 730, training loss: 422.8818359375 = 2.1589252948760986 + 50.0 * 8.414458274841309
Epoch 730, val loss: 2.1603622436523438
Epoch 740, training loss: 422.82183837890625 = 2.158567190170288 + 50.0 * 8.413265228271484
Epoch 740, val loss: 2.1600148677825928
Epoch 750, training loss: 422.7680969238281 = 2.1582212448120117 + 50.0 * 8.41219711303711
Epoch 750, val loss: 2.15966796875
Epoch 760, training loss: 422.9613342285156 = 2.157859206199646 + 50.0 * 8.416069030761719
Epoch 760, val loss: 2.1593215465545654
Epoch 770, training loss: 422.7650451660156 = 2.1575303077697754 + 50.0 * 8.412150382995605
Epoch 770, val loss: 2.1589622497558594
Epoch 780, training loss: 422.6143798828125 = 2.157169222831726 + 50.0 * 8.409144401550293
Epoch 780, val loss: 2.1586172580718994
Epoch 790, training loss: 422.6445617675781 = 2.156819462776184 + 50.0 * 8.409754753112793
Epoch 790, val loss: 2.158271312713623
Epoch 800, training loss: 422.51177978515625 = 2.1564751863479614 + 50.0 * 8.407106399536133
Epoch 800, val loss: 2.1579246520996094
Epoch 810, training loss: 422.45379638671875 = 2.156134605407715 + 50.0 * 8.405953407287598
Epoch 810, val loss: 2.1575815677642822
Epoch 820, training loss: 422.4132995605469 = 2.155787229537964 + 50.0 * 8.405150413513184
Epoch 820, val loss: 2.1572418212890625
Epoch 830, training loss: 422.3692626953125 = 2.155441641807556 + 50.0 * 8.404275894165039
Epoch 830, val loss: 2.1569011211395264
Epoch 840, training loss: 422.44525146484375 = 2.155108332633972 + 50.0 * 8.405802726745605
Epoch 840, val loss: 2.1565353870391846
Epoch 850, training loss: 422.2896728515625 = 2.1547415256500244 + 50.0 * 8.402698516845703
Epoch 850, val loss: 2.156189441680908
Epoch 860, training loss: 422.2416076660156 = 2.1544095277786255 + 50.0 * 8.40174388885498
Epoch 860, val loss: 2.155856132507324
Epoch 870, training loss: 422.1929626464844 = 2.1540735960006714 + 50.0 * 8.400777816772461
Epoch 870, val loss: 2.155515670776367
Epoch 880, training loss: 422.1483154296875 = 2.153730273246765 + 50.0 * 8.39989185333252
Epoch 880, val loss: 2.1551761627197266
Epoch 890, training loss: 422.30499267578125 = 2.1533972024917603 + 50.0 * 8.403031349182129
Epoch 890, val loss: 2.1548352241516113
Epoch 900, training loss: 422.1766052246094 = 2.153024911880493 + 50.0 * 8.400471687316895
Epoch 900, val loss: 2.1544852256774902
Epoch 910, training loss: 422.0230712890625 = 2.1526981592178345 + 50.0 * 8.397407531738281
Epoch 910, val loss: 2.154149293899536
Epoch 920, training loss: 422.00115966796875 = 2.152362823486328 + 50.0 * 8.39697551727295
Epoch 920, val loss: 2.1538116931915283
Epoch 930, training loss: 422.11956787109375 = 2.152017831802368 + 50.0 * 8.399351119995117
Epoch 930, val loss: 2.153472423553467
Epoch 940, training loss: 421.97113037109375 = 2.151678681373596 + 50.0 * 8.39638900756836
Epoch 940, val loss: 2.1531307697296143
Epoch 950, training loss: 421.89739990234375 = 2.151344656944275 + 50.0 * 8.39492130279541
Epoch 950, val loss: 2.152797222137451
Epoch 960, training loss: 421.8427734375 = 2.1510169506073 + 50.0 * 8.393835067749023
Epoch 960, val loss: 2.1524658203125
Epoch 970, training loss: 421.8466796875 = 2.1506857872009277 + 50.0 * 8.393919944763184
Epoch 970, val loss: 2.152132034301758
Epoch 980, training loss: 421.7873229980469 = 2.1503487825393677 + 50.0 * 8.392739295959473
Epoch 980, val loss: 2.1517977714538574
Epoch 990, training loss: 421.7531433105469 = 2.150020480155945 + 50.0 * 8.392062187194824
Epoch 990, val loss: 2.1514663696289062
Epoch 1000, training loss: 421.7037658691406 = 2.1496901512145996 + 50.0 * 8.391081809997559
Epoch 1000, val loss: 2.1511435508728027
Epoch 1010, training loss: 421.6678466796875 = 2.1493688821792603 + 50.0 * 8.390369415283203
Epoch 1010, val loss: 2.150818347930908
Epoch 1020, training loss: 421.8062438964844 = 2.1490273475646973 + 50.0 * 8.393144607543945
Epoch 1020, val loss: 2.1504907608032227
Epoch 1030, training loss: 421.7142333984375 = 2.148715615272522 + 50.0 * 8.391310691833496
Epoch 1030, val loss: 2.1501641273498535
Epoch 1040, training loss: 421.5592956542969 = 2.148396849632263 + 50.0 * 8.38821792602539
Epoch 1040, val loss: 2.1498374938964844
Epoch 1050, training loss: 421.5301208496094 = 2.1480783224105835 + 50.0 * 8.387640953063965
Epoch 1050, val loss: 2.1495230197906494
Epoch 1060, training loss: 421.52801513671875 = 2.147768259048462 + 50.0 * 8.387604713439941
Epoch 1060, val loss: 2.1492042541503906
Epoch 1070, training loss: 421.50390625 = 2.147446870803833 + 50.0 * 8.387128829956055
Epoch 1070, val loss: 2.1488852500915527
Epoch 1080, training loss: 421.4248352050781 = 2.147128462791443 + 50.0 * 8.385554313659668
Epoch 1080, val loss: 2.148571491241455
Epoch 1090, training loss: 421.4646911621094 = 2.1468050479888916 + 50.0 * 8.386357307434082
Epoch 1090, val loss: 2.1482601165771484
Epoch 1100, training loss: 421.4446716308594 = 2.1465091705322266 + 50.0 * 8.385963439941406
Epoch 1100, val loss: 2.1479363441467285
Epoch 1110, training loss: 421.3403625488281 = 2.146186947822571 + 50.0 * 8.383883476257324
Epoch 1110, val loss: 2.1476306915283203
Epoch 1120, training loss: 421.3045959472656 = 2.1458935737609863 + 50.0 * 8.383173942565918
Epoch 1120, val loss: 2.1473212242126465
Epoch 1130, training loss: 421.26422119140625 = 2.145576000213623 + 50.0 * 8.382372856140137
Epoch 1130, val loss: 2.1470212936401367
Epoch 1140, training loss: 421.2731018066406 = 2.1452778577804565 + 50.0 * 8.382556915283203
Epoch 1140, val loss: 2.1467156410217285
Epoch 1150, training loss: 421.2931213378906 = 2.1449700593948364 + 50.0 * 8.382963180541992
Epoch 1150, val loss: 2.146404504776001
Epoch 1160, training loss: 421.1737365722656 = 2.1446787118911743 + 50.0 * 8.38058090209961
Epoch 1160, val loss: 2.1461005210876465
Epoch 1170, training loss: 421.134521484375 = 2.1443692445755005 + 50.0 * 8.379802703857422
Epoch 1170, val loss: 2.1458029747009277
Epoch 1180, training loss: 421.1460876464844 = 2.144068121910095 + 50.0 * 8.380040168762207
Epoch 1180, val loss: 2.1455078125
Epoch 1190, training loss: 421.1044921875 = 2.1437758207321167 + 50.0 * 8.3792142868042
Epoch 1190, val loss: 2.145205020904541
Epoch 1200, training loss: 421.0978698730469 = 2.14348042011261 + 50.0 * 8.379087448120117
Epoch 1200, val loss: 2.144908905029297
Epoch 1210, training loss: 421.0821228027344 = 2.1431750059127808 + 50.0 * 8.378778457641602
Epoch 1210, val loss: 2.14461088180542
Epoch 1220, training loss: 420.9625549316406 = 2.1428972482681274 + 50.0 * 8.37639331817627
Epoch 1220, val loss: 2.1443185806274414
Epoch 1230, training loss: 420.95697021484375 = 2.142608165740967 + 50.0 * 8.376287460327148
Epoch 1230, val loss: 2.1440303325653076
Epoch 1240, training loss: 420.9090270996094 = 2.1423269510269165 + 50.0 * 8.375333786010742
Epoch 1240, val loss: 2.143740653991699
Epoch 1250, training loss: 421.04547119140625 = 2.1420481204986572 + 50.0 * 8.378067970275879
Epoch 1250, val loss: 2.1434550285339355
Epoch 1260, training loss: 420.9448547363281 = 2.1417137384414673 + 50.0 * 8.376062393188477
Epoch 1260, val loss: 2.1431570053100586
Epoch 1270, training loss: 420.8699951171875 = 2.141462206840515 + 50.0 * 8.374570846557617
Epoch 1270, val loss: 2.142873764038086
Epoch 1280, training loss: 420.79522705078125 = 2.141168713569641 + 50.0 * 8.37308120727539
Epoch 1280, val loss: 2.142584800720215
Epoch 1290, training loss: 420.8013610839844 = 2.140888810157776 + 50.0 * 8.373208999633789
Epoch 1290, val loss: 2.1423070430755615
Epoch 1300, training loss: 420.8531494140625 = 2.140617251396179 + 50.0 * 8.374250411987305
Epoch 1300, val loss: 2.142019271850586
Epoch 1310, training loss: 420.7909851074219 = 2.1403205394744873 + 50.0 * 8.373013496398926
Epoch 1310, val loss: 2.141730308532715
Epoch 1320, training loss: 420.7207336425781 = 2.14002525806427 + 50.0 * 8.371614456176758
Epoch 1320, val loss: 2.1414551734924316
Epoch 1330, training loss: 420.6676940917969 = 2.139766216278076 + 50.0 * 8.370558738708496
Epoch 1330, val loss: 2.1411759853363037
Epoch 1340, training loss: 420.6462097167969 = 2.13948655128479 + 50.0 * 8.370134353637695
Epoch 1340, val loss: 2.1409013271331787
Epoch 1350, training loss: 420.66357421875 = 2.1392202377319336 + 50.0 * 8.370487213134766
Epoch 1350, val loss: 2.140627384185791
Epoch 1360, training loss: 420.6700439453125 = 2.1389379501342773 + 50.0 * 8.370621681213379
Epoch 1360, val loss: 2.1403470039367676
Epoch 1370, training loss: 420.6009826660156 = 2.138659358024597 + 50.0 * 8.369246482849121
Epoch 1370, val loss: 2.1400699615478516
Epoch 1380, training loss: 420.5771789550781 = 2.138388991355896 + 50.0 * 8.368775367736816
Epoch 1380, val loss: 2.1397974491119385
Epoch 1390, training loss: 420.6067810058594 = 2.13811993598938 + 50.0 * 8.369373321533203
Epoch 1390, val loss: 2.139521598815918
Epoch 1400, training loss: 420.5226745605469 = 2.137837529182434 + 50.0 * 8.367696762084961
Epoch 1400, val loss: 2.139254570007324
Epoch 1410, training loss: 420.4869689941406 = 2.1375843286514282 + 50.0 * 8.366988182067871
Epoch 1410, val loss: 2.138983726501465
Epoch 1420, training loss: 420.4502258300781 = 2.1373156309127808 + 50.0 * 8.366257667541504
Epoch 1420, val loss: 2.1387176513671875
Epoch 1430, training loss: 420.56982421875 = 2.1370606422424316 + 50.0 * 8.36865520477295
Epoch 1430, val loss: 2.1384496688842773
Epoch 1440, training loss: 420.5907287597656 = 2.1367554664611816 + 50.0 * 8.36907958984375
Epoch 1440, val loss: 2.138174295425415
Epoch 1450, training loss: 420.4226989746094 = 2.136500597000122 + 50.0 * 8.365723609924316
Epoch 1450, val loss: 2.1379032135009766
Epoch 1460, training loss: 420.3682556152344 = 2.136237621307373 + 50.0 * 8.364640235900879
Epoch 1460, val loss: 2.137636184692383
Epoch 1470, training loss: 420.3536682128906 = 2.1359673738479614 + 50.0 * 8.364354133605957
Epoch 1470, val loss: 2.137378692626953
Epoch 1480, training loss: 420.35284423828125 = 2.1357141733169556 + 50.0 * 8.36434268951416
Epoch 1480, val loss: 2.1371212005615234
Epoch 1490, training loss: 420.4742736816406 = 2.1354455947875977 + 50.0 * 8.366776466369629
Epoch 1490, val loss: 2.136849880218506
Epoch 1500, training loss: 420.3387451171875 = 2.135183572769165 + 50.0 * 8.364070892333984
Epoch 1500, val loss: 2.136592149734497
Epoch 1510, training loss: 420.26507568359375 = 2.1349294185638428 + 50.0 * 8.362603187561035
Epoch 1510, val loss: 2.1363348960876465
Epoch 1520, training loss: 420.2400817871094 = 2.1346791982650757 + 50.0 * 8.36210823059082
Epoch 1520, val loss: 2.136080741882324
Epoch 1530, training loss: 420.2427062988281 = 2.1344302892684937 + 50.0 * 8.362165451049805
Epoch 1530, val loss: 2.1358280181884766
Epoch 1540, training loss: 420.56390380859375 = 2.1341769695281982 + 50.0 * 8.3685941696167
Epoch 1540, val loss: 2.1355690956115723
Epoch 1550, training loss: 420.1995849609375 = 2.1338928937911987 + 50.0 * 8.361313819885254
Epoch 1550, val loss: 2.135293960571289
Epoch 1560, training loss: 420.1907043457031 = 2.1336469650268555 + 50.0 * 8.361141204833984
Epoch 1560, val loss: 2.1350510120391846
Epoch 1570, training loss: 420.1524353027344 = 2.1334041357040405 + 50.0 * 8.360381126403809
Epoch 1570, val loss: 2.134803295135498
Epoch 1580, training loss: 420.1219787597656 = 2.133155107498169 + 50.0 * 8.359776496887207
Epoch 1580, val loss: 2.1345577239990234
Epoch 1590, training loss: 420.109375 = 2.132909059524536 + 50.0 * 8.359529495239258
Epoch 1590, val loss: 2.13431453704834
Epoch 1600, training loss: 420.2817687988281 = 2.132638454437256 + 50.0 * 8.362982749938965
Epoch 1600, val loss: 2.134070873260498
Epoch 1610, training loss: 420.14349365234375 = 2.1324328184127808 + 50.0 * 8.360220909118652
Epoch 1610, val loss: 2.13380765914917
Epoch 1620, training loss: 420.0558166503906 = 2.1321632862091064 + 50.0 * 8.35847282409668
Epoch 1620, val loss: 2.1335649490356445
Epoch 1630, training loss: 420.038818359375 = 2.1319286823272705 + 50.0 * 8.358138084411621
Epoch 1630, val loss: 2.133327007293701
Epoch 1640, training loss: 420.2003173828125 = 2.131692886352539 + 50.0 * 8.361372947692871
Epoch 1640, val loss: 2.1330788135528564
Epoch 1650, training loss: 420.06085205078125 = 2.1314265727996826 + 50.0 * 8.358588218688965
Epoch 1650, val loss: 2.1328253746032715
Epoch 1660, training loss: 420.0225524902344 = 2.1311893463134766 + 50.0 * 8.357827186584473
Epoch 1660, val loss: 2.1325888633728027
Epoch 1670, training loss: 419.98162841796875 = 2.130958080291748 + 50.0 * 8.357013702392578
Epoch 1670, val loss: 2.132350444793701
Epoch 1680, training loss: 419.9559326171875 = 2.1307170391082764 + 50.0 * 8.356504440307617
Epoch 1680, val loss: 2.132115364074707
Epoch 1690, training loss: 420.01385498046875 = 2.13047993183136 + 50.0 * 8.357666969299316
Epoch 1690, val loss: 2.1318819522857666
Epoch 1700, training loss: 419.9519958496094 = 2.1302297115325928 + 50.0 * 8.356435775756836
Epoch 1700, val loss: 2.1316285133361816
Epoch 1710, training loss: 419.92218017578125 = 2.1299943923950195 + 50.0 * 8.355843544006348
Epoch 1710, val loss: 2.131394624710083
Epoch 1720, training loss: 419.890380859375 = 2.1297614574432373 + 50.0 * 8.355212211608887
Epoch 1720, val loss: 2.131160259246826
Epoch 1730, training loss: 419.87261962890625 = 2.1295320987701416 + 50.0 * 8.35486125946045
Epoch 1730, val loss: 2.130930185317993
Epoch 1740, training loss: 419.911376953125 = 2.129292607307434 + 50.0 * 8.35564136505127
Epoch 1740, val loss: 2.1306989192962646
Epoch 1750, training loss: 419.9093017578125 = 2.1290496587753296 + 50.0 * 8.355605125427246
Epoch 1750, val loss: 2.1304526329040527
Epoch 1760, training loss: 419.8481750488281 = 2.128831386566162 + 50.0 * 8.354386329650879
Epoch 1760, val loss: 2.1302266120910645
Epoch 1770, training loss: 419.8141174316406 = 2.128593325614929 + 50.0 * 8.353710174560547
Epoch 1770, val loss: 2.129995346069336
Epoch 1780, training loss: 419.7939453125 = 2.1283682584762573 + 50.0 * 8.353311538696289
Epoch 1780, val loss: 2.129769802093506
Epoch 1790, training loss: 419.8263854980469 = 2.12814462184906 + 50.0 * 8.353964805603027
Epoch 1790, val loss: 2.129544258117676
Epoch 1800, training loss: 419.9036865234375 = 2.127898693084717 + 50.0 * 8.355515480041504
Epoch 1800, val loss: 2.129303455352783
Epoch 1810, training loss: 419.78271484375 = 2.1276702880859375 + 50.0 * 8.353100776672363
Epoch 1810, val loss: 2.1290760040283203
Epoch 1820, training loss: 419.7381896972656 = 2.127447009086609 + 50.0 * 8.352214813232422
Epoch 1820, val loss: 2.1288528442382812
Epoch 1830, training loss: 419.7226257324219 = 2.1272248029708862 + 50.0 * 8.351907730102539
Epoch 1830, val loss: 2.1286323070526123
Epoch 1840, training loss: 419.7761535644531 = 2.1269999742507935 + 50.0 * 8.352982521057129
Epoch 1840, val loss: 2.128411293029785
Epoch 1850, training loss: 419.7126770019531 = 2.126768112182617 + 50.0 * 8.351717948913574
Epoch 1850, val loss: 2.1281752586364746
Epoch 1860, training loss: 419.740966796875 = 2.1265512704849243 + 50.0 * 8.352288246154785
Epoch 1860, val loss: 2.127957820892334
Epoch 1870, training loss: 419.7308044433594 = 2.126317620277405 + 50.0 * 8.352089881896973
Epoch 1870, val loss: 2.1277289390563965
Epoch 1880, training loss: 419.666748046875 = 2.126097321510315 + 50.0 * 8.350812911987305
Epoch 1880, val loss: 2.1275100708007812
Epoch 1890, training loss: 419.65338134765625 = 2.125872254371643 + 50.0 * 8.350549697875977
Epoch 1890, val loss: 2.1272945404052734
Epoch 1900, training loss: 419.6463623046875 = 2.1256539821624756 + 50.0 * 8.350414276123047
Epoch 1900, val loss: 2.127082347869873
Epoch 1910, training loss: 419.6942443847656 = 2.125427722930908 + 50.0 * 8.3513765335083
Epoch 1910, val loss: 2.126863479614258
Epoch 1920, training loss: 419.6444091796875 = 2.1252185106277466 + 50.0 * 8.350383758544922
Epoch 1920, val loss: 2.1266355514526367
Epoch 1930, training loss: 419.6438903808594 = 2.1250146627426147 + 50.0 * 8.350378036499023
Epoch 1930, val loss: 2.1264231204986572
Epoch 1940, training loss: 419.6778869628906 = 2.124764084815979 + 50.0 * 8.351062774658203
Epoch 1940, val loss: 2.1262104511260986
Epoch 1950, training loss: 419.60467529296875 = 2.1245741844177246 + 50.0 * 8.349601745605469
Epoch 1950, val loss: 2.1259891986846924
Epoch 1960, training loss: 419.62457275390625 = 2.1243456602096558 + 50.0 * 8.350004196166992
Epoch 1960, val loss: 2.125781536102295
Epoch 1970, training loss: 419.5783996582031 = 2.1241390705108643 + 50.0 * 8.349084854125977
Epoch 1970, val loss: 2.1255621910095215
Epoch 1980, training loss: 419.53851318359375 = 2.1239280700683594 + 50.0 * 8.348291397094727
Epoch 1980, val loss: 2.125361442565918
Epoch 1990, training loss: 419.55169677734375 = 2.1237200498580933 + 50.0 * 8.348559379577637
Epoch 1990, val loss: 2.125152111053467
Epoch 2000, training loss: 419.57183837890625 = 2.1235156059265137 + 50.0 * 8.348966598510742
Epoch 2000, val loss: 2.124945640563965
Epoch 2010, training loss: 419.5618591308594 = 2.123299717903137 + 50.0 * 8.348771095275879
Epoch 2010, val loss: 2.124734401702881
Epoch 2020, training loss: 419.63134765625 = 2.123079299926758 + 50.0 * 8.350165367126465
Epoch 2020, val loss: 2.124511480331421
Epoch 2030, training loss: 419.51898193359375 = 2.1228607892990112 + 50.0 * 8.347922325134277
Epoch 2030, val loss: 2.124305009841919
Epoch 2040, training loss: 419.4901428222656 = 2.1226587295532227 + 50.0 * 8.347350120544434
Epoch 2040, val loss: 2.1240992546081543
Epoch 2050, training loss: 419.4595947265625 = 2.122458338737488 + 50.0 * 8.346742630004883
Epoch 2050, val loss: 2.123900890350342
Epoch 2060, training loss: 419.4444580078125 = 2.122257351875305 + 50.0 * 8.346444129943848
Epoch 2060, val loss: 2.123703956604004
Epoch 2070, training loss: 419.458740234375 = 2.122053384780884 + 50.0 * 8.346734046936035
Epoch 2070, val loss: 2.1235055923461914
Epoch 2080, training loss: 419.6329345703125 = 2.121849536895752 + 50.0 * 8.350221633911133
Epoch 2080, val loss: 2.123293161392212
Epoch 2090, training loss: 419.51824951171875 = 2.1216375827789307 + 50.0 * 8.347931861877441
Epoch 2090, val loss: 2.123082399368286
Epoch 2100, training loss: 419.4388122558594 = 2.121422052383423 + 50.0 * 8.34634780883789
Epoch 2100, val loss: 2.1228785514831543
Epoch 2110, training loss: 419.3927917480469 = 2.121227741241455 + 50.0 * 8.345431327819824
Epoch 2110, val loss: 2.122682571411133
Epoch 2120, training loss: 419.3909912109375 = 2.121027946472168 + 50.0 * 8.345398902893066
Epoch 2120, val loss: 2.1224863529205322
Epoch 2130, training loss: 419.40826416015625 = 2.1208266019821167 + 50.0 * 8.345748901367188
Epoch 2130, val loss: 2.1222925186157227
Epoch 2140, training loss: 419.4308166503906 = 2.1206271648406982 + 50.0 * 8.346203804016113
Epoch 2140, val loss: 2.122089385986328
Epoch 2150, training loss: 419.3592834472656 = 2.1204373836517334 + 50.0 * 8.34477710723877
Epoch 2150, val loss: 2.121898651123047
Epoch 2160, training loss: 419.6263122558594 = 2.1202340126037598 + 50.0 * 8.35012149810791
Epoch 2160, val loss: 2.121699333190918
Epoch 2170, training loss: 419.4414367675781 = 2.1199980974197388 + 50.0 * 8.346428871154785
Epoch 2170, val loss: 2.121474504470825
Epoch 2180, training loss: 419.3540344238281 = 2.1198312044143677 + 50.0 * 8.344683647155762
Epoch 2180, val loss: 2.121293544769287
Epoch 2190, training loss: 419.3131408691406 = 2.1196236610412598 + 50.0 * 8.343870162963867
Epoch 2190, val loss: 2.121096134185791
Epoch 2200, training loss: 419.3026123046875 = 2.1194396018981934 + 50.0 * 8.343663215637207
Epoch 2200, val loss: 2.120915174484253
Epoch 2210, training loss: 419.2911682128906 = 2.1192550659179688 + 50.0 * 8.343438148498535
Epoch 2210, val loss: 2.120728015899658
Epoch 2220, training loss: 419.2818298339844 = 2.1190673112869263 + 50.0 * 8.343255043029785
Epoch 2220, val loss: 2.1205434799194336
Epoch 2230, training loss: 419.2899475097656 = 2.1188820600509644 + 50.0 * 8.34342098236084
Epoch 2230, val loss: 2.1203622817993164
Epoch 2240, training loss: 419.5435485839844 = 2.1186782121658325 + 50.0 * 8.34849739074707
Epoch 2240, val loss: 2.120164394378662
Epoch 2250, training loss: 419.33502197265625 = 2.118453860282898 + 50.0 * 8.344330787658691
Epoch 2250, val loss: 2.1199493408203125
Epoch 2260, training loss: 419.27313232421875 = 2.1182886362075806 + 50.0 * 8.343096733093262
Epoch 2260, val loss: 2.119767189025879
Epoch 2270, training loss: 419.24847412109375 = 2.1180938482284546 + 50.0 * 8.342607498168945
Epoch 2270, val loss: 2.119581460952759
Epoch 2280, training loss: 419.23834228515625 = 2.11792254447937 + 50.0 * 8.342408180236816
Epoch 2280, val loss: 2.1194052696228027
Epoch 2290, training loss: 419.3354187011719 = 2.1177419424057007 + 50.0 * 8.344353675842285
Epoch 2290, val loss: 2.1192209720611572
Epoch 2300, training loss: 419.2243347167969 = 2.117535352706909 + 50.0 * 8.34213638305664
Epoch 2300, val loss: 2.1190223693847656
Epoch 2310, training loss: 419.2289733886719 = 2.1173489093780518 + 50.0 * 8.342232704162598
Epoch 2310, val loss: 2.118847370147705
Epoch 2320, training loss: 419.2023620605469 = 2.117173671722412 + 50.0 * 8.341703414916992
Epoch 2320, val loss: 2.118666887283325
Epoch 2330, training loss: 419.1834716796875 = 2.116995930671692 + 50.0 * 8.341329574584961
Epoch 2330, val loss: 2.1184916496276855
Epoch 2340, training loss: 419.18218994140625 = 2.116815209388733 + 50.0 * 8.341307640075684
Epoch 2340, val loss: 2.118316411972046
Epoch 2350, training loss: 419.2776794433594 = 2.1166226863861084 + 50.0 * 8.343221664428711
Epoch 2350, val loss: 2.118138313293457
Epoch 2360, training loss: 419.2106018066406 = 2.116454601287842 + 50.0 * 8.341882705688477
Epoch 2360, val loss: 2.1179428100585938
Epoch 2370, training loss: 419.1890563964844 = 2.1162540912628174 + 50.0 * 8.341456413269043
Epoch 2370, val loss: 2.117769241333008
Epoch 2380, training loss: 419.2197570800781 = 2.1160742044448853 + 50.0 * 8.342073440551758
Epoch 2380, val loss: 2.117582321166992
Epoch 2390, training loss: 419.14990234375 = 2.115894317626953 + 50.0 * 8.340680122375488
Epoch 2390, val loss: 2.117405414581299
Epoch 2400, training loss: 419.12432861328125 = 2.1157336235046387 + 50.0 * 8.340171813964844
Epoch 2400, val loss: 2.117237091064453
Epoch 2410, training loss: 419.1241760253906 = 2.1155558824539185 + 50.0 * 8.340171813964844
Epoch 2410, val loss: 2.1170687675476074
Epoch 2420, training loss: 419.2678527832031 = 2.1153852939605713 + 50.0 * 8.343049049377441
Epoch 2420, val loss: 2.1168909072875977
Epoch 2430, training loss: 419.1322937011719 = 2.1151950359344482 + 50.0 * 8.340341567993164
Epoch 2430, val loss: 2.116702079772949
Epoch 2440, training loss: 419.0904235839844 = 2.1150155067443848 + 50.0 * 8.339508056640625
Epoch 2440, val loss: 2.1165366172790527
Epoch 2450, training loss: 419.08709716796875 = 2.114846110343933 + 50.0 * 8.339445114135742
Epoch 2450, val loss: 2.116365432739258
Epoch 2460, training loss: 419.1695251464844 = 2.1146786212921143 + 50.0 * 8.341096878051758
Epoch 2460, val loss: 2.116194248199463
Epoch 2470, training loss: 419.082275390625 = 2.1144896745681763 + 50.0 * 8.33935546875
Epoch 2470, val loss: 2.1160130500793457
Epoch 2480, training loss: 419.0726013183594 = 2.114335298538208 + 50.0 * 8.339164733886719
Epoch 2480, val loss: 2.1158556938171387
Epoch 2490, training loss: 419.05572509765625 = 2.11415958404541 + 50.0 * 8.338830947875977
Epoch 2490, val loss: 2.115687131881714
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.39666666666666667
0.8601028761863364
=== training gcn model ===
Epoch 0, training loss: 531.3478393554688 = 2.2346231937408447 + 50.0 * 10.582263946533203
Epoch 0, val loss: 2.2328782081604004
Epoch 10, training loss: 531.31591796875 = 2.2178350687026978 + 50.0 * 10.581961631774902
Epoch 10, val loss: 2.2162930965423584
Epoch 20, training loss: 531.25146484375 = 2.203948974609375 + 50.0 * 10.580949783325195
Epoch 20, val loss: 2.2028961181640625
Epoch 30, training loss: 531.0411376953125 = 2.1966406106948853 + 50.0 * 10.576889991760254
Epoch 30, val loss: 2.196608543395996
Epoch 40, training loss: 530.17333984375 = 2.1964778900146484 + 50.0 * 10.559537887573242
Epoch 40, val loss: 2.196654796600342
Epoch 50, training loss: 526.8984375 = 2.198006510734558 + 50.0 * 10.494009017944336
Epoch 50, val loss: 2.1983351707458496
Epoch 60, training loss: 516.7902221679688 = 2.1989940404891968 + 50.0 * 10.291825294494629
Epoch 60, val loss: 2.1993894577026367
Epoch 70, training loss: 497.1824951171875 = 2.197616457939148 + 50.0 * 9.899697303771973
Epoch 70, val loss: 2.1981797218322754
Epoch 80, training loss: 484.73406982421875 = 2.197331190109253 + 50.0 * 9.650734901428223
Epoch 80, val loss: 2.1984620094299316
Epoch 90, training loss: 476.8583679199219 = 2.196110486984253 + 50.0 * 9.493245124816895
Epoch 90, val loss: 2.1973495483398438
Epoch 100, training loss: 468.5663757324219 = 2.1929845809936523 + 50.0 * 9.327467918395996
Epoch 100, val loss: 2.194042444229126
Epoch 110, training loss: 465.542724609375 = 2.188062071800232 + 50.0 * 9.267093658447266
Epoch 110, val loss: 2.1890950202941895
Epoch 120, training loss: 464.8800048828125 = 2.184202194213867 + 50.0 * 9.253915786743164
Epoch 120, val loss: 2.1851773262023926
Epoch 130, training loss: 464.0343933105469 = 2.1807920932769775 + 50.0 * 9.237071990966797
Epoch 130, val loss: 2.1817026138305664
Epoch 140, training loss: 463.13702392578125 = 2.1773362159729004 + 50.0 * 9.219193458557129
Epoch 140, val loss: 2.1782097816467285
Epoch 150, training loss: 462.025146484375 = 2.1751019954681396 + 50.0 * 9.197000503540039
Epoch 150, val loss: 2.1760168075561523
Epoch 160, training loss: 460.20697021484375 = 2.1739078760147095 + 50.0 * 9.160660743713379
Epoch 160, val loss: 2.174854040145874
Epoch 170, training loss: 456.875732421875 = 2.1740587949752808 + 50.0 * 9.094033241271973
Epoch 170, val loss: 2.1750826835632324
Epoch 180, training loss: 451.6779479980469 = 2.1760458946228027 + 50.0 * 8.99003791809082
Epoch 180, val loss: 2.177103042602539
Epoch 190, training loss: 447.5496826171875 = 2.1781551837921143 + 50.0 * 8.907430648803711
Epoch 190, val loss: 2.1792356967926025
Epoch 200, training loss: 444.1596374511719 = 2.1790411472320557 + 50.0 * 8.839612007141113
Epoch 200, val loss: 2.180166721343994
Epoch 210, training loss: 441.6268310546875 = 2.179368257522583 + 50.0 * 8.788949012756348
Epoch 210, val loss: 2.1805527210235596
Epoch 220, training loss: 440.2779846191406 = 2.1793471574783325 + 50.0 * 8.761972427368164
Epoch 220, val loss: 2.1805624961853027
Epoch 230, training loss: 439.4759826660156 = 2.178546667098999 + 50.0 * 8.745948791503906
Epoch 230, val loss: 2.179673194885254
Epoch 240, training loss: 438.77496337890625 = 2.177249312400818 + 50.0 * 8.731954574584961
Epoch 240, val loss: 2.1783766746520996
Epoch 250, training loss: 437.9683837890625 = 2.1761778593063354 + 50.0 * 8.71584415435791
Epoch 250, val loss: 2.1773338317871094
Epoch 260, training loss: 436.96234130859375 = 2.1756434440612793 + 50.0 * 8.695734024047852
Epoch 260, val loss: 2.176779270172119
Epoch 270, training loss: 435.74810791015625 = 2.175468921661377 + 50.0 * 8.671452522277832
Epoch 270, val loss: 2.1765480041503906
Epoch 280, training loss: 434.5227355957031 = 2.1752750873565674 + 50.0 * 8.646949768066406
Epoch 280, val loss: 2.1763975620269775
Epoch 290, training loss: 433.4573669433594 = 2.175053834915161 + 50.0 * 8.625646591186523
Epoch 290, val loss: 2.176156997680664
Epoch 300, training loss: 432.6545715332031 = 2.1745378971099854 + 50.0 * 8.609601020812988
Epoch 300, val loss: 2.175682544708252
Epoch 310, training loss: 431.9474182128906 = 2.1739903688430786 + 50.0 * 8.595468521118164
Epoch 310, val loss: 2.1750073432922363
Epoch 320, training loss: 431.4688415527344 = 2.173208475112915 + 50.0 * 8.585912704467773
Epoch 320, val loss: 2.1742360591888428
Epoch 330, training loss: 431.1061096191406 = 2.1723064184188843 + 50.0 * 8.578676223754883
Epoch 330, val loss: 2.173384189605713
Epoch 340, training loss: 430.83831787109375 = 2.171447992324829 + 50.0 * 8.57333755493164
Epoch 340, val loss: 2.1724789142608643
Epoch 350, training loss: 430.5156555175781 = 2.1705979108810425 + 50.0 * 8.566901206970215
Epoch 350, val loss: 2.1716978549957275
Epoch 360, training loss: 430.18475341796875 = 2.1698743104934692 + 50.0 * 8.560297966003418
Epoch 360, val loss: 2.17100191116333
Epoch 370, training loss: 429.8749694824219 = 2.1691596508026123 + 50.0 * 8.554116249084473
Epoch 370, val loss: 2.170382499694824
Epoch 380, training loss: 429.4847717285156 = 2.1685454845428467 + 50.0 * 8.546324729919434
Epoch 380, val loss: 2.169783115386963
Epoch 390, training loss: 429.0931396484375 = 2.1679974794387817 + 50.0 * 8.53850269317627
Epoch 390, val loss: 2.169200897216797
Epoch 400, training loss: 428.7359619140625 = 2.1675251722335815 + 50.0 * 8.53136920928955
Epoch 400, val loss: 2.1687283515930176
Epoch 410, training loss: 428.4324035644531 = 2.167088270187378 + 50.0 * 8.525306701660156
Epoch 410, val loss: 2.1682286262512207
Epoch 420, training loss: 428.0657043457031 = 2.1664865016937256 + 50.0 * 8.517984390258789
Epoch 420, val loss: 2.167731761932373
Epoch 430, training loss: 427.7394104003906 = 2.166053056716919 + 50.0 * 8.511466979980469
Epoch 430, val loss: 2.167275905609131
Epoch 440, training loss: 427.45782470703125 = 2.1656317710876465 + 50.0 * 8.505844116210938
Epoch 440, val loss: 2.1668331623077393
Epoch 450, training loss: 427.14117431640625 = 2.1650935411453247 + 50.0 * 8.499521255493164
Epoch 450, val loss: 2.1664061546325684
Epoch 460, training loss: 426.8267822265625 = 2.1646249294281006 + 50.0 * 8.493243217468262
Epoch 460, val loss: 2.1659555435180664
Epoch 470, training loss: 426.63653564453125 = 2.1641160249710083 + 50.0 * 8.489448547363281
Epoch 470, val loss: 2.1654648780822754
Epoch 480, training loss: 426.39263916015625 = 2.1636295318603516 + 50.0 * 8.484580039978027
Epoch 480, val loss: 2.1649575233459473
Epoch 490, training loss: 426.2027587890625 = 2.1631442308425903 + 50.0 * 8.480792045593262
Epoch 490, val loss: 2.1644749641418457
Epoch 500, training loss: 426.0384216308594 = 2.162644624710083 + 50.0 * 8.47751522064209
Epoch 500, val loss: 2.16402006149292
Epoch 510, training loss: 425.852294921875 = 2.1621793508529663 + 50.0 * 8.47380256652832
Epoch 510, val loss: 2.163578510284424
Epoch 520, training loss: 425.6861877441406 = 2.161768913269043 + 50.0 * 8.470488548278809
Epoch 520, val loss: 2.1631827354431152
Epoch 530, training loss: 425.5874938964844 = 2.16136634349823 + 50.0 * 8.468522071838379
Epoch 530, val loss: 2.162806510925293
Epoch 540, training loss: 425.4039611816406 = 2.1610080003738403 + 50.0 * 8.464859008789062
Epoch 540, val loss: 2.1624300479888916
Epoch 550, training loss: 425.22601318359375 = 2.1606298685073853 + 50.0 * 8.461307525634766
Epoch 550, val loss: 2.162079095840454
Epoch 560, training loss: 425.09649658203125 = 2.160247564315796 + 50.0 * 8.458724975585938
Epoch 560, val loss: 2.161736011505127
Epoch 570, training loss: 424.9472961425781 = 2.1598925590515137 + 50.0 * 8.455748558044434
Epoch 570, val loss: 2.161343574523926
Epoch 580, training loss: 424.8531799316406 = 2.159492254257202 + 50.0 * 8.453873634338379
Epoch 580, val loss: 2.1609904766082764
Epoch 590, training loss: 424.7120666503906 = 2.159140706062317 + 50.0 * 8.451058387756348
Epoch 590, val loss: 2.16064453125
Epoch 600, training loss: 424.59027099609375 = 2.1588045358657837 + 50.0 * 8.448629379272461
Epoch 600, val loss: 2.1602864265441895
Epoch 610, training loss: 424.64215087890625 = 2.1585099697113037 + 50.0 * 8.44967269897461
Epoch 610, val loss: 2.159897804260254
Epoch 620, training loss: 424.3938903808594 = 2.158034324645996 + 50.0 * 8.444717407226562
Epoch 620, val loss: 2.159508228302002
Epoch 630, training loss: 424.310546875 = 2.1576956510543823 + 50.0 * 8.4430570602417
Epoch 630, val loss: 2.1591625213623047
Epoch 640, training loss: 424.20684814453125 = 2.1573212146759033 + 50.0 * 8.440990447998047
Epoch 640, val loss: 2.1588134765625
Epoch 650, training loss: 424.10919189453125 = 2.1569554805755615 + 50.0 * 8.439044952392578
Epoch 650, val loss: 2.158458709716797
Epoch 660, training loss: 424.0437316894531 = 2.1566011905670166 + 50.0 * 8.437742233276367
Epoch 660, val loss: 2.158095598220825
Epoch 670, training loss: 424.014892578125 = 2.1561492681503296 + 50.0 * 8.437174797058105
Epoch 670, val loss: 2.15775203704834
Epoch 680, training loss: 423.86187744140625 = 2.15583336353302 + 50.0 * 8.434121131896973
Epoch 680, val loss: 2.157381057739258
Epoch 690, training loss: 423.7712707519531 = 2.1554951667785645 + 50.0 * 8.432315826416016
Epoch 690, val loss: 2.157043933868408
Epoch 700, training loss: 423.6838684082031 = 2.1551417112350464 + 50.0 * 8.430574417114258
Epoch 700, val loss: 2.156698226928711
Epoch 710, training loss: 423.70635986328125 = 2.15475594997406 + 50.0 * 8.431032180786133
Epoch 710, val loss: 2.1563668251037598
Epoch 720, training loss: 423.66217041015625 = 2.154482126235962 + 50.0 * 8.430153846740723
Epoch 720, val loss: 2.1559946537017822
Epoch 730, training loss: 423.4913330078125 = 2.1541131734848022 + 50.0 * 8.42674446105957
Epoch 730, val loss: 2.155646324157715
Epoch 740, training loss: 423.3906555175781 = 2.153749704360962 + 50.0 * 8.424737930297852
Epoch 740, val loss: 2.1553025245666504
Epoch 750, training loss: 423.3235778808594 = 2.1533846855163574 + 50.0 * 8.4234037399292
Epoch 750, val loss: 2.1549720764160156
Epoch 760, training loss: 423.3248596191406 = 2.1530455350875854 + 50.0 * 8.423436164855957
Epoch 760, val loss: 2.1546473503112793
Epoch 770, training loss: 423.267578125 = 2.1527140140533447 + 50.0 * 8.422297477722168
Epoch 770, val loss: 2.154280662536621
Epoch 780, training loss: 423.1457214355469 = 2.152358055114746 + 50.0 * 8.419867515563965
Epoch 780, val loss: 2.1539416313171387
Epoch 790, training loss: 423.0630798339844 = 2.152012586593628 + 50.0 * 8.418221473693848
Epoch 790, val loss: 2.15360164642334
Epoch 800, training loss: 422.9949035644531 = 2.151671290397644 + 50.0 * 8.416864395141602
Epoch 800, val loss: 2.1532702445983887
Epoch 810, training loss: 422.9416809082031 = 2.151335597038269 + 50.0 * 8.415806770324707
Epoch 810, val loss: 2.152932643890381
Epoch 820, training loss: 422.9565734863281 = 2.1509463787078857 + 50.0 * 8.416112899780273
Epoch 820, val loss: 2.1525943279266357
Epoch 830, training loss: 422.842529296875 = 2.150607943534851 + 50.0 * 8.413838386535645
Epoch 830, val loss: 2.1522467136383057
Epoch 840, training loss: 422.76385498046875 = 2.150261640548706 + 50.0 * 8.412271499633789
Epoch 840, val loss: 2.1519110202789307
Epoch 850, training loss: 422.7914123535156 = 2.1498918533325195 + 50.0 * 8.412830352783203
Epoch 850, val loss: 2.1515769958496094
Epoch 860, training loss: 422.65972900390625 = 2.1496163606643677 + 50.0 * 8.410202026367188
Epoch 860, val loss: 2.1512231826782227
Epoch 870, training loss: 422.58062744140625 = 2.1492578983306885 + 50.0 * 8.4086275100708
Epoch 870, val loss: 2.150895118713379
Epoch 880, training loss: 422.54296875 = 2.1489821672439575 + 50.0 * 8.407879829406738
Epoch 880, val loss: 2.1505508422851562
Epoch 890, training loss: 422.4638671875 = 2.148552656173706 + 50.0 * 8.406306266784668
Epoch 890, val loss: 2.150242805480957
Epoch 900, training loss: 422.4431457519531 = 2.148281693458557 + 50.0 * 8.40589714050293
Epoch 900, val loss: 2.149883270263672
Epoch 910, training loss: 422.33087158203125 = 2.147913932800293 + 50.0 * 8.403658866882324
Epoch 910, val loss: 2.1495442390441895
Epoch 920, training loss: 422.2713623046875 = 2.147577404975891 + 50.0 * 8.402475357055664
Epoch 920, val loss: 2.1492226123809814
Epoch 930, training loss: 422.3165588378906 = 2.147231340408325 + 50.0 * 8.403387069702148
Epoch 930, val loss: 2.1488900184631348
Epoch 940, training loss: 422.17816162109375 = 2.1469180583953857 + 50.0 * 8.400625228881836
Epoch 940, val loss: 2.148566246032715
Epoch 950, training loss: 422.0932922363281 = 2.1465903520584106 + 50.0 * 8.398934364318848
Epoch 950, val loss: 2.148240089416504
Epoch 960, training loss: 422.0903625488281 = 2.1462565660476685 + 50.0 * 8.398881912231445
Epoch 960, val loss: 2.147926092147827
Epoch 970, training loss: 422.21722412109375 = 2.1458706855773926 + 50.0 * 8.401427268981934
Epoch 970, val loss: 2.1475656032562256
Epoch 980, training loss: 421.94781494140625 = 2.1455544233322144 + 50.0 * 8.396044731140137
Epoch 980, val loss: 2.1472246646881104
Epoch 990, training loss: 421.8822021484375 = 2.1452311277389526 + 50.0 * 8.394739151000977
Epoch 990, val loss: 2.1469063758850098
Epoch 1000, training loss: 421.8271179199219 = 2.144911050796509 + 50.0 * 8.393644332885742
Epoch 1000, val loss: 2.146587371826172
Epoch 1010, training loss: 421.7667236328125 = 2.1445958614349365 + 50.0 * 8.39244270324707
Epoch 1010, val loss: 2.1462719440460205
Epoch 1020, training loss: 421.716796875 = 2.144279360771179 + 50.0 * 8.391449928283691
Epoch 1020, val loss: 2.145956039428711
Epoch 1030, training loss: 421.6732482910156 = 2.143952488899231 + 50.0 * 8.390585899353027
Epoch 1030, val loss: 2.1456449031829834
Epoch 1040, training loss: 422.0528259277344 = 2.143526315689087 + 50.0 * 8.398185729980469
Epoch 1040, val loss: 2.1453042030334473
Epoch 1050, training loss: 421.693359375 = 2.143209934234619 + 50.0 * 8.391002655029297
Epoch 1050, val loss: 2.144965648651123
Epoch 1060, training loss: 421.5806579589844 = 2.1429141759872437 + 50.0 * 8.388754844665527
Epoch 1060, val loss: 2.144641637802124
Epoch 1070, training loss: 421.51348876953125 = 2.142609119415283 + 50.0 * 8.387417793273926
Epoch 1070, val loss: 2.1443190574645996
Epoch 1080, training loss: 421.4630432128906 = 2.1423016786575317 + 50.0 * 8.386414527893066
Epoch 1080, val loss: 2.1440064907073975
Epoch 1090, training loss: 421.4195861816406 = 2.141996145248413 + 50.0 * 8.385551452636719
Epoch 1090, val loss: 2.143693685531616
Epoch 1100, training loss: 421.3812561035156 = 2.1416871547698975 + 50.0 * 8.384791374206543
Epoch 1100, val loss: 2.1433773040771484
Epoch 1110, training loss: 421.4619140625 = 2.1413843631744385 + 50.0 * 8.3864107131958
Epoch 1110, val loss: 2.143056631088257
Epoch 1120, training loss: 421.3797607421875 = 2.141006588935852 + 50.0 * 8.384775161743164
Epoch 1120, val loss: 2.1427128314971924
Epoch 1130, training loss: 421.29278564453125 = 2.1407188177108765 + 50.0 * 8.383041381835938
Epoch 1130, val loss: 2.1423940658569336
Epoch 1140, training loss: 421.2366027832031 = 2.1403802633285522 + 50.0 * 8.381924629211426
Epoch 1140, val loss: 2.142075538635254
Epoch 1150, training loss: 421.35211181640625 = 2.1401004791259766 + 50.0 * 8.38424015045166
Epoch 1150, val loss: 2.1417508125305176
Epoch 1160, training loss: 421.2249755859375 = 2.13971483707428 + 50.0 * 8.381705284118652
Epoch 1160, val loss: 2.1414356231689453
Epoch 1170, training loss: 421.136962890625 = 2.1394153833389282 + 50.0 * 8.379951477050781
Epoch 1170, val loss: 2.1411185264587402
Epoch 1180, training loss: 421.09808349609375 = 2.1391063928604126 + 50.0 * 8.379179954528809
Epoch 1180, val loss: 2.140805721282959
Epoch 1190, training loss: 421.0611572265625 = 2.1387935876846313 + 50.0 * 8.378447532653809
Epoch 1190, val loss: 2.1405036449432373
Epoch 1200, training loss: 421.0274963378906 = 2.1384938955307007 + 50.0 * 8.377779960632324
Epoch 1200, val loss: 2.140200614929199
Epoch 1210, training loss: 421.0633850097656 = 2.138172745704651 + 50.0 * 8.378503799438477
Epoch 1210, val loss: 2.1399011611938477
Epoch 1220, training loss: 420.9801330566406 = 2.137866735458374 + 50.0 * 8.376845359802246
Epoch 1220, val loss: 2.1395626068115234
Epoch 1230, training loss: 420.95538330078125 = 2.13753604888916 + 50.0 * 8.376357078552246
Epoch 1230, val loss: 2.1392698287963867
Epoch 1240, training loss: 420.9183349609375 = 2.1372334957122803 + 50.0 * 8.375621795654297
Epoch 1240, val loss: 2.1389667987823486
Epoch 1250, training loss: 421.0357971191406 = 2.1368786096572876 + 50.0 * 8.377978324890137
Epoch 1250, val loss: 2.138648271560669
Epoch 1260, training loss: 420.8456115722656 = 2.1366440057754517 + 50.0 * 8.374178886413574
Epoch 1260, val loss: 2.1383488178253174
Epoch 1270, training loss: 420.80096435546875 = 2.136342167854309 + 50.0 * 8.373291969299316
Epoch 1270, val loss: 2.1380414962768555
Epoch 1280, training loss: 420.762939453125 = 2.136034846305847 + 50.0 * 8.372537612915039
Epoch 1280, val loss: 2.1377501487731934
Epoch 1290, training loss: 420.7395324707031 = 2.135751247406006 + 50.0 * 8.372076034545898
Epoch 1290, val loss: 2.1374495029449463
Epoch 1300, training loss: 420.7460021972656 = 2.135464072227478 + 50.0 * 8.372210502624512
Epoch 1300, val loss: 2.137148141860962
Epoch 1310, training loss: 420.6651611328125 = 2.1351360082626343 + 50.0 * 8.370600700378418
Epoch 1310, val loss: 2.136868476867676
Epoch 1320, training loss: 420.6624450683594 = 2.134855031967163 + 50.0 * 8.370552062988281
Epoch 1320, val loss: 2.1365718841552734
Epoch 1330, training loss: 420.6478271484375 = 2.134523034095764 + 50.0 * 8.37026596069336
Epoch 1330, val loss: 2.1362619400024414
Epoch 1340, training loss: 420.5832214355469 = 2.1342450380325317 + 50.0 * 8.368979454040527
Epoch 1340, val loss: 2.1359572410583496
Epoch 1350, training loss: 420.5379333496094 = 2.133935332298279 + 50.0 * 8.368080139160156
Epoch 1350, val loss: 2.135667562484741
Epoch 1360, training loss: 420.5013427734375 = 2.133641481399536 + 50.0 * 8.367354393005371
Epoch 1360, val loss: 2.135385513305664
Epoch 1370, training loss: 420.6957702636719 = 2.1332976818084717 + 50.0 * 8.371249198913574
Epoch 1370, val loss: 2.1350855827331543
Epoch 1380, training loss: 420.4875183105469 = 2.133079767227173 + 50.0 * 8.367088317871094
Epoch 1380, val loss: 2.1347739696502686
Epoch 1390, training loss: 420.4043273925781 = 2.132745623588562 + 50.0 * 8.365431785583496
Epoch 1390, val loss: 2.1344776153564453
Epoch 1400, training loss: 420.3758544921875 = 2.132461428642273 + 50.0 * 8.3648681640625
Epoch 1400, val loss: 2.134202480316162
Epoch 1410, training loss: 420.34564208984375 = 2.1321797370910645 + 50.0 * 8.364269256591797
Epoch 1410, val loss: 2.1339197158813477
Epoch 1420, training loss: 420.3853454589844 = 2.131861090660095 + 50.0 * 8.365069389343262
Epoch 1420, val loss: 2.1336517333984375
Epoch 1430, training loss: 420.33709716796875 = 2.1315879821777344 + 50.0 * 8.364109992980957
Epoch 1430, val loss: 2.133297920227051
Epoch 1440, training loss: 420.307373046875 = 2.1312588453292847 + 50.0 * 8.36352252960205
Epoch 1440, val loss: 2.1330301761627197
Epoch 1450, training loss: 420.2384033203125 = 2.1309932470321655 + 50.0 * 8.36214828491211
Epoch 1450, val loss: 2.1327476501464844
Epoch 1460, training loss: 420.2117004394531 = 2.1307178735733032 + 50.0 * 8.36161994934082
Epoch 1460, val loss: 2.1324615478515625
Epoch 1470, training loss: 420.18743896484375 = 2.1304261684417725 + 50.0 * 8.361140251159668
Epoch 1470, val loss: 2.13218355178833
Epoch 1480, training loss: 420.2940979003906 = 2.1301090717315674 + 50.0 * 8.363280296325684
Epoch 1480, val loss: 2.131904363632202
Epoch 1490, training loss: 420.1629638671875 = 2.129851818084717 + 50.0 * 8.360662460327148
Epoch 1490, val loss: 2.131600856781006
Epoch 1500, training loss: 420.1219482421875 = 2.1295533180236816 + 50.0 * 8.359848022460938
Epoch 1500, val loss: 2.131321907043457
Epoch 1510, training loss: 420.098388671875 = 2.1292779445648193 + 50.0 * 8.359382629394531
Epoch 1510, val loss: 2.1310510635375977
Epoch 1520, training loss: 420.2192687988281 = 2.128962516784668 + 50.0 * 8.36180591583252
Epoch 1520, val loss: 2.130770206451416
Epoch 1530, training loss: 420.0849304199219 = 2.1287178993225098 + 50.0 * 8.359124183654785
Epoch 1530, val loss: 2.130474090576172
Epoch 1540, training loss: 420.01739501953125 = 2.1284161806106567 + 50.0 * 8.357779502868652
Epoch 1540, val loss: 2.130197763442993
Epoch 1550, training loss: 419.9827575683594 = 2.1281638145446777 + 50.0 * 8.357091903686523
Epoch 1550, val loss: 2.1299304962158203
Epoch 1560, training loss: 419.9589538574219 = 2.1278799772262573 + 50.0 * 8.356621742248535
Epoch 1560, val loss: 2.1296632289886475
Epoch 1570, training loss: 420.00775146484375 = 2.127642273902893 + 50.0 * 8.3576021194458
Epoch 1570, val loss: 2.129384994506836
Epoch 1580, training loss: 420.08428955078125 = 2.1273300647735596 + 50.0 * 8.359139442443848
Epoch 1580, val loss: 2.1290969848632812
Epoch 1590, training loss: 419.92340087890625 = 2.1270049810409546 + 50.0 * 8.355927467346191
Epoch 1590, val loss: 2.128798246383667
Epoch 1600, training loss: 419.8838195800781 = 2.1267446279525757 + 50.0 * 8.355141639709473
Epoch 1600, val loss: 2.1285367012023926
Epoch 1610, training loss: 419.8436279296875 = 2.1264703273773193 + 50.0 * 8.35434341430664
Epoch 1610, val loss: 2.128265857696533
Epoch 1620, training loss: 419.8197937011719 = 2.1262121200561523 + 50.0 * 8.35387134552002
Epoch 1620, val loss: 2.1280062198638916
Epoch 1630, training loss: 419.79705810546875 = 2.1259363889694214 + 50.0 * 8.353422164916992
Epoch 1630, val loss: 2.127744674682617
Epoch 1640, training loss: 419.7806701660156 = 2.1256725788116455 + 50.0 * 8.353099822998047
Epoch 1640, val loss: 2.1274771690368652
Epoch 1650, training loss: 420.05865478515625 = 2.125419855117798 + 50.0 * 8.358664512634277
Epoch 1650, val loss: 2.1271920204162598
Epoch 1660, training loss: 419.81982421875 = 2.125069737434387 + 50.0 * 8.35389518737793
Epoch 1660, val loss: 2.126899242401123
Epoch 1670, training loss: 419.7507019042969 = 2.124787926673889 + 50.0 * 8.352518081665039
Epoch 1670, val loss: 2.1266260147094727
Epoch 1680, training loss: 419.6954650878906 = 2.1245373487472534 + 50.0 * 8.351418495178223
Epoch 1680, val loss: 2.126366376876831
Epoch 1690, training loss: 419.6782531738281 = 2.124281406402588 + 50.0 * 8.351079940795898
Epoch 1690, val loss: 2.1261086463928223
Epoch 1700, training loss: 419.66156005859375 = 2.1240170001983643 + 50.0 * 8.350750923156738
Epoch 1700, val loss: 2.1258511543273926
Epoch 1710, training loss: 419.75299072265625 = 2.1237608194351196 + 50.0 * 8.352584838867188
Epoch 1710, val loss: 2.1255908012390137
Epoch 1720, training loss: 419.6705017089844 = 2.123500108718872 + 50.0 * 8.350939750671387
Epoch 1720, val loss: 2.1252918243408203
Epoch 1730, training loss: 419.62738037109375 = 2.1232045888900757 + 50.0 * 8.350083351135254
Epoch 1730, val loss: 2.125034809112549
Epoch 1740, training loss: 419.5958251953125 = 2.1229071617126465 + 50.0 * 8.349458694458008
Epoch 1740, val loss: 2.124774932861328
Epoch 1750, training loss: 419.57159423828125 = 2.1226446628570557 + 50.0 * 8.348978996276855
Epoch 1750, val loss: 2.1245222091674805
Epoch 1760, training loss: 419.5519104003906 = 2.1223868131637573 + 50.0 * 8.348590850830078
Epoch 1760, val loss: 2.1242666244506836
Epoch 1770, training loss: 419.60479736328125 = 2.1221014261245728 + 50.0 * 8.349654197692871
Epoch 1770, val loss: 2.124016284942627
Epoch 1780, training loss: 419.5145568847656 = 2.1218756437301636 + 50.0 * 8.347853660583496
Epoch 1780, val loss: 2.1237244606018066
Epoch 1790, training loss: 419.6069641113281 = 2.1216399669647217 + 50.0 * 8.349706649780273
Epoch 1790, val loss: 2.1234655380249023
Epoch 1800, training loss: 419.50164794921875 = 2.1213040351867676 + 50.0 * 8.347606658935547
Epoch 1800, val loss: 2.1232051849365234
Epoch 1810, training loss: 419.4637145996094 = 2.1210756301879883 + 50.0 * 8.346853256225586
Epoch 1810, val loss: 2.1229450702667236
Epoch 1820, training loss: 419.4422607421875 = 2.1208053827285767 + 50.0 * 8.346428871154785
Epoch 1820, val loss: 2.1227006912231445
Epoch 1830, training loss: 419.42657470703125 = 2.1205588579177856 + 50.0 * 8.346120834350586
Epoch 1830, val loss: 2.122450113296509
Epoch 1840, training loss: 419.41729736328125 = 2.1203101873397827 + 50.0 * 8.345939636230469
Epoch 1840, val loss: 2.1222033500671387
Epoch 1850, training loss: 419.6335144042969 = 2.1200597286224365 + 50.0 * 8.350269317626953
Epoch 1850, val loss: 2.1219496726989746
Epoch 1860, training loss: 419.4689025878906 = 2.119755744934082 + 50.0 * 8.346982955932617
Epoch 1860, val loss: 2.1216540336608887
Epoch 1870, training loss: 419.37628173828125 = 2.1194928884506226 + 50.0 * 8.345135688781738
Epoch 1870, val loss: 2.1214118003845215
Epoch 1880, training loss: 419.34759521484375 = 2.119246482849121 + 50.0 * 8.34456729888916
Epoch 1880, val loss: 2.1211647987365723
Epoch 1890, training loss: 419.33551025390625 = 2.1190069913864136 + 50.0 * 8.344329833984375
Epoch 1890, val loss: 2.120920181274414
Epoch 1900, training loss: 419.3542785644531 = 2.118769407272339 + 50.0 * 8.344710350036621
Epoch 1900, val loss: 2.1206796169281006
Epoch 1910, training loss: 419.4132385253906 = 2.118507146835327 + 50.0 * 8.345894813537598
Epoch 1910, val loss: 2.120418071746826
Epoch 1920, training loss: 419.2950744628906 = 2.1182096004486084 + 50.0 * 8.343537330627441
Epoch 1920, val loss: 2.1201508045196533
Epoch 1930, training loss: 419.2880554199219 = 2.1179646253585815 + 50.0 * 8.343401908874512
Epoch 1930, val loss: 2.1199116706848145
Epoch 1940, training loss: 419.30230712890625 = 2.117698550224304 + 50.0 * 8.3436918258667
Epoch 1940, val loss: 2.119677782058716
Epoch 1950, training loss: 419.3229064941406 = 2.117418646812439 + 50.0 * 8.344109535217285
Epoch 1950, val loss: 2.1194095611572266
Epoch 1960, training loss: 419.25274658203125 = 2.1172308921813965 + 50.0 * 8.342710494995117
Epoch 1960, val loss: 2.119163751602173
Epoch 1970, training loss: 419.2283935546875 = 2.1169581413269043 + 50.0 * 8.342228889465332
Epoch 1970, val loss: 2.118917942047119
Epoch 1980, training loss: 419.2103271484375 = 2.1167324781417847 + 50.0 * 8.341872215270996
Epoch 1980, val loss: 2.1186890602111816
Epoch 1990, training loss: 419.1966857910156 = 2.116486072540283 + 50.0 * 8.341604232788086
Epoch 1990, val loss: 2.1184558868408203
Epoch 2000, training loss: 419.1886901855469 = 2.116254687309265 + 50.0 * 8.341448783874512
Epoch 2000, val loss: 2.118220329284668
Epoch 2010, training loss: 419.3240051269531 = 2.116006016731262 + 50.0 * 8.344160079956055
Epoch 2010, val loss: 2.117976188659668
Epoch 2020, training loss: 419.21356201171875 = 2.1157525777816772 + 50.0 * 8.34195613861084
Epoch 2020, val loss: 2.117729425430298
Epoch 2030, training loss: 419.1783752441406 = 2.115498185157776 + 50.0 * 8.341257095336914
Epoch 2030, val loss: 2.117487907409668
Epoch 2040, training loss: 419.3126220703125 = 2.1152650117874146 + 50.0 * 8.343947410583496
Epoch 2040, val loss: 2.1172308921813965
Epoch 2050, training loss: 419.1873474121094 = 2.115005135536194 + 50.0 * 8.341446876525879
Epoch 2050, val loss: 2.1169791221618652
Epoch 2060, training loss: 419.1351013183594 = 2.1147310733795166 + 50.0 * 8.340407371520996
Epoch 2060, val loss: 2.1167407035827637
Epoch 2070, training loss: 419.10552978515625 = 2.114516854286194 + 50.0 * 8.339820861816406
Epoch 2070, val loss: 2.1165125370025635
Epoch 2080, training loss: 419.0904541015625 = 2.1142951250076294 + 50.0 * 8.339523315429688
Epoch 2080, val loss: 2.1162900924682617
Epoch 2090, training loss: 419.0753173828125 = 2.1140650510787964 + 50.0 * 8.339224815368652
Epoch 2090, val loss: 2.1160695552825928
Epoch 2100, training loss: 419.06396484375 = 2.113840699195862 + 50.0 * 8.33900260925293
Epoch 2100, val loss: 2.1158437728881836
Epoch 2110, training loss: 419.1063537597656 = 2.1136492490768433 + 50.0 * 8.33985424041748
Epoch 2110, val loss: 2.115605354309082
Epoch 2120, training loss: 419.113037109375 = 2.113368034362793 + 50.0 * 8.339993476867676
Epoch 2120, val loss: 2.1153762340545654
Epoch 2130, training loss: 419.0663146972656 = 2.113121747970581 + 50.0 * 8.33906364440918
Epoch 2130, val loss: 2.1150965690612793
Epoch 2140, training loss: 419.04010009765625 = 2.1128289699554443 + 50.0 * 8.338545799255371
Epoch 2140, val loss: 2.114877223968506
Epoch 2150, training loss: 419.0138854980469 = 2.1126283407211304 + 50.0 * 8.338025093078613
Epoch 2150, val loss: 2.1146669387817383
Epoch 2160, training loss: 419.00042724609375 = 2.112424850463867 + 50.0 * 8.337759971618652
Epoch 2160, val loss: 2.114445447921753
Epoch 2170, training loss: 418.98602294921875 = 2.1121875047683716 + 50.0 * 8.33747673034668
Epoch 2170, val loss: 2.114229440689087
Epoch 2180, training loss: 418.9765625 = 2.11197292804718 + 50.0 * 8.337291717529297
Epoch 2180, val loss: 2.114011764526367
Epoch 2190, training loss: 419.1383361816406 = 2.1117414236068726 + 50.0 * 8.340531349182129
Epoch 2190, val loss: 2.113785743713379
Epoch 2200, training loss: 419.0547790527344 = 2.111471652984619 + 50.0 * 8.338866233825684
Epoch 2200, val loss: 2.1135201454162598
Epoch 2210, training loss: 418.97930908203125 = 2.1112321615219116 + 50.0 * 8.337361335754395
Epoch 2210, val loss: 2.1132891178131104
Epoch 2220, training loss: 418.9464111328125 = 2.111028790473938 + 50.0 * 8.336708068847656
Epoch 2220, val loss: 2.113069534301758
Epoch 2230, training loss: 418.9286193847656 = 2.110804319381714 + 50.0 * 8.336356163024902
Epoch 2230, val loss: 2.112867832183838
Epoch 2240, training loss: 418.91796875 = 2.110606074333191 + 50.0 * 8.33614730834961
Epoch 2240, val loss: 2.1126608848571777
Epoch 2250, training loss: 419.1877746582031 = 2.110421657562256 + 50.0 * 8.341547012329102
Epoch 2250, val loss: 2.1124391555786133
Epoch 2260, training loss: 419.0102233886719 = 2.110090136528015 + 50.0 * 8.338003158569336
Epoch 2260, val loss: 2.112182140350342
Epoch 2270, training loss: 418.8863525390625 = 2.109897494316101 + 50.0 * 8.335529327392578
Epoch 2270, val loss: 2.1119723320007324
Epoch 2280, training loss: 418.88128662109375 = 2.109694719314575 + 50.0 * 8.335432052612305
Epoch 2280, val loss: 2.1117639541625977
Epoch 2290, training loss: 418.86651611328125 = 2.1094754934310913 + 50.0 * 8.3351411819458
Epoch 2290, val loss: 2.1115598678588867
Epoch 2300, training loss: 418.85357666015625 = 2.1092801094055176 + 50.0 * 8.334885597229004
Epoch 2300, val loss: 2.1113619804382324
Epoch 2310, training loss: 418.8853454589844 = 2.1090770959854126 + 50.0 * 8.335525512695312
Epoch 2310, val loss: 2.111154794692993
Epoch 2320, training loss: 418.847412109375 = 2.1088563203811646 + 50.0 * 8.334771156311035
Epoch 2320, val loss: 2.1109325885772705
Epoch 2330, training loss: 418.8244934082031 = 2.1086291074752808 + 50.0 * 8.334317207336426
Epoch 2330, val loss: 2.110720157623291
Epoch 2340, training loss: 418.8154602050781 = 2.1084216833114624 + 50.0 * 8.33414077758789
Epoch 2340, val loss: 2.1105215549468994
Epoch 2350, training loss: 418.8042297363281 = 2.1082171201705933 + 50.0 * 8.3339204788208
Epoch 2350, val loss: 2.1103200912475586
Epoch 2360, training loss: 418.827392578125 = 2.108008027076721 + 50.0 * 8.33438777923584
Epoch 2360, val loss: 2.110123872756958
Epoch 2370, training loss: 418.8459777832031 = 2.107760429382324 + 50.0 * 8.33476448059082
Epoch 2370, val loss: 2.1099019050598145
Epoch 2380, training loss: 418.8699951171875 = 2.1075326204299927 + 50.0 * 8.335248947143555
Epoch 2380, val loss: 2.109684705734253
Epoch 2390, training loss: 418.79498291015625 = 2.107372999191284 + 50.0 * 8.333752632141113
Epoch 2390, val loss: 2.109459400177002
Epoch 2400, training loss: 418.75921630859375 = 2.1071397066116333 + 50.0 * 8.333041191101074
Epoch 2400, val loss: 2.1092724800109863
Epoch 2410, training loss: 418.743896484375 = 2.1069546937942505 + 50.0 * 8.332738876342773
Epoch 2410, val loss: 2.1090800762176514
Epoch 2420, training loss: 418.7388610839844 = 2.106771469116211 + 50.0 * 8.3326416015625
Epoch 2420, val loss: 2.108889579772949
Epoch 2430, training loss: 418.8050231933594 = 2.1065627336502075 + 50.0 * 8.333969116210938
Epoch 2430, val loss: 2.108694553375244
Epoch 2440, training loss: 418.7633361816406 = 2.1063144207000732 + 50.0 * 8.33314037322998
Epoch 2440, val loss: 2.1084799766540527
Epoch 2450, training loss: 418.7141418457031 = 2.1061441898345947 + 50.0 * 8.332159996032715
Epoch 2450, val loss: 2.1082630157470703
Epoch 2460, training loss: 418.7108459472656 = 2.1059499979019165 + 50.0 * 8.332098007202148
Epoch 2460, val loss: 2.108074426651001
Epoch 2470, training loss: 418.7127990722656 = 2.1057448387145996 + 50.0 * 8.332140922546387
Epoch 2470, val loss: 2.1078851222991943
Epoch 2480, training loss: 418.7119445800781 = 2.1055524349212646 + 50.0 * 8.332127571105957
Epoch 2480, val loss: 2.1076953411102295
Epoch 2490, training loss: 418.66802978515625 = 2.1053680181503296 + 50.0 * 8.331253051757812
Epoch 2490, val loss: 2.1075034141540527
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.3973913043478261
0.8632181409838442
=== training gcn model ===
Epoch 0, training loss: 531.3045654296875 = 2.192121386528015 + 50.0 * 10.58224868774414
Epoch 0, val loss: 2.19053316116333
Epoch 10, training loss: 531.2803344726562 = 2.1891367435455322 + 50.0 * 10.581823348999023
Epoch 10, val loss: 2.1869688034057617
Epoch 20, training loss: 531.1929321289062 = 2.188024401664734 + 50.0 * 10.580097198486328
Epoch 20, val loss: 2.1856088638305664
Epoch 30, training loss: 530.8240356445312 = 2.1876367330551147 + 50.0 * 10.572728157043457
Epoch 30, val loss: 2.185155153274536
Epoch 40, training loss: 529.4357299804688 = 2.1876635551452637 + 50.0 * 10.544960975646973
Epoch 40, val loss: 2.1853089332580566
Epoch 50, training loss: 525.4591064453125 = 2.1881415843963623 + 50.0 * 10.465418815612793
Epoch 50, val loss: 2.186054229736328
Epoch 60, training loss: 515.8778686523438 = 2.188856601715088 + 50.0 * 10.273780822753906
Epoch 60, val loss: 2.1871161460876465
Epoch 70, training loss: 497.69708251953125 = 2.1894391775131226 + 50.0 * 9.910152435302734
Epoch 70, val loss: 2.1881465911865234
Epoch 80, training loss: 485.0019226074219 = 2.1880011558532715 + 50.0 * 9.656278610229492
Epoch 80, val loss: 2.1867659091949463
Epoch 90, training loss: 480.4688415527344 = 2.185835123062134 + 50.0 * 9.56566047668457
Epoch 90, val loss: 2.1846818923950195
Epoch 100, training loss: 474.4537353515625 = 2.184242844581604 + 50.0 * 9.445389747619629
Epoch 100, val loss: 2.183058738708496
Epoch 110, training loss: 465.3448791503906 = 2.18368136882782 + 50.0 * 9.263223648071289
Epoch 110, val loss: 2.1825294494628906
Epoch 120, training loss: 456.8309631347656 = 2.1837096214294434 + 50.0 * 9.092945098876953
Epoch 120, val loss: 2.1826107501983643
Epoch 130, training loss: 451.5491027832031 = 2.18376624584198 + 50.0 * 8.987306594848633
Epoch 130, val loss: 2.182741165161133
Epoch 140, training loss: 447.85943603515625 = 2.1840386390686035 + 50.0 * 8.913507461547852
Epoch 140, val loss: 2.1829569339752197
Epoch 150, training loss: 445.7999572753906 = 2.184563636779785 + 50.0 * 8.872307777404785
Epoch 150, val loss: 2.183398723602295
Epoch 160, training loss: 444.713134765625 = 2.1848469972610474 + 50.0 * 8.850565910339355
Epoch 160, val loss: 2.1836071014404297
Epoch 170, training loss: 443.5532531738281 = 2.1845303773880005 + 50.0 * 8.827374458312988
Epoch 170, val loss: 2.1832222938537598
Epoch 180, training loss: 442.18792724609375 = 2.184154748916626 + 50.0 * 8.80007553100586
Epoch 180, val loss: 2.182769298553467
Epoch 190, training loss: 440.54443359375 = 2.184124708175659 + 50.0 * 8.767206192016602
Epoch 190, val loss: 2.1826348304748535
Epoch 200, training loss: 439.0389404296875 = 2.184099555015564 + 50.0 * 8.737096786499023
Epoch 200, val loss: 2.1824474334716797
Epoch 210, training loss: 437.7947082519531 = 2.1835074424743652 + 50.0 * 8.712224006652832
Epoch 210, val loss: 2.181697368621826
Epoch 220, training loss: 437.0024719238281 = 2.1822861433029175 + 50.0 * 8.696403503417969
Epoch 220, val loss: 2.1803956031799316
Epoch 230, training loss: 436.45184326171875 = 2.180496573448181 + 50.0 * 8.685426712036133
Epoch 230, val loss: 2.178652763366699
Epoch 240, training loss: 435.9942932128906 = 2.17851984500885 + 50.0 * 8.676315307617188
Epoch 240, val loss: 2.176793098449707
Epoch 250, training loss: 435.54119873046875 = 2.176901340484619 + 50.0 * 8.667285919189453
Epoch 250, val loss: 2.1752631664276123
Epoch 260, training loss: 435.0704040527344 = 2.175702214241028 + 50.0 * 8.657894134521484
Epoch 260, val loss: 2.1741342544555664
Epoch 270, training loss: 434.53192138671875 = 2.1748095750808716 + 50.0 * 8.64714241027832
Epoch 270, val loss: 2.1732661724090576
Epoch 280, training loss: 433.9042663574219 = 2.174106240272522 + 50.0 * 8.634603500366211
Epoch 280, val loss: 2.1726083755493164
Epoch 290, training loss: 433.2364807128906 = 2.1735053062438965 + 50.0 * 8.621259689331055
Epoch 290, val loss: 2.17207407951355
Epoch 300, training loss: 432.5880126953125 = 2.173016309738159 + 50.0 * 8.60830020904541
Epoch 300, val loss: 2.171616554260254
Epoch 310, training loss: 431.9877624511719 = 2.1726107597351074 + 50.0 * 8.59630298614502
Epoch 310, val loss: 2.1711764335632324
Epoch 320, training loss: 431.4491271972656 = 2.17214035987854 + 50.0 * 8.585539817810059
Epoch 320, val loss: 2.1707491874694824
Epoch 330, training loss: 430.873291015625 = 2.1717809438705444 + 50.0 * 8.574029922485352
Epoch 330, val loss: 2.170370101928711
Epoch 340, training loss: 430.402587890625 = 2.1713948249816895 + 50.0 * 8.564623832702637
Epoch 340, val loss: 2.170017719268799
Epoch 350, training loss: 429.94268798828125 = 2.1711037158966064 + 50.0 * 8.555431365966797
Epoch 350, val loss: 2.169625759124756
Epoch 360, training loss: 429.554443359375 = 2.1706782579421997 + 50.0 * 8.547675132751465
Epoch 360, val loss: 2.1692934036254883
Epoch 370, training loss: 429.1783142089844 = 2.170326352119446 + 50.0 * 8.540160179138184
Epoch 370, val loss: 2.168903350830078
Epoch 380, training loss: 428.8840026855469 = 2.1699347496032715 + 50.0 * 8.534281730651855
Epoch 380, val loss: 2.16853666305542
Epoch 390, training loss: 428.4578552246094 = 2.169539451599121 + 50.0 * 8.525766372680664
Epoch 390, val loss: 2.168128252029419
Epoch 400, training loss: 428.08966064453125 = 2.169133424758911 + 50.0 * 8.518410682678223
Epoch 400, val loss: 2.167738914489746
Epoch 410, training loss: 427.8805236816406 = 2.1688002347946167 + 50.0 * 8.51423454284668
Epoch 410, val loss: 2.1673121452331543
Epoch 420, training loss: 427.520751953125 = 2.1683307886123657 + 50.0 * 8.507048606872559
Epoch 420, val loss: 2.166897773742676
Epoch 430, training loss: 427.1800231933594 = 2.1678677797317505 + 50.0 * 8.500243186950684
Epoch 430, val loss: 2.166506767272949
Epoch 440, training loss: 426.8974609375 = 2.1674007177352905 + 50.0 * 8.494601249694824
Epoch 440, val loss: 2.1661157608032227
Epoch 450, training loss: 426.6760559082031 = 2.1669505834579468 + 50.0 * 8.490181922912598
Epoch 450, val loss: 2.1657023429870605
Epoch 460, training loss: 426.5302734375 = 2.1664915084838867 + 50.0 * 8.487275123596191
Epoch 460, val loss: 2.1652779579162598
Epoch 470, training loss: 426.3026428222656 = 2.166095018386841 + 50.0 * 8.482730865478516
Epoch 470, val loss: 2.164822578430176
Epoch 480, training loss: 426.1288146972656 = 2.1655824184417725 + 50.0 * 8.479264259338379
Epoch 480, val loss: 2.16440486907959
Epoch 490, training loss: 425.94085693359375 = 2.1651856899261475 + 50.0 * 8.475513458251953
Epoch 490, val loss: 2.1639552116394043
Epoch 500, training loss: 425.75262451171875 = 2.1647228002548218 + 50.0 * 8.471757888793945
Epoch 500, val loss: 2.1635546684265137
Epoch 510, training loss: 425.57611083984375 = 2.164292812347412 + 50.0 * 8.468235969543457
Epoch 510, val loss: 2.1631600856781006
Epoch 520, training loss: 425.37896728515625 = 2.163896918296814 + 50.0 * 8.464301109313965
Epoch 520, val loss: 2.1627354621887207
Epoch 530, training loss: 425.2243957519531 = 2.163436532020569 + 50.0 * 8.461219787597656
Epoch 530, val loss: 2.1623377799987793
Epoch 540, training loss: 425.0988464355469 = 2.1630752086639404 + 50.0 * 8.458715438842773
Epoch 540, val loss: 2.1619062423706055
Epoch 550, training loss: 424.9493103027344 = 2.1625380516052246 + 50.0 * 8.455735206604004
Epoch 550, val loss: 2.161484956741333
Epoch 560, training loss: 424.72869873046875 = 2.1621521711349487 + 50.0 * 8.45133113861084
Epoch 560, val loss: 2.161076068878174
Epoch 570, training loss: 424.58489990234375 = 2.1617398262023926 + 50.0 * 8.448463439941406
Epoch 570, val loss: 2.160663604736328
Epoch 580, training loss: 424.4889831542969 = 2.1612900495529175 + 50.0 * 8.446554183959961
Epoch 580, val loss: 2.1602931022644043
Epoch 590, training loss: 424.3794250488281 = 2.1609346866607666 + 50.0 * 8.444369316101074
Epoch 590, val loss: 2.1598191261291504
Epoch 600, training loss: 424.2303771972656 = 2.1604795455932617 + 50.0 * 8.441397666931152
Epoch 600, val loss: 2.159412145614624
Epoch 610, training loss: 424.1202697753906 = 2.160054564476013 + 50.0 * 8.439204216003418
Epoch 610, val loss: 2.1590096950531006
Epoch 620, training loss: 424.12762451171875 = 2.1596256494522095 + 50.0 * 8.439359664916992
Epoch 620, val loss: 2.158592700958252
Epoch 630, training loss: 423.9794616699219 = 2.1591566801071167 + 50.0 * 8.436406135559082
Epoch 630, val loss: 2.1581521034240723
Epoch 640, training loss: 423.8545227050781 = 2.1587451696395874 + 50.0 * 8.433915138244629
Epoch 640, val loss: 2.1577324867248535
Epoch 650, training loss: 423.77203369140625 = 2.1583006381988525 + 50.0 * 8.43227481842041
Epoch 650, val loss: 2.1573214530944824
Epoch 660, training loss: 423.71710205078125 = 2.157897472381592 + 50.0 * 8.431183815002441
Epoch 660, val loss: 2.1568808555603027
Epoch 670, training loss: 423.6367492675781 = 2.157450556755066 + 50.0 * 8.429586410522461
Epoch 670, val loss: 2.1564431190490723
Epoch 680, training loss: 423.55767822265625 = 2.156972885131836 + 50.0 * 8.428013801574707
Epoch 680, val loss: 2.1560230255126953
Epoch 690, training loss: 423.5019226074219 = 2.1565804481506348 + 50.0 * 8.42690658569336
Epoch 690, val loss: 2.155592441558838
Epoch 700, training loss: 423.4278259277344 = 2.156129837036133 + 50.0 * 8.425434112548828
Epoch 700, val loss: 2.155161142349243
Epoch 710, training loss: 423.3524169921875 = 2.15567147731781 + 50.0 * 8.423934936523438
Epoch 710, val loss: 2.1547536849975586
Epoch 720, training loss: 423.362060546875 = 2.1552300453186035 + 50.0 * 8.4241361618042
Epoch 720, val loss: 2.1543428897857666
Epoch 730, training loss: 423.2688903808594 = 2.1548227071762085 + 50.0 * 8.422281265258789
Epoch 730, val loss: 2.153878688812256
Epoch 740, training loss: 423.19873046875 = 2.154383897781372 + 50.0 * 8.420886993408203
Epoch 740, val loss: 2.153482437133789
Epoch 750, training loss: 423.2088928222656 = 2.1539947986602783 + 50.0 * 8.421097755432129
Epoch 750, val loss: 2.153062343597412
Epoch 760, training loss: 423.1327209472656 = 2.1535013914108276 + 50.0 * 8.419584274291992
Epoch 760, val loss: 2.152639627456665
Epoch 770, training loss: 423.04608154296875 = 2.1531319618225098 + 50.0 * 8.417859077453613
Epoch 770, val loss: 2.152233600616455
Epoch 780, training loss: 422.9695739746094 = 2.152705669403076 + 50.0 * 8.416337013244629
Epoch 780, val loss: 2.1518383026123047
Epoch 790, training loss: 422.9173583984375 = 2.152297854423523 + 50.0 * 8.415301322937012
Epoch 790, val loss: 2.151454448699951
Epoch 800, training loss: 423.0649108886719 = 2.1518077850341797 + 50.0 * 8.418262481689453
Epoch 800, val loss: 2.151078701019287
Epoch 810, training loss: 422.82171630859375 = 2.15149188041687 + 50.0 * 8.41340446472168
Epoch 810, val loss: 2.1506495475769043
Epoch 820, training loss: 422.7895812988281 = 2.1511226892471313 + 50.0 * 8.412769317626953
Epoch 820, val loss: 2.150249481201172
Epoch 830, training loss: 422.73297119140625 = 2.1507182121276855 + 50.0 * 8.41164493560791
Epoch 830, val loss: 2.149871826171875
Epoch 840, training loss: 423.001220703125 = 2.150365471839905 + 50.0 * 8.417016983032227
Epoch 840, val loss: 2.1494832038879395
Epoch 850, training loss: 422.65509033203125 = 2.1498501300811768 + 50.0 * 8.410104751586914
Epoch 850, val loss: 2.1490859985351562
Epoch 860, training loss: 422.6272888183594 = 2.149473190307617 + 50.0 * 8.40955638885498
Epoch 860, val loss: 2.148728847503662
Epoch 870, training loss: 422.5487365722656 = 2.149123430252075 + 50.0 * 8.407992362976074
Epoch 870, val loss: 2.1483564376831055
Epoch 880, training loss: 422.5076904296875 = 2.148751974105835 + 50.0 * 8.40717887878418
Epoch 880, val loss: 2.147989511489868
Epoch 890, training loss: 422.4753112792969 = 2.148369312286377 + 50.0 * 8.406538963317871
Epoch 890, val loss: 2.1476259231567383
Epoch 900, training loss: 422.47119140625 = 2.1479392051696777 + 50.0 * 8.406464576721191
Epoch 900, val loss: 2.14725923538208
Epoch 910, training loss: 422.42205810546875 = 2.1476058959960938 + 50.0 * 8.405488967895508
Epoch 910, val loss: 2.146888494491577
Epoch 920, training loss: 422.3439636230469 = 2.147249221801758 + 50.0 * 8.403934478759766
Epoch 920, val loss: 2.146533250808716
Epoch 930, training loss: 422.2843322753906 = 2.1468876600265503 + 50.0 * 8.402749061584473
Epoch 930, val loss: 2.1461896896362305
Epoch 940, training loss: 422.2615661621094 = 2.146506905555725 + 50.0 * 8.402300834655762
Epoch 940, val loss: 2.145867347717285
Epoch 950, training loss: 422.259765625 = 2.146108388900757 + 50.0 * 8.402273178100586
Epoch 950, val loss: 2.1454973220825195
Epoch 960, training loss: 422.2164001464844 = 2.1457667350769043 + 50.0 * 8.401412963867188
Epoch 960, val loss: 2.1451525688171387
Epoch 970, training loss: 422.114013671875 = 2.1454254388809204 + 50.0 * 8.399372100830078
Epoch 970, val loss: 2.144819736480713
Epoch 980, training loss: 422.11456298828125 = 2.1450449228286743 + 50.0 * 8.39939022064209
Epoch 980, val loss: 2.1444997787475586
Epoch 990, training loss: 422.0174255371094 = 2.144742250442505 + 50.0 * 8.397453308105469
Epoch 990, val loss: 2.14412260055542
Epoch 1000, training loss: 421.9691162109375 = 2.144387722015381 + 50.0 * 8.39649486541748
Epoch 1000, val loss: 2.1438279151916504
Epoch 1010, training loss: 421.928466796875 = 2.1440820693969727 + 50.0 * 8.3956880569458
Epoch 1010, val loss: 2.1434803009033203
Epoch 1020, training loss: 422.2166442871094 = 2.143754482269287 + 50.0 * 8.401457786560059
Epoch 1020, val loss: 2.1431517601013184
Epoch 1030, training loss: 421.8477783203125 = 2.14335298538208 + 50.0 * 8.394088745117188
Epoch 1030, val loss: 2.1427814960479736
Epoch 1040, training loss: 421.8153076171875 = 2.143001914024353 + 50.0 * 8.39344596862793
Epoch 1040, val loss: 2.1424918174743652
Epoch 1050, training loss: 421.74420166015625 = 2.1426849365234375 + 50.0 * 8.392029762268066
Epoch 1050, val loss: 2.1421873569488525
Epoch 1060, training loss: 421.702392578125 = 2.142384171485901 + 50.0 * 8.391200065612793
Epoch 1060, val loss: 2.1418731212615967
Epoch 1070, training loss: 421.6610107421875 = 2.1420488357543945 + 50.0 * 8.390378952026367
Epoch 1070, val loss: 2.1415624618530273
Epoch 1080, training loss: 421.74688720703125 = 2.141689896583557 + 50.0 * 8.392104148864746
Epoch 1080, val loss: 2.141252279281616
Epoch 1090, training loss: 421.7053527832031 = 2.141395688056946 + 50.0 * 8.391279220581055
Epoch 1090, val loss: 2.140913486480713
Epoch 1100, training loss: 421.54656982421875 = 2.141060709953308 + 50.0 * 8.388110160827637
Epoch 1100, val loss: 2.1405978202819824
Epoch 1110, training loss: 421.5202331542969 = 2.1407216787338257 + 50.0 * 8.387590408325195
Epoch 1110, val loss: 2.1402840614318848
Epoch 1120, training loss: 421.4750671386719 = 2.1404051780700684 + 50.0 * 8.386693000793457
Epoch 1120, val loss: 2.1399788856506348
Epoch 1130, training loss: 421.580078125 = 2.1400479078292847 + 50.0 * 8.388800621032715
Epoch 1130, val loss: 2.1396894454956055
Epoch 1140, training loss: 421.66351318359375 = 2.1397359371185303 + 50.0 * 8.390475273132324
Epoch 1140, val loss: 2.1392760276794434
Epoch 1150, training loss: 421.45001220703125 = 2.139391541481018 + 50.0 * 8.386212348937988
Epoch 1150, val loss: 2.1389667987823486
Epoch 1160, training loss: 421.3722839355469 = 2.1390799283981323 + 50.0 * 8.384664535522461
Epoch 1160, val loss: 2.138664484024048
Epoch 1170, training loss: 421.3253479003906 = 2.1387603282928467 + 50.0 * 8.383731842041016
Epoch 1170, val loss: 2.138368606567383
Epoch 1180, training loss: 421.286376953125 = 2.1384509801864624 + 50.0 * 8.38295841217041
Epoch 1180, val loss: 2.1380605697631836
Epoch 1190, training loss: 421.26080322265625 = 2.1381348371505737 + 50.0 * 8.382453918457031
Epoch 1190, val loss: 2.137753486633301
Epoch 1200, training loss: 421.4361267089844 = 2.1378127336502075 + 50.0 * 8.385966300964355
Epoch 1200, val loss: 2.137429714202881
Epoch 1210, training loss: 421.26104736328125 = 2.137449026107788 + 50.0 * 8.382472038269043
Epoch 1210, val loss: 2.1371219158172607
Epoch 1220, training loss: 421.17462158203125 = 2.1371690034866333 + 50.0 * 8.380748748779297
Epoch 1220, val loss: 2.136814594268799
Epoch 1230, training loss: 421.1458435058594 = 2.136862874031067 + 50.0 * 8.380179405212402
Epoch 1230, val loss: 2.1365067958831787
Epoch 1240, training loss: 421.1169128417969 = 2.1365543603897095 + 50.0 * 8.379607200622559
Epoch 1240, val loss: 2.136215925216675
Epoch 1250, training loss: 421.299560546875 = 2.136328935623169 + 50.0 * 8.383264541625977
Epoch 1250, val loss: 2.1358633041381836
Epoch 1260, training loss: 421.1681823730469 = 2.1358052492141724 + 50.0 * 8.380647659301758
Epoch 1260, val loss: 2.135587692260742
Epoch 1270, training loss: 421.04473876953125 = 2.1356098651885986 + 50.0 * 8.378182411193848
Epoch 1270, val loss: 2.1352779865264893
Epoch 1280, training loss: 421.0030517578125 = 2.1352986097335815 + 50.0 * 8.377355575561523
Epoch 1280, val loss: 2.1349756717681885
Epoch 1290, training loss: 420.9753723144531 = 2.1349786520004272 + 50.0 * 8.376808166503906
Epoch 1290, val loss: 2.1347060203552246
Epoch 1300, training loss: 420.9466857910156 = 2.1347068548202515 + 50.0 * 8.376239776611328
Epoch 1300, val loss: 2.134411573410034
Epoch 1310, training loss: 421.0010986328125 = 2.1343793869018555 + 50.0 * 8.377334594726562
Epoch 1310, val loss: 2.1341285705566406
Epoch 1320, training loss: 420.9246520996094 = 2.1340699195861816 + 50.0 * 8.375811576843262
Epoch 1320, val loss: 2.1337904930114746
Epoch 1330, training loss: 420.90826416015625 = 2.133764386177063 + 50.0 * 8.375490188598633
Epoch 1330, val loss: 2.133519172668457
Epoch 1340, training loss: 420.8409423828125 = 2.133492588996887 + 50.0 * 8.374149322509766
Epoch 1340, val loss: 2.1332297325134277
Epoch 1350, training loss: 420.8125305175781 = 2.1332147121429443 + 50.0 * 8.373586654663086
Epoch 1350, val loss: 2.1329476833343506
Epoch 1360, training loss: 420.8341064453125 = 2.132921576499939 + 50.0 * 8.3740234375
Epoch 1360, val loss: 2.132673740386963
Epoch 1370, training loss: 420.765869140625 = 2.1326133012771606 + 50.0 * 8.372665405273438
Epoch 1370, val loss: 2.132387638092041
Epoch 1380, training loss: 420.7420349121094 = 2.1323336362838745 + 50.0 * 8.372194290161133
Epoch 1380, val loss: 2.1321048736572266
Epoch 1390, training loss: 420.8914794921875 = 2.132057785987854 + 50.0 * 8.375188827514648
Epoch 1390, val loss: 2.131822109222412
Epoch 1400, training loss: 420.7465515136719 = 2.131765842437744 + 50.0 * 8.372295379638672
Epoch 1400, val loss: 2.131521701812744
Epoch 1410, training loss: 420.67156982421875 = 2.1314605474472046 + 50.0 * 8.37080192565918
Epoch 1410, val loss: 2.1312692165374756
Epoch 1420, training loss: 420.630859375 = 2.1311932802200317 + 50.0 * 8.369993209838867
Epoch 1420, val loss: 2.131011724472046
Epoch 1430, training loss: 420.6697998046875 = 2.1308759450912476 + 50.0 * 8.37077808380127
Epoch 1430, val loss: 2.130765199661255
Epoch 1440, training loss: 420.59185791015625 = 2.130599021911621 + 50.0 * 8.36922550201416
Epoch 1440, val loss: 2.1304426193237305
Epoch 1450, training loss: 420.555419921875 = 2.1303796768188477 + 50.0 * 8.368500709533691
Epoch 1450, val loss: 2.130185604095459
Epoch 1460, training loss: 420.523193359375 = 2.1300973892211914 + 50.0 * 8.3678617477417
Epoch 1460, val loss: 2.129931926727295
Epoch 1470, training loss: 420.50152587890625 = 2.1298476457595825 + 50.0 * 8.367433547973633
Epoch 1470, val loss: 2.1296842098236084
Epoch 1480, training loss: 420.54632568359375 = 2.129538655281067 + 50.0 * 8.368335723876953
Epoch 1480, val loss: 2.1294517517089844
Epoch 1490, training loss: 420.6343078613281 = 2.129278540611267 + 50.0 * 8.370100975036621
Epoch 1490, val loss: 2.129136562347412
Epoch 1500, training loss: 420.485595703125 = 2.1290358304977417 + 50.0 * 8.367131233215332
Epoch 1500, val loss: 2.12886118888855
Epoch 1510, training loss: 420.4217529296875 = 2.1287494897842407 + 50.0 * 8.365859985351562
Epoch 1510, val loss: 2.1286301612854004
Epoch 1520, training loss: 420.38116455078125 = 2.1285070180892944 + 50.0 * 8.365053176879883
Epoch 1520, val loss: 2.128373622894287
Epoch 1530, training loss: 420.3597412109375 = 2.128267765045166 + 50.0 * 8.364629745483398
Epoch 1530, val loss: 2.1281301975250244
Epoch 1540, training loss: 420.4243469238281 = 2.128026843070984 + 50.0 * 8.365926742553711
Epoch 1540, val loss: 2.1278653144836426
Epoch 1550, training loss: 420.30828857421875 = 2.1277167797088623 + 50.0 * 8.363611221313477
Epoch 1550, val loss: 2.127599000930786
Epoch 1560, training loss: 420.3052062988281 = 2.1274787187576294 + 50.0 * 8.363554954528809
Epoch 1560, val loss: 2.1273369789123535
Epoch 1570, training loss: 420.28338623046875 = 2.1272170543670654 + 50.0 * 8.363122940063477
Epoch 1570, val loss: 2.127110719680786
Epoch 1580, training loss: 420.3868713378906 = 2.1270517110824585 + 50.0 * 8.365196228027344
Epoch 1580, val loss: 2.1268296241760254
Epoch 1590, training loss: 420.33599853515625 = 2.126646637916565 + 50.0 * 8.364187240600586
Epoch 1590, val loss: 2.1266088485717773
Epoch 1600, training loss: 420.249267578125 = 2.1264556646347046 + 50.0 * 8.362456321716309
Epoch 1600, val loss: 2.126339912414551
Epoch 1610, training loss: 420.18292236328125 = 2.126180410385132 + 50.0 * 8.36113452911377
Epoch 1610, val loss: 2.1261143684387207
Epoch 1620, training loss: 420.15606689453125 = 2.1259607076644897 + 50.0 * 8.360602378845215
Epoch 1620, val loss: 2.125884532928467
Epoch 1630, training loss: 420.14984130859375 = 2.1257243156433105 + 50.0 * 8.360482215881348
Epoch 1630, val loss: 2.1256442070007324
Epoch 1640, training loss: 420.2798156738281 = 2.1254998445510864 + 50.0 * 8.363086700439453
Epoch 1640, val loss: 2.1253859996795654
Epoch 1650, training loss: 420.14483642578125 = 2.1252024173736572 + 50.0 * 8.360392570495605
Epoch 1650, val loss: 2.1251447200775146
Epoch 1660, training loss: 420.08447265625 = 2.124943733215332 + 50.0 * 8.359190940856934
Epoch 1660, val loss: 2.1249139308929443
Epoch 1670, training loss: 420.05810546875 = 2.1247050762176514 + 50.0 * 8.358668327331543
Epoch 1670, val loss: 2.124685764312744
Epoch 1680, training loss: 420.0370178222656 = 2.1244797706604004 + 50.0 * 8.358250617980957
Epoch 1680, val loss: 2.124453544616699
Epoch 1690, training loss: 420.0522155761719 = 2.124225616455078 + 50.0 * 8.358559608459473
Epoch 1690, val loss: 2.1242246627807617
Epoch 1700, training loss: 420.19024658203125 = 2.123902201652527 + 50.0 * 8.361327171325684
Epoch 1700, val loss: 2.123953104019165
Epoch 1710, training loss: 420.00115966796875 = 2.123714327812195 + 50.0 * 8.357548713684082
Epoch 1710, val loss: 2.1237006187438965
Epoch 1720, training loss: 419.96673583984375 = 2.1234798431396484 + 50.0 * 8.356864929199219
Epoch 1720, val loss: 2.123471736907959
Epoch 1730, training loss: 419.95245361328125 = 2.1232357025146484 + 50.0 * 8.356584548950195
Epoch 1730, val loss: 2.12325382232666
Epoch 1740, training loss: 420.00048828125 = 2.1230274438858032 + 50.0 * 8.357549667358398
Epoch 1740, val loss: 2.123021125793457
Epoch 1750, training loss: 419.982421875 = 2.122782349586487 + 50.0 * 8.357192993164062
Epoch 1750, val loss: 2.1227355003356934
Epoch 1760, training loss: 419.927978515625 = 2.1224923133850098 + 50.0 * 8.356109619140625
Epoch 1760, val loss: 2.122551918029785
Epoch 1770, training loss: 419.9688415527344 = 2.1222258806228638 + 50.0 * 8.356932640075684
Epoch 1770, val loss: 2.1223106384277344
Epoch 1780, training loss: 419.8693542480469 = 2.1220110654830933 + 50.0 * 8.354947090148926
Epoch 1780, val loss: 2.122040271759033
Epoch 1790, training loss: 419.85272216796875 = 2.1217697858810425 + 50.0 * 8.354619026184082
Epoch 1790, val loss: 2.121837854385376
Epoch 1800, training loss: 419.8335876464844 = 2.1215425729751587 + 50.0 * 8.354240417480469
Epoch 1800, val loss: 2.121601104736328
Epoch 1810, training loss: 419.8167724609375 = 2.121322751045227 + 50.0 * 8.353909492492676
Epoch 1810, val loss: 2.1213862895965576
Epoch 1820, training loss: 419.80255126953125 = 2.121083974838257 + 50.0 * 8.353629112243652
Epoch 1820, val loss: 2.1211705207824707
Epoch 1830, training loss: 419.80889892578125 = 2.120836853981018 + 50.0 * 8.353760719299316
Epoch 1830, val loss: 2.1209640502929688
Epoch 1840, training loss: 420.0426025390625 = 2.1204971075057983 + 50.0 * 8.358442306518555
Epoch 1840, val loss: 2.1206765174865723
Epoch 1850, training loss: 419.7913513183594 = 2.1202911138534546 + 50.0 * 8.353421211242676
Epoch 1850, val loss: 2.1204428672790527
Epoch 1860, training loss: 419.76971435546875 = 2.120104193687439 + 50.0 * 8.352992057800293
Epoch 1860, val loss: 2.1201963424682617
Epoch 1870, training loss: 419.73089599609375 = 2.1198819875717163 + 50.0 * 8.35222053527832
Epoch 1870, val loss: 2.119980812072754
Epoch 1880, training loss: 419.7140808105469 = 2.1196576356887817 + 50.0 * 8.351888656616211
Epoch 1880, val loss: 2.119783878326416
Epoch 1890, training loss: 419.69390869140625 = 2.1194454431533813 + 50.0 * 8.351489067077637
Epoch 1890, val loss: 2.1195645332336426
Epoch 1900, training loss: 419.68310546875 = 2.1192299127578735 + 50.0 * 8.351277351379395
Epoch 1900, val loss: 2.119352102279663
Epoch 1910, training loss: 420.025390625 = 2.1190584897994995 + 50.0 * 8.358126640319824
Epoch 1910, val loss: 2.1190924644470215
Epoch 1920, training loss: 419.74029541015625 = 2.118675470352173 + 50.0 * 8.352432250976562
Epoch 1920, val loss: 2.1188716888427734
Epoch 1930, training loss: 419.66607666015625 = 2.11846661567688 + 50.0 * 8.3509521484375
Epoch 1930, val loss: 2.118678569793701
Epoch 1940, training loss: 419.62506103515625 = 2.1182944774627686 + 50.0 * 8.350135803222656
Epoch 1940, val loss: 2.1184427738189697
Epoch 1950, training loss: 419.6141662597656 = 2.1180726289749146 + 50.0 * 8.349922180175781
Epoch 1950, val loss: 2.1182403564453125
Epoch 1960, training loss: 419.7794189453125 = 2.117797374725342 + 50.0 * 8.353232383728027
Epoch 1960, val loss: 2.118044376373291
Epoch 1970, training loss: 419.64141845703125 = 2.1176117658615112 + 50.0 * 8.350476264953613
Epoch 1970, val loss: 2.117786407470703
Epoch 1980, training loss: 419.5996398925781 = 2.1173717975616455 + 50.0 * 8.349645614624023
Epoch 1980, val loss: 2.117587089538574
Epoch 1990, training loss: 419.55670166015625 = 2.1171844005584717 + 50.0 * 8.348790168762207
Epoch 1990, val loss: 2.117382526397705
Epoch 2000, training loss: 419.5367736816406 = 2.1169633865356445 + 50.0 * 8.348396301269531
Epoch 2000, val loss: 2.117190361022949
Epoch 2010, training loss: 419.52337646484375 = 2.1167575120925903 + 50.0 * 8.348132133483887
Epoch 2010, val loss: 2.1169896125793457
Epoch 2020, training loss: 419.55419921875 = 2.116517663002014 + 50.0 * 8.348753929138184
Epoch 2020, val loss: 2.116799831390381
Epoch 2030, training loss: 419.6274108886719 = 2.1162461042404175 + 50.0 * 8.350223541259766
Epoch 2030, val loss: 2.1165404319763184
Epoch 2040, training loss: 419.5003662109375 = 2.116081476211548 + 50.0 * 8.347685813903809
Epoch 2040, val loss: 2.116323471069336
Epoch 2050, training loss: 419.47650146484375 = 2.1158684492111206 + 50.0 * 8.347212791442871
Epoch 2050, val loss: 2.116124153137207
Epoch 2060, training loss: 419.4615783691406 = 2.1156541109085083 + 50.0 * 8.346918106079102
Epoch 2060, val loss: 2.1159324645996094
Epoch 2070, training loss: 419.4436950683594 = 2.1154654026031494 + 50.0 * 8.346564292907715
Epoch 2070, val loss: 2.1157333850860596
Epoch 2080, training loss: 419.42938232421875 = 2.1152496337890625 + 50.0 * 8.346282958984375
Epoch 2080, val loss: 2.1155455112457275
Epoch 2090, training loss: 419.4580993652344 = 2.1150496006011963 + 50.0 * 8.346860885620117
Epoch 2090, val loss: 2.115346908569336
Epoch 2100, training loss: 419.4620361328125 = 2.114816665649414 + 50.0 * 8.346944808959961
Epoch 2100, val loss: 2.1151227951049805
Epoch 2110, training loss: 419.5775451660156 = 2.114519238471985 + 50.0 * 8.349260330200195
Epoch 2110, val loss: 2.114929437637329
Epoch 2120, training loss: 419.4396057128906 = 2.1143964529037476 + 50.0 * 8.346504211425781
Epoch 2120, val loss: 2.1146645545959473
Epoch 2130, training loss: 419.3857116699219 = 2.114150285720825 + 50.0 * 8.345431327819824
Epoch 2130, val loss: 2.114501476287842
Epoch 2140, training loss: 419.3619079589844 = 2.113973617553711 + 50.0 * 8.344958305358887
Epoch 2140, val loss: 2.114292621612549
Epoch 2150, training loss: 419.3583984375 = 2.113779306411743 + 50.0 * 8.344892501831055
Epoch 2150, val loss: 2.1141180992126465
Epoch 2160, training loss: 419.4370422363281 = 2.1135753393173218 + 50.0 * 8.346468925476074
Epoch 2160, val loss: 2.1139233112335205
Epoch 2170, training loss: 419.3590087890625 = 2.113337993621826 + 50.0 * 8.344913482666016
Epoch 2170, val loss: 2.113701105117798
Epoch 2180, training loss: 419.3356018066406 = 2.1131112575531006 + 50.0 * 8.344449996948242
Epoch 2180, val loss: 2.113515853881836
Epoch 2190, training loss: 419.3082275390625 = 2.1129318475723267 + 50.0 * 8.343905448913574
Epoch 2190, val loss: 2.113308906555176
Epoch 2200, training loss: 419.3052978515625 = 2.112724542617798 + 50.0 * 8.343851089477539
Epoch 2200, val loss: 2.1131174564361572
Epoch 2210, training loss: 419.4135437011719 = 2.112468957901001 + 50.0 * 8.34602165222168
Epoch 2210, val loss: 2.1129415035247803
Epoch 2220, training loss: 419.2645568847656 = 2.1122976541519165 + 50.0 * 8.343045234680176
Epoch 2220, val loss: 2.1126935482025146
Epoch 2230, training loss: 419.2720947265625 = 2.1121195554733276 + 50.0 * 8.343199729919434
Epoch 2230, val loss: 2.1125049591064453
Epoch 2240, training loss: 419.36077880859375 = 2.1118701696395874 + 50.0 * 8.344978332519531
Epoch 2240, val loss: 2.1123251914978027
Epoch 2250, training loss: 419.25396728515625 = 2.111717104911804 + 50.0 * 8.34284496307373
Epoch 2250, val loss: 2.112102508544922
Epoch 2260, training loss: 419.24200439453125 = 2.1114931106567383 + 50.0 * 8.342610359191895
Epoch 2260, val loss: 2.111934185028076
Epoch 2270, training loss: 419.21563720703125 = 2.1113098859786987 + 50.0 * 8.342086791992188
Epoch 2270, val loss: 2.111752986907959
Epoch 2280, training loss: 419.20416259765625 = 2.11111843585968 + 50.0 * 8.3418607711792
Epoch 2280, val loss: 2.1115658283233643
Epoch 2290, training loss: 419.3479919433594 = 2.1108919382095337 + 50.0 * 8.344741821289062
Epoch 2290, val loss: 2.111372709274292
Epoch 2300, training loss: 419.2410583496094 = 2.1106767654418945 + 50.0 * 8.342607498168945
Epoch 2300, val loss: 2.1111557483673096
Epoch 2310, training loss: 419.2038269042969 = 2.1104941368103027 + 50.0 * 8.341866493225098
Epoch 2310, val loss: 2.1109542846679688
Epoch 2320, training loss: 419.1717529296875 = 2.1103066205978394 + 50.0 * 8.341228485107422
Epoch 2320, val loss: 2.11077880859375
Epoch 2330, training loss: 419.153076171875 = 2.1101181507110596 + 50.0 * 8.340859413146973
Epoch 2330, val loss: 2.1105968952178955
Epoch 2340, training loss: 419.18621826171875 = 2.1099458932876587 + 50.0 * 8.341525077819824
Epoch 2340, val loss: 2.1104090213775635
Epoch 2350, training loss: 419.2786560058594 = 2.109744429588318 + 50.0 * 8.343378067016602
Epoch 2350, val loss: 2.110166311264038
Epoch 2360, training loss: 419.14324951171875 = 2.1095058917999268 + 50.0 * 8.340675354003906
Epoch 2360, val loss: 2.110002040863037
Epoch 2370, training loss: 419.11181640625 = 2.109308362007141 + 50.0 * 8.340049743652344
Epoch 2370, val loss: 2.1098248958587646
Epoch 2380, training loss: 419.09954833984375 = 2.10913622379303 + 50.0 * 8.339808464050293
Epoch 2380, val loss: 2.109647750854492
Epoch 2390, training loss: 419.0943603515625 = 2.108947992324829 + 50.0 * 8.33970832824707
Epoch 2390, val loss: 2.1094841957092285
Epoch 2400, training loss: 419.1700744628906 = 2.1087372303009033 + 50.0 * 8.341226577758789
Epoch 2400, val loss: 2.1093263626098633
Epoch 2410, training loss: 419.1269226074219 = 2.108526110649109 + 50.0 * 8.340368270874023
Epoch 2410, val loss: 2.1090946197509766
Epoch 2420, training loss: 419.08831787109375 = 2.108332633972168 + 50.0 * 8.339599609375
Epoch 2420, val loss: 2.1089067459106445
Epoch 2430, training loss: 419.06121826171875 = 2.108199119567871 + 50.0 * 8.33906078338623
Epoch 2430, val loss: 2.1087119579315186
Epoch 2440, training loss: 419.0523986816406 = 2.108017921447754 + 50.0 * 8.338887214660645
Epoch 2440, val loss: 2.108553886413574
Epoch 2450, training loss: 419.0413818359375 = 2.1078457832336426 + 50.0 * 8.33867073059082
Epoch 2450, val loss: 2.108372211456299
Epoch 2460, training loss: 419.091064453125 = 2.107686996459961 + 50.0 * 8.339667320251465
Epoch 2460, val loss: 2.1081924438476562
Epoch 2470, training loss: 419.07025146484375 = 2.107470154762268 + 50.0 * 8.339255332946777
Epoch 2470, val loss: 2.107990026473999
Epoch 2480, training loss: 419.04034423828125 = 2.107226848602295 + 50.0 * 8.338662147521973
Epoch 2480, val loss: 2.107839584350586
Epoch 2490, training loss: 419.00787353515625 = 2.107081413269043 + 50.0 * 8.33801555633545
Epoch 2490, val loss: 2.107656955718994
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.396231884057971
0.8630007969282041
The final CL Acc:0.39676, 0.00048, The final GNN Acc:0.86211, 0.00142
Begin epxeriment: noisy_level: 0.2 cont_weight: 50 epoch: der1:0.4 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2500, cl_num_layers=2, cl_num_proj_hidden=64, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=64, num_proj_hidden=64, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 106398])
remove edge: torch.Size([2, 70844])
updated graph: torch.Size([2, 88594])
=== Noisy graph ===
/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:227: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 530.2183227539062 = 1.1033213138580322 + 50.0 * 10.58229923248291
Epoch 0, val loss: 1.1038535833358765
Epoch 10, training loss: 530.2013549804688 = 1.1005916595458984 + 50.0 * 10.582015037536621
Epoch 10, val loss: 1.1010947227478027
Epoch 20, training loss: 530.1414184570312 = 1.097691297531128 + 50.0 * 10.5808744430542
Epoch 20, val loss: 1.0981686115264893
Epoch 30, training loss: 529.8953247070312 = 1.0945863723754883 + 50.0 * 10.576014518737793
Epoch 30, val loss: 1.09503173828125
Epoch 40, training loss: 528.8999633789062 = 1.0910534858703613 + 50.0 * 10.556178092956543
Epoch 40, val loss: 1.091413974761963
Epoch 50, training loss: 525.2363891601562 = 1.0867959260940552 + 50.0 * 10.482992172241211
Epoch 50, val loss: 1.087083339691162
Epoch 60, training loss: 513.5657958984375 = 1.081954002380371 + 50.0 * 10.249676704406738
Epoch 60, val loss: 1.0821932554244995
Epoch 70, training loss: 491.8377380371094 = 1.0764076709747314 + 50.0 * 9.815226554870605
Epoch 70, val loss: 1.0766010284423828
Epoch 80, training loss: 485.193603515625 = 1.0710889101028442 + 50.0 * 9.682450294494629
Epoch 80, val loss: 1.0714303255081177
Epoch 90, training loss: 482.4471435546875 = 1.0665357112884521 + 50.0 * 9.627612113952637
Epoch 90, val loss: 1.0670417547225952
Epoch 100, training loss: 478.03173828125 = 1.0626609325408936 + 50.0 * 9.539381980895996
Epoch 100, val loss: 1.0632553100585938
Epoch 110, training loss: 471.1572265625 = 1.0587658882141113 + 50.0 * 9.401968955993652
Epoch 110, val loss: 1.0594589710235596
Epoch 120, training loss: 466.615234375 = 1.0551337003707886 + 50.0 * 9.311202049255371
Epoch 120, val loss: 1.0559178590774536
Epoch 130, training loss: 464.8176574707031 = 1.0512267351150513 + 50.0 * 9.275328636169434
Epoch 130, val loss: 1.051985740661621
Epoch 140, training loss: 462.00714111328125 = 1.0475965738296509 + 50.0 * 9.21919059753418
Epoch 140, val loss: 1.0484154224395752
Epoch 150, training loss: 458.1473693847656 = 1.044896125793457 + 50.0 * 9.142049789428711
Epoch 150, val loss: 1.0457991361618042
Epoch 160, training loss: 455.0832824707031 = 1.042770266532898 + 50.0 * 9.080810546875
Epoch 160, val loss: 1.043701171875
Epoch 170, training loss: 453.5307312011719 = 1.0405514240264893 + 50.0 * 9.049803733825684
Epoch 170, val loss: 1.041478157043457
Epoch 180, training loss: 451.4221496582031 = 1.0383251905441284 + 50.0 * 9.007676124572754
Epoch 180, val loss: 1.0393102169036865
Epoch 190, training loss: 448.5816345214844 = 1.0364928245544434 + 50.0 * 8.950902938842773
Epoch 190, val loss: 1.0375869274139404
Epoch 200, training loss: 445.69207763671875 = 1.0348718166351318 + 50.0 * 8.893143653869629
Epoch 200, val loss: 1.0360581874847412
Epoch 210, training loss: 443.86700439453125 = 1.0328900814056396 + 50.0 * 8.856681823730469
Epoch 210, val loss: 1.0340496301651
Epoch 220, training loss: 442.8476867675781 = 1.0299625396728516 + 50.0 * 8.83635425567627
Epoch 220, val loss: 1.0310533046722412
Epoch 230, training loss: 442.058837890625 = 1.026437759399414 + 50.0 * 8.820648193359375
Epoch 230, val loss: 1.027468204498291
Epoch 240, training loss: 441.26177978515625 = 1.0229929685592651 + 50.0 * 8.804776191711426
Epoch 240, val loss: 1.0240328311920166
Epoch 250, training loss: 440.3662414550781 = 1.019742727279663 + 50.0 * 8.786930084228516
Epoch 250, val loss: 1.0208399295806885
Epoch 260, training loss: 439.3790283203125 = 1.0165293216705322 + 50.0 * 8.767250061035156
Epoch 260, val loss: 1.0176693201065063
Epoch 270, training loss: 438.54876708984375 = 1.0131924152374268 + 50.0 * 8.750711441040039
Epoch 270, val loss: 1.0143475532531738
Epoch 280, training loss: 437.7767639160156 = 1.0095243453979492 + 50.0 * 8.735344886779785
Epoch 280, val loss: 1.010770559310913
Epoch 290, training loss: 437.1048889160156 = 1.0055509805679321 + 50.0 * 8.721986770629883
Epoch 290, val loss: 1.0068728923797607
Epoch 300, training loss: 436.4869689941406 = 1.0013067722320557 + 50.0 * 8.709712982177734
Epoch 300, val loss: 1.0027124881744385
Epoch 310, training loss: 435.94403076171875 = 0.9967654347419739 + 50.0 * 8.698945045471191
Epoch 310, val loss: 0.9982722401618958
Epoch 320, training loss: 435.5928649902344 = 0.991963267326355 + 50.0 * 8.692017555236816
Epoch 320, val loss: 0.9935238361358643
Epoch 330, training loss: 435.1025085449219 = 0.9867380857467651 + 50.0 * 8.682315826416016
Epoch 330, val loss: 0.9884729981422424
Epoch 340, training loss: 434.640869140625 = 0.9813195466995239 + 50.0 * 8.67319107055664
Epoch 340, val loss: 0.9831716418266296
Epoch 350, training loss: 434.1607971191406 = 0.9757150411605835 + 50.0 * 8.663702011108398
Epoch 350, val loss: 0.9777291417121887
Epoch 360, training loss: 433.6669006347656 = 0.9699286222457886 + 50.0 * 8.653939247131348
Epoch 360, val loss: 0.972054660320282
Epoch 370, training loss: 433.1912841796875 = 0.9639068841934204 + 50.0 * 8.644547462463379
Epoch 370, val loss: 0.9661892652511597
Epoch 380, training loss: 432.738037109375 = 0.9577137231826782 + 50.0 * 8.63560676574707
Epoch 380, val loss: 0.9601449966430664
Epoch 390, training loss: 432.5243225097656 = 0.9512740969657898 + 50.0 * 8.631461143493652
Epoch 390, val loss: 0.9538557529449463
Epoch 400, training loss: 432.0350341796875 = 0.9444983005523682 + 50.0 * 8.621810913085938
Epoch 400, val loss: 0.9472371935844421
Epoch 410, training loss: 431.7002258300781 = 0.9374861717224121 + 50.0 * 8.615254402160645
Epoch 410, val loss: 0.9404029846191406
Epoch 420, training loss: 431.4354248046875 = 0.930236279964447 + 50.0 * 8.610103607177734
Epoch 420, val loss: 0.93332839012146
Epoch 430, training loss: 431.2139892578125 = 0.9226701855659485 + 50.0 * 8.605826377868652
Epoch 430, val loss: 0.9260101914405823
Epoch 440, training loss: 430.91644287109375 = 0.9149354696273804 + 50.0 * 8.600029945373535
Epoch 440, val loss: 0.9184709191322327
Epoch 450, training loss: 430.6358337402344 = 0.9070782661437988 + 50.0 * 8.594574928283691
Epoch 450, val loss: 0.9108551144599915
Epoch 460, training loss: 430.3807678222656 = 0.8990907073020935 + 50.0 * 8.58963394165039
Epoch 460, val loss: 0.9031190872192383
Epoch 470, training loss: 430.13226318359375 = 0.8909434676170349 + 50.0 * 8.584826469421387
Epoch 470, val loss: 0.8951937556266785
Epoch 480, training loss: 429.8678283691406 = 0.8826492428779602 + 50.0 * 8.579703330993652
Epoch 480, val loss: 0.8871530890464783
Epoch 490, training loss: 429.7134094238281 = 0.8742272257804871 + 50.0 * 8.576783180236816
Epoch 490, val loss: 0.8790334463119507
Epoch 500, training loss: 429.5108947753906 = 0.8656232953071594 + 50.0 * 8.572905540466309
Epoch 500, val loss: 0.8706038594245911
Epoch 510, training loss: 429.2346496582031 = 0.8567818403244019 + 50.0 * 8.567557334899902
Epoch 510, val loss: 0.862069308757782
Epoch 520, training loss: 429.0196533203125 = 0.8478270769119263 + 50.0 * 8.563436508178711
Epoch 520, val loss: 0.8534460663795471
Epoch 530, training loss: 428.8497314453125 = 0.8388296365737915 + 50.0 * 8.56021785736084
Epoch 530, val loss: 0.8447644114494324
Epoch 540, training loss: 428.77459716796875 = 0.8297978043556213 + 50.0 * 8.5588960647583
Epoch 540, val loss: 0.8360520601272583
Epoch 550, training loss: 428.599609375 = 0.8207303285598755 + 50.0 * 8.555577278137207
Epoch 550, val loss: 0.8272904753684998
Epoch 560, training loss: 428.45123291015625 = 0.8117128014564514 + 50.0 * 8.552790641784668
Epoch 560, val loss: 0.8186218738555908
Epoch 570, training loss: 428.3197937011719 = 0.802763819694519 + 50.0 * 8.55034065246582
Epoch 570, val loss: 0.8100045919418335
Epoch 580, training loss: 428.20477294921875 = 0.7938956618309021 + 50.0 * 8.5482177734375
Epoch 580, val loss: 0.8014724850654602
Epoch 590, training loss: 428.23388671875 = 0.7851022481918335 + 50.0 * 8.548975944519043
Epoch 590, val loss: 0.7930417060852051
Epoch 600, training loss: 428.06048583984375 = 0.7764283418655396 + 50.0 * 8.54568099975586
Epoch 600, val loss: 0.7846981883049011
Epoch 610, training loss: 427.917236328125 = 0.7679686546325684 + 50.0 * 8.542984962463379
Epoch 610, val loss: 0.7765737771987915
Epoch 620, training loss: 427.8173828125 = 0.7596778273582458 + 50.0 * 8.541153907775879
Epoch 620, val loss: 0.7686562538146973
Epoch 630, training loss: 427.72125244140625 = 0.7515872120857239 + 50.0 * 8.539393424987793
Epoch 630, val loss: 0.760911226272583
Epoch 640, training loss: 427.82110595703125 = 0.7436895370483398 + 50.0 * 8.541548728942871
Epoch 640, val loss: 0.7534108757972717
Epoch 650, training loss: 427.613037109375 = 0.7360390424728394 + 50.0 * 8.5375394821167
Epoch 650, val loss: 0.7460317015647888
Epoch 660, training loss: 427.4521484375 = 0.7285984754562378 + 50.0 * 8.534470558166504
Epoch 660, val loss: 0.7389665842056274
Epoch 670, training loss: 427.35595703125 = 0.7214033603668213 + 50.0 * 8.53269100189209
Epoch 670, val loss: 0.7321478724479675
Epoch 680, training loss: 427.2464294433594 = 0.7145048379898071 + 50.0 * 8.530638694763184
Epoch 680, val loss: 0.7255903482437134
Epoch 690, training loss: 427.1575927734375 = 0.7078491449356079 + 50.0 * 8.5289945602417
Epoch 690, val loss: 0.7192714810371399
Epoch 700, training loss: 427.0985412597656 = 0.7014154195785522 + 50.0 * 8.527942657470703
Epoch 700, val loss: 0.7131601572036743
Epoch 710, training loss: 427.2164001464844 = 0.6951701641082764 + 50.0 * 8.530425071716309
Epoch 710, val loss: 0.7072720527648926
Epoch 720, training loss: 426.94775390625 = 0.6891655921936035 + 50.0 * 8.525171279907227
Epoch 720, val loss: 0.7016159892082214
Epoch 730, training loss: 426.80511474609375 = 0.6834465861320496 + 50.0 * 8.522433280944824
Epoch 730, val loss: 0.6962511539459229
Epoch 740, training loss: 426.7005310058594 = 0.6779611706733704 + 50.0 * 8.520451545715332
Epoch 740, val loss: 0.6911148428916931
Epoch 750, training loss: 426.6334228515625 = 0.6726954579353333 + 50.0 * 8.519214630126953
Epoch 750, val loss: 0.6862175464630127
Epoch 760, training loss: 426.6033020019531 = 0.6675874590873718 + 50.0 * 8.51871395111084
Epoch 760, val loss: 0.6814286708831787
Epoch 770, training loss: 426.5384521484375 = 0.6626951098442078 + 50.0 * 8.517515182495117
Epoch 770, val loss: 0.6768820285797119
Epoch 780, training loss: 426.4277038574219 = 0.6580699682235718 + 50.0 * 8.515392303466797
Epoch 780, val loss: 0.6725659370422363
Epoch 790, training loss: 426.3294372558594 = 0.6536489725112915 + 50.0 * 8.51351547241211
Epoch 790, val loss: 0.6684619188308716
Epoch 800, training loss: 426.2470703125 = 0.6494093537330627 + 50.0 * 8.511953353881836
Epoch 800, val loss: 0.6645120978355408
Epoch 810, training loss: 426.4757385253906 = 0.6453352570533752 + 50.0 * 8.516608238220215
Epoch 810, val loss: 0.660710871219635
Epoch 820, training loss: 426.1529846191406 = 0.6413907408714294 + 50.0 * 8.510231971740723
Epoch 820, val loss: 0.6570776700973511
Epoch 830, training loss: 426.0744934082031 = 0.6376516222953796 + 50.0 * 8.508736610412598
Epoch 830, val loss: 0.6536375284194946
Epoch 840, training loss: 425.9946594238281 = 0.6340734958648682 + 50.0 * 8.507211685180664
Epoch 840, val loss: 0.6503456234931946
Epoch 850, training loss: 425.9245300292969 = 0.6306411027908325 + 50.0 * 8.505877494812012
Epoch 850, val loss: 0.6472132205963135
Epoch 860, training loss: 425.8823547363281 = 0.6273469924926758 + 50.0 * 8.50510025024414
Epoch 860, val loss: 0.644224226474762
Epoch 870, training loss: 425.86224365234375 = 0.6241404414176941 + 50.0 * 8.504761695861816
Epoch 870, val loss: 0.641295313835144
Epoch 880, training loss: 425.8087463378906 = 0.6210616230964661 + 50.0 * 8.503753662109375
Epoch 880, val loss: 0.6385349035263062
Epoch 890, training loss: 425.7293395996094 = 0.6181440353393555 + 50.0 * 8.50222396850586
Epoch 890, val loss: 0.6358914375305176
Epoch 900, training loss: 425.66534423828125 = 0.6153519749641418 + 50.0 * 8.500999450683594
Epoch 900, val loss: 0.6333733797073364
Epoch 910, training loss: 425.6318359375 = 0.6126747131347656 + 50.0 * 8.500383377075195
Epoch 910, val loss: 0.630968451499939
Epoch 920, training loss: 425.5884704589844 = 0.6101104617118835 + 50.0 * 8.499567031860352
Epoch 920, val loss: 0.6286453008651733
Epoch 930, training loss: 425.4813537597656 = 0.6076521873474121 + 50.0 * 8.49747371673584
Epoch 930, val loss: 0.626438319683075
Epoch 940, training loss: 425.4955749511719 = 0.6052994728088379 + 50.0 * 8.49780559539795
Epoch 940, val loss: 0.6243081092834473
Epoch 950, training loss: 425.4585266113281 = 0.6030051708221436 + 50.0 * 8.497110366821289
Epoch 950, val loss: 0.6222649812698364
Epoch 960, training loss: 425.3616027832031 = 0.6007922291755676 + 50.0 * 8.495216369628906
Epoch 960, val loss: 0.6203346252441406
Epoch 970, training loss: 425.37237548828125 = 0.5986863374710083 + 50.0 * 8.495473861694336
Epoch 970, val loss: 0.6184793710708618
Epoch 980, training loss: 425.307861328125 = 0.596619188785553 + 50.0 * 8.494224548339844
Epoch 980, val loss: 0.6166015863418579
Epoch 990, training loss: 425.2345275878906 = 0.5946460366249084 + 50.0 * 8.4927978515625
Epoch 990, val loss: 0.6149051785469055
Epoch 1000, training loss: 425.1541442871094 = 0.5927638411521912 + 50.0 * 8.491227149963379
Epoch 1000, val loss: 0.6132544279098511
Epoch 1010, training loss: 425.12548828125 = 0.5909534692764282 + 50.0 * 8.490691184997559
Epoch 1010, val loss: 0.6116886138916016
Epoch 1020, training loss: 425.2354431152344 = 0.589195191860199 + 50.0 * 8.492924690246582
Epoch 1020, val loss: 0.6101784706115723
Epoch 1030, training loss: 425.0737609863281 = 0.5874808430671692 + 50.0 * 8.489725112915039
Epoch 1030, val loss: 0.6086809039115906
Epoch 1040, training loss: 424.9955749511719 = 0.585849940776825 + 50.0 * 8.488194465637207
Epoch 1040, val loss: 0.6072763204574585
Epoch 1050, training loss: 424.9463195800781 = 0.584280788898468 + 50.0 * 8.4872407913208
Epoch 1050, val loss: 0.6059381365776062
Epoch 1060, training loss: 425.0770263671875 = 0.5827707052230835 + 50.0 * 8.489885330200195
Epoch 1060, val loss: 0.6046131253242493
Epoch 1070, training loss: 424.9794616699219 = 0.5812496542930603 + 50.0 * 8.487964630126953
Epoch 1070, val loss: 0.6033486723899841
Epoch 1080, training loss: 424.8489990234375 = 0.5798014402389526 + 50.0 * 8.485383987426758
Epoch 1080, val loss: 0.602149248123169
Epoch 1090, training loss: 424.78961181640625 = 0.5784186124801636 + 50.0 * 8.484223365783691
Epoch 1090, val loss: 0.6009746193885803
Epoch 1100, training loss: 424.78515625 = 0.5770794153213501 + 50.0 * 8.484161376953125
Epoch 1100, val loss: 0.5998726487159729
Epoch 1110, training loss: 424.8896179199219 = 0.5757439732551575 + 50.0 * 8.48627758026123
Epoch 1110, val loss: 0.598733127117157
Epoch 1120, training loss: 424.7096862792969 = 0.5744355320930481 + 50.0 * 8.482705116271973
Epoch 1120, val loss: 0.597663938999176
Epoch 1130, training loss: 424.67840576171875 = 0.5731870532035828 + 50.0 * 8.482104301452637
Epoch 1130, val loss: 0.5965900421142578
Epoch 1140, training loss: 424.62164306640625 = 0.5719820857048035 + 50.0 * 8.480993270874023
Epoch 1140, val loss: 0.5956215262413025
Epoch 1150, training loss: 424.58685302734375 = 0.5708075761795044 + 50.0 * 8.480320930480957
Epoch 1150, val loss: 0.5946537256240845
Epoch 1160, training loss: 424.561279296875 = 0.56965571641922 + 50.0 * 8.479832649230957
Epoch 1160, val loss: 0.5937161445617676
Epoch 1170, training loss: 424.93170166015625 = 0.5685008764266968 + 50.0 * 8.487263679504395
Epoch 1170, val loss: 0.592795729637146
Epoch 1180, training loss: 424.55218505859375 = 0.5673297047615051 + 50.0 * 8.479697227478027
Epoch 1180, val loss: 0.5918194055557251
Epoch 1190, training loss: 424.47021484375 = 0.5662346482276917 + 50.0 * 8.478079795837402
Epoch 1190, val loss: 0.5909239053726196
Epoch 1200, training loss: 424.4311828613281 = 0.5651788115501404 + 50.0 * 8.477319717407227
Epoch 1200, val loss: 0.5901250839233398
Epoch 1210, training loss: 424.4005126953125 = 0.5641493797302246 + 50.0 * 8.476727485656738
Epoch 1210, val loss: 0.5893088579177856
Epoch 1220, training loss: 424.4066162109375 = 0.563125729560852 + 50.0 * 8.476869583129883
Epoch 1220, val loss: 0.5885382890701294
Epoch 1230, training loss: 424.3799743652344 = 0.5620890259742737 + 50.0 * 8.476357460021973
Epoch 1230, val loss: 0.587680459022522
Epoch 1240, training loss: 424.3363037109375 = 0.5610727071762085 + 50.0 * 8.475504875183105
Epoch 1240, val loss: 0.5868834257125854
Epoch 1250, training loss: 424.2842102050781 = 0.5600939989089966 + 50.0 * 8.474482536315918
Epoch 1250, val loss: 0.5861102938652039
Epoch 1260, training loss: 424.3230285644531 = 0.5591387152671814 + 50.0 * 8.4752779006958
Epoch 1260, val loss: 0.5853322148323059
Epoch 1270, training loss: 424.2191467285156 = 0.5581647753715515 + 50.0 * 8.473219871520996
Epoch 1270, val loss: 0.5845993757247925
Epoch 1280, training loss: 424.18157958984375 = 0.5572143793106079 + 50.0 * 8.472487449645996
Epoch 1280, val loss: 0.5838650465011597
Epoch 1290, training loss: 424.1500549316406 = 0.5562999844551086 + 50.0 * 8.471875190734863
Epoch 1290, val loss: 0.5831403136253357
Epoch 1300, training loss: 424.1092224121094 = 0.5553910732269287 + 50.0 * 8.471076965332031
Epoch 1300, val loss: 0.5824690461158752
Epoch 1310, training loss: 424.12457275390625 = 0.5544883012771606 + 50.0 * 8.471402168273926
Epoch 1310, val loss: 0.5817568898200989
Epoch 1320, training loss: 424.1824645996094 = 0.5535624027252197 + 50.0 * 8.472578048706055
Epoch 1320, val loss: 0.5809532999992371
Epoch 1330, training loss: 424.0453186035156 = 0.5526235699653625 + 50.0 * 8.469854354858398
Epoch 1330, val loss: 0.5802762508392334
Epoch 1340, training loss: 424.0014343261719 = 0.5517252683639526 + 50.0 * 8.468994140625
Epoch 1340, val loss: 0.5796257853507996
Epoch 1350, training loss: 423.95220947265625 = 0.550848126411438 + 50.0 * 8.468027114868164
Epoch 1350, val loss: 0.5789425373077393
Epoch 1360, training loss: 423.9848327636719 = 0.5499660968780518 + 50.0 * 8.468697547912598
Epoch 1360, val loss: 0.5783249735832214
Epoch 1370, training loss: 423.9305114746094 = 0.549055814743042 + 50.0 * 8.467629432678223
Epoch 1370, val loss: 0.5776045322418213
Epoch 1380, training loss: 423.8656311035156 = 0.5481626987457275 + 50.0 * 8.466349601745605
Epoch 1380, val loss: 0.5769048929214478
Epoch 1390, training loss: 423.8257751464844 = 0.5472894906997681 + 50.0 * 8.465569496154785
Epoch 1390, val loss: 0.5762732028961182
Epoch 1400, training loss: 423.7947692871094 = 0.5464213490486145 + 50.0 * 8.464966773986816
Epoch 1400, val loss: 0.5756148099899292
Epoch 1410, training loss: 423.79571533203125 = 0.545554518699646 + 50.0 * 8.46500301361084
Epoch 1410, val loss: 0.5749777555465698
Epoch 1420, training loss: 423.7874755859375 = 0.5446690320968628 + 50.0 * 8.464856147766113
Epoch 1420, val loss: 0.5743243098258972
Epoch 1430, training loss: 423.7239074707031 = 0.54378342628479 + 50.0 * 8.463602066040039
Epoch 1430, val loss: 0.5736441612243652
Epoch 1440, training loss: 423.736328125 = 0.5429115295410156 + 50.0 * 8.463868141174316
Epoch 1440, val loss: 0.5730069875717163
Epoch 1450, training loss: 423.68487548828125 = 0.5420454144477844 + 50.0 * 8.46285629272461
Epoch 1450, val loss: 0.5723652839660645
Epoch 1460, training loss: 423.7226867675781 = 0.5411708354949951 + 50.0 * 8.463630676269531
Epoch 1460, val loss: 0.5717297792434692
Epoch 1470, training loss: 423.6142883300781 = 0.5402942895889282 + 50.0 * 8.461480140686035
Epoch 1470, val loss: 0.5710330009460449
Epoch 1480, training loss: 423.5753479003906 = 0.5394244194030762 + 50.0 * 8.460718154907227
Epoch 1480, val loss: 0.5703908801078796
Epoch 1490, training loss: 423.71112060546875 = 0.5385493636131287 + 50.0 * 8.463451385498047
Epoch 1490, val loss: 0.5697528123855591
Epoch 1500, training loss: 423.602294921875 = 0.5376317501068115 + 50.0 * 8.46129322052002
Epoch 1500, val loss: 0.5690616369247437
Epoch 1510, training loss: 423.5783386230469 = 0.5367326140403748 + 50.0 * 8.460831642150879
Epoch 1510, val loss: 0.5684118270874023
Epoch 1520, training loss: 423.50482177734375 = 0.5358264446258545 + 50.0 * 8.459380149841309
Epoch 1520, val loss: 0.5677173733711243
Epoch 1530, training loss: 423.45440673828125 = 0.5349281430244446 + 50.0 * 8.458389282226562
Epoch 1530, val loss: 0.5669910907745361
Epoch 1540, training loss: 423.4556884765625 = 0.5340228080749512 + 50.0 * 8.458433151245117
Epoch 1540, val loss: 0.5663160085678101
Epoch 1550, training loss: 423.59521484375 = 0.5330953598022461 + 50.0 * 8.46124267578125
Epoch 1550, val loss: 0.5655757188796997
Epoch 1560, training loss: 423.42657470703125 = 0.5321419835090637 + 50.0 * 8.45788860321045
Epoch 1560, val loss: 0.564938485622406
Epoch 1570, training loss: 423.43572998046875 = 0.5312020778656006 + 50.0 * 8.458090782165527
Epoch 1570, val loss: 0.5642101764678955
Epoch 1580, training loss: 423.351806640625 = 0.5302574038505554 + 50.0 * 8.45643138885498
Epoch 1580, val loss: 0.5634734034538269
Epoch 1590, training loss: 423.33563232421875 = 0.5293161869049072 + 50.0 * 8.45612621307373
Epoch 1590, val loss: 0.5627861022949219
Epoch 1600, training loss: 423.3310546875 = 0.5283735990524292 + 50.0 * 8.456053733825684
Epoch 1600, val loss: 0.5620349049568176
Epoch 1610, training loss: 423.489013671875 = 0.5274122953414917 + 50.0 * 8.459232330322266
Epoch 1610, val loss: 0.5612592101097107
Epoch 1620, training loss: 423.3157043457031 = 0.5264139175415039 + 50.0 * 8.455785751342773
Epoch 1620, val loss: 0.5605491995811462
Epoch 1630, training loss: 423.2552795410156 = 0.5254400968551636 + 50.0 * 8.454596519470215
Epoch 1630, val loss: 0.5597695708274841
Epoch 1640, training loss: 423.2254638671875 = 0.5244773030281067 + 50.0 * 8.454019546508789
Epoch 1640, val loss: 0.5590440630912781
Epoch 1650, training loss: 423.2361145019531 = 0.5235104560852051 + 50.0 * 8.454252243041992
Epoch 1650, val loss: 0.5582898855209351
Epoch 1660, training loss: 423.4964599609375 = 0.5225067138671875 + 50.0 * 8.459479331970215
Epoch 1660, val loss: 0.5574827790260315
Epoch 1670, training loss: 423.21954345703125 = 0.521470844745636 + 50.0 * 8.453961372375488
Epoch 1670, val loss: 0.5567438006401062
Epoch 1680, training loss: 423.1529235839844 = 0.5204684734344482 + 50.0 * 8.452649116516113
Epoch 1680, val loss: 0.555992603302002
Epoch 1690, training loss: 423.1363220214844 = 0.5194768905639648 + 50.0 * 8.452337265014648
Epoch 1690, val loss: 0.5552158355712891
Epoch 1700, training loss: 423.13336181640625 = 0.5184775590896606 + 50.0 * 8.452298164367676
Epoch 1700, val loss: 0.5544986724853516
Epoch 1710, training loss: 423.2922058105469 = 0.5174598097801208 + 50.0 * 8.45549488067627
Epoch 1710, val loss: 0.5537128448486328
Epoch 1720, training loss: 423.1307067871094 = 0.5164185166358948 + 50.0 * 8.452285766601562
Epoch 1720, val loss: 0.5529326796531677
Epoch 1730, training loss: 423.07720947265625 = 0.5153921842575073 + 50.0 * 8.451236724853516
Epoch 1730, val loss: 0.5521513819694519
Epoch 1740, training loss: 423.06463623046875 = 0.5143662095069885 + 50.0 * 8.451004981994629
Epoch 1740, val loss: 0.5513970255851746
Epoch 1750, training loss: 423.1951904296875 = 0.5133274793624878 + 50.0 * 8.45363712310791
Epoch 1750, val loss: 0.5506272315979004
Epoch 1760, training loss: 423.0691833496094 = 0.5122635364532471 + 50.0 * 8.451138496398926
Epoch 1760, val loss: 0.5498247146606445
