nohup: ignoring input
Namespace(config='config.yaml', cuda=True, dataset='Pubmed', debug=True, defense_mode='prune', device_id=0, dis_weight=1, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=400, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 115198])
remove edge: torch.Size([2, 62260])
updated graph: torch.Size([2, 88810])
#Attach Nodes:80
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 27.002426147460938 = 1.1051559448242188 + 2.5 * 10.358907699584961
Epoch 0, val loss: 1.104956030845642
Epoch 10, training loss: 26.990236282348633 = 1.0960990190505981 + 2.5 * 10.357654571533203
Epoch 10, val loss: 1.0953179597854614
Epoch 20, training loss: 26.949474334716797 = 1.0835069417953491 + 2.5 * 10.346386909484863
Epoch 20, val loss: 1.0818403959274292
Epoch 30, training loss: 26.672494888305664 = 1.0673191547393799 + 2.5 * 10.242070198059082
Epoch 30, val loss: 1.0651053190231323
Epoch 40, training loss: 25.18017578125 = 1.0505069494247437 + 2.5 * 9.651867866516113
Epoch 40, val loss: 1.0476785898208618
Epoch 50, training loss: 24.746553421020508 = 1.0347105264663696 + 2.5 * 9.484737396240234
Epoch 50, val loss: 1.03214693069458
Epoch 60, training loss: 24.571104049682617 = 1.0198273658752441 + 2.5 * 9.420511245727539
Epoch 60, val loss: 1.017650842666626
Epoch 70, training loss: 24.517786026000977 = 1.0050969123840332 + 2.5 * 9.405076026916504
Epoch 70, val loss: 1.0031278133392334
Epoch 80, training loss: 24.433330535888672 = 0.9916120767593384 + 2.5 * 9.376688003540039
Epoch 80, val loss: 0.9902103543281555
Epoch 90, training loss: 24.256999969482422 = 0.9790408611297607 + 2.5 * 9.31118392944336
Epoch 90, val loss: 0.97800612449646
Epoch 100, training loss: 24.27191734313965 = 0.9671209454536438 + 2.5 * 9.321918487548828
Epoch 100, val loss: 0.9663879871368408
Epoch 110, training loss: 24.09342384338379 = 0.9540246725082397 + 2.5 * 9.255759239196777
Epoch 110, val loss: 0.9535897374153137
Epoch 120, training loss: 24.049474716186523 = 0.9389874339103699 + 2.5 * 9.244194984436035
Epoch 120, val loss: 0.939182460308075
Epoch 130, training loss: 24.0794677734375 = 0.9213094711303711 + 2.5 * 9.263262748718262
Epoch 130, val loss: 0.9221526384353638
Epoch 140, training loss: 24.00984001159668 = 0.9003247022628784 + 2.5 * 9.243806838989258
Epoch 140, val loss: 0.902338445186615
Epoch 150, training loss: 23.959266662597656 = 0.877177894115448 + 2.5 * 9.23283576965332
Epoch 150, val loss: 0.8802069425582886
Epoch 160, training loss: 23.915847778320312 = 0.851788341999054 + 2.5 * 9.225624084472656
Epoch 160, val loss: 0.8563039898872375
Epoch 170, training loss: 23.85623550415039 = 0.8238340020179749 + 2.5 * 9.212961196899414
Epoch 170, val loss: 0.8297739624977112
Epoch 180, training loss: 23.823278427124023 = 0.7936030030250549 + 2.5 * 9.211870193481445
Epoch 180, val loss: 0.8012415170669556
Epoch 190, training loss: 23.808265686035156 = 0.7614191174507141 + 2.5 * 9.218738555908203
Epoch 190, val loss: 0.7709771394729614
Epoch 200, training loss: 23.73828125 = 0.7273595929145813 + 2.5 * 9.204368591308594
Epoch 200, val loss: 0.7386564016342163
Epoch 210, training loss: 23.67906951904297 = 0.6922156810760498 + 2.5 * 9.194742202758789
Epoch 210, val loss: 0.7057628035545349
Epoch 220, training loss: 23.66140365600586 = 0.656807541847229 + 2.5 * 9.201838493347168
Epoch 220, val loss: 0.6729341745376587
Epoch 230, training loss: 23.64356803894043 = 0.6223641037940979 + 2.5 * 9.208481788635254
Epoch 230, val loss: 0.6406393051147461
Epoch 240, training loss: 23.575284957885742 = 0.5890842080116272 + 2.5 * 9.194479942321777
Epoch 240, val loss: 0.6099705100059509
Epoch 250, training loss: 23.527076721191406 = 0.5578017234802246 + 2.5 * 9.18770980834961
Epoch 250, val loss: 0.5814456939697266
Epoch 260, training loss: 23.517141342163086 = 0.5294357538223267 + 2.5 * 9.19508171081543
Epoch 260, val loss: 0.5551316738128662
Epoch 270, training loss: 23.47952651977539 = 0.5038678646087646 + 2.5 * 9.190263748168945
Epoch 270, val loss: 0.5324386954307556
Epoch 280, training loss: 23.47440528869629 = 0.48086807131767273 + 2.5 * 9.197415351867676
Epoch 280, val loss: 0.5120728015899658
Epoch 290, training loss: 23.409605026245117 = 0.4606182277202606 + 2.5 * 9.179594039916992
Epoch 290, val loss: 0.4937889277935028
Epoch 300, training loss: 23.411741256713867 = 0.4428284168243408 + 2.5 * 9.187564849853516
Epoch 300, val loss: 0.4786815941333771
Epoch 310, training loss: 23.402189254760742 = 0.42731744050979614 + 2.5 * 9.189949035644531
Epoch 310, val loss: 0.465609610080719
Epoch 320, training loss: 23.36505889892578 = 0.41379183530807495 + 2.5 * 9.180506706237793
Epoch 320, val loss: 0.45446285605430603
Epoch 330, training loss: 23.317588806152344 = 0.4021146595478058 + 2.5 * 9.166189193725586
Epoch 330, val loss: 0.4458439350128174
Epoch 340, training loss: 23.368267059326172 = 0.3917047083377838 + 2.5 * 9.190625190734863
Epoch 340, val loss: 0.4372180104255676
Epoch 350, training loss: 23.34507942199707 = 0.38279369473457336 + 2.5 * 9.184914588928223
Epoch 350, val loss: 0.43023374676704407
Epoch 360, training loss: 23.314956665039062 = 0.3749050199985504 + 2.5 * 9.176020622253418
Epoch 360, val loss: 0.42410558462142944
Epoch 370, training loss: 23.27949333190918 = 0.36780232191085815 + 2.5 * 9.164676666259766
Epoch 370, val loss: 0.4195207953453064
Epoch 380, training loss: 23.282350540161133 = 0.3614078760147095 + 2.5 * 9.168376922607422
Epoch 380, val loss: 0.41516146063804626
Epoch 390, training loss: 23.274438858032227 = 0.3556206226348877 + 2.5 * 9.167527198791504
Epoch 390, val loss: 0.4112895727157593
Epoch 400, training loss: 23.27347183227539 = 0.3503798246383667 + 2.5 * 9.16923713684082
Epoch 400, val loss: 0.40878814458847046
Epoch 410, training loss: 23.242307662963867 = 0.34559932351112366 + 2.5 * 9.158682823181152
Epoch 410, val loss: 0.40570521354675293
Epoch 420, training loss: 23.231748580932617 = 0.3411901891231537 + 2.5 * 9.15622329711914
Epoch 420, val loss: 0.4029170572757721
Epoch 430, training loss: 23.248212814331055 = 0.3370417356491089 + 2.5 * 9.164468765258789
Epoch 430, val loss: 0.40029844641685486
Epoch 440, training loss: 23.257625579833984 = 0.3332299590110779 + 2.5 * 9.169758796691895
Epoch 440, val loss: 0.39954450726509094
Epoch 450, training loss: 23.22221565246582 = 0.3295769989490509 + 2.5 * 9.157055854797363
Epoch 450, val loss: 0.39669930934906006
Epoch 460, training loss: 23.228328704833984 = 0.3261139690876007 + 2.5 * 9.16088581085205
Epoch 460, val loss: 0.3947620689868927
Epoch 470, training loss: 23.232744216918945 = 0.32282793521881104 + 2.5 * 9.163966178894043
Epoch 470, val loss: 0.3932800590991974
Epoch 480, training loss: 23.215497970581055 = 0.3197121024131775 + 2.5 * 9.15831470489502
Epoch 480, val loss: 0.3914259076118469
Epoch 490, training loss: 23.190656661987305 = 0.3167175054550171 + 2.5 * 9.149576187133789
Epoch 490, val loss: 0.3906364440917969
Epoch 500, training loss: 23.220136642456055 = 0.31386274099349976 + 2.5 * 9.16250991821289
Epoch 500, val loss: 0.39008545875549316
Epoch 510, training loss: 23.181142807006836 = 0.31115370988845825 + 2.5 * 9.147995948791504
Epoch 510, val loss: 0.3894605040550232
Epoch 520, training loss: 23.19835090637207 = 0.30843326449394226 + 2.5 * 9.155966758728027
Epoch 520, val loss: 0.38821083307266235
Epoch 530, training loss: 23.183887481689453 = 0.30592820048332214 + 2.5 * 9.15118408203125
Epoch 530, val loss: 0.38570210337638855
Epoch 540, training loss: 23.232955932617188 = 0.3033379018306732 + 2.5 * 9.171847343444824
Epoch 540, val loss: 0.3853301405906677
Epoch 550, training loss: 23.201427459716797 = 0.30093470215797424 + 2.5 * 9.160197257995605
Epoch 550, val loss: 0.3859194815158844
Epoch 560, training loss: 23.171785354614258 = 0.29860034584999084 + 2.5 * 9.149273872375488
Epoch 560, val loss: 0.3831658959388733
Epoch 570, training loss: 23.17516326904297 = 0.29626891016960144 + 2.5 * 9.151557922363281
Epoch 570, val loss: 0.3848206400871277
Epoch 580, training loss: 23.170137405395508 = 0.29413163661956787 + 2.5 * 9.150402069091797
Epoch 580, val loss: 0.38200560212135315
Epoch 590, training loss: 23.197641372680664 = 0.2918705940246582 + 2.5 * 9.162308692932129
Epoch 590, val loss: 0.3816804885864258
Epoch 600, training loss: 23.187705993652344 = 0.2897784411907196 + 2.5 * 9.159171104431152
Epoch 600, val loss: 0.3819776177406311
Epoch 610, training loss: 23.148052215576172 = 0.2877432703971863 + 2.5 * 9.144124031066895
Epoch 610, val loss: 0.38236358761787415
Epoch 620, training loss: 23.202573776245117 = 0.2857409417629242 + 2.5 * 9.166732788085938
Epoch 620, val loss: 0.3826664686203003
Epoch 630, training loss: 23.195152282714844 = 0.28397291898727417 + 2.5 * 9.164471626281738
Epoch 630, val loss: 0.3793592154979706
Epoch 640, training loss: 23.140504837036133 = 0.2817997932434082 + 2.5 * 9.143482208251953
Epoch 640, val loss: 0.38089510798454285
Epoch 650, training loss: 23.17216682434082 = 0.2799379229545593 + 2.5 * 9.156891822814941
Epoch 650, val loss: 0.3801012635231018
Epoch 660, training loss: 23.140993118286133 = 0.27803975343704224 + 2.5 * 9.145181655883789
Epoch 660, val loss: 0.3795888423919678
Epoch 670, training loss: 23.170541763305664 = 0.2762162983417511 + 2.5 * 9.157730102539062
Epoch 670, val loss: 0.37900158762931824
Epoch 680, training loss: 23.153860092163086 = 0.2744120955467224 + 2.5 * 9.151779174804688
Epoch 680, val loss: 0.3797970712184906
Epoch 690, training loss: 23.128807067871094 = 0.2727247178554535 + 2.5 * 9.142433166503906
Epoch 690, val loss: 0.3794952929019928
Epoch 700, training loss: 23.154386520385742 = 0.2709818482398987 + 2.5 * 9.153361320495605
Epoch 700, val loss: 0.3783279359340668
Epoch 710, training loss: 23.136844635009766 = 0.2692870497703552 + 2.5 * 9.14702320098877
Epoch 710, val loss: 0.37860071659088135
Epoch 720, training loss: 23.172712326049805 = 0.2675931751728058 + 2.5 * 9.162047386169434
Epoch 720, val loss: 0.37845349311828613
Epoch 730, training loss: 23.142223358154297 = 0.2659439146518707 + 2.5 * 9.150511741638184
Epoch 730, val loss: 0.37762123346328735
Epoch 740, training loss: 23.11616325378418 = 0.2642579972743988 + 2.5 * 9.140762329101562
Epoch 740, val loss: 0.37866535782814026
Epoch 750, training loss: 23.17852210998535 = 0.26275399327278137 + 2.5 * 9.16630744934082
Epoch 750, val loss: 0.3798378109931946
Epoch 760, training loss: 23.13387107849121 = 0.26110774278640747 + 2.5 * 9.149105072021484
Epoch 760, val loss: 0.38094428181648254
Epoch 770, training loss: 23.13465118408203 = 0.2595290243625641 + 2.5 * 9.150049209594727
Epoch 770, val loss: 0.37838566303253174
Epoch 780, training loss: 23.139549255371094 = 0.2584223449230194 + 2.5 * 9.152450561523438
Epoch 780, val loss: 0.3829222023487091
Epoch 790, training loss: 23.121723175048828 = 0.25700321793556213 + 2.5 * 9.14588737487793
Epoch 790, val loss: 0.3782224655151367
Epoch 800, training loss: 23.091867446899414 = 0.2550387680530548 + 2.5 * 9.13473129272461
Epoch 800, val loss: 0.38093122839927673
Epoch 810, training loss: 23.138050079345703 = 0.25353556871414185 + 2.5 * 9.15380573272705
Epoch 810, val loss: 0.37918820977211
Epoch 820, training loss: 23.15540313720703 = 0.2519146800041199 + 2.5 * 9.161395072937012
Epoch 820, val loss: 0.38034504652023315
Epoch 830, training loss: 23.123497009277344 = 0.25039011240005493 + 2.5 * 9.149242401123047
Epoch 830, val loss: 0.38022297620773315
Epoch 840, training loss: 23.078123092651367 = 0.24885012209415436 + 2.5 * 9.131709098815918
Epoch 840, val loss: 0.38131919503211975
Epoch 850, training loss: 23.122905731201172 = 0.24745556712150574 + 2.5 * 9.150179862976074
Epoch 850, val loss: 0.38058507442474365
Epoch 860, training loss: 23.19051170349121 = 0.24591557681560516 + 2.5 * 9.177838325500488
Epoch 860, val loss: 0.3833777904510498
Epoch 870, training loss: 23.12726402282715 = 0.2451491802930832 + 2.5 * 9.15284538269043
Epoch 870, val loss: 0.3808612525463104
Epoch 880, training loss: 23.122081756591797 = 0.2431139051914215 + 2.5 * 9.151586532592773
Epoch 880, val loss: 0.38261035084724426
Epoch 890, training loss: 23.103790283203125 = 0.24180300533771515 + 2.5 * 9.144795417785645
Epoch 890, val loss: 0.38275593519210815
Epoch 900, training loss: 23.10732078552246 = 0.24035926163196564 + 2.5 * 9.146784782409668
Epoch 900, val loss: 0.38435637950897217
Epoch 910, training loss: 23.095842361450195 = 0.23904423415660858 + 2.5 * 9.142719268798828
Epoch 910, val loss: 0.3854031562805176
Epoch 920, training loss: 23.118179321289062 = 0.2374454289674759 + 2.5 * 9.15229320526123
Epoch 920, val loss: 0.38404637575149536
Epoch 930, training loss: 23.080461502075195 = 0.23600569367408752 + 2.5 * 9.137782096862793
Epoch 930, val loss: 0.3847898840904236
Epoch 940, training loss: 23.077816009521484 = 0.23459723591804504 + 2.5 * 9.137287139892578
Epoch 940, val loss: 0.3847948908805847
Epoch 950, training loss: 23.0556640625 = 0.233223095536232 + 2.5 * 9.128976821899414
Epoch 950, val loss: 0.3848075270652771
Epoch 960, training loss: 23.107383728027344 = 0.2318338006734848 + 2.5 * 9.150219917297363
Epoch 960, val loss: 0.38745975494384766
Epoch 970, training loss: 23.05475425720215 = 0.23048430681228638 + 2.5 * 9.129708290100098
Epoch 970, val loss: 0.3866673707962036
Epoch 980, training loss: 23.098188400268555 = 0.22902247309684753 + 2.5 * 9.147665977478027
Epoch 980, val loss: 0.3880263566970825
Epoch 990, training loss: 23.055870056152344 = 0.22767150402069092 + 2.5 * 9.131279945373535
Epoch 990, val loss: 0.3891330361366272
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8498
Overall ASR: 0.7110
Flip ASR: 0.6396/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 26.9884033203125 = 1.0912407636642456 + 2.5 * 10.358865737915039
Epoch 0, val loss: 1.0894290208816528
Epoch 10, training loss: 26.976734161376953 = 1.0843133926391602 + 2.5 * 10.35696792602539
Epoch 10, val loss: 1.0821908712387085
Epoch 20, training loss: 26.923694610595703 = 1.0752583742141724 + 2.5 * 10.339374542236328
Epoch 20, val loss: 1.0727698802947998
Epoch 30, training loss: 26.4625186920166 = 1.0645720958709717 + 2.5 * 10.159178733825684
Epoch 30, val loss: 1.0619322061538696
Epoch 40, training loss: 25.017776489257812 = 1.0523887872695923 + 2.5 * 9.58615493774414
Epoch 40, val loss: 1.049592137336731
Epoch 50, training loss: 24.74634552001953 = 1.0388768911361694 + 2.5 * 9.482987403869629
Epoch 50, val loss: 1.0362473726272583
Epoch 60, training loss: 24.584924697875977 = 1.0249449014663696 + 2.5 * 9.423992156982422
Epoch 60, val loss: 1.0230368375778198
Epoch 70, training loss: 24.461406707763672 = 1.0113210678100586 + 2.5 * 9.380033493041992
Epoch 70, val loss: 1.0101839303970337
Epoch 80, training loss: 24.347509384155273 = 0.9981890916824341 + 2.5 * 9.339727401733398
Epoch 80, val loss: 0.9978132843971252
Epoch 90, training loss: 24.24498176574707 = 0.9869338274002075 + 2.5 * 9.303218841552734
Epoch 90, val loss: 0.9871925711631775
Epoch 100, training loss: 24.142162322998047 = 0.975898802280426 + 2.5 * 9.266505241394043
Epoch 100, val loss: 0.9768146276473999
Epoch 110, training loss: 24.115575790405273 = 0.9628831148147583 + 2.5 * 9.261076927185059
Epoch 110, val loss: 0.9642210602760315
Epoch 120, training loss: 24.065853118896484 = 0.946990966796875 + 2.5 * 9.24754524230957
Epoch 120, val loss: 0.9491478204727173
Epoch 130, training loss: 23.997379302978516 = 0.9282350540161133 + 2.5 * 9.22765827178955
Epoch 130, val loss: 0.9312505125999451
Epoch 140, training loss: 24.001676559448242 = 0.9068408012390137 + 2.5 * 9.237934112548828
Epoch 140, val loss: 0.9110972285270691
Epoch 150, training loss: 23.898832321166992 = 0.8828184008598328 + 2.5 * 9.206405639648438
Epoch 150, val loss: 0.8883050084114075
Epoch 160, training loss: 23.953174591064453 = 0.855491042137146 + 2.5 * 9.239072799682617
Epoch 160, val loss: 0.8624462485313416
Epoch 170, training loss: 23.86821937561035 = 0.8253010511398315 + 2.5 * 9.217167854309082
Epoch 170, val loss: 0.8339008092880249
Epoch 180, training loss: 23.775197982788086 = 0.7915785312652588 + 2.5 * 9.193448066711426
Epoch 180, val loss: 0.8021509647369385
Epoch 190, training loss: 23.75583267211914 = 0.7560849189758301 + 2.5 * 9.199899673461914
Epoch 190, val loss: 0.7690020203590393
Epoch 200, training loss: 23.725130081176758 = 0.7187471985816956 + 2.5 * 9.202552795410156
Epoch 200, val loss: 0.7334659695625305
Epoch 210, training loss: 23.703474044799805 = 0.6809712648391724 + 2.5 * 9.209001541137695
Epoch 210, val loss: 0.6982411742210388
Epoch 220, training loss: 23.615713119506836 = 0.6437318325042725 + 2.5 * 9.18879222869873
Epoch 220, val loss: 0.6635535359382629
Epoch 230, training loss: 23.6322078704834 = 0.6073858737945557 + 2.5 * 9.209928512573242
Epoch 230, val loss: 0.6299055814743042
Epoch 240, training loss: 23.52130126953125 = 0.5735865235328674 + 2.5 * 9.179085731506348
Epoch 240, val loss: 0.5991052985191345
Epoch 250, training loss: 23.508102416992188 = 0.5428206920623779 + 2.5 * 9.186113357543945
Epoch 250, val loss: 0.5707041025161743
Epoch 260, training loss: 23.497499465942383 = 0.5149025321006775 + 2.5 * 9.193038940429688
Epoch 260, val loss: 0.5449020266532898
Epoch 270, training loss: 23.423419952392578 = 0.49037355184555054 + 2.5 * 9.173218727111816
Epoch 270, val loss: 0.5226314663887024
Epoch 280, training loss: 23.42133140563965 = 0.4686327874660492 + 2.5 * 9.181078910827637
Epoch 280, val loss: 0.5032060146331787
Epoch 290, training loss: 23.38939666748047 = 0.4497992694377899 + 2.5 * 9.1758394241333
Epoch 290, val loss: 0.4868324398994446
Epoch 300, training loss: 23.407297134399414 = 0.4333796203136444 + 2.5 * 9.189566612243652
Epoch 300, val loss: 0.4731351435184479
Epoch 310, training loss: 23.345489501953125 = 0.4192177653312683 + 2.5 * 9.17050838470459
Epoch 310, val loss: 0.461290568113327
Epoch 320, training loss: 23.364625930786133 = 0.4068163335323334 + 2.5 * 9.183123588562012
Epoch 320, val loss: 0.4510551989078522
Epoch 330, training loss: 23.357887268066406 = 0.3960008919239044 + 2.5 * 9.184754371643066
Epoch 330, val loss: 0.4424416422843933
Epoch 340, training loss: 23.325428009033203 = 0.38645339012145996 + 2.5 * 9.175589561462402
Epoch 340, val loss: 0.43490228056907654
Epoch 350, training loss: 23.319889068603516 = 0.3780813217163086 + 2.5 * 9.176722526550293
Epoch 350, val loss: 0.4289858341217041
Epoch 360, training loss: 23.28032112121582 = 0.37071990966796875 + 2.5 * 9.163840293884277
Epoch 360, val loss: 0.42343074083328247
Epoch 370, training loss: 23.277606964111328 = 0.36405396461486816 + 2.5 * 9.165421485900879
Epoch 370, val loss: 0.41908419132232666
Epoch 380, training loss: 23.260868072509766 = 0.3580469787120819 + 2.5 * 9.161128044128418
Epoch 380, val loss: 0.4150778353214264
Epoch 390, training loss: 23.249662399291992 = 0.3525894284248352 + 2.5 * 9.158829689025879
Epoch 390, val loss: 0.41241317987442017
Epoch 400, training loss: 23.23348045349121 = 0.34751296043395996 + 2.5 * 9.154386520385742
Epoch 400, val loss: 0.40816476941108704
Epoch 410, training loss: 23.263916015625 = 0.3427659869194031 + 2.5 * 9.16845989227295
Epoch 410, val loss: 0.4069313704967499
Epoch 420, training loss: 23.26043128967285 = 0.3383757472038269 + 2.5 * 9.168822288513184
Epoch 420, val loss: 0.40375053882598877
Epoch 430, training loss: 23.254423141479492 = 0.33432623744010925 + 2.5 * 9.168039321899414
Epoch 430, val loss: 0.4017597734928131
Epoch 440, training loss: 23.25101089477539 = 0.3304375112056732 + 2.5 * 9.168230056762695
Epoch 440, val loss: 0.39972519874572754
Epoch 450, training loss: 23.23373031616211 = 0.3268534541130066 + 2.5 * 9.162751197814941
Epoch 450, val loss: 0.3978797495365143
Epoch 460, training loss: 23.203767776489258 = 0.3234737515449524 + 2.5 * 9.152117729187012
Epoch 460, val loss: 0.39613696932792664
Epoch 470, training loss: 23.21708869934082 = 0.320264995098114 + 2.5 * 9.158729553222656
Epoch 470, val loss: 0.3953476548194885
Epoch 480, training loss: 23.21231460571289 = 0.31717154383659363 + 2.5 * 9.15805721282959
Epoch 480, val loss: 0.3939700126647949
Epoch 490, training loss: 23.208311080932617 = 0.31416910886764526 + 2.5 * 9.1576566696167
Epoch 490, val loss: 0.3930271863937378
Epoch 500, training loss: 23.1912784576416 = 0.3113377094268799 + 2.5 * 9.151975631713867
Epoch 500, val loss: 0.39238834381103516
Epoch 510, training loss: 23.189123153686523 = 0.30851224064826965 + 2.5 * 9.152244567871094
Epoch 510, val loss: 0.3911985754966736
Epoch 520, training loss: 23.219087600708008 = 0.30596476793289185 + 2.5 * 9.16524887084961
Epoch 520, val loss: 0.39033520221710205
Epoch 530, training loss: 23.1695499420166 = 0.303335577249527 + 2.5 * 9.146486282348633
Epoch 530, val loss: 0.39007335901260376
Epoch 540, training loss: 23.18852424621582 = 0.3008514642715454 + 2.5 * 9.155069351196289
Epoch 540, val loss: 0.3883249759674072
Epoch 550, training loss: 23.174224853515625 = 0.29844310879707336 + 2.5 * 9.150312423706055
Epoch 550, val loss: 0.38753214478492737
Epoch 560, training loss: 23.216707229614258 = 0.2960907816886902 + 2.5 * 9.168246269226074
Epoch 560, val loss: 0.3878113329410553
Epoch 570, training loss: 23.164791107177734 = 0.2940140664577484 + 2.5 * 9.148310661315918
Epoch 570, val loss: 0.3850950002670288
Epoch 580, training loss: 23.15923500061035 = 0.2917180359363556 + 2.5 * 9.14700698852539
Epoch 580, val loss: 0.38729965686798096
Epoch 590, training loss: 23.16092300415039 = 0.2895241677761078 + 2.5 * 9.1485595703125
Epoch 590, val loss: 0.3849678039550781
Epoch 600, training loss: 23.17403793334961 = 0.2873564064502716 + 2.5 * 9.154672622680664
Epoch 600, val loss: 0.38532474637031555
Epoch 610, training loss: 23.18158721923828 = 0.28535717725753784 + 2.5 * 9.158492088317871
Epoch 610, val loss: 0.3857666254043579
Epoch 620, training loss: 23.163013458251953 = 0.28343918919563293 + 2.5 * 9.151829719543457
Epoch 620, val loss: 0.3839975893497467
Epoch 630, training loss: 23.12704849243164 = 0.281533420085907 + 2.5 * 9.138205528259277
Epoch 630, val loss: 0.3846297562122345
Epoch 640, training loss: 23.164146423339844 = 0.2795715630054474 + 2.5 * 9.153829574584961
Epoch 640, val loss: 0.38403868675231934
Epoch 650, training loss: 23.12278175354004 = 0.27776116132736206 + 2.5 * 9.138008117675781
Epoch 650, val loss: 0.3848312795162201
Epoch 660, training loss: 23.149621963500977 = 0.275870144367218 + 2.5 * 9.149500846862793
Epoch 660, val loss: 0.38332217931747437
Epoch 670, training loss: 23.135757446289062 = 0.2741241157054901 + 2.5 * 9.1446533203125
Epoch 670, val loss: 0.3852232098579407
Epoch 680, training loss: 23.155216217041016 = 0.27229398488998413 + 2.5 * 9.153169631958008
Epoch 680, val loss: 0.38301774859428406
Epoch 690, training loss: 23.134906768798828 = 0.270414263010025 + 2.5 * 9.145796775817871
Epoch 690, val loss: 0.3839469254016876
Epoch 700, training loss: 23.152565002441406 = 0.268646776676178 + 2.5 * 9.15356731414795
Epoch 700, val loss: 0.3831413984298706
Epoch 710, training loss: 23.16307258605957 = 0.26693978905677795 + 2.5 * 9.158452987670898
Epoch 710, val loss: 0.38285794854164124
Epoch 720, training loss: 23.10719871520996 = 0.26527130603790283 + 2.5 * 9.136770248413086
Epoch 720, val loss: 0.3832453489303589
Epoch 730, training loss: 23.164348602294922 = 0.2635548710823059 + 2.5 * 9.160317420959473
Epoch 730, val loss: 0.3841949701309204
Epoch 740, training loss: 23.127490997314453 = 0.2619079351425171 + 2.5 * 9.146233558654785
Epoch 740, val loss: 0.3848370313644409
Epoch 750, training loss: 23.125 = 0.2602003514766693 + 2.5 * 9.145919799804688
Epoch 750, val loss: 0.38437986373901367
Epoch 760, training loss: 23.10968017578125 = 0.25856342911720276 + 2.5 * 9.140446662902832
Epoch 760, val loss: 0.3841709792613983
Epoch 770, training loss: 23.12607192993164 = 0.2569985091686249 + 2.5 * 9.147629737854004
Epoch 770, val loss: 0.3852593004703522
Epoch 780, training loss: 23.104434967041016 = 0.2553727626800537 + 2.5 * 9.13962459564209
Epoch 780, val loss: 0.3847619891166687
Epoch 790, training loss: 23.13243865966797 = 0.25386887788772583 + 2.5 * 9.15142822265625
Epoch 790, val loss: 0.38442233204841614
Epoch 800, training loss: 23.108013153076172 = 0.25273486971855164 + 2.5 * 9.142110824584961
Epoch 800, val loss: 0.38279011845588684
Epoch 810, training loss: 23.118515014648438 = 0.2507944405078888 + 2.5 * 9.147088050842285
Epoch 810, val loss: 0.38543790578842163
Epoch 820, training loss: 23.11448097229004 = 0.24920570850372314 + 2.5 * 9.146109580993652
Epoch 820, val loss: 0.3847487270832062
Epoch 830, training loss: 23.095287322998047 = 0.24816679954528809 + 2.5 * 9.138848304748535
Epoch 830, val loss: 0.3888932764530182
Epoch 840, training loss: 23.118932723999023 = 0.24639858305454254 + 2.5 * 9.14901351928711
Epoch 840, val loss: 0.38473254442214966
Epoch 850, training loss: 23.123519897460938 = 0.24483175575733185 + 2.5 * 9.15147590637207
Epoch 850, val loss: 0.38803592324256897
Epoch 860, training loss: 23.093931198120117 = 0.24327516555786133 + 2.5 * 9.140262603759766
Epoch 860, val loss: 0.3870526850223541
Epoch 870, training loss: 23.083829879760742 = 0.2421305924654007 + 2.5 * 9.136679649353027
Epoch 870, val loss: 0.3909808397293091
Epoch 880, training loss: 23.104839324951172 = 0.24031537771224976 + 2.5 * 9.1458101272583
Epoch 880, val loss: 0.38911518454551697
Epoch 890, training loss: 23.07297706604004 = 0.2389022707939148 + 2.5 * 9.13362979888916
Epoch 890, val loss: 0.38827216625213623
Epoch 900, training loss: 23.099246978759766 = 0.2374332845211029 + 2.5 * 9.144725799560547
Epoch 900, val loss: 0.3900381624698639
Epoch 910, training loss: 23.09294891357422 = 0.2359352856874466 + 2.5 * 9.142805099487305
Epoch 910, val loss: 0.39111006259918213
Epoch 920, training loss: 23.085832595825195 = 0.23449641466140747 + 2.5 * 9.140534400939941
Epoch 920, val loss: 0.39038586616516113
Epoch 930, training loss: 23.063505172729492 = 0.23309196531772614 + 2.5 * 9.132165908813477
Epoch 930, val loss: 0.3914617896080017
Epoch 940, training loss: 23.061269760131836 = 0.231678307056427 + 2.5 * 9.131836891174316
Epoch 940, val loss: 0.39394935965538025
Epoch 950, training loss: 23.06777000427246 = 0.230565145611763 + 2.5 * 9.134881973266602
Epoch 950, val loss: 0.3905338644981384
Epoch 960, training loss: 23.095121383666992 = 0.22884364426136017 + 2.5 * 9.14651107788086
Epoch 960, val loss: 0.39195701479911804
Epoch 970, training loss: 23.056428909301758 = 0.2274656593799591 + 2.5 * 9.131585121154785
Epoch 970, val loss: 0.39320406317710876
Epoch 980, training loss: 23.099164962768555 = 0.22610479593276978 + 2.5 * 9.149224281311035
Epoch 980, val loss: 0.3944339454174042
Epoch 990, training loss: 23.07200813293457 = 0.22474151849746704 + 2.5 * 9.138906478881836
Epoch 990, val loss: 0.3957196772098541
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8498
Overall ASR: 0.7754
Flip ASR: 0.7194/1554 nodes
./selected_nodes/Pubmed/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 26.987350463867188 = 1.090347409248352 + 2.5 * 10.358800888061523
Epoch 0, val loss: 1.089333176612854
Epoch 10, training loss: 26.97389793395996 = 1.0831761360168457 + 2.5 * 10.35628890991211
Epoch 10, val loss: 1.0817216634750366
Epoch 20, training loss: 26.90062713623047 = 1.0740300416946411 + 2.5 * 10.330638885498047
Epoch 20, val loss: 1.0720546245574951
Epoch 30, training loss: 26.138376235961914 = 1.0638774633407593 + 2.5 * 10.029799461364746
Epoch 30, val loss: 1.0614428520202637
Epoch 40, training loss: 25.039962768554688 = 1.0528202056884766 + 2.5 * 9.594857215881348
Epoch 40, val loss: 1.0497950315475464
Epoch 50, training loss: 24.754262924194336 = 1.0400813817977905 + 2.5 * 9.485672950744629
Epoch 50, val loss: 1.0372350215911865
Epoch 60, training loss: 24.592439651489258 = 1.0268906354904175 + 2.5 * 9.426219940185547
Epoch 60, val loss: 1.0245782136917114
Epoch 70, training loss: 24.510656356811523 = 1.0135351419448853 + 2.5 * 9.398848533630371
Epoch 70, val loss: 1.0120872259140015
Epoch 80, training loss: 24.383018493652344 = 1.0015145540237427 + 2.5 * 9.352602005004883
Epoch 80, val loss: 1.0010255575180054
Epoch 90, training loss: 24.255163192749023 = 0.9910611510276794 + 2.5 * 9.305641174316406
Epoch 90, val loss: 0.9911673069000244
Epoch 100, training loss: 24.152267456054688 = 0.9812160134315491 + 2.5 * 9.268420219421387
Epoch 100, val loss: 0.9817329049110413
Epoch 110, training loss: 24.10883140563965 = 0.9701167345046997 + 2.5 * 9.255485534667969
Epoch 110, val loss: 0.9711014628410339
Epoch 120, training loss: 24.03927993774414 = 0.9570960998535156 + 2.5 * 9.232873916625977
Epoch 120, val loss: 0.9588029980659485
Epoch 130, training loss: 24.016122817993164 = 0.9414692521095276 + 2.5 * 9.22986125946045
Epoch 130, val loss: 0.944011926651001
Epoch 140, training loss: 24.032222747802734 = 0.9223888516426086 + 2.5 * 9.24393367767334
Epoch 140, val loss: 0.9258062243461609
Epoch 150, training loss: 23.940818786621094 = 0.8998870253562927 + 2.5 * 9.2163724899292
Epoch 150, val loss: 0.9048806428909302
Epoch 160, training loss: 23.918834686279297 = 0.8751798272132874 + 2.5 * 9.217462539672852
Epoch 160, val loss: 0.8814519047737122
Epoch 170, training loss: 23.867603302001953 = 0.8479231595993042 + 2.5 * 9.20787239074707
Epoch 170, val loss: 0.8557969927787781
Epoch 180, training loss: 23.803112030029297 = 0.818283200263977 + 2.5 * 9.193931579589844
Epoch 180, val loss: 0.8278861045837402
Epoch 190, training loss: 23.782554626464844 = 0.7861026525497437 + 2.5 * 9.198580741882324
Epoch 190, val loss: 0.7974412441253662
Epoch 200, training loss: 23.739059448242188 = 0.7516323328018188 + 2.5 * 9.194971084594727
Epoch 200, val loss: 0.7649463415145874
Epoch 210, training loss: 23.69305992126465 = 0.7159380912780762 + 2.5 * 9.190848350524902
Epoch 210, val loss: 0.7311768531799316
Epoch 220, training loss: 23.648427963256836 = 0.6796029806137085 + 2.5 * 9.187529563903809
Epoch 220, val loss: 0.6970199346542358
Epoch 230, training loss: 23.60878562927246 = 0.6440445184707642 + 2.5 * 9.185895919799805
Epoch 230, val loss: 0.6634750962257385
Epoch 240, training loss: 23.577314376831055 = 0.609331488609314 + 2.5 * 9.187192916870117
Epoch 240, val loss: 0.6310177445411682
Epoch 250, training loss: 23.570613861083984 = 0.5766499638557434 + 2.5 * 9.197585105895996
Epoch 250, val loss: 0.6008409261703491
Epoch 260, training loss: 23.517030715942383 = 0.546237051486969 + 2.5 * 9.18831729888916
Epoch 260, val loss: 0.572599470615387
Epoch 270, training loss: 23.47553825378418 = 0.5186445116996765 + 2.5 * 9.182757377624512
Epoch 270, val loss: 0.547095000743866
Epoch 280, training loss: 23.466581344604492 = 0.49382802844047546 + 2.5 * 9.189101219177246
Epoch 280, val loss: 0.5245747566223145
Epoch 290, training loss: 23.42928123474121 = 0.4717361629009247 + 2.5 * 9.18301773071289
Epoch 290, val loss: 0.5053684115409851
Epoch 300, training loss: 23.422014236450195 = 0.45224642753601074 + 2.5 * 9.187907218933105
Epoch 300, val loss: 0.48737815022468567
Epoch 310, training loss: 23.36033821105957 = 0.4353256821632385 + 2.5 * 9.170004844665527
Epoch 310, val loss: 0.4731055796146393
Epoch 320, training loss: 23.39097023010254 = 0.42055681347846985 + 2.5 * 9.188165664672852
Epoch 320, val loss: 0.46110835671424866
Epoch 330, training loss: 23.34443473815918 = 0.40776699781417847 + 2.5 * 9.174667358398438
Epoch 330, val loss: 0.45024946331977844
Epoch 340, training loss: 23.376710891723633 = 0.396737277507782 + 2.5 * 9.191988945007324
Epoch 340, val loss: 0.44237664341926575
Epoch 350, training loss: 23.31692886352539 = 0.3869629204273224 + 2.5 * 9.17198657989502
Epoch 350, val loss: 0.4351215660572052
Epoch 360, training loss: 23.319406509399414 = 0.37835973501205444 + 2.5 * 9.176419258117676
Epoch 360, val loss: 0.4276331067085266
Epoch 370, training loss: 23.314613342285156 = 0.37083035707473755 + 2.5 * 9.177513122558594
Epoch 370, val loss: 0.4230029284954071
Epoch 380, training loss: 23.316505432128906 = 0.3641206920146942 + 2.5 * 9.180953979492188
Epoch 380, val loss: 0.41774600744247437
Epoch 390, training loss: 23.269189834594727 = 0.3580699563026428 + 2.5 * 9.164447784423828
Epoch 390, val loss: 0.4141811430454254
Epoch 400, training loss: 23.286144256591797 = 0.35262081027030945 + 2.5 * 9.173409461975098
Epoch 400, val loss: 0.4100901484489441
Epoch 410, training loss: 23.27058219909668 = 0.34764838218688965 + 2.5 * 9.169173240661621
Epoch 410, val loss: 0.40709632635116577
Epoch 420, training loss: 23.294506072998047 = 0.34301528334617615 + 2.5 * 9.180596351623535
Epoch 420, val loss: 0.4053497314453125
Epoch 430, training loss: 23.26957130432129 = 0.3386806547641754 + 2.5 * 9.172356605529785
Epoch 430, val loss: 0.4020673632621765
Epoch 440, training loss: 23.227760314941406 = 0.3346772789955139 + 2.5 * 9.157233238220215
Epoch 440, val loss: 0.40156641602516174
Epoch 450, training loss: 23.251222610473633 = 0.33078980445861816 + 2.5 * 9.168172836303711
Epoch 450, val loss: 0.398525208234787
Epoch 460, training loss: 23.254615783691406 = 0.3271964490413666 + 2.5 * 9.170968055725098
Epoch 460, val loss: 0.39605602622032166
Epoch 470, training loss: 23.221302032470703 = 0.32394182682037354 + 2.5 * 9.158944129943848
Epoch 470, val loss: 0.3968103528022766
Epoch 480, training loss: 23.23419761657715 = 0.3205726146697998 + 2.5 * 9.165450096130371
Epoch 480, val loss: 0.3943360149860382
Epoch 490, training loss: 23.21611976623535 = 0.31738150119781494 + 2.5 * 9.15949535369873
Epoch 490, val loss: 0.3925758898258209
Epoch 500, training loss: 23.26317596435547 = 0.31435877084732056 + 2.5 * 9.179526329040527
Epoch 500, val loss: 0.3918076455593109
Epoch 510, training loss: 23.194555282592773 = 0.3114525377750397 + 2.5 * 9.153241157531738
Epoch 510, val loss: 0.39000093936920166
Epoch 520, training loss: 23.2446346282959 = 0.30867162346839905 + 2.5 * 9.174385070800781
Epoch 520, val loss: 0.38946476578712463
Epoch 530, training loss: 23.23079490661621 = 0.30604588985443115 + 2.5 * 9.169899940490723
Epoch 530, val loss: 0.38920989632606506
Epoch 540, training loss: 23.222238540649414 = 0.30347269773483276 + 2.5 * 9.167506217956543
Epoch 540, val loss: 0.38772037625312805
Epoch 550, training loss: 23.18002700805664 = 0.3010411858558655 + 2.5 * 9.151594161987305
Epoch 550, val loss: 0.3862098753452301
Epoch 560, training loss: 23.196189880371094 = 0.29872745275497437 + 2.5 * 9.158985137939453
Epoch 560, val loss: 0.3876589238643646
Epoch 570, training loss: 23.186294555664062 = 0.2962528467178345 + 2.5 * 9.15601634979248
Epoch 570, val loss: 0.3856958746910095
Epoch 580, training loss: 23.229907989501953 = 0.2938961982727051 + 2.5 * 9.174405097961426
Epoch 580, val loss: 0.3852843940258026
Epoch 590, training loss: 23.160661697387695 = 0.29172274470329285 + 2.5 * 9.147575378417969
Epoch 590, val loss: 0.38439449667930603
Epoch 600, training loss: 23.17096710205078 = 0.2895694375038147 + 2.5 * 9.152559280395508
Epoch 600, val loss: 0.38509202003479004
Epoch 610, training loss: 23.206254959106445 = 0.2876323163509369 + 2.5 * 9.167448997497559
Epoch 610, val loss: 0.3829701840877533
Epoch 620, training loss: 23.163297653198242 = 0.28533032536506653 + 2.5 * 9.1511869430542
Epoch 620, val loss: 0.3838900625705719
Epoch 630, training loss: 23.15203857421875 = 0.28323569893836975 + 2.5 * 9.147521018981934
Epoch 630, val loss: 0.38292694091796875
Epoch 640, training loss: 23.14404296875 = 0.28123676776885986 + 2.5 * 9.145122528076172
Epoch 640, val loss: 0.38398370146751404
Epoch 650, training loss: 23.144384384155273 = 0.2793026566505432 + 2.5 * 9.146032333374023
Epoch 650, val loss: 0.38256749510765076
Epoch 660, training loss: 23.16065788269043 = 0.2779639959335327 + 2.5 * 9.153078079223633
Epoch 660, val loss: 0.3853009343147278
Epoch 670, training loss: 23.16986846923828 = 0.2754659056663513 + 2.5 * 9.157761573791504
Epoch 670, val loss: 0.382633775472641
Epoch 680, training loss: 23.150585174560547 = 0.2736058831214905 + 2.5 * 9.150792121887207
Epoch 680, val loss: 0.38157570362091064
Epoch 690, training loss: 23.162029266357422 = 0.2717638611793518 + 2.5 * 9.156105995178223
Epoch 690, val loss: 0.3825708329677582
Epoch 700, training loss: 23.209575653076172 = 0.27001678943634033 + 2.5 * 9.175823211669922
Epoch 700, val loss: 0.3812614381313324
Epoch 710, training loss: 23.156795501708984 = 0.26851367950439453 + 2.5 * 9.155312538146973
Epoch 710, val loss: 0.38377809524536133
Epoch 720, training loss: 23.159910202026367 = 0.2665223777294159 + 2.5 * 9.157355308532715
Epoch 720, val loss: 0.38265499472618103
Epoch 730, training loss: 23.157291412353516 = 0.26476216316223145 + 2.5 * 9.157011032104492
Epoch 730, val loss: 0.3820928931236267
Epoch 740, training loss: 23.113733291625977 = 0.2631697952747345 + 2.5 * 9.140225410461426
Epoch 740, val loss: 0.38106653094291687
Epoch 750, training loss: 23.12548828125 = 0.2613273561000824 + 2.5 * 9.14566421508789
Epoch 750, val loss: 0.3811425268650055
Epoch 760, training loss: 23.113630294799805 = 0.2596680521965027 + 2.5 * 9.141584396362305
Epoch 760, val loss: 0.38026392459869385
Epoch 770, training loss: 23.10642433166504 = 0.2581312358379364 + 2.5 * 9.139317512512207
Epoch 770, val loss: 0.3836340308189392
Epoch 780, training loss: 23.122507095336914 = 0.2565211355686188 + 2.5 * 9.146394729614258
Epoch 780, val loss: 0.38081496953964233
Epoch 790, training loss: 23.134401321411133 = 0.2549184560775757 + 2.5 * 9.151792526245117
Epoch 790, val loss: 0.38045865297317505
Epoch 800, training loss: 23.13447380065918 = 0.25341540575027466 + 2.5 * 9.152422904968262
Epoch 800, val loss: 0.3798249661922455
Epoch 810, training loss: 23.10383415222168 = 0.25159701704978943 + 2.5 * 9.140894889831543
Epoch 810, val loss: 0.382375568151474
Epoch 820, training loss: 23.117143630981445 = 0.25002551078796387 + 2.5 * 9.14684772491455
Epoch 820, val loss: 0.38282209634780884
Epoch 830, training loss: 23.11188316345215 = 0.24865953624248505 + 2.5 * 9.145289421081543
Epoch 830, val loss: 0.3851149082183838
Epoch 840, training loss: 23.139863967895508 = 0.24687540531158447 + 2.5 * 9.157195091247559
Epoch 840, val loss: 0.3829561769962311
Epoch 850, training loss: 23.10757827758789 = 0.24535460770130157 + 2.5 * 9.144889831542969
Epoch 850, val loss: 0.3840528726577759
Epoch 860, training loss: 23.09950065612793 = 0.24391216039657593 + 2.5 * 9.14223575592041
Epoch 860, val loss: 0.3825265169143677
Epoch 870, training loss: 23.09307289123535 = 0.24236540496349335 + 2.5 * 9.140283584594727
Epoch 870, val loss: 0.3839613199234009
Epoch 880, training loss: 23.09234619140625 = 0.24091961979866028 + 2.5 * 9.140570640563965
Epoch 880, val loss: 0.3837636709213257
Epoch 890, training loss: 23.09741973876953 = 0.23935285210609436 + 2.5 * 9.143226623535156
Epoch 890, val loss: 0.38478076457977295
Epoch 900, training loss: 23.090620040893555 = 0.23787708580493927 + 2.5 * 9.141097068786621
Epoch 900, val loss: 0.38545069098472595
Epoch 910, training loss: 23.11006736755371 = 0.23651264607906342 + 2.5 * 9.149421691894531
Epoch 910, val loss: 0.3851511776447296
Epoch 920, training loss: 23.092634201049805 = 0.23503759503364563 + 2.5 * 9.143038749694824
Epoch 920, val loss: 0.385294109582901
Epoch 930, training loss: 23.120229721069336 = 0.23367857933044434 + 2.5 * 9.154620170593262
Epoch 930, val loss: 0.38879552483558655
Epoch 940, training loss: 23.100610733032227 = 0.23231935501098633 + 2.5 * 9.147316932678223
Epoch 940, val loss: 0.389077365398407
Epoch 950, training loss: 23.077110290527344 = 0.23086945712566376 + 2.5 * 9.138496398925781
Epoch 950, val loss: 0.3883015215396881
Epoch 960, training loss: 23.11937713623047 = 0.22944028675556183 + 2.5 * 9.155974388122559
Epoch 960, val loss: 0.3886994421482086
Epoch 970, training loss: 23.080089569091797 = 0.22808006405830383 + 2.5 * 9.140803337097168
Epoch 970, val loss: 0.39034754037857056
Epoch 980, training loss: 23.099300384521484 = 0.22678790986537933 + 2.5 * 9.149004936218262
run_preliminary.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/home/mfl5681/project-contrastive/RobustCL/model.py:222: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
Epoch 980, val loss: 0.38964229822158813
Epoch 990, training loss: 23.10557746887207 = 0.22533710300922394 + 2.5 * 9.152095794677734
Epoch 990, val loss: 0.39234140515327454
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8458
Overall ASR: 0.7830
Flip ASR: 0.7284/1554 nodes
The final ASR:0.75642, 0.03230, Accuracy:0.84847, 0.00191
#Attach Nodes:80
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 97588])
remove edge: torch.Size([2, 79724])
updated graph: torch.Size([2, 88664])
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8498
Overall ASR: 0.8154
Flip ASR: 0.7709/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8498
Overall ASR: 0.8448
Flip ASR: 0.8069/1554 nodes
./selected_nodes/Pubmed/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8534
Overall ASR: 0.8626
Flip ASR: 0.8288/1554 nodes
The final ASR:0.84094, 0.01945, Accuracy:0.85101, 0.00167
