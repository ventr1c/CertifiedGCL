{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1090869/817794642.py:14: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(add_edge_rate_1=0, add_edge_rate_2=0, attack='none', base_model='GCN', batch_size=128, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.01, cl_num_epochs=100, cl_num_layers=2, cl_weight_decay=1e-05, config='config.yaml', cont_weight=1, cuda=True, dataset='PROTEINS', debug=True, device_id=2, drop_edge_rate_1=0.1, drop_edge_rate_2=0.1, drop_feat_rate_1=0.1, drop_feat_rate_2=0.1, drop_node_rate_1=0.1, drop_node_rate_2=0.1, dropout=0.5, encoder_model='Grace', hidden=32, if_smoothed=False, no_cuda=False, num_hidden=32, num_proj_hidden=32, prob=0.8, seed=10, select_target_ratio=0.1, tau=0.2, test_model='GCN', train_lr=0.01, walk_length=10, weight_decay=0.0005)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "# !/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "import imp\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch_geometric.datasets import Planetoid,Reddit2,Flickr,PPI, TUDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "# from torch_geometric.loader import DataLoader\n",
    "# from help_funcs import prune_unrelated_edge,prune_unrelated_edge_isolated\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--debug', action='store_true',\n",
    "        default=True, help='debug mode')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--seed', type=int, default=10, help='Random seed.')\n",
    "parser.add_argument('--base_model', type=str, default='GCN', help='propagation model for encoder',\n",
    "                    choices=['GCN','GAT','GraphSage','GIN'])\n",
    "parser.add_argument('--if_smoothed', action='store_true', default=False)\n",
    "parser.add_argument('--encoder_model', type=str, default='Grace', help='propagation model for encoder',\n",
    "                    choices=['Grace','GraphCL'])\n",
    "parser.add_argument('--dataset', type=str, default='PROTEINS', \n",
    "                    help='Dataset',\n",
    "                    choices=['PROTEINS','MUTAG'])\n",
    "parser.add_argument('--train_lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=32,\n",
    "                    help='Number of hidden units of GConv model.')\n",
    "parser.add_argument('--num_hidden', type=int, default=32,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--num_proj_hidden', type=int, default=32,\n",
    "                    help='Number of hidden units in MLP.')\n",
    "# parser.add_argument('--thrd', type=float, default=0.5)\n",
    "# parser.add_argument('--target_class', type=int, default=0)\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "\n",
    "parser.add_argument('--test_model', type=str, default='GCN',\n",
    "                    choices=['GCN','GAT','GraphSage','GIN'],\n",
    "                    help='Model used to attack')\n",
    "# GPU setting\n",
    "parser.add_argument('--device_id', type=int, default=2,\n",
    "                    help=\"Threshold of prunning edges\")\n",
    "# Contrastive Learning setting\n",
    "parser.add_argument('--config', type=str, default=\"config.yaml\")\n",
    "parser.add_argument('--cl_lr', type=float, default=0.01)\n",
    "# parser.add_argument('--cl_num_proj_hidden', type=int, default=128)\n",
    "parser.add_argument('--cl_num_layers', type=int, default=2)\n",
    "parser.add_argument('--cl_activation', type=str, default='relu')\n",
    "parser.add_argument('--cl_base_model', type=str, default='GCNConv')\n",
    "parser.add_argument('--cont_weight', type=float, default=1)\n",
    "parser.add_argument('--add_edge_rate_1', type=float, default=0)\n",
    "parser.add_argument('--add_edge_rate_2', type=float, default=0)\n",
    "parser.add_argument('--drop_edge_rate_1', type=float, default=0.1)\n",
    "parser.add_argument('--drop_edge_rate_2', type=float, default=0.1)\n",
    "parser.add_argument('--drop_feat_rate_1', type=float, default=0.1)\n",
    "parser.add_argument('--drop_feat_rate_2', type=float, default=0.1)\n",
    "parser.add_argument('--drop_node_rate_1', type=float, default=0.1)\n",
    "parser.add_argument('--drop_node_rate_2', type=float, default=0.1)\n",
    "parser.add_argument('--tau', type=float, default=0.2)\n",
    "parser.add_argument('--cl_num_epochs', type=int, default=100)\n",
    "parser.add_argument('--cl_weight_decay', type=float, default=1e-5)\n",
    "parser.add_argument('--batch_size', default=128, type=int,\n",
    "                    help=\"batch_size of graph dataset\")\n",
    "parser.add_argument('--walk_length', default=10, type=int)\n",
    "# parser.add_argument('--select_thrh', type=float, default=0.8)\n",
    "\n",
    "# Attack\n",
    "parser.add_argument('--attack', type=str, default='none',\n",
    "                    choices=['nettack','random','none'],)\n",
    "parser.add_argument('--select_target_ratio', type=float, default=0.1,\n",
    "                    help=\"The number of selected target test nodes for targeted attack\")\n",
    "# Randomized Smoothing\n",
    "parser.add_argument('--prob', default=0.8, type=float,\n",
    "                    help=\"probability to keep the status for each binary entry\")\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_known_args()[0]\n",
    "args.cuda =  not args.no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(('cuda:{}' if torch.cuda.is_available() else 'cpu').format(args.device_id))\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "# torch.manual_seed(args.seed)\n",
    "# torch.cuda.manual_seed(args.seed)\n",
    "print(args)\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "from torch_geometric.utils import to_undirected\n",
    "import torch_geometric.transforms as T\n",
    "transform = T.Compose([T.NormalizeFeatures()])\n",
    "\n",
    "\n",
    "if(args.dataset == 'PROTEINS' or args.dataset == 'MUTAG'):\n",
    "    dataset = TUDataset(root='./data/', name=args.dataset, transform=None,use_node_attr = True)\n",
    "\n",
    "# data = dataset[0].to(device)\n",
    "dataloader = DataLoader(dataset, batch_size=args.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0,   0,   0,  ..., 127, 127, 127])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import mask_feature,add_random_edge,dropout_adj\n",
    "from torch_geometric.utils import to_undirected, to_dense_adj,to_torch_coo_tensor,dense_to_sparse\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "from GCL.eval import get_split\n",
    "\n",
    "for data in dataset:\n",
    "    data.edge_index = to_undirected(data.edge_index)\n",
    "    \n",
    "# for data in dataset:\n",
    "#     print(5/data.edge_index.shape[1])\n",
    "#     print(data.edge_index.shape,)\n",
    "#     edge_index, added_edges = add_random_edge(data.edge_index,p=5/data.edge_index.shape[1],force_undirected=False,)\n",
    "#     # edge_index = to_undirected(edge_index)\n",
    "#     row = torch.cat([added_edges[0], added_edges[1]])\n",
    "#     col = torch.cat([added_edges[1],added_edges[0]])\n",
    "#     added_edges = torch.stack([row,col])\n",
    "#     edge_index = torch.cat([data.edge_index,added_edges],dim=1)\n",
    "#     print(edge_index.shape)\n",
    "#     break\n",
    "\n",
    "# def sample_noise_all_dense(args,edge_index,edge_weight,device):\n",
    "#     adj = to_dense_adj(edge_index,edge_attr=edge_weight)[0]\n",
    "#     row_idx, col_idx = np.triu_indices(adj.shape[1])\n",
    "#     rand_inputs = torch.randint_like(adj[row_idx,col_idx], low=0, high=2, device=device)\n",
    "#     adj_noise = torch.zeros(adj.shape, device=device)\n",
    "#     m = Bernoulli(torch.tensor([args.prob]).to(device))\n",
    "#     mask = m.sample(adj[row_idx,col_idx].shape).squeeze(-1).int()\n",
    "#     adj_noise[row_idx,col_idx] = adj[row_idx,col_idx] * mask + rand_inputs * (1 - mask)\n",
    "#     adj_noise = adj_noise + adj_noise.t()\n",
    "#     ## diagonal elements set to be 0\n",
    "#     ind = np.diag_indices(adj_noise.shape[0]) \n",
    "#     adj_noise[ind[0],ind[1]] = adj[ind[0], ind[1]]\n",
    "#     edge_index, edge_weight = dense_to_sparse(adj_noise)\n",
    "#     return edge_index,edge_weight\n",
    "\n",
    "# def sample_noise_1by1_dense(args,edge_index,edge_weight,idx,device):\n",
    "#     adj = to_dense_adj(edge_index,edge_attr=edge_weight)[0]\n",
    "#     adj_noise = adj.clone().detach()\n",
    "#     # row_idx, col_idx = np.triu_indices(adj.shape[1])\n",
    "#     rand_inputs = torch.randint_like(adj[idx], low=0, high=2, device=device)\n",
    "#     # adj_noise = torch.zeros(adj.shape, device=device)\n",
    "#     m = Bernoulli(torch.tensor([args.prob]).to(device))\n",
    "#     mask = m.sample(adj[idx].shape).squeeze(-1).int()\n",
    "#     adj_noise[idx] = adj[idx] * mask + rand_inputs * (1 - mask)\n",
    "#     adj_noise[idx,idx] = adj[idx,idx]\n",
    "#     print(adj_noise)\n",
    "#     adj_noise[:,idx] = adj_noise[idx].t()\n",
    "#     # adj_noise = adj_noise + adj_noise.t()\n",
    "#     # ## diagonal elements set to be 0\n",
    "#     # ind = np.diag_indices(adj_noise.shape[0]) \n",
    "#     # adj_noise[ind[0],ind[1]] = adj[ind[0], ind[1]]\n",
    "#     edge_index, edge_weight = dense_to_sparse(adj_noise)\n",
    "#     return edge_index,edge_weight\n",
    "\n",
    "\n",
    "# for data in dataset:\n",
    "#     data = data.to(device)\n",
    "#     split = get_split(num_samples=data.x.shape[0], train_ratio=0.8, test_ratio=0.1)\n",
    "#     sample_noise_1by1_dense(args,data.edge_index,data.edge_weight,split['test'],device)\n",
    "#     # sparse_adj = to_torch_coo_tensor(data.edge_index,data.edge_weight)\n",
    "#     # print(sparse_adj.shape)\n",
    "#     # print(adj_noise.nonzero())\n",
    "#     # rand_inputs\n",
    "#     break\n",
    "for data in dataloader:\n",
    "    print(data.)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if(args.dataset == 'ogbn-arxiv'):\n",
    "#     nNode = data.x.shape[0]\n",
    "#     setattr(data,'train_mask',torch.zeros(nNode, dtype=torch.bool).to(device))\n",
    "#     # dataset[0].train_mask = torch.zeros(nEdge, dtype=torch.bool).to(device)\n",
    "#     data.val_mask = torch.zeros(nNode, dtype=torch.bool).to(device)\n",
    "#     data.test_mask = torch.zeros(nNode, dtype=torch.bool).to(device)\n",
    "#     data.y = data.y.squeeze(1)\n",
    "# # we build our own train test split \n",
    "# print(data)\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "# # from utils import get_split\n",
    "# # # data, idx_train, idx_val, idx_clean_test, idx_atk = get_split(args,data,device)\n",
    "# idx_train = data.train_mask.nonzero().flatten()\n",
    "# idx_val = data.val_mask.nonzero().flatten()\n",
    "# idx_clean_test = data.test_mask.nonzero().flatten()\n",
    "\n",
    "# from torch_geometric.utils import to_undirected\n",
    "# from utils import subgraph\n",
    "# data.edge_index = to_undirected(data.edge_index)\n",
    "# train_edge_index,_, edge_mask = subgraph(torch.bitwise_not(data.test_mask),data.edge_index,relabel_nodes=False)\n",
    "# mask_edge_index = data.edge_index[:,torch.bitwise_not(edge_mask)]\n",
    "# # filter out the unlabeled nodes except from training nodes and testing nodes, nonzero() is to get index, flatten is to get 1-d tensor\n",
    "# unlabeled_idx = (torch.bitwise_not(data.test_mask)&torch.bitwise_not(data.train_mask)).nonzero().flatten()\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "if(args.dataset == 'Cora'):\n",
    "    args.drop_edge_rate_1 = 0.2\n",
    "    args.drop_edge_rate_2 = 0.4\n",
    "    args.drop_feat_rate_1 = 0.3\n",
    "    args.drop_feat_rate_2 = 0.4\n",
    "    args.tau = 0.1\n",
    "    args.cl_lr = 0.0005\n",
    "    args.weight_decay = 1e-5\n",
    "    args.cl_num_epochs = 500\n",
    "    args.num_hidden = 128\n",
    "    args.hidden = 128\n",
    "    args.num_proj_hidden = 128\n",
    "elif(args.dataset == \"Pubmed\"):\n",
    "    args.drop_edge_rate_1 = 0.4\n",
    "    args.drop_edge_rate_2 = 0.1\n",
    "    args.drop_feat_rate_1 = 0.0\n",
    "    args.drop_feat_rate_2 = 0.2\n",
    "    args.tau = 0.1\n",
    "    args.cl_lr = 0.001\n",
    "    args.weight_decay = 1e-5\n",
    "    args.cl_num_epochs = 500\n",
    "    args.num_hidden = 256\n",
    "    args.hidden = 256\n",
    "elif(args.dataset == \"Citeseer\"):\n",
    "    args.drop_edge_rate_1 = 0.2\n",
    "    args.drop_edge_rate_2 = 0.0\n",
    "    args.drop_feat_rate_1 = 0.3\n",
    "    args.drop_feat_rate_2 = 0.2\n",
    "    args.tau = 0.1\n",
    "    args.cl_lr = 0.001\n",
    "    args.weight_decay = 1e-5\n",
    "    args.cl_num_epochs = 500\n",
    "    args.num_hidden = 256\n",
    "    args.hidden = 256\n",
    "\n",
    "# In[29]:\n",
    "import copy \n",
    "from models.construct import model_construct\n",
    "from construct_graph import *\n",
    "from models.GCN_CL import GCN_Encoder, Grace\n",
    "\n",
    "import os.path as osp\n",
    "import random\n",
    "from time import perf_counter as t\n",
    "import yaml\n",
    "from yaml import SafeLoader\n",
    "\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "from eval import label_classification,label_evaluation,label_classification_origin\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# from deeprobust.graph.defense import GCN\n",
    "# from deeprobust.graph.targeted_attack import Nettack\n",
    "from deeprobust.graph.utils import *\n",
    "# from deeprobust.graph.data import Dataset\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "from deeprobust.graph.data import Dataset, Pyg2Dpr, Dpr2Pyg\n",
    "from deeprobust.graph.defense import GCN\n",
    "from deeprobust.graph.targeted_attack import Nettack\n",
    "\n",
    "import GCL.augmentors as A\n",
    "from models.random_smooth import sample_noise,sample_noise_1by1,sample_noise_all\n",
    "from models.GraphCL import GConv, Encoder\n",
    "\n",
    "\n",
    "data = data.to(device)\n",
    "num_class = int(data.y.max()+1)\n",
    "input_dim = max(dataset.num_features, 1)\n",
    "\n",
    "rs = np.random.RandomState(args.seed)\n",
    "seeds = rs.randint(1000,size=1)\n",
    "\n",
    "accs = []\n",
    "for seed in seeds:\n",
    "    # Construct and train encoder\n",
    "    aug1 = A.Identity()\n",
    "    aug2 = A.RandomChoice([A.RWSampling(num_seeds=args.seed, walk_length=args.walk_length),\n",
    "                           A.NodeDropping(pn=args.drop_edge_rate_2),\n",
    "                           A.FeatureMasking(pf=args.drop_feat_rate_2),\n",
    "                           A.EdgeRemoving(pe=args.drop_node_rate_2)], 1)\n",
    "\n",
    "    gconv = GConv(input_dim=input_dim, hidden_dim=args.hidden, num_layers=2).to(device)\n",
    "    model = Encoder(args = args, encoder=gconv, augmentor=(aug1, aug2), input_dim=input_dim, hidden_dim=args.num_hidden, lr=args.cl_lr, tau=args.tau,num_epoch = args.cl_num_epochs, if_smoothed = args.if_smoothed,device = device)\n",
    "    model.fit(dataloader)\n",
    "    test_result = model.test(dataloader)\n",
    "    print(f'(E): Best test F1Mi={test_result[\"micro_f1\"]:.4f}, F1Ma={test_result[\"macro_f1\"]:.4f}')\n",
    "    \n",
    "# In[30]:\n",
    "    from deeprobust.graph.data import Dataset, Pyg2Dpr, Dpr2Pyg\n",
    "    from torch_geometric.utils import from_scipy_sparse_matrix\n",
    "    from scipy.sparse import csr_matrix\n",
    "    def single_test(adj, features, target_node, gcn=None):\n",
    "        if gcn is None:\n",
    "            # test on GCN (poisoning attack)\n",
    "            gcn = GCN(nfeat=features.shape[1],\n",
    "                    nhid=16,\n",
    "                    nclass=labels.max().item() + 1,\n",
    "                    dropout=0.5, device=device)\n",
    "\n",
    "            gcn = gcn.to(device)\n",
    "\n",
    "            gcn.fit(features, adj, labels, idx_train, idx_val, patience=30)\n",
    "            gcn.eval()\n",
    "            output = gcn.predict()\n",
    "        else:\n",
    "            # test on GCN (evasion attack)\n",
    "            output = gcn.predict(features, adj)\n",
    "        probs = torch.exp(output[[target_node]])\n",
    "\n",
    "        # acc_test = accuracy(output[[target_node]], labels[target_node])\n",
    "        acc_test = (output.argmax(1)[target_node] == labels[target_node])\n",
    "        return acc_test.item()\n",
    "\n",
    "    # Evaluation Metric: Robust Accuracy\n",
    "    # Node-level task\n",
    "    print(args.attack)\n",
    "    if(args.attack == 'random'):\n",
    "        import construct_graph\n",
    "        import copy\n",
    "        perturbation_sizes = list(range(1,6))\n",
    "        for n_perturbation in perturbation_sizes:\n",
    "            print(\"Perturbation Size:{}\".format(n_perturbation))\n",
    "            noisy_data = copy.deepcopy(data)\n",
    "            print(n_perturbation)\n",
    "            for idx in idx_clean_test:\n",
    "                noisy_data = construct_graph.generate_node_noisy(args,noisy_data,idx,n_perturbation,device)\n",
    "                noisy_data = noisy_data.to(device)\n",
    "            model.eval()\n",
    "            if(args.if_smoothed):\n",
    "                # noisy_data.edge_index,noisy_data.edge_weight = sample_noise_1by1(args, noisy_data.x, noisy_data.edge_index, noisy_data.edge_weight,idx_clean_test,device)\n",
    "                noisy_data.edge_index,noisy_data.edge_weight = sample_noise(args,noisy_data.edge_index, noisy_data.edge_weight,idx_clean_test, device)\n",
    "                z, _, _ = model(noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight)\n",
    "            else:\n",
    "                z, _, _ = model(noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight)\n",
    "            acc = label_evaluation(z, noisy_data.y, idx_train, idx_clean_test)\n",
    "            print(\"Accuracy:\",acc)\n",
    "    elif(args.attack == 'none'):\n",
    "        model.eval()\n",
    "        if(args.if_smoothed == True):\n",
    "            rs_edge_index, rs_edge_weight = sample_noise_all(args, data.edge_index, data.edge_weight, device)\n",
    "            z, _, _ = model(data.x, rs_edge_index, rs_edge_weight)\n",
    "        else:\n",
    "            z, _, _ = model(data.x, data.edge_index, data.edge_weight)\n",
    "            # acc = label_evaluation(z, data.y, idx_train, idx_clean_test)\n",
    "            # print(\"Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(add_edge_rate_1=0, add_edge_rate_2=0, attack='none', base_model='GCN', batch_size=128, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.01, cl_num_epochs=100, cl_num_layers=2, cl_weight_decay=1e-05, config='config.yaml', cont_weight=1, cuda=True, dataset='PROTEINS', debug=True, device_id=2, drop_edge_rate_1=0.1, drop_edge_rate_2=0.1, drop_feat_rate_1=0.1, drop_feat_rate_2=0.1, drop_node_rate_1=0.1, drop_node_rate_2=0.1, dropout=0.5, encoder_model='Grace', hidden=32, if_smoothed=False, no_cuda=False, num_hidden=32, num_proj_hidden=32, prob=1.0, seed=10, select_target_ratio=0.1, tau=0.2, test_model='GCN', train_lr=0.01, walk_length=10, weight_decay=0.0005)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(T): 100%|██████████| 100/100 [00:16<00:00,  5.97it/s, loss=15.7]\n",
      "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(E): Best test F1Mi=0.7117, F1Ma=0.7077\n",
      "none\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "# !/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "import imp\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch_geometric.datasets import Planetoid,Reddit2,Flickr,PPI, TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# from torch_geometric.loader import DataLoader\n",
    "# from help_funcs import prune_unrelated_edge,prune_unrelated_edge_isolated\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--debug', action='store_true',\n",
    "        default=True, help='debug mode')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--seed', type=int, default=10, help='Random seed.')\n",
    "parser.add_argument('--base_model', type=str, default='GCN', help='propagation model for encoder',\n",
    "                    choices=['GCN','GAT','GraphSage','GIN'])\n",
    "parser.add_argument('--if_smoothed', action='store_true', default=False)\n",
    "parser.add_argument('--encoder_model', type=str, default='Grace', help='propagation model for encoder',\n",
    "                    choices=['Grace','GraphCL'])\n",
    "parser.add_argument('--dataset', type=str, default='PROTEINS', \n",
    "                    help='Dataset',\n",
    "                    choices=['PROTEINS','MUTAG'])\n",
    "parser.add_argument('--train_lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=32,\n",
    "                    help='Number of hidden units of GConv model.')\n",
    "parser.add_argument('--num_hidden', type=int, default=32,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--num_proj_hidden', type=int, default=32,\n",
    "                    help='Number of hidden units in MLP.')\n",
    "# parser.add_argument('--thrd', type=float, default=0.5)\n",
    "# parser.add_argument('--target_class', type=int, default=0)\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "\n",
    "parser.add_argument('--test_model', type=str, default='GCN',\n",
    "                    choices=['GCN','GAT','GraphSage','GIN'],\n",
    "                    help='Model used to attack')\n",
    "# GPU setting\n",
    "parser.add_argument('--device_id', type=int, default=2,\n",
    "                    help=\"Threshold of prunning edges\")\n",
    "# Contrastive Learning setting\n",
    "parser.add_argument('--config', type=str, default=\"config.yaml\")\n",
    "parser.add_argument('--cl_lr', type=float, default=0.01)\n",
    "# parser.add_argument('--cl_num_proj_hidden', type=int, default=128)\n",
    "parser.add_argument('--cl_num_layers', type=int, default=2)\n",
    "parser.add_argument('--cl_activation', type=str, default='relu')\n",
    "parser.add_argument('--cl_base_model', type=str, default='GCNConv')\n",
    "parser.add_argument('--cont_weight', type=float, default=1)\n",
    "parser.add_argument('--add_edge_rate_1', type=float, default=0)\n",
    "parser.add_argument('--add_edge_rate_2', type=float, default=0)\n",
    "parser.add_argument('--drop_edge_rate_1', type=float, default=0.1)\n",
    "parser.add_argument('--drop_edge_rate_2', type=float, default=0.1)\n",
    "parser.add_argument('--drop_feat_rate_1', type=float, default=0.1)\n",
    "parser.add_argument('--drop_feat_rate_2', type=float, default=0.1)\n",
    "parser.add_argument('--drop_node_rate_1', type=float, default=0.1)\n",
    "parser.add_argument('--drop_node_rate_2', type=float, default=0.1)\n",
    "parser.add_argument('--tau', type=float, default=0.2)\n",
    "parser.add_argument('--cl_num_epochs', type=int, default=100)\n",
    "parser.add_argument('--cl_weight_decay', type=float, default=1e-5)\n",
    "parser.add_argument('--batch_size', default=128, type=int,\n",
    "                    help=\"batch_size of graph dataset\")\n",
    "parser.add_argument('--walk_length', default=10, type=int)\n",
    "# parser.add_argument('--select_thrh', type=float, default=0.8)\n",
    "\n",
    "# Attack\n",
    "parser.add_argument('--attack', type=str, default='none',\n",
    "                    choices=['nettack','random','none'],)\n",
    "parser.add_argument('--select_target_ratio', type=float, default=0.1,\n",
    "                    help=\"The number of selected target test nodes for targeted attack\")\n",
    "# Randomized Smoothing\n",
    "parser.add_argument('--prob', default=1.0, type=float,\n",
    "                    help=\"probability to keep the status for each binary entry\")\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_known_args()[0]\n",
    "args.cuda =  not args.no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(('cuda:{}' if torch.cuda.is_available() else 'cpu').format(args.device_id))\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "# torch.manual_seed(args.seed)\n",
    "# torch.cuda.manual_seed(args.seed)\n",
    "print(args)\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "from torch_geometric.utils import to_undirected\n",
    "import torch_geometric.transforms as T\n",
    "transform = T.Compose([T.NormalizeFeatures()])\n",
    "\n",
    "\n",
    "if(args.dataset == 'PROTEINS' or args.dataset == 'MUTAG'):\n",
    "    dataset = TUDataset(root='./data/', name=args.dataset, transform=None,use_node_attr = True)\n",
    "\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "from torch_geometric.utils import to_undirected\n",
    "for data in dataset:\n",
    "    data.edge_index = to_undirected(data.edge_index)\n",
    "dataloader = DataLoader(dataset, batch_size=args.batch_size)\n",
    "\n",
    "# if(args.dataset == 'ogbn-arxiv'):\n",
    "#     nNode = data.x.shape[0]\n",
    "#     setattr(data,'train_mask',torch.zeros(nNode, dtype=torch.bool).to(device))\n",
    "#     # dataset[0].train_mask = torch.zeros(nEdge, dtype=torch.bool).to(device)\n",
    "#     data.val_mask = torch.zeros(nNode, dtype=torch.bool).to(device)\n",
    "#     data.test_mask = torch.zeros(nNode, dtype=torch.bool).to(device)\n",
    "#     data.y = data.y.squeeze(1)\n",
    "# # we build our own train test split \n",
    "# print(data)\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "# # from utils import get_split\n",
    "# # # data, idx_train, idx_val, idx_clean_test, idx_atk = get_split(args,data,device)\n",
    "# idx_train = data.train_mask.nonzero().flatten()\n",
    "# idx_val = data.val_mask.nonzero().flatten()\n",
    "# idx_clean_test = data.test_mask.nonzero().flatten()\n",
    "\n",
    "# from torch_geometric.utils import to_undirected\n",
    "# from utils import subgraph\n",
    "# data.edge_index = to_undirected(data.edge_index)\n",
    "# train_edge_index,_, edge_mask = subgraph(torch.bitwise_not(data.test_mask),data.edge_index,relabel_nodes=False)\n",
    "# mask_edge_index = data.edge_index[:,torch.bitwise_not(edge_mask)]\n",
    "# # filter out the unlabeled nodes except from training nodes and testing nodes, nonzero() is to get index, flatten is to get 1-d tensor\n",
    "# unlabeled_idx = (torch.bitwise_not(data.test_mask)&torch.bitwise_not(data.train_mask)).nonzero().flatten()\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "if(args.dataset == 'Cora'):\n",
    "    args.drop_edge_rate_1 = 0.2\n",
    "    args.drop_edge_rate_2 = 0.4\n",
    "    args.drop_feat_rate_1 = 0.3\n",
    "    args.drop_feat_rate_2 = 0.4\n",
    "    args.tau = 0.1\n",
    "    args.cl_lr = 0.0005\n",
    "    args.weight_decay = 1e-5\n",
    "    args.cl_num_epochs = 500\n",
    "    args.num_hidden = 128\n",
    "    args.hidden = 128\n",
    "    args.num_proj_hidden = 128\n",
    "elif(args.dataset == \"Pubmed\"):\n",
    "    args.drop_edge_rate_1 = 0.4\n",
    "    args.drop_edge_rate_2 = 0.1\n",
    "    args.drop_feat_rate_1 = 0.0\n",
    "    args.drop_feat_rate_2 = 0.2\n",
    "    args.tau = 0.1\n",
    "    args.cl_lr = 0.001\n",
    "    args.weight_decay = 1e-5\n",
    "    args.cl_num_epochs = 500\n",
    "    args.num_hidden = 256\n",
    "    args.hidden = 256\n",
    "elif(args.dataset == \"Citeseer\"):\n",
    "    args.drop_edge_rate_1 = 0.2\n",
    "    args.drop_edge_rate_2 = 0.0\n",
    "    args.drop_feat_rate_1 = 0.3\n",
    "    args.drop_feat_rate_2 = 0.2\n",
    "    args.tau = 0.1\n",
    "    args.cl_lr = 0.001\n",
    "    args.weight_decay = 1e-5\n",
    "    args.cl_num_epochs = 500\n",
    "    args.num_hidden = 256\n",
    "    args.hidden = 256\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "import copy \n",
    "from models.construct import model_construct\n",
    "from construct_graph import *\n",
    "from models.GCN_CL import GCN_Encoder, Grace\n",
    "\n",
    "import os.path as osp\n",
    "import random\n",
    "from time import perf_counter as t\n",
    "import yaml\n",
    "from yaml import SafeLoader\n",
    "\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "from eval import label_classification,label_evaluation,label_classification_origin\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# from deeprobust.graph.defense import GCN\n",
    "# from deeprobust.graph.targeted_attack import Nettack\n",
    "from deeprobust.graph.utils import *\n",
    "# from deeprobust.graph.data import Dataset\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "from deeprobust.graph.data import Dataset, Pyg2Dpr, Dpr2Pyg\n",
    "from deeprobust.graph.defense import GCN\n",
    "from deeprobust.graph.targeted_attack import Nettack\n",
    "\n",
    "import GCL.augmentors as A\n",
    "from models.random_smooth import sample_noise,sample_noise_1by1,sample_noise_all\n",
    "from models.GraphCL import GConv, Encoder\n",
    "\n",
    "\n",
    "data = data.to(device)\n",
    "num_class = int(data.y.max()+1)\n",
    "input_dim = max(dataset.num_features, 1)\n",
    "\n",
    "rs = np.random.RandomState(args.seed)\n",
    "seeds = rs.randint(1000,size=1)\n",
    "\n",
    "accs = []\n",
    "for seed in seeds:\n",
    "    # Construct and train encoder\n",
    "    aug1 = A.Identity()\n",
    "    aug2 = A.RandomChoice([A.RWSampling(num_seeds=args.seed, walk_length=args.walk_length),\n",
    "                           A.NodeDropping(pn=args.drop_edge_rate_2),\n",
    "                           A.FeatureMasking(pf=args.drop_feat_rate_2),\n",
    "                           A.EdgeRemoving(pe=args.drop_node_rate_2)], 1)\n",
    "\n",
    "    gconv = GConv(input_dim=input_dim, hidden_dim=args.hidden, num_layers=2).to(device)\n",
    "    model = Encoder(args = args, encoder=gconv, augmentor=(aug1, aug2), input_dim=input_dim, hidden_dim=args.num_hidden, lr=args.cl_lr, tau=args.tau,num_epoch = args.cl_num_epochs, if_smoothed = args.if_smoothed,device = device)\n",
    "    model.fit(dataloader)\n",
    "    test_result = model.test(dataloader)\n",
    "    print(f'(E): Best test F1Mi={test_result[\"micro_f1\"]:.4f}, F1Ma={test_result[\"macro_f1\"]:.4f}')\n",
    "    \n",
    "# In[30]:\n",
    "    # Evaluation Metric: Robust Accuracy\n",
    "    # Node-level task\n",
    "    print(args.attack)\n",
    "    if(args.attack == 'random'):\n",
    "        import construct_graph\n",
    "        import copy\n",
    "        perturbation_sizes = list(range(1,6))\n",
    "        for n_perturbation in perturbation_sizes:\n",
    "            print(\"Perturbation Size:{}\".format(n_perturbation))\n",
    "            # noisy_dataset = copy.deepcopy(dataset)\n",
    "            print(n_perturbation)\n",
    "            noisy_dataset = []\n",
    "            for data in dataset:\n",
    "            # for i in range(len(noisy_dataset)):\n",
    "                noisy_data = construct_graph.generate_graph_noisy(args,data,n_perturbation,device)\n",
    "                # noisy_dataset[i] = noisy_data.to(device)\n",
    "                noisy_dataset.append(noisy_data)\n",
    "            noisy_dataloader = DataLoader(noisy_dataset, batch_size=args.batch_size)\n",
    "            # test_result = model.single_test(noisy_dataloader)\n",
    "            # for i in range(len(noisy_dataset)):\n",
    "            #     noisy_data = construct_graph.generate_graph_noisy(args,noisy_dataset[i],n_perturbation,device)\n",
    "            #     # noisy_dataset[i] = noisy_data.to(device)\n",
    "            #     # noisy_dataloader = DataLoader(noisy_dataset, batch_size=args.batch_size)\n",
    "            #     test_result = model.single_test(noisy_data)\n",
    "            print(f'(E): Best test F1Mi={test_result[\"micro_f1\"]:.4f}, F1Ma={test_result[\"macro_f1\"]:.4f}')\n",
    "            \n",
    "    # elif(args.attack == 'none'):\n",
    "    #     model.eval()\n",
    "    #     if(args.if_smoothed == True):\n",
    "    #         rs_edge_index, rs_edge_weight = sample_noise_all(args, data.edge_index, data.edge_weight, device)\n",
    "    #         z, _, _ = model(data.x, rs_edge_index, rs_edge_weight)\n",
    "    #     else:\n",
    "    #         z, _, _ = model(data.x, data.edge_index, data.edge_weight)\n",
    "        # acc = label_evaluation(z, data.y, idx_train, idx_clean_test)\n",
    "        # print(\"Accuracy:\",acc)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perturbation Size:1\n",
      "1\n",
      "(E): Best test F1Mi=0.7117, F1Ma=0.7077\n",
      "Perturbation Size:2\n",
      "2\n",
      "(E): Best test F1Mi=0.7117, F1Ma=0.7077\n",
      "Perturbation Size:3\n",
      "3\n",
      "(E): Best test F1Mi=0.7117, F1Ma=0.7077\n",
      "Perturbation Size:4\n",
      "4\n",
      "(E): Best test F1Mi=0.7117, F1Ma=0.7077\n",
      "Perturbation Size:5\n",
      "5\n",
      "(E): Best test F1Mi=0.7117, F1Ma=0.7077\n"
     ]
    }
   ],
   "source": [
    "import construct_graph\n",
    "import copy\n",
    "perturbation_sizes = list(range(1,6))\n",
    "for n_perturbation in perturbation_sizes:\n",
    "    print(\"Perturbation Size:{}\".format(n_perturbation))\n",
    "    # noisy_dataset = copy.deepcopy(dataset)\n",
    "    print(n_perturbation)\n",
    "    noisy_dataset = []\n",
    "    for data in dataset:\n",
    "    # for i in range(len(noisy_dataset)):\n",
    "        noisy_data = construct_graph.generate_graph_noisy(args,data,n_perturbation,device)\n",
    "        # noisy_dataset[i] = noisy_data.to(device)\n",
    "        noisy_dataset.append(noisy_data)\n",
    "    noisy_dataloader = DataLoader(noisy_dataset, batch_size=args.batch_size)\n",
    "    # test_result = model.single_test(noisy_dataloader)\n",
    "    # for i in range(len(noisy_dataset)):\n",
    "    #     noisy_data = construct_graph.generate_graph_noisy(args,noisy_dataset[i],n_perturbation,device)\n",
    "    #     # noisy_dataset[i] = noisy_data.to(device)\n",
    "    #     # noisy_dataloader = DataLoader(noisy_dataset, batch_size=args.batch_size)\n",
    "    #     test_result = model.single_test(noisy_data)\n",
    "    print(f'(E): Best test F1Mi={test_result[\"micro_f1\"]:.4f}, F1Ma={test_result[\"macro_f1\"]:.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(edge_index=[2, 27320], x=[7265, 4], y=[128], batch=[7265], ptr=[129])\n",
      "DataBatch(edge_index=[2, 28600], x=[7265, 4], y=[128], batch=[7265], ptr=[129])\n"
     ]
    }
   ],
   "source": [
    "for data in dataloader:\n",
    "    print(data)\n",
    "    break\n",
    "for data in noisy_dataloader:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_torch120",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ab847dfc59cee10fa08e4e9fed31787c275fa5742f67664facc345e6fad65e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
