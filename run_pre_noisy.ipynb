{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* implement unifyGNN+Contrastive\n",
    "* try different noisy and augmentation\n",
    "* try against with/without unnoticeable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(add_edge_rate_1=0, add_edge_rate_2=0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=0, cl_num_layers=2, cl_num_proj_hidden=32, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=3, dis_weight=1, drop_edge_rate_1=0, drop_edge_rate_2=0.5, drop_feat_rate_1=0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=0, inner=1, lr=0.01, model='GCN', no_cuda=False, noisy_level=0.25, num_hidden=32, num_proj_hidden=32, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=400, use_vs_number=True, vs_number=160, vs_ratio=0, weight_decay=0.0005)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]: \n",
    "\n",
    "import imp\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch_geometric.datasets import Planetoid,Reddit2,Flickr,PPI\n",
    "\n",
    "\n",
    "# from torch_geometric.loader import DataLoader\n",
    "# from help_funcs import prune_unrelated_edge,prune_unrelated_edge_isolated\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--debug', action='store_true',\n",
    "        default=True, help='debug mode')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--seed', type=int, default=10, help='Random seed.')\n",
    "parser.add_argument('--model', type=str, default='GCN', help='model',\n",
    "                    choices=['GCN','GAT','GraphSage','GIN'])\n",
    "parser.add_argument('--dataset', type=str, default='Pubmed', \n",
    "                    help='Dataset',\n",
    "                    choices=['Cora','Citeseer','Pubmed','PPI','Flickr','ogbn-arxiv','Reddit','Reddit2','Yelp'])\n",
    "parser.add_argument('--train_lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=32,\n",
    "                    help='Number of hidden units of backdoor model.')\n",
    "parser.add_argument('--num_hidden', type=int, default=32,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--num_proj_hidden', type=int, default=32,\n",
    "                    help='Number of hidden units in MLP.')\n",
    "parser.add_argument('--thrd', type=float, default=0.5)\n",
    "parser.add_argument('--target_class', type=int, default=0)\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('--epochs', type=int,  default=200, help='Number of epochs to train benign and backdoor model.')\n",
    "parser.add_argument('--trojan_epochs', type=int,  default=400, help='Number of epochs to train trigger generator.')\n",
    "parser.add_argument('--inner', type=int,  default=1, help='Number of inner')\n",
    "parser.add_argument('--temperature', type=float,  default=0.5, help='Temperature')\n",
    "# backdoor setting\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--trigger_size', type=int, default=3,\n",
    "                    help='tirgger_size')\n",
    "parser.add_argument('--use_vs_number', action='store_true', default=True,\n",
    "                    help=\"if use detailed number to decide Vs\")\n",
    "parser.add_argument('--vs_ratio', type=float, default=0,\n",
    "                    help=\"ratio of poisoning nodes relative to the full graph\")\n",
    "parser.add_argument('--vs_number', type=int, default=160,\n",
    "                    help=\"number of poisoning nodes relative to the full graph\")\n",
    "# defense setting\n",
    "parser.add_argument('--defense_mode', type=str, default=\"none\",\n",
    "                    choices=['prune', 'isolate', 'none'],\n",
    "                    help=\"Mode of defense\")\n",
    "parser.add_argument('--prune_thr', type=float, default=0.2,\n",
    "                    help=\"Threshold of prunning edges\")\n",
    "parser.add_argument('--target_loss_weight', type=float, default=1,\n",
    "                    help=\"Weight of optimize outter trigger generator\")\n",
    "parser.add_argument('--homo_loss_weight', type=float, default=0,\n",
    "                    help=\"Weight of optimize similarity loss\")\n",
    "parser.add_argument('--homo_boost_thrd', type=float, default=0.5,\n",
    "                    help=\"Threshold of increase similarity\")\n",
    "# attack setting\n",
    "parser.add_argument('--dis_weight', type=float, default=1,\n",
    "                    help=\"Weight of cluster distance\")\n",
    "parser.add_argument('--selection_method', type=str, default='cluster_degree',\n",
    "                    choices=['loss','conf','cluster','none','cluster_degree'],\n",
    "                    help='Method to select idx_attach for training trojan model (none means randomly select)')\n",
    "parser.add_argument('--test_model', type=str, default='GCN',\n",
    "                    choices=['GCN','GAT','GraphSage','GIN'],\n",
    "                    help='Model used to attack')\n",
    "parser.add_argument('--evaluate_mode', type=str, default='1by1',\n",
    "                    choices=['overall','1by1'],\n",
    "                    help='Model used to attack')\n",
    "# GPU setting\n",
    "parser.add_argument('--device_id', type=int, default=3,\n",
    "                    help=\"Threshold of prunning edges\")\n",
    "# GRACE setting\n",
    "parser.add_argument('--gpu_id', type=int, default=0)\n",
    "parser.add_argument('--config', type=str, default=\"config.yaml\")\n",
    "## Contrasitve setting\n",
    "parser.add_argument('--cl_lr', type=float, default=0.0002)\n",
    "# parser.add_argument('--cl_num_hidden', type=int, default=128)\n",
    "parser.add_argument('--cl_num_proj_hidden', type=int, default=32)\n",
    "parser.add_argument('--cl_num_layers', type=int, default=2)\n",
    "parser.add_argument('--cl_activation', type=str, default='relu')\n",
    "parser.add_argument('--cl_base_model', type=str, default='GCNConv')\n",
    "parser.add_argument('--cont_weight', type=float, default=100)\n",
    "parser.add_argument('--add_edge_rate_1', type=float, default=0)\n",
    "parser.add_argument('--add_edge_rate_2', type=float, default=0)\n",
    "parser.add_argument('--drop_edge_rate_1', type=float, default=0)\n",
    "parser.add_argument('--drop_edge_rate_2', type=float, default=0.5)\n",
    "parser.add_argument('--drop_feat_rate_1', type=float, default=0)\n",
    "parser.add_argument('--drop_feat_rate_2', type=float, default=0.5)\n",
    "parser.add_argument('--tau', type=float, default=0.4)\n",
    "parser.add_argument('--cl_num_epochs', type=int, default=0)\n",
    "parser.add_argument('--cl_weight_decay', type=float, default=0.00001)\n",
    "parser.add_argument('--cont_batch_size', type=int, default=0)\n",
    "parser.add_argument('--noisy_level', type=float, default=0.25)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_known_args()[0]\n",
    "args.cuda =  not args.no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(('cuda:{}' if torch.cuda.is_available() else 'cpu').format(args.device_id))\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "# torch.manual_seed(args.seed)\n",
    "# torch.cuda.manual_seed(args.seed)\n",
    "print(args)\n",
    "#%%\n",
    "from torch_geometric.utils import to_undirected\n",
    "import torch_geometric.transforms as T\n",
    "transform = T.Compose([T.NormalizeFeatures()])\n",
    "\n",
    "if(args.dataset == 'Cora' or args.dataset == 'Citeseer' or args.dataset == 'Pubmed'):\n",
    "    dataset = Planetoid(root='./data/', \\\n",
    "                        name=args.dataset,\\\n",
    "                        transform=transform)\n",
    "elif(args.dataset == 'Flickr'):\n",
    "    dataset = Flickr(root='./data/Flickr/', \\\n",
    "                    transform=transform)\n",
    "elif(args.dataset == 'Reddit2'):\n",
    "    dataset = Reddit2(root='./data/Reddit2/', \\\n",
    "                    transform=transform)\n",
    "elif(args.dataset == 'ogbn-arxiv'):\n",
    "    from ogb.nodeproppred import PygNodePropPredDataset\n",
    "    # Download and process data at './dataset/ogbg_molhiv/'\n",
    "    dataset = PygNodePropPredDataset(name = 'ogbn-arxiv', root='./data/')\n",
    "    split_idx = dataset.get_idx_split() \n",
    "\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "if(args.dataset == 'ogbn-arxiv'):\n",
    "    nNode = data.x.shape[0]\n",
    "    setattr(data,'train_mask',torch.zeros(nNode, dtype=torch.bool).to(device))\n",
    "    # dataset[0].train_mask = torch.zeros(nEdge, dtype=torch.bool).to(device)\n",
    "    data.val_mask = torch.zeros(nNode, dtype=torch.bool).to(device)\n",
    "    data.test_mask = torch.zeros(nNode, dtype=torch.bool).to(device)\n",
    "    data.y = data.y.squeeze(1)\n",
    "# we build our own train test split \n",
    "#%% \n",
    "from utils import get_split\n",
    "data, idx_train, idx_val, idx_clean_test, idx_atk = get_split(args,data,device)\n",
    "\n",
    "from torch_geometric.utils import to_undirected\n",
    "from utils import subgraph\n",
    "data.edge_index = to_undirected(data.edge_index)\n",
    "train_edge_index,_, edge_mask = subgraph(torch.bitwise_not(data.test_mask),data.edge_index,relabel_nodes=False)\n",
    "mask_edge_index = data.edge_index[:,torch.bitwise_not(edge_mask)]\n",
    "# filter out the unlabeled nodes except from training nodes and testing nodes, nonzero() is to get index, flatten is to get 1-d tensor\n",
    "unlabeled_idx = (torch.bitwise_not(data.test_mask)&torch.bitwise_not(data.train_mask)).nonzero().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_dense_adj, dense_to_sparse\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from scipy.linalg import expm\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.datasets import Planetoid, Amazon, Coauthor\n",
    "\n",
    "# from seeds import development_seed\n",
    "DATA_PATH = './data/'\n",
    "\n",
    "def get_dataset(name: str, use_lcc: bool = True) -> InMemoryDataset:\n",
    "    path = os.path.join(DATA_PATH, name)\n",
    "    if name in ['Cora', 'Citeseer', 'Pubmed']:\n",
    "        dataset = Planetoid(path, name)\n",
    "    elif name in ['Computers', 'Photo']:\n",
    "        dataset = Amazon(path, name)\n",
    "    elif name == 'CoauthorCS':\n",
    "        dataset = Coauthor(path, 'CS')\n",
    "    else:\n",
    "        raise Exception('Unknown dataset.')\n",
    "\n",
    "    if use_lcc:\n",
    "        lcc = get_largest_connected_component(dataset)\n",
    "\n",
    "        x_new = dataset.data.x[lcc]\n",
    "        y_new = dataset.data.y[lcc]\n",
    "\n",
    "        row, col = dataset.data.edge_index.numpy()\n",
    "        edges = [[i, j] for i, j in zip(row, col) if i in lcc and j in lcc]\n",
    "        edges = remap_edges(edges, get_node_mapper(lcc))\n",
    "        \n",
    "        data = Data(\n",
    "            x=x_new,\n",
    "            edge_index=torch.LongTensor(edges),\n",
    "            y=y_new,\n",
    "            train_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
    "            test_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
    "            val_mask=torch.zeros(y_new.size()[0], dtype=torch.bool)\n",
    "        )\n",
    "        dataset.data = data\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_component(dataset: InMemoryDataset, start: int = 0) -> set:\n",
    "    visited_nodes = set()\n",
    "    queued_nodes = set([start])\n",
    "    row, col = dataset.data.edge_index.numpy()\n",
    "    while queued_nodes:\n",
    "        current_node = queued_nodes.pop()\n",
    "        visited_nodes.update([current_node])\n",
    "        neighbors = col[np.where(row == current_node)[0]]\n",
    "        neighbors = [n for n in neighbors if n not in visited_nodes and n not in queued_nodes]\n",
    "        queued_nodes.update(neighbors)\n",
    "    return visited_nodes\n",
    "\n",
    "\n",
    "def get_largest_connected_component(dataset: InMemoryDataset) -> np.ndarray:\n",
    "    remaining_nodes = set(range(dataset.data.x.shape[0]))\n",
    "    comps = []\n",
    "    while remaining_nodes:\n",
    "        start = min(remaining_nodes)\n",
    "        comp = get_component(dataset, start)\n",
    "        comps.append(comp)\n",
    "        remaining_nodes = remaining_nodes.difference(comp)\n",
    "    return np.array(list(comps[np.argmax(list(map(len, comps)))]))\n",
    "\n",
    "\n",
    "def get_node_mapper(lcc: np.ndarray) -> dict:\n",
    "    mapper = {}\n",
    "    counter = 0\n",
    "    for node in lcc:\n",
    "        mapper[node] = counter\n",
    "        counter += 1\n",
    "    return mapper\n",
    "\n",
    "\n",
    "def remap_edges(edges: list, mapper: dict) -> list:\n",
    "    row = [e[0] for e in edges]\n",
    "    col = [e[1] for e in edges]\n",
    "    row = list(map(lambda x: mapper[x], row))\n",
    "    col = list(map(lambda x: mapper[x], col))\n",
    "    return [row, col]\n",
    "\n",
    "\n",
    "def get_adj_matrix(data) -> np.ndarray:\n",
    "    num_nodes = data.x.shape[0]\n",
    "    adj_matrix = np.zeros(shape=(num_nodes, num_nodes))\n",
    "    for i, j in zip(data.edge_index[0], data.edge_index[1]):\n",
    "        adj_matrix[i, j] = 1.\n",
    "    return adj_matrix\n",
    "\n",
    "\n",
    "def get_ppr_matrix(\n",
    "        adj_matrix: np.ndarray,\n",
    "        alpha: float = 0.1) -> np.ndarray:\n",
    "    num_nodes = adj_matrix.shape[0]\n",
    "    A_tilde = adj_matrix + np.eye(num_nodes)\n",
    "    D_tilde = np.diag(1/np.sqrt(A_tilde.sum(axis=1)))\n",
    "    H = D_tilde @ A_tilde @ D_tilde\n",
    "    return alpha * np.linalg.inv(np.eye(num_nodes) - (1 - alpha) * H)\n",
    "\n",
    "\n",
    "def get_heat_matrix(\n",
    "        adj_matrix: np.ndarray,\n",
    "        t: float = 5.0) -> np.ndarray:\n",
    "    num_nodes = adj_matrix.shape[0]\n",
    "    A_tilde = adj_matrix + np.eye(num_nodes)\n",
    "    D_tilde = np.diag(1/np.sqrt(A_tilde.sum(axis=1)))\n",
    "    H = D_tilde @ A_tilde @ D_tilde\n",
    "    return expm(-t * (np.eye(num_nodes) - H))\n",
    "\n",
    "\n",
    "def get_top_k_matrix(A: np.ndarray, k: int = 128) -> np.ndarray:\n",
    "    num_nodes = A.shape[0]\n",
    "    row_idx = np.arange(num_nodes)\n",
    "    A[A.argsort(axis=0)[:num_nodes - k], row_idx] = 0.\n",
    "    norm = A.sum(axis=0)\n",
    "    norm[norm <= 0] = 1 # avoid dividing by zero\n",
    "    return A/norm\n",
    "\n",
    "\n",
    "def get_clipped_matrix(A: np.ndarray, eps: float = 0.01) -> np.ndarray:\n",
    "    num_nodes = A.shape[0]\n",
    "    A[A < eps] = 0.\n",
    "    norm = A.sum(axis=0)\n",
    "    norm[norm <= 0] = 1 # avoid dividing by zero\n",
    "    return A/norm\n",
    "\n",
    "\n",
    "def set_train_val_test_split(\n",
    "        seed: int,\n",
    "        data: Data,\n",
    "        num_development: int = 1500,\n",
    "        num_per_class: int = 20) -> Data:\n",
    "    rnd_state = np.random.RandomState(development_seed)\n",
    "    num_nodes = data.y.shape[0]\n",
    "    development_idx = rnd_state.choice(num_nodes, num_development, replace=False)\n",
    "    test_idx = [i for i in np.arange(num_nodes) if i not in development_idx]\n",
    "\n",
    "    train_idx = []\n",
    "    rnd_state = np.random.RandomState(seed)\n",
    "    for c in range(data.y.max() + 1):\n",
    "        class_idx = development_idx[np.where(data.y[development_idx].cpu() == c)[0]]\n",
    "        train_idx.extend(rnd_state.choice(class_idx, num_per_class, replace=False))\n",
    "\n",
    "    val_idx = [i for i in development_idx if i not in train_idx]\n",
    "\n",
    "    def get_mask(idx):\n",
    "        mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        mask[idx] = 1\n",
    "        return mask\n",
    "\n",
    "    data.train_mask = get_mask(train_idx)\n",
    "    data.val_mask = get_mask(val_idx)\n",
    "    data.test_mask = get_mask(test_idx)\n",
    "\n",
    "    return data\n",
    "\n",
    "class PPRDataset(InMemoryDataset):\n",
    "    \"\"\"\n",
    "    Dataset preprocessed with GDC using PPR diffusion.\n",
    "    Note that this implementations is not scalable\n",
    "    since we directly invert the adjacency matrix.\n",
    "    \"\"\"\n",
    "    def __init__(self,noisy_data,\n",
    "                 name: str = 'Cora',\n",
    "                 use_lcc: bool = True,\n",
    "                 alpha: float = 0.1,\n",
    "                 k: int = 16,\n",
    "                 eps: float = None):\n",
    "        self.name = name\n",
    "        self.use_lcc = use_lcc\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "        self.eps = eps\n",
    "        self.noisy_data = noisy_data\n",
    "\n",
    "        super(PPRDataset, self).__init__(DATA_PATH)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> list:\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> list:\n",
    "        return [str(self) + '.pt']\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # base = get_dataset(name=self.name, use_lcc=self.use_lcc)\n",
    "        # generate adjacency matrix from sparse representation\n",
    "        adj_matrix = get_adj_matrix(self.noisy_data)\n",
    "        # obtain exact PPR matrix\n",
    "        ppr_matrix = get_ppr_matrix(adj_matrix,\n",
    "                                        alpha=self.alpha)\n",
    "\n",
    "        if self.k:\n",
    "            print(f'Selecting top {self.k} edges per node.')\n",
    "            ppr_matrix = get_top_k_matrix(ppr_matrix, k=self.k)\n",
    "        elif self.eps:\n",
    "            print(f'Selecting edges with weight greater than {self.eps}.')\n",
    "            ppr_matrix = get_clipped_matrix(ppr_matrix, eps=self.eps)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        # create PyG Data object\n",
    "        edges_i = []\n",
    "        edges_j = []\n",
    "        edge_attr = []\n",
    "        for i, row in enumerate(ppr_matrix):\n",
    "            for j in np.where(row > 0)[0]:\n",
    "                edges_i.append(i)\n",
    "                edges_j.append(j)\n",
    "                edge_attr.append(ppr_matrix[i, j])\n",
    "        edge_index = [edges_i, edges_j]\n",
    "\n",
    "        data = Data(\n",
    "            x=self.noisy_data.x,\n",
    "            edge_index=torch.LongTensor(edge_index),\n",
    "            edge_attr=torch.FloatTensor(edge_attr),\n",
    "            y=self.noisy_data.y,\n",
    "            train_mask=torch.zeros(self.noisy_data.train_mask.size()[0], dtype=torch.bool),\n",
    "            test_mask=torch.zeros(self.noisy_data.test_mask.size()[0], dtype=torch.bool),\n",
    "            val_mask=torch.zeros(self.noisy_data.val_mask.size()[0], dtype=torch.bool)\n",
    "        )\n",
    "\n",
    "        data, slices = self.collate([data])\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f'{self.name}_ppr_alpha={self.alpha}_k={self.k}_eps={self.eps}_lcc={self.use_lcc}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unify Contrastive GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.num_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw graph: torch.Size([2, 88648])\n",
      "add edge: torch.Size([2, 110790])\n",
      "remove edge: torch.Size([2, 66616])\n",
      "updated graph: torch.Size([2, 88758])\n",
      "=== Noisy graph ===\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 1059.32080078125 = 1.0883028507232666 + 100 * 10.58232593536377\n",
      "Epoch 0, val loss: 1.0863984823226929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/project-robust-contrastive/Robust-Contrastive-Learning/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.labels = torch.tensor(labels, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, training loss: 1059.3094482421875 = 1.0856796503067017 + 100 * 10.582237243652344\n",
      "Epoch 10, val loss: 1.083805799484253\n",
      "Epoch 20, training loss: 1059.28271484375 = 1.0830211639404297 + 100 * 10.58199691772461\n",
      "Epoch 20, val loss: 1.0812100172042847\n",
      "Epoch 30, training loss: 1059.2127685546875 = 1.0803284645080566 + 100 * 10.581324577331543\n",
      "Epoch 30, val loss: 1.0786203145980835\n",
      "Epoch 40, training loss: 1059.0184326171875 = 1.0774977207183838 + 100 * 10.5794095993042\n",
      "Epoch 40, val loss: 1.0759011507034302\n",
      "Epoch 50, training loss: 1058.4825439453125 = 1.0742725133895874 + 100 * 10.57408332824707\n",
      "Epoch 50, val loss: 1.072792410850525\n",
      "Epoch 60, training loss: 1057.058837890625 = 1.0705451965332031 + 100 * 10.559883117675781\n",
      "Epoch 60, val loss: 1.0692049264907837\n",
      "Epoch 70, training loss: 1053.510498046875 = 1.0663251876831055 + 100 * 10.524442672729492\n",
      "Epoch 70, val loss: 1.0651582479476929\n",
      "Epoch 80, training loss: 1045.495849609375 = 1.06161367893219 + 100 * 10.444342613220215\n",
      "Epoch 80, val loss: 1.0606650114059448\n",
      "Epoch 90, training loss: 1030.4530029296875 = 1.0564534664154053 + 100 * 10.293966293334961\n",
      "Epoch 90, val loss: 1.0557657480239868\n",
      "Epoch 100, training loss: 1010.1443481445312 = 1.0510708093643188 + 100 * 10.090932846069336\n",
      "Epoch 100, val loss: 1.05071222782135\n",
      "Epoch 110, training loss: 992.9030151367188 = 1.0464707612991333 + 100 * 9.91856575012207\n",
      "Epoch 110, val loss: 1.046509027481079\n",
      "Epoch 120, training loss: 985.3453979492188 = 1.0437462329864502 + 100 * 9.843016624450684\n",
      "Epoch 120, val loss: 1.0441335439682007\n",
      "Epoch 130, training loss: 978.8861694335938 = 1.0428942441940308 + 100 * 9.778432846069336\n",
      "Epoch 130, val loss: 1.0433671474456787\n",
      "Epoch 140, training loss: 972.9596557617188 = 1.0424736738204956 + 100 * 9.719171524047852\n",
      "Epoch 140, val loss: 1.0429127216339111\n",
      "Epoch 150, training loss: 966.737548828125 = 1.042251706123352 + 100 * 9.656952857971191\n",
      "Epoch 150, val loss: 1.0426805019378662\n",
      "Epoch 160, training loss: 958.7102661132812 = 1.0423822402954102 + 100 * 9.576679229736328\n",
      "Epoch 160, val loss: 1.0428194999694824\n",
      "Epoch 170, training loss: 950.583740234375 = 1.0425347089767456 + 100 * 9.49541187286377\n",
      "Epoch 170, val loss: 1.0429306030273438\n",
      "Epoch 180, training loss: 945.0806274414062 = 1.0423619747161865 + 100 * 9.440382957458496\n",
      "Epoch 180, val loss: 1.0427097082138062\n",
      "Epoch 190, training loss: 942.5933837890625 = 1.0418262481689453 + 100 * 9.415515899658203\n",
      "Epoch 190, val loss: 1.0421377420425415\n",
      "Epoch 200, training loss: 941.3863525390625 = 1.0409197807312012 + 100 * 9.403454780578613\n",
      "Epoch 200, val loss: 1.0412492752075195\n",
      "Epoch 210, training loss: 940.3580932617188 = 1.0399280786514282 + 100 * 9.393181800842285\n",
      "Epoch 210, val loss: 1.0403106212615967\n",
      "Epoch 220, training loss: 939.3471069335938 = 1.038936734199524 + 100 * 9.383081436157227\n",
      "Epoch 220, val loss: 1.03935968875885\n",
      "Epoch 230, training loss: 938.2557983398438 = 1.0380717515945435 + 100 * 9.372177124023438\n",
      "Epoch 230, val loss: 1.038541316986084\n",
      "Epoch 240, training loss: 936.9208374023438 = 1.0374315977096558 + 100 * 9.358834266662598\n",
      "Epoch 240, val loss: 1.0379389524459839\n",
      "Epoch 250, training loss: 935.26513671875 = 1.036994218826294 + 100 * 9.342281341552734\n",
      "Epoch 250, val loss: 1.0375239849090576\n",
      "Epoch 260, training loss: 933.04052734375 = 1.036624550819397 + 100 * 9.320038795471191\n",
      "Epoch 260, val loss: 1.0371878147125244\n",
      "Epoch 270, training loss: 929.75048828125 = 1.0365949869155884 + 100 * 9.287138938903809\n",
      "Epoch 270, val loss: 1.0372194051742554\n",
      "Epoch 280, training loss: 925.0067138671875 = 1.0370172262191772 + 100 * 9.239697456359863\n",
      "Epoch 280, val loss: 1.037680745124817\n",
      "Epoch 290, training loss: 919.0576782226562 = 1.0377919673919678 + 100 * 9.180198669433594\n",
      "Epoch 290, val loss: 1.0384474992752075\n",
      "Epoch 300, training loss: 913.320068359375 = 1.0387656688690186 + 100 * 9.12281322479248\n",
      "Epoch 300, val loss: 1.0393863916397095\n",
      "Epoch 310, training loss: 908.5104370117188 = 1.039571762084961 + 100 * 9.074708938598633\n",
      "Epoch 310, val loss: 1.0401647090911865\n",
      "Epoch 320, training loss: 903.6830444335938 = 1.040073037147522 + 100 * 9.026429176330566\n",
      "Epoch 320, val loss: 1.0406574010849\n",
      "Epoch 330, training loss: 899.0038452148438 = 1.040333867073059 + 100 * 8.979635238647461\n",
      "Epoch 330, val loss: 1.0409965515136719\n",
      "Epoch 340, training loss: 895.1890258789062 = 1.0404136180877686 + 100 * 8.941486358642578\n",
      "Epoch 340, val loss: 1.041094183921814\n",
      "Epoch 350, training loss: 892.6631469726562 = 1.0402309894561768 + 100 * 8.916229248046875\n",
      "Epoch 350, val loss: 1.0409634113311768\n",
      "Epoch 360, training loss: 891.072998046875 = 1.0398718118667603 + 100 * 8.900331497192383\n",
      "Epoch 360, val loss: 1.0406873226165771\n",
      "Epoch 370, training loss: 889.9005126953125 = 1.0395230054855347 + 100 * 8.888609886169434\n",
      "Epoch 370, val loss: 1.0404117107391357\n",
      "Epoch 380, training loss: 888.8028564453125 = 1.0392401218414307 + 100 * 8.877635955810547\n",
      "Epoch 380, val loss: 1.0402024984359741\n",
      "Epoch 390, training loss: 887.67333984375 = 1.0389825105667114 + 100 * 8.86634349822998\n",
      "Epoch 390, val loss: 1.0400053262710571\n",
      "Epoch 400, training loss: 886.4691772460938 = 1.0387550592422485 + 100 * 8.854304313659668\n",
      "Epoch 400, val loss: 1.0398468971252441\n",
      "Epoch 410, training loss: 885.2428588867188 = 1.03854501247406 + 100 * 8.842042922973633\n",
      "Epoch 410, val loss: 1.0396931171417236\n",
      "Epoch 420, training loss: 884.1219482421875 = 1.0383158922195435 + 100 * 8.830836296081543\n",
      "Epoch 420, val loss: 1.0395251512527466\n",
      "Epoch 430, training loss: 883.2348022460938 = 1.0380244255065918 + 100 * 8.821968078613281\n",
      "Epoch 430, val loss: 1.0392943620681763\n",
      "Epoch 440, training loss: 882.50927734375 = 1.0376663208007812 + 100 * 8.814716339111328\n",
      "Epoch 440, val loss: 1.0389984846115112\n",
      "Epoch 450, training loss: 881.9285888671875 = 1.037278652191162 + 100 * 8.808913230895996\n",
      "Epoch 450, val loss: 1.0386780500411987\n",
      "Epoch 460, training loss: 881.4658203125 = 1.0368754863739014 + 100 * 8.804289817810059\n",
      "Epoch 460, val loss: 1.0383498668670654\n",
      "Epoch 470, training loss: 880.8854370117188 = 1.0364842414855957 + 100 * 8.798489570617676\n",
      "Epoch 470, val loss: 1.0380189418792725\n",
      "Epoch 480, training loss: 880.2997436523438 = 1.0361254215240479 + 100 * 8.792635917663574\n",
      "Epoch 480, val loss: 1.0377148389816284\n",
      "Epoch 490, training loss: 879.6541748046875 = 1.0357927083969116 + 100 * 8.786184310913086\n",
      "Epoch 490, val loss: 1.0374490022659302\n",
      "Epoch 500, training loss: 879.0004272460938 = 1.035498857498169 + 100 * 8.779648780822754\n",
      "Epoch 500, val loss: 1.037231683731079\n",
      "Epoch 510, training loss: 878.0673217773438 = 1.035231590270996 + 100 * 8.770320892333984\n",
      "Epoch 510, val loss: 1.0370020866394043\n",
      "Epoch 520, training loss: 877.1231079101562 = 1.0350288152694702 + 100 * 8.760880470275879\n",
      "Epoch 520, val loss: 1.0368496179580688\n",
      "Epoch 530, training loss: 876.0723266601562 = 1.034865379333496 + 100 * 8.750374794006348\n",
      "Epoch 530, val loss: 1.0367212295532227\n",
      "Epoch 540, training loss: 875.03662109375 = 1.0347144603729248 + 100 * 8.740018844604492\n",
      "Epoch 540, val loss: 1.036600947380066\n",
      "Epoch 550, training loss: 874.0408935546875 = 1.0345250368118286 + 100 * 8.730063438415527\n",
      "Epoch 550, val loss: 1.0364463329315186\n",
      "Epoch 560, training loss: 873.151123046875 = 1.0342826843261719 + 100 * 8.721168518066406\n",
      "Epoch 560, val loss: 1.03623628616333\n",
      "Epoch 570, training loss: 872.4224243164062 = 1.0339760780334473 + 100 * 8.713884353637695\n",
      "Epoch 570, val loss: 1.0359848737716675\n",
      "Epoch 580, training loss: 871.8622436523438 = 1.0336021184921265 + 100 * 8.70828628540039\n",
      "Epoch 580, val loss: 1.0356601476669312\n",
      "Epoch 590, training loss: 871.4190063476562 = 1.0331668853759766 + 100 * 8.703858375549316\n",
      "Epoch 590, val loss: 1.0352869033813477\n",
      "Epoch 600, training loss: 870.9840087890625 = 1.0326807498931885 + 100 * 8.69951343536377\n",
      "Epoch 600, val loss: 1.034845232963562\n",
      "Epoch 610, training loss: 870.5795288085938 = 1.0321860313415527 + 100 * 8.695473670959473\n",
      "Epoch 610, val loss: 1.0344406366348267\n",
      "Epoch 620, training loss: 870.1942138671875 = 1.0317025184631348 + 100 * 8.691625595092773\n",
      "Epoch 620, val loss: 1.0340209007263184\n",
      "Epoch 630, training loss: 869.8508911132812 = 1.0311938524246216 + 100 * 8.688197135925293\n",
      "Epoch 630, val loss: 1.0335798263549805\n",
      "Epoch 640, training loss: 869.5507202148438 = 1.0306559801101685 + 100 * 8.685200691223145\n",
      "Epoch 640, val loss: 1.0331065654754639\n",
      "Epoch 650, training loss: 869.2957153320312 = 1.0300817489624023 + 100 * 8.682656288146973\n",
      "Epoch 650, val loss: 1.0325994491577148\n",
      "Epoch 660, training loss: 869.0767211914062 = 1.0294737815856934 + 100 * 8.680472373962402\n",
      "Epoch 660, val loss: 1.032057285308838\n",
      "Epoch 670, training loss: 868.88720703125 = 1.0288342237472534 + 100 * 8.678584098815918\n",
      "Epoch 670, val loss: 1.0314843654632568\n",
      "Epoch 680, training loss: 868.7259521484375 = 1.0281633138656616 + 100 * 8.67697811126709\n",
      "Epoch 680, val loss: 1.0308785438537598\n",
      "Epoch 690, training loss: 868.9640502929688 = 1.0274685621261597 + 100 * 8.679366111755371\n",
      "Epoch 690, val loss: 1.0302003622055054\n",
      "Epoch 700, training loss: 868.5255737304688 = 1.0266658067703247 + 100 * 8.674988746643066\n",
      "Epoch 700, val loss: 1.0295096635818481\n",
      "Epoch 710, training loss: 868.369384765625 = 1.0259389877319336 + 100 * 8.673434257507324\n",
      "Epoch 710, val loss: 1.028875708580017\n",
      "Epoch 720, training loss: 868.2232666015625 = 1.025193691253662 + 100 * 8.671980857849121\n",
      "Epoch 720, val loss: 1.0281907320022583\n",
      "Epoch 730, training loss: 868.1025390625 = 1.0244219303131104 + 100 * 8.670781135559082\n",
      "Epoch 730, val loss: 1.0274969339370728\n",
      "Epoch 740, training loss: 867.986328125 = 1.0236471891403198 + 100 * 8.66962718963623\n",
      "Epoch 740, val loss: 1.026800513267517\n",
      "Epoch 750, training loss: 867.872314453125 = 1.022856593132019 + 100 * 8.668495178222656\n",
      "Epoch 750, val loss: 1.0260875225067139\n",
      "Epoch 760, training loss: 867.75732421875 = 1.0220489501953125 + 100 * 8.667352676391602\n",
      "Epoch 760, val loss: 1.0253595113754272\n",
      "Epoch 770, training loss: 867.640625 = 1.0212332010269165 + 100 * 8.666193962097168\n",
      "Epoch 770, val loss: 1.0246248245239258\n",
      "Epoch 780, training loss: 867.5219116210938 = 1.0204027891159058 + 100 * 8.66501522064209\n",
      "Epoch 780, val loss: 1.0238783359527588\n",
      "Epoch 790, training loss: 867.4649658203125 = 1.0195634365081787 + 100 * 8.664453506469727\n",
      "Epoch 790, val loss: 1.0231200456619263\n",
      "Epoch 800, training loss: 867.3345947265625 = 1.0186810493469238 + 100 * 8.663159370422363\n",
      "Epoch 800, val loss: 1.0223278999328613\n",
      "Epoch 810, training loss: 867.1553955078125 = 1.0178165435791016 + 100 * 8.661375999450684\n",
      "Epoch 810, val loss: 1.0215506553649902\n",
      "Epoch 820, training loss: 867.03662109375 = 1.0169517993927002 + 100 * 8.660196304321289\n",
      "Epoch 820, val loss: 1.0207780599594116\n",
      "Epoch 830, training loss: 866.8904418945312 = 1.0160796642303467 + 100 * 8.658743858337402\n",
      "Epoch 830, val loss: 1.0199923515319824\n",
      "Epoch 840, training loss: 866.74365234375 = 1.0152041912078857 + 100 * 8.6572847366333\n",
      "Epoch 840, val loss: 1.0192043781280518\n",
      "Epoch 850, training loss: 866.7678833007812 = 1.014323353767395 + 100 * 8.657535552978516\n",
      "Epoch 850, val loss: 1.0183804035186768\n",
      "Epoch 860, training loss: 866.4944458007812 = 1.0133463144302368 + 100 * 8.654810905456543\n",
      "Epoch 860, val loss: 1.0175418853759766\n",
      "Epoch 870, training loss: 866.2936401367188 = 1.0124626159667969 + 100 * 8.652812004089355\n",
      "Epoch 870, val loss: 1.0167466402053833\n",
      "Epoch 880, training loss: 866.09521484375 = 1.0115762948989868 + 100 * 8.650835990905762\n",
      "Epoch 880, val loss: 1.015939712524414\n",
      "Epoch 890, training loss: 865.890380859375 = 1.0106741189956665 + 100 * 8.648797035217285\n",
      "Epoch 890, val loss: 1.015124797821045\n",
      "Epoch 900, training loss: 865.6851196289062 = 1.0097770690917969 + 100 * 8.646753311157227\n",
      "Epoch 900, val loss: 1.0143160820007324\n",
      "Epoch 910, training loss: 865.474365234375 = 1.0088633298873901 + 100 * 8.644655227661133\n",
      "Epoch 910, val loss: 1.0134947299957275\n",
      "Epoch 920, training loss: 865.4618530273438 = 1.0078903436660767 + 100 * 8.644539833068848\n",
      "Epoch 920, val loss: 1.0126227140426636\n",
      "Epoch 930, training loss: 865.0693969726562 = 1.0068992376327515 + 100 * 8.640625\n",
      "Epoch 930, val loss: 1.011716365814209\n",
      "Epoch 940, training loss: 864.8442993164062 = 1.0059627294540405 + 100 * 8.638382911682129\n",
      "Epoch 940, val loss: 1.0108543634414673\n",
      "Epoch 950, training loss: 864.648681640625 = 1.0049666166305542 + 100 * 8.63643741607666\n",
      "Epoch 950, val loss: 1.0099544525146484\n",
      "Epoch 960, training loss: 864.4698486328125 = 1.0039466619491577 + 100 * 8.634658813476562\n",
      "Epoch 960, val loss: 1.0090385675430298\n",
      "Epoch 970, training loss: 864.3489379882812 = 1.0029041767120361 + 100 * 8.63346004486084\n",
      "Epoch 970, val loss: 1.0080974102020264\n",
      "Epoch 980, training loss: 864.21533203125 = 1.0017828941345215 + 100 * 8.632135391235352\n",
      "Epoch 980, val loss: 1.0070569515228271\n",
      "Epoch 990, training loss: 864.0342407226562 = 1.000667929649353 + 100 * 8.630335807800293\n",
      "Epoch 990, val loss: 1.0060510635375977\n",
      "Epoch 1000, training loss: 863.9103393554688 = 0.9995298385620117 + 100 * 8.629108428955078\n",
      "Epoch 1000, val loss: 1.0050288438796997\n",
      "Epoch 1010, training loss: 863.7804565429688 = 0.9983577132225037 + 100 * 8.62782096862793\n",
      "Epoch 1010, val loss: 1.003961443901062\n",
      "Epoch 1020, training loss: 863.6793212890625 = 0.9971717000007629 + 100 * 8.626821517944336\n",
      "Epoch 1020, val loss: 1.002876877784729\n",
      "Epoch 1030, training loss: 863.599853515625 = 0.9959081411361694 + 100 * 8.626039505004883\n",
      "Epoch 1030, val loss: 1.0017266273498535\n",
      "Epoch 1040, training loss: 863.3929443359375 = 0.9946209788322449 + 100 * 8.623983383178711\n",
      "Epoch 1040, val loss: 1.0005855560302734\n",
      "Epoch 1050, training loss: 863.2648315429688 = 0.9933417439460754 + 100 * 8.62271499633789\n",
      "Epoch 1050, val loss: 0.9994625449180603\n",
      "Epoch 1060, training loss: 863.1052856445312 = 0.9920292496681213 + 100 * 8.621132850646973\n",
      "Epoch 1060, val loss: 0.9982811808586121\n",
      "Epoch 1070, training loss: 862.9376220703125 = 0.99069744348526 + 100 * 8.619468688964844\n",
      "Epoch 1070, val loss: 0.9971152544021606\n",
      "Epoch 1080, training loss: 862.9296875 = 0.989335834980011 + 100 * 8.619403839111328\n",
      "Epoch 1080, val loss: 0.9958943128585815\n",
      "Epoch 1090, training loss: 862.648193359375 = 0.9879408478736877 + 100 * 8.616602897644043\n",
      "Epoch 1090, val loss: 0.9946670532226562\n",
      "Epoch 1100, training loss: 862.418212890625 = 0.9866173267364502 + 100 * 8.6143159866333\n",
      "Epoch 1100, val loss: 0.9934906363487244\n",
      "Epoch 1110, training loss: 862.2152709960938 = 0.9853212833404541 + 100 * 8.612298965454102\n",
      "Epoch 1110, val loss: 0.9923334717750549\n",
      "Epoch 1120, training loss: 862.253173828125 = 0.984038770198822 + 100 * 8.612691879272461\n",
      "Epoch 1120, val loss: 0.9911680817604065\n",
      "Epoch 1130, training loss: 861.907958984375 = 0.9826943278312683 + 100 * 8.6092529296875\n",
      "Epoch 1130, val loss: 0.990006148815155\n",
      "Epoch 1140, training loss: 861.6934814453125 = 0.9814608693122864 + 100 * 8.607120513916016\n",
      "Epoch 1140, val loss: 0.9888963103294373\n",
      "Epoch 1150, training loss: 861.4945068359375 = 0.9802358150482178 + 100 * 8.605142593383789\n",
      "Epoch 1150, val loss: 0.9877970218658447\n",
      "Epoch 1160, training loss: 861.3442993164062 = 0.9790056943893433 + 100 * 8.603652954101562\n",
      "Epoch 1160, val loss: 0.986689567565918\n",
      "Epoch 1170, training loss: 861.1970825195312 = 0.9777818322181702 + 100 * 8.602192878723145\n",
      "Epoch 1170, val loss: 0.9855894446372986\n",
      "Epoch 1180, training loss: 861.2451782226562 = 0.9765337705612183 + 100 * 8.602685928344727\n",
      "Epoch 1180, val loss: 0.9844780564308167\n",
      "Epoch 1190, training loss: 861.076171875 = 0.9751717448234558 + 100 * 8.6010103225708\n",
      "Epoch 1190, val loss: 0.9831922650337219\n",
      "Epoch 1200, training loss: 860.8414306640625 = 0.973884642124176 + 100 * 8.598675727844238\n",
      "Epoch 1200, val loss: 0.9820497035980225\n",
      "Epoch 1210, training loss: 860.707763671875 = 0.9726306200027466 + 100 * 8.59735107421875\n",
      "Epoch 1210, val loss: 0.9809272289276123\n",
      "Epoch 1220, training loss: 860.5863037109375 = 0.9713671803474426 + 100 * 8.596149444580078\n",
      "Epoch 1220, val loss: 0.9797892570495605\n",
      "Epoch 1230, training loss: 860.473388671875 = 0.9701062440872192 + 100 * 8.595032691955566\n",
      "Epoch 1230, val loss: 0.9786463379859924\n",
      "Epoch 1240, training loss: 860.4014892578125 = 0.9688242673873901 + 100 * 8.594326972961426\n",
      "Epoch 1240, val loss: 0.9774878621101379\n",
      "Epoch 1250, training loss: 860.3237915039062 = 0.9674690961837769 + 100 * 8.593563079833984\n",
      "Epoch 1250, val loss: 0.976249098777771\n",
      "Epoch 1260, training loss: 860.2158813476562 = 0.9661023020744324 + 100 * 8.592497825622559\n",
      "Epoch 1260, val loss: 0.9750311970710754\n",
      "Epoch 1270, training loss: 860.125 = 0.9647611379623413 + 100 * 8.591602325439453\n",
      "Epoch 1270, val loss: 0.9738302826881409\n",
      "Epoch 1280, training loss: 860.0467529296875 = 0.963402271270752 + 100 * 8.59083366394043\n",
      "Epoch 1280, val loss: 0.9725978970527649\n",
      "Epoch 1290, training loss: 859.982666015625 = 0.9620308876037598 + 100 * 8.590206146240234\n",
      "Epoch 1290, val loss: 0.9713590145111084\n",
      "Epoch 1300, training loss: 859.9168090820312 = 0.9606395363807678 + 100 * 8.589561462402344\n",
      "Epoch 1300, val loss: 0.9701071977615356\n",
      "Epoch 1310, training loss: 859.857666015625 = 0.9592321515083313 + 100 * 8.588984489440918\n",
      "Epoch 1310, val loss: 0.9688359498977661\n",
      "Epoch 1320, training loss: 859.9286499023438 = 0.9578053951263428 + 100 * 8.58970832824707\n",
      "Epoch 1320, val loss: 0.9675302505493164\n",
      "Epoch 1330, training loss: 859.9353637695312 = 0.9562320709228516 + 100 * 8.589791297912598\n",
      "Epoch 1330, val loss: 0.9661151766777039\n",
      "Epoch 1340, training loss: 859.7059326171875 = 0.9547354578971863 + 100 * 8.587512016296387\n",
      "Epoch 1340, val loss: 0.9647812247276306\n",
      "Epoch 1350, training loss: 859.6444091796875 = 0.9533081650733948 + 100 * 8.58691120147705\n",
      "Epoch 1350, val loss: 0.9634836316108704\n",
      "Epoch 1360, training loss: 859.5949096679688 = 0.9518638253211975 + 100 * 8.586430549621582\n",
      "Epoch 1360, val loss: 0.9621796607971191\n",
      "Epoch 1370, training loss: 859.5436401367188 = 0.9504226446151733 + 100 * 8.585931777954102\n",
      "Epoch 1370, val loss: 0.9608767032623291\n",
      "Epoch 1380, training loss: 859.48828125 = 0.948973536491394 + 100 * 8.585392951965332\n",
      "Epoch 1380, val loss: 0.9595688581466675\n",
      "Epoch 1390, training loss: 859.4442749023438 = 0.9475141167640686 + 100 * 8.584967613220215\n",
      "Epoch 1390, val loss: 0.9582552909851074\n",
      "Epoch 1400, training loss: 859.6709594726562 = 0.9460079073905945 + 100 * 8.587249755859375\n",
      "Epoch 1400, val loss: 0.9568964838981628\n",
      "Epoch 1410, training loss: 859.3565063476562 = 0.9444414377212524 + 100 * 8.584120750427246\n",
      "Epoch 1410, val loss: 0.9554683566093445\n",
      "Epoch 1420, training loss: 859.3003540039062 = 0.942986249923706 + 100 * 8.583573341369629\n",
      "Epoch 1420, val loss: 0.9541438817977905\n",
      "Epoch 1430, training loss: 859.2362060546875 = 0.9415429830551147 + 100 * 8.58294677734375\n",
      "Epoch 1430, val loss: 0.9528470635414124\n",
      "Epoch 1440, training loss: 859.17578125 = 0.9400960803031921 + 100 * 8.582356452941895\n",
      "Epoch 1440, val loss: 0.9515396356582642\n",
      "Epoch 1450, training loss: 859.1226196289062 = 0.9386518001556396 + 100 * 8.581839561462402\n",
      "Epoch 1450, val loss: 0.9502301812171936\n",
      "Epoch 1460, training loss: 859.2326049804688 = 0.9371981620788574 + 100 * 8.582954406738281\n",
      "Epoch 1460, val loss: 0.948911726474762\n",
      "Epoch 1470, training loss: 859.05517578125 = 0.9356670379638672 + 100 * 8.581194877624512\n",
      "Epoch 1470, val loss: 0.9475154280662537\n",
      "Epoch 1480, training loss: 858.96826171875 = 0.9342109560966492 + 100 * 8.580340385437012\n",
      "Epoch 1480, val loss: 0.9462181925773621\n",
      "Epoch 1490, training loss: 858.8973999023438 = 0.9327905774116516 + 100 * 8.579646110534668\n",
      "Epoch 1490, val loss: 0.9449244141578674\n",
      "Epoch 1500, training loss: 858.83544921875 = 0.9313744902610779 + 100 * 8.57904052734375\n",
      "Epoch 1500, val loss: 0.9436442255973816\n",
      "Epoch 1510, training loss: 858.806884765625 = 0.9299637675285339 + 100 * 8.578768730163574\n",
      "Epoch 1510, val loss: 0.9423759579658508\n",
      "Epoch 1520, training loss: 858.8369140625 = 0.9284805059432983 + 100 * 8.579084396362305\n",
      "Epoch 1520, val loss: 0.9410233497619629\n",
      "Epoch 1530, training loss: 858.6571044921875 = 0.927000105381012 + 100 * 8.577301025390625\n",
      "Epoch 1530, val loss: 0.9396754503250122\n",
      "Epoch 1540, training loss: 858.5986938476562 = 0.9256152510643005 + 100 * 8.576730728149414\n",
      "Epoch 1540, val loss: 0.938435971736908\n",
      "Epoch 1550, training loss: 858.51953125 = 0.9242410063743591 + 100 * 8.575952529907227\n",
      "Epoch 1550, val loss: 0.9371975660324097\n",
      "Epoch 1560, training loss: 858.4525756835938 = 0.9228665232658386 + 100 * 8.575297355651855\n",
      "Epoch 1560, val loss: 0.9359606504440308\n",
      "Epoch 1570, training loss: 858.5823974609375 = 0.9214698076248169 + 100 * 8.57660961151123\n",
      "Epoch 1570, val loss: 0.9347081184387207\n",
      "Epoch 1580, training loss: 858.395751953125 = 0.9199699759483337 + 100 * 8.57475757598877\n",
      "Epoch 1580, val loss: 0.933303713798523\n",
      "Epoch 1590, training loss: 858.2587280273438 = 0.9185892939567566 + 100 * 8.57340145111084\n",
      "Epoch 1590, val loss: 0.9320907592773438\n",
      "Epoch 1600, training loss: 858.168701171875 = 0.9172428846359253 + 100 * 8.572514533996582\n",
      "Epoch 1600, val loss: 0.9308719635009766\n",
      "Epoch 1610, training loss: 858.0943603515625 = 0.9158864617347717 + 100 * 8.571784973144531\n",
      "Epoch 1610, val loss: 0.929643988609314\n",
      "Epoch 1620, training loss: 858.06005859375 = 0.9145297408103943 + 100 * 8.571455001831055\n",
      "Epoch 1620, val loss: 0.9284235835075378\n",
      "Epoch 1630, training loss: 858.0821533203125 = 0.9130493402481079 + 100 * 8.571691513061523\n",
      "Epoch 1630, val loss: 0.92703777551651\n",
      "Epoch 1640, training loss: 857.9811401367188 = 0.9115363955497742 + 100 * 8.570695877075195\n",
      "Epoch 1640, val loss: 0.9257065653800964\n",
      "Epoch 1650, training loss: 857.8600463867188 = 0.9101736545562744 + 100 * 8.569499015808105\n",
      "Epoch 1650, val loss: 0.9244807362556458\n",
      "Epoch 1660, training loss: 857.7640991210938 = 0.9088159799575806 + 100 * 8.56855297088623\n",
      "Epoch 1660, val loss: 0.923251748085022\n",
      "Epoch 1670, training loss: 857.7008056640625 = 0.9074466824531555 + 100 * 8.567933082580566\n",
      "Epoch 1670, val loss: 0.9220039248466492\n",
      "Epoch 1680, training loss: 857.6485595703125 = 0.9060592651367188 + 100 * 8.567424774169922\n",
      "Epoch 1680, val loss: 0.9207489490509033\n",
      "Epoch 1690, training loss: 857.8726196289062 = 0.9046167135238647 + 100 * 8.569680213928223\n",
      "Epoch 1690, val loss: 0.9194416999816895\n",
      "Epoch 1700, training loss: 857.606689453125 = 0.9030255675315857 + 100 * 8.567036628723145\n",
      "Epoch 1700, val loss: 0.9179635047912598\n",
      "Epoch 1710, training loss: 857.4837036132812 = 0.9015828371047974 + 100 * 8.565820693969727\n",
      "Epoch 1710, val loss: 0.9166708588600159\n",
      "Epoch 1720, training loss: 857.4303588867188 = 0.9001525044441223 + 100 * 8.565301895141602\n",
      "Epoch 1720, val loss: 0.9153646230697632\n",
      "Epoch 1730, training loss: 857.369140625 = 0.8986974358558655 + 100 * 8.564704895019531\n",
      "Epoch 1730, val loss: 0.9140366911888123\n",
      "Epoch 1740, training loss: 857.3218994140625 = 0.8972294330596924 + 100 * 8.564247131347656\n",
      "Epoch 1740, val loss: 0.9126929640769958\n",
      "Epoch 1750, training loss: 857.4141845703125 = 0.8957340121269226 + 100 * 8.565184593200684\n",
      "Epoch 1750, val loss: 0.9113050103187561\n",
      "Epoch 1760, training loss: 857.2385864257812 = 0.8940895795822144 + 100 * 8.563445091247559\n",
      "Epoch 1760, val loss: 0.9098125696182251\n",
      "Epoch 1770, training loss: 857.203125 = 0.8925756216049194 + 100 * 8.563105583190918\n",
      "Epoch 1770, val loss: 0.9084311127662659\n",
      "Epoch 1780, training loss: 857.1366577148438 = 0.8910921812057495 + 100 * 8.562455177307129\n",
      "Epoch 1780, val loss: 0.9070736765861511\n",
      "Epoch 1790, training loss: 857.0733032226562 = 0.8896015286445618 + 100 * 8.561837196350098\n",
      "Epoch 1790, val loss: 0.9057064652442932\n",
      "Epoch 1800, training loss: 857.019287109375 = 0.8881091475486755 + 100 * 8.561311721801758\n",
      "Epoch 1800, val loss: 0.9043416380882263\n",
      "Epoch 1810, training loss: 856.98974609375 = 0.8866147994995117 + 100 * 8.561031341552734\n",
      "Epoch 1810, val loss: 0.9029750227928162\n",
      "Epoch 1820, training loss: 857.175048828125 = 0.8850165009498596 + 100 * 8.56290054321289\n",
      "Epoch 1820, val loss: 0.9014771580696106\n",
      "Epoch 1830, training loss: 856.9400634765625 = 0.8833544850349426 + 100 * 8.560566902160645\n",
      "Epoch 1830, val loss: 0.8999702334403992\n",
      "Epoch 1840, training loss: 856.8306274414062 = 0.8818765878677368 + 100 * 8.559487342834473\n",
      "Epoch 1840, val loss: 0.898611307144165\n",
      "Epoch 1850, training loss: 856.7786865234375 = 0.8803919553756714 + 100 * 8.558982849121094\n",
      "Epoch 1850, val loss: 0.8972383737564087\n",
      "Epoch 1860, training loss: 856.7120361328125 = 0.8789035081863403 + 100 * 8.558331489562988\n",
      "Epoch 1860, val loss: 0.895871102809906\n",
      "Epoch 1870, training loss: 856.66455078125 = 0.8774228096008301 + 100 * 8.557870864868164\n",
      "Epoch 1870, val loss: 0.8945093154907227\n",
      "Epoch 1880, training loss: 856.689697265625 = 0.8759281635284424 + 100 * 8.558137893676758\n",
      "Epoch 1880, val loss: 0.8931140303611755\n",
      "Epoch 1890, training loss: 856.5929565429688 = 0.8743395209312439 + 100 * 8.557186126708984\n",
      "Epoch 1890, val loss: 0.8916577696800232\n",
      "Epoch 1900, training loss: 856.7564086914062 = 0.8727778196334839 + 100 * 8.558835983276367\n",
      "Epoch 1900, val loss: 0.8901854753494263\n",
      "Epoch 1910, training loss: 856.5130004882812 = 0.8711643815040588 + 100 * 8.556418418884277\n",
      "Epoch 1910, val loss: 0.8887495994567871\n",
      "Epoch 1920, training loss: 856.428955078125 = 0.8696747422218323 + 100 * 8.55559253692627\n",
      "Epoch 1920, val loss: 0.8873662948608398\n",
      "Epoch 1930, training loss: 856.370849609375 = 0.8682050108909607 + 100 * 8.555026054382324\n",
      "Epoch 1930, val loss: 0.8860212564468384\n",
      "Epoch 1940, training loss: 856.3223876953125 = 0.8667236566543579 + 100 * 8.554556846618652\n",
      "Epoch 1940, val loss: 0.8846563696861267\n",
      "Epoch 1950, training loss: 856.2813720703125 = 0.8652297258377075 + 100 * 8.554161071777344\n",
      "Epoch 1950, val loss: 0.8832768797874451\n",
      "Epoch 1960, training loss: 856.4271850585938 = 0.8636979460716248 + 100 * 8.555634498596191\n",
      "Epoch 1960, val loss: 0.8818491697311401\n",
      "Epoch 1970, training loss: 856.3199462890625 = 0.8620579242706299 + 100 * 8.55457878112793\n",
      "Epoch 1970, val loss: 0.8803242444992065\n",
      "Epoch 1980, training loss: 856.17529296875 = 0.8605137467384338 + 100 * 8.553147315979004\n",
      "Epoch 1980, val loss: 0.8789345026016235\n",
      "Epoch 1990, training loss: 856.1171875 = 0.8590226173400879 + 100 * 8.552581787109375\n",
      "Epoch 1990, val loss: 0.8775600790977478\n",
      "Epoch 2000, training loss: 856.0711059570312 = 0.8575167059898376 + 100 * 8.552135467529297\n",
      "Epoch 2000, val loss: 0.8761732578277588\n",
      "Epoch 2010, training loss: 856.040771484375 = 0.8560165762901306 + 100 * 8.551847457885742\n",
      "Epoch 2010, val loss: 0.8747953176498413\n",
      "Epoch 2020, training loss: 856.3386840820312 = 0.8544621467590332 + 100 * 8.554841995239258\n",
      "Epoch 2020, val loss: 0.8733693361282349\n",
      "Epoch 2030, training loss: 856.09326171875 = 0.8527123332023621 + 100 * 8.55240535736084\n",
      "Epoch 2030, val loss: 0.8717072606086731\n",
      "Epoch 2040, training loss: 855.9510498046875 = 0.8511908054351807 + 100 * 8.55099868774414\n",
      "Epoch 2040, val loss: 0.8703404664993286\n",
      "Epoch 2050, training loss: 855.8988647460938 = 0.8497040271759033 + 100 * 8.550491333007812\n",
      "Epoch 2050, val loss: 0.8689693808555603\n",
      "Epoch 2060, training loss: 855.8626098632812 = 0.8481966853141785 + 100 * 8.55014419555664\n",
      "Epoch 2060, val loss: 0.8675731420516968\n",
      "Epoch 2070, training loss: 855.8257446289062 = 0.8466829657554626 + 100 * 8.549790382385254\n",
      "Epoch 2070, val loss: 0.866178035736084\n",
      "Epoch 2080, training loss: 855.7925415039062 = 0.8451602458953857 + 100 * 8.549473762512207\n",
      "Epoch 2080, val loss: 0.864770233631134\n",
      "Epoch 2090, training loss: 855.7637329101562 = 0.8436288237571716 + 100 * 8.549201011657715\n",
      "Epoch 2090, val loss: 0.863355815410614\n",
      "Epoch 2100, training loss: 855.8245849609375 = 0.8420822024345398 + 100 * 8.549824714660645\n",
      "Epoch 2100, val loss: 0.8619182705879211\n",
      "Epoch 2110, training loss: 855.8201904296875 = 0.8404138684272766 + 100 * 8.549798011779785\n",
      "Epoch 2110, val loss: 0.8603529930114746\n",
      "Epoch 2120, training loss: 855.7499389648438 = 0.8388097882270813 + 100 * 8.549111366271973\n",
      "Epoch 2120, val loss: 0.8589219450950623\n",
      "Epoch 2130, training loss: 855.64990234375 = 0.8372853398323059 + 100 * 8.548126220703125\n",
      "Epoch 2130, val loss: 0.8574893474578857\n",
      "Epoch 2140, training loss: 855.60693359375 = 0.8357961773872375 + 100 * 8.547711372375488\n",
      "Epoch 2140, val loss: 0.8561164736747742\n",
      "Epoch 2150, training loss: 855.5911254882812 = 0.8343032002449036 + 100 * 8.547568321228027\n",
      "Epoch 2150, val loss: 0.8547343015670776\n",
      "Epoch 2160, training loss: 855.6204223632812 = 0.8327807188034058 + 100 * 8.547876358032227\n",
      "Epoch 2160, val loss: 0.8533023595809937\n",
      "Epoch 2170, training loss: 855.5514526367188 = 0.8312236070632935 + 100 * 8.547202110290527\n",
      "Epoch 2170, val loss: 0.8519037961959839\n",
      "Epoch 2180, training loss: 855.56201171875 = 0.8297032117843628 + 100 * 8.547323226928711\n",
      "Epoch 2180, val loss: 0.8504757881164551\n",
      "Epoch 2190, training loss: 855.4400634765625 = 0.8281676173210144 + 100 * 8.54611873626709\n",
      "Epoch 2190, val loss: 0.8490824103355408\n",
      "Epoch 2200, training loss: 855.416259765625 = 0.8267073631286621 + 100 * 8.54589557647705\n",
      "Epoch 2200, val loss: 0.8477387428283691\n",
      "Epoch 2210, training loss: 855.3818359375 = 0.8252609372138977 + 100 * 8.545565605163574\n",
      "Epoch 2210, val loss: 0.846411406993866\n",
      "Epoch 2220, training loss: 855.5923461914062 = 0.8237905502319336 + 100 * 8.547685623168945\n",
      "Epoch 2220, val loss: 0.8450821042060852\n",
      "Epoch 2230, training loss: 855.4058837890625 = 0.8221198320388794 + 100 * 8.54583740234375\n",
      "Epoch 2230, val loss: 0.8434726595878601\n",
      "Epoch 2240, training loss: 855.2743530273438 = 0.8206596374511719 + 100 * 8.544536590576172\n",
      "Epoch 2240, val loss: 0.8421551585197449\n",
      "Epoch 2250, training loss: 855.2550048828125 = 0.819274365901947 + 100 * 8.544357299804688\n",
      "Epoch 2250, val loss: 0.8409037590026855\n",
      "Epoch 2260, training loss: 855.201904296875 = 0.8178693056106567 + 100 * 8.543840408325195\n",
      "Epoch 2260, val loss: 0.8395944833755493\n",
      "Epoch 2270, training loss: 855.1665649414062 = 0.8164801597595215 + 100 * 8.543500900268555\n",
      "Epoch 2270, val loss: 0.8383132219314575\n",
      "Epoch 2280, training loss: 855.1390380859375 = 0.8150897026062012 + 100 * 8.54323959350586\n",
      "Epoch 2280, val loss: 0.8370362520217896\n",
      "Epoch 2290, training loss: 855.3026733398438 = 0.8136675953865051 + 100 * 8.544890403747559\n",
      "Epoch 2290, val loss: 0.8356884121894836\n",
      "Epoch 2300, training loss: 855.0791015625 = 0.8120858669281006 + 100 * 8.542670249938965\n",
      "Epoch 2300, val loss: 0.8342714309692383\n",
      "Epoch 2310, training loss: 855.0413818359375 = 0.8106738328933716 + 100 * 8.542306900024414\n",
      "Epoch 2310, val loss: 0.832958996295929\n",
      "Epoch 2320, training loss: 855.000244140625 = 0.8093367218971252 + 100 * 8.541909217834473\n",
      "Epoch 2320, val loss: 0.8317490816116333\n",
      "Epoch 2330, training loss: 855.0048828125 = 0.8079830408096313 + 100 * 8.541969299316406\n",
      "Epoch 2330, val loss: 0.8304791450500488\n",
      "Epoch 2340, training loss: 854.9444580078125 = 0.8065665364265442 + 100 * 8.54137897491455\n",
      "Epoch 2340, val loss: 0.829181969165802\n",
      "Epoch 2350, training loss: 854.9102783203125 = 0.8051945567131042 + 100 * 8.541050910949707\n",
      "Epoch 2350, val loss: 0.8279405832290649\n",
      "Epoch 2360, training loss: 854.8699951171875 = 0.803858757019043 + 100 * 8.540661811828613\n",
      "Epoch 2360, val loss: 0.8267039656639099\n",
      "Epoch 2370, training loss: 854.9453735351562 = 0.8025026321411133 + 100 * 8.541428565979004\n",
      "Epoch 2370, val loss: 0.8254722952842712\n",
      "Epoch 2380, training loss: 854.8052368164062 = 0.8010377287864685 + 100 * 8.54004192352295\n",
      "Epoch 2380, val loss: 0.8240727782249451\n",
      "Epoch 2390, training loss: 854.7845458984375 = 0.7996571660041809 + 100 * 8.539848327636719\n",
      "Epoch 2390, val loss: 0.8228476643562317\n",
      "Epoch 2400, training loss: 854.7505493164062 = 0.7983302474021912 + 100 * 8.539522171020508\n",
      "Epoch 2400, val loss: 0.8216002583503723\n",
      "Epoch 2410, training loss: 854.7373046875 = 0.7969915866851807 + 100 * 8.539402961730957\n",
      "Epoch 2410, val loss: 0.8203995823860168\n",
      "Epoch 2420, training loss: 855.0687866210938 = 0.7955697774887085 + 100 * 8.542732238769531\n",
      "Epoch 2420, val loss: 0.8190826773643494\n",
      "Epoch 2430, training loss: 854.7800903320312 = 0.7940095663070679 + 100 * 8.539860725402832\n",
      "Epoch 2430, val loss: 0.8175857067108154\n",
      "Epoch 2440, training loss: 854.6565551757812 = 0.7926726937294006 + 100 * 8.538639068603516\n",
      "Epoch 2440, val loss: 0.8164000511169434\n",
      "Epoch 2450, training loss: 854.6114501953125 = 0.7913464903831482 + 100 * 8.538201332092285\n",
      "Epoch 2450, val loss: 0.8151639699935913\n",
      "Epoch 2460, training loss: 854.5856323242188 = 0.7900075912475586 + 100 * 8.537956237792969\n",
      "Epoch 2460, val loss: 0.8139321208000183\n",
      "Epoch 2470, training loss: 854.56201171875 = 0.7886620163917542 + 100 * 8.537734031677246\n",
      "Epoch 2470, val loss: 0.8126869201660156\n",
      "Epoch 2480, training loss: 854.540771484375 = 0.7873098850250244 + 100 * 8.537534713745117\n",
      "Epoch 2480, val loss: 0.8114444613456726\n",
      "Epoch 2490, training loss: 854.5236206054688 = 0.7859454154968262 + 100 * 8.537376403808594\n",
      "Epoch 2490, val loss: 0.8101844787597656\n",
      "Epoch 2500, training loss: 854.689453125 = 0.784548282623291 + 100 * 8.53904914855957\n",
      "Epoch 2500, val loss: 0.8089069128036499\n",
      "Epoch 2510, training loss: 854.6282348632812 = 0.7830469608306885 + 100 * 8.5384521484375\n",
      "Epoch 2510, val loss: 0.8074942827224731\n",
      "Epoch 2520, training loss: 854.4706420898438 = 0.7816193103790283 + 100 * 8.536890029907227\n",
      "Epoch 2520, val loss: 0.8061726093292236\n",
      "Epoch 2530, training loss: 854.4508056640625 = 0.7802709341049194 + 100 * 8.536705017089844\n",
      "Epoch 2530, val loss: 0.804923951625824\n",
      "Epoch 2540, training loss: 854.4203491210938 = 0.7789273858070374 + 100 * 8.53641414642334\n",
      "Epoch 2540, val loss: 0.8036897778511047\n",
      "Epoch 2550, training loss: 854.4017333984375 = 0.7775822281837463 + 100 * 8.53624153137207\n",
      "Epoch 2550, val loss: 0.8024458885192871\n",
      "Epoch 2560, training loss: 854.4237670898438 = 0.7762245535850525 + 100 * 8.53647518157959\n",
      "Epoch 2560, val loss: 0.8011939525604248\n",
      "Epoch 2570, training loss: 854.5573120117188 = 0.774795651435852 + 100 * 8.537825584411621\n",
      "Epoch 2570, val loss: 0.7998992204666138\n",
      "Epoch 2580, training loss: 854.3489990234375 = 0.7732220888137817 + 100 * 8.535758018493652\n",
      "Epoch 2580, val loss: 0.7983576059341431\n",
      "Epoch 2590, training loss: 854.3688354492188 = 0.7718529105186462 + 100 * 8.535969734191895\n",
      "Epoch 2590, val loss: 0.7971662282943726\n",
      "Epoch 2600, training loss: 854.3031005859375 = 0.7705512642860413 + 100 * 8.535325050354004\n",
      "Epoch 2600, val loss: 0.7959344387054443\n",
      "Epoch 2610, training loss: 854.2882690429688 = 0.769242525100708 + 100 * 8.53519058227539\n",
      "Epoch 2610, val loss: 0.7947176098823547\n",
      "Epoch 2620, training loss: 854.2645263671875 = 0.76792973279953 + 100 * 8.534965515136719\n",
      "Epoch 2620, val loss: 0.7935160994529724\n",
      "Epoch 2630, training loss: 854.2437133789062 = 0.7666183710098267 + 100 * 8.534770965576172\n",
      "Epoch 2630, val loss: 0.7922998666763306\n",
      "Epoch 2640, training loss: 854.2369995117188 = 0.7653030753135681 + 100 * 8.534716606140137\n",
      "Epoch 2640, val loss: 0.7910864949226379\n",
      "Epoch 2650, training loss: 854.4395751953125 = 0.7639434933662415 + 100 * 8.53675651550293\n",
      "Epoch 2650, val loss: 0.7898391485214233\n",
      "Epoch 2660, training loss: 854.2781372070312 = 0.7625132203102112 + 100 * 8.53515625\n",
      "Epoch 2660, val loss: 0.7884368300437927\n",
      "Epoch 2670, training loss: 854.214111328125 = 0.7611804008483887 + 100 * 8.534529685974121\n",
      "Epoch 2670, val loss: 0.7872274518013\n",
      "Epoch 2680, training loss: 854.1988525390625 = 0.759878933429718 + 100 * 8.53438949584961\n",
      "Epoch 2680, val loss: 0.7860272526741028\n",
      "Epoch 2690, training loss: 854.15478515625 = 0.758569598197937 + 100 * 8.53396224975586\n",
      "Epoch 2690, val loss: 0.784809947013855\n",
      "Epoch 2700, training loss: 854.1101684570312 = 0.7573021650314331 + 100 * 8.533528327941895\n",
      "Epoch 2700, val loss: 0.7836635112762451\n",
      "Epoch 2710, training loss: 854.0934448242188 = 0.7560579180717468 + 100 * 8.533373832702637\n",
      "Epoch 2710, val loss: 0.7825055718421936\n",
      "Epoch 2720, training loss: 854.2216796875 = 0.7547874450683594 + 100 * 8.534668922424316\n",
      "Epoch 2720, val loss: 0.7812746167182922\n",
      "Epoch 2730, training loss: 854.0465087890625 = 0.7533394694328308 + 100 * 8.532931327819824\n",
      "Epoch 2730, val loss: 0.7800025343894958\n",
      "Epoch 2740, training loss: 854.0249633789062 = 0.7520642876625061 + 100 * 8.532729148864746\n",
      "Epoch 2740, val loss: 0.7788143157958984\n",
      "Epoch 2750, training loss: 854.00390625 = 0.7508764266967773 + 100 * 8.532530784606934\n",
      "Epoch 2750, val loss: 0.7777330875396729\n",
      "Epoch 2760, training loss: 853.9833374023438 = 0.7496792078018188 + 100 * 8.532336235046387\n",
      "Epoch 2760, val loss: 0.776627779006958\n",
      "Epoch 2770, training loss: 853.9548950195312 = 0.74847811460495 + 100 * 8.532064437866211\n",
      "Epoch 2770, val loss: 0.7755091786384583\n",
      "Epoch 2780, training loss: 853.9513549804688 = 0.7472841739654541 + 100 * 8.5320405960083\n",
      "Epoch 2780, val loss: 0.7744030356407166\n",
      "Epoch 2790, training loss: 854.082275390625 = 0.7460199594497681 + 100 * 8.53336238861084\n",
      "Epoch 2790, val loss: 0.7732117772102356\n",
      "Epoch 2800, training loss: 853.887451171875 = 0.7447064518928528 + 100 * 8.531427383422852\n",
      "Epoch 2800, val loss: 0.7720388770103455\n"
     ]
    }
   ],
   "source": [
    "import copy \n",
    "from model import UnifyModel, Encoder\n",
    "from models.construct import model_construct\n",
    "from construct_graph import *\n",
    "\n",
    "import os.path as osp\n",
    "import random\n",
    "from time import perf_counter as t\n",
    "import yaml\n",
    "from yaml import SafeLoader\n",
    "\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "data = data.to(device)\n",
    "num_class = int(data.y.max()+1)\n",
    "# args.cl_activation = ({'relu': F.relu, 'prelu': nn.PReLU()})[config['activation']]\n",
    "# args.cl_base_model = ({'GCNConv': GCNConv})[config['base_model']]\n",
    "\n",
    "noisy_data = construct_noisy_graph(data,perturb_ratio=args.noisy_level,mode='random_noise')\n",
    "noisy_data = noisy_data.to(device)\n",
    "\n",
    "# diff_dataset = PPRDataset(noisy_data,args.dataset)\n",
    "# diff_noisy_data = diff_dataset.data.to(device)\n",
    "\n",
    "seen_node_idx = (torch.bitwise_not(data.test_mask)).nonzero().flatten()\n",
    "idx_overall_test = (torch.bitwise_not(data.train_mask)&torch.bitwise_not(data.val_mask)).nonzero().flatten()\n",
    "\n",
    "\n",
    "\n",
    "final_cl_acc_noisy = []\n",
    "final_gnn_acc_noisy = []\n",
    "print(\"=== Noisy graph ===\")\n",
    "rs = np.random.RandomState(args.seed)\n",
    "seeds = rs.randint(1000,size=3)\n",
    "for seed in seeds:\n",
    "    np.random.seed(seed)\n",
    "    # torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed(seed)\n",
    "    '''Transductive'''\n",
    "    encoder = Encoder(dataset.num_features, args.num_hidden, args.cl_activation,\n",
    "                            base_model=args.cl_base_model, k=args.cl_num_layers).to(device)\n",
    "    model = UnifyModel(args, encoder, args.num_hidden, args.num_proj_hidden, args.cl_num_proj_hidden, num_class, args.tau, lr=args.cl_lr, weight_decay=args.cl_weight_decay, device=device).to(device)\n",
    "    # model = UnifyModel(args, encoder, args.cl_num_hidden, args.cl_num_proj_hidden, num_class, args.tau, lr=args.cl_lr, weight_decay=weight_decay, device=device,data1=noisy_data,data2=diff_noisy_data).to(device)\n",
    "    model.fit(args, noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_train,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=None)\n",
    "    # model.fit_1(args, noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_train,idx_val=idx_val,train_iters=num_epochs,cont_iters=num_epochs,seen_node_idx=None)\n",
    "    # x, edge_index,edge_weight,labels,idx_train,idx_val=None,cont_iters=None,train_iters=200,seen_node_idx = None\n",
    "    acc_cl = model.test(noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_clean_test)\n",
    "    print(\"Accuracy of GNN+CL: {}\".format(acc_cl))\n",
    "    final_cl_acc_noisy.append(acc_cl)\n",
    "    gnn_model = model_construct(args,'GCN',noisy_data,device)\n",
    "    gnn_model.fit(noisy_data.x, noisy_data.edge_index, None, noisy_data.y, idx_train, idx_val,train_iters=args.epochs,verbose=False)\n",
    "    clean_acc = gnn_model.test(noisy_data.x,noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_overall_test)\n",
    "    print(clean_acc)\n",
    "    final_gnn_acc_noisy.append(clean_acc)\n",
    "\n",
    "\n",
    "print('The final CL Acc:{:.5f}, {:.5f}, The final GNN Acc:{:.5f}, {:.5f}'\\\n",
    "            .format(np.average(final_cl_acc_noisy),np.std(final_cl_acc_noisy),np.average(final_gnn_acc_noisy),np.std(final_gnn_acc_noisy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw graph: torch.Size([2, 10556])\n",
      "add edge: torch.Size([2, 13092])\n",
      "remove edge: torch.Size([2, 7878])\n",
      "updated graph: torch.Size([2, 10414])\n",
      "=== Raw graph ===\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 861.644287109375 = 1.962094783782959 + 100 * 8.596821784973145\n",
      "Epoch 0, val loss: 1.9662855863571167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mfl5681/project-contrastive/RobustCL/model.py:224: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.labels = torch.tensor(labels, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, training loss: 861.5540161132812 = 1.9520117044448853 + 100 * 8.596019744873047\n",
      "Epoch 10, val loss: 1.9556394815444946\n",
      "Epoch 20, training loss: 861.0609741210938 = 1.939118504524231 + 100 * 8.591217994689941\n",
      "Epoch 20, val loss: 1.9420093297958374\n",
      "Epoch 30, training loss: 858.0274658203125 = 1.9224451780319214 + 100 * 8.561050415039062\n",
      "Epoch 30, val loss: 1.9245028495788574\n",
      "Epoch 40, training loss: 839.6718139648438 = 1.9022588729858398 + 100 * 8.377695083618164\n",
      "Epoch 40, val loss: 1.904028058052063\n",
      "Epoch 50, training loss: 761.4630737304688 = 1.8792352676391602 + 100 * 7.59583854675293\n",
      "Epoch 50, val loss: 1.8807604312896729\n",
      "Epoch 60, training loss: 730.5070190429688 = 1.8613781929016113 + 100 * 7.28645658493042\n",
      "Epoch 60, val loss: 1.8645652532577515\n",
      "Epoch 70, training loss: 710.8011474609375 = 1.848585605621338 + 100 * 7.0895256996154785\n",
      "Epoch 70, val loss: 1.8523708581924438\n",
      "Epoch 80, training loss: 697.0316162109375 = 1.8360241651535034 + 100 * 6.951956272125244\n",
      "Epoch 80, val loss: 1.8408387899398804\n",
      "Epoch 90, training loss: 687.4520874023438 = 1.825295090675354 + 100 * 6.856267929077148\n",
      "Epoch 90, val loss: 1.831041693687439\n",
      "Epoch 100, training loss: 679.5516967773438 = 1.8157169818878174 + 100 * 6.777359485626221\n",
      "Epoch 100, val loss: 1.8220014572143555\n",
      "Epoch 110, training loss: 674.1467895507812 = 1.807191252708435 + 100 * 6.723396301269531\n",
      "Epoch 110, val loss: 1.813523769378662\n",
      "Epoch 120, training loss: 669.7814331054688 = 1.7988237142562866 + 100 * 6.679825782775879\n",
      "Epoch 120, val loss: 1.80497407913208\n",
      "Epoch 130, training loss: 666.4983520507812 = 1.7906441688537598 + 100 * 6.6470770835876465\n",
      "Epoch 130, val loss: 1.796532392501831\n",
      "Epoch 140, training loss: 663.9078369140625 = 1.7824809551239014 + 100 * 6.621253967285156\n",
      "Epoch 140, val loss: 1.7879313230514526\n",
      "Epoch 150, training loss: 661.3467407226562 = 1.7740931510925293 + 100 * 6.595726013183594\n",
      "Epoch 150, val loss: 1.7790043354034424\n",
      "Epoch 160, training loss: 659.0733642578125 = 1.765576720237732 + 100 * 6.573078155517578\n",
      "Epoch 160, val loss: 1.769971251487732\n",
      "Epoch 170, training loss: 657.1400146484375 = 1.7567615509033203 + 100 * 6.553832530975342\n",
      "Epoch 170, val loss: 1.7608487606048584\n",
      "Epoch 180, training loss: 655.0401000976562 = 1.7476128339767456 + 100 * 6.532924652099609\n",
      "Epoch 180, val loss: 1.7514877319335938\n",
      "Epoch 190, training loss: 653.4053344726562 = 1.7379696369171143 + 100 * 6.516673564910889\n",
      "Epoch 190, val loss: 1.7417694330215454\n",
      "Epoch 200, training loss: 651.93701171875 = 1.7274413108825684 + 100 * 6.502095699310303\n",
      "Epoch 200, val loss: 1.7313748598098755\n",
      "Epoch 210, training loss: 650.5963745117188 = 1.7160470485687256 + 100 * 6.488803386688232\n",
      "Epoch 210, val loss: 1.72016179561615\n",
      "Epoch 220, training loss: 649.3917846679688 = 1.7037297487258911 + 100 * 6.4768805503845215\n",
      "Epoch 220, val loss: 1.7082310914993286\n",
      "Epoch 230, training loss: 648.361083984375 = 1.6904914379119873 + 100 * 6.466705799102783\n",
      "Epoch 230, val loss: 1.695399284362793\n",
      "Epoch 240, training loss: 647.592041015625 = 1.676134705543518 + 100 * 6.459158897399902\n",
      "Epoch 240, val loss: 1.6815961599349976\n",
      "Epoch 250, training loss: 646.5584716796875 = 1.6607218980789185 + 100 * 6.448977947235107\n",
      "Epoch 250, val loss: 1.6668637990951538\n",
      "Epoch 260, training loss: 645.6998901367188 = 1.6442391872406006 + 100 * 6.440556526184082\n",
      "Epoch 260, val loss: 1.651158094406128\n",
      "Epoch 270, training loss: 645.0428466796875 = 1.6265718936920166 + 100 * 6.434162616729736\n",
      "Epoch 270, val loss: 1.6344703435897827\n",
      "Epoch 280, training loss: 644.27392578125 = 1.6077070236206055 + 100 * 6.426661968231201\n",
      "Epoch 280, val loss: 1.6166326999664307\n",
      "Epoch 290, training loss: 643.6007690429688 = 1.5877104997634888 + 100 * 6.420130729675293\n",
      "Epoch 290, val loss: 1.5978906154632568\n",
      "Epoch 300, training loss: 643.0278930664062 = 1.5665810108184814 + 100 * 6.414613246917725\n",
      "Epoch 300, val loss: 1.5782240629196167\n",
      "Epoch 310, training loss: 642.4826049804688 = 1.5442839860916138 + 100 * 6.4093828201293945\n",
      "Epoch 310, val loss: 1.5574852228164673\n",
      "Epoch 320, training loss: 641.80859375 = 1.5211156606674194 + 100 * 6.402874946594238\n",
      "Epoch 320, val loss: 1.5360872745513916\n",
      "Epoch 330, training loss: 641.3373413085938 = 1.4970778226852417 + 100 * 6.398402690887451\n",
      "Epoch 330, val loss: 1.5140480995178223\n",
      "Epoch 340, training loss: 640.8582153320312 = 1.4721163511276245 + 100 * 6.393861293792725\n",
      "Epoch 340, val loss: 1.4912132024765015\n",
      "Epoch 350, training loss: 640.32373046875 = 1.4465010166168213 + 100 * 6.388772487640381\n",
      "Epoch 350, val loss: 1.4679700136184692\n",
      "Epoch 360, training loss: 639.826416015625 = 1.4203037023544312 + 100 * 6.384061336517334\n",
      "Epoch 360, val loss: 1.4444465637207031\n",
      "Epoch 370, training loss: 639.5508422851562 = 1.3937230110168457 + 100 * 6.381571292877197\n",
      "Epoch 370, val loss: 1.4205306768417358\n",
      "Epoch 380, training loss: 638.9314575195312 = 1.3667057752609253 + 100 * 6.37564754486084\n",
      "Epoch 380, val loss: 1.396637201309204\n",
      "Epoch 390, training loss: 638.4680786132812 = 1.339647650718689 + 100 * 6.371284008026123\n",
      "Epoch 390, val loss: 1.3728108406066895\n",
      "Epoch 400, training loss: 638.23974609375 = 1.3125503063201904 + 100 * 6.369271755218506\n",
      "Epoch 400, val loss: 1.349084496498108\n",
      "Epoch 410, training loss: 637.6969604492188 = 1.2856323719024658 + 100 * 6.3641133308410645\n",
      "Epoch 410, val loss: 1.3257800340652466\n",
      "Epoch 420, training loss: 637.5006713867188 = 1.258978247642517 + 100 * 6.362417221069336\n",
      "Epoch 420, val loss: 1.3026783466339111\n",
      "Epoch 430, training loss: 636.9859008789062 = 1.2325109243392944 + 100 * 6.357534408569336\n",
      "Epoch 430, val loss: 1.2801238298416138\n",
      "Epoch 440, training loss: 636.5913696289062 = 1.2066129446029663 + 100 * 6.353847503662109\n",
      "Epoch 440, val loss: 1.2582811117172241\n",
      "Epoch 450, training loss: 636.409912109375 = 1.181168556213379 + 100 * 6.352287769317627\n",
      "Epoch 450, val loss: 1.2369909286499023\n",
      "Epoch 460, training loss: 635.9525756835938 = 1.1563199758529663 + 100 * 6.347962856292725\n",
      "Epoch 460, val loss: 1.2164385318756104\n",
      "Epoch 470, training loss: 635.5538940429688 = 1.132115125656128 + 100 * 6.344217300415039\n",
      "Epoch 470, val loss: 1.1967854499816895\n",
      "Epoch 480, training loss: 635.1766357421875 = 1.1086210012435913 + 100 * 6.340680122375488\n",
      "Epoch 480, val loss: 1.1778860092163086\n",
      "Epoch 490, training loss: 634.95947265625 = 1.0859185457229614 + 100 * 6.338735580444336\n",
      "Epoch 490, val loss: 1.1599267721176147\n",
      "Epoch 500, training loss: 634.8842163085938 = 1.063576579093933 + 100 * 6.3382062911987305\n",
      "Epoch 500, val loss: 1.1423829793930054\n",
      "Epoch 510, training loss: 634.2880249023438 = 1.0420455932617188 + 100 * 6.332459449768066\n",
      "Epoch 510, val loss: 1.1258277893066406\n",
      "Epoch 520, training loss: 633.9965209960938 = 1.021382451057434 + 100 * 6.329751491546631\n",
      "Epoch 520, val loss: 1.11040461063385\n",
      "Epoch 530, training loss: 634.06298828125 = 1.0014241933822632 + 100 * 6.330615997314453\n",
      "Epoch 530, val loss: 1.0956507921218872\n",
      "Epoch 540, training loss: 633.9052734375 = 0.9818752408027649 + 100 * 6.3292341232299805\n",
      "Epoch 540, val loss: 1.0815705060958862\n",
      "Epoch 550, training loss: 633.365234375 = 0.9630163311958313 + 100 * 6.32402229309082\n",
      "Epoch 550, val loss: 1.0682185888290405\n",
      "Epoch 560, training loss: 633.0078125 = 0.9448854923248291 + 100 * 6.320629119873047\n",
      "Epoch 560, val loss: 1.0557570457458496\n",
      "Epoch 570, training loss: 633.3028564453125 = 0.9273720979690552 + 100 * 6.323754787445068\n",
      "Epoch 570, val loss: 1.043984055519104\n",
      "Epoch 580, training loss: 632.7069702148438 = 0.9099910855293274 + 100 * 6.317969799041748\n",
      "Epoch 580, val loss: 1.0323840379714966\n",
      "Epoch 590, training loss: 632.388671875 = 0.8932880759239197 + 100 * 6.314953327178955\n",
      "Epoch 590, val loss: 1.0216765403747559\n",
      "Epoch 600, training loss: 632.1367797851562 = 0.8771835565567017 + 100 * 6.312595844268799\n",
      "Epoch 600, val loss: 1.0115809440612793\n",
      "Epoch 610, training loss: 631.9033203125 = 0.861522376537323 + 100 * 6.310418128967285\n",
      "Epoch 610, val loss: 1.0019768476486206\n",
      "Epoch 620, training loss: 632.370361328125 = 0.8462414145469666 + 100 * 6.31524133682251\n",
      "Epoch 620, val loss: 0.9927024841308594\n",
      "Epoch 630, training loss: 631.7825927734375 = 0.8309228420257568 + 100 * 6.309516906738281\n",
      "Epoch 630, val loss: 0.9837766289710999\n",
      "Epoch 640, training loss: 631.4234008789062 = 0.8160586953163147 + 100 * 6.3060736656188965\n",
      "Epoch 640, val loss: 0.9750566482543945\n",
      "Epoch 650, training loss: 631.1806030273438 = 0.8017409443855286 + 100 * 6.303788185119629\n",
      "Epoch 650, val loss: 0.9670875668525696\n",
      "Epoch 660, training loss: 630.9603881835938 = 0.7877512574195862 + 100 * 6.3017258644104\n",
      "Epoch 660, val loss: 0.9595322012901306\n",
      "Epoch 670, training loss: 631.1371459960938 = 0.7740768790245056 + 100 * 6.303630828857422\n",
      "Epoch 670, val loss: 0.9526289105415344\n",
      "Epoch 680, training loss: 631.2088623046875 = 0.7601678371429443 + 100 * 6.3044867515563965\n",
      "Epoch 680, val loss: 0.944625735282898\n",
      "Epoch 690, training loss: 630.6681518554688 = 0.7467218637466431 + 100 * 6.2992143630981445\n",
      "Epoch 690, val loss: 0.9378786087036133\n",
      "Epoch 700, training loss: 630.4363403320312 = 0.7336817979812622 + 100 * 6.29702615737915\n",
      "Epoch 700, val loss: 0.9315248131752014\n",
      "Epoch 710, training loss: 630.2048950195312 = 0.7208901047706604 + 100 * 6.294840335845947\n",
      "Epoch 710, val loss: 0.9254133105278015\n",
      "Epoch 720, training loss: 630.0648193359375 = 0.708318829536438 + 100 * 6.293565273284912\n",
      "Epoch 720, val loss: 0.9195829033851624\n",
      "Epoch 730, training loss: 630.1180419921875 = 0.6958944201469421 + 100 * 6.294220924377441\n",
      "Epoch 730, val loss: 0.9138956069946289\n",
      "Epoch 740, training loss: 630.2467651367188 = 0.6836580634117126 + 100 * 6.295631408691406\n",
      "Epoch 740, val loss: 0.9084216356277466\n",
      "Epoch 750, training loss: 629.7432250976562 = 0.67139732837677 + 100 * 6.2907185554504395\n",
      "Epoch 750, val loss: 0.9030638337135315\n",
      "Epoch 760, training loss: 629.7554931640625 = 0.659441351890564 + 100 * 6.290960788726807\n",
      "Epoch 760, val loss: 0.8978237509727478\n",
      "Epoch 770, training loss: 629.3583374023438 = 0.6477389335632324 + 100 * 6.287105560302734\n",
      "Epoch 770, val loss: 0.8930339813232422\n",
      "Epoch 780, training loss: 629.23974609375 = 0.6362759470939636 + 100 * 6.28603458404541\n",
      "Epoch 780, val loss: 0.8885634541511536\n",
      "Epoch 790, training loss: 629.076416015625 = 0.6249932050704956 + 100 * 6.284514427185059\n",
      "Epoch 790, val loss: 0.8842433094978333\n",
      "Epoch 800, training loss: 629.4927978515625 = 0.6138454079627991 + 100 * 6.288789749145508\n",
      "Epoch 800, val loss: 0.879993736743927\n",
      "Epoch 810, training loss: 629.1080932617188 = 0.6025771498680115 + 100 * 6.285055160522461\n",
      "Epoch 810, val loss: 0.875501275062561\n",
      "Epoch 820, training loss: 628.7084350585938 = 0.5916978120803833 + 100 * 6.281167507171631\n",
      "Epoch 820, val loss: 0.8720659017562866\n",
      "Epoch 830, training loss: 628.6825561523438 = 0.5810576677322388 + 100 * 6.281014919281006\n",
      "Epoch 830, val loss: 0.8685292601585388\n",
      "Epoch 840, training loss: 628.8558959960938 = 0.5704875588417053 + 100 * 6.282854080200195\n",
      "Epoch 840, val loss: 0.8650528788566589\n",
      "Epoch 850, training loss: 628.4564208984375 = 0.559902012348175 + 100 * 6.278965473175049\n",
      "Epoch 850, val loss: 0.861520528793335\n",
      "Epoch 860, training loss: 628.281494140625 = 0.5496060848236084 + 100 * 6.277318954467773\n",
      "Epoch 860, val loss: 0.8585242629051208\n",
      "Epoch 870, training loss: 628.19140625 = 0.5395238399505615 + 100 * 6.27651834487915\n",
      "Epoch 870, val loss: 0.8555909991264343\n",
      "Epoch 880, training loss: 628.5211181640625 = 0.5295364260673523 + 100 * 6.279915809631348\n",
      "Epoch 880, val loss: 0.8526245355606079\n",
      "Epoch 890, training loss: 628.409423828125 = 0.5195180773735046 + 100 * 6.278899192810059\n",
      "Epoch 890, val loss: 0.8501153588294983\n",
      "Epoch 900, training loss: 627.9663696289062 = 0.5096813440322876 + 100 * 6.274566650390625\n",
      "Epoch 900, val loss: 0.8476756811141968\n",
      "Epoch 910, training loss: 627.7903442382812 = 0.5000954270362854 + 100 * 6.272902011871338\n",
      "Epoch 910, val loss: 0.8452358245849609\n",
      "Epoch 920, training loss: 628.3136596679688 = 0.4905988872051239 + 100 * 6.278230667114258\n",
      "Epoch 920, val loss: 0.8428605198860168\n",
      "Epoch 930, training loss: 627.8837890625 = 0.4811084270477295 + 100 * 6.274027347564697\n",
      "Epoch 930, val loss: 0.8412899374961853\n",
      "Epoch 940, training loss: 627.537353515625 = 0.4718198776245117 + 100 * 6.270655632019043\n",
      "Epoch 940, val loss: 0.8395143151283264\n",
      "Epoch 950, training loss: 627.4616088867188 = 0.4627259075641632 + 100 * 6.269989013671875\n",
      "Epoch 950, val loss: 0.8377026319503784\n",
      "Epoch 960, training loss: 627.7134399414062 = 0.453785240650177 + 100 * 6.27259635925293\n",
      "Epoch 960, val loss: 0.8361557722091675\n",
      "Epoch 970, training loss: 627.3477172851562 = 0.444847971200943 + 100 * 6.269029140472412\n",
      "Epoch 970, val loss: 0.8348276019096375\n",
      "Epoch 980, training loss: 627.3309326171875 = 0.43607696890830994 + 100 * 6.268948554992676\n",
      "Epoch 980, val loss: 0.833620011806488\n",
      "Epoch 990, training loss: 627.452392578125 = 0.4274100363254547 + 100 * 6.270249366760254\n",
      "Epoch 990, val loss: 0.8327633142471313\n",
      "=== picking the best model according to the performance on validation ===\n",
      "Accuracy of GNN+CL: 0.725925925925926\n",
      "0.8323668950975225\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 861.6256103515625 = 1.9400644302368164 + 100 * 8.596855163574219\n",
      "Epoch 0, val loss: 1.937163233757019\n",
      "Epoch 10, training loss: 861.558349609375 = 1.9318561553955078 + 100 * 8.596264839172363\n",
      "Epoch 10, val loss: 1.9292261600494385\n",
      "Epoch 20, training loss: 861.1806030273438 = 1.9214622974395752 + 100 * 8.592591285705566\n",
      "Epoch 20, val loss: 1.9189276695251465\n",
      "Epoch 30, training loss: 858.8081665039062 = 1.9076519012451172 + 100 * 8.569005012512207\n",
      "Epoch 30, val loss: 1.90499746799469\n",
      "Epoch 40, training loss: 846.364013671875 = 1.889906406402588 + 100 * 8.444741249084473\n",
      "Epoch 40, val loss: 1.8875696659088135\n",
      "Epoch 50, training loss: 796.50732421875 = 1.8686753511428833 + 100 * 7.946386814117432\n",
      "Epoch 50, val loss: 1.8671156167984009\n",
      "Epoch 60, training loss: 755.0669555664062 = 1.849339485168457 + 100 * 7.5321760177612305\n",
      "Epoch 60, val loss: 1.849808692932129\n",
      "Epoch 70, training loss: 724.7876586914062 = 1.8371189832687378 + 100 * 7.22950553894043\n",
      "Epoch 70, val loss: 1.838728904724121\n",
      "Epoch 80, training loss: 708.0046997070312 = 1.8260066509246826 + 100 * 7.061787128448486\n",
      "Epoch 80, val loss: 1.8283326625823975\n",
      "Epoch 90, training loss: 694.9285278320312 = 1.813997507095337 + 100 * 6.931145191192627\n",
      "Epoch 90, val loss: 1.8174866437911987\n",
      "Epoch 100, training loss: 684.7422485351562 = 1.8036574125289917 + 100 * 6.829385757446289\n",
      "Epoch 100, val loss: 1.8082479238510132\n",
      "Epoch 110, training loss: 678.2771606445312 = 1.7938802242279053 + 100 * 6.764832973480225\n",
      "Epoch 110, val loss: 1.799150824546814\n",
      "Epoch 120, training loss: 673.3226928710938 = 1.7830561399459839 + 100 * 6.715395927429199\n",
      "Epoch 120, val loss: 1.7888263463974\n",
      "Epoch 130, training loss: 669.338623046875 = 1.7717173099517822 + 100 * 6.675668716430664\n",
      "Epoch 130, val loss: 1.7781692743301392\n",
      "Epoch 140, training loss: 666.0255126953125 = 1.760380744934082 + 100 * 6.642651557922363\n",
      "Epoch 140, val loss: 1.7675225734710693\n",
      "Epoch 150, training loss: 663.3056640625 = 1.748305082321167 + 100 * 6.615573883056641\n",
      "Epoch 150, val loss: 1.7562090158462524\n",
      "Epoch 160, training loss: 660.9415893554688 = 1.735390305519104 + 100 * 6.592061996459961\n",
      "Epoch 160, val loss: 1.7441933155059814\n",
      "Epoch 170, training loss: 658.536865234375 = 1.7214373350143433 + 100 * 6.568154335021973\n",
      "Epoch 170, val loss: 1.7312922477722168\n",
      "Epoch 180, training loss: 656.5111694335938 = 1.7063114643096924 + 100 * 6.548048973083496\n",
      "Epoch 180, val loss: 1.717444658279419\n",
      "Epoch 190, training loss: 654.9179077148438 = 1.6898634433746338 + 100 * 6.532279968261719\n",
      "Epoch 190, val loss: 1.702542781829834\n",
      "Epoch 200, training loss: 653.2361450195312 = 1.6719268560409546 + 100 * 6.515642166137695\n",
      "Epoch 200, val loss: 1.68628990650177\n",
      "Epoch 210, training loss: 651.8942260742188 = 1.6526137590408325 + 100 * 6.502416133880615\n",
      "Epoch 210, val loss: 1.6690404415130615\n",
      "Epoch 220, training loss: 650.7894897460938 = 1.6319175958633423 + 100 * 6.491575717926025\n",
      "Epoch 220, val loss: 1.6506602764129639\n",
      "Epoch 230, training loss: 649.56298828125 = 1.6097544431686401 + 100 * 6.479532241821289\n",
      "Epoch 230, val loss: 1.6311829090118408\n",
      "Epoch 240, training loss: 648.5890502929688 = 1.5864372253417969 + 100 * 6.47002649307251\n",
      "Epoch 240, val loss: 1.6108583211898804\n",
      "Epoch 250, training loss: 647.7928466796875 = 1.5620551109313965 + 100 * 6.462307929992676\n",
      "Epoch 250, val loss: 1.5898971557617188\n",
      "Epoch 260, training loss: 646.763916015625 = 1.5367298126220703 + 100 * 6.452271461486816\n",
      "Epoch 260, val loss: 1.5684722661972046\n",
      "Epoch 270, training loss: 645.9243774414062 = 1.5108585357666016 + 100 * 6.4441351890563965\n",
      "Epoch 270, val loss: 1.546892523765564\n",
      "Epoch 280, training loss: 645.5167846679688 = 1.484607219696045 + 100 * 6.440321445465088\n",
      "Epoch 280, val loss: 1.5253788232803345\n",
      "Epoch 290, training loss: 644.4767456054688 = 1.458116888999939 + 100 * 6.4301862716674805\n",
      "Epoch 290, val loss: 1.5041255950927734\n",
      "Epoch 300, training loss: 643.72802734375 = 1.431667685508728 + 100 * 6.422964096069336\n",
      "Epoch 300, val loss: 1.4833946228027344\n",
      "Epoch 310, training loss: 643.0540161132812 = 1.4054725170135498 + 100 * 6.416485786437988\n",
      "Epoch 310, val loss: 1.4632140398025513\n",
      "Epoch 320, training loss: 642.5584106445312 = 1.379578948020935 + 100 * 6.411788463592529\n",
      "Epoch 320, val loss: 1.4437544345855713\n",
      "Epoch 330, training loss: 641.8997802734375 = 1.3540302515029907 + 100 * 6.405457973480225\n",
      "Epoch 330, val loss: 1.4248886108398438\n",
      "Epoch 340, training loss: 641.4293212890625 = 1.328943133354187 + 100 * 6.401004314422607\n",
      "Epoch 340, val loss: 1.4068434238433838\n",
      "Epoch 350, training loss: 640.7667236328125 = 1.3042826652526855 + 100 * 6.394624710083008\n",
      "Epoch 350, val loss: 1.3893426656723022\n",
      "Epoch 360, training loss: 640.5762329101562 = 1.279984951019287 + 100 * 6.392962455749512\n",
      "Epoch 360, val loss: 1.3725305795669556\n",
      "Epoch 370, training loss: 639.7859497070312 = 1.2560510635375977 + 100 * 6.385299205780029\n",
      "Epoch 370, val loss: 1.356040358543396\n",
      "Epoch 380, training loss: 639.291015625 = 1.232496738433838 + 100 * 6.380585193634033\n",
      "Epoch 380, val loss: 1.3399512767791748\n",
      "Epoch 390, training loss: 639.2095336914062 = 1.2092833518981934 + 100 * 6.380002498626709\n",
      "Epoch 390, val loss: 1.3243436813354492\n",
      "Epoch 400, training loss: 638.4850463867188 = 1.186273455619812 + 100 * 6.372987747192383\n",
      "Epoch 400, val loss: 1.3088674545288086\n",
      "Epoch 410, training loss: 637.9954833984375 = 1.1635804176330566 + 100 * 6.368319034576416\n",
      "Epoch 410, val loss: 1.2936829328536987\n",
      "Epoch 420, training loss: 637.7472534179688 = 1.1412807703018188 + 100 * 6.366059303283691\n",
      "Epoch 420, val loss: 1.2788461446762085\n",
      "Epoch 430, training loss: 637.5863037109375 = 1.1190540790557861 + 100 * 6.3646721839904785\n",
      "Epoch 430, val loss: 1.264147400856018\n",
      "Epoch 440, training loss: 636.9344482421875 = 1.0972316265106201 + 100 * 6.358372211456299\n",
      "Epoch 440, val loss: 1.2497097253799438\n",
      "Epoch 450, training loss: 636.5270385742188 = 1.0758075714111328 + 100 * 6.3545122146606445\n",
      "Epoch 450, val loss: 1.235626459121704\n",
      "Epoch 460, training loss: 636.1124877929688 = 1.0546503067016602 + 100 * 6.350578784942627\n",
      "Epoch 460, val loss: 1.2216525077819824\n",
      "Epoch 470, training loss: 635.7966918945312 = 1.0338270664215088 + 100 * 6.347628593444824\n",
      "Epoch 470, val loss: 1.2081234455108643\n",
      "Epoch 480, training loss: 635.850341796875 = 1.0133090019226074 + 100 * 6.34837007522583\n",
      "Epoch 480, val loss: 1.1946362257003784\n",
      "Epoch 490, training loss: 635.3618774414062 = 0.99311363697052 + 100 * 6.343688011169434\n",
      "Epoch 490, val loss: 1.1815155744552612\n",
      "Epoch 500, training loss: 635.1040649414062 = 0.9732193350791931 + 100 * 6.34130859375\n",
      "Epoch 500, val loss: 1.1686975955963135\n",
      "Epoch 510, training loss: 634.6011352539062 = 0.9536280632019043 + 100 * 6.336475372314453\n",
      "Epoch 510, val loss: 1.155906319618225\n",
      "Epoch 520, training loss: 634.3981323242188 = 0.9343708157539368 + 100 * 6.334637641906738\n",
      "Epoch 520, val loss: 1.1434568166732788\n",
      "Epoch 530, training loss: 634.11083984375 = 0.9154025912284851 + 100 * 6.331954479217529\n",
      "Epoch 530, val loss: 1.1311249732971191\n",
      "Epoch 540, training loss: 634.3854370117188 = 0.896727979183197 + 100 * 6.3348870277404785\n",
      "Epoch 540, val loss: 1.1191990375518799\n",
      "Epoch 550, training loss: 633.8603515625 = 0.8780961632728577 + 100 * 6.329822540283203\n",
      "Epoch 550, val loss: 1.1071574687957764\n",
      "Epoch 560, training loss: 633.3800048828125 = 0.8599026203155518 + 100 * 6.325201034545898\n",
      "Epoch 560, val loss: 1.0954796075820923\n",
      "Epoch 570, training loss: 633.0386352539062 = 0.8420446515083313 + 100 * 6.32196569442749\n",
      "Epoch 570, val loss: 1.0840187072753906\n",
      "Epoch 580, training loss: 633.2166748046875 = 0.8244935870170593 + 100 * 6.3239216804504395\n",
      "Epoch 580, val loss: 1.0727254152297974\n",
      "Epoch 590, training loss: 632.78955078125 = 0.807065486907959 + 100 * 6.319824695587158\n",
      "Epoch 590, val loss: 1.0618200302124023\n",
      "Epoch 600, training loss: 632.4755249023438 = 0.7899541258811951 + 100 * 6.316855430603027\n",
      "Epoch 600, val loss: 1.0508100986480713\n",
      "Epoch 610, training loss: 633.3728637695312 = 0.7731065154075623 + 100 * 6.325997352600098\n",
      "Epoch 610, val loss: 1.039899468421936\n",
      "Epoch 620, training loss: 632.2355346679688 = 0.7563152313232422 + 100 * 6.314792633056641\n",
      "Epoch 620, val loss: 1.029582142829895\n",
      "Epoch 630, training loss: 631.87548828125 = 0.7398608326911926 + 100 * 6.311356067657471\n",
      "Epoch 630, val loss: 1.0191726684570312\n",
      "Epoch 640, training loss: 631.612548828125 = 0.723746120929718 + 100 * 6.308887958526611\n",
      "Epoch 640, val loss: 1.0090540647506714\n",
      "Epoch 650, training loss: 631.3936767578125 = 0.7078512907028198 + 100 * 6.306858539581299\n",
      "Epoch 650, val loss: 0.999161422252655\n",
      "Epoch 660, training loss: 632.3093872070312 = 0.6921839714050293 + 100 * 6.316171646118164\n",
      "Epoch 660, val loss: 0.9894756078720093\n",
      "Epoch 670, training loss: 631.218017578125 = 0.676463782787323 + 100 * 6.305415630340576\n",
      "Epoch 670, val loss: 0.9797593951225281\n",
      "Epoch 680, training loss: 631.1270141601562 = 0.6610329151153564 + 100 * 6.304659843444824\n",
      "Epoch 680, val loss: 0.9701759815216064\n",
      "Epoch 690, training loss: 630.7429809570312 = 0.6458085775375366 + 100 * 6.300971508026123\n",
      "Epoch 690, val loss: 0.961169421672821\n",
      "Epoch 700, training loss: 631.1717529296875 = 0.6307500004768372 + 100 * 6.305409908294678\n",
      "Epoch 700, val loss: 0.9519364237785339\n",
      "Epoch 710, training loss: 630.5950317382812 = 0.6158049702644348 + 100 * 6.299792289733887\n",
      "Epoch 710, val loss: 0.9430593848228455\n",
      "Epoch 720, training loss: 630.30322265625 = 0.6010223031044006 + 100 * 6.297021865844727\n",
      "Epoch 720, val loss: 0.9343473315238953\n",
      "Epoch 730, training loss: 630.4652099609375 = 0.586448073387146 + 100 * 6.298787593841553\n",
      "Epoch 730, val loss: 0.9256606101989746\n",
      "Epoch 740, training loss: 630.027587890625 = 0.5719341039657593 + 100 * 6.294556140899658\n",
      "Epoch 740, val loss: 0.9171733856201172\n",
      "Epoch 750, training loss: 630.0643310546875 = 0.5575931668281555 + 100 * 6.295067310333252\n",
      "Epoch 750, val loss: 0.908751904964447\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mfl5681/project-contrastive/RobustCL/run_pre_noisy.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_pre_noisy.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m encoder \u001b[39m=\u001b[39m Encoder(dataset\u001b[39m.\u001b[39mnum_features, args\u001b[39m.\u001b[39mcl_num_hidden, args\u001b[39m.\u001b[39mcl_activation,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_pre_noisy.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m                         base_model\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mcl_base_model, k\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mcl_num_layers)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_pre_noisy.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m model \u001b[39m=\u001b[39m UnifyModel(args, encoder, args\u001b[39m.\u001b[39mcl_num_hidden, args\u001b[39m.\u001b[39mcl_num_proj_hidden, num_class, args\u001b[39m.\u001b[39mtau, lr\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mcl_lr, weight_decay\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mcl_weight_decay, device\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_pre_noisy.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(args, data\u001b[39m.\u001b[39;49mx, data\u001b[39m.\u001b[39;49medge_index,data\u001b[39m.\u001b[39;49medge_weight,data\u001b[39m.\u001b[39;49my,idx_train,idx_val\u001b[39m=\u001b[39;49midx_val,train_iters\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mcl_num_epochs,cont_iters\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mcl_num_epochs,seen_node_idx\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_pre_noisy.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39m# x, edge_index,edge_weight,labels,idx_train,idx_val=None,cont_iters=None,train_iters=200,seen_node_idx = None\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_pre_noisy.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m acc_cl \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtest(data\u001b[39m.\u001b[39mx, data\u001b[39m.\u001b[39medge_index,data\u001b[39m.\u001b[39medge_weight,data\u001b[39m.\u001b[39my,idx_clean_test)\n",
      "File \u001b[0;32m~/project-contrastive/RobustCL/model.py:232\u001b[0m, in \u001b[0;36mUnifyModel.fit\u001b[0;34m(self, args, x, edge_index, edge_weight, labels, idx_train, idx_val, train_iters, cont_iters, seen_node_idx)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_without_val(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels, idx_train, train_iters,verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    231\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 232\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_with_val(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabels, idx_train, idx_val, train_iters)\n",
      "File \u001b[0;32m~/project-contrastive/RobustCL/model.py:311\u001b[0m, in \u001b[0;36mUnifyModel._train_with_val\u001b[0;34m(self, labels, idx_train, idx_val, train_iters, verbose)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39mif\u001b[39;00m verbose \u001b[39mand\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    310\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, training loss: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m = \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m + \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m * \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(i, loss\u001b[39m.\u001b[39mitem(),clf_loss\u001b[39m.\u001b[39mitem(),\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcont_weight,cont_loss\u001b[39m.\u001b[39mitem()))\n\u001b[0;32m--> 311\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    312\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    315\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import copy \n",
    "from model import UnifyModel\n",
    "from models.construct import model_construct\n",
    "\n",
    "\n",
    "import os.path as osp\n",
    "import random\n",
    "from time import perf_counter as t\n",
    "import yaml\n",
    "from yaml import SafeLoader\n",
    "\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from model import Encoder, Model, drop_feature\n",
    "\n",
    "data = data.to(device)\n",
    "config = yaml.load(open(args.config), Loader=SafeLoader)[args.dataset]\n",
    "# num_epochs = config['num_epochs']\n",
    "# args.cl_lr = config['args.cl_lr']\n",
    "# weight_decay = config['weight_decay']\n",
    "# args.seed = config['seed']\n",
    "# args.cont_batch_size = config['cont_batch_size']\n",
    "# args.cont_weight = config['cont_weight']\n",
    "# args.add_edge_rate_1 = config['add_edge_rate_1']\n",
    "# args.add_edge_rate_2 = config['add_edge_rate_2']\n",
    "# args.drop_edge_rate_1 = config['drop_edge_rate_1']\n",
    "# args.drop_edge_rate_2 = config['drop_edge_rate_2']\n",
    "# args.drop_feat_rate_1 = config['drop_feature_rate_1']\n",
    "# args.drop_feat_rate_2 = config['drop_feature_rate_2']\n",
    "# args.add_edge_rate_1 = 0\n",
    "# args.add_edge_rate_2 = 0\n",
    "# args.drop_edge_rate_1 = 0.3\n",
    "# args.drop_edge_rate_2 = 0.5\n",
    "# args.drop_feat_rate_1 = 0.4\n",
    "# args.drop_feat_rate_2 = 0.4\n",
    "num_class = int(data.y.max()+1)\n",
    "\n",
    "noisy_data = construct_noisy_graph(data,perturb_ratio=0.25,mode='random_noise')\n",
    "noisy_data = noisy_data.to(device)\n",
    "seen_node_idx = (torch.bitwise_not(data.test_mask)).nonzero().flatten()\n",
    "idx_overall_test = (torch.bitwise_not(data.train_mask)&torch.bitwise_not(data.val_mask)).nonzero().flatten()\n",
    "\n",
    "\n",
    "final_cl_acc = []\n",
    "final_gnn_acc = []\n",
    "print(\"=== Raw graph ===\")\n",
    "rs = np.random.RandomState(args.seed)\n",
    "seeds = rs.randint(1000,size=3)\n",
    "for seed in seeds:\n",
    "    np.random.seed(seed)\n",
    "    # torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed(seed)\n",
    "    '''Transductive'''\n",
    "    encoder = Encoder(dataset.num_features, args.num_hidden, args.cl_activation,\n",
    "                            base_model=args.cl_base_model, k=args.cl_num_layers).to(device)\n",
    "    model = UnifyModel(args, encoder, args.num_hidden, args.num_proj_hidden, args.cl_num_proj_hidden, num_class, args.tau, lr=args.cl_lr, weight_decay=args.cl_weight_decay, device=device).to(device)\n",
    "    model.fit(args, data.x, data.edge_index,data.edge_weight,data.y,idx_train,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=None)\n",
    "    # x, edge_index,edge_weight,labels,idx_train,idx_val=None,cont_iters=None,train_iters=200,seen_node_idx = None\n",
    "    acc_cl = model.test(data.x, data.edge_index,data.edge_weight,data.y,idx_clean_test)\n",
    "    print(\"Accuracy of GNN+CL: {}\".format(acc_cl))\n",
    "    final_cl_acc.append(acc_cl)\n",
    "    gnn_model = model_construct(args,'GCN',data,device)\n",
    "    gnn_model.fit(data.x, data.edge_index, None, data.y, idx_train, idx_val,train_iters=args.epochs,verbose=False)\n",
    "    clean_acc = gnn_model.test(data.x,data.edge_index,data.edge_weight,data.y,idx_overall_test)\n",
    "    print(clean_acc)\n",
    "    final_gnn_acc.append(clean_acc)\n",
    "\n",
    "\n",
    "print('The final CL Acc:{:.5f}, {:.5f}, The final GNN Acc:{:.5f}, {:.5f}'\\\n",
    "            .format(np.average(final_cl_acc),np.std(final_cl_acc),np.average(final_gnn_acc),np.std(final_gnn_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noisy+DIffuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw graph: torch.Size([2, 10556])\n",
      "add edge: torch.Size([2, 13190])\n",
      "remove edge: torch.Size([2, 7998])\n",
      "updated graph: torch.Size([2, 10632])\n",
      "=== Noisy graph ===\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 861.6275024414062 = 1.9418861865997314 + 100 * 8.596856117248535\n",
      "Epoch 0, val loss: 1.9364506006240845\n",
      "Epoch 10, training loss: 848.9478759765625 = 1.897546648979187 + 100 * 8.470503807067871\n",
      "Epoch 10, val loss: 1.8890513181686401\n",
      "Epoch 20, training loss: 760.0466918945312 = 1.8456153869628906 + 100 * 7.582010746002197\n",
      "Epoch 20, val loss: 1.8392390012741089\n",
      "Epoch 30, training loss: 731.3434448242188 = 1.8136775493621826 + 100 * 7.295298099517822\n",
      "Epoch 30, val loss: 1.808591365814209\n",
      "Epoch 40, training loss: 714.7428588867188 = 1.788662314414978 + 100 * 7.129542350769043\n",
      "Epoch 40, val loss: 1.7784889936447144\n",
      "Epoch 50, training loss: 705.1812744140625 = 1.7627829313278198 + 100 * 7.034185409545898\n",
      "Epoch 50, val loss: 1.748166561126709\n",
      "Epoch 60, training loss: 695.6590576171875 = 1.7388758659362793 + 100 * 6.939201354980469\n",
      "Epoch 60, val loss: 1.7244641780853271\n",
      "Epoch 70, training loss: 689.8345336914062 = 1.7147310972213745 + 100 * 6.881198406219482\n",
      "Epoch 70, val loss: 1.6988224983215332\n",
      "Epoch 80, training loss: 684.240478515625 = 1.684454321861267 + 100 * 6.825560092926025\n",
      "Epoch 80, val loss: 1.669772744178772\n",
      "Epoch 90, training loss: 679.1021728515625 = 1.6641491651535034 + 100 * 6.774380207061768\n",
      "Epoch 90, val loss: 1.647947072982788\n",
      "Epoch 100, training loss: 676.1454467773438 = 1.6357783079147339 + 100 * 6.745096683502197\n",
      "Epoch 100, val loss: 1.6176695823669434\n",
      "Epoch 110, training loss: 671.8375244140625 = 1.60782790184021 + 100 * 6.702296733856201\n",
      "Epoch 110, val loss: 1.5917643308639526\n",
      "Epoch 120, training loss: 668.9812622070312 = 1.5809794664382935 + 100 * 6.674002647399902\n",
      "Epoch 120, val loss: 1.5651280879974365\n",
      "Epoch 130, training loss: 667.2160034179688 = 1.5507876873016357 + 100 * 6.656652450561523\n",
      "Epoch 130, val loss: 1.5334396362304688\n",
      "Epoch 140, training loss: 664.4934692382812 = 1.520186185836792 + 100 * 6.629732608795166\n",
      "Epoch 140, val loss: 1.5075812339782715\n",
      "Epoch 150, training loss: 664.570556640625 = 1.4938722848892212 + 100 * 6.63076639175415\n",
      "Epoch 150, val loss: 1.4794654846191406\n",
      "Epoch 160, training loss: 664.0630493164062 = 1.4620964527130127 + 100 * 6.626009464263916\n",
      "Epoch 160, val loss: 1.4516395330429077\n",
      "Epoch 170, training loss: 660.37841796875 = 1.431633710861206 + 100 * 6.589468002319336\n",
      "Epoch 170, val loss: 1.4242453575134277\n",
      "Epoch 180, training loss: 658.3257446289062 = 1.4055275917053223 + 100 * 6.569202423095703\n",
      "Epoch 180, val loss: 1.4011321067810059\n",
      "Epoch 190, training loss: 659.1428833007812 = 1.3766684532165527 + 100 * 6.577662467956543\n",
      "Epoch 190, val loss: 1.3742740154266357\n",
      "Epoch 200, training loss: 656.3142700195312 = 1.3466519117355347 + 100 * 6.549675941467285\n",
      "Epoch 200, val loss: 1.3487579822540283\n",
      "Epoch 210, training loss: 657.0576171875 = 1.3211767673492432 + 100 * 6.557364463806152\n",
      "Epoch 210, val loss: 1.3240406513214111\n",
      "Epoch 220, training loss: 654.1902465820312 = 1.2902557849884033 + 100 * 6.5289998054504395\n",
      "Epoch 220, val loss: 1.2974364757537842\n",
      "Epoch 230, training loss: 654.40478515625 = 1.2598240375518799 + 100 * 6.531449794769287\n",
      "Epoch 230, val loss: 1.2699055671691895\n",
      "Epoch 240, training loss: 653.74853515625 = 1.2298442125320435 + 100 * 6.525186538696289\n",
      "Epoch 240, val loss: 1.2427815198898315\n",
      "Epoch 250, training loss: 652.3123779296875 = 1.2035788297653198 + 100 * 6.5110883712768555\n",
      "Epoch 250, val loss: 1.220241904258728\n",
      "Epoch 260, training loss: 651.0294189453125 = 1.1788502931594849 + 100 * 6.498505592346191\n",
      "Epoch 260, val loss: 1.2020516395568848\n",
      "Epoch 270, training loss: 652.3500366210938 = 1.1488637924194336 + 100 * 6.512011528015137\n",
      "Epoch 270, val loss: 1.1731982231140137\n",
      "Epoch 280, training loss: 650.5430908203125 = 1.1179497241973877 + 100 * 6.494251728057861\n",
      "Epoch 280, val loss: 1.1497611999511719\n",
      "Epoch 290, training loss: 648.824951171875 = 1.095263123512268 + 100 * 6.477296829223633\n",
      "Epoch 290, val loss: 1.13498055934906\n",
      "Epoch 300, training loss: 648.9354858398438 = 1.0665699243545532 + 100 * 6.478688716888428\n",
      "Epoch 300, val loss: 1.110095500946045\n",
      "Epoch 310, training loss: 648.1292724609375 = 1.0417900085449219 + 100 * 6.470874786376953\n",
      "Epoch 310, val loss: 1.0901564359664917\n",
      "Epoch 320, training loss: 647.8590698242188 = 1.0199153423309326 + 100 * 6.4683918952941895\n",
      "Epoch 320, val loss: 1.073639988899231\n",
      "Epoch 330, training loss: 647.169189453125 = 0.9966698884963989 + 100 * 6.46172571182251\n",
      "Epoch 330, val loss: 1.0614732503890991\n",
      "Epoch 340, training loss: 647.5020751953125 = 0.9698240160942078 + 100 * 6.465322494506836\n",
      "Epoch 340, val loss: 1.039226770401001\n",
      "Epoch 350, training loss: 646.164306640625 = 0.9476423263549805 + 100 * 6.45216703414917\n",
      "Epoch 350, val loss: 1.0236902236938477\n",
      "Epoch 360, training loss: 645.9779052734375 = 0.9277147650718689 + 100 * 6.450501918792725\n",
      "Epoch 360, val loss: 1.01019287109375\n",
      "Epoch 370, training loss: 645.12548828125 = 0.9034939408302307 + 100 * 6.4422197341918945\n",
      "Epoch 370, val loss: 0.9942207336425781\n",
      "Epoch 380, training loss: 645.4757080078125 = 0.8838961124420166 + 100 * 6.445918083190918\n",
      "Epoch 380, val loss: 0.9798300266265869\n",
      "Epoch 390, training loss: 644.0909423828125 = 0.8609434962272644 + 100 * 6.432299613952637\n",
      "Epoch 390, val loss: 0.9657952189445496\n",
      "Epoch 400, training loss: 643.480224609375 = 0.8441293835639954 + 100 * 6.426361083984375\n",
      "Epoch 400, val loss: 0.9587718844413757\n",
      "Epoch 410, training loss: 643.7907104492188 = 0.82208251953125 + 100 * 6.429686546325684\n",
      "Epoch 410, val loss: 0.9428423643112183\n",
      "Epoch 420, training loss: 643.2448120117188 = 0.8023104667663574 + 100 * 6.42442512512207\n",
      "Epoch 420, val loss: 0.9308084845542908\n",
      "Epoch 430, training loss: 642.1791381835938 = 0.7845478057861328 + 100 * 6.413946151733398\n",
      "Epoch 430, val loss: 0.9229530096054077\n",
      "Epoch 440, training loss: 642.4922485351562 = 0.7641425132751465 + 100 * 6.417281150817871\n",
      "Epoch 440, val loss: 0.906264066696167\n",
      "Epoch 450, training loss: 641.5244750976562 = 0.7467882633209229 + 100 * 6.407777309417725\n",
      "Epoch 450, val loss: 0.8996762633323669\n",
      "Epoch 460, training loss: 642.0057373046875 = 0.7286452651023865 + 100 * 6.412771224975586\n",
      "Epoch 460, val loss: 0.8880949020385742\n",
      "Epoch 470, training loss: 640.7015380859375 = 0.7132714986801147 + 100 * 6.399882793426514\n",
      "Epoch 470, val loss: 0.8825547099113464\n",
      "Epoch 480, training loss: 641.8826293945312 = 0.6920902729034424 + 100 * 6.411905288696289\n",
      "Epoch 480, val loss: 0.8664342164993286\n",
      "Epoch 490, training loss: 640.7890625 = 0.6781553030014038 + 100 * 6.401108741760254\n",
      "Epoch 490, val loss: 0.8615360856056213\n",
      "Epoch 500, training loss: 639.535888671875 = 0.6671088337898254 + 100 * 6.388687610626221\n",
      "Epoch 500, val loss: 0.8584086298942566\n",
      "Epoch 510, training loss: 642.8735961914062 = 0.6566919088363647 + 100 * 6.422169208526611\n",
      "Epoch 510, val loss: 0.8551090955734253\n",
      "Epoch 520, training loss: 640.529541015625 = 0.6373589038848877 + 100 * 6.398921966552734\n",
      "Epoch 520, val loss: 0.8423970341682434\n",
      "Epoch 530, training loss: 639.1076049804688 = 0.6236010193824768 + 100 * 6.38484001159668\n",
      "Epoch 530, val loss: 0.8370981812477112\n",
      "Epoch 540, training loss: 639.101318359375 = 0.6133402585983276 + 100 * 6.3848795890808105\n",
      "Epoch 540, val loss: 0.8347387313842773\n",
      "Epoch 550, training loss: 639.0292358398438 = 0.6001339554786682 + 100 * 6.38429069519043\n",
      "Epoch 550, val loss: 0.8289783000946045\n",
      "Epoch 560, training loss: 638.2081298828125 = 0.5888285040855408 + 100 * 6.376193523406982\n",
      "Epoch 560, val loss: 0.8239976167678833\n",
      "Epoch 570, training loss: 638.2418212890625 = 0.5759431719779968 + 100 * 6.376658916473389\n",
      "Epoch 570, val loss: 0.8154981732368469\n",
      "Epoch 580, training loss: 638.7869262695312 = 0.5652849674224854 + 100 * 6.382216453552246\n",
      "Epoch 580, val loss: 0.8119034171104431\n",
      "Epoch 590, training loss: 637.5098266601562 = 0.5559126734733582 + 100 * 6.369539260864258\n",
      "Epoch 590, val loss: 0.8091481924057007\n",
      "Epoch 600, training loss: 638.3580932617188 = 0.547991156578064 + 100 * 6.378101348876953\n",
      "Epoch 600, val loss: 0.80866938829422\n",
      "Epoch 610, training loss: 638.9732666015625 = 0.5349536538124084 + 100 * 6.384382724761963\n",
      "Epoch 610, val loss: 0.8014382719993591\n",
      "Epoch 620, training loss: 637.1691284179688 = 0.5242087841033936 + 100 * 6.366448879241943\n",
      "Epoch 620, val loss: 0.7954917550086975\n",
      "Epoch 630, training loss: 636.5751342773438 = 0.5170226097106934 + 100 * 6.360580921173096\n",
      "Epoch 630, val loss: 0.7955236434936523\n",
      "Epoch 640, training loss: 638.4197387695312 = 0.5073521137237549 + 100 * 6.379124164581299\n",
      "Epoch 640, val loss: 0.7931056618690491\n",
      "Epoch 650, training loss: 637.3732299804688 = 0.4977502226829529 + 100 * 6.368754863739014\n",
      "Epoch 650, val loss: 0.7899861931800842\n",
      "Epoch 660, training loss: 636.3311767578125 = 0.48774102330207825 + 100 * 6.358434200286865\n",
      "Epoch 660, val loss: 0.7861679196357727\n",
      "Epoch 670, training loss: 636.1935424804688 = 0.4818361699581146 + 100 * 6.357117176055908\n",
      "Epoch 670, val loss: 0.7851928472518921\n",
      "Epoch 680, training loss: 638.751708984375 = 0.4730844795703888 + 100 * 6.382786273956299\n",
      "Epoch 680, val loss: 0.7810469269752502\n",
      "Epoch 690, training loss: 636.0681762695312 = 0.4638517200946808 + 100 * 6.356042861938477\n",
      "Epoch 690, val loss: 0.7761590480804443\n",
      "Epoch 700, training loss: 635.4922485351562 = 0.4570977985858917 + 100 * 6.350351333618164\n",
      "Epoch 700, val loss: 0.7752387523651123\n",
      "Epoch 710, training loss: 637.3612060546875 = 0.44948601722717285 + 100 * 6.369117259979248\n",
      "Epoch 710, val loss: 0.7762894034385681\n",
      "Epoch 720, training loss: 636.4683837890625 = 0.4405999183654785 + 100 * 6.3602776527404785\n",
      "Epoch 720, val loss: 0.7731257081031799\n",
      "Epoch 730, training loss: 636.1910400390625 = 0.4333285987377167 + 100 * 6.357576847076416\n",
      "Epoch 730, val loss: 0.7670995593070984\n",
      "Epoch 740, training loss: 635.349609375 = 0.4271891117095947 + 100 * 6.349224090576172\n",
      "Epoch 740, val loss: 0.7692822217941284\n",
      "Epoch 750, training loss: 637.2493896484375 = 0.4221670925617218 + 100 * 6.368272304534912\n",
      "Epoch 750, val loss: 0.76853346824646\n",
      "Epoch 760, training loss: 635.0310668945312 = 0.4118765592575073 + 100 * 6.346191883087158\n",
      "Epoch 760, val loss: 0.7615153193473816\n",
      "Epoch 770, training loss: 634.7171630859375 = 0.40661272406578064 + 100 * 6.343105316162109\n",
      "Epoch 770, val loss: 0.7611885070800781\n",
      "Epoch 780, training loss: 636.0640869140625 = 0.39951953291893005 + 100 * 6.356645584106445\n",
      "Epoch 780, val loss: 0.7616478800773621\n",
      "Epoch 790, training loss: 635.7239379882812 = 0.39179760217666626 + 100 * 6.353321552276611\n",
      "Epoch 790, val loss: 0.7556948065757751\n",
      "Epoch 800, training loss: 634.9149780273438 = 0.38673698902130127 + 100 * 6.345282554626465\n",
      "Epoch 800, val loss: 0.7569147348403931\n",
      "Epoch 810, training loss: 634.399658203125 = 0.38243526220321655 + 100 * 6.340171813964844\n",
      "Epoch 810, val loss: 0.7558572292327881\n",
      "Epoch 820, training loss: 635.6427612304688 = 0.37807902693748474 + 100 * 6.352647304534912\n",
      "Epoch 820, val loss: 0.7553092241287231\n",
      "Epoch 830, training loss: 635.2112426757812 = 0.3697269856929779 + 100 * 6.348414897918701\n",
      "Epoch 830, val loss: 0.7506685853004456\n",
      "Epoch 840, training loss: 634.3243408203125 = 0.36522743105888367 + 100 * 6.339591026306152\n",
      "Epoch 840, val loss: 0.7512746453285217\n",
      "Epoch 850, training loss: 634.6577758789062 = 0.36010831594467163 + 100 * 6.3429765701293945\n",
      "Epoch 850, val loss: 0.7489684224128723\n",
      "Epoch 860, training loss: 634.5314331054688 = 0.35523080825805664 + 100 * 6.341762065887451\n",
      "Epoch 860, val loss: 0.7495896220207214\n",
      "Epoch 870, training loss: 633.6841430664062 = 0.349960595369339 + 100 * 6.333341598510742\n",
      "Epoch 870, val loss: 0.7487584352493286\n",
      "Epoch 880, training loss: 634.0548095703125 = 0.34479689598083496 + 100 * 6.337100505828857\n",
      "Epoch 880, val loss: 0.7495181560516357\n",
      "Epoch 890, training loss: 634.3980102539062 = 0.3411153554916382 + 100 * 6.340569019317627\n",
      "Epoch 890, val loss: 0.7491872310638428\n",
      "Epoch 900, training loss: 634.15478515625 = 0.33575403690338135 + 100 * 6.33819055557251\n",
      "Epoch 900, val loss: 0.7451780438423157\n",
      "Epoch 910, training loss: 633.1964111328125 = 0.3302505910396576 + 100 * 6.3286614418029785\n",
      "Epoch 910, val loss: 0.7416749000549316\n",
      "Epoch 920, training loss: 633.3172607421875 = 0.3278762698173523 + 100 * 6.329893589019775\n",
      "Epoch 920, val loss: 0.746479332447052\n",
      "Epoch 930, training loss: 634.2876586914062 = 0.32306134700775146 + 100 * 6.339645862579346\n",
      "Epoch 930, val loss: 0.7378126978874207\n",
      "Epoch 940, training loss: 633.3906860351562 = 0.31855520606040955 + 100 * 6.330721378326416\n",
      "Epoch 940, val loss: 0.7392882704734802\n",
      "Epoch 950, training loss: 633.6802368164062 = 0.31265634298324585 + 100 * 6.333675384521484\n",
      "Epoch 950, val loss: 0.7382371425628662\n",
      "Epoch 960, training loss: 633.6351928710938 = 0.30989915132522583 + 100 * 6.333252906799316\n",
      "Epoch 960, val loss: 0.737333357334137\n",
      "Epoch 970, training loss: 632.4682006835938 = 0.30583420395851135 + 100 * 6.321623802185059\n",
      "Epoch 970, val loss: 0.7388951182365417\n",
      "Epoch 980, training loss: 632.314208984375 = 0.30333685874938965 + 100 * 6.320108890533447\n",
      "Epoch 980, val loss: 0.7379735708236694\n",
      "Epoch 990, training loss: 633.2953491210938 = 0.2996322214603424 + 100 * 6.329957008361816\n",
      "Epoch 990, val loss: 0.7363893985748291\n",
      "=== picking the best model according to the performance on validation ===\n",
      "Accuracy of GNN+CL: 0.7444444444444445\n",
      "0.7843964153927254\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 861.61572265625 = 1.9340059757232666 + 100 * 8.596817016601562\n",
      "Epoch 0, val loss: 1.9331543445587158\n",
      "Epoch 10, training loss: 829.1777954101562 = 1.8923553228378296 + 100 * 8.272854804992676\n",
      "Epoch 10, val loss: 1.8897552490234375\n",
      "Epoch 20, training loss: 749.75 = 1.8455866575241089 + 100 * 7.479043960571289\n",
      "Epoch 20, val loss: 1.8453446626663208\n",
      "Epoch 30, training loss: 725.14306640625 = 1.8157151937484741 + 100 * 7.233273029327393\n",
      "Epoch 30, val loss: 1.8112410306930542\n",
      "Epoch 40, training loss: 711.1557006835938 = 1.7896735668182373 + 100 * 7.093660354614258\n",
      "Epoch 40, val loss: 1.7766329050064087\n",
      "Epoch 50, training loss: 700.4847412109375 = 1.7638299465179443 + 100 * 6.987208843231201\n",
      "Epoch 50, val loss: 1.7453099489212036\n",
      "Epoch 60, training loss: 691.3570556640625 = 1.7472503185272217 + 100 * 6.8960981369018555\n",
      "Epoch 60, val loss: 1.7283118963241577\n",
      "Epoch 70, training loss: 685.134521484375 = 1.7256333827972412 + 100 * 6.8340888023376465\n",
      "Epoch 70, val loss: 1.7046388387680054\n",
      "Epoch 80, training loss: 679.1522216796875 = 1.7006292343139648 + 100 * 6.7745161056518555\n",
      "Epoch 80, val loss: 1.6782724857330322\n",
      "Epoch 90, training loss: 675.21875 = 1.676274299621582 + 100 * 6.735424995422363\n",
      "Epoch 90, val loss: 1.6530909538269043\n",
      "Epoch 100, training loss: 672.903076171875 = 1.6485207080841064 + 100 * 6.712545871734619\n",
      "Epoch 100, val loss: 1.6257619857788086\n",
      "Epoch 110, training loss: 668.7114868164062 = 1.624391794204712 + 100 * 6.670870780944824\n",
      "Epoch 110, val loss: 1.6027559041976929\n",
      "Epoch 120, training loss: 666.9005126953125 = 1.5988067388534546 + 100 * 6.653017044067383\n",
      "Epoch 120, val loss: 1.5767797231674194\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m model \u001b[39m=\u001b[39m UnifyModel(args, encoder, num_hidden, num_proj_hidden, num_class, tau, lr\u001b[39m=\u001b[39mlearning_rate, weight_decay\u001b[39m=\u001b[39mweight_decay, device\u001b[39m=\u001b[39mdevice,data1\u001b[39m=\u001b[39mnoisy_data,data2\u001b[39m=\u001b[39mdiff_noisy_data)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m# model.fit(args, noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_train,idx_val=idx_val,train_iters=num_epochs,cont_iters=num_epochs,seen_node_idx=None)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit_1(args, noisy_data\u001b[39m.\u001b[39;49mx, noisy_data\u001b[39m.\u001b[39;49medge_index,noisy_data\u001b[39m.\u001b[39;49medge_weight,noisy_data\u001b[39m.\u001b[39;49my,idx_train,idx_val\u001b[39m=\u001b[39;49midx_val,train_iters\u001b[39m=\u001b[39;49mnum_epochs,cont_iters\u001b[39m=\u001b[39;49mnum_epochs,seen_node_idx\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39m# x, edge_index,edge_weight,labels,idx_train,idx_val=None,cont_iters=None,train_iters=200,seen_node_idx = None\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m acc_cl \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtest(noisy_data\u001b[39m.\u001b[39mx, noisy_data\u001b[39m.\u001b[39medge_index,noisy_data\u001b[39m.\u001b[39medge_weight,noisy_data\u001b[39m.\u001b[39my,idx_clean_test)\n",
      "File \u001b[0;32m~/project-contrastive/RobustCL/model.py:247\u001b[0m, in \u001b[0;36mUnifyModel.fit_1\u001b[0;34m(self, args, x, edge_index, edge_weight, labels, idx_train, idx_val, train_iters, cont_iters, seen_node_idx)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_without_val(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels, idx_train, train_iters,verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    245\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m     \u001b[39m# self._train_with_val(self.labels, idx_train, idx_val, train_iters)\u001b[39;00m\n\u001b[0;32m--> 247\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_with_val_2(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabels, idx_train, idx_val, train_iters,verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/project-contrastive/RobustCL/model.py:377\u001b[0m, in \u001b[0;36mUnifyModel._train_with_val_2\u001b[0;34m(self, labels, idx_train, idx_val, train_iters, verbose)\u001b[0m\n\u001b[1;32m    373\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    376\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval()\n\u001b[0;32m--> 377\u001b[0m cont_embds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49medge_index,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49medge_weight)\n\u001b[1;32m    378\u001b[0m clf_loss_val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclf_loss(cont_embds,labels,idx_val)\n\u001b[1;32m    379\u001b[0m \u001b[39m# loss_val = clf_loss_val + self.cont_weight * cont_loss\u001b[39;00m\n",
      "File \u001b[0;32m~/project-contrastive/RobustCL/model.py:155\u001b[0m, in \u001b[0;36mUnifyModel.forward\u001b[0;34m(self, x, edge_index, edge_weights)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    154\u001b[0m             edge_index: torch\u001b[39m.\u001b[39mTensor, edge_weights\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 155\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x, edge_index, edge_weights)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/project-contrastive/RobustCL/model.py:46\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x, edge_index, edge_weights)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor, edge_index: torch\u001b[39m.\u001b[39mTensor, edge_weights\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     45\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk):\n\u001b[0;32m---> 46\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv[i](x, edge_index))\n\u001b[1;32m     47\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch_geometric/nn/conv/gcn_conv.py:175\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    173\u001b[0m cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_edge_index\n\u001b[1;32m    174\u001b[0m \u001b[39mif\u001b[39;00m cache \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     edge_index, edge_weight \u001b[39m=\u001b[39m gcn_norm(  \u001b[39m# yapf: disable\u001b[39;49;00m\n\u001b[1;32m    176\u001b[0m         edge_index, edge_weight, x\u001b[39m.\u001b[39;49msize(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnode_dim),\n\u001b[1;32m    177\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimproved, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_self_loops, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflow)\n\u001b[1;32m    178\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcached:\n\u001b[1;32m    179\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_edge_index \u001b[39m=\u001b[39m (edge_index, edge_weight)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch_geometric/nn/conv/gcn_conv.py:60\u001b[0m, in \u001b[0;36mgcn_norm\u001b[0;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[1;32m     56\u001b[0m     edge_weight \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones((edge_index\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m), ), dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m     57\u001b[0m                              device\u001b[39m=\u001b[39medge_index\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     59\u001b[0m \u001b[39mif\u001b[39;00m add_self_loops:\n\u001b[0;32m---> 60\u001b[0m     edge_index, tmp_edge_weight \u001b[39m=\u001b[39m add_remaining_self_loops(\n\u001b[1;32m     61\u001b[0m         edge_index, edge_weight, fill_value, num_nodes)\n\u001b[1;32m     62\u001b[0m     \u001b[39massert\u001b[39;00m tmp_edge_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     edge_weight \u001b[39m=\u001b[39m tmp_edge_weight\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch_geometric/utils/loop.py:224\u001b[0m, in \u001b[0;36madd_remaining_self_loops\u001b[0;34m(edge_index, edge_attr, fill_value, num_nodes)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo valid \u001b[39m\u001b[39m'\u001b[39m\u001b[39mfill_value\u001b[39m\u001b[39m'\u001b[39m\u001b[39m provided\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    223\u001b[0m     inv_mask \u001b[39m=\u001b[39m \u001b[39m~\u001b[39mmask\n\u001b[0;32m--> 224\u001b[0m     loop_attr[edge_index[\u001b[39m0\u001b[39m][inv_mask]] \u001b[39m=\u001b[39m edge_attr[inv_mask]\n\u001b[1;32m    226\u001b[0m     edge_attr \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([edge_attr[mask], loop_attr], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    228\u001b[0m edge_index \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([edge_index[:, mask], loop_index], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import copy \n",
    "# from model import UnifyModel\n",
    "# from models.construct import model_construct\n",
    "\n",
    "\n",
    "# import os.path as osp\n",
    "# import random\n",
    "# from time import perf_counter as t\n",
    "# import yaml\n",
    "# from yaml import SafeLoader\n",
    "\n",
    "# import torch\n",
    "# import torch_geometric.transforms as T\n",
    "# from model import Encoder, Model, drop_feature\n",
    "\n",
    "# data = data.to(device)\n",
    "# config = yaml.load(open(args.config), Loader=SafeLoader)[args.dataset]\n",
    "# # num_epochs = config['num_epochs']\n",
    "# # args.cl_lr = config['args.cl_lr']\n",
    "# # weight_decay = config['weight_decay']\n",
    "# args.seed = config['seed']\n",
    "# # args.cont_batch_size = config['cont_batch_size']\n",
    "# # args.cont_weight = config['cont_weight']\n",
    "# # args.add_edge_rate_1 = config['add_edge_rate_1']\n",
    "# # args.add_edge_rate_2 = config['add_edge_rate_2']\n",
    "# # args.drop_edge_rate_1 = config['drop_edge_rate_1']\n",
    "# # args.drop_edge_rate_2 = config['drop_edge_rate_2']\n",
    "# # args.drop_feat_rate_1 = config['drop_feature_rate_1']\n",
    "# # args.drop_feat_rate_2 = config['drop_feature_rate_2']\n",
    "# # args.add_edge_rate_1 = 0\n",
    "# # args.add_edge_rate_2 = 0\n",
    "# # args.drop_edge_rate_1 = 0.3\n",
    "# # args.drop_edge_rate_2 = 0.5\n",
    "# # args.drop_feat_rate_1 = 0.4\n",
    "# # args.drop_feat_rate_2 = 0.4\n",
    "# num_class = int(data.y.max()+1)\n",
    "\n",
    "# noisy_data = construct_noisy_graph(data,perturb_ratio=0.25,mode='random_noise')\n",
    "# noisy_data = noisy_data.to(device)\n",
    "\n",
    "# diff_dataset = PPRDataset(noisy_data,args.dataset)\n",
    "# diff_noisy_data = diff_dataset.data.to(device)\n",
    "\n",
    "# seen_node_idx = (torch.bitwise_not(data.test_mask)).nonzero().flatten()\n",
    "# idx_overall_test = (torch.bitwise_not(data.train_mask)&torch.bitwise_not(data.val_mask)).nonzero().flatten()\n",
    "\n",
    "\n",
    "\n",
    "# final_cl_acc_noisy = []\n",
    "# final_gnn_acc_noisy = []\n",
    "# print(\"=== Noisy graph ===\")\n",
    "# rs = np.random.RandomState(args.seed)\n",
    "# seeds = rs.randint(1000,size=3)\n",
    "# for seed in seeds:\n",
    "#     np.random.seed(seed)\n",
    "#     # torch.manual_seed(seed)\n",
    "#     # torch.cuda.manual_seed(seed)\n",
    "#     '''Transductive'''\n",
    "#     encoder = Encoder(dataset.num_features, args.num_hidden, args.cl_activation,\n",
    "#                             base_model=args.cl_base_model, k=args.cl_num_layers).to(device)\n",
    "#     # model = UnifyModel(args, encoder, args.cl_num_hidden, args.cl_num_proj_hidden, num_class, args.tau, lr=args.cl_lr, weight_decay=weight_decay, device=device).to(device)\n",
    "#     model = UnifyModel(args, encoder, args.cl_num_hidden, args.cl_num_proj_hidden, num_class, args.tau, lr=args.cl_lr, weight_decay=weight_decay, device=device,data1=noisy_data,data2=diff_noisy_data).to(device)\n",
    "#     # model.fit(args, noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_train,idx_val=idx_val,train_iters=num_epochs,cont_iters=num_epochs,seen_node_idx=None)\n",
    "#     model.fit_1(args, noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_train,idx_val=idx_val,train_iters=num_epochs,cont_iters=num_epochs,seen_node_idx=None)\n",
    "#     # x, edge_index,edge_weight,labels,idx_train,idx_val=None,cont_iters=None,train_iters=200,seen_node_idx = None\n",
    "#     acc_cl = model.test(noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_clean_test)\n",
    "#     print(\"Accuracy of GNN+CL: {}\".format(acc_cl))\n",
    "#     final_cl_acc_noisy.append(acc_cl)\n",
    "#     gnn_model = model_construct(args,'GCN',noisy_data,device)\n",
    "#     gnn_model.fit(noisy_data.x, noisy_data.edge_index, None, noisy_data.y, idx_train, idx_val,train_iters=args.epochs,verbose=False)\n",
    "#     clean_acc = gnn_model.test(noisy_data.x,noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_overall_test)\n",
    "#     print(clean_acc)\n",
    "#     final_gnn_acc_noisy.append(clean_acc)\n",
    "\n",
    "\n",
    "# print('The final CL Acc:{:.5f}, {:.5f}, The final GNN Acc:{:.5f}, {:.5f}'\\\n",
    "#             .format(np.average(final_cl_acc_noisy),np.std(final_cl_acc_noisy),np.average(final_gnn_acc_noisy),np.std(final_gnn_acc_noisy)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38_torch110')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4260eba67904b42d68a3963bc583366103d86fb6c89846e20de6072b78e7707"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
