{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* implement unifyGNN+Contrastive\n",
    "* try different noisy and augmentation\n",
    "* try against with/without unnoticeable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1632255/1168092800.py:6: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(add_edge_rate_1=0, add_edge_rate_2=0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3500, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=3, dis_weight=1, drop_edge_rate_1=0, drop_edge_rate_2=0.5, drop_feat_rate_1=0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=400, use_vs_number=True, vs_number=160, vs_ratio=0, weight_decay=0.0005)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]: \n",
    "\n",
    "import imp\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch_geometric.datasets import Planetoid,Reddit2,Flickr,PPI\n",
    "\n",
    "\n",
    "# from torch_geometric.loader import DataLoader\n",
    "# from help_funcs import prune_unrelated_edge,prune_unrelated_edge_isolated\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--debug', action='store_true',\n",
    "        default=True, help='debug mode')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--seed', type=int, default=10, help='Random seed.')\n",
    "parser.add_argument('--model', type=str, default='GCN', help='model',\n",
    "                    choices=['GCN','GAT','GraphSage','GIN'])\n",
    "parser.add_argument('--dataset', type=str, default='Cora', \n",
    "                    help='Dataset',\n",
    "                    choices=['Cora','Citeseer','Pubmed','PPI','Flickr','ogbn-arxiv','Reddit','Reddit2','Yelp'])\n",
    "parser.add_argument('--train_lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=128,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--proj_hidden', type=int, default=128,\n",
    "                    help='Number of hidden units in MLP.')\n",
    "parser.add_argument('--thrd', type=float, default=0.5)\n",
    "parser.add_argument('--target_class', type=int, default=0)\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('--epochs', type=int,  default=200, help='Number of epochs to train benign and backdoor model.')\n",
    "parser.add_argument('--trojan_epochs', type=int,  default=400, help='Number of epochs to train trigger generator.')\n",
    "parser.add_argument('--inner', type=int,  default=1, help='Number of inner')\n",
    "parser.add_argument('--temperature', type=float,  default=0.5, help='Temperature')\n",
    "# backdoor setting\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--trigger_size', type=int, default=3,\n",
    "                    help='tirgger_size')\n",
    "parser.add_argument('--use_vs_number', action='store_true', default=True,\n",
    "                    help=\"if use detailed number to decide Vs\")\n",
    "parser.add_argument('--vs_ratio', type=float, default=0,\n",
    "                    help=\"ratio of poisoning nodes relative to the full graph\")\n",
    "parser.add_argument('--vs_number', type=int, default=160,\n",
    "                    help=\"number of poisoning nodes relative to the full graph\")\n",
    "# defense setting\n",
    "parser.add_argument('--defense_mode', type=str, default=\"none\",\n",
    "                    choices=['prune', 'isolate', 'none'],\n",
    "                    help=\"Mode of defense\")\n",
    "parser.add_argument('--prune_thr', type=float, default=0.2,\n",
    "                    help=\"Threshold of prunning edges\")\n",
    "parser.add_argument('--target_loss_weight', type=float, default=1,\n",
    "                    help=\"Weight of optimize outter trigger generator\")\n",
    "parser.add_argument('--homo_loss_weight', type=float, default=0,\n",
    "                    help=\"Weight of optimize similarity loss\")\n",
    "parser.add_argument('--homo_boost_thrd', type=float, default=0.5,\n",
    "                    help=\"Threshold of increase similarity\")\n",
    "# attack setting\n",
    "parser.add_argument('--dis_weight', type=float, default=1,\n",
    "                    help=\"Weight of cluster distance\")\n",
    "parser.add_argument('--selection_method', type=str, default='cluster_degree',\n",
    "                    choices=['loss','conf','cluster','none','cluster_degree'],\n",
    "                    help='Method to select idx_attach for training trojan model (none means randomly select)')\n",
    "parser.add_argument('--test_model', type=str, default='GCN',\n",
    "                    choices=['GCN','GAT','GraphSage','GIN'],\n",
    "                    help='Model used to attack')\n",
    "parser.add_argument('--evaluate_mode', type=str, default='1by1',\n",
    "                    choices=['overall','1by1'],\n",
    "                    help='Model used to attack')\n",
    "# GPU setting\n",
    "parser.add_argument('--device_id', type=int, default=3,\n",
    "                    help=\"Threshold of prunning edges\")\n",
    "# GRACE setting\n",
    "parser.add_argument('--gpu_id', type=int, default=0)\n",
    "parser.add_argument('--config', type=str, default=\"config.yaml\")\n",
    "## Contrasitve setting\n",
    "parser.add_argument('--cl_lr', type=float, default=0.0002)\n",
    "parser.add_argument('--cl_num_hidden', type=int, default=128)\n",
    "parser.add_argument('--cl_num_proj_hidden', type=int, default=128)\n",
    "parser.add_argument('--cl_num_layers', type=int, default=2)\n",
    "parser.add_argument('--cl_activation', type=str, default='relu')\n",
    "parser.add_argument('--cl_base_model', type=str, default='GCNConv')\n",
    "parser.add_argument('--cont_weight', type=float, default=100)\n",
    "parser.add_argument('--add_edge_rate_1', type=float, default=0)\n",
    "parser.add_argument('--add_edge_rate_2', type=float, default=0)\n",
    "parser.add_argument('--drop_edge_rate_1', type=float, default=0)\n",
    "parser.add_argument('--drop_edge_rate_2', type=float, default=0.5)\n",
    "parser.add_argument('--drop_feat_rate_1', type=float, default=0)\n",
    "parser.add_argument('--drop_feat_rate_2', type=float, default=0.5)\n",
    "parser.add_argument('--tau', type=float, default=0.4)\n",
    "parser.add_argument('--cl_num_epochs', type=int, default=3500)\n",
    "parser.add_argument('--cl_weight_decay', type=float, default=0.00001)\n",
    "parser.add_argument('--cont_batch_size', type=int, default=0)\n",
    "parser.add_argument('--noisy_level', type=float, default=0.1)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_known_args()[0]\n",
    "args.cuda =  not args.no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(('cuda:{}' if torch.cuda.is_available() else 'cpu').format(args.device_id))\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "# torch.manual_seed(args.seed)\n",
    "# torch.cuda.manual_seed(args.seed)\n",
    "print(args)\n",
    "#%%\n",
    "from torch_geometric.utils import to_undirected\n",
    "import torch_geometric.transforms as T\n",
    "transform = T.Compose([T.NormalizeFeatures()])\n",
    "\n",
    "if(args.dataset == 'Cora' or args.dataset == 'Citeseer' or args.dataset == 'Pubmed'):\n",
    "    dataset = Planetoid(root='./data/', \\\n",
    "                        name=args.dataset,\\\n",
    "                        transform=transform)\n",
    "elif(args.dataset == 'Flickr'):\n",
    "    dataset = Flickr(root='./data/Flickr/', \\\n",
    "                    transform=transform)\n",
    "elif(args.dataset == 'Reddit2'):\n",
    "    dataset = Reddit2(root='./data/Reddit2/', \\\n",
    "                    transform=transform)\n",
    "elif(args.dataset == 'ogbn-arxiv'):\n",
    "    from ogb.nodeproppred import PygNodePropPredDataset\n",
    "    # Download and process data at './dataset/ogbg_molhiv/'\n",
    "    dataset = PygNodePropPredDataset(name = 'ogbn-arxiv', root='./data/')\n",
    "    split_idx = dataset.get_idx_split() \n",
    "\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "if(args.dataset == 'ogbn-arxiv'):\n",
    "    nNode = data.x.shape[0]\n",
    "    setattr(data,'train_mask',torch.zeros(nNode, dtype=torch.bool).to(device))\n",
    "    # dataset[0].train_mask = torch.zeros(nEdge, dtype=torch.bool).to(device)\n",
    "    data.val_mask = torch.zeros(nNode, dtype=torch.bool).to(device)\n",
    "    data.test_mask = torch.zeros(nNode, dtype=torch.bool).to(device)\n",
    "    data.y = data.y.squeeze(1)\n",
    "# we build our own train test split \n",
    "#%% \n",
    "from utils import get_split\n",
    "data, idx_train, idx_val, idx_clean_test, idx_atk = get_split(args,data,device)\n",
    "\n",
    "from torch_geometric.utils import to_undirected\n",
    "from utils import subgraph\n",
    "data.edge_index = to_undirected(data.edge_index)\n",
    "train_edge_index,_, edge_mask = subgraph(torch.bitwise_not(data.test_mask),data.edge_index,relabel_nodes=False)\n",
    "mask_edge_index = data.edge_index[:,torch.bitwise_not(edge_mask)]\n",
    "# filter out the unlabeled nodes except from training nodes and testing nodes, nonzero() is to get index, flatten is to get 1-d tensor\n",
    "unlabeled_idx = (torch.bitwise_not(data.test_mask)&torch.bitwise_not(data.train_mask)).nonzero().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_dense_adj, dense_to_sparse\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from scipy.linalg import expm\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.datasets import Planetoid, Amazon, Coauthor\n",
    "\n",
    "# from seeds import development_seed\n",
    "DATA_PATH = './data/'\n",
    "\n",
    "def get_dataset(name: str, use_lcc: bool = True) -> InMemoryDataset:\n",
    "    path = os.path.join(DATA_PATH, name)\n",
    "    if name in ['Cora', 'Citeseer', 'Pubmed']:\n",
    "        dataset = Planetoid(path, name)\n",
    "    elif name in ['Computers', 'Photo']:\n",
    "        dataset = Amazon(path, name)\n",
    "    elif name == 'CoauthorCS':\n",
    "        dataset = Coauthor(path, 'CS')\n",
    "    else:\n",
    "        raise Exception('Unknown dataset.')\n",
    "\n",
    "    if use_lcc:\n",
    "        lcc = get_largest_connected_component(dataset)\n",
    "\n",
    "        x_new = dataset.data.x[lcc]\n",
    "        y_new = dataset.data.y[lcc]\n",
    "\n",
    "        row, col = dataset.data.edge_index.numpy()\n",
    "        edges = [[i, j] for i, j in zip(row, col) if i in lcc and j in lcc]\n",
    "        edges = remap_edges(edges, get_node_mapper(lcc))\n",
    "        \n",
    "        data = Data(\n",
    "            x=x_new,\n",
    "            edge_index=torch.LongTensor(edges),\n",
    "            y=y_new,\n",
    "            train_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
    "            test_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
    "            val_mask=torch.zeros(y_new.size()[0], dtype=torch.bool)\n",
    "        )\n",
    "        dataset.data = data\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_component(dataset: InMemoryDataset, start: int = 0) -> set:\n",
    "    visited_nodes = set()\n",
    "    queued_nodes = set([start])\n",
    "    row, col = dataset.data.edge_index.numpy()\n",
    "    while queued_nodes:\n",
    "        current_node = queued_nodes.pop()\n",
    "        visited_nodes.update([current_node])\n",
    "        neighbors = col[np.where(row == current_node)[0]]\n",
    "        neighbors = [n for n in neighbors if n not in visited_nodes and n not in queued_nodes]\n",
    "        queued_nodes.update(neighbors)\n",
    "    return visited_nodes\n",
    "\n",
    "\n",
    "def get_largest_connected_component(dataset: InMemoryDataset) -> np.ndarray:\n",
    "    remaining_nodes = set(range(dataset.data.x.shape[0]))\n",
    "    comps = []\n",
    "    while remaining_nodes:\n",
    "        start = min(remaining_nodes)\n",
    "        comp = get_component(dataset, start)\n",
    "        comps.append(comp)\n",
    "        remaining_nodes = remaining_nodes.difference(comp)\n",
    "    return np.array(list(comps[np.argmax(list(map(len, comps)))]))\n",
    "\n",
    "\n",
    "def get_node_mapper(lcc: np.ndarray) -> dict:\n",
    "    mapper = {}\n",
    "    counter = 0\n",
    "    for node in lcc:\n",
    "        mapper[node] = counter\n",
    "        counter += 1\n",
    "    return mapper\n",
    "\n",
    "\n",
    "def remap_edges(edges: list, mapper: dict) -> list:\n",
    "    row = [e[0] for e in edges]\n",
    "    col = [e[1] for e in edges]\n",
    "    row = list(map(lambda x: mapper[x], row))\n",
    "    col = list(map(lambda x: mapper[x], col))\n",
    "    return [row, col]\n",
    "\n",
    "\n",
    "def get_adj_matrix(data) -> np.ndarray:\n",
    "    num_nodes = data.x.shape[0]\n",
    "    adj_matrix = np.zeros(shape=(num_nodes, num_nodes))\n",
    "    for i, j in zip(data.edge_index[0], data.edge_index[1]):\n",
    "        adj_matrix[i, j] = 1.\n",
    "    return adj_matrix\n",
    "\n",
    "\n",
    "def get_ppr_matrix(\n",
    "        adj_matrix: np.ndarray,\n",
    "        alpha: float = 0.1) -> np.ndarray:\n",
    "    num_nodes = adj_matrix.shape[0]\n",
    "    A_tilde = adj_matrix + np.eye(num_nodes)\n",
    "    D_tilde = np.diag(1/np.sqrt(A_tilde.sum(axis=1)))\n",
    "    H = D_tilde @ A_tilde @ D_tilde\n",
    "    return alpha * np.linalg.inv(np.eye(num_nodes) - (1 - alpha) * H)\n",
    "\n",
    "\n",
    "def get_heat_matrix(\n",
    "        adj_matrix: np.ndarray,\n",
    "        t: float = 5.0) -> np.ndarray:\n",
    "    num_nodes = adj_matrix.shape[0]\n",
    "    A_tilde = adj_matrix + np.eye(num_nodes)\n",
    "    D_tilde = np.diag(1/np.sqrt(A_tilde.sum(axis=1)))\n",
    "    H = D_tilde @ A_tilde @ D_tilde\n",
    "    return expm(-t * (np.eye(num_nodes) - H))\n",
    "\n",
    "\n",
    "def get_top_k_matrix(A: np.ndarray, k: int = 128) -> np.ndarray:\n",
    "    num_nodes = A.shape[0]\n",
    "    row_idx = np.arange(num_nodes)\n",
    "    A[A.argsort(axis=0)[:num_nodes - k], row_idx] = 0.\n",
    "    norm = A.sum(axis=0)\n",
    "    norm[norm <= 0] = 1 # avoid dividing by zero\n",
    "    return A/norm\n",
    "\n",
    "\n",
    "def get_clipped_matrix(A: np.ndarray, eps: float = 0.01) -> np.ndarray:\n",
    "    num_nodes = A.shape[0]\n",
    "    A[A < eps] = 0.\n",
    "    norm = A.sum(axis=0)\n",
    "    norm[norm <= 0] = 1 # avoid dividing by zero\n",
    "    return A/norm\n",
    "\n",
    "\n",
    "def set_train_val_test_split(\n",
    "        seed: int,\n",
    "        data: Data,\n",
    "        num_development: int = 1500,\n",
    "        num_per_class: int = 20) -> Data:\n",
    "    rnd_state = np.random.RandomState(development_seed)\n",
    "    num_nodes = data.y.shape[0]\n",
    "    development_idx = rnd_state.choice(num_nodes, num_development, replace=False)\n",
    "    test_idx = [i for i in np.arange(num_nodes) if i not in development_idx]\n",
    "\n",
    "    train_idx = []\n",
    "    rnd_state = np.random.RandomState(seed)\n",
    "    for c in range(data.y.max() + 1):\n",
    "        class_idx = development_idx[np.where(data.y[development_idx].cpu() == c)[0]]\n",
    "        train_idx.extend(rnd_state.choice(class_idx, num_per_class, replace=False))\n",
    "\n",
    "    val_idx = [i for i in development_idx if i not in train_idx]\n",
    "\n",
    "    def get_mask(idx):\n",
    "        mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        mask[idx] = 1\n",
    "        return mask\n",
    "\n",
    "    data.train_mask = get_mask(train_idx)\n",
    "    data.val_mask = get_mask(val_idx)\n",
    "    data.test_mask = get_mask(test_idx)\n",
    "\n",
    "    return data\n",
    "\n",
    "class PPRDataset(InMemoryDataset):\n",
    "    \"\"\"\n",
    "    Dataset preprocessed with GDC using PPR diffusion.\n",
    "    Note that this implementations is not scalable\n",
    "    since we directly invert the adjacency matrix.\n",
    "    \"\"\"\n",
    "    def __init__(self,noisy_data,\n",
    "                 name: str = 'Cora',\n",
    "                 use_lcc: bool = True,\n",
    "                 alpha: float = 0.1,\n",
    "                 k: int = 16,\n",
    "                 eps: float = None):\n",
    "        self.name = name\n",
    "        self.use_lcc = use_lcc\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "        self.eps = eps\n",
    "        self.noisy_data = noisy_data\n",
    "\n",
    "        super(PPRDataset, self).__init__(DATA_PATH)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> list:\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> list:\n",
    "        return [str(self) + '.pt']\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # base = get_dataset(name=self.name, use_lcc=self.use_lcc)\n",
    "        # generate adjacency matrix from sparse representation\n",
    "        adj_matrix = get_adj_matrix(self.noisy_data)\n",
    "        # obtain exact PPR matrix\n",
    "        ppr_matrix = get_ppr_matrix(adj_matrix,\n",
    "                                        alpha=self.alpha)\n",
    "\n",
    "        if self.k:\n",
    "            print(f'Selecting top {self.k} edges per node.')\n",
    "            ppr_matrix = get_top_k_matrix(ppr_matrix, k=self.k)\n",
    "        elif self.eps:\n",
    "            print(f'Selecting edges with weight greater than {self.eps}.')\n",
    "            ppr_matrix = get_clipped_matrix(ppr_matrix, eps=self.eps)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        # create PyG Data object\n",
    "        edges_i = []\n",
    "        edges_j = []\n",
    "        edge_attr = []\n",
    "        for i, row in enumerate(ppr_matrix):\n",
    "            for j in np.where(row > 0)[0]:\n",
    "                edges_i.append(i)\n",
    "                edges_j.append(j)\n",
    "                edge_attr.append(ppr_matrix[i, j])\n",
    "        edge_index = [edges_i, edges_j]\n",
    "\n",
    "        data = Data(\n",
    "            x=self.noisy_data.x,\n",
    "            edge_index=torch.LongTensor(edge_index),\n",
    "            edge_attr=torch.FloatTensor(edge_attr),\n",
    "            y=self.noisy_data.y,\n",
    "            train_mask=torch.zeros(self.noisy_data.train_mask.size()[0], dtype=torch.bool),\n",
    "            test_mask=torch.zeros(self.noisy_data.test_mask.size()[0], dtype=torch.bool),\n",
    "            val_mask=torch.zeros(self.noisy_data.val_mask.size()[0], dtype=torch.bool)\n",
    "        )\n",
    "\n",
    "        data, slices = self.collate([data])\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f'{self.name}_ppr_alpha={self.alpha}_k={self.k}_eps={self.eps}_lcc={self.use_lcc}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unify Contrastive GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw graph: torch.Size([2, 10556])\n",
      "add edge: torch.Size([2, 11612])\n",
      "remove edge: torch.Size([2, 9464])\n",
      "updated graph: torch.Size([2, 10520])\n",
      "=== Noisy graph ===\n",
      "=== training gcn model ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.labels = torch.tensor(labels, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss: 861.6282958984375 = 1.936873435974121 + 100 * 8.596914291381836\n",
      "Epoch 0, val loss: 1.9318678379058838\n",
      "Epoch 10, training loss: 861.6034545898438 = 1.9273982048034668 + 100 * 8.596760749816895\n",
      "Epoch 10, val loss: 1.9221259355545044\n",
      "Epoch 20, training loss: 861.3838500976562 = 1.9164669513702393 + 100 * 8.594674110412598\n",
      "Epoch 20, val loss: 1.9106049537658691\n",
      "Epoch 30, training loss: 858.71337890625 = 1.9035669565200806 + 100 * 8.568098068237305\n",
      "Epoch 30, val loss: 1.8970777988433838\n",
      "Epoch 40, training loss: 837.6519165039062 = 1.8873953819274902 + 100 * 8.357645034790039\n",
      "Epoch 40, val loss: 1.8805382251739502\n",
      "Epoch 50, training loss: 811.8319091796875 = 1.8729958534240723 + 100 * 8.099589347839355\n",
      "Epoch 50, val loss: 1.8672285079956055\n",
      "Epoch 60, training loss: 773.320068359375 = 1.8650013208389282 + 100 * 7.714550971984863\n",
      "Epoch 60, val loss: 1.85905921459198\n",
      "Epoch 70, training loss: 738.9059448242188 = 1.8574953079223633 + 100 * 7.370484828948975\n",
      "Epoch 70, val loss: 1.850788950920105\n",
      "Epoch 80, training loss: 721.3870849609375 = 1.8486484289169312 + 100 * 7.195384502410889\n",
      "Epoch 80, val loss: 1.8420718908309937\n",
      "Epoch 90, training loss: 711.8973999023438 = 1.8408551216125488 + 100 * 7.100565433502197\n",
      "Epoch 90, val loss: 1.8347275257110596\n",
      "Epoch 100, training loss: 703.4639892578125 = 1.8358794450759888 + 100 * 7.0162811279296875\n",
      "Epoch 100, val loss: 1.830093264579773\n",
      "Epoch 110, training loss: 696.834716796875 = 1.8321634531021118 + 100 * 6.95002555847168\n",
      "Epoch 110, val loss: 1.8262619972229004\n",
      "Epoch 120, training loss: 690.8368530273438 = 1.828200101852417 + 100 * 6.890086650848389\n",
      "Epoch 120, val loss: 1.82177734375\n",
      "Epoch 130, training loss: 684.7868041992188 = 1.8242677450180054 + 100 * 6.829625129699707\n",
      "Epoch 130, val loss: 1.8173774480819702\n",
      "Epoch 140, training loss: 681.27685546875 = 1.8209471702575684 + 100 * 6.794559478759766\n",
      "Epoch 140, val loss: 1.8136850595474243\n",
      "Epoch 150, training loss: 677.7344970703125 = 1.817823886871338 + 100 * 6.759166717529297\n",
      "Epoch 150, val loss: 1.8103084564208984\n",
      "Epoch 160, training loss: 674.7308959960938 = 1.8146953582763672 + 100 * 6.729161739349365\n",
      "Epoch 160, val loss: 1.8070809841156006\n",
      "Epoch 170, training loss: 672.2845458984375 = 1.8115414381027222 + 100 * 6.70473051071167\n",
      "Epoch 170, val loss: 1.8038549423217773\n",
      "Epoch 180, training loss: 670.1904907226562 = 1.808440923690796 + 100 * 6.683820724487305\n",
      "Epoch 180, val loss: 1.8006092309951782\n",
      "Epoch 190, training loss: 668.622802734375 = 1.805587649345398 + 100 * 6.6681718826293945\n",
      "Epoch 190, val loss: 1.7975547313690186\n",
      "Epoch 200, training loss: 667.4407348632812 = 1.8028160333633423 + 100 * 6.656379699707031\n",
      "Epoch 200, val loss: 1.7945204973220825\n",
      "Epoch 210, training loss: 665.7118530273438 = 1.7999464273452759 + 100 * 6.6391191482543945\n",
      "Epoch 210, val loss: 1.7915476560592651\n",
      "Epoch 220, training loss: 663.9663696289062 = 1.7972028255462646 + 100 * 6.621691703796387\n",
      "Epoch 220, val loss: 1.7887852191925049\n",
      "Epoch 230, training loss: 664.2362060546875 = 1.7945151329040527 + 100 * 6.624417304992676\n",
      "Epoch 230, val loss: 1.7859019041061401\n",
      "Epoch 240, training loss: 662.5197143554688 = 1.791402816772461 + 100 * 6.607283115386963\n",
      "Epoch 240, val loss: 1.7827824354171753\n",
      "Epoch 250, training loss: 661.4112548828125 = 1.7883479595184326 + 100 * 6.596229553222656\n",
      "Epoch 250, val loss: 1.7797785997390747\n",
      "Epoch 260, training loss: 660.7260131835938 = 1.7853924036026 + 100 * 6.5894060134887695\n",
      "Epoch 260, val loss: 1.7768807411193848\n",
      "Epoch 270, training loss: 660.76123046875 = 1.7824691534042358 + 100 * 6.589787483215332\n",
      "Epoch 270, val loss: 1.7739067077636719\n",
      "Epoch 280, training loss: 659.838134765625 = 1.77901029586792 + 100 * 6.580591678619385\n",
      "Epoch 280, val loss: 1.7705477476119995\n",
      "Epoch 290, training loss: 659.2400512695312 = 1.7757658958435059 + 100 * 6.574642658233643\n",
      "Epoch 290, val loss: 1.7674883604049683\n",
      "Epoch 300, training loss: 658.8014526367188 = 1.7727115154266357 + 100 * 6.570287227630615\n",
      "Epoch 300, val loss: 1.7645312547683716\n",
      "Epoch 310, training loss: 658.8659057617188 = 1.7692439556121826 + 100 * 6.570966720581055\n",
      "Epoch 310, val loss: 1.7612991333007812\n",
      "Epoch 320, training loss: 657.8113403320312 = 1.7657747268676758 + 100 * 6.560455799102783\n",
      "Epoch 320, val loss: 1.7578788995742798\n",
      "Epoch 330, training loss: 657.5432739257812 = 1.7624835968017578 + 100 * 6.557807445526123\n",
      "Epoch 330, val loss: 1.754854679107666\n",
      "Epoch 340, training loss: 656.8646240234375 = 1.7590305805206299 + 100 * 6.551055908203125\n",
      "Epoch 340, val loss: 1.751387119293213\n",
      "Epoch 350, training loss: 656.2105712890625 = 1.7555007934570312 + 100 * 6.544550895690918\n",
      "Epoch 350, val loss: 1.7482067346572876\n",
      "Epoch 360, training loss: 655.8613891601562 = 1.75191330909729 + 100 * 6.541094779968262\n",
      "Epoch 360, val loss: 1.7446379661560059\n",
      "Epoch 370, training loss: 654.9122314453125 = 1.7480976581573486 + 100 * 6.531641483306885\n",
      "Epoch 370, val loss: 1.7412923574447632\n",
      "Epoch 380, training loss: 654.1358642578125 = 1.7446341514587402 + 100 * 6.52391242980957\n",
      "Epoch 380, val loss: 1.7379978895187378\n",
      "Epoch 390, training loss: 653.7398071289062 = 1.7406870126724243 + 100 * 6.519991397857666\n",
      "Epoch 390, val loss: 1.7342792749404907\n",
      "Epoch 400, training loss: 653.6099243164062 = 1.736717700958252 + 100 * 6.51873254776001\n",
      "Epoch 400, val loss: 1.7308412790298462\n",
      "Epoch 410, training loss: 652.6605224609375 = 1.7325125932693481 + 100 * 6.509280204772949\n",
      "Epoch 410, val loss: 1.7265642881393433\n",
      "Epoch 420, training loss: 651.4790649414062 = 1.7285341024398804 + 100 * 6.4975056648254395\n",
      "Epoch 420, val loss: 1.7231043577194214\n",
      "Epoch 430, training loss: 651.8981323242188 = 1.724735140800476 + 100 * 6.501733779907227\n",
      "Epoch 430, val loss: 1.7195502519607544\n",
      "Epoch 440, training loss: 651.2410278320312 = 1.7195347547531128 + 100 * 6.495214939117432\n",
      "Epoch 440, val loss: 1.7146902084350586\n",
      "Epoch 450, training loss: 650.2809448242188 = 1.7149614095687866 + 100 * 6.485660076141357\n",
      "Epoch 450, val loss: 1.7106901407241821\n",
      "Epoch 460, training loss: 649.5090942382812 = 1.7107956409454346 + 100 * 6.477982997894287\n",
      "Epoch 460, val loss: 1.7068957090377808\n",
      "Epoch 470, training loss: 649.1854248046875 = 1.7057507038116455 + 100 * 6.474796772003174\n",
      "Epoch 470, val loss: 1.7023439407348633\n",
      "Epoch 480, training loss: 648.7788696289062 = 1.7000869512557983 + 100 * 6.47078800201416\n",
      "Epoch 480, val loss: 1.69728422164917\n",
      "Epoch 490, training loss: 648.4708862304688 = 1.695330262184143 + 100 * 6.4677557945251465\n",
      "Epoch 490, val loss: 1.6931064128875732\n",
      "Epoch 500, training loss: 648.2586669921875 = 1.6906524896621704 + 100 * 6.465680122375488\n",
      "Epoch 500, val loss: 1.6888561248779297\n",
      "Epoch 510, training loss: 648.0046997070312 = 1.6848177909851074 + 100 * 6.463199138641357\n",
      "Epoch 510, val loss: 1.6837049722671509\n",
      "Epoch 520, training loss: 647.3943481445312 = 1.6791657209396362 + 100 * 6.457152366638184\n",
      "Epoch 520, val loss: 1.67855966091156\n",
      "Epoch 530, training loss: 647.07958984375 = 1.674070119857788 + 100 * 6.454055309295654\n",
      "Epoch 530, val loss: 1.6740565299987793\n",
      "Epoch 540, training loss: 646.9404296875 = 1.668022632598877 + 100 * 6.452723979949951\n",
      "Epoch 540, val loss: 1.6688508987426758\n",
      "Epoch 550, training loss: 646.5689086914062 = 1.6615346670150757 + 100 * 6.449073791503906\n",
      "Epoch 550, val loss: 1.6629834175109863\n",
      "Epoch 560, training loss: 646.1121215820312 = 1.6559865474700928 + 100 * 6.444561004638672\n",
      "Epoch 560, val loss: 1.6583435535430908\n",
      "Epoch 570, training loss: 647.5114135742188 = 1.650300145149231 + 100 * 6.458610534667969\n",
      "Epoch 570, val loss: 1.6534453630447388\n",
      "Epoch 580, training loss: 646.0079956054688 = 1.6434155702590942 + 100 * 6.443645477294922\n",
      "Epoch 580, val loss: 1.647270679473877\n",
      "Epoch 590, training loss: 645.1233520507812 = 1.637352466583252 + 100 * 6.4348602294921875\n",
      "Epoch 590, val loss: 1.642324447631836\n",
      "Epoch 600, training loss: 646.0789794921875 = 1.6315433979034424 + 100 * 6.444474220275879\n",
      "Epoch 600, val loss: 1.6370733976364136\n",
      "Epoch 610, training loss: 645.5391235351562 = 1.6240525245666504 + 100 * 6.439151287078857\n",
      "Epoch 610, val loss: 1.6311213970184326\n",
      "Epoch 620, training loss: 644.2141723632812 = 1.6176170110702515 + 100 * 6.425965785980225\n",
      "Epoch 620, val loss: 1.6255475282669067\n",
      "Epoch 630, training loss: 644.3101196289062 = 1.6116119623184204 + 100 * 6.426984786987305\n",
      "Epoch 630, val loss: 1.6204650402069092\n",
      "Epoch 640, training loss: 644.6605224609375 = 1.6036239862442017 + 100 * 6.430568695068359\n",
      "Epoch 640, val loss: 1.6140327453613281\n",
      "Epoch 650, training loss: 643.7225341796875 = 1.5963833332061768 + 100 * 6.421261787414551\n",
      "Epoch 650, val loss: 1.6077115535736084\n",
      "Epoch 660, training loss: 643.7051391601562 = 1.5897654294967651 + 100 * 6.421153545379639\n",
      "Epoch 660, val loss: 1.6024638414382935\n",
      "Epoch 670, training loss: 642.858154296875 = 1.5820505619049072 + 100 * 6.412761211395264\n",
      "Epoch 670, val loss: 1.595855474472046\n",
      "Epoch 680, training loss: 642.5393676757812 = 1.5748459100723267 + 100 * 6.4096455574035645\n",
      "Epoch 680, val loss: 1.590040922164917\n",
      "Epoch 690, training loss: 642.7548217773438 = 1.567991018295288 + 100 * 6.411868572235107\n",
      "Epoch 690, val loss: 1.5845481157302856\n",
      "Epoch 700, training loss: 643.3549194335938 = 1.559712529182434 + 100 * 6.417952537536621\n",
      "Epoch 700, val loss: 1.576904296875\n",
      "Epoch 710, training loss: 642.8255615234375 = 1.5505707263946533 + 100 * 6.412749767303467\n",
      "Epoch 710, val loss: 1.570130467414856\n",
      "Epoch 720, training loss: 641.7780151367188 = 1.5433276891708374 + 100 * 6.402346611022949\n",
      "Epoch 720, val loss: 1.564212679862976\n",
      "Epoch 730, training loss: 641.62646484375 = 1.5361233949661255 + 100 * 6.400903224945068\n",
      "Epoch 730, val loss: 1.558239221572876\n",
      "Epoch 740, training loss: 642.8806762695312 = 1.5279587507247925 + 100 * 6.413527011871338\n",
      "Epoch 740, val loss: 1.5511736869812012\n",
      "Epoch 750, training loss: 641.8280029296875 = 1.518742322921753 + 100 * 6.403092384338379\n",
      "Epoch 750, val loss: 1.5442408323287964\n",
      "Epoch 760, training loss: 640.9678344726562 = 1.5101693868637085 + 100 * 6.394576549530029\n",
      "Epoch 760, val loss: 1.5372345447540283\n",
      "Epoch 770, training loss: 640.9375610351562 = 1.5024420022964478 + 100 * 6.394351482391357\n",
      "Epoch 770, val loss: 1.530835747718811\n",
      "Epoch 780, training loss: 641.30859375 = 1.494168758392334 + 100 * 6.398144721984863\n",
      "Epoch 780, val loss: 1.524301290512085\n",
      "Epoch 790, training loss: 641.0559692382812 = 1.4850329160690308 + 100 * 6.39570951461792\n",
      "Epoch 790, val loss: 1.5171856880187988\n",
      "Epoch 800, training loss: 640.5888671875 = 1.4760655164718628 + 100 * 6.391128063201904\n",
      "Epoch 800, val loss: 1.509602427482605\n",
      "Epoch 810, training loss: 640.2233276367188 = 1.4670041799545288 + 100 * 6.387563705444336\n",
      "Epoch 810, val loss: 1.5024231672286987\n",
      "Epoch 820, training loss: 639.7937622070312 = 1.4584852457046509 + 100 * 6.383352756500244\n",
      "Epoch 820, val loss: 1.496089220046997\n",
      "Epoch 830, training loss: 640.8800659179688 = 1.4499396085739136 + 100 * 6.394300937652588\n",
      "Epoch 830, val loss: 1.4895241260528564\n",
      "Epoch 840, training loss: 640.3903198242188 = 1.4392061233520508 + 100 * 6.3895111083984375\n",
      "Epoch 840, val loss: 1.4797691106796265\n",
      "Epoch 850, training loss: 639.5039672851562 = 1.4294414520263672 + 100 * 6.3807454109191895\n",
      "Epoch 850, val loss: 1.472690463066101\n",
      "Epoch 860, training loss: 639.4278564453125 = 1.4206953048706055 + 100 * 6.380071640014648\n",
      "Epoch 860, val loss: 1.4657835960388184\n",
      "Epoch 870, training loss: 639.675537109375 = 1.411306619644165 + 100 * 6.3826422691345215\n",
      "Epoch 870, val loss: 1.458389401435852\n",
      "Epoch 880, training loss: 639.3240356445312 = 1.4016928672790527 + 100 * 6.379223346710205\n",
      "Epoch 880, val loss: 1.451111912727356\n",
      "Epoch 890, training loss: 638.979736328125 = 1.392651915550232 + 100 * 6.375871181488037\n",
      "Epoch 890, val loss: 1.4441289901733398\n",
      "Epoch 900, training loss: 639.4141845703125 = 1.3828376531600952 + 100 * 6.380313873291016\n",
      "Epoch 900, val loss: 1.4362300634384155\n",
      "Epoch 910, training loss: 638.9024047851562 = 1.3722410202026367 + 100 * 6.375301361083984\n",
      "Epoch 910, val loss: 1.4285601377487183\n",
      "Epoch 920, training loss: 638.4398803710938 = 1.362641453742981 + 100 * 6.370771884918213\n",
      "Epoch 920, val loss: 1.4211348295211792\n",
      "Epoch 930, training loss: 638.7642211914062 = 1.353171706199646 + 100 * 6.374110698699951\n",
      "Epoch 930, val loss: 1.414307951927185\n",
      "Epoch 940, training loss: 638.4235229492188 = 1.3428189754486084 + 100 * 6.370806694030762\n",
      "Epoch 940, val loss: 1.405777931213379\n",
      "Epoch 950, training loss: 638.3622436523438 = 1.3325155973434448 + 100 * 6.370297431945801\n",
      "Epoch 950, val loss: 1.3979285955429077\n",
      "Epoch 960, training loss: 637.74951171875 = 1.3223822116851807 + 100 * 6.36427116394043\n",
      "Epoch 960, val loss: 1.3901946544647217\n",
      "Epoch 970, training loss: 637.9754638671875 = 1.3129959106445312 + 100 * 6.36662483215332\n",
      "Epoch 970, val loss: 1.3834617137908936\n",
      "Epoch 980, training loss: 638.04248046875 = 1.3025768995285034 + 100 * 6.367399215698242\n",
      "Epoch 980, val loss: 1.3752633333206177\n",
      "Epoch 990, training loss: 638.0045776367188 = 1.2926099300384521 + 100 * 6.367119789123535\n",
      "Epoch 990, val loss: 1.3678456544876099\n",
      "Epoch 1000, training loss: 638.4003295898438 = 1.281652808189392 + 100 * 6.37118673324585\n",
      "Epoch 1000, val loss: 1.358939528465271\n",
      "Epoch 1010, training loss: 637.4736938476562 = 1.2707828283309937 + 100 * 6.362029075622559\n",
      "Epoch 1010, val loss: 1.3517935276031494\n",
      "Epoch 1020, training loss: 637.163818359375 = 1.2615360021591187 + 100 * 6.359023094177246\n",
      "Epoch 1020, val loss: 1.344626545906067\n",
      "Epoch 1030, training loss: 637.4207153320312 = 1.2521528005599976 + 100 * 6.361685752868652\n",
      "Epoch 1030, val loss: 1.3381224870681763\n",
      "Epoch 1040, training loss: 637.37158203125 = 1.2412440776824951 + 100 * 6.361302852630615\n",
      "Epoch 1040, val loss: 1.3294340372085571\n",
      "Epoch 1050, training loss: 636.9892578125 = 1.2305611371994019 + 100 * 6.35758638381958\n",
      "Epoch 1050, val loss: 1.3216270208358765\n",
      "Epoch 1060, training loss: 636.8839721679688 = 1.221227765083313 + 100 * 6.356627464294434\n",
      "Epoch 1060, val loss: 1.3149305582046509\n",
      "Epoch 1070, training loss: 637.3250122070312 = 1.2114832401275635 + 100 * 6.361135482788086\n",
      "Epoch 1070, val loss: 1.3074116706848145\n",
      "Epoch 1080, training loss: 637.0864868164062 = 1.2011834383010864 + 100 * 6.358852863311768\n",
      "Epoch 1080, val loss: 1.3005554676055908\n",
      "Epoch 1090, training loss: 636.285400390625 = 1.190468430519104 + 100 * 6.350949287414551\n",
      "Epoch 1090, val loss: 1.292202353477478\n",
      "Epoch 1100, training loss: 638.0394897460938 = 1.1804425716400146 + 100 * 6.368590831756592\n",
      "Epoch 1100, val loss: 1.2847692966461182\n",
      "Epoch 1110, training loss: 636.4888916015625 = 1.1695551872253418 + 100 * 6.353193283081055\n",
      "Epoch 1110, val loss: 1.2774847745895386\n",
      "Epoch 1120, training loss: 636.049560546875 = 1.1600462198257446 + 100 * 6.348895072937012\n",
      "Epoch 1120, val loss: 1.270708680152893\n",
      "Epoch 1130, training loss: 635.8390502929688 = 1.1510757207870483 + 100 * 6.346879959106445\n",
      "Epoch 1130, val loss: 1.2646323442459106\n",
      "Epoch 1140, training loss: 637.5235595703125 = 1.1411380767822266 + 100 * 6.363824367523193\n",
      "Epoch 1140, val loss: 1.2577284574508667\n",
      "Epoch 1150, training loss: 636.4152221679688 = 1.1303125619888306 + 100 * 6.352849006652832\n",
      "Epoch 1150, val loss: 1.249251365661621\n",
      "Epoch 1160, training loss: 635.5748291015625 = 1.1203504800796509 + 100 * 6.344544887542725\n",
      "Epoch 1160, val loss: 1.2427265644073486\n",
      "Epoch 1170, training loss: 635.4354248046875 = 1.1114319562911987 + 100 * 6.343239784240723\n",
      "Epoch 1170, val loss: 1.2364312410354614\n",
      "Epoch 1180, training loss: 636.2144775390625 = 1.1022529602050781 + 100 * 6.3511223793029785\n",
      "Epoch 1180, val loss: 1.2308117151260376\n",
      "Epoch 1190, training loss: 636.073974609375 = 1.0913957357406616 + 100 * 6.349826335906982\n",
      "Epoch 1190, val loss: 1.221340298652649\n",
      "Epoch 1200, training loss: 635.404052734375 = 1.0811268091201782 + 100 * 6.343229293823242\n",
      "Epoch 1200, val loss: 1.2152254581451416\n",
      "Epoch 1210, training loss: 635.1796264648438 = 1.0724077224731445 + 100 * 6.3410725593566895\n",
      "Epoch 1210, val loss: 1.208810806274414\n",
      "Epoch 1220, training loss: 636.3651733398438 = 1.063231110572815 + 100 * 6.3530192375183105\n",
      "Epoch 1220, val loss: 1.202439308166504\n",
      "Epoch 1230, training loss: 635.6766967773438 = 1.0517491102218628 + 100 * 6.346249580383301\n",
      "Epoch 1230, val loss: 1.1944857835769653\n",
      "Epoch 1240, training loss: 635.1166381835938 = 1.0421940088272095 + 100 * 6.340744495391846\n",
      "Epoch 1240, val loss: 1.1877098083496094\n",
      "Epoch 1250, training loss: 634.7220458984375 = 1.0338146686553955 + 100 * 6.3368821144104\n",
      "Epoch 1250, val loss: 1.1826587915420532\n",
      "Epoch 1260, training loss: 635.864501953125 = 1.0250800848007202 + 100 * 6.34839391708374\n",
      "Epoch 1260, val loss: 1.176504135131836\n",
      "Epoch 1270, training loss: 634.6419677734375 = 1.0150120258331299 + 100 * 6.336269378662109\n",
      "Epoch 1270, val loss: 1.1695547103881836\n",
      "Epoch 1280, training loss: 634.406982421875 = 1.0060852766036987 + 100 * 6.334008693695068\n",
      "Epoch 1280, val loss: 1.1638168096542358\n",
      "Epoch 1290, training loss: 634.939208984375 = 0.9974660873413086 + 100 * 6.339417457580566\n",
      "Epoch 1290, val loss: 1.1578582525253296\n",
      "Epoch 1300, training loss: 634.4035034179688 = 0.9881428480148315 + 100 * 6.334153175354004\n",
      "Epoch 1300, val loss: 1.1517351865768433\n",
      "Epoch 1310, training loss: 634.7592163085938 = 0.9795796275138855 + 100 * 6.337796688079834\n",
      "Epoch 1310, val loss: 1.146105170249939\n",
      "Epoch 1320, training loss: 634.7372436523438 = 0.9699992537498474 + 100 * 6.337672710418701\n",
      "Epoch 1320, val loss: 1.139378547668457\n",
      "Epoch 1330, training loss: 634.195556640625 = 0.9609715938568115 + 100 * 6.332345485687256\n",
      "Epoch 1330, val loss: 1.1338790655136108\n",
      "Epoch 1340, training loss: 634.2576904296875 = 0.9526527523994446 + 100 * 6.333050727844238\n",
      "Epoch 1340, val loss: 1.1284104585647583\n",
      "Epoch 1350, training loss: 634.125732421875 = 0.9438228607177734 + 100 * 6.3318190574646\n",
      "Epoch 1350, val loss: 1.122923493385315\n",
      "Epoch 1360, training loss: 635.3363647460938 = 0.934739887714386 + 100 * 6.344016075134277\n",
      "Epoch 1360, val loss: 1.117318868637085\n",
      "Epoch 1370, training loss: 634.001220703125 = 0.9259461164474487 + 100 * 6.330752849578857\n",
      "Epoch 1370, val loss: 1.1110970973968506\n",
      "Epoch 1380, training loss: 634.181884765625 = 0.9176005721092224 + 100 * 6.332642555236816\n",
      "Epoch 1380, val loss: 1.1067191362380981\n",
      "Epoch 1390, training loss: 633.6505126953125 = 0.9087754487991333 + 100 * 6.327417850494385\n",
      "Epoch 1390, val loss: 1.1002159118652344\n",
      "Epoch 1400, training loss: 633.5716552734375 = 0.9009448289871216 + 100 * 6.326706886291504\n",
      "Epoch 1400, val loss: 1.0954391956329346\n",
      "Epoch 1410, training loss: 633.6632080078125 = 0.8931710124015808 + 100 * 6.327700138092041\n",
      "Epoch 1410, val loss: 1.0909268856048584\n",
      "Epoch 1420, training loss: 633.8451538085938 = 0.8852676749229431 + 100 * 6.329598903656006\n",
      "Epoch 1420, val loss: 1.085944652557373\n",
      "Epoch 1430, training loss: 634.9136962890625 = 0.8767374753952026 + 100 * 6.340370178222656\n",
      "Epoch 1430, val loss: 1.0801050662994385\n",
      "Epoch 1440, training loss: 633.59716796875 = 0.8676205277442932 + 100 * 6.327295303344727\n",
      "Epoch 1440, val loss: 1.0747333765029907\n",
      "Epoch 1450, training loss: 633.2416381835938 = 0.8599953651428223 + 100 * 6.323816299438477\n",
      "Epoch 1450, val loss: 1.069898247718811\n",
      "Epoch 1460, training loss: 632.9583740234375 = 0.8528671860694885 + 100 * 6.3210554122924805\n",
      "Epoch 1460, val loss: 1.0661860704421997\n",
      "Epoch 1470, training loss: 634.2906494140625 = 0.8456449508666992 + 100 * 6.3344502449035645\n",
      "Epoch 1470, val loss: 1.061797857284546\n",
      "Epoch 1480, training loss: 633.9534912109375 = 0.8366028666496277 + 100 * 6.3311686515808105\n",
      "Epoch 1480, val loss: 1.0548206567764282\n",
      "Epoch 1490, training loss: 633.376220703125 = 0.8280143141746521 + 100 * 6.325482368469238\n",
      "Epoch 1490, val loss: 1.050152063369751\n",
      "Epoch 1500, training loss: 632.9581909179688 = 0.820487380027771 + 100 * 6.321376800537109\n",
      "Epoch 1500, val loss: 1.046128511428833\n",
      "Epoch 1510, training loss: 632.9110107421875 = 0.8135855793952942 + 100 * 6.320973873138428\n",
      "Epoch 1510, val loss: 1.0417664051055908\n",
      "Epoch 1520, training loss: 633.6502685546875 = 0.8062307834625244 + 100 * 6.3284406661987305\n",
      "Epoch 1520, val loss: 1.0373340845108032\n",
      "Epoch 1530, training loss: 633.2211303710938 = 0.7983579635620117 + 100 * 6.324227809906006\n",
      "Epoch 1530, val loss: 1.0333030223846436\n",
      "Epoch 1540, training loss: 632.7882690429688 = 0.7911266684532166 + 100 * 6.319971084594727\n",
      "Epoch 1540, val loss: 1.028679609298706\n",
      "Epoch 1550, training loss: 633.0484008789062 = 0.7839381694793701 + 100 * 6.3226447105407715\n",
      "Epoch 1550, val loss: 1.0243116617202759\n",
      "Epoch 1560, training loss: 632.8770751953125 = 0.7761620879173279 + 100 * 6.321009159088135\n",
      "Epoch 1560, val loss: 1.0199018716812134\n",
      "Epoch 1570, training loss: 632.7507934570312 = 0.7690258026123047 + 100 * 6.319817543029785\n",
      "Epoch 1570, val loss: 1.0161386728286743\n",
      "Epoch 1580, training loss: 632.5750732421875 = 0.7622119784355164 + 100 * 6.31812858581543\n",
      "Epoch 1580, val loss: 1.0123682022094727\n",
      "Epoch 1590, training loss: 632.887451171875 = 0.755344808101654 + 100 * 6.321320533752441\n",
      "Epoch 1590, val loss: 1.0082933902740479\n",
      "Epoch 1600, training loss: 633.0460205078125 = 0.7482085824012756 + 100 * 6.3229780197143555\n",
      "Epoch 1600, val loss: 1.0038474798202515\n",
      "Epoch 1610, training loss: 632.3638916015625 = 0.7408721446990967 + 100 * 6.316230297088623\n",
      "Epoch 1610, val loss: 1.000333547592163\n",
      "Epoch 1620, training loss: 632.7322387695312 = 0.7342622876167297 + 100 * 6.319979667663574\n",
      "Epoch 1620, val loss: 0.9972390532493591\n",
      "Epoch 1630, training loss: 632.4530639648438 = 0.7271229028701782 + 100 * 6.317259311676025\n",
      "Epoch 1630, val loss: 0.9924237132072449\n",
      "Epoch 1640, training loss: 632.2715454101562 = 0.7206854224205017 + 100 * 6.3155083656311035\n",
      "Epoch 1640, val loss: 0.9888668656349182\n",
      "Epoch 1650, training loss: 632.51611328125 = 0.7142120599746704 + 100 * 6.318018913269043\n",
      "Epoch 1650, val loss: 0.98572838306427\n",
      "Epoch 1660, training loss: 632.2410888671875 = 0.7069718241691589 + 100 * 6.315341472625732\n",
      "Epoch 1660, val loss: 0.9812126159667969\n",
      "Epoch 1670, training loss: 631.9396362304688 = 0.7002519965171814 + 100 * 6.312393665313721\n",
      "Epoch 1670, val loss: 0.977933406829834\n",
      "Epoch 1680, training loss: 632.2374877929688 = 0.6939452886581421 + 100 * 6.315435409545898\n",
      "Epoch 1680, val loss: 0.97512286901474\n",
      "Epoch 1690, training loss: 632.3025512695312 = 0.6875255703926086 + 100 * 6.316150665283203\n",
      "Epoch 1690, val loss: 0.9713435173034668\n",
      "Epoch 1700, training loss: 631.9732055664062 = 0.6812044382095337 + 100 * 6.312919616699219\n",
      "Epoch 1700, val loss: 0.9677594304084778\n",
      "Epoch 1710, training loss: 632.6763916015625 = 0.6749553084373474 + 100 * 6.320014476776123\n",
      "Epoch 1710, val loss: 0.9644700884819031\n",
      "Epoch 1720, training loss: 632.0737915039062 = 0.6681656837463379 + 100 * 6.314056396484375\n",
      "Epoch 1720, val loss: 0.960734486579895\n",
      "Epoch 1730, training loss: 632.517822265625 = 0.6621885895729065 + 100 * 6.318556785583496\n",
      "Epoch 1730, val loss: 0.9573929309844971\n",
      "Epoch 1740, training loss: 631.60595703125 = 0.6553872227668762 + 100 * 6.309505462646484\n",
      "Epoch 1740, val loss: 0.9540213346481323\n",
      "Epoch 1750, training loss: 631.7470703125 = 0.6494016647338867 + 100 * 6.310976505279541\n",
      "Epoch 1750, val loss: 0.9516612887382507\n",
      "Epoch 1760, training loss: 631.9646606445312 = 0.6435959935188293 + 100 * 6.313210487365723\n",
      "Epoch 1760, val loss: 0.9483168125152588\n",
      "Epoch 1770, training loss: 631.3886108398438 = 0.6374192237854004 + 100 * 6.307512283325195\n",
      "Epoch 1770, val loss: 0.9450915455818176\n",
      "Epoch 1780, training loss: 632.2565307617188 = 0.6316241025924683 + 100 * 6.316248893737793\n",
      "Epoch 1780, val loss: 0.9416772723197937\n",
      "Epoch 1790, training loss: 631.6459350585938 = 0.6253403425216675 + 100 * 6.310205459594727\n",
      "Epoch 1790, val loss: 0.938599705696106\n",
      "Epoch 1800, training loss: 631.6779174804688 = 0.6195297241210938 + 100 * 6.31058406829834\n",
      "Epoch 1800, val loss: 0.9358876943588257\n",
      "Epoch 1810, training loss: 631.2955322265625 = 0.6134378910064697 + 100 * 6.306820869445801\n",
      "Epoch 1810, val loss: 0.9332125782966614\n",
      "Epoch 1820, training loss: 631.8544311523438 = 0.6074687242507935 + 100 * 6.312469482421875\n",
      "Epoch 1820, val loss: 0.9304636120796204\n",
      "Epoch 1830, training loss: 631.37548828125 = 0.6016228199005127 + 100 * 6.307738780975342\n",
      "Epoch 1830, val loss: 0.9266014099121094\n",
      "Epoch 1840, training loss: 631.2969970703125 = 0.5960684418678284 + 100 * 6.307009220123291\n",
      "Epoch 1840, val loss: 0.9245584011077881\n",
      "Epoch 1850, training loss: 631.1522827148438 = 0.5906596779823303 + 100 * 6.30561637878418\n",
      "Epoch 1850, val loss: 0.9220444560050964\n",
      "Epoch 1860, training loss: 631.6188354492188 = 0.5851418375968933 + 100 * 6.310337066650391\n",
      "Epoch 1860, val loss: 0.91977858543396\n",
      "Epoch 1870, training loss: 631.3140258789062 = 0.5794406533241272 + 100 * 6.307345867156982\n",
      "Epoch 1870, val loss: 0.9164445400238037\n",
      "Epoch 1880, training loss: 631.7595825195312 = 0.5741364359855652 + 100 * 6.311854362487793\n",
      "Epoch 1880, val loss: 0.913745641708374\n",
      "Epoch 1890, training loss: 631.1227416992188 = 0.5678899884223938 + 100 * 6.305548667907715\n",
      "Epoch 1890, val loss: 0.9109647274017334\n",
      "Epoch 1900, training loss: 631.1815795898438 = 0.5623561143875122 + 100 * 6.306191921234131\n",
      "Epoch 1900, val loss: 0.9079921245574951\n",
      "Epoch 1910, training loss: 631.0127563476562 = 0.557199239730835 + 100 * 6.304555416107178\n",
      "Epoch 1910, val loss: 0.9059938788414001\n",
      "Epoch 1920, training loss: 630.7604370117188 = 0.552016019821167 + 100 * 6.302084445953369\n",
      "Epoch 1920, val loss: 0.9036471247673035\n",
      "Epoch 1930, training loss: 631.1322631835938 = 0.5470166206359863 + 100 * 6.305852890014648\n",
      "Epoch 1930, val loss: 0.9016411304473877\n",
      "Epoch 1940, training loss: 630.7132568359375 = 0.5417537093162537 + 100 * 6.301714897155762\n",
      "Epoch 1940, val loss: 0.8988600969314575\n",
      "Epoch 1950, training loss: 631.4058227539062 = 0.5367926359176636 + 100 * 6.308690071105957\n",
      "Epoch 1950, val loss: 0.896488606929779\n",
      "Epoch 1960, training loss: 630.8358154296875 = 0.5309804677963257 + 100 * 6.303048133850098\n",
      "Epoch 1960, val loss: 0.8935141563415527\n",
      "Epoch 1970, training loss: 630.8330078125 = 0.5253410339355469 + 100 * 6.30307674407959\n",
      "Epoch 1970, val loss: 0.8912334442138672\n",
      "Epoch 1980, training loss: 630.4735107421875 = 0.5205510258674622 + 100 * 6.299529552459717\n",
      "Epoch 1980, val loss: 0.8889579772949219\n",
      "Epoch 1990, training loss: 630.491943359375 = 0.5157731771469116 + 100 * 6.299761772155762\n",
      "Epoch 1990, val loss: 0.8871453404426575\n",
      "Epoch 2000, training loss: 631.1175537109375 = 0.5110005140304565 + 100 * 6.306065559387207\n",
      "Epoch 2000, val loss: 0.8850411772727966\n",
      "Epoch 2010, training loss: 630.4426879882812 = 0.5058013200759888 + 100 * 6.299368858337402\n",
      "Epoch 2010, val loss: 0.8821251392364502\n",
      "Epoch 2020, training loss: 630.49560546875 = 0.5010643601417542 + 100 * 6.299945831298828\n",
      "Epoch 2020, val loss: 0.8800809383392334\n",
      "Epoch 2030, training loss: 631.0807495117188 = 0.496325820684433 + 100 * 6.305844306945801\n",
      "Epoch 2030, val loss: 0.8776764273643494\n",
      "Epoch 2040, training loss: 630.7442626953125 = 0.49099162220954895 + 100 * 6.302533149719238\n",
      "Epoch 2040, val loss: 0.8752089142799377\n",
      "Epoch 2050, training loss: 630.2711791992188 = 0.48596620559692383 + 100 * 6.297852039337158\n",
      "Epoch 2050, val loss: 0.8736202716827393\n",
      "Epoch 2060, training loss: 630.2171020507812 = 0.48157230019569397 + 100 * 6.2973551750183105\n",
      "Epoch 2060, val loss: 0.8718145489692688\n",
      "Epoch 2070, training loss: 630.88037109375 = 0.4770406186580658 + 100 * 6.304033279418945\n",
      "Epoch 2070, val loss: 0.8700267672538757\n",
      "Epoch 2080, training loss: 630.1038818359375 = 0.47252267599105835 + 100 * 6.296313762664795\n",
      "Epoch 2080, val loss: 0.8680158257484436\n",
      "Epoch 2090, training loss: 630.668701171875 = 0.4681625962257385 + 100 * 6.302005767822266\n",
      "Epoch 2090, val loss: 0.8660684823989868\n",
      "Epoch 2100, training loss: 630.6227416992188 = 0.4632721245288849 + 100 * 6.3015947341918945\n",
      "Epoch 2100, val loss: 0.8633518218994141\n",
      "Epoch 2110, training loss: 629.9825439453125 = 0.4583953320980072 + 100 * 6.295241832733154\n",
      "Epoch 2110, val loss: 0.8618569374084473\n",
      "Epoch 2120, training loss: 630.3655395507812 = 0.45402392745018005 + 100 * 6.299115180969238\n",
      "Epoch 2120, val loss: 0.8604773879051208\n",
      "Epoch 2130, training loss: 630.0963134765625 = 0.44962039589881897 + 100 * 6.296466827392578\n",
      "Epoch 2130, val loss: 0.8583462238311768\n",
      "Epoch 2140, training loss: 629.9453125 = 0.4455130696296692 + 100 * 6.2949981689453125\n",
      "Epoch 2140, val loss: 0.8565219044685364\n",
      "Epoch 2150, training loss: 629.90576171875 = 0.4414063096046448 + 100 * 6.294643402099609\n",
      "Epoch 2150, val loss: 0.8549843430519104\n",
      "Epoch 2160, training loss: 630.0814819335938 = 0.4372425973415375 + 100 * 6.296442031860352\n",
      "Epoch 2160, val loss: 0.8531501889228821\n",
      "Epoch 2170, training loss: 630.8597412109375 = 0.4329407513141632 + 100 * 6.3042683601379395\n",
      "Epoch 2170, val loss: 0.8514895439147949\n",
      "Epoch 2180, training loss: 629.9848022460938 = 0.42782631516456604 + 100 * 6.29556941986084\n",
      "Epoch 2180, val loss: 0.8494150042533875\n",
      "Epoch 2190, training loss: 629.7034301757812 = 0.42356032133102417 + 100 * 6.2927985191345215\n",
      "Epoch 2190, val loss: 0.8475971221923828\n",
      "Epoch 2200, training loss: 629.8473510742188 = 0.4197787344455719 + 100 * 6.294275283813477\n",
      "Epoch 2200, val loss: 0.8467925190925598\n",
      "Epoch 2210, training loss: 629.980712890625 = 0.415581077337265 + 100 * 6.295650959014893\n",
      "Epoch 2210, val loss: 0.8447635173797607\n",
      "Epoch 2220, training loss: 629.5175170898438 = 0.4115607738494873 + 100 * 6.291059494018555\n",
      "Epoch 2220, val loss: 0.8433231711387634\n",
      "Epoch 2230, training loss: 629.733642578125 = 0.4077780842781067 + 100 * 6.2932586669921875\n",
      "Epoch 2230, val loss: 0.8419702649116516\n",
      "Epoch 2240, training loss: 630.1834716796875 = 0.40382498502731323 + 100 * 6.297796726226807\n",
      "Epoch 2240, val loss: 0.8411847949028015\n",
      "Epoch 2250, training loss: 629.3479614257812 = 0.39972129464149475 + 100 * 6.289482116699219\n",
      "Epoch 2250, val loss: 0.8387467265129089\n",
      "Epoch 2260, training loss: 629.470458984375 = 0.3960322439670563 + 100 * 6.290744304656982\n",
      "Epoch 2260, val loss: 0.8372759819030762\n",
      "Epoch 2270, training loss: 631.1056518554688 = 0.3921806216239929 + 100 * 6.307135105133057\n",
      "Epoch 2270, val loss: 0.836117684841156\n",
      "Epoch 2280, training loss: 629.7293701171875 = 0.3876997232437134 + 100 * 6.293416500091553\n",
      "Epoch 2280, val loss: 0.8339923620223999\n",
      "Epoch 2290, training loss: 629.4193115234375 = 0.38395485281944275 + 100 * 6.290353298187256\n",
      "Epoch 2290, val loss: 0.8328204154968262\n",
      "Epoch 2300, training loss: 629.4904174804688 = 0.3803548216819763 + 100 * 6.29110050201416\n",
      "Epoch 2300, val loss: 0.8319011330604553\n",
      "Epoch 2310, training loss: 629.5322875976562 = 0.37665584683418274 + 100 * 6.291556358337402\n",
      "Epoch 2310, val loss: 0.8302664756774902\n",
      "Epoch 2320, training loss: 629.2423706054688 = 0.3731311857700348 + 100 * 6.288692474365234\n",
      "Epoch 2320, val loss: 0.8289942741394043\n",
      "Epoch 2330, training loss: 629.9398193359375 = 0.3695320785045624 + 100 * 6.295702934265137\n",
      "Epoch 2330, val loss: 0.8276692628860474\n",
      "Epoch 2340, training loss: 629.4304809570312 = 0.36574167013168335 + 100 * 6.290647506713867\n",
      "Epoch 2340, val loss: 0.8267450332641602\n",
      "Epoch 2350, training loss: 629.3176879882812 = 0.3623308837413788 + 100 * 6.289554119110107\n",
      "Epoch 2350, val loss: 0.8248928189277649\n",
      "Epoch 2360, training loss: 629.2073974609375 = 0.3587301969528198 + 100 * 6.288486957550049\n",
      "Epoch 2360, val loss: 0.8240654468536377\n",
      "Epoch 2370, training loss: 629.0677490234375 = 0.3553845286369324 + 100 * 6.287123680114746\n",
      "Epoch 2370, val loss: 0.8235146999359131\n",
      "Epoch 2380, training loss: 630.571533203125 = 0.3519628942012787 + 100 * 6.3021955490112305\n",
      "Epoch 2380, val loss: 0.8222631812095642\n",
      "Epoch 2390, training loss: 629.6493530273438 = 0.3480057716369629 + 100 * 6.293013572692871\n",
      "Epoch 2390, val loss: 0.8201475143432617\n",
      "Epoch 2400, training loss: 629.3671264648438 = 0.34458208084106445 + 100 * 6.290225028991699\n",
      "Epoch 2400, val loss: 0.8195523619651794\n",
      "Epoch 2410, training loss: 628.960693359375 = 0.34119006991386414 + 100 * 6.286195278167725\n",
      "Epoch 2410, val loss: 0.8183357119560242\n",
      "Epoch 2420, training loss: 628.9432373046875 = 0.3380991220474243 + 100 * 6.2860517501831055\n",
      "Epoch 2420, val loss: 0.8174474239349365\n",
      "Epoch 2430, training loss: 629.1821899414062 = 0.33494070172309875 + 100 * 6.2884721755981445\n",
      "Epoch 2430, val loss: 0.8166239261627197\n",
      "Epoch 2440, training loss: 629.2235717773438 = 0.33162397146224976 + 100 * 6.288919448852539\n",
      "Epoch 2440, val loss: 0.815350353717804\n",
      "Epoch 2450, training loss: 629.564453125 = 0.328194797039032 + 100 * 6.292362689971924\n",
      "Epoch 2450, val loss: 0.8144018054008484\n",
      "Epoch 2460, training loss: 628.811767578125 = 0.3250367343425751 + 100 * 6.284867763519287\n",
      "Epoch 2460, val loss: 0.81341153383255\n",
      "Epoch 2470, training loss: 629.4736938476562 = 0.3219800889492035 + 100 * 6.29151725769043\n",
      "Epoch 2470, val loss: 0.8119475245475769\n",
      "Epoch 2480, training loss: 628.5302734375 = 0.31844910979270935 + 100 * 6.282118320465088\n",
      "Epoch 2480, val loss: 0.8112718462944031\n",
      "Epoch 2490, training loss: 628.5947265625 = 0.315463125705719 + 100 * 6.282792091369629\n",
      "Epoch 2490, val loss: 0.8108675479888916\n",
      "Epoch 2500, training loss: 628.7034301757812 = 0.31264713406562805 + 100 * 6.283908367156982\n",
      "Epoch 2500, val loss: 0.8101083636283875\n",
      "Epoch 2510, training loss: 629.1685791015625 = 0.3097998797893524 + 100 * 6.28858757019043\n",
      "Epoch 2510, val loss: 0.8098021745681763\n",
      "Epoch 2520, training loss: 629.3779296875 = 0.3064323961734772 + 100 * 6.290714740753174\n",
      "Epoch 2520, val loss: 0.8077049255371094\n",
      "Epoch 2530, training loss: 628.57470703125 = 0.3032281696796417 + 100 * 6.28271484375\n",
      "Epoch 2530, val loss: 0.8069374561309814\n",
      "Epoch 2540, training loss: 628.4058227539062 = 0.30038243532180786 + 100 * 6.281054973602295\n",
      "Epoch 2540, val loss: 0.8068734407424927\n",
      "Epoch 2550, training loss: 629.31640625 = 0.2977672815322876 + 100 * 6.290185928344727\n",
      "Epoch 2550, val loss: 0.8060733079910278\n",
      "Epoch 2560, training loss: 628.672119140625 = 0.29435858130455017 + 100 * 6.283777236938477\n",
      "Epoch 2560, val loss: 0.8044297099113464\n",
      "Epoch 2570, training loss: 628.4068603515625 = 0.2913823425769806 + 100 * 6.281154632568359\n",
      "Epoch 2570, val loss: 0.8043835163116455\n",
      "Epoch 2580, training loss: 628.3392333984375 = 0.28879812359809875 + 100 * 6.28050422668457\n",
      "Epoch 2580, val loss: 0.8039436340332031\n",
      "Epoch 2590, training loss: 628.5181274414062 = 0.2862890660762787 + 100 * 6.282318115234375\n",
      "Epoch 2590, val loss: 0.8034167289733887\n",
      "Epoch 2600, training loss: 629.1683959960938 = 0.2835768759250641 + 100 * 6.288848400115967\n",
      "Epoch 2600, val loss: 0.8028275370597839\n",
      "Epoch 2610, training loss: 628.6825561523438 = 0.28056997060775757 + 100 * 6.284019470214844\n",
      "Epoch 2610, val loss: 0.8017874360084534\n",
      "Epoch 2620, training loss: 628.7583618164062 = 0.2779361605644226 + 100 * 6.284804344177246\n",
      "Epoch 2620, val loss: 0.800415575504303\n",
      "Epoch 2630, training loss: 628.8250122070312 = 0.2750561535358429 + 100 * 6.285499572753906\n",
      "Epoch 2630, val loss: 0.8002488017082214\n",
      "Epoch 2640, training loss: 629.2371215820312 = 0.27216625213623047 + 100 * 6.289649486541748\n",
      "Epoch 2640, val loss: 0.7996631860733032\n",
      "Epoch 2650, training loss: 628.3421630859375 = 0.2693498730659485 + 100 * 6.280727863311768\n",
      "Epoch 2650, val loss: 0.7989712953567505\n",
      "Epoch 2660, training loss: 628.2388916015625 = 0.26705169677734375 + 100 * 6.27971887588501\n",
      "Epoch 2660, val loss: 0.7990247011184692\n",
      "Epoch 2670, training loss: 628.2078857421875 = 0.26467791199684143 + 100 * 6.27943229675293\n",
      "Epoch 2670, val loss: 0.7987468838691711\n",
      "Epoch 2680, training loss: 629.0836791992188 = 0.26232093572616577 + 100 * 6.28821325302124\n",
      "Epoch 2680, val loss: 0.797724187374115\n",
      "Epoch 2690, training loss: 628.3756103515625 = 0.25944504141807556 + 100 * 6.281161785125732\n",
      "Epoch 2690, val loss: 0.7972788214683533\n",
      "Epoch 2700, training loss: 628.2168579101562 = 0.2569063603878021 + 100 * 6.279599666595459\n",
      "Epoch 2700, val loss: 0.7968143820762634\n",
      "Epoch 2710, training loss: 628.65185546875 = 0.25450611114501953 + 100 * 6.283973217010498\n",
      "Epoch 2710, val loss: 0.796343982219696\n",
      "Epoch 2720, training loss: 628.42236328125 = 0.2520170509815216 + 100 * 6.281703472137451\n",
      "Epoch 2720, val loss: 0.7959628701210022\n",
      "Epoch 2730, training loss: 628.666259765625 = 0.2496018260717392 + 100 * 6.2841668128967285\n",
      "Epoch 2730, val loss: 0.7953951358795166\n",
      "Epoch 2740, training loss: 628.0635375976562 = 0.24708588421344757 + 100 * 6.278164386749268\n",
      "Epoch 2740, val loss: 0.7954633831977844\n",
      "Epoch 2750, training loss: 628.2185668945312 = 0.24477921426296234 + 100 * 6.279737949371338\n",
      "Epoch 2750, val loss: 0.7954584956169128\n",
      "Epoch 2760, training loss: 628.7557373046875 = 0.24259139597415924 + 100 * 6.285130977630615\n",
      "Epoch 2760, val loss: 0.7950674891471863\n",
      "Epoch 2770, training loss: 628.2276000976562 = 0.24003471434116364 + 100 * 6.2798752784729\n",
      "Epoch 2770, val loss: 0.7941225171089172\n",
      "Epoch 2780, training loss: 628.14111328125 = 0.23757889866828918 + 100 * 6.279035568237305\n",
      "Epoch 2780, val loss: 0.7935839891433716\n",
      "Epoch 2790, training loss: 628.1605834960938 = 0.23537415266036987 + 100 * 6.279252052307129\n",
      "Epoch 2790, val loss: 0.7936691641807556\n",
      "Epoch 2800, training loss: 628.4247436523438 = 0.23316940665245056 + 100 * 6.281915664672852\n",
      "Epoch 2800, val loss: 0.7929545044898987\n",
      "Epoch 2810, training loss: 627.8649291992188 = 0.23094676434993744 + 100 * 6.276339530944824\n",
      "Epoch 2810, val loss: 0.7934723496437073\n",
      "Epoch 2820, training loss: 628.448486328125 = 0.2288963943719864 + 100 * 6.282196044921875\n",
      "Epoch 2820, val loss: 0.7934932708740234\n",
      "Epoch 2830, training loss: 628.0523681640625 = 0.22656047344207764 + 100 * 6.278258323669434\n",
      "Epoch 2830, val loss: 0.7924187779426575\n",
      "Epoch 2840, training loss: 628.1539916992188 = 0.22442519664764404 + 100 * 6.279295921325684\n",
      "Epoch 2840, val loss: 0.7920839190483093\n",
      "Epoch 2850, training loss: 628.4533081054688 = 0.22229495644569397 + 100 * 6.2823100090026855\n",
      "Epoch 2850, val loss: 0.7921478748321533\n",
      "Epoch 2860, training loss: 628.3919067382812 = 0.220024973154068 + 100 * 6.281718730926514\n",
      "Epoch 2860, val loss: 0.7913844585418701\n",
      "Epoch 2870, training loss: 627.8723754882812 = 0.21777722239494324 + 100 * 6.276546001434326\n",
      "Epoch 2870, val loss: 0.7916603088378906\n",
      "Epoch 2880, training loss: 627.7778930664062 = 0.21587830781936646 + 100 * 6.275619983673096\n",
      "Epoch 2880, val loss: 0.7917351126670837\n",
      "Epoch 2890, training loss: 628.0526733398438 = 0.21397936344146729 + 100 * 6.27838659286499\n",
      "Epoch 2890, val loss: 0.7916392683982849\n",
      "Epoch 2900, training loss: 628.5003662109375 = 0.21177798509597778 + 100 * 6.282885551452637\n",
      "Epoch 2900, val loss: 0.7912836670875549\n",
      "Epoch 2910, training loss: 627.9833984375 = 0.20963577926158905 + 100 * 6.277737617492676\n",
      "Epoch 2910, val loss: 0.7907983064651489\n",
      "Epoch 2920, training loss: 627.8115234375 = 0.2077077329158783 + 100 * 6.27603816986084\n",
      "Epoch 2920, val loss: 0.7912412881851196\n",
      "Epoch 2930, training loss: 628.5001220703125 = 0.20583947002887726 + 100 * 6.282943248748779\n",
      "Epoch 2930, val loss: 0.7915143370628357\n",
      "Epoch 2940, training loss: 627.6795654296875 = 0.2037001997232437 + 100 * 6.274758815765381\n",
      "Epoch 2940, val loss: 0.7905590534210205\n",
      "Epoch 2950, training loss: 627.6972045898438 = 0.20186874270439148 + 100 * 6.274953365325928\n",
      "Epoch 2950, val loss: 0.7904960513114929\n",
      "Epoch 2960, training loss: 627.7425537109375 = 0.20013539493083954 + 100 * 6.275424480438232\n",
      "Epoch 2960, val loss: 0.790855348110199\n",
      "Epoch 2970, training loss: 628.3721923828125 = 0.1983240246772766 + 100 * 6.281738758087158\n",
      "Epoch 2970, val loss: 0.7909235954284668\n",
      "Epoch 2980, training loss: 628.1893310546875 = 0.19629479944705963 + 100 * 6.279930114746094\n",
      "Epoch 2980, val loss: 0.7909482717514038\n",
      "Epoch 2990, training loss: 627.9978637695312 = 0.1942852884531021 + 100 * 6.278036117553711\n",
      "Epoch 2990, val loss: 0.7902349233627319\n",
      "Epoch 3000, training loss: 627.5127563476562 = 0.19240199029445648 + 100 * 6.2732038497924805\n",
      "Epoch 3000, val loss: 0.7903802394866943\n",
      "Epoch 3010, training loss: 627.7236938476562 = 0.19068215787410736 + 100 * 6.275330066680908\n",
      "Epoch 3010, val loss: 0.7905527949333191\n",
      "Epoch 3020, training loss: 627.6178588867188 = 0.18896372616291046 + 100 * 6.274288654327393\n",
      "Epoch 3020, val loss: 0.7905939221382141\n",
      "Epoch 3030, training loss: 627.9310302734375 = 0.18732918798923492 + 100 * 6.277437210083008\n",
      "Epoch 3030, val loss: 0.7907634377479553\n",
      "Epoch 3040, training loss: 627.745361328125 = 0.18550227582454681 + 100 * 6.275599002838135\n",
      "Epoch 3040, val loss: 0.7908372282981873\n",
      "Epoch 3050, training loss: 628.021728515625 = 0.18370839953422546 + 100 * 6.278379917144775\n",
      "Epoch 3050, val loss: 0.7912703156471252\n",
      "Epoch 3060, training loss: 628.1590576171875 = 0.18183568120002747 + 100 * 6.2797722816467285\n",
      "Epoch 3060, val loss: 0.790911078453064\n",
      "Epoch 3070, training loss: 627.4478149414062 = 0.18006514012813568 + 100 * 6.272677421569824\n",
      "Epoch 3070, val loss: 0.7903926968574524\n",
      "Epoch 3080, training loss: 627.671142578125 = 0.17845429480075836 + 100 * 6.274926662445068\n",
      "Epoch 3080, val loss: 0.7910680174827576\n",
      "Epoch 3090, training loss: 627.9043579101562 = 0.17678946256637573 + 100 * 6.277275085449219\n",
      "Epoch 3090, val loss: 0.7910463809967041\n",
      "Epoch 3100, training loss: 627.3270874023438 = 0.17520858347415924 + 100 * 6.271518707275391\n",
      "Epoch 3100, val loss: 0.7914080619812012\n",
      "Epoch 3110, training loss: 627.9451904296875 = 0.17363929748535156 + 100 * 6.27771520614624\n",
      "Epoch 3110, val loss: 0.7915791869163513\n",
      "Epoch 3120, training loss: 627.5986328125 = 0.17190948128700256 + 100 * 6.274266719818115\n",
      "Epoch 3120, val loss: 0.791129469871521\n",
      "Epoch 3130, training loss: 628.1787109375 = 0.1702747792005539 + 100 * 6.280084133148193\n",
      "Epoch 3130, val loss: 0.7910467982292175\n",
      "Epoch 3140, training loss: 627.4828491210938 = 0.16861841082572937 + 100 * 6.273142337799072\n",
      "Epoch 3140, val loss: 0.7916254997253418\n",
      "Epoch 3150, training loss: 627.5970458984375 = 0.16707409918308258 + 100 * 6.2743000984191895\n",
      "Epoch 3150, val loss: 0.7917378544807434\n",
      "Epoch 3160, training loss: 627.50048828125 = 0.16556908190250397 + 100 * 6.273349285125732\n",
      "Epoch 3160, val loss: 0.7916778922080994\n",
      "Epoch 3170, training loss: 627.477783203125 = 0.1640203446149826 + 100 * 6.27313756942749\n",
      "Epoch 3170, val loss: 0.7923333644866943\n",
      "Epoch 3180, training loss: 627.4483032226562 = 0.162542462348938 + 100 * 6.272857666015625\n",
      "Epoch 3180, val loss: 0.7928504347801208\n",
      "Epoch 3190, training loss: 627.6435546875 = 0.1611323207616806 + 100 * 6.274824142456055\n",
      "Epoch 3190, val loss: 0.7927521467208862\n",
      "Epoch 3200, training loss: 627.3508911132812 = 0.15957693755626678 + 100 * 6.271912574768066\n",
      "Epoch 3200, val loss: 0.793067991733551\n",
      "Epoch 3210, training loss: 627.6050415039062 = 0.15801678597927094 + 100 * 6.274470329284668\n",
      "Epoch 3210, val loss: 0.7929127812385559\n",
      "Epoch 3220, training loss: 627.35302734375 = 0.15657636523246765 + 100 * 6.2719645500183105\n",
      "Epoch 3220, val loss: 0.7929310202598572\n",
      "Epoch 3230, training loss: 627.7194213867188 = 0.15515665709972382 + 100 * 6.2756428718566895\n",
      "Epoch 3230, val loss: 0.793326199054718\n",
      "Epoch 3240, training loss: 627.3069458007812 = 0.15368148684501648 + 100 * 6.2715325355529785\n",
      "Epoch 3240, val loss: 0.793811559677124\n",
      "Epoch 3250, training loss: 627.4786376953125 = 0.15236251056194305 + 100 * 6.273262977600098\n",
      "Epoch 3250, val loss: 0.7941113710403442\n",
      "Epoch 3260, training loss: 627.6648559570312 = 0.15094827115535736 + 100 * 6.275138854980469\n",
      "Epoch 3260, val loss: 0.794131338596344\n",
      "Epoch 3270, training loss: 627.2239990234375 = 0.1494274139404297 + 100 * 6.270745754241943\n",
      "Epoch 3270, val loss: 0.7948067784309387\n",
      "Epoch 3280, training loss: 627.23486328125 = 0.1481865495443344 + 100 * 6.270866870880127\n",
      "Epoch 3280, val loss: 0.7947961091995239\n",
      "Epoch 3290, training loss: 627.3253173828125 = 0.1468927264213562 + 100 * 6.27178430557251\n",
      "Epoch 3290, val loss: 0.7957211136817932\n",
      "Epoch 3300, training loss: 627.6683349609375 = 0.14547838270664215 + 100 * 6.275228023529053\n",
      "Epoch 3300, val loss: 0.795778751373291\n",
      "Epoch 3310, training loss: 627.3572998046875 = 0.14397311210632324 + 100 * 6.2721333503723145\n",
      "Epoch 3310, val loss: 0.7952519655227661\n",
      "Epoch 3320, training loss: 627.0421142578125 = 0.14271579682826996 + 100 * 6.268993854522705\n",
      "Epoch 3320, val loss: 0.7956992387771606\n",
      "Epoch 3330, training loss: 627.01806640625 = 0.14152106642723083 + 100 * 6.268764972686768\n",
      "Epoch 3330, val loss: 0.7966857552528381\n",
      "Epoch 3340, training loss: 627.4801635742188 = 0.14032797515392303 + 100 * 6.273398399353027\n",
      "Epoch 3340, val loss: 0.7970927953720093\n",
      "Epoch 3350, training loss: 627.5438842773438 = 0.13891996443271637 + 100 * 6.274049758911133\n",
      "Epoch 3350, val loss: 0.7972138524055481\n",
      "Epoch 3360, training loss: 626.9327392578125 = 0.13753096759319305 + 100 * 6.2679524421691895\n",
      "Epoch 3360, val loss: 0.7971538305282593\n",
      "Epoch 3370, training loss: 626.8731079101562 = 0.13632583618164062 + 100 * 6.267367362976074\n",
      "Epoch 3370, val loss: 0.7978319525718689\n",
      "Epoch 3380, training loss: 626.9507446289062 = 0.1352207064628601 + 100 * 6.268155574798584\n",
      "Epoch 3380, val loss: 0.7985895276069641\n",
      "Epoch 3390, training loss: 627.530517578125 = 0.13411849737167358 + 100 * 6.2739644050598145\n",
      "Epoch 3390, val loss: 0.7989663481712341\n",
      "Epoch 3400, training loss: 627.4594116210938 = 0.13280074298381805 + 100 * 6.273265838623047\n",
      "Epoch 3400, val loss: 0.7992833256721497\n",
      "Epoch 3410, training loss: 626.9603271484375 = 0.13137999176979065 + 100 * 6.268289089202881\n",
      "Epoch 3410, val loss: 0.7992154359817505\n",
      "Epoch 3420, training loss: 626.9219360351562 = 0.1302243322134018 + 100 * 6.267917156219482\n",
      "Epoch 3420, val loss: 0.7997288107872009\n",
      "Epoch 3430, training loss: 626.959228515625 = 0.1291569471359253 + 100 * 6.268300533294678\n",
      "Epoch 3430, val loss: 0.8004555106163025\n",
      "Epoch 3440, training loss: 627.2908935546875 = 0.1280408799648285 + 100 * 6.271628379821777\n",
      "Epoch 3440, val loss: 0.8009353280067444\n",
      "Epoch 3450, training loss: 626.9205322265625 = 0.12686686217784882 + 100 * 6.2679362297058105\n",
      "Epoch 3450, val loss: 0.8011327385902405\n",
      "Epoch 3460, training loss: 627.2081298828125 = 0.1258115917444229 + 100 * 6.2708234786987305\n",
      "Epoch 3460, val loss: 0.8013550639152527\n",
      "Epoch 3470, training loss: 626.69921875 = 0.12468086183071136 + 100 * 6.265745162963867\n",
      "Epoch 3470, val loss: 0.8019884824752808\n",
      "Epoch 3480, training loss: 627.1858520507812 = 0.1236550584435463 + 100 * 6.2706217765808105\n",
      "Epoch 3480, val loss: 0.8025431632995605\n",
      "Epoch 3490, training loss: 627.3743896484375 = 0.12251827865839005 + 100 * 6.272518634796143\n",
      "Epoch 3490, val loss: 0.8025389313697815\n",
      "=== picking the best model according to the performance on validation ===\n",
      "Accuracy of GNN+CL: 0.7814814814814816\n",
      "0.808645229309436\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 861.6671752929688 = 1.9755189418792725 + 100 * 8.596916198730469\n",
      "Epoch 0, val loss: 1.9754242897033691\n",
      "Epoch 10, training loss: 861.6420288085938 = 1.9656171798706055 + 100 * 8.596763610839844\n",
      "Epoch 10, val loss: 1.9652786254882812\n",
      "Epoch 20, training loss: 861.4301147460938 = 1.9546947479248047 + 100 * 8.594754219055176\n",
      "Epoch 20, val loss: 1.9539151191711426\n",
      "Epoch 30, training loss: 858.6270751953125 = 1.9423307180404663 + 100 * 8.566847801208496\n",
      "Epoch 30, val loss: 1.9409217834472656\n",
      "Epoch 40, training loss: 831.9983520507812 = 1.9281628131866455 + 100 * 8.300702095031738\n",
      "Epoch 40, val loss: 1.925840973854065\n",
      "Epoch 50, training loss: 799.7127075195312 = 1.9141446352005005 + 100 * 7.977985858917236\n",
      "Epoch 50, val loss: 1.911163330078125\n",
      "Epoch 60, training loss: 787.4656982421875 = 1.9015074968338013 + 100 * 7.855642318725586\n",
      "Epoch 60, val loss: 1.8981943130493164\n",
      "Epoch 70, training loss: 771.1920776367188 = 1.8869832754135132 + 100 * 7.693051338195801\n",
      "Epoch 70, val loss: 1.8836816549301147\n",
      "Epoch 80, training loss: 743.4889526367188 = 1.8742177486419678 + 100 * 7.416147232055664\n",
      "Epoch 80, val loss: 1.8713386058807373\n",
      "Epoch 90, training loss: 729.8680419921875 = 1.862770915031433 + 100 * 7.280052661895752\n",
      "Epoch 90, val loss: 1.860960602760315\n",
      "Epoch 100, training loss: 722.4878540039062 = 1.853279709815979 + 100 * 7.206346035003662\n",
      "Epoch 100, val loss: 1.8523744344711304\n",
      "Epoch 110, training loss: 716.1260375976562 = 1.8465174436569214 + 100 * 7.142795562744141\n",
      "Epoch 110, val loss: 1.8458318710327148\n",
      "Epoch 120, training loss: 709.1586303710938 = 1.8416465520858765 + 100 * 7.073169708251953\n",
      "Epoch 120, val loss: 1.8408212661743164\n",
      "Epoch 130, training loss: 700.7830200195312 = 1.8375016450881958 + 100 * 6.989454746246338\n",
      "Epoch 130, val loss: 1.8364604711532593\n",
      "Epoch 140, training loss: 692.930419921875 = 1.8336385488510132 + 100 * 6.91096830368042\n",
      "Epoch 140, val loss: 1.8322210311889648\n",
      "Epoch 150, training loss: 687.7825317382812 = 1.8296730518341064 + 100 * 6.8595290184021\n",
      "Epoch 150, val loss: 1.8275859355926514\n",
      "Epoch 160, training loss: 683.2061157226562 = 1.8253357410430908 + 100 * 6.813807964324951\n",
      "Epoch 160, val loss: 1.8227128982543945\n",
      "Epoch 170, training loss: 679.359375 = 1.8212602138519287 + 100 * 6.775380611419678\n",
      "Epoch 170, val loss: 1.8181241750717163\n",
      "Epoch 180, training loss: 676.67041015625 = 1.8175890445709229 + 100 * 6.748528480529785\n",
      "Epoch 180, val loss: 1.8139644861221313\n",
      "Epoch 190, training loss: 674.9083251953125 = 1.8141279220581055 + 100 * 6.7309417724609375\n",
      "Epoch 190, val loss: 1.8101353645324707\n",
      "Epoch 200, training loss: 673.1182861328125 = 1.8107775449752808 + 100 * 6.713075160980225\n",
      "Epoch 200, val loss: 1.806592583656311\n",
      "Epoch 210, training loss: 671.7353515625 = 1.8076634407043457 + 100 * 6.699276447296143\n",
      "Epoch 210, val loss: 1.8034167289733887\n",
      "Epoch 220, training loss: 670.4912719726562 = 1.804679274559021 + 100 * 6.68686580657959\n",
      "Epoch 220, val loss: 1.8003599643707275\n",
      "Epoch 230, training loss: 669.533447265625 = 1.801727294921875 + 100 * 6.677317142486572\n",
      "Epoch 230, val loss: 1.7974313497543335\n",
      "Epoch 240, training loss: 668.5050659179688 = 1.7987968921661377 + 100 * 6.667063236236572\n",
      "Epoch 240, val loss: 1.794540524482727\n",
      "Epoch 250, training loss: 667.8269653320312 = 1.7960699796676636 + 100 * 6.660308837890625\n",
      "Epoch 250, val loss: 1.7917615175247192\n",
      "Epoch 260, training loss: 666.937744140625 = 1.7933794260025024 + 100 * 6.6514434814453125\n",
      "Epoch 260, val loss: 1.7889554500579834\n",
      "Epoch 270, training loss: 666.1019287109375 = 1.7908010482788086 + 100 * 6.643111705780029\n",
      "Epoch 270, val loss: 1.7862993478775024\n",
      "Epoch 280, training loss: 665.45068359375 = 1.7883415222167969 + 100 * 6.636623382568359\n",
      "Epoch 280, val loss: 1.7836052179336548\n",
      "Epoch 290, training loss: 664.8198852539062 = 1.7858794927597046 + 100 * 6.630340099334717\n",
      "Epoch 290, val loss: 1.7811769247055054\n",
      "Epoch 300, training loss: 663.6036987304688 = 1.783456802368164 + 100 * 6.6182026863098145\n",
      "Epoch 300, val loss: 1.778578281402588\n",
      "Epoch 310, training loss: 662.5525512695312 = 1.7812858819961548 + 100 * 6.607712268829346\n",
      "Epoch 310, val loss: 1.7762879133224487\n",
      "Epoch 320, training loss: 663.1737060546875 = 1.7788033485412598 + 100 * 6.613948822021484\n",
      "Epoch 320, val loss: 1.7735095024108887\n",
      "Epoch 330, training loss: 661.0110473632812 = 1.7759897708892822 + 100 * 6.592350482940674\n",
      "Epoch 330, val loss: 1.770460605621338\n",
      "Epoch 340, training loss: 659.70458984375 = 1.7735326290130615 + 100 * 6.579310417175293\n",
      "Epoch 340, val loss: 1.7681604623794556\n",
      "Epoch 350, training loss: 660.2061157226562 = 1.771114706993103 + 100 * 6.584350109100342\n",
      "Epoch 350, val loss: 1.7655065059661865\n",
      "Epoch 360, training loss: 657.6436157226562 = 1.7680304050445557 + 100 * 6.558755874633789\n",
      "Epoch 360, val loss: 1.7623229026794434\n",
      "Epoch 370, training loss: 656.6188354492188 = 1.76529860496521 + 100 * 6.548535346984863\n",
      "Epoch 370, val loss: 1.7594304084777832\n",
      "Epoch 380, training loss: 655.2662963867188 = 1.7624635696411133 + 100 * 6.535038471221924\n",
      "Epoch 380, val loss: 1.756460189819336\n",
      "Epoch 390, training loss: 654.7781982421875 = 1.7596073150634766 + 100 * 6.530186176300049\n",
      "Epoch 390, val loss: 1.753621220588684\n",
      "Epoch 400, training loss: 653.9158935546875 = 1.7563508749008179 + 100 * 6.521595478057861\n",
      "Epoch 400, val loss: 1.7498952150344849\n",
      "Epoch 410, training loss: 652.8956909179688 = 1.7529017925262451 + 100 * 6.511427402496338\n",
      "Epoch 410, val loss: 1.7466777563095093\n",
      "Epoch 420, training loss: 652.4650268554688 = 1.7492880821228027 + 100 * 6.507157325744629\n",
      "Epoch 420, val loss: 1.7429968118667603\n",
      "Epoch 430, training loss: 651.8308715820312 = 1.745508074760437 + 100 * 6.500854015350342\n",
      "Epoch 430, val loss: 1.7394074201583862\n",
      "Epoch 440, training loss: 651.1275634765625 = 1.7421847581863403 + 100 * 6.493853569030762\n",
      "Epoch 440, val loss: 1.7362998723983765\n",
      "Epoch 450, training loss: 650.4982299804688 = 1.7383595705032349 + 100 * 6.487598896026611\n",
      "Epoch 450, val loss: 1.732322096824646\n",
      "Epoch 460, training loss: 649.940185546875 = 1.7345649003982544 + 100 * 6.482056140899658\n",
      "Epoch 460, val loss: 1.7288017272949219\n",
      "Epoch 470, training loss: 649.2916259765625 = 1.731029748916626 + 100 * 6.4756059646606445\n",
      "Epoch 470, val loss: 1.7255817651748657\n",
      "Epoch 480, training loss: 651.825439453125 = 1.7271628379821777 + 100 * 6.500982761383057\n",
      "Epoch 480, val loss: 1.722110390663147\n",
      "Epoch 490, training loss: 648.9569091796875 = 1.722259521484375 + 100 * 6.472346782684326\n",
      "Epoch 490, val loss: 1.7172726392745972\n",
      "Epoch 500, training loss: 648.0311279296875 = 1.7180646657943726 + 100 * 6.463130474090576\n",
      "Epoch 500, val loss: 1.7132102251052856\n",
      "Epoch 510, training loss: 647.61572265625 = 1.7139071226119995 + 100 * 6.459018230438232\n",
      "Epoch 510, val loss: 1.7092760801315308\n",
      "Epoch 520, training loss: 647.2766723632812 = 1.7096989154815674 + 100 * 6.455669403076172\n",
      "Epoch 520, val loss: 1.705262541770935\n",
      "Epoch 530, training loss: 648.3075561523438 = 1.7049108743667603 + 100 * 6.466026306152344\n",
      "Epoch 530, val loss: 1.7007495164871216\n",
      "Epoch 540, training loss: 647.52197265625 = 1.6993714570999146 + 100 * 6.458225727081299\n",
      "Epoch 540, val loss: 1.695300817489624\n",
      "Epoch 550, training loss: 646.7221069335938 = 1.6944125890731812 + 100 * 6.450277328491211\n",
      "Epoch 550, val loss: 1.6907577514648438\n",
      "Epoch 560, training loss: 646.1455078125 = 1.6897213459014893 + 100 * 6.444558143615723\n",
      "Epoch 560, val loss: 1.6864473819732666\n",
      "Epoch 570, training loss: 647.2421264648438 = 1.6848838329315186 + 100 * 6.455572605133057\n",
      "Epoch 570, val loss: 1.6818938255310059\n",
      "Epoch 580, training loss: 647.1281127929688 = 1.6791812181472778 + 100 * 6.454489231109619\n",
      "Epoch 580, val loss: 1.6759847402572632\n",
      "Epoch 590, training loss: 645.7011108398438 = 1.6730269193649292 + 100 * 6.440280914306641\n",
      "Epoch 590, val loss: 1.6709070205688477\n",
      "Epoch 600, training loss: 645.0947875976562 = 1.667941689491272 + 100 * 6.434268474578857\n",
      "Epoch 600, val loss: 1.6660386323928833\n",
      "Epoch 610, training loss: 644.8596801757812 = 1.6628025770187378 + 100 * 6.431969165802002\n",
      "Epoch 610, val loss: 1.6615866422653198\n",
      "Epoch 620, training loss: 645.1919555664062 = 1.657369613647461 + 100 * 6.4353461265563965\n",
      "Epoch 620, val loss: 1.6566671133041382\n",
      "Epoch 630, training loss: 644.8214721679688 = 1.651328444480896 + 100 * 6.43170166015625\n",
      "Epoch 630, val loss: 1.650646686553955\n",
      "Epoch 640, training loss: 644.88427734375 = 1.644669532775879 + 100 * 6.432396411895752\n",
      "Epoch 640, val loss: 1.6446328163146973\n",
      "Epoch 650, training loss: 644.0260620117188 = 1.6384847164154053 + 100 * 6.42387580871582\n",
      "Epoch 650, val loss: 1.6391030550003052\n",
      "Epoch 660, training loss: 643.646728515625 = 1.6326628923416138 + 100 * 6.420140266418457\n",
      "Epoch 660, val loss: 1.6337729692459106\n",
      "Epoch 670, training loss: 644.2290649414062 = 1.6264079809188843 + 100 * 6.426026344299316\n",
      "Epoch 670, val loss: 1.6278828382492065\n",
      "Epoch 680, training loss: 643.1815185546875 = 1.6193140745162964 + 100 * 6.415621757507324\n",
      "Epoch 680, val loss: 1.6217564344406128\n",
      "Epoch 690, training loss: 643.0072631835938 = 1.612953543663025 + 100 * 6.413942813873291\n",
      "Epoch 690, val loss: 1.6159461736679077\n",
      "Epoch 700, training loss: 643.82568359375 = 1.6062161922454834 + 100 * 6.422194957733154\n",
      "Epoch 700, val loss: 1.6105847358703613\n",
      "Epoch 710, training loss: 642.7443237304688 = 1.598883867263794 + 100 * 6.411454200744629\n",
      "Epoch 710, val loss: 1.6029547452926636\n",
      "Epoch 720, training loss: 642.7159423828125 = 1.5920735597610474 + 100 * 6.411238193511963\n",
      "Epoch 720, val loss: 1.5972753763198853\n",
      "Epoch 730, training loss: 641.9605102539062 = 1.584219217300415 + 100 * 6.4037628173828125\n",
      "Epoch 730, val loss: 1.5903717279434204\n",
      "Epoch 740, training loss: 641.7930908203125 = 1.577013611793518 + 100 * 6.40216064453125\n",
      "Epoch 740, val loss: 1.5839900970458984\n",
      "Epoch 750, training loss: 641.441162109375 = 1.5702283382415771 + 100 * 6.398709297180176\n",
      "Epoch 750, val loss: 1.5778728723526\n",
      "Epoch 760, training loss: 643.2048950195312 = 1.5628341436386108 + 100 * 6.416420936584473\n",
      "Epoch 760, val loss: 1.5716044902801514\n",
      "Epoch 770, training loss: 641.2734985351562 = 1.5539954900741577 + 100 * 6.397194862365723\n",
      "Epoch 770, val loss: 1.5636718273162842\n",
      "Epoch 780, training loss: 640.9678955078125 = 1.546263575553894 + 100 * 6.394216537475586\n",
      "Epoch 780, val loss: 1.557092308998108\n",
      "Epoch 790, training loss: 640.8978271484375 = 1.5389187335968018 + 100 * 6.393589019775391\n",
      "Epoch 790, val loss: 1.5506633520126343\n",
      "Epoch 800, training loss: 640.8662109375 = 1.5307101011276245 + 100 * 6.393355369567871\n",
      "Epoch 800, val loss: 1.5433006286621094\n",
      "Epoch 810, training loss: 640.4998779296875 = 1.5221785306930542 + 100 * 6.389777183532715\n",
      "Epoch 810, val loss: 1.5358487367630005\n",
      "Epoch 820, training loss: 640.01806640625 = 1.5138013362884521 + 100 * 6.385042667388916\n",
      "Epoch 820, val loss: 1.5292764902114868\n",
      "Epoch 830, training loss: 640.156005859375 = 1.505778193473816 + 100 * 6.386502265930176\n",
      "Epoch 830, val loss: 1.5225521326065063\n",
      "Epoch 840, training loss: 640.5643920898438 = 1.4967204332351685 + 100 * 6.390676975250244\n",
      "Epoch 840, val loss: 1.5144295692443848\n",
      "Epoch 850, training loss: 639.8605346679688 = 1.4873453378677368 + 100 * 6.383731842041016\n",
      "Epoch 850, val loss: 1.5065107345581055\n",
      "Epoch 860, training loss: 639.4986572265625 = 1.4789143800735474 + 100 * 6.380197048187256\n",
      "Epoch 860, val loss: 1.4992976188659668\n",
      "Epoch 870, training loss: 639.9552001953125 = 1.4704341888427734 + 100 * 6.384847164154053\n",
      "Epoch 870, val loss: 1.4925328493118286\n",
      "Epoch 880, training loss: 639.220703125 = 1.4609365463256836 + 100 * 6.377597808837891\n",
      "Epoch 880, val loss: 1.4839223623275757\n",
      "Epoch 890, training loss: 639.2141723632812 = 1.4518333673477173 + 100 * 6.377623558044434\n",
      "Epoch 890, val loss: 1.4763083457946777\n",
      "Epoch 900, training loss: 638.9498901367188 = 1.4426698684692383 + 100 * 6.375072002410889\n",
      "Epoch 900, val loss: 1.468530297279358\n",
      "Epoch 910, training loss: 638.9059448242188 = 1.4336941242218018 + 100 * 6.374722003936768\n",
      "Epoch 910, val loss: 1.461424708366394\n",
      "Epoch 920, training loss: 638.83349609375 = 1.4239797592163086 + 100 * 6.374095439910889\n",
      "Epoch 920, val loss: 1.453154444694519\n",
      "Epoch 930, training loss: 639.03173828125 = 1.414366602897644 + 100 * 6.376173973083496\n",
      "Epoch 930, val loss: 1.4454213380813599\n",
      "Epoch 940, training loss: 638.6671752929688 = 1.4042173624038696 + 100 * 6.372629165649414\n",
      "Epoch 940, val loss: 1.4368579387664795\n",
      "Epoch 950, training loss: 638.253173828125 = 1.3949512243270874 + 100 * 6.368582248687744\n",
      "Epoch 950, val loss: 1.4285911321640015\n",
      "Epoch 960, training loss: 638.5296020507812 = 1.385759949684143 + 100 * 6.371438503265381\n",
      "Epoch 960, val loss: 1.4215444326400757\n",
      "Epoch 970, training loss: 638.5365600585938 = 1.3756258487701416 + 100 * 6.371609210968018\n",
      "Epoch 970, val loss: 1.4129648208618164\n",
      "Epoch 980, training loss: 637.899658203125 = 1.3654698133468628 + 100 * 6.365341663360596\n",
      "Epoch 980, val loss: 1.404487133026123\n",
      "Epoch 990, training loss: 637.95703125 = 1.3560266494750977 + 100 * 6.3660101890563965\n",
      "Epoch 990, val loss: 1.3965753316879272\n",
      "Epoch 1000, training loss: 638.0927734375 = 1.3460009098052979 + 100 * 6.367467403411865\n",
      "Epoch 1000, val loss: 1.388321876525879\n",
      "Epoch 1010, training loss: 637.7515258789062 = 1.3357080221176147 + 100 * 6.3641581535339355\n",
      "Epoch 1010, val loss: 1.3798949718475342\n",
      "Epoch 1020, training loss: 637.4112548828125 = 1.3260833024978638 + 100 * 6.360851287841797\n",
      "Epoch 1020, val loss: 1.371886968612671\n",
      "Epoch 1030, training loss: 638.0128173828125 = 1.3165264129638672 + 100 * 6.366962909698486\n",
      "Epoch 1030, val loss: 1.3641645908355713\n",
      "Epoch 1040, training loss: 637.4827880859375 = 1.3060483932495117 + 100 * 6.361767768859863\n",
      "Epoch 1040, val loss: 1.3553273677825928\n",
      "Epoch 1050, training loss: 637.1248779296875 = 1.2960014343261719 + 100 * 6.358288764953613\n",
      "Epoch 1050, val loss: 1.3476827144622803\n",
      "Epoch 1060, training loss: 636.9735107421875 = 1.2867828607559204 + 100 * 6.35686731338501\n",
      "Epoch 1060, val loss: 1.3398116827011108\n",
      "Epoch 1070, training loss: 637.4210815429688 = 1.2766163349151611 + 100 * 6.36144495010376\n",
      "Epoch 1070, val loss: 1.3320252895355225\n",
      "Epoch 1080, training loss: 636.7476806640625 = 1.2661179304122925 + 100 * 6.35481595993042\n",
      "Epoch 1080, val loss: 1.322888731956482\n",
      "Epoch 1090, training loss: 636.772705078125 = 1.256463646888733 + 100 * 6.355162143707275\n",
      "Epoch 1090, val loss: 1.3154733180999756\n",
      "Epoch 1100, training loss: 637.1016845703125 = 1.2468876838684082 + 100 * 6.358547687530518\n",
      "Epoch 1100, val loss: 1.307357668876648\n",
      "Epoch 1110, training loss: 637.3302001953125 = 1.2363086938858032 + 100 * 6.360939025878906\n",
      "Epoch 1110, val loss: 1.2993693351745605\n",
      "Epoch 1120, training loss: 636.491943359375 = 1.2264466285705566 + 100 * 6.352654933929443\n",
      "Epoch 1120, val loss: 1.2914990186691284\n",
      "Epoch 1130, training loss: 636.2861328125 = 1.2172720432281494 + 100 * 6.350688457489014\n",
      "Epoch 1130, val loss: 1.2839852571487427\n",
      "Epoch 1140, training loss: 636.5142211914062 = 1.208052396774292 + 100 * 6.353061676025391\n",
      "Epoch 1140, val loss: 1.276886224746704\n",
      "Epoch 1150, training loss: 636.1098022460938 = 1.198001503944397 + 100 * 6.349118232727051\n",
      "Epoch 1150, val loss: 1.269093632698059\n",
      "Epoch 1160, training loss: 636.459716796875 = 1.188241958618164 + 100 * 6.352715015411377\n",
      "Epoch 1160, val loss: 1.2615034580230713\n",
      "Epoch 1170, training loss: 636.3225708007812 = 1.178427815437317 + 100 * 6.351441383361816\n",
      "Epoch 1170, val loss: 1.2540773153305054\n",
      "Epoch 1180, training loss: 636.0017700195312 = 1.169106364250183 + 100 * 6.348326683044434\n",
      "Epoch 1180, val loss: 1.246740698814392\n",
      "Epoch 1190, training loss: 636.2588500976562 = 1.159286618232727 + 100 * 6.3509955406188965\n",
      "Epoch 1190, val loss: 1.2389949560165405\n",
      "Epoch 1200, training loss: 636.0341796875 = 1.1498347520828247 + 100 * 6.348843097686768\n",
      "Epoch 1200, val loss: 1.2308212518692017\n",
      "Epoch 1210, training loss: 635.8812255859375 = 1.1399900913238525 + 100 * 6.347412109375\n",
      "Epoch 1210, val loss: 1.2239830493927002\n",
      "Epoch 1220, training loss: 635.5999755859375 = 1.1306835412979126 + 100 * 6.344693183898926\n",
      "Epoch 1220, val loss: 1.2168573141098022\n",
      "Epoch 1230, training loss: 636.1011962890625 = 1.1212552785873413 + 100 * 6.349799156188965\n",
      "Epoch 1230, val loss: 1.2094852924346924\n",
      "Epoch 1240, training loss: 636.1768188476562 = 1.111721396446228 + 100 * 6.350651264190674\n",
      "Epoch 1240, val loss: 1.2015436887741089\n",
      "Epoch 1250, training loss: 635.254638671875 = 1.1018403768539429 + 100 * 6.341527938842773\n",
      "Epoch 1250, val loss: 1.1946498155593872\n",
      "Epoch 1260, training loss: 635.145263671875 = 1.0930756330490112 + 100 * 6.340521812438965\n",
      "Epoch 1260, val loss: 1.1876949071884155\n",
      "Epoch 1270, training loss: 635.7103881835938 = 1.0843110084533691 + 100 * 6.346261024475098\n",
      "Epoch 1270, val loss: 1.1816985607147217\n",
      "Epoch 1280, training loss: 634.9362182617188 = 1.0744984149932861 + 100 * 6.338616847991943\n",
      "Epoch 1280, val loss: 1.1736502647399902\n",
      "Epoch 1290, training loss: 634.760498046875 = 1.065442681312561 + 100 * 6.336950778961182\n",
      "Epoch 1290, val loss: 1.1668702363967896\n",
      "Epoch 1300, training loss: 634.997314453125 = 1.0569117069244385 + 100 * 6.339404582977295\n",
      "Epoch 1300, val loss: 1.1604219675064087\n",
      "Epoch 1310, training loss: 635.7264404296875 = 1.0480897426605225 + 100 * 6.346783638000488\n",
      "Epoch 1310, val loss: 1.1534520387649536\n",
      "Epoch 1320, training loss: 635.161865234375 = 1.0380467176437378 + 100 * 6.341238498687744\n",
      "Epoch 1320, val loss: 1.146682620048523\n",
      "Epoch 1330, training loss: 634.5764770507812 = 1.0292209386825562 + 100 * 6.335472106933594\n",
      "Epoch 1330, val loss: 1.1396169662475586\n",
      "Epoch 1340, training loss: 634.4710083007812 = 1.020798683166504 + 100 * 6.334502220153809\n",
      "Epoch 1340, val loss: 1.1339854001998901\n",
      "Epoch 1350, training loss: 635.2613525390625 = 1.0120433568954468 + 100 * 6.342493534088135\n",
      "Epoch 1350, val loss: 1.127285122871399\n",
      "Epoch 1360, training loss: 634.7953491210938 = 1.0025520324707031 + 100 * 6.33792781829834\n",
      "Epoch 1360, val loss: 1.1205017566680908\n",
      "Epoch 1370, training loss: 634.215087890625 = 0.9939160943031311 + 100 * 6.332211971282959\n",
      "Epoch 1370, val loss: 1.1141148805618286\n",
      "Epoch 1380, training loss: 634.0511474609375 = 0.9858910441398621 + 100 * 6.330652236938477\n",
      "Epoch 1380, val loss: 1.108406901359558\n",
      "Epoch 1390, training loss: 634.8939819335938 = 0.9779621362686157 + 100 * 6.339159965515137\n",
      "Epoch 1390, val loss: 1.1026304960250854\n",
      "Epoch 1400, training loss: 633.9777221679688 = 0.9684204459190369 + 100 * 6.330092906951904\n",
      "Epoch 1400, val loss: 1.0958486795425415\n",
      "Epoch 1410, training loss: 634.4932861328125 = 0.9597429633140564 + 100 * 6.335335731506348\n",
      "Epoch 1410, val loss: 1.0896497964859009\n",
      "Epoch 1420, training loss: 633.9410400390625 = 0.9510624408721924 + 100 * 6.329899787902832\n",
      "Epoch 1420, val loss: 1.0835708379745483\n",
      "Epoch 1430, training loss: 634.2222290039062 = 0.9428638219833374 + 100 * 6.332793712615967\n",
      "Epoch 1430, val loss: 1.0773632526397705\n",
      "Epoch 1440, training loss: 633.9385986328125 = 0.9342504739761353 + 100 * 6.330043315887451\n",
      "Epoch 1440, val loss: 1.071394920349121\n",
      "Epoch 1450, training loss: 633.6394653320312 = 0.9262477159500122 + 100 * 6.327132225036621\n",
      "Epoch 1450, val loss: 1.0659571886062622\n",
      "Epoch 1460, training loss: 634.6172485351562 = 0.9183878302574158 + 100 * 6.33698844909668\n",
      "Epoch 1460, val loss: 1.0609123706817627\n",
      "Epoch 1470, training loss: 633.5502319335938 = 0.9094641208648682 + 100 * 6.326407432556152\n",
      "Epoch 1470, val loss: 1.053909182548523\n",
      "Epoch 1480, training loss: 633.6890869140625 = 0.9014034867286682 + 100 * 6.327876567840576\n",
      "Epoch 1480, val loss: 1.048925518989563\n",
      "Epoch 1490, training loss: 633.308837890625 = 0.8932119607925415 + 100 * 6.324156761169434\n",
      "Epoch 1490, val loss: 1.0427669286727905\n",
      "Epoch 1500, training loss: 633.333251953125 = 0.885646641254425 + 100 * 6.32447624206543\n",
      "Epoch 1500, val loss: 1.0373016595840454\n",
      "Epoch 1510, training loss: 634.3741455078125 = 0.8777214884757996 + 100 * 6.334963798522949\n",
      "Epoch 1510, val loss: 1.0322911739349365\n",
      "Epoch 1520, training loss: 633.4675903320312 = 0.869352400302887 + 100 * 6.325982570648193\n",
      "Epoch 1520, val loss: 1.0262258052825928\n",
      "Epoch 1530, training loss: 634.1754760742188 = 0.8615809082984924 + 100 * 6.333138942718506\n",
      "Epoch 1530, val loss: 1.0206832885742188\n",
      "Epoch 1540, training loss: 633.0661010742188 = 0.8528680205345154 + 100 * 6.322132587432861\n",
      "Epoch 1540, val loss: 1.0148588418960571\n",
      "Epoch 1550, training loss: 633.0878295898438 = 0.8452759981155396 + 100 * 6.322425842285156\n",
      "Epoch 1550, val loss: 1.0095727443695068\n",
      "Epoch 1560, training loss: 633.3696899414062 = 0.837912380695343 + 100 * 6.325317859649658\n",
      "Epoch 1560, val loss: 1.0046356916427612\n",
      "Epoch 1570, training loss: 633.3817749023438 = 0.8300814032554626 + 100 * 6.325516700744629\n",
      "Epoch 1570, val loss: 0.9992207884788513\n",
      "Epoch 1580, training loss: 633.2887573242188 = 0.8218661546707153 + 100 * 6.324668884277344\n",
      "Epoch 1580, val loss: 0.9940575361251831\n",
      "Epoch 1590, training loss: 632.9979858398438 = 0.8144263029098511 + 100 * 6.321835517883301\n",
      "Epoch 1590, val loss: 0.9891947507858276\n",
      "Epoch 1600, training loss: 632.7402954101562 = 0.8072591423988342 + 100 * 6.319330215454102\n",
      "Epoch 1600, val loss: 0.9844338893890381\n",
      "Epoch 1610, training loss: 633.5577392578125 = 0.80023193359375 + 100 * 6.327575206756592\n",
      "Epoch 1610, val loss: 0.9800474047660828\n",
      "Epoch 1620, training loss: 632.8927001953125 = 0.7914760708808899 + 100 * 6.321012020111084\n",
      "Epoch 1620, val loss: 0.973883867263794\n",
      "Epoch 1630, training loss: 632.8530883789062 = 0.7838148474693298 + 100 * 6.320692539215088\n",
      "Epoch 1630, val loss: 0.9684373736381531\n",
      "Epoch 1640, training loss: 632.706787109375 = 0.7768073678016663 + 100 * 6.319299697875977\n",
      "Epoch 1640, val loss: 0.9643357992172241\n",
      "Epoch 1650, training loss: 633.4254150390625 = 0.7699158191680908 + 100 * 6.326555252075195\n",
      "Epoch 1650, val loss: 0.9594793319702148\n",
      "Epoch 1660, training loss: 632.4696655273438 = 0.7620511651039124 + 100 * 6.317076683044434\n",
      "Epoch 1660, val loss: 0.9547407627105713\n",
      "Epoch 1670, training loss: 632.6007690429688 = 0.7549253702163696 + 100 * 6.318458557128906\n",
      "Epoch 1670, val loss: 0.9499648213386536\n",
      "Epoch 1680, training loss: 633.2680053710938 = 0.7478621006011963 + 100 * 6.325201511383057\n",
      "Epoch 1680, val loss: 0.9450526833534241\n",
      "Epoch 1690, training loss: 632.404296875 = 0.7405701875686646 + 100 * 6.31663703918457\n",
      "Epoch 1690, val loss: 0.9408729076385498\n",
      "Epoch 1700, training loss: 632.8703002929688 = 0.7338835597038269 + 100 * 6.321363925933838\n",
      "Epoch 1700, val loss: 0.9362780451774597\n",
      "Epoch 1710, training loss: 632.4271240234375 = 0.7260958552360535 + 100 * 6.317010402679443\n",
      "Epoch 1710, val loss: 0.9315513968467712\n",
      "Epoch 1720, training loss: 632.3551025390625 = 0.719090461730957 + 100 * 6.316359996795654\n",
      "Epoch 1720, val loss: 0.9276040196418762\n",
      "Epoch 1730, training loss: 633.474609375 = 0.7119379639625549 + 100 * 6.327626705169678\n",
      "Epoch 1730, val loss: 0.9229185581207275\n",
      "Epoch 1740, training loss: 632.3893432617188 = 0.7043681144714355 + 100 * 6.316850185394287\n",
      "Epoch 1740, val loss: 0.9171667098999023\n",
      "Epoch 1750, training loss: 632.0673217773438 = 0.6977814435958862 + 100 * 6.313695907592773\n",
      "Epoch 1750, val loss: 0.9139800667762756\n",
      "Epoch 1760, training loss: 631.986328125 = 0.6914600729942322 + 100 * 6.312948703765869\n",
      "Epoch 1760, val loss: 0.9099740386009216\n",
      "Epoch 1770, training loss: 633.318603515625 = 0.6848349571228027 + 100 * 6.326337814331055\n",
      "Epoch 1770, val loss: 0.9057246446609497\n",
      "Epoch 1780, training loss: 632.1839599609375 = 0.677495002746582 + 100 * 6.315064430236816\n",
      "Epoch 1780, val loss: 0.901154100894928\n",
      "Epoch 1790, training loss: 632.9085083007812 = 0.6708245873451233 + 100 * 6.322376728057861\n",
      "Epoch 1790, val loss: 0.8967333436012268\n",
      "Epoch 1800, training loss: 631.9469604492188 = 0.663485586643219 + 100 * 6.312834739685059\n",
      "Epoch 1800, val loss: 0.8920547962188721\n",
      "Epoch 1810, training loss: 631.9943237304688 = 0.6572461128234863 + 100 * 6.313371181488037\n",
      "Epoch 1810, val loss: 0.8882807493209839\n",
      "Epoch 1820, training loss: 632.1288452148438 = 0.6509709358215332 + 100 * 6.3147783279418945\n",
      "Epoch 1820, val loss: 0.8842518329620361\n",
      "Epoch 1830, training loss: 632.4808959960938 = 0.6438189744949341 + 100 * 6.318370819091797\n",
      "Epoch 1830, val loss: 0.8805318474769592\n",
      "Epoch 1840, training loss: 632.0801391601562 = 0.6362786889076233 + 100 * 6.314438343048096\n",
      "Epoch 1840, val loss: 0.8749929070472717\n",
      "Epoch 1850, training loss: 631.7672729492188 = 0.6298110485076904 + 100 * 6.311374664306641\n",
      "Epoch 1850, val loss: 0.8714398145675659\n",
      "Epoch 1860, training loss: 631.57080078125 = 0.6238362789154053 + 100 * 6.309469699859619\n",
      "Epoch 1860, val loss: 0.8679518103599548\n",
      "Epoch 1870, training loss: 631.80712890625 = 0.6179358959197998 + 100 * 6.311892032623291\n",
      "Epoch 1870, val loss: 0.8645832538604736\n",
      "Epoch 1880, training loss: 631.8696899414062 = 0.6115109324455261 + 100 * 6.312581539154053\n",
      "Epoch 1880, val loss: 0.8604562878608704\n",
      "Epoch 1890, training loss: 632.6836547851562 = 0.6052016019821167 + 100 * 6.320784091949463\n",
      "Epoch 1890, val loss: 0.8562643527984619\n",
      "Epoch 1900, training loss: 631.6569213867188 = 0.5981824994087219 + 100 * 6.310586929321289\n",
      "Epoch 1900, val loss: 0.8524981737136841\n",
      "Epoch 1910, training loss: 632.0894165039062 = 0.5921329259872437 + 100 * 6.314972400665283\n",
      "Epoch 1910, val loss: 0.8488421440124512\n",
      "Epoch 1920, training loss: 631.5891723632812 = 0.5855560898780823 + 100 * 6.3100361824035645\n",
      "Epoch 1920, val loss: 0.8447279334068298\n",
      "Epoch 1930, training loss: 631.4331665039062 = 0.5796819925308228 + 100 * 6.308534622192383\n",
      "Epoch 1930, val loss: 0.8415037393569946\n",
      "Epoch 1940, training loss: 631.2246704101562 = 0.5738903880119324 + 100 * 6.306507587432861\n",
      "Epoch 1940, val loss: 0.8384590744972229\n",
      "Epoch 1950, training loss: 631.7997436523438 = 0.5682985782623291 + 100 * 6.312314510345459\n",
      "Epoch 1950, val loss: 0.8351650238037109\n",
      "Epoch 1960, training loss: 632.06787109375 = 0.561609148979187 + 100 * 6.315062999725342\n",
      "Epoch 1960, val loss: 0.8309516906738281\n",
      "Epoch 1970, training loss: 631.382568359375 = 0.5552594661712646 + 100 * 6.3082733154296875\n",
      "Epoch 1970, val loss: 0.8267555832862854\n",
      "Epoch 1980, training loss: 631.0731201171875 = 0.5496004819869995 + 100 * 6.305234909057617\n",
      "Epoch 1980, val loss: 0.8243907690048218\n",
      "Epoch 1990, training loss: 631.2884521484375 = 0.5443390607833862 + 100 * 6.307441711425781\n",
      "Epoch 1990, val loss: 0.8211124539375305\n",
      "Epoch 2000, training loss: 631.5790405273438 = 0.5379912257194519 + 100 * 6.310410499572754\n",
      "Epoch 2000, val loss: 0.81699138879776\n",
      "Epoch 2010, training loss: 631.1890869140625 = 0.5317341089248657 + 100 * 6.306573390960693\n",
      "Epoch 2010, val loss: 0.8133174180984497\n",
      "Epoch 2020, training loss: 631.0128784179688 = 0.5261425971984863 + 100 * 6.304867744445801\n",
      "Epoch 2020, val loss: 0.8104419708251953\n",
      "Epoch 2030, training loss: 631.1522216796875 = 0.5207631587982178 + 100 * 6.306314468383789\n",
      "Epoch 2030, val loss: 0.8071939945220947\n",
      "Epoch 2040, training loss: 631.0799560546875 = 0.5152149796485901 + 100 * 6.305647373199463\n",
      "Epoch 2040, val loss: 0.8044843077659607\n",
      "Epoch 2050, training loss: 631.7804565429688 = 0.5095940232276917 + 100 * 6.312708854675293\n",
      "Epoch 2050, val loss: 0.8019264936447144\n",
      "Epoch 2060, training loss: 630.9229736328125 = 0.5037447214126587 + 100 * 6.304192543029785\n",
      "Epoch 2060, val loss: 0.7977368831634521\n",
      "Epoch 2070, training loss: 631.2276000976562 = 0.4984619915485382 + 100 * 6.307291030883789\n",
      "Epoch 2070, val loss: 0.7955183386802673\n",
      "Epoch 2080, training loss: 630.8946533203125 = 0.4927337169647217 + 100 * 6.304018974304199\n",
      "Epoch 2080, val loss: 0.791750967502594\n",
      "Epoch 2090, training loss: 630.7517700195312 = 0.48751717805862427 + 100 * 6.302642822265625\n",
      "Epoch 2090, val loss: 0.7883644104003906\n",
      "Epoch 2100, training loss: 631.0235595703125 = 0.48240455985069275 + 100 * 6.305411338806152\n",
      "Epoch 2100, val loss: 0.7859750986099243\n",
      "Epoch 2110, training loss: 630.7415771484375 = 0.4770902693271637 + 100 * 6.302644729614258\n",
      "Epoch 2110, val loss: 0.7832152843475342\n",
      "Epoch 2120, training loss: 631.263427734375 = 0.4718891680240631 + 100 * 6.307915687561035\n",
      "Epoch 2120, val loss: 0.7811300754547119\n",
      "Epoch 2130, training loss: 631.141357421875 = 0.4665369987487793 + 100 * 6.306747913360596\n",
      "Epoch 2130, val loss: 0.7769303917884827\n",
      "Epoch 2140, training loss: 630.4819946289062 = 0.4611009657382965 + 100 * 6.300208568572998\n",
      "Epoch 2140, val loss: 0.7751643061637878\n",
      "Epoch 2150, training loss: 630.612548828125 = 0.4560965299606323 + 100 * 6.3015642166137695\n",
      "Epoch 2150, val loss: 0.7721186280250549\n",
      "Epoch 2160, training loss: 630.7410888671875 = 0.4510402977466583 + 100 * 6.302900314331055\n",
      "Epoch 2160, val loss: 0.7694660425186157\n",
      "Epoch 2170, training loss: 630.204833984375 = 0.44622504711151123 + 100 * 6.297585964202881\n",
      "Epoch 2170, val loss: 0.7670676112174988\n",
      "Epoch 2180, training loss: 630.5016479492188 = 0.4417493939399719 + 100 * 6.300598621368408\n",
      "Epoch 2180, val loss: 0.7645636200904846\n",
      "Epoch 2190, training loss: 630.7695922851562 = 0.436908096075058 + 100 * 6.3033270835876465\n",
      "Epoch 2190, val loss: 0.7624129056930542\n",
      "Epoch 2200, training loss: 630.4008178710938 = 0.4321056008338928 + 100 * 6.299686908721924\n",
      "Epoch 2200, val loss: 0.7602546215057373\n",
      "Epoch 2210, training loss: 631.33740234375 = 0.4272208511829376 + 100 * 6.309101581573486\n",
      "Epoch 2210, val loss: 0.7572842836380005\n",
      "Epoch 2220, training loss: 631.2730102539062 = 0.42165407538414 + 100 * 6.308513641357422\n",
      "Epoch 2220, val loss: 0.7549304962158203\n",
      "Epoch 2230, training loss: 630.3272705078125 = 0.41674500703811646 + 100 * 6.299105167388916\n",
      "Epoch 2230, val loss: 0.7513114213943481\n",
      "Epoch 2240, training loss: 630.0581665039062 = 0.4124331474304199 + 100 * 6.296457290649414\n",
      "Epoch 2240, val loss: 0.7500530481338501\n",
      "Epoch 2250, training loss: 630.0540161132812 = 0.4084163308143616 + 100 * 6.296456336975098\n",
      "Epoch 2250, val loss: 0.7477656006813049\n",
      "Epoch 2260, training loss: 630.3731689453125 = 0.4043682813644409 + 100 * 6.299688339233398\n",
      "Epoch 2260, val loss: 0.7459710240364075\n",
      "Epoch 2270, training loss: 630.9282836914062 = 0.39970865845680237 + 100 * 6.305285930633545\n",
      "Epoch 2270, val loss: 0.7433246374130249\n",
      "Epoch 2280, training loss: 630.0325317382812 = 0.39453524351119995 + 100 * 6.296380043029785\n",
      "Epoch 2280, val loss: 0.740880012512207\n",
      "Epoch 2290, training loss: 629.8385620117188 = 0.3903566300868988 + 100 * 6.2944817543029785\n",
      "Epoch 2290, val loss: 0.739225447177887\n",
      "Epoch 2300, training loss: 629.9544677734375 = 0.38641950488090515 + 100 * 6.295680522918701\n",
      "Epoch 2300, val loss: 0.7375795245170593\n",
      "Epoch 2310, training loss: 630.4550170898438 = 0.3823704123497009 + 100 * 6.300726413726807\n",
      "Epoch 2310, val loss: 0.7357914447784424\n",
      "Epoch 2320, training loss: 630.14453125 = 0.3780137002468109 + 100 * 6.297665119171143\n",
      "Epoch 2320, val loss: 0.7335779666900635\n",
      "Epoch 2330, training loss: 630.55029296875 = 0.3738064467906952 + 100 * 6.301764965057373\n",
      "Epoch 2330, val loss: 0.731187105178833\n",
      "Epoch 2340, training loss: 630.0791625976562 = 0.369441956281662 + 100 * 6.297097206115723\n",
      "Epoch 2340, val loss: 0.7298084497451782\n",
      "Epoch 2350, training loss: 630.09423828125 = 0.3654848635196686 + 100 * 6.297287464141846\n",
      "Epoch 2350, val loss: 0.7277457118034363\n",
      "Epoch 2360, training loss: 630.2367553710938 = 0.3614600896835327 + 100 * 6.298752784729004\n",
      "Epoch 2360, val loss: 0.7260940670967102\n",
      "Epoch 2370, training loss: 629.6039428710938 = 0.3577269911766052 + 100 * 6.292462348937988\n",
      "Epoch 2370, val loss: 0.7243407368659973\n",
      "Epoch 2380, training loss: 629.7972412109375 = 0.35422807931900024 + 100 * 6.294429779052734\n",
      "Epoch 2380, val loss: 0.7230535745620728\n",
      "Epoch 2390, training loss: 630.58837890625 = 0.3503853976726532 + 100 * 6.302379608154297\n",
      "Epoch 2390, val loss: 0.7215279936790466\n",
      "Epoch 2400, training loss: 630.5125732421875 = 0.34605562686920166 + 100 * 6.30166482925415\n",
      "Epoch 2400, val loss: 0.7187854647636414\n",
      "Epoch 2410, training loss: 629.9804077148438 = 0.3423096537590027 + 100 * 6.296380996704102\n",
      "Epoch 2410, val loss: 0.717039942741394\n",
      "Epoch 2420, training loss: 629.8992309570312 = 0.33850064873695374 + 100 * 6.295607089996338\n",
      "Epoch 2420, val loss: 0.7155290246009827\n",
      "Epoch 2430, training loss: 629.59619140625 = 0.3350525498390198 + 100 * 6.292611122131348\n",
      "Epoch 2430, val loss: 0.7143227458000183\n",
      "Epoch 2440, training loss: 629.8848266601562 = 0.3316541910171509 + 100 * 6.295531749725342\n",
      "Epoch 2440, val loss: 0.7131859660148621\n",
      "Epoch 2450, training loss: 629.6839599609375 = 0.3279438614845276 + 100 * 6.293560028076172\n",
      "Epoch 2450, val loss: 0.7122189998626709\n",
      "Epoch 2460, training loss: 630.4078979492188 = 0.32437917590141296 + 100 * 6.300835132598877\n",
      "Epoch 2460, val loss: 0.7109881639480591\n",
      "Epoch 2470, training loss: 630.1153564453125 = 0.3205583393573761 + 100 * 6.297947883605957\n",
      "Epoch 2470, val loss: 0.7079327702522278\n",
      "Epoch 2480, training loss: 629.5889282226562 = 0.3168953061103821 + 100 * 6.292720317840576\n",
      "Epoch 2480, val loss: 0.7074282169342041\n",
      "Epoch 2490, training loss: 629.4073486328125 = 0.3137368857860565 + 100 * 6.29093599319458\n",
      "Epoch 2490, val loss: 0.7060312628746033\n",
      "Epoch 2500, training loss: 630.3846435546875 = 0.31052812933921814 + 100 * 6.300740718841553\n",
      "Epoch 2500, val loss: 0.7049179077148438\n",
      "Epoch 2510, training loss: 629.484375 = 0.3069671094417572 + 100 * 6.291774272918701\n",
      "Epoch 2510, val loss: 0.7035866379737854\n",
      "Epoch 2520, training loss: 629.5038452148438 = 0.3037061095237732 + 100 * 6.292001247406006\n",
      "Epoch 2520, val loss: 0.7025734782218933\n",
      "Epoch 2530, training loss: 629.406982421875 = 0.30048584938049316 + 100 * 6.291065216064453\n",
      "Epoch 2530, val loss: 0.7010205984115601\n",
      "Epoch 2540, training loss: 629.78369140625 = 0.29735395312309265 + 100 * 6.294863224029541\n",
      "Epoch 2540, val loss: 0.7001206278800964\n",
      "Epoch 2550, training loss: 629.96337890625 = 0.2940676510334015 + 100 * 6.296693325042725\n",
      "Epoch 2550, val loss: 0.6985719203948975\n",
      "Epoch 2560, training loss: 629.3872680664062 = 0.29073259234428406 + 100 * 6.290965557098389\n",
      "Epoch 2560, val loss: 0.6977511048316956\n",
      "Epoch 2570, training loss: 629.1923828125 = 0.2877787947654724 + 100 * 6.289046287536621\n",
      "Epoch 2570, val loss: 0.6966949105262756\n",
      "Epoch 2580, training loss: 629.318359375 = 0.28503096103668213 + 100 * 6.290333271026611\n",
      "Epoch 2580, val loss: 0.6955253481864929\n",
      "Epoch 2590, training loss: 629.6066284179688 = 0.2819403111934662 + 100 * 6.293247222900391\n",
      "Epoch 2590, val loss: 0.6948952078819275\n",
      "Epoch 2600, training loss: 629.4666748046875 = 0.2788674831390381 + 100 * 6.2918782234191895\n",
      "Epoch 2600, val loss: 0.6935369372367859\n",
      "Epoch 2610, training loss: 630.5621337890625 = 0.2759768068790436 + 100 * 6.302861213684082\n",
      "Epoch 2610, val loss: 0.6923421621322632\n",
      "Epoch 2620, training loss: 629.293212890625 = 0.2726840078830719 + 100 * 6.290205001831055\n",
      "Epoch 2620, val loss: 0.6913465261459351\n",
      "Epoch 2630, training loss: 629.0469360351562 = 0.26997828483581543 + 100 * 6.287769794464111\n",
      "Epoch 2630, val loss: 0.6909751892089844\n",
      "Epoch 2640, training loss: 630.696533203125 = 0.2671220600605011 + 100 * 6.304294109344482\n",
      "Epoch 2640, val loss: 0.6898223161697388\n",
      "Epoch 2650, training loss: 629.3977661132812 = 0.2642361521720886 + 100 * 6.291335582733154\n",
      "Epoch 2650, val loss: 0.6890489459037781\n",
      "Epoch 2660, training loss: 629.3369140625 = 0.26136356592178345 + 100 * 6.290755748748779\n",
      "Epoch 2660, val loss: 0.6880923509597778\n",
      "Epoch 2670, training loss: 629.2633056640625 = 0.25868770480155945 + 100 * 6.290046215057373\n",
      "Epoch 2670, val loss: 0.687547504901886\n",
      "Epoch 2680, training loss: 628.855712890625 = 0.2560514509677887 + 100 * 6.285996437072754\n",
      "Epoch 2680, val loss: 0.6867875456809998\n",
      "Epoch 2690, training loss: 629.072021484375 = 0.2536483407020569 + 100 * 6.288183689117432\n",
      "Epoch 2690, val loss: 0.6865599751472473\n",
      "Epoch 2700, training loss: 629.45947265625 = 0.25095856189727783 + 100 * 6.29208517074585\n",
      "Epoch 2700, val loss: 0.6852877736091614\n",
      "Epoch 2710, training loss: 629.352783203125 = 0.24835063517093658 + 100 * 6.291044235229492\n",
      "Epoch 2710, val loss: 0.6846192479133606\n",
      "Epoch 2720, training loss: 629.6380004882812 = 0.24567683041095734 + 100 * 6.293923377990723\n",
      "Epoch 2720, val loss: 0.6841881275177002\n",
      "Epoch 2730, training loss: 629.6954956054688 = 0.24295903742313385 + 100 * 6.294525146484375\n",
      "Epoch 2730, val loss: 0.6841021180152893\n",
      "Epoch 2740, training loss: 628.8710327148438 = 0.2401086688041687 + 100 * 6.286309242248535\n",
      "Epoch 2740, val loss: 0.6827542185783386\n",
      "Epoch 2750, training loss: 628.811767578125 = 0.2378517985343933 + 100 * 6.285738945007324\n",
      "Epoch 2750, val loss: 0.6820760369300842\n",
      "Epoch 2760, training loss: 629.2994995117188 = 0.23556195199489594 + 100 * 6.290639877319336\n",
      "Epoch 2760, val loss: 0.6821706891059875\n",
      "Epoch 2770, training loss: 628.8972778320312 = 0.23291805386543274 + 100 * 6.2866435050964355\n",
      "Epoch 2770, val loss: 0.6809643507003784\n",
      "Epoch 2780, training loss: 628.9650268554688 = 0.23060089349746704 + 100 * 6.287344455718994\n",
      "Epoch 2780, val loss: 0.6811003088951111\n",
      "Epoch 2790, training loss: 628.9325561523438 = 0.22837872803211212 + 100 * 6.287041664123535\n",
      "Epoch 2790, val loss: 0.6805317997932434\n",
      "Epoch 2800, training loss: 628.7759399414062 = 0.22604645788669586 + 100 * 6.28549861907959\n",
      "Epoch 2800, val loss: 0.6799343824386597\n",
      "Epoch 2810, training loss: 628.8202514648438 = 0.223848357796669 + 100 * 6.285964012145996\n",
      "Epoch 2810, val loss: 0.6795929670333862\n",
      "Epoch 2820, training loss: 629.672119140625 = 0.22154390811920166 + 100 * 6.294505596160889\n",
      "Epoch 2820, val loss: 0.6791815161705017\n",
      "Epoch 2830, training loss: 628.7760009765625 = 0.219023659825325 + 100 * 6.28557014465332\n",
      "Epoch 2830, val loss: 0.6778351664543152\n",
      "Epoch 2840, training loss: 628.8939819335938 = 0.21693558990955353 + 100 * 6.286770343780518\n",
      "Epoch 2840, val loss: 0.6784178614616394\n",
      "Epoch 2850, training loss: 629.1780395507812 = 0.21476882696151733 + 100 * 6.289632320404053\n",
      "Epoch 2850, val loss: 0.6773631572723389\n",
      "Epoch 2860, training loss: 628.859130859375 = 0.21250388026237488 + 100 * 6.286466121673584\n",
      "Epoch 2860, val loss: 0.6766558885574341\n",
      "Epoch 2870, training loss: 628.5841674804688 = 0.21033763885498047 + 100 * 6.283738136291504\n",
      "Epoch 2870, val loss: 0.6764751672744751\n",
      "Epoch 2880, training loss: 628.4343872070312 = 0.20830939710140228 + 100 * 6.282260894775391\n",
      "Epoch 2880, val loss: 0.6764392852783203\n",
      "Epoch 2890, training loss: 629.337158203125 = 0.2062813639640808 + 100 * 6.291308403015137\n",
      "Epoch 2890, val loss: 0.6762822270393372\n",
      "Epoch 2900, training loss: 628.6229858398438 = 0.20401962101459503 + 100 * 6.284189701080322\n",
      "Epoch 2900, val loss: 0.675738513469696\n",
      "Epoch 2910, training loss: 628.4798583984375 = 0.20195452868938446 + 100 * 6.282778739929199\n",
      "Epoch 2910, val loss: 0.6755423545837402\n",
      "Epoch 2920, training loss: 628.4668579101562 = 0.20009715855121613 + 100 * 6.282667636871338\n",
      "Epoch 2920, val loss: 0.6754404306411743\n",
      "Epoch 2930, training loss: 628.8436889648438 = 0.1982312947511673 + 100 * 6.286454200744629\n",
      "Epoch 2930, val loss: 0.6749503016471863\n",
      "Epoch 2940, training loss: 628.7401733398438 = 0.19613252580165863 + 100 * 6.285440444946289\n",
      "Epoch 2940, val loss: 0.6749193668365479\n",
      "Epoch 2950, training loss: 628.4923706054688 = 0.1940668821334839 + 100 * 6.28298282623291\n",
      "Epoch 2950, val loss: 0.6747071146965027\n",
      "Epoch 2960, training loss: 628.228271484375 = 0.1921384334564209 + 100 * 6.280361175537109\n",
      "Epoch 2960, val loss: 0.6742942333221436\n",
      "Epoch 2970, training loss: 628.4026489257812 = 0.19039291143417358 + 100 * 6.282122611999512\n",
      "Epoch 2970, val loss: 0.6746930480003357\n",
      "Epoch 2980, training loss: 629.1138916015625 = 0.18853990733623505 + 100 * 6.2892537117004395\n",
      "Epoch 2980, val loss: 0.6748339533805847\n",
      "Epoch 2990, training loss: 628.7763061523438 = 0.18645933270454407 + 100 * 6.285898208618164\n",
      "Epoch 2990, val loss: 0.6740779280662537\n",
      "Epoch 3000, training loss: 628.534912109375 = 0.18443088233470917 + 100 * 6.283504486083984\n",
      "Epoch 3000, val loss: 0.6739413738250732\n",
      "Epoch 3010, training loss: 628.2041015625 = 0.1827096790075302 + 100 * 6.280213356018066\n",
      "Epoch 3010, val loss: 0.6735430955886841\n",
      "Epoch 3020, training loss: 628.1210327148438 = 0.1810777336359024 + 100 * 6.279399394989014\n",
      "Epoch 3020, val loss: 0.6735039949417114\n",
      "Epoch 3030, training loss: 628.32080078125 = 0.17949321866035461 + 100 * 6.2814130783081055\n",
      "Epoch 3030, val loss: 0.6732748746871948\n",
      "Epoch 3040, training loss: 628.416748046875 = 0.17778722941875458 + 100 * 6.2823896408081055\n",
      "Epoch 3040, val loss: 0.6731717586517334\n",
      "Epoch 3050, training loss: 628.9185180664062 = 0.1760040521621704 + 100 * 6.2874250411987305\n",
      "Epoch 3050, val loss: 0.6730315089225769\n",
      "Epoch 3060, training loss: 628.17529296875 = 0.17411395907402039 + 100 * 6.2800116539001465\n",
      "Epoch 3060, val loss: 0.673179030418396\n",
      "Epoch 3070, training loss: 627.891357421875 = 0.17254434525966644 + 100 * 6.277187824249268\n",
      "Epoch 3070, val loss: 0.6735274195671082\n",
      "Epoch 3080, training loss: 628.78564453125 = 0.17107023298740387 + 100 * 6.2861456871032715\n",
      "Epoch 3080, val loss: 0.6736937165260315\n",
      "Epoch 3090, training loss: 628.37548828125 = 0.16925126314163208 + 100 * 6.282062530517578\n",
      "Epoch 3090, val loss: 0.6724599599838257\n",
      "Epoch 3100, training loss: 628.2244873046875 = 0.16752870380878448 + 100 * 6.280569553375244\n",
      "Epoch 3100, val loss: 0.6730139255523682\n",
      "Epoch 3110, training loss: 628.3812866210938 = 0.1659630537033081 + 100 * 6.282153606414795\n",
      "Epoch 3110, val loss: 0.6733251214027405\n",
      "Epoch 3120, training loss: 628.0718994140625 = 0.16432490944862366 + 100 * 6.279076099395752\n",
      "Epoch 3120, val loss: 0.6731512546539307\n",
      "Epoch 3130, training loss: 628.388671875 = 0.16285088658332825 + 100 * 6.282258033752441\n",
      "Epoch 3130, val loss: 0.6729713082313538\n",
      "Epoch 3140, training loss: 627.6925659179688 = 0.16126638650894165 + 100 * 6.275313377380371\n",
      "Epoch 3140, val loss: 0.672792375087738\n",
      "Epoch 3150, training loss: 627.9152221679688 = 0.15983884036540985 + 100 * 6.277553558349609\n",
      "Epoch 3150, val loss: 0.6729345917701721\n",
      "Epoch 3160, training loss: 628.1997680664062 = 0.15847887098789215 + 100 * 6.280412673950195\n",
      "Epoch 3160, val loss: 0.6729051470756531\n",
      "Epoch 3170, training loss: 627.9942626953125 = 0.15687718987464905 + 100 * 6.278373718261719\n",
      "Epoch 3170, val loss: 0.6733125448226929\n",
      "Epoch 3180, training loss: 628.31005859375 = 0.15523701906204224 + 100 * 6.281548500061035\n",
      "Epoch 3180, val loss: 0.6734150052070618\n",
      "Epoch 3190, training loss: 628.1077880859375 = 0.1537204384803772 + 100 * 6.279540538787842\n",
      "Epoch 3190, val loss: 0.6728717088699341\n",
      "Epoch 3200, training loss: 627.7537841796875 = 0.1522969752550125 + 100 * 6.276015281677246\n",
      "Epoch 3200, val loss: 0.6733894348144531\n",
      "Epoch 3210, training loss: 627.759765625 = 0.1510566920042038 + 100 * 6.276087284088135\n",
      "Epoch 3210, val loss: 0.6731642484664917\n",
      "Epoch 3220, training loss: 628.283203125 = 0.14971840381622314 + 100 * 6.28133487701416\n",
      "Epoch 3220, val loss: 0.6731227040290833\n",
      "Epoch 3230, training loss: 627.9930419921875 = 0.148296058177948 + 100 * 6.278447151184082\n",
      "Epoch 3230, val loss: 0.6736869812011719\n",
      "Epoch 3240, training loss: 627.535888671875 = 0.14688043296337128 + 100 * 6.273890495300293\n",
      "Epoch 3240, val loss: 0.6736900210380554\n",
      "Epoch 3250, training loss: 628.4406127929688 = 0.1454668790102005 + 100 * 6.282951831817627\n",
      "Epoch 3250, val loss: 0.6741712689399719\n",
      "Epoch 3260, training loss: 627.912109375 = 0.14397288858890533 + 100 * 6.277681350708008\n",
      "Epoch 3260, val loss: 0.6736379265785217\n",
      "Epoch 3270, training loss: 627.396484375 = 0.14261063933372498 + 100 * 6.272538661956787\n",
      "Epoch 3270, val loss: 0.6741257309913635\n",
      "Epoch 3280, training loss: 627.7184448242188 = 0.14145883917808533 + 100 * 6.2757697105407715\n",
      "Epoch 3280, val loss: 0.6746756434440613\n",
      "Epoch 3290, training loss: 628.0296020507812 = 0.14010989665985107 + 100 * 6.278894424438477\n",
      "Epoch 3290, val loss: 0.6743418574333191\n",
      "Epoch 3300, training loss: 627.7163696289062 = 0.13886475563049316 + 100 * 6.27577543258667\n",
      "Epoch 3300, val loss: 0.6748241186141968\n",
      "Epoch 3310, training loss: 627.881103515625 = 0.1375313401222229 + 100 * 6.277435779571533\n",
      "Epoch 3310, val loss: 0.6749785542488098\n",
      "Epoch 3320, training loss: 627.4119262695312 = 0.13628500699996948 + 100 * 6.272756576538086\n",
      "Epoch 3320, val loss: 0.674940288066864\n",
      "Epoch 3330, training loss: 628.1168212890625 = 0.13506823778152466 + 100 * 6.279817581176758\n",
      "Epoch 3330, val loss: 0.6753868460655212\n",
      "Epoch 3340, training loss: 627.3524780273438 = 0.1337490826845169 + 100 * 6.272187232971191\n",
      "Epoch 3340, val loss: 0.6753713488578796\n",
      "Epoch 3350, training loss: 627.5471801757812 = 0.13267824053764343 + 100 * 6.274145126342773\n",
      "Epoch 3350, val loss: 0.6759616136550903\n",
      "Epoch 3360, training loss: 627.6305541992188 = 0.13146445155143738 + 100 * 6.274991035461426\n",
      "Epoch 3360, val loss: 0.6756043434143066\n",
      "Epoch 3370, training loss: 627.5147094726562 = 0.1303115040063858 + 100 * 6.273844242095947\n",
      "Epoch 3370, val loss: 0.6756405234336853\n",
      "Epoch 3380, training loss: 628.0298461914062 = 0.12926286458969116 + 100 * 6.279005527496338\n",
      "Epoch 3380, val loss: 0.6757970452308655\n",
      "Epoch 3390, training loss: 627.6149291992188 = 0.12792791426181793 + 100 * 6.274869918823242\n",
      "Epoch 3390, val loss: 0.6761776804924011\n",
      "Epoch 3400, training loss: 627.383544921875 = 0.12678983807563782 + 100 * 6.2725677490234375\n",
      "Epoch 3400, val loss: 0.6766522526741028\n",
      "Epoch 3410, training loss: 627.7203369140625 = 0.12558475136756897 + 100 * 6.275947093963623\n",
      "Epoch 3410, val loss: 0.6774073839187622\n",
      "Epoch 3420, training loss: 627.3627319335938 = 0.12448392063379288 + 100 * 6.2723822593688965\n",
      "Epoch 3420, val loss: 0.6773350238800049\n",
      "Epoch 3430, training loss: 627.8314819335938 = 0.12326616793870926 + 100 * 6.2770819664001465\n",
      "Epoch 3430, val loss: 0.677406907081604\n",
      "Epoch 3440, training loss: 627.198486328125 = 0.12209661304950714 + 100 * 6.270764350891113\n",
      "Epoch 3440, val loss: 0.6776586771011353\n",
      "Epoch 3450, training loss: 627.0512084960938 = 0.12110637128353119 + 100 * 6.269300937652588\n",
      "Epoch 3450, val loss: 0.6782561540603638\n",
      "Epoch 3460, training loss: 626.9583129882812 = 0.1201406940817833 + 100 * 6.2683820724487305\n",
      "Epoch 3460, val loss: 0.6788159012794495\n",
      "Epoch 3470, training loss: 628.31494140625 = 0.11915819346904755 + 100 * 6.281958103179932\n",
      "Epoch 3470, val loss: 0.6795589923858643\n",
      "Epoch 3480, training loss: 627.5530395507812 = 0.11788751184940338 + 100 * 6.274352073669434\n",
      "Epoch 3480, val loss: 0.6786425113677979\n",
      "Epoch 3490, training loss: 627.2462158203125 = 0.11680876463651657 + 100 * 6.271293640136719\n",
      "Epoch 3490, val loss: 0.6791236996650696\n",
      "=== picking the best model according to the performance on validation ===\n",
      "Accuracy of GNN+CL: 0.7925925925925926\n",
      "0.804955192409067\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 861.6220703125 = 1.9304183721542358 + 100 * 8.596916198730469\n",
      "Epoch 0, val loss: 1.9288355112075806\n",
      "Epoch 10, training loss: 861.5985107421875 = 1.9212719202041626 + 100 * 8.596772193908691\n",
      "Epoch 10, val loss: 1.9198237657546997\n",
      "Epoch 20, training loss: 861.4055786132812 = 1.9108541011810303 + 100 * 8.594947814941406\n",
      "Epoch 20, val loss: 1.909464716911316\n",
      "Epoch 30, training loss: 859.1173095703125 = 1.8985615968704224 + 100 * 8.572187423706055\n",
      "Epoch 30, val loss: 1.897339105606079\n",
      "Epoch 40, training loss: 840.314208984375 = 1.8828845024108887 + 100 * 8.384313583374023\n",
      "Epoch 40, val loss: 1.88231360912323\n",
      "Epoch 50, training loss: 806.8488159179688 = 1.8677678108215332 + 100 * 8.049810409545898\n",
      "Epoch 50, val loss: 1.8692106008529663\n",
      "Epoch 60, training loss: 766.5907592773438 = 1.8579522371292114 + 100 * 7.647327899932861\n",
      "Epoch 60, val loss: 1.8595877885818481\n",
      "Epoch 70, training loss: 740.9327392578125 = 1.8493547439575195 + 100 * 7.390833854675293\n",
      "Epoch 70, val loss: 1.850414752960205\n",
      "Epoch 80, training loss: 720.5531005859375 = 1.8406239748001099 + 100 * 7.187124729156494\n",
      "Epoch 80, val loss: 1.8417739868164062\n",
      "Epoch 90, training loss: 707.7619018554688 = 1.833280324935913 + 100 * 7.059286594390869\n",
      "Epoch 90, val loss: 1.8340493440628052\n",
      "Epoch 100, training loss: 701.3106079101562 = 1.8275474309921265 + 100 * 6.994830131530762\n",
      "Epoch 100, val loss: 1.8277733325958252\n",
      "Epoch 110, training loss: 697.1066284179688 = 1.822799801826477 + 100 * 6.952837944030762\n",
      "Epoch 110, val loss: 1.8224226236343384\n",
      "Epoch 120, training loss: 693.9451904296875 = 1.8183401823043823 + 100 * 6.921268463134766\n",
      "Epoch 120, val loss: 1.817363977432251\n",
      "Epoch 130, training loss: 691.5091552734375 = 1.8146716356277466 + 100 * 6.896944522857666\n",
      "Epoch 130, val loss: 1.813037633895874\n",
      "Epoch 140, training loss: 689.1307373046875 = 1.8115379810333252 + 100 * 6.873192310333252\n",
      "Epoch 140, val loss: 1.8092224597930908\n",
      "Epoch 150, training loss: 686.673095703125 = 1.808502197265625 + 100 * 6.8486456871032715\n",
      "Epoch 150, val loss: 1.8057078123092651\n",
      "Epoch 160, training loss: 683.3568115234375 = 1.8059498071670532 + 100 * 6.8155083656311035\n",
      "Epoch 160, val loss: 1.8027058839797974\n",
      "Epoch 170, training loss: 680.1793823242188 = 1.8038835525512695 + 100 * 6.783754825592041\n",
      "Epoch 170, val loss: 1.8000084161758423\n",
      "Epoch 180, training loss: 678.6231079101562 = 1.8012222051620483 + 100 * 6.768218994140625\n",
      "Epoch 180, val loss: 1.7968143224716187\n",
      "Epoch 190, training loss: 677.4063720703125 = 1.7981197834014893 + 100 * 6.756083011627197\n",
      "Epoch 190, val loss: 1.7933751344680786\n",
      "Epoch 200, training loss: 676.5079956054688 = 1.7954808473587036 + 100 * 6.747125148773193\n",
      "Epoch 200, val loss: 1.7903622388839722\n",
      "Epoch 210, training loss: 675.6895751953125 = 1.7930325269699097 + 100 * 6.7389655113220215\n",
      "Epoch 210, val loss: 1.787613034248352\n",
      "Epoch 220, training loss: 675.1493530273438 = 1.7903265953063965 + 100 * 6.733590126037598\n",
      "Epoch 220, val loss: 1.7847083806991577\n",
      "Epoch 230, training loss: 674.8067016601562 = 1.7876386642456055 + 100 * 6.730190277099609\n",
      "Epoch 230, val loss: 1.7819055318832397\n",
      "Epoch 240, training loss: 673.7555541992188 = 1.7849634885787964 + 100 * 6.719706058502197\n",
      "Epoch 240, val loss: 1.7791184186935425\n",
      "Epoch 250, training loss: 673.0485229492188 = 1.7824312448501587 + 100 * 6.712661266326904\n",
      "Epoch 250, val loss: 1.7764616012573242\n",
      "Epoch 260, training loss: 672.8302612304688 = 1.7799623012542725 + 100 * 6.710503101348877\n",
      "Epoch 260, val loss: 1.7739628553390503\n",
      "Epoch 270, training loss: 672.1116943359375 = 1.777103304862976 + 100 * 6.703346252441406\n",
      "Epoch 270, val loss: 1.7710628509521484\n",
      "Epoch 280, training loss: 670.9320068359375 = 1.7744803428649902 + 100 * 6.691575050354004\n",
      "Epoch 280, val loss: 1.7684078216552734\n",
      "Epoch 290, training loss: 669.72607421875 = 1.7722450494766235 + 100 * 6.679538726806641\n",
      "Epoch 290, val loss: 1.7663264274597168\n",
      "Epoch 300, training loss: 670.1383056640625 = 1.7701168060302734 + 100 * 6.683681488037109\n",
      "Epoch 300, val loss: 1.7643378973007202\n",
      "Epoch 310, training loss: 667.5535888671875 = 1.767179012298584 + 100 * 6.657864093780518\n",
      "Epoch 310, val loss: 1.7615565061569214\n",
      "Epoch 320, training loss: 665.9658813476562 = 1.7647753953933716 + 100 * 6.6420111656188965\n",
      "Epoch 320, val loss: 1.7594598531723022\n",
      "Epoch 330, training loss: 666.348876953125 = 1.7621877193450928 + 100 * 6.645866870880127\n",
      "Epoch 330, val loss: 1.7570644617080688\n",
      "Epoch 340, training loss: 664.40966796875 = 1.7587870359420776 + 100 * 6.626508712768555\n",
      "Epoch 340, val loss: 1.7539455890655518\n",
      "Epoch 350, training loss: 663.3660888671875 = 1.7557923793792725 + 100 * 6.616102695465088\n",
      "Epoch 350, val loss: 1.751345157623291\n",
      "Epoch 360, training loss: 662.7843017578125 = 1.7525062561035156 + 100 * 6.610317707061768\n",
      "Epoch 360, val loss: 1.7483094930648804\n",
      "Epoch 370, training loss: 662.0950927734375 = 1.7490055561065674 + 100 * 6.603460788726807\n",
      "Epoch 370, val loss: 1.7453181743621826\n",
      "Epoch 380, training loss: 661.0557861328125 = 1.7458603382110596 + 100 * 6.593099117279053\n",
      "Epoch 380, val loss: 1.742647409439087\n",
      "Epoch 390, training loss: 660.4207763671875 = 1.7425789833068848 + 100 * 6.586782455444336\n",
      "Epoch 390, val loss: 1.739891529083252\n",
      "Epoch 400, training loss: 660.2965087890625 = 1.7385854721069336 + 100 * 6.5855793952941895\n",
      "Epoch 400, val loss: 1.7364007234573364\n",
      "Epoch 410, training loss: 658.93896484375 = 1.7342219352722168 + 100 * 6.572047710418701\n",
      "Epoch 410, val loss: 1.7326644659042358\n",
      "Epoch 420, training loss: 658.258544921875 = 1.7307097911834717 + 100 * 6.565278053283691\n",
      "Epoch 420, val loss: 1.7296137809753418\n",
      "Epoch 430, training loss: 659.8112182617188 = 1.7267978191375732 + 100 * 6.580843925476074\n",
      "Epoch 430, val loss: 1.726238489151001\n",
      "Epoch 440, training loss: 657.4739990234375 = 1.7215821743011475 + 100 * 6.557524681091309\n",
      "Epoch 440, val loss: 1.721669316291809\n",
      "Epoch 450, training loss: 656.3735961914062 = 1.71770179271698 + 100 * 6.546558856964111\n",
      "Epoch 450, val loss: 1.7182974815368652\n",
      "Epoch 460, training loss: 655.4236450195312 = 1.7140835523605347 + 100 * 6.537095546722412\n",
      "Epoch 460, val loss: 1.7151620388031006\n",
      "Epoch 470, training loss: 654.6210327148438 = 1.7096152305603027 + 100 * 6.529114246368408\n",
      "Epoch 470, val loss: 1.7111974954605103\n",
      "Epoch 480, training loss: 654.114990234375 = 1.7050673961639404 + 100 * 6.524099349975586\n",
      "Epoch 480, val loss: 1.7071783542633057\n",
      "Epoch 490, training loss: 652.931396484375 = 1.701121211051941 + 100 * 6.512302875518799\n",
      "Epoch 490, val loss: 1.7036423683166504\n",
      "Epoch 500, training loss: 653.1309204101562 = 1.6960293054580688 + 100 * 6.51434850692749\n",
      "Epoch 500, val loss: 1.6992506980895996\n",
      "Epoch 510, training loss: 651.948486328125 = 1.6903282403945923 + 100 * 6.50258207321167\n",
      "Epoch 510, val loss: 1.694070816040039\n",
      "Epoch 520, training loss: 651.4319458007812 = 1.68569815158844 + 100 * 6.497462749481201\n",
      "Epoch 520, val loss: 1.6898462772369385\n",
      "Epoch 530, training loss: 651.0363159179688 = 1.680899739265442 + 100 * 6.49355411529541\n",
      "Epoch 530, val loss: 1.6856293678283691\n",
      "Epoch 540, training loss: 651.0225830078125 = 1.6753724813461304 + 100 * 6.493472576141357\n",
      "Epoch 540, val loss: 1.6807425022125244\n",
      "Epoch 550, training loss: 650.127685546875 = 1.6698938608169556 + 100 * 6.484577655792236\n",
      "Epoch 550, val loss: 1.6759059429168701\n",
      "Epoch 560, training loss: 652.646728515625 = 1.6645623445510864 + 100 * 6.509821891784668\n",
      "Epoch 560, val loss: 1.6708858013153076\n",
      "Epoch 570, training loss: 649.956298828125 = 1.657523274421692 + 100 * 6.482987403869629\n",
      "Epoch 570, val loss: 1.664849877357483\n",
      "Epoch 580, training loss: 649.2412719726562 = 1.6520065069198608 + 100 * 6.475893020629883\n",
      "Epoch 580, val loss: 1.6601285934448242\n",
      "Epoch 590, training loss: 648.8265380859375 = 1.6464694738388062 + 100 * 6.471800804138184\n",
      "Epoch 590, val loss: 1.655513048171997\n",
      "Epoch 600, training loss: 651.528076171875 = 1.6406660079956055 + 100 * 6.498874187469482\n",
      "Epoch 600, val loss: 1.6503461599349976\n",
      "Epoch 610, training loss: 648.6981811523438 = 1.6327452659606934 + 100 * 6.470654487609863\n",
      "Epoch 610, val loss: 1.6434471607208252\n",
      "Epoch 620, training loss: 648.24951171875 = 1.626808524131775 + 100 * 6.466227054595947\n",
      "Epoch 620, val loss: 1.6383687257766724\n",
      "Epoch 630, training loss: 647.7042846679688 = 1.621016263961792 + 100 * 6.460832595825195\n",
      "Epoch 630, val loss: 1.6335211992263794\n",
      "Epoch 640, training loss: 649.9283447265625 = 1.6147452592849731 + 100 * 6.48313570022583\n",
      "Epoch 640, val loss: 1.628255009651184\n",
      "Epoch 650, training loss: 647.130859375 = 1.6064409017562866 + 100 * 6.455244064331055\n",
      "Epoch 650, val loss: 1.6210224628448486\n",
      "Epoch 660, training loss: 646.8120727539062 = 1.6001849174499512 + 100 * 6.45211935043335\n",
      "Epoch 660, val loss: 1.6155961751937866\n",
      "Epoch 670, training loss: 646.5191650390625 = 1.5939383506774902 + 100 * 6.449252605438232\n",
      "Epoch 670, val loss: 1.6103216409683228\n",
      "Epoch 680, training loss: 646.6381225585938 = 1.587328314781189 + 100 * 6.450508117675781\n",
      "Epoch 680, val loss: 1.6047368049621582\n",
      "Epoch 690, training loss: 646.5458374023438 = 1.5791958570480347 + 100 * 6.4496660232543945\n",
      "Epoch 690, val loss: 1.5978257656097412\n",
      "Epoch 700, training loss: 645.961669921875 = 1.5715153217315674 + 100 * 6.443901538848877\n",
      "Epoch 700, val loss: 1.5910084247589111\n",
      "Epoch 710, training loss: 645.6096801757812 = 1.5645899772644043 + 100 * 6.440451145172119\n",
      "Epoch 710, val loss: 1.5852186679840088\n",
      "Epoch 720, training loss: 645.4028930664062 = 1.5575575828552246 + 100 * 6.438453197479248\n",
      "Epoch 720, val loss: 1.5793603658676147\n",
      "Epoch 730, training loss: 646.5628051757812 = 1.5488396883010864 + 100 * 6.45013952255249\n",
      "Epoch 730, val loss: 1.5713690519332886\n",
      "Epoch 740, training loss: 645.5147094726562 = 1.5391182899475098 + 100 * 6.439755916595459\n",
      "Epoch 740, val loss: 1.5631935596466064\n",
      "Epoch 750, training loss: 644.7412109375 = 1.5316623449325562 + 100 * 6.432095527648926\n",
      "Epoch 750, val loss: 1.5569448471069336\n",
      "Epoch 760, training loss: 644.8975830078125 = 1.5241378545761108 + 100 * 6.433734893798828\n",
      "Epoch 760, val loss: 1.5507575273513794\n",
      "Epoch 770, training loss: 644.4232177734375 = 1.514784574508667 + 100 * 6.429084300994873\n",
      "Epoch 770, val loss: 1.54239022731781\n",
      "Epoch 780, training loss: 644.49609375 = 1.5065727233886719 + 100 * 6.429894924163818\n",
      "Epoch 780, val loss: 1.5355732440948486\n",
      "Epoch 790, training loss: 643.91357421875 = 1.498865008354187 + 100 * 6.424147605895996\n",
      "Epoch 790, val loss: 1.5290281772613525\n",
      "Epoch 800, training loss: 644.5504760742188 = 1.4907164573669434 + 100 * 6.43059778213501\n",
      "Epoch 800, val loss: 1.5225883722305298\n",
      "Epoch 810, training loss: 644.355712890625 = 1.4808350801467896 + 100 * 6.428749084472656\n",
      "Epoch 810, val loss: 1.5132455825805664\n",
      "Epoch 820, training loss: 643.6326293945312 = 1.472193956375122 + 100 * 6.421604633331299\n",
      "Epoch 820, val loss: 1.505928874015808\n",
      "Epoch 830, training loss: 643.2869873046875 = 1.4642040729522705 + 100 * 6.418227672576904\n",
      "Epoch 830, val loss: 1.4994480609893799\n",
      "Epoch 840, training loss: 643.8721923828125 = 1.4547268152236938 + 100 * 6.424174785614014\n",
      "Epoch 840, val loss: 1.491224765777588\n",
      "Epoch 850, training loss: 643.4473266601562 = 1.4445958137512207 + 100 * 6.420027256011963\n",
      "Epoch 850, val loss: 1.4825139045715332\n",
      "Epoch 860, training loss: 642.705322265625 = 1.436120629310608 + 100 * 6.412692070007324\n",
      "Epoch 860, val loss: 1.4754791259765625\n",
      "Epoch 870, training loss: 642.5520629882812 = 1.4275245666503906 + 100 * 6.411245346069336\n",
      "Epoch 870, val loss: 1.4683994054794312\n",
      "Epoch 880, training loss: 642.9606323242188 = 1.418829083442688 + 100 * 6.4154181480407715\n",
      "Epoch 880, val loss: 1.4608014822006226\n",
      "Epoch 890, training loss: 642.0327758789062 = 1.408757209777832 + 100 * 6.406240463256836\n",
      "Epoch 890, val loss: 1.4524368047714233\n",
      "Epoch 900, training loss: 642.390625 = 1.3996888399124146 + 100 * 6.409908771514893\n",
      "Epoch 900, val loss: 1.4449162483215332\n",
      "Epoch 910, training loss: 642.0388793945312 = 1.3899213075637817 + 100 * 6.406489849090576\n",
      "Epoch 910, val loss: 1.4365224838256836\n",
      "Epoch 920, training loss: 641.7625732421875 = 1.3806647062301636 + 100 * 6.4038190841674805\n",
      "Epoch 920, val loss: 1.428676724433899\n",
      "Epoch 930, training loss: 641.6260375976562 = 1.3719538450241089 + 100 * 6.402541160583496\n",
      "Epoch 930, val loss: 1.4215906858444214\n",
      "Epoch 940, training loss: 642.6446533203125 = 1.3622030019760132 + 100 * 6.412824630737305\n",
      "Epoch 940, val loss: 1.4128689765930176\n",
      "Epoch 950, training loss: 641.1618041992188 = 1.3513118028640747 + 100 * 6.398104667663574\n",
      "Epoch 950, val loss: 1.4041869640350342\n",
      "Epoch 960, training loss: 641.0703735351562 = 1.3422929048538208 + 100 * 6.397280693054199\n",
      "Epoch 960, val loss: 1.396384596824646\n",
      "Epoch 970, training loss: 642.5818481445312 = 1.3325779438018799 + 100 * 6.412492752075195\n",
      "Epoch 970, val loss: 1.3881182670593262\n",
      "Epoch 980, training loss: 640.6655883789062 = 1.3215265274047852 + 100 * 6.3934407234191895\n",
      "Epoch 980, val loss: 1.3790512084960938\n",
      "Epoch 990, training loss: 640.41015625 = 1.3125747442245483 + 100 * 6.3909759521484375\n",
      "Epoch 990, val loss: 1.371886134147644\n",
      "Epoch 1000, training loss: 640.295166015625 = 1.3035486936569214 + 100 * 6.38991641998291\n",
      "Epoch 1000, val loss: 1.364585518836975\n",
      "Epoch 1010, training loss: 642.3413696289062 = 1.2931565046310425 + 100 * 6.410481929779053\n",
      "Epoch 1010, val loss: 1.3553061485290527\n",
      "Epoch 1020, training loss: 640.5877075195312 = 1.2816423177719116 + 100 * 6.39306116104126\n",
      "Epoch 1020, val loss: 1.3460662364959717\n",
      "Epoch 1030, training loss: 639.7760620117188 = 1.272392749786377 + 100 * 6.385036468505859\n",
      "Epoch 1030, val loss: 1.338477611541748\n",
      "Epoch 1040, training loss: 639.755126953125 = 1.263417363166809 + 100 * 6.38491678237915\n",
      "Epoch 1040, val loss: 1.3309696912765503\n",
      "Epoch 1050, training loss: 642.9165649414062 = 1.2538481950759888 + 100 * 6.416626930236816\n",
      "Epoch 1050, val loss: 1.3230057954788208\n",
      "Epoch 1060, training loss: 639.7451171875 = 1.2414826154708862 + 100 * 6.385036468505859\n",
      "Epoch 1060, val loss: 1.3126312494277954\n",
      "Epoch 1070, training loss: 639.68994140625 = 1.232153058052063 + 100 * 6.384577751159668\n",
      "Epoch 1070, val loss: 1.305040955543518\n",
      "Epoch 1080, training loss: 639.3071899414062 = 1.2230087518692017 + 100 * 6.3808417320251465\n",
      "Epoch 1080, val loss: 1.2980679273605347\n",
      "Epoch 1090, training loss: 640.1055297851562 = 1.2135566473007202 + 100 * 6.388919830322266\n",
      "Epoch 1090, val loss: 1.2901692390441895\n",
      "Epoch 1100, training loss: 639.2544555664062 = 1.2033978700637817 + 100 * 6.3805108070373535\n",
      "Epoch 1100, val loss: 1.2814098596572876\n",
      "Epoch 1110, training loss: 639.5835571289062 = 1.1936514377593994 + 100 * 6.383898735046387\n",
      "Epoch 1110, val loss: 1.273561954498291\n",
      "Epoch 1120, training loss: 638.7821655273438 = 1.1836302280426025 + 100 * 6.375985145568848\n",
      "Epoch 1120, val loss: 1.2659212350845337\n",
      "Epoch 1130, training loss: 638.5291137695312 = 1.1744102239608765 + 100 * 6.373546600341797\n",
      "Epoch 1130, val loss: 1.2587072849273682\n",
      "Epoch 1140, training loss: 638.927001953125 = 1.1649819612503052 + 100 * 6.377620220184326\n",
      "Epoch 1140, val loss: 1.2513445615768433\n",
      "Epoch 1150, training loss: 639.6867065429688 = 1.1544764041900635 + 100 * 6.385322093963623\n",
      "Epoch 1150, val loss: 1.2419928312301636\n",
      "Epoch 1160, training loss: 638.3192749023438 = 1.1440359354019165 + 100 * 6.3717522621154785\n",
      "Epoch 1160, val loss: 1.233994960784912\n",
      "Epoch 1170, training loss: 638.0336303710938 = 1.1353142261505127 + 100 * 6.368983268737793\n",
      "Epoch 1170, val loss: 1.2270127534866333\n",
      "Epoch 1180, training loss: 637.8510131835938 = 1.1263234615325928 + 100 * 6.367246627807617\n",
      "Epoch 1180, val loss: 1.220257043838501\n",
      "Epoch 1190, training loss: 638.8698120117188 = 1.117469072341919 + 100 * 6.377522945404053\n",
      "Epoch 1190, val loss: 1.2131998538970947\n",
      "Epoch 1200, training loss: 638.0425415039062 = 1.106905221939087 + 100 * 6.369356155395508\n",
      "Epoch 1200, val loss: 1.2044851779937744\n",
      "Epoch 1210, training loss: 637.7425537109375 = 1.0976130962371826 + 100 * 6.366449356079102\n",
      "Epoch 1210, val loss: 1.1976972818374634\n",
      "Epoch 1220, training loss: 637.4871826171875 = 1.08900785446167 + 100 * 6.363982200622559\n",
      "Epoch 1220, val loss: 1.191082239151001\n",
      "Epoch 1230, training loss: 638.1945190429688 = 1.079945683479309 + 100 * 6.371145725250244\n",
      "Epoch 1230, val loss: 1.1841694116592407\n",
      "Epoch 1240, training loss: 637.8685913085938 = 1.0703014135360718 + 100 * 6.367982864379883\n",
      "Epoch 1240, val loss: 1.1760505437850952\n",
      "Epoch 1250, training loss: 637.793701171875 = 1.061140537261963 + 100 * 6.367325305938721\n",
      "Epoch 1250, val loss: 1.169545292854309\n",
      "Epoch 1260, training loss: 637.07861328125 = 1.0516620874404907 + 100 * 6.360270023345947\n",
      "Epoch 1260, val loss: 1.1618225574493408\n",
      "Epoch 1270, training loss: 636.9002685546875 = 1.0431069135665894 + 100 * 6.358571529388428\n",
      "Epoch 1270, val loss: 1.1553165912628174\n",
      "Epoch 1280, training loss: 636.8709716796875 = 1.0348985195159912 + 100 * 6.358360767364502\n",
      "Epoch 1280, val loss: 1.14948308467865\n",
      "Epoch 1290, training loss: 638.4566040039062 = 1.0266615152359009 + 100 * 6.374299049377441\n",
      "Epoch 1290, val loss: 1.1424897909164429\n",
      "Epoch 1300, training loss: 636.8931274414062 = 1.016719937324524 + 100 * 6.358764171600342\n",
      "Epoch 1300, val loss: 1.135890007019043\n",
      "Epoch 1310, training loss: 636.448974609375 = 1.0087223052978516 + 100 * 6.354402542114258\n",
      "Epoch 1310, val loss: 1.129758596420288\n",
      "Epoch 1320, training loss: 636.384033203125 = 1.0009255409240723 + 100 * 6.3538312911987305\n",
      "Epoch 1320, val loss: 1.124104380607605\n",
      "Epoch 1330, training loss: 637.2096557617188 = 0.9931013584136963 + 100 * 6.362165451049805\n",
      "Epoch 1330, val loss: 1.117850661277771\n",
      "Epoch 1340, training loss: 637.5078125 = 0.9829344153404236 + 100 * 6.365249156951904\n",
      "Epoch 1340, val loss: 1.1107261180877686\n",
      "Epoch 1350, training loss: 636.6327514648438 = 0.9748044610023499 + 100 * 6.356579303741455\n",
      "Epoch 1350, val loss: 1.1042433977127075\n",
      "Epoch 1360, training loss: 636.1334838867188 = 0.9668428897857666 + 100 * 6.351666450500488\n",
      "Epoch 1360, val loss: 1.0989106893539429\n",
      "Epoch 1370, training loss: 635.89599609375 = 0.9594746828079224 + 100 * 6.349365234375\n",
      "Epoch 1370, val loss: 1.0935289859771729\n",
      "Epoch 1380, training loss: 636.1104125976562 = 0.9519322514533997 + 100 * 6.3515849113464355\n",
      "Epoch 1380, val loss: 1.0884064435958862\n",
      "Epoch 1390, training loss: 636.2086791992188 = 0.9427617192268372 + 100 * 6.352659225463867\n",
      "Epoch 1390, val loss: 1.0810034275054932\n",
      "Epoch 1400, training loss: 636.255126953125 = 0.9340856075286865 + 100 * 6.35321044921875\n",
      "Epoch 1400, val loss: 1.074790596961975\n",
      "Epoch 1410, training loss: 635.5332641601562 = 0.9268032312393188 + 100 * 6.346064567565918\n",
      "Epoch 1410, val loss: 1.0698384046554565\n",
      "Epoch 1420, training loss: 635.520751953125 = 0.9196189045906067 + 100 * 6.346011161804199\n",
      "Epoch 1420, val loss: 1.0648256540298462\n",
      "Epoch 1430, training loss: 635.53173828125 = 0.9125763773918152 + 100 * 6.34619140625\n",
      "Epoch 1430, val loss: 1.059949278831482\n",
      "Epoch 1440, training loss: 637.0983276367188 = 0.9046599864959717 + 100 * 6.361936569213867\n",
      "Epoch 1440, val loss: 1.054109811782837\n",
      "Epoch 1450, training loss: 635.7203369140625 = 0.8964042067527771 + 100 * 6.348238945007324\n",
      "Epoch 1450, val loss: 1.047791838645935\n",
      "Epoch 1460, training loss: 635.224609375 = 0.888964056968689 + 100 * 6.343356132507324\n",
      "Epoch 1460, val loss: 1.0428589582443237\n",
      "Epoch 1470, training loss: 635.2908325195312 = 0.882010281085968 + 100 * 6.344088077545166\n",
      "Epoch 1470, val loss: 1.0382925271987915\n",
      "Epoch 1480, training loss: 636.0377197265625 = 0.874847412109375 + 100 * 6.35162878036499\n",
      "Epoch 1480, val loss: 1.0334587097167969\n",
      "Epoch 1490, training loss: 635.3291625976562 = 0.8672913908958435 + 100 * 6.344618797302246\n",
      "Epoch 1490, val loss: 1.0278306007385254\n",
      "Epoch 1500, training loss: 635.3364868164062 = 0.8600022792816162 + 100 * 6.3447651863098145\n",
      "Epoch 1500, val loss: 1.0230056047439575\n",
      "Epoch 1510, training loss: 634.8364868164062 = 0.8529735207557678 + 100 * 6.339835166931152\n",
      "Epoch 1510, val loss: 1.0179593563079834\n",
      "Epoch 1520, training loss: 634.893310546875 = 0.8465244770050049 + 100 * 6.340468406677246\n",
      "Epoch 1520, val loss: 1.0135620832443237\n",
      "Epoch 1530, training loss: 635.6405029296875 = 0.8399494886398315 + 100 * 6.348005294799805\n",
      "Epoch 1530, val loss: 1.0090405941009521\n",
      "Epoch 1540, training loss: 634.7611694335938 = 0.8325446844100952 + 100 * 6.3392863273620605\n",
      "Epoch 1540, val loss: 1.0044350624084473\n",
      "Epoch 1550, training loss: 636.241943359375 = 0.8256357312202454 + 100 * 6.35416316986084\n",
      "Epoch 1550, val loss: 0.9997876286506653\n",
      "Epoch 1560, training loss: 634.97021484375 = 0.8183614611625671 + 100 * 6.341518402099609\n",
      "Epoch 1560, val loss: 0.9945074915885925\n",
      "Epoch 1570, training loss: 634.6567993164062 = 0.811618983745575 + 100 * 6.338451385498047\n",
      "Epoch 1570, val loss: 0.9902549386024475\n",
      "Epoch 1580, training loss: 634.5308227539062 = 0.8055323362350464 + 100 * 6.337253093719482\n",
      "Epoch 1580, val loss: 0.9864583611488342\n",
      "Epoch 1590, training loss: 635.8696899414062 = 0.7989805936813354 + 100 * 6.350707530975342\n",
      "Epoch 1590, val loss: 0.9819906949996948\n",
      "Epoch 1600, training loss: 634.88623046875 = 0.7918151021003723 + 100 * 6.340944290161133\n",
      "Epoch 1600, val loss: 0.9769166111946106\n",
      "Epoch 1610, training loss: 634.4718627929688 = 0.7855010628700256 + 100 * 6.3368635177612305\n",
      "Epoch 1610, val loss: 0.9729288816452026\n",
      "Epoch 1620, training loss: 635.0967407226562 = 0.7792996168136597 + 100 * 6.343174457550049\n",
      "Epoch 1620, val loss: 0.9687519073486328\n",
      "Epoch 1630, training loss: 634.1631469726562 = 0.7723808288574219 + 100 * 6.333907604217529\n",
      "Epoch 1630, val loss: 0.9646160006523132\n",
      "Epoch 1640, training loss: 634.5562744140625 = 0.7662535905838013 + 100 * 6.337900638580322\n",
      "Epoch 1640, val loss: 0.9607058167457581\n",
      "Epoch 1650, training loss: 634.3922729492188 = 0.7599082589149475 + 100 * 6.3363237380981445\n",
      "Epoch 1650, val loss: 0.9564962983131409\n",
      "Epoch 1660, training loss: 634.0609741210938 = 0.7539390921592712 + 100 * 6.333069801330566\n",
      "Epoch 1660, val loss: 0.9526306390762329\n",
      "Epoch 1670, training loss: 634.2578735351562 = 0.7482110261917114 + 100 * 6.33509635925293\n",
      "Epoch 1670, val loss: 0.949019730091095\n",
      "Epoch 1680, training loss: 634.6593017578125 = 0.7419127225875854 + 100 * 6.339174270629883\n",
      "Epoch 1680, val loss: 0.9448804259300232\n",
      "Epoch 1690, training loss: 634.0117797851562 = 0.7349487543106079 + 100 * 6.332768440246582\n",
      "Epoch 1690, val loss: 0.9406262040138245\n",
      "Epoch 1700, training loss: 633.8873291015625 = 0.7290865182876587 + 100 * 6.331582546234131\n",
      "Epoch 1700, val loss: 0.9369598627090454\n",
      "Epoch 1710, training loss: 633.814208984375 = 0.7234356999397278 + 100 * 6.330907344818115\n",
      "Epoch 1710, val loss: 0.9337549805641174\n",
      "Epoch 1720, training loss: 634.8319091796875 = 0.7176047563552856 + 100 * 6.3411431312561035\n",
      "Epoch 1720, val loss: 0.930298388004303\n",
      "Epoch 1730, training loss: 634.3803100585938 = 0.7111459970474243 + 100 * 6.336691856384277\n",
      "Epoch 1730, val loss: 0.9259628057479858\n",
      "Epoch 1740, training loss: 633.9564819335938 = 0.7053492665290833 + 100 * 6.3325114250183105\n",
      "Epoch 1740, val loss: 0.9221336245536804\n",
      "Epoch 1750, training loss: 633.560302734375 = 0.6997703313827515 + 100 * 6.3286051750183105\n",
      "Epoch 1750, val loss: 0.9190536141395569\n",
      "Epoch 1760, training loss: 633.919189453125 = 0.6942907571792603 + 100 * 6.332249164581299\n",
      "Epoch 1760, val loss: 0.9159209132194519\n",
      "Epoch 1770, training loss: 634.1072387695312 = 0.6881690621376038 + 100 * 6.334190368652344\n",
      "Epoch 1770, val loss: 0.9119367003440857\n",
      "Epoch 1780, training loss: 633.6946411132812 = 0.6815521121025085 + 100 * 6.330130577087402\n",
      "Epoch 1780, val loss: 0.9077445864677429\n",
      "Epoch 1790, training loss: 633.5838012695312 = 0.675994336605072 + 100 * 6.329078197479248\n",
      "Epoch 1790, val loss: 0.9042670726776123\n",
      "Epoch 1800, training loss: 633.3027954101562 = 0.6707690358161926 + 100 * 6.326320171356201\n",
      "Epoch 1800, val loss: 0.9014453291893005\n",
      "Epoch 1810, training loss: 633.4627685546875 = 0.6658154129981995 + 100 * 6.327969074249268\n",
      "Epoch 1810, val loss: 0.8983820080757141\n",
      "Epoch 1820, training loss: 634.0430297851562 = 0.66044020652771 + 100 * 6.333825588226318\n",
      "Epoch 1820, val loss: 0.8950690031051636\n",
      "Epoch 1830, training loss: 634.1815185546875 = 0.6543264985084534 + 100 * 6.335272312164307\n",
      "Epoch 1830, val loss: 0.8916022181510925\n",
      "Epoch 1840, training loss: 633.1566162109375 = 0.6487364172935486 + 100 * 6.32507848739624\n",
      "Epoch 1840, val loss: 0.8883358836174011\n",
      "Epoch 1850, training loss: 633.0579223632812 = 0.6435644626617432 + 100 * 6.324143409729004\n",
      "Epoch 1850, val loss: 0.8855779767036438\n",
      "Epoch 1860, training loss: 633.8194580078125 = 0.6383810639381409 + 100 * 6.33181095123291\n",
      "Epoch 1860, val loss: 0.8827239274978638\n",
      "Epoch 1870, training loss: 633.7281494140625 = 0.6326032280921936 + 100 * 6.3309550285339355\n",
      "Epoch 1870, val loss: 0.8785573840141296\n",
      "Epoch 1880, training loss: 633.090087890625 = 0.6265532374382019 + 100 * 6.3246355056762695\n",
      "Epoch 1880, val loss: 0.87531578540802\n",
      "Epoch 1890, training loss: 632.9345703125 = 0.6217930912971497 + 100 * 6.3231282234191895\n",
      "Epoch 1890, val loss: 0.8727924227714539\n",
      "Epoch 1900, training loss: 632.7640380859375 = 0.6167965531349182 + 100 * 6.32147216796875\n",
      "Epoch 1900, val loss: 0.8699571490287781\n",
      "Epoch 1910, training loss: 633.878662109375 = 0.6117441654205322 + 100 * 6.332668781280518\n",
      "Epoch 1910, val loss: 0.867268443107605\n",
      "Epoch 1920, training loss: 632.8759155273438 = 0.606346607208252 + 100 * 6.322696208953857\n",
      "Epoch 1920, val loss: 0.8637197613716125\n",
      "Epoch 1930, training loss: 633.4307250976562 = 0.6010683178901672 + 100 * 6.328296661376953\n",
      "Epoch 1930, val loss: 0.8606160879135132\n",
      "Epoch 1940, training loss: 632.6439208984375 = 0.5956283807754517 + 100 * 6.3204827308654785\n",
      "Epoch 1940, val loss: 0.8577286005020142\n",
      "Epoch 1950, training loss: 632.5715942382812 = 0.5907906889915466 + 100 * 6.319807529449463\n",
      "Epoch 1950, val loss: 0.8550962805747986\n",
      "Epoch 1960, training loss: 633.3109130859375 = 0.5859168171882629 + 100 * 6.327250003814697\n",
      "Epoch 1960, val loss: 0.8526114821434021\n",
      "Epoch 1970, training loss: 633.0582885742188 = 0.5809082388877869 + 100 * 6.324773788452148\n",
      "Epoch 1970, val loss: 0.8494685888290405\n",
      "Epoch 1980, training loss: 632.7511596679688 = 0.5754028558731079 + 100 * 6.321757793426514\n",
      "Epoch 1980, val loss: 0.8461014628410339\n",
      "Epoch 1990, training loss: 633.18310546875 = 0.5703986287117004 + 100 * 6.326127529144287\n",
      "Epoch 1990, val loss: 0.8434230089187622\n",
      "Epoch 2000, training loss: 632.4182739257812 = 0.5653270483016968 + 100 * 6.3185296058654785\n",
      "Epoch 2000, val loss: 0.8405826091766357\n",
      "Epoch 2010, training loss: 632.3449096679688 = 0.560706377029419 + 100 * 6.31784200668335\n",
      "Epoch 2010, val loss: 0.8384527564048767\n",
      "Epoch 2020, training loss: 632.8084106445312 = 0.5562392473220825 + 100 * 6.322522163391113\n",
      "Epoch 2020, val loss: 0.8359091877937317\n",
      "Epoch 2030, training loss: 632.569091796875 = 0.5511533617973328 + 100 * 6.320179462432861\n",
      "Epoch 2030, val loss: 0.832901656627655\n",
      "Epoch 2040, training loss: 632.9081420898438 = 0.5464388132095337 + 100 * 6.323616981506348\n",
      "Epoch 2040, val loss: 0.8300713300704956\n",
      "Epoch 2050, training loss: 632.4408569335938 = 0.5412296056747437 + 100 * 6.318995952606201\n",
      "Epoch 2050, val loss: 0.8275321125984192\n",
      "Epoch 2060, training loss: 632.312744140625 = 0.5364569425582886 + 100 * 6.317763328552246\n",
      "Epoch 2060, val loss: 0.8249457478523254\n",
      "Epoch 2070, training loss: 632.1598510742188 = 0.5321788191795349 + 100 * 6.316277027130127\n",
      "Epoch 2070, val loss: 0.8227423429489136\n",
      "Epoch 2080, training loss: 632.7642211914062 = 0.5277991890907288 + 100 * 6.322364807128906\n",
      "Epoch 2080, val loss: 0.8207111954689026\n",
      "Epoch 2090, training loss: 632.115966796875 = 0.5228496193885803 + 100 * 6.31593132019043\n",
      "Epoch 2090, val loss: 0.8174665570259094\n",
      "Epoch 2100, training loss: 633.5272827148438 = 0.5184869170188904 + 100 * 6.330088138580322\n",
      "Epoch 2100, val loss: 0.8151177763938904\n",
      "Epoch 2110, training loss: 632.6900024414062 = 0.5128083825111389 + 100 * 6.32177209854126\n",
      "Epoch 2110, val loss: 0.8120021820068359\n",
      "Epoch 2120, training loss: 632.1085205078125 = 0.5081363916397095 + 100 * 6.316004276275635\n",
      "Epoch 2120, val loss: 0.8096280097961426\n",
      "Epoch 2130, training loss: 631.8528442382812 = 0.5038900971412659 + 100 * 6.3134894371032715\n",
      "Epoch 2130, val loss: 0.8074250221252441\n",
      "Epoch 2140, training loss: 631.9448852539062 = 0.49988967180252075 + 100 * 6.314450263977051\n",
      "Epoch 2140, val loss: 0.8054289817810059\n",
      "Epoch 2150, training loss: 633.3590698242188 = 0.49566933512687683 + 100 * 6.328634262084961\n",
      "Epoch 2150, val loss: 0.8029216527938843\n",
      "Epoch 2160, training loss: 632.2332153320312 = 0.4905376434326172 + 100 * 6.317426681518555\n",
      "Epoch 2160, val loss: 0.8005229234695435\n",
      "Epoch 2170, training loss: 631.8867797851562 = 0.48651254177093506 + 100 * 6.314002990722656\n",
      "Epoch 2170, val loss: 0.7984791994094849\n",
      "Epoch 2180, training loss: 633.2156372070312 = 0.48192253708839417 + 100 * 6.32733678817749\n",
      "Epoch 2180, val loss: 0.7961975932121277\n",
      "Epoch 2190, training loss: 631.9223022460938 = 0.4775537848472595 + 100 * 6.314447402954102\n",
      "Epoch 2190, val loss: 0.7936065793037415\n",
      "Epoch 2200, training loss: 631.854736328125 = 0.4732024073600769 + 100 * 6.313815593719482\n",
      "Epoch 2200, val loss: 0.7914331555366516\n",
      "Epoch 2210, training loss: 632.3469848632812 = 0.46891266107559204 + 100 * 6.318780422210693\n",
      "Epoch 2210, val loss: 0.7893208265304565\n",
      "Epoch 2220, training loss: 631.600341796875 = 0.46460723876953125 + 100 * 6.311357498168945\n",
      "Epoch 2220, val loss: 0.7869795560836792\n",
      "Epoch 2230, training loss: 631.9840698242188 = 0.4606080651283264 + 100 * 6.315234184265137\n",
      "Epoch 2230, val loss: 0.7850260138511658\n",
      "Epoch 2240, training loss: 632.0476684570312 = 0.45640695095062256 + 100 * 6.31591272354126\n",
      "Epoch 2240, val loss: 0.7830239534378052\n",
      "Epoch 2250, training loss: 631.9779663085938 = 0.45212993025779724 + 100 * 6.315258026123047\n",
      "Epoch 2250, val loss: 0.7804632782936096\n",
      "Epoch 2260, training loss: 631.512451171875 = 0.4480339288711548 + 100 * 6.310644149780273\n",
      "Epoch 2260, val loss: 0.7787595391273499\n",
      "Epoch 2270, training loss: 631.8735961914062 = 0.44412946701049805 + 100 * 6.314294338226318\n",
      "Epoch 2270, val loss: 0.7769138813018799\n",
      "Epoch 2280, training loss: 632.1920166015625 = 0.4399056136608124 + 100 * 6.317521095275879\n",
      "Epoch 2280, val loss: 0.7749063372612\n",
      "Epoch 2290, training loss: 631.6911010742188 = 0.4357346296310425 + 100 * 6.312553405761719\n",
      "Epoch 2290, val loss: 0.772649347782135\n",
      "Epoch 2300, training loss: 631.6815185546875 = 0.43164461851119995 + 100 * 6.312499046325684\n",
      "Epoch 2300, val loss: 0.7703903913497925\n",
      "Epoch 2310, training loss: 631.7850341796875 = 0.42761221528053284 + 100 * 6.313574314117432\n",
      "Epoch 2310, val loss: 0.7683555483818054\n",
      "Epoch 2320, training loss: 632.2201538085938 = 0.42388445138931274 + 100 * 6.317962646484375\n",
      "Epoch 2320, val loss: 0.7663710713386536\n",
      "Epoch 2330, training loss: 631.3548583984375 = 0.4196152687072754 + 100 * 6.309352397918701\n",
      "Epoch 2330, val loss: 0.7643813490867615\n",
      "Epoch 2340, training loss: 631.3659057617188 = 0.41596823930740356 + 100 * 6.309499740600586\n",
      "Epoch 2340, val loss: 0.7626471519470215\n",
      "Epoch 2350, training loss: 631.57177734375 = 0.4124499559402466 + 100 * 6.311593055725098\n",
      "Epoch 2350, val loss: 0.7610220313072205\n",
      "Epoch 2360, training loss: 631.9677734375 = 0.4086143672466278 + 100 * 6.315591335296631\n",
      "Epoch 2360, val loss: 0.7587522864341736\n",
      "Epoch 2370, training loss: 631.4425048828125 = 0.40426748991012573 + 100 * 6.31038236618042\n",
      "Epoch 2370, val loss: 0.7568978071212769\n",
      "Epoch 2380, training loss: 631.2838745117188 = 0.4006601572036743 + 100 * 6.308832168579102\n",
      "Epoch 2380, val loss: 0.755497932434082\n",
      "Epoch 2390, training loss: 632.0369262695312 = 0.39708080887794495 + 100 * 6.3163981437683105\n",
      "Epoch 2390, val loss: 0.7535319924354553\n",
      "Epoch 2400, training loss: 631.7676391601562 = 0.39275062084198 + 100 * 6.313748836517334\n",
      "Epoch 2400, val loss: 0.7520747780799866\n",
      "Epoch 2410, training loss: 631.224365234375 = 0.38928472995758057 + 100 * 6.308350563049316\n",
      "Epoch 2410, val loss: 0.7495721578598022\n",
      "Epoch 2420, training loss: 631.1748046875 = 0.38570743799209595 + 100 * 6.307891368865967\n",
      "Epoch 2420, val loss: 0.7484005093574524\n",
      "Epoch 2430, training loss: 631.6823120117188 = 0.38211914896965027 + 100 * 6.31300163269043\n",
      "Epoch 2430, val loss: 0.7463460564613342\n",
      "Epoch 2440, training loss: 630.9369506835938 = 0.3784548342227936 + 100 * 6.30558443069458\n",
      "Epoch 2440, val loss: 0.7448042631149292\n",
      "Epoch 2450, training loss: 631.0402221679688 = 0.37510600686073303 + 100 * 6.3066511154174805\n",
      "Epoch 2450, val loss: 0.7433728575706482\n",
      "Epoch 2460, training loss: 631.4637451171875 = 0.3716788589954376 + 100 * 6.310920238494873\n",
      "Epoch 2460, val loss: 0.7415372133255005\n",
      "Epoch 2470, training loss: 631.3941040039062 = 0.36784231662750244 + 100 * 6.310262680053711\n",
      "Epoch 2470, val loss: 0.7404299378395081\n",
      "Epoch 2480, training loss: 631.0635986328125 = 0.36432066559791565 + 100 * 6.306992530822754\n",
      "Epoch 2480, val loss: 0.7379933595657349\n",
      "Epoch 2490, training loss: 631.01220703125 = 0.3607856333255768 + 100 * 6.306514263153076\n",
      "Epoch 2490, val loss: 0.7372829914093018\n",
      "Epoch 2500, training loss: 631.6458740234375 = 0.35729414224624634 + 100 * 6.312885761260986\n",
      "Epoch 2500, val loss: 0.7353265285491943\n",
      "Epoch 2510, training loss: 630.9428100585938 = 0.3540849983692169 + 100 * 6.305887699127197\n",
      "Epoch 2510, val loss: 0.7339962720870972\n",
      "Epoch 2520, training loss: 630.8549194335938 = 0.3508527874946594 + 100 * 6.3050408363342285\n",
      "Epoch 2520, val loss: 0.7325648069381714\n",
      "Epoch 2530, training loss: 631.6051635742188 = 0.3474121689796448 + 100 * 6.312577724456787\n",
      "Epoch 2530, val loss: 0.7309482097625732\n",
      "Epoch 2540, training loss: 630.9574584960938 = 0.3437577486038208 + 100 * 6.3061370849609375\n",
      "Epoch 2540, val loss: 0.7294407486915588\n",
      "Epoch 2550, training loss: 630.7059936523438 = 0.3407001793384552 + 100 * 6.303653240203857\n",
      "Epoch 2550, val loss: 0.728333055973053\n",
      "Epoch 2560, training loss: 631.0179443359375 = 0.33771371841430664 + 100 * 6.306802272796631\n",
      "Epoch 2560, val loss: 0.7272884845733643\n",
      "Epoch 2570, training loss: 630.6243896484375 = 0.3342505693435669 + 100 * 6.302901744842529\n",
      "Epoch 2570, val loss: 0.7256861925125122\n",
      "Epoch 2580, training loss: 630.6370849609375 = 0.33111831545829773 + 100 * 6.3030595779418945\n",
      "Epoch 2580, val loss: 0.7244104743003845\n",
      "Epoch 2590, training loss: 631.2775268554688 = 0.3280664086341858 + 100 * 6.309494495391846\n",
      "Epoch 2590, val loss: 0.7232488393783569\n",
      "Epoch 2600, training loss: 630.5287475585938 = 0.32469576597213745 + 100 * 6.3020405769348145\n",
      "Epoch 2600, val loss: 0.7216551899909973\n",
      "Epoch 2610, training loss: 630.5828857421875 = 0.3217243254184723 + 100 * 6.302611827850342\n",
      "Epoch 2610, val loss: 0.7208267450332642\n",
      "Epoch 2620, training loss: 631.5255126953125 = 0.31865379214286804 + 100 * 6.312068462371826\n",
      "Epoch 2620, val loss: 0.7196242809295654\n",
      "Epoch 2630, training loss: 630.4487915039062 = 0.3154336214065552 + 100 * 6.301333427429199\n",
      "Epoch 2630, val loss: 0.7180700898170471\n",
      "Epoch 2640, training loss: 630.2954711914062 = 0.31248247623443604 + 100 * 6.299829959869385\n",
      "Epoch 2640, val loss: 0.7171840071678162\n",
      "Epoch 2650, training loss: 630.4440307617188 = 0.30964913964271545 + 100 * 6.30134391784668\n",
      "Epoch 2650, val loss: 0.7158289551734924\n",
      "Epoch 2660, training loss: 631.9767456054688 = 0.30681416392326355 + 100 * 6.316699028015137\n",
      "Epoch 2660, val loss: 0.714617908000946\n",
      "Epoch 2670, training loss: 630.8355102539062 = 0.30341267585754395 + 100 * 6.305320739746094\n",
      "Epoch 2670, val loss: 0.713889479637146\n",
      "Epoch 2680, training loss: 630.893310546875 = 0.3006390929222107 + 100 * 6.305926322937012\n",
      "Epoch 2680, val loss: 0.7126479148864746\n",
      "Epoch 2690, training loss: 630.4692993164062 = 0.2976308763027191 + 100 * 6.3017168045043945\n",
      "Epoch 2690, val loss: 0.7116879224777222\n",
      "Epoch 2700, training loss: 630.1560668945312 = 0.29497140645980835 + 100 * 6.298610687255859\n",
      "Epoch 2700, val loss: 0.7106047868728638\n",
      "Epoch 2710, training loss: 630.5723266601562 = 0.2923605442047119 + 100 * 6.302799701690674\n",
      "Epoch 2710, val loss: 0.7097058892250061\n",
      "Epoch 2720, training loss: 630.4938354492188 = 0.2894468605518341 + 100 * 6.302043914794922\n",
      "Epoch 2720, val loss: 0.7088637351989746\n",
      "Epoch 2730, training loss: 631.2247314453125 = 0.286624014377594 + 100 * 6.309381008148193\n",
      "Epoch 2730, val loss: 0.7079949378967285\n",
      "Epoch 2740, training loss: 630.2171630859375 = 0.2836984694004059 + 100 * 6.299334526062012\n",
      "Epoch 2740, val loss: 0.7065379619598389\n",
      "Epoch 2750, training loss: 630.5510864257812 = 0.28099027276039124 + 100 * 6.302700996398926\n",
      "Epoch 2750, val loss: 0.7061985731124878\n",
      "Epoch 2760, training loss: 630.3837280273438 = 0.27804335951805115 + 100 * 6.3010573387146\n",
      "Epoch 2760, val loss: 0.7046815752983093\n",
      "Epoch 2770, training loss: 630.10205078125 = 0.2754598557949066 + 100 * 6.2982659339904785\n",
      "Epoch 2770, val loss: 0.7037990689277649\n",
      "Epoch 2780, training loss: 629.9917602539062 = 0.2729889452457428 + 100 * 6.297187805175781\n",
      "Epoch 2780, val loss: 0.7034038305282593\n",
      "Epoch 2790, training loss: 630.2347412109375 = 0.2706913352012634 + 100 * 6.299640655517578\n",
      "Epoch 2790, val loss: 0.7026346921920776\n",
      "Epoch 2800, training loss: 630.7474365234375 = 0.26802563667297363 + 100 * 6.3047943115234375\n",
      "Epoch 2800, val loss: 0.7022673487663269\n",
      "Epoch 2810, training loss: 630.0913696289062 = 0.26514700055122375 + 100 * 6.298262596130371\n",
      "Epoch 2810, val loss: 0.7010106444358826\n",
      "Epoch 2820, training loss: 630.074951171875 = 0.2626968026161194 + 100 * 6.298122406005859\n",
      "Epoch 2820, val loss: 0.7001170516014099\n",
      "Epoch 2830, training loss: 631.3185424804688 = 0.26006168127059937 + 100 * 6.310585021972656\n",
      "Epoch 2830, val loss: 0.7000070810317993\n",
      "Epoch 2840, training loss: 630.9362182617188 = 0.25746333599090576 + 100 * 6.306787490844727\n",
      "Epoch 2840, val loss: 0.6975342631340027\n",
      "Epoch 2850, training loss: 630.1090698242188 = 0.25483182072639465 + 100 * 6.298542499542236\n",
      "Epoch 2850, val loss: 0.6983309984207153\n",
      "Epoch 2860, training loss: 630.054931640625 = 0.2525555491447449 + 100 * 6.298023700714111\n",
      "Epoch 2860, val loss: 0.6970997452735901\n",
      "Epoch 2870, training loss: 630.412841796875 = 0.2500745952129364 + 100 * 6.3016276359558105\n",
      "Epoch 2870, val loss: 0.6965794563293457\n",
      "Epoch 2880, training loss: 629.6375732421875 = 0.24781568348407745 + 100 * 6.29389762878418\n",
      "Epoch 2880, val loss: 0.6958266496658325\n",
      "Epoch 2890, training loss: 629.9136352539062 = 0.24569538235664368 + 100 * 6.296679973602295\n",
      "Epoch 2890, val loss: 0.6953900456428528\n",
      "Epoch 2900, training loss: 630.070556640625 = 0.24341288208961487 + 100 * 6.298271179199219\n",
      "Epoch 2900, val loss: 0.6944952011108398\n",
      "Epoch 2910, training loss: 630.8448486328125 = 0.2408289611339569 + 100 * 6.306040287017822\n",
      "Epoch 2910, val loss: 0.6940138936042786\n",
      "Epoch 2920, training loss: 630.2525634765625 = 0.23846447467803955 + 100 * 6.300140857696533\n",
      "Epoch 2920, val loss: 0.6933525800704956\n",
      "Epoch 2930, training loss: 629.9140625 = 0.23604707419872284 + 100 * 6.296780586242676\n",
      "Epoch 2930, val loss: 0.6929816603660583\n",
      "Epoch 2940, training loss: 630.5145263671875 = 0.23387019336223602 + 100 * 6.302806377410889\n",
      "Epoch 2940, val loss: 0.6924383044242859\n",
      "Epoch 2950, training loss: 629.5249633789062 = 0.2314358353614807 + 100 * 6.292934894561768\n",
      "Epoch 2950, val loss: 0.6917511224746704\n",
      "Epoch 2960, training loss: 629.6660766601562 = 0.2293252795934677 + 100 * 6.294367790222168\n",
      "Epoch 2960, val loss: 0.690973162651062\n",
      "Epoch 2970, training loss: 629.7821044921875 = 0.22728317975997925 + 100 * 6.295547962188721\n",
      "Epoch 2970, val loss: 0.69083571434021\n",
      "Epoch 2980, training loss: 630.2648315429688 = 0.2250959426164627 + 100 * 6.3003973960876465\n",
      "Epoch 2980, val loss: 0.6900176405906677\n",
      "Epoch 2990, training loss: 630.1430053710938 = 0.22290416061878204 + 100 * 6.299201011657715\n",
      "Epoch 2990, val loss: 0.6901254057884216\n",
      "Epoch 3000, training loss: 629.40771484375 = 0.22076241672039032 + 100 * 6.291869640350342\n",
      "Epoch 3000, val loss: 0.6891745924949646\n",
      "Epoch 3010, training loss: 629.4927368164062 = 0.21882246434688568 + 100 * 6.292739391326904\n",
      "Epoch 3010, val loss: 0.6890614032745361\n",
      "Epoch 3020, training loss: 630.1768188476562 = 0.2168876677751541 + 100 * 6.299599647521973\n",
      "Epoch 3020, val loss: 0.6893914937973022\n",
      "Epoch 3030, training loss: 629.5759887695312 = 0.21448107063770294 + 100 * 6.293614864349365\n",
      "Epoch 3030, val loss: 0.6876519918441772\n",
      "Epoch 3040, training loss: 629.3179931640625 = 0.21243105828762054 + 100 * 6.291055679321289\n",
      "Epoch 3040, val loss: 0.6877346634864807\n",
      "Epoch 3050, training loss: 629.5169677734375 = 0.21063829958438873 + 100 * 6.293063640594482\n",
      "Epoch 3050, val loss: 0.6871399879455566\n",
      "Epoch 3060, training loss: 630.0286254882812 = 0.20867584645748138 + 100 * 6.29819917678833\n",
      "Epoch 3060, val loss: 0.6870021820068359\n",
      "Epoch 3070, training loss: 629.4486083984375 = 0.2064218372106552 + 100 * 6.292421817779541\n",
      "Epoch 3070, val loss: 0.6865702271461487\n",
      "Epoch 3080, training loss: 629.2567138671875 = 0.20462051033973694 + 100 * 6.290520668029785\n",
      "Epoch 3080, val loss: 0.6863762736320496\n",
      "Epoch 3090, training loss: 630.1828002929688 = 0.2028997540473938 + 100 * 6.299798965454102\n",
      "Epoch 3090, val loss: 0.6863623261451721\n",
      "Epoch 3100, training loss: 629.3218383789062 = 0.2008066177368164 + 100 * 6.291210174560547\n",
      "Epoch 3100, val loss: 0.6855115294456482\n",
      "Epoch 3110, training loss: 630.3081665039062 = 0.1988043189048767 + 100 * 6.301094055175781\n",
      "Epoch 3110, val loss: 0.6849331855773926\n",
      "Epoch 3120, training loss: 629.2984008789062 = 0.19684141874313354 + 100 * 6.291015625\n",
      "Epoch 3120, val loss: 0.6849173903465271\n",
      "Epoch 3130, training loss: 629.1768798828125 = 0.19505339860916138 + 100 * 6.289818286895752\n",
      "Epoch 3130, val loss: 0.6845890879631042\n",
      "Epoch 3140, training loss: 629.5609741210938 = 0.19336070120334625 + 100 * 6.293676376342773\n",
      "Epoch 3140, val loss: 0.6843463182449341\n",
      "Epoch 3150, training loss: 629.4572143554688 = 0.19140854477882385 + 100 * 6.29265832901001\n",
      "Epoch 3150, val loss: 0.684417188167572\n",
      "Epoch 3160, training loss: 629.1655883789062 = 0.18969251215457916 + 100 * 6.289759159088135\n",
      "Epoch 3160, val loss: 0.6837711930274963\n",
      "Epoch 3170, training loss: 629.0697631835938 = 0.18789713084697723 + 100 * 6.288818359375\n",
      "Epoch 3170, val loss: 0.6838764548301697\n",
      "Epoch 3180, training loss: 629.64599609375 = 0.1864444464445114 + 100 * 6.294595241546631\n",
      "Epoch 3180, val loss: 0.6841526627540588\n",
      "Epoch 3190, training loss: 629.329833984375 = 0.1845536231994629 + 100 * 6.291452407836914\n",
      "Epoch 3190, val loss: 0.6831426024436951\n",
      "Epoch 3200, training loss: 629.3016967773438 = 0.18277204036712646 + 100 * 6.291188716888428\n",
      "Epoch 3200, val loss: 0.6833808422088623\n",
      "Epoch 3210, training loss: 629.8267211914062 = 0.18107588589191437 + 100 * 6.296456336975098\n",
      "Epoch 3210, val loss: 0.6827203035354614\n",
      "Epoch 3220, training loss: 629.1397705078125 = 0.17932142317295074 + 100 * 6.289604663848877\n",
      "Epoch 3220, val loss: 0.6828823089599609\n",
      "Epoch 3230, training loss: 629.160888671875 = 0.1776597797870636 + 100 * 6.28983211517334\n",
      "Epoch 3230, val loss: 0.6827587485313416\n",
      "Epoch 3240, training loss: 629.3698120117188 = 0.17606373131275177 + 100 * 6.291937351226807\n",
      "Epoch 3240, val loss: 0.6820313334465027\n",
      "Epoch 3250, training loss: 628.9384765625 = 0.1744605153799057 + 100 * 6.28764009475708\n",
      "Epoch 3250, val loss: 0.6825504302978516\n",
      "Epoch 3260, training loss: 629.2940063476562 = 0.17300835251808167 + 100 * 6.291209697723389\n",
      "Epoch 3260, val loss: 0.6829344630241394\n",
      "Epoch 3270, training loss: 629.080078125 = 0.17135712504386902 + 100 * 6.289086818695068\n",
      "Epoch 3270, val loss: 0.6826773285865784\n",
      "Epoch 3280, training loss: 629.1380004882812 = 0.16964764893054962 + 100 * 6.2896833419799805\n",
      "Epoch 3280, val loss: 0.6825124621391296\n",
      "Epoch 3290, training loss: 628.839599609375 = 0.16817481815814972 + 100 * 6.286714553833008\n",
      "Epoch 3290, val loss: 0.682831346988678\n",
      "Epoch 3300, training loss: 629.7415161132812 = 0.16669504344463348 + 100 * 6.295748233795166\n",
      "Epoch 3300, val loss: 0.6822347640991211\n",
      "Epoch 3310, training loss: 629.4893188476562 = 0.16501054167747498 + 100 * 6.293242931365967\n",
      "Epoch 3310, val loss: 0.6833271384239197\n",
      "Epoch 3320, training loss: 629.0580444335938 = 0.1632985770702362 + 100 * 6.288947582244873\n",
      "Epoch 3320, val loss: 0.6818305253982544\n",
      "Epoch 3330, training loss: 629.1271362304688 = 0.1617775708436966 + 100 * 6.289653301239014\n",
      "Epoch 3330, val loss: 0.6824043393135071\n",
      "Epoch 3340, training loss: 628.6386108398438 = 0.16037385165691376 + 100 * 6.2847819328308105\n",
      "Epoch 3340, val loss: 0.6821833252906799\n",
      "Epoch 3350, training loss: 628.53564453125 = 0.15907053649425507 + 100 * 6.28376579284668\n",
      "Epoch 3350, val loss: 0.6827620267868042\n",
      "Epoch 3360, training loss: 629.476318359375 = 0.15787561237812042 + 100 * 6.293184280395508\n",
      "Epoch 3360, val loss: 0.6828553676605225\n",
      "Epoch 3370, training loss: 628.4005126953125 = 0.1561855971813202 + 100 * 6.282443523406982\n",
      "Epoch 3370, val loss: 0.6827487349510193\n",
      "Epoch 3380, training loss: 628.9380493164062 = 0.15482547879219055 + 100 * 6.287831783294678\n",
      "Epoch 3380, val loss: 0.6825704574584961\n",
      "Epoch 3390, training loss: 629.6256103515625 = 0.15347789227962494 + 100 * 6.2947211265563965\n",
      "Epoch 3390, val loss: 0.6823158860206604\n",
      "Epoch 3400, training loss: 629.0135498046875 = 0.15174296498298645 + 100 * 6.288618087768555\n",
      "Epoch 3400, val loss: 0.683034360408783\n",
      "Epoch 3410, training loss: 629.0916137695312 = 0.1504349559545517 + 100 * 6.289411544799805\n",
      "Epoch 3410, val loss: 0.6824460029602051\n",
      "Epoch 3420, training loss: 628.3921508789062 = 0.1490180343389511 + 100 * 6.282431125640869\n",
      "Epoch 3420, val loss: 0.68337482213974\n",
      "Epoch 3430, training loss: 628.9053344726562 = 0.1478356271982193 + 100 * 6.2875752449035645\n",
      "Epoch 3430, val loss: 0.6835644245147705\n",
      "Epoch 3440, training loss: 628.4603881835938 = 0.14648103713989258 + 100 * 6.283138751983643\n",
      "Epoch 3440, val loss: 0.6831680536270142\n",
      "Epoch 3450, training loss: 628.75732421875 = 0.14518630504608154 + 100 * 6.286121368408203\n",
      "Epoch 3450, val loss: 0.683513879776001\n",
      "Epoch 3460, training loss: 628.4696044921875 = 0.14390480518341064 + 100 * 6.283257007598877\n",
      "Epoch 3460, val loss: 0.6836509704589844\n",
      "Epoch 3470, training loss: 628.5087280273438 = 0.14268235862255096 + 100 * 6.283660411834717\n",
      "Epoch 3470, val loss: 0.6842874884605408\n",
      "Epoch 3480, training loss: 628.6795654296875 = 0.14141984283924103 + 100 * 6.285381317138672\n",
      "Epoch 3480, val loss: 0.6847219467163086\n",
      "Epoch 3490, training loss: 628.9532470703125 = 0.14014217257499695 + 100 * 6.288131237030029\n",
      "Epoch 3490, val loss: 0.6841852068901062\n",
      "=== picking the best model according to the performance on validation ===\n",
      "Accuracy of GNN+CL: 0.7814814814814816\n",
      "0.8060094886663153\n",
      "The final CL Acc:0.78519, 0.00524, The final GNN Acc:0.80654, 0.00155\n"
     ]
    }
   ],
   "source": [
    "import copy \n",
    "from model import UnifyModel, Encoder\n",
    "from models.construct import model_construct\n",
    "from construct_graph import *\n",
    "\n",
    "import os.path as osp\n",
    "import random\n",
    "from time import perf_counter as t\n",
    "import yaml\n",
    "from yaml import SafeLoader\n",
    "\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "data = data.to(device)\n",
    "num_class = int(data.y.max()+1)\n",
    "# args.cl_activation = ({'relu': F.relu, 'prelu': nn.PReLU()})[config['activation']]\n",
    "# args.cl_base_model = ({'GCNConv': GCNConv})[config['base_model']]\n",
    "\n",
    "noisy_data = construct_noisy_graph(data,perturb_ratio=args.noisy_level,mode='random_noise')\n",
    "noisy_data = noisy_data.to(device)\n",
    "\n",
    "# diff_dataset = PPRDataset(noisy_data,args.dataset)\n",
    "# diff_noisy_data = diff_dataset.data.to(device)\n",
    "\n",
    "seen_node_idx = (torch.bitwise_not(data.test_mask)).nonzero().flatten()\n",
    "idx_overall_test = (torch.bitwise_not(data.train_mask)&torch.bitwise_not(data.val_mask)).nonzero().flatten()\n",
    "\n",
    "\n",
    "\n",
    "final_cl_acc_noisy = []\n",
    "final_gnn_acc_noisy = []\n",
    "print(\"=== Noisy graph ===\")\n",
    "rs = np.random.RandomState(args.seed)\n",
    "seeds = rs.randint(1000,size=3)\n",
    "for seed in seeds:\n",
    "    np.random.seed(seed)\n",
    "    # torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed(seed)\n",
    "    '''Transductive'''\n",
    "    encoder = Encoder(dataset.num_features, args.cl_num_hidden, args.cl_activation,\n",
    "                            base_model=args.cl_base_model, k=args.cl_num_layers).to(device)\n",
    "    model = UnifyModel(args, encoder, args.cl_num_hidden, args.cl_num_proj_hidden, num_class, args.tau, lr=args.cl_lr, weight_decay=args.cl_weight_decay, device=device).to(device)\n",
    "    # model = UnifyModel(args, encoder, args.cl_num_hidden, args.cl_num_proj_hidden, num_class, args.tau, lr=args.cl_lr, weight_decay=weight_decay, device=device,data1=noisy_data,data2=diff_noisy_data).to(device)\n",
    "    model.fit(args, noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_train,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=None)\n",
    "    # model.fit_1(args, noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_train,idx_val=idx_val,train_iters=num_epochs,cont_iters=num_epochs,seen_node_idx=None)\n",
    "    # x, edge_index,edge_weight,labels,idx_train,idx_val=None,cont_iters=None,train_iters=200,seen_node_idx = None\n",
    "    acc_cl = model.test(noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_clean_test)\n",
    "    print(\"Accuracy of GNN+CL: {}\".format(acc_cl))\n",
    "    final_cl_acc_noisy.append(acc_cl)\n",
    "    gnn_model = model_construct(args,'GCN',noisy_data,device)\n",
    "    gnn_model.fit(noisy_data.x, noisy_data.edge_index, None, noisy_data.y, idx_train, idx_val,train_iters=args.epochs,verbose=False)\n",
    "    clean_acc = gnn_model.test(noisy_data.x,noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_overall_test)\n",
    "    print(clean_acc)\n",
    "    final_gnn_acc_noisy.append(clean_acc)\n",
    "\n",
    "\n",
    "print('The final CL Acc:{:.5f}, {:.5f}, The final GNN Acc:{:.5f}, {:.5f}'\\\n",
    "            .format(np.average(final_cl_acc_noisy),np.std(final_cl_acc_noisy),np.average(final_gnn_acc_noisy),np.std(final_gnn_acc_noisy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method UnifyModel.projection of UnifyModel(\n",
       "  (encoder): Encoder(\n",
       "    (conv): ModuleList(\n",
       "      (0): GCNConv(1433, 256)\n",
       "      (1): GCNConv(256, 128)\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fc1_c): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fc2_c): Linear(in_features=128, out_features=7, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw graph: torch.Size([2, 10556])\n",
      "add edge: torch.Size([2, 13092])\n",
      "remove edge: torch.Size([2, 7878])\n",
      "updated graph: torch.Size([2, 10414])\n",
      "=== Raw graph ===\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 861.644287109375 = 1.962094783782959 + 100 * 8.596821784973145\n",
      "Epoch 0, val loss: 1.9662855863571167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mfl5681/project-contrastive/RobustCL/model.py:224: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.labels = torch.tensor(labels, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, training loss: 861.5540161132812 = 1.9520117044448853 + 100 * 8.596019744873047\n",
      "Epoch 10, val loss: 1.9556394815444946\n",
      "Epoch 20, training loss: 861.0609741210938 = 1.939118504524231 + 100 * 8.591217994689941\n",
      "Epoch 20, val loss: 1.9420093297958374\n",
      "Epoch 30, training loss: 858.0274658203125 = 1.9224451780319214 + 100 * 8.561050415039062\n",
      "Epoch 30, val loss: 1.9245028495788574\n",
      "Epoch 40, training loss: 839.6718139648438 = 1.9022588729858398 + 100 * 8.377695083618164\n",
      "Epoch 40, val loss: 1.904028058052063\n",
      "Epoch 50, training loss: 761.4630737304688 = 1.8792352676391602 + 100 * 7.59583854675293\n",
      "Epoch 50, val loss: 1.8807604312896729\n",
      "Epoch 60, training loss: 730.5070190429688 = 1.8613781929016113 + 100 * 7.28645658493042\n",
      "Epoch 60, val loss: 1.8645652532577515\n",
      "Epoch 70, training loss: 710.8011474609375 = 1.848585605621338 + 100 * 7.0895256996154785\n",
      "Epoch 70, val loss: 1.8523708581924438\n",
      "Epoch 80, training loss: 697.0316162109375 = 1.8360241651535034 + 100 * 6.951956272125244\n",
      "Epoch 80, val loss: 1.8408387899398804\n",
      "Epoch 90, training loss: 687.4520874023438 = 1.825295090675354 + 100 * 6.856267929077148\n",
      "Epoch 90, val loss: 1.831041693687439\n",
      "Epoch 100, training loss: 679.5516967773438 = 1.8157169818878174 + 100 * 6.777359485626221\n",
      "Epoch 100, val loss: 1.8220014572143555\n",
      "Epoch 110, training loss: 674.1467895507812 = 1.807191252708435 + 100 * 6.723396301269531\n",
      "Epoch 110, val loss: 1.813523769378662\n",
      "Epoch 120, training loss: 669.7814331054688 = 1.7988237142562866 + 100 * 6.679825782775879\n",
      "Epoch 120, val loss: 1.80497407913208\n",
      "Epoch 130, training loss: 666.4983520507812 = 1.7906441688537598 + 100 * 6.6470770835876465\n",
      "Epoch 130, val loss: 1.796532392501831\n",
      "Epoch 140, training loss: 663.9078369140625 = 1.7824809551239014 + 100 * 6.621253967285156\n",
      "Epoch 140, val loss: 1.7879313230514526\n",
      "Epoch 150, training loss: 661.3467407226562 = 1.7740931510925293 + 100 * 6.595726013183594\n",
      "Epoch 150, val loss: 1.7790043354034424\n",
      "Epoch 160, training loss: 659.0733642578125 = 1.765576720237732 + 100 * 6.573078155517578\n",
      "Epoch 160, val loss: 1.769971251487732\n",
      "Epoch 170, training loss: 657.1400146484375 = 1.7567615509033203 + 100 * 6.553832530975342\n",
      "Epoch 170, val loss: 1.7608487606048584\n",
      "Epoch 180, training loss: 655.0401000976562 = 1.7476128339767456 + 100 * 6.532924652099609\n",
      "Epoch 180, val loss: 1.7514877319335938\n",
      "Epoch 190, training loss: 653.4053344726562 = 1.7379696369171143 + 100 * 6.516673564910889\n",
      "Epoch 190, val loss: 1.7417694330215454\n",
      "Epoch 200, training loss: 651.93701171875 = 1.7274413108825684 + 100 * 6.502095699310303\n",
      "Epoch 200, val loss: 1.7313748598098755\n",
      "Epoch 210, training loss: 650.5963745117188 = 1.7160470485687256 + 100 * 6.488803386688232\n",
      "Epoch 210, val loss: 1.72016179561615\n",
      "Epoch 220, training loss: 649.3917846679688 = 1.7037297487258911 + 100 * 6.4768805503845215\n",
      "Epoch 220, val loss: 1.7082310914993286\n",
      "Epoch 230, training loss: 648.361083984375 = 1.6904914379119873 + 100 * 6.466705799102783\n",
      "Epoch 230, val loss: 1.695399284362793\n",
      "Epoch 240, training loss: 647.592041015625 = 1.676134705543518 + 100 * 6.459158897399902\n",
      "Epoch 240, val loss: 1.6815961599349976\n",
      "Epoch 250, training loss: 646.5584716796875 = 1.6607218980789185 + 100 * 6.448977947235107\n",
      "Epoch 250, val loss: 1.6668637990951538\n",
      "Epoch 260, training loss: 645.6998901367188 = 1.6442391872406006 + 100 * 6.440556526184082\n",
      "Epoch 260, val loss: 1.651158094406128\n",
      "Epoch 270, training loss: 645.0428466796875 = 1.6265718936920166 + 100 * 6.434162616729736\n",
      "Epoch 270, val loss: 1.6344703435897827\n",
      "Epoch 280, training loss: 644.27392578125 = 1.6077070236206055 + 100 * 6.426661968231201\n",
      "Epoch 280, val loss: 1.6166326999664307\n",
      "Epoch 290, training loss: 643.6007690429688 = 1.5877104997634888 + 100 * 6.420130729675293\n",
      "Epoch 290, val loss: 1.5978906154632568\n",
      "Epoch 300, training loss: 643.0278930664062 = 1.5665810108184814 + 100 * 6.414613246917725\n",
      "Epoch 300, val loss: 1.5782240629196167\n",
      "Epoch 310, training loss: 642.4826049804688 = 1.5442839860916138 + 100 * 6.4093828201293945\n",
      "Epoch 310, val loss: 1.5574852228164673\n",
      "Epoch 320, training loss: 641.80859375 = 1.5211156606674194 + 100 * 6.402874946594238\n",
      "Epoch 320, val loss: 1.5360872745513916\n",
      "Epoch 330, training loss: 641.3373413085938 = 1.4970778226852417 + 100 * 6.398402690887451\n",
      "Epoch 330, val loss: 1.5140480995178223\n",
      "Epoch 340, training loss: 640.8582153320312 = 1.4721163511276245 + 100 * 6.393861293792725\n",
      "Epoch 340, val loss: 1.4912132024765015\n",
      "Epoch 350, training loss: 640.32373046875 = 1.4465010166168213 + 100 * 6.388772487640381\n",
      "Epoch 350, val loss: 1.4679700136184692\n",
      "Epoch 360, training loss: 639.826416015625 = 1.4203037023544312 + 100 * 6.384061336517334\n",
      "Epoch 360, val loss: 1.4444465637207031\n",
      "Epoch 370, training loss: 639.5508422851562 = 1.3937230110168457 + 100 * 6.381571292877197\n",
      "Epoch 370, val loss: 1.4205306768417358\n",
      "Epoch 380, training loss: 638.9314575195312 = 1.3667057752609253 + 100 * 6.37564754486084\n",
      "Epoch 380, val loss: 1.396637201309204\n",
      "Epoch 390, training loss: 638.4680786132812 = 1.339647650718689 + 100 * 6.371284008026123\n",
      "Epoch 390, val loss: 1.3728108406066895\n",
      "Epoch 400, training loss: 638.23974609375 = 1.3125503063201904 + 100 * 6.369271755218506\n",
      "Epoch 400, val loss: 1.349084496498108\n",
      "Epoch 410, training loss: 637.6969604492188 = 1.2856323719024658 + 100 * 6.3641133308410645\n",
      "Epoch 410, val loss: 1.3257800340652466\n",
      "Epoch 420, training loss: 637.5006713867188 = 1.258978247642517 + 100 * 6.362417221069336\n",
      "Epoch 420, val loss: 1.3026783466339111\n",
      "Epoch 430, training loss: 636.9859008789062 = 1.2325109243392944 + 100 * 6.357534408569336\n",
      "Epoch 430, val loss: 1.2801238298416138\n",
      "Epoch 440, training loss: 636.5913696289062 = 1.2066129446029663 + 100 * 6.353847503662109\n",
      "Epoch 440, val loss: 1.2582811117172241\n",
      "Epoch 450, training loss: 636.409912109375 = 1.181168556213379 + 100 * 6.352287769317627\n",
      "Epoch 450, val loss: 1.2369909286499023\n",
      "Epoch 460, training loss: 635.9525756835938 = 1.1563199758529663 + 100 * 6.347962856292725\n",
      "Epoch 460, val loss: 1.2164385318756104\n",
      "Epoch 470, training loss: 635.5538940429688 = 1.132115125656128 + 100 * 6.344217300415039\n",
      "Epoch 470, val loss: 1.1967854499816895\n",
      "Epoch 480, training loss: 635.1766357421875 = 1.1086210012435913 + 100 * 6.340680122375488\n",
      "Epoch 480, val loss: 1.1778860092163086\n",
      "Epoch 490, training loss: 634.95947265625 = 1.0859185457229614 + 100 * 6.338735580444336\n",
      "Epoch 490, val loss: 1.1599267721176147\n",
      "Epoch 500, training loss: 634.8842163085938 = 1.063576579093933 + 100 * 6.3382062911987305\n",
      "Epoch 500, val loss: 1.1423829793930054\n",
      "Epoch 510, training loss: 634.2880249023438 = 1.0420455932617188 + 100 * 6.332459449768066\n",
      "Epoch 510, val loss: 1.1258277893066406\n",
      "Epoch 520, training loss: 633.9965209960938 = 1.021382451057434 + 100 * 6.329751491546631\n",
      "Epoch 520, val loss: 1.11040461063385\n",
      "Epoch 530, training loss: 634.06298828125 = 1.0014241933822632 + 100 * 6.330615997314453\n",
      "Epoch 530, val loss: 1.0956507921218872\n",
      "Epoch 540, training loss: 633.9052734375 = 0.9818752408027649 + 100 * 6.3292341232299805\n",
      "Epoch 540, val loss: 1.0815705060958862\n",
      "Epoch 550, training loss: 633.365234375 = 0.9630163311958313 + 100 * 6.32402229309082\n",
      "Epoch 550, val loss: 1.0682185888290405\n",
      "Epoch 560, training loss: 633.0078125 = 0.9448854923248291 + 100 * 6.320629119873047\n",
      "Epoch 560, val loss: 1.0557570457458496\n",
      "Epoch 570, training loss: 633.3028564453125 = 0.9273720979690552 + 100 * 6.323754787445068\n",
      "Epoch 570, val loss: 1.043984055519104\n",
      "Epoch 580, training loss: 632.7069702148438 = 0.9099910855293274 + 100 * 6.317969799041748\n",
      "Epoch 580, val loss: 1.0323840379714966\n",
      "Epoch 590, training loss: 632.388671875 = 0.8932880759239197 + 100 * 6.314953327178955\n",
      "Epoch 590, val loss: 1.0216765403747559\n",
      "Epoch 600, training loss: 632.1367797851562 = 0.8771835565567017 + 100 * 6.312595844268799\n",
      "Epoch 600, val loss: 1.0115809440612793\n",
      "Epoch 610, training loss: 631.9033203125 = 0.861522376537323 + 100 * 6.310418128967285\n",
      "Epoch 610, val loss: 1.0019768476486206\n",
      "Epoch 620, training loss: 632.370361328125 = 0.8462414145469666 + 100 * 6.31524133682251\n",
      "Epoch 620, val loss: 0.9927024841308594\n",
      "Epoch 630, training loss: 631.7825927734375 = 0.8309228420257568 + 100 * 6.309516906738281\n",
      "Epoch 630, val loss: 0.9837766289710999\n",
      "Epoch 640, training loss: 631.4234008789062 = 0.8160586953163147 + 100 * 6.3060736656188965\n",
      "Epoch 640, val loss: 0.9750566482543945\n",
      "Epoch 650, training loss: 631.1806030273438 = 0.8017409443855286 + 100 * 6.303788185119629\n",
      "Epoch 650, val loss: 0.9670875668525696\n",
      "Epoch 660, training loss: 630.9603881835938 = 0.7877512574195862 + 100 * 6.3017258644104\n",
      "Epoch 660, val loss: 0.9595322012901306\n",
      "Epoch 670, training loss: 631.1371459960938 = 0.7740768790245056 + 100 * 6.303630828857422\n",
      "Epoch 670, val loss: 0.9526289105415344\n",
      "Epoch 680, training loss: 631.2088623046875 = 0.7601678371429443 + 100 * 6.3044867515563965\n",
      "Epoch 680, val loss: 0.944625735282898\n",
      "Epoch 690, training loss: 630.6681518554688 = 0.7467218637466431 + 100 * 6.2992143630981445\n",
      "Epoch 690, val loss: 0.9378786087036133\n",
      "Epoch 700, training loss: 630.4363403320312 = 0.7336817979812622 + 100 * 6.29702615737915\n",
      "Epoch 700, val loss: 0.9315248131752014\n",
      "Epoch 710, training loss: 630.2048950195312 = 0.7208901047706604 + 100 * 6.294840335845947\n",
      "Epoch 710, val loss: 0.9254133105278015\n",
      "Epoch 720, training loss: 630.0648193359375 = 0.708318829536438 + 100 * 6.293565273284912\n",
      "Epoch 720, val loss: 0.9195829033851624\n",
      "Epoch 730, training loss: 630.1180419921875 = 0.6958944201469421 + 100 * 6.294220924377441\n",
      "Epoch 730, val loss: 0.9138956069946289\n",
      "Epoch 740, training loss: 630.2467651367188 = 0.6836580634117126 + 100 * 6.295631408691406\n",
      "Epoch 740, val loss: 0.9084216356277466\n",
      "Epoch 750, training loss: 629.7432250976562 = 0.67139732837677 + 100 * 6.2907185554504395\n",
      "Epoch 750, val loss: 0.9030638337135315\n",
      "Epoch 760, training loss: 629.7554931640625 = 0.659441351890564 + 100 * 6.290960788726807\n",
      "Epoch 760, val loss: 0.8978237509727478\n",
      "Epoch 770, training loss: 629.3583374023438 = 0.6477389335632324 + 100 * 6.287105560302734\n",
      "Epoch 770, val loss: 0.8930339813232422\n",
      "Epoch 780, training loss: 629.23974609375 = 0.6362759470939636 + 100 * 6.28603458404541\n",
      "Epoch 780, val loss: 0.8885634541511536\n",
      "Epoch 790, training loss: 629.076416015625 = 0.6249932050704956 + 100 * 6.284514427185059\n",
      "Epoch 790, val loss: 0.8842433094978333\n",
      "Epoch 800, training loss: 629.4927978515625 = 0.6138454079627991 + 100 * 6.288789749145508\n",
      "Epoch 800, val loss: 0.879993736743927\n",
      "Epoch 810, training loss: 629.1080932617188 = 0.6025771498680115 + 100 * 6.285055160522461\n",
      "Epoch 810, val loss: 0.875501275062561\n",
      "Epoch 820, training loss: 628.7084350585938 = 0.5916978120803833 + 100 * 6.281167507171631\n",
      "Epoch 820, val loss: 0.8720659017562866\n",
      "Epoch 830, training loss: 628.6825561523438 = 0.5810576677322388 + 100 * 6.281014919281006\n",
      "Epoch 830, val loss: 0.8685292601585388\n",
      "Epoch 840, training loss: 628.8558959960938 = 0.5704875588417053 + 100 * 6.282854080200195\n",
      "Epoch 840, val loss: 0.8650528788566589\n",
      "Epoch 850, training loss: 628.4564208984375 = 0.559902012348175 + 100 * 6.278965473175049\n",
      "Epoch 850, val loss: 0.861520528793335\n",
      "Epoch 860, training loss: 628.281494140625 = 0.5496060848236084 + 100 * 6.277318954467773\n",
      "Epoch 860, val loss: 0.8585242629051208\n",
      "Epoch 870, training loss: 628.19140625 = 0.5395238399505615 + 100 * 6.27651834487915\n",
      "Epoch 870, val loss: 0.8555909991264343\n",
      "Epoch 880, training loss: 628.5211181640625 = 0.5295364260673523 + 100 * 6.279915809631348\n",
      "Epoch 880, val loss: 0.8526245355606079\n",
      "Epoch 890, training loss: 628.409423828125 = 0.5195180773735046 + 100 * 6.278899192810059\n",
      "Epoch 890, val loss: 0.8501153588294983\n",
      "Epoch 900, training loss: 627.9663696289062 = 0.5096813440322876 + 100 * 6.274566650390625\n",
      "Epoch 900, val loss: 0.8476756811141968\n",
      "Epoch 910, training loss: 627.7903442382812 = 0.5000954270362854 + 100 * 6.272902011871338\n",
      "Epoch 910, val loss: 0.8452358245849609\n",
      "Epoch 920, training loss: 628.3136596679688 = 0.4905988872051239 + 100 * 6.278230667114258\n",
      "Epoch 920, val loss: 0.8428605198860168\n",
      "Epoch 930, training loss: 627.8837890625 = 0.4811084270477295 + 100 * 6.274027347564697\n",
      "Epoch 930, val loss: 0.8412899374961853\n",
      "Epoch 940, training loss: 627.537353515625 = 0.4718198776245117 + 100 * 6.270655632019043\n",
      "Epoch 940, val loss: 0.8395143151283264\n",
      "Epoch 950, training loss: 627.4616088867188 = 0.4627259075641632 + 100 * 6.269989013671875\n",
      "Epoch 950, val loss: 0.8377026319503784\n",
      "Epoch 960, training loss: 627.7134399414062 = 0.453785240650177 + 100 * 6.27259635925293\n",
      "Epoch 960, val loss: 0.8361557722091675\n",
      "Epoch 970, training loss: 627.3477172851562 = 0.444847971200943 + 100 * 6.269029140472412\n",
      "Epoch 970, val loss: 0.8348276019096375\n",
      "Epoch 980, training loss: 627.3309326171875 = 0.43607696890830994 + 100 * 6.268948554992676\n",
      "Epoch 980, val loss: 0.833620011806488\n",
      "Epoch 990, training loss: 627.452392578125 = 0.4274100363254547 + 100 * 6.270249366760254\n",
      "Epoch 990, val loss: 0.8327633142471313\n",
      "=== picking the best model according to the performance on validation ===\n",
      "Accuracy of GNN+CL: 0.725925925925926\n",
      "0.8323668950975225\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 861.6256103515625 = 1.9400644302368164 + 100 * 8.596855163574219\n",
      "Epoch 0, val loss: 1.937163233757019\n",
      "Epoch 10, training loss: 861.558349609375 = 1.9318561553955078 + 100 * 8.596264839172363\n",
      "Epoch 10, val loss: 1.9292261600494385\n",
      "Epoch 20, training loss: 861.1806030273438 = 1.9214622974395752 + 100 * 8.592591285705566\n",
      "Epoch 20, val loss: 1.9189276695251465\n",
      "Epoch 30, training loss: 858.8081665039062 = 1.9076519012451172 + 100 * 8.569005012512207\n",
      "Epoch 30, val loss: 1.90499746799469\n",
      "Epoch 40, training loss: 846.364013671875 = 1.889906406402588 + 100 * 8.444741249084473\n",
      "Epoch 40, val loss: 1.8875696659088135\n",
      "Epoch 50, training loss: 796.50732421875 = 1.8686753511428833 + 100 * 7.946386814117432\n",
      "Epoch 50, val loss: 1.8671156167984009\n",
      "Epoch 60, training loss: 755.0669555664062 = 1.849339485168457 + 100 * 7.5321760177612305\n",
      "Epoch 60, val loss: 1.849808692932129\n",
      "Epoch 70, training loss: 724.7876586914062 = 1.8371189832687378 + 100 * 7.22950553894043\n",
      "Epoch 70, val loss: 1.838728904724121\n",
      "Epoch 80, training loss: 708.0046997070312 = 1.8260066509246826 + 100 * 7.061787128448486\n",
      "Epoch 80, val loss: 1.8283326625823975\n",
      "Epoch 90, training loss: 694.9285278320312 = 1.813997507095337 + 100 * 6.931145191192627\n",
      "Epoch 90, val loss: 1.8174866437911987\n",
      "Epoch 100, training loss: 684.7422485351562 = 1.8036574125289917 + 100 * 6.829385757446289\n",
      "Epoch 100, val loss: 1.8082479238510132\n",
      "Epoch 110, training loss: 678.2771606445312 = 1.7938802242279053 + 100 * 6.764832973480225\n",
      "Epoch 110, val loss: 1.799150824546814\n",
      "Epoch 120, training loss: 673.3226928710938 = 1.7830561399459839 + 100 * 6.715395927429199\n",
      "Epoch 120, val loss: 1.7888263463974\n",
      "Epoch 130, training loss: 669.338623046875 = 1.7717173099517822 + 100 * 6.675668716430664\n",
      "Epoch 130, val loss: 1.7781692743301392\n",
      "Epoch 140, training loss: 666.0255126953125 = 1.760380744934082 + 100 * 6.642651557922363\n",
      "Epoch 140, val loss: 1.7675225734710693\n",
      "Epoch 150, training loss: 663.3056640625 = 1.748305082321167 + 100 * 6.615573883056641\n",
      "Epoch 150, val loss: 1.7562090158462524\n",
      "Epoch 160, training loss: 660.9415893554688 = 1.735390305519104 + 100 * 6.592061996459961\n",
      "Epoch 160, val loss: 1.7441933155059814\n",
      "Epoch 170, training loss: 658.536865234375 = 1.7214373350143433 + 100 * 6.568154335021973\n",
      "Epoch 170, val loss: 1.7312922477722168\n",
      "Epoch 180, training loss: 656.5111694335938 = 1.7063114643096924 + 100 * 6.548048973083496\n",
      "Epoch 180, val loss: 1.717444658279419\n",
      "Epoch 190, training loss: 654.9179077148438 = 1.6898634433746338 + 100 * 6.532279968261719\n",
      "Epoch 190, val loss: 1.702542781829834\n",
      "Epoch 200, training loss: 653.2361450195312 = 1.6719268560409546 + 100 * 6.515642166137695\n",
      "Epoch 200, val loss: 1.68628990650177\n",
      "Epoch 210, training loss: 651.8942260742188 = 1.6526137590408325 + 100 * 6.502416133880615\n",
      "Epoch 210, val loss: 1.6690404415130615\n",
      "Epoch 220, training loss: 650.7894897460938 = 1.6319175958633423 + 100 * 6.491575717926025\n",
      "Epoch 220, val loss: 1.6506602764129639\n",
      "Epoch 230, training loss: 649.56298828125 = 1.6097544431686401 + 100 * 6.479532241821289\n",
      "Epoch 230, val loss: 1.6311829090118408\n",
      "Epoch 240, training loss: 648.5890502929688 = 1.5864372253417969 + 100 * 6.47002649307251\n",
      "Epoch 240, val loss: 1.6108583211898804\n",
      "Epoch 250, training loss: 647.7928466796875 = 1.5620551109313965 + 100 * 6.462307929992676\n",
      "Epoch 250, val loss: 1.5898971557617188\n",
      "Epoch 260, training loss: 646.763916015625 = 1.5367298126220703 + 100 * 6.452271461486816\n",
      "Epoch 260, val loss: 1.5684722661972046\n",
      "Epoch 270, training loss: 645.9243774414062 = 1.5108585357666016 + 100 * 6.4441351890563965\n",
      "Epoch 270, val loss: 1.546892523765564\n",
      "Epoch 280, training loss: 645.5167846679688 = 1.484607219696045 + 100 * 6.440321445465088\n",
      "Epoch 280, val loss: 1.5253788232803345\n",
      "Epoch 290, training loss: 644.4767456054688 = 1.458116888999939 + 100 * 6.4301862716674805\n",
      "Epoch 290, val loss: 1.5041255950927734\n",
      "Epoch 300, training loss: 643.72802734375 = 1.431667685508728 + 100 * 6.422964096069336\n",
      "Epoch 300, val loss: 1.4833946228027344\n",
      "Epoch 310, training loss: 643.0540161132812 = 1.4054725170135498 + 100 * 6.416485786437988\n",
      "Epoch 310, val loss: 1.4632140398025513\n",
      "Epoch 320, training loss: 642.5584106445312 = 1.379578948020935 + 100 * 6.411788463592529\n",
      "Epoch 320, val loss: 1.4437544345855713\n",
      "Epoch 330, training loss: 641.8997802734375 = 1.3540302515029907 + 100 * 6.405457973480225\n",
      "Epoch 330, val loss: 1.4248886108398438\n",
      "Epoch 340, training loss: 641.4293212890625 = 1.328943133354187 + 100 * 6.401004314422607\n",
      "Epoch 340, val loss: 1.4068434238433838\n",
      "Epoch 350, training loss: 640.7667236328125 = 1.3042826652526855 + 100 * 6.394624710083008\n",
      "Epoch 350, val loss: 1.3893426656723022\n",
      "Epoch 360, training loss: 640.5762329101562 = 1.279984951019287 + 100 * 6.392962455749512\n",
      "Epoch 360, val loss: 1.3725305795669556\n",
      "Epoch 370, training loss: 639.7859497070312 = 1.2560510635375977 + 100 * 6.385299205780029\n",
      "Epoch 370, val loss: 1.356040358543396\n",
      "Epoch 380, training loss: 639.291015625 = 1.232496738433838 + 100 * 6.380585193634033\n",
      "Epoch 380, val loss: 1.3399512767791748\n",
      "Epoch 390, training loss: 639.2095336914062 = 1.2092833518981934 + 100 * 6.380002498626709\n",
      "Epoch 390, val loss: 1.3243436813354492\n",
      "Epoch 400, training loss: 638.4850463867188 = 1.186273455619812 + 100 * 6.372987747192383\n",
      "Epoch 400, val loss: 1.3088674545288086\n",
      "Epoch 410, training loss: 637.9954833984375 = 1.1635804176330566 + 100 * 6.368319034576416\n",
      "Epoch 410, val loss: 1.2936829328536987\n",
      "Epoch 420, training loss: 637.7472534179688 = 1.1412807703018188 + 100 * 6.366059303283691\n",
      "Epoch 420, val loss: 1.2788461446762085\n",
      "Epoch 430, training loss: 637.5863037109375 = 1.1190540790557861 + 100 * 6.3646721839904785\n",
      "Epoch 430, val loss: 1.264147400856018\n",
      "Epoch 440, training loss: 636.9344482421875 = 1.0972316265106201 + 100 * 6.358372211456299\n",
      "Epoch 440, val loss: 1.2497097253799438\n",
      "Epoch 450, training loss: 636.5270385742188 = 1.0758075714111328 + 100 * 6.3545122146606445\n",
      "Epoch 450, val loss: 1.235626459121704\n",
      "Epoch 460, training loss: 636.1124877929688 = 1.0546503067016602 + 100 * 6.350578784942627\n",
      "Epoch 460, val loss: 1.2216525077819824\n",
      "Epoch 470, training loss: 635.7966918945312 = 1.0338270664215088 + 100 * 6.347628593444824\n",
      "Epoch 470, val loss: 1.2081234455108643\n",
      "Epoch 480, training loss: 635.850341796875 = 1.0133090019226074 + 100 * 6.34837007522583\n",
      "Epoch 480, val loss: 1.1946362257003784\n",
      "Epoch 490, training loss: 635.3618774414062 = 0.99311363697052 + 100 * 6.343688011169434\n",
      "Epoch 490, val loss: 1.1815155744552612\n",
      "Epoch 500, training loss: 635.1040649414062 = 0.9732193350791931 + 100 * 6.34130859375\n",
      "Epoch 500, val loss: 1.1686975955963135\n",
      "Epoch 510, training loss: 634.6011352539062 = 0.9536280632019043 + 100 * 6.336475372314453\n",
      "Epoch 510, val loss: 1.155906319618225\n",
      "Epoch 520, training loss: 634.3981323242188 = 0.9343708157539368 + 100 * 6.334637641906738\n",
      "Epoch 520, val loss: 1.1434568166732788\n",
      "Epoch 530, training loss: 634.11083984375 = 0.9154025912284851 + 100 * 6.331954479217529\n",
      "Epoch 530, val loss: 1.1311249732971191\n",
      "Epoch 540, training loss: 634.3854370117188 = 0.896727979183197 + 100 * 6.3348870277404785\n",
      "Epoch 540, val loss: 1.1191990375518799\n",
      "Epoch 550, training loss: 633.8603515625 = 0.8780961632728577 + 100 * 6.329822540283203\n",
      "Epoch 550, val loss: 1.1071574687957764\n",
      "Epoch 560, training loss: 633.3800048828125 = 0.8599026203155518 + 100 * 6.325201034545898\n",
      "Epoch 560, val loss: 1.0954796075820923\n",
      "Epoch 570, training loss: 633.0386352539062 = 0.8420446515083313 + 100 * 6.32196569442749\n",
      "Epoch 570, val loss: 1.0840187072753906\n",
      "Epoch 580, training loss: 633.2166748046875 = 0.8244935870170593 + 100 * 6.3239216804504395\n",
      "Epoch 580, val loss: 1.0727254152297974\n",
      "Epoch 590, training loss: 632.78955078125 = 0.807065486907959 + 100 * 6.319824695587158\n",
      "Epoch 590, val loss: 1.0618200302124023\n",
      "Epoch 600, training loss: 632.4755249023438 = 0.7899541258811951 + 100 * 6.316855430603027\n",
      "Epoch 600, val loss: 1.0508100986480713\n",
      "Epoch 610, training loss: 633.3728637695312 = 0.7731065154075623 + 100 * 6.325997352600098\n",
      "Epoch 610, val loss: 1.039899468421936\n",
      "Epoch 620, training loss: 632.2355346679688 = 0.7563152313232422 + 100 * 6.314792633056641\n",
      "Epoch 620, val loss: 1.029582142829895\n",
      "Epoch 630, training loss: 631.87548828125 = 0.7398608326911926 + 100 * 6.311356067657471\n",
      "Epoch 630, val loss: 1.0191726684570312\n",
      "Epoch 640, training loss: 631.612548828125 = 0.723746120929718 + 100 * 6.308887958526611\n",
      "Epoch 640, val loss: 1.0090540647506714\n",
      "Epoch 650, training loss: 631.3936767578125 = 0.7078512907028198 + 100 * 6.306858539581299\n",
      "Epoch 650, val loss: 0.999161422252655\n",
      "Epoch 660, training loss: 632.3093872070312 = 0.6921839714050293 + 100 * 6.316171646118164\n",
      "Epoch 660, val loss: 0.9894756078720093\n",
      "Epoch 670, training loss: 631.218017578125 = 0.676463782787323 + 100 * 6.305415630340576\n",
      "Epoch 670, val loss: 0.9797593951225281\n",
      "Epoch 680, training loss: 631.1270141601562 = 0.6610329151153564 + 100 * 6.304659843444824\n",
      "Epoch 680, val loss: 0.9701759815216064\n",
      "Epoch 690, training loss: 630.7429809570312 = 0.6458085775375366 + 100 * 6.300971508026123\n",
      "Epoch 690, val loss: 0.961169421672821\n",
      "Epoch 700, training loss: 631.1717529296875 = 0.6307500004768372 + 100 * 6.305409908294678\n",
      "Epoch 700, val loss: 0.9519364237785339\n",
      "Epoch 710, training loss: 630.5950317382812 = 0.6158049702644348 + 100 * 6.299792289733887\n",
      "Epoch 710, val loss: 0.9430593848228455\n",
      "Epoch 720, training loss: 630.30322265625 = 0.6010223031044006 + 100 * 6.297021865844727\n",
      "Epoch 720, val loss: 0.9343473315238953\n",
      "Epoch 730, training loss: 630.4652099609375 = 0.586448073387146 + 100 * 6.298787593841553\n",
      "Epoch 730, val loss: 0.9256606101989746\n",
      "Epoch 740, training loss: 630.027587890625 = 0.5719341039657593 + 100 * 6.294556140899658\n",
      "Epoch 740, val loss: 0.9171733856201172\n",
      "Epoch 750, training loss: 630.0643310546875 = 0.5575931668281555 + 100 * 6.295067310333252\n",
      "Epoch 750, val loss: 0.908751904964447\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mfl5681/project-contrastive/RobustCL/run_pre_noisy.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_pre_noisy.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m encoder \u001b[39m=\u001b[39m Encoder(dataset\u001b[39m.\u001b[39mnum_features, args\u001b[39m.\u001b[39mcl_num_hidden, args\u001b[39m.\u001b[39mcl_activation,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_pre_noisy.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m                         base_model\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mcl_base_model, k\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mcl_num_layers)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_pre_noisy.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m model \u001b[39m=\u001b[39m UnifyModel(args, encoder, args\u001b[39m.\u001b[39mcl_num_hidden, args\u001b[39m.\u001b[39mcl_num_proj_hidden, num_class, args\u001b[39m.\u001b[39mtau, lr\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mcl_lr, weight_decay\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mcl_weight_decay, device\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_pre_noisy.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(args, data\u001b[39m.\u001b[39;49mx, data\u001b[39m.\u001b[39;49medge_index,data\u001b[39m.\u001b[39;49medge_weight,data\u001b[39m.\u001b[39;49my,idx_train,idx_val\u001b[39m=\u001b[39;49midx_val,train_iters\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mcl_num_epochs,cont_iters\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mcl_num_epochs,seen_node_idx\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_pre_noisy.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39m# x, edge_index,edge_weight,labels,idx_train,idx_val=None,cont_iters=None,train_iters=200,seen_node_idx = None\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_pre_noisy.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m acc_cl \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtest(data\u001b[39m.\u001b[39mx, data\u001b[39m.\u001b[39medge_index,data\u001b[39m.\u001b[39medge_weight,data\u001b[39m.\u001b[39my,idx_clean_test)\n",
      "File \u001b[0;32m~/project-contrastive/RobustCL/model.py:232\u001b[0m, in \u001b[0;36mUnifyModel.fit\u001b[0;34m(self, args, x, edge_index, edge_weight, labels, idx_train, idx_val, train_iters, cont_iters, seen_node_idx)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_without_val(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels, idx_train, train_iters,verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    231\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 232\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_with_val(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabels, idx_train, idx_val, train_iters)\n",
      "File \u001b[0;32m~/project-contrastive/RobustCL/model.py:311\u001b[0m, in \u001b[0;36mUnifyModel._train_with_val\u001b[0;34m(self, labels, idx_train, idx_val, train_iters, verbose)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39mif\u001b[39;00m verbose \u001b[39mand\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    310\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, training loss: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m = \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m + \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m * \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(i, loss\u001b[39m.\u001b[39mitem(),clf_loss\u001b[39m.\u001b[39mitem(),\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcont_weight,cont_loss\u001b[39m.\u001b[39mitem()))\n\u001b[0;32m--> 311\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    312\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    315\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import copy \n",
    "from model import UnifyModel\n",
    "from models.construct import model_construct\n",
    "\n",
    "\n",
    "import os.path as osp\n",
    "import random\n",
    "from time import perf_counter as t\n",
    "import yaml\n",
    "from yaml import SafeLoader\n",
    "\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from model import Encoder, Model, drop_feature\n",
    "\n",
    "data = data.to(device)\n",
    "config = yaml.load(open(args.config), Loader=SafeLoader)[args.dataset]\n",
    "# num_epochs = config['num_epochs']\n",
    "# args.cl_lr = config['args.cl_lr']\n",
    "# weight_decay = config['weight_decay']\n",
    "# args.seed = config['seed']\n",
    "# args.cont_batch_size = config['cont_batch_size']\n",
    "# args.cont_weight = config['cont_weight']\n",
    "# args.add_edge_rate_1 = config['add_edge_rate_1']\n",
    "# args.add_edge_rate_2 = config['add_edge_rate_2']\n",
    "# args.drop_edge_rate_1 = config['drop_edge_rate_1']\n",
    "# args.drop_edge_rate_2 = config['drop_edge_rate_2']\n",
    "# args.drop_feat_rate_1 = config['drop_feature_rate_1']\n",
    "# args.drop_feat_rate_2 = config['drop_feature_rate_2']\n",
    "# args.add_edge_rate_1 = 0\n",
    "# args.add_edge_rate_2 = 0\n",
    "# args.drop_edge_rate_1 = 0.3\n",
    "# args.drop_edge_rate_2 = 0.5\n",
    "# args.drop_feat_rate_1 = 0.4\n",
    "# args.drop_feat_rate_2 = 0.4\n",
    "num_class = int(data.y.max()+1)\n",
    "\n",
    "noisy_data = construct_noisy_graph(data,perturb_ratio=0.25,mode='random_noise')\n",
    "noisy_data = noisy_data.to(device)\n",
    "seen_node_idx = (torch.bitwise_not(data.test_mask)).nonzero().flatten()\n",
    "idx_overall_test = (torch.bitwise_not(data.train_mask)&torch.bitwise_not(data.val_mask)).nonzero().flatten()\n",
    "\n",
    "\n",
    "final_cl_acc = []\n",
    "final_gnn_acc = []\n",
    "print(\"=== Raw graph ===\")\n",
    "rs = np.random.RandomState(args.seed)\n",
    "seeds = rs.randint(1000,size=3)\n",
    "for seed in seeds:\n",
    "    np.random.seed(seed)\n",
    "    # torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed(seed)\n",
    "    '''Transductive'''\n",
    "    encoder = Encoder(dataset.num_features, args.cl_num_hidden, args.cl_activation,\n",
    "                            base_model=args.cl_base_model, k=args.cl_num_layers).to(device)\n",
    "    model = UnifyModel(args, encoder, args.cl_num_hidden, args.cl_num_proj_hidden, num_class, args.tau, lr=args.cl_lr, weight_decay=args.cl_weight_decay, device=None).to(device)\n",
    "    model.fit(args, data.x, data.edge_index,data.edge_weight,data.y,idx_train,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=None)\n",
    "    # x, edge_index,edge_weight,labels,idx_train,idx_val=None,cont_iters=None,train_iters=200,seen_node_idx = None\n",
    "    acc_cl = model.test(data.x, data.edge_index,data.edge_weight,data.y,idx_clean_test)\n",
    "    print(\"Accuracy of GNN+CL: {}\".format(acc_cl))\n",
    "    final_cl_acc.append(acc_cl)\n",
    "    gnn_model = model_construct(args,'GCN',data,device)\n",
    "    gnn_model.fit(data.x, data.edge_index, None, data.y, idx_train, idx_val,train_iters=args.epochs,verbose=False)\n",
    "    clean_acc = gnn_model.test(data.x,data.edge_index,data.edge_weight,data.y,idx_overall_test)\n",
    "    print(clean_acc)\n",
    "    final_gnn_acc.append(clean_acc)\n",
    "\n",
    "\n",
    "print('The final CL Acc:{:.5f}, {:.5f}, The final GNN Acc:{:.5f}, {:.5f}'\\\n",
    "            .format(np.average(final_cl_acc),np.std(final_cl_acc),np.average(final_gnn_acc),np.std(final_gnn_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noisy+DIffuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw graph: torch.Size([2, 10556])\n",
      "add edge: torch.Size([2, 13190])\n",
      "remove edge: torch.Size([2, 7998])\n",
      "updated graph: torch.Size([2, 10632])\n",
      "=== Noisy graph ===\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 861.6275024414062 = 1.9418861865997314 + 100 * 8.596856117248535\n",
      "Epoch 0, val loss: 1.9364506006240845\n",
      "Epoch 10, training loss: 848.9478759765625 = 1.897546648979187 + 100 * 8.470503807067871\n",
      "Epoch 10, val loss: 1.8890513181686401\n",
      "Epoch 20, training loss: 760.0466918945312 = 1.8456153869628906 + 100 * 7.582010746002197\n",
      "Epoch 20, val loss: 1.8392390012741089\n",
      "Epoch 30, training loss: 731.3434448242188 = 1.8136775493621826 + 100 * 7.295298099517822\n",
      "Epoch 30, val loss: 1.808591365814209\n",
      "Epoch 40, training loss: 714.7428588867188 = 1.788662314414978 + 100 * 7.129542350769043\n",
      "Epoch 40, val loss: 1.7784889936447144\n",
      "Epoch 50, training loss: 705.1812744140625 = 1.7627829313278198 + 100 * 7.034185409545898\n",
      "Epoch 50, val loss: 1.748166561126709\n",
      "Epoch 60, training loss: 695.6590576171875 = 1.7388758659362793 + 100 * 6.939201354980469\n",
      "Epoch 60, val loss: 1.7244641780853271\n",
      "Epoch 70, training loss: 689.8345336914062 = 1.7147310972213745 + 100 * 6.881198406219482\n",
      "Epoch 70, val loss: 1.6988224983215332\n",
      "Epoch 80, training loss: 684.240478515625 = 1.684454321861267 + 100 * 6.825560092926025\n",
      "Epoch 80, val loss: 1.669772744178772\n",
      "Epoch 90, training loss: 679.1021728515625 = 1.6641491651535034 + 100 * 6.774380207061768\n",
      "Epoch 90, val loss: 1.647947072982788\n",
      "Epoch 100, training loss: 676.1454467773438 = 1.6357783079147339 + 100 * 6.745096683502197\n",
      "Epoch 100, val loss: 1.6176695823669434\n",
      "Epoch 110, training loss: 671.8375244140625 = 1.60782790184021 + 100 * 6.702296733856201\n",
      "Epoch 110, val loss: 1.5917643308639526\n",
      "Epoch 120, training loss: 668.9812622070312 = 1.5809794664382935 + 100 * 6.674002647399902\n",
      "Epoch 120, val loss: 1.5651280879974365\n",
      "Epoch 130, training loss: 667.2160034179688 = 1.5507876873016357 + 100 * 6.656652450561523\n",
      "Epoch 130, val loss: 1.5334396362304688\n",
      "Epoch 140, training loss: 664.4934692382812 = 1.520186185836792 + 100 * 6.629732608795166\n",
      "Epoch 140, val loss: 1.5075812339782715\n",
      "Epoch 150, training loss: 664.570556640625 = 1.4938722848892212 + 100 * 6.63076639175415\n",
      "Epoch 150, val loss: 1.4794654846191406\n",
      "Epoch 160, training loss: 664.0630493164062 = 1.4620964527130127 + 100 * 6.626009464263916\n",
      "Epoch 160, val loss: 1.4516395330429077\n",
      "Epoch 170, training loss: 660.37841796875 = 1.431633710861206 + 100 * 6.589468002319336\n",
      "Epoch 170, val loss: 1.4242453575134277\n",
      "Epoch 180, training loss: 658.3257446289062 = 1.4055275917053223 + 100 * 6.569202423095703\n",
      "Epoch 180, val loss: 1.4011321067810059\n",
      "Epoch 190, training loss: 659.1428833007812 = 1.3766684532165527 + 100 * 6.577662467956543\n",
      "Epoch 190, val loss: 1.3742740154266357\n",
      "Epoch 200, training loss: 656.3142700195312 = 1.3466519117355347 + 100 * 6.549675941467285\n",
      "Epoch 200, val loss: 1.3487579822540283\n",
      "Epoch 210, training loss: 657.0576171875 = 1.3211767673492432 + 100 * 6.557364463806152\n",
      "Epoch 210, val loss: 1.3240406513214111\n",
      "Epoch 220, training loss: 654.1902465820312 = 1.2902557849884033 + 100 * 6.5289998054504395\n",
      "Epoch 220, val loss: 1.2974364757537842\n",
      "Epoch 230, training loss: 654.40478515625 = 1.2598240375518799 + 100 * 6.531449794769287\n",
      "Epoch 230, val loss: 1.2699055671691895\n",
      "Epoch 240, training loss: 653.74853515625 = 1.2298442125320435 + 100 * 6.525186538696289\n",
      "Epoch 240, val loss: 1.2427815198898315\n",
      "Epoch 250, training loss: 652.3123779296875 = 1.2035788297653198 + 100 * 6.5110883712768555\n",
      "Epoch 250, val loss: 1.220241904258728\n",
      "Epoch 260, training loss: 651.0294189453125 = 1.1788502931594849 + 100 * 6.498505592346191\n",
      "Epoch 260, val loss: 1.2020516395568848\n",
      "Epoch 270, training loss: 652.3500366210938 = 1.1488637924194336 + 100 * 6.512011528015137\n",
      "Epoch 270, val loss: 1.1731982231140137\n",
      "Epoch 280, training loss: 650.5430908203125 = 1.1179497241973877 + 100 * 6.494251728057861\n",
      "Epoch 280, val loss: 1.1497611999511719\n",
      "Epoch 290, training loss: 648.824951171875 = 1.095263123512268 + 100 * 6.477296829223633\n",
      "Epoch 290, val loss: 1.13498055934906\n",
      "Epoch 300, training loss: 648.9354858398438 = 1.0665699243545532 + 100 * 6.478688716888428\n",
      "Epoch 300, val loss: 1.110095500946045\n",
      "Epoch 310, training loss: 648.1292724609375 = 1.0417900085449219 + 100 * 6.470874786376953\n",
      "Epoch 310, val loss: 1.0901564359664917\n",
      "Epoch 320, training loss: 647.8590698242188 = 1.0199153423309326 + 100 * 6.4683918952941895\n",
      "Epoch 320, val loss: 1.073639988899231\n",
      "Epoch 330, training loss: 647.169189453125 = 0.9966698884963989 + 100 * 6.46172571182251\n",
      "Epoch 330, val loss: 1.0614732503890991\n",
      "Epoch 340, training loss: 647.5020751953125 = 0.9698240160942078 + 100 * 6.465322494506836\n",
      "Epoch 340, val loss: 1.039226770401001\n",
      "Epoch 350, training loss: 646.164306640625 = 0.9476423263549805 + 100 * 6.45216703414917\n",
      "Epoch 350, val loss: 1.0236902236938477\n",
      "Epoch 360, training loss: 645.9779052734375 = 0.9277147650718689 + 100 * 6.450501918792725\n",
      "Epoch 360, val loss: 1.01019287109375\n",
      "Epoch 370, training loss: 645.12548828125 = 0.9034939408302307 + 100 * 6.4422197341918945\n",
      "Epoch 370, val loss: 0.9942207336425781\n",
      "Epoch 380, training loss: 645.4757080078125 = 0.8838961124420166 + 100 * 6.445918083190918\n",
      "Epoch 380, val loss: 0.9798300266265869\n",
      "Epoch 390, training loss: 644.0909423828125 = 0.8609434962272644 + 100 * 6.432299613952637\n",
      "Epoch 390, val loss: 0.9657952189445496\n",
      "Epoch 400, training loss: 643.480224609375 = 0.8441293835639954 + 100 * 6.426361083984375\n",
      "Epoch 400, val loss: 0.9587718844413757\n",
      "Epoch 410, training loss: 643.7907104492188 = 0.82208251953125 + 100 * 6.429686546325684\n",
      "Epoch 410, val loss: 0.9428423643112183\n",
      "Epoch 420, training loss: 643.2448120117188 = 0.8023104667663574 + 100 * 6.42442512512207\n",
      "Epoch 420, val loss: 0.9308084845542908\n",
      "Epoch 430, training loss: 642.1791381835938 = 0.7845478057861328 + 100 * 6.413946151733398\n",
      "Epoch 430, val loss: 0.9229530096054077\n",
      "Epoch 440, training loss: 642.4922485351562 = 0.7641425132751465 + 100 * 6.417281150817871\n",
      "Epoch 440, val loss: 0.906264066696167\n",
      "Epoch 450, training loss: 641.5244750976562 = 0.7467882633209229 + 100 * 6.407777309417725\n",
      "Epoch 450, val loss: 0.8996762633323669\n",
      "Epoch 460, training loss: 642.0057373046875 = 0.7286452651023865 + 100 * 6.412771224975586\n",
      "Epoch 460, val loss: 0.8880949020385742\n",
      "Epoch 470, training loss: 640.7015380859375 = 0.7132714986801147 + 100 * 6.399882793426514\n",
      "Epoch 470, val loss: 0.8825547099113464\n",
      "Epoch 480, training loss: 641.8826293945312 = 0.6920902729034424 + 100 * 6.411905288696289\n",
      "Epoch 480, val loss: 0.8664342164993286\n",
      "Epoch 490, training loss: 640.7890625 = 0.6781553030014038 + 100 * 6.401108741760254\n",
      "Epoch 490, val loss: 0.8615360856056213\n",
      "Epoch 500, training loss: 639.535888671875 = 0.6671088337898254 + 100 * 6.388687610626221\n",
      "Epoch 500, val loss: 0.8584086298942566\n",
      "Epoch 510, training loss: 642.8735961914062 = 0.6566919088363647 + 100 * 6.422169208526611\n",
      "Epoch 510, val loss: 0.8551090955734253\n",
      "Epoch 520, training loss: 640.529541015625 = 0.6373589038848877 + 100 * 6.398921966552734\n",
      "Epoch 520, val loss: 0.8423970341682434\n",
      "Epoch 530, training loss: 639.1076049804688 = 0.6236010193824768 + 100 * 6.38484001159668\n",
      "Epoch 530, val loss: 0.8370981812477112\n",
      "Epoch 540, training loss: 639.101318359375 = 0.6133402585983276 + 100 * 6.3848795890808105\n",
      "Epoch 540, val loss: 0.8347387313842773\n",
      "Epoch 550, training loss: 639.0292358398438 = 0.6001339554786682 + 100 * 6.38429069519043\n",
      "Epoch 550, val loss: 0.8289783000946045\n",
      "Epoch 560, training loss: 638.2081298828125 = 0.5888285040855408 + 100 * 6.376193523406982\n",
      "Epoch 560, val loss: 0.8239976167678833\n",
      "Epoch 570, training loss: 638.2418212890625 = 0.5759431719779968 + 100 * 6.376658916473389\n",
      "Epoch 570, val loss: 0.8154981732368469\n",
      "Epoch 580, training loss: 638.7869262695312 = 0.5652849674224854 + 100 * 6.382216453552246\n",
      "Epoch 580, val loss: 0.8119034171104431\n",
      "Epoch 590, training loss: 637.5098266601562 = 0.5559126734733582 + 100 * 6.369539260864258\n",
      "Epoch 590, val loss: 0.8091481924057007\n",
      "Epoch 600, training loss: 638.3580932617188 = 0.547991156578064 + 100 * 6.378101348876953\n",
      "Epoch 600, val loss: 0.80866938829422\n",
      "Epoch 610, training loss: 638.9732666015625 = 0.5349536538124084 + 100 * 6.384382724761963\n",
      "Epoch 610, val loss: 0.8014382719993591\n",
      "Epoch 620, training loss: 637.1691284179688 = 0.5242087841033936 + 100 * 6.366448879241943\n",
      "Epoch 620, val loss: 0.7954917550086975\n",
      "Epoch 630, training loss: 636.5751342773438 = 0.5170226097106934 + 100 * 6.360580921173096\n",
      "Epoch 630, val loss: 0.7955236434936523\n",
      "Epoch 640, training loss: 638.4197387695312 = 0.5073521137237549 + 100 * 6.379124164581299\n",
      "Epoch 640, val loss: 0.7931056618690491\n",
      "Epoch 650, training loss: 637.3732299804688 = 0.4977502226829529 + 100 * 6.368754863739014\n",
      "Epoch 650, val loss: 0.7899861931800842\n",
      "Epoch 660, training loss: 636.3311767578125 = 0.48774102330207825 + 100 * 6.358434200286865\n",
      "Epoch 660, val loss: 0.7861679196357727\n",
      "Epoch 670, training loss: 636.1935424804688 = 0.4818361699581146 + 100 * 6.357117176055908\n",
      "Epoch 670, val loss: 0.7851928472518921\n",
      "Epoch 680, training loss: 638.751708984375 = 0.4730844795703888 + 100 * 6.382786273956299\n",
      "Epoch 680, val loss: 0.7810469269752502\n",
      "Epoch 690, training loss: 636.0681762695312 = 0.4638517200946808 + 100 * 6.356042861938477\n",
      "Epoch 690, val loss: 0.7761590480804443\n",
      "Epoch 700, training loss: 635.4922485351562 = 0.4570977985858917 + 100 * 6.350351333618164\n",
      "Epoch 700, val loss: 0.7752387523651123\n",
      "Epoch 710, training loss: 637.3612060546875 = 0.44948601722717285 + 100 * 6.369117259979248\n",
      "Epoch 710, val loss: 0.7762894034385681\n",
      "Epoch 720, training loss: 636.4683837890625 = 0.4405999183654785 + 100 * 6.3602776527404785\n",
      "Epoch 720, val loss: 0.7731257081031799\n",
      "Epoch 730, training loss: 636.1910400390625 = 0.4333285987377167 + 100 * 6.357576847076416\n",
      "Epoch 730, val loss: 0.7670995593070984\n",
      "Epoch 740, training loss: 635.349609375 = 0.4271891117095947 + 100 * 6.349224090576172\n",
      "Epoch 740, val loss: 0.7692822217941284\n",
      "Epoch 750, training loss: 637.2493896484375 = 0.4221670925617218 + 100 * 6.368272304534912\n",
      "Epoch 750, val loss: 0.76853346824646\n",
      "Epoch 760, training loss: 635.0310668945312 = 0.4118765592575073 + 100 * 6.346191883087158\n",
      "Epoch 760, val loss: 0.7615153193473816\n",
      "Epoch 770, training loss: 634.7171630859375 = 0.40661272406578064 + 100 * 6.343105316162109\n",
      "Epoch 770, val loss: 0.7611885070800781\n",
      "Epoch 780, training loss: 636.0640869140625 = 0.39951953291893005 + 100 * 6.356645584106445\n",
      "Epoch 780, val loss: 0.7616478800773621\n",
      "Epoch 790, training loss: 635.7239379882812 = 0.39179760217666626 + 100 * 6.353321552276611\n",
      "Epoch 790, val loss: 0.7556948065757751\n",
      "Epoch 800, training loss: 634.9149780273438 = 0.38673698902130127 + 100 * 6.345282554626465\n",
      "Epoch 800, val loss: 0.7569147348403931\n",
      "Epoch 810, training loss: 634.399658203125 = 0.38243526220321655 + 100 * 6.340171813964844\n",
      "Epoch 810, val loss: 0.7558572292327881\n",
      "Epoch 820, training loss: 635.6427612304688 = 0.37807902693748474 + 100 * 6.352647304534912\n",
      "Epoch 820, val loss: 0.7553092241287231\n",
      "Epoch 830, training loss: 635.2112426757812 = 0.3697269856929779 + 100 * 6.348414897918701\n",
      "Epoch 830, val loss: 0.7506685853004456\n",
      "Epoch 840, training loss: 634.3243408203125 = 0.36522743105888367 + 100 * 6.339591026306152\n",
      "Epoch 840, val loss: 0.7512746453285217\n",
      "Epoch 850, training loss: 634.6577758789062 = 0.36010831594467163 + 100 * 6.3429765701293945\n",
      "Epoch 850, val loss: 0.7489684224128723\n",
      "Epoch 860, training loss: 634.5314331054688 = 0.35523080825805664 + 100 * 6.341762065887451\n",
      "Epoch 860, val loss: 0.7495896220207214\n",
      "Epoch 870, training loss: 633.6841430664062 = 0.349960595369339 + 100 * 6.333341598510742\n",
      "Epoch 870, val loss: 0.7487584352493286\n",
      "Epoch 880, training loss: 634.0548095703125 = 0.34479689598083496 + 100 * 6.337100505828857\n",
      "Epoch 880, val loss: 0.7495181560516357\n",
      "Epoch 890, training loss: 634.3980102539062 = 0.3411153554916382 + 100 * 6.340569019317627\n",
      "Epoch 890, val loss: 0.7491872310638428\n",
      "Epoch 900, training loss: 634.15478515625 = 0.33575403690338135 + 100 * 6.33819055557251\n",
      "Epoch 900, val loss: 0.7451780438423157\n",
      "Epoch 910, training loss: 633.1964111328125 = 0.3302505910396576 + 100 * 6.3286614418029785\n",
      "Epoch 910, val loss: 0.7416749000549316\n",
      "Epoch 920, training loss: 633.3172607421875 = 0.3278762698173523 + 100 * 6.329893589019775\n",
      "Epoch 920, val loss: 0.746479332447052\n",
      "Epoch 930, training loss: 634.2876586914062 = 0.32306134700775146 + 100 * 6.339645862579346\n",
      "Epoch 930, val loss: 0.7378126978874207\n",
      "Epoch 940, training loss: 633.3906860351562 = 0.31855520606040955 + 100 * 6.330721378326416\n",
      "Epoch 940, val loss: 0.7392882704734802\n",
      "Epoch 950, training loss: 633.6802368164062 = 0.31265634298324585 + 100 * 6.333675384521484\n",
      "Epoch 950, val loss: 0.7382371425628662\n",
      "Epoch 960, training loss: 633.6351928710938 = 0.30989915132522583 + 100 * 6.333252906799316\n",
      "Epoch 960, val loss: 0.737333357334137\n",
      "Epoch 970, training loss: 632.4682006835938 = 0.30583420395851135 + 100 * 6.321623802185059\n",
      "Epoch 970, val loss: 0.7388951182365417\n",
      "Epoch 980, training loss: 632.314208984375 = 0.30333685874938965 + 100 * 6.320108890533447\n",
      "Epoch 980, val loss: 0.7379735708236694\n",
      "Epoch 990, training loss: 633.2953491210938 = 0.2996322214603424 + 100 * 6.329957008361816\n",
      "Epoch 990, val loss: 0.7363893985748291\n",
      "=== picking the best model according to the performance on validation ===\n",
      "Accuracy of GNN+CL: 0.7444444444444445\n",
      "0.7843964153927254\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 861.61572265625 = 1.9340059757232666 + 100 * 8.596817016601562\n",
      "Epoch 0, val loss: 1.9331543445587158\n",
      "Epoch 10, training loss: 829.1777954101562 = 1.8923553228378296 + 100 * 8.272854804992676\n",
      "Epoch 10, val loss: 1.8897552490234375\n",
      "Epoch 20, training loss: 749.75 = 1.8455866575241089 + 100 * 7.479043960571289\n",
      "Epoch 20, val loss: 1.8453446626663208\n",
      "Epoch 30, training loss: 725.14306640625 = 1.8157151937484741 + 100 * 7.233273029327393\n",
      "Epoch 30, val loss: 1.8112410306930542\n",
      "Epoch 40, training loss: 711.1557006835938 = 1.7896735668182373 + 100 * 7.093660354614258\n",
      "Epoch 40, val loss: 1.7766329050064087\n",
      "Epoch 50, training loss: 700.4847412109375 = 1.7638299465179443 + 100 * 6.987208843231201\n",
      "Epoch 50, val loss: 1.7453099489212036\n",
      "Epoch 60, training loss: 691.3570556640625 = 1.7472503185272217 + 100 * 6.8960981369018555\n",
      "Epoch 60, val loss: 1.7283118963241577\n",
      "Epoch 70, training loss: 685.134521484375 = 1.7256333827972412 + 100 * 6.8340888023376465\n",
      "Epoch 70, val loss: 1.7046388387680054\n",
      "Epoch 80, training loss: 679.1522216796875 = 1.7006292343139648 + 100 * 6.7745161056518555\n",
      "Epoch 80, val loss: 1.6782724857330322\n",
      "Epoch 90, training loss: 675.21875 = 1.676274299621582 + 100 * 6.735424995422363\n",
      "Epoch 90, val loss: 1.6530909538269043\n",
      "Epoch 100, training loss: 672.903076171875 = 1.6485207080841064 + 100 * 6.712545871734619\n",
      "Epoch 100, val loss: 1.6257619857788086\n",
      "Epoch 110, training loss: 668.7114868164062 = 1.624391794204712 + 100 * 6.670870780944824\n",
      "Epoch 110, val loss: 1.6027559041976929\n",
      "Epoch 120, training loss: 666.9005126953125 = 1.5988067388534546 + 100 * 6.653017044067383\n",
      "Epoch 120, val loss: 1.5767797231674194\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m model \u001b[39m=\u001b[39m UnifyModel(args, encoder, num_hidden, num_proj_hidden, num_class, tau, lr\u001b[39m=\u001b[39mlearning_rate, weight_decay\u001b[39m=\u001b[39mweight_decay, device\u001b[39m=\u001b[39mdevice,data1\u001b[39m=\u001b[39mnoisy_data,data2\u001b[39m=\u001b[39mdiff_noisy_data)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m# model.fit(args, noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_train,idx_val=idx_val,train_iters=num_epochs,cont_iters=num_epochs,seen_node_idx=None)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit_1(args, noisy_data\u001b[39m.\u001b[39;49mx, noisy_data\u001b[39m.\u001b[39;49medge_index,noisy_data\u001b[39m.\u001b[39;49medge_weight,noisy_data\u001b[39m.\u001b[39;49my,idx_train,idx_val\u001b[39m=\u001b[39;49midx_val,train_iters\u001b[39m=\u001b[39;49mnum_epochs,cont_iters\u001b[39m=\u001b[39;49mnum_epochs,seen_node_idx\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39m# x, edge_index,edge_weight,labels,idx_train,idx_val=None,cont_iters=None,train_iters=200,seen_node_idx = None\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-contrastive/RobustCL/run_preliminary.ipynb#X45sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m acc_cl \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtest(noisy_data\u001b[39m.\u001b[39mx, noisy_data\u001b[39m.\u001b[39medge_index,noisy_data\u001b[39m.\u001b[39medge_weight,noisy_data\u001b[39m.\u001b[39my,idx_clean_test)\n",
      "File \u001b[0;32m~/project-contrastive/RobustCL/model.py:247\u001b[0m, in \u001b[0;36mUnifyModel.fit_1\u001b[0;34m(self, args, x, edge_index, edge_weight, labels, idx_train, idx_val, train_iters, cont_iters, seen_node_idx)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_without_val(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels, idx_train, train_iters,verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    245\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m     \u001b[39m# self._train_with_val(self.labels, idx_train, idx_val, train_iters)\u001b[39;00m\n\u001b[0;32m--> 247\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_with_val_2(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabels, idx_train, idx_val, train_iters,verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/project-contrastive/RobustCL/model.py:377\u001b[0m, in \u001b[0;36mUnifyModel._train_with_val_2\u001b[0;34m(self, labels, idx_train, idx_val, train_iters, verbose)\u001b[0m\n\u001b[1;32m    373\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    376\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval()\n\u001b[0;32m--> 377\u001b[0m cont_embds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49medge_index,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49medge_weight)\n\u001b[1;32m    378\u001b[0m clf_loss_val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclf_loss(cont_embds,labels,idx_val)\n\u001b[1;32m    379\u001b[0m \u001b[39m# loss_val = clf_loss_val + self.cont_weight * cont_loss\u001b[39;00m\n",
      "File \u001b[0;32m~/project-contrastive/RobustCL/model.py:155\u001b[0m, in \u001b[0;36mUnifyModel.forward\u001b[0;34m(self, x, edge_index, edge_weights)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    154\u001b[0m             edge_index: torch\u001b[39m.\u001b[39mTensor, edge_weights\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 155\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x, edge_index, edge_weights)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/project-contrastive/RobustCL/model.py:46\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x, edge_index, edge_weights)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor, edge_index: torch\u001b[39m.\u001b[39mTensor, edge_weights\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     45\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk):\n\u001b[0;32m---> 46\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv[i](x, edge_index))\n\u001b[1;32m     47\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch_geometric/nn/conv/gcn_conv.py:175\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    173\u001b[0m cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_edge_index\n\u001b[1;32m    174\u001b[0m \u001b[39mif\u001b[39;00m cache \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     edge_index, edge_weight \u001b[39m=\u001b[39m gcn_norm(  \u001b[39m# yapf: disable\u001b[39;49;00m\n\u001b[1;32m    176\u001b[0m         edge_index, edge_weight, x\u001b[39m.\u001b[39;49msize(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnode_dim),\n\u001b[1;32m    177\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimproved, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_self_loops, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflow)\n\u001b[1;32m    178\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcached:\n\u001b[1;32m    179\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_edge_index \u001b[39m=\u001b[39m (edge_index, edge_weight)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch_geometric/nn/conv/gcn_conv.py:60\u001b[0m, in \u001b[0;36mgcn_norm\u001b[0;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[1;32m     56\u001b[0m     edge_weight \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones((edge_index\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m), ), dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m     57\u001b[0m                              device\u001b[39m=\u001b[39medge_index\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     59\u001b[0m \u001b[39mif\u001b[39;00m add_self_loops:\n\u001b[0;32m---> 60\u001b[0m     edge_index, tmp_edge_weight \u001b[39m=\u001b[39m add_remaining_self_loops(\n\u001b[1;32m     61\u001b[0m         edge_index, edge_weight, fill_value, num_nodes)\n\u001b[1;32m     62\u001b[0m     \u001b[39massert\u001b[39;00m tmp_edge_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     edge_weight \u001b[39m=\u001b[39m tmp_edge_weight\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch_geometric/utils/loop.py:224\u001b[0m, in \u001b[0;36madd_remaining_self_loops\u001b[0;34m(edge_index, edge_attr, fill_value, num_nodes)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo valid \u001b[39m\u001b[39m'\u001b[39m\u001b[39mfill_value\u001b[39m\u001b[39m'\u001b[39m\u001b[39m provided\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    223\u001b[0m     inv_mask \u001b[39m=\u001b[39m \u001b[39m~\u001b[39mmask\n\u001b[0;32m--> 224\u001b[0m     loop_attr[edge_index[\u001b[39m0\u001b[39m][inv_mask]] \u001b[39m=\u001b[39m edge_attr[inv_mask]\n\u001b[1;32m    226\u001b[0m     edge_attr \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([edge_attr[mask], loop_attr], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    228\u001b[0m edge_index \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([edge_index[:, mask], loop_index], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import copy \n",
    "# from model import UnifyModel\n",
    "# from models.construct import model_construct\n",
    "\n",
    "\n",
    "# import os.path as osp\n",
    "# import random\n",
    "# from time import perf_counter as t\n",
    "# import yaml\n",
    "# from yaml import SafeLoader\n",
    "\n",
    "# import torch\n",
    "# import torch_geometric.transforms as T\n",
    "# from model import Encoder, Model, drop_feature\n",
    "\n",
    "# data = data.to(device)\n",
    "# config = yaml.load(open(args.config), Loader=SafeLoader)[args.dataset]\n",
    "# # num_epochs = config['num_epochs']\n",
    "# # args.cl_lr = config['args.cl_lr']\n",
    "# # weight_decay = config['weight_decay']\n",
    "# args.seed = config['seed']\n",
    "# # args.cont_batch_size = config['cont_batch_size']\n",
    "# # args.cont_weight = config['cont_weight']\n",
    "# # args.add_edge_rate_1 = config['add_edge_rate_1']\n",
    "# # args.add_edge_rate_2 = config['add_edge_rate_2']\n",
    "# # args.drop_edge_rate_1 = config['drop_edge_rate_1']\n",
    "# # args.drop_edge_rate_2 = config['drop_edge_rate_2']\n",
    "# # args.drop_feat_rate_1 = config['drop_feature_rate_1']\n",
    "# # args.drop_feat_rate_2 = config['drop_feature_rate_2']\n",
    "# # args.add_edge_rate_1 = 0\n",
    "# # args.add_edge_rate_2 = 0\n",
    "# # args.drop_edge_rate_1 = 0.3\n",
    "# # args.drop_edge_rate_2 = 0.5\n",
    "# # args.drop_feat_rate_1 = 0.4\n",
    "# # args.drop_feat_rate_2 = 0.4\n",
    "# num_class = int(data.y.max()+1)\n",
    "\n",
    "# noisy_data = construct_noisy_graph(data,perturb_ratio=0.25,mode='random_noise')\n",
    "# noisy_data = noisy_data.to(device)\n",
    "\n",
    "# diff_dataset = PPRDataset(noisy_data,args.dataset)\n",
    "# diff_noisy_data = diff_dataset.data.to(device)\n",
    "\n",
    "# seen_node_idx = (torch.bitwise_not(data.test_mask)).nonzero().flatten()\n",
    "# idx_overall_test = (torch.bitwise_not(data.train_mask)&torch.bitwise_not(data.val_mask)).nonzero().flatten()\n",
    "\n",
    "\n",
    "\n",
    "# final_cl_acc_noisy = []\n",
    "# final_gnn_acc_noisy = []\n",
    "# print(\"=== Noisy graph ===\")\n",
    "# rs = np.random.RandomState(args.seed)\n",
    "# seeds = rs.randint(1000,size=3)\n",
    "# for seed in seeds:\n",
    "#     np.random.seed(seed)\n",
    "#     # torch.manual_seed(seed)\n",
    "#     # torch.cuda.manual_seed(seed)\n",
    "#     '''Transductive'''\n",
    "#     encoder = Encoder(dataset.num_features, args.cl_num_hidden, args.cl_activation,\n",
    "#                             base_model=args.cl_base_model, k=args.cl_num_layers).to(device)\n",
    "#     # model = UnifyModel(args, encoder, args.cl_num_hidden, args.cl_num_proj_hidden, num_class, args.tau, lr=args.cl_lr, weight_decay=weight_decay, device=device).to(device)\n",
    "#     model = UnifyModel(args, encoder, args.cl_num_hidden, args.cl_num_proj_hidden, num_class, args.tau, lr=args.cl_lr, weight_decay=weight_decay, device=device,data1=noisy_data,data2=diff_noisy_data).to(device)\n",
    "#     # model.fit(args, noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_train,idx_val=idx_val,train_iters=num_epochs,cont_iters=num_epochs,seen_node_idx=None)\n",
    "#     model.fit_1(args, noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_train,idx_val=idx_val,train_iters=num_epochs,cont_iters=num_epochs,seen_node_idx=None)\n",
    "#     # x, edge_index,edge_weight,labels,idx_train,idx_val=None,cont_iters=None,train_iters=200,seen_node_idx = None\n",
    "#     acc_cl = model.test(noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_clean_test)\n",
    "#     print(\"Accuracy of GNN+CL: {}\".format(acc_cl))\n",
    "#     final_cl_acc_noisy.append(acc_cl)\n",
    "#     gnn_model = model_construct(args,'GCN',noisy_data,device)\n",
    "#     gnn_model.fit(noisy_data.x, noisy_data.edge_index, None, noisy_data.y, idx_train, idx_val,train_iters=args.epochs,verbose=False)\n",
    "#     clean_acc = gnn_model.test(noisy_data.x,noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_overall_test)\n",
    "#     print(clean_acc)\n",
    "#     final_gnn_acc_noisy.append(clean_acc)\n",
    "\n",
    "\n",
    "# print('The final CL Acc:{:.5f}, {:.5f}, The final GNN Acc:{:.5f}, {:.5f}'\\\n",
    "#             .format(np.average(final_cl_acc_noisy),np.std(final_cl_acc_noisy),np.average(final_gnn_acc_noisy),np.std(final_gnn_acc_noisy)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38_torch120': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ab847dfc59cee10fa08e4e9fed31787c275fa5742f67664facc345e6fad65e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
