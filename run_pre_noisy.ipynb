{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* implement unifyGNN+Contrastive\n",
    "* try different noisy and augmentation\n",
    "* try against with/without unnoticeable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(add_edge_rate_1=0, add_edge_rate_2=0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.005, cl_num_epochs=700, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=0.0005, clf_weight=1, config='config.yaml', cont_batch_size=0, cont_weight=2, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=3, dis_weight=1, drop_edge_rate_1=0, drop_edge_rate_2=0.4, drop_feat_rate_1=0, drop_feat_rate_2=0, dropout=0.5, epochs=200, evaluate_mode='1by1', hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0, inner=1, inv_weight=1, lr=0.001, model='GCN', no_cuda=False, noisy_level=0.2, num_hidden=128, num_proj_hidden=128, prune_thr=0.2, seed=10, select_thrh=0.8, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.1, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=400, use_vs_number=True, vs_number=0, vs_ratio=0, weight_decay=0.0005)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]: \n",
    "\n",
    "import imp\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch_geometric.datasets import Planetoid,Reddit2,Flickr,PPI\n",
    "\n",
    "\n",
    "# from torch_geometric.loader import DataLoader\n",
    "# from help_funcs import prune_unrelated_edge,prune_unrelated_edge_isolated\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--debug', action='store_true',\n",
    "        default=True, help='debug mode')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--seed', type=int, default=10, help='Random seed.')\n",
    "parser.add_argument('--model', type=str, default='GCN', help='model',\n",
    "                    choices=['GCN','GAT','GraphSage','GIN'])\n",
    "parser.add_argument('--dataset', type=str, default='Cora', \n",
    "                    help='Dataset',\n",
    "                    choices=['Cora','Citeseer','Pubmed','PPI','Flickr','ogbn-arxiv','Reddit','Reddit2','Yelp'])\n",
    "parser.add_argument('--train_lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=128,\n",
    "                    help='Number of hidden units of backdoor model.')\n",
    "parser.add_argument('--num_hidden', type=int, default=128,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--num_proj_hidden', type=int, default=128,\n",
    "                    help='Number of hidden units in MLP.')\n",
    "parser.add_argument('--thrd', type=float, default=0.5)\n",
    "parser.add_argument('--target_class', type=int, default=0)\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('--epochs', type=int,  default=200, help='Number of epochs to train benign and backdoor model.')\n",
    "parser.add_argument('--trojan_epochs', type=int,  default=400, help='Number of epochs to train trigger generator.')\n",
    "parser.add_argument('--inner', type=int,  default=1, help='Number of inner')\n",
    "parser.add_argument('--temperature', type=float,  default=0.5, help='Temperature')\n",
    "# backdoor setting\n",
    "parser.add_argument('--lr', type=float, default=0.001,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--trigger_size', type=int, default=3,\n",
    "                    help='tirgger_size')\n",
    "parser.add_argument('--use_vs_number', action='store_true', default=True,\n",
    "                    help=\"if use detailed number to decide Vs\")\n",
    "parser.add_argument('--vs_ratio', type=float, default=0,\n",
    "                    help=\"ratio of poisoning nodes relative to the full graph\")\n",
    "parser.add_argument('--vs_number', type=int, default=0,\n",
    "                    help=\"number of poisoning nodes relative to the full graph\")\n",
    "# defense setting\n",
    "parser.add_argument('--defense_mode', type=str, default=\"none\",\n",
    "                    choices=['prune', 'isolate', 'none'],\n",
    "                    help=\"Mode of defense\")\n",
    "parser.add_argument('--prune_thr', type=float, default=0.2,\n",
    "                    help=\"Threshold of prunning edges\")\n",
    "parser.add_argument('--target_loss_weight', type=float, default=1,\n",
    "                    help=\"Weight of optimize outter trigger generator\")\n",
    "parser.add_argument('--homo_loss_weight', type=float, default=0,\n",
    "                    help=\"Weight of optimize similarity loss\")\n",
    "parser.add_argument('--homo_boost_thrd', type=float, default=0.5,\n",
    "                    help=\"Threshold of increase similarity\")\n",
    "# attack setting\n",
    "parser.add_argument('--dis_weight', type=float, default=1,\n",
    "                    help=\"Weight of cluster distance\")\n",
    "parser.add_argument('--selection_method', type=str, default='cluster_degree',\n",
    "                    choices=['loss','conf','cluster','none','cluster_degree'],\n",
    "                    help='Method to select idx_attach for training trojan model (none means randomly select)')\n",
    "parser.add_argument('--test_model', type=str, default='GCN',\n",
    "                    choices=['GCN','GAT','GraphSage','GIN'],\n",
    "                    help='Model used to attack')\n",
    "parser.add_argument('--evaluate_mode', type=str, default='1by1',\n",
    "                    choices=['overall','1by1'],\n",
    "                    help='Model used to attack')\n",
    "# GPU setting\n",
    "parser.add_argument('--device_id', type=int, default=3,\n",
    "                    help=\"Threshold of prunning edges\")\n",
    "# GRACE setting\n",
    "# parser.add_argument('--gpu_id', type=int, default=0)\n",
    "parser.add_argument('--config', type=str, default=\"config.yaml\")\n",
    "## Contrasitve setting\n",
    "parser.add_argument('--cl_lr', type=float, default=0.005)\n",
    "# parser.add_argument('--cl_num_hidden', type=int, default=128)\n",
    "parser.add_argument('--cl_num_proj_hidden', type=int, default=128)\n",
    "parser.add_argument('--cl_num_layers', type=int, default=2)\n",
    "parser.add_argument('--cl_activation', type=str, default='relu')\n",
    "parser.add_argument('--cl_base_model', type=str, default='GCNConv')\n",
    "parser.add_argument('--cont_weight', type=float, default=2)\n",
    "parser.add_argument('--add_edge_rate_1', type=float, default=0)\n",
    "parser.add_argument('--add_edge_rate_2', type=float, default=0)\n",
    "parser.add_argument('--drop_edge_rate_1', type=float, default=0)\n",
    "parser.add_argument('--drop_edge_rate_2', type=float, default=0.4)\n",
    "parser.add_argument('--drop_feat_rate_1', type=float, default=0)\n",
    "parser.add_argument('--drop_feat_rate_2', type=float, default=0)\n",
    "parser.add_argument('--tau', type=float, default=0.1)\n",
    "parser.add_argument('--cl_num_epochs', type=int, default=700)\n",
    "parser.add_argument('--cl_weight_decay', type=float, default=5e-4)\n",
    "parser.add_argument('--cont_batch_size', type=int, default=0)\n",
    "parser.add_argument('--noisy_level', type=float, default=0.2)\n",
    "parser.add_argument('--clf_weight', type=float, default=1)\n",
    "parser.add_argument('--inv_weight', type=float, default=1)\n",
    "parser.add_argument('--select_thrh', type=float, default=0.8)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_known_args()[0]\n",
    "args.cuda =  not args.no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(('cuda:{}' if torch.cuda.is_available() else 'cpu').format(args.device_id))\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "# torch.manual_seed(args.seed)\n",
    "# torch.cuda.manual_seed(args.seed)\n",
    "print(args)\n",
    "#%%\n",
    "from torch_geometric.utils import to_undirected\n",
    "import torch_geometric.transforms as T\n",
    "transform = T.Compose([T.NormalizeFeatures()])\n",
    "\n",
    "if(args.dataset == 'Cora' or args.dataset == 'Citeseer' or args.dataset == 'Pubmed'):\n",
    "    dataset = Planetoid(root='./data/', \\\n",
    "                        name=args.dataset,\\\n",
    "                        transform=transform)\n",
    "elif(args.dataset == 'Flickr'):\n",
    "    dataset = Flickr(root='./data/Flickr/', \\\n",
    "                    transform=transform)\n",
    "elif(args.dataset == 'Reddit2'):\n",
    "    dataset = Reddit2(root='./data/Reddit2/', \\\n",
    "                    transform=transform)\n",
    "elif(args.dataset == 'ogbn-arxiv'):\n",
    "    from ogb.nodeproppred import PygNodePropPredDataset\n",
    "    # Download and process data at './dataset/ogbg_molhiv/'\n",
    "    dataset = PygNodePropPredDataset(name = 'ogbn-arxiv', root='./data/')\n",
    "    split_idx = dataset.get_idx_split() \n",
    "\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "if(args.dataset == 'ogbn-arxiv'):\n",
    "    nNode = data.x.shape[0]\n",
    "    setattr(data,'train_mask',torch.zeros(nNode, dtype=torch.bool).to(device))\n",
    "    # dataset[0].train_mask = torch.zeros(nEdge, dtype=torch.bool).to(device)\n",
    "    data.val_mask = torch.zeros(nNode, dtype=torch.bool).to(device)\n",
    "    data.test_mask = torch.zeros(nNode, dtype=torch.bool).to(device)\n",
    "    data.y = data.y.squeeze(1)\n",
    "# we build our own train test split \n",
    "#%% \n",
    "from utils import get_split\n",
    "# data, idx_train, idx_val, idx_clean_test, idx_atk = get_split(args,data,device)\n",
    "idx_train = data.train_mask.nonzero().flatten()\n",
    "idx_val = data.val_mask.nonzero().flatten()\n",
    "idx_clean_test = data.test_mask.nonzero().flatten()\n",
    "\n",
    "from torch_geometric.utils import to_undirected\n",
    "from utils import subgraph\n",
    "data.edge_index = to_undirected(data.edge_index)\n",
    "train_edge_index,_, edge_mask = subgraph(torch.bitwise_not(data.test_mask),data.edge_index,relabel_nodes=False)\n",
    "mask_edge_index = data.edge_index[:,torch.bitwise_not(edge_mask)]\n",
    "# filter out the unlabeled nodes except from training nodes and testing nodes, nonzero() is to get index, flatten is to get 1-d tensor\n",
    "unlabeled_idx = (torch.bitwise_not(data.test_mask)&torch.bitwise_not(data.train_mask)).nonzero().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_dense_adj, dense_to_sparse\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from scipy.linalg import expm\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.datasets import Planetoid, Amazon, Coauthor\n",
    "\n",
    "# from seeds import development_seed\n",
    "DATA_PATH = './data/'\n",
    "\n",
    "def get_dataset(name: str, use_lcc: bool = True) -> InMemoryDataset:\n",
    "    path = os.path.join(DATA_PATH, name)\n",
    "    if name in ['Cora', 'Citeseer', 'Pubmed']:\n",
    "        dataset = Planetoid(path, name)\n",
    "    elif name in ['Computers', 'Photo']:\n",
    "        dataset = Amazon(path, name)\n",
    "    elif name == 'CoauthorCS':\n",
    "        dataset = Coauthor(path, 'CS')\n",
    "    else:\n",
    "        raise Exception('Unknown dataset.')\n",
    "\n",
    "    if use_lcc:\n",
    "        lcc = get_largest_connected_component(dataset)\n",
    "\n",
    "        x_new = dataset.data.x[lcc]\n",
    "        y_new = dataset.data.y[lcc]\n",
    "\n",
    "        row, col = dataset.data.edge_index.numpy()\n",
    "        edges = [[i, j] for i, j in zip(row, col) if i in lcc and j in lcc]\n",
    "        edges = remap_edges(edges, get_node_mapper(lcc))\n",
    "        \n",
    "        data = Data(\n",
    "            x=x_new,\n",
    "            edge_index=torch.LongTensor(edges),\n",
    "            y=y_new,\n",
    "            train_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
    "            test_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),\n",
    "            val_mask=torch.zeros(y_new.size()[0], dtype=torch.bool)\n",
    "        )\n",
    "        dataset.data = data\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_component(dataset: InMemoryDataset, start: int = 0) -> set:\n",
    "    visited_nodes = set()\n",
    "    queued_nodes = set([start])\n",
    "    row, col = dataset.data.edge_index.numpy()\n",
    "    while queued_nodes:\n",
    "        current_node = queued_nodes.pop()\n",
    "        visited_nodes.update([current_node])\n",
    "        neighbors = col[np.where(row == current_node)[0]]\n",
    "        neighbors = [n for n in neighbors if n not in visited_nodes and n not in queued_nodes]\n",
    "        queued_nodes.update(neighbors)\n",
    "    return visited_nodes\n",
    "\n",
    "\n",
    "def get_largest_connected_component(dataset: InMemoryDataset) -> np.ndarray:\n",
    "    remaining_nodes = set(range(dataset.data.x.shape[0]))\n",
    "    comps = []\n",
    "    while remaining_nodes:\n",
    "        start = min(remaining_nodes)\n",
    "        comp = get_component(dataset, start)\n",
    "        comps.append(comp)\n",
    "        remaining_nodes = remaining_nodes.difference(comp)\n",
    "    return np.array(list(comps[np.argmax(list(map(len, comps)))]))\n",
    "\n",
    "\n",
    "def get_node_mapper(lcc: np.ndarray) -> dict:\n",
    "    mapper = {}\n",
    "    counter = 0\n",
    "    for node in lcc:\n",
    "        mapper[node] = counter\n",
    "        counter += 1\n",
    "    return mapper\n",
    "\n",
    "\n",
    "def remap_edges(edges: list, mapper: dict) -> list:\n",
    "    row = [e[0] for e in edges]\n",
    "    col = [e[1] for e in edges]\n",
    "    row = list(map(lambda x: mapper[x], row))\n",
    "    col = list(map(lambda x: mapper[x], col))\n",
    "    return [row, col]\n",
    "\n",
    "\n",
    "def get_adj_matrix(data) -> np.ndarray:\n",
    "    num_nodes = data.x.shape[0]\n",
    "    adj_matrix = np.zeros(shape=(num_nodes, num_nodes))\n",
    "    for i, j in zip(data.edge_index[0], data.edge_index[1]):\n",
    "        adj_matrix[i, j] = 1.\n",
    "    return adj_matrix\n",
    "\n",
    "\n",
    "def get_ppr_matrix(\n",
    "        adj_matrix: np.ndarray,\n",
    "        alpha: float = 0.1) -> np.ndarray:\n",
    "    num_nodes = adj_matrix.shape[0]\n",
    "    A_tilde = adj_matrix + np.eye(num_nodes)\n",
    "    D_tilde = np.diag(1/np.sqrt(A_tilde.sum(axis=1)))\n",
    "    H = D_tilde @ A_tilde @ D_tilde\n",
    "    return alpha * np.linalg.inv(np.eye(num_nodes) - (1 - alpha) * H)\n",
    "\n",
    "\n",
    "def get_heat_matrix(\n",
    "        adj_matrix: np.ndarray,\n",
    "        t: float = 5.0) -> np.ndarray:\n",
    "    num_nodes = adj_matrix.shape[0]\n",
    "    A_tilde = adj_matrix + np.eye(num_nodes)\n",
    "    D_tilde = np.diag(1/np.sqrt(A_tilde.sum(axis=1)))\n",
    "    H = D_tilde @ A_tilde @ D_tilde\n",
    "    return expm(-t * (np.eye(num_nodes) - H))\n",
    "\n",
    "\n",
    "def get_top_k_matrix(A: np.ndarray, k: int = 128) -> np.ndarray:\n",
    "    num_nodes = A.shape[0]\n",
    "    row_idx = np.arange(num_nodes)\n",
    "    A[A.argsort(axis=0)[:num_nodes - k], row_idx] = 0.\n",
    "    norm = A.sum(axis=0)\n",
    "    norm[norm <= 0] = 1 # avoid dividing by zero\n",
    "    return A/norm\n",
    "\n",
    "\n",
    "def get_clipped_matrix(A: np.ndarray, eps: float = 0.01) -> np.ndarray:\n",
    "    num_nodes = A.shape[0]\n",
    "    A[A < eps] = 0.\n",
    "    norm = A.sum(axis=0)\n",
    "    norm[norm <= 0] = 1 # avoid dividing by zero\n",
    "    return A/norm\n",
    "\n",
    "\n",
    "def set_train_val_test_split(\n",
    "        seed: int,\n",
    "        data: Data,\n",
    "        num_development: int = 1500,\n",
    "        num_per_class: int = 20) -> Data:\n",
    "    rnd_state = np.random.RandomState(development_seed)\n",
    "    num_nodes = data.y.shape[0]\n",
    "    development_idx = rnd_state.choice(num_nodes, num_development, replace=False)\n",
    "    test_idx = [i for i in np.arange(num_nodes) if i not in development_idx]\n",
    "\n",
    "    train_idx = []\n",
    "    rnd_state = np.random.RandomState(seed)\n",
    "    for c in range(data.y.max() + 1):\n",
    "        class_idx = development_idx[np.where(data.y[development_idx].cpu() == c)[0]]\n",
    "        train_idx.extend(rnd_state.choice(class_idx, num_per_class, replace=False))\n",
    "\n",
    "    val_idx = [i for i in development_idx if i not in train_idx]\n",
    "\n",
    "    def get_mask(idx):\n",
    "        mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        mask[idx] = 1\n",
    "        return mask\n",
    "\n",
    "    data.train_mask = get_mask(train_idx)\n",
    "    data.val_mask = get_mask(val_idx)\n",
    "    data.test_mask = get_mask(test_idx)\n",
    "\n",
    "    return data\n",
    "\n",
    "class PPRDataset(InMemoryDataset):\n",
    "    \"\"\"\n",
    "    Dataset preprocessed with GDC using PPR diffusion.\n",
    "    Note that this implementations is not scalable\n",
    "    since we directly invert the adjacency matrix.\n",
    "    \"\"\"\n",
    "    def __init__(self,noisy_data,\n",
    "                 name: str = 'Cora',\n",
    "                 use_lcc: bool = True,\n",
    "                 alpha: float = 0.1,\n",
    "                 k: int = 16,\n",
    "                 eps: float = None):\n",
    "        self.name = name\n",
    "        self.use_lcc = use_lcc\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "        self.eps = eps\n",
    "        self.noisy_data = noisy_data\n",
    "\n",
    "        super(PPRDataset, self).__init__(DATA_PATH)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> list:\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> list:\n",
    "        return [str(self) + '.pt']\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # base = get_dataset(name=self.name, use_lcc=self.use_lcc)\n",
    "        # generate adjacency matrix from sparse representation\n",
    "        adj_matrix = get_adj_matrix(self.noisy_data)\n",
    "        # obtain exact PPR matrix\n",
    "        ppr_matrix = get_ppr_matrix(adj_matrix,\n",
    "                                        alpha=self.alpha)\n",
    "\n",
    "        if self.k:\n",
    "            print(f'Selecting top {self.k} edges per node.')\n",
    "            ppr_matrix = get_top_k_matrix(ppr_matrix, k=self.k)\n",
    "        elif self.eps:\n",
    "            print(f'Selecting edges with weight greater than {self.eps}.')\n",
    "            ppr_matrix = get_clipped_matrix(ppr_matrix, eps=self.eps)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        # create PyG Data object\n",
    "        edges_i = []\n",
    "        edges_j = []\n",
    "        edge_attr = []\n",
    "        for i, row in enumerate(ppr_matrix):\n",
    "            for j in np.where(row > 0)[0]:\n",
    "                edges_i.append(i)\n",
    "                edges_j.append(j)\n",
    "                edge_attr.append(ppr_matrix[i, j])\n",
    "        edge_index = [edges_i, edges_j]\n",
    "\n",
    "        data = Data(\n",
    "            x=self.noisy_data.x,\n",
    "            edge_index=torch.LongTensor(edge_index),\n",
    "            edge_attr=torch.FloatTensor(edge_attr),\n",
    "            y=self.noisy_data.y,\n",
    "            train_mask=torch.zeros(self.noisy_data.train_mask.size()[0], dtype=torch.bool),\n",
    "            test_mask=torch.zeros(self.noisy_data.test_mask.size()[0], dtype=torch.bool),\n",
    "            val_mask=torch.zeros(self.noisy_data.val_mask.size()[0], dtype=torch.bool)\n",
    "        )\n",
    "\n",
    "        data, slices = self.collate([data])\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f'{self.name}_ppr_alpha={self.alpha}_k={self.k}_eps={self.eps}_lcc={self.use_lcc}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unify Contrastive GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw graph: torch.Size([2, 10556])\n",
      "add edge: torch.Size([2, 12610])\n",
      "remove edge: torch.Size([2, 8482])\n",
      "updated graph: torch.Size([2, 10536])\n",
      "=== Noisy graph ===\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 20.35089683532715 = 1.947382926940918 + 2 * 8.228079795837402\n",
      "Epoch 0, val loss: 1.9319618940353394\n",
      "Epoch 0, val acc: 0.316\n",
      "Epoch 10, training loss: 21.279483795166016 = 1.9288861751556396 + 2 * 8.709375381469727\n",
      "Epoch 10, val loss: 1.9348231554031372\n",
      "Epoch 10, val acc: 0.246\n",
      "Epoch 20, training loss: 20.995023727416992 = 1.8761121034622192 + 2 * 8.628826141357422\n",
      "Epoch 20, val loss: 1.8841718435287476\n",
      "Epoch 20, val acc: 0.466\n",
      "Epoch 30, training loss: 18.923307418823242 = 1.6449793577194214 + 2 * 7.8212151527404785\n",
      "Epoch 30, val loss: 1.7184700965881348\n",
      "Epoch 30, val acc: 0.538\n",
      "Epoch 40, training loss: 17.367284774780273 = 1.3293795585632324 + 2 * 7.357088565826416\n",
      "Epoch 40, val loss: 1.526990532875061\n",
      "Epoch 40, val acc: 0.64\n",
      "Epoch 50, training loss: 15.767108917236328 = 0.5967108011245728 + 2 * 7.298301696777344\n",
      "Epoch 50, val loss: 1.164535403251648\n",
      "Epoch 50, val acc: 0.66\n",
      "Epoch 60, training loss: 15.002500534057617 = 0.30792248249053955 + 2 * 7.200730323791504\n",
      "Epoch 60, val loss: 1.1196792125701904\n",
      "Epoch 60, val acc: 0.638\n",
      "Epoch 70, training loss: 14.840323448181152 = 0.289948433637619 + 2 * 7.143248558044434\n",
      "Epoch 70, val loss: 1.2037004232406616\n",
      "Epoch 70, val acc: 0.612\n",
      "Epoch 80, training loss: 14.614620208740234 = 0.2604815363883972 + 2 * 7.0483317375183105\n",
      "Epoch 80, val loss: 1.2548612356185913\n",
      "Epoch 80, val acc: 0.594\n",
      "Epoch 90, training loss: 14.520617485046387 = 0.2787040174007416 + 2 * 6.992573261260986\n",
      "Epoch 90, val loss: 1.2740336656570435\n",
      "Epoch 90, val acc: 0.59\n",
      "Epoch 100, training loss: 14.403450012207031 = 0.20470711588859558 + 2 * 6.98293399810791\n",
      "Epoch 100, val loss: 1.2636370658874512\n",
      "Epoch 100, val acc: 0.592\n",
      "Epoch 110, training loss: 14.421516418457031 = 0.27391573786735535 + 2 * 6.957700252532959\n",
      "Epoch 110, val loss: 1.3052297830581665\n",
      "Epoch 110, val acc: 0.5760000000000001\n",
      "Epoch 120, training loss: 14.356890678405762 = 0.27436232566833496 + 2 * 6.933310031890869\n",
      "Epoch 120, val loss: 1.3154805898666382\n",
      "Epoch 120, val acc: 0.58\n",
      "Epoch 130, training loss: 14.335394859313965 = 0.2537727952003479 + 2 * 6.931786060333252\n",
      "Epoch 130, val loss: 1.3201948404312134\n",
      "Epoch 130, val acc: 0.578\n",
      "Epoch 140, training loss: 14.349937438964844 = 0.22709275782108307 + 2 * 6.955503463745117\n",
      "Epoch 140, val loss: 1.393226981163025\n",
      "Epoch 140, val acc: 0.582\n",
      "Epoch 150, training loss: 14.163248062133789 = 0.22766326367855072 + 2 * 6.8697333335876465\n",
      "Epoch 150, val loss: 1.4114972352981567\n",
      "Epoch 150, val acc: 0.584\n",
      "Epoch 160, training loss: 14.15623664855957 = 0.20779024064540863 + 2 * 6.864329814910889\n",
      "Epoch 160, val loss: 1.4235329627990723\n",
      "Epoch 160, val acc: 0.582\n",
      "Epoch 170, training loss: 14.104829788208008 = 0.22438234090805054 + 2 * 6.832142353057861\n",
      "Epoch 170, val loss: 1.4627935886383057\n",
      "Epoch 170, val acc: 0.588\n",
      "Epoch 180, training loss: 14.03571605682373 = 0.25126907229423523 + 2 * 6.780797004699707\n",
      "Epoch 180, val loss: 1.5139392614364624\n",
      "Epoch 180, val acc: 0.58\n",
      "Epoch 190, training loss: 14.029441833496094 = 0.23369057476520538 + 2 * 6.803126811981201\n",
      "Epoch 190, val loss: 1.527841567993164\n",
      "Epoch 190, val acc: 0.5700000000000001\n",
      "Epoch 200, training loss: 14.098278045654297 = 0.22375309467315674 + 2 * 6.838375091552734\n",
      "Epoch 200, val loss: 1.5976731777191162\n",
      "Epoch 200, val acc: 0.5640000000000001\n",
      "Epoch 210, training loss: 14.059308052062988 = 0.23201607167720795 + 2 * 6.804388999938965\n",
      "Epoch 210, val loss: 1.5996699333190918\n",
      "Epoch 210, val acc: 0.5660000000000001\n",
      "Epoch 220, training loss: 13.927460670471191 = 0.21970947086811066 + 2 * 6.759000778198242\n",
      "Epoch 220, val loss: 1.6164900064468384\n",
      "Epoch 220, val acc: 0.552\n",
      "Epoch 230, training loss: 13.974345207214355 = 0.23004558682441711 + 2 * 6.7600250244140625\n",
      "Epoch 230, val loss: 1.556204080581665\n",
      "Epoch 230, val acc: 0.5680000000000001\n",
      "Epoch 240, training loss: 13.971531867980957 = 0.21346105635166168 + 2 * 6.785623550415039\n",
      "Epoch 240, val loss: 1.620185136795044\n",
      "Epoch 240, val acc: 0.554\n",
      "Epoch 250, training loss: 13.93553638458252 = 0.21557000279426575 + 2 * 6.748781681060791\n",
      "Epoch 250, val loss: 1.674962043762207\n",
      "Epoch 250, val acc: 0.546\n",
      "Epoch 260, training loss: 13.954808235168457 = 0.23733468353748322 + 2 * 6.755541801452637\n",
      "Epoch 260, val loss: 1.766152024269104\n",
      "Epoch 260, val acc: 0.54\n",
      "Epoch 270, training loss: 13.903582572937012 = 0.2086082100868225 + 2 * 6.735547065734863\n",
      "Epoch 270, val loss: 1.754608154296875\n",
      "Epoch 270, val acc: 0.544\n",
      "Epoch 280, training loss: 13.885173797607422 = 0.20032517611980438 + 2 * 6.7437825202941895\n",
      "Epoch 280, val loss: 1.723185420036316\n",
      "Epoch 280, val acc: 0.548\n",
      "Epoch 290, training loss: 13.838857650756836 = 0.19312818348407745 + 2 * 6.716131687164307\n",
      "Epoch 290, val loss: 1.7872332334518433\n",
      "Epoch 290, val acc: 0.542\n",
      "Epoch 300, training loss: 13.828716278076172 = 0.20702219009399414 + 2 * 6.69930362701416\n",
      "Epoch 300, val loss: 1.8007062673568726\n",
      "Epoch 300, val acc: 0.536\n",
      "Epoch 310, training loss: 13.851550102233887 = 0.22440868616104126 + 2 * 6.704082489013672\n",
      "Epoch 310, val loss: 1.7681632041931152\n",
      "Epoch 310, val acc: 0.528\n",
      "Epoch 320, training loss: 13.892891883850098 = 0.23736348748207092 + 2 * 6.734903335571289\n",
      "Epoch 320, val loss: 1.8503403663635254\n",
      "Epoch 320, val acc: 0.534\n",
      "Epoch 330, training loss: 13.772650718688965 = 0.20972149074077606 + 2 * 6.673659324645996\n",
      "Epoch 330, val loss: 1.818361520767212\n",
      "Epoch 330, val acc: 0.534\n",
      "Epoch 340, training loss: 13.72787857055664 = 0.2225094437599182 + 2 * 6.648699760437012\n",
      "Epoch 340, val loss: 1.865052342414856\n",
      "Epoch 340, val acc: 0.528\n",
      "Epoch 350, training loss: 13.716973304748535 = 0.2017488181591034 + 2 * 6.670255184173584\n",
      "Epoch 350, val loss: 1.8643147945404053\n",
      "Epoch 350, val acc: 0.534\n",
      "Epoch 360, training loss: 13.761125564575195 = 0.20680828392505646 + 2 * 6.676274299621582\n",
      "Epoch 360, val loss: 1.9074548482894897\n",
      "Epoch 360, val acc: 0.528\n",
      "Epoch 370, training loss: 13.742233276367188 = 0.21778695285320282 + 2 * 6.667825698852539\n",
      "Epoch 370, val loss: 1.8198693990707397\n",
      "Epoch 370, val acc: 0.528\n",
      "Epoch 380, training loss: 13.717902183532715 = 0.19492310285568237 + 2 * 6.659721374511719\n",
      "Epoch 380, val loss: 1.9495152235031128\n",
      "Epoch 380, val acc: 0.532\n",
      "Epoch 390, training loss: 13.735137939453125 = 0.20000059902668 + 2 * 6.672558784484863\n",
      "Epoch 390, val loss: 1.921587347984314\n",
      "Epoch 390, val acc: 0.53\n",
      "Epoch 400, training loss: 13.659269332885742 = 0.19356587529182434 + 2 * 6.637692451477051\n",
      "Epoch 400, val loss: 1.9247475862503052\n",
      "Epoch 400, val acc: 0.528\n",
      "Epoch 410, training loss: 13.72946548461914 = 0.21196940541267395 + 2 * 6.658473968505859\n",
      "Epoch 410, val loss: 1.9292409420013428\n",
      "Epoch 410, val acc: 0.532\n",
      "Epoch 420, training loss: 13.645957946777344 = 0.20868244767189026 + 2 * 6.625027179718018\n",
      "Epoch 420, val loss: 1.9060081243515015\n",
      "Epoch 420, val acc: 0.53\n",
      "Epoch 430, training loss: 13.6608304977417 = 0.20835937559604645 + 2 * 6.636260032653809\n",
      "Epoch 430, val loss: 1.9823204278945923\n",
      "Epoch 430, val acc: 0.524\n",
      "Epoch 440, training loss: 13.6771821975708 = 0.21068045496940613 + 2 * 6.625031471252441\n",
      "Epoch 440, val loss: 1.978078007698059\n",
      "Epoch 440, val acc: 0.526\n",
      "Epoch 450, training loss: 13.730542182922363 = 0.1906973272562027 + 2 * 6.67608118057251\n",
      "Epoch 450, val loss: 1.9790855646133423\n",
      "Epoch 450, val acc: 0.528\n",
      "Epoch 460, training loss: 13.614969253540039 = 0.19788897037506104 + 2 * 6.616225242614746\n",
      "Epoch 460, val loss: 1.9387582540512085\n",
      "Epoch 460, val acc: 0.534\n",
      "Epoch 470, training loss: 13.674619674682617 = 0.19956141710281372 + 2 * 6.627737045288086\n",
      "Epoch 470, val loss: 2.093940019607544\n",
      "Epoch 470, val acc: 0.512\n",
      "Epoch 480, training loss: 13.539519309997559 = 0.1902497261762619 + 2 * 6.576277256011963\n",
      "Epoch 480, val loss: 1.9769519567489624\n",
      "Epoch 480, val acc: 0.522\n",
      "Epoch 490, training loss: 13.730277061462402 = 0.19360686838626862 + 2 * 6.670376300811768\n",
      "Epoch 490, val loss: 2.043531656265259\n",
      "Epoch 490, val acc: 0.522\n",
      "Epoch 500, training loss: 13.617717742919922 = 0.21141493320465088 + 2 * 6.601070404052734\n",
      "Epoch 500, val loss: 2.013618230819702\n",
      "Epoch 500, val acc: 0.512\n",
      "Epoch 510, training loss: 13.657447814941406 = 0.200720876455307 + 2 * 6.635839939117432\n",
      "Epoch 510, val loss: 2.020822286605835\n",
      "Epoch 510, val acc: 0.506\n",
      "Epoch 520, training loss: 13.595390319824219 = 0.19835247099399567 + 2 * 6.596719741821289\n",
      "Epoch 520, val loss: 2.040581464767456\n",
      "Epoch 520, val acc: 0.512\n",
      "Epoch 530, training loss: 13.556005477905273 = 0.1692664921283722 + 2 * 6.599776744842529\n",
      "Epoch 530, val loss: 2.0626347064971924\n",
      "Epoch 530, val acc: 0.516\n",
      "Epoch 540, training loss: 13.653633117675781 = 0.1933382898569107 + 2 * 6.633925914764404\n",
      "Epoch 540, val loss: 2.039154052734375\n",
      "Epoch 540, val acc: 0.516\n",
      "Epoch 550, training loss: 13.528388977050781 = 0.17633366584777832 + 2 * 6.572469711303711\n",
      "Epoch 550, val loss: 2.0486254692077637\n",
      "Epoch 550, val acc: 0.516\n",
      "Epoch 560, training loss: 13.628450393676758 = 0.20666511356830597 + 2 * 6.60543966293335\n",
      "Epoch 560, val loss: 2.05317759513855\n",
      "Epoch 560, val acc: 0.508\n",
      "Epoch 570, training loss: 13.565215110778809 = 0.1972101777791977 + 2 * 6.588096618652344\n",
      "Epoch 570, val loss: 2.0773086547851562\n",
      "Epoch 570, val acc: 0.514\n",
      "Epoch 580, training loss: 13.680109024047852 = 0.19659744203090668 + 2 * 6.648624897003174\n",
      "Epoch 580, val loss: 2.0428566932678223\n",
      "Epoch 580, val acc: 0.514\n",
      "Epoch 590, training loss: 13.601438522338867 = 0.18976563215255737 + 2 * 6.595863342285156\n",
      "Epoch 590, val loss: 2.070683240890503\n",
      "Epoch 590, val acc: 0.51\n",
      "Epoch 600, training loss: 13.631970405578613 = 0.2144865095615387 + 2 * 6.623284816741943\n",
      "Epoch 600, val loss: 2.037193775177002\n",
      "Epoch 600, val acc: 0.512\n",
      "Epoch 610, training loss: 13.621880531311035 = 0.1928868591785431 + 2 * 6.624828338623047\n",
      "Epoch 610, val loss: 2.163405418395996\n",
      "Epoch 610, val acc: 0.5\n",
      "Epoch 620, training loss: 13.484719276428223 = 0.18712720274925232 + 2 * 6.55249547958374\n",
      "Epoch 620, val loss: 2.1326308250427246\n",
      "Epoch 620, val acc: 0.512\n",
      "Epoch 630, training loss: 13.613338470458984 = 0.2228892594575882 + 2 * 6.570591449737549\n",
      "Epoch 630, val loss: 2.1593830585479736\n",
      "Epoch 630, val acc: 0.496\n",
      "Epoch 640, training loss: 13.605236053466797 = 0.18743272125720978 + 2 * 6.599063396453857\n",
      "Epoch 640, val loss: 2.1067514419555664\n",
      "Epoch 640, val acc: 0.494\n",
      "Epoch 650, training loss: 13.531305313110352 = 0.17958654463291168 + 2 * 6.571264266967773\n",
      "Epoch 650, val loss: 2.164268970489502\n",
      "Epoch 650, val acc: 0.49\n",
      "Epoch 660, training loss: 13.615885734558105 = 0.18957318365573883 + 2 * 6.603113651275635\n",
      "Epoch 660, val loss: 2.112917184829712\n",
      "Epoch 660, val acc: 0.498\n",
      "Epoch 670, training loss: 13.552803039550781 = 0.20458665490150452 + 2 * 6.576674461364746\n",
      "Epoch 670, val loss: 2.1634318828582764\n",
      "Epoch 670, val acc: 0.494\n",
      "Epoch 680, training loss: 13.558350563049316 = 0.18970641493797302 + 2 * 6.580028057098389\n",
      "Epoch 680, val loss: 2.080043077468872\n",
      "Epoch 680, val acc: 0.506\n",
      "Epoch 690, training loss: 13.43261432647705 = 0.20315174758434296 + 2 * 6.515625\n",
      "Epoch 690, val loss: 2.1619691848754883\n",
      "Epoch 690, val acc: 0.49\n",
      "=== picking the best model according to the performance on validation ===\n",
      "Accuracy of GNN+CL: 0.66\n",
      "0.7369439071566731\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 20.316364288330078 = 1.9463253021240234 + 2 * 8.21181869506836\n",
      "Epoch 0, val loss: 1.9481197595596313\n",
      "Epoch 0, val acc: 0.168\n",
      "Epoch 10, training loss: 21.409061431884766 = 1.9397199153900146 + 2 * 8.772418975830078\n",
      "Epoch 10, val loss: 1.9316363334655762\n",
      "Epoch 10, val acc: 0.302\n",
      "Epoch 20, training loss: 21.206783294677734 = 1.8928046226501465 + 2 * 8.716462135314941\n",
      "Epoch 20, val loss: 1.8916261196136475\n",
      "Epoch 20, val acc: 0.64\n",
      "Epoch 30, training loss: 20.08856773376465 = 1.6667317152023315 + 2 * 8.380516052246094\n",
      "Epoch 30, val loss: 1.7084535360336304\n",
      "Epoch 30, val acc: 0.454\n",
      "Epoch 40, training loss: 18.28758430480957 = 1.31312096118927 + 2 * 7.812508583068848\n",
      "Epoch 40, val loss: 1.4767605066299438\n",
      "Epoch 40, val acc: 0.634\n",
      "Epoch 50, training loss: 16.522743225097656 = 0.7119497656822205 + 2 * 7.5306901931762695\n",
      "Epoch 50, val loss: 1.1317758560180664\n",
      "Epoch 50, val acc: 0.686\n",
      "Epoch 60, training loss: 15.493809700012207 = 0.32588139176368713 + 2 * 7.43632698059082\n",
      "Epoch 60, val loss: 0.9437698125839233\n",
      "Epoch 60, val acc: 0.706\n",
      "Epoch 70, training loss: 15.196747779846191 = 0.3006340563297272 + 2 * 7.305933475494385\n",
      "Epoch 70, val loss: 0.9099308848381042\n",
      "Epoch 70, val acc: 0.722\n",
      "Epoch 80, training loss: 14.923234939575195 = 0.2476349025964737 + 2 * 7.220983028411865\n",
      "Epoch 80, val loss: 0.9025649428367615\n",
      "Epoch 80, val acc: 0.732\n",
      "Epoch 90, training loss: 14.842926025390625 = 0.24138851463794708 + 2 * 7.188521862030029\n",
      "Epoch 90, val loss: 0.8890486359596252\n",
      "Epoch 90, val acc: 0.732\n",
      "Epoch 100, training loss: 14.808422088623047 = 0.2488516867160797 + 2 * 7.162461280822754\n",
      "Epoch 100, val loss: 0.86916583776474\n",
      "Epoch 100, val acc: 0.736\n",
      "Epoch 110, training loss: 14.727269172668457 = 0.27334290742874146 + 2 * 7.107348918914795\n",
      "Epoch 110, val loss: 0.8775720000267029\n",
      "Epoch 110, val acc: 0.732\n",
      "Epoch 120, training loss: 14.620092391967773 = 0.2214876115322113 + 2 * 7.084388732910156\n",
      "Epoch 120, val loss: 0.8753839731216431\n",
      "Epoch 120, val acc: 0.738\n",
      "Epoch 130, training loss: 14.532017707824707 = 0.23900607228279114 + 2 * 7.03347635269165\n",
      "Epoch 130, val loss: 0.9037696719169617\n",
      "Epoch 130, val acc: 0.732\n",
      "Epoch 140, training loss: 14.428557395935059 = 0.24182525277137756 + 2 * 6.987062931060791\n",
      "Epoch 140, val loss: 0.8962147831916809\n",
      "Epoch 140, val acc: 0.736\n",
      "Epoch 150, training loss: 14.482033729553223 = 0.2420644462108612 + 2 * 7.019168853759766\n",
      "Epoch 150, val loss: 0.9107237458229065\n",
      "Epoch 150, val acc: 0.74\n",
      "Epoch 160, training loss: 14.260334968566895 = 0.2242567241191864 + 2 * 6.914234638214111\n",
      "Epoch 160, val loss: 0.9375576376914978\n",
      "Epoch 160, val acc: 0.732\n",
      "Epoch 170, training loss: 14.212393760681152 = 0.2130577117204666 + 2 * 6.889555931091309\n",
      "Epoch 170, val loss: 0.9325045347213745\n",
      "Epoch 170, val acc: 0.724\n",
      "Epoch 180, training loss: 14.344733238220215 = 0.24247010052204132 + 2 * 6.932302474975586\n",
      "Epoch 180, val loss: 0.9389781355857849\n",
      "Epoch 180, val acc: 0.722\n",
      "Epoch 190, training loss: 14.276124954223633 = 0.2613958418369293 + 2 * 6.894522666931152\n",
      "Epoch 190, val loss: 0.9576788544654846\n",
      "Epoch 190, val acc: 0.728\n",
      "Epoch 200, training loss: 14.164780616760254 = 0.19891931116580963 + 2 * 6.879588603973389\n",
      "Epoch 200, val loss: 0.9481636881828308\n",
      "Epoch 200, val acc: 0.736\n",
      "Epoch 210, training loss: 14.177294731140137 = 0.2220216542482376 + 2 * 6.861209869384766\n",
      "Epoch 210, val loss: 0.9660941958427429\n",
      "Epoch 210, val acc: 0.73\n",
      "Epoch 220, training loss: 14.16146183013916 = 0.2297491878271103 + 2 * 6.851831912994385\n",
      "Epoch 220, val loss: 0.9662309885025024\n",
      "Epoch 220, val acc: 0.728\n",
      "Epoch 230, training loss: 14.152603149414062 = 0.20625540614128113 + 2 * 6.874046325683594\n",
      "Epoch 230, val loss: 0.9766442179679871\n",
      "Epoch 230, val acc: 0.73\n",
      "Epoch 240, training loss: 14.113621711730957 = 0.2082146257162094 + 2 * 6.841677188873291\n",
      "Epoch 240, val loss: 0.9740252494812012\n",
      "Epoch 240, val acc: 0.732\n",
      "Epoch 250, training loss: 14.105962753295898 = 0.2108546495437622 + 2 * 6.833711624145508\n",
      "Epoch 250, val loss: 0.9914212822914124\n",
      "Epoch 250, val acc: 0.73\n",
      "Epoch 260, training loss: 14.12248420715332 = 0.2143445611000061 + 2 * 6.853475093841553\n",
      "Epoch 260, val loss: 1.0060150623321533\n",
      "Epoch 260, val acc: 0.728\n",
      "Epoch 270, training loss: 13.986157417297363 = 0.21305301785469055 + 2 * 6.77699089050293\n",
      "Epoch 270, val loss: 0.9999982118606567\n",
      "Epoch 270, val acc: 0.73\n",
      "Epoch 280, training loss: 14.077286720275879 = 0.1759861558675766 + 2 * 6.838676452636719\n",
      "Epoch 280, val loss: 1.0042496919631958\n",
      "Epoch 280, val acc: 0.736\n",
      "Epoch 290, training loss: 13.933846473693848 = 0.18740713596343994 + 2 * 6.775855541229248\n",
      "Epoch 290, val loss: 1.0057716369628906\n",
      "Epoch 290, val acc: 0.728\n",
      "Epoch 300, training loss: 13.999360084533691 = 0.18594783544540405 + 2 * 6.802526473999023\n",
      "Epoch 300, val loss: 1.0200965404510498\n",
      "Epoch 300, val acc: 0.73\n",
      "Epoch 310, training loss: 13.985716819763184 = 0.18208645284175873 + 2 * 6.797588348388672\n",
      "Epoch 310, val loss: 1.022581934928894\n",
      "Epoch 310, val acc: 0.726\n",
      "Epoch 320, training loss: 13.958373069763184 = 0.18347127735614777 + 2 * 6.793612003326416\n",
      "Epoch 320, val loss: 1.0558514595031738\n",
      "Epoch 320, val acc: 0.72\n",
      "Epoch 330, training loss: 13.973356246948242 = 0.2057962566614151 + 2 * 6.771901607513428\n",
      "Epoch 330, val loss: 1.0541410446166992\n",
      "Epoch 330, val acc: 0.726\n",
      "Epoch 340, training loss: 14.027412414550781 = 0.22072212398052216 + 2 * 6.791585922241211\n",
      "Epoch 340, val loss: 1.0775994062423706\n",
      "Epoch 340, val acc: 0.724\n",
      "Epoch 350, training loss: 13.788816452026367 = 0.21137596666812897 + 2 * 6.688638210296631\n",
      "Epoch 350, val loss: 1.0629814863204956\n",
      "Epoch 350, val acc: 0.73\n",
      "Epoch 360, training loss: 13.974952697753906 = 0.21646052598953247 + 2 * 6.770655632019043\n",
      "Epoch 360, val loss: 1.073771595954895\n",
      "Epoch 360, val acc: 0.728\n",
      "Epoch 370, training loss: 13.908964157104492 = 0.1901722401380539 + 2 * 6.762819290161133\n",
      "Epoch 370, val loss: 1.1029495000839233\n",
      "Epoch 370, val acc: 0.724\n",
      "Epoch 380, training loss: 13.964999198913574 = 0.20729182660579681 + 2 * 6.773752689361572\n",
      "Epoch 380, val loss: 1.0648446083068848\n",
      "Epoch 380, val acc: 0.726\n",
      "Epoch 390, training loss: 14.001116752624512 = 0.20095153152942657 + 2 * 6.784140110015869\n",
      "Epoch 390, val loss: 1.0792546272277832\n",
      "Epoch 390, val acc: 0.724\n",
      "Epoch 400, training loss: 13.873726844787598 = 0.19977395236492157 + 2 * 6.743607997894287\n",
      "Epoch 400, val loss: 1.076816439628601\n",
      "Epoch 400, val acc: 0.722\n",
      "Epoch 410, training loss: 13.83327865600586 = 0.22251388430595398 + 2 * 6.706210136413574\n",
      "Epoch 410, val loss: 1.0909059047698975\n",
      "Epoch 410, val acc: 0.72\n",
      "Epoch 420, training loss: 13.936033248901367 = 0.2073144018650055 + 2 * 6.758451461791992\n",
      "Epoch 420, val loss: 1.1069766283035278\n",
      "Epoch 420, val acc: 0.726\n",
      "Epoch 430, training loss: 13.907021522521973 = 0.18882212042808533 + 2 * 6.748721599578857\n",
      "Epoch 430, val loss: 1.0969244241714478\n",
      "Epoch 430, val acc: 0.724\n",
      "Epoch 440, training loss: 13.887804985046387 = 0.18640698492527008 + 2 * 6.750359058380127\n",
      "Epoch 440, val loss: 1.081843376159668\n",
      "Epoch 440, val acc: 0.722\n",
      "Epoch 450, training loss: 13.795389175415039 = 0.17745275795459747 + 2 * 6.707666397094727\n",
      "Epoch 450, val loss: 1.1233950853347778\n",
      "Epoch 450, val acc: 0.726\n",
      "Epoch 460, training loss: 13.903268814086914 = 0.1821594536304474 + 2 * 6.761758804321289\n",
      "Epoch 460, val loss: 1.1042025089263916\n",
      "Epoch 460, val acc: 0.72\n",
      "Epoch 470, training loss: 13.858901023864746 = 0.19148246943950653 + 2 * 6.735054969787598\n",
      "Epoch 470, val loss: 1.1293208599090576\n",
      "Epoch 470, val acc: 0.726\n",
      "Epoch 480, training loss: 13.868131637573242 = 0.2076997011899948 + 2 * 6.730119705200195\n",
      "Epoch 480, val loss: 1.1034501791000366\n",
      "Epoch 480, val acc: 0.716\n",
      "Epoch 490, training loss: 13.822759628295898 = 0.17547829449176788 + 2 * 6.728304862976074\n",
      "Epoch 490, val loss: 1.1434376239776611\n",
      "Epoch 490, val acc: 0.724\n",
      "Epoch 500, training loss: 13.900873184204102 = 0.21421414613723755 + 2 * 6.7365827560424805\n",
      "Epoch 500, val loss: 1.1643685102462769\n",
      "Epoch 500, val acc: 0.73\n",
      "Epoch 510, training loss: 13.77788257598877 = 0.2019670456647873 + 2 * 6.692166805267334\n",
      "Epoch 510, val loss: 1.1261847019195557\n",
      "Epoch 510, val acc: 0.728\n",
      "Epoch 520, training loss: 13.781044960021973 = 0.18465253710746765 + 2 * 6.710406303405762\n",
      "Epoch 520, val loss: 1.1118252277374268\n",
      "Epoch 520, val acc: 0.73\n",
      "Epoch 530, training loss: 13.868448257446289 = 0.1943916529417038 + 2 * 6.737289905548096\n",
      "Epoch 530, val loss: 1.0977067947387695\n",
      "Epoch 530, val acc: 0.724\n",
      "Epoch 540, training loss: 13.700779914855957 = 0.18869000673294067 + 2 * 6.660190105438232\n",
      "Epoch 540, val loss: 1.1176297664642334\n",
      "Epoch 540, val acc: 0.722\n",
      "Epoch 550, training loss: 13.930469512939453 = 0.20537875592708588 + 2 * 6.7559356689453125\n",
      "Epoch 550, val loss: 1.1270533800125122\n",
      "Epoch 550, val acc: 0.726\n",
      "Epoch 560, training loss: 13.825445175170898 = 0.16990160942077637 + 2 * 6.723949432373047\n",
      "Epoch 560, val loss: 1.134279727935791\n",
      "Epoch 560, val acc: 0.726\n",
      "Epoch 570, training loss: 13.844568252563477 = 0.1941702961921692 + 2 * 6.7129597663879395\n",
      "Epoch 570, val loss: 1.160088062286377\n",
      "Epoch 570, val acc: 0.728\n",
      "Epoch 580, training loss: 13.73096752166748 = 0.18379443883895874 + 2 * 6.674821853637695\n",
      "Epoch 580, val loss: 1.1267644166946411\n",
      "Epoch 580, val acc: 0.724\n",
      "Epoch 590, training loss: 13.726996421813965 = 0.188918337225914 + 2 * 6.673009872436523\n",
      "Epoch 590, val loss: 1.1232638359069824\n",
      "Epoch 590, val acc: 0.726\n",
      "Epoch 600, training loss: 13.823128700256348 = 0.1834944188594818 + 2 * 6.715313911437988\n",
      "Epoch 600, val loss: 1.1537082195281982\n",
      "Epoch 600, val acc: 0.726\n",
      "Epoch 610, training loss: 13.667661666870117 = 0.1869204044342041 + 2 * 6.642593860626221\n",
      "Epoch 610, val loss: 1.1454811096191406\n",
      "Epoch 610, val acc: 0.726\n",
      "Epoch 620, training loss: 13.728926658630371 = 0.19159474968910217 + 2 * 6.673621654510498\n",
      "Epoch 620, val loss: 1.1217491626739502\n",
      "Epoch 620, val acc: 0.726\n",
      "Epoch 630, training loss: 13.793095588684082 = 0.19046691060066223 + 2 * 6.6974568367004395\n",
      "Epoch 630, val loss: 1.1119773387908936\n",
      "Epoch 630, val acc: 0.724\n",
      "Epoch 640, training loss: 13.763243675231934 = 0.170678049325943 + 2 * 6.689193248748779\n",
      "Epoch 640, val loss: 1.131935477256775\n",
      "Epoch 640, val acc: 0.732\n",
      "Epoch 650, training loss: 13.80386734008789 = 0.18265417218208313 + 2 * 6.714376926422119\n",
      "Epoch 650, val loss: 1.1228423118591309\n",
      "Epoch 650, val acc: 0.726\n",
      "Epoch 660, training loss: 13.80498218536377 = 0.1795523464679718 + 2 * 6.705370903015137\n",
      "Epoch 660, val loss: 1.126336693763733\n",
      "Epoch 660, val acc: 0.732\n",
      "Epoch 670, training loss: 13.783929824829102 = 0.19739043712615967 + 2 * 6.689210414886475\n",
      "Epoch 670, val loss: 1.1125550270080566\n",
      "Epoch 670, val acc: 0.732\n",
      "Epoch 680, training loss: 13.734317779541016 = 0.18384578824043274 + 2 * 6.670970439910889\n",
      "Epoch 680, val loss: 1.1033169031143188\n",
      "Epoch 680, val acc: 0.728\n",
      "Epoch 690, training loss: 13.800692558288574 = 0.17617976665496826 + 2 * 6.71620512008667\n",
      "Epoch 690, val loss: 1.142050862312317\n",
      "Epoch 690, val acc: 0.724\n",
      "=== picking the best model according to the performance on validation ===\n",
      "Accuracy of GNN+CL: 0.729\n",
      "0.7451644100580271\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 20.569089889526367 = 1.9477558135986328 + 2 * 8.336657524108887\n",
      "Epoch 0, val loss: 1.938476800918579\n",
      "Epoch 0, val acc: 0.058\n",
      "Epoch 10, training loss: 21.3707218170166 = 1.9319490194320679 + 2 * 8.750673294067383\n",
      "Epoch 10, val loss: 1.92997145652771\n",
      "Epoch 10, val acc: 0.37\n",
      "Epoch 20, training loss: 21.162988662719727 = 1.8884519338607788 + 2 * 8.687886238098145\n",
      "Epoch 20, val loss: 1.8967183828353882\n",
      "Epoch 20, val acc: 0.406\n",
      "Epoch 30, training loss: 19.712026596069336 = 1.5974197387695312 + 2 * 8.242956161499023\n",
      "Epoch 30, val loss: 1.67259681224823\n",
      "Epoch 30, val acc: 0.53\n",
      "Epoch 40, training loss: 17.656396865844727 = 1.1981580257415771 + 2 * 7.625331401824951\n",
      "Epoch 40, val loss: 1.4443506002426147\n",
      "Epoch 40, val acc: 0.6\n",
      "Epoch 50, training loss: 15.728795051574707 = 0.49862393736839294 + 2 * 7.359732627868652\n",
      "Epoch 50, val loss: 1.1224515438079834\n",
      "Epoch 50, val acc: 0.636\n",
      "Epoch 60, training loss: 15.296724319458008 = 0.34674060344696045 + 2 * 7.32674503326416\n",
      "Epoch 60, val loss: 1.1370943784713745\n",
      "Epoch 60, val acc: 0.62\n",
      "Epoch 70, training loss: 15.026111602783203 = 0.30748653411865234 + 2 * 7.223514556884766\n",
      "Epoch 70, val loss: 1.2408984899520874\n",
      "Epoch 70, val acc: 0.618\n",
      "Epoch 80, training loss: 14.77344036102295 = 0.25319379568099976 + 2 * 7.1332106590271\n",
      "Epoch 80, val loss: 1.3105168342590332\n",
      "Epoch 80, val acc: 0.59\n",
      "Epoch 90, training loss: 14.651345252990723 = 0.24906933307647705 + 2 * 7.086416244506836\n",
      "Epoch 90, val loss: 1.3189761638641357\n",
      "Epoch 90, val acc: 0.584\n",
      "Epoch 100, training loss: 14.56767749786377 = 0.23888550698757172 + 2 * 7.049869537353516\n",
      "Epoch 100, val loss: 1.385772943496704\n",
      "Epoch 100, val acc: 0.5760000000000001\n",
      "Epoch 110, training loss: 14.655147552490234 = 0.2633344829082489 + 2 * 7.075796604156494\n",
      "Epoch 110, val loss: 1.4516714811325073\n",
      "Epoch 110, val acc: 0.556\n",
      "Epoch 120, training loss: 14.501823425292969 = 0.2600925862789154 + 2 * 7.000569820404053\n",
      "Epoch 120, val loss: 1.4676960706710815\n",
      "Epoch 120, val acc: 0.5660000000000001\n",
      "Epoch 130, training loss: 14.427063941955566 = 0.2303372174501419 + 2 * 6.9856109619140625\n",
      "Epoch 130, val loss: 1.5027709007263184\n",
      "Epoch 130, val acc: 0.552\n",
      "Epoch 140, training loss: 14.361420631408691 = 0.22433778643608093 + 2 * 6.9610066413879395\n",
      "Epoch 140, val loss: 1.549413800239563\n",
      "Epoch 140, val acc: 0.556\n",
      "Epoch 150, training loss: 14.365626335144043 = 0.23713290691375732 + 2 * 6.9376349449157715\n",
      "Epoch 150, val loss: 1.5840977430343628\n",
      "Epoch 150, val acc: 0.546\n",
      "Epoch 160, training loss: 14.213526725769043 = 0.2143927365541458 + 2 * 6.894378185272217\n",
      "Epoch 160, val loss: 1.5766887664794922\n",
      "Epoch 160, val acc: 0.556\n",
      "Epoch 170, training loss: 14.189932823181152 = 0.23275907337665558 + 2 * 6.875260353088379\n",
      "Epoch 170, val loss: 1.5914921760559082\n",
      "Epoch 170, val acc: 0.556\n",
      "Epoch 180, training loss: 14.267247200012207 = 0.23039069771766663 + 2 * 6.918769359588623\n",
      "Epoch 180, val loss: 1.5996302366256714\n",
      "Epoch 180, val acc: 0.5680000000000001\n",
      "Epoch 190, training loss: 14.317560195922852 = 0.2497955858707428 + 2 * 6.923130035400391\n",
      "Epoch 190, val loss: 1.6002225875854492\n",
      "Epoch 190, val acc: 0.5640000000000001\n",
      "Epoch 200, training loss: 14.091995239257812 = 0.20754137635231018 + 2 * 6.834740161895752\n",
      "Epoch 200, val loss: 1.616582989692688\n",
      "Epoch 200, val acc: 0.562\n",
      "Epoch 210, training loss: 14.256482124328613 = 0.241842582821846 + 2 * 6.898972511291504\n",
      "Epoch 210, val loss: 1.6399130821228027\n",
      "Epoch 210, val acc: 0.558\n",
      "Epoch 220, training loss: 14.134714126586914 = 0.21693365275859833 + 2 * 6.855832099914551\n",
      "Epoch 220, val loss: 1.7005903720855713\n",
      "Epoch 220, val acc: 0.538\n",
      "Epoch 230, training loss: 14.10245132446289 = 0.21764618158340454 + 2 * 6.827123641967773\n",
      "Epoch 230, val loss: 1.7239187955856323\n",
      "Epoch 230, val acc: 0.552\n",
      "Epoch 240, training loss: 14.169572830200195 = 0.2175648808479309 + 2 * 6.872810363769531\n",
      "Epoch 240, val loss: 1.7532497644424438\n",
      "Epoch 240, val acc: 0.548\n",
      "Epoch 250, training loss: 14.005690574645996 = 0.19321714341640472 + 2 * 6.807967662811279\n",
      "Epoch 250, val loss: 1.7563785314559937\n",
      "Epoch 250, val acc: 0.552\n",
      "Epoch 260, training loss: 14.001542091369629 = 0.19352580606937408 + 2 * 6.800192356109619\n",
      "Epoch 260, val loss: 1.8002897500991821\n",
      "Epoch 260, val acc: 0.546\n",
      "Epoch 270, training loss: 14.020179748535156 = 0.2046527862548828 + 2 * 6.808093070983887\n",
      "Epoch 270, val loss: 1.8543773889541626\n",
      "Epoch 270, val acc: 0.542\n",
      "Epoch 280, training loss: 14.064168930053711 = 0.22140896320343018 + 2 * 6.817923069000244\n",
      "Epoch 280, val loss: 1.823286771774292\n",
      "Epoch 280, val acc: 0.554\n",
      "Epoch 290, training loss: 13.962677001953125 = 0.21075433492660522 + 2 * 6.759117603302002\n",
      "Epoch 290, val loss: 1.8570828437805176\n",
      "Epoch 290, val acc: 0.546\n",
      "Epoch 300, training loss: 14.036155700683594 = 0.21489056944847107 + 2 * 6.803097724914551\n",
      "Epoch 300, val loss: 1.862815022468567\n",
      "Epoch 300, val acc: 0.548\n",
      "Epoch 310, training loss: 13.923439979553223 = 0.221204936504364 + 2 * 6.744923114776611\n",
      "Epoch 310, val loss: 1.870425820350647\n",
      "Epoch 310, val acc: 0.538\n",
      "Epoch 320, training loss: 14.001182556152344 = 0.220158189535141 + 2 * 6.797266006469727\n",
      "Epoch 320, val loss: 1.8811849355697632\n",
      "Epoch 320, val acc: 0.546\n",
      "Epoch 330, training loss: 14.071402549743652 = 0.1865435391664505 + 2 * 6.818808078765869\n",
      "Epoch 330, val loss: 1.8817347288131714\n",
      "Epoch 330, val acc: 0.544\n",
      "Epoch 340, training loss: 13.912464141845703 = 0.18203449249267578 + 2 * 6.7662553787231445\n",
      "Epoch 340, val loss: 1.8984766006469727\n",
      "Epoch 340, val acc: 0.54\n",
      "Epoch 350, training loss: 13.941974639892578 = 0.1928313970565796 + 2 * 6.770512104034424\n",
      "Epoch 350, val loss: 1.9082918167114258\n",
      "Epoch 350, val acc: 0.54\n",
      "Epoch 360, training loss: 13.945700645446777 = 0.2052203267812729 + 2 * 6.7650465965271\n",
      "Epoch 360, val loss: 1.9048398733139038\n",
      "Epoch 360, val acc: 0.528\n",
      "Epoch 370, training loss: 13.808062553405762 = 0.19784249365329742 + 2 * 6.707963466644287\n",
      "Epoch 370, val loss: 1.9041951894760132\n",
      "Epoch 370, val acc: 0.536\n",
      "Epoch 380, training loss: 13.945330619812012 = 0.18926183879375458 + 2 * 6.766177177429199\n",
      "Epoch 380, val loss: 1.936331868171692\n",
      "Epoch 380, val acc: 0.534\n",
      "Epoch 390, training loss: 13.809625625610352 = 0.18977013230323792 + 2 * 6.713016510009766\n",
      "Epoch 390, val loss: 1.8985204696655273\n",
      "Epoch 390, val acc: 0.542\n",
      "Epoch 400, training loss: 13.900470733642578 = 0.19035790860652924 + 2 * 6.746674060821533\n",
      "Epoch 400, val loss: 2.008140802383423\n",
      "Epoch 400, val acc: 0.532\n",
      "Epoch 410, training loss: 13.943964958190918 = 0.2052418291568756 + 2 * 6.7666707038879395\n",
      "Epoch 410, val loss: 1.9593862295150757\n",
      "Epoch 410, val acc: 0.534\n",
      "Epoch 420, training loss: 13.864006042480469 = 0.219367116689682 + 2 * 6.705783367156982\n",
      "Epoch 420, val loss: 1.9528639316558838\n",
      "Epoch 420, val acc: 0.536\n",
      "Epoch 430, training loss: 13.941045761108398 = 0.18586555123329163 + 2 * 6.770585536956787\n",
      "Epoch 430, val loss: 2.0105061531066895\n",
      "Epoch 430, val acc: 0.53\n",
      "Epoch 440, training loss: 13.88390064239502 = 0.22917768359184265 + 2 * 6.731687545776367\n",
      "Epoch 440, val loss: 2.068915605545044\n",
      "Epoch 440, val acc: 0.528\n",
      "Epoch 450, training loss: 13.778883934020996 = 0.17450235784053802 + 2 * 6.706457614898682\n",
      "Epoch 450, val loss: 2.038557529449463\n",
      "Epoch 450, val acc: 0.532\n",
      "Epoch 460, training loss: 13.874515533447266 = 0.18313539028167725 + 2 * 6.745906829833984\n",
      "Epoch 460, val loss: 1.993272066116333\n",
      "Epoch 460, val acc: 0.532\n",
      "Epoch 470, training loss: 13.88857364654541 = 0.20924481749534607 + 2 * 6.7322096824646\n",
      "Epoch 470, val loss: 2.012831449508667\n",
      "Epoch 470, val acc: 0.538\n",
      "Epoch 480, training loss: 13.750896453857422 = 0.19167457520961761 + 2 * 6.678138256072998\n",
      "Epoch 480, val loss: 2.0522425174713135\n",
      "Epoch 480, val acc: 0.54\n",
      "Epoch 490, training loss: 13.740961074829102 = 0.17677026987075806 + 2 * 6.673644065856934\n",
      "Epoch 490, val loss: 2.0761005878448486\n",
      "Epoch 490, val acc: 0.534\n",
      "Epoch 500, training loss: 13.803606033325195 = 0.18518969416618347 + 2 * 6.70520544052124\n",
      "Epoch 500, val loss: 2.05731201171875\n",
      "Epoch 500, val acc: 0.534\n",
      "Epoch 510, training loss: 13.802580833435059 = 0.19586151838302612 + 2 * 6.695752143859863\n",
      "Epoch 510, val loss: 2.022510051727295\n",
      "Epoch 510, val acc: 0.536\n",
      "Epoch 520, training loss: 13.638232231140137 = 0.1862918585538864 + 2 * 6.614811420440674\n",
      "Epoch 520, val loss: 2.0440335273742676\n",
      "Epoch 520, val acc: 0.542\n",
      "Epoch 530, training loss: 13.776315689086914 = 0.17443808913230896 + 2 * 6.694777011871338\n",
      "Epoch 530, val loss: 2.0259621143341064\n",
      "Epoch 530, val acc: 0.538\n",
      "Epoch 540, training loss: 13.716068267822266 = 0.19854044914245605 + 2 * 6.655710220336914\n",
      "Epoch 540, val loss: 2.0419585704803467\n",
      "Epoch 540, val acc: 0.534\n",
      "Epoch 550, training loss: 13.702615737915039 = 0.1944923847913742 + 2 * 6.660261631011963\n",
      "Epoch 550, val loss: 2.0691142082214355\n",
      "Epoch 550, val acc: 0.534\n",
      "Epoch 560, training loss: 13.794770240783691 = 0.21356260776519775 + 2 * 6.687600135803223\n",
      "Epoch 560, val loss: 2.05389404296875\n",
      "Epoch 560, val acc: 0.536\n",
      "Epoch 570, training loss: 13.742931365966797 = 0.17679497599601746 + 2 * 6.689966678619385\n",
      "Epoch 570, val loss: 2.1174066066741943\n",
      "Epoch 570, val acc: 0.526\n",
      "Epoch 580, training loss: 13.758320808410645 = 0.18330597877502441 + 2 * 6.685548305511475\n",
      "Epoch 580, val loss: 2.100437641143799\n",
      "Epoch 580, val acc: 0.528\n",
      "Epoch 590, training loss: 13.785600662231445 = 0.19455347955226898 + 2 * 6.6994853019714355\n",
      "Epoch 590, val loss: 2.0804836750030518\n",
      "Epoch 590, val acc: 0.53\n",
      "Epoch 600, training loss: 13.747082710266113 = 0.20034801959991455 + 2 * 6.668588161468506\n",
      "Epoch 600, val loss: 2.0847225189208984\n",
      "Epoch 600, val acc: 0.532\n",
      "Epoch 610, training loss: 13.648518562316895 = 0.1705612987279892 + 2 * 6.644941329956055\n",
      "Epoch 610, val loss: 2.0573034286499023\n",
      "Epoch 610, val acc: 0.532\n",
      "Epoch 620, training loss: 13.777226448059082 = 0.18882322311401367 + 2 * 6.689812183380127\n",
      "Epoch 620, val loss: 2.1069867610931396\n",
      "Epoch 620, val acc: 0.536\n",
      "Epoch 630, training loss: 13.690886497497559 = 0.19468554854393005 + 2 * 6.647129058837891\n",
      "Epoch 630, val loss: 2.070128917694092\n",
      "Epoch 630, val acc: 0.536\n",
      "Epoch 640, training loss: 13.66141414642334 = 0.1992027908563614 + 2 * 6.625946044921875\n",
      "Epoch 640, val loss: 2.0667507648468018\n",
      "Epoch 640, val acc: 0.538\n",
      "Epoch 650, training loss: 13.681578636169434 = 0.21183784306049347 + 2 * 6.614696025848389\n",
      "Epoch 650, val loss: 2.0446743965148926\n",
      "Epoch 650, val acc: 0.536\n",
      "Epoch 660, training loss: 13.65076732635498 = 0.1771591603755951 + 2 * 6.6356401443481445\n",
      "Epoch 660, val loss: 2.0665111541748047\n",
      "Epoch 660, val acc: 0.536\n",
      "Epoch 670, training loss: 13.636186599731445 = 0.18613046407699585 + 2 * 6.624289512634277\n",
      "Epoch 670, val loss: 2.1199405193328857\n",
      "Epoch 670, val acc: 0.534\n",
      "Epoch 680, training loss: 13.703596115112305 = 0.20159341394901276 + 2 * 6.6577253341674805\n",
      "Epoch 680, val loss: 2.0928499698638916\n",
      "Epoch 680, val acc: 0.532\n",
      "Epoch 690, training loss: 13.626547813415527 = 0.1762043684720993 + 2 * 6.6242570877075195\n",
      "Epoch 690, val loss: 2.0688211917877197\n",
      "Epoch 690, val acc: 0.53\n",
      "=== picking the best model according to the performance on validation ===\n",
      "Accuracy of GNN+CL: 0.635\n",
      "0.7277562862669246\n",
      "The final CL Acc:0.67467, 0.03975, The final GNN Acc:0.73662, 0.00711\n"
     ]
    }
   ],
   "source": [
    "import copy \n",
    "from model import UnifyModel, Encoder, Encoder_new\n",
    "from models.construct import model_construct\n",
    "from construct_graph import *\n",
    "from models.GCN_CL import GCN_Encoder\n",
    "\n",
    "import os.path as osp\n",
    "import random\n",
    "from time import perf_counter as t\n",
    "import yaml\n",
    "from yaml import SafeLoader\n",
    "\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# args.cont_weight = 0\n",
    "# args.cl_num_epochs = 1200\n",
    "data = data.to(device)\n",
    "num_class = int(data.y.max()+1)\n",
    "# args.cl_activation = ({'relu': F.relu, 'prelu': nn.PReLU()})[config['activation']]\n",
    "# args.cl_base_model = ({'GCNConv': GCNConv})[config['base_model']]\n",
    "\n",
    "noisy_data = construct_noisy_graph(data,perturb_ratio=args.noisy_level,mode='random_noise')\n",
    "noisy_data = noisy_data.to(device)\n",
    "\n",
    "# diff_dataset = PPRDataset(noisy_data,args.dataset)\n",
    "# diff_noisy_data = diff_dataset.data.to(device)\n",
    "\n",
    "seen_node_idx = (torch.bitwise_not(data.test_mask)).nonzero().flatten()\n",
    "idx_overall_test = (torch.bitwise_not(data.train_mask)&torch.bitwise_not(data.val_mask)).nonzero().flatten()\n",
    "\n",
    "\n",
    "\n",
    "final_cl_acc_noisy = []\n",
    "final_gnn_acc_noisy = []\n",
    "print(\"=== Noisy graph ===\")\n",
    "rs = np.random.RandomState(args.seed)\n",
    "seeds = rs.randint(1000,size=3)\n",
    "for seed in seeds:\n",
    "    np.random.seed(seed)\n",
    "    # torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed(seed)\n",
    "    '''Transductive'''\n",
    "    # model = GCN_Encoder(args, data.x.shape[1], args.num_hidden, num_class, dropout=0.5, lr=args.cl_lr, weight_decay=args.cl_weight_decay, tau=args.tau, layer=2,device=device,use_ln=False,layer_norm_first=False)\n",
    "    model = GCN_Encoder(args, data.x.shape[1], args.num_hidden, num_class, unlabeled_idx,dropout=0.5, lr=0.01, weight_decay=args.cl_weight_decay, tau=args.tau, layer=2,device=device,use_ln=False,layer_norm_first=False)\n",
    "    model.fit(noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_train,idx_val=idx_val,train_iters=args.cl_num_epochs,seen_node_idx=None,verbose=True)\n",
    "    # encoder = Encoder(dataset.num_features, args.num_hidden, args.cl_activation,\n",
    "    #                         base_model=args.cl_base_model, k=args.cl_num_layers).to(device)\n",
    "    # # encoder = Encoder_new(dataset.num_features, args.num_hidden, args.cl_activation,\n",
    "    # #                         base_model=args.cl_base_model, k=args.cl_num_layers,mlp_hidden = 128, gcn_hidden=128).to(device)\n",
    "    # model = UnifyModel(args, encoder, args.num_hidden, args.num_proj_hidden, args.cl_num_proj_hidden, num_class, args.tau, lr=args.cl_lr, weight_decay=args.cl_weight_decay, device=device).to(device)\n",
    "    # # model = UnifyModel(args, encoder, args.cl_num_hidden, args.cl_num_proj_hidden, num_class, args.tau, lr=args.cl_lr, weight_decay=weight_decay, device=device,data1=noisy_data,data2=diff_noisy_data).to(device)\n",
    "    # model.fit(args, noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_train,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=None)\n",
    "    # model.fit_1(args, noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_train,idx_val=idx_val,train_iters=num_epochs,cont_iters=num_epochs,seen_node_idx=None)\n",
    "    # x, edge_index,edge_weight,labels,idx_train,idx_val=None,cont_iters=None,train_iters=200,seen_node_idx = None\n",
    "    acc_cl = model.test(noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_clean_test)\n",
    "    print(\"Accuracy of GNN+CL: {}\".format(acc_cl))\n",
    "    final_cl_acc_noisy.append(acc_cl)\n",
    "    gnn_model = model_construct(args,'GCN',noisy_data,device)\n",
    "    gnn_model.fit(noisy_data.x, noisy_data.edge_index, None, noisy_data.y, idx_train, idx_val,train_iters=args.epochs,verbose=False)\n",
    "    clean_acc = gnn_model.test(noisy_data.x,noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_overall_test)\n",
    "    print(clean_acc)\n",
    "    final_gnn_acc_noisy.append(clean_acc)\n",
    "\n",
    "\n",
    "print('The final CL Acc:{:.5f}, {:.5f}, The final GNN Acc:{:.5f}, {:.5f}'\\\n",
    "            .format(np.average(final_cl_acc_noisy),np.std(final_cl_acc_noisy),np.average(final_gnn_acc_noisy),np.std(final_gnn_acc_noisy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "from model import UnifyModel\n",
    "from models.construct import model_construct\n",
    "\n",
    "\n",
    "import os.path as osp\n",
    "import random\n",
    "from time import perf_counter as t\n",
    "import yaml\n",
    "from yaml import SafeLoader\n",
    "\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from model import Encoder, Model, drop_feature\n",
    "\n",
    "data = data.to(device)\n",
    "config = yaml.load(open(args.config), Loader=SafeLoader)[args.dataset]\n",
    "num_epochs = config['num_epochs']\n",
    "# args.cl_lr = 0.0001\n",
    "# args.weight_decay = 0.00001\n",
    "# args.cl_num_epochs = 3000\n",
    "# args.cl_num_epochs = 500\n",
    "# args.cl_lr = 0.0005\n",
    "# weight_decay = config['weight_decay']\n",
    "# args.seed = config['seed']\n",
    "# args.cont_batch_size = config['cont_batch_size']\n",
    "# args.cont_weight = config['cont_weight']\n",
    "# args.add_edge_rate_1 = config['add_edge_rate_1']\n",
    "# args.add_edge_rate_2 = config['add_edge_rate_2']\n",
    "# args.drop_edge_rate_1 = config['drop_edge_rate_1']\n",
    "# args.drop_edge_rate_2 = config['drop_edge_rate_2']\n",
    "# args.drop_feat_rate_1 = config['drop_feature_rate_1']\n",
    "# args.drop_feat_rate_2 = config['drop_feature_rate_2']\n",
    "# args.add_edge_rate_1 = 0\n",
    "# args.add_edge_rate_2 = 0\n",
    "# args.drop_edge_rate_1 = 0.1\n",
    "# args.drop_edge_rate_2 = 0.1\n",
    "# args.drop_feat_rate_1 = 0.3\n",
    "# args.drop_feat_rate_2 = 0.4\n",
    "# args.cont_weight = 10\n",
    "# args.weight_decay = 0.00005\n",
    "num_class = int(data.y.max()+1)\n",
    "\n",
    "# noisy_data = construct_noisy_graph(data,perturb_ratio=0.25,mode='random_noise')\n",
    "# noisy_data = noisy_data.to(device)\n",
    "seen_node_idx = (torch.bitwise_not(data.test_mask)).nonzero().flatten()\n",
    "idx_overall_test = (torch.bitwise_not(data.train_mask)&torch.bitwise_not(data.val_mask)).nonzero().flatten()\n",
    "\n",
    "\n",
    "final_cl_acc = []\n",
    "final_gnn_acc = []\n",
    "print(\"=== Raw graph ===\")\n",
    "rs = np.random.RandomState(args.seed)\n",
    "seeds = rs.randint(20,size=3)\n",
    "# seeds = [39788]\n",
    "for seed in seeds:\n",
    "    np.random.seed(seed)\n",
    "    # torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed(seed)\n",
    "    '''Transductive'''\n",
    "    # encoder = Encoder(dataset.num_features, args.num_hidden, args.cl_activation,\n",
    "    #                         base_model=args.cl_base_model, k=args.cl_num_layers).to(device)\n",
    "    # model = UnifyModel(args, encoder, args.num_hidden, args.num_proj_hidden, args.cl_num_proj_hidden, num_class, args.tau, lr=args.cl_lr, weight_decay=args.cl_weight_decay, device=device).to(device)\n",
    "    # model.fit(args, data.x, data.edge_index,data.edge_weight,data.y,idx_train,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=None)\n",
    "    model = GCN_Encoder(args, data.x.shape[1], args.num_hidden, num_class, dropout=0.5, lr=0.01, weight_decay=args.cl_weight_decay, tau=args.tau, layer=2,device=device,use_ln=False,layer_norm_first=False)\n",
    "    model.fit(noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_train,idx_val=idx_val,train_iters=args.cl_num_epochs,seen_node_idx=None,verbose=True)\n",
    "    \n",
    "    # x, edge_index,edge_weight,labels,idx_train,idx_val=None,cont_iters=None,train_iters=200,seen_node_idx = None\n",
    "    acc_cl = model.test(data.x, data.edge_index,data.edge_weight,data.y,idx_clean_test)\n",
    "    print(\"Accuracy of GNN+CL: {}\".format(acc_cl))\n",
    "    final_cl_acc.append(acc_cl)\n",
    "    gnn_model = model_construct(args,'GCN',data,device)\n",
    "    gnn_model.fit(data.x, data.edge_index, None, data.y, idx_train, idx_val,train_iters=args.epochs,verbose=False)\n",
    "    clean_acc = gnn_model.test(data.x,data.edge_index,data.edge_weight,data.y,idx_overall_test)\n",
    "    print(clean_acc)\n",
    "    final_gnn_acc.append(clean_acc)\n",
    "\n",
    "\n",
    "print('The final CL Acc:{:.5f}, {:.5f}, The final GNN Acc:{:.5f}, {:.5f}'\\\n",
    "            .format(np.average(final_cl_acc),np.std(final_cl_acc),np.average(final_gnn_acc),np.std(final_gnn_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noisy+DIffuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy \n",
    "# from model import UnifyModel\n",
    "# from models.construct import model_construct\n",
    "\n",
    "\n",
    "# import os.path as osp\n",
    "# import random\n",
    "# from time import perf_counter as t\n",
    "# import yaml\n",
    "# from yaml import SafeLoader\n",
    "\n",
    "# import torch\n",
    "# import torch_geometric.transforms as T\n",
    "# from model import Encoder, Model, drop_feature\n",
    "\n",
    "# data = data.to(device)\n",
    "# config = yaml.load(open(args.config), Loader=SafeLoader)[args.dataset]\n",
    "# # num_epochs = config['num_epochs']\n",
    "# # args.cl_lr = config['args.cl_lr']\n",
    "# # weight_decay = config['weight_decay']\n",
    "# args.seed = config['seed']\n",
    "# # args.cont_batch_size = config['cont_batch_size']\n",
    "# # args.cont_weight = config['cont_weight']\n",
    "# # args.add_edge_rate_1 = config['add_edge_rate_1']\n",
    "# # args.add_edge_rate_2 = config['add_edge_rate_2']\n",
    "# # args.drop_edge_rate_1 = config['drop_edge_rate_1']\n",
    "# # args.drop_edge_rate_2 = config['drop_edge_rate_2']\n",
    "# # args.drop_feat_rate_1 = config['drop_feature_rate_1']\n",
    "# # args.drop_feat_rate_2 = config['drop_feature_rate_2']\n",
    "# # args.add_edge_rate_1 = 0\n",
    "# # args.add_edge_rate_2 = 0\n",
    "# # args.drop_edge_rate_1 = 0.3\n",
    "# # args.drop_edge_rate_2 = 0.5\n",
    "# # args.drop_feat_rate_1 = 0.4\n",
    "# # args.drop_feat_rate_2 = 0.4\n",
    "# num_class = int(data.y.max()+1)\n",
    "\n",
    "# noisy_data = construct_noisy_graph(data,perturb_ratio=0.25,mode='random_noise')\n",
    "# noisy_data = noisy_data.to(device)\n",
    "\n",
    "# diff_dataset = PPRDataset(noisy_data,args.dataset)\n",
    "# diff_noisy_data = diff_dataset.data.to(device)\n",
    "\n",
    "# seen_node_idx = (torch.bitwise_not(data.test_mask)).nonzero().flatten()\n",
    "# idx_overall_test = (torch.bitwise_not(data.train_mask)&torch.bitwise_not(data.val_mask)).nonzero().flatten()\n",
    "\n",
    "\n",
    "\n",
    "# final_cl_acc_noisy = []\n",
    "# final_gnn_acc_noisy = []\n",
    "# print(\"=== Noisy graph ===\")\n",
    "# rs = np.random.RandomState(args.seed)\n",
    "# seeds = rs.randint(1000,size=3)\n",
    "# for seed in seeds:\n",
    "#     np.random.seed(seed)\n",
    "#     # torch.manual_seed(seed)\n",
    "#     # torch.cuda.manual_seed(seed)\n",
    "#     '''Transductive'''\n",
    "#     encoder = Encoder(dataset.num_features, args.num_hidden, args.cl_activation,\n",
    "#                             base_model=args.cl_base_model, k=args.cl_num_layers).to(device)\n",
    "#     # model = UnifyModel(args, encoder, args.cl_num_hidden, args.cl_num_proj_hidden, num_class, args.tau, lr=args.cl_lr, weight_decay=weight_decay, device=device).to(device)\n",
    "#     model = UnifyModel(args, encoder, args.cl_num_hidden, args.cl_num_proj_hidden, num_class, args.tau, lr=args.cl_lr, weight_decay=weight_decay, device=device,data1=noisy_data,data2=diff_noisy_data).to(device)\n",
    "#     # model.fit(args, noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_train,idx_val=idx_val,train_iters=num_epochs,cont_iters=num_epochs,seen_node_idx=None)\n",
    "#     model.fit_1(args, noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_train,idx_val=idx_val,train_iters=num_epochs,cont_iters=num_epochs,seen_node_idx=None)\n",
    "#     # x, edge_index,edge_weight,labels,idx_train,idx_val=None,cont_iters=None,train_iters=200,seen_node_idx = None\n",
    "#     acc_cl = model.test(noisy_data.x, noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_clean_test)\n",
    "#     print(\"Accuracy of GNN+CL: {}\".format(acc_cl))\n",
    "#     final_cl_acc_noisy.append(acc_cl)\n",
    "#     gnn_model = model_construct(args,'GCN',noisy_data,device)\n",
    "#     gnn_model.fit(noisy_data.x, noisy_data.edge_index, None, noisy_data.y, idx_train, idx_val,train_iters=args.epochs,verbose=False)\n",
    "#     clean_acc = gnn_model.test(noisy_data.x,noisy_data.edge_index,noisy_data.edge_weight,noisy_data.y,idx_overall_test)\n",
    "#     print(clean_acc)\n",
    "#     final_gnn_acc_noisy.append(clean_acc)\n",
    "\n",
    "\n",
    "# print('The final CL Acc:{:.5f}, {:.5f}, The final GNN Acc:{:.5f}, {:.5f}'\\\n",
    "#             .format(np.average(final_cl_acc_noisy),np.std(final_cl_acc_noisy),np.average(final_gnn_acc_noisy),np.std(final_gnn_acc_noisy)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38_torch110')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 11:38:47) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4260eba67904b42d68a3963bc583366103d86fb6c89846e20de6072b78e7707"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
