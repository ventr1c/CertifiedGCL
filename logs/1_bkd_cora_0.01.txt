Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.044008493423462 = 1.9602701663970947 + 0.01 * 8.373827934265137
Epoch 0, val loss: 1.9631544351577759
Epoch 10, training loss: 2.0335817337036133 = 1.9498443603515625 + 0.01 * 8.373740196228027
Epoch 10, val loss: 1.9529502391815186
Epoch 20, training loss: 2.0202300548553467 = 1.9364961385726929 + 0.01 * 8.373397827148438
Epoch 20, val loss: 1.9399183988571167
Epoch 30, training loss: 2.0005033016204834 = 1.9167799949645996 + 0.01 * 8.372328758239746
Epoch 30, val loss: 1.920699954032898
Epoch 40, training loss: 1.9702775478363037 = 1.8866161108016968 + 0.01 * 8.366138458251953
Epoch 40, val loss: 1.8917890787124634
Epoch 50, training loss: 1.9270344972610474 = 1.8438068628311157 + 0.01 * 8.32276725769043
Epoch 50, val loss: 1.852787971496582
Epoch 60, training loss: 1.877203345298767 = 1.7960481643676758 + 0.01 * 8.115513801574707
Epoch 60, val loss: 1.813570261001587
Epoch 70, training loss: 1.8324906826019287 = 1.7539926767349243 + 0.01 * 7.8498029708862305
Epoch 70, val loss: 1.7793025970458984
Epoch 80, training loss: 1.7773332595825195 = 1.701606035232544 + 0.01 * 7.572717189788818
Epoch 80, val loss: 1.7309733629226685
Epoch 90, training loss: 1.7036395072937012 = 1.6303719282150269 + 0.01 * 7.326755523681641
Epoch 90, val loss: 1.6685336828231812
Epoch 100, training loss: 1.6094108819961548 = 1.5382294654846191 + 0.01 * 7.1181464195251465
Epoch 100, val loss: 1.5921069383621216
Epoch 110, training loss: 1.5053712129592896 = 1.4349980354309082 + 0.01 * 7.037322044372559
Epoch 110, val loss: 1.50760018825531
Epoch 120, training loss: 1.4002833366394043 = 1.3307868242263794 + 0.01 * 6.949655532836914
Epoch 120, val loss: 1.4244251251220703
Epoch 130, training loss: 1.2994545698165894 = 1.2305867671966553 + 0.01 * 6.886783599853516
Epoch 130, val loss: 1.3461103439331055
Epoch 140, training loss: 1.205031156539917 = 1.1366474628448486 + 0.01 * 6.838374614715576
Epoch 140, val loss: 1.2736483812332153
Epoch 150, training loss: 1.1199724674224854 = 1.0519436597824097 + 0.01 * 6.802883625030518
Epoch 150, val loss: 1.209678053855896
Epoch 160, training loss: 1.045708417892456 = 0.9778836965560913 + 0.01 * 6.782466411590576
Epoch 160, val loss: 1.1555614471435547
Epoch 170, training loss: 0.9798609018325806 = 0.9121517539024353 + 0.01 * 6.770912170410156
Epoch 170, val loss: 1.1089489459991455
Epoch 180, training loss: 0.917836606502533 = 0.8502086400985718 + 0.01 * 6.762794494628906
Epoch 180, val loss: 1.0659832954406738
Epoch 190, training loss: 0.8557199239730835 = 0.7881339192390442 + 0.01 * 6.758599758148193
Epoch 190, val loss: 1.0229105949401855
Epoch 200, training loss: 0.7921508550643921 = 0.7245938777923584 + 0.01 * 6.7556986808776855
Epoch 200, val loss: 0.9786544442176819
Epoch 210, training loss: 0.7282344102859497 = 0.6607015132904053 + 0.01 * 6.753286838531494
Epoch 210, val loss: 0.9338851571083069
Epoch 220, training loss: 0.6663585901260376 = 0.5988471508026123 + 0.01 * 6.75114107131958
Epoch 220, val loss: 0.891250729560852
Epoch 230, training loss: 0.6089082360267639 = 0.5414189100265503 + 0.01 * 6.748934745788574
Epoch 230, val loss: 0.8539453148841858
Epoch 240, training loss: 0.5570856928825378 = 0.4896237552165985 + 0.01 * 6.746196269989014
Epoch 240, val loss: 0.8236125707626343
Epoch 250, training loss: 0.5103870034217834 = 0.4429510533809662 + 0.01 * 6.743594169616699
Epoch 250, val loss: 0.7996550798416138
Epoch 260, training loss: 0.46741968393325806 = 0.4000222682952881 + 0.01 * 6.739740371704102
Epoch 260, val loss: 0.7802026867866516
Epoch 270, training loss: 0.4271247386932373 = 0.35977116227149963 + 0.01 * 6.735356330871582
Epoch 270, val loss: 0.7640320062637329
Epoch 280, training loss: 0.3890186548233032 = 0.3216875195503235 + 0.01 * 6.733115196228027
Epoch 280, val loss: 0.7507113218307495
Epoch 290, training loss: 0.35291019082069397 = 0.2856259047985077 + 0.01 * 6.728428840637207
Epoch 290, val loss: 0.7399124503135681
Epoch 300, training loss: 0.31881728768348694 = 0.25156477093696594 + 0.01 * 6.725252628326416
Epoch 300, val loss: 0.7313825488090515
Epoch 310, training loss: 0.286887526512146 = 0.21967031061649323 + 0.01 * 6.721723556518555
Epoch 310, val loss: 0.7250651717185974
Epoch 320, training loss: 0.25758516788482666 = 0.1903647780418396 + 0.01 * 6.722040176391602
Epoch 320, val loss: 0.7215332984924316
Epoch 330, training loss: 0.23124003410339355 = 0.16408027708530426 + 0.01 * 6.715976715087891
Epoch 330, val loss: 0.7212688326835632
Epoch 340, training loss: 0.20823127031326294 = 0.1410931795835495 + 0.01 * 6.713808536529541
Epoch 340, val loss: 0.7244499921798706
Epoch 350, training loss: 0.18848177790641785 = 0.12138056755065918 + 0.01 * 6.71012020111084
Epoch 350, val loss: 0.7309061884880066
Epoch 360, training loss: 0.1718459129333496 = 0.10469319671392441 + 0.01 * 6.715271472930908
Epoch 360, val loss: 0.7401822209358215
Epoch 370, training loss: 0.15771791338920593 = 0.09065928310155869 + 0.01 * 6.7058634757995605
Epoch 370, val loss: 0.7516686320304871
Epoch 380, training loss: 0.14586059749126434 = 0.07885955274105072 + 0.01 * 6.700104713439941
Epoch 380, val loss: 0.7647074460983276
Epoch 390, training loss: 0.135877788066864 = 0.06891289353370667 + 0.01 * 6.696489334106445
Epoch 390, val loss: 0.778701901435852
Epoch 400, training loss: 0.12741979956626892 = 0.06050032377243042 + 0.01 * 6.691947937011719
Epoch 400, val loss: 0.7932141423225403
Epoch 410, training loss: 0.12030600756406784 = 0.05335892736911774 + 0.01 * 6.694708347320557
Epoch 410, val loss: 0.8078885078430176
Epoch 420, training loss: 0.11420080065727234 = 0.04728463664650917 + 0.01 * 6.691616058349609
Epoch 420, val loss: 0.8225694894790649
Epoch 430, training loss: 0.10891371965408325 = 0.04210042208433151 + 0.01 * 6.681329727172852
Epoch 430, val loss: 0.8370145559310913
Epoch 440, training loss: 0.10441923141479492 = 0.03766288980841637 + 0.01 * 6.675634384155273
Epoch 440, val loss: 0.8512477278709412
Epoch 450, training loss: 0.10055140405893326 = 0.033848248422145844 + 0.01 * 6.670315742492676
Epoch 450, val loss: 0.865227997303009
Epoch 460, training loss: 0.09721224009990692 = 0.030557475984096527 + 0.01 * 6.665476322174072
Epoch 460, val loss: 0.8787932395935059
Epoch 470, training loss: 0.09431052953004837 = 0.027708202600479126 + 0.01 * 6.6602325439453125
Epoch 470, val loss: 0.8919839859008789
Epoch 480, training loss: 0.09180660545825958 = 0.02523178979754448 + 0.01 * 6.6574811935424805
Epoch 480, val loss: 0.9047466516494751
Epoch 490, training loss: 0.08960600197315216 = 0.023068690672516823 + 0.01 * 6.653731822967529
Epoch 490, val loss: 0.9171384572982788
Epoch 500, training loss: 0.0876866951584816 = 0.02117137610912323 + 0.01 * 6.65153169631958
Epoch 500, val loss: 0.9291658401489258
Epoch 510, training loss: 0.08594143390655518 = 0.01950010471045971 + 0.01 * 6.644132614135742
Epoch 510, val loss: 0.9408285617828369
Epoch 520, training loss: 0.08444029092788696 = 0.018022410571575165 + 0.01 * 6.641788482666016
Epoch 520, val loss: 0.9520737528800964
Epoch 530, training loss: 0.08306637406349182 = 0.016710568219423294 + 0.01 * 6.635581016540527
Epoch 530, val loss: 0.9629639983177185
Epoch 540, training loss: 0.08187420666217804 = 0.015540669672191143 + 0.01 * 6.633354187011719
Epoch 540, val loss: 0.9735247492790222
Epoch 550, training loss: 0.08082703500986099 = 0.014493509195744991 + 0.01 * 6.633352279663086
Epoch 550, val loss: 0.9837393164634705
Epoch 560, training loss: 0.07978154718875885 = 0.013553469441831112 + 0.01 * 6.62280797958374
Epoch 560, val loss: 0.9936473965644836
Epoch 570, training loss: 0.07889029383659363 = 0.01270598080009222 + 0.01 * 6.618431091308594
Epoch 570, val loss: 1.0032541751861572
Epoch 580, training loss: 0.07811668515205383 = 0.01193958893418312 + 0.01 * 6.617709159851074
Epoch 580, val loss: 1.0125881433486938
Epoch 590, training loss: 0.0773308128118515 = 0.0112446965649724 + 0.01 * 6.608611583709717
Epoch 590, val loss: 1.0216174125671387
Epoch 600, training loss: 0.07681494206190109 = 0.01061243750154972 + 0.01 * 6.620250701904297
Epoch 600, val loss: 1.0303844213485718
Epoch 610, training loss: 0.07604234665632248 = 0.010035740211606026 + 0.01 * 6.600660800933838
Epoch 610, val loss: 1.0389385223388672
Epoch 620, training loss: 0.07558407634496689 = 0.009508193470537663 + 0.01 * 6.607588768005371
Epoch 620, val loss: 1.0472520589828491
Epoch 630, training loss: 0.07496632635593414 = 0.009024634957313538 + 0.01 * 6.594169616699219
Epoch 630, val loss: 1.0553274154663086
Epoch 640, training loss: 0.0744442418217659 = 0.008580067194998264 + 0.01 * 6.5864176750183105
Epoch 640, val loss: 1.0631680488586426
Epoch 650, training loss: 0.07416059076786041 = 0.008170146495103836 + 0.01 * 6.599045276641846
Epoch 650, val loss: 1.07081139087677
Epoch 660, training loss: 0.07355180382728577 = 0.007792002055794001 + 0.01 * 6.575980186462402
Epoch 660, val loss: 1.0782469511032104
Epoch 670, training loss: 0.07325989753007889 = 0.007442092057317495 + 0.01 * 6.581780433654785
Epoch 670, val loss: 1.085472583770752
Epoch 680, training loss: 0.07285593450069427 = 0.007117387373000383 + 0.01 * 6.573854446411133
Epoch 680, val loss: 1.0925521850585938
Epoch 690, training loss: 0.07245761156082153 = 0.0068158432841300964 + 0.01 * 6.5641770362854
Epoch 690, val loss: 1.0994504690170288
Epoch 700, training loss: 0.07222123444080353 = 0.006535152904689312 + 0.01 * 6.56860876083374
Epoch 700, val loss: 1.1061683893203735
Epoch 710, training loss: 0.07190432399511337 = 0.006273569073528051 + 0.01 * 6.563075542449951
Epoch 710, val loss: 1.1127173900604248
Epoch 720, training loss: 0.07160083949565887 = 0.006029495969414711 + 0.01 * 6.557134628295898
Epoch 720, val loss: 1.1191242933273315
Epoch 730, training loss: 0.07125714421272278 = 0.005801028106361628 + 0.01 * 6.545611381530762
Epoch 730, val loss: 1.1253806352615356
Epoch 740, training loss: 0.07104936242103577 = 0.005586755927652121 + 0.01 * 6.546260356903076
Epoch 740, val loss: 1.1314760446548462
Epoch 750, training loss: 0.07077619433403015 = 0.005385817494243383 + 0.01 * 6.539038181304932
Epoch 750, val loss: 1.1374609470367432
Epoch 760, training loss: 0.07062656432390213 = 0.005196909420192242 + 0.01 * 6.542965888977051
Epoch 760, val loss: 1.143283486366272
Epoch 770, training loss: 0.07040948420763016 = 0.005018966738134623 + 0.01 * 6.539052486419678
Epoch 770, val loss: 1.149003028869629
Epoch 780, training loss: 0.0702064260840416 = 0.004851349629461765 + 0.01 * 6.535507678985596
Epoch 780, val loss: 1.1545597314834595
Epoch 790, training loss: 0.07002745568752289 = 0.004693228285759687 + 0.01 * 6.533422946929932
Epoch 790, val loss: 1.1600290536880493
Epoch 800, training loss: 0.06985914707183838 = 0.004543973132967949 + 0.01 * 6.531517505645752
Epoch 800, val loss: 1.165350079536438
Epoch 810, training loss: 0.06962830573320389 = 0.004402729216963053 + 0.01 * 6.522558212280273
Epoch 810, val loss: 1.1705836057662964
Epoch 820, training loss: 0.0695563480257988 = 0.0042691840790212154 + 0.01 * 6.528717041015625
Epoch 820, val loss: 1.1756560802459717
Epoch 830, training loss: 0.06928262859582901 = 0.004142493009567261 + 0.01 * 6.514013767242432
Epoch 830, val loss: 1.1806648969650269
Epoch 840, training loss: 0.06933723390102386 = 0.00402225274592638 + 0.01 * 6.531497955322266
Epoch 840, val loss: 1.185587763786316
Epoch 850, training loss: 0.06909982860088348 = 0.003908189013600349 + 0.01 * 6.519164085388184
Epoch 850, val loss: 1.190354824066162
Epoch 860, training loss: 0.06890422105789185 = 0.003799783531576395 + 0.01 * 6.510444164276123
Epoch 860, val loss: 1.1950269937515259
Epoch 870, training loss: 0.0687459260225296 = 0.003696823725476861 + 0.01 * 6.504910469055176
Epoch 870, val loss: 1.1996209621429443
Epoch 880, training loss: 0.0686359703540802 = 0.003598622977733612 + 0.01 * 6.503734588623047
Epoch 880, val loss: 1.2041270732879639
Epoch 890, training loss: 0.06869141012430191 = 0.0035050371661782265 + 0.01 * 6.518637180328369
Epoch 890, val loss: 1.2084964513778687
Epoch 900, training loss: 0.068447045981884 = 0.003415830200538039 + 0.01 * 6.503121852874756
Epoch 900, val loss: 1.2128404378890991
Epoch 910, training loss: 0.06834337115287781 = 0.0033306675031781197 + 0.01 * 6.501270294189453
Epoch 910, val loss: 1.217082142829895
Epoch 920, training loss: 0.06827124208211899 = 0.003249373519793153 + 0.01 * 6.502186298370361
Epoch 920, val loss: 1.221192479133606
Epoch 930, training loss: 0.06809698045253754 = 0.0031715489458292723 + 0.01 * 6.4925432205200195
Epoch 930, val loss: 1.225287914276123
Epoch 940, training loss: 0.0681329295039177 = 0.003097178880125284 + 0.01 * 6.503575801849365
Epoch 940, val loss: 1.2292588949203491
Epoch 950, training loss: 0.06795641034841537 = 0.0030260977800935507 + 0.01 * 6.4930315017700195
Epoch 950, val loss: 1.2331706285476685
Epoch 960, training loss: 0.06780111789703369 = 0.002957989927381277 + 0.01 * 6.484312534332275
Epoch 960, val loss: 1.2369928359985352
Epoch 970, training loss: 0.06774313747882843 = 0.0028927302919328213 + 0.01 * 6.485040664672852
Epoch 970, val loss: 1.2407516241073608
Epoch 980, training loss: 0.06775350123643875 = 0.002830034587532282 + 0.01 * 6.49234676361084
Epoch 980, val loss: 1.2444206476211548
Epoch 990, training loss: 0.06763587146997452 = 0.0027699472848325968 + 0.01 * 6.4865922927856445
Epoch 990, val loss: 1.2480719089508057
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.4982
Flip ASR: 0.4000/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.022585391998291 = 1.9388468265533447 + 0.01 * 8.373860359191895
Epoch 0, val loss: 1.9396296739578247
Epoch 10, training loss: 2.0129098892211914 = 1.929172158241272 + 0.01 * 8.373761177062988
Epoch 10, val loss: 1.929377794265747
Epoch 20, training loss: 2.0008482933044434 = 1.9171137809753418 + 0.01 * 8.373443603515625
Epoch 20, val loss: 1.9162260293960571
Epoch 30, training loss: 1.9839510917663574 = 1.90022611618042 + 0.01 * 8.372493743896484
Epoch 30, val loss: 1.8975154161453247
Epoch 40, training loss: 1.9593888521194458 = 1.8757076263427734 + 0.01 * 8.368119239807129
Epoch 40, val loss: 1.8704038858413696
Epoch 50, training loss: 1.9248631000518799 = 1.8414636850357056 + 0.01 * 8.339944839477539
Epoch 50, val loss: 1.8334589004516602
Epoch 60, training loss: 1.881229281425476 = 1.7993688583374023 + 0.01 * 8.186041831970215
Epoch 60, val loss: 1.791133999824524
Epoch 70, training loss: 1.8320720195770264 = 1.7547591924667358 + 0.01 * 7.731277942657471
Epoch 70, val loss: 1.7510093450546265
Epoch 80, training loss: 1.7740854024887085 = 1.701884388923645 + 0.01 * 7.220096588134766
Epoch 80, val loss: 1.7060648202896118
Epoch 90, training loss: 1.7014539241790771 = 1.6305551528930664 + 0.01 * 7.089874744415283
Epoch 90, val loss: 1.645484209060669
Epoch 100, training loss: 1.6078858375549316 = 1.5372494459152222 + 0.01 * 7.0636396408081055
Epoch 100, val loss: 1.5661002397537231
Epoch 110, training loss: 1.4960238933563232 = 1.4257216453552246 + 0.01 * 7.030227184295654
Epoch 110, val loss: 1.474866271018982
Epoch 120, training loss: 1.3749938011169434 = 1.305008053779602 + 0.01 * 6.998571395874023
Epoch 120, val loss: 1.378165364265442
Epoch 130, training loss: 1.2532440423965454 = 1.1835544109344482 + 0.01 * 6.968964099884033
Epoch 130, val loss: 1.2826071977615356
Epoch 140, training loss: 1.138191819190979 = 1.0688695907592773 + 0.01 * 6.932225227355957
Epoch 140, val loss: 1.1939910650253296
Epoch 150, training loss: 1.035179853439331 = 0.9662345051765442 + 0.01 * 6.894532203674316
Epoch 150, val loss: 1.1166070699691772
Epoch 160, training loss: 0.9458451271057129 = 0.877198338508606 + 0.01 * 6.864681720733643
Epoch 160, val loss: 1.0512893199920654
Epoch 170, training loss: 0.8696824908256531 = 0.8012728095054626 + 0.01 * 6.840970039367676
Epoch 170, val loss: 0.9973840713500977
Epoch 180, training loss: 0.8053948879241943 = 0.7372122406959534 + 0.01 * 6.818267822265625
Epoch 180, val loss: 0.9533238410949707
Epoch 190, training loss: 0.7507513165473938 = 0.6827706098556519 + 0.01 * 6.798072338104248
Epoch 190, val loss: 0.9174355864524841
Epoch 200, training loss: 0.7026667594909668 = 0.6348694562911987 + 0.01 * 6.77972936630249
Epoch 200, val loss: 0.8876187205314636
Epoch 210, training loss: 0.6582630276679993 = 0.5905852317810059 + 0.01 * 6.767779350280762
Epoch 210, val loss: 0.8623093366622925
Epoch 220, training loss: 0.6150857210159302 = 0.5475280284881592 + 0.01 * 6.7557692527771
Epoch 220, val loss: 0.8405048251152039
Epoch 230, training loss: 0.571598470211029 = 0.504130482673645 + 0.01 * 6.746798515319824
Epoch 230, val loss: 0.8210139274597168
Epoch 240, training loss: 0.5275420546531677 = 0.4601302444934845 + 0.01 * 6.741180896759033
Epoch 240, val loss: 0.8031562566757202
Epoch 250, training loss: 0.4833572804927826 = 0.4159797728061676 + 0.01 * 6.737750053405762
Epoch 250, val loss: 0.7871175408363342
Epoch 260, training loss: 0.43995729088783264 = 0.3726312518119812 + 0.01 * 6.73260498046875
Epoch 260, val loss: 0.7734758257865906
Epoch 270, training loss: 0.3981890082359314 = 0.33092066645622253 + 0.01 * 6.726834297180176
Epoch 270, val loss: 0.762729823589325
Epoch 280, training loss: 0.3591012954711914 = 0.29175764322280884 + 0.01 * 6.7343668937683105
Epoch 280, val loss: 0.7554193139076233
Epoch 290, training loss: 0.3234778344631195 = 0.25630518794059753 + 0.01 * 6.717263698577881
Epoch 290, val loss: 0.7521629929542542
Epoch 300, training loss: 0.29202112555503845 = 0.2248942255973816 + 0.01 * 6.712689399719238
Epoch 300, val loss: 0.7530240416526794
Epoch 310, training loss: 0.26450490951538086 = 0.1974450796842575 + 0.01 * 6.705984592437744
Epoch 310, val loss: 0.7575197815895081
Epoch 320, training loss: 0.2410203069448471 = 0.17372940480709076 + 0.01 * 6.729090690612793
Epoch 320, val loss: 0.7653970122337341
Epoch 330, training loss: 0.22038257122039795 = 0.15340997278690338 + 0.01 * 6.697259902954102
Epoch 330, val loss: 0.7762876749038696
Epoch 340, training loss: 0.2029283046722412 = 0.13600386679172516 + 0.01 * 6.692443370819092
Epoch 340, val loss: 0.789307177066803
Epoch 350, training loss: 0.18790681660175323 = 0.12104511260986328 + 0.01 * 6.68617057800293
Epoch 350, val loss: 0.8040196895599365
Epoch 360, training loss: 0.17492952942848206 = 0.10812009125947952 + 0.01 * 6.680944919586182
Epoch 360, val loss: 0.8199750185012817
Epoch 370, training loss: 0.16391651332378387 = 0.09687953442335129 + 0.01 * 6.70369815826416
Epoch 370, val loss: 0.8369389176368713
Epoch 380, training loss: 0.15386974811553955 = 0.08704634755849838 + 0.01 * 6.682339668273926
Epoch 380, val loss: 0.8543914556503296
Epoch 390, training loss: 0.14507031440734863 = 0.07836399972438812 + 0.01 * 6.670630931854248
Epoch 390, val loss: 0.8723092675209045
Epoch 400, training loss: 0.1373213827610016 = 0.07064837217330933 + 0.01 * 6.667301654815674
Epoch 400, val loss: 0.8904580473899841
Epoch 410, training loss: 0.13037794828414917 = 0.06375084072351456 + 0.01 * 6.662710666656494
Epoch 410, val loss: 0.9088770151138306
Epoch 420, training loss: 0.12413491308689117 = 0.05752686783671379 + 0.01 * 6.6608052253723145
Epoch 420, val loss: 0.9273405075073242
Epoch 430, training loss: 0.11873309314250946 = 0.05190948396921158 + 0.01 * 6.682361125946045
Epoch 430, val loss: 0.945859432220459
Epoch 440, training loss: 0.11342233419418335 = 0.046829357743263245 + 0.01 * 6.659297466278076
Epoch 440, val loss: 0.9643649458885193
Epoch 450, training loss: 0.10873611271381378 = 0.04221291467547417 + 0.01 * 6.652319431304932
Epoch 450, val loss: 0.9828458428382874
Epoch 460, training loss: 0.10456198453903198 = 0.038078658282756805 + 0.01 * 6.648332595825195
Epoch 460, val loss: 1.0012224912643433
Epoch 470, training loss: 0.10086782276630402 = 0.034421633929014206 + 0.01 * 6.644618988037109
Epoch 470, val loss: 1.0195093154907227
Epoch 480, training loss: 0.0977543517947197 = 0.031225750222802162 + 0.01 * 6.652860641479492
Epoch 480, val loss: 1.0375803709030151
Epoch 490, training loss: 0.0948440432548523 = 0.028438951820135117 + 0.01 * 6.640508651733398
Epoch 490, val loss: 1.0553390979766846
Epoch 500, training loss: 0.09236949682235718 = 0.02600250393152237 + 0.01 * 6.636699676513672
Epoch 500, val loss: 1.0727245807647705
Epoch 510, training loss: 0.09018389880657196 = 0.02385805919766426 + 0.01 * 6.6325836181640625
Epoch 510, val loss: 1.0894830226898193
Epoch 520, training loss: 0.08824530988931656 = 0.021958982571959496 + 0.01 * 6.62863302230835
Epoch 520, val loss: 1.1058975458145142
Epoch 530, training loss: 0.08660610765218735 = 0.02027706615626812 + 0.01 * 6.632904052734375
Epoch 530, val loss: 1.1216216087341309
Epoch 540, training loss: 0.08497091382741928 = 0.018785392865538597 + 0.01 * 6.618551731109619
Epoch 540, val loss: 1.13687002658844
Epoch 550, training loss: 0.08373279124498367 = 0.017457420006394386 + 0.01 * 6.627537727355957
Epoch 550, val loss: 1.1515053510665894
Epoch 560, training loss: 0.08241575956344604 = 0.016271602362394333 + 0.01 * 6.614415645599365
Epoch 560, val loss: 1.165605902671814
Epoch 570, training loss: 0.08128189295530319 = 0.01520605944097042 + 0.01 * 6.607583522796631
Epoch 570, val loss: 1.1792880296707153
Epoch 580, training loss: 0.08025979995727539 = 0.01424393616616726 + 0.01 * 6.60158634185791
Epoch 580, val loss: 1.1924864053726196
Epoch 590, training loss: 0.07937890291213989 = 0.01337323896586895 + 0.01 * 6.600566387176514
Epoch 590, val loss: 1.205235242843628
Epoch 600, training loss: 0.07855458557605743 = 0.012583554722368717 + 0.01 * 6.597103595733643
Epoch 600, val loss: 1.2175780534744263
Epoch 610, training loss: 0.07778318226337433 = 0.011864248663187027 + 0.01 * 6.591893196105957
Epoch 610, val loss: 1.229575753211975
Epoch 620, training loss: 0.0772179439663887 = 0.011207306757569313 + 0.01 * 6.6010637283325195
Epoch 620, val loss: 1.2411645650863647
Epoch 630, training loss: 0.07649566233158112 = 0.010605884715914726 + 0.01 * 6.588977813720703
Epoch 630, val loss: 1.252345323562622
Epoch 640, training loss: 0.07580964267253876 = 0.010054056532680988 + 0.01 * 6.575558662414551
Epoch 640, val loss: 1.2632490396499634
Epoch 650, training loss: 0.0753917321562767 = 0.009546732529997826 + 0.01 * 6.584499835968018
Epoch 650, val loss: 1.2738202810287476
Epoch 660, training loss: 0.07488587498664856 = 0.009079208597540855 + 0.01 * 6.580666542053223
Epoch 660, val loss: 1.2840352058410645
Epoch 670, training loss: 0.0743134543299675 = 0.008647216483950615 + 0.01 * 6.566623687744141
Epoch 670, val loss: 1.294021725654602
Epoch 680, training loss: 0.07402689754962921 = 0.008247279562056065 + 0.01 * 6.5779619216918945
Epoch 680, val loss: 1.303678274154663
Epoch 690, training loss: 0.07351937890052795 = 0.00787646695971489 + 0.01 * 6.564291477203369
Epoch 690, val loss: 1.313063144683838
Epoch 700, training loss: 0.07310488820075989 = 0.007532116957008839 + 0.01 * 6.557277202606201
Epoch 700, val loss: 1.3222036361694336
Epoch 710, training loss: 0.07275061309337616 = 0.007211368530988693 + 0.01 * 6.553924560546875
Epoch 710, val loss: 1.3311079740524292
Epoch 720, training loss: 0.07256289571523666 = 0.006911770440638065 + 0.01 * 6.565112590789795
Epoch 720, val loss: 1.339770793914795
Epoch 730, training loss: 0.0721832737326622 = 0.006631913594901562 + 0.01 * 6.555136203765869
Epoch 730, val loss: 1.3481957912445068
Epoch 740, training loss: 0.07198649644851685 = 0.0063698855228722095 + 0.01 * 6.5616607666015625
Epoch 740, val loss: 1.3563836812973022
Epoch 750, training loss: 0.07161502540111542 = 0.006124272011220455 + 0.01 * 6.549075126647949
Epoch 750, val loss: 1.364472508430481
Epoch 760, training loss: 0.07127293944358826 = 0.005893693771213293 + 0.01 * 6.537924766540527
Epoch 760, val loss: 1.372274398803711
Epoch 770, training loss: 0.0710308775305748 = 0.005677048582583666 + 0.01 * 6.5353827476501465
Epoch 770, val loss: 1.3799095153808594
Epoch 780, training loss: 0.07075878977775574 = 0.005473149009048939 + 0.01 * 6.528563976287842
Epoch 780, val loss: 1.3874109983444214
Epoch 790, training loss: 0.07074017077684402 = 0.005280813667923212 + 0.01 * 6.54593563079834
Epoch 790, val loss: 1.3946911096572876
Epoch 800, training loss: 0.07040072232484818 = 0.005099662579596043 + 0.01 * 6.530106544494629
Epoch 800, val loss: 1.4018014669418335
Epoch 810, training loss: 0.07019850611686707 = 0.00492887943983078 + 0.01 * 6.526962757110596
Epoch 810, val loss: 1.4087283611297607
Epoch 820, training loss: 0.0700896605849266 = 0.0047673857770860195 + 0.01 * 6.532227516174316
Epoch 820, val loss: 1.415610432624817
Epoch 830, training loss: 0.06979281455278397 = 0.004614576697349548 + 0.01 * 6.517823696136475
Epoch 830, val loss: 1.4222619533538818
Epoch 840, training loss: 0.06958401948213577 = 0.004469944629818201 + 0.01 * 6.511407852172852
Epoch 840, val loss: 1.4287382364273071
Epoch 850, training loss: 0.06955572217702866 = 0.0043329340405762196 + 0.01 * 6.522278308868408
Epoch 850, val loss: 1.435050368309021
Epoch 860, training loss: 0.0693107545375824 = 0.004203074146062136 + 0.01 * 6.510767936706543
Epoch 860, val loss: 1.441346287727356
Epoch 870, training loss: 0.069276824593544 = 0.0040798974223434925 + 0.01 * 6.519692420959473
Epoch 870, val loss: 1.447450876235962
Epoch 880, training loss: 0.0690322294831276 = 0.003963104914873838 + 0.01 * 6.506912708282471
Epoch 880, val loss: 1.4533566236495972
Epoch 890, training loss: 0.06882404536008835 = 0.0038520325906574726 + 0.01 * 6.497201442718506
Epoch 890, val loss: 1.4591799974441528
Epoch 900, training loss: 0.06907567381858826 = 0.0037463458720594645 + 0.01 * 6.532932758331299
Epoch 900, val loss: 1.4648990631103516
Epoch 910, training loss: 0.06857530772686005 = 0.0036458750255405903 + 0.01 * 6.492943286895752
Epoch 910, val loss: 1.470441222190857
Epoch 920, training loss: 0.06850924342870712 = 0.003550130408257246 + 0.01 * 6.495912075042725
Epoch 920, val loss: 1.4759732484817505
Epoch 930, training loss: 0.06837368756532669 = 0.003458835417404771 + 0.01 * 6.491485595703125
Epoch 930, val loss: 1.4812376499176025
Epoch 940, training loss: 0.06824798136949539 = 0.003371933475136757 + 0.01 * 6.487605094909668
Epoch 940, val loss: 1.4864944219589233
Epoch 950, training loss: 0.0682220458984375 = 0.0032889158464968204 + 0.01 * 6.493312835693359
Epoch 950, val loss: 1.491652250289917
Epoch 960, training loss: 0.06840665638446808 = 0.0032094628550112247 + 0.01 * 6.51971960067749
Epoch 960, val loss: 1.496700406074524
Epoch 970, training loss: 0.06794122606515884 = 0.0031335200183093548 + 0.01 * 6.480770587921143
Epoch 970, val loss: 1.5015592575073242
Epoch 980, training loss: 0.06787081807851791 = 0.0030609751120209694 + 0.01 * 6.480984210968018
Epoch 980, val loss: 1.5063906908035278
Epoch 990, training loss: 0.06790979951620102 = 0.0029914684128016233 + 0.01 * 6.491833209991455
Epoch 990, val loss: 1.5110777616500854
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.7048
Flip ASR: 0.6800/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0265724658966064 = 1.9428346157073975 + 0.01 * 8.373793601989746
Epoch 0, val loss: 1.9427545070648193
Epoch 10, training loss: 2.0165390968322754 = 1.9328030347824097 + 0.01 * 8.373604774475098
Epoch 10, val loss: 1.9325511455535889
Epoch 20, training loss: 2.0045199394226074 = 1.920790433883667 + 0.01 * 8.372953414916992
Epoch 20, val loss: 1.9197163581848145
Epoch 30, training loss: 1.9881037473678589 = 1.9043995141983032 + 0.01 * 8.370429039001465
Epoch 30, val loss: 1.9014806747436523
Epoch 40, training loss: 1.96414053440094 = 1.8805866241455078 + 0.01 * 8.355389595031738
Epoch 40, val loss: 1.8748228549957275
Epoch 50, training loss: 1.9294605255126953 = 1.8467316627502441 + 0.01 * 8.272890090942383
Epoch 50, val loss: 1.837974190711975
Epoch 60, training loss: 1.8842439651489258 = 1.804277777671814 + 0.01 * 7.996623516082764
Epoch 60, val loss: 1.7949166297912598
Epoch 70, training loss: 1.8339855670928955 = 1.7598724365234375 + 0.01 * 7.411316394805908
Epoch 70, val loss: 1.7541364431381226
Epoch 80, training loss: 1.781171441078186 = 1.711114525794983 + 0.01 * 7.005696773529053
Epoch 80, val loss: 1.7114918231964111
Epoch 90, training loss: 1.716545581817627 = 1.6475789546966553 + 0.01 * 6.896658897399902
Epoch 90, val loss: 1.6570560932159424
Epoch 100, training loss: 1.632951021194458 = 1.5642997026443481 + 0.01 * 6.865127086639404
Epoch 100, val loss: 1.5868772268295288
Epoch 110, training loss: 1.531324863433838 = 1.462964415550232 + 0.01 * 6.836043357849121
Epoch 110, val loss: 1.5039193630218506
Epoch 120, training loss: 1.4212768077850342 = 1.3531036376953125 + 0.01 * 6.8173112869262695
Epoch 120, val loss: 1.416947841644287
Epoch 130, training loss: 1.3081388473510742 = 1.2401436567306519 + 0.01 * 6.799521446228027
Epoch 130, val loss: 1.329898715019226
Epoch 140, training loss: 1.1934398412704468 = 1.1256108283996582 + 0.01 * 6.782901763916016
Epoch 140, val loss: 1.243370771408081
Epoch 150, training loss: 1.0796605348587036 = 1.0119565725326538 + 0.01 * 6.770392417907715
Epoch 150, val loss: 1.157936692237854
Epoch 160, training loss: 0.9717655777931213 = 0.904126763343811 + 0.01 * 6.763882637023926
Epoch 160, val loss: 1.07757568359375
Epoch 170, training loss: 0.874703049659729 = 0.8070974349975586 + 0.01 * 6.760559558868408
Epoch 170, val loss: 1.0065501928329468
Epoch 180, training loss: 0.7908748984336853 = 0.723303496837616 + 0.01 * 6.757138252258301
Epoch 180, val loss: 0.9473303556442261
Epoch 190, training loss: 0.7197052836418152 = 0.6521701216697693 + 0.01 * 6.753516674041748
Epoch 190, val loss: 0.8998181223869324
Epoch 200, training loss: 0.65852952003479 = 0.591031551361084 + 0.01 * 6.7497968673706055
Epoch 200, val loss: 0.861258864402771
Epoch 210, training loss: 0.6044445037841797 = 0.5369892120361328 + 0.01 * 6.7455291748046875
Epoch 210, val loss: 0.8282250761985779
Epoch 220, training loss: 0.5554634928703308 = 0.48805901408195496 + 0.01 * 6.740447998046875
Epoch 220, val loss: 0.7990943193435669
Epoch 230, training loss: 0.5105658173561096 = 0.4432164132595062 + 0.01 * 6.734938621520996
Epoch 230, val loss: 0.773989737033844
Epoch 240, training loss: 0.4692653715610504 = 0.40198421478271484 + 0.01 * 6.728115081787109
Epoch 240, val loss: 0.7529784440994263
Epoch 250, training loss: 0.4313071370124817 = 0.3640907108783722 + 0.01 * 6.7216410636901855
Epoch 250, val loss: 0.7355806231498718
Epoch 260, training loss: 0.39653804898262024 = 0.3293803930282593 + 0.01 * 6.715766429901123
Epoch 260, val loss: 0.7214704155921936
Epoch 270, training loss: 0.3648003339767456 = 0.2976858615875244 + 0.01 * 6.711449146270752
Epoch 270, val loss: 0.7104997634887695
Epoch 280, training loss: 0.33578771352767944 = 0.2687607407569885 + 0.01 * 6.702695846557617
Epoch 280, val loss: 0.7024311423301697
Epoch 290, training loss: 0.3091784715652466 = 0.2422206997871399 + 0.01 * 6.695778846740723
Epoch 290, val loss: 0.6968494653701782
Epoch 300, training loss: 0.28458017110824585 = 0.21763642132282257 + 0.01 * 6.694375991821289
Epoch 300, val loss: 0.6935994029045105
Epoch 310, training loss: 0.2614794969558716 = 0.1946084201335907 + 0.01 * 6.687106132507324
Epoch 310, val loss: 0.6926429867744446
Epoch 320, training loss: 0.23978665471076965 = 0.17295345664024353 + 0.01 * 6.683319568634033
Epoch 320, val loss: 0.6937487721443176
Epoch 330, training loss: 0.21945592761039734 = 0.152649387717247 + 0.01 * 6.6806535720825195
Epoch 330, val loss: 0.6966045498847961
Epoch 340, training loss: 0.20073308050632477 = 0.1339413970708847 + 0.01 * 6.679168701171875
Epoch 340, val loss: 0.7013240456581116
Epoch 350, training loss: 0.18387450277805328 = 0.1171063557267189 + 0.01 * 6.676815032958984
Epoch 350, val loss: 0.7078239917755127
Epoch 360, training loss: 0.16908979415893555 = 0.1023263931274414 + 0.01 * 6.676340579986572
Epoch 360, val loss: 0.7158203125
Epoch 370, training loss: 0.15633034706115723 = 0.08960118144750595 + 0.01 * 6.672915935516357
Epoch 370, val loss: 0.7251378297805786
Epoch 380, training loss: 0.14547595381736755 = 0.07876810431480408 + 0.01 * 6.6707844734191895
Epoch 380, val loss: 0.7356734275817871
Epoch 390, training loss: 0.13625915348529816 = 0.06957469135522842 + 0.01 * 6.668446063995361
Epoch 390, val loss: 0.7471543550491333
Epoch 400, training loss: 0.1284283697605133 = 0.06175944581627846 + 0.01 * 6.6668925285339355
Epoch 400, val loss: 0.7594039440155029
Epoch 410, training loss: 0.12177237868309021 = 0.055089544504880905 + 0.01 * 6.668282985687256
Epoch 410, val loss: 0.7721929550170898
Epoch 420, training loss: 0.11598920822143555 = 0.049359265714883804 + 0.01 * 6.662994861602783
Epoch 420, val loss: 0.7854911088943481
Epoch 430, training loss: 0.11100845038890839 = 0.0444079153239727 + 0.01 * 6.6600542068481445
Epoch 430, val loss: 0.7990882992744446
Epoch 440, training loss: 0.10668225586414337 = 0.04010916128754616 + 0.01 * 6.6573100090026855
Epoch 440, val loss: 0.8128311038017273
Epoch 450, training loss: 0.10299107432365417 = 0.036361150443553925 + 0.01 * 6.662992477416992
Epoch 450, val loss: 0.8266002535820007
Epoch 460, training loss: 0.09962750226259232 = 0.03308230638504028 + 0.01 * 6.654519557952881
Epoch 460, val loss: 0.84029221534729
Epoch 470, training loss: 0.09670793265104294 = 0.030204983428120613 + 0.01 * 6.650295257568359
Epoch 470, val loss: 0.8537993431091309
Epoch 480, training loss: 0.09418269991874695 = 0.027672523632645607 + 0.01 * 6.651018142700195
Epoch 480, val loss: 0.8670644164085388
Epoch 490, training loss: 0.09187719225883484 = 0.02543778158724308 + 0.01 * 6.643941402435303
Epoch 490, val loss: 0.8799363970756531
Epoch 500, training loss: 0.08986366540193558 = 0.02345537580549717 + 0.01 * 6.640829563140869
Epoch 500, val loss: 0.892505943775177
Epoch 510, training loss: 0.08807110786437988 = 0.021690160036087036 + 0.01 * 6.638094902038574
Epoch 510, val loss: 0.9047527313232422
Epoch 520, training loss: 0.0864759087562561 = 0.020114872604608536 + 0.01 * 6.636104106903076
Epoch 520, val loss: 0.916583776473999
Epoch 530, training loss: 0.08506368845701218 = 0.0187071543186903 + 0.01 * 6.635653495788574
Epoch 530, val loss: 0.9281057119369507
Epoch 540, training loss: 0.08373255282640457 = 0.01744435541331768 + 0.01 * 6.628819465637207
Epoch 540, val loss: 0.9391465783119202
Epoch 550, training loss: 0.08258882164955139 = 0.016306284815073013 + 0.01 * 6.628254413604736
Epoch 550, val loss: 0.9499433636665344
Epoch 560, training loss: 0.0814695805311203 = 0.015277672559022903 + 0.01 * 6.619191646575928
Epoch 560, val loss: 0.9603807330131531
Epoch 570, training loss: 0.08055388927459717 = 0.014345120638608932 + 0.01 * 6.620877742767334
Epoch 570, val loss: 0.9705462455749512
Epoch 580, training loss: 0.07972834259271622 = 0.013498103246092796 + 0.01 * 6.623023986816406
Epoch 580, val loss: 0.9803178310394287
Epoch 590, training loss: 0.07879255712032318 = 0.01272684708237648 + 0.01 * 6.606571674346924
Epoch 590, val loss: 0.9898263216018677
Epoch 600, training loss: 0.07802917063236237 = 0.012022176757454872 + 0.01 * 6.600699424743652
Epoch 600, val loss: 0.9990736246109009
Epoch 610, training loss: 0.07742318511009216 = 0.011376813054084778 + 0.01 * 6.604637145996094
Epoch 610, val loss: 1.008101224899292
Epoch 620, training loss: 0.07670282572507858 = 0.010785152204334736 + 0.01 * 6.591767311096191
Epoch 620, val loss: 1.0167778730392456
Epoch 630, training loss: 0.07612195611000061 = 0.010240819305181503 + 0.01 * 6.588113784790039
Epoch 630, val loss: 1.0252137184143066
Epoch 640, training loss: 0.0758223682641983 = 0.00973875354975462 + 0.01 * 6.608361721038818
Epoch 640, val loss: 1.033470869064331
Epoch 650, training loss: 0.07505318522453308 = 0.009275245480239391 + 0.01 * 6.577794075012207
Epoch 650, val loss: 1.0414949655532837
Epoch 660, training loss: 0.07467438280582428 = 0.008845874108374119 + 0.01 * 6.582850933074951
Epoch 660, val loss: 1.0492497682571411
Epoch 670, training loss: 0.07412336021661758 = 0.008447522297501564 + 0.01 * 6.567584037780762
Epoch 670, val loss: 1.0568852424621582
Epoch 680, training loss: 0.07411398738622665 = 0.00807693786919117 + 0.01 * 6.603705406188965
Epoch 680, val loss: 1.0643260478973389
Epoch 690, training loss: 0.07336278259754181 = 0.007732506841421127 + 0.01 * 6.563027381896973
Epoch 690, val loss: 1.0714704990386963
Epoch 700, training loss: 0.07298119366168976 = 0.007410969119518995 + 0.01 * 6.557022571563721
Epoch 700, val loss: 1.0784550905227661
Epoch 710, training loss: 0.07260435819625854 = 0.007110149599611759 + 0.01 * 6.5494208335876465
Epoch 710, val loss: 1.0853303670883179
Epoch 720, training loss: 0.07254113256931305 = 0.006828043144196272 + 0.01 * 6.571309566497803
Epoch 720, val loss: 1.09210205078125
Epoch 730, training loss: 0.07201094180345535 = 0.006564249750226736 + 0.01 * 6.544669151306152
Epoch 730, val loss: 1.0986008644104004
Epoch 740, training loss: 0.07172414660453796 = 0.006316606421023607 + 0.01 * 6.5407538414001465
Epoch 740, val loss: 1.1049870252609253
Epoch 750, training loss: 0.07147466391324997 = 0.0060842689126729965 + 0.01 * 6.539039611816406
Epoch 750, val loss: 1.1112518310546875
Epoch 760, training loss: 0.07124428451061249 = 0.0058655631728470325 + 0.01 * 6.537871837615967
Epoch 760, val loss: 1.1173019409179688
Epoch 770, training loss: 0.07095042616128922 = 0.005659214686602354 + 0.01 * 6.529120922088623
Epoch 770, val loss: 1.123286485671997
Epoch 780, training loss: 0.0706835463643074 = 0.0054649910889565945 + 0.01 * 6.521855354309082
Epoch 780, val loss: 1.1291381120681763
Epoch 790, training loss: 0.0704655721783638 = 0.005281057674437761 + 0.01 * 6.51845121383667
Epoch 790, val loss: 1.1348336935043335
Epoch 800, training loss: 0.07032501697540283 = 0.0051073431968688965 + 0.01 * 6.521767616271973
Epoch 800, val loss: 1.1404833793640137
Epoch 810, training loss: 0.07015053182840347 = 0.004942574538290501 + 0.01 * 6.520795822143555
Epoch 810, val loss: 1.1459496021270752
Epoch 820, training loss: 0.06998913735151291 = 0.004787427373230457 + 0.01 * 6.520171165466309
Epoch 820, val loss: 1.1513608694076538
Epoch 830, training loss: 0.06979740411043167 = 0.004639470484107733 + 0.01 * 6.515793800354004
Epoch 830, val loss: 1.1565407514572144
Epoch 840, training loss: 0.06956902146339417 = 0.00449984148144722 + 0.01 * 6.506917476654053
Epoch 840, val loss: 1.161646842956543
Epoch 850, training loss: 0.06958705931901932 = 0.004367178771644831 + 0.01 * 6.5219879150390625
Epoch 850, val loss: 1.166688323020935
Epoch 860, training loss: 0.06933925300836563 = 0.004241242539137602 + 0.01 * 6.509801387786865
Epoch 860, val loss: 1.1714752912521362
Epoch 870, training loss: 0.0690917894244194 = 0.004121323116123676 + 0.01 * 6.49704647064209
Epoch 870, val loss: 1.176256775856018
Epoch 880, training loss: 0.06898057460784912 = 0.004007067531347275 + 0.01 * 6.497350215911865
Epoch 880, val loss: 1.1809889078140259
Epoch 890, training loss: 0.06897944211959839 = 0.003899242961779237 + 0.01 * 6.50801944732666
Epoch 890, val loss: 1.1855295896530151
Epoch 900, training loss: 0.0687924325466156 = 0.003795384429395199 + 0.01 * 6.499704837799072
Epoch 900, val loss: 1.1900206804275513
Epoch 910, training loss: 0.06858329474925995 = 0.00369652034714818 + 0.01 * 6.488677024841309
Epoch 910, val loss: 1.1943987607955933
Epoch 920, training loss: 0.0685340166091919 = 0.003602627431973815 + 0.01 * 6.493139266967773
Epoch 920, val loss: 1.198738932609558
Epoch 930, training loss: 0.0684366300702095 = 0.0035128870513290167 + 0.01 * 6.492373943328857
Epoch 930, val loss: 1.2028369903564453
Epoch 940, training loss: 0.06839895993471146 = 0.00342703890055418 + 0.01 * 6.497192859649658
Epoch 940, val loss: 1.206986904144287
Epoch 950, training loss: 0.06816334277391434 = 0.003344795433804393 + 0.01 * 6.4818549156188965
Epoch 950, val loss: 1.2110508680343628
Epoch 960, training loss: 0.06805040687322617 = 0.003266541985794902 + 0.01 * 6.478386878967285
Epoch 960, val loss: 1.2149617671966553
Epoch 970, training loss: 0.06800984591245651 = 0.0031913414131850004 + 0.01 * 6.4818501472473145
Epoch 970, val loss: 1.2187665700912476
Epoch 980, training loss: 0.0678236186504364 = 0.003119290340691805 + 0.01 * 6.470432758331299
Epoch 980, val loss: 1.2226200103759766
Epoch 990, training loss: 0.06778409332036972 = 0.0030501799192279577 + 0.01 * 6.473392009735107
Epoch 990, val loss: 1.2263154983520508
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9594
Flip ASR: 0.9511/225 nodes
The final ASR:0.72079, 0.18865, Accuracy:0.82840, 0.00462
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11614])
remove edge: torch.Size([2, 9516])
updated graph: torch.Size([2, 10574])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
The final ASR:0.97417, 0.00000, Accuracy:0.83086, 0.00349
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0195090770721436 = 1.9357720613479614 + 0.01 * 8.373710632324219
Epoch 0, val loss: 1.9316539764404297
Epoch 10, training loss: 2.0093190670013428 = 1.9255833625793457 + 0.01 * 8.373576164245605
Epoch 10, val loss: 1.9215549230575562
Epoch 20, training loss: 1.9967366456985474 = 1.9130065441131592 + 0.01 * 8.37301254272461
Epoch 20, val loss: 1.9089864492416382
Epoch 30, training loss: 1.978898048400879 = 1.8951926231384277 + 0.01 * 8.370546340942383
Epoch 30, val loss: 1.8913978338241577
Epoch 40, training loss: 1.9525185823440552 = 1.8689879179000854 + 0.01 * 8.353069305419922
Epoch 40, val loss: 1.8661977052688599
Epoch 50, training loss: 1.9156478643417358 = 1.8330161571502686 + 0.01 * 8.263172149658203
Epoch 50, val loss: 1.833619475364685
Epoch 60, training loss: 1.8724573850631714 = 1.7932322025299072 + 0.01 * 7.922518253326416
Epoch 60, val loss: 1.8013478517532349
Epoch 70, training loss: 1.8318694829940796 = 1.7549867630004883 + 0.01 * 7.688271999359131
Epoch 70, val loss: 1.7710542678833008
Epoch 80, training loss: 1.7784111499786377 = 1.7041411399841309 + 0.01 * 7.426995277404785
Epoch 80, val loss: 1.7265095710754395
Epoch 90, training loss: 1.7058231830596924 = 1.6339436769485474 + 0.01 * 7.18794584274292
Epoch 90, val loss: 1.6664403676986694
Epoch 100, training loss: 1.6137360334396362 = 1.5442405939102173 + 0.01 * 6.94954776763916
Epoch 100, val loss: 1.5936707258224487
Epoch 110, training loss: 1.514061450958252 = 1.445389747619629 + 0.01 * 6.867171764373779
Epoch 110, val loss: 1.5134440660476685
Epoch 120, training loss: 1.4127761125564575 = 1.3448389768600464 + 0.01 * 6.793708324432373
Epoch 120, val loss: 1.43476402759552
Epoch 130, training loss: 1.3117328882217407 = 1.2443385124206543 + 0.01 * 6.739433288574219
Epoch 130, val loss: 1.3583717346191406
Epoch 140, training loss: 1.2123080492019653 = 1.1451222896575928 + 0.01 * 6.718579292297363
Epoch 140, val loss: 1.2860568761825562
Epoch 150, training loss: 1.11539626121521 = 1.0483258962631226 + 0.01 * 6.70703125
Epoch 150, val loss: 1.2177820205688477
Epoch 160, training loss: 1.021588683128357 = 0.9546247124671936 + 0.01 * 6.696400165557861
Epoch 160, val loss: 1.152199625968933
Epoch 170, training loss: 0.9318020343780518 = 0.8649210929870605 + 0.01 * 6.6880974769592285
Epoch 170, val loss: 1.0892589092254639
Epoch 180, training loss: 0.847302258014679 = 0.7804942727088928 + 0.01 * 6.6807966232299805
Epoch 180, val loss: 1.030157208442688
Epoch 190, training loss: 0.7690227031707764 = 0.7022705674171448 + 0.01 * 6.675212860107422
Epoch 190, val loss: 0.9763091802597046
Epoch 200, training loss: 0.697426974773407 = 0.6307252645492554 + 0.01 * 6.670170307159424
Epoch 200, val loss: 0.9286289811134338
Epoch 210, training loss: 0.632829487323761 = 0.566174328327179 + 0.01 * 6.665514945983887
Epoch 210, val loss: 0.8872789740562439
Epoch 220, training loss: 0.575160562992096 = 0.5085474848747253 + 0.01 * 6.661310195922852
Epoch 220, val loss: 0.8523567914962769
Epoch 230, training loss: 0.5236107707023621 = 0.4570389688014984 + 0.01 * 6.6571807861328125
Epoch 230, val loss: 0.8234185576438904
Epoch 240, training loss: 0.47686877846717834 = 0.41033080220222473 + 0.01 * 6.653798580169678
Epoch 240, val loss: 0.7996487617492676
Epoch 250, training loss: 0.433848112821579 = 0.3673206567764282 + 0.01 * 6.652746677398682
Epoch 250, val loss: 0.7803134322166443
Epoch 260, training loss: 0.3939294219017029 = 0.3274281620979309 + 0.01 * 6.650125503540039
Epoch 260, val loss: 0.764434278011322
Epoch 270, training loss: 0.3568073809146881 = 0.2903132140636444 + 0.01 * 6.649417400360107
Epoch 270, val loss: 0.7510977387428284
Epoch 280, training loss: 0.32221096754074097 = 0.2557208836078644 + 0.01 * 6.649008750915527
Epoch 280, val loss: 0.7396287322044373
Epoch 290, training loss: 0.2899255156517029 = 0.22343961894512177 + 0.01 * 6.648590087890625
Epoch 290, val loss: 0.7295739054679871
Epoch 300, training loss: 0.2600680887699127 = 0.19358707964420319 + 0.01 * 6.64810037612915
Epoch 300, val loss: 0.7207257151603699
Epoch 310, training loss: 0.2330830693244934 = 0.16660863161087036 + 0.01 * 6.6474432945251465
Epoch 310, val loss: 0.7138457894325256
Epoch 320, training loss: 0.20935827493667603 = 0.14289194345474243 + 0.01 * 6.646632194519043
Epoch 320, val loss: 0.7093960046768188
Epoch 330, training loss: 0.1890057921409607 = 0.1225467324256897 + 0.01 * 6.6459059715271
Epoch 330, val loss: 0.7075667381286621
Epoch 340, training loss: 0.1718195676803589 = 0.10537087917327881 + 0.01 * 6.64486837387085
Epoch 340, val loss: 0.7085257768630981
Epoch 350, training loss: 0.1574246883392334 = 0.09098949283361435 + 0.01 * 6.643520355224609
Epoch 350, val loss: 0.7119973301887512
Epoch 360, training loss: 0.14541497826576233 = 0.07899148017168045 + 0.01 * 6.642349720001221
Epoch 360, val loss: 0.7175285220146179
Epoch 370, training loss: 0.13539215922355652 = 0.06898171454668045 + 0.01 * 6.641044616699219
Epoch 370, val loss: 0.7248060703277588
Epoch 380, training loss: 0.12702377140522003 = 0.060609448701143265 + 0.01 * 6.641432285308838
Epoch 380, val loss: 0.7334240078926086
Epoch 390, training loss: 0.1199561357498169 = 0.05357561260461807 + 0.01 * 6.638052463531494
Epoch 390, val loss: 0.7429618835449219
Epoch 400, training loss: 0.11400077491998672 = 0.047632910311222076 + 0.01 * 6.636786460876465
Epoch 400, val loss: 0.7532061338424683
Epoch 410, training loss: 0.10893319547176361 = 0.04258366674184799 + 0.01 * 6.634953022003174
Epoch 410, val loss: 0.763879656791687
Epoch 420, training loss: 0.1046154648065567 = 0.03826972469687462 + 0.01 * 6.634573459625244
Epoch 420, val loss: 0.7747966647148132
Epoch 430, training loss: 0.10088303685188293 = 0.03456313908100128 + 0.01 * 6.6319899559021
Epoch 430, val loss: 0.785872220993042
Epoch 440, training loss: 0.09765677899122238 = 0.03136034309864044 + 0.01 * 6.629643440246582
Epoch 440, val loss: 0.7968990206718445
Epoch 450, training loss: 0.09485384076833725 = 0.028577150776982307 + 0.01 * 6.627669334411621
Epoch 450, val loss: 0.8078455328941345
Epoch 460, training loss: 0.09245271980762482 = 0.02614542469382286 + 0.01 * 6.6307291984558105
Epoch 460, val loss: 0.8186783194541931
Epoch 470, training loss: 0.090255007147789 = 0.024009406566619873 + 0.01 * 6.6245598793029785
Epoch 470, val loss: 0.829322099685669
Epoch 480, training loss: 0.08833720535039902 = 0.022121818736195564 + 0.01 * 6.6215386390686035
Epoch 480, val loss: 0.8397685289382935
Epoch 490, training loss: 0.08665680885314941 = 0.020442096516489983 + 0.01 * 6.621471405029297
Epoch 490, val loss: 0.8500654101371765
Epoch 500, training loss: 0.08512096852064133 = 0.018943270668387413 + 0.01 * 6.617769718170166
Epoch 500, val loss: 0.8602228760719299
Epoch 510, training loss: 0.083755262196064 = 0.01760086417198181 + 0.01 * 6.6154398918151855
Epoch 510, val loss: 0.8701863884925842
Epoch 520, training loss: 0.08251741528511047 = 0.016393350437283516 + 0.01 * 6.6124067306518555
Epoch 520, val loss: 0.879874050617218
Epoch 530, training loss: 0.08142819255590439 = 0.015304473228752613 + 0.01 * 6.612371921539307
Epoch 530, val loss: 0.8893985152244568
Epoch 540, training loss: 0.08042272180318832 = 0.014321562834084034 + 0.01 * 6.610116004943848
Epoch 540, val loss: 0.898668646812439
Epoch 550, training loss: 0.07947886735200882 = 0.013430465944111347 + 0.01 * 6.60483980178833
Epoch 550, val loss: 0.9077415466308594
Epoch 560, training loss: 0.07864023000001907 = 0.012620016001164913 + 0.01 * 6.602021217346191
Epoch 560, val loss: 0.9166602492332458
Epoch 570, training loss: 0.07800760865211487 = 0.01188277080655098 + 0.01 * 6.612484455108643
Epoch 570, val loss: 0.9252952337265015
Epoch 580, training loss: 0.07720880955457687 = 0.011211949400603771 + 0.01 * 6.599686622619629
Epoch 580, val loss: 0.9336602091789246
Epoch 590, training loss: 0.0765303373336792 = 0.01059818547219038 + 0.01 * 6.5932159423828125
Epoch 590, val loss: 0.9418409466743469
Epoch 600, training loss: 0.07599327713251114 = 0.010035060346126556 + 0.01 * 6.595821857452393
Epoch 600, val loss: 0.9498287439346313
Epoch 610, training loss: 0.07540936768054962 = 0.009517857804894447 + 0.01 * 6.589151382446289
Epoch 610, val loss: 0.9576295018196106
Epoch 620, training loss: 0.07486692070960999 = 0.009041082113981247 + 0.01 * 6.582584381103516
Epoch 620, val loss: 0.9652116298675537
Epoch 630, training loss: 0.07438816130161285 = 0.00860099121928215 + 0.01 * 6.57871675491333
Epoch 630, val loss: 0.9726302623748779
Epoch 640, training loss: 0.07414994388818741 = 0.00819355808198452 + 0.01 * 6.595638275146484
Epoch 640, val loss: 0.9798268675804138
Epoch 650, training loss: 0.07359655946493149 = 0.007816636934876442 + 0.01 * 6.5779924392700195
Epoch 650, val loss: 0.986867368221283
Epoch 660, training loss: 0.07312977313995361 = 0.007466570474207401 + 0.01 * 6.566320419311523
Epoch 660, val loss: 0.9936797022819519
Epoch 670, training loss: 0.07275090366601944 = 0.007140397094190121 + 0.01 * 6.5610504150390625
Epoch 670, val loss: 1.0003859996795654
Epoch 680, training loss: 0.0725850835442543 = 0.006836770102381706 + 0.01 * 6.574831485748291
Epoch 680, val loss: 1.0068612098693848
Epoch 690, training loss: 0.0721062645316124 = 0.006553990766406059 + 0.01 * 6.555227279663086
Epoch 690, val loss: 1.013155221939087
Epoch 700, training loss: 0.07183362543582916 = 0.006289407145231962 + 0.01 * 6.554421901702881
Epoch 700, val loss: 1.0193352699279785
Epoch 710, training loss: 0.07152450829744339 = 0.006041805259883404 + 0.01 * 6.5482707023620605
Epoch 710, val loss: 1.0253902673721313
Epoch 720, training loss: 0.07134640216827393 = 0.0058099315501749516 + 0.01 * 6.553647041320801
Epoch 720, val loss: 1.031265377998352
Epoch 730, training loss: 0.07102777063846588 = 0.005592395085841417 + 0.01 * 6.543537139892578
Epoch 730, val loss: 1.0370434522628784
Epoch 740, training loss: 0.07073801755905151 = 0.005388333462178707 + 0.01 * 6.534968376159668
Epoch 740, val loss: 1.042602777481079
Epoch 750, training loss: 0.07045704871416092 = 0.005196230486035347 + 0.01 * 6.5260820388793945
Epoch 750, val loss: 1.0480880737304688
Epoch 760, training loss: 0.07035868614912033 = 0.005015142261981964 + 0.01 * 6.5343546867370605
Epoch 760, val loss: 1.0534276962280273
Epoch 770, training loss: 0.07015486061573029 = 0.0048448373563587666 + 0.01 * 6.531002521514893
Epoch 770, val loss: 1.0586810111999512
Epoch 780, training loss: 0.06985200196504593 = 0.004684003069996834 + 0.01 * 6.5167999267578125
Epoch 780, val loss: 1.0637407302856445
Epoch 790, training loss: 0.06974420696496964 = 0.004531837068498135 + 0.01 * 6.521237373352051
Epoch 790, val loss: 1.0687353610992432
Epoch 800, training loss: 0.06964541971683502 = 0.004388059489428997 + 0.01 * 6.525736331939697
Epoch 800, val loss: 1.0736329555511475
Epoch 810, training loss: 0.06932776421308517 = 0.004252268932759762 + 0.01 * 6.50754976272583
Epoch 810, val loss: 1.0783650875091553
Epoch 820, training loss: 0.06920584291219711 = 0.004123436752706766 + 0.01 * 6.508240699768066
Epoch 820, val loss: 1.083021879196167
Epoch 830, training loss: 0.06928115338087082 = 0.004001028370112181 + 0.01 * 6.528012752532959
Epoch 830, val loss: 1.0875909328460693
Epoch 840, training loss: 0.06891646981239319 = 0.003885079175233841 + 0.01 * 6.503138542175293
Epoch 840, val loss: 1.0920523405075073
Epoch 850, training loss: 0.0687650591135025 = 0.003774686250835657 + 0.01 * 6.499037265777588
Epoch 850, val loss: 1.0963643789291382
Epoch 860, training loss: 0.06875723600387573 = 0.0036696589086204767 + 0.01 * 6.508758068084717
Epoch 860, val loss: 1.100656509399414
Epoch 870, training loss: 0.06859409064054489 = 0.003569635096937418 + 0.01 * 6.502446174621582
Epoch 870, val loss: 1.104849934577942
Epoch 880, training loss: 0.06845689564943314 = 0.0034745712764561176 + 0.01 * 6.498232364654541
Epoch 880, val loss: 1.1088942289352417
Epoch 890, training loss: 0.06832675635814667 = 0.003383827628567815 + 0.01 * 6.494292736053467
Epoch 890, val loss: 1.1128696203231812
Epoch 900, training loss: 0.06825818121433258 = 0.0032973422203212976 + 0.01 * 6.496083736419678
Epoch 900, val loss: 1.1167796850204468
Epoch 910, training loss: 0.06809056550264359 = 0.0032148200552910566 + 0.01 * 6.487574577331543
Epoch 910, val loss: 1.1206165552139282
Epoch 920, training loss: 0.06791196018457413 = 0.003135942853987217 + 0.01 * 6.477602005004883
Epoch 920, val loss: 1.124322772026062
Epoch 930, training loss: 0.06799359619617462 = 0.0030606056097894907 + 0.01 * 6.493298530578613
Epoch 930, val loss: 1.127859354019165
Epoch 940, training loss: 0.0679190382361412 = 0.002989009255543351 + 0.01 * 6.4930033683776855
Epoch 940, val loss: 1.131474494934082
Epoch 950, training loss: 0.0676775872707367 = 0.0029205172322690487 + 0.01 * 6.475707530975342
Epoch 950, val loss: 1.1348072290420532
Epoch 960, training loss: 0.06752422451972961 = 0.002854785183444619 + 0.01 * 6.466943740844727
Epoch 960, val loss: 1.1381770372390747
Epoch 970, training loss: 0.06758467853069305 = 0.002791613107547164 + 0.01 * 6.479306697845459
Epoch 970, val loss: 1.141501784324646
Epoch 980, training loss: 0.06736251711845398 = 0.0027310599107295275 + 0.01 * 6.4631452560424805
Epoch 980, val loss: 1.1447645425796509
Epoch 990, training loss: 0.06733784824609756 = 0.002673015696927905 + 0.01 * 6.466483116149902
Epoch 990, val loss: 1.147953748703003
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.5203
Flip ASR: 0.4267/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0486900806427 = 1.9649522304534912 + 0.01 * 8.37378978729248
Epoch 0, val loss: 1.964752435684204
Epoch 10, training loss: 2.037201404571533 = 1.9534648656845093 + 0.01 * 8.373662948608398
Epoch 10, val loss: 1.9528447389602661
Epoch 20, training loss: 2.023059368133545 = 1.9393277168273926 + 0.01 * 8.373174667358398
Epoch 20, val loss: 1.9375746250152588
Epoch 30, training loss: 2.0032849311828613 = 1.9195691347122192 + 0.01 * 8.371575355529785
Epoch 30, val loss: 1.915664792060852
Epoch 40, training loss: 1.9741532802581787 = 1.8905210494995117 + 0.01 * 8.363227844238281
Epoch 40, val loss: 1.8834017515182495
Epoch 50, training loss: 1.9332573413848877 = 1.8501054048538208 + 0.01 * 8.315195083618164
Epoch 50, val loss: 1.8399561643600464
Epoch 60, training loss: 1.8854261636734009 = 1.8046294450759888 + 0.01 * 8.079673767089844
Epoch 60, val loss: 1.7950736284255981
Epoch 70, training loss: 1.8432186841964722 = 1.7654210329055786 + 0.01 * 7.779764652252197
Epoch 70, val loss: 1.7608510255813599
Epoch 80, training loss: 1.7952700853347778 = 1.720381259918213 + 0.01 * 7.488883972167969
Epoch 80, val loss: 1.7224291563034058
Epoch 90, training loss: 1.7309237718582153 = 1.657568335533142 + 0.01 * 7.335540294647217
Epoch 90, val loss: 1.6698389053344727
Epoch 100, training loss: 1.646785020828247 = 1.5745463371276855 + 0.01 * 7.223866939544678
Epoch 100, val loss: 1.6015715599060059
Epoch 110, training loss: 1.5508475303649902 = 1.4802443981170654 + 0.01 * 7.060308456420898
Epoch 110, val loss: 1.5267459154129028
Epoch 120, training loss: 1.4583914279937744 = 1.3888822793960571 + 0.01 * 6.950913429260254
Epoch 120, val loss: 1.4590388536453247
Epoch 130, training loss: 1.375609278678894 = 1.3068352937698364 + 0.01 * 6.87739372253418
Epoch 130, val loss: 1.405017375946045
Epoch 140, training loss: 1.301424503326416 = 1.2330158948898315 + 0.01 * 6.8408637046813965
Epoch 140, val loss: 1.3606175184249878
Epoch 150, training loss: 1.2328704595565796 = 1.1648039817810059 + 0.01 * 6.806649684906006
Epoch 150, val loss: 1.3204247951507568
Epoch 160, training loss: 1.1670845746994019 = 1.0993120670318604 + 0.01 * 6.777254581451416
Epoch 160, val loss: 1.2811285257339478
Epoch 170, training loss: 1.102847695350647 = 1.0352896451950073 + 0.01 * 6.755801677703857
Epoch 170, val loss: 1.2411783933639526
Epoch 180, training loss: 1.0403521060943604 = 0.9729385375976562 + 0.01 * 6.74135160446167
Epoch 180, val loss: 1.2000672817230225
Epoch 190, training loss: 0.9800498485565186 = 0.9127076268196106 + 0.01 * 6.734221458435059
Epoch 190, val loss: 1.1587716341018677
Epoch 200, training loss: 0.921003520488739 = 0.8536919355392456 + 0.01 * 6.731160640716553
Epoch 200, val loss: 1.116986870765686
Epoch 210, training loss: 0.8608686923980713 = 0.7935762405395508 + 0.01 * 6.729243278503418
Epoch 210, val loss: 1.073424220085144
Epoch 220, training loss: 0.7973530888557434 = 0.730075478553772 + 0.01 * 6.72775936126709
Epoch 220, val loss: 1.0261567831039429
Epoch 230, training loss: 0.7296863794326782 = 0.6624257564544678 + 0.01 * 6.726059436798096
Epoch 230, val loss: 0.9739598035812378
Epoch 240, training loss: 0.6595314741134644 = 0.5922918915748596 + 0.01 * 6.72395658493042
Epoch 240, val loss: 0.9187082052230835
Epoch 250, training loss: 0.5905481576919556 = 0.5233351588249207 + 0.01 * 6.721298694610596
Epoch 250, val loss: 0.8641870617866516
Epoch 260, training loss: 0.5265881419181824 = 0.4593967795372009 + 0.01 * 6.71913480758667
Epoch 260, val loss: 0.8154137134552002
Epoch 270, training loss: 0.46980130672454834 = 0.4026567041873932 + 0.01 * 6.714460372924805
Epoch 270, val loss: 0.7753698229789734
Epoch 280, training loss: 0.42047780752182007 = 0.35338231921195984 + 0.01 * 6.709549903869629
Epoch 280, val loss: 0.7452617883682251
Epoch 290, training loss: 0.3777790367603302 = 0.310744047164917 + 0.01 * 6.703499794006348
Epoch 290, val loss: 0.724359393119812
Epoch 300, training loss: 0.34050309658050537 = 0.27352625131607056 + 0.01 * 6.697684288024902
Epoch 300, val loss: 0.7110136151313782
Epoch 310, training loss: 0.30759817361831665 = 0.2406923919916153 + 0.01 * 6.690579891204834
Epoch 310, val loss: 0.7036704421043396
Epoch 320, training loss: 0.2784848213195801 = 0.2116488218307495 + 0.01 * 6.6836018562316895
Epoch 320, val loss: 0.7011968493461609
Epoch 330, training loss: 0.2529428005218506 = 0.18618683516979218 + 0.01 * 6.675597667694092
Epoch 330, val loss: 0.7023890018463135
Epoch 340, training loss: 0.23091915249824524 = 0.16413673758506775 + 0.01 * 6.6782426834106445
Epoch 340, val loss: 0.7061946392059326
Epoch 350, training loss: 0.2118157297372818 = 0.14517366886138916 + 0.01 * 6.664206504821777
Epoch 350, val loss: 0.7119327187538147
Epoch 360, training loss: 0.19547142088413239 = 0.12890316545963287 + 0.01 * 6.656825542449951
Epoch 360, val loss: 0.7189387083053589
Epoch 370, training loss: 0.18138091266155243 = 0.11487376689910889 + 0.01 * 6.650714874267578
Epoch 370, val loss: 0.7270705699920654
Epoch 380, training loss: 0.16918163001537323 = 0.10272574424743652 + 0.01 * 6.645588397979736
Epoch 380, val loss: 0.7360484004020691
Epoch 390, training loss: 0.15864896774291992 = 0.09218259900808334 + 0.01 * 6.646637439727783
Epoch 390, val loss: 0.745672881603241
Epoch 400, training loss: 0.1494511365890503 = 0.08306463062763214 + 0.01 * 6.638650417327881
Epoch 400, val loss: 0.7557318806648254
Epoch 410, training loss: 0.14153431355953217 = 0.07517623901367188 + 0.01 * 6.635807514190674
Epoch 410, val loss: 0.7660729885101318
Epoch 420, training loss: 0.13466334342956543 = 0.06833309680223465 + 0.01 * 6.633024215698242
Epoch 420, val loss: 0.7767706513404846
Epoch 430, training loss: 0.1286955177783966 = 0.062365494668483734 + 0.01 * 6.633002281188965
Epoch 430, val loss: 0.7876755595207214
Epoch 440, training loss: 0.12340753525495529 = 0.05712281912565231 + 0.01 * 6.628471851348877
Epoch 440, val loss: 0.7987730503082275
Epoch 450, training loss: 0.11879517138004303 = 0.0525004006922245 + 0.01 * 6.629477500915527
Epoch 450, val loss: 0.8099457025527954
Epoch 460, training loss: 0.11467348784208298 = 0.0484079048037529 + 0.01 * 6.626558303833008
Epoch 460, val loss: 0.8211101293563843
Epoch 470, training loss: 0.11096281558275223 = 0.04475031793117523 + 0.01 * 6.621249675750732
Epoch 470, val loss: 0.832236647605896
Epoch 480, training loss: 0.1076677143573761 = 0.041477132588624954 + 0.01 * 6.619057655334473
Epoch 480, val loss: 0.8433919548988342
Epoch 490, training loss: 0.1047423779964447 = 0.0385298952460289 + 0.01 * 6.621248245239258
Epoch 490, val loss: 0.8543674349784851
Epoch 500, training loss: 0.1020098477602005 = 0.03586283326148987 + 0.01 * 6.614701747894287
Epoch 500, val loss: 0.8652184009552002
Epoch 510, training loss: 0.09953740984201431 = 0.033429473638534546 + 0.01 * 6.610793590545654
Epoch 510, val loss: 0.8759944438934326
Epoch 520, training loss: 0.09727539122104645 = 0.031199466437101364 + 0.01 * 6.6075921058654785
Epoch 520, val loss: 0.8865943551063538
Epoch 530, training loss: 0.09522887319326401 = 0.02915242314338684 + 0.01 * 6.607645511627197
Epoch 530, val loss: 0.8971866965293884
Epoch 540, training loss: 0.0932723879814148 = 0.027264144271612167 + 0.01 * 6.600823879241943
Epoch 540, val loss: 0.9075144529342651
Epoch 550, training loss: 0.09148523211479187 = 0.025506606325507164 + 0.01 * 6.597862243652344
Epoch 550, val loss: 0.9178407788276672
Epoch 560, training loss: 0.0898200199007988 = 0.023860804736614227 + 0.01 * 6.595921993255615
Epoch 560, val loss: 0.9280738234519958
Epoch 570, training loss: 0.08821802586317062 = 0.02231699414551258 + 0.01 * 6.5901031494140625
Epoch 570, val loss: 0.938113272190094
Epoch 580, training loss: 0.08679385483264923 = 0.02086862176656723 + 0.01 * 6.592523574829102
Epoch 580, val loss: 0.9480753540992737
Epoch 590, training loss: 0.08537033200263977 = 0.01951659470796585 + 0.01 * 6.585373878479004
Epoch 590, val loss: 0.9579673409461975
Epoch 600, training loss: 0.08407244086265564 = 0.018258189782500267 + 0.01 * 6.581424713134766
Epoch 600, val loss: 0.9675941467285156
Epoch 610, training loss: 0.08291509747505188 = 0.01709173619747162 + 0.01 * 6.58233642578125
Epoch 610, val loss: 0.9772515296936035
Epoch 620, training loss: 0.08172514289617538 = 0.016017034649848938 + 0.01 * 6.5708112716674805
Epoch 620, val loss: 0.986752986907959
Epoch 630, training loss: 0.0807076245546341 = 0.015029434114694595 + 0.01 * 6.567818641662598
Epoch 630, val loss: 0.9961637258529663
Epoch 640, training loss: 0.07973694056272507 = 0.014123618602752686 + 0.01 * 6.5613322257995605
Epoch 640, val loss: 1.005449891090393
Epoch 650, training loss: 0.07889503240585327 = 0.013293925672769547 + 0.01 * 6.560110569000244
Epoch 650, val loss: 1.014412760734558
Epoch 660, training loss: 0.07807585597038269 = 0.01253824308514595 + 0.01 * 6.553761005401611
Epoch 660, val loss: 1.0231494903564453
Epoch 670, training loss: 0.07735998183488846 = 0.011845034547150135 + 0.01 * 6.551494598388672
Epoch 670, val loss: 1.0316760540008545
Epoch 680, training loss: 0.07667500525712967 = 0.01120723970234394 + 0.01 * 6.546777248382568
Epoch 680, val loss: 1.0398141145706177
Epoch 690, training loss: 0.07617407292127609 = 0.010618148371577263 + 0.01 * 6.555592060089111
Epoch 690, val loss: 1.0480279922485352
Epoch 700, training loss: 0.07545129954814911 = 0.010071019642055035 + 0.01 * 6.538028717041016
Epoch 700, val loss: 1.055909276008606
Epoch 710, training loss: 0.07482817769050598 = 0.009546215645968914 + 0.01 * 6.528196334838867
Epoch 710, val loss: 1.0637173652648926
Epoch 720, training loss: 0.0746275782585144 = 0.009054014459252357 + 0.01 * 6.557356357574463
Epoch 720, val loss: 1.0709975957870483
Epoch 730, training loss: 0.07381423562765121 = 0.008599438704550266 + 0.01 * 6.521480083465576
Epoch 730, val loss: 1.0781829357147217
Epoch 740, training loss: 0.07342901080846786 = 0.008184333331882954 + 0.01 * 6.524467468261719
Epoch 740, val loss: 1.0851060152053833
Epoch 750, training loss: 0.07291626185178757 = 0.007799879647791386 + 0.01 * 6.511638164520264
Epoch 750, val loss: 1.0920161008834839
Epoch 760, training loss: 0.072713702917099 = 0.007437624037265778 + 0.01 * 6.527608394622803
Epoch 760, val loss: 1.0984936952590942
Epoch 770, training loss: 0.07214877754449844 = 0.0071014822460711 + 0.01 * 6.504729270935059
Epoch 770, val loss: 1.1048413515090942
Epoch 780, training loss: 0.07180227339267731 = 0.006787236779928207 + 0.01 * 6.501503944396973
Epoch 780, val loss: 1.1110498905181885
Epoch 790, training loss: 0.07155952602624893 = 0.006493841297924519 + 0.01 * 6.506568431854248
Epoch 790, val loss: 1.116976261138916
Epoch 800, training loss: 0.07118555903434753 = 0.006219483911991119 + 0.01 * 6.496607780456543
Epoch 800, val loss: 1.1229645013809204
Epoch 810, training loss: 0.07093821465969086 = 0.005961388349533081 + 0.01 * 6.497683048248291
Epoch 810, val loss: 1.1287291049957275
Epoch 820, training loss: 0.07063490897417068 = 0.0057186828926205635 + 0.01 * 6.491622447967529
Epoch 820, val loss: 1.1343779563903809
Epoch 830, training loss: 0.070376917719841 = 0.005488756578415632 + 0.01 * 6.488816261291504
Epoch 830, val loss: 1.1398766040802002
Epoch 840, training loss: 0.07009627670049667 = 0.005273104645311832 + 0.01 * 6.4823174476623535
Epoch 840, val loss: 1.1452860832214355
Epoch 850, training loss: 0.06987041234970093 = 0.0050709443166852 + 0.01 * 6.479946613311768
Epoch 850, val loss: 1.150549054145813
Epoch 860, training loss: 0.06967415660619736 = 0.0048815603367984295 + 0.01 * 6.479259967803955
Epoch 860, val loss: 1.1556271314620972
Epoch 870, training loss: 0.06955670565366745 = 0.004703843500465155 + 0.01 * 6.485286712646484
Epoch 870, val loss: 1.1606593132019043
Epoch 880, training loss: 0.06931640952825546 = 0.004538483452051878 + 0.01 * 6.477792739868164
Epoch 880, val loss: 1.1654092073440552
Epoch 890, training loss: 0.06909376382827759 = 0.0043828971683979034 + 0.01 * 6.4710869789123535
Epoch 890, val loss: 1.1701743602752686
Epoch 900, training loss: 0.06888176500797272 = 0.004234146326780319 + 0.01 * 6.464761734008789
Epoch 900, val loss: 1.174761176109314
Epoch 910, training loss: 0.06873667985200882 = 0.004093268420547247 + 0.01 * 6.464341163635254
Epoch 910, val loss: 1.179276466369629
Epoch 920, training loss: 0.06856419146060944 = 0.003963342867791653 + 0.01 * 6.460084915161133
Epoch 920, val loss: 1.183666467666626
Epoch 930, training loss: 0.06850805133581161 = 0.003840504912659526 + 0.01 * 6.46675443649292
Epoch 930, val loss: 1.1879466772079468
Epoch 940, training loss: 0.06847836077213287 = 0.0037246798165142536 + 0.01 * 6.475368022918701
Epoch 940, val loss: 1.1921926736831665
Epoch 950, training loss: 0.06812465935945511 = 0.0036146652419120073 + 0.01 * 6.450999736785889
Epoch 950, val loss: 1.1963510513305664
Epoch 960, training loss: 0.06807652860879898 = 0.0035109345335513353 + 0.01 * 6.456559181213379
Epoch 960, val loss: 1.2003642320632935
Epoch 970, training loss: 0.0679248571395874 = 0.0034124027006328106 + 0.01 * 6.451245307922363
Epoch 970, val loss: 1.2043696641921997
Epoch 980, training loss: 0.06765966862440109 = 0.0033191938418895006 + 0.01 * 6.434047222137451
Epoch 980, val loss: 1.2081819772720337
Epoch 990, training loss: 0.06765539944171906 = 0.0032314839772880077 + 0.01 * 6.442392349243164
Epoch 990, val loss: 1.2119419574737549
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.3875
Flip ASR: 0.3556/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0092148780822754 = 1.925477147102356 + 0.01 * 8.373783111572266
Epoch 0, val loss: 1.9157679080963135
Epoch 10, training loss: 1.9998321533203125 = 1.916095495223999 + 0.01 * 8.373671531677246
Epoch 10, val loss: 1.907128930091858
Epoch 20, training loss: 1.988435983657837 = 1.9047034978866577 + 0.01 * 8.373254776000977
Epoch 20, val loss: 1.8963098526000977
Epoch 30, training loss: 1.9727606773376465 = 1.8890429735183716 + 0.01 * 8.371766090393066
Epoch 30, val loss: 1.8810802698135376
Epoch 40, training loss: 1.9500185251235962 = 1.866388201713562 + 0.01 * 8.363036155700684
Epoch 40, val loss: 1.8590917587280273
Epoch 50, training loss: 1.9178656339645386 = 1.8347760438919067 + 0.01 * 8.308954238891602
Epoch 50, val loss: 1.8294963836669922
Epoch 60, training loss: 1.8771677017211914 = 1.7966421842575073 + 0.01 * 8.052553176879883
Epoch 60, val loss: 1.7965145111083984
Epoch 70, training loss: 1.8330714702606201 = 1.7561101913452148 + 0.01 * 7.696133613586426
Epoch 70, val loss: 1.7633661031723022
Epoch 80, training loss: 1.7788763046264648 = 1.7037732601165771 + 0.01 * 7.5102996826171875
Epoch 80, val loss: 1.717684030532837
Epoch 90, training loss: 1.7041438817977905 = 1.630733847618103 + 0.01 * 7.34100341796875
Epoch 90, val loss: 1.6540260314941406
Epoch 100, training loss: 1.6083283424377441 = 1.536828875541687 + 0.01 * 7.1499505043029785
Epoch 100, val loss: 1.5760104656219482
Epoch 110, training loss: 1.5007331371307373 = 1.4309837818145752 + 0.01 * 6.974938869476318
Epoch 110, val loss: 1.4898290634155273
Epoch 120, training loss: 1.392196774482727 = 1.3228623867034912 + 0.01 * 6.933441638946533
Epoch 120, val loss: 1.4050042629241943
Epoch 130, training loss: 1.2862505912780762 = 1.2172253131866455 + 0.01 * 6.902534008026123
Epoch 130, val loss: 1.325660228729248
Epoch 140, training loss: 1.184906244277954 = 1.1161067485809326 + 0.01 * 6.879950523376465
Epoch 140, val loss: 1.2513669729232788
Epoch 150, training loss: 1.08887779712677 = 1.0203379392623901 + 0.01 * 6.853980541229248
Epoch 150, val loss: 1.1806867122650146
Epoch 160, training loss: 0.99854576587677 = 0.9302617907524109 + 0.01 * 6.828397274017334
Epoch 160, val loss: 1.1146950721740723
Epoch 170, training loss: 0.9141964316368103 = 0.846153974533081 + 0.01 * 6.8042449951171875
Epoch 170, val loss: 1.054057240486145
Epoch 180, training loss: 0.8355318903923035 = 0.7676897048950195 + 0.01 * 6.784217357635498
Epoch 180, val loss: 0.9983053803443909
Epoch 190, training loss: 0.7615947127342224 = 0.6938874125480652 + 0.01 * 6.770727634429932
Epoch 190, val loss: 0.9469766020774841
Epoch 200, training loss: 0.6914210915565491 = 0.6237775683403015 + 0.01 * 6.764350414276123
Epoch 200, val loss: 0.8993765115737915
Epoch 210, training loss: 0.6245715022087097 = 0.5569538474082947 + 0.01 * 6.7617645263671875
Epoch 210, val loss: 0.855087399482727
Epoch 220, training loss: 0.5615131258964539 = 0.4939216375350952 + 0.01 * 6.759150981903076
Epoch 220, val loss: 0.8144610524177551
Epoch 230, training loss: 0.5032719969749451 = 0.4357144832611084 + 0.01 * 6.7557501792907715
Epoch 230, val loss: 0.7781315445899963
Epoch 240, training loss: 0.45057976245880127 = 0.38306114077568054 + 0.01 * 6.751863956451416
Epoch 240, val loss: 0.7471250295639038
Epoch 250, training loss: 0.4034583568572998 = 0.33598417043685913 + 0.01 * 6.747416973114014
Epoch 250, val loss: 0.7218548059463501
Epoch 260, training loss: 0.36142924427986145 = 0.29400721192359924 + 0.01 * 6.7422027587890625
Epoch 260, val loss: 0.7022599577903748
Epoch 270, training loss: 0.32393157482147217 = 0.2565716505050659 + 0.01 * 6.735993385314941
Epoch 270, val loss: 0.6880643367767334
Epoch 280, training loss: 0.29055511951446533 = 0.22325710952281952 + 0.01 * 6.729800701141357
Epoch 280, val loss: 0.6788626909255981
Epoch 290, training loss: 0.261043906211853 = 0.19382493197917938 + 0.01 * 6.721896171569824
Epoch 290, val loss: 0.6743326187133789
Epoch 300, training loss: 0.23524707555770874 = 0.16811788082122803 + 0.01 * 6.712920665740967
Epoch 300, val loss: 0.6741644740104675
Epoch 310, training loss: 0.21296986937522888 = 0.14591945707798004 + 0.01 * 6.705040454864502
Epoch 310, val loss: 0.6778690814971924
Epoch 320, training loss: 0.19386905431747437 = 0.12691538035869598 + 0.01 * 6.69536828994751
Epoch 320, val loss: 0.6847785115242004
Epoch 330, training loss: 0.17758072912693024 = 0.11072658002376556 + 0.01 * 6.685415267944336
Epoch 330, val loss: 0.6942009925842285
Epoch 340, training loss: 0.16378441452980042 = 0.09697690606117249 + 0.01 * 6.680751800537109
Epoch 340, val loss: 0.7054826021194458
Epoch 350, training loss: 0.15198922157287598 = 0.08529013395309448 + 0.01 * 6.669908046722412
Epoch 350, val loss: 0.7181157469749451
Epoch 360, training loss: 0.14197182655334473 = 0.07533053308725357 + 0.01 * 6.66412878036499
Epoch 360, val loss: 0.7315998077392578
Epoch 370, training loss: 0.13340139389038086 = 0.0668119266629219 + 0.01 * 6.6589460372924805
Epoch 370, val loss: 0.7455809712409973
Epoch 380, training loss: 0.12605270743370056 = 0.05949995666742325 + 0.01 * 6.655276298522949
Epoch 380, val loss: 0.7598406076431274
Epoch 390, training loss: 0.11971674859523773 = 0.05320339649915695 + 0.01 * 6.6513352394104
Epoch 390, val loss: 0.7741352319717407
Epoch 400, training loss: 0.11424951255321503 = 0.04776464030146599 + 0.01 * 6.648487091064453
Epoch 400, val loss: 0.7882978916168213
Epoch 410, training loss: 0.10951394587755203 = 0.04304937273263931 + 0.01 * 6.646457672119141
Epoch 410, val loss: 0.802240788936615
Epoch 420, training loss: 0.10537846386432648 = 0.038946885615587234 + 0.01 * 6.643158435821533
Epoch 420, val loss: 0.815819501876831
Epoch 430, training loss: 0.10179279744625092 = 0.035364631563425064 + 0.01 * 6.64281702041626
Epoch 430, val loss: 0.8290634155273438
Epoch 440, training loss: 0.0986083596944809 = 0.03222622722387314 + 0.01 * 6.638213157653809
Epoch 440, val loss: 0.8419556021690369
Epoch 450, training loss: 0.09583697468042374 = 0.029464857652783394 + 0.01 * 6.63721227645874
Epoch 450, val loss: 0.8544281125068665
Epoch 460, training loss: 0.09335565567016602 = 0.027025839313864708 + 0.01 * 6.632981300354004
Epoch 460, val loss: 0.8665040135383606
Epoch 470, training loss: 0.09117061644792557 = 0.02485738880932331 + 0.01 * 6.631323337554932
Epoch 470, val loss: 0.8782321214675903
Epoch 480, training loss: 0.08920931071043015 = 0.022930612787604332 + 0.01 * 6.627870082855225
Epoch 480, val loss: 0.8896990418434143
Epoch 490, training loss: 0.08747629821300507 = 0.02120872586965561 + 0.01 * 6.6267571449279785
Epoch 490, val loss: 0.900887668132782
Epoch 500, training loss: 0.08588870614767075 = 0.019667865708470345 + 0.01 * 6.622084140777588
Epoch 500, val loss: 0.9116596579551697
Epoch 510, training loss: 0.08447925746440887 = 0.018283408135175705 + 0.01 * 6.619585037231445
Epoch 510, val loss: 0.9221907258033752
Epoch 520, training loss: 0.08318647742271423 = 0.017036357894539833 + 0.01 * 6.615012168884277
Epoch 520, val loss: 0.9324554800987244
Epoch 530, training loss: 0.08207016438245773 = 0.01590949296951294 + 0.01 * 6.616066932678223
Epoch 530, val loss: 0.9423925876617432
Epoch 540, training loss: 0.0810013860464096 = 0.01489039696753025 + 0.01 * 6.6110992431640625
Epoch 540, val loss: 0.9520054459571838
Epoch 550, training loss: 0.08000657707452774 = 0.01396616455167532 + 0.01 * 6.60404109954834
Epoch 550, val loss: 0.9613711833953857
Epoch 560, training loss: 0.0791301503777504 = 0.013124717399477959 + 0.01 * 6.600543022155762
Epoch 560, val loss: 0.9704583287239075
Epoch 570, training loss: 0.07832136750221252 = 0.012357150204479694 + 0.01 * 6.596421241760254
Epoch 570, val loss: 0.979275643825531
Epoch 580, training loss: 0.07758104801177979 = 0.011655915528535843 + 0.01 * 6.5925140380859375
Epoch 580, val loss: 0.9879053235054016
Epoch 590, training loss: 0.07690761983394623 = 0.011013584211468697 + 0.01 * 6.5894036293029785
Epoch 590, val loss: 0.9962830543518066
Epoch 600, training loss: 0.0762922391295433 = 0.010424892418086529 + 0.01 * 6.586734771728516
Epoch 600, val loss: 1.0045329332351685
Epoch 610, training loss: 0.07570897787809372 = 0.009884546510875225 + 0.01 * 6.5824432373046875
Epoch 610, val loss: 1.012447476387024
Epoch 620, training loss: 0.07518532127141953 = 0.009386532939970493 + 0.01 * 6.579878807067871
Epoch 620, val loss: 1.0202116966247559
Epoch 630, training loss: 0.07470650970935822 = 0.00892637949436903 + 0.01 * 6.5780134201049805
Epoch 630, val loss: 1.0277466773986816
Epoch 640, training loss: 0.0742608830332756 = 0.008500068448483944 + 0.01 * 6.576081275939941
Epoch 640, val loss: 1.0350919961929321
Epoch 650, training loss: 0.07382383942604065 = 0.008104213513433933 + 0.01 * 6.571962833404541
Epoch 650, val loss: 1.0422515869140625
Epoch 660, training loss: 0.07342372834682465 = 0.007734948303550482 + 0.01 * 6.568878173828125
Epoch 660, val loss: 1.0492630004882812
Epoch 670, training loss: 0.07312459498643875 = 0.007390181068331003 + 0.01 * 6.573441982269287
Epoch 670, val loss: 1.056137204170227
Epoch 680, training loss: 0.07272709906101227 = 0.007068832870572805 + 0.01 * 6.565826416015625
Epoch 680, val loss: 1.062804102897644
Epoch 690, training loss: 0.0724027082324028 = 0.006767595186829567 + 0.01 * 6.563511848449707
Epoch 690, val loss: 1.0693414211273193
Epoch 700, training loss: 0.07208812236785889 = 0.006486040074378252 + 0.01 * 6.560207843780518
Epoch 700, val loss: 1.0757832527160645
Epoch 710, training loss: 0.07181695848703384 = 0.006222916767001152 + 0.01 * 6.559404373168945
Epoch 710, val loss: 1.0821306705474854
Epoch 720, training loss: 0.07152590900659561 = 0.0059769474901258945 + 0.01 * 6.554896831512451
Epoch 720, val loss: 1.0883103609085083
Epoch 730, training loss: 0.07128337025642395 = 0.005746625829488039 + 0.01 * 6.553674697875977
Epoch 730, val loss: 1.0943557024002075
Epoch 740, training loss: 0.07107409834861755 = 0.0055303629487752914 + 0.01 * 6.554373741149902
Epoch 740, val loss: 1.1002525091171265
Epoch 750, training loss: 0.07082732021808624 = 0.0053270123898983 + 0.01 * 6.5500311851501465
Epoch 750, val loss: 1.1060535907745361
Epoch 760, training loss: 0.07058382779359818 = 0.005135555285960436 + 0.01 * 6.544826984405518
Epoch 760, val loss: 1.1116994619369507
Epoch 770, training loss: 0.0704672634601593 = 0.004955206532031298 + 0.01 * 6.551205158233643
Epoch 770, val loss: 1.1172891855239868
Epoch 780, training loss: 0.0702536478638649 = 0.0047854348085820675 + 0.01 * 6.546821594238281
Epoch 780, val loss: 1.1226636171340942
Epoch 790, training loss: 0.0700056403875351 = 0.004625621251761913 + 0.01 * 6.538002014160156
Epoch 790, val loss: 1.1280180215835571
Epoch 800, training loss: 0.06990545988082886 = 0.00447441590949893 + 0.01 * 6.543104648590088
Epoch 800, val loss: 1.1332449913024902
Epoch 810, training loss: 0.06972596794366837 = 0.004331485368311405 + 0.01 * 6.539448261260986
Epoch 810, val loss: 1.1383306980133057
Epoch 820, training loss: 0.06952173262834549 = 0.0041964370757341385 + 0.01 * 6.532529830932617
Epoch 820, val loss: 1.143326997756958
Epoch 830, training loss: 0.06936290115118027 = 0.004068329464644194 + 0.01 * 6.529457092285156
Epoch 830, val loss: 1.1482481956481934
Epoch 840, training loss: 0.06921634823083878 = 0.003946791402995586 + 0.01 * 6.526956081390381
Epoch 840, val loss: 1.1530715227127075
Epoch 850, training loss: 0.06941888481378555 = 0.003831245005130768 + 0.01 * 6.558764457702637
Epoch 850, val loss: 1.1578518152236938
Epoch 860, training loss: 0.06906071305274963 = 0.0037218586076050997 + 0.01 * 6.533885955810547
Epoch 860, val loss: 1.1624031066894531
Epoch 870, training loss: 0.0688592791557312 = 0.003617757000029087 + 0.01 * 6.5241522789001465
Epoch 870, val loss: 1.166909098625183
Epoch 880, training loss: 0.06869897246360779 = 0.0035185199230909348 + 0.01 * 6.518045425415039
Epoch 880, val loss: 1.1713848114013672
Epoch 890, training loss: 0.068704754114151 = 0.003423980437219143 + 0.01 * 6.528077125549316
Epoch 890, val loss: 1.1757313013076782
Epoch 900, training loss: 0.06852643191814423 = 0.0033339716028422117 + 0.01 * 6.5192461013793945
Epoch 900, val loss: 1.1799578666687012
Epoch 910, training loss: 0.06838700920343399 = 0.003248377703130245 + 0.01 * 6.513863563537598
Epoch 910, val loss: 1.184195637702942
Epoch 920, training loss: 0.06835860013961792 = 0.003166475798934698 + 0.01 * 6.51921272277832
Epoch 920, val loss: 1.188292384147644
Epoch 930, training loss: 0.06821262836456299 = 0.0030883734580129385 + 0.01 * 6.512425899505615
Epoch 930, val loss: 1.1923495531082153
Epoch 940, training loss: 0.06811781227588654 = 0.003013597335666418 + 0.01 * 6.5104217529296875
Epoch 940, val loss: 1.1962932348251343
Epoch 950, training loss: 0.06806369870901108 = 0.0029422612860798836 + 0.01 * 6.512143611907959
Epoch 950, val loss: 1.2002336978912354
Epoch 960, training loss: 0.06790897995233536 = 0.002873961115255952 + 0.01 * 6.5035014152526855
Epoch 960, val loss: 1.2039718627929688
Epoch 970, training loss: 0.06789182126522064 = 0.002808603923767805 + 0.01 * 6.508322238922119
Epoch 970, val loss: 1.2077875137329102
Epoch 980, training loss: 0.06776204705238342 = 0.0027459338307380676 + 0.01 * 6.501611232757568
Epoch 980, val loss: 1.2114295959472656
Epoch 990, training loss: 0.0677301213145256 = 0.0026860779616981745 + 0.01 * 6.5044050216674805
Epoch 990, val loss: 1.215085506439209
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.8708
Flip ASR: 0.8444/225 nodes
The final ASR:0.59287, 0.20391, Accuracy:0.81852, 0.02117
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11590])
remove edge: torch.Size([2, 9554])
updated graph: torch.Size([2, 10588])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9963
Flip ASR: 0.9956/225 nodes
The final ASR:0.98401, 0.00920, Accuracy:0.82840, 0.00462
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.022747755050659 = 1.9390093088150024 + 0.01 * 8.373851776123047
Epoch 0, val loss: 1.9315861463546753
Epoch 10, training loss: 2.0128376483917236 = 1.929099678993225 + 0.01 * 8.373786926269531
Epoch 10, val loss: 1.9224752187728882
Epoch 20, training loss: 2.000490427017212 = 1.916754961013794 + 0.01 * 8.37354850769043
Epoch 20, val loss: 1.9107236862182617
Epoch 30, training loss: 1.9829628467559814 = 1.8992347717285156 + 0.01 * 8.372807502746582
Epoch 30, val loss: 1.8938052654266357
Epoch 40, training loss: 1.9570586681365967 = 1.8733690977096558 + 0.01 * 8.368956565856934
Epoch 40, val loss: 1.869137167930603
Epoch 50, training loss: 1.9210374355316162 = 1.8376212120056152 + 0.01 * 8.34162425994873
Epoch 50, val loss: 1.8369381427764893
Epoch 60, training loss: 1.879275918006897 = 1.7972217798233032 + 0.01 * 8.20541763305664
Epoch 60, val loss: 1.8047842979431152
Epoch 70, training loss: 1.8373713493347168 = 1.7582005262374878 + 0.01 * 7.917079925537109
Epoch 70, val loss: 1.7755132913589478
Epoch 80, training loss: 1.7838566303253174 = 1.707879900932312 + 0.01 * 7.597675800323486
Epoch 80, val loss: 1.732979655265808
Epoch 90, training loss: 1.7119659185409546 = 1.6390923261642456 + 0.01 * 7.287363529205322
Epoch 90, val loss: 1.6737940311431885
Epoch 100, training loss: 1.622624158859253 = 1.552043080329895 + 0.01 * 7.058104038238525
Epoch 100, val loss: 1.6008204221725464
Epoch 110, training loss: 1.526052474975586 = 1.4562077522277832 + 0.01 * 6.984470367431641
Epoch 110, val loss: 1.5224993228912354
Epoch 120, training loss: 1.4284493923187256 = 1.3589844703674316 + 0.01 * 6.946489334106445
Epoch 120, val loss: 1.4479795694351196
Epoch 130, training loss: 1.3306429386138916 = 1.2613472938537598 + 0.01 * 6.9295654296875
Epoch 130, val loss: 1.3764463663101196
Epoch 140, training loss: 1.2324683666229248 = 1.1633614301681519 + 0.01 * 6.910688877105713
Epoch 140, val loss: 1.3053134679794312
Epoch 150, training loss: 1.1358171701431274 = 1.066889762878418 + 0.01 * 6.892739295959473
Epoch 150, val loss: 1.2360315322875977
Epoch 160, training loss: 1.0429654121398926 = 0.9742220640182495 + 0.01 * 6.8743367195129395
Epoch 160, val loss: 1.169888973236084
Epoch 170, training loss: 0.9550349712371826 = 0.8864870071411133 + 0.01 * 6.854798793792725
Epoch 170, val loss: 1.1070247888565063
Epoch 180, training loss: 0.8720412254333496 = 0.8036789298057556 + 0.01 * 6.836226463317871
Epoch 180, val loss: 1.0477287769317627
Epoch 190, training loss: 0.7941621541976929 = 0.7259054183959961 + 0.01 * 6.825672149658203
Epoch 190, val loss: 0.9921988248825073
Epoch 200, training loss: 0.7217022776603699 = 0.6535373330116272 + 0.01 * 6.816492557525635
Epoch 200, val loss: 0.9414964318275452
Epoch 210, training loss: 0.655247688293457 = 0.5871306657791138 + 0.01 * 6.811700344085693
Epoch 210, val loss: 0.8966320157051086
Epoch 220, training loss: 0.5953772664070129 = 0.5272881388664246 + 0.01 * 6.8089118003845215
Epoch 220, val loss: 0.8581624031066895
Epoch 230, training loss: 0.5421326756477356 = 0.47407102584838867 + 0.01 * 6.806164741516113
Epoch 230, val loss: 0.8266555070877075
Epoch 240, training loss: 0.4945472776889801 = 0.4265168607234955 + 0.01 * 6.803041934967041
Epoch 240, val loss: 0.8019787669181824
Epoch 250, training loss: 0.45103883743286133 = 0.38304510712623596 + 0.01 * 6.79937219619751
Epoch 250, val loss: 0.7831936478614807
Epoch 260, training loss: 0.4102931320667267 = 0.3423442542552948 + 0.01 * 6.794887542724609
Epoch 260, val loss: 0.7692930698394775
Epoch 270, training loss: 0.3718089461326599 = 0.3038632571697235 + 0.01 * 6.794569969177246
Epoch 270, val loss: 0.7592844367027283
Epoch 280, training loss: 0.3356677293777466 = 0.26781052350997925 + 0.01 * 6.785721302032471
Epoch 280, val loss: 0.7524715065956116
Epoch 290, training loss: 0.30255648493766785 = 0.23476295173168182 + 0.01 * 6.779354572296143
Epoch 290, val loss: 0.7490177154541016
Epoch 300, training loss: 0.2729714810848236 = 0.20525558292865753 + 0.01 * 6.771589279174805
Epoch 300, val loss: 0.7490629553794861
Epoch 310, training loss: 0.2471570074558258 = 0.17949725687503815 + 0.01 * 6.765975475311279
Epoch 310, val loss: 0.7523711323738098
Epoch 320, training loss: 0.22488334774971008 = 0.15732713043689728 + 0.01 * 6.755621910095215
Epoch 320, val loss: 0.7585654258728027
Epoch 330, training loss: 0.20577335357666016 = 0.13831289112567902 + 0.01 * 6.74604606628418
Epoch 330, val loss: 0.7671505212783813
Epoch 340, training loss: 0.18937046825885773 = 0.1219978779554367 + 0.01 * 6.7372589111328125
Epoch 340, val loss: 0.7777025699615479
Epoch 350, training loss: 0.1752968430519104 = 0.10796254128217697 + 0.01 * 6.733429431915283
Epoch 350, val loss: 0.7897087931632996
Epoch 360, training loss: 0.16305723786354065 = 0.0958305075764656 + 0.01 * 6.7226738929748535
Epoch 360, val loss: 0.8027688264846802
Epoch 370, training loss: 0.1524387151002884 = 0.08528659492731094 + 0.01 * 6.715212345123291
Epoch 370, val loss: 0.8166442513465881
Epoch 380, training loss: 0.14321772754192352 = 0.07609023153781891 + 0.01 * 6.71274995803833
Epoch 380, val loss: 0.8311665058135986
Epoch 390, training loss: 0.13514873385429382 = 0.06805330514907837 + 0.01 * 6.709542274475098
Epoch 390, val loss: 0.8460960388183594
Epoch 400, training loss: 0.12801851332187653 = 0.061016228049993515 + 0.01 * 6.700229167938232
Epoch 400, val loss: 0.8612044453620911
Epoch 410, training loss: 0.12181966006755829 = 0.05484766140580177 + 0.01 * 6.69719934463501
Epoch 410, val loss: 0.8763896822929382
Epoch 420, training loss: 0.11645069718360901 = 0.04943568632006645 + 0.01 * 6.70150089263916
Epoch 420, val loss: 0.8914721012115479
Epoch 430, training loss: 0.11160635203123093 = 0.044684506952762604 + 0.01 * 6.692184925079346
Epoch 430, val loss: 0.9063844084739685
Epoch 440, training loss: 0.10740703344345093 = 0.040505100041627884 + 0.01 * 6.6901936531066895
Epoch 440, val loss: 0.9209896922111511
Epoch 450, training loss: 0.1036750078201294 = 0.03681974112987518 + 0.01 * 6.6855268478393555
Epoch 450, val loss: 0.9352625012397766
Epoch 460, training loss: 0.1004028394818306 = 0.033562466502189636 + 0.01 * 6.684037685394287
Epoch 460, val loss: 0.9492030143737793
Epoch 470, training loss: 0.0974845439195633 = 0.030674662441015244 + 0.01 * 6.68098783493042
Epoch 470, val loss: 0.9627647399902344
Epoch 480, training loss: 0.09489739686250687 = 0.028106652200222015 + 0.01 * 6.679074287414551
Epoch 480, val loss: 0.975947380065918
Epoch 490, training loss: 0.09258981049060822 = 0.02581779658794403 + 0.01 * 6.677201747894287
Epoch 490, val loss: 0.9887145757675171
Epoch 500, training loss: 0.09048959612846375 = 0.02377069741487503 + 0.01 * 6.6718902587890625
Epoch 500, val loss: 1.0010931491851807
Epoch 510, training loss: 0.0886831060051918 = 0.021934479475021362 + 0.01 * 6.674862861633301
Epoch 510, val loss: 1.013092041015625
Epoch 520, training loss: 0.08697856962680817 = 0.020285744220018387 + 0.01 * 6.669281959533691
Epoch 520, val loss: 1.0247011184692383
Epoch 530, training loss: 0.08544258773326874 = 0.018800022080540657 + 0.01 * 6.664257049560547
Epoch 530, val loss: 1.0359963178634644
Epoch 540, training loss: 0.08406670391559601 = 0.0174579955637455 + 0.01 * 6.6608710289001465
Epoch 540, val loss: 1.0469335317611694
Epoch 550, training loss: 0.08283991366624832 = 0.016243310645222664 + 0.01 * 6.659660816192627
Epoch 550, val loss: 1.057579755783081
Epoch 560, training loss: 0.08171777427196503 = 0.015143165364861488 + 0.01 * 6.657461166381836
Epoch 560, val loss: 1.067943811416626
Epoch 570, training loss: 0.08069643378257751 = 0.014145582914352417 + 0.01 * 6.655085563659668
Epoch 570, val loss: 1.07796311378479
Epoch 580, training loss: 0.07975335419178009 = 0.013237969018518925 + 0.01 * 6.651538848876953
Epoch 580, val loss: 1.0877587795257568
Epoch 590, training loss: 0.0788910835981369 = 0.012410967610776424 + 0.01 * 6.648011684417725
Epoch 590, val loss: 1.0972687005996704
Epoch 600, training loss: 0.07818909734487534 = 0.011656144633889198 + 0.01 * 6.653295516967773
Epoch 600, val loss: 1.1065229177474976
Epoch 610, training loss: 0.07742547988891602 = 0.010966813191771507 + 0.01 * 6.645866870880127
Epoch 610, val loss: 1.1155714988708496
Epoch 620, training loss: 0.07675604522228241 = 0.010335628874599934 + 0.01 * 6.642041206359863
Epoch 620, val loss: 1.1243263483047485
Epoch 630, training loss: 0.07613547146320343 = 0.009756561368703842 + 0.01 * 6.637890815734863
Epoch 630, val loss: 1.132880687713623
Epoch 640, training loss: 0.07555539906024933 = 0.00922477338463068 + 0.01 * 6.633062839508057
Epoch 640, val loss: 1.1412047147750854
Epoch 650, training loss: 0.07507120072841644 = 0.008735531009733677 + 0.01 * 6.6335673332214355
Epoch 650, val loss: 1.149328589439392
Epoch 660, training loss: 0.07458961009979248 = 0.008285025134682655 + 0.01 * 6.630458831787109
Epoch 660, val loss: 1.1571632623672485
Epoch 670, training loss: 0.07414063066244125 = 0.007869635708630085 + 0.01 * 6.62709903717041
Epoch 670, val loss: 1.1648833751678467
Epoch 680, training loss: 0.07370955497026443 = 0.007485387846827507 + 0.01 * 6.622416973114014
Epoch 680, val loss: 1.1723830699920654
Epoch 690, training loss: 0.07332881540060043 = 0.00712928082793951 + 0.01 * 6.619953632354736
Epoch 690, val loss: 1.1796969175338745
Epoch 700, training loss: 0.0730639174580574 = 0.006798795890063047 + 0.01 * 6.626512050628662
Epoch 700, val loss: 1.1868396997451782
Epoch 710, training loss: 0.07263359427452087 = 0.006492451298981905 + 0.01 * 6.614114761352539
Epoch 710, val loss: 1.1937826871871948
Epoch 720, training loss: 0.07234201580286026 = 0.006207733880728483 + 0.01 * 6.613428592681885
Epoch 720, val loss: 1.2005548477172852
Epoch 730, training loss: 0.0722305178642273 = 0.005942407064139843 + 0.01 * 6.628811359405518
Epoch 730, val loss: 1.2071665525436401
Epoch 740, training loss: 0.07176952064037323 = 0.0056951940059661865 + 0.01 * 6.607432842254639
Epoch 740, val loss: 1.2135754823684692
Epoch 750, training loss: 0.0714743509888649 = 0.005464066285640001 + 0.01 * 6.601027965545654
Epoch 750, val loss: 1.219861626625061
Epoch 760, training loss: 0.07140173017978668 = 0.005247736349701881 + 0.01 * 6.6153998374938965
Epoch 760, val loss: 1.2259984016418457
Epoch 770, training loss: 0.0710018128156662 = 0.005045379977673292 + 0.01 * 6.595643520355225
Epoch 770, val loss: 1.231986165046692
Epoch 780, training loss: 0.07084233313798904 = 0.004855574574321508 + 0.01 * 6.5986762046813965
Epoch 780, val loss: 1.2378177642822266
Epoch 790, training loss: 0.07058968394994736 = 0.004677537363022566 + 0.01 * 6.591215133666992
Epoch 790, val loss: 1.2435458898544312
Epoch 800, training loss: 0.07036636769771576 = 0.004510051570832729 + 0.01 * 6.585631370544434
Epoch 800, val loss: 1.2491592168807983
Epoch 810, training loss: 0.07032200694084167 = 0.004352532792836428 + 0.01 * 6.59694766998291
Epoch 810, val loss: 1.2545361518859863
Epoch 820, training loss: 0.07006533443927765 = 0.0042043039575219154 + 0.01 * 6.586103439331055
Epoch 820, val loss: 1.2599334716796875
Epoch 830, training loss: 0.07016859948635101 = 0.004064425826072693 + 0.01 * 6.61041784286499
Epoch 830, val loss: 1.265087604522705
Epoch 840, training loss: 0.06966894865036011 = 0.003932815510779619 + 0.01 * 6.573613166809082
Epoch 840, val loss: 1.2701928615570068
Epoch 850, training loss: 0.06954528391361237 = 0.003808275330811739 + 0.01 * 6.57370138168335
Epoch 850, val loss: 1.2751057147979736
Epoch 860, training loss: 0.06938514113426208 = 0.0036906113382428885 + 0.01 * 6.569453716278076
Epoch 860, val loss: 1.2798831462860107
Epoch 870, training loss: 0.06922786682844162 = 0.00357936043292284 + 0.01 * 6.564850807189941
Epoch 870, val loss: 1.2846969366073608
Epoch 880, training loss: 0.06911694258451462 = 0.0034737619571387768 + 0.01 * 6.564318656921387
Epoch 880, val loss: 1.2893273830413818
Epoch 890, training loss: 0.0689355656504631 = 0.0033737521152943373 + 0.01 * 6.55618143081665
Epoch 890, val loss: 1.2938319444656372
Epoch 900, training loss: 0.06893449276685715 = 0.0032787900418043137 + 0.01 * 6.56557035446167
Epoch 900, val loss: 1.298247218132019
Epoch 910, training loss: 0.06881032884120941 = 0.003188882954418659 + 0.01 * 6.562144756317139
Epoch 910, val loss: 1.3026014566421509
Epoch 920, training loss: 0.06860071420669556 = 0.003103532362729311 + 0.01 * 6.549718379974365
Epoch 920, val loss: 1.3068751096725464
Epoch 930, training loss: 0.0685138925909996 = 0.0030223403591662645 + 0.01 * 6.549155235290527
Epoch 930, val loss: 1.3109207153320312
Epoch 940, training loss: 0.06842165440320969 = 0.0029452412854880095 + 0.01 * 6.547640800476074
Epoch 940, val loss: 1.314884066581726
Epoch 950, training loss: 0.06829463690519333 = 0.0028718919493258 + 0.01 * 6.542274475097656
Epoch 950, val loss: 1.3188966512680054
Epoch 960, training loss: 0.0682135820388794 = 0.0028017729055136442 + 0.01 * 6.541180610656738
Epoch 960, val loss: 1.3227052688598633
Epoch 970, training loss: 0.0680980458855629 = 0.002734903944656253 + 0.01 * 6.536314010620117
Epoch 970, val loss: 1.3265634775161743
Epoch 980, training loss: 0.0683656707406044 = 0.00267090555280447 + 0.01 * 6.56947660446167
Epoch 980, val loss: 1.3302043676376343
Epoch 990, training loss: 0.06787018477916718 = 0.0026098955422639847 + 0.01 * 6.526029586791992
Epoch 990, val loss: 1.3338062763214111
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.5092
Flip ASR: 0.4178/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0334956645965576 = 1.9497566223144531 + 0.01 * 8.373900413513184
Epoch 0, val loss: 1.9460111856460571
Epoch 10, training loss: 2.0228874683380127 = 1.939149260520935 + 0.01 * 8.373826026916504
Epoch 10, val loss: 1.9351969957351685
Epoch 20, training loss: 2.0100245475769043 = 1.9262892007827759 + 0.01 * 8.373534202575684
Epoch 20, val loss: 1.9214664697647095
Epoch 30, training loss: 1.9921232461929321 = 1.9083976745605469 + 0.01 * 8.372560501098633
Epoch 30, val loss: 1.901911973953247
Epoch 40, training loss: 1.9659755229949951 = 1.8822909593582153 + 0.01 * 8.36845588684082
Epoch 40, val loss: 1.873422384262085
Epoch 50, training loss: 1.9289895296096802 = 1.845530390739441 + 0.01 * 8.345909118652344
Epoch 50, val loss: 1.8347550630569458
Epoch 60, training loss: 1.8833446502685547 = 1.8008259534835815 + 0.01 * 8.251867294311523
Epoch 60, val loss: 1.7913589477539062
Epoch 70, training loss: 1.8368215560913086 = 1.7566503286361694 + 0.01 * 8.017119407653809
Epoch 70, val loss: 1.7523094415664673
Epoch 80, training loss: 1.7815684080123901 = 1.7056055068969727 + 0.01 * 7.596287727355957
Epoch 80, val loss: 1.706669569015503
Epoch 90, training loss: 1.7106434106826782 = 1.6359912157058716 + 0.01 * 7.465221405029297
Epoch 90, val loss: 1.645938515663147
Epoch 100, training loss: 1.6183464527130127 = 1.5451937913894653 + 0.01 * 7.315267086029053
Epoch 100, val loss: 1.569898009300232
Epoch 110, training loss: 1.5119953155517578 = 1.4405057430267334 + 0.01 * 7.148953914642334
Epoch 110, val loss: 1.4845061302185059
Epoch 120, training loss: 1.407167911529541 = 1.3359627723693848 + 0.01 * 7.120517730712891
Epoch 120, val loss: 1.4032517671585083
Epoch 130, training loss: 1.3099721670150757 = 1.2390769720077515 + 0.01 * 7.08952522277832
Epoch 130, val loss: 1.3325402736663818
Epoch 140, training loss: 1.2202861309051514 = 1.1496845483779907 + 0.01 * 7.06016206741333
Epoch 140, val loss: 1.2698038816452026
Epoch 150, training loss: 1.135776162147522 = 1.0655730962753296 + 0.01 * 7.020303249359131
Epoch 150, val loss: 1.2106389999389648
Epoch 160, training loss: 1.054907202720642 = 0.9851984977722168 + 0.01 * 6.970865726470947
Epoch 160, val loss: 1.1533446311950684
Epoch 170, training loss: 0.9773573875427246 = 0.9081614017486572 + 0.01 * 6.919597148895264
Epoch 170, val loss: 1.097578525543213
Epoch 180, training loss: 0.9037973880767822 = 0.8348919749259949 + 0.01 * 6.890544414520264
Epoch 180, val loss: 1.043874979019165
Epoch 190, training loss: 0.8354477882385254 = 0.7667746543884277 + 0.01 * 6.867314338684082
Epoch 190, val loss: 0.9939391016960144
Epoch 200, training loss: 0.7740683555603027 = 0.705489456653595 + 0.01 * 6.857892990112305
Epoch 200, val loss: 0.9499801993370056
Epoch 210, training loss: 0.7203214168548584 = 0.6518027782440186 + 0.01 * 6.851865291595459
Epoch 210, val loss: 0.9137125611305237
Epoch 220, training loss: 0.6731352806091309 = 0.6046677827835083 + 0.01 * 6.846748352050781
Epoch 220, val loss: 0.884746789932251
Epoch 230, training loss: 0.6304324865341187 = 0.5619955658912659 + 0.01 * 6.843693733215332
Epoch 230, val loss: 0.8611690402030945
Epoch 240, training loss: 0.5901219844818115 = 0.5217217206954956 + 0.01 * 6.840027809143066
Epoch 240, val loss: 0.8409994840621948
Epoch 250, training loss: 0.550635814666748 = 0.48227599263191223 + 0.01 * 6.835984230041504
Epoch 250, val loss: 0.8229538798332214
Epoch 260, training loss: 0.5110064148902893 = 0.4426825940608978 + 0.01 * 6.832383632659912
Epoch 260, val loss: 0.8064903020858765
Epoch 270, training loss: 0.470672070980072 = 0.4023860692977905 + 0.01 * 6.828598499298096
Epoch 270, val loss: 0.7914904952049255
Epoch 280, training loss: 0.4295375347137451 = 0.3612888753414154 + 0.01 * 6.8248677253723145
Epoch 280, val loss: 0.7779834270477295
Epoch 290, training loss: 0.3882713317871094 = 0.32006242871284485 + 0.01 * 6.820889949798584
Epoch 290, val loss: 0.7663984894752502
Epoch 300, training loss: 0.3481622636318207 = 0.27997907996177673 + 0.01 * 6.818317890167236
Epoch 300, val loss: 0.7577438354492188
Epoch 310, training loss: 0.3108368217945099 = 0.24269624054431915 + 0.01 * 6.81405782699585
Epoch 310, val loss: 0.7527599930763245
Epoch 320, training loss: 0.27775198221206665 = 0.20963922142982483 + 0.01 * 6.811276912689209
Epoch 320, val loss: 0.7518987655639648
Epoch 330, training loss: 0.24963855743408203 = 0.18152205646038055 + 0.01 * 6.81165075302124
Epoch 330, val loss: 0.7548839449882507
Epoch 340, training loss: 0.22632569074630737 = 0.15826167166233063 + 0.01 * 6.806401252746582
Epoch 340, val loss: 0.761288046836853
Epoch 350, training loss: 0.20715707540512085 = 0.13912919163703918 + 0.01 * 6.802788257598877
Epoch 350, val loss: 0.7704623937606812
Epoch 360, training loss: 0.19127827882766724 = 0.12327072769403458 + 0.01 * 6.800755500793457
Epoch 360, val loss: 0.7817063927650452
Epoch 370, training loss: 0.1779814064502716 = 0.10997214168310165 + 0.01 * 6.8009257316589355
Epoch 370, val loss: 0.7943921685218811
Epoch 380, training loss: 0.1666249781847 = 0.09869755059480667 + 0.01 * 6.79274320602417
Epoch 380, val loss: 0.808097779750824
Epoch 390, training loss: 0.15693405270576477 = 0.08903360366821289 + 0.01 * 6.790045738220215
Epoch 390, val loss: 0.8226433992385864
Epoch 400, training loss: 0.14869464933872223 = 0.08066213130950928 + 0.01 * 6.80325174331665
Epoch 400, val loss: 0.8378528952598572
Epoch 410, training loss: 0.14118900895118713 = 0.07336124777793884 + 0.01 * 6.78277587890625
Epoch 410, val loss: 0.8535835146903992
Epoch 420, training loss: 0.13470003008842468 = 0.06693950295448303 + 0.01 * 6.7760539054870605
Epoch 420, val loss: 0.8696762919425964
Epoch 430, training loss: 0.12895967066287994 = 0.06126176193356514 + 0.01 * 6.7697906494140625
Epoch 430, val loss: 0.8859567046165466
Epoch 440, training loss: 0.12396933883428574 = 0.05622496455907822 + 0.01 * 6.774437427520752
Epoch 440, val loss: 0.9022745490074158
Epoch 450, training loss: 0.11933562159538269 = 0.0517377071082592 + 0.01 * 6.759790897369385
Epoch 450, val loss: 0.9185802936553955
Epoch 460, training loss: 0.11524233222007751 = 0.04773079976439476 + 0.01 * 6.751152992248535
Epoch 460, val loss: 0.9346235394477844
Epoch 470, training loss: 0.11165398359298706 = 0.04412902519106865 + 0.01 * 6.752495765686035
Epoch 470, val loss: 0.9503371715545654
Epoch 480, training loss: 0.10837720334529877 = 0.040883805602788925 + 0.01 * 6.749340057373047
Epoch 480, val loss: 0.965664803981781
Epoch 490, training loss: 0.10521981120109558 = 0.03793958202004433 + 0.01 * 6.728022575378418
Epoch 490, val loss: 0.980560839176178
Epoch 500, training loss: 0.10247477144002914 = 0.03526274859905243 + 0.01 * 6.721202373504639
Epoch 500, val loss: 0.9950951337814331
Epoch 510, training loss: 0.10015305876731873 = 0.03281068801879883 + 0.01 * 6.734237194061279
Epoch 510, val loss: 1.0092077255249023
Epoch 520, training loss: 0.09762251377105713 = 0.030540993437170982 + 0.01 * 6.708151817321777
Epoch 520, val loss: 1.022789716720581
Epoch 530, training loss: 0.09541819244623184 = 0.028422797098755836 + 0.01 * 6.699539661407471
Epoch 530, val loss: 1.0360280275344849
Epoch 540, training loss: 0.09339649975299835 = 0.02644960954785347 + 0.01 * 6.694689750671387
Epoch 540, val loss: 1.0489164590835571
Epoch 550, training loss: 0.09143888205289841 = 0.024591440334916115 + 0.01 * 6.684744358062744
Epoch 550, val loss: 1.0615347623825073
Epoch 560, training loss: 0.08991771936416626 = 0.02284049615263939 + 0.01 * 6.707722187042236
Epoch 560, val loss: 1.07377290725708
Epoch 570, training loss: 0.08796039968729019 = 0.021195396780967712 + 0.01 * 6.67650032043457
Epoch 570, val loss: 1.0857750177383423
Epoch 580, training loss: 0.08632200211286545 = 0.01964772678911686 + 0.01 * 6.667428016662598
Epoch 580, val loss: 1.0977340936660767
Epoch 590, training loss: 0.08481181412935257 = 0.018199196085333824 + 0.01 * 6.661261558532715
Epoch 590, val loss: 1.1095508337020874
Epoch 600, training loss: 0.08380953967571259 = 0.016857217997312546 + 0.01 * 6.69523286819458
Epoch 600, val loss: 1.1212456226348877
Epoch 610, training loss: 0.08214782923460007 = 0.015625303611159325 + 0.01 * 6.652252674102783
Epoch 610, val loss: 1.1330578327178955
Epoch 620, training loss: 0.08093086630105972 = 0.014498678967356682 + 0.01 * 6.643218994140625
Epoch 620, val loss: 1.144479513168335
Epoch 630, training loss: 0.0799223929643631 = 0.013474313542246819 + 0.01 * 6.644807815551758
Epoch 630, val loss: 1.1557201147079468
Epoch 640, training loss: 0.07888983190059662 = 0.012539315037429333 + 0.01 * 6.63505220413208
Epoch 640, val loss: 1.1668177843093872
Epoch 650, training loss: 0.07802265882492065 = 0.01168860588222742 + 0.01 * 6.633405685424805
Epoch 650, val loss: 1.1775590181350708
Epoch 660, training loss: 0.0771874189376831 = 0.010922972112894058 + 0.01 * 6.6264448165893555
Epoch 660, val loss: 1.1880533695220947
Epoch 670, training loss: 0.07657956331968307 = 0.010230978019535542 + 0.01 * 6.634858131408691
Epoch 670, val loss: 1.1982975006103516
Epoch 680, training loss: 0.07572860270738602 = 0.009607784450054169 + 0.01 * 6.612082004547119
Epoch 680, val loss: 1.2083402872085571
Epoch 690, training loss: 0.0752297192811966 = 0.009046239778399467 + 0.01 * 6.618348121643066
Epoch 690, val loss: 1.2179908752441406
Epoch 700, training loss: 0.07456542551517487 = 0.008533951826393604 + 0.01 * 6.603147029876709
Epoch 700, val loss: 1.2271744012832642
Epoch 710, training loss: 0.07444735616445541 = 0.008069353178143501 + 0.01 * 6.637800693511963
Epoch 710, val loss: 1.2359256744384766
Epoch 720, training loss: 0.07364062964916229 = 0.007645597215741873 + 0.01 * 6.599503517150879
Epoch 720, val loss: 1.2447437047958374
Epoch 730, training loss: 0.07317723333835602 = 0.007260428741574287 + 0.01 * 6.591680526733398
Epoch 730, val loss: 1.2533804178237915
Epoch 740, training loss: 0.07288216799497604 = 0.006908987183123827 + 0.01 * 6.597317695617676
Epoch 740, val loss: 1.261875033378601
Epoch 750, training loss: 0.07233714312314987 = 0.006587430834770203 + 0.01 * 6.5749711990356445
Epoch 750, val loss: 1.2699506282806396
Epoch 760, training loss: 0.07202620804309845 = 0.006293980870395899 + 0.01 * 6.573222637176514
Epoch 760, val loss: 1.2778677940368652
Epoch 770, training loss: 0.07177344709634781 = 0.00602377112954855 + 0.01 * 6.574967861175537
Epoch 770, val loss: 1.2855184078216553
Epoch 780, training loss: 0.07153424620628357 = 0.005774904042482376 + 0.01 * 6.575934886932373
Epoch 780, val loss: 1.2929658889770508
Epoch 790, training loss: 0.07109209895133972 = 0.005544168874621391 + 0.01 * 6.554792881011963
Epoch 790, val loss: 1.3001621961593628
Epoch 800, training loss: 0.0711122453212738 = 0.005329990293830633 + 0.01 * 6.578225612640381
Epoch 800, val loss: 1.3071293830871582
Epoch 810, training loss: 0.07076136022806168 = 0.0051310365088284016 + 0.01 * 6.563032627105713
Epoch 810, val loss: 1.314085841178894
Epoch 820, training loss: 0.07042435556650162 = 0.004945575259625912 + 0.01 * 6.547877788543701
Epoch 820, val loss: 1.3207780122756958
Epoch 830, training loss: 0.07031413167715073 = 0.004770942497998476 + 0.01 * 6.554318904876709
Epoch 830, val loss: 1.3273173570632935
Epoch 840, training loss: 0.07007858902215958 = 0.004606475587934256 + 0.01 * 6.547211170196533
Epoch 840, val loss: 1.333815574645996
Epoch 850, training loss: 0.06986963748931885 = 0.0044485037215054035 + 0.01 * 6.542113304138184
Epoch 850, val loss: 1.340163230895996
Epoch 860, training loss: 0.06962187588214874 = 0.004299193620681763 + 0.01 * 6.532268524169922
Epoch 860, val loss: 1.3463752269744873
Epoch 870, training loss: 0.0695011168718338 = 0.004159989766776562 + 0.01 * 6.534112930297852
Epoch 870, val loss: 1.3525114059448242
Epoch 880, training loss: 0.0692184567451477 = 0.0040289610624313354 + 0.01 * 6.51894998550415
Epoch 880, val loss: 1.3585494756698608
Epoch 890, training loss: 0.0693277046084404 = 0.003906316589564085 + 0.01 * 6.542138576507568
Epoch 890, val loss: 1.3644251823425293
Epoch 900, training loss: 0.06898202747106552 = 0.0037912155967205763 + 0.01 * 6.5190815925598145
Epoch 900, val loss: 1.3701661825180054
Epoch 910, training loss: 0.06887632608413696 = 0.003682968905195594 + 0.01 * 6.519335746765137
Epoch 910, val loss: 1.3757717609405518
Epoch 920, training loss: 0.06870251148939133 = 0.003579733893275261 + 0.01 * 6.512277603149414
Epoch 920, val loss: 1.3811448812484741
Epoch 930, training loss: 0.06850700080394745 = 0.0034820346627384424 + 0.01 * 6.502496242523193
Epoch 930, val loss: 1.386501669883728
Epoch 940, training loss: 0.06862732768058777 = 0.00338944373652339 + 0.01 * 6.523788928985596
Epoch 940, val loss: 1.3916831016540527
Epoch 950, training loss: 0.06843653321266174 = 0.003301792312413454 + 0.01 * 6.513474464416504
Epoch 950, val loss: 1.396653652191162
Epoch 960, training loss: 0.0682358518242836 = 0.003218278056010604 + 0.01 * 6.5017571449279785
Epoch 960, val loss: 1.4016305208206177
Epoch 970, training loss: 0.06822102516889572 = 0.003138716798275709 + 0.01 * 6.508231163024902
Epoch 970, val loss: 1.4064916372299194
Epoch 980, training loss: 0.06800568848848343 = 0.0030627206433564425 + 0.01 * 6.494297504425049
Epoch 980, val loss: 1.4112213850021362
Epoch 990, training loss: 0.06792774051427841 = 0.0029903457034379244 + 0.01 * 6.493740081787109
Epoch 990, val loss: 1.4158320426940918
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.4908
Flip ASR: 0.4400/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.049084424972534 = 1.9653455018997192 + 0.01 * 8.373900413513184
Epoch 0, val loss: 1.9674584865570068
Epoch 10, training loss: 2.038673162460327 = 1.9549345970153809 + 0.01 * 8.373846054077148
Epoch 10, val loss: 1.9567416906356812
Epoch 20, training loss: 2.0256876945495605 = 1.9419515132904053 + 0.01 * 8.373625755310059
Epoch 20, val loss: 1.943369746208191
Epoch 30, training loss: 2.0072853565216064 = 1.923555612564087 + 0.01 * 8.372979164123535
Epoch 30, val loss: 1.9244084358215332
Epoch 40, training loss: 1.9797853231430054 = 1.8960857391357422 + 0.01 * 8.36996078491211
Epoch 40, val loss: 1.8964382410049438
Epoch 50, training loss: 1.9399702548980713 = 1.8564780950546265 + 0.01 * 8.349212646484375
Epoch 50, val loss: 1.8580552339553833
Epoch 60, training loss: 1.8923938274383545 = 1.8097738027572632 + 0.01 * 8.262007713317871
Epoch 60, val loss: 1.8172975778579712
Epoch 70, training loss: 1.848504900932312 = 1.76851224899292 + 0.01 * 7.99926233291626
Epoch 70, val loss: 1.7849448919296265
Epoch 80, training loss: 1.7979475259780884 = 1.7228330373764038 + 0.01 * 7.511452674865723
Epoch 80, val loss: 1.7450535297393799
Epoch 90, training loss: 1.7347750663757324 = 1.6625648736953735 + 0.01 * 7.221019268035889
Epoch 90, val loss: 1.691852331161499
Epoch 100, training loss: 1.6529545783996582 = 1.5819283723831177 + 0.01 * 7.102619647979736
Epoch 100, val loss: 1.6239917278289795
Epoch 110, training loss: 1.5526270866394043 = 1.4820460081100464 + 0.01 * 7.058107376098633
Epoch 110, val loss: 1.5423600673675537
Epoch 120, training loss: 1.4414048194885254 = 1.371272325515747 + 0.01 * 7.0132527351379395
Epoch 120, val loss: 1.4521396160125732
Epoch 130, training loss: 1.3271071910858154 = 1.2574220895767212 + 0.01 * 6.968504905700684
Epoch 130, val loss: 1.3602240085601807
Epoch 140, training loss: 1.213748812675476 = 1.1443583965301514 + 0.01 * 6.9390459060668945
Epoch 140, val loss: 1.2688713073730469
Epoch 150, training loss: 1.104429841041565 = 1.035224199295044 + 0.01 * 6.920566082000732
Epoch 150, val loss: 1.1813695430755615
Epoch 160, training loss: 1.002959966659546 = 0.9338874220848083 + 0.01 * 6.907259464263916
Epoch 160, val loss: 1.1012310981750488
Epoch 170, training loss: 0.911730170249939 = 0.8427774310112 + 0.01 * 6.8952717781066895
Epoch 170, val loss: 1.0306471586227417
Epoch 180, training loss: 0.8310707211494446 = 0.7622143626213074 + 0.01 * 6.885634422302246
Epoch 180, val loss: 0.9699234366416931
Epoch 190, training loss: 0.7599717378616333 = 0.6911674737930298 + 0.01 * 6.880429267883301
Epoch 190, val loss: 0.9188185930252075
Epoch 200, training loss: 0.6967736482620239 = 0.6279958486557007 + 0.01 * 6.877780914306641
Epoch 200, val loss: 0.8766838312149048
Epoch 210, training loss: 0.6398009657859802 = 0.5710489153862 + 0.01 * 6.875205993652344
Epoch 210, val loss: 0.8422991037368774
Epoch 220, training loss: 0.5877060294151306 = 0.5189765095710754 + 0.01 * 6.87295389175415
Epoch 220, val loss: 0.8144415020942688
Epoch 230, training loss: 0.5396139621734619 = 0.47091150283813477 + 0.01 * 6.870247840881348
Epoch 230, val loss: 0.7919038534164429
Epoch 240, training loss: 0.4948454201221466 = 0.42617133259773254 + 0.01 * 6.86740779876709
Epoch 240, val loss: 0.7734178900718689
Epoch 250, training loss: 0.4528366029262543 = 0.3841935098171234 + 0.01 * 6.864308834075928
Epoch 250, val loss: 0.7581018805503845
Epoch 260, training loss: 0.4131852388381958 = 0.3445672392845154 + 0.01 * 6.861799240112305
Epoch 260, val loss: 0.7455499768257141
Epoch 270, training loss: 0.3755955696105957 = 0.3070165812969208 + 0.01 * 6.8578972816467285
Epoch 270, val loss: 0.7355242967605591
Epoch 280, training loss: 0.340108722448349 = 0.2715572118759155 + 0.01 * 6.8551506996154785
Epoch 280, val loss: 0.7278124690055847
Epoch 290, training loss: 0.3069649040699005 = 0.2384641319513321 + 0.01 * 6.850078105926514
Epoch 290, val loss: 0.7223396301269531
Epoch 300, training loss: 0.2766385078430176 = 0.20816227793693542 + 0.01 * 6.8476243019104
Epoch 300, val loss: 0.7193945050239563
Epoch 310, training loss: 0.24946197867393494 = 0.18101748824119568 + 0.01 * 6.844449996948242
Epoch 310, val loss: 0.7191617488861084
Epoch 320, training loss: 0.2255726456642151 = 0.15720564126968384 + 0.01 * 6.836701393127441
Epoch 320, val loss: 0.7218182682991028
Epoch 330, training loss: 0.20495344698429108 = 0.1366327404975891 + 0.01 * 6.832070827484131
Epoch 330, val loss: 0.7272818088531494
Epoch 340, training loss: 0.18728788197040558 = 0.1190267875790596 + 0.01 * 6.826109409332275
Epoch 340, val loss: 0.735325276851654
Epoch 350, training loss: 0.1722128689289093 = 0.10402045398950577 + 0.01 * 6.819242477416992
Epoch 350, val loss: 0.7455894947052002
Epoch 360, training loss: 0.15944935381412506 = 0.09123613685369492 + 0.01 * 6.821321487426758
Epoch 360, val loss: 0.757688581943512
Epoch 370, training loss: 0.14839577674865723 = 0.08033181726932526 + 0.01 * 6.806395530700684
Epoch 370, val loss: 0.7712293267250061
Epoch 380, training loss: 0.13900846242904663 = 0.07100756466388702 + 0.01 * 6.800090789794922
Epoch 380, val loss: 0.7858362793922424
Epoch 390, training loss: 0.131067156791687 = 0.06301436573266983 + 0.01 * 6.80527925491333
Epoch 390, val loss: 0.8011139035224915
Epoch 400, training loss: 0.12404590845108032 = 0.05614825710654259 + 0.01 * 6.789765357971191
Epoch 400, val loss: 0.8168189525604248
Epoch 410, training loss: 0.1180485337972641 = 0.05023781210184097 + 0.01 * 6.78107213973999
Epoch 410, val loss: 0.8326542377471924
Epoch 420, training loss: 0.11311961710453033 = 0.045137081295251846 + 0.01 * 6.798254013061523
Epoch 420, val loss: 0.8485475778579712
Epoch 430, training loss: 0.1085507869720459 = 0.040730323642492294 + 0.01 * 6.782046794891357
Epoch 430, val loss: 0.8641633987426758
Epoch 440, training loss: 0.1046011671423912 = 0.03690836578607559 + 0.01 * 6.769280433654785
Epoch 440, val loss: 0.8794084191322327
Epoch 450, training loss: 0.10122047364711761 = 0.03357869014143944 + 0.01 * 6.764178276062012
Epoch 450, val loss: 0.8943105936050415
Epoch 460, training loss: 0.09834685921669006 = 0.030664591118693352 + 0.01 * 6.7682271003723145
Epoch 460, val loss: 0.90883868932724
Epoch 470, training loss: 0.09567223489284515 = 0.028103459626436234 + 0.01 * 6.756877422332764
Epoch 470, val loss: 0.9229086637496948
Epoch 480, training loss: 0.09336072206497192 = 0.025842497125267982 + 0.01 * 6.751822471618652
Epoch 480, val loss: 0.9364955425262451
Epoch 490, training loss: 0.09138704836368561 = 0.023839544504880905 + 0.01 * 6.754751205444336
Epoch 490, val loss: 0.94969242811203
Epoch 500, training loss: 0.08955080807209015 = 0.02205951139330864 + 0.01 * 6.7491302490234375
Epoch 500, val loss: 0.9624089002609253
Epoch 510, training loss: 0.08787892758846283 = 0.020470745861530304 + 0.01 * 6.740818500518799
Epoch 510, val loss: 0.9748089909553528
Epoch 520, training loss: 0.0864100456237793 = 0.01904737576842308 + 0.01 * 6.736267566680908
Epoch 520, val loss: 0.9867876768112183
Epoch 530, training loss: 0.08524076640605927 = 0.017767835408449173 + 0.01 * 6.747293472290039
Epoch 530, val loss: 0.9984006285667419
Epoch 540, training loss: 0.0839308500289917 = 0.016614723950624466 + 0.01 * 6.7316131591796875
Epoch 540, val loss: 1.0096997022628784
Epoch 550, training loss: 0.08282842487096786 = 0.015572130680084229 + 0.01 * 6.7256293296813965
Epoch 550, val loss: 1.0206217765808105
Epoch 560, training loss: 0.0818713828921318 = 0.014626692049205303 + 0.01 * 6.7244696617126465
Epoch 560, val loss: 1.0312579870224
Epoch 570, training loss: 0.08092872053384781 = 0.013767331838607788 + 0.01 * 6.716139316558838
Epoch 570, val loss: 1.0416399240493774
Epoch 580, training loss: 0.08007591217756271 = 0.012984062545001507 + 0.01 * 6.7091851234436035
Epoch 580, val loss: 1.0516225099563599
Epoch 590, training loss: 0.0794239193201065 = 0.012268328107893467 + 0.01 * 6.715559005737305
Epoch 590, val loss: 1.061347246170044
Epoch 600, training loss: 0.07862567901611328 = 0.01161295734345913 + 0.01 * 6.701272487640381
Epoch 600, val loss: 1.0707736015319824
Epoch 610, training loss: 0.07806763052940369 = 0.011011356487870216 + 0.01 * 6.705627918243408
Epoch 610, val loss: 1.0799884796142578
Epoch 620, training loss: 0.07741023600101471 = 0.010458145290613174 + 0.01 * 6.69520902633667
Epoch 620, val loss: 1.0888861417770386
Epoch 630, training loss: 0.07684730738401413 = 0.009948148392140865 + 0.01 * 6.689916133880615
Epoch 630, val loss: 1.097547173500061
Epoch 640, training loss: 0.07636970281600952 = 0.009477047249674797 + 0.01 * 6.689265251159668
Epoch 640, val loss: 1.1060508489608765
Epoch 650, training loss: 0.0758117213845253 = 0.009041502140462399 + 0.01 * 6.6770219802856445
Epoch 650, val loss: 1.114224910736084
Epoch 660, training loss: 0.07533910870552063 = 0.008637663908302784 + 0.01 * 6.670144557952881
Epoch 660, val loss: 1.1222032308578491
Epoch 670, training loss: 0.07502829283475876 = 0.008262445218861103 + 0.01 * 6.676584720611572
Epoch 670, val loss: 1.1300963163375854
Epoch 680, training loss: 0.07453842461109161 = 0.007913529872894287 + 0.01 * 6.662489414215088
Epoch 680, val loss: 1.1376606225967407
Epoch 690, training loss: 0.07418224215507507 = 0.007588432170450687 + 0.01 * 6.65938138961792
Epoch 690, val loss: 1.1450541019439697
Epoch 700, training loss: 0.07395613193511963 = 0.0072848196141421795 + 0.01 * 6.667131423950195
Epoch 700, val loss: 1.1522595882415771
Epoch 710, training loss: 0.07347311079502106 = 0.0070009538903832436 + 0.01 * 6.647215843200684
Epoch 710, val loss: 1.1592390537261963
Epoch 720, training loss: 0.07323311269283295 = 0.0067351446487009525 + 0.01 * 6.649796485900879
Epoch 720, val loss: 1.1661189794540405
Epoch 730, training loss: 0.07280487567186356 = 0.006485797930508852 + 0.01 * 6.631907939910889
Epoch 730, val loss: 1.1727168560028076
Epoch 740, training loss: 0.07259958982467651 = 0.006251566112041473 + 0.01 * 6.634802341461182
Epoch 740, val loss: 1.1792796850204468
Epoch 750, training loss: 0.07240845263004303 = 0.006031081546097994 + 0.01 * 6.637737274169922
Epoch 750, val loss: 1.1855778694152832
Epoch 760, training loss: 0.07206811755895615 = 0.005823592655360699 + 0.01 * 6.624453067779541
Epoch 760, val loss: 1.19173002243042
Epoch 770, training loss: 0.0719543993473053 = 0.005627897568047047 + 0.01 * 6.632649898529053
Epoch 770, val loss: 1.1977468729019165
Epoch 780, training loss: 0.07155782729387283 = 0.00544328335672617 + 0.01 * 6.611454486846924
Epoch 780, val loss: 1.203641414642334
Epoch 790, training loss: 0.07173130661249161 = 0.005268697626888752 + 0.01 * 6.646260738372803
Epoch 790, val loss: 1.2094415426254272
Epoch 800, training loss: 0.07110252976417542 = 0.0051038144156336784 + 0.01 * 6.599871635437012
Epoch 800, val loss: 1.2150161266326904
Epoch 810, training loss: 0.07089796662330627 = 0.004947880748659372 + 0.01 * 6.595008850097656
Epoch 810, val loss: 1.2206165790557861
Epoch 820, training loss: 0.07070578634738922 = 0.004799662157893181 + 0.01 * 6.590612411499023
Epoch 820, val loss: 1.2260282039642334
Epoch 830, training loss: 0.07056892663240433 = 0.004659057594835758 + 0.01 * 6.590987205505371
Epoch 830, val loss: 1.2313666343688965
Epoch 840, training loss: 0.07048206776380539 = 0.004525797441601753 + 0.01 * 6.595627307891846
Epoch 840, val loss: 1.2365303039550781
Epoch 850, training loss: 0.07016310840845108 = 0.0043989974074065685 + 0.01 * 6.576411724090576
Epoch 850, val loss: 1.2415904998779297
Epoch 860, training loss: 0.07011651247739792 = 0.0042781103402376175 + 0.01 * 6.583840370178223
Epoch 860, val loss: 1.2465516328811646
Epoch 870, training loss: 0.07002953439950943 = 0.004163255449384451 + 0.01 * 6.586627960205078
Epoch 870, val loss: 1.2515095472335815
Epoch 880, training loss: 0.06966326385736465 = 0.004053704906255007 + 0.01 * 6.560956001281738
Epoch 880, val loss: 1.2562963962554932
Epoch 890, training loss: 0.06969005614519119 = 0.003949310164898634 + 0.01 * 6.574075222015381
Epoch 890, val loss: 1.2609851360321045
Epoch 900, training loss: 0.06962492316961288 = 0.003849499858915806 + 0.01 * 6.577542781829834
Epoch 900, val loss: 1.2655894756317139
Epoch 910, training loss: 0.06930964440107346 = 0.003754235338419676 + 0.01 * 6.555541038513184
Epoch 910, val loss: 1.2700990438461304
Epoch 920, training loss: 0.06921537965536118 = 0.003662852803245187 + 0.01 * 6.555252552032471
Epoch 920, val loss: 1.2745369672775269
Epoch 930, training loss: 0.0690697729587555 = 0.0035758689045906067 + 0.01 * 6.5493903160095215
Epoch 930, val loss: 1.2787928581237793
Epoch 940, training loss: 0.06894407421350479 = 0.0034928645472973585 + 0.01 * 6.545121669769287
Epoch 940, val loss: 1.2830262184143066
Epoch 950, training loss: 0.06876494735479355 = 0.003412626450881362 + 0.01 * 6.535232067108154
Epoch 950, val loss: 1.28721022605896
Epoch 960, training loss: 0.06886700540781021 = 0.003336540423333645 + 0.01 * 6.553046703338623
Epoch 960, val loss: 1.2913434505462646
Epoch 970, training loss: 0.06853882223367691 = 0.003263202728703618 + 0.01 * 6.527561664581299
Epoch 970, val loss: 1.2952390909194946
Epoch 980, training loss: 0.06854084134101868 = 0.003193058306351304 + 0.01 * 6.534778594970703
Epoch 980, val loss: 1.299175500869751
Epoch 990, training loss: 0.06836728751659393 = 0.0031256666406989098 + 0.01 * 6.5241618156433105
Epoch 990, val loss: 1.3030461072921753
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.8192
Flip ASR: 0.7822/225 nodes
The final ASR:0.60640, 0.15066, Accuracy:0.82222, 0.01210
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11672])
remove edge: torch.Size([2, 9464])
updated graph: torch.Size([2, 10580])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9963
Flip ASR: 0.9956/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9963
Flip ASR: 0.9956/225 nodes
The final ASR:0.98893, 0.01044, Accuracy:0.83827, 0.00175
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0140573978424072 = 1.9303185939788818 + 0.01 * 8.373880386352539
Epoch 0, val loss: 1.9246989488601685
Epoch 10, training loss: 2.004531145095825 = 1.9207926988601685 + 0.01 * 8.3738374710083
Epoch 10, val loss: 1.9159626960754395
Epoch 20, training loss: 1.9926899671554565 = 1.9089536666870117 + 0.01 * 8.373629570007324
Epoch 20, val loss: 1.9047034978866577
Epoch 30, training loss: 1.9757250547409058 = 1.8919950723648071 + 0.01 * 8.372995376586914
Epoch 30, val loss: 1.888448715209961
Epoch 40, training loss: 1.9503482580184937 = 1.866650938987732 + 0.01 * 8.369732856750488
Epoch 40, val loss: 1.86459481716156
Epoch 50, training loss: 1.9145004749298096 = 1.831074833869934 + 0.01 * 8.342558860778809
Epoch 50, val loss: 1.832835078239441
Epoch 60, training loss: 1.8711397647857666 = 1.7896267175674438 + 0.01 * 8.151299476623535
Epoch 60, val loss: 1.79928457736969
Epoch 70, training loss: 1.8240795135498047 = 1.746803641319275 + 0.01 * 7.727586269378662
Epoch 70, val loss: 1.764560580253601
Epoch 80, training loss: 1.7650030851364136 = 1.6892001628875732 + 0.01 * 7.580292224884033
Epoch 80, val loss: 1.7123736143112183
Epoch 90, training loss: 1.6866223812103271 = 1.6119844913482666 + 0.01 * 7.463794231414795
Epoch 90, val loss: 1.6448981761932373
Epoch 100, training loss: 1.5899795293807983 = 1.5170772075653076 + 0.01 * 7.290232181549072
Epoch 100, val loss: 1.5662453174591064
Epoch 110, training loss: 1.4859213829040527 = 1.414504051208496 + 0.01 * 7.141735553741455
Epoch 110, val loss: 1.4807384014129639
Epoch 120, training loss: 1.3831812143325806 = 1.3119211196899414 + 0.01 * 7.1260085105896
Epoch 120, val loss: 1.3985930681228638
Epoch 130, training loss: 1.2839932441711426 = 1.2131175994873047 + 0.01 * 7.087559700012207
Epoch 130, val loss: 1.3226420879364014
Epoch 140, training loss: 1.1905323266983032 = 1.1199307441711426 + 0.01 * 7.0601582527160645
Epoch 140, val loss: 1.2532280683517456
Epoch 150, training loss: 1.1032780408859253 = 1.0331034660339355 + 0.01 * 7.017455577850342
Epoch 150, val loss: 1.1907261610031128
Epoch 160, training loss: 1.0216947793960571 = 0.9521370530128479 + 0.01 * 6.955772399902344
Epoch 160, val loss: 1.1335742473602295
Epoch 170, training loss: 0.9452406167984009 = 0.8762334585189819 + 0.01 * 6.900719165802002
Epoch 170, val loss: 1.0804903507232666
Epoch 180, training loss: 0.87311851978302 = 0.8044358491897583 + 0.01 * 6.868264198303223
Epoch 180, val loss: 1.0309067964553833
Epoch 190, training loss: 0.8045432567596436 = 0.7360724210739136 + 0.01 * 6.847086429595947
Epoch 190, val loss: 0.9852277040481567
Epoch 200, training loss: 0.7391828894615173 = 0.6708561778068542 + 0.01 * 6.832672119140625
Epoch 200, val loss: 0.9442633986473083
Epoch 210, training loss: 0.6769765615463257 = 0.6087468266487122 + 0.01 * 6.822974681854248
Epoch 210, val loss: 0.9089376926422119
Epoch 220, training loss: 0.6180738806724548 = 0.5499058365821838 + 0.01 * 6.816806793212891
Epoch 220, val loss: 0.8793743848800659
Epoch 230, training loss: 0.5629194378852844 = 0.4947968125343323 + 0.01 * 6.812263011932373
Epoch 230, val loss: 0.855547308921814
Epoch 240, training loss: 0.5120716094970703 = 0.4439326822757721 + 0.01 * 6.813892364501953
Epoch 240, val loss: 0.8373398184776306
Epoch 250, training loss: 0.46573111414909363 = 0.39769208431243896 + 0.01 * 6.803903102874756
Epoch 250, val loss: 0.8246651887893677
Epoch 260, training loss: 0.423920214176178 = 0.3559144139289856 + 0.01 * 6.800580978393555
Epoch 260, val loss: 0.8171119689941406
Epoch 270, training loss: 0.38595765829086304 = 0.31800001859664917 + 0.01 * 6.795764446258545
Epoch 270, val loss: 0.8140708208084106
Epoch 280, training loss: 0.3512320816516876 = 0.2833159863948822 + 0.01 * 6.791610240936279
Epoch 280, val loss: 0.8150290250778198
Epoch 290, training loss: 0.319449245929718 = 0.2515753209590912 + 0.01 * 6.7873921394348145
Epoch 290, val loss: 0.8195158839225769
Epoch 300, training loss: 0.2907430827617645 = 0.22291845083236694 + 0.01 * 6.7824625968933105
Epoch 300, val loss: 0.8273960947990417
Epoch 310, training loss: 0.265282541513443 = 0.19751806557178497 + 0.01 * 6.7764482498168945
Epoch 310, val loss: 0.8384097814559937
Epoch 320, training loss: 0.24305056035518646 = 0.17528383433818817 + 0.01 * 6.776672840118408
Epoch 320, val loss: 0.8520791530609131
Epoch 330, training loss: 0.22355006635189056 = 0.15590359270572662 + 0.01 * 6.764647483825684
Epoch 330, val loss: 0.8678333163261414
Epoch 340, training loss: 0.20657525956630707 = 0.1390063464641571 + 0.01 * 6.756891250610352
Epoch 340, val loss: 0.8849534392356873
Epoch 350, training loss: 0.19173938035964966 = 0.12422937154769897 + 0.01 * 6.75100040435791
Epoch 350, val loss: 0.9028905630111694
Epoch 360, training loss: 0.17866677045822144 = 0.11125770211219788 + 0.01 * 6.740907669067383
Epoch 360, val loss: 0.9212604761123657
Epoch 370, training loss: 0.16719818115234375 = 0.09982281923294067 + 0.01 * 6.737536907196045
Epoch 370, val loss: 0.9395948052406311
Epoch 380, training loss: 0.15705519914627075 = 0.08971476554870605 + 0.01 * 6.734044075012207
Epoch 380, val loss: 0.9579755067825317
Epoch 390, training loss: 0.14791914820671082 = 0.08075495064258575 + 0.01 * 6.716419696807861
Epoch 390, val loss: 0.9763324856758118
Epoch 400, training loss: 0.13989044725894928 = 0.07279617339372635 + 0.01 * 6.709427833557129
Epoch 400, val loss: 0.9946382641792297
Epoch 410, training loss: 0.13274049758911133 = 0.06571678072214127 + 0.01 * 6.702372074127197
Epoch 410, val loss: 1.012906551361084
Epoch 420, training loss: 0.12647049129009247 = 0.05941558629274368 + 0.01 * 6.705490589141846
Epoch 420, val loss: 1.0310533046722412
Epoch 430, training loss: 0.12073653936386108 = 0.05380396172404289 + 0.01 * 6.693258285522461
Epoch 430, val loss: 1.0491080284118652
Epoch 440, training loss: 0.1156485378742218 = 0.04879903048276901 + 0.01 * 6.684950828552246
Epoch 440, val loss: 1.0669426918029785
Epoch 450, training loss: 0.1113210916519165 = 0.044330380856990814 + 0.01 * 6.699070930480957
Epoch 450, val loss: 1.084591031074524
Epoch 460, training loss: 0.10712166130542755 = 0.040340911597013474 + 0.01 * 6.678074836730957
Epoch 460, val loss: 1.1019930839538574
Epoch 470, training loss: 0.10346987843513489 = 0.03677252307534218 + 0.01 * 6.669734954833984
Epoch 470, val loss: 1.1190427541732788
Epoch 480, training loss: 0.10023024678230286 = 0.033576514571905136 + 0.01 * 6.665373802185059
Epoch 480, val loss: 1.135809302330017
Epoch 490, training loss: 0.09741182625293732 = 0.030711345374584198 + 0.01 * 6.670048236846924
Epoch 490, val loss: 1.152197003364563
Epoch 500, training loss: 0.0947607010602951 = 0.028142094612121582 + 0.01 * 6.661860942840576
Epoch 500, val loss: 1.1681853532791138
Epoch 510, training loss: 0.09239843487739563 = 0.02583625726401806 + 0.01 * 6.656217575073242
Epoch 510, val loss: 1.1839364767074585
Epoch 520, training loss: 0.09025339782238007 = 0.023766344413161278 + 0.01 * 6.648705005645752
Epoch 520, val loss: 1.1991033554077148
Epoch 530, training loss: 0.08831728249788284 = 0.021907106041908264 + 0.01 * 6.641017913818359
Epoch 530, val loss: 1.2139925956726074
Epoch 540, training loss: 0.08663047105073929 = 0.020235998556017876 + 0.01 * 6.639447212219238
Epoch 540, val loss: 1.228403925895691
Epoch 550, training loss: 0.08507054299116135 = 0.018733346834778786 + 0.01 * 6.6337199211120605
Epoch 550, val loss: 1.2424484491348267
Epoch 560, training loss: 0.08379233628511429 = 0.017380589619278908 + 0.01 * 6.641175270080566
Epoch 560, val loss: 1.2561169862747192
Epoch 570, training loss: 0.08247765898704529 = 0.016161996871232986 + 0.01 * 6.631566047668457
Epoch 570, val loss: 1.2692649364471436
Epoch 580, training loss: 0.08127514272928238 = 0.015062524005770683 + 0.01 * 6.621262073516846
Epoch 580, val loss: 1.2821840047836304
Epoch 590, training loss: 0.08019044995307922 = 0.014068786054849625 + 0.01 * 6.612166404724121
Epoch 590, val loss: 1.2945492267608643
Epoch 600, training loss: 0.07935301959514618 = 0.013168839737772942 + 0.01 * 6.618417739868164
Epoch 600, val loss: 1.3065155744552612
Epoch 610, training loss: 0.07848106324672699 = 0.012352658435702324 + 0.01 * 6.61284065246582
Epoch 610, val loss: 1.3182389736175537
Epoch 620, training loss: 0.07760258764028549 = 0.011610729619860649 + 0.01 * 6.599185943603516
Epoch 620, val loss: 1.3294802904129028
Epoch 630, training loss: 0.0770321786403656 = 0.010934834368526936 + 0.01 * 6.609734535217285
Epoch 630, val loss: 1.3403300046920776
Epoch 640, training loss: 0.07625087350606918 = 0.010318531654775143 + 0.01 * 6.593234062194824
Epoch 640, val loss: 1.3509612083435059
Epoch 650, training loss: 0.07570205628871918 = 0.00975464191287756 + 0.01 * 6.5947418212890625
Epoch 650, val loss: 1.3611013889312744
Epoch 660, training loss: 0.07509837299585342 = 0.009237715974450111 + 0.01 * 6.586066246032715
Epoch 660, val loss: 1.371099829673767
Epoch 670, training loss: 0.07471299171447754 = 0.008762940764427185 + 0.01 * 6.595005512237549
Epoch 670, val loss: 1.3806781768798828
Epoch 680, training loss: 0.07416925579309464 = 0.008326075971126556 + 0.01 * 6.584318161010742
Epoch 680, val loss: 1.3899396657943726
Epoch 690, training loss: 0.0736330971121788 = 0.007923456840217113 + 0.01 * 6.570964813232422
Epoch 690, val loss: 1.398984432220459
Epoch 700, training loss: 0.07336205244064331 = 0.007551434449851513 + 0.01 * 6.581062316894531
Epoch 700, val loss: 1.4078179597854614
Epoch 710, training loss: 0.07292620837688446 = 0.007207212503999472 + 0.01 * 6.5718994140625
Epoch 710, val loss: 1.4162849187850952
Epoch 720, training loss: 0.07251016050577164 = 0.006888094823807478 + 0.01 * 6.562206745147705
Epoch 720, val loss: 1.4245657920837402
Epoch 730, training loss: 0.07224655151367188 = 0.0065915994346141815 + 0.01 * 6.565495014190674
Epoch 730, val loss: 1.4326988458633423
Epoch 740, training loss: 0.07185851037502289 = 0.006315663922578096 + 0.01 * 6.554284572601318
Epoch 740, val loss: 1.4403871297836304
Epoch 750, training loss: 0.07163461297750473 = 0.006058593280613422 + 0.01 * 6.5576019287109375
Epoch 750, val loss: 1.4480100870132446
Epoch 760, training loss: 0.07130765169858932 = 0.005818673875182867 + 0.01 * 6.548898220062256
Epoch 760, val loss: 1.455508828163147
Epoch 770, training loss: 0.07103526592254639 = 0.005594267044216394 + 0.01 * 6.544099807739258
Epoch 770, val loss: 1.4626436233520508
Epoch 780, training loss: 0.07085995376110077 = 0.005384246818721294 + 0.01 * 6.547571182250977
Epoch 780, val loss: 1.4696612358093262
Epoch 790, training loss: 0.07057071477174759 = 0.005187355913221836 + 0.01 * 6.538335800170898
Epoch 790, val loss: 1.476593017578125
Epoch 800, training loss: 0.07041443139314651 = 0.005002432037144899 + 0.01 * 6.541200160980225
Epoch 800, val loss: 1.483205795288086
Epoch 810, training loss: 0.07017745822668076 = 0.004828602075576782 + 0.01 * 6.534885883331299
Epoch 810, val loss: 1.4896565675735474
Epoch 820, training loss: 0.07003915309906006 = 0.004665003623813391 + 0.01 * 6.537415504455566
Epoch 820, val loss: 1.495968222618103
Epoch 830, training loss: 0.06973077356815338 = 0.004510763566941023 + 0.01 * 6.52200174331665
Epoch 830, val loss: 1.5021883249282837
Epoch 840, training loss: 0.06965599209070206 = 0.004365133121609688 + 0.01 * 6.529086589813232
Epoch 840, val loss: 1.5081932544708252
Epoch 850, training loss: 0.06948313117027283 = 0.004227618686854839 + 0.01 * 6.525551795959473
Epoch 850, val loss: 1.5140721797943115
Epoch 860, training loss: 0.0692952573299408 = 0.004097580444067717 + 0.01 * 6.519767761230469
Epoch 860, val loss: 1.519902229309082
Epoch 870, training loss: 0.0692027285695076 = 0.003974469378590584 + 0.01 * 6.522826194763184
Epoch 870, val loss: 1.5254278182983398
Epoch 880, training loss: 0.06905115395784378 = 0.003857791656628251 + 0.01 * 6.519336223602295
Epoch 880, val loss: 1.530869960784912
Epoch 890, training loss: 0.06881144642829895 = 0.00374726508744061 + 0.01 * 6.506418228149414
Epoch 890, val loss: 1.5362999439239502
Epoch 900, training loss: 0.0687781572341919 = 0.0036421697586774826 + 0.01 * 6.513598918914795
Epoch 900, val loss: 1.541536569595337
Epoch 910, training loss: 0.06865287572145462 = 0.0035422174260020256 + 0.01 * 6.51106595993042
Epoch 910, val loss: 1.5466587543487549
Epoch 920, training loss: 0.06845532357692719 = 0.0034472807310521603 + 0.01 * 6.500804901123047
Epoch 920, val loss: 1.551605224609375
Epoch 930, training loss: 0.06839847564697266 = 0.0033569373190402985 + 0.01 * 6.504153251647949
Epoch 930, val loss: 1.55646812915802
Epoch 940, training loss: 0.06825966387987137 = 0.0032709280494600534 + 0.01 * 6.498873233795166
Epoch 940, val loss: 1.561341404914856
Epoch 950, training loss: 0.06807000190019608 = 0.003188749309629202 + 0.01 * 6.488125324249268
Epoch 950, val loss: 1.5659492015838623
Epoch 960, training loss: 0.06805247813463211 = 0.0031103291548788548 + 0.01 * 6.494215488433838
Epoch 960, val loss: 1.5705206394195557
Epoch 970, training loss: 0.06808289140462875 = 0.0030355288181453943 + 0.01 * 6.504735946655273
Epoch 970, val loss: 1.5749493837356567
Epoch 980, training loss: 0.0678662434220314 = 0.0029640563298016787 + 0.01 * 6.4902191162109375
Epoch 980, val loss: 1.5793280601501465
Epoch 990, training loss: 0.06772420555353165 = 0.0028957631438970566 + 0.01 * 6.482844352722168
Epoch 990, val loss: 1.5835649967193604
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7667
Overall ASR: 0.5720
Flip ASR: 0.4978/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.026193380355835 = 1.9424543380737305 + 0.01 * 8.373905181884766
Epoch 0, val loss: 1.940687656402588
Epoch 10, training loss: 2.0161542892456055 = 1.9324159622192383 + 0.01 * 8.373844146728516
Epoch 10, val loss: 1.9306448698043823
Epoch 20, training loss: 2.0039029121398926 = 1.9201669692993164 + 0.01 * 8.373597145080566
Epoch 20, val loss: 1.9180177450180054
Epoch 30, training loss: 1.98677659034729 = 1.9030483961105347 + 0.01 * 8.372815132141113
Epoch 30, val loss: 1.9001374244689941
Epoch 40, training loss: 1.9616410732269287 = 1.8779542446136475 + 0.01 * 8.368683815002441
Epoch 40, val loss: 1.8740571737289429
Epoch 50, training loss: 1.9259288311004639 = 1.842531442642212 + 0.01 * 8.339742660522461
Epoch 50, val loss: 1.8384751081466675
Epoch 60, training loss: 1.882062554359436 = 1.8000845909118652 + 0.01 * 8.19779109954834
Epoch 60, val loss: 1.7989888191223145
Epoch 70, training loss: 1.8364343643188477 = 1.7589737176895142 + 0.01 * 7.74606990814209
Epoch 70, val loss: 1.7633156776428223
Epoch 80, training loss: 1.7855722904205322 = 1.7103158235549927 + 0.01 * 7.525643825531006
Epoch 80, val loss: 1.7198505401611328
Epoch 90, training loss: 1.7159124612808228 = 1.6427679061889648 + 0.01 * 7.314452171325684
Epoch 90, val loss: 1.660689353942871
Epoch 100, training loss: 1.6252588033676147 = 1.5532103776931763 + 0.01 * 7.204842567443848
Epoch 100, val loss: 1.5842852592468262
Epoch 110, training loss: 1.5167441368103027 = 1.4454225301742554 + 0.01 * 7.132161617279053
Epoch 110, val loss: 1.492946982383728
Epoch 120, training loss: 1.4003170728683472 = 1.329521894454956 + 0.01 * 7.079522609710693
Epoch 120, val loss: 1.396588921546936
Epoch 130, training loss: 1.2838761806488037 = 1.2136191129684448 + 0.01 * 7.025710105895996
Epoch 130, val loss: 1.303225040435791
Epoch 140, training loss: 1.1719186305999756 = 1.1020703315734863 + 0.01 * 6.984835624694824
Epoch 140, val loss: 1.2153202295303345
Epoch 150, training loss: 1.0666913986206055 = 0.9970299601554871 + 0.01 * 6.966145992279053
Epoch 150, val loss: 1.133675217628479
Epoch 160, training loss: 0.9689195156097412 = 0.8993542790412903 + 0.01 * 6.956521034240723
Epoch 160, val loss: 1.0576109886169434
Epoch 170, training loss: 0.8790935277938843 = 0.809596836566925 + 0.01 * 6.949667453765869
Epoch 170, val loss: 0.9875659942626953
Epoch 180, training loss: 0.7978700995445251 = 0.728434681892395 + 0.01 * 6.943543910980225
Epoch 180, val loss: 0.9246017932891846
Epoch 190, training loss: 0.7256183624267578 = 0.6562725901603699 + 0.01 * 6.934574604034424
Epoch 190, val loss: 0.8706273436546326
Epoch 200, training loss: 0.6617573499679565 = 0.5925416946411133 + 0.01 * 6.921568393707275
Epoch 200, val loss: 0.8264932036399841
Epoch 210, training loss: 0.604861855506897 = 0.5358232855796814 + 0.01 * 6.903855800628662
Epoch 210, val loss: 0.7915071845054626
Epoch 220, training loss: 0.5533798933029175 = 0.4845411777496338 + 0.01 * 6.88387393951416
Epoch 220, val loss: 0.7638970017433167
Epoch 230, training loss: 0.5062286257743835 = 0.43753793835639954 + 0.01 * 6.869067668914795
Epoch 230, val loss: 0.7419349551200867
Epoch 240, training loss: 0.462871789932251 = 0.3943150043487549 + 0.01 * 6.855678081512451
Epoch 240, val loss: 0.7246938347816467
Epoch 250, training loss: 0.4232339560985565 = 0.3547629117965698 + 0.01 * 6.847104549407959
Epoch 250, val loss: 0.7116274833679199
Epoch 260, training loss: 0.38711023330688477 = 0.3186718821525574 + 0.01 * 6.843834400177002
Epoch 260, val loss: 0.7024595141410828
Epoch 270, training loss: 0.354231595993042 = 0.2858564555644989 + 0.01 * 6.837512969970703
Epoch 270, val loss: 0.697477400302887
Epoch 280, training loss: 0.3243400454521179 = 0.25601494312286377 + 0.01 * 6.832510471343994
Epoch 280, val loss: 0.6964142322540283
Epoch 290, training loss: 0.29718995094299316 = 0.22889910638332367 + 0.01 * 6.8290839195251465
Epoch 290, val loss: 0.6987912654876709
Epoch 300, training loss: 0.2725986838340759 = 0.20433776080608368 + 0.01 * 6.8260931968688965
Epoch 300, val loss: 0.7043251395225525
Epoch 310, training loss: 0.25053656101226807 = 0.18231315910816193 + 0.01 * 6.822340965270996
Epoch 310, val loss: 0.7127354145050049
Epoch 320, training loss: 0.2309838831424713 = 0.1627846360206604 + 0.01 * 6.819924831390381
Epoch 320, val loss: 0.7236727476119995
Epoch 330, training loss: 0.21377089619636536 = 0.14559967815876007 + 0.01 * 6.817121505737305
Epoch 330, val loss: 0.7367538809776306
Epoch 340, training loss: 0.19867807626724243 = 0.13053922355175018 + 0.01 * 6.81388521194458
Epoch 340, val loss: 0.7516709566116333
Epoch 350, training loss: 0.18546062707901 = 0.11737636476755142 + 0.01 * 6.808427333831787
Epoch 350, val loss: 0.7680742740631104
Epoch 360, training loss: 0.17393526434898376 = 0.10588022321462631 + 0.01 * 6.805503845214844
Epoch 360, val loss: 0.7854817509651184
Epoch 370, training loss: 0.16383010149002075 = 0.09581834077835083 + 0.01 * 6.80117654800415
Epoch 370, val loss: 0.8035303354263306
Epoch 380, training loss: 0.1549578309059143 = 0.08699973672628403 + 0.01 * 6.795810222625732
Epoch 380, val loss: 0.8218863010406494
Epoch 390, training loss: 0.14718292653560638 = 0.07925502210855484 + 0.01 * 6.79279088973999
Epoch 390, val loss: 0.8402513265609741
Epoch 400, training loss: 0.14031226933002472 = 0.07244203984737396 + 0.01 * 6.787023067474365
Epoch 400, val loss: 0.8583776354789734
Epoch 410, training loss: 0.13422511518001556 = 0.06643031537532806 + 0.01 * 6.77947998046875
Epoch 410, val loss: 0.8762471675872803
Epoch 420, training loss: 0.12885123491287231 = 0.06110687553882599 + 0.01 * 6.774435043334961
Epoch 420, val loss: 0.8936739563941956
Epoch 430, training loss: 0.12405583262443542 = 0.056371983140707016 + 0.01 * 6.768385410308838
Epoch 430, val loss: 0.9106438159942627
Epoch 440, training loss: 0.11972332000732422 = 0.05213744565844536 + 0.01 * 6.75858736038208
Epoch 440, val loss: 0.9272424578666687
Epoch 450, training loss: 0.11615999042987823 = 0.048337727785110474 + 0.01 * 6.7822265625
Epoch 450, val loss: 0.9434410333633423
Epoch 460, training loss: 0.11245638132095337 = 0.04490988329052925 + 0.01 * 6.754650115966797
Epoch 460, val loss: 0.9592137336730957
Epoch 470, training loss: 0.10912247002124786 = 0.041789084672927856 + 0.01 * 6.733338356018066
Epoch 470, val loss: 0.9746344685554504
Epoch 480, training loss: 0.10616857558488846 = 0.03892682492733002 + 0.01 * 6.724174976348877
Epoch 480, val loss: 0.989659309387207
Epoch 490, training loss: 0.10345694422721863 = 0.03628840669989586 + 0.01 * 6.716854572296143
Epoch 490, val loss: 1.0042669773101807
Epoch 500, training loss: 0.10100684314966202 = 0.033859528601169586 + 0.01 * 6.714731693267822
Epoch 500, val loss: 1.0185368061065674
Epoch 510, training loss: 0.09857013076543808 = 0.031602390110492706 + 0.01 * 6.696774005889893
Epoch 510, val loss: 1.0325003862380981
Epoch 520, training loss: 0.09648159146308899 = 0.02948847971856594 + 0.01 * 6.699311256408691
Epoch 520, val loss: 1.0461827516555786
Epoch 530, training loss: 0.09432639181613922 = 0.027506275102496147 + 0.01 * 6.682011604309082
Epoch 530, val loss: 1.0595821142196655
Epoch 540, training loss: 0.09239628165960312 = 0.025638191029429436 + 0.01 * 6.675808906555176
Epoch 540, val loss: 1.0727028846740723
Epoch 550, training loss: 0.09062790125608444 = 0.02387402020394802 + 0.01 * 6.675388336181641
Epoch 550, val loss: 1.085602879524231
Epoch 560, training loss: 0.08892415463924408 = 0.02221129648387432 + 0.01 * 6.671286106109619
Epoch 560, val loss: 1.0982434749603271
Epoch 570, training loss: 0.0872378870844841 = 0.020649800077080727 + 0.01 * 6.658808708190918
Epoch 570, val loss: 1.11066472530365
Epoch 580, training loss: 0.08567129820585251 = 0.01919502578675747 + 0.01 * 6.647627353668213
Epoch 580, val loss: 1.1229791641235352
Epoch 590, training loss: 0.08425405621528625 = 0.017842082306742668 + 0.01 * 6.641197681427002
Epoch 590, val loss: 1.1353189945220947
Epoch 600, training loss: 0.08290169388055801 = 0.01659533381462097 + 0.01 * 6.630636215209961
Epoch 600, val loss: 1.1472572088241577
Epoch 610, training loss: 0.0816681757569313 = 0.015452718362212181 + 0.01 * 6.621545791625977
Epoch 610, val loss: 1.1591085195541382
Epoch 620, training loss: 0.08066016435623169 = 0.014410698786377907 + 0.01 * 6.6249470710754395
Epoch 620, val loss: 1.1708711385726929
Epoch 630, training loss: 0.07961754500865936 = 0.013466588221490383 + 0.01 * 6.615095615386963
Epoch 630, val loss: 1.1824074983596802
Epoch 640, training loss: 0.07868130505084991 = 0.012612489052116871 + 0.01 * 6.606881141662598
Epoch 640, val loss: 1.193779706954956
Epoch 650, training loss: 0.07789550721645355 = 0.011842356063425541 + 0.01 * 6.605315685272217
Epoch 650, val loss: 1.204899787902832
Epoch 660, training loss: 0.0770839974284172 = 0.011146368458867073 + 0.01 * 6.59376335144043
Epoch 660, val loss: 1.2158279418945312
Epoch 670, training loss: 0.07633334398269653 = 0.010514462366700172 + 0.01 * 6.581888675689697
Epoch 670, val loss: 1.2264478206634521
Epoch 680, training loss: 0.07580340653657913 = 0.009940612129867077 + 0.01 * 6.58627986907959
Epoch 680, val loss: 1.2368240356445312
Epoch 690, training loss: 0.0751655325293541 = 0.009418933652341366 + 0.01 * 6.574660301208496
Epoch 690, val loss: 1.2469253540039062
Epoch 700, training loss: 0.07474222034215927 = 0.008942775428295135 + 0.01 * 6.579944610595703
Epoch 700, val loss: 1.256677269935608
Epoch 710, training loss: 0.07418458163738251 = 0.008507026359438896 + 0.01 * 6.567755222320557
Epoch 710, val loss: 1.2661625146865845
Epoch 720, training loss: 0.07368700206279755 = 0.00810670293867588 + 0.01 * 6.558030128479004
Epoch 720, val loss: 1.2754665613174438
Epoch 730, training loss: 0.073283351957798 = 0.0077376882545650005 + 0.01 * 6.554566383361816
Epoch 730, val loss: 1.2845594882965088
Epoch 740, training loss: 0.07291345298290253 = 0.0073967487551271915 + 0.01 * 6.551670074462891
Epoch 740, val loss: 1.2934072017669678
Epoch 750, training loss: 0.07251626253128052 = 0.007078785914927721 + 0.01 * 6.543747425079346
Epoch 750, val loss: 1.3019537925720215
Epoch 760, training loss: 0.07227285951375961 = 0.006783244665712118 + 0.01 * 6.548961162567139
Epoch 760, val loss: 1.3102903366088867
Epoch 770, training loss: 0.07185369729995728 = 0.006508623715490103 + 0.01 * 6.5345072746276855
Epoch 770, val loss: 1.318501591682434
Epoch 780, training loss: 0.0715923085808754 = 0.006253073923289776 + 0.01 * 6.533924102783203
Epoch 780, val loss: 1.3265570402145386
Epoch 790, training loss: 0.07137870043516159 = 0.0060149552300572395 + 0.01 * 6.536374568939209
Epoch 790, val loss: 1.3344296216964722
Epoch 800, training loss: 0.07138590514659882 = 0.0057922061532735825 + 0.01 * 6.559370040893555
Epoch 800, val loss: 1.3420851230621338
Epoch 810, training loss: 0.07078041881322861 = 0.0055842469446361065 + 0.01 * 6.519617557525635
Epoch 810, val loss: 1.3494789600372314
Epoch 820, training loss: 0.07055618613958359 = 0.005389141850173473 + 0.01 * 6.516704559326172
Epoch 820, val loss: 1.3567830324172974
Epoch 830, training loss: 0.07029806822538376 = 0.0052056750282645226 + 0.01 * 6.509239673614502
Epoch 830, val loss: 1.363908290863037
Epoch 840, training loss: 0.07017412781715393 = 0.0050328620709478855 + 0.01 * 6.514126777648926
Epoch 840, val loss: 1.3708630800247192
Epoch 850, training loss: 0.06997284293174744 = 0.004870296455919743 + 0.01 * 6.510254859924316
Epoch 850, val loss: 1.3776816129684448
Epoch 860, training loss: 0.06995159387588501 = 0.0047164130955934525 + 0.01 * 6.523518085479736
Epoch 860, val loss: 1.3843214511871338
Epoch 870, training loss: 0.06970575451850891 = 0.004571235738694668 + 0.01 * 6.51345157623291
Epoch 870, val loss: 1.3908060789108276
Epoch 880, training loss: 0.06946265697479248 = 0.004433889407664537 + 0.01 * 6.502876281738281
Epoch 880, val loss: 1.3971710205078125
Epoch 890, training loss: 0.06928176432847977 = 0.004303837660700083 + 0.01 * 6.497792720794678
Epoch 890, val loss: 1.403421401977539
Epoch 900, training loss: 0.06930141150951385 = 0.004180307500064373 + 0.01 * 6.512110710144043
Epoch 900, val loss: 1.4094716310501099
Epoch 910, training loss: 0.06899942457675934 = 0.004063497297465801 + 0.01 * 6.493593215942383
Epoch 910, val loss: 1.415472149848938
Epoch 920, training loss: 0.06886793673038483 = 0.00395207991823554 + 0.01 * 6.491586208343506
Epoch 920, val loss: 1.4213643074035645
Epoch 930, training loss: 0.06868012249469757 = 0.0038463131058961153 + 0.01 * 6.483381748199463
Epoch 930, val loss: 1.4270764589309692
Epoch 940, training loss: 0.06855753064155579 = 0.003745647147297859 + 0.01 * 6.481188774108887
Epoch 940, val loss: 1.43272864818573
Epoch 950, training loss: 0.06846345216035843 = 0.003649602411314845 + 0.01 * 6.481385231018066
Epoch 950, val loss: 1.438172459602356
Epoch 960, training loss: 0.06838662177324295 = 0.0035580608528107405 + 0.01 * 6.482856273651123
Epoch 960, val loss: 1.4436322450637817
Epoch 970, training loss: 0.06825751066207886 = 0.003470807336270809 + 0.01 * 6.478670597076416
Epoch 970, val loss: 1.4489234685897827
Epoch 980, training loss: 0.0681614801287651 = 0.0033874630462378263 + 0.01 * 6.477401256561279
Epoch 980, val loss: 1.4541341066360474
Epoch 990, training loss: 0.06820100545883179 = 0.003307591425254941 + 0.01 * 6.489342212677002
Epoch 990, val loss: 1.4592314958572388
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.7343
Flip ASR: 0.6800/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.046013593673706 = 1.9622749090194702 + 0.01 * 8.373863220214844
Epoch 0, val loss: 1.968977451324463
Epoch 10, training loss: 2.0351696014404297 = 1.9514321088790894 + 0.01 * 8.373750686645508
Epoch 10, val loss: 1.95710289478302
Epoch 20, training loss: 2.021942615509033 = 1.938211441040039 + 0.01 * 8.373125076293945
Epoch 20, val loss: 1.9419716596603394
Epoch 30, training loss: 2.003267765045166 = 1.9195536375045776 + 0.01 * 8.371423721313477
Epoch 30, val loss: 1.920058012008667
Epoch 40, training loss: 1.976500391960144 = 1.8928260803222656 + 0.01 * 8.367427825927734
Epoch 40, val loss: 1.8893728256225586
Epoch 50, training loss: 1.9401774406433105 = 1.856728196144104 + 0.01 * 8.344927787780762
Epoch 50, val loss: 1.8511840105056763
Epoch 60, training loss: 1.8948429822921753 = 1.8124946355819702 + 0.01 * 8.234835624694824
Epoch 60, val loss: 1.808930516242981
Epoch 70, training loss: 1.846012830734253 = 1.7681176662445068 + 0.01 * 7.789514541625977
Epoch 70, val loss: 1.7696727514266968
Epoch 80, training loss: 1.8013237714767456 = 1.726008653640747 + 0.01 * 7.531513214111328
Epoch 80, val loss: 1.7310307025909424
Epoch 90, training loss: 1.7437325716018677 = 1.671135663986206 + 0.01 * 7.2596940994262695
Epoch 90, val loss: 1.6808053255081177
Epoch 100, training loss: 1.6686203479766846 = 1.5971306562423706 + 0.01 * 7.148975372314453
Epoch 100, val loss: 1.6182082891464233
Epoch 110, training loss: 1.5719726085662842 = 1.5010930299758911 + 0.01 * 7.087964057922363
Epoch 110, val loss: 1.5400997400283813
Epoch 120, training loss: 1.4618057012557983 = 1.391313076019287 + 0.01 * 7.049263954162598
Epoch 120, val loss: 1.4525502920150757
Epoch 130, training loss: 1.3505470752716064 = 1.280268907546997 + 0.01 * 7.027812957763672
Epoch 130, val loss: 1.3669050931930542
Epoch 140, training loss: 1.2453370094299316 = 1.1754189729690552 + 0.01 * 6.991797924041748
Epoch 140, val loss: 1.2879290580749512
Epoch 150, training loss: 1.1495548486709595 = 1.0801256895065308 + 0.01 * 6.9429192543029785
Epoch 150, val loss: 1.2174320220947266
Epoch 160, training loss: 1.0647317171096802 = 0.9958270192146301 + 0.01 * 6.890471458435059
Epoch 160, val loss: 1.1574844121932983
Epoch 170, training loss: 0.9899047613143921 = 0.9213892817497253 + 0.01 * 6.851546287536621
Epoch 170, val loss: 1.1064869165420532
Epoch 180, training loss: 0.9213862419128418 = 0.8530529737472534 + 0.01 * 6.833328723907471
Epoch 180, val loss: 1.0598740577697754
Epoch 190, training loss: 0.8549788594245911 = 0.7867117524147034 + 0.01 * 6.8267107009887695
Epoch 190, val loss: 1.0138771533966064
Epoch 200, training loss: 0.7879254221916199 = 0.7196986675262451 + 0.01 * 6.82267427444458
Epoch 200, val loss: 0.9674392342567444
Epoch 210, training loss: 0.7201675176620483 = 0.6519538760185242 + 0.01 * 6.821364879608154
Epoch 210, val loss: 0.9200901389122009
Epoch 220, training loss: 0.6533781886100769 = 0.5851713418960571 + 0.01 * 6.820687294006348
Epoch 220, val loss: 0.8740771412849426
Epoch 230, training loss: 0.5896098017692566 = 0.5213943719863892 + 0.01 * 6.821545124053955
Epoch 230, val loss: 0.8321552276611328
Epoch 240, training loss: 0.5303969383239746 = 0.4621735215187073 + 0.01 * 6.822343826293945
Epoch 240, val loss: 0.796565592288971
Epoch 250, training loss: 0.4766174554824829 = 0.40838852524757385 + 0.01 * 6.822891712188721
Epoch 250, val loss: 0.7681834697723389
Epoch 260, training loss: 0.42860254645347595 = 0.3603706657886505 + 0.01 * 6.823188781738281
Epoch 260, val loss: 0.746558427810669
Epoch 270, training loss: 0.38615667819976807 = 0.31791627407073975 + 0.01 * 6.824041843414307
Epoch 270, val loss: 0.7307137846946716
Epoch 280, training loss: 0.3486787676811218 = 0.2804429829120636 + 0.01 * 6.82357931137085
Epoch 280, val loss: 0.7199569940567017
Epoch 290, training loss: 0.3154616951942444 = 0.24723058938980103 + 0.01 * 6.823111534118652
Epoch 290, val loss: 0.7135571241378784
Epoch 300, training loss: 0.2857377529144287 = 0.21751335263252258 + 0.01 * 6.822438716888428
Epoch 300, val loss: 0.7109907865524292
Epoch 310, training loss: 0.2589750587940216 = 0.1907658725976944 + 0.01 * 6.820919036865234
Epoch 310, val loss: 0.711855947971344
Epoch 320, training loss: 0.23489797115325928 = 0.1667030304670334 + 0.01 * 6.819493770599365
Epoch 320, val loss: 0.715852677822113
Epoch 330, training loss: 0.2134339064359665 = 0.14527040719985962 + 0.01 * 6.816349983215332
Epoch 330, val loss: 0.722583532333374
Epoch 340, training loss: 0.19459617137908936 = 0.1264638453722 + 0.01 * 6.813231945037842
Epoch 340, val loss: 0.7315566539764404
Epoch 350, training loss: 0.17829933762550354 = 0.11019918322563171 + 0.01 * 6.810016632080078
Epoch 350, val loss: 0.7422149777412415
Epoch 360, training loss: 0.16433322429656982 = 0.09629843384027481 + 0.01 * 6.80348014831543
Epoch 360, val loss: 0.754181444644928
Epoch 370, training loss: 0.15253300964832306 = 0.08452731370925903 + 0.01 * 6.800569534301758
Epoch 370, val loss: 0.7669851183891296
Epoch 380, training loss: 0.1424950510263443 = 0.07458633929491043 + 0.01 * 6.7908711433410645
Epoch 380, val loss: 0.7802895903587341
Epoch 390, training loss: 0.13402411341667175 = 0.06617545336484909 + 0.01 * 6.784865379333496
Epoch 390, val loss: 0.7940424084663391
Epoch 400, training loss: 0.12673380970954895 = 0.05903516337275505 + 0.01 * 6.7698655128479
Epoch 400, val loss: 0.8077406287193298
Epoch 410, training loss: 0.12056706845760345 = 0.052937012165784836 + 0.01 * 6.76300573348999
Epoch 410, val loss: 0.8215281367301941
Epoch 420, training loss: 0.11525708436965942 = 0.047702543437480927 + 0.01 * 6.7554545402526855
Epoch 420, val loss: 0.8351045250892639
Epoch 430, training loss: 0.11061213910579681 = 0.043179530650377274 + 0.01 * 6.743260860443115
Epoch 430, val loss: 0.8484104871749878
Epoch 440, training loss: 0.10672961175441742 = 0.03924842178821564 + 0.01 * 6.748118877410889
Epoch 440, val loss: 0.8615475296974182
Epoch 450, training loss: 0.10307267308235168 = 0.0358126126229763 + 0.01 * 6.726005554199219
Epoch 450, val loss: 0.87430340051651
Epoch 460, training loss: 0.09999988973140717 = 0.03279006853699684 + 0.01 * 6.720981597900391
Epoch 460, val loss: 0.8867754340171814
Epoch 470, training loss: 0.09724652022123337 = 0.030117273330688477 + 0.01 * 6.712924957275391
Epoch 470, val loss: 0.898962676525116
Epoch 480, training loss: 0.09491415321826935 = 0.02774546481668949 + 0.01 * 6.716869354248047
Epoch 480, val loss: 0.9107998609542847
Epoch 490, training loss: 0.09268638491630554 = 0.025633390992879868 + 0.01 * 6.7052998542785645
Epoch 490, val loss: 0.9223300218582153
Epoch 500, training loss: 0.09071054309606552 = 0.02374250441789627 + 0.01 * 6.696804046630859
Epoch 500, val loss: 0.9336568713188171
Epoch 510, training loss: 0.08896635472774506 = 0.02204323373734951 + 0.01 * 6.692311763763428
Epoch 510, val loss: 0.9446613788604736
Epoch 520, training loss: 0.08746474236249924 = 0.020511671900749207 + 0.01 * 6.69530725479126
Epoch 520, val loss: 0.9554000496864319
Epoch 530, training loss: 0.08598151057958603 = 0.01912827603518963 + 0.01 * 6.685323238372803
Epoch 530, val loss: 0.9657599329948425
Epoch 540, training loss: 0.08468323945999146 = 0.017873860895633698 + 0.01 * 6.680938243865967
Epoch 540, val loss: 0.9759931564331055
Epoch 550, training loss: 0.08350945264101028 = 0.016731277108192444 + 0.01 * 6.677817344665527
Epoch 550, val loss: 0.9859530925750732
Epoch 560, training loss: 0.08250050991773605 = 0.015689050778746605 + 0.01 * 6.681145668029785
Epoch 560, val loss: 0.9957817792892456
Epoch 570, training loss: 0.08143403381109238 = 0.014735241420567036 + 0.01 * 6.669878959655762
Epoch 570, val loss: 1.0053144693374634
Epoch 580, training loss: 0.08049499243497849 = 0.013859828934073448 + 0.01 * 6.663516521453857
Epoch 580, val loss: 1.014644742012024
Epoch 590, training loss: 0.07967879623174667 = 0.013054357841610909 + 0.01 * 6.662444114685059
Epoch 590, val loss: 1.0238347053527832
Epoch 600, training loss: 0.07890472561120987 = 0.012313173152506351 + 0.01 * 6.65915584564209
Epoch 600, val loss: 1.032774806022644
Epoch 610, training loss: 0.07816015183925629 = 0.011630848981440067 + 0.01 * 6.65293025970459
Epoch 610, val loss: 1.0414938926696777
Epoch 620, training loss: 0.07756499201059341 = 0.011001738719642162 + 0.01 * 6.656325817108154
Epoch 620, val loss: 1.0500080585479736
Epoch 630, training loss: 0.07689492404460907 = 0.010421409271657467 + 0.01 * 6.647351264953613
Epoch 630, val loss: 1.0584110021591187
Epoch 640, training loss: 0.07630222290754318 = 0.00988470483571291 + 0.01 * 6.641751766204834
Epoch 640, val loss: 1.0665087699890137
Epoch 650, training loss: 0.07578213512897491 = 0.009387735277414322 + 0.01 * 6.639439582824707
Epoch 650, val loss: 1.0744481086730957
Epoch 660, training loss: 0.07527145743370056 = 0.008928390219807625 + 0.01 * 6.634306907653809
Epoch 660, val loss: 1.0822501182556152
Epoch 670, training loss: 0.07491788268089294 = 0.00850305613130331 + 0.01 * 6.641482830047607
Epoch 670, val loss: 1.0897464752197266
Epoch 680, training loss: 0.07434301823377609 = 0.008108826354146004 + 0.01 * 6.623419761657715
Epoch 680, val loss: 1.097024917602539
Epoch 690, training loss: 0.07391664385795593 = 0.007742110639810562 + 0.01 * 6.617453098297119
Epoch 690, val loss: 1.1041927337646484
Epoch 700, training loss: 0.07361958920955658 = 0.007400202564895153 + 0.01 * 6.621939182281494
Epoch 700, val loss: 1.1112744808197021
Epoch 710, training loss: 0.07319842278957367 = 0.007082316558808088 + 0.01 * 6.6116108894348145
Epoch 710, val loss: 1.1179959774017334
Epoch 720, training loss: 0.07285650819540024 = 0.006785778794437647 + 0.01 * 6.607072830200195
Epoch 720, val loss: 1.1245758533477783
Epoch 730, training loss: 0.07337550818920135 = 0.006508794613182545 + 0.01 * 6.686671257019043
Epoch 730, val loss: 1.1311237812042236
Epoch 740, training loss: 0.07226407527923584 = 0.006250182632356882 + 0.01 * 6.601389408111572
Epoch 740, val loss: 1.1372674703598022
Epoch 750, training loss: 0.07197948545217514 = 0.006007972173392773 + 0.01 * 6.597151756286621
Epoch 750, val loss: 1.1433731317520142
Epoch 760, training loss: 0.07165052741765976 = 0.005780394654721022 + 0.01 * 6.5870137214660645
Epoch 760, val loss: 1.1494172811508179
Epoch 770, training loss: 0.0713590458035469 = 0.005566164385527372 + 0.01 * 6.579288482666016
Epoch 770, val loss: 1.1552613973617554
Epoch 780, training loss: 0.0711517482995987 = 0.005364932119846344 + 0.01 * 6.578681945800781
Epoch 780, val loss: 1.1610888242721558
Epoch 790, training loss: 0.07109437882900238 = 0.005176802631467581 + 0.01 * 6.591757774353027
Epoch 790, val loss: 1.1665760278701782
Epoch 800, training loss: 0.07072760164737701 = 0.00499951746314764 + 0.01 * 6.572808265686035
Epoch 800, val loss: 1.1718963384628296
Epoch 810, training loss: 0.07085281610488892 = 0.004832885693758726 + 0.01 * 6.601993083953857
Epoch 810, val loss: 1.1771857738494873
Epoch 820, training loss: 0.07034865021705627 = 0.004675740841776133 + 0.01 * 6.567291259765625
Epoch 820, val loss: 1.1821757555007935
Epoch 830, training loss: 0.07009662687778473 = 0.004527055658400059 + 0.01 * 6.556957244873047
Epoch 830, val loss: 1.187249779701233
Epoch 840, training loss: 0.06985785067081451 = 0.004386053886264563 + 0.01 * 6.547179698944092
Epoch 840, val loss: 1.1921812295913696
Epoch 850, training loss: 0.07013759762048721 = 0.004252090584486723 + 0.01 * 6.588550567626953
Epoch 850, val loss: 1.1969391107559204
Epoch 860, training loss: 0.06987132132053375 = 0.0041259475983679295 + 0.01 * 6.574537754058838
Epoch 860, val loss: 1.2016054391860962
Epoch 870, training loss: 0.06948957592248917 = 0.004006362520158291 + 0.01 * 6.548321723937988
Epoch 870, val loss: 1.2060298919677734
Epoch 880, training loss: 0.0692814514040947 = 0.003892820794135332 + 0.01 * 6.538863182067871
Epoch 880, val loss: 1.2104910612106323
Epoch 890, training loss: 0.06912311911582947 = 0.0037846146151423454 + 0.01 * 6.53385066986084
Epoch 890, val loss: 1.214693307876587
Epoch 900, training loss: 0.06893537938594818 = 0.003681696020066738 + 0.01 * 6.525368690490723
Epoch 900, val loss: 1.2189598083496094
Epoch 910, training loss: 0.06882398575544357 = 0.003583537880331278 + 0.01 * 6.524044990539551
Epoch 910, val loss: 1.223111629486084
Epoch 920, training loss: 0.0688241645693779 = 0.0034900091122835875 + 0.01 * 6.533415794372559
Epoch 920, val loss: 1.2271943092346191
Epoch 930, training loss: 0.06854163110256195 = 0.003400878282263875 + 0.01 * 6.514075756072998
Epoch 930, val loss: 1.2311855554580688
Epoch 940, training loss: 0.06843657046556473 = 0.0033157190773636103 + 0.01 * 6.512085437774658
Epoch 940, val loss: 1.2349951267242432
Epoch 950, training loss: 0.06831016391515732 = 0.0032346711959689856 + 0.01 * 6.50754976272583
Epoch 950, val loss: 1.2388522624969482
Epoch 960, training loss: 0.06836948543787003 = 0.0031571683939546347 + 0.01 * 6.5212321281433105
Epoch 960, val loss: 1.2425971031188965
Epoch 970, training loss: 0.06829915195703506 = 0.0030831382609903812 + 0.01 * 6.52160120010376
Epoch 970, val loss: 1.246356725692749
Epoch 980, training loss: 0.06797663122415543 = 0.0030122539028525352 + 0.01 * 6.496438026428223
Epoch 980, val loss: 1.2498786449432373
Epoch 990, training loss: 0.06792338937520981 = 0.0029442596714943647 + 0.01 * 6.497913360595703
Epoch 990, val loss: 1.25344717502594
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.8266
Flip ASR: 0.8044/225 nodes
The final ASR:0.71095, 0.10525, Accuracy:0.80494, 0.02710
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11590])
remove edge: torch.Size([2, 9472])
updated graph: torch.Size([2, 10506])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9963
Flip ASR: 0.9956/225 nodes
The final ASR:0.98524, 0.00797, Accuracy:0.82716, 0.00175
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0271151065826416 = 1.9433764219284058 + 0.01 * 8.37386417388916
Epoch 0, val loss: 1.9438637495040894
Epoch 10, training loss: 2.0174529552459717 = 1.9337148666381836 + 0.01 * 8.373798370361328
Epoch 10, val loss: 1.9351085424423218
Epoch 20, training loss: 2.005842447280884 = 1.922107458114624 + 0.01 * 8.373505592346191
Epoch 20, val loss: 1.9241845607757568
Epoch 30, training loss: 1.9896721839904785 = 1.9059475660324097 + 0.01 * 8.372456550598145
Epoch 30, val loss: 1.908644437789917
Epoch 40, training loss: 1.9656492471694946 = 1.8819962739944458 + 0.01 * 8.3652925491333
Epoch 40, val loss: 1.8855997323989868
Epoch 50, training loss: 1.9304500818252563 = 1.8473559617996216 + 0.01 * 8.309412956237793
Epoch 50, val loss: 1.8535645008087158
Epoch 60, training loss: 1.8848965167999268 = 1.8050991296768188 + 0.01 * 7.979743957519531
Epoch 60, val loss: 1.8176531791687012
Epoch 70, training loss: 1.8425365686416626 = 1.7644580602645874 + 0.01 * 7.807845592498779
Epoch 70, val loss: 1.7844312191009521
Epoch 80, training loss: 1.7928893566131592 = 1.716382384300232 + 0.01 * 7.650699615478516
Epoch 80, val loss: 1.7409013509750366
Epoch 90, training loss: 1.7229501008987427 = 1.648484468460083 + 0.01 * 7.446558952331543
Epoch 90, val loss: 1.6822166442871094
Epoch 100, training loss: 1.6312137842178345 = 1.5591405630111694 + 0.01 * 7.207327365875244
Epoch 100, val loss: 1.6095775365829468
Epoch 110, training loss: 1.5272226333618164 = 1.4567984342575073 + 0.01 * 7.042418956756592
Epoch 110, val loss: 1.5258479118347168
Epoch 120, training loss: 1.4247546195983887 = 1.354926586151123 + 0.01 * 6.9827985763549805
Epoch 120, val loss: 1.4437836408615112
Epoch 130, training loss: 1.3284907341003418 = 1.2592107057571411 + 0.01 * 6.928007125854492
Epoch 130, val loss: 1.369020938873291
Epoch 140, training loss: 1.2385121583938599 = 1.1694893836975098 + 0.01 * 6.902278900146484
Epoch 140, val loss: 1.3016613721847534
Epoch 150, training loss: 1.154874324798584 = 1.0860446691513062 + 0.01 * 6.882960796356201
Epoch 150, val loss: 1.2404515743255615
Epoch 160, training loss: 1.0779145956039429 = 1.009289026260376 + 0.01 * 6.862560749053955
Epoch 160, val loss: 1.1854536533355713
Epoch 170, training loss: 1.0069081783294678 = 0.9384847283363342 + 0.01 * 6.842342853546143
Epoch 170, val loss: 1.134703516960144
Epoch 180, training loss: 0.9396697282791138 = 0.871397852897644 + 0.01 * 6.827188491821289
Epoch 180, val loss: 1.0868146419525146
Epoch 190, training loss: 0.8733675479888916 = 0.805190920829773 + 0.01 * 6.817666053771973
Epoch 190, val loss: 1.0394330024719238
Epoch 200, training loss: 0.8064002394676208 = 0.7382775545120239 + 0.01 * 6.8122711181640625
Epoch 200, val loss: 0.9915096759796143
Epoch 210, training loss: 0.7393910884857178 = 0.6713193655014038 + 0.01 * 6.807174205780029
Epoch 210, val loss: 0.9443440437316895
Epoch 220, training loss: 0.6746895909309387 = 0.6066269874572754 + 0.01 * 6.806260108947754
Epoch 220, val loss: 0.9008073806762695
Epoch 230, training loss: 0.6146263480186462 = 0.5466764569282532 + 0.01 * 6.79498815536499
Epoch 230, val loss: 0.8641020655632019
Epoch 240, training loss: 0.5606723427772522 = 0.4928010404109955 + 0.01 * 6.787130355834961
Epoch 240, val loss: 0.8356176018714905
Epoch 250, training loss: 0.5128673911094666 = 0.44506239891052246 + 0.01 * 6.7804975509643555
Epoch 250, val loss: 0.8150629997253418
Epoch 260, training loss: 0.47021663188934326 = 0.4025339186191559 + 0.01 * 6.7682719230651855
Epoch 260, val loss: 0.8008853793144226
Epoch 270, training loss: 0.4315633475780487 = 0.3639233708381653 + 0.01 * 6.763998031616211
Epoch 270, val loss: 0.7916911244392395
Epoch 280, training loss: 0.3956563472747803 = 0.3281913697719574 + 0.01 * 6.746498107910156
Epoch 280, val loss: 0.7866194844245911
Epoch 290, training loss: 0.361972838640213 = 0.2946424186229706 + 0.01 * 6.7330427169799805
Epoch 290, val loss: 0.7849416136741638
Epoch 300, training loss: 0.330252081155777 = 0.26301494240760803 + 0.01 * 6.723714351654053
Epoch 300, val loss: 0.7860231399536133
Epoch 310, training loss: 0.3004975914955139 = 0.23336048424243927 + 0.01 * 6.713710308074951
Epoch 310, val loss: 0.7896919250488281
Epoch 320, training loss: 0.27300170063972473 = 0.2058139443397522 + 0.01 * 6.718775749206543
Epoch 320, val loss: 0.7960801124572754
Epoch 330, training loss: 0.24747584760189056 = 0.1805649846792221 + 0.01 * 6.691086292266846
Epoch 330, val loss: 0.8050448298454285
Epoch 340, training loss: 0.22459015250205994 = 0.15775905549526215 + 0.01 * 6.683108806610107
Epoch 340, val loss: 0.8166367411613464
Epoch 350, training loss: 0.2043827772140503 = 0.13749143481254578 + 0.01 * 6.689134120941162
Epoch 350, val loss: 0.8306708931922913
Epoch 360, training loss: 0.1864672601222992 = 0.1197747141122818 + 0.01 * 6.669255256652832
Epoch 360, val loss: 0.8469554781913757
Epoch 370, training loss: 0.17110997438430786 = 0.10448966175317764 + 0.01 * 6.662032127380371
Epoch 370, val loss: 0.8652591705322266
Epoch 380, training loss: 0.15820154547691345 = 0.09142175316810608 + 0.01 * 6.677978515625
Epoch 380, val loss: 0.8851049542427063
Epoch 390, training loss: 0.14684197306632996 = 0.08032047748565674 + 0.01 * 6.652150630950928
Epoch 390, val loss: 0.9060498476028442
Epoch 400, training loss: 0.13736408948898315 = 0.07089237123727798 + 0.01 * 6.647171974182129
Epoch 400, val loss: 0.927669107913971
Epoch 410, training loss: 0.1292993724346161 = 0.06288135796785355 + 0.01 * 6.641800880432129
Epoch 410, val loss: 0.9494288563728333
Epoch 420, training loss: 0.12248644232749939 = 0.056067172437906265 + 0.01 * 6.641927242279053
Epoch 420, val loss: 0.9710219502449036
Epoch 430, training loss: 0.11663670837879181 = 0.05025651678442955 + 0.01 * 6.63801908493042
Epoch 430, val loss: 0.9920576214790344
Epoch 440, training loss: 0.11154840141534805 = 0.04527118802070618 + 0.01 * 6.627721309661865
Epoch 440, val loss: 1.0123603343963623
Epoch 450, training loss: 0.10722697526216507 = 0.040964916348457336 + 0.01 * 6.626206398010254
Epoch 450, val loss: 1.031904935836792
Epoch 460, training loss: 0.10350380837917328 = 0.03722307085990906 + 0.01 * 6.6280741691589355
Epoch 460, val loss: 1.0508432388305664
Epoch 470, training loss: 0.10010059922933578 = 0.0339520126581192 + 0.01 * 6.614859104156494
Epoch 470, val loss: 1.0690832138061523
Epoch 480, training loss: 0.0971899926662445 = 0.031069539487361908 + 0.01 * 6.6120452880859375
Epoch 480, val loss: 1.0868393182754517
Epoch 490, training loss: 0.09463060647249222 = 0.02851930260658264 + 0.01 * 6.611130237579346
Epoch 490, val loss: 1.1041234731674194
Epoch 500, training loss: 0.0923013761639595 = 0.026252733543515205 + 0.01 * 6.604864597320557
Epoch 500, val loss: 1.1210484504699707
Epoch 510, training loss: 0.09022919088602066 = 0.02422599494457245 + 0.01 * 6.6003193855285645
Epoch 510, val loss: 1.1375099420547485
Epoch 520, training loss: 0.08838546276092529 = 0.022408928722143173 + 0.01 * 6.597653388977051
Epoch 520, val loss: 1.1535784006118774
Epoch 530, training loss: 0.08672890067100525 = 0.020775631070137024 + 0.01 * 6.595326900482178
Epoch 530, val loss: 1.1691614389419556
Epoch 540, training loss: 0.08525204658508301 = 0.01930440217256546 + 0.01 * 6.594764709472656
Epoch 540, val loss: 1.1844455003738403
Epoch 550, training loss: 0.08384251594543457 = 0.017974484711885452 + 0.01 * 6.586803913116455
Epoch 550, val loss: 1.199212908744812
Epoch 560, training loss: 0.08278003334999084 = 0.016769759356975555 + 0.01 * 6.601027488708496
Epoch 560, val loss: 1.2136387825012207
Epoch 570, training loss: 0.08148874342441559 = 0.01567813567817211 + 0.01 * 6.581060409545898
Epoch 570, val loss: 1.2276729345321655
Epoch 580, training loss: 0.08046401292085648 = 0.014686149545013905 + 0.01 * 6.577786445617676
Epoch 580, val loss: 1.241235613822937
Epoch 590, training loss: 0.07954660803079605 = 0.01378212682902813 + 0.01 * 6.5764479637146
Epoch 590, val loss: 1.2544190883636475
Epoch 600, training loss: 0.07866835594177246 = 0.012957217171788216 + 0.01 * 6.571114540100098
Epoch 600, val loss: 1.2672847509384155
Epoch 610, training loss: 0.07786824554204941 = 0.01220323983579874 + 0.01 * 6.566500663757324
Epoch 610, val loss: 1.2796727418899536
Epoch 620, training loss: 0.07715021073818207 = 0.011512303724884987 + 0.01 * 6.563790798187256
Epoch 620, val loss: 1.291794776916504
Epoch 630, training loss: 0.07656561583280563 = 0.010877850465476513 + 0.01 * 6.568777084350586
Epoch 630, val loss: 1.3035625219345093
Epoch 640, training loss: 0.07592537254095078 = 0.01029516477137804 + 0.01 * 6.563020706176758
Epoch 640, val loss: 1.314924716949463
Epoch 650, training loss: 0.07538068294525146 = 0.009758698754012585 + 0.01 * 6.562198638916016
Epoch 650, val loss: 1.3260304927825928
Epoch 660, training loss: 0.07478492707014084 = 0.00926411896944046 + 0.01 * 6.552081108093262
Epoch 660, val loss: 1.3367139101028442
Epoch 670, training loss: 0.07436670362949371 = 0.008807245641946793 + 0.01 * 6.55594539642334
Epoch 670, val loss: 1.3471451997756958
Epoch 680, training loss: 0.07384171336889267 = 0.008384140208363533 + 0.01 * 6.545757293701172
Epoch 680, val loss: 1.3573083877563477
Epoch 690, training loss: 0.07344545423984528 = 0.007991690188646317 + 0.01 * 6.545375823974609
Epoch 690, val loss: 1.3671578168869019
Epoch 700, training loss: 0.07304612547159195 = 0.007627476472407579 + 0.01 * 6.541864395141602
Epoch 700, val loss: 1.3767554759979248
Epoch 710, training loss: 0.07271160930395126 = 0.007289161905646324 + 0.01 * 6.542244911193848
Epoch 710, val loss: 1.386055827140808
Epoch 720, training loss: 0.0723540335893631 = 0.006974031683057547 + 0.01 * 6.538000106811523
Epoch 720, val loss: 1.3951150178909302
Epoch 730, training loss: 0.0721622183918953 = 0.006680100224912167 + 0.01 * 6.548211574554443
Epoch 730, val loss: 1.403965711593628
Epoch 740, training loss: 0.07167840003967285 = 0.00640568183735013 + 0.01 * 6.5272722244262695
Epoch 740, val loss: 1.4124648571014404
Epoch 750, training loss: 0.07150109857320786 = 0.006148978602141142 + 0.01 * 6.53521203994751
Epoch 750, val loss: 1.420723557472229
Epoch 760, training loss: 0.07121361792087555 = 0.00590915372595191 + 0.01 * 6.530446529388428
Epoch 760, val loss: 1.4288800954818726
Epoch 770, training loss: 0.07094407826662064 = 0.005684525705873966 + 0.01 * 6.525955677032471
Epoch 770, val loss: 1.4367659091949463
Epoch 780, training loss: 0.07065293937921524 = 0.005473525729030371 + 0.01 * 6.517941951751709
Epoch 780, val loss: 1.4443919658660889
Epoch 790, training loss: 0.07050693035125732 = 0.005275083240121603 + 0.01 * 6.523184776306152
Epoch 790, val loss: 1.4518367052078247
Epoch 800, training loss: 0.07028438150882721 = 0.005088392179459333 + 0.01 * 6.519599437713623
Epoch 800, val loss: 1.4591567516326904
Epoch 810, training loss: 0.07013449817895889 = 0.004912768956273794 + 0.01 * 6.522172927856445
Epoch 810, val loss: 1.4662188291549683
Epoch 820, training loss: 0.06980710476636887 = 0.004747138358652592 + 0.01 * 6.5059967041015625
Epoch 820, val loss: 1.4731276035308838
Epoch 830, training loss: 0.0698603168129921 = 0.004590700846165419 + 0.01 * 6.5269622802734375
Epoch 830, val loss: 1.4798206090927124
Epoch 840, training loss: 0.06952939927577972 = 0.004443033132702112 + 0.01 * 6.508636951446533
Epoch 840, val loss: 1.486405611038208
Epoch 850, training loss: 0.06945320218801498 = 0.0043032304383814335 + 0.01 * 6.514997482299805
Epoch 850, val loss: 1.4927632808685303
Epoch 860, training loss: 0.06917750835418701 = 0.0041709416545927525 + 0.01 * 6.500657081604004
Epoch 860, val loss: 1.499062418937683
Epoch 870, training loss: 0.06904304772615433 = 0.004045546054840088 + 0.01 * 6.499750137329102
Epoch 870, val loss: 1.5051788091659546
Epoch 880, training loss: 0.06896132975816727 = 0.003926626872271299 + 0.01 * 6.503470420837402
Epoch 880, val loss: 1.5110851526260376
Epoch 890, training loss: 0.06883557140827179 = 0.0038136832881718874 + 0.01 * 6.502188682556152
Epoch 890, val loss: 1.5168815851211548
Epoch 900, training loss: 0.0685943141579628 = 0.0037065010983496904 + 0.01 * 6.488781452178955
Epoch 900, val loss: 1.5225602388381958
Epoch 910, training loss: 0.06849388778209686 = 0.0036045266315340996 + 0.01 * 6.488936424255371
Epoch 910, val loss: 1.5280832052230835
Epoch 920, training loss: 0.06844664365053177 = 0.0035073901526629925 + 0.01 * 6.493925094604492
Epoch 920, val loss: 1.5334489345550537
Epoch 930, training loss: 0.06840503215789795 = 0.0034148734994232655 + 0.01 * 6.499015808105469
Epoch 930, val loss: 1.5387983322143555
Epoch 940, training loss: 0.06820079684257507 = 0.003326595760881901 + 0.01 * 6.487420558929443
Epoch 940, val loss: 1.5439585447311401
Epoch 950, training loss: 0.06809724867343903 = 0.003242352744564414 + 0.01 * 6.485489845275879
Epoch 950, val loss: 1.548966407775879
Epoch 960, training loss: 0.06793604046106339 = 0.003162055741995573 + 0.01 * 6.477398872375488
Epoch 960, val loss: 1.5538946390151978
Epoch 970, training loss: 0.06806039810180664 = 0.0030852367635816336 + 0.01 * 6.497516632080078
Epoch 970, val loss: 1.5586460828781128
Epoch 980, training loss: 0.0678442046046257 = 0.0030119961593300104 + 0.01 * 6.483221530914307
Epoch 980, val loss: 1.5633693933486938
Epoch 990, training loss: 0.06773601472377777 = 0.002941923448815942 + 0.01 * 6.479409217834473
Epoch 990, val loss: 1.56791090965271
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.6347
Flip ASR: 0.5778/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0242247581481934 = 1.9404860734939575 + 0.01 * 8.373879432678223
Epoch 0, val loss: 1.9415878057479858
Epoch 10, training loss: 2.014449119567871 = 1.930711269378662 + 0.01 * 8.373778343200684
Epoch 10, val loss: 1.9309850931167603
Epoch 20, training loss: 2.0024759769439697 = 1.9187418222427368 + 0.01 * 8.373412132263184
Epoch 20, val loss: 1.917932152748108
Epoch 30, training loss: 1.9855233430862427 = 1.9018023014068604 + 0.01 * 8.37210464477539
Epoch 30, val loss: 1.899546504020691
Epoch 40, training loss: 1.959957242012024 = 1.8763166666030884 + 0.01 * 8.364056587219238
Epoch 40, val loss: 1.872072696685791
Epoch 50, training loss: 1.9223865270614624 = 1.8393207788467407 + 0.01 * 8.30657958984375
Epoch 50, val loss: 1.8333704471588135
Epoch 60, training loss: 1.8724374771118164 = 1.7926726341247559 + 0.01 * 7.976490020751953
Epoch 60, val loss: 1.7875251770019531
Epoch 70, training loss: 1.8193397521972656 = 1.7431124448776245 + 0.01 * 7.622735023498535
Epoch 70, val loss: 1.7414690256118774
Epoch 80, training loss: 1.756754755973816 = 1.6829310655593872 + 0.01 * 7.382371425628662
Epoch 80, val loss: 1.6859878301620483
Epoch 90, training loss: 1.6738011837005615 = 1.6016442775726318 + 0.01 * 7.215686321258545
Epoch 90, val loss: 1.6151701211929321
Epoch 100, training loss: 1.5708855390548706 = 1.4997751712799072 + 0.01 * 7.111037254333496
Epoch 100, val loss: 1.5309438705444336
Epoch 110, training loss: 1.4571274518966675 = 1.3861628770828247 + 0.01 * 7.096458911895752
Epoch 110, val loss: 1.4359068870544434
Epoch 120, training loss: 1.3418774604797363 = 1.2710065841674805 + 0.01 * 7.0870890617370605
Epoch 120, val loss: 1.3432127237319946
Epoch 130, training loss: 1.2295583486557007 = 1.158778429031372 + 0.01 * 7.077995777130127
Epoch 130, val loss: 1.2536981105804443
Epoch 140, training loss: 1.121852159500122 = 1.0511733293533325 + 0.01 * 7.0678887367248535
Epoch 140, val loss: 1.1705695390701294
Epoch 150, training loss: 1.021270990371704 = 0.9507549405097961 + 0.01 * 7.051609516143799
Epoch 150, val loss: 1.0949147939682007
Epoch 160, training loss: 0.9316555857658386 = 0.861360490322113 + 0.01 * 7.029510974884033
Epoch 160, val loss: 1.0300813913345337
Epoch 170, training loss: 0.8558875918388367 = 0.7858881950378418 + 0.01 * 6.999941349029541
Epoch 170, val loss: 0.978510856628418
Epoch 180, training loss: 0.7937885522842407 = 0.7241349816322327 + 0.01 * 6.965353965759277
Epoch 180, val loss: 0.9400569796562195
Epoch 190, training loss: 0.7424655556678772 = 0.6731002330780029 + 0.01 * 6.936534404754639
Epoch 190, val loss: 0.9121785759925842
Epoch 200, training loss: 0.69818115234375 = 0.6290876865386963 + 0.01 * 6.9093475341796875
Epoch 200, val loss: 0.8915867209434509
Epoch 210, training loss: 0.6579548120498657 = 0.5890611410140991 + 0.01 * 6.889369487762451
Epoch 210, val loss: 0.8755596876144409
Epoch 220, training loss: 0.6196399927139282 = 0.5509620904922485 + 0.01 * 6.867793083190918
Epoch 220, val loss: 0.8624923229217529
Epoch 230, training loss: 0.5819731950759888 = 0.5135125517845154 + 0.01 * 6.846067428588867
Epoch 230, val loss: 0.8512819409370422
Epoch 240, training loss: 0.5442477464675903 = 0.4759533703327179 + 0.01 * 6.829438209533691
Epoch 240, val loss: 0.8414371013641357
Epoch 250, training loss: 0.5059425234794617 = 0.4377892315387726 + 0.01 * 6.815328598022461
Epoch 250, val loss: 0.8323606252670288
Epoch 260, training loss: 0.46678483486175537 = 0.3986974358558655 + 0.01 * 6.808739185333252
Epoch 260, val loss: 0.8243670463562012
Epoch 270, training loss: 0.4270193874835968 = 0.35893136262893677 + 0.01 * 6.808803558349609
Epoch 270, val loss: 0.8174661993980408
Epoch 280, training loss: 0.3876352608203888 = 0.31956180930137634 + 0.01 * 6.807345867156982
Epoch 280, val loss: 0.8121581077575684
Epoch 290, training loss: 0.3502226173877716 = 0.28216099739074707 + 0.01 * 6.8061628341674805
Epoch 290, val loss: 0.809018611907959
Epoch 300, training loss: 0.3158976435661316 = 0.24783718585968018 + 0.01 * 6.806044101715088
Epoch 300, val loss: 0.8084719777107239
Epoch 310, training loss: 0.2851507365703583 = 0.21710048615932465 + 0.01 * 6.805026054382324
Epoch 310, val loss: 0.8108503222465515
Epoch 320, training loss: 0.2578650116920471 = 0.18985655903816223 + 0.01 * 6.800844669342041
Epoch 320, val loss: 0.8157365322113037
Epoch 330, training loss: 0.23365306854248047 = 0.1656630039215088 + 0.01 * 6.799007415771484
Epoch 330, val loss: 0.8230116367340088
Epoch 340, training loss: 0.21203827857971191 = 0.14405737817287445 + 0.01 * 6.79809045791626
Epoch 340, val loss: 0.8324708342552185
Epoch 350, training loss: 0.19289900362491608 = 0.1249275803565979 + 0.01 * 6.797142505645752
Epoch 350, val loss: 0.8447632193565369
Epoch 360, training loss: 0.17631453275680542 = 0.1083889752626419 + 0.01 * 6.792555809020996
Epoch 360, val loss: 0.8603394031524658
Epoch 370, training loss: 0.16257598996162415 = 0.09466442465782166 + 0.01 * 6.791157245635986
Epoch 370, val loss: 0.8786321878433228
Epoch 380, training loss: 0.15131840109825134 = 0.08343131840229034 + 0.01 * 6.788708209991455
Epoch 380, val loss: 0.8987883925437927
Epoch 390, training loss: 0.14193913340568542 = 0.07410372793674469 + 0.01 * 6.783541679382324
Epoch 390, val loss: 0.9201534986495972
Epoch 400, training loss: 0.13399258255958557 = 0.0661735013127327 + 0.01 * 6.7819085121154785
Epoch 400, val loss: 0.9422593712806702
Epoch 410, training loss: 0.12709183990955353 = 0.059338901191949844 + 0.01 * 6.775293350219727
Epoch 410, val loss: 0.9649034142494202
Epoch 420, training loss: 0.12113847583532333 = 0.053404562175273895 + 0.01 * 6.773391246795654
Epoch 420, val loss: 0.9877508282661438
Epoch 430, training loss: 0.11602272838354111 = 0.04823135584592819 + 0.01 * 6.77913761138916
Epoch 430, val loss: 1.010576844215393
Epoch 440, training loss: 0.11137020587921143 = 0.04371075704693794 + 0.01 * 6.765944957733154
Epoch 440, val loss: 1.0331294536590576
Epoch 450, training loss: 0.10733690857887268 = 0.03974812105298042 + 0.01 * 6.758878707885742
Epoch 450, val loss: 1.0553364753723145
Epoch 460, training loss: 0.1038808822631836 = 0.03626245632767677 + 0.01 * 6.761843204498291
Epoch 460, val loss: 1.0770634412765503
Epoch 470, training loss: 0.10066018998622894 = 0.03318833187222481 + 0.01 * 6.747186183929443
Epoch 470, val loss: 1.0982191562652588
Epoch 480, training loss: 0.09787453711032867 = 0.03046831116080284 + 0.01 * 6.740623474121094
Epoch 480, val loss: 1.118743896484375
Epoch 490, training loss: 0.09546516835689545 = 0.028053991496562958 + 0.01 * 6.74111795425415
Epoch 490, val loss: 1.1386053562164307
Epoch 500, training loss: 0.09316715598106384 = 0.025904782116413116 + 0.01 * 6.7262372970581055
Epoch 500, val loss: 1.1578644514083862
Epoch 510, training loss: 0.0912829339504242 = 0.023984936997294426 + 0.01 * 6.729799747467041
Epoch 510, val loss: 1.1765570640563965
Epoch 520, training loss: 0.0894683226943016 = 0.02226543426513672 + 0.01 * 6.72028923034668
Epoch 520, val loss: 1.1946958303451538
Epoch 530, training loss: 0.08772468566894531 = 0.020719755440950394 + 0.01 * 6.700493335723877
Epoch 530, val loss: 1.2123277187347412
Epoch 540, training loss: 0.08627193421125412 = 0.019326500594615936 + 0.01 * 6.694543361663818
Epoch 540, val loss: 1.2294647693634033
Epoch 550, training loss: 0.08506644517183304 = 0.018067771568894386 + 0.01 * 6.6998677253723145
Epoch 550, val loss: 1.2461187839508057
Epoch 560, training loss: 0.08380506187677383 = 0.016926700249314308 + 0.01 * 6.687836170196533
Epoch 560, val loss: 1.262365698814392
Epoch 570, training loss: 0.08265946060419083 = 0.015888450667262077 + 0.01 * 6.677101135253906
Epoch 570, val loss: 1.2782353162765503
Epoch 580, training loss: 0.08164636790752411 = 0.01494369376450777 + 0.01 * 6.670267581939697
Epoch 580, val loss: 1.2936538457870483
Epoch 590, training loss: 0.08082766830921173 = 0.014081794768571854 + 0.01 * 6.674587726593018
Epoch 590, val loss: 1.3086167573928833
Epoch 600, training loss: 0.07984894514083862 = 0.013292734511196613 + 0.01 * 6.65562105178833
Epoch 600, val loss: 1.323274850845337
Epoch 610, training loss: 0.07904604077339172 = 0.012568064965307713 + 0.01 * 6.64779806137085
Epoch 610, val loss: 1.3375834226608276
Epoch 620, training loss: 0.07839938998222351 = 0.011901353485882282 + 0.01 * 6.649803638458252
Epoch 620, val loss: 1.3515470027923584
Epoch 630, training loss: 0.07763712108135223 = 0.011287412606179714 + 0.01 * 6.6349711418151855
Epoch 630, val loss: 1.365141749382019
Epoch 640, training loss: 0.07702316343784332 = 0.010720955207943916 + 0.01 * 6.630221366882324
Epoch 640, val loss: 1.3783913850784302
Epoch 650, training loss: 0.0765252485871315 = 0.010196362622082233 + 0.01 * 6.6328887939453125
Epoch 650, val loss: 1.391389012336731
Epoch 660, training loss: 0.07598511874675751 = 0.009711104445159435 + 0.01 * 6.627401828765869
Epoch 660, val loss: 1.4041355848312378
Epoch 670, training loss: 0.07537250220775604 = 0.009260226041078568 + 0.01 * 6.611227512359619
Epoch 670, val loss: 1.4165152311325073
Epoch 680, training loss: 0.07490682601928711 = 0.008841071277856827 + 0.01 * 6.6065754890441895
Epoch 680, val loss: 1.4285697937011719
Epoch 690, training loss: 0.07456464320421219 = 0.008450833149254322 + 0.01 * 6.611380577087402
Epoch 690, val loss: 1.4404464960098267
Epoch 700, training loss: 0.07423408329486847 = 0.008087512105703354 + 0.01 * 6.614656925201416
Epoch 700, val loss: 1.452027440071106
Epoch 710, training loss: 0.07370370626449585 = 0.007748340722173452 + 0.01 * 6.595536231994629
Epoch 710, val loss: 1.4633841514587402
Epoch 720, training loss: 0.07320508360862732 = 0.007431872189044952 + 0.01 * 6.577321529388428
Epoch 720, val loss: 1.4743837118148804
Epoch 730, training loss: 0.07295215129852295 = 0.007135693449527025 + 0.01 * 6.581645488739014
Epoch 730, val loss: 1.4851080179214478
Epoch 740, training loss: 0.07255299389362335 = 0.006858328823000193 + 0.01 * 6.569467067718506
Epoch 740, val loss: 1.4957363605499268
Epoch 750, training loss: 0.07231423258781433 = 0.006597830913960934 + 0.01 * 6.571640491485596
Epoch 750, val loss: 1.506033182144165
Epoch 760, training loss: 0.07229150831699371 = 0.006353418808430433 + 0.01 * 6.593809127807617
Epoch 760, val loss: 1.5160871744155884
Epoch 770, training loss: 0.07172343134880066 = 0.006123806349933147 + 0.01 * 6.559962749481201
Epoch 770, val loss: 1.5260475873947144
Epoch 780, training loss: 0.07147414237260818 = 0.0059074885211884975 + 0.01 * 6.556664943695068
Epoch 780, val loss: 1.5357139110565186
Epoch 790, training loss: 0.07117349654436111 = 0.005703739821910858 + 0.01 * 6.546975612640381
Epoch 790, val loss: 1.5450751781463623
Epoch 800, training loss: 0.07101471722126007 = 0.005511497613042593 + 0.01 * 6.550321578979492
Epoch 800, val loss: 1.5542933940887451
Epoch 810, training loss: 0.07087043672800064 = 0.0053305188193917274 + 0.01 * 6.55399227142334
Epoch 810, val loss: 1.5633747577667236
Epoch 820, training loss: 0.07059584558010101 = 0.005158855114132166 + 0.01 * 6.543699264526367
Epoch 820, val loss: 1.5721558332443237
Epoch 830, training loss: 0.0703529417514801 = 0.0049967593513429165 + 0.01 * 6.535618305206299
Epoch 830, val loss: 1.580817699432373
Epoch 840, training loss: 0.07003864645957947 = 0.004843174945563078 + 0.01 * 6.519547462463379
Epoch 840, val loss: 1.589255928993225
Epoch 850, training loss: 0.07001514732837677 = 0.004697785712778568 + 0.01 * 6.531736373901367
Epoch 850, val loss: 1.5975062847137451
Epoch 860, training loss: 0.06991112232208252 = 0.004560168832540512 + 0.01 * 6.53509521484375
Epoch 860, val loss: 1.6056556701660156
Epoch 870, training loss: 0.06953329592943192 = 0.00442923791706562 + 0.01 * 6.510406494140625
Epoch 870, val loss: 1.6136168241500854
Epoch 880, training loss: 0.06958923488855362 = 0.004304803907871246 + 0.01 * 6.528443336486816
Epoch 880, val loss: 1.621267318725586
Epoch 890, training loss: 0.06929676234722137 = 0.004186699632555246 + 0.01 * 6.5110063552856445
Epoch 890, val loss: 1.6289764642715454
Epoch 900, training loss: 0.0690871849656105 = 0.004074014723300934 + 0.01 * 6.501317501068115
Epoch 900, val loss: 1.6364338397979736
Epoch 910, training loss: 0.06898300349712372 = 0.0039667896926403046 + 0.01 * 6.501622200012207
Epoch 910, val loss: 1.6436938047409058
Epoch 920, training loss: 0.06894505769014359 = 0.003864395199343562 + 0.01 * 6.508066654205322
Epoch 920, val loss: 1.6509490013122559
Epoch 930, training loss: 0.06887246668338776 = 0.0037666503340005875 + 0.01 * 6.510581970214844
Epoch 930, val loss: 1.658008337020874
Epoch 940, training loss: 0.06851882487535477 = 0.0036733639426529408 + 0.01 * 6.484546184539795
Epoch 940, val loss: 1.664830207824707
Epoch 950, training loss: 0.06849449872970581 = 0.003584334161132574 + 0.01 * 6.491016387939453
Epoch 950, val loss: 1.6715553998947144
Epoch 960, training loss: 0.06862661242485046 = 0.0034992406144738197 + 0.01 * 6.512737274169922
Epoch 960, val loss: 1.6782898902893066
Epoch 970, training loss: 0.06825423985719681 = 0.0034176346380263567 + 0.01 * 6.483660697937012
Epoch 970, val loss: 1.6847041845321655
Epoch 980, training loss: 0.06842726469039917 = 0.0033396254293620586 + 0.01 * 6.508764266967773
Epoch 980, val loss: 1.6909546852111816
Epoch 990, training loss: 0.06807214766740799 = 0.003264807630330324 + 0.01 * 6.480734348297119
Epoch 990, val loss: 1.6974090337753296
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.4945
Flip ASR: 0.4756/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.027604818344116 = 1.9438660144805908 + 0.01 * 8.37387466430664
Epoch 0, val loss: 1.9333604574203491
Epoch 10, training loss: 2.0170693397521973 = 1.9333317279815674 + 0.01 * 8.37375259399414
Epoch 10, val loss: 1.9232141971588135
Epoch 20, training loss: 2.0035300254821777 = 1.9197959899902344 + 0.01 * 8.37340259552002
Epoch 20, val loss: 1.9095425605773926
Epoch 30, training loss: 1.9840162992477417 = 1.9002946615219116 + 0.01 * 8.37216854095459
Epoch 30, val loss: 1.8895955085754395
Epoch 40, training loss: 1.9552335739135742 = 1.8715897798538208 + 0.01 * 8.36438274383545
Epoch 40, val loss: 1.8607598543167114
Epoch 50, training loss: 1.916292428970337 = 1.833141803741455 + 0.01 * 8.31506633758545
Epoch 50, val loss: 1.8245347738265991
Epoch 60, training loss: 1.8729811906814575 = 1.7925708293914795 + 0.01 * 8.041030883789062
Epoch 60, val loss: 1.7911969423294067
Epoch 70, training loss: 1.8317571878433228 = 1.7536633014678955 + 0.01 * 7.809391021728516
Epoch 70, val loss: 1.7613394260406494
Epoch 80, training loss: 1.7778395414352417 = 1.7014952898025513 + 0.01 * 7.634425640106201
Epoch 80, val loss: 1.717586874961853
Epoch 90, training loss: 1.7050220966339111 = 1.6305705308914185 + 0.01 * 7.445159912109375
Epoch 90, val loss: 1.6575677394866943
Epoch 100, training loss: 1.615310788154602 = 1.54325532913208 + 0.01 * 7.2055463790893555
Epoch 100, val loss: 1.5862852334976196
Epoch 110, training loss: 1.5225446224212646 = 1.452880859375 + 0.01 * 6.966382026672363
Epoch 110, val loss: 1.5133771896362305
Epoch 120, training loss: 1.4367350339889526 = 1.3675132989883423 + 0.01 * 6.922174453735352
Epoch 120, val loss: 1.4478507041931152
Epoch 130, training loss: 1.3548319339752197 = 1.285974144935608 + 0.01 * 6.8857831954956055
Epoch 130, val loss: 1.3865337371826172
Epoch 140, training loss: 1.272836685180664 = 1.2042478322982788 + 0.01 * 6.858881950378418
Epoch 140, val loss: 1.3265736103057861
Epoch 150, training loss: 1.1889499425888062 = 1.1205614805221558 + 0.01 * 6.838843822479248
Epoch 150, val loss: 1.2663640975952148
Epoch 160, training loss: 1.103185772895813 = 1.0349504947662354 + 0.01 * 6.823524475097656
Epoch 160, val loss: 1.2050549983978271
Epoch 170, training loss: 1.0169757604599 = 0.9488543272018433 + 0.01 * 6.812138557434082
Epoch 170, val loss: 1.142813801765442
Epoch 180, training loss: 0.9324814081192017 = 0.8644287586212158 + 0.01 * 6.805262088775635
Epoch 180, val loss: 1.081426978111267
Epoch 190, training loss: 0.8517470955848694 = 0.7837240695953369 + 0.01 * 6.802300453186035
Epoch 190, val loss: 1.0228341817855835
Epoch 200, training loss: 0.7763082981109619 = 0.708295464515686 + 0.01 * 6.801286220550537
Epoch 200, val loss: 0.9688094258308411
Epoch 210, training loss: 0.7075076699256897 = 0.6395038962364197 + 0.01 * 6.800379753112793
Epoch 210, val loss: 0.9209938645362854
Epoch 220, training loss: 0.6466928720474243 = 0.5787002444267273 + 0.01 * 6.7992634773254395
Epoch 220, val loss: 0.8808746933937073
Epoch 230, training loss: 0.5946489572525024 = 0.5266728401184082 + 0.01 * 6.797612190246582
Epoch 230, val loss: 0.8493458032608032
Epoch 240, training loss: 0.5510014295578003 = 0.4830487072467804 + 0.01 * 6.795274257659912
Epoch 240, val loss: 0.8260321021080017
Epoch 250, training loss: 0.5142284035682678 = 0.44630616903305054 + 0.01 * 6.792223930358887
Epoch 250, val loss: 0.8097273111343384
Epoch 260, training loss: 0.4822095036506653 = 0.41431787610054016 + 0.01 * 6.789161682128906
Epoch 260, val loss: 0.798578143119812
Epoch 270, training loss: 0.45276325941085815 = 0.38491934537887573 + 0.01 * 6.784389972686768
Epoch 270, val loss: 0.7905052304267883
Epoch 280, training loss: 0.4241468906402588 = 0.3563481271266937 + 0.01 * 6.779874801635742
Epoch 280, val loss: 0.783861517906189
Epoch 290, training loss: 0.3953344225883484 = 0.32758450508117676 + 0.01 * 6.774991035461426
Epoch 290, val loss: 0.77744460105896
Epoch 300, training loss: 0.36617374420166016 = 0.29845118522644043 + 0.01 * 6.772257328033447
Epoch 300, val loss: 0.7706418037414551
Epoch 310, training loss: 0.33701202273368835 = 0.2693519592285156 + 0.01 * 6.766006946563721
Epoch 310, val loss: 0.7633856534957886
Epoch 320, training loss: 0.3084458112716675 = 0.2408314198255539 + 0.01 * 6.761437892913818
Epoch 320, val loss: 0.7561487555503845
Epoch 330, training loss: 0.2808562219142914 = 0.21328984200954437 + 0.01 * 6.756637096405029
Epoch 330, val loss: 0.7495972514152527
Epoch 340, training loss: 0.2546321153640747 = 0.18711824715137482 + 0.01 * 6.7513885498046875
Epoch 340, val loss: 0.7444457411766052
Epoch 350, training loss: 0.23021236062049866 = 0.16268928349018097 + 0.01 * 6.752306938171387
Epoch 350, val loss: 0.741143524646759
Epoch 360, training loss: 0.20786339044570923 = 0.140409916639328 + 0.01 * 6.745346546173096
Epoch 360, val loss: 0.7399161458015442
Epoch 370, training loss: 0.18802523612976074 = 0.12062518298625946 + 0.01 * 6.7400054931640625
Epoch 370, val loss: 0.7409963607788086
Epoch 380, training loss: 0.1708630621433258 = 0.10351429879665375 + 0.01 * 6.7348761558532715
Epoch 380, val loss: 0.7444817423820496
Epoch 390, training loss: 0.15630027651786804 = 0.08899767696857452 + 0.01 * 6.730259418487549
Epoch 390, val loss: 0.7501369118690491
Epoch 400, training loss: 0.14409545063972473 = 0.07683168351650238 + 0.01 * 6.726376533508301
Epoch 400, val loss: 0.7575966715812683
Epoch 410, training loss: 0.13399910926818848 = 0.06670337170362473 + 0.01 * 6.729573726654053
Epoch 410, val loss: 0.7664600014686584
Epoch 420, training loss: 0.12547576427459717 = 0.058269012719392776 + 0.01 * 6.720675945281982
Epoch 420, val loss: 0.7763481140136719
Epoch 430, training loss: 0.11840339004993439 = 0.05121372640132904 + 0.01 * 6.718966484069824
Epoch 430, val loss: 0.786832332611084
Epoch 440, training loss: 0.11244480311870575 = 0.04528212174773216 + 0.01 * 6.716269016265869
Epoch 440, val loss: 0.7976471185684204
Epoch 450, training loss: 0.10740771889686584 = 0.040266938507556915 + 0.01 * 6.714077949523926
Epoch 450, val loss: 0.8085716962814331
Epoch 460, training loss: 0.10314677655696869 = 0.036003004759550095 + 0.01 * 6.714377403259277
Epoch 460, val loss: 0.8194668292999268
Epoch 470, training loss: 0.0994565486907959 = 0.03235730156302452 + 0.01 * 6.709924221038818
Epoch 470, val loss: 0.8302299380302429
Epoch 480, training loss: 0.09629590064287186 = 0.029219599440693855 + 0.01 * 6.707630634307861
Epoch 480, val loss: 0.8407294154167175
Epoch 490, training loss: 0.09359169751405716 = 0.026504719629883766 + 0.01 * 6.708698272705078
Epoch 490, val loss: 0.851015567779541
Epoch 500, training loss: 0.0911690816283226 = 0.02414313144981861 + 0.01 * 6.702595233917236
Epoch 500, val loss: 0.8609933853149414
Epoch 510, training loss: 0.08908379077911377 = 0.022077199071645737 + 0.01 * 6.70065975189209
Epoch 510, val loss: 0.8706712126731873
Epoch 520, training loss: 0.08723446726799011 = 0.020260542631149292 + 0.01 * 6.697392463684082
Epoch 520, val loss: 0.8800272345542908
Epoch 530, training loss: 0.08565931022167206 = 0.01865563914179802 + 0.01 * 6.7003679275512695
Epoch 530, val loss: 0.8891218900680542
Epoch 540, training loss: 0.08415351063013077 = 0.017232583835721016 + 0.01 * 6.6920928955078125
Epoch 540, val loss: 0.8979456424713135
Epoch 550, training loss: 0.08287837356328964 = 0.015965133905410767 + 0.01 * 6.691323757171631
Epoch 550, val loss: 0.9065079689025879
Epoch 560, training loss: 0.08173979073762894 = 0.014834677800536156 + 0.01 * 6.690511226654053
Epoch 560, val loss: 0.9148183465003967
Epoch 570, training loss: 0.0806717500090599 = 0.013822624459862709 + 0.01 * 6.68491268157959
Epoch 570, val loss: 0.9228646755218506
Epoch 580, training loss: 0.07973339408636093 = 0.01291243638843298 + 0.01 * 6.682096004486084
Epoch 580, val loss: 0.9306758046150208
Epoch 590, training loss: 0.07901371270418167 = 0.012091236189007759 + 0.01 * 6.6922478675842285
Epoch 590, val loss: 0.9383090138435364
Epoch 600, training loss: 0.07813750207424164 = 0.01134888269007206 + 0.01 * 6.678862571716309
Epoch 600, val loss: 0.9456954002380371
Epoch 610, training loss: 0.07741959393024445 = 0.010675176978111267 + 0.01 * 6.674441814422607
Epoch 610, val loss: 0.9528771042823792
Epoch 620, training loss: 0.07676538079977036 = 0.010061701759696007 + 0.01 * 6.67036771774292
Epoch 620, val loss: 0.9598774313926697
Epoch 630, training loss: 0.07633823901414871 = 0.009501625783741474 + 0.01 * 6.683660984039307
Epoch 630, val loss: 0.9667079448699951
Epoch 640, training loss: 0.07567396014928818 = 0.008990301750600338 + 0.01 * 6.668365955352783
Epoch 640, val loss: 0.9733140468597412
Epoch 650, training loss: 0.07515338808298111 = 0.008521742187440395 + 0.01 * 6.6631646156311035
Epoch 650, val loss: 0.9797518253326416
Epoch 660, training loss: 0.07469399273395538 = 0.008091004565358162 + 0.01 * 6.660299301147461
Epoch 660, val loss: 0.9860206842422485
Epoch 670, training loss: 0.074351467192173 = 0.007694174535572529 + 0.01 * 6.66572904586792
Epoch 670, val loss: 0.9921438694000244
Epoch 680, training loss: 0.07388108223676682 = 0.00732847023755312 + 0.01 * 6.655261039733887
Epoch 680, val loss: 0.998080313205719
Epoch 690, training loss: 0.07349938154220581 = 0.006990328896790743 + 0.01 * 6.650905132293701
Epoch 690, val loss: 1.0039061307907104
Epoch 700, training loss: 0.07321451604366302 = 0.006676850840449333 + 0.01 * 6.653766632080078
Epoch 700, val loss: 1.0095748901367188
Epoch 710, training loss: 0.0728708803653717 = 0.0063858418725430965 + 0.01 * 6.648504257202148
Epoch 710, val loss: 1.0150821208953857
Epoch 720, training loss: 0.07252483069896698 = 0.006115502677857876 + 0.01 * 6.640933036804199
Epoch 720, val loss: 1.0204768180847168
Epoch 730, training loss: 0.07224337011575699 = 0.005864159669727087 + 0.01 * 6.637921333312988
Epoch 730, val loss: 1.0256996154785156
Epoch 740, training loss: 0.071974016726017 = 0.005629518534988165 + 0.01 * 6.6344499588012695
Epoch 740, val loss: 1.0308305025100708
Epoch 750, training loss: 0.07192371785640717 = 0.005409999284893274 + 0.01 * 6.651371955871582
Epoch 750, val loss: 1.0358651876449585
Epoch 760, training loss: 0.07158297300338745 = 0.005205122288316488 + 0.01 * 6.637784957885742
Epoch 760, val loss: 1.0407230854034424
Epoch 770, training loss: 0.0712866559624672 = 0.005013450048863888 + 0.01 * 6.627320289611816
Epoch 770, val loss: 1.0454838275909424
Epoch 780, training loss: 0.07105174660682678 = 0.004833357874304056 + 0.01 * 6.6218390464782715
Epoch 780, val loss: 1.050134301185608
Epoch 790, training loss: 0.07109174877405167 = 0.004663861356675625 + 0.01 * 6.642788887023926
Epoch 790, val loss: 1.0547057390213013
Epoch 800, training loss: 0.0707637295126915 = 0.004504482727497816 + 0.01 * 6.625925064086914
Epoch 800, val loss: 1.0591404438018799
Epoch 810, training loss: 0.07047943025827408 = 0.004354347009211779 + 0.01 * 6.6125078201293945
Epoch 810, val loss: 1.06351900100708
Epoch 820, training loss: 0.07036588340997696 = 0.004212798550724983 + 0.01 * 6.61530876159668
Epoch 820, val loss: 1.0677824020385742
Epoch 830, training loss: 0.07019446790218353 = 0.004079516511410475 + 0.01 * 6.611495494842529
Epoch 830, val loss: 1.071900725364685
Epoch 840, training loss: 0.069993756711483 = 0.003953446168452501 + 0.01 * 6.604031085968018
Epoch 840, val loss: 1.07599937915802
Epoch 850, training loss: 0.06981927901506424 = 0.003833906492218375 + 0.01 * 6.598537445068359
Epoch 850, val loss: 1.0799484252929688
Epoch 860, training loss: 0.06993383914232254 = 0.003720790147781372 + 0.01 * 6.621304988861084
Epoch 860, val loss: 1.0839462280273438
Epoch 870, training loss: 0.0696418434381485 = 0.0036139225121587515 + 0.01 * 6.602792263031006
Epoch 870, val loss: 1.087586522102356
Epoch 880, training loss: 0.06943392753601074 = 0.003512545255944133 + 0.01 * 6.592138290405273
Epoch 880, val loss: 1.0913612842559814
Epoch 890, training loss: 0.06928564608097076 = 0.0034161622170358896 + 0.01 * 6.586948394775391
Epoch 890, val loss: 1.0949928760528564
Epoch 900, training loss: 0.06936465948820114 = 0.0033245235681533813 + 0.01 * 6.604013919830322
Epoch 900, val loss: 1.09852933883667
Epoch 910, training loss: 0.06908822804689407 = 0.0032374104484915733 + 0.01 * 6.585082054138184
Epoch 910, val loss: 1.101974606513977
Epoch 920, training loss: 0.06900831311941147 = 0.0031543998047709465 + 0.01 * 6.585391998291016
Epoch 920, val loss: 1.105381965637207
Epoch 930, training loss: 0.06888531893491745 = 0.003075223881751299 + 0.01 * 6.581009387969971
Epoch 930, val loss: 1.108688473701477
Epoch 940, training loss: 0.06888870894908905 = 0.0029999567195773125 + 0.01 * 6.588875770568848
Epoch 940, val loss: 1.1119924783706665
Epoch 950, training loss: 0.06858145445585251 = 0.0029283843468874693 + 0.01 * 6.565307140350342
Epoch 950, val loss: 1.1150857210159302
Epoch 960, training loss: 0.06857798993587494 = 0.002859918400645256 + 0.01 * 6.571807384490967
Epoch 960, val loss: 1.1181825399398804
Epoch 970, training loss: 0.06846718490123749 = 0.0027944634202867746 + 0.01 * 6.567271709442139
Epoch 970, val loss: 1.1212397813796997
Epoch 980, training loss: 0.0684087723493576 = 0.002731866668909788 + 0.01 * 6.567690372467041
Epoch 980, val loss: 1.124155044555664
Epoch 990, training loss: 0.0685109943151474 = 0.0026718920562416315 + 0.01 * 6.5839104652404785
Epoch 990, val loss: 1.1271079778671265
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9594
Flip ASR: 0.9511/225 nodes
The final ASR:0.69619, 0.19473, Accuracy:0.81852, 0.00800
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11576])
remove edge: torch.Size([2, 9476])
updated graph: torch.Size([2, 10496])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9631
Flip ASR: 0.9556/225 nodes
The final ASR:0.97048, 0.00522, Accuracy:0.83333, 0.00605
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0292303562164307 = 1.945491075515747 + 0.01 * 8.373939514160156
Epoch 0, val loss: 1.943690538406372
Epoch 10, training loss: 2.019606828689575 = 1.9358679056167603 + 0.01 * 8.373896598815918
Epoch 10, val loss: 1.9339981079101562
Epoch 20, training loss: 2.008152723312378 = 1.9244154691696167 + 0.01 * 8.373725891113281
Epoch 20, val loss: 1.922317385673523
Epoch 30, training loss: 1.992370367050171 = 1.908638596534729 + 0.01 * 8.37317943572998
Epoch 30, val loss: 1.906309962272644
Epoch 40, training loss: 1.9691178798675537 = 1.8854153156280518 + 0.01 * 8.370253562927246
Epoch 40, val loss: 1.8831138610839844
Epoch 50, training loss: 1.9355823993682861 = 1.8520961999893188 + 0.01 * 8.348625183105469
Epoch 50, val loss: 1.8512033224105835
Epoch 60, training loss: 1.8936381340026855 = 1.811360478401184 + 0.01 * 8.22776985168457
Epoch 60, val loss: 1.8159708976745605
Epoch 70, training loss: 1.8525172472000122 = 1.7727903127670288 + 0.01 * 7.972692966461182
Epoch 70, val loss: 1.7872475385665894
Epoch 80, training loss: 1.808886170387268 = 1.7308712005615234 + 0.01 * 7.801493167877197
Epoch 80, val loss: 1.7544817924499512
Epoch 90, training loss: 1.7478055953979492 = 1.6727927923202515 + 0.01 * 7.501281261444092
Epoch 90, val loss: 1.7058990001678467
Epoch 100, training loss: 1.6660038232803345 = 1.5937800407409668 + 0.01 * 7.222376346588135
Epoch 100, val loss: 1.6400704383850098
Epoch 110, training loss: 1.566296100616455 = 1.4954067468643188 + 0.01 * 7.08894157409668
Epoch 110, val loss: 1.5607056617736816
Epoch 120, training loss: 1.4567807912826538 = 1.3865283727645874 + 0.01 * 7.025237083435059
Epoch 120, val loss: 1.4733991622924805
Epoch 130, training loss: 1.3450217247009277 = 1.2753722667694092 + 0.01 * 6.964941501617432
Epoch 130, val loss: 1.386069655418396
Epoch 140, training loss: 1.2360033988952637 = 1.1667702198028564 + 0.01 * 6.92332124710083
Epoch 140, val loss: 1.3021430969238281
Epoch 150, training loss: 1.1335796117782593 = 1.064637541770935 + 0.01 * 6.894208908081055
Epoch 150, val loss: 1.2247329950332642
Epoch 160, training loss: 1.0399954319000244 = 0.9712238311767578 + 0.01 * 6.877161026000977
Epoch 160, val loss: 1.1550525426864624
Epoch 170, training loss: 0.9539795517921448 = 0.8852612376213074 + 0.01 * 6.871830940246582
Epoch 170, val loss: 1.090971827507019
Epoch 180, training loss: 0.8725879192352295 = 0.8039184212684631 + 0.01 * 6.86694860458374
Epoch 180, val loss: 1.0299760103225708
Epoch 190, training loss: 0.7943815588951111 = 0.725730836391449 + 0.01 * 6.865070343017578
Epoch 190, val loss: 0.9712969660758972
Epoch 200, training loss: 0.7199222445487976 = 0.6512787938117981 + 0.01 * 6.864346504211426
Epoch 200, val loss: 0.9165028929710388
Epoch 210, training loss: 0.6506089568138123 = 0.5819745063781738 + 0.01 * 6.863443374633789
Epoch 210, val loss: 0.867418110370636
Epoch 220, training loss: 0.5871810913085938 = 0.5185619592666626 + 0.01 * 6.861913204193115
Epoch 220, val loss: 0.8254673480987549
Epoch 230, training loss: 0.5290939211845398 = 0.4604959785938263 + 0.01 * 6.859793663024902
Epoch 230, val loss: 0.7905289530754089
Epoch 240, training loss: 0.47538501024246216 = 0.40679657459259033 + 0.01 * 6.858842372894287
Epoch 240, val loss: 0.7619134783744812
Epoch 250, training loss: 0.42547744512557983 = 0.3569020926952362 + 0.01 * 6.857535362243652
Epoch 250, val loss: 0.7387880086898804
Epoch 260, training loss: 0.3795507550239563 = 0.31101110577583313 + 0.01 * 6.853963375091553
Epoch 260, val loss: 0.7209055423736572
Epoch 270, training loss: 0.3383572995662689 = 0.26984524726867676 + 0.01 * 6.851205348968506
Epoch 270, val loss: 0.7083033919334412
Epoch 280, training loss: 0.3023125231266022 = 0.23383314907550812 + 0.01 * 6.847937107086182
Epoch 280, val loss: 0.700667142868042
Epoch 290, training loss: 0.271375447511673 = 0.2029467523097992 + 0.01 * 6.842870712280273
Epoch 290, val loss: 0.6975477337837219
Epoch 300, training loss: 0.24514752626419067 = 0.1767311543226242 + 0.01 * 6.84163761138916
Epoch 300, val loss: 0.6983582973480225
Epoch 310, training loss: 0.22288373112678528 = 0.15454408526420593 + 0.01 * 6.833964824676514
Epoch 310, val loss: 0.7023314833641052
Epoch 320, training loss: 0.20406416058540344 = 0.13572871685028076 + 0.01 * 6.8335442543029785
Epoch 320, val loss: 0.7088421583175659
Epoch 330, training loss: 0.18794631958007812 = 0.11971671134233475 + 0.01 * 6.822960376739502
Epoch 330, val loss: 0.7172235250473022
Epoch 340, training loss: 0.1741652488708496 = 0.10600884258747101 + 0.01 * 6.815641403198242
Epoch 340, val loss: 0.7269589900970459
Epoch 350, training loss: 0.16229620575904846 = 0.09420300275087357 + 0.01 * 6.809320449829102
Epoch 350, val loss: 0.7377299666404724
Epoch 360, training loss: 0.15200039744377136 = 0.08397137373685837 + 0.01 * 6.802902698516846
Epoch 360, val loss: 0.7491781115531921
Epoch 370, training loss: 0.14303624629974365 = 0.07506022602319717 + 0.01 * 6.79760217666626
Epoch 370, val loss: 0.7611345052719116
Epoch 380, training loss: 0.1352008879184723 = 0.0672716349363327 + 0.01 * 6.792924404144287
Epoch 380, val loss: 0.7734452486038208
Epoch 390, training loss: 0.1283324956893921 = 0.06044483557343483 + 0.01 * 6.788765907287598
Epoch 390, val loss: 0.7859262228012085
Epoch 400, training loss: 0.12231473624706268 = 0.05444526672363281 + 0.01 * 6.786946773529053
Epoch 400, val loss: 0.7986446619033813
Epoch 410, training loss: 0.11698515713214874 = 0.04916555806994438 + 0.01 * 6.781959533691406
Epoch 410, val loss: 0.8114055395126343
Epoch 420, training loss: 0.11227543652057648 = 0.044511888176202774 + 0.01 * 6.776354789733887
Epoch 420, val loss: 0.824248731136322
Epoch 430, training loss: 0.10819949209690094 = 0.04040585830807686 + 0.01 * 6.77936315536499
Epoch 430, val loss: 0.8370119333267212
Epoch 440, training loss: 0.10443940758705139 = 0.03678126260638237 + 0.01 * 6.765814781188965
Epoch 440, val loss: 0.8498749732971191
Epoch 450, training loss: 0.10118614882230759 = 0.03357495367527008 + 0.01 * 6.761119365692139
Epoch 450, val loss: 0.8626083731651306
Epoch 460, training loss: 0.098301962018013 = 0.030732374638319016 + 0.01 * 6.756959438323975
Epoch 460, val loss: 0.8753232955932617
Epoch 470, training loss: 0.09575176984071732 = 0.028207190334796906 + 0.01 * 6.754457950592041
Epoch 470, val loss: 0.8879013061523438
Epoch 480, training loss: 0.0934305340051651 = 0.025957927107810974 + 0.01 * 6.747260570526123
Epoch 480, val loss: 0.900314211845398
Epoch 490, training loss: 0.09139106422662735 = 0.023949861526489258 + 0.01 * 6.744120121002197
Epoch 490, val loss: 0.9125224947929382
Epoch 500, training loss: 0.0895543098449707 = 0.022153493016958237 + 0.01 * 6.740081310272217
Epoch 500, val loss: 0.9245312213897705
Epoch 510, training loss: 0.08787307143211365 = 0.020538881421089172 + 0.01 * 6.733419418334961
Epoch 510, val loss: 0.9363241195678711
Epoch 520, training loss: 0.08636565506458282 = 0.01908104494214058 + 0.01 * 6.728461265563965
Epoch 520, val loss: 0.9479285478591919
Epoch 530, training loss: 0.08502525091171265 = 0.017763478681445122 + 0.01 * 6.726177215576172
Epoch 530, val loss: 0.9592262506484985
Epoch 540, training loss: 0.08375096321105957 = 0.016571620479226112 + 0.01 * 6.717934608459473
Epoch 540, val loss: 0.9703137278556824
Epoch 550, training loss: 0.08261419087648392 = 0.015489756129682064 + 0.01 * 6.7124433517456055
Epoch 550, val loss: 0.9811654686927795
Epoch 560, training loss: 0.08168919384479523 = 0.0145059609785676 + 0.01 * 6.718323707580566
Epoch 560, val loss: 0.9917318820953369
Epoch 570, training loss: 0.08068358153104782 = 0.013611254282295704 + 0.01 * 6.70723295211792
Epoch 570, val loss: 1.0021064281463623
Epoch 580, training loss: 0.07981784641742706 = 0.012794951908290386 + 0.01 * 6.702290058135986
Epoch 580, val loss: 1.0121748447418213
Epoch 590, training loss: 0.07899223268032074 = 0.012049376033246517 + 0.01 * 6.694285869598389
Epoch 590, val loss: 1.0220272541046143
Epoch 600, training loss: 0.07820072025060654 = 0.011366710998117924 + 0.01 * 6.683401107788086
Epoch 600, val loss: 1.0315955877304077
Epoch 610, training loss: 0.07751975953578949 = 0.010740519501268864 + 0.01 * 6.677924156188965
Epoch 610, val loss: 1.040966510772705
Epoch 620, training loss: 0.07691718637943268 = 0.010165932588279247 + 0.01 * 6.675125598907471
Epoch 620, val loss: 1.0500366687774658
Epoch 630, training loss: 0.07642238587141037 = 0.009636998176574707 + 0.01 * 6.678539276123047
Epoch 630, val loss: 1.0589213371276855
Epoch 640, training loss: 0.0757407397031784 = 0.009149696677923203 + 0.01 * 6.659104824066162
Epoch 640, val loss: 1.0675300359725952
Epoch 650, training loss: 0.07526834309101105 = 0.008699939586222172 + 0.01 * 6.656840801239014
Epoch 650, val loss: 1.075911045074463
Epoch 660, training loss: 0.07486160099506378 = 0.008284484967589378 + 0.01 * 6.657711505889893
Epoch 660, val loss: 1.0841078758239746
Epoch 670, training loss: 0.07443289458751678 = 0.007899447344243526 + 0.01 * 6.653344631195068
Epoch 670, val loss: 1.0920789241790771
Epoch 680, training loss: 0.07386547327041626 = 0.00754193402826786 + 0.01 * 6.632354259490967
Epoch 680, val loss: 1.099806308746338
Epoch 690, training loss: 0.07346712052822113 = 0.007209551986306906 + 0.01 * 6.625756740570068
Epoch 690, val loss: 1.1073476076126099
Epoch 700, training loss: 0.07317140698432922 = 0.006900489330291748 + 0.01 * 6.627091884613037
Epoch 700, val loss: 1.1147361993789673
Epoch 710, training loss: 0.07283356040716171 = 0.006612210068851709 + 0.01 * 6.622134685516357
Epoch 710, val loss: 1.1219526529312134
Epoch 720, training loss: 0.07255736738443375 = 0.006342961452901363 + 0.01 * 6.621440887451172
Epoch 720, val loss: 1.1288502216339111
Epoch 730, training loss: 0.07214350998401642 = 0.006091549061238766 + 0.01 * 6.605196475982666
Epoch 730, val loss: 1.1356604099273682
Epoch 740, training loss: 0.07202278822660446 = 0.0058560362085700035 + 0.01 * 6.61667537689209
Epoch 740, val loss: 1.1422929763793945
Epoch 750, training loss: 0.07167623937129974 = 0.005635232198983431 + 0.01 * 6.604101181030273
Epoch 750, val loss: 1.1487414836883545
Epoch 760, training loss: 0.0712999552488327 = 0.005427921190857887 + 0.01 * 6.587203502655029
Epoch 760, val loss: 1.1550565958023071
Epoch 770, training loss: 0.07130906730890274 = 0.005232945550233126 + 0.01 * 6.607612609863281
Epoch 770, val loss: 1.1612094640731812
Epoch 780, training loss: 0.07087539881467819 = 0.005049630533903837 + 0.01 * 6.582577228546143
Epoch 780, val loss: 1.1671711206436157
Epoch 790, training loss: 0.07056832313537598 = 0.004876900464296341 + 0.01 * 6.5691423416137695
Epoch 790, val loss: 1.1729944944381714
Epoch 800, training loss: 0.07045512646436691 = 0.004713736474514008 + 0.01 * 6.57413911819458
Epoch 800, val loss: 1.178734302520752
Epoch 810, training loss: 0.0701858252286911 = 0.00455970736220479 + 0.01 * 6.5626115798950195
Epoch 810, val loss: 1.1842371225357056
Epoch 820, training loss: 0.07001584768295288 = 0.004414122551679611 + 0.01 * 6.5601725578308105
Epoch 820, val loss: 1.1896991729736328
Epoch 830, training loss: 0.06985123455524445 = 0.004276335705071688 + 0.01 * 6.55748987197876
Epoch 830, val loss: 1.1950339078903198
Epoch 840, training loss: 0.06964025646448135 = 0.004145691636949778 + 0.01 * 6.549456596374512
Epoch 840, val loss: 1.200234293937683
Epoch 850, training loss: 0.06948278099298477 = 0.004021809436380863 + 0.01 * 6.546097278594971
Epoch 850, val loss: 1.2052010297775269
Epoch 860, training loss: 0.06949260085821152 = 0.0039045587182044983 + 0.01 * 6.558804512023926
Epoch 860, val loss: 1.2102861404418945
Epoch 870, training loss: 0.06918114423751831 = 0.003793247975409031 + 0.01 * 6.538790225982666
Epoch 870, val loss: 1.2150325775146484
Epoch 880, training loss: 0.06911295652389526 = 0.0036873577628284693 + 0.01 * 6.542559623718262
Epoch 880, val loss: 1.2197803258895874
Epoch 890, training loss: 0.068921759724617 = 0.0035865074023604393 + 0.01 * 6.533525466918945
Epoch 890, val loss: 1.2244473695755005
Epoch 900, training loss: 0.06902837008237839 = 0.0034903320483863354 + 0.01 * 6.553804397583008
Epoch 900, val loss: 1.2289031744003296
Epoch 910, training loss: 0.06864595413208008 = 0.0033987502101808786 + 0.01 * 6.524720191955566
Epoch 910, val loss: 1.2333753108978271
Epoch 920, training loss: 0.0685746967792511 = 0.003311365144327283 + 0.01 * 6.526333332061768
Epoch 920, val loss: 1.2377123832702637
Epoch 930, training loss: 0.06843765825033188 = 0.0032279822044074535 + 0.01 * 6.520967960357666
Epoch 930, val loss: 1.2419352531433105
Epoch 940, training loss: 0.06840083003044128 = 0.0031483417842537165 + 0.01 * 6.525249481201172
Epoch 940, val loss: 1.246035099029541
Epoch 950, training loss: 0.06824477016925812 = 0.0030721835792064667 + 0.01 * 6.517258167266846
Epoch 950, val loss: 1.250140905380249
Epoch 960, training loss: 0.06820448487997055 = 0.0029992747586220503 + 0.01 * 6.52052116394043
Epoch 960, val loss: 1.2541067600250244
Epoch 970, training loss: 0.068051777780056 = 0.002929551061242819 + 0.01 * 6.5122222900390625
Epoch 970, val loss: 1.257989764213562
Epoch 980, training loss: 0.06812971085309982 = 0.0028626967687159777 + 0.01 * 6.526700973510742
Epoch 980, val loss: 1.2618848085403442
Epoch 990, training loss: 0.0678306296467781 = 0.002798695582896471 + 0.01 * 6.503193378448486
Epoch 990, val loss: 1.2655467987060547
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.6236
Flip ASR: 0.5511/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.025242805480957 = 1.9415035247802734 + 0.01 * 8.373934745788574
Epoch 0, val loss: 1.929356336593628
Epoch 10, training loss: 2.0149691104888916 = 1.9312312602996826 + 0.01 * 8.373794555664062
Epoch 10, val loss: 1.9184088706970215
Epoch 20, training loss: 2.002277374267578 = 1.918544888496399 + 0.01 * 8.373259544372559
Epoch 20, val loss: 1.9038472175598145
Epoch 30, training loss: 1.9842956066131592 = 1.9005815982818604 + 0.01 * 8.3713960647583
Epoch 30, val loss: 1.8825623989105225
Epoch 40, training loss: 1.9585436582565308 = 1.874894142150879 + 0.01 * 8.364949226379395
Epoch 40, val loss: 1.8529994487762451
Epoch 50, training loss: 1.9232608079910278 = 1.839920163154602 + 0.01 * 8.334059715270996
Epoch 50, val loss: 1.8160419464111328
Epoch 60, training loss: 1.8792375326156616 = 1.797660231590271 + 0.01 * 8.157729148864746
Epoch 60, val loss: 1.776677131652832
Epoch 70, training loss: 1.8331665992736816 = 1.754923939704895 + 0.01 * 7.824265480041504
Epoch 70, val loss: 1.7413623332977295
Epoch 80, training loss: 1.7812111377716064 = 1.705135703086853 + 0.01 * 7.607546806335449
Epoch 80, val loss: 1.698974847793579
Epoch 90, training loss: 1.710140347480774 = 1.6347200870513916 + 0.01 * 7.5420308113098145
Epoch 90, val loss: 1.638811707496643
Epoch 100, training loss: 1.6173628568649292 = 1.5424946546554565 + 0.01 * 7.486819267272949
Epoch 100, val loss: 1.5644773244857788
Epoch 110, training loss: 1.5148555040359497 = 1.4411306381225586 + 0.01 * 7.372481346130371
Epoch 110, val loss: 1.4865227937698364
Epoch 120, training loss: 1.4204387664794922 = 1.3478302955627441 + 0.01 * 7.260841369628906
Epoch 120, val loss: 1.419904112815857
Epoch 130, training loss: 1.3404059410095215 = 1.2686294317245483 + 0.01 * 7.177652359008789
Epoch 130, val loss: 1.368221402168274
Epoch 140, training loss: 1.2730317115783691 = 1.201855182647705 + 0.01 * 7.117658615112305
Epoch 140, val loss: 1.3267852067947388
Epoch 150, training loss: 1.212230920791626 = 1.141711950302124 + 0.01 * 7.051894187927246
Epoch 150, val loss: 1.2896826267242432
Epoch 160, training loss: 1.151693344116211 = 1.081721544265747 + 0.01 * 6.997183322906494
Epoch 160, val loss: 1.252460241317749
Epoch 170, training loss: 1.087692379951477 = 1.0181280374526978 + 0.01 * 6.956439018249512
Epoch 170, val loss: 1.2119100093841553
Epoch 180, training loss: 1.0199167728424072 = 0.9506290555000305 + 0.01 * 6.928767204284668
Epoch 180, val loss: 1.167333722114563
Epoch 190, training loss: 0.9498772621154785 = 0.8807804584503174 + 0.01 * 6.909681797027588
Epoch 190, val loss: 1.1202099323272705
Epoch 200, training loss: 0.8785938024520874 = 0.8096173405647278 + 0.01 * 6.89764928817749
Epoch 200, val loss: 1.0709784030914307
Epoch 210, training loss: 0.8062084317207336 = 0.7372946739196777 + 0.01 * 6.891376495361328
Epoch 210, val loss: 1.0196959972381592
Epoch 220, training loss: 0.7334640026092529 = 0.6645925045013428 + 0.01 * 6.887150287628174
Epoch 220, val loss: 0.9674506187438965
Epoch 230, training loss: 0.6634557247161865 = 0.5945937633514404 + 0.01 * 6.886196613311768
Epoch 230, val loss: 0.917240560054779
Epoch 240, training loss: 0.5988812446594238 = 0.5300515294075012 + 0.01 * 6.882970809936523
Epoch 240, val loss: 0.8726579546928406
Epoch 250, training loss: 0.540859580039978 = 0.4720444977283478 + 0.01 * 6.881511688232422
Epoch 250, val loss: 0.8361491560935974
Epoch 260, training loss: 0.4887988567352295 = 0.42000603675842285 + 0.01 * 6.879281997680664
Epoch 260, val loss: 0.807806134223938
Epoch 270, training loss: 0.44173088669776917 = 0.37294667959213257 + 0.01 * 6.878421306610107
Epoch 270, val loss: 0.7867698669433594
Epoch 280, training loss: 0.3985268473625183 = 0.32974326610565186 + 0.01 * 6.87835693359375
Epoch 280, val loss: 0.7715762853622437
Epoch 290, training loss: 0.35883039236068726 = 0.29004722833633423 + 0.01 * 6.8783183097839355
Epoch 290, val loss: 0.7611514925956726
Epoch 300, training loss: 0.3227284550666809 = 0.25392717123031616 + 0.01 * 6.880128383636475
Epoch 300, val loss: 0.7548909783363342
Epoch 310, training loss: 0.2903140187263489 = 0.22153393924236298 + 0.01 * 6.87800931930542
Epoch 310, val loss: 0.7521634101867676
Epoch 320, training loss: 0.261711984872818 = 0.19292910397052765 + 0.01 * 6.878288269042969
Epoch 320, val loss: 0.7526314854621887
Epoch 330, training loss: 0.2367774397134781 = 0.1680098921060562 + 0.01 * 6.8767547607421875
Epoch 330, val loss: 0.755573034286499
Epoch 340, training loss: 0.21520453691482544 = 0.1464322954416275 + 0.01 * 6.877224922180176
Epoch 340, val loss: 0.7602398991584778
Epoch 350, training loss: 0.19643262028694153 = 0.12770341336727142 + 0.01 * 6.872921466827393
Epoch 350, val loss: 0.7657148241996765
Epoch 360, training loss: 0.1801699697971344 = 0.11145331710577011 + 0.01 * 6.871666431427002
Epoch 360, val loss: 0.7717019319534302
Epoch 370, training loss: 0.16605031490325928 = 0.0973774716258049 + 0.01 * 6.867284297943115
Epoch 370, val loss: 0.7781785130500793
Epoch 380, training loss: 0.15381664037704468 = 0.0851849839091301 + 0.01 * 6.863166332244873
Epoch 380, val loss: 0.7854264974594116
Epoch 390, training loss: 0.14328381419181824 = 0.07468957453966141 + 0.01 * 6.859425067901611
Epoch 390, val loss: 0.7932363152503967
Epoch 400, training loss: 0.13417409360408783 = 0.06563355028629303 + 0.01 * 6.8540544509887695
Epoch 400, val loss: 0.8015668392181396
Epoch 410, training loss: 0.12640348076820374 = 0.057852379977703094 + 0.01 * 6.855109691619873
Epoch 410, val loss: 0.8104076981544495
Epoch 420, training loss: 0.11961928009986877 = 0.05118085816502571 + 0.01 * 6.843842506408691
Epoch 420, val loss: 0.8198200464248657
Epoch 430, training loss: 0.1138109341263771 = 0.04545130580663681 + 0.01 * 6.835962772369385
Epoch 430, val loss: 0.8296840190887451
Epoch 440, training loss: 0.10887384414672852 = 0.04051852598786354 + 0.01 * 6.8355326652526855
Epoch 440, val loss: 0.8398032784461975
Epoch 450, training loss: 0.10451500862836838 = 0.036274075508117676 + 0.01 * 6.824093818664551
Epoch 450, val loss: 0.8500474691390991
Epoch 460, training loss: 0.10087070614099503 = 0.032624535262584686 + 0.01 * 6.824617385864258
Epoch 460, val loss: 0.8604017496109009
Epoch 470, training loss: 0.09761044383049011 = 0.029478739947080612 + 0.01 * 6.813170433044434
Epoch 470, val loss: 0.8708054423332214
Epoch 480, training loss: 0.09485535323619843 = 0.026756420731544495 + 0.01 * 6.8098931312561035
Epoch 480, val loss: 0.8811492323875427
Epoch 490, training loss: 0.09245436638593674 = 0.02439303882420063 + 0.01 * 6.806132793426514
Epoch 490, val loss: 0.891325831413269
Epoch 500, training loss: 0.09049740433692932 = 0.02233307436108589 + 0.01 * 6.816433429718018
Epoch 500, val loss: 0.9013248682022095
Epoch 510, training loss: 0.08848854154348373 = 0.020530717447400093 + 0.01 * 6.795782566070557
Epoch 510, val loss: 0.9110859632492065
Epoch 520, training loss: 0.08680073916912079 = 0.01894407346844673 + 0.01 * 6.785667419433594
Epoch 520, val loss: 0.9206225275993347
Epoch 530, training loss: 0.08537651598453522 = 0.01753971166908741 + 0.01 * 6.783680438995361
Epoch 530, val loss: 0.9299058318138123
Epoch 540, training loss: 0.08405596762895584 = 0.016292089596390724 + 0.01 * 6.776388168334961
Epoch 540, val loss: 0.9389618635177612
Epoch 550, training loss: 0.0829124003648758 = 0.015177960507571697 + 0.01 * 6.773444652557373
Epoch 550, val loss: 0.9477565884590149
Epoch 560, training loss: 0.08175007998943329 = 0.014179125428199768 + 0.01 * 6.757095813751221
Epoch 560, val loss: 0.9562782645225525
Epoch 570, training loss: 0.08095346391201019 = 0.01327988039702177 + 0.01 * 6.767358779907227
Epoch 570, val loss: 0.9644997119903564
Epoch 580, training loss: 0.07993125915527344 = 0.012468562461435795 + 0.01 * 6.746270179748535
Epoch 580, val loss: 0.9725484251976013
Epoch 590, training loss: 0.07927756756544113 = 0.011733002960681915 + 0.01 * 6.754456520080566
Epoch 590, val loss: 0.9803562760353088
Epoch 600, training loss: 0.07841964066028595 = 0.011065542697906494 + 0.01 * 6.735409736633301
Epoch 600, val loss: 0.9879871606826782
Epoch 610, training loss: 0.07766256481409073 = 0.010457649827003479 + 0.01 * 6.720491886138916
Epoch 610, val loss: 0.9953957796096802
Epoch 620, training loss: 0.07726379483938217 = 0.009902344085276127 + 0.01 * 6.73614501953125
Epoch 620, val loss: 1.0025277137756348
Epoch 630, training loss: 0.07668707519769669 = 0.009394806809723377 + 0.01 * 6.729226589202881
Epoch 630, val loss: 1.0095535516738892
Epoch 640, training loss: 0.07613509893417358 = 0.008929646573960781 + 0.01 * 6.720545291900635
Epoch 640, val loss: 1.016335129737854
Epoch 650, training loss: 0.07545691728591919 = 0.008501734584569931 + 0.01 * 6.695518493652344
Epoch 650, val loss: 1.0229209661483765
Epoch 660, training loss: 0.07499489188194275 = 0.008106927387416363 + 0.01 * 6.6887969970703125
Epoch 660, val loss: 1.0292948484420776
Epoch 670, training loss: 0.07470513135194778 = 0.007742195390164852 + 0.01 * 6.696293830871582
Epoch 670, val loss: 1.035589575767517
Epoch 680, training loss: 0.07418684661388397 = 0.007403836585581303 + 0.01 * 6.6783013343811035
Epoch 680, val loss: 1.0417016744613647
Epoch 690, training loss: 0.07379240542650223 = 0.0070895724929869175 + 0.01 * 6.670283794403076
Epoch 690, val loss: 1.0476555824279785
Epoch 700, training loss: 0.07348812371492386 = 0.006797128356993198 + 0.01 * 6.669099807739258
Epoch 700, val loss: 1.0534446239471436
Epoch 710, training loss: 0.07321737706661224 = 0.006524784490466118 + 0.01 * 6.669259548187256
Epoch 710, val loss: 1.0590827465057373
Epoch 720, training loss: 0.07278414070606232 = 0.006270857527852058 + 0.01 * 6.6513285636901855
Epoch 720, val loss: 1.0646003484725952
Epoch 730, training loss: 0.0730513259768486 = 0.006033027078956366 + 0.01 * 6.7018303871154785
Epoch 730, val loss: 1.0699480772018433
Epoch 740, training loss: 0.07219748198986053 = 0.005810739006847143 + 0.01 * 6.638674259185791
Epoch 740, val loss: 1.0752089023590088
Epoch 750, training loss: 0.07191698253154755 = 0.005602400749921799 + 0.01 * 6.631458759307861
Epoch 750, val loss: 1.0802863836288452
Epoch 760, training loss: 0.07209214568138123 = 0.0054068188183009624 + 0.01 * 6.668532848358154
Epoch 760, val loss: 1.0852996110916138
Epoch 770, training loss: 0.07150928676128387 = 0.005223382730036974 + 0.01 * 6.6285905838012695
Epoch 770, val loss: 1.0902191400527954
Epoch 780, training loss: 0.07127165049314499 = 0.005050757434219122 + 0.01 * 6.622089385986328
Epoch 780, val loss: 1.0949935913085938
Epoch 790, training loss: 0.0713241770863533 = 0.00488789938390255 + 0.01 * 6.643627643585205
Epoch 790, val loss: 1.0997599363327026
Epoch 800, training loss: 0.0710713192820549 = 0.004734536167234182 + 0.01 * 6.633678436279297
Epoch 800, val loss: 1.1043119430541992
Epoch 810, training loss: 0.07071419060230255 = 0.0045896959491074085 + 0.01 * 6.6124491691589355
Epoch 810, val loss: 1.1088106632232666
Epoch 820, training loss: 0.07042719423770905 = 0.004452561028301716 + 0.01 * 6.597463130950928
Epoch 820, val loss: 1.1131973266601562
Epoch 830, training loss: 0.070471853017807 = 0.004322479944676161 + 0.01 * 6.614937782287598
Epoch 830, val loss: 1.1174960136413574
Epoch 840, training loss: 0.07006905972957611 = 0.004199132323265076 + 0.01 * 6.586993217468262
Epoch 840, val loss: 1.1217684745788574
Epoch 850, training loss: 0.06991356611251831 = 0.004081948194652796 + 0.01 * 6.5831618309021
Epoch 850, val loss: 1.1258805990219116
Epoch 860, training loss: 0.06974951922893524 = 0.003970668185502291 + 0.01 * 6.577885627746582
Epoch 860, val loss: 1.1300369501113892
Epoch 870, training loss: 0.06978999078273773 = 0.0038647986948490143 + 0.01 * 6.592519760131836
Epoch 870, val loss: 1.1339057683944702
Epoch 880, training loss: 0.0696006491780281 = 0.003764233784750104 + 0.01 * 6.583641529083252
Epoch 880, val loss: 1.1379221677780151
Epoch 890, training loss: 0.06937196850776672 = 0.0036683445796370506 + 0.01 * 6.570362567901611
Epoch 890, val loss: 1.1416925191879272
Epoch 900, training loss: 0.06950850784778595 = 0.003576565533876419 + 0.01 * 6.593194484710693
Epoch 900, val loss: 1.1454956531524658
Epoch 910, training loss: 0.06911731511354446 = 0.003489000955596566 + 0.01 * 6.562831401824951
Epoch 910, val loss: 1.1492031812667847
Epoch 920, training loss: 0.06898542493581772 = 0.0034053833223879337 + 0.01 * 6.558003902435303
Epoch 920, val loss: 1.1528679132461548
Epoch 930, training loss: 0.06893473118543625 = 0.0033253938890993595 + 0.01 * 6.560934066772461
Epoch 930, val loss: 1.1564524173736572
Epoch 940, training loss: 0.06874514371156693 = 0.0032488745637238026 + 0.01 * 6.54962682723999
Epoch 940, val loss: 1.1599781513214111
Epoch 950, training loss: 0.06902991980314255 = 0.0031755431555211544 + 0.01 * 6.585437774658203
Epoch 950, val loss: 1.1634641885757446
Epoch 960, training loss: 0.06867065280675888 = 0.003105448791757226 + 0.01 * 6.556520462036133
Epoch 960, val loss: 1.166835069656372
Epoch 970, training loss: 0.068783700466156 = 0.0030381795950233936 + 0.01 * 6.574552059173584
Epoch 970, val loss: 1.1702556610107422
Epoch 980, training loss: 0.06832297146320343 = 0.0029736990109086037 + 0.01 * 6.534927845001221
Epoch 980, val loss: 1.1734747886657715
Epoch 990, training loss: 0.06871724128723145 = 0.0029117614030838013 + 0.01 * 6.580548286437988
Epoch 990, val loss: 1.1766539812088013
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.6347
Flip ASR: 0.5867/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0415234565734863 = 1.9577844142913818 + 0.01 * 8.373902320861816
Epoch 0, val loss: 1.9656062126159668
Epoch 10, training loss: 2.0303919315338135 = 1.946653962135315 + 0.01 * 8.373807907104492
Epoch 10, val loss: 1.953755497932434
Epoch 20, training loss: 2.0162060260772705 = 1.9324707984924316 + 0.01 * 8.373522758483887
Epoch 20, val loss: 1.938425898551941
Epoch 30, training loss: 1.99567711353302 = 1.911952257156372 + 0.01 * 8.37248706817627
Epoch 30, val loss: 1.9163818359375
Epoch 40, training loss: 1.9649728536605835 = 1.8813105821609497 + 0.01 * 8.366226196289062
Epoch 40, val loss: 1.8842551708221436
Epoch 50, training loss: 1.9228101968765259 = 1.8395063877105713 + 0.01 * 8.330377578735352
Epoch 50, val loss: 1.8426438570022583
Epoch 60, training loss: 1.8764357566833496 = 1.794812560081482 + 0.01 * 8.162317276000977
Epoch 60, val loss: 1.8017789125442505
Epoch 70, training loss: 1.8332948684692383 = 1.7543293237686157 + 0.01 * 7.896550178527832
Epoch 70, val loss: 1.7651742696762085
Epoch 80, training loss: 1.7798100709915161 = 1.7024848461151123 + 0.01 * 7.732517242431641
Epoch 80, val loss: 1.7166143655776978
Epoch 90, training loss: 1.7078392505645752 = 1.6326026916503906 + 0.01 * 7.523659706115723
Epoch 90, val loss: 1.6546785831451416
Epoch 100, training loss: 1.6181449890136719 = 1.5444061756134033 + 0.01 * 7.373876571655273
Epoch 100, val loss: 1.5803385972976685
Epoch 110, training loss: 1.52235746383667 = 1.4502636194229126 + 0.01 * 7.209378719329834
Epoch 110, val loss: 1.5038180351257324
Epoch 120, training loss: 1.4326390027999878 = 1.361376404762268 + 0.01 * 7.126262664794922
Epoch 120, val loss: 1.4365655183792114
Epoch 130, training loss: 1.3507442474365234 = 1.279932975769043 + 0.01 * 7.081122875213623
Epoch 130, val loss: 1.3801908493041992
Epoch 140, training loss: 1.274332046508789 = 1.203818678855896 + 0.01 * 7.051333904266357
Epoch 140, val loss: 1.330976128578186
Epoch 150, training loss: 1.2008192539215088 = 1.1306862831115723 + 0.01 * 7.01330041885376
Epoch 150, val loss: 1.2850515842437744
Epoch 160, training loss: 1.1281673908233643 = 1.058513879776001 + 0.01 * 6.965349197387695
Epoch 160, val loss: 1.239952802658081
Epoch 170, training loss: 1.0555219650268555 = 0.9863392114639282 + 0.01 * 6.918278694152832
Epoch 170, val loss: 1.1943000555038452
Epoch 180, training loss: 0.9832117557525635 = 0.9143351912498474 + 0.01 * 6.887655258178711
Epoch 180, val loss: 1.1476691961288452
Epoch 190, training loss: 0.9119897484779358 = 0.8432495594024658 + 0.01 * 6.874019145965576
Epoch 190, val loss: 1.1003055572509766
Epoch 200, training loss: 0.842136025428772 = 0.7734420895576477 + 0.01 * 6.869390487670898
Epoch 200, val loss: 1.0521255731582642
Epoch 210, training loss: 0.7735210657119751 = 0.704847514629364 + 0.01 * 6.867352485656738
Epoch 210, val loss: 1.0035858154296875
Epoch 220, training loss: 0.7064266204833984 = 0.6377729177474976 + 0.01 * 6.8653693199157715
Epoch 220, val loss: 0.9557667970657349
Epoch 230, training loss: 0.6419762372970581 = 0.5733365416526794 + 0.01 * 6.863971710205078
Epoch 230, val loss: 0.9104012846946716
Epoch 240, training loss: 0.5817919969558716 = 0.513163149356842 + 0.01 * 6.862885475158691
Epoch 240, val loss: 0.8698409199714661
Epoch 250, training loss: 0.5272144079208374 = 0.4585980772972107 + 0.01 * 6.861634254455566
Epoch 250, val loss: 0.835944652557373
Epoch 260, training loss: 0.47838664054870605 = 0.40978652238845825 + 0.01 * 6.860011100769043
Epoch 260, val loss: 0.8093100786209106
Epoch 270, training loss: 0.43435025215148926 = 0.36576732993125916 + 0.01 * 6.858293056488037
Epoch 270, val loss: 0.7891288995742798
Epoch 280, training loss: 0.3940333425998688 = 0.3254680037498474 + 0.01 * 6.856533050537109
Epoch 280, val loss: 0.7738872766494751
Epoch 290, training loss: 0.35675615072250366 = 0.28821060061454773 + 0.01 * 6.854555606842041
Epoch 290, val loss: 0.7624903321266174
Epoch 300, training loss: 0.3223835229873657 = 0.25386106967926025 + 0.01 * 6.85224723815918
Epoch 300, val loss: 0.7544180154800415
Epoch 310, training loss: 0.29113858938217163 = 0.2226428985595703 + 0.01 * 6.849569320678711
Epoch 310, val loss: 0.7492333650588989
Epoch 320, training loss: 0.2631591856479645 = 0.19469888508319855 + 0.01 * 6.846029281616211
Epoch 320, val loss: 0.7469822764396667
Epoch 330, training loss: 0.23836153745651245 = 0.16993029415607452 + 0.01 * 6.843123435974121
Epoch 330, val loss: 0.7472562193870544
Epoch 340, training loss: 0.21645382046699524 = 0.14808233082294464 + 0.01 * 6.837149620056152
Epoch 340, val loss: 0.7496103048324585
Epoch 350, training loss: 0.19729971885681152 = 0.12890435755252838 + 0.01 * 6.839537143707275
Epoch 350, val loss: 0.7537413239479065
Epoch 360, training loss: 0.18045590817928314 = 0.11220430582761765 + 0.01 * 6.825160503387451
Epoch 360, val loss: 0.7596766948699951
Epoch 370, training loss: 0.16591709852218628 = 0.09777523577213287 + 0.01 * 6.814185619354248
Epoch 370, val loss: 0.7670033574104309
Epoch 380, training loss: 0.15353640913963318 = 0.0853937640786171 + 0.01 * 6.814264297485352
Epoch 380, val loss: 0.7755467891693115
Epoch 390, training loss: 0.14277634024620056 = 0.07481946796178818 + 0.01 * 6.795687675476074
Epoch 390, val loss: 0.7852938771247864
Epoch 400, training loss: 0.1336589753627777 = 0.06580033898353577 + 0.01 * 6.785862922668457
Epoch 400, val loss: 0.7959582805633545
Epoch 410, training loss: 0.12594832479953766 = 0.05810713395476341 + 0.01 * 6.784119606018066
Epoch 410, val loss: 0.8073517680168152
Epoch 420, training loss: 0.11926724016666412 = 0.05153824761509895 + 0.01 * 6.772900104522705
Epoch 420, val loss: 0.8193390369415283
Epoch 430, training loss: 0.11354313790798187 = 0.045915938913822174 + 0.01 * 6.762720108032227
Epoch 430, val loss: 0.8317153453826904
Epoch 440, training loss: 0.10872383415699005 = 0.04109037667512894 + 0.01 * 6.763346195220947
Epoch 440, val loss: 0.8442431688308716
Epoch 450, training loss: 0.10448367148637772 = 0.03693625330924988 + 0.01 * 6.75474214553833
Epoch 450, val loss: 0.8569677472114563
Epoch 460, training loss: 0.10085158050060272 = 0.03334905952215195 + 0.01 * 6.7502522468566895
Epoch 460, val loss: 0.8694433569908142
Epoch 470, training loss: 0.0976538360118866 = 0.03023943305015564 + 0.01 * 6.741440773010254
Epoch 470, val loss: 0.8818610310554504
Epoch 480, training loss: 0.09492365270853043 = 0.027533860877156258 + 0.01 * 6.738979816436768
Epoch 480, val loss: 0.8940391540527344
Epoch 490, training loss: 0.09250570088624954 = 0.025170037522912025 + 0.01 * 6.733566761016846
Epoch 490, val loss: 0.9059514403343201
Epoch 500, training loss: 0.09040099382400513 = 0.02309570647776127 + 0.01 * 6.730528831481934
Epoch 500, val loss: 0.9175539016723633
Epoch 510, training loss: 0.08851253241300583 = 0.02126827836036682 + 0.01 * 6.724425315856934
Epoch 510, val loss: 0.9288226962089539
Epoch 520, training loss: 0.08688519895076752 = 0.01965196616947651 + 0.01 * 6.723323822021484
Epoch 520, val loss: 0.9396814703941345
Epoch 530, training loss: 0.0853818729519844 = 0.01821673847734928 + 0.01 * 6.716513633728027
Epoch 530, val loss: 0.9503775238990784
Epoch 540, training loss: 0.08405989408493042 = 0.01693664863705635 + 0.01 * 6.712324619293213
Epoch 540, val loss: 0.9606232643127441
Epoch 550, training loss: 0.08286778628826141 = 0.015789736062288284 + 0.01 * 6.707805156707764
Epoch 550, val loss: 0.9707080721855164
Epoch 560, training loss: 0.08185259252786636 = 0.014759755693376064 + 0.01 * 6.709283828735352
Epoch 560, val loss: 0.9803021550178528
Epoch 570, training loss: 0.08085840195417404 = 0.013832218945026398 + 0.01 * 6.70261812210083
Epoch 570, val loss: 0.9897385239601135
Epoch 580, training loss: 0.07993631064891815 = 0.01299291756004095 + 0.01 * 6.694339275360107
Epoch 580, val loss: 0.9988322854042053
Epoch 590, training loss: 0.07914651930332184 = 0.01223075669258833 + 0.01 * 6.691576957702637
Epoch 590, val loss: 1.007712483406067
Epoch 600, training loss: 0.07845640182495117 = 0.011537008918821812 + 0.01 * 6.691939353942871
Epoch 600, val loss: 1.0162608623504639
Epoch 610, training loss: 0.07775931060314178 = 0.01090395636856556 + 0.01 * 6.685535907745361
Epoch 610, val loss: 1.0246126651763916
Epoch 620, training loss: 0.07713056355714798 = 0.010324528440833092 + 0.01 * 6.680603504180908
Epoch 620, val loss: 1.03272545337677
Epoch 630, training loss: 0.07662131637334824 = 0.009793293662369251 + 0.01 * 6.682802200317383
Epoch 630, val loss: 1.0404456853866577
Epoch 640, training loss: 0.07601957768201828 = 0.009305067360401154 + 0.01 * 6.671451091766357
Epoch 640, val loss: 1.0480924844741821
Epoch 650, training loss: 0.07552685588598251 = 0.008854753337800503 + 0.01 * 6.667210578918457
Epoch 650, val loss: 1.0555318593978882
Epoch 660, training loss: 0.0751345157623291 = 0.008438820950686932 + 0.01 * 6.669569969177246
Epoch 660, val loss: 1.0627331733703613
Epoch 670, training loss: 0.07462199032306671 = 0.008053996600210667 + 0.01 * 6.65679931640625
Epoch 670, val loss: 1.0697182416915894
Epoch 680, training loss: 0.07423967868089676 = 0.007696845103055239 + 0.01 * 6.65428352355957
Epoch 680, val loss: 1.0765057802200317
Epoch 690, training loss: 0.07390538603067398 = 0.007365091238170862 + 0.01 * 6.654029846191406
Epoch 690, val loss: 1.0832058191299438
Epoch 700, training loss: 0.07351662218570709 = 0.0070565263740718365 + 0.01 * 6.64600944519043
Epoch 700, val loss: 1.0896952152252197
Epoch 710, training loss: 0.07323731482028961 = 0.0067689018324017525 + 0.01 * 6.646841526031494
Epoch 710, val loss: 1.0959631204605103
Epoch 720, training loss: 0.07282369583845139 = 0.006500422954559326 + 0.01 * 6.632327556610107
Epoch 720, val loss: 1.1021728515625
Epoch 730, training loss: 0.07268048822879791 = 0.006248821038752794 + 0.01 * 6.643166542053223
Epoch 730, val loss: 1.1080944538116455
Epoch 740, training loss: 0.07232233136892319 = 0.006013074424117804 + 0.01 * 6.630926132202148
Epoch 740, val loss: 1.1139405965805054
Epoch 750, training loss: 0.07201272994279861 = 0.005792086478322744 + 0.01 * 6.622064590454102
Epoch 750, val loss: 1.1197090148925781
Epoch 760, training loss: 0.07208847999572754 = 0.00558437779545784 + 0.01 * 6.650410175323486
Epoch 760, val loss: 1.1252808570861816
Epoch 770, training loss: 0.07153090834617615 = 0.005389586556702852 + 0.01 * 6.614132404327393
Epoch 770, val loss: 1.1307833194732666
Epoch 780, training loss: 0.071268729865551 = 0.005206011235713959 + 0.01 * 6.606271743774414
Epoch 780, val loss: 1.1360632181167603
Epoch 790, training loss: 0.07113976031541824 = 0.005032793618738651 + 0.01 * 6.610697269439697
Epoch 790, val loss: 1.1411885023117065
Epoch 800, training loss: 0.07082907110452652 = 0.004869363270699978 + 0.01 * 6.595970630645752
Epoch 800, val loss: 1.1463350057601929
Epoch 810, training loss: 0.07090733200311661 = 0.004715087357908487 + 0.01 * 6.619225025177002
Epoch 810, val loss: 1.1511321067810059
Epoch 820, training loss: 0.07042038440704346 = 0.004569712560623884 + 0.01 * 6.5850677490234375
Epoch 820, val loss: 1.1561630964279175
Epoch 830, training loss: 0.07036463171243668 = 0.004431924317032099 + 0.01 * 6.593270778656006
Epoch 830, val loss: 1.1606721878051758
Epoch 840, training loss: 0.07013863325119019 = 0.004301365464925766 + 0.01 * 6.5837273597717285
Epoch 840, val loss: 1.1654126644134521
Epoch 850, training loss: 0.07035613805055618 = 0.0041773319244384766 + 0.01 * 6.617880821228027
Epoch 850, val loss: 1.169816017150879
Epoch 860, training loss: 0.06974967569112778 = 0.004059745464473963 + 0.01 * 6.568993091583252
Epoch 860, val loss: 1.1742287874221802
Epoch 870, training loss: 0.06974084675312042 = 0.003948056139051914 + 0.01 * 6.579278945922852
Epoch 870, val loss: 1.1784279346466064
Epoch 880, training loss: 0.06966400146484375 = 0.0038419000338763 + 0.01 * 6.582210540771484
Epoch 880, val loss: 1.1826727390289307
Epoch 890, training loss: 0.0693657398223877 = 0.0037410398945212364 + 0.01 * 6.562469959259033
Epoch 890, val loss: 1.1868644952774048
Epoch 900, training loss: 0.0692436546087265 = 0.003644907847046852 + 0.01 * 6.559875011444092
Epoch 900, val loss: 1.1907551288604736
Epoch 910, training loss: 0.06906774640083313 = 0.003553004004061222 + 0.01 * 6.551474571228027
Epoch 910, val loss: 1.1947004795074463
Epoch 920, training loss: 0.06902775913476944 = 0.0034653390757739544 + 0.01 * 6.556241989135742
Epoch 920, val loss: 1.1984806060791016
Epoch 930, training loss: 0.06884126365184784 = 0.0033815340138971806 + 0.01 * 6.54597282409668
Epoch 930, val loss: 1.2022933959960938
Epoch 940, training loss: 0.06879878789186478 = 0.0033013855572789907 + 0.01 * 6.549740791320801
Epoch 940, val loss: 1.2058935165405273
Epoch 950, training loss: 0.06865544617176056 = 0.003224857384338975 + 0.01 * 6.5430588722229
Epoch 950, val loss: 1.209742546081543
Epoch 960, training loss: 0.06899485737085342 = 0.0031514891888946295 + 0.01 * 6.584336757659912
Epoch 960, val loss: 1.2129034996032715
Epoch 970, training loss: 0.06844117492437363 = 0.0030814402271062136 + 0.01 * 6.53597354888916
Epoch 970, val loss: 1.2165751457214355
Epoch 980, training loss: 0.0683489441871643 = 0.003014232497662306 + 0.01 * 6.533471584320068
Epoch 980, val loss: 1.2198472023010254
Epoch 990, training loss: 0.06820013374090195 = 0.0029497549403458834 + 0.01 * 6.525038242340088
Epoch 990, val loss: 1.223010540008545
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9373
Flip ASR: 0.9244/225 nodes
The final ASR:0.73186, 0.14532, Accuracy:0.81605, 0.00630
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11586])
remove edge: torch.Size([2, 9470])
updated graph: torch.Size([2, 10500])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98278, 0.00696, Accuracy:0.83210, 0.00462
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0258073806762695 = 1.9420689344406128 + 0.01 * 8.373846054077148
Epoch 0, val loss: 1.9447661638259888
Epoch 10, training loss: 2.0158495903015137 = 1.9321118593215942 + 0.01 * 8.373761177062988
Epoch 10, val loss: 1.9348022937774658
Epoch 20, training loss: 2.003540277481079 = 1.919805645942688 + 0.01 * 8.373461723327637
Epoch 20, val loss: 1.9223289489746094
Epoch 30, training loss: 1.9860634803771973 = 1.9023381471633911 + 0.01 * 8.372532844543457
Epoch 30, val loss: 1.9047735929489136
Epoch 40, training loss: 1.960105538368225 = 1.8764293193817139 + 0.01 * 8.3676176071167
Epoch 40, val loss: 1.879425048828125
Epoch 50, training loss: 1.9232902526855469 = 1.8399419784545898 + 0.01 * 8.33482837677002
Epoch 50, val loss: 1.8459432125091553
Epoch 60, training loss: 1.8798421621322632 = 1.7982159852981567 + 0.01 * 8.162614822387695
Epoch 60, val loss: 1.8121366500854492
Epoch 70, training loss: 1.8377035856246948 = 1.7591458559036255 + 0.01 * 7.85577392578125
Epoch 70, val loss: 1.781890869140625
Epoch 80, training loss: 1.7853288650512695 = 1.7089781761169434 + 0.01 * 7.635063648223877
Epoch 80, val loss: 1.738003134727478
Epoch 90, training loss: 1.7134578227996826 = 1.6391191482543945 + 0.01 * 7.433864116668701
Epoch 90, val loss: 1.6782313585281372
Epoch 100, training loss: 1.6211719512939453 = 1.548648476600647 + 0.01 * 7.252344131469727
Epoch 100, val loss: 1.6048964262008667
Epoch 110, training loss: 1.5186890363693237 = 1.447147250175476 + 0.01 * 7.154179573059082
Epoch 110, val loss: 1.5245020389556885
Epoch 120, training loss: 1.4165109395980835 = 1.3458963632583618 + 0.01 * 7.061460494995117
Epoch 120, val loss: 1.4450219869613647
Epoch 130, training loss: 1.3183859586715698 = 1.2487577199935913 + 0.01 * 6.962823867797852
Epoch 130, val loss: 1.370382308959961
Epoch 140, training loss: 1.2260547876358032 = 1.1571210622787476 + 0.01 * 6.893373489379883
Epoch 140, val loss: 1.3014757633209229
Epoch 150, training loss: 1.141649842262268 = 1.0731656551361084 + 0.01 * 6.848419189453125
Epoch 150, val loss: 1.2387751340866089
Epoch 160, training loss: 1.0667428970336914 = 0.9984055757522583 + 0.01 * 6.833733558654785
Epoch 160, val loss: 1.182956576347351
Epoch 170, training loss: 0.9996743202209473 = 0.9314896464347839 + 0.01 * 6.818470478057861
Epoch 170, val loss: 1.133611798286438
Epoch 180, training loss: 0.9368075132369995 = 0.8686890006065369 + 0.01 * 6.811853408813477
Epoch 180, val loss: 1.0876802206039429
Epoch 190, training loss: 0.8744112253189087 = 0.8063431978225708 + 0.01 * 6.806804656982422
Epoch 190, val loss: 1.042514681816101
Epoch 200, training loss: 0.8110986351966858 = 0.7430621385574341 + 0.01 * 6.803648471832275
Epoch 200, val loss: 0.9971318244934082
Epoch 210, training loss: 0.748016893863678 = 0.6800132989883423 + 0.01 * 6.80035924911499
Epoch 210, val loss: 0.9536301493644714
Epoch 220, training loss: 0.6875232458114624 = 0.6195583343505859 + 0.01 * 6.796492576599121
Epoch 220, val loss: 0.9153854250907898
Epoch 230, training loss: 0.632113516330719 = 0.5641974806785583 + 0.01 * 6.791601657867432
Epoch 230, val loss: 0.8854814171791077
Epoch 240, training loss: 0.5835116505622864 = 0.5156601667404175 + 0.01 * 6.785149574279785
Epoch 240, val loss: 0.8656466007232666
Epoch 250, training loss: 0.5418915748596191 = 0.47412243485450745 + 0.01 * 6.77691650390625
Epoch 250, val loss: 0.8551815152168274
Epoch 260, training loss: 0.5058009624481201 = 0.43812933564186096 + 0.01 * 6.767161846160889
Epoch 260, val loss: 0.8515006899833679
Epoch 270, training loss: 0.47310617566108704 = 0.40547388792037964 + 0.01 * 6.763227939605713
Epoch 270, val loss: 0.8517478704452515
Epoch 280, training loss: 0.4415743052959442 = 0.3740804195404053 + 0.01 * 6.749388694763184
Epoch 280, val loss: 0.8535551428794861
Epoch 290, training loss: 0.4099574685096741 = 0.3425619602203369 + 0.01 * 6.739551067352295
Epoch 290, val loss: 0.8556089997291565
Epoch 300, training loss: 0.3778528571128845 = 0.31053924560546875 + 0.01 * 6.7313618659973145
Epoch 300, val loss: 0.8578096032142639
Epoch 310, training loss: 0.3459411859512329 = 0.2786915600299835 + 0.01 * 6.724963188171387
Epoch 310, val loss: 0.860435426235199
Epoch 320, training loss: 0.315394788980484 = 0.24818331003189087 + 0.01 * 6.7211480140686035
Epoch 320, val loss: 0.8637396693229675
Epoch 330, training loss: 0.2869771718978882 = 0.21980980038642883 + 0.01 * 6.716736793518066
Epoch 330, val loss: 0.8677616715431213
Epoch 340, training loss: 0.26090413331985474 = 0.19377917051315308 + 0.01 * 6.712498188018799
Epoch 340, val loss: 0.8723966479301453
Epoch 350, training loss: 0.2371547520160675 = 0.17006130516529083 + 0.01 * 6.709345817565918
Epoch 350, val loss: 0.8776200413703918
Epoch 360, training loss: 0.21572744846343994 = 0.14866532385349274 + 0.01 * 6.706213474273682
Epoch 360, val loss: 0.8834781050682068
Epoch 370, training loss: 0.19672808051109314 = 0.12969228625297546 + 0.01 * 6.703578948974609
Epoch 370, val loss: 0.8903417587280273
Epoch 380, training loss: 0.1801580935716629 = 0.11316424608230591 + 0.01 * 6.699385166168213
Epoch 380, val loss: 0.8985293507575989
Epoch 390, training loss: 0.16591337323188782 = 0.09896359592676163 + 0.01 * 6.694977760314941
Epoch 390, val loss: 0.9080313444137573
Epoch 400, training loss: 0.1537887156009674 = 0.08686230331659317 + 0.01 * 6.69264030456543
Epoch 400, val loss: 0.9189480543136597
Epoch 410, training loss: 0.14342711865901947 = 0.0765753909945488 + 0.01 * 6.685173034667969
Epoch 410, val loss: 0.9310879111289978
Epoch 420, training loss: 0.13461679220199585 = 0.067813441157341 + 0.01 * 6.68033504486084
Epoch 420, val loss: 0.9442101716995239
Epoch 430, training loss: 0.1270829439163208 = 0.06032179296016693 + 0.01 * 6.6761155128479
Epoch 430, val loss: 0.9580548405647278
Epoch 440, training loss: 0.12059681117534637 = 0.05388437211513519 + 0.01 * 6.671244144439697
Epoch 440, val loss: 0.9724039435386658
Epoch 450, training loss: 0.11499205231666565 = 0.04831749573349953 + 0.01 * 6.667455673217773
Epoch 450, val loss: 0.9869421124458313
Epoch 460, training loss: 0.11010459065437317 = 0.04347405582666397 + 0.01 * 6.6630539894104
Epoch 460, val loss: 1.0015342235565186
Epoch 470, training loss: 0.10600381344556808 = 0.03923756629228592 + 0.01 * 6.676624774932861
Epoch 470, val loss: 1.0160456895828247
Epoch 480, training loss: 0.1021365225315094 = 0.03552211448550224 + 0.01 * 6.661441326141357
Epoch 480, val loss: 1.0304368734359741
Epoch 490, training loss: 0.09878649562597275 = 0.03225041180849075 + 0.01 * 6.653608322143555
Epoch 490, val loss: 1.0446141958236694
Epoch 500, training loss: 0.09587427973747253 = 0.029360899701714516 + 0.01 * 6.651338577270508
Epoch 500, val loss: 1.0585857629776
Epoch 510, training loss: 0.09328930824995041 = 0.026804260909557343 + 0.01 * 6.648505210876465
Epoch 510, val loss: 1.0722850561141968
Epoch 520, training loss: 0.09100344777107239 = 0.024539615958929062 + 0.01 * 6.646383285522461
Epoch 520, val loss: 1.0857256650924683
Epoch 530, training loss: 0.08898524940013885 = 0.02253120206296444 + 0.01 * 6.64540433883667
Epoch 530, val loss: 1.0988519191741943
Epoch 540, training loss: 0.08716880530118942 = 0.020745301619172096 + 0.01 * 6.642350673675537
Epoch 540, val loss: 1.1117080450057983
Epoch 550, training loss: 0.08555310219526291 = 0.01915392465889454 + 0.01 * 6.639918327331543
Epoch 550, val loss: 1.1242667436599731
Epoch 560, training loss: 0.08411280065774918 = 0.01773272454738617 + 0.01 * 6.638007640838623
Epoch 560, val loss: 1.1365255117416382
Epoch 570, training loss: 0.08283604681491852 = 0.01646057330071926 + 0.01 * 6.637547492980957
Epoch 570, val loss: 1.1484801769256592
Epoch 580, training loss: 0.08164176344871521 = 0.015318882651627064 + 0.01 * 6.632287979125977
Epoch 580, val loss: 1.160103678703308
Epoch 590, training loss: 0.0806538313627243 = 0.014291047118604183 + 0.01 * 6.6362786293029785
Epoch 590, val loss: 1.1714529991149902
Epoch 600, training loss: 0.07969598472118378 = 0.013364513404667377 + 0.01 * 6.6331467628479
Epoch 600, val loss: 1.182427167892456
Epoch 610, training loss: 0.07879190146923065 = 0.012526554055511951 + 0.01 * 6.626534938812256
Epoch 610, val loss: 1.1931251287460327
Epoch 620, training loss: 0.07801125198602676 = 0.01176624745130539 + 0.01 * 6.624500274658203
Epoch 620, val loss: 1.2035367488861084
Epoch 630, training loss: 0.07730057835578918 = 0.011074461974203587 + 0.01 * 6.6226115226745605
Epoch 630, val loss: 1.2136954069137573
Epoch 640, training loss: 0.07675112783908844 = 0.010443723760545254 + 0.01 * 6.630740642547607
Epoch 640, val loss: 1.2235413789749146
Epoch 650, training loss: 0.07605952024459839 = 0.00986802950501442 + 0.01 * 6.619149208068848
Epoch 650, val loss: 1.2331507205963135
Epoch 660, training loss: 0.07550858706235886 = 0.009340661577880383 + 0.01 * 6.616792678833008
Epoch 660, val loss: 1.2424979209899902
Epoch 670, training loss: 0.07499714195728302 = 0.008856290951371193 + 0.01 * 6.6140851974487305
Epoch 670, val loss: 1.2515861988067627
Epoch 680, training loss: 0.07454684376716614 = 0.008410375565290451 + 0.01 * 6.613646984100342
Epoch 680, val loss: 1.2604246139526367
Epoch 690, training loss: 0.07410277426242828 = 0.007999354042112827 + 0.01 * 6.610342025756836
Epoch 690, val loss: 1.269051432609558
Epoch 700, training loss: 0.07369555532932281 = 0.007619849871844053 + 0.01 * 6.607570648193359
Epoch 700, val loss: 1.2774443626403809
Epoch 710, training loss: 0.07332933694124222 = 0.007268539629876614 + 0.01 * 6.606079578399658
Epoch 710, val loss: 1.2856148481369019
Epoch 720, training loss: 0.07304732501506805 = 0.006942647509276867 + 0.01 * 6.610467910766602
Epoch 720, val loss: 1.2935764789581299
Epoch 730, training loss: 0.07265310734510422 = 0.006639843340963125 + 0.01 * 6.601325988769531
Epoch 730, val loss: 1.3013468980789185
Epoch 740, training loss: 0.07244809716939926 = 0.006357991602271795 + 0.01 * 6.609010696411133
Epoch 740, val loss: 1.3088788986206055
Epoch 750, training loss: 0.0721326395869255 = 0.006095713935792446 + 0.01 * 6.603692531585693
Epoch 750, val loss: 1.3163022994995117
Epoch 760, training loss: 0.07180721312761307 = 0.005850998684763908 + 0.01 * 6.595621109008789
Epoch 760, val loss: 1.3234832286834717
Epoch 770, training loss: 0.07155483961105347 = 0.005622152704745531 + 0.01 * 6.593268394470215
Epoch 770, val loss: 1.330504298210144
Epoch 780, training loss: 0.07133308053016663 = 0.005407703574746847 + 0.01 * 6.5925374031066895
Epoch 780, val loss: 1.3373606204986572
Epoch 790, training loss: 0.07113521546125412 = 0.0052065919153392315 + 0.01 * 6.592862129211426
Epoch 790, val loss: 1.3440437316894531
Epoch 800, training loss: 0.07088708132505417 = 0.005017802119255066 + 0.01 * 6.586928367614746
Epoch 800, val loss: 1.3506195545196533
Epoch 810, training loss: 0.07079438865184784 = 0.0048403372056782246 + 0.01 * 6.595405578613281
Epoch 810, val loss: 1.3569921255111694
Epoch 820, training loss: 0.0705181434750557 = 0.004673480521887541 + 0.01 * 6.584466457366943
Epoch 820, val loss: 1.3631423711776733
Epoch 830, training loss: 0.07032561302185059 = 0.004516315646469593 + 0.01 * 6.580929279327393
Epoch 830, val loss: 1.3692691326141357
Epoch 840, training loss: 0.07015374302864075 = 0.004367982503026724 + 0.01 * 6.57857608795166
Epoch 840, val loss: 1.3752223253250122
Epoch 850, training loss: 0.0700492262840271 = 0.004227798897773027 + 0.01 * 6.5821428298950195
Epoch 850, val loss: 1.38102126121521
Epoch 860, training loss: 0.06986778229475021 = 0.0040952470153570175 + 0.01 * 6.577253818511963
Epoch 860, val loss: 1.3867249488830566
Epoch 870, training loss: 0.069683738052845 = 0.0039697494357824326 + 0.01 * 6.571398735046387
Epoch 870, val loss: 1.3922772407531738
Epoch 880, training loss: 0.06970434635877609 = 0.003850862616673112 + 0.01 * 6.585348606109619
Epoch 880, val loss: 1.3976439237594604
Epoch 890, training loss: 0.06944520026445389 = 0.003738380502909422 + 0.01 * 6.570682048797607
Epoch 890, val loss: 1.4029732942581177
Epoch 900, training loss: 0.06928850710391998 = 0.003631646977737546 + 0.01 * 6.565686225891113
Epoch 900, val loss: 1.408200740814209
Epoch 910, training loss: 0.06916779279708862 = 0.003530174493789673 + 0.01 * 6.563762187957764
Epoch 910, val loss: 1.4132872819900513
Epoch 920, training loss: 0.06911373883485794 = 0.0034336315002292395 + 0.01 * 6.568011283874512
Epoch 920, val loss: 1.4182339906692505
Epoch 930, training loss: 0.06893235445022583 = 0.003341683652251959 + 0.01 * 6.559067249298096
Epoch 930, val loss: 1.423109531402588
Epoch 940, training loss: 0.06883338838815689 = 0.0032540929969400167 + 0.01 * 6.557929992675781
Epoch 940, val loss: 1.4278877973556519
Epoch 950, training loss: 0.06873882561922073 = 0.0031706213485449553 + 0.01 * 6.556819915771484
Epoch 950, val loss: 1.4324475526809692
Epoch 960, training loss: 0.06862761825323105 = 0.0030910884961485863 + 0.01 * 6.553652763366699
Epoch 960, val loss: 1.437076449394226
Epoch 970, training loss: 0.06854160875082016 = 0.003015191527083516 + 0.01 * 6.552642345428467
Epoch 970, val loss: 1.4415525197982788
Epoch 980, training loss: 0.06849909573793411 = 0.0029426864348351955 + 0.01 * 6.555641174316406
Epoch 980, val loss: 1.4458098411560059
Epoch 990, training loss: 0.06832170486450195 = 0.0028734824154525995 + 0.01 * 6.544822692871094
Epoch 990, val loss: 1.450109839439392
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7407
Overall ASR: 0.4244
Flip ASR: 0.3244/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0460736751556396 = 1.962335467338562 + 0.01 * 8.373826026916504
Epoch 0, val loss: 1.9592572450637817
Epoch 10, training loss: 2.035454034805298 = 1.9517165422439575 + 0.01 * 8.373757362365723
Epoch 10, val loss: 1.9481689929962158
Epoch 20, training loss: 2.022861957550049 = 1.939126968383789 + 0.01 * 8.373493194580078
Epoch 20, val loss: 1.9349335432052612
Epoch 30, training loss: 2.005566358566284 = 1.9218382835388184 + 0.01 * 8.372804641723633
Epoch 30, val loss: 1.9167505502700806
Epoch 40, training loss: 1.9802958965301514 = 1.8965941667556763 + 0.01 * 8.370170593261719
Epoch 40, val loss: 1.8905056715011597
Epoch 50, training loss: 1.9436135292053223 = 1.8600746393203735 + 0.01 * 8.353886604309082
Epoch 50, val loss: 1.8536416292190552
Epoch 60, training loss: 1.8944330215454102 = 1.8119142055511475 + 0.01 * 8.251887321472168
Epoch 60, val loss: 1.8075346946716309
Epoch 70, training loss: 1.8366385698318481 = 1.758683681488037 + 0.01 * 7.795490264892578
Epoch 70, val loss: 1.7598333358764648
Epoch 80, training loss: 1.775457739830017 = 1.7010525465011597 + 0.01 * 7.440518379211426
Epoch 80, val loss: 1.7079248428344727
Epoch 90, training loss: 1.6992610692977905 = 1.6262047290802002 + 0.01 * 7.305630207061768
Epoch 90, val loss: 1.6406830549240112
Epoch 100, training loss: 1.6034295558929443 = 1.5309969186782837 + 0.01 * 7.243266582489014
Epoch 100, val loss: 1.5591739416122437
Epoch 110, training loss: 1.4954564571380615 = 1.4236960411071777 + 0.01 * 7.1760454177856445
Epoch 110, val loss: 1.4711737632751465
Epoch 120, training loss: 1.389502763748169 = 1.3182424306869507 + 0.01 * 7.126030921936035
Epoch 120, val loss: 1.3893095254898071
Epoch 130, training loss: 1.2914234399795532 = 1.2207022905349731 + 0.01 * 7.072118759155273
Epoch 130, val loss: 1.3182255029678345
Epoch 140, training loss: 1.198887825012207 = 1.128678798675537 + 0.01 * 7.020901203155518
Epoch 140, val loss: 1.253345251083374
Epoch 150, training loss: 1.1070947647094727 = 1.0373966693878174 + 0.01 * 6.969812870025635
Epoch 150, val loss: 1.190422534942627
Epoch 160, training loss: 1.014613151550293 = 0.9454537034034729 + 0.01 * 6.9159440994262695
Epoch 160, val loss: 1.1273159980773926
Epoch 170, training loss: 0.9251678586006165 = 0.8564774394035339 + 0.01 * 6.869039535522461
Epoch 170, val loss: 1.066583514213562
Epoch 180, training loss: 0.8441336154937744 = 0.7756970524787903 + 0.01 * 6.843657970428467
Epoch 180, val loss: 1.0123403072357178
Epoch 190, training loss: 0.7739643454551697 = 0.7056941390037537 + 0.01 * 6.827023029327393
Epoch 190, val loss: 0.9670909643173218
Epoch 200, training loss: 0.7135155200958252 = 0.6453646421432495 + 0.01 * 6.815088748931885
Epoch 200, val loss: 0.9303156733512878
Epoch 210, training loss: 0.6599642038345337 = 0.5919091105461121 + 0.01 * 6.8055100440979
Epoch 210, val loss: 0.9000337719917297
Epoch 220, training loss: 0.6110341548919678 = 0.543067216873169 + 0.01 * 6.796696186065674
Epoch 220, val loss: 0.8752403259277344
Epoch 230, training loss: 0.5657066702842712 = 0.49781686067581177 + 0.01 * 6.7889814376831055
Epoch 230, val loss: 0.855855405330658
Epoch 240, training loss: 0.5237307548522949 = 0.45592474937438965 + 0.01 * 6.7806010246276855
Epoch 240, val loss: 0.8421538472175598
Epoch 250, training loss: 0.4851025938987732 = 0.417374849319458 + 0.01 * 6.772773742675781
Epoch 250, val loss: 0.8336359262466431
Epoch 260, training loss: 0.4496893882751465 = 0.38204634189605713 + 0.01 * 6.764305114746094
Epoch 260, val loss: 0.8296105861663818
Epoch 270, training loss: 0.4172590672969818 = 0.3496989905834198 + 0.01 * 6.756006717681885
Epoch 270, val loss: 0.8293349742889404
Epoch 280, training loss: 0.3873842656612396 = 0.3198958933353424 + 0.01 * 6.748837471008301
Epoch 280, val loss: 0.8320215344429016
Epoch 290, training loss: 0.3595938980579376 = 0.2921779453754425 + 0.01 * 6.741596221923828
Epoch 290, val loss: 0.8367387652397156
Epoch 300, training loss: 0.3335672914981842 = 0.2662234902381897 + 0.01 * 6.734381198883057
Epoch 300, val loss: 0.8430978059768677
Epoch 310, training loss: 0.3091447353363037 = 0.2418307065963745 + 0.01 * 6.731403350830078
Epoch 310, val loss: 0.8504347801208496
Epoch 320, training loss: 0.2861478626728058 = 0.21891653537750244 + 0.01 * 6.723133563995361
Epoch 320, val loss: 0.8586487770080566
Epoch 330, training loss: 0.26462239027023315 = 0.19743499159812927 + 0.01 * 6.718738555908203
Epoch 330, val loss: 0.8674503564834595
Epoch 340, training loss: 0.24453619122505188 = 0.17737527191638947 + 0.01 * 6.716092586517334
Epoch 340, val loss: 0.8767247796058655
Epoch 350, training loss: 0.2258010059595108 = 0.15869282186031342 + 0.01 * 6.710818290710449
Epoch 350, val loss: 0.8861003518104553
Epoch 360, training loss: 0.20844405889511108 = 0.14136111736297607 + 0.01 * 6.708293437957764
Epoch 360, val loss: 0.8958649635314941
Epoch 370, training loss: 0.1926179826259613 = 0.125558540225029 + 0.01 * 6.705944061279297
Epoch 370, val loss: 0.9058085083961487
Epoch 380, training loss: 0.17862001061439514 = 0.11155835539102554 + 0.01 * 6.706165313720703
Epoch 380, val loss: 0.9161357283592224
Epoch 390, training loss: 0.1663074493408203 = 0.09928115457296371 + 0.01 * 6.702630043029785
Epoch 390, val loss: 0.9274892210960388
Epoch 400, training loss: 0.15544354915618896 = 0.08844827860593796 + 0.01 * 6.699527740478516
Epoch 400, val loss: 0.939741849899292
Epoch 410, training loss: 0.1458943784236908 = 0.07891816645860672 + 0.01 * 6.6976213455200195
Epoch 410, val loss: 0.9523102045059204
Epoch 420, training loss: 0.13751116394996643 = 0.07054585963487625 + 0.01 * 6.696530818939209
Epoch 420, val loss: 0.9649905562400818
Epoch 430, training loss: 0.13016225397586823 = 0.06319426745176315 + 0.01 * 6.696798801422119
Epoch 430, val loss: 0.9776463508605957
Epoch 440, training loss: 0.12368021160364151 = 0.05675281584262848 + 0.01 * 6.692739963531494
Epoch 440, val loss: 0.9901923537254333
Epoch 450, training loss: 0.11801102757453918 = 0.051110874861478806 + 0.01 * 6.690014839172363
Epoch 450, val loss: 1.0026146173477173
Epoch 460, training loss: 0.11305062472820282 = 0.0461733341217041 + 0.01 * 6.687729358673096
Epoch 460, val loss: 1.0149455070495605
Epoch 470, training loss: 0.10870921611785889 = 0.0418529212474823 + 0.01 * 6.685629844665527
Epoch 470, val loss: 1.02717125415802
Epoch 480, training loss: 0.10489800572395325 = 0.038062211126089096 + 0.01 * 6.683579921722412
Epoch 480, val loss: 1.0391474962234497
Epoch 490, training loss: 0.10154882073402405 = 0.03471815586090088 + 0.01 * 6.683066368103027
Epoch 490, val loss: 1.0510016679763794
Epoch 500, training loss: 0.09856773912906647 = 0.031757574528455734 + 0.01 * 6.681015968322754
Epoch 500, val loss: 1.0625991821289062
Epoch 510, training loss: 0.09591010212898254 = 0.029131010174751282 + 0.01 * 6.6779093742370605
Epoch 510, val loss: 1.0739344358444214
Epoch 520, training loss: 0.09354635328054428 = 0.02679475210607052 + 0.01 * 6.6751604080200195
Epoch 520, val loss: 1.0849592685699463
Epoch 530, training loss: 0.09147024154663086 = 0.024712033569812775 + 0.01 * 6.675820827484131
Epoch 530, val loss: 1.095699429512024
Epoch 540, training loss: 0.08956864476203918 = 0.02285158261656761 + 0.01 * 6.671706676483154
Epoch 540, val loss: 1.1062204837799072
Epoch 550, training loss: 0.0878739058971405 = 0.02118498459458351 + 0.01 * 6.6688923835754395
Epoch 550, val loss: 1.1164113283157349
Epoch 560, training loss: 0.08636173605918884 = 0.019687866792082787 + 0.01 * 6.667387008666992
Epoch 560, val loss: 1.1263190507888794
Epoch 570, training loss: 0.08497941493988037 = 0.018339689821004868 + 0.01 * 6.6639723777771
Epoch 570, val loss: 1.1360177993774414
Epoch 580, training loss: 0.08375181257724762 = 0.01712152548134327 + 0.01 * 6.663028717041016
Epoch 580, val loss: 1.1454155445098877
Epoch 590, training loss: 0.08273553103208542 = 0.016019107773900032 + 0.01 * 6.671642303466797
Epoch 590, val loss: 1.1545277833938599
Epoch 600, training loss: 0.0816284716129303 = 0.015020657330751419 + 0.01 * 6.660780906677246
Epoch 600, val loss: 1.163339376449585
Epoch 610, training loss: 0.0806698128581047 = 0.01411239244043827 + 0.01 * 6.655742645263672
Epoch 610, val loss: 1.1720104217529297
Epoch 620, training loss: 0.07981488108634949 = 0.013283107429742813 + 0.01 * 6.653177261352539
Epoch 620, val loss: 1.1805137395858765
Epoch 630, training loss: 0.07903188467025757 = 0.012525085359811783 + 0.01 * 6.650680065155029
Epoch 630, val loss: 1.1887004375457764
Epoch 640, training loss: 0.07834266126155853 = 0.011830838397145271 + 0.01 * 6.651182651519775
Epoch 640, val loss: 1.1967123746871948
Epoch 650, training loss: 0.07770617306232452 = 0.011194254271686077 + 0.01 * 6.6511921882629395
Epoch 650, val loss: 1.204522967338562
Epoch 660, training loss: 0.07707176357507706 = 0.010609780438244343 + 0.01 * 6.646198272705078
Epoch 660, val loss: 1.2120875120162964
Epoch 670, training loss: 0.07649704068899155 = 0.01007150113582611 + 0.01 * 6.64255428314209
Epoch 670, val loss: 1.2195196151733398
Epoch 680, training loss: 0.07596967369318008 = 0.009574329480528831 + 0.01 * 6.6395344734191895
Epoch 680, val loss: 1.2267160415649414
Epoch 690, training loss: 0.07551391422748566 = 0.009113986045122147 + 0.01 * 6.639992713928223
Epoch 690, val loss: 1.2337092161178589
Epoch 700, training loss: 0.07511856406927109 = 0.008687244728207588 + 0.01 * 6.643132209777832
Epoch 700, val loss: 1.2405462265014648
Epoch 710, training loss: 0.07463361322879791 = 0.008291242644190788 + 0.01 * 6.634237289428711
Epoch 710, val loss: 1.2471957206726074
Epoch 720, training loss: 0.07423115521669388 = 0.007922003977000713 + 0.01 * 6.630915641784668
Epoch 720, val loss: 1.2536778450012207
Epoch 730, training loss: 0.07400423288345337 = 0.007578291930258274 + 0.01 * 6.642594337463379
Epoch 730, val loss: 1.2600232362747192
Epoch 740, training loss: 0.07353344559669495 = 0.007258928380906582 + 0.01 * 6.6274518966674805
Epoch 740, val loss: 1.2662307024002075
Epoch 750, training loss: 0.07319464534521103 = 0.00696034450083971 + 0.01 * 6.623429775238037
Epoch 750, val loss: 1.2722041606903076
Epoch 760, training loss: 0.07288563996553421 = 0.006681295111775398 + 0.01 * 6.620434761047363
Epoch 760, val loss: 1.2781187295913696
Epoch 770, training loss: 0.07289019227027893 = 0.006420043297111988 + 0.01 * 6.64701509475708
Epoch 770, val loss: 1.2837638854980469
Epoch 780, training loss: 0.07241702824831009 = 0.0061751678586006165 + 0.01 * 6.624186038970947
Epoch 780, val loss: 1.2894151210784912
Epoch 790, training loss: 0.07210603356361389 = 0.005945202428847551 + 0.01 * 6.616083145141602
Epoch 790, val loss: 1.2948073148727417
Epoch 800, training loss: 0.07184749096632004 = 0.005728972610086203 + 0.01 * 6.611851692199707
Epoch 800, val loss: 1.3001502752304077
Epoch 810, training loss: 0.07160844653844833 = 0.005525107961148024 + 0.01 * 6.608334541320801
Epoch 810, val loss: 1.3053789138793945
Epoch 820, training loss: 0.07157678157091141 = 0.0053330776281654835 + 0.01 * 6.624370098114014
Epoch 820, val loss: 1.3104420900344849
Epoch 830, training loss: 0.07125276327133179 = 0.0051522040739655495 + 0.01 * 6.610056400299072
Epoch 830, val loss: 1.3153146505355835
Epoch 840, training loss: 0.07101145386695862 = 0.0049814581871032715 + 0.01 * 6.602999687194824
Epoch 840, val loss: 1.3201719522476196
Epoch 850, training loss: 0.07081367820501328 = 0.00482005113735795 + 0.01 * 6.599363327026367
Epoch 850, val loss: 1.3248848915100098
Epoch 860, training loss: 0.07064420729875565 = 0.0046671596355736256 + 0.01 * 6.597704887390137
Epoch 860, val loss: 1.329493761062622
Epoch 870, training loss: 0.07046347111463547 = 0.004522404167801142 + 0.01 * 6.594106674194336
Epoch 870, val loss: 1.3339630365371704
Epoch 880, training loss: 0.07035819441080093 = 0.004385144449770451 + 0.01 * 6.5973052978515625
Epoch 880, val loss: 1.3384039402008057
Epoch 890, training loss: 0.07014545053243637 = 0.00425484636798501 + 0.01 * 6.589060306549072
Epoch 890, val loss: 1.342753529548645
Epoch 900, training loss: 0.07001173496246338 = 0.004130893852561712 + 0.01 * 6.588084697723389
Epoch 900, val loss: 1.3469946384429932
Epoch 910, training loss: 0.06987594813108444 = 0.004013547673821449 + 0.01 * 6.586240291595459
Epoch 910, val loss: 1.3510385751724243
Epoch 920, training loss: 0.06972475349903107 = 0.003901643678545952 + 0.01 * 6.582310676574707
Epoch 920, val loss: 1.3551673889160156
Epoch 930, training loss: 0.06959553062915802 = 0.0037953509017825127 + 0.01 * 6.580018520355225
Epoch 930, val loss: 1.359124779701233
Epoch 940, training loss: 0.069633848965168 = 0.003694038139656186 + 0.01 * 6.593981742858887
Epoch 940, val loss: 1.3629523515701294
Epoch 950, training loss: 0.06937181204557419 = 0.0035974744241684675 + 0.01 * 6.577434062957764
Epoch 950, val loss: 1.3667124509811401
Epoch 960, training loss: 0.06925307959318161 = 0.0035054469481110573 + 0.01 * 6.574763774871826
Epoch 960, val loss: 1.3703031539916992
Epoch 970, training loss: 0.06911428272724152 = 0.003417553612962365 + 0.01 * 6.569673538208008
Epoch 970, val loss: 1.3739361763000488
Epoch 980, training loss: 0.06905430555343628 = 0.0033333071041852236 + 0.01 * 6.5721001625061035
Epoch 980, val loss: 1.377506971359253
Epoch 990, training loss: 0.06898603588342667 = 0.0032531742472201586 + 0.01 * 6.573286056518555
Epoch 990, val loss: 1.3808670043945312
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7815
Overall ASR: 0.7601
Flip ASR: 0.7200/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0337116718292236 = 1.9499731063842773 + 0.01 * 8.373847007751465
Epoch 0, val loss: 1.9493834972381592
Epoch 10, training loss: 2.0240836143493652 = 1.9403458833694458 + 0.01 * 8.373783111572266
Epoch 10, val loss: 1.9403765201568604
Epoch 20, training loss: 2.012517213821411 = 1.9287819862365723 + 0.01 * 8.37353229522705
Epoch 20, val loss: 1.9292645454406738
Epoch 30, training loss: 1.9966932535171509 = 1.912964940071106 + 0.01 * 8.37283706665039
Epoch 30, val loss: 1.9138739109039307
Epoch 40, training loss: 1.9735113382339478 = 1.8898143768310547 + 0.01 * 8.369695663452148
Epoch 40, val loss: 1.8914188146591187
Epoch 50, training loss: 1.9397329092025757 = 1.8562626838684082 + 0.01 * 8.347024917602539
Epoch 50, val loss: 1.859857201576233
Epoch 60, training loss: 1.8954901695251465 = 1.8133314847946167 + 0.01 * 8.215867042541504
Epoch 60, val loss: 1.822070598602295
Epoch 70, training loss: 1.8499869108200073 = 1.770365595817566 + 0.01 * 7.962128639221191
Epoch 70, val loss: 1.785964846611023
Epoch 80, training loss: 1.8007888793945312 = 1.7252380847930908 + 0.01 * 7.555074214935303
Epoch 80, val loss: 1.743945837020874
Epoch 90, training loss: 1.7351733446121216 = 1.6632107496261597 + 0.01 * 7.196258544921875
Epoch 90, val loss: 1.6881933212280273
Epoch 100, training loss: 1.6504768133163452 = 1.5812230110168457 + 0.01 * 6.925385475158691
Epoch 100, val loss: 1.6204479932785034
Epoch 110, training loss: 1.5517079830169678 = 1.482935905456543 + 0.01 * 6.877206325531006
Epoch 110, val loss: 1.5400526523590088
Epoch 120, training loss: 1.4479501247406006 = 1.3794307708740234 + 0.01 * 6.851930141448975
Epoch 120, val loss: 1.454967975616455
Epoch 130, training loss: 1.3457483053207397 = 1.2773643732070923 + 0.01 * 6.838391304016113
Epoch 130, val loss: 1.3729826211929321
Epoch 140, training loss: 1.2468435764312744 = 1.1785250902175903 + 0.01 * 6.831854343414307
Epoch 140, val loss: 1.2954221963882446
Epoch 150, training loss: 1.1508697271347046 = 1.0826010704040527 + 0.01 * 6.826868534088135
Epoch 150, val loss: 1.2216211557388306
Epoch 160, training loss: 1.05723237991333 = 0.9890076518058777 + 0.01 * 6.82247257232666
Epoch 160, val loss: 1.1496440172195435
Epoch 170, training loss: 0.965808629989624 = 0.897622287273407 + 0.01 * 6.818631172180176
Epoch 170, val loss: 1.0794658660888672
Epoch 180, training loss: 0.8773592114448547 = 0.8092135787010193 + 0.01 * 6.814563751220703
Epoch 180, val loss: 1.0111021995544434
Epoch 190, training loss: 0.7932853698730469 = 0.7251911163330078 + 0.01 * 6.809422492980957
Epoch 190, val loss: 0.9464481472969055
Epoch 200, training loss: 0.7156915664672852 = 0.6476515531539917 + 0.01 * 6.80400276184082
Epoch 200, val loss: 0.8881463408470154
Epoch 210, training loss: 0.6463712453842163 = 0.5784013867378235 + 0.01 * 6.7969865798950195
Epoch 210, val loss: 0.8388446569442749
Epoch 220, training loss: 0.5855020880699158 = 0.5176131725311279 + 0.01 * 6.788889408111572
Epoch 220, val loss: 0.7994547486305237
Epoch 230, training loss: 0.5315390229225159 = 0.46374478936195374 + 0.01 * 6.779421806335449
Epoch 230, val loss: 0.7683462500572205
Epoch 240, training loss: 0.48219138383865356 = 0.4145081341266632 + 0.01 * 6.768324851989746
Epoch 240, val loss: 0.7430112957954407
Epoch 250, training loss: 0.43533775210380554 = 0.36773401498794556 + 0.01 * 6.760373592376709
Epoch 250, val loss: 0.7212801575660706
Epoch 260, training loss: 0.3898470103740692 = 0.32235175371170044 + 0.01 * 6.74952507019043
Epoch 260, val loss: 0.7024776935577393
Epoch 270, training loss: 0.3460678160190582 = 0.27864930033683777 + 0.01 * 6.741852760314941
Epoch 270, val loss: 0.6869528889656067
Epoch 280, training loss: 0.30545181035995483 = 0.23809581995010376 + 0.01 * 6.735598564147949
Epoch 280, val loss: 0.6755361557006836
Epoch 290, training loss: 0.2696259021759033 = 0.20232005417346954 + 0.01 * 6.730584621429443
Epoch 290, val loss: 0.6690289974212646
Epoch 300, training loss: 0.2394130527973175 = 0.17215487360954285 + 0.01 * 6.725819110870361
Epoch 300, val loss: 0.6672425270080566
Epoch 310, training loss: 0.2146817445755005 = 0.14744848012924194 + 0.01 * 6.72332763671875
Epoch 310, val loss: 0.6698150634765625
Epoch 320, training loss: 0.19457367062568665 = 0.12737584114074707 + 0.01 * 6.719783782958984
Epoch 320, val loss: 0.6758837699890137
Epoch 330, training loss: 0.17807993292808533 = 0.11092931032180786 + 0.01 * 6.715063095092773
Epoch 330, val loss: 0.6844803690910339
Epoch 340, training loss: 0.16449657082557678 = 0.09727165848016739 + 0.01 * 6.7224907875061035
Epoch 340, val loss: 0.6948214769363403
Epoch 350, training loss: 0.1529146432876587 = 0.08577292412519455 + 0.01 * 6.714172840118408
Epoch 350, val loss: 0.7064120173454285
Epoch 360, training loss: 0.1430734544992447 = 0.07597202807664871 + 0.01 * 6.710143089294434
Epoch 360, val loss: 0.7188633680343628
Epoch 370, training loss: 0.13459226489067078 = 0.06754281371831894 + 0.01 * 6.704945087432861
Epoch 370, val loss: 0.7319717407226562
Epoch 380, training loss: 0.12727423012256622 = 0.06024527549743652 + 0.01 * 6.702895641326904
Epoch 380, val loss: 0.7454620599746704
Epoch 390, training loss: 0.12091083824634552 = 0.05390280485153198 + 0.01 * 6.700803279876709
Epoch 390, val loss: 0.7592092156410217
Epoch 400, training loss: 0.1154426634311676 = 0.0483822338283062 + 0.01 * 6.706043243408203
Epoch 400, val loss: 0.7731686234474182
Epoch 410, training loss: 0.11051498353481293 = 0.043574657291173935 + 0.01 * 6.694032669067383
Epoch 410, val loss: 0.7871014475822449
Epoch 420, training loss: 0.10628368705511093 = 0.03938119113445282 + 0.01 * 6.690249919891357
Epoch 420, val loss: 0.80108642578125
Epoch 430, training loss: 0.10263058543205261 = 0.03571970760822296 + 0.01 * 6.69108772277832
Epoch 430, val loss: 0.8148884177207947
Epoch 440, training loss: 0.09932802617549896 = 0.03251652792096138 + 0.01 * 6.681149959564209
Epoch 440, val loss: 0.8285447955131531
Epoch 450, training loss: 0.09647439420223236 = 0.029705410823225975 + 0.01 * 6.67689847946167
Epoch 450, val loss: 0.8419314622879028
Epoch 460, training loss: 0.09394238144159317 = 0.027230121195316315 + 0.01 * 6.6712260246276855
Epoch 460, val loss: 0.8550201654434204
Epoch 470, training loss: 0.09182478487491608 = 0.025045650079846382 + 0.01 * 6.677913665771484
Epoch 470, val loss: 0.8677160739898682
Epoch 480, training loss: 0.08976463973522186 = 0.023111792281270027 + 0.01 * 6.665285110473633
Epoch 480, val loss: 0.8800773024559021
Epoch 490, training loss: 0.08804676681756973 = 0.021390510722994804 + 0.01 * 6.66562557220459
Epoch 490, val loss: 0.892037570476532
Epoch 500, training loss: 0.08637967705726624 = 0.01985256001353264 + 0.01 * 6.652711868286133
Epoch 500, val loss: 0.903690755367279
Epoch 510, training loss: 0.0849558636546135 = 0.01847209967672825 + 0.01 * 6.648376941680908
Epoch 510, val loss: 0.914959728717804
Epoch 520, training loss: 0.08370260149240494 = 0.017230214551091194 + 0.01 * 6.6472392082214355
Epoch 520, val loss: 0.9259178042411804
Epoch 530, training loss: 0.08248253166675568 = 0.01610814593732357 + 0.01 * 6.6374382972717285
Epoch 530, val loss: 0.9365175366401672
Epoch 540, training loss: 0.08145375549793243 = 0.015090874396264553 + 0.01 * 6.636288166046143
Epoch 540, val loss: 0.9469190239906311
Epoch 550, training loss: 0.08044600486755371 = 0.014166572131216526 + 0.01 * 6.627943515777588
Epoch 550, val loss: 0.9570323824882507
Epoch 560, training loss: 0.07955455034971237 = 0.013324852101504803 + 0.01 * 6.6229705810546875
Epoch 560, val loss: 0.9668049216270447
Epoch 570, training loss: 0.07919652760028839 = 0.012555884197354317 + 0.01 * 6.664064884185791
Epoch 570, val loss: 0.9764037132263184
Epoch 580, training loss: 0.07811824977397919 = 0.011852819472551346 + 0.01 * 6.6265435218811035
Epoch 580, val loss: 0.9857333898544312
Epoch 590, training loss: 0.07733019441366196 = 0.01120747346431017 + 0.01 * 6.612271785736084
Epoch 590, val loss: 0.9947077035903931
Epoch 600, training loss: 0.07668683677911758 = 0.010613100603222847 + 0.01 * 6.6073737144470215
Epoch 600, val loss: 1.0035711526870728
Epoch 610, training loss: 0.07621525973081589 = 0.010066180489957333 + 0.01 * 6.614908218383789
Epoch 610, val loss: 1.0120729207992554
Epoch 620, training loss: 0.0756637305021286 = 0.009562788531184196 + 0.01 * 6.61009407043457
Epoch 620, val loss: 1.0205285549163818
Epoch 630, training loss: 0.0750875174999237 = 0.00909728929400444 + 0.01 * 6.599023342132568
Epoch 630, val loss: 1.0286099910736084
Epoch 640, training loss: 0.07470305263996124 = 0.008665714412927628 + 0.01 * 6.603733539581299
Epoch 640, val loss: 1.036550760269165
Epoch 650, training loss: 0.07411517947912216 = 0.008266644552350044 + 0.01 * 6.5848541259765625
Epoch 650, val loss: 1.0443073511123657
Epoch 660, training loss: 0.07379555702209473 = 0.007895848713815212 + 0.01 * 6.589970588684082
Epoch 660, val loss: 1.051814079284668
Epoch 670, training loss: 0.07351113855838776 = 0.00755126029253006 + 0.01 * 6.5959882736206055
Epoch 670, val loss: 1.0591988563537598
Epoch 680, training loss: 0.0729975625872612 = 0.007230391725897789 + 0.01 * 6.576716899871826
Epoch 680, val loss: 1.0663896799087524
Epoch 690, training loss: 0.07294028252363205 = 0.006930743344128132 + 0.01 * 6.600954055786133
Epoch 690, val loss: 1.0733810663223267
Epoch 700, training loss: 0.0723331868648529 = 0.006650455296039581 + 0.01 * 6.568273544311523
Epoch 700, val loss: 1.080113410949707
Epoch 710, training loss: 0.07213102281093597 = 0.006387920118868351 + 0.01 * 6.574310779571533
Epoch 710, val loss: 1.0868490934371948
Epoch 720, training loss: 0.0717487633228302 = 0.006142124533653259 + 0.01 * 6.56066370010376
Epoch 720, val loss: 1.0933488607406616
Epoch 730, training loss: 0.07156169414520264 = 0.005911535583436489 + 0.01 * 6.565016269683838
Epoch 730, val loss: 1.0996602773666382
Epoch 740, training loss: 0.07137446850538254 = 0.005694716703146696 + 0.01 * 6.567975044250488
Epoch 740, val loss: 1.1059448719024658
Epoch 750, training loss: 0.07104713469743729 = 0.005491269286721945 + 0.01 * 6.555586338043213
Epoch 750, val loss: 1.1119745969772339
Epoch 760, training loss: 0.07100214064121246 = 0.005299841053783894 + 0.01 * 6.570229530334473
Epoch 760, val loss: 1.1179163455963135
Epoch 770, training loss: 0.07056392729282379 = 0.005119445268064737 + 0.01 * 6.544448375701904
Epoch 770, val loss: 1.12367844581604
Epoch 780, training loss: 0.07043274492025375 = 0.004949158988893032 + 0.01 * 6.54835844039917
Epoch 780, val loss: 1.1293712854385376
Epoch 790, training loss: 0.07024834305047989 = 0.004788444843143225 + 0.01 * 6.545990467071533
Epoch 790, val loss: 1.1349236965179443
Epoch 800, training loss: 0.07001573592424393 = 0.004636403638869524 + 0.01 * 6.537932872772217
Epoch 800, val loss: 1.1403332948684692
Epoch 810, training loss: 0.06996462494134903 = 0.004492518026381731 + 0.01 * 6.547210216522217
Epoch 810, val loss: 1.1457000970840454
Epoch 820, training loss: 0.06967031955718994 = 0.0043563987128436565 + 0.01 * 6.5313920974731445
Epoch 820, val loss: 1.1508821249008179
Epoch 830, training loss: 0.06960055977106094 = 0.004227235447615385 + 0.01 * 6.537332534790039
Epoch 830, val loss: 1.1559414863586426
Epoch 840, training loss: 0.06942801922559738 = 0.004104801919311285 + 0.01 * 6.532321929931641
Epoch 840, val loss: 1.1610077619552612
Epoch 850, training loss: 0.06921398639678955 = 0.003988437354564667 + 0.01 * 6.522555351257324
Epoch 850, val loss: 1.1658474206924438
Epoch 860, training loss: 0.06902329623699188 = 0.0038777149748057127 + 0.01 * 6.5145583152771
Epoch 860, val loss: 1.1706315279006958
Epoch 870, training loss: 0.06893924623727798 = 0.0037724177818745375 + 0.01 * 6.516683101654053
Epoch 870, val loss: 1.175347924232483
Epoch 880, training loss: 0.068932443857193 = 0.0036720719654113054 + 0.01 * 6.526037693023682
Epoch 880, val loss: 1.179921269416809
Epoch 890, training loss: 0.06869684904813766 = 0.0035765182692557573 + 0.01 * 6.512033462524414
Epoch 890, val loss: 1.1844924688339233
Epoch 900, training loss: 0.06849920749664307 = 0.003485345747321844 + 0.01 * 6.501386642456055
Epoch 900, val loss: 1.1888467073440552
Epoch 910, training loss: 0.06836061179637909 = 0.0033986279740929604 + 0.01 * 6.496198654174805
Epoch 910, val loss: 1.193205714225769
Epoch 920, training loss: 0.06840623915195465 = 0.0033157081343233585 + 0.01 * 6.509052753448486
Epoch 920, val loss: 1.1974341869354248
Epoch 930, training loss: 0.06839484721422195 = 0.0032364532817155123 + 0.01 * 6.515839576721191
Epoch 930, val loss: 1.2015811204910278
Epoch 940, training loss: 0.06812825798988342 = 0.0031605903059244156 + 0.01 * 6.496767044067383
Epoch 940, val loss: 1.2057397365570068
Epoch 950, training loss: 0.06826406717300415 = 0.0030880733393132687 + 0.01 * 6.517599582672119
Epoch 950, val loss: 1.2097059488296509
Epoch 960, training loss: 0.06808186322450638 = 0.0030186462681740522 + 0.01 * 6.506321430206299
Epoch 960, val loss: 1.2137072086334229
Epoch 970, training loss: 0.06778649240732193 = 0.0029521246906369925 + 0.01 * 6.483437538146973
Epoch 970, val loss: 1.2175456285476685
Epoch 980, training loss: 0.06792384386062622 = 0.002888251328840852 + 0.01 * 6.503559112548828
Epoch 980, val loss: 1.2212350368499756
Epoch 990, training loss: 0.06771504878997803 = 0.002827070653438568 + 0.01 * 6.488798141479492
Epoch 990, val loss: 1.2249912023544312
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.8118
Flip ASR: 0.7733/225 nodes
The final ASR:0.66544, 0.17177, Accuracy:0.78148, 0.03326
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11614])
remove edge: torch.Size([2, 9560])
updated graph: torch.Size([2, 10618])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.97909, 0.00696, Accuracy:0.83333, 0.00907
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0292599201202393 = 1.9455217123031616 + 0.01 * 8.373811721801758
Epoch 0, val loss: 1.9512618780136108
Epoch 10, training loss: 2.019949197769165 = 1.9362118244171143 + 0.01 * 8.373732566833496
Epoch 10, val loss: 1.9420369863510132
Epoch 20, training loss: 2.0089054107666016 = 1.925171136856079 + 0.01 * 8.373424530029297
Epoch 20, val loss: 1.9309622049331665
Epoch 30, training loss: 1.9937856197357178 = 1.9100607633590698 + 0.01 * 8.372485160827637
Epoch 30, val loss: 1.9158209562301636
Epoch 40, training loss: 1.971491813659668 = 1.887812852859497 + 0.01 * 8.36789608001709
Epoch 40, val loss: 1.8938238620758057
Epoch 50, training loss: 1.9387322664260864 = 1.8553736209869385 + 0.01 * 8.335867881774902
Epoch 50, val loss: 1.862751841545105
Epoch 60, training loss: 1.8945139646530151 = 1.8131957054138184 + 0.01 * 8.131828308105469
Epoch 60, val loss: 1.8250676393508911
Epoch 70, training loss: 1.8457454442977905 = 1.7683244943618774 + 0.01 * 7.742090702056885
Epoch 70, val loss: 1.78749418258667
Epoch 80, training loss: 1.7926926612854004 = 1.7178577184677124 + 0.01 * 7.483497142791748
Epoch 80, val loss: 1.7428756952285767
Epoch 90, training loss: 1.7217469215393066 = 1.648990511894226 + 0.01 * 7.275638103485107
Epoch 90, val loss: 1.6822267770767212
Epoch 100, training loss: 1.6286145448684692 = 1.5579928159713745 + 0.01 * 7.062173366546631
Epoch 100, val loss: 1.6054502725601196
Epoch 110, training loss: 1.5185251235961914 = 1.448505163192749 + 0.01 * 7.002001762390137
Epoch 110, val loss: 1.5142329931259155
Epoch 120, training loss: 1.3999202251434326 = 1.33040452003479 + 0.01 * 6.951570987701416
Epoch 120, val loss: 1.4165635108947754
Epoch 130, training loss: 1.2802813053131104 = 1.2110916376113892 + 0.01 * 6.918968200683594
Epoch 130, val loss: 1.3202425241470337
Epoch 140, training loss: 1.1646159887313843 = 1.0956083536148071 + 0.01 * 6.90076208114624
Epoch 140, val loss: 1.229582667350769
Epoch 150, training loss: 1.0566835403442383 = 0.9878204464912415 + 0.01 * 6.8863067626953125
Epoch 150, val loss: 1.1462280750274658
Epoch 160, training loss: 0.9588062763214111 = 0.890107274055481 + 0.01 * 6.869902610778809
Epoch 160, val loss: 1.0716058015823364
Epoch 170, training loss: 0.8720932006835938 = 0.8036003708839417 + 0.01 * 6.8492817878723145
Epoch 170, val loss: 1.0062458515167236
Epoch 180, training loss: 0.7965940237045288 = 0.728326678276062 + 0.01 * 6.826732635498047
Epoch 180, val loss: 0.9507372975349426
Epoch 190, training loss: 0.7309281229972839 = 0.6628377437591553 + 0.01 * 6.809040546417236
Epoch 190, val loss: 0.9047333598136902
Epoch 200, training loss: 0.6726053953170776 = 0.6046525239944458 + 0.01 * 6.795289039611816
Epoch 200, val loss: 0.8664276599884033
Epoch 210, training loss: 0.6194063425064087 = 0.5515382289886475 + 0.01 * 6.786810874938965
Epoch 210, val loss: 0.8337830305099487
Epoch 220, training loss: 0.5700991749763489 = 0.5023070573806763 + 0.01 * 6.779209613800049
Epoch 220, val loss: 0.8056647181510925
Epoch 230, training loss: 0.5240449905395508 = 0.4562905728816986 + 0.01 * 6.775440216064453
Epoch 230, val loss: 0.7817135453224182
Epoch 240, training loss: 0.4802468717098236 = 0.41259220242500305 + 0.01 * 6.765467166900635
Epoch 240, val loss: 0.7617487907409668
Epoch 250, training loss: 0.4376371502876282 = 0.3700729012489319 + 0.01 * 6.7564263343811035
Epoch 250, val loss: 0.7451731562614441
Epoch 260, training loss: 0.39552658796310425 = 0.32803571224212646 + 0.01 * 6.749085903167725
Epoch 260, val loss: 0.7314229011535645
Epoch 270, training loss: 0.35435375571250916 = 0.28690505027770996 + 0.01 * 6.744871616363525
Epoch 270, val loss: 0.7204463481903076
Epoch 280, training loss: 0.3154752254486084 = 0.24813725054264069 + 0.01 * 6.733797073364258
Epoch 280, val loss: 0.7126645445823669
Epoch 290, training loss: 0.28046345710754395 = 0.213114395737648 + 0.01 * 6.7349066734313965
Epoch 290, val loss: 0.7085983157157898
Epoch 300, training loss: 0.25003957748413086 = 0.18285112082958221 + 0.01 * 6.718846321105957
Epoch 300, val loss: 0.7085720896720886
Epoch 310, training loss: 0.22475242614746094 = 0.15762962400913239 + 0.01 * 6.712279319763184
Epoch 310, val loss: 0.7124027013778687
Epoch 320, training loss: 0.20397379994392395 = 0.1369481235742569 + 0.01 * 6.702568531036377
Epoch 320, val loss: 0.7192850112915039
Epoch 330, training loss: 0.18688133358955383 = 0.11994964629411697 + 0.01 * 6.693169116973877
Epoch 330, val loss: 0.7284338474273682
Epoch 340, training loss: 0.17265164852142334 = 0.10581396520137787 + 0.01 * 6.683768272399902
Epoch 340, val loss: 0.7389423251152039
Epoch 350, training loss: 0.16068996489048004 = 0.09389029443264008 + 0.01 * 6.679967403411865
Epoch 350, val loss: 0.7503316402435303
Epoch 360, training loss: 0.1503930687904358 = 0.08370241522789001 + 0.01 * 6.669064998626709
Epoch 360, val loss: 0.7623124718666077
Epoch 370, training loss: 0.14193859696388245 = 0.07489027827978134 + 0.01 * 6.704833030700684
Epoch 370, val loss: 0.7746572494506836
Epoch 380, training loss: 0.1338120996952057 = 0.0672110766172409 + 0.01 * 6.660101890563965
Epoch 380, val loss: 0.787354588508606
Epoch 390, training loss: 0.12700867652893066 = 0.060466039925813675 + 0.01 * 6.654263496398926
Epoch 390, val loss: 0.8002400994300842
Epoch 400, training loss: 0.12093670666217804 = 0.05450492724776268 + 0.01 * 6.643178462982178
Epoch 400, val loss: 0.813287079334259
Epoch 410, training loss: 0.11559507250785828 = 0.04921984300017357 + 0.01 * 6.637523174285889
Epoch 410, val loss: 0.8264554738998413
Epoch 420, training loss: 0.11088034510612488 = 0.04453039541840553 + 0.01 * 6.6349945068359375
Epoch 420, val loss: 0.8397337198257446
Epoch 430, training loss: 0.10667498409748077 = 0.04038203880190849 + 0.01 * 6.629294395446777
Epoch 430, val loss: 0.852942168712616
Epoch 440, training loss: 0.10298910737037659 = 0.036714356392621994 + 0.01 * 6.627475738525391
Epoch 440, val loss: 0.8660485148429871
Epoch 450, training loss: 0.09965192526578903 = 0.03346902132034302 + 0.01 * 6.618290901184082
Epoch 450, val loss: 0.8791093826293945
Epoch 460, training loss: 0.09680716693401337 = 0.03059542365372181 + 0.01 * 6.6211748123168945
Epoch 460, val loss: 0.8920648097991943
Epoch 470, training loss: 0.09412971884012222 = 0.028051117435097694 + 0.01 * 6.607860565185547
Epoch 470, val loss: 0.9048484563827515
Epoch 480, training loss: 0.0917816087603569 = 0.02579095959663391 + 0.01 * 6.59906530380249
Epoch 480, val loss: 0.9174362421035767
Epoch 490, training loss: 0.08972449600696564 = 0.02377796173095703 + 0.01 * 6.594653606414795
Epoch 490, val loss: 0.9298111796379089
Epoch 500, training loss: 0.08793477714061737 = 0.021981406956911087 + 0.01 * 6.595337867736816
Epoch 500, val loss: 0.9419528245925903
Epoch 510, training loss: 0.08630543947219849 = 0.020373011007905006 + 0.01 * 6.59324312210083
Epoch 510, val loss: 0.9538112878799438
Epoch 520, training loss: 0.08486900478601456 = 0.01892993412911892 + 0.01 * 6.593907356262207
Epoch 520, val loss: 0.9654407501220703
Epoch 530, training loss: 0.08342616260051727 = 0.017631778493523598 + 0.01 * 6.57943868637085
Epoch 530, val loss: 0.976679801940918
Epoch 540, training loss: 0.08219967037439346 = 0.01645970344543457 + 0.01 * 6.573996543884277
Epoch 540, val loss: 0.9876798391342163
Epoch 550, training loss: 0.08113095909357071 = 0.015398509800434113 + 0.01 * 6.573245048522949
Epoch 550, val loss: 0.9983872175216675
Epoch 560, training loss: 0.08015561103820801 = 0.014437121339142323 + 0.01 * 6.5718488693237305
Epoch 560, val loss: 1.0087671279907227
Epoch 570, training loss: 0.07925882935523987 = 0.013564380817115307 + 0.01 * 6.5694451332092285
Epoch 570, val loss: 1.01887845993042
Epoch 580, training loss: 0.07833001762628555 = 0.012769165448844433 + 0.01 * 6.556085586547852
Epoch 580, val loss: 1.0286908149719238
Epoch 590, training loss: 0.07764378935098648 = 0.012042305432260036 + 0.01 * 6.5601487159729
Epoch 590, val loss: 1.0382238626480103
Epoch 600, training loss: 0.07688001543283463 = 0.011376745998859406 + 0.01 * 6.550326824188232
Epoch 600, val loss: 1.0475693941116333
Epoch 610, training loss: 0.07622269541025162 = 0.010766454041004181 + 0.01 * 6.545624256134033
Epoch 610, val loss: 1.05659019947052
Epoch 620, training loss: 0.07572651654481888 = 0.010205822996795177 + 0.01 * 6.552070140838623
Epoch 620, val loss: 1.0654042959213257
Epoch 630, training loss: 0.07508852332830429 = 0.009689497761428356 + 0.01 * 6.539903163909912
Epoch 630, val loss: 1.073982834815979
Epoch 640, training loss: 0.0746879056096077 = 0.00921260379254818 + 0.01 * 6.547530174255371
Epoch 640, val loss: 1.0823360681533813
Epoch 650, training loss: 0.07421571016311646 = 0.008772176690399647 + 0.01 * 6.544353008270264
Epoch 650, val loss: 1.090429425239563
Epoch 660, training loss: 0.07364772260189056 = 0.008364261128008366 + 0.01 * 6.528346061706543
Epoch 660, val loss: 1.098368763923645
Epoch 670, training loss: 0.07325319200754166 = 0.00798563752323389 + 0.01 * 6.5267558097839355
Epoch 670, val loss: 1.1060659885406494
Epoch 680, training loss: 0.07304937392473221 = 0.007633873727172613 + 0.01 * 6.541550159454346
Epoch 680, val loss: 1.113555908203125
Epoch 690, training loss: 0.07244854420423508 = 0.007306879386305809 + 0.01 * 6.514166355133057
Epoch 690, val loss: 1.1208974123001099
Epoch 700, training loss: 0.07216521352529526 = 0.007001778576523066 + 0.01 * 6.516343593597412
Epoch 700, val loss: 1.128056526184082
Epoch 710, training loss: 0.07181678712368011 = 0.006717135664075613 + 0.01 * 6.509964942932129
Epoch 710, val loss: 1.134993314743042
Epoch 720, training loss: 0.07158815860748291 = 0.006451512686908245 + 0.01 * 6.513664722442627
Epoch 720, val loss: 1.1417982578277588
Epoch 730, training loss: 0.07122629135847092 = 0.006202575284987688 + 0.01 * 6.502371311187744
Epoch 730, val loss: 1.148473858833313
Epoch 740, training loss: 0.07101188600063324 = 0.005969098769128323 + 0.01 * 6.504279136657715
Epoch 740, val loss: 1.1549409627914429
Epoch 750, training loss: 0.07098473608493805 = 0.005749944597482681 + 0.01 * 6.52347993850708
Epoch 750, val loss: 1.1612207889556885
Epoch 760, training loss: 0.07047910243272781 = 0.005544093903154135 + 0.01 * 6.493500709533691
Epoch 760, val loss: 1.1674391031265259
Epoch 770, training loss: 0.07037679105997086 = 0.005350422579795122 + 0.01 * 6.502636909484863
Epoch 770, val loss: 1.1734038591384888
Epoch 780, training loss: 0.07011117041110992 = 0.005168265197426081 + 0.01 * 6.494290828704834
Epoch 780, val loss: 1.1793051958084106
Epoch 790, training loss: 0.069974884390831 = 0.004996270872652531 + 0.01 * 6.497861385345459
Epoch 790, val loss: 1.1850800514221191
Epoch 800, training loss: 0.0696648508310318 = 0.004833870567381382 + 0.01 * 6.48309850692749
Epoch 800, val loss: 1.1907025575637817
Epoch 810, training loss: 0.06943640857934952 = 0.004680383484810591 + 0.01 * 6.475602626800537
Epoch 810, val loss: 1.1961256265640259
Epoch 820, training loss: 0.06939633935689926 = 0.004535253159701824 + 0.01 * 6.486108779907227
Epoch 820, val loss: 1.2015219926834106
Epoch 830, training loss: 0.0692058801651001 = 0.004397945944219828 + 0.01 * 6.4807939529418945
Epoch 830, val loss: 1.2067863941192627
Epoch 840, training loss: 0.0692502111196518 = 0.004267594311386347 + 0.01 * 6.49826192855835
Epoch 840, val loss: 1.211912751197815
Epoch 850, training loss: 0.06880901008844376 = 0.004144064150750637 + 0.01 * 6.466494560241699
Epoch 850, val loss: 1.2169166803359985
Epoch 860, training loss: 0.06896906346082687 = 0.00402668584138155 + 0.01 * 6.494237899780273
Epoch 860, val loss: 1.2217978239059448
Epoch 870, training loss: 0.06867485493421555 = 0.003915328998118639 + 0.01 * 6.4759521484375
Epoch 870, val loss: 1.2265595197677612
Epoch 880, training loss: 0.06847644597291946 = 0.0038093947805464268 + 0.01 * 6.466705799102783
Epoch 880, val loss: 1.2312830686569214
Epoch 890, training loss: 0.0682360976934433 = 0.0037084512878209352 + 0.01 * 6.452764511108398
Epoch 890, val loss: 1.2358688116073608
Epoch 900, training loss: 0.06817442178726196 = 0.003612259868532419 + 0.01 * 6.456216812133789
Epoch 900, val loss: 1.240297555923462
Epoch 910, training loss: 0.06814783811569214 = 0.0035206337925046682 + 0.01 * 6.46272087097168
Epoch 910, val loss: 1.244673490524292
Epoch 920, training loss: 0.06786283850669861 = 0.003433207981288433 + 0.01 * 6.442963600158691
Epoch 920, val loss: 1.248991847038269
Epoch 930, training loss: 0.06772105395793915 = 0.0033496865071356297 + 0.01 * 6.437137126922607
Epoch 930, val loss: 1.2531155347824097
Epoch 940, training loss: 0.06790582835674286 = 0.0032699434086680412 + 0.01 * 6.463589191436768
Epoch 940, val loss: 1.2571908235549927
Epoch 950, training loss: 0.06758546829223633 = 0.0031936904415488243 + 0.01 * 6.439177989959717
Epoch 950, val loss: 1.2612510919570923
Epoch 960, training loss: 0.06739705801010132 = 0.003120655659586191 + 0.01 * 6.427640914916992
Epoch 960, val loss: 1.2651528120040894
Epoch 970, training loss: 0.06731579452753067 = 0.0030507692135870457 + 0.01 * 6.4265031814575195
Epoch 970, val loss: 1.2690138816833496
Epoch 980, training loss: 0.06710896641016006 = 0.002983740298077464 + 0.01 * 6.412522315979004
Epoch 980, val loss: 1.2728557586669922
Epoch 990, training loss: 0.06726904213428497 = 0.0029193228110671043 + 0.01 * 6.434972286224365
Epoch 990, val loss: 1.2764867544174194
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.8598
Flip ASR: 0.8311/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.02097225189209 = 1.9372351169586182 + 0.01 * 8.373705863952637
Epoch 0, val loss: 1.931441068649292
Epoch 10, training loss: 2.0107996463775635 = 1.9270647764205933 + 0.01 * 8.37348747253418
Epoch 10, val loss: 1.9213130474090576
Epoch 20, training loss: 1.9978893995285034 = 1.9141613245010376 + 0.01 * 8.372812271118164
Epoch 20, val loss: 1.9080699682235718
Epoch 30, training loss: 1.9795602560043335 = 1.895858645439148 + 0.01 * 8.370162010192871
Epoch 30, val loss: 1.889067530632019
Epoch 40, training loss: 1.952635407447815 = 1.8691149950027466 + 0.01 * 8.352045059204102
Epoch 40, val loss: 1.8616509437561035
Epoch 50, training loss: 1.9145991802215576 = 1.832204818725586 + 0.01 * 8.239436149597168
Epoch 50, val loss: 1.8256220817565918
Epoch 60, training loss: 1.867430567741394 = 1.7891548871994019 + 0.01 * 7.827573299407959
Epoch 60, val loss: 1.7871774435043335
Epoch 70, training loss: 1.8196942806243896 = 1.7450073957443237 + 0.01 * 7.468685150146484
Epoch 70, val loss: 1.7496349811553955
Epoch 80, training loss: 1.7605050802230835 = 1.6892884969711304 + 0.01 * 7.121660232543945
Epoch 80, val loss: 1.7000999450683594
Epoch 90, training loss: 1.6839090585708618 = 1.6141180992126465 + 0.01 * 6.979094982147217
Epoch 90, val loss: 1.6343157291412354
Epoch 100, training loss: 1.5877792835235596 = 1.5182955265045166 + 0.01 * 6.9483723640441895
Epoch 100, val loss: 1.5535870790481567
Epoch 110, training loss: 1.481290578842163 = 1.4119490385055542 + 0.01 * 6.934154033660889
Epoch 110, val loss: 1.466644048690796
Epoch 120, training loss: 1.374598741531372 = 1.305323839187622 + 0.01 * 6.927487373352051
Epoch 120, val loss: 1.3820972442626953
Epoch 130, training loss: 1.2699722051620483 = 1.200800895690918 + 0.01 * 6.917132377624512
Epoch 130, val loss: 1.3010531663894653
Epoch 140, training loss: 1.167488932609558 = 1.0984476804733276 + 0.01 * 6.904121398925781
Epoch 140, val loss: 1.2222539186477661
Epoch 150, training loss: 1.0685497522354126 = 0.9996560215950012 + 0.01 * 6.889377593994141
Epoch 150, val loss: 1.1464121341705322
Epoch 160, training loss: 0.976053774356842 = 0.9073363542556763 + 0.01 * 6.871743679046631
Epoch 160, val loss: 1.0766067504882812
Epoch 170, training loss: 0.8927207589149475 = 0.824212908744812 + 0.01 * 6.850786209106445
Epoch 170, val loss: 1.0148687362670898
Epoch 180, training loss: 0.8202459216117859 = 0.7519830465316772 + 0.01 * 6.826287746429443
Epoch 180, val loss: 0.962834358215332
Epoch 190, training loss: 0.758470892906189 = 0.6904700994491577 + 0.01 * 6.800081729888916
Epoch 190, val loss: 0.9207472205162048
Epoch 200, training loss: 0.7052373886108398 = 0.6374197006225586 + 0.01 * 6.78176736831665
Epoch 200, val loss: 0.8866469860076904
Epoch 210, training loss: 0.657132625579834 = 0.5895033478736877 + 0.01 * 6.762929439544678
Epoch 210, val loss: 0.8574264645576477
Epoch 220, training loss: 0.6114124655723572 = 0.5439419150352478 + 0.01 * 6.747056007385254
Epoch 220, val loss: 0.8303770422935486
Epoch 230, training loss: 0.5667522549629211 = 0.4994162619113922 + 0.01 * 6.733599662780762
Epoch 230, val loss: 0.8043080568313599
Epoch 240, training loss: 0.5230016112327576 = 0.45573994517326355 + 0.01 * 6.726165771484375
Epoch 240, val loss: 0.7796274423599243
Epoch 250, training loss: 0.4803658425807953 = 0.41325050592422485 + 0.01 * 6.711533069610596
Epoch 250, val loss: 0.75737065076828
Epoch 260, training loss: 0.4391605257987976 = 0.37210389971733093 + 0.01 * 6.705661773681641
Epoch 260, val loss: 0.7377324104309082
Epoch 270, training loss: 0.39938950538635254 = 0.33241307735443115 + 0.01 * 6.697644233703613
Epoch 270, val loss: 0.7206555008888245
Epoch 280, training loss: 0.3615325093269348 = 0.2945932149887085 + 0.01 * 6.693929672241211
Epoch 280, val loss: 0.706200122833252
Epoch 290, training loss: 0.3264821171760559 = 0.25957512855529785 + 0.01 * 6.690700531005859
Epoch 290, val loss: 0.6949272155761719
Epoch 300, training loss: 0.2950201630592346 = 0.22816434502601624 + 0.01 * 6.685583591461182
Epoch 300, val loss: 0.6874471306800842
Epoch 310, training loss: 0.26733484864234924 = 0.20050488412380219 + 0.01 * 6.6829962730407715
Epoch 310, val loss: 0.6833571195602417
Epoch 320, training loss: 0.2429581880569458 = 0.1761162430047989 + 0.01 * 6.6841936111450195
Epoch 320, val loss: 0.6831113696098328
Epoch 330, training loss: 0.22108878195285797 = 0.1543091982603073 + 0.01 * 6.6779584884643555
Epoch 330, val loss: 0.6863235831260681
Epoch 340, training loss: 0.20207709074020386 = 0.13532504439353943 + 0.01 * 6.675205707550049
Epoch 340, val loss: 0.6922054290771484
Epoch 350, training loss: 0.1858908236026764 = 0.11915425956249237 + 0.01 * 6.673657417297363
Epoch 350, val loss: 0.7004662156105042
Epoch 360, training loss: 0.17195409536361694 = 0.1052309051156044 + 0.01 * 6.672318935394287
Epoch 360, val loss: 0.7098309993743896
Epoch 370, training loss: 0.1598597764968872 = 0.09317785501480103 + 0.01 * 6.668193340301514
Epoch 370, val loss: 0.7202150225639343
Epoch 380, training loss: 0.14935459196567535 = 0.08269836008548737 + 0.01 * 6.665623188018799
Epoch 380, val loss: 0.731785237789154
Epoch 390, training loss: 0.14012932777404785 = 0.0735202208161354 + 0.01 * 6.660910606384277
Epoch 390, val loss: 0.7442452907562256
Epoch 400, training loss: 0.13207000494003296 = 0.06548638641834259 + 0.01 * 6.65836238861084
Epoch 400, val loss: 0.7574599981307983
Epoch 410, training loss: 0.12498725950717926 = 0.05844767764210701 + 0.01 * 6.653957843780518
Epoch 410, val loss: 0.7714313268661499
Epoch 420, training loss: 0.11878137290477753 = 0.052285123616456985 + 0.01 * 6.649625301361084
Epoch 420, val loss: 0.78578782081604
Epoch 430, training loss: 0.1133919507265091 = 0.046907033771276474 + 0.01 * 6.648491859436035
Epoch 430, val loss: 0.800460934638977
Epoch 440, training loss: 0.10864254087209702 = 0.04221182316541672 + 0.01 * 6.64307165145874
Epoch 440, val loss: 0.8152003884315491
Epoch 450, training loss: 0.10449177026748657 = 0.0381154790520668 + 0.01 * 6.63762903213501
Epoch 450, val loss: 0.8298008441925049
Epoch 460, training loss: 0.1008957177400589 = 0.034542474895715714 + 0.01 * 6.635324001312256
Epoch 460, val loss: 0.8443694114685059
Epoch 470, training loss: 0.09772150218486786 = 0.03141843527555466 + 0.01 * 6.630306720733643
Epoch 470, val loss: 0.8586699962615967
Epoch 480, training loss: 0.09506731480360031 = 0.02867688238620758 + 0.01 * 6.63904333114624
Epoch 480, val loss: 0.8727660179138184
Epoch 490, training loss: 0.09253549575805664 = 0.026268083602190018 + 0.01 * 6.6267409324646
Epoch 490, val loss: 0.8864948749542236
Epoch 500, training loss: 0.09027570486068726 = 0.024141952395439148 + 0.01 * 6.613375663757324
Epoch 500, val loss: 0.899881899356842
Epoch 510, training loss: 0.08832558244466782 = 0.0222568791359663 + 0.01 * 6.606870651245117
Epoch 510, val loss: 0.9129992723464966
Epoch 520, training loss: 0.08696448057889938 = 0.02057996392250061 + 0.01 * 6.638452053070068
Epoch 520, val loss: 0.9257741570472717
Epoch 530, training loss: 0.0851564034819603 = 0.01908634416759014 + 0.01 * 6.607005596160889
Epoch 530, val loss: 0.9381185173988342
Epoch 540, training loss: 0.08370991796255112 = 0.017748989164829254 + 0.01 * 6.59609317779541
Epoch 540, val loss: 0.9501597285270691
Epoch 550, training loss: 0.08245108276605606 = 0.01654689572751522 + 0.01 * 6.590418815612793
Epoch 550, val loss: 0.9618167281150818
Epoch 560, training loss: 0.08130738139152527 = 0.01546233706176281 + 0.01 * 6.584504127502441
Epoch 560, val loss: 0.9732015132904053
Epoch 570, training loss: 0.08056868612766266 = 0.014481526799499989 + 0.01 * 6.608716011047363
Epoch 570, val loss: 0.9843237996101379
Epoch 580, training loss: 0.07938554137945175 = 0.013594272546470165 + 0.01 * 6.579127311706543
Epoch 580, val loss: 0.9950293898582458
Epoch 590, training loss: 0.07852806895971298 = 0.012787861749529839 + 0.01 * 6.5740203857421875
Epoch 590, val loss: 1.0054678916931152
Epoch 600, training loss: 0.07782638072967529 = 0.012053354643285275 + 0.01 * 6.5773024559021
Epoch 600, val loss: 1.015599250793457
Epoch 610, training loss: 0.07707072049379349 = 0.011382587254047394 + 0.01 * 6.568813323974609
Epoch 610, val loss: 1.0254459381103516
Epoch 620, training loss: 0.07646123319864273 = 0.010768458247184753 + 0.01 * 6.569277286529541
Epoch 620, val loss: 1.035056471824646
Epoch 630, training loss: 0.07590647041797638 = 0.01020489726215601 + 0.01 * 6.570157527923584
Epoch 630, val loss: 1.044359564781189
Epoch 640, training loss: 0.07526430487632751 = 0.009686567820608616 + 0.01 * 6.557774066925049
Epoch 640, val loss: 1.0534257888793945
Epoch 650, training loss: 0.07478693872690201 = 0.009208736941218376 + 0.01 * 6.557819843292236
Epoch 650, val loss: 1.0622764825820923
Epoch 660, training loss: 0.07433617115020752 = 0.008767645806074142 + 0.01 * 6.5568528175354
Epoch 660, val loss: 1.0708211660385132
Epoch 670, training loss: 0.07383328676223755 = 0.008359159342944622 + 0.01 * 6.547412872314453
Epoch 670, val loss: 1.0792330503463745
Epoch 680, training loss: 0.07356549054384232 = 0.007980180904269218 + 0.01 * 6.558531284332275
Epoch 680, val loss: 1.0874366760253906
Epoch 690, training loss: 0.07306203991174698 = 0.007628756109625101 + 0.01 * 6.543328762054443
Epoch 690, val loss: 1.0953967571258545
Epoch 700, training loss: 0.0727415606379509 = 0.007301608566194773 + 0.01 * 6.543995380401611
Epoch 700, val loss: 1.1031346321105957
Epoch 710, training loss: 0.07249519973993301 = 0.006996487267315388 + 0.01 * 6.54987096786499
Epoch 710, val loss: 1.1107580661773682
Epoch 720, training loss: 0.07210113853216171 = 0.006711550988256931 + 0.01 * 6.53895902633667
Epoch 720, val loss: 1.1181788444519043
Epoch 730, training loss: 0.07177425920963287 = 0.00644494267180562 + 0.01 * 6.532931804656982
Epoch 730, val loss: 1.1254421472549438
Epoch 740, training loss: 0.0715731680393219 = 0.00619542459025979 + 0.01 * 6.537774562835693
Epoch 740, val loss: 1.1324881315231323
Epoch 750, training loss: 0.07127800583839417 = 0.005961362272500992 + 0.01 * 6.5316643714904785
Epoch 750, val loss: 1.1394511461257935
Epoch 760, training loss: 0.07102788239717484 = 0.005741207394748926 + 0.01 * 6.528667449951172
Epoch 760, val loss: 1.146199107170105
Epoch 770, training loss: 0.07082050293684006 = 0.005534279625862837 + 0.01 * 6.528623104095459
Epoch 770, val loss: 1.1527971029281616
Epoch 780, training loss: 0.07055149972438812 = 0.005339521914720535 + 0.01 * 6.521197319030762
Epoch 780, val loss: 1.1591973304748535
Epoch 790, training loss: 0.07028650492429733 = 0.0051562911830842495 + 0.01 * 6.513021469116211
Epoch 790, val loss: 1.1655206680297852
Epoch 800, training loss: 0.07010731101036072 = 0.0049836221151053905 + 0.01 * 6.512369155883789
Epoch 800, val loss: 1.171674370765686
Epoch 810, training loss: 0.07005304843187332 = 0.0048205615021288395 + 0.01 * 6.523248672485352
Epoch 810, val loss: 1.1776820421218872
Epoch 820, training loss: 0.06980918347835541 = 0.004666830413043499 + 0.01 * 6.514235973358154
Epoch 820, val loss: 1.1836247444152832
Epoch 830, training loss: 0.06963088363409042 = 0.004521456081420183 + 0.01 * 6.5109429359436035
Epoch 830, val loss: 1.1894036531448364
Epoch 840, training loss: 0.06936285644769669 = 0.004383851308375597 + 0.01 * 6.497900485992432
Epoch 840, val loss: 1.1950087547302246
Epoch 850, training loss: 0.06924625486135483 = 0.004253358114510775 + 0.01 * 6.499289512634277
Epoch 850, val loss: 1.2005900144577026
Epoch 860, training loss: 0.06911679357290268 = 0.004129479639232159 + 0.01 * 6.49873161315918
Epoch 860, val loss: 1.205990195274353
Epoch 870, training loss: 0.06905391812324524 = 0.00401168130338192 + 0.01 * 6.504223823547363
Epoch 870, val loss: 1.2113227844238281
Epoch 880, training loss: 0.06875374168157578 = 0.003899854142218828 + 0.01 * 6.48538875579834
Epoch 880, val loss: 1.2164958715438843
Epoch 890, training loss: 0.0687677264213562 = 0.0037935185246169567 + 0.01 * 6.4974212646484375
Epoch 890, val loss: 1.2216159105300903
Epoch 900, training loss: 0.06859871745109558 = 0.003692327067255974 + 0.01 * 6.490638732910156
Epoch 900, val loss: 1.226545810699463
Epoch 910, training loss: 0.06856460869312286 = 0.003595921443775296 + 0.01 * 6.496869087219238
Epoch 910, val loss: 1.2314363718032837
Epoch 920, training loss: 0.0684695690870285 = 0.003503987332805991 + 0.01 * 6.49655818939209
Epoch 920, val loss: 1.236174464225769
Epoch 930, training loss: 0.06817735731601715 = 0.0034164211247116327 + 0.01 * 6.4760942459106445
Epoch 930, val loss: 1.2408852577209473
Epoch 940, training loss: 0.0679793432354927 = 0.003332780906930566 + 0.01 * 6.464656829833984
Epoch 940, val loss: 1.2454369068145752
Epoch 950, training loss: 0.06788510829210281 = 0.0032528508454561234 + 0.01 * 6.463226318359375
Epoch 950, val loss: 1.2498960494995117
Epoch 960, training loss: 0.06785055249929428 = 0.0031763496808707714 + 0.01 * 6.467421054840088
Epoch 960, val loss: 1.2543421983718872
Epoch 970, training loss: 0.06776759028434753 = 0.0031033530831336975 + 0.01 * 6.466423988342285
Epoch 970, val loss: 1.258578896522522
Epoch 980, training loss: 0.06764339655637741 = 0.0030334226321429014 + 0.01 * 6.460997581481934
Epoch 980, val loss: 1.2628339529037476
Epoch 990, training loss: 0.06755118817090988 = 0.002966596744954586 + 0.01 * 6.45845890045166
Epoch 990, val loss: 1.2668577432632446
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.8930
Flip ASR: 0.8711/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.04075288772583 = 1.9570149183273315 + 0.01 * 8.373785972595215
Epoch 0, val loss: 1.9527816772460938
Epoch 10, training loss: 2.029963731765747 = 1.946226954460144 + 0.01 * 8.373684883117676
Epoch 10, val loss: 1.9425584077835083
Epoch 20, training loss: 2.0165393352508545 = 1.9328058958053589 + 0.01 * 8.37335205078125
Epoch 20, val loss: 1.929459810256958
Epoch 30, training loss: 1.9976861476898193 = 1.9139626026153564 + 0.01 * 8.372357368469238
Epoch 30, val loss: 1.9107764959335327
Epoch 40, training loss: 1.9697641134262085 = 1.8860951662063599 + 0.01 * 8.366891860961914
Epoch 40, val loss: 1.8834503889083862
Epoch 50, training loss: 1.9306913614273071 = 1.847369909286499 + 0.01 * 8.332149505615234
Epoch 50, val loss: 1.8476648330688477
Epoch 60, training loss: 1.8865293264389038 = 1.8049297332763672 + 0.01 * 8.159964561462402
Epoch 60, val loss: 1.813357949256897
Epoch 70, training loss: 1.8460006713867188 = 1.767520785331726 + 0.01 * 7.847991466522217
Epoch 70, val loss: 1.7846941947937012
Epoch 80, training loss: 1.7973836660385132 = 1.7209177017211914 + 0.01 * 7.64659309387207
Epoch 80, val loss: 1.7438143491744995
Epoch 90, training loss: 1.731486201286316 = 1.6568552255630493 + 0.01 * 7.463102340698242
Epoch 90, val loss: 1.6887959241867065
Epoch 100, training loss: 1.6455479860305786 = 1.5728321075439453 + 0.01 * 7.271586894989014
Epoch 100, val loss: 1.619895339012146
Epoch 110, training loss: 1.5499608516693115 = 1.4794520139694214 + 0.01 * 7.050886631011963
Epoch 110, val loss: 1.54561185836792
Epoch 120, training loss: 1.455811858177185 = 1.386214256286621 + 0.01 * 6.959757328033447
Epoch 120, val loss: 1.471699595451355
Epoch 130, training loss: 1.3643536567687988 = 1.2950806617736816 + 0.01 * 6.927299976348877
Epoch 130, val loss: 1.4032377004623413
Epoch 140, training loss: 1.2740696668624878 = 1.2049801349639893 + 0.01 * 6.908956527709961
Epoch 140, val loss: 1.3382495641708374
Epoch 150, training loss: 1.18479585647583 = 1.1159350872039795 + 0.01 * 6.886080265045166
Epoch 150, val loss: 1.2742984294891357
Epoch 160, training loss: 1.098907232284546 = 1.030288815498352 + 0.01 * 6.86184549331665
Epoch 160, val loss: 1.2135156393051147
Epoch 170, training loss: 1.0181736946105957 = 0.9497896432876587 + 0.01 * 6.838409423828125
Epoch 170, val loss: 1.1567569971084595
Epoch 180, training loss: 0.9422478675842285 = 0.8740744590759277 + 0.01 * 6.8173418045043945
Epoch 180, val loss: 1.102531909942627
Epoch 190, training loss: 0.8697500824928284 = 0.801744818687439 + 0.01 * 6.8005266189575195
Epoch 190, val loss: 1.050125241279602
Epoch 200, training loss: 0.7998354434967041 = 0.7319473624229431 + 0.01 * 6.7888054847717285
Epoch 200, val loss: 0.9994143843650818
Epoch 210, training loss: 0.7324098944664001 = 0.6645902991294861 + 0.01 * 6.781960964202881
Epoch 210, val loss: 0.9513856172561646
Epoch 220, training loss: 0.6674819588661194 = 0.5997222065925598 + 0.01 * 6.77597713470459
Epoch 220, val loss: 0.90752112865448
Epoch 230, training loss: 0.6050578355789185 = 0.5373633503913879 + 0.01 * 6.769448280334473
Epoch 230, val loss: 0.8686156868934631
Epoch 240, training loss: 0.5452693104743958 = 0.4776492416858673 + 0.01 * 6.762006759643555
Epoch 240, val loss: 0.8346540331840515
Epoch 250, training loss: 0.48851460218429565 = 0.4209691286087036 + 0.01 * 6.7545485496521
Epoch 250, val loss: 0.8057485818862915
Epoch 260, training loss: 0.43529370427131653 = 0.36780598759651184 + 0.01 * 6.748772144317627
Epoch 260, val loss: 0.7816107273101807
Epoch 270, training loss: 0.3861332833766937 = 0.31871774792671204 + 0.01 * 6.741552829742432
Epoch 270, val loss: 0.7622259855270386
Epoch 280, training loss: 0.3416779637336731 = 0.2743352949619293 + 0.01 * 6.73426628112793
Epoch 280, val loss: 0.7475491762161255
Epoch 290, training loss: 0.3025239109992981 = 0.23524248600006104 + 0.01 * 6.7281413078308105
Epoch 290, val loss: 0.7375022172927856
Epoch 300, training loss: 0.26893383264541626 = 0.20169778168201447 + 0.01 * 6.723605632781982
Epoch 300, val loss: 0.7320122718811035
Epoch 310, training loss: 0.2406829595565796 = 0.17349578440189362 + 0.01 * 6.718716621398926
Epoch 310, val loss: 0.7305455207824707
Epoch 320, training loss: 0.21721145510673523 = 0.15006649494171143 + 0.01 * 6.714497089385986
Epoch 320, val loss: 0.7327073812484741
Epoch 330, training loss: 0.19775089621543884 = 0.1306583285331726 + 0.01 * 6.70925760269165
Epoch 330, val loss: 0.737853467464447
Epoch 340, training loss: 0.18157556653022766 = 0.11450713872909546 + 0.01 * 6.706843376159668
Epoch 340, val loss: 0.7453979253768921
Epoch 350, training loss: 0.16795209050178528 = 0.10095050930976868 + 0.01 * 6.70015811920166
Epoch 350, val loss: 0.7547958493232727
Epoch 360, training loss: 0.15640872716903687 = 0.08944117277860641 + 0.01 * 6.696756362915039
Epoch 360, val loss: 0.7655699253082275
Epoch 370, training loss: 0.14651212096214294 = 0.07956480979919434 + 0.01 * 6.694731712341309
Epoch 370, val loss: 0.7773271799087524
Epoch 380, training loss: 0.13792136311531067 = 0.07101130485534668 + 0.01 * 6.691005706787109
Epoch 380, val loss: 0.7897927165031433
Epoch 390, training loss: 0.13043931126594543 = 0.06355363130569458 + 0.01 * 6.688568592071533
Epoch 390, val loss: 0.8027610778808594
Epoch 400, training loss: 0.1238645389676094 = 0.057020604610443115 + 0.01 * 6.684393882751465
Epoch 400, val loss: 0.8160693049430847
Epoch 410, training loss: 0.1180860698223114 = 0.051284175366163254 + 0.01 * 6.680189609527588
Epoch 410, val loss: 0.8296020030975342
Epoch 420, training loss: 0.11301472038030624 = 0.046238481998443604 + 0.01 * 6.677623748779297
Epoch 420, val loss: 0.8433122634887695
Epoch 430, training loss: 0.10857096314430237 = 0.04179278388619423 + 0.01 * 6.677818298339844
Epoch 430, val loss: 0.8571539521217346
Epoch 440, training loss: 0.10461470484733582 = 0.03787047043442726 + 0.01 * 6.674423694610596
Epoch 440, val loss: 0.8710201382637024
Epoch 450, training loss: 0.10111129283905029 = 0.0344056598842144 + 0.01 * 6.6705641746521
Epoch 450, val loss: 0.8848535418510437
Epoch 460, training loss: 0.09801989048719406 = 0.03134150803089142 + 0.01 * 6.6678385734558105
Epoch 460, val loss: 0.898652195930481
Epoch 470, training loss: 0.09529101103544235 = 0.02862410433590412 + 0.01 * 6.666690826416016
Epoch 470, val loss: 0.9123837351799011
Epoch 480, training loss: 0.09284912794828415 = 0.0262085422873497 + 0.01 * 6.664058685302734
Epoch 480, val loss: 0.9259810447692871
Epoch 490, training loss: 0.09066270291805267 = 0.024057922884821892 + 0.01 * 6.660477638244629
Epoch 490, val loss: 0.939342737197876
Epoch 500, training loss: 0.08877791464328766 = 0.022139986976981163 + 0.01 * 6.663793087005615
Epoch 500, val loss: 0.9523950815200806
Epoch 510, training loss: 0.08699966222047806 = 0.020426856353878975 + 0.01 * 6.657280921936035
Epoch 510, val loss: 0.9651751518249512
Epoch 520, training loss: 0.08543309569358826 = 0.01889391429722309 + 0.01 * 6.653918266296387
Epoch 520, val loss: 0.9775968194007874
Epoch 530, training loss: 0.08403026312589645 = 0.017517099156975746 + 0.01 * 6.6513166427612305
Epoch 530, val loss: 0.9896835684776306
Epoch 540, training loss: 0.08276881277561188 = 0.016276881098747253 + 0.01 * 6.649193286895752
Epoch 540, val loss: 1.0014052391052246
Epoch 550, training loss: 0.08163478225469589 = 0.015157400630414486 + 0.01 * 6.647738456726074
Epoch 550, val loss: 1.0127991437911987
Epoch 560, training loss: 0.08058568835258484 = 0.014144834131002426 + 0.01 * 6.644084930419922
Epoch 560, val loss: 1.0238983631134033
Epoch 570, training loss: 0.0796412006020546 = 0.013224892318248749 + 0.01 * 6.64163064956665
Epoch 570, val loss: 1.0347129106521606
Epoch 580, training loss: 0.07887788116931915 = 0.012388038448989391 + 0.01 * 6.648984432220459
Epoch 580, val loss: 1.0452525615692139
Epoch 590, training loss: 0.07802247256040573 = 0.011629180982708931 + 0.01 * 6.639329433441162
Epoch 590, val loss: 1.0554399490356445
Epoch 600, training loss: 0.07729475200176239 = 0.010937409475445747 + 0.01 * 6.635734558105469
Epoch 600, val loss: 1.0653162002563477
Epoch 610, training loss: 0.0766291469335556 = 0.01030453760176897 + 0.01 * 6.632460594177246
Epoch 610, val loss: 1.0749781131744385
Epoch 620, training loss: 0.07603532820940018 = 0.009724101983010769 + 0.01 * 6.631122589111328
Epoch 620, val loss: 1.0843545198440552
Epoch 630, training loss: 0.07551628351211548 = 0.009191310033202171 + 0.01 * 6.632497310638428
Epoch 630, val loss: 1.0935025215148926
Epoch 640, training loss: 0.07497420161962509 = 0.008702333085238934 + 0.01 * 6.627187252044678
Epoch 640, val loss: 1.1024175882339478
Epoch 650, training loss: 0.07449570298194885 = 0.008251501247286797 + 0.01 * 6.624420166015625
Epoch 650, val loss: 1.1110631227493286
Epoch 660, training loss: 0.07404880225658417 = 0.007836122065782547 + 0.01 * 6.6212687492370605
Epoch 660, val loss: 1.1195098161697388
Epoch 670, training loss: 0.07365129888057709 = 0.007453135214745998 + 0.01 * 6.619816303253174
Epoch 670, val loss: 1.1277087926864624
Epoch 680, training loss: 0.07327669113874435 = 0.007098580710589886 + 0.01 * 6.61781120300293
Epoch 680, val loss: 1.1357300281524658
Epoch 690, training loss: 0.07294124364852905 = 0.0067700790241360664 + 0.01 * 6.617116451263428
Epoch 690, val loss: 1.1435661315917969
Epoch 700, training loss: 0.07259657979011536 = 0.006466005463153124 + 0.01 * 6.613057613372803
Epoch 700, val loss: 1.151199460029602
Epoch 710, training loss: 0.07231152802705765 = 0.00618312181904912 + 0.01 * 6.612840175628662
Epoch 710, val loss: 1.1586058139801025
Epoch 720, training loss: 0.07202569395303726 = 0.005919710732996464 + 0.01 * 6.610598564147949
Epoch 720, val loss: 1.1658401489257812
Epoch 730, training loss: 0.07175691425800323 = 0.005674390122294426 + 0.01 * 6.608252048492432
Epoch 730, val loss: 1.1728874444961548
Epoch 740, training loss: 0.07147388160228729 = 0.005445451475679874 + 0.01 * 6.602843284606934
Epoch 740, val loss: 1.179728388786316
Epoch 750, training loss: 0.07123389095067978 = 0.0052318875677883625 + 0.01 * 6.600200653076172
Epoch 750, val loss: 1.1864296197891235
Epoch 760, training loss: 0.0710284411907196 = 0.0050319889560341835 + 0.01 * 6.599645614624023
Epoch 760, val loss: 1.1929664611816406
Epoch 770, training loss: 0.07079171389341354 = 0.0048444331623613834 + 0.01 * 6.594727993011475
Epoch 770, val loss: 1.1993563175201416
Epoch 780, training loss: 0.07068166881799698 = 0.004668381530791521 + 0.01 * 6.6013288497924805
Epoch 780, val loss: 1.2055054903030396
Epoch 790, training loss: 0.070396788418293 = 0.004503494594246149 + 0.01 * 6.589329242706299
Epoch 790, val loss: 1.2115930318832397
Epoch 800, training loss: 0.07020752876996994 = 0.004348191432654858 + 0.01 * 6.585934162139893
Epoch 800, val loss: 1.217535376548767
Epoch 810, training loss: 0.07008885592222214 = 0.00420198542997241 + 0.01 * 6.588686943054199
Epoch 810, val loss: 1.2233178615570068
Epoch 820, training loss: 0.06982371211051941 = 0.004064158536493778 + 0.01 * 6.575955867767334
Epoch 820, val loss: 1.2289681434631348
Epoch 830, training loss: 0.06972737610340118 = 0.003933839965611696 + 0.01 * 6.5793538093566895
Epoch 830, val loss: 1.2344603538513184
Epoch 840, training loss: 0.06951351463794708 = 0.0038110576570034027 + 0.01 * 6.570245742797852
Epoch 840, val loss: 1.2398626804351807
Epoch 850, training loss: 0.0696151927113533 = 0.003694541286677122 + 0.01 * 6.592065334320068
Epoch 850, val loss: 1.2451481819152832
Epoch 860, training loss: 0.06922439485788345 = 0.0035846491809934378 + 0.01 * 6.5639753341674805
Epoch 860, val loss: 1.2502292394638062
Epoch 870, training loss: 0.06907402724027634 = 0.003480274463072419 + 0.01 * 6.559374809265137
Epoch 870, val loss: 1.2552844285964966
Epoch 880, training loss: 0.06908579170703888 = 0.0033815379720181227 + 0.01 * 6.570425987243652
Epoch 880, val loss: 1.2601845264434814
Epoch 890, training loss: 0.06878381967544556 = 0.00328749418258667 + 0.01 * 6.549633026123047
Epoch 890, val loss: 1.2649552822113037
Epoch 900, training loss: 0.06885424256324768 = 0.00319815706461668 + 0.01 * 6.565608978271484
Epoch 900, val loss: 1.269644021987915
Epoch 910, training loss: 0.06864286959171295 = 0.0031136986799538136 + 0.01 * 6.552917003631592
Epoch 910, val loss: 1.2742347717285156
Epoch 920, training loss: 0.06855035573244095 = 0.0030328212305903435 + 0.01 * 6.551753520965576
Epoch 920, val loss: 1.2787073850631714
Epoch 930, training loss: 0.06861022114753723 = 0.002956298878416419 + 0.01 * 6.565392017364502
Epoch 930, val loss: 1.2830981016159058
Epoch 940, training loss: 0.06819906085729599 = 0.0028834580443799496 + 0.01 * 6.531560897827148
Epoch 940, val loss: 1.2872995138168335
Epoch 950, training loss: 0.0682530477643013 = 0.002813768805935979 + 0.01 * 6.5439276695251465
Epoch 950, val loss: 1.2915095090866089
Epoch 960, training loss: 0.06806938350200653 = 0.0027473808731883764 + 0.01 * 6.532200813293457
Epoch 960, val loss: 1.2954708337783813
Epoch 970, training loss: 0.06795424222946167 = 0.0026840667705982924 + 0.01 * 6.527017593383789
Epoch 970, val loss: 1.2995108366012573
Epoch 980, training loss: 0.06793751567602158 = 0.0026232311502099037 + 0.01 * 6.531428337097168
Epoch 980, val loss: 1.3033803701400757
Epoch 990, training loss: 0.06776324659585953 = 0.0025652798358350992 + 0.01 * 6.519796848297119
Epoch 990, val loss: 1.307127594947815
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.7269
Flip ASR: 0.6756/225 nodes
The final ASR:0.82657, 0.07174, Accuracy:0.82222, 0.00524
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11642])
remove edge: torch.Size([2, 9464])
updated graph: torch.Size([2, 10550])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.98155, 0.00301, Accuracy:0.82840, 0.00462
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.035191297531128 = 1.9514530897140503 + 0.01 * 8.37381649017334
Epoch 0, val loss: 1.9432640075683594
Epoch 10, training loss: 2.0250017642974854 = 1.9412645101547241 + 0.01 * 8.373730659484863
Epoch 10, val loss: 1.933435320854187
Epoch 20, training loss: 2.012352466583252 = 1.9286184310913086 + 0.01 * 8.373405456542969
Epoch 20, val loss: 1.921082615852356
Epoch 30, training loss: 1.9945244789123535 = 1.910800576210022 + 0.01 * 8.372387886047363
Epoch 30, val loss: 1.9036182165145874
Epoch 40, training loss: 1.9682466983795166 = 1.8845746517181396 + 0.01 * 8.367205619812012
Epoch 40, val loss: 1.8781991004943848
Epoch 50, training loss: 1.9309659004211426 = 1.8476203680038452 + 0.01 * 8.334556579589844
Epoch 50, val loss: 1.8439289331436157
Epoch 60, training loss: 1.885859489440918 = 1.8041191101074219 + 0.01 * 8.174034118652344
Epoch 60, val loss: 1.8076727390289307
Epoch 70, training loss: 1.8404030799865723 = 1.7634984254837036 + 0.01 * 7.690464973449707
Epoch 70, val loss: 1.77759850025177
Epoch 80, training loss: 1.7905759811401367 = 1.7176933288574219 + 0.01 * 7.288269996643066
Epoch 80, val loss: 1.740382432937622
Epoch 90, training loss: 1.7270355224609375 = 1.6561158895492554 + 0.01 * 7.091969013214111
Epoch 90, val loss: 1.6877259016036987
Epoch 100, training loss: 1.6441011428833008 = 1.5741723775863647 + 0.01 * 6.9928717613220215
Epoch 100, val loss: 1.6182332038879395
Epoch 110, training loss: 1.5414652824401855 = 1.4718983173370361 + 0.01 * 6.956699371337891
Epoch 110, val loss: 1.5343737602233887
Epoch 120, training loss: 1.4263436794281006 = 1.3570587635040283 + 0.01 * 6.928491115570068
Epoch 120, val loss: 1.4417914152145386
Epoch 130, training loss: 1.3089112043380737 = 1.2398006916046143 + 0.01 * 6.911052227020264
Epoch 130, val loss: 1.3479844331741333
Epoch 140, training loss: 1.1953116655349731 = 1.126318335533142 + 0.01 * 6.899335861206055
Epoch 140, val loss: 1.256547451019287
Epoch 150, training loss: 1.0879130363464355 = 1.018969178199768 + 0.01 * 6.894387245178223
Epoch 150, val loss: 1.1696736812591553
Epoch 160, training loss: 0.9876728653907776 = 0.9187657833099365 + 0.01 * 6.89070987701416
Epoch 160, val loss: 1.0884811878204346
Epoch 170, training loss: 0.8960617780685425 = 0.8271901607513428 + 0.01 * 6.887162208557129
Epoch 170, val loss: 1.0151565074920654
Epoch 180, training loss: 0.8147386312484741 = 0.745893657207489 + 0.01 * 6.884499549865723
Epoch 180, val loss: 0.95155268907547
Epoch 190, training loss: 0.7441504001617432 = 0.675326943397522 + 0.01 * 6.88234806060791
Epoch 190, val loss: 0.8987419605255127
Epoch 200, training loss: 0.682451605796814 = 0.613638162612915 + 0.01 * 6.881345748901367
Epoch 200, val loss: 0.8562459945678711
Epoch 210, training loss: 0.6262403726577759 = 0.5574309229850769 + 0.01 * 6.880946159362793
Epoch 210, val loss: 0.8214915990829468
Epoch 220, training loss: 0.5724775791168213 = 0.5036714673042297 + 0.01 * 6.880613803863525
Epoch 220, val loss: 0.7913504242897034
Epoch 230, training loss: 0.519751250743866 = 0.4509519040584564 + 0.01 * 6.879935264587402
Epoch 230, val loss: 0.7637566924095154
Epoch 240, training loss: 0.4682997763156891 = 0.39951080083847046 + 0.01 * 6.878897190093994
Epoch 240, val loss: 0.7380257248878479
Epoch 250, training loss: 0.41912031173706055 = 0.3503444790840149 + 0.01 * 6.877583026885986
Epoch 250, val loss: 0.7143990993499756
Epoch 260, training loss: 0.3732118606567383 = 0.3044508099555969 + 0.01 * 6.876104831695557
Epoch 260, val loss: 0.69366854429245
Epoch 270, training loss: 0.3312305212020874 = 0.2624891698360443 + 0.01 * 6.874134540557861
Epoch 270, val loss: 0.6767123341560364
Epoch 280, training loss: 0.293606698513031 = 0.2248857617378235 + 0.01 * 6.872094631195068
Epoch 280, val loss: 0.6641054749488831
Epoch 290, training loss: 0.2606491148471832 = 0.19195246696472168 + 0.01 * 6.869664192199707
Epoch 290, val loss: 0.655998945236206
Epoch 300, training loss: 0.23251214623451233 = 0.1638375073671341 + 0.01 * 6.867464065551758
Epoch 300, val loss: 0.6522461771965027
Epoch 310, training loss: 0.20901057124137878 = 0.14037655293941498 + 0.01 * 6.863400936126709
Epoch 310, val loss: 0.6522291898727417
Epoch 320, training loss: 0.18969884514808655 = 0.12109784781932831 + 0.01 * 6.860100269317627
Epoch 320, val loss: 0.6553947925567627
Epoch 330, training loss: 0.17389534413814545 = 0.10532896965742111 + 0.01 * 6.856637477874756
Epoch 330, val loss: 0.6610307097434998
Epoch 340, training loss: 0.16087467968463898 = 0.09236670285463333 + 0.01 * 6.850797653198242
Epoch 340, val loss: 0.6685245037078857
Epoch 350, training loss: 0.15005353093147278 = 0.08160075545310974 + 0.01 * 6.845277786254883
Epoch 350, val loss: 0.6774322390556335
Epoch 360, training loss: 0.140974223613739 = 0.07255563884973526 + 0.01 * 6.841858863830566
Epoch 360, val loss: 0.687248945236206
Epoch 370, training loss: 0.1332094371318817 = 0.06486918032169342 + 0.01 * 6.834025859832764
Epoch 370, val loss: 0.6977032423019409
Epoch 380, training loss: 0.126526340842247 = 0.058269765228033066 + 0.01 * 6.825657367706299
Epoch 380, val loss: 0.7085995078086853
Epoch 390, training loss: 0.12075819075107574 = 0.05255914106965065 + 0.01 * 6.819905757904053
Epoch 390, val loss: 0.7197934985160828
Epoch 400, training loss: 0.11585243046283722 = 0.04758422449231148 + 0.01 * 6.826821327209473
Epoch 400, val loss: 0.7311776876449585
Epoch 410, training loss: 0.1111883595585823 = 0.04322928935289383 + 0.01 * 6.795907020568848
Epoch 410, val loss: 0.742644727230072
Epoch 420, training loss: 0.10723285377025604 = 0.03939707577228546 + 0.01 * 6.783577919006348
Epoch 420, val loss: 0.7541308999061584
Epoch 430, training loss: 0.10376876592636108 = 0.03601747378706932 + 0.01 * 6.7751288414001465
Epoch 430, val loss: 0.7655883431434631
Epoch 440, training loss: 0.10059353709220886 = 0.0330246165394783 + 0.01 * 6.756892204284668
Epoch 440, val loss: 0.7769200205802917
Epoch 450, training loss: 0.09779921919107437 = 0.030363673344254494 + 0.01 * 6.743554592132568
Epoch 450, val loss: 0.7882031798362732
Epoch 460, training loss: 0.09539153426885605 = 0.027994846925139427 + 0.01 * 6.739668369293213
Epoch 460, val loss: 0.7993084788322449
Epoch 470, training loss: 0.0934331864118576 = 0.025878284126520157 + 0.01 * 6.755490779876709
Epoch 470, val loss: 0.8102833032608032
Epoch 480, training loss: 0.09104935824871063 = 0.023984668776392937 + 0.01 * 6.7064690589904785
Epoch 480, val loss: 0.8209912180900574
Epoch 490, training loss: 0.08920516818761826 = 0.022281436249613762 + 0.01 * 6.692373275756836
Epoch 490, val loss: 0.831530749797821
Epoch 500, training loss: 0.08764028549194336 = 0.02074800804257393 + 0.01 * 6.689228057861328
Epoch 500, val loss: 0.8418790698051453
Epoch 510, training loss: 0.08623194694519043 = 0.0193619541823864 + 0.01 * 6.686999320983887
Epoch 510, val loss: 0.851939857006073
Epoch 520, training loss: 0.08493898808956146 = 0.01811075583100319 + 0.01 * 6.682823181152344
Epoch 520, val loss: 0.8617664575576782
Epoch 530, training loss: 0.08345852792263031 = 0.01697656698524952 + 0.01 * 6.648196220397949
Epoch 530, val loss: 0.8713191747665405
Epoch 540, training loss: 0.08237514644861221 = 0.015943771228194237 + 0.01 * 6.643137454986572
Epoch 540, val loss: 0.8806976675987244
Epoch 550, training loss: 0.08130840212106705 = 0.015002177096903324 + 0.01 * 6.630622386932373
Epoch 550, val loss: 0.8897708654403687
Epoch 560, training loss: 0.0803428515791893 = 0.01414150558412075 + 0.01 * 6.620134353637695
Epoch 560, val loss: 0.8986467719078064
Epoch 570, training loss: 0.07947874069213867 = 0.013352906331419945 + 0.01 * 6.612583637237549
Epoch 570, val loss: 0.9073540568351746
Epoch 580, training loss: 0.07877277582883835 = 0.01262846402823925 + 0.01 * 6.614431381225586
Epoch 580, val loss: 0.9158260226249695
Epoch 590, training loss: 0.07809977978467941 = 0.011963088996708393 + 0.01 * 6.613668918609619
Epoch 590, val loss: 0.9241239428520203
Epoch 600, training loss: 0.07737581431865692 = 0.01135020051151514 + 0.01 * 6.602561950683594
Epoch 600, val loss: 0.9321660399436951
Epoch 610, training loss: 0.07667756080627441 = 0.010784219950437546 + 0.01 * 6.589334011077881
Epoch 610, val loss: 0.9400392770767212
Epoch 620, training loss: 0.07619000226259232 = 0.010260673239827156 + 0.01 * 6.592933177947998
Epoch 620, val loss: 0.9477157592773438
Epoch 630, training loss: 0.07566659152507782 = 0.009776001796126366 + 0.01 * 6.589058876037598
Epoch 630, val loss: 0.9552126526832581
Epoch 640, training loss: 0.07516638934612274 = 0.009326946921646595 + 0.01 * 6.583943843841553
Epoch 640, val loss: 0.9625620245933533
Epoch 650, training loss: 0.07470966875553131 = 0.008909340016543865 + 0.01 * 6.580033302307129
Epoch 650, val loss: 0.9696967005729675
Epoch 660, training loss: 0.07410484552383423 = 0.00852100271731615 + 0.01 * 6.558384418487549
Epoch 660, val loss: 0.9766570925712585
Epoch 670, training loss: 0.0738702267408371 = 0.00815910566598177 + 0.01 * 6.571111679077148
Epoch 670, val loss: 0.9834654927253723
Epoch 680, training loss: 0.07338415831327438 = 0.007821498438715935 + 0.01 * 6.5562663078308105
Epoch 680, val loss: 0.9901347756385803
Epoch 690, training loss: 0.07297486066818237 = 0.007505971472710371 + 0.01 * 6.546889305114746
Epoch 690, val loss: 0.9966187477111816
Epoch 700, training loss: 0.07265037298202515 = 0.007210846990346909 + 0.01 * 6.543952941894531
Epoch 700, val loss: 1.002955436706543
Epoch 710, training loss: 0.07226002216339111 = 0.0069345589727163315 + 0.01 * 6.532546043395996
Epoch 710, val loss: 1.009148359298706
Epoch 720, training loss: 0.07219277322292328 = 0.006675093434751034 + 0.01 * 6.551767826080322
Epoch 720, val loss: 1.0151407718658447
Epoch 730, training loss: 0.07177606970071793 = 0.00643166434019804 + 0.01 * 6.534440994262695
Epoch 730, val loss: 1.0210472345352173
Epoch 740, training loss: 0.0715237408876419 = 0.00620272196829319 + 0.01 * 6.532102584838867
Epoch 740, val loss: 1.0267964601516724
Epoch 750, training loss: 0.07126006484031677 = 0.005987127777189016 + 0.01 * 6.527293682098389
Epoch 750, val loss: 1.0323960781097412
Epoch 760, training loss: 0.07108307629823685 = 0.005783801898360252 + 0.01 * 6.529927730560303
Epoch 760, val loss: 1.0379178524017334
Epoch 770, training loss: 0.07086370885372162 = 0.005592161789536476 + 0.01 * 6.527154922485352
Epoch 770, val loss: 1.0433202981948853
Epoch 780, training loss: 0.07051384449005127 = 0.005411049816757441 + 0.01 * 6.510279178619385
Epoch 780, val loss: 1.0485872030258179
Epoch 790, training loss: 0.07044854760169983 = 0.005239740014076233 + 0.01 * 6.520881175994873
Epoch 790, val loss: 1.0537934303283691
Epoch 800, training loss: 0.07015994936227798 = 0.005077465903013945 + 0.01 * 6.508248805999756
Epoch 800, val loss: 1.0588345527648926
Epoch 810, training loss: 0.07007668167352676 = 0.004923795349895954 + 0.01 * 6.515288829803467
Epoch 810, val loss: 1.0638507604599
Epoch 820, training loss: 0.06961143761873245 = 0.004777980037033558 + 0.01 * 6.4833455085754395
Epoch 820, val loss: 1.0686861276626587
Epoch 830, training loss: 0.06954902410507202 = 0.0046393475495278835 + 0.01 * 6.490967750549316
Epoch 830, val loss: 1.0734730958938599
Epoch 840, training loss: 0.069266177713871 = 0.004508018493652344 + 0.01 * 6.475816249847412
Epoch 840, val loss: 1.0781346559524536
Epoch 850, training loss: 0.06922174990177155 = 0.0043826596811413765 + 0.01 * 6.483908653259277
Epoch 850, val loss: 1.0827447175979614
Epoch 860, training loss: 0.06908437609672546 = 0.004263689275830984 + 0.01 * 6.4820685386657715
Epoch 860, val loss: 1.0871939659118652
Epoch 870, training loss: 0.06880693137645721 = 0.004150159657001495 + 0.01 * 6.465677261352539
Epoch 870, val loss: 1.0916635990142822
Epoch 880, training loss: 0.06878572702407837 = 0.004042040091007948 + 0.01 * 6.474369525909424
Epoch 880, val loss: 1.095908522605896
Epoch 890, training loss: 0.06850093603134155 = 0.003938945475965738 + 0.01 * 6.4561991691589355
Epoch 890, val loss: 1.100209355354309
Epoch 900, training loss: 0.0688033401966095 = 0.003840493271127343 + 0.01 * 6.4962849617004395
Epoch 900, val loss: 1.1043174266815186
Epoch 910, training loss: 0.06824473291635513 = 0.0037464192137122154 + 0.01 * 6.449831485748291
Epoch 910, val loss: 1.1083743572235107
Epoch 920, training loss: 0.06808466464281082 = 0.0036562280729413033 + 0.01 * 6.442843437194824
Epoch 920, val loss: 1.1124063730239868
Epoch 930, training loss: 0.06808619201183319 = 0.0035700404550880194 + 0.01 * 6.451615333557129
Epoch 930, val loss: 1.1162787675857544
Epoch 940, training loss: 0.06797228008508682 = 0.003487485693767667 + 0.01 * 6.448480129241943
Epoch 940, val loss: 1.1201351881027222
Epoch 950, training loss: 0.06772008538246155 = 0.003408665070310235 + 0.01 * 6.431142330169678
Epoch 950, val loss: 1.1239346265792847
Epoch 960, training loss: 0.06779921054840088 = 0.003332564840093255 + 0.01 * 6.446665287017822
Epoch 960, val loss: 1.1276514530181885
Epoch 970, training loss: 0.06762485206127167 = 0.003259877907112241 + 0.01 * 6.436497688293457
Epoch 970, val loss: 1.131273865699768
Epoch 980, training loss: 0.06753013283014297 = 0.0031902859918773174 + 0.01 * 6.433984756469727
Epoch 980, val loss: 1.1348366737365723
Epoch 990, training loss: 0.06740452349185944 = 0.0031232049223035574 + 0.01 * 6.428132057189941
Epoch 990, val loss: 1.138393759727478
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8481
Overall ASR: 0.6125
Flip ASR: 0.5422/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.023812770843506 = 1.9400739669799805 + 0.01 * 8.373878479003906
Epoch 0, val loss: 1.947083830833435
Epoch 10, training loss: 2.0139060020446777 = 1.9301681518554688 + 0.01 * 8.373786926269531
Epoch 10, val loss: 1.9356528520584106
Epoch 20, training loss: 2.0014517307281494 = 1.9177169799804688 + 0.01 * 8.373477935791016
Epoch 20, val loss: 1.9209644794464111
Epoch 30, training loss: 1.9837653636932373 = 1.900038719177246 + 0.01 * 8.372660636901855
Epoch 30, val loss: 1.8997615575790405
Epoch 40, training loss: 1.9580090045928955 = 1.874315619468689 + 0.01 * 8.369338989257812
Epoch 40, val loss: 1.8691221475601196
Epoch 50, training loss: 1.9226425886154175 = 1.8391696214675903 + 0.01 * 8.34729290008545
Epoch 50, val loss: 1.8289921283721924
Epoch 60, training loss: 1.8796343803405762 = 1.7975130081176758 + 0.01 * 8.212138175964355
Epoch 60, val loss: 1.784844160079956
Epoch 70, training loss: 1.8333399295806885 = 1.7556287050247192 + 0.01 * 7.771121025085449
Epoch 70, val loss: 1.7447084188461304
Epoch 80, training loss: 1.7823467254638672 = 1.7087825536727905 + 0.01 * 7.3564133644104
Epoch 80, val loss: 1.7029480934143066
Epoch 90, training loss: 1.7168227434158325 = 1.6450141668319702 + 0.01 * 7.180856227874756
Epoch 90, val loss: 1.6482504606246948
Epoch 100, training loss: 1.6314654350280762 = 1.5600824356079102 + 0.01 * 7.13830041885376
Epoch 100, val loss: 1.5760738849639893
Epoch 110, training loss: 1.5258657932281494 = 1.45489501953125 + 0.01 * 7.097078323364258
Epoch 110, val loss: 1.4881240129470825
Epoch 120, training loss: 1.408766269683838 = 1.3381222486495972 + 0.01 * 7.064406871795654
Epoch 120, val loss: 1.3922842741012573
Epoch 130, training loss: 1.2868627309799194 = 1.2165980339050293 + 0.01 * 7.026473045349121
Epoch 130, val loss: 1.2951440811157227
Epoch 140, training loss: 1.1645991802215576 = 1.0947885513305664 + 0.01 * 6.981069087982178
Epoch 140, val loss: 1.2006069421768188
Epoch 150, training loss: 1.0472571849822998 = 0.9779061675071716 + 0.01 * 6.935101509094238
Epoch 150, val loss: 1.1120363473892212
Epoch 160, training loss: 0.9405960440635681 = 0.8715096712112427 + 0.01 * 6.908639907836914
Epoch 160, val loss: 1.0335133075714111
Epoch 170, training loss: 0.8490565419197083 = 0.7800298929214478 + 0.01 * 6.902663707733154
Epoch 170, val loss: 0.9682996869087219
Epoch 180, training loss: 0.77329421043396 = 0.7042974233627319 + 0.01 * 6.8996782302856445
Epoch 180, val loss: 0.916871964931488
Epoch 190, training loss: 0.7104670405387878 = 0.6414802074432373 + 0.01 * 6.898685932159424
Epoch 190, val loss: 0.8765694499015808
Epoch 200, training loss: 0.6567317247390747 = 0.5877519845962524 + 0.01 * 6.897971153259277
Epoch 200, val loss: 0.8442788124084473
Epoch 210, training loss: 0.6093987822532654 = 0.540428638458252 + 0.01 * 6.897017002105713
Epoch 210, val loss: 0.8177247047424316
Epoch 220, training loss: 0.5668101906776428 = 0.49785852432250977 + 0.01 * 6.895167350769043
Epoch 220, val loss: 0.7955139875411987
Epoch 230, training loss: 0.5281531810760498 = 0.4592267870903015 + 0.01 * 6.892637729644775
Epoch 230, val loss: 0.7773622870445251
Epoch 240, training loss: 0.4924829602241516 = 0.4235844016075134 + 0.01 * 6.889854907989502
Epoch 240, val loss: 0.7626478672027588
Epoch 250, training loss: 0.45850688219070435 = 0.3896360695362091 + 0.01 * 6.887081623077393
Epoch 250, val loss: 0.7499818205833435
Epoch 260, training loss: 0.4249071478843689 = 0.35606849193573 + 0.01 * 6.883867263793945
Epoch 260, val loss: 0.7386101484298706
Epoch 270, training loss: 0.39093050360679626 = 0.32211482524871826 + 0.01 * 6.881568908691406
Epoch 270, val loss: 0.7278493046760559
Epoch 280, training loss: 0.35679346323013306 = 0.2880226671695709 + 0.01 * 6.877079963684082
Epoch 280, val loss: 0.7178143262863159
Epoch 290, training loss: 0.32356134057044983 = 0.2548286020755768 + 0.01 * 6.873273849487305
Epoch 290, val loss: 0.7088494300842285
Epoch 300, training loss: 0.2921554744243622 = 0.2234451025724411 + 0.01 * 6.871037006378174
Epoch 300, val loss: 0.7014410495758057
Epoch 310, training loss: 0.2635517716407776 = 0.19489581882953644 + 0.01 * 6.865596294403076
Epoch 310, val loss: 0.6959820985794067
Epoch 320, training loss: 0.23807914555072784 = 0.16948062181472778 + 0.01 * 6.859852313995361
Epoch 320, val loss: 0.692629873752594
Epoch 330, training loss: 0.21576040983200073 = 0.14719219505786896 + 0.01 * 6.856821537017822
Epoch 330, val loss: 0.6913277506828308
Epoch 340, training loss: 0.19639045000076294 = 0.12790286540985107 + 0.01 * 6.848758220672607
Epoch 340, val loss: 0.691682755947113
Epoch 350, training loss: 0.17981863021850586 = 0.11138884723186493 + 0.01 * 6.842977523803711
Epoch 350, val loss: 0.693633496761322
Epoch 360, training loss: 0.1655958592891693 = 0.097264364361763 + 0.01 * 6.833149433135986
Epoch 360, val loss: 0.6971457004547119
Epoch 370, training loss: 0.1534414142370224 = 0.08519136905670166 + 0.01 * 6.825004577636719
Epoch 370, val loss: 0.7018550038337708
Epoch 380, training loss: 0.14301493763923645 = 0.07487042248249054 + 0.01 * 6.814452171325684
Epoch 380, val loss: 0.7075280547142029
Epoch 390, training loss: 0.13414260745048523 = 0.06602033227682114 + 0.01 * 6.812228679656982
Epoch 390, val loss: 0.7140340805053711
Epoch 400, training loss: 0.12640078365802765 = 0.05840209126472473 + 0.01 * 6.799869060516357
Epoch 400, val loss: 0.7210906147956848
Epoch 410, training loss: 0.11987033486366272 = 0.05183032900094986 + 0.01 * 6.804000377655029
Epoch 410, val loss: 0.728689968585968
Epoch 420, training loss: 0.11399196088314056 = 0.04617156460881233 + 0.01 * 6.782039165496826
Epoch 420, val loss: 0.7365376949310303
Epoch 430, training loss: 0.10903584957122803 = 0.04128530994057655 + 0.01 * 6.775054454803467
Epoch 430, val loss: 0.7446892261505127
Epoch 440, training loss: 0.10473485291004181 = 0.03705791011452675 + 0.01 * 6.76769495010376
Epoch 440, val loss: 0.7529283165931702
Epoch 450, training loss: 0.10105873644351959 = 0.03339202329516411 + 0.01 * 6.766671180725098
Epoch 450, val loss: 0.7612703442573547
Epoch 460, training loss: 0.09783477336168289 = 0.030211277306079865 + 0.01 * 6.762349605560303
Epoch 460, val loss: 0.769569456577301
Epoch 470, training loss: 0.0949767455458641 = 0.027446044608950615 + 0.01 * 6.753069877624512
Epoch 470, val loss: 0.7777994871139526
Epoch 480, training loss: 0.09252740442752838 = 0.025033438578248024 + 0.01 * 6.749396800994873
Epoch 480, val loss: 0.7859305739402771
Epoch 490, training loss: 0.09031634777784348 = 0.022920088842511177 + 0.01 * 6.739625930786133
Epoch 490, val loss: 0.793907880783081
Epoch 500, training loss: 0.08853529393672943 = 0.021061686798930168 + 0.01 * 6.747360706329346
Epoch 500, val loss: 0.801712155342102
Epoch 510, training loss: 0.08669198304414749 = 0.01942313462495804 + 0.01 * 6.726884841918945
Epoch 510, val loss: 0.8093699812889099
Epoch 520, training loss: 0.08522242307662964 = 0.017971953377127647 + 0.01 * 6.7250471115112305
Epoch 520, val loss: 0.81683748960495
Epoch 530, training loss: 0.08389157056808472 = 0.016681423410773277 + 0.01 * 6.721014976501465
Epoch 530, val loss: 0.8240916132926941
Epoch 540, training loss: 0.08263732492923737 = 0.015530490316450596 + 0.01 * 6.710683822631836
Epoch 540, val loss: 0.8312568664550781
Epoch 550, training loss: 0.08157963305711746 = 0.014500926248729229 + 0.01 * 6.707870960235596
Epoch 550, val loss: 0.8381422758102417
Epoch 560, training loss: 0.08063095062971115 = 0.013575899414718151 + 0.01 * 6.70550537109375
Epoch 560, val loss: 0.8448132872581482
Epoch 570, training loss: 0.07977080345153809 = 0.012740916572511196 + 0.01 * 6.702988624572754
Epoch 570, val loss: 0.851373016834259
Epoch 580, training loss: 0.07891874760389328 = 0.011985600925981998 + 0.01 * 6.693315029144287
Epoch 580, val loss: 0.8577123284339905
Epoch 590, training loss: 0.07814409583806992 = 0.011300317943096161 + 0.01 * 6.684377670288086
Epoch 590, val loss: 0.8639444708824158
Epoch 600, training loss: 0.0775034949183464 = 0.010676504112780094 + 0.01 * 6.682699680328369
Epoch 600, val loss: 0.869967520236969
Epoch 610, training loss: 0.076938197016716 = 0.010107187554240227 + 0.01 * 6.683101177215576
Epoch 610, val loss: 0.8758202195167542
Epoch 620, training loss: 0.07627537101507187 = 0.00958664994686842 + 0.01 * 6.668872356414795
Epoch 620, val loss: 0.8815744519233704
Epoch 630, training loss: 0.07561840116977692 = 0.0091090714558959 + 0.01 * 6.650933265686035
Epoch 630, val loss: 0.8871750235557556
Epoch 640, training loss: 0.07509226351976395 = 0.008669831790030003 + 0.01 * 6.642243385314941
Epoch 640, val loss: 0.8926035165786743
Epoch 650, training loss: 0.07496693730354309 = 0.008264857344329357 + 0.01 * 6.67020845413208
Epoch 650, val loss: 0.8979613780975342
Epoch 660, training loss: 0.07415002584457397 = 0.007890119217336178 + 0.01 * 6.625990867614746
Epoch 660, val loss: 0.9030663967132568
Epoch 670, training loss: 0.07388386875391006 = 0.007542389910668135 + 0.01 * 6.634148120880127
Epoch 670, val loss: 0.9081241488456726
Epoch 680, training loss: 0.07342644035816193 = 0.0072190165519714355 + 0.01 * 6.6207427978515625
Epoch 680, val loss: 0.913040816783905
Epoch 690, training loss: 0.07302138209342957 = 0.0069180759601294994 + 0.01 * 6.610331058502197
Epoch 690, val loss: 0.9178576469421387
Epoch 700, training loss: 0.07266345620155334 = 0.006636293604969978 + 0.01 * 6.602716445922852
Epoch 700, val loss: 0.9225820302963257
Epoch 710, training loss: 0.07242972403764725 = 0.006373549345880747 + 0.01 * 6.605617523193359
Epoch 710, val loss: 0.9271834492683411
Epoch 720, training loss: 0.07245583832263947 = 0.00612769415602088 + 0.01 * 6.632814884185791
Epoch 720, val loss: 0.9316936135292053
Epoch 730, training loss: 0.07180354744195938 = 0.005897061433643103 + 0.01 * 6.590648651123047
Epoch 730, val loss: 0.9361010789871216
Epoch 740, training loss: 0.07168283313512802 = 0.00568052614107728 + 0.01 * 6.600231170654297
Epoch 740, val loss: 0.9403626918792725
Epoch 750, training loss: 0.07125317305326462 = 0.00547716673463583 + 0.01 * 6.577600955963135
Epoch 750, val loss: 0.9445786476135254
Epoch 760, training loss: 0.07115752249956131 = 0.005285360384732485 + 0.01 * 6.587216377258301
Epoch 760, val loss: 0.9487093091011047
Epoch 770, training loss: 0.07088003307580948 = 0.005104854237288237 + 0.01 * 6.577517986297607
Epoch 770, val loss: 0.952774167060852
Epoch 780, training loss: 0.07062610983848572 = 0.004934538155794144 + 0.01 * 6.569156646728516
Epoch 780, val loss: 0.9566775560379028
Epoch 790, training loss: 0.07045549154281616 = 0.0047736503183841705 + 0.01 * 6.568183898925781
Epoch 790, val loss: 0.9605640769004822
Epoch 800, training loss: 0.07020270824432373 = 0.004621478263288736 + 0.01 * 6.558123588562012
Epoch 800, val loss: 0.964333176612854
Epoch 810, training loss: 0.06994056701660156 = 0.004477333277463913 + 0.01 * 6.546323299407959
Epoch 810, val loss: 0.9680745601654053
Epoch 820, training loss: 0.07018548250198364 = 0.004340691026300192 + 0.01 * 6.584478855133057
Epoch 820, val loss: 0.9717287421226501
Epoch 830, training loss: 0.06956411898136139 = 0.004211008548736572 + 0.01 * 6.535311222076416
Epoch 830, val loss: 0.9753378629684448
Epoch 840, training loss: 0.06950323283672333 = 0.004088305868208408 + 0.01 * 6.541492938995361
Epoch 840, val loss: 0.9788214564323425
Epoch 850, training loss: 0.06934547424316406 = 0.003971601836383343 + 0.01 * 6.537387371063232
Epoch 850, val loss: 0.9822605848312378
Epoch 860, training loss: 0.06920358538627625 = 0.00386095535941422 + 0.01 * 6.534262657165527
Epoch 860, val loss: 0.9856681823730469
Epoch 870, training loss: 0.06905798614025116 = 0.003755591344088316 + 0.01 * 6.530239582061768
Epoch 870, val loss: 0.9889996647834778
Epoch 880, training loss: 0.06899192184209824 = 0.003655566368252039 + 0.01 * 6.53363561630249
Epoch 880, val loss: 0.9922153949737549
Epoch 890, training loss: 0.0687166303396225 = 0.0035602108109742403 + 0.01 * 6.5156426429748535
Epoch 890, val loss: 0.9954413771629333
Epoch 900, training loss: 0.06856726109981537 = 0.003469261107966304 + 0.01 * 6.509800434112549
Epoch 900, val loss: 0.998568594455719
Epoch 910, training loss: 0.06859412044286728 = 0.003382455324754119 + 0.01 * 6.521166801452637
Epoch 910, val loss: 1.001654028892517
Epoch 920, training loss: 0.0684172660112381 = 0.0032994276843965054 + 0.01 * 6.511783599853516
Epoch 920, val loss: 1.0047084093093872
Epoch 930, training loss: 0.06843272596597672 = 0.00322031625546515 + 0.01 * 6.521240711212158
Epoch 930, val loss: 1.0077093839645386
Epoch 940, training loss: 0.06804008781909943 = 0.0031447322107851505 + 0.01 * 6.489535331726074
Epoch 940, val loss: 1.0106147527694702
Epoch 950, training loss: 0.06810341775417328 = 0.003072535851970315 + 0.01 * 6.503088474273682
Epoch 950, val loss: 1.0134732723236084
Epoch 960, training loss: 0.0678912103176117 = 0.0030033921357244253 + 0.01 * 6.4887824058532715
Epoch 960, val loss: 1.0163021087646484
Epoch 970, training loss: 0.06773919612169266 = 0.0029372796416282654 + 0.01 * 6.480191707611084
Epoch 970, val loss: 1.0190678834915161
Epoch 980, training loss: 0.06779235601425171 = 0.0028738004621118307 + 0.01 * 6.491855621337891
Epoch 980, val loss: 1.0217821598052979
Epoch 990, training loss: 0.06767042726278305 = 0.0028129166457802057 + 0.01 * 6.485751628875732
Epoch 990, val loss: 1.0244576930999756
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.6347
Flip ASR: 0.5822/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0171635150909424 = 1.933424711227417 + 0.01 * 8.373869895935059
Epoch 0, val loss: 1.9307565689086914
Epoch 10, training loss: 2.0072426795959473 = 1.9235050678253174 + 0.01 * 8.373770713806152
Epoch 10, val loss: 1.921003818511963
Epoch 20, training loss: 1.994825005531311 = 1.911090612411499 + 0.01 * 8.373438835144043
Epoch 20, val loss: 1.9083441495895386
Epoch 30, training loss: 1.9770910739898682 = 1.8933664560317993 + 0.01 * 8.372467994689941
Epoch 30, val loss: 1.8899470567703247
Epoch 40, training loss: 1.9510447978973389 = 1.867366909980774 + 0.01 * 8.367785453796387
Epoch 40, val loss: 1.8631830215454102
Epoch 50, training loss: 1.9152249097824097 = 1.8318760395050049 + 0.01 * 8.334888458251953
Epoch 50, val loss: 1.8282605409622192
Epoch 60, training loss: 1.8734487295150757 = 1.7918636798858643 + 0.01 * 8.158504486083984
Epoch 60, val loss: 1.7916193008422852
Epoch 70, training loss: 1.8298176527023315 = 1.7516124248504639 + 0.01 * 7.82052755355835
Epoch 70, val loss: 1.7552506923675537
Epoch 80, training loss: 1.773515224456787 = 1.6989283561706543 + 0.01 * 7.4586896896362305
Epoch 80, val loss: 1.707252860069275
Epoch 90, training loss: 1.6997284889221191 = 1.6274102926254272 + 0.01 * 7.231815814971924
Epoch 90, val loss: 1.6455837488174438
Epoch 100, training loss: 1.6092835664749146 = 1.538319706916809 + 0.01 * 7.096381664276123
Epoch 100, val loss: 1.5722652673721313
Epoch 110, training loss: 1.5121392011642456 = 1.4415661096572876 + 0.01 * 7.057308197021484
Epoch 110, val loss: 1.4939764738082886
Epoch 120, training loss: 1.4160865545272827 = 1.3458364009857178 + 0.01 * 7.02501106262207
Epoch 120, val loss: 1.4198737144470215
Epoch 130, training loss: 1.3229732513427734 = 1.253005027770996 + 0.01 * 6.99682092666626
Epoch 130, val loss: 1.3503855466842651
Epoch 140, training loss: 1.2326322793960571 = 1.1629358530044556 + 0.01 * 6.9696431159973145
Epoch 140, val loss: 1.2845747470855713
Epoch 150, training loss: 1.1452288627624512 = 1.0757575035095215 + 0.01 * 6.947137355804443
Epoch 150, val loss: 1.2216591835021973
Epoch 160, training loss: 1.0616559982299805 = 0.9923482537269592 + 0.01 * 6.9307780265808105
Epoch 160, val loss: 1.1615511178970337
Epoch 170, training loss: 0.9822593927383423 = 0.9130866527557373 + 0.01 * 6.917276382446289
Epoch 170, val loss: 1.1053979396820068
Epoch 180, training loss: 0.9058279991149902 = 0.8367652297019958 + 0.01 * 6.906278610229492
Epoch 180, val loss: 1.051489233970642
Epoch 190, training loss: 0.8305982351303101 = 0.7616557478904724 + 0.01 * 6.894245624542236
Epoch 190, val loss: 0.9982163906097412
Epoch 200, training loss: 0.7559507489204407 = 0.6871174573898315 + 0.01 * 6.883328914642334
Epoch 200, val loss: 0.9449228644371033
Epoch 210, training loss: 0.6837723851203918 = 0.6150317192077637 + 0.01 * 6.874068737030029
Epoch 210, val loss: 0.8932285904884338
Epoch 220, training loss: 0.6176567673683167 = 0.5489923357963562 + 0.01 * 6.866443157196045
Epoch 220, val loss: 0.8468223214149475
Epoch 230, training loss: 0.5597761869430542 = 0.49117735028266907 + 0.01 * 6.859881401062012
Epoch 230, val loss: 0.8080918192863464
Epoch 240, training loss: 0.5102470517158508 = 0.44172242283821106 + 0.01 * 6.8524627685546875
Epoch 240, val loss: 0.7782641649246216
Epoch 250, training loss: 0.4676549434661865 = 0.399196982383728 + 0.01 * 6.845797061920166
Epoch 250, val loss: 0.7562713027000427
Epoch 260, training loss: 0.4299500286579132 = 0.36156323552131653 + 0.01 * 6.838678359985352
Epoch 260, val loss: 0.739992618560791
Epoch 270, training loss: 0.3953210115432739 = 0.32702457904815674 + 0.01 * 6.829644203186035
Epoch 270, val loss: 0.7277845144271851
Epoch 280, training loss: 0.36268216371536255 = 0.29439476132392883 + 0.01 * 6.828741550445557
Epoch 280, val loss: 0.7182540893554688
Epoch 290, training loss: 0.33148670196533203 = 0.2633235454559326 + 0.01 * 6.8163161277771
Epoch 290, val loss: 0.7111026048660278
Epoch 300, training loss: 0.30205512046813965 = 0.23394155502319336 + 0.01 * 6.811356067657471
Epoch 300, val loss: 0.7060762643814087
Epoch 310, training loss: 0.27471059560775757 = 0.20666314661502838 + 0.01 * 6.804745674133301
Epoch 310, val loss: 0.7037767767906189
Epoch 320, training loss: 0.24983590841293335 = 0.18185672163963318 + 0.01 * 6.797918319702148
Epoch 320, val loss: 0.7045742273330688
Epoch 330, training loss: 0.22761544585227966 = 0.15967710316181183 + 0.01 * 6.793835163116455
Epoch 330, val loss: 0.7085410356521606
Epoch 340, training loss: 0.20795586705207825 = 0.14008378982543945 + 0.01 * 6.787208557128906
Epoch 340, val loss: 0.7154421806335449
Epoch 350, training loss: 0.1907815933227539 = 0.12293543666601181 + 0.01 * 6.784615516662598
Epoch 350, val loss: 0.7251140475273132
Epoch 360, training loss: 0.17584750056266785 = 0.10801270604133606 + 0.01 * 6.783480644226074
Epoch 360, val loss: 0.7368952035903931
Epoch 370, training loss: 0.1628459095954895 = 0.09505514800548553 + 0.01 * 6.779076099395752
Epoch 370, val loss: 0.7504345178604126
Epoch 380, training loss: 0.1515713930130005 = 0.08382304757833481 + 0.01 * 6.774835109710693
Epoch 380, val loss: 0.7652227878570557
Epoch 390, training loss: 0.14182862639427185 = 0.07408968359231949 + 0.01 * 6.773894786834717
Epoch 390, val loss: 0.7808398008346558
Epoch 400, training loss: 0.13335302472114563 = 0.065656878054142 + 0.01 * 6.769615173339844
Epoch 400, val loss: 0.7969583868980408
Epoch 410, training loss: 0.1260252147912979 = 0.058361757546663284 + 0.01 * 6.766345977783203
Epoch 410, val loss: 0.8132603168487549
Epoch 420, training loss: 0.11969749629497528 = 0.052055444568395615 + 0.01 * 6.7642059326171875
Epoch 420, val loss: 0.8294751644134521
Epoch 430, training loss: 0.11422047019004822 = 0.046603862196207047 + 0.01 * 6.761661052703857
Epoch 430, val loss: 0.8454459309577942
Epoch 440, training loss: 0.10947292298078537 = 0.04188656806945801 + 0.01 * 6.758635997772217
Epoch 440, val loss: 0.8610170483589172
Epoch 450, training loss: 0.10547370463609695 = 0.03779636323451996 + 0.01 * 6.767734050750732
Epoch 450, val loss: 0.8762310743331909
Epoch 460, training loss: 0.1018141508102417 = 0.03424343839287758 + 0.01 * 6.757071495056152
Epoch 460, val loss: 0.8910784721374512
Epoch 470, training loss: 0.09865503758192062 = 0.03114449791610241 + 0.01 * 6.751054286956787
Epoch 470, val loss: 0.9055617451667786
Epoch 480, training loss: 0.09592816233634949 = 0.028430460020899773 + 0.01 * 6.749770164489746
Epoch 480, val loss: 0.9196407198905945
Epoch 490, training loss: 0.09351254999637604 = 0.02604464814066887 + 0.01 * 6.746790409088135
Epoch 490, val loss: 0.9333136081695557
Epoch 500, training loss: 0.09136711061000824 = 0.0239397082477808 + 0.01 * 6.742740631103516
Epoch 500, val loss: 0.9466364979743958
Epoch 510, training loss: 0.08947768062353134 = 0.02207476645708084 + 0.01 * 6.740291595458984
Epoch 510, val loss: 0.9595634341239929
Epoch 520, training loss: 0.08780252933502197 = 0.020417027175426483 + 0.01 * 6.738550662994385
Epoch 520, val loss: 0.9721747636795044
Epoch 530, training loss: 0.08628800511360168 = 0.01893915981054306 + 0.01 * 6.734884738922119
Epoch 530, val loss: 0.9844008684158325
Epoch 540, training loss: 0.08495260775089264 = 0.017616186290979385 + 0.01 * 6.733642101287842
Epoch 540, val loss: 0.996296763420105
Epoch 550, training loss: 0.08373703062534332 = 0.01642846129834652 + 0.01 * 6.730856895446777
Epoch 550, val loss: 1.0078877210617065
Epoch 560, training loss: 0.08260352164506912 = 0.015358631499111652 + 0.01 * 6.724488735198975
Epoch 560, val loss: 1.0191386938095093
Epoch 570, training loss: 0.0816381424665451 = 0.014391712844371796 + 0.01 * 6.724642753601074
Epoch 570, val loss: 1.030068039894104
Epoch 580, training loss: 0.08071775734424591 = 0.013515414670109749 + 0.01 * 6.720234394073486
Epoch 580, val loss: 1.0407092571258545
Epoch 590, training loss: 0.07992175966501236 = 0.012718871235847473 + 0.01 * 6.72028923034668
Epoch 590, val loss: 1.0510541200637817
Epoch 600, training loss: 0.07918436080217361 = 0.011993291787803173 + 0.01 * 6.719107627868652
Epoch 600, val loss: 1.0610637664794922
Epoch 610, training loss: 0.07842027395963669 = 0.011331073939800262 + 0.01 * 6.708920001983643
Epoch 610, val loss: 1.0708274841308594
Epoch 620, training loss: 0.07780870795249939 = 0.010724766179919243 + 0.01 * 6.7083940505981445
Epoch 620, val loss: 1.0803385972976685
Epoch 630, training loss: 0.07720347493886948 = 0.010168210603296757 + 0.01 * 6.703526973724365
Epoch 630, val loss: 1.089536190032959
Epoch 640, training loss: 0.0766577422618866 = 0.009656241163611412 + 0.01 * 6.700150012969971
Epoch 640, val loss: 1.0985257625579834
Epoch 650, training loss: 0.0761551484465599 = 0.009184522554278374 + 0.01 * 6.6970624923706055
Epoch 650, val loss: 1.1073004007339478
Epoch 660, training loss: 0.07571574300527573 = 0.00874923262745142 + 0.01 * 6.696650981903076
Epoch 660, val loss: 1.1158088445663452
Epoch 670, training loss: 0.0752573013305664 = 0.008346294984221458 + 0.01 * 6.691100597381592
Epoch 670, val loss: 1.1241151094436646
Epoch 680, training loss: 0.07484536617994308 = 0.007972384802997112 + 0.01 * 6.687298774719238
Epoch 680, val loss: 1.1321842670440674
Epoch 690, training loss: 0.07453601807355881 = 0.007625342346727848 + 0.01 * 6.691068172454834
Epoch 690, val loss: 1.140107274055481
Epoch 700, training loss: 0.0741179957985878 = 0.00730258971452713 + 0.01 * 6.681540489196777
Epoch 700, val loss: 1.1477683782577515
Epoch 710, training loss: 0.07376428693532944 = 0.007002032361924648 + 0.01 * 6.676225185394287
Epoch 710, val loss: 1.155220866203308
Epoch 720, training loss: 0.0734575018286705 = 0.006721597630530596 + 0.01 * 6.673590183258057
Epoch 720, val loss: 1.1625248193740845
Epoch 730, training loss: 0.07321614027023315 = 0.006459295284003019 + 0.01 * 6.675684928894043
Epoch 730, val loss: 1.1696051359176636
Epoch 740, training loss: 0.07289250195026398 = 0.006213945336639881 + 0.01 * 6.667855739593506
Epoch 740, val loss: 1.176522970199585
Epoch 750, training loss: 0.07264798134565353 = 0.005983952898532152 + 0.01 * 6.666402339935303
Epoch 750, val loss: 1.1832140684127808
Epoch 760, training loss: 0.07234109193086624 = 0.00576810585334897 + 0.01 * 6.657298564910889
Epoch 760, val loss: 1.189809799194336
Epoch 770, training loss: 0.07227295637130737 = 0.005565199069678783 + 0.01 * 6.6707763671875
Epoch 770, val loss: 1.1962323188781738
Epoch 780, training loss: 0.07193964719772339 = 0.005374480038881302 + 0.01 * 6.656517505645752
Epoch 780, val loss: 1.2024590969085693
Epoch 790, training loss: 0.07184183597564697 = 0.005194790195673704 + 0.01 * 6.6647047996521
Epoch 790, val loss: 1.208562970161438
Epoch 800, training loss: 0.07139157503843307 = 0.005025525111705065 + 0.01 * 6.6366047859191895
Epoch 800, val loss: 1.2144702672958374
Epoch 810, training loss: 0.07115396857261658 = 0.004865617956966162 + 0.01 * 6.628835678100586
Epoch 810, val loss: 1.2202720642089844
Epoch 820, training loss: 0.07114585489034653 = 0.004714407492429018 + 0.01 * 6.6431450843811035
Epoch 820, val loss: 1.2259693145751953
Epoch 830, training loss: 0.07093986123800278 = 0.004571930970996618 + 0.01 * 6.63679313659668
Epoch 830, val loss: 1.2313988208770752
Epoch 840, training loss: 0.07057356834411621 = 0.004436912015080452 + 0.01 * 6.61366605758667
Epoch 840, val loss: 1.2367620468139648
Epoch 850, training loss: 0.07041741907596588 = 0.004308792762458324 + 0.01 * 6.610863208770752
Epoch 850, val loss: 1.2419909238815308
Epoch 860, training loss: 0.0703873410820961 = 0.004187392070889473 + 0.01 * 6.619995594024658
Epoch 860, val loss: 1.2470364570617676
Epoch 870, training loss: 0.07022342085838318 = 0.004071884322911501 + 0.01 * 6.615153789520264
Epoch 870, val loss: 1.251979947090149
Epoch 880, training loss: 0.0700758844614029 = 0.003962238319218159 + 0.01 * 6.611364841461182
Epoch 880, val loss: 1.2567981481552124
Epoch 890, training loss: 0.07004677504301071 = 0.0038581322878599167 + 0.01 * 6.6188645362854
Epoch 890, val loss: 1.2615292072296143
Epoch 900, training loss: 0.06974077969789505 = 0.003758966224268079 + 0.01 * 6.59818172454834
Epoch 900, val loss: 1.2660218477249146
Epoch 910, training loss: 0.0694223940372467 = 0.0036643599160015583 + 0.01 * 6.575803279876709
Epoch 910, val loss: 1.2704932689666748
Epoch 920, training loss: 0.06976496428251266 = 0.0035741236060857773 + 0.01 * 6.619084358215332
Epoch 920, val loss: 1.27482271194458
Epoch 930, training loss: 0.06918755918741226 = 0.0034879539161920547 + 0.01 * 6.569960594177246
Epoch 930, val loss: 1.2789913415908813
Epoch 940, training loss: 0.0691666379570961 = 0.0034055730793625116 + 0.01 * 6.576107025146484
Epoch 940, val loss: 1.2832086086273193
Epoch 950, training loss: 0.06907472014427185 = 0.0033269200939685106 + 0.01 * 6.574779987335205
Epoch 950, val loss: 1.2872244119644165
Epoch 960, training loss: 0.06876524537801743 = 0.0032515907660126686 + 0.01 * 6.551365852355957
Epoch 960, val loss: 1.2912490367889404
Epoch 970, training loss: 0.06914466619491577 = 0.0031795098911970854 + 0.01 * 6.59651517868042
Epoch 970, val loss: 1.295153260231018
Epoch 980, training loss: 0.06860185414552689 = 0.0031104357913136482 + 0.01 * 6.5491414070129395
Epoch 980, val loss: 1.298891305923462
Epoch 990, training loss: 0.0686861202120781 = 0.0030441887211054564 + 0.01 * 6.564193248748779
Epoch 990, val loss: 1.3026918172836304
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.6863
Flip ASR: 0.6667/225 nodes
The final ASR:0.64453, 0.03092, Accuracy:0.83086, 0.01364
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11646])
remove edge: torch.Size([2, 9590])
updated graph: torch.Size([2, 10680])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9963
Flip ASR: 0.9956/225 nodes
The final ASR:0.98155, 0.01044, Accuracy:0.83210, 0.00761
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.036311388015747 = 1.9525723457336426 + 0.01 * 8.373905181884766
Epoch 0, val loss: 1.9596830606460571
Epoch 10, training loss: 2.026323080062866 = 1.94258451461792 + 0.01 * 8.373847007751465
Epoch 10, val loss: 1.950287938117981
Epoch 20, training loss: 2.014155387878418 = 1.9304189682006836 + 0.01 * 8.373641014099121
Epoch 20, val loss: 1.93840491771698
Epoch 30, training loss: 1.9971892833709717 = 1.9134595394134521 + 0.01 * 8.372976303100586
Epoch 30, val loss: 1.9214812517166138
Epoch 40, training loss: 1.9719740152359009 = 1.888278841972351 + 0.01 * 8.369514465332031
Epoch 40, val loss: 1.8964135646820068
Epoch 50, training loss: 1.935680866241455 = 1.8522512912750244 + 0.01 * 8.3429536819458
Epoch 50, val loss: 1.8618483543395996
Epoch 60, training loss: 1.8908205032348633 = 1.8088346719741821 + 0.01 * 8.198588371276855
Epoch 60, val loss: 1.8234786987304688
Epoch 70, training loss: 1.8459265232086182 = 1.7674142122268677 + 0.01 * 7.851232528686523
Epoch 70, val loss: 1.788792371749878
Epoch 80, training loss: 1.796994686126709 = 1.719582438468933 + 0.01 * 7.741220951080322
Epoch 80, val loss: 1.7446844577789307
Epoch 90, training loss: 1.729130744934082 = 1.6528410911560059 + 0.01 * 7.628960132598877
Epoch 90, val loss: 1.6843969821929932
Epoch 100, training loss: 1.640146255493164 = 1.5650687217712402 + 0.01 * 7.507754802703857
Epoch 100, val loss: 1.610235333442688
Epoch 110, training loss: 1.536204218864441 = 1.463215947151184 + 0.01 * 7.298830509185791
Epoch 110, val loss: 1.5261099338531494
Epoch 120, training loss: 1.4322186708450317 = 1.3602429628372192 + 0.01 * 7.197573661804199
Epoch 120, val loss: 1.4433478116989136
Epoch 130, training loss: 1.3345917463302612 = 1.2630044221878052 + 0.01 * 7.158730983734131
Epoch 130, val loss: 1.3687232732772827
Epoch 140, training loss: 1.2435473203659058 = 1.1724400520324707 + 0.01 * 7.110722064971924
Epoch 140, val loss: 1.3029701709747314
Epoch 150, training loss: 1.1586345434188843 = 1.0880072116851807 + 0.01 * 7.062728404998779
Epoch 150, val loss: 1.243463158607483
Epoch 160, training loss: 1.0790480375289917 = 1.0088555812835693 + 0.01 * 7.019246578216553
Epoch 160, val loss: 1.18829345703125
Epoch 170, training loss: 1.0045782327651978 = 0.9346515536308289 + 0.01 * 6.992666721343994
Epoch 170, val loss: 1.136702537536621
Epoch 180, training loss: 0.9339737892150879 = 0.8642158508300781 + 0.01 * 6.975796222686768
Epoch 180, val loss: 1.087857723236084
Epoch 190, training loss: 0.864829421043396 = 0.7952673435211182 + 0.01 * 6.956206321716309
Epoch 190, val loss: 1.040614366531372
Epoch 200, training loss: 0.795502245426178 = 0.7261784076690674 + 0.01 * 6.932384014129639
Epoch 200, val loss: 0.9945415258407593
Epoch 210, training loss: 0.7262164354324341 = 0.6571289300918579 + 0.01 * 6.908750534057617
Epoch 210, val loss: 0.9505625367164612
Epoch 220, training loss: 0.6591247320175171 = 0.5902575850486755 + 0.01 * 6.886712074279785
Epoch 220, val loss: 0.9113447666168213
Epoch 230, training loss: 0.5968207716941833 = 0.528120756149292 + 0.01 * 6.870002269744873
Epoch 230, val loss: 0.8798971176147461
Epoch 240, training loss: 0.5408579707145691 = 0.4722995460033417 + 0.01 * 6.855842590332031
Epoch 240, val loss: 0.8579603433609009
Epoch 250, training loss: 0.491271048784256 = 0.4228212535381317 + 0.01 * 6.844979286193848
Epoch 250, val loss: 0.8449800610542297
Epoch 260, training loss: 0.4472138583660126 = 0.3788605034351349 + 0.01 * 6.835336685180664
Epoch 260, val loss: 0.8398475646972656
Epoch 270, training loss: 0.4076899290084839 = 0.3393878638744354 + 0.01 * 6.830206871032715
Epoch 270, val loss: 0.841042697429657
Epoch 280, training loss: 0.37179476022720337 = 0.3035946488380432 + 0.01 * 6.820013046264648
Epoch 280, val loss: 0.8471396565437317
Epoch 290, training loss: 0.33910903334617615 = 0.27095523476600647 + 0.01 * 6.815379619598389
Epoch 290, val loss: 0.8571421504020691
Epoch 300, training loss: 0.30929750204086304 = 0.24118416011333466 + 0.01 * 6.811332702636719
Epoch 300, val loss: 0.8702583909034729
Epoch 310, training loss: 0.2822110950946808 = 0.21412411332130432 + 0.01 * 6.8086981773376465
Epoch 310, val loss: 0.8858457207679749
Epoch 320, training loss: 0.257839560508728 = 0.1897207349538803 + 0.01 * 6.811883926391602
Epoch 320, val loss: 0.9034507870674133
Epoch 330, training loss: 0.2360035479068756 = 0.1679273545742035 + 0.01 * 6.807619571685791
Epoch 330, val loss: 0.9226517677307129
Epoch 340, training loss: 0.21666991710662842 = 0.14862261712551117 + 0.01 * 6.804729461669922
Epoch 340, val loss: 0.9432047605514526
Epoch 350, training loss: 0.19965818524360657 = 0.13162343204021454 + 0.01 * 6.8034749031066895
Epoch 350, val loss: 0.9647427201271057
Epoch 360, training loss: 0.18474474549293518 = 0.11672540754079819 + 0.01 * 6.801934242248535
Epoch 360, val loss: 0.9869409799575806
Epoch 370, training loss: 0.17171527445316315 = 0.10371189564466476 + 0.01 * 6.800338268280029
Epoch 370, val loss: 1.009407639503479
Epoch 380, training loss: 0.16035807132720947 = 0.09237229079008102 + 0.01 * 6.798579216003418
Epoch 380, val loss: 1.0317637920379639
Epoch 390, training loss: 0.15046891570091248 = 0.08250291645526886 + 0.01 * 6.796600341796875
Epoch 390, val loss: 1.0537691116333008
Epoch 400, training loss: 0.1418602466583252 = 0.07391607761383057 + 0.01 * 6.794417858123779
Epoch 400, val loss: 1.0752795934677124
Epoch 410, training loss: 0.13442683219909668 = 0.0664365142583847 + 0.01 * 6.7990312576293945
Epoch 410, val loss: 1.0961493253707886
Epoch 420, training loss: 0.12782302498817444 = 0.05990448221564293 + 0.01 * 6.791854381561279
Epoch 420, val loss: 1.1163339614868164
Epoch 430, training loss: 0.12205439805984497 = 0.054179299622774124 + 0.01 * 6.787510395050049
Epoch 430, val loss: 1.136060118675232
Epoch 440, training loss: 0.11697941273450851 = 0.049140483140945435 + 0.01 * 6.78389310836792
Epoch 440, val loss: 1.1551496982574463
Epoch 450, training loss: 0.11249426007270813 = 0.044689420610666275 + 0.01 * 6.780484676361084
Epoch 450, val loss: 1.1737899780273438
Epoch 460, training loss: 0.10854499042034149 = 0.0407467745244503 + 0.01 * 6.779821395874023
Epoch 460, val loss: 1.1918903589248657
Epoch 470, training loss: 0.10503996908664703 = 0.03724786266684532 + 0.01 * 6.779210567474365
Epoch 470, val loss: 1.2095086574554443
Epoch 480, training loss: 0.10184398293495178 = 0.03413863852620125 + 0.01 * 6.770534038543701
Epoch 480, val loss: 1.2265928983688354
Epoch 490, training loss: 0.0990307480096817 = 0.031368155032396317 + 0.01 * 6.76625919342041
Epoch 490, val loss: 1.2433209419250488
Epoch 500, training loss: 0.09651192277669907 = 0.028893962502479553 + 0.01 * 6.761795997619629
Epoch 500, val loss: 1.259531855583191
Epoch 510, training loss: 0.09426289796829224 = 0.02667868509888649 + 0.01 * 6.758421421051025
Epoch 510, val loss: 1.2753723859786987
Epoch 520, training loss: 0.09230543673038483 = 0.02469157986342907 + 0.01 * 6.761385440826416
Epoch 520, val loss: 1.290655493736267
Epoch 530, training loss: 0.09042167663574219 = 0.022905640304088593 + 0.01 * 6.751604080200195
Epoch 530, val loss: 1.3056248426437378
Epoch 540, training loss: 0.08872473239898682 = 0.021296748891472816 + 0.01 * 6.742798328399658
Epoch 540, val loss: 1.3201720714569092
Epoch 550, training loss: 0.08732573688030243 = 0.019843891263008118 + 0.01 * 6.748184680938721
Epoch 550, val loss: 1.3342756032943726
Epoch 560, training loss: 0.08591115474700928 = 0.01852947100996971 + 0.01 * 6.738168239593506
Epoch 560, val loss: 1.3479992151260376
Epoch 570, training loss: 0.08468736708164215 = 0.017337700352072716 + 0.01 * 6.73496675491333
Epoch 570, val loss: 1.361318826675415
Epoch 580, training loss: 0.08354207128286362 = 0.016255134716629982 + 0.01 * 6.728693962097168
Epoch 580, val loss: 1.3743127584457397
Epoch 590, training loss: 0.08248221129179001 = 0.015269541181623936 + 0.01 * 6.721267223358154
Epoch 590, val loss: 1.3868786096572876
Epoch 600, training loss: 0.08154429495334625 = 0.014370263554155827 + 0.01 * 6.717403411865234
Epoch 600, val loss: 1.3991144895553589
Epoch 610, training loss: 0.08070028573274612 = 0.013548322021961212 + 0.01 * 6.71519660949707
Epoch 610, val loss: 1.411036491394043
Epoch 620, training loss: 0.07978104799985886 = 0.01279535610228777 + 0.01 * 6.698568820953369
Epoch 620, val loss: 1.4226138591766357
Epoch 630, training loss: 0.07906553149223328 = 0.01210402324795723 + 0.01 * 6.696150302886963
Epoch 630, val loss: 1.4339039325714111
Epoch 640, training loss: 0.078365258872509 = 0.011467800475656986 + 0.01 * 6.689746379852295
Epoch 640, val loss: 1.4449201822280884
Epoch 650, training loss: 0.07787451148033142 = 0.01088184304535389 + 0.01 * 6.699267387390137
Epoch 650, val loss: 1.455519199371338
Epoch 660, training loss: 0.07721053063869476 = 0.010341819375753403 + 0.01 * 6.686870574951172
Epoch 660, val loss: 1.4658796787261963
Epoch 670, training loss: 0.07660087198019028 = 0.00984262116253376 + 0.01 * 6.675825119018555
Epoch 670, val loss: 1.475993275642395
Epoch 680, training loss: 0.07607124745845795 = 0.009380178526043892 + 0.01 * 6.669106960296631
Epoch 680, val loss: 1.4857240915298462
Epoch 690, training loss: 0.07564251124858856 = 0.008950695395469666 + 0.01 * 6.669181823730469
Epoch 690, val loss: 1.4953503608703613
Epoch 700, training loss: 0.07531558722257614 = 0.008551694452762604 + 0.01 * 6.676389217376709
Epoch 700, val loss: 1.5045610666275024
Epoch 710, training loss: 0.07473940402269363 = 0.008180527947843075 + 0.01 * 6.655887603759766
Epoch 710, val loss: 1.5135904550552368
Epoch 720, training loss: 0.07434481382369995 = 0.00783450622111559 + 0.01 * 6.651030540466309
Epoch 720, val loss: 1.5224136114120483
Epoch 730, training loss: 0.07401253283023834 = 0.0075112697668373585 + 0.01 * 6.650125980377197
Epoch 730, val loss: 1.5309956073760986
Epoch 740, training loss: 0.07374230772256851 = 0.007208868861198425 + 0.01 * 6.65334415435791
Epoch 740, val loss: 1.5393317937850952
Epoch 750, training loss: 0.07336071133613586 = 0.006925805006176233 + 0.01 * 6.643490314483643
Epoch 750, val loss: 1.547467827796936
Epoch 760, training loss: 0.07320989668369293 = 0.0066604698076844215 + 0.01 * 6.654942989349365
Epoch 760, val loss: 1.5553556680679321
Epoch 770, training loss: 0.07279161363840103 = 0.006411408539861441 + 0.01 * 6.6380205154418945
Epoch 770, val loss: 1.563125491142273
Epoch 780, training loss: 0.07245075702667236 = 0.006177497562021017 + 0.01 * 6.627326011657715
Epoch 780, val loss: 1.5706236362457275
Epoch 790, training loss: 0.07229597866535187 = 0.005957364570349455 + 0.01 * 6.633861541748047
Epoch 790, val loss: 1.577953815460205
Epoch 800, training loss: 0.07201803475618362 = 0.005749893840402365 + 0.01 * 6.626813888549805
Epoch 800, val loss: 1.5851324796676636
Epoch 810, training loss: 0.07175912708044052 = 0.005554198753088713 + 0.01 * 6.620492935180664
Epoch 810, val loss: 1.5920766592025757
Epoch 820, training loss: 0.07146381586790085 = 0.005369494669139385 + 0.01 * 6.609432697296143
Epoch 820, val loss: 1.5989043712615967
Epoch 830, training loss: 0.07127153128385544 = 0.005194743163883686 + 0.01 * 6.607678413391113
Epoch 830, val loss: 1.605589747428894
Epoch 840, training loss: 0.07108642905950546 = 0.0050294483080506325 + 0.01 * 6.605698585510254
Epoch 840, val loss: 1.6120190620422363
Epoch 850, training loss: 0.07093576341867447 = 0.004873288795351982 + 0.01 * 6.606247425079346
Epoch 850, val loss: 1.6183663606643677
Epoch 860, training loss: 0.0707060769200325 = 0.004725312814116478 + 0.01 * 6.598076343536377
Epoch 860, val loss: 1.6245262622833252
Epoch 870, training loss: 0.07064178586006165 = 0.004584807902574539 + 0.01 * 6.605698108673096
Epoch 870, val loss: 1.6305311918258667
Epoch 880, training loss: 0.07039601355791092 = 0.00445141363888979 + 0.01 * 6.594459533691406
Epoch 880, val loss: 1.6364362239837646
Epoch 890, training loss: 0.0701776072382927 = 0.004324679262936115 + 0.01 * 6.585292816162109
Epoch 890, val loss: 1.642216444015503
Epoch 900, training loss: 0.07021639496088028 = 0.004203996621072292 + 0.01 * 6.601240158081055
Epoch 900, val loss: 1.6478331089019775
Epoch 910, training loss: 0.06996002793312073 = 0.004089166410267353 + 0.01 * 6.587086200714111
Epoch 910, val loss: 1.6533911228179932
Epoch 920, training loss: 0.0697677880525589 = 0.003979642875492573 + 0.01 * 6.5788140296936035
Epoch 920, val loss: 1.6587620973587036
Epoch 930, training loss: 0.06964587420225143 = 0.0038753203116357327 + 0.01 * 6.577054977416992
Epoch 930, val loss: 1.6640574932098389
Epoch 940, training loss: 0.06953857094049454 = 0.0037757104728370905 + 0.01 * 6.576286315917969
Epoch 940, val loss: 1.6692092418670654
Epoch 950, training loss: 0.06942939758300781 = 0.0036806820426136255 + 0.01 * 6.57487154006958
Epoch 950, val loss: 1.6742247343063354
Epoch 960, training loss: 0.0693218931555748 = 0.0035898739006370306 + 0.01 * 6.573201656341553
Epoch 960, val loss: 1.6791304349899292
Epoch 970, training loss: 0.06922167539596558 = 0.003503171494230628 + 0.01 * 6.571850299835205
Epoch 970, val loss: 1.6840375661849976
Epoch 980, training loss: 0.0690227523446083 = 0.003420037217438221 + 0.01 * 6.560271263122559
Epoch 980, val loss: 1.6886587142944336
Epoch 990, training loss: 0.0689309760928154 = 0.0033404454588890076 + 0.01 * 6.559053421020508
Epoch 990, val loss: 1.6933115720748901
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0255579948425293 = 1.9418189525604248 + 0.01 * 8.373894691467285
Epoch 0, val loss: 1.9424031972885132
Epoch 10, training loss: 2.0155093669891357 = 1.931770920753479 + 0.01 * 8.37384033203125
Epoch 10, val loss: 1.9324204921722412
Epoch 20, training loss: 2.0034284591674805 = 1.919692039489746 + 0.01 * 8.373637199401855
Epoch 20, val loss: 1.920243740081787
Epoch 30, training loss: 1.986729383468628 = 1.9029990434646606 + 0.01 * 8.373040199279785
Epoch 30, val loss: 1.9034807682037354
Epoch 40, training loss: 1.9622222185134888 = 1.8785223960876465 + 0.01 * 8.369980812072754
Epoch 40, val loss: 1.8793926239013672
Epoch 50, training loss: 1.9272910356521606 = 1.8438376188278198 + 0.01 * 8.34534740447998
Epoch 50, val loss: 1.8468159437179565
Epoch 60, training loss: 1.8841173648834229 = 1.80223548412323 + 0.01 * 8.188190460205078
Epoch 60, val loss: 1.8113305568695068
Epoch 70, training loss: 1.840151071548462 = 1.7618463039398193 + 0.01 * 7.830471038818359
Epoch 70, val loss: 1.779411792755127
Epoch 80, training loss: 1.7897225618362427 = 1.7137141227722168 + 0.01 * 7.600838661193848
Epoch 80, val loss: 1.7368909120559692
Epoch 90, training loss: 1.722244143486023 = 1.6480399370193481 + 0.01 * 7.420417308807373
Epoch 90, val loss: 1.6784471273422241
Epoch 100, training loss: 1.635192632675171 = 1.5620286464691162 + 0.01 * 7.316395282745361
Epoch 100, val loss: 1.6057137250900269
Epoch 110, training loss: 1.534557580947876 = 1.4621505737304688 + 0.01 * 7.240702152252197
Epoch 110, val loss: 1.5231657028198242
Epoch 120, training loss: 1.4294469356536865 = 1.357939600944519 + 0.01 * 7.150728702545166
Epoch 120, val loss: 1.4368566274642944
Epoch 130, training loss: 1.3231712579727173 = 1.252640962600708 + 0.01 * 7.053025245666504
Epoch 130, val loss: 1.3507311344146729
Epoch 140, training loss: 1.215057611465454 = 1.145356297492981 + 0.01 * 6.97013521194458
Epoch 140, val loss: 1.2642179727554321
Epoch 150, training loss: 1.105118751525879 = 1.0358699560165405 + 0.01 * 6.924882411956787
Epoch 150, val loss: 1.1759800910949707
Epoch 160, training loss: 0.9955230951309204 = 0.9263930320739746 + 0.01 * 6.913005828857422
Epoch 160, val loss: 1.0880929231643677
Epoch 170, training loss: 0.8912156820297241 = 0.8221465349197388 + 0.01 * 6.9069132804870605
Epoch 170, val loss: 1.0056654214859009
Epoch 180, training loss: 0.7970621585845947 = 0.7280321717262268 + 0.01 * 6.902996063232422
Epoch 180, val loss: 0.9341282844543457
Epoch 190, training loss: 0.7145124077796936 = 0.6455264687538147 + 0.01 * 6.898596286773682
Epoch 190, val loss: 0.8756416440010071
Epoch 200, training loss: 0.6424654126167297 = 0.5735413432121277 + 0.01 * 6.89240837097168
Epoch 200, val loss: 0.8299015760421753
Epoch 210, training loss: 0.5791917443275452 = 0.5103484988212585 + 0.01 * 6.884322166442871
Epoch 210, val loss: 0.7948665022850037
Epoch 220, training loss: 0.5233662724494934 = 0.4546228349208832 + 0.01 * 6.874345302581787
Epoch 220, val loss: 0.7687399983406067
Epoch 230, training loss: 0.4739893078804016 = 0.405346542596817 + 0.01 * 6.864276885986328
Epoch 230, val loss: 0.749596893787384
Epoch 240, training loss: 0.4302193820476532 = 0.36169812083244324 + 0.01 * 6.852125644683838
Epoch 240, val loss: 0.7360432147979736
Epoch 250, training loss: 0.39134424924850464 = 0.3229053020477295 + 0.01 * 6.84389591217041
Epoch 250, val loss: 0.7273339033126831
Epoch 260, training loss: 0.35658079385757446 = 0.2882450222969055 + 0.01 * 6.833577632904053
Epoch 260, val loss: 0.7228565812110901
Epoch 270, training loss: 0.3252900242805481 = 0.25705111026763916 + 0.01 * 6.823892116546631
Epoch 270, val loss: 0.722183346748352
Epoch 280, training loss: 0.29695820808410645 = 0.22879919409751892 + 0.01 * 6.815902233123779
Epoch 280, val loss: 0.7246211171150208
Epoch 290, training loss: 0.271293967962265 = 0.2031390517950058 + 0.01 * 6.815492630004883
Epoch 290, val loss: 0.729671835899353
Epoch 300, training loss: 0.247966468334198 = 0.1799001693725586 + 0.01 * 6.806630611419678
Epoch 300, val loss: 0.737066924571991
Epoch 310, training loss: 0.22701582312583923 = 0.15898706018924713 + 0.01 * 6.802875518798828
Epoch 310, val loss: 0.7466017007827759
Epoch 320, training loss: 0.2083214372396469 = 0.14031320810317993 + 0.01 * 6.800822734832764
Epoch 320, val loss: 0.758036196231842
Epoch 330, training loss: 0.19173923134803772 = 0.12376543879508972 + 0.01 * 6.797379493713379
Epoch 330, val loss: 0.7710292339324951
Epoch 340, training loss: 0.17715150117874146 = 0.10919642448425293 + 0.01 * 6.795508861541748
Epoch 340, val loss: 0.7852272987365723
Epoch 350, training loss: 0.16435274481773376 = 0.09643297642469406 + 0.01 * 6.7919769287109375
Epoch 350, val loss: 0.8002681732177734
Epoch 360, training loss: 0.15318557620048523 = 0.08529532700777054 + 0.01 * 6.789024353027344
Epoch 360, val loss: 0.8157364726066589
Epoch 370, training loss: 0.14345362782478333 = 0.07560116797685623 + 0.01 * 6.785246849060059
Epoch 370, val loss: 0.8312799334526062
Epoch 380, training loss: 0.13502073287963867 = 0.06717626005411148 + 0.01 * 6.78444766998291
Epoch 380, val loss: 0.8467106819152832
Epoch 390, training loss: 0.12764696776866913 = 0.05986115708947182 + 0.01 * 6.778581142425537
Epoch 390, val loss: 0.8618730306625366
Epoch 400, training loss: 0.12126424163579941 = 0.05350659042596817 + 0.01 * 6.775765419006348
Epoch 400, val loss: 0.8767240047454834
Epoch 410, training loss: 0.11571662127971649 = 0.04798387363553047 + 0.01 * 6.7732744216918945
Epoch 410, val loss: 0.8912675380706787
Epoch 420, training loss: 0.11086353659629822 = 0.043176762759685516 + 0.01 * 6.768677711486816
Epoch 420, val loss: 0.9054463505744934
Epoch 430, training loss: 0.10665874183177948 = 0.03898528218269348 + 0.01 * 6.767346382141113
Epoch 430, val loss: 0.9192712903022766
Epoch 440, training loss: 0.10295708477497101 = 0.03532348945736885 + 0.01 * 6.763360023498535
Epoch 440, val loss: 0.9326887130737305
Epoch 450, training loss: 0.09968861937522888 = 0.0321161150932312 + 0.01 * 6.757250785827637
Epoch 450, val loss: 0.9457681179046631
Epoch 460, training loss: 0.09682419896125793 = 0.029298096895217896 + 0.01 * 6.752610206604004
Epoch 460, val loss: 0.9584508538246155
Epoch 470, training loss: 0.09438800811767578 = 0.02681577578186989 + 0.01 * 6.757224082946777
Epoch 470, val loss: 0.9707739949226379
Epoch 480, training loss: 0.09206010401248932 = 0.02462105266749859 + 0.01 * 6.743905067443848
Epoch 480, val loss: 0.9827315807342529
Epoch 490, training loss: 0.09003648161888123 = 0.022671859711408615 + 0.01 * 6.7364630699157715
Epoch 490, val loss: 0.9943691492080688
Epoch 500, training loss: 0.08847949653863907 = 0.020931847393512726 + 0.01 * 6.754765033721924
Epoch 500, val loss: 1.0056877136230469
Epoch 510, training loss: 0.08669029176235199 = 0.019377924501895905 + 0.01 * 6.731236934661865
Epoch 510, val loss: 1.0167185068130493
Epoch 520, training loss: 0.08520155400037766 = 0.017982810735702515 + 0.01 * 6.721874713897705
Epoch 520, val loss: 1.0274953842163086
Epoch 530, training loss: 0.08391997218132019 = 0.0167255736887455 + 0.01 * 6.719440460205078
Epoch 530, val loss: 1.0380324125289917
Epoch 540, training loss: 0.08284898102283478 = 0.015591418370604515 + 0.01 * 6.7257561683654785
Epoch 540, val loss: 1.048291563987732
Epoch 550, training loss: 0.08163955807685852 = 0.014568347483873367 + 0.01 * 6.707120895385742
Epoch 550, val loss: 1.0582265853881836
Epoch 560, training loss: 0.0806347131729126 = 0.01364200934767723 + 0.01 * 6.699270725250244
Epoch 560, val loss: 1.0679036378860474
Epoch 570, training loss: 0.08001135289669037 = 0.012801084667444229 + 0.01 * 6.72102689743042
Epoch 570, val loss: 1.0773065090179443
Epoch 580, training loss: 0.07901604473590851 = 0.012038537301123142 + 0.01 * 6.697751045227051
Epoch 580, val loss: 1.0863990783691406
Epoch 590, training loss: 0.07824918627738953 = 0.011344384402036667 + 0.01 * 6.6904802322387695
Epoch 590, val loss: 1.0952017307281494
Epoch 600, training loss: 0.07746803760528564 = 0.010710497386753559 + 0.01 * 6.675754070281982
Epoch 600, val loss: 1.1037460565567017
Epoch 610, training loss: 0.07690238207578659 = 0.010130367241799831 + 0.01 * 6.677201747894287
Epoch 610, val loss: 1.1120989322662354
Epoch 620, training loss: 0.07623228430747986 = 0.009598120115697384 + 0.01 * 6.663416385650635
Epoch 620, val loss: 1.1201567649841309
Epoch 630, training loss: 0.07592679560184479 = 0.009109647013247013 + 0.01 * 6.68171501159668
Epoch 630, val loss: 1.1279919147491455
Epoch 640, training loss: 0.07524818181991577 = 0.0086600873619318 + 0.01 * 6.658809661865234
Epoch 640, val loss: 1.1355959177017212
Epoch 650, training loss: 0.07480472326278687 = 0.008245937526226044 + 0.01 * 6.65587854385376
Epoch 650, val loss: 1.1429882049560547
Epoch 660, training loss: 0.0743861272931099 = 0.007863269187510014 + 0.01 * 6.652286052703857
Epoch 660, val loss: 1.1501506567001343
Epoch 670, training loss: 0.07397157698869705 = 0.007508872542530298 + 0.01 * 6.646270751953125
Epoch 670, val loss: 1.1571102142333984
Epoch 680, training loss: 0.07342799007892609 = 0.007180479355156422 + 0.01 * 6.624751091003418
Epoch 680, val loss: 1.1638891696929932
Epoch 690, training loss: 0.07379579544067383 = 0.0068753263913095 + 0.01 * 6.692047119140625
Epoch 690, val loss: 1.1704347133636475
Epoch 700, training loss: 0.07274629175662994 = 0.006592010613530874 + 0.01 * 6.6154279708862305
Epoch 700, val loss: 1.1768168210983276
Epoch 710, training loss: 0.07232149690389633 = 0.00632803188636899 + 0.01 * 6.59934663772583
Epoch 710, val loss: 1.1830198764801025
Epoch 720, training loss: 0.07205961644649506 = 0.006081110332161188 + 0.01 * 6.597850322723389
Epoch 720, val loss: 1.1890395879745483
Epoch 730, training loss: 0.0719199851155281 = 0.005850114859640598 + 0.01 * 6.606987476348877
Epoch 730, val loss: 1.1949626207351685
Epoch 740, training loss: 0.07159829139709473 = 0.005633645225316286 + 0.01 * 6.59646463394165
Epoch 740, val loss: 1.2007074356079102
Epoch 750, training loss: 0.07122213393449783 = 0.005430686753243208 + 0.01 * 6.5791449546813965
Epoch 750, val loss: 1.2063225507736206
Epoch 760, training loss: 0.07118743658065796 = 0.005239991471171379 + 0.01 * 6.5947442054748535
Epoch 760, val loss: 1.2117540836334229
Epoch 770, training loss: 0.07084430009126663 = 0.005061047617346048 + 0.01 * 6.5783257484436035
Epoch 770, val loss: 1.217032551765442
Epoch 780, training loss: 0.07049518823623657 = 0.004892279859632254 + 0.01 * 6.560290813446045
Epoch 780, val loss: 1.2222366333007812
Epoch 790, training loss: 0.07040420174598694 = 0.004732829984277487 + 0.01 * 6.567136764526367
Epoch 790, val loss: 1.2273303270339966
Epoch 800, training loss: 0.07038460671901703 = 0.004582535475492477 + 0.01 * 6.580206871032715
Epoch 800, val loss: 1.2322026491165161
Epoch 810, training loss: 0.06991948932409286 = 0.0044406563974916935 + 0.01 * 6.5478835105896
Epoch 810, val loss: 1.2370387315750122
Epoch 820, training loss: 0.06986581534147263 = 0.004306209739297628 + 0.01 * 6.555960178375244
Epoch 820, val loss: 1.2417327165603638
Epoch 830, training loss: 0.06980754435062408 = 0.004178901668637991 + 0.01 * 6.562864303588867
Epoch 830, val loss: 1.2463302612304688
Epoch 840, training loss: 0.06939418613910675 = 0.004058065824210644 + 0.01 * 6.533612251281738
Epoch 840, val loss: 1.2508602142333984
Epoch 850, training loss: 0.06933842599391937 = 0.003943236079066992 + 0.01 * 6.539519309997559
Epoch 850, val loss: 1.255228042602539
Epoch 860, training loss: 0.06916320323944092 = 0.0038343698251992464 + 0.01 * 6.532884120941162
Epoch 860, val loss: 1.2595875263214111
Epoch 870, training loss: 0.0690777599811554 = 0.0037308039609342813 + 0.01 * 6.534695625305176
Epoch 870, val loss: 1.2637611627578735
Epoch 880, training loss: 0.06895928829908371 = 0.00363233988173306 + 0.01 * 6.532695293426514
Epoch 880, val loss: 1.2679064273834229
Epoch 890, training loss: 0.06871756166219711 = 0.0035384090151637793 + 0.01 * 6.517915725708008
Epoch 890, val loss: 1.2719851732254028
Epoch 900, training loss: 0.06862706691026688 = 0.0034486670047044754 + 0.01 * 6.517840385437012
Epoch 900, val loss: 1.2759366035461426
Epoch 910, training loss: 0.06864503026008606 = 0.003363032126799226 + 0.01 * 6.528199672698975
Epoch 910, val loss: 1.279819369316101
Epoch 920, training loss: 0.06846342980861664 = 0.003281316254287958 + 0.01 * 6.518211364746094
Epoch 920, val loss: 1.2836401462554932
Epoch 930, training loss: 0.0683809444308281 = 0.0032032320741564035 + 0.01 * 6.517770767211914
Epoch 930, val loss: 1.2873485088348389
Epoch 940, training loss: 0.06826604902744293 = 0.0031284461729228497 + 0.01 * 6.513760089874268
Epoch 940, val loss: 1.2909904718399048
Epoch 950, training loss: 0.06831963360309601 = 0.0030569371301680803 + 0.01 * 6.526269912719727
Epoch 950, val loss: 1.2945952415466309
Epoch 960, training loss: 0.0680258497595787 = 0.0029884655959904194 + 0.01 * 6.5037384033203125
Epoch 960, val loss: 1.2980543375015259
Epoch 970, training loss: 0.06817672401666641 = 0.0029228359926491976 + 0.01 * 6.525389194488525
Epoch 970, val loss: 1.3014919757843018
Epoch 980, training loss: 0.06794194877147675 = 0.0028598462231457233 + 0.01 * 6.508210182189941
Epoch 980, val loss: 1.3048737049102783
Epoch 990, training loss: 0.06776387244462967 = 0.0027995628770440817 + 0.01 * 6.49643087387085
Epoch 990, val loss: 1.3081622123718262
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.6162
Flip ASR: 0.5378/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.053370714187622 = 1.9696322679519653 + 0.01 * 8.373844146728516
Epoch 0, val loss: 1.96380615234375
Epoch 10, training loss: 2.0425164699554443 = 1.9587793350219727 + 0.01 * 8.373710632324219
Epoch 10, val loss: 1.9529078006744385
Epoch 20, training loss: 2.029470920562744 = 1.9457392692565918 + 0.01 * 8.37315559387207
Epoch 20, val loss: 1.9392515420913696
Epoch 30, training loss: 2.011277437210083 = 1.9275647401809692 + 0.01 * 8.37126636505127
Epoch 30, val loss: 1.9196832180023193
Epoch 40, training loss: 1.9843389987945557 = 1.9007092714309692 + 0.01 * 8.362967491149902
Epoch 40, val loss: 1.890683889389038
Epoch 50, training loss: 1.944700002670288 = 1.8615754842758179 + 0.01 * 8.312451362609863
Epoch 50, val loss: 1.8498088121414185
Epoch 60, training loss: 1.8908640146255493 = 1.8113861083984375 + 0.01 * 7.947790145874023
Epoch 60, val loss: 1.801261067390442
Epoch 70, training loss: 1.8373937606811523 = 1.7615917921066284 + 0.01 * 7.580191612243652
Epoch 70, val loss: 1.758680820465088
Epoch 80, training loss: 1.7876790761947632 = 1.714020848274231 + 0.01 * 7.365823268890381
Epoch 80, val loss: 1.7186123132705688
Epoch 90, training loss: 1.7242605686187744 = 1.6520583629608154 + 0.01 * 7.220223426818848
Epoch 90, val loss: 1.6646366119384766
Epoch 100, training loss: 1.6405996084213257 = 1.569663405418396 + 0.01 * 7.093618392944336
Epoch 100, val loss: 1.5953969955444336
Epoch 110, training loss: 1.537757396697998 = 1.4677609205245972 + 0.01 * 6.999643802642822
Epoch 110, val loss: 1.5133334398269653
Epoch 120, training loss: 1.428623914718628 = 1.3590420484542847 + 0.01 * 6.958183288574219
Epoch 120, val loss: 1.4306148290634155
Epoch 130, training loss: 1.324646234512329 = 1.2553542852401733 + 0.01 * 6.929197311401367
Epoch 130, val loss: 1.3560595512390137
Epoch 140, training loss: 1.2298500537872314 = 1.1607917547225952 + 0.01 * 6.905828952789307
Epoch 140, val loss: 1.2907685041427612
Epoch 150, training loss: 1.1443430185317993 = 1.075525164604187 + 0.01 * 6.881782531738281
Epoch 150, val loss: 1.2333041429519653
Epoch 160, training loss: 1.0682730674743652 = 0.9996747970581055 + 0.01 * 6.859830379486084
Epoch 160, val loss: 1.1831687688827515
Epoch 170, training loss: 1.0006482601165771 = 0.9322452545166016 + 0.01 * 6.840298652648926
Epoch 170, val loss: 1.1392502784729004
Epoch 180, training loss: 0.9380320906639099 = 0.8697828054428101 + 0.01 * 6.824930667877197
Epoch 180, val loss: 1.098908543586731
Epoch 190, training loss: 0.8767281770706177 = 0.808570146560669 + 0.01 * 6.8158040046691895
Epoch 190, val loss: 1.0589962005615234
Epoch 200, training loss: 0.8144087195396423 = 0.7462921142578125 + 0.01 * 6.811661720275879
Epoch 200, val loss: 1.017714500427246
Epoch 210, training loss: 0.7511445879936218 = 0.6830320358276367 + 0.01 * 6.811257839202881
Epoch 210, val loss: 0.975865364074707
Epoch 220, training loss: 0.6886491775512695 = 0.6205335259437561 + 0.01 * 6.8115668296813965
Epoch 220, val loss: 0.9355587363243103
Epoch 230, training loss: 0.6287328004837036 = 0.5606071949005127 + 0.01 * 6.812563896179199
Epoch 230, val loss: 0.898830235004425
Epoch 240, training loss: 0.5723656415939331 = 0.5042267441749573 + 0.01 * 6.81389045715332
Epoch 240, val loss: 0.8671078681945801
Epoch 250, training loss: 0.5198831558227539 = 0.45172956585884094 + 0.01 * 6.815356731414795
Epoch 250, val loss: 0.8407612442970276
Epoch 260, training loss: 0.47162413597106934 = 0.4034563899040222 + 0.01 * 6.816772937774658
Epoch 260, val loss: 0.8192026615142822
Epoch 270, training loss: 0.42777642607688904 = 0.35959744453430176 + 0.01 * 6.817897796630859
Epoch 270, val loss: 0.802161455154419
Epoch 280, training loss: 0.3884700834751129 = 0.3202846646308899 + 0.01 * 6.818542003631592
Epoch 280, val loss: 0.7894738912582397
Epoch 290, training loss: 0.3536217510700226 = 0.2854330837726593 + 0.01 * 6.818865776062012
Epoch 290, val loss: 0.7806869745254517
Epoch 300, training loss: 0.322740375995636 = 0.25455525517463684 + 0.01 * 6.818512439727783
Epoch 300, val loss: 0.7753148674964905
Epoch 310, training loss: 0.29503124952316284 = 0.22685357928276062 + 0.01 * 6.817768096923828
Epoch 310, val loss: 0.7729950547218323
Epoch 320, training loss: 0.26966437697410583 = 0.2014959454536438 + 0.01 * 6.816842555999756
Epoch 320, val loss: 0.773248553276062
Epoch 330, training loss: 0.24608737230300903 = 0.17793114483356476 + 0.01 * 6.815621852874756
Epoch 330, val loss: 0.775407075881958
Epoch 340, training loss: 0.2241636961698532 = 0.15602625906467438 + 0.01 * 6.813743591308594
Epoch 340, val loss: 0.778988778591156
Epoch 350, training loss: 0.20408761501312256 = 0.135963574051857 + 0.01 * 6.812404155731201
Epoch 350, val loss: 0.7836352586746216
Epoch 360, training loss: 0.18598926067352295 = 0.1178925633430481 + 0.01 * 6.8096699714660645
Epoch 360, val loss: 0.7892436385154724
Epoch 370, training loss: 0.17002823948860168 = 0.1019650399684906 + 0.01 * 6.806321144104004
Epoch 370, val loss: 0.7956973910331726
Epoch 380, training loss: 0.15621361136436462 = 0.08817866444587708 + 0.01 * 6.803494930267334
Epoch 380, val loss: 0.8028672933578491
Epoch 390, training loss: 0.1443873941898346 = 0.0764114111661911 + 0.01 * 6.797597885131836
Epoch 390, val loss: 0.810300350189209
Epoch 400, training loss: 0.1344534158706665 = 0.0665259063243866 + 0.01 * 6.792751312255859
Epoch 400, val loss: 0.8180233240127563
Epoch 410, training loss: 0.12612873315811157 = 0.058272674679756165 + 0.01 * 6.785606861114502
Epoch 410, val loss: 0.8262228965759277
Epoch 420, training loss: 0.11916284263134003 = 0.05139121040701866 + 0.01 * 6.777163028717041
Epoch 420, val loss: 0.8347522616386414
Epoch 430, training loss: 0.11329725384712219 = 0.04561569169163704 + 0.01 * 6.7681565284729
Epoch 430, val loss: 0.8435766696929932
Epoch 440, training loss: 0.10834372043609619 = 0.040757641196250916 + 0.01 * 6.758608341217041
Epoch 440, val loss: 0.8526217937469482
Epoch 450, training loss: 0.10414467006921768 = 0.0366431325674057 + 0.01 * 6.7501540184021
Epoch 450, val loss: 0.8616676330566406
Epoch 460, training loss: 0.10047169774770737 = 0.033118538558483124 + 0.01 * 6.735315799713135
Epoch 460, val loss: 0.8707346320152283
Epoch 470, training loss: 0.09729728102684021 = 0.03007708117365837 + 0.01 * 6.722020149230957
Epoch 470, val loss: 0.8797124028205872
Epoch 480, training loss: 0.09491501748561859 = 0.027441352605819702 + 0.01 * 6.747366428375244
Epoch 480, val loss: 0.8885855674743652
Epoch 490, training loss: 0.09223591536283493 = 0.025144904851913452 + 0.01 * 6.70910120010376
Epoch 490, val loss: 0.8972712755203247
Epoch 500, training loss: 0.09007920324802399 = 0.02312818542122841 + 0.01 * 6.695101261138916
Epoch 500, val loss: 0.9058419466018677
Epoch 510, training loss: 0.08827270567417145 = 0.021347088739275932 + 0.01 * 6.692561626434326
Epoch 510, val loss: 0.9142734408378601
Epoch 520, training loss: 0.08665246516466141 = 0.019767120480537415 + 0.01 * 6.688534736633301
Epoch 520, val loss: 0.9224174618721008
Epoch 530, training loss: 0.0851040631532669 = 0.018361613154411316 + 0.01 * 6.674245357513428
Epoch 530, val loss: 0.9303454160690308
Epoch 540, training loss: 0.08380351215600967 = 0.017104774713516235 + 0.01 * 6.6698737144470215
Epoch 540, val loss: 0.9381021857261658
Epoch 550, training loss: 0.08260035514831543 = 0.015980811789631844 + 0.01 * 6.661954879760742
Epoch 550, val loss: 0.9457208514213562
Epoch 560, training loss: 0.08158090710639954 = 0.01496924739331007 + 0.01 * 6.661166667938232
Epoch 560, val loss: 0.9530377388000488
Epoch 570, training loss: 0.08058619499206543 = 0.014055437408387661 + 0.01 * 6.653075695037842
Epoch 570, val loss: 0.9601571559906006
Epoch 580, training loss: 0.07963262498378754 = 0.01322656124830246 + 0.01 * 6.640606880187988
Epoch 580, val loss: 0.967160701751709
Epoch 590, training loss: 0.07885241508483887 = 0.012472315691411495 + 0.01 * 6.638010025024414
Epoch 590, val loss: 0.9740560054779053
Epoch 600, training loss: 0.07803020626306534 = 0.011782871559262276 + 0.01 * 6.6247334480285645
Epoch 600, val loss: 0.9806240797042847
Epoch 610, training loss: 0.07735193520784378 = 0.011148911900818348 + 0.01 * 6.620302200317383
Epoch 610, val loss: 0.9872031807899475
Epoch 620, training loss: 0.07681223005056381 = 0.010566232725977898 + 0.01 * 6.624599456787109
Epoch 620, val loss: 0.9935263395309448
Epoch 630, training loss: 0.07624924182891846 = 0.01002911850810051 + 0.01 * 6.622012615203857
Epoch 630, val loss: 0.9997223019599915
Epoch 640, training loss: 0.07558941096067429 = 0.00952781643718481 + 0.01 * 6.606159687042236
Epoch 640, val loss: 1.0057268142700195
Epoch 650, training loss: 0.07529556751251221 = 0.009064367972314358 + 0.01 * 6.623119831085205
Epoch 650, val loss: 1.0115073919296265
Epoch 660, training loss: 0.07462605088949203 = 0.008634139783680439 + 0.01 * 6.599191188812256
Epoch 660, val loss: 1.017256736755371
Epoch 670, training loss: 0.07421514391899109 = 0.008233720436692238 + 0.01 * 6.598142623901367
Epoch 670, val loss: 1.0227783918380737
Epoch 680, training loss: 0.07373214513063431 = 0.007863196544349194 + 0.01 * 6.58689546585083
Epoch 680, val loss: 1.0282458066940308
Epoch 690, training loss: 0.07347536087036133 = 0.00751870172098279 + 0.01 * 6.59566593170166
Epoch 690, val loss: 1.033600091934204
Epoch 700, training loss: 0.07295405119657516 = 0.007197446655482054 + 0.01 * 6.575660705566406
Epoch 700, val loss: 1.038839340209961
Epoch 710, training loss: 0.0728127658367157 = 0.006897426210343838 + 0.01 * 6.591533660888672
Epoch 710, val loss: 1.0440326929092407
Epoch 720, training loss: 0.07230941951274872 = 0.0066169449128210545 + 0.01 * 6.569247722625732
Epoch 720, val loss: 1.049018144607544
Epoch 730, training loss: 0.0720030888915062 = 0.006353963632136583 + 0.01 * 6.56491231918335
Epoch 730, val loss: 1.0538392066955566
Epoch 740, training loss: 0.07175449281930923 = 0.0061069815419614315 + 0.01 * 6.564751148223877
Epoch 740, val loss: 1.0586798191070557
Epoch 750, training loss: 0.07140425592660904 = 0.005875338334590197 + 0.01 * 6.552891254425049
Epoch 750, val loss: 1.0633097887039185
Epoch 760, training loss: 0.07121273875236511 = 0.005657591857016087 + 0.01 * 6.555514812469482
Epoch 760, val loss: 1.0678678750991821
Epoch 770, training loss: 0.07118463516235352 = 0.005452776327729225 + 0.01 * 6.573185920715332
Epoch 770, val loss: 1.072439432144165
Epoch 780, training loss: 0.07071828842163086 = 0.005259879399091005 + 0.01 * 6.545841217041016
Epoch 780, val loss: 1.0767865180969238
Epoch 790, training loss: 0.07055511325597763 = 0.005077995825558901 + 0.01 * 6.5477118492126465
Epoch 790, val loss: 1.0810848474502563
Epoch 800, training loss: 0.07067473232746124 = 0.004906069487333298 + 0.01 * 6.576866626739502
Epoch 800, val loss: 1.0853183269500732
Epoch 810, training loss: 0.07015890628099442 = 0.004744193982332945 + 0.01 * 6.541471481323242
Epoch 810, val loss: 1.089362621307373
Epoch 820, training loss: 0.06998582929372787 = 0.0045915343798696995 + 0.01 * 6.539430141448975
Epoch 820, val loss: 1.0934271812438965
Epoch 830, training loss: 0.07000850141048431 = 0.004446880426257849 + 0.01 * 6.556162357330322
Epoch 830, val loss: 1.0974152088165283
Epoch 840, training loss: 0.06962833553552628 = 0.00430968077853322 + 0.01 * 6.53186559677124
Epoch 840, val loss: 1.1011579036712646
Epoch 850, training loss: 0.06940902769565582 = 0.004179531708359718 + 0.01 * 6.522950172424316
Epoch 850, val loss: 1.1050007343292236
Epoch 860, training loss: 0.06939953565597534 = 0.004055919125676155 + 0.01 * 6.534361839294434
Epoch 860, val loss: 1.108748197555542
Epoch 870, training loss: 0.06940524280071259 = 0.003938380163162947 + 0.01 * 6.54668664932251
Epoch 870, val loss: 1.1123833656311035
Epoch 880, training loss: 0.06893622130155563 = 0.0038268023636192083 + 0.01 * 6.510941505432129
Epoch 880, val loss: 1.1159330606460571
Epoch 890, training loss: 0.06901772320270538 = 0.0037207193672657013 + 0.01 * 6.52970027923584
Epoch 890, val loss: 1.1194030046463013
Epoch 900, training loss: 0.0688260942697525 = 0.0036198031157255173 + 0.01 * 6.520628929138184
Epoch 900, val loss: 1.1228125095367432
Epoch 910, training loss: 0.06857898831367493 = 0.0035235846880823374 + 0.01 * 6.505540370941162
Epoch 910, val loss: 1.1261872053146362
Epoch 920, training loss: 0.06843771785497665 = 0.0034318873658776283 + 0.01 * 6.500583171844482
Epoch 920, val loss: 1.1295111179351807
Epoch 930, training loss: 0.06842640042304993 = 0.0033444054424762726 + 0.01 * 6.508199691772461
Epoch 930, val loss: 1.1327978372573853
Epoch 940, training loss: 0.06837809830904007 = 0.0032607608009129763 + 0.01 * 6.5117340087890625
Epoch 940, val loss: 1.1359503269195557
Epoch 950, training loss: 0.0681457668542862 = 0.0031807045452296734 + 0.01 * 6.496506214141846
Epoch 950, val loss: 1.1390657424926758
Epoch 960, training loss: 0.06852307915687561 = 0.003104280913248658 + 0.01 * 6.541879653930664
Epoch 960, val loss: 1.1421018838882446
Epoch 970, training loss: 0.06803473085165024 = 0.003031357191503048 + 0.01 * 6.500337600708008
Epoch 970, val loss: 1.1450626850128174
Epoch 980, training loss: 0.06802389770746231 = 0.002961704507470131 + 0.01 * 6.506219863891602
Epoch 980, val loss: 1.1480319499969482
Epoch 990, training loss: 0.06804300099611282 = 0.002894868841394782 + 0.01 * 6.514813423156738
Epoch 990, val loss: 1.1508859395980835
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.8598
Flip ASR: 0.8356/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0108988285064697 = 1.9271608591079712 + 0.01 * 8.373807907104492
Epoch 0, val loss: 1.916768193244934
Epoch 10, training loss: 2.0007195472717285 = 1.9169832468032837 + 0.01 * 8.373641014099121
Epoch 10, val loss: 1.9069528579711914
Epoch 20, training loss: 1.9883763790130615 = 1.9046460390090942 + 0.01 * 8.373028755187988
Epoch 20, val loss: 1.8946454524993896
Epoch 30, training loss: 1.9714351892471313 = 1.8877284526824951 + 0.01 * 8.370671272277832
Epoch 30, val loss: 1.8773987293243408
Epoch 40, training loss: 1.947067141532898 = 1.8634854555130005 + 0.01 * 8.358174324035645
Epoch 40, val loss: 1.8528300523757935
Epoch 50, training loss: 1.9132003784179688 = 1.8303779363632202 + 0.01 * 8.282238960266113
Epoch 50, val loss: 1.8207581043243408
Epoch 60, training loss: 1.8698527812957764 = 1.791195034980774 + 0.01 * 7.86577033996582
Epoch 60, val loss: 1.7856577634811401
Epoch 70, training loss: 1.8249200582504272 = 1.7493821382522583 + 0.01 * 7.553786754608154
Epoch 70, val loss: 1.749369502067566
Epoch 80, training loss: 1.7687450647354126 = 1.6955780982971191 + 0.01 * 7.316695690155029
Epoch 80, val loss: 1.7013221979141235
Epoch 90, training loss: 1.6944891214370728 = 1.6224303245544434 + 0.01 * 7.205881118774414
Epoch 90, val loss: 1.63834810256958
Epoch 100, training loss: 1.5999345779418945 = 1.5289733409881592 + 0.01 * 7.096124649047852
Epoch 100, val loss: 1.5621403455734253
Epoch 110, training loss: 1.4943097829818726 = 1.4238554239273071 + 0.01 * 7.045437812805176
Epoch 110, val loss: 1.4803338050842285
Epoch 120, training loss: 1.387481689453125 = 1.3172956705093384 + 0.01 * 7.018607139587402
Epoch 120, val loss: 1.401437520980835
Epoch 130, training loss: 1.2836285829544067 = 1.213742971420288 + 0.01 * 6.988564968109131
Epoch 130, val loss: 1.327438473701477
Epoch 140, training loss: 1.184634804725647 = 1.1150883436203003 + 0.01 * 6.954644203186035
Epoch 140, val loss: 1.258331060409546
Epoch 150, training loss: 1.092930555343628 = 1.0237014293670654 + 0.01 * 6.922913074493408
Epoch 150, val loss: 1.1944248676300049
Epoch 160, training loss: 1.0097085237503052 = 0.9407324194908142 + 0.01 * 6.897611141204834
Epoch 160, val loss: 1.1372076272964478
Epoch 170, training loss: 0.9339454174041748 = 0.8651552200317383 + 0.01 * 6.879019737243652
Epoch 170, val loss: 1.086017370223999
Epoch 180, training loss: 0.8631234765052795 = 0.7944461703300476 + 0.01 * 6.867732048034668
Epoch 180, val loss: 1.0384578704833984
Epoch 190, training loss: 0.7952048182487488 = 0.7266284227371216 + 0.01 * 6.857639789581299
Epoch 190, val loss: 0.9934642910957336
Epoch 200, training loss: 0.7289368510246277 = 0.6604268550872803 + 0.01 * 6.851001262664795
Epoch 200, val loss: 0.9505605101585388
Epoch 210, training loss: 0.6634572744369507 = 0.5949979424476624 + 0.01 * 6.845932960510254
Epoch 210, val loss: 0.9091023802757263
Epoch 220, training loss: 0.5985385179519653 = 0.5301165580749512 + 0.01 * 6.842198848724365
Epoch 220, val loss: 0.8686792850494385
Epoch 230, training loss: 0.5351336002349854 = 0.4667382538318634 + 0.01 * 6.839537143707275
Epoch 230, val loss: 0.8304601311683655
Epoch 240, training loss: 0.4750942289829254 = 0.40672045946121216 + 0.01 * 6.837376594543457
Epoch 240, val loss: 0.7961686253547668
Epoch 250, training loss: 0.42001163959503174 = 0.3516671061515808 + 0.01 * 6.834454536437988
Epoch 250, val loss: 0.7665808796882629
Epoch 260, training loss: 0.3708174228668213 = 0.30249664187431335 + 0.01 * 6.832079887390137
Epoch 260, val loss: 0.7424007058143616
Epoch 270, training loss: 0.3279207944869995 = 0.2596329152584076 + 0.01 * 6.828786373138428
Epoch 270, val loss: 0.7239628434181213
Epoch 280, training loss: 0.2912845015525818 = 0.22304774820804596 + 0.01 * 6.823675155639648
Epoch 280, val loss: 0.7111520171165466
Epoch 290, training loss: 0.2604827880859375 = 0.19229498505592346 + 0.01 * 6.818780899047852
Epoch 290, val loss: 0.7035962343215942
Epoch 300, training loss: 0.2348005473613739 = 0.1666783094406128 + 0.01 * 6.812223434448242
Epoch 300, val loss: 0.7006595134735107
Epoch 310, training loss: 0.21365010738372803 = 0.14537103474140167 + 0.01 * 6.827906608581543
Epoch 310, val loss: 0.7012518048286438
Epoch 320, training loss: 0.19574016332626343 = 0.1276482790708542 + 0.01 * 6.809188365936279
Epoch 320, val loss: 0.704570472240448
Epoch 330, training loss: 0.18075627088546753 = 0.11278636753559113 + 0.01 * 6.796990394592285
Epoch 330, val loss: 0.7098450064659119
Epoch 340, training loss: 0.16807422041893005 = 0.10019640624523163 + 0.01 * 6.78778076171875
Epoch 340, val loss: 0.7164883613586426
Epoch 350, training loss: 0.15721341967582703 = 0.08940472453832626 + 0.01 * 6.780869483947754
Epoch 350, val loss: 0.7242291569709778
Epoch 360, training loss: 0.1478998064994812 = 0.08005600422620773 + 0.01 * 6.784379482269287
Epoch 360, val loss: 0.7327488660812378
Epoch 370, training loss: 0.13965247571468353 = 0.07186606526374817 + 0.01 * 6.778641223907471
Epoch 370, val loss: 0.7418957352638245
Epoch 380, training loss: 0.1322639286518097 = 0.06459096074104309 + 0.01 * 6.767295837402344
Epoch 380, val loss: 0.7513849139213562
Epoch 390, training loss: 0.12567707896232605 = 0.05809217691421509 + 0.01 * 6.758489608764648
Epoch 390, val loss: 0.7612513303756714
Epoch 400, training loss: 0.11981994658708572 = 0.052284739911556244 + 0.01 * 6.753520965576172
Epoch 400, val loss: 0.7713174223899841
Epoch 410, training loss: 0.1146126389503479 = 0.04712787643074989 + 0.01 * 6.748476505279541
Epoch 410, val loss: 0.7817312479019165
Epoch 420, training loss: 0.11010520160198212 = 0.04258033260703087 + 0.01 * 6.752487659454346
Epoch 420, val loss: 0.7924838662147522
Epoch 430, training loss: 0.10603967308998108 = 0.038589898496866226 + 0.01 * 6.744976997375488
Epoch 430, val loss: 0.8035087585449219
Epoch 440, training loss: 0.10244797170162201 = 0.03507889062166214 + 0.01 * 6.736908435821533
Epoch 440, val loss: 0.8148212432861328
Epoch 450, training loss: 0.0993613451719284 = 0.031985945999622345 + 0.01 * 6.737540245056152
Epoch 450, val loss: 0.8259521722793579
Epoch 460, training loss: 0.09654586762189865 = 0.02925724722445011 + 0.01 * 6.728862285614014
Epoch 460, val loss: 0.8371415734291077
Epoch 470, training loss: 0.0940895527601242 = 0.0268452949821949 + 0.01 * 6.72442626953125
Epoch 470, val loss: 0.8480827808380127
Epoch 480, training loss: 0.09196410328149796 = 0.0247091893106699 + 0.01 * 6.725491523742676
Epoch 480, val loss: 0.8589196801185608
Epoch 490, training loss: 0.08998363465070724 = 0.02281319908797741 + 0.01 * 6.717043399810791
Epoch 490, val loss: 0.8693998456001282
Epoch 500, training loss: 0.08825640380382538 = 0.02112572267651558 + 0.01 * 6.713068008422852
Epoch 500, val loss: 0.8798694610595703
Epoch 510, training loss: 0.0867290049791336 = 0.019618749618530273 + 0.01 * 6.711025714874268
Epoch 510, val loss: 0.8899425268173218
Epoch 520, training loss: 0.08529520034790039 = 0.01826745830476284 + 0.01 * 6.702774524688721
Epoch 520, val loss: 0.8998891115188599
Epoch 530, training loss: 0.08428014069795609 = 0.017045853659510612 + 0.01 * 6.723428726196289
Epoch 530, val loss: 0.909524142742157
Epoch 540, training loss: 0.08292248845100403 = 0.01594310626387596 + 0.01 * 6.697938442230225
Epoch 540, val loss: 0.9188084602355957
Epoch 550, training loss: 0.08188889175653458 = 0.014945303089916706 + 0.01 * 6.694358825683594
Epoch 550, val loss: 0.927860677242279
Epoch 560, training loss: 0.08092837035655975 = 0.014039686881005764 + 0.01 * 6.688868045806885
Epoch 560, val loss: 0.9367204308509827
Epoch 570, training loss: 0.08006150275468826 = 0.013214997947216034 + 0.01 * 6.684650421142578
Epoch 570, val loss: 0.9453563690185547
Epoch 580, training loss: 0.079313263297081 = 0.012462927028536797 + 0.01 * 6.685033798217773
Epoch 580, val loss: 0.953677237033844
Epoch 590, training loss: 0.07854346930980682 = 0.011775835417211056 + 0.01 * 6.676763534545898
Epoch 590, val loss: 0.9618440866470337
Epoch 600, training loss: 0.07784688472747803 = 0.011145229451358318 + 0.01 * 6.670166015625
Epoch 600, val loss: 0.9697819352149963
Epoch 610, training loss: 0.0772237703204155 = 0.010566381737589836 + 0.01 * 6.665738582611084
Epoch 610, val loss: 0.9775365591049194
Epoch 620, training loss: 0.07664866745471954 = 0.010033706203103065 + 0.01 * 6.661496639251709
Epoch 620, val loss: 0.9850316047668457
Epoch 630, training loss: 0.07621129602193832 = 0.009541698731482029 + 0.01 * 6.6669602394104
Epoch 630, val loss: 0.9924008846282959
Epoch 640, training loss: 0.07567722350358963 = 0.009086420759558678 + 0.01 * 6.6590800285339355
Epoch 640, val loss: 0.9995519518852234
Epoch 650, training loss: 0.07518266886472702 = 0.00866453442722559 + 0.01 * 6.651813983917236
Epoch 650, val loss: 1.0065072774887085
Epoch 660, training loss: 0.07485183328390121 = 0.008272833190858364 + 0.01 * 6.657899856567383
Epoch 660, val loss: 1.013277292251587
Epoch 670, training loss: 0.07439616322517395 = 0.007908684201538563 + 0.01 * 6.648747444152832
Epoch 670, val loss: 1.0199064016342163
Epoch 680, training loss: 0.07412411272525787 = 0.007569354958832264 + 0.01 * 6.655475616455078
Epoch 680, val loss: 1.026396632194519
Epoch 690, training loss: 0.07365764677524567 = 0.007253370713442564 + 0.01 * 6.640427589416504
Epoch 690, val loss: 1.0326597690582275
Epoch 700, training loss: 0.07332209497690201 = 0.006957662291824818 + 0.01 * 6.636443138122559
Epoch 700, val loss: 1.0387568473815918
Epoch 710, training loss: 0.07301463186740875 = 0.006680889520794153 + 0.01 * 6.633374214172363
Epoch 710, val loss: 1.0447267293930054
Epoch 720, training loss: 0.07274117320775986 = 0.006421653088182211 + 0.01 * 6.631952285766602
Epoch 720, val loss: 1.0505568981170654
Epoch 730, training loss: 0.07238864153623581 = 0.006178960669785738 + 0.01 * 6.620968341827393
Epoch 730, val loss: 1.05624258518219
Epoch 740, training loss: 0.07216217368841171 = 0.005950779654085636 + 0.01 * 6.621140003204346
Epoch 740, val loss: 1.0617755651474
Epoch 750, training loss: 0.07191066443920135 = 0.005735968705266714 + 0.01 * 6.617469310760498
Epoch 750, val loss: 1.0672595500946045
Epoch 760, training loss: 0.07183587551116943 = 0.005533621646463871 + 0.01 * 6.630225658416748
Epoch 760, val loss: 1.0725493431091309
Epoch 770, training loss: 0.07148852944374084 = 0.005343219731003046 + 0.01 * 6.61453104019165
Epoch 770, val loss: 1.0776325464248657
Epoch 780, training loss: 0.07131126523017883 = 0.005163813941180706 + 0.01 * 6.614745616912842
Epoch 780, val loss: 1.0827406644821167
Epoch 790, training loss: 0.07109524309635162 = 0.004993840120732784 + 0.01 * 6.610140323638916
Epoch 790, val loss: 1.0876668691635132
Epoch 800, training loss: 0.07081878930330276 = 0.004833321552723646 + 0.01 * 6.598547458648682
Epoch 800, val loss: 1.0924160480499268
Epoch 810, training loss: 0.07060351222753525 = 0.0046812440268695354 + 0.01 * 6.592226982116699
Epoch 810, val loss: 1.0971125364303589
Epoch 820, training loss: 0.07066397368907928 = 0.004537309519946575 + 0.01 * 6.612667083740234
Epoch 820, val loss: 1.101778268814087
Epoch 830, training loss: 0.07035798579454422 = 0.004401009995490313 + 0.01 * 6.595697402954102
Epoch 830, val loss: 1.1061701774597168
Epoch 840, training loss: 0.07007304579019547 = 0.004271683283150196 + 0.01 * 6.580135822296143
Epoch 840, val loss: 1.1104984283447266
Epoch 850, training loss: 0.07009813189506531 = 0.004148728214204311 + 0.01 * 6.594940662384033
Epoch 850, val loss: 1.1148072481155396
Epoch 860, training loss: 0.06984603404998779 = 0.0040318830870091915 + 0.01 * 6.58141565322876
Epoch 860, val loss: 1.1190459728240967
Epoch 870, training loss: 0.06973344832658768 = 0.003920844756066799 + 0.01 * 6.581260681152344
Epoch 870, val loss: 1.1230422258377075
Epoch 880, training loss: 0.06948227435350418 = 0.0038148819003254175 + 0.01 * 6.566739559173584
Epoch 880, val loss: 1.127083659172058
Epoch 890, training loss: 0.0695013776421547 = 0.0037141318898648024 + 0.01 * 6.5787248611450195
Epoch 890, val loss: 1.130998134613037
Epoch 900, training loss: 0.06925864517688751 = 0.003618269693106413 + 0.01 * 6.564037799835205
Epoch 900, val loss: 1.1347969770431519
Epoch 910, training loss: 0.0692739188671112 = 0.0035266580525785685 + 0.01 * 6.574726104736328
Epoch 910, val loss: 1.1385836601257324
Epoch 920, training loss: 0.06909655034542084 = 0.0034391579683870077 + 0.01 * 6.565739154815674
Epoch 920, val loss: 1.1422407627105713
Epoch 930, training loss: 0.0689653679728508 = 0.00335550750605762 + 0.01 * 6.560986042022705
Epoch 930, val loss: 1.1457234621047974
Epoch 940, training loss: 0.068802110850811 = 0.003275706432759762 + 0.01 * 6.552640438079834
Epoch 940, val loss: 1.1493128538131714
Epoch 950, training loss: 0.06870463490486145 = 0.003199226688593626 + 0.01 * 6.550540924072266
Epoch 950, val loss: 1.1525951623916626
Epoch 960, training loss: 0.06867331266403198 = 0.0031260112300515175 + 0.01 * 6.554730415344238
Epoch 960, val loss: 1.1559749841690063
Epoch 970, training loss: 0.06852715462446213 = 0.0030559266451746225 + 0.01 * 6.547122478485107
Epoch 970, val loss: 1.1592265367507935
Epoch 980, training loss: 0.06840869784355164 = 0.002988819731399417 + 0.01 * 6.541987419128418
Epoch 980, val loss: 1.1624665260314941
Epoch 990, training loss: 0.06828943639993668 = 0.002924397587776184 + 0.01 * 6.536503791809082
Epoch 990, val loss: 1.1655231714248657
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.7491
Flip ASR: 0.7156/225 nodes
The final ASR:0.74170, 0.09956, Accuracy:0.82099, 0.00462
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11654])
remove edge: torch.Size([2, 9510])
updated graph: torch.Size([2, 10608])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.97909, 0.00460, Accuracy:0.83333, 0.00524
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.034883499145508 = 1.9511440992355347 + 0.01 * 8.37392807006836
Epoch 0, val loss: 1.944300889968872
Epoch 10, training loss: 2.025496006011963 = 1.9417572021484375 + 0.01 * 8.373888969421387
Epoch 10, val loss: 1.9354979991912842
Epoch 20, training loss: 2.014089345932007 = 1.9303522109985352 + 0.01 * 8.373723983764648
Epoch 20, val loss: 1.9246478080749512
Epoch 30, training loss: 1.9981069564819336 = 1.9143741130828857 + 0.01 * 8.373282432556152
Epoch 30, val loss: 1.909544587135315
Epoch 40, training loss: 1.9742485284805298 = 1.8905340433120728 + 0.01 * 8.371450424194336
Epoch 40, val loss: 1.887405276298523
Epoch 50, training loss: 1.939094066619873 = 1.855499505996704 + 0.01 * 8.359456062316895
Epoch 50, val loss: 1.8560734987258911
Epoch 60, training loss: 1.8930109739303589 = 1.810006856918335 + 0.01 * 8.300413131713867
Epoch 60, val loss: 1.8182172775268555
Epoch 70, training loss: 1.842481017112732 = 1.7624211311340332 + 0.01 * 8.005990028381348
Epoch 70, val loss: 1.7814326286315918
Epoch 80, training loss: 1.789286732673645 = 1.711760401725769 + 0.01 * 7.7526326179504395
Epoch 80, val loss: 1.7394402027130127
Epoch 90, training loss: 1.7190070152282715 = 1.6442242860794067 + 0.01 * 7.478268146514893
Epoch 90, val loss: 1.6799410581588745
Epoch 100, training loss: 1.6299188137054443 = 1.5565710067749023 + 0.01 * 7.3347859382629395
Epoch 100, val loss: 1.6058143377304077
Epoch 110, training loss: 1.5229734182357788 = 1.4505796432495117 + 0.01 * 7.239372730255127
Epoch 110, val loss: 1.5204461812973022
Epoch 120, training loss: 1.4085646867752075 = 1.336946964263916 + 0.01 * 7.161777496337891
Epoch 120, val loss: 1.428971529006958
Epoch 130, training loss: 1.298511266708374 = 1.2273560762405396 + 0.01 * 7.115523338317871
Epoch 130, val loss: 1.3408052921295166
Epoch 140, training loss: 1.1980935335159302 = 1.1272352933883667 + 0.01 * 7.0858283042907715
Epoch 140, val loss: 1.2591593265533447
Epoch 150, training loss: 1.1051079034805298 = 1.034501075744629 + 0.01 * 7.060685157775879
Epoch 150, val loss: 1.1835384368896484
Epoch 160, training loss: 1.01597261428833 = 0.9456400275230408 + 0.01 * 7.033257961273193
Epoch 160, val loss: 1.111595630645752
Epoch 170, training loss: 0.9288360476493835 = 0.8587914109230042 + 0.01 * 7.004463195800781
Epoch 170, val loss: 1.0422924757003784
Epoch 180, training loss: 0.8439668416976929 = 0.7742159962654114 + 0.01 * 6.975082874298096
Epoch 180, val loss: 0.9763491153717041
Epoch 190, training loss: 0.7632454633712769 = 0.693761944770813 + 0.01 * 6.948354721069336
Epoch 190, val loss: 0.9150565266609192
Epoch 200, training loss: 0.688668966293335 = 0.6193628311157227 + 0.01 * 6.930613994598389
Epoch 200, val loss: 0.8604114651679993
Epoch 210, training loss: 0.6211289167404175 = 0.5518901944160461 + 0.01 * 6.923871994018555
Epoch 210, val loss: 0.8134231567382812
Epoch 220, training loss: 0.5604453682899475 = 0.49122563004493713 + 0.01 * 6.921974182128906
Epoch 220, val loss: 0.7741028666496277
Epoch 230, training loss: 0.5057326555252075 = 0.4365290403366089 + 0.01 * 6.92036247253418
Epoch 230, val loss: 0.7415518164634705
Epoch 240, training loss: 0.4562940299510956 = 0.3871053159236908 + 0.01 * 6.918872356414795
Epoch 240, val loss: 0.7145717144012451
Epoch 250, training loss: 0.4118298888206482 = 0.3426624536514282 + 0.01 * 6.916741847991943
Epoch 250, val loss: 0.692729115486145
Epoch 260, training loss: 0.3722717761993408 = 0.30312907695770264 + 0.01 * 6.914271354675293
Epoch 260, val loss: 0.6757045984268188
Epoch 270, training loss: 0.3372812271118164 = 0.2681645452976227 + 0.01 * 6.911667346954346
Epoch 270, val loss: 0.6629911661148071
Epoch 280, training loss: 0.3062136471271515 = 0.2371234893798828 + 0.01 * 6.909016132354736
Epoch 280, val loss: 0.6540282368659973
Epoch 290, training loss: 0.27836108207702637 = 0.20928533375263214 + 0.01 * 6.907576084136963
Epoch 290, val loss: 0.648165762424469
Epoch 300, training loss: 0.2532249093055725 = 0.18418577313423157 + 0.01 * 6.903914451599121
Epoch 300, val loss: 0.644855260848999
Epoch 310, training loss: 0.23067507147789001 = 0.16166119277477264 + 0.01 * 6.9013872146606445
Epoch 310, val loss: 0.6435266137123108
Epoch 320, training loss: 0.21067912876605988 = 0.14169524610042572 + 0.01 * 6.898388385772705
Epoch 320, val loss: 0.6439407467842102
Epoch 330, training loss: 0.1932012140750885 = 0.12424534559249878 + 0.01 * 6.89558744430542
Epoch 330, val loss: 0.6460350751876831
Epoch 340, training loss: 0.17808958888053894 = 0.10916724801063538 + 0.01 * 6.892235279083252
Epoch 340, val loss: 0.6497583985328674
Epoch 350, training loss: 0.16512426733970642 = 0.09624295681715012 + 0.01 * 6.888131141662598
Epoch 350, val loss: 0.6549334526062012
Epoch 360, training loss: 0.15406903624534607 = 0.08521920442581177 + 0.01 * 6.884982585906982
Epoch 360, val loss: 0.6614436507225037
Epoch 370, training loss: 0.14460763335227966 = 0.07581930607557297 + 0.01 * 6.878833293914795
Epoch 370, val loss: 0.6691152453422546
Epoch 380, training loss: 0.13649366796016693 = 0.0677727684378624 + 0.01 * 6.8720903396606445
Epoch 380, val loss: 0.6777120232582092
Epoch 390, training loss: 0.12952014803886414 = 0.06084585189819336 + 0.01 * 6.867428779602051
Epoch 390, val loss: 0.6870351433753967
Epoch 400, training loss: 0.12342750281095505 = 0.05485130846500397 + 0.01 * 6.857619285583496
Epoch 400, val loss: 0.6969226598739624
Epoch 410, training loss: 0.1181536316871643 = 0.049638330936431885 + 0.01 * 6.8515305519104
Epoch 410, val loss: 0.7072563767433167
Epoch 420, training loss: 0.11347723007202148 = 0.045084964483976364 + 0.01 * 6.839227199554443
Epoch 420, val loss: 0.7176795601844788
Epoch 430, training loss: 0.10942503809928894 = 0.041091594845056534 + 0.01 * 6.83334493637085
Epoch 430, val loss: 0.7282664775848389
Epoch 440, training loss: 0.10585693269968033 = 0.03757614642381668 + 0.01 * 6.828078746795654
Epoch 440, val loss: 0.7387306690216064
Epoch 450, training loss: 0.10255496203899384 = 0.03447097912430763 + 0.01 * 6.8083977699279785
Epoch 450, val loss: 0.7492703199386597
Epoch 460, training loss: 0.09976203739643097 = 0.03171846270561218 + 0.01 * 6.804357528686523
Epoch 460, val loss: 0.7595645785331726
Epoch 470, training loss: 0.09711910784244537 = 0.02927258424460888 + 0.01 * 6.784652233123779
Epoch 470, val loss: 0.7697762250900269
Epoch 480, training loss: 0.09483934938907623 = 0.027090022340416908 + 0.01 * 6.774933338165283
Epoch 480, val loss: 0.7796809077262878
Epoch 490, training loss: 0.09273980557918549 = 0.025135738775134087 + 0.01 * 6.760406970977783
Epoch 490, val loss: 0.7894126176834106
Epoch 500, training loss: 0.09087629616260529 = 0.02338024601340294 + 0.01 * 6.749605178833008
Epoch 500, val loss: 0.798941433429718
Epoch 510, training loss: 0.08936022222042084 = 0.021798420697450638 + 0.01 * 6.756180286407471
Epoch 510, val loss: 0.8082363605499268
Epoch 520, training loss: 0.08781213313341141 = 0.020370319485664368 + 0.01 * 6.7441816329956055
Epoch 520, val loss: 0.8172937631607056
Epoch 530, training loss: 0.08634243905544281 = 0.019076339900493622 + 0.01 * 6.72661018371582
Epoch 530, val loss: 0.8260910511016846
Epoch 540, training loss: 0.08505833894014359 = 0.017899861559271812 + 0.01 * 6.715847492218018
Epoch 540, val loss: 0.8346475958824158
Epoch 550, training loss: 0.08404719084501266 = 0.016827577725052834 + 0.01 * 6.721961975097656
Epoch 550, val loss: 0.8430264592170715
Epoch 560, training loss: 0.08292481303215027 = 0.015850726515054703 + 0.01 * 6.707408905029297
Epoch 560, val loss: 0.8511070013046265
Epoch 570, training loss: 0.08195187896490097 = 0.014957563951611519 + 0.01 * 6.699431896209717
Epoch 570, val loss: 0.8589973449707031
Epoch 580, training loss: 0.08104433119297028 = 0.014138015918433666 + 0.01 * 6.690631866455078
Epoch 580, val loss: 0.8666887879371643
Epoch 590, training loss: 0.08028210699558258 = 0.013384359888732433 + 0.01 * 6.689775466918945
Epoch 590, val loss: 0.8741704821586609
Epoch 600, training loss: 0.07946769893169403 = 0.012690111994743347 + 0.01 * 6.677758693695068
Epoch 600, val loss: 0.8814438581466675
Epoch 610, training loss: 0.078917495906353 = 0.012049401178956032 + 0.01 * 6.686809539794922
Epoch 610, val loss: 0.8885600566864014
Epoch 620, training loss: 0.0782073438167572 = 0.011457718908786774 + 0.01 * 6.674962997436523
Epoch 620, val loss: 0.89543217420578
Epoch 630, training loss: 0.07754471153020859 = 0.010909637436270714 + 0.01 * 6.663507461547852
Epoch 630, val loss: 0.9021413326263428
Epoch 640, training loss: 0.07710286974906921 = 0.010400597006082535 + 0.01 * 6.670227527618408
Epoch 640, val loss: 0.9086527228355408
Epoch 650, training loss: 0.07650750875473022 = 0.0099276602268219 + 0.01 * 6.657984733581543
Epoch 650, val loss: 0.9150298237800598
Epoch 660, training loss: 0.07596052438020706 = 0.009487400762736797 + 0.01 * 6.647313117980957
Epoch 660, val loss: 0.921295702457428
Epoch 670, training loss: 0.07565387338399887 = 0.009076854214072227 + 0.01 * 6.6577019691467285
Epoch 670, val loss: 0.9273735284805298
Epoch 680, training loss: 0.07512529194355011 = 0.008694007992744446 + 0.01 * 6.643128871917725
Epoch 680, val loss: 0.9332171678543091
Epoch 690, training loss: 0.07473079115152359 = 0.00833656545728445 + 0.01 * 6.639422416687012
Epoch 690, val loss: 0.938957929611206
Epoch 700, training loss: 0.07426384836435318 = 0.008002064190804958 + 0.01 * 6.626178741455078
Epoch 700, val loss: 0.9446088075637817
Epoch 710, training loss: 0.0741332396864891 = 0.007688338868319988 + 0.01 * 6.644490718841553
Epoch 710, val loss: 0.9500717520713806
Epoch 720, training loss: 0.07366066426038742 = 0.0073938858695328236 + 0.01 * 6.626677513122559
Epoch 720, val loss: 0.9553739428520203
Epoch 730, training loss: 0.0732993334531784 = 0.0071167410351336 + 0.01 * 6.618258953094482
Epoch 730, val loss: 0.9606721997261047
Epoch 740, training loss: 0.07304529845714569 = 0.006855035666376352 + 0.01 * 6.6190266609191895
Epoch 740, val loss: 0.9658112525939941
Epoch 750, training loss: 0.07267748564481735 = 0.006607524119317532 + 0.01 * 6.606996059417725
Epoch 750, val loss: 0.9709262251853943
Epoch 760, training loss: 0.07241672277450562 = 0.006373496260493994 + 0.01 * 6.604322910308838
Epoch 760, val loss: 0.9758725762367249
Epoch 770, training loss: 0.07236512005329132 = 0.0061517320573329926 + 0.01 * 6.621338367462158
Epoch 770, val loss: 0.98088538646698
Epoch 780, training loss: 0.07186001539230347 = 0.005941464100033045 + 0.01 * 6.591855049133301
Epoch 780, val loss: 0.9856688380241394
Epoch 790, training loss: 0.07188413292169571 = 0.0057422989048063755 + 0.01 * 6.61418342590332
Epoch 790, val loss: 0.9904899597167969
Epoch 800, training loss: 0.0714605376124382 = 0.005553987808525562 + 0.01 * 6.5906548500061035
Epoch 800, val loss: 0.9951232671737671
Epoch 810, training loss: 0.0712280198931694 = 0.00537582952529192 + 0.01 * 6.585218906402588
Epoch 810, val loss: 0.999700129032135
Epoch 820, training loss: 0.07105392962694168 = 0.0052063604816794395 + 0.01 * 6.584756851196289
Epoch 820, val loss: 1.004241704940796
Epoch 830, training loss: 0.07087109982967377 = 0.005046206992119551 + 0.01 * 6.582489967346191
Epoch 830, val loss: 1.008599877357483
Epoch 840, training loss: 0.0705728754401207 = 0.0048942421562969685 + 0.01 * 6.567863464355469
Epoch 840, val loss: 1.0129531621932983
Epoch 850, training loss: 0.07063601166009903 = 0.004749480169266462 + 0.01 * 6.588653564453125
Epoch 850, val loss: 1.017160177230835
Epoch 860, training loss: 0.07031609863042831 = 0.004612861666828394 + 0.01 * 6.570323944091797
Epoch 860, val loss: 1.0211915969848633
Epoch 870, training loss: 0.07020053267478943 = 0.004482013639062643 + 0.01 * 6.57185173034668
Epoch 870, val loss: 1.0253045558929443
Epoch 880, training loss: 0.07002625614404678 = 0.0043578119948506355 + 0.01 * 6.5668439865112305
Epoch 880, val loss: 1.029292345046997
Epoch 890, training loss: 0.06976226717233658 = 0.004239698871970177 + 0.01 * 6.552257061004639
Epoch 890, val loss: 1.033084750175476
Epoch 900, training loss: 0.06986632198095322 = 0.004126538056880236 + 0.01 * 6.573978424072266
Epoch 900, val loss: 1.0369834899902344
Epoch 910, training loss: 0.06959439814090729 = 0.00401899591088295 + 0.01 * 6.5575408935546875
Epoch 910, val loss: 1.0406330823898315
Epoch 920, training loss: 0.06959268450737 = 0.0039163678884506226 + 0.01 * 6.567631721496582
Epoch 920, val loss: 1.0443271398544312
Epoch 930, training loss: 0.06929369270801544 = 0.0038181981071829796 + 0.01 * 6.547549724578857
Epoch 930, val loss: 1.0478788614273071
Epoch 940, training loss: 0.06930136680603027 = 0.0037241920363157988 + 0.01 * 6.557717323303223
Epoch 940, val loss: 1.0513675212860107
Epoch 950, training loss: 0.0689711794257164 = 0.003634441876783967 + 0.01 * 6.533673286437988
Epoch 950, val loss: 1.0547932386398315
Epoch 960, training loss: 0.06902050971984863 = 0.0035485157277435064 + 0.01 * 6.547199249267578
Epoch 960, val loss: 1.0582726001739502
Epoch 970, training loss: 0.06887379288673401 = 0.003466216614469886 + 0.01 * 6.540757656097412
Epoch 970, val loss: 1.0614556074142456
Epoch 980, training loss: 0.06875123083591461 = 0.0033874984364956617 + 0.01 * 6.536373615264893
Epoch 980, val loss: 1.0647480487823486
Epoch 990, training loss: 0.06889625638723373 = 0.003311819862574339 + 0.01 * 6.558444023132324
Epoch 990, val loss: 1.0679211616516113
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.6531
Flip ASR: 0.5867/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0171351432800293 = 1.9333958625793457 + 0.01 * 8.373930931091309
Epoch 0, val loss: 1.922057867050171
Epoch 10, training loss: 2.0072429180145264 = 1.9235039949417114 + 0.01 * 8.373889923095703
Epoch 10, val loss: 1.9124329090118408
Epoch 20, training loss: 1.9952603578567505 = 1.9115227460861206 + 0.01 * 8.373757362365723
Epoch 20, val loss: 1.9002918004989624
Epoch 30, training loss: 1.9786726236343384 = 1.8949388265609741 + 0.01 * 8.373384475708008
Epoch 30, val loss: 1.8832201957702637
Epoch 40, training loss: 1.9546066522598267 = 1.8708888292312622 + 0.01 * 8.371787071228027
Epoch 40, val loss: 1.8587608337402344
Epoch 50, training loss: 1.921342134475708 = 1.8377362489700317 + 0.01 * 8.360586166381836
Epoch 50, val loss: 1.8266340494155884
Epoch 60, training loss: 1.8822364807128906 = 1.7992726564407349 + 0.01 * 8.296379089355469
Epoch 60, val loss: 1.7932530641555786
Epoch 70, training loss: 1.8416954278945923 = 1.7610365152359009 + 0.01 * 8.065890312194824
Epoch 70, val loss: 1.7639249563217163
Epoch 80, training loss: 1.792222023010254 = 1.712877869606018 + 0.01 * 7.934412002563477
Epoch 80, val loss: 1.7250471115112305
Epoch 90, training loss: 1.7227468490600586 = 1.6460133790969849 + 0.01 * 7.673344135284424
Epoch 90, val loss: 1.6687284708023071
Epoch 100, training loss: 1.6322791576385498 = 1.5590416193008423 + 0.01 * 7.32375431060791
Epoch 100, val loss: 1.5956807136535645
Epoch 110, training loss: 1.5294396877288818 = 1.457720160484314 + 0.01 * 7.171955108642578
Epoch 110, val loss: 1.512949824333191
Epoch 120, training loss: 1.421228051185608 = 1.3499478101730347 + 0.01 * 7.128024101257324
Epoch 120, val loss: 1.4278275966644287
Epoch 130, training loss: 1.3099703788757324 = 1.2389966249465942 + 0.01 * 7.097373962402344
Epoch 130, val loss: 1.341849684715271
Epoch 140, training loss: 1.1968125104904175 = 1.1259348392486572 + 0.01 * 7.087766170501709
Epoch 140, val loss: 1.2554994821548462
Epoch 150, training loss: 1.0846632719039917 = 1.0138517618179321 + 0.01 * 7.08114767074585
Epoch 150, val loss: 1.170461893081665
Epoch 160, training loss: 0.9777219891548157 = 0.9070049524307251 + 0.01 * 7.071702003479004
Epoch 160, val loss: 1.0888820886611938
Epoch 170, training loss: 0.879844605922699 = 0.8092935681343079 + 0.01 * 7.055104732513428
Epoch 170, val loss: 1.0141226053237915
Epoch 180, training loss: 0.7933547496795654 = 0.723078191280365 + 0.01 * 7.027655601501465
Epoch 180, val loss: 0.9486474990844727
Epoch 190, training loss: 0.7184880971908569 = 0.6484935283660889 + 0.01 * 6.999459266662598
Epoch 190, val loss: 0.893973708152771
Epoch 200, training loss: 0.6538487076759338 = 0.584097146987915 + 0.01 * 6.975154399871826
Epoch 200, val loss: 0.849911630153656
Epoch 210, training loss: 0.597555935382843 = 0.5279264450073242 + 0.01 * 6.962951183319092
Epoch 210, val loss: 0.814965009689331
Epoch 220, training loss: 0.5476013422012329 = 0.47808408737182617 + 0.01 * 6.951723575592041
Epoch 220, val loss: 0.7875635623931885
Epoch 230, training loss: 0.5022550225257874 = 0.4327569007873535 + 0.01 * 6.9498114585876465
Epoch 230, val loss: 0.7659921646118164
Epoch 240, training loss: 0.45985546708106995 = 0.3904876410961151 + 0.01 * 6.936782360076904
Epoch 240, val loss: 0.7493128180503845
Epoch 250, training loss: 0.41956496238708496 = 0.3502892553806305 + 0.01 * 6.927571773529053
Epoch 250, val loss: 0.7364953756332397
Epoch 260, training loss: 0.3809824287891388 = 0.31177541613578796 + 0.01 * 6.920700550079346
Epoch 260, val loss: 0.7268459796905518
Epoch 270, training loss: 0.344296932220459 = 0.27516281604766846 + 0.01 * 6.9134135246276855
Epoch 270, val loss: 0.72036212682724
Epoch 280, training loss: 0.3101223409175873 = 0.24099203944206238 + 0.01 * 6.913029670715332
Epoch 280, val loss: 0.7169162034988403
Epoch 290, training loss: 0.2789115309715271 = 0.20992375910282135 + 0.01 * 6.898776054382324
Epoch 290, val loss: 0.7164925336837769
Epoch 300, training loss: 0.2514769732952118 = 0.18248161673545837 + 0.01 * 6.899535179138184
Epoch 300, val loss: 0.7193697690963745
Epoch 310, training loss: 0.22768843173980713 = 0.1588197648525238 + 0.01 * 6.886866569519043
Epoch 310, val loss: 0.7252525687217712
Epoch 320, training loss: 0.20752431452274323 = 0.13871034979820251 + 0.01 * 6.881396293640137
Epoch 320, val loss: 0.7337985038757324
Epoch 330, training loss: 0.19047671556472778 = 0.12171511352062225 + 0.01 * 6.876160621643066
Epoch 330, val loss: 0.7445494532585144
Epoch 340, training loss: 0.17604205012321472 = 0.10733367502689362 + 0.01 * 6.870838165283203
Epoch 340, val loss: 0.7569906115531921
Epoch 350, training loss: 0.16367724537849426 = 0.09509672969579697 + 0.01 * 6.858052730560303
Epoch 350, val loss: 0.7707492709159851
Epoch 360, training loss: 0.15322345495224 = 0.08460953086614609 + 0.01 * 6.861392021179199
Epoch 360, val loss: 0.7855054140090942
Epoch 370, training loss: 0.1439719945192337 = 0.07555209845304489 + 0.01 * 6.841989517211914
Epoch 370, val loss: 0.8010948300361633
Epoch 380, training loss: 0.13608187437057495 = 0.0676746517419815 + 0.01 * 6.840722560882568
Epoch 380, val loss: 0.8173343539237976
Epoch 390, training loss: 0.12912210822105408 = 0.060790129005908966 + 0.01 * 6.8331990242004395
Epoch 390, val loss: 0.8340420722961426
Epoch 400, training loss: 0.12301668524742126 = 0.05474511906504631 + 0.01 * 6.827157497406006
Epoch 400, val loss: 0.8511713743209839
Epoch 410, training loss: 0.11755955219268799 = 0.04942193999886513 + 0.01 * 6.813760757446289
Epoch 410, val loss: 0.8685247302055359
Epoch 420, training loss: 0.1128322035074234 = 0.04472525045275688 + 0.01 * 6.810696125030518
Epoch 420, val loss: 0.8859373927116394
Epoch 430, training loss: 0.1086098849773407 = 0.040582749992609024 + 0.01 * 6.8027143478393555
Epoch 430, val loss: 0.9033223986625671
Epoch 440, training loss: 0.1048424169421196 = 0.036922089755535126 + 0.01 * 6.7920331954956055
Epoch 440, val loss: 0.9205541610717773
Epoch 450, training loss: 0.1015847772359848 = 0.03368333354592323 + 0.01 * 6.790144920349121
Epoch 450, val loss: 0.937553882598877
Epoch 460, training loss: 0.09856438636779785 = 0.03081684000790119 + 0.01 * 6.774755001068115
Epoch 460, val loss: 0.9542403817176819
Epoch 470, training loss: 0.09592387080192566 = 0.028272371739149094 + 0.01 * 6.76515007019043
Epoch 470, val loss: 0.9705725908279419
Epoch 480, training loss: 0.09361432492733002 = 0.026009760797023773 + 0.01 * 6.760456562042236
Epoch 480, val loss: 0.9864572882652283
Epoch 490, training loss: 0.09147690236568451 = 0.02399478293955326 + 0.01 * 6.7482123374938965
Epoch 490, val loss: 1.0018515586853027
Epoch 500, training loss: 0.08960483968257904 = 0.02219434268772602 + 0.01 * 6.741049766540527
Epoch 500, val loss: 1.0167953968048096
Epoch 510, training loss: 0.08807768672704697 = 0.020583009347319603 + 0.01 * 6.749467372894287
Epoch 510, val loss: 1.031274437904358
Epoch 520, training loss: 0.08639588952064514 = 0.019137067720294 + 0.01 * 6.725882053375244
Epoch 520, val loss: 1.0452431440353394
Epoch 530, training loss: 0.08501285314559937 = 0.017833877354860306 + 0.01 * 6.717897415161133
Epoch 530, val loss: 1.0588271617889404
Epoch 540, training loss: 0.08392976969480515 = 0.016657495871186256 + 0.01 * 6.727227687835693
Epoch 540, val loss: 1.0719588994979858
Epoch 550, training loss: 0.08272077888250351 = 0.015593640506267548 + 0.01 * 6.712714195251465
Epoch 550, val loss: 1.0847227573394775
Epoch 560, training loss: 0.08189261704683304 = 0.014627874828875065 + 0.01 * 6.726474761962891
Epoch 560, val loss: 1.0971360206604004
Epoch 570, training loss: 0.08080361038446426 = 0.013749849982559681 + 0.01 * 6.705375671386719
Epoch 570, val loss: 1.1091545820236206
Epoch 580, training loss: 0.07997126877307892 = 0.012949125841259956 + 0.01 * 6.70221471786499
Epoch 580, val loss: 1.1208593845367432
Epoch 590, training loss: 0.07904055714607239 = 0.012217261828482151 + 0.01 * 6.682330131530762
Epoch 590, val loss: 1.1322178840637207
Epoch 600, training loss: 0.07828906923532486 = 0.011546804569661617 + 0.01 * 6.674226760864258
Epoch 600, val loss: 1.1432628631591797
Epoch 610, training loss: 0.07800156623125076 = 0.010931051336228848 + 0.01 * 6.7070512771606445
Epoch 610, val loss: 1.1540720462799072
Epoch 620, training loss: 0.07701145857572556 = 0.01036508847028017 + 0.01 * 6.664636611938477
Epoch 620, val loss: 1.1644865274429321
Epoch 630, training loss: 0.07645410299301147 = 0.009843651205301285 + 0.01 * 6.661045551300049
Epoch 630, val loss: 1.1746513843536377
Epoch 640, training loss: 0.07587405294179916 = 0.009362312033772469 + 0.01 * 6.651174545288086
Epoch 640, val loss: 1.184610366821289
Epoch 650, training loss: 0.07533209770917892 = 0.008916771039366722 + 0.01 * 6.641532897949219
Epoch 650, val loss: 1.194275975227356
Epoch 660, training loss: 0.07489091157913208 = 0.008503667078912258 + 0.01 * 6.6387248039245605
Epoch 660, val loss: 1.2036935091018677
Epoch 670, training loss: 0.07439424097537994 = 0.008120255544781685 + 0.01 * 6.62739896774292
Epoch 670, val loss: 1.2128971815109253
Epoch 680, training loss: 0.07438590377569199 = 0.007763310801237822 + 0.01 * 6.662259578704834
Epoch 680, val loss: 1.2218682765960693
Epoch 690, training loss: 0.07362199574708939 = 0.007431210018694401 + 0.01 * 6.619079113006592
Epoch 690, val loss: 1.2305625677108765
Epoch 700, training loss: 0.07346021384000778 = 0.007121304050087929 + 0.01 * 6.633891582489014
Epoch 700, val loss: 1.2390917539596558
Epoch 710, training loss: 0.07308227568864822 = 0.006831462495028973 + 0.01 * 6.625081539154053
Epoch 710, val loss: 1.2474604845046997
Epoch 720, training loss: 0.07267045229673386 = 0.006560062523931265 + 0.01 * 6.611039161682129
Epoch 720, val loss: 1.2555837631225586
Epoch 730, training loss: 0.07235366106033325 = 0.006305130664259195 + 0.01 * 6.60485315322876
Epoch 730, val loss: 1.2635488510131836
Epoch 740, training loss: 0.0721546933054924 = 0.006065888796001673 + 0.01 * 6.608880519866943
Epoch 740, val loss: 1.2712737321853638
Epoch 750, training loss: 0.07183319330215454 = 0.005840960890054703 + 0.01 * 6.599222660064697
Epoch 750, val loss: 1.2789530754089355
Epoch 760, training loss: 0.07173667848110199 = 0.005629525985568762 + 0.01 * 6.610715866088867
Epoch 760, val loss: 1.2862995862960815
Epoch 770, training loss: 0.0714341253042221 = 0.005430413410067558 + 0.01 * 6.60037088394165
Epoch 770, val loss: 1.293623924255371
Epoch 780, training loss: 0.07102516293525696 = 0.005242984741926193 + 0.01 * 6.578218460083008
Epoch 780, val loss: 1.3007135391235352
Epoch 790, training loss: 0.07097546756267548 = 0.005065759178251028 + 0.01 * 6.590970993041992
Epoch 790, val loss: 1.3076205253601074
Epoch 800, training loss: 0.07059445232152939 = 0.004898826591670513 + 0.01 * 6.569562911987305
Epoch 800, val loss: 1.314452886581421
Epoch 810, training loss: 0.07048431038856506 = 0.004740662407130003 + 0.01 * 6.574365139007568
Epoch 810, val loss: 1.3209764957427979
Epoch 820, training loss: 0.0702660009264946 = 0.004591705277562141 + 0.01 * 6.567430019378662
Epoch 820, val loss: 1.3275989294052124
Epoch 830, training loss: 0.07023309171199799 = 0.0044500017538666725 + 0.01 * 6.578309535980225
Epoch 830, val loss: 1.3338496685028076
Epoch 840, training loss: 0.06994611024856567 = 0.004316178150475025 + 0.01 * 6.562993049621582
Epoch 840, val loss: 1.3401159048080444
Epoch 850, training loss: 0.06974157691001892 = 0.00418886449187994 + 0.01 * 6.555271148681641
Epoch 850, val loss: 1.346190094947815
Epoch 860, training loss: 0.06966709345579147 = 0.004067923408001661 + 0.01 * 6.559917449951172
Epoch 860, val loss: 1.3519808053970337
Epoch 870, training loss: 0.06970039755105972 = 0.003953464329242706 + 0.01 * 6.574693202972412
Epoch 870, val loss: 1.3579295873641968
Epoch 880, training loss: 0.06939466297626495 = 0.0038445014506578445 + 0.01 * 6.55501651763916
Epoch 880, val loss: 1.3635432720184326
Epoch 890, training loss: 0.06927210837602615 = 0.003740332555025816 + 0.01 * 6.553177356719971
Epoch 890, val loss: 1.3690983057022095
Epoch 900, training loss: 0.06912001967430115 = 0.0036414770875126123 + 0.01 * 6.547853946685791
Epoch 900, val loss: 1.3746182918548584
Epoch 910, training loss: 0.06900885701179504 = 0.003546883352100849 + 0.01 * 6.546197414398193
Epoch 910, val loss: 1.3798794746398926
Epoch 920, training loss: 0.0690341368317604 = 0.0034568726550787687 + 0.01 * 6.5577263832092285
Epoch 920, val loss: 1.3850830793380737
Epoch 930, training loss: 0.06877048313617706 = 0.003370963502675295 + 0.01 * 6.539952278137207
Epoch 930, val loss: 1.3902349472045898
Epoch 940, training loss: 0.06865259259939194 = 0.003288716310635209 + 0.01 * 6.536387920379639
Epoch 940, val loss: 1.3952020406723022
Epoch 950, training loss: 0.06867408752441406 = 0.003210316877812147 + 0.01 * 6.546377182006836
Epoch 950, val loss: 1.400140404701233
Epoch 960, training loss: 0.06847775727510452 = 0.00313510256819427 + 0.01 * 6.534265995025635
Epoch 960, val loss: 1.4048908948898315
Epoch 970, training loss: 0.06835922598838806 = 0.0030631728004664183 + 0.01 * 6.529605865478516
Epoch 970, val loss: 1.4096113443374634
Epoch 980, training loss: 0.06830572336912155 = 0.002993942005559802 + 0.01 * 6.5311784744262695
Epoch 980, val loss: 1.4141443967819214
Epoch 990, training loss: 0.06820004433393478 = 0.0029281156603246927 + 0.01 * 6.527193069458008
Epoch 990, val loss: 1.4186879396438599
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.7749
Flip ASR: 0.7289/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.015320301055908 = 1.9315811395645142 + 0.01 * 8.373910903930664
Epoch 0, val loss: 1.9265046119689941
Epoch 10, training loss: 2.006376266479492 = 1.9226375818252563 + 0.01 * 8.373859405517578
Epoch 10, val loss: 1.917791485786438
Epoch 20, training loss: 1.9955596923828125 = 1.911823034286499 + 0.01 * 8.37366771697998
Epoch 20, val loss: 1.9068578481674194
Epoch 30, training loss: 1.980433464050293 = 1.896702527999878 + 0.01 * 8.373088836669922
Epoch 30, val loss: 1.8912124633789062
Epoch 40, training loss: 1.9579576253890991 = 1.874255657196045 + 0.01 * 8.370198249816895
Epoch 40, val loss: 1.8678858280181885
Epoch 50, training loss: 1.9253414869308472 = 1.8418482542037964 + 0.01 * 8.349328994750977
Epoch 50, val loss: 1.8352428674697876
Epoch 60, training loss: 1.884859561920166 = 1.8025460243225098 + 0.01 * 8.231359481811523
Epoch 60, val loss: 1.7989388704299927
Epoch 70, training loss: 1.8425204753875732 = 1.7635538578033447 + 0.01 * 7.8966569900512695
Epoch 70, val loss: 1.7665672302246094
Epoch 80, training loss: 1.7910422086715698 = 1.7145508527755737 + 0.01 * 7.6491312980651855
Epoch 80, val loss: 1.7253392934799194
Epoch 90, training loss: 1.7199209928512573 = 1.6466659307479858 + 0.01 * 7.325503826141357
Epoch 90, val loss: 1.667306661605835
Epoch 100, training loss: 1.6282422542572021 = 1.5566108226776123 + 0.01 * 7.163144588470459
Epoch 100, val loss: 1.591207504272461
Epoch 110, training loss: 1.5216559171676636 = 1.450666069984436 + 0.01 * 7.09898567199707
Epoch 110, val loss: 1.503680944442749
Epoch 120, training loss: 1.4093399047851562 = 1.3389824628829956 + 0.01 * 7.035747528076172
Epoch 120, val loss: 1.4150466918945312
Epoch 130, training loss: 1.2964545488357544 = 1.2265408039093018 + 0.01 * 6.9913716316223145
Epoch 130, val loss: 1.3293126821517944
Epoch 140, training loss: 1.1856927871704102 = 1.116042137145996 + 0.01 * 6.965064525604248
Epoch 140, val loss: 1.2465801239013672
Epoch 150, training loss: 1.081007719039917 = 1.0115301609039307 + 0.01 * 6.947755336761475
Epoch 150, val loss: 1.1685441732406616
Epoch 160, training loss: 0.9845947623252869 = 0.9152354598045349 + 0.01 * 6.935928821563721
Epoch 160, val loss: 1.0966066122055054
Epoch 170, training loss: 0.8965732455253601 = 0.8272873759269714 + 0.01 * 6.928587913513184
Epoch 170, val loss: 1.030997633934021
Epoch 180, training loss: 0.8165762424468994 = 0.7473278641700745 + 0.01 * 6.9248366355896
Epoch 180, val loss: 0.9714856147766113
Epoch 190, training loss: 0.7441602349281311 = 0.6749333739280701 + 0.01 * 6.922685146331787
Epoch 190, val loss: 0.9185319542884827
Epoch 200, training loss: 0.6781244874000549 = 0.6089125275611877 + 0.01 * 6.921195983886719
Epoch 200, val loss: 0.8718039393424988
Epoch 210, training loss: 0.616743266582489 = 0.54755038022995 + 0.01 * 6.91928768157959
Epoch 210, val loss: 0.8305155038833618
Epoch 220, training loss: 0.5588727593421936 = 0.4897003769874573 + 0.01 * 6.917240619659424
Epoch 220, val loss: 0.7944256663322449
Epoch 230, training loss: 0.5042885541915894 = 0.4351291358470917 + 0.01 * 6.9159440994262695
Epoch 230, val loss: 0.7630211114883423
Epoch 240, training loss: 0.4533456265926361 = 0.3842089772224426 + 0.01 * 6.9136643409729
Epoch 240, val loss: 0.7360019087791443
Epoch 250, training loss: 0.40636366605758667 = 0.33724597096443176 + 0.01 * 6.911769866943359
Epoch 250, val loss: 0.7131754755973816
Epoch 260, training loss: 0.3634662628173828 = 0.2943638265132904 + 0.01 * 6.910244941711426
Epoch 260, val loss: 0.694607675075531
Epoch 270, training loss: 0.32484960556030273 = 0.2557610869407654 + 0.01 * 6.908850193023682
Epoch 270, val loss: 0.6804538369178772
Epoch 280, training loss: 0.2906545102596283 = 0.22157877683639526 + 0.01 * 6.907573699951172
Epoch 280, val loss: 0.6708255410194397
Epoch 290, training loss: 0.2608622908592224 = 0.1917867511510849 + 0.01 * 6.907552719116211
Epoch 290, val loss: 0.6655502915382385
Epoch 300, training loss: 0.2352040708065033 = 0.16614489257335663 + 0.01 * 6.905917167663574
Epoch 300, val loss: 0.6643867492675781
Epoch 310, training loss: 0.21330174803733826 = 0.14425508677959442 + 0.01 * 6.904666423797607
Epoch 310, val loss: 0.6669149398803711
Epoch 320, training loss: 0.1946801394224167 = 0.1256229430437088 + 0.01 * 6.905719757080078
Epoch 320, val loss: 0.6725301742553711
Epoch 330, training loss: 0.17875763773918152 = 0.10973397642374039 + 0.01 * 6.902367115020752
Epoch 330, val loss: 0.6806221008300781
Epoch 340, training loss: 0.16516783833503723 = 0.09617844223976135 + 0.01 * 6.898940086364746
Epoch 340, val loss: 0.6907826662063599
Epoch 350, training loss: 0.15355724096298218 = 0.08457381278276443 + 0.01 * 6.898342609405518
Epoch 350, val loss: 0.7024577856063843
Epoch 360, training loss: 0.14356687664985657 = 0.0746367797255516 + 0.01 * 6.893009185791016
Epoch 360, val loss: 0.715175211429596
Epoch 370, training loss: 0.13500845432281494 = 0.06610819697380066 + 0.01 * 6.890025615692139
Epoch 370, val loss: 0.7285125851631165
Epoch 380, training loss: 0.1276816576719284 = 0.05877899378538132 + 0.01 * 6.890266418457031
Epoch 380, val loss: 0.7422958016395569
Epoch 390, training loss: 0.12129687517881393 = 0.05247388035058975 + 0.01 * 6.882299423217773
Epoch 390, val loss: 0.7562727928161621
Epoch 400, training loss: 0.11593210697174072 = 0.047038834542036057 + 0.01 * 6.889327526092529
Epoch 400, val loss: 0.7702506184577942
Epoch 410, training loss: 0.11104641109704971 = 0.04234384745359421 + 0.01 * 6.870256423950195
Epoch 410, val loss: 0.7840434312820435
Epoch 420, training loss: 0.10687170922756195 = 0.038270220160484314 + 0.01 * 6.860149383544922
Epoch 420, val loss: 0.7976102828979492
Epoch 430, training loss: 0.10331571102142334 = 0.034721847623586655 + 0.01 * 6.859386444091797
Epoch 430, val loss: 0.8109400272369385
Epoch 440, training loss: 0.1000840961933136 = 0.03162665292620659 + 0.01 * 6.845745086669922
Epoch 440, val loss: 0.8238835334777832
Epoch 450, training loss: 0.09720838069915771 = 0.028916265815496445 + 0.01 * 6.829212188720703
Epoch 450, val loss: 0.8364612460136414
Epoch 460, training loss: 0.09496162831783295 = 0.026532964780926704 + 0.01 * 6.842866897583008
Epoch 460, val loss: 0.8486682176589966
Epoch 470, training loss: 0.09244035184383392 = 0.024433059617877007 + 0.01 * 6.800729274749756
Epoch 470, val loss: 0.8604523539543152
Epoch 480, training loss: 0.0904834121465683 = 0.022571071982383728 + 0.01 * 6.791234016418457
Epoch 480, val loss: 0.8719343543052673
Epoch 490, training loss: 0.0888008177280426 = 0.02091258391737938 + 0.01 * 6.78882360458374
Epoch 490, val loss: 0.8830968141555786
Epoch 500, training loss: 0.08712738752365112 = 0.019433874636888504 + 0.01 * 6.769351482391357
Epoch 500, val loss: 0.8938564658164978
Epoch 510, training loss: 0.08575589954853058 = 0.01811075024306774 + 0.01 * 6.764514923095703
Epoch 510, val loss: 0.9042758941650391
Epoch 520, training loss: 0.08441577106714249 = 0.016920560970902443 + 0.01 * 6.749521255493164
Epoch 520, val loss: 0.9143811464309692
Epoch 530, training loss: 0.08346324414014816 = 0.01584649831056595 + 0.01 * 6.761674880981445
Epoch 530, val loss: 0.9242031574249268
Epoch 540, training loss: 0.08231263607740402 = 0.014875182881951332 + 0.01 * 6.74374532699585
Epoch 540, val loss: 0.9337440729141235
Epoch 550, training loss: 0.08125826716423035 = 0.01399267464876175 + 0.01 * 6.726559162139893
Epoch 550, val loss: 0.94302898645401
Epoch 560, training loss: 0.0806862860918045 = 0.013189040124416351 + 0.01 * 6.749724864959717
Epoch 560, val loss: 0.9520822167396545
Epoch 570, training loss: 0.07974091172218323 = 0.012457520700991154 + 0.01 * 6.728339195251465
Epoch 570, val loss: 0.9607692956924438
Epoch 580, training loss: 0.07885929197072983 = 0.011788341216742992 + 0.01 * 6.707095146179199
Epoch 580, val loss: 0.9692235589027405
Epoch 590, training loss: 0.07821185141801834 = 0.011173692531883717 + 0.01 * 6.703815460205078
Epoch 590, val loss: 0.977480411529541
Epoch 600, training loss: 0.07767980545759201 = 0.010607955045998096 + 0.01 * 6.7071852684021
Epoch 600, val loss: 0.9855284690856934
Epoch 610, training loss: 0.07709357887506485 = 0.010087202303111553 + 0.01 * 6.700638294219971
Epoch 610, val loss: 0.9933094382286072
Epoch 620, training loss: 0.0764559730887413 = 0.009607064537703991 + 0.01 * 6.684891223907471
Epoch 620, val loss: 1.0009135007858276
Epoch 630, training loss: 0.07597766816616058 = 0.00916280783712864 + 0.01 * 6.681486129760742
Epoch 630, val loss: 1.0083061456680298
Epoch 640, training loss: 0.07554708421230316 = 0.008750593289732933 + 0.01 * 6.6796488761901855
Epoch 640, val loss: 1.0155391693115234
Epoch 650, training loss: 0.07502692937850952 = 0.008368199691176414 + 0.01 * 6.665873050689697
Epoch 650, val loss: 1.0225204229354858
Epoch 660, training loss: 0.07465602457523346 = 0.008012715727090836 + 0.01 * 6.66433048248291
Epoch 660, val loss: 1.0293477773666382
Epoch 670, training loss: 0.07420475780963898 = 0.00768138887360692 + 0.01 * 6.652336597442627
Epoch 670, val loss: 1.035970687866211
Epoch 680, training loss: 0.07393409311771393 = 0.007371837738901377 + 0.01 * 6.656225681304932
Epoch 680, val loss: 1.0424619913101196
Epoch 690, training loss: 0.07389695197343826 = 0.007082244846969843 + 0.01 * 6.6814703941345215
Epoch 690, val loss: 1.048768401145935
Epoch 700, training loss: 0.07324200868606567 = 0.0068118637427687645 + 0.01 * 6.643014430999756
Epoch 700, val loss: 1.0549544095993042
Epoch 710, training loss: 0.0730244517326355 = 0.006558496039360762 + 0.01 * 6.6465959548950195
Epoch 710, val loss: 1.0609984397888184
Epoch 720, training loss: 0.07268255949020386 = 0.006320335436612368 + 0.01 * 6.6362223625183105
Epoch 720, val loss: 1.0668883323669434
Epoch 730, training loss: 0.0723484456539154 = 0.006096482742577791 + 0.01 * 6.62519645690918
Epoch 730, val loss: 1.0726510286331177
Epoch 740, training loss: 0.07217392325401306 = 0.005885460413992405 + 0.01 * 6.628846168518066
Epoch 740, val loss: 1.0783498287200928
Epoch 750, training loss: 0.07190455496311188 = 0.005686531774699688 + 0.01 * 6.621802806854248
Epoch 750, val loss: 1.083848476409912
Epoch 760, training loss: 0.07187081128358841 = 0.005498823244124651 + 0.01 * 6.6371989250183105
Epoch 760, val loss: 1.08927583694458
Epoch 770, training loss: 0.07149827480316162 = 0.005321499891579151 + 0.01 * 6.617677211761475
Epoch 770, val loss: 1.0945478677749634
Epoch 780, training loss: 0.07112115621566772 = 0.0051537747494876385 + 0.01 * 6.596738338470459
Epoch 780, val loss: 1.0997223854064941
Epoch 790, training loss: 0.07116315513849258 = 0.004994949791580439 + 0.01 * 6.616820335388184
Epoch 790, val loss: 1.104792594909668
Epoch 800, training loss: 0.0707850530743599 = 0.004845054820179939 + 0.01 * 6.594000339508057
Epoch 800, val loss: 1.1096874475479126
Epoch 810, training loss: 0.0707092136144638 = 0.004702286329120398 + 0.01 * 6.6006927490234375
Epoch 810, val loss: 1.1145474910736084
Epoch 820, training loss: 0.07070274651050568 = 0.004567041527479887 + 0.01 * 6.613570690155029
Epoch 820, val loss: 1.1192445755004883
Epoch 830, training loss: 0.07034939527511597 = 0.00443843100219965 + 0.01 * 6.5910964012146
Epoch 830, val loss: 1.123874545097351
Epoch 840, training loss: 0.07008551806211472 = 0.004316018428653479 + 0.01 * 6.5769500732421875
Epoch 840, val loss: 1.1284106969833374
Epoch 850, training loss: 0.06994683295488358 = 0.004199625458568335 + 0.01 * 6.574721336364746
Epoch 850, val loss: 1.1328449249267578
Epoch 860, training loss: 0.06975653022527695 = 0.004088480491191149 + 0.01 * 6.566804885864258
Epoch 860, val loss: 1.1372339725494385
Epoch 870, training loss: 0.06981120258569717 = 0.003982546739280224 + 0.01 * 6.5828657150268555
Epoch 870, val loss: 1.1414672136306763
Epoch 880, training loss: 0.06957561522722244 = 0.0038816609885543585 + 0.01 * 6.569395065307617
Epoch 880, val loss: 1.1456589698791504
Epoch 890, training loss: 0.06942424923181534 = 0.0037852704990655184 + 0.01 * 6.56389856338501
Epoch 890, val loss: 1.1497838497161865
Epoch 900, training loss: 0.06930860877037048 = 0.0036929023917764425 + 0.01 * 6.56157112121582
Epoch 900, val loss: 1.1538665294647217
Epoch 910, training loss: 0.06901027262210846 = 0.0036049557384103537 + 0.01 * 6.540531635284424
Epoch 910, val loss: 1.1577858924865723
Epoch 920, training loss: 0.06907255202531815 = 0.0035204021260142326 + 0.01 * 6.555215358734131
Epoch 920, val loss: 1.1617259979248047
Epoch 930, training loss: 0.06905902177095413 = 0.0034399856813251972 + 0.01 * 6.561903953552246
Epoch 930, val loss: 1.1655364036560059
Epoch 940, training loss: 0.06877733021974564 = 0.0033629576209932566 + 0.01 * 6.541437149047852
Epoch 940, val loss: 1.1692347526550293
Epoch 950, training loss: 0.0689978301525116 = 0.0032888236455619335 + 0.01 * 6.570900917053223
Epoch 950, val loss: 1.1729506254196167
Epoch 960, training loss: 0.06859458237886429 = 0.0032181309070438147 + 0.01 * 6.53764533996582
Epoch 960, val loss: 1.1765213012695312
Epoch 970, training loss: 0.0683678388595581 = 0.0031496866140514612 + 0.01 * 6.521815299987793
Epoch 970, val loss: 1.1801457405090332
Epoch 980, training loss: 0.06854386627674103 = 0.0030840898398309946 + 0.01 * 6.545977592468262
Epoch 980, val loss: 1.1836681365966797
Epoch 990, training loss: 0.06799425184726715 = 0.003021319629624486 + 0.01 * 6.497293472290039
Epoch 990, val loss: 1.1871026754379272
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.5830
Flip ASR: 0.5511/225 nodes
The final ASR:0.67036, 0.07928, Accuracy:0.83210, 0.00761
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11592])
remove edge: torch.Size([2, 9546])
updated graph: torch.Size([2, 10582])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.98278, 0.00460, Accuracy:0.83457, 0.00175
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0.3 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0343050956726074 = 1.9505666494369507 + 0.01 * 8.373849868774414
Epoch 0, val loss: 1.948441982269287
Epoch 10, training loss: 2.024747610092163 = 1.9410101175308228 + 0.01 * 8.373757362365723
Epoch 10, val loss: 1.939062237739563
Epoch 20, training loss: 2.0129647254943848 = 1.9292298555374146 + 0.01 * 8.373483657836914
Epoch 20, val loss: 1.9274107217788696
Epoch 30, training loss: 1.9965318441390991 = 1.9128042459487915 + 0.01 * 8.372761726379395
Epoch 30, val loss: 1.9113839864730835
Epoch 40, training loss: 1.9720278978347778 = 1.8883322477340698 + 0.01 * 8.369560241699219
Epoch 40, val loss: 1.8881877660751343
Epoch 50, training loss: 1.9364937543869019 = 1.8530161380767822 + 0.01 * 8.347760200500488
Epoch 50, val loss: 1.856580376625061
Epoch 60, training loss: 1.892665982246399 = 1.8103958368301392 + 0.01 * 8.227018356323242
Epoch 60, val loss: 1.822182536125183
Epoch 70, training loss: 1.8499900102615356 = 1.770861268043518 + 0.01 * 7.912871837615967
Epoch 70, val loss: 1.792431116104126
Epoch 80, training loss: 1.8012127876281738 = 1.7252968549728394 + 0.01 * 7.591588497161865
Epoch 80, val loss: 1.7512195110321045
Epoch 90, training loss: 1.7351499795913696 = 1.661817193031311 + 0.01 * 7.33328104019165
Epoch 90, val loss: 1.6937474012374878
Epoch 100, training loss: 1.6493685245513916 = 1.5775142908096313 + 0.01 * 7.185429573059082
Epoch 100, val loss: 1.6231374740600586
Epoch 110, training loss: 1.550703525543213 = 1.4797940254211426 + 0.01 * 7.090954780578613
Epoch 110, val loss: 1.5436069965362549
Epoch 120, training loss: 1.452936053276062 = 1.3829033374786377 + 0.01 * 7.00327730178833
Epoch 120, val loss: 1.4650485515594482
Epoch 130, training loss: 1.3598260879516602 = 1.2904081344604492 + 0.01 * 6.941797256469727
Epoch 130, val loss: 1.3913607597351074
Epoch 140, training loss: 1.2706197500228882 = 1.2016233205795288 + 0.01 * 6.8996405601501465
Epoch 140, val loss: 1.321386456489563
Epoch 150, training loss: 1.1860992908477783 = 1.1174448728561401 + 0.01 * 6.865444183349609
Epoch 150, val loss: 1.2560679912567139
Epoch 160, training loss: 1.1094181537628174 = 1.0410255193710327 + 0.01 * 6.839262008666992
Epoch 160, val loss: 1.1989006996154785
Epoch 170, training loss: 1.0421638488769531 = 0.9739205241203308 + 0.01 * 6.824326515197754
Epoch 170, val loss: 1.1515134572982788
Epoch 180, training loss: 0.9819831252098083 = 0.9138329029083252 + 0.01 * 6.815023422241211
Epoch 180, val loss: 1.1112140417099
Epoch 190, training loss: 0.9244503974914551 = 0.8563790321350098 + 0.01 * 6.807136535644531
Epoch 190, val loss: 1.0741279125213623
Epoch 200, training loss: 0.8659293055534363 = 0.7979283928871155 + 0.01 * 6.800093173980713
Epoch 200, val loss: 1.0368750095367432
Epoch 210, training loss: 0.8053472638130188 = 0.7374074459075928 + 0.01 * 6.793981552124023
Epoch 210, val loss: 0.9991468787193298
Epoch 220, training loss: 0.7438716292381287 = 0.6759949922561646 + 0.01 * 6.787664413452148
Epoch 220, val loss: 0.9623087644577026
Epoch 230, training loss: 0.6840729117393494 = 0.6162644028663635 + 0.01 * 6.780849456787109
Epoch 230, val loss: 0.9286152720451355
Epoch 240, training loss: 0.6287358403205872 = 0.5609486699104309 + 0.01 * 6.778715133666992
Epoch 240, val loss: 0.9004606604576111
Epoch 250, training loss: 0.5792907476425171 = 0.5115952491760254 + 0.01 * 6.7695488929748535
Epoch 250, val loss: 0.879108726978302
Epoch 260, training loss: 0.5356616973876953 = 0.46805399656295776 + 0.01 * 6.760772705078125
Epoch 260, val loss: 0.864509642124176
Epoch 270, training loss: 0.4963652491569519 = 0.42883533239364624 + 0.01 * 6.752993106842041
Epoch 270, val loss: 0.8547868132591248
Epoch 280, training loss: 0.4594781696796417 = 0.3920184075832367 + 0.01 * 6.745976448059082
Epoch 280, val loss: 0.8480491638183594
Epoch 290, training loss: 0.4235575795173645 = 0.35615548491477966 + 0.01 * 6.740211009979248
Epoch 290, val loss: 0.843267560005188
Epoch 300, training loss: 0.38815152645111084 = 0.32079172134399414 + 0.01 * 6.735982418060303
Epoch 300, val loss: 0.8401457667350769
Epoch 310, training loss: 0.35360971093177795 = 0.28625792264938354 + 0.01 * 6.735178470611572
Epoch 310, val loss: 0.8390225172042847
Epoch 320, training loss: 0.32045942544937134 = 0.25313800573349 + 0.01 * 6.732143878936768
Epoch 320, val loss: 0.8404399156570435
Epoch 330, training loss: 0.2894372045993805 = 0.22212718427181244 + 0.01 * 6.731002330780029
Epoch 330, val loss: 0.8447318077087402
Epoch 340, training loss: 0.2613045573234558 = 0.19400238990783691 + 0.01 * 6.730215072631836
Epoch 340, val loss: 0.8521432280540466
Epoch 350, training loss: 0.2366001307964325 = 0.16930131614208221 + 0.01 * 6.729882717132568
Epoch 350, val loss: 0.862704336643219
Epoch 360, training loss: 0.21534031629562378 = 0.1480567753314972 + 0.01 * 6.728353500366211
Epoch 360, val loss: 0.8757423162460327
Epoch 370, training loss: 0.19718724489212036 = 0.12991173565387726 + 0.01 * 6.727550983428955
Epoch 370, val loss: 0.8907579779624939
Epoch 380, training loss: 0.18164929747581482 = 0.11438695341348648 + 0.01 * 6.726234436035156
Epoch 380, val loss: 0.9072462320327759
Epoch 390, training loss: 0.1682935655117035 = 0.1010458692908287 + 0.01 * 6.724769592285156
Epoch 390, val loss: 0.924735426902771
Epoch 400, training loss: 0.1567271649837494 = 0.08949510008096695 + 0.01 * 6.723206520080566
Epoch 400, val loss: 0.9425272345542908
Epoch 410, training loss: 0.14666783809661865 = 0.07945174723863602 + 0.01 * 6.721608638763428
Epoch 410, val loss: 0.9603091478347778
Epoch 420, training loss: 0.13790102303028107 = 0.0707017257809639 + 0.01 * 6.7199296951293945
Epoch 420, val loss: 0.9777560830116272
Epoch 430, training loss: 0.1302594095468521 = 0.0630723237991333 + 0.01 * 6.718708515167236
Epoch 430, val loss: 0.9946727752685547
Epoch 440, training loss: 0.12358374148607254 = 0.056419290602207184 + 0.01 * 6.716444969177246
Epoch 440, val loss: 1.0109360218048096
Epoch 450, training loss: 0.11775802075862885 = 0.050618913024663925 + 0.01 * 6.713911056518555
Epoch 450, val loss: 1.026552677154541
Epoch 460, training loss: 0.11269141733646393 = 0.04555635526776314 + 0.01 * 6.71350622177124
Epoch 460, val loss: 1.041452407836914
Epoch 470, training loss: 0.10823868215084076 = 0.041134312748909 + 0.01 * 6.710437297821045
Epoch 470, val loss: 1.0557749271392822
Epoch 480, training loss: 0.10434290766716003 = 0.037273336201906204 + 0.01 * 6.706957817077637
Epoch 480, val loss: 1.0694785118103027
Epoch 490, training loss: 0.1009308323264122 = 0.03388747572898865 + 0.01 * 6.704336166381836
Epoch 490, val loss: 1.0826503038406372
Epoch 500, training loss: 0.09790025651454926 = 0.030910173431038857 + 0.01 * 6.699008464813232
Epoch 500, val loss: 1.0953621864318848
Epoch 510, training loss: 0.09523424506187439 = 0.028288088738918304 + 0.01 * 6.694615840911865
Epoch 510, val loss: 1.107591152191162
Epoch 520, training loss: 0.09288116544485092 = 0.02597012370824814 + 0.01 * 6.691104412078857
Epoch 520, val loss: 1.1194018125534058
Epoch 530, training loss: 0.09077762067317963 = 0.02391408383846283 + 0.01 * 6.686354160308838
Epoch 530, val loss: 1.1308354139328003
Epoch 540, training loss: 0.08890629559755325 = 0.022084826603531837 + 0.01 * 6.68214750289917
Epoch 540, val loss: 1.1418852806091309
Epoch 550, training loss: 0.08729395270347595 = 0.020452186465263367 + 0.01 * 6.684176921844482
Epoch 550, val loss: 1.152574062347412
Epoch 560, training loss: 0.0857711136341095 = 0.01899126172065735 + 0.01 * 6.677985191345215
Epoch 560, val loss: 1.1628741025924683
Epoch 570, training loss: 0.08443310111761093 = 0.01767963357269764 + 0.01 * 6.675346851348877
Epoch 570, val loss: 1.172805905342102
Epoch 580, training loss: 0.08314935863018036 = 0.016498466953635216 + 0.01 * 6.665089130401611
Epoch 580, val loss: 1.182489275932312
Epoch 590, training loss: 0.08206191658973694 = 0.015432218089699745 + 0.01 * 6.662970066070557
Epoch 590, val loss: 1.1918400526046753
Epoch 600, training loss: 0.0810425728559494 = 0.014467183500528336 + 0.01 * 6.657539367675781
Epoch 600, val loss: 1.2009202241897583
Epoch 610, training loss: 0.08015209436416626 = 0.013591420836746693 + 0.01 * 6.65606689453125
Epoch 610, val loss: 1.209730863571167
Epoch 620, training loss: 0.07928948104381561 = 0.012794572860002518 + 0.01 * 6.649490833282471
Epoch 620, val loss: 1.2182443141937256
Epoch 630, training loss: 0.07895074784755707 = 0.012067532166838646 + 0.01 * 6.688321590423584
Epoch 630, val loss: 1.2265254259109497
Epoch 640, training loss: 0.07796666771173477 = 0.011403416283428669 + 0.01 * 6.656324863433838
Epoch 640, val loss: 1.2345821857452393
Epoch 650, training loss: 0.07724706083536148 = 0.010795282199978828 + 0.01 * 6.645178318023682
Epoch 650, val loss: 1.2423655986785889
Epoch 660, training loss: 0.07661732286214828 = 0.010236616246402264 + 0.01 * 6.638071060180664
Epoch 660, val loss: 1.2500011920928955
Epoch 670, training loss: 0.07607328146696091 = 0.009722293354570866 + 0.01 * 6.635099411010742
Epoch 670, val loss: 1.2574210166931152
Epoch 680, training loss: 0.07568490505218506 = 0.009247577749192715 + 0.01 * 6.643733024597168
Epoch 680, val loss: 1.2646291255950928
Epoch 690, training loss: 0.075222447514534 = 0.008809405378997326 + 0.01 * 6.6413044929504395
Epoch 690, val loss: 1.2716169357299805
Epoch 700, training loss: 0.07469520717859268 = 0.008403671905398369 + 0.01 * 6.629153251647949
Epoch 700, val loss: 1.2784343957901
Epoch 710, training loss: 0.07429543137550354 = 0.008027557283639908 + 0.01 * 6.626787185668945
Epoch 710, val loss: 1.2850831747055054
Epoch 720, training loss: 0.07394008338451385 = 0.007677650079131126 + 0.01 * 6.626243591308594
Epoch 720, val loss: 1.2915751934051514
Epoch 730, training loss: 0.07363339513540268 = 0.007352128624916077 + 0.01 * 6.628127098083496
Epoch 730, val loss: 1.2978922128677368
Epoch 740, training loss: 0.07325273007154465 = 0.007048611529171467 + 0.01 * 6.620411396026611
Epoch 740, val loss: 1.3040432929992676
Epoch 750, training loss: 0.07298120111227036 = 0.006764950696378946 + 0.01 * 6.6216254234313965
Epoch 750, val loss: 1.310051679611206
Epoch 760, training loss: 0.0726412907242775 = 0.0064999861642718315 + 0.01 * 6.614130973815918
Epoch 760, val loss: 1.3159008026123047
Epoch 770, training loss: 0.07239334285259247 = 0.006251547485589981 + 0.01 * 6.6141791343688965
Epoch 770, val loss: 1.3215914964675903
Epoch 780, training loss: 0.07218652218580246 = 0.0060188183560967445 + 0.01 * 6.6167707443237305
Epoch 780, val loss: 1.3271633386611938
Epoch 790, training loss: 0.07189641892910004 = 0.005800202488899231 + 0.01 * 6.609622001647949
Epoch 790, val loss: 1.332620620727539
Epoch 800, training loss: 0.07164590060710907 = 0.005594345740973949 + 0.01 * 6.605155944824219
Epoch 800, val loss: 1.3379398584365845
Epoch 810, training loss: 0.07143647968769073 = 0.0053999279625713825 + 0.01 * 6.6036553382873535
Epoch 810, val loss: 1.3431512117385864
Epoch 820, training loss: 0.07127251476049423 = 0.0052157072350382805 + 0.01 * 6.605681419372559
Epoch 820, val loss: 1.348237156867981
Epoch 830, training loss: 0.07103434950113297 = 0.005040646996349096 + 0.01 * 6.59937047958374
Epoch 830, val loss: 1.3532660007476807
Epoch 840, training loss: 0.07091591507196426 = 0.004873848520219326 + 0.01 * 6.604206562042236
Epoch 840, val loss: 1.3581300973892212
Epoch 850, training loss: 0.07066129148006439 = 0.004714334383606911 + 0.01 * 6.594696044921875
Epoch 850, val loss: 1.362964153289795
Epoch 860, training loss: 0.07053471356630325 = 0.004561687353998423 + 0.01 * 6.5973029136657715
Epoch 860, val loss: 1.3676996231079102
Epoch 870, training loss: 0.07029926031827927 = 0.004415410105139017 + 0.01 * 6.588385105133057
Epoch 870, val loss: 1.3722660541534424
Epoch 880, training loss: 0.0705333948135376 = 0.004275460261851549 + 0.01 * 6.62579345703125
Epoch 880, val loss: 1.3767579793930054
Epoch 890, training loss: 0.06997592002153397 = 0.004141637124121189 + 0.01 * 6.583428382873535
Epoch 890, val loss: 1.3811136484146118
Epoch 900, training loss: 0.06986343115568161 = 0.00401384336873889 + 0.01 * 6.584958553314209
Epoch 900, val loss: 1.3854787349700928
Epoch 910, training loss: 0.06979868561029434 = 0.0038921195082366467 + 0.01 * 6.5906572341918945
Epoch 910, val loss: 1.3897278308868408
Epoch 920, training loss: 0.0695824921131134 = 0.0037760057020932436 + 0.01 * 6.580648899078369
Epoch 920, val loss: 1.3938511610031128
Epoch 930, training loss: 0.06953113526105881 = 0.0036652677226811647 + 0.01 * 6.586586952209473
Epoch 930, val loss: 1.397927165031433
Epoch 940, training loss: 0.06928697228431702 = 0.0035598708782345057 + 0.01 * 6.572710037231445
Epoch 940, val loss: 1.4018580913543701
Epoch 950, training loss: 0.0691928043961525 = 0.0034593765158206224 + 0.01 * 6.573342323303223
Epoch 950, val loss: 1.4057722091674805
Epoch 960, training loss: 0.06917546689510345 = 0.003363480791449547 + 0.01 * 6.581198215484619
Epoch 960, val loss: 1.4095600843429565
Epoch 970, training loss: 0.06893839687108994 = 0.003272273112088442 + 0.01 * 6.566612243652344
Epoch 970, val loss: 1.413259506225586
Epoch 980, training loss: 0.06881760060787201 = 0.0031853686086833477 + 0.01 * 6.563223361968994
Epoch 980, val loss: 1.4169245958328247
Epoch 990, training loss: 0.06899787485599518 = 0.003102742601186037 + 0.01 * 6.589513778686523
Epoch 990, val loss: 1.4205033779144287
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.5461
Flip ASR: 0.4667/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.031432867050171 = 1.947694182395935 + 0.01 * 8.373869895935059
Epoch 0, val loss: 1.9425909519195557
Epoch 10, training loss: 2.0218300819396973 = 1.9380919933319092 + 0.01 * 8.37381649017334
Epoch 10, val loss: 1.9327096939086914
Epoch 20, training loss: 2.010417938232422 = 1.9266819953918457 + 0.01 * 8.37360668182373
Epoch 20, val loss: 1.9208475351333618
Epoch 30, training loss: 1.9947080612182617 = 1.9109770059585571 + 0.01 * 8.3731107711792
Epoch 30, val loss: 1.9045984745025635
Epoch 40, training loss: 1.9716917276382446 = 1.8879762887954712 + 0.01 * 8.371540069580078
Epoch 40, val loss: 1.8809481859207153
Epoch 50, training loss: 1.9380327463150024 = 1.8544092178344727 + 0.01 * 8.362354278564453
Epoch 50, val loss: 1.8472890853881836
Epoch 60, training loss: 1.8930848836898804 = 1.810133457183838 + 0.01 * 8.295140266418457
Epoch 60, val loss: 1.8050074577331543
Epoch 70, training loss: 1.8403472900390625 = 1.7613279819488525 + 0.01 * 7.901928901672363
Epoch 70, val loss: 1.7607965469360352
Epoch 80, training loss: 1.7811020612716675 = 1.7070634365081787 + 0.01 * 7.403857707977295
Epoch 80, val loss: 1.7108638286590576
Epoch 90, training loss: 1.7056477069854736 = 1.6337307691574097 + 0.01 * 7.191699981689453
Epoch 90, val loss: 1.644453763961792
Epoch 100, training loss: 1.6079951524734497 = 1.5365407466888428 + 0.01 * 7.145443916320801
Epoch 100, val loss: 1.5609763860702515
Epoch 110, training loss: 1.4903349876403809 = 1.419260025024414 + 0.01 * 7.107499122619629
Epoch 110, val loss: 1.4635478258132935
Epoch 120, training loss: 1.3632616996765137 = 1.2926422357559204 + 0.01 * 7.0619425773620605
Epoch 120, val loss: 1.3606728315353394
Epoch 130, training loss: 1.235982060432434 = 1.1657832860946655 + 0.01 * 7.0198774337768555
Epoch 130, val loss: 1.2612625360488892
Epoch 140, training loss: 1.1149976253509521 = 1.0450444221496582 + 0.01 * 6.995316028594971
Epoch 140, val loss: 1.169325828552246
Epoch 150, training loss: 1.00509774684906 = 0.9352900981903076 + 0.01 * 6.980764389038086
Epoch 150, val loss: 1.0884990692138672
Epoch 160, training loss: 0.9090238809585571 = 0.8393688201904297 + 0.01 * 6.965508937835693
Epoch 160, val loss: 1.0199497938156128
Epoch 170, training loss: 0.8270748257637024 = 0.7576006650924683 + 0.01 * 6.947417259216309
Epoch 170, val loss: 0.9634937644004822
Epoch 180, training loss: 0.7576663494110107 = 0.6883757710456848 + 0.01 * 6.929060935974121
Epoch 180, val loss: 0.9180470108985901
Epoch 190, training loss: 0.6981562376022339 = 0.6290426254272461 + 0.01 * 6.9113593101501465
Epoch 190, val loss: 0.8823351860046387
Epoch 200, training loss: 0.645943820476532 = 0.5769756436347961 + 0.01 * 6.8968186378479
Epoch 200, val loss: 0.8545395135879517
Epoch 210, training loss: 0.5991247296333313 = 0.5302561521530151 + 0.01 * 6.886857509613037
Epoch 210, val loss: 0.8326911926269531
Epoch 220, training loss: 0.5564447641372681 = 0.4876532256603241 + 0.01 * 6.879154205322266
Epoch 220, val loss: 0.8157175183296204
Epoch 230, training loss: 0.5172619223594666 = 0.4485291838645935 + 0.01 * 6.873272895812988
Epoch 230, val loss: 0.8030140399932861
Epoch 240, training loss: 0.4811449944972992 = 0.4124607443809509 + 0.01 * 6.868425369262695
Epoch 240, val loss: 0.7939580082893372
Epoch 250, training loss: 0.447626531124115 = 0.3789765238761902 + 0.01 * 6.864999771118164
Epoch 250, val loss: 0.7882232666015625
Epoch 260, training loss: 0.41600263118743896 = 0.3474055528640747 + 0.01 * 6.859707832336426
Epoch 260, val loss: 0.7851520776748657
Epoch 270, training loss: 0.3856148421764374 = 0.3170633912086487 + 0.01 * 6.8551459312438965
Epoch 270, val loss: 0.783935010433197
Epoch 280, training loss: 0.3560181260108948 = 0.2875175178050995 + 0.01 * 6.850061416625977
Epoch 280, val loss: 0.7840050458908081
Epoch 290, training loss: 0.3272750973701477 = 0.2588287591934204 + 0.01 * 6.844633102416992
Epoch 290, val loss: 0.7853835225105286
Epoch 300, training loss: 0.29984050989151 = 0.23144899308681488 + 0.01 * 6.839150428771973
Epoch 300, val loss: 0.7881982326507568
Epoch 310, training loss: 0.2743498980998993 = 0.20602953433990479 + 0.01 * 6.832035541534424
Epoch 310, val loss: 0.7927770614624023
Epoch 320, training loss: 0.25131434202194214 = 0.18299336731433868 + 0.01 * 6.832098484039307
Epoch 320, val loss: 0.7993987202644348
Epoch 330, training loss: 0.2306964099407196 = 0.1625082939863205 + 0.01 * 6.818812847137451
Epoch 330, val loss: 0.8078956604003906
Epoch 340, training loss: 0.21236325800418854 = 0.14426586031913757 + 0.01 * 6.80974006652832
Epoch 340, val loss: 0.8180343508720398
Epoch 350, training loss: 0.19610224664211273 = 0.12803010642528534 + 0.01 * 6.807214260101318
Epoch 350, val loss: 0.8293144106864929
Epoch 360, training loss: 0.181587353348732 = 0.11361374706029892 + 0.01 * 6.797360897064209
Epoch 360, val loss: 0.8416085839271545
Epoch 370, training loss: 0.16854619979858398 = 0.10071447491645813 + 0.01 * 6.783173084259033
Epoch 370, val loss: 0.85465008020401
Epoch 380, training loss: 0.15692462027072906 = 0.08914944529533386 + 0.01 * 6.777517318725586
Epoch 380, val loss: 0.8682016730308533
Epoch 390, training loss: 0.1469445526599884 = 0.07924069464206696 + 0.01 * 6.7703857421875
Epoch 390, val loss: 0.8828080892562866
Epoch 400, training loss: 0.13854816555976868 = 0.07087596505880356 + 0.01 * 6.767220973968506
Epoch 400, val loss: 0.8987387418746948
Epoch 410, training loss: 0.1312616467475891 = 0.0636526495218277 + 0.01 * 6.760900974273682
Epoch 410, val loss: 0.9152739644050598
Epoch 420, training loss: 0.12490225583314896 = 0.05736052244901657 + 0.01 * 6.754173755645752
Epoch 420, val loss: 0.931831419467926
Epoch 430, training loss: 0.11932110786437988 = 0.051858510822057724 + 0.01 * 6.746259689331055
Epoch 430, val loss: 0.9478475451469421
Epoch 440, training loss: 0.1144474595785141 = 0.04701577126979828 + 0.01 * 6.743168830871582
Epoch 440, val loss: 0.9636378288269043
Epoch 450, training loss: 0.11007475852966309 = 0.04273047670722008 + 0.01 * 6.7344279289245605
Epoch 450, val loss: 0.979019820690155
Epoch 460, training loss: 0.10617496818304062 = 0.03893643617630005 + 0.01 * 6.72385311126709
Epoch 460, val loss: 0.9942263960838318
Epoch 470, training loss: 0.10274768620729446 = 0.03557095676660538 + 0.01 * 6.717673301696777
Epoch 470, val loss: 1.009028673171997
Epoch 480, training loss: 0.09971010684967041 = 0.0325782410800457 + 0.01 * 6.713186264038086
Epoch 480, val loss: 1.0236008167266846
Epoch 490, training loss: 0.09706752002239227 = 0.02990877814590931 + 0.01 * 6.715874671936035
Epoch 490, val loss: 1.0377674102783203
Epoch 500, training loss: 0.09457419812679291 = 0.027522673830389977 + 0.01 * 6.7051520347595215
Epoch 500, val loss: 1.051631212234497
Epoch 510, training loss: 0.09239989519119263 = 0.025386735796928406 + 0.01 * 6.701315879821777
Epoch 510, val loss: 1.065239667892456
Epoch 520, training loss: 0.0904509648680687 = 0.023468950763344765 + 0.01 * 6.698201656341553
Epoch 520, val loss: 1.0784410238265991
Epoch 530, training loss: 0.0886184424161911 = 0.02174411341547966 + 0.01 * 6.687433242797852
Epoch 530, val loss: 1.0912978649139404
Epoch 540, training loss: 0.08711886405944824 = 0.020190339535474777 + 0.01 * 6.692852020263672
Epoch 540, val loss: 1.1038997173309326
Epoch 550, training loss: 0.08566547930240631 = 0.01879020594060421 + 0.01 * 6.687527656555176
Epoch 550, val loss: 1.1160743236541748
Epoch 560, training loss: 0.08430039882659912 = 0.01752559468150139 + 0.01 * 6.677480697631836
Epoch 560, val loss: 1.1279094219207764
Epoch 570, training loss: 0.08305764198303223 = 0.01638045720756054 + 0.01 * 6.667718410491943
Epoch 570, val loss: 1.1394805908203125
Epoch 580, training loss: 0.08198239654302597 = 0.015342125669121742 + 0.01 * 6.664027690887451
Epoch 580, val loss: 1.150725245475769
Epoch 590, training loss: 0.08095650374889374 = 0.014397880993783474 + 0.01 * 6.655862331390381
Epoch 590, val loss: 1.161585807800293
Epoch 600, training loss: 0.08018636703491211 = 0.01353797223418951 + 0.01 * 6.664839267730713
Epoch 600, val loss: 1.172244668006897
Epoch 610, training loss: 0.07922576367855072 = 0.012753303162753582 + 0.01 * 6.647246837615967
Epoch 610, val loss: 1.182475209236145
Epoch 620, training loss: 0.07847250998020172 = 0.012035513296723366 + 0.01 * 6.643699645996094
Epoch 620, val loss: 1.1924322843551636
Epoch 630, training loss: 0.07781330496072769 = 0.011377139948308468 + 0.01 * 6.643616199493408
Epoch 630, val loss: 1.2021355628967285
Epoch 640, training loss: 0.0770971029996872 = 0.010772722773253918 + 0.01 * 6.6324381828308105
Epoch 640, val loss: 1.211517095565796
Epoch 650, training loss: 0.07640838623046875 = 0.010216783732175827 + 0.01 * 6.619159698486328
Epoch 650, val loss: 1.220720648765564
Epoch 660, training loss: 0.07594411820173264 = 0.009703700430691242 + 0.01 * 6.624041557312012
Epoch 660, val loss: 1.2296242713928223
Epoch 670, training loss: 0.07551181316375732 = 0.009230266325175762 + 0.01 * 6.62815523147583
Epoch 670, val loss: 1.2382640838623047
Epoch 680, training loss: 0.07490615546703339 = 0.008792821317911148 + 0.01 * 6.61133337020874
Epoch 680, val loss: 1.2466663122177124
Epoch 690, training loss: 0.07444749027490616 = 0.008387306705117226 + 0.01 * 6.606018543243408
Epoch 690, val loss: 1.2547625303268433
Epoch 700, training loss: 0.07404319196939468 = 0.008010609075427055 + 0.01 * 6.6032586097717285
Epoch 700, val loss: 1.2627146244049072
Epoch 710, training loss: 0.07362876832485199 = 0.007660259492695332 + 0.01 * 6.596850395202637
Epoch 710, val loss: 1.270377278327942
Epoch 720, training loss: 0.07333731651306152 = 0.0073342532850801945 + 0.01 * 6.600306510925293
Epoch 720, val loss: 1.27787446975708
Epoch 730, training loss: 0.07298889011144638 = 0.0070304470136761665 + 0.01 * 6.595844268798828
Epoch 730, val loss: 1.2852091789245605
Epoch 740, training loss: 0.07260988652706146 = 0.006746362894773483 + 0.01 * 6.5863518714904785
Epoch 740, val loss: 1.2922221422195435
Epoch 750, training loss: 0.07243917137384415 = 0.0064801922999322414 + 0.01 * 6.595897674560547
Epoch 750, val loss: 1.2991496324539185
Epoch 760, training loss: 0.07201410084962845 = 0.006230891682207584 + 0.01 * 6.5783209800720215
Epoch 760, val loss: 1.3059223890304565
Epoch 770, training loss: 0.07187218964099884 = 0.005996799096465111 + 0.01 * 6.587538719177246
Epoch 770, val loss: 1.312514305114746
Epoch 780, training loss: 0.07154130935668945 = 0.0057768928818404675 + 0.01 * 6.576441764831543
Epoch 780, val loss: 1.3189305067062378
Epoch 790, training loss: 0.07122219353914261 = 0.005570230074226856 + 0.01 * 6.565196514129639
Epoch 790, val loss: 1.3252015113830566
Epoch 800, training loss: 0.07113765180110931 = 0.005375581327825785 + 0.01 * 6.576207637786865
Epoch 800, val loss: 1.331273078918457
Epoch 810, training loss: 0.07086450606584549 = 0.005192367825657129 + 0.01 * 6.567213535308838
Epoch 810, val loss: 1.3373072147369385
Epoch 820, training loss: 0.0707128718495369 = 0.005019448231905699 + 0.01 * 6.569342136383057
Epoch 820, val loss: 1.3430622816085815
Epoch 830, training loss: 0.07053215801715851 = 0.004856365732848644 + 0.01 * 6.567579746246338
Epoch 830, val loss: 1.348785400390625
Epoch 840, training loss: 0.07024821639060974 = 0.00470189331099391 + 0.01 * 6.554633140563965
Epoch 840, val loss: 1.35438871383667
Epoch 850, training loss: 0.07014478743076324 = 0.00455568777397275 + 0.01 * 6.558909893035889
Epoch 850, val loss: 1.3597090244293213
Epoch 860, training loss: 0.06992852687835693 = 0.004417501389980316 + 0.01 * 6.551102638244629
Epoch 860, val loss: 1.365078330039978
Epoch 870, training loss: 0.06967449933290482 = 0.00428635161370039 + 0.01 * 6.538815498352051
Epoch 870, val loss: 1.370289921760559
Epoch 880, training loss: 0.06955942511558533 = 0.004161893390119076 + 0.01 * 6.539753437042236
Epoch 880, val loss: 1.3753160238265991
Epoch 890, training loss: 0.0694868266582489 = 0.004043808672577143 + 0.01 * 6.544301509857178
Epoch 890, val loss: 1.3803212642669678
Epoch 900, training loss: 0.0692271888256073 = 0.00393154751509428 + 0.01 * 6.529564380645752
Epoch 900, val loss: 1.385151982307434
Epoch 910, training loss: 0.06927062571048737 = 0.0038247781340032816 + 0.01 * 6.544585227966309
Epoch 910, val loss: 1.3898893594741821
Epoch 920, training loss: 0.06912417709827423 = 0.0037230069283396006 + 0.01 * 6.540116786956787
Epoch 920, val loss: 1.3944799900054932
Epoch 930, training loss: 0.06883959472179413 = 0.003626073244959116 + 0.01 * 6.521352291107178
Epoch 930, val loss: 1.3989741802215576
Epoch 940, training loss: 0.06878169625997543 = 0.003533529583364725 + 0.01 * 6.524816989898682
Epoch 940, val loss: 1.4033573865890503
Epoch 950, training loss: 0.06858137249946594 = 0.0034453459084033966 + 0.01 * 6.5136027336120605
Epoch 950, val loss: 1.4077448844909668
Epoch 960, training loss: 0.068417027592659 = 0.0033611140679568052 + 0.01 * 6.50559139251709
Epoch 960, val loss: 1.412009835243225
Epoch 970, training loss: 0.06880516558885574 = 0.0032804508227854967 + 0.01 * 6.552471160888672
Epoch 970, val loss: 1.4160321950912476
Epoch 980, training loss: 0.06833591312170029 = 0.003203495405614376 + 0.01 * 6.513241767883301
Epoch 980, val loss: 1.4202098846435547
Epoch 990, training loss: 0.06827165186405182 = 0.003129960037767887 + 0.01 * 6.514169692993164
Epoch 990, val loss: 1.4241501092910767
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.6531
Flip ASR: 0.6044/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0227110385894775 = 1.9389727115631104 + 0.01 * 8.373833656311035
Epoch 0, val loss: 1.9396569728851318
Epoch 10, training loss: 2.0128445625305176 = 1.9291070699691772 + 0.01 * 8.373748779296875
Epoch 10, val loss: 1.9298062324523926
Epoch 20, training loss: 2.000825881958008 = 1.9170912504196167 + 0.01 * 8.373455047607422
Epoch 20, val loss: 1.9173028469085693
Epoch 30, training loss: 1.9840983152389526 = 1.9003725051879883 + 0.01 * 8.372578620910645
Epoch 30, val loss: 1.8994985818862915
Epoch 40, training loss: 1.9596291780471802 = 1.8759446144104004 + 0.01 * 8.368453025817871
Epoch 40, val loss: 1.8733333349227905
Epoch 50, training loss: 1.9250314235687256 = 1.8416348695755005 + 0.01 * 8.339651107788086
Epoch 50, val loss: 1.837539792060852
Epoch 60, training loss: 1.8819019794464111 = 1.800406813621521 + 0.01 * 8.14951229095459
Epoch 60, val loss: 1.7979539632797241
Epoch 70, training loss: 1.8354805707931519 = 1.7589138746261597 + 0.01 * 7.656668186187744
Epoch 70, val loss: 1.762834906578064
Epoch 80, training loss: 1.7808398008346558 = 1.7085978984832764 + 0.01 * 7.224191665649414
Epoch 80, val loss: 1.7215825319290161
Epoch 90, training loss: 1.710024356842041 = 1.639297604560852 + 0.01 * 7.072678089141846
Epoch 90, val loss: 1.663596272468567
Epoch 100, training loss: 1.6176601648330688 = 1.5473827123641968 + 0.01 * 7.02774715423584
Epoch 100, val loss: 1.585473656654358
Epoch 110, training loss: 1.5076663494110107 = 1.4376899003982544 + 0.01 * 6.997644901275635
Epoch 110, val loss: 1.494821548461914
Epoch 120, training loss: 1.3894795179367065 = 1.319703221321106 + 0.01 * 6.97763204574585
Epoch 120, val loss: 1.399870753288269
Epoch 130, training loss: 1.2713439464569092 = 1.2017048597335815 + 0.01 * 6.963912487030029
Epoch 130, val loss: 1.3072209358215332
Epoch 140, training loss: 1.1594700813293457 = 1.0899677276611328 + 0.01 * 6.95023250579834
Epoch 140, val loss: 1.2218034267425537
Epoch 150, training loss: 1.0583401918411255 = 0.9890085458755493 + 0.01 * 6.933169841766357
Epoch 150, val loss: 1.1462739706039429
Epoch 160, training loss: 0.9687345623970032 = 0.8996550440788269 + 0.01 * 6.907949447631836
Epoch 160, val loss: 1.0809104442596436
Epoch 170, training loss: 0.8895299434661865 = 0.8207967877388 + 0.01 * 6.873314380645752
Epoch 170, val loss: 1.0245357751846313
Epoch 180, training loss: 0.8200327754020691 = 0.7516607046127319 + 0.01 * 6.837205410003662
Epoch 180, val loss: 0.9758852124214172
Epoch 190, training loss: 0.7596768140792847 = 0.6915305256843567 + 0.01 * 6.814627170562744
Epoch 190, val loss: 0.9346356391906738
Epoch 200, training loss: 0.7067795991897583 = 0.6388280987739563 + 0.01 * 6.795149803161621
Epoch 200, val loss: 0.8999098539352417
Epoch 210, training loss: 0.6589454412460327 = 0.5910897254943848 + 0.01 * 6.785572052001953
Epoch 210, val loss: 0.8704020977020264
Epoch 220, training loss: 0.6135614514350891 = 0.5457873344421387 + 0.01 * 6.77741003036499
Epoch 220, val loss: 0.8444166779518127
Epoch 230, training loss: 0.5688667297363281 = 0.5011565685272217 + 0.01 * 6.7710137367248535
Epoch 230, val loss: 0.8211382627487183
Epoch 240, training loss: 0.5241307020187378 = 0.4564865231513977 + 0.01 * 6.764419078826904
Epoch 240, val loss: 0.7999903559684753
Epoch 250, training loss: 0.47976362705230713 = 0.4120846390724182 + 0.01 * 6.767899990081787
Epoch 250, val loss: 0.7809438109397888
Epoch 260, training loss: 0.4363946318626404 = 0.3687880337238312 + 0.01 * 6.760659217834473
Epoch 260, val loss: 0.7643043994903564
Epoch 270, training loss: 0.39474356174468994 = 0.3272039294242859 + 0.01 * 6.753962516784668
Epoch 270, val loss: 0.7498723864555359
Epoch 280, training loss: 0.3550494611263275 = 0.2875313460826874 + 0.01 * 6.75181245803833
Epoch 280, val loss: 0.7373033165931702
Epoch 290, training loss: 0.3174569606781006 = 0.2499741166830063 + 0.01 * 6.7482829093933105
Epoch 290, val loss: 0.7267467379570007
Epoch 300, training loss: 0.28251564502716064 = 0.21507051587104797 + 0.01 * 6.744512557983398
Epoch 300, val loss: 0.718765914440155
Epoch 310, training loss: 0.25098875164985657 = 0.18358832597732544 + 0.01 * 6.740042209625244
Epoch 310, val loss: 0.7137917280197144
Epoch 320, training loss: 0.22354340553283691 = 0.15617555379867554 + 0.01 * 6.736785411834717
Epoch 320, val loss: 0.7120939493179321
Epoch 330, training loss: 0.20033526420593262 = 0.13299350440502167 + 0.01 * 6.734175205230713
Epoch 330, val loss: 0.7139438986778259
Epoch 340, training loss: 0.18102926015853882 = 0.11374007910490036 + 0.01 * 6.728917598724365
Epoch 340, val loss: 0.7189476490020752
Epoch 350, training loss: 0.1651138961315155 = 0.09785086661577225 + 0.01 * 6.726302623748779
Epoch 350, val loss: 0.7263782024383545
Epoch 360, training loss: 0.151933491230011 = 0.08471337705850601 + 0.01 * 6.722011566162109
Epoch 360, val loss: 0.7356104850769043
Epoch 370, training loss: 0.1409965455532074 = 0.07380948960781097 + 0.01 * 6.718705654144287
Epoch 370, val loss: 0.7461051940917969
Epoch 380, training loss: 0.13187330961227417 = 0.064714714884758 + 0.01 * 6.715859413146973
Epoch 380, val loss: 0.7574169039726257
Epoch 390, training loss: 0.12421003729104996 = 0.05708698928356171 + 0.01 * 6.712304592132568
Epoch 390, val loss: 0.7692718505859375
Epoch 400, training loss: 0.11774749308824539 = 0.05064509063959122 + 0.01 * 6.710240364074707
Epoch 400, val loss: 0.7813257575035095
Epoch 410, training loss: 0.11231635510921478 = 0.04517525061964989 + 0.01 * 6.714110374450684
Epoch 410, val loss: 0.7934864163398743
Epoch 420, training loss: 0.10753268003463745 = 0.04050372540950775 + 0.01 * 6.702895641326904
Epoch 420, val loss: 0.805570125579834
Epoch 430, training loss: 0.10348401963710785 = 0.036490026861429214 + 0.01 * 6.699398994445801
Epoch 430, val loss: 0.8174725770950317
Epoch 440, training loss: 0.10002262890338898 = 0.033021871000528336 + 0.01 * 6.700076103210449
Epoch 440, val loss: 0.8292950391769409
Epoch 450, training loss: 0.09694002568721771 = 0.030008994042873383 + 0.01 * 6.693103313446045
Epoch 450, val loss: 0.8408596515655518
Epoch 460, training loss: 0.09431588649749756 = 0.027379779145121574 + 0.01 * 6.693610668182373
Epoch 460, val loss: 0.8522287011146545
Epoch 470, training loss: 0.09196683764457703 = 0.025077026337385178 + 0.01 * 6.688980579376221
Epoch 470, val loss: 0.8633096218109131
Epoch 480, training loss: 0.0898592621088028 = 0.023050444200634956 + 0.01 * 6.680881977081299
Epoch 480, val loss: 0.8741330504417419
Epoch 490, training loss: 0.08812473714351654 = 0.021259095519781113 + 0.01 * 6.6865644454956055
Epoch 490, val loss: 0.8846691846847534
Epoch 500, training loss: 0.08635757118463516 = 0.019670026376843452 + 0.01 * 6.668754577636719
Epoch 500, val loss: 0.8948799967765808
Epoch 510, training loss: 0.08494138717651367 = 0.018254917114973068 + 0.01 * 6.668647289276123
Epoch 510, val loss: 0.9047714471817017
Epoch 520, training loss: 0.08358635008335114 = 0.01698940433561802 + 0.01 * 6.659694671630859
Epoch 520, val loss: 0.9144424200057983
Epoch 530, training loss: 0.08259982615709305 = 0.015854964032769203 + 0.01 * 6.6744866371154785
Epoch 530, val loss: 0.9237883687019348
Epoch 540, training loss: 0.0813770443201065 = 0.014835266396403313 + 0.01 * 6.654177665710449
Epoch 540, val loss: 0.9328835606575012
Epoch 550, training loss: 0.08034402132034302 = 0.013914681039750576 + 0.01 * 6.642934322357178
Epoch 550, val loss: 0.9417112469673157
Epoch 560, training loss: 0.0797601193189621 = 0.013081427663564682 + 0.01 * 6.667869567871094
Epoch 560, val loss: 0.9503338932991028
Epoch 570, training loss: 0.07864478975534439 = 0.012324884533882141 + 0.01 * 6.631990432739258
Epoch 570, val loss: 0.9586050510406494
Epoch 580, training loss: 0.07791467010974884 = 0.011635609902441502 + 0.01 * 6.627906322479248
Epoch 580, val loss: 0.9666905999183655
Epoch 590, training loss: 0.07720759510993958 = 0.011005192063748837 + 0.01 * 6.620240688323975
Epoch 590, val loss: 0.9745917320251465
Epoch 600, training loss: 0.0765814334154129 = 0.01042746752500534 + 0.01 * 6.615396499633789
Epoch 600, val loss: 0.9823834300041199
Epoch 610, training loss: 0.0760880634188652 = 0.00989722553640604 + 0.01 * 6.619084358215332
Epoch 610, val loss: 0.989840567111969
Epoch 620, training loss: 0.07554095983505249 = 0.009409282356500626 + 0.01 * 6.6131672859191895
Epoch 620, val loss: 0.9972033500671387
Epoch 630, training loss: 0.07494459301233292 = 0.008958936668932438 + 0.01 * 6.598565578460693
Epoch 630, val loss: 1.0043559074401855
Epoch 640, training loss: 0.07482980191707611 = 0.00854229275137186 + 0.01 * 6.628751277923584
Epoch 640, val loss: 1.0112016201019287
Epoch 650, training loss: 0.07411892712116241 = 0.008157468400895596 + 0.01 * 6.5961456298828125
Epoch 650, val loss: 1.0180996656417847
Epoch 660, training loss: 0.07372553646564484 = 0.007800490129739046 + 0.01 * 6.592504978179932
Epoch 660, val loss: 1.0247724056243896
Epoch 670, training loss: 0.07354103028774261 = 0.007468602620065212 + 0.01 * 6.607242584228516
Epoch 670, val loss: 1.031220555305481
Epoch 680, training loss: 0.07297530025243759 = 0.00715939374640584 + 0.01 * 6.58159065246582
Epoch 680, val loss: 1.0375152826309204
Epoch 690, training loss: 0.07275349646806717 = 0.0068709757179021835 + 0.01 * 6.588252067565918
Epoch 690, val loss: 1.0436179637908936
Epoch 700, training loss: 0.07237368822097778 = 0.006601881235837936 + 0.01 * 6.577181339263916
Epoch 700, val loss: 1.04977285861969
Epoch 710, training loss: 0.07205994427204132 = 0.006349770352244377 + 0.01 * 6.571017265319824
Epoch 710, val loss: 1.0556297302246094
Epoch 720, training loss: 0.07181241363286972 = 0.006113457027822733 + 0.01 * 6.569896221160889
Epoch 720, val loss: 1.0613681077957153
Epoch 730, training loss: 0.07160797715187073 = 0.005891919136047363 + 0.01 * 6.571606159210205
Epoch 730, val loss: 1.06712007522583
Epoch 740, training loss: 0.07131955027580261 = 0.005683685187250376 + 0.01 * 6.563587188720703
Epoch 740, val loss: 1.07258141040802
Epoch 750, training loss: 0.0712835043668747 = 0.0054880366660654545 + 0.01 * 6.5795464515686035
Epoch 750, val loss: 1.0780242681503296
Epoch 760, training loss: 0.07079527527093887 = 0.0053034634329378605 + 0.01 * 6.54918098449707
Epoch 760, val loss: 1.0833251476287842
Epoch 770, training loss: 0.07057318836450577 = 0.00512945419177413 + 0.01 * 6.544373512268066
Epoch 770, val loss: 1.0884886980056763
Epoch 780, training loss: 0.07073447108268738 = 0.004965088330209255 + 0.01 * 6.576938629150391
Epoch 780, val loss: 1.0937137603759766
Epoch 790, training loss: 0.0701688900589943 = 0.004809520207345486 + 0.01 * 6.5359368324279785
Epoch 790, val loss: 1.0986402034759521
Epoch 800, training loss: 0.06999970227479935 = 0.0046625626273453236 + 0.01 * 6.5337138175964355
Epoch 800, val loss: 1.103518009185791
Epoch 810, training loss: 0.06995020806789398 = 0.004523519426584244 + 0.01 * 6.542669296264648
Epoch 810, val loss: 1.1083852052688599
Epoch 820, training loss: 0.06962571293115616 = 0.004391780123114586 + 0.01 * 6.523393154144287
Epoch 820, val loss: 1.1129978895187378
Epoch 830, training loss: 0.06955581903457642 = 0.004266571253538132 + 0.01 * 6.528924465179443
Epoch 830, val loss: 1.1175954341888428
Epoch 840, training loss: 0.06933684647083282 = 0.004147591069340706 + 0.01 * 6.518925666809082
Epoch 840, val loss: 1.1221156120300293
Epoch 850, training loss: 0.06928891688585281 = 0.004034555051475763 + 0.01 * 6.5254364013671875
Epoch 850, val loss: 1.1265406608581543
Epoch 860, training loss: 0.0691860094666481 = 0.003926795441657305 + 0.01 * 6.52592134475708
Epoch 860, val loss: 1.1308035850524902
Epoch 870, training loss: 0.06894215941429138 = 0.003824360202997923 + 0.01 * 6.51177978515625
Epoch 870, val loss: 1.1351735591888428
Epoch 880, training loss: 0.06900878250598907 = 0.003726710332557559 + 0.01 * 6.528207302093506
Epoch 880, val loss: 1.1393462419509888
Epoch 890, training loss: 0.06871198117733002 = 0.00363348051905632 + 0.01 * 6.50784969329834
Epoch 890, val loss: 1.1434173583984375
Epoch 900, training loss: 0.06859587877988815 = 0.003544570878148079 + 0.01 * 6.505130767822266
Epoch 900, val loss: 1.1474844217300415
Epoch 910, training loss: 0.06845102459192276 = 0.0034594687167555094 + 0.01 * 6.499155521392822
Epoch 910, val loss: 1.1513665914535522
Epoch 920, training loss: 0.06829243153333664 = 0.0033783474937081337 + 0.01 * 6.491408824920654
Epoch 920, val loss: 1.1552809476852417
Epoch 930, training loss: 0.06831354647874832 = 0.0033007217571139336 + 0.01 * 6.501283168792725
Epoch 930, val loss: 1.1590803861618042
Epoch 940, training loss: 0.06803960353136063 = 0.003226622473448515 + 0.01 * 6.481297969818115
Epoch 940, val loss: 1.1627620458602905
Epoch 950, training loss: 0.06811080873012543 = 0.003155491314828396 + 0.01 * 6.495532035827637
Epoch 950, val loss: 1.1664477586746216
Epoch 960, training loss: 0.06781147420406342 = 0.003087512217462063 + 0.01 * 6.472396373748779
Epoch 960, val loss: 1.1700642108917236
Epoch 970, training loss: 0.06791682541370392 = 0.003022341290488839 + 0.01 * 6.489448070526123
Epoch 970, val loss: 1.1736117601394653
Epoch 980, training loss: 0.06775585561990738 = 0.0029595494270324707 + 0.01 * 6.479630947113037
Epoch 980, val loss: 1.1769791841506958
Epoch 990, training loss: 0.06752046197652817 = 0.0028994653839617968 + 0.01 * 6.462099552154541
Epoch 990, val loss: 1.180383324623108
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.7786
Flip ASR: 0.7422/225 nodes
The final ASR:0.65929, 0.09501, Accuracy:0.81481, 0.00907
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11606])
remove edge: torch.Size([2, 9468])
updated graph: torch.Size([2, 10518])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.98032, 0.00348, Accuracy:0.83580, 0.00349
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0.3 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.032853364944458 = 1.9491158723831177 + 0.01 * 8.373758316040039
Epoch 0, val loss: 1.952987551689148
Epoch 10, training loss: 2.0223286151885986 = 1.9385923147201538 + 0.01 * 8.373629570007324
Epoch 10, val loss: 1.943252682685852
Epoch 20, training loss: 2.009401798248291 = 1.9256696701049805 + 0.01 * 8.373209953308105
Epoch 20, val loss: 1.9310332536697388
Epoch 30, training loss: 1.9913076162338257 = 1.9075891971588135 + 0.01 * 8.37183952331543
Epoch 30, val loss: 1.9137545824050903
Epoch 40, training loss: 1.9646669626235962 = 1.881028413772583 + 0.01 * 8.363852500915527
Epoch 40, val loss: 1.888683557510376
Epoch 50, training loss: 1.9270267486572266 = 1.8439085483551025 + 0.01 * 8.3118257522583
Epoch 50, val loss: 1.8554755449295044
Epoch 60, training loss: 1.8814805746078491 = 1.8014366626739502 + 0.01 * 8.004396438598633
Epoch 60, val loss: 1.8208335638046265
Epoch 70, training loss: 1.8382407426834106 = 1.7613390684127808 + 0.01 * 7.690169334411621
Epoch 70, val loss: 1.7874581813812256
Epoch 80, training loss: 1.78559148311615 = 1.7121375799179077 + 0.01 * 7.345392227172852
Epoch 80, val loss: 1.741476058959961
Epoch 90, training loss: 1.7170829772949219 = 1.6460120677947998 + 0.01 * 7.107091903686523
Epoch 90, val loss: 1.6838113069534302
Epoch 100, training loss: 1.6314783096313477 = 1.5616979598999023 + 0.01 * 6.978032112121582
Epoch 100, val loss: 1.6148178577423096
Epoch 110, training loss: 1.537224292755127 = 1.4681296348571777 + 0.01 * 6.909460544586182
Epoch 110, val loss: 1.5403410196304321
Epoch 120, training loss: 1.4439588785171509 = 1.3753159046173096 + 0.01 * 6.864299774169922
Epoch 120, val loss: 1.468889832496643
Epoch 130, training loss: 1.3538564443588257 = 1.2854927778244019 + 0.01 * 6.836366176605225
Epoch 130, val loss: 1.4015753269195557
Epoch 140, training loss: 1.2652915716171265 = 1.1971547603607178 + 0.01 * 6.813683032989502
Epoch 140, val loss: 1.3370585441589355
Epoch 150, training loss: 1.1783820390701294 = 1.1103980541229248 + 0.01 * 6.798398971557617
Epoch 150, val loss: 1.274476170539856
Epoch 160, training loss: 1.0944749116897583 = 1.0265580415725708 + 0.01 * 6.79168176651001
Epoch 160, val loss: 1.2141622304916382
Epoch 170, training loss: 1.0138946771621704 = 0.9460094571113586 + 0.01 * 6.788523197174072
Epoch 170, val loss: 1.1559481620788574
Epoch 180, training loss: 0.9356815814971924 = 0.8678237795829773 + 0.01 * 6.78577995300293
Epoch 180, val loss: 1.0990500450134277
Epoch 190, training loss: 0.8588131666183472 = 0.7909882664680481 + 0.01 * 6.7824931144714355
Epoch 190, val loss: 1.043200135231018
Epoch 200, training loss: 0.7833132743835449 = 0.7155245542526245 + 0.01 * 6.778871059417725
Epoch 200, val loss: 0.9890169501304626
Epoch 210, training loss: 0.7106044888496399 = 0.642857551574707 + 0.01 * 6.774693012237549
Epoch 210, val loss: 0.9393250346183777
Epoch 220, training loss: 0.6422479152679443 = 0.57454514503479 + 0.01 * 6.7702765464782715
Epoch 220, val loss: 0.8970156908035278
Epoch 230, training loss: 0.5789796113967896 = 0.5113233923912048 + 0.01 * 6.765625
Epoch 230, val loss: 0.863637387752533
Epoch 240, training loss: 0.5208422541618347 = 0.45323336124420166 + 0.01 * 6.76088809967041
Epoch 240, val loss: 0.8392952680587769
Epoch 250, training loss: 0.4673957824707031 = 0.39985063672065735 + 0.01 * 6.754515647888184
Epoch 250, val loss: 0.8225769400596619
Epoch 260, training loss: 0.41804754734039307 = 0.3505626320838928 + 0.01 * 6.748490810394287
Epoch 260, val loss: 0.8115636706352234
Epoch 270, training loss: 0.3723593056201935 = 0.3048704266548157 + 0.01 * 6.748887538909912
Epoch 270, val loss: 0.8045502305030823
Epoch 280, training loss: 0.330037385225296 = 0.26263660192489624 + 0.01 * 6.740079402923584
Epoch 280, val loss: 0.8005142211914062
Epoch 290, training loss: 0.2914448380470276 = 0.22409826517105103 + 0.01 * 6.734657287597656
Epoch 290, val loss: 0.7990118861198425
Epoch 300, training loss: 0.2570854127407074 = 0.18979425728321075 + 0.01 * 6.729116439819336
Epoch 300, val loss: 0.800051212310791
Epoch 310, training loss: 0.227464959025383 = 0.16020961105823517 + 0.01 * 6.725534915924072
Epoch 310, val loss: 0.8036770224571228
Epoch 320, training loss: 0.20272104442119598 = 0.13553166389465332 + 0.01 * 6.71893835067749
Epoch 320, val loss: 0.8101234436035156
Epoch 330, training loss: 0.18258054554462433 = 0.11543858796358109 + 0.01 * 6.71419620513916
Epoch 330, val loss: 0.8194891810417175
Epoch 340, training loss: 0.16637307405471802 = 0.09927240759134293 + 0.01 * 6.710065841674805
Epoch 340, val loss: 0.8314266800880432
Epoch 350, training loss: 0.15333643555641174 = 0.08627098053693771 + 0.01 * 6.706545352935791
Epoch 350, val loss: 0.8454149961471558
Epoch 360, training loss: 0.14276286959648132 = 0.07572781294584274 + 0.01 * 6.703505992889404
Epoch 360, val loss: 0.8609954118728638
Epoch 370, training loss: 0.13407620787620544 = 0.06705653667449951 + 0.01 * 6.701967716217041
Epoch 370, val loss: 0.8777230978012085
Epoch 380, training loss: 0.1268017590045929 = 0.05981532111763954 + 0.01 * 6.698643684387207
Epoch 380, val loss: 0.8951490521430969
Epoch 390, training loss: 0.12065315246582031 = 0.05368589982390404 + 0.01 * 6.696724891662598
Epoch 390, val loss: 0.9129878878593445
Epoch 400, training loss: 0.11547326296567917 = 0.048442497849464417 + 0.01 * 6.7030768394470215
Epoch 400, val loss: 0.9308358430862427
Epoch 410, training loss: 0.11085610836744308 = 0.043918028473854065 + 0.01 * 6.693808078765869
Epoch 410, val loss: 0.9485741257667542
Epoch 420, training loss: 0.1068747341632843 = 0.03998276963829994 + 0.01 * 6.689196586608887
Epoch 420, val loss: 0.9658625721931458
Epoch 430, training loss: 0.10339730232954025 = 0.036536529660224915 + 0.01 * 6.68607759475708
Epoch 430, val loss: 0.9826741814613342
Epoch 440, training loss: 0.10034076869487762 = 0.03350067138671875 + 0.01 * 6.684009552001953
Epoch 440, val loss: 0.9989483952522278
Epoch 450, training loss: 0.09769944101572037 = 0.030814072117209435 + 0.01 * 6.688537120819092
Epoch 450, val loss: 1.0146127939224243
Epoch 460, training loss: 0.09523564577102661 = 0.028427356854081154 + 0.01 * 6.6808295249938965
Epoch 460, val loss: 1.0297400951385498
Epoch 470, training loss: 0.09308400750160217 = 0.02629736438393593 + 0.01 * 6.678664207458496
Epoch 470, val loss: 1.0443629026412964
Epoch 480, training loss: 0.09114134311676025 = 0.024388868361711502 + 0.01 * 6.675247669219971
Epoch 480, val loss: 1.0585150718688965
Epoch 490, training loss: 0.08940312266349792 = 0.022674793377518654 + 0.01 * 6.672833442687988
Epoch 490, val loss: 1.0722439289093018
Epoch 500, training loss: 0.08782374858856201 = 0.021132031455636024 + 0.01 * 6.669172286987305
Epoch 500, val loss: 1.0855538845062256
Epoch 510, training loss: 0.08643210679292679 = 0.019738739356398582 + 0.01 * 6.669336795806885
Epoch 510, val loss: 1.0984066724777222
Epoch 520, training loss: 0.08511924743652344 = 0.0184769444167614 + 0.01 * 6.664229869842529
Epoch 520, val loss: 1.110945224761963
Epoch 530, training loss: 0.08394499123096466 = 0.017331112176179886 + 0.01 * 6.6613874435424805
Epoch 530, val loss: 1.12312912940979
Epoch 540, training loss: 0.08297479152679443 = 0.0162877906113863 + 0.01 * 6.668700218200684
Epoch 540, val loss: 1.134932279586792
Epoch 550, training loss: 0.08193910866975784 = 0.015336438082158566 + 0.01 * 6.660266876220703
Epoch 550, val loss: 1.1465007066726685
Epoch 560, training loss: 0.08101162314414978 = 0.014466485008597374 + 0.01 * 6.654513835906982
Epoch 560, val loss: 1.1577198505401611
Epoch 570, training loss: 0.08022219687700272 = 0.013669086620211601 + 0.01 * 6.655311107635498
Epoch 570, val loss: 1.1686198711395264
Epoch 580, training loss: 0.07950345426797867 = 0.012936933897435665 + 0.01 * 6.656651973724365
Epoch 580, val loss: 1.179316759109497
Epoch 590, training loss: 0.07869155704975128 = 0.01226325985044241 + 0.01 * 6.642829418182373
Epoch 590, val loss: 1.1897655725479126
Epoch 600, training loss: 0.07824047654867172 = 0.011642135679721832 + 0.01 * 6.659834384918213
Epoch 600, val loss: 1.1998778581619263
Epoch 610, training loss: 0.07743830233812332 = 0.011069012805819511 + 0.01 * 6.636928558349609
Epoch 610, val loss: 1.2097747325897217
Epoch 620, training loss: 0.07686998695135117 = 0.010538620874285698 + 0.01 * 6.633136749267578
Epoch 620, val loss: 1.2194684743881226
Epoch 630, training loss: 0.07644050568342209 = 0.010046844370663166 + 0.01 * 6.6393656730651855
Epoch 630, val loss: 1.2288661003112793
Epoch 640, training loss: 0.07588313519954681 = 0.009590376168489456 + 0.01 * 6.629276275634766
Epoch 640, val loss: 1.2380752563476562
Epoch 650, training loss: 0.07539283484220505 = 0.009165952913463116 + 0.01 * 6.6226887702941895
Epoch 650, val loss: 1.2470911741256714
Epoch 660, training loss: 0.07505682110786438 = 0.008770543150603771 + 0.01 * 6.628628253936768
Epoch 660, val loss: 1.2558430433273315
Epoch 670, training loss: 0.07471540570259094 = 0.008401562459766865 + 0.01 * 6.631383895874023
Epoch 670, val loss: 1.2644919157028198
Epoch 680, training loss: 0.0742514431476593 = 0.008057025261223316 + 0.01 * 6.619441509246826
Epoch 680, val loss: 1.2729072570800781
Epoch 690, training loss: 0.07380420714616776 = 0.007734799291938543 + 0.01 * 6.606940746307373
Epoch 690, val loss: 1.281046986579895
Epoch 700, training loss: 0.07367390394210815 = 0.007432540412992239 + 0.01 * 6.624136447906494
Epoch 700, val loss: 1.2890610694885254
Epoch 710, training loss: 0.07315948605537415 = 0.007149714045226574 + 0.01 * 6.600977897644043
Epoch 710, val loss: 1.2969586849212646
Epoch 720, training loss: 0.07287410646677017 = 0.006883544381707907 + 0.01 * 6.599055767059326
Epoch 720, val loss: 1.3046027421951294
Epoch 730, training loss: 0.07254082709550858 = 0.006633284501731396 + 0.01 * 6.590754508972168
Epoch 730, val loss: 1.312166452407837
Epoch 740, training loss: 0.07237860560417175 = 0.006397290155291557 + 0.01 * 6.5981316566467285
Epoch 740, val loss: 1.3195841312408447
Epoch 750, training loss: 0.07210841029882431 = 0.006174844224005938 + 0.01 * 6.593356609344482
Epoch 750, val loss: 1.3267948627471924
Epoch 760, training loss: 0.07183130085468292 = 0.0059650051407516 + 0.01 * 6.586630344390869
Epoch 760, val loss: 1.3338786363601685
Epoch 770, training loss: 0.07159975916147232 = 0.005766860209405422 + 0.01 * 6.583290100097656
Epoch 770, val loss: 1.3408373594284058
Epoch 780, training loss: 0.07129951566457748 = 0.005579705815762281 + 0.01 * 6.571981430053711
Epoch 780, val loss: 1.3477169275283813
Epoch 790, training loss: 0.0710592195391655 = 0.005402500741183758 + 0.01 * 6.565672397613525
Epoch 790, val loss: 1.3543670177459717
Epoch 800, training loss: 0.07082855701446533 = 0.005234662909060717 + 0.01 * 6.559389591217041
Epoch 800, val loss: 1.3609673976898193
Epoch 810, training loss: 0.07079483568668365 = 0.005075495224446058 + 0.01 * 6.571934700012207
Epoch 810, val loss: 1.367501974105835
Epoch 820, training loss: 0.07065248489379883 = 0.004924213048070669 + 0.01 * 6.572826862335205
Epoch 820, val loss: 1.3738425970077515
Epoch 830, training loss: 0.07025597989559174 = 0.004780962131917477 + 0.01 * 6.547502040863037
Epoch 830, val loss: 1.3800384998321533
Epoch 840, training loss: 0.07009761035442352 = 0.004644723609089851 + 0.01 * 6.545289039611816
Epoch 840, val loss: 1.3862054347991943
Epoch 850, training loss: 0.07001522183418274 = 0.004515037871897221 + 0.01 * 6.550018787384033
Epoch 850, val loss: 1.3920512199401855
Epoch 860, training loss: 0.06994020938873291 = 0.0043918234296143055 + 0.01 * 6.554838180541992
Epoch 860, val loss: 1.3980121612548828
Epoch 870, training loss: 0.06961226463317871 = 0.004274376202374697 + 0.01 * 6.533788681030273
Epoch 870, val loss: 1.4037346839904785
Epoch 880, training loss: 0.06953024864196777 = 0.0041624195873737335 + 0.01 * 6.536782741546631
Epoch 880, val loss: 1.409361720085144
Epoch 890, training loss: 0.0693395659327507 = 0.004055725876241922 + 0.01 * 6.528384685516357
Epoch 890, val loss: 1.414826512336731
Epoch 900, training loss: 0.06924330443143845 = 0.003953907173126936 + 0.01 * 6.528940200805664
Epoch 900, val loss: 1.420378565788269
Epoch 910, training loss: 0.0692102387547493 = 0.0038565306458622217 + 0.01 * 6.535370826721191
Epoch 910, val loss: 1.425600528717041
Epoch 920, training loss: 0.0689905434846878 = 0.003763631684705615 + 0.01 * 6.52269172668457
Epoch 920, val loss: 1.4309017658233643
Epoch 930, training loss: 0.06896170973777771 = 0.003674667328596115 + 0.01 * 6.528703689575195
Epoch 930, val loss: 1.4360853433609009
Epoch 940, training loss: 0.06879445910453796 = 0.0035896271001547575 + 0.01 * 6.520483016967773
Epoch 940, val loss: 1.4411662817001343
Epoch 950, training loss: 0.0686333179473877 = 0.0035079591907560825 + 0.01 * 6.51253604888916
Epoch 950, val loss: 1.446197271347046
Epoch 960, training loss: 0.0684269443154335 = 0.003429683391004801 + 0.01 * 6.49972677230835
Epoch 960, val loss: 1.4508388042449951
Epoch 970, training loss: 0.06835276633501053 = 0.0033549373038113117 + 0.01 * 6.499783039093018
Epoch 970, val loss: 1.4558544158935547
Epoch 980, training loss: 0.06829258054494858 = 0.0032827986869961023 + 0.01 * 6.500978469848633
Epoch 980, val loss: 1.4604414701461792
Epoch 990, training loss: 0.06816960871219635 = 0.003213627729564905 + 0.01 * 6.495597839355469
Epoch 990, val loss: 1.4652692079544067
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.5978
Flip ASR: 0.5244/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0223023891448975 = 1.9385653734207153 + 0.01 * 8.37370777130127
Epoch 0, val loss: 1.9400972127914429
Epoch 10, training loss: 2.012361764907837 = 1.9286264181137085 + 0.01 * 8.373530387878418
Epoch 10, val loss: 1.9297159910202026
Epoch 20, training loss: 1.9998446702957153 = 1.9161155223846436 + 0.01 * 8.37291431427002
Epoch 20, val loss: 1.9158354997634888
Epoch 30, training loss: 1.9821252822875977 = 1.8984153270721436 + 0.01 * 8.370992660522461
Epoch 30, val loss: 1.895308494567871
Epoch 40, training loss: 1.9560034275054932 = 1.8724100589752197 + 0.01 * 8.359336853027344
Epoch 40, val loss: 1.864743709564209
Epoch 50, training loss: 1.9191761016845703 = 1.8365237712860107 + 0.01 * 8.265236854553223
Epoch 50, val loss: 1.8234152793884277
Epoch 60, training loss: 1.8722000122070312 = 1.7951443195343018 + 0.01 * 7.70557165145874
Epoch 60, val loss: 1.778767704963684
Epoch 70, training loss: 1.8277775049209595 = 1.7543102502822876 + 0.01 * 7.346730709075928
Epoch 70, val loss: 1.7390658855438232
Epoch 80, training loss: 1.7774035930633545 = 1.706609845161438 + 0.01 * 7.079378128051758
Epoch 80, val loss: 1.697482705116272
Epoch 90, training loss: 1.712766408920288 = 1.6431798934936523 + 0.01 * 6.958654880523682
Epoch 90, val loss: 1.6443805694580078
Epoch 100, training loss: 1.6292027235031128 = 1.5601177215576172 + 0.01 * 6.908501148223877
Epoch 100, val loss: 1.5752830505371094
Epoch 110, training loss: 1.5293760299682617 = 1.4606285095214844 + 0.01 * 6.874751091003418
Epoch 110, val loss: 1.4954475164413452
Epoch 120, training loss: 1.4225393533706665 = 1.3539538383483887 + 0.01 * 6.858550071716309
Epoch 120, val loss: 1.4129233360290527
Epoch 130, training loss: 1.315846562385559 = 1.2474452257156372 + 0.01 * 6.840136528015137
Epoch 130, val loss: 1.3333837985992432
Epoch 140, training loss: 1.2134355306625366 = 1.1452562808990479 + 0.01 * 6.817922115325928
Epoch 140, val loss: 1.2599940299987793
Epoch 150, training loss: 1.1197621822357178 = 1.0518125295639038 + 0.01 * 6.79496431350708
Epoch 150, val loss: 1.1940040588378906
Epoch 160, training loss: 1.036888599395752 = 0.969110369682312 + 0.01 * 6.777829170227051
Epoch 160, val loss: 1.1354206800460815
Epoch 170, training loss: 0.9633945226669312 = 0.8957186937332153 + 0.01 * 6.767582893371582
Epoch 170, val loss: 1.0837591886520386
Epoch 180, training loss: 0.8961976766586304 = 0.8285700678825378 + 0.01 * 6.762758731842041
Epoch 180, val loss: 1.036543846130371
Epoch 190, training loss: 0.8326197862625122 = 0.7650599479675293 + 0.01 * 6.755983829498291
Epoch 190, val loss: 0.9917099475860596
Epoch 200, training loss: 0.7717121839523315 = 0.7041913866996765 + 0.01 * 6.75208044052124
Epoch 200, val loss: 0.9498615264892578
Epoch 210, training loss: 0.7127531170845032 = 0.6452704071998596 + 0.01 * 6.7482686042785645
Epoch 210, val loss: 0.9116466641426086
Epoch 220, training loss: 0.6546903848648071 = 0.587244987487793 + 0.01 * 6.744539737701416
Epoch 220, val loss: 0.8749056458473206
Epoch 230, training loss: 0.5965286493301392 = 0.5291204452514648 + 0.01 * 6.740821838378906
Epoch 230, val loss: 0.8390350341796875
Epoch 240, training loss: 0.5389212369918823 = 0.47156500816345215 + 0.01 * 6.735620975494385
Epoch 240, val loss: 0.8044677972793579
Epoch 250, training loss: 0.48377007246017456 = 0.41644051671028137 + 0.01 * 6.732954502105713
Epoch 250, val loss: 0.7721598744392395
Epoch 260, training loss: 0.4331282079219818 = 0.36588165163993835 + 0.01 * 6.724656581878662
Epoch 260, val loss: 0.7446684241294861
Epoch 270, training loss: 0.38812440633773804 = 0.3209524154663086 + 0.01 * 6.717197895050049
Epoch 270, val loss: 0.7235991954803467
Epoch 280, training loss: 0.3489319086074829 = 0.28180670738220215 + 0.01 * 6.712520122528076
Epoch 280, val loss: 0.7087492346763611
Epoch 290, training loss: 0.3145636022090912 = 0.247481569647789 + 0.01 * 6.7082037925720215
Epoch 290, val loss: 0.6992603540420532
Epoch 300, training loss: 0.28429561853408813 = 0.21723568439483643 + 0.01 * 6.705992698669434
Epoch 300, val loss: 0.6938067078590393
Epoch 310, training loss: 0.25763240456581116 = 0.1905820369720459 + 0.01 * 6.7050371170043945
Epoch 310, val loss: 0.6922425031661987
Epoch 320, training loss: 0.2343660444021225 = 0.16731944680213928 + 0.01 * 6.704659938812256
Epoch 320, val loss: 0.6944538950920105
Epoch 330, training loss: 0.21426230669021606 = 0.147218257188797 + 0.01 * 6.704405784606934
Epoch 330, val loss: 0.6997913122177124
Epoch 340, training loss: 0.1969681978225708 = 0.1299292892217636 + 0.01 * 6.703890800476074
Epoch 340, val loss: 0.707824170589447
Epoch 350, training loss: 0.182123064994812 = 0.11509101092815399 + 0.01 * 6.7032060623168945
Epoch 350, val loss: 0.7183787226676941
Epoch 360, training loss: 0.16936805844306946 = 0.10234401375055313 + 0.01 * 6.702404499053955
Epoch 360, val loss: 0.7308351397514343
Epoch 370, training loss: 0.15837834775447845 = 0.0913625955581665 + 0.01 * 6.70157527923584
Epoch 370, val loss: 0.7446565628051758
Epoch 380, training loss: 0.14883241057395935 = 0.08182597160339355 + 0.01 * 6.700644493103027
Epoch 380, val loss: 0.7593250870704651
Epoch 390, training loss: 0.14043459296226501 = 0.07343825697898865 + 0.01 * 6.699633598327637
Epoch 390, val loss: 0.7742109894752502
Epoch 400, training loss: 0.13286375999450684 = 0.06587423384189606 + 0.01 * 6.698952674865723
Epoch 400, val loss: 0.7890204787254333
Epoch 410, training loss: 0.12566937506198883 = 0.058685366064310074 + 0.01 * 6.69840145111084
Epoch 410, val loss: 0.8036225438117981
Epoch 420, training loss: 0.11853337287902832 = 0.051565445959568024 + 0.01 * 6.696793079376221
Epoch 420, val loss: 0.8174524307250977
Epoch 430, training loss: 0.1118190735578537 = 0.04486572742462158 + 0.01 * 6.6953349113464355
Epoch 430, val loss: 0.8312913775444031
Epoch 440, training loss: 0.1066383570432663 = 0.03969826549291611 + 0.01 * 6.694009304046631
Epoch 440, val loss: 0.8448593616485596
Epoch 450, training loss: 0.10250969231128693 = 0.03557630628347397 + 0.01 * 6.693338394165039
Epoch 450, val loss: 0.8597846627235413
Epoch 460, training loss: 0.09905505925416946 = 0.032139651477336884 + 0.01 * 6.6915411949157715
Epoch 460, val loss: 0.8751695156097412
Epoch 470, training loss: 0.09609616547822952 = 0.029201628640294075 + 0.01 * 6.689454078674316
Epoch 470, val loss: 0.8902185559272766
Epoch 480, training loss: 0.0935620591044426 = 0.02664984203875065 + 0.01 * 6.691221714019775
Epoch 480, val loss: 0.9047629833221436
Epoch 490, training loss: 0.09128332138061523 = 0.024416668340563774 + 0.01 * 6.686665058135986
Epoch 490, val loss: 0.9192036986351013
Epoch 500, training loss: 0.08929753303527832 = 0.022455494850873947 + 0.01 * 6.684204578399658
Epoch 500, val loss: 0.9329521656036377
Epoch 510, training loss: 0.08754550665616989 = 0.020718099549412727 + 0.01 * 6.682740688323975
Epoch 510, val loss: 0.94619220495224
Epoch 520, training loss: 0.0860038697719574 = 0.019172418862581253 + 0.01 * 6.683145523071289
Epoch 520, val loss: 0.9592252373695374
Epoch 530, training loss: 0.0845879390835762 = 0.017790505662560463 + 0.01 * 6.679743766784668
Epoch 530, val loss: 0.9717357754707336
Epoch 540, training loss: 0.08332132548093796 = 0.016551397740840912 + 0.01 * 6.676992893218994
Epoch 540, val loss: 0.9840843081474304
Epoch 550, training loss: 0.08218824118375778 = 0.015435641631484032 + 0.01 * 6.675259590148926
Epoch 550, val loss: 0.9958786368370056
Epoch 560, training loss: 0.08116699010133743 = 0.01442845817655325 + 0.01 * 6.673853397369385
Epoch 560, val loss: 1.007459044456482
Epoch 570, training loss: 0.0802498310804367 = 0.01351640559732914 + 0.01 * 6.673343181610107
Epoch 570, val loss: 1.0185953378677368
Epoch 580, training loss: 0.0793846845626831 = 0.01268810871988535 + 0.01 * 6.6696577072143555
Epoch 580, val loss: 1.0295127630233765
Epoch 590, training loss: 0.07860750705003738 = 0.011933335103094578 + 0.01 * 6.667417526245117
Epoch 590, val loss: 1.040117621421814
Epoch 600, training loss: 0.07790801674127579 = 0.011244046501815319 + 0.01 * 6.666397571563721
Epoch 600, val loss: 1.0503873825073242
Epoch 610, training loss: 0.07723592966794968 = 0.010612012818455696 + 0.01 * 6.6623921394348145
Epoch 610, val loss: 1.0604209899902344
Epoch 620, training loss: 0.07664033025503159 = 0.010030562058091164 + 0.01 * 6.660976886749268
Epoch 620, val loss: 1.070226788520813
Epoch 630, training loss: 0.07611102610826492 = 0.009493422694504261 + 0.01 * 6.661759853363037
Epoch 630, val loss: 1.0796926021575928
Epoch 640, training loss: 0.07555603981018066 = 0.008995827287435532 + 0.01 * 6.656021595001221
Epoch 640, val loss: 1.0890378952026367
Epoch 650, training loss: 0.0750621110200882 = 0.00853613018989563 + 0.01 * 6.652597904205322
Epoch 650, val loss: 1.098354697227478
Epoch 660, training loss: 0.07465719431638718 = 0.008109471760690212 + 0.01 * 6.6547722816467285
Epoch 660, val loss: 1.107210397720337
Epoch 670, training loss: 0.07423263788223267 = 0.007713927421718836 + 0.01 * 6.6518707275390625
Epoch 670, val loss: 1.116063117980957
Epoch 680, training loss: 0.07378767430782318 = 0.00734783336520195 + 0.01 * 6.643984317779541
Epoch 680, val loss: 1.1246364116668701
Epoch 690, training loss: 0.07343094050884247 = 0.007007726468145847 + 0.01 * 6.642321586608887
Epoch 690, val loss: 1.1330548524856567
Epoch 700, training loss: 0.07316796481609344 = 0.006691652815788984 + 0.01 * 6.647631645202637
Epoch 700, val loss: 1.1411535739898682
Epoch 710, training loss: 0.07276928424835205 = 0.006397710181772709 + 0.01 * 6.637157917022705
Epoch 710, val loss: 1.1491968631744385
Epoch 720, training loss: 0.07251490652561188 = 0.006124445702880621 + 0.01 * 6.639046669006348
Epoch 720, val loss: 1.1570860147476196
Epoch 730, training loss: 0.07218647003173828 = 0.005869890097528696 + 0.01 * 6.631658554077148
Epoch 730, val loss: 1.1646709442138672
Epoch 740, training loss: 0.07193553447723389 = 0.005632810760289431 + 0.01 * 6.630272388458252
Epoch 740, val loss: 1.1722146272659302
Epoch 750, training loss: 0.07164960354566574 = 0.005411265883594751 + 0.01 * 6.623833656311035
Epoch 750, val loss: 1.1794780492782593
Epoch 760, training loss: 0.07150737196207047 = 0.00520390085875988 + 0.01 * 6.63034725189209
Epoch 760, val loss: 1.186544418334961
Epoch 770, training loss: 0.07118413597345352 = 0.005009988788515329 + 0.01 * 6.617415428161621
Epoch 770, val loss: 1.1934995651245117
Epoch 780, training loss: 0.07097912579774857 = 0.00482847960665822 + 0.01 * 6.61506462097168
Epoch 780, val loss: 1.2003309726715088
Epoch 790, training loss: 0.07077933102846146 = 0.004658018704503775 + 0.01 * 6.612131118774414
Epoch 790, val loss: 1.2068251371383667
Epoch 800, training loss: 0.07064278423786163 = 0.004497812129557133 + 0.01 * 6.614497184753418
Epoch 800, val loss: 1.213221788406372
Epoch 810, training loss: 0.0703546404838562 = 0.004347103647887707 + 0.01 * 6.6007537841796875
Epoch 810, val loss: 1.2195920944213867
Epoch 820, training loss: 0.07023327797651291 = 0.004205281846225262 + 0.01 * 6.602799892425537
Epoch 820, val loss: 1.2256722450256348
Epoch 830, training loss: 0.06999996304512024 = 0.004072421696037054 + 0.01 * 6.592754364013672
Epoch 830, val loss: 1.2317007780075073
Epoch 840, training loss: 0.06976710259914398 = 0.003946642857044935 + 0.01 * 6.5820465087890625
Epoch 840, val loss: 1.2374168634414673
Epoch 850, training loss: 0.06965117156505585 = 0.003827729495242238 + 0.01 * 6.5823445320129395
Epoch 850, val loss: 1.243233323097229
Epoch 860, training loss: 0.06948817521333694 = 0.0037152820732444525 + 0.01 * 6.577289581298828
Epoch 860, val loss: 1.2486993074417114
Epoch 870, training loss: 0.06948903203010559 = 0.0036087846383452415 + 0.01 * 6.588024616241455
Epoch 870, val loss: 1.254179835319519
Epoch 880, training loss: 0.06913299858570099 = 0.0035079349763691425 + 0.01 * 6.562506675720215
Epoch 880, val loss: 1.2593110799789429
Epoch 890, training loss: 0.06911194324493408 = 0.003412415273487568 + 0.01 * 6.569952964782715
Epoch 890, val loss: 1.2644035816192627
Epoch 900, training loss: 0.0689081996679306 = 0.0033225668594241142 + 0.01 * 6.558563709259033
Epoch 900, val loss: 1.2694004774093628
Epoch 910, training loss: 0.06923792511224747 = 0.0032368178945034742 + 0.01 * 6.60011100769043
Epoch 910, val loss: 1.2742480039596558
Epoch 920, training loss: 0.06861415505409241 = 0.0031556226313114166 + 0.01 * 6.5458526611328125
Epoch 920, val loss: 1.2788562774658203
Epoch 930, training loss: 0.0685906857252121 = 0.003078320063650608 + 0.01 * 6.551237106323242
Epoch 930, val loss: 1.2834079265594482
Epoch 940, training loss: 0.06839770078659058 = 0.00300442217849195 + 0.01 * 6.539327621459961
Epoch 940, val loss: 1.288025140762329
Epoch 950, training loss: 0.06815987825393677 = 0.002934342250227928 + 0.01 * 6.52255392074585
Epoch 950, val loss: 1.2922897338867188
Epoch 960, training loss: 0.06798975169658661 = 0.0028674653731286526 + 0.01 * 6.512228488922119
Epoch 960, val loss: 1.296592116355896
Epoch 970, training loss: 0.0680040568113327 = 0.0028036492876708508 + 0.01 * 6.520040988922119
Epoch 970, val loss: 1.300735592842102
Epoch 980, training loss: 0.06800256669521332 = 0.002742880256846547 + 0.01 * 6.5259690284729
Epoch 980, val loss: 1.3046823740005493
Epoch 990, training loss: 0.06779569387435913 = 0.0026846379041671753 + 0.01 * 6.511106014251709
Epoch 990, val loss: 1.3086532354354858
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.7786
Flip ASR: 0.7378/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.04068922996521 = 1.9569523334503174 + 0.01 * 8.373685836791992
Epoch 0, val loss: 1.9586848020553589
Epoch 10, training loss: 2.030486822128296 = 1.9467512369155884 + 0.01 * 8.373547554016113
Epoch 10, val loss: 1.9484878778457642
Epoch 20, training loss: 2.0179309844970703 = 1.9342002868652344 + 0.01 * 8.37306022644043
Epoch 20, val loss: 1.9353365898132324
Epoch 30, training loss: 1.999995470046997 = 1.9162803888320923 + 0.01 * 8.37151050567627
Epoch 30, val loss: 1.9162940979003906
Epoch 40, training loss: 1.9729187488555908 = 1.889288067817688 + 0.01 * 8.363072395324707
Epoch 40, val loss: 1.8877427577972412
Epoch 50, training loss: 1.933450698852539 = 1.8503793478012085 + 0.01 * 8.307138442993164
Epoch 50, val loss: 1.8479279279708862
Epoch 60, training loss: 1.8833141326904297 = 1.8035552501678467 + 0.01 * 7.975891590118408
Epoch 60, val loss: 1.8035006523132324
Epoch 70, training loss: 1.833693265914917 = 1.7586735486984253 + 0.01 * 7.501972675323486
Epoch 70, val loss: 1.7631900310516357
Epoch 80, training loss: 1.7787320613861084 = 1.7074724435806274 + 0.01 * 7.125967979431152
Epoch 80, val loss: 1.7162202596664429
Epoch 90, training loss: 1.709088921546936 = 1.6391048431396484 + 0.01 * 6.998404502868652
Epoch 90, val loss: 1.6560121774673462
Epoch 100, training loss: 1.6206352710723877 = 1.5511484146118164 + 0.01 * 6.948689937591553
Epoch 100, val loss: 1.5816166400909424
Epoch 110, training loss: 1.5187318325042725 = 1.4495631456375122 + 0.01 * 6.916867256164551
Epoch 110, val loss: 1.4985201358795166
Epoch 120, training loss: 1.4132441282272339 = 1.344165563583374 + 0.01 * 6.907858848571777
Epoch 120, val loss: 1.4153084754943848
Epoch 130, training loss: 1.3082181215286255 = 1.2391916513442993 + 0.01 * 6.902650833129883
Epoch 130, val loss: 1.335724115371704
Epoch 140, training loss: 1.2027415037155151 = 1.1337504386901855 + 0.01 * 6.899102210998535
Epoch 140, val loss: 1.2579584121704102
Epoch 150, training loss: 1.0978924036026 = 1.028952717781067 + 0.01 * 6.8939666748046875
Epoch 150, val loss: 1.181792140007019
Epoch 160, training loss: 0.9964287877082825 = 0.9275515079498291 + 0.01 * 6.887728214263916
Epoch 160, val loss: 1.1091182231903076
Epoch 170, training loss: 0.9016218185424805 = 0.8328179121017456 + 0.01 * 6.8803887367248535
Epoch 170, val loss: 1.041918158531189
Epoch 180, training loss: 0.8160825967788696 = 0.7473585605621338 + 0.01 * 6.872401237487793
Epoch 180, val loss: 0.982295036315918
Epoch 190, training loss: 0.7405431866645813 = 0.6718997359275818 + 0.01 * 6.864345550537109
Epoch 190, val loss: 0.9312676787376404
Epoch 200, training loss: 0.6741005778312683 = 0.605537474155426 + 0.01 * 6.856309413909912
Epoch 200, val loss: 0.8880139589309692
Epoch 210, training loss: 0.6152100563049316 = 0.5467278361320496 + 0.01 * 6.848221302032471
Epoch 210, val loss: 0.8514989614486694
Epoch 220, training loss: 0.5625838041305542 = 0.49418121576309204 + 0.01 * 6.840260982513428
Epoch 220, val loss: 0.8210198283195496
Epoch 230, training loss: 0.5153316259384155 = 0.44700756669044495 + 0.01 * 6.8324079513549805
Epoch 230, val loss: 0.7965774536132812
Epoch 240, training loss: 0.47264429926872253 = 0.4043949544429779 + 0.01 * 6.824934959411621
Epoch 240, val loss: 0.777478039264679
Epoch 250, training loss: 0.4336642622947693 = 0.36549466848373413 + 0.01 * 6.816959381103516
Epoch 250, val loss: 0.7626683115959167
Epoch 260, training loss: 0.397592693567276 = 0.3294869363307953 + 0.01 * 6.810576438903809
Epoch 260, val loss: 0.750848650932312
Epoch 270, training loss: 0.36374562978744507 = 0.2957442104816437 + 0.01 * 6.800140380859375
Epoch 270, val loss: 0.7412823438644409
Epoch 280, training loss: 0.33182859420776367 = 0.2639308273792267 + 0.01 * 6.789775371551514
Epoch 280, val loss: 0.7334880828857422
Epoch 290, training loss: 0.3020121157169342 = 0.23409752547740936 + 0.01 * 6.791459560394287
Epoch 290, val loss: 0.7276469469070435
Epoch 300, training loss: 0.2743290066719055 = 0.20662559568881989 + 0.01 * 6.770341396331787
Epoch 300, val loss: 0.7234572768211365
Epoch 310, training loss: 0.24931052327156067 = 0.18171359598636627 + 0.01 * 6.759692192077637
Epoch 310, val loss: 0.7208564877510071
Epoch 320, training loss: 0.22698365151882172 = 0.1594732105731964 + 0.01 * 6.751044273376465
Epoch 320, val loss: 0.7200881242752075
Epoch 330, training loss: 0.20735692977905273 = 0.13985265791416168 + 0.01 * 6.75042724609375
Epoch 330, val loss: 0.7208248972892761
Epoch 340, training loss: 0.18995675444602966 = 0.12262551486492157 + 0.01 * 6.73312520980835
Epoch 340, val loss: 0.7231788039207458
Epoch 350, training loss: 0.17478778958320618 = 0.1075422391295433 + 0.01 * 6.724555969238281
Epoch 350, val loss: 0.7271461486816406
Epoch 360, training loss: 0.1616465449333191 = 0.09445270895957947 + 0.01 * 6.719383239746094
Epoch 360, val loss: 0.7325999140739441
Epoch 370, training loss: 0.1503603756427765 = 0.08318428695201874 + 0.01 * 6.71760892868042
Epoch 370, val loss: 0.7390637993812561
Epoch 380, training loss: 0.1406119167804718 = 0.07351555675268173 + 0.01 * 6.709636211395264
Epoch 380, val loss: 0.7464426159858704
Epoch 390, training loss: 0.13225701451301575 = 0.06519801914691925 + 0.01 * 6.705899238586426
Epoch 390, val loss: 0.7543165683746338
Epoch 400, training loss: 0.12501803040504456 = 0.058029793202877045 + 0.01 * 6.698822975158691
Epoch 400, val loss: 0.7625036239624023
Epoch 410, training loss: 0.11879298090934753 = 0.05183438956737518 + 0.01 * 6.695859432220459
Epoch 410, val loss: 0.7708531022071838
Epoch 420, training loss: 0.1134621798992157 = 0.04647425562143326 + 0.01 * 6.698792457580566
Epoch 420, val loss: 0.7792403697967529
Epoch 430, training loss: 0.1086740791797638 = 0.041829124093055725 + 0.01 * 6.68449592590332
Epoch 430, val loss: 0.7876036763191223
Epoch 440, training loss: 0.10458511859178543 = 0.03778619319200516 + 0.01 * 6.6798930168151855
Epoch 440, val loss: 0.795961856842041
Epoch 450, training loss: 0.10101185739040375 = 0.03425624966621399 + 0.01 * 6.67556095123291
Epoch 450, val loss: 0.8042770624160767
Epoch 460, training loss: 0.09802628308534622 = 0.031165389344096184 + 0.01 * 6.686089515686035
Epoch 460, val loss: 0.8125402927398682
Epoch 470, training loss: 0.09513712674379349 = 0.028454599902033806 + 0.01 * 6.668252944946289
Epoch 470, val loss: 0.820730984210968
Epoch 480, training loss: 0.09271596372127533 = 0.026067642495036125 + 0.01 * 6.66483211517334
Epoch 480, val loss: 0.8288767337799072
Epoch 490, training loss: 0.09058398008346558 = 0.02395983785390854 + 0.01 * 6.66241455078125
Epoch 490, val loss: 0.8369207382202148
Epoch 500, training loss: 0.08859068900346756 = 0.02209288440644741 + 0.01 * 6.6497802734375
Epoch 500, val loss: 0.844886064529419
Epoch 510, training loss: 0.08689616620540619 = 0.02043246664106846 + 0.01 * 6.646369934082031
Epoch 510, val loss: 0.8527775406837463
Epoch 520, training loss: 0.08535885810852051 = 0.018952365964651108 + 0.01 * 6.640649795532227
Epoch 520, val loss: 0.8605210185050964
Epoch 530, training loss: 0.0843108668923378 = 0.01762731373310089 + 0.01 * 6.668355464935303
Epoch 530, val loss: 0.8681321740150452
Epoch 540, training loss: 0.08270063251256943 = 0.016439883038401604 + 0.01 * 6.62607479095459
Epoch 540, val loss: 0.8755933046340942
Epoch 550, training loss: 0.08160877227783203 = 0.015370081178843975 + 0.01 * 6.6238694190979
Epoch 550, val loss: 0.8829202055931091
Epoch 560, training loss: 0.08057331293821335 = 0.0144036915153265 + 0.01 * 6.61696195602417
Epoch 560, val loss: 0.8901184797286987
Epoch 570, training loss: 0.07965356111526489 = 0.013528612442314625 + 0.01 * 6.612494468688965
Epoch 570, val loss: 0.8971689343452454
Epoch 580, training loss: 0.07891669124364853 = 0.012734094634652138 + 0.01 * 6.618259906768799
Epoch 580, val loss: 0.9040982127189636
Epoch 590, training loss: 0.0781676173210144 = 0.012011151760816574 + 0.01 * 6.615646839141846
Epoch 590, val loss: 0.9108486771583557
Epoch 600, training loss: 0.07728077471256256 = 0.011352542787790298 + 0.01 * 6.592823505401611
Epoch 600, val loss: 0.9174286723136902
Epoch 610, training loss: 0.07668586820363998 = 0.010749285109341145 + 0.01 * 6.593657970428467
Epoch 610, val loss: 0.9238611459732056
Epoch 620, training loss: 0.07589543610811234 = 0.010196802206337452 + 0.01 * 6.569863319396973
Epoch 620, val loss: 0.930183470249176
Epoch 630, training loss: 0.07535622268915176 = 0.009688549675047398 + 0.01 * 6.56676721572876
Epoch 630, val loss: 0.9363434910774231
Epoch 640, training loss: 0.07487423717975616 = 0.009220434352755547 + 0.01 * 6.565380573272705
Epoch 640, val loss: 0.9423536062240601
Epoch 650, training loss: 0.07461030036211014 = 0.00878859218209982 + 0.01 * 6.5821709632873535
Epoch 650, val loss: 0.9482302069664001
Epoch 660, training loss: 0.07387474924325943 = 0.008388913236558437 + 0.01 * 6.548583984375
Epoch 660, val loss: 0.9539628624916077
Epoch 670, training loss: 0.0736754834651947 = 0.008018393069505692 + 0.01 * 6.565709590911865
Epoch 670, val loss: 0.9595578908920288
Epoch 680, training loss: 0.07306113839149475 = 0.007674846798181534 + 0.01 * 6.538629531860352
Epoch 680, val loss: 0.9650481939315796
Epoch 690, training loss: 0.07287705689668655 = 0.007354872766882181 + 0.01 * 6.552218437194824
Epoch 690, val loss: 0.9703892469406128
Epoch 700, training loss: 0.07241257280111313 = 0.007056948728859425 + 0.01 * 6.535562515258789
Epoch 700, val loss: 0.9756295680999756
Epoch 710, training loss: 0.07208538055419922 = 0.00677879061549902 + 0.01 * 6.530658721923828
Epoch 710, val loss: 0.9807018041610718
Epoch 720, training loss: 0.07206916064023972 = 0.006519109010696411 + 0.01 * 6.5550055503845215
Epoch 720, val loss: 0.9857165217399597
Epoch 730, training loss: 0.07148203998804092 = 0.006275645457208157 + 0.01 * 6.520639419555664
Epoch 730, val loss: 0.9905855655670166
Epoch 740, training loss: 0.07120970636606216 = 0.006047600414603949 + 0.01 * 6.516210556030273
Epoch 740, val loss: 0.995346188545227
Epoch 750, training loss: 0.07106474041938782 = 0.005833461880683899 + 0.01 * 6.523128032684326
Epoch 750, val loss: 1.0000412464141846
Epoch 760, training loss: 0.07066228240728378 = 0.0056318375281989574 + 0.01 * 6.503044605255127
Epoch 760, val loss: 1.0045913457870483
Epoch 770, training loss: 0.07039987295866013 = 0.0054422360844910145 + 0.01 * 6.495763778686523
Epoch 770, val loss: 1.009049892425537
Epoch 780, training loss: 0.07070491462945938 = 0.005263515282422304 + 0.01 * 6.544140338897705
Epoch 780, val loss: 1.0134203433990479
Epoch 790, training loss: 0.06996970623731613 = 0.005095118191093206 + 0.01 * 6.487459182739258
Epoch 790, val loss: 1.017700433731079
Epoch 800, training loss: 0.06980028003454208 = 0.004935800097882748 + 0.01 * 6.486447811126709
Epoch 800, val loss: 1.0218291282653809
Epoch 810, training loss: 0.06991756707429886 = 0.0047853305004537106 + 0.01 * 6.513223648071289
Epoch 810, val loss: 1.0259191989898682
Epoch 820, training loss: 0.06942122429609299 = 0.004642649553716183 + 0.01 * 6.47785758972168
Epoch 820, val loss: 1.029897689819336
Epoch 830, training loss: 0.06985782831907272 = 0.004507373087108135 + 0.01 * 6.535046100616455
Epoch 830, val loss: 1.0337884426116943
Epoch 840, training loss: 0.06905794888734818 = 0.004378975834697485 + 0.01 * 6.467897415161133
Epoch 840, val loss: 1.037612795829773
Epoch 850, training loss: 0.06888533383607864 = 0.004256915766745806 + 0.01 * 6.462841987609863
Epoch 850, val loss: 1.041344404220581
Epoch 860, training loss: 0.06878210604190826 = 0.004140815697610378 + 0.01 * 6.4641289710998535
Epoch 860, val loss: 1.0449999570846558
Epoch 870, training loss: 0.06848466396331787 = 0.004030380863696337 + 0.01 * 6.445427894592285
Epoch 870, val loss: 1.0485972166061401
Epoch 880, training loss: 0.06884508579969406 = 0.003925376106053591 + 0.01 * 6.491971492767334
Epoch 880, val loss: 1.05210542678833
Epoch 890, training loss: 0.06817372143268585 = 0.0038250081706792116 + 0.01 * 6.434871673583984
Epoch 890, val loss: 1.0555449724197388
Epoch 900, training loss: 0.06821143627166748 = 0.0037294356152415276 + 0.01 * 6.448200225830078
Epoch 900, val loss: 1.0589135885238647
Epoch 910, training loss: 0.06808552145957947 = 0.0036383166443556547 + 0.01 * 6.444720268249512
Epoch 910, val loss: 1.0622137784957886
Epoch 920, training loss: 0.06784985959529877 = 0.0035509797744452953 + 0.01 * 6.429888725280762
Epoch 920, val loss: 1.0654568672180176
Epoch 930, training loss: 0.06774888932704926 = 0.003467580070719123 + 0.01 * 6.428131103515625
Epoch 930, val loss: 1.068622350692749
Epoch 940, training loss: 0.06767256557941437 = 0.0033877685200423002 + 0.01 * 6.4284796714782715
Epoch 940, val loss: 1.0717318058013916
Epoch 950, training loss: 0.06751193106174469 = 0.003311388660222292 + 0.01 * 6.4200544357299805
Epoch 950, val loss: 1.0747891664505005
Epoch 960, training loss: 0.06751310080289841 = 0.003238114295527339 + 0.01 * 6.427499294281006
Epoch 960, val loss: 1.0777736902236938
Epoch 970, training loss: 0.06736666709184647 = 0.003167747985571623 + 0.01 * 6.419891834259033
Epoch 970, val loss: 1.080702304840088
Epoch 980, training loss: 0.0673220083117485 = 0.0031004282645881176 + 0.01 * 6.422158718109131
Epoch 980, val loss: 1.083619236946106
Epoch 990, training loss: 0.06719732284545898 = 0.003035537665709853 + 0.01 * 6.4161787033081055
Epoch 990, val loss: 1.086413025856018
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.6605
Flip ASR: 0.6222/225 nodes
The final ASR:0.67897, 0.07496, Accuracy:0.82099, 0.01222
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11660])
remove edge: torch.Size([2, 9506])
updated graph: torch.Size([2, 10610])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.97909, 0.00969, Accuracy:0.83210, 0.00761
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0.3 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.015019178390503 = 1.9312808513641357 + 0.01 * 8.373835563659668
Epoch 0, val loss: 1.930207371711731
Epoch 10, training loss: 2.0051662921905518 = 1.9214287996292114 + 0.01 * 8.373751640319824
Epoch 10, val loss: 1.9207499027252197
Epoch 20, training loss: 1.9928770065307617 = 1.9091421365737915 + 0.01 * 8.373489379882812
Epoch 20, val loss: 1.9091613292694092
Epoch 30, training loss: 1.975572109222412 = 1.8918441534042358 + 0.01 * 8.372798919677734
Epoch 30, val loss: 1.8931808471679688
Epoch 40, training loss: 1.9503366947174072 = 1.8666402101516724 + 0.01 * 8.369654655456543
Epoch 40, val loss: 1.8707154989242554
Epoch 50, training loss: 1.9159364700317383 = 1.8324726819992065 + 0.01 * 8.346381187438965
Epoch 50, val loss: 1.8422147035598755
Epoch 60, training loss: 1.8764660358428955 = 1.7945444583892822 + 0.01 * 8.19215202331543
Epoch 60, val loss: 1.8135170936584473
Epoch 70, training loss: 1.8348076343536377 = 1.7561770677566528 + 0.01 * 7.8630595207214355
Epoch 70, val loss: 1.7817816734313965
Epoch 80, training loss: 1.780105471611023 = 1.7040021419525146 + 0.01 * 7.610330104827881
Epoch 80, val loss: 1.733267903327942
Epoch 90, training loss: 1.70591402053833 = 1.6320574283599854 + 0.01 * 7.385659694671631
Epoch 90, val loss: 1.671202540397644
Epoch 100, training loss: 1.611722469329834 = 1.5401835441589355 + 0.01 * 7.15389347076416
Epoch 100, val loss: 1.5977342128753662
Epoch 110, training loss: 1.5081859827041626 = 1.4374048709869385 + 0.01 * 7.078115940093994
Epoch 110, val loss: 1.5139955282211304
Epoch 120, training loss: 1.4021116495132446 = 1.3320178985595703 + 0.01 * 7.009369850158691
Epoch 120, val loss: 1.4292181730270386
Epoch 130, training loss: 1.296554684638977 = 1.22691011428833 + 0.01 * 6.964457988739014
Epoch 130, val loss: 1.3449110984802246
Epoch 140, training loss: 1.1933964490890503 = 1.1240808963775635 + 0.01 * 6.931551456451416
Epoch 140, val loss: 1.2641152143478394
Epoch 150, training loss: 1.0962032079696655 = 1.0271342992782593 + 0.01 * 6.906888484954834
Epoch 150, val loss: 1.1905299425125122
Epoch 160, training loss: 1.0068459510803223 = 0.9379163980484009 + 0.01 * 6.892953872680664
Epoch 160, val loss: 1.124976396560669
Epoch 170, training loss: 0.9244128465652466 = 0.8556216359138489 + 0.01 * 6.879122257232666
Epoch 170, val loss: 1.0661242008209229
Epoch 180, training loss: 0.8475968837738037 = 0.7789111137390137 + 0.01 * 6.868579864501953
Epoch 180, val loss: 1.0130013227462769
Epoch 190, training loss: 0.776138424873352 = 0.7075430750846863 + 0.01 * 6.859537124633789
Epoch 190, val loss: 0.9660748243331909
Epoch 200, training loss: 0.7102773189544678 = 0.6417816877365112 + 0.01 * 6.849562168121338
Epoch 200, val loss: 0.926153302192688
Epoch 210, training loss: 0.6499936580657959 = 0.5816026926040649 + 0.01 * 6.839098930358887
Epoch 210, val loss: 0.893276035785675
Epoch 220, training loss: 0.5950268507003784 = 0.5267385840415955 + 0.01 * 6.828823566436768
Epoch 220, val loss: 0.8674723505973816
Epoch 230, training loss: 0.5447888970375061 = 0.4765941798686981 + 0.01 * 6.819472312927246
Epoch 230, val loss: 0.8482986688613892
Epoch 240, training loss: 0.49845248460769653 = 0.4303741753101349 + 0.01 * 6.807832717895508
Epoch 240, val loss: 0.8351125717163086
Epoch 250, training loss: 0.4552379846572876 = 0.38724395632743835 + 0.01 * 6.79940128326416
Epoch 250, val loss: 0.8274652361869812
Epoch 260, training loss: 0.4144711494445801 = 0.34653741121292114 + 0.01 * 6.793373107910156
Epoch 260, val loss: 0.8248019814491272
Epoch 270, training loss: 0.3757362365722656 = 0.3079598546028137 + 0.01 * 6.777637481689453
Epoch 270, val loss: 0.8266351222991943
Epoch 280, training loss: 0.3393247127532959 = 0.27161744236946106 + 0.01 * 6.77072811126709
Epoch 280, val loss: 0.8325144648551941
Epoch 290, training loss: 0.30564138293266296 = 0.2379986196756363 + 0.01 * 6.764277458190918
Epoch 290, val loss: 0.8419732451438904
Epoch 300, training loss: 0.27518266439437866 = 0.2076333910226822 + 0.01 * 6.754929065704346
Epoch 300, val loss: 0.8546635508537292
Epoch 310, training loss: 0.24825868010520935 = 0.18079149723052979 + 0.01 * 6.746718406677246
Epoch 310, val loss: 0.8705570101737976
Epoch 320, training loss: 0.22496983408927917 = 0.1574273705482483 + 0.01 * 6.754245758056641
Epoch 320, val loss: 0.8889905214309692
Epoch 330, training loss: 0.2046433687210083 = 0.1372966170310974 + 0.01 * 6.734675884246826
Epoch 330, val loss: 0.9094463586807251
Epoch 340, training loss: 0.18736527860164642 = 0.12000811100006104 + 0.01 * 6.735716819763184
Epoch 340, val loss: 0.9313039183616638
Epoch 350, training loss: 0.1724284291267395 = 0.10518872737884521 + 0.01 * 6.72396993637085
Epoch 350, val loss: 0.9541621208190918
Epoch 360, training loss: 0.15968823432922363 = 0.09248458594083786 + 0.01 * 6.720365047454834
Epoch 360, val loss: 0.9775104522705078
Epoch 370, training loss: 0.14874857664108276 = 0.08159957081079483 + 0.01 * 6.714901447296143
Epoch 370, val loss: 1.001236081123352
Epoch 380, training loss: 0.13940472900867462 = 0.07227113842964172 + 0.01 * 6.713359355926514
Epoch 380, val loss: 1.0248554944992065
Epoch 390, training loss: 0.13134479522705078 = 0.0642646849155426 + 0.01 * 6.708010673522949
Epoch 390, val loss: 1.0481575727462769
Epoch 400, training loss: 0.12436634302139282 = 0.05737707391381264 + 0.01 * 6.69892692565918
Epoch 400, val loss: 1.0709365606307983
Epoch 410, training loss: 0.11841058731079102 = 0.051436182111501694 + 0.01 * 6.697440147399902
Epoch 410, val loss: 1.0929853916168213
Epoch 420, training loss: 0.11325863748788834 = 0.046290770173072815 + 0.01 * 6.696786880493164
Epoch 420, val loss: 1.1142545938491821
Epoch 430, training loss: 0.10873641073703766 = 0.041821740567684174 + 0.01 * 6.691466808319092
Epoch 430, val loss: 1.1347893476486206
Epoch 440, training loss: 0.10485373437404633 = 0.037921417504549026 + 0.01 * 6.693232536315918
Epoch 440, val loss: 1.154554843902588
Epoch 450, training loss: 0.10132835060358047 = 0.03450436890125275 + 0.01 * 6.682398319244385
Epoch 450, val loss: 1.1735477447509766
Epoch 460, training loss: 0.09822003543376923 = 0.03149675950407982 + 0.01 * 6.672327041625977
Epoch 460, val loss: 1.1918954849243164
Epoch 470, training loss: 0.09555280953645706 = 0.02883799374103546 + 0.01 * 6.671481609344482
Epoch 470, val loss: 1.2096234560012817
Epoch 480, training loss: 0.09310206770896912 = 0.026478968560695648 + 0.01 * 6.6623101234436035
Epoch 480, val loss: 1.226799726486206
Epoch 490, training loss: 0.09102825820446014 = 0.02437930554151535 + 0.01 * 6.664895057678223
Epoch 490, val loss: 1.2433584928512573
Epoch 500, training loss: 0.08907942473888397 = 0.022508468478918076 + 0.01 * 6.657095432281494
Epoch 500, val loss: 1.2594043016433716
Epoch 510, training loss: 0.0873623788356781 = 0.02083529904484749 + 0.01 * 6.652707576751709
Epoch 510, val loss: 1.2749935388565063
Epoch 520, training loss: 0.08603745698928833 = 0.01933453232049942 + 0.01 * 6.670292854309082
Epoch 520, val loss: 1.2900443077087402
Epoch 530, training loss: 0.08440247178077698 = 0.017985031008720398 + 0.01 * 6.641744136810303
Epoch 530, val loss: 1.3047082424163818
Epoch 540, training loss: 0.08317970484495163 = 0.01676712930202484 + 0.01 * 6.6412577629089355
Epoch 540, val loss: 1.3189456462860107
Epoch 550, training loss: 0.08204558491706848 = 0.01566481962800026 + 0.01 * 6.6380767822265625
Epoch 550, val loss: 1.3328288793563843
Epoch 560, training loss: 0.08097770065069199 = 0.01466597430408001 + 0.01 * 6.6311726570129395
Epoch 560, val loss: 1.3463189601898193
Epoch 570, training loss: 0.08003372699022293 = 0.01375879067927599 + 0.01 * 6.627493858337402
Epoch 570, val loss: 1.3594493865966797
Epoch 580, training loss: 0.07917355746030807 = 0.012931975536048412 + 0.01 * 6.62415885925293
Epoch 580, val loss: 1.3721429109573364
Epoch 590, training loss: 0.0785234346985817 = 0.012176976539194584 + 0.01 * 6.634646415710449
Epoch 590, val loss: 1.3845747709274292
Epoch 600, training loss: 0.07769453525543213 = 0.011486741714179516 + 0.01 * 6.620779991149902
Epoch 600, val loss: 1.39664626121521
Epoch 610, training loss: 0.07703103870153427 = 0.01085478812456131 + 0.01 * 6.6176252365112305
Epoch 610, val loss: 1.4083752632141113
Epoch 620, training loss: 0.07641253620386124 = 0.010274702683091164 + 0.01 * 6.613783836364746
Epoch 620, val loss: 1.4197463989257812
Epoch 630, training loss: 0.07579874247312546 = 0.009741035290062428 + 0.01 * 6.605770587921143
Epoch 630, val loss: 1.4308242797851562
Epoch 640, training loss: 0.0754367783665657 = 0.009248620830476284 + 0.01 * 6.618815898895264
Epoch 640, val loss: 1.4416297674179077
Epoch 650, training loss: 0.0748264491558075 = 0.008794759400188923 + 0.01 * 6.603168487548828
Epoch 650, val loss: 1.452149510383606
Epoch 660, training loss: 0.07436394691467285 = 0.008374758064746857 + 0.01 * 6.59891939163208
Epoch 660, val loss: 1.4623234272003174
Epoch 670, training loss: 0.07397086173295975 = 0.007985763251781464 + 0.01 * 6.598509788513184
Epoch 670, val loss: 1.472150206565857
Epoch 680, training loss: 0.07350193709135056 = 0.007625436410307884 + 0.01 * 6.587650299072266
Epoch 680, val loss: 1.4818599224090576
Epoch 690, training loss: 0.07315780222415924 = 0.007290575187653303 + 0.01 * 6.586723327636719
Epoch 690, val loss: 1.491204023361206
Epoch 700, training loss: 0.07285313308238983 = 0.006978908088058233 + 0.01 * 6.587422847747803
Epoch 700, val loss: 1.5002974271774292
Epoch 710, training loss: 0.07252746820449829 = 0.006688433233648539 + 0.01 * 6.583903789520264
Epoch 710, val loss: 1.509161353111267
Epoch 720, training loss: 0.07222652435302734 = 0.0064171296544373035 + 0.01 * 6.580939292907715
Epoch 720, val loss: 1.5177783966064453
Epoch 730, training loss: 0.07198497653007507 = 0.006163554731756449 + 0.01 * 6.5821428298950195
Epoch 730, val loss: 1.526119589805603
Epoch 740, training loss: 0.07165461778640747 = 0.0059266043826937675 + 0.01 * 6.5728020668029785
Epoch 740, val loss: 1.5342941284179688
Epoch 750, training loss: 0.07140552997589111 = 0.005704141221940517 + 0.01 * 6.570138931274414
Epoch 750, val loss: 1.542250156402588
Epoch 760, training loss: 0.07115042209625244 = 0.0054953694343566895 + 0.01 * 6.565505504608154
Epoch 760, val loss: 1.5499365329742432
Epoch 770, training loss: 0.07108881324529648 = 0.005299101117998362 + 0.01 * 6.5789713859558105
Epoch 770, val loss: 1.5573610067367554
Epoch 780, training loss: 0.07070734351873398 = 0.0051143961027264595 + 0.01 * 6.559294700622559
Epoch 780, val loss: 1.5647729635238647
Epoch 790, training loss: 0.07077234983444214 = 0.004940457176417112 + 0.01 * 6.583189487457275
Epoch 790, val loss: 1.571855902671814
Epoch 800, training loss: 0.07043587416410446 = 0.004776477348059416 + 0.01 * 6.565939903259277
Epoch 800, val loss: 1.5788222551345825
Epoch 810, training loss: 0.07011856883764267 = 0.0046218871138989925 + 0.01 * 6.549668312072754
Epoch 810, val loss: 1.5855504274368286
Epoch 820, training loss: 0.06994814425706863 = 0.004475485999137163 + 0.01 * 6.547265529632568
Epoch 820, val loss: 1.5921608209609985
Epoch 830, training loss: 0.06980989128351212 = 0.0043369485065341 + 0.01 * 6.547294616699219
Epoch 830, val loss: 1.598595380783081
Epoch 840, training loss: 0.06964782625436783 = 0.004205747973173857 + 0.01 * 6.54420804977417
Epoch 840, val loss: 1.6049494743347168
Epoch 850, training loss: 0.06944257766008377 = 0.0040813288651406765 + 0.01 * 6.5361247062683105
Epoch 850, val loss: 1.6111087799072266
Epoch 860, training loss: 0.06939852982759476 = 0.003963196184486151 + 0.01 * 6.5435333251953125
Epoch 860, val loss: 1.6169782876968384
Epoch 870, training loss: 0.06924132257699966 = 0.0038512477185577154 + 0.01 * 6.539007186889648
Epoch 870, val loss: 1.6229666471481323
Epoch 880, training loss: 0.06906162202358246 = 0.0037446320056915283 + 0.01 * 6.531699180603027
Epoch 880, val loss: 1.6286215782165527
Epoch 890, training loss: 0.0689886212348938 = 0.0036431506741791964 + 0.01 * 6.534547328948975
Epoch 890, val loss: 1.6341204643249512
Epoch 900, training loss: 0.06885188817977905 = 0.0035465587861835957 + 0.01 * 6.530533313751221
Epoch 900, val loss: 1.6396734714508057
Epoch 910, training loss: 0.06881044805049896 = 0.003454487305134535 + 0.01 * 6.53559684753418
Epoch 910, val loss: 1.644887924194336
Epoch 920, training loss: 0.06865715980529785 = 0.003366585820913315 + 0.01 * 6.529057025909424
Epoch 920, val loss: 1.6501283645629883
Epoch 930, training loss: 0.06847935914993286 = 0.0032827770337462425 + 0.01 * 6.51965856552124
Epoch 930, val loss: 1.655238151550293
Epoch 940, training loss: 0.06852424144744873 = 0.003202559892088175 + 0.01 * 6.532168388366699
Epoch 940, val loss: 1.6601073741912842
Epoch 950, training loss: 0.06832172721624374 = 0.0031261735130101442 + 0.01 * 6.519555568695068
Epoch 950, val loss: 1.6649960279464722
Epoch 960, training loss: 0.06823389232158661 = 0.0030529433861374855 + 0.01 * 6.518095016479492
Epoch 960, val loss: 1.6697897911071777
Epoch 970, training loss: 0.06809373944997787 = 0.002983013866469264 + 0.01 * 6.511073112487793
Epoch 970, val loss: 1.6742085218429565
Epoch 980, training loss: 0.06800182908773422 = 0.002916062716394663 + 0.01 * 6.508577346801758
Epoch 980, val loss: 1.6788034439086914
Epoch 990, training loss: 0.06789078563451767 = 0.0028518212493509054 + 0.01 * 6.503896713256836
Epoch 990, val loss: 1.683363914489746
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7741
Overall ASR: 0.4686
Flip ASR: 0.3733/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.03950572013855 = 1.9557678699493408 + 0.01 * 8.373791694641113
Epoch 0, val loss: 1.9513640403747559
Epoch 10, training loss: 2.02838397026062 = 1.944647192955017 + 0.01 * 8.373672485351562
Epoch 10, val loss: 1.9401428699493408
Epoch 20, training loss: 2.014723539352417 = 1.9309906959533691 + 0.01 * 8.37329387664795
Epoch 20, val loss: 1.9258513450622559
Epoch 30, training loss: 1.995460033416748 = 1.911738634109497 + 0.01 * 8.372138023376465
Epoch 30, val loss: 1.9053059816360474
Epoch 40, training loss: 1.9669227600097656 = 1.8832576274871826 + 0.01 * 8.366509437561035
Epoch 40, val loss: 1.8750231266021729
Epoch 50, training loss: 1.9266669750213623 = 1.8433200120925903 + 0.01 * 8.334693908691406
Epoch 50, val loss: 1.8343089818954468
Epoch 60, training loss: 1.8796778917312622 = 1.7976999282836914 + 0.01 * 8.197792053222656
Epoch 60, val loss: 1.7923232316970825
Epoch 70, training loss: 1.8342615365982056 = 1.7565847635269165 + 0.01 * 7.767672538757324
Epoch 70, val loss: 1.7591382265090942
Epoch 80, training loss: 1.7807447910308838 = 1.7079347372055054 + 0.01 * 7.281000137329102
Epoch 80, val loss: 1.7188751697540283
Epoch 90, training loss: 1.7127196788787842 = 1.6416451930999756 + 0.01 * 7.107450485229492
Epoch 90, val loss: 1.662131905555725
Epoch 100, training loss: 1.6245478391647339 = 1.5539311170578003 + 0.01 * 7.061676025390625
Epoch 100, val loss: 1.5871962308883667
Epoch 110, training loss: 1.5184000730514526 = 1.448073387145996 + 0.01 * 7.032670497894287
Epoch 110, val loss: 1.4987659454345703
Epoch 120, training loss: 1.402428388595581 = 1.3323931694030762 + 0.01 * 7.0035176277160645
Epoch 120, val loss: 1.4028735160827637
Epoch 130, training loss: 1.2844617366790771 = 1.2145752906799316 + 0.01 * 6.988649368286133
Epoch 130, val loss: 1.3074846267700195
Epoch 140, training loss: 1.1698840856552124 = 1.1000926494598389 + 0.01 * 6.97913932800293
Epoch 140, val loss: 1.2156376838684082
Epoch 150, training loss: 1.0632528066635132 = 0.9935540556907654 + 0.01 * 6.969876766204834
Epoch 150, val loss: 1.1313098669052124
Epoch 160, training loss: 0.9681752920150757 = 0.8986097574234009 + 0.01 * 6.956556797027588
Epoch 160, val loss: 1.0583003759384155
Epoch 170, training loss: 0.8862711191177368 = 0.8168808221817017 + 0.01 * 6.939029693603516
Epoch 170, val loss: 0.9976779222488403
Epoch 180, training loss: 0.8174623847007751 = 0.7482697367668152 + 0.01 * 6.919266700744629
Epoch 180, val loss: 0.9495014548301697
Epoch 190, training loss: 0.7599966526031494 = 0.6909440159797668 + 0.01 * 6.905264854431152
Epoch 190, val loss: 0.9121865034103394
Epoch 200, training loss: 0.7107892632484436 = 0.6418883204460144 + 0.01 * 6.890094757080078
Epoch 200, val loss: 0.8835383653640747
Epoch 210, training loss: 0.6667724847793579 = 0.5979833006858826 + 0.01 * 6.8789167404174805
Epoch 210, val loss: 0.8607652187347412
Epoch 220, training loss: 0.6256190538406372 = 0.5569278001785278 + 0.01 * 6.8691229820251465
Epoch 220, val loss: 0.8420186638832092
Epoch 230, training loss: 0.5861861705780029 = 0.5175766348838806 + 0.01 * 6.860952854156494
Epoch 230, val loss: 0.8263869881629944
Epoch 240, training loss: 0.5481008291244507 = 0.4795927405357361 + 0.01 * 6.850808143615723
Epoch 240, val loss: 0.8132200837135315
Epoch 250, training loss: 0.5115354061126709 = 0.44309210777282715 + 0.01 * 6.844329357147217
Epoch 250, val loss: 0.8023074269294739
Epoch 260, training loss: 0.4767277240753174 = 0.4083890914916992 + 0.01 * 6.833861827850342
Epoch 260, val loss: 0.7936531901359558
Epoch 270, training loss: 0.4436662197113037 = 0.3754050135612488 + 0.01 * 6.826120376586914
Epoch 270, val loss: 0.7865233421325684
Epoch 280, training loss: 0.41207534074783325 = 0.3439200520515442 + 0.01 * 6.815530300140381
Epoch 280, val loss: 0.7800605297088623
Epoch 290, training loss: 0.3817487061023712 = 0.31367018818855286 + 0.01 * 6.8078508377075195
Epoch 290, val loss: 0.7742279171943665
Epoch 300, training loss: 0.3526099622249603 = 0.2846052050590515 + 0.01 * 6.800475597381592
Epoch 300, val loss: 0.7691526412963867
Epoch 310, training loss: 0.324604868888855 = 0.2566906213760376 + 0.01 * 6.791426658630371
Epoch 310, val loss: 0.765167772769928
Epoch 320, training loss: 0.29772818088531494 = 0.22986920177936554 + 0.01 * 6.785898685455322
Epoch 320, val loss: 0.7622995972633362
Epoch 330, training loss: 0.271992564201355 = 0.20424750447273254 + 0.01 * 6.77450704574585
Epoch 330, val loss: 0.7607035636901855
Epoch 340, training loss: 0.24772652983665466 = 0.18004216253757477 + 0.01 * 6.768435955047607
Epoch 340, val loss: 0.7602335810661316
Epoch 350, training loss: 0.22523924708366394 = 0.1576630026102066 + 0.01 * 6.757624626159668
Epoch 350, val loss: 0.7611409425735474
Epoch 360, training loss: 0.20501506328582764 = 0.13748469948768616 + 0.01 * 6.753035545349121
Epoch 360, val loss: 0.7636845707893372
Epoch 370, training loss: 0.18728235363960266 = 0.11978226155042648 + 0.01 * 6.750010013580322
Epoch 370, val loss: 0.7682186961174011
Epoch 380, training loss: 0.17195358872413635 = 0.1045144721865654 + 0.01 * 6.743911266326904
Epoch 380, val loss: 0.7747147083282471
Epoch 390, training loss: 0.15879952907562256 = 0.09149662405252457 + 0.01 * 6.730289936065674
Epoch 390, val loss: 0.7829773426055908
Epoch 400, training loss: 0.14768990874290466 = 0.08047229796648026 + 0.01 * 6.721762180328369
Epoch 400, val loss: 0.7927133440971375
Epoch 410, training loss: 0.13827893137931824 = 0.071143239736557 + 0.01 * 6.713569164276123
Epoch 410, val loss: 0.8036783933639526
Epoch 420, training loss: 0.1303648203611374 = 0.06321769207715988 + 0.01 * 6.714713096618652
Epoch 420, val loss: 0.8154706358909607
Epoch 430, training loss: 0.12348924577236176 = 0.05645030364394188 + 0.01 * 6.70389461517334
Epoch 430, val loss: 0.8277093768119812
Epoch 440, training loss: 0.1177331954240799 = 0.050649162381887436 + 0.01 * 6.708403587341309
Epoch 440, val loss: 0.8402131795883179
Epoch 450, training loss: 0.11260713636875153 = 0.045651473104953766 + 0.01 * 6.695566654205322
Epoch 450, val loss: 0.8526752591133118
Epoch 460, training loss: 0.10816851258277893 = 0.04131854698061943 + 0.01 * 6.684996604919434
Epoch 460, val loss: 0.8650971055030823
Epoch 470, training loss: 0.10448199510574341 = 0.03754347190260887 + 0.01 * 6.693851947784424
Epoch 470, val loss: 0.8774333000183105
Epoch 480, training loss: 0.10103647410869598 = 0.03424707055091858 + 0.01 * 6.678940296173096
Epoch 480, val loss: 0.8895745277404785
Epoch 490, training loss: 0.09805658459663391 = 0.031355444341897964 + 0.01 * 6.670114040374756
Epoch 490, val loss: 0.9015464186668396
Epoch 500, training loss: 0.09559227526187897 = 0.028802238404750824 + 0.01 * 6.679003715515137
Epoch 500, val loss: 0.9133353233337402
Epoch 510, training loss: 0.0931941419839859 = 0.026539672166109085 + 0.01 * 6.66544771194458
Epoch 510, val loss: 0.9247893691062927
Epoch 520, training loss: 0.09115263819694519 = 0.02452557161450386 + 0.01 * 6.6627068519592285
Epoch 520, val loss: 0.9360023140907288
Epoch 530, training loss: 0.08937550336122513 = 0.022726772353053093 + 0.01 * 6.664873123168945
Epoch 530, val loss: 0.946942925453186
Epoch 540, training loss: 0.08765764534473419 = 0.021115539595484734 + 0.01 * 6.654210567474365
Epoch 540, val loss: 0.9576915502548218
Epoch 550, training loss: 0.08615700155496597 = 0.01966695487499237 + 0.01 * 6.6490044593811035
Epoch 550, val loss: 0.9681580662727356
Epoch 560, training loss: 0.08474110811948776 = 0.018360326066613197 + 0.01 * 6.638078689575195
Epoch 560, val loss: 0.9784144759178162
Epoch 570, training loss: 0.08360083401203156 = 0.01717895083129406 + 0.01 * 6.64218807220459
Epoch 570, val loss: 0.9884310364723206
Epoch 580, training loss: 0.08241458237171173 = 0.016107510775327682 + 0.01 * 6.630706787109375
Epoch 580, val loss: 0.9982647895812988
Epoch 590, training loss: 0.08144945651292801 = 0.015133997425436974 + 0.01 * 6.631545543670654
Epoch 590, val loss: 1.0078859329223633
Epoch 600, training loss: 0.0805932879447937 = 0.014247496612370014 + 0.01 * 6.634578704833984
Epoch 600, val loss: 1.0171841382980347
Epoch 610, training loss: 0.07961991429328918 = 0.013439154252409935 + 0.01 * 6.618075847625732
Epoch 610, val loss: 1.0262519121170044
Epoch 620, training loss: 0.07900278270244598 = 0.012698776088654995 + 0.01 * 6.630400657653809
Epoch 620, val loss: 1.0350918769836426
Epoch 630, training loss: 0.07821032404899597 = 0.012019891291856766 + 0.01 * 6.619043827056885
Epoch 630, val loss: 1.0437979698181152
Epoch 640, training loss: 0.07760578393936157 = 0.011396040208637714 + 0.01 * 6.620974063873291
Epoch 640, val loss: 1.052144169807434
Epoch 650, training loss: 0.07690320909023285 = 0.010821701027452946 + 0.01 * 6.608150959014893
Epoch 650, val loss: 1.0604188442230225
Epoch 660, training loss: 0.07631707936525345 = 0.010291136801242828 + 0.01 * 6.602594375610352
Epoch 660, val loss: 1.0683335065841675
Epoch 670, training loss: 0.07581965625286102 = 0.009800750762224197 + 0.01 * 6.601890563964844
Epoch 670, val loss: 1.076155662536621
Epoch 680, training loss: 0.0753236636519432 = 0.009346161969006062 + 0.01 * 6.597749710083008
Epoch 680, val loss: 1.0837533473968506
Epoch 690, training loss: 0.07489900290966034 = 0.008924107998609543 + 0.01 * 6.597490310668945
Epoch 690, val loss: 1.0911604166030884
Epoch 700, training loss: 0.07447098195552826 = 0.0085318423807621 + 0.01 * 6.59391450881958
Epoch 700, val loss: 1.098403811454773
Epoch 710, training loss: 0.0739990621805191 = 0.008166586980223656 + 0.01 * 6.583247661590576
Epoch 710, val loss: 1.1054317951202393
Epoch 720, training loss: 0.07371898740530014 = 0.00782592874020338 + 0.01 * 6.589306354522705
Epoch 720, val loss: 1.112335443496704
Epoch 730, training loss: 0.07330621778964996 = 0.007508073467761278 + 0.01 * 6.579814434051514
Epoch 730, val loss: 1.1190544366836548
Epoch 740, training loss: 0.07305682450532913 = 0.0072105517610907555 + 0.01 * 6.584627151489258
Epoch 740, val loss: 1.125545859336853
Epoch 750, training loss: 0.07261644303798676 = 0.006931924261152744 + 0.01 * 6.56845235824585
Epoch 750, val loss: 1.1320083141326904
Epoch 760, training loss: 0.07239638268947601 = 0.006670351140201092 + 0.01 * 6.572603225708008
Epoch 760, val loss: 1.1382737159729004
Epoch 770, training loss: 0.07223184406757355 = 0.006424869876354933 + 0.01 * 6.580698013305664
Epoch 770, val loss: 1.1443415880203247
Epoch 780, training loss: 0.07175306230783463 = 0.0061943624168634415 + 0.01 * 6.555870532989502
Epoch 780, val loss: 1.1503270864486694
Epoch 790, training loss: 0.07164419442415237 = 0.0059769959188997746 + 0.01 * 6.566720008850098
Epoch 790, val loss: 1.1562108993530273
Epoch 800, training loss: 0.07130326330661774 = 0.005772303324192762 + 0.01 * 6.553096294403076
Epoch 800, val loss: 1.1618529558181763
Epoch 810, training loss: 0.0711604654788971 = 0.0055792867206037045 + 0.01 * 6.558117866516113
Epoch 810, val loss: 1.1673985719680786
Epoch 820, training loss: 0.07082371413707733 = 0.005396634805947542 + 0.01 * 6.542708396911621
Epoch 820, val loss: 1.1728954315185547
Epoch 830, training loss: 0.07077896595001221 = 0.0052237603813409805 + 0.01 * 6.555520534515381
Epoch 830, val loss: 1.178236484527588
Epoch 840, training loss: 0.07046064734458923 = 0.005060425028204918 + 0.01 * 6.540022373199463
Epoch 840, val loss: 1.1835062503814697
Epoch 850, training loss: 0.07030823081731796 = 0.004905493464320898 + 0.01 * 6.540273666381836
Epoch 850, val loss: 1.1885948181152344
Epoch 860, training loss: 0.0702051892876625 = 0.004758589901030064 + 0.01 * 6.5446600914001465
Epoch 860, val loss: 1.1935646533966064
Epoch 870, training loss: 0.0700279101729393 = 0.004619573708623648 + 0.01 * 6.540833473205566
Epoch 870, val loss: 1.1985093355178833
Epoch 880, training loss: 0.06972172111272812 = 0.004487250931560993 + 0.01 * 6.523447036743164
Epoch 880, val loss: 1.2032995223999023
Epoch 890, training loss: 0.06967245042324066 = 0.004361315164715052 + 0.01 * 6.531113624572754
Epoch 890, val loss: 1.2079315185546875
Epoch 900, training loss: 0.0694614052772522 = 0.004241820424795151 + 0.01 * 6.52195930480957
Epoch 900, val loss: 1.2125517129898071
Epoch 910, training loss: 0.0693512037396431 = 0.004127866122871637 + 0.01 * 6.522334098815918
Epoch 910, val loss: 1.2170594930648804
Epoch 920, training loss: 0.06921667605638504 = 0.004019363783299923 + 0.01 * 6.519731044769287
Epoch 920, val loss: 1.2214711904525757
Epoch 930, training loss: 0.06899630278348923 = 0.0039160228334367275 + 0.01 * 6.50802755355835
Epoch 930, val loss: 1.22577702999115
Epoch 940, training loss: 0.06891269981861115 = 0.003817197633907199 + 0.01 * 6.509550094604492
Epoch 940, val loss: 1.2299524545669556
Epoch 950, training loss: 0.06878123432397842 = 0.0037227917928248644 + 0.01 * 6.5058441162109375
Epoch 950, val loss: 1.2341076135635376
Epoch 960, training loss: 0.06872576475143433 = 0.003632541513070464 + 0.01 * 6.509322643280029
Epoch 960, val loss: 1.2381596565246582
Epoch 970, training loss: 0.06871359795331955 = 0.0035462602972984314 + 0.01 * 6.516733646392822
Epoch 970, val loss: 1.242053747177124
Epoch 980, training loss: 0.06850387901067734 = 0.003463790286332369 + 0.01 * 6.504008769989014
Epoch 980, val loss: 1.245989203453064
Epoch 990, training loss: 0.06876948475837708 = 0.0033846942242234945 + 0.01 * 6.538479328155518
Epoch 990, val loss: 1.249774694442749
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.4502
Flip ASR: 0.4044/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0382721424102783 = 1.9545339345932007 + 0.01 * 8.37381362915039
Epoch 0, val loss: 1.9543617963790894
Epoch 10, training loss: 2.0279555320739746 = 1.944218397140503 + 0.01 * 8.373705863952637
Epoch 10, val loss: 1.9443492889404297
Epoch 20, training loss: 2.0154025554656982 = 1.9316692352294922 + 0.01 * 8.373327255249023
Epoch 20, val loss: 1.9314703941345215
Epoch 30, training loss: 1.997948408126831 = 1.9142282009124756 + 0.01 * 8.37201976776123
Epoch 30, val loss: 1.9130825996398926
Epoch 40, training loss: 1.9721752405166626 = 1.8885408639907837 + 0.01 * 8.363439559936523
Epoch 40, val loss: 1.885955572128296
Epoch 50, training loss: 1.934704303741455 = 1.8516671657562256 + 0.01 * 8.303715705871582
Epoch 50, val loss: 1.8482948541641235
Epoch 60, training loss: 1.8874887228012085 = 1.806316614151001 + 0.01 * 8.117208480834961
Epoch 60, val loss: 1.8057647943496704
Epoch 70, training loss: 1.839112401008606 = 1.7629554271697998 + 0.01 * 7.615692138671875
Epoch 70, val loss: 1.7695409059524536
Epoch 80, training loss: 1.7874709367752075 = 1.7156949043273926 + 0.01 * 7.1776041984558105
Epoch 80, val loss: 1.729619026184082
Epoch 90, training loss: 1.7216665744781494 = 1.6514897346496582 + 0.01 * 7.017679691314697
Epoch 90, val loss: 1.674920678138733
Epoch 100, training loss: 1.6375412940979004 = 1.5678805112838745 + 0.01 * 6.966073989868164
Epoch 100, val loss: 1.6049368381500244
Epoch 110, training loss: 1.5385725498199463 = 1.4692332744598389 + 0.01 * 6.93392276763916
Epoch 110, val loss: 1.5244160890579224
Epoch 120, training loss: 1.4348140954971313 = 1.3656655550003052 + 0.01 * 6.914849281311035
Epoch 120, val loss: 1.4420766830444336
Epoch 130, training loss: 1.331627368927002 = 1.2626595497131348 + 0.01 * 6.896786689758301
Epoch 130, val loss: 1.363613486289978
Epoch 140, training loss: 1.2312405109405518 = 1.1624274253845215 + 0.01 * 6.881303787231445
Epoch 140, val loss: 1.290029525756836
Epoch 150, training loss: 1.1356382369995117 = 1.06691575050354 + 0.01 * 6.872247219085693
Epoch 150, val loss: 1.2207521200180054
Epoch 160, training loss: 1.0464746952056885 = 0.9777444005012512 + 0.01 * 6.873025417327881
Epoch 160, val loss: 1.1550809144973755
Epoch 170, training loss: 0.9630596041679382 = 0.8943372964859009 + 0.01 * 6.872232913970947
Epoch 170, val loss: 1.0918552875518799
Epoch 180, training loss: 0.8831921815872192 = 0.814473032951355 + 0.01 * 6.871918201446533
Epoch 180, val loss: 1.029620885848999
Epoch 190, training loss: 0.8049495220184326 = 0.7362309098243713 + 0.01 * 6.871862411499023
Epoch 190, val loss: 0.9679309725761414
Epoch 200, training loss: 0.7279207706451416 = 0.6592137813568115 + 0.01 * 6.870702266693115
Epoch 200, val loss: 0.9086698293685913
Epoch 210, training loss: 0.6533825993537903 = 0.5846561789512634 + 0.01 * 6.872640132904053
Epoch 210, val loss: 0.8544992804527283
Epoch 220, training loss: 0.5830183029174805 = 0.514339029788971 + 0.01 * 6.867928981781006
Epoch 220, val loss: 0.8080629706382751
Epoch 230, training loss: 0.518360435962677 = 0.44970497488975525 + 0.01 * 6.865546703338623
Epoch 230, val loss: 0.7706634998321533
Epoch 240, training loss: 0.46011775732040405 = 0.39149385690689087 + 0.01 * 6.862391471862793
Epoch 240, val loss: 0.7420032620429993
Epoch 250, training loss: 0.40845978260040283 = 0.3398609459400177 + 0.01 * 6.8598833084106445
Epoch 250, val loss: 0.7210798859596252
Epoch 260, training loss: 0.36318856477737427 = 0.2946120500564575 + 0.01 * 6.857651710510254
Epoch 260, val loss: 0.7068008780479431
Epoch 270, training loss: 0.32379117608070374 = 0.2552523910999298 + 0.01 * 6.853878021240234
Epoch 270, val loss: 0.6987117528915405
Epoch 280, training loss: 0.2896850109100342 = 0.22118456661701202 + 0.01 * 6.850042819976807
Epoch 280, val loss: 0.6959877610206604
Epoch 290, training loss: 0.26028749346733093 = 0.19181397557258606 + 0.01 * 6.847352981567383
Epoch 290, val loss: 0.6977099180221558
Epoch 300, training loss: 0.23503512144088745 = 0.16660283505916595 + 0.01 * 6.8432297706604
Epoch 300, val loss: 0.7030529975891113
Epoch 310, training loss: 0.21341541409492493 = 0.14502625167369843 + 0.01 * 6.838916301727295
Epoch 310, val loss: 0.7113247513771057
Epoch 320, training loss: 0.1949813961982727 = 0.1266103833913803 + 0.01 * 6.837101936340332
Epoch 320, val loss: 0.7218978404998779
Epoch 330, training loss: 0.17925477027893066 = 0.11091785877943039 + 0.01 * 6.8336920738220215
Epoch 330, val loss: 0.734255850315094
Epoch 340, training loss: 0.1657606065273285 = 0.09753607213497162 + 0.01 * 6.822452545166016
Epoch 340, val loss: 0.7480226755142212
Epoch 350, training loss: 0.15428410470485687 = 0.08610538393259048 + 0.01 * 6.817872524261475
Epoch 350, val loss: 0.7626957297325134
Epoch 360, training loss: 0.14471793174743652 = 0.0763218030333519 + 0.01 * 6.839613914489746
Epoch 360, val loss: 0.777992308139801
Epoch 370, training loss: 0.13604989647865295 = 0.06792206317186356 + 0.01 * 6.8127827644348145
Epoch 370, val loss: 0.793830156326294
Epoch 380, training loss: 0.12865179777145386 = 0.06067817285656929 + 0.01 * 6.797362327575684
Epoch 380, val loss: 0.8099182844161987
Epoch 390, training loss: 0.12247661501169205 = 0.05440741777420044 + 0.01 * 6.806920051574707
Epoch 390, val loss: 0.8261131048202515
Epoch 400, training loss: 0.11682294309139252 = 0.04896353930234909 + 0.01 * 6.785940647125244
Epoch 400, val loss: 0.8423229455947876
Epoch 410, training loss: 0.11198698729276657 = 0.044212400913238525 + 0.01 * 6.777459144592285
Epoch 410, val loss: 0.8584538102149963
Epoch 420, training loss: 0.10781671106815338 = 0.0400531105697155 + 0.01 * 6.776360034942627
Epoch 420, val loss: 0.8744694590568542
Epoch 430, training loss: 0.10414955019950867 = 0.036403462290763855 + 0.01 * 6.774608612060547
Epoch 430, val loss: 0.8903593420982361
Epoch 440, training loss: 0.10083100199699402 = 0.03318832442164421 + 0.01 * 6.764267444610596
Epoch 440, val loss: 0.905989408493042
Epoch 450, training loss: 0.09791064262390137 = 0.030349941924214363 + 0.01 * 6.756070137023926
Epoch 450, val loss: 0.9213922619819641
Epoch 460, training loss: 0.09540802985429764 = 0.027838848531246185 + 0.01 * 6.756918430328369
Epoch 460, val loss: 0.9364569783210754
Epoch 470, training loss: 0.09308886528015137 = 0.025610707700252533 + 0.01 * 6.74781608581543
Epoch 470, val loss: 0.951195478439331
Epoch 480, training loss: 0.09125469624996185 = 0.02362835966050625 + 0.01 * 6.762633323669434
Epoch 480, val loss: 0.965530514717102
Epoch 490, training loss: 0.08924393355846405 = 0.021863218396902084 + 0.01 * 6.738071918487549
Epoch 490, val loss: 0.979408323764801
Epoch 500, training loss: 0.08761917054653168 = 0.020285826176404953 + 0.01 * 6.733335018157959
Epoch 500, val loss: 0.9928569793701172
Epoch 510, training loss: 0.08616378158330917 = 0.018871061503887177 + 0.01 * 6.729272365570068
Epoch 510, val loss: 1.005967378616333
Epoch 520, training loss: 0.08481169492006302 = 0.017597921192646027 + 0.01 * 6.721377849578857
Epoch 520, val loss: 1.018717646598816
Epoch 530, training loss: 0.08363398909568787 = 0.016450276598334312 + 0.01 * 6.7183709144592285
Epoch 530, val loss: 1.0310122966766357
Epoch 540, training loss: 0.0825953483581543 = 0.015414114110171795 + 0.01 * 6.718123912811279
Epoch 540, val loss: 1.0429339408874512
Epoch 550, training loss: 0.08187787234783173 = 0.01447458378970623 + 0.01 * 6.740328788757324
Epoch 550, val loss: 1.0544826984405518
Epoch 560, training loss: 0.08063105493783951 = 0.013621525838971138 + 0.01 * 6.700953483581543
Epoch 560, val loss: 1.065713882446289
Epoch 570, training loss: 0.07983077317476273 = 0.012844241224229336 + 0.01 * 6.698653221130371
Epoch 570, val loss: 1.076590657234192
Epoch 580, training loss: 0.07908979803323746 = 0.012133914045989513 + 0.01 * 6.6955885887146
Epoch 580, val loss: 1.087101936340332
Epoch 590, training loss: 0.07832448184490204 = 0.011483344249427319 + 0.01 * 6.6841139793396
Epoch 590, val loss: 1.097345232963562
Epoch 600, training loss: 0.07787850499153137 = 0.010886311531066895 + 0.01 * 6.699219703674316
Epoch 600, val loss: 1.1072254180908203
Epoch 610, training loss: 0.07699993997812271 = 0.010338300839066505 + 0.01 * 6.666164398193359
Epoch 610, val loss: 1.1168464422225952
Epoch 620, training loss: 0.07647287845611572 = 0.009832946583628654 + 0.01 * 6.6639933586120605
Epoch 620, val loss: 1.1261205673217773
Epoch 630, training loss: 0.07609985768795013 = 0.009366677142679691 + 0.01 * 6.673318386077881
Epoch 630, val loss: 1.1351779699325562
Epoch 640, training loss: 0.07536756992340088 = 0.00893544964492321 + 0.01 * 6.64321231842041
Epoch 640, val loss: 1.1440058946609497
Epoch 650, training loss: 0.07513794302940369 = 0.008535618893802166 + 0.01 * 6.660233020782471
Epoch 650, val loss: 1.1525018215179443
Epoch 660, training loss: 0.07447454333305359 = 0.008164716884493828 + 0.01 * 6.630982875823975
Epoch 660, val loss: 1.1608041524887085
Epoch 670, training loss: 0.07432211935520172 = 0.007819795049726963 + 0.01 * 6.650232791900635
Epoch 670, val loss: 1.1688857078552246
Epoch 680, training loss: 0.07368535548448563 = 0.00749870203435421 + 0.01 * 6.6186652183532715
Epoch 680, val loss: 1.176659107208252
Epoch 690, training loss: 0.07345356047153473 = 0.00719914585351944 + 0.01 * 6.625441551208496
Epoch 690, val loss: 1.1842786073684692
Epoch 700, training loss: 0.07315663248300552 = 0.006919234059751034 + 0.01 * 6.623739719390869
Epoch 700, val loss: 1.1917123794555664
Epoch 710, training loss: 0.07292212545871735 = 0.006657526828348637 + 0.01 * 6.626460075378418
Epoch 710, val loss: 1.1988524198532104
Epoch 720, training loss: 0.07244569808244705 = 0.006412317510694265 + 0.01 * 6.60333776473999
Epoch 720, val loss: 1.2059361934661865
Epoch 730, training loss: 0.0722012147307396 = 0.006182019133120775 + 0.01 * 6.601919651031494
Epoch 730, val loss: 1.212665319442749
Epoch 740, training loss: 0.07183922827243805 = 0.00596512109041214 + 0.01 * 6.587410926818848
Epoch 740, val loss: 1.21937894821167
Epoch 750, training loss: 0.07198113948106766 = 0.0057610152289271355 + 0.01 * 6.622012615203857
Epoch 750, val loss: 1.2258728742599487
Epoch 760, training loss: 0.07129182666540146 = 0.005568851716816425 + 0.01 * 6.572297096252441
Epoch 760, val loss: 1.232122540473938
Epoch 770, training loss: 0.07107636332511902 = 0.005387447774410248 + 0.01 * 6.568891525268555
Epoch 770, val loss: 1.238342046737671
Epoch 780, training loss: 0.07092402875423431 = 0.005216158460825682 + 0.01 * 6.57078742980957
Epoch 780, val loss: 1.2443468570709229
Epoch 790, training loss: 0.07060428708791733 = 0.0050542885437607765 + 0.01 * 6.555000305175781
Epoch 790, val loss: 1.2502672672271729
Epoch 800, training loss: 0.07037826627492905 = 0.004900817293673754 + 0.01 * 6.5477447509765625
Epoch 800, val loss: 1.256029486656189
Epoch 810, training loss: 0.07026299089193344 = 0.004755509551614523 + 0.01 * 6.550747871398926
Epoch 810, val loss: 1.2616913318634033
Epoch 820, training loss: 0.07017771154642105 = 0.0046174111776053905 + 0.01 * 6.5560302734375
Epoch 820, val loss: 1.2671682834625244
Epoch 830, training loss: 0.070174440741539 = 0.004486353136599064 + 0.01 * 6.5688090324401855
Epoch 830, val loss: 1.2725982666015625
Epoch 840, training loss: 0.06971677392721176 = 0.0043620760552585125 + 0.01 * 6.5354695320129395
Epoch 840, val loss: 1.2777730226516724
Epoch 850, training loss: 0.06949853152036667 = 0.004243732430040836 + 0.01 * 6.525479793548584
Epoch 850, val loss: 1.282892107963562
Epoch 860, training loss: 0.0692950040102005 = 0.004130995366722345 + 0.01 * 6.5164008140563965
Epoch 860, val loss: 1.2879630327224731
Epoch 870, training loss: 0.06927480548620224 = 0.004023710265755653 + 0.01 * 6.525109767913818
Epoch 870, val loss: 1.2928999662399292
Epoch 880, training loss: 0.06921523064374924 = 0.003921321127563715 + 0.01 * 6.529391288757324
Epoch 880, val loss: 1.2976157665252686
Epoch 890, training loss: 0.06896207481622696 = 0.0038236777763813734 + 0.01 * 6.513840198516846
Epoch 890, val loss: 1.3023532629013062
Epoch 900, training loss: 0.06895429641008377 = 0.003730359487235546 + 0.01 * 6.522394180297852
Epoch 900, val loss: 1.306911826133728
Epoch 910, training loss: 0.06871899962425232 = 0.0036412396002560854 + 0.01 * 6.507775783538818
Epoch 910, val loss: 1.3114670515060425
Epoch 920, training loss: 0.06859925389289856 = 0.003555894596502185 + 0.01 * 6.504336357116699
Epoch 920, val loss: 1.3158363103866577
Epoch 930, training loss: 0.06828078627586365 = 0.003474298631772399 + 0.01 * 6.480648994445801
Epoch 930, val loss: 1.3202096223831177
Epoch 940, training loss: 0.06837049871683121 = 0.003395935520529747 + 0.01 * 6.497456073760986
Epoch 940, val loss: 1.3244987726211548
Epoch 950, training loss: 0.06814909726381302 = 0.003321042750030756 + 0.01 * 6.4828057289123535
Epoch 950, val loss: 1.3286031484603882
Epoch 960, training loss: 0.06821254640817642 = 0.003249058034271002 + 0.01 * 6.496349334716797
Epoch 960, val loss: 1.3326725959777832
Epoch 970, training loss: 0.06793064624071121 = 0.0031801152508705854 + 0.01 * 6.475053310394287
Epoch 970, val loss: 1.3366944789886475
Epoch 980, training loss: 0.06786295771598816 = 0.0031138185877352953 + 0.01 * 6.474913597106934
Epoch 980, val loss: 1.3406511545181274
Epoch 990, training loss: 0.06783562898635864 = 0.003050025785341859 + 0.01 * 6.478559970855713
Epoch 990, val loss: 1.3444132804870605
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.7491
Flip ASR: 0.7200/225 nodes
The final ASR:0.55597, 0.13676, Accuracy:0.80494, 0.02229
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11618])
remove edge: torch.Size([2, 9626])
updated graph: torch.Size([2, 10688])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98278, 0.00460, Accuracy:0.83333, 0.00800
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0179507732391357 = 1.9342119693756104 + 0.01 * 8.373882293701172
Epoch 0, val loss: 1.9305462837219238
Epoch 10, training loss: 2.0080692768096924 = 1.9243309497833252 + 0.01 * 8.373835563659668
Epoch 10, val loss: 1.9213064908981323
Epoch 20, training loss: 1.9957232475280762 = 1.9119868278503418 + 0.01 * 8.373639106750488
Epoch 20, val loss: 1.9094581604003906
Epoch 30, training loss: 1.9783084392547607 = 1.894577980041504 + 0.01 * 8.373045921325684
Epoch 30, val loss: 1.8925291299819946
Epoch 40, training loss: 1.952776312828064 = 1.8690742254257202 + 0.01 * 8.370205879211426
Epoch 40, val loss: 1.8680394887924194
Epoch 50, training loss: 1.9175266027450562 = 1.834030270576477 + 0.01 * 8.349638938903809
Epoch 50, val loss: 1.8363897800445557
Epoch 60, training loss: 1.8769530057907104 = 1.7946583032608032 + 0.01 * 8.22946834564209
Epoch 60, val loss: 1.8048381805419922
Epoch 70, training loss: 1.8334356546401978 = 1.7545089721679688 + 0.01 * 7.892667293548584
Epoch 70, val loss: 1.7737053632736206
Epoch 80, training loss: 1.7782446146011353 = 1.7007755041122437 + 0.01 * 7.746906757354736
Epoch 80, val loss: 1.7278366088867188
Epoch 90, training loss: 1.7028100490570068 = 1.6268887519836426 + 0.01 * 7.592125415802002
Epoch 90, val loss: 1.6643537282943726
Epoch 100, training loss: 1.6089863777160645 = 1.5348776578903198 + 0.01 * 7.410874366760254
Epoch 100, val loss: 1.5882242918014526
Epoch 110, training loss: 1.5081530809402466 = 1.436491847038269 + 0.01 * 7.166121959686279
Epoch 110, val loss: 1.5096925497055054
Epoch 120, training loss: 1.4104305505752563 = 1.3396897315979004 + 0.01 * 7.07407808303833
Epoch 120, val loss: 1.4367657899856567
Epoch 130, training loss: 1.3156864643096924 = 1.2453747987747192 + 0.01 * 7.031161308288574
Epoch 130, val loss: 1.3699461221694946
Epoch 140, training loss: 1.224947214126587 = 1.1548429727554321 + 0.01 * 7.010426044464111
Epoch 140, val loss: 1.3094478845596313
Epoch 150, training loss: 1.139752745628357 = 1.069847822189331 + 0.01 * 6.9904913902282715
Epoch 150, val loss: 1.2543684244155884
Epoch 160, training loss: 1.0605700016021729 = 0.9908641576766968 + 0.01 * 6.970582962036133
Epoch 160, val loss: 1.203963041305542
Epoch 170, training loss: 0.9861505627632141 = 0.9166203141212463 + 0.01 * 6.953026294708252
Epoch 170, val loss: 1.1565053462982178
Epoch 180, training loss: 0.9139962196350098 = 0.8446186184883118 + 0.01 * 6.937762260437012
Epoch 180, val loss: 1.1094156503677368
Epoch 190, training loss: 0.8415994048118591 = 0.7723426222801208 + 0.01 * 6.925677299499512
Epoch 190, val loss: 1.060863733291626
Epoch 200, training loss: 0.7679815292358398 = 0.6988028883934021 + 0.01 * 6.917863845825195
Epoch 200, val loss: 1.0102407932281494
Epoch 210, training loss: 0.694360613822937 = 0.6252239346504211 + 0.01 * 6.913664817810059
Epoch 210, val loss: 0.9588789343833923
Epoch 220, training loss: 0.6232783794403076 = 0.5541584491729736 + 0.01 * 6.911993980407715
Epoch 220, val loss: 0.9093406200408936
Epoch 230, training loss: 0.556981086730957 = 0.48786383867263794 + 0.01 * 6.9117279052734375
Epoch 230, val loss: 0.86456298828125
Epoch 240, training loss: 0.4964044392108917 = 0.42728444933891296 + 0.01 * 6.9120001792907715
Epoch 240, val loss: 0.8258547782897949
Epoch 250, training loss: 0.44163885712623596 = 0.3725142478942871 + 0.01 * 6.912460803985596
Epoch 250, val loss: 0.7934870719909668
Epoch 260, training loss: 0.39236223697662354 = 0.323233038187027 + 0.01 * 6.912919521331787
Epoch 260, val loss: 0.7669299244880676
Epoch 270, training loss: 0.3482774794101715 = 0.27914541959762573 + 0.01 * 6.913206100463867
Epoch 270, val loss: 0.7454760670661926
Epoch 280, training loss: 0.3091488182544708 = 0.24001683294773102 + 0.01 * 6.913198947906494
Epoch 280, val loss: 0.7283405065536499
Epoch 290, training loss: 0.2748453915119171 = 0.20571722090244293 + 0.01 * 6.912816524505615
Epoch 290, val loss: 0.7151866555213928
Epoch 300, training loss: 0.24524912238121033 = 0.17612887918949127 + 0.01 * 6.912024021148682
Epoch 300, val loss: 0.7058830261230469
Epoch 310, training loss: 0.22012628614902496 = 0.15101847052574158 + 0.01 * 6.910781383514404
Epoch 310, val loss: 0.7004760503768921
Epoch 320, training loss: 0.1990806758403778 = 0.12998197972774506 + 0.01 * 6.9098687171936035
Epoch 320, val loss: 0.6987311840057373
Epoch 330, training loss: 0.1815614104270935 = 0.1124827191233635 + 0.01 * 6.907868385314941
Epoch 330, val loss: 0.7002848982810974
Epoch 340, training loss: 0.16699576377868652 = 0.09794440865516663 + 0.01 * 6.9051361083984375
Epoch 340, val loss: 0.7047123908996582
Epoch 350, training loss: 0.15483924746513367 = 0.08582302182912827 + 0.01 * 6.9016218185424805
Epoch 350, val loss: 0.7114986777305603
Epoch 360, training loss: 0.14463824033737183 = 0.07564923167228699 + 0.01 * 6.898901462554932
Epoch 360, val loss: 0.7201057076454163
Epoch 370, training loss: 0.13597598671913147 = 0.06704793870449066 + 0.01 * 6.892806053161621
Epoch 370, val loss: 0.7299621105194092
Epoch 380, training loss: 0.12857836484909058 = 0.059726301580667496 + 0.01 * 6.885206699371338
Epoch 380, val loss: 0.7406635284423828
Epoch 390, training loss: 0.12226231396198273 = 0.053457994014024734 + 0.01 * 6.880432605743408
Epoch 390, val loss: 0.7518752217292786
Epoch 400, training loss: 0.1167801171541214 = 0.04806241765618324 + 0.01 * 6.871769428253174
Epoch 400, val loss: 0.763375461101532
Epoch 410, training loss: 0.11200793087482452 = 0.043393176048994064 + 0.01 * 6.861474990844727
Epoch 410, val loss: 0.7749744653701782
Epoch 420, training loss: 0.10791245847940445 = 0.03933172672986984 + 0.01 * 6.8580732345581055
Epoch 420, val loss: 0.7865283489227295
Epoch 430, training loss: 0.10423088073730469 = 0.035782311111688614 + 0.01 * 6.844857692718506
Epoch 430, val loss: 0.7980276942253113
Epoch 440, training loss: 0.10104586184024811 = 0.03266505151987076 + 0.01 * 6.838081359863281
Epoch 440, val loss: 0.8094025254249573
Epoch 450, training loss: 0.09828846156597137 = 0.029917849227786064 + 0.01 * 6.837061405181885
Epoch 450, val loss: 0.8205916881561279
Epoch 460, training loss: 0.09574338048696518 = 0.02748791128396988 + 0.01 * 6.825547218322754
Epoch 460, val loss: 0.8315817713737488
Epoch 470, training loss: 0.09354391694068909 = 0.025332005694508553 + 0.01 * 6.821191787719727
Epoch 470, val loss: 0.8423668146133423
Epoch 480, training loss: 0.09152692556381226 = 0.02341003343462944 + 0.01 * 6.811689853668213
Epoch 480, val loss: 0.8530333638191223
Epoch 490, training loss: 0.08975619822740555 = 0.021691784262657166 + 0.01 * 6.806441307067871
Epoch 490, val loss: 0.8634142875671387
Epoch 500, training loss: 0.08816776424646378 = 0.020151376724243164 + 0.01 * 6.801639080047607
Epoch 500, val loss: 0.8736017346382141
Epoch 510, training loss: 0.08672963082790375 = 0.018766019493341446 + 0.01 * 6.796360969543457
Epoch 510, val loss: 0.8835926651954651
Epoch 520, training loss: 0.08542125672101974 = 0.01751682162284851 + 0.01 * 6.7904438972473145
Epoch 520, val loss: 0.8933536410331726
Epoch 530, training loss: 0.08420560508966446 = 0.016387030482292175 + 0.01 * 6.781857967376709
Epoch 530, val loss: 0.9028623104095459
Epoch 540, training loss: 0.08308231830596924 = 0.015362950973212719 + 0.01 * 6.771937370300293
Epoch 540, val loss: 0.9121986627578735
Epoch 550, training loss: 0.0822163075208664 = 0.014432549476623535 + 0.01 * 6.778375625610352
Epoch 550, val loss: 0.921257495880127
Epoch 560, training loss: 0.08120612800121307 = 0.013585800305008888 + 0.01 * 6.762033462524414
Epoch 560, val loss: 0.9301115870475769
Epoch 570, training loss: 0.08033467829227448 = 0.01281255204230547 + 0.01 * 6.7522125244140625
Epoch 570, val loss: 0.9388478994369507
Epoch 580, training loss: 0.07970157265663147 = 0.012104648165404797 + 0.01 * 6.759692668914795
Epoch 580, val loss: 0.9473823308944702
Epoch 590, training loss: 0.07892204821109772 = 0.011455840431153774 + 0.01 * 6.7466206550598145
Epoch 590, val loss: 0.9557173848152161
Epoch 600, training loss: 0.07820254564285278 = 0.010859262198209763 + 0.01 * 6.734327793121338
Epoch 600, val loss: 0.9638105630874634
Epoch 610, training loss: 0.0775475725531578 = 0.010309991426765919 + 0.01 * 6.723757743835449
Epoch 610, val loss: 0.9717497825622559
Epoch 620, training loss: 0.0773586556315422 = 0.009803267195820808 + 0.01 * 6.755539417266846
Epoch 620, val loss: 0.9794785380363464
Epoch 630, training loss: 0.07645078748464584 = 0.009335407055914402 + 0.01 * 6.711538314819336
Epoch 630, val loss: 0.9870648384094238
Epoch 640, training loss: 0.07594317197799683 = 0.008902067318558693 + 0.01 * 6.704110622406006
Epoch 640, val loss: 0.9944338202476501
Epoch 650, training loss: 0.07553243637084961 = 0.008499860763549805 + 0.01 * 6.703258037567139
Epoch 650, val loss: 1.0016288757324219
Epoch 660, training loss: 0.07501411437988281 = 0.00812621135264635 + 0.01 * 6.6887898445129395
Epoch 660, val loss: 1.0086451768875122
Epoch 670, training loss: 0.07468876242637634 = 0.007778175640851259 + 0.01 * 6.691059112548828
Epoch 670, val loss: 1.0155093669891357
Epoch 680, training loss: 0.0744524747133255 = 0.007453625090420246 + 0.01 * 6.69988489151001
Epoch 680, val loss: 1.0222116708755493
Epoch 690, training loss: 0.07383625954389572 = 0.007150755263864994 + 0.01 * 6.668550968170166
Epoch 690, val loss: 1.0287636518478394
Epoch 700, training loss: 0.07358257472515106 = 0.006867724470794201 + 0.01 * 6.671485424041748
Epoch 700, val loss: 1.0351691246032715
Epoch 710, training loss: 0.07345853745937347 = 0.00660256203263998 + 0.01 * 6.6855974197387695
Epoch 710, val loss: 1.0414013862609863
Epoch 720, training loss: 0.07293041050434113 = 0.006354292389005423 + 0.01 * 6.65761137008667
Epoch 720, val loss: 1.0475068092346191
Epoch 730, training loss: 0.07260285317897797 = 0.006121182814240456 + 0.01 * 6.648167133331299
Epoch 730, val loss: 1.05349862575531
Epoch 740, training loss: 0.07253643125295639 = 0.0059018745087087154 + 0.01 * 6.663455963134766
Epoch 740, val loss: 1.059314250946045
Epoch 750, training loss: 0.07208764553070068 = 0.005695555359125137 + 0.01 * 6.639208793640137
Epoch 750, val loss: 1.0650265216827393
Epoch 760, training loss: 0.07179923355579376 = 0.005501230247318745 + 0.01 * 6.629800319671631
Epoch 760, val loss: 1.0706387758255005
Epoch 770, training loss: 0.07159314304590225 = 0.0053172847256064415 + 0.01 * 6.6275858879089355
Epoch 770, val loss: 1.0761727094650269
Epoch 780, training loss: 0.07160410284996033 = 0.005142959300428629 + 0.01 * 6.646114826202393
Epoch 780, val loss: 1.0815255641937256
Epoch 790, training loss: 0.07112091779708862 = 0.00497810821980238 + 0.01 * 6.61428165435791
Epoch 790, val loss: 1.0867712497711182
Epoch 800, training loss: 0.07094429433345795 = 0.004821867682039738 + 0.01 * 6.612242698669434
Epoch 800, val loss: 1.0919849872589111
Epoch 810, training loss: 0.07076302170753479 = 0.00467354990541935 + 0.01 * 6.60894775390625
Epoch 810, val loss: 1.0970327854156494
Epoch 820, training loss: 0.07053821533918381 = 0.0045325723476707935 + 0.01 * 6.600564479827881
Epoch 820, val loss: 1.1020560264587402
Epoch 830, training loss: 0.07046210020780563 = 0.004398596007376909 + 0.01 * 6.606349945068359
Epoch 830, val loss: 1.1069731712341309
Epoch 840, training loss: 0.07023938745260239 = 0.004270927049219608 + 0.01 * 6.596846580505371
Epoch 840, val loss: 1.111811637878418
Epoch 850, training loss: 0.07002204656600952 = 0.004149448126554489 + 0.01 * 6.587260723114014
Epoch 850, val loss: 1.116598129272461
Epoch 860, training loss: 0.06995188444852829 = 0.0040337578393518925 + 0.01 * 6.591812610626221
Epoch 860, val loss: 1.1212414503097534
Epoch 870, training loss: 0.0697462260723114 = 0.003923513926565647 + 0.01 * 6.582271099090576
Epoch 870, val loss: 1.1258538961410522
Epoch 880, training loss: 0.06966550648212433 = 0.003818573895841837 + 0.01 * 6.584693431854248
Epoch 880, val loss: 1.1303455829620361
Epoch 890, training loss: 0.06955208629369736 = 0.003718346357345581 + 0.01 * 6.5833740234375
Epoch 890, val loss: 1.1348472833633423
Epoch 900, training loss: 0.06936703622341156 = 0.0036225386429578066 + 0.01 * 6.57444953918457
Epoch 900, val loss: 1.139098048210144
Epoch 910, training loss: 0.06921355426311493 = 0.003531102789565921 + 0.01 * 6.568244934082031
Epoch 910, val loss: 1.1434240341186523
Epoch 920, training loss: 0.06941017508506775 = 0.003443381981924176 + 0.01 * 6.5966796875
Epoch 920, val loss: 1.1476420164108276
Epoch 930, training loss: 0.06892129778862 = 0.0033601410686969757 + 0.01 * 6.55611515045166
Epoch 930, val loss: 1.1517541408538818
Epoch 940, training loss: 0.06894034892320633 = 0.003280349774286151 + 0.01 * 6.566000461578369
Epoch 940, val loss: 1.1557567119598389
Epoch 950, training loss: 0.06885623186826706 = 0.0032038260251283646 + 0.01 * 6.565240859985352
Epoch 950, val loss: 1.1597540378570557
Epoch 960, training loss: 0.06860672682523727 = 0.0031305805314332247 + 0.01 * 6.547615051269531
Epoch 960, val loss: 1.1636563539505005
Epoch 970, training loss: 0.0686013326048851 = 0.0030603387858718634 + 0.01 * 6.5540995597839355
Epoch 970, val loss: 1.1674902439117432
Epoch 980, training loss: 0.06852341443300247 = 0.0029930132441222668 + 0.01 * 6.553040504455566
Epoch 980, val loss: 1.1713740825653076
Epoch 990, training loss: 0.06827976554632187 = 0.0029284674674272537 + 0.01 * 6.535130500793457
Epoch 990, val loss: 1.1750050783157349
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.6568
Flip ASR: 0.5867/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.030000925064087 = 1.946262240409851 + 0.01 * 8.37386703491211
Epoch 0, val loss: 1.9414135217666626
Epoch 10, training loss: 2.0199549198150635 = 1.936217188835144 + 0.01 * 8.373764991760254
Epoch 10, val loss: 1.930192470550537
Epoch 20, training loss: 2.0076491832733154 = 1.923915147781372 + 0.01 * 8.373397827148438
Epoch 20, val loss: 1.9161605834960938
Epoch 30, training loss: 1.9904038906097412 = 1.9066818952560425 + 0.01 * 8.372198104858398
Epoch 30, val loss: 1.896236538887024
Epoch 40, training loss: 1.9651641845703125 = 1.8815006017684937 + 0.01 * 8.366358757019043
Epoch 40, val loss: 1.8671159744262695
Epoch 50, training loss: 1.92892324924469 = 1.8456183671951294 + 0.01 * 8.330488204956055
Epoch 50, val loss: 1.8268539905548096
Epoch 60, training loss: 1.8818564414978027 = 1.8004730939865112 + 0.01 * 8.13833236694336
Epoch 60, val loss: 1.7800525426864624
Epoch 70, training loss: 1.8289108276367188 = 1.7522485256195068 + 0.01 * 7.666228294372559
Epoch 70, val loss: 1.7363266944885254
Epoch 80, training loss: 1.7706149816513062 = 1.6966073513031006 + 0.01 * 7.400760173797607
Epoch 80, val loss: 1.6897318363189697
Epoch 90, training loss: 1.6965856552124023 = 1.6236602067947388 + 0.01 * 7.292545795440674
Epoch 90, val loss: 1.626966118812561
Epoch 100, training loss: 1.6014959812164307 = 1.5290216207504272 + 0.01 * 7.247437477111816
Epoch 100, val loss: 1.544944405555725
Epoch 110, training loss: 1.4873310327529907 = 1.4151966571807861 + 0.01 * 7.213436603546143
Epoch 110, val loss: 1.4496861696243286
Epoch 120, training loss: 1.3622450828552246 = 1.2905250787734985 + 0.01 * 7.172004699707031
Epoch 120, val loss: 1.3475089073181152
Epoch 130, training loss: 1.2345423698425293 = 1.163311243057251 + 0.01 * 7.123118877410889
Epoch 130, val loss: 1.2468892335891724
Epoch 140, training loss: 1.1125407218933105 = 1.041804552078247 + 0.01 * 7.073615074157715
Epoch 140, val loss: 1.153630256652832
Epoch 150, training loss: 1.004428505897522 = 0.9339508414268494 + 0.01 * 7.047768592834473
Epoch 150, val loss: 1.0729610919952393
Epoch 160, training loss: 0.9144634008407593 = 0.8441441655158997 + 0.01 * 7.031923770904541
Epoch 160, val loss: 1.0076491832733154
Epoch 170, training loss: 0.8418245911598206 = 0.7717058658599854 + 0.01 * 7.011874675750732
Epoch 170, val loss: 0.9577354192733765
Epoch 180, training loss: 0.7829594612121582 = 0.7131023406982422 + 0.01 * 6.9857096672058105
Epoch 180, val loss: 0.9199817776679993
Epoch 190, training loss: 0.7336375117301941 = 0.6640539765357971 + 0.01 * 6.9583516120910645
Epoch 190, val loss: 0.8909066915512085
Epoch 200, training loss: 0.6902779936790466 = 0.6208635568618774 + 0.01 * 6.941442012786865
Epoch 200, val loss: 0.8681227564811707
Epoch 210, training loss: 0.6500662565231323 = 0.5807771682739258 + 0.01 * 6.9289069175720215
Epoch 210, val loss: 0.8498656153678894
Epoch 220, training loss: 0.6113523244857788 = 0.5421407222747803 + 0.01 * 6.921157360076904
Epoch 220, val loss: 0.8341811299324036
Epoch 230, training loss: 0.5731726288795471 = 0.5039933919906616 + 0.01 * 6.917923450469971
Epoch 230, val loss: 0.8193541169166565
Epoch 240, training loss: 0.5348920822143555 = 0.4657343029975891 + 0.01 * 6.915778636932373
Epoch 240, val loss: 0.8049836754798889
Epoch 250, training loss: 0.4962376356124878 = 0.42709821462631226 + 0.01 * 6.913942337036133
Epoch 250, val loss: 0.79095059633255
Epoch 260, training loss: 0.4571308493614197 = 0.38801226019859314 + 0.01 * 6.911859035491943
Epoch 260, val loss: 0.7772811055183411
Epoch 270, training loss: 0.41779232025146484 = 0.3486989438533783 + 0.01 * 6.909336566925049
Epoch 270, val loss: 0.7640138268470764
Epoch 280, training loss: 0.37909790873527527 = 0.31003397703170776 + 0.01 * 6.906392574310303
Epoch 280, val loss: 0.7514322400093079
Epoch 290, training loss: 0.3421868681907654 = 0.27313387393951416 + 0.01 * 6.905298233032227
Epoch 290, val loss: 0.740632176399231
Epoch 300, training loss: 0.30834928154945374 = 0.23934537172317505 + 0.01 * 6.900390625
Epoch 300, val loss: 0.7323447465896606
Epoch 310, training loss: 0.2785230278968811 = 0.20956185460090637 + 0.01 * 6.896118640899658
Epoch 310, val loss: 0.7274367809295654
Epoch 320, training loss: 0.2529507875442505 = 0.18403080105781555 + 0.01 * 6.892000198364258
Epoch 320, val loss: 0.7259073853492737
Epoch 330, training loss: 0.23121079802513123 = 0.16233448684215546 + 0.01 * 6.887631416320801
Epoch 330, val loss: 0.7277538776397705
Epoch 340, training loss: 0.2126825749874115 = 0.14386221766471863 + 0.01 * 6.882036209106445
Epoch 340, val loss: 0.7325341701507568
Epoch 350, training loss: 0.19675326347351074 = 0.1279648244380951 + 0.01 * 6.878844738006592
Epoch 350, val loss: 0.7397276163101196
Epoch 360, training loss: 0.18291492760181427 = 0.11420168727636337 + 0.01 * 6.871324062347412
Epoch 360, val loss: 0.748773455619812
Epoch 370, training loss: 0.1708540916442871 = 0.10219109058380127 + 0.01 * 6.8663010597229
Epoch 370, val loss: 0.7591445446014404
Epoch 380, training loss: 0.1600804328918457 = 0.09152643382549286 + 0.01 * 6.855399131774902
Epoch 380, val loss: 0.770541787147522
Epoch 390, training loss: 0.15042825043201447 = 0.08195159584283829 + 0.01 * 6.847665309906006
Epoch 390, val loss: 0.7826970219612122
Epoch 400, training loss: 0.1417616605758667 = 0.0733160674571991 + 0.01 * 6.844560146331787
Epoch 400, val loss: 0.7955321669578552
Epoch 410, training loss: 0.1341332197189331 = 0.06559646874666214 + 0.01 * 6.85367488861084
Epoch 410, val loss: 0.808725118637085
Epoch 420, training loss: 0.1271016001701355 = 0.05879594385623932 + 0.01 * 6.830565929412842
Epoch 420, val loss: 0.8223980069160461
Epoch 430, training loss: 0.12105277180671692 = 0.05286907032132149 + 0.01 * 6.818370819091797
Epoch 430, val loss: 0.8362874388694763
Epoch 440, training loss: 0.11585022509098053 = 0.04769144952297211 + 0.01 * 6.815877914428711
Epoch 440, val loss: 0.8501657843589783
Epoch 450, training loss: 0.11121003329753876 = 0.0431523397564888 + 0.01 * 6.805769443511963
Epoch 450, val loss: 0.864090621471405
Epoch 460, training loss: 0.1071237176656723 = 0.039160195738077164 + 0.01 * 6.796351909637451
Epoch 460, val loss: 0.8777238130569458
Epoch 470, training loss: 0.10362802445888519 = 0.03564820811152458 + 0.01 * 6.797982215881348
Epoch 470, val loss: 0.8911585807800293
Epoch 480, training loss: 0.100350521504879 = 0.032550714910030365 + 0.01 * 6.7799811363220215
Epoch 480, val loss: 0.904354453086853
Epoch 490, training loss: 0.09755362570285797 = 0.029813459143042564 + 0.01 * 6.774016380310059
Epoch 490, val loss: 0.9172983169555664
Epoch 500, training loss: 0.09508507698774338 = 0.0273833479732275 + 0.01 * 6.770173072814941
Epoch 500, val loss: 0.9298813939094543
Epoch 510, training loss: 0.0927194282412529 = 0.025222457945346832 + 0.01 * 6.749697208404541
Epoch 510, val loss: 0.9421899914741516
Epoch 520, training loss: 0.0908222496509552 = 0.023293741047382355 + 0.01 * 6.7528510093688965
Epoch 520, val loss: 0.9541146755218506
Epoch 530, training loss: 0.08895900845527649 = 0.021570753306150436 + 0.01 * 6.738826274871826
Epoch 530, val loss: 0.9657471776008606
Epoch 540, training loss: 0.08736775815486908 = 0.02002798765897751 + 0.01 * 6.7339768409729
Epoch 540, val loss: 0.9769723415374756
Epoch 550, training loss: 0.08585555851459503 = 0.018642356619238853 + 0.01 * 6.721320152282715
Epoch 550, val loss: 0.9879738092422485
Epoch 560, training loss: 0.08455804735422134 = 0.017392167821526527 + 0.01 * 6.716588497161865
Epoch 560, val loss: 0.9985905289649963
Epoch 570, training loss: 0.08327958732843399 = 0.016261480748653412 + 0.01 * 6.701810836791992
Epoch 570, val loss: 1.0089291334152222
Epoch 580, training loss: 0.08213486522436142 = 0.015235529281198978 + 0.01 * 6.689933776855469
Epoch 580, val loss: 1.0189553499221802
Epoch 590, training loss: 0.08125268667936325 = 0.014303071424365044 + 0.01 * 6.6949615478515625
Epoch 590, val loss: 1.0287173986434937
Epoch 600, training loss: 0.08045141398906708 = 0.013454960659146309 + 0.01 * 6.699645519256592
Epoch 600, val loss: 1.0381550788879395
Epoch 610, training loss: 0.07941577583551407 = 0.01268173661082983 + 0.01 * 6.673404216766357
Epoch 610, val loss: 1.0473676919937134
Epoch 620, training loss: 0.07864005863666534 = 0.011974220164120197 + 0.01 * 6.666584014892578
Epoch 620, val loss: 1.0563421249389648
Epoch 630, training loss: 0.07811270654201508 = 0.011326335370540619 + 0.01 * 6.678637504577637
Epoch 630, val loss: 1.0649932622909546
Epoch 640, training loss: 0.07726667821407318 = 0.010730917565524578 + 0.01 * 6.653576374053955
Epoch 640, val loss: 1.0734567642211914
Epoch 650, training loss: 0.07670784741640091 = 0.010182375088334084 + 0.01 * 6.652547359466553
Epoch 650, val loss: 1.0816736221313477
Epoch 660, training loss: 0.07607487589120865 = 0.009676209650933743 + 0.01 * 6.639866828918457
Epoch 660, val loss: 1.089701771736145
Epoch 670, training loss: 0.0755905956029892 = 0.009208716452121735 + 0.01 * 6.638187885284424
Epoch 670, val loss: 1.0974972248077393
Epoch 680, training loss: 0.07540709525346756 = 0.008775652386248112 + 0.01 * 6.663144588470459
Epoch 680, val loss: 1.1050955057144165
Epoch 690, training loss: 0.07460903376340866 = 0.008374038152396679 + 0.01 * 6.623500347137451
Epoch 690, val loss: 1.1125001907348633
Epoch 700, training loss: 0.0742727592587471 = 0.0080010537058115 + 0.01 * 6.627171039581299
Epoch 700, val loss: 1.119706392288208
Epoch 710, training loss: 0.07377580553293228 = 0.007654718589037657 + 0.01 * 6.6121087074279785
Epoch 710, val loss: 1.1266947984695435
Epoch 720, training loss: 0.0736159086227417 = 0.0073326351121068 + 0.01 * 6.628326892852783
Epoch 720, val loss: 1.1335316896438599
Epoch 730, training loss: 0.07313568145036697 = 0.007032431196421385 + 0.01 * 6.610324859619141
Epoch 730, val loss: 1.1401958465576172
Epoch 740, training loss: 0.07270408421754837 = 0.006751717999577522 + 0.01 * 6.595236778259277
Epoch 740, val loss: 1.146704912185669
Epoch 750, training loss: 0.07250276952981949 = 0.006488943938165903 + 0.01 * 6.601382732391357
Epoch 750, val loss: 1.1530869007110596
Epoch 760, training loss: 0.07212240993976593 = 0.006242578849196434 + 0.01 * 6.587983131408691
Epoch 760, val loss: 1.1592845916748047
Epoch 770, training loss: 0.0719078928232193 = 0.006011676043272018 + 0.01 * 6.5896220207214355
Epoch 770, val loss: 1.165328860282898
Epoch 780, training loss: 0.07172651588916779 = 0.005794660188257694 + 0.01 * 6.593185901641846
Epoch 780, val loss: 1.1712942123413086
Epoch 790, training loss: 0.07147370278835297 = 0.005590439308434725 + 0.01 * 6.588326454162598
Epoch 790, val loss: 1.1770126819610596
Epoch 800, training loss: 0.07117484509944916 = 0.005398397333920002 + 0.01 * 6.5776448249816895
Epoch 800, val loss: 1.182685375213623
Epoch 810, training loss: 0.07091082632541656 = 0.005217324011027813 + 0.01 * 6.569350719451904
Epoch 810, val loss: 1.188184142112732
Epoch 820, training loss: 0.07077097147703171 = 0.005046247038990259 + 0.01 * 6.57247257232666
Epoch 820, val loss: 1.193589210510254
Epoch 830, training loss: 0.07055382430553436 = 0.004884669557213783 + 0.01 * 6.566915988922119
Epoch 830, val loss: 1.1988575458526611
Epoch 840, training loss: 0.07042689621448517 = 0.004731748253107071 + 0.01 * 6.569514751434326
Epoch 840, val loss: 1.2040177583694458
Epoch 850, training loss: 0.07019387930631638 = 0.004587092436850071 + 0.01 * 6.560678958892822
Epoch 850, val loss: 1.2090438604354858
Epoch 860, training loss: 0.07004530727863312 = 0.004449933301657438 + 0.01 * 6.559537410736084
Epoch 860, val loss: 1.2139676809310913
Epoch 870, training loss: 0.06985443085432053 = 0.004319759085774422 + 0.01 * 6.553467750549316
Epoch 870, val loss: 1.2188302278518677
Epoch 880, training loss: 0.06972749531269073 = 0.00419625686481595 + 0.01 * 6.553123950958252
Epoch 880, val loss: 1.2235162258148193
Epoch 890, training loss: 0.06958268582820892 = 0.004078791011124849 + 0.01 * 6.550389766693115
Epoch 890, val loss: 1.2281779050827026
Epoch 900, training loss: 0.0694328024983406 = 0.003967030439525843 + 0.01 * 6.546577453613281
Epoch 900, val loss: 1.232684850692749
Epoch 910, training loss: 0.06939844787120819 = 0.0038604221772402525 + 0.01 * 6.553802967071533
Epoch 910, val loss: 1.237134337425232
Epoch 920, training loss: 0.06913390755653381 = 0.0037589746061712503 + 0.01 * 6.5374932289123535
Epoch 920, val loss: 1.241473913192749
Epoch 930, training loss: 0.06891237199306488 = 0.0036621501203626394 + 0.01 * 6.525022506713867
Epoch 930, val loss: 1.2457011938095093
Epoch 940, training loss: 0.06887715309858322 = 0.003569809952750802 + 0.01 * 6.530734062194824
Epoch 940, val loss: 1.2499130964279175
Epoch 950, training loss: 0.06885191798210144 = 0.003481542458757758 + 0.01 * 6.5370378494262695
Epoch 950, val loss: 1.2539825439453125
Epoch 960, training loss: 0.06865735352039337 = 0.0033971613738685846 + 0.01 * 6.52601957321167
Epoch 960, val loss: 1.2580163478851318
Epoch 970, training loss: 0.06848259270191193 = 0.0033163740299642086 + 0.01 * 6.516622066497803
Epoch 970, val loss: 1.2619291543960571
Epoch 980, training loss: 0.06861796975135803 = 0.0032390502747148275 + 0.01 * 6.5378923416137695
Epoch 980, val loss: 1.265810251235962
Epoch 990, training loss: 0.06831830739974976 = 0.0031648394651710987 + 0.01 * 6.515346527099609
Epoch 990, val loss: 1.269574761390686
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.4945
Flip ASR: 0.4533/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0408451557159424 = 1.9571058750152588 + 0.01 * 8.373920440673828
Epoch 0, val loss: 1.9553691148757935
Epoch 10, training loss: 2.030585765838623 = 1.946846842765808 + 0.01 * 8.373886108398438
Epoch 10, val loss: 1.9445463418960571
Epoch 20, training loss: 2.0182933807373047 = 1.934556484222412 + 0.01 * 8.373702049255371
Epoch 20, val loss: 1.9314824342727661
Epoch 30, training loss: 2.0014889240264893 = 1.9177570343017578 + 0.01 * 8.373189926147461
Epoch 30, val loss: 1.9135504961013794
Epoch 40, training loss: 1.9768892526626587 = 1.8931785821914673 + 0.01 * 8.371068954467773
Epoch 40, val loss: 1.887629747390747
Epoch 50, training loss: 1.9412068128585815 = 1.8576488494873047 + 0.01 * 8.35580062866211
Epoch 50, val loss: 1.8514137268066406
Epoch 60, training loss: 1.8951482772827148 = 1.8124271631240845 + 0.01 * 8.272107124328613
Epoch 60, val loss: 1.8085633516311646
Epoch 70, training loss: 1.8468983173370361 = 1.767426609992981 + 0.01 * 7.947168827056885
Epoch 70, val loss: 1.770552635192871
Epoch 80, training loss: 1.798653483390808 = 1.7213847637176514 + 0.01 * 7.726872444152832
Epoch 80, val loss: 1.731773018836975
Epoch 90, training loss: 1.7329695224761963 = 1.658812165260315 + 0.01 * 7.415738582611084
Epoch 90, val loss: 1.6779080629348755
Epoch 100, training loss: 1.6481611728668213 = 1.5757559537887573 + 0.01 * 7.240520000457764
Epoch 100, val loss: 1.6084628105163574
Epoch 110, training loss: 1.543765902519226 = 1.4722373485565186 + 0.01 * 7.152857303619385
Epoch 110, val loss: 1.5240859985351562
Epoch 120, training loss: 1.4281795024871826 = 1.3573760986328125 + 0.01 * 7.080337047576904
Epoch 120, val loss: 1.4316869974136353
Epoch 130, training loss: 1.3095369338989258 = 1.2391185760498047 + 0.01 * 7.041833877563477
Epoch 130, val loss: 1.3395576477050781
Epoch 140, training loss: 1.193859338760376 = 1.1235991716384888 + 0.01 * 7.0260114669799805
Epoch 140, val loss: 1.2511277198791504
Epoch 150, training loss: 1.0857025384902954 = 1.0155422687530518 + 0.01 * 7.016025543212891
Epoch 150, val loss: 1.169700026512146
Epoch 160, training loss: 0.9871465563774109 = 0.9171043038368225 + 0.01 * 7.004223346710205
Epoch 160, val loss: 1.0955673456192017
Epoch 170, training loss: 0.8979793787002563 = 0.8280835151672363 + 0.01 * 6.98958683013916
Epoch 170, val loss: 1.027761459350586
Epoch 180, training loss: 0.8175868391990662 = 0.7478311061859131 + 0.01 * 6.975573539733887
Epoch 180, val loss: 0.9657402634620667
Epoch 190, training loss: 0.7454489469528198 = 0.6758031845092773 + 0.01 * 6.964574813842773
Epoch 190, val loss: 0.910437285900116
Epoch 200, training loss: 0.6804412603378296 = 0.6108705401420593 + 0.01 * 6.957073211669922
Epoch 200, val loss: 0.8622618913650513
Epoch 210, training loss: 0.6206578016281128 = 0.5511351823806763 + 0.01 * 6.952259063720703
Epoch 210, val loss: 0.8209139704704285
Epoch 220, training loss: 0.5643638968467712 = 0.4948732852935791 + 0.01 * 6.949063777923584
Epoch 220, val loss: 0.785466194152832
Epoch 230, training loss: 0.5106425285339355 = 0.4411753714084625 + 0.01 * 6.946718215942383
Epoch 230, val loss: 0.7554709911346436
Epoch 240, training loss: 0.45926833152770996 = 0.38982295989990234 + 0.01 * 6.944537162780762
Epoch 240, val loss: 0.7301033735275269
Epoch 250, training loss: 0.4106394648551941 = 0.3412131071090698 + 0.01 * 6.9426374435424805
Epoch 250, val loss: 0.7092362642288208
Epoch 260, training loss: 0.36549144983291626 = 0.2960854470729828 + 0.01 * 6.94059944152832
Epoch 260, val loss: 0.692753255367279
Epoch 270, training loss: 0.3246285319328308 = 0.2552442252635956 + 0.01 * 6.938431739807129
Epoch 270, val loss: 0.6806965470314026
Epoch 280, training loss: 0.28862008452415466 = 0.21926067769527435 + 0.01 * 6.935941219329834
Epoch 280, val loss: 0.6730362772941589
Epoch 290, training loss: 0.2576647698879242 = 0.1883385330438614 + 0.01 * 6.932624816894531
Epoch 290, val loss: 0.6695114374160767
Epoch 300, training loss: 0.23150955140590668 = 0.16221769154071808 + 0.01 * 6.9291863441467285
Epoch 300, val loss: 0.6695845723152161
Epoch 310, training loss: 0.20960134267807007 = 0.14036650955677032 + 0.01 * 6.923482894897461
Epoch 310, val loss: 0.6730160713195801
Epoch 320, training loss: 0.19131749868392944 = 0.1221405416727066 + 0.01 * 6.91769552230835
Epoch 320, val loss: 0.6791738867759705
Epoch 330, training loss: 0.1760084331035614 = 0.10689686238765717 + 0.01 * 6.911158084869385
Epoch 330, val loss: 0.6875947713851929
Epoch 340, training loss: 0.1630970537662506 = 0.09406822174787521 + 0.01 * 6.902883529663086
Epoch 340, val loss: 0.6977435350418091
Epoch 350, training loss: 0.15212088823318481 = 0.08320043981075287 + 0.01 * 6.892045497894287
Epoch 350, val loss: 0.7091898322105408
Epoch 360, training loss: 0.142744779586792 = 0.07393833249807358 + 0.01 * 6.880645751953125
Epoch 360, val loss: 0.7217163443565369
Epoch 370, training loss: 0.1347329020500183 = 0.0660032331943512 + 0.01 * 6.87296724319458
Epoch 370, val loss: 0.7349406480789185
Epoch 380, training loss: 0.12783992290496826 = 0.05916469544172287 + 0.01 * 6.867523193359375
Epoch 380, val loss: 0.748771071434021
Epoch 390, training loss: 0.12186122685670853 = 0.0532425194978714 + 0.01 * 6.861871242523193
Epoch 390, val loss: 0.7630516886711121
Epoch 400, training loss: 0.11663351953029633 = 0.04808959737420082 + 0.01 * 6.854393005371094
Epoch 400, val loss: 0.7775800824165344
Epoch 410, training loss: 0.11203429102897644 = 0.04358696565032005 + 0.01 * 6.844732284545898
Epoch 410, val loss: 0.7923557162284851
Epoch 420, training loss: 0.10802477598190308 = 0.03963444009423256 + 0.01 * 6.839033126831055
Epoch 420, val loss: 0.8072137832641602
Epoch 430, training loss: 0.10445061326026917 = 0.0361536480486393 + 0.01 * 6.8296966552734375
Epoch 430, val loss: 0.821967601776123
Epoch 440, training loss: 0.10127595067024231 = 0.03307810053229332 + 0.01 * 6.819784641265869
Epoch 440, val loss: 0.8367897272109985
Epoch 450, training loss: 0.09870648384094238 = 0.0303525160998106 + 0.01 * 6.835396766662598
Epoch 450, val loss: 0.851372241973877
Epoch 460, training loss: 0.09606315195560455 = 0.027933329343795776 + 0.01 * 6.812982082366943
Epoch 460, val loss: 0.8656986355781555
Epoch 470, training loss: 0.09377166628837585 = 0.025778349488973618 + 0.01 * 6.7993316650390625
Epoch 470, val loss: 0.8797155618667603
Epoch 480, training loss: 0.0917750671505928 = 0.023850755766034126 + 0.01 * 6.792431354522705
Epoch 480, val loss: 0.8934842348098755
Epoch 490, training loss: 0.09011007845401764 = 0.02212277613580227 + 0.01 * 6.798730373382568
Epoch 490, val loss: 0.9069327712059021
Epoch 500, training loss: 0.08837982267141342 = 0.020570723339915276 + 0.01 * 6.780910015106201
Epoch 500, val loss: 0.9200229644775391
Epoch 510, training loss: 0.08696391433477402 = 0.019173018634319305 + 0.01 * 6.779089450836182
Epoch 510, val loss: 0.9327547550201416
Epoch 520, training loss: 0.0855863094329834 = 0.01791103184223175 + 0.01 * 6.7675275802612305
Epoch 520, val loss: 0.9452458024024963
Epoch 530, training loss: 0.08447984606027603 = 0.016768140718340874 + 0.01 * 6.771170616149902
Epoch 530, val loss: 0.9572046399116516
Epoch 540, training loss: 0.0833386480808258 = 0.015732046216726303 + 0.01 * 6.760660648345947
Epoch 540, val loss: 0.9689328670501709
Epoch 550, training loss: 0.0822945162653923 = 0.014789181761443615 + 0.01 * 6.750533580780029
Epoch 550, val loss: 0.980262041091919
Epoch 560, training loss: 0.08154978603124619 = 0.013929233886301517 + 0.01 * 6.762055397033691
Epoch 560, val loss: 0.9911559224128723
Epoch 570, training loss: 0.08050936460494995 = 0.013144044205546379 + 0.01 * 6.736532211303711
Epoch 570, val loss: 1.001840591430664
Epoch 580, training loss: 0.07991230487823486 = 0.012424669228494167 + 0.01 * 6.748763561248779
Epoch 580, val loss: 1.0121229887008667
Epoch 590, training loss: 0.07904298603534698 = 0.01176498830318451 + 0.01 * 6.727799892425537
Epoch 590, val loss: 1.0221349000930786
Epoch 600, training loss: 0.07835184037685394 = 0.011158169247210026 + 0.01 * 6.719367504119873
Epoch 600, val loss: 1.0318777561187744
Epoch 610, training loss: 0.07773677259683609 = 0.010598882101476192 + 0.01 * 6.713788986206055
Epoch 610, val loss: 1.041224718093872
Epoch 620, training loss: 0.07718992978334427 = 0.01008255872875452 + 0.01 * 6.710737228393555
Epoch 620, val loss: 1.0504690408706665
Epoch 630, training loss: 0.07674475014209747 = 0.009605137631297112 + 0.01 * 6.713961124420166
Epoch 630, val loss: 1.059247612953186
Epoch 640, training loss: 0.07608997821807861 = 0.009162791073322296 + 0.01 * 6.692718505859375
Epoch 640, val loss: 1.0678789615631104
Epoch 650, training loss: 0.07575951516628265 = 0.008751604706048965 + 0.01 * 6.700791358947754
Epoch 650, val loss: 1.0762889385223389
Epoch 660, training loss: 0.07518928498029709 = 0.008369809947907925 + 0.01 * 6.681947708129883
Epoch 660, val loss: 1.0844777822494507
Epoch 670, training loss: 0.07493846118450165 = 0.008013315498828888 + 0.01 * 6.692514896392822
Epoch 670, val loss: 1.0923757553100586
Epoch 680, training loss: 0.07448267936706543 = 0.007680051028728485 + 0.01 * 6.680263042449951
Epoch 680, val loss: 1.1002167463302612
Epoch 690, training loss: 0.0741550475358963 = 0.00736963888630271 + 0.01 * 6.67854118347168
Epoch 690, val loss: 1.1075502634048462
Epoch 700, training loss: 0.07379980385303497 = 0.007080491632223129 + 0.01 * 6.671931743621826
Epoch 700, val loss: 1.115043044090271
Epoch 710, training loss: 0.07334873825311661 = 0.0068089948035776615 + 0.01 * 6.6539740562438965
Epoch 710, val loss: 1.1220288276672363
Epoch 720, training loss: 0.07310323417186737 = 0.0065543209202587605 + 0.01 * 6.654891490936279
Epoch 720, val loss: 1.1290308237075806
Epoch 730, training loss: 0.0728224515914917 = 0.006315373815596104 + 0.01 * 6.650707721710205
Epoch 730, val loss: 1.1357842683792114
Epoch 740, training loss: 0.07268397510051727 = 0.006090110167860985 + 0.01 * 6.659386157989502
Epoch 740, val loss: 1.1424615383148193
Epoch 750, training loss: 0.07218829542398453 = 0.005878211464732885 + 0.01 * 6.631008625030518
Epoch 750, val loss: 1.148894190788269
Epoch 760, training loss: 0.07222742587327957 = 0.005678684450685978 + 0.01 * 6.654874324798584
Epoch 760, val loss: 1.1551045179367065
Epoch 770, training loss: 0.07173982262611389 = 0.005490490701049566 + 0.01 * 6.624933242797852
Epoch 770, val loss: 1.1613432168960571
Epoch 780, training loss: 0.07161244750022888 = 0.005312565714120865 + 0.01 * 6.629987716674805
Epoch 780, val loss: 1.1672025918960571
Epoch 790, training loss: 0.07126225531101227 = 0.005144503898918629 + 0.01 * 6.611774921417236
Epoch 790, val loss: 1.173033595085144
Epoch 800, training loss: 0.07152023911476135 = 0.004985379055142403 + 0.01 * 6.653486251831055
Epoch 800, val loss: 1.1788101196289062
Epoch 810, training loss: 0.07097171992063522 = 0.004834756720811129 + 0.01 * 6.613696575164795
Epoch 810, val loss: 1.1843692064285278
Epoch 820, training loss: 0.07107504457235336 = 0.004691500682383776 + 0.01 * 6.638354301452637
Epoch 820, val loss: 1.1897059679031372
Epoch 830, training loss: 0.07063741236925125 = 0.004555893130600452 + 0.01 * 6.608151435852051
Epoch 830, val loss: 1.1950818300247192
Epoch 840, training loss: 0.0707610473036766 = 0.004426584113389254 + 0.01 * 6.633446216583252
Epoch 840, val loss: 1.2001842260360718
Epoch 850, training loss: 0.07017401605844498 = 0.004304222762584686 + 0.01 * 6.586979389190674
Epoch 850, val loss: 1.205302119255066
Epoch 860, training loss: 0.07027629017829895 = 0.004187398590147495 + 0.01 * 6.608889102935791
Epoch 860, val loss: 1.2101993560791016
Epoch 870, training loss: 0.06995554268360138 = 0.004076176322996616 + 0.01 * 6.587936878204346
Epoch 870, val loss: 1.2150973081588745
Epoch 880, training loss: 0.0697261393070221 = 0.003970210440456867 + 0.01 * 6.575592517852783
Epoch 880, val loss: 1.2197717428207397
Epoch 890, training loss: 0.06987730413675308 = 0.0038689658977091312 + 0.01 * 6.600833892822266
Epoch 890, val loss: 1.2244313955307007
Epoch 900, training loss: 0.0694950744509697 = 0.003772337455302477 + 0.01 * 6.572273254394531
Epoch 900, val loss: 1.2289925813674927
Epoch 910, training loss: 0.06942835450172424 = 0.003680143505334854 + 0.01 * 6.574820518493652
Epoch 910, val loss: 1.233330249786377
Epoch 920, training loss: 0.06920360773801804 = 0.0035920124500989914 + 0.01 * 6.561159610748291
Epoch 920, val loss: 1.2377740144729614
Epoch 930, training loss: 0.06951770931482315 = 0.0035075105261057615 + 0.01 * 6.601019859313965
Epoch 930, val loss: 1.2419843673706055
Epoch 940, training loss: 0.06909293681383133 = 0.0034269497264176607 + 0.01 * 6.566598892211914
Epoch 940, val loss: 1.2461446523666382
Epoch 950, training loss: 0.0689881443977356 = 0.0033495298121124506 + 0.01 * 6.563861846923828
Epoch 950, val loss: 1.2501546144485474
Epoch 960, training loss: 0.06880051642656326 = 0.003275298047810793 + 0.01 * 6.552521705627441
Epoch 960, val loss: 1.2542445659637451
Epoch 970, training loss: 0.06882687658071518 = 0.003204227425158024 + 0.01 * 6.562265396118164
Epoch 970, val loss: 1.2580455541610718
Epoch 980, training loss: 0.06874442845582962 = 0.003136072074994445 + 0.01 * 6.560835838317871
Epoch 980, val loss: 1.2619613409042358
Epoch 990, training loss: 0.06839431077241898 = 0.0030707146506756544 + 0.01 * 6.532360076904297
Epoch 990, val loss: 1.2656182050704956
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.8856
Flip ASR: 0.8622/225 nodes
The final ASR:0.67897, 0.16045, Accuracy:0.82346, 0.00761
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11594])
remove edge: torch.Size([2, 9476])
updated graph: torch.Size([2, 10514])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.97663, 0.00174, Accuracy:0.83086, 0.00630
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.02136492729187 = 1.9376263618469238 + 0.01 * 8.373848915100098
Epoch 0, val loss: 1.939591646194458
Epoch 10, training loss: 2.011012554168701 = 1.9272749423980713 + 0.01 * 8.373773574829102
Epoch 10, val loss: 1.9296486377716064
Epoch 20, training loss: 1.9984381198883057 = 1.9147032499313354 + 0.01 * 8.373485565185547
Epoch 20, val loss: 1.9170736074447632
Epoch 30, training loss: 1.9811984300613403 = 1.897472620010376 + 0.01 * 8.372584342956543
Epoch 30, val loss: 1.8994264602661133
Epoch 40, training loss: 1.9566397666931152 = 1.872959852218628 + 0.01 * 8.367996215820312
Epoch 40, val loss: 1.8744417428970337
Epoch 50, training loss: 1.9231730699539185 = 1.83977472782135 + 0.01 * 8.339836120605469
Epoch 50, val loss: 1.8419266939163208
Epoch 60, training loss: 1.8845285177230835 = 1.8023160696029663 + 0.01 * 8.221248626708984
Epoch 60, val loss: 1.8083996772766113
Epoch 70, training loss: 1.8447929620742798 = 1.7658628225326538 + 0.01 * 7.893011569976807
Epoch 70, val loss: 1.7782975435256958
Epoch 80, training loss: 1.7963629961013794 = 1.7192928791046143 + 0.01 * 7.707006931304932
Epoch 80, val loss: 1.7384912967681885
Epoch 90, training loss: 1.729656457901001 = 1.6550618410110474 + 0.01 * 7.459456920623779
Epoch 90, val loss: 1.684261441230774
Epoch 100, training loss: 1.643364667892456 = 1.5710259675979614 + 0.01 * 7.233870983123779
Epoch 100, val loss: 1.6144558191299438
Epoch 110, training loss: 1.5452479124069214 = 1.474705457687378 + 0.01 * 7.0542473793029785
Epoch 110, val loss: 1.534024715423584
Epoch 120, training loss: 1.4445483684539795 = 1.3745731115341187 + 0.01 * 6.997523784637451
Epoch 120, val loss: 1.4533734321594238
Epoch 130, training loss: 1.3409171104431152 = 1.271275281906128 + 0.01 * 6.964183330535889
Epoch 130, val loss: 1.371610403060913
Epoch 140, training loss: 1.2321611642837524 = 1.1628347635269165 + 0.01 * 6.932644367218018
Epoch 140, val loss: 1.2871648073196411
Epoch 150, training loss: 1.1204348802566528 = 1.0513510704040527 + 0.01 * 6.908386707305908
Epoch 150, val loss: 1.201314091682434
Epoch 160, training loss: 1.0110297203063965 = 0.9420814514160156 + 0.01 * 6.894826889038086
Epoch 160, val loss: 1.119760274887085
Epoch 170, training loss: 0.9083440899848938 = 0.8394582271575928 + 0.01 * 6.888584136962891
Epoch 170, val loss: 1.0449790954589844
Epoch 180, training loss: 0.8143278956413269 = 0.7454608678817749 + 0.01 * 6.886700630187988
Epoch 180, val loss: 0.9781941771507263
Epoch 190, training loss: 0.7297154664993286 = 0.6608548760414124 + 0.01 * 6.886058807373047
Epoch 190, val loss: 0.9192749857902527
Epoch 200, training loss: 0.6550232768058777 = 0.5861729383468628 + 0.01 * 6.885035991668701
Epoch 200, val loss: 0.8687607645988464
Epoch 210, training loss: 0.5904483199119568 = 0.5216150283813477 + 0.01 * 6.883330345153809
Epoch 210, val loss: 0.8273129463195801
Epoch 220, training loss: 0.5350238680839539 = 0.46621331572532654 + 0.01 * 6.881054401397705
Epoch 220, val loss: 0.7945713996887207
Epoch 230, training loss: 0.4866156280040741 = 0.4178310036659241 + 0.01 * 6.878461837768555
Epoch 230, val loss: 0.7691642642021179
Epoch 240, training loss: 0.44273120164871216 = 0.3739723563194275 + 0.01 * 6.875885963439941
Epoch 240, val loss: 0.7485940456390381
Epoch 250, training loss: 0.4015595316886902 = 0.3328213691711426 + 0.01 * 6.873816013336182
Epoch 250, val loss: 0.7314682006835938
Epoch 260, training loss: 0.3622692823410034 = 0.29354822635650635 + 0.01 * 6.872104644775391
Epoch 260, val loss: 0.7167073488235474
Epoch 270, training loss: 0.32486605644226074 = 0.25615647435188293 + 0.01 * 6.870959758758545
Epoch 270, val loss: 0.7039677500724792
Epoch 280, training loss: 0.28991198539733887 = 0.22120821475982666 + 0.01 * 6.8703789710998535
Epoch 280, val loss: 0.693461000919342
Epoch 290, training loss: 0.2581787705421448 = 0.18948444724082947 + 0.01 * 6.869433879852295
Epoch 290, val loss: 0.6856637597084045
Epoch 300, training loss: 0.23034827411174774 = 0.16166476905345917 + 0.01 * 6.868350982666016
Epoch 300, val loss: 0.681030809879303
Epoch 310, training loss: 0.2066957950592041 = 0.13802559673786163 + 0.01 * 6.867020130157471
Epoch 310, val loss: 0.6795098781585693
Epoch 320, training loss: 0.18700261414051056 = 0.11835824698209763 + 0.01 * 6.864436626434326
Epoch 320, val loss: 0.6811354756355286
Epoch 330, training loss: 0.17074716091156006 = 0.10213868319988251 + 0.01 * 6.860848426818848
Epoch 330, val loss: 0.6854943633079529
Epoch 340, training loss: 0.15731316804885864 = 0.08874741941690445 + 0.01 * 6.8565754890441895
Epoch 340, val loss: 0.6920569539070129
Epoch 350, training loss: 0.14612673223018646 = 0.07761488109827042 + 0.01 * 6.851185321807861
Epoch 350, val loss: 0.7003570795059204
Epoch 360, training loss: 0.13672414422035217 = 0.06828302145004272 + 0.01 * 6.844111442565918
Epoch 360, val loss: 0.7099112868309021
Epoch 370, training loss: 0.12875351309776306 = 0.060397759079933167 + 0.01 * 6.835576057434082
Epoch 370, val loss: 0.7203141450881958
Epoch 380, training loss: 0.12198443710803986 = 0.05368972197175026 + 0.01 * 6.829472064971924
Epoch 380, val loss: 0.7312679290771484
Epoch 390, training loss: 0.11611750721931458 = 0.047945182770490646 + 0.01 * 6.817232131958008
Epoch 390, val loss: 0.7426326274871826
Epoch 400, training loss: 0.11122378706932068 = 0.04299421235918999 + 0.01 * 6.822957515716553
Epoch 400, val loss: 0.7542298436164856
Epoch 410, training loss: 0.1067226231098175 = 0.03870679810643196 + 0.01 * 6.801582336425781
Epoch 410, val loss: 0.765912652015686
Epoch 420, training loss: 0.10292714834213257 = 0.034979794174432755 + 0.01 * 6.794735908508301
Epoch 420, val loss: 0.7775684595108032
Epoch 430, training loss: 0.09960424154996872 = 0.031727395951747894 + 0.01 * 6.787684440612793
Epoch 430, val loss: 0.7891720533370972
Epoch 440, training loss: 0.0966913104057312 = 0.028879862278699875 + 0.01 * 6.781145095825195
Epoch 440, val loss: 0.8005608320236206
Epoch 450, training loss: 0.09423855692148209 = 0.026379654183983803 + 0.01 * 6.785890579223633
Epoch 450, val loss: 0.8117477893829346
Epoch 460, training loss: 0.09188947826623917 = 0.024177836254239082 + 0.01 * 6.771164894104004
Epoch 460, val loss: 0.82267826795578
Epoch 470, training loss: 0.0899064689874649 = 0.022231023758649826 + 0.01 * 6.767544269561768
Epoch 470, val loss: 0.8333382606506348
Epoch 480, training loss: 0.08812130987644196 = 0.02050374448299408 + 0.01 * 6.761756896972656
Epoch 480, val loss: 0.8438197374343872
Epoch 490, training loss: 0.08653713762760162 = 0.018966063857078552 + 0.01 * 6.757107734680176
Epoch 490, val loss: 0.85396808385849
Epoch 500, training loss: 0.08518081903457642 = 0.017592675983905792 + 0.01 * 6.758814334869385
Epoch 500, val loss: 0.863913357257843
Epoch 510, training loss: 0.08388512581586838 = 0.016363205388188362 + 0.01 * 6.75219202041626
Epoch 510, val loss: 0.8735353946685791
Epoch 520, training loss: 0.0826806053519249 = 0.015258707106113434 + 0.01 * 6.742189884185791
Epoch 520, val loss: 0.8829140067100525
Epoch 530, training loss: 0.08167774975299835 = 0.014263413846492767 + 0.01 * 6.741434097290039
Epoch 530, val loss: 0.8920561671257019
Epoch 540, training loss: 0.08069021254777908 = 0.013364103622734547 + 0.01 * 6.732610702514648
Epoch 540, val loss: 0.9009154438972473
Epoch 550, training loss: 0.07984048873186111 = 0.012548997066915035 + 0.01 * 6.729148864746094
Epoch 550, val loss: 0.9095660448074341
Epoch 560, training loss: 0.07905744016170502 = 0.011808126233518124 + 0.01 * 6.724931716918945
Epoch 560, val loss: 0.918004035949707
Epoch 570, training loss: 0.07837362587451935 = 0.011132944375276566 + 0.01 * 6.724068641662598
Epoch 570, val loss: 0.9261907935142517
Epoch 580, training loss: 0.07762397080659866 = 0.010516961105167866 + 0.01 * 6.710700988769531
Epoch 580, val loss: 0.9341222643852234
Epoch 590, training loss: 0.07700001448392868 = 0.009953430853784084 + 0.01 * 6.704658508300781
Epoch 590, val loss: 0.9418982863426208
Epoch 600, training loss: 0.07644964754581451 = 0.009436214342713356 + 0.01 * 6.701343536376953
Epoch 600, val loss: 0.9494212865829468
Epoch 610, training loss: 0.0758981704711914 = 0.008960706181824207 + 0.01 * 6.693746089935303
Epoch 610, val loss: 0.9567402005195618
Epoch 620, training loss: 0.07537379860877991 = 0.008522174321115017 + 0.01 * 6.685162544250488
Epoch 620, val loss: 0.9638850688934326
Epoch 630, training loss: 0.0749950259923935 = 0.008117015473544598 + 0.01 * 6.687801837921143
Epoch 630, val loss: 0.9708824753761292
Epoch 640, training loss: 0.07448290288448334 = 0.007742745336145163 + 0.01 * 6.674015522003174
Epoch 640, val loss: 0.9777061343193054
Epoch 650, training loss: 0.07413293421268463 = 0.00739605538547039 + 0.01 * 6.673687934875488
Epoch 650, val loss: 0.9843520522117615
Epoch 660, training loss: 0.07367011904716492 = 0.007074366789311171 + 0.01 * 6.659575462341309
Epoch 660, val loss: 0.9908106923103333
Epoch 670, training loss: 0.07333391904830933 = 0.006775021553039551 + 0.01 * 6.655889987945557
Epoch 670, val loss: 0.9971206188201904
Epoch 680, training loss: 0.07288200408220291 = 0.006496266927570105 + 0.01 * 6.63857364654541
Epoch 680, val loss: 1.0031745433807373
Epoch 690, training loss: 0.0725647509098053 = 0.006236056797206402 + 0.01 * 6.632870197296143
Epoch 690, val loss: 1.0091537237167358
Epoch 700, training loss: 0.0726088285446167 = 0.005992725025862455 + 0.01 * 6.6616106033325195
Epoch 700, val loss: 1.0150694847106934
Epoch 710, training loss: 0.07194364815950394 = 0.005765576381236315 + 0.01 * 6.617806911468506
Epoch 710, val loss: 1.0205901861190796
Epoch 720, training loss: 0.07172302156686783 = 0.005552554503083229 + 0.01 * 6.61704683303833
Epoch 720, val loss: 1.0262243747711182
Epoch 730, training loss: 0.07149630039930344 = 0.005352572537958622 + 0.01 * 6.614372730255127
Epoch 730, val loss: 1.0315001010894775
Epoch 740, training loss: 0.0712188258767128 = 0.005164469126611948 + 0.01 * 6.605436325073242
Epoch 740, val loss: 1.0368369817733765
Epoch 750, training loss: 0.0709153562784195 = 0.004987572785466909 + 0.01 * 6.592778205871582
Epoch 750, val loss: 1.041930079460144
Epoch 760, training loss: 0.07072519510984421 = 0.004821075592190027 + 0.01 * 6.590412139892578
Epoch 760, val loss: 1.0469346046447754
Epoch 770, training loss: 0.07052353769540787 = 0.00466374633833766 + 0.01 * 6.585979461669922
Epoch 770, val loss: 1.0518684387207031
Epoch 780, training loss: 0.07031504809856415 = 0.004515182692557573 + 0.01 * 6.579986572265625
Epoch 780, val loss: 1.0565775632858276
Epoch 790, training loss: 0.07007350027561188 = 0.004374535288661718 + 0.01 * 6.569896697998047
Epoch 790, val loss: 1.061311960220337
Epoch 800, training loss: 0.06997471302747726 = 0.004241553600877523 + 0.01 * 6.573315620422363
Epoch 800, val loss: 1.0658113956451416
Epoch 810, training loss: 0.07000449299812317 = 0.0041153812780976295 + 0.01 * 6.588911056518555
Epoch 810, val loss: 1.0703513622283936
Epoch 820, training loss: 0.06967870146036148 = 0.003995802719146013 + 0.01 * 6.568289756774902
Epoch 820, val loss: 1.0746173858642578
Epoch 830, training loss: 0.06962129473686218 = 0.003882115241140127 + 0.01 * 6.573917865753174
Epoch 830, val loss: 1.0790313482284546
Epoch 840, training loss: 0.06941889226436615 = 0.0037743111606687307 + 0.01 * 6.564458847045898
Epoch 840, val loss: 1.0830990076065063
Epoch 850, training loss: 0.06917371600866318 = 0.003671705024316907 + 0.01 * 6.550200939178467
Epoch 850, val loss: 1.087254285812378
Epoch 860, training loss: 0.06921333819627762 = 0.0035739634186029434 + 0.01 * 6.563938140869141
Epoch 860, val loss: 1.0912305116653442
Epoch 870, training loss: 0.06887175887823105 = 0.00348071800544858 + 0.01 * 6.539104461669922
Epoch 870, val loss: 1.0951101779937744
Epoch 880, training loss: 0.0689862072467804 = 0.0033916872926056385 + 0.01 * 6.559451580047607
Epoch 880, val loss: 1.0991744995117188
Epoch 890, training loss: 0.06877509504556656 = 0.0033067872282117605 + 0.01 * 6.5468316078186035
Epoch 890, val loss: 1.102807641029358
Epoch 900, training loss: 0.06855472177267075 = 0.003225724445655942 + 0.01 * 6.532900333404541
Epoch 900, val loss: 1.106545090675354
Epoch 910, training loss: 0.06857826560735703 = 0.0031483040656894445 + 0.01 * 6.542996406555176
Epoch 910, val loss: 1.1102133989334106
Epoch 920, training loss: 0.06846414506435394 = 0.003074206179007888 + 0.01 * 6.538993835449219
Epoch 920, val loss: 1.1138088703155518
Epoch 930, training loss: 0.06827621906995773 = 0.003003170946612954 + 0.01 * 6.527304649353027
Epoch 930, val loss: 1.1172876358032227
Epoch 940, training loss: 0.06818114966154099 = 0.0029353208374232054 + 0.01 * 6.524582862854004
Epoch 940, val loss: 1.1207280158996582
Epoch 950, training loss: 0.06809892505407333 = 0.002870303113013506 + 0.01 * 6.522861957550049
Epoch 950, val loss: 1.1241191625595093
Epoch 960, training loss: 0.0679502859711647 = 0.0028079429175704718 + 0.01 * 6.5142340660095215
Epoch 960, val loss: 1.1273759603500366
Epoch 970, training loss: 0.06795769184827805 = 0.0027482626028358936 + 0.01 * 6.520942687988281
Epoch 970, val loss: 1.1305785179138184
Epoch 980, training loss: 0.06782445311546326 = 0.002690887777134776 + 0.01 * 6.5133562088012695
Epoch 980, val loss: 1.1338188648223877
Epoch 990, training loss: 0.06782225519418716 = 0.0026358237955719233 + 0.01 * 6.518643856048584
Epoch 990, val loss: 1.1369997262954712
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.6937
Flip ASR: 0.6311/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0353872776031494 = 1.9516489505767822 + 0.01 * 8.373838424682617
Epoch 0, val loss: 1.9514811038970947
Epoch 10, training loss: 2.0249295234680176 = 1.9411916732788086 + 0.01 * 8.373777389526367
Epoch 10, val loss: 1.9411860704421997
Epoch 20, training loss: 2.0129384994506836 = 1.9292038679122925 + 0.01 * 8.37346363067627
Epoch 20, val loss: 1.928770899772644
Epoch 30, training loss: 1.9965949058532715 = 1.9128711223602295 + 0.01 * 8.372376441955566
Epoch 30, val loss: 1.9113010168075562
Epoch 40, training loss: 1.9728384017944336 = 1.889164686203003 + 0.01 * 8.367365837097168
Epoch 40, val loss: 1.885616421699524
Epoch 50, training loss: 1.938489317893982 = 1.855120301246643 + 0.01 * 8.33690071105957
Epoch 50, val loss: 1.8494515419006348
Epoch 60, training loss: 1.8925015926361084 = 1.810937762260437 + 0.01 * 8.156387329101562
Epoch 60, val loss: 1.8049578666687012
Epoch 70, training loss: 1.8402718305587769 = 1.7633447647094727 + 0.01 * 7.692709445953369
Epoch 70, val loss: 1.760732650756836
Epoch 80, training loss: 1.7883024215698242 = 1.7135405540466309 + 0.01 * 7.47618293762207
Epoch 80, val loss: 1.7161602973937988
Epoch 90, training loss: 1.7221672534942627 = 1.6485968828201294 + 0.01 * 7.3570356369018555
Epoch 90, val loss: 1.66010582447052
Epoch 100, training loss: 1.636428713798523 = 1.5637753009796143 + 0.01 * 7.26533842086792
Epoch 100, val loss: 1.5899630784988403
Epoch 110, training loss: 1.532173991203308 = 1.4602347612380981 + 0.01 * 7.193926811218262
Epoch 110, val loss: 1.5073516368865967
Epoch 120, training loss: 1.4191064834594727 = 1.3477685451507568 + 0.01 * 7.133792400360107
Epoch 120, val loss: 1.419901967048645
Epoch 130, training loss: 1.306452989578247 = 1.2355533838272095 + 0.01 * 7.089961528778076
Epoch 130, val loss: 1.333954095840454
Epoch 140, training loss: 1.1978501081466675 = 1.127242088317871 + 0.01 * 7.060796737670898
Epoch 140, val loss: 1.2517787218093872
Epoch 150, training loss: 1.0931910276412964 = 1.0228571891784668 + 0.01 * 7.033380031585693
Epoch 150, val loss: 1.1710840463638306
Epoch 160, training loss: 0.9934924840927124 = 0.9234856963157654 + 0.01 * 7.00067663192749
Epoch 160, val loss: 1.0945254564285278
Epoch 170, training loss: 0.9010351300239563 = 0.8313755989074707 + 0.01 * 6.965951919555664
Epoch 170, val loss: 1.0244998931884766
Epoch 180, training loss: 0.817215085029602 = 0.7478690147399902 + 0.01 * 6.934606552124023
Epoch 180, val loss: 0.9627904891967773
Epoch 190, training loss: 0.7423599362373352 = 0.6732076406478882 + 0.01 * 6.91522741317749
Epoch 190, val loss: 0.9098129868507385
Epoch 200, training loss: 0.6756868362426758 = 0.6066893935203552 + 0.01 * 6.899744987487793
Epoch 200, val loss: 0.8654874563217163
Epoch 210, training loss: 0.6160940527915955 = 0.5471712946891785 + 0.01 * 6.892275333404541
Epoch 210, val loss: 0.828549861907959
Epoch 220, training loss: 0.5623654723167419 = 0.49348846077919006 + 0.01 * 6.887700080871582
Epoch 220, val loss: 0.7972607016563416
Epoch 230, training loss: 0.5134949684143066 = 0.44464173913002014 + 0.01 * 6.885322570800781
Epoch 230, val loss: 0.7706895470619202
Epoch 240, training loss: 0.469223290681839 = 0.40038952231407166 + 0.01 * 6.883377552032471
Epoch 240, val loss: 0.7487773895263672
Epoch 250, training loss: 0.42980313301086426 = 0.3609848618507385 + 0.01 * 6.881828308105469
Epoch 250, val loss: 0.7315135598182678
Epoch 260, training loss: 0.3954504728317261 = 0.32663679122924805 + 0.01 * 6.881367206573486
Epoch 260, val loss: 0.7185640335083008
Epoch 270, training loss: 0.36575984954833984 = 0.2969631254673004 + 0.01 * 6.879673957824707
Epoch 270, val loss: 0.7092230916023254
Epoch 280, training loss: 0.33982211351394653 = 0.2710350751876831 + 0.01 * 6.8787031173706055
Epoch 280, val loss: 0.7029123306274414
Epoch 290, training loss: 0.31658604741096497 = 0.24780535697937012 + 0.01 * 6.878069877624512
Epoch 290, val loss: 0.6988683342933655
Epoch 300, training loss: 0.29519400000572205 = 0.2264179289340973 + 0.01 * 6.877607345581055
Epoch 300, val loss: 0.6964896321296692
Epoch 310, training loss: 0.2751316428184509 = 0.20635928213596344 + 0.01 * 6.877235412597656
Epoch 310, val loss: 0.6953823566436768
Epoch 320, training loss: 0.25610268115997314 = 0.187333881855011 + 0.01 * 6.876878261566162
Epoch 320, val loss: 0.6952065825462341
Epoch 330, training loss: 0.23820656538009644 = 0.1694420725107193 + 0.01 * 6.876450061798096
Epoch 330, val loss: 0.696203351020813
Epoch 340, training loss: 0.22176000475883484 = 0.15300126373767853 + 0.01 * 6.875874042510986
Epoch 340, val loss: 0.6984308362007141
Epoch 350, training loss: 0.20684535801410675 = 0.13809221982955933 + 0.01 * 6.875314235687256
Epoch 350, val loss: 0.7022693753242493
Epoch 360, training loss: 0.19349929690361023 = 0.12475720047950745 + 0.01 * 6.874209403991699
Epoch 360, val loss: 0.7075548768043518
Epoch 370, training loss: 0.18164965510368347 = 0.11292566359043121 + 0.01 * 6.872400283813477
Epoch 370, val loss: 0.7141950130462646
Epoch 380, training loss: 0.1711718589067459 = 0.10245587676763535 + 0.01 * 6.871598243713379
Epoch 380, val loss: 0.7220517992973328
Epoch 390, training loss: 0.1618758738040924 = 0.09319044649600983 + 0.01 * 6.868542194366455
Epoch 390, val loss: 0.7309852838516235
Epoch 400, training loss: 0.1536567360162735 = 0.08497012406587601 + 0.01 * 6.868661403656006
Epoch 400, val loss: 0.7407174706459045
Epoch 410, training loss: 0.14628437161445618 = 0.07766100764274597 + 0.01 * 6.8623366355896
Epoch 410, val loss: 0.7510073781013489
Epoch 420, training loss: 0.13969433307647705 = 0.0710902065038681 + 0.01 * 6.86041259765625
Epoch 420, val loss: 0.7620652914047241
Epoch 430, training loss: 0.13371367752552032 = 0.06515707820653915 + 0.01 * 6.855660438537598
Epoch 430, val loss: 0.7732683420181274
Epoch 440, training loss: 0.12825742363929749 = 0.059787727892398834 + 0.01 * 6.846969127655029
Epoch 440, val loss: 0.7848843336105347
Epoch 450, training loss: 0.12325809895992279 = 0.054875876754522324 + 0.01 * 6.838222026824951
Epoch 450, val loss: 0.7965866923332214
Epoch 460, training loss: 0.11874925345182419 = 0.050409138202667236 + 0.01 * 6.834012031555176
Epoch 460, val loss: 0.8087323904037476
Epoch 470, training loss: 0.11467037349939346 = 0.04641685634851456 + 0.01 * 6.825352191925049
Epoch 470, val loss: 0.8207794427871704
Epoch 480, training loss: 0.11100442707538605 = 0.042852092534303665 + 0.01 * 6.8152337074279785
Epoch 480, val loss: 0.832974374294281
Epoch 490, training loss: 0.10782818496227264 = 0.03962196782231331 + 0.01 * 6.820621490478516
Epoch 490, val loss: 0.8455143570899963
Epoch 500, training loss: 0.10458195209503174 = 0.03665950149297714 + 0.01 * 6.792244911193848
Epoch 500, val loss: 0.8579615950584412
Epoch 510, training loss: 0.10186545550823212 = 0.033904366195201874 + 0.01 * 6.796109199523926
Epoch 510, val loss: 0.8705161809921265
Epoch 520, training loss: 0.0991540402173996 = 0.031340789049863815 + 0.01 * 6.781324863433838
Epoch 520, val loss: 0.8828932642936707
Epoch 530, training loss: 0.09663236141204834 = 0.028953665867447853 + 0.01 * 6.76786994934082
Epoch 530, val loss: 0.8949971795082092
Epoch 540, training loss: 0.09431993961334229 = 0.026777997612953186 + 0.01 * 6.754194259643555
Epoch 540, val loss: 0.9071634411811829
Epoch 550, training loss: 0.09229651093482971 = 0.024811286479234695 + 0.01 * 6.748523235321045
Epoch 550, val loss: 0.9192069172859192
Epoch 560, training loss: 0.0908517986536026 = 0.023028355091810226 + 0.01 * 6.782343864440918
Epoch 560, val loss: 0.9311742782592773
Epoch 570, training loss: 0.08872945606708527 = 0.021417126059532166 + 0.01 * 6.7312331199646
Epoch 570, val loss: 0.9428953528404236
Epoch 580, training loss: 0.08725228905677795 = 0.019968565553426743 + 0.01 * 6.728373050689697
Epoch 580, val loss: 0.9542830586433411
Epoch 590, training loss: 0.08579282462596893 = 0.01865645870566368 + 0.01 * 6.71363639831543
Epoch 590, val loss: 0.965522825717926
Epoch 600, training loss: 0.08503764122724533 = 0.017451956868171692 + 0.01 * 6.758568286895752
Epoch 600, val loss: 0.9765031337738037
Epoch 610, training loss: 0.08348986506462097 = 0.01634739153087139 + 0.01 * 6.714247226715088
Epoch 610, val loss: 0.9871752858161926
Epoch 620, training loss: 0.08228539675474167 = 0.015330983325839043 + 0.01 * 6.695441246032715
Epoch 620, val loss: 0.9976108074188232
Epoch 630, training loss: 0.08134299516677856 = 0.014401067048311234 + 0.01 * 6.694192886352539
Epoch 630, val loss: 1.0078166723251343
Epoch 640, training loss: 0.08032475411891937 = 0.013556062243878841 + 0.01 * 6.6768693923950195
Epoch 640, val loss: 1.0177537202835083
Epoch 650, training loss: 0.07980182021856308 = 0.012790019623935223 + 0.01 * 6.701180458068848
Epoch 650, val loss: 1.027484655380249
Epoch 660, training loss: 0.0787748396396637 = 0.012094156816601753 + 0.01 * 6.6680684089660645
Epoch 660, val loss: 1.0367778539657593
Epoch 670, training loss: 0.07805684953927994 = 0.011461072601377964 + 0.01 * 6.6595778465271
Epoch 670, val loss: 1.0457898378372192
Epoch 680, training loss: 0.07751443237066269 = 0.010881492868065834 + 0.01 * 6.663294315338135
Epoch 680, val loss: 1.054502248764038
Epoch 690, training loss: 0.07675983011722565 = 0.010347142815589905 + 0.01 * 6.641269207000732
Epoch 690, val loss: 1.0630605220794678
Epoch 700, training loss: 0.076305091381073 = 0.009853583760559559 + 0.01 * 6.645150661468506
Epoch 700, val loss: 1.0711100101470947
Epoch 710, training loss: 0.07576720416545868 = 0.009397786110639572 + 0.01 * 6.636941909790039
Epoch 710, val loss: 1.079103708267212
Epoch 720, training loss: 0.07524728029966354 = 0.008976063691079617 + 0.01 * 6.627121448516846
Epoch 720, val loss: 1.0867253541946411
Epoch 730, training loss: 0.07518818229436874 = 0.008585444651544094 + 0.01 * 6.660274028778076
Epoch 730, val loss: 1.0942028760910034
Epoch 740, training loss: 0.07430433481931686 = 0.008222945034503937 + 0.01 * 6.6081390380859375
Epoch 740, val loss: 1.1015185117721558
Epoch 750, training loss: 0.07402992248535156 = 0.007883301936089993 + 0.01 * 6.614662170410156
Epoch 750, val loss: 1.1085295677185059
Epoch 760, training loss: 0.07372643798589706 = 0.007562577724456787 + 0.01 * 6.6163859367370605
Epoch 760, val loss: 1.115378737449646
Epoch 770, training loss: 0.07330478727817535 = 0.007262142840772867 + 0.01 * 6.604264736175537
Epoch 770, val loss: 1.1219621896743774
Epoch 780, training loss: 0.0730731189250946 = 0.006982183083891869 + 0.01 * 6.60909366607666
Epoch 780, val loss: 1.1285115480422974
Epoch 790, training loss: 0.07268837839365005 = 0.00671943835914135 + 0.01 * 6.596893787384033
Epoch 790, val loss: 1.134812355041504
Epoch 800, training loss: 0.07241011410951614 = 0.006472102832049131 + 0.01 * 6.593801021575928
Epoch 800, val loss: 1.141013503074646
Epoch 810, training loss: 0.07203555107116699 = 0.006239844020456076 + 0.01 * 6.579570293426514
Epoch 810, val loss: 1.1469454765319824
Epoch 820, training loss: 0.07180414348840714 = 0.006022093817591667 + 0.01 * 6.578205108642578
Epoch 820, val loss: 1.1528291702270508
Epoch 830, training loss: 0.07157169282436371 = 0.005816904362291098 + 0.01 * 6.575478553771973
Epoch 830, val loss: 1.1585917472839355
Epoch 840, training loss: 0.0712428092956543 = 0.00562305236235261 + 0.01 * 6.561975955963135
Epoch 840, val loss: 1.16420316696167
Epoch 850, training loss: 0.07136102020740509 = 0.005440439563244581 + 0.01 * 6.5920586585998535
Epoch 850, val loss: 1.169717788696289
Epoch 860, training loss: 0.07080262154340744 = 0.005267960485070944 + 0.01 * 6.553465843200684
Epoch 860, val loss: 1.1750187873840332
Epoch 870, training loss: 0.07065322995185852 = 0.005104702431708574 + 0.01 * 6.554852485656738
Epoch 870, val loss: 1.180226445198059
Epoch 880, training loss: 0.07056988030672073 = 0.004950216505676508 + 0.01 * 6.561966419219971
Epoch 880, val loss: 1.1852061748504639
Epoch 890, training loss: 0.07029006630182266 = 0.0048037683591246605 + 0.01 * 6.5486297607421875
Epoch 890, val loss: 1.1901428699493408
Epoch 900, training loss: 0.07001961767673492 = 0.004665052518248558 + 0.01 * 6.53545618057251
Epoch 900, val loss: 1.1949928998947144
Epoch 910, training loss: 0.06986647844314575 = 0.004533044993877411 + 0.01 * 6.533343315124512
Epoch 910, val loss: 1.1995561122894287
Epoch 920, training loss: 0.06982843577861786 = 0.004407824017107487 + 0.01 * 6.5420613288879395
Epoch 920, val loss: 1.2042256593704224
Epoch 930, training loss: 0.0696728304028511 = 0.004288568161427975 + 0.01 * 6.538425922393799
Epoch 930, val loss: 1.2086412906646729
Epoch 940, training loss: 0.06943391263484955 = 0.004175205715000629 + 0.01 * 6.525870323181152
Epoch 940, val loss: 1.2130627632141113
Epoch 950, training loss: 0.06934002041816711 = 0.0040666526183485985 + 0.01 * 6.527336597442627
Epoch 950, val loss: 1.2172412872314453
Epoch 960, training loss: 0.06923690438270569 = 0.003963502123951912 + 0.01 * 6.527340888977051
Epoch 960, val loss: 1.2213995456695557
Epoch 970, training loss: 0.0691571980714798 = 0.003864932805299759 + 0.01 * 6.529227256774902
Epoch 970, val loss: 1.2255136966705322
Epoch 980, training loss: 0.0691285952925682 = 0.0037710382603108883 + 0.01 * 6.5357561111450195
Epoch 980, val loss: 1.2294466495513916
Epoch 990, training loss: 0.06873388588428497 = 0.0036809213925153017 + 0.01 * 6.50529670715332
Epoch 990, val loss: 1.2333344221115112
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.7934
Flip ASR: 0.7600/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0330750942230225 = 1.9493367671966553 + 0.01 * 8.37382698059082
Epoch 0, val loss: 1.9582557678222656
Epoch 10, training loss: 2.0229735374450684 = 1.9392362833023071 + 0.01 * 8.373714447021484
Epoch 10, val loss: 1.9474003314971924
Epoch 20, training loss: 2.0105063915252686 = 1.9267728328704834 + 0.01 * 8.373350143432617
Epoch 20, val loss: 1.9336581230163574
Epoch 30, training loss: 1.99302077293396 = 1.9092979431152344 + 0.01 * 8.372286796569824
Epoch 30, val loss: 1.9142234325408936
Epoch 40, training loss: 1.9671446084976196 = 1.8834760189056396 + 0.01 * 8.366854667663574
Epoch 40, val loss: 1.8859598636627197
Epoch 50, training loss: 1.930152177810669 = 1.8468233346939087 + 0.01 * 8.332887649536133
Epoch 50, val loss: 1.847217082977295
Epoch 60, training loss: 1.883512258529663 = 1.802163004875183 + 0.01 * 8.134930610656738
Epoch 60, val loss: 1.8028466701507568
Epoch 70, training loss: 1.8368605375289917 = 1.758453369140625 + 0.01 * 7.840718746185303
Epoch 70, val loss: 1.7619632482528687
Epoch 80, training loss: 1.7859447002410889 = 1.7109551429748535 + 0.01 * 7.498951435089111
Epoch 80, val loss: 1.7181124687194824
Epoch 90, training loss: 1.7207446098327637 = 1.6481428146362305 + 0.01 * 7.2601823806762695
Epoch 90, val loss: 1.6638253927230835
Epoch 100, training loss: 1.636599063873291 = 1.565471887588501 + 0.01 * 7.112716197967529
Epoch 100, val loss: 1.5962657928466797
Epoch 110, training loss: 1.5354231595993042 = 1.4651695489883423 + 0.01 * 7.025364398956299
Epoch 110, val loss: 1.516640305519104
Epoch 120, training loss: 1.4270877838134766 = 1.35707426071167 + 0.01 * 7.00135612487793
Epoch 120, val loss: 1.4327815771102905
Epoch 130, training loss: 1.3183797597885132 = 1.2485226392745972 + 0.01 * 6.985711097717285
Epoch 130, val loss: 1.3497693538665771
Epoch 140, training loss: 1.2111815214157104 = 1.1414353847503662 + 0.01 * 6.974618434906006
Epoch 140, val loss: 1.2680275440216064
Epoch 150, training loss: 1.1052625179290771 = 1.0356303453445435 + 0.01 * 6.96321439743042
Epoch 150, val loss: 1.1865546703338623
Epoch 160, training loss: 1.0019358396530151 = 0.9324135780334473 + 0.01 * 6.952231407165527
Epoch 160, val loss: 1.1059948205947876
Epoch 170, training loss: 0.9044066667556763 = 0.8349812626838684 + 0.01 * 6.942537307739258
Epoch 170, val loss: 1.0303049087524414
Epoch 180, training loss: 0.8155462741851807 = 0.7462025284767151 + 0.01 * 6.9343719482421875
Epoch 180, val loss: 0.9623070359230042
Epoch 190, training loss: 0.7362870573997498 = 0.667006254196167 + 0.01 * 6.928079128265381
Epoch 190, val loss: 0.9033498167991638
Epoch 200, training loss: 0.6656650304794312 = 0.5964246988296509 + 0.01 * 6.924033164978027
Epoch 200, val loss: 0.8533975481987
Epoch 210, training loss: 0.6019499897956848 = 0.5327308773994446 + 0.01 * 6.921908855438232
Epoch 210, val loss: 0.8111262321472168
Epoch 220, training loss: 0.5437232851982117 = 0.47451531887054443 + 0.01 * 6.920795917510986
Epoch 220, val loss: 0.7751215696334839
Epoch 230, training loss: 0.4902864396572113 = 0.4210856258869171 + 0.01 * 6.92008113861084
Epoch 230, val loss: 0.7447831630706787
Epoch 240, training loss: 0.4415539503097534 = 0.3723588287830353 + 0.01 * 6.919511795043945
Epoch 240, val loss: 0.7201945781707764
Epoch 250, training loss: 0.3974510431289673 = 0.3282586932182312 + 0.01 * 6.919234275817871
Epoch 250, val loss: 0.7012484669685364
Epoch 260, training loss: 0.35771846771240234 = 0.28852808475494385 + 0.01 * 6.919039726257324
Epoch 260, val loss: 0.6872391700744629
Epoch 270, training loss: 0.3218849003314972 = 0.2527013123035431 + 0.01 * 6.91835880279541
Epoch 270, val loss: 0.6773739457130432
Epoch 280, training loss: 0.2894414961338043 = 0.22026589512825012 + 0.01 * 6.917561054229736
Epoch 280, val loss: 0.6710683107376099
Epoch 290, training loss: 0.2601473331451416 = 0.19097909331321716 + 0.01 * 6.916824817657471
Epoch 290, val loss: 0.6682584285736084
Epoch 300, training loss: 0.23406153917312622 = 0.16491414606571198 + 0.01 * 6.914740085601807
Epoch 300, val loss: 0.668573796749115
Epoch 310, training loss: 0.21120911836624146 = 0.14208364486694336 + 0.01 * 6.912546634674072
Epoch 310, val loss: 0.67143315076828
Epoch 320, training loss: 0.19169604778289795 = 0.122574582695961 + 0.01 * 6.912147045135498
Epoch 320, val loss: 0.6770395040512085
Epoch 330, training loss: 0.17519918084144592 = 0.10613379627466202 + 0.01 * 6.906538963317871
Epoch 330, val loss: 0.6845456957817078
Epoch 340, training loss: 0.1614064872264862 = 0.09239014238119125 + 0.01 * 6.901634216308594
Epoch 340, val loss: 0.6939272880554199
Epoch 350, training loss: 0.1499011516571045 = 0.08092492073774338 + 0.01 * 6.897623538970947
Epoch 350, val loss: 0.7047854661941528
Epoch 360, training loss: 0.14032436907291412 = 0.0713336318731308 + 0.01 * 6.899073600769043
Epoch 360, val loss: 0.7166101336479187
Epoch 370, training loss: 0.1320810317993164 = 0.06325128674507141 + 0.01 * 6.882974624633789
Epoch 370, val loss: 0.7290285229682922
Epoch 380, training loss: 0.12511596083641052 = 0.05639462545514107 + 0.01 * 6.872133731842041
Epoch 380, val loss: 0.7418171167373657
Epoch 390, training loss: 0.11921339482069016 = 0.050543710589408875 + 0.01 * 6.866968631744385
Epoch 390, val loss: 0.7547722458839417
Epoch 400, training loss: 0.11400069296360016 = 0.045529481023550034 + 0.01 * 6.847121715545654
Epoch 400, val loss: 0.7677997946739197
Epoch 410, training loss: 0.10954898595809937 = 0.04119724780321121 + 0.01 * 6.835174083709717
Epoch 410, val loss: 0.7806402444839478
Epoch 420, training loss: 0.10565453767776489 = 0.03743070736527443 + 0.01 * 6.822382926940918
Epoch 420, val loss: 0.7933598756790161
Epoch 430, training loss: 0.10219208896160126 = 0.034136105328798294 + 0.01 * 6.805599212646484
Epoch 430, val loss: 0.8059213757514954
Epoch 440, training loss: 0.09920653700828552 = 0.031242825090885162 + 0.01 * 6.796370983123779
Epoch 440, val loss: 0.818215012550354
Epoch 450, training loss: 0.0968380942940712 = 0.028707368299365044 + 0.01 * 6.813072204589844
Epoch 450, val loss: 0.8301196694374084
Epoch 460, training loss: 0.09435328096151352 = 0.02646581269800663 + 0.01 * 6.7887468338012695
Epoch 460, val loss: 0.8418025374412537
Epoch 470, training loss: 0.09220597892999649 = 0.024471387267112732 + 0.01 * 6.773458957672119
Epoch 470, val loss: 0.8531944155693054
Epoch 480, training loss: 0.09032091498374939 = 0.02268938533961773 + 0.01 * 6.763152599334717
Epoch 480, val loss: 0.8643157482147217
Epoch 490, training loss: 0.08862897753715515 = 0.021091552451252937 + 0.01 * 6.7537431716918945
Epoch 490, val loss: 0.8751809000968933
Epoch 500, training loss: 0.08709884434938431 = 0.019654424861073494 + 0.01 * 6.744441986083984
Epoch 500, val loss: 0.8858070969581604
Epoch 510, training loss: 0.08571553230285645 = 0.01835787668824196 + 0.01 * 6.735764980316162
Epoch 510, val loss: 0.8962157964706421
Epoch 520, training loss: 0.08458926528692245 = 0.01718573085963726 + 0.01 * 6.740353584289551
Epoch 520, val loss: 0.9063725471496582
Epoch 530, training loss: 0.08344784379005432 = 0.01612517610192299 + 0.01 * 6.732267379760742
Epoch 530, val loss: 0.916225254535675
Epoch 540, training loss: 0.08229801058769226 = 0.015161771327257156 + 0.01 * 6.713624000549316
Epoch 540, val loss: 0.925851047039032
Epoch 550, training loss: 0.08157118409872055 = 0.014285906217992306 + 0.01 * 6.728528022766113
Epoch 550, val loss: 0.9351722598075867
Epoch 560, training loss: 0.08052533864974976 = 0.013488255441188812 + 0.01 * 6.703708171844482
Epoch 560, val loss: 0.944273054599762
Epoch 570, training loss: 0.07969729602336884 = 0.012758079916238785 + 0.01 * 6.6939215660095215
Epoch 570, val loss: 0.9531397223472595
Epoch 580, training loss: 0.07905679941177368 = 0.012088626623153687 + 0.01 * 6.696817398071289
Epoch 580, val loss: 0.9617264270782471
Epoch 590, training loss: 0.07823623716831207 = 0.011473107151687145 + 0.01 * 6.676313400268555
Epoch 590, val loss: 0.970138669013977
Epoch 600, training loss: 0.07758908718824387 = 0.01090683788061142 + 0.01 * 6.668225288391113
Epoch 600, val loss: 0.97835773229599
Epoch 610, training loss: 0.07708188146352768 = 0.010384706780314445 + 0.01 * 6.669717311859131
Epoch 610, val loss: 0.9862659573554993
Epoch 620, training loss: 0.07654228061437607 = 0.009901813231408596 + 0.01 * 6.664046764373779
Epoch 620, val loss: 0.9940729141235352
Epoch 630, training loss: 0.07592171430587769 = 0.009454534389078617 + 0.01 * 6.6467180252075195
Epoch 630, val loss: 1.0016294717788696
Epoch 640, training loss: 0.07552707195281982 = 0.009039539843797684 + 0.01 * 6.648752689361572
Epoch 640, val loss: 1.008971095085144
Epoch 650, training loss: 0.07514563202857971 = 0.008653189055621624 + 0.01 * 6.649244785308838
Epoch 650, val loss: 1.016234040260315
Epoch 660, training loss: 0.07454852759838104 = 0.008293681778013706 + 0.01 * 6.625484466552734
Epoch 660, val loss: 1.0232453346252441
Epoch 670, training loss: 0.07407859712839127 = 0.007958396337926388 + 0.01 * 6.612020015716553
Epoch 670, val loss: 1.0301077365875244
Epoch 680, training loss: 0.07373685389757156 = 0.007644674275070429 + 0.01 * 6.609218120574951
Epoch 680, val loss: 1.036810040473938
Epoch 690, training loss: 0.07361017912626266 = 0.007350971456617117 + 0.01 * 6.62592077255249
Epoch 690, val loss: 1.0434051752090454
Epoch 700, training loss: 0.07308345288038254 = 0.007076263427734375 + 0.01 * 6.600718975067139
Epoch 700, val loss: 1.0497881174087524
Epoch 710, training loss: 0.07277475297451019 = 0.0068182614631950855 + 0.01 * 6.595648765563965
Epoch 710, val loss: 1.0560429096221924
Epoch 720, training loss: 0.07252448052167892 = 0.006575662177056074 + 0.01 * 6.594882488250732
Epoch 720, val loss: 1.0621381998062134
Epoch 730, training loss: 0.07233414053916931 = 0.006347603630274534 + 0.01 * 6.598653793334961
Epoch 730, val loss: 1.0681425333023071
Epoch 740, training loss: 0.07195872813463211 = 0.006132391281425953 + 0.01 * 6.5826334953308105
Epoch 740, val loss: 1.0739511251449585
Epoch 750, training loss: 0.07166869193315506 = 0.005929284263402224 + 0.01 * 6.573940753936768
Epoch 750, val loss: 1.0797176361083984
Epoch 760, training loss: 0.07160608470439911 = 0.0057375552132725716 + 0.01 * 6.58685302734375
Epoch 760, val loss: 1.0852736234664917
Epoch 770, training loss: 0.07111315429210663 = 0.005556461866945028 + 0.01 * 6.555668830871582
Epoch 770, val loss: 1.0907995700836182
Epoch 780, training loss: 0.07104096561670303 = 0.005384845193475485 + 0.01 * 6.565611839294434
Epoch 780, val loss: 1.096212387084961
Epoch 790, training loss: 0.07081509381532669 = 0.0052223424427211285 + 0.01 * 6.559275150299072
Epoch 790, val loss: 1.1014230251312256
Epoch 800, training loss: 0.0705818384885788 = 0.005068102851510048 + 0.01 * 6.5513739585876465
Epoch 800, val loss: 1.1066312789916992
Epoch 810, training loss: 0.07047092169523239 = 0.004921650048345327 + 0.01 * 6.554926872253418
Epoch 810, val loss: 1.1117074489593506
Epoch 820, training loss: 0.07031260430812836 = 0.004782658535987139 + 0.01 * 6.552994728088379
Epoch 820, val loss: 1.1166878938674927
Epoch 830, training loss: 0.07014010846614838 = 0.004650368355214596 + 0.01 * 6.54897403717041
Epoch 830, val loss: 1.121525764465332
Epoch 840, training loss: 0.06986171007156372 = 0.004524288233369589 + 0.01 * 6.533742427825928
Epoch 840, val loss: 1.1263116598129272
Epoch 850, training loss: 0.06968847662210464 = 0.004404246807098389 + 0.01 * 6.528423309326172
Epoch 850, val loss: 1.1310175657272339
Epoch 860, training loss: 0.06953506916761398 = 0.0042897779494524 + 0.01 * 6.524529457092285
Epoch 860, val loss: 1.135663390159607
Epoch 870, training loss: 0.06934861093759537 = 0.004180483985692263 + 0.01 * 6.516812801361084
Epoch 870, val loss: 1.140173316001892
Epoch 880, training loss: 0.06929171085357666 = 0.004076167941093445 + 0.01 * 6.521554470062256
Epoch 880, val loss: 1.1445916891098022
Epoch 890, training loss: 0.06940903514623642 = 0.003976361360400915 + 0.01 * 6.543267726898193
Epoch 890, val loss: 1.148964524269104
Epoch 900, training loss: 0.06917111575603485 = 0.00388114876113832 + 0.01 * 6.528996467590332
Epoch 900, val loss: 1.1532799005508423
Epoch 910, training loss: 0.06890670955181122 = 0.003790010930970311 + 0.01 * 6.511670112609863
Epoch 910, val loss: 1.157477855682373
Epoch 920, training loss: 0.06887241452932358 = 0.003702705493196845 + 0.01 * 6.516970634460449
Epoch 920, val loss: 1.1615900993347168
Epoch 930, training loss: 0.06888023763895035 = 0.0036190852988511324 + 0.01 * 6.526115894317627
Epoch 930, val loss: 1.1656386852264404
Epoch 940, training loss: 0.06847410649061203 = 0.0035389310214668512 + 0.01 * 6.493517875671387
Epoch 940, val loss: 1.169667363166809
Epoch 950, training loss: 0.06862007081508636 = 0.0034619728103280067 + 0.01 * 6.515810012817383
Epoch 950, val loss: 1.17361319065094
Epoch 960, training loss: 0.06833592057228088 = 0.003388145472854376 + 0.01 * 6.494777679443359
Epoch 960, val loss: 1.1773970127105713
Epoch 970, training loss: 0.06827449053525925 = 0.00331723690032959 + 0.01 * 6.495725154876709
Epoch 970, val loss: 1.181210994720459
Epoch 980, training loss: 0.0681200698018074 = 0.0032491530291736126 + 0.01 * 6.487091541290283
Epoch 980, val loss: 1.1849466562271118
Epoch 990, training loss: 0.06815830618143082 = 0.0031836656853556633 + 0.01 * 6.497464656829834
Epoch 990, val loss: 1.188600778579712
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9041
Flip ASR: 0.8844/225 nodes
The final ASR:0.79705, 0.08591, Accuracy:0.81728, 0.01429
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11616])
remove edge: torch.Size([2, 9512])
updated graph: torch.Size([2, 10572])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98032, 0.00627, Accuracy:0.82840, 0.00462
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.030538558959961 = 1.9467995166778564 + 0.01 * 8.37391471862793
Epoch 0, val loss: 1.9363892078399658
Epoch 10, training loss: 2.019618511199951 = 1.9358799457550049 + 0.01 * 8.373856544494629
Epoch 10, val loss: 1.9262102842330933
Epoch 20, training loss: 2.006340265274048 = 1.9226036071777344 + 0.01 * 8.3736572265625
Epoch 20, val loss: 1.9134836196899414
Epoch 30, training loss: 1.9880694150924683 = 1.904339075088501 + 0.01 * 8.373028755187988
Epoch 30, val loss: 1.8957405090332031
Epoch 40, training loss: 1.961778163909912 = 1.8780810832977295 + 0.01 * 8.36970329284668
Epoch 40, val loss: 1.8705202341079712
Epoch 50, training loss: 1.9255298376083374 = 1.8420796394348145 + 0.01 * 8.345023155212402
Epoch 50, val loss: 1.8378093242645264
Epoch 60, training loss: 1.8836965560913086 = 1.801525354385376 + 0.01 * 8.217119216918945
Epoch 60, val loss: 1.805734634399414
Epoch 70, training loss: 1.8444416522979736 = 1.7652952671051025 + 0.01 * 7.914638996124268
Epoch 70, val loss: 1.78012216091156
Epoch 80, training loss: 1.7978018522262573 = 1.7205572128295898 + 0.01 * 7.724458694458008
Epoch 80, val loss: 1.742324709892273
Epoch 90, training loss: 1.7327179908752441 = 1.6579958200454712 + 0.01 * 7.472214221954346
Epoch 90, val loss: 1.6878361701965332
Epoch 100, training loss: 1.6479225158691406 = 1.575578212738037 + 0.01 * 7.23443603515625
Epoch 100, val loss: 1.619432806968689
Epoch 110, training loss: 1.5527257919311523 = 1.4815480709075928 + 0.01 * 7.117766857147217
Epoch 110, val loss: 1.544211506843567
Epoch 120, training loss: 1.4578633308410645 = 1.387118935585022 + 0.01 * 7.0744404792785645
Epoch 120, val loss: 1.4700219631195068
Epoch 130, training loss: 1.3655674457550049 = 1.2951550483703613 + 0.01 * 7.041240215301514
Epoch 130, val loss: 1.3994413614273071
Epoch 140, training loss: 1.273952603340149 = 1.2038495540618896 + 0.01 * 7.010310173034668
Epoch 140, val loss: 1.33045494556427
Epoch 150, training loss: 1.182612419128418 = 1.11277174949646 + 0.01 * 6.984065532684326
Epoch 150, val loss: 1.26261305809021
Epoch 160, training loss: 1.093881607055664 = 1.0242847204208374 + 0.01 * 6.959686756134033
Epoch 160, val loss: 1.197927713394165
Epoch 170, training loss: 1.0098323822021484 = 0.9404590725898743 + 0.01 * 6.9373297691345215
Epoch 170, val loss: 1.1377416849136353
Epoch 180, training loss: 0.9310839772224426 = 0.8618419766426086 + 0.01 * 6.924200057983398
Epoch 180, val loss: 1.0816478729248047
Epoch 190, training loss: 0.8574628829956055 = 0.7883060574531555 + 0.01 * 6.915683269500732
Epoch 190, val loss: 1.0295755863189697
Epoch 200, training loss: 0.7892113924026489 = 0.7200682759284973 + 0.01 * 6.914312839508057
Epoch 200, val loss: 0.9816914796829224
Epoch 210, training loss: 0.7263666391372681 = 0.6572260856628418 + 0.01 * 6.914054870605469
Epoch 210, val loss: 0.9387234449386597
Epoch 220, training loss: 0.6680143475532532 = 0.598868727684021 + 0.01 * 6.914560794830322
Epoch 220, val loss: 0.9003832340240479
Epoch 230, training loss: 0.6124633550643921 = 0.5433095097541809 + 0.01 * 6.915386199951172
Epoch 230, val loss: 0.8654526472091675
Epoch 240, training loss: 0.5581169724464417 = 0.4889533221721649 + 0.01 * 6.916366100311279
Epoch 240, val loss: 0.8330078125
Epoch 250, training loss: 0.5044874548912048 = 0.43531179428100586 + 0.01 * 6.917564868927002
Epoch 250, val loss: 0.8034020066261292
Epoch 260, training loss: 0.4519663453102112 = 0.38277071714401245 + 0.01 * 6.919562816619873
Epoch 260, val loss: 0.7770861387252808
Epoch 270, training loss: 0.40141820907592773 = 0.3322078585624695 + 0.01 * 6.921034812927246
Epoch 270, val loss: 0.7547409534454346
Epoch 280, training loss: 0.3541224002838135 = 0.2848953902721405 + 0.01 * 6.922701358795166
Epoch 280, val loss: 0.7366994023323059
Epoch 290, training loss: 0.311414897441864 = 0.24217121303081512 + 0.01 * 6.924368858337402
Epoch 290, val loss: 0.7234245538711548
Epoch 300, training loss: 0.2743503749370575 = 0.20509235560894012 + 0.01 * 6.925801753997803
Epoch 300, val loss: 0.7154179811477661
Epoch 310, training loss: 0.24322912096977234 = 0.1739715188741684 + 0.01 * 6.925759315490723
Epoch 310, val loss: 0.7125105261802673
Epoch 320, training loss: 0.21765820682048798 = 0.14839699864387512 + 0.01 * 6.926121234893799
Epoch 320, val loss: 0.714227557182312
Epoch 330, training loss: 0.19680006802082062 = 0.1275465339422226 + 0.01 * 6.925353527069092
Epoch 330, val loss: 0.71966952085495
Epoch 340, training loss: 0.17973878979682922 = 0.11050504446029663 + 0.01 * 6.923375606536865
Epoch 340, val loss: 0.7280465364456177
Epoch 350, training loss: 0.16567359864711761 = 0.09645307809114456 + 0.01 * 6.922052383422852
Epoch 350, val loss: 0.738538920879364
Epoch 360, training loss: 0.15392687916755676 = 0.08474469929933548 + 0.01 * 6.918217658996582
Epoch 360, val loss: 0.7504137754440308
Epoch 370, training loss: 0.144081711769104 = 0.07489442080259323 + 0.01 * 6.918728351593018
Epoch 370, val loss: 0.7631706595420837
Epoch 380, training loss: 0.13562776148319244 = 0.06653179228305817 + 0.01 * 6.909597396850586
Epoch 380, val loss: 0.7763670682907104
Epoch 390, training loss: 0.12850342690944672 = 0.05937732756137848 + 0.01 * 6.912610054016113
Epoch 390, val loss: 0.789710283279419
Epoch 400, training loss: 0.12220150977373123 = 0.05322521924972534 + 0.01 * 6.897629261016846
Epoch 400, val loss: 0.8030291199684143
Epoch 410, training loss: 0.11679284274578094 = 0.047908663749694824 + 0.01 * 6.888418197631836
Epoch 410, val loss: 0.8160863518714905
Epoch 420, training loss: 0.11208322644233704 = 0.043292272835969925 + 0.01 * 6.879095077514648
Epoch 420, val loss: 0.8289324045181274
Epoch 430, training loss: 0.10802130401134491 = 0.03926467150449753 + 0.01 * 6.8756632804870605
Epoch 430, val loss: 0.8413764834403992
Epoch 440, training loss: 0.10435132682323456 = 0.035738859325647354 + 0.01 * 6.861247539520264
Epoch 440, val loss: 0.8535846471786499
Epoch 450, training loss: 0.10157545655965805 = 0.03264019638299942 + 0.01 * 6.893526077270508
Epoch 450, val loss: 0.8653780817985535
Epoch 460, training loss: 0.09837822616100311 = 0.02991313301026821 + 0.01 * 6.8465094566345215
Epoch 460, val loss: 0.8769142627716064
Epoch 470, training loss: 0.09588281065225601 = 0.02750188112258911 + 0.01 * 6.838093280792236
Epoch 470, val loss: 0.8879885077476501
Epoch 480, training loss: 0.09351642429828644 = 0.025363784283399582 + 0.01 * 6.8152642250061035
Epoch 480, val loss: 0.8988144993782043
Epoch 490, training loss: 0.09120280295610428 = 0.023460863158106804 + 0.01 * 6.774194240570068
Epoch 490, val loss: 0.9094254374504089
Epoch 500, training loss: 0.08972828835248947 = 0.021765582263469696 + 0.01 * 6.796270847320557
Epoch 500, val loss: 0.9194632172584534
Epoch 510, training loss: 0.0879124104976654 = 0.020250948145985603 + 0.01 * 6.766146659851074
Epoch 510, val loss: 0.9293321967124939
Epoch 520, training loss: 0.0862693190574646 = 0.01888943836092949 + 0.01 * 6.7379889488220215
Epoch 520, val loss: 0.9389371871948242
Epoch 530, training loss: 0.0849376916885376 = 0.0176624096930027 + 0.01 * 6.727528095245361
Epoch 530, val loss: 0.9481812715530396
Epoch 540, training loss: 0.08421409130096436 = 0.01655084267258644 + 0.01 * 6.766324996948242
Epoch 540, val loss: 0.9571000933647156
Epoch 550, training loss: 0.08257362991571426 = 0.01554532814770937 + 0.01 * 6.7028303146362305
Epoch 550, val loss: 0.9659249782562256
Epoch 560, training loss: 0.081511490046978 = 0.014632215723395348 + 0.01 * 6.68792724609375
Epoch 560, val loss: 0.9742586612701416
Epoch 570, training loss: 0.08053392171859741 = 0.013798915781080723 + 0.01 * 6.6735005378723145
Epoch 570, val loss: 0.9824790954589844
Epoch 580, training loss: 0.07969749718904495 = 0.013036763295531273 + 0.01 * 6.666073322296143
Epoch 580, val loss: 0.9905297160148621
Epoch 590, training loss: 0.07905402779579163 = 0.01233704760670662 + 0.01 * 6.671698093414307
Epoch 590, val loss: 0.9982132315635681
Epoch 600, training loss: 0.07828835397958755 = 0.011695223860442638 + 0.01 * 6.659313201904297
Epoch 600, val loss: 1.0059412717819214
Epoch 610, training loss: 0.07769188284873962 = 0.011104170233011246 + 0.01 * 6.658771991729736
Epoch 610, val loss: 1.013206124305725
Epoch 620, training loss: 0.0770646333694458 = 0.010558639653027058 + 0.01 * 6.650599002838135
Epoch 620, val loss: 1.0205459594726562
Epoch 630, training loss: 0.07644599676132202 = 0.010054427199065685 + 0.01 * 6.639157295227051
Epoch 630, val loss: 1.0274418592453003
Epoch 640, training loss: 0.07588295638561249 = 0.00958697684109211 + 0.01 * 6.629598140716553
Epoch 640, val loss: 1.034399151802063
Epoch 650, training loss: 0.07572396099567413 = 0.009152969345450401 + 0.01 * 6.657098770141602
Epoch 650, val loss: 1.040984869003296
Epoch 660, training loss: 0.07499420642852783 = 0.008749660104513168 + 0.01 * 6.624455451965332
Epoch 660, val loss: 1.0474592447280884
Epoch 670, training loss: 0.07445237785577774 = 0.008374331519007683 + 0.01 * 6.607804775238037
Epoch 670, val loss: 1.0537768602371216
Epoch 680, training loss: 0.07433649152517319 = 0.008023514412343502 + 0.01 * 6.631298065185547
Epoch 680, val loss: 1.0600216388702393
Epoch 690, training loss: 0.07365339994430542 = 0.00769617548212409 + 0.01 * 6.595722675323486
Epoch 690, val loss: 1.0658818483352661
Epoch 700, training loss: 0.07340063899755478 = 0.007389365695416927 + 0.01 * 6.6011271476745605
Epoch 700, val loss: 1.0718019008636475
Epoch 710, training loss: 0.0729961022734642 = 0.007102516014128923 + 0.01 * 6.589358329772949
Epoch 710, val loss: 1.0774927139282227
Epoch 720, training loss: 0.07287108898162842 = 0.006832730956375599 + 0.01 * 6.603835582733154
Epoch 720, val loss: 1.0831135511398315
Epoch 730, training loss: 0.07241947948932648 = 0.006579932756721973 + 0.01 * 6.583954334259033
Epoch 730, val loss: 1.0885584354400635
Epoch 740, training loss: 0.07229387760162354 = 0.006341269239783287 + 0.01 * 6.595261096954346
Epoch 740, val loss: 1.0938862562179565
Epoch 750, training loss: 0.07188396155834198 = 0.006117312237620354 + 0.01 * 6.576664924621582
Epoch 750, val loss: 1.0988709926605225
Epoch 760, training loss: 0.07164843380451202 = 0.005906412843614817 + 0.01 * 6.574202537536621
Epoch 760, val loss: 1.1040202379226685
Epoch 770, training loss: 0.07130490988492966 = 0.005706897005438805 + 0.01 * 6.5598015785217285
Epoch 770, val loss: 1.1089506149291992
Epoch 780, training loss: 0.07113419473171234 = 0.005518035963177681 + 0.01 * 6.56161642074585
Epoch 780, val loss: 1.113699197769165
Epoch 790, training loss: 0.07112271338701248 = 0.00533978221938014 + 0.01 * 6.578293800354004
Epoch 790, val loss: 1.1184196472167969
Epoch 800, training loss: 0.0708189383149147 = 0.005170789081603289 + 0.01 * 6.564814567565918
Epoch 800, val loss: 1.1229737997055054
Epoch 810, training loss: 0.07081723213195801 = 0.0050107515417039394 + 0.01 * 6.580647945404053
Epoch 810, val loss: 1.1274833679199219
Epoch 820, training loss: 0.07024147361516953 = 0.00485902838408947 + 0.01 * 6.538244724273682
Epoch 820, val loss: 1.131784200668335
Epoch 830, training loss: 0.0702049732208252 = 0.004715226124972105 + 0.01 * 6.548974990844727
Epoch 830, val loss: 1.1360458135604858
Epoch 840, training loss: 0.06995907425880432 = 0.004578433930873871 + 0.01 * 6.538064479827881
Epoch 840, val loss: 1.1402599811553955
Epoch 850, training loss: 0.07017925381660461 = 0.004448268562555313 + 0.01 * 6.573098182678223
Epoch 850, val loss: 1.1443485021591187
Epoch 860, training loss: 0.06964337825775146 = 0.004324397072196007 + 0.01 * 6.531898021697998
Epoch 860, val loss: 1.1482384204864502
Epoch 870, training loss: 0.0695941224694252 = 0.004206900950521231 + 0.01 * 6.538722038269043
Epoch 870, val loss: 1.1522300243377686
Epoch 880, training loss: 0.0693252682685852 = 0.004094199743121862 + 0.01 * 6.523107051849365
Epoch 880, val loss: 1.1558910608291626
Epoch 890, training loss: 0.06916306167840958 = 0.003987741190940142 + 0.01 * 6.517531871795654
Epoch 890, val loss: 1.1598596572875977
Epoch 900, training loss: 0.06908997148275375 = 0.003885142970830202 + 0.01 * 6.520483016967773
Epoch 900, val loss: 1.163255214691162
Epoch 910, training loss: 0.06918468326330185 = 0.003787643276154995 + 0.01 * 6.539704322814941
Epoch 910, val loss: 1.167029619216919
Epoch 920, training loss: 0.06874154508113861 = 0.0036938234698027372 + 0.01 * 6.504772186279297
Epoch 920, val loss: 1.170346736907959
Epoch 930, training loss: 0.06861309707164764 = 0.003604917088523507 + 0.01 * 6.500818252563477
Epoch 930, val loss: 1.1739524602890015
Epoch 940, training loss: 0.06861284375190735 = 0.0035191653296351433 + 0.01 * 6.509367942810059
Epoch 940, val loss: 1.1770333051681519
Epoch 950, training loss: 0.06853269785642624 = 0.003437548177316785 + 0.01 * 6.509515285491943
Epoch 950, val loss: 1.1805275678634644
Epoch 960, training loss: 0.06848283112049103 = 0.0033591347746551037 + 0.01 * 6.5123701095581055
Epoch 960, val loss: 1.183716893196106
Epoch 970, training loss: 0.06823333352804184 = 0.003283619647845626 + 0.01 * 6.494971752166748
Epoch 970, val loss: 1.1868188381195068
Epoch 980, training loss: 0.06811662763357162 = 0.0032116002403199673 + 0.01 * 6.490502834320068
Epoch 980, val loss: 1.1899892091751099
Epoch 990, training loss: 0.06789325922727585 = 0.0031418746802955866 + 0.01 * 6.475139141082764
Epoch 990, val loss: 1.192832589149475
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.6421
Flip ASR: 0.5733/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.017068386077881 = 1.9333299398422241 + 0.01 * 8.37385368347168
Epoch 0, val loss: 1.9240082502365112
Epoch 10, training loss: 2.006532669067383 = 1.922795057296753 + 0.01 * 8.373773574829102
Epoch 10, val loss: 1.913374423980713
Epoch 20, training loss: 1.9933478832244873 = 1.909613013267517 + 0.01 * 8.37348461151123
Epoch 20, val loss: 1.8995633125305176
Epoch 30, training loss: 1.9747505187988281 = 1.891026258468628 + 0.01 * 8.372431755065918
Epoch 30, val loss: 1.8798279762268066
Epoch 40, training loss: 1.9478206634521484 = 1.8641693592071533 + 0.01 * 8.36513614654541
Epoch 40, val loss: 1.851690649986267
Epoch 50, training loss: 1.9117287397384644 = 1.8285804986953735 + 0.01 * 8.314828872680664
Epoch 50, val loss: 1.8166909217834473
Epoch 60, training loss: 1.8707199096679688 = 1.7902785539627075 + 0.01 * 8.044135093688965
Epoch 60, val loss: 1.7836192846298218
Epoch 70, training loss: 1.8270317316055298 = 1.7481859922409058 + 0.01 * 7.884573459625244
Epoch 70, val loss: 1.7494856119155884
Epoch 80, training loss: 1.7662409543991089 = 1.6894594430923462 + 0.01 * 7.6781535148620605
Epoch 80, val loss: 1.7002708911895752
Epoch 90, training loss: 1.684570074081421 = 1.6099905967712402 + 0.01 * 7.457947731018066
Epoch 90, val loss: 1.63224458694458
Epoch 100, training loss: 1.5892086029052734 = 1.516880750656128 + 0.01 * 7.232779502868652
Epoch 100, val loss: 1.5537333488464355
Epoch 110, training loss: 1.4954543113708496 = 1.4240771532058716 + 0.01 * 7.137710094451904
Epoch 110, val loss: 1.4797348976135254
Epoch 120, training loss: 1.406754493713379 = 1.33586847782135 + 0.01 * 7.088598728179932
Epoch 120, val loss: 1.4166315793991089
Epoch 130, training loss: 1.3208714723587036 = 1.2503262758255005 + 0.01 * 7.0545220375061035
Epoch 130, val loss: 1.3607728481292725
Epoch 140, training loss: 1.238069772720337 = 1.1677398681640625 + 0.01 * 7.032996654510498
Epoch 140, val loss: 1.3088096380233765
Epoch 150, training loss: 1.1598633527755737 = 1.0896520614624023 + 0.01 * 7.021124362945557
Epoch 150, val loss: 1.2607403993606567
Epoch 160, training loss: 1.0867112874984741 = 1.0165740251541138 + 0.01 * 7.013728141784668
Epoch 160, val loss: 1.215755581855774
Epoch 170, training loss: 1.0175968408584595 = 0.9475539326667786 + 0.01 * 7.004295825958252
Epoch 170, val loss: 1.1729141473770142
Epoch 180, training loss: 0.9507922530174255 = 0.8808858394622803 + 0.01 * 6.990642547607422
Epoch 180, val loss: 1.1306090354919434
Epoch 190, training loss: 0.885341227054596 = 0.8156164288520813 + 0.01 * 6.972479343414307
Epoch 190, val loss: 1.0881098508834839
Epoch 200, training loss: 0.8220828771591187 = 0.7525650858879089 + 0.01 * 6.951776027679443
Epoch 200, val loss: 1.0464195013046265
Epoch 210, training loss: 0.7631866931915283 = 0.6938738226890564 + 0.01 * 6.931285381317139
Epoch 210, val loss: 1.0076063871383667
Epoch 220, training loss: 0.7101898789405823 = 0.6410138010978699 + 0.01 * 6.917607307434082
Epoch 220, val loss: 0.9736064672470093
Epoch 230, training loss: 0.6627382040023804 = 0.5936517715454102 + 0.01 * 6.908640384674072
Epoch 230, val loss: 0.9444105625152588
Epoch 240, training loss: 0.6194709539413452 = 0.5504543781280518 + 0.01 * 6.901656627655029
Epoch 240, val loss: 0.9190612435340881
Epoch 250, training loss: 0.5792919397354126 = 0.5103362798690796 + 0.01 * 6.895565986633301
Epoch 250, val loss: 0.8965907692909241
Epoch 260, training loss: 0.5416040420532227 = 0.4726771414279938 + 0.01 * 6.892693042755127
Epoch 260, val loss: 0.8762021064758301
Epoch 270, training loss: 0.5055874586105347 = 0.4367228150367737 + 0.01 * 6.886466979980469
Epoch 270, val loss: 0.8572220206260681
Epoch 280, training loss: 0.4704399108886719 = 0.40160495042800903 + 0.01 * 6.883497714996338
Epoch 280, val loss: 0.839316725730896
Epoch 290, training loss: 0.4353988766670227 = 0.3666088581085205 + 0.01 * 6.879003524780273
Epoch 290, val loss: 0.8222067952156067
Epoch 300, training loss: 0.40052974224090576 = 0.3317764699459076 + 0.01 * 6.875327110290527
Epoch 300, val loss: 0.8067137598991394
Epoch 310, training loss: 0.3665693402290344 = 0.29782411456108093 + 0.01 * 6.8745222091674805
Epoch 310, val loss: 0.7934706807136536
Epoch 320, training loss: 0.3341549038887024 = 0.2654712498188019 + 0.01 * 6.868365287780762
Epoch 320, val loss: 0.7827932238578796
Epoch 330, training loss: 0.30411505699157715 = 0.23550769686698914 + 0.01 * 6.8607378005981445
Epoch 330, val loss: 0.7753395438194275
Epoch 340, training loss: 0.27718549966812134 = 0.20848332345485687 + 0.01 * 6.870216369628906
Epoch 340, val loss: 0.7712612152099609
Epoch 350, training loss: 0.2530847489833832 = 0.184565931558609 + 0.01 * 6.851881980895996
Epoch 350, val loss: 0.7700560688972473
Epoch 360, training loss: 0.23200541734695435 = 0.1635836809873581 + 0.01 * 6.842174530029297
Epoch 360, val loss: 0.7716363668441772
Epoch 370, training loss: 0.2136395126581192 = 0.14517833292484283 + 0.01 * 6.846118450164795
Epoch 370, val loss: 0.7755445837974548
Epoch 380, training loss: 0.19732901453971863 = 0.12905429303646088 + 0.01 * 6.82747220993042
Epoch 380, val loss: 0.7813937664031982
Epoch 390, training loss: 0.18297350406646729 = 0.11482520401477814 + 0.01 * 6.814830780029297
Epoch 390, val loss: 0.788628339767456
Epoch 400, training loss: 0.17044895887374878 = 0.10220520198345184 + 0.01 * 6.824375152587891
Epoch 400, val loss: 0.7969081997871399
Epoch 410, training loss: 0.1588885635137558 = 0.09088712930679321 + 0.01 * 6.800143241882324
Epoch 410, val loss: 0.8061611652374268
Epoch 420, training loss: 0.1486382931470871 = 0.08076807111501694 + 0.01 * 6.787022113800049
Epoch 420, val loss: 0.8159770369529724
Epoch 430, training loss: 0.13965821266174316 = 0.0716446042060852 + 0.01 * 6.801360607147217
Epoch 430, val loss: 0.8261995315551758
Epoch 440, training loss: 0.13080240786075592 = 0.06317189335823059 + 0.01 * 6.763051509857178
Epoch 440, val loss: 0.836566686630249
Epoch 450, training loss: 0.12251052260398865 = 0.05496596917510033 + 0.01 * 6.754456043243408
Epoch 450, val loss: 0.8473497033119202
Epoch 460, training loss: 0.1144413948059082 = 0.04688949137926102 + 0.01 * 6.755190372467041
Epoch 460, val loss: 0.858474850654602
Epoch 470, training loss: 0.10706990957260132 = 0.03964121267199516 + 0.01 * 6.7428693771362305
Epoch 470, val loss: 0.8703818917274475
Epoch 480, training loss: 0.10166830569505692 = 0.034260913729667664 + 0.01 * 6.740739345550537
Epoch 480, val loss: 0.8833179473876953
Epoch 490, training loss: 0.09749409556388855 = 0.030413690954446793 + 0.01 * 6.708041191101074
Epoch 490, val loss: 0.8969765305519104
Epoch 500, training loss: 0.0945689007639885 = 0.027347145602107048 + 0.01 * 6.722175598144531
Epoch 500, val loss: 0.9101383686065674
Epoch 510, training loss: 0.09164674580097198 = 0.024776728823781013 + 0.01 * 6.687001705169678
Epoch 510, val loss: 0.9224531650543213
Epoch 520, training loss: 0.08979351818561554 = 0.02258187346160412 + 0.01 * 6.721164226531982
Epoch 520, val loss: 0.9340853095054626
Epoch 530, training loss: 0.0875198021531105 = 0.020697729662060738 + 0.01 * 6.6822075843811035
Epoch 530, val loss: 0.9451591372489929
Epoch 540, training loss: 0.08581738919019699 = 0.019060974940657616 + 0.01 * 6.6756415367126465
Epoch 540, val loss: 0.9558663964271545
Epoch 550, training loss: 0.0843370258808136 = 0.017630457878112793 + 0.01 * 6.670656681060791
Epoch 550, val loss: 0.9662808179855347
Epoch 560, training loss: 0.08284405618906021 = 0.016367392614483833 + 0.01 * 6.6476664543151855
Epoch 560, val loss: 0.9763434529304504
Epoch 570, training loss: 0.08173462003469467 = 0.015246654860675335 + 0.01 * 6.648797035217285
Epoch 570, val loss: 0.9861255884170532
Epoch 580, training loss: 0.08085165172815323 = 0.014247119426727295 + 0.01 * 6.6604533195495605
Epoch 580, val loss: 0.9955474734306335
Epoch 590, training loss: 0.07957997918128967 = 0.013350842520594597 + 0.01 * 6.622913360595703
Epoch 590, val loss: 1.0046035051345825
Epoch 600, training loss: 0.07892459630966187 = 0.012543448247015476 + 0.01 * 6.638114929199219
Epoch 600, val loss: 1.013350009918213
Epoch 610, training loss: 0.07783946394920349 = 0.011814277619123459 + 0.01 * 6.602519512176514
Epoch 610, val loss: 1.0218456983566284
Epoch 620, training loss: 0.07719648629426956 = 0.011152329854667187 + 0.01 * 6.604416370391846
Epoch 620, val loss: 1.0300776958465576
Epoch 630, training loss: 0.07665880024433136 = 0.010549451224505901 + 0.01 * 6.610935688018799
Epoch 630, val loss: 1.0380462408065796
Epoch 640, training loss: 0.07587544620037079 = 0.00999926496297121 + 0.01 * 6.587618350982666
Epoch 640, val loss: 1.0457971096038818
Epoch 650, training loss: 0.07531934976577759 = 0.00949510745704174 + 0.01 * 6.582424163818359
Epoch 650, val loss: 1.0533143281936646
Epoch 660, training loss: 0.07492969930171967 = 0.00903194211423397 + 0.01 * 6.589776039123535
Epoch 660, val loss: 1.0605648756027222
Epoch 670, training loss: 0.07434585690498352 = 0.00860627181828022 + 0.01 * 6.573958873748779
Epoch 670, val loss: 1.0676062107086182
Epoch 680, training loss: 0.07400238513946533 = 0.00821282621473074 + 0.01 * 6.578956127166748
Epoch 680, val loss: 1.0744644403457642
Epoch 690, training loss: 0.07358020544052124 = 0.007848884910345078 + 0.01 * 6.573132038116455
Epoch 690, val loss: 1.0811022520065308
Epoch 700, training loss: 0.07303214818239212 = 0.007510439492762089 + 0.01 * 6.552170753479004
Epoch 700, val loss: 1.087570071220398
Epoch 710, training loss: 0.07303478568792343 = 0.007196270860731602 + 0.01 * 6.5838518142700195
Epoch 710, val loss: 1.0939275026321411
Epoch 720, training loss: 0.07241300493478775 = 0.006902747787535191 + 0.01 * 6.551025867462158
Epoch 720, val loss: 1.0999356508255005
Epoch 730, training loss: 0.07223829627037048 = 0.00662923464551568 + 0.01 * 6.560906410217285
Epoch 730, val loss: 1.1058968305587769
Epoch 740, training loss: 0.07181154936552048 = 0.0063736275769770145 + 0.01 * 6.543792247772217
Epoch 740, val loss: 1.111794352531433
Epoch 750, training loss: 0.07177808880805969 = 0.006133631803095341 + 0.01 * 6.564445972442627
Epoch 750, val loss: 1.1174309253692627
Epoch 760, training loss: 0.0712476596236229 = 0.005908510182052851 + 0.01 * 6.533915042877197
Epoch 760, val loss: 1.1230279207229614
Epoch 770, training loss: 0.07105837762355804 = 0.005696932319551706 + 0.01 * 6.536144733428955
Epoch 770, val loss: 1.128375768661499
Epoch 780, training loss: 0.07070653885602951 = 0.005497433710843325 + 0.01 * 6.520910739898682
Epoch 780, val loss: 1.13370943069458
Epoch 790, training loss: 0.0706772580742836 = 0.005308814346790314 + 0.01 * 6.536844253540039
Epoch 790, val loss: 1.1389626264572144
Epoch 800, training loss: 0.0703568309545517 = 0.0051313843578100204 + 0.01 * 6.5225443840026855
Epoch 800, val loss: 1.1440354585647583
Epoch 810, training loss: 0.07005444914102554 = 0.004963981918990612 + 0.01 * 6.509047508239746
Epoch 810, val loss: 1.1490074396133423
Epoch 820, training loss: 0.06995666772127151 = 0.004805292002856731 + 0.01 * 6.515138149261475
Epoch 820, val loss: 1.1538209915161133
Epoch 830, training loss: 0.06977803260087967 = 0.004655750468373299 + 0.01 * 6.512228488922119
Epoch 830, val loss: 1.1585482358932495
Epoch 840, training loss: 0.06964689493179321 = 0.0045135971158742905 + 0.01 * 6.513329982757568
Epoch 840, val loss: 1.1632276773452759
Epoch 850, training loss: 0.06942514330148697 = 0.004379520658403635 + 0.01 * 6.5045623779296875
Epoch 850, val loss: 1.1676620244979858
Epoch 860, training loss: 0.069294773042202 = 0.004251490347087383 + 0.01 * 6.504328727722168
Epoch 860, val loss: 1.1721447706222534
Epoch 870, training loss: 0.06926067918539047 = 0.004130493383854628 + 0.01 * 6.51301908493042
Epoch 870, val loss: 1.1764522790908813
Epoch 880, training loss: 0.06899601966142654 = 0.00401472020894289 + 0.01 * 6.498129844665527
Epoch 880, val loss: 1.180670142173767
Epoch 890, training loss: 0.06887038797140121 = 0.0039051894564181566 + 0.01 * 6.496520042419434
Epoch 890, val loss: 1.184792399406433
Epoch 900, training loss: 0.06852518767118454 = 0.003800527425482869 + 0.01 * 6.472466468811035
Epoch 900, val loss: 1.188909649848938
Epoch 910, training loss: 0.06853578239679337 = 0.003700598608702421 + 0.01 * 6.483518600463867
Epoch 910, val loss: 1.1928801536560059
Epoch 920, training loss: 0.06833825260400772 = 0.003605840029194951 + 0.01 * 6.473241329193115
Epoch 920, val loss: 1.1967014074325562
Epoch 930, training loss: 0.06819083541631699 = 0.003514830721542239 + 0.01 * 6.467600345611572
Epoch 930, val loss: 1.2005432844161987
Epoch 940, training loss: 0.06838550418615341 = 0.0034285460133105516 + 0.01 * 6.495696067810059
Epoch 940, val loss: 1.2042287588119507
Epoch 950, training loss: 0.0679730772972107 = 0.0033454415388405323 + 0.01 * 6.46276330947876
Epoch 950, val loss: 1.2079070806503296
Epoch 960, training loss: 0.06807249784469604 = 0.003266380401328206 + 0.01 * 6.480611801147461
Epoch 960, val loss: 1.2114791870117188
Epoch 970, training loss: 0.06811194866895676 = 0.003190272720530629 + 0.01 * 6.492167949676514
Epoch 970, val loss: 1.2149444818496704
Epoch 980, training loss: 0.06773129105567932 = 0.003118014894425869 + 0.01 * 6.461328029632568
Epoch 980, val loss: 1.2183963060379028
Epoch 990, training loss: 0.06750936061143875 = 0.0030482355505228043 + 0.01 * 6.446112632751465
Epoch 990, val loss: 1.2217044830322266
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.5498
Flip ASR: 0.4978/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.03251314163208 = 1.9487740993499756 + 0.01 * 8.373907089233398
Epoch 0, val loss: 1.9456669092178345
Epoch 10, training loss: 2.0214667320251465 = 1.9377285242080688 + 0.01 * 8.373832702636719
Epoch 10, val loss: 1.934590458869934
Epoch 20, training loss: 2.007357358932495 = 1.9236210584640503 + 0.01 * 8.373626708984375
Epoch 20, val loss: 1.919740080833435
Epoch 30, training loss: 1.9872300624847412 = 1.9034998416900635 + 0.01 * 8.373027801513672
Epoch 30, val loss: 1.8981366157531738
Epoch 40, training loss: 1.9578602313995361 = 1.8741579055786133 + 0.01 * 8.370237350463867
Epoch 40, val loss: 1.866943359375
Epoch 50, training loss: 1.9180092811584473 = 1.8344807624816895 + 0.01 * 8.35285758972168
Epoch 50, val loss: 1.8269470930099487
Epoch 60, training loss: 1.8747286796569824 = 1.7919259071350098 + 0.01 * 8.28027629852295
Epoch 60, val loss: 1.7892215251922607
Epoch 70, training loss: 1.8338431119918823 = 1.7539232969284058 + 0.01 * 7.991976261138916
Epoch 70, val loss: 1.7593283653259277
Epoch 80, training loss: 1.782698392868042 = 1.7043006420135498 + 0.01 * 7.839773178100586
Epoch 80, val loss: 1.7178775072097778
Epoch 90, training loss: 1.7128368616104126 = 1.6362643241882324 + 0.01 * 7.65725564956665
Epoch 90, val loss: 1.6600230932235718
Epoch 100, training loss: 1.6242108345031738 = 1.549184799194336 + 0.01 * 7.502597808837891
Epoch 100, val loss: 1.5869481563568115
Epoch 110, training loss: 1.5270471572875977 = 1.4537906646728516 + 0.01 * 7.325646877288818
Epoch 110, val loss: 1.5090869665145874
Epoch 120, training loss: 1.4318935871124268 = 1.3599134683609009 + 0.01 * 7.198007583618164
Epoch 120, val loss: 1.436024785041809
Epoch 130, training loss: 1.3390321731567383 = 1.267641305923462 + 0.01 * 7.139085292816162
Epoch 130, val loss: 1.3676300048828125
Epoch 140, training loss: 1.245024561882019 = 1.1739933490753174 + 0.01 * 7.103117942810059
Epoch 140, val loss: 1.300076961517334
Epoch 150, training loss: 1.1483941078186035 = 1.077538013458252 + 0.01 * 7.085607528686523
Epoch 150, val loss: 1.2301996946334839
Epoch 160, training loss: 1.0503438711166382 = 0.9796140193939209 + 0.01 * 7.0729804039001465
Epoch 160, val loss: 1.158145546913147
Epoch 170, training loss: 0.9540975689888 = 0.8834465146064758 + 0.01 * 7.065103530883789
Epoch 170, val loss: 1.0868695974349976
Epoch 180, training loss: 0.8632028698921204 = 0.7926068902015686 + 0.01 * 7.059599876403809
Epoch 180, val loss: 1.0193320512771606
Epoch 190, training loss: 0.7804048657417297 = 0.7098617553710938 + 0.01 * 7.054312229156494
Epoch 190, val loss: 0.9580108523368835
Epoch 200, training loss: 0.7069457173347473 = 0.6364553570747375 + 0.01 * 7.049038410186768
Epoch 200, val loss: 0.904633641242981
Epoch 210, training loss: 0.6428207755088806 = 0.5723803043365479 + 0.01 * 7.0440473556518555
Epoch 210, val loss: 0.8600872755050659
Epoch 220, training loss: 0.587095320224762 = 0.5167069435119629 + 0.01 * 7.038839817047119
Epoch 220, val loss: 0.8235495090484619
Epoch 230, training loss: 0.5383326411247253 = 0.46800926327705383 + 0.01 * 7.032336711883545
Epoch 230, val loss: 0.7943127751350403
Epoch 240, training loss: 0.4948340356349945 = 0.4246067702770233 + 0.01 * 7.022727012634277
Epoch 240, val loss: 0.7715290784835815
Epoch 250, training loss: 0.4546618163585663 = 0.38454174995422363 + 0.01 * 7.012006759643555
Epoch 250, val loss: 0.7535760402679443
Epoch 260, training loss: 0.4159403145313263 = 0.3460097312927246 + 0.01 * 6.993058204650879
Epoch 260, val loss: 0.7385744452476501
Epoch 270, training loss: 0.3777538537979126 = 0.3080131411552429 + 0.01 * 6.974071979522705
Epoch 270, val loss: 0.7256256937980652
Epoch 280, training loss: 0.34016862511634827 = 0.2706400156021118 + 0.01 * 6.9528608322143555
Epoch 280, val loss: 0.7144671678543091
Epoch 290, training loss: 0.3042404055595398 = 0.23487398028373718 + 0.01 * 6.936643600463867
Epoch 290, val loss: 0.7056914567947388
Epoch 300, training loss: 0.27136802673339844 = 0.20208430290222168 + 0.01 * 6.928371429443359
Epoch 300, val loss: 0.699791431427002
Epoch 310, training loss: 0.24251505732536316 = 0.1733415573835373 + 0.01 * 6.9173502922058105
Epoch 310, val loss: 0.6970824599266052
Epoch 320, training loss: 0.21797001361846924 = 0.14889004826545715 + 0.01 * 6.907996654510498
Epoch 320, val loss: 0.6971147060394287
Epoch 330, training loss: 0.19736313819885254 = 0.12834471464157104 + 0.01 * 6.90184211730957
Epoch 330, val loss: 0.6995188593864441
Epoch 340, training loss: 0.18002474308013916 = 0.11110454797744751 + 0.01 * 6.8920207023620605
Epoch 340, val loss: 0.7038542628288269
Epoch 350, training loss: 0.16543135046958923 = 0.0965823084115982 + 0.01 * 6.884903430938721
Epoch 350, val loss: 0.7097886204719543
Epoch 360, training loss: 0.15302687883377075 = 0.08429685235023499 + 0.01 * 6.873003005981445
Epoch 360, val loss: 0.7168336510658264
Epoch 370, training loss: 0.14252746105194092 = 0.07386805862188339 + 0.01 * 6.865941047668457
Epoch 370, val loss: 0.7247307300567627
Epoch 380, training loss: 0.13360154628753662 = 0.06499112397432327 + 0.01 * 6.861042022705078
Epoch 380, val loss: 0.7333164215087891
Epoch 390, training loss: 0.12591210007667542 = 0.05742994323372841 + 0.01 * 6.848215579986572
Epoch 390, val loss: 0.7423248291015625
Epoch 400, training loss: 0.11931966990232468 = 0.050980344414711 + 0.01 * 6.833932876586914
Epoch 400, val loss: 0.7516545653343201
Epoch 410, training loss: 0.11382009088993073 = 0.04546858370304108 + 0.01 * 6.835151195526123
Epoch 410, val loss: 0.76120924949646
Epoch 420, training loss: 0.10890436917543411 = 0.04074748605489731 + 0.01 * 6.815688610076904
Epoch 420, val loss: 0.770881175994873
Epoch 430, training loss: 0.10476276278495789 = 0.036686502397060394 + 0.01 * 6.807626247406006
Epoch 430, val loss: 0.7806178331375122
Epoch 440, training loss: 0.10124188661575317 = 0.03317922726273537 + 0.01 * 6.8062663078308105
Epoch 440, val loss: 0.790358304977417
Epoch 450, training loss: 0.0980721190571785 = 0.030137496069073677 + 0.01 * 6.793462753295898
Epoch 450, val loss: 0.8000853657722473
Epoch 460, training loss: 0.09534662961959839 = 0.02748522162437439 + 0.01 * 6.7861409187316895
Epoch 460, val loss: 0.8097636699676514
Epoch 470, training loss: 0.0929718166589737 = 0.02516203559935093 + 0.01 * 6.780978679656982
Epoch 470, val loss: 0.8193490505218506
Epoch 480, training loss: 0.09087102115154266 = 0.023118741810321808 + 0.01 * 6.775228023529053
Epoch 480, val loss: 0.8288056254386902
Epoch 490, training loss: 0.08905348181724548 = 0.021312614902853966 + 0.01 * 6.7740864753723145
Epoch 490, val loss: 0.838129997253418
Epoch 500, training loss: 0.08735361695289612 = 0.019710572436451912 + 0.01 * 6.7643046379089355
Epoch 500, val loss: 0.8472886085510254
Epoch 510, training loss: 0.08586220443248749 = 0.018283363431692123 + 0.01 * 6.7578840255737305
Epoch 510, val loss: 0.8562527298927307
Epoch 520, training loss: 0.084510937333107 = 0.017005901783704758 + 0.01 * 6.750504016876221
Epoch 520, val loss: 0.8650076985359192
Epoch 530, training loss: 0.08327674865722656 = 0.015858393162488937 + 0.01 * 6.741836071014404
Epoch 530, val loss: 0.8735817074775696
Epoch 540, training loss: 0.08217489719390869 = 0.014822965487837791 + 0.01 * 6.735193252563477
Epoch 540, val loss: 0.8819877505302429
Epoch 550, training loss: 0.08136588335037231 = 0.013886740431189537 + 0.01 * 6.7479143142700195
Epoch 550, val loss: 0.8902117609977722
Epoch 560, training loss: 0.08030901104211807 = 0.013038885779678822 + 0.01 * 6.7270121574401855
Epoch 560, val loss: 0.8982513546943665
Epoch 570, training loss: 0.0794348493218422 = 0.012267930433154106 + 0.01 * 6.716691970825195
Epoch 570, val loss: 0.9060623049736023
Epoch 580, training loss: 0.07870130240917206 = 0.011563687585294247 + 0.01 * 6.713761806488037
Epoch 580, val loss: 0.9137523174285889
Epoch 590, training loss: 0.07799229770898819 = 0.010919436812400818 + 0.01 * 6.707286357879639
Epoch 590, val loss: 0.9212570786476135
Epoch 600, training loss: 0.07739175111055374 = 0.01032905001193285 + 0.01 * 6.706270217895508
Epoch 600, val loss: 0.9285807609558105
Epoch 610, training loss: 0.07675550132989883 = 0.00978594459593296 + 0.01 * 6.696956157684326
Epoch 610, val loss: 0.9357507228851318
Epoch 620, training loss: 0.07623675465583801 = 0.009286213666200638 + 0.01 * 6.695054531097412
Epoch 620, val loss: 0.9427305459976196
Epoch 630, training loss: 0.07570198178291321 = 0.008825563825666904 + 0.01 * 6.6876420974731445
Epoch 630, val loss: 0.949539840221405
Epoch 640, training loss: 0.07525483518838882 = 0.008399545215070248 + 0.01 * 6.685528755187988
Epoch 640, val loss: 0.9562090635299683
Epoch 650, training loss: 0.07489874958992004 = 0.00800706259906292 + 0.01 * 6.689168930053711
Epoch 650, val loss: 0.9626564383506775
Epoch 660, training loss: 0.07438645511865616 = 0.0076432316564023495 + 0.01 * 6.674322605133057
Epoch 660, val loss: 0.9689708352088928
Epoch 670, training loss: 0.07392267882823944 = 0.007304856553673744 + 0.01 * 6.661782264709473
Epoch 670, val loss: 0.9751541614532471
Epoch 680, training loss: 0.07370471209287643 = 0.006990016438066959 + 0.01 * 6.6714701652526855
Epoch 680, val loss: 0.9811894297599792
Epoch 690, training loss: 0.07335620373487473 = 0.006697622127830982 + 0.01 * 6.665858268737793
Epoch 690, val loss: 0.9870468974113464
Epoch 700, training loss: 0.07300584763288498 = 0.006423367653042078 + 0.01 * 6.658247947692871
Epoch 700, val loss: 0.9927904009819031
Epoch 710, training loss: 0.07263360172510147 = 0.006167247425764799 + 0.01 * 6.64663553237915
Epoch 710, val loss: 0.9984612464904785
Epoch 720, training loss: 0.07245215773582458 = 0.005928576458245516 + 0.01 * 6.652358531951904
Epoch 720, val loss: 1.0038750171661377
Epoch 730, training loss: 0.07224196195602417 = 0.005704299081116915 + 0.01 * 6.653766632080078
Epoch 730, val loss: 1.0092501640319824
Epoch 740, training loss: 0.07187845557928085 = 0.005494231358170509 + 0.01 * 6.638422966003418
Epoch 740, val loss: 1.0145198106765747
Epoch 750, training loss: 0.07166224718093872 = 0.005296121817082167 + 0.01 * 6.636613368988037
Epoch 750, val loss: 1.0196480751037598
Epoch 760, training loss: 0.07132028788328171 = 0.005108499433845282 + 0.01 * 6.621179103851318
Epoch 760, val loss: 1.0248783826828003
Epoch 770, training loss: 0.0712963417172432 = 0.004929966758936644 + 0.01 * 6.6366376876831055
Epoch 770, val loss: 1.029951572418213
Epoch 780, training loss: 0.07094995677471161 = 0.004763143137097359 + 0.01 * 6.618681907653809
Epoch 780, val loss: 1.0348347425460815
Epoch 790, training loss: 0.07079886645078659 = 0.004606381058692932 + 0.01 * 6.619248390197754
Epoch 790, val loss: 1.0394532680511475
Epoch 800, training loss: 0.07057821750640869 = 0.004458755254745483 + 0.01 * 6.611946105957031
Epoch 800, val loss: 1.0440489053726196
Epoch 810, training loss: 0.07044842839241028 = 0.004319054074585438 + 0.01 * 6.6129374504089355
Epoch 810, val loss: 1.0485237836837769
Epoch 820, training loss: 0.07019554078578949 = 0.004186852835118771 + 0.01 * 6.600869178771973
Epoch 820, val loss: 1.0528815984725952
Epoch 830, training loss: 0.07012522220611572 = 0.0040616123005747795 + 0.01 * 6.606361389160156
Epoch 830, val loss: 1.0571494102478027
Epoch 840, training loss: 0.06993594020605087 = 0.003942946437746286 + 0.01 * 6.599299907684326
Epoch 840, val loss: 1.0612983703613281
Epoch 850, training loss: 0.069847472012043 = 0.0038303581532090902 + 0.01 * 6.601711750030518
Epoch 850, val loss: 1.065410852432251
Epoch 860, training loss: 0.06956198811531067 = 0.0037235335912555456 + 0.01 * 6.583845615386963
Epoch 860, val loss: 1.0694218873977661
Epoch 870, training loss: 0.06944654136896133 = 0.0036219244357198477 + 0.01 * 6.582461833953857
Epoch 870, val loss: 1.0733380317687988
Epoch 880, training loss: 0.06931348890066147 = 0.0035252568777650595 + 0.01 * 6.578823566436768
Epoch 880, val loss: 1.0771342515945435
Epoch 890, training loss: 0.06931199878454208 = 0.0034332650247961283 + 0.01 * 6.587873458862305
Epoch 890, val loss: 1.0809338092803955
Epoch 900, training loss: 0.06909184902906418 = 0.003345526522025466 + 0.01 * 6.57463264465332
Epoch 900, val loss: 1.0845750570297241
Epoch 910, training loss: 0.06914123892784119 = 0.003261930774897337 + 0.01 * 6.587930679321289
Epoch 910, val loss: 1.0881879329681396
Epoch 920, training loss: 0.06886040419340134 = 0.003182072890922427 + 0.01 * 6.567832946777344
Epoch 920, val loss: 1.091675877571106
Epoch 930, training loss: 0.06902910768985748 = 0.003105745417997241 + 0.01 * 6.592336177825928
Epoch 930, val loss: 1.095179796218872
Epoch 940, training loss: 0.06859579682350159 = 0.0030328892171382904 + 0.01 * 6.556291580200195
Epoch 940, val loss: 1.0984904766082764
Epoch 950, training loss: 0.06851281970739365 = 0.002963252831250429 + 0.01 * 6.554956912994385
Epoch 950, val loss: 1.1018108129501343
Epoch 960, training loss: 0.06864874064922333 = 0.002896504942327738 + 0.01 * 6.575223922729492
Epoch 960, val loss: 1.1050240993499756
Epoch 970, training loss: 0.06835343688726425 = 0.0028327612672001123 + 0.01 * 6.552067756652832
Epoch 970, val loss: 1.108170747756958
Epoch 980, training loss: 0.0683046355843544 = 0.0027715968899428844 + 0.01 * 6.553304672241211
Epoch 980, val loss: 1.1112849712371826
Epoch 990, training loss: 0.06814464181661606 = 0.0027131354436278343 + 0.01 * 6.543150424957275
Epoch 990, val loss: 1.1142693758010864
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.9373
Flip ASR: 0.9244/225 nodes
The final ASR:0.70972, 0.16525, Accuracy:0.82963, 0.01048
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11612])
remove edge: torch.Size([2, 9450])
updated graph: torch.Size([2, 10506])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
The final ASR:0.97786, 0.00301, Accuracy:0.83580, 0.00349
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0.4 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0371832847595215 = 1.9534451961517334 + 0.01 * 8.373800277709961
Epoch 0, val loss: 1.959394931793213
Epoch 10, training loss: 2.026670217514038 = 1.942933440208435 + 0.01 * 8.373682022094727
Epoch 10, val loss: 1.9495232105255127
Epoch 20, training loss: 2.0135912895202637 = 1.929857850074768 + 0.01 * 8.373348236083984
Epoch 20, val loss: 1.9368385076522827
Epoch 30, training loss: 1.995119571685791 = 1.911395788192749 + 0.01 * 8.37237548828125
Epoch 30, val loss: 1.9183952808380127
Epoch 40, training loss: 1.9674819707870483 = 1.88381028175354 + 0.01 * 8.367164611816406
Epoch 40, val loss: 1.8907549381256104
Epoch 50, training loss: 1.9281073808670044 = 1.8448764085769653 + 0.01 * 8.323094367980957
Epoch 50, val loss: 1.8530992269515991
Epoch 60, training loss: 1.8810267448425293 = 1.8005261421203613 + 0.01 * 8.05006217956543
Epoch 60, val loss: 1.8141669034957886
Epoch 70, training loss: 1.8361873626708984 = 1.7603071928024292 + 0.01 * 7.588019371032715
Epoch 70, val loss: 1.781005859375
Epoch 80, training loss: 1.784209132194519 = 1.712282419204712 + 0.01 * 7.192677021026611
Epoch 80, val loss: 1.7385475635528564
Epoch 90, training loss: 1.7173564434051514 = 1.647530198097229 + 0.01 * 6.982620716094971
Epoch 90, val loss: 1.6823432445526123
Epoch 100, training loss: 1.6308183670043945 = 1.5616374015808105 + 0.01 * 6.918095588684082
Epoch 100, val loss: 1.609827995300293
Epoch 110, training loss: 1.5294063091278076 = 1.4606281518936157 + 0.01 * 6.877821445465088
Epoch 110, val loss: 1.5259476900100708
Epoch 120, training loss: 1.4242721796035767 = 1.3557312488555908 + 0.01 * 6.854090690612793
Epoch 120, val loss: 1.4405796527862549
Epoch 130, training loss: 1.321462631225586 = 1.2531334161758423 + 0.01 * 6.832918643951416
Epoch 130, val loss: 1.359701156616211
Epoch 140, training loss: 1.2236372232437134 = 1.1555218696594238 + 0.01 * 6.8115315437316895
Epoch 140, val loss: 1.2855360507965088
Epoch 150, training loss: 1.1338903903961182 = 1.0660003423690796 + 0.01 * 6.788999557495117
Epoch 150, val loss: 1.2198162078857422
Epoch 160, training loss: 1.0543327331542969 = 0.9865795373916626 + 0.01 * 6.775324821472168
Epoch 160, val loss: 1.1642887592315674
Epoch 170, training loss: 0.9834029078483582 = 0.9158048629760742 + 0.01 * 6.759805202484131
Epoch 170, val loss: 1.1172932386398315
Epoch 180, training loss: 0.9175835251808167 = 0.8500568270683289 + 0.01 * 6.752671718597412
Epoch 180, val loss: 1.0752828121185303
Epoch 190, training loss: 0.8536208868026733 = 0.7861815690994263 + 0.01 * 6.743934154510498
Epoch 190, val loss: 1.0347609519958496
Epoch 200, training loss: 0.7901054620742798 = 0.7227343916893005 + 0.01 * 6.7371039390563965
Epoch 200, val loss: 0.9944143891334534
Epoch 210, training loss: 0.7270652651786804 = 0.6597514748573303 + 0.01 * 6.731378078460693
Epoch 210, val loss: 0.9544006586074829
Epoch 220, training loss: 0.6650668382644653 = 0.5977835655212402 + 0.01 * 6.728328704833984
Epoch 220, val loss: 0.914914608001709
Epoch 230, training loss: 0.6050464510917664 = 0.5378109216690063 + 0.01 * 6.723552703857422
Epoch 230, val loss: 0.8765676021575928
Epoch 240, training loss: 0.5482897162437439 = 0.48110291361808777 + 0.01 * 6.718681335449219
Epoch 240, val loss: 0.8407725095748901
Epoch 250, training loss: 0.4956095516681671 = 0.42844149470329285 + 0.01 * 6.716806888580322
Epoch 250, val loss: 0.8088013529777527
Epoch 260, training loss: 0.4470500349998474 = 0.379905641078949 + 0.01 * 6.714439868927002
Epoch 260, val loss: 0.7816342115402222
Epoch 270, training loss: 0.402212917804718 = 0.3350597023963928 + 0.01 * 6.7153215408325195
Epoch 270, val loss: 0.7591519951820374
Epoch 280, training loss: 0.360641211271286 = 0.29351067543029785 + 0.01 * 6.713054656982422
Epoch 280, val loss: 0.7408910393714905
Epoch 290, training loss: 0.3224961757659912 = 0.25537148118019104 + 0.01 * 6.712470054626465
Epoch 290, val loss: 0.7265063524246216
Epoch 300, training loss: 0.2882433831691742 = 0.22112756967544556 + 0.01 * 6.711581707000732
Epoch 300, val loss: 0.7158631086349487
Epoch 310, training loss: 0.25828152894973755 = 0.1911792755126953 + 0.01 * 6.7102251052856445
Epoch 310, val loss: 0.7090395092964172
Epoch 320, training loss: 0.2325613796710968 = 0.16546748578548431 + 0.01 * 6.709390640258789
Epoch 320, val loss: 0.7058087587356567
Epoch 330, training loss: 0.21070262789726257 = 0.14361268281936646 + 0.01 * 6.7089948654174805
Epoch 330, val loss: 0.7057217359542847
Epoch 340, training loss: 0.19217047095298767 = 0.12509360909461975 + 0.01 * 6.7076873779296875
Epoch 340, val loss: 0.7083629369735718
Epoch 350, training loss: 0.17644384503364563 = 0.10938476026058197 + 0.01 * 6.705909729003906
Epoch 350, val loss: 0.7132010459899902
Epoch 360, training loss: 0.16314750909805298 = 0.09602560102939606 + 0.01 * 6.712191104888916
Epoch 360, val loss: 0.7198535799980164
Epoch 370, training loss: 0.15167348086833954 = 0.08463496714830399 + 0.01 * 6.703851222991943
Epoch 370, val loss: 0.7280532717704773
Epoch 380, training loss: 0.14190331101417542 = 0.07489112764596939 + 0.01 * 6.7012176513671875
Epoch 380, val loss: 0.7374208569526672
Epoch 390, training loss: 0.13352259993553162 = 0.06653338670730591 + 0.01 * 6.6989216804504395
Epoch 390, val loss: 0.7476478219032288
Epoch 400, training loss: 0.12633134424686432 = 0.0593462735414505 + 0.01 * 6.698507308959961
Epoch 400, val loss: 0.7585908770561218
Epoch 410, training loss: 0.12012404948472977 = 0.05315013229846954 + 0.01 * 6.697391510009766
Epoch 410, val loss: 0.7699490189552307
Epoch 420, training loss: 0.11472354829311371 = 0.04779262840747833 + 0.01 * 6.693091869354248
Epoch 420, val loss: 0.781593382358551
Epoch 430, training loss: 0.110068678855896 = 0.04314510151743889 + 0.01 * 6.692358493804932
Epoch 430, val loss: 0.7934006452560425
Epoch 440, training loss: 0.10599540919065475 = 0.03910059481859207 + 0.01 * 6.689481735229492
Epoch 440, val loss: 0.8051794767379761
Epoch 450, training loss: 0.1024351418018341 = 0.03556748852133751 + 0.01 * 6.686765193939209
Epoch 450, val loss: 0.8168925642967224
Epoch 460, training loss: 0.09930883347988129 = 0.0324697382748127 + 0.01 * 6.683910369873047
Epoch 460, val loss: 0.8284937739372253
Epoch 470, training loss: 0.09662023186683655 = 0.02974293939769268 + 0.01 * 6.6877288818359375
Epoch 470, val loss: 0.8398997783660889
Epoch 480, training loss: 0.09416177868843079 = 0.02733401395380497 + 0.01 * 6.682776927947998
Epoch 480, val loss: 0.8511454463005066
Epoch 490, training loss: 0.0919775664806366 = 0.025198619812726974 + 0.01 * 6.677895545959473
Epoch 490, val loss: 0.862150251865387
Epoch 500, training loss: 0.09009755402803421 = 0.02329809032380581 + 0.01 * 6.679946422576904
Epoch 500, val loss: 0.8729385733604431
Epoch 510, training loss: 0.08834365755319595 = 0.021601064130663872 + 0.01 * 6.674259662628174
Epoch 510, val loss: 0.8834690451622009
Epoch 520, training loss: 0.08676697313785553 = 0.020079292356967926 + 0.01 * 6.668767929077148
Epoch 520, val loss: 0.893790602684021
Epoch 530, training loss: 0.08536885678768158 = 0.01870875060558319 + 0.01 * 6.666010856628418
Epoch 530, val loss: 0.9039052128791809
Epoch 540, training loss: 0.0842616856098175 = 0.01747058890759945 + 0.01 * 6.679109573364258
Epoch 540, val loss: 0.9138123393058777
Epoch 550, training loss: 0.08298499882221222 = 0.016350936144590378 + 0.01 * 6.663406848907471
Epoch 550, val loss: 0.9234668016433716
Epoch 560, training loss: 0.081900954246521 = 0.015334239229559898 + 0.01 * 6.656671524047852
Epoch 560, val loss: 0.9329316020011902
Epoch 570, training loss: 0.08096186816692352 = 0.014407770708203316 + 0.01 * 6.655410289764404
Epoch 570, val loss: 0.942183256149292
Epoch 580, training loss: 0.08011800795793533 = 0.013561350293457508 + 0.01 * 6.655665874481201
Epoch 580, val loss: 0.9511905312538147
Epoch 590, training loss: 0.07926760613918304 = 0.012786167673766613 + 0.01 * 6.648144245147705
Epoch 590, val loss: 0.9600490927696228
Epoch 600, training loss: 0.07852865010499954 = 0.012074030004441738 + 0.01 * 6.6454620361328125
Epoch 600, val loss: 0.9687047004699707
Epoch 610, training loss: 0.07784940302371979 = 0.011418545618653297 + 0.01 * 6.6430864334106445
Epoch 610, val loss: 0.9771590828895569
Epoch 620, training loss: 0.0772397592663765 = 0.010814403183758259 + 0.01 * 6.64253568649292
Epoch 620, val loss: 0.9854357838630676
Epoch 630, training loss: 0.07663197815418243 = 0.010255060158669949 + 0.01 * 6.637691497802734
Epoch 630, val loss: 0.9935218095779419
Epoch 640, training loss: 0.07610763609409332 = 0.009735995903611183 + 0.01 * 6.63716459274292
Epoch 640, val loss: 1.0014768838882446
Epoch 650, training loss: 0.07554566860198975 = 0.009253354743123055 + 0.01 * 6.6292314529418945
Epoch 650, val loss: 1.0091993808746338
Epoch 660, training loss: 0.07509515434503555 = 0.008804075419902802 + 0.01 * 6.62910795211792
Epoch 660, val loss: 1.0167672634124756
Epoch 670, training loss: 0.07468292117118835 = 0.008386330679059029 + 0.01 * 6.629659652709961
Epoch 670, val loss: 1.0241739749908447
Epoch 680, training loss: 0.07419842481613159 = 0.007997824810445309 + 0.01 * 6.620059967041016
Epoch 680, val loss: 1.031367301940918
Epoch 690, training loss: 0.07385986298322678 = 0.007635354995727539 + 0.01 * 6.622450828552246
Epoch 690, val loss: 1.0384368896484375
Epoch 700, training loss: 0.07344748824834824 = 0.007297749165445566 + 0.01 * 6.614974498748779
Epoch 700, val loss: 1.045330286026001
Epoch 710, training loss: 0.07310070097446442 = 0.006982700899243355 + 0.01 * 6.611800193786621
Epoch 710, val loss: 1.0520340204238892
Epoch 720, training loss: 0.07275611907243729 = 0.00668838107958436 + 0.01 * 6.606773853302002
Epoch 720, val loss: 1.0585730075836182
Epoch 730, training loss: 0.07246096432209015 = 0.006413247901946306 + 0.01 * 6.604771137237549
Epoch 730, val loss: 1.0650206804275513
Epoch 740, training loss: 0.0721808522939682 = 0.0061554512940347195 + 0.01 * 6.602540016174316
Epoch 740, val loss: 1.0712605714797974
Epoch 750, training loss: 0.07186242192983627 = 0.005913837812840939 + 0.01 * 6.594858646392822
Epoch 750, val loss: 1.0774005651474
Epoch 760, training loss: 0.0716819167137146 = 0.005687015131115913 + 0.01 * 6.599490165710449
Epoch 760, val loss: 1.083387017250061
Epoch 770, training loss: 0.07149679213762283 = 0.005474143661558628 + 0.01 * 6.602264881134033
Epoch 770, val loss: 1.0892757177352905
Epoch 780, training loss: 0.07114458084106445 = 0.0052741714753210545 + 0.01 * 6.58704137802124
Epoch 780, val loss: 1.09503173828125
Epoch 790, training loss: 0.07101204991340637 = 0.0050859106704592705 + 0.01 * 6.592614650726318
Epoch 790, val loss: 1.1005408763885498
Epoch 800, training loss: 0.07072077691555023 = 0.004908635746687651 + 0.01 * 6.581214427947998
Epoch 800, val loss: 1.106034278869629
Epoch 810, training loss: 0.07052995264530182 = 0.004741370212286711 + 0.01 * 6.578858375549316
Epoch 810, val loss: 1.1114354133605957
Epoch 820, training loss: 0.07035012543201447 = 0.004583614878356457 + 0.01 * 6.576651573181152
Epoch 820, val loss: 1.1166778802871704
Epoch 830, training loss: 0.07011061906814575 = 0.004434715490788221 + 0.01 * 6.567591190338135
Epoch 830, val loss: 1.1217821836471558
Epoch 840, training loss: 0.07007668167352676 = 0.004294064361602068 + 0.01 * 6.578261375427246
Epoch 840, val loss: 1.1267343759536743
Epoch 850, training loss: 0.06973681598901749 = 0.00416115066036582 + 0.01 * 6.557567119598389
Epoch 850, val loss: 1.1316547393798828
Epoch 860, training loss: 0.069618821144104 = 0.004035218618810177 + 0.01 * 6.5583600997924805
Epoch 860, val loss: 1.136427640914917
Epoch 870, training loss: 0.06937386095523834 = 0.003915754146873951 + 0.01 * 6.545810222625732
Epoch 870, val loss: 1.1411105394363403
Epoch 880, training loss: 0.06938006728887558 = 0.003802309511229396 + 0.01 * 6.557775974273682
Epoch 880, val loss: 1.1456204652786255
Epoch 890, training loss: 0.06918590515851974 = 0.0036946535110473633 + 0.01 * 6.5491251945495605
Epoch 890, val loss: 1.15015709400177
Epoch 900, training loss: 0.06900274753570557 = 0.0035924657713621855 + 0.01 * 6.541028022766113
Epoch 900, val loss: 1.1545298099517822
Epoch 910, training loss: 0.06906481087207794 = 0.003495250130072236 + 0.01 * 6.556955814361572
Epoch 910, val loss: 1.1587194204330444
Epoch 920, training loss: 0.0688076913356781 = 0.0034030417446047068 + 0.01 * 6.540464878082275
Epoch 920, val loss: 1.1629598140716553
Epoch 930, training loss: 0.06869793683290482 = 0.0033150305971503258 + 0.01 * 6.538290500640869
Epoch 930, val loss: 1.1670249700546265
Epoch 940, training loss: 0.06863442808389664 = 0.003231195267289877 + 0.01 * 6.540323257446289
Epoch 940, val loss: 1.1711350679397583
Epoch 950, training loss: 0.06841357797384262 = 0.0031511399429291487 + 0.01 * 6.526244163513184
Epoch 950, val loss: 1.1749515533447266
Epoch 960, training loss: 0.06847263127565384 = 0.0030748231802135706 + 0.01 * 6.539781093597412
Epoch 960, val loss: 1.1787585020065308
Epoch 970, training loss: 0.06809013336896896 = 0.0030020184349268675 + 0.01 * 6.508811950683594
Epoch 970, val loss: 1.182610273361206
Epoch 980, training loss: 0.06856583058834076 = 0.002932420466095209 + 0.01 * 6.563340663909912
Epoch 980, val loss: 1.1862940788269043
Epoch 990, training loss: 0.06792114675045013 = 0.002865766640752554 + 0.01 * 6.505537986755371
Epoch 990, val loss: 1.1899131536483765
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.6199
Flip ASR: 0.5511/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.024597644805908 = 1.9408607482910156 + 0.01 * 8.37369441986084
Epoch 0, val loss: 1.9355370998382568
Epoch 10, training loss: 2.014342784881592 = 1.9306122064590454 + 0.01 * 8.373051643371582
Epoch 10, val loss: 1.9238864183425903
Epoch 20, training loss: 2.002089500427246 = 1.9183814525604248 + 0.01 * 8.370808601379395
Epoch 20, val loss: 1.9092246294021606
Epoch 30, training loss: 1.9853719472885132 = 1.901707649230957 + 0.01 * 8.366427421569824
Epoch 30, val loss: 1.8888972997665405
Epoch 40, training loss: 1.9615988731384277 = 1.8780231475830078 + 0.01 * 8.35757827758789
Epoch 40, val loss: 1.8607664108276367
Epoch 50, training loss: 1.9279314279556274 = 1.84508216381073 + 0.01 * 8.284923553466797
Epoch 50, val loss: 1.8245933055877686
Epoch 60, training loss: 1.8832955360412598 = 1.8036187887191772 + 0.01 * 7.967674732208252
Epoch 60, val loss: 1.784231185913086
Epoch 70, training loss: 1.8353796005249023 = 1.7597076892852783 + 0.01 * 7.567193984985352
Epoch 70, val loss: 1.7469984292984009
Epoch 80, training loss: 1.7846324443817139 = 1.7116209268569946 + 0.01 * 7.301146030426025
Epoch 80, val loss: 1.7072385549545288
Epoch 90, training loss: 1.7159970998764038 = 1.6441335678100586 + 0.01 * 7.186351776123047
Epoch 90, val loss: 1.6499570608139038
Epoch 100, training loss: 1.6243255138397217 = 1.5534993410110474 + 0.01 * 7.0826215744018555
Epoch 100, val loss: 1.575530767440796
Epoch 110, training loss: 1.5163718461990356 = 1.4462542533874512 + 0.01 * 7.011760711669922
Epoch 110, val loss: 1.490979790687561
Epoch 120, training loss: 1.407913327217102 = 1.338140606880188 + 0.01 * 6.977276802062988
Epoch 120, val loss: 1.4095038175582886
Epoch 130, training loss: 1.3085503578186035 = 1.2389113903045654 + 0.01 * 6.963896751403809
Epoch 130, val loss: 1.3388018608093262
Epoch 140, training loss: 1.2180784940719604 = 1.148582935333252 + 0.01 * 6.94956111907959
Epoch 140, val loss: 1.2771642208099365
Epoch 150, training loss: 1.131569266319275 = 1.0622481107711792 + 0.01 * 6.932111740112305
Epoch 150, val loss: 1.2181603908538818
Epoch 160, training loss: 1.0454223155975342 = 0.9762983322143555 + 0.01 * 6.912393569946289
Epoch 160, val loss: 1.159420132637024
Epoch 170, training loss: 0.9602853059768677 = 0.8913657069206238 + 0.01 * 6.891960144042969
Epoch 170, val loss: 1.1014074087142944
Epoch 180, training loss: 0.87982177734375 = 0.8110606074333191 + 0.01 * 6.87611722946167
Epoch 180, val loss: 1.0462491512298584
Epoch 190, training loss: 0.8075048327445984 = 0.7388796806335449 + 0.01 * 6.862515449523926
Epoch 190, val loss: 0.997563362121582
Epoch 200, training loss: 0.7445204854011536 = 0.6760203242301941 + 0.01 * 6.8500165939331055
Epoch 200, val loss: 0.9569088220596313
Epoch 210, training loss: 0.6897946000099182 = 0.6212829947471619 + 0.01 * 6.851158142089844
Epoch 210, val loss: 0.9239610433578491
Epoch 220, training loss: 0.6406862735748291 = 0.5723311901092529 + 0.01 * 6.835506439208984
Epoch 220, val loss: 0.8968434929847717
Epoch 230, training loss: 0.5952845215797424 = 0.5270283818244934 + 0.01 * 6.825615406036377
Epoch 230, val loss: 0.8739660978317261
Epoch 240, training loss: 0.5523738265037537 = 0.48420631885528564 + 0.01 * 6.816749572753906
Epoch 240, val loss: 0.8540239930152893
Epoch 250, training loss: 0.5112611651420593 = 0.4431925415992737 + 0.01 * 6.8068623542785645
Epoch 250, val loss: 0.8369221687316895
Epoch 260, training loss: 0.47182729840278625 = 0.4036429524421692 + 0.01 * 6.8184356689453125
Epoch 260, val loss: 0.8229265213012695
Epoch 270, training loss: 0.43345266580581665 = 0.3655411899089813 + 0.01 * 6.791149139404297
Epoch 270, val loss: 0.8113569021224976
Epoch 280, training loss: 0.3966219425201416 = 0.3288845121860504 + 0.01 * 6.773741722106934
Epoch 280, val loss: 0.8022841215133667
Epoch 290, training loss: 0.3615892827510834 = 0.29400238394737244 + 0.01 * 6.7586894035339355
Epoch 290, val loss: 0.7956886291503906
Epoch 300, training loss: 0.3288167715072632 = 0.2611537277698517 + 0.01 * 6.7663044929504395
Epoch 300, val loss: 0.7912757396697998
Epoch 310, training loss: 0.2982456088066101 = 0.23080620169639587 + 0.01 * 6.743940353393555
Epoch 310, val loss: 0.7894946336746216
Epoch 320, training loss: 0.27067282795906067 = 0.20334091782569885 + 0.01 * 6.733190059661865
Epoch 320, val loss: 0.7903454303741455
Epoch 330, training loss: 0.24622347950935364 = 0.17881479859352112 + 0.01 * 6.740868091583252
Epoch 330, val loss: 0.793972373008728
Epoch 340, training loss: 0.22478598356246948 = 0.15735121071338654 + 0.01 * 6.7434773445129395
Epoch 340, val loss: 0.8005989789962769
Epoch 350, training loss: 0.20583102107048035 = 0.13872270286083221 + 0.01 * 6.710831165313721
Epoch 350, val loss: 0.8097284436225891
Epoch 360, training loss: 0.18968135118484497 = 0.12261641770601273 + 0.01 * 6.706493854522705
Epoch 360, val loss: 0.8208438754081726
Epoch 370, training loss: 0.17581486701965332 = 0.10874107480049133 + 0.01 * 6.70737886428833
Epoch 370, val loss: 0.8336870670318604
Epoch 380, training loss: 0.16372591257095337 = 0.09676460921764374 + 0.01 * 6.69612979888916
Epoch 380, val loss: 0.8478463888168335
Epoch 390, training loss: 0.15328770875930786 = 0.08641979843378067 + 0.01 * 6.686790943145752
Epoch 390, val loss: 0.8631578087806702
Epoch 400, training loss: 0.14429010450839996 = 0.07745318114757538 + 0.01 * 6.683692455291748
Epoch 400, val loss: 0.8792945742607117
Epoch 410, training loss: 0.13635662198066711 = 0.06960286945104599 + 0.01 * 6.675375461578369
Epoch 410, val loss: 0.8958975672721863
Epoch 420, training loss: 0.12987983226776123 = 0.0627237930893898 + 0.01 * 6.715604305267334
Epoch 420, val loss: 0.9129233360290527
Epoch 430, training loss: 0.1234867200255394 = 0.05672015994787216 + 0.01 * 6.676656246185303
Epoch 430, val loss: 0.9301806688308716
Epoch 440, training loss: 0.11803974211215973 = 0.051420923322439194 + 0.01 * 6.661882400512695
Epoch 440, val loss: 0.9475030899047852
Epoch 450, training loss: 0.11324067413806915 = 0.046696987003088 + 0.01 * 6.6543684005737305
Epoch 450, val loss: 0.9648747444152832
Epoch 460, training loss: 0.10917479544878006 = 0.042545661330223083 + 0.01 * 6.662913799285889
Epoch 460, val loss: 0.9822028279304504
Epoch 470, training loss: 0.10530553013086319 = 0.03888583183288574 + 0.01 * 6.641970157623291
Epoch 470, val loss: 0.9994211792945862
Epoch 480, training loss: 0.10211468487977982 = 0.035636380314826965 + 0.01 * 6.647830486297607
Epoch 480, val loss: 1.0163146257400513
Epoch 490, training loss: 0.09908522665500641 = 0.03274951130151749 + 0.01 * 6.633571624755859
Epoch 490, val loss: 1.032801866531372
Epoch 500, training loss: 0.09653116762638092 = 0.030178572982549667 + 0.01 * 6.63525915145874
Epoch 500, val loss: 1.0489269495010376
Epoch 510, training loss: 0.0941542536020279 = 0.027885692194104195 + 0.01 * 6.626856327056885
Epoch 510, val loss: 1.064626932144165
Epoch 520, training loss: 0.09204256534576416 = 0.025838006287813187 + 0.01 * 6.620455741882324
Epoch 520, val loss: 1.0798721313476562
Epoch 530, training loss: 0.09030430763959885 = 0.024003885686397552 + 0.01 * 6.630042552947998
Epoch 530, val loss: 1.0946063995361328
Epoch 540, training loss: 0.0884857028722763 = 0.022354857996106148 + 0.01 * 6.61308479309082
Epoch 540, val loss: 1.108875036239624
Epoch 550, training loss: 0.08701787889003754 = 0.020868269726634026 + 0.01 * 6.61496114730835
Epoch 550, val loss: 1.1227177381515503
Epoch 560, training loss: 0.08551760762929916 = 0.01952308975160122 + 0.01 * 6.599451541900635
Epoch 560, val loss: 1.1360998153686523
Epoch 570, training loss: 0.08436822891235352 = 0.018303724005818367 + 0.01 * 6.606451034545898
Epoch 570, val loss: 1.1491049528121948
Epoch 580, training loss: 0.08318733423948288 = 0.017194977030158043 + 0.01 * 6.599236011505127
Epoch 580, val loss: 1.1617640256881714
Epoch 590, training loss: 0.08220196515321732 = 0.016184212639927864 + 0.01 * 6.601775646209717
Epoch 590, val loss: 1.174033761024475
Epoch 600, training loss: 0.08110544085502625 = 0.015262032859027386 + 0.01 * 6.584341526031494
Epoch 600, val loss: 1.1859408617019653
Epoch 610, training loss: 0.08031230419874191 = 0.014416837133467197 + 0.01 * 6.5895466804504395
Epoch 610, val loss: 1.1975224018096924
Epoch 620, training loss: 0.07941317558288574 = 0.013633422553539276 + 0.01 * 6.577975273132324
Epoch 620, val loss: 1.2087981700897217
Epoch 630, training loss: 0.0787246897816658 = 0.012905399315059185 + 0.01 * 6.5819292068481445
Epoch 630, val loss: 1.2197755575180054
Epoch 640, training loss: 0.0781882181763649 = 0.0122334910556674 + 0.01 * 6.595473289489746
Epoch 640, val loss: 1.2304444313049316
Epoch 650, training loss: 0.07718571275472641 = 0.011625675484538078 + 0.01 * 6.556004047393799
Epoch 650, val loss: 1.2408621311187744
Epoch 660, training loss: 0.07658601552248001 = 0.011065437458455563 + 0.01 * 6.552058219909668
Epoch 660, val loss: 1.250929355621338
Epoch 670, training loss: 0.07601795345544815 = 0.010546863079071045 + 0.01 * 6.547109127044678
Epoch 670, val loss: 1.2607197761535645
Epoch 680, training loss: 0.07559801638126373 = 0.010066607035696507 + 0.01 * 6.553141117095947
Epoch 680, val loss: 1.2703322172164917
Epoch 690, training loss: 0.0750131905078888 = 0.00962052121758461 + 0.01 * 6.539267063140869
Epoch 690, val loss: 1.279670238494873
Epoch 700, training loss: 0.07458212971687317 = 0.00920515600591898 + 0.01 * 6.537697792053223
Epoch 700, val loss: 1.2887415885925293
Epoch 710, training loss: 0.07411370426416397 = 0.008817941881716251 + 0.01 * 6.529576778411865
Epoch 710, val loss: 1.2975932359695435
Epoch 720, training loss: 0.07373511046171188 = 0.008456389419734478 + 0.01 * 6.527872562408447
Epoch 720, val loss: 1.3061946630477905
Epoch 730, training loss: 0.07335149496793747 = 0.008117950521409512 + 0.01 * 6.523354530334473
Epoch 730, val loss: 1.3145886659622192
Epoch 740, training loss: 0.07313369959592819 = 0.007801407016813755 + 0.01 * 6.533229827880859
Epoch 740, val loss: 1.3227849006652832
Epoch 750, training loss: 0.07268527150154114 = 0.007505522575229406 + 0.01 * 6.517975330352783
Epoch 750, val loss: 1.330734372138977
Epoch 760, training loss: 0.07238271832466125 = 0.007227874360978603 + 0.01 * 6.51548433303833
Epoch 760, val loss: 1.3384469747543335
Epoch 770, training loss: 0.07216233015060425 = 0.006966978777199984 + 0.01 * 6.519535064697266
Epoch 770, val loss: 1.3459962606430054
Epoch 780, training loss: 0.07176460325717926 = 0.006721281446516514 + 0.01 * 6.504332542419434
Epoch 780, val loss: 1.353381633758545
Epoch 790, training loss: 0.07147851586341858 = 0.006489735096693039 + 0.01 * 6.49887752532959
Epoch 790, val loss: 1.36056387424469
Epoch 800, training loss: 0.07123710215091705 = 0.006270966026932001 + 0.01 * 6.496613502502441
Epoch 800, val loss: 1.3676173686981201
Epoch 810, training loss: 0.07104236632585526 = 0.006064452696591616 + 0.01 * 6.497791290283203
Epoch 810, val loss: 1.3744187355041504
Epoch 820, training loss: 0.07077719271183014 = 0.005869294982403517 + 0.01 * 6.490790367126465
Epoch 820, val loss: 1.3811068534851074
Epoch 830, training loss: 0.07082395255565643 = 0.005684453994035721 + 0.01 * 6.513949394226074
Epoch 830, val loss: 1.387642741203308
Epoch 840, training loss: 0.0702885240316391 = 0.005509429145604372 + 0.01 * 6.477909564971924
Epoch 840, val loss: 1.3940092325210571
Epoch 850, training loss: 0.07044006884098053 = 0.005343403667211533 + 0.01 * 6.50966739654541
Epoch 850, val loss: 1.400239109992981
Epoch 860, training loss: 0.06999467313289642 = 0.005185876041650772 + 0.01 * 6.480880260467529
Epoch 860, val loss: 1.4063359498977661
Epoch 870, training loss: 0.06991741061210632 = 0.005036036018282175 + 0.01 * 6.488138198852539
Epoch 870, val loss: 1.4122979640960693
Epoch 880, training loss: 0.0695049986243248 = 0.004893579985946417 + 0.01 * 6.461142063140869
Epoch 880, val loss: 1.4181245565414429
Epoch 890, training loss: 0.06953929364681244 = 0.00475793844088912 + 0.01 * 6.478135108947754
Epoch 890, val loss: 1.423806071281433
Epoch 900, training loss: 0.06932985037565231 = 0.004628563765436411 + 0.01 * 6.470129013061523
Epoch 900, val loss: 1.4293969869613647
Epoch 910, training loss: 0.0689644142985344 = 0.004505387507379055 + 0.01 * 6.4459028244018555
Epoch 910, val loss: 1.4348137378692627
Epoch 920, training loss: 0.06901324540376663 = 0.004387855529785156 + 0.01 * 6.462539196014404
Epoch 920, val loss: 1.4401369094848633
Epoch 930, training loss: 0.06877342611551285 = 0.004275552462786436 + 0.01 * 6.449787616729736
Epoch 930, val loss: 1.4453694820404053
Epoch 940, training loss: 0.0687292292714119 = 0.004168466664850712 + 0.01 * 6.456076622009277
Epoch 940, val loss: 1.4504474401474
Epoch 950, training loss: 0.06850819289684296 = 0.004066099878400564 + 0.01 * 6.444209098815918
Epoch 950, val loss: 1.4554558992385864
Epoch 960, training loss: 0.06843691319227219 = 0.003968019969761372 + 0.01 * 6.446889400482178
Epoch 960, val loss: 1.4603135585784912
Epoch 970, training loss: 0.06842365860939026 = 0.0038741298485547304 + 0.01 * 6.454953193664551
Epoch 970, val loss: 1.4651418924331665
Epoch 980, training loss: 0.06819597631692886 = 0.0037842863239347935 + 0.01 * 6.441169261932373
Epoch 980, val loss: 1.4697825908660889
Epoch 990, training loss: 0.06793354451656342 = 0.003698185319080949 + 0.01 * 6.4235358238220215
Epoch 990, val loss: 1.474377989768982
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.4428
Flip ASR: 0.4756/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0278306007385254 = 1.9440927505493164 + 0.01 * 8.37378978729248
Epoch 0, val loss: 1.9381638765335083
Epoch 10, training loss: 2.017418384552002 = 1.933681607246399 + 0.01 * 8.373673439025879
Epoch 10, val loss: 1.9280006885528564
Epoch 20, training loss: 2.0045809745788574 = 1.9208476543426514 + 0.01 * 8.373340606689453
Epoch 20, val loss: 1.915190577507019
Epoch 30, training loss: 1.9866890907287598 = 1.9029645919799805 + 0.01 * 8.372452735900879
Epoch 30, val loss: 1.8970619440078735
Epoch 40, training loss: 1.9605112075805664 = 1.8768259286880493 + 0.01 * 8.368526458740234
Epoch 40, val loss: 1.8706470727920532
Epoch 50, training loss: 1.9236562252044678 = 1.840248465538025 + 0.01 * 8.340773582458496
Epoch 50, val loss: 1.8350180387496948
Epoch 60, training loss: 1.8781747817993164 = 1.7967848777770996 + 0.01 * 8.138993263244629
Epoch 60, val loss: 1.7959403991699219
Epoch 70, training loss: 1.8292758464813232 = 1.7531592845916748 + 0.01 * 7.611651420593262
Epoch 70, val loss: 1.7598389387130737
Epoch 80, training loss: 1.7730865478515625 = 1.7003923654556274 + 0.01 * 7.2694220542907715
Epoch 80, val loss: 1.7146159410476685
Epoch 90, training loss: 1.6989047527313232 = 1.6272099018096924 + 0.01 * 7.169484615325928
Epoch 90, val loss: 1.6515533924102783
Epoch 100, training loss: 1.6017662286758423 = 1.5303572416305542 + 0.01 * 7.140896320343018
Epoch 100, val loss: 1.5701634883880615
Epoch 110, training loss: 1.485196590423584 = 1.4142143726348877 + 0.01 * 7.098224639892578
Epoch 110, val loss: 1.4751907587051392
Epoch 120, training loss: 1.3598859310150146 = 1.2895067930221558 + 0.01 * 7.037919521331787
Epoch 120, val loss: 1.3743648529052734
Epoch 130, training loss: 1.2363651990890503 = 1.1666373014450073 + 0.01 * 6.972790241241455
Epoch 130, val loss: 1.277495265007019
Epoch 140, training loss: 1.1216245889663696 = 1.0523349046707153 + 0.01 * 6.9289655685424805
Epoch 140, val loss: 1.189615249633789
Epoch 150, training loss: 1.0193756818771362 = 0.950300931930542 + 0.01 * 6.907474517822266
Epoch 150, val loss: 1.1138988733291626
Epoch 160, training loss: 0.9294005632400513 = 0.8605449795722961 + 0.01 * 6.88555908203125
Epoch 160, val loss: 1.0498881340026855
Epoch 170, training loss: 0.8498167991638184 = 0.7811705470085144 + 0.01 * 6.864622116088867
Epoch 170, val loss: 0.9959004521369934
Epoch 180, training loss: 0.7787333130836487 = 0.7102524042129517 + 0.01 * 6.848093032836914
Epoch 180, val loss: 0.949762761592865
Epoch 190, training loss: 0.7146708369255066 = 0.6463170051574707 + 0.01 * 6.835382461547852
Epoch 190, val loss: 0.9101340770721436
Epoch 200, training loss: 0.6564178466796875 = 0.5881713628768921 + 0.01 * 6.824646472930908
Epoch 200, val loss: 0.8760519623756409
Epoch 210, training loss: 0.6030013561248779 = 0.5348591208457947 + 0.01 * 6.8142242431640625
Epoch 210, val loss: 0.8468915820121765
Epoch 220, training loss: 0.5535740852355957 = 0.485542893409729 + 0.01 * 6.803122520446777
Epoch 220, val loss: 0.8224717974662781
Epoch 230, training loss: 0.5073627233505249 = 0.43943697214126587 + 0.01 * 6.792577743530273
Epoch 230, val loss: 0.8026940226554871
Epoch 240, training loss: 0.4635266959667206 = 0.39569759368896484 + 0.01 * 6.78291130065918
Epoch 240, val loss: 0.7871080636978149
Epoch 250, training loss: 0.42139196395874023 = 0.3536354899406433 + 0.01 * 6.77564811706543
Epoch 250, val loss: 0.7752529382705688
Epoch 260, training loss: 0.380892276763916 = 0.3131699562072754 + 0.01 * 6.772230625152588
Epoch 260, val loss: 0.7669087648391724
Epoch 270, training loss: 0.3426641523838043 = 0.27498942613601685 + 0.01 * 6.767473220825195
Epoch 270, val loss: 0.7623516321182251
Epoch 280, training loss: 0.3077526390552521 = 0.24011336266994476 + 0.01 * 6.763927459716797
Epoch 280, val loss: 0.7614626288414001
Epoch 290, training loss: 0.2769632935523987 = 0.209336057305336 + 0.01 * 6.762723445892334
Epoch 290, val loss: 0.7643184065818787
Epoch 300, training loss: 0.2504405379295349 = 0.18281784653663635 + 0.01 * 6.762269496917725
Epoch 300, val loss: 0.770408570766449
Epoch 310, training loss: 0.227810800075531 = 0.16018880903720856 + 0.01 * 6.762198448181152
Epoch 310, val loss: 0.778957188129425
Epoch 320, training loss: 0.20844706892967224 = 0.14085349440574646 + 0.01 * 6.759357929229736
Epoch 320, val loss: 0.7897237539291382
Epoch 330, training loss: 0.19180747866630554 = 0.1242222934961319 + 0.01 * 6.758519172668457
Epoch 330, val loss: 0.8020243048667908
Epoch 340, training loss: 0.1774032711982727 = 0.1098310723900795 + 0.01 * 6.757219314575195
Epoch 340, val loss: 0.8156275153160095
Epoch 350, training loss: 0.1648857593536377 = 0.0973275676369667 + 0.01 * 6.755819797515869
Epoch 350, val loss: 0.8302064538002014
Epoch 360, training loss: 0.1539854258298874 = 0.08644063025712967 + 0.01 * 6.754479885101318
Epoch 360, val loss: 0.8454007506370544
Epoch 370, training loss: 0.14449501037597656 = 0.07695455849170685 + 0.01 * 6.754045009613037
Epoch 370, val loss: 0.861076831817627
Epoch 380, training loss: 0.13620178401470184 = 0.06868044286966324 + 0.01 * 6.752134323120117
Epoch 380, val loss: 0.8769961595535278
Epoch 390, training loss: 0.12893855571746826 = 0.06144220381975174 + 0.01 * 6.749634742736816
Epoch 390, val loss: 0.8929822444915771
Epoch 400, training loss: 0.1225757896900177 = 0.05509882792830467 + 0.01 * 6.747696876525879
Epoch 400, val loss: 0.9089866876602173
Epoch 410, training loss: 0.11699351668357849 = 0.049539271742105484 + 0.01 * 6.745424270629883
Epoch 410, val loss: 0.9248485565185547
Epoch 420, training loss: 0.11216845363378525 = 0.044666483998298645 + 0.01 * 6.750197410583496
Epoch 420, val loss: 0.9405481815338135
Epoch 430, training loss: 0.10782395303249359 = 0.040392015129327774 + 0.01 * 6.743194103240967
Epoch 430, val loss: 0.9560253024101257
Epoch 440, training loss: 0.1040290892124176 = 0.036636993288993835 + 0.01 * 6.7392096519470215
Epoch 440, val loss: 0.9712596535682678
Epoch 450, training loss: 0.10069385170936584 = 0.033331725746393204 + 0.01 * 6.736212730407715
Epoch 450, val loss: 0.9861306548118591
Epoch 460, training loss: 0.09775535762310028 = 0.030416276305913925 + 0.01 * 6.733907699584961
Epoch 460, val loss: 1.0006450414657593
Epoch 470, training loss: 0.09515070170164108 = 0.027839012444019318 + 0.01 * 6.731168746948242
Epoch 470, val loss: 1.014844298362732
Epoch 480, training loss: 0.09282287210226059 = 0.025554612278938293 + 0.01 * 6.726826190948486
Epoch 480, val loss: 1.028662919998169
Epoch 490, training loss: 0.09084486216306686 = 0.023524509742856026 + 0.01 * 6.7320356369018555
Epoch 490, val loss: 1.0421969890594482
Epoch 500, training loss: 0.08893346786499023 = 0.021716561168432236 + 0.01 * 6.721690654754639
Epoch 500, val loss: 1.0553407669067383
Epoch 510, training loss: 0.08727912604808807 = 0.020102310925722122 + 0.01 * 6.717680931091309
Epoch 510, val loss: 1.0681571960449219
Epoch 520, training loss: 0.08590592443943024 = 0.018656861037015915 + 0.01 * 6.7249064445495605
Epoch 520, val loss: 1.0806422233581543
Epoch 530, training loss: 0.08445946872234344 = 0.017359741032123566 + 0.01 * 6.709972858428955
Epoch 530, val loss: 1.092755913734436
Epoch 540, training loss: 0.08330996334552765 = 0.016192173585295677 + 0.01 * 6.71177864074707
Epoch 540, val loss: 1.1045726537704468
Epoch 550, training loss: 0.0821790024638176 = 0.015138612128794193 + 0.01 * 6.704039573669434
Epoch 550, val loss: 1.1161179542541504
Epoch 560, training loss: 0.08115062862634659 = 0.014185242354869843 + 0.01 * 6.696538925170898
Epoch 560, val loss: 1.1273528337478638
Epoch 570, training loss: 0.08030800521373749 = 0.013320117257535458 + 0.01 * 6.698788642883301
Epoch 570, val loss: 1.138270616531372
Epoch 580, training loss: 0.07954851537942886 = 0.012533397413790226 + 0.01 * 6.701512336730957
Epoch 580, val loss: 1.148890733718872
Epoch 590, training loss: 0.07868289947509766 = 0.01181658823043108 + 0.01 * 6.686631202697754
Epoch 590, val loss: 1.1592565774917603
Epoch 600, training loss: 0.077974833548069 = 0.011161526665091515 + 0.01 * 6.681330680847168
Epoch 600, val loss: 1.1692951917648315
Epoch 610, training loss: 0.07729850709438324 = 0.010561381466686726 + 0.01 * 6.673712730407715
Epoch 610, val loss: 1.179059386253357
Epoch 620, training loss: 0.07683086395263672 = 0.01001104898750782 + 0.01 * 6.681982040405273
Epoch 620, val loss: 1.1885871887207031
Epoch 630, training loss: 0.07619895040988922 = 0.009505623951554298 + 0.01 * 6.669332981109619
Epoch 630, val loss: 1.197839379310608
Epoch 640, training loss: 0.07564116269350052 = 0.00903947651386261 + 0.01 * 6.660168647766113
Epoch 640, val loss: 1.2068334817886353
Epoch 650, training loss: 0.07533378154039383 = 0.008608709089457989 + 0.01 * 6.6725077629089355
Epoch 650, val loss: 1.215599536895752
Epoch 660, training loss: 0.07482963800430298 = 0.008210312575101852 + 0.01 * 6.661933422088623
Epoch 660, val loss: 1.2242050170898438
Epoch 670, training loss: 0.07441801577806473 = 0.007841350510716438 + 0.01 * 6.6576666831970215
Epoch 670, val loss: 1.2325018644332886
Epoch 680, training loss: 0.07402897626161575 = 0.00749862240627408 + 0.01 * 6.653035640716553
Epoch 680, val loss: 1.2406586408615112
Epoch 690, training loss: 0.07359231263399124 = 0.007180443499237299 + 0.01 * 6.6411871910095215
Epoch 690, val loss: 1.2485541105270386
Epoch 700, training loss: 0.0731399729847908 = 0.006883728317916393 + 0.01 * 6.625625133514404
Epoch 700, val loss: 1.2563436031341553
Epoch 710, training loss: 0.0731830894947052 = 0.006606559734791517 + 0.01 * 6.65765380859375
Epoch 710, val loss: 1.2638856172561646
Epoch 720, training loss: 0.07253946363925934 = 0.006348215974867344 + 0.01 * 6.619124889373779
Epoch 720, val loss: 1.2712197303771973
Epoch 730, training loss: 0.07220964133739471 = 0.006106234155595303 + 0.01 * 6.610340595245361
Epoch 730, val loss: 1.2784485816955566
Epoch 740, training loss: 0.07216503471136093 = 0.005879349075257778 + 0.01 * 6.628568649291992
Epoch 740, val loss: 1.2854599952697754
Epoch 750, training loss: 0.07182570546865463 = 0.00566621869802475 + 0.01 * 6.6159491539001465
Epoch 750, val loss: 1.2923980951309204
Epoch 760, training loss: 0.0715075209736824 = 0.005466018337756395 + 0.01 * 6.60414981842041
Epoch 760, val loss: 1.299114465713501
Epoch 770, training loss: 0.07130774855613708 = 0.005277525633573532 + 0.01 * 6.60302209854126
Epoch 770, val loss: 1.30567467212677
Epoch 780, training loss: 0.07106123864650726 = 0.005099900998175144 + 0.01 * 6.596133708953857
Epoch 780, val loss: 1.3121366500854492
Epoch 790, training loss: 0.07076861709356308 = 0.004932389594614506 + 0.01 * 6.583622932434082
Epoch 790, val loss: 1.318393349647522
Epoch 800, training loss: 0.07059074938297272 = 0.004774291068315506 + 0.01 * 6.581645488739014
Epoch 800, val loss: 1.3245341777801514
Epoch 810, training loss: 0.07042285799980164 = 0.004624825902283192 + 0.01 * 6.579802989959717
Epoch 810, val loss: 1.3305668830871582
Epoch 820, training loss: 0.07024981826543808 = 0.004483288154006004 + 0.01 * 6.576653003692627
Epoch 820, val loss: 1.3364568948745728
Epoch 830, training loss: 0.07013137638568878 = 0.004349081311374903 + 0.01 * 6.5782294273376465
Epoch 830, val loss: 1.3422726392745972
Epoch 840, training loss: 0.06992888450622559 = 0.004221954848617315 + 0.01 * 6.570693016052246
Epoch 840, val loss: 1.347902774810791
Epoch 850, training loss: 0.06977604329586029 = 0.00410128477960825 + 0.01 * 6.56747579574585
Epoch 850, val loss: 1.3534289598464966
Epoch 860, training loss: 0.06966885179281235 = 0.003986548166722059 + 0.01 * 6.568230628967285
Epoch 860, val loss: 1.3588824272155762
Epoch 870, training loss: 0.06945770978927612 = 0.003877718700096011 + 0.01 * 6.5579986572265625
Epoch 870, val loss: 1.3642059564590454
Epoch 880, training loss: 0.06938046216964722 = 0.00377407087944448 + 0.01 * 6.560639381408691
Epoch 880, val loss: 1.3693903684616089
Epoch 890, training loss: 0.0695396214723587 = 0.003675241721794009 + 0.01 * 6.586437702178955
Epoch 890, val loss: 1.3745343685150146
Epoch 900, training loss: 0.06907748430967331 = 0.003581227734684944 + 0.01 * 6.549626350402832
Epoch 900, val loss: 1.3795665502548218
Epoch 910, training loss: 0.06891151517629623 = 0.003491452196612954 + 0.01 * 6.542006015777588
Epoch 910, val loss: 1.3844573497772217
Epoch 920, training loss: 0.06888535618782043 = 0.003405784722417593 + 0.01 * 6.547956943511963
Epoch 920, val loss: 1.3892120122909546
Epoch 930, training loss: 0.06869401782751083 = 0.003324145218357444 + 0.01 * 6.5369873046875
Epoch 930, val loss: 1.3939263820648193
Epoch 940, training loss: 0.06858285516500473 = 0.0032461583614349365 + 0.01 * 6.533669471740723
Epoch 940, val loss: 1.3985955715179443
Epoch 950, training loss: 0.0683257058262825 = 0.0031714141368865967 + 0.01 * 6.5154290199279785
Epoch 950, val loss: 1.4031774997711182
Epoch 960, training loss: 0.06846209615468979 = 0.0030997921712696552 + 0.01 * 6.536230564117432
Epoch 960, val loss: 1.4076021909713745
Epoch 970, training loss: 0.06869594007730484 = 0.003031344385817647 + 0.01 * 6.566460132598877
Epoch 970, val loss: 1.4120354652404785
Epoch 980, training loss: 0.06815796345472336 = 0.002965951571241021 + 0.01 * 6.519201278686523
Epoch 980, val loss: 1.4162826538085938
Epoch 990, training loss: 0.06792210787534714 = 0.0029031645972281694 + 0.01 * 6.501894474029541
Epoch 990, val loss: 1.4204717874526978
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.7712
Flip ASR: 0.7378/225 nodes
The final ASR:0.61132, 0.13421, Accuracy:0.81358, 0.01823
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11658])
remove edge: torch.Size([2, 9494])
updated graph: torch.Size([2, 10596])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98524, 0.00797, Accuracy:0.83457, 0.00873
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0.4 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0498735904693604 = 1.9661364555358887 + 0.01 * 8.3737211227417
Epoch 0, val loss: 1.967025876045227
Epoch 10, training loss: 2.0390937328338623 = 1.9553580284118652 + 0.01 * 8.373578071594238
Epoch 10, val loss: 1.9565916061401367
Epoch 20, training loss: 2.025716781616211 = 1.9419852495193481 + 0.01 * 8.373149871826172
Epoch 20, val loss: 1.9438090324401855
Epoch 30, training loss: 2.0068562030792236 = 1.9231386184692383 + 0.01 * 8.371747970581055
Epoch 30, val loss: 1.9259593486785889
Epoch 40, training loss: 1.9785346984863281 = 1.894898772239685 + 0.01 * 8.36359691619873
Epoch 40, val loss: 1.899657964706421
Epoch 50, training loss: 1.9375779628753662 = 1.8544703722000122 + 0.01 * 8.310762405395508
Epoch 50, val loss: 1.8636658191680908
Epoch 60, training loss: 1.8872214555740356 = 1.8072503805160522 + 0.01 * 7.997106552124023
Epoch 60, val loss: 1.8250341415405273
Epoch 70, training loss: 1.8410558700561523 = 1.7646733522415161 + 0.01 * 7.638254165649414
Epoch 70, val loss: 1.790317177772522
Epoch 80, training loss: 1.7899259328842163 = 1.7161062955856323 + 0.01 * 7.381967067718506
Epoch 80, val loss: 1.7437020540237427
Epoch 90, training loss: 1.7219144105911255 = 1.649841070175171 + 0.01 * 7.207332611083984
Epoch 90, val loss: 1.6830945014953613
Epoch 100, training loss: 1.6347577571868896 = 1.563768982887268 + 0.01 * 7.098877906799316
Epoch 100, val loss: 1.6105419397354126
Epoch 110, training loss: 1.5345529317855835 = 1.464105248451233 + 0.01 * 7.044773101806641
Epoch 110, val loss: 1.5269954204559326
Epoch 120, training loss: 1.4317537546157837 = 1.3619534969329834 + 0.01 * 6.98002290725708
Epoch 120, val loss: 1.442192792892456
Epoch 130, training loss: 1.3318672180175781 = 1.2628449201583862 + 0.01 * 6.902233600616455
Epoch 130, val loss: 1.3604884147644043
Epoch 140, training loss: 1.2367507219314575 = 1.1685549020767212 + 0.01 * 6.819587707519531
Epoch 140, val loss: 1.285975456237793
Epoch 150, training loss: 1.1469104290008545 = 1.0792160034179688 + 0.01 * 6.76943826675415
Epoch 150, val loss: 1.217132568359375
Epoch 160, training loss: 1.0619615316390991 = 0.9944298267364502 + 0.01 * 6.75316858291626
Epoch 160, val loss: 1.15341055393219
Epoch 170, training loss: 0.9816091656684875 = 0.9142577052116394 + 0.01 * 6.735146522521973
Epoch 170, val loss: 1.0943163633346558
Epoch 180, training loss: 0.9072208404541016 = 0.8399585485458374 + 0.01 * 6.726230621337891
Epoch 180, val loss: 1.0405603647232056
Epoch 190, training loss: 0.8403307795524597 = 0.7731517553329468 + 0.01 * 6.717904567718506
Epoch 190, val loss: 0.9938465356826782
Epoch 200, training loss: 0.7820947766304016 = 0.7149581909179688 + 0.01 * 6.713656902313232
Epoch 200, val loss: 0.9553957581520081
Epoch 210, training loss: 0.7317767143249512 = 0.6646962761878967 + 0.01 * 6.708042144775391
Epoch 210, val loss: 0.9251407980918884
Epoch 220, training loss: 0.6870489716529846 = 0.6199986934661865 + 0.01 * 6.7050275802612305
Epoch 220, val loss: 0.9007894992828369
Epoch 230, training loss: 0.6451038718223572 = 0.5780736804008484 + 0.01 * 6.7030181884765625
Epoch 230, val loss: 0.8800439834594727
Epoch 240, training loss: 0.6040415167808533 = 0.5370224714279175 + 0.01 * 6.701904296875
Epoch 240, val loss: 0.8604199290275574
Epoch 250, training loss: 0.5635493993759155 = 0.49653738737106323 + 0.01 * 6.701199531555176
Epoch 250, val loss: 0.8413443565368652
Epoch 260, training loss: 0.5238181352615356 = 0.4568082094192505 + 0.01 * 6.700995922088623
Epoch 260, val loss: 0.8231884837150574
Epoch 270, training loss: 0.4849168062210083 = 0.41790488362312317 + 0.01 * 6.701192855834961
Epoch 270, val loss: 0.8065944910049438
Epoch 280, training loss: 0.44670015573501587 = 0.3796795606613159 + 0.01 * 6.702060222625732
Epoch 280, val loss: 0.7919872403144836
Epoch 290, training loss: 0.40928971767425537 = 0.3422602415084839 + 0.01 * 6.702949047088623
Epoch 290, val loss: 0.7804142236709595
Epoch 300, training loss: 0.3727111220359802 = 0.30566543340682983 + 0.01 * 6.7045698165893555
Epoch 300, val loss: 0.7719494104385376
Epoch 310, training loss: 0.33708539605140686 = 0.2700255811214447 + 0.01 * 6.705981254577637
Epoch 310, val loss: 0.7663279175758362
Epoch 320, training loss: 0.30296480655670166 = 0.2358904778957367 + 0.01 * 6.707433223724365
Epoch 320, val loss: 0.7630871534347534
Epoch 330, training loss: 0.27121996879577637 = 0.2041344940662384 + 0.01 * 6.708548069000244
Epoch 330, val loss: 0.7621580362319946
Epoch 340, training loss: 0.2427016794681549 = 0.17561140656471252 + 0.01 * 6.709026336669922
Epoch 340, val loss: 0.763788640499115
Epoch 350, training loss: 0.2178598940372467 = 0.15077242255210876 + 0.01 * 6.708746910095215
Epoch 350, val loss: 0.7680331468582153
Epoch 360, training loss: 0.19669222831726074 = 0.12961283326148987 + 0.01 * 6.707939624786377
Epoch 360, val loss: 0.7747677564620972
Epoch 370, training loss: 0.17884373664855957 = 0.11177846789360046 + 0.01 * 6.7065277099609375
Epoch 370, val loss: 0.7835305333137512
Epoch 380, training loss: 0.16385418176651 = 0.09680593758821487 + 0.01 * 6.704824924468994
Epoch 380, val loss: 0.7938295602798462
Epoch 390, training loss: 0.15127038955688477 = 0.08423785865306854 + 0.01 * 6.70325231552124
Epoch 390, val loss: 0.8052071332931519
Epoch 400, training loss: 0.14068248867988586 = 0.07366957515478134 + 0.01 * 6.701291561126709
Epoch 400, val loss: 0.8172362446784973
Epoch 410, training loss: 0.13174006342887878 = 0.06475567817687988 + 0.01 * 6.698437690734863
Epoch 410, val loss: 0.8297119140625
Epoch 420, training loss: 0.12416024506092072 = 0.05720934644341469 + 0.01 * 6.695090293884277
Epoch 420, val loss: 0.8425077199935913
Epoch 430, training loss: 0.11776594072580338 = 0.050798527896404266 + 0.01 * 6.696741580963135
Epoch 430, val loss: 0.8554587960243225
Epoch 440, training loss: 0.11221956461668015 = 0.045332685112953186 + 0.01 * 6.688687801361084
Epoch 440, val loss: 0.8684728741645813
Epoch 450, training loss: 0.10750243067741394 = 0.04065258055925369 + 0.01 * 6.684985160827637
Epoch 450, val loss: 0.8815465569496155
Epoch 460, training loss: 0.10345934331417084 = 0.036626722663640976 + 0.01 * 6.683261871337891
Epoch 460, val loss: 0.8945407867431641
Epoch 470, training loss: 0.09990549087524414 = 0.03314817324280739 + 0.01 * 6.675731182098389
Epoch 470, val loss: 0.9074358940124512
Epoch 480, training loss: 0.09685990959405899 = 0.030131913721561432 + 0.01 * 6.672799587249756
Epoch 480, val loss: 0.9201340079307556
Epoch 490, training loss: 0.09415340423583984 = 0.02749895676970482 + 0.01 * 6.665444850921631
Epoch 490, val loss: 0.932756245136261
Epoch 500, training loss: 0.09182816743850708 = 0.02519051916897297 + 0.01 * 6.663764953613281
Epoch 500, val loss: 0.9450768232345581
Epoch 510, training loss: 0.08974643051624298 = 0.02315709926187992 + 0.01 * 6.658933162689209
Epoch 510, val loss: 0.9571623802185059
Epoch 520, training loss: 0.08795343339443207 = 0.021358240395784378 + 0.01 * 6.659518718719482
Epoch 520, val loss: 0.9690003395080566
Epoch 530, training loss: 0.08630861341953278 = 0.019760657101869583 + 0.01 * 6.6547956466674805
Epoch 530, val loss: 0.9805907011032104
Epoch 540, training loss: 0.08484620600938797 = 0.01833682507276535 + 0.01 * 6.650938510894775
Epoch 540, val loss: 0.9918578267097473
Epoch 550, training loss: 0.08351365476846695 = 0.017062939703464508 + 0.01 * 6.645071983337402
Epoch 550, val loss: 1.0028860569000244
Epoch 560, training loss: 0.08233511447906494 = 0.015918951481580734 + 0.01 * 6.641616344451904
Epoch 560, val loss: 1.0135701894760132
Epoch 570, training loss: 0.08125270903110504 = 0.014888230711221695 + 0.01 * 6.636448383331299
Epoch 570, val loss: 1.024058222770691
Epoch 580, training loss: 0.08027050644159317 = 0.013956830836832523 + 0.01 * 6.6313676834106445
Epoch 580, val loss: 1.0342199802398682
Epoch 590, training loss: 0.07936297357082367 = 0.013112669810652733 + 0.01 * 6.625030040740967
Epoch 590, val loss: 1.0441303253173828
Epoch 600, training loss: 0.07853186875581741 = 0.012345446273684502 + 0.01 * 6.6186418533325195
Epoch 600, val loss: 1.0537580251693726
Epoch 610, training loss: 0.07785288989543915 = 0.011645953170955181 + 0.01 * 6.620694160461426
Epoch 610, val loss: 1.0631357431411743
Epoch 620, training loss: 0.07721837610006332 = 0.011006630957126617 + 0.01 * 6.621174335479736
Epoch 620, val loss: 1.0723015069961548
Epoch 630, training loss: 0.07657011598348618 = 0.010421025566756725 + 0.01 * 6.614909648895264
Epoch 630, val loss: 1.0811940431594849
Epoch 640, training loss: 0.07598432153463364 = 0.009883065707981586 + 0.01 * 6.610125541687012
Epoch 640, val loss: 1.0898679494857788
Epoch 650, training loss: 0.07546594738960266 = 0.009387610480189323 + 0.01 * 6.607833385467529
Epoch 650, val loss: 1.0983201265335083
Epoch 660, training loss: 0.0749918594956398 = 0.008930807001888752 + 0.01 * 6.606104850769043
Epoch 660, val loss: 1.106533169746399
Epoch 670, training loss: 0.0745583027601242 = 0.008508926257491112 + 0.01 * 6.604937553405762
Epoch 670, val loss: 1.114561915397644
Epoch 680, training loss: 0.0741158276796341 = 0.008118239231407642 + 0.01 * 6.599758625030518
Epoch 680, val loss: 1.1223986148834229
Epoch 690, training loss: 0.07372619956731796 = 0.007755456957966089 + 0.01 * 6.597074508666992
Epoch 690, val loss: 1.1300512552261353
Epoch 700, training loss: 0.07341686636209488 = 0.007418271619826555 + 0.01 * 6.599859237670898
Epoch 700, val loss: 1.1374930143356323
Epoch 710, training loss: 0.07305619865655899 = 0.007104376796633005 + 0.01 * 6.595182418823242
Epoch 710, val loss: 1.1448103189468384
Epoch 720, training loss: 0.07274847477674484 = 0.0068117063492536545 + 0.01 * 6.593676567077637
Epoch 720, val loss: 1.151910424232483
Epoch 730, training loss: 0.07247135043144226 = 0.006538247223943472 + 0.01 * 6.5933098793029785
Epoch 730, val loss: 1.1588313579559326
Epoch 740, training loss: 0.07215644419193268 = 0.0062823062762618065 + 0.01 * 6.587413787841797
Epoch 740, val loss: 1.1655610799789429
Epoch 750, training loss: 0.07185245305299759 = 0.006042518652975559 + 0.01 * 6.58099365234375
Epoch 750, val loss: 1.172157883644104
Epoch 760, training loss: 0.0716199055314064 = 0.005817882716655731 + 0.01 * 6.580202102661133
Epoch 760, val loss: 1.1785720586776733
Epoch 770, training loss: 0.0714406743645668 = 0.005606640595942736 + 0.01 * 6.58340311050415
Epoch 770, val loss: 1.1848077774047852
Epoch 780, training loss: 0.07113929837942123 = 0.0054082185961306095 + 0.01 * 6.573108196258545
Epoch 780, val loss: 1.1909215450286865
Epoch 790, training loss: 0.07100038975477219 = 0.005221147555857897 + 0.01 * 6.577923774719238
Epoch 790, val loss: 1.1968551874160767
Epoch 800, training loss: 0.0707995817065239 = 0.005044931545853615 + 0.01 * 6.575465202331543
Epoch 800, val loss: 1.2026907205581665
Epoch 810, training loss: 0.07057379186153412 = 0.00487868906930089 + 0.01 * 6.569509983062744
Epoch 810, val loss: 1.2083141803741455
Epoch 820, training loss: 0.07038495689630508 = 0.004721698351204395 + 0.01 * 6.566325664520264
Epoch 820, val loss: 1.2138633728027344
Epoch 830, training loss: 0.07032923400402069 = 0.004573095589876175 + 0.01 * 6.575613498687744
Epoch 830, val loss: 1.2192342281341553
Epoch 840, training loss: 0.07008422911167145 = 0.004432485904544592 + 0.01 * 6.565174579620361
Epoch 840, val loss: 1.2245107889175415
Epoch 850, training loss: 0.06997895985841751 = 0.0042991964146494865 + 0.01 * 6.567976951599121
Epoch 850, val loss: 1.2296643257141113
Epoch 860, training loss: 0.06977184116840363 = 0.004172945395112038 + 0.01 * 6.559889793395996
Epoch 860, val loss: 1.2346460819244385
Epoch 870, training loss: 0.06958779692649841 = 0.0040532988496124744 + 0.01 * 6.553449630737305
Epoch 870, val loss: 1.2395578622817993
Epoch 880, training loss: 0.06946513056755066 = 0.003939529415220022 + 0.01 * 6.552560329437256
Epoch 880, val loss: 1.2443434000015259
Epoch 890, training loss: 0.06930923461914062 = 0.0038314114790409803 + 0.01 * 6.5477824211120605
Epoch 890, val loss: 1.2490615844726562
Epoch 900, training loss: 0.06912869215011597 = 0.003728431649506092 + 0.01 * 6.54002571105957
Epoch 900, val loss: 1.2536402940750122
Epoch 910, training loss: 0.06905778497457504 = 0.0036304364912211895 + 0.01 * 6.5427350997924805
Epoch 910, val loss: 1.2580243349075317
Epoch 920, training loss: 0.06891410797834396 = 0.003537343814969063 + 0.01 * 6.537676811218262
Epoch 920, val loss: 1.2624543905258179
Epoch 930, training loss: 0.06880471855401993 = 0.003448501229286194 + 0.01 * 6.5356221199035645
Epoch 930, val loss: 1.266750693321228
Epoch 940, training loss: 0.0685967281460762 = 0.003363501513376832 + 0.01 * 6.523322105407715
Epoch 940, val loss: 1.270903468132019
Epoch 950, training loss: 0.06849557906389236 = 0.003282504854723811 + 0.01 * 6.521307945251465
Epoch 950, val loss: 1.2749966382980347
Epoch 960, training loss: 0.06860950589179993 = 0.003205003682523966 + 0.01 * 6.540450572967529
Epoch 960, val loss: 1.2789363861083984
Epoch 970, training loss: 0.06875589489936829 = 0.003130886238068342 + 0.01 * 6.562501430511475
Epoch 970, val loss: 1.2829052209854126
Epoch 980, training loss: 0.06837329268455505 = 0.003060043789446354 + 0.01 * 6.531324863433838
Epoch 980, val loss: 1.2867424488067627
Epoch 990, training loss: 0.06807175278663635 = 0.0029923401307314634 + 0.01 * 6.507941246032715
Epoch 990, val loss: 1.290411114692688
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7741
Overall ASR: 0.4613
Flip ASR: 0.3644/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0422496795654297 = 1.9585121870040894 + 0.01 * 8.373757362365723
Epoch 0, val loss: 1.9658578634262085
Epoch 10, training loss: 2.032007932662964 = 1.9482717514038086 + 0.01 * 8.373611450195312
Epoch 10, val loss: 1.9542604684829712
Epoch 20, training loss: 2.0194520950317383 = 1.9357212781906128 + 0.01 * 8.37308120727539
Epoch 20, val loss: 1.9396240711212158
Epoch 30, training loss: 2.0020811557769775 = 1.9183664321899414 + 0.01 * 8.371463775634766
Epoch 30, val loss: 1.9190641641616821
Epoch 40, training loss: 1.976784586906433 = 1.893147349357605 + 0.01 * 8.363728523254395
Epoch 40, val loss: 1.8892606496810913
Epoch 50, training loss: 1.9403047561645508 = 1.8571792840957642 + 0.01 * 8.312553405761719
Epoch 50, val loss: 1.847960114479065
Epoch 60, training loss: 1.8902873992919922 = 1.8117927312850952 + 0.01 * 7.849465847015381
Epoch 60, val loss: 1.7990273237228394
Epoch 70, training loss: 1.8372052907943726 = 1.763819932937622 + 0.01 * 7.338531494140625
Epoch 70, val loss: 1.7526874542236328
Epoch 80, training loss: 1.7822442054748535 = 1.7115349769592285 + 0.01 * 7.070917129516602
Epoch 80, val loss: 1.7070403099060059
Epoch 90, training loss: 1.7141971588134766 = 1.644520878791809 + 0.01 * 6.967622756958008
Epoch 90, val loss: 1.6501129865646362
Epoch 100, training loss: 1.626250982284546 = 1.5568721294403076 + 0.01 * 6.9378838539123535
Epoch 100, val loss: 1.5747705698013306
Epoch 110, training loss: 1.5188052654266357 = 1.449661374092102 + 0.01 * 6.914390563964844
Epoch 110, val loss: 1.4847791194915771
Epoch 120, training loss: 1.4001965522766113 = 1.3312549591064453 + 0.01 * 6.894154071807861
Epoch 120, val loss: 1.388566017150879
Epoch 130, training loss: 1.2793364524841309 = 1.2105400562286377 + 0.01 * 6.879635810852051
Epoch 130, val loss: 1.2923390865325928
Epoch 140, training loss: 1.1619987487792969 = 1.0933098793029785 + 0.01 * 6.868888854980469
Epoch 140, val loss: 1.2010612487792969
Epoch 150, training loss: 1.0526785850524902 = 0.9840766191482544 + 0.01 * 6.860198974609375
Epoch 150, val loss: 1.1166682243347168
Epoch 160, training loss: 0.9539751410484314 = 0.8854278326034546 + 0.01 * 6.854729175567627
Epoch 160, val loss: 1.0417006015777588
Epoch 170, training loss: 0.8660696148872375 = 0.797562837600708 + 0.01 * 6.85067892074585
Epoch 170, val loss: 0.975627064704895
Epoch 180, training loss: 0.7879402041435242 = 0.719469428062439 + 0.01 * 6.847077369689941
Epoch 180, val loss: 0.9186197519302368
Epoch 190, training loss: 0.718296468257904 = 0.6498562693595886 + 0.01 * 6.844021797180176
Epoch 190, val loss: 0.869786262512207
Epoch 200, training loss: 0.6558674573898315 = 0.5874543786048889 + 0.01 * 6.84130859375
Epoch 200, val loss: 0.8284202218055725
Epoch 210, training loss: 0.599351167678833 = 0.5309693217277527 + 0.01 * 6.838181495666504
Epoch 210, val loss: 0.7935280799865723
Epoch 220, training loss: 0.5476855039596558 = 0.4793434143066406 + 0.01 * 6.834206581115723
Epoch 220, val loss: 0.7642232179641724
Epoch 230, training loss: 0.5002443194389343 = 0.43195220828056335 + 0.01 * 6.829213619232178
Epoch 230, val loss: 0.7402740716934204
Epoch 240, training loss: 0.45648205280303955 = 0.38825032114982605 + 0.01 * 6.823173522949219
Epoch 240, val loss: 0.7217679619789124
Epoch 250, training loss: 0.4163082540035248 = 0.34814754128456116 + 0.01 * 6.816072463989258
Epoch 250, val loss: 0.7084874510765076
Epoch 260, training loss: 0.3796154260635376 = 0.3115347921848297 + 0.01 * 6.808063507080078
Epoch 260, val loss: 0.6997240781784058
Epoch 270, training loss: 0.34616565704345703 = 0.27817678451538086 + 0.01 * 6.79888916015625
Epoch 270, val loss: 0.6947757601737976
Epoch 280, training loss: 0.31567490100860596 = 0.24778378009796143 + 0.01 * 6.789111614227295
Epoch 280, val loss: 0.6929214596748352
Epoch 290, training loss: 0.2879124581813812 = 0.2201220840215683 + 0.01 * 6.779037952423096
Epoch 290, val loss: 0.6936900019645691
Epoch 300, training loss: 0.26278120279312134 = 0.1950993686914444 + 0.01 * 6.768181800842285
Epoch 300, val loss: 0.6970337629318237
Epoch 310, training loss: 0.24030384421348572 = 0.17273324728012085 + 0.01 * 6.757059574127197
Epoch 310, val loss: 0.702876627445221
Epoch 320, training loss: 0.22045990824699402 = 0.1529829055070877 + 0.01 * 6.747701168060303
Epoch 320, val loss: 0.7111011743545532
Epoch 330, training loss: 0.2030876725912094 = 0.1357003003358841 + 0.01 * 6.738737106323242
Epoch 330, val loss: 0.721497118473053
Epoch 340, training loss: 0.18792715668678284 = 0.12061113864183426 + 0.01 * 6.731601715087891
Epoch 340, val loss: 0.733808159828186
Epoch 350, training loss: 0.17461025714874268 = 0.10735085606575012 + 0.01 * 6.725941181182861
Epoch 350, val loss: 0.7474895715713501
Epoch 360, training loss: 0.16285015642642975 = 0.095645971596241 + 0.01 * 6.720418930053711
Epoch 360, val loss: 0.7621578574180603
Epoch 370, training loss: 0.15243078768253326 = 0.08529175817966461 + 0.01 * 6.713903427124023
Epoch 370, val loss: 0.7777078747749329
Epoch 380, training loss: 0.14308157563209534 = 0.07602620869874954 + 0.01 * 6.705535888671875
Epoch 380, val loss: 0.7937257289886475
Epoch 390, training loss: 0.13479898869991302 = 0.06776133179664612 + 0.01 * 6.703765869140625
Epoch 390, val loss: 0.8103073239326477
Epoch 400, training loss: 0.12737108767032623 = 0.06040392816066742 + 0.01 * 6.69671630859375
Epoch 400, val loss: 0.8272086381912231
Epoch 410, training loss: 0.12079350650310516 = 0.05390351638197899 + 0.01 * 6.688999176025391
Epoch 410, val loss: 0.8445811867713928
Epoch 420, training loss: 0.1151837632060051 = 0.04826413094997406 + 0.01 * 6.691963195800781
Epoch 420, val loss: 0.8619765639305115
Epoch 430, training loss: 0.11015324294567108 = 0.0433725081384182 + 0.01 * 6.678073883056641
Epoch 430, val loss: 0.879563570022583
Epoch 440, training loss: 0.10585135966539383 = 0.03912721574306488 + 0.01 * 6.672414302825928
Epoch 440, val loss: 0.897088885307312
Epoch 450, training loss: 0.10205094516277313 = 0.03541983664035797 + 0.01 * 6.663110733032227
Epoch 450, val loss: 0.9143387675285339
Epoch 460, training loss: 0.09896351397037506 = 0.03217688202857971 + 0.01 * 6.67866325378418
Epoch 460, val loss: 0.9312648773193359
Epoch 470, training loss: 0.09589244425296783 = 0.029335293918848038 + 0.01 * 6.655715465545654
Epoch 470, val loss: 0.9476693272590637
Epoch 480, training loss: 0.09330311417579651 = 0.02683427929878235 + 0.01 * 6.646883964538574
Epoch 480, val loss: 0.9636569619178772
Epoch 490, training loss: 0.09104509651660919 = 0.024627232924103737 + 0.01 * 6.641786575317383
Epoch 490, val loss: 0.9791486859321594
Epoch 500, training loss: 0.08915606886148453 = 0.022674037143588066 + 0.01 * 6.648202896118164
Epoch 500, val loss: 0.9941727519035339
Epoch 510, training loss: 0.08730698376893997 = 0.020938925445079803 + 0.01 * 6.636806011199951
Epoch 510, val loss: 1.008623480796814
Epoch 520, training loss: 0.08576139807701111 = 0.019392434507608414 + 0.01 * 6.636897087097168
Epoch 520, val loss: 1.0225484371185303
Epoch 530, training loss: 0.08429867774248123 = 0.018009936437010765 + 0.01 * 6.628873825073242
Epoch 530, val loss: 1.0360008478164673
Epoch 540, training loss: 0.0829816460609436 = 0.016769882291555405 + 0.01 * 6.621176719665527
Epoch 540, val loss: 1.0489751100540161
Epoch 550, training loss: 0.0818791538476944 = 0.015654103830456734 + 0.01 * 6.6225056648254395
Epoch 550, val loss: 1.061532974243164
Epoch 560, training loss: 0.0808325707912445 = 0.014647572301328182 + 0.01 * 6.618500232696533
Epoch 560, val loss: 1.0736662149429321
Epoch 570, training loss: 0.07987697422504425 = 0.013736184686422348 + 0.01 * 6.614078521728516
Epoch 570, val loss: 1.0854089260101318
Epoch 580, training loss: 0.0789918601512909 = 0.012908997014164925 + 0.01 * 6.6082868576049805
Epoch 580, val loss: 1.0967659950256348
Epoch 590, training loss: 0.07821217179298401 = 0.012156563811004162 + 0.01 * 6.605561256408691
Epoch 590, val loss: 1.107724905014038
Epoch 600, training loss: 0.07765921950340271 = 0.011469748802483082 + 0.01 * 6.6189470291137695
Epoch 600, val loss: 1.1183390617370605
Epoch 610, training loss: 0.07683159410953522 = 0.010839514434337616 + 0.01 * 6.599207878112793
Epoch 610, val loss: 1.1287119388580322
Epoch 620, training loss: 0.07622043788433075 = 0.01026092004030943 + 0.01 * 6.59595251083374
Epoch 620, val loss: 1.1387617588043213
Epoch 630, training loss: 0.07565198838710785 = 0.009729137644171715 + 0.01 * 6.59228515625
Epoch 630, val loss: 1.1484754085540771
Epoch 640, training loss: 0.07536709308624268 = 0.009239809587597847 + 0.01 * 6.612728595733643
Epoch 640, val loss: 1.1579118967056274
Epoch 650, training loss: 0.0747036337852478 = 0.008789771236479282 + 0.01 * 6.591386795043945
Epoch 650, val loss: 1.1671040058135986
Epoch 660, training loss: 0.07429030537605286 = 0.008375730365514755 + 0.01 * 6.591457366943359
Epoch 660, val loss: 1.1759337186813354
Epoch 670, training loss: 0.07380589097738266 = 0.007992567494511604 + 0.01 * 6.581332683563232
Epoch 670, val loss: 1.1845343112945557
Epoch 680, training loss: 0.07340535521507263 = 0.007637161761522293 + 0.01 * 6.576818943023682
Epoch 680, val loss: 1.1928786039352417
Epoch 690, training loss: 0.07308071851730347 = 0.007306910119950771 + 0.01 * 6.577380657196045
Epoch 690, val loss: 1.2010326385498047
Epoch 700, training loss: 0.07274753600358963 = 0.006999474484473467 + 0.01 * 6.574806213378906
Epoch 700, val loss: 1.2089475393295288
Epoch 710, training loss: 0.07245370000600815 = 0.006712689064443111 + 0.01 * 6.574100971221924
Epoch 710, val loss: 1.2166277170181274
Epoch 720, training loss: 0.07226486504077911 = 0.0064449617639184 + 0.01 * 6.581990718841553
Epoch 720, val loss: 1.2241438627243042
Epoch 730, training loss: 0.07181624323129654 = 0.006194670684635639 + 0.01 * 6.562157154083252
Epoch 730, val loss: 1.23147714138031
Epoch 740, training loss: 0.07168646156787872 = 0.005960232112556696 + 0.01 * 6.572622776031494
Epoch 740, val loss: 1.2385789155960083
Epoch 750, training loss: 0.07130967825651169 = 0.005740461405366659 + 0.01 * 6.556921482086182
Epoch 750, val loss: 1.2455267906188965
Epoch 760, training loss: 0.07113202661275864 = 0.005534134339541197 + 0.01 * 6.559789657592773
Epoch 760, val loss: 1.252253770828247
Epoch 770, training loss: 0.0709148645401001 = 0.005340352188795805 + 0.01 * 6.557451248168945
Epoch 770, val loss: 1.2588856220245361
Epoch 780, training loss: 0.07066156715154648 = 0.005157969892024994 + 0.01 * 6.550360202789307
Epoch 780, val loss: 1.265259027481079
Epoch 790, training loss: 0.07043879479169846 = 0.004986039828509092 + 0.01 * 6.545275688171387
Epoch 790, val loss: 1.2715641260147095
Epoch 800, training loss: 0.0701771229505539 = 0.004823711700737476 + 0.01 * 6.535340785980225
Epoch 800, val loss: 1.2776782512664795
Epoch 810, training loss: 0.07019884884357452 = 0.004670548718422651 + 0.01 * 6.552830219268799
Epoch 810, val loss: 1.283677339553833
Epoch 820, training loss: 0.06991147994995117 = 0.004525844939053059 + 0.01 * 6.5385637283325195
Epoch 820, val loss: 1.2895094156265259
Epoch 830, training loss: 0.06976119428873062 = 0.00438880268484354 + 0.01 * 6.537239074707031
Epoch 830, val loss: 1.2952176332473755
Epoch 840, training loss: 0.06955946981906891 = 0.004258954897522926 + 0.01 * 6.530052185058594
Epoch 840, val loss: 1.3008531332015991
Epoch 850, training loss: 0.06949407607316971 = 0.00413602264598012 + 0.01 * 6.5358052253723145
Epoch 850, val loss: 1.3062692880630493
Epoch 860, training loss: 0.06913236528635025 = 0.004019617103040218 + 0.01 * 6.511274814605713
Epoch 860, val loss: 1.3116931915283203
Epoch 870, training loss: 0.06918521970510483 = 0.003908948507159948 + 0.01 * 6.527627468109131
Epoch 870, val loss: 1.3169230222702026
Epoch 880, training loss: 0.06892136484384537 = 0.0038037290796637535 + 0.01 * 6.511763572692871
Epoch 880, val loss: 1.3221014738082886
Epoch 890, training loss: 0.06878546625375748 = 0.0037036631256341934 + 0.01 * 6.508180618286133
Epoch 890, val loss: 1.327083706855774
Epoch 900, training loss: 0.06887054443359375 = 0.0036084481980651617 + 0.01 * 6.526209831237793
Epoch 900, val loss: 1.332065224647522
Epoch 910, training loss: 0.06842675060033798 = 0.0035177082754671574 + 0.01 * 6.490904331207275
Epoch 910, val loss: 1.3369399309158325
Epoch 920, training loss: 0.06846119463443756 = 0.0034312717616558075 + 0.01 * 6.502992630004883
Epoch 920, val loss: 1.3415745496749878
Epoch 930, training loss: 0.06832389533519745 = 0.0033488667104393244 + 0.01 * 6.497503280639648
Epoch 930, val loss: 1.3462448120117188
Epoch 940, training loss: 0.06809113919734955 = 0.0032700274605304003 + 0.01 * 6.482110977172852
Epoch 940, val loss: 1.3507905006408691
Epoch 950, training loss: 0.06812401115894318 = 0.003194715129211545 + 0.01 * 6.492929935455322
Epoch 950, val loss: 1.3551770448684692
Epoch 960, training loss: 0.06779403984546661 = 0.003122697351500392 + 0.01 * 6.467134475708008
Epoch 960, val loss: 1.3595701456069946
Epoch 970, training loss: 0.06782616674900055 = 0.0030537392012774944 + 0.01 * 6.477242946624756
Epoch 970, val loss: 1.3638142347335815
Epoch 980, training loss: 0.06769337505102158 = 0.002987668151035905 + 0.01 * 6.4705705642700195
Epoch 980, val loss: 1.3680826425552368
Epoch 990, training loss: 0.06756500899791718 = 0.0029243214521557093 + 0.01 * 6.464068412780762
Epoch 990, val loss: 1.3721181154251099
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.8930
Flip ASR: 0.8756/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0286502838134766 = 1.9449131488800049 + 0.01 * 8.373724937438965
Epoch 0, val loss: 1.9456206560134888
Epoch 10, training loss: 2.018115520477295 = 1.9343798160552979 + 0.01 * 8.373566627502441
Epoch 10, val loss: 1.9346731901168823
Epoch 20, training loss: 2.0049591064453125 = 1.9212291240692139 + 0.01 * 8.373004913330078
Epoch 20, val loss: 1.9208520650863647
Epoch 30, training loss: 1.9861944913864136 = 1.902484655380249 + 0.01 * 8.370985984802246
Epoch 30, val loss: 1.9012548923492432
Epoch 40, training loss: 1.9583555459976196 = 1.8747706413269043 + 0.01 * 8.358488082885742
Epoch 40, val loss: 1.8728768825531006
Epoch 50, training loss: 1.9198732376098633 = 1.8369516134262085 + 0.01 * 8.292159080505371
Epoch 50, val loss: 1.8361560106277466
Epoch 60, training loss: 1.875159740447998 = 1.7954020500183105 + 0.01 * 7.975769996643066
Epoch 60, val loss: 1.7997523546218872
Epoch 70, training loss: 1.8333873748779297 = 1.756521463394165 + 0.01 * 7.686596870422363
Epoch 70, val loss: 1.7668242454528809
Epoch 80, training loss: 1.7805263996124268 = 1.7068346738815308 + 0.01 * 7.369176387786865
Epoch 80, val loss: 1.7215826511383057
Epoch 90, training loss: 1.7102659940719604 = 1.639107346534729 + 0.01 * 7.115867614746094
Epoch 90, val loss: 1.6615782976150513
Epoch 100, training loss: 1.6212931871414185 = 1.5521435737609863 + 0.01 * 6.9149580001831055
Epoch 100, val loss: 1.5884759426116943
Epoch 110, training loss: 1.5223795175552368 = 1.453749418258667 + 0.01 * 6.863008975982666
Epoch 110, val loss: 1.5067293643951416
Epoch 120, training loss: 1.42083740234375 = 1.3523613214492798 + 0.01 * 6.847606182098389
Epoch 120, val loss: 1.4241067171096802
Epoch 130, training loss: 1.3208521604537964 = 1.2524083852767944 + 0.01 * 6.8443756103515625
Epoch 130, val loss: 1.3444517850875854
Epoch 140, training loss: 1.2236751317977905 = 1.1552934646606445 + 0.01 * 6.838167190551758
Epoch 140, val loss: 1.2692978382110596
Epoch 150, training loss: 1.1308015584945679 = 1.062499761581421 + 0.01 * 6.830185413360596
Epoch 150, val loss: 1.1981899738311768
Epoch 160, training loss: 1.0437700748443604 = 0.9755340814590454 + 0.01 * 6.823594093322754
Epoch 160, val loss: 1.132371425628662
Epoch 170, training loss: 0.9628558158874512 = 0.8946518301963806 + 0.01 * 6.820399761199951
Epoch 170, val loss: 1.0718977451324463
Epoch 180, training loss: 0.8867418169975281 = 0.8185418248176575 + 0.01 * 6.819998264312744
Epoch 180, val loss: 1.0156046152114868
Epoch 190, training loss: 0.8136814832687378 = 0.7454793453216553 + 0.01 * 6.820215225219727
Epoch 190, val loss: 0.9619051218032837
Epoch 200, training loss: 0.7435890436172485 = 0.6753944158554077 + 0.01 * 6.819461822509766
Epoch 200, val loss: 0.9105210304260254
Epoch 210, training loss: 0.6782271265983582 = 0.6100515127182007 + 0.01 * 6.817564010620117
Epoch 210, val loss: 0.863468587398529
Epoch 220, training loss: 0.6196008324623108 = 0.5514540076255798 + 0.01 * 6.8146843910217285
Epoch 220, val loss: 0.8231731057167053
Epoch 230, training loss: 0.5683433413505554 = 0.5002357959747314 + 0.01 * 6.8107523918151855
Epoch 230, val loss: 0.7908897399902344
Epoch 240, training loss: 0.5234840512275696 = 0.4554283916950226 + 0.01 * 6.805563926696777
Epoch 240, val loss: 0.7659592628479004
Epoch 250, training loss: 0.4832623600959778 = 0.4152713716030121 + 0.01 * 6.799098491668701
Epoch 250, val loss: 0.7466796040534973
Epoch 260, training loss: 0.44587600231170654 = 0.3779614269733429 + 0.01 * 6.79145622253418
Epoch 260, val loss: 0.7310633659362793
Epoch 270, training loss: 0.40996086597442627 = 0.3421122431755066 + 0.01 * 6.7848639488220215
Epoch 270, val loss: 0.7179408669471741
Epoch 280, training loss: 0.37456437945365906 = 0.30681437253952026 + 0.01 * 6.775001049041748
Epoch 280, val loss: 0.7066300511360168
Epoch 290, training loss: 0.3395592272281647 = 0.27189943194389343 + 0.01 * 6.765980243682861
Epoch 290, val loss: 0.6967124938964844
Epoch 300, training loss: 0.3053086996078491 = 0.2377496212720871 + 0.01 * 6.755908966064453
Epoch 300, val loss: 0.688292920589447
Epoch 310, training loss: 0.2726766765117645 = 0.20521847903728485 + 0.01 * 6.745819568634033
Epoch 310, val loss: 0.6819231510162354
Epoch 320, training loss: 0.24281765520572662 = 0.17544397711753845 + 0.01 * 6.737368106842041
Epoch 320, val loss: 0.6782025098800659
Epoch 330, training loss: 0.21656368672847748 = 0.14925509691238403 + 0.01 * 6.73085880279541
Epoch 330, val loss: 0.6774728894233704
Epoch 340, training loss: 0.1941969394683838 = 0.12697097659111023 + 0.01 * 6.722596645355225
Epoch 340, val loss: 0.6799418330192566
Epoch 350, training loss: 0.17562270164489746 = 0.10842116177082062 + 0.01 * 6.720153331756592
Epoch 350, val loss: 0.685599148273468
Epoch 360, training loss: 0.1603008508682251 = 0.09314320236444473 + 0.01 * 6.715765476226807
Epoch 360, val loss: 0.6940107345581055
Epoch 370, training loss: 0.14764906466007233 = 0.08055818825960159 + 0.01 * 6.70908784866333
Epoch 370, val loss: 0.7046703100204468
Epoch 380, training loss: 0.1371772736310959 = 0.07013628631830215 + 0.01 * 6.704098701477051
Epoch 380, val loss: 0.717054009437561
Epoch 390, training loss: 0.12846720218658447 = 0.061438076198101044 + 0.01 * 6.7029128074646
Epoch 390, val loss: 0.7306237816810608
Epoch 400, training loss: 0.12105889618396759 = 0.05412169173359871 + 0.01 * 6.693719863891602
Epoch 400, val loss: 0.7450246810913086
Epoch 410, training loss: 0.11484356969594955 = 0.04792366921901703 + 0.01 * 6.691989898681641
Epoch 410, val loss: 0.7599262595176697
Epoch 420, training loss: 0.1095026358962059 = 0.04264533519744873 + 0.01 * 6.685730457305908
Epoch 420, val loss: 0.7751166820526123
Epoch 430, training loss: 0.10491588711738586 = 0.03812888637185097 + 0.01 * 6.678699970245361
Epoch 430, val loss: 0.7902673482894897
Epoch 440, training loss: 0.10109208524227142 = 0.034249141812324524 + 0.01 * 6.684294700622559
Epoch 440, val loss: 0.8052467703819275
Epoch 450, training loss: 0.09761931002140045 = 0.030910488218069077 + 0.01 * 6.670882701873779
Epoch 450, val loss: 0.8203230500221252
Epoch 460, training loss: 0.09464412927627563 = 0.028023073449730873 + 0.01 * 6.662106037139893
Epoch 460, val loss: 0.8348669409751892
Epoch 470, training loss: 0.0920945405960083 = 0.02551327645778656 + 0.01 * 6.6581268310546875
Epoch 470, val loss: 0.8492785692214966
Epoch 480, training loss: 0.08979558944702148 = 0.02332271821796894 + 0.01 * 6.647287368774414
Epoch 480, val loss: 0.8632327914237976
Epoch 490, training loss: 0.08788185566663742 = 0.0214023869484663 + 0.01 * 6.647946834564209
Epoch 490, val loss: 0.876858651638031
Epoch 500, training loss: 0.08614751696586609 = 0.019717130810022354 + 0.01 * 6.643038272857666
Epoch 500, val loss: 0.8899575471878052
Epoch 510, training loss: 0.08457809686660767 = 0.01822759583592415 + 0.01 * 6.6350507736206055
Epoch 510, val loss: 0.9027132987976074
Epoch 520, training loss: 0.08319517225027084 = 0.01690332591533661 + 0.01 * 6.629184722900391
Epoch 520, val loss: 0.9150538444519043
Epoch 530, training loss: 0.08196818828582764 = 0.015720076858997345 + 0.01 * 6.62481164932251
Epoch 530, val loss: 0.9271140694618225
Epoch 540, training loss: 0.08102144300937653 = 0.014658347703516483 + 0.01 * 6.63631010055542
Epoch 540, val loss: 0.9388065934181213
Epoch 550, training loss: 0.07993737608194351 = 0.013703382574021816 + 0.01 * 6.62339973449707
Epoch 550, val loss: 0.9501599073410034
Epoch 560, training loss: 0.0789865180850029 = 0.012837739661335945 + 0.01 * 6.614878177642822
Epoch 560, val loss: 0.9613498449325562
Epoch 570, training loss: 0.07822371274232864 = 0.01205342449247837 + 0.01 * 6.617029190063477
Epoch 570, val loss: 0.9720941781997681
Epoch 580, training loss: 0.07743079215288162 = 0.011340640485286713 + 0.01 * 6.609014987945557
Epoch 580, val loss: 0.9825314283370972
Epoch 590, training loss: 0.07673099637031555 = 0.010691117495298386 + 0.01 * 6.603987693786621
Epoch 590, val loss: 0.9927309155464172
Epoch 600, training loss: 0.07611881196498871 = 0.010098236612975597 + 0.01 * 6.602057933807373
Epoch 600, val loss: 1.002663254737854
Epoch 610, training loss: 0.07554488629102707 = 0.009554854594171047 + 0.01 * 6.599003314971924
Epoch 610, val loss: 1.0123536586761475
Epoch 620, training loss: 0.07500830292701721 = 0.00905535090714693 + 0.01 * 6.595294952392578
Epoch 620, val loss: 1.0218162536621094
Epoch 630, training loss: 0.0747215673327446 = 0.008595192804932594 + 0.01 * 6.612637519836426
Epoch 630, val loss: 1.0311250686645508
Epoch 640, training loss: 0.07408683001995087 = 0.008173123933374882 + 0.01 * 6.591371059417725
Epoch 640, val loss: 1.0400238037109375
Epoch 650, training loss: 0.07368440181016922 = 0.0077836583368480206 + 0.01 * 6.59007453918457
Epoch 650, val loss: 1.0487173795700073
Epoch 660, training loss: 0.07326826453208923 = 0.007422758266329765 + 0.01 * 6.584550380706787
Epoch 660, val loss: 1.057224988937378
Epoch 670, training loss: 0.07301414757966995 = 0.007087835110723972 + 0.01 * 6.592631816864014
Epoch 670, val loss: 1.0655723810195923
Epoch 680, training loss: 0.07261882722377777 = 0.0067771850153803825 + 0.01 * 6.584164619445801
Epoch 680, val loss: 1.0736485719680786
Epoch 690, training loss: 0.07225693762302399 = 0.0064879488199949265 + 0.01 * 6.57689905166626
Epoch 690, val loss: 1.0815727710723877
Epoch 700, training loss: 0.07205140590667725 = 0.006218370050191879 + 0.01 * 6.583303928375244
Epoch 700, val loss: 1.0892606973648071
Epoch 710, training loss: 0.07171580940485 = 0.005967248696833849 + 0.01 * 6.574855804443359
Epoch 710, val loss: 1.096832036972046
Epoch 720, training loss: 0.07140423357486725 = 0.005732135381549597 + 0.01 * 6.5672101974487305
Epoch 720, val loss: 1.1041921377182007
Epoch 730, training loss: 0.07133117318153381 = 0.00551196001470089 + 0.01 * 6.581921577453613
Epoch 730, val loss: 1.111366868019104
Epoch 740, training loss: 0.0709950253367424 = 0.005306457169353962 + 0.01 * 6.568856716156006
Epoch 740, val loss: 1.1183736324310303
Epoch 750, training loss: 0.07076945900917053 = 0.005113585386425257 + 0.01 * 6.565587520599365
Epoch 750, val loss: 1.1251885890960693
Epoch 760, training loss: 0.07051697373390198 = 0.0049323332495987415 + 0.01 * 6.5584635734558105
Epoch 760, val loss: 1.131838321685791
Epoch 770, training loss: 0.07028317451477051 = 0.004761683288961649 + 0.01 * 6.552149295806885
Epoch 770, val loss: 1.138361930847168
Epoch 780, training loss: 0.07017233967781067 = 0.004600402433425188 + 0.01 * 6.557193756103516
Epoch 780, val loss: 1.1447895765304565
Epoch 790, training loss: 0.07003206759691238 = 0.004448705818504095 + 0.01 * 6.5583367347717285
Epoch 790, val loss: 1.1509941816329956
Epoch 800, training loss: 0.06980003416538239 = 0.004305175971239805 + 0.01 * 6.549485683441162
Epoch 800, val loss: 1.1571236848831177
Epoch 810, training loss: 0.06980588287115097 = 0.004169391933828592 + 0.01 * 6.5636491775512695
Epoch 810, val loss: 1.1630465984344482
Epoch 820, training loss: 0.06947731226682663 = 0.004041025415062904 + 0.01 * 6.543629169464111
Epoch 820, val loss: 1.1689515113830566
Epoch 830, training loss: 0.0692933201789856 = 0.00391949899494648 + 0.01 * 6.537382125854492
Epoch 830, val loss: 1.1746459007263184
Epoch 840, training loss: 0.06940809637308121 = 0.0038039872888475657 + 0.01 * 6.560410499572754
Epoch 840, val loss: 1.18025803565979
Epoch 850, training loss: 0.06897187232971191 = 0.0036945536267012358 + 0.01 * 6.527731895446777
Epoch 850, val loss: 1.1857637166976929
Epoch 860, training loss: 0.06909984350204468 = 0.0035907039418816566 + 0.01 * 6.550914287567139
Epoch 860, val loss: 1.1910914182662964
Epoch 870, training loss: 0.06885578483343124 = 0.003492363728582859 + 0.01 * 6.536342144012451
Epoch 870, val loss: 1.1963624954223633
Epoch 880, training loss: 0.06857354938983917 = 0.0033987490460276604 + 0.01 * 6.517480373382568
Epoch 880, val loss: 1.2014694213867188
Epoch 890, training loss: 0.06862512975931168 = 0.00330953486263752 + 0.01 * 6.531559944152832
Epoch 890, val loss: 1.2064956426620483
Epoch 900, training loss: 0.06845968216657639 = 0.0032249242067337036 + 0.01 * 6.5234761238098145
Epoch 900, val loss: 1.21140456199646
Epoch 910, training loss: 0.06829625368118286 = 0.0031439000740647316 + 0.01 * 6.515235424041748
Epoch 910, val loss: 1.2162326574325562
Epoch 920, training loss: 0.06829284131526947 = 0.003066882025450468 + 0.01 * 6.52259635925293
Epoch 920, val loss: 1.220853567123413
Epoch 930, training loss: 0.06808673590421677 = 0.002993389731273055 + 0.01 * 6.509334564208984
Epoch 930, val loss: 1.2255499362945557
Epoch 940, training loss: 0.06805765628814697 = 0.0029232304077595472 + 0.01 * 6.5134429931640625
Epoch 940, val loss: 1.2300899028778076
Epoch 950, training loss: 0.06774136424064636 = 0.0028561074286699295 + 0.01 * 6.488526344299316
Epoch 950, val loss: 1.2344971895217896
Epoch 960, training loss: 0.06783424317836761 = 0.0027918005362153053 + 0.01 * 6.504244804382324
Epoch 960, val loss: 1.2388458251953125
Epoch 970, training loss: 0.06770257651805878 = 0.0027304119430482388 + 0.01 * 6.497216701507568
Epoch 970, val loss: 1.243116021156311
Epoch 980, training loss: 0.06784503161907196 = 0.002671574940904975 + 0.01 * 6.517345428466797
Epoch 980, val loss: 1.2472641468048096
Epoch 990, training loss: 0.06747966259717941 = 0.002615518169477582 + 0.01 * 6.486414432525635
Epoch 990, val loss: 1.2513494491577148
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.9520
Flip ASR: 0.9422/225 nodes
The final ASR:0.76876, 0.21877, Accuracy:0.79753, 0.01720
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11672])
remove edge: torch.Size([2, 9530])
updated graph: torch.Size([2, 10646])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9668
Flip ASR: 0.9600/225 nodes
The final ASR:0.97540, 0.00758, Accuracy:0.83210, 0.00462
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0.4 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.028747320175171 = 1.9450091123580933 + 0.01 * 8.373817443847656
Epoch 0, val loss: 1.9425084590911865
Epoch 10, training loss: 2.0183515548706055 = 1.9346145391464233 + 0.01 * 8.373710632324219
Epoch 10, val loss: 1.9325097799301147
Epoch 20, training loss: 2.0051937103271484 = 1.9214597940444946 + 0.01 * 8.373390197753906
Epoch 20, val loss: 1.9195493459701538
Epoch 30, training loss: 1.986336588859558 = 1.9026120901107788 + 0.01 * 8.372451782226562
Epoch 30, val loss: 1.9008262157440186
Epoch 40, training loss: 1.9583014249801636 = 1.8746222257614136 + 0.01 * 8.36791706085205
Epoch 40, val loss: 1.8735774755477905
Epoch 50, training loss: 1.9196513891220093 = 1.8362151384353638 + 0.01 * 8.343622207641602
Epoch 50, val loss: 1.838559865951538
Epoch 60, training loss: 1.8766857385635376 = 1.7943644523620605 + 0.01 * 8.232129096984863
Epoch 60, val loss: 1.8053802251815796
Epoch 70, training loss: 1.8359324932098389 = 1.7562947273254395 + 0.01 * 7.963776588439941
Epoch 70, val loss: 1.777223825454712
Epoch 80, training loss: 1.7834136486053467 = 1.7079854011535645 + 0.01 * 7.542829990386963
Epoch 80, val loss: 1.7363570928573608
Epoch 90, training loss: 1.7142093181610107 = 1.641992211341858 + 0.01 * 7.221709728240967
Epoch 90, val loss: 1.6802761554718018
Epoch 100, training loss: 1.6270142793655396 = 1.5559825897216797 + 0.01 * 7.1031694412231445
Epoch 100, val loss: 1.6097233295440674
Epoch 110, training loss: 1.527153491973877 = 1.4567389488220215 + 0.01 * 7.041448593139648
Epoch 110, val loss: 1.5303568840026855
Epoch 120, training loss: 1.4235780239105225 = 1.353554368019104 + 0.01 * 7.002371311187744
Epoch 120, val loss: 1.4494531154632568
Epoch 130, training loss: 1.3213351964950562 = 1.251524567604065 + 0.01 * 6.981060981750488
Epoch 130, val loss: 1.3692848682403564
Epoch 140, training loss: 1.222334384918213 = 1.152674913406372 + 0.01 * 6.9659504890441895
Epoch 140, val loss: 1.291603446006775
Epoch 150, training loss: 1.1285945177078247 = 1.0590555667877197 + 0.01 * 6.953900337219238
Epoch 150, val loss: 1.217343807220459
Epoch 160, training loss: 1.0412590503692627 = 0.9718102812767029 + 0.01 * 6.944871425628662
Epoch 160, val loss: 1.1486775875091553
Epoch 170, training loss: 0.9593985676765442 = 0.8899976015090942 + 0.01 * 6.940094470977783
Epoch 170, val loss: 1.0858522653579712
Epoch 180, training loss: 0.8809558153152466 = 0.81160968542099 + 0.01 * 6.934612274169922
Epoch 180, val loss: 1.027351975440979
Epoch 190, training loss: 0.8049644231796265 = 0.7356557250022888 + 0.01 * 6.930868625640869
Epoch 190, val loss: 0.9722836017608643
Epoch 200, training loss: 0.732496976852417 = 0.663221538066864 + 0.01 * 6.927541255950928
Epoch 200, val loss: 0.9210830926895142
Epoch 210, training loss: 0.6657098531723022 = 0.5964695811271667 + 0.01 * 6.9240288734436035
Epoch 210, val loss: 0.8753631114959717
Epoch 220, training loss: 0.6062896847724915 = 0.5370792746543884 + 0.01 * 6.921041965484619
Epoch 220, val loss: 0.8366899490356445
Epoch 230, training loss: 0.5541621446609497 = 0.4850131869316101 + 0.01 * 6.914897918701172
Epoch 230, val loss: 0.8055659532546997
Epoch 240, training loss: 0.5076512694358826 = 0.4385658800601959 + 0.01 * 6.908538818359375
Epoch 240, val loss: 0.7805290222167969
Epoch 250, training loss: 0.4644472002983093 = 0.3954412341117859 + 0.01 * 6.90059757232666
Epoch 250, val loss: 0.7596676349639893
Epoch 260, training loss: 0.42284050583839417 = 0.35389986634254456 + 0.01 * 6.894064426422119
Epoch 260, val loss: 0.7414664626121521
Epoch 270, training loss: 0.38220274448394775 = 0.31334778666496277 + 0.01 * 6.885494709014893
Epoch 270, val loss: 0.7253879308700562
Epoch 280, training loss: 0.34328004717826843 = 0.27437976002693176 + 0.01 * 6.8900299072265625
Epoch 280, val loss: 0.7115886807441711
Epoch 290, training loss: 0.3069453835487366 = 0.23824428021907806 + 0.01 * 6.870110034942627
Epoch 290, val loss: 0.7004020810127258
Epoch 300, training loss: 0.27455154061317444 = 0.2059619128704071 + 0.01 * 6.858962535858154
Epoch 300, val loss: 0.6922184228897095
Epoch 310, training loss: 0.2464495450258255 = 0.17792858183383942 + 0.01 * 6.8520965576171875
Epoch 310, val loss: 0.6873536705970764
Epoch 320, training loss: 0.2225780487060547 = 0.15400101244449615 + 0.01 * 6.8577046394348145
Epoch 320, val loss: 0.6856713891029358
Epoch 330, training loss: 0.2020668089389801 = 0.13371893763542175 + 0.01 * 6.834787845611572
Epoch 330, val loss: 0.6868239045143127
Epoch 340, training loss: 0.1849602311849594 = 0.11652664095163345 + 0.01 * 6.843359470367432
Epoch 340, val loss: 0.6905353665351868
Epoch 350, training loss: 0.1701262891292572 = 0.10194652527570724 + 0.01 * 6.817976474761963
Epoch 350, val loss: 0.6962350010871887
Epoch 360, training loss: 0.1576484739780426 = 0.08954731374979019 + 0.01 * 6.810115814208984
Epoch 360, val loss: 0.7033546566963196
Epoch 370, training loss: 0.14702540636062622 = 0.07898314297199249 + 0.01 * 6.804226398468018
Epoch 370, val loss: 0.7115468382835388
Epoch 380, training loss: 0.13793379068374634 = 0.06996200978755951 + 0.01 * 6.797178268432617
Epoch 380, val loss: 0.7204530239105225
Epoch 390, training loss: 0.13015443086624146 = 0.062235645949840546 + 0.01 * 6.791878700256348
Epoch 390, val loss: 0.7298709154129028
Epoch 400, training loss: 0.12346761673688889 = 0.055597491562366486 + 0.01 * 6.787012577056885
Epoch 400, val loss: 0.7396343350410461
Epoch 410, training loss: 0.11765576899051666 = 0.04987366124987602 + 0.01 * 6.7782111167907715
Epoch 410, val loss: 0.749632716178894
Epoch 420, training loss: 0.1126457154750824 = 0.04491719231009483 + 0.01 * 6.772851943969727
Epoch 420, val loss: 0.7597043514251709
Epoch 430, training loss: 0.10872790217399597 = 0.04060631990432739 + 0.01 * 6.812158107757568
Epoch 430, val loss: 0.7698166966438293
Epoch 440, training loss: 0.10456257313489914 = 0.03685227036476135 + 0.01 * 6.771030426025391
Epoch 440, val loss: 0.7798420190811157
Epoch 450, training loss: 0.10111517459154129 = 0.03356529772281647 + 0.01 * 6.754988193511963
Epoch 450, val loss: 0.7897838354110718
Epoch 460, training loss: 0.09817077964544296 = 0.030674248933792114 + 0.01 * 6.749652862548828
Epoch 460, val loss: 0.7996448278427124
Epoch 470, training loss: 0.09556445479393005 = 0.028125159442424774 + 0.01 * 6.743929386138916
Epoch 470, val loss: 0.8093233704566956
Epoch 480, training loss: 0.09326750785112381 = 0.025870854035019875 + 0.01 * 6.739665508270264
Epoch 480, val loss: 0.8188799619674683
Epoch 490, training loss: 0.09123412519693375 = 0.02386871539056301 + 0.01 * 6.736540794372559
Epoch 490, val loss: 0.8281814455986023
Epoch 500, training loss: 0.08931836485862732 = 0.02208518795669079 + 0.01 * 6.723317623138428
Epoch 500, val loss: 0.8373265862464905
Epoch 510, training loss: 0.08801306039094925 = 0.020489543676376343 + 0.01 * 6.752351760864258
Epoch 510, val loss: 0.846205472946167
Epoch 520, training loss: 0.08615930378437042 = 0.01906195655465126 + 0.01 * 6.70973539352417
Epoch 520, val loss: 0.8549023270606995
Epoch 530, training loss: 0.08482763916254044 = 0.01777828484773636 + 0.01 * 6.704935550689697
Epoch 530, val loss: 0.8634375333786011
Epoch 540, training loss: 0.0835665762424469 = 0.016620850190520287 + 0.01 * 6.694572448730469
Epoch 540, val loss: 0.8717570900917053
Epoch 550, training loss: 0.08250804990530014 = 0.015575031749904156 + 0.01 * 6.693302154541016
Epoch 550, val loss: 0.879848062992096
Epoch 560, training loss: 0.08159630745649338 = 0.0146259143948555 + 0.01 * 6.6970391273498535
Epoch 560, val loss: 0.8877383470535278
Epoch 570, training loss: 0.08053270727396011 = 0.013762771151959896 + 0.01 * 6.676993370056152
Epoch 570, val loss: 0.8954251408576965
Epoch 580, training loss: 0.07975047081708908 = 0.012975580990314484 + 0.01 * 6.677489280700684
Epoch 580, val loss: 0.9029296636581421
Epoch 590, training loss: 0.0788910910487175 = 0.012256951071321964 + 0.01 * 6.663414001464844
Epoch 590, val loss: 0.9102593660354614
Epoch 600, training loss: 0.07832884043455124 = 0.011598901823163033 + 0.01 * 6.672994136810303
Epoch 600, val loss: 0.9173880219459534
Epoch 610, training loss: 0.077627032995224 = 0.010995697230100632 + 0.01 * 6.66313362121582
Epoch 610, val loss: 0.9243740439414978
Epoch 620, training loss: 0.07691887021064758 = 0.010440800338983536 + 0.01 * 6.647806644439697
Epoch 620, val loss: 0.9311703443527222
Epoch 630, training loss: 0.07653462886810303 = 0.009929537773132324 + 0.01 * 6.6605095863342285
Epoch 630, val loss: 0.9378336668014526
Epoch 640, training loss: 0.0759325623512268 = 0.009457606822252274 + 0.01 * 6.647495746612549
Epoch 640, val loss: 0.9443067908287048
Epoch 650, training loss: 0.07535162568092346 = 0.009021022357046604 + 0.01 * 6.633060932159424
Epoch 650, val loss: 0.9506754279136658
Epoch 660, training loss: 0.07489603012800217 = 0.00861575547605753 + 0.01 * 6.628027439117432
Epoch 660, val loss: 0.956845223903656
Epoch 670, training loss: 0.07453003525733948 = 0.00823962315917015 + 0.01 * 6.6290411949157715
Epoch 670, val loss: 0.9628639221191406
Epoch 680, training loss: 0.07402344048023224 = 0.007889936678111553 + 0.01 * 6.613350868225098
Epoch 680, val loss: 0.9687464833259583
Epoch 690, training loss: 0.07387971132993698 = 0.007563856430351734 + 0.01 * 6.631585121154785
Epoch 690, val loss: 0.9745312929153442
Epoch 700, training loss: 0.07349500060081482 = 0.00725959800183773 + 0.01 * 6.62354040145874
Epoch 700, val loss: 0.9800645709037781
Epoch 710, training loss: 0.07300824671983719 = 0.006975553929805756 + 0.01 * 6.603269577026367
Epoch 710, val loss: 0.9855802059173584
Epoch 720, training loss: 0.07268981635570526 = 0.006709670182317495 + 0.01 * 6.598014831542969
Epoch 720, val loss: 0.9908778667449951
Epoch 730, training loss: 0.07241049408912659 = 0.00646023266017437 + 0.01 * 6.595026016235352
Epoch 730, val loss: 0.9960814118385315
Epoch 740, training loss: 0.07211047410964966 = 0.006225839257240295 + 0.01 * 6.58846378326416
Epoch 740, val loss: 1.0011731386184692
Epoch 750, training loss: 0.07194674760103226 = 0.006005505565553904 + 0.01 * 6.594124794006348
Epoch 750, val loss: 1.0061651468276978
Epoch 760, training loss: 0.07198008894920349 = 0.005798030644655228 + 0.01 * 6.618205547332764
Epoch 760, val loss: 1.0110621452331543
Epoch 770, training loss: 0.07140620052814484 = 0.005602829623967409 + 0.01 * 6.580337047576904
Epoch 770, val loss: 1.0157830715179443
Epoch 780, training loss: 0.07143829017877579 = 0.00541871041059494 + 0.01 * 6.60195779800415
Epoch 780, val loss: 1.0204815864562988
Epoch 790, training loss: 0.07091625779867172 = 0.00524466997012496 + 0.01 * 6.5671586990356445
Epoch 790, val loss: 1.0250214338302612
Epoch 800, training loss: 0.07073330879211426 = 0.005080211441963911 + 0.01 * 6.565310001373291
Epoch 800, val loss: 1.0295048952102661
Epoch 810, training loss: 0.07048580795526505 = 0.004924390930682421 + 0.01 * 6.556141376495361
Epoch 810, val loss: 1.033874273300171
Epoch 820, training loss: 0.07035735994577408 = 0.004776582587510347 + 0.01 * 6.558077812194824
Epoch 820, val loss: 1.0381852388381958
Epoch 830, training loss: 0.07010649889707565 = 0.004636645317077637 + 0.01 * 6.546985626220703
Epoch 830, val loss: 1.0423178672790527
Epoch 840, training loss: 0.07003680616617203 = 0.0045037115924060345 + 0.01 * 6.553309440612793
Epoch 840, val loss: 1.0464560985565186
Epoch 850, training loss: 0.06978029012680054 = 0.00437759468331933 + 0.01 * 6.54026985168457
Epoch 850, val loss: 1.050436019897461
Epoch 860, training loss: 0.06967455893754959 = 0.00425749784335494 + 0.01 * 6.541706085205078
Epoch 860, val loss: 1.0543992519378662
Epoch 870, training loss: 0.06961330771446228 = 0.004143252968788147 + 0.01 * 6.547005653381348
Epoch 870, val loss: 1.0582420825958252
Epoch 880, training loss: 0.069257453083992 = 0.004034386947751045 + 0.01 * 6.5223069190979
Epoch 880, val loss: 1.0620180368423462
Epoch 890, training loss: 0.06937641650438309 = 0.003930586390197277 + 0.01 * 6.544583797454834
Epoch 890, val loss: 1.0657440423965454
Epoch 900, training loss: 0.06916900724172592 = 0.003831417066976428 + 0.01 * 6.533759593963623
Epoch 900, val loss: 1.0693659782409668
Epoch 910, training loss: 0.06904798001050949 = 0.0037367334589362144 + 0.01 * 6.531125068664551
Epoch 910, val loss: 1.0729302167892456
Epoch 920, training loss: 0.06886567920446396 = 0.003646315773949027 + 0.01 * 6.521936893463135
Epoch 920, val loss: 1.0764825344085693
Epoch 930, training loss: 0.06869916617870331 = 0.003559868084266782 + 0.01 * 6.513930320739746
Epoch 930, val loss: 1.0799586772918701
Epoch 940, training loss: 0.06859847158193588 = 0.0034771040081977844 + 0.01 * 6.512136936187744
Epoch 940, val loss: 1.0833642482757568
Epoch 950, training loss: 0.06843449175357819 = 0.003397793509066105 + 0.01 * 6.503669738769531
Epoch 950, val loss: 1.0866761207580566
Epoch 960, training loss: 0.06830071657896042 = 0.0033218825701624155 + 0.01 * 6.497882843017578
Epoch 960, val loss: 1.089979648590088
Epoch 970, training loss: 0.06823726743459702 = 0.0032490286976099014 + 0.01 * 6.498824119567871
Epoch 970, val loss: 1.0932121276855469
Epoch 980, training loss: 0.06817740947008133 = 0.0031792467925697565 + 0.01 * 6.499816417694092
Epoch 980, val loss: 1.0963577032089233
Epoch 990, training loss: 0.06799113750457764 = 0.0031122067011892796 + 0.01 * 6.487893104553223
Epoch 990, val loss: 1.0994733572006226
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.5941
Flip ASR: 0.5111/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.052403450012207 = 1.9686654806137085 + 0.01 * 8.37380313873291
Epoch 0, val loss: 1.9661587476730347
Epoch 10, training loss: 2.0404155254364014 = 1.9566785097122192 + 0.01 * 8.373712539672852
Epoch 10, val loss: 1.9535378217697144
Epoch 20, training loss: 2.0258779525756836 = 1.9421437978744507 + 0.01 * 8.373407363891602
Epoch 20, val loss: 1.9378912448883057
Epoch 30, training loss: 2.005746603012085 = 1.9220212697982788 + 0.01 * 8.372526168823242
Epoch 30, val loss: 1.9161227941513062
Epoch 40, training loss: 1.9763696193695068 = 1.8926892280578613 + 0.01 * 8.3680419921875
Epoch 40, val loss: 1.8847540616989136
Epoch 50, training loss: 1.9351660013198853 = 1.8517979383468628 + 0.01 * 8.336812019348145
Epoch 50, val loss: 1.8428752422332764
Epoch 60, training loss: 1.8857967853546143 = 1.8041203022003174 + 0.01 * 8.167644500732422
Epoch 60, val loss: 1.7984163761138916
Epoch 70, training loss: 1.838424563407898 = 1.7595950365066528 + 0.01 * 7.8829522132873535
Epoch 70, val loss: 1.7610505819320679
Epoch 80, training loss: 1.7836358547210693 = 1.7078938484191895 + 0.01 * 7.574202537536621
Epoch 80, val loss: 1.7164913415908813
Epoch 90, training loss: 1.7114771604537964 = 1.6375906467437744 + 0.01 * 7.388646602630615
Epoch 90, val loss: 1.6556183099746704
Epoch 100, training loss: 1.6182290315628052 = 1.5464422702789307 + 0.01 * 7.178677082061768
Epoch 100, val loss: 1.577993631362915
Epoch 110, training loss: 1.5182483196258545 = 1.4473073482513428 + 0.01 * 7.0940985679626465
Epoch 110, val loss: 1.4967052936553955
Epoch 120, training loss: 1.4254918098449707 = 1.3551214933395386 + 0.01 * 7.0370330810546875
Epoch 120, val loss: 1.427202582359314
Epoch 130, training loss: 1.3420689105987549 = 1.2722036838531494 + 0.01 * 6.986523628234863
Epoch 130, val loss: 1.3705521821975708
Epoch 140, training loss: 1.2647521495819092 = 1.1953327655792236 + 0.01 * 6.941941261291504
Epoch 140, val loss: 1.3202903270721436
Epoch 150, training loss: 1.1894816160202026 = 1.1203490495681763 + 0.01 * 6.9132561683654785
Epoch 150, val loss: 1.2711424827575684
Epoch 160, training loss: 1.1130456924438477 = 1.0440737009048462 + 0.01 * 6.897202491760254
Epoch 160, val loss: 1.2216482162475586
Epoch 170, training loss: 1.0345338582992554 = 0.9656556248664856 + 0.01 * 6.887821674346924
Epoch 170, val loss: 1.1701968908309937
Epoch 180, training loss: 0.9556795954704285 = 0.8868820667266846 + 0.01 * 6.879752159118652
Epoch 180, val loss: 1.1180589199066162
Epoch 190, training loss: 0.8803547620773315 = 0.811613917350769 + 0.01 * 6.874085426330566
Epoch 190, val loss: 1.0683166980743408
Epoch 200, training loss: 0.8126245737075806 = 0.743927538394928 + 0.01 * 6.869703769683838
Epoch 200, val loss: 1.02452552318573
Epoch 210, training loss: 0.7541038990020752 = 0.685443103313446 + 0.01 * 6.866078853607178
Epoch 210, val loss: 0.9883663654327393
Epoch 220, training loss: 0.7032830715179443 = 0.634661078453064 + 0.01 * 6.862202167510986
Epoch 220, val loss: 0.9589343070983887
Epoch 230, training loss: 0.6570404767990112 = 0.5884473323822021 + 0.01 * 6.859316349029541
Epoch 230, val loss: 0.9336782097816467
Epoch 240, training loss: 0.6124817132949829 = 0.543931245803833 + 0.01 * 6.855050086975098
Epoch 240, val loss: 0.9103561043739319
Epoch 250, training loss: 0.5679347515106201 = 0.4994467794895172 + 0.01 * 6.848796367645264
Epoch 250, val loss: 0.8879619240760803
Epoch 260, training loss: 0.5232205986976624 = 0.45479685068130493 + 0.01 * 6.842372417449951
Epoch 260, val loss: 0.8666365146636963
Epoch 270, training loss: 0.47923368215560913 = 0.41087767481803894 + 0.01 * 6.835599422454834
Epoch 270, val loss: 0.8474827408790588
Epoch 280, training loss: 0.4371407628059387 = 0.3688671886920929 + 0.01 * 6.827357769012451
Epoch 280, val loss: 0.8316181898117065
Epoch 290, training loss: 0.39791229367256165 = 0.3296656906604767 + 0.01 * 6.824660301208496
Epoch 290, val loss: 0.819347620010376
Epoch 300, training loss: 0.36189812421798706 = 0.2937476336956024 + 0.01 * 6.8150506019592285
Epoch 300, val loss: 0.8107462525367737
Epoch 310, training loss: 0.3292286694049835 = 0.2611273229122162 + 0.01 * 6.810133934020996
Epoch 310, val loss: 0.8057905435562134
Epoch 320, training loss: 0.2998061776161194 = 0.2317664474248886 + 0.01 * 6.80397367477417
Epoch 320, val loss: 0.8044089674949646
Epoch 330, training loss: 0.2736126780509949 = 0.2056245505809784 + 0.01 * 6.798811435699463
Epoch 330, val loss: 0.8066099286079407
Epoch 340, training loss: 0.25043243169784546 = 0.18250420689582825 + 0.01 * 6.792823314666748
Epoch 340, val loss: 0.8124629259109497
Epoch 350, training loss: 0.23014217615127563 = 0.1621858775615692 + 0.01 * 6.795629024505615
Epoch 350, val loss: 0.8214998245239258
Epoch 360, training loss: 0.21209335327148438 = 0.14423857629299164 + 0.01 * 6.785477161407471
Epoch 360, val loss: 0.833308756351471
Epoch 370, training loss: 0.19619929790496826 = 0.12836694717407227 + 0.01 * 6.783234119415283
Epoch 370, val loss: 0.8470704555511475
Epoch 380, training loss: 0.1824447512626648 = 0.11464570462703705 + 0.01 * 6.779904365539551
Epoch 380, val loss: 0.8624467253684998
Epoch 390, training loss: 0.17083239555358887 = 0.10298695415258408 + 0.01 * 6.784544944763184
Epoch 390, val loss: 0.879225492477417
Epoch 400, training loss: 0.1607983112335205 = 0.09303932636976242 + 0.01 * 6.775899410247803
Epoch 400, val loss: 0.8970069885253906
Epoch 410, training loss: 0.15218299627304077 = 0.08445848524570465 + 0.01 * 6.772451877593994
Epoch 410, val loss: 0.9153797626495361
Epoch 420, training loss: 0.14467215538024902 = 0.07700525969266891 + 0.01 * 6.766690254211426
Epoch 420, val loss: 0.9337716698646545
Epoch 430, training loss: 0.13820435106754303 = 0.07047726213932037 + 0.01 * 6.772708892822266
Epoch 430, val loss: 0.9522172808647156
Epoch 440, training loss: 0.13234052062034607 = 0.06473033875226974 + 0.01 * 6.761017799377441
Epoch 440, val loss: 0.9706113338470459
Epoch 450, training loss: 0.127205029129982 = 0.0596393346786499 + 0.01 * 6.756569862365723
Epoch 450, val loss: 0.9888837933540344
Epoch 460, training loss: 0.1226208508014679 = 0.055095989257097244 + 0.01 * 6.752486705780029
Epoch 460, val loss: 1.007031798362732
Epoch 470, training loss: 0.11853794753551483 = 0.0510249100625515 + 0.01 * 6.7513041496276855
Epoch 470, val loss: 1.0247256755828857
Epoch 480, training loss: 0.11483271420001984 = 0.0473497211933136 + 0.01 * 6.7482991218566895
Epoch 480, val loss: 1.0420821905136108
Epoch 490, training loss: 0.11151672899723053 = 0.04402904585003853 + 0.01 * 6.748767852783203
Epoch 490, val loss: 1.0591291189193726
Epoch 500, training loss: 0.10841622948646545 = 0.041021402925252914 + 0.01 * 6.739482879638672
Epoch 500, val loss: 1.0757973194122314
Epoch 510, training loss: 0.10576729476451874 = 0.03827003762125969 + 0.01 * 6.749725818634033
Epoch 510, val loss: 1.09200918674469
Epoch 520, training loss: 0.10309986025094986 = 0.03573931008577347 + 0.01 * 6.73605489730835
Epoch 520, val loss: 1.1079108715057373
Epoch 530, training loss: 0.10068225860595703 = 0.03339545801281929 + 0.01 * 6.728679656982422
Epoch 530, val loss: 1.1233556270599365
Epoch 540, training loss: 0.09860294312238693 = 0.03121468611061573 + 0.01 * 6.738826274871826
Epoch 540, val loss: 1.1385515928268433
Epoch 550, training loss: 0.09643437713384628 = 0.029163576662540436 + 0.01 * 6.727080345153809
Epoch 550, val loss: 1.153344750404358
Epoch 560, training loss: 0.09441433101892471 = 0.027222884818911552 + 0.01 * 6.719144821166992
Epoch 560, val loss: 1.1678261756896973
Epoch 570, training loss: 0.09272322058677673 = 0.025372985750436783 + 0.01 * 6.7350239753723145
Epoch 570, val loss: 1.18199622631073
Epoch 580, training loss: 0.09074847400188446 = 0.023606229573488235 + 0.01 * 6.714223861694336
Epoch 580, val loss: 1.1959412097930908
Epoch 590, training loss: 0.08908022940158844 = 0.021917108446359634 + 0.01 * 6.716312408447266
Epoch 590, val loss: 1.2096058130264282
Epoch 600, training loss: 0.08736675977706909 = 0.02030639350414276 + 0.01 * 6.7060370445251465
Epoch 600, val loss: 1.2231374979019165
Epoch 610, training loss: 0.08576717972755432 = 0.018781736493110657 + 0.01 * 6.698544502258301
Epoch 610, val loss: 1.2363606691360474
Epoch 620, training loss: 0.08435186743736267 = 0.01735505275428295 + 0.01 * 6.699681758880615
Epoch 620, val loss: 1.2494010925292969
Epoch 630, training loss: 0.0830930545926094 = 0.016031138598918915 + 0.01 * 6.7061920166015625
Epoch 630, val loss: 1.2622253894805908
Epoch 640, training loss: 0.08175545930862427 = 0.014817639254033566 + 0.01 * 6.693781852722168
Epoch 640, val loss: 1.2751020193099976
Epoch 650, training loss: 0.08058573305606842 = 0.013718980364501476 + 0.01 * 6.686676025390625
Epoch 650, val loss: 1.2877326011657715
Epoch 660, training loss: 0.07966478914022446 = 0.012733153998851776 + 0.01 * 6.6931633949279785
Epoch 660, val loss: 1.299950361251831
Epoch 670, training loss: 0.0785859078168869 = 0.011834505945444107 + 0.01 * 6.675140380859375
Epoch 670, val loss: 1.3117010593414307
Epoch 680, training loss: 0.07774218916893005 = 0.011030851863324642 + 0.01 * 6.671133995056152
Epoch 680, val loss: 1.3233518600463867
Epoch 690, training loss: 0.07694937288761139 = 0.010317650623619556 + 0.01 * 6.663172245025635
Epoch 690, val loss: 1.3344444036483765
Epoch 700, training loss: 0.0762849971652031 = 0.009683090262115002 + 0.01 * 6.660191059112549
Epoch 700, val loss: 1.3451719284057617
Epoch 710, training loss: 0.07581072300672531 = 0.00910992082208395 + 0.01 * 6.670080184936523
Epoch 710, val loss: 1.3554396629333496
Epoch 720, training loss: 0.07520786672830582 = 0.008593169040977955 + 0.01 * 6.661469459533691
Epoch 720, val loss: 1.3655201196670532
Epoch 730, training loss: 0.07459211349487305 = 0.008123250678181648 + 0.01 * 6.646886348724365
Epoch 730, val loss: 1.3752487897872925
Epoch 740, training loss: 0.07440374791622162 = 0.007694902829825878 + 0.01 * 6.67088508605957
Epoch 740, val loss: 1.3845824003219604
Epoch 750, training loss: 0.07370080798864365 = 0.00730694318190217 + 0.01 * 6.6393866539001465
Epoch 750, val loss: 1.3936468362808228
Epoch 760, training loss: 0.07327071577310562 = 0.006956036668270826 + 0.01 * 6.631468296051025
Epoch 760, val loss: 1.4023470878601074
Epoch 770, training loss: 0.07316973060369492 = 0.006634451448917389 + 0.01 * 6.653527736663818
Epoch 770, val loss: 1.410617470741272
Epoch 780, training loss: 0.07263283431529999 = 0.006339691113680601 + 0.01 * 6.629314422607422
Epoch 780, val loss: 1.4186846017837524
Epoch 790, training loss: 0.0723147839307785 = 0.0060684820637106895 + 0.01 * 6.624629974365234
Epoch 790, val loss: 1.4265729188919067
Epoch 800, training loss: 0.07196075469255447 = 0.005818859674036503 + 0.01 * 6.614189624786377
Epoch 800, val loss: 1.4341057538986206
Epoch 810, training loss: 0.07174582034349442 = 0.005588251631706953 + 0.01 * 6.615757465362549
Epoch 810, val loss: 1.4415466785430908
Epoch 820, training loss: 0.07148279994726181 = 0.005374348256736994 + 0.01 * 6.61084508895874
Epoch 820, val loss: 1.4485894441604614
Epoch 830, training loss: 0.07125326991081238 = 0.005175229161977768 + 0.01 * 6.607804775238037
Epoch 830, val loss: 1.4555305242538452
Epoch 840, training loss: 0.07103564590215683 = 0.004988935310393572 + 0.01 * 6.604671001434326
Epoch 840, val loss: 1.4623533487319946
Epoch 850, training loss: 0.07084492594003677 = 0.0048135737888514996 + 0.01 * 6.603135108947754
Epoch 850, val loss: 1.4688383340835571
Epoch 860, training loss: 0.0706171840429306 = 0.0046493434347212315 + 0.01 * 6.596784591674805
Epoch 860, val loss: 1.4751477241516113
Epoch 870, training loss: 0.07049431651830673 = 0.004495581611990929 + 0.01 * 6.5998735427856445
Epoch 870, val loss: 1.481432557106018
Epoch 880, training loss: 0.070083849132061 = 0.0043509965762495995 + 0.01 * 6.5732855796813965
Epoch 880, val loss: 1.4874279499053955
Epoch 890, training loss: 0.06995051354169846 = 0.004215003456920385 + 0.01 * 6.573551654815674
Epoch 890, val loss: 1.4932667016983032
Epoch 900, training loss: 0.06988092511892319 = 0.004086926579475403 + 0.01 * 6.579400062561035
Epoch 900, val loss: 1.4989838600158691
Epoch 910, training loss: 0.0696958601474762 = 0.003965806681662798 + 0.01 * 6.573005199432373
Epoch 910, val loss: 1.504557490348816
Epoch 920, training loss: 0.06961755454540253 = 0.0038513592444360256 + 0.01 * 6.576619625091553
Epoch 920, val loss: 1.509779453277588
Epoch 930, training loss: 0.06939335912466049 = 0.0037432785611599684 + 0.01 * 6.565008163452148
Epoch 930, val loss: 1.5151740312576294
Epoch 940, training loss: 0.06924164295196533 = 0.0036404640413820744 + 0.01 * 6.560117721557617
Epoch 940, val loss: 1.520363211631775
Epoch 950, training loss: 0.06893419474363327 = 0.0035429704003036022 + 0.01 * 6.539123058319092
Epoch 950, val loss: 1.5253316164016724
Epoch 960, training loss: 0.06886668503284454 = 0.0034503561910241842 + 0.01 * 6.541632652282715
Epoch 960, val loss: 1.530191421508789
Epoch 970, training loss: 0.06911303102970123 = 0.0033622761256992817 + 0.01 * 6.575075626373291
Epoch 970, val loss: 1.534993290901184
Epoch 980, training loss: 0.06874643266201019 = 0.00327826919965446 + 0.01 * 6.546816349029541
Epoch 980, val loss: 1.5396403074264526
Epoch 990, training loss: 0.0685887262225151 = 0.0031983773224055767 + 0.01 * 6.539035320281982
Epoch 990, val loss: 1.5442103147506714
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.2657
Flip ASR: 0.2089/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0396382808685303 = 1.955899953842163 + 0.01 * 8.373822212219238
Epoch 0, val loss: 1.9545000791549683
Epoch 10, training loss: 2.0288283824920654 = 1.9450911283493042 + 0.01 * 8.373723030090332
Epoch 10, val loss: 1.943892002105713
Epoch 20, training loss: 2.015935182571411 = 1.9322011470794678 + 0.01 * 8.373401641845703
Epoch 20, val loss: 1.930399775505066
Epoch 30, training loss: 1.9985030889511108 = 1.9147775173187256 + 0.01 * 8.372560501098633
Epoch 30, val loss: 1.911320447921753
Epoch 40, training loss: 1.973511815071106 = 1.8898205757141113 + 0.01 * 8.369122505187988
Epoch 40, val loss: 1.883604884147644
Epoch 50, training loss: 1.9379278421401978 = 1.8544936180114746 + 0.01 * 8.343424797058105
Epoch 50, val loss: 1.8452270030975342
Epoch 60, training loss: 1.890900731086731 = 1.8094531297683716 + 0.01 * 8.144756317138672
Epoch 60, val loss: 1.7992827892303467
Epoch 70, training loss: 1.839552879333496 = 1.7626702785491943 + 0.01 * 7.688265800476074
Epoch 70, val loss: 1.7556874752044678
Epoch 80, training loss: 1.789116382598877 = 1.715782642364502 + 0.01 * 7.333375930786133
Epoch 80, val loss: 1.7150053977966309
Epoch 90, training loss: 1.7268109321594238 = 1.6547744274139404 + 0.01 * 7.203649520874023
Epoch 90, val loss: 1.6649253368377686
Epoch 100, training loss: 1.6443183422088623 = 1.5727980136871338 + 0.01 * 7.152037620544434
Epoch 100, val loss: 1.5999869108200073
Epoch 110, training loss: 1.5391836166381836 = 1.4682152271270752 + 0.01 * 7.096844673156738
Epoch 110, val loss: 1.5173896551132202
Epoch 120, training loss: 1.4187397956848145 = 1.3482229709625244 + 0.01 * 7.051685333251953
Epoch 120, val loss: 1.4237966537475586
Epoch 130, training loss: 1.2937653064727783 = 1.2235538959503174 + 0.01 * 7.021136283874512
Epoch 130, val loss: 1.327779769897461
Epoch 140, training loss: 1.1715677976608276 = 1.101613998413086 + 0.01 * 6.995385646820068
Epoch 140, val loss: 1.2356914281845093
Epoch 150, training loss: 1.0573921203613281 = 0.9876781702041626 + 0.01 * 6.971398830413818
Epoch 150, val loss: 1.1517266035079956
Epoch 160, training loss: 0.954241156578064 = 0.8847429156303406 + 0.01 * 6.949823379516602
Epoch 160, val loss: 1.0779231786727905
Epoch 170, training loss: 0.8631010055541992 = 0.7939003705978394 + 0.01 * 6.9200663566589355
Epoch 170, val loss: 1.0149306058883667
Epoch 180, training loss: 0.7839462161064148 = 0.7148908376693726 + 0.01 * 6.905537128448486
Epoch 180, val loss: 0.961911678314209
Epoch 190, training loss: 0.7153793573379517 = 0.646538257598877 + 0.01 * 6.884108543395996
Epoch 190, val loss: 0.9180389046669006
Epoch 200, training loss: 0.6556841135025024 = 0.5869596004486084 + 0.01 * 6.872448444366455
Epoch 200, val loss: 0.883586049079895
Epoch 210, training loss: 0.6026767492294312 = 0.5340368747711182 + 0.01 * 6.863986492156982
Epoch 210, val loss: 0.8585477471351624
Epoch 220, training loss: 0.554252564907074 = 0.4857308566570282 + 0.01 * 6.852172374725342
Epoch 220, val loss: 0.8402547836303711
Epoch 230, training loss: 0.5091924071311951 = 0.4407270848751068 + 0.01 * 6.846533298492432
Epoch 230, val loss: 0.8260425329208374
Epoch 240, training loss: 0.4666528105735779 = 0.3982318341732025 + 0.01 * 6.842097282409668
Epoch 240, val loss: 0.8145179152488708
Epoch 250, training loss: 0.42627817392349243 = 0.357913613319397 + 0.01 * 6.836455345153809
Epoch 250, val loss: 0.804747462272644
Epoch 260, training loss: 0.3881314992904663 = 0.3198074996471405 + 0.01 * 6.832401275634766
Epoch 260, val loss: 0.7962771654129028
Epoch 270, training loss: 0.3525327444076538 = 0.2842092216014862 + 0.01 * 6.832352638244629
Epoch 270, val loss: 0.7891301512718201
Epoch 280, training loss: 0.31984156370162964 = 0.2515515983104706 + 0.01 * 6.82899808883667
Epoch 280, val loss: 0.7833160758018494
Epoch 290, training loss: 0.2905459702014923 = 0.22230613231658936 + 0.01 * 6.823983669281006
Epoch 290, val loss: 0.7795079946517944
Epoch 300, training loss: 0.2648313641548157 = 0.19662553071975708 + 0.01 * 6.820584297180176
Epoch 300, val loss: 0.7777389883995056
Epoch 310, training loss: 0.24244950711727142 = 0.1742776334285736 + 0.01 * 6.817187309265137
Epoch 310, val loss: 0.7777422666549683
Epoch 320, training loss: 0.22306181490421295 = 0.1549091339111328 + 0.01 * 6.815268039703369
Epoch 320, val loss: 0.779521107673645
Epoch 330, training loss: 0.20622551441192627 = 0.13806352019309998 + 0.01 * 6.816198825836182
Epoch 330, val loss: 0.7825196385383606
Epoch 340, training loss: 0.1914457082748413 = 0.12333862483501434 + 0.01 * 6.810708045959473
Epoch 340, val loss: 0.7862401604652405
Epoch 350, training loss: 0.17841395735740662 = 0.11032252013683319 + 0.01 * 6.809144973754883
Epoch 350, val loss: 0.7906683683395386
Epoch 360, training loss: 0.1666279286146164 = 0.09856680780649185 + 0.01 * 6.806112289428711
Epoch 360, val loss: 0.7952078580856323
Epoch 370, training loss: 0.15588000416755676 = 0.08781597763299942 + 0.01 * 6.806403636932373
Epoch 370, val loss: 0.7997464537620544
Epoch 380, training loss: 0.14584881067276 = 0.0777738094329834 + 0.01 * 6.807501316070557
Epoch 380, val loss: 0.8044722676277161
Epoch 390, training loss: 0.13638216257095337 = 0.06837530434131622 + 0.01 * 6.800685405731201
Epoch 390, val loss: 0.8093525767326355
Epoch 400, training loss: 0.12797264754772186 = 0.05999750271439552 + 0.01 * 6.79751443862915
Epoch 400, val loss: 0.8153579831123352
Epoch 410, training loss: 0.12085571140050888 = 0.05286388844251633 + 0.01 * 6.799182415008545
Epoch 410, val loss: 0.8228752017021179
Epoch 420, training loss: 0.11480201780796051 = 0.04682756960391998 + 0.01 * 6.797445297241211
Epoch 420, val loss: 0.8321232199668884
Epoch 430, training loss: 0.10964164137840271 = 0.0417255237698555 + 0.01 * 6.791612148284912
Epoch 430, val loss: 0.8424926400184631
Epoch 440, training loss: 0.10526023060083389 = 0.0373942106962204 + 0.01 * 6.786602020263672
Epoch 440, val loss: 0.8534544706344604
Epoch 450, training loss: 0.10158204287290573 = 0.033676378428936005 + 0.01 * 6.790566444396973
Epoch 450, val loss: 0.8646110892295837
Epoch 460, training loss: 0.09832090884447098 = 0.030476108193397522 + 0.01 * 6.784480571746826
Epoch 460, val loss: 0.8757054805755615
Epoch 470, training loss: 0.09547924995422363 = 0.027706138789653778 + 0.01 * 6.777311325073242
Epoch 470, val loss: 0.8867741227149963
Epoch 480, training loss: 0.09308221936225891 = 0.02529563568532467 + 0.01 * 6.778658390045166
Epoch 480, val loss: 0.8975203633308411
Epoch 490, training loss: 0.09088395535945892 = 0.02318718098104 + 0.01 * 6.76967716217041
Epoch 490, val loss: 0.9080541133880615
Epoch 500, training loss: 0.08902984857559204 = 0.021332042291760445 + 0.01 * 6.76978063583374
Epoch 500, val loss: 0.9183475375175476
Epoch 510, training loss: 0.08732748031616211 = 0.01969357207417488 + 0.01 * 6.763391017913818
Epoch 510, val loss: 0.9283189177513123
Epoch 520, training loss: 0.08587043732404709 = 0.018240081146359444 + 0.01 * 6.763036251068115
Epoch 520, val loss: 0.9381143450737
Epoch 530, training loss: 0.084508515894413 = 0.016944371163845062 + 0.01 * 6.756414413452148
Epoch 530, val loss: 0.9475924372673035
Epoch 540, training loss: 0.08327525109052658 = 0.015782346948981285 + 0.01 * 6.749290466308594
Epoch 540, val loss: 0.9568096995353699
Epoch 550, training loss: 0.082350954413414 = 0.014736544340848923 + 0.01 * 6.761440753936768
Epoch 550, val loss: 0.9657928347587585
Epoch 560, training loss: 0.08128796517848969 = 0.013794990256428719 + 0.01 * 6.749297618865967
Epoch 560, val loss: 0.9745829105377197
Epoch 570, training loss: 0.08052101731300354 = 0.012943439185619354 + 0.01 * 6.757757663726807
Epoch 570, val loss: 0.983083188533783
Epoch 580, training loss: 0.07950456440448761 = 0.01217180211097002 + 0.01 * 6.7332763671875
Epoch 580, val loss: 0.9913463592529297
Epoch 590, training loss: 0.07894211262464523 = 0.011469850316643715 + 0.01 * 6.747226715087891
Epoch 590, val loss: 0.9993858337402344
Epoch 600, training loss: 0.07816659659147263 = 0.010830072686076164 + 0.01 * 6.733652114868164
Epoch 600, val loss: 1.007209300994873
Epoch 610, training loss: 0.07743506878614426 = 0.010245085693895817 + 0.01 * 6.718998432159424
Epoch 610, val loss: 1.0148295164108276
Epoch 620, training loss: 0.07685933262109756 = 0.009707905352115631 + 0.01 * 6.715142726898193
Epoch 620, val loss: 1.0222456455230713
Epoch 630, training loss: 0.07639946788549423 = 0.009214242920279503 + 0.01 * 6.718522548675537
Epoch 630, val loss: 1.029481053352356
Epoch 640, training loss: 0.0759940892457962 = 0.008759919553995132 + 0.01 * 6.723416805267334
Epoch 640, val loss: 1.0365763902664185
Epoch 650, training loss: 0.07540147751569748 = 0.008341039530932903 + 0.01 * 6.7060441970825195
Epoch 650, val loss: 1.0434534549713135
Epoch 660, training loss: 0.074961356818676 = 0.007953986525535583 + 0.01 * 6.700737476348877
Epoch 660, val loss: 1.0501195192337036
Epoch 670, training loss: 0.07455478608608246 = 0.007596113719046116 + 0.01 * 6.695867538452148
Epoch 670, val loss: 1.0566741228103638
Epoch 680, training loss: 0.07430137693881989 = 0.007264108397066593 + 0.01 * 6.7037272453308105
Epoch 680, val loss: 1.0630416870117188
Epoch 690, training loss: 0.07370921969413757 = 0.006955801974982023 + 0.01 * 6.675342082977295
Epoch 690, val loss: 1.069172978401184
Epoch 700, training loss: 0.0734957605600357 = 0.006668983958661556 + 0.01 * 6.682677745819092
Epoch 700, val loss: 1.0752432346343994
Epoch 710, training loss: 0.07299911230802536 = 0.006401723716408014 + 0.01 * 6.659738540649414
Epoch 710, val loss: 1.0810626745224
Epoch 720, training loss: 0.07280312478542328 = 0.006151988636702299 + 0.01 * 6.665113925933838
Epoch 720, val loss: 1.086842656135559
Epoch 730, training loss: 0.07237476855516434 = 0.005918582901358604 + 0.01 * 6.645618438720703
Epoch 730, val loss: 1.0924139022827148
Epoch 740, training loss: 0.07289987802505493 = 0.005699983332306147 + 0.01 * 6.719989776611328
Epoch 740, val loss: 1.0978693962097168
Epoch 750, training loss: 0.07204082608222961 = 0.005495534744113684 + 0.01 * 6.654529094696045
Epoch 750, val loss: 1.1031577587127686
Epoch 760, training loss: 0.07169853150844574 = 0.005303418729454279 + 0.01 * 6.639511585235596
Epoch 760, val loss: 1.108418345451355
Epoch 770, training loss: 0.07134393602609634 = 0.005122642032802105 + 0.01 * 6.622129917144775
Epoch 770, val loss: 1.1134742498397827
Epoch 780, training loss: 0.07128160446882248 = 0.004952526185661554 + 0.01 * 6.632907867431641
Epoch 780, val loss: 1.118455171585083
Epoch 790, training loss: 0.07083387672901154 = 0.004792081192135811 + 0.01 * 6.604179859161377
Epoch 790, val loss: 1.123313546180725
Epoch 800, training loss: 0.07088736444711685 = 0.0046407300978899 + 0.01 * 6.624663829803467
Epoch 800, val loss: 1.1280806064605713
Epoch 810, training loss: 0.07048188894987106 = 0.004497732035815716 + 0.01 * 6.598416328430176
Epoch 810, val loss: 1.1327118873596191
Epoch 820, training loss: 0.0703979954123497 = 0.004362522158771753 + 0.01 * 6.603547096252441
Epoch 820, val loss: 1.1371756792068481
Epoch 830, training loss: 0.0701783075928688 = 0.004234542604535818 + 0.01 * 6.594376087188721
Epoch 830, val loss: 1.141741156578064
Epoch 840, training loss: 0.06988535076379776 = 0.0041134594939649105 + 0.01 * 6.5771894454956055
Epoch 840, val loss: 1.1460152864456177
Epoch 850, training loss: 0.0700078159570694 = 0.00399843929335475 + 0.01 * 6.600937843322754
Epoch 850, val loss: 1.1502439975738525
Epoch 860, training loss: 0.06980694085359573 = 0.0038894719909876585 + 0.01 * 6.591746807098389
Epoch 860, val loss: 1.1544685363769531
Epoch 870, training loss: 0.06943818181753159 = 0.0037855699192732573 + 0.01 * 6.565261363983154
Epoch 870, val loss: 1.1585078239440918
Epoch 880, training loss: 0.06948971748352051 = 0.003686900483444333 + 0.01 * 6.580282211303711
Epoch 880, val loss: 1.1624752283096313
Epoch 890, training loss: 0.06914585083723068 = 0.0035931027960032225 + 0.01 * 6.555274963378906
Epoch 890, val loss: 1.1663237810134888
Epoch 900, training loss: 0.06900101900100708 = 0.0035035978071391582 + 0.01 * 6.549742221832275
Epoch 900, val loss: 1.1701139211654663
Epoch 910, training loss: 0.0689544752240181 = 0.003418429521843791 + 0.01 * 6.553604602813721
Epoch 910, val loss: 1.1738165616989136
Epoch 920, training loss: 0.06871906667947769 = 0.0033369397278875113 + 0.01 * 6.53821325302124
Epoch 920, val loss: 1.177531123161316
Epoch 930, training loss: 0.06841704994440079 = 0.0032591752242296934 + 0.01 * 6.515787601470947
Epoch 930, val loss: 1.1811316013336182
Epoch 940, training loss: 0.0684700757265091 = 0.003184477798640728 + 0.01 * 6.528560161590576
Epoch 940, val loss: 1.1846287250518799
Epoch 950, training loss: 0.06851988285779953 = 0.0031133093871176243 + 0.01 * 6.5406575202941895
Epoch 950, val loss: 1.188006043434143
Epoch 960, training loss: 0.06840673834085464 = 0.003045253921300173 + 0.01 * 6.536148548126221
Epoch 960, val loss: 1.191476821899414
Epoch 970, training loss: 0.06798658519983292 = 0.002979954006150365 + 0.01 * 6.5006632804870605
Epoch 970, val loss: 1.1947976350784302
Epoch 980, training loss: 0.06792216747999191 = 0.002917190082371235 + 0.01 * 6.500497341156006
Epoch 980, val loss: 1.1980254650115967
Epoch 990, training loss: 0.06787671148777008 = 0.0028571118600666523 + 0.01 * 6.501960277557373
Epoch 990, val loss: 1.2011749744415283
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.8487
Flip ASR: 0.8222/225 nodes
The final ASR:0.56950, 0.23865, Accuracy:0.81481, 0.00302
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11644])
remove edge: torch.Size([2, 9562])
updated graph: torch.Size([2, 10650])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98155, 0.00522, Accuracy:0.82963, 0.00524
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0124869346618652 = 1.9287484884262085 + 0.01 * 8.373851776123047
Epoch 0, val loss: 1.9285651445388794
Epoch 10, training loss: 2.0023889541625977 = 1.9186508655548096 + 0.01 * 8.373799324035645
Epoch 10, val loss: 1.9185621738433838
Epoch 20, training loss: 1.9901371002197266 = 1.9064013957977295 + 0.01 * 8.37357234954834
Epoch 20, val loss: 1.9064563512802124
Epoch 30, training loss: 1.9732098579406738 = 1.8894816637039185 + 0.01 * 8.372823715209961
Epoch 30, val loss: 1.889928936958313
Epoch 40, training loss: 1.9487049579620361 = 1.8650182485580444 + 0.01 * 8.368670463562012
Epoch 40, val loss: 1.8666021823883057
Epoch 50, training loss: 1.915258765220642 = 1.8318730592727661 + 0.01 * 8.338568687438965
Epoch 50, val loss: 1.8368504047393799
Epoch 60, training loss: 1.8764197826385498 = 1.7949769496917725 + 0.01 * 8.144289016723633
Epoch 60, val loss: 1.8073192834854126
Epoch 70, training loss: 1.8371620178222656 = 1.7579801082611084 + 0.01 * 7.918191909790039
Epoch 70, val loss: 1.7787508964538574
Epoch 80, training loss: 1.7842117547988892 = 1.7070751190185547 + 0.01 * 7.71366548538208
Epoch 80, val loss: 1.7358410358428955
Epoch 90, training loss: 1.710057020187378 = 1.6352076530456543 + 0.01 * 7.484941005706787
Epoch 90, val loss: 1.6757044792175293
Epoch 100, training loss: 1.6142679452896118 = 1.5408000946044922 + 0.01 * 7.34678840637207
Epoch 100, val loss: 1.600114107131958
Epoch 110, training loss: 1.5052592754364014 = 1.4326848983764648 + 0.01 * 7.257436752319336
Epoch 110, val loss: 1.5150591135025024
Epoch 120, training loss: 1.3934153318405151 = 1.3218646049499512 + 0.01 * 7.1550679206848145
Epoch 120, val loss: 1.4289647340774536
Epoch 130, training loss: 1.2863222360610962 = 1.2155869007110596 + 0.01 * 7.073530673980713
Epoch 130, val loss: 1.347227692604065
Epoch 140, training loss: 1.188807725906372 = 1.1185598373413086 + 0.01 * 7.024785995483398
Epoch 140, val loss: 1.2735165357589722
Epoch 150, training loss: 1.1052029132843018 = 1.0353211164474487 + 0.01 * 6.988181114196777
Epoch 150, val loss: 1.2120580673217773
Epoch 160, training loss: 1.0360658168792725 = 0.9664943814277649 + 0.01 * 6.9571404457092285
Epoch 160, val loss: 1.1638790369033813
Epoch 170, training loss: 0.9770398139953613 = 0.9076699018478394 + 0.01 * 6.936992168426514
Epoch 170, val loss: 1.125186800956726
Epoch 180, training loss: 0.9217321872711182 = 0.8524883985519409 + 0.01 * 6.924380302429199
Epoch 180, val loss: 1.0899715423583984
Epoch 190, training loss: 0.8653501868247986 = 0.7961947917938232 + 0.01 * 6.9155402183532715
Epoch 190, val loss: 1.053396463394165
Epoch 200, training loss: 0.8060705065727234 = 0.7369739413261414 + 0.01 * 6.90965461730957
Epoch 200, val loss: 1.0132920742034912
Epoch 210, training loss: 0.7447068691253662 = 0.6756601929664612 + 0.01 * 6.904669284820557
Epoch 210, val loss: 0.9711415767669678
Epoch 220, training loss: 0.6831949949264526 = 0.6141954064369202 + 0.01 * 6.899956226348877
Epoch 220, val loss: 0.9294255375862122
Epoch 230, training loss: 0.623476505279541 = 0.5545244812965393 + 0.01 * 6.895203590393066
Epoch 230, val loss: 0.890807569026947
Epoch 240, training loss: 0.567150354385376 = 0.49824005365371704 + 0.01 * 6.891028881072998
Epoch 240, val loss: 0.856972873210907
Epoch 250, training loss: 0.5150498151779175 = 0.4461927115917206 + 0.01 * 6.8857102394104
Epoch 250, val loss: 0.8283900618553162
Epoch 260, training loss: 0.46706143021583557 = 0.3981482684612274 + 0.01 * 6.891315937042236
Epoch 260, val loss: 0.8046849370002747
Epoch 270, training loss: 0.42222926020622253 = 0.3534615933895111 + 0.01 * 6.876766204833984
Epoch 270, val loss: 0.7849628925323486
Epoch 280, training loss: 0.3800666928291321 = 0.31136658787727356 + 0.01 * 6.870009422302246
Epoch 280, val loss: 0.7686810493469238
Epoch 290, training loss: 0.3402160704135895 = 0.2715682089328766 + 0.01 * 6.864786148071289
Epoch 290, val loss: 0.7557801008224487
Epoch 300, training loss: 0.3030135929584503 = 0.23440569639205933 + 0.01 * 6.8607892990112305
Epoch 300, val loss: 0.7463243007659912
Epoch 310, training loss: 0.2692699432373047 = 0.20064403116703033 + 0.01 * 6.86259126663208
Epoch 310, val loss: 0.7402029633522034
Epoch 320, training loss: 0.23955050110816956 = 0.170998677611351 + 0.01 * 6.855182647705078
Epoch 320, val loss: 0.7372082471847534
Epoch 330, training loss: 0.21418654918670654 = 0.14571279287338257 + 0.01 * 6.847375869750977
Epoch 330, val loss: 0.7373607754707336
Epoch 340, training loss: 0.19301970303058624 = 0.12455093115568161 + 0.01 * 6.846877574920654
Epoch 340, val loss: 0.7404601573944092
Epoch 350, training loss: 0.17543953657150269 = 0.10700030624866486 + 0.01 * 6.843923568725586
Epoch 350, val loss: 0.7462379336357117
Epoch 360, training loss: 0.1608409285545349 = 0.09247291833162308 + 0.01 * 6.8368000984191895
Epoch 360, val loss: 0.7541055679321289
Epoch 370, training loss: 0.14869758486747742 = 0.08040113002061844 + 0.01 * 6.829645156860352
Epoch 370, val loss: 0.7635806202888489
Epoch 380, training loss: 0.1385907530784607 = 0.07031828165054321 + 0.01 * 6.827247619628906
Epoch 380, val loss: 0.7740247249603271
Epoch 390, training loss: 0.13011077046394348 = 0.06185702607035637 + 0.01 * 6.825374126434326
Epoch 390, val loss: 0.785239040851593
Epoch 400, training loss: 0.12281733006238937 = 0.05470699816942215 + 0.01 * 6.811033248901367
Epoch 400, val loss: 0.796893835067749
Epoch 410, training loss: 0.11668771505355835 = 0.0486270971596241 + 0.01 * 6.8060622215271
Epoch 410, val loss: 0.8088609576225281
Epoch 420, training loss: 0.1114305779337883 = 0.043429359793663025 + 0.01 * 6.800121784210205
Epoch 420, val loss: 0.8209289908409119
Epoch 430, training loss: 0.10684572160243988 = 0.03895914927124977 + 0.01 * 6.7886576652526855
Epoch 430, val loss: 0.8330336809158325
Epoch 440, training loss: 0.1032833606004715 = 0.03509608283638954 + 0.01 * 6.818728446960449
Epoch 440, val loss: 0.8450365662574768
Epoch 450, training loss: 0.09959767013788223 = 0.03175165504217148 + 0.01 * 6.78460168838501
Epoch 450, val loss: 0.8568500876426697
Epoch 460, training loss: 0.0966217964887619 = 0.028839996084570885 + 0.01 * 6.778180122375488
Epoch 460, val loss: 0.8684784770011902
Epoch 470, training loss: 0.09388467669487 = 0.026294736191630363 + 0.01 * 6.758994102478027
Epoch 470, val loss: 0.8799417018890381
Epoch 480, training loss: 0.09156635403633118 = 0.024060765281319618 + 0.01 * 6.750558853149414
Epoch 480, val loss: 0.891110897064209
Epoch 490, training loss: 0.08950859308242798 = 0.022092141211032867 + 0.01 * 6.741645336151123
Epoch 490, val loss: 0.9020405411720276
Epoch 500, training loss: 0.0876513421535492 = 0.0203542597591877 + 0.01 * 6.729707717895508
Epoch 500, val loss: 0.9126226305961609
Epoch 510, training loss: 0.08620169758796692 = 0.01881551742553711 + 0.01 * 6.73861837387085
Epoch 510, val loss: 0.9228903651237488
Epoch 520, training loss: 0.08462972193956375 = 0.0174460019916296 + 0.01 * 6.718371868133545
Epoch 520, val loss: 0.9329468607902527
Epoch 530, training loss: 0.08358869701623917 = 0.016221925616264343 + 0.01 * 6.736677169799805
Epoch 530, val loss: 0.9427133798599243
Epoch 540, training loss: 0.08220039308071136 = 0.015126320533454418 + 0.01 * 6.7074079513549805
Epoch 540, val loss: 0.952191174030304
Epoch 550, training loss: 0.08114565908908844 = 0.014141511172056198 + 0.01 * 6.700415134429932
Epoch 550, val loss: 0.9613652229309082
Epoch 560, training loss: 0.08029808849096298 = 0.013254060409963131 + 0.01 * 6.704402923583984
Epoch 560, val loss: 0.9703061580657959
Epoch 570, training loss: 0.07930818200111389 = 0.01245217677205801 + 0.01 * 6.685600757598877
Epoch 570, val loss: 0.978996217250824
Epoch 580, training loss: 0.07853398472070694 = 0.011724449694156647 + 0.01 * 6.680953502655029
Epoch 580, val loss: 0.9874367117881775
Epoch 590, training loss: 0.07783843576908112 = 0.011062084697186947 + 0.01 * 6.677635192871094
Epoch 590, val loss: 0.9956633448600769
Epoch 600, training loss: 0.0772370845079422 = 0.010457556694746017 + 0.01 * 6.677952289581299
Epoch 600, val loss: 1.0036777257919312
Epoch 610, training loss: 0.07655619829893112 = 0.009904353879392147 + 0.01 * 6.665184497833252
Epoch 610, val loss: 1.0114612579345703
Epoch 620, training loss: 0.07588323950767517 = 0.00939758401364088 + 0.01 * 6.648566246032715
Epoch 620, val loss: 1.0190240144729614
Epoch 630, training loss: 0.07539451122283936 = 0.008932048454880714 + 0.01 * 6.646246433258057
Epoch 630, val loss: 1.0264053344726562
Epoch 640, training loss: 0.07509730756282806 = 0.008502807468175888 + 0.01 * 6.659450531005859
Epoch 640, val loss: 1.0335863828659058
Epoch 650, training loss: 0.07448339462280273 = 0.008106186985969543 + 0.01 * 6.637721061706543
Epoch 650, val loss: 1.0406653881072998
Epoch 660, training loss: 0.07397197186946869 = 0.007738993037492037 + 0.01 * 6.623298168182373
Epoch 660, val loss: 1.0475475788116455
Epoch 670, training loss: 0.07403600215911865 = 0.007398336194455624 + 0.01 * 6.663766384124756
Epoch 670, val loss: 1.0542669296264648
Epoch 680, training loss: 0.07323327660560608 = 0.007082293275743723 + 0.01 * 6.61509895324707
Epoch 680, val loss: 1.060823917388916
Epoch 690, training loss: 0.07291872799396515 = 0.006788091734051704 + 0.01 * 6.613063812255859
Epoch 690, val loss: 1.0672554969787598
Epoch 700, training loss: 0.07271708548069 = 0.006513730622828007 + 0.01 * 6.620336055755615
Epoch 700, val loss: 1.0735139846801758
Epoch 710, training loss: 0.0724308118224144 = 0.0062579503282904625 + 0.01 * 6.617286205291748
Epoch 710, val loss: 1.079629898071289
Epoch 720, training loss: 0.07205291092395782 = 0.006018626503646374 + 0.01 * 6.603428840637207
Epoch 720, val loss: 1.0856101512908936
Epoch 730, training loss: 0.07184690237045288 = 0.00579422153532505 + 0.01 * 6.605268478393555
Epoch 730, val loss: 1.0914417505264282
Epoch 740, training loss: 0.07146801054477692 = 0.005583662074059248 + 0.01 * 6.588435649871826
Epoch 740, val loss: 1.097164273262024
Epoch 750, training loss: 0.07126447558403015 = 0.005385724827647209 + 0.01 * 6.587874889373779
Epoch 750, val loss: 1.1027870178222656
Epoch 760, training loss: 0.07100099325180054 = 0.005199561361223459 + 0.01 * 6.580143451690674
Epoch 760, val loss: 1.1082843542099
Epoch 770, training loss: 0.07088214159011841 = 0.005024392157793045 + 0.01 * 6.5857744216918945
Epoch 770, val loss: 1.113682508468628
Epoch 780, training loss: 0.07073456048965454 = 0.004859334323555231 + 0.01 * 6.587522506713867
Epoch 780, val loss: 1.118926763534546
Epoch 790, training loss: 0.07054655998945236 = 0.004703692626208067 + 0.01 * 6.584287166595459
Epoch 790, val loss: 1.1240510940551758
Epoch 800, training loss: 0.07032647728919983 = 0.004556391853839159 + 0.01 * 6.577008247375488
Epoch 800, val loss: 1.1291229724884033
Epoch 810, training loss: 0.07007633149623871 = 0.004416821524500847 + 0.01 * 6.565950870513916
Epoch 810, val loss: 1.1340761184692383
Epoch 820, training loss: 0.07025768607854843 = 0.004284521099179983 + 0.01 * 6.597316741943359
Epoch 820, val loss: 1.1389409303665161
Epoch 830, training loss: 0.06969798356294632 = 0.00415920140221715 + 0.01 * 6.553877830505371
Epoch 830, val loss: 1.1437456607818604
Epoch 840, training loss: 0.06960587948560715 = 0.004040183965116739 + 0.01 * 6.556570053100586
Epoch 840, val loss: 1.1484202146530151
Epoch 850, training loss: 0.06953465938568115 = 0.003927099984139204 + 0.01 * 6.560756683349609
Epoch 850, val loss: 1.1530396938323975
Epoch 860, training loss: 0.06931654363870621 = 0.0038196302484720945 + 0.01 * 6.549691200256348
Epoch 860, val loss: 1.1575442552566528
Epoch 870, training loss: 0.06944490224123001 = 0.003717228537425399 + 0.01 * 6.57276725769043
Epoch 870, val loss: 1.161994457244873
Epoch 880, training loss: 0.06919371336698532 = 0.0036198035813868046 + 0.01 * 6.5573906898498535
Epoch 880, val loss: 1.16641366481781
Epoch 890, training loss: 0.068947933614254 = 0.00352685060352087 + 0.01 * 6.542108535766602
Epoch 890, val loss: 1.170708417892456
Epoch 900, training loss: 0.0688796117901802 = 0.003438195213675499 + 0.01 * 6.54414176940918
Epoch 900, val loss: 1.1749536991119385
Epoch 910, training loss: 0.06896992772817612 = 0.0033536420669406652 + 0.01 * 6.561628818511963
Epoch 910, val loss: 1.179127812385559
Epoch 920, training loss: 0.06859255582094193 = 0.00327281653881073 + 0.01 * 6.531973838806152
Epoch 920, val loss: 1.183204174041748
Epoch 930, training loss: 0.06859762221574783 = 0.003195516299456358 + 0.01 * 6.540210723876953
Epoch 930, val loss: 1.1872053146362305
Epoch 940, training loss: 0.06835244596004486 = 0.0031215280760079622 + 0.01 * 6.523092269897461
Epoch 940, val loss: 1.1912165880203247
Epoch 950, training loss: 0.06828752160072327 = 0.003050699597224593 + 0.01 * 6.523682594299316
Epoch 950, val loss: 1.1950922012329102
Epoch 960, training loss: 0.06835731863975525 = 0.0029827728867530823 + 0.01 * 6.537454605102539
Epoch 960, val loss: 1.1989699602127075
Epoch 970, training loss: 0.06820030510425568 = 0.0029177304822951555 + 0.01 * 6.528257369995117
Epoch 970, val loss: 1.2027381658554077
Epoch 980, training loss: 0.06809637695550919 = 0.0028554752934724092 + 0.01 * 6.524090766906738
Epoch 980, val loss: 1.206403136253357
Epoch 990, training loss: 0.06791863590478897 = 0.002795638982206583 + 0.01 * 6.51230001449585
Epoch 990, val loss: 1.2101256847381592
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.4207
Flip ASR: 0.3200/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0249929428100586 = 1.9412542581558228 + 0.01 * 8.373876571655273
Epoch 0, val loss: 1.9399570226669312
Epoch 10, training loss: 2.0145251750946045 = 1.930787444114685 + 0.01 * 8.373775482177734
Epoch 10, val loss: 1.928868293762207
Epoch 20, training loss: 2.001469135284424 = 1.9177346229553223 + 0.01 * 8.373443603515625
Epoch 20, val loss: 1.9148249626159668
Epoch 30, training loss: 1.982898235321045 = 1.8991750478744507 + 0.01 * 8.372323036193848
Epoch 30, val loss: 1.8948482275009155
Epoch 40, training loss: 1.955499291419983 = 1.8718363046646118 + 0.01 * 8.366300582885742
Epoch 40, val loss: 1.8659896850585938
Epoch 50, training loss: 1.9174933433532715 = 1.8342047929763794 + 0.01 * 8.328849792480469
Epoch 50, val loss: 1.828054666519165
Epoch 60, training loss: 1.8724706172943115 = 1.7909680604934692 + 0.01 * 8.150261878967285
Epoch 60, val loss: 1.7875761985778809
Epoch 70, training loss: 1.8265786170959473 = 1.7471901178359985 + 0.01 * 7.9388508796691895
Epoch 70, val loss: 1.7478322982788086
Epoch 80, training loss: 1.7672746181488037 = 1.6908156871795654 + 0.01 * 7.645888328552246
Epoch 80, val loss: 1.695626974105835
Epoch 90, training loss: 1.6881448030471802 = 1.6140683889389038 + 0.01 * 7.407637596130371
Epoch 90, val loss: 1.627280592918396
Epoch 100, training loss: 1.5891653299331665 = 1.5164939165115356 + 0.01 * 7.267141819000244
Epoch 100, val loss: 1.5445140600204468
Epoch 110, training loss: 1.4809800386428833 = 1.4088820219039917 + 0.01 * 7.2097978591918945
Epoch 110, val loss: 1.4565179347991943
Epoch 120, training loss: 1.375183343887329 = 1.303623914718628 + 0.01 * 7.155948638916016
Epoch 120, val loss: 1.3759269714355469
Epoch 130, training loss: 1.277800440788269 = 1.2066847085952759 + 0.01 * 7.111571311950684
Epoch 130, val loss: 1.3065524101257324
Epoch 140, training loss: 1.1891413927078247 = 1.1183322668075562 + 0.01 * 7.0809125900268555
Epoch 140, val loss: 1.2469569444656372
Epoch 150, training loss: 1.10580575466156 = 1.035131812095642 + 0.01 * 7.067393779754639
Epoch 150, val loss: 1.1920838356018066
Epoch 160, training loss: 1.0252857208251953 = 0.9546667337417603 + 0.01 * 7.061898708343506
Epoch 160, val loss: 1.1391847133636475
Epoch 170, training loss: 0.9478868246078491 = 0.8773133158683777 + 0.01 * 7.057351589202881
Epoch 170, val loss: 1.0879429578781128
Epoch 180, training loss: 0.8753358125686646 = 0.8048206567764282 + 0.01 * 7.051516056060791
Epoch 180, val loss: 1.0401124954223633
Epoch 190, training loss: 0.8088589906692505 = 0.7384175658226013 + 0.01 * 7.044139385223389
Epoch 190, val loss: 0.9967789053916931
Epoch 200, training loss: 0.7482147812843323 = 0.6778697371482849 + 0.01 * 7.034505367279053
Epoch 200, val loss: 0.9581084251403809
Epoch 210, training loss: 0.6921157240867615 = 0.6219035983085632 + 0.01 * 7.021212100982666
Epoch 210, val loss: 0.9235928058624268
Epoch 220, training loss: 0.6392561197280884 = 0.5692340731620789 + 0.01 * 7.002203464508057
Epoch 220, val loss: 0.8920095562934875
Epoch 230, training loss: 0.5893703103065491 = 0.5194805264472961 + 0.01 * 6.988979339599609
Epoch 230, val loss: 0.8631466627120972
Epoch 240, training loss: 0.5426990389823914 = 0.4730933904647827 + 0.01 * 6.96056604385376
Epoch 240, val loss: 0.8374125957489014
Epoch 250, training loss: 0.49995163083076477 = 0.43054813146591187 + 0.01 * 6.940350532531738
Epoch 250, val loss: 0.8152581453323364
Epoch 260, training loss: 0.46085792779922485 = 0.3916025161743164 + 0.01 * 6.925540447235107
Epoch 260, val loss: 0.7970988154411316
Epoch 270, training loss: 0.42464056611061096 = 0.3554789125919342 + 0.01 * 6.916166305541992
Epoch 270, val loss: 0.7827330231666565
Epoch 280, training loss: 0.39069727063179016 = 0.3216349482536316 + 0.01 * 6.9062323570251465
Epoch 280, val loss: 0.7716769576072693
Epoch 290, training loss: 0.3583429455757141 = 0.28942549228668213 + 0.01 * 6.8917436599731445
Epoch 290, val loss: 0.7626623511314392
Epoch 300, training loss: 0.3274990916252136 = 0.25866109132766724 + 0.01 * 6.8838019371032715
Epoch 300, val loss: 0.7559004426002502
Epoch 310, training loss: 0.29804328083992004 = 0.22929322719573975 + 0.01 * 6.87500524520874
Epoch 310, val loss: 0.7509260773658752
Epoch 320, training loss: 0.2704758644104004 = 0.20181286334991455 + 0.01 * 6.866302013397217
Epoch 320, val loss: 0.7479568123817444
Epoch 330, training loss: 0.2453441619873047 = 0.17678216099739075 + 0.01 * 6.856199741363525
Epoch 330, val loss: 0.7475276589393616
Epoch 340, training loss: 0.22280217707157135 = 0.15429212152957916 + 0.01 * 6.851006031036377
Epoch 340, val loss: 0.7492052316665649
Epoch 350, training loss: 0.2030046284198761 = 0.13451527059078217 + 0.01 * 6.848936080932617
Epoch 350, val loss: 0.7531349658966064
Epoch 360, training loss: 0.18582767248153687 = 0.11738217622041702 + 0.01 * 6.844549655914307
Epoch 360, val loss: 0.7593247294425964
Epoch 370, training loss: 0.17100557684898376 = 0.10259655863046646 + 0.01 * 6.8409013748168945
Epoch 370, val loss: 0.7674293518066406
Epoch 380, training loss: 0.15826120972633362 = 0.08990038931369781 + 0.01 * 6.836081504821777
Epoch 380, val loss: 0.7772494554519653
Epoch 390, training loss: 0.14733603596687317 = 0.07902854681015015 + 0.01 * 6.830749034881592
Epoch 390, val loss: 0.7882944941520691
Epoch 400, training loss: 0.13800400495529175 = 0.06972077488899231 + 0.01 * 6.828322410583496
Epoch 400, val loss: 0.8005213737487793
Epoch 410, training loss: 0.12999916076660156 = 0.06176484748721123 + 0.01 * 6.823431968688965
Epoch 410, val loss: 0.8133925795555115
Epoch 420, training loss: 0.12321695685386658 = 0.054971206933259964 + 0.01 * 6.8245744705200195
Epoch 420, val loss: 0.8268324136734009
Epoch 430, training loss: 0.11732058972120285 = 0.0491483137011528 + 0.01 * 6.817227840423584
Epoch 430, val loss: 0.8404460549354553
Epoch 440, training loss: 0.11225338280200958 = 0.044141512364149094 + 0.01 * 6.811186790466309
Epoch 440, val loss: 0.8542125225067139
Epoch 450, training loss: 0.10795652866363525 = 0.039815228432416916 + 0.01 * 6.814130783081055
Epoch 450, val loss: 0.8679701685905457
Epoch 460, training loss: 0.1041310727596283 = 0.03606540337204933 + 0.01 * 6.806567668914795
Epoch 460, val loss: 0.8815320134162903
Epoch 470, training loss: 0.10076271742582321 = 0.03279785066843033 + 0.01 * 6.796486854553223
Epoch 470, val loss: 0.894898533821106
Epoch 480, training loss: 0.09795007854700089 = 0.029936769977211952 + 0.01 * 6.801331520080566
Epoch 480, val loss: 0.9081132411956787
Epoch 490, training loss: 0.09532281011343002 = 0.027420498430728912 + 0.01 * 6.790231227874756
Epoch 490, val loss: 0.9209649562835693
Epoch 500, training loss: 0.09303941577672958 = 0.02519412524998188 + 0.01 * 6.784529685974121
Epoch 500, val loss: 0.9335327744483948
Epoch 510, training loss: 0.09098351746797562 = 0.023211432620882988 + 0.01 * 6.77720832824707
Epoch 510, val loss: 0.9458568096160889
Epoch 520, training loss: 0.08914010226726532 = 0.021441582590341568 + 0.01 * 6.769852161407471
Epoch 520, val loss: 0.9579050540924072
Epoch 530, training loss: 0.08746610581874847 = 0.019858989864587784 + 0.01 * 6.760711669921875
Epoch 530, val loss: 0.9697072505950928
Epoch 540, training loss: 0.08607310056686401 = 0.01844247803092003 + 0.01 * 6.763062477111816
Epoch 540, val loss: 0.9811853766441345
Epoch 550, training loss: 0.08472698926925659 = 0.017172399908304214 + 0.01 * 6.755458354949951
Epoch 550, val loss: 0.9923621416091919
Epoch 560, training loss: 0.08346522599458694 = 0.01602681539952755 + 0.01 * 6.74384069442749
Epoch 560, val loss: 1.0032612085342407
Epoch 570, training loss: 0.08245877921581268 = 0.01499124988913536 + 0.01 * 6.746753692626953
Epoch 570, val loss: 1.013899564743042
Epoch 580, training loss: 0.08135701715946198 = 0.014053959399461746 + 0.01 * 6.730305194854736
Epoch 580, val loss: 1.0242515802383423
Epoch 590, training loss: 0.08069729804992676 = 0.013203669339418411 + 0.01 * 6.749362468719482
Epoch 590, val loss: 1.0343081951141357
Epoch 600, training loss: 0.07954847067594528 = 0.012431989423930645 + 0.01 * 6.711648464202881
Epoch 600, val loss: 1.0440754890441895
Epoch 610, training loss: 0.07877228409051895 = 0.011728690005838871 + 0.01 * 6.704360008239746
Epoch 610, val loss: 1.0536127090454102
Epoch 620, training loss: 0.07812611758708954 = 0.011085491627454758 + 0.01 * 6.7040629386901855
Epoch 620, val loss: 1.062859296798706
Epoch 630, training loss: 0.07746203243732452 = 0.01049731019884348 + 0.01 * 6.69647216796875
Epoch 630, val loss: 1.0718811750411987
Epoch 640, training loss: 0.07750476896762848 = 0.009957714937627316 + 0.01 * 6.754705429077148
Epoch 640, val loss: 1.0806242227554321
Epoch 650, training loss: 0.07626289874315262 = 0.009462212212383747 + 0.01 * 6.680068492889404
Epoch 650, val loss: 1.089137315750122
Epoch 660, training loss: 0.0757434219121933 = 0.009005228988826275 + 0.01 * 6.6738200187683105
Epoch 660, val loss: 1.0974286794662476
Epoch 670, training loss: 0.07538259774446487 = 0.008582797832787037 + 0.01 * 6.6799798011779785
Epoch 670, val loss: 1.1055457592010498
Epoch 680, training loss: 0.07500811666250229 = 0.00819193385541439 + 0.01 * 6.6816182136535645
Epoch 680, val loss: 1.1134164333343506
Epoch 690, training loss: 0.07440091669559479 = 0.007829224690794945 + 0.01 * 6.657169342041016
Epoch 690, val loss: 1.1211268901824951
Epoch 700, training loss: 0.07404147833585739 = 0.00749267777428031 + 0.01 * 6.654880523681641
Epoch 700, val loss: 1.1285765171051025
Epoch 710, training loss: 0.07364768534898758 = 0.007179382257163525 + 0.01 * 6.6468305587768555
Epoch 710, val loss: 1.1358668804168701
Epoch 720, training loss: 0.07358230650424957 = 0.006887348834425211 + 0.01 * 6.669496059417725
Epoch 720, val loss: 1.1429436206817627
Epoch 730, training loss: 0.07292838394641876 = 0.006614936515688896 + 0.01 * 6.631344795227051
Epoch 730, val loss: 1.1498429775238037
Epoch 740, training loss: 0.07261915504932404 = 0.006360194645822048 + 0.01 * 6.625896453857422
Epoch 740, val loss: 1.1565495729446411
Epoch 750, training loss: 0.07227249443531036 = 0.0061213732697069645 + 0.01 * 6.6151123046875
Epoch 750, val loss: 1.163129448890686
Epoch 760, training loss: 0.07197348773479462 = 0.005897343158721924 + 0.01 * 6.607614517211914
Epoch 760, val loss: 1.1695080995559692
Epoch 770, training loss: 0.07194717973470688 = 0.0056871832348406315 + 0.01 * 6.625999450683594
Epoch 770, val loss: 1.1757477521896362
Epoch 780, training loss: 0.07154503464698792 = 0.005489465314894915 + 0.01 * 6.605556488037109
Epoch 780, val loss: 1.1818028688430786
Epoch 790, training loss: 0.07154383510351181 = 0.005303375422954559 + 0.01 * 6.6240458488464355
Epoch 790, val loss: 1.1877280473709106
Epoch 800, training loss: 0.07102153450250626 = 0.005127795040607452 + 0.01 * 6.58937406539917
Epoch 800, val loss: 1.1935497522354126
Epoch 810, training loss: 0.07085978239774704 = 0.0049621532671153545 + 0.01 * 6.5897626876831055
Epoch 810, val loss: 1.1991846561431885
Epoch 820, training loss: 0.07071346044540405 = 0.004805395845323801 + 0.01 * 6.59080696105957
Epoch 820, val loss: 1.2047014236450195
Epoch 830, training loss: 0.07045870274305344 = 0.0046571712009608746 + 0.01 * 6.580153465270996
Epoch 830, val loss: 1.210109829902649
Epoch 840, training loss: 0.07032482326030731 = 0.004516734275966883 + 0.01 * 6.580809116363525
Epoch 840, val loss: 1.215361475944519
Epoch 850, training loss: 0.06997163593769073 = 0.004383645486086607 + 0.01 * 6.5587992668151855
Epoch 850, val loss: 1.2204872369766235
Epoch 860, training loss: 0.06995605677366257 = 0.004257295746356249 + 0.01 * 6.569875717163086
Epoch 860, val loss: 1.2255109548568726
Epoch 870, training loss: 0.06960028409957886 = 0.0041372813284397125 + 0.01 * 6.546300888061523
Epoch 870, val loss: 1.2303719520568848
Epoch 880, training loss: 0.06970023363828659 = 0.004023152869194746 + 0.01 * 6.567708492279053
Epoch 880, val loss: 1.2351614236831665
Epoch 890, training loss: 0.06939572840929031 = 0.003914729226380587 + 0.01 * 6.548100471496582
Epoch 890, val loss: 1.2398053407669067
Epoch 900, training loss: 0.06919030845165253 = 0.0038114471826702356 + 0.01 * 6.537886142730713
Epoch 900, val loss: 1.2443509101867676
Epoch 910, training loss: 0.06930553168058395 = 0.003712976351380348 + 0.01 * 6.559256076812744
Epoch 910, val loss: 1.2487998008728027
Epoch 920, training loss: 0.06897350400686264 = 0.0036190450191497803 + 0.01 * 6.5354461669921875
Epoch 920, val loss: 1.2531538009643555
Epoch 930, training loss: 0.06891698390245438 = 0.003529400797560811 + 0.01 * 6.538758277893066
Epoch 930, val loss: 1.2574213743209839
Epoch 940, training loss: 0.06867900490760803 = 0.00344363902695477 + 0.01 * 6.523536682128906
Epoch 940, val loss: 1.2616113424301147
Epoch 950, training loss: 0.06873614341020584 = 0.0033616458531469107 + 0.01 * 6.537450313568115
Epoch 950, val loss: 1.2656826972961426
Epoch 960, training loss: 0.06855655461549759 = 0.003283177735283971 + 0.01 * 6.527337551116943
Epoch 960, val loss: 1.269699215888977
Epoch 970, training loss: 0.06880608201026917 = 0.0032082651741802692 + 0.01 * 6.559781551361084
Epoch 970, val loss: 1.2736115455627441
Epoch 980, training loss: 0.06819435954093933 = 0.0031364578753709793 + 0.01 * 6.505790710449219
Epoch 980, val loss: 1.2774708271026611
Epoch 990, training loss: 0.06809733062982559 = 0.0030676554888486862 + 0.01 * 6.502967834472656
Epoch 990, val loss: 1.281219244003296
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.6384
Flip ASR: 0.6000/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0271215438842773 = 1.9433823823928833 + 0.01 * 8.373917579650879
Epoch 0, val loss: 1.9433530569076538
Epoch 10, training loss: 2.017348527908325 = 1.9336098432540894 + 0.01 * 8.373862266540527
Epoch 10, val loss: 1.9333891868591309
Epoch 20, training loss: 2.005157709121704 = 1.921420931816101 + 0.01 * 8.373678207397461
Epoch 20, val loss: 1.9209551811218262
Epoch 30, training loss: 1.9876713752746582 = 1.9039398431777954 + 0.01 * 8.373150825500488
Epoch 30, val loss: 1.9032450914382935
Epoch 40, training loss: 1.961390495300293 = 1.8776839971542358 + 0.01 * 8.370645523071289
Epoch 40, val loss: 1.8771926164627075
Epoch 50, training loss: 1.9240880012512207 = 1.840562105178833 + 0.01 * 8.35258960723877
Epoch 50, val loss: 1.8422759771347046
Epoch 60, training loss: 1.880738615989685 = 1.798123836517334 + 0.01 * 8.261478424072266
Epoch 60, val loss: 1.8060004711151123
Epoch 70, training loss: 1.8374385833740234 = 1.7579669952392578 + 0.01 * 7.947155475616455
Epoch 70, val loss: 1.772045373916626
Epoch 80, training loss: 1.7850685119628906 = 1.7071261405944824 + 0.01 * 7.7942376136779785
Epoch 80, val loss: 1.7247589826583862
Epoch 90, training loss: 1.7150911092758179 = 1.6385917663574219 + 0.01 * 7.64993953704834
Epoch 90, val loss: 1.6636995077133179
Epoch 100, training loss: 1.6268788576126099 = 1.551558256149292 + 0.01 * 7.532063007354736
Epoch 100, val loss: 1.590969204902649
Epoch 110, training loss: 1.5290460586547852 = 1.4555401802062988 + 0.01 * 7.350585460662842
Epoch 110, val loss: 1.5112824440002441
Epoch 120, training loss: 1.4308757781982422 = 1.3589547872543335 + 0.01 * 7.192093849182129
Epoch 120, val loss: 1.4340747594833374
Epoch 130, training loss: 1.336061954498291 = 1.2647727727890015 + 0.01 * 7.1289167404174805
Epoch 130, val loss: 1.362276315689087
Epoch 140, training loss: 1.24325430393219 = 1.1724629402160645 + 0.01 * 7.079139232635498
Epoch 140, val loss: 1.2943449020385742
Epoch 150, training loss: 1.1515791416168213 = 1.0809602737426758 + 0.01 * 7.061885833740234
Epoch 150, val loss: 1.227879285812378
Epoch 160, training loss: 1.0606008768081665 = 0.9900984168052673 + 0.01 * 7.050243377685547
Epoch 160, val loss: 1.1632564067840576
Epoch 170, training loss: 0.9713490605354309 = 0.9009554982185364 + 0.01 * 7.0393548011779785
Epoch 170, val loss: 1.1007189750671387
Epoch 180, training loss: 0.8859964609146118 = 0.8157309889793396 + 0.01 * 7.026546478271484
Epoch 180, val loss: 1.041178822517395
Epoch 190, training loss: 0.8067725300788879 = 0.7366848587989807 + 0.01 * 7.00876522064209
Epoch 190, val loss: 0.986270010471344
Epoch 200, training loss: 0.7343714237213135 = 0.6645071506500244 + 0.01 * 6.9864277839660645
Epoch 200, val loss: 0.9369025826454163
Epoch 210, training loss: 0.6684653759002686 = 0.598812460899353 + 0.01 * 6.965291976928711
Epoch 210, val loss: 0.8931746482849121
Epoch 220, training loss: 0.6086047887802124 = 0.5390867590904236 + 0.01 * 6.951800346374512
Epoch 220, val loss: 0.8546143174171448
Epoch 230, training loss: 0.5547460913658142 = 0.4852825403213501 + 0.01 * 6.946353912353516
Epoch 230, val loss: 0.8212961554527283
Epoch 240, training loss: 0.5069224238395691 = 0.43749478459358215 + 0.01 * 6.94276237487793
Epoch 240, val loss: 0.7934901118278503
Epoch 250, training loss: 0.4647284150123596 = 0.39532268047332764 + 0.01 * 6.940572261810303
Epoch 250, val loss: 0.7714344263076782
Epoch 260, training loss: 0.4272482991218567 = 0.35786643624305725 + 0.01 * 6.93818473815918
Epoch 260, val loss: 0.7546584606170654
Epoch 270, training loss: 0.39332839846611023 = 0.32397153973579407 + 0.01 * 6.935687065124512
Epoch 270, val loss: 0.7422060370445251
Epoch 280, training loss: 0.3618621826171875 = 0.2925330102443695 + 0.01 * 6.932915687561035
Epoch 280, val loss: 0.7326605916023254
Epoch 290, training loss: 0.33183753490448 = 0.26253819465637207 + 0.01 * 6.929934978485107
Epoch 290, val loss: 0.7248952984809875
Epoch 300, training loss: 0.3024224638938904 = 0.23315171897411346 + 0.01 * 6.927074432373047
Epoch 300, val loss: 0.718339741230011
Epoch 310, training loss: 0.273362934589386 = 0.2041013389825821 + 0.01 * 6.926158905029297
Epoch 310, val loss: 0.712995707988739
Epoch 320, training loss: 0.24528929591178894 = 0.17606961727142334 + 0.01 * 6.921967506408691
Epoch 320, val loss: 0.7092494368553162
Epoch 330, training loss: 0.219707190990448 = 0.1505146622657776 + 0.01 * 6.919253826141357
Epoch 330, val loss: 0.7079172134399414
Epoch 340, training loss: 0.19786138832569122 = 0.12869803607463837 + 0.01 * 6.916335105895996
Epoch 340, val loss: 0.7095223069190979
Epoch 350, training loss: 0.17991824448108673 = 0.11077500134706497 + 0.01 * 6.9143242835998535
Epoch 350, val loss: 0.7137994766235352
Epoch 360, training loss: 0.16523048281669617 = 0.09612330794334412 + 0.01 * 6.910717964172363
Epoch 360, val loss: 0.7201057076454163
Epoch 370, training loss: 0.15304073691368103 = 0.08396919071674347 + 0.01 * 6.907155513763428
Epoch 370, val loss: 0.7277604341506958
Epoch 380, training loss: 0.14278525114059448 = 0.07370926439762115 + 0.01 * 6.907598495483398
Epoch 380, val loss: 0.7363855838775635
Epoch 390, training loss: 0.13392741978168488 = 0.06494500488042831 + 0.01 * 6.8982415199279785
Epoch 390, val loss: 0.7457671761512756
Epoch 400, training loss: 0.12633535265922546 = 0.05740965157747269 + 0.01 * 6.892570495605469
Epoch 400, val loss: 0.7556263208389282
Epoch 410, training loss: 0.11988445371389389 = 0.0509197860956192 + 0.01 * 6.896467208862305
Epoch 410, val loss: 0.7659568786621094
Epoch 420, training loss: 0.11412911117076874 = 0.04533258453011513 + 0.01 * 6.879652500152588
Epoch 420, val loss: 0.7766394019126892
Epoch 430, training loss: 0.10925275832414627 = 0.040523186326026917 + 0.01 * 6.872957229614258
Epoch 430, val loss: 0.7876018285751343
Epoch 440, training loss: 0.10502058267593384 = 0.03638416528701782 + 0.01 * 6.863641738891602
Epoch 440, val loss: 0.7985932230949402
Epoch 450, training loss: 0.10136767476797104 = 0.03281307965517044 + 0.01 * 6.855459690093994
Epoch 450, val loss: 0.8096628785133362
Epoch 460, training loss: 0.0981031283736229 = 0.029723450541496277 + 0.01 * 6.837967872619629
Epoch 460, val loss: 0.820600152015686
Epoch 470, training loss: 0.09529988467693329 = 0.027038859203457832 + 0.01 * 6.8261027336120605
Epoch 470, val loss: 0.8314688801765442
Epoch 480, training loss: 0.09289927780628204 = 0.024696748703718185 + 0.01 * 6.820252895355225
Epoch 480, val loss: 0.8421900272369385
Epoch 490, training loss: 0.09077781438827515 = 0.02264385297894478 + 0.01 * 6.813396453857422
Epoch 490, val loss: 0.8527458906173706
Epoch 500, training loss: 0.08895666152238846 = 0.020836539566516876 + 0.01 * 6.812012195587158
Epoch 500, val loss: 0.8630887269973755
Epoch 510, training loss: 0.08724283427000046 = 0.019238794222474098 + 0.01 * 6.800404071807861
Epoch 510, val loss: 0.8732455372810364
Epoch 520, training loss: 0.08568085730075836 = 0.01781967282295227 + 0.01 * 6.786118507385254
Epoch 520, val loss: 0.8831631541252136
Epoch 530, training loss: 0.08442568778991699 = 0.01655394770205021 + 0.01 * 6.787174224853516
Epoch 530, val loss: 0.8928424119949341
Epoch 540, training loss: 0.08319127559661865 = 0.015422407537698746 + 0.01 * 6.776886463165283
Epoch 540, val loss: 0.9022607207298279
Epoch 550, training loss: 0.08208232372999191 = 0.014405438676476479 + 0.01 * 6.767688751220703
Epoch 550, val loss: 0.9114648103713989
Epoch 560, training loss: 0.08108974248170853 = 0.0134878633543849 + 0.01 * 6.760188102722168
Epoch 560, val loss: 0.9204072952270508
Epoch 570, training loss: 0.0804130956530571 = 0.012657729908823967 + 0.01 * 6.77553653717041
Epoch 570, val loss: 0.9291322827339172
Epoch 580, training loss: 0.07948493212461472 = 0.011906323954463005 + 0.01 * 6.7578606605529785
Epoch 580, val loss: 0.9375672936439514
Epoch 590, training loss: 0.07865941524505615 = 0.011222509667277336 + 0.01 * 6.743690490722656
Epoch 590, val loss: 0.9457941651344299
Epoch 600, training loss: 0.07798701524734497 = 0.010598823428153992 + 0.01 * 6.738819122314453
Epoch 600, val loss: 0.9537364840507507
Epoch 610, training loss: 0.07739490270614624 = 0.010028400458395481 + 0.01 * 6.736650466918945
Epoch 610, val loss: 0.9615245461463928
Epoch 620, training loss: 0.07676354795694351 = 0.009505446068942547 + 0.01 * 6.725810527801514
Epoch 620, val loss: 0.9690415859222412
Epoch 630, training loss: 0.07622117549180984 = 0.009024558588862419 + 0.01 * 6.719661712646484
Epoch 630, val loss: 0.97639399766922
Epoch 640, training loss: 0.07580292969942093 = 0.008581405505537987 + 0.01 * 6.722152233123779
Epoch 640, val loss: 0.9835237860679626
Epoch 650, training loss: 0.07534600794315338 = 0.00817299447953701 + 0.01 * 6.717301368713379
Epoch 650, val loss: 0.9904444813728333
Epoch 660, training loss: 0.07515817135572433 = 0.007795022800564766 + 0.01 * 6.7363152503967285
Epoch 660, val loss: 0.997210681438446
Epoch 670, training loss: 0.07444203644990921 = 0.007445260416716337 + 0.01 * 6.699677467346191
Epoch 670, val loss: 1.0037257671356201
Epoch 680, training loss: 0.07415458559989929 = 0.0071210009045898914 + 0.01 * 6.703358173370361
Epoch 680, val loss: 1.0100600719451904
Epoch 690, training loss: 0.07366958260536194 = 0.006819058209657669 + 0.01 * 6.685051918029785
Epoch 690, val loss: 1.0162773132324219
Epoch 700, training loss: 0.07354322075843811 = 0.006537751294672489 + 0.01 * 6.700547218322754
Epoch 700, val loss: 1.0222549438476562
Epoch 710, training loss: 0.07306668907403946 = 0.006276226136833429 + 0.01 * 6.679046630859375
Epoch 710, val loss: 1.0281416177749634
Epoch 720, training loss: 0.07277187705039978 = 0.0060316347517073154 + 0.01 * 6.67402458190918
Epoch 720, val loss: 1.0338013172149658
Epoch 730, training loss: 0.07248289883136749 = 0.0058021387085318565 + 0.01 * 6.668076038360596
Epoch 730, val loss: 1.039380431175232
Epoch 740, training loss: 0.07238517701625824 = 0.005586820654571056 + 0.01 * 6.679836273193359
Epoch 740, val loss: 1.0447617769241333
Epoch 750, training loss: 0.07199644297361374 = 0.0053856512531638145 + 0.01 * 6.661078929901123
Epoch 750, val loss: 1.0500128269195557
Epoch 760, training loss: 0.0718187466263771 = 0.005196318030357361 + 0.01 * 6.662243366241455
Epoch 760, val loss: 1.055126428604126
Epoch 770, training loss: 0.07147356122732162 = 0.0050180573016405106 + 0.01 * 6.645550727844238
Epoch 770, val loss: 1.0601085424423218
Epoch 780, training loss: 0.07143206894397736 = 0.004849931225180626 + 0.01 * 6.6582136154174805
Epoch 780, val loss: 1.0649478435516357
Epoch 790, training loss: 0.07120606303215027 = 0.004691902082413435 + 0.01 * 6.651416301727295
Epoch 790, val loss: 1.0696879625320435
Epoch 800, training loss: 0.07104896754026413 = 0.004542370792478323 + 0.01 * 6.650659561157227
Epoch 800, val loss: 1.0742641687393188
Epoch 810, training loss: 0.07072057574987411 = 0.004401476122438908 + 0.01 * 6.6319098472595215
Epoch 810, val loss: 1.0787626504898071
Epoch 820, training loss: 0.07044723629951477 = 0.004267965909093618 + 0.01 * 6.617927074432373
Epoch 820, val loss: 1.0831218957901
Epoch 830, training loss: 0.07031291723251343 = 0.004141294397413731 + 0.01 * 6.617162227630615
Epoch 830, val loss: 1.0874000787734985
Epoch 840, training loss: 0.07029521465301514 = 0.004021562170237303 + 0.01 * 6.627365589141846
Epoch 840, val loss: 1.0915058851242065
Epoch 850, training loss: 0.07000309228897095 = 0.003907997626811266 + 0.01 * 6.609508991241455
Epoch 850, val loss: 1.0955685377120972
Epoch 860, training loss: 0.06994394958019257 = 0.0038002715446054935 + 0.01 * 6.614367961883545
Epoch 860, val loss: 1.099464774131775
Epoch 870, training loss: 0.0699722096323967 = 0.003697757376357913 + 0.01 * 6.627445220947266
Epoch 870, val loss: 1.103332281112671
Epoch 880, training loss: 0.06963125616312027 = 0.0036003487184643745 + 0.01 * 6.603090763092041
Epoch 880, val loss: 1.1070276498794556
Epoch 890, training loss: 0.0695081502199173 = 0.0035074532497674227 + 0.01 * 6.600070476531982
Epoch 890, val loss: 1.110676646232605
Epoch 900, training loss: 0.06931149214506149 = 0.003418851410970092 + 0.01 * 6.589264869689941
Epoch 900, val loss: 1.1142733097076416
Epoch 910, training loss: 0.0690583810210228 = 0.003334322478622198 + 0.01 * 6.572405815124512
Epoch 910, val loss: 1.1177489757537842
Epoch 920, training loss: 0.06916310638189316 = 0.0032535498030483723 + 0.01 * 6.59095573425293
Epoch 920, val loss: 1.1211938858032227
Epoch 930, training loss: 0.06886310875415802 = 0.0031764772720634937 + 0.01 * 6.568662643432617
Epoch 930, val loss: 1.124448299407959
Epoch 940, training loss: 0.06907584518194199 = 0.0031028022058308125 + 0.01 * 6.597304344177246
Epoch 940, val loss: 1.1277185678482056
Epoch 950, training loss: 0.06875472515821457 = 0.0030324161052703857 + 0.01 * 6.572231292724609
Epoch 950, val loss: 1.130847692489624
Epoch 960, training loss: 0.06857392191886902 = 0.002964954823255539 + 0.01 * 6.560896396636963
Epoch 960, val loss: 1.133934736251831
Epoch 970, training loss: 0.06844020634889603 = 0.002900318941101432 + 0.01 * 6.553988933563232
Epoch 970, val loss: 1.1369463205337524
Epoch 980, training loss: 0.06861688941717148 = 0.002838301006704569 + 0.01 * 6.577858924865723
Epoch 980, val loss: 1.139896273612976
Epoch 990, training loss: 0.06811657547950745 = 0.002778891008347273 + 0.01 * 6.533769130706787
Epoch 990, val loss: 1.142793893814087
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.8229
Flip ASR: 0.7956/225 nodes
The final ASR:0.62731, 0.16439, Accuracy:0.82593, 0.01048
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11536])
remove edge: torch.Size([2, 9510])
updated graph: torch.Size([2, 10490])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9963
Flip ASR: 0.9956/225 nodes
The final ASR:0.98155, 0.01044, Accuracy:0.82840, 0.00462
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0197501182556152 = 1.9360114336013794 + 0.01 * 8.37386417388916
Epoch 0, val loss: 1.9428532123565674
Epoch 10, training loss: 2.010354995727539 = 1.92661714553833 + 0.01 * 8.373793601989746
Epoch 10, val loss: 1.933059811592102
Epoch 20, training loss: 1.9987540245056152 = 1.9150186777114868 + 0.01 * 8.37353515625
Epoch 20, val loss: 1.9208588600158691
Epoch 30, training loss: 1.982364535331726 = 1.8986371755599976 + 0.01 * 8.372733116149902
Epoch 30, val loss: 1.9036670923233032
Epoch 40, training loss: 1.9581180810928345 = 1.8744326829910278 + 0.01 * 8.368535041809082
Epoch 40, val loss: 1.878697395324707
Epoch 50, training loss: 1.9238693714141846 = 1.8404767513275146 + 0.01 * 8.339261054992676
Epoch 50, val loss: 1.8453240394592285
Epoch 60, training loss: 1.8816990852355957 = 1.7999120950698853 + 0.01 * 8.178705215454102
Epoch 60, val loss: 1.8086895942687988
Epoch 70, training loss: 1.8368624448776245 = 1.7576959133148193 + 0.01 * 7.916653633117676
Epoch 70, val loss: 1.7728005647659302
Epoch 80, training loss: 1.7809303998947144 = 1.704355001449585 + 0.01 * 7.657538890838623
Epoch 80, val loss: 1.7259718179702759
Epoch 90, training loss: 1.7056050300598145 = 1.631612777709961 + 0.01 * 7.399230480194092
Epoch 90, val loss: 1.6631430387496948
Epoch 100, training loss: 1.609286904335022 = 1.537353754043579 + 0.01 * 7.1933112144470215
Epoch 100, val loss: 1.5847382545471191
Epoch 110, training loss: 1.499047040939331 = 1.4283638000488281 + 0.01 * 7.06833028793335
Epoch 110, val loss: 1.496071696281433
Epoch 120, training loss: 1.3855891227722168 = 1.315480351448059 + 0.01 * 7.010882377624512
Epoch 120, val loss: 1.408448338508606
Epoch 130, training loss: 1.2755893468856812 = 1.2058876752853394 + 0.01 * 6.9701714515686035
Epoch 130, val loss: 1.3272536993026733
Epoch 140, training loss: 1.1739387512207031 = 1.104440450668335 + 0.01 * 6.949824333190918
Epoch 140, val loss: 1.2555274963378906
Epoch 150, training loss: 1.0835926532745361 = 1.0142215490341187 + 0.01 * 6.937106609344482
Epoch 150, val loss: 1.1931568384170532
Epoch 160, training loss: 1.0038219690322876 = 0.9345316886901855 + 0.01 * 6.929022789001465
Epoch 160, val loss: 1.138468623161316
Epoch 170, training loss: 0.9309574365615845 = 0.8617203235626221 + 0.01 * 6.923710823059082
Epoch 170, val loss: 1.0883477926254272
Epoch 180, training loss: 0.8609104752540588 = 0.7917217016220093 + 0.01 * 6.91887903213501
Epoch 180, val loss: 1.0393568277359009
Epoch 190, training loss: 0.7912169098854065 = 0.7220834493637085 + 0.01 * 6.913346767425537
Epoch 190, val loss: 0.9891643524169922
Epoch 200, training loss: 0.7217973470687866 = 0.6527382135391235 + 0.01 * 6.9059157371521
Epoch 200, val loss: 0.9378634691238403
Epoch 210, training loss: 0.6541298627853394 = 0.5851669311523438 + 0.01 * 6.896292209625244
Epoch 210, val loss: 0.8870410323143005
Epoch 220, training loss: 0.5899419188499451 = 0.5210888981819153 + 0.01 * 6.8853044509887695
Epoch 220, val loss: 0.8392737507820129
Epoch 230, training loss: 0.5305602550506592 = 0.4618285596370697 + 0.01 * 6.873170852661133
Epoch 230, val loss: 0.7968930006027222
Epoch 240, training loss: 0.47647544741630554 = 0.40788641571998596 + 0.01 * 6.858902454376221
Epoch 240, val loss: 0.7611377835273743
Epoch 250, training loss: 0.42768675088882446 = 0.3592216968536377 + 0.01 * 6.846506118774414
Epoch 250, val loss: 0.7317535877227783
Epoch 260, training loss: 0.38375186920166016 = 0.3154067099094391 + 0.01 * 6.834517002105713
Epoch 260, val loss: 0.7075258493423462
Epoch 270, training loss: 0.3441455364227295 = 0.2758101224899292 + 0.01 * 6.833542346954346
Epoch 270, val loss: 0.6873276233673096
Epoch 280, training loss: 0.30814695358276367 = 0.2399829477071762 + 0.01 * 6.816400051116943
Epoch 280, val loss: 0.6704115271568298
Epoch 290, training loss: 0.2757182717323303 = 0.20766204595565796 + 0.01 * 6.805621147155762
Epoch 290, val loss: 0.6562427878379822
Epoch 300, training loss: 0.24686533212661743 = 0.17885655164718628 + 0.01 * 6.800878524780273
Epoch 300, val loss: 0.6446933150291443
Epoch 310, training loss: 0.22161850333213806 = 0.15371862053871155 + 0.01 * 6.789988040924072
Epoch 310, val loss: 0.6359183192253113
Epoch 320, training loss: 0.20007553696632385 = 0.13224485516548157 + 0.01 * 6.7830681800842285
Epoch 320, val loss: 0.6300919055938721
Epoch 330, training loss: 0.18190881609916687 = 0.11416149884462357 + 0.01 * 6.7747321128845215
Epoch 330, val loss: 0.6271690726280212
Epoch 340, training loss: 0.16672837734222412 = 0.09903646260499954 + 0.01 * 6.769190788269043
Epoch 340, val loss: 0.6268811225891113
Epoch 350, training loss: 0.1540307104587555 = 0.08639688789844513 + 0.01 * 6.763382434844971
Epoch 350, val loss: 0.6289278864860535
Epoch 360, training loss: 0.14338253438472748 = 0.07579788565635681 + 0.01 * 6.75846529006958
Epoch 360, val loss: 0.6329095959663391
Epoch 370, training loss: 0.13441205024719238 = 0.06686805188655853 + 0.01 * 6.754399299621582
Epoch 370, val loss: 0.638379693031311
Epoch 380, training loss: 0.12681809067726135 = 0.05930240452289581 + 0.01 * 6.75156831741333
Epoch 380, val loss: 0.6449534893035889
Epoch 390, training loss: 0.12032486498355865 = 0.05285727605223656 + 0.01 * 6.746758937835693
Epoch 390, val loss: 0.652374267578125
Epoch 400, training loss: 0.1147240400314331 = 0.0473317988216877 + 0.01 * 6.739224910736084
Epoch 400, val loss: 0.6603512167930603
Epoch 410, training loss: 0.10998650640249252 = 0.04256930202245712 + 0.01 * 6.741720676422119
Epoch 410, val loss: 0.6687134504318237
Epoch 420, training loss: 0.10580838471651077 = 0.038449667394161224 + 0.01 * 6.7358717918396
Epoch 420, val loss: 0.6772964000701904
Epoch 430, training loss: 0.10214914381504059 = 0.034869201481342316 + 0.01 * 6.727994441986084
Epoch 430, val loss: 0.6859980821609497
Epoch 440, training loss: 0.09895150363445282 = 0.03174296393990517 + 0.01 * 6.72085428237915
Epoch 440, val loss: 0.6947507858276367
Epoch 450, training loss: 0.09632337838411331 = 0.02899899333715439 + 0.01 * 6.732438564300537
Epoch 450, val loss: 0.7034899592399597
Epoch 460, training loss: 0.09369850158691406 = 0.026585735380649567 + 0.01 * 6.711276531219482
Epoch 460, val loss: 0.7121574878692627
Epoch 470, training loss: 0.09164012968540192 = 0.02445349656045437 + 0.01 * 6.718663215637207
Epoch 470, val loss: 0.7207183241844177
Epoch 480, training loss: 0.08962129056453705 = 0.022562963888049126 + 0.01 * 6.705832481384277
Epoch 480, val loss: 0.7291191816329956
Epoch 490, training loss: 0.0878816545009613 = 0.020878883078694344 + 0.01 * 6.700277805328369
Epoch 490, val loss: 0.7373847961425781
Epoch 500, training loss: 0.08630456030368805 = 0.019371863454580307 + 0.01 * 6.693269729614258
Epoch 500, val loss: 0.745552122592926
Epoch 510, training loss: 0.08505543321371078 = 0.018018461763858795 + 0.01 * 6.703697204589844
Epoch 510, val loss: 0.7535406947135925
Epoch 520, training loss: 0.08378520607948303 = 0.01680067740380764 + 0.01 * 6.698452472686768
Epoch 520, val loss: 0.7613710761070251
Epoch 530, training loss: 0.08257193863391876 = 0.015703894197940826 + 0.01 * 6.68680477142334
Epoch 530, val loss: 0.7689882516860962
Epoch 540, training loss: 0.08148267865180969 = 0.01471199095249176 + 0.01 * 6.677068710327148
Epoch 540, val loss: 0.7764521837234497
Epoch 550, training loss: 0.08057590574026108 = 0.013811206445097923 + 0.01 * 6.676469802856445
Epoch 550, val loss: 0.7837575078010559
Epoch 560, training loss: 0.07972782105207443 = 0.01299189031124115 + 0.01 * 6.673593044281006
Epoch 560, val loss: 0.7908952236175537
Epoch 570, training loss: 0.07895487546920776 = 0.012244834564626217 + 0.01 * 6.671004295349121
Epoch 570, val loss: 0.7978895306587219
Epoch 580, training loss: 0.07812183350324631 = 0.011561988852918148 + 0.01 * 6.655984878540039
Epoch 580, val loss: 0.8046821355819702
Epoch 590, training loss: 0.07762699574232101 = 0.010936019010841846 + 0.01 * 6.669097900390625
Epoch 590, val loss: 0.8112757802009583
Epoch 600, training loss: 0.07693766057491302 = 0.010362350381910801 + 0.01 * 6.657531261444092
Epoch 600, val loss: 0.8177976012229919
Epoch 610, training loss: 0.07639819383621216 = 0.009834078140556812 + 0.01 * 6.656411170959473
Epoch 610, val loss: 0.8240671753883362
Epoch 620, training loss: 0.07581634819507599 = 0.009347101673483849 + 0.01 * 6.64692497253418
Epoch 620, val loss: 0.8302499651908875
Epoch 630, training loss: 0.07532121986150742 = 0.00889661442488432 + 0.01 * 6.642460823059082
Epoch 630, val loss: 0.836260199546814
Epoch 640, training loss: 0.0749526172876358 = 0.008480565622448921 + 0.01 * 6.647205352783203
Epoch 640, val loss: 0.8421717286109924
Epoch 650, training loss: 0.074390709400177 = 0.008094937540590763 + 0.01 * 6.62957763671875
Epoch 650, val loss: 0.8478771448135376
Epoch 660, training loss: 0.07417675107717514 = 0.0077367727644741535 + 0.01 * 6.643997669219971
Epoch 660, val loss: 0.8534768223762512
Epoch 670, training loss: 0.0736878365278244 = 0.007403627038002014 + 0.01 * 6.628421306610107
Epoch 670, val loss: 0.8589842915534973
Epoch 680, training loss: 0.07324974238872528 = 0.007093008607625961 + 0.01 * 6.615674018859863
Epoch 680, val loss: 0.8643106818199158
Epoch 690, training loss: 0.07303319871425629 = 0.006802808493375778 + 0.01 * 6.6230387687683105
Epoch 690, val loss: 0.8695082664489746
Epoch 700, training loss: 0.07264851778745651 = 0.006531903520226479 + 0.01 * 6.611661434173584
Epoch 700, val loss: 0.8746442794799805
Epoch 710, training loss: 0.07233880460262299 = 0.0062781129963696 + 0.01 * 6.606069564819336
Epoch 710, val loss: 0.8796243667602539
Epoch 720, training loss: 0.0720730572938919 = 0.006040221080183983 + 0.01 * 6.603283882141113
Epoch 720, val loss: 0.8845109343528748
Epoch 730, training loss: 0.07183914631605148 = 0.005816877353936434 + 0.01 * 6.602227210998535
Epoch 730, val loss: 0.8892651796340942
Epoch 740, training loss: 0.07159921526908875 = 0.005606996361166239 + 0.01 * 6.599222183227539
Epoch 740, val loss: 0.8939541578292847
Epoch 750, training loss: 0.07139963656663895 = 0.00540948798879981 + 0.01 * 6.599015235900879
Epoch 750, val loss: 0.8984686732292175
Epoch 760, training loss: 0.0711672455072403 = 0.005223528016358614 + 0.01 * 6.594372272491455
Epoch 760, val loss: 0.9029329419136047
Epoch 770, training loss: 0.0709502249956131 = 0.0050481166690588 + 0.01 * 6.590210437774658
Epoch 770, val loss: 0.9073176383972168
Epoch 780, training loss: 0.07078216969966888 = 0.004882493056356907 + 0.01 * 6.589968204498291
Epoch 780, val loss: 0.9115658402442932
Epoch 790, training loss: 0.07056979835033417 = 0.004726149141788483 + 0.01 * 6.584364891052246
Epoch 790, val loss: 0.9158101081848145
Epoch 800, training loss: 0.0704261064529419 = 0.004578045569360256 + 0.01 * 6.584805965423584
Epoch 800, val loss: 0.9198964834213257
Epoch 810, training loss: 0.07023723423480988 = 0.004437697120010853 + 0.01 * 6.579953670501709
Epoch 810, val loss: 0.923909068107605
Epoch 820, training loss: 0.07030584663152695 = 0.004304736386984587 + 0.01 * 6.60011100769043
Epoch 820, val loss: 0.9278636574745178
Epoch 830, training loss: 0.06994803994894028 = 0.0041785952635109425 + 0.01 * 6.576944351196289
Epoch 830, val loss: 0.9316905736923218
Epoch 840, training loss: 0.06976724416017532 = 0.004058978520333767 + 0.01 * 6.570826530456543
Epoch 840, val loss: 0.9354723691940308
Epoch 850, training loss: 0.06981581449508667 = 0.003945020493119955 + 0.01 * 6.587079048156738
Epoch 850, val loss: 0.9391297101974487
Epoch 860, training loss: 0.0695032849907875 = 0.003836935618892312 + 0.01 * 6.566634654998779
Epoch 860, val loss: 0.9428334832191467
Epoch 870, training loss: 0.06928236037492752 = 0.003733845427632332 + 0.01 * 6.554851531982422
Epoch 870, val loss: 0.9463506937026978
Epoch 880, training loss: 0.06911695003509521 = 0.0036356239579617977 + 0.01 * 6.54813289642334
Epoch 880, val loss: 0.9498351812362671
Epoch 890, training loss: 0.06915228813886642 = 0.003541998565196991 + 0.01 * 6.561028957366943
Epoch 890, val loss: 0.9532869458198547
Epoch 900, training loss: 0.06892392784357071 = 0.0034525925293564796 + 0.01 * 6.547133445739746
Epoch 900, val loss: 0.9565931558609009
Epoch 910, training loss: 0.0688326433300972 = 0.0033672875724732876 + 0.01 * 6.546535491943359
Epoch 910, val loss: 0.9598923325538635
Epoch 920, training loss: 0.0687456727027893 = 0.003285760059952736 + 0.01 * 6.545991897583008
Epoch 920, val loss: 0.9631214141845703
Epoch 930, training loss: 0.06866300106048584 = 0.0032078095246106386 + 0.01 * 6.54551887512207
Epoch 930, val loss: 0.9662806987762451
Epoch 940, training loss: 0.06856649369001389 = 0.0031331758946180344 + 0.01 * 6.543331623077393
Epoch 940, val loss: 0.9694756269454956
Epoch 950, training loss: 0.06851339340209961 = 0.00306165823712945 + 0.01 * 6.545173168182373
Epoch 950, val loss: 0.9724414944648743
Epoch 960, training loss: 0.06824864447116852 = 0.002993264701217413 + 0.01 * 6.525537967681885
Epoch 960, val loss: 0.975530207157135
Epoch 970, training loss: 0.06814949214458466 = 0.002927543828263879 + 0.01 * 6.522195339202881
Epoch 970, val loss: 0.9784485101699829
Epoch 980, training loss: 0.06810938566923141 = 0.0028645519632846117 + 0.01 * 6.5244832038879395
Epoch 980, val loss: 0.9813029170036316
Epoch 990, training loss: 0.06798220425844193 = 0.002804187824949622 + 0.01 * 6.517801284790039
Epoch 990, val loss: 0.9841902256011963
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.6125
Flip ASR: 0.5422/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.018826961517334 = 1.9350881576538086 + 0.01 * 8.37387752532959
Epoch 0, val loss: 1.929948329925537
Epoch 10, training loss: 2.0094034671783447 = 1.925665259361267 + 0.01 * 8.373824119567871
Epoch 10, val loss: 1.9208109378814697
Epoch 20, training loss: 1.998147964477539 = 1.9144119024276733 + 0.01 * 8.373611450195312
Epoch 20, val loss: 1.9099681377410889
Epoch 30, training loss: 1.9825263023376465 = 1.898796558380127 + 0.01 * 8.372973442077637
Epoch 30, val loss: 1.8951596021652222
Epoch 40, training loss: 1.959498405456543 = 1.8757985830307007 + 0.01 * 8.369976997375488
Epoch 40, val loss: 1.873694896697998
Epoch 50, training loss: 1.9261478185653687 = 1.8426769971847534 + 0.01 * 8.347077369689941
Epoch 50, val loss: 1.844065546989441
Epoch 60, training loss: 1.8826478719711304 = 1.8005868196487427 + 0.01 * 8.206100463867188
Epoch 60, val loss: 1.8090728521347046
Epoch 70, training loss: 1.8339027166366577 = 1.754569411277771 + 0.01 * 7.933326721191406
Epoch 70, val loss: 1.7714651823043823
Epoch 80, training loss: 1.774439811706543 = 1.6972217559814453 + 0.01 * 7.7218017578125
Epoch 80, val loss: 1.7197340726852417
Epoch 90, training loss: 1.6940898895263672 = 1.6189403533935547 + 0.01 * 7.514954090118408
Epoch 90, val loss: 1.6511635780334473
Epoch 100, training loss: 1.594329833984375 = 1.5211961269378662 + 0.01 * 7.3133697509765625
Epoch 100, val loss: 1.571232795715332
Epoch 110, training loss: 1.48745858669281 = 1.414871335029602 + 0.01 * 7.258722305297852
Epoch 110, val loss: 1.4828791618347168
Epoch 120, training loss: 1.382015585899353 = 1.3099676370620728 + 0.01 * 7.20479154586792
Epoch 120, val loss: 1.3972047567367554
Epoch 130, training loss: 1.281700849533081 = 1.2102346420288086 + 0.01 * 7.1466193199157715
Epoch 130, val loss: 1.3170706033706665
Epoch 140, training loss: 1.187837839126587 = 1.117031455039978 + 0.01 * 7.080644130706787
Epoch 140, val loss: 1.2442333698272705
Epoch 150, training loss: 1.100736141204834 = 1.030577540397644 + 0.01 * 7.015865802764893
Epoch 150, val loss: 1.1787017583847046
Epoch 160, training loss: 1.019213080406189 = 0.949510931968689 + 0.01 * 6.970215320587158
Epoch 160, val loss: 1.1180146932601929
Epoch 170, training loss: 0.9414517879486084 = 0.8719872832298279 + 0.01 * 6.946447372436523
Epoch 170, val loss: 1.0604223012924194
Epoch 180, training loss: 0.8661787509918213 = 0.796866238117218 + 0.01 * 6.931252956390381
Epoch 180, val loss: 1.0045536756515503
Epoch 190, training loss: 0.7937764525413513 = 0.7245775461196899 + 0.01 * 6.919890880584717
Epoch 190, val loss: 0.9515939354896545
Epoch 200, training loss: 0.725436806678772 = 0.656341552734375 + 0.01 * 6.9095234870910645
Epoch 200, val loss: 0.9039880633354187
Epoch 210, training loss: 0.6618895530700684 = 0.5928859114646912 + 0.01 * 6.900363922119141
Epoch 210, val loss: 0.8637681603431702
Epoch 220, training loss: 0.6032606363296509 = 0.5343174934387207 + 0.01 * 6.894316673278809
Epoch 220, val loss: 0.8315730690956116
Epoch 230, training loss: 0.5492836236953735 = 0.48042353987693787 + 0.01 * 6.8860063552856445
Epoch 230, val loss: 0.8066394329071045
Epoch 240, training loss: 0.49964481592178345 = 0.4308379292488098 + 0.01 * 6.88068962097168
Epoch 240, val loss: 0.7879518270492554
Epoch 250, training loss: 0.453933984041214 = 0.38517287373542786 + 0.01 * 6.8761115074157715
Epoch 250, val loss: 0.7748211622238159
Epoch 260, training loss: 0.41191601753234863 = 0.34320101141929626 + 0.01 * 6.871499538421631
Epoch 260, val loss: 0.766416072845459
Epoch 270, training loss: 0.3734882175922394 = 0.3048021197319031 + 0.01 * 6.868610382080078
Epoch 270, val loss: 0.7622500658035278
Epoch 280, training loss: 0.3385712206363678 = 0.26992708444595337 + 0.01 * 6.864414215087891
Epoch 280, val loss: 0.7615799903869629
Epoch 290, training loss: 0.30717921257019043 = 0.23855680227279663 + 0.01 * 6.862239360809326
Epoch 290, val loss: 0.7640615105628967
Epoch 300, training loss: 0.2792282998561859 = 0.21061111986637115 + 0.01 * 6.861719131469727
Epoch 300, val loss: 0.7694284319877625
Epoch 310, training loss: 0.2544645667076111 = 0.185883566737175 + 0.01 * 6.85809850692749
Epoch 310, val loss: 0.7773345112800598
Epoch 320, training loss: 0.23266364634037018 = 0.16411350667476654 + 0.01 * 6.855014324188232
Epoch 320, val loss: 0.7874289155006409
Epoch 330, training loss: 0.2135084867477417 = 0.1449870616197586 + 0.01 * 6.852142810821533
Epoch 330, val loss: 0.7993904948234558
Epoch 340, training loss: 0.19667448103427887 = 0.12818238139152527 + 0.01 * 6.849209785461426
Epoch 340, val loss: 0.8129245042800903
Epoch 350, training loss: 0.1819002777338028 = 0.11342940479516983 + 0.01 * 6.847087383270264
Epoch 350, val loss: 0.8276761174201965
Epoch 360, training loss: 0.16894207894802094 = 0.10048593580722809 + 0.01 * 6.845614433288574
Epoch 360, val loss: 0.8433091044425964
Epoch 370, training loss: 0.15755102038383484 = 0.08914554119110107 + 0.01 * 6.840548515319824
Epoch 370, val loss: 0.8596564531326294
Epoch 380, training loss: 0.14757950603961945 = 0.07921553403139114 + 0.01 * 6.836397647857666
Epoch 380, val loss: 0.8766148090362549
Epoch 390, training loss: 0.1388397216796875 = 0.07051742076873779 + 0.01 * 6.832230091094971
Epoch 390, val loss: 0.8940282464027405
Epoch 400, training loss: 0.1311991810798645 = 0.06289569288492203 + 0.01 * 6.830349922180176
Epoch 400, val loss: 0.9118158221244812
Epoch 410, training loss: 0.12447600066661835 = 0.056219134479761124 + 0.01 * 6.825687408447266
Epoch 410, val loss: 0.9298362135887146
Epoch 420, training loss: 0.11855120956897736 = 0.05037090927362442 + 0.01 * 6.81803035736084
Epoch 420, val loss: 0.9479964375495911
Epoch 430, training loss: 0.11343999207019806 = 0.045249830931425095 + 0.01 * 6.819016456604004
Epoch 430, val loss: 0.9660946130752563
Epoch 440, training loss: 0.10886497050523758 = 0.04076983779668808 + 0.01 * 6.809513092041016
Epoch 440, val loss: 0.9839946627616882
Epoch 450, training loss: 0.10488618165254593 = 0.03684829920530319 + 0.01 * 6.803788661956787
Epoch 450, val loss: 1.0015981197357178
Epoch 460, training loss: 0.10136862099170685 = 0.03341022878885269 + 0.01 * 6.795839309692383
Epoch 460, val loss: 1.018810749053955
Epoch 470, training loss: 0.0982903316617012 = 0.030390672385692596 + 0.01 * 6.789966106414795
Epoch 470, val loss: 1.035558819770813
Epoch 480, training loss: 0.09560340642929077 = 0.02773549035191536 + 0.01 * 6.786791801452637
Epoch 480, val loss: 1.0517051219940186
Epoch 490, training loss: 0.09318471699953079 = 0.02539750374853611 + 0.01 * 6.778721809387207
Epoch 490, val loss: 1.0672156810760498
Epoch 500, training loss: 0.09102214872837067 = 0.02333049848675728 + 0.01 * 6.7691650390625
Epoch 500, val loss: 1.082351565361023
Epoch 510, training loss: 0.08919599652290344 = 0.02149716392159462 + 0.01 * 6.76988410949707
Epoch 510, val loss: 1.0968910455703735
Epoch 520, training loss: 0.08738316595554352 = 0.01986752077937126 + 0.01 * 6.751564025878906
Epoch 520, val loss: 1.111012578010559
Epoch 530, training loss: 0.08585214614868164 = 0.018413785845041275 + 0.01 * 6.743836879730225
Epoch 530, val loss: 1.1246553659439087
Epoch 540, training loss: 0.08452343940734863 = 0.01711273565888405 + 0.01 * 6.7410712242126465
Epoch 540, val loss: 1.1378414630889893
Epoch 550, training loss: 0.08325912803411484 = 0.01594589650630951 + 0.01 * 6.7313232421875
Epoch 550, val loss: 1.1506141424179077
Epoch 560, training loss: 0.08213149011135101 = 0.014895566739141941 + 0.01 * 6.723592758178711
Epoch 560, val loss: 1.1629247665405273
Epoch 570, training loss: 0.08117683976888657 = 0.013947069644927979 + 0.01 * 6.722977161407471
Epoch 570, val loss: 1.1748367547988892
Epoch 580, training loss: 0.08025161176919937 = 0.013088655658066273 + 0.01 * 6.7162957191467285
Epoch 580, val loss: 1.1864005327224731
Epoch 590, training loss: 0.07951722294092178 = 0.012309126555919647 + 0.01 * 6.720809459686279
Epoch 590, val loss: 1.1975427865982056
Epoch 600, training loss: 0.07872068136930466 = 0.011600037105381489 + 0.01 * 6.712064266204834
Epoch 600, val loss: 1.2083954811096191
Epoch 610, training loss: 0.07799598574638367 = 0.010953143239021301 + 0.01 * 6.704284191131592
Epoch 610, val loss: 1.2188541889190674
Epoch 620, training loss: 0.0772804245352745 = 0.010361026041209698 + 0.01 * 6.6919403076171875
Epoch 620, val loss: 1.2290489673614502
Epoch 630, training loss: 0.0769193097949028 = 0.00981757789850235 + 0.01 * 6.7101731300354
Epoch 630, val loss: 1.2389436960220337
Epoch 640, training loss: 0.07620923221111298 = 0.00931896548718214 + 0.01 * 6.689027309417725
Epoch 640, val loss: 1.2484643459320068
Epoch 650, training loss: 0.07564254105091095 = 0.008859905414283276 + 0.01 * 6.678264141082764
Epoch 650, val loss: 1.2577582597732544
Epoch 660, training loss: 0.07521121948957443 = 0.008435854688286781 + 0.01 * 6.677536964416504
Epoch 660, val loss: 1.2668012380599976
Epoch 670, training loss: 0.07473419606685638 = 0.008043462410569191 + 0.01 * 6.669073581695557
Epoch 670, val loss: 1.2755976915359497
Epoch 680, training loss: 0.07444724440574646 = 0.007679468486458063 + 0.01 * 6.6767778396606445
Epoch 680, val loss: 1.2841321229934692
Epoch 690, training loss: 0.0742112323641777 = 0.0073416633531451225 + 0.01 * 6.686957359313965
Epoch 690, val loss: 1.2924035787582397
Epoch 700, training loss: 0.07370902597904205 = 0.0070281862281262875 + 0.01 * 6.668084144592285
Epoch 700, val loss: 1.3005495071411133
Epoch 710, training loss: 0.07329536229372025 = 0.006736202165484428 + 0.01 * 6.6559157371521
Epoch 710, val loss: 1.3083757162094116
Epoch 720, training loss: 0.07295423001050949 = 0.006463603116571903 + 0.01 * 6.649062633514404
Epoch 720, val loss: 1.316038966178894
Epoch 730, training loss: 0.07281091064214706 = 0.006208732724189758 + 0.01 * 6.660218238830566
Epoch 730, val loss: 1.3235256671905518
Epoch 740, training loss: 0.07242844998836517 = 0.005970354191958904 + 0.01 * 6.645810127258301
Epoch 740, val loss: 1.330776572227478
Epoch 750, training loss: 0.07221020013093948 = 0.005746921990066767 + 0.01 * 6.646327972412109
Epoch 750, val loss: 1.3378496170043945
Epoch 760, training loss: 0.07189854234457016 = 0.005537244491279125 + 0.01 * 6.636129856109619
Epoch 760, val loss: 1.3447867631912231
Epoch 770, training loss: 0.07177507132291794 = 0.005340165458619595 + 0.01 * 6.643490791320801
Epoch 770, val loss: 1.3515020608901978
Epoch 780, training loss: 0.07150858640670776 = 0.0051548597402870655 + 0.01 * 6.635372638702393
Epoch 780, val loss: 1.3580528497695923
Epoch 790, training loss: 0.07121287286281586 = 0.0049802036955952644 + 0.01 * 6.623266696929932
Epoch 790, val loss: 1.3645048141479492
Epoch 800, training loss: 0.07110144197940826 = 0.004815343767404556 + 0.01 * 6.628609657287598
Epoch 800, val loss: 1.370749592781067
Epoch 810, training loss: 0.07083635777235031 = 0.004659776575863361 + 0.01 * 6.617657661437988
Epoch 810, val loss: 1.3769021034240723
Epoch 820, training loss: 0.07089993357658386 = 0.004512582905590534 + 0.01 * 6.638734817504883
Epoch 820, val loss: 1.3828786611557007
Epoch 830, training loss: 0.07059723138809204 = 0.004373578820377588 + 0.01 * 6.6223649978637695
Epoch 830, val loss: 1.3886798620224
Epoch 840, training loss: 0.07039729505777359 = 0.004241802729666233 + 0.01 * 6.615549564361572
Epoch 840, val loss: 1.3944170475006104
Epoch 850, training loss: 0.07021375000476837 = 0.004116755444556475 + 0.01 * 6.609699249267578
Epoch 850, val loss: 1.4000314474105835
Epoch 860, training loss: 0.07020413130521774 = 0.0039979456923902035 + 0.01 * 6.62061882019043
Epoch 860, val loss: 1.4055273532867432
Epoch 870, training loss: 0.0699596181511879 = 0.003885251237079501 + 0.01 * 6.607436656951904
Epoch 870, val loss: 1.4108089208602905
Epoch 880, training loss: 0.06987931579351425 = 0.003778113517910242 + 0.01 * 6.6101202964782715
Epoch 880, val loss: 1.416045904159546
Epoch 890, training loss: 0.0696411281824112 = 0.0036761437077075243 + 0.01 * 6.596498012542725
Epoch 890, val loss: 1.421179175376892
Epoch 900, training loss: 0.06955701112747192 = 0.0035789906978607178 + 0.01 * 6.59780216217041
Epoch 900, val loss: 1.4261953830718994
Epoch 910, training loss: 0.0694316029548645 = 0.003486355533823371 + 0.01 * 6.59452486038208
Epoch 910, val loss: 1.4310535192489624
Epoch 920, training loss: 0.06934203207492828 = 0.003397993976250291 + 0.01 * 6.594404220581055
Epoch 920, val loss: 1.4358553886413574
Epoch 930, training loss: 0.06917091459035873 = 0.0033136431593447924 + 0.01 * 6.585727214813232
Epoch 930, val loss: 1.4405584335327148
Epoch 940, training loss: 0.06914644688367844 = 0.00323301088064909 + 0.01 * 6.591344356536865
Epoch 940, val loss: 1.445182204246521
Epoch 950, training loss: 0.06903956830501556 = 0.003155970247462392 + 0.01 * 6.588359832763672
Epoch 950, val loss: 1.4497473239898682
Epoch 960, training loss: 0.06882067769765854 = 0.003082236275076866 + 0.01 * 6.5738444328308105
Epoch 960, val loss: 1.4541584253311157
Epoch 970, training loss: 0.06884979456663132 = 0.0030116583220660686 + 0.01 * 6.583813667297363
Epoch 970, val loss: 1.4584678411483765
Epoch 980, training loss: 0.06881426274776459 = 0.0029440210200846195 + 0.01 * 6.587024211883545
Epoch 980, val loss: 1.462743878364563
Epoch 990, training loss: 0.06857404857873917 = 0.002879363251850009 + 0.01 * 6.5694684982299805
Epoch 990, val loss: 1.4667953252792358
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.5867
Flip ASR: 0.5156/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.028122663497925 = 1.9443844556808472 + 0.01 * 8.37382698059082
Epoch 0, val loss: 1.9418975114822388
Epoch 10, training loss: 2.0182230472564697 = 1.9344860315322876 + 0.01 * 8.373705863952637
Epoch 10, val loss: 1.9325968027114868
Epoch 20, training loss: 2.0058369636535645 = 1.922103762626648 + 0.01 * 8.373311042785645
Epoch 20, val loss: 1.9206866025924683
Epoch 30, training loss: 1.9882057905197144 = 1.904487133026123 + 0.01 * 8.371867179870605
Epoch 30, val loss: 1.9034935235977173
Epoch 40, training loss: 1.9617606401443481 = 1.8781335353851318 + 0.01 * 8.362714767456055
Epoch 40, val loss: 1.8779685497283936
Epoch 50, training loss: 1.9238847494125366 = 1.8407975435256958 + 0.01 * 8.308720588684082
Epoch 50, val loss: 1.843423843383789
Epoch 60, training loss: 1.8778618574142456 = 1.7973923683166504 + 0.01 * 8.046953201293945
Epoch 60, val loss: 1.806557297706604
Epoch 70, training loss: 1.8346368074417114 = 1.756007432937622 + 0.01 * 7.862938404083252
Epoch 70, val loss: 1.772337794303894
Epoch 80, training loss: 1.7806017398834229 = 1.7044944763183594 + 0.01 * 7.610722064971924
Epoch 80, val loss: 1.7260698080062866
Epoch 90, training loss: 1.7068843841552734 = 1.6334407329559326 + 0.01 * 7.344367027282715
Epoch 90, val loss: 1.6644188165664673
Epoch 100, training loss: 1.6142181158065796 = 1.5426005125045776 + 0.01 * 7.1617655754089355
Epoch 100, val loss: 1.5886873006820679
Epoch 110, training loss: 1.5149322748184204 = 1.4437721967697144 + 0.01 * 7.1160101890563965
Epoch 110, val loss: 1.509069800376892
Epoch 120, training loss: 1.4176946878433228 = 1.3469150066375732 + 0.01 * 7.077969551086426
Epoch 120, val loss: 1.4341588020324707
Epoch 130, training loss: 1.3240092992782593 = 1.2534877061843872 + 0.01 * 7.052154541015625
Epoch 130, val loss: 1.364066481590271
Epoch 140, training loss: 1.2329386472702026 = 1.162582278251648 + 0.01 * 7.035641193389893
Epoch 140, val loss: 1.2973705530166626
Epoch 150, training loss: 1.1452680826187134 = 1.0750294923782349 + 0.01 * 7.023855686187744
Epoch 150, val loss: 1.2338173389434814
Epoch 160, training loss: 1.062549114227295 = 0.9924275875091553 + 0.01 * 7.012158393859863
Epoch 160, val loss: 1.174063801765442
Epoch 170, training loss: 0.9851744771003723 = 0.9152118563652039 + 0.01 * 6.996263027191162
Epoch 170, val loss: 1.118444561958313
Epoch 180, training loss: 0.9115930199623108 = 0.8418571949005127 + 0.01 * 6.973582744598389
Epoch 180, val loss: 1.0656554698944092
Epoch 190, training loss: 0.8396459221839905 = 0.7702070474624634 + 0.01 * 6.943887233734131
Epoch 190, val loss: 1.0135440826416016
Epoch 200, training loss: 0.7680144906044006 = 0.6988410353660583 + 0.01 * 6.917343616485596
Epoch 200, val loss: 0.9613229036331177
Epoch 210, training loss: 0.6973176598548889 = 0.6283342838287354 + 0.01 * 6.8983354568481445
Epoch 210, val loss: 0.9095491170883179
Epoch 220, training loss: 0.6292406320571899 = 0.5604286789894104 + 0.01 * 6.881194591522217
Epoch 220, val loss: 0.8614204525947571
Epoch 230, training loss: 0.5657855868339539 = 0.4970943033695221 + 0.01 * 6.869131088256836
Epoch 230, val loss: 0.8198440074920654
Epoch 240, training loss: 0.507758378982544 = 0.43924248218536377 + 0.01 * 6.851589202880859
Epoch 240, val loss: 0.7866312265396118
Epoch 250, training loss: 0.45506811141967773 = 0.3866848647594452 + 0.01 * 6.838326454162598
Epoch 250, val loss: 0.7613607048988342
Epoch 260, training loss: 0.40702685713768005 = 0.33874890208244324 + 0.01 * 6.827795505523682
Epoch 260, val loss: 0.7417869567871094
Epoch 270, training loss: 0.3630733788013458 = 0.29489949345588684 + 0.01 * 6.817389488220215
Epoch 270, val loss: 0.7258960008621216
Epoch 280, training loss: 0.32299959659576416 = 0.254927396774292 + 0.01 * 6.8072190284729
Epoch 280, val loss: 0.713139533996582
Epoch 290, training loss: 0.2870834171772003 = 0.2190946489572525 + 0.01 * 6.798876762390137
Epoch 290, val loss: 0.7040508985519409
Epoch 300, training loss: 0.2556295394897461 = 0.1876782774925232 + 0.01 * 6.7951250076293945
Epoch 300, val loss: 0.6990987658500671
Epoch 310, training loss: 0.22855940461158752 = 0.1606718748807907 + 0.01 * 6.788753509521484
Epoch 310, val loss: 0.6984238028526306
Epoch 320, training loss: 0.20561465620994568 = 0.1377650499343872 + 0.01 * 6.784961223602295
Epoch 320, val loss: 0.7018688321113586
Epoch 330, training loss: 0.18630091845989227 = 0.11848127096891403 + 0.01 * 6.7819647789001465
Epoch 330, val loss: 0.7086988091468811
Epoch 340, training loss: 0.17006835341453552 = 0.10229542851448059 + 0.01 * 6.7772932052612305
Epoch 340, val loss: 0.718244731426239
Epoch 350, training loss: 0.15652284026145935 = 0.08872029930353165 + 0.01 * 6.7802534103393555
Epoch 350, val loss: 0.7296720743179321
Epoch 360, training loss: 0.14497880637645721 = 0.07733719050884247 + 0.01 * 6.764162063598633
Epoch 360, val loss: 0.74248868227005
Epoch 370, training loss: 0.1354253590106964 = 0.06777302920818329 + 0.01 * 6.765233516693115
Epoch 370, val loss: 0.7561488151550293
Epoch 380, training loss: 0.1272837519645691 = 0.059728678315877914 + 0.01 * 6.755507469177246
Epoch 380, val loss: 0.7702846527099609
Epoch 390, training loss: 0.12044958025217056 = 0.052942439913749695 + 0.01 * 6.75071382522583
Epoch 390, val loss: 0.7846558690071106
Epoch 400, training loss: 0.11467011272907257 = 0.047198336571455 + 0.01 * 6.747177600860596
Epoch 400, val loss: 0.7991403341293335
Epoch 410, training loss: 0.109698586165905 = 0.0423121377825737 + 0.01 * 6.738645076751709
Epoch 410, val loss: 0.8135464787483215
Epoch 420, training loss: 0.10549620538949966 = 0.03812733292579651 + 0.01 * 6.736887454986572
Epoch 420, val loss: 0.8278422951698303
Epoch 430, training loss: 0.10183887183666229 = 0.03452392667531967 + 0.01 * 6.731494426727295
Epoch 430, val loss: 0.8418956995010376
Epoch 440, training loss: 0.09860578179359436 = 0.03140424191951752 + 0.01 * 6.720154285430908
Epoch 440, val loss: 0.8556992411613464
Epoch 450, training loss: 0.09585929661989212 = 0.02868730016052723 + 0.01 * 6.71720027923584
Epoch 450, val loss: 0.8692682385444641
Epoch 460, training loss: 0.09342078119516373 = 0.026311859488487244 + 0.01 * 6.710892200469971
Epoch 460, val loss: 0.8824842572212219
Epoch 470, training loss: 0.09121859818696976 = 0.024223389104008675 + 0.01 * 6.699521064758301
Epoch 470, val loss: 0.8954128623008728
Epoch 480, training loss: 0.08955080062150955 = 0.022376827895641327 + 0.01 * 6.717397689819336
Epoch 480, val loss: 0.9079410433769226
Epoch 490, training loss: 0.08773744851350784 = 0.020739229395985603 + 0.01 * 6.699821949005127
Epoch 490, val loss: 0.9201623797416687
Epoch 500, training loss: 0.08622682094573975 = 0.019278842955827713 + 0.01 * 6.694798469543457
Epoch 500, val loss: 0.9320048093795776
Epoch 510, training loss: 0.0847679078578949 = 0.017972715198993683 + 0.01 * 6.6795196533203125
Epoch 510, val loss: 0.9434704184532166
Epoch 520, training loss: 0.08370253443717957 = 0.016799213364720345 + 0.01 * 6.690332412719727
Epoch 520, val loss: 0.9546874165534973
Epoch 530, training loss: 0.08245223760604858 = 0.015741290524601936 + 0.01 * 6.671095371246338
Epoch 530, val loss: 0.96552574634552
Epoch 540, training loss: 0.08147203922271729 = 0.014784124679863453 + 0.01 * 6.6687912940979
Epoch 540, val loss: 0.9760482311248779
Epoch 550, training loss: 0.0804944857954979 = 0.01391372550278902 + 0.01 * 6.65807580947876
Epoch 550, val loss: 0.9863297939300537
Epoch 560, training loss: 0.07965319603681564 = 0.013120061717927456 + 0.01 * 6.653313159942627
Epoch 560, val loss: 0.9963560700416565
Epoch 570, training loss: 0.0788174718618393 = 0.012394849210977554 + 0.01 * 6.642262935638428
Epoch 570, val loss: 1.0060771703720093
Epoch 580, training loss: 0.07807622849941254 = 0.011731253005564213 + 0.01 * 6.63449764251709
Epoch 580, val loss: 1.0154500007629395
Epoch 590, training loss: 0.07742714881896973 = 0.011122941970825195 + 0.01 * 6.630421161651611
Epoch 590, val loss: 1.024683952331543
Epoch 600, training loss: 0.07691988348960876 = 0.01056241150945425 + 0.01 * 6.63574743270874
Epoch 600, val loss: 1.0335652828216553
Epoch 610, training loss: 0.07637560367584229 = 0.010045395232737064 + 0.01 * 6.633021354675293
Epoch 610, val loss: 1.0423016548156738
Epoch 620, training loss: 0.07583645731210709 = 0.009567463770508766 + 0.01 * 6.626899719238281
Epoch 620, val loss: 1.0506601333618164
Epoch 630, training loss: 0.07522588968276978 = 0.009125725366175175 + 0.01 * 6.610016822814941
Epoch 630, val loss: 1.0589479207992554
Epoch 640, training loss: 0.07476310431957245 = 0.008715163916349411 + 0.01 * 6.604794502258301
Epoch 640, val loss: 1.0668997764587402
Epoch 650, training loss: 0.0743207186460495 = 0.008333844132721424 + 0.01 * 6.598687648773193
Epoch 650, val loss: 1.074647307395935
Epoch 660, training loss: 0.07381313294172287 = 0.007978738285601139 + 0.01 * 6.583439826965332
Epoch 660, val loss: 1.0823023319244385
Epoch 670, training loss: 0.07353401184082031 = 0.007647555321455002 + 0.01 * 6.588645935058594
Epoch 670, val loss: 1.089633822441101
Epoch 680, training loss: 0.07309959828853607 = 0.007338325493037701 + 0.01 * 6.576128005981445
Epoch 680, val loss: 1.0969090461730957
Epoch 690, training loss: 0.07274001091718674 = 0.007048678118735552 + 0.01 * 6.5691328048706055
Epoch 690, val loss: 1.1039199829101562
Epoch 700, training loss: 0.07252674549818039 = 0.0067770034074783325 + 0.01 * 6.574974536895752
Epoch 700, val loss: 1.110813021659851
Epoch 710, training loss: 0.07220742851495743 = 0.006522339768707752 + 0.01 * 6.568509101867676
Epoch 710, val loss: 1.1174228191375732
Epoch 720, training loss: 0.07192747294902802 = 0.006283450406044722 + 0.01 * 6.564403057098389
Epoch 720, val loss: 1.1239879131317139
Epoch 730, training loss: 0.07171864807605743 = 0.006058563943952322 + 0.01 * 6.5660080909729
Epoch 730, val loss: 1.130287766456604
Epoch 740, training loss: 0.07133680582046509 = 0.005846699234098196 + 0.01 * 6.549010753631592
Epoch 740, val loss: 1.1365761756896973
Epoch 750, training loss: 0.07108136266469955 = 0.005646416451781988 + 0.01 * 6.543494701385498
Epoch 750, val loss: 1.1426143646240234
Epoch 760, training loss: 0.0710558071732521 = 0.005457253661006689 + 0.01 * 6.5598554611206055
Epoch 760, val loss: 1.1486103534698486
Epoch 770, training loss: 0.07064682990312576 = 0.005279107950627804 + 0.01 * 6.53677225112915
Epoch 770, val loss: 1.1543151140213013
Epoch 780, training loss: 0.07049981504678726 = 0.005110181402415037 + 0.01 * 6.538963317871094
Epoch 780, val loss: 1.1600422859191895
Epoch 790, training loss: 0.07042865455150604 = 0.004950063768774271 + 0.01 * 6.5478596687316895
Epoch 790, val loss: 1.1656134128570557
Epoch 800, training loss: 0.07015203684568405 = 0.004798402078449726 + 0.01 * 6.53536319732666
Epoch 800, val loss: 1.171019434928894
Epoch 810, training loss: 0.0700848326086998 = 0.004654063377529383 + 0.01 * 6.543076992034912
Epoch 810, val loss: 1.1763060092926025
Epoch 820, training loss: 0.06983010470867157 = 0.004517503082752228 + 0.01 * 6.5312604904174805
Epoch 820, val loss: 1.1815221309661865
Epoch 830, training loss: 0.06957557797431946 = 0.004387270659208298 + 0.01 * 6.5188307762146
Epoch 830, val loss: 1.1866031885147095
Epoch 840, training loss: 0.06957734376192093 = 0.00426310021430254 + 0.01 * 6.531424522399902
Epoch 840, val loss: 1.1915969848632812
Epoch 850, training loss: 0.06941655278205872 = 0.004145163111388683 + 0.01 * 6.527139663696289
Epoch 850, val loss: 1.1965135335922241
Epoch 860, training loss: 0.06922372430562973 = 0.004032773897051811 + 0.01 * 6.519094944000244
Epoch 860, val loss: 1.2012358903884888
Epoch 870, training loss: 0.06919321417808533 = 0.003925357945263386 + 0.01 * 6.526785850524902
Epoch 870, val loss: 1.2059111595153809
Epoch 880, training loss: 0.06895146518945694 = 0.0038235716056078672 + 0.01 * 6.512789726257324
Epoch 880, val loss: 1.2105164527893066
Epoch 890, training loss: 0.06881318986415863 = 0.003725775983184576 + 0.01 * 6.50874137878418
Epoch 890, val loss: 1.2149934768676758
Epoch 900, training loss: 0.06893657147884369 = 0.003632455598562956 + 0.01 * 6.530411720275879
Epoch 900, val loss: 1.2193562984466553
Epoch 910, training loss: 0.06853945553302765 = 0.003543173661455512 + 0.01 * 6.499628067016602
Epoch 910, val loss: 1.22366201877594
Epoch 920, training loss: 0.06837354600429535 = 0.003457943443208933 + 0.01 * 6.491560935974121
Epoch 920, val loss: 1.2278525829315186
Epoch 930, training loss: 0.06835486739873886 = 0.003375878557562828 + 0.01 * 6.497899055480957
Epoch 930, val loss: 1.2319194078445435
Epoch 940, training loss: 0.06822945177555084 = 0.0032972302287817 + 0.01 * 6.493222236633301
Epoch 940, val loss: 1.236015796661377
Epoch 950, training loss: 0.06819933652877808 = 0.003222052939236164 + 0.01 * 6.497727870941162
Epoch 950, val loss: 1.2399499416351318
Epoch 960, training loss: 0.06793291121721268 = 0.003149909432977438 + 0.01 * 6.478300094604492
Epoch 960, val loss: 1.2438653707504272
Epoch 970, training loss: 0.06801161915063858 = 0.0030809277668595314 + 0.01 * 6.493069171905518
Epoch 970, val loss: 1.2477059364318848
Epoch 980, training loss: 0.06774628162384033 = 0.003014621092006564 + 0.01 * 6.473165988922119
Epoch 980, val loss: 1.2513720989227295
Epoch 990, training loss: 0.06782882660627365 = 0.0029509158339351416 + 0.01 * 6.487791538238525
Epoch 990, val loss: 1.2549582719802856
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9299
Flip ASR: 0.9156/225 nodes
The final ASR:0.70972, 0.15604, Accuracy:0.82716, 0.01429
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11622])
remove edge: torch.Size([2, 9492])
updated graph: torch.Size([2, 10558])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.9557
Flip ASR: 0.9467/225 nodes
The final ASR:0.97048, 0.01086, Accuracy:0.83580, 0.00972
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.022688627243042 = 1.9389498233795166 + 0.01 * 8.373883247375488
Epoch 0, val loss: 1.9362603425979614
Epoch 10, training loss: 2.0133273601531982 = 1.9295891523361206 + 0.01 * 8.373812675476074
Epoch 10, val loss: 1.9273018836975098
Epoch 20, training loss: 2.001434803009033 = 1.917698860168457 + 0.01 * 8.37358570098877
Epoch 20, val loss: 1.9156396389007568
Epoch 30, training loss: 1.9842509031295776 = 1.9005221128463745 + 0.01 * 8.372876167297363
Epoch 30, val loss: 1.8985434770584106
Epoch 40, training loss: 1.9582791328430176 = 1.874588131904602 + 0.01 * 8.369097709655762
Epoch 40, val loss: 1.8730549812316895
Epoch 50, training loss: 1.9212701320648193 = 1.8378349542617798 + 0.01 * 8.343523025512695
Epoch 50, val loss: 1.8389555215835571
Epoch 60, training loss: 1.8782659769058228 = 1.7963272333145142 + 0.01 * 8.193879127502441
Epoch 60, val loss: 1.8049001693725586
Epoch 70, training loss: 1.8372918367385864 = 1.7575958967208862 + 0.01 * 7.969597816467285
Epoch 70, val loss: 1.7749072313308716
Epoch 80, training loss: 1.7830100059509277 = 1.7058370113372803 + 0.01 * 7.71729850769043
Epoch 80, val loss: 1.7299925088882446
Epoch 90, training loss: 1.7086563110351562 = 1.6341031789779663 + 0.01 * 7.455308437347412
Epoch 90, val loss: 1.6690946817398071
Epoch 100, training loss: 1.615031361579895 = 1.5414191484451294 + 0.01 * 7.3612260818481445
Epoch 100, val loss: 1.5933550596237183
Epoch 110, training loss: 1.510390281677246 = 1.437135934829712 + 0.01 * 7.325436592102051
Epoch 110, val loss: 1.5086135864257812
Epoch 120, training loss: 1.403860330581665 = 1.3311116695404053 + 0.01 * 7.274872303009033
Epoch 120, val loss: 1.425204873085022
Epoch 130, training loss: 1.2986029386520386 = 1.2265119552612305 + 0.01 * 7.209096908569336
Epoch 130, val loss: 1.343084692955017
Epoch 140, training loss: 1.196702241897583 = 1.1253061294555664 + 0.01 * 7.139615058898926
Epoch 140, val loss: 1.2649459838867188
Epoch 150, training loss: 1.1017706394195557 = 1.0309927463531494 + 0.01 * 7.077783584594727
Epoch 150, val loss: 1.193967342376709
Epoch 160, training loss: 1.0155826807022095 = 0.9451367855072021 + 0.01 * 7.044588565826416
Epoch 160, val loss: 1.1314183473587036
Epoch 170, training loss: 0.9368718266487122 = 0.8666123151779175 + 0.01 * 7.025951862335205
Epoch 170, val loss: 1.0762617588043213
Epoch 180, training loss: 0.8642778992652893 = 0.7941647171974182 + 0.01 * 7.011318683624268
Epoch 180, val loss: 1.0271803140640259
Epoch 190, training loss: 0.7979040741920471 = 0.7279238104820251 + 0.01 * 6.998027801513672
Epoch 190, val loss: 0.9844660758972168
Epoch 200, training loss: 0.7382209300994873 = 0.6683876514434814 + 0.01 * 6.983327388763428
Epoch 200, val loss: 0.9490753412246704
Epoch 210, training loss: 0.684707522392273 = 0.6150429248809814 + 0.01 * 6.966461181640625
Epoch 210, val loss: 0.9206765294075012
Epoch 220, training loss: 0.635797381401062 = 0.5662671327590942 + 0.01 * 6.953028202056885
Epoch 220, val loss: 0.8977795243263245
Epoch 230, training loss: 0.5895141363143921 = 0.5201529264450073 + 0.01 * 6.93612003326416
Epoch 230, val loss: 0.8787145018577576
Epoch 240, training loss: 0.5446804761886597 = 0.47535181045532227 + 0.01 * 6.932868480682373
Epoch 240, val loss: 0.8620084524154663
Epoch 250, training loss: 0.500707745552063 = 0.4316138029098511 + 0.01 * 6.909391403198242
Epoch 250, val loss: 0.8470669388771057
Epoch 260, training loss: 0.45846593379974365 = 0.38950884342193604 + 0.01 * 6.895710468292236
Epoch 260, val loss: 0.8342204689979553
Epoch 270, training loss: 0.41876688599586487 = 0.35000529885292053 + 0.01 * 6.876159191131592
Epoch 270, val loss: 0.8242932558059692
Epoch 280, training loss: 0.38248181343078613 = 0.31370651721954346 + 0.01 * 6.877529144287109
Epoch 280, val loss: 0.8184174299240112
Epoch 290, training loss: 0.34915632009506226 = 0.28059715032577515 + 0.01 * 6.855917453765869
Epoch 290, val loss: 0.8168317675590515
Epoch 300, training loss: 0.3186737596988678 = 0.25022411346435547 + 0.01 * 6.84496545791626
Epoch 300, val loss: 0.8190035820007324
Epoch 310, training loss: 0.2905667722225189 = 0.22211319208145142 + 0.01 * 6.845357894897461
Epoch 310, val loss: 0.824263870716095
Epoch 320, training loss: 0.26440516114234924 = 0.19606223702430725 + 0.01 * 6.834292411804199
Epoch 320, val loss: 0.8316489458084106
Epoch 330, training loss: 0.2405698299407959 = 0.17227645218372345 + 0.01 * 6.829339027404785
Epoch 330, val loss: 0.840703547000885
Epoch 340, training loss: 0.21946853399276733 = 0.15107320249080658 + 0.01 * 6.839532375335693
Epoch 340, val loss: 0.85135817527771
Epoch 350, training loss: 0.20079734921455383 = 0.1325586885213852 + 0.01 * 6.823866844177246
Epoch 350, val loss: 0.8633312582969666
Epoch 360, training loss: 0.18473121523857117 = 0.11657632887363434 + 0.01 * 6.815488338470459
Epoch 360, val loss: 0.8765409588813782
Epoch 370, training loss: 0.17105931043624878 = 0.1028478667140007 + 0.01 * 6.821144104003906
Epoch 370, val loss: 0.8908776640892029
Epoch 380, training loss: 0.15914024412631989 = 0.09106393903493881 + 0.01 * 6.807631015777588
Epoch 380, val loss: 0.9062038064002991
Epoch 390, training loss: 0.1488916277885437 = 0.08092022687196732 + 0.01 * 6.7971391677856445
Epoch 390, val loss: 0.9221632480621338
Epoch 400, training loss: 0.14009472727775574 = 0.07216346263885498 + 0.01 * 6.793125629425049
Epoch 400, val loss: 0.938672661781311
Epoch 410, training loss: 0.13252097368240356 = 0.06458573043346405 + 0.01 * 6.793523788452148
Epoch 410, val loss: 0.9554919004440308
Epoch 420, training loss: 0.12579558789730072 = 0.058011993765830994 + 0.01 * 6.778359413146973
Epoch 420, val loss: 0.9724898934364319
Epoch 430, training loss: 0.12007571756839752 = 0.05229553207755089 + 0.01 * 6.778018474578857
Epoch 430, val loss: 0.9893917441368103
Epoch 440, training loss: 0.11516369879245758 = 0.047305576503276825 + 0.01 * 6.7858123779296875
Epoch 440, val loss: 1.0061731338500977
Epoch 450, training loss: 0.11056786775588989 = 0.04293786361813545 + 0.01 * 6.763000965118408
Epoch 450, val loss: 1.0226640701293945
Epoch 460, training loss: 0.10686756670475006 = 0.039096858352422714 + 0.01 * 6.777071475982666
Epoch 460, val loss: 1.0387299060821533
Epoch 470, training loss: 0.10326632857322693 = 0.03571071848273277 + 0.01 * 6.755560874938965
Epoch 470, val loss: 1.0543639659881592
Epoch 480, training loss: 0.10015776008367538 = 0.03271261602640152 + 0.01 * 6.744514465332031
Epoch 480, val loss: 1.0696067810058594
Epoch 490, training loss: 0.09763358533382416 = 0.030048146843910217 + 0.01 * 6.758543968200684
Epoch 490, val loss: 1.0843560695648193
Epoch 500, training loss: 0.09506384283304214 = 0.02767564356327057 + 0.01 * 6.7388200759887695
Epoch 500, val loss: 1.0987228155136108
Epoch 510, training loss: 0.09317223727703094 = 0.025553705170750618 + 0.01 * 6.761853218078613
Epoch 510, val loss: 1.1125913858413696
Epoch 520, training loss: 0.09076625108718872 = 0.02365756966173649 + 0.01 * 6.710868835449219
Epoch 520, val loss: 1.1261399984359741
Epoch 530, training loss: 0.08954790234565735 = 0.021954994648694992 + 0.01 * 6.75929069519043
Epoch 530, val loss: 1.1391913890838623
Epoch 540, training loss: 0.08753613382577896 = 0.02042347937822342 + 0.01 * 6.711265563964844
Epoch 540, val loss: 1.1519320011138916
Epoch 550, training loss: 0.0860024243593216 = 0.01904110610485077 + 0.01 * 6.696132183074951
Epoch 550, val loss: 1.1642266511917114
Epoch 560, training loss: 0.08482637256383896 = 0.017790645360946655 + 0.01 * 6.703573226928711
Epoch 560, val loss: 1.176164150238037
Epoch 570, training loss: 0.08350548148155212 = 0.016658712178468704 + 0.01 * 6.684676647186279
Epoch 570, val loss: 1.1877996921539307
Epoch 580, training loss: 0.0824681967496872 = 0.015630075708031654 + 0.01 * 6.683812618255615
Epoch 580, val loss: 1.1990764141082764
Epoch 590, training loss: 0.08137308061122894 = 0.01469404622912407 + 0.01 * 6.667902946472168
Epoch 590, val loss: 1.2100005149841309
Epoch 600, training loss: 0.08071493357419968 = 0.013840439729392529 + 0.01 * 6.6874494552612305
Epoch 600, val loss: 1.220560908317566
Epoch 610, training loss: 0.07967612147331238 = 0.013061430305242538 + 0.01 * 6.66146993637085
Epoch 610, val loss: 1.2308745384216309
Epoch 620, training loss: 0.07882847636938095 = 0.012347804382443428 + 0.01 * 6.648067474365234
Epoch 620, val loss: 1.2408344745635986
Epoch 630, training loss: 0.0785638689994812 = 0.011692053638398647 + 0.01 * 6.68718147277832
Epoch 630, val loss: 1.2504712343215942
Epoch 640, training loss: 0.07756928354501724 = 0.011089354753494263 + 0.01 * 6.647993087768555
Epoch 640, val loss: 1.2599724531173706
Epoch 650, training loss: 0.07711505889892578 = 0.010533543303608894 + 0.01 * 6.658151626586914
Epoch 650, val loss: 1.2690558433532715
Epoch 660, training loss: 0.07646944373846054 = 0.010021115653216839 + 0.01 * 6.644833087921143
Epoch 660, val loss: 1.2779881954193115
Epoch 670, training loss: 0.07576629519462585 = 0.009547029621899128 + 0.01 * 6.621926784515381
Epoch 670, val loss: 1.2866100072860718
Epoch 680, training loss: 0.07522707432508469 = 0.009108246304094791 + 0.01 * 6.611883163452148
Epoch 680, val loss: 1.294986605644226
Epoch 690, training loss: 0.07470228523015976 = 0.008700997568666935 + 0.01 * 6.600129127502441
Epoch 690, val loss: 1.3032283782958984
Epoch 700, training loss: 0.07464500516653061 = 0.008322210982441902 + 0.01 * 6.632279872894287
Epoch 700, val loss: 1.3111382722854614
Epoch 710, training loss: 0.07408232986927032 = 0.007969493977725506 + 0.01 * 6.611283779144287
Epoch 710, val loss: 1.3189520835876465
Epoch 720, training loss: 0.07352214306592941 = 0.007640281226485968 + 0.01 * 6.588186264038086
Epoch 720, val loss: 1.3264588117599487
Epoch 730, training loss: 0.07313196361064911 = 0.007332781795412302 + 0.01 * 6.57991886138916
Epoch 730, val loss: 1.3338091373443604
Epoch 740, training loss: 0.0732012391090393 = 0.007045321632176638 + 0.01 * 6.615592002868652
Epoch 740, val loss: 1.3409870862960815
Epoch 750, training loss: 0.07241074740886688 = 0.006776054855436087 + 0.01 * 6.563468933105469
Epoch 750, val loss: 1.3479887247085571
Epoch 760, training loss: 0.0723237544298172 = 0.0065233116038143635 + 0.01 * 6.580044269561768
Epoch 760, val loss: 1.3547053337097168
Epoch 770, training loss: 0.07193964719772339 = 0.006286162417382002 + 0.01 * 6.5653486251831055
Epoch 770, val loss: 1.3614236116409302
Epoch 780, training loss: 0.07184943556785583 = 0.00606303196400404 + 0.01 * 6.578640937805176
Epoch 780, val loss: 1.3677878379821777
Epoch 790, training loss: 0.0713580995798111 = 0.00585284223780036 + 0.01 * 6.550526142120361
Epoch 790, val loss: 1.3741739988327026
Epoch 800, training loss: 0.0712970569729805 = 0.005654347129166126 + 0.01 * 6.564270496368408
Epoch 800, val loss: 1.3803021907806396
Epoch 810, training loss: 0.07080470025539398 = 0.0054673305712640285 + 0.01 * 6.5337371826171875
Epoch 810, val loss: 1.3862967491149902
Epoch 820, training loss: 0.07069726288318634 = 0.005290414672344923 + 0.01 * 6.540685653686523
Epoch 820, val loss: 1.3921606540679932
Epoch 830, training loss: 0.07053941488265991 = 0.005123000591993332 + 0.01 * 6.5416412353515625
Epoch 830, val loss: 1.3979182243347168
Epoch 840, training loss: 0.07020877301692963 = 0.004964628256857395 + 0.01 * 6.524414539337158
Epoch 840, val loss: 1.4035229682922363
Epoch 850, training loss: 0.07021987438201904 = 0.004814477171748877 + 0.01 * 6.540539741516113
Epoch 850, val loss: 1.4089508056640625
Epoch 860, training loss: 0.07006435841321945 = 0.004671980626881123 + 0.01 * 6.5392374992370605
Epoch 860, val loss: 1.4143410921096802
Epoch 870, training loss: 0.07000724971294403 = 0.004536681342869997 + 0.01 * 6.547056674957275
Epoch 870, val loss: 1.419519066810608
Epoch 880, training loss: 0.06957638263702393 = 0.004407973960042 + 0.01 * 6.516841411590576
Epoch 880, val loss: 1.4246608018875122
Epoch 890, training loss: 0.06967656314373016 = 0.004285458475351334 + 0.01 * 6.539111137390137
Epoch 890, val loss: 1.429611325263977
Epoch 900, training loss: 0.06923750787973404 = 0.0041687991470098495 + 0.01 * 6.506870746612549
Epoch 900, val loss: 1.4345306158065796
Epoch 910, training loss: 0.06911242008209229 = 0.004057614132761955 + 0.01 * 6.505480766296387
Epoch 910, val loss: 1.4392811059951782
Epoch 920, training loss: 0.0689944326877594 = 0.003951573744416237 + 0.01 * 6.504286289215088
Epoch 920, val loss: 1.4439114332199097
Epoch 930, training loss: 0.0689164325594902 = 0.003850586712360382 + 0.01 * 6.506584644317627
Epoch 930, val loss: 1.4485599994659424
Epoch 940, training loss: 0.06877736747264862 = 0.00375401065684855 + 0.01 * 6.502336025238037
Epoch 940, val loss: 1.4529314041137695
Epoch 950, training loss: 0.06872767955064774 = 0.003661736845970154 + 0.01 * 6.506594181060791
Epoch 950, val loss: 1.4573572874069214
Epoch 960, training loss: 0.06839839369058609 = 0.003573284251615405 + 0.01 * 6.482511520385742
Epoch 960, val loss: 1.4615434408187866
Epoch 970, training loss: 0.06838901340961456 = 0.0034887995570898056 + 0.01 * 6.490021705627441
Epoch 970, val loss: 1.465727686882019
Epoch 980, training loss: 0.06836102157831192 = 0.003407868091017008 + 0.01 * 6.4953155517578125
Epoch 980, val loss: 1.469887614250183
Epoch 990, training loss: 0.0680973008275032 = 0.003330117790028453 + 0.01 * 6.476718425750732
Epoch 990, val loss: 1.473849892616272
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.7889
Overall ASR: 0.4133
Flip ASR: 0.3111/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0426578521728516 = 1.9589186906814575 + 0.01 * 8.373906135559082
Epoch 0, val loss: 1.9489927291870117
Epoch 10, training loss: 2.0300776958465576 = 1.9463396072387695 + 0.01 * 8.373809814453125
Epoch 10, val loss: 1.9350255727767944
Epoch 20, training loss: 2.01479172706604 = 1.9310566186904907 + 0.01 * 8.373519897460938
Epoch 20, val loss: 1.9168764352798462
Epoch 30, training loss: 1.9941331148147583 = 1.910404920578003 + 0.01 * 8.37281608581543
Epoch 30, val loss: 1.891837477684021
Epoch 40, training loss: 1.9658821821212769 = 1.8821768760681152 + 0.01 * 8.370532035827637
Epoch 40, val loss: 1.8584067821502686
Epoch 50, training loss: 1.9286816120147705 = 1.8451045751571655 + 0.01 * 8.357707023620605
Epoch 50, val loss: 1.8173434734344482
Epoch 60, training loss: 1.8838809728622437 = 1.8009496927261353 + 0.01 * 8.293126106262207
Epoch 60, val loss: 1.7735731601715088
Epoch 70, training loss: 1.83735191822052 = 1.7572566270828247 + 0.01 * 8.009526252746582
Epoch 70, val loss: 1.7365046739578247
Epoch 80, training loss: 1.7903884649276733 = 1.7121638059616089 + 0.01 * 7.8224663734436035
Epoch 80, val loss: 1.70103919506073
Epoch 90, training loss: 1.7272777557373047 = 1.6511969566345215 + 0.01 * 7.6080780029296875
Epoch 90, val loss: 1.650602102279663
Epoch 100, training loss: 1.6437522172927856 = 1.5688518285751343 + 0.01 * 7.490036964416504
Epoch 100, val loss: 1.5813199281692505
Epoch 110, training loss: 1.5409197807312012 = 1.4669573307037354 + 0.01 * 7.396247863769531
Epoch 110, val loss: 1.4980412721633911
Epoch 120, training loss: 1.430771827697754 = 1.357464075088501 + 0.01 * 7.3307785987854
Epoch 120, val loss: 1.413891077041626
Epoch 130, training loss: 1.3229913711547852 = 1.2500896453857422 + 0.01 * 7.290174961090088
Epoch 130, val loss: 1.3372076749801636
Epoch 140, training loss: 1.2203580141067505 = 1.1477444171905518 + 0.01 * 7.261357307434082
Epoch 140, val loss: 1.2674211263656616
Epoch 150, training loss: 1.1237996816635132 = 1.0514930486679077 + 0.01 * 7.230664253234863
Epoch 150, val loss: 1.2019965648651123
Epoch 160, training loss: 1.033819317817688 = 0.9620141386985779 + 0.01 * 7.180522918701172
Epoch 160, val loss: 1.1403181552886963
Epoch 170, training loss: 0.9506796002388 = 0.8795135617256165 + 0.01 * 7.116601467132568
Epoch 170, val loss: 1.0835106372833252
Epoch 180, training loss: 0.874444842338562 = 0.8037970662117004 + 0.01 * 7.06477689743042
Epoch 180, val loss: 1.0324231386184692
Epoch 190, training loss: 0.8055499792098999 = 0.7351237535476685 + 0.01 * 7.042619705200195
Epoch 190, val loss: 0.9870480298995972
Epoch 200, training loss: 0.74425208568573 = 0.673917293548584 + 0.01 * 7.033480644226074
Epoch 200, val loss: 0.9480438828468323
Epoch 210, training loss: 0.6899208426475525 = 0.6196149587631226 + 0.01 * 7.030587673187256
Epoch 210, val loss: 0.9147292971611023
Epoch 220, training loss: 0.6412664651870728 = 0.5709810256958008 + 0.01 * 7.028542995452881
Epoch 220, val loss: 0.8870484232902527
Epoch 230, training loss: 0.5967063903808594 = 0.5264437794685364 + 0.01 * 7.0262603759765625
Epoch 230, val loss: 0.8638952374458313
Epoch 240, training loss: 0.5554226636886597 = 0.48518869280815125 + 0.01 * 7.02340030670166
Epoch 240, val loss: 0.8440557718276978
Epoch 250, training loss: 0.5169875621795654 = 0.44678646326065063 + 0.01 * 7.020112037658691
Epoch 250, val loss: 0.8269085884094238
Epoch 260, training loss: 0.48063090443611145 = 0.4104529917240143 + 0.01 * 7.017791748046875
Epoch 260, val loss: 0.8117048740386963
Epoch 270, training loss: 0.44569191336631775 = 0.3755553364753723 + 0.01 * 7.013657093048096
Epoch 270, val loss: 0.7981008887290955
Epoch 280, training loss: 0.411687970161438 = 0.34158915281295776 + 0.01 * 7.009882926940918
Epoch 280, val loss: 0.7862082123756409
Epoch 290, training loss: 0.3785569369792938 = 0.3084929883480072 + 0.01 * 7.006394863128662
Epoch 290, val loss: 0.7759659886360168
Epoch 300, training loss: 0.34631437063217163 = 0.27627262473106384 + 0.01 * 7.00417423248291
Epoch 300, val loss: 0.7679808735847473
Epoch 310, training loss: 0.31547486782073975 = 0.24547213315963745 + 0.01 * 7.000272750854492
Epoch 310, val loss: 0.7630096077919006
Epoch 320, training loss: 0.2867790460586548 = 0.2168101668357849 + 0.01 * 6.996889591217041
Epoch 320, val loss: 0.7616754770278931
Epoch 330, training loss: 0.2608780860900879 = 0.19093766808509827 + 0.01 * 6.994042873382568
Epoch 330, val loss: 0.7639849781990051
Epoch 340, training loss: 0.23816323280334473 = 0.1682480126619339 + 0.01 * 6.99152135848999
Epoch 340, val loss: 0.7694648504257202
Epoch 350, training loss: 0.21854157745838165 = 0.14865685999393463 + 0.01 * 6.988471984863281
Epoch 350, val loss: 0.7773783206939697
Epoch 360, training loss: 0.2017393410205841 = 0.1319020390510559 + 0.01 * 6.983729839324951
Epoch 360, val loss: 0.7871816754341125
Epoch 370, training loss: 0.18740275502204895 = 0.1175723671913147 + 0.01 * 6.983039855957031
Epoch 370, val loss: 0.798520565032959
Epoch 380, training loss: 0.17501728236675262 = 0.10525922477245331 + 0.01 * 6.97580623626709
Epoch 380, val loss: 0.8110440373420715
Epoch 390, training loss: 0.16426943242549896 = 0.09458024054765701 + 0.01 * 6.968919277191162
Epoch 390, val loss: 0.8244413733482361
Epoch 400, training loss: 0.154872864484787 = 0.08521129935979843 + 0.01 * 6.96615743637085
Epoch 400, val loss: 0.8384531736373901
Epoch 410, training loss: 0.14647705852985382 = 0.07690895348787308 + 0.01 * 6.95681095123291
Epoch 410, val loss: 0.8528875708580017
Epoch 420, training loss: 0.1389540135860443 = 0.06948665529489517 + 0.01 * 6.946735382080078
Epoch 420, val loss: 0.8677424788475037
Epoch 430, training loss: 0.13220897316932678 = 0.06279067695140839 + 0.01 * 6.941828727722168
Epoch 430, val loss: 0.8827046751976013
Epoch 440, training loss: 0.12605790793895721 = 0.056733980774879456 + 0.01 * 6.9323930740356445
Epoch 440, val loss: 0.8978476524353027
Epoch 450, training loss: 0.12058651447296143 = 0.05124237760901451 + 0.01 * 6.934414386749268
Epoch 450, val loss: 0.9129504561424255
Epoch 460, training loss: 0.11541183292865753 = 0.04627865552902222 + 0.01 * 6.913317680358887
Epoch 460, val loss: 0.9282699227333069
Epoch 470, training loss: 0.1107444018125534 = 0.041755273938179016 + 0.01 * 6.8989129066467285
Epoch 470, val loss: 0.9431030750274658
Epoch 480, training loss: 0.1066531240940094 = 0.037717562168836594 + 0.01 * 6.893556594848633
Epoch 480, val loss: 0.9576824903488159
Epoch 490, training loss: 0.10298016667366028 = 0.03415202721953392 + 0.01 * 6.882813453674316
Epoch 490, val loss: 0.9719356894493103
Epoch 500, training loss: 0.09985055774450302 = 0.031017879024147987 + 0.01 * 6.883268356323242
Epoch 500, val loss: 0.985806405544281
Epoch 510, training loss: 0.09688733518123627 = 0.02827063389122486 + 0.01 * 6.86167049407959
Epoch 510, val loss: 0.9993340373039246
Epoch 520, training loss: 0.09433083236217499 = 0.02584812045097351 + 0.01 * 6.848271369934082
Epoch 520, val loss: 1.0124229192733765
Epoch 530, training loss: 0.09205891191959381 = 0.023696474730968475 + 0.01 * 6.836243629455566
Epoch 530, val loss: 1.0254147052764893
Epoch 540, training loss: 0.0904061421751976 = 0.021783879026770592 + 0.01 * 6.8622260093688965
Epoch 540, val loss: 1.0379374027252197
Epoch 550, training loss: 0.08826985955238342 = 0.02008664235472679 + 0.01 * 6.81832218170166
Epoch 550, val loss: 1.0501525402069092
Epoch 560, training loss: 0.08667408674955368 = 0.018572455272078514 + 0.01 * 6.8101630210876465
Epoch 560, val loss: 1.0619502067565918
Epoch 570, training loss: 0.08517007529735565 = 0.01721678115427494 + 0.01 * 6.795330047607422
Epoch 570, val loss: 1.0735137462615967
Epoch 580, training loss: 0.08405148237943649 = 0.015998607501387596 + 0.01 * 6.8052873611450195
Epoch 580, val loss: 1.0848965644836426
Epoch 590, training loss: 0.08277644217014313 = 0.014892117120325565 + 0.01 * 6.788432598114014
Epoch 590, val loss: 1.0959270000457764
Epoch 600, training loss: 0.08154039829969406 = 0.01387872640043497 + 0.01 * 6.766167640686035
Epoch 600, val loss: 1.1066982746124268
Epoch 610, training loss: 0.08076252788305283 = 0.012967993505299091 + 0.01 * 6.779453277587891
Epoch 610, val loss: 1.1171742677688599
Epoch 620, training loss: 0.07970249652862549 = 0.012162071652710438 + 0.01 * 6.754042625427246
Epoch 620, val loss: 1.1272743940353394
Epoch 630, training loss: 0.07883323729038239 = 0.011431828141212463 + 0.01 * 6.74014139175415
Epoch 630, val loss: 1.1370588541030884
Epoch 640, training loss: 0.0781150683760643 = 0.010770426131784916 + 0.01 * 6.734464168548584
Epoch 640, val loss: 1.1466206312179565
Epoch 650, training loss: 0.07737184315919876 = 0.01016999315470457 + 0.01 * 6.720185279846191
Epoch 650, val loss: 1.1559358835220337
Epoch 660, training loss: 0.07680255174636841 = 0.009624786674976349 + 0.01 * 6.717776298522949
Epoch 660, val loss: 1.165055513381958
Epoch 670, training loss: 0.0761822760105133 = 0.009127059951424599 + 0.01 * 6.705521583557129
Epoch 670, val loss: 1.1738075017929077
Epoch 680, training loss: 0.0757305771112442 = 0.008670340292155743 + 0.01 * 6.706024169921875
Epoch 680, val loss: 1.1823053359985352
Epoch 690, training loss: 0.0752720758318901 = 0.008244780823588371 + 0.01 * 6.702729225158691
Epoch 690, val loss: 1.1909985542297363
Epoch 700, training loss: 0.07463834434747696 = 0.007846399210393429 + 0.01 * 6.679194450378418
Epoch 700, val loss: 1.1990221738815308
Epoch 710, training loss: 0.07432304322719574 = 0.0074801514856517315 + 0.01 * 6.68428897857666
Epoch 710, val loss: 1.2066824436187744
Epoch 720, training loss: 0.073825404047966 = 0.00714216148480773 + 0.01 * 6.6683244705200195
Epoch 720, val loss: 1.2143819332122803
Epoch 730, training loss: 0.0734679102897644 = 0.0068287961184978485 + 0.01 * 6.663911819458008
Epoch 730, val loss: 1.2219820022583008
Epoch 740, training loss: 0.0730612650513649 = 0.006537700537592173 + 0.01 * 6.6523566246032715
Epoch 740, val loss: 1.2291228771209717
Epoch 750, training loss: 0.07285905629396439 = 0.00626660231500864 + 0.01 * 6.659245491027832
Epoch 750, val loss: 1.2362799644470215
Epoch 760, training loss: 0.07251990586519241 = 0.006013964302837849 + 0.01 * 6.6505937576293945
Epoch 760, val loss: 1.2429726123809814
Epoch 770, training loss: 0.07215924561023712 = 0.005777773912996054 + 0.01 * 6.638147354125977
Epoch 770, val loss: 1.249771237373352
Epoch 780, training loss: 0.07200983166694641 = 0.005557245574891567 + 0.01 * 6.645258903503418
Epoch 780, val loss: 1.2561533451080322
Epoch 790, training loss: 0.07166033238172531 = 0.005350780673325062 + 0.01 * 6.630954742431641
Epoch 790, val loss: 1.2625153064727783
Epoch 800, training loss: 0.07144196331501007 = 0.005157263018190861 + 0.01 * 6.628470420837402
Epoch 800, val loss: 1.2687214612960815
Epoch 810, training loss: 0.07120309770107269 = 0.004975549876689911 + 0.01 * 6.62275505065918
Epoch 810, val loss: 1.2747043371200562
Epoch 820, training loss: 0.07104994356632233 = 0.004786572884768248 + 0.01 * 6.62633752822876
Epoch 820, val loss: 1.2809007167816162
Epoch 830, training loss: 0.0707189217209816 = 0.004622441716492176 + 0.01 * 6.60964822769165
Epoch 830, val loss: 1.2860685586929321
Epoch 840, training loss: 0.070595882833004 = 0.004470274783670902 + 0.01 * 6.612560749053955
Epoch 840, val loss: 1.291780710220337
Epoch 850, training loss: 0.07049648463726044 = 0.004326715599745512 + 0.01 * 6.616977214813232
Epoch 850, val loss: 1.2974202632904053
Epoch 860, training loss: 0.07014121860265732 = 0.004191266372799873 + 0.01 * 6.594995498657227
Epoch 860, val loss: 1.3028507232666016
Epoch 870, training loss: 0.07019629329442978 = 0.004063134081661701 + 0.01 * 6.613316059112549
Epoch 870, val loss: 1.3080281019210815
Epoch 880, training loss: 0.06984218209981918 = 0.003941934090107679 + 0.01 * 6.590024948120117
Epoch 880, val loss: 1.3132398128509521
Epoch 890, training loss: 0.06974014639854431 = 0.003827113891020417 + 0.01 * 6.591302871704102
Epoch 890, val loss: 1.3181648254394531
Epoch 900, training loss: 0.06953047960996628 = 0.0037178879138082266 + 0.01 * 6.581259727478027
Epoch 900, val loss: 1.323112964630127
Epoch 910, training loss: 0.06942040473222733 = 0.003614156972616911 + 0.01 * 6.580624580383301
Epoch 910, val loss: 1.3279317617416382
Epoch 920, training loss: 0.06930965930223465 = 0.0035153632052242756 + 0.01 * 6.579430103302002
Epoch 920, val loss: 1.3326588869094849
Epoch 930, training loss: 0.06906180828809738 = 0.0034216330386698246 + 0.01 * 6.5640177726745605
Epoch 930, val loss: 1.3371789455413818
Epoch 940, training loss: 0.06896110624074936 = 0.003331982996314764 + 0.01 * 6.562912940979004
Epoch 940, val loss: 1.341614007949829
Epoch 950, training loss: 0.06894682347774506 = 0.0032463122624903917 + 0.01 * 6.5700507164001465
Epoch 950, val loss: 1.346089243888855
Epoch 960, training loss: 0.06877323985099792 = 0.003164777997881174 + 0.01 * 6.56084680557251
Epoch 960, val loss: 1.3502925634384155
Epoch 970, training loss: 0.06865277141332626 = 0.003087051911279559 + 0.01 * 6.556571960449219
Epoch 970, val loss: 1.3545228242874146
Epoch 980, training loss: 0.06855026632547379 = 0.0030124783515930176 + 0.01 * 6.553778648376465
Epoch 980, val loss: 1.358599066734314
Epoch 990, training loss: 0.06844993680715561 = 0.0029411697760224342 + 0.01 * 6.550876617431641
Epoch 990, val loss: 1.362634301185608
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.5314
Flip ASR: 0.4756/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0176548957824707 = 1.9339157342910767 + 0.01 * 8.37390422821045
Epoch 0, val loss: 1.9295732975006104
Epoch 10, training loss: 2.0080220699310303 = 1.924283504486084 + 0.01 * 8.373849868774414
Epoch 10, val loss: 1.9197016954421997
Epoch 20, training loss: 1.9961297512054443 = 1.9123930931091309 + 0.01 * 8.373668670654297
Epoch 20, val loss: 1.9072835445404053
Epoch 30, training loss: 1.9793556928634644 = 1.895624041557312 + 0.01 * 8.373165130615234
Epoch 30, val loss: 1.8897987604141235
Epoch 40, training loss: 1.9544837474822998 = 1.8707740306854248 + 0.01 * 8.37096881866455
Epoch 40, val loss: 1.8644376993179321
Epoch 50, training loss: 1.9195951223373413 = 1.8360261917114258 + 0.01 * 8.356894493103027
Epoch 50, val loss: 1.830945611000061
Epoch 60, training loss: 1.8791357278823853 = 1.796291470527649 + 0.01 * 8.284431457519531
Epoch 60, val loss: 1.7967151403427124
Epoch 70, training loss: 1.8391207456588745 = 1.758603811264038 + 0.01 * 8.051694869995117
Epoch 70, val loss: 1.7672678232192993
Epoch 80, training loss: 1.7888044118881226 = 1.710540533065796 + 0.01 * 7.826385498046875
Epoch 80, val loss: 1.7272093296051025
Epoch 90, training loss: 1.718696117401123 = 1.6440532207489014 + 0.01 * 7.464291095733643
Epoch 90, val loss: 1.6715734004974365
Epoch 100, training loss: 1.6285789012908936 = 1.5563054084777832 + 0.01 * 7.227344989776611
Epoch 100, val loss: 1.599541425704956
Epoch 110, training loss: 1.5248467922210693 = 1.4533835649490356 + 0.01 * 7.146320819854736
Epoch 110, val loss: 1.5164214372634888
Epoch 120, training loss: 1.4158517122268677 = 1.3447562456130981 + 0.01 * 7.109546184539795
Epoch 120, val loss: 1.4318283796310425
Epoch 130, training loss: 1.3060790300369263 = 1.2352325916290283 + 0.01 * 7.084648132324219
Epoch 130, val loss: 1.349161148071289
Epoch 140, training loss: 1.1972638368606567 = 1.1265828609466553 + 0.01 * 7.0680975914001465
Epoch 140, val loss: 1.2678112983703613
Epoch 150, training loss: 1.0923031568527222 = 1.0217982530593872 + 0.01 * 7.0504865646362305
Epoch 150, val loss: 1.1900995969772339
Epoch 160, training loss: 0.9939989447593689 = 0.9237077832221985 + 0.01 * 7.029118061065674
Epoch 160, val loss: 1.116805076599121
Epoch 170, training loss: 0.9034382104873657 = 0.8333578109741211 + 0.01 * 7.008042812347412
Epoch 170, val loss: 1.0487085580825806
Epoch 180, training loss: 0.8207666873931885 = 0.7508082389831543 + 0.01 * 6.995842933654785
Epoch 180, val loss: 0.9860377311706543
Epoch 190, training loss: 0.7458038330078125 = 0.6759210228919983 + 0.01 * 6.988282680511475
Epoch 190, val loss: 0.9296175837516785
Epoch 200, training loss: 0.6778818368911743 = 0.6080223917961121 + 0.01 * 6.985947132110596
Epoch 200, val loss: 0.8802599310874939
Epoch 210, training loss: 0.6155005097389221 = 0.5456511378288269 + 0.01 * 6.984938144683838
Epoch 210, val loss: 0.8379741907119751
Epoch 220, training loss: 0.5570391416549683 = 0.4871952533721924 + 0.01 * 6.984390735626221
Epoch 220, val loss: 0.8018227815628052
Epoch 230, training loss: 0.5016061067581177 = 0.43176159262657166 + 0.01 * 6.984451770782471
Epoch 230, val loss: 0.7707960605621338
Epoch 240, training loss: 0.44902464747428894 = 0.37917467951774597 + 0.01 * 6.984997272491455
Epoch 240, val loss: 0.7442929744720459
Epoch 250, training loss: 0.3997175395488739 = 0.32986024022102356 + 0.01 * 6.985729694366455
Epoch 250, val loss: 0.7226819396018982
Epoch 260, training loss: 0.3544054925441742 = 0.2845402657985687 + 0.01 * 6.9865217208862305
Epoch 260, val loss: 0.7065277695655823
Epoch 270, training loss: 0.3137568235397339 = 0.24388357996940613 + 0.01 * 6.987323760986328
Epoch 270, val loss: 0.6960643529891968
Epoch 280, training loss: 0.2781849503517151 = 0.2083035111427307 + 0.01 * 6.988144397735596
Epoch 280, val loss: 0.6910264492034912
Epoch 290, training loss: 0.24773651361465454 = 0.17784707248210907 + 0.01 * 6.988943576812744
Epoch 290, val loss: 0.6909888386726379
Epoch 300, training loss: 0.22209122776985168 = 0.1521940380334854 + 0.01 * 6.989718437194824
Epoch 300, val loss: 0.6951817870140076
Epoch 310, training loss: 0.20070651173591614 = 0.13080209493637085 + 0.01 * 6.990440845489502
Epoch 310, val loss: 0.7028061747550964
Epoch 320, training loss: 0.18294291198253632 = 0.11303148418664932 + 0.01 * 6.991142749786377
Epoch 320, val loss: 0.7130621671676636
Epoch 330, training loss: 0.16817083954811096 = 0.09825267642736435 + 0.01 * 6.99181604385376
Epoch 330, val loss: 0.7252700328826904
Epoch 340, training loss: 0.15582281351089478 = 0.0859009176492691 + 0.01 * 6.992190837860107
Epoch 340, val loss: 0.7389408946037292
Epoch 350, training loss: 0.14545005559921265 = 0.07552433013916016 + 0.01 * 6.9925737380981445
Epoch 350, val loss: 0.7536071538925171
Epoch 360, training loss: 0.1366744190454483 = 0.0667460635304451 + 0.01 * 6.992835521697998
Epoch 360, val loss: 0.7689606547355652
Epoch 370, training loss: 0.12921161949634552 = 0.05928708240389824 + 0.01 * 6.9924540519714355
Epoch 370, val loss: 0.7846384644508362
Epoch 380, training loss: 0.12284406274557114 = 0.05292023718357086 + 0.01 * 6.992383003234863
Epoch 380, val loss: 0.8005660176277161
Epoch 390, training loss: 0.11737910658121109 = 0.04745960235595703 + 0.01 * 6.991950511932373
Epoch 390, val loss: 0.816550612449646
Epoch 400, training loss: 0.11265702545642853 = 0.042754750698804855 + 0.01 * 6.990227699279785
Epoch 400, val loss: 0.8325608968734741
Epoch 410, training loss: 0.10858135670423508 = 0.03868100047111511 + 0.01 * 6.9900360107421875
Epoch 410, val loss: 0.8483319878578186
Epoch 420, training loss: 0.10501286387443542 = 0.03513799235224724 + 0.01 * 6.98748779296875
Epoch 420, val loss: 0.8638167381286621
Epoch 430, training loss: 0.10190056264400482 = 0.03203997388482094 + 0.01 * 6.986059188842773
Epoch 430, val loss: 0.8789892792701721
Epoch 440, training loss: 0.09916355460882187 = 0.029321705922484398 + 0.01 * 6.984184741973877
Epoch 440, val loss: 0.893877387046814
Epoch 450, training loss: 0.09673894196748734 = 0.02692546136677265 + 0.01 * 6.981348514556885
Epoch 450, val loss: 0.9083924889564514
Epoch 460, training loss: 0.09458944201469421 = 0.02480398491024971 + 0.01 * 6.978545665740967
Epoch 460, val loss: 0.9225650429725647
Epoch 470, training loss: 0.0927114337682724 = 0.022918514907360077 + 0.01 * 6.979291915893555
Epoch 470, val loss: 0.9363697171211243
Epoch 480, training loss: 0.0909733772277832 = 0.021236812695860863 + 0.01 * 6.973656177520752
Epoch 480, val loss: 0.9498046636581421
Epoch 490, training loss: 0.08942429721355438 = 0.019732361659407616 + 0.01 * 6.969193458557129
Epoch 490, val loss: 0.9628956913948059
Epoch 500, training loss: 0.08804382383823395 = 0.018381362780928612 + 0.01 * 6.966246128082275
Epoch 500, val loss: 0.9756625890731812
Epoch 510, training loss: 0.08677440881729126 = 0.01716424711048603 + 0.01 * 6.961016654968262
Epoch 510, val loss: 0.988083004951477
Epoch 520, training loss: 0.08571841567754745 = 0.016064628958702087 + 0.01 * 6.965378761291504
Epoch 520, val loss: 1.0001493692398071
Epoch 530, training loss: 0.08459082245826721 = 0.015068464912474155 + 0.01 * 6.952236175537109
Epoch 530, val loss: 1.0119297504425049
Epoch 540, training loss: 0.08362320810556412 = 0.014163346961140633 + 0.01 * 6.945986270904541
Epoch 540, val loss: 1.0233734846115112
Epoch 550, training loss: 0.08286674320697784 = 0.013338728807866573 + 0.01 * 6.95280122756958
Epoch 550, val loss: 1.0344799757003784
Epoch 560, training loss: 0.08192742615938187 = 0.012585488148033619 + 0.01 * 6.9341936111450195
Epoch 560, val loss: 1.0453602075576782
Epoch 570, training loss: 0.08119959384202957 = 0.011896063573658466 + 0.01 * 6.930352687835693
Epoch 570, val loss: 1.0559494495391846
Epoch 580, training loss: 0.08050511032342911 = 0.01126366201788187 + 0.01 * 6.924144744873047
Epoch 580, val loss: 1.0662524700164795
Epoch 590, training loss: 0.07977767288684845 = 0.010682223364710808 + 0.01 * 6.909544944763184
Epoch 590, val loss: 1.0762754678726196
Epoch 600, training loss: 0.0792263001203537 = 0.01014656387269497 + 0.01 * 6.907973766326904
Epoch 600, val loss: 1.0859977006912231
Epoch 610, training loss: 0.07868843525648117 = 0.009653118439018726 + 0.01 * 6.903531551361084
Epoch 610, val loss: 1.0955755710601807
Epoch 620, training loss: 0.07804619520902634 = 0.009197150357067585 + 0.01 * 6.884904384613037
Epoch 620, val loss: 1.1046881675720215
Epoch 630, training loss: 0.07758999615907669 = 0.00877457670867443 + 0.01 * 6.881541728973389
Epoch 630, val loss: 1.1137183904647827
Epoch 640, training loss: 0.07751874625682831 = 0.008383478969335556 + 0.01 * 6.91352653503418
Epoch 640, val loss: 1.1224108934402466
Epoch 650, training loss: 0.07675190269947052 = 0.008020883426070213 + 0.01 * 6.873102188110352
Epoch 650, val loss: 1.130860447883606
Epoch 660, training loss: 0.0761096328496933 = 0.0076830945909023285 + 0.01 * 6.842654705047607
Epoch 660, val loss: 1.1390515565872192
Epoch 670, training loss: 0.07575958222150803 = 0.007368086837232113 + 0.01 * 6.839149475097656
Epoch 670, val loss: 1.147106409072876
Epoch 680, training loss: 0.07544264197349548 = 0.007074443623423576 + 0.01 * 6.836819648742676
Epoch 680, val loss: 1.1549129486083984
Epoch 690, training loss: 0.07528828084468842 = 0.00680049043148756 + 0.01 * 6.848779201507568
Epoch 690, val loss: 1.1624585390090942
Epoch 700, training loss: 0.07456250488758087 = 0.006544462870806456 + 0.01 * 6.801804065704346
Epoch 700, val loss: 1.1698304414749146
Epoch 710, training loss: 0.07431548088788986 = 0.006304622162133455 + 0.01 * 6.80108642578125
Epoch 710, val loss: 1.1769541501998901
Epoch 720, training loss: 0.07418478280305862 = 0.006079436279833317 + 0.01 * 6.810534954071045
Epoch 720, val loss: 1.1838845014572144
Epoch 730, training loss: 0.07343373447656631 = 0.005868111737072468 + 0.01 * 6.756562232971191
Epoch 730, val loss: 1.1905947923660278
Epoch 740, training loss: 0.07314582169055939 = 0.005669421516358852 + 0.01 * 6.747640132904053
Epoch 740, val loss: 1.1973814964294434
Epoch 750, training loss: 0.07316190749406815 = 0.005482964683324099 + 0.01 * 6.767894268035889
Epoch 750, val loss: 1.2035883665084839
Epoch 760, training loss: 0.07282983511686325 = 0.005307053215801716 + 0.01 * 6.752277851104736
Epoch 760, val loss: 1.2098162174224854
Epoch 770, training loss: 0.07233518362045288 = 0.005141118541359901 + 0.01 * 6.719407081604004
Epoch 770, val loss: 1.215928554534912
Epoch 780, training loss: 0.07250989228487015 = 0.004984082188457251 + 0.01 * 6.7525811195373535
Epoch 780, val loss: 1.2218109369277954
Epoch 790, training loss: 0.07195843756198883 = 0.004835629370063543 + 0.01 * 6.712280750274658
Epoch 790, val loss: 1.227632999420166
Epoch 800, training loss: 0.07186363637447357 = 0.00469488138332963 + 0.01 * 6.7168755531311035
Epoch 800, val loss: 1.2331700325012207
Epoch 810, training loss: 0.07130464166402817 = 0.004561427515000105 + 0.01 * 6.67432165145874
Epoch 810, val loss: 1.2386754751205444
Epoch 820, training loss: 0.07107562571763992 = 0.004434986039996147 + 0.01 * 6.664063930511475
Epoch 820, val loss: 1.2440741062164307
Epoch 830, training loss: 0.07133273035287857 = 0.004314879886806011 + 0.01 * 6.701785564422607
Epoch 830, val loss: 1.2492709159851074
Epoch 840, training loss: 0.07073564827442169 = 0.004200492054224014 + 0.01 * 6.653515815734863
Epoch 840, val loss: 1.254525899887085
Epoch 850, training loss: 0.07060045748949051 = 0.004091742914170027 + 0.01 * 6.650871753692627
Epoch 850, val loss: 1.2593557834625244
Epoch 860, training loss: 0.07066994905471802 = 0.003987929783761501 + 0.01 * 6.668201923370361
Epoch 860, val loss: 1.2644031047821045
Epoch 870, training loss: 0.07022323459386826 = 0.0038891942240297794 + 0.01 * 6.63340425491333
Epoch 870, val loss: 1.2691718339920044
Epoch 880, training loss: 0.07056885957717896 = 0.003794487565755844 + 0.01 * 6.677437782287598
Epoch 880, val loss: 1.2739375829696655
Epoch 890, training loss: 0.06989350914955139 = 0.0037043741904199123 + 0.01 * 6.618913650512695
Epoch 890, val loss: 1.2785346508026123
Epoch 900, training loss: 0.070088692009449 = 0.0036180715542286634 + 0.01 * 6.6470627784729
Epoch 900, val loss: 1.2830432653427124
Epoch 910, training loss: 0.06950453668832779 = 0.0035355319269001484 + 0.01 * 6.596900463104248
Epoch 910, val loss: 1.2876287698745728
Epoch 920, training loss: 0.0695599839091301 = 0.0034564747475087643 + 0.01 * 6.610351085662842
Epoch 920, val loss: 1.2918585538864136
Epoch 930, training loss: 0.06923128664493561 = 0.003380577312782407 + 0.01 * 6.585071086883545
Epoch 930, val loss: 1.2962722778320312
Epoch 940, training loss: 0.06887996196746826 = 0.003307655453681946 + 0.01 * 6.557230472564697
Epoch 940, val loss: 1.3003863096237183
Epoch 950, training loss: 0.06881284713745117 = 0.003237835131585598 + 0.01 * 6.557501316070557
Epoch 950, val loss: 1.3046367168426514
Epoch 960, training loss: 0.068805031478405 = 0.0031706029549241066 + 0.01 * 6.563443183898926
Epoch 960, val loss: 1.308625340461731
Epoch 970, training loss: 0.06879442930221558 = 0.0031059840694069862 + 0.01 * 6.568844795227051
Epoch 970, val loss: 1.3127148151397705
Epoch 980, training loss: 0.06841473281383514 = 0.003043909789994359 + 0.01 * 6.537082195281982
Epoch 980, val loss: 1.31663978099823
Epoch 990, training loss: 0.06842957437038422 = 0.002984136575832963 + 0.01 * 6.544544219970703
Epoch 990, val loss: 1.3204861879348755
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.8819
Flip ASR: 0.8578/225 nodes
The final ASR:0.60886, 0.19901, Accuracy:0.80370, 0.01210
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11582])
remove edge: torch.Size([2, 9564])
updated graph: torch.Size([2, 10590])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.97909, 0.00627, Accuracy:0.83210, 0.00698
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.4 der2:0 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0575265884399414 = 1.9737883806228638 + 0.01 * 8.37382698059082
Epoch 0, val loss: 1.9751561880111694
Epoch 10, training loss: 2.046050548553467 = 1.9623134136199951 + 0.01 * 8.3737211227417
Epoch 10, val loss: 1.964208960533142
Epoch 20, training loss: 2.031722068786621 = 1.9479881525039673 + 0.01 * 8.373392105102539
Epoch 20, val loss: 1.9500616788864136
Epoch 30, training loss: 2.011314868927002 = 1.927592158317566 + 0.01 * 8.372274398803711
Epoch 30, val loss: 1.92964768409729
Epoch 40, training loss: 1.9808436632156372 = 1.89719557762146 + 0.01 * 8.364811897277832
Epoch 40, val loss: 1.899519443511963
Epoch 50, training loss: 1.9376473426818848 = 1.8544741868972778 + 0.01 * 8.31731128692627
Epoch 50, val loss: 1.8595592975616455
Epoch 60, training loss: 1.8889623880386353 = 1.8070703744888306 + 0.01 * 8.189199447631836
Epoch 60, val loss: 1.820657730102539
Epoch 70, training loss: 1.8456519842147827 = 1.7665461301803589 + 0.01 * 7.910580635070801
Epoch 70, val loss: 1.7898170948028564
Epoch 80, training loss: 1.7937743663787842 = 1.7191972732543945 + 0.01 * 7.457709789276123
Epoch 80, val loss: 1.7480889558792114
Epoch 90, training loss: 1.7263383865356445 = 1.6553863286972046 + 0.01 * 7.095202922821045
Epoch 90, val loss: 1.692976474761963
Epoch 100, training loss: 1.6402016878128052 = 1.5710211992263794 + 0.01 * 6.918049335479736
Epoch 100, val loss: 1.622971534729004
Epoch 110, training loss: 1.5438334941864014 = 1.4752955436706543 + 0.01 * 6.853795051574707
Epoch 110, val loss: 1.5447473526000977
Epoch 120, training loss: 1.451222538948059 = 1.3829283714294434 + 0.01 * 6.829416275024414
Epoch 120, val loss: 1.471842646598816
Epoch 130, training loss: 1.3661071062088013 = 1.2979735136032104 + 0.01 * 6.813356876373291
Epoch 130, val loss: 1.4067203998565674
Epoch 140, training loss: 1.2849022150039673 = 1.2169641256332397 + 0.01 * 6.793808460235596
Epoch 140, val loss: 1.345878005027771
Epoch 150, training loss: 1.2053025960922241 = 1.1374865770339966 + 0.01 * 6.7815985679626465
Epoch 150, val loss: 1.2866548299789429
Epoch 160, training loss: 1.1272660493850708 = 1.0595488548278809 + 0.01 * 6.771719455718994
Epoch 160, val loss: 1.2307504415512085
Epoch 170, training loss: 1.0516796112060547 = 0.9840142726898193 + 0.01 * 6.766538143157959
Epoch 170, val loss: 1.1777245998382568
Epoch 180, training loss: 0.9790393710136414 = 0.9114248156547546 + 0.01 * 6.761456489562988
Epoch 180, val loss: 1.127936840057373
Epoch 190, training loss: 0.9088072776794434 = 0.8412355184555054 + 0.01 * 6.757175922393799
Epoch 190, val loss: 1.0812091827392578
Epoch 200, training loss: 0.8399947881698608 = 0.7724347710609436 + 0.01 * 6.756000995635986
Epoch 200, val loss: 1.0366861820220947
Epoch 210, training loss: 0.7724205851554871 = 0.7049116492271423 + 0.01 * 6.750895023345947
Epoch 210, val loss: 0.9948806166648865
Epoch 220, training loss: 0.7070878148078918 = 0.6396359205245972 + 0.01 * 6.745190620422363
Epoch 220, val loss: 0.9559006094932556
Epoch 230, training loss: 0.6454178094863892 = 0.5778632760047913 + 0.01 * 6.7554545402526855
Epoch 230, val loss: 0.9207957983016968
Epoch 240, training loss: 0.5881347060203552 = 0.5207613110542297 + 0.01 * 6.737339019775391
Epoch 240, val loss: 0.8902179002761841
Epoch 250, training loss: 0.5361092686653137 = 0.4688388407230377 + 0.01 * 6.727043151855469
Epoch 250, val loss: 0.8648474216461182
Epoch 260, training loss: 0.4890018105506897 = 0.42175963521003723 + 0.01 * 6.724217891693115
Epoch 260, val loss: 0.8446077108383179
Epoch 270, training loss: 0.44590187072753906 = 0.3787809908390045 + 0.01 * 6.712087631225586
Epoch 270, val loss: 0.8287991881370544
Epoch 280, training loss: 0.40621912479400635 = 0.33919525146484375 + 0.01 * 6.702388286590576
Epoch 280, val loss: 0.8167294263839722
Epoch 290, training loss: 0.3695072531700134 = 0.30255773663520813 + 0.01 * 6.694952964782715
Epoch 290, val loss: 0.8078891038894653
Epoch 300, training loss: 0.3355119228363037 = 0.26864609122276306 + 0.01 * 6.686583995819092
Epoch 300, val loss: 0.8023253083229065
Epoch 310, training loss: 0.3040453791618347 = 0.2372608333826065 + 0.01 * 6.67845344543457
Epoch 310, val loss: 0.799979031085968
Epoch 320, training loss: 0.27527159452438354 = 0.20843808352947235 + 0.01 * 6.683351039886475
Epoch 320, val loss: 0.8008833527565002
Epoch 330, training loss: 0.24905504286289215 = 0.18237972259521484 + 0.01 * 6.667531967163086
Epoch 330, val loss: 0.805008590221405
Epoch 340, training loss: 0.22583940625190735 = 0.15925389528274536 + 0.01 * 6.658552169799805
Epoch 340, val loss: 0.8122172951698303
Epoch 350, training loss: 0.20579805970191956 = 0.1391465961933136 + 0.01 * 6.665145397186279
Epoch 350, val loss: 0.8219510912895203
Epoch 360, training loss: 0.18833741545677185 = 0.12189541012048721 + 0.01 * 6.644201278686523
Epoch 360, val loss: 0.8339870572090149
Epoch 370, training loss: 0.17372959852218628 = 0.10716763883829117 + 0.01 * 6.656196594238281
Epoch 370, val loss: 0.8477796316146851
Epoch 380, training loss: 0.1609649956226349 = 0.09460850805044174 + 0.01 * 6.635649681091309
Epoch 380, val loss: 0.8628860116004944
Epoch 390, training loss: 0.15022245049476624 = 0.08386534452438354 + 0.01 * 6.635710716247559
Epoch 390, val loss: 0.8788831233978271
Epoch 400, training loss: 0.14093083143234253 = 0.07463525235652924 + 0.01 * 6.629558086395264
Epoch 400, val loss: 0.8954134583473206
Epoch 410, training loss: 0.13295243680477142 = 0.0666690394282341 + 0.01 * 6.628340244293213
Epoch 410, val loss: 0.9121450781822205
Epoch 420, training loss: 0.12595713138580322 = 0.05975840613245964 + 0.01 * 6.619872570037842
Epoch 420, val loss: 0.9287947416305542
Epoch 430, training loss: 0.11985136568546295 = 0.053737442940473557 + 0.01 * 6.611392021179199
Epoch 430, val loss: 0.9452190399169922
Epoch 440, training loss: 0.11453385651111603 = 0.04847467690706253 + 0.01 * 6.605917930603027
Epoch 440, val loss: 0.9612565636634827
Epoch 450, training loss: 0.1099729835987091 = 0.0438619889318943 + 0.01 * 6.611100196838379
Epoch 450, val loss: 0.9768213629722595
Epoch 460, training loss: 0.10621915757656097 = 0.039805036038160324 + 0.01 * 6.641412734985352
Epoch 460, val loss: 0.9919139742851257
Epoch 470, training loss: 0.10226617753505707 = 0.03623582050204277 + 0.01 * 6.603036403656006
Epoch 470, val loss: 1.006512999534607
Epoch 480, training loss: 0.09903090447187424 = 0.03308432549238205 + 0.01 * 6.594657897949219
Epoch 480, val loss: 1.0206594467163086
Epoch 490, training loss: 0.09623520821332932 = 0.03029303066432476 + 0.01 * 6.594217777252197
Epoch 490, val loss: 1.034328818321228
Epoch 500, training loss: 0.09374484419822693 = 0.027816882357001305 + 0.01 * 6.592796325683594
Epoch 500, val loss: 1.0475084781646729
Epoch 510, training loss: 0.09143669903278351 = 0.02561425045132637 + 0.01 * 6.582244396209717
Epoch 510, val loss: 1.060234546661377
Epoch 520, training loss: 0.08952260762453079 = 0.02364901453256607 + 0.01 * 6.587359428405762
Epoch 520, val loss: 1.0725282430648804
Epoch 530, training loss: 0.0877617597579956 = 0.021891865879297256 + 0.01 * 6.586989879608154
Epoch 530, val loss: 1.0844305753707886
Epoch 540, training loss: 0.08613940328359604 = 0.02031777612864971 + 0.01 * 6.582162380218506
Epoch 540, val loss: 1.095877766609192
Epoch 550, training loss: 0.08462843298912048 = 0.01890350505709648 + 0.01 * 6.572492599487305
Epoch 550, val loss: 1.10696280002594
Epoch 560, training loss: 0.08345798403024673 = 0.017630262300372124 + 0.01 * 6.582772254943848
Epoch 560, val loss: 1.1176047325134277
Epoch 570, training loss: 0.08221758902072906 = 0.016481589525938034 + 0.01 * 6.573599815368652
Epoch 570, val loss: 1.1279497146606445
Epoch 580, training loss: 0.08106829971075058 = 0.01544151734560728 + 0.01 * 6.562678337097168
Epoch 580, val loss: 1.1379013061523438
Epoch 590, training loss: 0.08004789799451828 = 0.014497332274913788 + 0.01 * 6.555056571960449
Epoch 590, val loss: 1.1475636959075928
Epoch 600, training loss: 0.07919040322303772 = 0.01363924890756607 + 0.01 * 6.555115222930908
Epoch 600, val loss: 1.1568353176116943
Epoch 610, training loss: 0.07838189601898193 = 0.012857085093855858 + 0.01 * 6.552481174468994
Epoch 610, val loss: 1.165846347808838
Epoch 620, training loss: 0.0775715634226799 = 0.012142270803451538 + 0.01 * 6.542929649353027
Epoch 620, val loss: 1.1745336055755615
Epoch 630, training loss: 0.07729887217283249 = 0.011487158946692944 + 0.01 * 6.581171989440918
Epoch 630, val loss: 1.1829137802124023
Epoch 640, training loss: 0.07624409347772598 = 0.010886671021580696 + 0.01 * 6.535742282867432
Epoch 640, val loss: 1.1910613775253296
Epoch 650, training loss: 0.07573848962783813 = 0.010334196500480175 + 0.01 * 6.540429592132568
Epoch 650, val loss: 1.1989253759384155
Epoch 660, training loss: 0.07505271583795547 = 0.00982560683041811 + 0.01 * 6.522710800170898
Epoch 660, val loss: 1.2065166234970093
Epoch 670, training loss: 0.07462812960147858 = 0.009355727583169937 + 0.01 * 6.527240753173828
Epoch 670, val loss: 1.2138680219650269
Epoch 680, training loss: 0.07431434094905853 = 0.008920915424823761 + 0.01 * 6.539342403411865
Epoch 680, val loss: 1.2209969758987427
Epoch 690, training loss: 0.07367406785488129 = 0.008518237620592117 + 0.01 * 6.515583515167236
Epoch 690, val loss: 1.2279105186462402
Epoch 700, training loss: 0.07342159003019333 = 0.008144273422658443 + 0.01 * 6.527731895446777
Epoch 700, val loss: 1.23464035987854
Epoch 710, training loss: 0.07291059195995331 = 0.007796491030603647 + 0.01 * 6.511410236358643
Epoch 710, val loss: 1.241096019744873
Epoch 720, training loss: 0.07282807677984238 = 0.0074723223224282265 + 0.01 * 6.535575866699219
Epoch 720, val loss: 1.2473870515823364
Epoch 730, training loss: 0.07208817452192307 = 0.007170278113335371 + 0.01 * 6.491789817810059
Epoch 730, val loss: 1.253466248512268
Epoch 740, training loss: 0.0719112679362297 = 0.0068879202008247375 + 0.01 * 6.502335071563721
Epoch 740, val loss: 1.2593836784362793
Epoch 750, training loss: 0.07153631746768951 = 0.006623886991292238 + 0.01 * 6.491243362426758
Epoch 750, val loss: 1.2651169300079346
Epoch 760, training loss: 0.07129941135644913 = 0.006376638077199459 + 0.01 * 6.492277145385742
Epoch 760, val loss: 1.2706990242004395
Epoch 770, training loss: 0.07104072719812393 = 0.006144561339169741 + 0.01 * 6.489616870880127
Epoch 770, val loss: 1.2760597467422485
Epoch 780, training loss: 0.07081518322229385 = 0.005926661659032106 + 0.01 * 6.488852500915527
Epoch 780, val loss: 1.2813282012939453
Epoch 790, training loss: 0.07076632976531982 = 0.005721366498619318 + 0.01 * 6.504496097564697
Epoch 790, val loss: 1.2864494323730469
Epoch 800, training loss: 0.07031427323818207 = 0.0055283657275140285 + 0.01 * 6.478590488433838
Epoch 800, val loss: 1.2913532257080078
Epoch 810, training loss: 0.07005150616168976 = 0.005346223711967468 + 0.01 * 6.470528602600098
Epoch 810, val loss: 1.2961671352386475
Epoch 820, training loss: 0.07011830806732178 = 0.005174180958420038 + 0.01 * 6.494412422180176
Epoch 820, val loss: 1.3008761405944824
Epoch 830, training loss: 0.06953795254230499 = 0.005011700559407473 + 0.01 * 6.452625274658203
Epoch 830, val loss: 1.3054254055023193
Epoch 840, training loss: 0.0694509819149971 = 0.004858086351305246 + 0.01 * 6.459290027618408
Epoch 840, val loss: 1.3098403215408325
Epoch 850, training loss: 0.06925615668296814 = 0.004712467081844807 + 0.01 * 6.454369068145752
Epoch 850, val loss: 1.314189076423645
Epoch 860, training loss: 0.06911977380514145 = 0.004574327263981104 + 0.01 * 6.454545021057129
Epoch 860, val loss: 1.3183844089508057
Epoch 870, training loss: 0.0690670982003212 = 0.004443508572876453 + 0.01 * 6.4623589515686035
Epoch 870, val loss: 1.3224643468856812
Epoch 880, training loss: 0.0689411461353302 = 0.004318918567150831 + 0.01 * 6.462223529815674
Epoch 880, val loss: 1.3264473676681519
Epoch 890, training loss: 0.06847097724676132 = 0.0042005861178040504 + 0.01 * 6.427039623260498
Epoch 890, val loss: 1.330359935760498
Epoch 900, training loss: 0.06822232902050018 = 0.004088126588612795 + 0.01 * 6.413420677185059
Epoch 900, val loss: 1.3341565132141113
Epoch 910, training loss: 0.06817494332790375 = 0.00398073997348547 + 0.01 * 6.41942024230957
Epoch 910, val loss: 1.3378002643585205
Epoch 920, training loss: 0.06797587871551514 = 0.003878497751429677 + 0.01 * 6.409738540649414
Epoch 920, val loss: 1.3413628339767456
Epoch 930, training loss: 0.06804712116718292 = 0.003780984552577138 + 0.01 * 6.426614284515381
Epoch 930, val loss: 1.3448902368545532
Epoch 940, training loss: 0.06792566180229187 = 0.0036877004895359278 + 0.01 * 6.4237961769104
Epoch 940, val loss: 1.3482733964920044
Epoch 950, training loss: 0.06751962751150131 = 0.0035986194852739573 + 0.01 * 6.3921003341674805
Epoch 950, val loss: 1.3515623807907104
Epoch 960, training loss: 0.06755369901657104 = 0.0035133801866322756 + 0.01 * 6.404032230377197
Epoch 960, val loss: 1.354821801185608
Epoch 970, training loss: 0.06730583310127258 = 0.003431908320635557 + 0.01 * 6.387392520904541
Epoch 970, val loss: 1.357973575592041
Epoch 980, training loss: 0.06746481359004974 = 0.003353661624714732 + 0.01 * 6.4111151695251465
Epoch 980, val loss: 1.3610420227050781
Epoch 990, training loss: 0.06711366027593613 = 0.003278976073488593 + 0.01 * 6.383468151092529
Epoch 990, val loss: 1.3639705181121826
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.4 der2:0 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.037571668624878 = 1.9538341760635376 + 0.01 * 8.373743057250977
Epoch 0, val loss: 1.9569119215011597
Epoch 10, training loss: 2.027235507965088 = 1.943500280380249 + 0.01 * 8.373534202575684
Epoch 10, val loss: 1.9469531774520874
Epoch 20, training loss: 2.014360189437866 = 1.9306308031082153 + 0.01 * 8.372928619384766
Epoch 20, val loss: 1.934645414352417
Epoch 30, training loss: 1.9962825775146484 = 1.9125744104385376 + 0.01 * 8.37081241607666
Epoch 30, val loss: 1.9173864126205444
Epoch 40, training loss: 1.9694243669509888 = 1.8858563899993896 + 0.01 * 8.356793403625488
Epoch 40, val loss: 1.8922441005706787
Epoch 50, training loss: 1.9301156997680664 = 1.8475302457809448 + 0.01 * 8.258545875549316
Epoch 50, val loss: 1.8576879501342773
Epoch 60, training loss: 1.878701090812683 = 1.8008856773376465 + 0.01 * 7.7815375328063965
Epoch 60, val loss: 1.8184388875961304
Epoch 70, training loss: 1.829594612121582 = 1.754673957824707 + 0.01 * 7.492059707641602
Epoch 70, val loss: 1.7794910669326782
Epoch 80, training loss: 1.7741224765777588 = 1.701371669769287 + 0.01 * 7.275086402893066
Epoch 80, val loss: 1.7285929918289185
Epoch 90, training loss: 1.6999006271362305 = 1.6289036273956299 + 0.01 * 7.099700450897217
Epoch 90, val loss: 1.6621267795562744
Epoch 100, training loss: 1.606218695640564 = 1.5363285541534424 + 0.01 * 6.989016532897949
Epoch 100, val loss: 1.5833994150161743
Epoch 110, training loss: 1.5031191110610962 = 1.4338444471359253 + 0.01 * 6.927464485168457
Epoch 110, val loss: 1.4987432956695557
Epoch 120, training loss: 1.4012525081634521 = 1.3323264122009277 + 0.01 * 6.892605781555176
Epoch 120, val loss: 1.4164507389068604
Epoch 130, training loss: 1.3055713176727295 = 1.2368019819259644 + 0.01 * 6.876939296722412
Epoch 130, val loss: 1.3410942554473877
Epoch 140, training loss: 1.2136965990066528 = 1.1450399160385132 + 0.01 * 6.865668773651123
Epoch 140, val loss: 1.2704155445098877
Epoch 150, training loss: 1.1217344999313354 = 1.053220510482788 + 0.01 * 6.8513946533203125
Epoch 150, val loss: 1.2002357244491577
Epoch 160, training loss: 1.0285487174987793 = 0.960222065448761 + 0.01 * 6.83266019821167
Epoch 160, val loss: 1.1296026706695557
Epoch 170, training loss: 0.9375959038734436 = 0.8695048689842224 + 0.01 * 6.809101581573486
Epoch 170, val loss: 1.060801386833191
Epoch 180, training loss: 0.854744553565979 = 0.7869105935096741 + 0.01 * 6.783398628234863
Epoch 180, val loss: 0.9993165135383606
Epoch 190, training loss: 0.7835318446159363 = 0.7159112691879272 + 0.01 * 6.762059211730957
Epoch 190, val loss: 0.948284924030304
Epoch 200, training loss: 0.723236620426178 = 0.6557334661483765 + 0.01 * 6.750317096710205
Epoch 200, val loss: 0.9074767231941223
Epoch 210, training loss: 0.6705197095870972 = 0.6030701398849487 + 0.01 * 6.744958877563477
Epoch 210, val loss: 0.8744021058082581
Epoch 220, training loss: 0.6218661069869995 = 0.5544987320899963 + 0.01 * 6.73673677444458
Epoch 220, val loss: 0.846055269241333
Epoch 230, training loss: 0.5749686360359192 = 0.5076813101768494 + 0.01 * 6.728734016418457
Epoch 230, val loss: 0.8201161026954651
Epoch 240, training loss: 0.528936505317688 = 0.46174266934394836 + 0.01 * 6.719385147094727
Epoch 240, val loss: 0.7958365678787231
Epoch 250, training loss: 0.4839824140071869 = 0.41688820719718933 + 0.01 * 6.709421157836914
Epoch 250, val loss: 0.7735000848770142
Epoch 260, training loss: 0.44086000323295593 = 0.3738768696784973 + 0.01 * 6.698313236236572
Epoch 260, val loss: 0.7536913156509399
Epoch 270, training loss: 0.40030622482299805 = 0.33343949913978577 + 0.01 * 6.686672210693359
Epoch 270, val loss: 0.7369133234024048
Epoch 280, training loss: 0.3629035949707031 = 0.296170175075531 + 0.01 * 6.673340320587158
Epoch 280, val loss: 0.7234644293785095
Epoch 290, training loss: 0.32887089252471924 = 0.26227298378944397 + 0.01 * 6.659791469573975
Epoch 290, val loss: 0.7128047347068787
Epoch 300, training loss: 0.2981323003768921 = 0.2315710186958313 + 0.01 * 6.656126499176025
Epoch 300, val loss: 0.7048479914665222
Epoch 310, training loss: 0.27015042304992676 = 0.20375703275203705 + 0.01 * 6.639337539672852
Epoch 310, val loss: 0.6992585062980652
Epoch 320, training loss: 0.24484652280807495 = 0.17855487763881683 + 0.01 * 6.62916374206543
Epoch 320, val loss: 0.6957888603210449
Epoch 330, training loss: 0.22221538424491882 = 0.15589453279972076 + 0.01 * 6.632085800170898
Epoch 330, val loss: 0.6944392323493958
Epoch 340, training loss: 0.2019631415605545 = 0.13577789068222046 + 0.01 * 6.61852502822876
Epoch 340, val loss: 0.6950680613517761
Epoch 350, training loss: 0.18426404893398285 = 0.1181647926568985 + 0.01 * 6.609925746917725
Epoch 350, val loss: 0.6978879570960999
Epoch 360, training loss: 0.16920676827430725 = 0.10295652598142624 + 0.01 * 6.625024318695068
Epoch 360, val loss: 0.7028939723968506
Epoch 370, training loss: 0.15605272352695465 = 0.08997496217489243 + 0.01 * 6.607776641845703
Epoch 370, val loss: 0.7100235819816589
Epoch 380, training loss: 0.14498622715473175 = 0.07896867394447327 + 0.01 * 6.601755619049072
Epoch 380, val loss: 0.7189757823944092
Epoch 390, training loss: 0.13562174141407013 = 0.0696597695350647 + 0.01 * 6.596197605133057
Epoch 390, val loss: 0.7295058965682983
Epoch 400, training loss: 0.12770044803619385 = 0.06176574155688286 + 0.01 * 6.593471050262451
Epoch 400, val loss: 0.7413967847824097
Epoch 410, training loss: 0.12101796269416809 = 0.05503523349761963 + 0.01 * 6.598272800445557
Epoch 410, val loss: 0.7543226480484009
Epoch 420, training loss: 0.11514582484960556 = 0.04926276206970215 + 0.01 * 6.588306427001953
Epoch 420, val loss: 0.767980694770813
Epoch 430, training loss: 0.11014336347579956 = 0.044269297271966934 + 0.01 * 6.587406635284424
Epoch 430, val loss: 0.7822537422180176
Epoch 440, training loss: 0.10574957728385925 = 0.0399261899292469 + 0.01 * 6.582338809967041
Epoch 440, val loss: 0.7968811392784119
Epoch 450, training loss: 0.10194754600524902 = 0.03613034263253212 + 0.01 * 6.581720352172852
Epoch 450, val loss: 0.8117201328277588
Epoch 460, training loss: 0.09855690598487854 = 0.032800424844026566 + 0.01 * 6.575647830963135
Epoch 460, val loss: 0.8265615105628967
Epoch 470, training loss: 0.0957329049706459 = 0.02986939810216427 + 0.01 * 6.586350917816162
Epoch 470, val loss: 0.8413710594177246
Epoch 480, training loss: 0.09297968447208405 = 0.027286585420370102 + 0.01 * 6.569310188293457
Epoch 480, val loss: 0.8560075163841248
Epoch 490, training loss: 0.09069105237722397 = 0.02500350959599018 + 0.01 * 6.56875467300415
Epoch 490, val loss: 0.8703835606575012
Epoch 500, training loss: 0.08861587941646576 = 0.022978655993938446 + 0.01 * 6.563722610473633
Epoch 500, val loss: 0.8845752477645874
Epoch 510, training loss: 0.08677153289318085 = 0.021177666261792183 + 0.01 * 6.559386730194092
Epoch 510, val loss: 0.8984214067459106
Epoch 520, training loss: 0.08519614487886429 = 0.01957167126238346 + 0.01 * 6.562447547912598
Epoch 520, val loss: 0.9119980931282043
Epoch 530, training loss: 0.0837392807006836 = 0.018137510865926743 + 0.01 * 6.560177326202393
Epoch 530, val loss: 0.9251954555511475
Epoch 540, training loss: 0.08239270001649857 = 0.01685359887778759 + 0.01 * 6.553910255432129
Epoch 540, val loss: 0.9379971027374268
Epoch 550, training loss: 0.08119003474712372 = 0.01569962687790394 + 0.01 * 6.549041271209717
Epoch 550, val loss: 0.9505020976066589
Epoch 560, training loss: 0.08019721508026123 = 0.014658752828836441 + 0.01 * 6.5538458824157715
Epoch 560, val loss: 0.9626626372337341
Epoch 570, training loss: 0.07922282814979553 = 0.013718497939407825 + 0.01 * 6.550433158874512
Epoch 570, val loss: 0.9744745492935181
Epoch 580, training loss: 0.07831791043281555 = 0.012866501696407795 + 0.01 * 6.545140743255615
Epoch 580, val loss: 0.9859623908996582
Epoch 590, training loss: 0.07752133905887604 = 0.01209250558167696 + 0.01 * 6.542882919311523
Epoch 590, val loss: 0.9971060156822205
Epoch 600, training loss: 0.07682070881128311 = 0.01138775609433651 + 0.01 * 6.543294906616211
Epoch 600, val loss: 1.007956862449646
Epoch 610, training loss: 0.0760556161403656 = 0.010743711143732071 + 0.01 * 6.531191349029541
Epoch 610, val loss: 1.0185449123382568
Epoch 620, training loss: 0.07554472982883453 = 0.010154695250093937 + 0.01 * 6.539003372192383
Epoch 620, val loss: 1.0288280248641968
Epoch 630, training loss: 0.07489050179719925 = 0.00961472000926733 + 0.01 * 6.527577877044678
Epoch 630, val loss: 1.0388599634170532
Epoch 640, training loss: 0.07434888929128647 = 0.009118447080254555 + 0.01 * 6.523044586181641
Epoch 640, val loss: 1.0486146211624146
Epoch 650, training loss: 0.07402165234088898 = 0.008661543019115925 + 0.01 * 6.5360107421875
Epoch 650, val loss: 1.0581096410751343
Epoch 660, training loss: 0.07341311126947403 = 0.008240683935582638 + 0.01 * 6.517242431640625
Epoch 660, val loss: 1.0673772096633911
Epoch 670, training loss: 0.07302328199148178 = 0.007851623930037022 + 0.01 * 6.5171661376953125
Epoch 670, val loss: 1.076403260231018
Epoch 680, training loss: 0.0726233720779419 = 0.0074912672862410545 + 0.01 * 6.513210296630859
Epoch 680, val loss: 1.0851492881774902
Epoch 690, training loss: 0.07253815978765488 = 0.007157173473387957 + 0.01 * 6.538098335266113
Epoch 690, val loss: 1.0936840772628784
Epoch 700, training loss: 0.07197758555412292 = 0.006847092881798744 + 0.01 * 6.513049602508545
Epoch 700, val loss: 1.1020355224609375
Epoch 710, training loss: 0.0716053918004036 = 0.006558287423104048 + 0.01 * 6.504711151123047
Epoch 710, val loss: 1.1100965738296509
Epoch 720, training loss: 0.07145440578460693 = 0.006288787815719843 + 0.01 * 6.516561985015869
Epoch 720, val loss: 1.1180219650268555
Epoch 730, training loss: 0.07100484520196915 = 0.006037497892975807 + 0.01 * 6.496735095977783
Epoch 730, val loss: 1.1257601976394653
Epoch 740, training loss: 0.07091083377599716 = 0.00580255500972271 + 0.01 * 6.510828018188477
Epoch 740, val loss: 1.133255124092102
Epoch 750, training loss: 0.07053741812705994 = 0.005582515615969896 + 0.01 * 6.495490074157715
Epoch 750, val loss: 1.1406515836715698
Epoch 760, training loss: 0.07054453343153 = 0.005376386921852827 + 0.01 * 6.516815185546875
Epoch 760, val loss: 1.1477850675582886
Epoch 770, training loss: 0.07001292705535889 = 0.005183056462556124 + 0.01 * 6.482987403869629
Epoch 770, val loss: 1.1548221111297607
Epoch 780, training loss: 0.06978975981473923 = 0.005001549609005451 + 0.01 * 6.47882080078125
Epoch 780, val loss: 1.1616265773773193
Epoch 790, training loss: 0.07007292658090591 = 0.004830527119338512 + 0.01 * 6.524240493774414
Epoch 790, val loss: 1.1682820320129395
Epoch 800, training loss: 0.06934086233377457 = 0.004669754300266504 + 0.01 * 6.467111110687256
Epoch 800, val loss: 1.174778699874878
Epoch 810, training loss: 0.06918228417634964 = 0.00451815826818347 + 0.01 * 6.466412544250488
Epoch 810, val loss: 1.181114673614502
Epoch 820, training loss: 0.06911662220954895 = 0.004374723415821791 + 0.01 * 6.4741902351379395
Epoch 820, val loss: 1.1873339414596558
Epoch 830, training loss: 0.06888607889413834 = 0.004239207599312067 + 0.01 * 6.464686870574951
Epoch 830, val loss: 1.1934345960617065
Epoch 840, training loss: 0.06866180896759033 = 0.004110817797482014 + 0.01 * 6.455099105834961
Epoch 840, val loss: 1.1993550062179565
Epoch 850, training loss: 0.06862200051546097 = 0.003989154007285833 + 0.01 * 6.463284969329834
Epoch 850, val loss: 1.2052007913589478
Epoch 860, training loss: 0.06844229996204376 = 0.0038740430027246475 + 0.01 * 6.4568257331848145
Epoch 860, val loss: 1.2108286619186401
Epoch 870, training loss: 0.06829708814620972 = 0.0037649814039468765 + 0.01 * 6.453211307525635
Epoch 870, val loss: 1.216416835784912
Epoch 880, training loss: 0.068229079246521 = 0.003661344526335597 + 0.01 * 6.4567742347717285
Epoch 880, val loss: 1.2218561172485352
Epoch 890, training loss: 0.06808420270681381 = 0.00356287625618279 + 0.01 * 6.452132701873779
Epoch 890, val loss: 1.227206826210022
Epoch 900, training loss: 0.06782165169715881 = 0.0034691274631768465 + 0.01 * 6.435253143310547
Epoch 900, val loss: 1.2323312759399414
Epoch 910, training loss: 0.06782601773738861 = 0.0033799794036895037 + 0.01 * 6.444604396820068
Epoch 910, val loss: 1.237497091293335
Epoch 920, training loss: 0.06761017441749573 = 0.003294955240562558 + 0.01 * 6.431522369384766
Epoch 920, val loss: 1.2424606084823608
Epoch 930, training loss: 0.06773718446493149 = 0.0032137439120560884 + 0.01 * 6.4523444175720215
Epoch 930, val loss: 1.2473851442337036
Epoch 940, training loss: 0.06742178648710251 = 0.0031362769659608603 + 0.01 * 6.428550720214844
Epoch 940, val loss: 1.252070426940918
Epoch 950, training loss: 0.06750722974538803 = 0.0030623958446085453 + 0.01 * 6.444483280181885
Epoch 950, val loss: 1.2567734718322754
Epoch 960, training loss: 0.0671384260058403 = 0.0029916916973888874 + 0.01 * 6.414673328399658
Epoch 960, val loss: 1.2614130973815918
Epoch 970, training loss: 0.06721619516611099 = 0.0029239205177873373 + 0.01 * 6.429227828979492
Epoch 970, val loss: 1.26595139503479
Epoch 980, training loss: 0.06710626929998398 = 0.002859080908820033 + 0.01 * 6.424719333648682
Epoch 980, val loss: 1.2703748941421509
Epoch 990, training loss: 0.06722541153430939 = 0.002797011286020279 + 0.01 * 6.442840099334717
Epoch 990, val loss: 1.2746940851211548
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.4 der2:0 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0220446586608887 = 1.9383057355880737 + 0.01 * 8.37388801574707
Epoch 0, val loss: 1.943974494934082
Epoch 10, training loss: 2.012410879135132 = 1.9286726713180542 + 0.01 * 8.373809814453125
Epoch 10, val loss: 1.9344643354415894
Epoch 20, training loss: 2.000671625137329 = 1.9169360399246216 + 0.01 * 8.373556137084961
Epoch 20, val loss: 1.922745943069458
Epoch 30, training loss: 1.9844404458999634 = 1.9007129669189453 + 0.01 * 8.372753143310547
Epoch 30, val loss: 1.9066381454467773
Epoch 40, training loss: 1.9607123136520386 = 1.8770277500152588 + 0.01 * 8.368452072143555
Epoch 40, val loss: 1.8834867477416992
Epoch 50, training loss: 1.927101731300354 = 1.8436880111694336 + 0.01 * 8.341377258300781
Epoch 50, val loss: 1.8522454500198364
Epoch 60, training loss: 1.8861720561981201 = 1.8038123846054077 + 0.01 * 8.23596477508545
Epoch 60, val loss: 1.8176029920578003
Epoch 70, training loss: 1.8460214138031006 = 1.7656844854354858 + 0.01 * 8.033698081970215
Epoch 70, val loss: 1.7851661443710327
Epoch 80, training loss: 1.7971923351287842 = 1.7205599546432495 + 0.01 * 7.663234710693359
Epoch 80, val loss: 1.743396520614624
Epoch 90, training loss: 1.7297261953353882 = 1.6564284563064575 + 0.01 * 7.329768657684326
Epoch 90, val loss: 1.6876939535140991
Epoch 100, training loss: 1.6437139511108398 = 1.5725562572479248 + 0.01 * 7.115766525268555
Epoch 100, val loss: 1.6200066804885864
Epoch 110, training loss: 1.5456806421279907 = 1.4756444692611694 + 0.01 * 7.003614902496338
Epoch 110, val loss: 1.541108250617981
Epoch 120, training loss: 1.446272850036621 = 1.376907229423523 + 0.01 * 6.9365644454956055
Epoch 120, val loss: 1.4610474109649658
Epoch 130, training loss: 1.350404143333435 = 1.281267762184143 + 0.01 * 6.913639068603516
Epoch 130, val loss: 1.385114312171936
Epoch 140, training loss: 1.2571213245391846 = 1.1881579160690308 + 0.01 * 6.896341800689697
Epoch 140, val loss: 1.3135994672775269
Epoch 150, training loss: 1.1645454168319702 = 1.0957056283950806 + 0.01 * 6.883984565734863
Epoch 150, val loss: 1.2435623407363892
Epoch 160, training loss: 1.0704048871994019 = 1.0016554594039917 + 0.01 * 6.874943256378174
Epoch 160, val loss: 1.171938419342041
Epoch 170, training loss: 0.9750398397445679 = 0.9063629508018494 + 0.01 * 6.867691516876221
Epoch 170, val loss: 1.0984662771224976
Epoch 180, training loss: 0.8826844692230225 = 0.8140625357627869 + 0.01 * 6.862194538116455
Epoch 180, val loss: 1.027495265007019
Epoch 190, training loss: 0.7984380125999451 = 0.7298556566238403 + 0.01 * 6.858236312866211
Epoch 190, val loss: 0.9647191762924194
Epoch 200, training loss: 0.7241550087928772 = 0.6556061506271362 + 0.01 * 6.854886531829834
Epoch 200, val loss: 0.913094162940979
Epoch 210, training loss: 0.6580129265785217 = 0.5894866585731506 + 0.01 * 6.852628231048584
Epoch 210, val loss: 0.8711097240447998
Epoch 220, training loss: 0.5970903635025024 = 0.5285933017730713 + 0.01 * 6.849703788757324
Epoch 220, val loss: 0.8358281254768372
Epoch 230, training loss: 0.5393111109733582 = 0.47084829211235046 + 0.01 * 6.846280574798584
Epoch 230, val loss: 0.8046818375587463
Epoch 240, training loss: 0.4839625954627991 = 0.4155442714691162 + 0.01 * 6.841831684112549
Epoch 240, val loss: 0.7767322063446045
Epoch 250, training loss: 0.43147847056388855 = 0.3630950450897217 + 0.01 * 6.838343143463135
Epoch 250, val loss: 0.7528432607650757
Epoch 260, training loss: 0.3830571472644806 = 0.3147090673446655 + 0.01 * 6.834807395935059
Epoch 260, val loss: 0.7343753576278687
Epoch 270, training loss: 0.3397192060947418 = 0.27144598960876465 + 0.01 * 6.8273210525512695
Epoch 270, val loss: 0.7223532795906067
Epoch 280, training loss: 0.3020409047603607 = 0.23382781445980072 + 0.01 * 6.8213090896606445
Epoch 280, val loss: 0.7171828150749207
Epoch 290, training loss: 0.2699805200099945 = 0.2017654925584793 + 0.01 * 6.821503639221191
Epoch 290, val loss: 0.7183417081832886
Epoch 300, training loss: 0.24285143613815308 = 0.17474789917469025 + 0.01 * 6.810354709625244
Epoch 300, val loss: 0.7247048616409302
Epoch 310, training loss: 0.22010044753551483 = 0.15205694735050201 + 0.01 * 6.80435037612915
Epoch 310, val loss: 0.7353214025497437
Epoch 320, training loss: 0.20094850659370422 = 0.13296250998973846 + 0.01 * 6.798599720001221
Epoch 320, val loss: 0.7490933537483215
Epoch 330, training loss: 0.18516656756401062 = 0.11680325120687485 + 0.01 * 6.836332321166992
Epoch 330, val loss: 0.7650719285011292
Epoch 340, training loss: 0.17120802402496338 = 0.10307341068983078 + 0.01 * 6.8134613037109375
Epoch 340, val loss: 0.7826250195503235
Epoch 350, training loss: 0.15918520092964172 = 0.091318279504776 + 0.01 * 6.7866926193237305
Epoch 350, val loss: 0.8009467124938965
Epoch 360, training loss: 0.14901408553123474 = 0.08118592202663422 + 0.01 * 6.782815456390381
Epoch 360, val loss: 0.8197283148765564
Epoch 370, training loss: 0.1401740461587906 = 0.07240886241197586 + 0.01 * 6.776518821716309
Epoch 370, val loss: 0.8388163447380066
Epoch 380, training loss: 0.1324964165687561 = 0.06477740406990051 + 0.01 * 6.771901607513428
Epoch 380, val loss: 0.857903003692627
Epoch 390, training loss: 0.1257954090833664 = 0.058120906352996826 + 0.01 * 6.767450332641602
Epoch 390, val loss: 0.876830518245697
Epoch 400, training loss: 0.11998708546161652 = 0.052300069481134415 + 0.01 * 6.768702030181885
Epoch 400, val loss: 0.8954625129699707
Epoch 410, training loss: 0.11486402899026871 = 0.047206997871398926 + 0.01 * 6.765703201293945
Epoch 410, val loss: 0.9138159155845642
Epoch 420, training loss: 0.11031870543956757 = 0.04273943230509758 + 0.01 * 6.757927894592285
Epoch 420, val loss: 0.9317165017127991
Epoch 430, training loss: 0.10634039342403412 = 0.03880826756358147 + 0.01 * 6.753213405609131
Epoch 430, val loss: 0.9491801857948303
Epoch 440, training loss: 0.10281242430210114 = 0.035340357571840286 + 0.01 * 6.747206687927246
Epoch 440, val loss: 0.9661824703216553
Epoch 450, training loss: 0.09974315762519836 = 0.03227473422884941 + 0.01 * 6.746842384338379
Epoch 450, val loss: 0.9827337265014648
Epoch 460, training loss: 0.09695194661617279 = 0.029561664909124374 + 0.01 * 6.7390289306640625
Epoch 460, val loss: 0.9987940788269043
Epoch 470, training loss: 0.09450206160545349 = 0.02715492807328701 + 0.01 * 6.734713077545166
Epoch 470, val loss: 1.014333963394165
Epoch 480, training loss: 0.09244989603757858 = 0.025014126673340797 + 0.01 * 6.743577480316162
Epoch 480, val loss: 1.0294041633605957
Epoch 490, training loss: 0.09042228013277054 = 0.023107057437300682 + 0.01 * 6.731522560119629
Epoch 490, val loss: 1.0440200567245483
Epoch 500, training loss: 0.08865097165107727 = 0.02140171453356743 + 0.01 * 6.724925518035889
Epoch 500, val loss: 1.058109998703003
Epoch 510, training loss: 0.08708839118480682 = 0.019873496145009995 + 0.01 * 6.721489906311035
Epoch 510, val loss: 1.0718069076538086
Epoch 520, training loss: 0.0856202244758606 = 0.01849978044629097 + 0.01 * 6.712045192718506
Epoch 520, val loss: 1.0850565433502197
Epoch 530, training loss: 0.08433490246534348 = 0.017262659966945648 + 0.01 * 6.707224369049072
Epoch 530, val loss: 1.0978708267211914
Epoch 540, training loss: 0.08318234980106354 = 0.016147416085004807 + 0.01 * 6.703493595123291
Epoch 540, val loss: 1.110244631767273
Epoch 550, training loss: 0.08215918391942978 = 0.015137375332415104 + 0.01 * 6.702180862426758
Epoch 550, val loss: 1.1222542524337769
Epoch 560, training loss: 0.08107981085777283 = 0.014220147393643856 + 0.01 * 6.685966491699219
Epoch 560, val loss: 1.133862853050232
Epoch 570, training loss: 0.08024822175502777 = 0.013385692611336708 + 0.01 * 6.686253547668457
Epoch 570, val loss: 1.145206093788147
Epoch 580, training loss: 0.0794236809015274 = 0.012626019306480885 + 0.01 * 6.679765701293945
Epoch 580, val loss: 1.156090259552002
Epoch 590, training loss: 0.0786372721195221 = 0.011931690387427807 + 0.01 * 6.670558929443359
Epoch 590, val loss: 1.1666061878204346
Epoch 600, training loss: 0.07801596075296402 = 0.011295837350189686 + 0.01 * 6.672011852264404
Epoch 600, val loss: 1.1768896579742432
Epoch 610, training loss: 0.07735839486122131 = 0.01071237027645111 + 0.01 * 6.664602756500244
Epoch 610, val loss: 1.186833143234253
Epoch 620, training loss: 0.07666846364736557 = 0.010175025090575218 + 0.01 * 6.649343967437744
Epoch 620, val loss: 1.1964894533157349
Epoch 630, training loss: 0.07617401331663132 = 0.009678867645561695 + 0.01 * 6.649514675140381
Epoch 630, val loss: 1.2059084177017212
Epoch 640, training loss: 0.07562080770730972 = 0.009220438078045845 + 0.01 * 6.6400370597839355
Epoch 640, val loss: 1.215089201927185
Epoch 650, training loss: 0.07517612725496292 = 0.008796085603535175 + 0.01 * 6.638003826141357
Epoch 650, val loss: 1.2239388227462769
Epoch 660, training loss: 0.07477311789989471 = 0.008402918465435505 + 0.01 * 6.637019634246826
Epoch 660, val loss: 1.2325799465179443
Epoch 670, training loss: 0.07424942404031754 = 0.008038065396249294 + 0.01 * 6.621135711669922
Epoch 670, val loss: 1.240962028503418
Epoch 680, training loss: 0.07392089813947678 = 0.007698363158851862 + 0.01 * 6.62225341796875
Epoch 680, val loss: 1.249146580696106
Epoch 690, training loss: 0.07356952130794525 = 0.0073812673799693584 + 0.01 * 6.618825435638428
Epoch 690, val loss: 1.2571300268173218
Epoch 700, training loss: 0.07319144159555435 = 0.007084968034178019 + 0.01 * 6.610647678375244
Epoch 700, val loss: 1.264934778213501
Epoch 710, training loss: 0.07286291569471359 = 0.0068080550990998745 + 0.01 * 6.605485916137695
Epoch 710, val loss: 1.272509217262268
Epoch 720, training loss: 0.07257191091775894 = 0.006548628676682711 + 0.01 * 6.602328777313232
Epoch 720, val loss: 1.279871940612793
Epoch 730, training loss: 0.07229945808649063 = 0.006305285729467869 + 0.01 * 6.599417209625244
Epoch 730, val loss: 1.287139654159546
Epoch 740, training loss: 0.07211478799581528 = 0.006076680961996317 + 0.01 * 6.603811264038086
Epoch 740, val loss: 1.2941625118255615
Epoch 750, training loss: 0.07188146561384201 = 0.005861691199243069 + 0.01 * 6.601977825164795
Epoch 750, val loss: 1.3010801076889038
Epoch 760, training loss: 0.0715690553188324 = 0.0056594498455524445 + 0.01 * 6.590960502624512
Epoch 760, val loss: 1.3077917098999023
Epoch 770, training loss: 0.07139788568019867 = 0.0054686167277395725 + 0.01 * 6.592926979064941
Epoch 770, val loss: 1.3143970966339111
Epoch 780, training loss: 0.07102365046739578 = 0.005288700573146343 + 0.01 * 6.573495388031006
Epoch 780, val loss: 1.3207937479019165
Epoch 790, training loss: 0.0708724781870842 = 0.005118594039231539 + 0.01 * 6.5753889083862305
Epoch 790, val loss: 1.3270610570907593
Epoch 800, training loss: 0.07067606598138809 = 0.004957639612257481 + 0.01 * 6.571843147277832
Epoch 800, val loss: 1.3332557678222656
Epoch 810, training loss: 0.07066122442483902 = 0.004805288277566433 + 0.01 * 6.5855937004089355
Epoch 810, val loss: 1.339249610900879
Epoch 820, training loss: 0.07028160989284515 = 0.004661087412387133 + 0.01 * 6.562051773071289
Epoch 820, val loss: 1.3451406955718994
Epoch 830, training loss: 0.07043728232383728 = 0.00452425004914403 + 0.01 * 6.59130334854126
Epoch 830, val loss: 1.3509410619735718
Epoch 840, training loss: 0.06997383385896683 = 0.004394128452986479 + 0.01 * 6.5579705238342285
Epoch 840, val loss: 1.3565467596054077
Epoch 850, training loss: 0.0697551965713501 = 0.004270675592124462 + 0.01 * 6.548452854156494
Epoch 850, val loss: 1.3621114492416382
Epoch 860, training loss: 0.06995651125907898 = 0.004153154790401459 + 0.01 * 6.580336093902588
Epoch 860, val loss: 1.3674938678741455
Epoch 870, training loss: 0.0695447027683258 = 0.004041275475174189 + 0.01 * 6.550343036651611
Epoch 870, val loss: 1.3728312253952026
Epoch 880, training loss: 0.06930513679981232 = 0.003934710286557674 + 0.01 * 6.537042617797852
Epoch 880, val loss: 1.378056287765503
Epoch 890, training loss: 0.06933081895112991 = 0.0038330117240548134 + 0.01 * 6.54978084564209
Epoch 890, val loss: 1.3831872940063477
Epoch 900, training loss: 0.06913718581199646 = 0.0037359632551670074 + 0.01 * 6.540122032165527
Epoch 900, val loss: 1.388188362121582
Epoch 910, training loss: 0.06905938684940338 = 0.0036431741900742054 + 0.01 * 6.541621208190918
Epoch 910, val loss: 1.3931084871292114
Epoch 920, training loss: 0.06887967884540558 = 0.003554603783413768 + 0.01 * 6.532507419586182
Epoch 920, val loss: 1.3979231119155884
Epoch 930, training loss: 0.06881573796272278 = 0.003469920950010419 + 0.01 * 6.534581661224365
Epoch 930, val loss: 1.4026720523834229
Epoch 940, training loss: 0.06862736493349075 = 0.0033886884339153767 + 0.01 * 6.523868083953857
Epoch 940, val loss: 1.4073047637939453
Epoch 950, training loss: 0.0685213953256607 = 0.0033111735247075558 + 0.01 * 6.521022796630859
Epoch 950, val loss: 1.4118796586990356
Epoch 960, training loss: 0.06846605241298676 = 0.0032367536332458258 + 0.01 * 6.522929668426514
Epoch 960, val loss: 1.4163577556610107
Epoch 970, training loss: 0.06857100874185562 = 0.0031652424950152636 + 0.01 * 6.540576457977295
Epoch 970, val loss: 1.4207167625427246
Epoch 980, training loss: 0.06819920241832733 = 0.0030967413913458586 + 0.01 * 6.5102458000183105
Epoch 980, val loss: 1.4250617027282715
Epoch 990, training loss: 0.06817058473825455 = 0.003031218657270074 + 0.01 * 6.513936996459961
Epoch 990, val loss: 1.4292975664138794
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0195436477661133 = 1.9358046054840088 + 0.01 * 8.373900413513184
Epoch 0, val loss: 1.9340189695358276
Epoch 10, training loss: 2.0100760459899902 = 1.926337718963623 + 0.01 * 8.373838424682617
Epoch 10, val loss: 1.9251668453216553
Epoch 20, training loss: 1.9987670183181763 = 1.915030837059021 + 0.01 * 8.373620986938477
Epoch 20, val loss: 1.9143409729003906
Epoch 30, training loss: 1.9832829236984253 = 1.8995529413223267 + 0.01 * 8.37299919128418
Epoch 30, val loss: 1.8993462324142456
Epoch 40, training loss: 1.9606165885925293 = 1.876912236213684 + 0.01 * 8.370429992675781
Epoch 40, val loss: 1.8775627613067627
Epoch 50, training loss: 1.9278502464294434 = 1.8443115949630737 + 0.01 * 8.353869438171387
Epoch 50, val loss: 1.8473328351974487
Epoch 60, training loss: 1.8850133419036865 = 1.802490234375 + 0.01 * 8.252310752868652
Epoch 60, val loss: 1.8114755153656006
Epoch 70, training loss: 1.8340038061141968 = 1.756603717803955 + 0.01 * 7.74000883102417
Epoch 70, val loss: 1.7747617959976196
Epoch 80, training loss: 1.7769474983215332 = 1.7019870281219482 + 0.01 * 7.496053218841553
Epoch 80, val loss: 1.728499412536621
Epoch 90, training loss: 1.7005672454833984 = 1.6279247999191284 + 0.01 * 7.264246940612793
Epoch 90, val loss: 1.6648606061935425
Epoch 100, training loss: 1.6016409397125244 = 1.53006911277771 + 0.01 * 7.157176971435547
Epoch 100, val loss: 1.5845565795898438
Epoch 110, training loss: 1.4825975894927979 = 1.411759376525879 + 0.01 * 7.083815574645996
Epoch 110, val loss: 1.489503026008606
Epoch 120, training loss: 1.3546628952026367 = 1.2845017910003662 + 0.01 * 7.016113758087158
Epoch 120, val loss: 1.387685775756836
Epoch 130, training loss: 1.2311487197875977 = 1.1614872217178345 + 0.01 * 6.966145038604736
Epoch 130, val loss: 1.29047429561615
Epoch 140, training loss: 1.1209934949874878 = 1.0516492128372192 + 0.01 * 6.934432506561279
Epoch 140, val loss: 1.205416202545166
Epoch 150, training loss: 1.0258339643478394 = 0.9567151069641113 + 0.01 * 6.9118828773498535
Epoch 150, val loss: 1.1327232122421265
Epoch 160, training loss: 0.9415175914764404 = 0.8725684881210327 + 0.01 * 6.89491081237793
Epoch 160, val loss: 1.069435954093933
Epoch 170, training loss: 0.8631091713905334 = 0.7942615151405334 + 0.01 * 6.884765625
Epoch 170, val loss: 1.0112428665161133
Epoch 180, training loss: 0.7873066067695618 = 0.7185093760490417 + 0.01 * 6.879724502563477
Epoch 180, val loss: 0.9555941224098206
Epoch 190, training loss: 0.7128284573554993 = 0.644066333770752 + 0.01 * 6.876214027404785
Epoch 190, val loss: 0.9019932746887207
Epoch 200, training loss: 0.6400123834609985 = 0.5712881088256836 + 0.01 * 6.872427463531494
Epoch 200, val loss: 0.8512399196624756
Epoch 210, training loss: 0.5702056884765625 = 0.5015305280685425 + 0.01 * 6.867517948150635
Epoch 210, val loss: 0.8055863380432129
Epoch 220, training loss: 0.5050369501113892 = 0.43642228841781616 + 0.01 * 6.861465930938721
Epoch 220, val loss: 0.7670122385025024
Epoch 230, training loss: 0.4459441602230072 = 0.3774007260799408 + 0.01 * 6.854342937469482
Epoch 230, val loss: 0.7363203167915344
Epoch 240, training loss: 0.3935580253601074 = 0.3250690996646881 + 0.01 * 6.848894119262695
Epoch 240, val loss: 0.7134323716163635
Epoch 250, training loss: 0.3476852774620056 = 0.2792990803718567 + 0.01 * 6.838618278503418
Epoch 250, val loss: 0.6976742148399353
Epoch 260, training loss: 0.30791476368904114 = 0.2396058589220047 + 0.01 * 6.8308916091918945
Epoch 260, val loss: 0.6880795955657959
Epoch 270, training loss: 0.27367863059043884 = 0.20545311272144318 + 0.01 * 6.82255220413208
Epoch 270, val loss: 0.6839378476142883
Epoch 280, training loss: 0.24451680481433868 = 0.17636924982070923 + 0.01 * 6.814755916595459
Epoch 280, val loss: 0.6846343278884888
Epoch 290, training loss: 0.2199162244796753 = 0.15183448791503906 + 0.01 * 6.808172702789307
Epoch 290, val loss: 0.6895080804824829
Epoch 300, training loss: 0.19930890202522278 = 0.13128547370433807 + 0.01 * 6.802341938018799
Epoch 300, val loss: 0.6978167295455933
Epoch 310, training loss: 0.18207022547721863 = 0.11410656571388245 + 0.01 * 6.796366214752197
Epoch 310, val loss: 0.7088738083839417
Epoch 320, training loss: 0.16764947772026062 = 0.09973105043172836 + 0.01 * 6.791842460632324
Epoch 320, val loss: 0.7220355272293091
Epoch 330, training loss: 0.15556856989860535 = 0.08766629546880722 + 0.01 * 6.790227890014648
Epoch 330, val loss: 0.73665452003479
Epoch 340, training loss: 0.1453302800655365 = 0.07749957591295242 + 0.01 * 6.783071517944336
Epoch 340, val loss: 0.7523245215415955
Epoch 350, training loss: 0.13667963445186615 = 0.06888752430677414 + 0.01 * 6.779211521148682
Epoch 350, val loss: 0.7685946226119995
Epoch 360, training loss: 0.12926533818244934 = 0.061549000442028046 + 0.01 * 6.771633625030518
Epoch 360, val loss: 0.7852463126182556
Epoch 370, training loss: 0.12296189367771149 = 0.05526289716362953 + 0.01 * 6.769899368286133
Epoch 370, val loss: 0.8019875884056091
Epoch 380, training loss: 0.11747534573078156 = 0.04984387382864952 + 0.01 * 6.763147354125977
Epoch 380, val loss: 0.8187726140022278
Epoch 390, training loss: 0.11269573867321014 = 0.04514092206954956 + 0.01 * 6.755481719970703
Epoch 390, val loss: 0.8354568481445312
Epoch 400, training loss: 0.1085880771279335 = 0.04103996604681015 + 0.01 * 6.7548112869262695
Epoch 400, val loss: 0.8518646359443665
Epoch 410, training loss: 0.10489220172166824 = 0.037448398768901825 + 0.01 * 6.744380474090576
Epoch 410, val loss: 0.8680506944656372
Epoch 420, training loss: 0.10166503489017487 = 0.034287478774785995 + 0.01 * 6.73775577545166
Epoch 420, val loss: 0.8839123249053955
Epoch 430, training loss: 0.09878027439117432 = 0.03149252012372017 + 0.01 * 6.728775501251221
Epoch 430, val loss: 0.8993840217590332
Epoch 440, training loss: 0.09644640982151031 = 0.029013635590672493 + 0.01 * 6.743277549743652
Epoch 440, val loss: 0.9145122170448303
Epoch 450, training loss: 0.09401626139879227 = 0.02680971287190914 + 0.01 * 6.7206549644470215
Epoch 450, val loss: 0.9292309880256653
Epoch 460, training loss: 0.09195724129676819 = 0.024840649217367172 + 0.01 * 6.7116594314575195
Epoch 460, val loss: 0.9435278177261353
Epoch 470, training loss: 0.09013181179761887 = 0.02307479828596115 + 0.01 * 6.7057013511657715
Epoch 470, val loss: 0.9575153589248657
Epoch 480, training loss: 0.08851639926433563 = 0.021486183628439903 + 0.01 * 6.70302152633667
Epoch 480, val loss: 0.9711474180221558
Epoch 490, training loss: 0.08711030334234238 = 0.020055167376995087 + 0.01 * 6.705513954162598
Epoch 490, val loss: 0.984324038028717
Epoch 500, training loss: 0.08562628924846649 = 0.018761390820145607 + 0.01 * 6.686490535736084
Epoch 500, val loss: 0.9971101880073547
Epoch 510, training loss: 0.08439610153436661 = 0.017587900161743164 + 0.01 * 6.680819988250732
Epoch 510, val loss: 1.009553074836731
Epoch 520, training loss: 0.08336563408374786 = 0.016520410776138306 + 0.01 * 6.68452262878418
Epoch 520, val loss: 1.0217069387435913
Epoch 530, training loss: 0.08232985436916351 = 0.015547572635114193 + 0.01 * 6.678228378295898
Epoch 530, val loss: 1.0334980487823486
Epoch 540, training loss: 0.08131136000156403 = 0.014658275991678238 + 0.01 * 6.665308952331543
Epoch 540, val loss: 1.044997215270996
Epoch 550, training loss: 0.08064065128564835 = 0.013843497261404991 + 0.01 * 6.679715156555176
Epoch 550, val loss: 1.0561116933822632
Epoch 560, training loss: 0.07964228093624115 = 0.013096502050757408 + 0.01 * 6.654577732086182
Epoch 560, val loss: 1.0669811964035034
Epoch 570, training loss: 0.07892406731843948 = 0.012409075163304806 + 0.01 * 6.651499271392822
Epoch 570, val loss: 1.0776019096374512
Epoch 580, training loss: 0.07825280725955963 = 0.01177472434937954 + 0.01 * 6.647808074951172
Epoch 580, val loss: 1.0878689289093018
Epoch 590, training loss: 0.07766880840063095 = 0.011188718490302563 + 0.01 * 6.648009300231934
Epoch 590, val loss: 1.0978968143463135
Epoch 600, training loss: 0.07700782269239426 = 0.01064674649387598 + 0.01 * 6.636107921600342
Epoch 600, val loss: 1.1076858043670654
Epoch 610, training loss: 0.07652688771486282 = 0.010144020430743694 + 0.01 * 6.63828706741333
Epoch 610, val loss: 1.1172599792480469
Epoch 620, training loss: 0.07598428428173065 = 0.009676788002252579 + 0.01 * 6.6307501792907715
Epoch 620, val loss: 1.1265803575515747
Epoch 630, training loss: 0.07555045932531357 = 0.009242038242518902 + 0.01 * 6.630842685699463
Epoch 630, val loss: 1.135604977607727
Epoch 640, training loss: 0.07507961988449097 = 0.00883736927062273 + 0.01 * 6.624224662780762
Epoch 640, val loss: 1.14443838596344
Epoch 650, training loss: 0.07467065751552582 = 0.008459627628326416 + 0.01 * 6.621102809906006
Epoch 650, val loss: 1.1532013416290283
Epoch 660, training loss: 0.07433062046766281 = 0.008106677792966366 + 0.01 * 6.62239408493042
Epoch 660, val loss: 1.1615263223648071
Epoch 670, training loss: 0.07384100556373596 = 0.007776683196425438 + 0.01 * 6.6064324378967285
Epoch 670, val loss: 1.1697907447814941
Epoch 680, training loss: 0.07371672242879868 = 0.007467468269169331 + 0.01 * 6.624925136566162
Epoch 680, val loss: 1.1778095960617065
Epoch 690, training loss: 0.07316172868013382 = 0.007177602965384722 + 0.01 * 6.598412036895752
Epoch 690, val loss: 1.1856673955917358
Epoch 700, training loss: 0.07290374487638474 = 0.006905328948050737 + 0.01 * 6.599841117858887
Epoch 700, val loss: 1.193355917930603
Epoch 710, training loss: 0.07256471365690231 = 0.006649368442595005 + 0.01 * 6.5915350914001465
Epoch 710, val loss: 1.200894832611084
Epoch 720, training loss: 0.07247815281152725 = 0.0064082215540111065 + 0.01 * 6.606993198394775
Epoch 720, val loss: 1.208130955696106
Epoch 730, training loss: 0.07202741503715515 = 0.006181283853948116 + 0.01 * 6.58461332321167
Epoch 730, val loss: 1.2153640985488892
Epoch 740, training loss: 0.07191307097673416 = 0.0059670028276741505 + 0.01 * 6.594606399536133
Epoch 740, val loss: 1.2223354578018188
Epoch 750, training loss: 0.07153358310461044 = 0.005764728412032127 + 0.01 * 6.57688570022583
Epoch 750, val loss: 1.2292184829711914
Epoch 760, training loss: 0.07140965759754181 = 0.0055735185742378235 + 0.01 * 6.583613872528076
Epoch 760, val loss: 1.2359539270401
Epoch 770, training loss: 0.0711037665605545 = 0.005392604041844606 + 0.01 * 6.5711164474487305
Epoch 770, val loss: 1.2425042390823364
Epoch 780, training loss: 0.0711488276720047 = 0.005221148021519184 + 0.01 * 6.59276819229126
Epoch 780, val loss: 1.248953104019165
Epoch 790, training loss: 0.07062717527151108 = 0.005058864131569862 + 0.01 * 6.556831359863281
Epoch 790, val loss: 1.2551811933517456
Epoch 800, training loss: 0.0704914927482605 = 0.004904887638986111 + 0.01 * 6.558660507202148
Epoch 800, val loss: 1.2613517045974731
Epoch 810, training loss: 0.07025550305843353 = 0.004758226685225964 + 0.01 * 6.5497283935546875
Epoch 810, val loss: 1.2674446105957031
Epoch 820, training loss: 0.07022251188755035 = 0.004619251936674118 + 0.01 * 6.56032657623291
Epoch 820, val loss: 1.2731850147247314
Epoch 830, training loss: 0.06998515874147415 = 0.004487406928092241 + 0.01 * 6.549775123596191
Epoch 830, val loss: 1.2790626287460327
Epoch 840, training loss: 0.06985767185688019 = 0.004361737985163927 + 0.01 * 6.549592971801758
Epoch 840, val loss: 1.284644365310669
Epoch 850, training loss: 0.06968008726835251 = 0.004241853021085262 + 0.01 * 6.543823719024658
Epoch 850, val loss: 1.2903245687484741
Epoch 860, training loss: 0.06956437230110168 = 0.004127467051148415 + 0.01 * 6.5436906814575195
Epoch 860, val loss: 1.2956151962280273
Epoch 870, training loss: 0.06948335468769073 = 0.004018520470708609 + 0.01 * 6.546483516693115
Epoch 870, val loss: 1.3009785413742065
Epoch 880, training loss: 0.06920668482780457 = 0.003914481494575739 + 0.01 * 6.5292205810546875
Epoch 880, val loss: 1.3062191009521484
Epoch 890, training loss: 0.06939808279275894 = 0.0038151461631059647 + 0.01 * 6.558294296264648
Epoch 890, val loss: 1.3112529516220093
Epoch 900, training loss: 0.06898882240056992 = 0.003720225067809224 + 0.01 * 6.526860237121582
Epoch 900, val loss: 1.3163042068481445
Epoch 910, training loss: 0.06883200258016586 = 0.0036293664015829563 + 0.01 * 6.520263671875
Epoch 910, val loss: 1.3212672472000122
Epoch 920, training loss: 0.06871118396520615 = 0.003542265621945262 + 0.01 * 6.516891956329346
Epoch 920, val loss: 1.3261009454727173
Epoch 930, training loss: 0.06859104335308075 = 0.0034589101560413837 + 0.01 * 6.513213634490967
Epoch 930, val loss: 1.330836296081543
Epoch 940, training loss: 0.06850112974643707 = 0.0033790715970098972 + 0.01 * 6.512206554412842
Epoch 940, val loss: 1.3354369401931763
Epoch 950, training loss: 0.06837396323680878 = 0.0033025031443685293 + 0.01 * 6.50714635848999
Epoch 950, val loss: 1.3400826454162598
Epoch 960, training loss: 0.06839118897914886 = 0.003229104680940509 + 0.01 * 6.516208648681641
Epoch 960, val loss: 1.344480276107788
Epoch 970, training loss: 0.06828111410140991 = 0.0031586738768965006 + 0.01 * 6.51224422454834
Epoch 970, val loss: 1.3489288091659546
Epoch 980, training loss: 0.0682375505566597 = 0.003090905724093318 + 0.01 * 6.514664649963379
Epoch 980, val loss: 1.3533183336257935
Epoch 990, training loss: 0.06797976046800613 = 0.003025873564183712 + 0.01 * 6.495388507843018
Epoch 990, val loss: 1.3575166463851929
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.041844367980957 = 1.9581055641174316 + 0.01 * 8.37389087677002
Epoch 0, val loss: 1.953032374382019
Epoch 10, training loss: 2.0305564403533936 = 1.946818232536316 + 0.01 * 8.373824119567871
Epoch 10, val loss: 1.942071795463562
Epoch 20, training loss: 2.0163519382476807 = 1.932615876197815 + 0.01 * 8.373602867126465
Epoch 20, val loss: 1.9277671575546265
Epoch 30, training loss: 1.995766520500183 = 1.9120383262634277 + 0.01 * 8.372822761535645
Epoch 30, val loss: 1.9066365957260132
Epoch 40, training loss: 1.965086817741394 = 1.8814091682434082 + 0.01 * 8.367767333984375
Epoch 40, val loss: 1.875386118888855
Epoch 50, training loss: 1.9230151176452637 = 1.8396729230880737 + 0.01 * 8.334222793579102
Epoch 50, val loss: 1.83513605594635
Epoch 60, training loss: 1.878326177597046 = 1.7965304851531982 + 0.01 * 8.179574966430664
Epoch 60, val loss: 1.799843192100525
Epoch 70, training loss: 1.839707612991333 = 1.7603251934051514 + 0.01 * 7.9382476806640625
Epoch 70, val loss: 1.7743911743164062
Epoch 80, training loss: 1.7912030220031738 = 1.7129074335098267 + 0.01 * 7.82956075668335
Epoch 80, val loss: 1.7359400987625122
Epoch 90, training loss: 1.7247440814971924 = 1.6476773023605347 + 0.01 * 7.706679344177246
Epoch 90, val loss: 1.6807901859283447
Epoch 100, training loss: 1.6400830745697021 = 1.5641849040985107 + 0.01 * 7.589820861816406
Epoch 100, val loss: 1.6111875772476196
Epoch 110, training loss: 1.5479856729507446 = 1.4740291833877563 + 0.01 * 7.395654201507568
Epoch 110, val loss: 1.5382623672485352
Epoch 120, training loss: 1.458254337310791 = 1.3866393566131592 + 0.01 * 7.161499977111816
Epoch 120, val loss: 1.4727668762207031
Epoch 130, training loss: 1.3738179206848145 = 1.3026695251464844 + 0.01 * 7.114835739135742
Epoch 130, val loss: 1.4147189855575562
Epoch 140, training loss: 1.2900625467300415 = 1.2193684577941895 + 0.01 * 7.069411754608154
Epoch 140, val loss: 1.3597614765167236
Epoch 150, training loss: 1.2066200971603394 = 1.136411428451538 + 0.01 * 7.020869731903076
Epoch 150, val loss: 1.305509090423584
Epoch 160, training loss: 1.1250839233398438 = 1.0554136037826538 + 0.01 * 6.9670305252075195
Epoch 160, val loss: 1.2516534328460693
Epoch 170, training loss: 1.0472978353500366 = 0.9781007766723633 + 0.01 * 6.919704437255859
Epoch 170, val loss: 1.1989543437957764
Epoch 180, training loss: 0.9743912816047668 = 0.905525267124176 + 0.01 * 6.886599540710449
Epoch 180, val loss: 1.1487001180648804
Epoch 190, training loss: 0.9067475199699402 = 0.8380315899848938 + 0.01 * 6.871592044830322
Epoch 190, val loss: 1.1026840209960938
Epoch 200, training loss: 0.8439920544624329 = 0.7753238081932068 + 0.01 * 6.866824150085449
Epoch 200, val loss: 1.061981201171875
Epoch 210, training loss: 0.7852568030357361 = 0.7166284918785095 + 0.01 * 6.862833023071289
Epoch 210, val loss: 1.0261656045913696
Epoch 220, training loss: 0.7290962338447571 = 0.6604946851730347 + 0.01 * 6.860157012939453
Epoch 220, val loss: 0.993667483329773
Epoch 230, training loss: 0.6734534502029419 = 0.6048704385757446 + 0.01 * 6.858304500579834
Epoch 230, val loss: 0.9621446132659912
Epoch 240, training loss: 0.6163756251335144 = 0.5478013753890991 + 0.01 * 6.857425689697266
Epoch 240, val loss: 0.9296949505805969
Epoch 250, training loss: 0.556846559047699 = 0.4882734715938568 + 0.01 * 6.857310771942139
Epoch 250, val loss: 0.8954733610153198
Epoch 260, training loss: 0.4957202076911926 = 0.4271448254585266 + 0.01 * 6.857537746429443
Epoch 260, val loss: 0.8607795834541321
Epoch 270, training loss: 0.4354677200317383 = 0.36688995361328125 + 0.01 * 6.857775688171387
Epoch 270, val loss: 0.8284713625907898
Epoch 280, training loss: 0.3792087435722351 = 0.31063112616539 + 0.01 * 6.857762336730957
Epoch 280, val loss: 0.8017070889472961
Epoch 290, training loss: 0.3294216990470886 = 0.2608456313610077 + 0.01 * 6.857608318328857
Epoch 290, val loss: 0.782438337802887
Epoch 300, training loss: 0.2870977222919464 = 0.21853430569171906 + 0.01 * 6.8563408851623535
Epoch 300, val loss: 0.7706668376922607
Epoch 310, training loss: 0.2519720196723938 = 0.1834297627210617 + 0.01 * 6.854226112365723
Epoch 310, val loss: 0.765300452709198
Epoch 320, training loss: 0.22317171096801758 = 0.15465566515922546 + 0.01 * 6.85160493850708
Epoch 320, val loss: 0.7648795247077942
Epoch 330, training loss: 0.199675053358078 = 0.13120628893375397 + 0.01 * 6.8468756675720215
Epoch 330, val loss: 0.7681598663330078
Epoch 340, training loss: 0.18048906326293945 = 0.11207559704780579 + 0.01 * 6.841347694396973
Epoch 340, val loss: 0.774147629737854
Epoch 350, training loss: 0.16475041210651398 = 0.09640660881996155 + 0.01 * 6.834380626678467
Epoch 350, val loss: 0.782197892665863
Epoch 360, training loss: 0.15180513262748718 = 0.08351173996925354 + 0.01 * 6.829339981079102
Epoch 360, val loss: 0.791816234588623
Epoch 370, training loss: 0.14104177057743073 = 0.07284093648195267 + 0.01 * 6.8200836181640625
Epoch 370, val loss: 0.802672803401947
Epoch 380, training loss: 0.1320895254611969 = 0.06395468860864639 + 0.01 * 6.813482761383057
Epoch 380, val loss: 0.814444899559021
Epoch 390, training loss: 0.12460392713546753 = 0.05651136115193367 + 0.01 * 6.8092570304870605
Epoch 390, val loss: 0.8268676400184631
Epoch 400, training loss: 0.11825881898403168 = 0.050235316157341 + 0.01 * 6.8023505210876465
Epoch 400, val loss: 0.8397740721702576
Epoch 410, training loss: 0.11289726197719574 = 0.04490976780653 + 0.01 * 6.798749923706055
Epoch 410, val loss: 0.8528520464897156
Epoch 420, training loss: 0.1083092987537384 = 0.04036249220371246 + 0.01 * 6.794681072235107
Epoch 420, val loss: 0.8660211563110352
Epoch 430, training loss: 0.10438099503517151 = 0.036457180976867676 + 0.01 * 6.792381763458252
Epoch 430, val loss: 0.8791207671165466
Epoch 440, training loss: 0.10099153220653534 = 0.03308402746915817 + 0.01 * 6.790750980377197
Epoch 440, val loss: 0.8919720649719238
Epoch 450, training loss: 0.09801231324672699 = 0.030154120177030563 + 0.01 * 6.785819053649902
Epoch 450, val loss: 0.9045751690864563
Epoch 460, training loss: 0.09541011601686478 = 0.027595309540629387 + 0.01 * 6.78148078918457
Epoch 460, val loss: 0.9168480634689331
Epoch 470, training loss: 0.09317737072706223 = 0.025348922237753868 + 0.01 * 6.7828450202941895
Epoch 470, val loss: 0.9287480711936951
Epoch 480, training loss: 0.09114042669534683 = 0.023367634043097496 + 0.01 * 6.777278900146484
Epoch 480, val loss: 0.9403429627418518
Epoch 490, training loss: 0.08934412896633148 = 0.021611500531435013 + 0.01 * 6.773263454437256
Epoch 490, val loss: 0.951540470123291
Epoch 500, training loss: 0.08773145824670792 = 0.02004796266555786 + 0.01 * 6.768349647521973
Epoch 500, val loss: 0.962390124797821
Epoch 510, training loss: 0.08641356974840164 = 0.018649959936738014 + 0.01 * 6.776361465454102
Epoch 510, val loss: 0.9729026556015015
Epoch 520, training loss: 0.08501911163330078 = 0.017396029084920883 + 0.01 * 6.7623090744018555
Epoch 520, val loss: 0.9831445813179016
Epoch 530, training loss: 0.08385612070560455 = 0.016267240047454834 + 0.01 * 6.758888244628906
Epoch 530, val loss: 0.9930058121681213
Epoch 540, training loss: 0.08280622959136963 = 0.015247331000864506 + 0.01 * 6.755889892578125
Epoch 540, val loss: 1.0026710033416748
Epoch 550, training loss: 0.0818420872092247 = 0.014322427101433277 + 0.01 * 6.751965522766113
Epoch 550, val loss: 1.011979103088379
Epoch 560, training loss: 0.08097848296165466 = 0.013481094501912594 + 0.01 * 6.749738693237305
Epoch 560, val loss: 1.0210692882537842
Epoch 570, training loss: 0.0801701694726944 = 0.012714121490716934 + 0.01 * 6.745604991912842
Epoch 570, val loss: 1.0298963785171509
Epoch 580, training loss: 0.07944446802139282 = 0.01201315876096487 + 0.01 * 6.743131160736084
Epoch 580, val loss: 1.0384547710418701
Epoch 590, training loss: 0.0787668228149414 = 0.011370901949703693 + 0.01 * 6.739591598510742
Epoch 590, val loss: 1.0467779636383057
Epoch 600, training loss: 0.07816314697265625 = 0.010780762881040573 + 0.01 * 6.7382378578186035
Epoch 600, val loss: 1.0548502206802368
Epoch 610, training loss: 0.07757102698087692 = 0.010237258858978748 + 0.01 * 6.733376979827881
Epoch 610, val loss: 1.0626944303512573
Epoch 620, training loss: 0.077081099152565 = 0.009735798463225365 + 0.01 * 6.734529972076416
Epoch 620, val loss: 1.0703426599502563
Epoch 630, training loss: 0.07653972506523132 = 0.009272188879549503 + 0.01 * 6.726754188537598
Epoch 630, val loss: 1.0778089761734009
Epoch 640, training loss: 0.07608556747436523 = 0.008842811919748783 + 0.01 * 6.724275588989258
Epoch 640, val loss: 1.0850766897201538
Epoch 650, training loss: 0.0756138265132904 = 0.008444211445748806 + 0.01 * 6.71696138381958
Epoch 650, val loss: 1.0921728610992432
Epoch 660, training loss: 0.0752529576420784 = 0.008073651231825352 + 0.01 * 6.717930793762207
Epoch 660, val loss: 1.0990502834320068
Epoch 670, training loss: 0.07483597844839096 = 0.007728574797511101 + 0.01 * 6.710740089416504
Epoch 670, val loss: 1.1057971715927124
Epoch 680, training loss: 0.07471705973148346 = 0.00740656815469265 + 0.01 * 6.731049060821533
Epoch 680, val loss: 1.1122862100601196
Epoch 690, training loss: 0.07417622953653336 = 0.007106274366378784 + 0.01 * 6.706995487213135
Epoch 690, val loss: 1.1187390089035034
Epoch 700, training loss: 0.07381026446819305 = 0.006825445685535669 + 0.01 * 6.698482036590576
Epoch 700, val loss: 1.1249078512191772
Epoch 710, training loss: 0.07351222634315491 = 0.0065621426329016685 + 0.01 * 6.695008754730225
Epoch 710, val loss: 1.1310091018676758
Epoch 720, training loss: 0.07328031957149506 = 0.006314842030405998 + 0.01 * 6.696547985076904
Epoch 720, val loss: 1.1369332075119019
Epoch 730, training loss: 0.07298145443201065 = 0.006082732696086168 + 0.01 * 6.6898722648620605
Epoch 730, val loss: 1.1427167654037476
Epoch 740, training loss: 0.07281792163848877 = 0.005864548962563276 + 0.01 * 6.695337772369385
Epoch 740, val loss: 1.1483606100082397
Epoch 750, training loss: 0.0724668875336647 = 0.005659143906086683 + 0.01 * 6.680775165557861
Epoch 750, val loss: 1.1538511514663696
Epoch 760, training loss: 0.07223525643348694 = 0.005465861409902573 + 0.01 * 6.676939964294434
Epoch 760, val loss: 1.1592568159103394
Epoch 770, training loss: 0.07199489325284958 = 0.005283372011035681 + 0.01 * 6.671152591705322
Epoch 770, val loss: 1.1645011901855469
Epoch 780, training loss: 0.07181217521429062 = 0.005111052189022303 + 0.01 * 6.670112609863281
Epoch 780, val loss: 1.1696029901504517
Epoch 790, training loss: 0.07159647345542908 = 0.004948009271174669 + 0.01 * 6.664846420288086
Epoch 790, val loss: 1.1747134923934937
Epoch 800, training loss: 0.07156415283679962 = 0.004793855827301741 + 0.01 * 6.677030086517334
Epoch 800, val loss: 1.1795916557312012
Epoch 810, training loss: 0.07111986726522446 = 0.004647588822990656 + 0.01 * 6.647227764129639
Epoch 810, val loss: 1.1844805479049683
Epoch 820, training loss: 0.07101857662200928 = 0.004508919548243284 + 0.01 * 6.650966167449951
Epoch 820, val loss: 1.189238429069519
Epoch 830, training loss: 0.07098949700593948 = 0.004377680364996195 + 0.01 * 6.661181926727295
Epoch 830, val loss: 1.1936709880828857
Epoch 840, training loss: 0.07061143964529037 = 0.0042530326172709465 + 0.01 * 6.635840892791748
Epoch 840, val loss: 1.1983140707015991
Epoch 850, training loss: 0.07086219638586044 = 0.004134729970246553 + 0.01 * 6.6727471351623535
Epoch 850, val loss: 1.2026731967926025
Epoch 860, training loss: 0.07025662064552307 = 0.004022051580250263 + 0.01 * 6.623456954956055
Epoch 860, val loss: 1.2070865631103516
Epoch 870, training loss: 0.07014969736337662 = 0.003914866130799055 + 0.01 * 6.623483657836914
Epoch 870, val loss: 1.2113128900527954
Epoch 880, training loss: 0.0699782446026802 = 0.0038127435836941004 + 0.01 * 6.616550445556641
Epoch 880, val loss: 1.2153741121292114
Epoch 890, training loss: 0.06988192349672318 = 0.0037152920849621296 + 0.01 * 6.616662979125977
Epoch 890, val loss: 1.2194225788116455
Epoch 900, training loss: 0.07001406699419022 = 0.003622401040047407 + 0.01 * 6.639166831970215
Epoch 900, val loss: 1.223130226135254
Epoch 910, training loss: 0.06961885094642639 = 0.0035339610185474157 + 0.01 * 6.608489513397217
Epoch 910, val loss: 1.2272928953170776
Epoch 920, training loss: 0.06932085752487183 = 0.003449543146416545 + 0.01 * 6.587131500244141
Epoch 920, val loss: 1.2310346364974976
Epoch 930, training loss: 0.06924513727426529 = 0.0033686840906739235 + 0.01 * 6.587645530700684
Epoch 930, val loss: 1.2346068620681763
Epoch 940, training loss: 0.06921794265508652 = 0.003291481174528599 + 0.01 * 6.592646598815918
Epoch 940, val loss: 1.23828125
Epoch 950, training loss: 0.06938392668962479 = 0.0032172526698559523 + 0.01 * 6.616667747497559
Epoch 950, val loss: 1.2418607473373413
Epoch 960, training loss: 0.06889014691114426 = 0.0031463962513953447 + 0.01 * 6.574375629425049
Epoch 960, val loss: 1.2451976537704468
Epoch 970, training loss: 0.06875307112932205 = 0.0030783808324486017 + 0.01 * 6.567469596862793
Epoch 970, val loss: 1.2485357522964478
Epoch 980, training loss: 0.06862097978591919 = 0.003013123758137226 + 0.01 * 6.56078577041626
Epoch 980, val loss: 1.2518627643585205
Epoch 990, training loss: 0.06864306330680847 = 0.002950409660115838 + 0.01 * 6.569265842437744
Epoch 990, val loss: 1.2551203966140747
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.7417
Flip ASR: 0.7022/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0362961292266846 = 1.9525580406188965 + 0.01 * 8.373805046081543
Epoch 0, val loss: 1.9424935579299927
Epoch 10, training loss: 2.0262515544891357 = 1.9425140619277954 + 0.01 * 8.373757362365723
Epoch 10, val loss: 1.9330464601516724
Epoch 20, training loss: 2.0144031047821045 = 1.930668592453003 + 0.01 * 8.373451232910156
Epoch 20, val loss: 1.9213900566101074
Epoch 30, training loss: 1.9980422258377075 = 1.9143180847167969 + 0.01 * 8.37241268157959
Epoch 30, val loss: 1.9050744771957397
Epoch 40, training loss: 1.9738224744796753 = 1.8901548385620117 + 0.01 * 8.366768836975098
Epoch 40, val loss: 1.881019115447998
Epoch 50, training loss: 1.938006043434143 = 1.8547272682189941 + 0.01 * 8.327879905700684
Epoch 50, val loss: 1.8466418981552124
Epoch 60, training loss: 1.8898833990097046 = 1.8090462684631348 + 0.01 * 8.083715438842773
Epoch 60, val loss: 1.8051165342330933
Epoch 70, training loss: 1.8380351066589355 = 1.7617396116256714 + 0.01 * 7.629546165466309
Epoch 70, val loss: 1.7646771669387817
Epoch 80, training loss: 1.7839404344558716 = 1.7095047235488892 + 0.01 * 7.443570137023926
Epoch 80, val loss: 1.7178620100021362
Epoch 90, training loss: 1.7119688987731934 = 1.6386399269104004 + 0.01 * 7.332901954650879
Epoch 90, val loss: 1.6543775796890259
Epoch 100, training loss: 1.618519902229309 = 1.546032428741455 + 0.01 * 7.248744964599609
Epoch 100, val loss: 1.5749446153640747
Epoch 110, training loss: 1.5076494216918945 = 1.4358081817626953 + 0.01 * 7.18412971496582
Epoch 110, val loss: 1.4838225841522217
Epoch 120, training loss: 1.39159095287323 = 1.320410132408142 + 0.01 * 7.118078231811523
Epoch 120, val loss: 1.3904976844787598
Epoch 130, training loss: 1.2794530391693115 = 1.209049105644226 + 0.01 * 7.04039192199707
Epoch 130, val loss: 1.3030188083648682
Epoch 140, training loss: 1.1762242317199707 = 1.1064472198486328 + 0.01 * 6.977704048156738
Epoch 140, val loss: 1.223706841468811
Epoch 150, training loss: 1.082991123199463 = 1.0135546922683716 + 0.01 * 6.9436469078063965
Epoch 150, val loss: 1.1522823572158813
Epoch 160, training loss: 1.0002968311309814 = 0.9310482144355774 + 0.01 * 6.924860000610352
Epoch 160, val loss: 1.0902084112167358
Epoch 170, training loss: 0.927966833114624 = 0.8589245676994324 + 0.01 * 6.904226303100586
Epoch 170, val loss: 1.038914442062378
Epoch 180, training loss: 0.8647641539573669 = 0.7959182858467102 + 0.01 * 6.884584426879883
Epoch 180, val loss: 0.9976980686187744
Epoch 190, training loss: 0.8086106777191162 = 0.7399047613143921 + 0.01 * 6.870594024658203
Epoch 190, val loss: 0.9650056958198547
Epoch 200, training loss: 0.757127583026886 = 0.6885031461715698 + 0.01 * 6.862444877624512
Epoch 200, val loss: 0.9385520219802856
Epoch 210, training loss: 0.7081145644187927 = 0.6395318508148193 + 0.01 * 6.858270645141602
Epoch 210, val loss: 0.9163132309913635
Epoch 220, training loss: 0.6599540710449219 = 0.591399073600769 + 0.01 * 6.855502128601074
Epoch 220, val loss: 0.8960481882095337
Epoch 230, training loss: 0.6117526292800903 = 0.5432173013687134 + 0.01 * 6.8535356521606445
Epoch 230, val loss: 0.8754889965057373
Epoch 240, training loss: 0.5633039474487305 = 0.4947862923145294 + 0.01 * 6.851762771606445
Epoch 240, val loss: 0.8531844615936279
Epoch 250, training loss: 0.5150854587554932 = 0.44658344984054565 + 0.01 * 6.85020112991333
Epoch 250, val loss: 0.8295075297355652
Epoch 260, training loss: 0.4681916832923889 = 0.39970138669013977 + 0.01 * 6.849031448364258
Epoch 260, val loss: 0.8059681057929993
Epoch 270, training loss: 0.42387354373931885 = 0.3554007411003113 + 0.01 * 6.847278594970703
Epoch 270, val loss: 0.7849562764167786
Epoch 280, training loss: 0.38310202956199646 = 0.314641535282135 + 0.01 * 6.846049785614014
Epoch 280, val loss: 0.7684244513511658
Epoch 290, training loss: 0.34625130891799927 = 0.27779948711395264 + 0.01 * 6.845180511474609
Epoch 290, val loss: 0.7571800351142883
Epoch 300, training loss: 0.31344565749168396 = 0.24499855935573578 + 0.01 * 6.844710826873779
Epoch 300, val loss: 0.751500129699707
Epoch 310, training loss: 0.2846560478210449 = 0.2162228226661682 + 0.01 * 6.843321800231934
Epoch 310, val loss: 0.7513941526412964
Epoch 320, training loss: 0.25972574949264526 = 0.19130264222621918 + 0.01 * 6.842311382293701
Epoch 320, val loss: 0.7562708258628845
Epoch 330, training loss: 0.23835641145706177 = 0.16994434595108032 + 0.01 * 6.841207504272461
Epoch 330, val loss: 0.765444815158844
Epoch 340, training loss: 0.22004041075706482 = 0.15162131190299988 + 0.01 * 6.841910362243652
Epoch 340, val loss: 0.7780206203460693
Epoch 350, training loss: 0.20419995486736298 = 0.13581432402133942 + 0.01 * 6.838563442230225
Epoch 350, val loss: 0.7931953072547913
Epoch 360, training loss: 0.19043630361557007 = 0.12206711620092392 + 0.01 * 6.836918830871582
Epoch 360, val loss: 0.8103450536727905
Epoch 370, training loss: 0.17836953699588776 = 0.11002323031425476 + 0.01 * 6.834630489349365
Epoch 370, val loss: 0.8290013670921326
Epoch 380, training loss: 0.1677415370941162 = 0.09940312802791595 + 0.01 * 6.833840847015381
Epoch 380, val loss: 0.8487352132797241
Epoch 390, training loss: 0.15828391909599304 = 0.08998890966176987 + 0.01 * 6.829502105712891
Epoch 390, val loss: 0.869232714176178
Epoch 400, training loss: 0.1498795747756958 = 0.08161083608865738 + 0.01 * 6.826874732971191
Epoch 400, val loss: 0.8902896046638489
Epoch 410, training loss: 0.14235475659370422 = 0.07413026690483093 + 0.01 * 6.822448253631592
Epoch 410, val loss: 0.9116986393928528
Epoch 420, training loss: 0.13561372458934784 = 0.06743168085813522 + 0.01 * 6.818204879760742
Epoch 420, val loss: 0.9332126975059509
Epoch 430, training loss: 0.12962405383586884 = 0.06141696125268936 + 0.01 * 6.820709228515625
Epoch 430, val loss: 0.9546729326248169
Epoch 440, training loss: 0.12409152090549469 = 0.05601111054420471 + 0.01 * 6.808041095733643
Epoch 440, val loss: 0.976078987121582
Epoch 450, training loss: 0.11914809048175812 = 0.05114370957016945 + 0.01 * 6.800438404083252
Epoch 450, val loss: 0.9972346425056458
Epoch 460, training loss: 0.1147034615278244 = 0.046760525554418564 + 0.01 * 6.794293403625488
Epoch 460, val loss: 1.0180317163467407
Epoch 470, training loss: 0.11072719842195511 = 0.04281705617904663 + 0.01 * 6.791014194488525
Epoch 470, val loss: 1.0385453701019287
Epoch 480, training loss: 0.1070433035492897 = 0.03926771879196167 + 0.01 * 6.77755880355835
Epoch 480, val loss: 1.058841347694397
Epoch 490, training loss: 0.10374574363231659 = 0.03607480973005295 + 0.01 * 6.767093181610107
Epoch 490, val loss: 1.0785856246948242
Epoch 500, training loss: 0.10077226907014847 = 0.03320607542991638 + 0.01 * 6.756619453430176
Epoch 500, val loss: 1.0979846715927124
Epoch 510, training loss: 0.0980541780591011 = 0.03062499314546585 + 0.01 * 6.742918968200684
Epoch 510, val loss: 1.1168595552444458
Epoch 520, training loss: 0.09584201127290726 = 0.028302744030952454 + 0.01 * 6.753926753997803
Epoch 520, val loss: 1.1351295709609985
Epoch 530, training loss: 0.09352676570415497 = 0.02622094191610813 + 0.01 * 6.7305827140808105
Epoch 530, val loss: 1.1529381275177002
Epoch 540, training loss: 0.09151403605937958 = 0.024346960708498955 + 0.01 * 6.716707229614258
Epoch 540, val loss: 1.170172929763794
Epoch 550, training loss: 0.08992543816566467 = 0.022656014189124107 + 0.01 * 6.72694206237793
Epoch 550, val loss: 1.186929702758789
Epoch 560, training loss: 0.08811604976654053 = 0.021129895001649857 + 0.01 * 6.698615550994873
Epoch 560, val loss: 1.203050136566162
Epoch 570, training loss: 0.08665883541107178 = 0.019746895879507065 + 0.01 * 6.6911940574646
Epoch 570, val loss: 1.2187843322753906
Epoch 580, training loss: 0.08524145185947418 = 0.018490318208932877 + 0.01 * 6.675113677978516
Epoch 580, val loss: 1.2341022491455078
Epoch 590, training loss: 0.08425308018922806 = 0.017350323498249054 + 0.01 * 6.6902756690979
Epoch 590, val loss: 1.2489368915557861
Epoch 600, training loss: 0.08294952660799026 = 0.016314685344696045 + 0.01 * 6.663484573364258
Epoch 600, val loss: 1.2632629871368408
Epoch 610, training loss: 0.08188458532094955 = 0.015369058586657047 + 0.01 * 6.651553153991699
Epoch 610, val loss: 1.2771919965744019
Epoch 620, training loss: 0.08102753758430481 = 0.014503415673971176 + 0.01 * 6.652411937713623
Epoch 620, val loss: 1.2907848358154297
Epoch 630, training loss: 0.08019618690013885 = 0.013709981925785542 + 0.01 * 6.64862060546875
Epoch 630, val loss: 1.3039820194244385
Epoch 640, training loss: 0.07934948056936264 = 0.012981000356376171 + 0.01 * 6.636848449707031
Epoch 640, val loss: 1.3168281316757202
Epoch 650, training loss: 0.0786191076040268 = 0.012309876270592213 + 0.01 * 6.630922794342041
Epoch 650, val loss: 1.3292351961135864
Epoch 660, training loss: 0.07800935953855515 = 0.01169202383607626 + 0.01 * 6.6317338943481445
Epoch 660, val loss: 1.3413323163986206
Epoch 670, training loss: 0.07725442200899124 = 0.011121225543320179 + 0.01 * 6.6133198738098145
Epoch 670, val loss: 1.3530535697937012
Epoch 680, training loss: 0.0768803283572197 = 0.010592936538159847 + 0.01 * 6.628738880157471
Epoch 680, val loss: 1.3644983768463135
Epoch 690, training loss: 0.07621628791093826 = 0.01010344922542572 + 0.01 * 6.611284255981445
Epoch 690, val loss: 1.3756498098373413
Epoch 700, training loss: 0.0757082849740982 = 0.009648576378822327 + 0.01 * 6.605971336364746
Epoch 700, val loss: 1.3864686489105225
Epoch 710, training loss: 0.07526317238807678 = 0.009224805980920792 + 0.01 * 6.603837013244629
Epoch 710, val loss: 1.3970849514007568
Epoch 720, training loss: 0.07480406761169434 = 0.00883011519908905 + 0.01 * 6.597395420074463
Epoch 720, val loss: 1.407320499420166
Epoch 730, training loss: 0.07432655245065689 = 0.008461842313408852 + 0.01 * 6.586471080780029
Epoch 730, val loss: 1.417359471321106
Epoch 740, training loss: 0.07405053079128265 = 0.008117898367345333 + 0.01 * 6.593263149261475
Epoch 740, val loss: 1.4271345138549805
Epoch 750, training loss: 0.07363152503967285 = 0.007796511985361576 + 0.01 * 6.58350133895874
Epoch 750, val loss: 1.4365789890289307
Epoch 760, training loss: 0.07327601313591003 = 0.007495330646634102 + 0.01 * 6.578068256378174
Epoch 760, val loss: 1.4458554983139038
Epoch 770, training loss: 0.07292256504297256 = 0.007212823256850243 + 0.01 * 6.570974349975586
Epoch 770, val loss: 1.454879641532898
Epoch 780, training loss: 0.07260897010564804 = 0.006947523448616266 + 0.01 * 6.5661444664001465
Epoch 780, val loss: 1.4636377096176147
Epoch 790, training loss: 0.07234197109937668 = 0.006697916425764561 + 0.01 * 6.564405918121338
Epoch 790, val loss: 1.4721322059631348
Epoch 800, training loss: 0.07203834503889084 = 0.006462867837399244 + 0.01 * 6.5575480461120605
Epoch 800, val loss: 1.4804925918579102
Epoch 810, training loss: 0.07184233516454697 = 0.00624118885025382 + 0.01 * 6.560114860534668
Epoch 810, val loss: 1.4886292219161987
Epoch 820, training loss: 0.07171189785003662 = 0.00603198679164052 + 0.01 * 6.567991256713867
Epoch 820, val loss: 1.4965265989303589
Epoch 830, training loss: 0.07131005078554153 = 0.005834439303725958 + 0.01 * 6.5475616455078125
Epoch 830, val loss: 1.5043110847473145
Epoch 840, training loss: 0.0712752714753151 = 0.005647578742355108 + 0.01 * 6.562769412994385
Epoch 840, val loss: 1.511823058128357
Epoch 850, training loss: 0.07085557281970978 = 0.005470689851790667 + 0.01 * 6.538488388061523
Epoch 850, val loss: 1.5192128419876099
Epoch 860, training loss: 0.07072710990905762 = 0.005303145386278629 + 0.01 * 6.542396545410156
Epoch 860, val loss: 1.5263962745666504
Epoch 870, training loss: 0.07057300955057144 = 0.005144206807017326 + 0.01 * 6.542880535125732
Epoch 870, val loss: 1.5333812236785889
Epoch 880, training loss: 0.07033618539571762 = 0.0049933213740587234 + 0.01 * 6.5342864990234375
Epoch 880, val loss: 1.540353775024414
Epoch 890, training loss: 0.07015825808048248 = 0.004849962890148163 + 0.01 * 6.530829906463623
Epoch 890, val loss: 1.5469822883605957
Epoch 900, training loss: 0.06997068971395493 = 0.004713599104434252 + 0.01 * 6.5257086753845215
Epoch 900, val loss: 1.5535085201263428
Epoch 910, training loss: 0.06977441161870956 = 0.004583807196468115 + 0.01 * 6.519061088562012
Epoch 910, val loss: 1.559893250465393
Epoch 920, training loss: 0.06966613978147507 = 0.004460223019123077 + 0.01 * 6.520591735839844
Epoch 920, val loss: 1.5661629438400269
Epoch 930, training loss: 0.06946493685245514 = 0.004342370200902224 + 0.01 * 6.512257099151611
Epoch 930, val loss: 1.5722600221633911
Epoch 940, training loss: 0.06942249089479446 = 0.004229998681694269 + 0.01 * 6.519249439239502
Epoch 940, val loss: 1.5782283544540405
Epoch 950, training loss: 0.0691736713051796 = 0.004122754093259573 + 0.01 * 6.505091667175293
Epoch 950, val loss: 1.5840303897857666
Epoch 960, training loss: 0.06918167322874069 = 0.004020335152745247 + 0.01 * 6.516134262084961
Epoch 960, val loss: 1.5897090435028076
Epoch 970, training loss: 0.06898799538612366 = 0.003922421485185623 + 0.01 * 6.506556987762451
Epoch 970, val loss: 1.595312476158142
Epoch 980, training loss: 0.06885461509227753 = 0.0038286561612039804 + 0.01 * 6.502595901489258
Epoch 980, val loss: 1.6007436513900757
Epoch 990, training loss: 0.06868506968021393 = 0.0037390217185020447 + 0.01 * 6.49460506439209
Epoch 990, val loss: 1.6060973405838013
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.8376
Flip ASR: 0.8089/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.035916328430176 = 1.9521777629852295 + 0.01 * 8.373860359191895
Epoch 0, val loss: 1.9476673603057861
Epoch 10, training loss: 2.025754690170288 = 1.942016363143921 + 0.01 * 8.373823165893555
Epoch 10, val loss: 1.9378042221069336
Epoch 20, training loss: 2.013864517211914 = 1.9301282167434692 + 0.01 * 8.373639106750488
Epoch 20, val loss: 1.9259306192398071
Epoch 30, training loss: 1.9974859952926636 = 1.9137552976608276 + 0.01 * 8.373072624206543
Epoch 30, val loss: 1.9093865156173706
Epoch 40, training loss: 1.9733937978744507 = 1.8896905183792114 + 0.01 * 8.370329856872559
Epoch 40, val loss: 1.8853108882904053
Epoch 50, training loss: 1.9382612705230713 = 1.8547778129577637 + 0.01 * 8.348342895507812
Epoch 50, val loss: 1.8515046834945679
Epoch 60, training loss: 1.8916833400726318 = 1.8094935417175293 + 0.01 * 8.218978881835938
Epoch 60, val loss: 1.8103407621383667
Epoch 70, training loss: 1.8383455276489258 = 1.7603833675384521 + 0.01 * 7.79622220993042
Epoch 70, val loss: 1.768146276473999
Epoch 80, training loss: 1.7820645570755005 = 1.7065073251724243 + 0.01 * 7.555724620819092
Epoch 80, val loss: 1.7202212810516357
Epoch 90, training loss: 1.709863543510437 = 1.636904239654541 + 0.01 * 7.295932769775391
Epoch 90, val loss: 1.6570873260498047
Epoch 100, training loss: 1.6188501119613647 = 1.547195553779602 + 0.01 * 7.165457725524902
Epoch 100, val loss: 1.5793304443359375
Epoch 110, training loss: 1.5102769136428833 = 1.439138412475586 + 0.01 * 7.113853454589844
Epoch 110, val loss: 1.490055799484253
Epoch 120, training loss: 1.3938263654708862 = 1.3229433298110962 + 0.01 * 7.088301181793213
Epoch 120, val loss: 1.3964767456054688
Epoch 130, training loss: 1.2768527269363403 = 1.2061378955841064 + 0.01 * 7.0714802742004395
Epoch 130, val loss: 1.3036260604858398
Epoch 140, training loss: 1.1629623174667358 = 1.0923857688903809 + 0.01 * 7.057659149169922
Epoch 140, val loss: 1.2136764526367188
Epoch 150, training loss: 1.0547780990600586 = 0.9843204617500305 + 0.01 * 7.0457682609558105
Epoch 150, val loss: 1.1275506019592285
Epoch 160, training loss: 0.9552464485168457 = 0.8849329352378845 + 0.01 * 7.0313496589660645
Epoch 160, val loss: 1.0482025146484375
Epoch 170, training loss: 0.8665550947189331 = 0.7964501976966858 + 0.01 * 7.010488033294678
Epoch 170, val loss: 0.9784578680992126
Epoch 180, training loss: 0.7892249822616577 = 0.7193890810012817 + 0.01 * 6.983593463897705
Epoch 180, val loss: 0.9197307229042053
Epoch 190, training loss: 0.7222147583961487 = 0.652646541595459 + 0.01 * 6.956821918487549
Epoch 190, val loss: 0.8717650175094604
Epoch 200, training loss: 0.6636295318603516 = 0.5942477583885193 + 0.01 * 6.938178539276123
Epoch 200, val loss: 0.8336185812950134
Epoch 210, training loss: 0.6116660833358765 = 0.5423603057861328 + 0.01 * 6.930578708648682
Epoch 210, val loss: 0.8033019304275513
Epoch 220, training loss: 0.564958930015564 = 0.49572378396987915 + 0.01 * 6.92351770401001
Epoch 220, val loss: 0.7794381380081177
Epoch 230, training loss: 0.5226685404777527 = 0.45348572731018066 + 0.01 * 6.918280124664307
Epoch 230, val loss: 0.7608411312103271
Epoch 240, training loss: 0.48410308361053467 = 0.4149652421474457 + 0.01 * 6.913783073425293
Epoch 240, val loss: 0.7465471029281616
Epoch 250, training loss: 0.448682963848114 = 0.37958836555480957 + 0.01 * 6.909460544586182
Epoch 250, val loss: 0.7360444068908691
Epoch 260, training loss: 0.415995717048645 = 0.34694135189056396 + 0.01 * 6.905435562133789
Epoch 260, val loss: 0.7288371324539185
Epoch 270, training loss: 0.3857995569705963 = 0.3167841136455536 + 0.01 * 6.901544094085693
Epoch 270, val loss: 0.7244566082954407
Epoch 280, training loss: 0.357974648475647 = 0.2889905571937561 + 0.01 * 6.898407936096191
Epoch 280, val loss: 0.7226927876472473
Epoch 290, training loss: 0.332242876291275 = 0.26329758763313293 + 0.01 * 6.894528388977051
Epoch 290, val loss: 0.7233145833015442
Epoch 300, training loss: 0.3081771731376648 = 0.23927631974220276 + 0.01 * 6.890084266662598
Epoch 300, val loss: 0.7257159352302551
Epoch 310, training loss: 0.28529495000839233 = 0.21643754839897156 + 0.01 * 6.885740280151367
Epoch 310, val loss: 0.7295576333999634
Epoch 320, training loss: 0.26328393816947937 = 0.19447487592697144 + 0.01 * 6.88090705871582
Epoch 320, val loss: 0.7345668077468872
Epoch 330, training loss: 0.24220618605613708 = 0.1734594702720642 + 0.01 * 6.87467098236084
Epoch 330, val loss: 0.7406852841377258
Epoch 340, training loss: 0.22254908084869385 = 0.15375642478466034 + 0.01 * 6.879264831542969
Epoch 340, val loss: 0.7477394342422485
Epoch 350, training loss: 0.20438508689403534 = 0.13574939966201782 + 0.01 * 6.8635687828063965
Epoch 350, val loss: 0.7559225559234619
Epoch 360, training loss: 0.18813127279281616 = 0.11960916221141815 + 0.01 * 6.8522114753723145
Epoch 360, val loss: 0.7650966644287109
Epoch 370, training loss: 0.17379267513751984 = 0.10529891401529312 + 0.01 * 6.849376201629639
Epoch 370, val loss: 0.7752876281738281
Epoch 380, training loss: 0.16108788549900055 = 0.0927329882979393 + 0.01 * 6.8354902267456055
Epoch 380, val loss: 0.7862110733985901
Epoch 390, training loss: 0.15004324913024902 = 0.08180513978004456 + 0.01 * 6.823812007904053
Epoch 390, val loss: 0.7978898286819458
Epoch 400, training loss: 0.14066749811172485 = 0.07235417515039444 + 0.01 * 6.831332683563232
Epoch 400, val loss: 0.8103369474411011
Epoch 410, training loss: 0.13231290876865387 = 0.0642227753996849 + 0.01 * 6.809013366699219
Epoch 410, val loss: 0.8234293460845947
Epoch 420, training loss: 0.1251506209373474 = 0.05722914636135101 + 0.01 * 6.792146682739258
Epoch 420, val loss: 0.8371457457542419
Epoch 430, training loss: 0.11916528642177582 = 0.05119774863123894 + 0.01 * 6.796753883361816
Epoch 430, val loss: 0.8514036536216736
Epoch 440, training loss: 0.11373090744018555 = 0.045988716185092926 + 0.01 * 6.774219036102295
Epoch 440, val loss: 0.8659148216247559
Epoch 450, training loss: 0.10919404774904251 = 0.041469983756542206 + 0.01 * 6.772406578063965
Epoch 450, val loss: 0.8805606365203857
Epoch 460, training loss: 0.10516956448554993 = 0.0375378280878067 + 0.01 * 6.763174057006836
Epoch 460, val loss: 0.8952791690826416
Epoch 470, training loss: 0.10155926644802094 = 0.03410070016980171 + 0.01 * 6.745857238769531
Epoch 470, val loss: 0.9099606275558472
Epoch 480, training loss: 0.09856754541397095 = 0.031086010858416557 + 0.01 * 6.748153209686279
Epoch 480, val loss: 0.9244707822799683
Epoch 490, training loss: 0.09578121453523636 = 0.028435632586479187 + 0.01 * 6.734558582305908
Epoch 490, val loss: 0.938686192035675
Epoch 500, training loss: 0.09344617277383804 = 0.0260954350233078 + 0.01 * 6.735074043273926
Epoch 500, val loss: 0.9526439309120178
Epoch 510, training loss: 0.09111955761909485 = 0.024023614823818207 + 0.01 * 6.709594249725342
Epoch 510, val loss: 0.9662984013557434
Epoch 520, training loss: 0.08916310220956802 = 0.022181177511811256 + 0.01 * 6.698192596435547
Epoch 520, val loss: 0.9795822501182556
Epoch 530, training loss: 0.08746412396430969 = 0.02053889073431492 + 0.01 * 6.69252347946167
Epoch 530, val loss: 0.9925176501274109
Epoch 540, training loss: 0.08598297834396362 = 0.019071064889431 + 0.01 * 6.691191673278809
Epoch 540, val loss: 1.0049769878387451
Epoch 550, training loss: 0.0847029834985733 = 0.017753757536411285 + 0.01 * 6.694922924041748
Epoch 550, val loss: 1.017275333404541
Epoch 560, training loss: 0.08336864411830902 = 0.01656796969473362 + 0.01 * 6.680067539215088
Epoch 560, val loss: 1.0290858745574951
Epoch 570, training loss: 0.08218751102685928 = 0.015498087741434574 + 0.01 * 6.668942451477051
Epoch 570, val loss: 1.0406923294067383
Epoch 580, training loss: 0.08121782541275024 = 0.014530696906149387 + 0.01 * 6.668713092803955
Epoch 580, val loss: 1.0518468618392944
Epoch 590, training loss: 0.0801914781332016 = 0.013652535155415535 + 0.01 * 6.653894901275635
Epoch 590, val loss: 1.0626722574234009
Epoch 600, training loss: 0.07929175347089767 = 0.01285446435213089 + 0.01 * 6.643729209899902
Epoch 600, val loss: 1.0732216835021973
Epoch 610, training loss: 0.07849384844303131 = 0.012126537039875984 + 0.01 * 6.636731147766113
Epoch 610, val loss: 1.0834380388259888
Epoch 620, training loss: 0.0777629092335701 = 0.011460311710834503 + 0.01 * 6.630259990692139
Epoch 620, val loss: 1.09333336353302
Epoch 630, training loss: 0.07708922773599625 = 0.010848936624825 + 0.01 * 6.624029159545898
Epoch 630, val loss: 1.1029551029205322
Epoch 640, training loss: 0.0766572654247284 = 0.010285076685249805 + 0.01 * 6.637218475341797
Epoch 640, val loss: 1.1123754978179932
Epoch 650, training loss: 0.07597416639328003 = 0.00976487435400486 + 0.01 * 6.62092924118042
Epoch 650, val loss: 1.1215441226959229
Epoch 660, training loss: 0.0754094198346138 = 0.009283586405217648 + 0.01 * 6.612583160400391
Epoch 660, val loss: 1.1304585933685303
Epoch 670, training loss: 0.07485052198171616 = 0.008836783468723297 + 0.01 * 6.601373672485352
Epoch 670, val loss: 1.139267921447754
Epoch 680, training loss: 0.07458817958831787 = 0.008420705795288086 + 0.01 * 6.616747856140137
Epoch 680, val loss: 1.147928237915039
Epoch 690, training loss: 0.07402420789003372 = 0.008032312616705894 + 0.01 * 6.599189281463623
Epoch 690, val loss: 1.1564089059829712
Epoch 700, training loss: 0.0736207440495491 = 0.00767036247998476 + 0.01 * 6.595037937164307
Epoch 700, val loss: 1.1646909713745117
Epoch 710, training loss: 0.0732283741235733 = 0.007331010419875383 + 0.01 * 6.589736461639404
Epoch 710, val loss: 1.1728472709655762
Epoch 720, training loss: 0.0732661634683609 = 0.007013213355094194 + 0.01 * 6.625295162200928
Epoch 720, val loss: 1.1807795763015747
Epoch 730, training loss: 0.07258369028568268 = 0.006717492826282978 + 0.01 * 6.5866193771362305
Epoch 730, val loss: 1.1886439323425293
Epoch 740, training loss: 0.07235172390937805 = 0.006439986173063517 + 0.01 * 6.591174125671387
Epoch 740, val loss: 1.196272611618042
Epoch 750, training loss: 0.07192075252532959 = 0.006180171854794025 + 0.01 * 6.5740580558776855
Epoch 750, val loss: 1.2038426399230957
Epoch 760, training loss: 0.0715966671705246 = 0.005935548339039087 + 0.01 * 6.566112041473389
Epoch 760, val loss: 1.2112462520599365
Epoch 770, training loss: 0.07158210873603821 = 0.005705887917429209 + 0.01 * 6.58762264251709
Epoch 770, val loss: 1.2183691263198853
Epoch 780, training loss: 0.07119143009185791 = 0.005490495823323727 + 0.01 * 6.570093154907227
Epoch 780, val loss: 1.225488543510437
Epoch 790, training loss: 0.07107244431972504 = 0.005287580657750368 + 0.01 * 6.578486442565918
Epoch 790, val loss: 1.2322572469711304
Epoch 800, training loss: 0.07066001743078232 = 0.005097055807709694 + 0.01 * 6.556296348571777
Epoch 800, val loss: 1.2390282154083252
Epoch 810, training loss: 0.07052381336688995 = 0.004917209967970848 + 0.01 * 6.560660362243652
Epoch 810, val loss: 1.2455356121063232
Epoch 820, training loss: 0.07026705145835876 = 0.0047480338253080845 + 0.01 * 6.5519022941589355
Epoch 820, val loss: 1.2520887851715088
Epoch 830, training loss: 0.07007227838039398 = 0.004588324576616287 + 0.01 * 6.548395156860352
Epoch 830, val loss: 1.2581939697265625
Epoch 840, training loss: 0.0700991302728653 = 0.004437778610736132 + 0.01 * 6.566134929656982
Epoch 840, val loss: 1.2642369270324707
Epoch 850, training loss: 0.0696856677532196 = 0.004296028986573219 + 0.01 * 6.53896427154541
Epoch 850, val loss: 1.2702016830444336
Epoch 860, training loss: 0.06948806345462799 = 0.004161763936281204 + 0.01 * 6.532629489898682
Epoch 860, val loss: 1.2760423421859741
Epoch 870, training loss: 0.06941980123519897 = 0.004034258890897036 + 0.01 * 6.5385541915893555
Epoch 870, val loss: 1.2817360162734985
Epoch 880, training loss: 0.0692407414317131 = 0.003913483116775751 + 0.01 * 6.532725811004639
Epoch 880, val loss: 1.2873114347457886
Epoch 890, training loss: 0.06913705915212631 = 0.0037988275289535522 + 0.01 * 6.533823013305664
Epoch 890, val loss: 1.292641043663025
Epoch 900, training loss: 0.06892383098602295 = 0.0036899175029248 + 0.01 * 6.5233917236328125
Epoch 900, val loss: 1.2980471849441528
Epoch 910, training loss: 0.06887196004390717 = 0.0035863418597728014 + 0.01 * 6.528561592102051
Epoch 910, val loss: 1.3032526969909668
Epoch 920, training loss: 0.06860382854938507 = 0.003487909212708473 + 0.01 * 6.511591911315918
Epoch 920, val loss: 1.308367133140564
Epoch 930, training loss: 0.06846807897090912 = 0.0033940633293241262 + 0.01 * 6.507401943206787
Epoch 930, val loss: 1.313368797302246
Epoch 940, training loss: 0.0685005635023117 = 0.003304628189653158 + 0.01 * 6.519593238830566
Epoch 940, val loss: 1.3181231021881104
Epoch 950, training loss: 0.0683942586183548 = 0.003219577018171549 + 0.01 * 6.517468452453613
Epoch 950, val loss: 1.3229795694351196
Epoch 960, training loss: 0.06820884346961975 = 0.003138491418212652 + 0.01 * 6.507035255432129
Epoch 960, val loss: 1.3276455402374268
Epoch 970, training loss: 0.06804478168487549 = 0.0030609911773353815 + 0.01 * 6.498379230499268
Epoch 970, val loss: 1.3321086168289185
Epoch 980, training loss: 0.06832811236381531 = 0.0029870476573705673 + 0.01 * 6.534106254577637
Epoch 980, val loss: 1.3365612030029297
Epoch 990, training loss: 0.06787177175283432 = 0.002916432684287429 + 0.01 * 6.4955339431762695
Epoch 990, val loss: 1.340950608253479
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.9336
Flip ASR: 0.9200/225 nodes
The final ASR:0.83764, 0.07834, Accuracy:0.80864, 0.00630
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11648])
remove edge: torch.Size([2, 9500])
updated graph: torch.Size([2, 10592])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.97909, 0.00969, Accuracy:0.83086, 0.00761
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.03848934173584 = 1.9547499418258667 + 0.01 * 8.373948097229004
Epoch 0, val loss: 1.9628790616989136
Epoch 10, training loss: 2.027461528778076 = 1.9437224864959717 + 0.01 * 8.373905181884766
Epoch 10, val loss: 1.9517557621002197
Epoch 20, training loss: 2.0137202739715576 = 1.9299826622009277 + 0.01 * 8.373753547668457
Epoch 20, val loss: 1.937799096107483
Epoch 30, training loss: 1.994443655014038 = 1.9107111692428589 + 0.01 * 8.373252868652344
Epoch 30, val loss: 1.918050765991211
Epoch 40, training loss: 1.9662806987762451 = 1.8825746774673462 + 0.01 * 8.370606422424316
Epoch 40, val loss: 1.8894187211990356
Epoch 50, training loss: 1.9274412393569946 = 1.8439244031906128 + 0.01 * 8.351682662963867
Epoch 50, val loss: 1.8519560098648071
Epoch 60, training loss: 1.8836220502853394 = 1.8010005950927734 + 0.01 * 8.262140274047852
Epoch 60, val loss: 1.814375877380371
Epoch 70, training loss: 1.843422293663025 = 1.7627723217010498 + 0.01 * 8.064998626708984
Epoch 70, val loss: 1.7829999923706055
Epoch 80, training loss: 1.794501543045044 = 1.7152421474456787 + 0.01 * 7.925941467285156
Epoch 80, val loss: 1.741175889968872
Epoch 90, training loss: 1.7259957790374756 = 1.6489421129226685 + 0.01 * 7.705361843109131
Epoch 90, val loss: 1.6840213537216187
Epoch 100, training loss: 1.6369177103042603 = 1.5616594552993774 + 0.01 * 7.525826930999756
Epoch 100, val loss: 1.610762119293213
Epoch 110, training loss: 1.5358960628509521 = 1.46230149269104 + 0.01 * 7.359462261199951
Epoch 110, val loss: 1.5296847820281982
Epoch 120, training loss: 1.4343640804290771 = 1.3620885610580444 + 0.01 * 7.227550983428955
Epoch 120, val loss: 1.449689507484436
Epoch 130, training loss: 1.3357887268066406 = 1.2644649744033813 + 0.01 * 7.132374286651611
Epoch 130, val loss: 1.3741590976715088
Epoch 140, training loss: 1.2406179904937744 = 1.1697616577148438 + 0.01 * 7.085632801055908
Epoch 140, val loss: 1.3034155368804932
Epoch 150, training loss: 1.1499953269958496 = 1.0794051885604858 + 0.01 * 7.059019088745117
Epoch 150, val loss: 1.2363303899765015
Epoch 160, training loss: 1.0654785633087158 = 0.99516761302948 + 0.01 * 7.031101226806641
Epoch 160, val loss: 1.1740901470184326
Epoch 170, training loss: 0.9871892333030701 = 0.9171642065048218 + 0.01 * 7.002501010894775
Epoch 170, val loss: 1.1162134408950806
Epoch 180, training loss: 0.9134874939918518 = 0.8436793684959412 + 0.01 * 6.980814456939697
Epoch 180, val loss: 1.0612375736236572
Epoch 190, training loss: 0.8424695730209351 = 0.7728193998336792 + 0.01 * 6.965014934539795
Epoch 190, val loss: 1.0080337524414062
Epoch 200, training loss: 0.7735052704811096 = 0.7039763331413269 + 0.01 * 6.952895164489746
Epoch 200, val loss: 0.9562146067619324
Epoch 210, training loss: 0.7070485353469849 = 0.6376109719276428 + 0.01 * 6.943756580352783
Epoch 210, val loss: 0.9069717526435852
Epoch 220, training loss: 0.6433802247047424 = 0.5739803314208984 + 0.01 * 6.939988136291504
Epoch 220, val loss: 0.8618751168251038
Epoch 230, training loss: 0.5827977061271667 = 0.5134730339050293 + 0.01 * 6.932466983795166
Epoch 230, val loss: 0.8223351240158081
Epoch 240, training loss: 0.5264645218849182 = 0.4571998715400696 + 0.01 * 6.926466464996338
Epoch 240, val loss: 0.7894631624221802
Epoch 250, training loss: 0.4756838381290436 = 0.4064723551273346 + 0.01 * 6.92114782333374
Epoch 250, val loss: 0.7636117339134216
Epoch 260, training loss: 0.43087446689605713 = 0.36172667145729065 + 0.01 * 6.914780616760254
Epoch 260, val loss: 0.7438842058181763
Epoch 270, training loss: 0.3913428783416748 = 0.3222571611404419 + 0.01 * 6.908570766448975
Epoch 270, val loss: 0.7293116450309753
Epoch 280, training loss: 0.35591593384742737 = 0.2868800759315491 + 0.01 * 6.903585910797119
Epoch 280, val loss: 0.7187175154685974
Epoch 290, training loss: 0.32358425855636597 = 0.2545955181121826 + 0.01 * 6.898873805999756
Epoch 290, val loss: 0.7111058831214905
Epoch 300, training loss: 0.29384154081344604 = 0.22486479580402374 + 0.01 * 6.8976731300354
Epoch 300, val loss: 0.7058882117271423
Epoch 310, training loss: 0.2665365934371948 = 0.1975858509540558 + 0.01 * 6.895074367523193
Epoch 310, val loss: 0.702907383441925
Epoch 320, training loss: 0.24184656143188477 = 0.17296132445335388 + 0.01 * 6.888524055480957
Epoch 320, val loss: 0.7021288871765137
Epoch 330, training loss: 0.22009804844856262 = 0.15121784806251526 + 0.01 * 6.888020992279053
Epoch 330, val loss: 0.7036793231964111
Epoch 340, training loss: 0.20124243199825287 = 0.13238944113254547 + 0.01 * 6.885299205780029
Epoch 340, val loss: 0.7076857686042786
Epoch 350, training loss: 0.18507999181747437 = 0.11626853048801422 + 0.01 * 6.8811469078063965
Epoch 350, val loss: 0.7138842940330505
Epoch 360, training loss: 0.17130909860134125 = 0.10252722352743149 + 0.01 * 6.878187656402588
Epoch 360, val loss: 0.7219952940940857
Epoch 370, training loss: 0.15954557061195374 = 0.090799480676651 + 0.01 * 6.874609470367432
Epoch 370, val loss: 0.7317876219749451
Epoch 380, training loss: 0.1494736671447754 = 0.08075179904699326 + 0.01 * 6.872186660766602
Epoch 380, val loss: 0.742843508720398
Epoch 390, training loss: 0.14078012108802795 = 0.07210510224103928 + 0.01 * 6.867502212524414
Epoch 390, val loss: 0.7548628449440002
Epoch 400, training loss: 0.13325698673725128 = 0.06462931632995605 + 0.01 * 6.862767219543457
Epoch 400, val loss: 0.7675707936286926
Epoch 410, training loss: 0.126750186085701 = 0.058144260197877884 + 0.01 * 6.860593318939209
Epoch 410, val loss: 0.7806892991065979
Epoch 420, training loss: 0.12107308208942413 = 0.05250224471092224 + 0.01 * 6.857083797454834
Epoch 420, val loss: 0.7940199971199036
Epoch 430, training loss: 0.11607896536588669 = 0.0475768968462944 + 0.01 * 6.8502068519592285
Epoch 430, val loss: 0.8074408769607544
Epoch 440, training loss: 0.11172081530094147 = 0.043262213468551636 + 0.01 * 6.845860004425049
Epoch 440, val loss: 0.8208460211753845
Epoch 450, training loss: 0.10794457793235779 = 0.0394718274474144 + 0.01 * 6.847275257110596
Epoch 450, val loss: 0.8340832591056824
Epoch 460, training loss: 0.10450460016727448 = 0.03613165766000748 + 0.01 * 6.837294578552246
Epoch 460, val loss: 0.847130537033081
Epoch 470, training loss: 0.10146388411521912 = 0.03317644074559212 + 0.01 * 6.828744888305664
Epoch 470, val loss: 0.8599563241004944
Epoch 480, training loss: 0.098807193338871 = 0.0305520948022604 + 0.01 * 6.825509548187256
Epoch 480, val loss: 0.8725751638412476
Epoch 490, training loss: 0.09646241366863251 = 0.028215188533067703 + 0.01 * 6.824723243713379
Epoch 490, val loss: 0.8849115967750549
Epoch 500, training loss: 0.09425315260887146 = 0.02612757310271263 + 0.01 * 6.812558174133301
Epoch 500, val loss: 0.8969570994377136
Epoch 510, training loss: 0.09251600503921509 = 0.02425609715282917 + 0.01 * 6.825991153717041
Epoch 510, val loss: 0.9086779952049255
Epoch 520, training loss: 0.0905875414609909 = 0.022574320435523987 + 0.01 * 6.801321983337402
Epoch 520, val loss: 0.920194149017334
Epoch 530, training loss: 0.08897534012794495 = 0.02105790190398693 + 0.01 * 6.791743755340576
Epoch 530, val loss: 0.9314049482345581
Epoch 540, training loss: 0.08777519315481186 = 0.019686872139573097 + 0.01 * 6.808832168579102
Epoch 540, val loss: 0.9423443675041199
Epoch 550, training loss: 0.08629889041185379 = 0.01844491995871067 + 0.01 * 6.785397529602051
Epoch 550, val loss: 0.9529985189437866
Epoch 560, training loss: 0.08505794405937195 = 0.01731637492775917 + 0.01 * 6.774157524108887
Epoch 560, val loss: 0.9633762836456299
Epoch 570, training loss: 0.08393537253141403 = 0.01628822274506092 + 0.01 * 6.764715194702148
Epoch 570, val loss: 0.9734610319137573
Epoch 580, training loss: 0.08296035975217819 = 0.015349452383816242 + 0.01 * 6.7610907554626465
Epoch 580, val loss: 0.9833357334136963
Epoch 590, training loss: 0.08220630884170532 = 0.01449042558670044 + 0.01 * 6.7715888023376465
Epoch 590, val loss: 0.9928775429725647
Epoch 600, training loss: 0.08111590147018433 = 0.013703516684472561 + 0.01 * 6.741238594055176
Epoch 600, val loss: 1.002242922782898
Epoch 610, training loss: 0.08043448626995087 = 0.012980291619896889 + 0.01 * 6.745419979095459
Epoch 610, val loss: 1.0112553834915161
Epoch 620, training loss: 0.07973355054855347 = 0.0123142059892416 + 0.01 * 6.741934299468994
Epoch 620, val loss: 1.020114541053772
Epoch 630, training loss: 0.07895779609680176 = 0.011700510047376156 + 0.01 * 6.725728988647461
Epoch 630, val loss: 1.0287299156188965
Epoch 640, training loss: 0.07826849073171616 = 0.011133046820759773 + 0.01 * 6.7135443687438965
Epoch 640, val loss: 1.0370901823043823
Epoch 650, training loss: 0.07780636847019196 = 0.010607825592160225 + 0.01 * 6.719854831695557
Epoch 650, val loss: 1.0451858043670654
Epoch 660, training loss: 0.07709769904613495 = 0.010121255181729794 + 0.01 * 6.6976447105407715
Epoch 660, val loss: 1.0531566143035889
Epoch 670, training loss: 0.07670861482620239 = 0.009669180028140545 + 0.01 * 6.703944206237793
Epoch 670, val loss: 1.0608328580856323
Epoch 680, training loss: 0.07615949958562851 = 0.00924838986247778 + 0.01 * 6.691111087799072
Epoch 680, val loss: 1.0683953762054443
Epoch 690, training loss: 0.07560810446739197 = 0.00885657500475645 + 0.01 * 6.675153732299805
Epoch 690, val loss: 1.0756522417068481
Epoch 700, training loss: 0.07534254342317581 = 0.008490781299769878 + 0.01 * 6.685176372528076
Epoch 700, val loss: 1.0827940702438354
Epoch 710, training loss: 0.07485930621623993 = 0.008148754015564919 + 0.01 * 6.671055316925049
Epoch 710, val loss: 1.089736819267273
Epoch 720, training loss: 0.07480324059724808 = 0.007828536443412304 + 0.01 * 6.6974711418151855
Epoch 720, val loss: 1.0965211391448975
Epoch 730, training loss: 0.07409266382455826 = 0.007528745103627443 + 0.01 * 6.6563920974731445
Epoch 730, val loss: 1.1030693054199219
Epoch 740, training loss: 0.0737978145480156 = 0.007247410248965025 + 0.01 * 6.655040264129639
Epoch 740, val loss: 1.1095407009124756
Epoch 750, training loss: 0.07344695180654526 = 0.006983173545449972 + 0.01 * 6.6463775634765625
Epoch 750, val loss: 1.1157974004745483
Epoch 760, training loss: 0.07299850881099701 = 0.006734324153512716 + 0.01 * 6.626418113708496
Epoch 760, val loss: 1.121964454650879
Epoch 770, training loss: 0.0729350745677948 = 0.006499888841062784 + 0.01 * 6.643518924713135
Epoch 770, val loss: 1.1278331279754639
Epoch 780, training loss: 0.07239445298910141 = 0.00627871323376894 + 0.01 * 6.611574172973633
Epoch 780, val loss: 1.1337941884994507
Epoch 790, training loss: 0.07231257855892181 = 0.006069698836654425 + 0.01 * 6.624288558959961
Epoch 790, val loss: 1.139358639717102
Epoch 800, training loss: 0.07188040763139725 = 0.005872300826013088 + 0.01 * 6.600810527801514
Epoch 800, val loss: 1.1450692415237427
Epoch 810, training loss: 0.07195562869310379 = 0.005685270763933659 + 0.01 * 6.627036094665527
Epoch 810, val loss: 1.1503311395645142
Epoch 820, training loss: 0.07146719843149185 = 0.005508337635546923 + 0.01 * 6.59588623046875
Epoch 820, val loss: 1.1557519435882568
Epoch 830, training loss: 0.07199385017156601 = 0.005340579431504011 + 0.01 * 6.665327548980713
Epoch 830, val loss: 1.1608258485794067
Epoch 840, training loss: 0.07115098834037781 = 0.005181573797017336 + 0.01 * 6.5969414710998535
Epoch 840, val loss: 1.1659557819366455
Epoch 850, training loss: 0.07104858011007309 = 0.005030551925301552 + 0.01 * 6.601802825927734
Epoch 850, val loss: 1.1708683967590332
Epoch 860, training loss: 0.0707271620631218 = 0.00488681998103857 + 0.01 * 6.584034442901611
Epoch 860, val loss: 1.1756792068481445
Epoch 870, training loss: 0.0704253688454628 = 0.004749827552586794 + 0.01 * 6.567554473876953
Epoch 870, val loss: 1.1804699897766113
Epoch 880, training loss: 0.07058295607566833 = 0.004619250539690256 + 0.01 * 6.596370697021484
Epoch 880, val loss: 1.1849573850631714
Epoch 890, training loss: 0.07008850574493408 = 0.004494919441640377 + 0.01 * 6.559358596801758
Epoch 890, val loss: 1.189615249633789
Epoch 900, training loss: 0.0699654072523117 = 0.00437634252011776 + 0.01 * 6.558906078338623
Epoch 900, val loss: 1.1938234567642212
Epoch 910, training loss: 0.0699671134352684 = 0.0042633614502847195 + 0.01 * 6.570374965667725
Epoch 910, val loss: 1.1983509063720703
Epoch 920, training loss: 0.06962871551513672 = 0.004155317787081003 + 0.01 * 6.547339916229248
Epoch 920, val loss: 1.2024677991867065
Epoch 930, training loss: 0.06970144063234329 = 0.004051866941154003 + 0.01 * 6.564957618713379
Epoch 930, val loss: 1.2066389322280884
Epoch 940, training loss: 0.06963447481393814 = 0.003952876199036837 + 0.01 * 6.568160057067871
Epoch 940, val loss: 1.210756778717041
Epoch 950, training loss: 0.06929975003004074 = 0.0038581264670938253 + 0.01 * 6.544162750244141
Epoch 950, val loss: 1.2145906686782837
Epoch 960, training loss: 0.06928522884845734 = 0.003767498303204775 + 0.01 * 6.551773548126221
Epoch 960, val loss: 1.2185651063919067
Epoch 970, training loss: 0.06913416087627411 = 0.0036805085837841034 + 0.01 * 6.545365810394287
Epoch 970, val loss: 1.2223104238510132
Epoch 980, training loss: 0.0689917728304863 = 0.0035971179604530334 + 0.01 * 6.53946590423584
Epoch 980, val loss: 1.2261672019958496
Epoch 990, training loss: 0.06888420879840851 = 0.0035169622860848904 + 0.01 * 6.53672456741333
Epoch 990, val loss: 1.229636311531067
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.4317
Flip ASR: 0.3333/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0402817726135254 = 1.9565422534942627 + 0.01 * 8.373948097229004
Epoch 0, val loss: 1.963389277458191
Epoch 10, training loss: 2.0301456451416016 = 1.9464064836502075 + 0.01 * 8.373915672302246
Epoch 10, val loss: 1.952240228652954
Epoch 20, training loss: 2.0181119441986084 = 1.934374213218689 + 0.01 * 8.37377643585205
Epoch 20, val loss: 1.9386019706726074
Epoch 30, training loss: 2.001366376876831 = 1.9176326990127563 + 0.01 * 8.373369216918945
Epoch 30, val loss: 1.9194973707199097
Epoch 40, training loss: 1.976525068283081 = 1.8928074836730957 + 0.01 * 8.371757507324219
Epoch 40, val loss: 1.891296625137329
Epoch 50, training loss: 1.940460205078125 = 1.856842041015625 + 0.01 * 8.361820220947266
Epoch 50, val loss: 1.8514808416366577
Epoch 60, training loss: 1.894791841506958 = 1.8116605281829834 + 0.01 * 8.313136100769043
Epoch 60, val loss: 1.804691195487976
Epoch 70, training loss: 1.8478262424468994 = 1.7666562795639038 + 0.01 * 8.1170015335083
Epoch 70, val loss: 1.7624526023864746
Epoch 80, training loss: 1.7969486713409424 = 1.7175366878509521 + 0.01 * 7.941195011138916
Epoch 80, val loss: 1.7183552980422974
Epoch 90, training loss: 1.7272531986236572 = 1.6512484550476074 + 0.01 * 7.60047721862793
Epoch 90, val loss: 1.6599310636520386
Epoch 100, training loss: 1.637451171875 = 1.5633357763290405 + 0.01 * 7.411543846130371
Epoch 100, val loss: 1.5846960544586182
Epoch 110, training loss: 1.5310649871826172 = 1.4583218097686768 + 0.01 * 7.274322032928467
Epoch 110, val loss: 1.499076008796692
Epoch 120, training loss: 1.4199624061584473 = 1.3481496572494507 + 0.01 * 7.181280136108398
Epoch 120, val loss: 1.4133363962173462
Epoch 130, training loss: 1.3133922815322876 = 1.2422187328338623 + 0.01 * 7.1173505783081055
Epoch 130, val loss: 1.3350578546524048
Epoch 140, training loss: 1.2145084142684937 = 1.1437073945999146 + 0.01 * 7.080100059509277
Epoch 140, val loss: 1.2653884887695312
Epoch 150, training loss: 1.1234569549560547 = 1.0528337955474854 + 0.01 * 7.06231689453125
Epoch 150, val loss: 1.2012885808944702
Epoch 160, training loss: 1.0394920110702515 = 0.9689313173294067 + 0.01 * 7.05607271194458
Epoch 160, val loss: 1.1419239044189453
Epoch 170, training loss: 0.9612524509429932 = 0.8907297849655151 + 0.01 * 7.05226469039917
Epoch 170, val loss: 1.0860180854797363
Epoch 180, training loss: 0.8863610625267029 = 0.8158993124961853 + 0.01 * 7.0461745262146
Epoch 180, val loss: 1.0318819284439087
Epoch 190, training loss: 0.8126705288887024 = 0.7423051595687866 + 0.01 * 7.03653621673584
Epoch 190, val loss: 0.9782550930976868
Epoch 200, training loss: 0.7397829294204712 = 0.6695579290390015 + 0.01 * 7.02250337600708
Epoch 200, val loss: 0.9253610968589783
Epoch 210, training loss: 0.6696133613586426 = 0.5995907187461853 + 0.01 * 7.002267360687256
Epoch 210, val loss: 0.8753407597541809
Epoch 220, training loss: 0.6049939393997192 = 0.5351628661155701 + 0.01 * 6.98310661315918
Epoch 220, val loss: 0.8314992785453796
Epoch 230, training loss: 0.5471835136413574 = 0.4776061475276947 + 0.01 * 6.957738399505615
Epoch 230, val loss: 0.7958900332450867
Epoch 240, training loss: 0.49594247341156006 = 0.42652949690818787 + 0.01 * 6.941298007965088
Epoch 240, val loss: 0.7678067088127136
Epoch 250, training loss: 0.45004597306251526 = 0.380705326795578 + 0.01 * 6.934064865112305
Epoch 250, val loss: 0.7459986209869385
Epoch 260, training loss: 0.4082772731781006 = 0.33897218108177185 + 0.01 * 6.930510997772217
Epoch 260, val loss: 0.7287567853927612
Epoch 270, training loss: 0.3696877062320709 = 0.30040279030799866 + 0.01 * 6.928492069244385
Epoch 270, val loss: 0.7148748636245728
Epoch 280, training loss: 0.3338409662246704 = 0.2645852565765381 + 0.01 * 6.925570964813232
Epoch 280, val loss: 0.7038294076919556
Epoch 290, training loss: 0.3007252812385559 = 0.23147183656692505 + 0.01 * 6.925346374511719
Epoch 290, val loss: 0.6952081918716431
Epoch 300, training loss: 0.2703303098678589 = 0.20113864541053772 + 0.01 * 6.919167518615723
Epoch 300, val loss: 0.6891136765480042
Epoch 310, training loss: 0.24291309714317322 = 0.17379336059093475 + 0.01 * 6.911972999572754
Epoch 310, val loss: 0.6858257055282593
Epoch 320, training loss: 0.21865297853946686 = 0.1495533287525177 + 0.01 * 6.9099650382995605
Epoch 320, val loss: 0.6851097941398621
Epoch 330, training loss: 0.197482168674469 = 0.12852735817432404 + 0.01 * 6.895481109619141
Epoch 330, val loss: 0.6871142983436584
Epoch 340, training loss: 0.17956827580928802 = 0.11069359630346298 + 0.01 * 6.887467861175537
Epoch 340, val loss: 0.691399335861206
Epoch 350, training loss: 0.16449449956417084 = 0.09571444988250732 + 0.01 * 6.878005027770996
Epoch 350, val loss: 0.6979964375495911
Epoch 360, training loss: 0.15198767185211182 = 0.08329504728317261 + 0.01 * 6.869262218475342
Epoch 360, val loss: 0.7065176367759705
Epoch 370, training loss: 0.1414790153503418 = 0.07297582179307938 + 0.01 * 6.8503193855285645
Epoch 370, val loss: 0.7162331342697144
Epoch 380, training loss: 0.13271453976631165 = 0.06436247378587723 + 0.01 * 6.835207462310791
Epoch 380, val loss: 0.7269188165664673
Epoch 390, training loss: 0.12540768086910248 = 0.05712807923555374 + 0.01 * 6.82796049118042
Epoch 390, val loss: 0.7381283044815063
Epoch 400, training loss: 0.11938894540071487 = 0.051028281450271606 + 0.01 * 6.836066246032715
Epoch 400, val loss: 0.7496272325515747
Epoch 410, training loss: 0.11380621790885925 = 0.045833516865968704 + 0.01 * 6.797269821166992
Epoch 410, val loss: 0.7612012028694153
Epoch 420, training loss: 0.10923515260219574 = 0.04136846587061882 + 0.01 * 6.786668300628662
Epoch 420, val loss: 0.7727862596511841
Epoch 430, training loss: 0.10533656179904938 = 0.03750358149409294 + 0.01 * 6.783297538757324
Epoch 430, val loss: 0.7843075394630432
Epoch 440, training loss: 0.10183234512805939 = 0.03414801135659218 + 0.01 * 6.768433094024658
Epoch 440, val loss: 0.7956275939941406
Epoch 450, training loss: 0.09879173338413239 = 0.03121928684413433 + 0.01 * 6.75724458694458
Epoch 450, val loss: 0.8066896200180054
Epoch 460, training loss: 0.0960548147559166 = 0.028643518686294556 + 0.01 * 6.7411298751831055
Epoch 460, val loss: 0.8175630569458008
Epoch 470, training loss: 0.09434130787849426 = 0.026365701109170914 + 0.01 * 6.797561168670654
Epoch 470, val loss: 0.8281772136688232
Epoch 480, training loss: 0.09170901775360107 = 0.024348555132746696 + 0.01 * 6.736046314239502
Epoch 480, val loss: 0.8384418487548828
Epoch 490, training loss: 0.08972330391407013 = 0.022551143541932106 + 0.01 * 6.717216491699219
Epoch 490, val loss: 0.8484586477279663
Epoch 500, training loss: 0.08800961077213287 = 0.020945128053426743 + 0.01 * 6.706448078155518
Epoch 500, val loss: 0.8582553267478943
Epoch 510, training loss: 0.08715987205505371 = 0.019507093355059624 + 0.01 * 6.765277862548828
Epoch 510, val loss: 0.8677985668182373
Epoch 520, training loss: 0.08524782955646515 = 0.018221182748675346 + 0.01 * 6.702664375305176
Epoch 520, val loss: 0.8770219683647156
Epoch 530, training loss: 0.08398587256669998 = 0.017063824459910393 + 0.01 * 6.692204475402832
Epoch 530, val loss: 0.8859542012214661
Epoch 540, training loss: 0.08288516104221344 = 0.016016604378819466 + 0.01 * 6.686856269836426
Epoch 540, val loss: 0.8946764469146729
Epoch 550, training loss: 0.08178679645061493 = 0.015067273750901222 + 0.01 * 6.671952247619629
Epoch 550, val loss: 0.9032164216041565
Epoch 560, training loss: 0.08094916492700577 = 0.014203411526978016 + 0.01 * 6.674575328826904
Epoch 560, val loss: 0.9114820957183838
Epoch 570, training loss: 0.0799877718091011 = 0.013414432294666767 + 0.01 * 6.657333850860596
Epoch 570, val loss: 0.9195513725280762
Epoch 580, training loss: 0.07927713543176651 = 0.012692650780081749 + 0.01 * 6.658448219299316
Epoch 580, val loss: 0.9273846745491028
Epoch 590, training loss: 0.07859095931053162 = 0.012031716294586658 + 0.01 * 6.655923843383789
Epoch 590, val loss: 0.9350430369377136
Epoch 600, training loss: 0.07782365381717682 = 0.011423724703490734 + 0.01 * 6.639993190765381
Epoch 600, val loss: 0.9423949122428894
Epoch 610, training loss: 0.07741408050060272 = 0.010863112285733223 + 0.01 * 6.655097007751465
Epoch 610, val loss: 0.9496880173683167
Epoch 620, training loss: 0.0766768828034401 = 0.010345655493438244 + 0.01 * 6.63312292098999
Epoch 620, val loss: 0.956757128238678
Epoch 630, training loss: 0.07647233456373215 = 0.00986634660512209 + 0.01 * 6.6605987548828125
Epoch 630, val loss: 0.9636849761009216
Epoch 640, training loss: 0.07566165924072266 = 0.009421906433999538 + 0.01 * 6.6239752769470215
Epoch 640, val loss: 0.9704544544219971
Epoch 650, training loss: 0.07543639093637466 = 0.00900832749903202 + 0.01 * 6.6428070068359375
Epoch 650, val loss: 0.9770409464836121
Epoch 660, training loss: 0.07494273781776428 = 0.0086231529712677 + 0.01 * 6.631958961486816
Epoch 660, val loss: 0.9835057258605957
Epoch 670, training loss: 0.07450985163450241 = 0.008263970725238323 + 0.01 * 6.624588489532471
Epoch 670, val loss: 0.9897863268852234
Epoch 680, training loss: 0.0739903599023819 = 0.007928532548248768 + 0.01 * 6.60618257522583
Epoch 680, val loss: 0.9959530234336853
Epoch 690, training loss: 0.07380055636167526 = 0.007614314556121826 + 0.01 * 6.618624687194824
Epoch 690, val loss: 1.001948595046997
Epoch 700, training loss: 0.07332272827625275 = 0.0073203397914767265 + 0.01 * 6.600239276885986
Epoch 700, val loss: 1.0078485012054443
Epoch 710, training loss: 0.07297489047050476 = 0.007044934201985598 + 0.01 * 6.592996120452881
Epoch 710, val loss: 1.0135877132415771
Epoch 720, training loss: 0.07286103069782257 = 0.006785781122744083 + 0.01 * 6.607524871826172
Epoch 720, val loss: 1.0192025899887085
Epoch 730, training loss: 0.0725533589720726 = 0.006542351562529802 + 0.01 * 6.601100921630859
Epoch 730, val loss: 1.0246944427490234
Epoch 740, training loss: 0.07228592783212662 = 0.006313613150268793 + 0.01 * 6.597231388092041
Epoch 740, val loss: 1.0301263332366943
Epoch 750, training loss: 0.07189010083675385 = 0.00609777495265007 + 0.01 * 6.579232692718506
Epoch 750, val loss: 1.0353702306747437
Epoch 760, training loss: 0.07156403362751007 = 0.005894262343645096 + 0.01 * 6.566977500915527
Epoch 760, val loss: 1.0405840873718262
Epoch 770, training loss: 0.07138469070196152 = 0.0057019442319869995 + 0.01 * 6.56827449798584
Epoch 770, val loss: 1.0456382036209106
Epoch 780, training loss: 0.0713685154914856 = 0.0055198450572788715 + 0.01 * 6.584867477416992
Epoch 780, val loss: 1.050628662109375
Epoch 790, training loss: 0.0709884762763977 = 0.005347584839910269 + 0.01 * 6.564088821411133
Epoch 790, val loss: 1.055472731590271
Epoch 800, training loss: 0.07089612632989883 = 0.005184324458241463 + 0.01 * 6.571180820465088
Epoch 800, val loss: 1.060292363166809
Epoch 810, training loss: 0.07068202644586563 = 0.005029840394854546 + 0.01 * 6.565218448638916
Epoch 810, val loss: 1.0649784803390503
Epoch 820, training loss: 0.07061220705509186 = 0.0048831067979335785 + 0.01 * 6.572909832000732
Epoch 820, val loss: 1.069549560546875
Epoch 830, training loss: 0.07018506526947021 = 0.00474356347694993 + 0.01 * 6.5441508293151855
Epoch 830, val loss: 1.0740786790847778
Epoch 840, training loss: 0.07012856751680374 = 0.004610720556229353 + 0.01 * 6.551784992218018
Epoch 840, val loss: 1.0785163640975952
Epoch 850, training loss: 0.069944366812706 = 0.004484081175178289 + 0.01 * 6.5460286140441895
Epoch 850, val loss: 1.0828959941864014
Epoch 860, training loss: 0.0699925497174263 = 0.004363439977169037 + 0.01 * 6.562911033630371
Epoch 860, val loss: 1.0871915817260742
Epoch 870, training loss: 0.06973499059677124 = 0.004248296841979027 + 0.01 * 6.548669338226318
Epoch 870, val loss: 1.091436505317688
Epoch 880, training loss: 0.06949564814567566 = 0.004138365853577852 + 0.01 * 6.535728454589844
Epoch 880, val loss: 1.095563292503357
Epoch 890, training loss: 0.06952043622732162 = 0.004033262841403484 + 0.01 * 6.548717975616455
Epoch 890, val loss: 1.0996707677841187
Epoch 900, training loss: 0.06918100267648697 = 0.0039329067803919315 + 0.01 * 6.52480936050415
Epoch 900, val loss: 1.1036163568496704
Epoch 910, training loss: 0.06964132934808731 = 0.0038366636727005243 + 0.01 * 6.580467224121094
Epoch 910, val loss: 1.1076290607452393
Epoch 920, training loss: 0.06892137229442596 = 0.0037445512134581804 + 0.01 * 6.5176825523376465
Epoch 920, val loss: 1.1114917993545532
Epoch 930, training loss: 0.06917933374643326 = 0.003656053217127919 + 0.01 * 6.552328586578369
Epoch 930, val loss: 1.1152689456939697
Epoch 940, training loss: 0.06871034950017929 = 0.0035703135654330254 + 0.01 * 6.514004230499268
Epoch 940, val loss: 1.1190325021743774
Epoch 950, training loss: 0.06890841573476791 = 0.003487977432087064 + 0.01 * 6.542044639587402
Epoch 950, val loss: 1.122704029083252
Epoch 960, training loss: 0.06856396049261093 = 0.0034089116379618645 + 0.01 * 6.515505313873291
Epoch 960, val loss: 1.1263693571090698
Epoch 970, training loss: 0.06840645521879196 = 0.00333267729729414 + 0.01 * 6.507378101348877
Epoch 970, val loss: 1.1298906803131104
Epoch 980, training loss: 0.06841953098773956 = 0.0032592033967375755 + 0.01 * 6.516032695770264
Epoch 980, val loss: 1.1333651542663574
Epoch 990, training loss: 0.06818307936191559 = 0.003188794944435358 + 0.01 * 6.4994282722473145
Epoch 990, val loss: 1.1369271278381348
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.5387
Flip ASR: 0.4978/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.008840799331665 = 1.925101637840271 + 0.01 * 8.373909950256348
Epoch 0, val loss: 1.9213511943817139
Epoch 10, training loss: 1.9994068145751953 = 1.9156683683395386 + 0.01 * 8.373849868774414
Epoch 10, val loss: 1.9119542837142944
Epoch 20, training loss: 1.987929105758667 = 1.9041931629180908 + 0.01 * 8.3735933303833
Epoch 20, val loss: 1.8997807502746582
Epoch 30, training loss: 1.9719738960266113 = 1.8882464170455933 + 0.01 * 8.372748374938965
Epoch 30, val loss: 1.8823766708374023
Epoch 40, training loss: 1.9488314390182495 = 1.8651440143585205 + 0.01 * 8.368743896484375
Epoch 40, val loss: 1.8572328090667725
Epoch 50, training loss: 1.9164645671844482 = 1.83302640914917 + 0.01 * 8.343815803527832
Epoch 50, val loss: 1.8236356973648071
Epoch 60, training loss: 1.875882625579834 = 1.7935959100723267 + 0.01 * 8.228671073913574
Epoch 60, val loss: 1.7850260734558105
Epoch 70, training loss: 1.8297017812728882 = 1.7503546476364136 + 0.01 * 7.93471097946167
Epoch 70, val loss: 1.7455239295959473
Epoch 80, training loss: 1.7738046646118164 = 1.6976721286773682 + 0.01 * 7.613258361816406
Epoch 80, val loss: 1.6986531019210815
Epoch 90, training loss: 1.7011494636535645 = 1.6273335218429565 + 0.01 * 7.381598472595215
Epoch 90, val loss: 1.638043999671936
Epoch 100, training loss: 1.6104758977890015 = 1.5380128622055054 + 0.01 * 7.2462992668151855
Epoch 100, val loss: 1.5632693767547607
Epoch 110, training loss: 1.5066769123077393 = 1.435661792755127 + 0.01 * 7.101506233215332
Epoch 110, val loss: 1.4791637659072876
Epoch 120, training loss: 1.3988620042800903 = 1.328408122062683 + 0.01 * 7.045391082763672
Epoch 120, val loss: 1.3943251371383667
Epoch 130, training loss: 1.2910943031311035 = 1.2208138704299927 + 0.01 * 7.028038501739502
Epoch 130, val loss: 1.3133981227874756
Epoch 140, training loss: 1.1850582361221313 = 1.1148546934127808 + 0.01 * 7.020350456237793
Epoch 140, val loss: 1.2368732690811157
Epoch 150, training loss: 1.081129550933838 = 1.0110374689102173 + 0.01 * 7.009210586547852
Epoch 150, val loss: 1.1627521514892578
Epoch 160, training loss: 0.9796663522720337 = 0.9096788763999939 + 0.01 * 6.998750686645508
Epoch 160, val loss: 1.0899553298950195
Epoch 170, training loss: 0.8818472623825073 = 0.8119426369667053 + 0.01 * 6.990459442138672
Epoch 170, val loss: 1.0187534093856812
Epoch 180, training loss: 0.7902159690856934 = 0.7203801274299622 + 0.01 * 6.983581066131592
Epoch 180, val loss: 0.9514256119728088
Epoch 190, training loss: 0.7071011662483215 = 0.6373184323310852 + 0.01 * 6.978271007537842
Epoch 190, val loss: 0.8915194272994995
Epoch 200, training loss: 0.6333568692207336 = 0.563612163066864 + 0.01 * 6.974470138549805
Epoch 200, val loss: 0.8409600853919983
Epoch 210, training loss: 0.568240761756897 = 0.49852558970451355 + 0.01 * 6.97151517868042
Epoch 210, val loss: 0.799597442150116
Epoch 220, training loss: 0.5103623270988464 = 0.4406726658344269 + 0.01 * 6.968964576721191
Epoch 220, val loss: 0.7669565677642822
Epoch 230, training loss: 0.4582863748073578 = 0.388619989156723 + 0.01 * 6.96663761138916
Epoch 230, val loss: 0.7407910227775574
Epoch 240, training loss: 0.41099098324775696 = 0.34134525060653687 + 0.01 * 6.964573860168457
Epoch 240, val loss: 0.7195857167243958
Epoch 250, training loss: 0.36807698011398315 = 0.29845085740089417 + 0.01 * 6.962613582611084
Epoch 250, val loss: 0.7026503682136536
Epoch 260, training loss: 0.32963675260543823 = 0.26002559065818787 + 0.01 * 6.961114883422852
Epoch 260, val loss: 0.6896517276763916
Epoch 270, training loss: 0.2957664728164673 = 0.2261681705713272 + 0.01 * 6.959829807281494
Epoch 270, val loss: 0.6803580522537231
Epoch 280, training loss: 0.2662808895111084 = 0.19669421017169952 + 0.01 * 6.958667278289795
Epoch 280, val loss: 0.6745959520339966
Epoch 290, training loss: 0.24083924293518066 = 0.17125852406024933 + 0.01 * 6.958071231842041
Epoch 290, val loss: 0.6721177697181702
Epoch 300, training loss: 0.21894031763076782 = 0.1493701934814453 + 0.01 * 6.957012176513672
Epoch 300, val loss: 0.6724956631660461
Epoch 310, training loss: 0.20000508427619934 = 0.13044676184654236 + 0.01 * 6.955831527709961
Epoch 310, val loss: 0.675563395023346
Epoch 320, training loss: 0.18365240097045898 = 0.1141078919172287 + 0.01 * 6.954452037811279
Epoch 320, val loss: 0.6811180710792542
Epoch 330, training loss: 0.16958099603652954 = 0.10003455728292465 + 0.01 * 6.954643726348877
Epoch 330, val loss: 0.6888026595115662
Epoch 340, training loss: 0.1575259268283844 = 0.08801136165857315 + 0.01 * 6.951457500457764
Epoch 340, val loss: 0.698413074016571
Epoch 350, training loss: 0.14719796180725098 = 0.07770321518182755 + 0.01 * 6.949473857879639
Epoch 350, val loss: 0.7091913819313049
Epoch 360, training loss: 0.13832618296146393 = 0.06885803490877151 + 0.01 * 6.946815013885498
Epoch 360, val loss: 0.7206726670265198
Epoch 370, training loss: 0.1307050883769989 = 0.06126358360052109 + 0.01 * 6.944150447845459
Epoch 370, val loss: 0.7325544357299805
Epoch 380, training loss: 0.12413991242647171 = 0.05472516268491745 + 0.01 * 6.9414753913879395
Epoch 380, val loss: 0.7445995211601257
Epoch 390, training loss: 0.11847099661827087 = 0.049069568514823914 + 0.01 * 6.94014310836792
Epoch 390, val loss: 0.7566239833831787
Epoch 400, training loss: 0.11349664628505707 = 0.044156525284051895 + 0.01 * 6.9340128898620605
Epoch 400, val loss: 0.7685835957527161
Epoch 410, training loss: 0.10915467143058777 = 0.03987421095371246 + 0.01 * 6.928046226501465
Epoch 410, val loss: 0.7804809212684631
Epoch 420, training loss: 0.10535628348588943 = 0.03613630682229996 + 0.01 * 6.921997547149658
Epoch 420, val loss: 0.7923041582107544
Epoch 430, training loss: 0.1020054966211319 = 0.03286280483007431 + 0.01 * 6.91426944732666
Epoch 430, val loss: 0.8039073348045349
Epoch 440, training loss: 0.09909386932849884 = 0.02998613752424717 + 0.01 * 6.910773277282715
Epoch 440, val loss: 0.815311849117279
Epoch 450, training loss: 0.09646579623222351 = 0.027451302856206894 + 0.01 * 6.901450157165527
Epoch 450, val loss: 0.8264198303222656
Epoch 460, training loss: 0.09408234059810638 = 0.025211043655872345 + 0.01 * 6.887129783630371
Epoch 460, val loss: 0.8373661637306213
Epoch 470, training loss: 0.09198765456676483 = 0.023224180564284325 + 0.01 * 6.876347064971924
Epoch 470, val loss: 0.8481414914131165
Epoch 480, training loss: 0.0900796428322792 = 0.021458270028233528 + 0.01 * 6.862137794494629
Epoch 480, val loss: 0.8585858941078186
Epoch 490, training loss: 0.08843761682510376 = 0.019883988425135612 + 0.01 * 6.855362892150879
Epoch 490, val loss: 0.8687970638275146
Epoch 500, training loss: 0.08670888096094131 = 0.018478311598300934 + 0.01 * 6.823057174682617
Epoch 500, val loss: 0.8785233497619629
Epoch 510, training loss: 0.08535978198051453 = 0.017215784639120102 + 0.01 * 6.814399719238281
Epoch 510, val loss: 0.8881213068962097
Epoch 520, training loss: 0.08459699153900146 = 0.016077345237135887 + 0.01 * 6.851964473724365
Epoch 520, val loss: 0.8974982500076294
Epoch 530, training loss: 0.0830940306186676 = 0.015052009373903275 + 0.01 * 6.804202556610107
Epoch 530, val loss: 0.9064368605613708
Epoch 540, training loss: 0.08192753046751022 = 0.01412560511380434 + 0.01 * 6.7801923751831055
Epoch 540, val loss: 0.9151171445846558
Epoch 550, training loss: 0.08091173321008682 = 0.01328496914356947 + 0.01 * 6.762677192687988
Epoch 550, val loss: 0.9236308336257935
Epoch 560, training loss: 0.08029352128505707 = 0.012520862743258476 + 0.01 * 6.777266025543213
Epoch 560, val loss: 0.9318837523460388
Epoch 570, training loss: 0.0792810246348381 = 0.011824874207377434 + 0.01 * 6.745615005493164
Epoch 570, val loss: 0.9397670030593872
Epoch 580, training loss: 0.07855941355228424 = 0.011188038624823093 + 0.01 * 6.737137794494629
Epoch 580, val loss: 0.9475221037864685
Epoch 590, training loss: 0.07787756621837616 = 0.010603896342217922 + 0.01 * 6.727367401123047
Epoch 590, val loss: 0.954964280128479
Epoch 600, training loss: 0.0772833451628685 = 0.010066625662147999 + 0.01 * 6.7216715812683105
Epoch 600, val loss: 0.9624016880989075
Epoch 610, training loss: 0.0767495259642601 = 0.009571880102157593 + 0.01 * 6.717764377593994
Epoch 610, val loss: 0.9694474935531616
Epoch 620, training loss: 0.07620995491743088 = 0.009115290828049183 + 0.01 * 6.709466457366943
Epoch 620, val loss: 0.9764513969421387
Epoch 630, training loss: 0.07604506611824036 = 0.008692516013979912 + 0.01 * 6.735255241394043
Epoch 630, val loss: 0.9830626249313354
Epoch 640, training loss: 0.07519672811031342 = 0.00830119289457798 + 0.01 * 6.689553737640381
Epoch 640, val loss: 0.9896737337112427
Epoch 650, training loss: 0.07471337914466858 = 0.007938007824122906 + 0.01 * 6.67753791809082
Epoch 650, val loss: 0.9961126446723938
Epoch 660, training loss: 0.07456948608160019 = 0.007600296754390001 + 0.01 * 6.696918964385986
Epoch 660, val loss: 1.002449631690979
Epoch 670, training loss: 0.07400341331958771 = 0.0072854855097830296 + 0.01 * 6.671792984008789
Epoch 670, val loss: 1.008378028869629
Epoch 680, training loss: 0.0736161321401596 = 0.006991962902247906 + 0.01 * 6.662417411804199
Epoch 680, val loss: 1.0144120454788208
Epoch 690, training loss: 0.07340112328529358 = 0.006716747302561998 + 0.01 * 6.668437480926514
Epoch 690, val loss: 1.020251989364624
Epoch 700, training loss: 0.07294613122940063 = 0.006459587253630161 + 0.01 * 6.648654460906982
Epoch 700, val loss: 1.0257724523544312
Epoch 710, training loss: 0.07281024754047394 = 0.006218384951353073 + 0.01 * 6.659186363220215
Epoch 710, val loss: 1.0313409566879272
Epoch 720, training loss: 0.07234293967485428 = 0.005992104299366474 + 0.01 * 6.6350836753845215
Epoch 720, val loss: 1.0366363525390625
Epoch 730, training loss: 0.07229965180158615 = 0.005779048893600702 + 0.01 * 6.652060031890869
Epoch 730, val loss: 1.041874885559082
Epoch 740, training loss: 0.07177318632602692 = 0.005579059012234211 + 0.01 * 6.619412899017334
Epoch 740, val loss: 1.0469435453414917
Epoch 750, training loss: 0.07177286595106125 = 0.005390672944486141 + 0.01 * 6.638219356536865
Epoch 750, val loss: 1.052071213722229
Epoch 760, training loss: 0.07145188003778458 = 0.005212806630879641 + 0.01 * 6.623907566070557
Epoch 760, val loss: 1.0569084882736206
Epoch 770, training loss: 0.07144756615161896 = 0.005045556463301182 + 0.01 * 6.640201091766357
Epoch 770, val loss: 1.0617417097091675
Epoch 780, training loss: 0.0710250586271286 = 0.004886441398411989 + 0.01 * 6.613862037658691
Epoch 780, val loss: 1.0662530660629272
Epoch 790, training loss: 0.07064912468194962 = 0.0047365049831569195 + 0.01 * 6.591262340545654
Epoch 790, val loss: 1.0709067583084106
Epoch 800, training loss: 0.07067069411277771 = 0.004593994934111834 + 0.01 * 6.607669830322266
Epoch 800, val loss: 1.075301170349121
Epoch 810, training loss: 0.07033105939626694 = 0.004459775984287262 + 0.01 * 6.587128162384033
Epoch 810, val loss: 1.0796457529067993
Epoch 820, training loss: 0.07023986428976059 = 0.004331760108470917 + 0.01 * 6.590810775756836
Epoch 820, val loss: 1.083975076675415
Epoch 830, training loss: 0.07023610174655914 = 0.00421047443524003 + 0.01 * 6.60256290435791
Epoch 830, val loss: 1.0880323648452759
Epoch 840, training loss: 0.06988602876663208 = 0.004095088690519333 + 0.01 * 6.579093933105469
Epoch 840, val loss: 1.0922316312789917
Epoch 850, training loss: 0.06974120438098907 = 0.003984875977039337 + 0.01 * 6.5756330490112305
Epoch 850, val loss: 1.0962133407592773
Epoch 860, training loss: 0.06952796131372452 = 0.0038803790230304003 + 0.01 * 6.564757823944092
Epoch 860, val loss: 1.1000723838806152
Epoch 870, training loss: 0.06958827376365662 = 0.003780640894547105 + 0.01 * 6.580763816833496
Epoch 870, val loss: 1.10381019115448
Epoch 880, training loss: 0.06917735934257507 = 0.0036854036152362823 + 0.01 * 6.549196243286133
Epoch 880, val loss: 1.1075547933578491
Epoch 890, training loss: 0.069366954267025 = 0.003594353562220931 + 0.01 * 6.577260494232178
Epoch 890, val loss: 1.1112438440322876
Epoch 900, training loss: 0.06940264254808426 = 0.0035074835177510977 + 0.01 * 6.589516639709473
Epoch 900, val loss: 1.1149269342422485
Epoch 910, training loss: 0.0688442811369896 = 0.003424342256039381 + 0.01 * 6.541994571685791
Epoch 910, val loss: 1.1182621717453003
Epoch 920, training loss: 0.06914161145687103 = 0.003344450145959854 + 0.01 * 6.579716682434082
Epoch 920, val loss: 1.1218607425689697
Epoch 930, training loss: 0.06845764070749283 = 0.0032678721472620964 + 0.01 * 6.518977642059326
Epoch 930, val loss: 1.1251463890075684
Epoch 940, training loss: 0.06863190978765488 = 0.0031944187358021736 + 0.01 * 6.5437493324279785
Epoch 940, val loss: 1.1285169124603271
Epoch 950, training loss: 0.06843053549528122 = 0.0031246093567460775 + 0.01 * 6.530592441558838
Epoch 950, val loss: 1.1318308115005493
Epoch 960, training loss: 0.06847935914993286 = 0.0030572039540857077 + 0.01 * 6.542215824127197
Epoch 960, val loss: 1.134909749031067
Epoch 970, training loss: 0.06822676211595535 = 0.00299239344894886 + 0.01 * 6.5234375
Epoch 970, val loss: 1.1381003856658936
Epoch 980, training loss: 0.06839168071746826 = 0.0029305173084139824 + 0.01 * 6.546116828918457
Epoch 980, val loss: 1.1412410736083984
Epoch 990, training loss: 0.06792189925909042 = 0.0028705173172056675 + 0.01 * 6.505138874053955
Epoch 990, val loss: 1.1442373991012573
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.5756
Flip ASR: 0.5333/225 nodes
The final ASR:0.51538, 0.06103, Accuracy:0.81605, 0.00873
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11584])
remove edge: torch.Size([2, 9464])
updated graph: torch.Size([2, 10492])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9963
Flip ASR: 0.9956/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.98524, 0.00797, Accuracy:0.83827, 0.00630
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.4 der2:0.2 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.023268222808838 = 1.9395297765731812 + 0.01 * 8.373848915100098
Epoch 0, val loss: 1.934279203414917
Epoch 10, training loss: 2.01408314704895 = 1.9303454160690308 + 0.01 * 8.373762130737305
Epoch 10, val loss: 1.9253610372543335
Epoch 20, training loss: 2.0026845932006836 = 1.9189496040344238 + 0.01 * 8.373503684997559
Epoch 20, val loss: 1.9142040014266968
Epoch 30, training loss: 1.9865466356277466 = 1.9028184413909912 + 0.01 * 8.372815132141113
Epoch 30, val loss: 1.8982810974121094
Epoch 40, training loss: 1.9621434211730957 = 1.878443717956543 + 0.01 * 8.369973182678223
Epoch 40, val loss: 1.874395728111267
Epoch 50, training loss: 1.9265260696411133 = 1.8430241346359253 + 0.01 * 8.350189208984375
Epoch 50, val loss: 1.8411154747009277
Epoch 60, training loss: 1.8816560506820679 = 1.7994009256362915 + 0.01 * 8.225512504577637
Epoch 60, val loss: 1.803682804107666
Epoch 70, training loss: 1.831117033958435 = 1.753602385520935 + 0.01 * 7.751470565795898
Epoch 70, val loss: 1.7665672302246094
Epoch 80, training loss: 1.77140212059021 = 1.697226881980896 + 0.01 * 7.417527675628662
Epoch 80, val loss: 1.7174017429351807
Epoch 90, training loss: 1.6952259540557861 = 1.6230413913726807 + 0.01 * 7.2184576988220215
Epoch 90, val loss: 1.6526199579238892
Epoch 100, training loss: 1.6003148555755615 = 1.529634714126587 + 0.01 * 7.068020343780518
Epoch 100, val loss: 1.5733418464660645
Epoch 110, training loss: 1.4960126876831055 = 1.4262446165084839 + 0.01 * 6.976803779602051
Epoch 110, val loss: 1.4876295328140259
Epoch 120, training loss: 1.3898694515228271 = 1.3206195831298828 + 0.01 * 6.924985885620117
Epoch 120, val loss: 1.4052599668502808
Epoch 130, training loss: 1.2861220836639404 = 1.2171822786331177 + 0.01 * 6.893985748291016
Epoch 130, val loss: 1.3286601305007935
Epoch 140, training loss: 1.1866521835327148 = 1.1178936958312988 + 0.01 * 6.875853538513184
Epoch 140, val loss: 1.2572581768035889
Epoch 150, training loss: 1.0920565128326416 = 1.0233912467956543 + 0.01 * 6.866525173187256
Epoch 150, val loss: 1.1900361776351929
Epoch 160, training loss: 1.002496600151062 = 0.933862566947937 + 0.01 * 6.863401889801025
Epoch 160, val loss: 1.1259137392044067
Epoch 170, training loss: 0.9176608920097351 = 0.8490259647369385 + 0.01 * 6.863495349884033
Epoch 170, val loss: 1.0646833181381226
Epoch 180, training loss: 0.8369749784469604 = 0.7683313488960266 + 0.01 * 6.86436128616333
Epoch 180, val loss: 1.005855679512024
Epoch 190, training loss: 0.7603052258491516 = 0.6916519999504089 + 0.01 * 6.865320205688477
Epoch 190, val loss: 0.9497747421264648
Epoch 200, training loss: 0.6881898045539856 = 0.6195265054702759 + 0.01 * 6.866331577301025
Epoch 200, val loss: 0.8974695205688477
Epoch 210, training loss: 0.6214607954025269 = 0.5527900457382202 + 0.01 * 6.8670735359191895
Epoch 210, val loss: 0.8502199053764343
Epoch 220, training loss: 0.5604352355003357 = 0.4917616546154022 + 0.01 * 6.867358684539795
Epoch 220, val loss: 0.8089094161987305
Epoch 230, training loss: 0.5047594308853149 = 0.43608662486076355 + 0.01 * 6.867278099060059
Epoch 230, val loss: 0.7740999460220337
Epoch 240, training loss: 0.4536135494709015 = 0.38494378328323364 + 0.01 * 6.866976737976074
Epoch 240, val loss: 0.7452427744865417
Epoch 250, training loss: 0.4064042568206787 = 0.3377378284931183 + 0.01 * 6.866641998291016
Epoch 250, val loss: 0.7218949794769287
Epoch 260, training loss: 0.36317136883735657 = 0.29450711607933044 + 0.01 * 6.86642599105835
Epoch 260, val loss: 0.7036865949630737
Epoch 270, training loss: 0.3243163228034973 = 0.25565287470817566 + 0.01 * 6.8663434982299805
Epoch 270, val loss: 0.6903883218765259
Epoch 280, training loss: 0.2902081608772278 = 0.2215445190668106 + 0.01 * 6.866365432739258
Epoch 280, val loss: 0.681956946849823
Epoch 290, training loss: 0.26087120175361633 = 0.19220666587352753 + 0.01 * 6.866452693939209
Epoch 290, val loss: 0.6780112981796265
Epoch 300, training loss: 0.23596519231796265 = 0.16729985177516937 + 0.01 * 6.86653470993042
Epoch 300, val loss: 0.6778371334075928
Epoch 310, training loss: 0.21493396162986755 = 0.14626851677894592 + 0.01 * 6.866545677185059
Epoch 310, val loss: 0.6806763410568237
Epoch 320, training loss: 0.19716539978981018 = 0.1285008043050766 + 0.01 * 6.866459846496582
Epoch 320, val loss: 0.6859736442565918
Epoch 330, training loss: 0.1820971667766571 = 0.11343535035848618 + 0.01 * 6.86618185043335
Epoch 330, val loss: 0.693105936050415
Epoch 340, training loss: 0.16923467814922333 = 0.10057830810546875 + 0.01 * 6.865637302398682
Epoch 340, val loss: 0.7014715075492859
Epoch 350, training loss: 0.15818104147911072 = 0.08953224867582321 + 0.01 * 6.864880084991455
Epoch 350, val loss: 0.7107592821121216
Epoch 360, training loss: 0.14862343668937683 = 0.07998348772525787 + 0.01 * 6.863994121551514
Epoch 360, val loss: 0.7206006646156311
Epoch 370, training loss: 0.14031049609184265 = 0.07168237119913101 + 0.01 * 6.86281156539917
Epoch 370, val loss: 0.7307848334312439
Epoch 380, training loss: 0.13304495811462402 = 0.06443291902542114 + 0.01 * 6.861203670501709
Epoch 380, val loss: 0.7411274909973145
Epoch 390, training loss: 0.12668228149414062 = 0.058078832924366 + 0.01 * 6.860343933105469
Epoch 390, val loss: 0.7514814138412476
Epoch 400, training loss: 0.12106910347938538 = 0.052491750568151474 + 0.01 * 6.857736110687256
Epoch 400, val loss: 0.7617976069450378
Epoch 410, training loss: 0.11611111462116241 = 0.04756517335772514 + 0.01 * 6.8545942306518555
Epoch 410, val loss: 0.7719751000404358
Epoch 420, training loss: 0.1117238700389862 = 0.04321080818772316 + 0.01 * 6.851306915283203
Epoch 420, val loss: 0.7819611430168152
Epoch 430, training loss: 0.10784366726875305 = 0.03935275971889496 + 0.01 * 6.849090576171875
Epoch 430, val loss: 0.7917712330818176
Epoch 440, training loss: 0.10436420887708664 = 0.035928986966609955 + 0.01 * 6.843522548675537
Epoch 440, val loss: 0.801406741142273
Epoch 450, training loss: 0.10125945508480072 = 0.03288636729121208 + 0.01 * 6.837308883666992
Epoch 450, val loss: 0.8108493089675903
Epoch 460, training loss: 0.09870494902133942 = 0.03017714060842991 + 0.01 * 6.852780818939209
Epoch 460, val loss: 0.8201065063476562
Epoch 470, training loss: 0.09607294946908951 = 0.02776121348142624 + 0.01 * 6.831173419952393
Epoch 470, val loss: 0.8291904926300049
Epoch 480, training loss: 0.09379768371582031 = 0.025602221488952637 + 0.01 * 6.819546222686768
Epoch 480, val loss: 0.8380857706069946
Epoch 490, training loss: 0.09174278378486633 = 0.023668993264436722 + 0.01 * 6.807378768920898
Epoch 490, val loss: 0.8468198776245117
Epoch 500, training loss: 0.08999136090278625 = 0.02193422242999077 + 0.01 * 6.805713653564453
Epoch 500, val loss: 0.8553705215454102
Epoch 510, training loss: 0.08836249262094498 = 0.020375274121761322 + 0.01 * 6.798721790313721
Epoch 510, val loss: 0.8636888265609741
Epoch 520, training loss: 0.08679769188165665 = 0.018971018493175507 + 0.01 * 6.782667636871338
Epoch 520, val loss: 0.87187260389328
Epoch 530, training loss: 0.08539024740457535 = 0.01770292967557907 + 0.01 * 6.768732070922852
Epoch 530, val loss: 0.8798406720161438
Epoch 540, training loss: 0.08422858268022537 = 0.016555165871977806 + 0.01 * 6.767341613769531
Epoch 540, val loss: 0.8876610994338989
Epoch 550, training loss: 0.08307524770498276 = 0.015514080412685871 + 0.01 * 6.7561163902282715
Epoch 550, val loss: 0.8952467441558838
Epoch 560, training loss: 0.0820448249578476 = 0.014567653648555279 + 0.01 * 6.747717380523682
Epoch 560, val loss: 0.9027335047721863
Epoch 570, training loss: 0.08103872090578079 = 0.01370534859597683 + 0.01 * 6.733336925506592
Epoch 570, val loss: 0.9099857211112976
Epoch 580, training loss: 0.08019313216209412 = 0.012917913496494293 + 0.01 * 6.727522373199463
Epoch 580, val loss: 0.9171047210693359
Epoch 590, training loss: 0.07942526787519455 = 0.012197220697999 + 0.01 * 6.722805023193359
Epoch 590, val loss: 0.9240626692771912
Epoch 600, training loss: 0.07866571098566055 = 0.011536392383277416 + 0.01 * 6.712932109832764
Epoch 600, val loss: 0.9309078454971313
Epoch 610, training loss: 0.07808665931224823 = 0.010928932577371597 + 0.01 * 6.7157721519470215
Epoch 610, val loss: 0.9376173615455627
Epoch 620, training loss: 0.07740148901939392 = 0.010369982570409775 + 0.01 * 6.703150749206543
Epoch 620, val loss: 0.9441561102867126
Epoch 630, training loss: 0.07681514322757721 = 0.009854423813521862 + 0.01 * 6.696072101593018
Epoch 630, val loss: 0.950528085231781
Epoch 640, training loss: 0.07627367973327637 = 0.009378446266055107 + 0.01 * 6.689523220062256
Epoch 640, val loss: 0.956807553768158
Epoch 650, training loss: 0.07579144090414047 = 0.008937983773648739 + 0.01 * 6.685345649719238
Epoch 650, val loss: 0.9629136919975281
Epoch 660, training loss: 0.0752594992518425 = 0.008530019782483578 + 0.01 * 6.672948360443115
Epoch 660, val loss: 0.9689136743545532
Epoch 670, training loss: 0.0748458206653595 = 0.008150840178132057 + 0.01 * 6.669498443603516
Epoch 670, val loss: 0.9747622013092041
Epoch 680, training loss: 0.07443362474441528 = 0.0077984752133488655 + 0.01 * 6.663515090942383
Epoch 680, val loss: 0.9805341958999634
Epoch 690, training loss: 0.07399192452430725 = 0.00746997632086277 + 0.01 * 6.652195453643799
Epoch 690, val loss: 0.9861663579940796
Epoch 700, training loss: 0.07378016412258148 = 0.0071632470935583115 + 0.01 * 6.661691665649414
Epoch 700, val loss: 0.991680920124054
Epoch 710, training loss: 0.07329247146844864 = 0.006877318024635315 + 0.01 * 6.641515254974365
Epoch 710, val loss: 0.9971067309379578
Epoch 720, training loss: 0.07297853380441666 = 0.006609490606933832 + 0.01 * 6.636904239654541
Epoch 720, val loss: 1.0023614168167114
Epoch 730, training loss: 0.0727287232875824 = 0.006358622573316097 + 0.01 * 6.637010097503662
Epoch 730, val loss: 1.007548451423645
Epoch 740, training loss: 0.07239963859319687 = 0.006123523693531752 + 0.01 * 6.6276116371154785
Epoch 740, val loss: 1.0126450061798096
Epoch 750, training loss: 0.07210378348827362 = 0.005902542732656002 + 0.01 * 6.620123863220215
Epoch 750, val loss: 1.017612099647522
Epoch 760, training loss: 0.07199954241514206 = 0.005694475490599871 + 0.01 * 6.63050651550293
Epoch 760, val loss: 1.0224599838256836
Epoch 770, training loss: 0.07163043320178986 = 0.005498767364770174 + 0.01 * 6.613166809082031
Epoch 770, val loss: 1.0272696018218994
Epoch 780, training loss: 0.07144610583782196 = 0.00531433941796422 + 0.01 * 6.6131768226623535
Epoch 780, val loss: 1.031922459602356
Epoch 790, training loss: 0.07115599513053894 = 0.0051404573023319244 + 0.01 * 6.601553440093994
Epoch 790, val loss: 1.0364948511123657
Epoch 800, training loss: 0.07104316353797913 = 0.0049760653637349606 + 0.01 * 6.606710433959961
Epoch 800, val loss: 1.0409772396087646
Epoch 810, training loss: 0.07076065987348557 = 0.004820498172193766 + 0.01 * 6.5940165519714355
Epoch 810, val loss: 1.0453414916992188
Epoch 820, training loss: 0.07053551822900772 = 0.004673546180129051 + 0.01 * 6.5861968994140625
Epoch 820, val loss: 1.049654245376587
Epoch 830, training loss: 0.07035136967897415 = 0.004534049890935421 + 0.01 * 6.581731796264648
Epoch 830, val loss: 1.053847312927246
Epoch 840, training loss: 0.07027550786733627 = 0.004401520360261202 + 0.01 * 6.587399005889893
Epoch 840, val loss: 1.0580110549926758
Epoch 850, training loss: 0.06997615098953247 = 0.004276250023394823 + 0.01 * 6.5699896812438965
Epoch 850, val loss: 1.0620197057724
Epoch 860, training loss: 0.07005058974027634 = 0.00415697880089283 + 0.01 * 6.589361190795898
Epoch 860, val loss: 1.0659514665603638
Epoch 870, training loss: 0.0697263777256012 = 0.004044024273753166 + 0.01 * 6.568235397338867
Epoch 870, val loss: 1.069809079170227
Epoch 880, training loss: 0.06957191973924637 = 0.003936065826565027 + 0.01 * 6.56358528137207
Epoch 880, val loss: 1.073624610900879
Epoch 890, training loss: 0.06955757737159729 = 0.003833415685221553 + 0.01 * 6.57241678237915
Epoch 890, val loss: 1.0773141384124756
Epoch 900, training loss: 0.06930084526538849 = 0.0037355876993387938 + 0.01 * 6.556526184082031
Epoch 900, val loss: 1.0809870958328247
Epoch 910, training loss: 0.06942931562662125 = 0.003642289899289608 + 0.01 * 6.578702926635742
Epoch 910, val loss: 1.0845277309417725
Epoch 920, training loss: 0.06904277205467224 = 0.0035529357846826315 + 0.01 * 6.548983573913574
Epoch 920, val loss: 1.0880944728851318
Epoch 930, training loss: 0.06884193420410156 = 0.0034679630771279335 + 0.01 * 6.537397384643555
Epoch 930, val loss: 1.0915253162384033
Epoch 940, training loss: 0.06875351071357727 = 0.003386590164154768 + 0.01 * 6.5366926193237305
Epoch 940, val loss: 1.094934344291687
Epoch 950, training loss: 0.06858771294355392 = 0.0033089600037783384 + 0.01 * 6.5278754234313965
Epoch 950, val loss: 1.0982333421707153
Epoch 960, training loss: 0.06861787289381027 = 0.0032345023937523365 + 0.01 * 6.538337230682373
Epoch 960, val loss: 1.1015243530273438
Epoch 970, training loss: 0.06839141994714737 = 0.0031633018516004086 + 0.01 * 6.5228118896484375
Epoch 970, val loss: 1.1046881675720215
Epoch 980, training loss: 0.0683676153421402 = 0.00309519050642848 + 0.01 * 6.527242660522461
Epoch 980, val loss: 1.1077851057052612
Epoch 990, training loss: 0.06812211871147156 = 0.0030296638142317533 + 0.01 * 6.5092453956604
Epoch 990, val loss: 1.1109462976455688
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.4 der2:0.2 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.033863067626953 = 1.9501255750656128 + 0.01 * 8.373754501342773
Epoch 0, val loss: 1.95084547996521
Epoch 10, training loss: 2.0234196186065674 = 1.9396836757659912 + 0.01 * 8.373595237731934
Epoch 10, val loss: 1.9406450986862183
Epoch 20, training loss: 2.0099966526031494 = 1.9262652397155762 + 0.01 * 8.373141288757324
Epoch 20, val loss: 1.9270132780075073
Epoch 30, training loss: 1.9906659126281738 = 1.9069492816925049 + 0.01 * 8.371668815612793
Epoch 30, val loss: 1.9070696830749512
Epoch 40, training loss: 1.9618552923202515 = 1.878233551979065 + 0.01 * 8.362175941467285
Epoch 40, val loss: 1.8777507543563843
Epoch 50, training loss: 1.9217333793640137 = 1.8387759923934937 + 0.01 * 8.29573917388916
Epoch 50, val loss: 1.8395847082138062
Epoch 60, training loss: 1.8756097555160522 = 1.795949935913086 + 0.01 * 7.965979099273682
Epoch 60, val loss: 1.8032599687576294
Epoch 70, training loss: 1.8335847854614258 = 1.7569050788879395 + 0.01 * 7.667970180511475
Epoch 70, val loss: 1.7733054161071777
Epoch 80, training loss: 1.7800602912902832 = 1.707323431968689 + 0.01 * 7.2736897468566895
Epoch 80, val loss: 1.7314722537994385
Epoch 90, training loss: 1.710141897201538 = 1.6402208805084229 + 0.01 * 6.992105484008789
Epoch 90, val loss: 1.6737157106399536
Epoch 100, training loss: 1.6230303049087524 = 1.5541229248046875 + 0.01 * 6.890742778778076
Epoch 100, val loss: 1.6023221015930176
Epoch 110, training loss: 1.526357889175415 = 1.4577531814575195 + 0.01 * 6.8604655265808105
Epoch 110, val loss: 1.5247166156768799
Epoch 120, training loss: 1.4297950267791748 = 1.3614211082458496 + 0.01 * 6.837395668029785
Epoch 120, val loss: 1.4504876136779785
Epoch 130, training loss: 1.334604024887085 = 1.2664430141448975 + 0.01 * 6.816102027893066
Epoch 130, val loss: 1.3795400857925415
Epoch 140, training loss: 1.239136815071106 = 1.1711721420288086 + 0.01 * 6.796468257904053
Epoch 140, val loss: 1.3099886178970337
Epoch 150, training loss: 1.143562912940979 = 1.075740098953247 + 0.01 * 6.7822771072387695
Epoch 150, val loss: 1.2400239706039429
Epoch 160, training loss: 1.0492802858352661 = 0.981541097164154 + 0.01 * 6.773916721343994
Epoch 160, val loss: 1.1701146364212036
Epoch 170, training loss: 0.9571606516838074 = 0.8894685506820679 + 0.01 * 6.769209384918213
Epoch 170, val loss: 1.101026177406311
Epoch 180, training loss: 0.8674119114875793 = 0.7997454404830933 + 0.01 * 6.766646862030029
Epoch 180, val loss: 1.0327274799346924
Epoch 190, training loss: 0.7806680202484131 = 0.7130423784255981 + 0.01 * 6.762564182281494
Epoch 190, val loss: 0.9669712781906128
Epoch 200, training loss: 0.6984869241714478 = 0.630897581577301 + 0.01 * 6.758935928344727
Epoch 200, val loss: 0.9057087302207947
Epoch 210, training loss: 0.6224084496498108 = 0.5548620820045471 + 0.01 * 6.754637241363525
Epoch 210, val loss: 0.8517265319824219
Epoch 220, training loss: 0.5531227588653564 = 0.4856187403202057 + 0.01 * 6.750405311584473
Epoch 220, val loss: 0.8068110942840576
Epoch 230, training loss: 0.49037864804267883 = 0.4229165315628052 + 0.01 * 6.746211528778076
Epoch 230, val loss: 0.770992636680603
Epoch 240, training loss: 0.4338323473930359 = 0.36641260981559753 + 0.01 * 6.741974830627441
Epoch 240, val loss: 0.7433019280433655
Epoch 250, training loss: 0.38318657875061035 = 0.3158152401447296 + 0.01 * 6.737133026123047
Epoch 250, val loss: 0.7225423455238342
Epoch 260, training loss: 0.33834534883499146 = 0.27098554372787476 + 0.01 * 6.735980987548828
Epoch 260, val loss: 0.7075363993644714
Epoch 270, training loss: 0.2992222309112549 = 0.2319173365831375 + 0.01 * 6.730489253997803
Epoch 270, val loss: 0.6975231170654297
Epoch 280, training loss: 0.2657252550125122 = 0.19849595427513123 + 0.01 * 6.722929954528809
Epoch 280, val loss: 0.6921828985214233
Epoch 290, training loss: 0.237482950091362 = 0.1702800989151001 + 0.01 * 6.720285415649414
Epoch 290, val loss: 0.6911889314651489
Epoch 300, training loss: 0.21385887265205383 = 0.14668558537960052 + 0.01 * 6.7173285484313965
Epoch 300, val loss: 0.6942425966262817
Epoch 310, training loss: 0.19409343600273132 = 0.12704326212406158 + 0.01 * 6.705017566680908
Epoch 310, val loss: 0.7007225155830383
Epoch 320, training loss: 0.17771294713020325 = 0.11071378737688065 + 0.01 * 6.699916362762451
Epoch 320, val loss: 0.7099155783653259
Epoch 330, training loss: 0.16402211785316467 = 0.0970798060297966 + 0.01 * 6.6942315101623535
Epoch 330, val loss: 0.721257209777832
Epoch 340, training loss: 0.15253567695617676 = 0.08560074865818024 + 0.01 * 6.693493843078613
Epoch 340, val loss: 0.7341579794883728
Epoch 350, training loss: 0.14269950985908508 = 0.07585269212722778 + 0.01 * 6.684682369232178
Epoch 350, val loss: 0.7481315732002258
Epoch 360, training loss: 0.1343957781791687 = 0.06750895828008652 + 0.01 * 6.688682556152344
Epoch 360, val loss: 0.76288241147995
Epoch 370, training loss: 0.12712733447551727 = 0.06032828614115715 + 0.01 * 6.679904460906982
Epoch 370, val loss: 0.7782886624336243
Epoch 380, training loss: 0.12086431682109833 = 0.05411956086754799 + 0.01 * 6.674476146697998
Epoch 380, val loss: 0.7940321564674377
Epoch 390, training loss: 0.11543954908847809 = 0.04872692748904228 + 0.01 * 6.671262741088867
Epoch 390, val loss: 0.8100084662437439
Epoch 400, training loss: 0.11069199442863464 = 0.044028379023075104 + 0.01 * 6.666361331939697
Epoch 400, val loss: 0.8259574174880981
Epoch 410, training loss: 0.10653109103441238 = 0.03992139548063278 + 0.01 * 6.6609697341918945
Epoch 410, val loss: 0.8419163823127747
Epoch 420, training loss: 0.10302375257015228 = 0.03632071614265442 + 0.01 * 6.670303821563721
Epoch 420, val loss: 0.8578025698661804
Epoch 430, training loss: 0.09976696223020554 = 0.03315979242324829 + 0.01 * 6.660717010498047
Epoch 430, val loss: 0.8734540939331055
Epoch 440, training loss: 0.09688901901245117 = 0.03037659451365471 + 0.01 * 6.651242256164551
Epoch 440, val loss: 0.8887470960617065
Epoch 450, training loss: 0.09436984360218048 = 0.027915358543395996 + 0.01 * 6.645448684692383
Epoch 450, val loss: 0.9038004279136658
Epoch 460, training loss: 0.09212866425514221 = 0.02573167346417904 + 0.01 * 6.6396989822387695
Epoch 460, val loss: 0.9184882044792175
Epoch 470, training loss: 0.09035005420446396 = 0.02378818392753601 + 0.01 * 6.656187057495117
Epoch 470, val loss: 0.9328790903091431
Epoch 480, training loss: 0.08834490925073624 = 0.022054143249988556 + 0.01 * 6.6290764808654785
Epoch 480, val loss: 0.9468090534210205
Epoch 490, training loss: 0.08679007738828659 = 0.020497450605034828 + 0.01 * 6.629262924194336
Epoch 490, val loss: 0.9603163003921509
Epoch 500, training loss: 0.08536671847105026 = 0.019099488854408264 + 0.01 * 6.626723289489746
Epoch 500, val loss: 0.9735772013664246
Epoch 510, training loss: 0.0840238481760025 = 0.017841016873717308 + 0.01 * 6.618283271789551
Epoch 510, val loss: 0.986283004283905
Epoch 520, training loss: 0.08309628069400787 = 0.01670471951365471 + 0.01 * 6.639155864715576
Epoch 520, val loss: 0.998677134513855
Epoch 530, training loss: 0.0816667377948761 = 0.015676699578762054 + 0.01 * 6.59900426864624
Epoch 530, val loss: 1.0106699466705322
Epoch 540, training loss: 0.08068446815013885 = 0.014742007479071617 + 0.01 * 6.5942463874816895
Epoch 540, val loss: 1.0224162340164185
Epoch 550, training loss: 0.07989631593227386 = 0.013889488764107227 + 0.01 * 6.600682735443115
Epoch 550, val loss: 1.03369140625
Epoch 560, training loss: 0.07906030863523483 = 0.013111029751598835 + 0.01 * 6.594927787780762
Epoch 560, val loss: 1.04478919506073
Epoch 570, training loss: 0.07830724865198135 = 0.01239925529807806 + 0.01 * 6.590799331665039
Epoch 570, val loss: 1.0554835796356201
Epoch 580, training loss: 0.07742886245250702 = 0.011746960692107677 + 0.01 * 6.568190574645996
Epoch 580, val loss: 1.0658438205718994
Epoch 590, training loss: 0.07699050009250641 = 0.011147108860313892 + 0.01 * 6.584339141845703
Epoch 590, val loss: 1.0758854150772095
Epoch 600, training loss: 0.0762743204832077 = 0.010594533756375313 + 0.01 * 6.567979335784912
Epoch 600, val loss: 1.0857458114624023
Epoch 610, training loss: 0.07567263394594193 = 0.010083488188683987 + 0.01 * 6.558914661407471
Epoch 610, val loss: 1.0954022407531738
Epoch 620, training loss: 0.07530342042446136 = 0.009609995409846306 + 0.01 * 6.569342136383057
Epoch 620, val loss: 1.1045619249343872
Epoch 630, training loss: 0.07469023019075394 = 0.009171606972813606 + 0.01 * 6.551862716674805
Epoch 630, val loss: 1.1137455701828003
Epoch 640, training loss: 0.07421182841062546 = 0.008764296770095825 + 0.01 * 6.544753551483154
Epoch 640, val loss: 1.1224530935287476
Epoch 650, training loss: 0.0738321989774704 = 0.008385531604290009 + 0.01 * 6.544666767120361
Epoch 650, val loss: 1.1311051845550537
Epoch 660, training loss: 0.07331949472427368 = 0.00803245510905981 + 0.01 * 6.5287041664123535
Epoch 660, val loss: 1.1394743919372559
Epoch 670, training loss: 0.07315131276845932 = 0.007702774368226528 + 0.01 * 6.544854164123535
Epoch 670, val loss: 1.1475552320480347
Epoch 680, training loss: 0.07269524037837982 = 0.007395617663860321 + 0.01 * 6.529962539672852
Epoch 680, val loss: 1.1554981470108032
Epoch 690, training loss: 0.07230085134506226 = 0.007107951212674379 + 0.01 * 6.519290447235107
Epoch 690, val loss: 1.1632105112075806
Epoch 700, training loss: 0.07196792215108871 = 0.0068381172604858875 + 0.01 * 6.5129804611206055
Epoch 700, val loss: 1.1707795858383179
Epoch 710, training loss: 0.07209160923957825 = 0.006584932096302509 + 0.01 * 6.5506672859191895
Epoch 710, val loss: 1.1779608726501465
Epoch 720, training loss: 0.07138610631227493 = 0.006347410846501589 + 0.01 * 6.503870010375977
Epoch 720, val loss: 1.185203194618225
Epoch 730, training loss: 0.07114589214324951 = 0.006123967934399843 + 0.01 * 6.502192974090576
Epoch 730, val loss: 1.1922607421875
Epoch 740, training loss: 0.0709490180015564 = 0.005913136526942253 + 0.01 * 6.5035881996154785
Epoch 740, val loss: 1.1990808248519897
Epoch 750, training loss: 0.07073094695806503 = 0.005714201368391514 + 0.01 * 6.501675128936768
Epoch 750, val loss: 1.2056931257247925
Epoch 760, training loss: 0.07041151076555252 = 0.005526310298591852 + 0.01 * 6.48852014541626
Epoch 760, val loss: 1.2122554779052734
Epoch 770, training loss: 0.07050799578428268 = 0.0053485543467104435 + 0.01 * 6.515944480895996
Epoch 770, val loss: 1.2185636758804321
Epoch 780, training loss: 0.06993541121482849 = 0.005180285777896643 + 0.01 * 6.475512981414795
Epoch 780, val loss: 1.2248541116714478
Epoch 790, training loss: 0.06986891478300095 = 0.005021611228585243 + 0.01 * 6.4847307205200195
Epoch 790, val loss: 1.2308329343795776
Epoch 800, training loss: 0.06983531266450882 = 0.004870075266808271 + 0.01 * 6.496524333953857
Epoch 800, val loss: 1.236867070198059
Epoch 810, training loss: 0.06932959705591202 = 0.004727283027023077 + 0.01 * 6.460231781005859
Epoch 810, val loss: 1.2425557374954224
Epoch 820, training loss: 0.06910447031259537 = 0.004591347649693489 + 0.01 * 6.45131254196167
Epoch 820, val loss: 1.2481998205184937
Epoch 830, training loss: 0.06924263387918472 = 0.004462162498384714 + 0.01 * 6.4780473709106445
Epoch 830, val loss: 1.2537689208984375
Epoch 840, training loss: 0.06904064118862152 = 0.00433865562081337 + 0.01 * 6.470199108123779
Epoch 840, val loss: 1.2591915130615234
Epoch 850, training loss: 0.06886561214923859 = 0.004221437033265829 + 0.01 * 6.464417457580566
Epoch 850, val loss: 1.2644885778427124
Epoch 860, training loss: 0.06845295429229736 = 0.004109813831746578 + 0.01 * 6.434313774108887
Epoch 860, val loss: 1.269724726676941
Epoch 870, training loss: 0.06851023435592651 = 0.004003223497420549 + 0.01 * 6.4507012367248535
Epoch 870, val loss: 1.274800419807434
Epoch 880, training loss: 0.06851136684417725 = 0.003901486983522773 + 0.01 * 6.460987567901611
Epoch 880, val loss: 1.2798131704330444
Epoch 890, training loss: 0.06801693141460419 = 0.003804972395300865 + 0.01 * 6.421195983886719
Epoch 890, val loss: 1.2846311330795288
Epoch 900, training loss: 0.0680491104722023 = 0.003712024772539735 + 0.01 * 6.433708667755127
Epoch 900, val loss: 1.2894785404205322
Epoch 910, training loss: 0.06812205910682678 = 0.0036235658917576075 + 0.01 * 6.449849605560303
Epoch 910, val loss: 1.2941431999206543
Epoch 920, training loss: 0.06778421252965927 = 0.003539390629157424 + 0.01 * 6.424482345581055
Epoch 920, val loss: 1.298633098602295
Epoch 930, training loss: 0.06765026599168777 = 0.003458314575254917 + 0.01 * 6.419195652008057
Epoch 930, val loss: 1.3031576871871948
Epoch 940, training loss: 0.06739319860935211 = 0.0033805910497903824 + 0.01 * 6.401261329650879
Epoch 940, val loss: 1.3075212240219116
Epoch 950, training loss: 0.06752584874629974 = 0.0033060889691114426 + 0.01 * 6.421975612640381
Epoch 950, val loss: 1.311887502670288
Epoch 960, training loss: 0.0673387348651886 = 0.0032348891254514456 + 0.01 * 6.4103851318359375
Epoch 960, val loss: 1.3160656690597534
Epoch 970, training loss: 0.06706051528453827 = 0.003166607115417719 + 0.01 * 6.389390468597412
Epoch 970, val loss: 1.320178747177124
Epoch 980, training loss: 0.06708827614784241 = 0.0031007896177470684 + 0.01 * 6.398748874664307
Epoch 980, val loss: 1.324188470840454
Epoch 990, training loss: 0.06689252704381943 = 0.0030377795919775963 + 0.01 * 6.385474681854248
Epoch 990, val loss: 1.3282771110534668
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.4 der2:0.2 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0257346630096436 = 1.9419959783554077 + 0.01 * 8.37387752532959
Epoch 0, val loss: 1.9395928382873535
Epoch 10, training loss: 2.015195608139038 = 1.931457757949829 + 0.01 * 8.37379264831543
Epoch 10, val loss: 1.929108738899231
Epoch 20, training loss: 2.001856803894043 = 1.9181212186813354 + 0.01 * 8.373568534851074
Epoch 20, val loss: 1.9157145023345947
Epoch 30, training loss: 1.9829659461975098 = 1.899236798286438 + 0.01 * 8.372919082641602
Epoch 30, val loss: 1.8968122005462646
Epoch 40, training loss: 1.955356478691101 = 1.8716601133346558 + 0.01 * 8.36964225769043
Epoch 40, val loss: 1.8698817491531372
Epoch 50, training loss: 1.9181315898895264 = 1.8346567153930664 + 0.01 * 8.34748649597168
Epoch 50, val loss: 1.836127758026123
Epoch 60, training loss: 1.8775091171264648 = 1.7951565980911255 + 0.01 * 8.235252380371094
Epoch 60, val loss: 1.80477774143219
Epoch 70, training loss: 1.8365719318389893 = 1.75654137134552 + 0.01 * 8.003060340881348
Epoch 70, val loss: 1.7749744653701782
Epoch 80, training loss: 1.7826077938079834 = 1.7045470476150513 + 0.01 * 7.806075096130371
Epoch 80, val loss: 1.7305872440338135
Epoch 90, training loss: 1.7093473672866821 = 1.63296377658844 + 0.01 * 7.638359069824219
Epoch 90, val loss: 1.669782042503357
Epoch 100, training loss: 1.618102788925171 = 1.5443214178085327 + 0.01 * 7.378136157989502
Epoch 100, val loss: 1.5968296527862549
Epoch 110, training loss: 1.5235675573349 = 1.451849341392517 + 0.01 * 7.171826362609863
Epoch 110, val loss: 1.522347092628479
Epoch 120, training loss: 1.4339160919189453 = 1.362980604171753 + 0.01 * 7.093554973602295
Epoch 120, val loss: 1.4558615684509277
Epoch 130, training loss: 1.3484102487564087 = 1.278049349784851 + 0.01 * 7.0360870361328125
Epoch 130, val loss: 1.3969261646270752
Epoch 140, training loss: 1.2666287422180176 = 1.196648120880127 + 0.01 * 6.998068332672119
Epoch 140, val loss: 1.3426576852798462
Epoch 150, training loss: 1.1883649826049805 = 1.1186619997024536 + 0.01 * 6.9702935218811035
Epoch 150, val loss: 1.2936686277389526
Epoch 160, training loss: 1.1130026578903198 = 1.0435768365859985 + 0.01 * 6.942585468292236
Epoch 160, val loss: 1.249369740486145
Epoch 170, training loss: 1.0397179126739502 = 0.970601499080658 + 0.01 * 6.91163969039917
Epoch 170, val loss: 1.2074836492538452
Epoch 180, training loss: 0.9686683416366577 = 0.8998057842254639 + 0.01 * 6.886255264282227
Epoch 180, val loss: 1.1663126945495605
Epoch 190, training loss: 0.9005698561668396 = 0.8319208025932312 + 0.01 * 6.864907741546631
Epoch 190, val loss: 1.1258232593536377
Epoch 200, training loss: 0.8358756899833679 = 0.7673782706260681 + 0.01 * 6.8497419357299805
Epoch 200, val loss: 1.0861971378326416
Epoch 210, training loss: 0.7741102576255798 = 0.7057031393051147 + 0.01 * 6.840710639953613
Epoch 210, val loss: 1.0476363897323608
Epoch 220, training loss: 0.7138463854789734 = 0.6455283164978027 + 0.01 * 6.831805229187012
Epoch 220, val loss: 1.0091781616210938
Epoch 230, training loss: 0.6535502076148987 = 0.5853024125099182 + 0.01 * 6.824779987335205
Epoch 230, val loss: 0.9700465202331543
Epoch 240, training loss: 0.5925508737564087 = 0.5243744850158691 + 0.01 * 6.817636966705322
Epoch 240, val loss: 0.9310225248336792
Epoch 250, training loss: 0.5315759778022766 = 0.4634642004966736 + 0.01 * 6.8111796379089355
Epoch 250, val loss: 0.8938432335853577
Epoch 260, training loss: 0.4727121591567993 = 0.404654324054718 + 0.01 * 6.805785179138184
Epoch 260, val loss: 0.8613202571868896
Epoch 270, training loss: 0.41853559017181396 = 0.35051727294921875 + 0.01 * 6.801833629608154
Epoch 270, val loss: 0.8356592059135437
Epoch 280, training loss: 0.37086182832717896 = 0.3028818964958191 + 0.01 * 6.797993183135986
Epoch 280, val loss: 0.8182446956634521
Epoch 290, training loss: 0.3300716280937195 = 0.26210758090019226 + 0.01 * 6.796403884887695
Epoch 290, val loss: 0.8089203834533691
Epoch 300, training loss: 0.2951321601867676 = 0.2272017002105713 + 0.01 * 6.793045997619629
Epoch 300, val loss: 0.8063217997550964
Epoch 310, training loss: 0.26472926139831543 = 0.1968144178390503 + 0.01 * 6.7914862632751465
Epoch 310, val loss: 0.8087933659553528
Epoch 320, training loss: 0.2380211055278778 = 0.1701263189315796 + 0.01 * 6.789477825164795
Epoch 320, val loss: 0.8150731325149536
Epoch 330, training loss: 0.21473418176174164 = 0.14684134721755981 + 0.01 * 6.789283275604248
Epoch 330, val loss: 0.8243370056152344
Epoch 340, training loss: 0.19470658898353577 = 0.12684309482574463 + 0.01 * 6.786349296569824
Epoch 340, val loss: 0.8358786106109619
Epoch 350, training loss: 0.1777435839176178 = 0.10990644246339798 + 0.01 * 6.7837138175964355
Epoch 350, val loss: 0.8493237495422363
Epoch 360, training loss: 0.16349297761917114 = 0.09568525105714798 + 0.01 * 6.7807722091674805
Epoch 360, val loss: 0.8644079566001892
Epoch 370, training loss: 0.1515592336654663 = 0.08378282934427261 + 0.01 * 6.7776408195495605
Epoch 370, val loss: 0.8805620670318604
Epoch 380, training loss: 0.1415884792804718 = 0.07381591945886612 + 0.01 * 6.777255535125732
Epoch 380, val loss: 0.8976052403450012
Epoch 390, training loss: 0.1331629455089569 = 0.06543486565351486 + 0.01 * 6.77280855178833
Epoch 390, val loss: 0.9151858687400818
Epoch 400, training loss: 0.12603046000003815 = 0.05834466964006424 + 0.01 * 6.768579483032227
Epoch 400, val loss: 0.9330649971961975
Epoch 410, training loss: 0.11994799971580505 = 0.052302513271570206 + 0.01 * 6.7645487785339355
Epoch 410, val loss: 0.9509232044219971
Epoch 420, training loss: 0.11476368457078934 = 0.0471111461520195 + 0.01 * 6.765254020690918
Epoch 420, val loss: 0.9687908887863159
Epoch 430, training loss: 0.11019044369459152 = 0.04260982573032379 + 0.01 * 6.75806188583374
Epoch 430, val loss: 0.9863463044166565
Epoch 440, training loss: 0.1062016636133194 = 0.038671866059303284 + 0.01 * 6.7529802322387695
Epoch 440, val loss: 1.0037987232208252
Epoch 450, training loss: 0.10271900147199631 = 0.03520236909389496 + 0.01 * 6.751663684844971
Epoch 450, val loss: 1.0208117961883545
Epoch 460, training loss: 0.09960673004388809 = 0.03214079141616821 + 0.01 * 6.746593952178955
Epoch 460, val loss: 1.0375672578811646
Epoch 470, training loss: 0.09682479500770569 = 0.02942662686109543 + 0.01 * 6.739817142486572
Epoch 470, val loss: 1.0537914037704468
Epoch 480, training loss: 0.09435135871171951 = 0.027009909972548485 + 0.01 * 6.734145164489746
Epoch 480, val loss: 1.0697143077850342
Epoch 490, training loss: 0.09230214357376099 = 0.024852236732840538 + 0.01 * 6.744990825653076
Epoch 490, val loss: 1.0851396322250366
Epoch 500, training loss: 0.09022315591573715 = 0.022924216464161873 + 0.01 * 6.729894638061523
Epoch 500, val loss: 1.1002507209777832
Epoch 510, training loss: 0.0883801281452179 = 0.021197404712438583 + 0.01 * 6.718273162841797
Epoch 510, val loss: 1.1149274110794067
Epoch 520, training loss: 0.08678613603115082 = 0.0196469034999609 + 0.01 * 6.713923454284668
Epoch 520, val loss: 1.129251480102539
Epoch 530, training loss: 0.08536096662282944 = 0.018251977860927582 + 0.01 * 6.7108988761901855
Epoch 530, val loss: 1.1431571245193481
Epoch 540, training loss: 0.08403398841619492 = 0.016997070983052254 + 0.01 * 6.7036919593811035
Epoch 540, val loss: 1.1566935777664185
Epoch 550, training loss: 0.08287684619426727 = 0.015865659341216087 + 0.01 * 6.701118469238281
Epoch 550, val loss: 1.169685959815979
Epoch 560, training loss: 0.08177600055932999 = 0.014841311611235142 + 0.01 * 6.693469047546387
Epoch 560, val loss: 1.182422161102295
Epoch 570, training loss: 0.08077804744243622 = 0.013910874724388123 + 0.01 * 6.686717510223389
Epoch 570, val loss: 1.1947332620620728
Epoch 580, training loss: 0.07988721877336502 = 0.013063887134194374 + 0.01 * 6.682333469390869
Epoch 580, val loss: 1.2067757844924927
Epoch 590, training loss: 0.07930771261453629 = 0.012293180450797081 + 0.01 * 6.70145320892334
Epoch 590, val loss: 1.2183449268341064
Epoch 600, training loss: 0.0783374011516571 = 0.011590898036956787 + 0.01 * 6.6746506690979
Epoch 600, val loss: 1.229557752609253
Epoch 610, training loss: 0.07764965295791626 = 0.01094833668321371 + 0.01 * 6.670131683349609
Epoch 610, val loss: 1.2405378818511963
Epoch 620, training loss: 0.07700767368078232 = 0.010358599945902824 + 0.01 * 6.664906978607178
Epoch 620, val loss: 1.2511292695999146
Epoch 630, training loss: 0.07642048597335815 = 0.00981616135686636 + 0.01 * 6.660432815551758
Epoch 630, val loss: 1.261457920074463
Epoch 640, training loss: 0.07590196281671524 = 0.009316462092101574 + 0.01 * 6.658550262451172
Epoch 640, val loss: 1.2714028358459473
Epoch 650, training loss: 0.07547051459550858 = 0.008855895139276981 + 0.01 * 6.66146183013916
Epoch 650, val loss: 1.2811225652694702
Epoch 660, training loss: 0.07494106888771057 = 0.008430085144937038 + 0.01 * 6.651098728179932
Epoch 660, val loss: 1.2906451225280762
Epoch 670, training loss: 0.07449657469987869 = 0.008035950362682343 + 0.01 * 6.646062850952148
Epoch 670, val loss: 1.299723744392395
Epoch 680, training loss: 0.0740794762969017 = 0.007670573890209198 + 0.01 * 6.640890598297119
Epoch 680, val loss: 1.3086258172988892
Epoch 690, training loss: 0.07370199263095856 = 0.00733093312010169 + 0.01 * 6.637105941772461
Epoch 690, val loss: 1.317322850227356
Epoch 700, training loss: 0.0734214037656784 = 0.00701473094522953 + 0.01 * 6.640667915344238
Epoch 700, val loss: 1.325705885887146
Epoch 710, training loss: 0.07302425056695938 = 0.006720001809298992 + 0.01 * 6.630425453186035
Epoch 710, val loss: 1.333978533744812
Epoch 720, training loss: 0.07273907214403152 = 0.006444833241403103 + 0.01 * 6.62942361831665
Epoch 720, val loss: 1.341883659362793
Epoch 730, training loss: 0.07252927869558334 = 0.006187714170664549 + 0.01 * 6.634156227111816
Epoch 730, val loss: 1.349616289138794
Epoch 740, training loss: 0.07213976234197617 = 0.00594692025333643 + 0.01 * 6.619284629821777
Epoch 740, val loss: 1.3572272062301636
Epoch 750, training loss: 0.07195626944303513 = 0.005721000023186207 + 0.01 * 6.6235270500183105
Epoch 750, val loss: 1.3645111322402954
Epoch 760, training loss: 0.07178681343793869 = 0.0055090393871068954 + 0.01 * 6.627777576446533
Epoch 760, val loss: 1.3717129230499268
Epoch 770, training loss: 0.07143451273441315 = 0.005309964995831251 + 0.01 * 6.612455368041992
Epoch 770, val loss: 1.378582239151001
Epoch 780, training loss: 0.0712457001209259 = 0.005122622009366751 + 0.01 * 6.612307548522949
Epoch 780, val loss: 1.3852920532226562
Epoch 790, training loss: 0.07101655006408691 = 0.0049461182206869125 + 0.01 * 6.607043266296387
Epoch 790, val loss: 1.3920036554336548
Epoch 800, training loss: 0.0707755982875824 = 0.0047796037979424 + 0.01 * 6.599599361419678
Epoch 800, val loss: 1.3984510898590088
Epoch 810, training loss: 0.07076266407966614 = 0.004622272681444883 + 0.01 * 6.614039421081543
Epoch 810, val loss: 1.4046555757522583
Epoch 820, training loss: 0.07053330540657043 = 0.004473567008972168 + 0.01 * 6.605974197387695
Epoch 820, val loss: 1.4108996391296387
Epoch 830, training loss: 0.07029764354228973 = 0.0043329945765435696 + 0.01 * 6.59646463394165
Epoch 830, val loss: 1.416800618171692
Epoch 840, training loss: 0.07013949751853943 = 0.004199836403131485 + 0.01 * 6.593966484069824
Epoch 840, val loss: 1.4227436780929565
Epoch 850, training loss: 0.06997688114643097 = 0.004073459655046463 + 0.01 * 6.590342998504639
Epoch 850, val loss: 1.4284348487854004
Epoch 860, training loss: 0.06989022344350815 = 0.003953626379370689 + 0.01 * 6.5936598777771
Epoch 860, val loss: 1.4339931011199951
Epoch 870, training loss: 0.06970695406198502 = 0.003839673940092325 + 0.01 * 6.586728096008301
Epoch 870, val loss: 1.4395509958267212
Epoch 880, training loss: 0.06966260075569153 = 0.003731532022356987 + 0.01 * 6.593106746673584
Epoch 880, val loss: 1.444877028465271
Epoch 890, training loss: 0.0694059506058693 = 0.0036286150570958853 + 0.01 * 6.577733993530273
Epoch 890, val loss: 1.4500547647476196
Epoch 900, training loss: 0.06935679912567139 = 0.003530567279085517 + 0.01 * 6.58262300491333
Epoch 900, val loss: 1.4551451206207275
Epoch 910, training loss: 0.06919779628515244 = 0.0034372361842542887 + 0.01 * 6.576056480407715
Epoch 910, val loss: 1.4601608514785767
Epoch 920, training loss: 0.06906764209270477 = 0.0033481952268630266 + 0.01 * 6.5719451904296875
Epoch 920, val loss: 1.4650365114212036
Epoch 930, training loss: 0.0689162090420723 = 0.0032632157672196627 + 0.01 * 6.5652995109558105
Epoch 930, val loss: 1.469773769378662
Epoch 940, training loss: 0.06884212791919708 = 0.0031820472795516253 + 0.01 * 6.566008567810059
Epoch 940, val loss: 1.4744129180908203
Epoch 950, training loss: 0.06896429508924484 = 0.0031044255010783672 + 0.01 * 6.585987091064453
Epoch 950, val loss: 1.4790698289871216
Epoch 960, training loss: 0.06869310140609741 = 0.003030445659533143 + 0.01 * 6.566265106201172
Epoch 960, val loss: 1.483411431312561
Epoch 970, training loss: 0.06852643936872482 = 0.0029595664236694574 + 0.01 * 6.556687355041504
Epoch 970, val loss: 1.4877300262451172
Epoch 980, training loss: 0.0684194415807724 = 0.002891730982810259 + 0.01 * 6.552771091461182
Epoch 980, val loss: 1.4921070337295532
Epoch 990, training loss: 0.06854195892810822 = 0.002826729090884328 + 0.01 * 6.571523189544678
Epoch 990, val loss: 1.4962393045425415
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.5203
Flip ASR: 0.4267/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0253686904907227 = 1.941630244255066 + 0.01 * 8.373836517333984
Epoch 0, val loss: 1.9392011165618896
Epoch 10, training loss: 2.0148751735687256 = 1.9311387538909912 + 0.01 * 8.373641967773438
Epoch 10, val loss: 1.9270670413970947
Epoch 20, training loss: 2.002230167388916 = 1.9185010194778442 + 0.01 * 8.372923851013184
Epoch 20, val loss: 1.9117209911346436
Epoch 30, training loss: 1.9847095012664795 = 1.9010019302368164 + 0.01 * 8.370760917663574
Epoch 30, val loss: 1.8899500370025635
Epoch 40, training loss: 1.959504246711731 = 1.8758654594421387 + 0.01 * 8.363882064819336
Epoch 40, val loss: 1.8589859008789062
Epoch 50, training loss: 1.9245871305465698 = 1.8412322998046875 + 0.01 * 8.335480690002441
Epoch 50, val loss: 1.818664789199829
Epoch 60, training loss: 1.88162100315094 = 1.7998378276824951 + 0.01 * 8.178318977355957
Epoch 60, val loss: 1.7755255699157715
Epoch 70, training loss: 1.8352689743041992 = 1.7578204870224 + 0.01 * 7.744844436645508
Epoch 70, val loss: 1.7381762266159058
Epoch 80, training loss: 1.7845954895019531 = 1.7088297605514526 + 0.01 * 7.5765790939331055
Epoch 80, val loss: 1.6987998485565186
Epoch 90, training loss: 1.7178428173065186 = 1.6430796384811401 + 0.01 * 7.476313591003418
Epoch 90, val loss: 1.6463098526000977
Epoch 100, training loss: 1.6321442127227783 = 1.5591187477111816 + 0.01 * 7.302544593811035
Epoch 100, val loss: 1.5782800912857056
Epoch 110, training loss: 1.5353076457977295 = 1.4638166427612305 + 0.01 * 7.149094581604004
Epoch 110, val loss: 1.502427101135254
Epoch 120, training loss: 1.436944842338562 = 1.365966558456421 + 0.01 * 7.097828388214111
Epoch 120, val loss: 1.4286706447601318
Epoch 130, training loss: 1.3414865732192993 = 1.270858883857727 + 0.01 * 7.062767505645752
Epoch 130, val loss: 1.3607500791549683
Epoch 140, training loss: 1.2495001554489136 = 1.1791056394577026 + 0.01 * 7.039450168609619
Epoch 140, val loss: 1.298004388809204
Epoch 150, training loss: 1.1601532697677612 = 1.0899243354797363 + 0.01 * 7.022898197174072
Epoch 150, val loss: 1.237309455871582
Epoch 160, training loss: 1.072847604751587 = 1.0027306079864502 + 0.01 * 7.011704921722412
Epoch 160, val loss: 1.1778582334518433
Epoch 170, training loss: 0.9878794550895691 = 0.917853057384491 + 0.01 * 7.002638816833496
Epoch 170, val loss: 1.1196662187576294
Epoch 180, training loss: 0.9061771035194397 = 0.8362624049186707 + 0.01 * 6.9914703369140625
Epoch 180, val loss: 1.0630218982696533
Epoch 190, training loss: 0.828976035118103 = 0.759197473526001 + 0.01 * 6.977858543395996
Epoch 190, val loss: 1.0091876983642578
Epoch 200, training loss: 0.7578476071357727 = 0.688218891620636 + 0.01 * 6.962870121002197
Epoch 200, val loss: 0.9589869976043701
Epoch 210, training loss: 0.694172203540802 = 0.6246802806854248 + 0.01 * 6.949191570281982
Epoch 210, val loss: 0.9140423536300659
Epoch 220, training loss: 0.638390064239502 = 0.5689868927001953 + 0.01 * 6.940316200256348
Epoch 220, val loss: 0.8753405809402466
Epoch 230, training loss: 0.5898670554161072 = 0.5205678939819336 + 0.01 * 6.92991828918457
Epoch 230, val loss: 0.8431443572044373
Epoch 240, training loss: 0.5472428798675537 = 0.4780122637748718 + 0.01 * 6.923062801361084
Epoch 240, val loss: 0.8170771598815918
Epoch 250, training loss: 0.5086113214492798 = 0.4394397437572479 + 0.01 * 6.91715669631958
Epoch 250, val loss: 0.7954770922660828
Epoch 260, training loss: 0.4721771478652954 = 0.4030543267726898 + 0.01 * 6.912283420562744
Epoch 260, val loss: 0.7770470380783081
Epoch 270, training loss: 0.436448872089386 = 0.36739668250083923 + 0.01 * 6.905218601226807
Epoch 270, val loss: 0.7602294683456421
Epoch 280, training loss: 0.4007686972618103 = 0.33174940943717957 + 0.01 * 6.9019293785095215
Epoch 280, val loss: 0.7444037795066833
Epoch 290, training loss: 0.36511293053627014 = 0.29617106914520264 + 0.01 * 6.894186019897461
Epoch 290, val loss: 0.7300837635993958
Epoch 300, training loss: 0.33012574911117554 = 0.26124894618988037 + 0.01 * 6.887679100036621
Epoch 300, val loss: 0.7175655961036682
Epoch 310, training loss: 0.2968100607395172 = 0.22796417772769928 + 0.01 * 6.88458776473999
Epoch 310, val loss: 0.7079318761825562
Epoch 320, training loss: 0.2660040557384491 = 0.19726262986660004 + 0.01 * 6.874141693115234
Epoch 320, val loss: 0.7012062072753906
Epoch 330, training loss: 0.23860284686088562 = 0.16992518305778503 + 0.01 * 6.867767333984375
Epoch 330, val loss: 0.6981161236763
Epoch 340, training loss: 0.21558606624603271 = 0.14696922898292542 + 0.01 * 6.8616838455200195
Epoch 340, val loss: 0.6983903050422668
Epoch 350, training loss: 0.19635748863220215 = 0.12776699662208557 + 0.01 * 6.8590497970581055
Epoch 350, val loss: 0.7013731598854065
Epoch 360, training loss: 0.18035247921943665 = 0.11181246489286423 + 0.01 * 6.854001522064209
Epoch 360, val loss: 0.706989586353302
Epoch 370, training loss: 0.16692174971103668 = 0.09853219240903854 + 0.01 * 6.838955879211426
Epoch 370, val loss: 0.7154080867767334
Epoch 380, training loss: 0.15558965504169464 = 0.08734070509672165 + 0.01 * 6.82489538192749
Epoch 380, val loss: 0.725911557674408
Epoch 390, training loss: 0.14590919017791748 = 0.07777675986289978 + 0.01 * 6.813243389129639
Epoch 390, val loss: 0.7374680638313293
Epoch 400, training loss: 0.13767355680465698 = 0.06947875767946243 + 0.01 * 6.819479465484619
Epoch 400, val loss: 0.7497290372848511
Epoch 410, training loss: 0.1301816999912262 = 0.062230855226516724 + 0.01 * 6.795085430145264
Epoch 410, val loss: 0.7625138163566589
Epoch 420, training loss: 0.12370000779628754 = 0.05584608390927315 + 0.01 * 6.785393238067627
Epoch 420, val loss: 0.7756632566452026
Epoch 430, training loss: 0.11836990714073181 = 0.050198744982481 + 0.01 * 6.817116737365723
Epoch 430, val loss: 0.7888625264167786
Epoch 440, training loss: 0.11312343925237656 = 0.04524758458137512 + 0.01 * 6.787585258483887
Epoch 440, val loss: 0.8021828532218933
Epoch 450, training loss: 0.10858076810836792 = 0.04093848168849945 + 0.01 * 6.764228820800781
Epoch 450, val loss: 0.8152645826339722
Epoch 460, training loss: 0.1047777533531189 = 0.03719291836023331 + 0.01 * 6.75848388671875
Epoch 460, val loss: 0.8281193971633911
Epoch 470, training loss: 0.10148678719997406 = 0.0339336097240448 + 0.01 * 6.755317687988281
Epoch 470, val loss: 0.8404668569564819
Epoch 480, training loss: 0.09848668426275253 = 0.031080154702067375 + 0.01 * 6.740652561187744
Epoch 480, val loss: 0.8526324033737183
Epoch 490, training loss: 0.09586909413337708 = 0.02856503054499626 + 0.01 * 6.730407238006592
Epoch 490, val loss: 0.8645893931388855
Epoch 500, training loss: 0.09365607053041458 = 0.026358723640441895 + 0.01 * 6.729734897613525
Epoch 500, val loss: 0.8762465715408325
Epoch 510, training loss: 0.09181022644042969 = 0.02440005913376808 + 0.01 * 6.741016387939453
Epoch 510, val loss: 0.8875106573104858
Epoch 520, training loss: 0.08976311981678009 = 0.02265371009707451 + 0.01 * 6.710940837860107
Epoch 520, val loss: 0.8984636664390564
Epoch 530, training loss: 0.08812733739614487 = 0.021099040284752846 + 0.01 * 6.7028303146362305
Epoch 530, val loss: 0.9092808961868286
Epoch 540, training loss: 0.08696568757295609 = 0.019696185365319252 + 0.01 * 6.726950645446777
Epoch 540, val loss: 0.919743537902832
Epoch 550, training loss: 0.08533912152051926 = 0.018427910283207893 + 0.01 * 6.691121578216553
Epoch 550, val loss: 0.9299975037574768
Epoch 560, training loss: 0.08431549370288849 = 0.017280928790569305 + 0.01 * 6.703456401824951
Epoch 560, val loss: 0.939927875995636
Epoch 570, training loss: 0.08302298188209534 = 0.016234178096055984 + 0.01 * 6.678880214691162
Epoch 570, val loss: 0.9495679140090942
Epoch 580, training loss: 0.08202408254146576 = 0.015272479504346848 + 0.01 * 6.675159931182861
Epoch 580, val loss: 0.9589947462081909
Epoch 590, training loss: 0.08110763132572174 = 0.0143935177475214 + 0.01 * 6.671411991119385
Epoch 590, val loss: 0.9681326150894165
Epoch 600, training loss: 0.08041422069072723 = 0.013584828935563564 + 0.01 * 6.682939052581787
Epoch 600, val loss: 0.977005124092102
Epoch 610, training loss: 0.07948596030473709 = 0.012841840274631977 + 0.01 * 6.664412021636963
Epoch 610, val loss: 0.9857891798019409
Epoch 620, training loss: 0.07869406044483185 = 0.012153228744864464 + 0.01 * 6.654082775115967
Epoch 620, val loss: 0.9942768216133118
Epoch 630, training loss: 0.07804179191589355 = 0.011516612023115158 + 0.01 * 6.652518272399902
Epoch 630, val loss: 1.0025384426116943
Epoch 640, training loss: 0.07742410898208618 = 0.010927748866379261 + 0.01 * 6.6496357917785645
Epoch 640, val loss: 1.0106103420257568
Epoch 650, training loss: 0.07677587866783142 = 0.010382418520748615 + 0.01 * 6.639345645904541
Epoch 650, val loss: 1.018464207649231
Epoch 660, training loss: 0.07632645219564438 = 0.009874196723103523 + 0.01 * 6.645226001739502
Epoch 660, val loss: 1.0260558128356934
Epoch 670, training loss: 0.07569439709186554 = 0.009401378221809864 + 0.01 * 6.629302024841309
Epoch 670, val loss: 1.0335140228271484
Epoch 680, training loss: 0.07521090656518936 = 0.008961389772593975 + 0.01 * 6.6249518394470215
Epoch 680, val loss: 1.0407264232635498
Epoch 690, training loss: 0.07480654865503311 = 0.008548697456717491 + 0.01 * 6.6257853507995605
Epoch 690, val loss: 1.0477705001831055
Epoch 700, training loss: 0.0743495374917984 = 0.008161910809576511 + 0.01 * 6.618762969970703
Epoch 700, val loss: 1.0546681880950928
Epoch 710, training loss: 0.07394684851169586 = 0.007801518775522709 + 0.01 * 6.614533424377441
Epoch 710, val loss: 1.0613740682601929
Epoch 720, training loss: 0.07354746013879776 = 0.007464493624866009 + 0.01 * 6.608296871185303
Epoch 720, val loss: 1.0679612159729004
Epoch 730, training loss: 0.07340291887521744 = 0.007148026954382658 + 0.01 * 6.625489234924316
Epoch 730, val loss: 1.074359655380249
Epoch 740, training loss: 0.07287102192640305 = 0.006852254271507263 + 0.01 * 6.601876735687256
Epoch 740, val loss: 1.0806089639663696
Epoch 750, training loss: 0.07251647114753723 = 0.006575658451765776 + 0.01 * 6.594081401824951
Epoch 750, val loss: 1.0867197513580322
Epoch 760, training loss: 0.07227432727813721 = 0.006316139362752438 + 0.01 * 6.595818519592285
Epoch 760, val loss: 1.0926299095153809
Epoch 770, training loss: 0.07192210108041763 = 0.00607275078073144 + 0.01 * 6.584935188293457
Epoch 770, val loss: 1.0984647274017334
Epoch 780, training loss: 0.07175242155790329 = 0.005843993742018938 + 0.01 * 6.590842247009277
Epoch 780, val loss: 1.1041494607925415
Epoch 790, training loss: 0.0717010423541069 = 0.005627341568470001 + 0.01 * 6.607369899749756
Epoch 790, val loss: 1.1096726655960083
Epoch 800, training loss: 0.07125715911388397 = 0.0054228221997618675 + 0.01 * 6.583433628082275
Epoch 800, val loss: 1.1151353120803833
Epoch 810, training loss: 0.07097126543521881 = 0.005229014903306961 + 0.01 * 6.574224948883057
Epoch 810, val loss: 1.1204097270965576
Epoch 820, training loss: 0.07084884494543076 = 0.0050453683361411095 + 0.01 * 6.580347537994385
Epoch 820, val loss: 1.1256496906280518
Epoch 830, training loss: 0.070668525993824 = 0.004872074816375971 + 0.01 * 6.579645156860352
Epoch 830, val loss: 1.1307599544525146
Epoch 840, training loss: 0.07034805417060852 = 0.004708974622189999 + 0.01 * 6.563908100128174
Epoch 840, val loss: 1.135879397392273
Epoch 850, training loss: 0.07028327137231827 = 0.004557368345558643 + 0.01 * 6.572589874267578
Epoch 850, val loss: 1.1408485174179077
Epoch 860, training loss: 0.07003480195999146 = 0.004417372401803732 + 0.01 * 6.561742782592773
Epoch 860, val loss: 1.1457247734069824
Epoch 870, training loss: 0.06995852291584015 = 0.00428498862311244 + 0.01 * 6.567354202270508
Epoch 870, val loss: 1.1505095958709717
Epoch 880, training loss: 0.06980448961257935 = 0.004161091987043619 + 0.01 * 6.564340114593506
Epoch 880, val loss: 1.1552579402923584
Epoch 890, training loss: 0.06959173083305359 = 0.00404313812032342 + 0.01 * 6.554859161376953
Epoch 890, val loss: 1.159834384918213
Epoch 900, training loss: 0.0693604126572609 = 0.003931872546672821 + 0.01 * 6.542853832244873
Epoch 900, val loss: 1.16428804397583
Epoch 910, training loss: 0.0692765861749649 = 0.0038256379775702953 + 0.01 * 6.545094966888428
Epoch 910, val loss: 1.1687501668930054
Epoch 920, training loss: 0.0690925344824791 = 0.003724500769749284 + 0.01 * 6.536803245544434
Epoch 920, val loss: 1.1729899644851685
Epoch 930, training loss: 0.0691370889544487 = 0.0036281547509133816 + 0.01 * 6.550893306732178
Epoch 930, val loss: 1.1772723197937012
Epoch 940, training loss: 0.06894851475954056 = 0.0035361761692911386 + 0.01 * 6.541233539581299
Epoch 940, val loss: 1.1814656257629395
Epoch 950, training loss: 0.06872489303350449 = 0.003448649076744914 + 0.01 * 6.527624607086182
Epoch 950, val loss: 1.1856199502944946
Epoch 960, training loss: 0.06865337491035461 = 0.0033650370314717293 + 0.01 * 6.528833389282227
Epoch 960, val loss: 1.1896508932113647
Epoch 970, training loss: 0.06843860447406769 = 0.003284768434241414 + 0.01 * 6.515383720397949
Epoch 970, val loss: 1.1936551332473755
Epoch 980, training loss: 0.0683363601565361 = 0.003208185313269496 + 0.01 * 6.512817859649658
Epoch 980, val loss: 1.1976203918457031
Epoch 990, training loss: 0.06831983476877213 = 0.0031348122283816338 + 0.01 * 6.518502712249756
Epoch 990, val loss: 1.2014139890670776
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.6494
Flip ASR: 0.6133/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.03389310836792 = 1.9501546621322632 + 0.01 * 8.373834609985352
Epoch 0, val loss: 1.9448773860931396
Epoch 10, training loss: 2.0221896171569824 = 1.9384535551071167 + 0.01 * 8.373615264892578
Epoch 10, val loss: 1.93234384059906
Epoch 20, training loss: 2.007627010345459 = 1.9238965511322021 + 0.01 * 8.373034477233887
Epoch 20, val loss: 1.9156769514083862
Epoch 30, training loss: 1.9872442483901978 = 1.9035289287567139 + 0.01 * 8.37153148651123
Epoch 30, val loss: 1.891680121421814
Epoch 40, training loss: 1.9593602418899536 = 1.8756848573684692 + 0.01 * 8.367538452148438
Epoch 40, val loss: 1.8603357076644897
Epoch 50, training loss: 1.9235881567001343 = 1.840096116065979 + 0.01 * 8.34920883178711
Epoch 50, val loss: 1.8248357772827148
Epoch 60, training loss: 1.8808602094650269 = 1.7983357906341553 + 0.01 * 8.25244426727295
Epoch 60, val loss: 1.7883145809173584
Epoch 70, training loss: 1.8360968828201294 = 1.7562806606292725 + 0.01 * 7.9816203117370605
Epoch 70, val loss: 1.7542036771774292
Epoch 80, training loss: 1.7837435007095337 = 1.708656907081604 + 0.01 * 7.508655071258545
Epoch 80, val loss: 1.7125589847564697
Epoch 90, training loss: 1.715785264968872 = 1.6432981491088867 + 0.01 * 7.248706340789795
Epoch 90, val loss: 1.6550897359848022
Epoch 100, training loss: 1.627054214477539 = 1.5561563968658447 + 0.01 * 7.089781284332275
Epoch 100, val loss: 1.5826705694198608
Epoch 110, training loss: 1.5255095958709717 = 1.4552192687988281 + 0.01 * 7.029033660888672
Epoch 110, val loss: 1.502428412437439
Epoch 120, training loss: 1.4249156713485718 = 1.3549604415893555 + 0.01 * 6.995527744293213
Epoch 120, val loss: 1.4262526035308838
Epoch 130, training loss: 1.3332186937332153 = 1.263502836227417 + 0.01 * 6.971583366394043
Epoch 130, val loss: 1.3609806299209595
Epoch 140, training loss: 1.2499666213989258 = 1.1804542541503906 + 0.01 * 6.951232433319092
Epoch 140, val loss: 1.3045718669891357
Epoch 150, training loss: 1.1712008714675903 = 1.1019099950790405 + 0.01 * 6.9290852546691895
Epoch 150, val loss: 1.2526694536209106
Epoch 160, training loss: 1.0932743549346924 = 1.0241895914077759 + 0.01 * 6.908470630645752
Epoch 160, val loss: 1.200769066810608
Epoch 170, training loss: 1.014642596244812 = 0.9457067847251892 + 0.01 * 6.893582344055176
Epoch 170, val loss: 1.147264003753662
Epoch 180, training loss: 0.9359846115112305 = 0.8671170473098755 + 0.01 * 6.8867573738098145
Epoch 180, val loss: 1.0917202234268188
Epoch 190, training loss: 0.858444094657898 = 0.7895861864089966 + 0.01 * 6.8857903480529785
Epoch 190, val loss: 1.0356723070144653
Epoch 200, training loss: 0.782875657081604 = 0.7140163779258728 + 0.01 * 6.885929584503174
Epoch 200, val loss: 0.9801636934280396
Epoch 210, training loss: 0.7105473279953003 = 0.6416888236999512 + 0.01 * 6.885853290557861
Epoch 210, val loss: 0.9267025589942932
Epoch 220, training loss: 0.6436227560043335 = 0.5747641921043396 + 0.01 * 6.885854244232178
Epoch 220, val loss: 0.8776068687438965
Epoch 230, training loss: 0.5840659141540527 = 0.5152173042297363 + 0.01 * 6.88485860824585
Epoch 230, val loss: 0.8355280756950378
Epoch 240, training loss: 0.5324628353118896 = 0.46363040804862976 + 0.01 * 6.883240699768066
Epoch 240, val loss: 0.8016939163208008
Epoch 250, training loss: 0.4877139627933502 = 0.4189054071903229 + 0.01 * 6.880855560302734
Epoch 250, val loss: 0.7754297852516174
Epoch 260, training loss: 0.4478752017021179 = 0.37908726930618286 + 0.01 * 6.878792762756348
Epoch 260, val loss: 0.7548741698265076
Epoch 270, training loss: 0.4110282063484192 = 0.3422854244709015 + 0.01 * 6.874279022216797
Epoch 270, val loss: 0.7379129528999329
Epoch 280, training loss: 0.37602245807647705 = 0.30730852484703064 + 0.01 * 6.871394634246826
Epoch 280, val loss: 0.7240196466445923
Epoch 290, training loss: 0.3426660895347595 = 0.2739979326725006 + 0.01 * 6.866815090179443
Epoch 290, val loss: 0.7130919098854065
Epoch 300, training loss: 0.3113177716732025 = 0.24270068109035492 + 0.01 * 6.861709117889404
Epoch 300, val loss: 0.7049901485443115
Epoch 310, training loss: 0.2824900448322296 = 0.21382328867912292 + 0.01 * 6.866676330566406
Epoch 310, val loss: 0.6996511220932007
Epoch 320, training loss: 0.2561907172203064 = 0.18767423927783966 + 0.01 * 6.851648807525635
Epoch 320, val loss: 0.6968496441841125
Epoch 330, training loss: 0.2328590750694275 = 0.16439732909202576 + 0.01 * 6.846174240112305
Epoch 330, val loss: 0.6966688632965088
Epoch 340, training loss: 0.21230463683605194 = 0.1439046710729599 + 0.01 * 6.839996814727783
Epoch 340, val loss: 0.6988365054130554
Epoch 350, training loss: 0.19438381493091583 = 0.12598390877246857 + 0.01 * 6.839990615844727
Epoch 350, val loss: 0.702867329120636
Epoch 360, training loss: 0.17865222692489624 = 0.11039016395807266 + 0.01 * 6.826205730438232
Epoch 360, val loss: 0.7086715698242188
Epoch 370, training loss: 0.1650676429271698 = 0.09686319530010223 + 0.01 * 6.820445537567139
Epoch 370, val loss: 0.7157501578330994
Epoch 380, training loss: 0.15323348343372345 = 0.08516974002122879 + 0.01 * 6.806374549865723
Epoch 380, val loss: 0.7238195538520813
Epoch 390, training loss: 0.14304420351982117 = 0.07506998628377914 + 0.01 * 6.797421455383301
Epoch 390, val loss: 0.7327317595481873
Epoch 400, training loss: 0.13440006971359253 = 0.06636460870504379 + 0.01 * 6.803546905517578
Epoch 400, val loss: 0.7421477437019348
Epoch 410, training loss: 0.12673014402389526 = 0.05887896567583084 + 0.01 * 6.78511905670166
Epoch 410, val loss: 0.75199294090271
Epoch 420, training loss: 0.1201232373714447 = 0.05243043228983879 + 0.01 * 6.769281387329102
Epoch 420, val loss: 0.7619379758834839
Epoch 430, training loss: 0.11489605903625488 = 0.04686420038342476 + 0.01 * 6.80318546295166
Epoch 430, val loss: 0.7719797492027283
Epoch 440, training loss: 0.10972313582897186 = 0.04206166788935661 + 0.01 * 6.766146659851074
Epoch 440, val loss: 0.7819924354553223
Epoch 450, training loss: 0.10537292808294296 = 0.03790614753961563 + 0.01 * 6.746678352355957
Epoch 450, val loss: 0.7920682430267334
Epoch 460, training loss: 0.10175952315330505 = 0.034299056977033615 + 0.01 * 6.746046543121338
Epoch 460, val loss: 0.8019877076148987
Epoch 470, training loss: 0.098492331802845 = 0.0311591774225235 + 0.01 * 6.733315467834473
Epoch 470, val loss: 0.8117026686668396
Epoch 480, training loss: 0.09576332569122314 = 0.02841784618794918 + 0.01 * 6.7345476150512695
Epoch 480, val loss: 0.8213122487068176
Epoch 490, training loss: 0.09310581535100937 = 0.026013344526290894 + 0.01 * 6.70924711227417
Epoch 490, val loss: 0.8307676911354065
Epoch 500, training loss: 0.09110690653324127 = 0.023898940533399582 + 0.01 * 6.720797061920166
Epoch 500, val loss: 0.8399453163146973
Epoch 510, training loss: 0.08900048583745956 = 0.02203441970050335 + 0.01 * 6.696606636047363
Epoch 510, val loss: 0.8489232063293457
Epoch 520, training loss: 0.0872165784239769 = 0.02038104087114334 + 0.01 * 6.683553695678711
Epoch 520, val loss: 0.8577398657798767
Epoch 530, training loss: 0.08581538498401642 = 0.018909534439444542 + 0.01 * 6.690584659576416
Epoch 530, val loss: 0.866364061832428
Epoch 540, training loss: 0.08442992717027664 = 0.017594294622540474 + 0.01 * 6.683563709259033
Epoch 540, val loss: 0.874724805355072
Epoch 550, training loss: 0.08310380578041077 = 0.0164165236055851 + 0.01 * 6.668728351593018
Epoch 550, val loss: 0.8829126954078674
Epoch 560, training loss: 0.08189832419157028 = 0.015356674790382385 + 0.01 * 6.654165267944336
Epoch 560, val loss: 0.8909286260604858
Epoch 570, training loss: 0.08111820369958878 = 0.014399921521544456 + 0.01 * 6.671828269958496
Epoch 570, val loss: 0.8986834287643433
Epoch 580, training loss: 0.0799538642168045 = 0.013534702360630035 + 0.01 * 6.641916275024414
Epoch 580, val loss: 0.9062530994415283
Epoch 590, training loss: 0.07925406098365784 = 0.012748518027365208 + 0.01 * 6.650554656982422
Epoch 590, val loss: 0.9136379361152649
Epoch 600, training loss: 0.07841639220714569 = 0.01203242689371109 + 0.01 * 6.638396739959717
Epoch 600, val loss: 0.9208695292472839
Epoch 610, training loss: 0.07769743353128433 = 0.011378394439816475 + 0.01 * 6.631904602050781
Epoch 610, val loss: 0.9278733134269714
Epoch 620, training loss: 0.07699308544397354 = 0.01077946089208126 + 0.01 * 6.621362209320068
Epoch 620, val loss: 0.9347136616706848
Epoch 630, training loss: 0.07645094394683838 = 0.01022981759160757 + 0.01 * 6.62211275100708
Epoch 630, val loss: 0.9414219856262207
Epoch 640, training loss: 0.07581795752048492 = 0.00972458440810442 + 0.01 * 6.609337329864502
Epoch 640, val loss: 0.9478656649589539
Epoch 650, training loss: 0.07539930939674377 = 0.009258683770895004 + 0.01 * 6.614062309265137
Epoch 650, val loss: 0.9542253017425537
Epoch 660, training loss: 0.07516710460186005 = 0.008828243240714073 + 0.01 * 6.633885860443115
Epoch 660, val loss: 0.9604398608207703
Epoch 670, training loss: 0.07439350336790085 = 0.008429651148617268 + 0.01 * 6.5963850021362305
Epoch 670, val loss: 0.9664175510406494
Epoch 680, training loss: 0.07416191697120667 = 0.008060242980718613 + 0.01 * 6.610167026519775
Epoch 680, val loss: 0.9723302721977234
Epoch 690, training loss: 0.07365750521421432 = 0.007716864347457886 + 0.01 * 6.594064235687256
Epoch 690, val loss: 0.9780186414718628
Epoch 700, training loss: 0.07325723767280579 = 0.007397375535219908 + 0.01 * 6.585986137390137
Epoch 700, val loss: 0.983665406703949
Epoch 710, training loss: 0.07296706736087799 = 0.007099071051925421 + 0.01 * 6.5868000984191895
Epoch 710, val loss: 0.989140510559082
Epoch 720, training loss: 0.07261679321527481 = 0.006820397451519966 + 0.01 * 6.579639911651611
Epoch 720, val loss: 0.9944825172424316
Epoch 730, training loss: 0.07220158725976944 = 0.006559493951499462 + 0.01 * 6.564209938049316
Epoch 730, val loss: 0.9997057914733887
Epoch 740, training loss: 0.07208818197250366 = 0.006315091624855995 + 0.01 * 6.5773091316223145
Epoch 740, val loss: 1.00483238697052
Epoch 750, training loss: 0.0716366171836853 = 0.006085851229727268 + 0.01 * 6.555076599121094
Epoch 750, val loss: 1.0097731351852417
Epoch 760, training loss: 0.07144079357385635 = 0.005870291031897068 + 0.01 * 6.557050704956055
Epoch 760, val loss: 1.0146467685699463
Epoch 770, training loss: 0.07111179083585739 = 0.005667654797434807 + 0.01 * 6.5444135665893555
Epoch 770, val loss: 1.0194005966186523
Epoch 780, training loss: 0.07091692090034485 = 0.005476577207446098 + 0.01 * 6.544034004211426
Epoch 780, val loss: 1.0240694284439087
Epoch 790, training loss: 0.070701465010643 = 0.005295998882502317 + 0.01 * 6.540546894073486
Epoch 790, val loss: 1.0285871028900146
Epoch 800, training loss: 0.07060924172401428 = 0.005125341471284628 + 0.01 * 6.548389911651611
Epoch 800, val loss: 1.0329961776733398
Epoch 810, training loss: 0.07039747387170792 = 0.00496467063203454 + 0.01 * 6.543280601501465
Epoch 810, val loss: 1.037312388420105
Epoch 820, training loss: 0.07021192461252213 = 0.00481238029897213 + 0.01 * 6.539954662322998
Epoch 820, val loss: 1.041587471961975
Epoch 830, training loss: 0.06996399909257889 = 0.004668171051889658 + 0.01 * 6.52958345413208
Epoch 830, val loss: 1.045730710029602
Epoch 840, training loss: 0.06979618221521378 = 0.004531401209533215 + 0.01 * 6.526478290557861
Epoch 840, val loss: 1.0498404502868652
Epoch 850, training loss: 0.06998826563358307 = 0.004401362035423517 + 0.01 * 6.558690547943115
Epoch 850, val loss: 1.0538297891616821
Epoch 860, training loss: 0.06946158409118652 = 0.00427804421633482 + 0.01 * 6.518354415893555
Epoch 860, val loss: 1.057723045349121
Epoch 870, training loss: 0.06930966675281525 = 0.004160674288868904 + 0.01 * 6.514899253845215
Epoch 870, val loss: 1.0615572929382324
Epoch 880, training loss: 0.06911808252334595 = 0.004048961214721203 + 0.01 * 6.5069122314453125
Epoch 880, val loss: 1.0653170347213745
Epoch 890, training loss: 0.0691504254937172 = 0.003942535258829594 + 0.01 * 6.520789623260498
Epoch 890, val loss: 1.0690209865570068
Epoch 900, training loss: 0.06893109530210495 = 0.0038409996777772903 + 0.01 * 6.509009838104248
Epoch 900, val loss: 1.0726386308670044
Epoch 910, training loss: 0.06873384863138199 = 0.003744120942428708 + 0.01 * 6.498973369598389
Epoch 910, val loss: 1.0761778354644775
Epoch 920, training loss: 0.06868748366832733 = 0.0036515281535685062 + 0.01 * 6.50359582901001
Epoch 920, val loss: 1.079609751701355
Epoch 930, training loss: 0.0685839056968689 = 0.003563110949471593 + 0.01 * 6.502079486846924
Epoch 930, val loss: 1.083090901374817
Epoch 940, training loss: 0.06836210191249847 = 0.0034784749150276184 + 0.01 * 6.488362789154053
Epoch 940, val loss: 1.0863760709762573
Epoch 950, training loss: 0.06827887147665024 = 0.003397531807422638 + 0.01 * 6.488134384155273
Epoch 950, val loss: 1.089682698249817
Epoch 960, training loss: 0.06811107695102692 = 0.0033199996687471867 + 0.01 * 6.47910737991333
Epoch 960, val loss: 1.092886209487915
Epoch 970, training loss: 0.06812237948179245 = 0.0032456612680107355 + 0.01 * 6.487671852111816
Epoch 970, val loss: 1.0960144996643066
Epoch 980, training loss: 0.06794138252735138 = 0.0031744297593832016 + 0.01 * 6.4766950607299805
Epoch 980, val loss: 1.0991182327270508
Epoch 990, training loss: 0.06794683635234833 = 0.0031060113105922937 + 0.01 * 6.4840826988220215
Epoch 990, val loss: 1.102168083190918
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.5129
Flip ASR: 0.4711/225 nodes
The final ASR:0.56089, 0.06269, Accuracy:0.82099, 0.00349
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11584])
remove edge: torch.Size([2, 9544])
updated graph: torch.Size([2, 10572])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.97909, 0.00174, Accuracy:0.83704, 0.00524
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0209529399871826 = 1.9372142553329468 + 0.01 * 8.373871803283691
Epoch 0, val loss: 1.9344295263290405
Epoch 10, training loss: 2.0113134384155273 = 1.9275754690170288 + 0.01 * 8.373807907104492
Epoch 10, val loss: 1.9249941110610962
Epoch 20, training loss: 1.9990501403808594 = 1.9153146743774414 + 0.01 * 8.373547554016113
Epoch 20, val loss: 1.912893295288086
Epoch 30, training loss: 1.9811978340148926 = 1.8974705934524536 + 0.01 * 8.372726440429688
Epoch 30, val loss: 1.8954339027404785
Epoch 40, training loss: 1.9543397426605225 = 1.8706576824188232 + 0.01 * 8.368204116821289
Epoch 40, val loss: 1.869982361793518
Epoch 50, training loss: 1.9168912172317505 = 1.8335248231887817 + 0.01 * 8.336639404296875
Epoch 50, val loss: 1.8371244668960571
Epoch 60, training loss: 1.8746213912963867 = 1.792846441268921 + 0.01 * 8.177492141723633
Epoch 60, val loss: 1.8055962324142456
Epoch 70, training loss: 1.8334769010543823 = 1.7542392015457153 + 0.01 * 7.923771858215332
Epoch 70, val loss: 1.776115894317627
Epoch 80, training loss: 1.779772400856018 = 1.702275037765503 + 0.01 * 7.74973201751709
Epoch 80, val loss: 1.7310203313827515
Epoch 90, training loss: 1.7061371803283691 = 1.630850911140442 + 0.01 * 7.528626918792725
Epoch 90, val loss: 1.670422911643982
Epoch 100, training loss: 1.612044334411621 = 1.538509488105774 + 0.01 * 7.353482246398926
Epoch 100, val loss: 1.596233606338501
Epoch 110, training loss: 1.506510615348816 = 1.4341938495635986 + 0.01 * 7.2316789627075195
Epoch 110, val loss: 1.5124655961990356
Epoch 120, training loss: 1.4006105661392212 = 1.3292462825775146 + 0.01 * 7.136425018310547
Epoch 120, val loss: 1.4294812679290771
Epoch 130, training loss: 1.3002797365188599 = 1.2298219203948975 + 0.01 * 7.0457868576049805
Epoch 130, val loss: 1.350121021270752
Epoch 140, training loss: 1.2089812755584717 = 1.138961911201477 + 0.01 * 7.00193452835083
Epoch 140, val loss: 1.278452754020691
Epoch 150, training loss: 1.128486156463623 = 1.0586715936660767 + 0.01 * 6.981462001800537
Epoch 150, val loss: 1.2156474590301514
Epoch 160, training loss: 1.0587053298950195 = 0.989097535610199 + 0.01 * 6.960783958435059
Epoch 160, val loss: 1.1633729934692383
Epoch 170, training loss: 0.9972600340843201 = 0.9279088377952576 + 0.01 * 6.935118675231934
Epoch 170, val loss: 1.1198196411132812
Epoch 180, training loss: 0.9394050240516663 = 0.8703254461288452 + 0.01 * 6.907956123352051
Epoch 180, val loss: 1.0806143283843994
Epoch 190, training loss: 0.8801205158233643 = 0.8112199306488037 + 0.01 * 6.8900580406188965
Epoch 190, val loss: 1.0405570268630981
Epoch 200, training loss: 0.8164095878601074 = 0.7476470470428467 + 0.01 * 6.876255512237549
Epoch 200, val loss: 0.9966811537742615
Epoch 210, training loss: 0.7491328120231628 = 0.6804615259170532 + 0.01 * 6.867126941680908
Epoch 210, val loss: 0.9500054121017456
Epoch 220, training loss: 0.6830667853355408 = 0.6144694089889526 + 0.01 * 6.859736919403076
Epoch 220, val loss: 0.9050558805465698
Epoch 230, training loss: 0.6236890554428101 = 0.5551397800445557 + 0.01 * 6.854926109313965
Epoch 230, val loss: 0.8671069145202637
Epoch 240, training loss: 0.5738343596458435 = 0.5053901672363281 + 0.01 * 6.844420433044434
Epoch 240, val loss: 0.8386608958244324
Epoch 250, training loss: 0.5330984592437744 = 0.4647493362426758 + 0.01 * 6.8349151611328125
Epoch 250, val loss: 0.8192539811134338
Epoch 260, training loss: 0.4988431930541992 = 0.43060430884361267 + 0.01 * 6.823887825012207
Epoch 260, val loss: 0.8060860633850098
Epoch 270, training loss: 0.46815937757492065 = 0.40001311898231506 + 0.01 * 6.814625263214111
Epoch 270, val loss: 0.7961910367012024
Epoch 280, training loss: 0.43871456384658813 = 0.37064307928085327 + 0.01 * 6.807150363922119
Epoch 280, val loss: 0.7877855896949768
Epoch 290, training loss: 0.40911996364593506 = 0.3411356508731842 + 0.01 * 6.798430442810059
Epoch 290, val loss: 0.7802390456199646
Epoch 300, training loss: 0.3793143928050995 = 0.3113879859447479 + 0.01 * 6.792640209197998
Epoch 300, val loss: 0.7735329270362854
Epoch 310, training loss: 0.3501896858215332 = 0.28229328989982605 + 0.01 * 6.789638042449951
Epoch 310, val loss: 0.7683297395706177
Epoch 320, training loss: 0.32270747423171997 = 0.25484293699264526 + 0.01 * 6.786452293395996
Epoch 320, val loss: 0.7653241157531738
Epoch 330, training loss: 0.29710879921913147 = 0.22928468883037567 + 0.01 * 6.782411575317383
Epoch 330, val loss: 0.76454758644104
Epoch 340, training loss: 0.27289819717407227 = 0.205104798078537 + 0.01 * 6.779338359832764
Epoch 340, val loss: 0.7656638622283936
Epoch 350, training loss: 0.2494874894618988 = 0.18173620104789734 + 0.01 * 6.7751288414001465
Epoch 350, val loss: 0.7686260342597961
Epoch 360, training loss: 0.22691816091537476 = 0.159187451004982 + 0.01 * 6.7730712890625
Epoch 360, val loss: 0.7737307548522949
Epoch 370, training loss: 0.20578481256961823 = 0.13808631896972656 + 0.01 * 6.7698493003845215
Epoch 370, val loss: 0.7812213897705078
Epoch 380, training loss: 0.1868380904197693 = 0.11915811896324158 + 0.01 * 6.7679972648620605
Epoch 380, val loss: 0.790894091129303
Epoch 390, training loss: 0.17037805914878845 = 0.10273603349924088 + 0.01 * 6.764202117919922
Epoch 390, val loss: 0.8023366928100586
Epoch 400, training loss: 0.15634945034980774 = 0.08874349296092987 + 0.01 * 6.760595798492432
Epoch 400, val loss: 0.8149566054344177
Epoch 410, training loss: 0.14449341595172882 = 0.07691916823387146 + 0.01 * 6.757424831390381
Epoch 410, val loss: 0.8284231424331665
Epoch 420, training loss: 0.13454094529151917 = 0.06696260720491409 + 0.01 * 6.7578349113464355
Epoch 420, val loss: 0.8423224687576294
Epoch 430, training loss: 0.1260879784822464 = 0.058594170957803726 + 0.01 * 6.749381065368652
Epoch 430, val loss: 0.8563544154167175
Epoch 440, training loss: 0.11900578439235687 = 0.05155289173126221 + 0.01 * 6.745289325714111
Epoch 440, val loss: 0.8703261017799377
Epoch 450, training loss: 0.11319850385189056 = 0.04561372846364975 + 0.01 * 6.758477687835693
Epoch 450, val loss: 0.8841824531555176
Epoch 460, training loss: 0.10801669955253601 = 0.040591154247522354 + 0.01 * 6.742555141448975
Epoch 460, val loss: 0.8977774977684021
Epoch 470, training loss: 0.10365387797355652 = 0.036321356892585754 + 0.01 * 6.73325252532959
Epoch 470, val loss: 0.9111738204956055
Epoch 480, training loss: 0.09993375837802887 = 0.03267046436667442 + 0.01 * 6.726330280303955
Epoch 480, val loss: 0.9243636727333069
Epoch 490, training loss: 0.09681209176778793 = 0.02953103929758072 + 0.01 * 6.728105545043945
Epoch 490, val loss: 0.93736732006073
Epoch 500, training loss: 0.0941014438867569 = 0.02682049386203289 + 0.01 * 6.728095054626465
Epoch 500, val loss: 0.9500598907470703
Epoch 510, training loss: 0.09157554805278778 = 0.024466892704367638 + 0.01 * 6.710865497589111
Epoch 510, val loss: 0.96241694688797
Epoch 520, training loss: 0.08947828412055969 = 0.022410403937101364 + 0.01 * 6.706788539886475
Epoch 520, val loss: 0.9744831323623657
Epoch 530, training loss: 0.08771441131830215 = 0.020606061443686485 + 0.01 * 6.710834980010986
Epoch 530, val loss: 0.9862426519393921
Epoch 540, training loss: 0.08596587181091309 = 0.019015774130821228 + 0.01 * 6.695009708404541
Epoch 540, val loss: 0.9976598620414734
Epoch 550, training loss: 0.08459405601024628 = 0.017606545239686966 + 0.01 * 6.698750972747803
Epoch 550, val loss: 1.0087921619415283
Epoch 560, training loss: 0.08323611319065094 = 0.016352802515029907 + 0.01 * 6.688331127166748
Epoch 560, val loss: 1.0195791721343994
Epoch 570, training loss: 0.08200258761644363 = 0.015232359059154987 + 0.01 * 6.677022457122803
Epoch 570, val loss: 1.030111312866211
Epoch 580, training loss: 0.08107686787843704 = 0.014227616600692272 + 0.01 * 6.684925556182861
Epoch 580, val loss: 1.0402690172195435
Epoch 590, training loss: 0.07999233901500702 = 0.013323955237865448 + 0.01 * 6.6668381690979
Epoch 590, val loss: 1.050134539604187
Epoch 600, training loss: 0.07915688306093216 = 0.012507551349699497 + 0.01 * 6.664933681488037
Epoch 600, val loss: 1.0597349405288696
Epoch 610, training loss: 0.07834366708993912 = 0.011767766438424587 + 0.01 * 6.657590389251709
Epoch 610, val loss: 1.0689903497695923
Epoch 620, training loss: 0.07762876898050308 = 0.011095769703388214 + 0.01 * 6.653299808502197
Epoch 620, val loss: 1.0779629945755005
Epoch 630, training loss: 0.07704394310712814 = 0.010482546873390675 + 0.01 * 6.656139373779297
Epoch 630, val loss: 1.0867117643356323
Epoch 640, training loss: 0.0764540359377861 = 0.009922483004629612 + 0.01 * 6.65315580368042
Epoch 640, val loss: 1.0952249765396118
Epoch 650, training loss: 0.07576372474431992 = 0.009409659542143345 + 0.01 * 6.635406970977783
Epoch 650, val loss: 1.1034072637557983
Epoch 660, training loss: 0.07522778958082199 = 0.008938177488744259 + 0.01 * 6.628961086273193
Epoch 660, val loss: 1.1114126443862915
Epoch 670, training loss: 0.07480794936418533 = 0.008503946475684643 + 0.01 * 6.630400657653809
Epoch 670, val loss: 1.1191949844360352
Epoch 680, training loss: 0.07436501979827881 = 0.00810343399643898 + 0.01 * 6.626158714294434
Epoch 680, val loss: 1.1267247200012207
Epoch 690, training loss: 0.07395472377538681 = 0.007732979021966457 + 0.01 * 6.622174263000488
Epoch 690, val loss: 1.134081244468689
Epoch 700, training loss: 0.07351390272378922 = 0.007390032988041639 + 0.01 * 6.612387657165527
Epoch 700, val loss: 1.141169786453247
Epoch 710, training loss: 0.0731232762336731 = 0.007071362342685461 + 0.01 * 6.605191707611084
Epoch 710, val loss: 1.1481378078460693
Epoch 720, training loss: 0.07289104908704758 = 0.006774709559977055 + 0.01 * 6.611634254455566
Epoch 720, val loss: 1.1549113988876343
Epoch 730, training loss: 0.07273248583078384 = 0.00649832421913743 + 0.01 * 6.623416900634766
Epoch 730, val loss: 1.1614494323730469
Epoch 740, training loss: 0.07225389778614044 = 0.0062405988574028015 + 0.01 * 6.601329803466797
Epoch 740, val loss: 1.1679081916809082
Epoch 750, training loss: 0.0718952864408493 = 0.005999685265123844 + 0.01 * 6.589560508728027
Epoch 750, val loss: 1.1741626262664795
Epoch 760, training loss: 0.07179298996925354 = 0.0057739331386983395 + 0.01 * 6.6019062995910645
Epoch 760, val loss: 1.1802440881729126
Epoch 770, training loss: 0.07178337126970291 = 0.0055623226799070835 + 0.01 * 6.622105598449707
Epoch 770, val loss: 1.1861475706100464
Epoch 780, training loss: 0.07121369987726212 = 0.0053636753000319 + 0.01 * 6.585002899169922
Epoch 780, val loss: 1.191939353942871
Epoch 790, training loss: 0.07095362991094589 = 0.005176975391805172 + 0.01 * 6.577665328979492
Epoch 790, val loss: 1.19758939743042
Epoch 800, training loss: 0.07095693051815033 = 0.00500096008181572 + 0.01 * 6.595597743988037
Epoch 800, val loss: 1.2030749320983887
Epoch 810, training loss: 0.07059670239686966 = 0.004835379309952259 + 0.01 * 6.576131820678711
Epoch 810, val loss: 1.2084295749664307
Epoch 820, training loss: 0.07036613672971725 = 0.004678867757320404 + 0.01 * 6.568727016448975
Epoch 820, val loss: 1.2136591672897339
Epoch 830, training loss: 0.07018040865659714 = 0.004531012382358313 + 0.01 * 6.564939975738525
Epoch 830, val loss: 1.2187975645065308
Epoch 840, training loss: 0.07000909745693207 = 0.004390912130475044 + 0.01 * 6.561818599700928
Epoch 840, val loss: 1.2237474918365479
Epoch 850, training loss: 0.06978845596313477 = 0.004258355591446161 + 0.01 * 6.553009986877441
Epoch 850, val loss: 1.2286713123321533
Epoch 860, training loss: 0.0697895884513855 = 0.004132552538067102 + 0.01 * 6.565703868865967
Epoch 860, val loss: 1.2333886623382568
Epoch 870, training loss: 0.06960301846265793 = 0.004013465717434883 + 0.01 * 6.558955669403076
Epoch 870, val loss: 1.2381607294082642
Epoch 880, training loss: 0.06949001550674438 = 0.0039003549609333277 + 0.01 * 6.558966159820557
Epoch 880, val loss: 1.2426691055297852
Epoch 890, training loss: 0.0693170428276062 = 0.003792818635702133 + 0.01 * 6.552422523498535
Epoch 890, val loss: 1.2471425533294678
Epoch 900, training loss: 0.06914469599723816 = 0.0036903610453009605 + 0.01 * 6.545433521270752
Epoch 900, val loss: 1.2515400648117065
Epoch 910, training loss: 0.06896703690290451 = 0.0035927612334489822 + 0.01 * 6.53742790222168
Epoch 910, val loss: 1.2557622194290161
Epoch 920, training loss: 0.06882156431674957 = 0.0034998003393411636 + 0.01 * 6.532176494598389
Epoch 920, val loss: 1.2599669694900513
Epoch 930, training loss: 0.06886011362075806 = 0.003411033423617482 + 0.01 * 6.544908046722412
Epoch 930, val loss: 1.2640407085418701
Epoch 940, training loss: 0.06866244971752167 = 0.003326314501464367 + 0.01 * 6.533614158630371
Epoch 940, val loss: 1.2680391073226929
Epoch 950, training loss: 0.06854914128780365 = 0.003245317842811346 + 0.01 * 6.5303826332092285
Epoch 950, val loss: 1.2719991207122803
Epoch 960, training loss: 0.06845713406801224 = 0.003167966613546014 + 0.01 * 6.52891731262207
Epoch 960, val loss: 1.2757470607757568
Epoch 970, training loss: 0.06842183321714401 = 0.003093993989750743 + 0.01 * 6.5327839851379395
Epoch 970, val loss: 1.2796417474746704
Epoch 980, training loss: 0.06846875697374344 = 0.003023166675120592 + 0.01 * 6.544558525085449
Epoch 980, val loss: 1.2832249402999878
Epoch 990, training loss: 0.0681910514831543 = 0.0029552096966654062 + 0.01 * 6.523583889007568
Epoch 990, val loss: 1.2868337631225586
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.4797
Flip ASR: 0.3867/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0151143074035645 = 1.9313757419586182 + 0.01 * 8.373868942260742
Epoch 0, val loss: 1.9285861253738403
Epoch 10, training loss: 2.005760908126831 = 1.922023057937622 + 0.01 * 8.373790740966797
Epoch 10, val loss: 1.9185329675674438
Epoch 20, training loss: 1.994133472442627 = 1.9103981256484985 + 0.01 * 8.373539924621582
Epoch 20, val loss: 1.9061148166656494
Epoch 30, training loss: 1.977689266204834 = 1.893962025642395 + 0.01 * 8.372719764709473
Epoch 30, val loss: 1.8889095783233643
Epoch 40, training loss: 1.9534355401992798 = 1.8697569370269775 + 0.01 * 8.36785888671875
Epoch 40, val loss: 1.8640727996826172
Epoch 50, training loss: 1.9192063808441162 = 1.835878610610962 + 0.01 * 8.332780838012695
Epoch 50, val loss: 1.8309898376464844
Epoch 60, training loss: 1.8775430917739868 = 1.795952558517456 + 0.01 * 8.159052848815918
Epoch 60, val loss: 1.7957483530044556
Epoch 70, training loss: 1.8340851068496704 = 1.7553260326385498 + 0.01 * 7.875906467437744
Epoch 70, val loss: 1.762803554534912
Epoch 80, training loss: 1.7798147201538086 = 1.7031759023666382 + 0.01 * 7.663876533508301
Epoch 80, val loss: 1.7183526754379272
Epoch 90, training loss: 1.7052085399627686 = 1.6311309337615967 + 0.01 * 7.407758712768555
Epoch 90, val loss: 1.6569856405258179
Epoch 100, training loss: 1.6109439134597778 = 1.5388671159744263 + 0.01 * 7.207674503326416
Epoch 100, val loss: 1.5810508728027344
Epoch 110, training loss: 1.5055994987487793 = 1.4346193075180054 + 0.01 * 7.098016738891602
Epoch 110, val loss: 1.4964559078216553
Epoch 120, training loss: 1.3973227739334106 = 1.3267745971679688 + 0.01 * 7.054815292358398
Epoch 120, val loss: 1.4108608961105347
Epoch 130, training loss: 1.2882217168807983 = 1.2179450988769531 + 0.01 * 7.027663230895996
Epoch 130, val loss: 1.3252403736114502
Epoch 140, training loss: 1.1788967847824097 = 1.108756184577942 + 0.01 * 7.014059066772461
Epoch 140, val loss: 1.2403113842010498
Epoch 150, training loss: 1.0719454288482666 = 1.0019673109054565 + 0.01 * 6.997806072235107
Epoch 150, val loss: 1.158740758895874
Epoch 160, training loss: 0.9726793766021729 = 0.9029114842414856 + 0.01 * 6.976792335510254
Epoch 160, val loss: 1.084749698638916
Epoch 170, training loss: 0.8865718841552734 = 0.8170933723449707 + 0.01 * 6.947853088378906
Epoch 170, val loss: 1.0223017930984497
Epoch 180, training loss: 0.8159494400024414 = 0.7468052506446838 + 0.01 * 6.914418697357178
Epoch 180, val loss: 0.9731544852256775
Epoch 190, training loss: 0.7592589855194092 = 0.6902981996536255 + 0.01 * 6.896076679229736
Epoch 190, val loss: 0.9360153079032898
Epoch 200, training loss: 0.7123098969459534 = 0.6435905694961548 + 0.01 * 6.8719353675842285
Epoch 200, val loss: 0.9077600836753845
Epoch 210, training loss: 0.671104371547699 = 0.6024706959724426 + 0.01 * 6.863365650177002
Epoch 210, val loss: 0.8850434422492981
Epoch 220, training loss: 0.6322329640388489 = 0.5637001395225525 + 0.01 * 6.8532819747924805
Epoch 220, val loss: 0.8654101490974426
Epoch 230, training loss: 0.5936153531074524 = 0.5251694917678833 + 0.01 * 6.844587326049805
Epoch 230, val loss: 0.8473311066627502
Epoch 240, training loss: 0.5542831420898438 = 0.4859299957752228 + 0.01 * 6.835316181182861
Epoch 240, val loss: 0.8304684162139893
Epoch 250, training loss: 0.5142548084259033 = 0.44599685072898865 + 0.01 * 6.825798511505127
Epoch 250, val loss: 0.814991295337677
Epoch 260, training loss: 0.4742729067802429 = 0.4061160683631897 + 0.01 * 6.815682888031006
Epoch 260, val loss: 0.8010264039039612
Epoch 270, training loss: 0.4350167214870453 = 0.36690300703048706 + 0.01 * 6.81137228012085
Epoch 270, val loss: 0.7882176041603088
Epoch 280, training loss: 0.39652132987976074 = 0.32853490114212036 + 0.01 * 6.798644542694092
Epoch 280, val loss: 0.7758609056472778
Epoch 290, training loss: 0.35930919647216797 = 0.29138514399528503 + 0.01 * 6.7924065589904785
Epoch 290, val loss: 0.7645137906074524
Epoch 300, training loss: 0.3236982226371765 = 0.2558267116546631 + 0.01 * 6.7871503829956055
Epoch 300, val loss: 0.7545056939125061
Epoch 310, training loss: 0.29039761424064636 = 0.22246849536895752 + 0.01 * 6.792912006378174
Epoch 310, val loss: 0.74678635597229
Epoch 320, training loss: 0.2596416771411896 = 0.19182494282722473 + 0.01 * 6.781672477722168
Epoch 320, val loss: 0.7415385842323303
Epoch 330, training loss: 0.2323491871356964 = 0.16454973816871643 + 0.01 * 6.779943943023682
Epoch 330, val loss: 0.7391627430915833
Epoch 340, training loss: 0.2088409960269928 = 0.14107000827789307 + 0.01 * 6.777098655700684
Epoch 340, val loss: 0.7397734522819519
Epoch 350, training loss: 0.18923074007034302 = 0.12149021029472351 + 0.01 * 6.774054050445557
Epoch 350, val loss: 0.7438262701034546
Epoch 360, training loss: 0.17300154268741608 = 0.10529788583517075 + 0.01 * 6.770366191864014
Epoch 360, val loss: 0.750861406326294
Epoch 370, training loss: 0.159742534160614 = 0.0919097363948822 + 0.01 * 6.7832794189453125
Epoch 370, val loss: 0.760429322719574
Epoch 380, training loss: 0.14851681888103485 = 0.08080582320690155 + 0.01 * 6.77109956741333
Epoch 380, val loss: 0.7719212770462036
Epoch 390, training loss: 0.13913890719413757 = 0.07150651514530182 + 0.01 * 6.763239860534668
Epoch 390, val loss: 0.7848002910614014
Epoch 400, training loss: 0.13121342658996582 = 0.06363274157047272 + 0.01 * 6.758069038391113
Epoch 400, val loss: 0.7985544204711914
Epoch 410, training loss: 0.12444816529750824 = 0.05691276490688324 + 0.01 * 6.7535400390625
Epoch 410, val loss: 0.812965989112854
Epoch 420, training loss: 0.11863049864768982 = 0.05113497003912926 + 0.01 * 6.749553203582764
Epoch 420, val loss: 0.8278108835220337
Epoch 430, training loss: 0.11359430104494095 = 0.0461391806602478 + 0.01 * 6.745512008666992
Epoch 430, val loss: 0.8428753614425659
Epoch 440, training loss: 0.10934123396873474 = 0.04180135205388069 + 0.01 * 6.753988265991211
Epoch 440, val loss: 0.8579617142677307
Epoch 450, training loss: 0.10544228553771973 = 0.03801543638110161 + 0.01 * 6.742684364318848
Epoch 450, val loss: 0.8729942440986633
Epoch 460, training loss: 0.10204055905342102 = 0.03469480574131012 + 0.01 * 6.7345757484436035
Epoch 460, val loss: 0.8878843188285828
Epoch 470, training loss: 0.09917226433753967 = 0.031770650297403336 + 0.01 * 6.740160942077637
Epoch 470, val loss: 0.9026288390159607
Epoch 480, training loss: 0.09646988660097122 = 0.029186077415943146 + 0.01 * 6.728381156921387
Epoch 480, val loss: 0.9170980453491211
Epoch 490, training loss: 0.09411955624818802 = 0.026891274377703667 + 0.01 * 6.722827911376953
Epoch 490, val loss: 0.9313615560531616
Epoch 500, training loss: 0.09206739068031311 = 0.024845851585268974 + 0.01 * 6.722154140472412
Epoch 500, val loss: 0.9452908635139465
Epoch 510, training loss: 0.09017598628997803 = 0.023018594831228256 + 0.01 * 6.715739727020264
Epoch 510, val loss: 0.9590001106262207
Epoch 520, training loss: 0.08849963545799255 = 0.02138117328286171 + 0.01 * 6.711845874786377
Epoch 520, val loss: 0.9723259806632996
Epoch 530, training loss: 0.08697829395532608 = 0.019908467307686806 + 0.01 * 6.7069830894470215
Epoch 530, val loss: 0.9855023622512817
Epoch 540, training loss: 0.08560419082641602 = 0.018580591306090355 + 0.01 * 6.702360153198242
Epoch 540, val loss: 0.9982819557189941
Epoch 550, training loss: 0.08449116349220276 = 0.017381425946950912 + 0.01 * 6.710973262786865
Epoch 550, val loss: 1.0108537673950195
Epoch 560, training loss: 0.08325853943824768 = 0.016296163201332092 + 0.01 * 6.696237564086914
Epoch 560, val loss: 1.0230036973953247
Epoch 570, training loss: 0.08221520483493805 = 0.015309634618461132 + 0.01 * 6.690557479858398
Epoch 570, val loss: 1.0348939895629883
Epoch 580, training loss: 0.08126280456781387 = 0.014410617761313915 + 0.01 * 6.6852192878723145
Epoch 580, val loss: 1.046499490737915
Epoch 590, training loss: 0.08042409271001816 = 0.013589543290436268 + 0.01 * 6.683454990386963
Epoch 590, val loss: 1.057869553565979
Epoch 600, training loss: 0.0796002596616745 = 0.012837388552725315 + 0.01 * 6.676287651062012
Epoch 600, val loss: 1.0689390897750854
Epoch 610, training loss: 0.07899598777294159 = 0.012146227061748505 + 0.01 * 6.684976577758789
Epoch 610, val loss: 1.079797387123108
Epoch 620, training loss: 0.0781751424074173 = 0.011510870419442654 + 0.01 * 6.666427135467529
Epoch 620, val loss: 1.0903598070144653
Epoch 630, training loss: 0.07761237025260925 = 0.010924465022981167 + 0.01 * 6.668790340423584
Epoch 630, val loss: 1.1006367206573486
Epoch 640, training loss: 0.0770229622721672 = 0.010382107459008694 + 0.01 * 6.664085388183594
Epoch 640, val loss: 1.1108460426330566
Epoch 650, training loss: 0.07641149312257767 = 0.009879181161522865 + 0.01 * 6.653231620788574
Epoch 650, val loss: 1.1207975149154663
Epoch 660, training loss: 0.0760498046875 = 0.009412155486643314 + 0.01 * 6.663764953613281
Epoch 660, val loss: 1.1304281949996948
Epoch 670, training loss: 0.0754881501197815 = 0.008979243226349354 + 0.01 * 6.650890827178955
Epoch 670, val loss: 1.140001893043518
Epoch 680, training loss: 0.07498326152563095 = 0.008575664833188057 + 0.01 * 6.6407599449157715
Epoch 680, val loss: 1.149267554283142
Epoch 690, training loss: 0.0747092142701149 = 0.00819983147084713 + 0.01 * 6.650938034057617
Epoch 690, val loss: 1.1582592725753784
Epoch 700, training loss: 0.0742497369647026 = 0.007849536836147308 + 0.01 * 6.64001989364624
Epoch 700, val loss: 1.1672108173370361
Epoch 710, training loss: 0.07387377321720123 = 0.007521928753703833 + 0.01 * 6.6351847648620605
Epoch 710, val loss: 1.1758644580841064
Epoch 720, training loss: 0.0734594538807869 = 0.007215762976557016 + 0.01 * 6.6243696212768555
Epoch 720, val loss: 1.1842844486236572
Epoch 730, training loss: 0.07320397347211838 = 0.006929023191332817 + 0.01 * 6.627494812011719
Epoch 730, val loss: 1.192689299583435
Epoch 740, training loss: 0.07299201190471649 = 0.006660225335508585 + 0.01 * 6.633179187774658
Epoch 740, val loss: 1.2007478475570679
Epoch 750, training loss: 0.0726042091846466 = 0.006407764740288258 + 0.01 * 6.6196441650390625
Epoch 750, val loss: 1.2087361812591553
Epoch 760, training loss: 0.07236218452453613 = 0.0061711277812719345 + 0.01 * 6.619105339050293
Epoch 760, val loss: 1.2164382934570312
Epoch 770, training loss: 0.07201127707958221 = 0.005948301404714584 + 0.01 * 6.606297016143799
Epoch 770, val loss: 1.2240521907806396
Epoch 780, training loss: 0.07187217473983765 = 0.0057379319332540035 + 0.01 * 6.613424301147461
Epoch 780, val loss: 1.2315168380737305
Epoch 790, training loss: 0.07168015092611313 = 0.005541039630770683 + 0.01 * 6.613911151885986
Epoch 790, val loss: 1.2386330366134644
Epoch 800, training loss: 0.07133850455284119 = 0.005354073364287615 + 0.01 * 6.598443031311035
Epoch 800, val loss: 1.2458796501159668
Epoch 810, training loss: 0.07135455310344696 = 0.00517773674800992 + 0.01 * 6.617681980133057
Epoch 810, val loss: 1.2528212070465088
Epoch 820, training loss: 0.07086694985628128 = 0.005011814646422863 + 0.01 * 6.585513591766357
Epoch 820, val loss: 1.2595404386520386
Epoch 830, training loss: 0.07067782431840897 = 0.004854218568652868 + 0.01 * 6.582360744476318
Epoch 830, val loss: 1.266235589981079
Epoch 840, training loss: 0.07075969874858856 = 0.0047054169699549675 + 0.01 * 6.605428695678711
Epoch 840, val loss: 1.2727837562561035
Epoch 850, training loss: 0.07032505422830582 = 0.004564316011965275 + 0.01 * 6.576074123382568
Epoch 850, val loss: 1.2790695428848267
Epoch 860, training loss: 0.07031512260437012 = 0.004430028609931469 + 0.01 * 6.588509559631348
Epoch 860, val loss: 1.2853223085403442
Epoch 870, training loss: 0.07008300721645355 = 0.004303282126784325 + 0.01 * 6.577972412109375
Epoch 870, val loss: 1.2913799285888672
Epoch 880, training loss: 0.06985388696193695 = 0.004181834869086742 + 0.01 * 6.567205429077148
Epoch 880, val loss: 1.2973703145980835
Epoch 890, training loss: 0.06983090937137604 = 0.004066733177751303 + 0.01 * 6.576417446136475
Epoch 890, val loss: 1.3032363653182983
Epoch 900, training loss: 0.0695159062743187 = 0.003957082517445087 + 0.01 * 6.555881977081299
Epoch 900, val loss: 1.3089210987091064
Epoch 910, training loss: 0.06957817077636719 = 0.0038522263057529926 + 0.01 * 6.57259464263916
Epoch 910, val loss: 1.3143953084945679
Epoch 920, training loss: 0.06942304223775864 = 0.0037533058784902096 + 0.01 * 6.566973686218262
Epoch 920, val loss: 1.3198879957199097
Epoch 930, training loss: 0.06947141140699387 = 0.003658125875517726 + 0.01 * 6.581328868865967
Epoch 930, val loss: 1.325175404548645
Epoch 940, training loss: 0.0690225288271904 = 0.0035680108703672886 + 0.01 * 6.545452117919922
Epoch 940, val loss: 1.3302574157714844
Epoch 950, training loss: 0.06888946145772934 = 0.0034809198696166277 + 0.01 * 6.540854454040527
Epoch 950, val loss: 1.3355157375335693
Epoch 960, training loss: 0.06902021169662476 = 0.003398388624191284 + 0.01 * 6.562182426452637
Epoch 960, val loss: 1.3405358791351318
Epoch 970, training loss: 0.0687558650970459 = 0.0033192222472280264 + 0.01 * 6.543664455413818
Epoch 970, val loss: 1.3453114032745361
Epoch 980, training loss: 0.06878412514925003 = 0.003243129001930356 + 0.01 * 6.554100036621094
Epoch 980, val loss: 1.3501073122024536
Epoch 990, training loss: 0.06856322288513184 = 0.003170288633555174 + 0.01 * 6.53929328918457
Epoch 990, val loss: 1.3548561334609985
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.6827
Flip ASR: 0.6267/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0503902435302734 = 1.9666576385498047 + 0.01 * 8.373265266418457
Epoch 0, val loss: 1.9745655059814453
Epoch 10, training loss: 2.0355899333953857 = 1.951892614364624 + 0.01 * 8.369721412658691
Epoch 10, val loss: 1.9554039239883423
Epoch 20, training loss: 2.017195463180542 = 1.9335505962371826 + 0.01 * 8.364482879638672
Epoch 20, val loss: 1.9307535886764526
Epoch 30, training loss: 1.9943825006484985 = 1.910779595375061 + 0.01 * 8.360286712646484
Epoch 30, val loss: 1.9019863605499268
Epoch 40, training loss: 1.965936303138733 = 1.882498860359192 + 0.01 * 8.34374713897705
Epoch 40, val loss: 1.8699780702590942
Epoch 50, training loss: 1.9269713163375854 = 1.8445690870285034 + 0.01 * 8.24022102355957
Epoch 50, val loss: 1.8304499387741089
Epoch 60, training loss: 1.8801597356796265 = 1.799534797668457 + 0.01 * 8.062494277954102
Epoch 60, val loss: 1.7877577543258667
Epoch 70, training loss: 1.8382879495620728 = 1.7591173648834229 + 0.01 * 7.917056560516357
Epoch 70, val loss: 1.7541254758834839
Epoch 80, training loss: 1.7936508655548096 = 1.7175772190093994 + 0.01 * 7.607368469238281
Epoch 80, val loss: 1.7201350927352905
Epoch 90, training loss: 1.7349094152450562 = 1.6613274812698364 + 0.01 * 7.3581929206848145
Epoch 90, val loss: 1.6735228300094604
Epoch 100, training loss: 1.6570124626159668 = 1.584739327430725 + 0.01 * 7.2273149490356445
Epoch 100, val loss: 1.6103750467300415
Epoch 110, training loss: 1.562841773033142 = 1.4918280839920044 + 0.01 * 7.101373195648193
Epoch 110, val loss: 1.5366405248641968
Epoch 120, training loss: 1.4669371843338013 = 1.3964978456497192 + 0.01 * 7.043937683105469
Epoch 120, val loss: 1.4654229879379272
Epoch 130, training loss: 1.3773521184921265 = 1.3073362112045288 + 0.01 * 7.001592636108398
Epoch 130, val loss: 1.4037768840789795
Epoch 140, training loss: 1.294095516204834 = 1.2244466543197632 + 0.01 * 6.964890480041504
Epoch 140, val loss: 1.3507623672485352
Epoch 150, training loss: 1.2161517143249512 = 1.146833062171936 + 0.01 * 6.931865692138672
Epoch 150, val loss: 1.302795648574829
Epoch 160, training loss: 1.1423258781433105 = 1.0732476711273193 + 0.01 * 6.907819747924805
Epoch 160, val loss: 1.2574845552444458
Epoch 170, training loss: 1.0714995861053467 = 1.002565622329712 + 0.01 * 6.89339542388916
Epoch 170, val loss: 1.2114312648773193
Epoch 180, training loss: 1.0032024383544922 = 0.9343402981758118 + 0.01 * 6.886219024658203
Epoch 180, val loss: 1.1647112369537354
Epoch 190, training loss: 0.9366423487663269 = 0.8678207397460938 + 0.01 * 6.882160186767578
Epoch 190, val loss: 1.1177693605422974
Epoch 200, training loss: 0.8700763583183289 = 0.8012877702713013 + 0.01 * 6.878859043121338
Epoch 200, val loss: 1.0698037147521973
Epoch 210, training loss: 0.8021034002304077 = 0.733344316482544 + 0.01 * 6.875905513763428
Epoch 210, val loss: 1.0205726623535156
Epoch 220, training loss: 0.7332051396369934 = 0.664444088935852 + 0.01 * 6.876102924346924
Epoch 220, val loss: 0.9701521992683411
Epoch 230, training loss: 0.6657693982124329 = 0.5970657467842102 + 0.01 * 6.870363235473633
Epoch 230, val loss: 0.9217347502708435
Epoch 240, training loss: 0.6023827195167542 = 0.5337113738059998 + 0.01 * 6.867134094238281
Epoch 240, val loss: 0.8778260350227356
Epoch 250, training loss: 0.5443081855773926 = 0.4756813943386078 + 0.01 * 6.862678050994873
Epoch 250, val loss: 0.8410558700561523
Epoch 260, training loss: 0.49062228202819824 = 0.4219914972782135 + 0.01 * 6.863080024719238
Epoch 260, val loss: 0.8107393383979797
Epoch 270, training loss: 0.4399387240409851 = 0.3714051842689514 + 0.01 * 6.8533525466918945
Epoch 270, val loss: 0.7861846089363098
Epoch 280, training loss: 0.3916282057762146 = 0.32311612367630005 + 0.01 * 6.851208686828613
Epoch 280, val loss: 0.76547771692276
Epoch 290, training loss: 0.34623730182647705 = 0.2777811884880066 + 0.01 * 6.845611572265625
Epoch 290, val loss: 0.7487539052963257
Epoch 300, training loss: 0.30497220158576965 = 0.2365584820508957 + 0.01 * 6.841371059417725
Epoch 300, val loss: 0.7357202172279358
Epoch 310, training loss: 0.2689526081085205 = 0.20057915151119232 + 0.01 * 6.837347507476807
Epoch 310, val loss: 0.7269976735115051
Epoch 320, training loss: 0.2386915683746338 = 0.17026951909065247 + 0.01 * 6.842205047607422
Epoch 320, val loss: 0.7227057814598083
Epoch 330, training loss: 0.21359679102897644 = 0.14527128636837006 + 0.01 * 6.832550048828125
Epoch 330, val loss: 0.7225421667098999
Epoch 340, training loss: 0.19307386875152588 = 0.12475556880235672 + 0.01 * 6.831829071044922
Epoch 340, val loss: 0.7258222699165344
Epoch 350, training loss: 0.17612029612064362 = 0.10784805566072464 + 0.01 * 6.827224254608154
Epoch 350, val loss: 0.7319476008415222
Epoch 360, training loss: 0.16210143268108368 = 0.09385321289300919 + 0.01 * 6.824821949005127
Epoch 360, val loss: 0.7403590083122253
Epoch 370, training loss: 0.1504007875919342 = 0.0821823999285698 + 0.01 * 6.82183837890625
Epoch 370, val loss: 0.7503026127815247
Epoch 380, training loss: 0.14070071280002594 = 0.07238222658634186 + 0.01 * 6.831848621368408
Epoch 380, val loss: 0.7615821957588196
Epoch 390, training loss: 0.13226646184921265 = 0.06410273909568787 + 0.01 * 6.816371917724609
Epoch 390, val loss: 0.7737192511558533
Epoch 400, training loss: 0.125217005610466 = 0.057070355862379074 + 0.01 * 6.8146653175354
Epoch 400, val loss: 0.7864731550216675
Epoch 410, training loss: 0.11916166543960571 = 0.05105723440647125 + 0.01 * 6.810442924499512
Epoch 410, val loss: 0.7995763421058655
Epoch 420, training loss: 0.11395376920700073 = 0.04588460922241211 + 0.01 * 6.806916236877441
Epoch 420, val loss: 0.8129828572273254
Epoch 430, training loss: 0.10947516560554504 = 0.04142286628484726 + 0.01 * 6.805230140686035
Epoch 430, val loss: 0.8266679644584656
Epoch 440, training loss: 0.10554404556751251 = 0.037544284015893936 + 0.01 * 6.799976348876953
Epoch 440, val loss: 0.84037846326828
Epoch 450, training loss: 0.10211020708084106 = 0.03415553271770477 + 0.01 * 6.795467853546143
Epoch 450, val loss: 0.8540682196617126
Epoch 460, training loss: 0.09936128556728363 = 0.03118663839995861 + 0.01 * 6.817464828491211
Epoch 460, val loss: 0.8676328659057617
Epoch 470, training loss: 0.0965830534696579 = 0.028577568009495735 + 0.01 * 6.800548553466797
Epoch 470, val loss: 0.8809700608253479
Epoch 480, training loss: 0.09410696476697922 = 0.026272110641002655 + 0.01 * 6.7834858894348145
Epoch 480, val loss: 0.8941103219985962
Epoch 490, training loss: 0.09202203154563904 = 0.02422592230141163 + 0.01 * 6.779611110687256
Epoch 490, val loss: 0.9071061015129089
Epoch 500, training loss: 0.09013015776872635 = 0.02240358293056488 + 0.01 * 6.772657871246338
Epoch 500, val loss: 0.9198013544082642
Epoch 510, training loss: 0.08873986452817917 = 0.020775804296135902 + 0.01 * 6.796406269073486
Epoch 510, val loss: 0.9322584867477417
Epoch 520, training loss: 0.08695541322231293 = 0.019318029284477234 + 0.01 * 6.763738632202148
Epoch 520, val loss: 0.9444167613983154
Epoch 530, training loss: 0.08557640761137009 = 0.018008209764957428 + 0.01 * 6.756820201873779
Epoch 530, val loss: 0.9563056230545044
Epoch 540, training loss: 0.08453058451414108 = 0.016827724874019623 + 0.01 * 6.7702860832214355
Epoch 540, val loss: 0.9678542613983154
Epoch 550, training loss: 0.08324819803237915 = 0.01576264761388302 + 0.01 * 6.7485551834106445
Epoch 550, val loss: 0.9790635704994202
Epoch 560, training loss: 0.0822322815656662 = 0.014798365533351898 + 0.01 * 6.743391513824463
Epoch 560, val loss: 0.9899720549583435
Epoch 570, training loss: 0.08139082044363022 = 0.013921869918704033 + 0.01 * 6.7468953132629395
Epoch 570, val loss: 1.0006159543991089
Epoch 580, training loss: 0.08037463575601578 = 0.013123570941388607 + 0.01 * 6.725106716156006
Epoch 580, val loss: 1.0109920501708984
Epoch 590, training loss: 0.07999119907617569 = 0.012394726276397705 + 0.01 * 6.759647369384766
Epoch 590, val loss: 1.0210429430007935
Epoch 600, training loss: 0.07897622883319855 = 0.011728603392839432 + 0.01 * 6.724762439727783
Epoch 600, val loss: 1.030887484550476
Epoch 610, training loss: 0.07822439074516296 = 0.011117943562567234 + 0.01 * 6.710644721984863
Epoch 610, val loss: 1.040358543395996
Epoch 620, training loss: 0.07761682569980621 = 0.01055595651268959 + 0.01 * 6.706087589263916
Epoch 620, val loss: 1.0496459007263184
Epoch 630, training loss: 0.07715815305709839 = 0.010038744658231735 + 0.01 * 6.711941719055176
Epoch 630, val loss: 1.0586881637573242
Epoch 640, training loss: 0.0765380859375 = 0.009560555219650269 + 0.01 * 6.697752952575684
Epoch 640, val loss: 1.067487120628357
Epoch 650, training loss: 0.07592621445655823 = 0.009119111113250256 + 0.01 * 6.680710792541504
Epoch 650, val loss: 1.0760588645935059
Epoch 660, training loss: 0.07559235394001007 = 0.008711016736924648 + 0.01 * 6.688133716583252
Epoch 660, val loss: 1.0843576192855835
Epoch 670, training loss: 0.07504600286483765 = 0.008332491852343082 + 0.01 * 6.671351432800293
Epoch 670, val loss: 1.0924004316329956
Epoch 680, training loss: 0.07503440976142883 = 0.007980464957654476 + 0.01 * 6.705394268035889
Epoch 680, val loss: 1.100284457206726
Epoch 690, training loss: 0.07423260807991028 = 0.007652311585843563 + 0.01 * 6.658030033111572
Epoch 690, val loss: 1.1079883575439453
Epoch 700, training loss: 0.07395236194133759 = 0.007345672696828842 + 0.01 * 6.660669803619385
Epoch 700, val loss: 1.115524172782898
Epoch 710, training loss: 0.07372426986694336 = 0.007059936877340078 + 0.01 * 6.666432857513428
Epoch 710, val loss: 1.1228471994400024
Epoch 720, training loss: 0.07306189090013504 = 0.006791571620851755 + 0.01 * 6.627032279968262
Epoch 720, val loss: 1.12996244430542
Epoch 730, training loss: 0.07276888191699982 = 0.006540391128510237 + 0.01 * 6.622849464416504
Epoch 730, val loss: 1.1369006633758545
Epoch 740, training loss: 0.07252295315265656 = 0.006305117160081863 + 0.01 * 6.621783256530762
Epoch 740, val loss: 1.1437560319900513
Epoch 750, training loss: 0.07249473035335541 = 0.006083443760871887 + 0.01 * 6.6411285400390625
Epoch 750, val loss: 1.150356650352478
Epoch 760, training loss: 0.07203233242034912 = 0.005875270813703537 + 0.01 * 6.615705966949463
Epoch 760, val loss: 1.156864047050476
Epoch 770, training loss: 0.07191884517669678 = 0.005678769666701555 + 0.01 * 6.624007701873779
Epoch 770, val loss: 1.1631700992584229
Epoch 780, training loss: 0.07147956639528275 = 0.005493764765560627 + 0.01 * 6.598580360412598
Epoch 780, val loss: 1.1694159507751465
Epoch 790, training loss: 0.07150298357009888 = 0.005318882875144482 + 0.01 * 6.618410587310791
Epoch 790, val loss: 1.175471305847168
Epoch 800, training loss: 0.07102089375257492 = 0.00515335239470005 + 0.01 * 6.58675479888916
Epoch 800, val loss: 1.1813993453979492
Epoch 810, training loss: 0.07128936052322388 = 0.004996830131858587 + 0.01 * 6.629253387451172
Epoch 810, val loss: 1.1871649026870728
Epoch 820, training loss: 0.07051791250705719 = 0.004848523531109095 + 0.01 * 6.566938877105713
Epoch 820, val loss: 1.1928366422653198
Epoch 830, training loss: 0.07032591104507446 = 0.004707616288214922 + 0.01 * 6.561829090118408
Epoch 830, val loss: 1.1983709335327148
Epoch 840, training loss: 0.07034538686275482 = 0.004574209917336702 + 0.01 * 6.577117919921875
Epoch 840, val loss: 1.2037984132766724
Epoch 850, training loss: 0.07011190056800842 = 0.004447183106094599 + 0.01 * 6.566472053527832
Epoch 850, val loss: 1.2091031074523926
Epoch 860, training loss: 0.06989486515522003 = 0.004326393827795982 + 0.01 * 6.556847095489502
Epoch 860, val loss: 1.2143523693084717
Epoch 870, training loss: 0.06952206045389175 = 0.004211300052702427 + 0.01 * 6.531075954437256
Epoch 870, val loss: 1.2194141149520874
Epoch 880, training loss: 0.06961438804864883 = 0.004101463593542576 + 0.01 * 6.551292419433594
Epoch 880, val loss: 1.2243684530258179
Epoch 890, training loss: 0.06943073868751526 = 0.00399696733802557 + 0.01 * 6.543376922607422
Epoch 890, val loss: 1.229243278503418
Epoch 900, training loss: 0.06909539550542831 = 0.0038971726316958666 + 0.01 * 6.519822120666504
Epoch 900, val loss: 1.2340552806854248
Epoch 910, training loss: 0.06919042766094208 = 0.003801810322329402 + 0.01 * 6.538862228393555
Epoch 910, val loss: 1.2387733459472656
Epoch 920, training loss: 0.06892204284667969 = 0.003710355842486024 + 0.01 * 6.5211687088012695
Epoch 920, val loss: 1.2433316707611084
Epoch 930, training loss: 0.06868238002061844 = 0.003623116062954068 + 0.01 * 6.505927085876465
Epoch 930, val loss: 1.2478837966918945
Epoch 940, training loss: 0.06860297173261642 = 0.0035395461600273848 + 0.01 * 6.50634241104126
Epoch 940, val loss: 1.2522878646850586
Epoch 950, training loss: 0.06853927671909332 = 0.003459308296442032 + 0.01 * 6.507997035980225
Epoch 950, val loss: 1.2566407918930054
Epoch 960, training loss: 0.06875035911798477 = 0.003382542170584202 + 0.01 * 6.536782264709473
Epoch 960, val loss: 1.26090407371521
Epoch 970, training loss: 0.06835953891277313 = 0.0033089742064476013 + 0.01 * 6.505056381225586
Epoch 970, val loss: 1.2650290727615356
Epoch 980, training loss: 0.06833896785974503 = 0.003238540841266513 + 0.01 * 6.510042667388916
Epoch 980, val loss: 1.2691383361816406
Epoch 990, training loss: 0.06814588606357574 = 0.003170549403876066 + 0.01 * 6.497533798217773
Epoch 990, val loss: 1.2731273174285889
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.6753
Flip ASR: 0.6844/225 nodes
The final ASR:0.61255, 0.09398, Accuracy:0.82469, 0.00972
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11646])
remove edge: torch.Size([2, 9560])
updated graph: torch.Size([2, 10650])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
The final ASR:0.97294, 0.00174, Accuracy:0.83210, 0.00924
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.013453722000122 = 1.9297153949737549 + 0.01 * 8.373824119567871
Epoch 0, val loss: 1.9268155097961426
Epoch 10, training loss: 2.0040082931518555 = 1.9202709197998047 + 0.01 * 8.373741149902344
Epoch 10, val loss: 1.9176673889160156
Epoch 20, training loss: 1.9924607276916504 = 1.9087269306182861 + 0.01 * 8.373384475708008
Epoch 20, val loss: 1.9066492319107056
Epoch 30, training loss: 1.9761587381362915 = 1.8924376964569092 + 0.01 * 8.37210750579834
Epoch 30, val loss: 1.8915120363235474
Epoch 40, training loss: 1.9518033266067505 = 1.8681647777557373 + 0.01 * 8.363856315612793
Epoch 40, val loss: 1.8698564767837524
Epoch 50, training loss: 1.9170832633972168 = 1.8340495824813843 + 0.01 * 8.303370475769043
Epoch 50, val loss: 1.8411359786987305
Epoch 60, training loss: 1.8732417821884155 = 1.7934962511062622 + 0.01 * 7.974548816680908
Epoch 60, val loss: 1.8089265823364258
Epoch 70, training loss: 1.8270515203475952 = 1.7492696046829224 + 0.01 * 7.778191566467285
Epoch 70, val loss: 1.7717657089233398
Epoch 80, training loss: 1.7661941051483154 = 1.6895564794540405 + 0.01 * 7.663765907287598
Epoch 80, val loss: 1.7183057069778442
Epoch 90, training loss: 1.6841500997543335 = 1.608306646347046 + 0.01 * 7.584343910217285
Epoch 90, val loss: 1.649965524673462
Epoch 100, training loss: 1.5864756107330322 = 1.512342929840088 + 0.01 * 7.413266181945801
Epoch 100, val loss: 1.574394702911377
Epoch 110, training loss: 1.4881340265274048 = 1.4167243242263794 + 0.01 * 7.140971660614014
Epoch 110, val loss: 1.4975669384002686
Epoch 120, training loss: 1.3990713357925415 = 1.3282551765441895 + 0.01 * 7.081612586975098
Epoch 120, val loss: 1.4280143976211548
Epoch 130, training loss: 1.31831955909729 = 1.2481272220611572 + 0.01 * 7.019233703613281
Epoch 130, val loss: 1.367174506187439
Epoch 140, training loss: 1.24565589427948 = 1.1757497787475586 + 0.01 * 6.990610122680664
Epoch 140, val loss: 1.3148175477981567
Epoch 150, training loss: 1.1785764694213867 = 1.1088509559631348 + 0.01 * 6.972556114196777
Epoch 150, val loss: 1.2681387662887573
Epoch 160, training loss: 1.1139720678329468 = 1.0443994998931885 + 0.01 * 6.957255840301514
Epoch 160, val loss: 1.2237871885299683
Epoch 170, training loss: 1.0509629249572754 = 0.9815382361412048 + 0.01 * 6.942475318908691
Epoch 170, val loss: 1.180363416671753
Epoch 180, training loss: 0.9898889660835266 = 0.920634925365448 + 0.01 * 6.9254045486450195
Epoch 180, val loss: 1.138049840927124
Epoch 190, training loss: 0.9299889206886292 = 0.8609192967414856 + 0.01 * 6.9069647789001465
Epoch 190, val loss: 1.0960153341293335
Epoch 200, training loss: 0.8694607615470886 = 0.800578773021698 + 0.01 * 6.8882012367248535
Epoch 200, val loss: 1.0532560348510742
Epoch 210, training loss: 0.8071913719177246 = 0.7384541034698486 + 0.01 * 6.873724460601807
Epoch 210, val loss: 1.0091426372528076
Epoch 220, training loss: 0.7436010837554932 = 0.6749956011772156 + 0.01 * 6.860547065734863
Epoch 220, val loss: 0.9649208188056946
Epoch 230, training loss: 0.6800391674041748 = 0.6115332841873169 + 0.01 * 6.850587844848633
Epoch 230, val loss: 0.9219868779182434
Epoch 240, training loss: 0.617633581161499 = 0.5491998195648193 + 0.01 * 6.84337854385376
Epoch 240, val loss: 0.8816338181495667
Epoch 250, training loss: 0.5571542978286743 = 0.48878535628318787 + 0.01 * 6.836893081665039
Epoch 250, val loss: 0.8450245261192322
Epoch 260, training loss: 0.49939537048339844 = 0.43108245730400085 + 0.01 * 6.831292152404785
Epoch 260, val loss: 0.8129420280456543
Epoch 270, training loss: 0.4451788663864136 = 0.37690970301628113 + 0.01 * 6.82691764831543
Epoch 270, val loss: 0.7862820029258728
Epoch 280, training loss: 0.39529862999916077 = 0.32705163955688477 + 0.01 * 6.8246989250183105
Epoch 280, val loss: 0.7651491165161133
Epoch 290, training loss: 0.35041970014572144 = 0.28220734000205994 + 0.01 * 6.821237087249756
Epoch 290, val loss: 0.7494776844978333
Epoch 300, training loss: 0.31114545464515686 = 0.24295544624328613 + 0.01 * 6.819000720977783
Epoch 300, val loss: 0.7393681406974792
Epoch 310, training loss: 0.27761852741241455 = 0.2094496339559555 + 0.01 * 6.816890239715576
Epoch 310, val loss: 0.7344903945922852
Epoch 320, training loss: 0.24942371249198914 = 0.1812744289636612 + 0.01 * 6.81492805480957
Epoch 320, val loss: 0.7346033453941345
Epoch 330, training loss: 0.2257925420999527 = 0.1576646864414215 + 0.01 * 6.812785625457764
Epoch 330, val loss: 0.7390081882476807
Epoch 340, training loss: 0.20590627193450928 = 0.13779588043689728 + 0.01 * 6.81104040145874
Epoch 340, val loss: 0.7468160390853882
Epoch 350, training loss: 0.18902288377285004 = 0.12094464153051376 + 0.01 * 6.80782413482666
Epoch 350, val loss: 0.7570348978042603
Epoch 360, training loss: 0.1745695173740387 = 0.10652211308479309 + 0.01 * 6.804739475250244
Epoch 360, val loss: 0.7689939737319946
Epoch 370, training loss: 0.16212034225463867 = 0.09410636872053146 + 0.01 * 6.801397800445557
Epoch 370, val loss: 0.7821131944656372
Epoch 380, training loss: 0.15137417614459991 = 0.08337250351905823 + 0.01 * 6.800167083740234
Epoch 380, val loss: 0.795958936214447
Epoch 390, training loss: 0.1420140266418457 = 0.07406360656023026 + 0.01 * 6.795042037963867
Epoch 390, val loss: 0.8102820515632629
Epoch 400, training loss: 0.13386337459087372 = 0.06597008556127548 + 0.01 * 6.7893290519714355
Epoch 400, val loss: 0.8249462246894836
Epoch 410, training loss: 0.12675584852695465 = 0.05892207846045494 + 0.01 * 6.783376693725586
Epoch 410, val loss: 0.8397867679595947
Epoch 420, training loss: 0.12056779116392136 = 0.052779391407966614 + 0.01 * 6.778840065002441
Epoch 420, val loss: 0.8546817898750305
Epoch 430, training loss: 0.11516653001308441 = 0.047419194132089615 + 0.01 * 6.774733543395996
Epoch 430, val loss: 0.8695147037506104
Epoch 440, training loss: 0.11039712280035019 = 0.04273383319377899 + 0.01 * 6.766328811645508
Epoch 440, val loss: 0.884117841720581
Epoch 450, training loss: 0.1062311977148056 = 0.038631655275821686 + 0.01 * 6.759954452514648
Epoch 450, val loss: 0.8985018134117126
Epoch 460, training loss: 0.10262925922870636 = 0.035031601786613464 + 0.01 * 6.759766101837158
Epoch 460, val loss: 0.9125479459762573
Epoch 470, training loss: 0.09937674552202225 = 0.03186650574207306 + 0.01 * 6.751023769378662
Epoch 470, val loss: 0.9263339042663574
Epoch 480, training loss: 0.09647461026906967 = 0.02907709591090679 + 0.01 * 6.73975133895874
Epoch 480, val loss: 0.9396944642066956
Epoch 490, training loss: 0.09398753196001053 = 0.026612119749188423 + 0.01 * 6.737541675567627
Epoch 490, val loss: 0.9527775645256042
Epoch 500, training loss: 0.09177746623754501 = 0.024428514763712883 + 0.01 * 6.7348952293396
Epoch 500, val loss: 0.965433657169342
Epoch 510, training loss: 0.08977380394935608 = 0.02248966321349144 + 0.01 * 6.728414535522461
Epoch 510, val loss: 0.9778173565864563
Epoch 520, training loss: 0.08800150454044342 = 0.020763030275702477 + 0.01 * 6.723847389221191
Epoch 520, val loss: 0.9897522926330566
Epoch 530, training loss: 0.08639402687549591 = 0.019221944734454155 + 0.01 * 6.717207908630371
Epoch 530, val loss: 1.001407504081726
Epoch 540, training loss: 0.08493899554014206 = 0.017842335626482964 + 0.01 * 6.7096662521362305
Epoch 540, val loss: 1.0126503705978394
Epoch 550, training loss: 0.08369667083024979 = 0.01660384237766266 + 0.01 * 6.709283351898193
Epoch 550, val loss: 1.0235426425933838
Epoch 560, training loss: 0.08251770585775375 = 0.015488888137042522 + 0.01 * 6.702882289886475
Epoch 560, val loss: 1.0341668128967285
Epoch 570, training loss: 0.08140929788351059 = 0.014482269994914532 + 0.01 * 6.6927032470703125
Epoch 570, val loss: 1.0444449186325073
Epoch 580, training loss: 0.08060916513204575 = 0.013570992276072502 + 0.01 * 6.703816890716553
Epoch 580, val loss: 1.0543854236602783
Epoch 590, training loss: 0.07958681881427765 = 0.012744714505970478 + 0.01 * 6.684210300445557
Epoch 590, val loss: 1.0640844106674194
Epoch 600, training loss: 0.07878899574279785 = 0.011993014253675938 + 0.01 * 6.679598331451416
Epoch 600, val loss: 1.0735292434692383
Epoch 610, training loss: 0.07806883752346039 = 0.01130708772689104 + 0.01 * 6.676175117492676
Epoch 610, val loss: 1.0826832056045532
Epoch 620, training loss: 0.07741635292768478 = 0.010679519735276699 + 0.01 * 6.6736836433410645
Epoch 620, val loss: 1.0916045904159546
Epoch 630, training loss: 0.07678070664405823 = 0.010104564018547535 + 0.01 * 6.667614459991455
Epoch 630, val loss: 1.1002436876296997
Epoch 640, training loss: 0.07620953768491745 = 0.009576523676514626 + 0.01 * 6.663301944732666
Epoch 640, val loss: 1.1086785793304443
Epoch 650, training loss: 0.07573050260543823 = 0.009089893661439419 + 0.01 * 6.664061069488525
Epoch 650, val loss: 1.1169207096099854
Epoch 660, training loss: 0.07520198076963425 = 0.008640097454190254 + 0.01 * 6.656188488006592
Epoch 660, val loss: 1.1249116659164429
Epoch 670, training loss: 0.07473038136959076 = 0.008222119882702827 + 0.01 * 6.650826454162598
Epoch 670, val loss: 1.1328237056732178
Epoch 680, training loss: 0.07436574995517731 = 0.007831772789359093 + 0.01 * 6.653397560119629
Epoch 680, val loss: 1.140618920326233
Epoch 690, training loss: 0.0738869458436966 = 0.007467064540833235 + 0.01 * 6.641988277435303
Epoch 690, val loss: 1.148214340209961
Epoch 700, training loss: 0.07351306825876236 = 0.0071258870884776115 + 0.01 * 6.638718605041504
Epoch 700, val loss: 1.155733585357666
Epoch 710, training loss: 0.07321526855230331 = 0.006806180812418461 + 0.01 * 6.640909194946289
Epoch 710, val loss: 1.1631271839141846
Epoch 720, training loss: 0.07285759598016739 = 0.006507185287773609 + 0.01 * 6.635040760040283
Epoch 720, val loss: 1.1703041791915894
Epoch 730, training loss: 0.07253515720367432 = 0.006227036472409964 + 0.01 * 6.630812168121338
Epoch 730, val loss: 1.1774179935455322
Epoch 740, training loss: 0.07220423966646194 = 0.005964838899672031 + 0.01 * 6.6239399909973145
Epoch 740, val loss: 1.1843233108520508
Epoch 750, training loss: 0.07197088748216629 = 0.005718935746699572 + 0.01 * 6.625195026397705
Epoch 750, val loss: 1.1910860538482666
Epoch 760, training loss: 0.07161995023488998 = 0.005488497670739889 + 0.01 * 6.613145351409912
Epoch 760, val loss: 1.1977020502090454
Epoch 770, training loss: 0.07139226049184799 = 0.005272382870316505 + 0.01 * 6.611988067626953
Epoch 770, val loss: 1.204138994216919
Epoch 780, training loss: 0.07113315165042877 = 0.005069407168775797 + 0.01 * 6.606374740600586
Epoch 780, val loss: 1.2105200290679932
Epoch 790, training loss: 0.07096822559833527 = 0.004879015497863293 + 0.01 * 6.608921527862549
Epoch 790, val loss: 1.216607928276062
Epoch 800, training loss: 0.07071289420127869 = 0.004700211342424154 + 0.01 * 6.601268291473389
Epoch 800, val loss: 1.222596287727356
Epoch 810, training loss: 0.07057193666696548 = 0.004532082937657833 + 0.01 * 6.603985786437988
Epoch 810, val loss: 1.2284184694290161
Epoch 820, training loss: 0.07029367983341217 = 0.00437398673966527 + 0.01 * 6.5919694900512695
Epoch 820, val loss: 1.234156847000122
Epoch 830, training loss: 0.07016890496015549 = 0.004224867094308138 + 0.01 * 6.594404220581055
Epoch 830, val loss: 1.2397361993789673
Epoch 840, training loss: 0.06993220001459122 = 0.004084092099219561 + 0.01 * 6.584810733795166
Epoch 840, val loss: 1.2452090978622437
Epoch 850, training loss: 0.06981160491704941 = 0.003951419610530138 + 0.01 * 6.5860185623168945
Epoch 850, val loss: 1.250336766242981
Epoch 860, training loss: 0.06956890970468521 = 0.003826220752671361 + 0.01 * 6.574268817901611
Epoch 860, val loss: 1.2555553913116455
Epoch 870, training loss: 0.06940583139657974 = 0.003707634285092354 + 0.01 * 6.569819927215576
Epoch 870, val loss: 1.2606186866760254
Epoch 880, training loss: 0.06939037889242172 = 0.0035953072365373373 + 0.01 * 6.579507350921631
Epoch 880, val loss: 1.2653874158859253
Epoch 890, training loss: 0.06918904185295105 = 0.003489083843305707 + 0.01 * 6.569995880126953
Epoch 890, val loss: 1.2702100276947021
Epoch 900, training loss: 0.06903466582298279 = 0.0033882493153214455 + 0.01 * 6.564641952514648
Epoch 900, val loss: 1.2749537229537964
Epoch 910, training loss: 0.06884089857339859 = 0.0032923955004662275 + 0.01 * 6.5548505783081055
Epoch 910, val loss: 1.2795344591140747
Epoch 920, training loss: 0.06897784769535065 = 0.0032011796720325947 + 0.01 * 6.577666759490967
Epoch 920, val loss: 1.2838958501815796
Epoch 930, training loss: 0.06874922662973404 = 0.003114759223535657 + 0.01 * 6.563446521759033
Epoch 930, val loss: 1.2882604598999023
Epoch 940, training loss: 0.06859182566404343 = 0.0030326650012284517 + 0.01 * 6.5559163093566895
Epoch 940, val loss: 1.292487621307373
Epoch 950, training loss: 0.06851190328598022 = 0.00295446440577507 + 0.01 * 6.555744647979736
Epoch 950, val loss: 1.2966824769973755
Epoch 960, training loss: 0.0683567151427269 = 0.002879929728806019 + 0.01 * 6.547678470611572
Epoch 960, val loss: 1.3007328510284424
Epoch 970, training loss: 0.06831017136573792 = 0.0028088409453630447 + 0.01 * 6.550133228302002
Epoch 970, val loss: 1.3046144247055054
Epoch 980, training loss: 0.06816615909337997 = 0.0027410550974309444 + 0.01 * 6.542510509490967
Epoch 980, val loss: 1.3085881471633911
Epoch 990, training loss: 0.06820554286241531 = 0.0026763647329062223 + 0.01 * 6.552917957305908
Epoch 990, val loss: 1.3123061656951904
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.6605
Flip ASR: 0.5956/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.014883518218994 = 1.931145429611206 + 0.01 * 8.373817443847656
Epoch 0, val loss: 1.9209529161453247
Epoch 10, training loss: 2.004714012145996 = 1.9209768772125244 + 0.01 * 8.373712539672852
Epoch 10, val loss: 1.9105274677276611
Epoch 20, training loss: 1.9922046661376953 = 1.9084718227386475 + 0.01 * 8.373289108276367
Epoch 20, val loss: 1.8971073627471924
Epoch 30, training loss: 1.9743722677230835 = 1.8906546831130981 + 0.01 * 8.371761322021484
Epoch 30, val loss: 1.8776875734329224
Epoch 40, training loss: 1.9481098651885986 = 1.8644773960113525 + 0.01 * 8.363250732421875
Epoch 40, val loss: 1.8496168851852417
Epoch 50, training loss: 1.912139654159546 = 1.828997015953064 + 0.01 * 8.314262390136719
Epoch 50, val loss: 1.8136392831802368
Epoch 60, training loss: 1.8704895973205566 = 1.78990638256073 + 0.01 * 8.058320045471191
Epoch 60, val loss: 1.777970552444458
Epoch 70, training loss: 1.8298051357269287 = 1.7518898248672485 + 0.01 * 7.7915263175964355
Epoch 70, val loss: 1.7455196380615234
Epoch 80, training loss: 1.7780306339263916 = 1.7026926279067993 + 0.01 * 7.5338006019592285
Epoch 80, val loss: 1.7029277086257935
Epoch 90, training loss: 1.7092946767807007 = 1.635616421699524 + 0.01 * 7.367824554443359
Epoch 90, val loss: 1.6459333896636963
Epoch 100, training loss: 1.6236056089401245 = 1.5510282516479492 + 0.01 * 7.2577385902404785
Epoch 100, val loss: 1.5767302513122559
Epoch 110, training loss: 1.5299495458602905 = 1.4583297967910767 + 0.01 * 7.161978244781494
Epoch 110, val loss: 1.5028088092803955
Epoch 120, training loss: 1.4383958578109741 = 1.3676658868789673 + 0.01 * 7.072991847991943
Epoch 120, val loss: 1.434335708618164
Epoch 130, training loss: 1.350786805152893 = 1.2806333303451538 + 0.01 * 7.015346527099609
Epoch 130, val loss: 1.3706413507461548
Epoch 140, training loss: 1.2645210027694702 = 1.1948323249816895 + 0.01 * 6.968870162963867
Epoch 140, val loss: 1.308979868888855
Epoch 150, training loss: 1.1777504682540894 = 1.1085559129714966 + 0.01 * 6.919454097747803
Epoch 150, val loss: 1.246104121208191
Epoch 160, training loss: 1.090571403503418 = 1.0216981172561646 + 0.01 * 6.887333393096924
Epoch 160, val loss: 1.1817164421081543
Epoch 170, training loss: 1.004088282585144 = 0.9353160858154297 + 0.01 * 6.877219200134277
Epoch 170, val loss: 1.1172494888305664
Epoch 180, training loss: 0.919556736946106 = 0.8507999777793884 + 0.01 * 6.875678539276123
Epoch 180, val loss: 1.0548573732376099
Epoch 190, training loss: 0.8383188247680664 = 0.7695595622062683 + 0.01 * 6.875927448272705
Epoch 190, val loss: 0.9963808655738831
Epoch 200, training loss: 0.7620049118995667 = 0.6932448744773865 + 0.01 * 6.876002788543701
Epoch 200, val loss: 0.9430249929428101
Epoch 210, training loss: 0.6925327181816101 = 0.6237670183181763 + 0.01 * 6.876568794250488
Epoch 210, val loss: 0.8966922163963318
Epoch 220, training loss: 0.6313775181770325 = 0.562606155872345 + 0.01 * 6.877134323120117
Epoch 220, val loss: 0.8580889105796814
Epoch 230, training loss: 0.5784447193145752 = 0.5096700191497803 + 0.01 * 6.877472877502441
Epoch 230, val loss: 0.8268409967422485
Epoch 240, training loss: 0.5325503349304199 = 0.46377426385879517 + 0.01 * 6.8776068687438965
Epoch 240, val loss: 0.8027065992355347
Epoch 250, training loss: 0.4920876622200012 = 0.42331257462501526 + 0.01 * 6.877507209777832
Epoch 250, val loss: 0.7848131656646729
Epoch 260, training loss: 0.4556606411933899 = 0.3868900239467621 + 0.01 * 6.8770623207092285
Epoch 260, val loss: 0.7708905339241028
Epoch 270, training loss: 0.42234575748443604 = 0.35358530282974243 + 0.01 * 6.876045227050781
Epoch 270, val loss: 0.7596821188926697
Epoch 280, training loss: 0.39142683148384094 = 0.32268163561820984 + 0.01 * 6.874520301818848
Epoch 280, val loss: 0.75046306848526
Epoch 290, training loss: 0.3622581958770752 = 0.2935325503349304 + 0.01 * 6.872566223144531
Epoch 290, val loss: 0.742544412612915
Epoch 300, training loss: 0.3341284990310669 = 0.2654321491718292 + 0.01 * 6.869634628295898
Epoch 300, val loss: 0.7360561490058899
Epoch 310, training loss: 0.30684053897857666 = 0.2381710261106491 + 0.01 * 6.866952896118164
Epoch 310, val loss: 0.730469822883606
Epoch 320, training loss: 0.28032809495925903 = 0.21169455349445343 + 0.01 * 6.863354682922363
Epoch 320, val loss: 0.7257264256477356
Epoch 330, training loss: 0.254417359828949 = 0.18582195043563843 + 0.01 * 6.859539985656738
Epoch 330, val loss: 0.7219906449317932
Epoch 340, training loss: 0.22952455282211304 = 0.1609279364347458 + 0.01 * 6.8596625328063965
Epoch 340, val loss: 0.7197760939598083
Epoch 350, training loss: 0.2062162309885025 = 0.13771642744541168 + 0.01 * 6.849980354309082
Epoch 350, val loss: 0.7193976044654846
Epoch 360, training loss: 0.18561844527721405 = 0.11718402057886124 + 0.01 * 6.843442440032959
Epoch 360, val loss: 0.7209888696670532
Epoch 370, training loss: 0.16827571392059326 = 0.09988440573215485 + 0.01 * 6.8391313552856445
Epoch 370, val loss: 0.7251258492469788
Epoch 380, training loss: 0.15385623276233673 = 0.08559510111808777 + 0.01 * 6.826113224029541
Epoch 380, val loss: 0.7314667701721191
Epoch 390, training loss: 0.14201411604881287 = 0.07386090606451035 + 0.01 * 6.815321922302246
Epoch 390, val loss: 0.7393065690994263
Epoch 400, training loss: 0.13239340484142303 = 0.06412018835544586 + 0.01 * 6.827321529388428
Epoch 400, val loss: 0.7480285167694092
Epoch 410, training loss: 0.12387098371982574 = 0.05592386797070503 + 0.01 * 6.794711589813232
Epoch 410, val loss: 0.7580034732818604
Epoch 420, training loss: 0.11690863221883774 = 0.04902748018503189 + 0.01 * 6.788115501403809
Epoch 420, val loss: 0.7686717510223389
Epoch 430, training loss: 0.11109752953052521 = 0.04327336326241493 + 0.01 * 6.782416343688965
Epoch 430, val loss: 0.7795212268829346
Epoch 440, training loss: 0.10615147650241852 = 0.03847737982869148 + 0.01 * 6.7674102783203125
Epoch 440, val loss: 0.7908062934875488
Epoch 450, training loss: 0.10212363302707672 = 0.03445252776145935 + 0.01 * 6.767110824584961
Epoch 450, val loss: 0.8019960522651672
Epoch 460, training loss: 0.09857478737831116 = 0.031036894768476486 + 0.01 * 6.753789901733398
Epoch 460, val loss: 0.8131920695304871
Epoch 470, training loss: 0.09604385495185852 = 0.028109263628721237 + 0.01 * 6.793458938598633
Epoch 470, val loss: 0.8241372108459473
Epoch 480, training loss: 0.0930391326546669 = 0.02558363415300846 + 0.01 * 6.745550155639648
Epoch 480, val loss: 0.8348806500434875
Epoch 490, training loss: 0.09072796255350113 = 0.023386307060718536 + 0.01 * 6.734165668487549
Epoch 490, val loss: 0.8454357981681824
Epoch 500, training loss: 0.08878044039011002 = 0.021462472155690193 + 0.01 * 6.731797218322754
Epoch 500, val loss: 0.8557038903236389
Epoch 510, training loss: 0.08695737272500992 = 0.019768431782722473 + 0.01 * 6.718894004821777
Epoch 510, val loss: 0.8657538294792175
Epoch 520, training loss: 0.08540219068527222 = 0.01826808974146843 + 0.01 * 6.713410377502441
Epoch 520, val loss: 0.8755555152893066
Epoch 530, training loss: 0.08417922258377075 = 0.01693463884294033 + 0.01 * 6.724458694458008
Epoch 530, val loss: 0.8850975036621094
Epoch 540, training loss: 0.08278471231460571 = 0.01574566774070263 + 0.01 * 6.703904151916504
Epoch 540, val loss: 0.8944010138511658
Epoch 550, training loss: 0.08150164783000946 = 0.014681209810078144 + 0.01 * 6.682043552398682
Epoch 550, val loss: 0.9034211039543152
Epoch 560, training loss: 0.08071735501289368 = 0.013724086806178093 + 0.01 * 6.699326515197754
Epoch 560, val loss: 0.9121861457824707
Epoch 570, training loss: 0.07964574545621872 = 0.01286326814442873 + 0.01 * 6.678247451782227
Epoch 570, val loss: 0.9207495450973511
Epoch 580, training loss: 0.07875382900238037 = 0.01208446267992258 + 0.01 * 6.666936874389648
Epoch 580, val loss: 0.9290027022361755
Epoch 590, training loss: 0.07807198911905289 = 0.01137759443372488 + 0.01 * 6.669439315795898
Epoch 590, val loss: 0.9370478391647339
Epoch 600, training loss: 0.07728926837444305 = 0.010734427720308304 + 0.01 * 6.655484199523926
Epoch 600, val loss: 0.944945752620697
Epoch 610, training loss: 0.07664555311203003 = 0.010147466324269772 + 0.01 * 6.649808883666992
Epoch 610, val loss: 0.9526125192642212
Epoch 620, training loss: 0.07607553899288177 = 0.009610585868358612 + 0.01 * 6.646495819091797
Epoch 620, val loss: 0.9599930644035339
Epoch 630, training loss: 0.07549145072698593 = 0.009118442423641682 + 0.01 * 6.637300968170166
Epoch 630, val loss: 0.9672556519508362
Epoch 640, training loss: 0.07484991103410721 = 0.008665507659316063 + 0.01 * 6.6184401512146
Epoch 640, val loss: 0.9743276238441467
Epoch 650, training loss: 0.07484782487154007 = 0.008247937075793743 + 0.01 * 6.659989356994629
Epoch 650, val loss: 0.9812314510345459
Epoch 660, training loss: 0.07395141571760178 = 0.007862335070967674 + 0.01 * 6.608908176422119
Epoch 660, val loss: 0.9879192113876343
Epoch 670, training loss: 0.07369513064622879 = 0.007505515590310097 + 0.01 * 6.618961811065674
Epoch 670, val loss: 0.9943558573722839
Epoch 680, training loss: 0.07341798394918442 = 0.007174464873969555 + 0.01 * 6.624351978302002
Epoch 680, val loss: 1.0007710456848145
Epoch 690, training loss: 0.07288030534982681 = 0.0068676467053592205 + 0.01 * 6.601265907287598
Epoch 690, val loss: 1.0069472789764404
Epoch 700, training loss: 0.07248730212450027 = 0.006582601927220821 + 0.01 * 6.590470314025879
Epoch 700, val loss: 1.01290762424469
Epoch 710, training loss: 0.07237247377634048 = 0.006316805258393288 + 0.01 * 6.605567455291748
Epoch 710, val loss: 1.0187724828720093
Epoch 720, training loss: 0.071980319917202 = 0.00606873631477356 + 0.01 * 6.591158390045166
Epoch 720, val loss: 1.0244992971420288
Epoch 730, training loss: 0.07177995890378952 = 0.005836599040776491 + 0.01 * 6.59433650970459
Epoch 730, val loss: 1.0300055742263794
Epoch 740, training loss: 0.07140117138624191 = 0.0056192572228610516 + 0.01 * 6.578191757202148
Epoch 740, val loss: 1.0354846715927124
Epoch 750, training loss: 0.0712445080280304 = 0.005415134131908417 + 0.01 * 6.582937717437744
Epoch 750, val loss: 1.040793776512146
Epoch 760, training loss: 0.07081291079521179 = 0.005223456770181656 + 0.01 * 6.558946132659912
Epoch 760, val loss: 1.0459797382354736
Epoch 770, training loss: 0.07112601399421692 = 0.005043288227170706 + 0.01 * 6.608272552490234
Epoch 770, val loss: 1.0509681701660156
Epoch 780, training loss: 0.07033303380012512 = 0.0048738401383161545 + 0.01 * 6.545919418334961
Epoch 780, val loss: 1.0559717416763306
Epoch 790, training loss: 0.07030561566352844 = 0.004714099690318108 + 0.01 * 6.559151649475098
Epoch 790, val loss: 1.0606698989868164
Epoch 800, training loss: 0.0699903815984726 = 0.004563475027680397 + 0.01 * 6.542690753936768
Epoch 800, val loss: 1.065497875213623
Epoch 810, training loss: 0.06983021646738052 = 0.004420888144522905 + 0.01 * 6.540932655334473
Epoch 810, val loss: 1.0699853897094727
Epoch 820, training loss: 0.06969398260116577 = 0.004285994451493025 + 0.01 * 6.540799140930176
Epoch 820, val loss: 1.0745314359664917
Epoch 830, training loss: 0.06952771544456482 = 0.004158178810030222 + 0.01 * 6.536954402923584
Epoch 830, val loss: 1.0788938999176025
Epoch 840, training loss: 0.06938408315181732 = 0.004037036560475826 + 0.01 * 6.534704685211182
Epoch 840, val loss: 1.083186149597168
Epoch 850, training loss: 0.06937682628631592 = 0.003921983297914267 + 0.01 * 6.5454840660095215
Epoch 850, val loss: 1.0874255895614624
Epoch 860, training loss: 0.06895412504673004 = 0.003812754061073065 + 0.01 * 6.5141377449035645
Epoch 860, val loss: 1.0915017127990723
Epoch 870, training loss: 0.06899544596672058 = 0.003709041513502598 + 0.01 * 6.5286407470703125
Epoch 870, val loss: 1.0955734252929688
Epoch 880, training loss: 0.06875860691070557 = 0.003610302461311221 + 0.01 * 6.514830589294434
Epoch 880, val loss: 1.0995113849639893
Epoch 890, training loss: 0.06871248781681061 = 0.0035161678679287434 + 0.01 * 6.519632339477539
Epoch 890, val loss: 1.1034163236618042
Epoch 900, training loss: 0.06861267238855362 = 0.0034264051355421543 + 0.01 * 6.518627166748047
Epoch 900, val loss: 1.1071897745132446
Epoch 910, training loss: 0.06838753819465637 = 0.003340919967740774 + 0.01 * 6.504662036895752
Epoch 910, val loss: 1.1109158992767334
Epoch 920, training loss: 0.06834084540605545 = 0.0032593829091638327 + 0.01 * 6.508146286010742
Epoch 920, val loss: 1.1145532131195068
Epoch 930, training loss: 0.06833645701408386 = 0.0031814281828701496 + 0.01 * 6.5155029296875
Epoch 930, val loss: 1.118140459060669
Epoch 940, training loss: 0.06817726045846939 = 0.003106918651610613 + 0.01 * 6.5070343017578125
Epoch 940, val loss: 1.121619462966919
Epoch 950, training loss: 0.06810139119625092 = 0.003035652218386531 + 0.01 * 6.5065741539001465
Epoch 950, val loss: 1.1250572204589844
Epoch 960, training loss: 0.06785313040018082 = 0.0029674945399165154 + 0.01 * 6.4885640144348145
Epoch 960, val loss: 1.1283729076385498
Epoch 970, training loss: 0.06799618154764175 = 0.0029022584203630686 + 0.01 * 6.509392261505127
Epoch 970, val loss: 1.1316769123077393
Epoch 980, training loss: 0.06763128936290741 = 0.0028396383859217167 + 0.01 * 6.479165077209473
Epoch 980, val loss: 1.1349170207977295
Epoch 990, training loss: 0.0675952211022377 = 0.0027795806527137756 + 0.01 * 6.481564044952393
Epoch 990, val loss: 1.1380459070205688
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.8007
Flip ASR: 0.7733/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.027339458465576 = 1.9436007738113403 + 0.01 * 8.373858451843262
Epoch 0, val loss: 1.9410128593444824
Epoch 10, training loss: 2.017066717147827 = 1.9333289861679077 + 0.01 * 8.373771667480469
Epoch 10, val loss: 1.9304101467132568
Epoch 20, training loss: 2.004101037979126 = 1.9203661680221558 + 0.01 * 8.373491287231445
Epoch 20, val loss: 1.9168511629104614
Epoch 30, training loss: 1.9854884147644043 = 1.9017620086669922 + 0.01 * 8.372641563415527
Epoch 30, val loss: 1.897172212600708
Epoch 40, training loss: 1.958012580871582 = 1.8743321895599365 + 0.01 * 8.368036270141602
Epoch 40, val loss: 1.8684666156768799
Epoch 50, training loss: 1.9205124378204346 = 1.8371751308441162 + 0.01 * 8.333736419677734
Epoch 50, val loss: 1.8314756155014038
Epoch 60, training loss: 1.8774902820587158 = 1.7966011762619019 + 0.01 * 8.088913917541504
Epoch 60, val loss: 1.7957993745803833
Epoch 70, training loss: 1.8349761962890625 = 1.7573931217193604 + 0.01 * 7.758302688598633
Epoch 70, val loss: 1.765854835510254
Epoch 80, training loss: 1.7812849283218384 = 1.7067413330078125 + 0.01 * 7.4543585777282715
Epoch 80, val loss: 1.7266749143600464
Epoch 90, training loss: 1.7118690013885498 = 1.6385016441345215 + 0.01 * 7.336737632751465
Epoch 90, val loss: 1.6712232828140259
Epoch 100, training loss: 1.6237167119979858 = 1.5508934259414673 + 0.01 * 7.282326698303223
Epoch 100, val loss: 1.5990618467330933
Epoch 110, training loss: 1.5231107473373413 = 1.4506219625473022 + 0.01 * 7.248882293701172
Epoch 110, val loss: 1.519828200340271
Epoch 120, training loss: 1.4152851104736328 = 1.343208909034729 + 0.01 * 7.207622528076172
Epoch 120, val loss: 1.4382387399673462
Epoch 130, training loss: 1.3029429912567139 = 1.2314918041229248 + 0.01 * 7.145118713378906
Epoch 130, val loss: 1.3543413877487183
Epoch 140, training loss: 1.1881351470947266 = 1.1175528764724731 + 0.01 * 7.058223247528076
Epoch 140, val loss: 1.2689486742019653
Epoch 150, training loss: 1.0752742290496826 = 1.0054959058761597 + 0.01 * 6.977837562561035
Epoch 150, val loss: 1.185365915298462
Epoch 160, training loss: 0.9708512425422668 = 0.9014694094657898 + 0.01 * 6.938185691833496
Epoch 160, val loss: 1.1094536781311035
Epoch 170, training loss: 0.8797846436500549 = 0.8106362223625183 + 0.01 * 6.914844036102295
Epoch 170, val loss: 1.0463387966156006
Epoch 180, training loss: 0.8030462861061096 = 0.7340456247329712 + 0.01 * 6.900065898895264
Epoch 180, val loss: 0.996317982673645
Epoch 190, training loss: 0.7377833127975464 = 0.6688758730888367 + 0.01 * 6.890742301940918
Epoch 190, val loss: 0.9573140740394592
Epoch 200, training loss: 0.6802884936332703 = 0.6114510893821716 + 0.01 * 6.883740425109863
Epoch 200, val loss: 0.9259599447250366
Epoch 210, training loss: 0.6278565526008606 = 0.559078574180603 + 0.01 * 6.877799987792969
Epoch 210, val loss: 0.9002158045768738
Epoch 220, training loss: 0.5792474150657654 = 0.5105253458023071 + 0.01 * 6.872208595275879
Epoch 220, val loss: 0.8796784281730652
Epoch 230, training loss: 0.5341303944587708 = 0.4654346704483032 + 0.01 * 6.869573593139648
Epoch 230, val loss: 0.8642210364341736
Epoch 240, training loss: 0.4921775758266449 = 0.4235624372959137 + 0.01 * 6.861514568328857
Epoch 240, val loss: 0.8537202477455139
Epoch 250, training loss: 0.45315301418304443 = 0.384613960981369 + 0.01 * 6.85390567779541
Epoch 250, val loss: 0.8478271961212158
Epoch 260, training loss: 0.4169047176837921 = 0.34838005900382996 + 0.01 * 6.852466106414795
Epoch 260, val loss: 0.8463834524154663
Epoch 270, training loss: 0.38313475251197815 = 0.31472644209861755 + 0.01 * 6.840831279754639
Epoch 270, val loss: 0.8487581014633179
Epoch 280, training loss: 0.35175561904907227 = 0.2834049463272095 + 0.01 * 6.835066318511963
Epoch 280, val loss: 0.8540369272232056
Epoch 290, training loss: 0.32239991426467896 = 0.2541545331478119 + 0.01 * 6.824539661407471
Epoch 290, val loss: 0.8616804480552673
Epoch 300, training loss: 0.295049250125885 = 0.2267952710390091 + 0.01 * 6.825397491455078
Epoch 300, val loss: 0.8715066313743591
Epoch 310, training loss: 0.2695455849170685 = 0.20143598318099976 + 0.01 * 6.810960292816162
Epoch 310, val loss: 0.8834460377693176
Epoch 320, training loss: 0.24629934132099152 = 0.17830269038677216 + 0.01 * 6.799665451049805
Epoch 320, val loss: 0.8975909352302551
Epoch 330, training loss: 0.22552260756492615 = 0.1575530767440796 + 0.01 * 6.796954154968262
Epoch 330, val loss: 0.9140029549598694
Epoch 340, training loss: 0.20705538988113403 = 0.13919061422348022 + 0.01 * 6.7864766120910645
Epoch 340, val loss: 0.9326727986335754
Epoch 350, training loss: 0.19083014130592346 = 0.12305045127868652 + 0.01 * 6.777968406677246
Epoch 350, val loss: 0.9534899592399597
Epoch 360, training loss: 0.17660880088806152 = 0.10890085995197296 + 0.01 * 6.7707953453063965
Epoch 360, val loss: 0.9761717915534973
Epoch 370, training loss: 0.16415850818157196 = 0.09652001410722733 + 0.01 * 6.763849258422852
Epoch 370, val loss: 1.0003101825714111
Epoch 380, training loss: 0.1533023715019226 = 0.0856916680932045 + 0.01 * 6.761070728302002
Epoch 380, val loss: 1.0254888534545898
Epoch 390, training loss: 0.14374491572380066 = 0.07621034234762192 + 0.01 * 6.753458499908447
Epoch 390, val loss: 1.0513978004455566
Epoch 400, training loss: 0.13534747064113617 = 0.0678984522819519 + 0.01 * 6.74490213394165
Epoch 400, val loss: 1.0777592658996582
Epoch 410, training loss: 0.12815234065055847 = 0.06060861051082611 + 0.01 * 6.754373073577881
Epoch 410, val loss: 1.1043330430984497
Epoch 420, training loss: 0.12164989858865738 = 0.054220713675022125 + 0.01 * 6.742918491363525
Epoch 420, val loss: 1.1308116912841797
Epoch 430, training loss: 0.11595101654529572 = 0.04862378165125847 + 0.01 * 6.732723236083984
Epoch 430, val loss: 1.1570639610290527
Epoch 440, training loss: 0.11101323366165161 = 0.043719563633203506 + 0.01 * 6.729366779327393
Epoch 440, val loss: 1.1828237771987915
Epoch 450, training loss: 0.10664321482181549 = 0.03942307457327843 + 0.01 * 6.7220139503479
Epoch 450, val loss: 1.207856297492981
Epoch 460, training loss: 0.10288311541080475 = 0.03565987944602966 + 0.01 * 6.722323417663574
Epoch 460, val loss: 1.2319972515106201
Epoch 470, training loss: 0.09951320290565491 = 0.032363638281822205 + 0.01 * 6.714956283569336
Epoch 470, val loss: 1.2551708221435547
Epoch 480, training loss: 0.09653544425964355 = 0.029471274465322495 + 0.01 * 6.706417083740234
Epoch 480, val loss: 1.2773287296295166
Epoch 490, training loss: 0.09400593489408493 = 0.026926560327410698 + 0.01 * 6.707937240600586
Epoch 490, val loss: 1.2985166311264038
Epoch 500, training loss: 0.09169257432222366 = 0.024683348834514618 + 0.01 * 6.700922966003418
Epoch 500, val loss: 1.3187123537063599
Epoch 510, training loss: 0.08959099650382996 = 0.022700373083353043 + 0.01 * 6.689062595367432
Epoch 510, val loss: 1.3379744291305542
Epoch 520, training loss: 0.08786655962467194 = 0.020940635353326797 + 0.01 * 6.692593097686768
Epoch 520, val loss: 1.3563593626022339
Epoch 530, training loss: 0.08636998385190964 = 0.019375808537006378 + 0.01 * 6.699417591094971
Epoch 530, val loss: 1.3738986253738403
Epoch 540, training loss: 0.08477261662483215 = 0.017980163916945457 + 0.01 * 6.6792449951171875
Epoch 540, val loss: 1.390586495399475
Epoch 550, training loss: 0.08346888422966003 = 0.016729701310396194 + 0.01 * 6.6739182472229
Epoch 550, val loss: 1.406626582145691
Epoch 560, training loss: 0.08227197825908661 = 0.015605563297867775 + 0.01 * 6.666642189025879
Epoch 560, val loss: 1.4219611883163452
Epoch 570, training loss: 0.08154395967721939 = 0.014591953717172146 + 0.01 * 6.695200443267822
Epoch 570, val loss: 1.4366273880004883
Epoch 580, training loss: 0.08029236644506454 = 0.013676518574357033 + 0.01 * 6.661585330963135
Epoch 580, val loss: 1.4507777690887451
Epoch 590, training loss: 0.0793711394071579 = 0.012846165336668491 + 0.01 * 6.6524977684021
Epoch 590, val loss: 1.4644007682800293
Epoch 600, training loss: 0.07873330265283585 = 0.01209100242704153 + 0.01 * 6.664229869842529
Epoch 600, val loss: 1.4775530099868774
Epoch 610, training loss: 0.07788879424333572 = 0.011402986012399197 + 0.01 * 6.648581027984619
Epoch 610, val loss: 1.4901237487792969
Epoch 620, training loss: 0.07732705771923065 = 0.010774080641567707 + 0.01 * 6.65529727935791
Epoch 620, val loss: 1.5023255348205566
Epoch 630, training loss: 0.07658632099628448 = 0.010199233889579773 + 0.01 * 6.63870906829834
Epoch 630, val loss: 1.514068841934204
Epoch 640, training loss: 0.07596424221992493 = 0.009671150706708431 + 0.01 * 6.629309177398682
Epoch 640, val loss: 1.5254254341125488
Epoch 650, training loss: 0.07560372352600098 = 0.009184828959405422 + 0.01 * 6.641889572143555
Epoch 650, val loss: 1.5363788604736328
Epoch 660, training loss: 0.07492763549089432 = 0.008734619244933128 + 0.01 * 6.619301795959473
Epoch 660, val loss: 1.5471419095993042
Epoch 670, training loss: 0.0745571032166481 = 0.008318347856402397 + 0.01 * 6.623875617980957
Epoch 670, val loss: 1.5575567483901978
Epoch 680, training loss: 0.07400943338871002 = 0.0079334806650877 + 0.01 * 6.607594966888428
Epoch 680, val loss: 1.5675452947616577
Epoch 690, training loss: 0.07363449037075043 = 0.007577337324619293 + 0.01 * 6.605715751647949
Epoch 690, val loss: 1.5772325992584229
Epoch 700, training loss: 0.07323901355266571 = 0.00724703399464488 + 0.01 * 6.599198341369629
Epoch 700, val loss: 1.5865309238433838
Epoch 710, training loss: 0.07291003316640854 = 0.006939775310456753 + 0.01 * 6.5970258712768555
Epoch 710, val loss: 1.5956333875656128
Epoch 720, training loss: 0.07265390455722809 = 0.006653360556811094 + 0.01 * 6.600054740905762
Epoch 720, val loss: 1.6044747829437256
Epoch 730, training loss: 0.07231806963682175 = 0.006386114284396172 + 0.01 * 6.59319543838501
Epoch 730, val loss: 1.613073468208313
Epoch 740, training loss: 0.07212214916944504 = 0.00613615196198225 + 0.01 * 6.598599910736084
Epoch 740, val loss: 1.621362566947937
Epoch 750, training loss: 0.07173233479261398 = 0.0059022982604801655 + 0.01 * 6.583003997802734
Epoch 750, val loss: 1.6294888257980347
Epoch 760, training loss: 0.07140983641147614 = 0.005682908929884434 + 0.01 * 6.57269287109375
Epoch 760, val loss: 1.637323021888733
Epoch 770, training loss: 0.071255624294281 = 0.005476987920701504 + 0.01 * 6.577863693237305
Epoch 770, val loss: 1.6449896097183228
Epoch 780, training loss: 0.07095043361186981 = 0.00528342230245471 + 0.01 * 6.5667009353637695
Epoch 780, val loss: 1.6524431705474854
Epoch 790, training loss: 0.07093319296836853 = 0.0051012490876019 + 0.01 * 6.583194255828857
Epoch 790, val loss: 1.6596753597259521
Epoch 800, training loss: 0.07055883854627609 = 0.004929702263325453 + 0.01 * 6.56291389465332
Epoch 800, val loss: 1.6667425632476807
Epoch 810, training loss: 0.0706392377614975 = 0.004767716396600008 + 0.01 * 6.587152004241943
Epoch 810, val loss: 1.6735615730285645
Epoch 820, training loss: 0.07023759931325912 = 0.004614931531250477 + 0.01 * 6.5622663497924805
Epoch 820, val loss: 1.680314064025879
Epoch 830, training loss: 0.07001876085996628 = 0.004470534157007933 + 0.01 * 6.5548224449157715
Epoch 830, val loss: 1.6867516040802002
Epoch 840, training loss: 0.0697847232222557 = 0.004333868157118559 + 0.01 * 6.545085430145264
Epoch 840, val loss: 1.6930949687957764
Epoch 850, training loss: 0.06991369277238846 = 0.004204291384667158 + 0.01 * 6.5709404945373535
Epoch 850, val loss: 1.6993069648742676
Epoch 860, training loss: 0.0694672241806984 = 0.0040814527310431 + 0.01 * 6.538577556610107
Epoch 860, val loss: 1.7052544355392456
Epoch 870, training loss: 0.06942715495824814 = 0.003964999690651894 + 0.01 * 6.546216011047363
Epoch 870, val loss: 1.7111836671829224
Epoch 880, training loss: 0.06912507116794586 = 0.0038542412221431732 + 0.01 * 6.527082920074463
Epoch 880, val loss: 1.7168506383895874
Epoch 890, training loss: 0.0691717192530632 = 0.0037487626541405916 + 0.01 * 6.542296409606934
Epoch 890, val loss: 1.7225027084350586
Epoch 900, training loss: 0.06926219165325165 = 0.0036485709715634584 + 0.01 * 6.561362266540527
Epoch 900, val loss: 1.7279860973358154
Epoch 910, training loss: 0.06876534223556519 = 0.0035532312467694283 + 0.01 * 6.52121114730835
Epoch 910, val loss: 1.733272671699524
Epoch 920, training loss: 0.06873549520969391 = 0.0034623336978256702 + 0.01 * 6.527316093444824
Epoch 920, val loss: 1.7384417057037354
Epoch 930, training loss: 0.06853247433900833 = 0.003375705797225237 + 0.01 * 6.515676975250244
Epoch 930, val loss: 1.7435458898544312
Epoch 940, training loss: 0.0685022845864296 = 0.0032929221633821726 + 0.01 * 6.520936012268066
Epoch 940, val loss: 1.7485682964324951
Epoch 950, training loss: 0.06829163432121277 = 0.003213863354176283 + 0.01 * 6.507777214050293
Epoch 950, val loss: 1.7533754110336304
Epoch 960, training loss: 0.06840095669031143 = 0.0031382327433675528 + 0.01 * 6.526272773742676
Epoch 960, val loss: 1.758118748664856
Epoch 970, training loss: 0.06809744238853455 = 0.003066018922254443 + 0.01 * 6.503142833709717
Epoch 970, val loss: 1.7627737522125244
Epoch 980, training loss: 0.06791634112596512 = 0.0029969224706292152 + 0.01 * 6.491942405700684
Epoch 980, val loss: 1.7672635316848755
Epoch 990, training loss: 0.06811311095952988 = 0.0029307573568075895 + 0.01 * 6.518235206604004
Epoch 990, val loss: 1.7716740369796753
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.7970
Flip ASR: 0.7556/225 nodes
The final ASR:0.75277, 0.06525, Accuracy:0.79877, 0.00630
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11614])
remove edge: torch.Size([2, 9482])
updated graph: torch.Size([2, 10540])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
The final ASR:0.97540, 0.00174, Accuracy:0.82963, 0.00524
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0382320880889893 = 1.954493522644043 + 0.01 * 8.373855590820312
Epoch 0, val loss: 1.9441245794296265
Epoch 10, training loss: 2.0277199745178223 = 1.9439822435379028 + 0.01 * 8.373773574829102
Epoch 10, val loss: 1.9340598583221436
Epoch 20, training loss: 2.01483416557312 = 1.931099534034729 + 0.01 * 8.373458862304688
Epoch 20, val loss: 1.921356439590454
Epoch 30, training loss: 1.9967749118804932 = 1.913051962852478 + 0.01 * 8.372293472290039
Epoch 30, val loss: 1.9032018184661865
Epoch 40, training loss: 1.970204472541809 = 1.8865537643432617 + 0.01 * 8.365076065063477
Epoch 40, val loss: 1.8765631914138794
Epoch 50, training loss: 1.932672381401062 = 1.8494932651519775 + 0.01 * 8.317911148071289
Epoch 50, val loss: 1.8408344984054565
Epoch 60, training loss: 1.8872534036636353 = 1.8066397905349731 + 0.01 * 8.061360359191895
Epoch 60, val loss: 1.804399847984314
Epoch 70, training loss: 1.8471169471740723 = 1.7679426670074463 + 0.01 * 7.917434215545654
Epoch 70, val loss: 1.777248740196228
Epoch 80, training loss: 1.799111008644104 = 1.7231844663619995 + 0.01 * 7.592653751373291
Epoch 80, val loss: 1.7431516647338867
Epoch 90, training loss: 1.7338253259658813 = 1.6613746881484985 + 0.01 * 7.245068550109863
Epoch 90, val loss: 1.690826177597046
Epoch 100, training loss: 1.649786114692688 = 1.5790396928787231 + 0.01 * 7.0746378898620605
Epoch 100, val loss: 1.6200752258300781
Epoch 110, training loss: 1.5521397590637207 = 1.4820624589920044 + 0.01 * 7.00773286819458
Epoch 110, val loss: 1.5404127836227417
Epoch 120, training loss: 1.4497674703598022 = 1.3800936937332153 + 0.01 * 6.9673752784729
Epoch 120, val loss: 1.4600149393081665
Epoch 130, training loss: 1.3466650247573853 = 1.2772248983383179 + 0.01 * 6.944012641906738
Epoch 130, val loss: 1.380180835723877
Epoch 140, training loss: 1.242356300354004 = 1.1730819940567017 + 0.01 * 6.927430152893066
Epoch 140, val loss: 1.2997452020645142
Epoch 150, training loss: 1.1383360624313354 = 1.0691810846328735 + 0.01 * 6.915495872497559
Epoch 150, val loss: 1.2187175750732422
Epoch 160, training loss: 1.0373072624206543 = 0.9681941866874695 + 0.01 * 6.911304950714111
Epoch 160, val loss: 1.140568494796753
Epoch 170, training loss: 0.9410394430160522 = 0.8719547390937805 + 0.01 * 6.908472537994385
Epoch 170, val loss: 1.0672224760055542
Epoch 180, training loss: 0.8505468368530273 = 0.7814793586730957 + 0.01 * 6.906750679016113
Epoch 180, val loss: 0.9997463822364807
Epoch 190, training loss: 0.7672370672225952 = 0.6981717944145203 + 0.01 * 6.906525611877441
Epoch 190, val loss: 0.9393817186355591
Epoch 200, training loss: 0.6929694414138794 = 0.6239092946052551 + 0.01 * 6.906015396118164
Epoch 200, val loss: 0.8874076008796692
Epoch 210, training loss: 0.6285471320152283 = 0.5594791173934937 + 0.01 * 6.906803607940674
Epoch 210, val loss: 0.8446789383888245
Epoch 220, training loss: 0.5729398727416992 = 0.5038911700248718 + 0.01 * 6.9048686027526855
Epoch 220, val loss: 0.8107825517654419
Epoch 230, training loss: 0.5243671536445618 = 0.4553423821926117 + 0.01 * 6.90247917175293
Epoch 230, val loss: 0.7846516370773315
Epoch 240, training loss: 0.48121950030326843 = 0.41222652792930603 + 0.01 * 6.899296283721924
Epoch 240, val loss: 0.7647743225097656
Epoch 250, training loss: 0.44243425130844116 = 0.37343254685401917 + 0.01 * 6.900171279907227
Epoch 250, val loss: 0.7496407628059387
Epoch 260, training loss: 0.4070892930030823 = 0.33817681670188904 + 0.01 * 6.89124870300293
Epoch 260, val loss: 0.737949788570404
Epoch 270, training loss: 0.3745516836643219 = 0.3056935667991638 + 0.01 * 6.885811805725098
Epoch 270, val loss: 0.7289661169052124
Epoch 280, training loss: 0.34395831823349 = 0.27514180541038513 + 0.01 * 6.881651878356934
Epoch 280, val loss: 0.7220700979232788
Epoch 290, training loss: 0.3145692050457001 = 0.24582596123218536 + 0.01 * 6.874324321746826
Epoch 290, val loss: 0.7166965007781982
Epoch 300, training loss: 0.28622785210609436 = 0.2175564020872116 + 0.01 * 6.867146015167236
Epoch 300, val loss: 0.7125616669654846
Epoch 310, training loss: 0.2594356834888458 = 0.1908002346754074 + 0.01 * 6.863544464111328
Epoch 310, val loss: 0.7100553512573242
Epoch 320, training loss: 0.23487643897533417 = 0.1663517951965332 + 0.01 * 6.852464199066162
Epoch 320, val loss: 0.7097024917602539
Epoch 330, training loss: 0.2131112664937973 = 0.14470548927783966 + 0.01 * 6.840577602386475
Epoch 330, val loss: 0.7119353413581848
Epoch 340, training loss: 0.1944265514612198 = 0.12594301998615265 + 0.01 * 6.848353385925293
Epoch 340, val loss: 0.7168202996253967
Epoch 350, training loss: 0.17815881967544556 = 0.10986871272325516 + 0.01 * 6.829011917114258
Epoch 350, val loss: 0.724065899848938
Epoch 360, training loss: 0.16435422003269196 = 0.09615673124790192 + 0.01 * 6.819749355316162
Epoch 360, val loss: 0.7332379221916199
Epoch 370, training loss: 0.15262815356254578 = 0.08448286354541779 + 0.01 * 6.814528942108154
Epoch 370, val loss: 0.7439645528793335
Epoch 380, training loss: 0.14251959323883057 = 0.07453258335590363 + 0.01 * 6.798701286315918
Epoch 380, val loss: 0.7557016611099243
Epoch 390, training loss: 0.13384000957012177 = 0.06602398306131363 + 0.01 * 6.78160285949707
Epoch 390, val loss: 0.7681766748428345
Epoch 400, training loss: 0.12716937065124512 = 0.05873372033238411 + 0.01 * 6.843564987182617
Epoch 400, val loss: 0.7810459136962891
Epoch 410, training loss: 0.12025371193885803 = 0.05248650535941124 + 0.01 * 6.7767205238342285
Epoch 410, val loss: 0.7940521240234375
Epoch 420, training loss: 0.11475919187068939 = 0.04709520563483238 + 0.01 * 6.766398906707764
Epoch 420, val loss: 0.8070589303970337
Epoch 430, training loss: 0.10991504043340683 = 0.04242325574159622 + 0.01 * 6.749178886413574
Epoch 430, val loss: 0.8199882507324219
Epoch 440, training loss: 0.10586811602115631 = 0.03835827857255936 + 0.01 * 6.750984191894531
Epoch 440, val loss: 0.8327555656433105
Epoch 450, training loss: 0.10222643613815308 = 0.0348159484565258 + 0.01 * 6.741048336029053
Epoch 450, val loss: 0.845160961151123
Epoch 460, training loss: 0.09900631010532379 = 0.031717073172330856 + 0.01 * 6.728923797607422
Epoch 460, val loss: 0.8573697805404663
Epoch 470, training loss: 0.09617321193218231 = 0.02899622917175293 + 0.01 * 6.717698097229004
Epoch 470, val loss: 0.8692158460617065
Epoch 480, training loss: 0.09367921203374863 = 0.026597904041409492 + 0.01 * 6.708130836486816
Epoch 480, val loss: 0.8807780742645264
Epoch 490, training loss: 0.09148485213518143 = 0.02447509951889515 + 0.01 * 6.70097541809082
Epoch 490, val loss: 0.8919804096221924
Epoch 500, training loss: 0.08968674391508102 = 0.0225896704941988 + 0.01 * 6.709707736968994
Epoch 500, val loss: 0.9028604626655579
Epoch 510, training loss: 0.08794433623552322 = 0.020908592268824577 + 0.01 * 6.7035746574401855
Epoch 510, val loss: 0.9134790301322937
Epoch 520, training loss: 0.08622591197490692 = 0.019401108846068382 + 0.01 * 6.682480335235596
Epoch 520, val loss: 0.9237853288650513
Epoch 530, training loss: 0.08482705801725388 = 0.018041228875517845 + 0.01 * 6.678583145141602
Epoch 530, val loss: 0.933868408203125
Epoch 540, training loss: 0.08352501690387726 = 0.016814295202493668 + 0.01 * 6.671072483062744
Epoch 540, val loss: 0.9437320828437805
Epoch 550, training loss: 0.08262395113706589 = 0.01570158265531063 + 0.01 * 6.69223690032959
Epoch 550, val loss: 0.9532917141914368
Epoch 560, training loss: 0.08135970681905746 = 0.014691420830786228 + 0.01 * 6.666828632354736
Epoch 560, val loss: 0.9627088904380798
Epoch 570, training loss: 0.0803997814655304 = 0.013770091347396374 + 0.01 * 6.66296911239624
Epoch 570, val loss: 0.9718615412712097
Epoch 580, training loss: 0.07942812144756317 = 0.012928673066198826 + 0.01 * 6.649945259094238
Epoch 580, val loss: 0.9807973504066467
Epoch 590, training loss: 0.07919833809137344 = 0.012157775461673737 + 0.01 * 6.704056739807129
Epoch 590, val loss: 0.9896206259727478
Epoch 600, training loss: 0.07778237760066986 = 0.011452827602624893 + 0.01 * 6.632955551147461
Epoch 600, val loss: 0.9981608986854553
Epoch 610, training loss: 0.07712281495332718 = 0.010806061327457428 + 0.01 * 6.6316752433776855
Epoch 610, val loss: 1.006455898284912
Epoch 620, training loss: 0.07650695741176605 = 0.010211729444563389 + 0.01 * 6.629522800445557
Epoch 620, val loss: 1.0147324800491333
Epoch 630, training loss: 0.0758621022105217 = 0.009664654731750488 + 0.01 * 6.619745254516602
Epoch 630, val loss: 1.0226682424545288
Epoch 640, training loss: 0.07539981603622437 = 0.009160922840237617 + 0.01 * 6.623889446258545
Epoch 640, val loss: 1.0305391550064087
Epoch 650, training loss: 0.0748249813914299 = 0.008696225471794605 + 0.01 * 6.612875461578369
Epoch 650, val loss: 1.038135051727295
Epoch 660, training loss: 0.07437773048877716 = 0.008266950026154518 + 0.01 * 6.611078262329102
Epoch 660, val loss: 1.0456938743591309
Epoch 670, training loss: 0.07393550127744675 = 0.007869936525821686 + 0.01 * 6.606556415557861
Epoch 670, val loss: 1.052910566329956
Epoch 680, training loss: 0.07352568209171295 = 0.0075018275529146194 + 0.01 * 6.602385520935059
Epoch 680, val loss: 1.0601015090942383
Epoch 690, training loss: 0.07304234802722931 = 0.007160183507949114 + 0.01 * 6.588216304779053
Epoch 690, val loss: 1.0671223402023315
Epoch 700, training loss: 0.0730050653219223 = 0.006842500064522028 + 0.01 * 6.616256237030029
Epoch 700, val loss: 1.0739493370056152
Epoch 710, training loss: 0.07249557971954346 = 0.006547214463353157 + 0.01 * 6.594837188720703
Epoch 710, val loss: 1.0806171894073486
Epoch 720, training loss: 0.07211291790008545 = 0.006272252183407545 + 0.01 * 6.584067344665527
Epoch 720, val loss: 1.0871233940124512
Epoch 730, training loss: 0.07176464796066284 = 0.006015668623149395 + 0.01 * 6.5748982429504395
Epoch 730, val loss: 1.0935553312301636
Epoch 740, training loss: 0.07167518883943558 = 0.005775674246251583 + 0.01 * 6.589951515197754
Epoch 740, val loss: 1.0998104810714722
Epoch 750, training loss: 0.07124312222003937 = 0.005551002454012632 + 0.01 * 6.569211959838867
Epoch 750, val loss: 1.105912208557129
Epoch 760, training loss: 0.07097791880369186 = 0.005340582225471735 + 0.01 * 6.5637335777282715
Epoch 760, val loss: 1.1118810176849365
Epoch 770, training loss: 0.07093726098537445 = 0.005142882000654936 + 0.01 * 6.579438209533691
Epoch 770, val loss: 1.1177542209625244
Epoch 780, training loss: 0.0706823393702507 = 0.004957511555403471 + 0.01 * 6.572482585906982
Epoch 780, val loss: 1.1235780715942383
Epoch 790, training loss: 0.07028777152299881 = 0.004783207084983587 + 0.01 * 6.550457000732422
Epoch 790, val loss: 1.1291013956069946
Epoch 800, training loss: 0.070214182138443 = 0.004619156941771507 + 0.01 * 6.559502124786377
Epoch 800, val loss: 1.1346626281738281
Epoch 810, training loss: 0.06990208476781845 = 0.004464553203433752 + 0.01 * 6.543753623962402
Epoch 810, val loss: 1.1400519609451294
Epoch 820, training loss: 0.0698370635509491 = 0.004318621009588242 + 0.01 * 6.551844120025635
Epoch 820, val loss: 1.1452816724777222
Epoch 830, training loss: 0.06964026391506195 = 0.004180897027254105 + 0.01 * 6.545937538146973
Epoch 830, val loss: 1.1504933834075928
Epoch 840, training loss: 0.06975575536489487 = 0.004050598479807377 + 0.01 * 6.570516109466553
Epoch 840, val loss: 1.1554850339889526
Epoch 850, training loss: 0.0692848339676857 = 0.003927362151443958 + 0.01 * 6.535747051239014
Epoch 850, val loss: 1.160491943359375
Epoch 860, training loss: 0.06905632466077805 = 0.0038104350678622723 + 0.01 * 6.5245890617370605
Epoch 860, val loss: 1.1653889417648315
Epoch 870, training loss: 0.06902574002742767 = 0.0036995350383222103 + 0.01 * 6.532620429992676
Epoch 870, val loss: 1.1701797246932983
Epoch 880, training loss: 0.06893156468868256 = 0.003594453912228346 + 0.01 * 6.533711910247803
Epoch 880, val loss: 1.1748347282409668
Epoch 890, training loss: 0.06868848204612732 = 0.0034945218358188868 + 0.01 * 6.51939582824707
Epoch 890, val loss: 1.1793993711471558
Epoch 900, training loss: 0.06900647282600403 = 0.0033994580153375864 + 0.01 * 6.560701370239258
Epoch 900, val loss: 1.1839139461517334
Epoch 910, training loss: 0.06838378310203552 = 0.003309156047180295 + 0.01 * 6.507462978363037
Epoch 910, val loss: 1.1883735656738281
Epoch 920, training loss: 0.06831514835357666 = 0.0032230359502136707 + 0.01 * 6.50921106338501
Epoch 920, val loss: 1.192754864692688
Epoch 930, training loss: 0.06818924099206924 = 0.0031409261282533407 + 0.01 * 6.504831790924072
Epoch 930, val loss: 1.1970289945602417
Epoch 940, training loss: 0.06817975640296936 = 0.003062585135921836 + 0.01 * 6.511717319488525
Epoch 940, val loss: 1.2012277841567993
Epoch 950, training loss: 0.06812294572591782 = 0.0029878527857363224 + 0.01 * 6.513509273529053
Epoch 950, val loss: 1.2052628993988037
Epoch 960, training loss: 0.06799395382404327 = 0.0029164489824324846 + 0.01 * 6.507750988006592
Epoch 960, val loss: 1.2093552350997925
Epoch 970, training loss: 0.06792108714580536 = 0.0028482016641646624 + 0.01 * 6.507288932800293
Epoch 970, val loss: 1.213285207748413
Epoch 980, training loss: 0.06760765612125397 = 0.0027829536702483892 + 0.01 * 6.482470512390137
Epoch 980, val loss: 1.217139482498169
Epoch 990, training loss: 0.067900650203228 = 0.002720475662499666 + 0.01 * 6.518017292022705
Epoch 990, val loss: 1.2210326194763184
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.5461
Flip ASR: 0.4667/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0337417125701904 = 1.9500032663345337 + 0.01 * 8.373847007751465
Epoch 0, val loss: 1.9496008157730103
Epoch 10, training loss: 2.0223119258880615 = 1.938576102256775 + 0.01 * 8.373592376708984
Epoch 10, val loss: 1.936753511428833
Epoch 20, training loss: 2.0078907012939453 = 1.9241650104522705 + 0.01 * 8.372565269470215
Epoch 20, val loss: 1.9192711114883423
Epoch 30, training loss: 1.9875437021255493 = 1.903849482536316 + 0.01 * 8.36941909790039
Epoch 30, val loss: 1.8937819004058838
Epoch 40, training loss: 1.9592362642288208 = 1.8756273984909058 + 0.01 * 8.36088752746582
Epoch 40, val loss: 1.8595998287200928
Epoch 50, training loss: 1.9220434427261353 = 1.8389049768447876 + 0.01 * 8.313848495483398
Epoch 50, val loss: 1.819085717201233
Epoch 60, training loss: 1.8766337633132935 = 1.7963546514511108 + 0.01 * 8.02790641784668
Epoch 60, val loss: 1.7776432037353516
Epoch 70, training loss: 1.8322304487228394 = 1.7549492120742798 + 0.01 * 7.728118896484375
Epoch 70, val loss: 1.7420307397842407
Epoch 80, training loss: 1.7815065383911133 = 1.7067174911499023 + 0.01 * 7.47891092300415
Epoch 80, val loss: 1.701648473739624
Epoch 90, training loss: 1.7127466201782227 = 1.6390312910079956 + 0.01 * 7.3715362548828125
Epoch 90, val loss: 1.6457487344741821
Epoch 100, training loss: 1.6225130558013916 = 1.5496764183044434 + 0.01 * 7.283667087554932
Epoch 100, val loss: 1.5726498365402222
Epoch 110, training loss: 1.519397258758545 = 1.4473506212234497 + 0.01 * 7.204658031463623
Epoch 110, val loss: 1.491583228111267
Epoch 120, training loss: 1.4174281358718872 = 1.3458333015441895 + 0.01 * 7.159485340118408
Epoch 120, val loss: 1.4166910648345947
Epoch 130, training loss: 1.3242534399032593 = 1.2530603408813477 + 0.01 * 7.119311809539795
Epoch 130, val loss: 1.3537958860397339
Epoch 140, training loss: 1.2403591871261597 = 1.1695212125778198 + 0.01 * 7.083798408508301
Epoch 140, val loss: 1.3007559776306152
Epoch 150, training loss: 1.1620147228240967 = 1.091439962387085 + 0.01 * 7.0574750900268555
Epoch 150, val loss: 1.2519642114639282
Epoch 160, training loss: 1.0849086046218872 = 1.0144859552383423 + 0.01 * 7.042262077331543
Epoch 160, val loss: 1.2032992839813232
Epoch 170, training loss: 1.0076560974121094 = 0.9373362064361572 + 0.01 * 7.031993865966797
Epoch 170, val loss: 1.1535619497299194
Epoch 180, training loss: 0.9315399527549744 = 0.8613220453262329 + 0.01 * 7.021788597106934
Epoch 180, val loss: 1.103571891784668
Epoch 190, training loss: 0.8589547872543335 = 0.7888665795326233 + 0.01 * 7.008819580078125
Epoch 190, val loss: 1.0555163621902466
Epoch 200, training loss: 0.7922716736793518 = 0.7223507761955261 + 0.01 * 6.992089748382568
Epoch 200, val loss: 1.0120632648468018
Epoch 210, training loss: 0.7327123880386353 = 0.6629651188850403 + 0.01 * 6.974725723266602
Epoch 210, val loss: 0.9753434062004089
Epoch 220, training loss: 0.6796811819076538 = 0.6101204752922058 + 0.01 * 6.956070899963379
Epoch 220, val loss: 0.9455716609954834
Epoch 230, training loss: 0.6315183639526367 = 0.5620886087417603 + 0.01 * 6.942976951599121
Epoch 230, val loss: 0.9217088222503662
Epoch 240, training loss: 0.5863406658172607 = 0.5170233249664307 + 0.01 * 6.931735038757324
Epoch 240, val loss: 0.9023883938789368
Epoch 250, training loss: 0.5426896214485168 = 0.4734543561935425 + 0.01 * 6.923525810241699
Epoch 250, val loss: 0.886277437210083
Epoch 260, training loss: 0.4996994733810425 = 0.4305201470851898 + 0.01 * 6.917932510375977
Epoch 260, val loss: 0.8723717331886292
Epoch 270, training loss: 0.45734870433807373 = 0.38821083307266235 + 0.01 * 6.913785457611084
Epoch 270, val loss: 0.8607996106147766
Epoch 280, training loss: 0.41638609766960144 = 0.347293883562088 + 0.01 * 6.90922212600708
Epoch 280, val loss: 0.8529821038246155
Epoch 290, training loss: 0.3778931498527527 = 0.30886194109916687 + 0.01 * 6.903120040893555
Epoch 290, val loss: 0.8494864106178284
Epoch 300, training loss: 0.3426187038421631 = 0.273635596036911 + 0.01 * 6.898312091827393
Epoch 300, val loss: 0.8502484560012817
Epoch 310, training loss: 0.31106749176979065 = 0.24212898313999176 + 0.01 * 6.893850803375244
Epoch 310, val loss: 0.8548844456672668
Epoch 320, training loss: 0.2832888960838318 = 0.21441131830215454 + 0.01 * 6.887756824493408
Epoch 320, val loss: 0.8630874752998352
Epoch 330, training loss: 0.2590208053588867 = 0.19020038843154907 + 0.01 * 6.882041931152344
Epoch 330, val loss: 0.8742198348045349
Epoch 340, training loss: 0.23806841671466827 = 0.1692468523979187 + 0.01 * 6.8821563720703125
Epoch 340, val loss: 0.8875691890716553
Epoch 350, training loss: 0.21989305317401886 = 0.15118789672851562 + 0.01 * 6.870515823364258
Epoch 350, val loss: 0.9029953479766846
Epoch 360, training loss: 0.2042037844657898 = 0.13557302951812744 + 0.01 * 6.863076686859131
Epoch 360, val loss: 0.9197697043418884
Epoch 370, training loss: 0.19052201509475708 = 0.12195181846618652 + 0.01 * 6.857020378112793
Epoch 370, val loss: 0.9375559687614441
Epoch 380, training loss: 0.17848451435565948 = 0.10991499572992325 + 0.01 * 6.85695219039917
Epoch 380, val loss: 0.9558969140052795
Epoch 390, training loss: 0.16780614852905273 = 0.09928488731384277 + 0.01 * 6.85212516784668
Epoch 390, val loss: 0.9747639894485474
Epoch 400, training loss: 0.1582811176776886 = 0.08987786620855331 + 0.01 * 6.840325832366943
Epoch 400, val loss: 0.9941956400871277
Epoch 410, training loss: 0.14981184899806976 = 0.08148115873336792 + 0.01 * 6.83306884765625
Epoch 410, val loss: 1.013577938079834
Epoch 420, training loss: 0.14220067858695984 = 0.07392275333404541 + 0.01 * 6.827792167663574
Epoch 420, val loss: 1.0331448316574097
Epoch 430, training loss: 0.1354454755783081 = 0.067140594124794 + 0.01 * 6.830488681793213
Epoch 430, val loss: 1.0526853799819946
Epoch 440, training loss: 0.12930768728256226 = 0.061034899204969406 + 0.01 * 6.8272786140441895
Epoch 440, val loss: 1.0721015930175781
Epoch 450, training loss: 0.1236865222454071 = 0.055525895208120346 + 0.01 * 6.816062927246094
Epoch 450, val loss: 1.091123342514038
Epoch 460, training loss: 0.11865531653165817 = 0.050553664565086365 + 0.01 * 6.8101654052734375
Epoch 460, val loss: 1.1099584102630615
Epoch 470, training loss: 0.11417803168296814 = 0.04607269540429115 + 0.01 * 6.810533046722412
Epoch 470, val loss: 1.128154993057251
Epoch 480, training loss: 0.11009421944618225 = 0.04206923022866249 + 0.01 * 6.802498817443848
Epoch 480, val loss: 1.1461961269378662
Epoch 490, training loss: 0.10642794519662857 = 0.03847084939479828 + 0.01 * 6.795709609985352
Epoch 490, val loss: 1.1636254787445068
Epoch 500, training loss: 0.10315479338169098 = 0.03523940593004227 + 0.01 * 6.791539192199707
Epoch 500, val loss: 1.180585265159607
Epoch 510, training loss: 0.10023640096187592 = 0.03234763443470001 + 0.01 * 6.788876533508301
Epoch 510, val loss: 1.196774959564209
Epoch 520, training loss: 0.09754842519760132 = 0.029749885201454163 + 0.01 * 6.7798542976379395
Epoch 520, val loss: 1.2125608921051025
Epoch 530, training loss: 0.09520615637302399 = 0.027427366003394127 + 0.01 * 6.777878761291504
Epoch 530, val loss: 1.227941870689392
Epoch 540, training loss: 0.09306256473064423 = 0.025338225066661835 + 0.01 * 6.772434234619141
Epoch 540, val loss: 1.2428758144378662
Epoch 550, training loss: 0.09112918376922607 = 0.02346108853816986 + 0.01 * 6.766809463500977
Epoch 550, val loss: 1.2572739124298096
Epoch 560, training loss: 0.08938989788293839 = 0.021769337356090546 + 0.01 * 6.76205587387085
Epoch 560, val loss: 1.2714087963104248
Epoch 570, training loss: 0.0878608226776123 = 0.020248189568519592 + 0.01 * 6.761263370513916
Epoch 570, val loss: 1.28508722782135
Epoch 580, training loss: 0.08640168607234955 = 0.018878305330872536 + 0.01 * 6.752338409423828
Epoch 580, val loss: 1.2984634637832642
Epoch 590, training loss: 0.08515332639217377 = 0.017640981823205948 + 0.01 * 6.751235008239746
Epoch 590, val loss: 1.311489462852478
Epoch 600, training loss: 0.08398360013961792 = 0.01651894301176071 + 0.01 * 6.746465682983398
Epoch 600, val loss: 1.3242219686508179
Epoch 610, training loss: 0.08284011483192444 = 0.01550243142992258 + 0.01 * 6.733768939971924
Epoch 610, val loss: 1.3364992141723633
Epoch 620, training loss: 0.08205340802669525 = 0.014577021822333336 + 0.01 * 6.74763822555542
Epoch 620, val loss: 1.3484517335891724
Epoch 630, training loss: 0.08090029656887054 = 0.013734129257500172 + 0.01 * 6.716616630554199
Epoch 630, val loss: 1.3601583242416382
Epoch 640, training loss: 0.08033137768507004 = 0.01296513807028532 + 0.01 * 6.736624240875244
Epoch 640, val loss: 1.3715425729751587
Epoch 650, training loss: 0.07933178544044495 = 0.012262252159416676 + 0.01 * 6.706953525543213
Epoch 650, val loss: 1.382559061050415
Epoch 660, training loss: 0.0786367878317833 = 0.011616955511271954 + 0.01 * 6.701983451843262
Epoch 660, val loss: 1.3933111429214478
Epoch 670, training loss: 0.07817830890417099 = 0.011024984531104565 + 0.01 * 6.715332984924316
Epoch 670, val loss: 1.4039617776870728
Epoch 680, training loss: 0.07740794867277145 = 0.010480283759534359 + 0.01 * 6.692766189575195
Epoch 680, val loss: 1.4140925407409668
Epoch 690, training loss: 0.07681375741958618 = 0.009977455250918865 + 0.01 * 6.683630466461182
Epoch 690, val loss: 1.4240211248397827
Epoch 700, training loss: 0.07637936621904373 = 0.009513218887150288 + 0.01 * 6.686614513397217
Epoch 700, val loss: 1.433706521987915
Epoch 710, training loss: 0.07581760734319687 = 0.009083142504096031 + 0.01 * 6.6734466552734375
Epoch 710, val loss: 1.4431625604629517
Epoch 720, training loss: 0.07546635717153549 = 0.008683502674102783 + 0.01 * 6.678285598754883
Epoch 720, val loss: 1.4524226188659668
Epoch 730, training loss: 0.0749867856502533 = 0.008311784826219082 + 0.01 * 6.6675004959106445
Epoch 730, val loss: 1.4613091945648193
Epoch 740, training loss: 0.0745391771197319 = 0.007966441102325916 + 0.01 * 6.657273292541504
Epoch 740, val loss: 1.4700305461883545
Epoch 750, training loss: 0.07427096366882324 = 0.0076417215168476105 + 0.01 * 6.662923812866211
Epoch 750, val loss: 1.4787453413009644
Epoch 760, training loss: 0.07375112175941467 = 0.007335255853831768 + 0.01 * 6.641586780548096
Epoch 760, val loss: 1.4869736433029175
Epoch 770, training loss: 0.07342571020126343 = 0.007052132859826088 + 0.01 * 6.637357711791992
Epoch 770, val loss: 1.4950064420700073
Epoch 780, training loss: 0.07319769263267517 = 0.006785992532968521 + 0.01 * 6.641169548034668
Epoch 780, val loss: 1.5029020309448242
Epoch 790, training loss: 0.07274704426527023 = 0.006536349654197693 + 0.01 * 6.621069431304932
Epoch 790, val loss: 1.510758399963379
Epoch 800, training loss: 0.07259733229875565 = 0.006301469169557095 + 0.01 * 6.629586696624756
Epoch 800, val loss: 1.5184636116027832
Epoch 810, training loss: 0.0724693313241005 = 0.0060806432738900185 + 0.01 * 6.638869285583496
Epoch 810, val loss: 1.5259432792663574
Epoch 820, training loss: 0.07189115881919861 = 0.005872919224202633 + 0.01 * 6.601823806762695
Epoch 820, val loss: 1.533262848854065
Epoch 830, training loss: 0.07220582664012909 = 0.005676810629665852 + 0.01 * 6.652901649475098
Epoch 830, val loss: 1.5404144525527954
Epoch 840, training loss: 0.07153143733739853 = 0.005491009913384914 + 0.01 * 6.6040425300598145
Epoch 840, val loss: 1.5473078489303589
Epoch 850, training loss: 0.07128774374723434 = 0.005315593909472227 + 0.01 * 6.597215175628662
Epoch 850, val loss: 1.554070234298706
Epoch 860, training loss: 0.07098035514354706 = 0.005149222444742918 + 0.01 * 6.583113670349121
Epoch 860, val loss: 1.5607775449752808
Epoch 870, training loss: 0.07106740027666092 = 0.0049920896999537945 + 0.01 * 6.6075310707092285
Epoch 870, val loss: 1.5673670768737793
Epoch 880, training loss: 0.07067910581827164 = 0.004840843379497528 + 0.01 * 6.583826541900635
Epoch 880, val loss: 1.5737581253051758
Epoch 890, training loss: 0.07041692733764648 = 0.004694448318332434 + 0.01 * 6.572248458862305
Epoch 890, val loss: 1.5800057649612427
Epoch 900, training loss: 0.070309579372406 = 0.004559854045510292 + 0.01 * 6.574973106384277
Epoch 900, val loss: 1.5860849618911743
Epoch 910, training loss: 0.07009700685739517 = 0.00443235831335187 + 0.01 * 6.566465377807617
Epoch 910, val loss: 1.592298984527588
Epoch 920, training loss: 0.07000581175088882 = 0.004311153199523687 + 0.01 * 6.5694661140441895
Epoch 920, val loss: 1.598183274269104
Epoch 930, training loss: 0.06975455582141876 = 0.004195760935544968 + 0.01 * 6.555879592895508
Epoch 930, val loss: 1.6039700508117676
Epoch 940, training loss: 0.06961112469434738 = 0.004085905849933624 + 0.01 * 6.552521705627441
Epoch 940, val loss: 1.6096655130386353
Epoch 950, training loss: 0.06949068605899811 = 0.0039811632595956326 + 0.01 * 6.550952434539795
Epoch 950, val loss: 1.615261435508728
Epoch 960, training loss: 0.06948710978031158 = 0.0038812225684523582 + 0.01 * 6.560588836669922
Epoch 960, val loss: 1.620742917060852
Epoch 970, training loss: 0.06926421821117401 = 0.003785695880651474 + 0.01 * 6.547852039337158
Epoch 970, val loss: 1.6261439323425293
Epoch 980, training loss: 0.06907854229211807 = 0.0036944940220564604 + 0.01 * 6.53840446472168
Epoch 980, val loss: 1.6314144134521484
Epoch 990, training loss: 0.06921989470720291 = 0.003607176709920168 + 0.01 * 6.561272144317627
Epoch 990, val loss: 1.6366664171218872
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7815
Overall ASR: 0.7269
Flip ASR: 0.7022/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0450599193573 = 1.9613208770751953 + 0.01 * 8.373907089233398
Epoch 0, val loss: 1.9630639553070068
Epoch 10, training loss: 2.0339367389678955 = 1.9501981735229492 + 0.01 * 8.373851776123047
Epoch 10, val loss: 1.9518228769302368
Epoch 20, training loss: 2.0206527709960938 = 1.9369159936904907 + 0.01 * 8.373687744140625
Epoch 20, val loss: 1.9381012916564941
Epoch 30, training loss: 2.002304792404175 = 1.9185726642608643 + 0.01 * 8.373212814331055
Epoch 30, val loss: 1.9192360639572144
Epoch 40, training loss: 1.9754638671875 = 1.8917553424835205 + 0.01 * 8.370851516723633
Epoch 40, val loss: 1.8921723365783691
Epoch 50, training loss: 1.9373799562454224 = 1.8538620471954346 + 0.01 * 8.351791381835938
Epoch 50, val loss: 1.8555587530136108
Epoch 60, training loss: 1.8915315866470337 = 1.8092255592346191 + 0.01 * 8.230600357055664
Epoch 60, val loss: 1.816499948501587
Epoch 70, training loss: 1.8508336544036865 = 1.770420789718628 + 0.01 * 8.041292190551758
Epoch 70, val loss: 1.7854114770889282
Epoch 80, training loss: 1.807222604751587 = 1.7285547256469727 + 0.01 * 7.866792678833008
Epoch 80, val loss: 1.7469291687011719
Epoch 90, training loss: 1.7461296319961548 = 1.6704862117767334 + 0.01 * 7.5643439292907715
Epoch 90, val loss: 1.6950581073760986
Epoch 100, training loss: 1.6646919250488281 = 1.592271089553833 + 0.01 * 7.242089748382568
Epoch 100, val loss: 1.6288737058639526
Epoch 110, training loss: 1.5684747695922852 = 1.4971994161605835 + 0.01 * 7.127529621124268
Epoch 110, val loss: 1.5497366189956665
Epoch 120, training loss: 1.4686436653137207 = 1.3977301120758057 + 0.01 * 7.0913496017456055
Epoch 120, val loss: 1.4678003787994385
Epoch 130, training loss: 1.370847463607788 = 1.3002136945724487 + 0.01 * 7.063375473022461
Epoch 130, val loss: 1.3888577222824097
Epoch 140, training loss: 1.2734904289245605 = 1.2029798030853271 + 0.01 * 7.0510640144348145
Epoch 140, val loss: 1.3124433755874634
Epoch 150, training loss: 1.173824429512024 = 1.1034488677978516 + 0.01 * 7.037550926208496
Epoch 150, val loss: 1.2357569932937622
Epoch 160, training loss: 1.070828914642334 = 1.0006271600723267 + 0.01 * 7.020171642303467
Epoch 160, val loss: 1.1572136878967285
Epoch 170, training loss: 0.965874195098877 = 0.8958472013473511 + 0.01 * 7.002699375152588
Epoch 170, val loss: 1.0775997638702393
Epoch 180, training loss: 0.8632401823997498 = 0.7933359146118164 + 0.01 * 6.990428924560547
Epoch 180, val loss: 1.000473976135254
Epoch 190, training loss: 0.7682825326919556 = 0.6984702348709106 + 0.01 * 6.981227397918701
Epoch 190, val loss: 0.9303779602050781
Epoch 200, training loss: 0.6849154829978943 = 0.6151630878448486 + 0.01 * 6.975241184234619
Epoch 200, val loss: 0.8713752627372742
Epoch 210, training loss: 0.614317774772644 = 0.5445932149887085 + 0.01 * 6.9724531173706055
Epoch 210, val loss: 0.8254345655441284
Epoch 220, training loss: 0.5551362633705139 = 0.4854823648929596 + 0.01 * 6.965388774871826
Epoch 220, val loss: 0.7921102046966553
Epoch 230, training loss: 0.5049684643745422 = 0.43536531925201416 + 0.01 * 6.960312366485596
Epoch 230, val loss: 0.7688588500022888
Epoch 240, training loss: 0.46146073937416077 = 0.3919246196746826 + 0.01 * 6.953611373901367
Epoch 240, val loss: 0.7525967955589294
Epoch 250, training loss: 0.42285555601119995 = 0.35338348150253296 + 0.01 * 6.947206974029541
Epoch 250, val loss: 0.7410170435905457
Epoch 260, training loss: 0.3880850672721863 = 0.3185763359069824 + 0.01 * 6.950872421264648
Epoch 260, val loss: 0.7326289415359497
Epoch 270, training loss: 0.35602355003356934 = 0.28672683238983154 + 0.01 * 6.929672718048096
Epoch 270, val loss: 0.7267143130302429
Epoch 280, training loss: 0.3264034390449524 = 0.25717929005622864 + 0.01 * 6.922413349151611
Epoch 280, val loss: 0.722761332988739
Epoch 290, training loss: 0.2985989451408386 = 0.22946837544441223 + 0.01 * 6.913055419921875
Epoch 290, val loss: 0.7206389904022217
Epoch 300, training loss: 0.27256718277931213 = 0.20354929566383362 + 0.01 * 6.90178918838501
Epoch 300, val loss: 0.7204763889312744
Epoch 310, training loss: 0.24870242178440094 = 0.17970801889896393 + 0.01 * 6.899440765380859
Epoch 310, val loss: 0.7224140167236328
Epoch 320, training loss: 0.22728927433490753 = 0.15832823514938354 + 0.01 * 6.896103858947754
Epoch 320, val loss: 0.7266513109207153
Epoch 330, training loss: 0.20827344059944153 = 0.13950900733470917 + 0.01 * 6.876442909240723
Epoch 330, val loss: 0.7331838011741638
Epoch 340, training loss: 0.19182991981506348 = 0.12312093377113342 + 0.01 * 6.870898723602295
Epoch 340, val loss: 0.7416620850563049
Epoch 350, training loss: 0.17763292789459229 = 0.10896170884370804 + 0.01 * 6.867121696472168
Epoch 350, val loss: 0.7517590522766113
Epoch 360, training loss: 0.16536304354667664 = 0.09675002843141556 + 0.01 * 6.861300945281982
Epoch 360, val loss: 0.7631828188896179
Epoch 370, training loss: 0.15470801293849945 = 0.08615925908088684 + 0.01 * 6.854875564575195
Epoch 370, val loss: 0.7755051255226135
Epoch 380, training loss: 0.14538699388504028 = 0.07692679762840271 + 0.01 * 6.846019268035889
Epoch 380, val loss: 0.7884857654571533
Epoch 390, training loss: 0.13737043738365173 = 0.06885973364114761 + 0.01 * 6.851071357727051
Epoch 390, val loss: 0.8017452955245972
Epoch 400, training loss: 0.1302076280117035 = 0.061809394508600235 + 0.01 * 6.839824199676514
Epoch 400, val loss: 0.8151336312294006
Epoch 410, training loss: 0.1239430159330368 = 0.0556376613676548 + 0.01 * 6.830535888671875
Epoch 410, val loss: 0.8285501599311829
Epoch 420, training loss: 0.11849017441272736 = 0.0502273328602314 + 0.01 * 6.826284408569336
Epoch 420, val loss: 0.8418662548065186
Epoch 430, training loss: 0.11377613991498947 = 0.045481644570827484 + 0.01 * 6.829449653625488
Epoch 430, val loss: 0.8549813628196716
Epoch 440, training loss: 0.10944099724292755 = 0.04131270572543144 + 0.01 * 6.81282901763916
Epoch 440, val loss: 0.8677805662155151
Epoch 450, training loss: 0.10569079220294952 = 0.03763944283127785 + 0.01 * 6.8051347732543945
Epoch 450, val loss: 0.8802767992019653
Epoch 460, training loss: 0.10237766802310944 = 0.0343942828476429 + 0.01 * 6.798338413238525
Epoch 460, val loss: 0.8923999071121216
Epoch 470, training loss: 0.09941015392541885 = 0.03152035176753998 + 0.01 * 6.788980484008789
Epoch 470, val loss: 0.9041990637779236
Epoch 480, training loss: 0.09692104160785675 = 0.028966696932911873 + 0.01 * 6.795434951782227
Epoch 480, val loss: 0.9156345725059509
Epoch 490, training loss: 0.09455154091119766 = 0.02669505774974823 + 0.01 * 6.785648822784424
Epoch 490, val loss: 0.9266769289970398
Epoch 500, training loss: 0.09239471703767776 = 0.024667052552103996 + 0.01 * 6.772766590118408
Epoch 500, val loss: 0.9373660087585449
Epoch 510, training loss: 0.0904647707939148 = 0.02284974977374077 + 0.01 * 6.761502265930176
Epoch 510, val loss: 0.9476545453071594
Epoch 520, training loss: 0.08877544850111008 = 0.021217575296759605 + 0.01 * 6.755787372589111
Epoch 520, val loss: 0.9575999975204468
Epoch 530, training loss: 0.08723102509975433 = 0.0197479035705328 + 0.01 * 6.748312473297119
Epoch 530, val loss: 0.9672433137893677
Epoch 540, training loss: 0.08587248623371124 = 0.01842127926647663 + 0.01 * 6.745121002197266
Epoch 540, val loss: 0.9765336513519287
Epoch 550, training loss: 0.08471324294805527 = 0.017221122980117798 + 0.01 * 6.749212265014648
Epoch 550, val loss: 0.9855602979660034
Epoch 560, training loss: 0.08346769213676453 = 0.01613258384168148 + 0.01 * 6.733510494232178
Epoch 560, val loss: 0.9942533373832703
Epoch 570, training loss: 0.08242767304182053 = 0.015143165364861488 + 0.01 * 6.728451251983643
Epoch 570, val loss: 1.0026733875274658
Epoch 580, training loss: 0.08134599030017853 = 0.014242000877857208 + 0.01 * 6.710399150848389
Epoch 580, val loss: 1.0108506679534912
Epoch 590, training loss: 0.08060736209154129 = 0.01341861579567194 + 0.01 * 6.718874931335449
Epoch 590, val loss: 1.0187432765960693
Epoch 600, training loss: 0.07977248728275299 = 0.012665648013353348 + 0.01 * 6.710684776306152
Epoch 600, val loss: 1.0264439582824707
Epoch 610, training loss: 0.07900291681289673 = 0.011975612491369247 + 0.01 * 6.702730655670166
Epoch 610, val loss: 1.0338327884674072
Epoch 620, training loss: 0.0782703310251236 = 0.011341540142893791 + 0.01 * 6.6928791999816895
Epoch 620, val loss: 1.0410178899765015
Epoch 630, training loss: 0.0776868388056755 = 0.010757635347545147 + 0.01 * 6.692920684814453
Epoch 630, val loss: 1.0480060577392578
Epoch 640, training loss: 0.07698414474725723 = 0.010219007730484009 + 0.01 * 6.676514148712158
Epoch 640, val loss: 1.0547775030136108
Epoch 650, training loss: 0.07658515870571136 = 0.009721143171191216 + 0.01 * 6.6864013671875
Epoch 650, val loss: 1.0613347291946411
Epoch 660, training loss: 0.07603995501995087 = 0.009260458871722221 + 0.01 * 6.67794942855835
Epoch 660, val loss: 1.0677419900894165
Epoch 670, training loss: 0.07545897364616394 = 0.008832958526909351 + 0.01 * 6.662601470947266
Epoch 670, val loss: 1.0739283561706543
Epoch 680, training loss: 0.07506244629621506 = 0.008436101488769054 + 0.01 * 6.66263484954834
Epoch 680, val loss: 1.0799323320388794
Epoch 690, training loss: 0.07463321089744568 = 0.008066996932029724 + 0.01 * 6.65662145614624
Epoch 690, val loss: 1.0858334302902222
Epoch 700, training loss: 0.07433044910430908 = 0.007723262533545494 + 0.01 * 6.66071891784668
Epoch 700, val loss: 1.091460108757019
Epoch 710, training loss: 0.07379145175218582 = 0.007402537856251001 + 0.01 * 6.638891696929932
Epoch 710, val loss: 1.097005844116211
Epoch 720, training loss: 0.07368531078100204 = 0.0071025690995156765 + 0.01 * 6.658274173736572
Epoch 720, val loss: 1.1023283004760742
Epoch 730, training loss: 0.07312797009944916 = 0.006822093389928341 + 0.01 * 6.630587577819824
Epoch 730, val loss: 1.1075655221939087
Epoch 740, training loss: 0.07287970930337906 = 0.006558968219906092 + 0.01 * 6.632074356079102
Epoch 740, val loss: 1.1126182079315186
Epoch 750, training loss: 0.07284706830978394 = 0.00631217285990715 + 0.01 * 6.653489589691162
Epoch 750, val loss: 1.1175354719161987
Epoch 760, training loss: 0.072174072265625 = 0.006080740597099066 + 0.01 * 6.609333515167236
Epoch 760, val loss: 1.122367024421692
Epoch 770, training loss: 0.0720008909702301 = 0.005862739402800798 + 0.01 * 6.613814830780029
Epoch 770, val loss: 1.1270521879196167
Epoch 780, training loss: 0.0716991275548935 = 0.00565730407834053 + 0.01 * 6.604183197021484
Epoch 780, val loss: 1.1315970420837402
Epoch 790, training loss: 0.07159698754549026 = 0.005463519133627415 + 0.01 * 6.613347053527832
Epoch 790, val loss: 1.1360187530517578
Epoch 800, training loss: 0.07131151109933853 = 0.005280758254230022 + 0.01 * 6.603075981140137
Epoch 800, val loss: 1.1403075456619263
Epoch 810, training loss: 0.07104282826185226 = 0.005108259152621031 + 0.01 * 6.593456745147705
Epoch 810, val loss: 1.144476056098938
Epoch 820, training loss: 0.07110811024904251 = 0.004945024382323027 + 0.01 * 6.616308689117432
Epoch 820, val loss: 1.1485778093338013
Epoch 830, training loss: 0.07065273821353912 = 0.00479062320664525 + 0.01 * 6.586211204528809
Epoch 830, val loss: 1.152546763420105
Epoch 840, training loss: 0.07048320025205612 = 0.004644270520657301 + 0.01 * 6.583893299102783
Epoch 840, val loss: 1.156432032585144
Epoch 850, training loss: 0.07038360834121704 = 0.0045052808709442616 + 0.01 * 6.587832927703857
Epoch 850, val loss: 1.1602410078048706
Epoch 860, training loss: 0.07006832212209702 = 0.004373321775346994 + 0.01 * 6.56950044631958
Epoch 860, val loss: 1.163934350013733
Epoch 870, training loss: 0.0701942965388298 = 0.004247716628015041 + 0.01 * 6.594658374786377
Epoch 870, val loss: 1.167536973953247
Epoch 880, training loss: 0.0699075236916542 = 0.0041283960454165936 + 0.01 * 6.5779128074646
Epoch 880, val loss: 1.1710478067398071
Epoch 890, training loss: 0.06961141526699066 = 0.004014838486909866 + 0.01 * 6.559657096862793
Epoch 890, val loss: 1.1745283603668213
Epoch 900, training loss: 0.06963023543357849 = 0.00390672730281949 + 0.01 * 6.572350978851318
Epoch 900, val loss: 1.1777660846710205
Epoch 910, training loss: 0.06956872344017029 = 0.0038033360615372658 + 0.01 * 6.576539039611816
Epoch 910, val loss: 1.1811269521713257
Epoch 920, training loss: 0.06924139708280563 = 0.0037047299556434155 + 0.01 * 6.5536675453186035
Epoch 920, val loss: 1.184264063835144
Epoch 930, training loss: 0.06907415390014648 = 0.0036103769671171904 + 0.01 * 6.546377182006836
Epoch 930, val loss: 1.1874336004257202
Epoch 940, training loss: 0.06904644519090652 = 0.0035201485734432936 + 0.01 * 6.552630424499512
Epoch 940, val loss: 1.190381407737732
Epoch 950, training loss: 0.06903927028179169 = 0.0034337814431637526 + 0.01 * 6.560549259185791
Epoch 950, val loss: 1.1934462785720825
Epoch 960, training loss: 0.06875751912593842 = 0.0033514590468257666 + 0.01 * 6.540606498718262
Epoch 960, val loss: 1.1962597370147705
Epoch 970, training loss: 0.0687684640288353 = 0.0032721073366701603 + 0.01 * 6.549635410308838
Epoch 970, val loss: 1.1991682052612305
Epoch 980, training loss: 0.06853896379470825 = 0.0031958555337041616 + 0.01 * 6.534311294555664
Epoch 980, val loss: 1.2019213438034058
Epoch 990, training loss: 0.06841911375522614 = 0.0031230261083692312 + 0.01 * 6.529609203338623
Epoch 990, val loss: 1.2046754360198975
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.8598
Flip ASR: 0.8311/225 nodes
The final ASR:0.71095, 0.12855, Accuracy:0.80123, 0.01848
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11606])
remove edge: torch.Size([2, 9480])
updated graph: torch.Size([2, 10530])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.97786, 0.00301, Accuracy:0.83086, 0.00462
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.4 der2:0.3 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.03149676322937 = 1.9477592706680298 + 0.01 * 8.373740196228027
Epoch 0, val loss: 1.9544867277145386
Epoch 10, training loss: 2.021914005279541 = 1.938177466392517 + 0.01 * 8.373644828796387
Epoch 10, val loss: 1.9455487728118896
Epoch 20, training loss: 2.010342836380005 = 1.9266102313995361 + 0.01 * 8.37325668334961
Epoch 20, val loss: 1.9343425035476685
Epoch 30, training loss: 1.9943201541900635 = 1.9106000661849976 + 0.01 * 8.372008323669434
Epoch 30, val loss: 1.9184656143188477
Epoch 40, training loss: 1.9708772897720337 = 1.8872287273406982 + 0.01 * 8.364853858947754
Epoch 40, val loss: 1.8951478004455566
Epoch 50, training loss: 1.936779499053955 = 1.8536677360534668 + 0.01 * 8.311176300048828
Epoch 50, val loss: 1.862115502357483
Epoch 60, training loss: 1.890358328819275 = 1.810443639755249 + 0.01 * 7.991466999053955
Epoch 60, val loss: 1.8214308023452759
Epoch 70, training loss: 1.8385497331619263 = 1.763024926185608 + 0.01 * 7.552482604980469
Epoch 70, val loss: 1.779022216796875
Epoch 80, training loss: 1.780740737915039 = 1.7084547281265259 + 0.01 * 7.228606700897217
Epoch 80, val loss: 1.7294988632202148
Epoch 90, training loss: 1.7076115608215332 = 1.6365598440170288 + 0.01 * 7.1051740646362305
Epoch 90, val loss: 1.6642112731933594
Epoch 100, training loss: 1.6124385595321655 = 1.5419880151748657 + 0.01 * 7.045054912567139
Epoch 100, val loss: 1.5804872512817383
Epoch 110, training loss: 1.497140645980835 = 1.4269797801971436 + 0.01 * 7.016086101531982
Epoch 110, val loss: 1.4828004837036133
Epoch 120, training loss: 1.3718125820159912 = 1.3017888069152832 + 0.01 * 7.002382755279541
Epoch 120, val loss: 1.3809585571289062
Epoch 130, training loss: 1.2480355501174927 = 1.1781229972839355 + 0.01 * 6.991250514984131
Epoch 130, val loss: 1.2842432260513306
Epoch 140, training loss: 1.1350141763687134 = 1.0652108192443848 + 0.01 * 6.980332374572754
Epoch 140, val loss: 1.1985914707183838
Epoch 150, training loss: 1.036025047302246 = 0.9663669466972351 + 0.01 * 6.965809345245361
Epoch 150, val loss: 1.1251775026321411
Epoch 160, training loss: 0.94908207654953 = 0.8796553611755371 + 0.01 * 6.942671298980713
Epoch 160, val loss: 1.0616490840911865
Epoch 170, training loss: 0.8706624507904053 = 0.8015850186347961 + 0.01 * 6.907741546630859
Epoch 170, val loss: 1.0059782266616821
Epoch 180, training loss: 0.7983908653259277 = 0.7297529578208923 + 0.01 * 6.863790988922119
Epoch 180, val loss: 0.9567663073539734
Epoch 190, training loss: 0.7316048741340637 = 0.6632052063941956 + 0.01 * 6.839968681335449
Epoch 190, val loss: 0.914095938205719
Epoch 200, training loss: 0.669800341129303 = 0.601624608039856 + 0.01 * 6.817573547363281
Epoch 200, val loss: 0.877901554107666
Epoch 210, training loss: 0.6125532388687134 = 0.5444437265396118 + 0.01 * 6.8109540939331055
Epoch 210, val loss: 0.8475697636604309
Epoch 220, training loss: 0.5589601397514343 = 0.490940123796463 + 0.01 * 6.802004337310791
Epoch 220, val loss: 0.8217082023620605
Epoch 230, training loss: 0.508418083190918 = 0.44047221541404724 + 0.01 * 6.794583797454834
Epoch 230, val loss: 0.7988149523735046
Epoch 240, training loss: 0.46044978499412537 = 0.3925793468952179 + 0.01 * 6.787045001983643
Epoch 240, val loss: 0.7781957387924194
Epoch 250, training loss: 0.4151788353919983 = 0.3473896384239197 + 0.01 * 6.778921127319336
Epoch 250, val loss: 0.7607059478759766
Epoch 260, training loss: 0.37328168749809265 = 0.3055140972137451 + 0.01 * 6.776759624481201
Epoch 260, val loss: 0.7480238080024719
Epoch 270, training loss: 0.3351646065711975 = 0.26751837134361267 + 0.01 * 6.764622688293457
Epoch 270, val loss: 0.7407065629959106
Epoch 280, training loss: 0.3013162910938263 = 0.23374971747398376 + 0.01 * 6.756658554077148
Epoch 280, val loss: 0.7385064959526062
Epoch 290, training loss: 0.27177661657333374 = 0.20427756011486053 + 0.01 * 6.749906539916992
Epoch 290, val loss: 0.7407256364822388
Epoch 300, training loss: 0.24632962048053741 = 0.17890207469463348 + 0.01 * 6.742754936218262
Epoch 300, val loss: 0.7465564608573914
Epoch 310, training loss: 0.224630206823349 = 0.15720875561237335 + 0.01 * 6.742146015167236
Epoch 310, val loss: 0.7551897764205933
Epoch 320, training loss: 0.205985426902771 = 0.13868434727191925 + 0.01 * 6.730108737945557
Epoch 320, val loss: 0.7659130096435547
Epoch 330, training loss: 0.19019030034542084 = 0.12281639873981476 + 0.01 * 6.737390518188477
Epoch 330, val loss: 0.7782260775566101
Epoch 340, training loss: 0.17636698484420776 = 0.10917306691408157 + 0.01 * 6.7193922996521
Epoch 340, val loss: 0.7916276454925537
Epoch 350, training loss: 0.1645456850528717 = 0.09739028662443161 + 0.01 * 6.715539455413818
Epoch 350, val loss: 0.8059957027435303
Epoch 360, training loss: 0.154267355799675 = 0.08717426657676697 + 0.01 * 6.709309101104736
Epoch 360, val loss: 0.821140706539154
Epoch 370, training loss: 0.14532402157783508 = 0.07826992869377136 + 0.01 * 6.705410480499268
Epoch 370, val loss: 0.8368328809738159
Epoch 380, training loss: 0.1375008374452591 = 0.07049909234046936 + 0.01 * 6.700174808502197
Epoch 380, val loss: 0.852917492389679
Epoch 390, training loss: 0.13070213794708252 = 0.06369554996490479 + 0.01 * 6.700658321380615
Epoch 390, val loss: 0.8691503405570984
Epoch 400, training loss: 0.12466048449277878 = 0.05772784352302551 + 0.01 * 6.693264007568359
Epoch 400, val loss: 0.885445237159729
Epoch 410, training loss: 0.1193699836730957 = 0.05247650295495987 + 0.01 * 6.689348220825195
Epoch 410, val loss: 0.9016320109367371
Epoch 420, training loss: 0.11470627784729004 = 0.04783852770924568 + 0.01 * 6.686775207519531
Epoch 420, val loss: 0.9177207350730896
Epoch 430, training loss: 0.11056426167488098 = 0.043728720396757126 + 0.01 * 6.683554649353027
Epoch 430, val loss: 0.9336413145065308
Epoch 440, training loss: 0.1068776547908783 = 0.04007140174508095 + 0.01 * 6.6806254386901855
Epoch 440, val loss: 0.949353039264679
Epoch 450, training loss: 0.10358306020498276 = 0.03680412471294403 + 0.01 * 6.67789363861084
Epoch 450, val loss: 0.964821457862854
Epoch 460, training loss: 0.10083868354558945 = 0.03387712687253952 + 0.01 * 6.696155548095703
Epoch 460, val loss: 0.9800341725349426
Epoch 470, training loss: 0.09803277254104614 = 0.031259555369615555 + 0.01 * 6.6773223876953125
Epoch 470, val loss: 0.9949338436126709
Epoch 480, training loss: 0.09561578184366226 = 0.028906524181365967 + 0.01 * 6.670925617218018
Epoch 480, val loss: 1.0095884799957275
Epoch 490, training loss: 0.0934692844748497 = 0.02678489126265049 + 0.01 * 6.6684393882751465
Epoch 490, val loss: 1.0239402055740356
Epoch 500, training loss: 0.09152031689882278 = 0.024866817519068718 + 0.01 * 6.665349960327148
Epoch 500, val loss: 1.0380688905715942
Epoch 510, training loss: 0.08975377678871155 = 0.02312871441245079 + 0.01 * 6.662506580352783
Epoch 510, val loss: 1.0519466400146484
Epoch 520, training loss: 0.08814975619316101 = 0.021551014855504036 + 0.01 * 6.659874439239502
Epoch 520, val loss: 1.0655196905136108
Epoch 530, training loss: 0.08669353276491165 = 0.02011731080710888 + 0.01 * 6.657622337341309
Epoch 530, val loss: 1.0788593292236328
Epoch 540, training loss: 0.08539734035730362 = 0.01881241984665394 + 0.01 * 6.658492565155029
Epoch 540, val loss: 1.0918807983398438
Epoch 550, training loss: 0.08415786176919937 = 0.017622865736484528 + 0.01 * 6.653500080108643
Epoch 550, val loss: 1.104600191116333
Epoch 560, training loss: 0.08303355425596237 = 0.016535820439457893 + 0.01 * 6.649774074554443
Epoch 560, val loss: 1.1170306205749512
Epoch 570, training loss: 0.08233155310153961 = 0.015541307628154755 + 0.01 * 6.679024696350098
Epoch 570, val loss: 1.12916898727417
Epoch 580, training loss: 0.08114807307720184 = 0.014633161947131157 + 0.01 * 6.651491165161133
Epoch 580, val loss: 1.1409796476364136
Epoch 590, training loss: 0.0802215039730072 = 0.013800361193716526 + 0.01 * 6.642114162445068
Epoch 590, val loss: 1.15250563621521
Epoch 600, training loss: 0.07942705601453781 = 0.013034561648964882 + 0.01 * 6.639249801635742
Epoch 600, val loss: 1.1637839078903198
Epoch 610, training loss: 0.07868999987840652 = 0.012329031713306904 + 0.01 * 6.636096954345703
Epoch 610, val loss: 1.174804449081421
Epoch 620, training loss: 0.07801076769828796 = 0.011677988804876804 + 0.01 * 6.6332783699035645
Epoch 620, val loss: 1.1855796575546265
Epoch 630, training loss: 0.07737971842288971 = 0.011076292023062706 + 0.01 * 6.630342960357666
Epoch 630, val loss: 1.196109414100647
Epoch 640, training loss: 0.07690703868865967 = 0.01051939558237791 + 0.01 * 6.638764381408691
Epoch 640, val loss: 1.2063759565353394
Epoch 650, training loss: 0.07627473026514053 = 0.01000397838652134 + 0.01 * 6.627075672149658
Epoch 650, val loss: 1.2163785696029663
Epoch 660, training loss: 0.07576555758714676 = 0.009525850415229797 + 0.01 * 6.623970985412598
Epoch 660, val loss: 1.2261261940002441
Epoch 670, training loss: 0.07532810419797897 = 0.009081646800041199 + 0.01 * 6.624646186828613
Epoch 670, val loss: 1.2356746196746826
Epoch 680, training loss: 0.07483471184968948 = 0.008668734692037106 + 0.01 * 6.616597652435303
Epoch 680, val loss: 1.2449432611465454
Epoch 690, training loss: 0.07448987662792206 = 0.00828402116894722 + 0.01 * 6.620585918426514
Epoch 690, val loss: 1.254006028175354
Epoch 700, training loss: 0.07408848404884338 = 0.007925339974462986 + 0.01 * 6.616314888000488
Epoch 700, val loss: 1.2628365755081177
Epoch 710, training loss: 0.07369321584701538 = 0.007590349763631821 + 0.01 * 6.610287189483643
Epoch 710, val loss: 1.271471619606018
Epoch 720, training loss: 0.07339460402727127 = 0.007277209777384996 + 0.01 * 6.611739158630371
Epoch 720, val loss: 1.2799257040023804
Epoch 730, training loss: 0.07303060591220856 = 0.006984293926507235 + 0.01 * 6.604631423950195
Epoch 730, val loss: 1.2881232500076294
Epoch 740, training loss: 0.07277530431747437 = 0.0067097460851073265 + 0.01 * 6.606555938720703
Epoch 740, val loss: 1.2961570024490356
Epoch 750, training loss: 0.07242678105831146 = 0.006452293135225773 + 0.01 * 6.597448825836182
Epoch 750, val loss: 1.3039823770523071
Epoch 760, training loss: 0.0722256600856781 = 0.006210412830114365 + 0.01 * 6.60152530670166
Epoch 760, val loss: 1.3116538524627686
Epoch 770, training loss: 0.07194947451353073 = 0.0059829349629580975 + 0.01 * 6.596653938293457
Epoch 770, val loss: 1.3191211223602295
Epoch 780, training loss: 0.07172410190105438 = 0.0057687414810061455 + 0.01 * 6.595536231994629
Epoch 780, val loss: 1.326441764831543
Epoch 790, training loss: 0.07148882746696472 = 0.0055670603178441525 + 0.01 * 6.592176914215088
Epoch 790, val loss: 1.3336032629013062
Epoch 800, training loss: 0.07122039049863815 = 0.005377085879445076 + 0.01 * 6.584331035614014
Epoch 800, val loss: 1.340460181236267
Epoch 810, training loss: 0.0709969624876976 = 0.005197478458285332 + 0.01 * 6.579948425292969
Epoch 810, val loss: 1.3473135232925415
Epoch 820, training loss: 0.07077690213918686 = 0.0050278883427381516 + 0.01 * 6.574901580810547
Epoch 820, val loss: 1.353966236114502
Epoch 830, training loss: 0.07063708454370499 = 0.004867066163569689 + 0.01 * 6.577001571655273
Epoch 830, val loss: 1.3604599237442017
Epoch 840, training loss: 0.07045365124940872 = 0.0047150603495538235 + 0.01 * 6.573858737945557
Epoch 840, val loss: 1.366822361946106
Epoch 850, training loss: 0.07028400152921677 = 0.0045704650692641735 + 0.01 * 6.571354389190674
Epoch 850, val loss: 1.3729978799819946
Epoch 860, training loss: 0.07017438113689423 = 0.004434157628566027 + 0.01 * 6.57402229309082
Epoch 860, val loss: 1.3791102170944214
Epoch 870, training loss: 0.06993640959262848 = 0.004303955473005772 + 0.01 * 6.5632452964782715
Epoch 870, val loss: 1.3849396705627441
Epoch 880, training loss: 0.06992010027170181 = 0.0041805654764175415 + 0.01 * 6.573953628540039
Epoch 880, val loss: 1.3908206224441528
Epoch 890, training loss: 0.06957778334617615 = 0.004063152242451906 + 0.01 * 6.5514631271362305
Epoch 890, val loss: 1.396506667137146
Epoch 900, training loss: 0.06955001503229141 = 0.00395167525857687 + 0.01 * 6.559834003448486
Epoch 900, val loss: 1.4020217657089233
Epoch 910, training loss: 0.06930378079414368 = 0.0038451331201940775 + 0.01 * 6.545864582061768
Epoch 910, val loss: 1.4074742794036865
Epoch 920, training loss: 0.06922969967126846 = 0.0037436888087540865 + 0.01 * 6.5486016273498535
Epoch 920, val loss: 1.412764310836792
Epoch 930, training loss: 0.0692322850227356 = 0.0036471537314355373 + 0.01 * 6.5585126876831055
Epoch 930, val loss: 1.417928695678711
Epoch 940, training loss: 0.0690193772315979 = 0.0035548910964280367 + 0.01 * 6.546448707580566
Epoch 940, val loss: 1.423030972480774
Epoch 950, training loss: 0.06883176416158676 = 0.003466959809884429 + 0.01 * 6.536480903625488
Epoch 950, val loss: 1.4279719591140747
Epoch 960, training loss: 0.06865116208791733 = 0.003382947528734803 + 0.01 * 6.526821613311768
Epoch 960, val loss: 1.432871699333191
Epoch 970, training loss: 0.06870696693658829 = 0.0033023653086274862 + 0.01 * 6.540460586547852
Epoch 970, val loss: 1.4375940561294556
Epoch 980, training loss: 0.06850608438253403 = 0.0032260084990411997 + 0.01 * 6.5280070304870605
Epoch 980, val loss: 1.4422814846038818
Epoch 990, training loss: 0.06840022653341293 = 0.0031523036304861307 + 0.01 * 6.524792671203613
Epoch 990, val loss: 1.4466978311538696
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.5203
Flip ASR: 0.4667/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.01379132270813 = 1.930053472518921 + 0.01 * 8.373774528503418
Epoch 0, val loss: 1.9205751419067383
Epoch 10, training loss: 2.0036511421203613 = 1.919914960861206 + 0.01 * 8.37360954284668
Epoch 10, val loss: 1.9106786251068115
Epoch 20, training loss: 1.9908345937728882 = 1.9071035385131836 + 0.01 * 8.373102188110352
Epoch 20, val loss: 1.8978002071380615
Epoch 30, training loss: 1.9726366996765137 = 1.888923168182373 + 0.01 * 8.371347427368164
Epoch 30, val loss: 1.8793706893920898
Epoch 40, training loss: 1.9458942413330078 = 1.862295150756836 + 0.01 * 8.359906196594238
Epoch 40, val loss: 1.852959156036377
Epoch 50, training loss: 1.9087532758712769 = 1.826035499572754 + 0.01 * 8.271775245666504
Epoch 50, val loss: 1.8190830945968628
Epoch 60, training loss: 1.8642380237579346 = 1.7852014303207397 + 0.01 * 7.903656005859375
Epoch 60, val loss: 1.7850861549377441
Epoch 70, training loss: 1.81805419921875 = 1.7426837682724 + 0.01 * 7.537045955657959
Epoch 70, val loss: 1.7514010667800903
Epoch 80, training loss: 1.7573037147521973 = 1.6847237348556519 + 0.01 * 7.258002758026123
Epoch 80, val loss: 1.7025288343429565
Epoch 90, training loss: 1.6778216361999512 = 1.6062262058258057 + 0.01 * 7.159547805786133
Epoch 90, val loss: 1.636573314666748
Epoch 100, training loss: 1.5805015563964844 = 1.509363055229187 + 0.01 * 7.113848686218262
Epoch 100, val loss: 1.5591086149215698
Epoch 110, training loss: 1.4762226343154907 = 1.4054946899414062 + 0.01 * 7.072798728942871
Epoch 110, val loss: 1.4752269983291626
Epoch 120, training loss: 1.3726210594177246 = 1.3021539449691772 + 0.01 * 7.046710968017578
Epoch 120, val loss: 1.394205093383789
Epoch 130, training loss: 1.2728068828582764 = 1.2025723457336426 + 0.01 * 7.023451805114746
Epoch 130, val loss: 1.3165267705917358
Epoch 140, training loss: 1.1794383525848389 = 1.1094728708267212 + 0.01 * 6.996553897857666
Epoch 140, val loss: 1.244651436805725
Epoch 150, training loss: 1.0950857400894165 = 1.0253738164901733 + 0.01 * 6.971189498901367
Epoch 150, val loss: 1.1805644035339355
Epoch 160, training loss: 1.0196415185928345 = 0.9501802325248718 + 0.01 * 6.946127414703369
Epoch 160, val loss: 1.124133586883545
Epoch 170, training loss: 0.9505102634429932 = 0.8813605308532715 + 0.01 * 6.914974212646484
Epoch 170, val loss: 1.0745033025741577
Epoch 180, training loss: 0.8852766752243042 = 0.8164076209068298 + 0.01 * 6.886906147003174
Epoch 180, val loss: 1.028760313987732
Epoch 190, training loss: 0.8234918117523193 = 0.7548742294311523 + 0.01 * 6.861755847930908
Epoch 190, val loss: 0.985921323299408
Epoch 200, training loss: 0.766181468963623 = 0.6977866888046265 + 0.01 * 6.839475154876709
Epoch 200, val loss: 0.9466031193733215
Epoch 210, training loss: 0.7138712406158447 = 0.6456186771392822 + 0.01 * 6.825253486633301
Epoch 210, val loss: 0.9114542007446289
Epoch 220, training loss: 0.6654723286628723 = 0.5973012447357178 + 0.01 * 6.817111015319824
Epoch 220, val loss: 0.8807815313339233
Epoch 230, training loss: 0.6192060708999634 = 0.5511151552200317 + 0.01 * 6.8090901374816895
Epoch 230, val loss: 0.8539149165153503
Epoch 240, training loss: 0.5736218690872192 = 0.5055899620056152 + 0.01 * 6.803189754486084
Epoch 240, val loss: 0.8302007913589478
Epoch 250, training loss: 0.528038501739502 = 0.4600568115711212 + 0.01 * 6.79817008972168
Epoch 250, val loss: 0.8093869090080261
Epoch 260, training loss: 0.48270493745803833 = 0.4147355854511261 + 0.01 * 6.796936988830566
Epoch 260, val loss: 0.7915170788764954
Epoch 270, training loss: 0.43853944540023804 = 0.3706218898296356 + 0.01 * 6.791754245758057
Epoch 270, val loss: 0.7769123315811157
Epoch 280, training loss: 0.39693304896354675 = 0.3290541470050812 + 0.01 * 6.787890434265137
Epoch 280, val loss: 0.7657819390296936
Epoch 290, training loss: 0.3588221073150635 = 0.2909749448299408 + 0.01 * 6.784715175628662
Epoch 290, val loss: 0.7580991387367249
Epoch 300, training loss: 0.3245207369327545 = 0.25671759247779846 + 0.01 * 6.780315399169922
Epoch 300, val loss: 0.7539101839065552
Epoch 310, training loss: 0.29406774044036865 = 0.2263106256723404 + 0.01 * 6.775711536407471
Epoch 310, val loss: 0.7535728216171265
Epoch 320, training loss: 0.2671717405319214 = 0.19946834444999695 + 0.01 * 6.7703399658203125
Epoch 320, val loss: 0.7571680545806885
Epoch 330, training loss: 0.24396130442619324 = 0.17609906196594238 + 0.01 * 6.786224842071533
Epoch 330, val loss: 0.7648318409919739
Epoch 340, training loss: 0.22356563806533813 = 0.1559440940618515 + 0.01 * 6.762155532836914
Epoch 340, val loss: 0.7759518623352051
Epoch 350, training loss: 0.20607808232307434 = 0.13851085305213928 + 0.01 * 6.7567219734191895
Epoch 350, val loss: 0.7897323369979858
Epoch 360, training loss: 0.19090819358825684 = 0.12342534214258194 + 0.01 * 6.74828577041626
Epoch 360, val loss: 0.8055399656295776
Epoch 370, training loss: 0.17795972526073456 = 0.11028961837291718 + 0.01 * 6.7670111656188965
Epoch 370, val loss: 0.8225885033607483
Epoch 380, training loss: 0.16621550917625427 = 0.0987662747502327 + 0.01 * 6.744924545288086
Epoch 380, val loss: 0.8402960896492004
Epoch 390, training loss: 0.15592122077941895 = 0.0885632261633873 + 0.01 * 6.7357988357543945
Epoch 390, val loss: 0.8584202527999878
Epoch 400, training loss: 0.14672355353832245 = 0.07945509999990463 + 0.01 * 6.7268452644348145
Epoch 400, val loss: 0.8767698407173157
Epoch 410, training loss: 0.13872849941253662 = 0.07135200500488281 + 0.01 * 6.737649440765381
Epoch 410, val loss: 0.8951162099838257
Epoch 420, training loss: 0.13127923011779785 = 0.06405816972255707 + 0.01 * 6.722106456756592
Epoch 420, val loss: 0.9132477641105652
Epoch 430, training loss: 0.12468379735946655 = 0.05745868384838104 + 0.01 * 6.7225117683410645
Epoch 430, val loss: 0.9312271475791931
Epoch 440, training loss: 0.11862164735794067 = 0.05151249095797539 + 0.01 * 6.710915565490723
Epoch 440, val loss: 0.9490718245506287
Epoch 450, training loss: 0.11317412555217743 = 0.04615657404065132 + 0.01 * 6.701755523681641
Epoch 450, val loss: 0.9666804671287537
Epoch 460, training loss: 0.10848317295312881 = 0.04132223129272461 + 0.01 * 6.716094493865967
Epoch 460, val loss: 0.9840562343597412
Epoch 470, training loss: 0.10410802811384201 = 0.03712005168199539 + 0.01 * 6.698797702789307
Epoch 470, val loss: 1.001319169998169
Epoch 480, training loss: 0.10039813816547394 = 0.03349066525697708 + 0.01 * 6.690747261047363
Epoch 480, val loss: 1.01853609085083
Epoch 490, training loss: 0.09721581637859344 = 0.03036121465265751 + 0.01 * 6.685460567474365
Epoch 490, val loss: 1.0354374647140503
Epoch 500, training loss: 0.09451355040073395 = 0.02764749526977539 + 0.01 * 6.686605453491211
Epoch 500, val loss: 1.051816701889038
Epoch 510, training loss: 0.09201942384243011 = 0.025291841477155685 + 0.01 * 6.672758102416992
Epoch 510, val loss: 1.0677282810211182
Epoch 520, training loss: 0.08985137939453125 = 0.02322659082710743 + 0.01 * 6.662478923797607
Epoch 520, val loss: 1.0831282138824463
Epoch 530, training loss: 0.08820689469575882 = 0.02140744961798191 + 0.01 * 6.6799445152282715
Epoch 530, val loss: 1.097990870475769
Epoch 540, training loss: 0.08636511117219925 = 0.019796160981059074 + 0.01 * 6.656895160675049
Epoch 540, val loss: 1.1123929023742676
Epoch 550, training loss: 0.08489567041397095 = 0.0183656495064497 + 0.01 * 6.65300178527832
Epoch 550, val loss: 1.1263381242752075
Epoch 560, training loss: 0.08352766931056976 = 0.01709061488509178 + 0.01 * 6.643706321716309
Epoch 560, val loss: 1.1399093866348267
Epoch 570, training loss: 0.08258819580078125 = 0.015948612242937088 + 0.01 * 6.663958549499512
Epoch 570, val loss: 1.1530126333236694
Epoch 580, training loss: 0.08130715787410736 = 0.014922726899385452 + 0.01 * 6.6384429931640625
Epoch 580, val loss: 1.165661334991455
Epoch 590, training loss: 0.08029676228761673 = 0.013997524045407772 + 0.01 * 6.6299238204956055
Epoch 590, val loss: 1.1779145002365112
Epoch 600, training loss: 0.07943141460418701 = 0.01315914187580347 + 0.01 * 6.627227783203125
Epoch 600, val loss: 1.189736008644104
Epoch 610, training loss: 0.07876332104206085 = 0.012397464364767075 + 0.01 * 6.6365861892700195
Epoch 610, val loss: 1.2013016939163208
Epoch 620, training loss: 0.07791221141815186 = 0.011703369207680225 + 0.01 * 6.620883941650391
Epoch 620, val loss: 1.212429165840149
Epoch 630, training loss: 0.07716351002454758 = 0.01106970850378275 + 0.01 * 6.609380722045898
Epoch 630, val loss: 1.2232038974761963
Epoch 640, training loss: 0.0767107605934143 = 0.01048919279128313 + 0.01 * 6.622156620025635
Epoch 640, val loss: 1.2337490320205688
Epoch 650, training loss: 0.0760967880487442 = 0.009956351481378078 + 0.01 * 6.614043712615967
Epoch 650, val loss: 1.243911862373352
Epoch 660, training loss: 0.07572635263204575 = 0.009466550312936306 + 0.01 * 6.625980377197266
Epoch 660, val loss: 1.253730297088623
Epoch 670, training loss: 0.07499412447214127 = 0.009015207178890705 + 0.01 * 6.597891330718994
Epoch 670, val loss: 1.2633154392242432
Epoch 680, training loss: 0.07454882562160492 = 0.008597726933658123 + 0.01 * 6.595109939575195
Epoch 680, val loss: 1.2726393938064575
Epoch 690, training loss: 0.07409556210041046 = 0.008210993371903896 + 0.01 * 6.588456630706787
Epoch 690, val loss: 1.2816625833511353
Epoch 700, training loss: 0.07377351820468903 = 0.007852301932871342 + 0.01 * 6.5921220779418945
Epoch 700, val loss: 1.290443778038025
Epoch 710, training loss: 0.0734589546918869 = 0.007518810220062733 + 0.01 * 6.5940141677856445
Epoch 710, val loss: 1.2989050149917603
Epoch 720, training loss: 0.0729997307062149 = 0.00720827654004097 + 0.01 * 6.5791449546813965
Epoch 720, val loss: 1.3072035312652588
Epoch 730, training loss: 0.07279586046934128 = 0.006918372120708227 + 0.01 * 6.587749004364014
Epoch 730, val loss: 1.3152879476547241
Epoch 740, training loss: 0.07236488908529282 = 0.006648065987974405 + 0.01 * 6.571682453155518
Epoch 740, val loss: 1.3230191469192505
Epoch 750, training loss: 0.0720934122800827 = 0.00639483192935586 + 0.01 * 6.569858074188232
Epoch 750, val loss: 1.3306206464767456
Epoch 760, training loss: 0.07175011187791824 = 0.006157479248940945 + 0.01 * 6.559263229370117
Epoch 760, val loss: 1.3380529880523682
Epoch 770, training loss: 0.07185010612010956 = 0.005934758111834526 + 0.01 * 6.5915350914001465
Epoch 770, val loss: 1.3451770544052124
Epoch 780, training loss: 0.07130692154169083 = 0.005725764203816652 + 0.01 * 6.5581159591674805
Epoch 780, val loss: 1.3521366119384766
Epoch 790, training loss: 0.07101759314537048 = 0.005529191344976425 + 0.01 * 6.548840522766113
Epoch 790, val loss: 1.3590081930160522
Epoch 800, training loss: 0.07095684111118317 = 0.005343795754015446 + 0.01 * 6.561304569244385
Epoch 800, val loss: 1.3655962944030762
Epoch 810, training loss: 0.0706152468919754 = 0.005169053561985493 + 0.01 * 6.544619083404541
Epoch 810, val loss: 1.3720407485961914
Epoch 820, training loss: 0.07040782272815704 = 0.0050039165653288364 + 0.01 * 6.540390968322754
Epoch 820, val loss: 1.3783507347106934
Epoch 830, training loss: 0.07024727016687393 = 0.004848176147788763 + 0.01 * 6.539909839630127
Epoch 830, val loss: 1.3844399452209473
Epoch 840, training loss: 0.0700584277510643 = 0.00470063416287303 + 0.01 * 6.5357794761657715
Epoch 840, val loss: 1.3903979063034058
Epoch 850, training loss: 0.06985516101121902 = 0.004561333917081356 + 0.01 * 6.529382705688477
Epoch 850, val loss: 1.3962324857711792
Epoch 860, training loss: 0.06978733837604523 = 0.004428868182003498 + 0.01 * 6.535846710205078
Epoch 860, val loss: 1.4018720388412476
Epoch 870, training loss: 0.06961118429899216 = 0.004303229972720146 + 0.01 * 6.530795574188232
Epoch 870, val loss: 1.4074267148971558
Epoch 880, training loss: 0.0695515125989914 = 0.0041837845928967 + 0.01 * 6.536773204803467
Epoch 880, val loss: 1.4128406047821045
Epoch 890, training loss: 0.0693502277135849 = 0.00407019630074501 + 0.01 * 6.528003215789795
Epoch 890, val loss: 1.4180891513824463
Epoch 900, training loss: 0.06901960074901581 = 0.003962323535233736 + 0.01 * 6.505727767944336
Epoch 900, val loss: 1.4232618808746338
Epoch 910, training loss: 0.06898433715105057 = 0.003859402844682336 + 0.01 * 6.512494087219238
Epoch 910, val loss: 1.4281749725341797
Epoch 920, training loss: 0.0688135102391243 = 0.003761370200663805 + 0.01 * 6.505213737487793
Epoch 920, val loss: 1.4331547021865845
Epoch 930, training loss: 0.06866741180419922 = 0.003667741548269987 + 0.01 * 6.499967098236084
Epoch 930, val loss: 1.4378602504730225
Epoch 940, training loss: 0.0687483549118042 = 0.0035785974469035864 + 0.01 * 6.5169758796691895
Epoch 940, val loss: 1.4426156282424927
Epoch 950, training loss: 0.06842001527547836 = 0.0034932666458189487 + 0.01 * 6.492675304412842
Epoch 950, val loss: 1.4471306800842285
Epoch 960, training loss: 0.06823232769966125 = 0.003411754034459591 + 0.01 * 6.482057571411133
Epoch 960, val loss: 1.4515942335128784
Epoch 970, training loss: 0.06808558851480484 = 0.003333746688440442 + 0.01 * 6.475183963775635
Epoch 970, val loss: 1.4559822082519531
Epoch 980, training loss: 0.06822040677070618 = 0.0032588078174740076 + 0.01 * 6.496159553527832
Epoch 980, val loss: 1.4602210521697998
Epoch 990, training loss: 0.0679217129945755 = 0.0031873807311058044 + 0.01 * 6.473433017730713
Epoch 990, val loss: 1.4644038677215576
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.2841
Flip ASR: 0.2311/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0186257362365723 = 1.9348880052566528 + 0.01 * 8.373781204223633
Epoch 0, val loss: 1.9384175539016724
Epoch 10, training loss: 2.0085091590881348 = 1.9247722625732422 + 0.01 * 8.37368392944336
Epoch 10, val loss: 1.9285974502563477
Epoch 20, training loss: 1.9957208633422852 = 1.9119873046875 + 0.01 * 8.373351097106934
Epoch 20, val loss: 1.9157005548477173
Epoch 30, training loss: 1.977446436882019 = 1.8937232494354248 + 0.01 * 8.372323036193848
Epoch 30, val loss: 1.89691162109375
Epoch 40, training loss: 1.9505141973495483 = 1.866847276687622 + 0.01 * 8.366687774658203
Epoch 40, val loss: 1.8693592548370361
Epoch 50, training loss: 1.9135386943817139 = 1.8302861452102661 + 0.01 * 8.3252534866333
Epoch 50, val loss: 1.8337504863739014
Epoch 60, training loss: 1.8710530996322632 = 1.7899916172027588 + 0.01 * 8.1061429977417
Epoch 60, val loss: 1.797691822052002
Epoch 70, training loss: 1.8267922401428223 = 1.7493399381637573 + 0.01 * 7.7452263832092285
Epoch 70, val loss: 1.7624343633651733
Epoch 80, training loss: 1.7674487829208374 = 1.6942561864852905 + 0.01 * 7.3192644119262695
Epoch 80, val loss: 1.7154771089553833
Epoch 90, training loss: 1.6894707679748535 = 1.618214726448059 + 0.01 * 7.125606060028076
Epoch 90, val loss: 1.6533236503601074
Epoch 100, training loss: 1.5922397375106812 = 1.5217314958572388 + 0.01 * 7.050828456878662
Epoch 100, val loss: 1.5748628377914429
Epoch 110, training loss: 1.4845218658447266 = 1.414687991142273 + 0.01 * 6.983389854431152
Epoch 110, val loss: 1.4880895614624023
Epoch 120, training loss: 1.3756262063980103 = 1.3062376976013184 + 0.01 * 6.938851833343506
Epoch 120, val loss: 1.403804898262024
Epoch 130, training loss: 1.2693980932235718 = 1.2003320455551147 + 0.01 * 6.906600475311279
Epoch 130, val loss: 1.3234449625015259
Epoch 140, training loss: 1.170230746269226 = 1.1013870239257812 + 0.01 * 6.884369850158691
Epoch 140, val loss: 1.2499704360961914
Epoch 150, training loss: 1.081897497177124 = 1.0131652355194092 + 0.01 * 6.873225688934326
Epoch 150, val loss: 1.1851789951324463
Epoch 160, training loss: 1.003835678100586 = 0.9352322220802307 + 0.01 * 6.860348224639893
Epoch 160, val loss: 1.1277707815170288
Epoch 170, training loss: 0.9323300123214722 = 0.863762378692627 + 0.01 * 6.856764793395996
Epoch 170, val loss: 1.0744715929031372
Epoch 180, training loss: 0.8636190891265869 = 0.7950785160064697 + 0.01 * 6.854056358337402
Epoch 180, val loss: 1.0229523181915283
Epoch 190, training loss: 0.7965034246444702 = 0.7279793620109558 + 0.01 * 6.852406978607178
Epoch 190, val loss: 0.9731038212776184
Epoch 200, training loss: 0.7317785024642944 = 0.6632719039916992 + 0.01 * 6.8506574630737305
Epoch 200, val loss: 0.9267019629478455
Epoch 210, training loss: 0.6702556610107422 = 0.6017699241638184 + 0.01 * 6.848576068878174
Epoch 210, val loss: 0.8854261636734009
Epoch 220, training loss: 0.6117279529571533 = 0.5432743430137634 + 0.01 * 6.8453593254089355
Epoch 220, val loss: 0.849430501461029
Epoch 230, training loss: 0.5556530952453613 = 0.48723822832107544 + 0.01 * 6.841484069824219
Epoch 230, val loss: 0.8177226781845093
Epoch 240, training loss: 0.5017848014831543 = 0.4334348142147064 + 0.01 * 6.834998607635498
Epoch 240, val loss: 0.7896528840065002
Epoch 250, training loss: 0.45036137104034424 = 0.38207384943962097 + 0.01 * 6.8287529945373535
Epoch 250, val loss: 0.7653616666793823
Epoch 260, training loss: 0.40192535519599915 = 0.3337249457836151 + 0.01 * 6.820041656494141
Epoch 260, val loss: 0.7456836700439453
Epoch 270, training loss: 0.35718095302581787 = 0.2890828847885132 + 0.01 * 6.8098063468933105
Epoch 270, val loss: 0.7313781976699829
Epoch 280, training loss: 0.3168638050556183 = 0.24883142113685608 + 0.01 * 6.8032379150390625
Epoch 280, val loss: 0.7224671244621277
Epoch 290, training loss: 0.2814212441444397 = 0.2134515941143036 + 0.01 * 6.7969651222229
Epoch 290, val loss: 0.7185333967208862
Epoch 300, training loss: 0.2509618401527405 = 0.18311789631843567 + 0.01 * 6.784393787384033
Epoch 300, val loss: 0.719119131565094
Epoch 310, training loss: 0.2253483533859253 = 0.15759891271591187 + 0.01 * 6.774945259094238
Epoch 310, val loss: 0.7236183285713196
Epoch 320, training loss: 0.2040681540966034 = 0.13638757169246674 + 0.01 * 6.768059253692627
Epoch 320, val loss: 0.7312591671943665
Epoch 330, training loss: 0.1864405870437622 = 0.11880269646644592 + 0.01 * 6.763788223266602
Epoch 330, val loss: 0.7414776682853699
Epoch 340, training loss: 0.17172351479530334 = 0.10416614264249802 + 0.01 * 6.7557373046875
Epoch 340, val loss: 0.7536640763282776
Epoch 350, training loss: 0.1593952476978302 = 0.09188982844352722 + 0.01 * 6.750541687011719
Epoch 350, val loss: 0.767393946647644
Epoch 360, training loss: 0.14907501637935638 = 0.08149738609790802 + 0.01 * 6.757762908935547
Epoch 360, val loss: 0.782299816608429
Epoch 370, training loss: 0.1401601880788803 = 0.07263141870498657 + 0.01 * 6.7528767585754395
Epoch 370, val loss: 0.7980499267578125
Epoch 380, training loss: 0.13244104385375977 = 0.06499902158975601 + 0.01 * 6.744202613830566
Epoch 380, val loss: 0.8145298361778259
Epoch 390, training loss: 0.12575216591358185 = 0.05837233364582062 + 0.01 * 6.737983703613281
Epoch 390, val loss: 0.8316769003868103
Epoch 400, training loss: 0.11990725994110107 = 0.052581366151571274 + 0.01 * 6.732590198516846
Epoch 400, val loss: 0.8493320345878601
Epoch 410, training loss: 0.11477774381637573 = 0.04749371483922005 + 0.01 * 6.728403568267822
Epoch 410, val loss: 0.867422342300415
Epoch 420, training loss: 0.11025889217853546 = 0.04301123693585396 + 0.01 * 6.724766254425049
Epoch 420, val loss: 0.8857986927032471
Epoch 430, training loss: 0.10626346617937088 = 0.039055339992046356 + 0.01 * 6.720812797546387
Epoch 430, val loss: 0.9043050408363342
Epoch 440, training loss: 0.10275305807590485 = 0.03555527701973915 + 0.01 * 6.719778060913086
Epoch 440, val loss: 0.9228023290634155
Epoch 450, training loss: 0.09957684576511383 = 0.03245403245091438 + 0.01 * 6.712281227111816
Epoch 450, val loss: 0.9411382079124451
Epoch 460, training loss: 0.09679065644741058 = 0.02970382198691368 + 0.01 * 6.708683013916016
Epoch 460, val loss: 0.9591724276542664
Epoch 470, training loss: 0.09436789155006409 = 0.02726146951317787 + 0.01 * 6.710641860961914
Epoch 470, val loss: 0.976885199546814
Epoch 480, training loss: 0.09216219931840897 = 0.0250899288803339 + 0.01 * 6.7072272300720215
Epoch 480, val loss: 0.9941582679748535
Epoch 490, training loss: 0.09016450494527817 = 0.02315565198659897 + 0.01 * 6.70088529586792
Epoch 490, val loss: 1.0110020637512207
Epoch 500, training loss: 0.08852095901966095 = 0.0214284285902977 + 0.01 * 6.709252834320068
Epoch 500, val loss: 1.0272995233535767
Epoch 510, training loss: 0.08679026365280151 = 0.019882626831531525 + 0.01 * 6.690763473510742
Epoch 510, val loss: 1.0430688858032227
Epoch 520, training loss: 0.08532136678695679 = 0.01849319227039814 + 0.01 * 6.682817459106445
Epoch 520, val loss: 1.0583680868148804
Epoch 530, training loss: 0.08398266136646271 = 0.01724293641746044 + 0.01 * 6.673972129821777
Epoch 530, val loss: 1.0731720924377441
Epoch 540, training loss: 0.08284170925617218 = 0.016116570681333542 + 0.01 * 6.672513961791992
Epoch 540, val loss: 1.0875414609909058
Epoch 550, training loss: 0.08174546808004379 = 0.01509802881628275 + 0.01 * 6.6647443771362305
Epoch 550, val loss: 1.1014567613601685
Epoch 560, training loss: 0.08112604916095734 = 0.014174229465425014 + 0.01 * 6.6951823234558105
Epoch 560, val loss: 1.1148974895477295
Epoch 570, training loss: 0.07987707853317261 = 0.01333580445498228 + 0.01 * 6.654128074645996
Epoch 570, val loss: 1.1279875040054321
Epoch 580, training loss: 0.07909148931503296 = 0.012571636587381363 + 0.01 * 6.651985168457031
Epoch 580, val loss: 1.1406604051589966
Epoch 590, training loss: 0.07838350534439087 = 0.011873098090291023 + 0.01 * 6.651041030883789
Epoch 590, val loss: 1.1529264450073242
Epoch 600, training loss: 0.07763562351465225 = 0.011234407313168049 + 0.01 * 6.6401214599609375
Epoch 600, val loss: 1.1649152040481567
Epoch 610, training loss: 0.07696541398763657 = 0.010648456402122974 + 0.01 * 6.631695747375488
Epoch 610, val loss: 1.1765128374099731
Epoch 620, training loss: 0.07672030478715897 = 0.010109261609613895 + 0.01 * 6.661104202270508
Epoch 620, val loss: 1.1877559423446655
Epoch 630, training loss: 0.07591915130615234 = 0.009613174945116043 + 0.01 * 6.630597114562988
Epoch 630, val loss: 1.198748230934143
Epoch 640, training loss: 0.07549065351486206 = 0.00915476307272911 + 0.01 * 6.633589744567871
Epoch 640, val loss: 1.209380865097046
Epoch 650, training loss: 0.07488002628087997 = 0.008730870671570301 + 0.01 * 6.614915370941162
Epoch 650, val loss: 1.2197096347808838
Epoch 660, training loss: 0.07441304624080658 = 0.008337799459695816 + 0.01 * 6.607524394989014
Epoch 660, val loss: 1.2297773361206055
Epoch 670, training loss: 0.07405927032232285 = 0.007972314953804016 + 0.01 * 6.608695983886719
Epoch 670, val loss: 1.2395708560943604
Epoch 680, training loss: 0.073794424533844 = 0.0076326304115355015 + 0.01 * 6.616179943084717
Epoch 680, val loss: 1.2491123676300049
Epoch 690, training loss: 0.0732477530837059 = 0.007316311821341515 + 0.01 * 6.593144416809082
Epoch 690, val loss: 1.2584513425827026
Epoch 700, training loss: 0.07295919209718704 = 0.007020806428045034 + 0.01 * 6.593838214874268
Epoch 700, val loss: 1.2674514055252075
Epoch 710, training loss: 0.07262814044952393 = 0.00674462178722024 + 0.01 * 6.588352203369141
Epoch 710, val loss: 1.2763196229934692
Epoch 720, training loss: 0.072297602891922 = 0.0064861960709095 + 0.01 * 6.581141471862793
Epoch 720, val loss: 1.2849576473236084
Epoch 730, training loss: 0.07206268608570099 = 0.006243716459721327 + 0.01 * 6.581897735595703
Epoch 730, val loss: 1.2933359146118164
Epoch 740, training loss: 0.07195782661437988 = 0.0060162837617099285 + 0.01 * 6.594154357910156
Epoch 740, val loss: 1.3015573024749756
Epoch 750, training loss: 0.07149846851825714 = 0.0058025699108839035 + 0.01 * 6.569589614868164
Epoch 750, val loss: 1.309548020362854
Epoch 760, training loss: 0.07125990837812424 = 0.0056015183217823505 + 0.01 * 6.5658392906188965
Epoch 760, val loss: 1.317352056503296
Epoch 770, training loss: 0.07115858793258667 = 0.005411979742348194 + 0.01 * 6.574660778045654
Epoch 770, val loss: 1.3249696493148804
Epoch 780, training loss: 0.07086045295000076 = 0.005233210511505604 + 0.01 * 6.5627241134643555
Epoch 780, val loss: 1.332473874092102
Epoch 790, training loss: 0.07074873894453049 = 0.005064352415502071 + 0.01 * 6.56843900680542
Epoch 790, val loss: 1.3397281169891357
Epoch 800, training loss: 0.07034609466791153 = 0.00490474421530962 + 0.01 * 6.544135093688965
Epoch 800, val loss: 1.3468464612960815
Epoch 810, training loss: 0.0703965350985527 = 0.004753580782562494 + 0.01 * 6.564295291900635
Epoch 810, val loss: 1.3538120985031128
Epoch 820, training loss: 0.06996888667345047 = 0.004610441625118256 + 0.01 * 6.535844802856445
Epoch 820, val loss: 1.3606375455856323
Epoch 830, training loss: 0.07002577930688858 = 0.004474627785384655 + 0.01 * 6.555115222930908
Epoch 830, val loss: 1.367248773574829
Epoch 840, training loss: 0.06984715163707733 = 0.004346153233200312 + 0.01 * 6.550099849700928
Epoch 840, val loss: 1.373763918876648
Epoch 850, training loss: 0.06953337788581848 = 0.004223931580781937 + 0.01 * 6.530944347381592
Epoch 850, val loss: 1.3800935745239258
Epoch 860, training loss: 0.06942437589168549 = 0.004107764922082424 + 0.01 * 6.531661033630371
Epoch 860, val loss: 1.3862533569335938
Epoch 870, training loss: 0.06928721070289612 = 0.003997358027845621 + 0.01 * 6.528985977172852
Epoch 870, val loss: 1.392414927482605
Epoch 880, training loss: 0.06910395622253418 = 0.003891971195116639 + 0.01 * 6.521198272705078
Epoch 880, val loss: 1.398262858390808
Epoch 890, training loss: 0.06894270330667496 = 0.003791699418798089 + 0.01 * 6.515100479125977
Epoch 890, val loss: 1.404150128364563
Epoch 900, training loss: 0.06883811950683594 = 0.0036959063727408648 + 0.01 * 6.514221668243408
Epoch 900, val loss: 1.4098303318023682
Epoch 910, training loss: 0.06879711896181107 = 0.003604680998250842 + 0.01 * 6.5192437171936035
Epoch 910, val loss: 1.4154396057128906
Epoch 920, training loss: 0.06844865530729294 = 0.003517269855365157 + 0.01 * 6.493138313293457
Epoch 920, val loss: 1.4208837747573853
Epoch 930, training loss: 0.06843380630016327 = 0.003433841746300459 + 0.01 * 6.499997138977051
Epoch 930, val loss: 1.426252007484436
Epoch 940, training loss: 0.06831865012645721 = 0.003354037180542946 + 0.01 * 6.496461391448975
Epoch 940, val loss: 1.4314653873443604
Epoch 950, training loss: 0.068314328789711 = 0.0032777946908026934 + 0.01 * 6.503653526306152
Epoch 950, val loss: 1.436625361442566
Epoch 960, training loss: 0.06818222254514694 = 0.0032045659609138966 + 0.01 * 6.49776554107666
Epoch 960, val loss: 1.4414937496185303
Epoch 970, training loss: 0.06804477423429489 = 0.0031347558833658695 + 0.01 * 6.491002082824707
Epoch 970, val loss: 1.4465211629867554
Epoch 980, training loss: 0.0678592249751091 = 0.0030675302259624004 + 0.01 * 6.4791693687438965
Epoch 980, val loss: 1.451296091079712
Epoch 990, training loss: 0.06779712438583374 = 0.003003140911459923 + 0.01 * 6.479398250579834
Epoch 990, val loss: 1.4560744762420654
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.8339
Flip ASR: 0.8000/225 nodes
The final ASR:0.54613, 0.22520, Accuracy:0.80988, 0.01364
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11572])
remove edge: torch.Size([2, 9510])
updated graph: torch.Size([2, 10526])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
The final ASR:0.97540, 0.00174, Accuracy:0.82716, 0.00462
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.4 der2:0.3 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.03265118598938 = 1.9489130973815918 + 0.01 * 8.373805046081543
Epoch 0, val loss: 1.9548418521881104
Epoch 10, training loss: 2.02339768409729 = 1.9396607875823975 + 0.01 * 8.373685836791992
Epoch 10, val loss: 1.9462883472442627
Epoch 20, training loss: 2.011747121810913 = 1.9280142784118652 + 0.01 * 8.37327766418457
Epoch 20, val loss: 1.9351449012756348
Epoch 30, training loss: 1.9951190948486328 = 1.9113988876342773 + 0.01 * 8.372015953063965
Epoch 30, val loss: 1.919037103652954
Epoch 40, training loss: 1.9702526330947876 = 1.8866020441055298 + 0.01 * 8.365059852600098
Epoch 40, val loss: 1.895034909248352
Epoch 50, training loss: 1.9340134859085083 = 1.850847601890564 + 0.01 * 8.316588401794434
Epoch 50, val loss: 1.8614307641983032
Epoch 60, training loss: 1.8869692087173462 = 1.8068429231643677 + 0.01 * 8.01262378692627
Epoch 60, val loss: 1.8226591348648071
Epoch 70, training loss: 1.840501308441162 = 1.7640321254730225 + 0.01 * 7.646917819976807
Epoch 70, val loss: 1.7855770587921143
Epoch 80, training loss: 1.788916826248169 = 1.715678095817566 + 0.01 * 7.3238725662231445
Epoch 80, val loss: 1.74009370803833
Epoch 90, training loss: 1.7210050821304321 = 1.650055170059204 + 0.01 * 7.094996452331543
Epoch 90, val loss: 1.6811641454696655
Epoch 100, training loss: 1.632678747177124 = 1.562788724899292 + 0.01 * 6.989004611968994
Epoch 100, val loss: 1.606106162071228
Epoch 110, training loss: 1.5263547897338867 = 1.4567620754241943 + 0.01 * 6.959268569946289
Epoch 110, val loss: 1.5158475637435913
Epoch 120, training loss: 1.411183476448059 = 1.3418242931365967 + 0.01 * 6.935924053192139
Epoch 120, val loss: 1.4199573993682861
Epoch 130, training loss: 1.2938164472579956 = 1.2247346639633179 + 0.01 * 6.908178806304932
Epoch 130, val loss: 1.3247082233428955
Epoch 140, training loss: 1.177510380744934 = 1.1087124347686768 + 0.01 * 6.879792213439941
Epoch 140, val loss: 1.2328811883926392
Epoch 150, training loss: 1.0666884183883667 = 0.9981322884559631 + 0.01 * 6.855611324310303
Epoch 150, val loss: 1.1468145847320557
Epoch 160, training loss: 0.9639185667037964 = 0.895500659942627 + 0.01 * 6.841790199279785
Epoch 160, val loss: 1.0678352117538452
Epoch 170, training loss: 0.8693791031837463 = 0.8010131120681763 + 0.01 * 6.836599349975586
Epoch 170, val loss: 0.9961119294166565
Epoch 180, training loss: 0.7829731702804565 = 0.7146273851394653 + 0.01 * 6.834580421447754
Epoch 180, val loss: 0.9312642812728882
Epoch 190, training loss: 0.7057433724403381 = 0.6374188661575317 + 0.01 * 6.832452774047852
Epoch 190, val loss: 0.8746967911720276
Epoch 200, training loss: 0.6388806104660034 = 0.5705832839012146 + 0.01 * 6.8297343254089355
Epoch 200, val loss: 0.8280321955680847
Epoch 210, training loss: 0.5822530388832092 = 0.5139942765235901 + 0.01 * 6.825876235961914
Epoch 210, val loss: 0.791795551776886
Epoch 220, training loss: 0.534270167350769 = 0.466065913438797 + 0.01 * 6.820428848266602
Epoch 220, val loss: 0.7645420432090759
Epoch 230, training loss: 0.4926706850528717 = 0.42453891038894653 + 0.01 * 6.813177585601807
Epoch 230, val loss: 0.74385005235672
Epoch 240, training loss: 0.45516568422317505 = 0.38712286949157715 + 0.01 * 6.804279804229736
Epoch 240, val loss: 0.7273163795471191
Epoch 250, training loss: 0.4198704659938812 = 0.35192596912384033 + 0.01 * 6.794449329376221
Epoch 250, val loss: 0.7132465839385986
Epoch 260, training loss: 0.3855762481689453 = 0.31772515177726746 + 0.01 * 6.78510856628418
Epoch 260, val loss: 0.7005917429924011
Epoch 270, training loss: 0.3517932593822479 = 0.28407910466194153 + 0.01 * 6.771414756774902
Epoch 270, val loss: 0.6893391013145447
Epoch 280, training loss: 0.31879952549934387 = 0.2512267529964447 + 0.01 * 6.757277488708496
Epoch 280, val loss: 0.6797320246696472
Epoch 290, training loss: 0.287381112575531 = 0.2199353277683258 + 0.01 * 6.7445783615112305
Epoch 290, val loss: 0.6721807718276978
Epoch 300, training loss: 0.25847694277763367 = 0.19119901955127716 + 0.01 * 6.727793216705322
Epoch 300, val loss: 0.6673175096511841
Epoch 310, training loss: 0.2329435646533966 = 0.1657744199037552 + 0.01 * 6.716915130615234
Epoch 310, val loss: 0.6651638746261597
Epoch 320, training loss: 0.2109944224357605 = 0.1439303457736969 + 0.01 * 6.70640754699707
Epoch 320, val loss: 0.6657740473747253
Epoch 330, training loss: 0.19238990545272827 = 0.12544411420822144 + 0.01 * 6.694578647613525
Epoch 330, val loss: 0.6689077615737915
Epoch 340, training loss: 0.1767597496509552 = 0.10986001789569855 + 0.01 * 6.689973831176758
Epoch 340, val loss: 0.6741201877593994
Epoch 350, training loss: 0.16350513696670532 = 0.0966743752360344 + 0.01 * 6.683077335357666
Epoch 350, val loss: 0.6808962225914001
Epoch 360, training loss: 0.15223757922649384 = 0.08544684946537018 + 0.01 * 6.679072856903076
Epoch 360, val loss: 0.6888191103935242
Epoch 370, training loss: 0.14257562160491943 = 0.07582627981901169 + 0.01 * 6.674933433532715
Epoch 370, val loss: 0.6974823474884033
Epoch 380, training loss: 0.13435332477092743 = 0.06753914803266525 + 0.01 * 6.681417465209961
Epoch 380, val loss: 0.7066977620124817
Epoch 390, training loss: 0.1270860731601715 = 0.06037985906004906 + 0.01 * 6.670621395111084
Epoch 390, val loss: 0.7162440419197083
Epoch 400, training loss: 0.1208541989326477 = 0.05416705831885338 + 0.01 * 6.6687140464782715
Epoch 400, val loss: 0.7260584235191345
Epoch 410, training loss: 0.11540798842906952 = 0.048755839467048645 + 0.01 * 6.665215015411377
Epoch 410, val loss: 0.7359740138053894
Epoch 420, training loss: 0.11065740883350372 = 0.04402710869908333 + 0.01 * 6.66303014755249
Epoch 420, val loss: 0.7459204196929932
Epoch 430, training loss: 0.1064922958612442 = 0.03988304361701012 + 0.01 * 6.660925388336182
Epoch 430, val loss: 0.7558718323707581
Epoch 440, training loss: 0.10282860696315765 = 0.036241497844457626 + 0.01 * 6.658710956573486
Epoch 440, val loss: 0.7656970024108887
Epoch 450, training loss: 0.09960058331489563 = 0.03303341567516327 + 0.01 * 6.656716823577881
Epoch 450, val loss: 0.7754128575325012
Epoch 460, training loss: 0.09682369232177734 = 0.03020002879202366 + 0.01 * 6.66236686706543
Epoch 460, val loss: 0.7849768400192261
Epoch 470, training loss: 0.09423892199993134 = 0.02769280970096588 + 0.01 * 6.654611110687256
Epoch 470, val loss: 0.7943877577781677
Epoch 480, training loss: 0.09197139739990234 = 0.02546600252389908 + 0.01 * 6.650539875030518
Epoch 480, val loss: 0.8036459684371948
Epoch 490, training loss: 0.08996983617544174 = 0.02348240278661251 + 0.01 * 6.648743152618408
Epoch 490, val loss: 0.8127245903015137
Epoch 500, training loss: 0.0881744846701622 = 0.021710194647312164 + 0.01 * 6.646429061889648
Epoch 500, val loss: 0.8216583132743835
Epoch 510, training loss: 0.0865861028432846 = 0.0201223473995924 + 0.01 * 6.6463751792907715
Epoch 510, val loss: 0.8304077982902527
Epoch 520, training loss: 0.08511888235807419 = 0.018696680665016174 + 0.01 * 6.642220497131348
Epoch 520, val loss: 0.8389939069747925
Epoch 530, training loss: 0.08383969962596893 = 0.017413249239325523 + 0.01 * 6.642645359039307
Epoch 530, val loss: 0.8473618030548096
Epoch 540, training loss: 0.08264169096946716 = 0.016254892572760582 + 0.01 * 6.6386799812316895
Epoch 540, val loss: 0.85555499792099
Epoch 550, training loss: 0.08159623295068741 = 0.015207065269351006 + 0.01 * 6.638916969299316
Epoch 550, val loss: 0.8635585904121399
Epoch 560, training loss: 0.08062083274126053 = 0.014257142320275307 + 0.01 * 6.636369705200195
Epoch 560, val loss: 0.8713695406913757
Epoch 570, training loss: 0.07971522957086563 = 0.013393238186836243 + 0.01 * 6.632199287414551
Epoch 570, val loss: 0.8789806962013245
Epoch 580, training loss: 0.07894188910722733 = 0.012605877593159676 + 0.01 * 6.633601665496826
Epoch 580, val loss: 0.8864297270774841
Epoch 590, training loss: 0.07816838473081589 = 0.011886952444911003 + 0.01 * 6.628142833709717
Epoch 590, val loss: 0.8937191963195801
Epoch 600, training loss: 0.07751280069351196 = 0.011228722520172596 + 0.01 * 6.6284074783325195
Epoch 600, val loss: 0.9008229374885559
Epoch 610, training loss: 0.07686270773410797 = 0.010624888353049755 + 0.01 * 6.623782157897949
Epoch 610, val loss: 0.9077702760696411
Epoch 620, training loss: 0.07632733136415482 = 0.010069824755191803 + 0.01 * 6.625750541687012
Epoch 620, val loss: 0.9145495295524597
Epoch 630, training loss: 0.07576187700033188 = 0.009558990597724915 + 0.01 * 6.620288848876953
Epoch 630, val loss: 0.9212226271629333
Epoch 640, training loss: 0.07526858150959015 = 0.009087607264518738 + 0.01 * 6.61809778213501
Epoch 640, val loss: 0.9276924133300781
Epoch 650, training loss: 0.07482674717903137 = 0.008651698008179665 + 0.01 * 6.6175055503845215
Epoch 650, val loss: 0.9340516328811646
Epoch 660, training loss: 0.07438064366579056 = 0.00824818853288889 + 0.01 * 6.613245964050293
Epoch 660, val loss: 0.9402680397033691
Epoch 670, training loss: 0.07400307059288025 = 0.007873859256505966 + 0.01 * 6.612921237945557
Epoch 670, val loss: 0.9463637471199036
Epoch 680, training loss: 0.07363753765821457 = 0.007525953929871321 + 0.01 * 6.611158847808838
Epoch 680, val loss: 0.9523113369941711
Epoch 690, training loss: 0.0732763260602951 = 0.0072020310908555984 + 0.01 * 6.6074299812316895
Epoch 690, val loss: 0.9581502079963684
Epoch 700, training loss: 0.07310020178556442 = 0.006900156382471323 + 0.01 * 6.620005130767822
Epoch 700, val loss: 0.9638444781303406
Epoch 710, training loss: 0.07265004515647888 = 0.006618421524763107 + 0.01 * 6.6031622886657715
Epoch 710, val loss: 0.9694076180458069
Epoch 720, training loss: 0.07236909121274948 = 0.006355137564241886 + 0.01 * 6.601395606994629
Epoch 720, val loss: 0.9748436808586121
Epoch 730, training loss: 0.07209731638431549 = 0.006108609028160572 + 0.01 * 6.598870277404785
Epoch 730, val loss: 0.980204701423645
Epoch 740, training loss: 0.0718676820397377 = 0.005877346731722355 + 0.01 * 6.599033832550049
Epoch 740, val loss: 0.9854437708854675
Epoch 750, training loss: 0.07160385698080063 = 0.005660088732838631 + 0.01 * 6.594377040863037
Epoch 750, val loss: 0.9905804395675659
Epoch 760, training loss: 0.0714058130979538 = 0.005455911159515381 + 0.01 * 6.594990253448486
Epoch 760, val loss: 0.9956115484237671
Epoch 770, training loss: 0.07118605077266693 = 0.00526390178129077 + 0.01 * 6.592215061187744
Epoch 770, val loss: 1.0005367994308472
Epoch 780, training loss: 0.07097698748111725 = 0.005083056166768074 + 0.01 * 6.589393615722656
Epoch 780, val loss: 1.0053718090057373
Epoch 790, training loss: 0.07084599882364273 = 0.004912427626550198 + 0.01 * 6.593357086181641
Epoch 790, val loss: 1.0101234912872314
Epoch 800, training loss: 0.0706307664513588 = 0.004751232452690601 + 0.01 * 6.587953567504883
Epoch 800, val loss: 1.0147496461868286
Epoch 810, training loss: 0.07040384411811829 = 0.0045990231446921825 + 0.01 * 6.580482482910156
Epoch 810, val loss: 1.0193440914154053
Epoch 820, training loss: 0.07031923532485962 = 0.004455016925930977 + 0.01 * 6.586422443389893
Epoch 820, val loss: 1.0238388776779175
Epoch 830, training loss: 0.0701386108994484 = 0.0043184878304600716 + 0.01 * 6.582012176513672
Epoch 830, val loss: 1.0282154083251953
Epoch 840, training loss: 0.069970041513443 = 0.004189214203506708 + 0.01 * 6.578083038330078
Epoch 840, val loss: 1.0325499773025513
Epoch 850, training loss: 0.06983768939971924 = 0.004066476132720709 + 0.01 * 6.577121257781982
Epoch 850, val loss: 1.0368105173110962
Epoch 860, training loss: 0.06972166895866394 = 0.003949850331991911 + 0.01 * 6.577182292938232
Epoch 860, val loss: 1.040961742401123
Epoch 870, training loss: 0.06954652070999146 = 0.003839147510007024 + 0.01 * 6.570736885070801
Epoch 870, val loss: 1.045097827911377
Epoch 880, training loss: 0.06940609216690063 = 0.003733749268576503 + 0.01 * 6.567234516143799
Epoch 880, val loss: 1.0490976572036743
Epoch 890, training loss: 0.06926414370536804 = 0.0036333983298391104 + 0.01 * 6.563075065612793
Epoch 890, val loss: 1.0531055927276611
Epoch 900, training loss: 0.06913096457719803 = 0.0035377773456275463 + 0.01 * 6.559319496154785
Epoch 900, val loss: 1.0569732189178467
Epoch 910, training loss: 0.06911211460828781 = 0.00344681553542614 + 0.01 * 6.566530227661133
Epoch 910, val loss: 1.060802936553955
Epoch 920, training loss: 0.06893941015005112 = 0.0033599217422306538 + 0.01 * 6.557949066162109
Epoch 920, val loss: 1.0645567178726196
Epoch 930, training loss: 0.06881552934646606 = 0.0032770121470093727 + 0.01 * 6.553852081298828
Epoch 930, val loss: 1.0682611465454102
Epoch 940, training loss: 0.06872683763504028 = 0.0031977291218936443 + 0.01 * 6.552910804748535
Epoch 940, val loss: 1.0719033479690552
Epoch 950, training loss: 0.06865505129098892 = 0.0031219564843922853 + 0.01 * 6.553309917449951
Epoch 950, val loss: 1.0754543542861938
Epoch 960, training loss: 0.06856320798397064 = 0.0030494071543216705 + 0.01 * 6.551380634307861
Epoch 960, val loss: 1.0789473056793213
Epoch 970, training loss: 0.0684223398566246 = 0.0029800874181091785 + 0.01 * 6.544225215911865
Epoch 970, val loss: 1.082405924797058
Epoch 980, training loss: 0.06829538941383362 = 0.0029136340599507093 + 0.01 * 6.5381760597229
Epoch 980, val loss: 1.085742473602295
Epoch 990, training loss: 0.06827874481678009 = 0.002849902492016554 + 0.01 * 6.542884349822998
Epoch 990, val loss: 1.089070439338684
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.6790
Flip ASR: 0.6178/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0290253162384033 = 1.9452883005142212 + 0.01 * 8.373696327209473
Epoch 0, val loss: 1.9389363527297974
Epoch 10, training loss: 2.0191218852996826 = 1.9353857040405273 + 0.01 * 8.373626708984375
Epoch 10, val loss: 1.9285979270935059
Epoch 20, training loss: 2.007392406463623 = 1.9236595630645752 + 0.01 * 8.373283386230469
Epoch 20, val loss: 1.9160922765731812
Epoch 30, training loss: 1.9912678003311157 = 1.907544732093811 + 0.01 * 8.372303009033203
Epoch 30, val loss: 1.8987270593643188
Epoch 40, training loss: 1.967505693435669 = 1.88383150100708 + 0.01 * 8.367423057556152
Epoch 40, val loss: 1.873306155204773
Epoch 50, training loss: 1.932602882385254 = 1.8493096828460693 + 0.01 * 8.32932186126709
Epoch 50, val loss: 1.8375425338745117
Epoch 60, training loss: 1.8856310844421387 = 1.805371880531311 + 0.01 * 8.02591609954834
Epoch 60, val loss: 1.7953886985778809
Epoch 70, training loss: 1.835883378982544 = 1.7599358558654785 + 0.01 * 7.594749450683594
Epoch 70, val loss: 1.7558931112289429
Epoch 80, training loss: 1.7810285091400146 = 1.7083063125610352 + 0.01 * 7.272215843200684
Epoch 80, val loss: 1.7113116979599
Epoch 90, training loss: 1.708166241645813 = 1.6371251344680786 + 0.01 * 7.104116439819336
Epoch 90, val loss: 1.649444818496704
Epoch 100, training loss: 1.6151232719421387 = 1.5447053909301758 + 0.01 * 7.041789531707764
Epoch 100, val loss: 1.570317268371582
Epoch 110, training loss: 1.5085976123809814 = 1.4385679960250854 + 0.01 * 7.002964973449707
Epoch 110, val loss: 1.4827052354812622
Epoch 120, training loss: 1.4001719951629639 = 1.3304932117462158 + 0.01 * 6.9678826332092285
Epoch 120, val loss: 1.3979793787002563
Epoch 130, training loss: 1.2952980995178223 = 1.2260894775390625 + 0.01 * 6.920865535736084
Epoch 130, val loss: 1.3196461200714111
Epoch 140, training loss: 1.194049596786499 = 1.1253489255905151 + 0.01 * 6.870068550109863
Epoch 140, val loss: 1.2464836835861206
Epoch 150, training loss: 1.0954221487045288 = 1.0271542072296143 + 0.01 * 6.826791763305664
Epoch 150, val loss: 1.176244854927063
Epoch 160, training loss: 1.0009214878082275 = 0.9329515099525452 + 0.01 * 6.797001361846924
Epoch 160, val loss: 1.1096760034561157
Epoch 170, training loss: 0.9152615070343018 = 0.8475096821784973 + 0.01 * 6.775183200836182
Epoch 170, val loss: 1.0505985021591187
Epoch 180, training loss: 0.8428267240524292 = 0.7752304673194885 + 0.01 * 6.7596259117126465
Epoch 180, val loss: 1.0024452209472656
Epoch 190, training loss: 0.7844071388244629 = 0.716937780380249 + 0.01 * 6.746939182281494
Epoch 190, val loss: 0.9660404920578003
Epoch 200, training loss: 0.7374075651168823 = 0.6700519919395447 + 0.01 * 6.7355546951293945
Epoch 200, val loss: 0.9397266507148743
Epoch 210, training loss: 0.6979737281799316 = 0.6307024955749512 + 0.01 * 6.727126598358154
Epoch 210, val loss: 0.9206786751747131
Epoch 220, training loss: 0.6623945236206055 = 0.5951957106590271 + 0.01 * 6.719881057739258
Epoch 220, val loss: 0.9057801365852356
Epoch 230, training loss: 0.6275758743286133 = 0.5604259967803955 + 0.01 * 6.714987754821777
Epoch 230, val loss: 0.892888069152832
Epoch 240, training loss: 0.5913442373275757 = 0.5242289900779724 + 0.01 * 6.7115254402160645
Epoch 240, val loss: 0.8805992007255554
Epoch 250, training loss: 0.552537202835083 = 0.48544618487358093 + 0.01 * 6.709104061126709
Epoch 250, val loss: 0.8688437938690186
Epoch 260, training loss: 0.5112075805664062 = 0.44413021206855774 + 0.01 * 6.707734107971191
Epoch 260, val loss: 0.8583346009254456
Epoch 270, training loss: 0.46848994493484497 = 0.4014272093772888 + 0.01 * 6.706274032592773
Epoch 270, val loss: 0.8504678010940552
Epoch 280, training loss: 0.42623430490493774 = 0.35918647050857544 + 0.01 * 6.704783916473389
Epoch 280, val loss: 0.8467865586280823
Epoch 290, training loss: 0.38602951169013977 = 0.31898632645606995 + 0.01 * 6.704319000244141
Epoch 290, val loss: 0.8474843502044678
Epoch 300, training loss: 0.3486148715019226 = 0.28158238530158997 + 0.01 * 6.703250408172607
Epoch 300, val loss: 0.851939857006073
Epoch 310, training loss: 0.31439051032066345 = 0.24737229943275452 + 0.01 * 6.701822280883789
Epoch 310, val loss: 0.8588027358055115
Epoch 320, training loss: 0.28368616104125977 = 0.21667616069316864 + 0.01 * 6.701001167297363
Epoch 320, val loss: 0.8674918413162231
Epoch 330, training loss: 0.25653159618377686 = 0.18953053653240204 + 0.01 * 6.700107097625732
Epoch 330, val loss: 0.8774527311325073
Epoch 340, training loss: 0.23270723223686218 = 0.165711909532547 + 0.01 * 6.699532985687256
Epoch 340, val loss: 0.888380229473114
Epoch 350, training loss: 0.2117592990398407 = 0.14477220177650452 + 0.01 * 6.698709487915039
Epoch 350, val loss: 0.8997383117675781
Epoch 360, training loss: 0.19329190254211426 = 0.126312717795372 + 0.01 * 6.697918891906738
Epoch 360, val loss: 0.911274254322052
Epoch 370, training loss: 0.1770162135362625 = 0.11004474759101868 + 0.01 * 6.697146415710449
Epoch 370, val loss: 0.922859251499176
Epoch 380, training loss: 0.16272783279418945 = 0.09576570987701416 + 0.01 * 6.696211814880371
Epoch 380, val loss: 0.9345009922981262
Epoch 390, training loss: 0.15020844340324402 = 0.08324933797121048 + 0.01 * 6.695911407470703
Epoch 390, val loss: 0.9461928606033325
Epoch 400, training loss: 0.1393134593963623 = 0.07235465198755264 + 0.01 * 6.69588041305542
Epoch 400, val loss: 0.9578878879547119
Epoch 410, training loss: 0.12993071973323822 = 0.06297975778579712 + 0.01 * 6.695096015930176
Epoch 410, val loss: 0.9693177342414856
Epoch 420, training loss: 0.12197765707969666 = 0.05504223704338074 + 0.01 * 6.693542003631592
Epoch 420, val loss: 0.9810567498207092
Epoch 430, training loss: 0.11532007902860641 = 0.048394739627838135 + 0.01 * 6.692534446716309
Epoch 430, val loss: 0.9929948449134827
Epoch 440, training loss: 0.10972876846790314 = 0.04281485825777054 + 0.01 * 6.6913909912109375
Epoch 440, val loss: 1.005016565322876
Epoch 450, training loss: 0.10507218539714813 = 0.03813566640019417 + 0.01 * 6.693652153015137
Epoch 450, val loss: 1.0171252489089966
Epoch 460, training loss: 0.1010722815990448 = 0.03418179228901863 + 0.01 * 6.689049243927002
Epoch 460, val loss: 1.0293071269989014
Epoch 470, training loss: 0.09768574684858322 = 0.030804144218564034 + 0.01 * 6.688160419464111
Epoch 470, val loss: 1.041198492050171
Epoch 480, training loss: 0.09476283937692642 = 0.02789677493274212 + 0.01 * 6.686606407165527
Epoch 480, val loss: 1.0529651641845703
Epoch 490, training loss: 0.09222465753555298 = 0.025379925966262817 + 0.01 * 6.684473514556885
Epoch 490, val loss: 1.0645045042037964
Epoch 500, training loss: 0.09002459794282913 = 0.023186998441815376 + 0.01 * 6.683760166168213
Epoch 500, val loss: 1.0759943723678589
Epoch 510, training loss: 0.08807796984910965 = 0.021268578246235847 + 0.01 * 6.680939197540283
Epoch 510, val loss: 1.0871965885162354
Epoch 520, training loss: 0.08639691770076752 = 0.01958324946463108 + 0.01 * 6.68136739730835
Epoch 520, val loss: 1.0982205867767334
Epoch 530, training loss: 0.08490331470966339 = 0.018093813210725784 + 0.01 * 6.680949687957764
Epoch 530, val loss: 1.1090753078460693
Epoch 540, training loss: 0.08351970463991165 = 0.01677181012928486 + 0.01 * 6.6747894287109375
Epoch 540, val loss: 1.1196520328521729
Epoch 550, training loss: 0.08231759071350098 = 0.01559252105653286 + 0.01 * 6.672507286071777
Epoch 550, val loss: 1.1299976110458374
Epoch 560, training loss: 0.08124317973852158 = 0.014536135829985142 + 0.01 * 6.670704364776611
Epoch 560, val loss: 1.1400845050811768
Epoch 570, training loss: 0.08037069439888 = 0.013586991466581821 + 0.01 * 6.678370475769043
Epoch 570, val loss: 1.1498886346817017
Epoch 580, training loss: 0.0793997272849083 = 0.012731683440506458 + 0.01 * 6.666804313659668
Epoch 580, val loss: 1.1594791412353516
Epoch 590, training loss: 0.07859231531620026 = 0.01195700652897358 + 0.01 * 6.663531303405762
Epoch 590, val loss: 1.1687672138214111
Epoch 600, training loss: 0.07785672694444656 = 0.011253301985561848 + 0.01 * 6.660342216491699
Epoch 600, val loss: 1.177956223487854
Epoch 610, training loss: 0.07718906551599503 = 0.010612313635647297 + 0.01 * 6.657675266265869
Epoch 610, val loss: 1.1867668628692627
Epoch 620, training loss: 0.0766431987285614 = 0.010028616525232792 + 0.01 * 6.661458492279053
Epoch 620, val loss: 1.1954455375671387
Epoch 630, training loss: 0.07605480402708054 = 0.00949485320597887 + 0.01 * 6.655994892120361
Epoch 630, val loss: 1.2038451433181763
Epoch 640, training loss: 0.0755063146352768 = 0.00900582317262888 + 0.01 * 6.650049686431885
Epoch 640, val loss: 1.2121137380599976
Epoch 650, training loss: 0.0750076174736023 = 0.008556311018764973 + 0.01 * 6.645130634307861
Epoch 650, val loss: 1.2200984954833984
Epoch 660, training loss: 0.07457618415355682 = 0.008138438686728477 + 0.01 * 6.643774509429932
Epoch 660, val loss: 1.227858066558838
Epoch 670, training loss: 0.07413001358509064 = 0.007751242257654667 + 0.01 * 6.637877464294434
Epoch 670, val loss: 1.2355636358261108
Epoch 680, training loss: 0.07403251528739929 = 0.007392250932753086 + 0.01 * 6.664026737213135
Epoch 680, val loss: 1.2429790496826172
Epoch 690, training loss: 0.07346479594707489 = 0.007061819080263376 + 0.01 * 6.640298366546631
Epoch 690, val loss: 1.2502835988998413
Epoch 700, training loss: 0.07304726541042328 = 0.006758222822099924 + 0.01 * 6.628904819488525
Epoch 700, val loss: 1.2573195695877075
Epoch 710, training loss: 0.07272899895906448 = 0.006475211586803198 + 0.01 * 6.6253790855407715
Epoch 710, val loss: 1.2642914056777954
Epoch 720, training loss: 0.07241880893707275 = 0.006211530417203903 + 0.01 * 6.620728492736816
Epoch 720, val loss: 1.2710540294647217
Epoch 730, training loss: 0.0722222775220871 = 0.005965549033135176 + 0.01 * 6.625673294067383
Epoch 730, val loss: 1.2775827646255493
Epoch 740, training loss: 0.07188163697719574 = 0.005736096296459436 + 0.01 * 6.614553928375244
Epoch 740, val loss: 1.284119725227356
Epoch 750, training loss: 0.07155497372150421 = 0.005522402469068766 + 0.01 * 6.603257656097412
Epoch 750, val loss: 1.2902778387069702
Epoch 760, training loss: 0.07135094702243805 = 0.005322110373526812 + 0.01 * 6.602883338928223
Epoch 760, val loss: 1.296356201171875
Epoch 770, training loss: 0.07136724889278412 = 0.005134224891662598 + 0.01 * 6.623302459716797
Epoch 770, val loss: 1.302398681640625
Epoch 780, training loss: 0.07092021405696869 = 0.004957420751452446 + 0.01 * 6.596279621124268
Epoch 780, val loss: 1.3081386089324951
Epoch 790, training loss: 0.07072053849697113 = 0.004791207145899534 + 0.01 * 6.592933177947998
Epoch 790, val loss: 1.313751220703125
Epoch 800, training loss: 0.07052049785852432 = 0.00463447580114007 + 0.01 * 6.588602542877197
Epoch 800, val loss: 1.3193646669387817
Epoch 810, training loss: 0.07029607146978378 = 0.004486516118049622 + 0.01 * 6.580955982208252
Epoch 810, val loss: 1.3245998620986938
Epoch 820, training loss: 0.07012200355529785 = 0.004346794914454222 + 0.01 * 6.577520847320557
Epoch 820, val loss: 1.330000877380371
Epoch 830, training loss: 0.06992838531732559 = 0.00421452010050416 + 0.01 * 6.571386814117432
Epoch 830, val loss: 1.3350056409835815
Epoch 840, training loss: 0.06980451941490173 = 0.0040892548859119415 + 0.01 * 6.571527004241943
Epoch 840, val loss: 1.3401063680648804
Epoch 850, training loss: 0.06963552534580231 = 0.00397046934813261 + 0.01 * 6.5665059089660645
Epoch 850, val loss: 1.345050573348999
Epoch 860, training loss: 0.06957526504993439 = 0.003857656614854932 + 0.01 * 6.571760654449463
Epoch 860, val loss: 1.3497146368026733
Epoch 870, training loss: 0.06943660974502563 = 0.0037508688401430845 + 0.01 * 6.56857442855835
Epoch 870, val loss: 1.3545171022415161
Epoch 880, training loss: 0.06942135095596313 = 0.003649350255727768 + 0.01 * 6.577199935913086
Epoch 880, val loss: 1.3590599298477173
Epoch 890, training loss: 0.06908142566680908 = 0.003552541835233569 + 0.01 * 6.552888870239258
Epoch 890, val loss: 1.3636189699172974
Epoch 900, training loss: 0.06902594864368439 = 0.00345992692746222 + 0.01 * 6.5566020011901855
Epoch 900, val loss: 1.367983341217041
Epoch 910, training loss: 0.06883125752210617 = 0.0033714156597852707 + 0.01 * 6.545983791351318
Epoch 910, val loss: 1.3722388744354248
Epoch 920, training loss: 0.0688721239566803 = 0.0032869239803403616 + 0.01 * 6.558520317077637
Epoch 920, val loss: 1.3764114379882812
Epoch 930, training loss: 0.06872644275426865 = 0.0032064151018857956 + 0.01 * 6.552002429962158
Epoch 930, val loss: 1.3805688619613647
Epoch 940, training loss: 0.06856083869934082 = 0.0031295414082705975 + 0.01 * 6.543129920959473
Epoch 940, val loss: 1.384566068649292
Epoch 950, training loss: 0.0684451088309288 = 0.0030562637839466333 + 0.01 * 6.53888463973999
Epoch 950, val loss: 1.3885899782180786
Epoch 960, training loss: 0.06849251687526703 = 0.0029862551018595695 + 0.01 * 6.550626277923584
Epoch 960, val loss: 1.3924980163574219
Epoch 970, training loss: 0.06830613315105438 = 0.002919364720582962 + 0.01 * 6.5386762619018555
Epoch 970, val loss: 1.3962347507476807
Epoch 980, training loss: 0.06816940754652023 = 0.0028552687726914883 + 0.01 * 6.531413555145264
Epoch 980, val loss: 1.3999186754226685
Epoch 990, training loss: 0.06815837323665619 = 0.002793692983686924 + 0.01 * 6.536468029022217
Epoch 990, val loss: 1.4036040306091309
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7667
Overall ASR: 0.8339
Flip ASR: 0.8178/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0253994464874268 = 1.9416626691818237 + 0.01 * 8.373669624328613
Epoch 0, val loss: 1.94143807888031
Epoch 10, training loss: 2.0160813331604004 = 1.932345986366272 + 0.01 * 8.373546600341797
Epoch 10, val loss: 1.9326295852661133
Epoch 20, training loss: 2.004453182220459 = 1.9207229614257812 + 0.01 * 8.37303352355957
Epoch 20, val loss: 1.9212580919265747
Epoch 30, training loss: 1.9877023696899414 = 1.903991937637329 + 0.01 * 8.371042251586914
Epoch 30, val loss: 1.9046789407730103
Epoch 40, training loss: 1.9621495008468628 = 1.8785845041275024 + 0.01 * 8.356505393981934
Epoch 40, val loss: 1.8796597719192505
Epoch 50, training loss: 1.924100637435913 = 1.8416774272918701 + 0.01 * 8.242318153381348
Epoch 50, val loss: 1.8447214365005493
Epoch 60, training loss: 1.8753907680511475 = 1.7971035242080688 + 0.01 * 7.828727722167969
Epoch 60, val loss: 1.8055155277252197
Epoch 70, training loss: 1.8290622234344482 = 1.754034399986267 + 0.01 * 7.502781867980957
Epoch 70, val loss: 1.7685598134994507
Epoch 80, training loss: 1.773874282836914 = 1.701784372329712 + 0.01 * 7.208993434906006
Epoch 80, val loss: 1.720908761024475
Epoch 90, training loss: 1.6998209953308105 = 1.6295597553253174 + 0.01 * 7.026130199432373
Epoch 90, val loss: 1.6589024066925049
Epoch 100, training loss: 1.6050398349761963 = 1.5354989767074585 + 0.01 * 6.954090595245361
Epoch 100, val loss: 1.5812592506408691
Epoch 110, training loss: 1.4958397150039673 = 1.426642894744873 + 0.01 * 6.919687271118164
Epoch 110, val loss: 1.4918876886367798
Epoch 120, training loss: 1.3850799798965454 = 1.3161145448684692 + 0.01 * 6.896545886993408
Epoch 120, val loss: 1.4033807516098022
Epoch 130, training loss: 1.2804324626922607 = 1.211665391921997 + 0.01 * 6.876707077026367
Epoch 130, val loss: 1.3211477994918823
Epoch 140, training loss: 1.1861555576324463 = 1.1176069974899292 + 0.01 * 6.854851245880127
Epoch 140, val loss: 1.2491307258605957
Epoch 150, training loss: 1.1038397550582886 = 1.0354958772659302 + 0.01 * 6.834393501281738
Epoch 150, val loss: 1.188257098197937
Epoch 160, training loss: 1.032065749168396 = 0.9638872146606445 + 0.01 * 6.81785249710083
Epoch 160, val loss: 1.1365207433700562
Epoch 170, training loss: 0.9670379757881165 = 0.8989576697349548 + 0.01 * 6.808030128479004
Epoch 170, val loss: 1.09058678150177
Epoch 180, training loss: 0.9046971797943115 = 0.8367141485214233 + 0.01 * 6.798301696777344
Epoch 180, val loss: 1.0467594861984253
Epoch 190, training loss: 0.8432098031044006 = 0.7753047943115234 + 0.01 * 6.790501117706299
Epoch 190, val loss: 1.003138542175293
Epoch 200, training loss: 0.783452033996582 = 0.7156338691711426 + 0.01 * 6.781816005706787
Epoch 200, val loss: 0.9606639742851257
Epoch 210, training loss: 0.7274308800697327 = 0.6596929430961609 + 0.01 * 6.773795127868652
Epoch 210, val loss: 0.9213669896125793
Epoch 220, training loss: 0.675754725933075 = 0.6081072092056274 + 0.01 * 6.7647504806518555
Epoch 220, val loss: 0.8861492872238159
Epoch 230, training loss: 0.6268225312232971 = 0.5592579245567322 + 0.01 * 6.756458759307861
Epoch 230, val loss: 0.8546229600906372
Epoch 240, training loss: 0.5783354043960571 = 0.5108023285865784 + 0.01 * 6.75330924987793
Epoch 240, val loss: 0.8254402279853821
Epoch 250, training loss: 0.5287957191467285 = 0.46136510372161865 + 0.01 * 6.743062496185303
Epoch 250, val loss: 0.7980471849441528
Epoch 260, training loss: 0.4783380925655365 = 0.41096803545951843 + 0.01 * 6.737005710601807
Epoch 260, val loss: 0.7725198268890381
Epoch 270, training loss: 0.4281817078590393 = 0.36088496446609497 + 0.01 * 6.729674816131592
Epoch 270, val loss: 0.7495810985565186
Epoch 280, training loss: 0.3801330327987671 = 0.31288477778434753 + 0.01 * 6.72482442855835
Epoch 280, val loss: 0.729975700378418
Epoch 290, training loss: 0.33576640486717224 = 0.26859012246131897 + 0.01 * 6.717629432678223
Epoch 290, val loss: 0.7144150733947754
Epoch 300, training loss: 0.29629379510879517 = 0.22916202247142792 + 0.01 * 6.713179111480713
Epoch 300, val loss: 0.7033967971801758
Epoch 310, training loss: 0.2623560130596161 = 0.19524703919887543 + 0.01 * 6.7108964920043945
Epoch 310, val loss: 0.6973814368247986
Epoch 320, training loss: 0.23385468125343323 = 0.16679880023002625 + 0.01 * 6.7055888175964355
Epoch 320, val loss: 0.6963343024253845
Epoch 330, training loss: 0.21035057306289673 = 0.14326058328151703 + 0.01 * 6.708999156951904
Epoch 330, val loss: 0.6997905969619751
Epoch 340, training loss: 0.19085517525672913 = 0.1238786056637764 + 0.01 * 6.697657585144043
Epoch 340, val loss: 0.7070226073265076
Epoch 350, training loss: 0.1748407483100891 = 0.10785791277885437 + 0.01 * 6.698284149169922
Epoch 350, val loss: 0.716950535774231
Epoch 360, training loss: 0.1614299714565277 = 0.09450704604387283 + 0.01 * 6.6922926902771
Epoch 360, val loss: 0.7287595272064209
Epoch 370, training loss: 0.15011408925056458 = 0.08326882869005203 + 0.01 * 6.684525489807129
Epoch 370, val loss: 0.7418124079704285
Epoch 380, training loss: 0.14057296514511108 = 0.07371029257774353 + 0.01 * 6.686268329620361
Epoch 380, val loss: 0.7556638121604919
Epoch 390, training loss: 0.13230538368225098 = 0.06551598757505417 + 0.01 * 6.678938865661621
Epoch 390, val loss: 0.7699763774871826
Epoch 400, training loss: 0.12516728043556213 = 0.05843760073184967 + 0.01 * 6.672967433929443
Epoch 400, val loss: 0.7845409512519836
Epoch 410, training loss: 0.11904319375753403 = 0.052294157445430756 + 0.01 * 6.674903869628906
Epoch 410, val loss: 0.7992711067199707
Epoch 420, training loss: 0.1136130541563034 = 0.04694783315062523 + 0.01 * 6.666522979736328
Epoch 420, val loss: 0.8141341805458069
Epoch 430, training loss: 0.10887737572193146 = 0.04228133708238602 + 0.01 * 6.659604072570801
Epoch 430, val loss: 0.8290593028068542
Epoch 440, training loss: 0.10471905767917633 = 0.03819727525115013 + 0.01 * 6.652177810668945
Epoch 440, val loss: 0.8439468741416931
Epoch 450, training loss: 0.10129882395267487 = 0.034615371376276016 + 0.01 * 6.6683454513549805
Epoch 450, val loss: 0.8588306307792664
Epoch 460, training loss: 0.0979139655828476 = 0.031473081558942795 + 0.01 * 6.644089221954346
Epoch 460, val loss: 0.873585045337677
Epoch 470, training loss: 0.09507090598344803 = 0.02870878577232361 + 0.01 * 6.63621187210083
Epoch 470, val loss: 0.88808274269104
Epoch 480, training loss: 0.09292516857385635 = 0.026270529255270958 + 0.01 * 6.665463924407959
Epoch 480, val loss: 0.9023479223251343
Epoch 490, training loss: 0.09039704501628876 = 0.024118399247527122 + 0.01 * 6.627864360809326
Epoch 490, val loss: 0.9161783456802368
Epoch 500, training loss: 0.08850133419036865 = 0.022211866453289986 + 0.01 * 6.628946781158447
Epoch 500, val loss: 0.9295793175697327
Epoch 510, training loss: 0.08671398460865021 = 0.02051704004406929 + 0.01 * 6.61969518661499
Epoch 510, val loss: 0.9426746964454651
Epoch 520, training loss: 0.08512528240680695 = 0.019006405025720596 + 0.01 * 6.611887454986572
Epoch 520, val loss: 0.9552748203277588
Epoch 530, training loss: 0.08376240730285645 = 0.017655987292528152 + 0.01 * 6.610641956329346
Epoch 530, val loss: 0.9675046801567078
Epoch 540, training loss: 0.08267924189567566 = 0.016442878171801567 + 0.01 * 6.623636245727539
Epoch 540, val loss: 0.9793053269386292
Epoch 550, training loss: 0.08141356706619263 = 0.015351040288805962 + 0.01 * 6.606252670288086
Epoch 550, val loss: 0.9907897710800171
Epoch 560, training loss: 0.08033258467912674 = 0.014363202266395092 + 0.01 * 6.596938610076904
Epoch 560, val loss: 1.001801609992981
Epoch 570, training loss: 0.07937080413103104 = 0.0134652154520154 + 0.01 * 6.590559005737305
Epoch 570, val loss: 1.0125564336776733
Epoch 580, training loss: 0.0785350427031517 = 0.012646016664803028 + 0.01 * 6.588902473449707
Epoch 580, val loss: 1.0230003595352173
Epoch 590, training loss: 0.07792223989963531 = 0.011898286640644073 + 0.01 * 6.602395534515381
Epoch 590, val loss: 1.0330625772476196
Epoch 600, training loss: 0.07700744271278381 = 0.011215170845389366 + 0.01 * 6.579226970672607
Epoch 600, val loss: 1.042875051498413
Epoch 610, training loss: 0.07631592452526093 = 0.010588439181447029 + 0.01 * 6.572749137878418
Epoch 610, val loss: 1.0524541139602661
Epoch 620, training loss: 0.07577111572027206 = 0.010012514889240265 + 0.01 * 6.575860500335693
Epoch 620, val loss: 1.0617015361785889
Epoch 630, training loss: 0.07515770196914673 = 0.009483259171247482 + 0.01 * 6.567444801330566
Epoch 630, val loss: 1.0707122087478638
Epoch 640, training loss: 0.0747702419757843 = 0.0089962063357234 + 0.01 * 6.577404022216797
Epoch 640, val loss: 1.0794838666915894
Epoch 650, training loss: 0.07418037205934525 = 0.008547901175916195 + 0.01 * 6.563247203826904
Epoch 650, val loss: 1.0879310369491577
Epoch 660, training loss: 0.07379826158285141 = 0.008133714087307453 + 0.01 * 6.566454887390137
Epoch 660, val loss: 1.0960896015167236
Epoch 670, training loss: 0.0732806846499443 = 0.007751015946269035 + 0.01 * 6.552966594696045
Epoch 670, val loss: 1.10417640209198
Epoch 680, training loss: 0.07293407618999481 = 0.007396575063467026 + 0.01 * 6.553750514984131
Epoch 680, val loss: 1.1119017601013184
Epoch 690, training loss: 0.07248940318822861 = 0.007067798171192408 + 0.01 * 6.542160511016846
Epoch 690, val loss: 1.119489073753357
Epoch 700, training loss: 0.07227108627557755 = 0.006762032397091389 + 0.01 * 6.550905227661133
Epoch 700, val loss: 1.1267874240875244
Epoch 710, training loss: 0.0719941109418869 = 0.006477782968431711 + 0.01 * 6.551632881164551
Epoch 710, val loss: 1.1340250968933105
Epoch 720, training loss: 0.07152190804481506 = 0.006212989799678326 + 0.01 * 6.5308918952941895
Epoch 720, val loss: 1.1409459114074707
Epoch 730, training loss: 0.07125041633844376 = 0.005965673830360174 + 0.01 * 6.528474807739258
Epoch 730, val loss: 1.147692322731018
Epoch 740, training loss: 0.07111486047506332 = 0.005734574515372515 + 0.01 * 6.538028717041016
Epoch 740, val loss: 1.1544021368026733
Epoch 750, training loss: 0.07095544040203094 = 0.0055182017385959625 + 0.01 * 6.543724536895752
Epoch 750, val loss: 1.160792589187622
Epoch 760, training loss: 0.07051710039377213 = 0.005316311027854681 + 0.01 * 6.520079135894775
Epoch 760, val loss: 1.1670893430709839
Epoch 770, training loss: 0.07036187499761581 = 0.0051267207600176334 + 0.01 * 6.523515701293945
Epoch 770, val loss: 1.173154592514038
Epoch 780, training loss: 0.07009284198284149 = 0.004948577377945185 + 0.01 * 6.514426231384277
Epoch 780, val loss: 1.1791203022003174
Epoch 790, training loss: 0.07011588662862778 = 0.0047812252305448055 + 0.01 * 6.533466339111328
Epoch 790, val loss: 1.184979796409607
Epoch 800, training loss: 0.06971216201782227 = 0.004623375833034515 + 0.01 * 6.508878707885742
Epoch 800, val loss: 1.1906039714813232
Epoch 810, training loss: 0.06958675384521484 = 0.004474489018321037 + 0.01 * 6.511226654052734
Epoch 810, val loss: 1.1961030960083008
Epoch 820, training loss: 0.0692576989531517 = 0.00433428306132555 + 0.01 * 6.4923415184021
Epoch 820, val loss: 1.2015230655670166
Epoch 830, training loss: 0.06923042982816696 = 0.004201692994683981 + 0.01 * 6.50287389755249
Epoch 830, val loss: 1.2067755460739136
Epoch 840, training loss: 0.06888268142938614 = 0.004076187498867512 + 0.01 * 6.480649471282959
Epoch 840, val loss: 1.2118804454803467
Epoch 850, training loss: 0.06868388503789902 = 0.0039571854285895824 + 0.01 * 6.472670555114746
Epoch 850, val loss: 1.2168680429458618
Epoch 860, training loss: 0.0691116452217102 = 0.003844547551125288 + 0.01 * 6.526710510253906
Epoch 860, val loss: 1.2217260599136353
Epoch 870, training loss: 0.06837643682956696 = 0.003737937891855836 + 0.01 * 6.463850021362305
Epoch 870, val loss: 1.2265270948410034
Epoch 880, training loss: 0.0684494599699974 = 0.003636707318946719 + 0.01 * 6.4812750816345215
Epoch 880, val loss: 1.2311311960220337
Epoch 890, training loss: 0.06815335154533386 = 0.0035404383670538664 + 0.01 * 6.461291313171387
Epoch 890, val loss: 1.235714316368103
Epoch 900, training loss: 0.06817266345024109 = 0.0034487040247768164 + 0.01 * 6.472396373748779
Epoch 900, val loss: 1.2401374578475952
Epoch 910, training loss: 0.06782262772321701 = 0.0033615201245993376 + 0.01 * 6.446110248565674
Epoch 910, val loss: 1.2445735931396484
Epoch 920, training loss: 0.0676463320851326 = 0.0032782431226223707 + 0.01 * 6.436809539794922
Epoch 920, val loss: 1.2487608194351196
Epoch 930, training loss: 0.06792675703763962 = 0.0031989356502890587 + 0.01 * 6.472782135009766
Epoch 930, val loss: 1.2529454231262207
Epoch 940, training loss: 0.06762845814228058 = 0.0031232053879648447 + 0.01 * 6.450524806976318
Epoch 940, val loss: 1.2569599151611328
Epoch 950, training loss: 0.06754689663648605 = 0.003050840925425291 + 0.01 * 6.449605941772461
Epoch 950, val loss: 1.2609394788742065
Epoch 960, training loss: 0.0673571527004242 = 0.0029815537855029106 + 0.01 * 6.437560081481934
Epoch 960, val loss: 1.2649192810058594
Epoch 970, training loss: 0.06722654402256012 = 0.002915349556133151 + 0.01 * 6.431118965148926
Epoch 970, val loss: 1.2687486410140991
Epoch 980, training loss: 0.06702646613121033 = 0.0028517995961010456 + 0.01 * 6.417466640472412
Epoch 980, val loss: 1.272472620010376
Epoch 990, training loss: 0.06706997752189636 = 0.0027910247445106506 + 0.01 * 6.4278950691223145
Epoch 990, val loss: 1.2761363983154297
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.8598
Flip ASR: 0.8311/225 nodes
The final ASR:0.79090, 0.07985, Accuracy:0.79877, 0.02444
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11558])
remove edge: torch.Size([2, 9570])
updated graph: torch.Size([2, 10572])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.98155, 0.00797, Accuracy:0.83333, 0.00907
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.4 der2:0.3 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.019928455352783 = 1.936189889907837 + 0.01 * 8.373846054077148
Epoch 0, val loss: 1.9284067153930664
Epoch 10, training loss: 2.0100648403167725 = 1.9263269901275635 + 0.01 * 8.373783111572266
Epoch 10, val loss: 1.9193122386932373
Epoch 20, training loss: 1.9980056285858154 = 1.9142699241638184 + 0.01 * 8.37357234954834
Epoch 20, val loss: 1.9080082178115845
Epoch 30, training loss: 1.9811619520187378 = 1.8974323272705078 + 0.01 * 8.372958183288574
Epoch 30, val loss: 1.8920679092407227
Epoch 40, training loss: 1.9565192461013794 = 1.8728187084197998 + 0.01 * 8.370055198669434
Epoch 40, val loss: 1.8690546751022339
Epoch 50, training loss: 1.9220683574676514 = 1.8385683298110962 + 0.01 * 8.350007057189941
Epoch 50, val loss: 1.8386893272399902
Epoch 60, training loss: 1.8814283609390259 = 1.7990373373031616 + 0.01 * 8.239103317260742
Epoch 60, val loss: 1.8075470924377441
Epoch 70, training loss: 1.8413103818893433 = 1.7615615129470825 + 0.01 * 7.974882125854492
Epoch 70, val loss: 1.7795013189315796
Epoch 80, training loss: 1.7902402877807617 = 1.714285135269165 + 0.01 * 7.595510005950928
Epoch 80, val loss: 1.7381246089935303
Epoch 90, training loss: 1.721078634262085 = 1.6483283042907715 + 0.01 * 7.275028228759766
Epoch 90, val loss: 1.6812325716018677
Epoch 100, training loss: 1.634002685546875 = 1.5627022981643677 + 0.01 * 7.1300368309021
Epoch 100, val loss: 1.611178994178772
Epoch 110, training loss: 1.535036563873291 = 1.4642895460128784 + 0.01 * 7.074707984924316
Epoch 110, val loss: 1.5319451093673706
Epoch 120, training loss: 1.4336464405059814 = 1.363261103630066 + 0.01 * 7.038538932800293
Epoch 120, val loss: 1.45149827003479
Epoch 130, training loss: 1.3326268196105957 = 1.2624744176864624 + 0.01 * 7.015239715576172
Epoch 130, val loss: 1.3708072900772095
Epoch 140, training loss: 1.2319105863571167 = 1.1619787216186523 + 0.01 * 6.993188858032227
Epoch 140, val loss: 1.2905137538909912
Epoch 150, training loss: 1.132523775100708 = 1.0628283023834229 + 0.01 * 6.9695515632629395
Epoch 150, val loss: 1.211910367012024
Epoch 160, training loss: 1.0356152057647705 = 0.9660976529121399 + 0.01 * 6.951755523681641
Epoch 160, val loss: 1.1367021799087524
Epoch 170, training loss: 0.9411733746528625 = 0.8718054890632629 + 0.01 * 6.9367899894714355
Epoch 170, val loss: 1.0651557445526123
Epoch 180, training loss: 0.8492902517318726 = 0.7800122499465942 + 0.01 * 6.92780065536499
Epoch 180, val loss: 0.9969517588615417
Epoch 190, training loss: 0.7615640759468079 = 0.6923583745956421 + 0.01 * 6.9205708503723145
Epoch 190, val loss: 0.9329869747161865
Epoch 200, training loss: 0.6812936067581177 = 0.6120891571044922 + 0.01 * 6.920445442199707
Epoch 200, val loss: 0.8765278458595276
Epoch 210, training loss: 0.6112569570541382 = 0.5421202182769775 + 0.01 * 6.91367244720459
Epoch 210, val loss: 0.8305896520614624
Epoch 220, training loss: 0.552093505859375 = 0.48301389813423157 + 0.01 * 6.907961368560791
Epoch 220, val loss: 0.7961153984069824
Epoch 230, training loss: 0.5023028254508972 = 0.4332846999168396 + 0.01 * 6.9018144607543945
Epoch 230, val loss: 0.7718822360038757
Epoch 240, training loss: 0.45963889360427856 = 0.3905736207962036 + 0.01 * 6.9065260887146
Epoch 240, val loss: 0.7551737427711487
Epoch 250, training loss: 0.4215000867843628 = 0.3525846004486084 + 0.01 * 6.891549587249756
Epoch 250, val loss: 0.7434994578361511
Epoch 260, training loss: 0.38641357421875 = 0.31752926111221313 + 0.01 * 6.888431072235107
Epoch 260, val loss: 0.7351815700531006
Epoch 270, training loss: 0.3531525135040283 = 0.28433090448379517 + 0.01 * 6.882160663604736
Epoch 270, val loss: 0.7290738224983215
Epoch 280, training loss: 0.32134366035461426 = 0.2525712847709656 + 0.01 * 6.8772358894348145
Epoch 280, val loss: 0.7244057655334473
Epoch 290, training loss: 0.2911224961280823 = 0.22238978743553162 + 0.01 * 6.873270034790039
Epoch 290, val loss: 0.7208132743835449
Epoch 300, training loss: 0.26303619146347046 = 0.19433097541332245 + 0.01 * 6.870521068572998
Epoch 300, val loss: 0.7187171578407288
Epoch 310, training loss: 0.23764768242835999 = 0.16903690993785858 + 0.01 * 6.861077308654785
Epoch 310, val loss: 0.7185551524162292
Epoch 320, training loss: 0.21541175246238708 = 0.14688250422477722 + 0.01 * 6.8529253005981445
Epoch 320, val loss: 0.7209230661392212
Epoch 330, training loss: 0.19630753993988037 = 0.12785424292087555 + 0.01 * 6.845330715179443
Epoch 330, val loss: 0.7259840369224548
Epoch 340, training loss: 0.1800396740436554 = 0.11166077107191086 + 0.01 * 6.837889671325684
Epoch 340, val loss: 0.7334082722663879
Epoch 350, training loss: 0.16621387004852295 = 0.09789808839559555 + 0.01 * 6.831577777862549
Epoch 350, val loss: 0.7429240942001343
Epoch 360, training loss: 0.15442445874214172 = 0.0861760824918747 + 0.01 * 6.824837684631348
Epoch 360, val loss: 0.7539880871772766
Epoch 370, training loss: 0.1443067044019699 = 0.07615075260400772 + 0.01 * 6.815595626831055
Epoch 370, val loss: 0.7661078572273254
Epoch 380, training loss: 0.13565754890441895 = 0.06754866242408752 + 0.01 * 6.810887813568115
Epoch 380, val loss: 0.7789312601089478
Epoch 390, training loss: 0.12816673517227173 = 0.06014501303434372 + 0.01 * 6.802172660827637
Epoch 390, val loss: 0.7920767664909363
Epoch 400, training loss: 0.12171479314565659 = 0.05375460535287857 + 0.01 * 6.796018600463867
Epoch 400, val loss: 0.8053948283195496
Epoch 410, training loss: 0.11612969636917114 = 0.04822096973657608 + 0.01 * 6.790872573852539
Epoch 410, val loss: 0.8186577558517456
Epoch 420, training loss: 0.11119307577610016 = 0.04340806603431702 + 0.01 * 6.778501033782959
Epoch 420, val loss: 0.8317431807518005
Epoch 430, training loss: 0.10726943612098694 = 0.039210330694913864 + 0.01 * 6.805910587310791
Epoch 430, val loss: 0.8445730805397034
Epoch 440, training loss: 0.10331124067306519 = 0.03555116429924965 + 0.01 * 6.776007652282715
Epoch 440, val loss: 0.8569875955581665
Epoch 450, training loss: 0.10002274811267853 = 0.0323442779481411 + 0.01 * 6.767846584320068
Epoch 450, val loss: 0.8690783381462097
Epoch 460, training loss: 0.09708143770694733 = 0.029519902542233467 + 0.01 * 6.7561540603637695
Epoch 460, val loss: 0.880894124507904
Epoch 470, training loss: 0.09448552876710892 = 0.027022795751690865 + 0.01 * 6.746273994445801
Epoch 470, val loss: 0.8923934102058411
Epoch 480, training loss: 0.09251685440540314 = 0.02481008879840374 + 0.01 * 6.770676612854004
Epoch 480, val loss: 0.9035875201225281
Epoch 490, training loss: 0.09028955549001694 = 0.02285424806177616 + 0.01 * 6.743530750274658
Epoch 490, val loss: 0.9144380688667297
Epoch 500, training loss: 0.08846767991781235 = 0.021113360300660133 + 0.01 * 6.735432147979736
Epoch 500, val loss: 0.9249379634857178
Epoch 510, training loss: 0.08701559901237488 = 0.01955900713801384 + 0.01 * 6.745659351348877
Epoch 510, val loss: 0.9351786375045776
Epoch 520, training loss: 0.08548523485660553 = 0.018171031028032303 + 0.01 * 6.731420516967773
Epoch 520, val loss: 0.9450498223304749
Epoch 530, training loss: 0.08405611664056778 = 0.016924485564231873 + 0.01 * 6.713163375854492
Epoch 530, val loss: 0.9546917080879211
Epoch 540, training loss: 0.08290368318557739 = 0.015802007168531418 + 0.01 * 6.710168361663818
Epoch 540, val loss: 0.9640505313873291
Epoch 550, training loss: 0.0817544162273407 = 0.014787139371037483 + 0.01 * 6.696728229522705
Epoch 550, val loss: 0.9730851054191589
Epoch 560, training loss: 0.08097708225250244 = 0.013864831067621708 + 0.01 * 6.7112250328063965
Epoch 560, val loss: 0.9819250106811523
Epoch 570, training loss: 0.08005982637405396 = 0.01303060818463564 + 0.01 * 6.7029218673706055
Epoch 570, val loss: 0.9904220104217529
Epoch 580, training loss: 0.07907884567975998 = 0.012273216620087624 + 0.01 * 6.680563449859619
Epoch 580, val loss: 0.9987274408340454
Epoch 590, training loss: 0.07851693779230118 = 0.011581956408917904 + 0.01 * 6.693498134613037
Epoch 590, val loss: 1.0067921876907349
Epoch 600, training loss: 0.07780379056930542 = 0.010950249619781971 + 0.01 * 6.685353755950928
Epoch 600, val loss: 1.0145361423492432
Epoch 610, training loss: 0.07711991667747498 = 0.010371573269367218 + 0.01 * 6.674834728240967
Epoch 610, val loss: 1.0221538543701172
Epoch 620, training loss: 0.07656187564134598 = 0.009839944541454315 + 0.01 * 6.6721930503845215
Epoch 620, val loss: 1.0295311212539673
Epoch 630, training loss: 0.07610836625099182 = 0.009350940585136414 + 0.01 * 6.6757426261901855
Epoch 630, val loss: 1.0367335081100464
Epoch 640, training loss: 0.07552505284547806 = 0.008899730630218983 + 0.01 * 6.662531852722168
Epoch 640, val loss: 1.0436943769454956
Epoch 650, training loss: 0.0753060132265091 = 0.00848260335624218 + 0.01 * 6.682340621948242
Epoch 650, val loss: 1.05050528049469
Epoch 660, training loss: 0.0746750757098198 = 0.008096939884126186 + 0.01 * 6.657813549041748
Epoch 660, val loss: 1.0571144819259644
Epoch 670, training loss: 0.07426384091377258 = 0.007738744840025902 + 0.01 * 6.652509689331055
Epoch 670, val loss: 1.063544750213623
Epoch 680, training loss: 0.07387541234493256 = 0.0074060470797121525 + 0.01 * 6.646936893463135
Epoch 680, val loss: 1.069804072380066
Epoch 690, training loss: 0.07356321066617966 = 0.007096112240105867 + 0.01 * 6.646710395812988
Epoch 690, val loss: 1.0759177207946777
Epoch 700, training loss: 0.07321152091026306 = 0.006807372905313969 + 0.01 * 6.640414714813232
Epoch 700, val loss: 1.081786870956421
Epoch 710, training loss: 0.07294163852930069 = 0.006537737790495157 + 0.01 * 6.640390396118164
Epoch 710, val loss: 1.0876071453094482
Epoch 720, training loss: 0.07250900566577911 = 0.006285530980676413 + 0.01 * 6.622347354888916
Epoch 720, val loss: 1.0932022333145142
Epoch 730, training loss: 0.0723593458533287 = 0.00604900112375617 + 0.01 * 6.631034851074219
Epoch 730, val loss: 1.0987275838851929
Epoch 740, training loss: 0.07200668007135391 = 0.0058274720795452595 + 0.01 * 6.617920875549316
Epoch 740, val loss: 1.1040648221969604
Epoch 750, training loss: 0.07165200263261795 = 0.005619051866233349 + 0.01 * 6.60329532623291
Epoch 750, val loss: 1.109312653541565
Epoch 760, training loss: 0.07140374928712845 = 0.005423050839453936 + 0.01 * 6.598069667816162
Epoch 760, val loss: 1.1143767833709717
Epoch 770, training loss: 0.07164189964532852 = 0.0052384077571332455 + 0.01 * 6.640349388122559
Epoch 770, val loss: 1.119387149810791
Epoch 780, training loss: 0.07101371139287949 = 0.005064374767243862 + 0.01 * 6.59493350982666
Epoch 780, val loss: 1.1241694688796997
Epoch 790, training loss: 0.07075855135917664 = 0.004900176078081131 + 0.01 * 6.585838317871094
Epoch 790, val loss: 1.128907322883606
Epoch 800, training loss: 0.07061997801065445 = 0.0047449273988604546 + 0.01 * 6.587505340576172
Epoch 800, val loss: 1.1335023641586304
Epoch 810, training loss: 0.07036449015140533 = 0.004597935825586319 + 0.01 * 6.57665491104126
Epoch 810, val loss: 1.138022541999817
Epoch 820, training loss: 0.07023361325263977 = 0.0044587161391973495 + 0.01 * 6.577489852905273
Epoch 820, val loss: 1.142396330833435
Epoch 830, training loss: 0.07015410810709 = 0.0043264348059892654 + 0.01 * 6.582767963409424
Epoch 830, val loss: 1.1467639207839966
Epoch 840, training loss: 0.06990742683410645 = 0.004201082978397608 + 0.01 * 6.570634365081787
Epoch 840, val loss: 1.150960087776184
Epoch 850, training loss: 0.06985656917095184 = 0.004081865772604942 + 0.01 * 6.577470302581787
Epoch 850, val loss: 1.1551333665847778
Epoch 860, training loss: 0.06997565925121307 = 0.003968416713178158 + 0.01 * 6.600724220275879
Epoch 860, val loss: 1.1592108011245728
Epoch 870, training loss: 0.06963369250297546 = 0.0038608198519796133 + 0.01 * 6.577287673950195
Epoch 870, val loss: 1.16312837600708
Epoch 880, training loss: 0.06931661069393158 = 0.0037588588893413544 + 0.01 * 6.555774688720703
Epoch 880, val loss: 1.1669567823410034
Epoch 890, training loss: 0.06912852078676224 = 0.0036614600103348494 + 0.01 * 6.546706199645996
Epoch 890, val loss: 1.1707119941711426
Epoch 900, training loss: 0.0692661851644516 = 0.003568447194993496 + 0.01 * 6.569774150848389
Epoch 900, val loss: 1.1743547916412354
Epoch 910, training loss: 0.06902062147855759 = 0.003479718929156661 + 0.01 * 6.55409049987793
Epoch 910, val loss: 1.177976369857788
Epoch 920, training loss: 0.0688556656241417 = 0.0033949532080441713 + 0.01 * 6.546071529388428
Epoch 920, val loss: 1.1815104484558105
Epoch 930, training loss: 0.0686858594417572 = 0.0033141877502202988 + 0.01 * 6.537167072296143
Epoch 930, val loss: 1.184958815574646
Epoch 940, training loss: 0.06858639419078827 = 0.0032367012463510036 + 0.01 * 6.534969329833984
Epoch 940, val loss: 1.1883331537246704
Epoch 950, training loss: 0.06871586292982101 = 0.0031625526025891304 + 0.01 * 6.555331230163574
Epoch 950, val loss: 1.1916126012802124
Epoch 960, training loss: 0.06857708841562271 = 0.003091567661613226 + 0.01 * 6.5485520362854
Epoch 960, val loss: 1.1948646306991577
Epoch 970, training loss: 0.06841736286878586 = 0.0030235436279326677 + 0.01 * 6.539382457733154
Epoch 970, val loss: 1.198014259338379
Epoch 980, training loss: 0.06836417317390442 = 0.0029583966825157404 + 0.01 * 6.540578365325928
Epoch 980, val loss: 1.2010698318481445
Epoch 990, training loss: 0.06816540658473969 = 0.0028958425391465425 + 0.01 * 6.526956558227539
Epoch 990, val loss: 1.2041269540786743
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0537662506103516 = 1.9700276851654053 + 0.01 * 8.373846054077148
Epoch 0, val loss: 1.9701420068740845
Epoch 10, training loss: 2.04183030128479 = 1.9580930471420288 + 0.01 * 8.373719215393066
Epoch 10, val loss: 1.9582847356796265
Epoch 20, training loss: 2.0267181396484375 = 1.9429842233657837 + 0.01 * 8.37338638305664
Epoch 20, val loss: 1.9432930946350098
Epoch 30, training loss: 2.0053789615631104 = 1.9216554164886475 + 0.01 * 8.372364044189453
Epoch 30, val loss: 1.9224306344985962
Epoch 40, training loss: 1.9740195274353027 = 1.89035964012146 + 0.01 * 8.365988731384277
Epoch 40, val loss: 1.892583966255188
Epoch 50, training loss: 1.9306472539901733 = 1.8474141359329224 + 0.01 * 8.323307991027832
Epoch 50, val loss: 1.8540784120559692
Epoch 60, training loss: 1.8820641040802002 = 1.8011236190795898 + 0.01 * 8.0940523147583
Epoch 60, val loss: 1.817193627357483
Epoch 70, training loss: 1.8420295715332031 = 1.7627766132354736 + 0.01 * 7.925292491912842
Epoch 70, val loss: 1.7876088619232178
Epoch 80, training loss: 1.7929010391235352 = 1.7163090705871582 + 0.01 * 7.659196853637695
Epoch 80, val loss: 1.745388150215149
Epoch 90, training loss: 1.726216435432434 = 1.652880311012268 + 0.01 * 7.333617210388184
Epoch 90, val loss: 1.6898378133773804
Epoch 100, training loss: 1.642052412033081 = 1.5710991621017456 + 0.01 * 7.095320701599121
Epoch 100, val loss: 1.6227803230285645
Epoch 110, training loss: 1.5517257452011108 = 1.481770634651184 + 0.01 * 6.995509147644043
Epoch 110, val loss: 1.5525213479995728
Epoch 120, training loss: 1.4683294296264648 = 1.3988256454467773 + 0.01 * 6.95037317276001
Epoch 120, val loss: 1.4899476766586304
Epoch 130, training loss: 1.3925689458847046 = 1.3232698440551758 + 0.01 * 6.929907321929932
Epoch 130, val loss: 1.4333508014678955
Epoch 140, training loss: 1.3186109066009521 = 1.2494884729385376 + 0.01 * 6.912239074707031
Epoch 140, val loss: 1.3787912130355835
Epoch 150, training loss: 1.2423311471939087 = 1.1733176708221436 + 0.01 * 6.901345252990723
Epoch 150, val loss: 1.3218289613723755
Epoch 160, training loss: 1.1639940738677979 = 1.09504234790802 + 0.01 * 6.895175457000732
Epoch 160, val loss: 1.263516902923584
Epoch 170, training loss: 1.0864920616149902 = 1.0175541639328003 + 0.01 * 6.893788814544678
Epoch 170, val loss: 1.2065441608428955
Epoch 180, training loss: 1.011979579925537 = 0.943049967288971 + 0.01 * 6.8929619789123535
Epoch 180, val loss: 1.1525834798812866
Epoch 190, training loss: 0.9402109384536743 = 0.8712670207023621 + 0.01 * 6.8943915367126465
Epoch 190, val loss: 1.100324034690857
Epoch 200, training loss: 0.8696526288986206 = 0.800690233707428 + 0.01 * 6.896242618560791
Epoch 200, val loss: 1.0485560894012451
Epoch 210, training loss: 0.7994770407676697 = 0.730495810508728 + 0.01 * 6.898123741149902
Epoch 210, val loss: 0.9963623285293579
Epoch 220, training loss: 0.7307670712471008 = 0.6617675423622131 + 0.01 * 6.899950981140137
Epoch 220, val loss: 0.9450318217277527
Epoch 230, training loss: 0.666246771812439 = 0.5972360372543335 + 0.01 * 6.901071071624756
Epoch 230, val loss: 0.8975898027420044
Epoch 240, training loss: 0.6079847812652588 = 0.538973867893219 + 0.01 * 6.901094436645508
Epoch 240, val loss: 0.8571248650550842
Epoch 250, training loss: 0.5563678741455078 = 0.4873558282852173 + 0.01 * 6.90120792388916
Epoch 250, val loss: 0.8248077034950256
Epoch 260, training loss: 0.5102584362030029 = 0.44127798080444336 + 0.01 * 6.898046970367432
Epoch 260, val loss: 0.8000338673591614
Epoch 270, training loss: 0.46794480085372925 = 0.39900484681129456 + 0.01 * 6.893995761871338
Epoch 270, val loss: 0.7813916802406311
Epoch 280, training loss: 0.4279569089412689 = 0.35898345708847046 + 0.01 * 6.897345066070557
Epoch 280, val loss: 0.7672035694122314
Epoch 290, training loss: 0.38914406299591064 = 0.32028961181640625 + 0.01 * 6.885444641113281
Epoch 290, val loss: 0.756193995475769
Epoch 300, training loss: 0.35157710313796997 = 0.28279799222946167 + 0.01 * 6.877910137176514
Epoch 300, val loss: 0.7478110790252686
Epoch 310, training loss: 0.3159692883491516 = 0.24715439975261688 + 0.01 * 6.881489276885986
Epoch 310, val loss: 0.7422953248023987
Epoch 320, training loss: 0.2829504907131195 = 0.21432934701442719 + 0.01 * 6.8621134757995605
Epoch 320, val loss: 0.7402654886245728
Epoch 330, training loss: 0.2537813186645508 = 0.18524563312530518 + 0.01 * 6.853568077087402
Epoch 330, val loss: 0.7422214150428772
Epoch 340, training loss: 0.22901922464370728 = 0.1602858304977417 + 0.01 * 6.873340129852295
Epoch 340, val loss: 0.7478155493736267
Epoch 350, training loss: 0.2076719105243683 = 0.13933171331882477 + 0.01 * 6.834019184112549
Epoch 350, val loss: 0.7564502954483032
Epoch 360, training loss: 0.19013559818267822 = 0.12186483293771744 + 0.01 * 6.827077388763428
Epoch 360, val loss: 0.7674054503440857
Epoch 370, training loss: 0.17549505829811096 = 0.10727419704198837 + 0.01 * 6.822085857391357
Epoch 370, val loss: 0.7801643013954163
Epoch 380, training loss: 0.163026824593544 = 0.09502484649419785 + 0.01 * 6.800198078155518
Epoch 380, val loss: 0.7940787076950073
Epoch 390, training loss: 0.15246760845184326 = 0.08462833613157272 + 0.01 * 6.7839274406433105
Epoch 390, val loss: 0.8088860511779785
Epoch 400, training loss: 0.14346155524253845 = 0.07570640742778778 + 0.01 * 6.775515556335449
Epoch 400, val loss: 0.8244504332542419
Epoch 410, training loss: 0.13579188287258148 = 0.06799142807722092 + 0.01 * 6.780045509338379
Epoch 410, val loss: 0.8405289649963379
Epoch 420, training loss: 0.12891820073127747 = 0.061267223209142685 + 0.01 * 6.765098571777344
Epoch 420, val loss: 0.8569180965423584
Epoch 430, training loss: 0.1233026459813118 = 0.05535885691642761 + 0.01 * 6.794379234313965
Epoch 430, val loss: 0.8737800717353821
Epoch 440, training loss: 0.11763264238834381 = 0.05015633627772331 + 0.01 * 6.747631072998047
Epoch 440, val loss: 0.890787661075592
Epoch 450, training loss: 0.11301589012145996 = 0.045546092092990875 + 0.01 * 6.746979713439941
Epoch 450, val loss: 0.9078831672668457
Epoch 460, training loss: 0.1089407354593277 = 0.04144572094082832 + 0.01 * 6.749502182006836
Epoch 460, val loss: 0.9251849055290222
Epoch 470, training loss: 0.10503390431404114 = 0.03779695928096771 + 0.01 * 6.723694324493408
Epoch 470, val loss: 0.9424174427986145
Epoch 480, training loss: 0.10211408883333206 = 0.0345441997051239 + 0.01 * 6.756989002227783
Epoch 480, val loss: 0.9594438076019287
Epoch 490, training loss: 0.09890910983085632 = 0.03164878860116005 + 0.01 * 6.72603178024292
Epoch 490, val loss: 0.9763777256011963
Epoch 500, training loss: 0.09611374139785767 = 0.02906801924109459 + 0.01 * 6.7045722007751465
Epoch 500, val loss: 0.9928809404373169
Epoch 510, training loss: 0.09422573447227478 = 0.02676505222916603 + 0.01 * 6.746068000793457
Epoch 510, val loss: 1.0090348720550537
Epoch 520, training loss: 0.09165692329406738 = 0.024712925776839256 + 0.01 * 6.694399356842041
Epoch 520, val loss: 1.024659514427185
Epoch 530, training loss: 0.08985969424247742 = 0.022879118099808693 + 0.01 * 6.698057651519775
Epoch 530, val loss: 1.0398452281951904
Epoch 540, training loss: 0.08800075948238373 = 0.02123894728720188 + 0.01 * 6.676181316375732
Epoch 540, val loss: 1.0543875694274902
Epoch 550, training loss: 0.0864136666059494 = 0.019769910722970963 + 0.01 * 6.6643757820129395
Epoch 550, val loss: 1.0683797597885132
Epoch 560, training loss: 0.08519715070724487 = 0.01844954863190651 + 0.01 * 6.674759864807129
Epoch 560, val loss: 1.0818339586257935
Epoch 570, training loss: 0.0844515711069107 = 0.017259543761610985 + 0.01 * 6.719202518463135
Epoch 570, val loss: 1.094655990600586
Epoch 580, training loss: 0.0826883614063263 = 0.016186080873012543 + 0.01 * 6.650228500366211
Epoch 580, val loss: 1.1071773767471313
Epoch 590, training loss: 0.08146502822637558 = 0.015213110484182835 + 0.01 * 6.625191688537598
Epoch 590, val loss: 1.1190818548202515
Epoch 600, training loss: 0.08062288910150528 = 0.014328762888908386 + 0.01 * 6.629412651062012
Epoch 600, val loss: 1.1306684017181396
Epoch 610, training loss: 0.08025355637073517 = 0.01352363545447588 + 0.01 * 6.67299222946167
Epoch 610, val loss: 1.141815423965454
Epoch 620, training loss: 0.07916826009750366 = 0.012788714841008186 + 0.01 * 6.637955188751221
Epoch 620, val loss: 1.1525487899780273
Epoch 630, training loss: 0.07826422154903412 = 0.012115994468331337 + 0.01 * 6.614822864532471
Epoch 630, val loss: 1.1629352569580078
Epoch 640, training loss: 0.07778668403625488 = 0.011497932486236095 + 0.01 * 6.628875732421875
Epoch 640, val loss: 1.172993779182434
Epoch 650, training loss: 0.07698220759630203 = 0.010929256677627563 + 0.01 * 6.605295181274414
Epoch 650, val loss: 1.182730793952942
Epoch 660, training loss: 0.07679881900548935 = 0.010404367931187153 + 0.01 * 6.639444828033447
Epoch 660, val loss: 1.1921404600143433
Epoch 670, training loss: 0.07567054778337479 = 0.009920230135321617 + 0.01 * 6.575031757354736
Epoch 670, val loss: 1.2013944387435913
Epoch 680, training loss: 0.07543924450874329 = 0.009471636265516281 + 0.01 * 6.596760272979736
Epoch 680, val loss: 1.2101655006408691
Epoch 690, training loss: 0.07478852570056915 = 0.009055926464498043 + 0.01 * 6.573260307312012
Epoch 690, val loss: 1.21890127658844
Epoch 700, training loss: 0.07415517419576645 = 0.008669109083712101 + 0.01 * 6.5486063957214355
Epoch 700, val loss: 1.227237582206726
Epoch 710, training loss: 0.07391028106212616 = 0.008309073746204376 + 0.01 * 6.560120582580566
Epoch 710, val loss: 1.235406756401062
Epoch 720, training loss: 0.07381017506122589 = 0.00797303020954132 + 0.01 * 6.583714485168457
Epoch 720, val loss: 1.243292212486267
Epoch 730, training loss: 0.07311322540044785 = 0.007659370545297861 + 0.01 * 6.545385837554932
Epoch 730, val loss: 1.2510831356048584
Epoch 740, training loss: 0.07295706123113632 = 0.007365532219409943 + 0.01 * 6.559153079986572
Epoch 740, val loss: 1.258543610572815
Epoch 750, training loss: 0.07220397889614105 = 0.007090405561029911 + 0.01 * 6.51135778427124
Epoch 750, val loss: 1.2658288478851318
Epoch 760, training loss: 0.07220353931188583 = 0.006832024082541466 + 0.01 * 6.537151336669922
Epoch 760, val loss: 1.272864580154419
Epoch 770, training loss: 0.07171784341335297 = 0.006589074153453112 + 0.01 * 6.512876510620117
Epoch 770, val loss: 1.2798758745193481
Epoch 780, training loss: 0.0713476836681366 = 0.0063605643808841705 + 0.01 * 6.498712539672852
Epoch 780, val loss: 1.2866066694259644
Epoch 790, training loss: 0.07119116187095642 = 0.006145216524600983 + 0.01 * 6.504594802856445
Epoch 790, val loss: 1.2932448387145996
Epoch 800, training loss: 0.0709189772605896 = 0.005941777024418116 + 0.01 * 6.497719764709473
Epoch 800, val loss: 1.299574375152588
Epoch 810, training loss: 0.07070292532444 = 0.005749895237386227 + 0.01 * 6.495303153991699
Epoch 810, val loss: 1.3058302402496338
Epoch 820, training loss: 0.07036808133125305 = 0.005568483378738165 + 0.01 * 6.4799604415893555
Epoch 820, val loss: 1.311964750289917
Epoch 830, training loss: 0.07040873914957047 = 0.005396574269980192 + 0.01 * 6.501216411590576
Epoch 830, val loss: 1.317870020866394
Epoch 840, training loss: 0.07009300589561462 = 0.0052337865345180035 + 0.01 * 6.485922336578369
Epoch 840, val loss: 1.323813796043396
Epoch 850, training loss: 0.06972602009773254 = 0.005079145077615976 + 0.01 * 6.464687824249268
Epoch 850, val loss: 1.3294355869293213
Epoch 860, training loss: 0.06983921676874161 = 0.0049325330182909966 + 0.01 * 6.490668773651123
Epoch 860, val loss: 1.3349814414978027
Epoch 870, training loss: 0.06960559636354446 = 0.004793265834450722 + 0.01 * 6.4812331199646
Epoch 870, val loss: 1.3405296802520752
Epoch 880, training loss: 0.06929587572813034 = 0.004660558886826038 + 0.01 * 6.463531494140625
Epoch 880, val loss: 1.3457926511764526
Epoch 890, training loss: 0.06914827972650528 = 0.004534273408353329 + 0.01 * 6.461400985717773
Epoch 890, val loss: 1.3509910106658936
Epoch 900, training loss: 0.06902443617582321 = 0.00441402243450284 + 0.01 * 6.461041450500488
Epoch 900, val loss: 1.3561346530914307
Epoch 910, training loss: 0.06874249875545502 = 0.004299100488424301 + 0.01 * 6.444340229034424
Epoch 910, val loss: 1.36111319065094
Epoch 920, training loss: 0.06851758062839508 = 0.004189545288681984 + 0.01 * 6.432804107666016
Epoch 920, val loss: 1.3659926652908325
Epoch 930, training loss: 0.06883534044027328 = 0.004084807354956865 + 0.01 * 6.475053310394287
Epoch 930, val loss: 1.3708406686782837
Epoch 940, training loss: 0.0681929737329483 = 0.003984730690717697 + 0.01 * 6.420825004577637
Epoch 940, val loss: 1.3755229711532593
Epoch 950, training loss: 0.06842201948165894 = 0.0038889856077730656 + 0.01 * 6.453303337097168
Epoch 950, val loss: 1.3801578283309937
Epoch 960, training loss: 0.06819526106119156 = 0.0037972782738506794 + 0.01 * 6.439798831939697
Epoch 960, val loss: 1.3847229480743408
Epoch 970, training loss: 0.06789247691631317 = 0.0037094783037900925 + 0.01 * 6.418299674987793
Epoch 970, val loss: 1.3890435695648193
Epoch 980, training loss: 0.06778432428836823 = 0.0036253146827220917 + 0.01 * 6.415901184082031
Epoch 980, val loss: 1.3934247493743896
Epoch 990, training loss: 0.06792326271533966 = 0.0035445441026240587 + 0.01 * 6.437871932983398
Epoch 990, val loss: 1.3977162837982178
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.5166
Flip ASR: 0.4267/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.034275531768799 = 1.9505369663238525 + 0.01 * 8.373866081237793
Epoch 0, val loss: 1.9554287195205688
Epoch 10, training loss: 2.0237176418304443 = 1.9399794340133667 + 0.01 * 8.373826026916504
Epoch 10, val loss: 1.9436743259429932
Epoch 20, training loss: 2.0111477375030518 = 1.9274115562438965 + 0.01 * 8.373627662658691
Epoch 20, val loss: 1.9292564392089844
Epoch 30, training loss: 1.9938712120056152 = 1.9101403951644897 + 0.01 * 8.373085975646973
Epoch 30, val loss: 1.9092190265655518
Epoch 40, training loss: 1.9685850143432617 = 1.8848769664764404 + 0.01 * 8.370802879333496
Epoch 40, val loss: 1.8802353143692017
Epoch 50, training loss: 1.9322105646133423 = 1.8486708402633667 + 0.01 * 8.353968620300293
Epoch 50, val loss: 1.8402420282363892
Epoch 60, training loss: 1.8854262828826904 = 1.803066372871399 + 0.01 * 8.235995292663574
Epoch 60, val loss: 1.7932490110397339
Epoch 70, training loss: 1.835394024848938 = 1.756367564201355 + 0.01 * 7.902651786804199
Epoch 70, val loss: 1.7491964101791382
Epoch 80, training loss: 1.781152606010437 = 1.7046576738357544 + 0.01 * 7.64949893951416
Epoch 80, val loss: 1.702506422996521
Epoch 90, training loss: 1.7082010507583618 = 1.6342052221298218 + 0.01 * 7.399582862854004
Epoch 90, val loss: 1.6424310207366943
Epoch 100, training loss: 1.6143991947174072 = 1.5424624681472778 + 0.01 * 7.193670272827148
Epoch 100, val loss: 1.5671550035476685
Epoch 110, training loss: 1.507891058921814 = 1.436525821685791 + 0.01 * 7.136521339416504
Epoch 110, val loss: 1.4809235334396362
Epoch 120, training loss: 1.4001679420471191 = 1.3292901515960693 + 0.01 * 7.087773323059082
Epoch 120, val loss: 1.3969202041625977
Epoch 130, training loss: 1.299343466758728 = 1.2288906574249268 + 0.01 * 7.045276641845703
Epoch 130, val loss: 1.3225452899932861
Epoch 140, training loss: 1.2064776420593262 = 1.1364401578903198 + 0.01 * 7.003746509552002
Epoch 140, val loss: 1.2569270133972168
Epoch 150, training loss: 1.1183087825775146 = 1.048625111579895 + 0.01 * 6.96837043762207
Epoch 150, val loss: 1.1962062120437622
Epoch 160, training loss: 1.032623291015625 = 0.9631848335266113 + 0.01 * 6.943845748901367
Epoch 160, val loss: 1.1369646787643433
Epoch 170, training loss: 0.9498912692070007 = 0.8805701732635498 + 0.01 * 6.932109832763672
Epoch 170, val loss: 1.080203890800476
Epoch 180, training loss: 0.8715981245040894 = 0.8023666739463806 + 0.01 * 6.9231438636779785
Epoch 180, val loss: 1.0266329050064087
Epoch 190, training loss: 0.799250602722168 = 0.7300736904144287 + 0.01 * 6.917690277099609
Epoch 190, val loss: 0.9776961803436279
Epoch 200, training loss: 0.7336559295654297 = 0.6645306944847107 + 0.01 * 6.9125261306762695
Epoch 200, val loss: 0.933820903301239
Epoch 210, training loss: 0.6745244860649109 = 0.605449378490448 + 0.01 * 6.907512664794922
Epoch 210, val loss: 0.8947444558143616
Epoch 220, training loss: 0.6206401586532593 = 0.5516096949577332 + 0.01 * 6.903043746948242
Epoch 220, val loss: 0.8597297668457031
Epoch 230, training loss: 0.570553183555603 = 0.5015991926193237 + 0.01 * 6.8954010009765625
Epoch 230, val loss: 0.8283562660217285
Epoch 240, training loss: 0.5230767130851746 = 0.4542059600353241 + 0.01 * 6.887077808380127
Epoch 240, val loss: 0.7999493479728699
Epoch 250, training loss: 0.4772960841655731 = 0.4084866940975189 + 0.01 * 6.880938529968262
Epoch 250, val loss: 0.7740173935890198
Epoch 260, training loss: 0.4328077435493469 = 0.3640950620174408 + 0.01 * 6.871267318725586
Epoch 260, val loss: 0.7510711550712585
Epoch 270, training loss: 0.38976430892944336 = 0.3211471438407898 + 0.01 * 6.861717224121094
Epoch 270, val loss: 0.7315455675125122
Epoch 280, training loss: 0.34917184710502625 = 0.2806220054626465 + 0.01 * 6.854983329772949
Epoch 280, val loss: 0.7166572213172913
Epoch 290, training loss: 0.3123970627784729 = 0.24389871954917908 + 0.01 * 6.84983491897583
Epoch 290, val loss: 0.706102728843689
Epoch 300, training loss: 0.27982813119888306 = 0.21141016483306885 + 0.01 * 6.841797828674316
Epoch 300, val loss: 0.7016493678092957
Epoch 310, training loss: 0.25172683596611023 = 0.1833806037902832 + 0.01 * 6.834623336791992
Epoch 310, val loss: 0.7019270062446594
Epoch 320, training loss: 0.2277640700340271 = 0.15947513282299042 + 0.01 * 6.82889461517334
Epoch 320, val loss: 0.7065343856811523
Epoch 330, training loss: 0.20766490697860718 = 0.13922540843486786 + 0.01 * 6.843950271606445
Epoch 330, val loss: 0.7143511176109314
Epoch 340, training loss: 0.19025057554244995 = 0.12200336903333664 + 0.01 * 6.824721336364746
Epoch 340, val loss: 0.7245928049087524
Epoch 350, training loss: 0.17541910707950592 = 0.1072593405842781 + 0.01 * 6.815977096557617
Epoch 350, val loss: 0.736703634262085
Epoch 360, training loss: 0.162665456533432 = 0.09455431252717972 + 0.01 * 6.8111138343811035
Epoch 360, val loss: 0.7501223683357239
Epoch 370, training loss: 0.15162065625190735 = 0.08354523032903671 + 0.01 * 6.80754280090332
Epoch 370, val loss: 0.7644364237785339
Epoch 380, training loss: 0.14201533794403076 = 0.07398233562707901 + 0.01 * 6.803299427032471
Epoch 380, val loss: 0.7793421149253845
Epoch 390, training loss: 0.13377858698368073 = 0.06568633019924164 + 0.01 * 6.809225559234619
Epoch 390, val loss: 0.7946408987045288
Epoch 400, training loss: 0.12646017968654633 = 0.058487068861722946 + 0.01 * 6.797310829162598
Epoch 400, val loss: 0.8101805448532104
Epoch 410, training loss: 0.12013143301010132 = 0.0522327683866024 + 0.01 * 6.789866924285889
Epoch 410, val loss: 0.8257836103439331
Epoch 420, training loss: 0.11466199159622192 = 0.046800997108221054 + 0.01 * 6.786098957061768
Epoch 420, val loss: 0.8412644267082214
Epoch 430, training loss: 0.10991647839546204 = 0.04208442196249962 + 0.01 * 6.783205986022949
Epoch 430, val loss: 0.8567937016487122
Epoch 440, training loss: 0.10578680038452148 = 0.03798019140958786 + 0.01 * 6.780661106109619
Epoch 440, val loss: 0.8720864057540894
Epoch 450, training loss: 0.10214616358280182 = 0.03440601006150246 + 0.01 * 6.774015426635742
Epoch 450, val loss: 0.887233555316925
Epoch 460, training loss: 0.09893152117729187 = 0.031285665929317474 + 0.01 * 6.764585494995117
Epoch 460, val loss: 0.9021227359771729
Epoch 470, training loss: 0.0961790457367897 = 0.0285505298525095 + 0.01 * 6.762851715087891
Epoch 470, val loss: 0.9167147278785706
Epoch 480, training loss: 0.0938066616654396 = 0.02614775113761425 + 0.01 * 6.765891075134277
Epoch 480, val loss: 0.9308967590332031
Epoch 490, training loss: 0.09150037914514542 = 0.024030141532421112 + 0.01 * 6.747024059295654
Epoch 490, val loss: 0.9447435140609741
Epoch 500, training loss: 0.08954983204603195 = 0.02215607650578022 + 0.01 * 6.739375591278076
Epoch 500, val loss: 0.9582664370536804
Epoch 510, training loss: 0.08792993426322937 = 0.02048993483185768 + 0.01 * 6.74399995803833
Epoch 510, val loss: 0.9713781476020813
Epoch 520, training loss: 0.08646738529205322 = 0.01900504343211651 + 0.01 * 6.74623441696167
Epoch 520, val loss: 0.9840779900550842
Epoch 530, training loss: 0.08494382351636887 = 0.017678244039416313 + 0.01 * 6.726557731628418
Epoch 530, val loss: 0.9965108036994934
Epoch 540, training loss: 0.08369789272546768 = 0.016487227752804756 + 0.01 * 6.721066951751709
Epoch 540, val loss: 1.0084810256958008
Epoch 550, training loss: 0.08263620734214783 = 0.015414034016430378 + 0.01 * 6.722217082977295
Epoch 550, val loss: 1.0201339721679688
Epoch 560, training loss: 0.08142474293708801 = 0.014445616863667965 + 0.01 * 6.697912693023682
Epoch 560, val loss: 1.0313917398452759
Epoch 570, training loss: 0.08080587536096573 = 0.013568654656410217 + 0.01 * 6.723722457885742
Epoch 570, val loss: 1.0423604249954224
Epoch 580, training loss: 0.07958979904651642 = 0.012773982249200344 + 0.01 * 6.681581974029541
Epoch 580, val loss: 1.0529707670211792
Epoch 590, training loss: 0.07881421595811844 = 0.01204981654882431 + 0.01 * 6.6764397621154785
Epoch 590, val loss: 1.0632413625717163
Epoch 600, training loss: 0.07815472781658173 = 0.011388910934329033 + 0.01 * 6.676581382751465
Epoch 600, val loss: 1.0732781887054443
Epoch 610, training loss: 0.07743332535028458 = 0.010784992016851902 + 0.01 * 6.6648335456848145
Epoch 610, val loss: 1.0830130577087402
Epoch 620, training loss: 0.0767764002084732 = 0.01023099198937416 + 0.01 * 6.654541492462158
Epoch 620, val loss: 1.0924338102340698
Epoch 630, training loss: 0.07616056501865387 = 0.00972046423703432 + 0.01 * 6.644010066986084
Epoch 630, val loss: 1.1016411781311035
Epoch 640, training loss: 0.07562362402677536 = 0.00924942921847105 + 0.01 * 6.637419700622559
Epoch 640, val loss: 1.1105575561523438
Epoch 650, training loss: 0.07534042745828629 = 0.008815093897283077 + 0.01 * 6.652533531188965
Epoch 650, val loss: 1.119276523590088
Epoch 660, training loss: 0.07482336461544037 = 0.008412723429501057 + 0.01 * 6.641064167022705
Epoch 660, val loss: 1.1276555061340332
Epoch 670, training loss: 0.07439873367547989 = 0.008040426298975945 + 0.01 * 6.635830879211426
Epoch 670, val loss: 1.1359614133834839
Epoch 680, training loss: 0.0739072784781456 = 0.00769421923905611 + 0.01 * 6.621305465698242
Epoch 680, val loss: 1.1439251899719238
Epoch 690, training loss: 0.07371212542057037 = 0.007372192572802305 + 0.01 * 6.633993148803711
Epoch 690, val loss: 1.1516990661621094
Epoch 700, training loss: 0.07322953641414642 = 0.00707236398011446 + 0.01 * 6.61571741104126
Epoch 700, val loss: 1.159389853477478
Epoch 710, training loss: 0.07319577038288116 = 0.0067917839623987675 + 0.01 * 6.640398979187012
Epoch 710, val loss: 1.1667957305908203
Epoch 720, training loss: 0.072587750852108 = 0.0065295700915157795 + 0.01 * 6.605818271636963
Epoch 720, val loss: 1.173986792564392
Epoch 730, training loss: 0.07240143418312073 = 0.006284529808908701 + 0.01 * 6.611690998077393
Epoch 730, val loss: 1.181081771850586
Epoch 740, training loss: 0.07187063246965408 = 0.006054319906979799 + 0.01 * 6.581631660461426
Epoch 740, val loss: 1.1879494190216064
Epoch 750, training loss: 0.07171761244535446 = 0.005838151089847088 + 0.01 * 6.58794641494751
Epoch 750, val loss: 1.1946825981140137
Epoch 760, training loss: 0.0715928003191948 = 0.005634672939777374 + 0.01 * 6.595812797546387
Epoch 760, val loss: 1.2012975215911865
Epoch 770, training loss: 0.0712168738245964 = 0.0054432665929198265 + 0.01 * 6.577361106872559
Epoch 770, val loss: 1.207756519317627
Epoch 780, training loss: 0.07127028703689575 = 0.005262468010187149 + 0.01 * 6.60078239440918
Epoch 780, val loss: 1.213966965675354
Epoch 790, training loss: 0.07084941118955612 = 0.005092053674161434 + 0.01 * 6.575736045837402
Epoch 790, val loss: 1.2202380895614624
Epoch 800, training loss: 0.07048705965280533 = 0.004930759314447641 + 0.01 * 6.555630683898926
Epoch 800, val loss: 1.2262321710586548
Epoch 810, training loss: 0.07051560282707214 = 0.004778033588081598 + 0.01 * 6.573757171630859
Epoch 810, val loss: 1.2321017980575562
Epoch 820, training loss: 0.07031868398189545 = 0.004633444827049971 + 0.01 * 6.568524360656738
Epoch 820, val loss: 1.2379120588302612
Epoch 830, training loss: 0.06999179720878601 = 0.004496217239648104 + 0.01 * 6.549558162689209
Epoch 830, val loss: 1.243626356124878
Epoch 840, training loss: 0.0698816105723381 = 0.004366065841168165 + 0.01 * 6.551555156707764
Epoch 840, val loss: 1.2491801977157593
Epoch 850, training loss: 0.06993962079286575 = 0.004242201801389456 + 0.01 * 6.569741725921631
Epoch 850, val loss: 1.2545533180236816
Epoch 860, training loss: 0.0696045309305191 = 0.004124671686440706 + 0.01 * 6.547986030578613
Epoch 860, val loss: 1.2599657773971558
Epoch 870, training loss: 0.06945756077766418 = 0.004012718331068754 + 0.01 * 6.5444841384887695
Epoch 870, val loss: 1.2651727199554443
Epoch 880, training loss: 0.06943189352750778 = 0.003906171303242445 + 0.01 * 6.552572727203369
Epoch 880, val loss: 1.2702746391296387
Epoch 890, training loss: 0.06929963827133179 = 0.0038045733235776424 + 0.01 * 6.549506664276123
Epoch 890, val loss: 1.2752443552017212
Epoch 900, training loss: 0.06924846768379211 = 0.003707673866301775 + 0.01 * 6.554079532623291
Epoch 900, val loss: 1.2801395654678345
Epoch 910, training loss: 0.06887956708669662 = 0.0036153115797787905 + 0.01 * 6.526425838470459
Epoch 910, val loss: 1.2850244045257568
Epoch 920, training loss: 0.0693911612033844 = 0.0035270750522613525 + 0.01 * 6.586408615112305
Epoch 920, val loss: 1.2897865772247314
Epoch 930, training loss: 0.06862007081508636 = 0.00344251305796206 + 0.01 * 6.51775598526001
Epoch 930, val loss: 1.2943507432937622
Epoch 940, training loss: 0.06877971440553665 = 0.0033618162851780653 + 0.01 * 6.541790008544922
Epoch 940, val loss: 1.2989150285720825
Epoch 950, training loss: 0.06862461566925049 = 0.0032847337424755096 + 0.01 * 6.533987998962402
Epoch 950, val loss: 1.3033965826034546
Epoch 960, training loss: 0.06833923608064651 = 0.003210799302905798 + 0.01 * 6.512843608856201
Epoch 960, val loss: 1.3077216148376465
Epoch 970, training loss: 0.06836661696434021 = 0.0031399019062519073 + 0.01 * 6.522671222686768
Epoch 970, val loss: 1.3120379447937012
Epoch 980, training loss: 0.06814831495285034 = 0.003071759594604373 + 0.01 * 6.507655620574951
Epoch 980, val loss: 1.3162448406219482
Epoch 990, training loss: 0.06808272749185562 = 0.003006520913913846 + 0.01 * 6.5076212882995605
Epoch 990, val loss: 1.3204017877578735
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7815
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0231873989105225 = 1.9394482374191284 + 0.01 * 8.373910903930664
Epoch 0, val loss: 1.9387010335922241
Epoch 10, training loss: 2.013518810272217 = 1.9297802448272705 + 0.01 * 8.373862266540527
Epoch 10, val loss: 1.9284460544586182
Epoch 20, training loss: 2.0020251274108887 = 1.9182884693145752 + 0.01 * 8.373674392700195
Epoch 20, val loss: 1.9160771369934082
Epoch 30, training loss: 1.9863616228103638 = 1.902630090713501 + 0.01 * 8.373150825500488
Epoch 30, val loss: 1.8992713689804077
Epoch 40, training loss: 1.9637596607208252 = 1.8800541162490845 + 0.01 * 8.370553016662598
Epoch 40, val loss: 1.8753492832183838
Epoch 50, training loss: 1.9312678575515747 = 1.8477951288223267 + 0.01 * 8.347268104553223
Epoch 50, val loss: 1.8424237966537476
Epoch 60, training loss: 1.8885828256607056 = 1.8071045875549316 + 0.01 * 8.147818565368652
Epoch 60, val loss: 1.8040611743927002
Epoch 70, training loss: 1.842545747756958 = 1.764190435409546 + 0.01 * 7.835535526275635
Epoch 70, val loss: 1.768146276473999
Epoch 80, training loss: 1.7902535200119019 = 1.7147026062011719 + 0.01 * 7.555091381072998
Epoch 80, val loss: 1.728326678276062
Epoch 90, training loss: 1.720805287361145 = 1.6471275091171265 + 0.01 * 7.3677778244018555
Epoch 90, val loss: 1.6723942756652832
Epoch 100, training loss: 1.6293373107910156 = 1.5572742223739624 + 0.01 * 7.206305980682373
Epoch 100, val loss: 1.5975656509399414
Epoch 110, training loss: 1.5196154117584229 = 1.4488005638122559 + 0.01 * 7.081490516662598
Epoch 110, val loss: 1.5106956958770752
Epoch 120, training loss: 1.4022088050842285 = 1.3320199251174927 + 0.01 * 7.018892288208008
Epoch 120, val loss: 1.4208430051803589
Epoch 130, training loss: 1.287618637084961 = 1.2177281379699707 + 0.01 * 6.989053249359131
Epoch 130, val loss: 1.3356865644454956
Epoch 140, training loss: 1.1808592081069946 = 1.111160397529602 + 0.01 * 6.969883918762207
Epoch 140, val loss: 1.2585219144821167
Epoch 150, training loss: 1.082680106163025 = 1.0130926370620728 + 0.01 * 6.958747386932373
Epoch 150, val loss: 1.1877260208129883
Epoch 160, training loss: 0.9916086792945862 = 0.9220833778381348 + 0.01 * 6.952532768249512
Epoch 160, val loss: 1.1214911937713623
Epoch 170, training loss: 0.9049442410469055 = 0.8354537487030029 + 0.01 * 6.949047088623047
Epoch 170, val loss: 1.0573503971099854
Epoch 180, training loss: 0.8210949897766113 = 0.7516412734985352 + 0.01 * 6.945369243621826
Epoch 180, val loss: 0.9944507479667664
Epoch 190, training loss: 0.7408446073532104 = 0.6714306473731995 + 0.01 * 6.941396713256836
Epoch 190, val loss: 0.934842586517334
Epoch 200, training loss: 0.6665573120117188 = 0.5972001552581787 + 0.01 * 6.935717582702637
Epoch 200, val loss: 0.8820472955703735
Epoch 210, training loss: 0.599928617477417 = 0.5306417942047119 + 0.01 * 6.928683757781982
Epoch 210, val loss: 0.8392694592475891
Epoch 220, training loss: 0.5408853888511658 = 0.47168052196502686 + 0.01 * 6.920485973358154
Epoch 220, val loss: 0.8073548674583435
Epoch 230, training loss: 0.4880386292934418 = 0.4189360737800598 + 0.01 * 6.9102559089660645
Epoch 230, val loss: 0.7852272987365723
Epoch 240, training loss: 0.43991711735725403 = 0.37091901898384094 + 0.01 * 6.899808883666992
Epoch 240, val loss: 0.7704117298126221
Epoch 250, training loss: 0.39564159512519836 = 0.3267102837562561 + 0.01 * 6.893131732940674
Epoch 250, val loss: 0.7604180574417114
Epoch 260, training loss: 0.35475996136665344 = 0.28591254353523254 + 0.01 * 6.884742736816406
Epoch 260, val loss: 0.7537411451339722
Epoch 270, training loss: 0.31741049885749817 = 0.2486320436000824 + 0.01 * 6.877846717834473
Epoch 270, val loss: 0.7498249411582947
Epoch 280, training loss: 0.28397101163864136 = 0.2151421755552292 + 0.01 * 6.882883548736572
Epoch 280, val loss: 0.7485854029655457
Epoch 290, training loss: 0.254389226436615 = 0.18565891683101654 + 0.01 * 6.873030662536621
Epoch 290, val loss: 0.7499209642410278
Epoch 300, training loss: 0.228817880153656 = 0.16014324128627777 + 0.01 * 6.8674635887146
Epoch 300, val loss: 0.7535160779953003
Epoch 310, training loss: 0.20700083673000336 = 0.13836224377155304 + 0.01 * 6.8638596534729
Epoch 310, val loss: 0.7592793107032776
Epoch 320, training loss: 0.1885710060596466 = 0.11996255815029144 + 0.01 * 6.86084508895874
Epoch 320, val loss: 0.7669540643692017
Epoch 330, training loss: 0.17309674620628357 = 0.10450740903615952 + 0.01 * 6.858932971954346
Epoch 330, val loss: 0.7762459516525269
Epoch 340, training loss: 0.16009411215782166 = 0.09154403954744339 + 0.01 * 6.855008125305176
Epoch 340, val loss: 0.7868061661720276
Epoch 350, training loss: 0.14913681149482727 = 0.08064483106136322 + 0.01 * 6.849198341369629
Epoch 350, val loss: 0.7986323833465576
Epoch 360, training loss: 0.13998454809188843 = 0.07143627107143402 + 0.01 * 6.854827880859375
Epoch 360, val loss: 0.8113142251968384
Epoch 370, training loss: 0.1320260465145111 = 0.06361375749111176 + 0.01 * 6.841228008270264
Epoch 370, val loss: 0.824641764163971
Epoch 380, training loss: 0.12530750036239624 = 0.05692474544048309 + 0.01 * 6.83827543258667
Epoch 380, val loss: 0.8385804295539856
Epoch 390, training loss: 0.11949916183948517 = 0.05116553232073784 + 0.01 * 6.833363056182861
Epoch 390, val loss: 0.8529196977615356
Epoch 400, training loss: 0.11456147581338882 = 0.04617524892091751 + 0.01 * 6.838622570037842
Epoch 400, val loss: 0.8674240708351135
Epoch 410, training loss: 0.11006123572587967 = 0.041824668645858765 + 0.01 * 6.823657035827637
Epoch 410, val loss: 0.8822079300880432
Epoch 420, training loss: 0.1062006950378418 = 0.038010090589523315 + 0.01 * 6.819060802459717
Epoch 420, val loss: 0.8971914052963257
Epoch 430, training loss: 0.10281437635421753 = 0.03465215489268303 + 0.01 * 6.816221714019775
Epoch 430, val loss: 0.912155270576477
Epoch 440, training loss: 0.09980389475822449 = 0.03168822452425957 + 0.01 * 6.811567783355713
Epoch 440, val loss: 0.9270138740539551
Epoch 450, training loss: 0.09713449329137802 = 0.029065467417240143 + 0.01 * 6.806902885437012
Epoch 450, val loss: 0.9418491125106812
Epoch 460, training loss: 0.09475327283143997 = 0.026738977059721947 + 0.01 * 6.801429748535156
Epoch 460, val loss: 0.9563584923744202
Epoch 470, training loss: 0.09265338629484177 = 0.024670643731951714 + 0.01 * 6.798274040222168
Epoch 470, val loss: 0.9706875085830688
Epoch 480, training loss: 0.0907517820596695 = 0.022828031331300735 + 0.01 * 6.792375087738037
Epoch 480, val loss: 0.9846911430358887
Epoch 490, training loss: 0.08904485404491425 = 0.021182287484407425 + 0.01 * 6.786256790161133
Epoch 490, val loss: 0.9984133243560791
Epoch 500, training loss: 0.08753108233213425 = 0.019707471132278442 + 0.01 * 6.7823615074157715
Epoch 500, val loss: 1.011788010597229
Epoch 510, training loss: 0.08610375225543976 = 0.018382351845502853 + 0.01 * 6.772139549255371
Epoch 510, val loss: 1.0248085260391235
Epoch 520, training loss: 0.0849401131272316 = 0.01718801259994507 + 0.01 * 6.775210380554199
Epoch 520, val loss: 1.037468671798706
Epoch 530, training loss: 0.08372370898723602 = 0.016109390184283257 + 0.01 * 6.76143217086792
Epoch 530, val loss: 1.0497896671295166
Epoch 540, training loss: 0.08272035419940948 = 0.01513171847909689 + 0.01 * 6.758863925933838
Epoch 540, val loss: 1.0618234872817993
Epoch 550, training loss: 0.08181464672088623 = 0.014243030920624733 + 0.01 * 6.757161617279053
Epoch 550, val loss: 1.0734996795654297
Epoch 560, training loss: 0.0808725580573082 = 0.013432973064482212 + 0.01 * 6.743958950042725
Epoch 560, val loss: 1.0848400592803955
Epoch 570, training loss: 0.08020244538784027 = 0.012692112475633621 + 0.01 * 6.751032829284668
Epoch 570, val loss: 1.0958415269851685
Epoch 580, training loss: 0.07933761179447174 = 0.012013605795800686 + 0.01 * 6.732400894165039
Epoch 580, val loss: 1.1064497232437134
Epoch 590, training loss: 0.0786699578166008 = 0.011390835978090763 + 0.01 * 6.727912425994873
Epoch 590, val loss: 1.1169527769088745
Epoch 600, training loss: 0.0780673623085022 = 0.010817384347319603 + 0.01 * 6.724997520446777
Epoch 600, val loss: 1.1269980669021606
Epoch 610, training loss: 0.07753235101699829 = 0.0102882981300354 + 0.01 * 6.724405288696289
Epoch 610, val loss: 1.1368669271469116
Epoch 620, training loss: 0.07694754004478455 = 0.009799263440072536 + 0.01 * 6.714827537536621
Epoch 620, val loss: 1.1465141773223877
Epoch 630, training loss: 0.07639055699110031 = 0.009346493519842625 + 0.01 * 6.704406261444092
Epoch 630, val loss: 1.1557672023773193
Epoch 640, training loss: 0.07594358921051025 = 0.008926100097596645 + 0.01 * 6.701748847961426
Epoch 640, val loss: 1.1648014783859253
Epoch 650, training loss: 0.0755544975399971 = 0.008535022847354412 + 0.01 * 6.7019476890563965
Epoch 650, val loss: 1.1737380027770996
Epoch 660, training loss: 0.07533308863639832 = 0.00817085336893797 + 0.01 * 6.716224193572998
Epoch 660, val loss: 1.1823235750198364
Epoch 670, training loss: 0.0746445432305336 = 0.007832030765712261 + 0.01 * 6.6812520027160645
Epoch 670, val loss: 1.1907198429107666
Epoch 680, training loss: 0.07422982901334763 = 0.007515373639762402 + 0.01 * 6.671445846557617
Epoch 680, val loss: 1.1988279819488525
Epoch 690, training loss: 0.07405127584934235 = 0.007219040300697088 + 0.01 * 6.683224201202393
Epoch 690, val loss: 1.2067760229110718
Epoch 700, training loss: 0.07382547110319138 = 0.006941333878785372 + 0.01 * 6.688413619995117
Epoch 700, val loss: 1.2146106958389282
Epoch 710, training loss: 0.0732673853635788 = 0.006681167986243963 + 0.01 * 6.6586222648620605
Epoch 710, val loss: 1.2220470905303955
Epoch 720, training loss: 0.07297857105731964 = 0.0064369067549705505 + 0.01 * 6.6541666984558105
Epoch 720, val loss: 1.2294858694076538
Epoch 730, training loss: 0.0729953944683075 = 0.006207047961652279 + 0.01 * 6.678834915161133
Epoch 730, val loss: 1.2366828918457031
Epoch 740, training loss: 0.07240404188632965 = 0.005990687757730484 + 0.01 * 6.641335964202881
Epoch 740, val loss: 1.2436517477035522
Epoch 750, training loss: 0.07231587171554565 = 0.005786742549389601 + 0.01 * 6.6529130935668945
Epoch 750, val loss: 1.25048828125
Epoch 760, training loss: 0.07188215106725693 = 0.005594587419182062 + 0.01 * 6.628756046295166
Epoch 760, val loss: 1.2571284770965576
Epoch 770, training loss: 0.07163282483816147 = 0.005412841681391001 + 0.01 * 6.6219987869262695
Epoch 770, val loss: 1.2636820077896118
Epoch 780, training loss: 0.071720190346241 = 0.005240798927843571 + 0.01 * 6.647939682006836
Epoch 780, val loss: 1.2700949907302856
Epoch 790, training loss: 0.07122369110584259 = 0.005077938549220562 + 0.01 * 6.614575386047363
Epoch 790, val loss: 1.2762047052383423
Epoch 800, training loss: 0.07104731351137161 = 0.004923623520880938 + 0.01 * 6.612369060516357
Epoch 800, val loss: 1.282379388809204
Epoch 810, training loss: 0.0709160640835762 = 0.004777333699166775 + 0.01 * 6.61387300491333
Epoch 810, val loss: 1.2883012294769287
Epoch 820, training loss: 0.07065587490797043 = 0.004638437647372484 + 0.01 * 6.601743698120117
Epoch 820, val loss: 1.2940987348556519
Epoch 830, training loss: 0.0703771710395813 = 0.004506472963839769 + 0.01 * 6.587069511413574
Epoch 830, val loss: 1.2998106479644775
Epoch 840, training loss: 0.07021467387676239 = 0.0043808394111692905 + 0.01 * 6.583383560180664
Epoch 840, val loss: 1.3054355382919312
Epoch 850, training loss: 0.07010739296674728 = 0.004261441063135862 + 0.01 * 6.584595203399658
Epoch 850, val loss: 1.3108843564987183
Epoch 860, training loss: 0.07023908942937851 = 0.004147619009017944 + 0.01 * 6.609147548675537
Epoch 860, val loss: 1.3162120580673218
Epoch 870, training loss: 0.06976193189620972 = 0.00403926195576787 + 0.01 * 6.572266578674316
Epoch 870, val loss: 1.3214209079742432
Epoch 880, training loss: 0.06978204101324081 = 0.003935738001018763 + 0.01 * 6.584630489349365
Epoch 880, val loss: 1.3266481161117554
Epoch 890, training loss: 0.06946555525064468 = 0.0038368592504411936 + 0.01 * 6.562869548797607
Epoch 890, val loss: 1.3316320180892944
Epoch 900, training loss: 0.06978705525398254 = 0.0037423905450850725 + 0.01 * 6.604466438293457
Epoch 900, val loss: 1.3366209268569946
Epoch 910, training loss: 0.06932494789361954 = 0.003652262268587947 + 0.01 * 6.567269325256348
Epoch 910, val loss: 1.3413887023925781
Epoch 920, training loss: 0.06910412013530731 = 0.003565919818356633 + 0.01 * 6.55381965637207
Epoch 920, val loss: 1.3461978435516357
Epoch 930, training loss: 0.06929069757461548 = 0.003483327105641365 + 0.01 * 6.580737113952637
Epoch 930, val loss: 1.3508777618408203
Epoch 940, training loss: 0.06893312186002731 = 0.0034040804021060467 + 0.01 * 6.55290412902832
Epoch 940, val loss: 1.3553836345672607
Epoch 950, training loss: 0.06893082708120346 = 0.003328166902065277 + 0.01 * 6.560266017913818
Epoch 950, val loss: 1.3598754405975342
Epoch 960, training loss: 0.06854784488677979 = 0.0032552918419241905 + 0.01 * 6.529255390167236
Epoch 960, val loss: 1.3643304109573364
Epoch 970, training loss: 0.06851798295974731 = 0.003185353707522154 + 0.01 * 6.533263206481934
Epoch 970, val loss: 1.3686505556106567
Epoch 980, training loss: 0.06846839189529419 = 0.003118278691545129 + 0.01 * 6.5350117683410645
Epoch 980, val loss: 1.3728677034378052
Epoch 990, training loss: 0.06844908744096756 = 0.003053736174479127 + 0.01 * 6.539535045623779
Epoch 990, val loss: 1.377048373222351
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.9336
Flip ASR: 0.9200/225 nodes
The final ASR:0.81058, 0.20879, Accuracy:0.79753, 0.01145
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11594])
remove edge: torch.Size([2, 9420])
updated graph: torch.Size([2, 10458])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98401, 0.00460, Accuracy:0.82963, 0.00605
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.058432102203369 = 1.9746934175491333 + 0.01 * 8.373869895935059
Epoch 0, val loss: 1.975285530090332
Epoch 10, training loss: 2.0472049713134766 = 1.963467001914978 + 0.01 * 8.373806953430176
Epoch 10, val loss: 1.9643524885177612
Epoch 20, training loss: 2.0332539081573486 = 1.9495183229446411 + 0.01 * 8.373568534851074
Epoch 20, val loss: 1.9502061605453491
Epoch 30, training loss: 2.0135207176208496 = 1.9297927618026733 + 0.01 * 8.372795104980469
Epoch 30, val loss: 1.9297155141830444
Epoch 40, training loss: 1.9842207431793213 = 1.9005364179611206 + 0.01 * 8.368428230285645
Epoch 40, val loss: 1.899211049079895
Epoch 50, training loss: 1.9428008794784546 = 1.8594133853912354 + 0.01 * 8.338751792907715
Epoch 50, val loss: 1.8579809665679932
Epoch 60, training loss: 1.8942761421203613 = 1.812455177307129 + 0.01 * 8.182093620300293
Epoch 60, val loss: 1.816376805305481
Epoch 70, training loss: 1.8528947830200195 = 1.7729419469833374 + 0.01 * 7.9952778816223145
Epoch 70, val loss: 1.7872593402862549
Epoch 80, training loss: 1.8063490390777588 = 1.728928804397583 + 0.01 * 7.742028713226318
Epoch 80, val loss: 1.7518486976623535
Epoch 90, training loss: 1.7423015832901 = 1.6679574251174927 + 0.01 * 7.434414863586426
Epoch 90, val loss: 1.7004761695861816
Epoch 100, training loss: 1.6584510803222656 = 1.5856690406799316 + 0.01 * 7.278203010559082
Epoch 100, val loss: 1.6313135623931885
Epoch 110, training loss: 1.5594792366027832 = 1.4872931241989136 + 0.01 * 7.218611240386963
Epoch 110, val loss: 1.5490165948867798
Epoch 120, training loss: 1.4570289850234985 = 1.3854576349258423 + 0.01 * 7.157133102416992
Epoch 120, val loss: 1.466309666633606
Epoch 130, training loss: 1.3562352657318115 = 1.2852790355682373 + 0.01 * 7.095621109008789
Epoch 130, val loss: 1.3882489204406738
Epoch 140, training loss: 1.255453109741211 = 1.18497896194458 + 0.01 * 7.047415256500244
Epoch 140, val loss: 1.3122996091842651
Epoch 150, training loss: 1.1546663045883179 = 1.0845236778259277 + 0.01 * 7.014261722564697
Epoch 150, val loss: 1.238093376159668
Epoch 160, training loss: 1.0567610263824463 = 0.9869127869606018 + 0.01 * 6.984823703765869
Epoch 160, val loss: 1.1676316261291504
Epoch 170, training loss: 0.9654396772384644 = 0.8959059119224548 + 0.01 * 6.953375816345215
Epoch 170, val loss: 1.1034208536148071
Epoch 180, training loss: 0.8839288949966431 = 0.8147633671760559 + 0.01 * 6.916550636291504
Epoch 180, val loss: 1.047824501991272
Epoch 190, training loss: 0.8140202760696411 = 0.7451871037483215 + 0.01 * 6.8833160400390625
Epoch 190, val loss: 1.0020021200180054
Epoch 200, training loss: 0.7548344135284424 = 0.6862845420837402 + 0.01 * 6.854986190795898
Epoch 200, val loss: 0.965347409248352
Epoch 210, training loss: 0.7033380270004272 = 0.6349928975105286 + 0.01 * 6.8345112800598145
Epoch 210, val loss: 0.9354922771453857
Epoch 220, training loss: 0.6557585000991821 = 0.5875214338302612 + 0.01 * 6.82370662689209
Epoch 220, val loss: 0.909593939781189
Epoch 230, training loss: 0.6088098287582397 = 0.5406520962715149 + 0.01 * 6.815771579742432
Epoch 230, val loss: 0.8850576281547546
Epoch 240, training loss: 0.5605941414833069 = 0.49244338274002075 + 0.01 * 6.8150739669799805
Epoch 240, val loss: 0.8600737452507019
Epoch 250, training loss: 0.510513424873352 = 0.4424086809158325 + 0.01 * 6.8104753494262695
Epoch 250, val loss: 0.8338210582733154
Epoch 260, training loss: 0.459567666053772 = 0.3914927542209625 + 0.01 * 6.807490348815918
Epoch 260, val loss: 0.80722576379776
Epoch 270, training loss: 0.40985047817230225 = 0.3417902886867523 + 0.01 * 6.806018829345703
Epoch 270, val loss: 0.7826263308525085
Epoch 280, training loss: 0.3641623854637146 = 0.2960473597049713 + 0.01 * 6.811503887176514
Epoch 280, val loss: 0.7633039951324463
Epoch 290, training loss: 0.32405492663383484 = 0.2560250759124756 + 0.01 * 6.802985668182373
Epoch 290, val loss: 0.7509095072746277
Epoch 300, training loss: 0.28986746072769165 = 0.22187376022338867 + 0.01 * 6.799370765686035
Epoch 300, val loss: 0.7447107434272766
Epoch 310, training loss: 0.2609929144382477 = 0.19298602640628815 + 0.01 * 6.800689220428467
Epoch 310, val loss: 0.7433813810348511
Epoch 320, training loss: 0.23655635118484497 = 0.1685972660779953 + 0.01 * 6.795908451080322
Epoch 320, val loss: 0.7459427118301392
Epoch 330, training loss: 0.215859055519104 = 0.14794810116291046 + 0.01 * 6.791094779968262
Epoch 330, val loss: 0.751438558101654
Epoch 340, training loss: 0.1982538402080536 = 0.1303863525390625 + 0.01 * 6.786748886108398
Epoch 340, val loss: 0.7592334151268005
Epoch 350, training loss: 0.18318402767181396 = 0.11535797268152237 + 0.01 * 6.782606601715088
Epoch 350, val loss: 0.7688313722610474
Epoch 360, training loss: 0.17022699117660522 = 0.1024201288819313 + 0.01 * 6.78068733215332
Epoch 360, val loss: 0.779751181602478
Epoch 370, training loss: 0.15897180140018463 = 0.09121764451265335 + 0.01 * 6.775415897369385
Epoch 370, val loss: 0.7916979193687439
Epoch 380, training loss: 0.14917010068893433 = 0.08146850019693375 + 0.01 * 6.770160675048828
Epoch 380, val loss: 0.8043544888496399
Epoch 390, training loss: 0.14066073298454285 = 0.07293818891048431 + 0.01 * 6.77225399017334
Epoch 390, val loss: 0.8175381422042847
Epoch 400, training loss: 0.13311271369457245 = 0.06544910371303558 + 0.01 * 6.766361236572266
Epoch 400, val loss: 0.83109450340271
Epoch 410, training loss: 0.12638074159622192 = 0.05885454639792442 + 0.01 * 6.75261926651001
Epoch 410, val loss: 0.8448508977890015
Epoch 420, training loss: 0.12058012187480927 = 0.05303650721907616 + 0.01 * 6.754362106323242
Epoch 420, val loss: 0.8586894869804382
Epoch 430, training loss: 0.11534878611564636 = 0.04790651425719261 + 0.01 * 6.744226932525635
Epoch 430, val loss: 0.8724501729011536
Epoch 440, training loss: 0.1107746809720993 = 0.04337475448846817 + 0.01 * 6.739993095397949
Epoch 440, val loss: 0.8860326409339905
Epoch 450, training loss: 0.10665620863437653 = 0.03936806693673134 + 0.01 * 6.728814601898193
Epoch 450, val loss: 0.8993685245513916
Epoch 460, training loss: 0.10331141203641891 = 0.03582840412855148 + 0.01 * 6.748301029205322
Epoch 460, val loss: 0.9123563766479492
Epoch 470, training loss: 0.10003949701786041 = 0.03270518407225609 + 0.01 * 6.733430862426758
Epoch 470, val loss: 0.9249556064605713
Epoch 480, training loss: 0.09706896543502808 = 0.02993931993842125 + 0.01 * 6.7129645347595215
Epoch 480, val loss: 0.9371447563171387
Epoch 490, training loss: 0.0945378765463829 = 0.027483118698000908 + 0.01 * 6.705475807189941
Epoch 490, val loss: 0.9489529132843018
Epoch 500, training loss: 0.09228257834911346 = 0.025296170264482498 + 0.01 * 6.698640823364258
Epoch 500, val loss: 0.9603767395019531
Epoch 510, training loss: 0.09119173139333725 = 0.02334565669298172 + 0.01 * 6.784607887268066
Epoch 510, val loss: 0.9714072942733765
Epoch 520, training loss: 0.08880296349525452 = 0.02161334455013275 + 0.01 * 6.7189621925354
Epoch 520, val loss: 0.9819716811180115
Epoch 530, training loss: 0.08706437051296234 = 0.020065054297447205 + 0.01 * 6.699931621551514
Epoch 530, val loss: 0.9921460151672363
Epoch 540, training loss: 0.08548160642385483 = 0.018675073981285095 + 0.01 * 6.6806535720825195
Epoch 540, val loss: 1.0019891262054443
Epoch 550, training loss: 0.08416114002466202 = 0.017421528697013855 + 0.01 * 6.673961162567139
Epoch 550, val loss: 1.0115370750427246
Epoch 560, training loss: 0.08295050263404846 = 0.01628769561648369 + 0.01 * 6.666280269622803
Epoch 560, val loss: 1.0207650661468506
Epoch 570, training loss: 0.08188767731189728 = 0.015259531326591969 + 0.01 * 6.662814140319824
Epoch 570, val loss: 1.0297049283981323
Epoch 580, training loss: 0.08131188154220581 = 0.014326090924441814 + 0.01 * 6.69857931137085
Epoch 580, val loss: 1.0383350849151611
Epoch 590, training loss: 0.08014565706253052 = 0.013479236513376236 + 0.01 * 6.666641712188721
Epoch 590, val loss: 1.0466712713241577
Epoch 600, training loss: 0.07923842966556549 = 0.012707707472145557 + 0.01 * 6.653071880340576
Epoch 600, val loss: 1.0547620058059692
Epoch 610, training loss: 0.07843825221061707 = 0.01200162898749113 + 0.01 * 6.643662929534912
Epoch 610, val loss: 1.0625805854797363
Epoch 620, training loss: 0.07786225527524948 = 0.011352963745594025 + 0.01 * 6.650928974151611
Epoch 620, val loss: 1.0701895952224731
Epoch 630, training loss: 0.07714875042438507 = 0.0107578719034791 + 0.01 * 6.639088153839111
Epoch 630, val loss: 1.0775444507598877
Epoch 640, training loss: 0.07664728909730911 = 0.010210549458861351 + 0.01 * 6.643673896789551
Epoch 640, val loss: 1.0846612453460693
Epoch 650, training loss: 0.07591211795806885 = 0.009704984724521637 + 0.01 * 6.620713233947754
Epoch 650, val loss: 1.0915454626083374
Epoch 660, training loss: 0.07567960768938065 = 0.00923613365739584 + 0.01 * 6.644347190856934
Epoch 660, val loss: 1.0982385873794556
Epoch 670, training loss: 0.07510305941104889 = 0.008802356198430061 + 0.01 * 6.630070686340332
Epoch 670, val loss: 1.1046957969665527
Epoch 680, training loss: 0.07463792711496353 = 0.00839947909116745 + 0.01 * 6.623844623565674
Epoch 680, val loss: 1.1109907627105713
Epoch 690, training loss: 0.07412463426589966 = 0.008024397306144238 + 0.01 * 6.6100239753723145
Epoch 690, val loss: 1.1170728206634521
Epoch 700, training loss: 0.07374707609415054 = 0.007674591150134802 + 0.01 * 6.607248783111572
Epoch 700, val loss: 1.1229866743087769
Epoch 710, training loss: 0.07335162162780762 = 0.007348270155489445 + 0.01 * 6.600335597991943
Epoch 710, val loss: 1.1287353038787842
Epoch 720, training loss: 0.07304375618696213 = 0.007043436635285616 + 0.01 * 6.600032329559326
Epoch 720, val loss: 1.134328007698059
Epoch 730, training loss: 0.07269064337015152 = 0.006758396979421377 + 0.01 * 6.59322452545166
Epoch 730, val loss: 1.1397385597229004
Epoch 740, training loss: 0.07275113463401794 = 0.006491378415375948 + 0.01 * 6.625976085662842
Epoch 740, val loss: 1.1450132131576538
Epoch 750, training loss: 0.07215927541255951 = 0.006241017021238804 + 0.01 * 6.591826438903809
Epoch 750, val loss: 1.1501362323760986
Epoch 760, training loss: 0.07188903540372849 = 0.006006001494824886 + 0.01 * 6.588303089141846
Epoch 760, val loss: 1.1551198959350586
Epoch 770, training loss: 0.07151417434215546 = 0.005785028450191021 + 0.01 * 6.572915077209473
Epoch 770, val loss: 1.1599262952804565
Epoch 780, training loss: 0.07125163823366165 = 0.005577065050601959 + 0.01 * 6.56745719909668
Epoch 780, val loss: 1.1646342277526855
Epoch 790, training loss: 0.07122375816106796 = 0.005381475668400526 + 0.01 * 6.584228515625
Epoch 790, val loss: 1.1692112684249878
Epoch 800, training loss: 0.07089916616678238 = 0.005196965299546719 + 0.01 * 6.570220470428467
Epoch 800, val loss: 1.1736469268798828
Epoch 810, training loss: 0.07065293192863464 = 0.005022609140723944 + 0.01 * 6.563032627105713
Epoch 810, val loss: 1.1779838800430298
Epoch 820, training loss: 0.07051657140254974 = 0.004857699386775494 + 0.01 * 6.565887928009033
Epoch 820, val loss: 1.1822129487991333
Epoch 830, training loss: 0.07017911970615387 = 0.004701781552284956 + 0.01 * 6.547734260559082
Epoch 830, val loss: 1.1863223314285278
Epoch 840, training loss: 0.07032357156276703 = 0.00455400301143527 + 0.01 * 6.576956748962402
Epoch 840, val loss: 1.190342903137207
Epoch 850, training loss: 0.0698607936501503 = 0.00441449461504817 + 0.01 * 6.54463005065918
Epoch 850, val loss: 1.1942170858383179
Epoch 860, training loss: 0.06983840465545654 = 0.004282248672097921 + 0.01 * 6.555615425109863
Epoch 860, val loss: 1.1980373859405518
Epoch 870, training loss: 0.06987656652927399 = 0.00415666913613677 + 0.01 * 6.571990013122559
Epoch 870, val loss: 1.2017319202423096
Epoch 880, training loss: 0.06942141801118851 = 0.0040375953540205956 + 0.01 * 6.538382530212402
Epoch 880, val loss: 1.2053358554840088
Epoch 890, training loss: 0.06935390084981918 = 0.003924195189028978 + 0.01 * 6.542970180511475
Epoch 890, val loss: 1.208827257156372
Epoch 900, training loss: 0.06920412182807922 = 0.003816149663180113 + 0.01 * 6.538797378540039
Epoch 900, val loss: 1.2122855186462402
Epoch 910, training loss: 0.06904858350753784 = 0.0037131658755242825 + 0.01 * 6.533541679382324
Epoch 910, val loss: 1.2156264781951904
Epoch 920, training loss: 0.06948849558830261 = 0.003615081310272217 + 0.01 * 6.587341785430908
Epoch 920, val loss: 1.2188904285430908
Epoch 930, training loss: 0.06884525716304779 = 0.0035216212272644043 + 0.01 * 6.5323638916015625
Epoch 930, val loss: 1.222075343132019
Epoch 940, training loss: 0.06865128874778748 = 0.003432700177654624 + 0.01 * 6.521859169006348
Epoch 940, val loss: 1.225171685218811
Epoch 950, training loss: 0.06854431331157684 = 0.003347430843859911 + 0.01 * 6.519688129425049
Epoch 950, val loss: 1.2282114028930664
Epoch 960, training loss: 0.06854549795389175 = 0.0032662302255630493 + 0.01 * 6.527926921844482
Epoch 960, val loss: 1.231171727180481
Epoch 970, training loss: 0.06828448921442032 = 0.0031884543132036924 + 0.01 * 6.509603500366211
Epoch 970, val loss: 1.234074354171753
Epoch 980, training loss: 0.06840133666992188 = 0.0031140728387981653 + 0.01 * 6.528726100921631
Epoch 980, val loss: 1.236899733543396
Epoch 990, training loss: 0.0682196095585823 = 0.003042830154299736 + 0.01 * 6.5176777839660645
Epoch 990, val loss: 1.2396732568740845
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.4317
Flip ASR: 0.3289/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0410985946655273 = 1.9573601484298706 + 0.01 * 8.373852729797363
Epoch 0, val loss: 1.9622318744659424
Epoch 10, training loss: 2.0301930904388428 = 1.9464553594589233 + 0.01 * 8.373779296875
Epoch 10, val loss: 1.9510618448257446
Epoch 20, training loss: 2.0169570446014404 = 1.9332221746444702 + 0.01 * 8.373488426208496
Epoch 20, val loss: 1.9367187023162842
Epoch 30, training loss: 1.9986982345581055 = 1.914971947669983 + 0.01 * 8.37263011932373
Epoch 30, val loss: 1.9164118766784668
Epoch 40, training loss: 1.9718596935272217 = 1.8881726264953613 + 0.01 * 8.368708610534668
Epoch 40, val loss: 1.8865737915039062
Epoch 50, training loss: 1.9336599111557007 = 1.8502132892608643 + 0.01 * 8.344661712646484
Epoch 50, val loss: 1.8455429077148438
Epoch 60, training loss: 1.8871729373931885 = 1.8048961162567139 + 0.01 * 8.227686882019043
Epoch 60, val loss: 1.799832820892334
Epoch 70, training loss: 1.840412974357605 = 1.7614846229553223 + 0.01 * 7.892836570739746
Epoch 70, val loss: 1.7590407133102417
Epoch 80, training loss: 1.7890836000442505 = 1.7124755382537842 + 0.01 * 7.660810470581055
Epoch 80, val loss: 1.7139451503753662
Epoch 90, training loss: 1.721269965171814 = 1.6462363004684448 + 0.01 * 7.503364562988281
Epoch 90, val loss: 1.6562581062316895
Epoch 100, training loss: 1.633144497871399 = 1.5591098070144653 + 0.01 * 7.403469085693359
Epoch 100, val loss: 1.583121657371521
Epoch 110, training loss: 1.5306016206741333 = 1.4574518203735352 + 0.01 * 7.314981460571289
Epoch 110, val loss: 1.4999641180038452
Epoch 120, training loss: 1.427525281906128 = 1.3551931381225586 + 0.01 * 7.233214855194092
Epoch 120, val loss: 1.4194658994674683
Epoch 130, training loss: 1.3331130743026733 = 1.2618197202682495 + 0.01 * 7.129337787628174
Epoch 130, val loss: 1.3490164279937744
Epoch 140, training loss: 1.248435139656067 = 1.177896499633789 + 0.01 * 7.053865432739258
Epoch 140, val loss: 1.2880139350891113
Epoch 150, training loss: 1.170845627784729 = 1.1007120609283447 + 0.01 * 7.013360023498535
Epoch 150, val loss: 1.2331441640853882
Epoch 160, training loss: 1.0968656539916992 = 1.0270174741744995 + 0.01 * 6.984813690185547
Epoch 160, val loss: 1.1801931858062744
Epoch 170, training loss: 1.0247573852539062 = 0.9551790952682495 + 0.01 * 6.957828044891357
Epoch 170, val loss: 1.1278870105743408
Epoch 180, training loss: 0.9543163776397705 = 0.8849915266036987 + 0.01 * 6.9324822425842285
Epoch 180, val loss: 1.0765923261642456
Epoch 190, training loss: 0.8852866291999817 = 0.8161370754241943 + 0.01 * 6.914957523345947
Epoch 190, val loss: 1.0272252559661865
Epoch 200, training loss: 0.8171147108078003 = 0.7480718493461609 + 0.01 * 6.904284954071045
Epoch 200, val loss: 0.9802242517471313
Epoch 210, training loss: 0.7499881386756897 = 0.6809980273246765 + 0.01 * 6.8990092277526855
Epoch 210, val loss: 0.936044454574585
Epoch 220, training loss: 0.6851809024810791 = 0.6162253618240356 + 0.01 * 6.895552635192871
Epoch 220, val loss: 0.8952478170394897
Epoch 230, training loss: 0.624087929725647 = 0.5551640391349792 + 0.01 * 6.892389297485352
Epoch 230, val loss: 0.8590649366378784
Epoch 240, training loss: 0.5677275657653809 = 0.4988366365432739 + 0.01 * 6.88909387588501
Epoch 240, val loss: 0.8286865949630737
Epoch 250, training loss: 0.5163887739181519 = 0.4475247263908386 + 0.01 * 6.886402130126953
Epoch 250, val loss: 0.8041374087333679
Epoch 260, training loss: 0.4695979058742523 = 0.4007645547389984 + 0.01 * 6.883335113525391
Epoch 260, val loss: 0.7851906418800354
Epoch 270, training loss: 0.4267582893371582 = 0.35796868801116943 + 0.01 * 6.878961086273193
Epoch 270, val loss: 0.7707875370979309
Epoch 280, training loss: 0.38733914494514465 = 0.31858593225479126 + 0.01 * 6.875322341918945
Epoch 280, val loss: 0.760194718837738
Epoch 290, training loss: 0.3510940968990326 = 0.2823478579521179 + 0.01 * 6.8746232986450195
Epoch 290, val loss: 0.7529983520507812
Epoch 300, training loss: 0.3180917203426361 = 0.2494032084941864 + 0.01 * 6.868851184844971
Epoch 300, val loss: 0.7492111921310425
Epoch 310, training loss: 0.28863584995269775 = 0.2199830412864685 + 0.01 * 6.865279674530029
Epoch 310, val loss: 0.7488217353820801
Epoch 320, training loss: 0.26287418603897095 = 0.19427458941936493 + 0.01 * 6.859958648681641
Epoch 320, val loss: 0.7519822716712952
Epoch 330, training loss: 0.24080103635787964 = 0.17224401235580444 + 0.01 * 6.855702877044678
Epoch 330, val loss: 0.7583854794502258
Epoch 340, training loss: 0.22205230593681335 = 0.15351668000221252 + 0.01 * 6.853562355041504
Epoch 340, val loss: 0.7676740288734436
Epoch 350, training loss: 0.20604470372200012 = 0.13758614659309387 + 0.01 * 6.845856189727783
Epoch 350, val loss: 0.7795013189315796
Epoch 360, training loss: 0.1923040747642517 = 0.12391263991594315 + 0.01 * 6.839143753051758
Epoch 360, val loss: 0.793258786201477
Epoch 370, training loss: 0.180327370762825 = 0.11202367395162582 + 0.01 * 6.83036994934082
Epoch 370, val loss: 0.8085700869560242
Epoch 380, training loss: 0.16990934312343597 = 0.10155809670686722 + 0.01 * 6.835124969482422
Epoch 380, val loss: 0.8252032399177551
Epoch 390, training loss: 0.16042429208755493 = 0.09224506467580795 + 0.01 * 6.817923069000244
Epoch 390, val loss: 0.8428549766540527
Epoch 400, training loss: 0.15192365646362305 = 0.0838961973786354 + 0.01 * 6.802746772766113
Epoch 400, val loss: 0.8612784743309021
Epoch 410, training loss: 0.14432433247566223 = 0.07635252922773361 + 0.01 * 6.79718017578125
Epoch 410, val loss: 0.8801056146621704
Epoch 420, training loss: 0.13747158646583557 = 0.06950933486223221 + 0.01 * 6.7962260246276855
Epoch 420, val loss: 0.8992380499839783
Epoch 430, training loss: 0.1312103420495987 = 0.06329219788312912 + 0.01 * 6.791814804077148
Epoch 430, val loss: 0.9185044169425964
Epoch 440, training loss: 0.12544670701026917 = 0.057654112577438354 + 0.01 * 6.779260158538818
Epoch 440, val loss: 0.938033938407898
Epoch 450, training loss: 0.12034623324871063 = 0.0525546632707119 + 0.01 * 6.779156684875488
Epoch 450, val loss: 0.9577182531356812
Epoch 460, training loss: 0.11571565270423889 = 0.047983914613723755 + 0.01 * 6.773174285888672
Epoch 460, val loss: 0.9774077534675598
Epoch 470, training loss: 0.11160364747047424 = 0.04387775808572769 + 0.01 * 6.772588729858398
Epoch 470, val loss: 0.9969498515129089
Epoch 480, training loss: 0.10777023434638977 = 0.04018097370862961 + 0.01 * 6.758925914764404
Epoch 480, val loss: 1.0161056518554688
Epoch 490, training loss: 0.10438765585422516 = 0.03684283420443535 + 0.01 * 6.754481792449951
Epoch 490, val loss: 1.0348116159439087
Epoch 500, training loss: 0.10143205523490906 = 0.03379953280091286 + 0.01 * 6.763251781463623
Epoch 500, val loss: 1.0530458688735962
Epoch 510, training loss: 0.09846307337284088 = 0.031020045280456543 + 0.01 * 6.744302749633789
Epoch 510, val loss: 1.0703498125076294
Epoch 520, training loss: 0.0958692654967308 = 0.02849034033715725 + 0.01 * 6.7378926277160645
Epoch 520, val loss: 1.0872644186019897
Epoch 530, training loss: 0.0936458483338356 = 0.026183409616351128 + 0.01 * 6.746243953704834
Epoch 530, val loss: 1.1037368774414062
Epoch 540, training loss: 0.09149173647165298 = 0.02410491742193699 + 0.01 * 6.738682270050049
Epoch 540, val loss: 1.1196986436843872
Epoch 550, training loss: 0.08943089097738266 = 0.022232696413993835 + 0.01 * 6.71981954574585
Epoch 550, val loss: 1.1350762844085693
Epoch 560, training loss: 0.08775102347135544 = 0.020551664754748344 + 0.01 * 6.719935894012451
Epoch 560, val loss: 1.1499818563461304
Epoch 570, training loss: 0.0861349105834961 = 0.01904185302555561 + 0.01 * 6.709306240081787
Epoch 570, val loss: 1.164448857307434
Epoch 580, training loss: 0.08474917709827423 = 0.01767769083380699 + 0.01 * 6.70714807510376
Epoch 580, val loss: 1.1784627437591553
Epoch 590, training loss: 0.0835590660572052 = 0.01644314080476761 + 0.01 * 6.711592674255371
Epoch 590, val loss: 1.1918684244155884
Epoch 600, training loss: 0.08236882090568542 = 0.0153201874345541 + 0.01 * 6.70486307144165
Epoch 600, val loss: 1.2048174142837524
Epoch 610, training loss: 0.08119793981313705 = 0.014301473274827003 + 0.01 * 6.689647197723389
Epoch 610, val loss: 1.217584490776062
Epoch 620, training loss: 0.0804014801979065 = 0.013368818908929825 + 0.01 * 6.703266143798828
Epoch 620, val loss: 1.2297483682632446
Epoch 630, training loss: 0.07922211289405823 = 0.012511015869677067 + 0.01 * 6.671109676361084
Epoch 630, val loss: 1.241497278213501
Epoch 640, training loss: 0.07853319495916367 = 0.011721553280949593 + 0.01 * 6.681164741516113
Epoch 640, val loss: 1.2529621124267578
Epoch 650, training loss: 0.07771112024784088 = 0.011001369915902615 + 0.01 * 6.6709747314453125
Epoch 650, val loss: 1.2639645338058472
Epoch 660, training loss: 0.077052541077137 = 0.010351010598242283 + 0.01 * 6.6701531410217285
Epoch 660, val loss: 1.2747193574905396
Epoch 670, training loss: 0.07635926455259323 = 0.009767419658601284 + 0.01 * 6.659184455871582
Epoch 670, val loss: 1.284997820854187
Epoch 680, training loss: 0.075827956199646 = 0.009240204468369484 + 0.01 * 6.658775329589844
Epoch 680, val loss: 1.2950109243392944
Epoch 690, training loss: 0.07525321841239929 = 0.008764037862420082 + 0.01 * 6.648918628692627
Epoch 690, val loss: 1.3046232461929321
Epoch 700, training loss: 0.07482802867889404 = 0.008328848518431187 + 0.01 * 6.649918556213379
Epoch 700, val loss: 1.3139598369598389
Epoch 710, training loss: 0.07422599196434021 = 0.007930364459753036 + 0.01 * 6.629563331604004
Epoch 710, val loss: 1.3230164051055908
Epoch 720, training loss: 0.07377008348703384 = 0.0075636329129338264 + 0.01 * 6.620644569396973
Epoch 720, val loss: 1.3317452669143677
Epoch 730, training loss: 0.07340813428163528 = 0.007225488778203726 + 0.01 * 6.618264198303223
Epoch 730, val loss: 1.3402601480484009
Epoch 740, training loss: 0.07325755059719086 = 0.006912766490131617 + 0.01 * 6.6344780921936035
Epoch 740, val loss: 1.3485463857650757
Epoch 750, training loss: 0.07287836074829102 = 0.006623647641390562 + 0.01 * 6.625471115112305
Epoch 750, val loss: 1.3566067218780518
Epoch 760, training loss: 0.07235238701105118 = 0.0063551319763064384 + 0.01 * 6.599725723266602
Epoch 760, val loss: 1.3644298315048218
Epoch 770, training loss: 0.07219262421131134 = 0.0061048176139593124 + 0.01 * 6.608780860900879
Epoch 770, val loss: 1.372079610824585
Epoch 780, training loss: 0.07197065651416779 = 0.005871157627552748 + 0.01 * 6.609950542449951
Epoch 780, val loss: 1.3794994354248047
Epoch 790, training loss: 0.07164641469717026 = 0.0056536546908319 + 0.01 * 6.599275588989258
Epoch 790, val loss: 1.3867498636245728
Epoch 800, training loss: 0.07170265167951584 = 0.005449302028864622 + 0.01 * 6.625334739685059
Epoch 800, val loss: 1.393830418586731
Epoch 810, training loss: 0.07116585224866867 = 0.005257979966700077 + 0.01 * 6.590787410736084
Epoch 810, val loss: 1.40072500705719
Epoch 820, training loss: 0.07089494168758392 = 0.005077729467302561 + 0.01 * 6.581721305847168
Epoch 820, val loss: 1.407414197921753
Epoch 830, training loss: 0.07066015154123306 = 0.004908042028546333 + 0.01 * 6.575211048126221
Epoch 830, val loss: 1.4140183925628662
Epoch 840, training loss: 0.07041434943675995 = 0.004747698549181223 + 0.01 * 6.5666656494140625
Epoch 840, val loss: 1.4204554557800293
Epoch 850, training loss: 0.07049571722745895 = 0.004596383310854435 + 0.01 * 6.589933395385742
Epoch 850, val loss: 1.4267168045043945
Epoch 860, training loss: 0.07023385167121887 = 0.00445336801931262 + 0.01 * 6.578048229217529
Epoch 860, val loss: 1.4328951835632324
Epoch 870, training loss: 0.06999655067920685 = 0.004317227751016617 + 0.01 * 6.567933082580566
Epoch 870, val loss: 1.4389073848724365
Epoch 880, training loss: 0.06982004642486572 = 0.004188962746411562 + 0.01 * 6.563108444213867
Epoch 880, val loss: 1.4447740316390991
Epoch 890, training loss: 0.06965340673923492 = 0.004066838417202234 + 0.01 * 6.558656692504883
Epoch 890, val loss: 1.450556755065918
Epoch 900, training loss: 0.06957738846540451 = 0.003950568847358227 + 0.01 * 6.562682628631592
Epoch 900, val loss: 1.4562098979949951
Epoch 910, training loss: 0.06946120411157608 = 0.0038403572980314493 + 0.01 * 6.562084674835205
Epoch 910, val loss: 1.461801290512085
Epoch 920, training loss: 0.0691458061337471 = 0.003735222155228257 + 0.01 * 6.541058540344238
Epoch 920, val loss: 1.4672561883926392
Epoch 930, training loss: 0.06930813938379288 = 0.003634953871369362 + 0.01 * 6.567318916320801
Epoch 930, val loss: 1.4725276231765747
Epoch 940, training loss: 0.06895242631435394 = 0.003539767349138856 + 0.01 * 6.541265964508057
Epoch 940, val loss: 1.477792739868164
Epoch 950, training loss: 0.06884025782346725 = 0.0034483480267226696 + 0.01 * 6.539191246032715
Epoch 950, val loss: 1.4829527139663696
Epoch 960, training loss: 0.06867619603872299 = 0.0033614663407206535 + 0.01 * 6.531473159790039
Epoch 960, val loss: 1.4879180192947388
Epoch 970, training loss: 0.06878373771905899 = 0.003278101561591029 + 0.01 * 6.550563812255859
Epoch 970, val loss: 1.4929251670837402
Epoch 980, training loss: 0.0683814287185669 = 0.0031988308764994144 + 0.01 * 6.5182600021362305
Epoch 980, val loss: 1.497730016708374
Epoch 990, training loss: 0.0683075487613678 = 0.0031229977030307055 + 0.01 * 6.5184550285339355
Epoch 990, val loss: 1.502456784248352
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.7786
Flip ASR: 0.7422/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0273594856262207 = 1.9436211585998535 + 0.01 * 8.373838424682617
Epoch 0, val loss: 1.9442837238311768
Epoch 10, training loss: 2.0167043209075928 = 1.9329670667648315 + 0.01 * 8.373726844787598
Epoch 10, val loss: 1.9331461191177368
Epoch 20, training loss: 2.0032896995544434 = 1.919555425643921 + 0.01 * 8.373435974121094
Epoch 20, val loss: 1.919118046760559
Epoch 30, training loss: 1.9842950105667114 = 1.9005686044692993 + 0.01 * 8.372641563415527
Epoch 30, val loss: 1.8994927406311035
Epoch 40, training loss: 1.9566799402236938 = 1.8729908466339111 + 0.01 * 8.368905067443848
Epoch 40, val loss: 1.8718225955963135
Epoch 50, training loss: 1.9192123413085938 = 1.8357820510864258 + 0.01 * 8.343029022216797
Epoch 50, val loss: 1.836654782295227
Epoch 60, training loss: 1.8750048875808716 = 1.7935106754302979 + 0.01 * 8.149422645568848
Epoch 60, val loss: 1.8001594543457031
Epoch 70, training loss: 1.826568603515625 = 1.7486851215362549 + 0.01 * 7.788352012634277
Epoch 70, val loss: 1.7621183395385742
Epoch 80, training loss: 1.7661395072937012 = 1.6917561292648315 + 0.01 * 7.43834114074707
Epoch 80, val loss: 1.7118996381759644
Epoch 90, training loss: 1.688904047012329 = 1.6169929504394531 + 0.01 * 7.1911139488220215
Epoch 90, val loss: 1.6469101905822754
Epoch 100, training loss: 1.5953329801559448 = 1.5246824026107788 + 0.01 * 7.065053939819336
Epoch 100, val loss: 1.5683281421661377
Epoch 110, training loss: 1.4923678636550903 = 1.4220869541168213 + 0.01 * 7.0280961990356445
Epoch 110, val loss: 1.4835567474365234
Epoch 120, training loss: 1.3863720893859863 = 1.316239356994629 + 0.01 * 7.013269424438477
Epoch 120, val loss: 1.39945650100708
Epoch 130, training loss: 1.279304027557373 = 1.209303379058838 + 0.01 * 7.000063419342041
Epoch 130, val loss: 1.3177167177200317
Epoch 140, training loss: 1.1729707717895508 = 1.1030259132385254 + 0.01 * 6.994482040405273
Epoch 140, val loss: 1.2373125553131104
Epoch 150, training loss: 1.0695195198059082 = 0.9996289610862732 + 0.01 * 6.989054203033447
Epoch 150, val loss: 1.1605113744735718
Epoch 160, training loss: 0.9709723591804504 = 0.901170551776886 + 0.01 * 6.980179786682129
Epoch 160, val loss: 1.08881413936615
Epoch 170, training loss: 0.8785790801048279 = 0.8089087009429932 + 0.01 * 6.967040061950684
Epoch 170, val loss: 1.0222448110580444
Epoch 180, training loss: 0.7928236722946167 = 0.7233255505561829 + 0.01 * 6.949814319610596
Epoch 180, val loss: 0.9612689018249512
Epoch 190, training loss: 0.7135790586471558 = 0.6442796587944031 + 0.01 * 6.929940700531006
Epoch 190, val loss: 0.9065419435501099
Epoch 200, training loss: 0.6401330828666687 = 0.5710319876670837 + 0.01 * 6.910111904144287
Epoch 200, val loss: 0.8581175804138184
Epoch 210, training loss: 0.5719997882843018 = 0.5030425190925598 + 0.01 * 6.895724773406982
Epoch 210, val loss: 0.8161702752113342
Epoch 220, training loss: 0.509093165397644 = 0.44027024507522583 + 0.01 * 6.882290363311768
Epoch 220, val loss: 0.781246542930603
Epoch 230, training loss: 0.45196259021759033 = 0.3832381069660187 + 0.01 * 6.872448444366455
Epoch 230, val loss: 0.7539340853691101
Epoch 240, training loss: 0.40117424726486206 = 0.33251604437828064 + 0.01 * 6.86582088470459
Epoch 240, val loss: 0.7340782284736633
Epoch 250, training loss: 0.35686367750167847 = 0.2882996201515198 + 0.01 * 6.856407642364502
Epoch 250, val loss: 0.7205938100814819
Epoch 260, training loss: 0.31886473298072815 = 0.2503627836704254 + 0.01 * 6.850194931030273
Epoch 260, val loss: 0.7125953435897827
Epoch 270, training loss: 0.2865869700908661 = 0.21812748908996582 + 0.01 * 6.845948219299316
Epoch 270, val loss: 0.7091761827468872
Epoch 280, training loss: 0.25916507840156555 = 0.19080348312854767 + 0.01 * 6.8361592292785645
Epoch 280, val loss: 0.709510087966919
Epoch 290, training loss: 0.23587819933891296 = 0.16757826507091522 + 0.01 * 6.82999324798584
Epoch 290, val loss: 0.7130088806152344
Epoch 300, training loss: 0.2160070836544037 = 0.14773637056350708 + 0.01 * 6.827070713043213
Epoch 300, val loss: 0.7189942598342896
Epoch 310, training loss: 0.19889500737190247 = 0.13069601356983185 + 0.01 * 6.819899559020996
Epoch 310, val loss: 0.7270025610923767
Epoch 320, training loss: 0.1841365396976471 = 0.11598604917526245 + 0.01 * 6.81505012512207
Epoch 320, val loss: 0.7366828322410583
Epoch 330, training loss: 0.17132478952407837 = 0.10322533547878265 + 0.01 * 6.8099446296691895
Epoch 330, val loss: 0.7477617263793945
Epoch 340, training loss: 0.1601717174053192 = 0.09211026132106781 + 0.01 * 6.806146621704102
Epoch 340, val loss: 0.7599583268165588
Epoch 350, training loss: 0.150426983833313 = 0.08239956200122833 + 0.01 * 6.802741527557373
Epoch 350, val loss: 0.7729892134666443
Epoch 360, training loss: 0.14187785983085632 = 0.07388849556446075 + 0.01 * 6.7989373207092285
Epoch 360, val loss: 0.7866624593734741
Epoch 370, training loss: 0.13436146080493927 = 0.06640971451997757 + 0.01 * 6.795175075531006
Epoch 370, val loss: 0.8008398413658142
Epoch 380, training loss: 0.12779667973518372 = 0.05982746183872223 + 0.01 * 6.796921730041504
Epoch 380, val loss: 0.8153732419013977
Epoch 390, training loss: 0.12193573266267776 = 0.05403047800064087 + 0.01 * 6.790525913238525
Epoch 390, val loss: 0.8300784826278687
Epoch 400, training loss: 0.11676599830389023 = 0.048920728266239166 + 0.01 * 6.784526824951172
Epoch 400, val loss: 0.844865083694458
Epoch 410, training loss: 0.1121993362903595 = 0.04441085830330849 + 0.01 * 6.778847694396973
Epoch 410, val loss: 0.8596199750900269
Epoch 420, training loss: 0.10815159976482391 = 0.04042572155594826 + 0.01 * 6.772587776184082
Epoch 420, val loss: 0.8742530941963196
Epoch 430, training loss: 0.10486955940723419 = 0.03689717501401901 + 0.01 * 6.797238349914551
Epoch 430, val loss: 0.8886963725090027
Epoch 440, training loss: 0.1014728844165802 = 0.033770602196455 + 0.01 * 6.770228862762451
Epoch 440, val loss: 0.9028729200363159
Epoch 450, training loss: 0.09858522564172745 = 0.030992189422249794 + 0.01 * 6.759304046630859
Epoch 450, val loss: 0.9168426990509033
Epoch 460, training loss: 0.09603316336870193 = 0.028514914214611053 + 0.01 * 6.751824855804443
Epoch 460, val loss: 0.9304715991020203
Epoch 470, training loss: 0.09375397861003876 = 0.02629633992910385 + 0.01 * 6.745763778686523
Epoch 470, val loss: 0.9438177347183228
Epoch 480, training loss: 0.09171406924724579 = 0.02430710382759571 + 0.01 * 6.740696907043457
Epoch 480, val loss: 0.9568148851394653
Epoch 490, training loss: 0.0899546891450882 = 0.02252286486327648 + 0.01 * 6.743182182312012
Epoch 490, val loss: 0.9695445895195007
Epoch 500, training loss: 0.08820783346891403 = 0.02091839909553528 + 0.01 * 6.728943347930908
Epoch 500, val loss: 0.9818590879440308
Epoch 510, training loss: 0.086848184466362 = 0.019471462815999985 + 0.01 * 6.737671852111816
Epoch 510, val loss: 0.9938169121742249
Epoch 520, training loss: 0.08532563596963882 = 0.01816572993993759 + 0.01 * 6.7159905433654785
Epoch 520, val loss: 1.005476713180542
Epoch 530, training loss: 0.0842638611793518 = 0.016983024775981903 + 0.01 * 6.728084087371826
Epoch 530, val loss: 1.0168946981430054
Epoch 540, training loss: 0.08301323652267456 = 0.015911299735307693 + 0.01 * 6.710194110870361
Epoch 540, val loss: 1.0279078483581543
Epoch 550, training loss: 0.08186963945627213 = 0.014936997555196285 + 0.01 * 6.693264484405518
Epoch 550, val loss: 1.0385812520980835
Epoch 560, training loss: 0.08112802356481552 = 0.014048333279788494 + 0.01 * 6.7079691886901855
Epoch 560, val loss: 1.0489798784255981
Epoch 570, training loss: 0.08011849224567413 = 0.013238710351288319 + 0.01 * 6.687978267669678
Epoch 570, val loss: 1.0591926574707031
Epoch 580, training loss: 0.0792897418141365 = 0.012498004361987114 + 0.01 * 6.679173946380615
Epoch 580, val loss: 1.0690195560455322
Epoch 590, training loss: 0.07853122055530548 = 0.011819688603281975 + 0.01 * 6.6711530685424805
Epoch 590, val loss: 1.078672170639038
Epoch 600, training loss: 0.07784918695688248 = 0.011196887120604515 + 0.01 * 6.6652302742004395
Epoch 600, val loss: 1.0880314111709595
Epoch 610, training loss: 0.07709772884845734 = 0.010623735375702381 + 0.01 * 6.647399425506592
Epoch 610, val loss: 1.0971083641052246
Epoch 620, training loss: 0.07682111114263535 = 0.010095251724123955 + 0.01 * 6.672585964202881
Epoch 620, val loss: 1.1059825420379639
Epoch 630, training loss: 0.07598228752613068 = 0.009607471525669098 + 0.01 * 6.637481689453125
Epoch 630, val loss: 1.1145720481872559
Epoch 640, training loss: 0.07569004595279694 = 0.009156052023172379 + 0.01 * 6.65339994430542
Epoch 640, val loss: 1.1229745149612427
Epoch 650, training loss: 0.07507559657096863 = 0.008737835101783276 + 0.01 * 6.633776664733887
Epoch 650, val loss: 1.1311835050582886
Epoch 660, training loss: 0.07450834661722183 = 0.008349643088877201 + 0.01 * 6.615870475769043
Epoch 660, val loss: 1.1390950679779053
Epoch 670, training loss: 0.07416193187236786 = 0.00798838958144188 + 0.01 * 6.617353916168213
Epoch 670, val loss: 1.146896243095398
Epoch 680, training loss: 0.07385128736495972 = 0.007652072235941887 + 0.01 * 6.619921684265137
Epoch 680, val loss: 1.1543523073196411
Epoch 690, training loss: 0.07336578518152237 = 0.007338827941566706 + 0.01 * 6.602695465087891
Epoch 690, val loss: 1.1618119478225708
Epoch 700, training loss: 0.0733390599489212 = 0.007045937702059746 + 0.01 * 6.629312515258789
Epoch 700, val loss: 1.169005274772644
Epoch 710, training loss: 0.0727388858795166 = 0.0067721265368163586 + 0.01 * 6.596676349639893
Epoch 710, val loss: 1.176062822341919
Epoch 720, training loss: 0.07254111021757126 = 0.006515473127365112 + 0.01 * 6.602563858032227
Epoch 720, val loss: 1.182965874671936
Epoch 730, training loss: 0.07214324176311493 = 0.006274550221860409 + 0.01 * 6.586869239807129
Epoch 730, val loss: 1.1897023916244507
Epoch 740, training loss: 0.07191497087478638 = 0.006047876551747322 + 0.01 * 6.586709976196289
Epoch 740, val loss: 1.196341633796692
Epoch 750, training loss: 0.0716555193066597 = 0.0058348034508526325 + 0.01 * 6.582071781158447
Epoch 750, val loss: 1.202682614326477
Epoch 760, training loss: 0.0713736042380333 = 0.005634377710521221 + 0.01 * 6.573923110961914
Epoch 760, val loss: 1.2090178728103638
Epoch 770, training loss: 0.07149536907672882 = 0.005445251241326332 + 0.01 * 6.605011940002441
Epoch 770, val loss: 1.2150993347167969
Epoch 780, training loss: 0.07087501883506775 = 0.005267058499157429 + 0.01 * 6.56079626083374
Epoch 780, val loss: 1.2211576700210571
Epoch 790, training loss: 0.07107431441545486 = 0.005098687019199133 + 0.01 * 6.59756326675415
Epoch 790, val loss: 1.2269893884658813
Epoch 800, training loss: 0.07048115879297256 = 0.004939609207212925 + 0.01 * 6.554154872894287
Epoch 800, val loss: 1.2327615022659302
Epoch 810, training loss: 0.07034288346767426 = 0.004788945894688368 + 0.01 * 6.555394172668457
Epoch 810, val loss: 1.2383692264556885
Epoch 820, training loss: 0.07018361240625381 = 0.004645994398742914 + 0.01 * 6.5537614822387695
Epoch 820, val loss: 1.243883728981018
Epoch 830, training loss: 0.0699404776096344 = 0.0045104059390723705 + 0.01 * 6.5430073738098145
Epoch 830, val loss: 1.249283790588379
Epoch 840, training loss: 0.07005597651004791 = 0.004381536040455103 + 0.01 * 6.567444324493408
Epoch 840, val loss: 1.2544535398483276
Epoch 850, training loss: 0.06964783370494843 = 0.004259356297552586 + 0.01 * 6.538847923278809
Epoch 850, val loss: 1.2597309350967407
Epoch 860, training loss: 0.06955637782812119 = 0.004143090918660164 + 0.01 * 6.5413289070129395
Epoch 860, val loss: 1.2647020816802979
Epoch 870, training loss: 0.06928767263889313 = 0.004032384138554335 + 0.01 * 6.525529384613037
Epoch 870, val loss: 1.2696632146835327
Epoch 880, training loss: 0.0692349374294281 = 0.003926744684576988 + 0.01 * 6.530818939208984
Epoch 880, val loss: 1.2744601964950562
Epoch 890, training loss: 0.06907494366168976 = 0.00382631435059011 + 0.01 * 6.524863243103027
Epoch 890, val loss: 1.2792470455169678
Epoch 900, training loss: 0.0690520778298378 = 0.0037302826531231403 + 0.01 * 6.532179355621338
Epoch 900, val loss: 1.2837228775024414
Epoch 910, training loss: 0.06895209848880768 = 0.0036388738080859184 + 0.01 * 6.531322956085205
Epoch 910, val loss: 1.2884647846221924
Epoch 920, training loss: 0.06871756166219711 = 0.0035513138864189386 + 0.01 * 6.51662540435791
Epoch 920, val loss: 1.2927526235580444
Epoch 930, training loss: 0.06866941601037979 = 0.003467672038823366 + 0.01 * 6.520174503326416
Epoch 930, val loss: 1.2972828149795532
Epoch 940, training loss: 0.06852579861879349 = 0.0033874742221087217 + 0.01 * 6.5138325691223145
Epoch 940, val loss: 1.3013687133789062
Epoch 950, training loss: 0.06861318647861481 = 0.0033108354546129704 + 0.01 * 6.530235290527344
Epoch 950, val loss: 1.3057422637939453
Epoch 960, training loss: 0.06830092519521713 = 0.0032372872810810804 + 0.01 * 6.506364345550537
Epoch 960, val loss: 1.3097193241119385
Epoch 970, training loss: 0.06806528568267822 = 0.0031668629962950945 + 0.01 * 6.489842891693115
Epoch 970, val loss: 1.3136968612670898
Epoch 980, training loss: 0.06808547675609589 = 0.003099416382610798 + 0.01 * 6.498605728149414
Epoch 980, val loss: 1.3177318572998047
Epoch 990, training loss: 0.06801923364400864 = 0.003034466877579689 + 0.01 * 6.498476982116699
Epoch 990, val loss: 1.3215240240097046
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.7306
Flip ASR: 0.6756/225 nodes
The final ASR:0.64699, 0.15346, Accuracy:0.82346, 0.00924
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11636])
remove edge: torch.Size([2, 9508])
updated graph: torch.Size([2, 10588])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.97909, 0.00174, Accuracy:0.82963, 0.00524
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0337016582489014 = 1.9499626159667969 + 0.01 * 8.37390422821045
Epoch 0, val loss: 1.949182391166687
Epoch 10, training loss: 2.0230772495269775 = 1.9393389225006104 + 0.01 * 8.373831748962402
Epoch 10, val loss: 1.9392752647399902
Epoch 20, training loss: 2.0099778175354004 = 1.9262416362762451 + 0.01 * 8.373607635498047
Epoch 20, val loss: 1.92669677734375
Epoch 30, training loss: 1.9917232990264893 = 1.9079936742782593 + 0.01 * 8.372958183288574
Epoch 30, val loss: 1.9088516235351562
Epoch 40, training loss: 1.964827537536621 = 1.881130576133728 + 0.01 * 8.3696928024292
Epoch 40, val loss: 1.8826972246170044
Epoch 50, training loss: 1.9270918369293213 = 1.8436414003372192 + 0.01 * 8.345047950744629
Epoch 50, val loss: 1.8478208780288696
Epoch 60, training loss: 1.8835183382034302 = 1.8015987873077393 + 0.01 * 8.191951751708984
Epoch 60, val loss: 1.81313157081604
Epoch 70, training loss: 1.8438169956207275 = 1.7640355825424194 + 0.01 * 7.9781413078308105
Epoch 70, val loss: 1.7839691638946533
Epoch 80, training loss: 1.7929362058639526 = 1.7152807712554932 + 0.01 * 7.765549182891846
Epoch 80, val loss: 1.7403535842895508
Epoch 90, training loss: 1.7237168550491333 = 1.6486024856567383 + 0.01 * 7.51143741607666
Epoch 90, val loss: 1.6823513507843018
Epoch 100, training loss: 1.6357697248458862 = 1.5637083053588867 + 0.01 * 7.206144332885742
Epoch 100, val loss: 1.6124624013900757
Epoch 110, training loss: 1.5413569211959839 = 1.4707919359207153 + 0.01 * 7.056493282318115
Epoch 110, val loss: 1.5365842580795288
Epoch 120, training loss: 1.4495470523834229 = 1.379298448562622 + 0.01 * 7.024866104125977
Epoch 120, val loss: 1.4641923904418945
Epoch 130, training loss: 1.3604028224945068 = 1.2905186414718628 + 0.01 * 6.988418102264404
Epoch 130, val loss: 1.3950177431106567
Epoch 140, training loss: 1.27250337600708 = 1.2028288841247559 + 0.01 * 6.967453956604004
Epoch 140, val loss: 1.3278216123580933
Epoch 150, training loss: 1.1866507530212402 = 1.117103099822998 + 0.01 * 6.95476770401001
Epoch 150, val loss: 1.2630103826522827
Epoch 160, training loss: 1.1056325435638428 = 1.036163568496704 + 0.01 * 6.946896076202393
Epoch 160, val loss: 1.202621579170227
Epoch 170, training loss: 1.0306644439697266 = 0.9612346291542053 + 0.01 * 6.942977428436279
Epoch 170, val loss: 1.1476775407791138
Epoch 180, training loss: 0.9604883790016174 = 0.8910619020462036 + 0.01 * 6.9426493644714355
Epoch 180, val loss: 1.0966932773590088
Epoch 190, training loss: 0.8924604058265686 = 0.8230301141738892 + 0.01 * 6.943029403686523
Epoch 190, val loss: 1.0475932359695435
Epoch 200, training loss: 0.8243541121482849 = 0.7549129724502563 + 0.01 * 6.9441118240356445
Epoch 200, val loss: 0.9983318448066711
Epoch 210, training loss: 0.7557728290557861 = 0.6863240599632263 + 0.01 * 6.944875240325928
Epoch 210, val loss: 0.9487681984901428
Epoch 220, training loss: 0.6884410381317139 = 0.6189910769462585 + 0.01 * 6.944998741149902
Epoch 220, val loss: 0.9002271294593811
Epoch 230, training loss: 0.6253808736801147 = 0.5559388399124146 + 0.01 * 6.9442009925842285
Epoch 230, val loss: 0.8553926944732666
Epoch 240, training loss: 0.569258987903595 = 0.49983343482017517 + 0.01 * 6.9425578117370605
Epoch 240, val loss: 0.8174171447753906
Epoch 250, training loss: 0.5208421349525452 = 0.45144543051719666 + 0.01 * 6.939670085906982
Epoch 250, val loss: 0.7869857549667358
Epoch 260, training loss: 0.4788828492164612 = 0.4095265567302704 + 0.01 * 6.935628890991211
Epoch 260, val loss: 0.763201892375946
Epoch 270, training loss: 0.441289484500885 = 0.3719888925552368 + 0.01 * 6.930060386657715
Epoch 270, val loss: 0.7442013025283813
Epoch 280, training loss: 0.40621909499168396 = 0.33698156476020813 + 0.01 * 6.923753261566162
Epoch 280, val loss: 0.7282674908638
Epoch 290, training loss: 0.37239399552345276 = 0.30325236916542053 + 0.01 * 6.914161682128906
Epoch 290, val loss: 0.7144598364830017
Epoch 300, training loss: 0.3391651213169098 = 0.2701112627983093 + 0.01 * 6.905386447906494
Epoch 300, val loss: 0.7021574974060059
Epoch 310, training loss: 0.3063884973526001 = 0.23743678629398346 + 0.01 * 6.895170211791992
Epoch 310, val loss: 0.6910372376441956
Epoch 320, training loss: 0.27456241846084595 = 0.2057337611913681 + 0.01 * 6.882864952087402
Epoch 320, val loss: 0.6812271475791931
Epoch 330, training loss: 0.24483387172222137 = 0.1760970503091812 + 0.01 * 6.873682498931885
Epoch 330, val loss: 0.6731476783752441
Epoch 340, training loss: 0.2183828055858612 = 0.1497320979833603 + 0.01 * 6.865070343017578
Epoch 340, val loss: 0.667613685131073
Epoch 350, training loss: 0.1957813799381256 = 0.12725457549095154 + 0.01 * 6.852681636810303
Epoch 350, val loss: 0.6650315523147583
Epoch 360, training loss: 0.17701366543769836 = 0.10858474671840668 + 0.01 * 6.842891693115234
Epoch 360, val loss: 0.6656012535095215
Epoch 370, training loss: 0.161589115858078 = 0.0932609811425209 + 0.01 * 6.832813262939453
Epoch 370, val loss: 0.6689038276672363
Epoch 380, training loss: 0.14895564317703247 = 0.08070036768913269 + 0.01 * 6.825528144836426
Epoch 380, val loss: 0.6745116114616394
Epoch 390, training loss: 0.13848361372947693 = 0.07037114351987839 + 0.01 * 6.8112473487854
Epoch 390, val loss: 0.6818494200706482
Epoch 400, training loss: 0.12981247901916504 = 0.061820391565561295 + 0.01 * 6.799208641052246
Epoch 400, val loss: 0.6905590891838074
Epoch 410, training loss: 0.12265384197235107 = 0.05468354746699333 + 0.01 * 6.7970290184021
Epoch 410, val loss: 0.7001991271972656
Epoch 420, training loss: 0.11656360328197479 = 0.048673491925001144 + 0.01 * 6.789011478424072
Epoch 420, val loss: 0.7104966640472412
Epoch 430, training loss: 0.11130928993225098 = 0.04356810078024864 + 0.01 * 6.774118900299072
Epoch 430, val loss: 0.7212169170379639
Epoch 440, training loss: 0.10735013335943222 = 0.039196476340293884 + 0.01 * 6.815365791320801
Epoch 440, val loss: 0.7321455478668213
Epoch 450, training loss: 0.10305435955524445 = 0.03543735295534134 + 0.01 * 6.7617011070251465
Epoch 450, val loss: 0.7430757880210876
Epoch 460, training loss: 0.09976460039615631 = 0.03218023106455803 + 0.01 * 6.758436679840088
Epoch 460, val loss: 0.7539560198783875
Epoch 470, training loss: 0.0967949777841568 = 0.029339546337723732 + 0.01 * 6.745543479919434
Epoch 470, val loss: 0.7647305130958557
Epoch 480, training loss: 0.09425736963748932 = 0.0268488097935915 + 0.01 * 6.740856170654297
Epoch 480, val loss: 0.7753804922103882
Epoch 490, training loss: 0.09206071496009827 = 0.024656396359205246 + 0.01 * 6.740432262420654
Epoch 490, val loss: 0.7858599424362183
Epoch 500, training loss: 0.09006503224372864 = 0.022719144821166992 + 0.01 * 6.734589099884033
Epoch 500, val loss: 0.7961615324020386
Epoch 510, training loss: 0.08817267417907715 = 0.021000435575842857 + 0.01 * 6.717223644256592
Epoch 510, val loss: 0.8062130808830261
Epoch 520, training loss: 0.08676795661449432 = 0.019468095153570175 + 0.01 * 6.729986190795898
Epoch 520, val loss: 0.8160557150840759
Epoch 530, training loss: 0.0852743461728096 = 0.018100060522556305 + 0.01 * 6.717428684234619
Epoch 530, val loss: 0.8256471753120422
Epoch 540, training loss: 0.08389237523078918 = 0.016873160377144814 + 0.01 * 6.7019219398498535
Epoch 540, val loss: 0.8350154757499695
Epoch 550, training loss: 0.0826919749379158 = 0.015768000856041908 + 0.01 * 6.692397594451904
Epoch 550, val loss: 0.8441583514213562
Epoch 560, training loss: 0.08174073696136475 = 0.014769480563700199 + 0.01 * 6.6971259117126465
Epoch 560, val loss: 0.8531161546707153
Epoch 570, training loss: 0.08074171096086502 = 0.013865920715034008 + 0.01 * 6.687579154968262
Epoch 570, val loss: 0.8618071675300598
Epoch 580, training loss: 0.07991372048854828 = 0.01304468885064125 + 0.01 * 6.686903953552246
Epoch 580, val loss: 0.8703200817108154
Epoch 590, training loss: 0.07901743799448013 = 0.012297029606997967 + 0.01 * 6.672040939331055
Epoch 590, val loss: 0.8785958886146545
Epoch 600, training loss: 0.07836608588695526 = 0.01161366980522871 + 0.01 * 6.675241947174072
Epoch 600, val loss: 0.8866954445838928
Epoch 610, training loss: 0.0776071846485138 = 0.010987491346895695 + 0.01 * 6.6619696617126465
Epoch 610, val loss: 0.8945972323417664
Epoch 620, training loss: 0.07711263746023178 = 0.010411664843559265 + 0.01 * 6.670097351074219
Epoch 620, val loss: 0.9023445844650269
Epoch 630, training loss: 0.07639287412166595 = 0.009882669895887375 + 0.01 * 6.6510210037231445
Epoch 630, val loss: 0.9099205136299133
Epoch 640, training loss: 0.07588444650173187 = 0.009393755346536636 + 0.01 * 6.649069309234619
Epoch 640, val loss: 0.9173345565795898
Epoch 650, training loss: 0.07534420490264893 = 0.008940836414694786 + 0.01 * 6.640336513519287
Epoch 650, val loss: 0.9246039390563965
Epoch 660, training loss: 0.07489535212516785 = 0.008520584553480148 + 0.01 * 6.637476444244385
Epoch 660, val loss: 0.9317442774772644
Epoch 670, training loss: 0.07452519237995148 = 0.008130829781293869 + 0.01 * 6.639436721801758
Epoch 670, val loss: 0.9386734962463379
Epoch 680, training loss: 0.07412388920783997 = 0.007768353912979364 + 0.01 * 6.63555383682251
Epoch 680, val loss: 0.9454822540283203
Epoch 690, training loss: 0.0736488401889801 = 0.007430154364556074 + 0.01 * 6.621869087219238
Epoch 690, val loss: 0.95218425989151
Epoch 700, training loss: 0.07347411662340164 = 0.007113910745829344 + 0.01 * 6.636020660400391
Epoch 700, val loss: 0.9587849974632263
Epoch 710, training loss: 0.07307993620634079 = 0.006818691734224558 + 0.01 * 6.626124858856201
Epoch 710, val loss: 0.9652693271636963
Epoch 720, training loss: 0.07266828417778015 = 0.006541699171066284 + 0.01 * 6.612658500671387
Epoch 720, val loss: 0.9716004729270935
Epoch 730, training loss: 0.07238920778036118 = 0.006281836424022913 + 0.01 * 6.610736846923828
Epoch 730, val loss: 0.977910041809082
Epoch 740, training loss: 0.07210429012775421 = 0.006037828046828508 + 0.01 * 6.6066460609436035
Epoch 740, val loss: 0.9840517640113831
Epoch 750, training loss: 0.07192333042621613 = 0.005808238871395588 + 0.01 * 6.611508846282959
Epoch 750, val loss: 0.9901100397109985
Epoch 760, training loss: 0.07153744995594025 = 0.0055921743623912334 + 0.01 * 6.594527721405029
Epoch 760, val loss: 0.9960426688194275
Epoch 770, training loss: 0.07136866450309753 = 0.005388366989791393 + 0.01 * 6.598030090332031
Epoch 770, val loss: 1.001922607421875
Epoch 780, training loss: 0.07108427584171295 = 0.005196034908294678 + 0.01 * 6.588824272155762
Epoch 780, val loss: 1.0076916217803955
Epoch 790, training loss: 0.07099560648202896 = 0.005014491733163595 + 0.01 * 6.598111629486084
Epoch 790, val loss: 1.013337254524231
Epoch 800, training loss: 0.07063326239585876 = 0.004842899739742279 + 0.01 * 6.579036712646484
Epoch 800, val loss: 1.0188963413238525
Epoch 810, training loss: 0.07052469998598099 = 0.004680604208260775 + 0.01 * 6.584409713745117
Epoch 810, val loss: 1.0243686437606812
Epoch 820, training loss: 0.07037468999624252 = 0.004526970442384481 + 0.01 * 6.584771633148193
Epoch 820, val loss: 1.029732584953308
Epoch 830, training loss: 0.07004707306623459 = 0.004381541628390551 + 0.01 * 6.566552639007568
Epoch 830, val loss: 1.0349862575531006
Epoch 840, training loss: 0.06991645693778992 = 0.004243587143719196 + 0.01 * 6.567286491394043
Epoch 840, val loss: 1.0401908159255981
Epoch 850, training loss: 0.06973403692245483 = 0.0041125016286969185 + 0.01 * 6.562153339385986
Epoch 850, val loss: 1.045264720916748
Epoch 860, training loss: 0.06960412114858627 = 0.003988326061517 + 0.01 * 6.561579704284668
Epoch 860, val loss: 1.0502784252166748
Epoch 870, training loss: 0.06940396875143051 = 0.003870370564982295 + 0.01 * 6.553360462188721
Epoch 870, val loss: 1.0551668405532837
Epoch 880, training loss: 0.06932947039604187 = 0.0037581585347652435 + 0.01 * 6.557131767272949
Epoch 880, val loss: 1.0600297451019287
Epoch 890, training loss: 0.06925015151500702 = 0.003651450388133526 + 0.01 * 6.559870719909668
Epoch 890, val loss: 1.064742088317871
Epoch 900, training loss: 0.06909926235675812 = 0.003550165332853794 + 0.01 * 6.554909706115723
Epoch 900, val loss: 1.0693966150283813
Epoch 910, training loss: 0.06893857568502426 = 0.003453455865383148 + 0.01 * 6.5485124588012695
Epoch 910, val loss: 1.0739829540252686
Epoch 920, training loss: 0.06886865943670273 = 0.003361200448125601 + 0.01 * 6.550745964050293
Epoch 920, val loss: 1.0784966945648193
Epoch 930, training loss: 0.06862194836139679 = 0.003273243783041835 + 0.01 * 6.534870624542236
Epoch 930, val loss: 1.0829154253005981
Epoch 940, training loss: 0.06873615086078644 = 0.0031892862170934677 + 0.01 * 6.554686546325684
Epoch 940, val loss: 1.0872441530227661
Epoch 950, training loss: 0.06855148822069168 = 0.0031089982949197292 + 0.01 * 6.544248580932617
Epoch 950, val loss: 1.091550588607788
Epoch 960, training loss: 0.06840702146291733 = 0.003032371401786804 + 0.01 * 6.5374650955200195
Epoch 960, val loss: 1.0957075357437134
Epoch 970, training loss: 0.06832996010780334 = 0.0029591303318738937 + 0.01 * 6.537083148956299
Epoch 970, val loss: 1.0998562574386597
Epoch 980, training loss: 0.0681615024805069 = 0.002888806164264679 + 0.01 * 6.5272698402404785
Epoch 980, val loss: 1.103898048400879
Epoch 990, training loss: 0.06819925457239151 = 0.0028216836508363485 + 0.01 * 6.537757396697998
Epoch 990, val loss: 1.1079379320144653
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.5129
Flip ASR: 0.4311/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0284180641174316 = 1.9446791410446167 + 0.01 * 8.373882293701172
Epoch 0, val loss: 1.9449717998504639
Epoch 10, training loss: 2.0182597637176514 = 1.9345215559005737 + 0.01 * 8.373809814453125
Epoch 10, val loss: 1.933879017829895
Epoch 20, training loss: 2.005707025527954 = 1.921971321105957 + 0.01 * 8.373568534851074
Epoch 20, val loss: 1.9199042320251465
Epoch 30, training loss: 1.9882049560546875 = 1.9044764041900635 + 0.01 * 8.372859954833984
Epoch 30, val loss: 1.9003018140792847
Epoch 40, training loss: 1.962592601776123 = 1.8788988590240479 + 0.01 * 8.369379043579102
Epoch 40, val loss: 1.8719786405563354
Epoch 50, training loss: 1.9262902736663818 = 1.842832326889038 + 0.01 * 8.345789909362793
Epoch 50, val loss: 1.8334753513336182
Epoch 60, training loss: 1.8819513320922852 = 1.7999281883239746 + 0.01 * 8.202315330505371
Epoch 60, val loss: 1.7916686534881592
Epoch 70, training loss: 1.8357993364334106 = 1.7562453746795654 + 0.01 * 7.955398082733154
Epoch 70, val loss: 1.754368782043457
Epoch 80, training loss: 1.7783128023147583 = 1.7019124031066895 + 0.01 * 7.6400370597839355
Epoch 80, val loss: 1.7095036506652832
Epoch 90, training loss: 1.7014931440353394 = 1.6282864809036255 + 0.01 * 7.320667266845703
Epoch 90, val loss: 1.6469720602035522
Epoch 100, training loss: 1.6063716411590576 = 1.5343348979949951 + 0.01 * 7.203680038452148
Epoch 100, val loss: 1.5669190883636475
Epoch 110, training loss: 1.502152919769287 = 1.4306888580322266 + 0.01 * 7.146408557891846
Epoch 110, val loss: 1.4817594289779663
Epoch 120, training loss: 1.3972928524017334 = 1.3262783288955688 + 0.01 * 7.101447105407715
Epoch 120, val loss: 1.4012383222579956
Epoch 130, training loss: 1.2943251132965088 = 1.2235591411590576 + 0.01 * 7.076591968536377
Epoch 130, val loss: 1.3249260187149048
Epoch 140, training loss: 1.193745493888855 = 1.1231963634490967 + 0.01 * 7.054910659790039
Epoch 140, val loss: 1.251799464225769
Epoch 150, training loss: 1.097118616104126 = 1.0268267393112183 + 0.01 * 7.029189109802246
Epoch 150, val loss: 1.1818567514419556
Epoch 160, training loss: 1.0065213441848755 = 0.9365143179893494 + 0.01 * 7.000698566436768
Epoch 160, val loss: 1.1163727045059204
Epoch 170, training loss: 0.9244904518127441 = 0.8546878695487976 + 0.01 * 6.980257034301758
Epoch 170, val loss: 1.0570948123931885
Epoch 180, training loss: 0.8532769083976746 = 0.7836874127388 + 0.01 * 6.958948135375977
Epoch 180, val loss: 1.0068026781082153
Epoch 190, training loss: 0.7936633229255676 = 0.7242556810379028 + 0.01 * 6.940765380859375
Epoch 190, val loss: 0.9665883183479309
Epoch 200, training loss: 0.7438392639160156 = 0.6745300889015198 + 0.01 * 6.930917263031006
Epoch 200, val loss: 0.9355905055999756
Epoch 210, training loss: 0.7002614140510559 = 0.6311590671539307 + 0.01 * 6.9102349281311035
Epoch 210, val loss: 0.9114505648612976
Epoch 220, training loss: 0.6596193313598633 = 0.5906602144241333 + 0.01 * 6.895913600921631
Epoch 220, val loss: 0.8920342922210693
Epoch 230, training loss: 0.6193199753761292 = 0.5504013299942017 + 0.01 * 6.891864776611328
Epoch 230, val loss: 0.8760126233100891
Epoch 240, training loss: 0.5778417587280273 = 0.5090768337249756 + 0.01 * 6.876491546630859
Epoch 240, val loss: 0.8629613518714905
Epoch 250, training loss: 0.535513162612915 = 0.46683067083358765 + 0.01 * 6.868250846862793
Epoch 250, val loss: 0.8524991869926453
Epoch 260, training loss: 0.49310705065727234 = 0.4245033264160156 + 0.01 * 6.8603715896606445
Epoch 260, val loss: 0.8444605469703674
Epoch 270, training loss: 0.45213085412979126 = 0.3835490345954895 + 0.01 * 6.85818338394165
Epoch 270, val loss: 0.8383952975273132
Epoch 280, training loss: 0.4137077331542969 = 0.34515252709388733 + 0.01 * 6.855521202087402
Epoch 280, val loss: 0.8340824246406555
Epoch 290, training loss: 0.377887487411499 = 0.30939480662345886 + 0.01 * 6.849267482757568
Epoch 290, val loss: 0.8316850662231445
Epoch 300, training loss: 0.3443937301635742 = 0.2758634388446808 + 0.01 * 6.853027820587158
Epoch 300, val loss: 0.8308624029159546
Epoch 310, training loss: 0.3128127455711365 = 0.24435122311115265 + 0.01 * 6.846153736114502
Epoch 310, val loss: 0.8312863707542419
Epoch 320, training loss: 0.2834489047527313 = 0.21504485607147217 + 0.01 * 6.840405464172363
Epoch 320, val loss: 0.8331206440925598
Epoch 330, training loss: 0.2569025754928589 = 0.18849994242191315 + 0.01 * 6.840263366699219
Epoch 330, val loss: 0.8365553021430969
Epoch 340, training loss: 0.233548104763031 = 0.1651732176542282 + 0.01 * 6.837489128112793
Epoch 340, val loss: 0.841943621635437
Epoch 350, training loss: 0.21350762248039246 = 0.14517176151275635 + 0.01 * 6.833586692810059
Epoch 350, val loss: 0.8493101000785828
Epoch 360, training loss: 0.1965378075838089 = 0.12825731933116913 + 0.01 * 6.828049182891846
Epoch 360, val loss: 0.8584464192390442
Epoch 370, training loss: 0.18234682083129883 = 0.11396505683660507 + 0.01 * 6.83817720413208
Epoch 370, val loss: 0.8691794276237488
Epoch 380, training loss: 0.1700735241174698 = 0.10185444355010986 + 0.01 * 6.821908473968506
Epoch 380, val loss: 0.8811201453208923
Epoch 390, training loss: 0.1596783846616745 = 0.09153158962726593 + 0.01 * 6.8146796226501465
Epoch 390, val loss: 0.8940144181251526
Epoch 400, training loss: 0.15078747272491455 = 0.08267304301261902 + 0.01 * 6.811443328857422
Epoch 400, val loss: 0.9076598882675171
Epoch 410, training loss: 0.1434095948934555 = 0.07503704726696014 + 0.01 * 6.837255001068115
Epoch 410, val loss: 0.9219262003898621
Epoch 420, training loss: 0.1364717334508896 = 0.06842292100191116 + 0.01 * 6.804881572723389
Epoch 420, val loss: 0.9365220665931702
Epoch 430, training loss: 0.13061821460723877 = 0.06265608221292496 + 0.01 * 6.7962141036987305
Epoch 430, val loss: 0.9514607787132263
Epoch 440, training loss: 0.12546409666538239 = 0.05760675296187401 + 0.01 * 6.7857346534729
Epoch 440, val loss: 0.9665347337722778
Epoch 450, training loss: 0.12106035649776459 = 0.05316031724214554 + 0.01 * 6.790004253387451
Epoch 450, val loss: 0.9816826581954956
Epoch 460, training loss: 0.11711513251066208 = 0.04922880977392197 + 0.01 * 6.788632392883301
Epoch 460, val loss: 0.996676504611969
Epoch 470, training loss: 0.11345542222261429 = 0.0457380935549736 + 0.01 * 6.771733283996582
Epoch 470, val loss: 1.0115832090377808
Epoch 480, training loss: 0.11023041605949402 = 0.04262113571166992 + 0.01 * 6.760928153991699
Epoch 480, val loss: 1.02610445022583
Epoch 490, training loss: 0.1074269711971283 = 0.03981959819793701 + 0.01 * 6.760737419128418
Epoch 490, val loss: 1.0403845310211182
Epoch 500, training loss: 0.10476823896169662 = 0.03729208558797836 + 0.01 * 6.747615814208984
Epoch 500, val loss: 1.0543904304504395
Epoch 510, training loss: 0.10247677564620972 = 0.03497835248708725 + 0.01 * 6.749842166900635
Epoch 510, val loss: 1.0679881572723389
Epoch 520, training loss: 0.1002005785703659 = 0.03282715007662773 + 0.01 * 6.737342834472656
Epoch 520, val loss: 1.0811982154846191
Epoch 530, training loss: 0.0981045812368393 = 0.03082316182553768 + 0.01 * 6.728141784667969
Epoch 530, val loss: 1.0941635370254517
Epoch 540, training loss: 0.09618958830833435 = 0.02893562987446785 + 0.01 * 6.725396156311035
Epoch 540, val loss: 1.1065926551818848
Epoch 550, training loss: 0.09432075172662735 = 0.02715461701154709 + 0.01 * 6.71661376953125
Epoch 550, val loss: 1.1186437606811523
Epoch 560, training loss: 0.09253766387701035 = 0.025460435077548027 + 0.01 * 6.7077226638793945
Epoch 560, val loss: 1.1304219961166382
Epoch 570, training loss: 0.09080043435096741 = 0.023812251165509224 + 0.01 * 6.698818683624268
Epoch 570, val loss: 1.141886830329895
Epoch 580, training loss: 0.08907759189605713 = 0.022161072120070457 + 0.01 * 6.691652297973633
Epoch 580, val loss: 1.1532058715820312
Epoch 590, training loss: 0.0876910388469696 = 0.02048249915242195 + 0.01 * 6.72085428237915
Epoch 590, val loss: 1.1644731760025024
Epoch 600, training loss: 0.08564108610153198 = 0.01877661794424057 + 0.01 * 6.6864471435546875
Epoch 600, val loss: 1.1753612756729126
Epoch 610, training loss: 0.08421245217323303 = 0.017108473926782608 + 0.01 * 6.7103986740112305
Epoch 610, val loss: 1.1864928007125854
Epoch 620, training loss: 0.08235853165388107 = 0.015513320453464985 + 0.01 * 6.684520721435547
Epoch 620, val loss: 1.1979129314422607
Epoch 630, training loss: 0.08081206679344177 = 0.014061030931770802 + 0.01 * 6.675103187561035
Epoch 630, val loss: 1.208771824836731
Epoch 640, training loss: 0.07952183485031128 = 0.01281662192195654 + 0.01 * 6.670521259307861
Epoch 640, val loss: 1.2197061777114868
Epoch 650, training loss: 0.07842131704092026 = 0.011780195869505405 + 0.01 * 6.664112567901611
Epoch 650, val loss: 1.2305272817611694
Epoch 660, training loss: 0.07749280333518982 = 0.010908206924796104 + 0.01 * 6.658459663391113
Epoch 660, val loss: 1.2410614490509033
Epoch 670, training loss: 0.07682083547115326 = 0.01017345767468214 + 0.01 * 6.664738178253174
Epoch 670, val loss: 1.2510915994644165
Epoch 680, training loss: 0.07594332098960876 = 0.00954405590891838 + 0.01 * 6.639926910400391
Epoch 680, val loss: 1.2610386610031128
Epoch 690, training loss: 0.07566303759813309 = 0.008996881544589996 + 0.01 * 6.666615962982178
Epoch 690, val loss: 1.270395040512085
Epoch 700, training loss: 0.07476499676704407 = 0.008512360043823719 + 0.01 * 6.625263690948486
Epoch 700, val loss: 1.2796533107757568
Epoch 710, training loss: 0.0745396614074707 = 0.008076615631580353 + 0.01 * 6.646304607391357
Epoch 710, val loss: 1.2883682250976562
Epoch 720, training loss: 0.07395412772893906 = 0.007685328368097544 + 0.01 * 6.626880645751953
Epoch 720, val loss: 1.297200083732605
Epoch 730, training loss: 0.07350271940231323 = 0.0073296865448355675 + 0.01 * 6.617303371429443
Epoch 730, val loss: 1.3055179119110107
Epoch 740, training loss: 0.0732213631272316 = 0.007004559971392155 + 0.01 * 6.62168025970459
Epoch 740, val loss: 1.3138147592544556
Epoch 750, training loss: 0.07285004109144211 = 0.006704644300043583 + 0.01 * 6.614540100097656
Epoch 750, val loss: 1.321565866470337
Epoch 760, training loss: 0.07261978834867477 = 0.006428577937185764 + 0.01 * 6.619121551513672
Epoch 760, val loss: 1.329436182975769
Epoch 770, training loss: 0.07224168628454208 = 0.0061732749454677105 + 0.01 * 6.606841087341309
Epoch 770, val loss: 1.3369512557983398
Epoch 780, training loss: 0.07198625057935715 = 0.005937253125011921 + 0.01 * 6.604899883270264
Epoch 780, val loss: 1.3442940711975098
Epoch 790, training loss: 0.07162874191999435 = 0.005717217922210693 + 0.01 * 6.591152191162109
Epoch 790, val loss: 1.3513396978378296
Epoch 800, training loss: 0.07164571434259415 = 0.005511486902832985 + 0.01 * 6.6134233474731445
Epoch 800, val loss: 1.3582706451416016
Epoch 810, training loss: 0.07119031250476837 = 0.005318955983966589 + 0.01 * 6.5871357917785645
Epoch 810, val loss: 1.3651009798049927
Epoch 820, training loss: 0.071394182741642 = 0.00513802794739604 + 0.01 * 6.62561559677124
Epoch 820, val loss: 1.3717682361602783
Epoch 830, training loss: 0.07076261937618256 = 0.004968149121850729 + 0.01 * 6.579446792602539
Epoch 830, val loss: 1.3782175779342651
Epoch 840, training loss: 0.07077866047620773 = 0.0048082503490149975 + 0.01 * 6.597041130065918
Epoch 840, val loss: 1.384438395500183
Epoch 850, training loss: 0.07037712633609772 = 0.004657795187085867 + 0.01 * 6.571932792663574
Epoch 850, val loss: 1.3905112743377686
Epoch 860, training loss: 0.07014496624469757 = 0.004515776876360178 + 0.01 * 6.5629191398620605
Epoch 860, val loss: 1.3965328931808472
Epoch 870, training loss: 0.07015156745910645 = 0.004381257575005293 + 0.01 * 6.57703161239624
Epoch 870, val loss: 1.402417540550232
Epoch 880, training loss: 0.06984690576791763 = 0.004253971856087446 + 0.01 * 6.5592942237854
Epoch 880, val loss: 1.408133625984192
Epoch 890, training loss: 0.0696415975689888 = 0.004133305046707392 + 0.01 * 6.55082893371582
Epoch 890, val loss: 1.4137431383132935
Epoch 900, training loss: 0.07003370672464371 = 0.0040188501589000225 + 0.01 * 6.601485729217529
Epoch 900, val loss: 1.419175148010254
Epoch 910, training loss: 0.06944259256124496 = 0.003910491243004799 + 0.01 * 6.553210735321045
Epoch 910, val loss: 1.424499273300171
Epoch 920, training loss: 0.06935376673936844 = 0.0038074164185673 + 0.01 * 6.554635047912598
Epoch 920, val loss: 1.4296865463256836
Epoch 930, training loss: 0.0691675916314125 = 0.0037093611899763346 + 0.01 * 6.545823097229004
Epoch 930, val loss: 1.4348458051681519
Epoch 940, training loss: 0.06924848258495331 = 0.0036156608257442713 + 0.01 * 6.563282489776611
Epoch 940, val loss: 1.4398646354675293
Epoch 950, training loss: 0.0689917653799057 = 0.0035263074096292257 + 0.01 * 6.54654598236084
Epoch 950, val loss: 1.4447450637817383
Epoch 960, training loss: 0.06911168247461319 = 0.00344102387316525 + 0.01 * 6.567066669464111
Epoch 960, val loss: 1.4495288133621216
Epoch 970, training loss: 0.06906790286302567 = 0.0033595438580960035 + 0.01 * 6.570836544036865
Epoch 970, val loss: 1.454187273979187
Epoch 980, training loss: 0.06882727891206741 = 0.0032817863393574953 + 0.01 * 6.554549217224121
Epoch 980, val loss: 1.4586647748947144
Epoch 990, training loss: 0.0685596764087677 = 0.0032075117342174053 + 0.01 * 6.535216331481934
Epoch 990, val loss: 1.4632166624069214
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7704
Overall ASR: 0.8782
Flip ASR: 0.8578/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0337412357330322 = 1.9500025510787964 + 0.01 * 8.37386703491211
Epoch 0, val loss: 1.9501310586929321
Epoch 10, training loss: 2.0229125022888184 = 1.9391745328903198 + 0.01 * 8.373807907104492
Epoch 10, val loss: 1.9389535188674927
Epoch 20, training loss: 2.0096380710601807 = 1.925902247428894 + 0.01 * 8.373577117919922
Epoch 20, val loss: 1.9250808954238892
Epoch 30, training loss: 1.9908567667007446 = 1.9071286916732788 + 0.01 * 8.37281322479248
Epoch 30, val loss: 1.9055354595184326
Epoch 40, training loss: 1.9628090858459473 = 1.879124402999878 + 0.01 * 8.368464469909668
Epoch 40, val loss: 1.8769080638885498
Epoch 50, training loss: 1.9232429265975952 = 1.839841365814209 + 0.01 * 8.340157508850098
Epoch 50, val loss: 1.838457465171814
Epoch 60, training loss: 1.8769209384918213 = 1.7946668863296509 + 0.01 * 8.2254056930542
Epoch 60, val loss: 1.7980270385742188
Epoch 70, training loss: 1.832529067993164 = 1.7525005340576172 + 0.01 * 8.002851486206055
Epoch 70, val loss: 1.762891411781311
Epoch 80, training loss: 1.778910756111145 = 1.7000586986541748 + 0.01 * 7.8852081298828125
Epoch 80, val loss: 1.716959834098816
Epoch 90, training loss: 1.7052167654037476 = 1.628089427947998 + 0.01 * 7.712728977203369
Epoch 90, val loss: 1.6555010080337524
Epoch 100, training loss: 1.6117455959320068 = 1.5369161367416382 + 0.01 * 7.482947826385498
Epoch 100, val loss: 1.5818085670471191
Epoch 110, training loss: 1.5108404159545898 = 1.4379675388336182 + 0.01 * 7.287287712097168
Epoch 110, val loss: 1.5052870512008667
Epoch 120, training loss: 1.415160894393921 = 1.342515230178833 + 0.01 * 7.264566898345947
Epoch 120, val loss: 1.433834433555603
Epoch 130, training loss: 1.3248777389526367 = 1.252523422241211 + 0.01 * 7.235434532165527
Epoch 130, val loss: 1.3704091310501099
Epoch 140, training loss: 1.2378337383270264 = 1.1655864715576172 + 0.01 * 7.224725246429443
Epoch 140, val loss: 1.310123085975647
Epoch 150, training loss: 1.152177333831787 = 1.080047369003296 + 0.01 * 7.2129926681518555
Epoch 150, val loss: 1.2489802837371826
Epoch 160, training loss: 1.0682265758514404 = 0.9962653517723083 + 0.01 * 7.196126937866211
Epoch 160, val loss: 1.1870951652526855
Epoch 170, training loss: 0.9868059158325195 = 0.9150667190551758 + 0.01 * 7.173918724060059
Epoch 170, val loss: 1.1259684562683105
Epoch 180, training loss: 0.9082444906234741 = 0.8367428183555603 + 0.01 * 7.150167942047119
Epoch 180, val loss: 1.0668293237686157
Epoch 190, training loss: 0.832962691783905 = 0.7617043852806091 + 0.01 * 7.125829219818115
Epoch 190, val loss: 1.0106970071792603
Epoch 200, training loss: 0.7620754837989807 = 0.6910677552223206 + 0.01 * 7.100770473480225
Epoch 200, val loss: 0.9587895274162292
Epoch 210, training loss: 0.6964164972305298 = 0.6256617903709412 + 0.01 * 7.075469017028809
Epoch 210, val loss: 0.9120804071426392
Epoch 220, training loss: 0.6357517838478088 = 0.5652139186859131 + 0.01 * 7.053786754608154
Epoch 220, val loss: 0.87111496925354
Epoch 230, training loss: 0.5791311264038086 = 0.5088087916374207 + 0.01 * 7.032236576080322
Epoch 230, val loss: 0.8355327248573303
Epoch 240, training loss: 0.5258836150169373 = 0.45575961470603943 + 0.01 * 7.012400150299072
Epoch 240, val loss: 0.8051250576972961
Epoch 250, training loss: 0.47587525844573975 = 0.40590766072273254 + 0.01 * 6.996759414672852
Epoch 250, val loss: 0.7801758646965027
Epoch 260, training loss: 0.4292225241661072 = 0.35939839482307434 + 0.01 * 6.982414722442627
Epoch 260, val loss: 0.7603806853294373
Epoch 270, training loss: 0.3861199915409088 = 0.3164764940738678 + 0.01 * 6.964348793029785
Epoch 270, val loss: 0.744979739189148
Epoch 280, training loss: 0.3468315303325653 = 0.2773785889148712 + 0.01 * 6.945294380187988
Epoch 280, val loss: 0.7336767911911011
Epoch 290, training loss: 0.31163352727890015 = 0.2422388643026352 + 0.01 * 6.939465045928955
Epoch 290, val loss: 0.7264897227287292
Epoch 300, training loss: 0.2803725302219391 = 0.21113961935043335 + 0.01 * 6.923290729522705
Epoch 300, val loss: 0.7234710454940796
Epoch 310, training loss: 0.25308939814567566 = 0.1840049922466278 + 0.01 * 6.908441066741943
Epoch 310, val loss: 0.7245793342590332
Epoch 320, training loss: 0.2295840084552765 = 0.16058184206485748 + 0.01 * 6.900216579437256
Epoch 320, val loss: 0.7294440865516663
Epoch 330, training loss: 0.20940136909484863 = 0.14048470556735992 + 0.01 * 6.891665935516357
Epoch 330, val loss: 0.7374076843261719
Epoch 340, training loss: 0.19215402007102966 = 0.12328440696001053 + 0.01 * 6.886961936950684
Epoch 340, val loss: 0.7480410933494568
Epoch 350, training loss: 0.1773301512002945 = 0.10855507105588913 + 0.01 * 6.877508163452148
Epoch 350, val loss: 0.7607313990592957
Epoch 360, training loss: 0.1646212488412857 = 0.09590590745210648 + 0.01 * 6.87153434753418
Epoch 360, val loss: 0.775088369846344
Epoch 370, training loss: 0.15368790924549103 = 0.08499978482723236 + 0.01 * 6.868812561035156
Epoch 370, val loss: 0.7904810309410095
Epoch 380, training loss: 0.1441078931093216 = 0.07554619759321213 + 0.01 * 6.856169700622559
Epoch 380, val loss: 0.8067471981048584
Epoch 390, training loss: 0.13583742082118988 = 0.06731180101633072 + 0.01 * 6.852561950683594
Epoch 390, val loss: 0.823588490486145
Epoch 400, training loss: 0.12861937284469604 = 0.06011582165956497 + 0.01 * 6.850354194641113
Epoch 400, val loss: 0.8407382965087891
Epoch 410, training loss: 0.12221154570579529 = 0.05382170528173447 + 0.01 * 6.838984489440918
Epoch 410, val loss: 0.8580948114395142
Epoch 420, training loss: 0.11667726188898087 = 0.04831361025571823 + 0.01 * 6.836365222930908
Epoch 420, val loss: 0.8754962086677551
Epoch 430, training loss: 0.11183512955904007 = 0.04350002110004425 + 0.01 * 6.8335113525390625
Epoch 430, val loss: 0.892753005027771
Epoch 440, training loss: 0.10755376517772675 = 0.039296891540288925 + 0.01 * 6.825687885284424
Epoch 440, val loss: 0.9097814559936523
Epoch 450, training loss: 0.10379765927791595 = 0.035622235387563705 + 0.01 * 6.817543029785156
Epoch 450, val loss: 0.9263758063316345
Epoch 460, training loss: 0.10054916888475418 = 0.03240381181240082 + 0.01 * 6.814536094665527
Epoch 460, val loss: 0.9425504803657532
Epoch 470, training loss: 0.09776493906974792 = 0.0295838862657547 + 0.01 * 6.818105697631836
Epoch 470, val loss: 0.9581106901168823
Epoch 480, training loss: 0.09516936540603638 = 0.027105923742055893 + 0.01 * 6.806344985961914
Epoch 480, val loss: 0.9731860160827637
Epoch 490, training loss: 0.09287227690219879 = 0.024916108697652817 + 0.01 * 6.795617580413818
Epoch 490, val loss: 0.9877499938011169
Epoch 500, training loss: 0.09085732698440552 = 0.022973479703068733 + 0.01 * 6.788385391235352
Epoch 500, val loss: 1.0018547773361206
Epoch 510, training loss: 0.08951865136623383 = 0.021245066076517105 + 0.01 * 6.827358722686768
Epoch 510, val loss: 1.0155302286148071
Epoch 520, training loss: 0.08750627934932709 = 0.019705193117260933 + 0.01 * 6.78010892868042
Epoch 520, val loss: 1.0286517143249512
Epoch 530, training loss: 0.08607695251703262 = 0.01832597143948078 + 0.01 * 6.7750983238220215
Epoch 530, val loss: 1.0414146184921265
Epoch 540, training loss: 0.08482924848794937 = 0.017084555700421333 + 0.01 * 6.774469375610352
Epoch 540, val loss: 1.053794026374817
Epoch 550, training loss: 0.08357223123311996 = 0.015965210273861885 + 0.01 * 6.760702610015869
Epoch 550, val loss: 1.0657378435134888
Epoch 560, training loss: 0.0825251117348671 = 0.014953539706766605 + 0.01 * 6.757157802581787
Epoch 560, val loss: 1.0773757696151733
Epoch 570, training loss: 0.08149897307157516 = 0.014035853557288647 + 0.01 * 6.746312618255615
Epoch 570, val loss: 1.088720440864563
Epoch 580, training loss: 0.08074720203876495 = 0.013200391083955765 + 0.01 * 6.75468111038208
Epoch 580, val loss: 1.0996993780136108
Epoch 590, training loss: 0.07986314594745636 = 0.012440393678843975 + 0.01 * 6.742275714874268
Epoch 590, val loss: 1.110404372215271
Epoch 600, training loss: 0.0790587067604065 = 0.011747945100069046 + 0.01 * 6.731075763702393
Epoch 600, val loss: 1.1207255125045776
Epoch 610, training loss: 0.07835976779460907 = 0.011113026179373264 + 0.01 * 6.724674701690674
Epoch 610, val loss: 1.1307646036148071
Epoch 620, training loss: 0.07797497510910034 = 0.01052882894873619 + 0.01 * 6.744614601135254
Epoch 620, val loss: 1.140478491783142
Epoch 630, training loss: 0.07722389698028564 = 0.0099928118288517 + 0.01 * 6.723108291625977
Epoch 630, val loss: 1.1500623226165771
Epoch 640, training loss: 0.07663824409246445 = 0.00949801504611969 + 0.01 * 6.714023113250732
Epoch 640, val loss: 1.1592621803283691
Epoch 650, training loss: 0.07613121718168259 = 0.00903947651386261 + 0.01 * 6.709174156188965
Epoch 650, val loss: 1.1683175563812256
Epoch 660, training loss: 0.07568088173866272 = 0.008615711703896523 + 0.01 * 6.706517219543457
Epoch 660, val loss: 1.1772146224975586
Epoch 670, training loss: 0.07521003484725952 = 0.008224471472203732 + 0.01 * 6.698556423187256
Epoch 670, val loss: 1.1856821775436401
Epoch 680, training loss: 0.07473477721214294 = 0.007861544378101826 + 0.01 * 6.687323093414307
Epoch 680, val loss: 1.194031000137329
Epoch 690, training loss: 0.07434561848640442 = 0.007523758336901665 + 0.01 * 6.682185649871826
Epoch 690, val loss: 1.2021633386611938
Epoch 700, training loss: 0.0740937888622284 = 0.007209436967968941 + 0.01 * 6.6884355545043945
Epoch 700, val loss: 1.2100763320922852
Epoch 710, training loss: 0.07356377691030502 = 0.006916305050253868 + 0.01 * 6.66474723815918
Epoch 710, val loss: 1.2178034782409668
Epoch 720, training loss: 0.07326728105545044 = 0.006642322521656752 + 0.01 * 6.662496089935303
Epoch 720, val loss: 1.2252253293991089
Epoch 730, training loss: 0.07291361689567566 = 0.006386390887200832 + 0.01 * 6.652722358703613
Epoch 730, val loss: 1.2326006889343262
Epoch 740, training loss: 0.07273495197296143 = 0.006146775558590889 + 0.01 * 6.658817768096924
Epoch 740, val loss: 1.2396430969238281
Epoch 750, training loss: 0.0723300352692604 = 0.005922325886785984 + 0.01 * 6.640771389007568
Epoch 750, val loss: 1.2466148138046265
Epoch 760, training loss: 0.07216454297304153 = 0.005711281206458807 + 0.01 * 6.645326137542725
Epoch 760, val loss: 1.2534170150756836
Epoch 770, training loss: 0.07182418555021286 = 0.005512484349310398 + 0.01 * 6.631170272827148
Epoch 770, val loss: 1.2599530220031738
Epoch 780, training loss: 0.07158704102039337 = 0.005325205624103546 + 0.01 * 6.62618350982666
Epoch 780, val loss: 1.2664793729782104
Epoch 790, training loss: 0.07154446095228195 = 0.005148566327989101 + 0.01 * 6.639589309692383
Epoch 790, val loss: 1.2727571725845337
Epoch 800, training loss: 0.07123903185129166 = 0.004982170648872852 + 0.01 * 6.625685691833496
Epoch 800, val loss: 1.278943657875061
Epoch 810, training loss: 0.07102755457162857 = 0.004824922885745764 + 0.01 * 6.620263576507568
Epoch 810, val loss: 1.284942388534546
Epoch 820, training loss: 0.07089051604270935 = 0.0046761841513216496 + 0.01 * 6.621433258056641
Epoch 820, val loss: 1.2908594608306885
Epoch 830, training loss: 0.0704738050699234 = 0.004535061772912741 + 0.01 * 6.593873977661133
Epoch 830, val loss: 1.2965930700302124
Epoch 840, training loss: 0.07052282989025116 = 0.00440108310431242 + 0.01 * 6.612175464630127
Epoch 840, val loss: 1.3022750616073608
Epoch 850, training loss: 0.07022427767515182 = 0.004274062346667051 + 0.01 * 6.595021724700928
Epoch 850, val loss: 1.307766318321228
Epoch 860, training loss: 0.07000883668661118 = 0.0041534085758030415 + 0.01 * 6.585543155670166
Epoch 860, val loss: 1.313119649887085
Epoch 870, training loss: 0.06986183673143387 = 0.004038812592625618 + 0.01 * 6.582302570343018
Epoch 870, val loss: 1.3183974027633667
Epoch 880, training loss: 0.06974907964468002 = 0.003929671365767717 + 0.01 * 6.581940650939941
Epoch 880, val loss: 1.323531985282898
Epoch 890, training loss: 0.06972246617078781 = 0.003825665917247534 + 0.01 * 6.5896806716918945
Epoch 890, val loss: 1.3286410570144653
Epoch 900, training loss: 0.06949953734874725 = 0.003726392053067684 + 0.01 * 6.577314376831055
Epoch 900, val loss: 1.3335187435150146
Epoch 910, training loss: 0.06940936297178268 = 0.0036317643243819475 + 0.01 * 6.577760219573975
Epoch 910, val loss: 1.3383551836013794
Epoch 920, training loss: 0.06925762444734573 = 0.0035414984449744225 + 0.01 * 6.57161283493042
Epoch 920, val loss: 1.3431434631347656
Epoch 930, training loss: 0.06904830038547516 = 0.0034552263095974922 + 0.01 * 6.559307098388672
Epoch 930, val loss: 1.3477630615234375
Epoch 940, training loss: 0.06899484992027283 = 0.0033725909888744354 + 0.01 * 6.562225818634033
Epoch 940, val loss: 1.3523333072662354
Epoch 950, training loss: 0.06895426660776138 = 0.0032935747876763344 + 0.01 * 6.566069602966309
Epoch 950, val loss: 1.356735110282898
Epoch 960, training loss: 0.06880640238523483 = 0.0032181863207370043 + 0.01 * 6.558821678161621
Epoch 960, val loss: 1.3612014055252075
Epoch 970, training loss: 0.06881026178598404 = 0.00314593268558383 + 0.01 * 6.566432952880859
Epoch 970, val loss: 1.3654066324234009
Epoch 980, training loss: 0.06854243576526642 = 0.003076528199017048 + 0.01 * 6.546590328216553
Epoch 980, val loss: 1.3696095943450928
Epoch 990, training loss: 0.06840500980615616 = 0.003009889740496874 + 0.01 * 6.5395121574401855
Epoch 990, val loss: 1.3737026453018188
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.7343
Flip ASR: 0.6933/225 nodes
The final ASR:0.70849, 0.15025, Accuracy:0.80741, 0.02772
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11546])
remove edge: torch.Size([2, 9518])
updated graph: torch.Size([2, 10508])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8481
Overall ASR: 0.9963
Flip ASR: 0.9956/225 nodes
The final ASR:0.98647, 0.00920, Accuracy:0.83333, 0.01090
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.4 der2:0.4 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.025395393371582 = 1.9416570663452148 + 0.01 * 8.3738431930542
Epoch 0, val loss: 1.9454166889190674
Epoch 10, training loss: 2.0151543617248535 = 1.9314171075820923 + 0.01 * 8.373732566833496
Epoch 10, val loss: 1.934728980064392
Epoch 20, training loss: 2.002392530441284 = 1.9186577796936035 + 0.01 * 8.373467445373535
Epoch 20, val loss: 1.9210500717163086
Epoch 30, training loss: 1.9845510721206665 = 1.9008229970932007 + 0.01 * 8.372809410095215
Epoch 30, val loss: 1.9018009901046753
Epoch 40, training loss: 1.9585667848587036 = 1.8748632669448853 + 0.01 * 8.370353698730469
Epoch 40, val loss: 1.8741364479064941
Epoch 50, training loss: 1.922868013381958 = 1.8393020629882812 + 0.01 * 8.356589317321777
Epoch 50, val loss: 1.8380922079086304
Epoch 60, training loss: 1.8821109533309937 = 1.7993406057357788 + 0.01 * 8.277037620544434
Epoch 60, val loss: 1.8020919561386108
Epoch 70, training loss: 1.841774582862854 = 1.7623728513717651 + 0.01 * 7.940178394317627
Epoch 70, val loss: 1.7733780145645142
Epoch 80, training loss: 1.7911148071289062 = 1.716369390487671 + 0.01 * 7.474545955657959
Epoch 80, val loss: 1.7364879846572876
Epoch 90, training loss: 1.7252659797668457 = 1.6529196500778198 + 0.01 * 7.234638690948486
Epoch 90, val loss: 1.6833360195159912
Epoch 100, training loss: 1.6412476301193237 = 1.56989324092865 + 0.01 * 7.135442733764648
Epoch 100, val loss: 1.6136677265167236
Epoch 110, training loss: 1.5423035621643066 = 1.4717673063278198 + 0.01 * 7.053626537322998
Epoch 110, val loss: 1.533927321434021
Epoch 120, training loss: 1.435428500175476 = 1.3656266927719116 + 0.01 * 6.980180263519287
Epoch 120, val loss: 1.4483827352523804
Epoch 130, training loss: 1.3252266645431519 = 1.255874514579773 + 0.01 * 6.935212135314941
Epoch 130, val loss: 1.3618104457855225
Epoch 140, training loss: 1.2146912813186646 = 1.1456106901168823 + 0.01 * 6.908062934875488
Epoch 140, val loss: 1.27579927444458
Epoch 150, training loss: 1.1079250574111938 = 1.0390139818191528 + 0.01 * 6.8911051750183105
Epoch 150, val loss: 1.1937847137451172
Epoch 160, training loss: 1.0087066888809204 = 0.9399130344390869 + 0.01 * 6.879360198974609
Epoch 160, val loss: 1.1186949014663696
Epoch 170, training loss: 0.9187796115875244 = 0.8500567078590393 + 0.01 * 6.872292995452881
Epoch 170, val loss: 1.0515722036361694
Epoch 180, training loss: 0.8382793664932251 = 0.7696520090103149 + 0.01 * 6.862735748291016
Epoch 180, val loss: 0.9927606582641602
Epoch 190, training loss: 0.7666239738464355 = 0.698076605796814 + 0.01 * 6.854734897613525
Epoch 190, val loss: 0.9425594210624695
Epoch 200, training loss: 0.7022847533226013 = 0.6337805390357971 + 0.01 * 6.850423812866211
Epoch 200, val loss: 0.9008456468582153
Epoch 210, training loss: 0.6430713534355164 = 0.5746663808822632 + 0.01 * 6.840496063232422
Epoch 210, val loss: 0.8662546873092651
Epoch 220, training loss: 0.586947500705719 = 0.5186141729354858 + 0.01 * 6.8333306312561035
Epoch 220, val loss: 0.8369035720825195
Epoch 230, training loss: 0.5323657989501953 = 0.4640978276729584 + 0.01 * 6.826797008514404
Epoch 230, val loss: 0.8113119602203369
Epoch 240, training loss: 0.4786874055862427 = 0.41047966480255127 + 0.01 * 6.820773124694824
Epoch 240, val loss: 0.7878514528274536
Epoch 250, training loss: 0.42667728662490845 = 0.3585215210914612 + 0.01 * 6.815576076507568
Epoch 250, val loss: 0.766997754573822
Epoch 260, training loss: 0.37812384963035583 = 0.3100447356700897 + 0.01 * 6.807910919189453
Epoch 260, val loss: 0.7505796551704407
Epoch 270, training loss: 0.3345911204814911 = 0.26658567786216736 + 0.01 * 6.800543308258057
Epoch 270, val loss: 0.7395644187927246
Epoch 280, training loss: 0.29697147011756897 = 0.22877340018749237 + 0.01 * 6.819807529449463
Epoch 280, val loss: 0.733877956867218
Epoch 290, training loss: 0.26449328660964966 = 0.19648919999599457 + 0.01 * 6.800407886505127
Epoch 290, val loss: 0.7329728007316589
Epoch 300, training loss: 0.23702609539031982 = 0.169138565659523 + 0.01 * 6.788753032684326
Epoch 300, val loss: 0.7363266944885254
Epoch 310, training loss: 0.21388757228851318 = 0.14603623747825623 + 0.01 * 6.785134315490723
Epoch 310, val loss: 0.7431467771530151
Epoch 320, training loss: 0.19434192776679993 = 0.12653838098049164 + 0.01 * 6.780354022979736
Epoch 320, val loss: 0.752746045589447
Epoch 330, training loss: 0.17784635722637177 = 0.1100795790553093 + 0.01 * 6.776678085327148
Epoch 330, val loss: 0.7645378112792969
Epoch 340, training loss: 0.16388772428035736 = 0.09616070240736008 + 0.01 * 6.772702217102051
Epoch 340, val loss: 0.7778772711753845
Epoch 350, training loss: 0.15213558077812195 = 0.0843634381890297 + 0.01 * 6.777215003967285
Epoch 350, val loss: 0.7922699451446533
Epoch 360, training loss: 0.14204923808574677 = 0.07433947175741196 + 0.01 * 6.770977020263672
Epoch 360, val loss: 0.8074524402618408
Epoch 370, training loss: 0.13342159986495972 = 0.06577862799167633 + 0.01 * 6.764297008514404
Epoch 370, val loss: 0.823121190071106
Epoch 380, training loss: 0.12601575255393982 = 0.05843409523367882 + 0.01 * 6.7581658363342285
Epoch 380, val loss: 0.8391122221946716
Epoch 390, training loss: 0.1196635365486145 = 0.05210623890161514 + 0.01 * 6.755730152130127
Epoch 390, val loss: 0.855200469493866
Epoch 400, training loss: 0.11413528025150299 = 0.04663645848631859 + 0.01 * 6.749882698059082
Epoch 400, val loss: 0.8714583516120911
Epoch 410, training loss: 0.10934881120920181 = 0.0418921560049057 + 0.01 * 6.745665550231934
Epoch 410, val loss: 0.887650191783905
Epoch 420, training loss: 0.10516410320997238 = 0.03776353597640991 + 0.01 * 6.74005651473999
Epoch 420, val loss: 0.9037404656410217
Epoch 430, training loss: 0.10161612182855606 = 0.03415106236934662 + 0.01 * 6.7465057373046875
Epoch 430, val loss: 0.919708788394928
Epoch 440, training loss: 0.09830286353826523 = 0.031000712886452675 + 0.01 * 6.730215549468994
Epoch 440, val loss: 0.9352447390556335
Epoch 450, training loss: 0.09549842774868011 = 0.028241774067282677 + 0.01 * 6.72566556930542
Epoch 450, val loss: 0.9503992199897766
Epoch 460, training loss: 0.09321026504039764 = 0.025817370042204857 + 0.01 * 6.739289283752441
Epoch 460, val loss: 0.9651039242744446
Epoch 470, training loss: 0.09085245430469513 = 0.02368156425654888 + 0.01 * 6.71708869934082
Epoch 470, val loss: 0.9793669581413269
Epoch 480, training loss: 0.08891522884368896 = 0.02179245464503765 + 0.01 * 6.712277412414551
Epoch 480, val loss: 0.9931584000587463
Epoch 490, training loss: 0.08718887716531754 = 0.020115656778216362 + 0.01 * 6.707322120666504
Epoch 490, val loss: 1.0064749717712402
Epoch 500, training loss: 0.0857205018401146 = 0.018622076138854027 + 0.01 * 6.709842205047607
Epoch 500, val loss: 1.0193324089050293
Epoch 510, training loss: 0.08425703644752502 = 0.017289211973547935 + 0.01 * 6.69678258895874
Epoch 510, val loss: 1.0317388772964478
Epoch 520, training loss: 0.08302096277475357 = 0.016095248982310295 + 0.01 * 6.69257116317749
Epoch 520, val loss: 1.0437349081039429
Epoch 530, training loss: 0.08193302899599075 = 0.01502176932990551 + 0.01 * 6.691125869750977
Epoch 530, val loss: 1.0553616285324097
Epoch 540, training loss: 0.080988809466362 = 0.014053857885301113 + 0.01 * 6.693495273590088
Epoch 540, val loss: 1.066509485244751
Epoch 550, training loss: 0.08002172410488129 = 0.013180078938603401 + 0.01 * 6.684164524078369
Epoch 550, val loss: 1.0773813724517822
Epoch 560, training loss: 0.07918601483106613 = 0.012387649156153202 + 0.01 * 6.679836750030518
Epoch 560, val loss: 1.0878390073776245
Epoch 570, training loss: 0.07841440290212631 = 0.011666658334434032 + 0.01 * 6.674775123596191
Epoch 570, val loss: 1.0980262756347656
Epoch 580, training loss: 0.07770563662052155 = 0.011009018868207932 + 0.01 * 6.6696624755859375
Epoch 580, val loss: 1.1078108549118042
Epoch 590, training loss: 0.07706547528505325 = 0.010408187285065651 + 0.01 * 6.66572904586792
Epoch 590, val loss: 1.1173598766326904
Epoch 600, training loss: 0.07649724930524826 = 0.009857416152954102 + 0.01 * 6.6639838218688965
Epoch 600, val loss: 1.1266319751739502
Epoch 610, training loss: 0.07603234052658081 = 0.00935134943574667 + 0.01 * 6.668099403381348
Epoch 610, val loss: 1.1356364488601685
Epoch 620, training loss: 0.0754864364862442 = 0.008885789662599564 + 0.01 * 6.660065174102783
Epoch 620, val loss: 1.1443675756454468
Epoch 630, training loss: 0.07506168633699417 = 0.008456445299088955 + 0.01 * 6.660523891448975
Epoch 630, val loss: 1.1528233289718628
Epoch 640, training loss: 0.0745483934879303 = 0.008060094900429249 + 0.01 * 6.648829460144043
Epoch 640, val loss: 1.1611167192459106
Epoch 650, training loss: 0.07416746765375137 = 0.00769276125356555 + 0.01 * 6.647470951080322
Epoch 650, val loss: 1.1691330671310425
Epoch 660, training loss: 0.07377522438764572 = 0.007351761683821678 + 0.01 * 6.642346382141113
Epoch 660, val loss: 1.1769428253173828
Epoch 670, training loss: 0.07345951348543167 = 0.007034535054117441 + 0.01 * 6.642498016357422
Epoch 670, val loss: 1.1845461130142212
Epoch 680, training loss: 0.07312680780887604 = 0.006738802418112755 + 0.01 * 6.638800621032715
Epoch 680, val loss: 1.191925287246704
Epoch 690, training loss: 0.07285920530557632 = 0.006462583784013987 + 0.01 * 6.639662265777588
Epoch 690, val loss: 1.1991891860961914
Epoch 700, training loss: 0.07260558009147644 = 0.006203848402947187 + 0.01 * 6.640173435211182
Epoch 700, val loss: 1.2062289714813232
Epoch 710, training loss: 0.07230838388204575 = 0.005961133167147636 + 0.01 * 6.634725093841553
Epoch 710, val loss: 1.2131308317184448
Epoch 720, training loss: 0.07212278991937637 = 0.005733058787882328 + 0.01 * 6.638973236083984
Epoch 720, val loss: 1.2199193239212036
Epoch 730, training loss: 0.07176323235034943 = 0.005518470425158739 + 0.01 * 6.624475955963135
Epoch 730, val loss: 1.2265135049819946
Epoch 740, training loss: 0.07151531428098679 = 0.005312951747328043 + 0.01 * 6.620236396789551
Epoch 740, val loss: 1.2332297563552856
Epoch 750, training loss: 0.07133350521326065 = 0.005120082292705774 + 0.01 * 6.621342658996582
Epoch 750, val loss: 1.2397488355636597
Epoch 760, training loss: 0.07114889472723007 = 0.004938111174851656 + 0.01 * 6.6210784912109375
Epoch 760, val loss: 1.2459888458251953
Epoch 770, training loss: 0.07088414579629898 = 0.004765972029417753 + 0.01 * 6.611817359924316
Epoch 770, val loss: 1.252233862876892
Epoch 780, training loss: 0.07070448249578476 = 0.004602935630828142 + 0.01 * 6.610154628753662
Epoch 780, val loss: 1.2583038806915283
Epoch 790, training loss: 0.07054128497838974 = 0.004448726773262024 + 0.01 * 6.609255790710449
Epoch 790, val loss: 1.2643225193023682
Epoch 800, training loss: 0.07045050710439682 = 0.004302690736949444 + 0.01 * 6.614781856536865
Epoch 800, val loss: 1.2701760530471802
Epoch 810, training loss: 0.0701829269528389 = 0.004164065234363079 + 0.01 * 6.601886749267578
Epoch 810, val loss: 1.2758822441101074
Epoch 820, training loss: 0.07005040347576141 = 0.004032548982650042 + 0.01 * 6.601785182952881
Epoch 820, val loss: 1.2815698385238647
Epoch 830, training loss: 0.06990315020084381 = 0.003907547798007727 + 0.01 * 6.599560737609863
Epoch 830, val loss: 1.2870854139328003
Epoch 840, training loss: 0.06971926242113113 = 0.003789140610024333 + 0.01 * 6.593011856079102
Epoch 840, val loss: 1.2925279140472412
Epoch 850, training loss: 0.06976881623268127 = 0.003676752792671323 + 0.01 * 6.6092071533203125
Epoch 850, val loss: 1.2976733446121216
Epoch 860, training loss: 0.06947943568229675 = 0.003570286091417074 + 0.01 * 6.590914726257324
Epoch 860, val loss: 1.3027946949005127
Epoch 870, training loss: 0.06932269036769867 = 0.003468874143436551 + 0.01 * 6.585381984710693
Epoch 870, val loss: 1.3078267574310303
Epoch 880, training loss: 0.06930521875619888 = 0.003372362582013011 + 0.01 * 6.59328556060791
Epoch 880, val loss: 1.3127954006195068
Epoch 890, training loss: 0.06914118677377701 = 0.003280365141108632 + 0.01 * 6.586082458496094
Epoch 890, val loss: 1.31754469871521
Epoch 900, training loss: 0.06895270198583603 = 0.0031926908995956182 + 0.01 * 6.576001167297363
Epoch 900, val loss: 1.3223376274108887
Epoch 910, training loss: 0.06888829916715622 = 0.0031091144774109125 + 0.01 * 6.577918529510498
Epoch 910, val loss: 1.3269543647766113
Epoch 920, training loss: 0.06876739114522934 = 0.0030293159652501345 + 0.01 * 6.573807716369629
Epoch 920, val loss: 1.3314745426177979
Epoch 930, training loss: 0.06880733370780945 = 0.0029531933832913637 + 0.01 * 6.585414409637451
Epoch 930, val loss: 1.336014986038208
Epoch 940, training loss: 0.06865311414003372 = 0.002880524378269911 + 0.01 * 6.577259540557861
Epoch 940, val loss: 1.3402459621429443
Epoch 950, training loss: 0.06845775246620178 = 0.002811070764437318 + 0.01 * 6.56466817855835
Epoch 950, val loss: 1.3446178436279297
Epoch 960, training loss: 0.06853818148374557 = 0.0027447077445685863 + 0.01 * 6.579348087310791
Epoch 960, val loss: 1.348719596862793
Epoch 970, training loss: 0.06828361004590988 = 0.0026812488213181496 + 0.01 * 6.56023645401001
Epoch 970, val loss: 1.352734923362732
Epoch 980, training loss: 0.06841433048248291 = 0.0026205533649772406 + 0.01 * 6.5793776512146
Epoch 980, val loss: 1.3567842245101929
Epoch 990, training loss: 0.06811713427305222 = 0.0025626111309975386 + 0.01 * 6.555452346801758
Epoch 990, val loss: 1.3607532978057861
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.5055
Flip ASR: 0.4089/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0349018573760986 = 1.951164960861206 + 0.01 * 8.373692512512207
Epoch 0, val loss: 1.9502538442611694
Epoch 10, training loss: 2.024578094482422 = 1.940843105316162 + 0.01 * 8.373501777648926
Epoch 10, val loss: 1.9389981031417847
Epoch 20, training loss: 2.011925220489502 = 1.9281980991363525 + 0.01 * 8.372712135314941
Epoch 20, val loss: 1.924712896347046
Epoch 30, training loss: 1.9941166639328003 = 1.910414695739746 + 0.01 * 8.370195388793945
Epoch 30, val loss: 1.9044005870819092
Epoch 40, training loss: 1.9675320386886597 = 1.8839330673217773 + 0.01 * 8.359901428222656
Epoch 40, val loss: 1.8743976354599
Epoch 50, training loss: 1.9288572072982788 = 1.8459866046905518 + 0.01 * 8.287057876586914
Epoch 50, val loss: 1.8329343795776367
Epoch 60, training loss: 1.8757562637329102 = 1.7991575002670288 + 0.01 * 7.659878253936768
Epoch 60, val loss: 1.78522527217865
Epoch 70, training loss: 1.8265039920806885 = 1.7520904541015625 + 0.01 * 7.441359043121338
Epoch 70, val loss: 1.7414802312850952
Epoch 80, training loss: 1.7725108861923218 = 1.699591040611267 + 0.01 * 7.29198694229126
Epoch 80, val loss: 1.6939529180526733
Epoch 90, training loss: 1.7005451917648315 = 1.6293692588806152 + 0.01 * 7.117593765258789
Epoch 90, val loss: 1.6320735216140747
Epoch 100, training loss: 1.6099942922592163 = 1.5398285388946533 + 0.01 * 7.01657772064209
Epoch 100, val loss: 1.5562447309494019
Epoch 110, training loss: 1.5073388814926147 = 1.4375263452529907 + 0.01 * 6.981257438659668
Epoch 110, val loss: 1.4711967706680298
Epoch 120, training loss: 1.4031902551651 = 1.3336095809936523 + 0.01 * 6.958065986633301
Epoch 120, val loss: 1.3888130187988281
Epoch 130, training loss: 1.3035331964492798 = 1.2340584993362427 + 0.01 * 6.9474663734436035
Epoch 130, val loss: 1.3144278526306152
Epoch 140, training loss: 1.2095214128494263 = 1.1401677131652832 + 0.01 * 6.935369491577148
Epoch 140, val loss: 1.247712254524231
Epoch 150, training loss: 1.1202375888824463 = 1.0510154962539673 + 0.01 * 6.922211170196533
Epoch 150, val loss: 1.1858692169189453
Epoch 160, training loss: 1.0342293977737427 = 0.9651482701301575 + 0.01 * 6.908110618591309
Epoch 160, val loss: 1.1267807483673096
Epoch 170, training loss: 0.9511374831199646 = 0.8822109699249268 + 0.01 * 6.8926496505737305
Epoch 170, val loss: 1.0690770149230957
Epoch 180, training loss: 0.8710047006607056 = 0.8022340536117554 + 0.01 * 6.877067565917969
Epoch 180, val loss: 1.0127671957015991
Epoch 190, training loss: 0.7940865755081177 = 0.7254593968391418 + 0.01 * 6.862715244293213
Epoch 190, val loss: 0.957997739315033
Epoch 200, training loss: 0.7213050127029419 = 0.6527908444404602 + 0.01 * 6.8514180183410645
Epoch 200, val loss: 0.906531035900116
Epoch 210, training loss: 0.6538407802581787 = 0.5854232311248779 + 0.01 * 6.841757297515869
Epoch 210, val loss: 0.860217273235321
Epoch 220, training loss: 0.5921485424041748 = 0.5238113403320312 + 0.01 * 6.8337225914001465
Epoch 220, val loss: 0.8195399641990662
Epoch 230, training loss: 0.5358952283859253 = 0.4676513373851776 + 0.01 * 6.82438850402832
Epoch 230, val loss: 0.7849564552307129
Epoch 240, training loss: 0.48445695638656616 = 0.41627630591392517 + 0.01 * 6.818065166473389
Epoch 240, val loss: 0.7561824917793274
Epoch 250, training loss: 0.4370979070663452 = 0.3690301775932312 + 0.01 * 6.806772708892822
Epoch 250, val loss: 0.7322139143943787
Epoch 260, training loss: 0.3932477533817291 = 0.32526811957359314 + 0.01 * 6.7979631423950195
Epoch 260, val loss: 0.7122218608856201
Epoch 270, training loss: 0.3527067005634308 = 0.2846354842185974 + 0.01 * 6.8071208000183105
Epoch 270, val loss: 0.6952795386314392
Epoch 280, training loss: 0.3152785003185272 = 0.24732795357704163 + 0.01 * 6.795054912567139
Epoch 280, val loss: 0.6812267303466797
Epoch 290, training loss: 0.2814863920211792 = 0.21365387737751007 + 0.01 * 6.783249855041504
Epoch 290, val loss: 0.669833779335022
Epoch 300, training loss: 0.25168249011039734 = 0.18390823900699615 + 0.01 * 6.777424335479736
Epoch 300, val loss: 0.6616169810295105
Epoch 310, training loss: 0.22587281465530396 = 0.15816116333007812 + 0.01 * 6.771165370941162
Epoch 310, val loss: 0.6567102074623108
Epoch 320, training loss: 0.20391212403774261 = 0.1362590491771698 + 0.01 * 6.765307426452637
Epoch 320, val loss: 0.6549109816551208
Epoch 330, training loss: 0.18550390005111694 = 0.11785592138767242 + 0.01 * 6.764797687530518
Epoch 330, val loss: 0.6561009287834167
Epoch 340, training loss: 0.16997891664505005 = 0.10241708904504776 + 0.01 * 6.756182670593262
Epoch 340, val loss: 0.6598759889602661
Epoch 350, training loss: 0.15695974230766296 = 0.08941283077001572 + 0.01 * 6.754692077636719
Epoch 350, val loss: 0.6655495762825012
Epoch 360, training loss: 0.1458534598350525 = 0.07836953550577164 + 0.01 * 6.748392581939697
Epoch 360, val loss: 0.6729588508605957
Epoch 370, training loss: 0.13632801175117493 = 0.06892652809619904 + 0.01 * 6.74014949798584
Epoch 370, val loss: 0.6813695430755615
Epoch 380, training loss: 0.128221794962883 = 0.0608547143638134 + 0.01 * 6.736708164215088
Epoch 380, val loss: 0.6905909180641174
Epoch 390, training loss: 0.12138035893440247 = 0.05397697910666466 + 0.01 * 6.74033784866333
Epoch 390, val loss: 0.7005287408828735
Epoch 400, training loss: 0.1154547929763794 = 0.04813578724861145 + 0.01 * 6.731900691986084
Epoch 400, val loss: 0.7105332016944885
Epoch 410, training loss: 0.11038385331630707 = 0.043168600648641586 + 0.01 * 6.721524715423584
Epoch 410, val loss: 0.7207099795341492
Epoch 420, training loss: 0.10609385371208191 = 0.038934677839279175 + 0.01 * 6.715918064117432
Epoch 420, val loss: 0.7309861183166504
Epoch 430, training loss: 0.10243856906890869 = 0.0353064090013504 + 0.01 * 6.713216304779053
Epoch 430, val loss: 0.7413028478622437
Epoch 440, training loss: 0.09923811256885529 = 0.032164957374334335 + 0.01 * 6.707315921783447
Epoch 440, val loss: 0.7517107129096985
Epoch 450, training loss: 0.09640681743621826 = 0.029417725279927254 + 0.01 * 6.698909282684326
Epoch 450, val loss: 0.7620799541473389
Epoch 460, training loss: 0.09408555924892426 = 0.027011234313249588 + 0.01 * 6.707432270050049
Epoch 460, val loss: 0.7722043991088867
Epoch 470, training loss: 0.09182882308959961 = 0.02489941194653511 + 0.01 * 6.692940711975098
Epoch 470, val loss: 0.782200276851654
Epoch 480, training loss: 0.08992607146501541 = 0.023029988631606102 + 0.01 * 6.689608573913574
Epoch 480, val loss: 0.7919808030128479
Epoch 490, training loss: 0.08826832473278046 = 0.02136591449379921 + 0.01 * 6.690240859985352
Epoch 490, val loss: 0.8016309142112732
Epoch 500, training loss: 0.08669838309288025 = 0.019879896193742752 + 0.01 * 6.681849479675293
Epoch 500, val loss: 0.8110539317131042
Epoch 510, training loss: 0.08526328951120377 = 0.018549688160419464 + 0.01 * 6.671360492706299
Epoch 510, val loss: 0.8202290534973145
Epoch 520, training loss: 0.08413628488779068 = 0.017351744696497917 + 0.01 * 6.6784539222717285
Epoch 520, val loss: 0.8291833400726318
Epoch 530, training loss: 0.08292374759912491 = 0.016270890831947327 + 0.01 * 6.665286064147949
Epoch 530, val loss: 0.8379511833190918
Epoch 540, training loss: 0.08203054964542389 = 0.0152913061901927 + 0.01 * 6.673924922943115
Epoch 540, val loss: 0.8464913964271545
Epoch 550, training loss: 0.08097102493047714 = 0.014400787651538849 + 0.01 * 6.657023906707764
Epoch 550, val loss: 0.8549069762229919
Epoch 560, training loss: 0.08031675219535828 = 0.013587400317192078 + 0.01 * 6.672935485839844
Epoch 560, val loss: 0.8630241751670837
Epoch 570, training loss: 0.07933234423398972 = 0.0128437215462327 + 0.01 * 6.648862361907959
Epoch 570, val loss: 0.8710588216781616
Epoch 580, training loss: 0.07858526706695557 = 0.012163827195763588 + 0.01 * 6.642144203186035
Epoch 580, val loss: 0.8787498474121094
Epoch 590, training loss: 0.07818658649921417 = 0.011539909988641739 + 0.01 * 6.664667129516602
Epoch 590, val loss: 0.8863095641136169
Epoch 600, training loss: 0.07736042141914368 = 0.01096578873693943 + 0.01 * 6.639463901519775
Epoch 600, val loss: 0.8938189148902893
Epoch 610, training loss: 0.07691925764083862 = 0.010436671786010265 + 0.01 * 6.648259162902832
Epoch 610, val loss: 0.9009958505630493
Epoch 620, training loss: 0.07631624490022659 = 0.009947833605110645 + 0.01 * 6.636841297149658
Epoch 620, val loss: 0.9081388711929321
Epoch 630, training loss: 0.07570220530033112 = 0.009495196864008904 + 0.01 * 6.620700836181641
Epoch 630, val loss: 0.9150741100311279
Epoch 640, training loss: 0.07563358545303345 = 0.009074989706277847 + 0.01 * 6.655860424041748
Epoch 640, val loss: 0.9218943119049072
Epoch 650, training loss: 0.07487142086029053 = 0.00868497509509325 + 0.01 * 6.6186442375183105
Epoch 650, val loss: 0.9286059141159058
Epoch 660, training loss: 0.07474671304225922 = 0.008321410976350307 + 0.01 * 6.6425299644470215
Epoch 660, val loss: 0.9350777268409729
Epoch 670, training loss: 0.07402252405881882 = 0.007982438430190086 + 0.01 * 6.604008674621582
Epoch 670, val loss: 0.9414818286895752
Epoch 680, training loss: 0.07369540631771088 = 0.007664057891815901 + 0.01 * 6.603135108947754
Epoch 680, val loss: 0.947693943977356
Epoch 690, training loss: 0.07343924045562744 = 0.007365530822426081 + 0.01 * 6.6073713302612305
Epoch 690, val loss: 0.9537516236305237
Epoch 700, training loss: 0.07303310185670853 = 0.007086318917572498 + 0.01 * 6.5946784019470215
Epoch 700, val loss: 0.9597846269607544
Epoch 710, training loss: 0.07275092601776123 = 0.006824580952525139 + 0.01 * 6.592634677886963
Epoch 710, val loss: 0.9656089544296265
Epoch 720, training loss: 0.07249967008829117 = 0.006578704807907343 + 0.01 * 6.59209680557251
Epoch 720, val loss: 0.9713567495346069
Epoch 730, training loss: 0.07217549532651901 = 0.006349046714603901 + 0.01 * 6.582644939422607
Epoch 730, val loss: 0.9770288467407227
Epoch 740, training loss: 0.07198342680931091 = 0.0061331200413405895 + 0.01 * 6.585031509399414
Epoch 740, val loss: 0.9825897216796875
Epoch 750, training loss: 0.07163069397211075 = 0.005929639097303152 + 0.01 * 6.57010555267334
Epoch 750, val loss: 0.9880027770996094
Epoch 760, training loss: 0.07154504954814911 = 0.005737392697483301 + 0.01 * 6.580765724182129
Epoch 760, val loss: 0.993380606174469
Epoch 770, training loss: 0.07135532796382904 = 0.005555738694965839 + 0.01 * 6.579958915710449
Epoch 770, val loss: 0.9985738396644592
Epoch 780, training loss: 0.07100081443786621 = 0.005383998621255159 + 0.01 * 6.561681747436523
Epoch 780, val loss: 1.0037184953689575
Epoch 790, training loss: 0.07083161175251007 = 0.005221528932452202 + 0.01 * 6.561008453369141
Epoch 790, val loss: 1.0086458921432495
Epoch 800, training loss: 0.07052398473024368 = 0.005067219492048025 + 0.01 * 6.5456767082214355
Epoch 800, val loss: 1.0136253833770752
Epoch 810, training loss: 0.07049965858459473 = 0.004921036772429943 + 0.01 * 6.557862758636475
Epoch 810, val loss: 1.018445611000061
Epoch 820, training loss: 0.07023433595895767 = 0.004781866446137428 + 0.01 * 6.5452470779418945
Epoch 820, val loss: 1.0231425762176514
Epoch 830, training loss: 0.0699491947889328 = 0.004649536218494177 + 0.01 * 6.529966354370117
Epoch 830, val loss: 1.0277220010757446
Epoch 840, training loss: 0.06990312039852142 = 0.00452375877648592 + 0.01 * 6.537935733795166
Epoch 840, val loss: 1.0323656797409058
Epoch 850, training loss: 0.06966788321733475 = 0.00440381933003664 + 0.01 * 6.526406288146973
Epoch 850, val loss: 1.0367431640625
Epoch 860, training loss: 0.06956466287374496 = 0.004289863631129265 + 0.01 * 6.527480125427246
Epoch 860, val loss: 1.0412391424179077
Epoch 870, training loss: 0.06931763142347336 = 0.0041811782866716385 + 0.01 * 6.513645648956299
Epoch 870, val loss: 1.0454431772232056
Epoch 880, training loss: 0.06920341402292252 = 0.004077450837939978 + 0.01 * 6.512596607208252
Epoch 880, val loss: 1.0497384071350098
Epoch 890, training loss: 0.06920427083969116 = 0.003978437278419733 + 0.01 * 6.5225830078125
Epoch 890, val loss: 1.0538270473480225
Epoch 900, training loss: 0.0691462904214859 = 0.0038838102482259274 + 0.01 * 6.526248455047607
Epoch 900, val loss: 1.0579267740249634
Epoch 910, training loss: 0.06891941279172897 = 0.003792991628870368 + 0.01 * 6.5126423835754395
Epoch 910, val loss: 1.0619596242904663
Epoch 920, training loss: 0.06861209124326706 = 0.003705483628436923 + 0.01 * 6.490660667419434
Epoch 920, val loss: 1.06585693359375
Epoch 930, training loss: 0.06869398802518845 = 0.003621459938585758 + 0.01 * 6.5072526931762695
Epoch 930, val loss: 1.0697153806686401
Epoch 940, training loss: 0.0683399885892868 = 0.0035407952964305878 + 0.01 * 6.47991943359375
Epoch 940, val loss: 1.073479175567627
Epoch 950, training loss: 0.06829541176557541 = 0.0034633378963917494 + 0.01 * 6.483207702636719
Epoch 950, val loss: 1.0772539377212524
Epoch 960, training loss: 0.06850495934486389 = 0.003388988086953759 + 0.01 * 6.511597633361816
Epoch 960, val loss: 1.0809084177017212
Epoch 970, training loss: 0.06801480799913406 = 0.0033178844023495913 + 0.01 * 6.469692230224609
Epoch 970, val loss: 1.0844812393188477
Epoch 980, training loss: 0.068364217877388 = 0.0032499011140316725 + 0.01 * 6.51143217086792
Epoch 980, val loss: 1.088036060333252
Epoch 990, training loss: 0.06780790537595749 = 0.003184549743309617 + 0.01 * 6.46233606338501
Epoch 990, val loss: 1.0915378332138062
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.5203
Flip ASR: 0.4622/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.046891927719116 = 1.963154911994934 + 0.01 * 8.373709678649902
Epoch 0, val loss: 1.9693288803100586
Epoch 10, training loss: 2.035461902618408 = 1.951727271080017 + 0.01 * 8.373470306396484
Epoch 10, val loss: 1.956811785697937
Epoch 20, training loss: 2.021411418914795 = 1.9376856088638306 + 0.01 * 8.372590065002441
Epoch 20, val loss: 1.941026210784912
Epoch 30, training loss: 2.001708984375 = 1.9180136919021606 + 0.01 * 8.369540214538574
Epoch 30, val loss: 1.9186367988586426
Epoch 40, training loss: 1.9730523824691772 = 1.889497995376587 + 0.01 * 8.355440139770508
Epoch 40, val loss: 1.8864680528640747
Epoch 50, training loss: 1.932478666305542 = 1.849859595298767 + 0.01 * 8.261913299560547
Epoch 50, val loss: 1.8434163331985474
Epoch 60, training loss: 1.8806458711624146 = 1.8022651672363281 + 0.01 * 7.838073253631592
Epoch 60, val loss: 1.795732021331787
Epoch 70, training loss: 1.8302394151687622 = 1.7559969425201416 + 0.01 * 7.4242424964904785
Epoch 70, val loss: 1.7539349794387817
Epoch 80, training loss: 1.778612732887268 = 1.7066397666931152 + 0.01 * 7.197297096252441
Epoch 80, val loss: 1.7111321687698364
Epoch 90, training loss: 1.7128428220748901 = 1.6419415473937988 + 0.01 * 7.090125560760498
Epoch 90, val loss: 1.6564534902572632
Epoch 100, training loss: 1.6269524097442627 = 1.556672215461731 + 0.01 * 7.028018474578857
Epoch 100, val loss: 1.585619330406189
Epoch 110, training loss: 1.5242725610733032 = 1.454421043395996 + 0.01 * 6.985151290893555
Epoch 110, val loss: 1.503292441368103
Epoch 120, training loss: 1.4160622358322144 = 1.3464716672897339 + 0.01 * 6.959056854248047
Epoch 120, val loss: 1.4201959371566772
Epoch 130, training loss: 1.3090969324111938 = 1.2397139072418213 + 0.01 * 6.938302993774414
Epoch 130, val loss: 1.3417723178863525
Epoch 140, training loss: 1.2036908864974976 = 1.1345301866531372 + 0.01 * 6.916065692901611
Epoch 140, val loss: 1.2673712968826294
Epoch 150, training loss: 1.1002132892608643 = 1.031338095664978 + 0.01 * 6.887522220611572
Epoch 150, val loss: 1.195811152458191
Epoch 160, training loss: 1.0010027885437012 = 0.9324745535850525 + 0.01 * 6.852818012237549
Epoch 160, val loss: 1.1271528005599976
Epoch 170, training loss: 0.9095008373260498 = 0.8412084579467773 + 0.01 * 6.829237461090088
Epoch 170, val loss: 1.0626567602157593
Epoch 180, training loss: 0.8282893300056458 = 0.7602453231811523 + 0.01 * 6.804401397705078
Epoch 180, val loss: 1.0054244995117188
Epoch 190, training loss: 0.7580130100250244 = 0.6901124119758606 + 0.01 * 6.790061950683594
Epoch 190, val loss: 0.9575279951095581
Epoch 200, training loss: 0.6975208520889282 = 0.6296817064285278 + 0.01 * 6.783917427062988
Epoch 200, val loss: 0.9186432957649231
Epoch 210, training loss: 0.645314633846283 = 0.577580451965332 + 0.01 * 6.773418426513672
Epoch 210, val loss: 0.8877043128013611
Epoch 220, training loss: 0.6003355383872986 = 0.5326393246650696 + 0.01 * 6.769621849060059
Epoch 220, val loss: 0.8632022142410278
Epoch 230, training loss: 0.5616924166679382 = 0.4940386712551117 + 0.01 * 6.765376091003418
Epoch 230, val loss: 0.8443003296852112
Epoch 240, training loss: 0.5284127593040466 = 0.46083149313926697 + 0.01 * 6.758127689361572
Epoch 240, val loss: 0.8301823735237122
Epoch 250, training loss: 0.49935001134872437 = 0.4317861497402191 + 0.01 * 6.756385326385498
Epoch 250, val loss: 0.8198369145393372
Epoch 260, training loss: 0.4731706380844116 = 0.4056521952152252 + 0.01 * 6.751844882965088
Epoch 260, val loss: 0.8121698498725891
Epoch 270, training loss: 0.44853466749191284 = 0.38103801012039185 + 0.01 * 6.749664306640625
Epoch 270, val loss: 0.8059065341949463
Epoch 280, training loss: 0.4241487383842468 = 0.35668322443962097 + 0.01 * 6.746551990509033
Epoch 280, val loss: 0.8003768920898438
Epoch 290, training loss: 0.3990246057510376 = 0.331585168838501 + 0.01 * 6.74394416809082
Epoch 290, val loss: 0.7948632836341858
Epoch 300, training loss: 0.3724885582923889 = 0.30505484342575073 + 0.01 * 6.743370532989502
Epoch 300, val loss: 0.7888979315757751
Epoch 310, training loss: 0.34430715441703796 = 0.2768544852733612 + 0.01 * 6.745265960693359
Epoch 310, val loss: 0.7820967435836792
Epoch 320, training loss: 0.3149823546409607 = 0.24757468700408936 + 0.01 * 6.740765571594238
Epoch 320, val loss: 0.7750927805900574
Epoch 330, training loss: 0.2852832078933716 = 0.21787035465240479 + 0.01 * 6.741285800933838
Epoch 330, val loss: 0.768355131149292
Epoch 340, training loss: 0.25620776414871216 = 0.1888171285390854 + 0.01 * 6.739065170288086
Epoch 340, val loss: 0.7633851766586304
Epoch 350, training loss: 0.22914962470531464 = 0.16178525984287262 + 0.01 * 6.73643684387207
Epoch 350, val loss: 0.761379599571228
Epoch 360, training loss: 0.20494845509529114 = 0.13757577538490295 + 0.01 * 6.737268447875977
Epoch 360, val loss: 0.7626523375511169
Epoch 370, training loss: 0.18399377167224884 = 0.1166614517569542 + 0.01 * 6.733232021331787
Epoch 370, val loss: 0.7671119570732117
Epoch 380, training loss: 0.16646505892276764 = 0.09911877661943436 + 0.01 * 6.734628200531006
Epoch 380, val loss: 0.7740440964698792
Epoch 390, training loss: 0.15186867117881775 = 0.08463308960199356 + 0.01 * 6.72355842590332
Epoch 390, val loss: 0.7834060192108154
Epoch 400, training loss: 0.13994717597961426 = 0.07274652272462845 + 0.01 * 6.720065116882324
Epoch 400, val loss: 0.7946000099182129
Epoch 410, training loss: 0.13024961948394775 = 0.06298697739839554 + 0.01 * 6.726264953613281
Epoch 410, val loss: 0.8073967695236206
Epoch 420, training loss: 0.12209023535251617 = 0.05495068058371544 + 0.01 * 6.713955879211426
Epoch 420, val loss: 0.8210873603820801
Epoch 430, training loss: 0.11538279056549072 = 0.048291150480508804 + 0.01 * 6.709164142608643
Epoch 430, val loss: 0.8352698087692261
Epoch 440, training loss: 0.10996173322200775 = 0.042741235345602036 + 0.01 * 6.722050189971924
Epoch 440, val loss: 0.849604070186615
Epoch 450, training loss: 0.10506594926118851 = 0.03808550536632538 + 0.01 * 6.698044300079346
Epoch 450, val loss: 0.8638121485710144
Epoch 460, training loss: 0.10107463598251343 = 0.034149374812841415 + 0.01 * 6.692526340484619
Epoch 460, val loss: 0.8777307868003845
Epoch 470, training loss: 0.09785660356283188 = 0.03080066293478012 + 0.01 * 6.705594539642334
Epoch 470, val loss: 0.8913211226463318
Epoch 480, training loss: 0.0948483794927597 = 0.027933970093727112 + 0.01 * 6.691441059112549
Epoch 480, val loss: 0.9043987989425659
Epoch 490, training loss: 0.09223858267068863 = 0.025463951751589775 + 0.01 * 6.677463054656982
Epoch 490, val loss: 0.9170849919319153
Epoch 500, training loss: 0.09012908488512039 = 0.023319371044635773 + 0.01 * 6.680971622467041
Epoch 500, val loss: 0.929397463798523
Epoch 510, training loss: 0.08817166835069656 = 0.02144722454249859 + 0.01 * 6.6724443435668945
Epoch 510, val loss: 0.9412176012992859
Epoch 520, training loss: 0.08653916418552399 = 0.019801713526248932 + 0.01 * 6.673745155334473
Epoch 520, val loss: 0.9526177048683167
Epoch 530, training loss: 0.08494774252176285 = 0.018347986042499542 + 0.01 * 6.659976005554199
Epoch 530, val loss: 0.9635927677154541
Epoch 540, training loss: 0.08354373276233673 = 0.0170560572296381 + 0.01 * 6.648767948150635
Epoch 540, val loss: 0.9741784930229187
Epoch 550, training loss: 0.08261336386203766 = 0.015902668237686157 + 0.01 * 6.671069622039795
Epoch 550, val loss: 0.9844450950622559
Epoch 560, training loss: 0.08133777230978012 = 0.014870154671370983 + 0.01 * 6.646761894226074
Epoch 560, val loss: 0.9942620396614075
Epoch 570, training loss: 0.08031431585550308 = 0.013940629549324512 + 0.01 * 6.637368202209473
Epoch 570, val loss: 1.003774881362915
Epoch 580, training loss: 0.07965162396430969 = 0.013099255971610546 + 0.01 * 6.655237197875977
Epoch 580, val loss: 1.0130271911621094
Epoch 590, training loss: 0.07866644114255905 = 0.012336613610386848 + 0.01 * 6.6329827308654785
Epoch 590, val loss: 1.0219124555587769
Epoch 600, training loss: 0.0779123604297638 = 0.011640175245702267 + 0.01 * 6.627218723297119
Epoch 600, val loss: 1.030461072921753
Epoch 610, training loss: 0.0772690400481224 = 0.01100396178662777 + 0.01 * 6.626507759094238
Epoch 610, val loss: 1.0388479232788086
Epoch 620, training loss: 0.0767408013343811 = 0.010420236736536026 + 0.01 * 6.632057189941406
Epoch 620, val loss: 1.0469326972961426
Epoch 630, training loss: 0.07612669467926025 = 0.009884197264909744 + 0.01 * 6.624249458312988
Epoch 630, val loss: 1.0547471046447754
Epoch 640, training loss: 0.07541730999946594 = 0.009390298277139664 + 0.01 * 6.602701663970947
Epoch 640, val loss: 1.0622972249984741
Epoch 650, training loss: 0.07503773272037506 = 0.00893431156873703 + 0.01 * 6.610342025756836
Epoch 650, val loss: 1.069792628288269
Epoch 660, training loss: 0.0745333582162857 = 0.008512924425303936 + 0.01 * 6.602043628692627
Epoch 660, val loss: 1.0769152641296387
Epoch 670, training loss: 0.07409452646970749 = 0.008122168481349945 + 0.01 * 6.597235679626465
Epoch 670, val loss: 1.0839099884033203
Epoch 680, training loss: 0.07366208732128143 = 0.007759490050375462 + 0.01 * 6.590260028839111
Epoch 680, val loss: 1.0907306671142578
Epoch 690, training loss: 0.07326316088438034 = 0.00742232333868742 + 0.01 * 6.5840840339660645
Epoch 690, val loss: 1.0974128246307373
Epoch 700, training loss: 0.07291129231452942 = 0.00710863946005702 + 0.01 * 6.580265522003174
Epoch 700, val loss: 1.1038190126419067
Epoch 710, training loss: 0.07259448617696762 = 0.006815937347710133 + 0.01 * 6.577854633331299
Epoch 710, val loss: 1.110039472579956
Epoch 720, training loss: 0.07227084040641785 = 0.006542369723320007 + 0.01 * 6.572847366333008
Epoch 720, val loss: 1.1161469221115112
Epoch 730, training loss: 0.07215458154678345 = 0.006287010386586189 + 0.01 * 6.586757659912109
Epoch 730, val loss: 1.1221225261688232
Epoch 740, training loss: 0.0718703418970108 = 0.006048054900020361 + 0.01 * 6.582229137420654
Epoch 740, val loss: 1.127842903137207
Epoch 750, training loss: 0.07142419368028641 = 0.0058236028999090195 + 0.01 * 6.560059070587158
Epoch 750, val loss: 1.1334799528121948
Epoch 760, training loss: 0.07109767198562622 = 0.005612763110548258 + 0.01 * 6.54849100112915
Epoch 760, val loss: 1.1389964818954468
Epoch 770, training loss: 0.07097560167312622 = 0.0054151262156665325 + 0.01 * 6.556047439575195
Epoch 770, val loss: 1.1443684101104736
Epoch 780, training loss: 0.07064322382211685 = 0.005229515489190817 + 0.01 * 6.541370868682861
Epoch 780, val loss: 1.1495416164398193
Epoch 790, training loss: 0.0706324353814125 = 0.005054620560258627 + 0.01 * 6.55778169631958
Epoch 790, val loss: 1.1546881198883057
Epoch 800, training loss: 0.07025721669197083 = 0.004889762494713068 + 0.01 * 6.536745071411133
Epoch 800, val loss: 1.1595810651779175
Epoch 810, training loss: 0.06994428485631943 = 0.004733847454190254 + 0.01 * 6.52104377746582
Epoch 810, val loss: 1.1645033359527588
Epoch 820, training loss: 0.06991132348775864 = 0.00458660488948226 + 0.01 * 6.532471656799316
Epoch 820, val loss: 1.1692485809326172
Epoch 830, training loss: 0.06966479867696762 = 0.0044472236186265945 + 0.01 * 6.52175760269165
Epoch 830, val loss: 1.1738719940185547
Epoch 840, training loss: 0.06965705752372742 = 0.004315150436013937 + 0.01 * 6.534190654754639
Epoch 840, val loss: 1.1784335374832153
Epoch 850, training loss: 0.06949976831674576 = 0.004190320149064064 + 0.01 * 6.530945301055908
Epoch 850, val loss: 1.1827970743179321
Epoch 860, training loss: 0.06912296265363693 = 0.004071652889251709 + 0.01 * 6.505130767822266
Epoch 860, val loss: 1.1871153116226196
Epoch 870, training loss: 0.06903389096260071 = 0.00395912304520607 + 0.01 * 6.507476329803467
Epoch 870, val loss: 1.1913948059082031
Epoch 880, training loss: 0.06890354305505753 = 0.0038520696107298136 + 0.01 * 6.505147457122803
Epoch 880, val loss: 1.195499300956726
Epoch 890, training loss: 0.06872924417257309 = 0.0037502211052924395 + 0.01 * 6.497902870178223
Epoch 890, val loss: 1.199567437171936
Epoch 900, training loss: 0.06893850117921829 = 0.0036533025559037924 + 0.01 * 6.528520107269287
Epoch 900, val loss: 1.2035354375839233
Epoch 910, training loss: 0.0684259906411171 = 0.0035609703045338392 + 0.01 * 6.486502647399902
Epoch 910, val loss: 1.207418441772461
Epoch 920, training loss: 0.06828492134809494 = 0.0034729556646198034 + 0.01 * 6.481196880340576
Epoch 920, val loss: 1.2112407684326172
Epoch 930, training loss: 0.06823772192001343 = 0.003389030694961548 + 0.01 * 6.484869480133057
Epoch 930, val loss: 1.2148362398147583
Epoch 940, training loss: 0.06800321489572525 = 0.0033086256589740515 + 0.01 * 6.469459533691406
Epoch 940, val loss: 1.2184611558914185
Epoch 950, training loss: 0.06787646561861038 = 0.0032317040022462606 + 0.01 * 6.464476585388184
Epoch 950, val loss: 1.2219880819320679
Epoch 960, training loss: 0.06821799278259277 = 0.0031580731738358736 + 0.01 * 6.505992412567139
Epoch 960, val loss: 1.2254860401153564
Epoch 970, training loss: 0.06772765517234802 = 0.0030875818338245153 + 0.01 * 6.464007377624512
Epoch 970, val loss: 1.2288738489151
Epoch 980, training loss: 0.06777790188789368 = 0.003020128235220909 + 0.01 * 6.475777626037598
Epoch 980, val loss: 1.2322713136672974
Epoch 990, training loss: 0.06763464212417603 = 0.0029555619694292545 + 0.01 * 6.467907905578613
Epoch 990, val loss: 1.2354239225387573
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.6531
Flip ASR: 0.6000/225 nodes
The final ASR:0.55966, 0.06638, Accuracy:0.82716, 0.00462
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11650])
remove edge: torch.Size([2, 9470])
updated graph: torch.Size([2, 10564])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98032, 0.00627, Accuracy:0.83333, 0.00605
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.4 der2:0.4 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.031641960144043 = 1.947904348373413 + 0.01 * 8.373760223388672
Epoch 0, val loss: 1.9440163373947144
Epoch 10, training loss: 2.022113800048828 = 1.9383766651153564 + 0.01 * 8.373705863952637
Epoch 10, val loss: 1.9346996545791626
Epoch 20, training loss: 2.0108983516693115 = 1.927164077758789 + 0.01 * 8.373433113098145
Epoch 20, val loss: 1.9233235120773315
Epoch 30, training loss: 1.9955006837844849 = 1.9117735624313354 + 0.01 * 8.372714042663574
Epoch 30, val loss: 1.9073513746261597
Epoch 40, training loss: 1.9723937511444092 = 1.8886960744857788 + 0.01 * 8.369768142700195
Epoch 40, val loss: 1.88345205783844
Epoch 50, training loss: 1.9379055500030518 = 1.8544108867645264 + 0.01 * 8.349467277526855
Epoch 50, val loss: 1.8491743803024292
Epoch 60, training loss: 1.8915462493896484 = 1.8095366954803467 + 0.01 * 8.200959205627441
Epoch 60, val loss: 1.807639241218567
Epoch 70, training loss: 1.8384956121444702 = 1.7621458768844604 + 0.01 * 7.634977340698242
Epoch 70, val loss: 1.7687655687332153
Epoch 80, training loss: 1.7815576791763306 = 1.708648443222046 + 0.01 * 7.290926456451416
Epoch 80, val loss: 1.7249656915664673
Epoch 90, training loss: 1.7062199115753174 = 1.634989857673645 + 0.01 * 7.123011589050293
Epoch 90, val loss: 1.6620607376098633
Epoch 100, training loss: 1.608221173286438 = 1.5375803709030151 + 0.01 * 7.064084053039551
Epoch 100, val loss: 1.579302191734314
Epoch 110, training loss: 1.4916131496429443 = 1.4212909936904907 + 0.01 * 7.0322184562683105
Epoch 110, val loss: 1.4833412170410156
Epoch 120, training loss: 1.3661916255950928 = 1.2960854768753052 + 0.01 * 7.010609149932861
Epoch 120, val loss: 1.3822710514068604
Epoch 130, training loss: 1.2428232431411743 = 1.1728991270065308 + 0.01 * 6.992417335510254
Epoch 130, val loss: 1.2857246398925781
Epoch 140, training loss: 1.1295480728149414 = 1.0597937107086182 + 0.01 * 6.97543478012085
Epoch 140, val loss: 1.1992619037628174
Epoch 150, training loss: 1.0298691987991333 = 0.9602957367897034 + 0.01 * 6.957341194152832
Epoch 150, val loss: 1.1248688697814941
Epoch 160, training loss: 0.9420688152313232 = 0.8727124929428101 + 0.01 * 6.935634136199951
Epoch 160, val loss: 1.0610255002975464
Epoch 170, training loss: 0.861980676651001 = 0.7929125428199768 + 0.01 * 6.906811237335205
Epoch 170, val loss: 1.0049750804901123
Epoch 180, training loss: 0.7860540747642517 = 0.7173288464546204 + 0.01 * 6.872523784637451
Epoch 180, val loss: 0.9536473155021667
Epoch 190, training loss: 0.7130279541015625 = 0.6446352005004883 + 0.01 * 6.839272975921631
Epoch 190, val loss: 0.9058839082717896
Epoch 200, training loss: 0.6435025930404663 = 0.5753187537193298 + 0.01 * 6.818386077880859
Epoch 200, val loss: 0.8624288439750671
Epoch 210, training loss: 0.5785825848579407 = 0.5105792880058289 + 0.01 * 6.800332069396973
Epoch 210, val loss: 0.8244910836219788
Epoch 220, training loss: 0.5190920829772949 = 0.4512432813644409 + 0.01 * 6.784877777099609
Epoch 220, val loss: 0.7931334376335144
Epoch 230, training loss: 0.46525388956069946 = 0.39754772186279297 + 0.01 * 6.7706170082092285
Epoch 230, val loss: 0.7687893509864807
Epoch 240, training loss: 0.4170558452606201 = 0.34942322969436646 + 0.01 * 6.763260841369629
Epoch 240, val loss: 0.7509591579437256
Epoch 250, training loss: 0.37410852313041687 = 0.30664896965026855 + 0.01 * 6.7459564208984375
Epoch 250, val loss: 0.7391637563705444
Epoch 260, training loss: 0.33625534176826477 = 0.2689109146595001 + 0.01 * 6.734441757202148
Epoch 260, val loss: 0.7327068448066711
Epoch 270, training loss: 0.30323970317840576 = 0.23585334420204163 + 0.01 * 6.738635540008545
Epoch 270, val loss: 0.7308624982833862
Epoch 280, training loss: 0.27427059412002563 = 0.2070687711238861 + 0.01 * 6.720183849334717
Epoch 280, val loss: 0.732947051525116
Epoch 290, training loss: 0.24927210807800293 = 0.18212124705314636 + 0.01 * 6.715085506439209
Epoch 290, val loss: 0.7382473945617676
Epoch 300, training loss: 0.22769194841384888 = 0.16058918833732605 + 0.01 * 6.7102766036987305
Epoch 300, val loss: 0.7462611794471741
Epoch 310, training loss: 0.20909741520881653 = 0.14203767478466034 + 0.01 * 6.7059736251831055
Epoch 310, val loss: 0.7565574049949646
Epoch 320, training loss: 0.19306182861328125 = 0.12603798508644104 + 0.01 * 6.702385425567627
Epoch 320, val loss: 0.7687191963195801
Epoch 330, training loss: 0.17921873927116394 = 0.11220243573188782 + 0.01 * 6.701630115509033
Epoch 330, val loss: 0.782538115978241
Epoch 340, training loss: 0.1671791970729828 = 0.10019288212060928 + 0.01 * 6.698631286621094
Epoch 340, val loss: 0.7977254390716553
Epoch 350, training loss: 0.15665873885154724 = 0.08972147852182388 + 0.01 * 6.693725109100342
Epoch 350, val loss: 0.8139622211456299
Epoch 360, training loss: 0.14744630455970764 = 0.08056242763996124 + 0.01 * 6.688388824462891
Epoch 360, val loss: 0.8310762047767639
Epoch 370, training loss: 0.13949784636497498 = 0.07253623008728027 + 0.01 * 6.696162223815918
Epoch 370, val loss: 0.848793089389801
Epoch 380, training loss: 0.13234564661979675 = 0.06549502164125443 + 0.01 * 6.685062885284424
Epoch 380, val loss: 0.8668972253799438
Epoch 390, training loss: 0.12607654929161072 = 0.05930713191628456 + 0.01 * 6.676941871643066
Epoch 390, val loss: 0.8852162957191467
Epoch 400, training loss: 0.1206405907869339 = 0.05386057868599892 + 0.01 * 6.6780009269714355
Epoch 400, val loss: 0.903562068939209
Epoch 410, training loss: 0.11576031893491745 = 0.049058228731155396 + 0.01 * 6.670208930969238
Epoch 410, val loss: 0.921775758266449
Epoch 420, training loss: 0.11146754026412964 = 0.04481222853064537 + 0.01 * 6.665530681610107
Epoch 420, val loss: 0.9397726655006409
Epoch 430, training loss: 0.10765492916107178 = 0.04104788973927498 + 0.01 * 6.660704612731934
Epoch 430, val loss: 0.9575021266937256
Epoch 440, training loss: 0.10429416596889496 = 0.03770282492041588 + 0.01 * 6.659134864807129
Epoch 440, val loss: 0.9748774766921997
Epoch 450, training loss: 0.10125431418418884 = 0.03472360596060753 + 0.01 * 6.653070449829102
Epoch 450, val loss: 0.9918070435523987
Epoch 460, training loss: 0.09859378635883331 = 0.03206057474017143 + 0.01 * 6.653321266174316
Epoch 460, val loss: 1.008319616317749
Epoch 470, training loss: 0.09614676237106323 = 0.029675064608454704 + 0.01 * 6.647170066833496
Epoch 470, val loss: 1.0243498086929321
Epoch 480, training loss: 0.09396883845329285 = 0.027531664818525314 + 0.01 * 6.643717288970947
Epoch 480, val loss: 1.0399415493011475
Epoch 490, training loss: 0.09195445477962494 = 0.02559957280755043 + 0.01 * 6.635488510131836
Epoch 490, val loss: 1.055094599723816
Epoch 500, training loss: 0.09023118019104004 = 0.023852895945310593 + 0.01 * 6.637828826904297
Epoch 500, val loss: 1.0697938203811646
Epoch 510, training loss: 0.08861032128334045 = 0.022271567955613136 + 0.01 * 6.633875846862793
Epoch 510, val loss: 1.0840637683868408
Epoch 520, training loss: 0.08704232424497604 = 0.02083684504032135 + 0.01 * 6.620548248291016
Epoch 520, val loss: 1.097875714302063
Epoch 530, training loss: 0.08569417893886566 = 0.019530653953552246 + 0.01 * 6.616352558135986
Epoch 530, val loss: 1.1112985610961914
Epoch 540, training loss: 0.08448576927185059 = 0.018339460715651512 + 0.01 * 6.614631175994873
Epoch 540, val loss: 1.1243062019348145
Epoch 550, training loss: 0.08340438455343246 = 0.017250919714570045 + 0.01 * 6.615346908569336
Epoch 550, val loss: 1.1369332075119019
Epoch 560, training loss: 0.08229027688503265 = 0.016254965215921402 + 0.01 * 6.603531837463379
Epoch 560, val loss: 1.1491813659667969
Epoch 570, training loss: 0.08135059475898743 = 0.015341713093221188 + 0.01 * 6.600888252258301
Epoch 570, val loss: 1.1610305309295654
Epoch 580, training loss: 0.08065668493509293 = 0.014501981437206268 + 0.01 * 6.6154704093933105
Epoch 580, val loss: 1.1725419759750366
Epoch 590, training loss: 0.07978039979934692 = 0.013728836551308632 + 0.01 * 6.605156421661377
Epoch 590, val loss: 1.183728575706482
Epoch 600, training loss: 0.07896953076124191 = 0.013016232289373875 + 0.01 * 6.595329761505127
Epoch 600, val loss: 1.1945918798446655
Epoch 610, training loss: 0.07820555567741394 = 0.012357477098703384 + 0.01 * 6.584808349609375
Epoch 610, val loss: 1.2051712274551392
Epoch 620, training loss: 0.0776587724685669 = 0.011747505515813828 + 0.01 * 6.591126918792725
Epoch 620, val loss: 1.2153494358062744
Epoch 630, training loss: 0.07696894556283951 = 0.011182566173374653 + 0.01 * 6.578637599945068
Epoch 630, val loss: 1.2253186702728271
Epoch 640, training loss: 0.07671854645013809 = 0.010657740756869316 + 0.01 * 6.606080532073975
Epoch 640, val loss: 1.234967827796936
Epoch 650, training loss: 0.0759740099310875 = 0.010170197114348412 + 0.01 * 6.580381393432617
Epoch 650, val loss: 1.2443522214889526
Epoch 660, training loss: 0.07544520497322083 = 0.00971654336899519 + 0.01 * 6.572866439819336
Epoch 660, val loss: 1.2534903287887573
Epoch 670, training loss: 0.07493124902248383 = 0.009292899630963802 + 0.01 * 6.5638346672058105
Epoch 670, val loss: 1.2624043226242065
Epoch 680, training loss: 0.07480322569608688 = 0.008897044695913792 + 0.01 * 6.59061861038208
Epoch 680, val loss: 1.2709962129592896
Epoch 690, training loss: 0.0741223618388176 = 0.008527638390660286 + 0.01 * 6.559473037719727
Epoch 690, val loss: 1.2794336080551147
Epoch 700, training loss: 0.07377847284078598 = 0.008181875571608543 + 0.01 * 6.559659957885742
Epoch 700, val loss: 1.2876489162445068
Epoch 710, training loss: 0.07361999899148941 = 0.007857332937419415 + 0.01 * 6.576266765594482
Epoch 710, val loss: 1.2956150770187378
Epoch 720, training loss: 0.07305710762739182 = 0.007553273346275091 + 0.01 * 6.550384044647217
Epoch 720, val loss: 1.3033816814422607
Epoch 730, training loss: 0.07279909402132034 = 0.00726719805970788 + 0.01 * 6.553189277648926
Epoch 730, val loss: 1.3109573125839233
Epoch 740, training loss: 0.07247155904769897 = 0.006998088210821152 + 0.01 * 6.547347068786621
Epoch 740, val loss: 1.3183196783065796
Epoch 750, training loss: 0.07217926532030106 = 0.006744592450559139 + 0.01 * 6.543467044830322
Epoch 750, val loss: 1.3255290985107422
Epoch 760, training loss: 0.07191933691501617 = 0.006505711004137993 + 0.01 * 6.541362285614014
Epoch 760, val loss: 1.332556128501892
Epoch 770, training loss: 0.07165013253688812 = 0.0062804757617414 + 0.01 * 6.536965847015381
Epoch 770, val loss: 1.339323878288269
Epoch 780, training loss: 0.07139378041028976 = 0.006067658308893442 + 0.01 * 6.532612323760986
Epoch 780, val loss: 1.3460067510604858
Epoch 790, training loss: 0.0713440328836441 = 0.005866486579179764 + 0.01 * 6.547754287719727
Epoch 790, val loss: 1.3524740934371948
Epoch 800, training loss: 0.07114194333553314 = 0.005676293279975653 + 0.01 * 6.546565055847168
Epoch 800, val loss: 1.3588331937789917
Epoch 810, training loss: 0.07077277451753616 = 0.005496219266206026 + 0.01 * 6.527655601501465
Epoch 810, val loss: 1.3649671077728271
Epoch 820, training loss: 0.07066136598587036 = 0.00532548176124692 + 0.01 * 6.533588886260986
Epoch 820, val loss: 1.3709625005722046
Epoch 830, training loss: 0.07033973932266235 = 0.005163294728845358 + 0.01 * 6.51764440536499
Epoch 830, val loss: 1.3768166303634644
Epoch 840, training loss: 0.07016116380691528 = 0.005009443964809179 + 0.01 * 6.515172481536865
Epoch 840, val loss: 1.3825911283493042
Epoch 850, training loss: 0.07005177438259125 = 0.004863185808062553 + 0.01 * 6.518858909606934
Epoch 850, val loss: 1.3881020545959473
Epoch 860, training loss: 0.06999626755714417 = 0.004724203143268824 + 0.01 * 6.5272064208984375
Epoch 860, val loss: 1.3935761451721191
Epoch 870, training loss: 0.06973537802696228 = 0.0045919641852378845 + 0.01 * 6.514341354370117
Epoch 870, val loss: 1.3989626169204712
Epoch 880, training loss: 0.06949404627084732 = 0.004466080106794834 + 0.01 * 6.502796649932861
Epoch 880, val loss: 1.4040839672088623
Epoch 890, training loss: 0.06931693106889725 = 0.00434618117287755 + 0.01 * 6.497075080871582
Epoch 890, val loss: 1.4091291427612305
Epoch 900, training loss: 0.06918084621429443 = 0.00423182500526309 + 0.01 * 6.49490213394165
Epoch 900, val loss: 1.4140677452087402
Epoch 910, training loss: 0.06939098984003067 = 0.004122663754969835 + 0.01 * 6.526832580566406
Epoch 910, val loss: 1.41887366771698
Epoch 920, training loss: 0.06893576681613922 = 0.0040182690136134624 + 0.01 * 6.491750240325928
Epoch 920, val loss: 1.4236690998077393
Epoch 930, training loss: 0.06888213753700256 = 0.003918647766113281 + 0.01 * 6.496349334716797
Epoch 930, val loss: 1.4282400608062744
Epoch 940, training loss: 0.06881831586360931 = 0.0038233522791415453 + 0.01 * 6.499495983123779
Epoch 940, val loss: 1.4327789545059204
Epoch 950, training loss: 0.06855012476444244 = 0.0037323629949241877 + 0.01 * 6.481776237487793
Epoch 950, val loss: 1.43712317943573
Epoch 960, training loss: 0.06863881647586823 = 0.0036452191416174173 + 0.01 * 6.499360084533691
Epoch 960, val loss: 1.4413877725601196
Epoch 970, training loss: 0.06825482100248337 = 0.00356168020516634 + 0.01 * 6.469314098358154
Epoch 970, val loss: 1.4456462860107422
Epoch 980, training loss: 0.0681385025382042 = 0.0034816048573702574 + 0.01 * 6.465689659118652
Epoch 980, val loss: 1.449718952178955
Epoch 990, training loss: 0.06841129809617996 = 0.0034047290682792664 + 0.01 * 6.500657081604004
Epoch 990, val loss: 1.45366370677948
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.5904
Flip ASR: 0.5156/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.028995990753174 = 1.9452588558197021 + 0.01 * 8.373719215393066
Epoch 0, val loss: 1.9434561729431152
Epoch 10, training loss: 2.0185818672180176 = 1.9348461627960205 + 0.01 * 8.37357234954834
Epoch 10, val loss: 1.93262779712677
Epoch 20, training loss: 2.0054988861083984 = 1.921767234802246 + 0.01 * 8.373167991638184
Epoch 20, val loss: 1.9188544750213623
Epoch 30, training loss: 1.9869754314422607 = 1.9032554626464844 + 0.01 * 8.371991157531738
Epoch 30, val loss: 1.89944589138031
Epoch 40, training loss: 1.9597952365875244 = 1.8761377334594727 + 0.01 * 8.36574649810791
Epoch 40, val loss: 1.871575951576233
Epoch 50, training loss: 1.9223772287368774 = 1.8391032218933105 + 0.01 * 8.327397346496582
Epoch 50, val loss: 1.8355194330215454
Epoch 60, training loss: 1.8789745569229126 = 1.797703504562378 + 0.01 * 8.127108573913574
Epoch 60, val loss: 1.7993524074554443
Epoch 70, training loss: 1.8357633352279663 = 1.757359266281128 + 0.01 * 7.840412139892578
Epoch 70, val loss: 1.7660322189331055
Epoch 80, training loss: 1.780268669128418 = 1.7043704986572266 + 0.01 * 7.589815616607666
Epoch 80, val loss: 1.7200523614883423
Epoch 90, training loss: 1.7052315473556519 = 1.6313472986221313 + 0.01 * 7.388425827026367
Epoch 90, val loss: 1.6572245359420776
Epoch 100, training loss: 1.6114691495895386 = 1.539886474609375 + 0.01 * 7.1582722663879395
Epoch 100, val loss: 1.5806623697280884
Epoch 110, training loss: 1.5134155750274658 = 1.4429199695587158 + 0.01 * 7.049561023712158
Epoch 110, val loss: 1.5001567602157593
Epoch 120, training loss: 1.4193332195281982 = 1.3495839834213257 + 0.01 * 6.974922180175781
Epoch 120, val loss: 1.427554726600647
Epoch 130, training loss: 1.3306353092193604 = 1.26154625415802 + 0.01 * 6.908906936645508
Epoch 130, val loss: 1.3644416332244873
Epoch 140, training loss: 1.2470452785491943 = 1.1784242391586304 + 0.01 * 6.86210298538208
Epoch 140, val loss: 1.3089396953582764
Epoch 150, training loss: 1.1660699844360352 = 1.097764492034912 + 0.01 * 6.830554008483887
Epoch 150, val loss: 1.257323980331421
Epoch 160, training loss: 1.084277629852295 = 1.016183614730835 + 0.01 * 6.80940580368042
Epoch 160, val loss: 1.205781102180481
Epoch 170, training loss: 1.0010673999786377 = 0.9331187009811401 + 0.01 * 6.794873237609863
Epoch 170, val loss: 1.1526427268981934
Epoch 180, training loss: 0.9181122779846191 = 0.8502870202064514 + 0.01 * 6.782526016235352
Epoch 180, val loss: 1.0993415117263794
Epoch 190, training loss: 0.8373901844024658 = 0.769659698009491 + 0.01 * 6.773046016693115
Epoch 190, val loss: 1.047540545463562
Epoch 200, training loss: 0.7608168125152588 = 0.6931543946266174 + 0.01 * 6.766239643096924
Epoch 200, val loss: 0.9989563226699829
Epoch 210, training loss: 0.6904277205467224 = 0.6228237748146057 + 0.01 * 6.760393142700195
Epoch 210, val loss: 0.9553254246711731
Epoch 220, training loss: 0.62786865234375 = 0.5603135228157043 + 0.01 * 6.755516052246094
Epoch 220, val loss: 0.9182382225990295
Epoch 230, training loss: 0.5735495686531067 = 0.5060592293739319 + 0.01 * 6.749033451080322
Epoch 230, val loss: 0.8887742161750793
Epoch 240, training loss: 0.5265480279922485 = 0.4590972363948822 + 0.01 * 6.7450761795043945
Epoch 240, val loss: 0.8665661811828613
Epoch 250, training loss: 0.4850986897945404 = 0.4176775813102722 + 0.01 * 6.742110729217529
Epoch 250, val loss: 0.8504580855369568
Epoch 260, training loss: 0.4474034607410431 = 0.38001248240470886 + 0.01 * 6.7390971183776855
Epoch 260, val loss: 0.8392261266708374
Epoch 270, training loss: 0.4123733639717102 = 0.3450091481208801 + 0.01 * 6.736420631408691
Epoch 270, val loss: 0.8319499492645264
Epoch 280, training loss: 0.3796992897987366 = 0.3123551607131958 + 0.01 * 6.734412670135498
Epoch 280, val loss: 0.8283836841583252
Epoch 290, training loss: 0.3494003415107727 = 0.2820867598056793 + 0.01 * 6.731358528137207
Epoch 290, val loss: 0.8281681537628174
Epoch 300, training loss: 0.3213810324668884 = 0.2540927231311798 + 0.01 * 6.728829860687256
Epoch 300, val loss: 0.830779492855072
Epoch 310, training loss: 0.29532086849212646 = 0.22805923223495483 + 0.01 * 6.726162433624268
Epoch 310, val loss: 0.8356779217720032
Epoch 320, training loss: 0.27088549733161926 = 0.20365455746650696 + 0.01 * 6.723093032836914
Epoch 320, val loss: 0.8422296643257141
Epoch 330, training loss: 0.24789077043533325 = 0.18069496750831604 + 0.01 * 6.71958065032959
Epoch 330, val loss: 0.8500269651412964
Epoch 340, training loss: 0.22645318508148193 = 0.159247487783432 + 0.01 * 6.720569133758545
Epoch 340, val loss: 0.8585923314094543
Epoch 350, training loss: 0.2067040205001831 = 0.1395741105079651 + 0.01 * 6.712991237640381
Epoch 350, val loss: 0.8680408596992493
Epoch 360, training loss: 0.18908078968524933 = 0.12197979539632797 + 0.01 * 6.710099220275879
Epoch 360, val loss: 0.8782517313957214
Epoch 370, training loss: 0.17366111278533936 = 0.10661644488573074 + 0.01 * 6.704465866088867
Epoch 370, val loss: 0.8894593119621277
Epoch 380, training loss: 0.1604861617088318 = 0.09343362599611282 + 0.01 * 6.705254077911377
Epoch 380, val loss: 0.9014374613761902
Epoch 390, training loss: 0.14922618865966797 = 0.08223151415586472 + 0.01 * 6.699467658996582
Epoch 390, val loss: 0.9141846895217896
Epoch 400, training loss: 0.13971200585365295 = 0.07271624356508255 + 0.01 * 6.699577331542969
Epoch 400, val loss: 0.9273209571838379
Epoch 410, training loss: 0.13153594732284546 = 0.06459837406873703 + 0.01 * 6.693756580352783
Epoch 410, val loss: 0.9407756328582764
Epoch 420, training loss: 0.12456060200929642 = 0.05762951076030731 + 0.01 * 6.693109035491943
Epoch 420, val loss: 0.9544445872306824
Epoch 430, training loss: 0.11850804090499878 = 0.05161654204130173 + 0.01 * 6.689150333404541
Epoch 430, val loss: 0.9680277705192566
Epoch 440, training loss: 0.11324232816696167 = 0.04639645293354988 + 0.01 * 6.684587478637695
Epoch 440, val loss: 0.981503427028656
Epoch 450, training loss: 0.10866378247737885 = 0.04184058681130409 + 0.01 * 6.682319164276123
Epoch 450, val loss: 0.9948399066925049
Epoch 460, training loss: 0.10469546914100647 = 0.037855375558137894 + 0.01 * 6.684010028839111
Epoch 460, val loss: 1.0079820156097412
Epoch 470, training loss: 0.10115620493888855 = 0.03435554727911949 + 0.01 * 6.680066108703613
Epoch 470, val loss: 1.0209243297576904
Epoch 480, training loss: 0.09803501516580582 = 0.031267568469047546 + 0.01 * 6.67674446105957
Epoch 480, val loss: 1.0336310863494873
Epoch 490, training loss: 0.09531334042549133 = 0.02852800115942955 + 0.01 * 6.678534030914307
Epoch 490, val loss: 1.0460188388824463
Epoch 500, training loss: 0.09284531325101852 = 0.026103271171450615 + 0.01 * 6.674203872680664
Epoch 500, val loss: 1.0582711696624756
Epoch 510, training loss: 0.09067623317241669 = 0.023953381925821304 + 0.01 * 6.672285079956055
Epoch 510, val loss: 1.070072054862976
Epoch 520, training loss: 0.08876919746398926 = 0.022043021395802498 + 0.01 * 6.6726179122924805
Epoch 520, val loss: 1.0817663669586182
Epoch 530, training loss: 0.08702386915683746 = 0.020341526716947556 + 0.01 * 6.668233871459961
Epoch 530, val loss: 1.0930684804916382
Epoch 540, training loss: 0.08551071584224701 = 0.018823321908712387 + 0.01 * 6.668740272521973
Epoch 540, val loss: 1.1040836572647095
Epoch 550, training loss: 0.08412377536296844 = 0.017465420067310333 + 0.01 * 6.665835857391357
Epoch 550, val loss: 1.1147804260253906
Epoch 560, training loss: 0.08288811147212982 = 0.016247296705842018 + 0.01 * 6.664082050323486
Epoch 560, val loss: 1.1251908540725708
Epoch 570, training loss: 0.08175323903560638 = 0.015151731669902802 + 0.01 * 6.660150527954102
Epoch 570, val loss: 1.135330080986023
Epoch 580, training loss: 0.08076457679271698 = 0.014163706451654434 + 0.01 * 6.660086631774902
Epoch 580, val loss: 1.1452059745788574
Epoch 590, training loss: 0.07982955127954483 = 0.013270847499370575 + 0.01 * 6.65587043762207
Epoch 590, val loss: 1.1547517776489258
Epoch 600, training loss: 0.07901827245950699 = 0.012462110258638859 + 0.01 * 6.655616283416748
Epoch 600, val loss: 1.1640827655792236
Epoch 610, training loss: 0.07824639976024628 = 0.011727267876267433 + 0.01 * 6.651913642883301
Epoch 610, val loss: 1.1731208562850952
Epoch 620, training loss: 0.07757981866598129 = 0.011057852767407894 + 0.01 * 6.652196884155273
Epoch 620, val loss: 1.181944727897644
Epoch 630, training loss: 0.07693126797676086 = 0.010446515865623951 + 0.01 * 6.648475170135498
Epoch 630, val loss: 1.1905202865600586
Epoch 640, training loss: 0.07640895992517471 = 0.009886997751891613 + 0.01 * 6.652196884155273
Epoch 640, val loss: 1.1988545656204224
Epoch 650, training loss: 0.07582547515630722 = 0.009374091401696205 + 0.01 * 6.645138740539551
Epoch 650, val loss: 1.2070214748382568
Epoch 660, training loss: 0.07530762255191803 = 0.008902686648070812 + 0.01 * 6.640493869781494
Epoch 660, val loss: 1.214874029159546
Epoch 670, training loss: 0.07491002231836319 = 0.00846845656633377 + 0.01 * 6.6441569328308105
Epoch 670, val loss: 1.2225290536880493
Epoch 680, training loss: 0.0744527280330658 = 0.008067430928349495 + 0.01 * 6.638530254364014
Epoch 680, val loss: 1.2300325632095337
Epoch 690, training loss: 0.07404422760009766 = 0.007696493994444609 + 0.01 * 6.6347737312316895
Epoch 690, val loss: 1.23726224899292
Epoch 700, training loss: 0.07368828356266022 = 0.007352849002927542 + 0.01 * 6.633543491363525
Epoch 700, val loss: 1.2443335056304932
Epoch 710, training loss: 0.07335269451141357 = 0.007033715024590492 + 0.01 * 6.631898403167725
Epoch 710, val loss: 1.251222848892212
Epoch 720, training loss: 0.07301616668701172 = 0.006736997049301863 + 0.01 * 6.627917766571045
Epoch 720, val loss: 1.2579230070114136
Epoch 730, training loss: 0.07274578511714935 = 0.006460650358349085 + 0.01 * 6.628513336181641
Epoch 730, val loss: 1.2644689083099365
Epoch 740, training loss: 0.0724804624915123 = 0.006202733609825373 + 0.01 * 6.627772808074951
Epoch 740, val loss: 1.2708971500396729
Epoch 750, training loss: 0.0721694752573967 = 0.005961666814982891 + 0.01 * 6.620781421661377
Epoch 750, val loss: 1.277132511138916
Epoch 760, training loss: 0.07204513996839523 = 0.005735982675105333 + 0.01 * 6.630915641784668
Epoch 760, val loss: 1.2831999063491821
Epoch 770, training loss: 0.07174310833215714 = 0.005524457432329655 + 0.01 * 6.621865272521973
Epoch 770, val loss: 1.2892186641693115
Epoch 780, training loss: 0.07147254049777985 = 0.005325954407453537 + 0.01 * 6.614659309387207
Epoch 780, val loss: 1.2949894666671753
Epoch 790, training loss: 0.07131250202655792 = 0.005139286629855633 + 0.01 * 6.617321968078613
Epoch 790, val loss: 1.3006677627563477
Epoch 800, training loss: 0.07105284184217453 = 0.004963606130331755 + 0.01 * 6.608923435211182
Epoch 800, val loss: 1.3062294721603394
Epoch 810, training loss: 0.07088964432477951 = 0.004797942470759153 + 0.01 * 6.609170436859131
Epoch 810, val loss: 1.311644434928894
Epoch 820, training loss: 0.07106149196624756 = 0.00464156037196517 + 0.01 * 6.641993045806885
Epoch 820, val loss: 1.316948413848877
Epoch 830, training loss: 0.0706237182021141 = 0.0044942633248865604 + 0.01 * 6.612945556640625
Epoch 830, val loss: 1.322156310081482
Epoch 840, training loss: 0.07034467905759811 = 0.004355071112513542 + 0.01 * 6.5989603996276855
Epoch 840, val loss: 1.3271684646606445
Epoch 850, training loss: 0.0701860561966896 = 0.004223235882818699 + 0.01 * 6.596282482147217
Epoch 850, val loss: 1.3321442604064941
Epoch 860, training loss: 0.07005925476551056 = 0.004098145756870508 + 0.01 * 6.596110820770264
Epoch 860, val loss: 1.3369855880737305
Epoch 870, training loss: 0.06993982195854187 = 0.003979342523962259 + 0.01 * 6.596048355102539
Epoch 870, val loss: 1.3417936563491821
Epoch 880, training loss: 0.0697941929101944 = 0.0038665931206196547 + 0.01 * 6.5927605628967285
Epoch 880, val loss: 1.346409559249878
Epoch 890, training loss: 0.06972495466470718 = 0.0037595448084175587 + 0.01 * 6.596540927886963
Epoch 890, val loss: 1.3509405851364136
Epoch 900, training loss: 0.06957581639289856 = 0.0036577414721250534 + 0.01 * 6.5918073654174805
Epoch 900, val loss: 1.355457067489624
Epoch 910, training loss: 0.06940034031867981 = 0.003560936078429222 + 0.01 * 6.583940029144287
Epoch 910, val loss: 1.359763264656067
Epoch 920, training loss: 0.0692390576004982 = 0.003468647599220276 + 0.01 * 6.577041149139404
Epoch 920, val loss: 1.3640474081039429
Epoch 930, training loss: 0.06913897395133972 = 0.003380581270903349 + 0.01 * 6.575839042663574
Epoch 930, val loss: 1.3682522773742676
Epoch 940, training loss: 0.0690724104642868 = 0.00329661276191473 + 0.01 * 6.577579498291016
Epoch 940, val loss: 1.372353196144104
Epoch 950, training loss: 0.06890697032213211 = 0.003216393990442157 + 0.01 * 6.569057941436768
Epoch 950, val loss: 1.3763551712036133
Epoch 960, training loss: 0.0688273087143898 = 0.0031397812999784946 + 0.01 * 6.568753242492676
Epoch 960, val loss: 1.3803181648254395
Epoch 970, training loss: 0.06877182424068451 = 0.003066523466259241 + 0.01 * 6.570530414581299
Epoch 970, val loss: 1.3840744495391846
Epoch 980, training loss: 0.06863906979560852 = 0.0029965590219944715 + 0.01 * 6.56425142288208
Epoch 980, val loss: 1.387844443321228
Epoch 990, training loss: 0.06866051256656647 = 0.002929584588855505 + 0.01 * 6.573092937469482
Epoch 990, val loss: 1.3915587663650513
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7519
Overall ASR: 0.8413
Flip ASR: 0.8089/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0154500007629395 = 1.9317126274108887 + 0.01 * 8.37374496459961
Epoch 0, val loss: 1.936219573020935
Epoch 10, training loss: 2.0066633224487305 = 1.922926664352417 + 0.01 * 8.373663902282715
Epoch 10, val loss: 1.9278756380081177
Epoch 20, training loss: 1.9963637590408325 = 1.9126313924789429 + 0.01 * 8.373231887817383
Epoch 20, val loss: 1.9177589416503906
Epoch 30, training loss: 1.9820772409439087 = 1.8983583450317383 + 0.01 * 8.371888160705566
Epoch 30, val loss: 1.9034191370010376
Epoch 40, training loss: 1.9608328342437744 = 1.8771730661392212 + 0.01 * 8.365983009338379
Epoch 40, val loss: 1.8820738792419434
Epoch 50, training loss: 1.9294817447662354 = 1.8462228775024414 + 0.01 * 8.325882911682129
Epoch 50, val loss: 1.851369023323059
Epoch 60, training loss: 1.8854546546936035 = 1.8054169416427612 + 0.01 * 8.003768920898438
Epoch 60, val loss: 1.812109112739563
Epoch 70, training loss: 1.8332270383834839 = 1.758834719657898 + 0.01 * 7.4392290115356445
Epoch 70, val loss: 1.767640471458435
Epoch 80, training loss: 1.7765636444091797 = 1.7047457695007324 + 0.01 * 7.181788444519043
Epoch 80, val loss: 1.7154580354690552
Epoch 90, training loss: 1.7046356201171875 = 1.634061336517334 + 0.01 * 7.057429790496826
Epoch 90, val loss: 1.6511099338531494
Epoch 100, training loss: 1.6125454902648926 = 1.5429147481918335 + 0.01 * 6.963077545166016
Epoch 100, val loss: 1.5746923685073853
Epoch 110, training loss: 1.5032480955123901 = 1.434031367301941 + 0.01 * 6.9216766357421875
Epoch 110, val loss: 1.4850221872329712
Epoch 120, training loss: 1.3837194442749023 = 1.3145970106124878 + 0.01 * 6.912242889404297
Epoch 120, val loss: 1.3866223096847534
Epoch 130, training loss: 1.2622665166854858 = 1.193232774734497 + 0.01 * 6.903369426727295
Epoch 130, val loss: 1.28800368309021
Epoch 140, training loss: 1.1453014612197876 = 1.0763344764709473 + 0.01 * 6.896698474884033
Epoch 140, val loss: 1.1945326328277588
Epoch 150, training loss: 1.0352007150650024 = 0.9663004279136658 + 0.01 * 6.8900299072265625
Epoch 150, val loss: 1.107732892036438
Epoch 160, training loss: 0.9322439432144165 = 0.8634053468704224 + 0.01 * 6.883862495422363
Epoch 160, val loss: 1.026887059211731
Epoch 170, training loss: 0.8365105986595154 = 0.7677276730537415 + 0.01 * 6.878293991088867
Epoch 170, val loss: 0.9519872665405273
Epoch 180, training loss: 0.7489308714866638 = 0.6801910400390625 + 0.01 * 6.87398099899292
Epoch 180, val loss: 0.8840704560279846
Epoch 190, training loss: 0.6704866886138916 = 0.6017806529998779 + 0.01 * 6.870604991912842
Epoch 190, val loss: 0.8251652121543884
Epoch 200, training loss: 0.6011338829994202 = 0.53245609998703 + 0.01 * 6.867778301239014
Epoch 200, val loss: 0.7760108113288879
Epoch 210, training loss: 0.5398736000061035 = 0.4712219834327698 + 0.01 * 6.8651604652404785
Epoch 210, val loss: 0.7360350489616394
Epoch 220, training loss: 0.4852561950683594 = 0.416630357503891 + 0.01 * 6.862585067749023
Epoch 220, val loss: 0.7040858268737793
Epoch 230, training loss: 0.435968279838562 = 0.3673679828643799 + 0.01 * 6.8600311279296875
Epoch 230, val loss: 0.6788063049316406
Epoch 240, training loss: 0.39121121168136597 = 0.3226373791694641 + 0.01 * 6.857382774353027
Epoch 240, val loss: 0.6589543223381042
Epoch 250, training loss: 0.35049182176589966 = 0.28194668889045715 + 0.01 * 6.854512691497803
Epoch 250, val loss: 0.6435122489929199
Epoch 260, training loss: 0.313686341047287 = 0.24517442286014557 + 0.01 * 6.851191997528076
Epoch 260, val loss: 0.6326369643211365
Epoch 270, training loss: 0.28082987666130066 = 0.21235859394073486 + 0.01 * 6.847127914428711
Epoch 270, val loss: 0.6256875991821289
Epoch 280, training loss: 0.2519598603248596 = 0.18354174494743347 + 0.01 * 6.8418121337890625
Epoch 280, val loss: 0.6225278377532959
Epoch 290, training loss: 0.22697392106056213 = 0.1586303412914276 + 0.01 * 6.834357261657715
Epoch 290, val loss: 0.6228712797164917
Epoch 300, training loss: 0.20563678443431854 = 0.13740281760692596 + 0.01 * 6.823396682739258
Epoch 300, val loss: 0.6263180375099182
Epoch 310, training loss: 0.18758250772953033 = 0.11947309225797653 + 0.01 * 6.810941696166992
Epoch 310, val loss: 0.6323862671852112
Epoch 320, training loss: 0.17234832048416138 = 0.10439357906579971 + 0.01 * 6.795475006103516
Epoch 320, val loss: 0.6408116221427917
Epoch 330, training loss: 0.15950800478458405 = 0.09171172976493835 + 0.01 * 6.779627323150635
Epoch 330, val loss: 0.6510528326034546
Epoch 340, training loss: 0.14867731928825378 = 0.08100371062755585 + 0.01 * 6.767362117767334
Epoch 340, val loss: 0.6627123355865479
Epoch 350, training loss: 0.1395195573568344 = 0.07190288603305817 + 0.01 * 6.761667251586914
Epoch 350, val loss: 0.6754931807518005
Epoch 360, training loss: 0.13169917464256287 = 0.06412754207849503 + 0.01 * 6.757164478302002
Epoch 360, val loss: 0.689042866230011
Epoch 370, training loss: 0.12497344613075256 = 0.05743638053536415 + 0.01 * 6.753705978393555
Epoch 370, val loss: 0.7032753229141235
Epoch 380, training loss: 0.1191406399011612 = 0.051643285900354385 + 0.01 * 6.7497358322143555
Epoch 380, val loss: 0.717854917049408
Epoch 390, training loss: 0.11406314373016357 = 0.046597231179475784 + 0.01 * 6.746591091156006
Epoch 390, val loss: 0.7327514886856079
Epoch 400, training loss: 0.10962668061256409 = 0.04218200966715813 + 0.01 * 6.744466781616211
Epoch 400, val loss: 0.7477943897247314
Epoch 410, training loss: 0.10570196807384491 = 0.03830531984567642 + 0.01 * 6.7396650314331055
Epoch 410, val loss: 0.7628505825996399
Epoch 420, training loss: 0.1022462546825409 = 0.034890275448560715 + 0.01 * 6.735598564147949
Epoch 420, val loss: 0.777829647064209
Epoch 430, training loss: 0.09920021891593933 = 0.03187359496951103 + 0.01 * 6.732663154602051
Epoch 430, val loss: 0.7927238345146179
Epoch 440, training loss: 0.0964885726571083 = 0.029202699661254883 + 0.01 * 6.728587627410889
Epoch 440, val loss: 0.8073686957359314
Epoch 450, training loss: 0.09411043673753738 = 0.026832083240151405 + 0.01 * 6.727835655212402
Epoch 450, val loss: 0.8217265009880066
Epoch 460, training loss: 0.09191527962684631 = 0.024723868817090988 + 0.01 * 6.719141960144043
Epoch 460, val loss: 0.8357658386230469
Epoch 470, training loss: 0.09001951664686203 = 0.022843576967716217 + 0.01 * 6.717594146728516
Epoch 470, val loss: 0.8494120240211487
Epoch 480, training loss: 0.08825335651636124 = 0.021162061020731926 + 0.01 * 6.709129333496094
Epoch 480, val loss: 0.8627328276634216
Epoch 490, training loss: 0.0866788774728775 = 0.019651567563414574 + 0.01 * 6.702730655670166
Epoch 490, val loss: 0.8757104873657227
Epoch 500, training loss: 0.08529528975486755 = 0.01828966662287712 + 0.01 * 6.700562477111816
Epoch 500, val loss: 0.8883766531944275
Epoch 510, training loss: 0.08401298522949219 = 0.017057139426469803 + 0.01 * 6.695584774017334
Epoch 510, val loss: 0.900700569152832
Epoch 520, training loss: 0.08280893415212631 = 0.01593739725649357 + 0.01 * 6.6871538162231445
Epoch 520, val loss: 0.9127868413925171
Epoch 530, training loss: 0.08172054588794708 = 0.014917331747710705 + 0.01 * 6.68032169342041
Epoch 530, val loss: 0.9245902895927429
Epoch 540, training loss: 0.08082805573940277 = 0.01398810837417841 + 0.01 * 6.683994770050049
Epoch 540, val loss: 0.936033308506012
Epoch 550, training loss: 0.07987582683563232 = 0.013143157586455345 + 0.01 * 6.673266887664795
Epoch 550, val loss: 0.9471936225891113
Epoch 560, training loss: 0.0790218934416771 = 0.012370914220809937 + 0.01 * 6.665098190307617
Epoch 560, val loss: 0.9580296277999878
Epoch 570, training loss: 0.07827013731002808 = 0.011663188226521015 + 0.01 * 6.6606950759887695
Epoch 570, val loss: 0.9686328172683716
Epoch 580, training loss: 0.07768131792545319 = 0.011013759300112724 + 0.01 * 6.666755676269531
Epoch 580, val loss: 0.9790137410163879
Epoch 590, training loss: 0.0769243836402893 = 0.010417076759040356 + 0.01 * 6.650731086730957
Epoch 590, val loss: 0.9891330003738403
Epoch 600, training loss: 0.07634656131267548 = 0.009867513552308083 + 0.01 * 6.647904872894287
Epoch 600, val loss: 0.9990119338035583
Epoch 610, training loss: 0.07590946555137634 = 0.009360605850815773 + 0.01 * 6.654886245727539
Epoch 610, val loss: 1.008692741394043
Epoch 620, training loss: 0.07532096654176712 = 0.008892511017620564 + 0.01 * 6.642845630645752
Epoch 620, val loss: 1.0180413722991943
Epoch 630, training loss: 0.07484584301710129 = 0.008458965457975864 + 0.01 * 6.638688087463379
Epoch 630, val loss: 1.027261734008789
Epoch 640, training loss: 0.07447437942028046 = 0.008058171719312668 + 0.01 * 6.641620635986328
Epoch 640, val loss: 1.0362374782562256
Epoch 650, training loss: 0.07396285235881805 = 0.007686271797865629 + 0.01 * 6.627657890319824
Epoch 650, val loss: 1.0449265241622925
Epoch 660, training loss: 0.0736110731959343 = 0.007340567652136087 + 0.01 * 6.627050876617432
Epoch 660, val loss: 1.053443193435669
Epoch 670, training loss: 0.07318878918886185 = 0.007019371725618839 + 0.01 * 6.616941928863525
Epoch 670, val loss: 1.0617908239364624
Epoch 680, training loss: 0.0731172263622284 = 0.006719748489558697 + 0.01 * 6.639748573303223
Epoch 680, val loss: 1.069927453994751
Epoch 690, training loss: 0.07263948768377304 = 0.006440607365220785 + 0.01 * 6.6198883056640625
Epoch 690, val loss: 1.07785165309906
Epoch 700, training loss: 0.07223328948020935 = 0.00618010712787509 + 0.01 * 6.605318546295166
Epoch 700, val loss: 1.0856552124023438
Epoch 710, training loss: 0.07196273654699326 = 0.005935812368988991 + 0.01 * 6.602692604064941
Epoch 710, val loss: 1.0932741165161133
Epoch 720, training loss: 0.07173381745815277 = 0.00570710189640522 + 0.01 * 6.6026716232299805
Epoch 720, val loss: 1.1006759405136108
Epoch 730, training loss: 0.07149623334407806 = 0.0054925247095525265 + 0.01 * 6.60037088394165
Epoch 730, val loss: 1.1080037355422974
Epoch 740, training loss: 0.07119923830032349 = 0.005290884058922529 + 0.01 * 6.590836048126221
Epoch 740, val loss: 1.1150846481323242
Epoch 750, training loss: 0.07104699313640594 = 0.005101440940052271 + 0.01 * 6.594555377960205
Epoch 750, val loss: 1.1221116781234741
Epoch 760, training loss: 0.07079462707042694 = 0.004923358093947172 + 0.01 * 6.587127208709717
Epoch 760, val loss: 1.1289002895355225
Epoch 770, training loss: 0.0707123875617981 = 0.0047554983757436275 + 0.01 * 6.595689296722412
Epoch 770, val loss: 1.1355104446411133
Epoch 780, training loss: 0.07044069468975067 = 0.004597546067088842 + 0.01 * 6.584314823150635
Epoch 780, val loss: 1.1420789957046509
Epoch 790, training loss: 0.07023513317108154 = 0.004448211286216974 + 0.01 * 6.5786919593811035
Epoch 790, val loss: 1.1483405828475952
Epoch 800, training loss: 0.07011934369802475 = 0.004307067487388849 + 0.01 * 6.5812273025512695
Epoch 800, val loss: 1.1546045541763306
Epoch 810, training loss: 0.06987188011407852 = 0.004173703026026487 + 0.01 * 6.569817543029785
Epoch 810, val loss: 1.1607012748718262
Epoch 820, training loss: 0.06980517506599426 = 0.004047010093927383 + 0.01 * 6.575817108154297
Epoch 820, val loss: 1.166674256324768
Epoch 830, training loss: 0.069611556828022 = 0.003927314188331366 + 0.01 * 6.568424224853516
Epoch 830, val loss: 1.1723809242248535
Epoch 840, training loss: 0.0695452094078064 = 0.0038136846851557493 + 0.01 * 6.573153018951416
Epoch 840, val loss: 1.1781466007232666
Epoch 850, training loss: 0.06930382549762726 = 0.0037062251940369606 + 0.01 * 6.559760093688965
Epoch 850, val loss: 1.1836040019989014
Epoch 860, training loss: 0.06908766180276871 = 0.003604097058996558 + 0.01 * 6.548356056213379
Epoch 860, val loss: 1.189042329788208
Epoch 870, training loss: 0.06902287900447845 = 0.0035070083104074 + 0.01 * 6.551586627960205
Epoch 870, val loss: 1.194365382194519
Epoch 880, training loss: 0.0688166618347168 = 0.003414537524804473 + 0.01 * 6.540213108062744
Epoch 880, val loss: 1.1996060609817505
Epoch 890, training loss: 0.06890598684549332 = 0.003326418111100793 + 0.01 * 6.557957172393799
Epoch 890, val loss: 1.2047027349472046
Epoch 900, training loss: 0.06864096224308014 = 0.0032426759134978056 + 0.01 * 6.539828300476074
Epoch 900, val loss: 1.2097222805023193
Epoch 910, training loss: 0.06847993284463882 = 0.003162776120007038 + 0.01 * 6.5317158699035645
Epoch 910, val loss: 1.2146224975585938
Epoch 920, training loss: 0.06845062226057053 = 0.003086530137807131 + 0.01 * 6.536409378051758
Epoch 920, val loss: 1.219403862953186
Epoch 930, training loss: 0.06840191781520844 = 0.003013729117810726 + 0.01 * 6.538818836212158
Epoch 930, val loss: 1.224177360534668
Epoch 940, training loss: 0.06817774474620819 = 0.0029440857470035553 + 0.01 * 6.5233659744262695
Epoch 940, val loss: 1.2287226915359497
Epoch 950, training loss: 0.06813683360815048 = 0.0028776165563613176 + 0.01 * 6.5259222984313965
Epoch 950, val loss: 1.2332687377929688
Epoch 960, training loss: 0.06809957325458527 = 0.002814191160723567 + 0.01 * 6.528538703918457
Epoch 960, val loss: 1.2377047538757324
Epoch 970, training loss: 0.06782941520214081 = 0.0027534274850040674 + 0.01 * 6.507598400115967
Epoch 970, val loss: 1.241982340812683
Epoch 980, training loss: 0.06800869852304459 = 0.0026952459011226892 + 0.01 * 6.531345367431641
Epoch 980, val loss: 1.2462248802185059
Epoch 990, training loss: 0.06763038039207458 = 0.0026394312735646963 + 0.01 * 6.499095439910889
Epoch 990, val loss: 1.2504007816314697
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.7491
Flip ASR: 0.7111/225 nodes
The final ASR:0.72694, 0.10363, Accuracy:0.79383, 0.03205
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11618])
remove edge: torch.Size([2, 9498])
updated graph: torch.Size([2, 10560])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
The final ASR:0.97417, 0.00301, Accuracy:0.83457, 0.00761
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.4 der2:0.4 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0377769470214844 = 1.9540382623672485 + 0.01 * 8.373860359191895
Epoch 0, val loss: 1.9597927331924438
Epoch 10, training loss: 2.027198553085327 = 1.9434608221054077 + 0.01 * 8.373773574829102
Epoch 10, val loss: 1.948954701423645
Epoch 20, training loss: 2.0139849185943604 = 1.930249571800232 + 0.01 * 8.373534202575684
Epoch 20, val loss: 1.9351445436477661
Epoch 30, training loss: 1.9950820207595825 = 1.9113534688949585 + 0.01 * 8.372854232788086
Epoch 30, val loss: 1.9152072668075562
Epoch 40, training loss: 1.9667385816574097 = 1.8830400705337524 + 0.01 * 8.369853019714355
Epoch 40, val loss: 1.885527491569519
Epoch 50, training loss: 1.9269341230392456 = 1.8434230089187622 + 0.01 * 8.351113319396973
Epoch 50, val loss: 1.8455535173416138
Epoch 60, training loss: 1.8809629678726196 = 1.7984051704406738 + 0.01 * 8.255776405334473
Epoch 60, val loss: 1.8043068647384644
Epoch 70, training loss: 1.8355270624160767 = 1.7554552555084229 + 0.01 * 8.007176399230957
Epoch 70, val loss: 1.7683417797088623
Epoch 80, training loss: 1.777254343032837 = 1.7007520198822021 + 0.01 * 7.650227069854736
Epoch 80, val loss: 1.7209007740020752
Epoch 90, training loss: 1.7008219957351685 = 1.626960039138794 + 0.01 * 7.38619327545166
Epoch 90, val loss: 1.6559016704559326
Epoch 100, training loss: 1.6083451509475708 = 1.535343885421753 + 0.01 * 7.3001275062561035
Epoch 100, val loss: 1.576570749282837
Epoch 110, training loss: 1.5103427171707153 = 1.4375540018081665 + 0.01 * 7.278870582580566
Epoch 110, val loss: 1.4944056272506714
Epoch 120, training loss: 1.4136682748794556 = 1.3413594961166382 + 0.01 * 7.230882167816162
Epoch 120, val loss: 1.418375849723816
Epoch 130, training loss: 1.3168628215789795 = 1.2452082633972168 + 0.01 * 7.165452003479004
Epoch 130, val loss: 1.3456568717956543
Epoch 140, training loss: 1.2178112268447876 = 1.1468687057495117 + 0.01 * 7.094255447387695
Epoch 140, val loss: 1.2736117839813232
Epoch 150, training loss: 1.1193124055862427 = 1.0488277673721313 + 0.01 * 7.048466682434082
Epoch 150, val loss: 1.2020620107650757
Epoch 160, training loss: 1.0249969959259033 = 0.9547852277755737 + 0.01 * 7.021180629730225
Epoch 160, val loss: 1.1331048011779785
Epoch 170, training loss: 0.9365752935409546 = 0.8666224479675293 + 0.01 * 6.995284557342529
Epoch 170, val loss: 1.068137764930725
Epoch 180, training loss: 0.8540974259376526 = 0.7843860387802124 + 0.01 * 6.971137523651123
Epoch 180, val loss: 1.0085636377334595
Epoch 190, training loss: 0.7772706747055054 = 0.7077142596244812 + 0.01 * 6.955638408660889
Epoch 190, val loss: 0.9552454352378845
Epoch 200, training loss: 0.7063955068588257 = 0.6369596123695374 + 0.01 * 6.943592548370361
Epoch 200, val loss: 0.9085696935653687
Epoch 210, training loss: 0.6422828435897827 = 0.5729213356971741 + 0.01 * 6.936150550842285
Epoch 210, val loss: 0.8690657019615173
Epoch 220, training loss: 0.5852537155151367 = 0.5159547328948975 + 0.01 * 6.929901599884033
Epoch 220, val loss: 0.8371794819831848
Epoch 230, training loss: 0.534919261932373 = 0.4656735360622406 + 0.01 * 6.9245758056640625
Epoch 230, val loss: 0.8126302361488342
Epoch 240, training loss: 0.4906393885612488 = 0.4214381277561188 + 0.01 * 6.920124530792236
Epoch 240, val loss: 0.7944967150688171
Epoch 250, training loss: 0.4515354335308075 = 0.38237088918685913 + 0.01 * 6.916455268859863
Epoch 250, val loss: 0.7815229296684265
Epoch 260, training loss: 0.4164745807647705 = 0.34732624888420105 + 0.01 * 6.914831638336182
Epoch 260, val loss: 0.772451639175415
Epoch 270, training loss: 0.3841940462589264 = 0.3150854706764221 + 0.01 * 6.910858154296875
Epoch 270, val loss: 0.7660303115844727
Epoch 280, training loss: 0.35375669598579407 = 0.2846677601337433 + 0.01 * 6.908892631530762
Epoch 280, val loss: 0.7617605924606323
Epoch 290, training loss: 0.32461899518966675 = 0.2555558979511261 + 0.01 * 6.906309604644775
Epoch 290, val loss: 0.7592876553535461
Epoch 300, training loss: 0.2968176603317261 = 0.22778065502643585 + 0.01 * 6.9037017822265625
Epoch 300, val loss: 0.7586184144020081
Epoch 310, training loss: 0.2707677483558655 = 0.2017631083726883 + 0.01 * 6.900463581085205
Epoch 310, val loss: 0.7598496675491333
Epoch 320, training loss: 0.24697649478912354 = 0.1780194193124771 + 0.01 * 6.895706653594971
Epoch 320, val loss: 0.7633540034294128
Epoch 330, training loss: 0.22581073641777039 = 0.15688395500183105 + 0.01 * 6.892678737640381
Epoch 330, val loss: 0.7693804502487183
Epoch 340, training loss: 0.20729628205299377 = 0.13843664526939392 + 0.01 * 6.885963439941406
Epoch 340, val loss: 0.778076708316803
Epoch 350, training loss: 0.19128982722759247 = 0.12250806391239166 + 0.01 * 6.878176689147949
Epoch 350, val loss: 0.7892183661460876
Epoch 360, training loss: 0.1775817722082138 = 0.10880101472139359 + 0.01 * 6.87807559967041
Epoch 360, val loss: 0.8026220202445984
Epoch 370, training loss: 0.1656333953142166 = 0.09698697179555893 + 0.01 * 6.864642143249512
Epoch 370, val loss: 0.8178892135620117
Epoch 380, training loss: 0.15542495250701904 = 0.0867575854063034 + 0.01 * 6.866735935211182
Epoch 380, val loss: 0.8346363306045532
Epoch 390, training loss: 0.14633381366729736 = 0.07785572111606598 + 0.01 * 6.847809314727783
Epoch 390, val loss: 0.8524883389472961
Epoch 400, training loss: 0.1385376751422882 = 0.07007284462451935 + 0.01 * 6.846482753753662
Epoch 400, val loss: 0.8711145520210266
Epoch 410, training loss: 0.13161858916282654 = 0.06324619799852371 + 0.01 * 6.837238788604736
Epoch 410, val loss: 0.8902186751365662
Epoch 420, training loss: 0.12548913061618805 = 0.05724038556218147 + 0.01 * 6.824873924255371
Epoch 420, val loss: 0.9095672369003296
Epoch 430, training loss: 0.12007273733615875 = 0.05194832757115364 + 0.01 * 6.812441349029541
Epoch 430, val loss: 0.9288913011550903
Epoch 440, training loss: 0.11540982127189636 = 0.04727538675069809 + 0.01 * 6.813443660736084
Epoch 440, val loss: 0.9479902386665344
Epoch 450, training loss: 0.11124351620674133 = 0.04314302280545235 + 0.01 * 6.810050010681152
Epoch 450, val loss: 0.9667038321495056
Epoch 460, training loss: 0.10748301446437836 = 0.03948195278644562 + 0.01 * 6.800106048583984
Epoch 460, val loss: 0.9849914312362671
Epoch 470, training loss: 0.10414130985736847 = 0.03623076155781746 + 0.01 * 6.7910542488098145
Epoch 470, val loss: 1.002769947052002
Epoch 480, training loss: 0.1011502742767334 = 0.0333351232111454 + 0.01 * 6.781515121459961
Epoch 480, val loss: 1.020068645477295
Epoch 490, training loss: 0.09860488027334213 = 0.030750593170523643 + 0.01 * 6.785428524017334
Epoch 490, val loss: 1.0368386507034302
Epoch 500, training loss: 0.09619801491498947 = 0.028440363705158234 + 0.01 * 6.775765419006348
Epoch 500, val loss: 1.0530680418014526
Epoch 510, training loss: 0.09401658922433853 = 0.02636590413749218 + 0.01 * 6.765068531036377
Epoch 510, val loss: 1.0688107013702393
Epoch 520, training loss: 0.09210361540317535 = 0.02449432574212551 + 0.01 * 6.760929107666016
Epoch 520, val loss: 1.0840717554092407
Epoch 530, training loss: 0.09032829105854034 = 0.022798005491495132 + 0.01 * 6.7530293464660645
Epoch 530, val loss: 1.0988138914108276
Epoch 540, training loss: 0.08871262520551682 = 0.021255239844322205 + 0.01 * 6.745738983154297
Epoch 540, val loss: 1.1131788492202759
Epoch 550, training loss: 0.08730082958936691 = 0.0198512002825737 + 0.01 * 6.7449631690979
Epoch 550, val loss: 1.1271950006484985
Epoch 560, training loss: 0.08584313094615936 = 0.018572170287370682 + 0.01 * 6.727096080780029
Epoch 560, val loss: 1.1407647132873535
Epoch 570, training loss: 0.08471397310495377 = 0.01740471087396145 + 0.01 * 6.730926513671875
Epoch 570, val loss: 1.1540658473968506
Epoch 580, training loss: 0.08356048911809921 = 0.01633869856595993 + 0.01 * 6.722178936004639
Epoch 580, val loss: 1.1669303178787231
Epoch 590, training loss: 0.08247487246990204 = 0.015363523736596107 + 0.01 * 6.711134910583496
Epoch 590, val loss: 1.1794397830963135
Epoch 600, training loss: 0.08148784190416336 = 0.014469677582383156 + 0.01 * 6.701816558837891
Epoch 600, val loss: 1.1916323900222778
Epoch 610, training loss: 0.08080492168664932 = 0.013650682754814625 + 0.01 * 6.715424060821533
Epoch 610, val loss: 1.2034140825271606
Epoch 620, training loss: 0.0799393504858017 = 0.012899558991193771 + 0.01 * 6.7039794921875
Epoch 620, val loss: 1.2148226499557495
Epoch 630, training loss: 0.07909494638442993 = 0.012208039872348309 + 0.01 * 6.688691139221191
Epoch 630, val loss: 1.2259896993637085
Epoch 640, training loss: 0.07834610342979431 = 0.011569847352802753 + 0.01 * 6.67762565612793
Epoch 640, val loss: 1.236872911453247
Epoch 650, training loss: 0.07806649804115295 = 0.010980242863297462 + 0.01 * 6.708625793457031
Epoch 650, val loss: 1.247385025024414
Epoch 660, training loss: 0.07713620364665985 = 0.0104356799274683 + 0.01 * 6.670052528381348
Epoch 660, val loss: 1.2576311826705933
Epoch 670, training loss: 0.07666611671447754 = 0.009931539185345173 + 0.01 * 6.673457622528076
Epoch 670, val loss: 1.2675678730010986
Epoch 680, training loss: 0.07607010006904602 = 0.009463664144277573 + 0.01 * 6.660644054412842
Epoch 680, val loss: 1.2772161960601807
Epoch 690, training loss: 0.07556210458278656 = 0.009028912521898746 + 0.01 * 6.653319358825684
Epoch 690, val loss: 1.2866237163543701
Epoch 700, training loss: 0.07517212629318237 = 0.008624258451163769 + 0.01 * 6.654787063598633
Epoch 700, val loss: 1.2958123683929443
Epoch 710, training loss: 0.07469457387924194 = 0.008247344754636288 + 0.01 * 6.644722938537598
Epoch 710, val loss: 1.3047306537628174
Epoch 720, training loss: 0.07433150708675385 = 0.00789511390030384 + 0.01 * 6.64363956451416
Epoch 720, val loss: 1.3134362697601318
Epoch 730, training loss: 0.0739278495311737 = 0.007566163782030344 + 0.01 * 6.636168956756592
Epoch 730, val loss: 1.3219009637832642
Epoch 740, training loss: 0.07363972812891006 = 0.007258413825184107 + 0.01 * 6.638131618499756
Epoch 740, val loss: 1.3301485776901245
Epoch 750, training loss: 0.07339097559452057 = 0.006969883106648922 + 0.01 * 6.642109394073486
Epoch 750, val loss: 1.338208794593811
Epoch 760, training loss: 0.07310859113931656 = 0.006699238438159227 + 0.01 * 6.640934944152832
Epoch 760, val loss: 1.346071720123291
Epoch 770, training loss: 0.07267329096794128 = 0.006445266306400299 + 0.01 * 6.622802257537842
Epoch 770, val loss: 1.3536865711212158
Epoch 780, training loss: 0.07237748056650162 = 0.006206329446285963 + 0.01 * 6.617115497589111
Epoch 780, val loss: 1.3611078262329102
Epoch 790, training loss: 0.072062648832798 = 0.005981168244034052 + 0.01 * 6.608148097991943
Epoch 790, val loss: 1.368415355682373
Epoch 800, training loss: 0.07188080251216888 = 0.0057688611559569836 + 0.01 * 6.611194610595703
Epoch 800, val loss: 1.375454306602478
Epoch 810, training loss: 0.07163066416978836 = 0.005568793974816799 + 0.01 * 6.606187343597412
Epoch 810, val loss: 1.3823984861373901
Epoch 820, training loss: 0.07142843306064606 = 0.0053799147717654705 + 0.01 * 6.604851722717285
Epoch 820, val loss: 1.3891218900680542
Epoch 830, training loss: 0.07120116800069809 = 0.005201148800551891 + 0.01 * 6.600002288818359
Epoch 830, val loss: 1.3956342935562134
Epoch 840, training loss: 0.07105868309736252 = 0.005032147280871868 + 0.01 * 6.602653980255127
Epoch 840, val loss: 1.4019403457641602
Epoch 850, training loss: 0.07082194089889526 = 0.0048718745820224285 + 0.01 * 6.595006942749023
Epoch 850, val loss: 1.4081370830535889
Epoch 860, training loss: 0.07062474638223648 = 0.004720105789601803 + 0.01 * 6.590464115142822
Epoch 860, val loss: 1.4141381978988647
Epoch 870, training loss: 0.07041510939598083 = 0.004575951956212521 + 0.01 * 6.583915710449219
Epoch 870, val loss: 1.4201043844223022
Epoch 880, training loss: 0.07052090018987656 = 0.004439042415469885 + 0.01 * 6.6081862449646
Epoch 880, val loss: 1.425828456878662
Epoch 890, training loss: 0.07012978941202164 = 0.004309424664825201 + 0.01 * 6.58203649520874
Epoch 890, val loss: 1.4314290285110474
Epoch 900, training loss: 0.06989237666130066 = 0.004185996484011412 + 0.01 * 6.570638179779053
Epoch 900, val loss: 1.4369257688522339
Epoch 910, training loss: 0.06973395496606827 = 0.004068407695740461 + 0.01 * 6.566555500030518
Epoch 910, val loss: 1.4422667026519775
Epoch 920, training loss: 0.06977298855781555 = 0.003956494387239218 + 0.01 * 6.581649303436279
Epoch 920, val loss: 1.4474916458129883
Epoch 930, training loss: 0.06945348531007767 = 0.0038496479392051697 + 0.01 * 6.5603837966918945
Epoch 930, val loss: 1.4526350498199463
Epoch 940, training loss: 0.06927033513784409 = 0.003747709561139345 + 0.01 * 6.552262783050537
Epoch 940, val loss: 1.4576066732406616
Epoch 950, training loss: 0.06930665671825409 = 0.003650237340480089 + 0.01 * 6.5656418800354
Epoch 950, val loss: 1.4624569416046143
Epoch 960, training loss: 0.06918041408061981 = 0.003557510208338499 + 0.01 * 6.562290191650391
Epoch 960, val loss: 1.4672383069992065
Epoch 970, training loss: 0.06906323879957199 = 0.0034686282742768526 + 0.01 * 6.55946159362793
Epoch 970, val loss: 1.4718871116638184
Epoch 980, training loss: 0.06886192411184311 = 0.003383610863238573 + 0.01 * 6.547831058502197
Epoch 980, val loss: 1.4764487743377686
Epoch 990, training loss: 0.06889373064041138 = 0.0033022789284586906 + 0.01 * 6.559145927429199
Epoch 990, val loss: 1.480875015258789
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.4945
Flip ASR: 0.3911/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0307843685150146 = 1.9470462799072266 + 0.01 * 8.373809814453125
Epoch 0, val loss: 1.94532310962677
Epoch 10, training loss: 2.020803451538086 = 1.9370663166046143 + 0.01 * 8.373724937438965
Epoch 10, val loss: 1.9357749223709106
Epoch 20, training loss: 2.0087594985961914 = 1.9250253438949585 + 0.01 * 8.373408317565918
Epoch 20, val loss: 1.9238152503967285
Epoch 30, training loss: 1.9919973611831665 = 1.9082720279693604 + 0.01 * 8.372528076171875
Epoch 30, val loss: 1.9067950248718262
Epoch 40, training loss: 1.9672024250030518 = 1.8835135698318481 + 0.01 * 8.368882179260254
Epoch 40, val loss: 1.8816391229629517
Epoch 50, training loss: 1.9316140413284302 = 1.848138451576233 + 0.01 * 8.347557067871094
Epoch 50, val loss: 1.846709132194519
Epoch 60, training loss: 1.8873376846313477 = 1.8051341772079468 + 0.01 * 8.220354080200195
Epoch 60, val loss: 1.8073718547821045
Epoch 70, training loss: 1.840978741645813 = 1.7630897760391235 + 0.01 * 7.7889018058776855
Epoch 70, val loss: 1.77195143699646
Epoch 80, training loss: 1.786571979522705 = 1.7134414911270142 + 0.01 * 7.31304931640625
Epoch 80, val loss: 1.728689432144165
Epoch 90, training loss: 1.7164723873138428 = 1.645372748374939 + 0.01 * 7.109969615936279
Epoch 90, val loss: 1.6706786155700684
Epoch 100, training loss: 1.6281965970993042 = 1.5579885244369507 + 0.01 * 7.020808219909668
Epoch 100, val loss: 1.5989137887954712
Epoch 110, training loss: 1.5280770063400269 = 1.4584952592849731 + 0.01 * 6.958169460296631
Epoch 110, val loss: 1.5162135362625122
Epoch 120, training loss: 1.4246740341186523 = 1.3554373979568481 + 0.01 * 6.9236602783203125
Epoch 120, val loss: 1.4323279857635498
Epoch 130, training loss: 1.3217753171920776 = 1.2527376413345337 + 0.01 * 6.903772354125977
Epoch 130, val loss: 1.3485342264175415
Epoch 140, training loss: 1.220449447631836 = 1.1515748500823975 + 0.01 * 6.887455940246582
Epoch 140, val loss: 1.2680562734603882
Epoch 150, training loss: 1.122573733329773 = 1.0538021326065063 + 0.01 * 6.877161979675293
Epoch 150, val loss: 1.1913416385650635
Epoch 160, training loss: 1.029163122177124 = 0.9604734778404236 + 0.01 * 6.8689703941345215
Epoch 160, val loss: 1.1192913055419922
Epoch 170, training loss: 0.9405307173728943 = 0.871893584728241 + 0.01 * 6.863711833953857
Epoch 170, val loss: 1.0513447523117065
Epoch 180, training loss: 0.8569686412811279 = 0.7884150743484497 + 0.01 * 6.8553595542907715
Epoch 180, val loss: 0.9883073568344116
Epoch 190, training loss: 0.7793807983398438 = 0.7109034657478333 + 0.01 * 6.847735404968262
Epoch 190, val loss: 0.9318891167640686
Epoch 200, training loss: 0.7090227007865906 = 0.6405760049819946 + 0.01 * 6.844669342041016
Epoch 200, val loss: 0.8841727375984192
Epoch 210, training loss: 0.6465743780136108 = 0.5782079696655273 + 0.01 * 6.836638450622559
Epoch 210, val loss: 0.846223771572113
Epoch 220, training loss: 0.5918640494346619 = 0.523581862449646 + 0.01 * 6.82821798324585
Epoch 220, val loss: 0.817619264125824
Epoch 230, training loss: 0.5438442230224609 = 0.47563865780830383 + 0.01 * 6.820558547973633
Epoch 230, val loss: 0.7968616485595703
Epoch 240, training loss: 0.500932514667511 = 0.432740718126297 + 0.01 * 6.819178581237793
Epoch 240, val loss: 0.7820029258728027
Epoch 250, training loss: 0.4613759517669678 = 0.3932684659957886 + 0.01 * 6.81074857711792
Epoch 250, val loss: 0.771382212638855
Epoch 260, training loss: 0.42417678236961365 = 0.35609790682792664 + 0.01 * 6.807888031005859
Epoch 260, val loss: 0.763918399810791
Epoch 270, training loss: 0.388753205537796 = 0.32070937752723694 + 0.01 * 6.804383754730225
Epoch 270, val loss: 0.7589859962463379
Epoch 280, training loss: 0.3550507426261902 = 0.28703978657722473 + 0.01 * 6.801094055175781
Epoch 280, val loss: 0.7560489177703857
Epoch 290, training loss: 0.32334548234939575 = 0.2553274929523468 + 0.01 * 6.801799774169922
Epoch 290, val loss: 0.7549744844436646
Epoch 300, training loss: 0.2938939034938812 = 0.2259114384651184 + 0.01 * 6.798247337341309
Epoch 300, val loss: 0.7558487057685852
Epoch 310, training loss: 0.2670692503452301 = 0.19909140467643738 + 0.01 * 6.797784328460693
Epoch 310, val loss: 0.758974552154541
Epoch 320, training loss: 0.2430434226989746 = 0.17506876587867737 + 0.01 * 6.797466278076172
Epoch 320, val loss: 0.7647206783294678
Epoch 330, training loss: 0.22176077961921692 = 0.1537998914718628 + 0.01 * 6.7960896492004395
Epoch 330, val loss: 0.7732809782028198
Epoch 340, training loss: 0.20305994153022766 = 0.13511040806770325 + 0.01 * 6.7949538230896
Epoch 340, val loss: 0.7845858931541443
Epoch 350, training loss: 0.18669329583644867 = 0.11875977367162704 + 0.01 * 6.7933526039123535
Epoch 350, val loss: 0.7982544302940369
Epoch 360, training loss: 0.1724233776330948 = 0.10449675470590591 + 0.01 * 6.792662143707275
Epoch 360, val loss: 0.8139548897743225
Epoch 370, training loss: 0.15997165441513062 = 0.09205049276351929 + 0.01 * 6.792116641998291
Epoch 370, val loss: 0.8312689661979675
Epoch 380, training loss: 0.14911216497421265 = 0.08120199292898178 + 0.01 * 6.791016578674316
Epoch 380, val loss: 0.8497505187988281
Epoch 390, training loss: 0.13963055610656738 = 0.07174596935510635 + 0.01 * 6.788459300994873
Epoch 390, val loss: 0.869103729724884
Epoch 400, training loss: 0.1313966065645218 = 0.06351224333047867 + 0.01 * 6.788436412811279
Epoch 400, val loss: 0.888983428478241
Epoch 410, training loss: 0.12421394139528275 = 0.05635778605937958 + 0.01 * 6.785615921020508
Epoch 410, val loss: 0.9091349840164185
Epoch 420, training loss: 0.11804068088531494 = 0.0501549206674099 + 0.01 * 6.788576602935791
Epoch 420, val loss: 0.9291888475418091
Epoch 430, training loss: 0.11261168867349625 = 0.04478941857814789 + 0.01 * 6.782227516174316
Epoch 430, val loss: 0.9488915205001831
Epoch 440, training loss: 0.10794172435998917 = 0.04014941304922104 + 0.01 * 6.779231548309326
Epoch 440, val loss: 0.9680928587913513
Epoch 450, training loss: 0.10403003543615341 = 0.03613390773534775 + 0.01 * 6.789613246917725
Epoch 450, val loss: 0.9866620302200317
Epoch 460, training loss: 0.10043378919363022 = 0.03265313804149628 + 0.01 * 6.778065204620361
Epoch 460, val loss: 1.0045520067214966
Epoch 470, training loss: 0.09735181927680969 = 0.029625043272972107 + 0.01 * 6.772677898406982
Epoch 470, val loss: 1.0217441320419312
Epoch 480, training loss: 0.09477156400680542 = 0.026982340961694717 + 0.01 * 6.7789225578308105
Epoch 480, val loss: 1.0382969379425049
Epoch 490, training loss: 0.09234294295310974 = 0.024670513346791267 + 0.01 * 6.767243385314941
Epoch 490, val loss: 1.054213523864746
Epoch 500, training loss: 0.09027543663978577 = 0.022637879475951195 + 0.01 * 6.7637553215026855
Epoch 500, val loss: 1.069556474685669
Epoch 510, training loss: 0.08844856172800064 = 0.02084277756512165 + 0.01 * 6.7605791091918945
Epoch 510, val loss: 1.0843658447265625
Epoch 520, training loss: 0.08691151440143585 = 0.019251510500907898 + 0.01 * 6.766000747680664
Epoch 520, val loss: 1.0986769199371338
Epoch 530, training loss: 0.08540316671133041 = 0.01783689670264721 + 0.01 * 6.756627082824707
Epoch 530, val loss: 1.112456202507019
Epoch 540, training loss: 0.08409405499696732 = 0.016574405133724213 + 0.01 * 6.751965045928955
Epoch 540, val loss: 1.1257665157318115
Epoch 550, training loss: 0.08305200934410095 = 0.015443462878465652 + 0.01 * 6.760855197906494
Epoch 550, val loss: 1.1386440992355347
Epoch 560, training loss: 0.08190568536520004 = 0.014428003691136837 + 0.01 * 6.747768402099609
Epoch 560, val loss: 1.1511056423187256
Epoch 570, training loss: 0.0809420645236969 = 0.01351234596222639 + 0.01 * 6.742972373962402
Epoch 570, val loss: 1.1631600856781006
Epoch 580, training loss: 0.08018596470355988 = 0.01268420647829771 + 0.01 * 6.750176429748535
Epoch 580, val loss: 1.1748731136322021
Epoch 590, training loss: 0.07931658625602722 = 0.011933363042771816 + 0.01 * 6.738322734832764
Epoch 590, val loss: 1.1861677169799805
Epoch 600, training loss: 0.07854712754487991 = 0.011250780895352364 + 0.01 * 6.729634761810303
Epoch 600, val loss: 1.1971428394317627
Epoch 610, training loss: 0.0780547484755516 = 0.010628077201545238 + 0.01 * 6.7426676750183105
Epoch 610, val loss: 1.2077769041061401
Epoch 620, training loss: 0.07729537785053253 = 0.010059438645839691 + 0.01 * 6.7235941886901855
Epoch 620, val loss: 1.2181036472320557
Epoch 630, training loss: 0.07678497582674026 = 0.009538248181343079 + 0.01 * 6.724673271179199
Epoch 630, val loss: 1.2280938625335693
Epoch 640, training loss: 0.07615744322538376 = 0.009059496223926544 + 0.01 * 6.709794521331787
Epoch 640, val loss: 1.2378040552139282
Epoch 650, training loss: 0.07606277614831924 = 0.008618412539362907 + 0.01 * 6.744436264038086
Epoch 650, val loss: 1.247253179550171
Epoch 660, training loss: 0.0753859281539917 = 0.008212591521441936 + 0.01 * 6.717333793640137
Epoch 660, val loss: 1.256402611732483
Epoch 670, training loss: 0.07490848749876022 = 0.007837441749870777 + 0.01 * 6.707104682922363
Epoch 670, val loss: 1.2653263807296753
Epoch 680, training loss: 0.0745108351111412 = 0.0074899946339428425 + 0.01 * 6.702084541320801
Epoch 680, val loss: 1.273966908454895
Epoch 690, training loss: 0.07405856996774673 = 0.007167529780417681 + 0.01 * 6.689103603363037
Epoch 690, val loss: 1.2823959589004517
Epoch 700, training loss: 0.0736948773264885 = 0.006867631338536739 + 0.01 * 6.682724952697754
Epoch 700, val loss: 1.2905852794647217
Epoch 710, training loss: 0.07351968437433243 = 0.006588553078472614 + 0.01 * 6.693113327026367
Epoch 710, val loss: 1.2985676527023315
Epoch 720, training loss: 0.07305366545915604 = 0.00632824981585145 + 0.01 * 6.672541618347168
Epoch 720, val loss: 1.3063029050827026
Epoch 730, training loss: 0.07299045473337173 = 0.006084928754717112 + 0.01 * 6.690552711486816
Epoch 730, val loss: 1.3138610124588013
Epoch 740, training loss: 0.07270534336566925 = 0.00585744297131896 + 0.01 * 6.684789657592773
Epoch 740, val loss: 1.321192979812622
Epoch 750, training loss: 0.07240080088376999 = 0.005644628312438726 + 0.01 * 6.675617694854736
Epoch 750, val loss: 1.3283767700195312
Epoch 760, training loss: 0.07207947224378586 = 0.005444906651973724 + 0.01 * 6.663456916809082
Epoch 760, val loss: 1.335290551185608
Epoch 770, training loss: 0.07172375172376633 = 0.0052572679705917835 + 0.01 * 6.646648406982422
Epoch 770, val loss: 1.34213125705719
Epoch 780, training loss: 0.07153049856424332 = 0.005080605391412973 + 0.01 * 6.644989490509033
Epoch 780, val loss: 1.3487507104873657
Epoch 790, training loss: 0.07140563428401947 = 0.00491417245939374 + 0.01 * 6.649146556854248
Epoch 790, val loss: 1.3552112579345703
Epoch 800, training loss: 0.07118794322013855 = 0.00475742481648922 + 0.01 * 6.643052101135254
Epoch 800, val loss: 1.3615206480026245
Epoch 810, training loss: 0.07121001183986664 = 0.004609300754964352 + 0.01 * 6.660070896148682
Epoch 810, val loss: 1.3677006959915161
Epoch 820, training loss: 0.0707436054944992 = 0.0044694701209664345 + 0.01 * 6.627413749694824
Epoch 820, val loss: 1.373666763305664
Epoch 830, training loss: 0.07061813771724701 = 0.004336921498179436 + 0.01 * 6.628121852874756
Epoch 830, val loss: 1.3795266151428223
Epoch 840, training loss: 0.07039447128772736 = 0.0042115310207009315 + 0.01 * 6.6182942390441895
Epoch 840, val loss: 1.3852046728134155
Epoch 850, training loss: 0.07036943733692169 = 0.00409253453835845 + 0.01 * 6.627689838409424
Epoch 850, val loss: 1.3907668590545654
Epoch 860, training loss: 0.07001926004886627 = 0.003979456145316362 + 0.01 * 6.603980541229248
Epoch 860, val loss: 1.3962112665176392
Epoch 870, training loss: 0.06998375803232193 = 0.0038720627781003714 + 0.01 * 6.611170291900635
Epoch 870, val loss: 1.4015381336212158
Epoch 880, training loss: 0.06978031247854233 = 0.003769787261262536 + 0.01 * 6.601052761077881
Epoch 880, val loss: 1.4067295789718628
Epoch 890, training loss: 0.06974499672651291 = 0.003672700608149171 + 0.01 * 6.607229709625244
Epoch 890, val loss: 1.4118191003799438
Epoch 900, training loss: 0.0695202425122261 = 0.003580081043764949 + 0.01 * 6.5940165519714355
Epoch 900, val loss: 1.4167455434799194
Epoch 910, training loss: 0.06929987668991089 = 0.0034916524309664965 + 0.01 * 6.580821990966797
Epoch 910, val loss: 1.4216396808624268
Epoch 920, training loss: 0.06916157901287079 = 0.0034073502756655216 + 0.01 * 6.575423240661621
Epoch 920, val loss: 1.4264304637908936
Epoch 930, training loss: 0.06937078386545181 = 0.003326840465888381 + 0.01 * 6.604394435882568
Epoch 930, val loss: 1.4310837984085083
Epoch 940, training loss: 0.06886965036392212 = 0.003249881789088249 + 0.01 * 6.561976909637451
Epoch 940, val loss: 1.4356422424316406
Epoch 950, training loss: 0.0688234493136406 = 0.0031761808786541224 + 0.01 * 6.564726829528809
Epoch 950, val loss: 1.4401576519012451
Epoch 960, training loss: 0.06883581727743149 = 0.0031057638116180897 + 0.01 * 6.573005199432373
Epoch 960, val loss: 1.4445894956588745
Epoch 970, training loss: 0.06872083991765976 = 0.003038202878087759 + 0.01 * 6.568263530731201
Epoch 970, val loss: 1.4489259719848633
Epoch 980, training loss: 0.06864020228385925 = 0.0029734005220234394 + 0.01 * 6.566680431365967
Epoch 980, val loss: 1.4532456398010254
Epoch 990, training loss: 0.06884495168924332 = 0.002911226125434041 + 0.01 * 6.593372821807861
Epoch 990, val loss: 1.4574434757232666
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.7860
Flip ASR: 0.7422/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0122811794281006 = 1.9285426139831543 + 0.01 * 8.373855590820312
Epoch 0, val loss: 1.9265844821929932
Epoch 10, training loss: 2.0026516914367676 = 1.9189139604568481 + 0.01 * 8.373766899108887
Epoch 10, val loss: 1.9170479774475098
Epoch 20, training loss: 1.991196632385254 = 1.907462239265442 + 0.01 * 8.373434066772461
Epoch 20, val loss: 1.9049264192581177
Epoch 30, training loss: 1.9755418300628662 = 1.8918168544769287 + 0.01 * 8.37249755859375
Epoch 30, val loss: 1.8875137567520142
Epoch 40, training loss: 1.952958583831787 = 1.8692647218704224 + 0.01 * 8.369389533996582
Epoch 40, val loss: 1.8621059656143188
Epoch 50, training loss: 1.921258568763733 = 1.8377057313919067 + 0.01 * 8.3552827835083
Epoch 50, val loss: 1.8281174898147583
Epoch 60, training loss: 1.8806580305099487 = 1.797984004020691 + 0.01 * 8.26740550994873
Epoch 60, val loss: 1.7893692255020142
Epoch 70, training loss: 1.8313050270080566 = 1.7534254789352417 + 0.01 * 7.787951946258545
Epoch 70, val loss: 1.750245213508606
Epoch 80, training loss: 1.7764544486999512 = 1.7022625207901 + 0.01 * 7.419196128845215
Epoch 80, val loss: 1.7068713903427124
Epoch 90, training loss: 1.7070807218551636 = 1.6343477964401245 + 0.01 * 7.273289203643799
Epoch 90, val loss: 1.6500482559204102
Epoch 100, training loss: 1.6169594526290894 = 1.5448611974716187 + 0.01 * 7.209826946258545
Epoch 100, val loss: 1.5773643255233765
Epoch 110, training loss: 1.507392406463623 = 1.4356310367584229 + 0.01 * 7.176133632659912
Epoch 110, val loss: 1.4905123710632324
Epoch 120, training loss: 1.3876099586486816 = 1.3160322904586792 + 0.01 * 7.157768249511719
Epoch 120, val loss: 1.3966988325119019
Epoch 130, training loss: 1.2691336870193481 = 1.1977070569992065 + 0.01 * 7.142663478851318
Epoch 130, val loss: 1.3051350116729736
Epoch 140, training loss: 1.160667896270752 = 1.0893831253051758 + 0.01 * 7.128477096557617
Epoch 140, val loss: 1.221856713294983
Epoch 150, training loss: 1.0647473335266113 = 0.9936090707778931 + 0.01 * 7.113821506500244
Epoch 150, val loss: 1.1480191946029663
Epoch 160, training loss: 0.978808581829071 = 0.907850444316864 + 0.01 * 7.095815658569336
Epoch 160, val loss: 1.0824275016784668
Epoch 170, training loss: 0.8992892503738403 = 0.8285697102546692 + 0.01 * 7.071953773498535
Epoch 170, val loss: 1.0237559080123901
Epoch 180, training loss: 0.8229910731315613 = 0.7525752186775208 + 0.01 * 7.041584014892578
Epoch 180, val loss: 0.9689542651176453
Epoch 190, training loss: 0.7487751245498657 = 0.678636908531189 + 0.01 * 7.013820171356201
Epoch 190, val loss: 0.9160649180412292
Epoch 200, training loss: 0.6774065494537354 = 0.6074493527412415 + 0.01 * 6.9957194328308105
Epoch 200, val loss: 0.866421639919281
Epoch 210, training loss: 0.6101677417755127 = 0.5402984023094177 + 0.01 * 6.986937046051025
Epoch 210, val loss: 0.8218775987625122
Epoch 220, training loss: 0.5480570793151855 = 0.47824352979660034 + 0.01 * 6.981352806091309
Epoch 220, val loss: 0.7831767201423645
Epoch 230, training loss: 0.4913584589958191 = 0.42159777879714966 + 0.01 * 6.976066589355469
Epoch 230, val loss: 0.7511259317398071
Epoch 240, training loss: 0.4399276673793793 = 0.3702220022678375 + 0.01 * 6.970566272735596
Epoch 240, val loss: 0.7260147929191589
Epoch 250, training loss: 0.393473356962204 = 0.3238198459148407 + 0.01 * 6.965350151062012
Epoch 250, val loss: 0.7068458199501038
Epoch 260, training loss: 0.3516994118690491 = 0.2821151316165924 + 0.01 * 6.95842981338501
Epoch 260, val loss: 0.6926896572113037
Epoch 270, training loss: 0.31441590189933777 = 0.24489624798297882 + 0.01 * 6.951966285705566
Epoch 270, val loss: 0.6829169988632202
Epoch 280, training loss: 0.28166013956069946 = 0.21216119825839996 + 0.01 * 6.949894428253174
Epoch 280, val loss: 0.6769116520881653
Epoch 290, training loss: 0.2534678876399994 = 0.18406277894973755 + 0.01 * 6.9405107498168945
Epoch 290, val loss: 0.674356997013092
Epoch 300, training loss: 0.22955933213233948 = 0.16025295853614807 + 0.01 * 6.930636405944824
Epoch 300, val loss: 0.6747379302978516
Epoch 310, training loss: 0.20943138003349304 = 0.14018666744232178 + 0.01 * 6.924471378326416
Epoch 310, val loss: 0.677950918674469
Epoch 320, training loss: 0.19240489602088928 = 0.12319368124008179 + 0.01 * 6.921121120452881
Epoch 320, val loss: 0.683134138584137
Epoch 330, training loss: 0.17787861824035645 = 0.10875886678695679 + 0.01 * 6.911974906921387
Epoch 330, val loss: 0.6903010606765747
Epoch 340, training loss: 0.16539794206619263 = 0.09638721495866776 + 0.01 * 6.901072978973389
Epoch 340, val loss: 0.6988148093223572
Epoch 350, training loss: 0.15462365746498108 = 0.08571305871009827 + 0.01 * 6.891059875488281
Epoch 350, val loss: 0.7087859511375427
Epoch 360, training loss: 0.1453372985124588 = 0.07644882053136826 + 0.01 * 6.888848304748535
Epoch 360, val loss: 0.7196219563484192
Epoch 370, training loss: 0.13719500601291656 = 0.06836985796689987 + 0.01 * 6.882514953613281
Epoch 370, val loss: 0.7312008738517761
Epoch 380, training loss: 0.12996703386306763 = 0.06130924075841904 + 0.01 * 6.865780353546143
Epoch 380, val loss: 0.7432546019554138
Epoch 390, training loss: 0.12378939241170883 = 0.05512581765651703 + 0.01 * 6.866357803344727
Epoch 390, val loss: 0.7556581497192383
Epoch 400, training loss: 0.11833296716213226 = 0.04970383271574974 + 0.01 * 6.862913131713867
Epoch 400, val loss: 0.7681865096092224
Epoch 410, training loss: 0.11349418759346008 = 0.044944752007722855 + 0.01 * 6.854944229125977
Epoch 410, val loss: 0.7808763384819031
Epoch 420, training loss: 0.10911944508552551 = 0.04076123237609863 + 0.01 * 6.835821151733398
Epoch 420, val loss: 0.7935959696769714
Epoch 430, training loss: 0.10535339266061783 = 0.03707926720380783 + 0.01 * 6.8274126052856445
Epoch 430, val loss: 0.8063327074050903
Epoch 440, training loss: 0.10205438733100891 = 0.03383339196443558 + 0.01 * 6.822099685668945
Epoch 440, val loss: 0.8189433217048645
Epoch 450, training loss: 0.09927114099264145 = 0.030964473262429237 + 0.01 * 6.830667018890381
Epoch 450, val loss: 0.8314682841300964
Epoch 460, training loss: 0.09651166945695877 = 0.028423616662621498 + 0.01 * 6.808805465698242
Epoch 460, val loss: 0.8437725901603699
Epoch 470, training loss: 0.0942283421754837 = 0.026165548712015152 + 0.01 * 6.806280136108398
Epoch 470, val loss: 0.8559236526489258
Epoch 480, training loss: 0.0922684520483017 = 0.02415289916098118 + 0.01 * 6.8115553855896
Epoch 480, val loss: 0.867787778377533
Epoch 490, training loss: 0.09035854041576385 = 0.022354472428560257 + 0.01 * 6.8004069328308105
Epoch 490, val loss: 0.8794411420822144
Epoch 500, training loss: 0.08861769735813141 = 0.020742740482091904 + 0.01 * 6.787496089935303
Epoch 500, val loss: 0.8908178210258484
Epoch 510, training loss: 0.08709311485290527 = 0.019294092431664467 + 0.01 * 6.779902458190918
Epoch 510, val loss: 0.9019354581832886
Epoch 520, training loss: 0.0857536643743515 = 0.017988907173275948 + 0.01 * 6.7764763832092285
Epoch 520, val loss: 0.9127131700515747
Epoch 530, training loss: 0.0844661220908165 = 0.016809634864330292 + 0.01 * 6.76564884185791
Epoch 530, val loss: 0.9232561588287354
Epoch 540, training loss: 0.08340990543365479 = 0.015741081908345222 + 0.01 * 6.76688289642334
Epoch 540, val loss: 0.9335254430770874
Epoch 550, training loss: 0.0823158472776413 = 0.014770741574466228 + 0.01 * 6.754510879516602
Epoch 550, val loss: 0.9435476660728455
Epoch 560, training loss: 0.08135677874088287 = 0.013886980712413788 + 0.01 * 6.7469801902771
Epoch 560, val loss: 0.9533648490905762
Epoch 570, training loss: 0.08065053075551987 = 0.013081124052405357 + 0.01 * 6.756940841674805
Epoch 570, val loss: 0.9629022479057312
Epoch 580, training loss: 0.07984336465597153 = 0.012345296330749989 + 0.01 * 6.749806880950928
Epoch 580, val loss: 0.9721582531929016
Epoch 590, training loss: 0.07896322757005692 = 0.01167090330272913 + 0.01 * 6.7292327880859375
Epoch 590, val loss: 0.9812371134757996
Epoch 600, training loss: 0.07829368859529495 = 0.01105132233351469 + 0.01 * 6.724236965179443
Epoch 600, val loss: 0.9900597333908081
Epoch 610, training loss: 0.07774360477924347 = 0.010481489822268486 + 0.01 * 6.7262115478515625
Epoch 610, val loss: 0.9986559748649597
Epoch 620, training loss: 0.07712146639823914 = 0.009956534951925278 + 0.01 * 6.716493606567383
Epoch 620, val loss: 1.0070255994796753
Epoch 630, training loss: 0.07685773074626923 = 0.009471364319324493 + 0.01 * 6.7386369705200195
Epoch 630, val loss: 1.0152664184570312
Epoch 640, training loss: 0.07615964859724045 = 0.009022915735840797 + 0.01 * 6.7136735916137695
Epoch 640, val loss: 1.0232638120651245
Epoch 650, training loss: 0.07579419761896133 = 0.008607393130660057 + 0.01 * 6.718680381774902
Epoch 650, val loss: 1.0309975147247314
Epoch 660, training loss: 0.07518559694290161 = 0.008221651427447796 + 0.01 * 6.696394920349121
Epoch 660, val loss: 1.0386452674865723
Epoch 670, training loss: 0.07469750940799713 = 0.007862911559641361 + 0.01 * 6.683459758758545
Epoch 670, val loss: 1.0460304021835327
Epoch 680, training loss: 0.07439964264631271 = 0.007528065703809261 + 0.01 * 6.687158107757568
Epoch 680, val loss: 1.0532552003860474
Epoch 690, training loss: 0.07397911697626114 = 0.007215428166091442 + 0.01 * 6.676369667053223
Epoch 690, val loss: 1.0603753328323364
Epoch 700, training loss: 0.07368327677249908 = 0.00692350510507822 + 0.01 * 6.67597770690918
Epoch 700, val loss: 1.0672688484191895
Epoch 710, training loss: 0.07335836440324783 = 0.006650418508797884 + 0.01 * 6.670794486999512
Epoch 710, val loss: 1.0739879608154297
Epoch 720, training loss: 0.07320579141378403 = 0.006395100615918636 + 0.01 * 6.6810688972473145
Epoch 720, val loss: 1.0806264877319336
Epoch 730, training loss: 0.07276973128318787 = 0.006155988201498985 + 0.01 * 6.661374568939209
Epoch 730, val loss: 1.0870591402053833
Epoch 740, training loss: 0.07267969101667404 = 0.005931525491178036 + 0.01 * 6.674816608428955
Epoch 740, val loss: 1.093270182609558
Epoch 750, training loss: 0.07218386232852936 = 0.005720335990190506 + 0.01 * 6.646352767944336
Epoch 750, val loss: 1.0994046926498413
Epoch 760, training loss: 0.07216723263263702 = 0.005521491635590792 + 0.01 * 6.66457462310791
Epoch 760, val loss: 1.1054118871688843
Epoch 770, training loss: 0.07167929410934448 = 0.005334031768143177 + 0.01 * 6.634526252746582
Epoch 770, val loss: 1.1112889051437378
Epoch 780, training loss: 0.07168839126825333 = 0.005157096311450005 + 0.01 * 6.653129577636719
Epoch 780, val loss: 1.1170390844345093
Epoch 790, training loss: 0.07125373184680939 = 0.004990058485418558 + 0.01 * 6.626367568969727
Epoch 790, val loss: 1.122617244720459
Epoch 800, training loss: 0.0710788145661354 = 0.004832173697650433 + 0.01 * 6.624664306640625
Epoch 800, val loss: 1.127981424331665
Epoch 810, training loss: 0.07084342837333679 = 0.004682839848101139 + 0.01 * 6.616058826446533
Epoch 810, val loss: 1.1333792209625244
Epoch 820, training loss: 0.07080112397670746 = 0.004541188478469849 + 0.01 * 6.625993728637695
Epoch 820, val loss: 1.1386032104492188
Epoch 830, training loss: 0.07061100751161575 = 0.004406953230500221 + 0.01 * 6.620405673980713
Epoch 830, val loss: 1.143668293952942
Epoch 840, training loss: 0.07032454758882523 = 0.0042794537730515 + 0.01 * 6.604508876800537
Epoch 840, val loss: 1.1487126350402832
Epoch 850, training loss: 0.07026524096727371 = 0.004158475436270237 + 0.01 * 6.6106767654418945
Epoch 850, val loss: 1.153577446937561
Epoch 860, training loss: 0.06995797157287598 = 0.004043342545628548 + 0.01 * 6.591463088989258
Epoch 860, val loss: 1.158410668373108
Epoch 870, training loss: 0.06999970227479935 = 0.003933626692742109 + 0.01 * 6.606607913970947
Epoch 870, val loss: 1.163093090057373
Epoch 880, training loss: 0.06981445103883743 = 0.003829175839200616 + 0.01 * 6.598527908325195
Epoch 880, val loss: 1.1677111387252808
Epoch 890, training loss: 0.06971337646245956 = 0.0037296710070222616 + 0.01 * 6.5983710289001465
Epoch 890, val loss: 1.1721876859664917
Epoch 900, training loss: 0.06948183476924896 = 0.003634514519944787 + 0.01 * 6.5847320556640625
Epoch 900, val loss: 1.1766982078552246
Epoch 910, training loss: 0.06956138461828232 = 0.0035437392070889473 + 0.01 * 6.60176420211792
Epoch 910, val loss: 1.1809858083724976
Epoch 920, training loss: 0.06923294067382812 = 0.0034571015276014805 + 0.01 * 6.577584266662598
Epoch 920, val loss: 1.1852920055389404
Epoch 930, training loss: 0.06930658966302872 = 0.0033743148669600487 + 0.01 * 6.593227863311768
Epoch 930, val loss: 1.189411997795105
Epoch 940, training loss: 0.06909191608428955 = 0.0032950870227068663 + 0.01 * 6.579683303833008
Epoch 940, val loss: 1.1935197114944458
Epoch 950, training loss: 0.0688556581735611 = 0.003219256643205881 + 0.01 * 6.563640594482422
Epoch 950, val loss: 1.1975834369659424
Epoch 960, training loss: 0.06889156252145767 = 0.0031464295461773872 + 0.01 * 6.5745134353637695
Epoch 960, val loss: 1.2015143632888794
Epoch 970, training loss: 0.06867507100105286 = 0.003076705848798156 + 0.01 * 6.5598368644714355
Epoch 970, val loss: 1.205414056777954
Epoch 980, training loss: 0.06850480288267136 = 0.00300982897169888 + 0.01 * 6.549497604370117
Epoch 980, val loss: 1.209219217300415
Epoch 990, training loss: 0.06853099167346954 = 0.002945739310234785 + 0.01 * 6.558526039123535
Epoch 990, val loss: 1.2129417657852173
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9631
Flip ASR: 0.9556/225 nodes
The final ASR:0.74785, 0.19321, Accuracy:0.81852, 0.00605
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11586])
remove edge: torch.Size([2, 9498])
updated graph: torch.Size([2, 10528])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.97786, 0.00603, Accuracy:0.83210, 0.00761
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.038577079772949 = 1.9548382759094238 + 0.01 * 8.373878479003906
Epoch 0, val loss: 1.949783205986023
Epoch 10, training loss: 2.0277655124664307 = 1.9440276622772217 + 0.01 * 8.373795509338379
Epoch 10, val loss: 1.9396185874938965
Epoch 20, training loss: 2.01438307762146 = 1.9306472539901733 + 0.01 * 8.373580932617188
Epoch 20, val loss: 1.9266518354415894
Epoch 30, training loss: 1.995726466178894 = 1.9119962453842163 + 0.01 * 8.373025894165039
Epoch 30, val loss: 1.9083852767944336
Epoch 40, training loss: 1.968645691871643 = 1.8849393129348755 + 0.01 * 8.370637893676758
Epoch 40, val loss: 1.8822027444839478
Epoch 50, training loss: 1.931037187576294 = 1.8475004434585571 + 0.01 * 8.353677749633789
Epoch 50, val loss: 1.847765326499939
Epoch 60, training loss: 1.8868842124938965 = 1.8043879270553589 + 0.01 * 8.249634742736816
Epoch 60, val loss: 1.8125691413879395
Epoch 70, training loss: 1.844009518623352 = 1.7642866373062134 + 0.01 * 7.972288131713867
Epoch 70, val loss: 1.783735752105713
Epoch 80, training loss: 1.7931506633758545 = 1.7157007455825806 + 0.01 * 7.744992256164551
Epoch 80, val loss: 1.7436950206756592
Epoch 90, training loss: 1.7227261066436768 = 1.6483051776885986 + 0.01 * 7.442089080810547
Epoch 90, val loss: 1.68553626537323
Epoch 100, training loss: 1.63433837890625 = 1.5616905689239502 + 0.01 * 7.264779567718506
Epoch 100, val loss: 1.6139100790023804
Epoch 110, training loss: 1.5359210968017578 = 1.464056134223938 + 0.01 * 7.186498641967773
Epoch 110, val loss: 1.5360850095748901
Epoch 120, training loss: 1.4369101524353027 = 1.3657461404800415 + 0.01 * 7.116400718688965
Epoch 120, val loss: 1.4582164287567139
Epoch 130, training loss: 1.3392677307128906 = 1.2683277130126953 + 0.01 * 7.094007968902588
Epoch 130, val loss: 1.3832637071609497
Epoch 140, training loss: 1.2416377067565918 = 1.1708033084869385 + 0.01 * 7.083436965942383
Epoch 140, val loss: 1.309194803237915
Epoch 150, training loss: 1.145805835723877 = 1.0750346183776855 + 0.01 * 7.077118873596191
Epoch 150, val loss: 1.2379281520843506
Epoch 160, training loss: 1.0547157526016235 = 0.9840047359466553 + 0.01 * 7.071104526519775
Epoch 160, val loss: 1.171087384223938
Epoch 170, training loss: 0.9702388048171997 = 0.8996165990829468 + 0.01 * 7.0622239112854
Epoch 170, val loss: 1.1091499328613281
Epoch 180, training loss: 0.8930419087409973 = 0.8225674033164978 + 0.01 * 7.047452449798584
Epoch 180, val loss: 1.0527093410491943
Epoch 190, training loss: 0.8236109614372253 = 0.7533872127532959 + 0.01 * 7.022375583648682
Epoch 190, val loss: 1.002872109413147
Epoch 200, training loss: 0.7621880769729614 = 0.6923231482505798 + 0.01 * 6.986495018005371
Epoch 200, val loss: 0.9606359601020813
Epoch 210, training loss: 0.7077360153198242 = 0.6382125616073608 + 0.01 * 6.952343940734863
Epoch 210, val loss: 0.9263507127761841
Epoch 220, training loss: 0.6576651334762573 = 0.5884699821472168 + 0.01 * 6.919516086578369
Epoch 220, val loss: 0.8988202214241028
Epoch 230, training loss: 0.6092767715454102 = 0.5403159856796265 + 0.01 * 6.896076202392578
Epoch 230, val loss: 0.8760632276535034
Epoch 240, training loss: 0.5607840418815613 = 0.4919886291027069 + 0.01 * 6.879542350769043
Epoch 240, val loss: 0.8560221195220947
Epoch 250, training loss: 0.5119093656539917 = 0.4430198669433594 + 0.01 * 6.888949394226074
Epoch 250, val loss: 0.8369091749191284
Epoch 260, training loss: 0.4628872871398926 = 0.39421337842941284 + 0.01 * 6.86738920211792
Epoch 260, val loss: 0.8183159828186035
Epoch 270, training loss: 0.4153404235839844 = 0.34669238328933716 + 0.01 * 6.864805698394775
Epoch 270, val loss: 0.8005844354629517
Epoch 280, training loss: 0.36978405714035034 = 0.3012489080429077 + 0.01 * 6.853513717651367
Epoch 280, val loss: 0.7838213443756104
Epoch 290, training loss: 0.32712480425834656 = 0.25861942768096924 + 0.01 * 6.85053825378418
Epoch 290, val loss: 0.7684731483459473
Epoch 300, training loss: 0.28831031918525696 = 0.2199024260044098 + 0.01 * 6.840789794921875
Epoch 300, val loss: 0.7562847137451172
Epoch 310, training loss: 0.2544783353805542 = 0.18609033524990082 + 0.01 * 6.838798999786377
Epoch 310, val loss: 0.7485393285751343
Epoch 320, training loss: 0.2259702980518341 = 0.15761065483093262 + 0.01 * 6.835964202880859
Epoch 320, val loss: 0.746045708656311
Epoch 330, training loss: 0.20230454206466675 = 0.13408567011356354 + 0.01 * 6.821888446807861
Epoch 330, val loss: 0.7484883069992065
Epoch 340, training loss: 0.18294988572597504 = 0.11480141431093216 + 0.01 * 6.814846992492676
Epoch 340, val loss: 0.7551425695419312
Epoch 350, training loss: 0.16718152165412903 = 0.09896329790353775 + 0.01 * 6.821822166442871
Epoch 350, val loss: 0.7650113105773926
Epoch 360, training loss: 0.1540151685476303 = 0.08585811406373978 + 0.01 * 6.815705299377441
Epoch 360, val loss: 0.7772265672683716
Epoch 370, training loss: 0.14297349750995636 = 0.07491972297430038 + 0.01 * 6.805377960205078
Epoch 370, val loss: 0.7910168170928955
Epoch 380, training loss: 0.1337536871433258 = 0.06571224331855774 + 0.01 * 6.804143905639648
Epoch 380, val loss: 0.8057934641838074
Epoch 390, training loss: 0.12586933374404907 = 0.057922497391700745 + 0.01 * 6.794684886932373
Epoch 390, val loss: 0.8211208581924438
Epoch 400, training loss: 0.11914326250553131 = 0.05130166932940483 + 0.01 * 6.784160137176514
Epoch 400, val loss: 0.8365936279296875
Epoch 410, training loss: 0.11343907564878464 = 0.045654818415641785 + 0.01 * 6.778425693511963
Epoch 410, val loss: 0.8519746661186218
Epoch 420, training loss: 0.10849840939044952 = 0.040824759751558304 + 0.01 * 6.767364501953125
Epoch 420, val loss: 0.8670746684074402
Epoch 430, training loss: 0.10426616668701172 = 0.036681484431028366 + 0.01 * 6.7584686279296875
Epoch 430, val loss: 0.8818325400352478
Epoch 440, training loss: 0.10073620826005936 = 0.03311663120985031 + 0.01 * 6.76195764541626
Epoch 440, val loss: 0.8961517214775085
Epoch 450, training loss: 0.09758125245571136 = 0.03003791533410549 + 0.01 * 6.754333972930908
Epoch 450, val loss: 0.9099792838096619
Epoch 460, training loss: 0.09488129615783691 = 0.027365008369088173 + 0.01 * 6.751628875732422
Epoch 460, val loss: 0.9233397841453552
Epoch 470, training loss: 0.09252741187810898 = 0.025032730773091316 + 0.01 * 6.749468803405762
Epoch 470, val loss: 0.9361851811408997
Epoch 480, training loss: 0.09034203737974167 = 0.022989360615611076 + 0.01 * 6.735267639160156
Epoch 480, val loss: 0.9485583305358887
Epoch 490, training loss: 0.0883675292134285 = 0.021190300583839417 + 0.01 * 6.717723369598389
Epoch 490, val loss: 0.9604877829551697
Epoch 500, training loss: 0.08691111952066422 = 0.019598131999373436 + 0.01 * 6.731298446655273
Epoch 500, val loss: 0.9719436168670654
Epoch 510, training loss: 0.08531110733747482 = 0.01818370260298252 + 0.01 * 6.712740898132324
Epoch 510, val loss: 0.9830378890037537
Epoch 520, training loss: 0.08393795788288116 = 0.016921106725931168 + 0.01 * 6.701685428619385
Epoch 520, val loss: 0.9937894344329834
Epoch 530, training loss: 0.08356381207704544 = 0.015790224075317383 + 0.01 * 6.7773590087890625
Epoch 530, val loss: 1.0041375160217285
Epoch 540, training loss: 0.08180780708789825 = 0.014777195639908314 + 0.01 * 6.703061103820801
Epoch 540, val loss: 1.014164924621582
Epoch 550, training loss: 0.08064098656177521 = 0.013864233158528805 + 0.01 * 6.677675724029541
Epoch 550, val loss: 1.0238282680511475
Epoch 560, training loss: 0.07994811236858368 = 0.013036943040788174 + 0.01 * 6.691117286682129
Epoch 560, val loss: 1.0332090854644775
Epoch 570, training loss: 0.07901781797409058 = 0.01228609960526228 + 0.01 * 6.673172473907471
Epoch 570, val loss: 1.0423024892807007
Epoch 580, training loss: 0.0782226175069809 = 0.011602331884205341 + 0.01 * 6.662028789520264
Epoch 580, val loss: 1.051131010055542
Epoch 590, training loss: 0.0774693563580513 = 0.010978150181472301 + 0.01 * 6.649121284484863
Epoch 590, val loss: 1.0596709251403809
Epoch 600, training loss: 0.07694529742002487 = 0.010406283661723137 + 0.01 * 6.653901100158691
Epoch 600, val loss: 1.068016767501831
Epoch 610, training loss: 0.07644037157297134 = 0.009881502948701382 + 0.01 * 6.655886650085449
Epoch 610, val loss: 1.0760648250579834
Epoch 620, training loss: 0.07607807964086533 = 0.00939932931214571 + 0.01 * 6.667874813079834
Epoch 620, val loss: 1.0839287042617798
Epoch 630, training loss: 0.07543165981769562 = 0.0089553352445364 + 0.01 * 6.647632598876953
Epoch 630, val loss: 1.0915138721466064
Epoch 640, training loss: 0.07519247382879257 = 0.008544881828129292 + 0.01 * 6.664759159088135
Epoch 640, val loss: 1.098973035812378
Epoch 650, training loss: 0.0743359923362732 = 0.008164774626493454 + 0.01 * 6.617122173309326
Epoch 650, val loss: 1.1061997413635254
Epoch 660, training loss: 0.07407300919294357 = 0.007812073919922113 + 0.01 * 6.626093864440918
Epoch 660, val loss: 1.1132303476333618
Epoch 670, training loss: 0.07348723709583282 = 0.007484009023755789 + 0.01 * 6.600322723388672
Epoch 670, val loss: 1.120119333267212
Epoch 680, training loss: 0.07325177639722824 = 0.007178197149187326 + 0.01 * 6.607358455657959
Epoch 680, val loss: 1.1268290281295776
Epoch 690, training loss: 0.07286982983350754 = 0.0068931663408875465 + 0.01 * 6.597666263580322
Epoch 690, val loss: 1.1333715915679932
Epoch 700, training loss: 0.07255791127681732 = 0.00662675965577364 + 0.01 * 6.593115329742432
Epoch 700, val loss: 1.1397852897644043
Epoch 710, training loss: 0.07238088548183441 = 0.006377439480274916 + 0.01 * 6.600345134735107
Epoch 710, val loss: 1.1460322141647339
Epoch 720, training loss: 0.07227195799350739 = 0.006143795792013407 + 0.01 * 6.612816333770752
Epoch 720, val loss: 1.1521456241607666
Epoch 730, training loss: 0.07175051420927048 = 0.005924302153289318 + 0.01 * 6.5826215744018555
Epoch 730, val loss: 1.158097267150879
Epoch 740, training loss: 0.0714288130402565 = 0.005717941094189882 + 0.01 * 6.57108736038208
Epoch 740, val loss: 1.1639328002929688
Epoch 750, training loss: 0.07121293991804123 = 0.005523533094674349 + 0.01 * 6.56894063949585
Epoch 750, val loss: 1.1696698665618896
Epoch 760, training loss: 0.07099276036024094 = 0.005340519826859236 + 0.01 * 6.565224647521973
Epoch 760, val loss: 1.1752692461013794
Epoch 770, training loss: 0.07083337008953094 = 0.0051677413284778595 + 0.01 * 6.566563606262207
Epoch 770, val loss: 1.1806952953338623
Epoch 780, training loss: 0.07046956568956375 = 0.005004612263292074 + 0.01 * 6.54649543762207
Epoch 780, val loss: 1.186055064201355
Epoch 790, training loss: 0.07047134637832642 = 0.004850262310355902 + 0.01 * 6.562108516693115
Epoch 790, val loss: 1.1912686824798584
Epoch 800, training loss: 0.07016633450984955 = 0.0047040884383022785 + 0.01 * 6.546225070953369
Epoch 800, val loss: 1.1964049339294434
Epoch 810, training loss: 0.07000402361154556 = 0.004565638955682516 + 0.01 * 6.5438385009765625
Epoch 810, val loss: 1.2013731002807617
Epoch 820, training loss: 0.06987502425909042 = 0.0044340770691633224 + 0.01 * 6.544095039367676
Epoch 820, val loss: 1.2063263654708862
Epoch 830, training loss: 0.06960499286651611 = 0.0043091666884720325 + 0.01 * 6.529582500457764
Epoch 830, val loss: 1.2111316919326782
Epoch 840, training loss: 0.06973476707935333 = 0.004190356936305761 + 0.01 * 6.554441452026367
Epoch 840, val loss: 1.2158740758895874
Epoch 850, training loss: 0.06956783682107925 = 0.004077481105923653 + 0.01 * 6.549036026000977
Epoch 850, val loss: 1.2204402685165405
Epoch 860, training loss: 0.06907305866479874 = 0.003969923593103886 + 0.01 * 6.510313510894775
Epoch 860, val loss: 1.2250021696090698
Epoch 870, training loss: 0.06908835470676422 = 0.0038673921953886747 + 0.01 * 6.522096157073975
Epoch 870, val loss: 1.2294245958328247
Epoch 880, training loss: 0.06901082396507263 = 0.003769788658246398 + 0.01 * 6.524104118347168
Epoch 880, val loss: 1.2337825298309326
Epoch 890, training loss: 0.06874268501996994 = 0.003676466876640916 + 0.01 * 6.506621837615967
Epoch 890, val loss: 1.238021731376648
Epoch 900, training loss: 0.0686098262667656 = 0.003587359329685569 + 0.01 * 6.502246856689453
Epoch 900, val loss: 1.2422364950180054
Epoch 910, training loss: 0.06866283714771271 = 0.003502178704366088 + 0.01 * 6.516066551208496
Epoch 910, val loss: 1.2463150024414062
Epoch 920, training loss: 0.06851493567228317 = 0.0034207343123853207 + 0.01 * 6.509420394897461
Epoch 920, val loss: 1.2503530979156494
Epoch 930, training loss: 0.06836781650781631 = 0.003342555370181799 + 0.01 * 6.50252628326416
Epoch 930, val loss: 1.2543113231658936
Epoch 940, training loss: 0.06803733855485916 = 0.003267789725214243 + 0.01 * 6.476955413818359
Epoch 940, val loss: 1.2582122087478638
Epoch 950, training loss: 0.068046934902668 = 0.0031961051281541586 + 0.01 * 6.485083103179932
Epoch 950, val loss: 1.262030839920044
Epoch 960, training loss: 0.0680493637919426 = 0.0031273425556719303 + 0.01 * 6.492201805114746
Epoch 960, val loss: 1.265782356262207
Epoch 970, training loss: 0.06795615702867508 = 0.003061360912397504 + 0.01 * 6.489480018615723
Epoch 970, val loss: 1.2694830894470215
Epoch 980, training loss: 0.06779506057500839 = 0.0029979716055095196 + 0.01 * 6.479708671569824
Epoch 980, val loss: 1.2731050252914429
Epoch 990, training loss: 0.06768369674682617 = 0.002937107812613249 + 0.01 * 6.474658966064453
Epoch 990, val loss: 1.2766363620758057
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.5351
Flip ASR: 0.4533/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.03855037689209 = 1.9548115730285645 + 0.01 * 8.373888969421387
Epoch 0, val loss: 1.9566622972488403
Epoch 10, training loss: 2.0279197692871094 = 1.9441814422607422 + 0.01 * 8.373842239379883
Epoch 10, val loss: 1.9456144571304321
Epoch 20, training loss: 2.0151798725128174 = 1.931443214416504 + 0.01 * 8.3736572265625
Epoch 20, val loss: 1.9317389726638794
Epoch 30, training loss: 1.9977563619613647 = 1.9140244722366333 + 0.01 * 8.373191833496094
Epoch 30, val loss: 1.9120535850524902
Epoch 40, training loss: 1.972622275352478 = 1.888907551765442 + 0.01 * 8.37147331237793
Epoch 40, val loss: 1.8834065198898315
Epoch 50, training loss: 1.9370778799057007 = 1.8534709215164185 + 0.01 * 8.36069107055664
Epoch 50, val loss: 1.8440338373184204
Epoch 60, training loss: 1.892916202545166 = 1.8099055290222168 + 0.01 * 8.301071166992188
Epoch 60, val loss: 1.7989336252212524
Epoch 70, training loss: 1.8475067615509033 = 1.7671481370925903 + 0.01 * 8.035868644714355
Epoch 70, val loss: 1.7590630054473877
Epoch 80, training loss: 1.7977817058563232 = 1.720008134841919 + 0.01 * 7.7773590087890625
Epoch 80, val loss: 1.7169069051742554
Epoch 90, training loss: 1.7291102409362793 = 1.6546638011932373 + 0.01 * 7.444642543792725
Epoch 90, val loss: 1.659701943397522
Epoch 100, training loss: 1.6408666372299194 = 1.5683516263961792 + 0.01 * 7.251500606536865
Epoch 100, val loss: 1.5852346420288086
Epoch 110, training loss: 1.5366259813308716 = 1.4647961854934692 + 0.01 * 7.1829833984375
Epoch 110, val loss: 1.4986138343811035
Epoch 120, training loss: 1.4248409271240234 = 1.3534959554672241 + 0.01 * 7.134498596191406
Epoch 120, val loss: 1.4090721607208252
Epoch 130, training loss: 1.310976505279541 = 1.2401182651519775 + 0.01 * 7.085825443267822
Epoch 130, val loss: 1.3202800750732422
Epoch 140, training loss: 1.1974917650222778 = 1.1270699501037598 + 0.01 * 7.042178153991699
Epoch 140, val loss: 1.2347509860992432
Epoch 150, training loss: 1.087515115737915 = 1.0174299478530884 + 0.01 * 7.008518218994141
Epoch 150, val loss: 1.1531603336334229
Epoch 160, training loss: 0.9855026602745056 = 0.9156601428985596 + 0.01 * 6.984252452850342
Epoch 160, val loss: 1.0794422626495361
Epoch 170, training loss: 0.8957639336585999 = 0.8260664343833923 + 0.01 * 6.969748497009277
Epoch 170, val loss: 1.017242431640625
Epoch 180, training loss: 0.8199164271354675 = 0.7503015398979187 + 0.01 * 6.961490154266357
Epoch 180, val loss: 0.9678727984428406
Epoch 190, training loss: 0.7566941976547241 = 0.6871213316917419 + 0.01 * 6.957286834716797
Epoch 190, val loss: 0.930060625076294
Epoch 200, training loss: 0.70279461145401 = 0.6332494616508484 + 0.01 * 6.954517364501953
Epoch 200, val loss: 0.9007680416107178
Epoch 210, training loss: 0.6548736691474915 = 0.5853623747825623 + 0.01 * 6.951131820678711
Epoch 210, val loss: 0.8771247267723083
Epoch 220, training loss: 0.610785961151123 = 0.5413069128990173 + 0.01 * 6.94790506362915
Epoch 220, val loss: 0.8573138117790222
Epoch 230, training loss: 0.569563090801239 = 0.5001153349876404 + 0.01 * 6.9447736740112305
Epoch 230, val loss: 0.8407435417175293
Epoch 240, training loss: 0.5307220816612244 = 0.46130064129829407 + 0.01 * 6.942145824432373
Epoch 240, val loss: 0.8274136185646057
Epoch 250, training loss: 0.4939880669116974 = 0.4246109127998352 + 0.01 * 6.93771505355835
Epoch 250, val loss: 0.8170469999313354
Epoch 260, training loss: 0.45938119292259216 = 0.39001885056495667 + 0.01 * 6.9362335205078125
Epoch 260, val loss: 0.8095396161079407
Epoch 270, training loss: 0.42679065465927124 = 0.3574824631214142 + 0.01 * 6.930820941925049
Epoch 270, val loss: 0.8047953248023987
Epoch 280, training loss: 0.3961624503135681 = 0.32688212394714355 + 0.01 * 6.928033351898193
Epoch 280, val loss: 0.8021889925003052
Epoch 290, training loss: 0.36702612042427063 = 0.2977586090564728 + 0.01 * 6.926750659942627
Epoch 290, val loss: 0.8015496730804443
Epoch 300, training loss: 0.3389938175678253 = 0.26978862285614014 + 0.01 * 6.920519828796387
Epoch 300, val loss: 0.8026552796363831
Epoch 310, training loss: 0.31208890676498413 = 0.2428925335407257 + 0.01 * 6.919638633728027
Epoch 310, val loss: 0.8053773045539856
Epoch 320, training loss: 0.2864794135093689 = 0.21733975410461426 + 0.01 * 6.913964748382568
Epoch 320, val loss: 0.8100202679634094
Epoch 330, training loss: 0.26281067728996277 = 0.1937124878168106 + 0.01 * 6.909820079803467
Epoch 330, val loss: 0.8167974948883057
Epoch 340, training loss: 0.24143972992897034 = 0.17236168682575226 + 0.01 * 6.9078049659729
Epoch 340, val loss: 0.8260117769241333
Epoch 350, training loss: 0.2222547084093094 = 0.15326297283172607 + 0.01 * 6.899173736572266
Epoch 350, val loss: 0.8380054831504822
Epoch 360, training loss: 0.2054947316646576 = 0.13643820583820343 + 0.01 * 6.9056525230407715
Epoch 360, val loss: 0.8524216413497925
Epoch 370, training loss: 0.19052870571613312 = 0.12165442854166031 + 0.01 * 6.887427806854248
Epoch 370, val loss: 0.8688087463378906
Epoch 380, training loss: 0.1774662286043167 = 0.10866642743349075 + 0.01 * 6.879980564117432
Epoch 380, val loss: 0.8865832686424255
Epoch 390, training loss: 0.16583499312400818 = 0.0970892533659935 + 0.01 * 6.874573230743408
Epoch 390, val loss: 0.9052813649177551
Epoch 400, training loss: 0.15541648864746094 = 0.08674697577953339 + 0.01 * 6.866950988769531
Epoch 400, val loss: 0.9241237640380859
Epoch 410, training loss: 0.1460496485233307 = 0.0773656815290451 + 0.01 * 6.868396759033203
Epoch 410, val loss: 0.9430376887321472
Epoch 420, training loss: 0.137326180934906 = 0.06875594705343246 + 0.01 * 6.857024192810059
Epoch 420, val loss: 0.9620279669761658
Epoch 430, training loss: 0.12937802076339722 = 0.060949333012104034 + 0.01 * 6.842867851257324
Epoch 430, val loss: 0.9812474250793457
Epoch 440, training loss: 0.12266243994235992 = 0.053994711488485336 + 0.01 * 6.866772651672363
Epoch 440, val loss: 1.0004345178604126
Epoch 450, training loss: 0.11632648855447769 = 0.047937825322151184 + 0.01 * 6.838866233825684
Epoch 450, val loss: 1.0197151899337769
Epoch 460, training loss: 0.11104245483875275 = 0.04278024658560753 + 0.01 * 6.826220989227295
Epoch 460, val loss: 1.038942813873291
Epoch 470, training loss: 0.10654527693986893 = 0.038366660475730896 + 0.01 * 6.817861557006836
Epoch 470, val loss: 1.058115839958191
Epoch 480, training loss: 0.1027788370847702 = 0.034566838294267654 + 0.01 * 6.821199417114258
Epoch 480, val loss: 1.0771074295043945
Epoch 490, training loss: 0.09931391477584839 = 0.03128338232636452 + 0.01 * 6.803053379058838
Epoch 490, val loss: 1.09577214717865
Epoch 500, training loss: 0.09640857577323914 = 0.02842569909989834 + 0.01 * 6.798287391662598
Epoch 500, val loss: 1.1139286756515503
Epoch 510, training loss: 0.09381170570850372 = 0.0259327944368124 + 0.01 * 6.787891387939453
Epoch 510, val loss: 1.131717324256897
Epoch 520, training loss: 0.09178240597248077 = 0.023747388273477554 + 0.01 * 6.803501605987549
Epoch 520, val loss: 1.1489874124526978
Epoch 530, training loss: 0.08966929465532303 = 0.02182505466043949 + 0.01 * 6.784424304962158
Epoch 530, val loss: 1.1657315492630005
Epoch 540, training loss: 0.08784663677215576 = 0.02012556418776512 + 0.01 * 6.77210807800293
Epoch 540, val loss: 1.1820142269134521
Epoch 550, training loss: 0.08634679019451141 = 0.01861647143959999 + 0.01 * 6.7730326652526855
Epoch 550, val loss: 1.1976532936096191
Epoch 560, training loss: 0.08489618450403214 = 0.0172736719250679 + 0.01 * 6.762251377105713
Epoch 560, val loss: 1.2128455638885498
Epoch 570, training loss: 0.08367247134447098 = 0.01607111655175686 + 0.01 * 6.760135650634766
Epoch 570, val loss: 1.2275303602218628
Epoch 580, training loss: 0.08257387578487396 = 0.014992582611739635 + 0.01 * 6.758129596710205
Epoch 580, val loss: 1.2417417764663696
Epoch 590, training loss: 0.08136717230081558 = 0.01402252446860075 + 0.01 * 6.7344651222229
Epoch 590, val loss: 1.2554056644439697
Epoch 600, training loss: 0.08109338581562042 = 0.013145249336957932 + 0.01 * 6.794813632965088
Epoch 600, val loss: 1.2685905694961548
Epoch 610, training loss: 0.07963314652442932 = 0.01235469151288271 + 0.01 * 6.727845668792725
Epoch 610, val loss: 1.281321406364441
Epoch 620, training loss: 0.07878582179546356 = 0.011637460440397263 + 0.01 * 6.714836120605469
Epoch 620, val loss: 1.2936304807662964
Epoch 630, training loss: 0.078251414000988 = 0.010983625426888466 + 0.01 * 6.726779460906982
Epoch 630, val loss: 1.305544137954712
Epoch 640, training loss: 0.07750061899423599 = 0.010387231595814228 + 0.01 * 6.711338520050049
Epoch 640, val loss: 1.3170174360275269
Epoch 650, training loss: 0.0769072026014328 = 0.009841084480285645 + 0.01 * 6.706611633300781
Epoch 650, val loss: 1.3281853199005127
Epoch 660, training loss: 0.07630795240402222 = 0.009339626878499985 + 0.01 * 6.696832180023193
Epoch 660, val loss: 1.3390151262283325
Epoch 670, training loss: 0.07588562369346619 = 0.008878967724740505 + 0.01 * 6.7006659507751465
Epoch 670, val loss: 1.3494954109191895
Epoch 680, training loss: 0.07526589184999466 = 0.00845409743487835 + 0.01 * 6.681179046630859
Epoch 680, val loss: 1.3596166372299194
Epoch 690, training loss: 0.07503709942102432 = 0.008061305619776249 + 0.01 * 6.697579860687256
Epoch 690, val loss: 1.3695063591003418
Epoch 700, training loss: 0.07450126111507416 = 0.0076976739801466465 + 0.01 * 6.680359363555908
Epoch 700, val loss: 1.3790613412857056
Epoch 710, training loss: 0.07397998869419098 = 0.007360659074038267 + 0.01 * 6.661933422088623
Epoch 710, val loss: 1.3883862495422363
Epoch 720, training loss: 0.0736430436372757 = 0.0070476047694683075 + 0.01 * 6.659543991088867
Epoch 720, val loss: 1.3973815441131592
Epoch 730, training loss: 0.07333233207464218 = 0.00675675505772233 + 0.01 * 6.657558441162109
Epoch 730, val loss: 1.4061651229858398
Epoch 740, training loss: 0.07337341457605362 = 0.006485450081527233 + 0.01 * 6.6887969970703125
Epoch 740, val loss: 1.4147285223007202
Epoch 750, training loss: 0.07261992245912552 = 0.006231921724975109 + 0.01 * 6.638800621032715
Epoch 750, val loss: 1.4229681491851807
Epoch 760, training loss: 0.07239869982004166 = 0.005994358565658331 + 0.01 * 6.640434741973877
Epoch 760, val loss: 1.4310789108276367
Epoch 770, training loss: 0.07210153341293335 = 0.005771870259195566 + 0.01 * 6.632966995239258
Epoch 770, val loss: 1.4389569759368896
Epoch 780, training loss: 0.07185468822717667 = 0.005562862381339073 + 0.01 * 6.629182815551758
Epoch 780, val loss: 1.4466477632522583
Epoch 790, training loss: 0.07196815311908722 = 0.005366915371268988 + 0.01 * 6.660123825073242
Epoch 790, val loss: 1.4541664123535156
Epoch 800, training loss: 0.07136447727680206 = 0.005182626191526651 + 0.01 * 6.618185520172119
Epoch 800, val loss: 1.4614983797073364
Epoch 810, training loss: 0.07116726040840149 = 0.00500880554318428 + 0.01 * 6.615845203399658
Epoch 810, val loss: 1.468583106994629
Epoch 820, training loss: 0.07115069776773453 = 0.004845078568905592 + 0.01 * 6.6305623054504395
Epoch 820, val loss: 1.4755849838256836
Epoch 830, training loss: 0.0707177072763443 = 0.0046901279129087925 + 0.01 * 6.602758407592773
Epoch 830, val loss: 1.4823566675186157
Epoch 840, training loss: 0.07049231231212616 = 0.004543522838503122 + 0.01 * 6.594879150390625
Epoch 840, val loss: 1.4890388250350952
Epoch 850, training loss: 0.07039196044206619 = 0.004404785111546516 + 0.01 * 6.59871768951416
Epoch 850, val loss: 1.495524525642395
Epoch 860, training loss: 0.07026823610067368 = 0.004273712635040283 + 0.01 * 6.599452495574951
Epoch 860, val loss: 1.5018755197525024
Epoch 870, training loss: 0.07000409066677094 = 0.004149256739765406 + 0.01 * 6.585483551025391
Epoch 870, val loss: 1.5080550909042358
Epoch 880, training loss: 0.06993076205253601 = 0.004031306132674217 + 0.01 * 6.589945316314697
Epoch 880, val loss: 1.5141204595565796
Epoch 890, training loss: 0.06968110799789429 = 0.003918998874723911 + 0.01 * 6.576210975646973
Epoch 890, val loss: 1.5200693607330322
Epoch 900, training loss: 0.06980331987142563 = 0.0038123938720673323 + 0.01 * 6.599092483520508
Epoch 900, val loss: 1.525913953781128
Epoch 910, training loss: 0.06935214251279831 = 0.003710852935910225 + 0.01 * 6.56412935256958
Epoch 910, val loss: 1.5315468311309814
Epoch 920, training loss: 0.06937278807163239 = 0.0036139502190053463 + 0.01 * 6.5758843421936035
Epoch 920, val loss: 1.5370707511901855
Epoch 930, training loss: 0.06915194541215897 = 0.0035217152908444405 + 0.01 * 6.563022613525391
Epoch 930, val loss: 1.542546272277832
Epoch 940, training loss: 0.06892020255327225 = 0.0034335425589233637 + 0.01 * 6.548666477203369
Epoch 940, val loss: 1.5478615760803223
Epoch 950, training loss: 0.06893161684274673 = 0.0033493624068796635 + 0.01 * 6.558225631713867
Epoch 950, val loss: 1.5530891418457031
Epoch 960, training loss: 0.06879503279924393 = 0.0032690453808754683 + 0.01 * 6.55259895324707
Epoch 960, val loss: 1.5582196712493896
Epoch 970, training loss: 0.06865392625331879 = 0.0031922729685902596 + 0.01 * 6.546165466308594
Epoch 970, val loss: 1.5631911754608154
Epoch 980, training loss: 0.06851634383201599 = 0.0031187348067760468 + 0.01 * 6.539761066436768
Epoch 980, val loss: 1.568151831626892
Epoch 990, training loss: 0.06862550973892212 = 0.003048298880457878 + 0.01 * 6.557721138000488
Epoch 990, val loss: 1.5729236602783203
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7889
Overall ASR: 0.5277
Flip ASR: 0.4667/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0419440269470215 = 1.9582056999206543 + 0.01 * 8.373836517333984
Epoch 0, val loss: 1.9517569541931152
Epoch 10, training loss: 2.0309178829193115 = 1.9471803903579712 + 0.01 * 8.373750686645508
Epoch 10, val loss: 1.9399418830871582
Epoch 20, training loss: 2.017672538757324 = 1.9339377880096436 + 0.01 * 8.373477935791016
Epoch 20, val loss: 1.9254399538040161
Epoch 30, training loss: 1.9994843006134033 = 1.915757417678833 + 0.01 * 8.372687339782715
Epoch 30, val loss: 1.9051882028579712
Epoch 40, training loss: 1.9732853174209595 = 1.889594316482544 + 0.01 * 8.369096755981445
Epoch 40, val loss: 1.876333236694336
Epoch 50, training loss: 1.9365085363388062 = 1.853053331375122 + 0.01 * 8.34552001953125
Epoch 50, val loss: 1.8377307653427124
Epoch 60, training loss: 1.8909112215042114 = 1.808860182762146 + 0.01 * 8.205109596252441
Epoch 60, val loss: 1.7949883937835693
Epoch 70, training loss: 1.8457964658737183 = 1.7662042379379272 + 0.01 * 7.959217548370361
Epoch 70, val loss: 1.7587120532989502
Epoch 80, training loss: 1.7975406646728516 = 1.7220591306686401 + 0.01 * 7.548147678375244
Epoch 80, val loss: 1.7221293449401855
Epoch 90, training loss: 1.7363195419311523 = 1.6638036966323853 + 0.01 * 7.251590728759766
Epoch 90, val loss: 1.6726094484329224
Epoch 100, training loss: 1.6571015119552612 = 1.5857876539230347 + 0.01 * 7.131385326385498
Epoch 100, val loss: 1.6070759296417236
Epoch 110, training loss: 1.558849811553955 = 1.488385558128357 + 0.01 * 7.0464277267456055
Epoch 110, val loss: 1.5281189680099487
Epoch 120, training loss: 1.4527735710144043 = 1.3828097581863403 + 0.01 * 6.9963765144348145
Epoch 120, val loss: 1.4469953775405884
Epoch 130, training loss: 1.347906470298767 = 1.2782655954360962 + 0.01 * 6.964083194732666
Epoch 130, val loss: 1.3702856302261353
Epoch 140, training loss: 1.2446037530899048 = 1.1751365661621094 + 0.01 * 6.946723461151123
Epoch 140, val loss: 1.2973922491073608
Epoch 150, training loss: 1.1416336297988892 = 1.072318196296692 + 0.01 * 6.931543350219727
Epoch 150, val loss: 1.2238304615020752
Epoch 160, training loss: 1.039677619934082 = 0.9705110788345337 + 0.01 * 6.916649341583252
Epoch 160, val loss: 1.1506539583206177
Epoch 170, training loss: 0.9424267411231995 = 0.8733710646629333 + 0.01 * 6.905569553375244
Epoch 170, val loss: 1.0814248323440552
Epoch 180, training loss: 0.8545724749565125 = 0.7855704426765442 + 0.01 * 6.900205612182617
Epoch 180, val loss: 1.0201971530914307
Epoch 190, training loss: 0.7788676023483276 = 0.7099041938781738 + 0.01 * 6.8963398933410645
Epoch 190, val loss: 0.9692559242248535
Epoch 200, training loss: 0.7142817974090576 = 0.6453580260276794 + 0.01 * 6.892377853393555
Epoch 200, val loss: 0.9282142519950867
Epoch 210, training loss: 0.6575627326965332 = 0.5886878967285156 + 0.01 * 6.887482643127441
Epoch 210, val loss: 0.8947345018386841
Epoch 220, training loss: 0.6058979630470276 = 0.537078320980072 + 0.01 * 6.881965637207031
Epoch 220, val loss: 0.8665453195571899
Epoch 230, training loss: 0.5576297640800476 = 0.4888586103916168 + 0.01 * 6.877114295959473
Epoch 230, val loss: 0.8423505425453186
Epoch 240, training loss: 0.5119822025299072 = 0.4432847797870636 + 0.01 * 6.869744300842285
Epoch 240, val loss: 0.8217024207115173
Epoch 250, training loss: 0.4687868058681488 = 0.4001610577106476 + 0.01 * 6.862574577331543
Epoch 250, val loss: 0.8045774698257446
Epoch 260, training loss: 0.4278552234172821 = 0.3592982590198517 + 0.01 * 6.855695724487305
Epoch 260, val loss: 0.7910207509994507
Epoch 270, training loss: 0.38911786675453186 = 0.32060617208480835 + 0.01 * 6.851170063018799
Epoch 270, val loss: 0.7805064916610718
Epoch 280, training loss: 0.35242047905921936 = 0.2839832603931427 + 0.01 * 6.843722820281982
Epoch 280, val loss: 0.7728461027145386
Epoch 290, training loss: 0.31784406304359436 = 0.24946358799934387 + 0.01 * 6.838048458099365
Epoch 290, val loss: 0.7681126594543457
Epoch 300, training loss: 0.2856117784976959 = 0.21726536750793457 + 0.01 * 6.834641456604004
Epoch 300, val loss: 0.7659603953361511
Epoch 310, training loss: 0.25651535391807556 = 0.1881725937128067 + 0.01 * 6.834275245666504
Epoch 310, val loss: 0.7670789361000061
Epoch 320, training loss: 0.23094436526298523 = 0.16261880099773407 + 0.01 * 6.832557201385498
Epoch 320, val loss: 0.7710978388786316
Epoch 330, training loss: 0.20888639986515045 = 0.1406112164258957 + 0.01 * 6.827518463134766
Epoch 330, val loss: 0.7782990336418152
Epoch 340, training loss: 0.19017226994037628 = 0.1218995749950409 + 0.01 * 6.827269554138184
Epoch 340, val loss: 0.7881404757499695
Epoch 350, training loss: 0.17429906129837036 = 0.10610122978687286 + 0.01 * 6.819782257080078
Epoch 350, val loss: 0.7999787330627441
Epoch 360, training loss: 0.16092103719711304 = 0.09276039898395538 + 0.01 * 6.816064357757568
Epoch 360, val loss: 0.8133059740066528
Epoch 370, training loss: 0.1495581567287445 = 0.08147800713777542 + 0.01 * 6.808015823364258
Epoch 370, val loss: 0.8276665806770325
Epoch 380, training loss: 0.13990093767642975 = 0.07190299779176712 + 0.01 * 6.7997941970825195
Epoch 380, val loss: 0.8425697088241577
Epoch 390, training loss: 0.1318131536245346 = 0.06374647468328476 + 0.01 * 6.806668281555176
Epoch 390, val loss: 0.8576869964599609
Epoch 400, training loss: 0.12469181418418884 = 0.05677950009703636 + 0.01 * 6.791231155395508
Epoch 400, val loss: 0.8727014064788818
Epoch 410, training loss: 0.11868572235107422 = 0.05080227926373482 + 0.01 * 6.788343906402588
Epoch 410, val loss: 0.887569010257721
Epoch 420, training loss: 0.1135302186012268 = 0.04565335437655449 + 0.01 * 6.787685871124268
Epoch 420, val loss: 0.9021685123443604
Epoch 430, training loss: 0.1090129166841507 = 0.04120771959424019 + 0.01 * 6.780519485473633
Epoch 430, val loss: 0.9164477586746216
Epoch 440, training loss: 0.10505495965480804 = 0.0373503677546978 + 0.01 * 6.770458698272705
Epoch 440, val loss: 0.9303634762763977
Epoch 450, training loss: 0.10158427059650421 = 0.03398735448718071 + 0.01 * 6.7596917152404785
Epoch 450, val loss: 0.9440245032310486
Epoch 460, training loss: 0.09866659343242645 = 0.031044719740748405 + 0.01 * 6.762187480926514
Epoch 460, val loss: 0.9573575854301453
Epoch 470, training loss: 0.09598991274833679 = 0.028465375304222107 + 0.01 * 6.752453804016113
Epoch 470, val loss: 0.9703289866447449
Epoch 480, training loss: 0.09363435953855515 = 0.026189832016825676 + 0.01 * 6.744452476501465
Epoch 480, val loss: 0.9829622507095337
Epoch 490, training loss: 0.09152405709028244 = 0.024173526093363762 + 0.01 * 6.735053062438965
Epoch 490, val loss: 0.9952643513679504
Epoch 500, training loss: 0.08980470150709152 = 0.022381432354450226 + 0.01 * 6.7423272132873535
Epoch 500, val loss: 1.007272481918335
Epoch 510, training loss: 0.08802992850542068 = 0.02078511007130146 + 0.01 * 6.724482536315918
Epoch 510, val loss: 1.0189300775527954
Epoch 520, training loss: 0.08653835207223892 = 0.01935671456158161 + 0.01 * 6.718164443969727
Epoch 520, val loss: 1.0302152633666992
Epoch 530, training loss: 0.08532706648111343 = 0.01807422935962677 + 0.01 * 6.725283622741699
Epoch 530, val loss: 1.041218638420105
Epoch 540, training loss: 0.08413530141115189 = 0.016919821500778198 + 0.01 * 6.721548080444336
Epoch 540, val loss: 1.0518438816070557
Epoch 550, training loss: 0.08272260427474976 = 0.01587679237127304 + 0.01 * 6.684581279754639
Epoch 550, val loss: 1.0622085332870483
Epoch 560, training loss: 0.08183658868074417 = 0.014931365847587585 + 0.01 * 6.69052267074585
Epoch 560, val loss: 1.0722849369049072
Epoch 570, training loss: 0.0808788314461708 = 0.014072634279727936 + 0.01 * 6.680620193481445
Epoch 570, val loss: 1.082087516784668
Epoch 580, training loss: 0.0798778161406517 = 0.013289290480315685 + 0.01 * 6.658853054046631
Epoch 580, val loss: 1.091610312461853
Epoch 590, training loss: 0.0793147161602974 = 0.012573211453855038 + 0.01 * 6.674150466918945
Epoch 590, val loss: 1.1008425951004028
Epoch 600, training loss: 0.07843872159719467 = 0.011918434873223305 + 0.01 * 6.652029037475586
Epoch 600, val loss: 1.1098055839538574
Epoch 610, training loss: 0.07775749266147614 = 0.011316926218569279 + 0.01 * 6.644056797027588
Epoch 610, val loss: 1.1185663938522339
Epoch 620, training loss: 0.07719971984624863 = 0.010762556456029415 + 0.01 * 6.643716335296631
Epoch 620, val loss: 1.127079725265503
Epoch 630, training loss: 0.07661090791225433 = 0.010251062922179699 + 0.01 * 6.635984897613525
Epoch 630, val loss: 1.1353579759597778
Epoch 640, training loss: 0.07632071524858475 = 0.009778148494660854 + 0.01 * 6.654256820678711
Epoch 640, val loss: 1.1435189247131348
Epoch 650, training loss: 0.07552777230739594 = 0.009339800104498863 + 0.01 * 6.6187968254089355
Epoch 650, val loss: 1.1513525247573853
Epoch 660, training loss: 0.07496548444032669 = 0.008933045901358128 + 0.01 * 6.603244304656982
Epoch 660, val loss: 1.159058690071106
Epoch 670, training loss: 0.07472368329763412 = 0.008554632775485516 + 0.01 * 6.616905212402344
Epoch 670, val loss: 1.1665009260177612
Epoch 680, training loss: 0.07420552521944046 = 0.008201743476092815 + 0.01 * 6.600378513336182
Epoch 680, val loss: 1.1738170385360718
Epoch 690, training loss: 0.07418949156999588 = 0.007871771231293678 + 0.01 * 6.631772041320801
Epoch 690, val loss: 1.1809605360031128
Epoch 700, training loss: 0.07344166934490204 = 0.007564125582575798 + 0.01 * 6.587754249572754
Epoch 700, val loss: 1.1879336833953857
Epoch 710, training loss: 0.07315786182880402 = 0.007274676114320755 + 0.01 * 6.588319301605225
Epoch 710, val loss: 1.1947084665298462
Epoch 720, training loss: 0.07297176122665405 = 0.007003186736255884 + 0.01 * 6.59685754776001
Epoch 720, val loss: 1.20133376121521
Epoch 730, training loss: 0.0723860114812851 = 0.0067487419582903385 + 0.01 * 6.563726902008057
Epoch 730, val loss: 1.207871913909912
Epoch 740, training loss: 0.07231194525957108 = 0.006509192753583193 + 0.01 * 6.580275535583496
Epoch 740, val loss: 1.2142181396484375
Epoch 750, training loss: 0.07219268381595612 = 0.006282961927354336 + 0.01 * 6.590971946716309
Epoch 750, val loss: 1.2204111814498901
Epoch 760, training loss: 0.07179902493953705 = 0.0060700005851686 + 0.01 * 6.572902679443359
Epoch 760, val loss: 1.2265688180923462
Epoch 770, training loss: 0.07165099680423737 = 0.0058684758841991425 + 0.01 * 6.578252792358398
Epoch 770, val loss: 1.2325501441955566
Epoch 780, training loss: 0.07146116346120834 = 0.005678134970366955 + 0.01 * 6.578303337097168
Epoch 780, val loss: 1.2384175062179565
Epoch 790, training loss: 0.07104112207889557 = 0.0054974243976175785 + 0.01 * 6.554369926452637
Epoch 790, val loss: 1.244077444076538
Epoch 800, training loss: 0.07079389691352844 = 0.005326720420271158 + 0.01 * 6.546718120574951
Epoch 800, val loss: 1.2497738599777222
Epoch 810, training loss: 0.07044064253568649 = 0.005164438858628273 + 0.01 * 6.527620792388916
Epoch 810, val loss: 1.2552520036697388
Epoch 820, training loss: 0.07065396755933762 = 0.0050103203393518925 + 0.01 * 6.564364433288574
Epoch 820, val loss: 1.260643482208252
Epoch 830, training loss: 0.07021788507699966 = 0.004864474292844534 + 0.01 * 6.535340785980225
Epoch 830, val loss: 1.265961766242981
Epoch 840, training loss: 0.06990720331668854 = 0.004725497215986252 + 0.01 * 6.518171310424805
Epoch 840, val loss: 1.2711281776428223
Epoch 850, training loss: 0.07005853950977325 = 0.004593185149133205 + 0.01 * 6.546535491943359
Epoch 850, val loss: 1.2762017250061035
Epoch 860, training loss: 0.0697176456451416 = 0.00446718093007803 + 0.01 * 6.525046348571777
Epoch 860, val loss: 1.2812230587005615
Epoch 870, training loss: 0.06947195529937744 = 0.0043470184318721294 + 0.01 * 6.512494087219238
Epoch 870, val loss: 1.2861065864562988
Epoch 880, training loss: 0.06928564608097076 = 0.0042321826331317425 + 0.01 * 6.505346298217773
Epoch 880, val loss: 1.2908499240875244
Epoch 890, training loss: 0.06924855709075928 = 0.004122693557292223 + 0.01 * 6.5125861167907715
Epoch 890, val loss: 1.2956234216690063
Epoch 900, training loss: 0.06900846213102341 = 0.004017910920083523 + 0.01 * 6.4990553855896
Epoch 900, val loss: 1.3001997470855713
Epoch 910, training loss: 0.06887182593345642 = 0.003917990252375603 + 0.01 * 6.4953837394714355
Epoch 910, val loss: 1.3047682046890259
Epoch 920, training loss: 0.06897743046283722 = 0.0038224479649215937 + 0.01 * 6.515498638153076
Epoch 920, val loss: 1.3092526197433472
Epoch 930, training loss: 0.06867202371358871 = 0.003730567405000329 + 0.01 * 6.49414587020874
Epoch 930, val loss: 1.3136018514633179
Epoch 940, training loss: 0.06845489889383316 = 0.0036429031752049923 + 0.01 * 6.481200218200684
Epoch 940, val loss: 1.318006157875061
Epoch 950, training loss: 0.06856900453567505 = 0.0035588338505476713 + 0.01 * 6.501016616821289
Epoch 950, val loss: 1.3222347497940063
Epoch 960, training loss: 0.0682426169514656 = 0.0034782036673277617 + 0.01 * 6.476441383361816
Epoch 960, val loss: 1.3263658285140991
Epoch 970, training loss: 0.06816466897726059 = 0.0034009283408522606 + 0.01 * 6.47637414932251
Epoch 970, val loss: 1.3304829597473145
Epoch 980, training loss: 0.0681103765964508 = 0.003326778532937169 + 0.01 * 6.478359699249268
Epoch 980, val loss: 1.3345242738723755
Epoch 990, training loss: 0.06793175637722015 = 0.0032552022021263838 + 0.01 * 6.467655658721924
Epoch 990, val loss: 1.3384345769882202
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.68512, 0.21746, Accuracy:0.80370, 0.01090
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11622])
remove edge: torch.Size([2, 9526])
updated graph: torch.Size([2, 10592])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.98032, 0.00627, Accuracy:0.83086, 0.00462
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0187313556671143 = 1.9349929094314575 + 0.01 * 8.373847961425781
Epoch 0, val loss: 1.9357779026031494
Epoch 10, training loss: 2.009235143661499 = 1.9254975318908691 + 0.01 * 8.373763084411621
Epoch 10, val loss: 1.9264382123947144
Epoch 20, training loss: 1.9975595474243164 = 1.913824439048767 + 0.01 * 8.373517036437988
Epoch 20, val loss: 1.914820671081543
Epoch 30, training loss: 1.9809969663619995 = 1.8972681760787964 + 0.01 * 8.372878074645996
Epoch 30, val loss: 1.898478388786316
Epoch 40, training loss: 1.9561994075775146 = 1.8725003004074097 + 0.01 * 8.36990737915039
Epoch 40, val loss: 1.8746610879898071
Epoch 50, training loss: 1.9210338592529297 = 1.837562084197998 + 0.01 * 8.347172737121582
Epoch 50, val loss: 1.8428102731704712
Epoch 60, training loss: 1.8797398805618286 = 1.7975908517837524 + 0.01 * 8.214900970458984
Epoch 60, val loss: 1.8095779418945312
Epoch 70, training loss: 1.8379888534545898 = 1.758899450302124 + 0.01 * 7.908941268920898
Epoch 70, val loss: 1.7776453495025635
Epoch 80, training loss: 1.7852109670639038 = 1.7082998752593994 + 0.01 * 7.691112041473389
Epoch 80, val loss: 1.731594443321228
Epoch 90, training loss: 1.7135545015335083 = 1.6390682458877563 + 0.01 * 7.448625087738037
Epoch 90, val loss: 1.6713242530822754
Epoch 100, training loss: 1.6237810850143433 = 1.550859808921814 + 0.01 * 7.292132377624512
Epoch 100, val loss: 1.5991698503494263
Epoch 110, training loss: 1.5260629653930664 = 1.4538023471832275 + 0.01 * 7.226065158843994
Epoch 110, val loss: 1.5198004245758057
Epoch 120, training loss: 1.4264718294143677 = 1.3550833463668823 + 0.01 * 7.138851165771484
Epoch 120, val loss: 1.4417016506195068
Epoch 130, training loss: 1.3258647918701172 = 1.2553350925445557 + 0.01 * 7.052972316741943
Epoch 130, val loss: 1.3634943962097168
Epoch 140, training loss: 1.2240653038024902 = 1.1540898084640503 + 0.01 * 6.997549533843994
Epoch 140, val loss: 1.285796046257019
Epoch 150, training loss: 1.1235713958740234 = 1.0538891553878784 + 0.01 * 6.968222618103027
Epoch 150, val loss: 1.2100379467010498
Epoch 160, training loss: 1.0279101133346558 = 0.958412230014801 + 0.01 * 6.949793815612793
Epoch 160, val loss: 1.1387600898742676
Epoch 170, training loss: 0.9388716816902161 = 0.8694795370101929 + 0.01 * 6.939214706420898
Epoch 170, val loss: 1.0732872486114502
Epoch 180, training loss: 0.8564288020133972 = 0.7871063947677612 + 0.01 * 6.932241439819336
Epoch 180, val loss: 1.0130990743637085
Epoch 190, training loss: 0.7802489399909973 = 0.7109776735305786 + 0.01 * 6.927126884460449
Epoch 190, val loss: 0.9589899182319641
Epoch 200, training loss: 0.7099157571792603 = 0.6406832337379456 + 0.01 * 6.923253536224365
Epoch 200, val loss: 0.9116612672805786
Epoch 210, training loss: 0.6449611783027649 = 0.5757685303688049 + 0.01 * 6.919265270233154
Epoch 210, val loss: 0.8718538880348206
Epoch 220, training loss: 0.5849646329879761 = 0.5158039331436157 + 0.01 * 6.916073322296143
Epoch 220, val loss: 0.8391719460487366
Epoch 230, training loss: 0.5296502113342285 = 0.4605298936367035 + 0.01 * 6.912028789520264
Epoch 230, val loss: 0.8129045367240906
Epoch 240, training loss: 0.4789755344390869 = 0.409891813993454 + 0.01 * 6.9083733558654785
Epoch 240, val loss: 0.792031466960907
Epoch 250, training loss: 0.4329764246940613 = 0.36392083764076233 + 0.01 * 6.905559539794922
Epoch 250, val loss: 0.7759407162666321
Epoch 260, training loss: 0.39146536588668823 = 0.3224482834339142 + 0.01 * 6.901707172393799
Epoch 260, val loss: 0.7642958164215088
Epoch 270, training loss: 0.35388731956481934 = 0.2848953902721405 + 0.01 * 6.899194240570068
Epoch 270, val loss: 0.7563783526420593
Epoch 280, training loss: 0.31956881284713745 = 0.25058263540267944 + 0.01 * 6.898618698120117
Epoch 280, val loss: 0.751711368560791
Epoch 290, training loss: 0.2880287766456604 = 0.21907325088977814 + 0.01 * 6.895553112030029
Epoch 290, val loss: 0.7500542998313904
Epoch 300, training loss: 0.2592925727367401 = 0.1903633028268814 + 0.01 * 6.892927169799805
Epoch 300, val loss: 0.7514477372169495
Epoch 310, training loss: 0.23370683193206787 = 0.1647491455078125 + 0.01 * 6.895768642425537
Epoch 310, val loss: 0.7557693719863892
Epoch 320, training loss: 0.21131277084350586 = 0.14243821799755096 + 0.01 * 6.887455940246582
Epoch 320, val loss: 0.7628364562988281
Epoch 330, training loss: 0.1921614408493042 = 0.12335003912448883 + 0.01 * 6.881140232086182
Epoch 330, val loss: 0.7724242806434631
Epoch 340, training loss: 0.17596599459648132 = 0.1071532815694809 + 0.01 * 6.881272315979004
Epoch 340, val loss: 0.7841410636901855
Epoch 350, training loss: 0.1621362864971161 = 0.09344038367271423 + 0.01 * 6.869589328765869
Epoch 350, val loss: 0.7975949645042419
Epoch 360, training loss: 0.1504591703414917 = 0.08182400465011597 + 0.01 * 6.863516330718994
Epoch 360, val loss: 0.8123537302017212
Epoch 370, training loss: 0.14058710634708405 = 0.0719621405005455 + 0.01 * 6.862496852874756
Epoch 370, val loss: 0.8280258774757385
Epoch 380, training loss: 0.13202734291553497 = 0.06355088949203491 + 0.01 * 6.847645282745361
Epoch 380, val loss: 0.844318151473999
Epoch 390, training loss: 0.12468172609806061 = 0.056346964091062546 + 0.01 * 6.833477020263672
Epoch 390, val loss: 0.8610625267028809
Epoch 400, training loss: 0.11861830204725266 = 0.05015689134597778 + 0.01 * 6.846141338348389
Epoch 400, val loss: 0.877959668636322
Epoch 410, training loss: 0.11304497718811035 = 0.044830918312072754 + 0.01 * 6.821406364440918
Epoch 410, val loss: 0.8948766589164734
Epoch 420, training loss: 0.1082790344953537 = 0.0402347557246685 + 0.01 * 6.804427623748779
Epoch 420, val loss: 0.9116259217262268
Epoch 430, training loss: 0.10443100333213806 = 0.036254677921533585 + 0.01 * 6.81763219833374
Epoch 430, val loss: 0.9280782341957092
Epoch 440, training loss: 0.10077791661024094 = 0.032801687717437744 + 0.01 * 6.797623157501221
Epoch 440, val loss: 0.9440628290176392
Epoch 450, training loss: 0.09762777388095856 = 0.029793284833431244 + 0.01 * 6.783449172973633
Epoch 450, val loss: 0.9595885276794434
Epoch 460, training loss: 0.09508059918880463 = 0.027163013815879822 + 0.01 * 6.791759014129639
Epoch 460, val loss: 0.9745199680328369
Epoch 470, training loss: 0.09264034032821655 = 0.02486283704638481 + 0.01 * 6.7777509689331055
Epoch 470, val loss: 0.9890540242195129
Epoch 480, training loss: 0.0903857871890068 = 0.02283911220729351 + 0.01 * 6.75466775894165
Epoch 480, val loss: 1.0030323266983032
Epoch 490, training loss: 0.0885051041841507 = 0.021048948168754578 + 0.01 * 6.745615482330322
Epoch 490, val loss: 1.0165398120880127
Epoch 500, training loss: 0.0868370309472084 = 0.019458815455436707 + 0.01 * 6.737821578979492
Epoch 500, val loss: 1.0296274423599243
Epoch 510, training loss: 0.08538535982370377 = 0.018042242154479027 + 0.01 * 6.734311580657959
Epoch 510, val loss: 1.04220449924469
Epoch 520, training loss: 0.08413833379745483 = 0.016780419275164604 + 0.01 * 6.7357916831970215
Epoch 520, val loss: 1.0545099973678589
Epoch 530, training loss: 0.08282177150249481 = 0.015649383887648582 + 0.01 * 6.717238903045654
Epoch 530, val loss: 1.066262125968933
Epoch 540, training loss: 0.08171705901622772 = 0.014630304649472237 + 0.01 * 6.708675861358643
Epoch 540, val loss: 1.0777374505996704
Epoch 550, training loss: 0.08074251562356949 = 0.013709940947592258 + 0.01 * 6.7032575607299805
Epoch 550, val loss: 1.0887821912765503
Epoch 560, training loss: 0.07988523691892624 = 0.012877250090241432 + 0.01 * 6.700798988342285
Epoch 560, val loss: 1.0995588302612305
Epoch 570, training loss: 0.07904506474733353 = 0.012120679020881653 + 0.01 * 6.69243860244751
Epoch 570, val loss: 1.1100046634674072
Epoch 580, training loss: 0.07832489907741547 = 0.01143241673707962 + 0.01 * 6.689248561859131
Epoch 580, val loss: 1.1200971603393555
Epoch 590, training loss: 0.07769901305437088 = 0.010805313475430012 + 0.01 * 6.6893696784973145
Epoch 590, val loss: 1.129826545715332
Epoch 600, training loss: 0.07704076170921326 = 0.010231410153210163 + 0.01 * 6.680934906005859
Epoch 600, val loss: 1.1393567323684692
Epoch 610, training loss: 0.07632274925708771 = 0.009704582393169403 + 0.01 * 6.661816596984863
Epoch 610, val loss: 1.1485588550567627
Epoch 620, training loss: 0.07611765712499619 = 0.009219588711857796 + 0.01 * 6.689806938171387
Epoch 620, val loss: 1.1574296951293945
Epoch 630, training loss: 0.07537567615509033 = 0.008773593232035637 + 0.01 * 6.660208225250244
Epoch 630, val loss: 1.1662559509277344
Epoch 640, training loss: 0.07485941052436829 = 0.008361514657735825 + 0.01 * 6.649790287017822
Epoch 640, val loss: 1.1746940612792969
Epoch 650, training loss: 0.07440786808729172 = 0.007979968562722206 + 0.01 * 6.6427903175354
Epoch 650, val loss: 1.1829161643981934
Epoch 660, training loss: 0.07417003810405731 = 0.00762593001127243 + 0.01 * 6.654411315917969
Epoch 660, val loss: 1.190925121307373
Epoch 670, training loss: 0.07367175817489624 = 0.007297181524336338 + 0.01 * 6.637457847595215
Epoch 670, val loss: 1.1988211870193481
Epoch 680, training loss: 0.07328399270772934 = 0.006990994792431593 + 0.01 * 6.629300117492676
Epoch 680, val loss: 1.2064117193222046
Epoch 690, training loss: 0.07306557148694992 = 0.006705671548843384 + 0.01 * 6.635990142822266
Epoch 690, val loss: 1.2137584686279297
Epoch 700, training loss: 0.07267382740974426 = 0.006439576856791973 + 0.01 * 6.623425483703613
Epoch 700, val loss: 1.221014142036438
Epoch 710, training loss: 0.07234180718660355 = 0.006190616171807051 + 0.01 * 6.615119457244873
Epoch 710, val loss: 1.2279964685440063
Epoch 720, training loss: 0.07217201590538025 = 0.005957482848316431 + 0.01 * 6.621453285217285
Epoch 720, val loss: 1.234938621520996
Epoch 730, training loss: 0.07177480310201645 = 0.005738521460443735 + 0.01 * 6.603628635406494
Epoch 730, val loss: 1.2416150569915771
Epoch 740, training loss: 0.07175637036561966 = 0.005532616749405861 + 0.01 * 6.622375965118408
Epoch 740, val loss: 1.2482104301452637
Epoch 750, training loss: 0.0713530108332634 = 0.005339016672223806 + 0.01 * 6.6013994216918945
Epoch 750, val loss: 1.254508376121521
Epoch 760, training loss: 0.0712318941950798 = 0.005156814586371183 + 0.01 * 6.607508182525635
Epoch 760, val loss: 1.2607299089431763
Epoch 770, training loss: 0.0709749385714531 = 0.004985088482499123 + 0.01 * 6.59898567199707
Epoch 770, val loss: 1.2668339014053345
Epoch 780, training loss: 0.07079686969518661 = 0.004822936840355396 + 0.01 * 6.597393035888672
Epoch 780, val loss: 1.272777795791626
Epoch 790, training loss: 0.07044626027345657 = 0.00466963043436408 + 0.01 * 6.577663421630859
Epoch 790, val loss: 1.2785488367080688
Epoch 800, training loss: 0.070318304002285 = 0.004524612799286842 + 0.01 * 6.57936954498291
Epoch 800, val loss: 1.2841664552688599
Epoch 810, training loss: 0.07015768438577652 = 0.0043873474933207035 + 0.01 * 6.577033996582031
Epoch 810, val loss: 1.2897257804870605
Epoch 820, training loss: 0.0701051577925682 = 0.004257285036146641 + 0.01 * 6.584787368774414
Epoch 820, val loss: 1.2952507734298706
Epoch 830, training loss: 0.06981982290744781 = 0.004133669659495354 + 0.01 * 6.568614959716797
Epoch 830, val loss: 1.300456166267395
Epoch 840, training loss: 0.06990810483694077 = 0.004016290884464979 + 0.01 * 6.589181900024414
Epoch 840, val loss: 1.3056312799453735
Epoch 850, training loss: 0.06961667537689209 = 0.0039048055186867714 + 0.01 * 6.571187496185303
Epoch 850, val loss: 1.3107223510742188
Epoch 860, training loss: 0.06950385868549347 = 0.0037988449912518263 + 0.01 * 6.570501804351807
Epoch 860, val loss: 1.3156956434249878
Epoch 870, training loss: 0.0691947191953659 = 0.0036980079021304846 + 0.01 * 6.549671649932861
Epoch 870, val loss: 1.3205454349517822
Epoch 880, training loss: 0.06907910853624344 = 0.0036018050741404295 + 0.01 * 6.547730922698975
Epoch 880, val loss: 1.325282096862793
Epoch 890, training loss: 0.06896796077489853 = 0.003509898902848363 + 0.01 * 6.545806407928467
Epoch 890, val loss: 1.3298859596252441
Epoch 900, training loss: 0.06883788853883743 = 0.003422356443479657 + 0.01 * 6.541553497314453
Epoch 900, val loss: 1.3344541788101196
Epoch 910, training loss: 0.06876605749130249 = 0.003338629612699151 + 0.01 * 6.54274320602417
Epoch 910, val loss: 1.338803768157959
Epoch 920, training loss: 0.0686558336019516 = 0.003258956829085946 + 0.01 * 6.539687633514404
Epoch 920, val loss: 1.343255877494812
Epoch 930, training loss: 0.06848487257957458 = 0.003182709449902177 + 0.01 * 6.530216217041016
Epoch 930, val loss: 1.3473496437072754
Epoch 940, training loss: 0.06851603090763092 = 0.0031098397448658943 + 0.01 * 6.540618896484375
Epoch 940, val loss: 1.3515721559524536
Epoch 950, training loss: 0.06825897842645645 = 0.003039838280528784 + 0.01 * 6.521914005279541
Epoch 950, val loss: 1.355637788772583
Epoch 960, training loss: 0.06836315989494324 = 0.002972623100504279 + 0.01 * 6.539053440093994
Epoch 960, val loss: 1.3595534563064575
Epoch 970, training loss: 0.0681053027510643 = 0.002908374648541212 + 0.01 * 6.519693374633789
Epoch 970, val loss: 1.3635321855545044
Epoch 980, training loss: 0.06807690113782883 = 0.0028466617222875357 + 0.01 * 6.523024559020996
Epoch 980, val loss: 1.367327332496643
Epoch 990, training loss: 0.0680672898888588 = 0.002787466626614332 + 0.01 * 6.527982711791992
Epoch 990, val loss: 1.3710694313049316
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.5240
Flip ASR: 0.4311/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.03945255279541 = 1.955714464187622 + 0.01 * 8.373800277709961
Epoch 0, val loss: 1.954111099243164
Epoch 10, training loss: 2.028688669204712 = 1.9449516534805298 + 0.01 * 8.37370777130127
Epoch 10, val loss: 1.942296028137207
Epoch 20, training loss: 2.0158474445343018 = 1.9321132898330688 + 0.01 * 8.373408317565918
Epoch 20, val loss: 1.9278465509414673
Epoch 30, training loss: 1.9981151819229126 = 1.9143896102905273 + 0.01 * 8.372553825378418
Epoch 30, val loss: 1.9077788591384888
Epoch 40, training loss: 1.9722340106964111 = 1.8885488510131836 + 0.01 * 8.368520736694336
Epoch 40, val loss: 1.8787150382995605
Epoch 50, training loss: 1.9353750944137573 = 1.8519694805145264 + 0.01 * 8.340566635131836
Epoch 50, val loss: 1.8390493392944336
Epoch 60, training loss: 1.889665961265564 = 1.8079215288162231 + 0.01 * 8.174440383911133
Epoch 60, val loss: 1.7953797578811646
Epoch 70, training loss: 1.84518301486969 = 1.766745924949646 + 0.01 * 7.843708515167236
Epoch 70, val loss: 1.7602559328079224
Epoch 80, training loss: 1.7959438562393188 = 1.7215293645858765 + 0.01 * 7.441447734832764
Epoch 80, val loss: 1.723939299583435
Epoch 90, training loss: 1.7310633659362793 = 1.6590560674667358 + 0.01 * 7.2007293701171875
Epoch 90, val loss: 1.6711642742156982
Epoch 100, training loss: 1.6459747552871704 = 1.5750620365142822 + 0.01 * 7.091268539428711
Epoch 100, val loss: 1.5986989736557007
Epoch 110, training loss: 1.544547200202942 = 1.474192500114441 + 0.01 * 7.0354743003845215
Epoch 110, val loss: 1.5151994228363037
Epoch 120, training loss: 1.4387301206588745 = 1.3687113523483276 + 0.01 * 7.0018744468688965
Epoch 120, val loss: 1.4333027601242065
Epoch 130, training loss: 1.3361098766326904 = 1.2663315534591675 + 0.01 * 6.977833271026611
Epoch 130, val loss: 1.3564136028289795
Epoch 140, training loss: 1.2395626306533813 = 1.1700118780136108 + 0.01 * 6.955080032348633
Epoch 140, val loss: 1.2865463495254517
Epoch 150, training loss: 1.150967001914978 = 1.0816597938537598 + 0.01 * 6.930717468261719
Epoch 150, val loss: 1.2221786975860596
Epoch 160, training loss: 1.0710135698318481 = 1.0019385814666748 + 0.01 * 6.907504558563232
Epoch 160, val loss: 1.163801908493042
Epoch 170, training loss: 0.9982240200042725 = 0.929279625415802 + 0.01 * 6.894442081451416
Epoch 170, val loss: 1.1108311414718628
Epoch 180, training loss: 0.9290838837623596 = 0.8602404594421387 + 0.01 * 6.884340286254883
Epoch 180, val loss: 1.061113715171814
Epoch 190, training loss: 0.8599698543548584 = 0.7912170886993408 + 0.01 * 6.875278472900391
Epoch 190, val loss: 1.0119643211364746
Epoch 200, training loss: 0.7887955904006958 = 0.7201361656188965 + 0.01 * 6.8659443855285645
Epoch 200, val loss: 0.9615679979324341
Epoch 210, training loss: 0.7162842750549316 = 0.6477213501930237 + 0.01 * 6.856293201446533
Epoch 210, val loss: 0.9102774262428284
Epoch 220, training loss: 0.6451697945594788 = 0.5767289996147156 + 0.01 * 6.844079494476318
Epoch 220, val loss: 0.8606593012809753
Epoch 230, training loss: 0.5786265134811401 = 0.510302722454071 + 0.01 * 6.832381725311279
Epoch 230, val loss: 0.8152152299880981
Epoch 240, training loss: 0.5185303092002869 = 0.45029059052467346 + 0.01 * 6.823973655700684
Epoch 240, val loss: 0.776020348072052
Epoch 250, training loss: 0.46496841311454773 = 0.396881639957428 + 0.01 * 6.808676719665527
Epoch 250, val loss: 0.743783712387085
Epoch 260, training loss: 0.41727012395858765 = 0.34922727942466736 + 0.01 * 6.804286003112793
Epoch 260, val loss: 0.7187188267707825
Epoch 270, training loss: 0.3741232752799988 = 0.30617251992225647 + 0.01 * 6.795074939727783
Epoch 270, val loss: 0.6997021436691284
Epoch 280, training loss: 0.33517974615097046 = 0.26726192235946655 + 0.01 * 6.791780948638916
Epoch 280, val loss: 0.6859487891197205
Epoch 290, training loss: 0.30039405822753906 = 0.23259006440639496 + 0.01 * 6.780398845672607
Epoch 290, val loss: 0.6769160032272339
Epoch 300, training loss: 0.2700180113315582 = 0.20226356387138367 + 0.01 * 6.775444984436035
Epoch 300, val loss: 0.6725173592567444
Epoch 310, training loss: 0.24388189613819122 = 0.17624393105506897 + 0.01 * 6.763796329498291
Epoch 310, val loss: 0.6723372340202332
Epoch 320, training loss: 0.22179488837718964 = 0.15418054163455963 + 0.01 * 6.761434555053711
Epoch 320, val loss: 0.6758280992507935
Epoch 330, training loss: 0.2032165676355362 = 0.13568507134914398 + 0.01 * 6.75314998626709
Epoch 330, val loss: 0.6822881698608398
Epoch 340, training loss: 0.18796007335186005 = 0.12028735131025314 + 0.01 * 6.767272472381592
Epoch 340, val loss: 0.6913285255432129
Epoch 350, training loss: 0.17483742535114288 = 0.10740946233272552 + 0.01 * 6.742796421051025
Epoch 350, val loss: 0.7021660208702087
Epoch 360, training loss: 0.16388821601867676 = 0.09651195257902145 + 0.01 * 6.737625598907471
Epoch 360, val loss: 0.7144299149513245
Epoch 370, training loss: 0.1545385867357254 = 0.0872335210442543 + 0.01 * 6.730506420135498
Epoch 370, val loss: 0.7276016473770142
Epoch 380, training loss: 0.1464889943599701 = 0.07925653457641602 + 0.01 * 6.7232465744018555
Epoch 380, val loss: 0.7414517998695374
Epoch 390, training loss: 0.13955235481262207 = 0.07234511524438858 + 0.01 * 6.720725059509277
Epoch 390, val loss: 0.7558842897415161
Epoch 400, training loss: 0.13343805074691772 = 0.06631811708211899 + 0.01 * 6.71199369430542
Epoch 400, val loss: 0.7709833383560181
Epoch 410, training loss: 0.12819595634937286 = 0.06102875620126724 + 0.01 * 6.716720104217529
Epoch 410, val loss: 0.7861781120300293
Epoch 420, training loss: 0.12342190742492676 = 0.05636988952755928 + 0.01 * 6.705202102661133
Epoch 420, val loss: 0.8014248609542847
Epoch 430, training loss: 0.11930210888385773 = 0.052254222333431244 + 0.01 * 6.704788684844971
Epoch 430, val loss: 0.816657304763794
Epoch 440, training loss: 0.11561717838048935 = 0.04860226809978485 + 0.01 * 6.701490879058838
Epoch 440, val loss: 0.83182692527771
Epoch 450, training loss: 0.1123090386390686 = 0.045341383665800095 + 0.01 * 6.696766376495361
Epoch 450, val loss: 0.8466068506240845
Epoch 460, training loss: 0.1092948466539383 = 0.04241486266255379 + 0.01 * 6.687999248504639
Epoch 460, val loss: 0.8610557913780212
Epoch 470, training loss: 0.1065819263458252 = 0.0397697314620018 + 0.01 * 6.681219577789307
Epoch 470, val loss: 0.8753166794776917
Epoch 480, training loss: 0.10416950285434723 = 0.03736817464232445 + 0.01 * 6.680132865905762
Epoch 480, val loss: 0.8892205953598022
Epoch 490, training loss: 0.10196883976459503 = 0.03518463298678398 + 0.01 * 6.6784210205078125
Epoch 490, val loss: 0.9029080867767334
Epoch 500, training loss: 0.09988304227590561 = 0.03318618983030319 + 0.01 * 6.669685363769531
Epoch 500, val loss: 0.9161840677261353
Epoch 510, training loss: 0.09801338613033295 = 0.0313459187746048 + 0.01 * 6.666746616363525
Epoch 510, val loss: 0.9291365742683411
Epoch 520, training loss: 0.09627433866262436 = 0.02964165061712265 + 0.01 * 6.66326904296875
Epoch 520, val loss: 0.9417954683303833
Epoch 530, training loss: 0.09464214742183685 = 0.028063487261533737 + 0.01 * 6.657866477966309
Epoch 530, val loss: 0.9543268084526062
Epoch 540, training loss: 0.09309528023004532 = 0.026597099378705025 + 0.01 * 6.649818420410156
Epoch 540, val loss: 0.9662603139877319
Epoch 550, training loss: 0.0917942002415657 = 0.025228561833500862 + 0.01 * 6.656564235687256
Epoch 550, val loss: 0.9780523180961609
Epoch 560, training loss: 0.09042859077453613 = 0.023948801681399345 + 0.01 * 6.647978782653809
Epoch 560, val loss: 0.9898384213447571
Epoch 570, training loss: 0.08916079998016357 = 0.02274174615740776 + 0.01 * 6.641906261444092
Epoch 570, val loss: 1.001249074935913
Epoch 580, training loss: 0.08799165487289429 = 0.021603470668196678 + 0.01 * 6.638818264007568
Epoch 580, val loss: 1.0124609470367432
Epoch 590, training loss: 0.08683881163597107 = 0.02052566595375538 + 0.01 * 6.631315231323242
Epoch 590, val loss: 1.0236340761184692
Epoch 600, training loss: 0.08576910197734833 = 0.01950146071612835 + 0.01 * 6.626764297485352
Epoch 600, val loss: 1.0344574451446533
Epoch 610, training loss: 0.08482380211353302 = 0.018510926514863968 + 0.01 * 6.631287097930908
Epoch 610, val loss: 1.0453364849090576
Epoch 620, training loss: 0.08389070630073547 = 0.01756117306649685 + 0.01 * 6.632953643798828
Epoch 620, val loss: 1.0558713674545288
Epoch 630, training loss: 0.08285467326641083 = 0.01665472239255905 + 0.01 * 6.6199951171875
Epoch 630, val loss: 1.0663044452667236
Epoch 640, training loss: 0.08195686340332031 = 0.015786325559020042 + 0.01 * 6.617053985595703
Epoch 640, val loss: 1.0766322612762451
Epoch 650, training loss: 0.08107093721628189 = 0.014954748563468456 + 0.01 * 6.611618995666504
Epoch 650, val loss: 1.08686101436615
Epoch 660, training loss: 0.08031356334686279 = 0.014162138104438782 + 0.01 * 6.615142822265625
Epoch 660, val loss: 1.0970089435577393
Epoch 670, training loss: 0.07951779663562775 = 0.013412605971097946 + 0.01 * 6.610518932342529
Epoch 670, val loss: 1.106801152229309
Epoch 680, training loss: 0.07870244234800339 = 0.012705513276159763 + 0.01 * 6.5996928215026855
Epoch 680, val loss: 1.1167737245559692
Epoch 690, training loss: 0.07800902426242828 = 0.012039989233016968 + 0.01 * 6.5969038009643555
Epoch 690, val loss: 1.1263957023620605
Epoch 700, training loss: 0.07748042047023773 = 0.011411629617214203 + 0.01 * 6.606879234313965
Epoch 700, val loss: 1.1357053518295288
Epoch 710, training loss: 0.0768105536699295 = 0.010823730379343033 + 0.01 * 6.598681926727295
Epoch 710, val loss: 1.1449527740478516
Epoch 720, training loss: 0.07617659121751785 = 0.010272949002683163 + 0.01 * 6.5903639793396
Epoch 720, val loss: 1.1540617942810059
Epoch 730, training loss: 0.07561201602220535 = 0.009759475477039814 + 0.01 * 6.585254192352295
Epoch 730, val loss: 1.1627541780471802
Epoch 740, training loss: 0.07519537955522537 = 0.009282258339226246 + 0.01 * 6.591311931610107
Epoch 740, val loss: 1.1714450120925903
Epoch 750, training loss: 0.07461052387952805 = 0.008836805820465088 + 0.01 * 6.577371597290039
Epoch 750, val loss: 1.1798124313354492
Epoch 760, training loss: 0.07447013258934021 = 0.008420972153544426 + 0.01 * 6.604916095733643
Epoch 760, val loss: 1.1879181861877441
Epoch 770, training loss: 0.0737389400601387 = 0.00803245510905981 + 0.01 * 6.570648670196533
Epoch 770, val loss: 1.1959668397903442
Epoch 780, training loss: 0.0734698623418808 = 0.007668971084058285 + 0.01 * 6.580089092254639
Epoch 780, val loss: 1.203741431236267
Epoch 790, training loss: 0.07301685214042664 = 0.007328438572585583 + 0.01 * 6.568841457366943
Epoch 790, val loss: 1.2113451957702637
Epoch 800, training loss: 0.07273442298173904 = 0.007009867113083601 + 0.01 * 6.572456359863281
Epoch 800, val loss: 1.218653917312622
Epoch 810, training loss: 0.07254599034786224 = 0.006710808724164963 + 0.01 * 6.583518028259277
Epoch 810, val loss: 1.2257558107376099
Epoch 820, training loss: 0.072076715528965 = 0.006430913228541613 + 0.01 * 6.564579963684082
Epoch 820, val loss: 1.2327845096588135
Epoch 830, training loss: 0.07184867560863495 = 0.00616941973567009 + 0.01 * 6.567925930023193
Epoch 830, val loss: 1.2395318746566772
Epoch 840, training loss: 0.07151831686496735 = 0.005923936143517494 + 0.01 * 6.559438228607178
Epoch 840, val loss: 1.2462754249572754
Epoch 850, training loss: 0.07119972258806229 = 0.005694120191037655 + 0.01 * 6.550560474395752
Epoch 850, val loss: 1.252703070640564
Epoch 860, training loss: 0.0709574967622757 = 0.0054783206433057785 + 0.01 * 6.547917366027832
Epoch 860, val loss: 1.2589861154556274
Epoch 870, training loss: 0.07078017294406891 = 0.005276266019791365 + 0.01 * 6.55039119720459
Epoch 870, val loss: 1.2651598453521729
Epoch 880, training loss: 0.07052906602621078 = 0.005085781682282686 + 0.01 * 6.544328689575195
Epoch 880, val loss: 1.2712661027908325
Epoch 890, training loss: 0.07029158622026443 = 0.004906642250716686 + 0.01 * 6.53849458694458
Epoch 890, val loss: 1.2770256996154785
Epoch 900, training loss: 0.0703410729765892 = 0.004738294053822756 + 0.01 * 6.560277938842773
Epoch 900, val loss: 1.2828452587127686
Epoch 910, training loss: 0.06994109600782394 = 0.004579764325171709 + 0.01 * 6.5361328125
Epoch 910, val loss: 1.2884535789489746
Epoch 920, training loss: 0.06985112279653549 = 0.004430531524121761 + 0.01 * 6.542059421539307
Epoch 920, val loss: 1.2938684225082397
Epoch 930, training loss: 0.0696382224559784 = 0.004289498087018728 + 0.01 * 6.534872531890869
Epoch 930, val loss: 1.2993491888046265
Epoch 940, training loss: 0.06946368515491486 = 0.00415622815489769 + 0.01 * 6.5307464599609375
Epoch 940, val loss: 1.3044968843460083
Epoch 950, training loss: 0.06928405910730362 = 0.0040305075235664845 + 0.01 * 6.525354862213135
Epoch 950, val loss: 1.3095561265945435
Epoch 960, training loss: 0.06923720240592957 = 0.003911489620804787 + 0.01 * 6.532571315765381
Epoch 960, val loss: 1.3145537376403809
Epoch 970, training loss: 0.06896843761205673 = 0.003798965597525239 + 0.01 * 6.516947269439697
Epoch 970, val loss: 1.319504976272583
Epoch 980, training loss: 0.06896340847015381 = 0.003692240919917822 + 0.01 * 6.5271172523498535
Epoch 980, val loss: 1.3242948055267334
Epoch 990, training loss: 0.06872831284999847 = 0.00359143503010273 + 0.01 * 6.513688087463379
Epoch 990, val loss: 1.3288277387619019
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.7048
Flip ASR: 0.6667/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.054882049560547 = 1.971144199371338 + 0.01 * 8.373793601989746
Epoch 0, val loss: 1.9769768714904785
Epoch 10, training loss: 2.043649673461914 = 1.9599131345748901 + 0.01 * 8.373645782470703
Epoch 10, val loss: 1.965825080871582
Epoch 20, training loss: 2.0293097496032715 = 1.94557785987854 + 0.01 * 8.373201370239258
Epoch 20, val loss: 1.9511163234710693
Epoch 30, training loss: 2.0086450576782227 = 1.9249303340911865 + 0.01 * 8.37146282196045
Epoch 30, val loss: 1.929661750793457
Epoch 40, training loss: 1.9775402545928955 = 1.8939372301101685 + 0.01 * 8.360304832458496
Epoch 40, val loss: 1.8978956937789917
Epoch 50, training loss: 1.9333887100219727 = 1.8503652811050415 + 0.01 * 8.302340507507324
Epoch 50, val loss: 1.855356216430664
Epoch 60, training loss: 1.8826993703842163 = 1.8019986152648926 + 0.01 * 8.070077896118164
Epoch 60, val loss: 1.8123985528945923
Epoch 70, training loss: 1.8409608602523804 = 1.7617087364196777 + 0.01 * 7.925215244293213
Epoch 70, val loss: 1.7777446508407593
Epoch 80, training loss: 1.7921429872512817 = 1.715585708618164 + 0.01 * 7.655730724334717
Epoch 80, val loss: 1.7352324724197388
Epoch 90, training loss: 1.7266182899475098 = 1.6533087491989136 + 0.01 * 7.330948829650879
Epoch 90, val loss: 1.6826235055923462
Epoch 100, training loss: 1.6425251960754395 = 1.5719484090805054 + 0.01 * 7.057673454284668
Epoch 100, val loss: 1.6172670125961304
Epoch 110, training loss: 1.5492979288101196 = 1.4791368246078491 + 0.01 * 7.016108989715576
Epoch 110, val loss: 1.5421172380447388
Epoch 120, training loss: 1.4567370414733887 = 1.3868664503097534 + 0.01 * 6.987060546875
Epoch 120, val loss: 1.4679746627807617
Epoch 130, training loss: 1.3684194087982178 = 1.2987542152404785 + 0.01 * 6.966525554656982
Epoch 130, val loss: 1.3990200757980347
Epoch 140, training loss: 1.2807124853134155 = 1.21122407913208 + 0.01 * 6.948843002319336
Epoch 140, val loss: 1.3310012817382812
Epoch 150, training loss: 1.1910911798477173 = 1.1217907667160034 + 0.01 * 6.930045127868652
Epoch 150, val loss: 1.2614620923995972
Epoch 160, training loss: 1.0995124578475952 = 1.0304138660430908 + 0.01 * 6.909855842590332
Epoch 160, val loss: 1.1914217472076416
Epoch 170, training loss: 1.0080419778823853 = 0.939131498336792 + 0.01 * 6.891047477722168
Epoch 170, val loss: 1.12106192111969
Epoch 180, training loss: 0.9195364713668823 = 0.8507599830627441 + 0.01 * 6.8776469230651855
Epoch 180, val loss: 1.0528607368469238
Epoch 190, training loss: 0.8364468216896057 = 0.7677527070045471 + 0.01 * 6.869410991668701
Epoch 190, val loss: 0.9892169237136841
Epoch 200, training loss: 0.7598951458930969 = 0.691300094127655 + 0.01 * 6.859506607055664
Epoch 200, val loss: 0.9315255880355835
Epoch 210, training loss: 0.6900830864906311 = 0.6215788722038269 + 0.01 * 6.850423336029053
Epoch 210, val loss: 0.8805996775627136
Epoch 220, training loss: 0.6267287731170654 = 0.5583047866821289 + 0.01 * 6.8423967361450195
Epoch 220, val loss: 0.8365640640258789
Epoch 230, training loss: 0.5693762898445129 = 0.5010075569152832 + 0.01 * 6.83687162399292
Epoch 230, val loss: 0.7990138530731201
Epoch 240, training loss: 0.5174036026000977 = 0.4491104185581207 + 0.01 * 6.829319000244141
Epoch 240, val loss: 0.7681774497032166
Epoch 250, training loss: 0.4701958894729614 = 0.4019080698490143 + 0.01 * 6.828782558441162
Epoch 250, val loss: 0.7440711855888367
Epoch 260, training loss: 0.4269922375679016 = 0.35878315567970276 + 0.01 * 6.820908546447754
Epoch 260, val loss: 0.7260764241218567
Epoch 270, training loss: 0.38740047812461853 = 0.3192359209060669 + 0.01 * 6.816456317901611
Epoch 270, val loss: 0.7130533456802368
Epoch 280, training loss: 0.35116297006607056 = 0.28302624821662903 + 0.01 * 6.813670635223389
Epoch 280, val loss: 0.7037526965141296
Epoch 290, training loss: 0.3183087408542633 = 0.25009772181510925 + 0.01 * 6.821101188659668
Epoch 290, val loss: 0.6976293325424194
Epoch 300, training loss: 0.2886330187320709 = 0.2205067127943039 + 0.01 * 6.812629699707031
Epoch 300, val loss: 0.6944971084594727
Epoch 310, training loss: 0.26232588291168213 = 0.19423115253448486 + 0.01 * 6.809474468231201
Epoch 310, val loss: 0.6941531896591187
Epoch 320, training loss: 0.23921296000480652 = 0.17114290595054626 + 0.01 * 6.807004451751709
Epoch 320, val loss: 0.696232795715332
Epoch 330, training loss: 0.21903690695762634 = 0.15099087357521057 + 0.01 * 6.80460262298584
Epoch 330, val loss: 0.7004734873771667
Epoch 340, training loss: 0.20153123140335083 = 0.1334398239850998 + 0.01 * 6.809141635894775
Epoch 340, val loss: 0.7063987255096436
Epoch 350, training loss: 0.18617194890975952 = 0.11814261227846146 + 0.01 * 6.802933692932129
Epoch 350, val loss: 0.7136977314949036
Epoch 360, training loss: 0.1727662980556488 = 0.10477208346128464 + 0.01 * 6.7994208335876465
Epoch 360, val loss: 0.7220291495323181
Epoch 370, training loss: 0.16101527214050293 = 0.09304703027009964 + 0.01 * 6.796825408935547
Epoch 370, val loss: 0.7310032248497009
Epoch 380, training loss: 0.1506870687007904 = 0.08274795114994049 + 0.01 * 6.793912410736084
Epoch 380, val loss: 0.7403506636619568
Epoch 390, training loss: 0.1416093409061432 = 0.07369306683540344 + 0.01 * 6.791626453399658
Epoch 390, val loss: 0.7498660087585449
Epoch 400, training loss: 0.13361427187919617 = 0.06572648137807846 + 0.01 * 6.788779258728027
Epoch 400, val loss: 0.7593759298324585
Epoch 410, training loss: 0.12658119201660156 = 0.05872074514627457 + 0.01 * 6.786045074462891
Epoch 410, val loss: 0.7688432335853577
Epoch 420, training loss: 0.12038405239582062 = 0.05256553739309311 + 0.01 * 6.781851768493652
Epoch 420, val loss: 0.7782095074653625
Epoch 430, training loss: 0.1149674504995346 = 0.04716026410460472 + 0.01 * 6.780718803405762
Epoch 430, val loss: 0.7874191403388977
Epoch 440, training loss: 0.11019501090049744 = 0.042419541627168655 + 0.01 * 6.777547359466553
Epoch 440, val loss: 0.7964701652526855
Epoch 450, training loss: 0.10596829652786255 = 0.03826471045613289 + 0.01 * 6.770358562469482
Epoch 450, val loss: 0.8054078817367554
Epoch 460, training loss: 0.10229776799678802 = 0.03462205454707146 + 0.01 * 6.767571449279785
Epoch 460, val loss: 0.8142703771591187
Epoch 470, training loss: 0.09905147552490234 = 0.03142762556672096 + 0.01 * 6.762385845184326
Epoch 470, val loss: 0.8230626583099365
Epoch 480, training loss: 0.0961981788277626 = 0.02862134389579296 + 0.01 * 6.757683277130127
Epoch 480, val loss: 0.8317905068397522
Epoch 490, training loss: 0.09378989785909653 = 0.026151487603783607 + 0.01 * 6.76384162902832
Epoch 490, val loss: 0.8404856324195862
Epoch 500, training loss: 0.09145490825176239 = 0.023975197225809097 + 0.01 * 6.747971057891846
Epoch 500, val loss: 0.8490570783615112
Epoch 510, training loss: 0.08946521580219269 = 0.022051654756069183 + 0.01 * 6.741355895996094
Epoch 510, val loss: 0.8575095534324646
Epoch 520, training loss: 0.0877920538187027 = 0.020345842465758324 + 0.01 * 6.7446208000183105
Epoch 520, val loss: 0.8658373355865479
Epoch 530, training loss: 0.08616278320550919 = 0.018829533830285072 + 0.01 * 6.733325004577637
Epoch 530, val loss: 0.8739722967147827
Epoch 540, training loss: 0.08469627052545547 = 0.01747683435678482 + 0.01 * 6.7219438552856445
Epoch 540, val loss: 0.8819565176963806
Epoch 550, training loss: 0.08387570083141327 = 0.016265666112303734 + 0.01 * 6.761003494262695
Epoch 550, val loss: 0.8897731304168701
Epoch 560, training loss: 0.08240356296300888 = 0.015180262736976147 + 0.01 * 6.722330570220947
Epoch 560, val loss: 0.8974178433418274
Epoch 570, training loss: 0.08122508972883224 = 0.014203044585883617 + 0.01 * 6.702205181121826
Epoch 570, val loss: 0.904879093170166
Epoch 580, training loss: 0.08026333153247833 = 0.013319657184183598 + 0.01 * 6.6943678855896
Epoch 580, val loss: 0.9121794700622559
Epoch 590, training loss: 0.07945090532302856 = 0.012518571689724922 + 0.01 * 6.693233966827393
Epoch 590, val loss: 0.9193140268325806
Epoch 600, training loss: 0.07879874110221863 = 0.011791527271270752 + 0.01 * 6.700721263885498
Epoch 600, val loss: 0.9262650012969971
Epoch 610, training loss: 0.07797785848379135 = 0.011129806749522686 + 0.01 * 6.684805870056152
Epoch 610, val loss: 0.9329911470413208
Epoch 620, training loss: 0.07731909304857254 = 0.010525180958211422 + 0.01 * 6.679390907287598
Epoch 620, val loss: 0.9396060109138489
Epoch 630, training loss: 0.07671649754047394 = 0.009971780702471733 + 0.01 * 6.674471378326416
Epoch 630, val loss: 0.9459969401359558
Epoch 640, training loss: 0.076247438788414 = 0.009463753551244736 + 0.01 * 6.67836856842041
Epoch 640, val loss: 0.952311635017395
Epoch 650, training loss: 0.07547233253717422 = 0.008996273390948772 + 0.01 * 6.647606372833252
Epoch 650, val loss: 0.9583907723426819
Epoch 660, training loss: 0.07522308826446533 = 0.008565228432416916 + 0.01 * 6.665785789489746
Epoch 660, val loss: 0.9643110632896423
Epoch 670, training loss: 0.07462625205516815 = 0.0081675099208951 + 0.01 * 6.6458740234375
Epoch 670, val loss: 0.9701327681541443
Epoch 680, training loss: 0.07404746860265732 = 0.00779876159504056 + 0.01 * 6.624870777130127
Epoch 680, val loss: 0.9757236242294312
Epoch 690, training loss: 0.0740455761551857 = 0.007456254679709673 + 0.01 * 6.658931732177734
Epoch 690, val loss: 0.9812178015708923
Epoch 700, training loss: 0.07333669066429138 = 0.007138512097299099 + 0.01 * 6.619818210601807
Epoch 700, val loss: 0.9866005182266235
Epoch 710, training loss: 0.07305556535720825 = 0.006842310540378094 + 0.01 * 6.621325969696045
Epoch 710, val loss: 0.9918259382247925
Epoch 720, training loss: 0.07258908450603485 = 0.006565792486071587 + 0.01 * 6.602329254150391
Epoch 720, val loss: 0.9969116449356079
Epoch 730, training loss: 0.07240013778209686 = 0.006307376082986593 + 0.01 * 6.609275817871094
Epoch 730, val loss: 1.0018670558929443
Epoch 740, training loss: 0.07206941395998001 = 0.006065884605050087 + 0.01 * 6.600353240966797
Epoch 740, val loss: 1.0067524909973145
Epoch 750, training loss: 0.07181408256292343 = 0.005839210934937 + 0.01 * 6.597486972808838
Epoch 750, val loss: 1.0115009546279907
Epoch 760, training loss: 0.07152920961380005 = 0.005626363679766655 + 0.01 * 6.59028434753418
Epoch 760, val loss: 1.016096830368042
Epoch 770, training loss: 0.07127699255943298 = 0.0054264734499156475 + 0.01 * 6.585052013397217
Epoch 770, val loss: 1.0206832885742188
Epoch 780, training loss: 0.07113172113895416 = 0.005237956065684557 + 0.01 * 6.589375972747803
Epoch 780, val loss: 1.0250437259674072
Epoch 790, training loss: 0.07100661098957062 = 0.005060697440057993 + 0.01 * 6.5945916175842285
Epoch 790, val loss: 1.029342770576477
Epoch 800, training loss: 0.07052837312221527 = 0.004893566481769085 + 0.01 * 6.563481330871582
Epoch 800, val loss: 1.033565640449524
Epoch 810, training loss: 0.07052964717149734 = 0.004735707305371761 + 0.01 * 6.5793938636779785
Epoch 810, val loss: 1.0377233028411865
Epoch 820, training loss: 0.07021474093198776 = 0.004586372058838606 + 0.01 * 6.562837600708008
Epoch 820, val loss: 1.0417251586914062
Epoch 830, training loss: 0.06991279870271683 = 0.004444925580173731 + 0.01 * 6.546787261962891
Epoch 830, val loss: 1.0456383228302002
Epoch 840, training loss: 0.06990054994821548 = 0.004310937132686377 + 0.01 * 6.558961391448975
Epoch 840, val loss: 1.049443244934082
Epoch 850, training loss: 0.06958864629268646 = 0.00418427586555481 + 0.01 * 6.5404372215271
Epoch 850, val loss: 1.0532087087631226
Epoch 860, training loss: 0.06950070708990097 = 0.004063781816512346 + 0.01 * 6.543692588806152
Epoch 860, val loss: 1.0568571090698242
Epoch 870, training loss: 0.06930501013994217 = 0.00394959282130003 + 0.01 * 6.535542011260986
Epoch 870, val loss: 1.0604586601257324
Epoch 880, training loss: 0.06918378174304962 = 0.0038408779073506594 + 0.01 * 6.534290790557861
Epoch 880, val loss: 1.0639582872390747
Epoch 890, training loss: 0.0692094936966896 = 0.0037373951636254787 + 0.01 * 6.547209739685059
Epoch 890, val loss: 1.0674238204956055
Epoch 900, training loss: 0.0689118355512619 = 0.0036387622822076082 + 0.01 * 6.527307510375977
Epoch 900, val loss: 1.0707474946975708
Epoch 910, training loss: 0.06874462962150574 = 0.0035446714609861374 + 0.01 * 6.51999568939209
Epoch 910, val loss: 1.0740169286727905
Epoch 920, training loss: 0.06861574947834015 = 0.003454793943092227 + 0.01 * 6.516095161437988
Epoch 920, val loss: 1.0772292613983154
Epoch 930, training loss: 0.0684625655412674 = 0.0033691192511469126 + 0.01 * 6.509344100952148
Epoch 930, val loss: 1.0803817510604858
Epoch 940, training loss: 0.06841082125902176 = 0.003287222469225526 + 0.01 * 6.512359619140625
Epoch 940, val loss: 1.0834437608718872
Epoch 950, training loss: 0.06824963539838791 = 0.003208970883861184 + 0.01 * 6.504066467285156
Epoch 950, val loss: 1.0864499807357788
Epoch 960, training loss: 0.06829279661178589 = 0.003134059952571988 + 0.01 * 6.51587438583374
Epoch 960, val loss: 1.089390754699707
Epoch 970, training loss: 0.06805098056793213 = 0.003062432399019599 + 0.01 * 6.498855113983154
Epoch 970, val loss: 1.0923035144805908
Epoch 980, training loss: 0.06810730695724487 = 0.002993631875142455 + 0.01 * 6.511368274688721
Epoch 980, val loss: 1.095129132270813
Epoch 990, training loss: 0.06785029172897339 = 0.00292760762386024 + 0.01 * 6.4922685623168945
Epoch 990, val loss: 1.0978893041610718
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.8339
Flip ASR: 0.8044/225 nodes
The final ASR:0.68758, 0.12713, Accuracy:0.82593, 0.00524
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11628])
remove edge: torch.Size([2, 9478])
updated graph: torch.Size([2, 10550])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 1.0000
Flip ASR: 1.0000/225 nodes
The final ASR:0.98278, 0.01218, Accuracy:0.83333, 0.00800
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.033130645751953 = 1.9493916034698486 + 0.01 * 8.373892784118652
Epoch 0, val loss: 1.950323224067688
Epoch 10, training loss: 2.0221965312957764 = 1.9384582042694092 + 0.01 * 8.373838424682617
Epoch 10, val loss: 1.9392399787902832
Epoch 20, training loss: 2.008808135986328 = 1.9250715970993042 + 0.01 * 8.373658180236816
Epoch 20, val loss: 1.925419569015503
Epoch 30, training loss: 1.989958643913269 = 1.906227469444275 + 0.01 * 8.373119354248047
Epoch 30, val loss: 1.9059592485427856
Epoch 40, training loss: 1.9622304439544678 = 1.878525972366333 + 0.01 * 8.370441436767578
Epoch 40, val loss: 1.8778291940689087
Epoch 50, training loss: 1.923869013786316 = 1.8403501510620117 + 0.01 * 8.35189151763916
Epoch 50, val loss: 1.841059684753418
Epoch 60, training loss: 1.8806906938552856 = 1.7980175018310547 + 0.01 * 8.267324447631836
Epoch 60, val loss: 1.8050885200500488
Epoch 70, training loss: 1.8407471179962158 = 1.7603421211242676 + 0.01 * 8.040496826171875
Epoch 70, val loss: 1.77674400806427
Epoch 80, training loss: 1.7918071746826172 = 1.712936520576477 + 0.01 * 7.8870673179626465
Epoch 80, val loss: 1.7370917797088623
Epoch 90, training loss: 1.7237228155136108 = 1.6469584703445435 + 0.01 * 7.6764373779296875
Epoch 90, val loss: 1.6802451610565186
Epoch 100, training loss: 1.636185884475708 = 1.5614423751831055 + 0.01 * 7.474355697631836
Epoch 100, val loss: 1.6082298755645752
Epoch 110, training loss: 1.5357096195220947 = 1.4620442390441895 + 0.01 * 7.366533279418945
Epoch 110, val loss: 1.5261857509613037
Epoch 120, training loss: 1.4319976568222046 = 1.3590224981307983 + 0.01 * 7.297515392303467
Epoch 120, val loss: 1.4420428276062012
Epoch 130, training loss: 1.3286844491958618 = 1.2567440271377563 + 0.01 * 7.194040775299072
Epoch 130, val loss: 1.360614538192749
Epoch 140, training loss: 1.2275789976119995 = 1.1565542221069336 + 0.01 * 7.102475166320801
Epoch 140, val loss: 1.2822043895721436
Epoch 150, training loss: 1.1316725015640259 = 1.061192274093628 + 0.01 * 7.048020362854004
Epoch 150, val loss: 1.2080503702163696
Epoch 160, training loss: 1.0428121089935303 = 0.9726011157035828 + 0.01 * 7.021100044250488
Epoch 160, val loss: 1.1402251720428467
Epoch 170, training loss: 0.9602023363113403 = 0.8901079297065735 + 0.01 * 7.0094428062438965
Epoch 170, val loss: 1.0785025358200073
Epoch 180, training loss: 0.8819233775138855 = 0.8119083046913147 + 0.01 * 7.0015058517456055
Epoch 180, val loss: 1.0218617916107178
Epoch 190, training loss: 0.8074824213981628 = 0.7375180721282959 + 0.01 * 6.996434211730957
Epoch 190, val loss: 0.970360517501831
Epoch 200, training loss: 0.7380236387252808 = 0.6680968999862671 + 0.01 * 6.992674350738525
Epoch 200, val loss: 0.9253866672515869
Epoch 210, training loss: 0.6750339269638062 = 0.6051434874534607 + 0.01 * 6.989047050476074
Epoch 210, val loss: 0.8885605335235596
Epoch 220, training loss: 0.6192077398300171 = 0.5493625998497009 + 0.01 * 6.9845147132873535
Epoch 220, val loss: 0.860969066619873
Epoch 230, training loss: 0.5700823068618774 = 0.5002873539924622 + 0.01 * 6.979496479034424
Epoch 230, val loss: 0.8421931266784668
Epoch 240, training loss: 0.5263423919677734 = 0.45660269260406494 + 0.01 * 6.973969459533691
Epoch 240, val loss: 0.8303547501564026
Epoch 250, training loss: 0.486460417509079 = 0.4167885482311249 + 0.01 * 6.967186450958252
Epoch 250, val loss: 0.823443591594696
Epoch 260, training loss: 0.44925960898399353 = 0.37964928150177 + 0.01 * 6.961032867431641
Epoch 260, val loss: 0.8196456432342529
Epoch 270, training loss: 0.4140264391899109 = 0.3445126712322235 + 0.01 * 6.951377868652344
Epoch 270, val loss: 0.8182529211044312
Epoch 280, training loss: 0.380479633808136 = 0.31103718280792236 + 0.01 * 6.944246292114258
Epoch 280, val loss: 0.8189296126365662
Epoch 290, training loss: 0.3484356701374054 = 0.27905771136283875 + 0.01 * 6.9377970695495605
Epoch 290, val loss: 0.8216085433959961
Epoch 300, training loss: 0.3177945017814636 = 0.24854588508605957 + 0.01 * 6.924863338470459
Epoch 300, val loss: 0.8264875411987305
Epoch 310, training loss: 0.2887856364250183 = 0.21962255239486694 + 0.01 * 6.91630744934082
Epoch 310, val loss: 0.8337715864181519
Epoch 320, training loss: 0.26160603761672974 = 0.19254490733146667 + 0.01 * 6.906113624572754
Epoch 320, val loss: 0.8434761762619019
Epoch 330, training loss: 0.23678262531757355 = 0.16772720217704773 + 0.01 * 6.905542373657227
Epoch 330, val loss: 0.8553631901741028
Epoch 340, training loss: 0.21455252170562744 = 0.14556393027305603 + 0.01 * 6.898859024047852
Epoch 340, val loss: 0.8688615560531616
Epoch 350, training loss: 0.19506751000881195 = 0.12617823481559753 + 0.01 * 6.888927936553955
Epoch 350, val loss: 0.8838074207305908
Epoch 360, training loss: 0.17824390530586243 = 0.10941977053880692 + 0.01 * 6.8824143409729
Epoch 360, val loss: 0.8998402953147888
Epoch 370, training loss: 0.16376763582229614 = 0.0950121134519577 + 0.01 * 6.875553131103516
Epoch 370, val loss: 0.9167261123657227
Epoch 380, training loss: 0.15139560401439667 = 0.08266069740056992 + 0.01 * 6.873490810394287
Epoch 380, val loss: 0.9342375993728638
Epoch 390, training loss: 0.14072559773921967 = 0.0720813199877739 + 0.01 * 6.86442756652832
Epoch 390, val loss: 0.952311098575592
Epoch 400, training loss: 0.13162541389465332 = 0.06303656846284866 + 0.01 * 6.858884334564209
Epoch 400, val loss: 0.9710351824760437
Epoch 410, training loss: 0.12389805912971497 = 0.055322181433439255 + 0.01 * 6.8575873374938965
Epoch 410, val loss: 0.9902011156082153
Epoch 420, training loss: 0.1171887144446373 = 0.04875204712152481 + 0.01 * 6.843667030334473
Epoch 420, val loss: 1.0097252130508423
Epoch 430, training loss: 0.11156730353832245 = 0.04315297305583954 + 0.01 * 6.841433525085449
Epoch 430, val loss: 1.0293859243392944
Epoch 440, training loss: 0.10675062239170074 = 0.0383722148835659 + 0.01 * 6.837840557098389
Epoch 440, val loss: 1.0490771532058716
Epoch 450, training loss: 0.10260163247585297 = 0.03427783027291298 + 0.01 * 6.832380294799805
Epoch 450, val loss: 1.068580150604248
Epoch 460, training loss: 0.09901148825883865 = 0.030764823779463768 + 0.01 * 6.824666500091553
Epoch 460, val loss: 1.0878512859344482
Epoch 470, training loss: 0.09589507430791855 = 0.02774079330265522 + 0.01 * 6.815428256988525
Epoch 470, val loss: 1.1065804958343506
Epoch 480, training loss: 0.0931607112288475 = 0.02512945793569088 + 0.01 * 6.803125381469727
Epoch 480, val loss: 1.1248860359191895
Epoch 490, training loss: 0.09083306044340134 = 0.022865450009703636 + 0.01 * 6.7967610359191895
Epoch 490, val loss: 1.1426385641098022
Epoch 500, training loss: 0.08878584206104279 = 0.020894965156912804 + 0.01 * 6.789087295532227
Epoch 500, val loss: 1.1598275899887085
Epoch 510, training loss: 0.08697327971458435 = 0.01917119510471821 + 0.01 * 6.780208587646484
Epoch 510, val loss: 1.1764312982559204
Epoch 520, training loss: 0.0854249894618988 = 0.01765652559697628 + 0.01 * 6.776846408843994
Epoch 520, val loss: 1.1925132274627686
Epoch 530, training loss: 0.08412861824035645 = 0.01631932705640793 + 0.01 * 6.780929088592529
Epoch 530, val loss: 1.2081109285354614
Epoch 540, training loss: 0.08285467326641083 = 0.015133530832827091 + 0.01 * 6.7721147537231445
Epoch 540, val loss: 1.2231106758117676
Epoch 550, training loss: 0.0816551074385643 = 0.014077645726501942 + 0.01 * 6.757745742797852
Epoch 550, val loss: 1.2376718521118164
Epoch 560, training loss: 0.08066698908805847 = 0.013133504427969456 + 0.01 * 6.753348350524902
Epoch 560, val loss: 1.2516616582870483
Epoch 570, training loss: 0.07989311218261719 = 0.012285919860005379 + 0.01 * 6.760719299316406
Epoch 570, val loss: 1.2651976346969604
Epoch 580, training loss: 0.07899575680494308 = 0.01152227632701397 + 0.01 * 6.747347831726074
Epoch 580, val loss: 1.2784074544906616
Epoch 590, training loss: 0.07818270474672318 = 0.010831719264388084 + 0.01 * 6.735098361968994
Epoch 590, val loss: 1.2910778522491455
Epoch 600, training loss: 0.07770942151546478 = 0.010205025784671307 + 0.01 * 6.750439643859863
Epoch 600, val loss: 1.303350567817688
Epoch 610, training loss: 0.0770091786980629 = 0.009635216556489468 + 0.01 * 6.737396240234375
Epoch 610, val loss: 1.3153693675994873
Epoch 620, training loss: 0.07632570713758469 = 0.009115093387663364 + 0.01 * 6.721061706542969
Epoch 620, val loss: 1.3268460035324097
Epoch 630, training loss: 0.07591105997562408 = 0.008638739585876465 + 0.01 * 6.727231979370117
Epoch 630, val loss: 1.3380167484283447
Epoch 640, training loss: 0.07542461156845093 = 0.008201703429222107 + 0.01 * 6.722290992736816
Epoch 640, val loss: 1.3489630222320557
Epoch 650, training loss: 0.07489272207021713 = 0.007799791172146797 + 0.01 * 6.709293365478516
Epoch 650, val loss: 1.3595253229141235
Epoch 660, training loss: 0.07447454333305359 = 0.007429157849401236 + 0.01 * 6.704538822174072
Epoch 660, val loss: 1.369752049446106
Epoch 670, training loss: 0.07415412366390228 = 0.007086473982781172 + 0.01 * 6.706765174865723
Epoch 670, val loss: 1.379758358001709
Epoch 680, training loss: 0.07369111478328705 = 0.006769236177206039 + 0.01 * 6.692188739776611
Epoch 680, val loss: 1.3894422054290771
Epoch 690, training loss: 0.07339148223400116 = 0.006475028581917286 + 0.01 * 6.69164514541626
Epoch 690, val loss: 1.3988722562789917
Epoch 700, training loss: 0.07310249656438828 = 0.006201328244060278 + 0.01 * 6.690116882324219
Epoch 700, val loss: 1.4081119298934937
Epoch 710, training loss: 0.07273994386196136 = 0.0059464434161782265 + 0.01 * 6.67935037612915
Epoch 710, val loss: 1.4170122146606445
Epoch 720, training loss: 0.07257974147796631 = 0.005708772223442793 + 0.01 * 6.687097072601318
Epoch 720, val loss: 1.425671935081482
Epoch 730, training loss: 0.0723194107413292 = 0.0054866657592356205 + 0.01 * 6.683274269104004
Epoch 730, val loss: 1.4342318773269653
Epoch 740, training loss: 0.07205533981323242 = 0.005278744734823704 + 0.01 * 6.677659511566162
Epoch 740, val loss: 1.4425519704818726
Epoch 750, training loss: 0.07174621522426605 = 0.005083798430860043 + 0.01 * 6.666241645812988
Epoch 750, val loss: 1.4506111145019531
Epoch 760, training loss: 0.07156345993280411 = 0.004900854546576738 + 0.01 * 6.666260719299316
Epoch 760, val loss: 1.4584310054779053
Epoch 770, training loss: 0.07136791944503784 = 0.00472892913967371 + 0.01 * 6.6638994216918945
Epoch 770, val loss: 1.466142177581787
Epoch 780, training loss: 0.07111816108226776 = 0.004567093215882778 + 0.01 * 6.655107021331787
Epoch 780, val loss: 1.4736520051956177
Epoch 790, training loss: 0.07103614509105682 = 0.004414668772369623 + 0.01 * 6.6621479988098145
Epoch 790, val loss: 1.481024980545044
Epoch 800, training loss: 0.0706992819905281 = 0.004270885605365038 + 0.01 * 6.642839431762695
Epoch 800, val loss: 1.4881682395935059
Epoch 810, training loss: 0.07071219384670258 = 0.004135070368647575 + 0.01 * 6.657712459564209
Epoch 810, val loss: 1.4950752258300781
Epoch 820, training loss: 0.07052232325077057 = 0.004006722941994667 + 0.01 * 6.651560306549072
Epoch 820, val loss: 1.502026081085205
Epoch 830, training loss: 0.07024332880973816 = 0.0038852421566843987 + 0.01 * 6.635808944702148
Epoch 830, val loss: 1.5086816549301147
Epoch 840, training loss: 0.07016532123088837 = 0.0037700815591961145 + 0.01 * 6.639523983001709
Epoch 840, val loss: 1.5151575803756714
Epoch 850, training loss: 0.06994879245758057 = 0.0036608537193387747 + 0.01 * 6.6287946701049805
Epoch 850, val loss: 1.5215169191360474
Epoch 860, training loss: 0.0698525533080101 = 0.0035571337211877108 + 0.01 * 6.629542350769043
Epoch 860, val loss: 1.527811050415039
Epoch 870, training loss: 0.06960310786962509 = 0.003458581166341901 + 0.01 * 6.614452838897705
Epoch 870, val loss: 1.5339109897613525
Epoch 880, training loss: 0.06988522410392761 = 0.003364724339917302 + 0.01 * 6.652049541473389
Epoch 880, val loss: 1.5397566556930542
Epoch 890, training loss: 0.06951898336410522 = 0.0032757162116467953 + 0.01 * 6.624326705932617
Epoch 890, val loss: 1.5456575155258179
Epoch 900, training loss: 0.06957322359085083 = 0.0031909975223243237 + 0.01 * 6.638223171234131
Epoch 900, val loss: 1.5513566732406616
Epoch 910, training loss: 0.06933872401714325 = 0.0031103380024433136 + 0.01 * 6.622838497161865
Epoch 910, val loss: 1.5568997859954834
Epoch 920, training loss: 0.06908904761075974 = 0.0030333090107887983 + 0.01 * 6.605573654174805
Epoch 920, val loss: 1.5624141693115234
Epoch 930, training loss: 0.0689004436135292 = 0.00295965070836246 + 0.01 * 6.594079494476318
Epoch 930, val loss: 1.5677251815795898
Epoch 940, training loss: 0.06879999488592148 = 0.002889217808842659 + 0.01 * 6.591078281402588
Epoch 940, val loss: 1.5729576349258423
Epoch 950, training loss: 0.06876803934574127 = 0.002821855479851365 + 0.01 * 6.594618797302246
Epoch 950, val loss: 1.5781326293945312
Epoch 960, training loss: 0.06888072937726974 = 0.0027573585975915194 + 0.01 * 6.612337589263916
Epoch 960, val loss: 1.5830081701278687
Epoch 970, training loss: 0.06864011287689209 = 0.002695702714845538 + 0.01 * 6.5944414138793945
Epoch 970, val loss: 1.588064193725586
Epoch 980, training loss: 0.06858265399932861 = 0.002636613789945841 + 0.01 * 6.5946044921875
Epoch 980, val loss: 1.5928455591201782
Epoch 990, training loss: 0.06833959370851517 = 0.002579813590273261 + 0.01 * 6.575977802276611
Epoch 990, val loss: 1.5976133346557617
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7370
Overall ASR: 0.4945
Flip ASR: 0.4044/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.033393621444702 = 1.9496546983718872 + 0.01 * 8.373888969421387
Epoch 0, val loss: 1.9499597549438477
Epoch 10, training loss: 2.0229804515838623 = 1.9392421245574951 + 0.01 * 8.373827934265137
Epoch 10, val loss: 1.9391900300979614
Epoch 20, training loss: 2.0104291439056396 = 1.9266929626464844 + 0.01 * 8.37360954284668
Epoch 20, val loss: 1.92622971534729
Epoch 30, training loss: 1.9929864406585693 = 1.9092568159103394 + 0.01 * 8.372968673706055
Epoch 30, val loss: 1.9083433151245117
Epoch 40, training loss: 1.9672099351882935 = 1.8835091590881348 + 0.01 * 8.370080947875977
Epoch 40, val loss: 1.8823258876800537
Epoch 50, training loss: 1.930330753326416 = 1.8468308448791504 + 0.01 * 8.349987030029297
Epoch 50, val loss: 1.846493124961853
Epoch 60, training loss: 1.8836336135864258 = 1.8015648126602173 + 0.01 * 8.206881523132324
Epoch 60, val loss: 1.804897665977478
Epoch 70, training loss: 1.833174705505371 = 1.7544007301330566 + 0.01 * 7.877396583557129
Epoch 70, val loss: 1.7634323835372925
Epoch 80, training loss: 1.7743988037109375 = 1.6980509757995605 + 0.01 * 7.634782314300537
Epoch 80, val loss: 1.7119776010513306
Epoch 90, training loss: 1.6969820261001587 = 1.6220945119857788 + 0.01 * 7.488746166229248
Epoch 90, val loss: 1.6446853876113892
Epoch 100, training loss: 1.6000920534133911 = 1.5262154340744019 + 0.01 * 7.387660026550293
Epoch 100, val loss: 1.5645486116409302
Epoch 110, training loss: 1.4920567274093628 = 1.4187216758728027 + 0.01 * 7.333507537841797
Epoch 110, val loss: 1.4763641357421875
Epoch 120, training loss: 1.383669376373291 = 1.3108065128326416 + 0.01 * 7.286283493041992
Epoch 120, val loss: 1.3922486305236816
Epoch 130, training loss: 1.2808904647827148 = 1.2085591554641724 + 0.01 * 7.2331342697143555
Epoch 130, val loss: 1.3159923553466797
Epoch 140, training loss: 1.1856350898742676 = 1.113829255104065 + 0.01 * 7.180579662322998
Epoch 140, val loss: 1.2476600408554077
Epoch 150, training loss: 1.0977336168289185 = 1.0262573957443237 + 0.01 * 7.147620677947998
Epoch 150, val loss: 1.1855239868164062
Epoch 160, training loss: 1.016201138496399 = 0.9448767304420471 + 0.01 * 7.13244104385376
Epoch 160, val loss: 1.129223108291626
Epoch 170, training loss: 0.9393996596336365 = 0.8682175874710083 + 0.01 * 7.1182074546813965
Epoch 170, val loss: 1.0772048234939575
Epoch 180, training loss: 0.86537766456604 = 0.7943702340126038 + 0.01 * 7.1007399559021
Epoch 180, val loss: 1.0276317596435547
Epoch 190, training loss: 0.7932112812995911 = 0.7224055528640747 + 0.01 * 7.080575466156006
Epoch 190, val loss: 0.9794750213623047
Epoch 200, training loss: 0.7235590815544128 = 0.6529436111450195 + 0.01 * 7.061546325683594
Epoch 200, val loss: 0.9336771368980408
Epoch 210, training loss: 0.6579150557518005 = 0.5874595642089844 + 0.01 * 7.045547008514404
Epoch 210, val loss: 0.8920711278915405
Epoch 220, training loss: 0.5976284742355347 = 0.5273116827011108 + 0.01 * 7.031676292419434
Epoch 220, val loss: 0.8570488691329956
Epoch 230, training loss: 0.5434683561325073 = 0.47323116660118103 + 0.01 * 7.023717880249023
Epoch 230, val loss: 0.8302303552627563
Epoch 240, training loss: 0.49535512924194336 = 0.4252578020095825 + 0.01 * 7.0097336769104
Epoch 240, val loss: 0.8116106986999512
Epoch 250, training loss: 0.45286881923675537 = 0.38284221291542053 + 0.01 * 7.002662181854248
Epoch 250, val loss: 0.7998255491256714
Epoch 260, training loss: 0.4148884415626526 = 0.34503331780433655 + 0.01 * 6.9855122566223145
Epoch 260, val loss: 0.7936760187149048
Epoch 270, training loss: 0.3806088864803314 = 0.31086674332618713 + 0.01 * 6.974213600158691
Epoch 270, val loss: 0.7920851111412048
Epoch 280, training loss: 0.3493008017539978 = 0.27967318892478943 + 0.01 * 6.962761402130127
Epoch 280, val loss: 0.7942182421684265
Epoch 290, training loss: 0.32063060998916626 = 0.251101553440094 + 0.01 * 6.952907085418701
Epoch 290, val loss: 0.7992522716522217
Epoch 300, training loss: 0.29441162943840027 = 0.22496537864208221 + 0.01 * 6.944624423980713
Epoch 300, val loss: 0.8067063689231873
Epoch 310, training loss: 0.2704692482948303 = 0.20111338794231415 + 0.01 * 6.935585021972656
Epoch 310, val loss: 0.8161328434944153
Epoch 320, training loss: 0.24864482879638672 = 0.17936260998249054 + 0.01 * 6.928221702575684
Epoch 320, val loss: 0.8273575305938721
Epoch 330, training loss: 0.22878648340702057 = 0.15952961146831512 + 0.01 * 6.925687313079834
Epoch 330, val loss: 0.8403158783912659
Epoch 340, training loss: 0.21062733232975006 = 0.14145803451538086 + 0.01 * 6.916930198669434
Epoch 340, val loss: 0.8548164963722229
Epoch 350, training loss: 0.19436082243919373 = 0.1252722144126892 + 0.01 * 6.908860206604004
Epoch 350, val loss: 0.8704373240470886
Epoch 360, training loss: 0.1801334023475647 = 0.11113148182630539 + 0.01 * 6.900191307067871
Epoch 360, val loss: 0.8870539665222168
Epoch 370, training loss: 0.1678045094013214 = 0.09890887886285782 + 0.01 * 6.88956356048584
Epoch 370, val loss: 0.9044281840324402
Epoch 380, training loss: 0.15722815692424774 = 0.08829277753829956 + 0.01 * 6.893537998199463
Epoch 380, val loss: 0.9225216507911682
Epoch 390, training loss: 0.14789128303527832 = 0.07906893640756607 + 0.01 * 6.882235527038574
Epoch 390, val loss: 0.9411333799362183
Epoch 400, training loss: 0.13972675800323486 = 0.07102565467357635 + 0.01 * 6.870110988616943
Epoch 400, val loss: 0.9600156545639038
Epoch 410, training loss: 0.13259950280189514 = 0.06399184465408325 + 0.01 * 6.860766887664795
Epoch 410, val loss: 0.9790721535682678
Epoch 420, training loss: 0.12633894383907318 = 0.05782764405012131 + 0.01 * 6.851130485534668
Epoch 420, val loss: 0.9980177283287048
Epoch 430, training loss: 0.12095947563648224 = 0.05241267755627632 + 0.01 * 6.854679584503174
Epoch 430, val loss: 1.016856074333191
Epoch 440, training loss: 0.11613146960735321 = 0.047646813094615936 + 0.01 * 6.848465919494629
Epoch 440, val loss: 1.0351516008377075
Epoch 450, training loss: 0.1117115318775177 = 0.04343157634139061 + 0.01 * 6.827995777130127
Epoch 450, val loss: 1.053145170211792
Epoch 460, training loss: 0.10799293220043182 = 0.03969110548496246 + 0.01 * 6.8301825523376465
Epoch 460, val loss: 1.0707850456237793
Epoch 470, training loss: 0.1047629714012146 = 0.036368582397699356 + 0.01 * 6.8394389152526855
Epoch 470, val loss: 1.0879877805709839
Epoch 480, training loss: 0.10147900879383087 = 0.03340956196188927 + 0.01 * 6.806945323944092
Epoch 480, val loss: 1.1045753955841064
Epoch 490, training loss: 0.09874344617128372 = 0.030764607712626457 + 0.01 * 6.7978835105896
Epoch 490, val loss: 1.1208956241607666
Epoch 500, training loss: 0.09650640189647675 = 0.02839469723403454 + 0.01 * 6.8111701011657715
Epoch 500, val loss: 1.1367372274398804
Epoch 510, training loss: 0.09408309310674667 = 0.02626929245889187 + 0.01 * 6.7813801765441895
Epoch 510, val loss: 1.152096152305603
Epoch 520, training loss: 0.09214955568313599 = 0.024357931688427925 + 0.01 * 6.779162406921387
Epoch 520, val loss: 1.1669946908950806
Epoch 530, training loss: 0.09030419588088989 = 0.022633958607912064 + 0.01 * 6.767024040222168
Epoch 530, val loss: 1.1814231872558594
Epoch 540, training loss: 0.08870600908994675 = 0.021077504381537437 + 0.01 * 6.762850761413574
Epoch 540, val loss: 1.1953831911087036
Epoch 550, training loss: 0.08716708421707153 = 0.019670385867357254 + 0.01 * 6.749670505523682
Epoch 550, val loss: 1.2088924646377563
Epoch 560, training loss: 0.08595408499240875 = 0.01839490607380867 + 0.01 * 6.755917549133301
Epoch 560, val loss: 1.2220078706741333
Epoch 570, training loss: 0.08468258380889893 = 0.01723807118833065 + 0.01 * 6.744451522827148
Epoch 570, val loss: 1.234739899635315
Epoch 580, training loss: 0.08356775343418121 = 0.016185401007533073 + 0.01 * 6.738234996795654
Epoch 580, val loss: 1.2469614744186401
Epoch 590, training loss: 0.08246871829032898 = 0.0152250612154603 + 0.01 * 6.724366188049316
Epoch 590, val loss: 1.2588945627212524
Epoch 600, training loss: 0.08167295902967453 = 0.014345933683216572 + 0.01 * 6.732702732086182
Epoch 600, val loss: 1.270423173904419
Epoch 610, training loss: 0.08070102334022522 = 0.013541006483137608 + 0.01 * 6.716001987457275
Epoch 610, val loss: 1.2815433740615845
Epoch 620, training loss: 0.07992107421159744 = 0.012803511694073677 + 0.01 * 6.711756706237793
Epoch 620, val loss: 1.292367696762085
Epoch 630, training loss: 0.07927272468805313 = 0.01212560199201107 + 0.01 * 6.714712142944336
Epoch 630, val loss: 1.3027714490890503
Epoch 640, training loss: 0.0784965455532074 = 0.011501237750053406 + 0.01 * 6.699531078338623
Epoch 640, val loss: 1.3129724264144897
Epoch 650, training loss: 0.07783711701631546 = 0.010925515554845333 + 0.01 * 6.691160202026367
Epoch 650, val loss: 1.3227670192718506
Epoch 660, training loss: 0.07735970616340637 = 0.010393988341093063 + 0.01 * 6.6965718269348145
Epoch 660, val loss: 1.3321889638900757
Epoch 670, training loss: 0.07667317241430283 = 0.009901952929794788 + 0.01 * 6.677121639251709
Epoch 670, val loss: 1.3414978981018066
Epoch 680, training loss: 0.07614673674106598 = 0.00944574549794197 + 0.01 * 6.670098781585693
Epoch 680, val loss: 1.3505198955535889
Epoch 690, training loss: 0.07574265450239182 = 0.009021916426718235 + 0.01 * 6.672073841094971
Epoch 690, val loss: 1.3591541051864624
Epoch 700, training loss: 0.07530411332845688 = 0.008627069182693958 + 0.01 * 6.667704105377197
Epoch 700, val loss: 1.3675622940063477
Epoch 710, training loss: 0.07496367394924164 = 0.008258991874754429 + 0.01 * 6.670468807220459
Epoch 710, val loss: 1.3758693933486938
Epoch 720, training loss: 0.07443314045667648 = 0.007915586233139038 + 0.01 * 6.6517558097839355
Epoch 720, val loss: 1.3838481903076172
Epoch 730, training loss: 0.0741191878914833 = 0.007594899740070105 + 0.01 * 6.652429103851318
Epoch 730, val loss: 1.3915714025497437
Epoch 740, training loss: 0.07400866597890854 = 0.007294843904674053 + 0.01 * 6.671382904052734
Epoch 740, val loss: 1.399081826210022
Epoch 750, training loss: 0.07338149100542068 = 0.0070134918205440044 + 0.01 * 6.6367998123168945
Epoch 750, val loss: 1.4064691066741943
Epoch 760, training loss: 0.0730949267745018 = 0.006749392952769995 + 0.01 * 6.6345534324646
Epoch 760, val loss: 1.4137152433395386
Epoch 770, training loss: 0.07285195589065552 = 0.006501617841422558 + 0.01 * 6.635034084320068
Epoch 770, val loss: 1.420527458190918
Epoch 780, training loss: 0.0726265013217926 = 0.006268661003559828 + 0.01 * 6.635784149169922
Epoch 780, val loss: 1.4274235963821411
Epoch 790, training loss: 0.07222149521112442 = 0.006049203686416149 + 0.01 * 6.617229461669922
Epoch 790, val loss: 1.4339483976364136
Epoch 800, training loss: 0.07203295826911926 = 0.005842562764883041 + 0.01 * 6.619039535522461
Epoch 800, val loss: 1.4403645992279053
Epoch 810, training loss: 0.07189028710126877 = 0.005647799000144005 + 0.01 * 6.62424898147583
Epoch 810, val loss: 1.446661114692688
Epoch 820, training loss: 0.0715143084526062 = 0.0054633039981126785 + 0.01 * 6.605100631713867
Epoch 820, val loss: 1.4528424739837646
Epoch 830, training loss: 0.0713111013174057 = 0.0052888235077261925 + 0.01 * 6.602228164672852
Epoch 830, val loss: 1.4587554931640625
Epoch 840, training loss: 0.07105837762355804 = 0.005123940762132406 + 0.01 * 6.593443393707275
Epoch 840, val loss: 1.4645272493362427
Epoch 850, training loss: 0.07100296765565872 = 0.004967871587723494 + 0.01 * 6.603509902954102
Epoch 850, val loss: 1.4703127145767212
Epoch 860, training loss: 0.07078030705451965 = 0.004819805268198252 + 0.01 * 6.59605073928833
Epoch 860, val loss: 1.4758142232894897
Epoch 870, training loss: 0.07067392766475677 = 0.004679452162235975 + 0.01 * 6.599448204040527
Epoch 870, val loss: 1.481135368347168
Epoch 880, training loss: 0.07046956568956375 = 0.004545765463262796 + 0.01 * 6.592380046844482
Epoch 880, val loss: 1.4865591526031494
Epoch 890, training loss: 0.07020942121744156 = 0.004418863914906979 + 0.01 * 6.5790557861328125
Epoch 890, val loss: 1.491677165031433
Epoch 900, training loss: 0.07010459154844284 = 0.004297827836126089 + 0.01 * 6.580676555633545
Epoch 900, val loss: 1.4966479539871216
Epoch 910, training loss: 0.07007608562707901 = 0.00418255478143692 + 0.01 * 6.589353084564209
Epoch 910, val loss: 1.5016655921936035
Epoch 920, training loss: 0.07000096142292023 = 0.0040725902654230595 + 0.01 * 6.592836856842041
Epoch 920, val loss: 1.506414532661438
Epoch 930, training loss: 0.06976675242185593 = 0.003967852331697941 + 0.01 * 6.579890251159668
Epoch 930, val loss: 1.5112489461898804
Epoch 940, training loss: 0.06944531202316284 = 0.003867632243782282 + 0.01 * 6.557767868041992
Epoch 940, val loss: 1.5158723592758179
Epoch 950, training loss: 0.06926683336496353 = 0.0037717618979513645 + 0.01 * 6.5495076179504395
Epoch 950, val loss: 1.5202933549880981
Epoch 960, training loss: 0.06962399184703827 = 0.003680497407913208 + 0.01 * 6.594349384307861
Epoch 960, val loss: 1.5247465372085571
Epoch 970, training loss: 0.06908759474754333 = 0.003592630149796605 + 0.01 * 6.549496173858643
Epoch 970, val loss: 1.5290756225585938
Epoch 980, training loss: 0.06913381069898605 = 0.0035090199671685696 + 0.01 * 6.562479019165039
Epoch 980, val loss: 1.5332541465759277
Epoch 990, training loss: 0.06897105276584625 = 0.003428724827244878 + 0.01 * 6.554232597351074
Epoch 990, val loss: 1.5373992919921875
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.7491
Flip ASR: 0.7111/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.033315420150757 = 1.9495770931243896 + 0.01 * 8.373842239379883
Epoch 0, val loss: 1.9491907358169556
Epoch 10, training loss: 2.0225441455841064 = 1.9388078451156616 + 0.01 * 8.373620986938477
Epoch 10, val loss: 1.936981201171875
Epoch 20, training loss: 2.0091769695281982 = 1.9254474639892578 + 0.01 * 8.372950553894043
Epoch 20, val loss: 1.9214545488357544
Epoch 30, training loss: 1.9907265901565552 = 1.9070186614990234 + 0.01 * 8.370789527893066
Epoch 30, val loss: 1.8998677730560303
Epoch 40, training loss: 1.9642510414123535 = 1.8806111812591553 + 0.01 * 8.363982200622559
Epoch 40, val loss: 1.8693398237228394
Epoch 50, training loss: 1.9277653694152832 = 1.8444279432296753 + 0.01 * 8.333747863769531
Epoch 50, val loss: 1.8294291496276855
Epoch 60, training loss: 1.8826824426651 = 1.801344633102417 + 0.01 * 8.13377857208252
Epoch 60, val loss: 1.785944938659668
Epoch 70, training loss: 1.8375957012176514 = 1.7594943046569824 + 0.01 * 7.81013822555542
Epoch 70, val loss: 1.7494148015975952
Epoch 80, training loss: 1.789585828781128 = 1.7137606143951416 + 0.01 * 7.582526683807373
Epoch 80, val loss: 1.7127978801727295
Epoch 90, training loss: 1.7255945205688477 = 1.6518051624298096 + 0.01 * 7.378931999206543
Epoch 90, val loss: 1.6629356145858765
Epoch 100, training loss: 1.6413264274597168 = 1.5686774253845215 + 0.01 * 7.264901161193848
Epoch 100, val loss: 1.5958905220031738
Epoch 110, training loss: 1.5364304780960083 = 1.4641740322113037 + 0.01 * 7.225645542144775
Epoch 110, val loss: 1.5139920711517334
Epoch 120, training loss: 1.4190895557403564 = 1.34707772731781 + 0.01 * 7.201178073883057
Epoch 120, val loss: 1.4240546226501465
Epoch 130, training loss: 1.2968014478683472 = 1.225035548210144 + 0.01 * 7.176586627960205
Epoch 130, val loss: 1.331913948059082
Epoch 140, training loss: 1.175257682800293 = 1.1036776304244995 + 0.01 * 7.15800666809082
Epoch 140, val loss: 1.2414182424545288
Epoch 150, training loss: 1.0604056119918823 = 0.9889293909072876 + 0.01 * 7.147626876831055
Epoch 150, val loss: 1.156079649925232
Epoch 160, training loss: 0.9565134048461914 = 0.8851031064987183 + 0.01 * 7.141026973724365
Epoch 160, val loss: 1.0796698331832886
Epoch 170, training loss: 0.8655277490615845 = 0.7942132949829102 + 0.01 * 7.131443023681641
Epoch 170, val loss: 1.0140882730484009
Epoch 180, training loss: 0.7868050336837769 = 0.7156713008880615 + 0.01 * 7.113371849060059
Epoch 180, val loss: 0.9585016369819641
Epoch 190, training loss: 0.7179964184761047 = 0.6471843123435974 + 0.01 * 7.081209182739258
Epoch 190, val loss: 0.9111809134483337
Epoch 200, training loss: 0.6565734148025513 = 0.5860568284988403 + 0.01 * 7.051661491394043
Epoch 200, val loss: 0.8700789213180542
Epoch 210, training loss: 0.6002094149589539 = 0.5300692319869995 + 0.01 * 7.014017105102539
Epoch 210, val loss: 0.8337284922599792
Epoch 220, training loss: 0.5478380918502808 = 0.4779212474822998 + 0.01 * 6.991685390472412
Epoch 220, val loss: 0.8014888167381287
Epoch 230, training loss: 0.49876323342323303 = 0.4290967583656311 + 0.01 * 6.966647624969482
Epoch 230, val loss: 0.7735896706581116
Epoch 240, training loss: 0.4529320001602173 = 0.383393257856369 + 0.01 * 6.953874588012695
Epoch 240, val loss: 0.7497367858886719
Epoch 250, training loss: 0.4101640284061432 = 0.3408028185367584 + 0.01 * 6.936120986938477
Epoch 250, val loss: 0.730250895023346
Epoch 260, training loss: 0.37052029371261597 = 0.30136311054229736 + 0.01 * 6.915717124938965
Epoch 260, val loss: 0.7150993347167969
Epoch 270, training loss: 0.33434224128723145 = 0.2651095390319824 + 0.01 * 6.923269271850586
Epoch 270, val loss: 0.7040629386901855
Epoch 280, training loss: 0.3011241853237152 = 0.23212672770023346 + 0.01 * 6.899744987487793
Epoch 280, val loss: 0.6969624161720276
Epoch 290, training loss: 0.27140820026397705 = 0.20259425044059753 + 0.01 * 6.881396293640137
Epoch 290, val loss: 0.6936297416687012
Epoch 300, training loss: 0.24537847936153412 = 0.17659543454647064 + 0.01 * 6.878304958343506
Epoch 300, val loss: 0.6932862401008606
Epoch 310, training loss: 0.22261303663253784 = 0.15400820970535278 + 0.01 * 6.860482692718506
Epoch 310, val loss: 0.6957345604896545
Epoch 320, training loss: 0.20297497510910034 = 0.13444125652313232 + 0.01 * 6.853372097015381
Epoch 320, val loss: 0.7001804113388062
Epoch 330, training loss: 0.18592023849487305 = 0.11750087887048721 + 0.01 * 6.841935634613037
Epoch 330, val loss: 0.7063612937927246
Epoch 340, training loss: 0.17121940851211548 = 0.1028662621974945 + 0.01 * 6.835315227508545
Epoch 340, val loss: 0.7141124606132507
Epoch 350, training loss: 0.15850333869457245 = 0.0902312770485878 + 0.01 * 6.827206611633301
Epoch 350, val loss: 0.7231909036636353
Epoch 360, training loss: 0.14759725332260132 = 0.07935865968465805 + 0.01 * 6.823859691619873
Epoch 360, val loss: 0.7332583069801331
Epoch 370, training loss: 0.13800537586212158 = 0.07004835456609726 + 0.01 * 6.7957024574279785
Epoch 370, val loss: 0.7441653609275818
Epoch 380, training loss: 0.13029694557189941 = 0.06202356889843941 + 0.01 * 6.827338218688965
Epoch 380, val loss: 0.755456268787384
Epoch 390, training loss: 0.12289842963218689 = 0.05514001101255417 + 0.01 * 6.775842189788818
Epoch 390, val loss: 0.7667209506034851
Epoch 400, training loss: 0.11694616824388504 = 0.0492141991853714 + 0.01 * 6.773196697235107
Epoch 400, val loss: 0.7781696319580078
Epoch 410, training loss: 0.11203552782535553 = 0.044127825647592545 + 0.01 * 6.790770053863525
Epoch 410, val loss: 0.7894459366798401
Epoch 420, training loss: 0.10735517740249634 = 0.039762940257787704 + 0.01 * 6.7592244148254395
Epoch 420, val loss: 0.8006101250648499
Epoch 430, training loss: 0.10355071723461151 = 0.03596942126750946 + 0.01 * 6.758130073547363
Epoch 430, val loss: 0.811538815498352
Epoch 440, training loss: 0.10009691119194031 = 0.03267776221036911 + 0.01 * 6.741915225982666
Epoch 440, val loss: 0.8222949504852295
Epoch 450, training loss: 0.09727081656455994 = 0.029811890795826912 + 0.01 * 6.7458930015563965
Epoch 450, val loss: 0.8328678607940674
Epoch 460, training loss: 0.09464794397354126 = 0.02730555832386017 + 0.01 * 6.734238624572754
Epoch 460, val loss: 0.8431971669197083
Epoch 470, training loss: 0.09222888201475143 = 0.025103742256760597 + 0.01 * 6.712514400482178
Epoch 470, val loss: 0.8532810807228088
Epoch 480, training loss: 0.09017020463943481 = 0.023159032687544823 + 0.01 * 6.701117515563965
Epoch 480, val loss: 0.8631229996681213
Epoch 490, training loss: 0.08836820721626282 = 0.021435072645545006 + 0.01 * 6.693313121795654
Epoch 490, val loss: 0.872775673866272
Epoch 500, training loss: 0.0869513601064682 = 0.01990104280412197 + 0.01 * 6.705031871795654
Epoch 500, val loss: 0.8822142481803894
Epoch 510, training loss: 0.08524970710277557 = 0.01853489689528942 + 0.01 * 6.671481132507324
Epoch 510, val loss: 0.8913404941558838
Epoch 520, training loss: 0.08395438641309738 = 0.017308434471488 + 0.01 * 6.664595127105713
Epoch 520, val loss: 0.9002209901809692
Epoch 530, training loss: 0.08312788605690002 = 0.016202691942453384 + 0.01 * 6.692519664764404
Epoch 530, val loss: 0.9089335203170776
Epoch 540, training loss: 0.08181355893611908 = 0.015206987038254738 + 0.01 * 6.66065788269043
Epoch 540, val loss: 0.9173341989517212
Epoch 550, training loss: 0.08102427423000336 = 0.014305554330348969 + 0.01 * 6.671872138977051
Epoch 550, val loss: 0.9256018996238708
Epoch 560, training loss: 0.0800432339310646 = 0.013484954833984375 + 0.01 * 6.65582799911499
Epoch 560, val loss: 0.933586061000824
Epoch 570, training loss: 0.07915794849395752 = 0.012736786156892776 + 0.01 * 6.642116546630859
Epoch 570, val loss: 0.9414449334144592
Epoch 580, training loss: 0.07832509279251099 = 0.01205379981547594 + 0.01 * 6.627129077911377
Epoch 580, val loss: 0.9489765167236328
Epoch 590, training loss: 0.07785719633102417 = 0.01142775360494852 + 0.01 * 6.6429443359375
Epoch 590, val loss: 0.9564393162727356
Epoch 600, training loss: 0.0769226923584938 = 0.010853290557861328 + 0.01 * 6.606940269470215
Epoch 600, val loss: 0.9636371731758118
Epoch 610, training loss: 0.076823890209198 = 0.010323655791580677 + 0.01 * 6.650023460388184
Epoch 610, val loss: 0.9706667065620422
Epoch 620, training loss: 0.07595645636320114 = 0.009835965931415558 + 0.01 * 6.612049102783203
Epoch 620, val loss: 0.9774851202964783
Epoch 630, training loss: 0.0758446753025055 = 0.009384123608469963 + 0.01 * 6.646055221557617
Epoch 630, val loss: 0.9842443466186523
Epoch 640, training loss: 0.07493933290243149 = 0.008965743705630302 + 0.01 * 6.5973591804504395
Epoch 640, val loss: 0.9906814098358154
Epoch 650, training loss: 0.07451587170362473 = 0.008576149120926857 + 0.01 * 6.593972682952881
Epoch 650, val loss: 0.9970940351486206
Epoch 660, training loss: 0.0740995705127716 = 0.008212324231863022 + 0.01 * 6.588724136352539
Epoch 660, val loss: 1.0032765865325928
Epoch 670, training loss: 0.073667973279953 = 0.007872533984482288 + 0.01 * 6.579543590545654
Epoch 670, val loss: 1.009397268295288
Epoch 680, training loss: 0.07335453480482101 = 0.007556000724434853 + 0.01 * 6.579853534698486
Epoch 680, val loss: 1.0153226852416992
Epoch 690, training loss: 0.07319985330104828 = 0.007261187769472599 + 0.01 * 6.59386682510376
Epoch 690, val loss: 1.0211564302444458
Epoch 700, training loss: 0.07269072532653809 = 0.006985490210354328 + 0.01 * 6.570523262023926
Epoch 700, val loss: 1.0268172025680542
Epoch 710, training loss: 0.07242833822965622 = 0.006725833751261234 + 0.01 * 6.570250988006592
Epoch 710, val loss: 1.0322935581207275
Epoch 720, training loss: 0.07202338427305222 = 0.006482159718871117 + 0.01 * 6.554122447967529
Epoch 720, val loss: 1.0377229452133179
Epoch 730, training loss: 0.07178109884262085 = 0.006251447834074497 + 0.01 * 6.5529656410217285
Epoch 730, val loss: 1.0430383682250977
Epoch 740, training loss: 0.07148469984531403 = 0.006034775171428919 + 0.01 * 6.544992446899414
Epoch 740, val loss: 1.048243522644043
Epoch 750, training loss: 0.0712699294090271 = 0.0058307950384914875 + 0.01 * 6.5439133644104
Epoch 750, val loss: 1.053251028060913
Epoch 760, training loss: 0.07120206207036972 = 0.005638085305690765 + 0.01 * 6.556397914886475
Epoch 760, val loss: 1.0581841468811035
Epoch 770, training loss: 0.07078981399536133 = 0.005456112790852785 + 0.01 * 6.533370018005371
Epoch 770, val loss: 1.063084602355957
Epoch 780, training loss: 0.070548415184021 = 0.005284104496240616 + 0.01 * 6.526431560516357
Epoch 780, val loss: 1.0677915811538696
Epoch 790, training loss: 0.07027813792228699 = 0.005121160298585892 + 0.01 * 6.515697479248047
Epoch 790, val loss: 1.0724068880081177
Epoch 800, training loss: 0.07018665969371796 = 0.004966420121490955 + 0.01 * 6.522024154663086
Epoch 800, val loss: 1.0769567489624023
Epoch 810, training loss: 0.07003753632307053 = 0.004819903988391161 + 0.01 * 6.521763324737549
Epoch 810, val loss: 1.0813686847686768
Epoch 820, training loss: 0.06980979442596436 = 0.004680273123085499 + 0.01 * 6.5129523277282715
Epoch 820, val loss: 1.0857819318771362
Epoch 830, training loss: 0.06977745145559311 = 0.004547697491943836 + 0.01 * 6.522974967956543
Epoch 830, val loss: 1.090026617050171
Epoch 840, training loss: 0.069444939494133 = 0.004421472083777189 + 0.01 * 6.502346992492676
Epoch 840, val loss: 1.0942258834838867
Epoch 850, training loss: 0.06921545416116714 = 0.004301061853766441 + 0.01 * 6.491439342498779
Epoch 850, val loss: 1.0983399152755737
Epoch 860, training loss: 0.06923427432775497 = 0.0041860430501401424 + 0.01 * 6.504823684692383
Epoch 860, val loss: 1.1024330854415894
Epoch 870, training loss: 0.06905733793973923 = 0.0040764822624623775 + 0.01 * 6.4980854988098145
Epoch 870, val loss: 1.106372356414795
Epoch 880, training loss: 0.06888335198163986 = 0.003971760626882315 + 0.01 * 6.491158962249756
Epoch 880, val loss: 1.1102732419967651
Epoch 890, training loss: 0.06870729476213455 = 0.00387156056240201 + 0.01 * 6.4835734367370605
Epoch 890, val loss: 1.1140756607055664
Epoch 900, training loss: 0.06858839839696884 = 0.0037758643738925457 + 0.01 * 6.481253623962402
Epoch 900, val loss: 1.117832899093628
Epoch 910, training loss: 0.06848554313182831 = 0.003684090217575431 + 0.01 * 6.4801459312438965
Epoch 910, val loss: 1.1215425729751587
Epoch 920, training loss: 0.06850443035364151 = 0.0035964723210781813 + 0.01 * 6.4907965660095215
Epoch 920, val loss: 1.125150442123413
Epoch 930, training loss: 0.06820990890264511 = 0.0035121391993016005 + 0.01 * 6.4697771072387695
Epoch 930, val loss: 1.1287122964859009
Epoch 940, training loss: 0.06818599253892899 = 0.0034313227515667677 + 0.01 * 6.475467681884766
Epoch 940, val loss: 1.1322330236434937
Epoch 950, training loss: 0.06813488155603409 = 0.0033533175010234118 + 0.01 * 6.478156089782715
Epoch 950, val loss: 1.135666847229004
Epoch 960, training loss: 0.06795527040958405 = 0.0032782519701868296 + 0.01 * 6.4677019119262695
Epoch 960, val loss: 1.1390304565429688
Epoch 970, training loss: 0.06811876595020294 = 0.0032060379162430763 + 0.01 * 6.491272926330566
Epoch 970, val loss: 1.1423417329788208
Epoch 980, training loss: 0.06780975311994553 = 0.0031366830226033926 + 0.01 * 6.4673075675964355
Epoch 980, val loss: 1.145588755607605
Epoch 990, training loss: 0.0676196739077568 = 0.0030700755305588245 + 0.01 * 6.454959869384766
Epoch 990, val loss: 1.148763656616211
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.6679
Flip ASR: 0.6267/225 nodes
The final ASR:0.63715, 0.10620, Accuracy:0.78765, 0.03947
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11602])
remove edge: torch.Size([2, 9510])
updated graph: torch.Size([2, 10556])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
The final ASR:0.97540, 0.00174, Accuracy:0.82716, 0.00175
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.5 der2:0 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.033351421356201 = 1.949613094329834 + 0.01 * 8.373832702636719
Epoch 0, val loss: 1.9425268173217773
Epoch 10, training loss: 2.0235047340393066 = 1.9397672414779663 + 0.01 * 8.37374496459961
Epoch 10, val loss: 1.933526635169983
Epoch 20, training loss: 2.011502504348755 = 1.9277678728103638 + 0.01 * 8.373456001281738
Epoch 20, val loss: 1.9222949743270874
Epoch 30, training loss: 1.9948503971099854 = 1.9111241102218628 + 0.01 * 8.372634887695312
Epoch 30, val loss: 1.9065332412719727
Epoch 40, training loss: 1.9701753854751587 = 1.8864860534667969 + 0.01 * 8.368937492370605
Epoch 40, val loss: 1.883234977722168
Epoch 50, training loss: 1.9344291687011719 = 1.850984811782837 + 0.01 * 8.344438552856445
Epoch 50, val loss: 1.8509927988052368
Epoch 60, training loss: 1.8896009922027588 = 1.807620644569397 + 0.01 * 8.19803237915039
Epoch 60, val loss: 1.8149923086166382
Epoch 70, training loss: 1.8441458940505981 = 1.7654690742492676 + 0.01 * 7.867678642272949
Epoch 70, val loss: 1.7826210260391235
Epoch 80, training loss: 1.7904918193817139 = 1.7175039052963257 + 0.01 * 7.298793792724609
Epoch 80, val loss: 1.741689920425415
Epoch 90, training loss: 1.7219246625900269 = 1.6512268781661987 + 0.01 * 7.069782257080078
Epoch 90, val loss: 1.6836051940917969
Epoch 100, training loss: 1.634373664855957 = 1.5645833015441895 + 0.01 * 6.979040145874023
Epoch 100, val loss: 1.6111329793930054
Epoch 110, training loss: 1.5341593027114868 = 1.4648375511169434 + 0.01 * 6.9321746826171875
Epoch 110, val loss: 1.5317589044570923
Epoch 120, training loss: 1.431410312652588 = 1.3624584674835205 + 0.01 * 6.895181655883789
Epoch 120, val loss: 1.4533811807632446
Epoch 130, training loss: 1.3306217193603516 = 1.26192307472229 + 0.01 * 6.869869709014893
Epoch 130, val loss: 1.3779045343399048
Epoch 140, training loss: 1.2329943180084229 = 1.1644691228866577 + 0.01 * 6.85252571105957
Epoch 140, val loss: 1.3065052032470703
Epoch 150, training loss: 1.141232967376709 = 1.0728106498718262 + 0.01 * 6.842236518859863
Epoch 150, val loss: 1.2397501468658447
Epoch 160, training loss: 1.0571799278259277 = 0.9888046383857727 + 0.01 * 6.837529182434082
Epoch 160, val loss: 1.1797937154769897
Epoch 170, training loss: 0.9793877601623535 = 0.911033570766449 + 0.01 * 6.835417747497559
Epoch 170, val loss: 1.1248544454574585
Epoch 180, training loss: 0.9040325284004211 = 0.8356949090957642 + 0.01 * 6.833759784698486
Epoch 180, val loss: 1.0721274614334106
Epoch 190, training loss: 0.8277633190155029 = 0.7594536542892456 + 0.01 * 6.830967426300049
Epoch 190, val loss: 1.018957495689392
Epoch 200, training loss: 0.7497727870941162 = 0.6814985871315002 + 0.01 * 6.827422618865967
Epoch 200, val loss: 0.9652800559997559
Epoch 210, training loss: 0.6725398898124695 = 0.6043086051940918 + 0.01 * 6.823127269744873
Epoch 210, val loss: 0.9132720828056335
Epoch 220, training loss: 0.5998613834381104 = 0.531680166721344 + 0.01 * 6.818122386932373
Epoch 220, val loss: 0.8667570948600769
Epoch 230, training loss: 0.5342152714729309 = 0.46609699726104736 + 0.01 * 6.811828136444092
Epoch 230, val loss: 0.8283765912055969
Epoch 240, training loss: 0.47608593106269836 = 0.40803489089012146 + 0.01 * 6.805104732513428
Epoch 240, val loss: 0.7984515428543091
Epoch 250, training loss: 0.4246962070465088 = 0.3567056357860565 + 0.01 * 6.79905891418457
Epoch 250, val loss: 0.7758892774581909
Epoch 260, training loss: 0.37901920080184937 = 0.3111051023006439 + 0.01 * 6.791409015655518
Epoch 260, val loss: 0.75941401720047
Epoch 270, training loss: 0.3381747901439667 = 0.27032485604286194 + 0.01 * 6.784994602203369
Epoch 270, val loss: 0.7480576038360596
Epoch 280, training loss: 0.30149248242378235 = 0.23368853330612183 + 0.01 * 6.780395030975342
Epoch 280, val loss: 0.7410164475440979
Epoch 290, training loss: 0.2686246335506439 = 0.20090600848197937 + 0.01 * 6.771862983703613
Epoch 290, val loss: 0.7378512620925903
Epoch 300, training loss: 0.23966234922409058 = 0.17200632393360138 + 0.01 * 6.765603065490723
Epoch 300, val loss: 0.7385439872741699
Epoch 310, training loss: 0.2147967666387558 = 0.147190660238266 + 0.01 * 6.760610580444336
Epoch 310, val loss: 0.7431604862213135
Epoch 320, training loss: 0.19397228956222534 = 0.1263989806175232 + 0.01 * 6.757330417633057
Epoch 320, val loss: 0.7513784170150757
Epoch 330, training loss: 0.1767272651195526 = 0.10918525606393814 + 0.01 * 6.7542009353637695
Epoch 330, val loss: 0.7625914812088013
Epoch 340, training loss: 0.16245004534721375 = 0.0949365496635437 + 0.01 * 6.751349449157715
Epoch 340, val loss: 0.7761377692222595
Epoch 350, training loss: 0.1505814492702484 = 0.0830715224146843 + 0.01 * 6.750993251800537
Epoch 350, val loss: 0.791253387928009
Epoch 360, training loss: 0.1406022310256958 = 0.07312219589948654 + 0.01 * 6.748003005981445
Epoch 360, val loss: 0.8074288964271545
Epoch 370, training loss: 0.1321413815021515 = 0.06472045183181763 + 0.01 * 6.742092609405518
Epoch 370, val loss: 0.8240925073623657
Epoch 380, training loss: 0.12503686547279358 = 0.05758038908243179 + 0.01 * 6.745646953582764
Epoch 380, val loss: 0.8407810926437378
Epoch 390, training loss: 0.11885824799537659 = 0.051482535898685455 + 0.01 * 6.737571716308594
Epoch 390, val loss: 0.8574010133743286
Epoch 400, training loss: 0.11354700475931168 = 0.046242572367191315 + 0.01 * 6.730443477630615
Epoch 400, val loss: 0.8737486004829407
Epoch 410, training loss: 0.10903702676296234 = 0.04171764478087425 + 0.01 * 6.731938362121582
Epoch 410, val loss: 0.8897244930267334
Epoch 420, training loss: 0.10501925647258759 = 0.03779517859220505 + 0.01 * 6.722408294677734
Epoch 420, val loss: 0.90519779920578
Epoch 430, training loss: 0.10153713822364807 = 0.03437712788581848 + 0.01 * 6.716001510620117
Epoch 430, val loss: 0.9201963543891907
Epoch 440, training loss: 0.09859754145145416 = 0.03138786554336548 + 0.01 * 6.720967769622803
Epoch 440, val loss: 0.9348301887512207
Epoch 450, training loss: 0.09582043439149857 = 0.02876291237771511 + 0.01 * 6.705751895904541
Epoch 450, val loss: 0.948918342590332
Epoch 460, training loss: 0.09347088634967804 = 0.026447826996445656 + 0.01 * 6.702306270599365
Epoch 460, val loss: 0.9626480937004089
Epoch 470, training loss: 0.09147517383098602 = 0.02439844235777855 + 0.01 * 6.7076735496521
Epoch 470, val loss: 0.9758551716804504
Epoch 480, training loss: 0.08947718143463135 = 0.022577116265892982 + 0.01 * 6.690007209777832
Epoch 480, val loss: 0.9886305332183838
Epoch 490, training loss: 0.08776608109474182 = 0.02095159702003002 + 0.01 * 6.681448459625244
Epoch 490, val loss: 1.0010181665420532
Epoch 500, training loss: 0.08642111718654633 = 0.01949547417461872 + 0.01 * 6.692564487457275
Epoch 500, val loss: 1.0129667520523071
Epoch 510, training loss: 0.08486566692590714 = 0.01818748004734516 + 0.01 * 6.667819023132324
Epoch 510, val loss: 1.0245656967163086
Epoch 520, training loss: 0.08363226056098938 = 0.017008081078529358 + 0.01 * 6.662418365478516
Epoch 520, val loss: 1.0358473062515259
Epoch 530, training loss: 0.08262915909290314 = 0.015941163524985313 + 0.01 * 6.668799877166748
Epoch 530, val loss: 1.0466867685317993
Epoch 540, training loss: 0.0815705806016922 = 0.014973245561122894 + 0.01 * 6.659733295440674
Epoch 540, val loss: 1.0572669506072998
Epoch 550, training loss: 0.08060715347528458 = 0.01409225631505251 + 0.01 * 6.6514892578125
Epoch 550, val loss: 1.0675137042999268
Epoch 560, training loss: 0.079783596098423 = 0.013288878835737705 + 0.01 * 6.649472236633301
Epoch 560, val loss: 1.0774180889129639
Epoch 570, training loss: 0.07901474088430405 = 0.012554077431559563 + 0.01 * 6.646066665649414
Epoch 570, val loss: 1.0870059728622437
Epoch 580, training loss: 0.07826055586338043 = 0.01188034936785698 + 0.01 * 6.638020992279053
Epoch 580, val loss: 1.0963683128356934
Epoch 590, training loss: 0.07761602103710175 = 0.011260930448770523 + 0.01 * 6.635509490966797
Epoch 590, val loss: 1.1054534912109375
Epoch 600, training loss: 0.07706422358751297 = 0.010690473951399326 + 0.01 * 6.6373748779296875
Epoch 600, val loss: 1.1142550706863403
Epoch 610, training loss: 0.07649590820074081 = 0.0101639898493886 + 0.01 * 6.6331915855407715
Epoch 610, val loss: 1.1228656768798828
Epoch 620, training loss: 0.07594375312328339 = 0.009677132591605186 + 0.01 * 6.626661777496338
Epoch 620, val loss: 1.1311568021774292
Epoch 630, training loss: 0.07546387612819672 = 0.009225800633430481 + 0.01 * 6.623807907104492
Epoch 630, val loss: 1.1393213272094727
Epoch 640, training loss: 0.07499273866415024 = 0.008807167410850525 + 0.01 * 6.618557453155518
Epoch 640, val loss: 1.1471707820892334
Epoch 650, training loss: 0.07459622621536255 = 0.008418175391852856 + 0.01 * 6.617805480957031
Epoch 650, val loss: 1.1549056768417358
Epoch 660, training loss: 0.07417399436235428 = 0.008055922575294971 + 0.01 * 6.611807346343994
Epoch 660, val loss: 1.1623843908309937
Epoch 670, training loss: 0.07387979328632355 = 0.007717857602983713 + 0.01 * 6.616194248199463
Epoch 670, val loss: 1.1696676015853882
Epoch 680, training loss: 0.07349241524934769 = 0.007402007933706045 + 0.01 * 6.6090407371521
Epoch 680, val loss: 1.1768807172775269
Epoch 690, training loss: 0.07320185750722885 = 0.007106406148523092 + 0.01 * 6.60954475402832
Epoch 690, val loss: 1.183870553970337
Epoch 700, training loss: 0.07302383333444595 = 0.006829550489783287 + 0.01 * 6.6194281578063965
Epoch 700, val loss: 1.190595269203186
Epoch 710, training loss: 0.07262855023145676 = 0.00657009007409215 + 0.01 * 6.605845928192139
Epoch 710, val loss: 1.1972954273223877
Epoch 720, training loss: 0.07229384779930115 = 0.00632629357278347 + 0.01 * 6.596755504608154
Epoch 720, val loss: 1.203711986541748
Epoch 730, training loss: 0.07203473150730133 = 0.006096970289945602 + 0.01 * 6.593776702880859
Epoch 730, val loss: 1.2100731134414673
Epoch 740, training loss: 0.07185491919517517 = 0.00588091928511858 + 0.01 * 6.597400188446045
Epoch 740, val loss: 1.2162525653839111
Epoch 750, training loss: 0.07197065651416779 = 0.005677408073097467 + 0.01 * 6.629324913024902
Epoch 750, val loss: 1.2221887111663818
Epoch 760, training loss: 0.0713953971862793 = 0.005485810339450836 + 0.01 * 6.590959072113037
Epoch 760, val loss: 1.228210210800171
Epoch 770, training loss: 0.07116495072841644 = 0.00530492328107357 + 0.01 * 6.586002826690674
Epoch 770, val loss: 1.2338941097259521
Epoch 780, training loss: 0.07094008475542068 = 0.005133801605552435 + 0.01 * 6.580628871917725
Epoch 780, val loss: 1.2395319938659668
Epoch 790, training loss: 0.07077515870332718 = 0.0049715726636350155 + 0.01 * 6.580358982086182
Epoch 790, val loss: 1.2450262308120728
Epoch 800, training loss: 0.07059083133935928 = 0.004817751236259937 + 0.01 * 6.577308177947998
Epoch 800, val loss: 1.2504093647003174
Epoch 810, training loss: 0.07043030858039856 = 0.004671982955187559 + 0.01 * 6.575832366943359
Epoch 810, val loss: 1.255662441253662
Epoch 820, training loss: 0.07020007818937302 = 0.004533705301582813 + 0.01 * 6.5666375160217285
Epoch 820, val loss: 1.2608524560928345
Epoch 830, training loss: 0.07009388506412506 = 0.004402410238981247 + 0.01 * 6.56914758682251
Epoch 830, val loss: 1.2657403945922852
Epoch 840, training loss: 0.06997407227754593 = 0.004277495667338371 + 0.01 * 6.569657325744629
Epoch 840, val loss: 1.270904302597046
Epoch 850, training loss: 0.06983024626970291 = 0.0041586910374462605 + 0.01 * 6.567155838012695
Epoch 850, val loss: 1.2756205797195435
Epoch 860, training loss: 0.0698479562997818 = 0.004045441746711731 + 0.01 * 6.580251693725586
Epoch 860, val loss: 1.2803422212600708
Epoch 870, training loss: 0.069585420191288 = 0.003937643487006426 + 0.01 * 6.564777374267578
Epoch 870, val loss: 1.2850850820541382
Epoch 880, training loss: 0.06939230114221573 = 0.003834903473034501 + 0.01 * 6.555739879608154
Epoch 880, val loss: 1.2894890308380127
Epoch 890, training loss: 0.06929855048656464 = 0.0037368107587099075 + 0.01 * 6.556174278259277
Epoch 890, val loss: 1.293977975845337
Epoch 900, training loss: 0.06913281232118607 = 0.0036431527696549892 + 0.01 * 6.548965930938721
Epoch 900, val loss: 1.2983161211013794
Epoch 910, training loss: 0.06897527724504471 = 0.0035536130890250206 + 0.01 * 6.542166709899902
Epoch 910, val loss: 1.3025104999542236
Epoch 920, training loss: 0.06886951625347137 = 0.0034681300166994333 + 0.01 * 6.5401387214660645
Epoch 920, val loss: 1.3067705631256104
Epoch 930, training loss: 0.06879996508359909 = 0.003386344062164426 + 0.01 * 6.541362762451172
Epoch 930, val loss: 1.3109244108200073
Epoch 940, training loss: 0.06880106031894684 = 0.0033079625573009253 + 0.01 * 6.549309730529785
Epoch 940, val loss: 1.3149185180664062
Epoch 950, training loss: 0.06867460906505585 = 0.0032330104149878025 + 0.01 * 6.544159889221191
Epoch 950, val loss: 1.3188871145248413
Epoch 960, training loss: 0.06858659535646439 = 0.003160917665809393 + 0.01 * 6.542567729949951
Epoch 960, val loss: 1.3226853609085083
Epoch 970, training loss: 0.06857283413410187 = 0.003091884544119239 + 0.01 * 6.548095226287842
Epoch 970, val loss: 1.3266220092773438
Epoch 980, training loss: 0.06830158084630966 = 0.0030256544705480337 + 0.01 * 6.52759313583374
Epoch 980, val loss: 1.3303797245025635
Epoch 990, training loss: 0.06851612031459808 = 0.00296205491758883 + 0.01 * 6.5554070472717285
Epoch 990, val loss: 1.3339072465896606
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.7122
Flip ASR: 0.6622/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.03281307220459 = 1.9490747451782227 + 0.01 * 8.373835563659668
Epoch 0, val loss: 1.9527416229248047
Epoch 10, training loss: 2.0219287872314453 = 1.9381921291351318 + 0.01 * 8.373664855957031
Epoch 10, val loss: 1.9412869215011597
Epoch 20, training loss: 2.0082056522369385 = 1.9244742393493652 + 0.01 * 8.373146057128906
Epoch 20, val loss: 1.9259216785430908
Epoch 30, training loss: 1.988870620727539 = 1.9051536321640015 + 0.01 * 8.371705055236816
Epoch 30, val loss: 1.9034055471420288
Epoch 40, training loss: 1.9605292081832886 = 1.8768612146377563 + 0.01 * 8.366798400878906
Epoch 40, val loss: 1.870423436164856
Epoch 50, training loss: 1.920794129371643 = 1.8373945951461792 + 0.01 * 8.339958190917969
Epoch 50, val loss: 1.8264217376708984
Epoch 60, training loss: 1.8723454475402832 = 1.7910486459732056 + 0.01 * 8.129679679870605
Epoch 60, val loss: 1.7790510654449463
Epoch 70, training loss: 1.8227931261062622 = 1.7462793588638306 + 0.01 * 7.651373863220215
Epoch 70, val loss: 1.7377616167068481
Epoch 80, training loss: 1.768287181854248 = 1.693323016166687 + 0.01 * 7.4964141845703125
Epoch 80, val loss: 1.691254734992981
Epoch 90, training loss: 1.6953661441802979 = 1.621142029762268 + 0.01 * 7.422409534454346
Epoch 90, val loss: 1.6310991048812866
Epoch 100, training loss: 1.6018937826156616 = 1.5288927555084229 + 0.01 * 7.300105571746826
Epoch 100, val loss: 1.5569944381713867
Epoch 110, training loss: 1.499626874923706 = 1.427579641342163 + 0.01 * 7.2047247886657715
Epoch 110, val loss: 1.4753633737564087
Epoch 120, training loss: 1.401305913925171 = 1.330044150352478 + 0.01 * 7.126178741455078
Epoch 120, val loss: 1.4005273580551147
Epoch 130, training loss: 1.3124511241912842 = 1.2418582439422607 + 0.01 * 7.059286594390869
Epoch 130, val loss: 1.3366810083389282
Epoch 140, training loss: 1.2333543300628662 = 1.1631089448928833 + 0.01 * 7.024540424346924
Epoch 140, val loss: 1.2820812463760376
Epoch 150, training loss: 1.160758137702942 = 1.0906809568405151 + 0.01 * 7.007719993591309
Epoch 150, val loss: 1.2334792613983154
Epoch 160, training loss: 1.0914642810821533 = 1.021562933921814 + 0.01 * 6.990137577056885
Epoch 160, val loss: 1.1872519254684448
Epoch 170, training loss: 1.0243589878082275 = 0.9545921683311462 + 0.01 * 6.976678371429443
Epoch 170, val loss: 1.1423717737197876
Epoch 180, training loss: 0.9591494798660278 = 0.8895421028137207 + 0.01 * 6.960736274719238
Epoch 180, val loss: 1.0990656614303589
Epoch 190, training loss: 0.8951752781867981 = 0.8257637023925781 + 0.01 * 6.941159725189209
Epoch 190, val loss: 1.0570234060287476
Epoch 200, training loss: 0.8313705325126648 = 0.7621979713439941 + 0.01 * 6.917254447937012
Epoch 200, val loss: 1.0154860019683838
Epoch 210, training loss: 0.7677170038223267 = 0.6987953186035156 + 0.01 * 6.892168045043945
Epoch 210, val loss: 0.9745064973831177
Epoch 220, training loss: 0.7054552435874939 = 0.6367149949073792 + 0.01 * 6.874022483825684
Epoch 220, val loss: 0.9351614117622375
Epoch 230, training loss: 0.6461437344551086 = 0.5775099396705627 + 0.01 * 6.863378047943115
Epoch 230, val loss: 0.8989200592041016
Epoch 240, training loss: 0.5908259153366089 = 0.5222715139389038 + 0.01 * 6.855443477630615
Epoch 240, val loss: 0.8673042058944702
Epoch 250, training loss: 0.5396223664283752 = 0.4711337685585022 + 0.01 * 6.848857402801514
Epoch 250, val loss: 0.8403478264808655
Epoch 260, training loss: 0.4923859238624573 = 0.42395246028900146 + 0.01 * 6.843347072601318
Epoch 260, val loss: 0.818268895149231
Epoch 270, training loss: 0.448691725730896 = 0.3803130090236664 + 0.01 * 6.837871551513672
Epoch 270, val loss: 0.800677478313446
Epoch 280, training loss: 0.40833231806755066 = 0.3399946093559265 + 0.01 * 6.833771705627441
Epoch 280, val loss: 0.7868091464042664
Epoch 290, training loss: 0.37136995792388916 = 0.3030873239040375 + 0.01 * 6.8282647132873535
Epoch 290, val loss: 0.7765154838562012
Epoch 300, training loss: 0.3379754424095154 = 0.2697608172893524 + 0.01 * 6.821461200714111
Epoch 300, val loss: 0.7697665691375732
Epoch 310, training loss: 0.3079572916030884 = 0.23979397118091583 + 0.01 * 6.816333293914795
Epoch 310, val loss: 0.7656334638595581
Epoch 320, training loss: 0.28101441264152527 = 0.21293975412845612 + 0.01 * 6.807465076446533
Epoch 320, val loss: 0.7651500105857849
Epoch 330, training loss: 0.25682520866394043 = 0.18881964683532715 + 0.01 * 6.800557613372803
Epoch 330, val loss: 0.7670914530754089
Epoch 340, training loss: 0.23510031402111053 = 0.16714312136173248 + 0.01 * 6.795719146728516
Epoch 340, val loss: 0.7713719010353088
Epoch 350, training loss: 0.21547846496105194 = 0.14759448170661926 + 0.01 * 6.788398742675781
Epoch 350, val loss: 0.7773348689079285
Epoch 360, training loss: 0.1979433298110962 = 0.13000816106796265 + 0.01 * 6.793517589569092
Epoch 360, val loss: 0.7849488854408264
Epoch 370, training loss: 0.1822519600391388 = 0.11446189880371094 + 0.01 * 6.779006004333496
Epoch 370, val loss: 0.7936488389968872
Epoch 380, training loss: 0.16854599118232727 = 0.10081644356250763 + 0.01 * 6.772953987121582
Epoch 380, val loss: 0.8033152222633362
Epoch 390, training loss: 0.1566043496131897 = 0.0889427661895752 + 0.01 * 6.766159534454346
Epoch 390, val loss: 0.8134236931800842
Epoch 400, training loss: 0.14625833928585052 = 0.07861266285181046 + 0.01 * 6.764567852020264
Epoch 400, val loss: 0.8244917988777161
Epoch 410, training loss: 0.13711491227149963 = 0.069523885846138 + 0.01 * 6.759102821350098
Epoch 410, val loss: 0.8362237811088562
Epoch 420, training loss: 0.12904560565948486 = 0.061512455344200134 + 0.01 * 6.7533159255981445
Epoch 420, val loss: 0.8478819131851196
Epoch 430, training loss: 0.12186306715011597 = 0.05436895415186882 + 0.01 * 6.749411106109619
Epoch 430, val loss: 0.8597956299781799
Epoch 440, training loss: 0.11552765965461731 = 0.048053521662950516 + 0.01 * 6.747413635253906
Epoch 440, val loss: 0.871829092502594
Epoch 450, training loss: 0.10998187959194183 = 0.04256823658943176 + 0.01 * 6.741364479064941
Epoch 450, val loss: 0.883821427822113
Epoch 460, training loss: 0.10534324496984482 = 0.03789854794740677 + 0.01 * 6.74446964263916
Epoch 460, val loss: 0.8957026600837708
Epoch 470, training loss: 0.10129962861537933 = 0.033941835165023804 + 0.01 * 6.735779285430908
Epoch 470, val loss: 0.9078347086906433
Epoch 480, training loss: 0.0978890061378479 = 0.03056183084845543 + 0.01 * 6.732717990875244
Epoch 480, val loss: 0.9201955795288086
Epoch 490, training loss: 0.09499859064817429 = 0.0276601891964674 + 0.01 * 6.733840465545654
Epoch 490, val loss: 0.9324673414230347
Epoch 500, training loss: 0.0924280509352684 = 0.025152644142508507 + 0.01 * 6.727540969848633
Epoch 500, val loss: 0.9444544911384583
Epoch 510, training loss: 0.09026658535003662 = 0.022978737950325012 + 0.01 * 6.728785037994385
Epoch 510, val loss: 0.9563812017440796
Epoch 520, training loss: 0.08830087631940842 = 0.021074555814266205 + 0.01 * 6.72263240814209
Epoch 520, val loss: 0.9678723216056824
Epoch 530, training loss: 0.0866311639547348 = 0.01939830556511879 + 0.01 * 6.7232866287231445
Epoch 530, val loss: 0.9789329171180725
Epoch 540, training loss: 0.08506904542446136 = 0.017920609563589096 + 0.01 * 6.714844226837158
Epoch 540, val loss: 0.989837646484375
Epoch 550, training loss: 0.0838170051574707 = 0.01661212183535099 + 0.01 * 6.720488548278809
Epoch 550, val loss: 1.0004360675811768
Epoch 560, training loss: 0.08259041607379913 = 0.015450919046998024 + 0.01 * 6.713950157165527
Epoch 560, val loss: 1.010609745979309
Epoch 570, training loss: 0.08153344690799713 = 0.014410953968763351 + 0.01 * 6.712248802185059
Epoch 570, val loss: 1.0205656290054321
Epoch 580, training loss: 0.08054085820913315 = 0.013476118445396423 + 0.01 * 6.706474304199219
Epoch 580, val loss: 1.0302218198776245
Epoch 590, training loss: 0.07978135347366333 = 0.012634743936359882 + 0.01 * 6.714661121368408
Epoch 590, val loss: 1.0396565198898315
Epoch 600, training loss: 0.07890476286411285 = 0.011874066665768623 + 0.01 * 6.703069686889648
Epoch 600, val loss: 1.0487271547317505
Epoch 610, training loss: 0.07814838737249374 = 0.011184332892298698 + 0.01 * 6.696405410766602
Epoch 610, val loss: 1.0576331615447998
Epoch 620, training loss: 0.07744217664003372 = 0.010556206107139587 + 0.01 * 6.688597202301025
Epoch 620, val loss: 1.066243052482605
Epoch 630, training loss: 0.07689138501882553 = 0.009982177056372166 + 0.01 * 6.690921306610107
Epoch 630, val loss: 1.074676513671875
Epoch 640, training loss: 0.07646862417459488 = 0.00945567898452282 + 0.01 * 6.701294422149658
Epoch 640, val loss: 1.0829460620880127
Epoch 650, training loss: 0.07577375322580338 = 0.008973388001322746 + 0.01 * 6.680036544799805
Epoch 650, val loss: 1.0909026861190796
Epoch 660, training loss: 0.07527808845043182 = 0.008529644459486008 + 0.01 * 6.674844264984131
Epoch 660, val loss: 1.0985543727874756
Epoch 670, training loss: 0.07483765482902527 = 0.008120049722492695 + 0.01 * 6.671760559082031
Epoch 670, val loss: 1.1061649322509766
Epoch 680, training loss: 0.07467765361070633 = 0.007742616347968578 + 0.01 * 6.6935038566589355
Epoch 680, val loss: 1.1134114265441895
Epoch 690, training loss: 0.07405868917703629 = 0.007393608335405588 + 0.01 * 6.666508674621582
Epoch 690, val loss: 1.1207053661346436
Epoch 700, training loss: 0.07374485582113266 = 0.007067876867949963 + 0.01 * 6.667698383331299
Epoch 700, val loss: 1.1275767087936401
Epoch 710, training loss: 0.07326860725879669 = 0.006765160243958235 + 0.01 * 6.650345325469971
Epoch 710, val loss: 1.1343960762023926
Epoch 720, training loss: 0.07302087545394897 = 0.006483590230345726 + 0.01 * 6.653728485107422
Epoch 720, val loss: 1.1410692930221558
Epoch 730, training loss: 0.07280932366847992 = 0.006221879739314318 + 0.01 * 6.658743858337402
Epoch 730, val loss: 1.147456169128418
Epoch 740, training loss: 0.0723482221364975 = 0.0059779551811516285 + 0.01 * 6.637026786804199
Epoch 740, val loss: 1.153823971748352
Epoch 750, training loss: 0.07227783650159836 = 0.005749700125306845 + 0.01 * 6.652813911437988
Epoch 750, val loss: 1.1599440574645996
Epoch 760, training loss: 0.07198690623044968 = 0.005536071956157684 + 0.01 * 6.645083427429199
Epoch 760, val loss: 1.1660254001617432
Epoch 770, training loss: 0.07155125588178635 = 0.005335335619747639 + 0.01 * 6.621592044830322
Epoch 770, val loss: 1.1718205213546753
Epoch 780, training loss: 0.07141072303056717 = 0.005146825686097145 + 0.01 * 6.626389980316162
Epoch 780, val loss: 1.177650809288025
Epoch 790, training loss: 0.07114703953266144 = 0.004969023633748293 + 0.01 * 6.617801666259766
Epoch 790, val loss: 1.1832045316696167
Epoch 800, training loss: 0.07101737707853317 = 0.004801381845027208 + 0.01 * 6.621599197387695
Epoch 800, val loss: 1.1887365579605103
Epoch 810, training loss: 0.07077369838953018 = 0.004643966909497976 + 0.01 * 6.612972736358643
Epoch 810, val loss: 1.1939787864685059
Epoch 820, training loss: 0.07082834094762802 = 0.004495628178119659 + 0.01 * 6.633271217346191
Epoch 820, val loss: 1.1992043256759644
Epoch 830, training loss: 0.07030609995126724 = 0.004355580545961857 + 0.01 * 6.595052242279053
Epoch 830, val loss: 1.204392671585083
Epoch 840, training loss: 0.0700596421957016 = 0.00422307662665844 + 0.01 * 6.583656311035156
Epoch 840, val loss: 1.2092994451522827
Epoch 850, training loss: 0.07011311501264572 = 0.004097579047083855 + 0.01 * 6.601553916931152
Epoch 850, val loss: 1.2141575813293457
Epoch 860, training loss: 0.06986831873655319 = 0.003978734370321035 + 0.01 * 6.588958263397217
Epoch 860, val loss: 1.2189675569534302
Epoch 870, training loss: 0.06960351765155792 = 0.0038662366569042206 + 0.01 * 6.573728084564209
Epoch 870, val loss: 1.2235475778579712
Epoch 880, training loss: 0.06954295188188553 = 0.0037594190798699856 + 0.01 * 6.578352928161621
Epoch 880, val loss: 1.2281073331832886
Epoch 890, training loss: 0.06935957074165344 = 0.0036579135339707136 + 0.01 * 6.570165634155273
Epoch 890, val loss: 1.2324564456939697
Epoch 900, training loss: 0.06926833093166351 = 0.003561283927410841 + 0.01 * 6.570704936981201
Epoch 900, val loss: 1.2368817329406738
Epoch 910, training loss: 0.06913312524557114 = 0.00346918567083776 + 0.01 * 6.566394329071045
Epoch 910, val loss: 1.2410038709640503
Epoch 920, training loss: 0.06891711056232452 = 0.0033814723137766123 + 0.01 * 6.553563594818115
Epoch 920, val loss: 1.245161771774292
Epoch 930, training loss: 0.06872295588254929 = 0.00329775083810091 + 0.01 * 6.542520523071289
Epoch 930, val loss: 1.2492645978927612
Epoch 940, training loss: 0.06909533590078354 = 0.003217733930796385 + 0.01 * 6.5877604484558105
Epoch 940, val loss: 1.2531768083572388
Epoch 950, training loss: 0.06860204041004181 = 0.0031412718817591667 + 0.01 * 6.546077251434326
Epoch 950, val loss: 1.2571051120758057
Epoch 960, training loss: 0.0685485452413559 = 0.003068139310926199 + 0.01 * 6.548040390014648
Epoch 960, val loss: 1.2609411478042603
Epoch 970, training loss: 0.06837990880012512 = 0.0029977536760270596 + 0.01 * 6.538215160369873
Epoch 970, val loss: 1.2647367715835571
Epoch 980, training loss: 0.06831268221139908 = 0.002930267946794629 + 0.01 * 6.538241386413574
Epoch 980, val loss: 1.2684494256973267
Epoch 990, training loss: 0.06806549429893494 = 0.002865532645955682 + 0.01 * 6.519996166229248
Epoch 990, val loss: 1.2720372676849365
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.5941
Flip ASR: 0.5422/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0589160919189453 = 1.9751781225204468 + 0.01 * 8.37380599975586
Epoch 0, val loss: 1.9842140674591064
Epoch 10, training loss: 2.047658681869507 = 1.963921308517456 + 0.01 * 8.373733520507812
Epoch 10, val loss: 1.9725723266601562
Epoch 20, training loss: 2.0343234539031982 = 1.9505888223648071 + 0.01 * 8.373455047607422
Epoch 20, val loss: 1.9585293531417847
Epoch 30, training loss: 2.016118288040161 = 1.9323909282684326 + 0.01 * 8.372726440429688
Epoch 30, val loss: 1.9391825199127197
Epoch 40, training loss: 1.9892200231552124 = 1.905524492263794 + 0.01 * 8.369555473327637
Epoch 40, val loss: 1.9107275009155273
Epoch 50, training loss: 1.9499750137329102 = 1.8665169477462769 + 0.01 * 8.345803260803223
Epoch 50, val loss: 1.8705823421478271
Epoch 60, training loss: 1.9009212255477905 = 1.8187434673309326 + 0.01 * 8.217774391174316
Epoch 60, val loss: 1.8250495195388794
Epoch 70, training loss: 1.8525422811508179 = 1.7746760845184326 + 0.01 * 7.786617755889893
Epoch 70, val loss: 1.787999153137207
Epoch 80, training loss: 1.8007372617721558 = 1.7291226387023926 + 0.01 * 7.161466598510742
Epoch 80, val loss: 1.7500076293945312
Epoch 90, training loss: 1.7400143146514893 = 1.669918179512024 + 0.01 * 7.009613037109375
Epoch 90, val loss: 1.6999588012695312
Epoch 100, training loss: 1.660771369934082 = 1.59116792678833 + 0.01 * 6.960346221923828
Epoch 100, val loss: 1.6340447664260864
Epoch 110, training loss: 1.5648844242095947 = 1.4954850673675537 + 0.01 * 6.939940452575684
Epoch 110, val loss: 1.556239366531372
Epoch 120, training loss: 1.462165355682373 = 1.3929084539413452 + 0.01 * 6.925695896148682
Epoch 120, val loss: 1.4754586219787598
Epoch 130, training loss: 1.360129952430725 = 1.2909425497055054 + 0.01 * 6.918737411499023
Epoch 130, val loss: 1.3982328176498413
Epoch 140, training loss: 1.2600164413452148 = 1.1909098625183105 + 0.01 * 6.910653591156006
Epoch 140, val loss: 1.3248859643936157
Epoch 150, training loss: 1.1611777544021606 = 1.0921484231948853 + 0.01 * 6.902928352355957
Epoch 150, val loss: 1.251231074333191
Epoch 160, training loss: 1.0643343925476074 = 0.9953835606575012 + 0.01 * 6.895084857940674
Epoch 160, val loss: 1.1788004636764526
Epoch 170, training loss: 0.9714070558547974 = 0.902544379234314 + 0.01 * 6.886269569396973
Epoch 170, val loss: 1.108974575996399
Epoch 180, training loss: 0.8849021196365356 = 0.8161419034004211 + 0.01 * 6.8760247230529785
Epoch 180, val loss: 1.044142246246338
Epoch 190, training loss: 0.8068475127220154 = 0.7381876707077026 + 0.01 * 6.8659844398498535
Epoch 190, val loss: 0.9860395789146423
Epoch 200, training loss: 0.7373769283294678 = 0.6688050031661987 + 0.01 * 6.8571953773498535
Epoch 200, val loss: 0.9355352520942688
Epoch 210, training loss: 0.6745768189430237 = 0.6060752868652344 + 0.01 * 6.850155353546143
Epoch 210, val loss: 0.8915819525718689
Epoch 220, training loss: 0.61580890417099 = 0.5474001169204712 + 0.01 * 6.840876579284668
Epoch 220, val loss: 0.8528165221214294
Epoch 230, training loss: 0.5595452785491943 = 0.4912378191947937 + 0.01 * 6.830746650695801
Epoch 230, val loss: 0.8183506727218628
Epoch 240, training loss: 0.5056827664375305 = 0.43746790289878845 + 0.01 * 6.821487903594971
Epoch 240, val loss: 0.788409948348999
Epoch 250, training loss: 0.4551040530204773 = 0.3870013356208801 + 0.01 * 6.810272216796875
Epoch 250, val loss: 0.7640854716300964
Epoch 260, training loss: 0.4086945652961731 = 0.34070447087287903 + 0.01 * 6.799009323120117
Epoch 260, val loss: 0.7459006309509277
Epoch 270, training loss: 0.36689770221710205 = 0.299041211605072 + 0.01 * 6.785649299621582
Epoch 270, val loss: 0.7336211204528809
Epoch 280, training loss: 0.3297547698020935 = 0.26203665137290955 + 0.01 * 6.771811008453369
Epoch 280, val loss: 0.726002037525177
Epoch 290, training loss: 0.2970049977302551 = 0.22940954566001892 + 0.01 * 6.75954532623291
Epoch 290, val loss: 0.7222795486450195
Epoch 300, training loss: 0.2683255672454834 = 0.20072917640209198 + 0.01 * 6.75963830947876
Epoch 300, val loss: 0.7219410538673401
Epoch 310, training loss: 0.24306795001029968 = 0.17558172345161438 + 0.01 * 6.748622417449951
Epoch 310, val loss: 0.7245656847953796
Epoch 320, training loss: 0.22084254026412964 = 0.15355686843395233 + 0.01 * 6.728567600250244
Epoch 320, val loss: 0.7296834588050842
Epoch 330, training loss: 0.20158076286315918 = 0.13432788848876953 + 0.01 * 6.725286960601807
Epoch 330, val loss: 0.7368825674057007
Epoch 340, training loss: 0.18481841683387756 = 0.11764770746231079 + 0.01 * 6.717070579528809
Epoch 340, val loss: 0.7457226514816284
Epoch 350, training loss: 0.17037467658519745 = 0.10325884819030762 + 0.01 * 6.711583137512207
Epoch 350, val loss: 0.7557873725891113
Epoch 360, training loss: 0.15799173712730408 = 0.09090525656938553 + 0.01 * 6.70864725112915
Epoch 360, val loss: 0.7668323516845703
Epoch 370, training loss: 0.14734141528606415 = 0.08033118396997452 + 0.01 * 6.701023101806641
Epoch 370, val loss: 0.778465986251831
Epoch 380, training loss: 0.13826382160186768 = 0.07128232717514038 + 0.01 * 6.698149681091309
Epoch 380, val loss: 0.7904260754585266
Epoch 390, training loss: 0.1304415464401245 = 0.06352566927671432 + 0.01 * 6.691587448120117
Epoch 390, val loss: 0.8025553226470947
Epoch 400, training loss: 0.12378919124603271 = 0.056858550757169724 + 0.01 * 6.6930646896362305
Epoch 400, val loss: 0.8146489858627319
Epoch 410, training loss: 0.11793754994869232 = 0.051094938069581985 + 0.01 * 6.684260845184326
Epoch 410, val loss: 0.8267238140106201
Epoch 420, training loss: 0.11291542649269104 = 0.04609179124236107 + 0.01 * 6.682363033294678
Epoch 420, val loss: 0.8386150002479553
Epoch 430, training loss: 0.10849850624799728 = 0.04172857105731964 + 0.01 * 6.6769938468933105
Epoch 430, val loss: 0.8504631519317627
Epoch 440, training loss: 0.1046275943517685 = 0.037905145436525345 + 0.01 * 6.672244548797607
Epoch 440, val loss: 0.8621160984039307
Epoch 450, training loss: 0.10131799429655075 = 0.034542351961135864 + 0.01 * 6.6775641441345215
Epoch 450, val loss: 0.8735734820365906
Epoch 460, training loss: 0.0982757955789566 = 0.03157610073685646 + 0.01 * 6.66996955871582
Epoch 460, val loss: 0.8849910497665405
Epoch 470, training loss: 0.09557728469371796 = 0.028951440006494522 + 0.01 * 6.66258430480957
Epoch 470, val loss: 0.896003007888794
Epoch 480, training loss: 0.0932035818696022 = 0.026621408760547638 + 0.01 * 6.658217430114746
Epoch 480, val loss: 0.9069012403488159
Epoch 490, training loss: 0.0911199077963829 = 0.02454659342765808 + 0.01 * 6.657331466674805
Epoch 490, val loss: 0.9175419211387634
Epoch 500, training loss: 0.08924426883459091 = 0.02269372157752514 + 0.01 * 6.655054569244385
Epoch 500, val loss: 0.9279447793960571
Epoch 510, training loss: 0.08751337230205536 = 0.02103424444794655 + 0.01 * 6.647912979125977
Epoch 510, val loss: 0.9381241202354431
Epoch 520, training loss: 0.08598992973566055 = 0.019543970003724098 + 0.01 * 6.644596099853516
Epoch 520, val loss: 0.9480434656143188
Epoch 530, training loss: 0.0846402496099472 = 0.018202710896730423 + 0.01 * 6.643753528594971
Epoch 530, val loss: 0.957675039768219
Epoch 540, training loss: 0.08335238695144653 = 0.016993053257465363 + 0.01 * 6.6359333992004395
Epoch 540, val loss: 0.9670525193214417
Epoch 550, training loss: 0.08229934424161911 = 0.0158985685557127 + 0.01 * 6.640077590942383
Epoch 550, val loss: 0.9761661291122437
Epoch 560, training loss: 0.08122540265321732 = 0.014906210824847221 + 0.01 * 6.631918907165527
Epoch 560, val loss: 0.9850916266441345
Epoch 570, training loss: 0.08029596507549286 = 0.014003903605043888 + 0.01 * 6.62920618057251
Epoch 570, val loss: 0.9937112331390381
Epoch 580, training loss: 0.07942364364862442 = 0.013181610032916069 + 0.01 * 6.624203205108643
Epoch 580, val loss: 1.0020695924758911
Epoch 590, training loss: 0.07863184064626694 = 0.012430088594555855 + 0.01 * 6.620175361633301
Epoch 590, val loss: 1.010256290435791
Epoch 600, training loss: 0.0780467763543129 = 0.011742719449102879 + 0.01 * 6.630405902862549
Epoch 600, val loss: 1.0180881023406982
Epoch 610, training loss: 0.07726733386516571 = 0.011112995445728302 + 0.01 * 6.615434169769287
Epoch 610, val loss: 1.0258349180221558
Epoch 620, training loss: 0.07677959650754929 = 0.010534033179283142 + 0.01 * 6.624556541442871
Epoch 620, val loss: 1.0333259105682373
Epoch 630, training loss: 0.07619602978229523 = 0.010001515038311481 + 0.01 * 6.619451522827148
Epoch 630, val loss: 1.0406137704849243
Epoch 640, training loss: 0.0755620077252388 = 0.009510457515716553 + 0.01 * 6.605154991149902
Epoch 640, val loss: 1.0476794242858887
Epoch 650, training loss: 0.07506952434778214 = 0.009056192822754383 + 0.01 * 6.601332664489746
Epoch 650, val loss: 1.0545473098754883
Epoch 660, training loss: 0.07466597855091095 = 0.008635114878416061 + 0.01 * 6.603086471557617
Epoch 660, val loss: 1.061247706413269
Epoch 670, training loss: 0.07427679002285004 = 0.008244347758591175 + 0.01 * 6.603243827819824
Epoch 670, val loss: 1.0677772760391235
Epoch 680, training loss: 0.0737745463848114 = 0.007881388999521732 + 0.01 * 6.589315891265869
Epoch 680, val loss: 1.074144721031189
Epoch 690, training loss: 0.07350759208202362 = 0.007543307263404131 + 0.01 * 6.596428394317627
Epoch 690, val loss: 1.08035409450531
Epoch 700, training loss: 0.07322561740875244 = 0.0072283437475562096 + 0.01 * 6.599727153778076
Epoch 700, val loss: 1.0863507986068726
Epoch 710, training loss: 0.07292821258306503 = 0.006934551056474447 + 0.01 * 6.599365711212158
Epoch 710, val loss: 1.09223210811615
Epoch 720, training loss: 0.07246320694684982 = 0.006660095881670713 + 0.01 * 6.580311298370361
Epoch 720, val loss: 1.0978600978851318
Epoch 730, training loss: 0.07217628508806229 = 0.006403124425560236 + 0.01 * 6.5773162841796875
Epoch 730, val loss: 1.1034183502197266
Epoch 740, training loss: 0.07196313887834549 = 0.006161965895444155 + 0.01 * 6.580117225646973
Epoch 740, val loss: 1.1088262796401978
Epoch 750, training loss: 0.07179167866706848 = 0.0059355478733778 + 0.01 * 6.585612773895264
Epoch 750, val loss: 1.1139897108078003
Epoch 760, training loss: 0.07145822793245316 = 0.005722921807318926 + 0.01 * 6.573530673980713
Epoch 760, val loss: 1.1191500425338745
Epoch 770, training loss: 0.0712655782699585 = 0.005522862542420626 + 0.01 * 6.5742716789245605
Epoch 770, val loss: 1.1240581274032593
Epoch 780, training loss: 0.07099031656980515 = 0.0053343200124800205 + 0.01 * 6.5655999183654785
Epoch 780, val loss: 1.1289411783218384
Epoch 790, training loss: 0.07105407118797302 = 0.005156481638550758 + 0.01 * 6.589759349822998
Epoch 790, val loss: 1.1336089372634888
Epoch 800, training loss: 0.0705774649977684 = 0.0049888058565557 + 0.01 * 6.558866500854492
Epoch 800, val loss: 1.1382824182510376
Epoch 810, training loss: 0.07032115012407303 = 0.004830192308872938 + 0.01 * 6.549096584320068
Epoch 810, val loss: 1.1427613496780396
Epoch 820, training loss: 0.0702674612402916 = 0.00468007056042552 + 0.01 * 6.55873966217041
Epoch 820, val loss: 1.1471707820892334
Epoch 830, training loss: 0.0699639767408371 = 0.004537881352007389 + 0.01 * 6.542609691619873
Epoch 830, val loss: 1.1514440774917603
Epoch 840, training loss: 0.0698632225394249 = 0.0044030784629285336 + 0.01 * 6.546014785766602
Epoch 840, val loss: 1.1555492877960205
Epoch 850, training loss: 0.06964261829853058 = 0.004275163169950247 + 0.01 * 6.536745071411133
Epoch 850, val loss: 1.1596312522888184
Epoch 860, training loss: 0.06976590305566788 = 0.004153579473495483 + 0.01 * 6.561232566833496
Epoch 860, val loss: 1.1634966135025024
Epoch 870, training loss: 0.06937013566493988 = 0.004038169980049133 + 0.01 * 6.533196926116943
Epoch 870, val loss: 1.1674002408981323
Epoch 880, training loss: 0.06927604228258133 = 0.003928286023437977 + 0.01 * 6.534775733947754
Epoch 880, val loss: 1.1710845232009888
Epoch 890, training loss: 0.06915874779224396 = 0.0038236973341554403 + 0.01 * 6.533504962921143
Epoch 890, val loss: 1.174788475036621
Epoch 900, training loss: 0.06905730068683624 = 0.0037239345256239176 + 0.01 * 6.533337116241455
Epoch 900, val loss: 1.178360939025879
Epoch 910, training loss: 0.06886479258537292 = 0.003628901205956936 + 0.01 * 6.523589134216309
Epoch 910, val loss: 1.1818408966064453
Epoch 920, training loss: 0.06868015974760056 = 0.003538191318511963 + 0.01 * 6.51419734954834
Epoch 920, val loss: 1.1852940320968628
Epoch 930, training loss: 0.0688314139842987 = 0.0034515734296292067 + 0.01 * 6.537984371185303
Epoch 930, val loss: 1.1886392831802368
Epoch 940, training loss: 0.0686689168214798 = 0.0033688517287373543 + 0.01 * 6.530006408691406
Epoch 940, val loss: 1.1919370889663696
Epoch 950, training loss: 0.06867704540491104 = 0.0032897621858865023 + 0.01 * 6.538728713989258
Epoch 950, val loss: 1.1951675415039062
Epoch 960, training loss: 0.06835231184959412 = 0.0032141150441020727 + 0.01 * 6.513820171356201
Epoch 960, val loss: 1.1982619762420654
Epoch 970, training loss: 0.06825955957174301 = 0.00314160855486989 + 0.01 * 6.511794567108154
Epoch 970, val loss: 1.2013401985168457
Epoch 980, training loss: 0.06816396117210388 = 0.003072198946028948 + 0.01 * 6.509175777435303
Epoch 980, val loss: 1.204369306564331
Epoch 990, training loss: 0.06788098067045212 = 0.0030055628158152103 + 0.01 * 6.487541675567627
Epoch 990, val loss: 1.2072248458862305
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.7638
Flip ASR: 0.7156/225 nodes
The final ASR:0.69004, 0.07104, Accuracy:0.81975, 0.00698
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11616])
remove edge: torch.Size([2, 9472])
updated graph: torch.Size([2, 10532])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98032, 0.00627, Accuracy:0.83333, 0.00524
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.5 der2:0 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.041245222091675 = 1.9575072526931763 + 0.01 * 8.373793601989746
Epoch 0, val loss: 1.9629158973693848
Epoch 10, training loss: 2.031182289123535 = 1.9474451541900635 + 0.01 * 8.373708724975586
Epoch 10, val loss: 1.952957272529602
Epoch 20, training loss: 2.0190892219543457 = 1.935355544090271 + 0.01 * 8.373357772827148
Epoch 20, val loss: 1.9408930540084839
Epoch 30, training loss: 2.002415418624878 = 1.9186912775039673 + 0.01 * 8.372403144836426
Epoch 30, val loss: 1.924281358718872
Epoch 40, training loss: 1.9776238203048706 = 1.8939433097839355 + 0.01 * 8.36805248260498
Epoch 40, val loss: 1.8998053073883057
Epoch 50, training loss: 1.9412716627120972 = 1.8579012155532837 + 0.01 * 8.337039947509766
Epoch 50, val loss: 1.865484356880188
Epoch 60, training loss: 1.8944169282913208 = 1.8130393028259277 + 0.01 * 8.137758255004883
Epoch 60, val loss: 1.8260620832443237
Epoch 70, training loss: 1.8489665985107422 = 1.7713890075683594 + 0.01 * 7.757756233215332
Epoch 70, val loss: 1.7926057577133179
Epoch 80, training loss: 1.8001912832260132 = 1.7272671461105347 + 0.01 * 7.292416095733643
Epoch 80, val loss: 1.753590703010559
Epoch 90, training loss: 1.7370126247406006 = 1.6664222478866577 + 0.01 * 7.0590434074401855
Epoch 90, val loss: 1.7004936933517456
Epoch 100, training loss: 1.6544376611709595 = 1.5846046209335327 + 0.01 * 6.983307838439941
Epoch 100, val loss: 1.6321662664413452
Epoch 110, training loss: 1.552488088607788 = 1.4833773374557495 + 0.01 * 6.911081314086914
Epoch 110, val loss: 1.5490831136703491
Epoch 120, training loss: 1.441740870475769 = 1.3731985092163086 + 0.01 * 6.8542327880859375
Epoch 120, val loss: 1.459320306777954
Epoch 130, training loss: 1.3304635286331177 = 1.2621705532073975 + 0.01 * 6.829298496246338
Epoch 130, val loss: 1.3687517642974854
Epoch 140, training loss: 1.220662236213684 = 1.1525273323059082 + 0.01 * 6.813493251800537
Epoch 140, val loss: 1.2806733846664429
Epoch 150, training loss: 1.1142511367797852 = 1.0462367534637451 + 0.01 * 6.801436901092529
Epoch 150, val loss: 1.1963022947311401
Epoch 160, training loss: 1.0139942169189453 = 0.9460908770561218 + 0.01 * 6.790336608886719
Epoch 160, val loss: 1.117903470993042
Epoch 170, training loss: 0.9217731356620789 = 0.8539983034133911 + 0.01 * 6.7774810791015625
Epoch 170, val loss: 1.0467172861099243
Epoch 180, training loss: 0.8384262919425964 = 0.7707731127738953 + 0.01 * 6.765317440032959
Epoch 180, val loss: 0.9834000468254089
Epoch 190, training loss: 0.7639003992080688 = 0.6963878870010376 + 0.01 * 6.751248359680176
Epoch 190, val loss: 0.9286499619483948
Epoch 200, training loss: 0.6973705887794495 = 0.6299673914909363 + 0.01 * 6.740319728851318
Epoch 200, val loss: 0.8824851512908936
Epoch 210, training loss: 0.6373582482337952 = 0.5700544714927673 + 0.01 * 6.730377197265625
Epoch 210, val loss: 0.8443302512168884
Epoch 220, training loss: 0.5826150178909302 = 0.5153806209564209 + 0.01 * 6.723443031311035
Epoch 220, val loss: 0.8131185173988342
Epoch 230, training loss: 0.5323342084884644 = 0.46515393257141113 + 0.01 * 6.7180280685424805
Epoch 230, val loss: 0.7877198457717896
Epoch 240, training loss: 0.4859727621078491 = 0.4188493490219116 + 0.01 * 6.712342262268066
Epoch 240, val loss: 0.7672476768493652
Epoch 250, training loss: 0.44330647587776184 = 0.3762133717536926 + 0.01 * 6.709311485290527
Epoch 250, val loss: 0.7511033415794373
Epoch 260, training loss: 0.40430450439453125 = 0.33723244071006775 + 0.01 * 6.707207679748535
Epoch 260, val loss: 0.7389894723892212
Epoch 270, training loss: 0.3690037727355957 = 0.30194833874702454 + 0.01 * 6.7055439949035645
Epoch 270, val loss: 0.7307736873626709
Epoch 280, training loss: 0.33718928694725037 = 0.2701510190963745 + 0.01 * 6.703826427459717
Epoch 280, val loss: 0.7260494828224182
Epoch 290, training loss: 0.30837956070899963 = 0.2413558065891266 + 0.01 * 6.702375888824463
Epoch 290, val loss: 0.724288284778595
Epoch 300, training loss: 0.2820547819137573 = 0.21505236625671387 + 0.01 * 6.70024299621582
Epoch 300, val loss: 0.7251745462417603
Epoch 310, training loss: 0.257913738489151 = 0.1909327507019043 + 0.01 * 6.698099613189697
Epoch 310, val loss: 0.7281971573829651
Epoch 320, training loss: 0.23590707778930664 = 0.16892504692077637 + 0.01 * 6.698202610015869
Epoch 320, val loss: 0.7331522703170776
Epoch 330, training loss: 0.21602916717529297 = 0.1490921527147293 + 0.01 * 6.693702697753906
Epoch 330, val loss: 0.7400245666503906
Epoch 340, training loss: 0.19848915934562683 = 0.13148482143878937 + 0.01 * 6.700433731079102
Epoch 340, val loss: 0.7486701011657715
Epoch 350, training loss: 0.1829511821269989 = 0.11605410277843475 + 0.01 * 6.689708709716797
Epoch 350, val loss: 0.7591097354888916
Epoch 360, training loss: 0.16949433088302612 = 0.10261502116918564 + 0.01 * 6.687931537628174
Epoch 360, val loss: 0.7711068987846375
Epoch 370, training loss: 0.15777482092380524 = 0.09092584252357483 + 0.01 * 6.6848978996276855
Epoch 370, val loss: 0.7844266295433044
Epoch 380, training loss: 0.14756757020950317 = 0.08074718713760376 + 0.01 * 6.682037353515625
Epoch 380, val loss: 0.7987364530563354
Epoch 390, training loss: 0.13866791129112244 = 0.07187732309103012 + 0.01 * 6.679059028625488
Epoch 390, val loss: 0.8137555718421936
Epoch 400, training loss: 0.13105876743793488 = 0.06415002793073654 + 0.01 * 6.690874099731445
Epoch 400, val loss: 0.8291483521461487
Epoch 410, training loss: 0.12419141829013824 = 0.05742237716913223 + 0.01 * 6.676904201507568
Epoch 410, val loss: 0.8447044491767883
Epoch 420, training loss: 0.11826218664646149 = 0.0515633188188076 + 0.01 * 6.669886589050293
Epoch 420, val loss: 0.8601484298706055
Epoch 430, training loss: 0.11310910433530807 = 0.04645805060863495 + 0.01 * 6.66510534286499
Epoch 430, val loss: 0.8754046559333801
Epoch 440, training loss: 0.10860776156187057 = 0.04200388491153717 + 0.01 * 6.6603875160217285
Epoch 440, val loss: 0.8903464078903198
Epoch 450, training loss: 0.10471421480178833 = 0.03811157867312431 + 0.01 * 6.660263538360596
Epoch 450, val loss: 0.9048160314559937
Epoch 460, training loss: 0.10127957910299301 = 0.03470608592033386 + 0.01 * 6.657349586486816
Epoch 460, val loss: 0.9188196063041687
Epoch 470, training loss: 0.09822224080562592 = 0.03171331062912941 + 0.01 * 6.650893688201904
Epoch 470, val loss: 0.9323350191116333
Epoch 480, training loss: 0.09551042318344116 = 0.029074328020215034 + 0.01 * 6.643610000610352
Epoch 480, val loss: 0.9454569220542908
Epoch 490, training loss: 0.09313744306564331 = 0.026738863438367844 + 0.01 * 6.639858722686768
Epoch 490, val loss: 0.9581646919250488
Epoch 500, training loss: 0.09104839712381363 = 0.024667128920555115 + 0.01 * 6.638127326965332
Epoch 500, val loss: 0.9704259634017944
Epoch 510, training loss: 0.08916300535202026 = 0.02282295934855938 + 0.01 * 6.634004592895508
Epoch 510, val loss: 0.9823405146598816
Epoch 520, training loss: 0.08748830854892731 = 0.02117639221251011 + 0.01 * 6.631191730499268
Epoch 520, val loss: 0.9938440322875977
Epoch 530, training loss: 0.08601287007331848 = 0.019700340926647186 + 0.01 * 6.631253242492676
Epoch 530, val loss: 1.0050368309020996
Epoch 540, training loss: 0.08463995158672333 = 0.018374964594841003 + 0.01 * 6.626499176025391
Epoch 540, val loss: 1.0157983303070068
Epoch 550, training loss: 0.08338069915771484 = 0.01717943698167801 + 0.01 * 6.620126247406006
Epoch 550, val loss: 1.026318073272705
Epoch 560, training loss: 0.08230315148830414 = 0.01609860733151436 + 0.01 * 6.620453834533691
Epoch 560, val loss: 1.0364947319030762
Epoch 570, training loss: 0.08120648562908173 = 0.015117844566702843 + 0.01 * 6.608863830566406
Epoch 570, val loss: 1.0464072227478027
Epoch 580, training loss: 0.08038872480392456 = 0.014226083643734455 + 0.01 * 6.616264343261719
Epoch 580, val loss: 1.056016206741333
Epoch 590, training loss: 0.0794408842921257 = 0.013413759879767895 + 0.01 * 6.602712631225586
Epoch 590, val loss: 1.0653553009033203
Epoch 600, training loss: 0.07868082076311111 = 0.012671300210058689 + 0.01 * 6.6009521484375
Epoch 600, val loss: 1.0744476318359375
Epoch 610, training loss: 0.07794546335935593 = 0.011990988627076149 + 0.01 * 6.595448017120361
Epoch 610, val loss: 1.0832712650299072
Epoch 620, training loss: 0.07751618325710297 = 0.011366285383701324 + 0.01 * 6.614989757537842
Epoch 620, val loss: 1.091817855834961
Epoch 630, training loss: 0.07670986652374268 = 0.010791699402034283 + 0.01 * 6.591817378997803
Epoch 630, val loss: 1.100193738937378
Epoch 640, training loss: 0.07614269107580185 = 0.010261920280754566 + 0.01 * 6.588077068328857
Epoch 640, val loss: 1.1083471775054932
Epoch 650, training loss: 0.07564759254455566 = 0.009772219695150852 + 0.01 * 6.5875372886657715
Epoch 650, val loss: 1.1162502765655518
Epoch 660, training loss: 0.07520285248756409 = 0.009318683296442032 + 0.01 * 6.5884175300598145
Epoch 660, val loss: 1.1239819526672363
Epoch 670, training loss: 0.07474009692668915 = 0.008898217231035233 + 0.01 * 6.5841875076293945
Epoch 670, val loss: 1.1315033435821533
Epoch 680, training loss: 0.07431527972221375 = 0.008507560938596725 + 0.01 * 6.5807719230651855
Epoch 680, val loss: 1.1388455629348755
Epoch 690, training loss: 0.07399078458547592 = 0.008143934421241283 + 0.01 * 6.584685325622559
Epoch 690, val loss: 1.1460506916046143
Epoch 700, training loss: 0.07356343418359756 = 0.007805166766047478 + 0.01 * 6.575826644897461
Epoch 700, val loss: 1.1530084609985352
Epoch 710, training loss: 0.07320773601531982 = 0.007488883100450039 + 0.01 * 6.571885585784912
Epoch 710, val loss: 1.159839391708374
Epoch 720, training loss: 0.07287904620170593 = 0.007193074561655521 + 0.01 * 6.568597316741943
Epoch 720, val loss: 1.166532278060913
Epoch 730, training loss: 0.07271774113178253 = 0.0069160074926912785 + 0.01 * 6.580173492431641
Epoch 730, val loss: 1.1730163097381592
Epoch 740, training loss: 0.07234655320644379 = 0.00665617547929287 + 0.01 * 6.569037914276123
Epoch 740, val loss: 1.1793639659881592
Epoch 750, training loss: 0.0720076709985733 = 0.006412428803741932 + 0.01 * 6.559524059295654
Epoch 750, val loss: 1.1855714321136475
Epoch 760, training loss: 0.07186576724052429 = 0.00618378072977066 + 0.01 * 6.5681986808776855
Epoch 760, val loss: 1.1915991306304932
Epoch 770, training loss: 0.07154735922813416 = 0.005968476179987192 + 0.01 * 6.557888507843018
Epoch 770, val loss: 1.1975297927856445
Epoch 780, training loss: 0.071335569024086 = 0.005765459965914488 + 0.01 * 6.557011127471924
Epoch 780, val loss: 1.2032808065414429
Epoch 790, training loss: 0.07106979191303253 = 0.00557399308308959 + 0.01 * 6.549579620361328
Epoch 790, val loss: 1.2089612483978271
Epoch 800, training loss: 0.0710059180855751 = 0.005393119528889656 + 0.01 * 6.561280250549316
Epoch 800, val loss: 1.2144393920898438
Epoch 810, training loss: 0.07067379355430603 = 0.005222440231591463 + 0.01 * 6.545135498046875
Epoch 810, val loss: 1.2198666334152222
Epoch 820, training loss: 0.07046505808830261 = 0.00506080687046051 + 0.01 * 6.5404253005981445
Epoch 820, val loss: 1.2251795530319214
Epoch 830, training loss: 0.07030684500932693 = 0.004907608963549137 + 0.01 * 6.539923667907715
Epoch 830, val loss: 1.2303707599639893
Epoch 840, training loss: 0.07019350677728653 = 0.004762642085552216 + 0.01 * 6.543086528778076
Epoch 840, val loss: 1.2354320287704468
Epoch 850, training loss: 0.07020033895969391 = 0.004625083412975073 + 0.01 * 6.557525634765625
Epoch 850, val loss: 1.2403793334960938
Epoch 860, training loss: 0.06982079893350601 = 0.004494387190788984 + 0.01 * 6.532641410827637
Epoch 860, val loss: 1.2453110218048096
Epoch 870, training loss: 0.06965696066617966 = 0.004370240028947592 + 0.01 * 6.528672218322754
Epoch 870, val loss: 1.2500641345977783
Epoch 880, training loss: 0.07012497633695602 = 0.004251984879374504 + 0.01 * 6.587299346923828
Epoch 880, val loss: 1.2547527551651
Epoch 890, training loss: 0.06948218494653702 = 0.004139663185924292 + 0.01 * 6.534252166748047
Epoch 890, val loss: 1.2593410015106201
Epoch 900, training loss: 0.06920145452022552 = 0.004032763186842203 + 0.01 * 6.51686954498291
Epoch 900, val loss: 1.2638098001480103
Epoch 910, training loss: 0.06911924481391907 = 0.003930867183953524 + 0.01 * 6.518837928771973
Epoch 910, val loss: 1.268254280090332
Epoch 920, training loss: 0.06896969676017761 = 0.0038335013668984175 + 0.01 * 6.513619899749756
Epoch 920, val loss: 1.2725498676300049
Epoch 930, training loss: 0.06916031241416931 = 0.0037407923955470324 + 0.01 * 6.541952133178711
Epoch 930, val loss: 1.2768672704696655
Epoch 940, training loss: 0.06871543079614639 = 0.0036520666908472776 + 0.01 * 6.506336688995361
Epoch 940, val loss: 1.2809447050094604
Epoch 950, training loss: 0.06869667023420334 = 0.003567183157429099 + 0.01 * 6.512948989868164
Epoch 950, val loss: 1.2849770784378052
Epoch 960, training loss: 0.06840155273675919 = 0.003486169036477804 + 0.01 * 6.491538047790527
Epoch 960, val loss: 1.2890266180038452
Epoch 970, training loss: 0.06828085333108902 = 0.0034084392245858908 + 0.01 * 6.487241744995117
Epoch 970, val loss: 1.2928413152694702
Epoch 980, training loss: 0.06834038347005844 = 0.0033341911621391773 + 0.01 * 6.500618934631348
Epoch 980, val loss: 1.2967432737350464
Epoch 990, training loss: 0.06822854280471802 = 0.0032629352062940598 + 0.01 * 6.496560573577881
Epoch 990, val loss: 1.3003047704696655
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.5277
Flip ASR: 0.4444/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.029707908630371 = 1.945969820022583 + 0.01 * 8.373815536499023
Epoch 0, val loss: 1.953755497932434
Epoch 10, training loss: 2.0204272270202637 = 1.9366902112960815 + 0.01 * 8.373708724975586
Epoch 10, val loss: 1.94422447681427
Epoch 20, training loss: 2.0089213848114014 = 1.9251874685287476 + 0.01 * 8.373400688171387
Epoch 20, val loss: 1.9324254989624023
Epoch 30, training loss: 1.99266517162323 = 1.9089395999908447 + 0.01 * 8.37255573272705
Epoch 30, val loss: 1.9159972667694092
Epoch 40, training loss: 1.968423843383789 = 1.8847390413284302 + 0.01 * 8.36848258972168
Epoch 40, val loss: 1.8919119834899902
Epoch 50, training loss: 1.9330940246582031 = 1.8497024774551392 + 0.01 * 8.33915901184082
Epoch 50, val loss: 1.8580598831176758
Epoch 60, training loss: 1.8870048522949219 = 1.8051849603652954 + 0.01 * 8.181987762451172
Epoch 60, val loss: 1.8168647289276123
Epoch 70, training loss: 1.8355157375335693 = 1.7576491832733154 + 0.01 * 7.786655902862549
Epoch 70, val loss: 1.7729852199554443
Epoch 80, training loss: 1.7778637409210205 = 1.702003836631775 + 0.01 * 7.58598518371582
Epoch 80, val loss: 1.720203161239624
Epoch 90, training loss: 1.699664831161499 = 1.6254831552505493 + 0.01 * 7.418166160583496
Epoch 90, val loss: 1.6521228551864624
Epoch 100, training loss: 1.6006479263305664 = 1.5282366275787354 + 0.01 * 7.241126537322998
Epoch 100, val loss: 1.5704644918441772
Epoch 110, training loss: 1.490114450454712 = 1.4189188480377197 + 0.01 * 7.119561195373535
Epoch 110, val loss: 1.4790359735488892
Epoch 120, training loss: 1.381623387336731 = 1.3108584880828857 + 0.01 * 7.076492786407471
Epoch 120, val loss: 1.3922832012176514
Epoch 130, training loss: 1.2817175388336182 = 1.2114157676696777 + 0.01 * 7.030180931091309
Epoch 130, val loss: 1.3166897296905518
Epoch 140, training loss: 1.1915223598480225 = 1.12153160572052 + 0.01 * 6.99907922744751
Epoch 140, val loss: 1.2516981363296509
Epoch 150, training loss: 1.1099148988723755 = 1.0401909351348877 + 0.01 * 6.9723920822143555
Epoch 150, val loss: 1.1945419311523438
Epoch 160, training loss: 1.0350385904312134 = 0.9655563831329346 + 0.01 * 6.948225975036621
Epoch 160, val loss: 1.1438044309616089
Epoch 170, training loss: 0.9645310044288635 = 0.8953016996383667 + 0.01 * 6.92293119430542
Epoch 170, val loss: 1.0973281860351562
Epoch 180, training loss: 0.8959001302719116 = 0.8269690275192261 + 0.01 * 6.8931121826171875
Epoch 180, val loss: 1.0529299974441528
Epoch 190, training loss: 0.8280478715896606 = 0.7594693899154663 + 0.01 * 6.857851505279541
Epoch 190, val loss: 1.0095934867858887
Epoch 200, training loss: 0.762220025062561 = 0.6938914656639099 + 0.01 * 6.83285665512085
Epoch 200, val loss: 0.9687440991401672
Epoch 210, training loss: 0.7002989053726196 = 0.6322362422943115 + 0.01 * 6.806268215179443
Epoch 210, val loss: 0.9325648546218872
Epoch 220, training loss: 0.6432813405990601 = 0.5753705501556396 + 0.01 * 6.791081428527832
Epoch 220, val loss: 0.9024919271469116
Epoch 230, training loss: 0.5906355381011963 = 0.5228453278541565 + 0.01 * 6.779021263122559
Epoch 230, val loss: 0.8781822919845581
Epoch 240, training loss: 0.5413943529129028 = 0.4737131893634796 + 0.01 * 6.768117904663086
Epoch 240, val loss: 0.8584570288658142
Epoch 250, training loss: 0.49495524168014526 = 0.42734527587890625 + 0.01 * 6.760995864868164
Epoch 250, val loss: 0.8426657915115356
Epoch 260, training loss: 0.45094025135040283 = 0.38343703746795654 + 0.01 * 6.750320911407471
Epoch 260, val loss: 0.8307514786720276
Epoch 270, training loss: 0.40937182307243347 = 0.34196290373802185 + 0.01 * 6.740891933441162
Epoch 270, val loss: 0.8231277465820312
Epoch 280, training loss: 0.3705418109893799 = 0.3032262325286865 + 0.01 * 6.731558322906494
Epoch 280, val loss: 0.819946825504303
Epoch 290, training loss: 0.3349209427833557 = 0.2676489055156708 + 0.01 * 6.727203369140625
Epoch 290, val loss: 0.8211371898651123
Epoch 300, training loss: 0.30284354090690613 = 0.23563770949840546 + 0.01 * 6.720582962036133
Epoch 300, val loss: 0.826072633266449
Epoch 310, training loss: 0.2744927406311035 = 0.20730085670948029 + 0.01 * 6.719188213348389
Epoch 310, val loss: 0.8341159224510193
Epoch 320, training loss: 0.24961653351783752 = 0.18251663446426392 + 0.01 * 6.709990978240967
Epoch 320, val loss: 0.8448632955551147
Epoch 330, training loss: 0.22799204289913177 = 0.16094276309013367 + 0.01 * 6.704928398132324
Epoch 330, val loss: 0.8576590418815613
Epoch 340, training loss: 0.20916327834129333 = 0.14216195046901703 + 0.01 * 6.700133800506592
Epoch 340, val loss: 0.8718619346618652
Epoch 350, training loss: 0.19279679656028748 = 0.12584766745567322 + 0.01 * 6.694912433624268
Epoch 350, val loss: 0.8872183561325073
Epoch 360, training loss: 0.1785513311624527 = 0.11165730655193329 + 0.01 * 6.6894025802612305
Epoch 360, val loss: 0.9032977223396301
Epoch 370, training loss: 0.16620168089866638 = 0.09928790479898453 + 0.01 * 6.69137716293335
Epoch 370, val loss: 0.920150637626648
Epoch 380, training loss: 0.15520958602428436 = 0.08840387314558029 + 0.01 * 6.680571556091309
Epoch 380, val loss: 0.9371393918991089
Epoch 390, training loss: 0.1455533355474472 = 0.078804150223732 + 0.01 * 6.6749186515808105
Epoch 390, val loss: 0.9541976451873779
Epoch 400, training loss: 0.1370665282011032 = 0.07038519531488419 + 0.01 * 6.668133735656738
Epoch 400, val loss: 0.9714815616607666
Epoch 410, training loss: 0.12969088554382324 = 0.0630609542131424 + 0.01 * 6.662993907928467
Epoch 410, val loss: 0.9886367321014404
Epoch 420, training loss: 0.12334994226694107 = 0.056746482849121094 + 0.01 * 6.660346031188965
Epoch 420, val loss: 1.005674123764038
Epoch 430, training loss: 0.11782611906528473 = 0.051279399544000626 + 0.01 * 6.654672622680664
Epoch 430, val loss: 1.0225681066513062
Epoch 440, training loss: 0.11298854649066925 = 0.046507980674505234 + 0.01 * 6.648056507110596
Epoch 440, val loss: 1.0393164157867432
Epoch 450, training loss: 0.10877493023872375 = 0.04232402890920639 + 0.01 * 6.645090579986572
Epoch 450, val loss: 1.0558454990386963
Epoch 460, training loss: 0.10503284633159637 = 0.03861691802740097 + 0.01 * 6.641592979431152
Epoch 460, val loss: 1.0721770524978638
Epoch 470, training loss: 0.10171789675951004 = 0.0353027880191803 + 0.01 * 6.641510963439941
Epoch 470, val loss: 1.0879640579223633
Epoch 480, training loss: 0.09870009124279022 = 0.03233521804213524 + 0.01 * 6.63648796081543
Epoch 480, val loss: 1.1035795211791992
Epoch 490, training loss: 0.0959945023059845 = 0.029657838866114616 + 0.01 * 6.633666038513184
Epoch 490, val loss: 1.1186693906784058
Epoch 500, training loss: 0.09350934624671936 = 0.027227137237787247 + 0.01 * 6.628221035003662
Epoch 500, val loss: 1.1335046291351318
Epoch 510, training loss: 0.0913110002875328 = 0.02502378821372986 + 0.01 * 6.628721237182617
Epoch 510, val loss: 1.1481252908706665
Epoch 520, training loss: 0.08928917348384857 = 0.02304830215871334 + 0.01 * 6.624087333679199
Epoch 520, val loss: 1.1621458530426025
Epoch 530, training loss: 0.08752796798944473 = 0.021274849772453308 + 0.01 * 6.625311851501465
Epoch 530, val loss: 1.1762794256210327
Epoch 540, training loss: 0.08587522804737091 = 0.019681159406900406 + 0.01 * 6.6194071769714355
Epoch 540, val loss: 1.1899837255477905
Epoch 550, training loss: 0.08442416042089462 = 0.018254287540912628 + 0.01 * 6.616987228393555
Epoch 550, val loss: 1.2035013437271118
Epoch 560, training loss: 0.0831606537103653 = 0.016981791704893112 + 0.01 * 6.617886543273926
Epoch 560, val loss: 1.2166852951049805
Epoch 570, training loss: 0.08196362853050232 = 0.015842609107494354 + 0.01 * 6.612102031707764
Epoch 570, val loss: 1.2295749187469482
Epoch 580, training loss: 0.08096536993980408 = 0.014817501418292522 + 0.01 * 6.6147871017456055
Epoch 580, val loss: 1.2423276901245117
Epoch 590, training loss: 0.0800003856420517 = 0.013894335366785526 + 0.01 * 6.610605239868164
Epoch 590, val loss: 1.2547707557678223
Epoch 600, training loss: 0.07912079989910126 = 0.01305780466645956 + 0.01 * 6.60629940032959
Epoch 600, val loss: 1.2670729160308838
Epoch 610, training loss: 0.07832857966423035 = 0.01229802705347538 + 0.01 * 6.603055953979492
Epoch 610, val loss: 1.2789809703826904
Epoch 620, training loss: 0.07763661444187164 = 0.011605451814830303 + 0.01 * 6.603116512298584
Epoch 620, val loss: 1.2906503677368164
Epoch 630, training loss: 0.0769633874297142 = 0.010972742922604084 + 0.01 * 6.599064350128174
Epoch 630, val loss: 1.3019362688064575
Epoch 640, training loss: 0.07635378837585449 = 0.01039237342774868 + 0.01 * 6.596141815185547
Epoch 640, val loss: 1.313022494316101
Epoch 650, training loss: 0.07580132782459259 = 0.009858665987849236 + 0.01 * 6.594266414642334
Epoch 650, val loss: 1.3238193988800049
Epoch 660, training loss: 0.07528074085712433 = 0.009366585873067379 + 0.01 * 6.5914154052734375
Epoch 660, val loss: 1.3344166278839111
Epoch 670, training loss: 0.07489380985498428 = 0.008912389166653156 + 0.01 * 6.598141670227051
Epoch 670, val loss: 1.3446404933929443
Epoch 680, training loss: 0.07441972941160202 = 0.008492368273437023 + 0.01 * 6.592736721038818
Epoch 680, val loss: 1.3547265529632568
Epoch 690, training loss: 0.07398150116205215 = 0.008102736435830593 + 0.01 * 6.587876796722412
Epoch 690, val loss: 1.3645143508911133
Epoch 700, training loss: 0.07358916103839874 = 0.007740612141788006 + 0.01 * 6.584855079650879
Epoch 700, val loss: 1.3740994930267334
Epoch 710, training loss: 0.0732383131980896 = 0.0074037909507751465 + 0.01 * 6.583452224731445
Epoch 710, val loss: 1.383379578590393
Epoch 720, training loss: 0.0729048028588295 = 0.007090283557772636 + 0.01 * 6.581451892852783
Epoch 720, val loss: 1.392409324645996
Epoch 730, training loss: 0.07262059301137924 = 0.006797925569117069 + 0.01 * 6.5822672843933105
Epoch 730, val loss: 1.4012706279754639
Epoch 740, training loss: 0.07229065895080566 = 0.006524683441966772 + 0.01 * 6.576598167419434
Epoch 740, val loss: 1.409953236579895
Epoch 750, training loss: 0.072015181183815 = 0.006269042380154133 + 0.01 * 6.574613571166992
Epoch 750, val loss: 1.418350338935852
Epoch 760, training loss: 0.07179383188486099 = 0.006029497366398573 + 0.01 * 6.576434135437012
Epoch 760, val loss: 1.4266277551651
Epoch 770, training loss: 0.07151664048433304 = 0.00580473430454731 + 0.01 * 6.571191310882568
Epoch 770, val loss: 1.4347455501556396
Epoch 780, training loss: 0.07127462327480316 = 0.005593850277364254 + 0.01 * 6.568077087402344
Epoch 780, val loss: 1.4425832033157349
Epoch 790, training loss: 0.07111133635044098 = 0.005395411979407072 + 0.01 * 6.571592330932617
Epoch 790, val loss: 1.4503053426742554
Epoch 800, training loss: 0.0708421990275383 = 0.005208753049373627 + 0.01 * 6.563344478607178
Epoch 800, val loss: 1.4578086137771606
Epoch 810, training loss: 0.07065220922231674 = 0.005032726097851992 + 0.01 * 6.561948299407959
Epoch 810, val loss: 1.465165376663208
Epoch 820, training loss: 0.07051430642604828 = 0.004866333212703466 + 0.01 * 6.564797401428223
Epoch 820, val loss: 1.4723718166351318
Epoch 830, training loss: 0.07029005140066147 = 0.004709129687398672 + 0.01 * 6.55809211730957
Epoch 830, val loss: 1.47939932346344
Epoch 840, training loss: 0.07021930813789368 = 0.004560243804007769 + 0.01 * 6.565906524658203
Epoch 840, val loss: 1.4863275289535522
Epoch 850, training loss: 0.0700148493051529 = 0.004419406875967979 + 0.01 * 6.559544086456299
Epoch 850, val loss: 1.4930927753448486
Epoch 860, training loss: 0.06982965022325516 = 0.004285970237106085 + 0.01 * 6.554368495941162
Epoch 860, val loss: 1.4996966123580933
Epoch 870, training loss: 0.06965924054384232 = 0.004159204196184874 + 0.01 * 6.550004005432129
Epoch 870, val loss: 1.5062098503112793
Epoch 880, training loss: 0.06961130350828171 = 0.004038654267787933 + 0.01 * 6.557265281677246
Epoch 880, val loss: 1.5125911235809326
Epoch 890, training loss: 0.06952710449695587 = 0.003924363758414984 + 0.01 * 6.560274600982666
Epoch 890, val loss: 1.518715739250183
Epoch 900, training loss: 0.0692795142531395 = 0.0038156374357640743 + 0.01 * 6.546387672424316
Epoch 900, val loss: 1.5248196125030518
Epoch 910, training loss: 0.06915738433599472 = 0.003712016623467207 + 0.01 * 6.544537544250488
Epoch 910, val loss: 1.5308191776275635
Epoch 920, training loss: 0.06908933818340302 = 0.003613336244598031 + 0.01 * 6.547600269317627
Epoch 920, val loss: 1.536537528038025
Epoch 930, training loss: 0.06891933083534241 = 0.003519064513966441 + 0.01 * 6.540026664733887
Epoch 930, val loss: 1.5423583984375
Epoch 940, training loss: 0.06887457519769669 = 0.003429134376347065 + 0.01 * 6.544544219970703
Epoch 940, val loss: 1.5478659868240356
Epoch 950, training loss: 0.06878507137298584 = 0.0033432228956371546 + 0.01 * 6.544185161590576
Epoch 950, val loss: 1.5534169673919678
Epoch 960, training loss: 0.06866706907749176 = 0.0032611030619591475 + 0.01 * 6.5405964851379395
Epoch 960, val loss: 1.5588558912277222
Epoch 970, training loss: 0.06851642578840256 = 0.0031826694030314684 + 0.01 * 6.5333757400512695
Epoch 970, val loss: 1.5640915632247925
Epoch 980, training loss: 0.06850174069404602 = 0.003107453463599086 + 0.01 * 6.539429187774658
Epoch 980, val loss: 1.569331169128418
Epoch 990, training loss: 0.06834117323160172 = 0.0030356799252331257 + 0.01 * 6.5305495262146
Epoch 990, val loss: 1.5742453336715698
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.6015
Flip ASR: 0.5422/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.036813259124756 = 1.9530775547027588 + 0.01 * 8.373568534851074
Epoch 0, val loss: 1.948216199874878
Epoch 10, training loss: 2.023599624633789 = 1.9398694038391113 + 0.01 * 8.373025894165039
Epoch 10, val loss: 1.9317857027053833
Epoch 20, training loss: 2.0077686309814453 = 1.9240443706512451 + 0.01 * 8.372437477111816
Epoch 20, val loss: 1.910914659500122
Epoch 30, training loss: 1.9877142906188965 = 1.9039947986602783 + 0.01 * 8.371953010559082
Epoch 30, val loss: 1.8851604461669922
Epoch 40, training loss: 1.963119387626648 = 1.8794188499450684 + 0.01 * 8.370057106018066
Epoch 40, val loss: 1.8569632768630981
Epoch 50, training loss: 1.9312653541564941 = 1.8476874828338623 + 0.01 * 8.35778522491455
Epoch 50, val loss: 1.824514627456665
Epoch 60, training loss: 1.890699863433838 = 1.8079184293746948 + 0.01 * 8.278138160705566
Epoch 60, val loss: 1.787821650505066
Epoch 70, training loss: 1.8464939594268799 = 1.7660797834396362 + 0.01 * 8.04141616821289
Epoch 70, val loss: 1.7542753219604492
Epoch 80, training loss: 1.7965948581695557 = 1.721935510635376 + 0.01 * 7.465936660766602
Epoch 80, val loss: 1.7207967042922974
Epoch 90, training loss: 1.733618974685669 = 1.662091851234436 + 0.01 * 7.152709484100342
Epoch 90, val loss: 1.6728920936584473
Epoch 100, training loss: 1.6505913734436035 = 1.5799556970596313 + 0.01 * 7.063566207885742
Epoch 100, val loss: 1.6066526174545288
Epoch 110, training loss: 1.5467349290847778 = 1.4766998291015625 + 0.01 * 7.003512382507324
Epoch 110, val loss: 1.5260380506515503
Epoch 120, training loss: 1.4340096712112427 = 1.3646045923233032 + 0.01 * 6.940510272979736
Epoch 120, val loss: 1.4421809911727905
Epoch 130, training loss: 1.325177788734436 = 1.2562264204025269 + 0.01 * 6.895136833190918
Epoch 130, val loss: 1.3647679090499878
Epoch 140, training loss: 1.2257939577102661 = 1.1571663618087769 + 0.01 * 6.862759113311768
Epoch 140, val loss: 1.2969603538513184
Epoch 150, training loss: 1.1373430490493774 = 1.0689712762832642 + 0.01 * 6.837172031402588
Epoch 150, val loss: 1.2369635105133057
Epoch 160, training loss: 1.0592758655548096 = 0.9910765886306763 + 0.01 * 6.8199238777160645
Epoch 160, val loss: 1.1823668479919434
Epoch 170, training loss: 0.9894176125526428 = 0.9213230609893799 + 0.01 * 6.809455394744873
Epoch 170, val loss: 1.131699562072754
Epoch 180, training loss: 0.924518346786499 = 0.8565112352371216 + 0.01 * 6.800711631774902
Epoch 180, val loss: 1.0831494331359863
Epoch 190, training loss: 0.8606048822402954 = 0.7926656603813171 + 0.01 * 6.793924808502197
Epoch 190, val loss: 1.0342206954956055
Epoch 200, training loss: 0.7947992086410522 = 0.7269381880760193 + 0.01 * 6.786103248596191
Epoch 200, val loss: 0.9832599759101868
Epoch 210, training loss: 0.7266326546669006 = 0.658825159072876 + 0.01 * 6.780750274658203
Epoch 210, val loss: 0.9304086565971375
Epoch 220, training loss: 0.6581230759620667 = 0.5904179215431213 + 0.01 * 6.770517826080322
Epoch 220, val loss: 0.8778778910636902
Epoch 230, training loss: 0.5925990343093872 = 0.5249741673469543 + 0.01 * 6.762485504150391
Epoch 230, val loss: 0.8298748731613159
Epoch 240, training loss: 0.532747745513916 = 0.46520715951919556 + 0.01 * 6.754061222076416
Epoch 240, val loss: 0.7898488640785217
Epoch 250, training loss: 0.47977346181869507 = 0.4122835397720337 + 0.01 * 6.7489914894104
Epoch 250, val loss: 0.7594674229621887
Epoch 260, training loss: 0.43391865491867065 = 0.3665085434913635 + 0.01 * 6.741013050079346
Epoch 260, val loss: 0.7381866574287415
Epoch 270, training loss: 0.3947000503540039 = 0.3273143768310547 + 0.01 * 6.738569259643555
Epoch 270, val loss: 0.7244398593902588
Epoch 280, training loss: 0.3609061539173126 = 0.29361289739608765 + 0.01 * 6.729324817657471
Epoch 280, val loss: 0.716748058795929
Epoch 290, training loss: 0.3315275013446808 = 0.26424193382263184 + 0.01 * 6.728556156158447
Epoch 290, val loss: 0.7139567732810974
Epoch 300, training loss: 0.3052154779434204 = 0.23794925212860107 + 0.01 * 6.726624488830566
Epoch 300, val loss: 0.7146012783050537
Epoch 310, training loss: 0.281170517206192 = 0.21392150223255157 + 0.01 * 6.7249016761779785
Epoch 310, val loss: 0.7182015776634216
Epoch 320, training loss: 0.25893235206604004 = 0.19168782234191895 + 0.01 * 6.724454402923584
Epoch 320, val loss: 0.7240399718284607
Epoch 330, training loss: 0.23835566639900208 = 0.17113393545150757 + 0.01 * 6.72217321395874
Epoch 330, val loss: 0.7317416667938232
Epoch 340, training loss: 0.21945716440677643 = 0.15224647521972656 + 0.01 * 6.721068859100342
Epoch 340, val loss: 0.7409718632698059
Epoch 350, training loss: 0.2023344337940216 = 0.135128453373909 + 0.01 * 6.720599174499512
Epoch 350, val loss: 0.7515193819999695
Epoch 360, training loss: 0.1869584023952484 = 0.11975602060556412 + 0.01 * 6.720237731933594
Epoch 360, val loss: 0.7631138563156128
Epoch 370, training loss: 0.17330622673034668 = 0.106110580265522 + 0.01 * 6.7195658683776855
Epoch 370, val loss: 0.7756605744361877
Epoch 380, training loss: 0.1612740457057953 = 0.09408222883939743 + 0.01 * 6.719181537628174
Epoch 380, val loss: 0.7889289855957031
Epoch 390, training loss: 0.15071514248847961 = 0.08352452516555786 + 0.01 * 6.719060897827148
Epoch 390, val loss: 0.8028479218482971
Epoch 400, training loss: 0.14150874316692352 = 0.07433129101991653 + 0.01 * 6.717745304107666
Epoch 400, val loss: 0.8174228668212891
Epoch 410, training loss: 0.13349434733390808 = 0.06632724404335022 + 0.01 * 6.716709613800049
Epoch 410, val loss: 0.8323829770088196
Epoch 420, training loss: 0.12650509178638458 = 0.0593486949801445 + 0.01 * 6.715639591217041
Epoch 420, val loss: 0.8476238250732422
Epoch 430, training loss: 0.12042891979217529 = 0.053281594067811966 + 0.01 * 6.714732646942139
Epoch 430, val loss: 0.8629571795463562
Epoch 440, training loss: 0.11513467133045197 = 0.04799051955342293 + 0.01 * 6.714415550231934
Epoch 440, val loss: 0.8782581090927124
Epoch 450, training loss: 0.11050410568714142 = 0.04337800666689873 + 0.01 * 6.712610244750977
Epoch 450, val loss: 0.8934398889541626
Epoch 460, training loss: 0.1064593493938446 = 0.03934866562485695 + 0.01 * 6.711069107055664
Epoch 460, val loss: 0.9084393978118896
Epoch 470, training loss: 0.1029227077960968 = 0.03581820800900459 + 0.01 * 6.710450649261475
Epoch 470, val loss: 0.9232321977615356
Epoch 480, training loss: 0.09979899227619171 = 0.032713647931814194 + 0.01 * 6.708535194396973
Epoch 480, val loss: 0.9377557635307312
Epoch 490, training loss: 0.09703703969717026 = 0.029963955283164978 + 0.01 * 6.707308769226074
Epoch 490, val loss: 0.9519951939582825
Epoch 500, training loss: 0.09459559619426727 = 0.027533426880836487 + 0.01 * 6.706217288970947
Epoch 500, val loss: 0.9659899473190308
Epoch 510, training loss: 0.09242066740989685 = 0.025376509875059128 + 0.01 * 6.704415798187256
Epoch 510, val loss: 0.9797137379646301
Epoch 520, training loss: 0.09050910919904709 = 0.02345464937388897 + 0.01 * 6.705446243286133
Epoch 520, val loss: 0.9931423664093018
Epoch 530, training loss: 0.08877231180667877 = 0.0217440128326416 + 0.01 * 6.7028303146362305
Epoch 530, val loss: 1.006212592124939
Epoch 540, training loss: 0.08721484988927841 = 0.020213080570101738 + 0.01 * 6.70017671585083
Epoch 540, val loss: 1.018926978111267
Epoch 550, training loss: 0.08582346886396408 = 0.018838373944163322 + 0.01 * 6.698509216308594
Epoch 550, val loss: 1.0313369035720825
Epoch 560, training loss: 0.08456921577453613 = 0.017600294202566147 + 0.01 * 6.696892261505127
Epoch 560, val loss: 1.0434521436691284
Epoch 570, training loss: 0.08343196660280228 = 0.016477957367897034 + 0.01 * 6.695400714874268
Epoch 570, val loss: 1.055267572402954
Epoch 580, training loss: 0.08242066949605942 = 0.015458284877240658 + 0.01 * 6.6962385177612305
Epoch 580, val loss: 1.0667568445205688
Epoch 590, training loss: 0.0814594253897667 = 0.014529605396091938 + 0.01 * 6.692981719970703
Epoch 590, val loss: 1.0779979228973389
Epoch 600, training loss: 0.08058543503284454 = 0.013678810559213161 + 0.01 * 6.690662384033203
Epoch 600, val loss: 1.0889947414398193
Epoch 610, training loss: 0.07981324940919876 = 0.012896927073597908 + 0.01 * 6.691632270812988
Epoch 610, val loss: 1.0997979640960693
Epoch 620, training loss: 0.07906380295753479 = 0.012177404947578907 + 0.01 * 6.6886396408081055
Epoch 620, val loss: 1.1104321479797363
Epoch 630, training loss: 0.07837402820587158 = 0.011513407342135906 + 0.01 * 6.686062812805176
Epoch 630, val loss: 1.1208354234695435
Epoch 640, training loss: 0.07778195291757584 = 0.010903077200055122 + 0.01 * 6.687888145446777
Epoch 640, val loss: 1.1310261487960815
Epoch 650, training loss: 0.07716145366430283 = 0.010338089428842068 + 0.01 * 6.682336330413818
Epoch 650, val loss: 1.1410173177719116
Epoch 660, training loss: 0.07661929726600647 = 0.009813252836465836 + 0.01 * 6.680603981018066
Epoch 660, val loss: 1.1508451700210571
Epoch 670, training loss: 0.07611481845378876 = 0.009325679391622543 + 0.01 * 6.678913593292236
Epoch 670, val loss: 1.1605088710784912
Epoch 680, training loss: 0.07563520222902298 = 0.008871801197528839 + 0.01 * 6.676340103149414
Epoch 680, val loss: 1.1699808835983276
Epoch 690, training loss: 0.07519476860761642 = 0.008447213098406792 + 0.01 * 6.674755573272705
Epoch 690, val loss: 1.179348349571228
Epoch 700, training loss: 0.07477591931819916 = 0.008051917888224125 + 0.01 * 6.67240047454834
Epoch 700, val loss: 1.1885713338851929
Epoch 710, training loss: 0.07440859079360962 = 0.0076824091374874115 + 0.01 * 6.672618389129639
Epoch 710, val loss: 1.1976441144943237
Epoch 720, training loss: 0.07404721528291702 = 0.0073373219929635525 + 0.01 * 6.670989513397217
Epoch 720, val loss: 1.2065528631210327
Epoch 730, training loss: 0.07372017949819565 = 0.007014542352408171 + 0.01 * 6.6705641746521
Epoch 730, val loss: 1.215330719947815
Epoch 740, training loss: 0.07340648025274277 = 0.006714241113513708 + 0.01 * 6.669224262237549
Epoch 740, val loss: 1.2239761352539062
Epoch 750, training loss: 0.07306398451328278 = 0.0064338697120547295 + 0.01 * 6.66301155090332
Epoch 750, val loss: 1.232406497001648
Epoch 760, training loss: 0.07277476042509079 = 0.00617103511467576 + 0.01 * 6.660373210906982
Epoch 760, val loss: 1.2406846284866333
Epoch 770, training loss: 0.07256557792425156 = 0.005924468394368887 + 0.01 * 6.664111137390137
Epoch 770, val loss: 1.248846411705017
Epoch 780, training loss: 0.07229386270046234 = 0.005693100392818451 + 0.01 * 6.66007661819458
Epoch 780, val loss: 1.2568268775939941
Epoch 790, training loss: 0.07201486080884933 = 0.005476256832480431 + 0.01 * 6.653860569000244
Epoch 790, val loss: 1.2646458148956299
Epoch 800, training loss: 0.07182428240776062 = 0.005272338632494211 + 0.01 * 6.6551947593688965
Epoch 800, val loss: 1.2723464965820312
Epoch 810, training loss: 0.07157589495182037 = 0.005081574898213148 + 0.01 * 6.64943265914917
Epoch 810, val loss: 1.2797999382019043
Epoch 820, training loss: 0.07137443870306015 = 0.004902464337646961 + 0.01 * 6.647197246551514
Epoch 820, val loss: 1.287118911743164
Epoch 830, training loss: 0.07116751372814178 = 0.0047335498966276646 + 0.01 * 6.643396854400635
Epoch 830, val loss: 1.2943196296691895
Epoch 840, training loss: 0.07102694362401962 = 0.004574172664433718 + 0.01 * 6.64527702331543
Epoch 840, val loss: 1.3013356924057007
Epoch 850, training loss: 0.07080987095832825 = 0.004423657897859812 + 0.01 * 6.638621807098389
Epoch 850, val loss: 1.308233380317688
Epoch 860, training loss: 0.07064394652843475 = 0.00428130105137825 + 0.01 * 6.636264324188232
Epoch 860, val loss: 1.3150118589401245
Epoch 870, training loss: 0.07051382213830948 = 0.004146709572523832 + 0.01 * 6.636711597442627
Epoch 870, val loss: 1.3216382265090942
Epoch 880, training loss: 0.07039995491504669 = 0.004019524902105331 + 0.01 * 6.638043403625488
Epoch 880, val loss: 1.3281197547912598
Epoch 890, training loss: 0.07016919553279877 = 0.0038989672902971506 + 0.01 * 6.627022743225098
Epoch 890, val loss: 1.334449052810669
Epoch 900, training loss: 0.07002852857112885 = 0.0037846839986741543 + 0.01 * 6.62438440322876
Epoch 900, val loss: 1.3407020568847656
Epoch 910, training loss: 0.07005330175161362 = 0.0036764962133020163 + 0.01 * 6.637681007385254
Epoch 910, val loss: 1.3467249870300293
Epoch 920, training loss: 0.06980261951684952 = 0.003573992755264044 + 0.01 * 6.622862815856934
Epoch 920, val loss: 1.3526558876037598
Epoch 930, training loss: 0.06962765008211136 = 0.003476732410490513 + 0.01 * 6.615091800689697
Epoch 930, val loss: 1.3584896326065063
Epoch 940, training loss: 0.0694713443517685 = 0.003384267445653677 + 0.01 * 6.608707904815674
Epoch 940, val loss: 1.3641533851623535
Epoch 950, training loss: 0.06944318115711212 = 0.003296236740425229 + 0.01 * 6.614695072174072
Epoch 950, val loss: 1.3697068691253662
Epoch 960, training loss: 0.06936918944120407 = 0.0032125203870236874 + 0.01 * 6.61566686630249
Epoch 960, val loss: 1.3751951456069946
Epoch 970, training loss: 0.0691724419593811 = 0.003132697194814682 + 0.01 * 6.603974342346191
Epoch 970, val loss: 1.380488395690918
Epoch 980, training loss: 0.06901749968528748 = 0.0030568556394428015 + 0.01 * 6.596064567565918
Epoch 980, val loss: 1.3856534957885742
Epoch 990, training loss: 0.06888844072818756 = 0.0029845060780644417 + 0.01 * 6.590393543243408
Epoch 990, val loss: 1.3907057046890259
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.8303
Flip ASR: 0.8267/225 nodes
The final ASR:0.65314, 0.12882, Accuracy:0.80123, 0.00462
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11672])
remove edge: torch.Size([2, 9514])
updated graph: torch.Size([2, 10630])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9631
Flip ASR: 0.9556/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8481
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
The final ASR:0.97048, 0.00603, Accuracy:0.83704, 0.00907
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.5 der2:0 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.014347791671753 = 1.930609107017517 + 0.01 * 8.373872756958008
Epoch 0, val loss: 1.9298739433288574
Epoch 10, training loss: 2.0047926902770996 = 1.9210549592971802 + 0.01 * 8.373783111572266
Epoch 10, val loss: 1.9210631847381592
Epoch 20, training loss: 1.9926402568817139 = 1.9089051485061646 + 0.01 * 8.373513221740723
Epoch 20, val loss: 1.9095451831817627
Epoch 30, training loss: 1.97511887550354 = 1.891391396522522 + 0.01 * 8.3727445602417
Epoch 30, val loss: 1.8927112817764282
Epoch 40, training loss: 1.9490966796875 = 1.8654046058654785 + 0.01 * 8.369202613830566
Epoch 40, val loss: 1.8682315349578857
Epoch 50, training loss: 1.9135342836380005 = 1.8300976753234863 + 0.01 * 8.343659400939941
Epoch 50, val loss: 1.8370519876480103
Epoch 60, training loss: 1.8743598461151123 = 1.7921479940414429 + 0.01 * 8.22119140625
Epoch 60, val loss: 1.8069839477539062
Epoch 70, training loss: 1.8324247598648071 = 1.7541977167129517 + 0.01 * 7.822701454162598
Epoch 70, val loss: 1.774658203125
Epoch 80, training loss: 1.7761751413345337 = 1.701627254486084 + 0.01 * 7.454791069030762
Epoch 80, val loss: 1.7265125513076782
Epoch 90, training loss: 1.7013075351715088 = 1.6295090913772583 + 0.01 * 7.179840564727783
Epoch 90, val loss: 1.665342926979065
Epoch 100, training loss: 1.60844886302948 = 1.5380607843399048 + 0.01 * 7.038808822631836
Epoch 100, val loss: 1.5902093648910522
Epoch 110, training loss: 1.510244369506836 = 1.440307855606079 + 0.01 * 6.993649005889893
Epoch 110, val loss: 1.510364294052124
Epoch 120, training loss: 1.4143681526184082 = 1.3446404933929443 + 0.01 * 6.972763538360596
Epoch 120, val loss: 1.4346026182174683
Epoch 130, training loss: 1.3204795122146606 = 1.2509269714355469 + 0.01 * 6.955253601074219
Epoch 130, val loss: 1.3623343706130981
Epoch 140, training loss: 1.2266167402267456 = 1.157259225845337 + 0.01 * 6.935751914978027
Epoch 140, val loss: 1.290413737297058
Epoch 150, training loss: 1.1327745914459229 = 1.0635919570922852 + 0.01 * 6.918267250061035
Epoch 150, val loss: 1.219556450843811
Epoch 160, training loss: 1.0399013757705688 = 0.9708898067474365 + 0.01 * 6.901154041290283
Epoch 160, val loss: 1.150078535079956
Epoch 170, training loss: 0.9497056603431702 = 0.8808441758155823 + 0.01 * 6.886148929595947
Epoch 170, val loss: 1.0839332342147827
Epoch 180, training loss: 0.8638728857040405 = 0.7951387166976929 + 0.01 * 6.873415470123291
Epoch 180, val loss: 1.022477388381958
Epoch 190, training loss: 0.7833592295646667 = 0.7147485017776489 + 0.01 * 6.861074447631836
Epoch 190, val loss: 0.966646671295166
Epoch 200, training loss: 0.7089479565620422 = 0.640394926071167 + 0.01 * 6.855305194854736
Epoch 200, val loss: 0.9169103503227234
Epoch 210, training loss: 0.6414338946342468 = 0.5730102062225342 + 0.01 * 6.842367172241211
Epoch 210, val loss: 0.87436842918396
Epoch 220, training loss: 0.5815258622169495 = 0.5132356882095337 + 0.01 * 6.829017639160156
Epoch 220, val loss: 0.8398654460906982
Epoch 230, training loss: 0.5291439890861511 = 0.4608905017375946 + 0.01 * 6.825349807739258
Epoch 230, val loss: 0.8132172226905823
Epoch 240, training loss: 0.4828765392303467 = 0.4148279130458832 + 0.01 * 6.804861068725586
Epoch 240, val loss: 0.7934053540229797
Epoch 250, training loss: 0.44145914912223816 = 0.37347477674484253 + 0.01 * 6.798437118530273
Epoch 250, val loss: 0.7785805463790894
Epoch 260, training loss: 0.4033709764480591 = 0.33550286293029785 + 0.01 * 6.786810398101807
Epoch 260, val loss: 0.7673368453979492
Epoch 270, training loss: 0.36777549982070923 = 0.30003973841667175 + 0.01 * 6.773576736450195
Epoch 270, val loss: 0.7587730884552002
Epoch 280, training loss: 0.33419689536094666 = 0.2666049301624298 + 0.01 * 6.759195804595947
Epoch 280, val loss: 0.7524349689483643
Epoch 290, training loss: 0.3026849031448364 = 0.2351289689540863 + 0.01 * 6.755594253540039
Epoch 290, val loss: 0.7484645843505859
Epoch 300, training loss: 0.2733839154243469 = 0.20590227842330933 + 0.01 * 6.748165607452393
Epoch 300, val loss: 0.7472072243690491
Epoch 310, training loss: 0.24677351117134094 = 0.17936550080776215 + 0.01 * 6.74080228805542
Epoch 310, val loss: 0.7491366267204285
Epoch 320, training loss: 0.22334080934524536 = 0.15593720972537994 + 0.01 * 6.740359783172607
Epoch 320, val loss: 0.754379391670227
Epoch 330, training loss: 0.2029409408569336 = 0.13569413125514984 + 0.01 * 6.724681377410889
Epoch 330, val loss: 0.7629038095474243
Epoch 340, training loss: 0.18564115464687347 = 0.1184244379401207 + 0.01 * 6.721672058105469
Epoch 340, val loss: 0.774258553981781
Epoch 350, training loss: 0.17093831300735474 = 0.10377496480941772 + 0.01 * 6.716334819793701
Epoch 350, val loss: 0.7879425287246704
Epoch 360, training loss: 0.15852907299995422 = 0.09136877954006195 + 0.01 * 6.716029644012451
Epoch 360, val loss: 0.8033568263053894
Epoch 370, training loss: 0.14790022373199463 = 0.0808401107788086 + 0.01 * 6.706011772155762
Epoch 370, val loss: 0.8199873566627502
Epoch 380, training loss: 0.13889475166797638 = 0.07185457646846771 + 0.01 * 6.704017639160156
Epoch 380, val loss: 0.8374110460281372
Epoch 390, training loss: 0.13111993670463562 = 0.06414351612329483 + 0.01 * 6.697641372680664
Epoch 390, val loss: 0.8552338480949402
Epoch 400, training loss: 0.12446840107440948 = 0.05749712139368057 + 0.01 * 6.6971282958984375
Epoch 400, val loss: 0.8731303811073303
Epoch 410, training loss: 0.11865590512752533 = 0.05172990262508392 + 0.01 * 6.692600727081299
Epoch 410, val loss: 0.8909379839897156
Epoch 420, training loss: 0.11357596516609192 = 0.04670281335711479 + 0.01 * 6.687315940856934
Epoch 420, val loss: 0.9085183143615723
Epoch 430, training loss: 0.1091722846031189 = 0.04230361059308052 + 0.01 * 6.686868190765381
Epoch 430, val loss: 0.9257654547691345
Epoch 440, training loss: 0.10527792572975159 = 0.03844556212425232 + 0.01 * 6.683236598968506
Epoch 440, val loss: 0.9425731897354126
Epoch 450, training loss: 0.10186919569969177 = 0.035051748156547546 + 0.01 * 6.6817450523376465
Epoch 450, val loss: 0.9588770270347595
Epoch 460, training loss: 0.09881061315536499 = 0.0320548377931118 + 0.01 * 6.675577163696289
Epoch 460, val loss: 0.9747478365898132
Epoch 470, training loss: 0.09619396179914474 = 0.029402146115899086 + 0.01 * 6.6791815757751465
Epoch 470, val loss: 0.9901395440101624
Epoch 480, training loss: 0.09372812509536743 = 0.027050480246543884 + 0.01 * 6.667764663696289
Epoch 480, val loss: 1.005050778388977
Epoch 490, training loss: 0.09160050004720688 = 0.02495644986629486 + 0.01 * 6.66440486907959
Epoch 490, val loss: 1.0195404291152954
Epoch 500, training loss: 0.08972105383872986 = 0.023086193948984146 + 0.01 * 6.663486480712891
Epoch 500, val loss: 1.0335783958435059
Epoch 510, training loss: 0.08804074674844742 = 0.021411580964922905 + 0.01 * 6.662916660308838
Epoch 510, val loss: 1.047149658203125
Epoch 520, training loss: 0.08646244555711746 = 0.01990838535130024 + 0.01 * 6.655406475067139
Epoch 520, val loss: 1.0602537393569946
Epoch 530, training loss: 0.08510582894086838 = 0.018554702401161194 + 0.01 * 6.6551127433776855
Epoch 530, val loss: 1.0729643106460571
Epoch 540, training loss: 0.08391185104846954 = 0.017333680763840675 + 0.01 * 6.657817363739014
Epoch 540, val loss: 1.0853594541549683
Epoch 550, training loss: 0.08273754268884659 = 0.016230715438723564 + 0.01 * 6.65068244934082
Epoch 550, val loss: 1.0973325967788696
Epoch 560, training loss: 0.08166167140007019 = 0.01523018628358841 + 0.01 * 6.643148422241211
Epoch 560, val loss: 1.1089756488800049
Epoch 570, training loss: 0.08084601908922195 = 0.014320047572255135 + 0.01 * 6.652597427368164
Epoch 570, val loss: 1.1202183961868286
Epoch 580, training loss: 0.07989761978387833 = 0.013491536490619183 + 0.01 * 6.640608787536621
Epoch 580, val loss: 1.1310937404632568
Epoch 590, training loss: 0.07912590354681015 = 0.012734326533973217 + 0.01 * 6.639158248901367
Epoch 590, val loss: 1.1416746377944946
Epoch 600, training loss: 0.07842815667390823 = 0.012040937319397926 + 0.01 * 6.638721942901611
Epoch 600, val loss: 1.1519906520843506
Epoch 610, training loss: 0.07767340540885925 = 0.011404398828744888 + 0.01 * 6.6269001960754395
Epoch 610, val loss: 1.1619653701782227
Epoch 620, training loss: 0.07711660861968994 = 0.01081889495253563 + 0.01 * 6.6297712326049805
Epoch 620, val loss: 1.1716794967651367
Epoch 630, training loss: 0.07648584246635437 = 0.010279734618961811 + 0.01 * 6.62061071395874
Epoch 630, val loss: 1.1810427904129028
Epoch 640, training loss: 0.07593489438295364 = 0.009781732223927975 + 0.01 * 6.615316390991211
Epoch 640, val loss: 1.1902227401733398
Epoch 650, training loss: 0.07548580318689346 = 0.009320691227912903 + 0.01 * 6.616511344909668
Epoch 650, val loss: 1.1991177797317505
Epoch 660, training loss: 0.07501506805419922 = 0.008893624879419804 + 0.01 * 6.612144470214844
Epoch 660, val loss: 1.2077782154083252
Epoch 670, training loss: 0.07460552453994751 = 0.008497469127178192 + 0.01 * 6.610805988311768
Epoch 670, val loss: 1.2161574363708496
Epoch 680, training loss: 0.07415573298931122 = 0.008128690533339977 + 0.01 * 6.6027045249938965
Epoch 680, val loss: 1.2243497371673584
Epoch 690, training loss: 0.07380662858486176 = 0.007784615736454725 + 0.01 * 6.60220193862915
Epoch 690, val loss: 1.232326626777649
Epoch 700, training loss: 0.07349956035614014 = 0.007463525980710983 + 0.01 * 6.603602886199951
Epoch 700, val loss: 1.240064024925232
Epoch 710, training loss: 0.07320398837327957 = 0.007163940463215113 + 0.01 * 6.604004859924316
Epoch 710, val loss: 1.2476338148117065
Epoch 720, training loss: 0.07283306866884232 = 0.006883414462208748 + 0.01 * 6.59496545791626
Epoch 720, val loss: 1.2549867630004883
Epoch 730, training loss: 0.07253715395927429 = 0.006620281375944614 + 0.01 * 6.591687202453613
Epoch 730, val loss: 1.2621864080429077
Epoch 740, training loss: 0.07224451005458832 = 0.006373363547027111 + 0.01 * 6.587115287780762
Epoch 740, val loss: 1.2692421674728394
Epoch 750, training loss: 0.07197123020887375 = 0.006141684949398041 + 0.01 * 6.5829548835754395
Epoch 750, val loss: 1.2760645151138306
Epoch 760, training loss: 0.07171167433261871 = 0.005923563614487648 + 0.01 * 6.578811168670654
Epoch 760, val loss: 1.2827767133712769
Epoch 770, training loss: 0.07172876596450806 = 0.005718131083995104 + 0.01 * 6.601063251495361
Epoch 770, val loss: 1.2893705368041992
Epoch 780, training loss: 0.07140481472015381 = 0.005524580366909504 + 0.01 * 6.588023662567139
Epoch 780, val loss: 1.295714259147644
Epoch 790, training loss: 0.07116332650184631 = 0.005342122633010149 + 0.01 * 6.582120418548584
Epoch 790, val loss: 1.3019710779190063
Epoch 800, training loss: 0.07089941948652267 = 0.005169394426047802 + 0.01 * 6.57300329208374
Epoch 800, val loss: 1.3080670833587646
Epoch 810, training loss: 0.07070019096136093 = 0.005006126128137112 + 0.01 * 6.569406509399414
Epoch 810, val loss: 1.3139915466308594
Epoch 820, training loss: 0.0705375149846077 = 0.004851439502090216 + 0.01 * 6.568607807159424
Epoch 820, val loss: 1.3197935819625854
Epoch 830, training loss: 0.07031555473804474 = 0.004704611375927925 + 0.01 * 6.561094760894775
Epoch 830, val loss: 1.3255022764205933
Epoch 840, training loss: 0.0701923817396164 = 0.004565296694636345 + 0.01 * 6.562708377838135
Epoch 840, val loss: 1.3311069011688232
Epoch 850, training loss: 0.0700688436627388 = 0.004433086141943932 + 0.01 * 6.563575744628906
Epoch 850, val loss: 1.336553692817688
Epoch 860, training loss: 0.06995134800672531 = 0.004307533148676157 + 0.01 * 6.5643815994262695
Epoch 860, val loss: 1.3418998718261719
Epoch 870, training loss: 0.069705069065094 = 0.004188145510852337 + 0.01 * 6.551692485809326
Epoch 870, val loss: 1.3470978736877441
Epoch 880, training loss: 0.06956945359706879 = 0.00407444080337882 + 0.01 * 6.549501419067383
Epoch 880, val loss: 1.3521946668624878
Epoch 890, training loss: 0.06962995231151581 = 0.003966018091887236 + 0.01 * 6.5663933753967285
Epoch 890, val loss: 1.3572511672973633
Epoch 900, training loss: 0.06935792416334152 = 0.003862640354782343 + 0.01 * 6.5495285987854
Epoch 900, val loss: 1.3621416091918945
Epoch 910, training loss: 0.06928670406341553 = 0.003763996995985508 + 0.01 * 6.552270889282227
Epoch 910, val loss: 1.3669461011886597
Epoch 920, training loss: 0.06909873336553574 = 0.0036698465701192617 + 0.01 * 6.54288911819458
Epoch 920, val loss: 1.3717280626296997
Epoch 930, training loss: 0.06898089498281479 = 0.003579814685508609 + 0.01 * 6.5401082038879395
Epoch 930, val loss: 1.3763402700424194
Epoch 940, training loss: 0.06892062723636627 = 0.003493581200018525 + 0.01 * 6.5427045822143555
Epoch 940, val loss: 1.38095223903656
Epoch 950, training loss: 0.06878568232059479 = 0.003411288373172283 + 0.01 * 6.537439823150635
Epoch 950, val loss: 1.3853485584259033
Epoch 960, training loss: 0.06860697269439697 = 0.0033323445823043585 + 0.01 * 6.527462959289551
Epoch 960, val loss: 1.3897854089736938
Epoch 970, training loss: 0.0685541108250618 = 0.0032567819580435753 + 0.01 * 6.529732704162598
Epoch 970, val loss: 1.3940566778182983
Epoch 980, training loss: 0.0683862492442131 = 0.003184419358149171 + 0.01 * 6.5201826095581055
Epoch 980, val loss: 1.3982187509536743
Epoch 990, training loss: 0.06850075721740723 = 0.0031148502603173256 + 0.01 * 6.538590908050537
Epoch 990, val loss: 1.4023981094360352
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.5683
Flip ASR: 0.4844/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.037569999694824 = 1.9538308382034302 + 0.01 * 8.373912811279297
Epoch 0, val loss: 1.954712152481079
Epoch 10, training loss: 2.0279247760772705 = 1.9441860914230347 + 0.01 * 8.373866081237793
Epoch 10, val loss: 1.9456158876419067
Epoch 20, training loss: 2.0164358615875244 = 1.9326988458633423 + 0.01 * 8.373702049255371
Epoch 20, val loss: 1.9345786571502686
Epoch 30, training loss: 2.0007758140563965 = 1.9170429706573486 + 0.01 * 8.373294830322266
Epoch 30, val loss: 1.9193596839904785
Epoch 40, training loss: 1.9777482748031616 = 1.8940284252166748 + 0.01 * 8.371981620788574
Epoch 40, val loss: 1.897017240524292
Epoch 50, training loss: 1.9436510801315308 = 1.8599979877471924 + 0.01 * 8.365309715270996
Epoch 50, val loss: 1.8648017644882202
Epoch 60, training loss: 1.8990366458892822 = 1.8157840967178345 + 0.01 * 8.325249671936035
Epoch 60, val loss: 1.8251503705978394
Epoch 70, training loss: 1.8515220880508423 = 1.770379662513733 + 0.01 * 8.11424732208252
Epoch 70, val loss: 1.785104513168335
Epoch 80, training loss: 1.7982888221740723 = 1.7227157354354858 + 0.01 * 7.557309150695801
Epoch 80, val loss: 1.7388144731521606
Epoch 90, training loss: 1.7318397760391235 = 1.658562421798706 + 0.01 * 7.327735900878906
Epoch 90, val loss: 1.6807533502578735
Epoch 100, training loss: 1.6456575393676758 = 1.5741791725158691 + 0.01 * 7.1478352546691895
Epoch 100, val loss: 1.6113293170928955
Epoch 110, training loss: 1.5429741144180298 = 1.4724417924880981 + 0.01 * 7.053228378295898
Epoch 110, val loss: 1.5273925065994263
Epoch 120, training loss: 1.4337104558944702 = 1.3634644746780396 + 0.01 * 7.024602890014648
Epoch 120, val loss: 1.4373892545700073
Epoch 130, training loss: 1.3253165483474731 = 1.2552582025527954 + 0.01 * 7.005833148956299
Epoch 130, val loss: 1.3482252359390259
Epoch 140, training loss: 1.220055103302002 = 1.1501375436782837 + 0.01 * 6.991756439208984
Epoch 140, val loss: 1.2642626762390137
Epoch 150, training loss: 1.1192879676818848 = 1.049500584602356 + 0.01 * 6.97874116897583
Epoch 150, val loss: 1.185514211654663
Epoch 160, training loss: 1.0243397951126099 = 0.9546917676925659 + 0.01 * 6.9648003578186035
Epoch 160, val loss: 1.1138747930526733
Epoch 170, training loss: 0.9359349012374878 = 0.866415798664093 + 0.01 * 6.951910018920898
Epoch 170, val loss: 1.0492948293685913
Epoch 180, training loss: 0.8541457056999207 = 0.7847410440444946 + 0.01 * 6.940465927124023
Epoch 180, val loss: 0.9909153580665588
Epoch 190, training loss: 0.7788316607475281 = 0.7095256447792053 + 0.01 * 6.930600166320801
Epoch 190, val loss: 0.9388972520828247
Epoch 200, training loss: 0.7095381617546082 = 0.6403159499168396 + 0.01 * 6.922219276428223
Epoch 200, val loss: 0.8926202654838562
Epoch 210, training loss: 0.6456377506256104 = 0.5764878392219543 + 0.01 * 6.914994239807129
Epoch 210, val loss: 0.8517097234725952
Epoch 220, training loss: 0.5866522192955017 = 0.5175600051879883 + 0.01 * 6.909224033355713
Epoch 220, val loss: 0.8158621788024902
Epoch 230, training loss: 0.5322718024253845 = 0.4632592797279358 + 0.01 * 6.90125036239624
Epoch 230, val loss: 0.7849311828613281
Epoch 240, training loss: 0.48203137516975403 = 0.41309550404548645 + 0.01 * 6.8935866355896
Epoch 240, val loss: 0.7584810853004456
Epoch 250, training loss: 0.43524253368377686 = 0.3663843870162964 + 0.01 * 6.885814666748047
Epoch 250, val loss: 0.7362469434738159
Epoch 260, training loss: 0.39138978719711304 = 0.3226363956928253 + 0.01 * 6.875339508056641
Epoch 260, val loss: 0.7179105877876282
Epoch 270, training loss: 0.35064697265625 = 0.28196460008621216 + 0.01 * 6.868237018585205
Epoch 270, val loss: 0.7038059234619141
Epoch 280, training loss: 0.31354185938835144 = 0.24495218694210052 + 0.01 * 6.8589677810668945
Epoch 280, val loss: 0.6937087774276733
Epoch 290, training loss: 0.2807941436767578 = 0.21232886612415314 + 0.01 * 6.846529483795166
Epoch 290, val loss: 0.6875572800636292
Epoch 300, training loss: 0.25274595618247986 = 0.18436464667320251 + 0.01 * 6.838130950927734
Epoch 300, val loss: 0.6854093670845032
Epoch 310, training loss: 0.22905388474464417 = 0.16075511276721954 + 0.01 * 6.829876899719238
Epoch 310, val loss: 0.6869162321090698
Epoch 320, training loss: 0.20924493670463562 = 0.1408623456954956 + 0.01 * 6.838260173797607
Epoch 320, val loss: 0.6911484599113464
Epoch 330, training loss: 0.19220121204853058 = 0.12399867922067642 + 0.01 * 6.820253372192383
Epoch 330, val loss: 0.6975170969963074
Epoch 340, training loss: 0.1777157485485077 = 0.10954910516738892 + 0.01 * 6.816665172576904
Epoch 340, val loss: 0.7055127620697021
Epoch 350, training loss: 0.16510707139968872 = 0.09698720276355743 + 0.01 * 6.811987400054932
Epoch 350, val loss: 0.7150506973266602
Epoch 360, training loss: 0.15396994352340698 = 0.08589448034763336 + 0.01 * 6.807547092437744
Epoch 360, val loss: 0.7254994511604309
Epoch 370, training loss: 0.14426133036613464 = 0.07618799060583115 + 0.01 * 6.807334899902344
Epoch 370, val loss: 0.7368270754814148
Epoch 380, training loss: 0.13578099012374878 = 0.06775390356779099 + 0.01 * 6.802708625793457
Epoch 380, val loss: 0.7486270070075989
Epoch 390, training loss: 0.1284635365009308 = 0.060453951358795166 + 0.01 * 6.800958156585693
Epoch 390, val loss: 0.7608556747436523
Epoch 400, training loss: 0.12205428630113602 = 0.05409805476665497 + 0.01 * 6.795623302459717
Epoch 400, val loss: 0.773141622543335
Epoch 410, training loss: 0.11649682372808456 = 0.04854735732078552 + 0.01 * 6.794947147369385
Epoch 410, val loss: 0.785672128200531
Epoch 420, training loss: 0.11154626309871674 = 0.043638940900564194 + 0.01 * 6.790731906890869
Epoch 420, val loss: 0.7982878088951111
Epoch 430, training loss: 0.10712677240371704 = 0.0392693392932415 + 0.01 * 6.785743713378906
Epoch 430, val loss: 0.8106382489204407
Epoch 440, training loss: 0.10328084230422974 = 0.03545708209276199 + 0.01 * 6.782376289367676
Epoch 440, val loss: 0.8232839703559875
Epoch 450, training loss: 0.10001156479120255 = 0.03215556591749191 + 0.01 * 6.785600185394287
Epoch 450, val loss: 0.8362138867378235
Epoch 460, training loss: 0.09705974906682968 = 0.02928760088980198 + 0.01 * 6.777215480804443
Epoch 460, val loss: 0.8491947650909424
Epoch 470, training loss: 0.09450148046016693 = 0.02678634040057659 + 0.01 * 6.771513938903809
Epoch 470, val loss: 0.8621353507041931
Epoch 480, training loss: 0.0923742726445198 = 0.024592282250523567 + 0.01 * 6.778199672698975
Epoch 480, val loss: 0.8749182224273682
Epoch 490, training loss: 0.09028557687997818 = 0.022662028670310974 + 0.01 * 6.762354850769043
Epoch 490, val loss: 0.8874719738960266
Epoch 500, training loss: 0.08855970948934555 = 0.020956529304385185 + 0.01 * 6.760318279266357
Epoch 500, val loss: 0.8997841477394104
Epoch 510, training loss: 0.08700063824653625 = 0.01944245956838131 + 0.01 * 6.755817890167236
Epoch 510, val loss: 0.9118123650550842
Epoch 520, training loss: 0.08558806777000427 = 0.01809169538319111 + 0.01 * 6.749637603759766
Epoch 520, val loss: 0.9235205054283142
Epoch 530, training loss: 0.08433422446250916 = 0.016880609095096588 + 0.01 * 6.745361804962158
Epoch 530, val loss: 0.9349951148033142
Epoch 540, training loss: 0.08321531116962433 = 0.015790391713380814 + 0.01 * 6.742491722106934
Epoch 540, val loss: 0.9461278319358826
Epoch 550, training loss: 0.08216467499732971 = 0.014804616570472717 + 0.01 * 6.736006259918213
Epoch 550, val loss: 0.9569254517555237
Epoch 560, training loss: 0.08122693002223969 = 0.013911186717450619 + 0.01 * 6.731574535369873
Epoch 560, val loss: 0.9674583077430725
Epoch 570, training loss: 0.08033579587936401 = 0.013099432922899723 + 0.01 * 6.723636627197266
Epoch 570, val loss: 0.9777095317840576
Epoch 580, training loss: 0.07980254292488098 = 0.0123606501147151 + 0.01 * 6.744189739227295
Epoch 580, val loss: 0.9876541495323181
Epoch 590, training loss: 0.07889805734157562 = 0.011686685495078564 + 0.01 * 6.721137523651123
Epoch 590, val loss: 0.9973597526550293
Epoch 600, training loss: 0.07819382846355438 = 0.011069586500525475 + 0.01 * 6.712423801422119
Epoch 600, val loss: 1.0068185329437256
Epoch 610, training loss: 0.07774795591831207 = 0.010502934455871582 + 0.01 * 6.7245025634765625
Epoch 610, val loss: 1.0159517526626587
Epoch 620, training loss: 0.07695093750953674 = 0.009982090443372726 + 0.01 * 6.696884632110596
Epoch 620, val loss: 1.0249487161636353
Epoch 630, training loss: 0.07649816572666168 = 0.009501677937805653 + 0.01 * 6.699648857116699
Epoch 630, val loss: 1.0336494445800781
Epoch 640, training loss: 0.07607094198465347 = 0.009057743474841118 + 0.01 * 6.701319694519043
Epoch 640, val loss: 1.0422319173812866
Epoch 650, training loss: 0.07549672573804855 = 0.008646471425890923 + 0.01 * 6.685025691986084
Epoch 650, val loss: 1.0505390167236328
Epoch 660, training loss: 0.07502760738134384 = 0.008264950476586819 + 0.01 * 6.676265716552734
Epoch 660, val loss: 1.0586543083190918
Epoch 670, training loss: 0.0749676525592804 = 0.007910281419754028 + 0.01 * 6.705737590789795
Epoch 670, val loss: 1.0665748119354248
Epoch 680, training loss: 0.07428723573684692 = 0.00757991336286068 + 0.01 * 6.670732021331787
Epoch 680, val loss: 1.0743111371994019
Epoch 690, training loss: 0.0739520713686943 = 0.007271964568644762 + 0.01 * 6.668010711669922
Epoch 690, val loss: 1.081844449043274
Epoch 700, training loss: 0.07358835637569427 = 0.006984058301895857 + 0.01 * 6.660429954528809
Epoch 700, val loss: 1.0892398357391357
Epoch 710, training loss: 0.07323174923658371 = 0.006714727263897657 + 0.01 * 6.651702880859375
Epoch 710, val loss: 1.0964226722717285
Epoch 720, training loss: 0.07306729257106781 = 0.006462167948484421 + 0.01 * 6.6605119705200195
Epoch 720, val loss: 1.103506326675415
Epoch 730, training loss: 0.0727047249674797 = 0.0062252008356153965 + 0.01 * 6.647952556610107
Epoch 730, val loss: 1.1104130744934082
Epoch 740, training loss: 0.07267358154058456 = 0.006002300884574652 + 0.01 * 6.667128562927246
Epoch 740, val loss: 1.117079496383667
Epoch 750, training loss: 0.0722898468375206 = 0.00579313887283206 + 0.01 * 6.6496710777282715
Epoch 750, val loss: 1.1237436532974243
Epoch 760, training loss: 0.07203396409749985 = 0.00559559790417552 + 0.01 * 6.6438374519348145
Epoch 760, val loss: 1.1301344633102417
Epoch 770, training loss: 0.0716959536075592 = 0.0054096803069114685 + 0.01 * 6.628627300262451
Epoch 770, val loss: 1.1365405321121216
Epoch 780, training loss: 0.07151348888874054 = 0.005233880132436752 + 0.01 * 6.6279616355896
Epoch 780, val loss: 1.1426562070846558
Epoch 790, training loss: 0.07138218730688095 = 0.005067488644272089 + 0.01 * 6.631470203399658
Epoch 790, val loss: 1.1487597227096558
Epoch 800, training loss: 0.07110851258039474 = 0.004910225514322519 + 0.01 * 6.619829177856445
Epoch 800, val loss: 1.1547281742095947
Epoch 810, training loss: 0.07096857577562332 = 0.004761175252497196 + 0.01 * 6.6207404136657715
Epoch 810, val loss: 1.1605430841445923
Epoch 820, training loss: 0.07071086019277573 = 0.004619653802365065 + 0.01 * 6.609120845794678
Epoch 820, val loss: 1.1662814617156982
Epoch 830, training loss: 0.07087753713130951 = 0.004485427401959896 + 0.01 * 6.6392107009887695
Epoch 830, val loss: 1.1718509197235107
Epoch 840, training loss: 0.07043236494064331 = 0.004357684403657913 + 0.01 * 6.607468605041504
Epoch 840, val loss: 1.177370548248291
Epoch 850, training loss: 0.0702202245593071 = 0.004236532375216484 + 0.01 * 6.598369121551514
Epoch 850, val loss: 1.182666301727295
Epoch 860, training loss: 0.07006274908781052 = 0.004121162462979555 + 0.01 * 6.594159126281738
Epoch 860, val loss: 1.188025951385498
Epoch 870, training loss: 0.07006611675024033 = 0.004011161159723997 + 0.01 * 6.605495452880859
Epoch 870, val loss: 1.1931045055389404
Epoch 880, training loss: 0.06994127482175827 = 0.003906554542481899 + 0.01 * 6.6034722328186035
Epoch 880, val loss: 1.19832444190979
Epoch 890, training loss: 0.06963179260492325 = 0.003806573571637273 + 0.01 * 6.582521438598633
Epoch 890, val loss: 1.203253149986267
Epoch 900, training loss: 0.06970628350973129 = 0.003711003577336669 + 0.01 * 6.599528789520264
Epoch 900, val loss: 1.2081388235092163
Epoch 910, training loss: 0.06944868713617325 = 0.0036199097521603107 + 0.01 * 6.582878112792969
Epoch 910, val loss: 1.2130268812179565
Epoch 920, training loss: 0.069345623254776 = 0.0035326748620718718 + 0.01 * 6.581294536590576
Epoch 920, val loss: 1.217726707458496
Epoch 930, training loss: 0.06915900111198425 = 0.003449158277362585 + 0.01 * 6.570984840393066
Epoch 930, val loss: 1.2224332094192505
Epoch 940, training loss: 0.06926145404577255 = 0.0033691313583403826 + 0.01 * 6.589232444763184
Epoch 940, val loss: 1.226980209350586
Epoch 950, training loss: 0.06898277997970581 = 0.0032926155254244804 + 0.01 * 6.569016933441162
Epoch 950, val loss: 1.231503963470459
Epoch 960, training loss: 0.0690523013472557 = 0.003219191450625658 + 0.01 * 6.583311080932617
Epoch 960, val loss: 1.2358261346817017
Epoch 970, training loss: 0.06884660571813583 = 0.0031488484237343073 + 0.01 * 6.5697760581970215
Epoch 970, val loss: 1.2403531074523926
Epoch 980, training loss: 0.06864510476589203 = 0.0030813126359134912 + 0.01 * 6.5563788414001465
Epoch 980, val loss: 1.2445720434188843
Epoch 990, training loss: 0.06869535148143768 = 0.003016343340277672 + 0.01 * 6.567900657653809
Epoch 990, val loss: 1.2488007545471191
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.5720
Flip ASR: 0.5067/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.017076253890991 = 1.9333378076553345 + 0.01 * 8.373844146728516
Epoch 0, val loss: 1.9333360195159912
Epoch 10, training loss: 2.006237268447876 = 1.9225001335144043 + 0.01 * 8.373720169067383
Epoch 10, val loss: 1.9215730428695679
Epoch 20, training loss: 1.9929908514022827 = 1.9092572927474976 + 0.01 * 8.373357772827148
Epoch 20, val loss: 1.9061657190322876
Epoch 30, training loss: 1.9748049974441528 = 1.8910801410675049 + 0.01 * 8.372483253479004
Epoch 30, val loss: 1.8842689990997314
Epoch 40, training loss: 1.9493764638900757 = 1.865686058998108 + 0.01 * 8.36904239654541
Epoch 40, val loss: 1.854143500328064
Epoch 50, training loss: 1.9156301021575928 = 1.8321518898010254 + 0.01 * 8.347824096679688
Epoch 50, val loss: 1.8173296451568604
Epoch 60, training loss: 1.8752472400665283 = 1.7931747436523438 + 0.01 * 8.207254409790039
Epoch 60, val loss: 1.7794618606567383
Epoch 70, training loss: 1.8313895463943481 = 1.7536835670471191 + 0.01 * 7.770601749420166
Epoch 70, val loss: 1.7451430559158325
Epoch 80, training loss: 1.7803056240081787 = 1.705713152885437 + 0.01 * 7.459251880645752
Epoch 80, val loss: 1.7038323879241943
Epoch 90, training loss: 1.710854172706604 = 1.637831687927246 + 0.01 * 7.302245616912842
Epoch 90, val loss: 1.6472021341323853
Epoch 100, training loss: 1.6179826259613037 = 1.5464755296707153 + 0.01 * 7.150713920593262
Epoch 100, val loss: 1.573153018951416
Epoch 110, training loss: 1.5111618041992188 = 1.4402966499328613 + 0.01 * 7.086513996124268
Epoch 110, val loss: 1.488864541053772
Epoch 120, training loss: 1.405280351638794 = 1.3347318172454834 + 0.01 * 7.0548505783081055
Epoch 120, val loss: 1.409346342086792
Epoch 130, training loss: 1.3092241287231445 = 1.2389390468597412 + 0.01 * 7.028512477874756
Epoch 130, val loss: 1.3420562744140625
Epoch 140, training loss: 1.223464012145996 = 1.1534228324890137 + 0.01 * 7.004116058349609
Epoch 140, val loss: 1.2844867706298828
Epoch 150, training loss: 1.1436797380447388 = 1.0738537311553955 + 0.01 * 6.982600688934326
Epoch 150, val loss: 1.2310993671417236
Epoch 160, training loss: 1.0660582780838013 = 0.996422290802002 + 0.01 * 6.963596820831299
Epoch 160, val loss: 1.1775825023651123
Epoch 170, training loss: 0.9892818927764893 = 0.9198272824287415 + 0.01 * 6.945462703704834
Epoch 170, val loss: 1.1236823797225952
Epoch 180, training loss: 0.9133195281028748 = 0.8440303206443787 + 0.01 * 6.9289231300354
Epoch 180, val loss: 1.0699690580368042
Epoch 190, training loss: 0.8378153443336487 = 0.7686811089515686 + 0.01 * 6.913421630859375
Epoch 190, val loss: 1.0171195268630981
Epoch 200, training loss: 0.7626184821128845 = 0.6936428546905518 + 0.01 * 6.897562503814697
Epoch 200, val loss: 0.966025710105896
Epoch 210, training loss: 0.6887227296829224 = 0.6198623180389404 + 0.01 * 6.886042594909668
Epoch 210, val loss: 0.91859370470047
Epoch 220, training loss: 0.6179932355880737 = 0.5492742657661438 + 0.01 * 6.871898651123047
Epoch 220, val loss: 0.877275824546814
Epoch 230, training loss: 0.5528724789619446 = 0.48425495624542236 + 0.01 * 6.861752986907959
Epoch 230, val loss: 0.843748152256012
Epoch 240, training loss: 0.4953615069389343 = 0.4267526865005493 + 0.01 * 6.860881328582764
Epoch 240, val loss: 0.8186697959899902
Epoch 250, training loss: 0.4459090232849121 = 0.3774828016757965 + 0.01 * 6.842623233795166
Epoch 250, val loss: 0.8015235662460327
Epoch 260, training loss: 0.4042949080467224 = 0.3359524607658386 + 0.01 * 6.834244728088379
Epoch 260, val loss: 0.7916620969772339
Epoch 270, training loss: 0.3692070245742798 = 0.30091995000839233 + 0.01 * 6.828708648681641
Epoch 270, val loss: 0.7880139350891113
Epoch 280, training loss: 0.3390458822250366 = 0.27085256576538086 + 0.01 * 6.81933069229126
Epoch 280, val loss: 0.7890937328338623
Epoch 290, training loss: 0.3124474883079529 = 0.24429500102996826 + 0.01 * 6.815248489379883
Epoch 290, val loss: 0.7933530807495117
Epoch 300, training loss: 0.28813016414642334 = 0.22005411982536316 + 0.01 * 6.807605266571045
Epoch 300, val loss: 0.7995374202728271
Epoch 310, training loss: 0.26538926362991333 = 0.19736382365226746 + 0.01 * 6.802544116973877
Epoch 310, val loss: 0.8067837953567505
Epoch 320, training loss: 0.24390646815299988 = 0.17590810358524323 + 0.01 * 6.799836158752441
Epoch 320, val loss: 0.8146620988845825
Epoch 330, training loss: 0.22360849380493164 = 0.15569815039634705 + 0.01 * 6.791035175323486
Epoch 330, val loss: 0.8230365514755249
Epoch 340, training loss: 0.20483729243278503 = 0.13698241114616394 + 0.01 * 6.785488605499268
Epoch 340, val loss: 0.8318488597869873
Epoch 350, training loss: 0.18787190318107605 = 0.12006237357854843 + 0.01 * 6.780953407287598
Epoch 350, val loss: 0.841193437576294
Epoch 360, training loss: 0.17295107245445251 = 0.10511404275894165 + 0.01 * 6.783702850341797
Epoch 360, val loss: 0.8510048985481262
Epoch 370, training loss: 0.1598614752292633 = 0.09212267398834229 + 0.01 * 6.7738800048828125
Epoch 370, val loss: 0.8612343072891235
Epoch 380, training loss: 0.1485998034477234 = 0.08093633502721786 + 0.01 * 6.766345977783203
Epoch 380, val loss: 0.8719750046730042
Epoch 390, training loss: 0.13896945118904114 = 0.07135654985904694 + 0.01 * 6.761291027069092
Epoch 390, val loss: 0.8831533193588257
Epoch 400, training loss: 0.1307707130908966 = 0.06317723542451859 + 0.01 * 6.7593488693237305
Epoch 400, val loss: 0.8948361873626709
Epoch 410, training loss: 0.12371497601270676 = 0.05620381981134415 + 0.01 * 6.751115798950195
Epoch 410, val loss: 0.9068842530250549
Epoch 420, training loss: 0.11774563789367676 = 0.05024429038167 + 0.01 * 6.7501349449157715
Epoch 420, val loss: 0.9192444086074829
Epoch 430, training loss: 0.11260063946247101 = 0.04513636231422424 + 0.01 * 6.7464280128479
Epoch 430, val loss: 0.9318479299545288
Epoch 440, training loss: 0.10810284316539764 = 0.04074104502797127 + 0.01 * 6.736179351806641
Epoch 440, val loss: 0.94468092918396
Epoch 450, training loss: 0.10428868979215622 = 0.03693925589323044 + 0.01 * 6.734943389892578
Epoch 450, val loss: 0.957629382610321
Epoch 460, training loss: 0.1009054183959961 = 0.03363296017050743 + 0.01 * 6.727245330810547
Epoch 460, val loss: 0.9707075357437134
Epoch 470, training loss: 0.09800595790147781 = 0.030742457136511803 + 0.01 * 6.7263503074646
Epoch 470, val loss: 0.9837654232978821
Epoch 480, training loss: 0.09544683247804642 = 0.02820271998643875 + 0.01 * 6.724411487579346
Epoch 480, val loss: 0.9969085454940796
Epoch 490, training loss: 0.09317665547132492 = 0.025959700345993042 + 0.01 * 6.721695899963379
Epoch 490, val loss: 1.0099064111709595
Epoch 500, training loss: 0.09103993326425552 = 0.023969056084752083 + 0.01 * 6.707087993621826
Epoch 500, val loss: 1.0227915048599243
Epoch 510, training loss: 0.08920452743768692 = 0.022195197641849518 + 0.01 * 6.70093297958374
Epoch 510, val loss: 1.0355451107025146
Epoch 520, training loss: 0.08768966794013977 = 0.02060787007212639 + 0.01 * 6.708179950714111
Epoch 520, val loss: 1.0480486154556274
Epoch 530, training loss: 0.08620220422744751 = 0.01918414607644081 + 0.01 * 6.70180606842041
Epoch 530, val loss: 1.0603975057601929
Epoch 540, training loss: 0.08478028327226639 = 0.01790112815797329 + 0.01 * 6.687915325164795
Epoch 540, val loss: 1.0724599361419678
Epoch 550, training loss: 0.08403444290161133 = 0.016741476953029633 + 0.01 * 6.729296684265137
Epoch 550, val loss: 1.0843145847320557
Epoch 560, training loss: 0.08254794031381607 = 0.015691572800278664 + 0.01 * 6.6856369972229
Epoch 560, val loss: 1.0959020853042603
Epoch 570, training loss: 0.08144685626029968 = 0.014739220030605793 + 0.01 * 6.6707634925842285
Epoch 570, val loss: 1.1071995496749878
Epoch 580, training loss: 0.08053737878799438 = 0.013871530070900917 + 0.01 * 6.6665849685668945
Epoch 580, val loss: 1.1182582378387451
Epoch 590, training loss: 0.07977545261383057 = 0.013078659772872925 + 0.01 * 6.669679641723633
Epoch 590, val loss: 1.1291086673736572
Epoch 600, training loss: 0.07911201566457748 = 0.012353434227406979 + 0.01 * 6.675858020782471
Epoch 600, val loss: 1.1396499872207642
Epoch 610, training loss: 0.07828252017498016 = 0.011688468046486378 + 0.01 * 6.659404754638672
Epoch 610, val loss: 1.1499468088150024
Epoch 620, training loss: 0.07757527381181717 = 0.011077660135924816 + 0.01 * 6.649762153625488
Epoch 620, val loss: 1.1599531173706055
Epoch 630, training loss: 0.07700783014297485 = 0.010514662601053715 + 0.01 * 6.649317264556885
Epoch 630, val loss: 1.169814944267273
Epoch 640, training loss: 0.0767705887556076 = 0.009995078667998314 + 0.01 * 6.67755126953125
Epoch 640, val loss: 1.1794476509094238
Epoch 650, training loss: 0.07595084607601166 = 0.009515766054391861 + 0.01 * 6.643507957458496
Epoch 650, val loss: 1.188791275024414
Epoch 660, training loss: 0.07546966522932053 = 0.009072120301425457 + 0.01 * 6.639754772186279
Epoch 660, val loss: 1.1979154348373413
Epoch 670, training loss: 0.07499224692583084 = 0.008659965358674526 + 0.01 * 6.633228302001953
Epoch 670, val loss: 1.2068015336990356
Epoch 680, training loss: 0.07456494122743607 = 0.008276858367025852 + 0.01 * 6.628808498382568
Epoch 680, val loss: 1.2155507802963257
Epoch 690, training loss: 0.07422766089439392 = 0.00792020931839943 + 0.01 * 6.630744934082031
Epoch 690, val loss: 1.2240843772888184
Epoch 700, training loss: 0.07390086352825165 = 0.0075875804759562016 + 0.01 * 6.631328582763672
Epoch 700, val loss: 1.2324411869049072
Epoch 710, training loss: 0.07348126918077469 = 0.007277089171111584 + 0.01 * 6.620418071746826
Epoch 710, val loss: 1.2405180931091309
Epoch 720, training loss: 0.07318073511123657 = 0.006986651569604874 + 0.01 * 6.619409084320068
Epoch 720, val loss: 1.248534083366394
Epoch 730, training loss: 0.07285359501838684 = 0.0067147668451070786 + 0.01 * 6.613883018493652
Epoch 730, val loss: 1.2562899589538574
Epoch 740, training loss: 0.0726081132888794 = 0.006459785625338554 + 0.01 * 6.614832878112793
Epoch 740, val loss: 1.2639102935791016
Epoch 750, training loss: 0.07231134176254272 = 0.006220586597919464 + 0.01 * 6.609076023101807
Epoch 750, val loss: 1.2713274955749512
Epoch 760, training loss: 0.07207679748535156 = 0.0059954700991511345 + 0.01 * 6.608132839202881
Epoch 760, val loss: 1.2786169052124023
Epoch 770, training loss: 0.07177229970693588 = 0.005783585831522942 + 0.01 * 6.598871231079102
Epoch 770, val loss: 1.285701870918274
Epoch 780, training loss: 0.07162754237651825 = 0.0055840094573795795 + 0.01 * 6.604353427886963
Epoch 780, val loss: 1.2926833629608154
Epoch 790, training loss: 0.07132662832736969 = 0.005395581945776939 + 0.01 * 6.593104839324951
Epoch 790, val loss: 1.2995291948318481
Epoch 800, training loss: 0.07121369242668152 = 0.005217809230089188 + 0.01 * 6.599587917327881
Epoch 800, val loss: 1.3061778545379639
Epoch 810, training loss: 0.07094695419073105 = 0.005050003994256258 + 0.01 * 6.589694976806641
Epoch 810, val loss: 1.3127508163452148
Epoch 820, training loss: 0.07079223543405533 = 0.004890905227512121 + 0.01 * 6.590133190155029
Epoch 820, val loss: 1.3190683126449585
Epoch 830, training loss: 0.07063836604356766 = 0.0047404090873897076 + 0.01 * 6.5897955894470215
Epoch 830, val loss: 1.3253874778747559
Epoch 840, training loss: 0.07040001451969147 = 0.004597466439008713 + 0.01 * 6.580254554748535
Epoch 840, val loss: 1.3315002918243408
Epoch 850, training loss: 0.07026253640651703 = 0.004461945034563541 + 0.01 * 6.580059051513672
Epoch 850, val loss: 1.3375564813613892
Epoch 860, training loss: 0.07005869597196579 = 0.004333221819251776 + 0.01 * 6.572547912597656
Epoch 860, val loss: 1.3434088230133057
Epoch 870, training loss: 0.07001662254333496 = 0.00421080831438303 + 0.01 * 6.5805816650390625
Epoch 870, val loss: 1.349178433418274
Epoch 880, training loss: 0.06976355612277985 = 0.004094335716217756 + 0.01 * 6.566922187805176
Epoch 880, val loss: 1.3548575639724731
Epoch 890, training loss: 0.06968743354082108 = 0.003983349539339542 + 0.01 * 6.570408821105957
Epoch 890, val loss: 1.3604214191436768
Epoch 900, training loss: 0.06956880539655685 = 0.003877521026879549 + 0.01 * 6.56912899017334
Epoch 900, val loss: 1.3658597469329834
Epoch 910, training loss: 0.06931120902299881 = 0.0037767798639833927 + 0.01 * 6.55344295501709
Epoch 910, val loss: 1.3712148666381836
Epoch 920, training loss: 0.06925427168607712 = 0.0036804305855184793 + 0.01 * 6.557384014129639
Epoch 920, val loss: 1.376407504081726
Epoch 930, training loss: 0.0691724568605423 = 0.0035886962432414293 + 0.01 * 6.558376312255859
Epoch 930, val loss: 1.3815909624099731
Epoch 940, training loss: 0.06904876232147217 = 0.0035007966216653585 + 0.01 * 6.5547966957092285
Epoch 940, val loss: 1.3865866661071777
Epoch 950, training loss: 0.06897503137588501 = 0.003416772000491619 + 0.01 * 6.555826663970947
Epoch 950, val loss: 1.391568899154663
Epoch 960, training loss: 0.06873266398906708 = 0.0033364237751811743 + 0.01 * 6.5396246910095215
Epoch 960, val loss: 1.3964120149612427
Epoch 970, training loss: 0.06868485361337662 = 0.003259341698139906 + 0.01 * 6.5425519943237305
Epoch 970, val loss: 1.4012137651443481
Epoch 980, training loss: 0.0685601755976677 = 0.003185562090948224 + 0.01 * 6.537461280822754
Epoch 980, val loss: 1.4058440923690796
Epoch 990, training loss: 0.06862541288137436 = 0.003114714054390788 + 0.01 * 6.551070213317871
Epoch 990, val loss: 1.4104253053665161
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7741
Overall ASR: 0.7454
Flip ASR: 0.7289/225 nodes
The final ASR:0.62854, 0.08264, Accuracy:0.79753, 0.01720
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11604])
remove edge: torch.Size([2, 9476])
updated graph: torch.Size([2, 10524])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.98032, 0.00348, Accuracy:0.83333, 0.00524
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0431299209594727 = 1.9593909978866577 + 0.01 * 8.373900413513184
Epoch 0, val loss: 1.965760588645935
Epoch 10, training loss: 2.033535957336426 = 1.9497977495193481 + 0.01 * 8.373832702636719
Epoch 10, val loss: 1.956518530845642
Epoch 20, training loss: 2.0215659141540527 = 1.9378299713134766 + 0.01 * 8.373592376708984
Epoch 20, val loss: 1.9444894790649414
Epoch 30, training loss: 2.004317283630371 = 1.9205889701843262 + 0.01 * 8.372835159301758
Epoch 30, val loss: 1.926741123199463
Epoch 40, training loss: 1.9782524108886719 = 1.8945643901824951 + 0.01 * 8.368803977966309
Epoch 40, val loss: 1.8999437093734741
Epoch 50, training loss: 1.940168857574463 = 1.8567924499511719 + 0.01 * 8.337644577026367
Epoch 50, val loss: 1.8621171712875366
Epoch 60, training loss: 1.891631841659546 = 1.81010103225708 + 0.01 * 8.153075218200684
Epoch 60, val loss: 1.8188437223434448
Epoch 70, training loss: 1.8439345359802246 = 1.7653230428695679 + 0.01 * 7.861152172088623
Epoch 70, val loss: 1.7815182209014893
Epoch 80, training loss: 1.7918134927749634 = 1.717383861541748 + 0.01 * 7.442966461181641
Epoch 80, val loss: 1.7402743101119995
Epoch 90, training loss: 1.7257351875305176 = 1.6536478996276855 + 0.01 * 7.208728313446045
Epoch 90, val loss: 1.683931827545166
Epoch 100, training loss: 1.6413755416870117 = 1.5701717138290405 + 0.01 * 7.12038516998291
Epoch 100, val loss: 1.6112747192382812
Epoch 110, training loss: 1.53924560546875 = 1.4686188697814941 + 0.01 * 7.0626726150512695
Epoch 110, val loss: 1.52568781375885
Epoch 120, training loss: 1.4273003339767456 = 1.3570252656936646 + 0.01 * 7.027512073516846
Epoch 120, val loss: 1.4349863529205322
Epoch 130, training loss: 1.3132009506225586 = 1.2431278228759766 + 0.01 * 7.007312774658203
Epoch 130, val loss: 1.3439918756484985
Epoch 140, training loss: 1.2010776996612549 = 1.131151795387268 + 0.01 * 6.992596626281738
Epoch 140, val loss: 1.256173849105835
Epoch 150, training loss: 1.094319462776184 = 1.024531602859497 + 0.01 * 6.978780746459961
Epoch 150, val loss: 1.1738132238388062
Epoch 160, training loss: 0.9954453110694885 = 0.9257821440696716 + 0.01 * 6.966315269470215
Epoch 160, val loss: 1.0984224081039429
Epoch 170, training loss: 0.9052927494049072 = 0.8357048034667969 + 0.01 * 6.958795070648193
Epoch 170, val loss: 1.0313457250595093
Epoch 180, training loss: 0.8231260776519775 = 0.7535539269447327 + 0.01 * 6.957215309143066
Epoch 180, val loss: 0.9723418354988098
Epoch 190, training loss: 0.7478737235069275 = 0.6782905459403992 + 0.01 * 6.958317756652832
Epoch 190, val loss: 0.92058265209198
Epoch 200, training loss: 0.6791629791259766 = 0.6095696091651917 + 0.01 * 6.9593353271484375
Epoch 200, val loss: 0.8757481575012207
Epoch 210, training loss: 0.6170058250427246 = 0.5474086999893188 + 0.01 * 6.959712505340576
Epoch 210, val loss: 0.8378443121910095
Epoch 220, training loss: 0.5613667368888855 = 0.4917730987071991 + 0.01 * 6.959364414215088
Epoch 220, val loss: 0.8067592978477478
Epoch 230, training loss: 0.5119497776031494 = 0.4423671364784241 + 0.01 * 6.9582624435424805
Epoch 230, val loss: 0.7820993661880493
Epoch 240, training loss: 0.46818119287490845 = 0.398614764213562 + 0.01 * 6.9566426277160645
Epoch 240, val loss: 0.7631829380989075
Epoch 250, training loss: 0.42924171686172485 = 0.3596959710121155 + 0.01 * 6.954573154449463
Epoch 250, val loss: 0.7487357258796692
Epoch 260, training loss: 0.3941808044910431 = 0.3246675133705139 + 0.01 * 6.951330184936523
Epoch 260, val loss: 0.737701952457428
Epoch 270, training loss: 0.362050861120224 = 0.2925749719142914 + 0.01 * 6.947587966918945
Epoch 270, val loss: 0.7295401096343994
Epoch 280, training loss: 0.3320026397705078 = 0.26258283853530884 + 0.01 * 6.941978454589844
Epoch 280, val loss: 0.7239476442337036
Epoch 290, training loss: 0.30346107482910156 = 0.2341257482767105 + 0.01 * 6.933533191680908
Epoch 290, val loss: 0.7208624482154846
Epoch 300, training loss: 0.27625855803489685 = 0.20702478289604187 + 0.01 * 6.923376560211182
Epoch 300, val loss: 0.7203440070152283
Epoch 310, training loss: 0.2506735026836395 = 0.181540846824646 + 0.01 * 6.913265228271484
Epoch 310, val loss: 0.7225598096847534
Epoch 320, training loss: 0.22714880108833313 = 0.1582023799419403 + 0.01 * 6.894641399383545
Epoch 320, val loss: 0.7277102470397949
Epoch 330, training loss: 0.2063983976840973 = 0.1374703198671341 + 0.01 * 6.892807960510254
Epoch 330, val loss: 0.7356090545654297
Epoch 340, training loss: 0.18815648555755615 = 0.11952169239521027 + 0.01 * 6.863478660583496
Epoch 340, val loss: 0.7460740804672241
Epoch 350, training loss: 0.17271098494529724 = 0.10421797633171082 + 0.01 * 6.849301815032959
Epoch 350, val loss: 0.7584810853004456
Epoch 360, training loss: 0.159636989235878 = 0.09127039462327957 + 0.01 * 6.836659908294678
Epoch 360, val loss: 0.772253155708313
Epoch 370, training loss: 0.14851056039333344 = 0.08032209426164627 + 0.01 * 6.818846702575684
Epoch 370, val loss: 0.7866155505180359
Epoch 380, training loss: 0.1391621083021164 = 0.07105039060115814 + 0.01 * 6.811172008514404
Epoch 380, val loss: 0.8014704585075378
Epoch 390, training loss: 0.13116982579231262 = 0.06318441778421402 + 0.01 * 6.79854154586792
Epoch 390, val loss: 0.8164321184158325
Epoch 400, training loss: 0.12440043687820435 = 0.05648185685276985 + 0.01 * 6.791858196258545
Epoch 400, val loss: 0.8311709761619568
Epoch 410, training loss: 0.11856263875961304 = 0.050735585391521454 + 0.01 * 6.782705307006836
Epoch 410, val loss: 0.8458303809165955
Epoch 420, training loss: 0.11352089047431946 = 0.045784685760736465 + 0.01 * 6.773620128631592
Epoch 420, val loss: 0.8602243065834045
Epoch 430, training loss: 0.10910509526729584 = 0.041491519659757614 + 0.01 * 6.761358261108398
Epoch 430, val loss: 0.8743422031402588
Epoch 440, training loss: 0.10529409348964691 = 0.037749093025922775 + 0.01 * 6.754500389099121
Epoch 440, val loss: 0.8881136178970337
Epoch 450, training loss: 0.10192326456308365 = 0.03447275608778 + 0.01 * 6.74505090713501
Epoch 450, val loss: 0.9015940427780151
Epoch 460, training loss: 0.09909556061029434 = 0.03158953785896301 + 0.01 * 6.750602722167969
Epoch 460, val loss: 0.9147258400917053
Epoch 470, training loss: 0.09639687836170197 = 0.02904265746474266 + 0.01 * 6.735422134399414
Epoch 470, val loss: 0.927476704120636
Epoch 480, training loss: 0.09417708218097687 = 0.026782233268022537 + 0.01 * 6.7394843101501465
Epoch 480, val loss: 0.9399152994155884
Epoch 490, training loss: 0.09202856570482254 = 0.024769581854343414 + 0.01 * 6.725898265838623
Epoch 490, val loss: 0.95197594165802
Epoch 500, training loss: 0.09010101854801178 = 0.02297103777527809 + 0.01 * 6.712998390197754
Epoch 500, val loss: 0.9637423753738403
Epoch 510, training loss: 0.08846377581357956 = 0.02135763317346573 + 0.01 * 6.710614204406738
Epoch 510, val loss: 0.9751208424568176
Epoch 520, training loss: 0.08692390471696854 = 0.019905686378479004 + 0.01 * 6.701821804046631
Epoch 520, val loss: 0.9862005114555359
Epoch 530, training loss: 0.08589203655719757 = 0.0185944065451622 + 0.01 * 6.729763031005859
Epoch 530, val loss: 0.9969439506530762
Epoch 540, training loss: 0.08434048295021057 = 0.017409514635801315 + 0.01 * 6.693097114562988
Epoch 540, val loss: 1.0073744058609009
Epoch 550, training loss: 0.08316174894571304 = 0.016334056854248047 + 0.01 * 6.682769298553467
Epoch 550, val loss: 1.0174895524978638
Epoch 560, training loss: 0.08228718489408493 = 0.015354703180491924 + 0.01 * 6.693248271942139
Epoch 560, val loss: 1.027363657951355
Epoch 570, training loss: 0.08126027882099152 = 0.01446166355162859 + 0.01 * 6.679861545562744
Epoch 570, val loss: 1.0368750095367432
Epoch 580, training loss: 0.08047132194042206 = 0.013644916005432606 + 0.01 * 6.682640552520752
Epoch 580, val loss: 1.0462549924850464
Epoch 590, training loss: 0.07958898693323135 = 0.0128964614123106 + 0.01 * 6.669252872467041
Epoch 590, val loss: 1.0552701950073242
Epoch 600, training loss: 0.07885149866342545 = 0.012208717875182629 + 0.01 * 6.664278507232666
Epoch 600, val loss: 1.0640852451324463
Epoch 610, training loss: 0.07829704880714417 = 0.011576416902244091 + 0.01 * 6.672063827514648
Epoch 610, val loss: 1.072711706161499
Epoch 620, training loss: 0.07757396250963211 = 0.010994006879627705 + 0.01 * 6.657995700836182
Epoch 620, val loss: 1.0810407400131226
Epoch 630, training loss: 0.07694573700428009 = 0.010455545037984848 + 0.01 * 6.64901876449585
Epoch 630, val loss: 1.0891636610031128
Epoch 640, training loss: 0.07632794976234436 = 0.009957071393728256 + 0.01 * 6.637087345123291
Epoch 640, val loss: 1.0970927476882935
Epoch 650, training loss: 0.0761159136891365 = 0.009494727477431297 + 0.01 * 6.662118911743164
Epoch 650, val loss: 1.104879379272461
Epoch 660, training loss: 0.07545563578605652 = 0.00906546600162983 + 0.01 * 6.639017581939697
Epoch 660, val loss: 1.11237370967865
Epoch 670, training loss: 0.07500982284545898 = 0.008666370064020157 + 0.01 * 6.634345531463623
Epoch 670, val loss: 1.1197607517242432
Epoch 680, training loss: 0.07452752441167831 = 0.008294600062072277 + 0.01 * 6.623292922973633
Epoch 680, val loss: 1.1269463300704956
Epoch 690, training loss: 0.0741603821516037 = 0.007947396486997604 + 0.01 * 6.6212992668151855
Epoch 690, val loss: 1.1339340209960938
Epoch 700, training loss: 0.07391487061977386 = 0.007623311132192612 + 0.01 * 6.629156112670898
Epoch 700, val loss: 1.1408532857894897
Epoch 710, training loss: 0.07342362403869629 = 0.007320364471524954 + 0.01 * 6.610325813293457
Epoch 710, val loss: 1.1475179195404053
Epoch 720, training loss: 0.07320891320705414 = 0.0070363241247832775 + 0.01 * 6.617259502410889
Epoch 720, val loss: 1.1540963649749756
Epoch 730, training loss: 0.07281781733036041 = 0.006769801955670118 + 0.01 * 6.604801654815674
Epoch 730, val loss: 1.1604183912277222
Epoch 740, training loss: 0.0724511444568634 = 0.006519448943436146 + 0.01 * 6.593169689178467
Epoch 740, val loss: 1.166722297668457
Epoch 750, training loss: 0.07235026359558105 = 0.006283760070800781 + 0.01 * 6.606650352478027
Epoch 750, val loss: 1.1728758811950684
Epoch 760, training loss: 0.07197089493274689 = 0.006061982363462448 + 0.01 * 6.5908918380737305
Epoch 760, val loss: 1.1788777112960815
Epoch 770, training loss: 0.07183351367712021 = 0.005852941889315844 + 0.01 * 6.598056793212891
Epoch 770, val loss: 1.184712529182434
Epoch 780, training loss: 0.07154542207717896 = 0.005655832123011351 + 0.01 * 6.588959217071533
Epoch 780, val loss: 1.1904455423355103
Epoch 790, training loss: 0.07134012132883072 = 0.005469480529427528 + 0.01 * 6.587064743041992
Epoch 790, val loss: 1.196040391921997
Epoch 800, training loss: 0.07100240141153336 = 0.005293247289955616 + 0.01 * 6.570915222167969
Epoch 800, val loss: 1.2015674114227295
Epoch 810, training loss: 0.07081052660942078 = 0.005126211326569319 + 0.01 * 6.568432331085205
Epoch 810, val loss: 1.206999659538269
Epoch 820, training loss: 0.0710192546248436 = 0.004968076478689909 + 0.01 * 6.6051177978515625
Epoch 820, val loss: 1.212312936782837
Epoch 830, training loss: 0.07062339782714844 = 0.004818174988031387 + 0.01 * 6.5805230140686035
Epoch 830, val loss: 1.2174242734909058
Epoch 840, training loss: 0.07046134024858475 = 0.004676009528338909 + 0.01 * 6.578533172607422
Epoch 840, val loss: 1.2225308418273926
Epoch 850, training loss: 0.07012059539556503 = 0.004540908616036177 + 0.01 * 6.557969093322754
Epoch 850, val loss: 1.2274887561798096
Epoch 860, training loss: 0.069939985871315 = 0.004412316717207432 + 0.01 * 6.552767276763916
Epoch 860, val loss: 1.232373833656311
Epoch 870, training loss: 0.06982553005218506 = 0.0042901118285954 + 0.01 * 6.553541660308838
Epoch 870, val loss: 1.2371641397476196
Epoch 880, training loss: 0.0698489248752594 = 0.004173696972429752 + 0.01 * 6.567523002624512
Epoch 880, val loss: 1.2418862581253052
Epoch 890, training loss: 0.06947017461061478 = 0.004062681458890438 + 0.01 * 6.540749549865723
Epoch 890, val loss: 1.2464470863342285
Epoch 900, training loss: 0.0694403350353241 = 0.003956765867769718 + 0.01 * 6.5483574867248535
Epoch 900, val loss: 1.2509615421295166
Epoch 910, training loss: 0.06921382248401642 = 0.003855657996609807 + 0.01 * 6.535816669464111
Epoch 910, val loss: 1.2553397417068481
Epoch 920, training loss: 0.06926219165325165 = 0.0037590465508401394 + 0.01 * 6.550314426422119
Epoch 920, val loss: 1.259669303894043
Epoch 930, training loss: 0.06901554763317108 = 0.003666736651211977 + 0.01 * 6.534881114959717
Epoch 930, val loss: 1.2639188766479492
Epoch 940, training loss: 0.0688537135720253 = 0.0035785583313554525 + 0.01 * 6.527515888214111
Epoch 940, val loss: 1.2680985927581787
Epoch 950, training loss: 0.06879054009914398 = 0.0034940335899591446 + 0.01 * 6.529650688171387
Epoch 950, val loss: 1.2721699476242065
Epoch 960, training loss: 0.0687379390001297 = 0.0034131589345633984 + 0.01 * 6.5324788093566895
Epoch 960, val loss: 1.276141881942749
Epoch 970, training loss: 0.06865963339805603 = 0.0033356528729200363 + 0.01 * 6.532398223876953
Epoch 970, val loss: 1.2801380157470703
Epoch 980, training loss: 0.06856095045804977 = 0.0032613719813525677 + 0.01 * 6.529958248138428
Epoch 980, val loss: 1.283995509147644
Epoch 990, training loss: 0.06844186037778854 = 0.0031900182366371155 + 0.01 * 6.525184154510498
Epoch 990, val loss: 1.2877495288848877
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.6568
Flip ASR: 0.5911/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.031925678253174 = 1.9481867551803589 + 0.01 * 8.373892784118652
Epoch 0, val loss: 1.9519633054733276
Epoch 10, training loss: 2.021610975265503 = 1.9378728866577148 + 0.01 * 8.373814582824707
Epoch 10, val loss: 1.9412806034088135
Epoch 20, training loss: 2.0090272426605225 = 1.925291895866394 + 0.01 * 8.373543739318848
Epoch 20, val loss: 1.9277174472808838
Epoch 30, training loss: 1.991236686706543 = 1.9075095653533936 + 0.01 * 8.372715950012207
Epoch 30, val loss: 1.9082732200622559
Epoch 40, training loss: 1.964874029159546 = 1.8811888694763184 + 0.01 * 8.368520736694336
Epoch 40, val loss: 1.8798810243606567
Epoch 50, training loss: 1.927331566810608 = 1.8439416885375977 + 0.01 * 8.3389892578125
Epoch 50, val loss: 1.8410812616348267
Epoch 60, training loss: 1.88058602809906 = 1.7987499237060547 + 0.01 * 8.183613777160645
Epoch 60, val loss: 1.7968308925628662
Epoch 70, training loss: 1.8328980207443237 = 1.7537219524383545 + 0.01 * 7.91760778427124
Epoch 70, val loss: 1.7541173696517944
Epoch 80, training loss: 1.7779146432876587 = 1.700778603553772 + 0.01 * 7.713599681854248
Epoch 80, val loss: 1.7041767835617065
Epoch 90, training loss: 1.7041239738464355 = 1.6289645433425903 + 0.01 * 7.5159454345703125
Epoch 90, val loss: 1.6412720680236816
Epoch 100, training loss: 1.6098629236221313 = 1.5360631942749023 + 0.01 * 7.379973411560059
Epoch 100, val loss: 1.5647852420806885
Epoch 110, training loss: 1.5019389390945435 = 1.4292744398117065 + 0.01 * 7.266454219818115
Epoch 110, val loss: 1.4806550741195679
Epoch 120, training loss: 1.3937647342681885 = 1.3217533826828003 + 0.01 * 7.20112943649292
Epoch 120, val loss: 1.400020718574524
Epoch 130, training loss: 1.292803406715393 = 1.2213821411132812 + 0.01 * 7.142125606536865
Epoch 130, val loss: 1.3285527229309082
Epoch 140, training loss: 1.2012717723846436 = 1.1302560567855835 + 0.01 * 7.101569175720215
Epoch 140, val loss: 1.2664607763290405
Epoch 150, training loss: 1.119899034500122 = 1.0491341352462769 + 0.01 * 7.076490879058838
Epoch 150, val loss: 1.2118115425109863
Epoch 160, training loss: 1.0485727787017822 = 0.9779582619667053 + 0.01 * 7.061452388763428
Epoch 160, val loss: 1.1641055345535278
Epoch 170, training loss: 0.9852502942085266 = 0.9147800207138062 + 0.01 * 7.047025680541992
Epoch 170, val loss: 1.1213412284851074
Epoch 180, training loss: 0.926540195941925 = 0.8562355637550354 + 0.01 * 7.030465602874756
Epoch 180, val loss: 1.0817853212356567
Epoch 190, training loss: 0.8688021302223206 = 0.7986888289451599 + 0.01 * 7.011332035064697
Epoch 190, val loss: 1.043206810951233
Epoch 200, training loss: 0.8097521662712097 = 0.7398499846458435 + 0.01 * 6.9902191162109375
Epoch 200, val loss: 1.003922462463379
Epoch 210, training loss: 0.7492512464523315 = 0.6795302033424377 + 0.01 * 6.972107410430908
Epoch 210, val loss: 0.9633042216300964
Epoch 220, training loss: 0.6887600421905518 = 0.6191681623458862 + 0.01 * 6.959188461303711
Epoch 220, val loss: 0.9228913187980652
Epoch 230, training loss: 0.6299731731414795 = 0.560488224029541 + 0.01 * 6.948493003845215
Epoch 230, val loss: 0.884788453578949
Epoch 240, training loss: 0.5742468237876892 = 0.5048446655273438 + 0.01 * 6.940217018127441
Epoch 240, val loss: 0.8513743281364441
Epoch 250, training loss: 0.5225099921226501 = 0.45314648747444153 + 0.01 * 6.936351299285889
Epoch 250, val loss: 0.824003279209137
Epoch 260, training loss: 0.47515931725502014 = 0.40584632754325867 + 0.01 * 6.931299686431885
Epoch 260, val loss: 0.8031675219535828
Epoch 270, training loss: 0.4322274625301361 = 0.3629496395587921 + 0.01 * 6.9277825355529785
Epoch 270, val loss: 0.7887186408042908
Epoch 280, training loss: 0.3934638798236847 = 0.3242095112800598 + 0.01 * 6.925436019897461
Epoch 280, val loss: 0.7801727056503296
Epoch 290, training loss: 0.3584843575954437 = 0.28925445675849915 + 0.01 * 6.922989845275879
Epoch 290, val loss: 0.7769301533699036
Epoch 300, training loss: 0.3269992768764496 = 0.25778961181640625 + 0.01 * 6.920966148376465
Epoch 300, val loss: 0.7781196236610413
Epoch 310, training loss: 0.2988089621067047 = 0.22961795330047607 + 0.01 * 6.919100761413574
Epoch 310, val loss: 0.7831063866615295
Epoch 320, training loss: 0.27366626262664795 = 0.20449520647525787 + 0.01 * 6.917105197906494
Epoch 320, val loss: 0.7909594774246216
Epoch 330, training loss: 0.25153231620788574 = 0.18239149451255798 + 0.01 * 6.914083003997803
Epoch 330, val loss: 0.8013349175453186
Epoch 340, training loss: 0.23206910490989685 = 0.16291411221027374 + 0.01 * 6.915499210357666
Epoch 340, val loss: 0.8136391639709473
Epoch 350, training loss: 0.21485267579555511 = 0.1457403004169464 + 0.01 * 6.911237716674805
Epoch 350, val loss: 0.8269016146659851
Epoch 360, training loss: 0.19964231550693512 = 0.13058146834373474 + 0.01 * 6.906085014343262
Epoch 360, val loss: 0.8407391309738159
Epoch 370, training loss: 0.1862284541130066 = 0.11720751225948334 + 0.01 * 6.902093887329102
Epoch 370, val loss: 0.8546545505523682
Epoch 380, training loss: 0.17439818382263184 = 0.10541681945323944 + 0.01 * 6.898135662078857
Epoch 380, val loss: 0.8683599829673767
Epoch 390, training loss: 0.1640179604291916 = 0.09505348652601242 + 0.01 * 6.89644718170166
Epoch 390, val loss: 0.8819822072982788
Epoch 400, training loss: 0.15484116971492767 = 0.08596325665712357 + 0.01 * 6.887791633605957
Epoch 400, val loss: 0.895123302936554
Epoch 410, training loss: 0.1468624770641327 = 0.07797709107398987 + 0.01 * 6.888538360595703
Epoch 410, val loss: 0.9081104397773743
Epoch 420, training loss: 0.1396653652191162 = 0.07096422463655472 + 0.01 * 6.870113849639893
Epoch 420, val loss: 0.9205162525177002
Epoch 430, training loss: 0.13363045454025269 = 0.06481995433568954 + 0.01 * 6.881049633026123
Epoch 430, val loss: 0.9326667785644531
Epoch 440, training loss: 0.12798339128494263 = 0.059439320117235184 + 0.01 * 6.854406833648682
Epoch 440, val loss: 0.9443367123603821
Epoch 450, training loss: 0.12315380573272705 = 0.054715827107429504 + 0.01 * 6.8437981605529785
Epoch 450, val loss: 0.9559492468833923
Epoch 460, training loss: 0.11885404586791992 = 0.05055723339319229 + 0.01 * 6.829681396484375
Epoch 460, val loss: 0.9672103524208069
Epoch 470, training loss: 0.11526966094970703 = 0.04690132662653923 + 0.01 * 6.836833953857422
Epoch 470, val loss: 0.9780031442642212
Epoch 480, training loss: 0.11183822900056839 = 0.0436871200799942 + 0.01 * 6.815110683441162
Epoch 480, val loss: 0.9884788393974304
Epoch 490, training loss: 0.10891076922416687 = 0.04083437845110893 + 0.01 * 6.807638645172119
Epoch 490, val loss: 0.9990115761756897
Epoch 500, training loss: 0.10624000430107117 = 0.038277801126241684 + 0.01 * 6.796220779418945
Epoch 500, val loss: 1.0093936920166016
Epoch 510, training loss: 0.10385555773973465 = 0.03598412871360779 + 0.01 * 6.787143230438232
Epoch 510, val loss: 1.0193599462509155
Epoch 520, training loss: 0.10171271860599518 = 0.03391167148947716 + 0.01 * 6.780104160308838
Epoch 520, val loss: 1.0290181636810303
Epoch 530, training loss: 0.09985572099685669 = 0.03201926872134209 + 0.01 * 6.783644676208496
Epoch 530, val loss: 1.0385429859161377
Epoch 540, training loss: 0.09799361228942871 = 0.03028825670480728 + 0.01 * 6.770535945892334
Epoch 540, val loss: 1.0477293729782104
Epoch 550, training loss: 0.096433624625206 = 0.028691943734884262 + 0.01 * 6.774167537689209
Epoch 550, val loss: 1.0566720962524414
Epoch 560, training loss: 0.09479433298110962 = 0.027202891185879707 + 0.01 * 6.759144306182861
Epoch 560, val loss: 1.0656744241714478
Epoch 570, training loss: 0.09329728782176971 = 0.02580612525343895 + 0.01 * 6.74911642074585
Epoch 570, val loss: 1.0743411779403687
Epoch 580, training loss: 0.09207207709550858 = 0.024486660957336426 + 0.01 * 6.758542060852051
Epoch 580, val loss: 1.0830260515213013
Epoch 590, training loss: 0.09071118384599686 = 0.023234600201249123 + 0.01 * 6.747658729553223
Epoch 590, val loss: 1.0915364027023315
Epoch 600, training loss: 0.08937235176563263 = 0.02203797549009323 + 0.01 * 6.733437538146973
Epoch 600, val loss: 1.0999492406845093
Epoch 610, training loss: 0.08817735314369202 = 0.02088814228773117 + 0.01 * 6.728921413421631
Epoch 610, val loss: 1.1081960201263428
Epoch 620, training loss: 0.08694963157176971 = 0.019780989736318588 + 0.01 * 6.716863632202148
Epoch 620, val loss: 1.1163296699523926
Epoch 630, training loss: 0.0860934779047966 = 0.01871442049741745 + 0.01 * 6.737905979156494
Epoch 630, val loss: 1.1243746280670166
Epoch 640, training loss: 0.08485347032546997 = 0.01769137755036354 + 0.01 * 6.7162089347839355
Epoch 640, val loss: 1.132495641708374
Epoch 650, training loss: 0.08376936614513397 = 0.016720352694392204 + 0.01 * 6.704901218414307
Epoch 650, val loss: 1.1406031847000122
Epoch 660, training loss: 0.08303773403167725 = 0.01580946519970894 + 0.01 * 6.722826957702637
Epoch 660, val loss: 1.1489455699920654
Epoch 670, training loss: 0.08210025727748871 = 0.01496835146099329 + 0.01 * 6.713191032409668
Epoch 670, val loss: 1.1570764780044556
Epoch 680, training loss: 0.08107109367847443 = 0.014193486422300339 + 0.01 * 6.687760353088379
Epoch 680, val loss: 1.1653755903244019
Epoch 690, training loss: 0.080462247133255 = 0.013480369932949543 + 0.01 * 6.698187351226807
Epoch 690, val loss: 1.1734882593154907
Epoch 700, training loss: 0.07960516959428787 = 0.012816248461604118 + 0.01 * 6.678892135620117
Epoch 700, val loss: 1.1812999248504639
Epoch 710, training loss: 0.07879387587308884 = 0.012198814190924168 + 0.01 * 6.659506320953369
Epoch 710, val loss: 1.1893200874328613
Epoch 720, training loss: 0.07834241539239883 = 0.011622428894042969 + 0.01 * 6.671998977661133
Epoch 720, val loss: 1.1970428228378296
Epoch 730, training loss: 0.07776442915201187 = 0.011084331199526787 + 0.01 * 6.6680097579956055
Epoch 730, val loss: 1.204520583152771
Epoch 740, training loss: 0.07738129794597626 = 0.010580549947917461 + 0.01 * 6.680075168609619
Epoch 740, val loss: 1.2118377685546875
Epoch 750, training loss: 0.0766817107796669 = 0.010103569366037846 + 0.01 * 6.6578145027160645
Epoch 750, val loss: 1.2189980745315552
Epoch 760, training loss: 0.07606346160173416 = 0.00965164415538311 + 0.01 * 6.641181945800781
Epoch 760, val loss: 1.2257148027420044
Epoch 770, training loss: 0.07549947500228882 = 0.009216376580297947 + 0.01 * 6.628310203552246
Epoch 770, val loss: 1.2322908639907837
Epoch 780, training loss: 0.0753583088517189 = 0.008802161552011967 + 0.01 * 6.655614852905273
Epoch 780, val loss: 1.2388111352920532
Epoch 790, training loss: 0.07459317892789841 = 0.008405406959354877 + 0.01 * 6.618777275085449
Epoch 790, val loss: 1.245052456855774
Epoch 800, training loss: 0.07426802814006805 = 0.008026682771742344 + 0.01 * 6.624134540557861
Epoch 800, val loss: 1.2512136697769165
Epoch 810, training loss: 0.07402925938367844 = 0.007662782911211252 + 0.01 * 6.636648178100586
Epoch 810, val loss: 1.2569383382797241
Epoch 820, training loss: 0.07369866222143173 = 0.007312637288123369 + 0.01 * 6.638602256774902
Epoch 820, val loss: 1.2626010179519653
Epoch 830, training loss: 0.07310949265956879 = 0.006974236574023962 + 0.01 * 6.613525867462158
Epoch 830, val loss: 1.2680931091308594
Epoch 840, training loss: 0.07259165495634079 = 0.0066431001760065556 + 0.01 * 6.594855785369873
Epoch 840, val loss: 1.2735016345977783
Epoch 850, training loss: 0.07247824221849442 = 0.006323273293673992 + 0.01 * 6.615497589111328
Epoch 850, val loss: 1.278673529624939
Epoch 860, training loss: 0.0719791054725647 = 0.006014513783156872 + 0.01 * 6.596459865570068
Epoch 860, val loss: 1.2833843231201172
Epoch 870, training loss: 0.07164417207241058 = 0.005719398148357868 + 0.01 * 6.592477321624756
Epoch 870, val loss: 1.288222074508667
Epoch 880, training loss: 0.07123593240976334 = 0.005436888430267572 + 0.01 * 6.579904079437256
Epoch 880, val loss: 1.2929831743240356
Epoch 890, training loss: 0.07082010805606842 = 0.005167745519429445 + 0.01 * 6.5652360916137695
Epoch 890, val loss: 1.2973493337631226
Epoch 900, training loss: 0.07044528424739838 = 0.004915246739983559 + 0.01 * 6.553003787994385
Epoch 900, val loss: 1.3017064332962036
Epoch 910, training loss: 0.07122882455587387 = 0.004683361854404211 + 0.01 * 6.65454626083374
Epoch 910, val loss: 1.306033968925476
Epoch 920, training loss: 0.07008815556764603 = 0.004469581879675388 + 0.01 * 6.5618577003479
Epoch 920, val loss: 1.3101904392242432
Epoch 930, training loss: 0.06969606131315231 = 0.00427325488999486 + 0.01 * 6.542280197143555
Epoch 930, val loss: 1.3143211603164673
Epoch 940, training loss: 0.06954974681138992 = 0.004091739188879728 + 0.01 * 6.545801162719727
Epoch 940, val loss: 1.3182847499847412
Epoch 950, training loss: 0.06925512850284576 = 0.0039246296510100365 + 0.01 * 6.533050060272217
Epoch 950, val loss: 1.3223248720169067
Epoch 960, training loss: 0.06936532258987427 = 0.0037702801637351513 + 0.01 * 6.559504985809326
Epoch 960, val loss: 1.3262289762496948
Epoch 970, training loss: 0.06912317126989365 = 0.0036271722055971622 + 0.01 * 6.549600124359131
Epoch 970, val loss: 1.3301169872283936
Epoch 980, training loss: 0.06898076832294464 = 0.0034946862142533064 + 0.01 * 6.548608303070068
Epoch 980, val loss: 1.3339133262634277
Epoch 990, training loss: 0.06864982098340988 = 0.003371934872120619 + 0.01 * 6.527789115905762
Epoch 990, val loss: 1.337558388710022
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7815
Overall ASR: 0.7934
Flip ASR: 0.7644/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0236563682556152 = 1.9399174451828003 + 0.01 * 8.373903274536133
Epoch 0, val loss: 1.9336023330688477
Epoch 10, training loss: 2.01371431350708 = 1.9299758672714233 + 0.01 * 8.373842239379883
Epoch 10, val loss: 1.9241911172866821
Epoch 20, training loss: 2.001622438430786 = 1.9178860187530518 + 0.01 * 8.373651504516602
Epoch 20, val loss: 1.9121441841125488
Epoch 30, training loss: 1.9847431182861328 = 1.9010121822357178 + 0.01 * 8.373087882995605
Epoch 30, val loss: 1.8949553966522217
Epoch 40, training loss: 1.9599788188934326 = 1.8762741088867188 + 0.01 * 8.370474815368652
Epoch 40, val loss: 1.8699860572814941
Epoch 50, training loss: 1.9253805875778198 = 1.8418506383895874 + 0.01 * 8.352991104125977
Epoch 50, val loss: 1.8367469310760498
Epoch 60, training loss: 1.8848292827606201 = 1.8022478818893433 + 0.01 * 8.258138656616211
Epoch 60, val loss: 1.8025667667388916
Epoch 70, training loss: 1.8449350595474243 = 1.764769434928894 + 0.01 * 8.016562461853027
Epoch 70, val loss: 1.7732930183410645
Epoch 80, training loss: 1.796656608581543 = 1.718253254890442 + 0.01 * 7.840333938598633
Epoch 80, val loss: 1.7342627048492432
Epoch 90, training loss: 1.7301690578460693 = 1.6548367738723755 + 0.01 * 7.533233165740967
Epoch 90, val loss: 1.6808974742889404
Epoch 100, training loss: 1.6455392837524414 = 1.5730375051498413 + 0.01 * 7.250175476074219
Epoch 100, val loss: 1.6133791208267212
Epoch 110, training loss: 1.5515811443328857 = 1.4805593490600586 + 0.01 * 7.1021833419799805
Epoch 110, val loss: 1.5378503799438477
Epoch 120, training loss: 1.4569871425628662 = 1.3865679502487183 + 0.01 * 7.041924953460693
Epoch 120, val loss: 1.4645963907241821
Epoch 130, training loss: 1.3619219064712524 = 1.2919284105300903 + 0.01 * 6.99934720993042
Epoch 130, val loss: 1.3927428722381592
Epoch 140, training loss: 1.2643054723739624 = 1.1945537328720093 + 0.01 * 6.975178241729736
Epoch 140, val loss: 1.3198826313018799
Epoch 150, training loss: 1.163802981376648 = 1.0942097902297974 + 0.01 * 6.959320545196533
Epoch 150, val loss: 1.2444452047348022
Epoch 160, training loss: 1.0630720853805542 = 0.9935861825942993 + 0.01 * 6.948586940765381
Epoch 160, val loss: 1.1683040857315063
Epoch 170, training loss: 0.9651885628700256 = 0.8957571387290955 + 0.01 * 6.943140983581543
Epoch 170, val loss: 1.0932021141052246
Epoch 180, training loss: 0.8717610836029053 = 0.802348792552948 + 0.01 * 6.941232204437256
Epoch 180, val loss: 1.0213141441345215
Epoch 190, training loss: 0.7838761806488037 = 0.7144704461097717 + 0.01 * 6.940570831298828
Epoch 190, val loss: 0.954085648059845
Epoch 200, training loss: 0.7027832865715027 = 0.633386492729187 + 0.01 * 6.939679145812988
Epoch 200, val loss: 0.8948143720626831
Epoch 210, training loss: 0.6294817328453064 = 0.5601043105125427 + 0.01 * 6.937742233276367
Epoch 210, val loss: 0.8456202745437622
Epoch 220, training loss: 0.5642852783203125 = 0.4949391782283783 + 0.01 * 6.934612274169922
Epoch 220, val loss: 0.806914746761322
Epoch 230, training loss: 0.5069060325622559 = 0.43760061264038086 + 0.01 * 6.930539131164551
Epoch 230, val loss: 0.7773822546005249
Epoch 240, training loss: 0.45644259452819824 = 0.38719654083251953 + 0.01 * 6.924606800079346
Epoch 240, val loss: 0.7551608681678772
Epoch 250, training loss: 0.41176408529281616 = 0.3425639867782593 + 0.01 * 6.920011043548584
Epoch 250, val loss: 0.7388796806335449
Epoch 260, training loss: 0.3717341721057892 = 0.30261823534965515 + 0.01 * 6.911593437194824
Epoch 260, val loss: 0.7275553941726685
Epoch 270, training loss: 0.3354560434818268 = 0.2664341926574707 + 0.01 * 6.902184963226318
Epoch 270, val loss: 0.7204228043556213
Epoch 280, training loss: 0.30238425731658936 = 0.2334420084953308 + 0.01 * 6.89422607421875
Epoch 280, val loss: 0.7168646454811096
Epoch 290, training loss: 0.2723312973976135 = 0.20347583293914795 + 0.01 * 6.885548114776611
Epoch 290, val loss: 0.7164148688316345
Epoch 300, training loss: 0.24543562531471252 = 0.176637202501297 + 0.01 * 6.879842758178711
Epoch 300, val loss: 0.7187246680259705
Epoch 310, training loss: 0.22186361253261566 = 0.15310226380825043 + 0.01 * 6.876134872436523
Epoch 310, val loss: 0.7235463261604309
Epoch 320, training loss: 0.2015906125307083 = 0.13292735815048218 + 0.01 * 6.866325378417969
Epoch 320, val loss: 0.7306740880012512
Epoch 330, training loss: 0.18454089760780334 = 0.11591546982526779 + 0.01 * 6.86254358291626
Epoch 330, val loss: 0.7398438453674316
Epoch 340, training loss: 0.17025664448738098 = 0.10167263448238373 + 0.01 * 6.858400821685791
Epoch 340, val loss: 0.7507317066192627
Epoch 350, training loss: 0.15826749801635742 = 0.0897231176495552 + 0.01 * 6.8544392585754395
Epoch 350, val loss: 0.7629351615905762
Epoch 360, training loss: 0.14812862873077393 = 0.07962425798177719 + 0.01 * 6.850438117980957
Epoch 360, val loss: 0.7761746048927307
Epoch 370, training loss: 0.13948622345924377 = 0.07101071625947952 + 0.01 * 6.847549915313721
Epoch 370, val loss: 0.7900618314743042
Epoch 380, training loss: 0.13207539916038513 = 0.06359823048114777 + 0.01 * 6.847716331481934
Epoch 380, val loss: 0.8044151067733765
Epoch 390, training loss: 0.12559428811073303 = 0.05716972053050995 + 0.01 * 6.842455863952637
Epoch 390, val loss: 0.8191688060760498
Epoch 400, training loss: 0.1199236512184143 = 0.05155857279896736 + 0.01 * 6.836507320404053
Epoch 400, val loss: 0.8340849280357361
Epoch 410, training loss: 0.11496232450008392 = 0.04663562774658203 + 0.01 * 6.832669734954834
Epoch 410, val loss: 0.8492228388786316
Epoch 420, training loss: 0.11059297621250153 = 0.04229928180575371 + 0.01 * 6.829369068145752
Epoch 420, val loss: 0.864388644695282
Epoch 430, training loss: 0.10671374201774597 = 0.03846665844321251 + 0.01 * 6.824707984924316
Epoch 430, val loss: 0.8796136975288391
Epoch 440, training loss: 0.10335667431354523 = 0.035070329904556274 + 0.01 * 6.828634262084961
Epoch 440, val loss: 0.8948226571083069
Epoch 450, training loss: 0.10024355351924896 = 0.032057493925094604 + 0.01 * 6.818605899810791
Epoch 450, val loss: 0.9098818898200989
Epoch 460, training loss: 0.09753170609474182 = 0.029380938038229942 + 0.01 * 6.81507682800293
Epoch 460, val loss: 0.9248172044754028
Epoch 470, training loss: 0.09509852528572083 = 0.026997093111276627 + 0.01 * 6.810143947601318
Epoch 470, val loss: 0.9395564198493958
Epoch 480, training loss: 0.09292855113744736 = 0.024870125576853752 + 0.01 * 6.805842876434326
Epoch 480, val loss: 0.9540396332740784
Epoch 490, training loss: 0.09100610017776489 = 0.022967848926782608 + 0.01 * 6.803825855255127
Epoch 490, val loss: 0.9682769775390625
Epoch 500, training loss: 0.08934107422828674 = 0.021264243870973587 + 0.01 * 6.80768346786499
Epoch 500, val loss: 0.9821155667304993
Epoch 510, training loss: 0.0876871645450592 = 0.019735146313905716 + 0.01 * 6.795201778411865
Epoch 510, val loss: 0.9956281781196594
Epoch 520, training loss: 0.08626894652843475 = 0.018356744199991226 + 0.01 * 6.791219711303711
Epoch 520, val loss: 1.0087945461273193
Epoch 530, training loss: 0.08498135209083557 = 0.01710866019129753 + 0.01 * 6.787269592285156
Epoch 530, val loss: 1.021694540977478
Epoch 540, training loss: 0.08380353450775146 = 0.01597384363412857 + 0.01 * 6.7829694747924805
Epoch 540, val loss: 1.0343149900436401
Epoch 550, training loss: 0.08297812193632126 = 0.014941594563424587 + 0.01 * 6.803652763366699
Epoch 550, val loss: 1.0467206239700317
Epoch 560, training loss: 0.08179667592048645 = 0.014007109217345715 + 0.01 * 6.778956890106201
Epoch 560, val loss: 1.0586810111999512
Epoch 570, training loss: 0.0808679535984993 = 0.013151988387107849 + 0.01 * 6.771596431732178
Epoch 570, val loss: 1.070446252822876
Epoch 580, training loss: 0.08003901690244675 = 0.012363879010081291 + 0.01 * 6.767513751983643
Epoch 580, val loss: 1.0821542739868164
Epoch 590, training loss: 0.07939399778842926 = 0.011635254137217999 + 0.01 * 6.775874614715576
Epoch 590, val loss: 1.0936475992202759
Epoch 600, training loss: 0.07864245027303696 = 0.01096445880830288 + 0.01 * 6.767798900604248
Epoch 600, val loss: 1.1049870252609253
Epoch 610, training loss: 0.07790679484605789 = 0.010345632210373878 + 0.01 * 6.75611686706543
Epoch 610, val loss: 1.116094946861267
Epoch 620, training loss: 0.07729588449001312 = 0.009775643236935139 + 0.01 * 6.752023696899414
Epoch 620, val loss: 1.1270169019699097
Epoch 630, training loss: 0.07671817392110825 = 0.009250420145690441 + 0.01 * 6.746776103973389
Epoch 630, val loss: 1.137695074081421
Epoch 640, training loss: 0.07627470791339874 = 0.008765466511249542 + 0.01 * 6.750924587249756
Epoch 640, val loss: 1.1481285095214844
Epoch 650, training loss: 0.07574664056301117 = 0.008318058215081692 + 0.01 * 6.742857933044434
Epoch 650, val loss: 1.1583876609802246
Epoch 660, training loss: 0.07525311410427094 = 0.007904494181275368 + 0.01 * 6.734862327575684
Epoch 660, val loss: 1.1683874130249023
Epoch 670, training loss: 0.07504334300756454 = 0.007521824911236763 + 0.01 * 6.752151966094971
Epoch 670, val loss: 1.1781561374664307
Epoch 680, training loss: 0.07447774708271027 = 0.007168460637331009 + 0.01 * 6.730928421020508
Epoch 680, val loss: 1.18771231174469
Epoch 690, training loss: 0.07407445460557938 = 0.006840093992650509 + 0.01 * 6.72343635559082
Epoch 690, val loss: 1.1969941854476929
Epoch 700, training loss: 0.07371093332767487 = 0.006535569205880165 + 0.01 * 6.717536449432373
Epoch 700, val loss: 1.2060949802398682
Epoch 710, training loss: 0.07337359338998795 = 0.00625255610793829 + 0.01 * 6.712103843688965
Epoch 710, val loss: 1.2149931192398071
Epoch 720, training loss: 0.07342551648616791 = 0.005988494958728552 + 0.01 * 6.743702411651611
Epoch 720, val loss: 1.2236181497573853
Epoch 730, training loss: 0.0727805644273758 = 0.005743545945733786 + 0.01 * 6.703701972961426
Epoch 730, val loss: 1.2320269346237183
Epoch 740, training loss: 0.07249603420495987 = 0.005514392629265785 + 0.01 * 6.698164463043213
Epoch 740, val loss: 1.2402446269989014
Epoch 750, training loss: 0.07253056019544601 = 0.005300333257764578 + 0.01 * 6.723022937774658
Epoch 750, val loss: 1.248300313949585
Epoch 760, training loss: 0.07199513912200928 = 0.005099853966385126 + 0.01 * 6.689528465270996
Epoch 760, val loss: 1.2561146020889282
Epoch 770, training loss: 0.07177148759365082 = 0.004911657422780991 + 0.01 * 6.685983180999756
Epoch 770, val loss: 1.2637324333190918
Epoch 780, training loss: 0.07166741043329239 = 0.004735010676085949 + 0.01 * 6.693240165710449
Epoch 780, val loss: 1.2712002992630005
Epoch 790, training loss: 0.07130803167819977 = 0.004569861572235823 + 0.01 * 6.673817157745361
Epoch 790, val loss: 1.2784780263900757
Epoch 800, training loss: 0.07124456763267517 = 0.004414013121277094 + 0.01 * 6.683055400848389
Epoch 800, val loss: 1.2855637073516846
Epoch 810, training loss: 0.07103974372148514 = 0.004267707467079163 + 0.01 * 6.677204132080078
Epoch 810, val loss: 1.292478322982788
Epoch 820, training loss: 0.07075237482786179 = 0.004129420500248671 + 0.01 * 6.662295818328857
Epoch 820, val loss: 1.2992959022521973
Epoch 830, training loss: 0.07059523463249207 = 0.0039988793432712555 + 0.01 * 6.6596360206604
Epoch 830, val loss: 1.3058834075927734
Epoch 840, training loss: 0.07053939998149872 = 0.0038756057620048523 + 0.01 * 6.666379928588867
Epoch 840, val loss: 1.3124088048934937
Epoch 850, training loss: 0.07038162648677826 = 0.003759372280910611 + 0.01 * 6.66222620010376
Epoch 850, val loss: 1.3186193704605103
Epoch 860, training loss: 0.07010297477245331 = 0.0036492859944701195 + 0.01 * 6.645369052886963
Epoch 860, val loss: 1.3248459100723267
Epoch 870, training loss: 0.07040611654520035 = 0.0035449147690087557 + 0.01 * 6.686120986938477
Epoch 870, val loss: 1.330863118171692
Epoch 880, training loss: 0.06986158341169357 = 0.0034464308992028236 + 0.01 * 6.641515254974365
Epoch 880, val loss: 1.3366105556488037
Epoch 890, training loss: 0.07012412697076797 = 0.003352496540173888 + 0.01 * 6.677163600921631
Epoch 890, val loss: 1.3424650430679321
Epoch 900, training loss: 0.06954779475927353 = 0.003263754304498434 + 0.01 * 6.628404140472412
Epoch 900, val loss: 1.3479312658309937
Epoch 910, training loss: 0.06931496411561966 = 0.003179133404046297 + 0.01 * 6.613583087921143
Epoch 910, val loss: 1.3535233736038208
Epoch 920, training loss: 0.06935811042785645 = 0.0030984864570200443 + 0.01 * 6.625962257385254
Epoch 920, val loss: 1.3587884902954102
Epoch 930, training loss: 0.06916114687919617 = 0.003022079123184085 + 0.01 * 6.613906383514404
Epoch 930, val loss: 1.3640164136886597
Epoch 940, training loss: 0.06903858482837677 = 0.002948988927528262 + 0.01 * 6.608959197998047
Epoch 940, val loss: 1.3691442012786865
Epoch 950, training loss: 0.06917327642440796 = 0.0028793581295758486 + 0.01 * 6.629391670227051
Epoch 950, val loss: 1.3740909099578857
Epoch 960, training loss: 0.06901005655527115 = 0.0028131213039159775 + 0.01 * 6.619693279266357
Epoch 960, val loss: 1.3789699077606201
Epoch 970, training loss: 0.06877139210700989 = 0.0027495233807712793 + 0.01 * 6.602187156677246
Epoch 970, val loss: 1.3837069272994995
Epoch 980, training loss: 0.06880424916744232 = 0.002688961336389184 + 0.01 * 6.611529350280762
Epoch 980, val loss: 1.3884313106536865
Epoch 990, training loss: 0.06833168864250183 = 0.002630804665386677 + 0.01 * 6.570087909698486
Epoch 990, val loss: 1.392959713935852
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9336
Flip ASR: 0.9200/225 nodes
The final ASR:0.79459, 0.11299, Accuracy:0.80494, 0.02145
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11616])
remove edge: torch.Size([2, 9582])
updated graph: torch.Size([2, 10642])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98032, 0.00627, Accuracy:0.82716, 0.00462
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0220422744750977 = 1.9383034706115723 + 0.01 * 8.373889923095703
Epoch 0, val loss: 1.9426302909851074
Epoch 10, training loss: 2.011948347091675 = 1.9282101392745972 + 0.01 * 8.373827934265137
Epoch 10, val loss: 1.9323387145996094
Epoch 20, training loss: 1.9996224641799927 = 1.915886640548706 + 0.01 * 8.37358570098877
Epoch 20, val loss: 1.9194484949111938
Epoch 30, training loss: 1.982450246810913 = 1.8987219333648682 + 0.01 * 8.37282943725586
Epoch 30, val loss: 1.9013893604278564
Epoch 40, training loss: 1.9572014808654785 = 1.8735154867172241 + 0.01 * 8.3685941696167
Epoch 40, val loss: 1.8752310276031494
Epoch 50, training loss: 1.9219329357147217 = 1.8385592699050903 + 0.01 * 8.337363243103027
Epoch 50, val loss: 1.8408311605453491
Epoch 60, training loss: 1.881493091583252 = 1.7999770641326904 + 0.01 * 8.15160846710205
Epoch 60, val loss: 1.8076896667480469
Epoch 70, training loss: 1.8437575101852417 = 1.7646889686584473 + 0.01 * 7.906859874725342
Epoch 70, val loss: 1.780637502670288
Epoch 80, training loss: 1.7941429615020752 = 1.7183092832565308 + 0.01 * 7.583373069763184
Epoch 80, val loss: 1.7418855428695679
Epoch 90, training loss: 1.7268071174621582 = 1.6539114713668823 + 0.01 * 7.28956413269043
Epoch 90, val loss: 1.6873821020126343
Epoch 100, training loss: 1.640333890914917 = 1.5689127445220947 + 0.01 * 7.142111301422119
Epoch 100, val loss: 1.6170092821121216
Epoch 110, training loss: 1.5430275201797485 = 1.4723260402679443 + 0.01 * 7.070152282714844
Epoch 110, val loss: 1.5395227670669556
Epoch 120, training loss: 1.4460033178329468 = 1.3757647275924683 + 0.01 * 7.023857116699219
Epoch 120, val loss: 1.4650585651397705
Epoch 130, training loss: 1.3504984378814697 = 1.2806079387664795 + 0.01 * 6.989045143127441
Epoch 130, val loss: 1.3940953016281128
Epoch 140, training loss: 1.2554148435592651 = 1.1858246326446533 + 0.01 * 6.959018230438232
Epoch 140, val loss: 1.3238191604614258
Epoch 150, training loss: 1.1624940633773804 = 1.093119740486145 + 0.01 * 6.937427520751953
Epoch 150, val loss: 1.254155158996582
Epoch 160, training loss: 1.0749130249023438 = 1.0056735277175903 + 0.01 * 6.923950672149658
Epoch 160, val loss: 1.1882741451263428
Epoch 170, training loss: 0.9938408136367798 = 0.9246760010719299 + 0.01 * 6.9164838790893555
Epoch 170, val loss: 1.1267054080963135
Epoch 180, training loss: 0.9177693724632263 = 0.8486314415931702 + 0.01 * 6.913791656494141
Epoch 180, val loss: 1.068657636642456
Epoch 190, training loss: 0.8449265956878662 = 0.7758155465126038 + 0.01 * 6.911102771759033
Epoch 190, val loss: 1.0122177600860596
Epoch 200, training loss: 0.7753177881240845 = 0.7062240839004517 + 0.01 * 6.909368515014648
Epoch 200, val loss: 0.9581993818283081
Epoch 210, training loss: 0.7095201015472412 = 0.6404399871826172 + 0.01 * 6.9080095291137695
Epoch 210, val loss: 0.9085842967033386
Epoch 220, training loss: 0.6471220254898071 = 0.5780553221702576 + 0.01 * 6.906668663024902
Epoch 220, val loss: 0.86394202709198
Epoch 230, training loss: 0.5869190096855164 = 0.5178709030151367 + 0.01 * 6.904810428619385
Epoch 230, val loss: 0.8234046101570129
Epoch 240, training loss: 0.5284482836723328 = 0.4594290554523468 + 0.01 * 6.901922702789307
Epoch 240, val loss: 0.7865760922431946
Epoch 250, training loss: 0.4727945327758789 = 0.4038190543651581 + 0.01 * 6.897546291351318
Epoch 250, val loss: 0.753980278968811
Epoch 260, training loss: 0.4218948483467102 = 0.35298025608062744 + 0.01 * 6.891460418701172
Epoch 260, val loss: 0.7275238037109375
Epoch 270, training loss: 0.3769116997718811 = 0.3080669045448303 + 0.01 * 6.88447904586792
Epoch 270, val loss: 0.708061695098877
Epoch 280, training loss: 0.3376768231391907 = 0.2689080536365509 + 0.01 * 6.876875400543213
Epoch 280, val loss: 0.6952193379402161
Epoch 290, training loss: 0.3031398057937622 = 0.2344418168067932 + 0.01 * 6.869799613952637
Epoch 290, val loss: 0.6877167820930481
Epoch 300, training loss: 0.27208659052848816 = 0.20350131392478943 + 0.01 * 6.858528137207031
Epoch 300, val loss: 0.6841380596160889
Epoch 310, training loss: 0.2440129518508911 = 0.1754692792892456 + 0.01 * 6.854367256164551
Epoch 310, val loss: 0.683489203453064
Epoch 320, training loss: 0.2187836468219757 = 0.15033398568630219 + 0.01 * 6.84496545791626
Epoch 320, val loss: 0.6852924227714539
Epoch 330, training loss: 0.1967710256576538 = 0.12838876247406006 + 0.01 * 6.838225841522217
Epoch 330, val loss: 0.6892591714859009
Epoch 340, training loss: 0.17815366387367249 = 0.10980825871229172 + 0.01 * 6.834540367126465
Epoch 340, val loss: 0.69520503282547
Epoch 350, training loss: 0.16272425651550293 = 0.09442511945962906 + 0.01 * 6.829914093017578
Epoch 350, val loss: 0.7028886079788208
Epoch 360, training loss: 0.15007564425468445 = 0.08181369304656982 + 0.01 * 6.826196193695068
Epoch 360, val loss: 0.7120678424835205
Epoch 370, training loss: 0.13967201113700867 = 0.07145904749631882 + 0.01 * 6.821295738220215
Epoch 370, val loss: 0.7223618626594543
Epoch 380, training loss: 0.13106131553649902 = 0.06289210915565491 + 0.01 * 6.816919803619385
Epoch 380, val loss: 0.7334827780723572
Epoch 390, training loss: 0.1238783597946167 = 0.05573445186018944 + 0.01 * 6.814391613006592
Epoch 390, val loss: 0.7451256513595581
Epoch 400, training loss: 0.11781762540340424 = 0.04969817027449608 + 0.01 * 6.81194543838501
Epoch 400, val loss: 0.7570565938949585
Epoch 410, training loss: 0.1126631498336792 = 0.04456062242388725 + 0.01 * 6.810253620147705
Epoch 410, val loss: 0.769133985042572
Epoch 420, training loss: 0.10819762945175171 = 0.04015357419848442 + 0.01 * 6.804405212402344
Epoch 420, val loss: 0.781213104724884
Epoch 430, training loss: 0.1043146550655365 = 0.03634551167488098 + 0.01 * 6.796914577484131
Epoch 430, val loss: 0.7932723164558411
Epoch 440, training loss: 0.1010904461145401 = 0.03303457424044609 + 0.01 * 6.805586814880371
Epoch 440, val loss: 0.805180013179779
Epoch 450, training loss: 0.09801863133907318 = 0.03014267608523369 + 0.01 * 6.787595272064209
Epoch 450, val loss: 0.8168584704399109
Epoch 460, training loss: 0.0954313576221466 = 0.027603814378380775 + 0.01 * 6.782754898071289
Epoch 460, val loss: 0.8283390998840332
Epoch 470, training loss: 0.0931495726108551 = 0.025364143773913383 + 0.01 * 6.778542518615723
Epoch 470, val loss: 0.8395479321479797
Epoch 480, training loss: 0.09112904965877533 = 0.02338062971830368 + 0.01 * 6.774842262268066
Epoch 480, val loss: 0.8505463600158691
Epoch 490, training loss: 0.08940789103507996 = 0.021618584170937538 + 0.01 * 6.7789306640625
Epoch 490, val loss: 0.8611887693405151
Epoch 500, training loss: 0.08769537508487701 = 0.02004818059504032 + 0.01 * 6.7647199630737305
Epoch 500, val loss: 0.871620774269104
Epoch 510, training loss: 0.08624851703643799 = 0.018642334267497063 + 0.01 * 6.760618209838867
Epoch 510, val loss: 0.8817926049232483
Epoch 520, training loss: 0.08491978049278259 = 0.017378706485033035 + 0.01 * 6.75410795211792
Epoch 520, val loss: 0.8916156888008118
Epoch 530, training loss: 0.0840388610959053 = 0.016239913180470467 + 0.01 * 6.779895305633545
Epoch 530, val loss: 0.9012889266014099
Epoch 540, training loss: 0.08278007805347443 = 0.01521351933479309 + 0.01 * 6.756656169891357
Epoch 540, val loss: 0.9106934070587158
Epoch 550, training loss: 0.0817006453871727 = 0.014283997938036919 + 0.01 * 6.741664409637451
Epoch 550, val loss: 0.9197806119918823
Epoch 560, training loss: 0.08087801933288574 = 0.01343910489231348 + 0.01 * 6.743891716003418
Epoch 560, val loss: 0.9286468625068665
Epoch 570, training loss: 0.07998785376548767 = 0.01266929879784584 + 0.01 * 6.731856346130371
Epoch 570, val loss: 0.9373323321342468
Epoch 580, training loss: 0.07925921678543091 = 0.011966048739850521 + 0.01 * 6.729316711425781
Epoch 580, val loss: 0.9457408785820007
Epoch 590, training loss: 0.07859424501657486 = 0.011322202160954475 + 0.01 * 6.727204322814941
Epoch 590, val loss: 0.9539381265640259
Epoch 600, training loss: 0.0779777392745018 = 0.010731619782745838 + 0.01 * 6.724611759185791
Epoch 600, val loss: 0.9619050025939941
Epoch 610, training loss: 0.07732881605625153 = 0.010187975130975246 + 0.01 * 6.714084148406982
Epoch 610, val loss: 0.9696935415267944
Epoch 620, training loss: 0.07678758352994919 = 0.009687804616987705 + 0.01 * 6.709978103637695
Epoch 620, val loss: 0.9772650599479675
Epoch 630, training loss: 0.07629195600748062 = 0.009225942194461823 + 0.01 * 6.706601619720459
Epoch 630, val loss: 0.9846217632293701
Epoch 640, training loss: 0.07591161131858826 = 0.008799297735095024 + 0.01 * 6.711231231689453
Epoch 640, val loss: 0.991790771484375
Epoch 650, training loss: 0.0753398984670639 = 0.00840356107801199 + 0.01 * 6.693633556365967
Epoch 650, val loss: 0.998819887638092
Epoch 660, training loss: 0.07511606812477112 = 0.008035977371037006 + 0.01 * 6.708008766174316
Epoch 660, val loss: 1.005651831626892
Epoch 670, training loss: 0.07451286166906357 = 0.007694104686379433 + 0.01 * 6.681875705718994
Epoch 670, val loss: 1.0123224258422852
Epoch 680, training loss: 0.0741133838891983 = 0.00737566314637661 + 0.01 * 6.673771858215332
Epoch 680, val loss: 1.018776535987854
Epoch 690, training loss: 0.07389753311872482 = 0.00707860616967082 + 0.01 * 6.6818928718566895
Epoch 690, val loss: 1.0250812768936157
Epoch 700, training loss: 0.07343693822622299 = 0.0068010794930160046 + 0.01 * 6.663586616516113
Epoch 700, val loss: 1.0312089920043945
Epoch 710, training loss: 0.07312566787004471 = 0.006541213486343622 + 0.01 * 6.658445358276367
Epoch 710, val loss: 1.0372878313064575
Epoch 720, training loss: 0.07303708046674728 = 0.006298021413385868 + 0.01 * 6.673906326293945
Epoch 720, val loss: 1.0431432723999023
Epoch 730, training loss: 0.07264228165149689 = 0.006070256698876619 + 0.01 * 6.65720272064209
Epoch 730, val loss: 1.048825979232788
Epoch 740, training loss: 0.07223105430603027 = 0.005856073461472988 + 0.01 * 6.637498378753662
Epoch 740, val loss: 1.0544307231903076
Epoch 750, training loss: 0.07190561294555664 = 0.005654812324792147 + 0.01 * 6.625080108642578
Epoch 750, val loss: 1.059820294380188
Epoch 760, training loss: 0.07187789678573608 = 0.005465332418680191 + 0.01 * 6.641257286071777
Epoch 760, val loss: 1.0651829242706299
Epoch 770, training loss: 0.07140606641769409 = 0.0052867308259010315 + 0.01 * 6.611933708190918
Epoch 770, val loss: 1.0703586339950562
Epoch 780, training loss: 0.07140569388866425 = 0.005117973778396845 + 0.01 * 6.628772258758545
Epoch 780, val loss: 1.0753949880599976
Epoch 790, training loss: 0.0709133967757225 = 0.0049588317051529884 + 0.01 * 6.595456600189209
Epoch 790, val loss: 1.0803769826889038
Epoch 800, training loss: 0.07079221308231354 = 0.004808137193322182 + 0.01 * 6.598408222198486
Epoch 800, val loss: 1.0852195024490356
Epoch 810, training loss: 0.07089072465896606 = 0.004665283486247063 + 0.01 * 6.622543811798096
Epoch 810, val loss: 1.0899999141693115
Epoch 820, training loss: 0.07040636241436005 = 0.004530072677880526 + 0.01 * 6.5876288414001465
Epoch 820, val loss: 1.0945937633514404
Epoch 830, training loss: 0.0700797438621521 = 0.004401938524097204 + 0.01 * 6.567780494689941
Epoch 830, val loss: 1.0991421937942505
Epoch 840, training loss: 0.07024651020765305 = 0.004280127119272947 + 0.01 * 6.5966386795043945
Epoch 840, val loss: 1.103562831878662
Epoch 850, training loss: 0.06986185908317566 = 0.004164176061749458 + 0.01 * 6.56976842880249
Epoch 850, val loss: 1.1079370975494385
Epoch 860, training loss: 0.06961371004581451 = 0.004053804092109203 + 0.01 * 6.555990695953369
Epoch 860, val loss: 1.112197756767273
Epoch 870, training loss: 0.06968990713357925 = 0.003948642406612635 + 0.01 * 6.574126720428467
Epoch 870, val loss: 1.1164100170135498
Epoch 880, training loss: 0.06941523402929306 = 0.0038483846001327038 + 0.01 * 6.556685447692871
Epoch 880, val loss: 1.1205179691314697
Epoch 890, training loss: 0.06961913406848907 = 0.0037524893414229155 + 0.01 * 6.58666467666626
Epoch 890, val loss: 1.1246066093444824
Epoch 900, training loss: 0.06919911503791809 = 0.003661181079223752 + 0.01 * 6.553793907165527
Epoch 900, val loss: 1.128533959388733
Epoch 910, training loss: 0.06925084441900253 = 0.0035738875158131123 + 0.01 * 6.567695617675781
Epoch 910, val loss: 1.1324821710586548
Epoch 920, training loss: 0.06883978843688965 = 0.0034902081824839115 + 0.01 * 6.5349578857421875
Epoch 920, val loss: 1.1362955570220947
Epoch 930, training loss: 0.06881218403577805 = 0.00341016030870378 + 0.01 * 6.5402021408081055
Epoch 930, val loss: 1.14007568359375
Epoch 940, training loss: 0.06851200759410858 = 0.0033335620537400246 + 0.01 * 6.5178446769714355
Epoch 940, val loss: 1.1438517570495605
Epoch 950, training loss: 0.06850824505090714 = 0.003260013647377491 + 0.01 * 6.524823188781738
Epoch 950, val loss: 1.1474729776382446
Epoch 960, training loss: 0.06848111003637314 = 0.0031894997227936983 + 0.01 * 6.52916145324707
Epoch 960, val loss: 1.1511861085891724
Epoch 970, training loss: 0.06817903369665146 = 0.003121910849586129 + 0.01 * 6.505712509155273
Epoch 970, val loss: 1.1547300815582275
Epoch 980, training loss: 0.06821616739034653 = 0.0030568824149668217 + 0.01 * 6.515928745269775
Epoch 980, val loss: 1.1583181619644165
Epoch 990, training loss: 0.06814470887184143 = 0.0029944465495646 + 0.01 * 6.515026569366455
Epoch 990, val loss: 1.161763072013855
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.5720
Flip ASR: 0.4933/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.049595355987549 = 1.9658565521240234 + 0.01 * 8.373884201049805
Epoch 0, val loss: 1.9669207334518433
Epoch 10, training loss: 2.0384562015533447 = 1.9547182321548462 + 0.01 * 8.373801231384277
Epoch 10, val loss: 1.9551775455474854
Epoch 20, training loss: 2.0246429443359375 = 1.9409074783325195 + 0.01 * 8.373543739318848
Epoch 20, val loss: 1.9405992031097412
Epoch 30, training loss: 2.005091667175293 = 1.9213639497756958 + 0.01 * 8.372777938842773
Epoch 30, val loss: 1.9201610088348389
Epoch 40, training loss: 1.9760175943374634 = 1.8923321962356567 + 0.01 * 8.368537902832031
Epoch 40, val loss: 1.8902206420898438
Epoch 50, training loss: 1.9345682859420776 = 1.8511955738067627 + 0.01 * 8.33726978302002
Epoch 50, val loss: 1.8492192029953003
Epoch 60, training loss: 1.8848532438278198 = 1.8030544519424438 + 0.01 * 8.179874420166016
Epoch 60, val loss: 1.8049172163009644
Epoch 70, training loss: 1.83963942527771 = 1.7602753639221191 + 0.01 * 7.936409950256348
Epoch 70, val loss: 1.7691501379013062
Epoch 80, training loss: 1.7902776002883911 = 1.7128130197525024 + 0.01 * 7.746458530426025
Epoch 80, val loss: 1.727607011795044
Epoch 90, training loss: 1.7234344482421875 = 1.6481012105941772 + 0.01 * 7.533325672149658
Epoch 90, val loss: 1.6710405349731445
Epoch 100, training loss: 1.635547161102295 = 1.5626267194747925 + 0.01 * 7.292050361633301
Epoch 100, val loss: 1.5992885828018188
Epoch 110, training loss: 1.5306535959243774 = 1.459930419921875 + 0.01 * 7.072315692901611
Epoch 110, val loss: 1.5142713785171509
Epoch 120, training loss: 1.4235131740570068 = 1.353150486946106 + 0.01 * 7.0362629890441895
Epoch 120, val loss: 1.4276325702667236
Epoch 130, training loss: 1.3213144540786743 = 1.2512203454971313 + 0.01 * 7.009407043457031
Epoch 130, val loss: 1.3479957580566406
Epoch 140, training loss: 1.2256819009780884 = 1.1558324098587036 + 0.01 * 6.984946250915527
Epoch 140, val loss: 1.2752772569656372
Epoch 150, training loss: 1.136162281036377 = 1.0665167570114136 + 0.01 * 6.964552879333496
Epoch 150, val loss: 1.2068876028060913
Epoch 160, training loss: 1.0528465509414673 = 0.9833468198776245 + 0.01 * 6.949978828430176
Epoch 160, val loss: 1.1431121826171875
Epoch 170, training loss: 0.9758546948432922 = 0.9064384698867798 + 0.01 * 6.94162130355835
Epoch 170, val loss: 1.0838418006896973
Epoch 180, training loss: 0.904362142086029 = 0.8349891304969788 + 0.01 * 6.937303066253662
Epoch 180, val loss: 1.0287398099899292
Epoch 190, training loss: 0.8376375436782837 = 0.7682915329933167 + 0.01 * 6.934600830078125
Epoch 190, val loss: 0.9770660400390625
Epoch 200, training loss: 0.776165783405304 = 0.7068395614624023 + 0.01 * 6.932620048522949
Epoch 200, val loss: 0.9296727180480957
Epoch 210, training loss: 0.7206700444221497 = 0.6513590216636658 + 0.01 * 6.931100845336914
Epoch 210, val loss: 0.8877956867218018
Epoch 220, training loss: 0.6706010699272156 = 0.6013065576553345 + 0.01 * 6.929449081420898
Epoch 220, val loss: 0.8515872359275818
Epoch 230, training loss: 0.6239718198776245 = 0.5546990633010864 + 0.01 * 6.927277565002441
Epoch 230, val loss: 0.819670557975769
Epoch 240, training loss: 0.5785539746284485 = 0.5093116164207458 + 0.01 * 6.9242353439331055
Epoch 240, val loss: 0.7903393507003784
Epoch 250, training loss: 0.5331032276153564 = 0.463887095451355 + 0.01 * 6.92161226272583
Epoch 250, val loss: 0.7620857954025269
Epoch 260, training loss: 0.48766300082206726 = 0.4184991419315338 + 0.01 * 6.916385650634766
Epoch 260, val loss: 0.7347708344459534
Epoch 270, training loss: 0.443115770816803 = 0.3740125894546509 + 0.01 * 6.910318851470947
Epoch 270, val loss: 0.7089274525642395
Epoch 280, training loss: 0.4003976285457611 = 0.33134302496910095 + 0.01 * 6.905461311340332
Epoch 280, val loss: 0.6854814291000366
Epoch 290, training loss: 0.3602214455604553 = 0.29126983880996704 + 0.01 * 6.895162105560303
Epoch 290, val loss: 0.6653401851654053
Epoch 300, training loss: 0.3233015835285187 = 0.2544485032558441 + 0.01 * 6.885308742523193
Epoch 300, val loss: 0.6494873762130737
Epoch 310, training loss: 0.2899993658065796 = 0.22125424444675446 + 0.01 * 6.874513626098633
Epoch 310, val loss: 0.637848973274231
Epoch 320, training loss: 0.2606261968612671 = 0.19199270009994507 + 0.01 * 6.863350868225098
Epoch 320, val loss: 0.6300609111785889
Epoch 330, training loss: 0.23523423075675964 = 0.16673912107944489 + 0.01 * 6.849510669708252
Epoch 330, val loss: 0.6259136199951172
Epoch 340, training loss: 0.21379899978637695 = 0.1453055441379547 + 0.01 * 6.849346160888672
Epoch 340, val loss: 0.6250426769256592
Epoch 350, training loss: 0.19565708935260773 = 0.12729796767234802 + 0.01 * 6.835912227630615
Epoch 350, val loss: 0.6268501877784729
Epoch 360, training loss: 0.18051844835281372 = 0.11229702085256577 + 0.01 * 6.822142601013184
Epoch 360, val loss: 0.6310252547264099
Epoch 370, training loss: 0.1679428219795227 = 0.09987591952085495 + 0.01 * 6.8066911697387695
Epoch 370, val loss: 0.6369419693946838
Epoch 380, training loss: 0.15756994485855103 = 0.08959992229938507 + 0.01 * 6.79700231552124
Epoch 380, val loss: 0.6440995335578918
Epoch 390, training loss: 0.14899379014968872 = 0.08103866130113602 + 0.01 * 6.795513153076172
Epoch 390, val loss: 0.6521252393722534
Epoch 400, training loss: 0.14156144857406616 = 0.0737946629524231 + 0.01 * 6.776679515838623
Epoch 400, val loss: 0.6605538725852966
Epoch 410, training loss: 0.13526758551597595 = 0.06758411228656769 + 0.01 * 6.768348217010498
Epoch 410, val loss: 0.669339120388031
Epoch 420, training loss: 0.12972864508628845 = 0.06220352277159691 + 0.01 * 6.752511978149414
Epoch 420, val loss: 0.6782976388931274
Epoch 430, training loss: 0.12501561641693115 = 0.057493433356285095 + 0.01 * 6.752217769622803
Epoch 430, val loss: 0.6874037981033325
Epoch 440, training loss: 0.12083417922258377 = 0.05333720147609711 + 0.01 * 6.749697685241699
Epoch 440, val loss: 0.6965073347091675
Epoch 450, training loss: 0.11685772985219955 = 0.04963512718677521 + 0.01 * 6.722260475158691
Epoch 450, val loss: 0.7056283950805664
Epoch 460, training loss: 0.11387908458709717 = 0.04630912467837334 + 0.01 * 6.7569966316223145
Epoch 460, val loss: 0.7146450877189636
Epoch 470, training loss: 0.11037717014551163 = 0.043304577469825745 + 0.01 * 6.707259178161621
Epoch 470, val loss: 0.723601758480072
Epoch 480, training loss: 0.10768760740756989 = 0.040570322424173355 + 0.01 * 6.711728572845459
Epoch 480, val loss: 0.732347309589386
Epoch 490, training loss: 0.10495805740356445 = 0.038072746247053146 + 0.01 * 6.688531398773193
Epoch 490, val loss: 0.7409296035766602
Epoch 500, training loss: 0.10330231487751007 = 0.03577573969960213 + 0.01 * 6.752656936645508
Epoch 500, val loss: 0.7493396997451782
Epoch 510, training loss: 0.10062666237354279 = 0.03366309776902199 + 0.01 * 6.696356296539307
Epoch 510, val loss: 0.7575014233589172
Epoch 520, training loss: 0.09842458367347717 = 0.031701937317848206 + 0.01 * 6.67226505279541
Epoch 520, val loss: 0.7655349373817444
Epoch 530, training loss: 0.09673216938972473 = 0.029863759875297546 + 0.01 * 6.686841011047363
Epoch 530, val loss: 0.773367702960968
Epoch 540, training loss: 0.09473960101604462 = 0.028131868690252304 + 0.01 * 6.660772800445557
Epoch 540, val loss: 0.7810141444206238
Epoch 550, training loss: 0.09295198321342468 = 0.026487698778510094 + 0.01 * 6.64642858505249
Epoch 550, val loss: 0.7885213494300842
Epoch 560, training loss: 0.09137695282697678 = 0.024937475100159645 + 0.01 * 6.643948078155518
Epoch 560, val loss: 0.7959122657775879
Epoch 570, training loss: 0.08977936953306198 = 0.023453325033187866 + 0.01 * 6.632604598999023
Epoch 570, val loss: 0.8032159209251404
Epoch 580, training loss: 0.08839592337608337 = 0.022012006491422653 + 0.01 * 6.638391494750977
Epoch 580, val loss: 0.8102962374687195
Epoch 590, training loss: 0.0869818776845932 = 0.020625434815883636 + 0.01 * 6.635644435882568
Epoch 590, val loss: 0.8172435760498047
Epoch 600, training loss: 0.08546164631843567 = 0.019298765808343887 + 0.01 * 6.616287708282471
Epoch 600, val loss: 0.8240190148353577
Epoch 610, training loss: 0.084269218146801 = 0.018031664192676544 + 0.01 * 6.62375545501709
Epoch 610, val loss: 0.8307262659072876
Epoch 620, training loss: 0.08290787786245346 = 0.016812384128570557 + 0.01 * 6.609549522399902
Epoch 620, val loss: 0.8373934626579285
Epoch 630, training loss: 0.08180706202983856 = 0.015666089951992035 + 0.01 * 6.614097595214844
Epoch 630, val loss: 0.8438843488693237
Epoch 640, training loss: 0.08058593422174454 = 0.01460788119584322 + 0.01 * 6.597805023193359
Epoch 640, val loss: 0.8505327105522156
Epoch 650, training loss: 0.07957739382982254 = 0.013639104552567005 + 0.01 * 6.593829154968262
Epoch 650, val loss: 0.8571207523345947
Epoch 660, training loss: 0.07876335084438324 = 0.012761394493281841 + 0.01 * 6.60019588470459
Epoch 660, val loss: 0.863719642162323
Epoch 670, training loss: 0.07782178372144699 = 0.011962753720581532 + 0.01 * 6.585903167724609
Epoch 670, val loss: 0.8704626560211182
Epoch 680, training loss: 0.07702475041151047 = 0.01125164981931448 + 0.01 * 6.577310085296631
Epoch 680, val loss: 0.877221405506134
Epoch 690, training loss: 0.07635261118412018 = 0.010607441887259483 + 0.01 * 6.574516773223877
Epoch 690, val loss: 0.8839222192764282
Epoch 700, training loss: 0.07576500624418259 = 0.010019690729677677 + 0.01 * 6.5745320320129395
Epoch 700, val loss: 0.8906006813049316
Epoch 710, training loss: 0.07517679780721664 = 0.00948032084852457 + 0.01 * 6.569648265838623
Epoch 710, val loss: 0.8971526026725769
Epoch 720, training loss: 0.07467439025640488 = 0.008987726643681526 + 0.01 * 6.568666458129883
Epoch 720, val loss: 0.9037529230117798
Epoch 730, training loss: 0.0742078423500061 = 0.008538279682397842 + 0.01 * 6.566956043243408
Epoch 730, val loss: 0.9102680087089539
Epoch 740, training loss: 0.07370493561029434 = 0.00812737550586462 + 0.01 * 6.557756423950195
Epoch 740, val loss: 0.9166003465652466
Epoch 750, training loss: 0.07326706498861313 = 0.007749519776552916 + 0.01 * 6.551754951477051
Epoch 750, val loss: 0.9229110479354858
Epoch 760, training loss: 0.07301120460033417 = 0.007402140647172928 + 0.01 * 6.560906410217285
Epoch 760, val loss: 0.9290268421173096
Epoch 770, training loss: 0.0725071057677269 = 0.00708194263279438 + 0.01 * 6.542516708374023
Epoch 770, val loss: 0.9350825548171997
Epoch 780, training loss: 0.07222121208906174 = 0.006785925943404436 + 0.01 * 6.543528079986572
Epoch 780, val loss: 0.9409533143043518
Epoch 790, training loss: 0.07206051796674728 = 0.006511409301310778 + 0.01 * 6.554910659790039
Epoch 790, val loss: 0.9467734694480896
Epoch 800, training loss: 0.07160883396863937 = 0.006255498621612787 + 0.01 * 6.535333633422852
Epoch 800, val loss: 0.9524678587913513
Epoch 810, training loss: 0.07121347635984421 = 0.0060163638554513454 + 0.01 * 6.519711494445801
Epoch 810, val loss: 0.9580342769622803
Epoch 820, training loss: 0.07139813899993896 = 0.0057932003401219845 + 0.01 * 6.560494422912598
Epoch 820, val loss: 0.9634930491447449
Epoch 830, training loss: 0.0707911029458046 = 0.00558346975594759 + 0.01 * 6.520763397216797
Epoch 830, val loss: 0.9688801169395447
Epoch 840, training loss: 0.07049968093633652 = 0.005386343225836754 + 0.01 * 6.51133394241333
Epoch 840, val loss: 0.9740763306617737
Epoch 850, training loss: 0.07038430124521255 = 0.005201683379709721 + 0.01 * 6.518261909484863
Epoch 850, val loss: 0.9790876507759094
Epoch 860, training loss: 0.07017327845096588 = 0.00502855284139514 + 0.01 * 6.514472484588623
Epoch 860, val loss: 0.9841563105583191
Epoch 870, training loss: 0.06996941566467285 = 0.004865299444645643 + 0.01 * 6.510412216186523
Epoch 870, val loss: 0.989025890827179
Epoch 880, training loss: 0.06980384141206741 = 0.004711837973445654 + 0.01 * 6.509200572967529
Epoch 880, val loss: 0.9938299059867859
Epoch 890, training loss: 0.06952257454395294 = 0.004567257594317198 + 0.01 * 6.495532035827637
Epoch 890, val loss: 0.998601496219635
Epoch 900, training loss: 0.06974229216575623 = 0.0044305999763309956 + 0.01 * 6.531169414520264
Epoch 900, val loss: 1.003222942352295
Epoch 910, training loss: 0.06923741102218628 = 0.004301583394408226 + 0.01 * 6.493582725524902
Epoch 910, val loss: 1.0077146291732788
Epoch 920, training loss: 0.06934710592031479 = 0.004179645795375109 + 0.01 * 6.516745567321777
Epoch 920, val loss: 1.0121798515319824
Epoch 930, training loss: 0.06900804489850998 = 0.004064163658767939 + 0.01 * 6.494388103485107
Epoch 930, val loss: 1.0164990425109863
Epoch 940, training loss: 0.06874651461839676 = 0.003954686224460602 + 0.01 * 6.479183197021484
Epoch 940, val loss: 1.0207914113998413
Epoch 950, training loss: 0.06879273802042007 = 0.00385050056502223 + 0.01 * 6.4942240715026855
Epoch 950, val loss: 1.0248878002166748
Epoch 960, training loss: 0.0684974268078804 = 0.0037514010909944773 + 0.01 * 6.474602222442627
Epoch 960, val loss: 1.0290212631225586
Epoch 970, training loss: 0.06857044249773026 = 0.003657037392258644 + 0.01 * 6.4913411140441895
Epoch 970, val loss: 1.0330281257629395
Epoch 980, training loss: 0.06838014721870422 = 0.003566533327102661 + 0.01 * 6.4813618659973145
Epoch 980, val loss: 1.0369927883148193
Epoch 990, training loss: 0.06836900860071182 = 0.003479763399809599 + 0.01 * 6.488924503326416
Epoch 990, val loss: 1.040881872177124
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.4133
Flip ASR: 0.3733/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.022648811340332 = 1.9389101266860962 + 0.01 * 8.373872756958008
Epoch 0, val loss: 1.9398735761642456
Epoch 10, training loss: 2.0122439861297607 = 1.9285056591033936 + 0.01 * 8.373823165893555
Epoch 10, val loss: 1.9291653633117676
Epoch 20, training loss: 2.000278949737549 = 1.9165427684783936 + 0.01 * 8.373611450195312
Epoch 20, val loss: 1.9167568683624268
Epoch 30, training loss: 1.984421968460083 = 1.9006917476654053 + 0.01 * 8.373017311096191
Epoch 30, val loss: 1.9003725051879883
Epoch 40, training loss: 1.9618208408355713 = 1.8781148195266724 + 0.01 * 8.370604515075684
Epoch 40, val loss: 1.8773672580718994
Epoch 50, training loss: 1.9297409057617188 = 1.8461993932724 + 0.01 * 8.354146957397461
Epoch 50, val loss: 1.845888376235962
Epoch 60, training loss: 1.8876219987869263 = 1.80523681640625 + 0.01 * 8.238513946533203
Epoch 60, val loss: 1.8078526258468628
Epoch 70, training loss: 1.8366868495941162 = 1.7597180604934692 + 0.01 * 7.696880340576172
Epoch 70, val loss: 1.7687993049621582
Epoch 80, training loss: 1.780210256576538 = 1.7062619924545288 + 0.01 * 7.3948235511779785
Epoch 80, val loss: 1.723752737045288
Epoch 90, training loss: 1.7056152820587158 = 1.6334196329116821 + 0.01 * 7.219570636749268
Epoch 90, val loss: 1.6617094278335571
Epoch 100, training loss: 1.608688473701477 = 1.5369861125946045 + 0.01 * 7.170238971710205
Epoch 100, val loss: 1.5814661979675293
Epoch 110, training loss: 1.4903815984725952 = 1.4190574884414673 + 0.01 * 7.132415294647217
Epoch 110, val loss: 1.4861054420471191
Epoch 120, training loss: 1.3619126081466675 = 1.291100263595581 + 0.01 * 7.081236839294434
Epoch 120, val loss: 1.3836827278137207
Epoch 130, training loss: 1.2372184991836548 = 1.1669743061065674 + 0.01 * 7.024416923522949
Epoch 130, val loss: 1.2870680093765259
Epoch 140, training loss: 1.1252859830856323 = 1.0555453300476074 + 0.01 * 6.974061965942383
Epoch 140, val loss: 1.2018686532974243
Epoch 150, training loss: 1.0273804664611816 = 0.9579429626464844 + 0.01 * 6.943756580352783
Epoch 150, val loss: 1.1283601522445679
Epoch 160, training loss: 0.940876841545105 = 0.8716065883636475 + 0.01 * 6.927027702331543
Epoch 160, val loss: 1.0647461414337158
Epoch 170, training loss: 0.8639658093452454 = 0.794789731502533 + 0.01 * 6.917609691619873
Epoch 170, val loss: 1.0098012685775757
Epoch 180, training loss: 0.7959021329879761 = 0.7267645597457886 + 0.01 * 6.913754940032959
Epoch 180, val loss: 0.9625568389892578
Epoch 190, training loss: 0.735539436340332 = 0.6664177179336548 + 0.01 * 6.912174701690674
Epoch 190, val loss: 0.9219889044761658
Epoch 200, training loss: 0.680678129196167 = 0.6115692257881165 + 0.01 * 6.910887718200684
Epoch 200, val loss: 0.8866491317749023
Epoch 210, training loss: 0.6289321780204773 = 0.5598382949829102 + 0.01 * 6.909389019012451
Epoch 210, val loss: 0.8549922704696655
Epoch 220, training loss: 0.5784913301467896 = 0.5094166994094849 + 0.01 * 6.907462120056152
Epoch 220, val loss: 0.8257686495780945
Epoch 230, training loss: 0.5285744667053223 = 0.45952317118644714 + 0.01 * 6.90513277053833
Epoch 230, val loss: 0.7986709475517273
Epoch 240, training loss: 0.479458749294281 = 0.41043204069137573 + 0.01 * 6.90267276763916
Epoch 240, val loss: 0.7744309902191162
Epoch 250, training loss: 0.4320133626461029 = 0.36301809549331665 + 0.01 * 6.899527072906494
Epoch 250, val loss: 0.7541239857673645
Epoch 260, training loss: 0.3872354328632355 = 0.3182724118232727 + 0.01 * 6.89630126953125
Epoch 260, val loss: 0.7380715012550354
Epoch 270, training loss: 0.3458441495895386 = 0.276917964220047 + 0.01 * 6.892620086669922
Epoch 270, val loss: 0.7260645627975464
Epoch 280, training loss: 0.30824437737464905 = 0.2393569052219391 + 0.01 * 6.8887481689453125
Epoch 280, val loss: 0.7176938056945801
Epoch 290, training loss: 0.27469485998153687 = 0.20585817098617554 + 0.01 * 6.883669376373291
Epoch 290, val loss: 0.7127832770347595
Epoch 300, training loss: 0.24537165462970734 = 0.17659132182598114 + 0.01 * 6.878033638000488
Epoch 300, val loss: 0.7111740708351135
Epoch 310, training loss: 0.22026383876800537 = 0.15155500173568726 + 0.01 * 6.870882987976074
Epoch 310, val loss: 0.7129687666893005
Epoch 320, training loss: 0.19907255470752716 = 0.1304498165845871 + 0.01 * 6.862274169921875
Epoch 320, val loss: 0.7178927063941956
Epoch 330, training loss: 0.18134650588035583 = 0.11280590295791626 + 0.01 * 6.854060649871826
Epoch 330, val loss: 0.7257022857666016
Epoch 340, training loss: 0.16658788919448853 = 0.09811742603778839 + 0.01 * 6.847045421600342
Epoch 340, val loss: 0.7357373237609863
Epoch 350, training loss: 0.15428990125656128 = 0.08587335050106049 + 0.01 * 6.8416547775268555
Epoch 350, val loss: 0.7475218176841736
Epoch 360, training loss: 0.1439610719680786 = 0.07562901079654694 + 0.01 * 6.833207130432129
Epoch 360, val loss: 0.7604761719703674
Epoch 370, training loss: 0.13526380062103271 = 0.06700005382299423 + 0.01 * 6.826375484466553
Epoch 370, val loss: 0.774224579334259
Epoch 380, training loss: 0.12788176536560059 = 0.05968531221151352 + 0.01 * 6.819644927978516
Epoch 380, val loss: 0.7884100079536438
Epoch 390, training loss: 0.12159498780965805 = 0.05343863368034363 + 0.01 * 6.815635681152344
Epoch 390, val loss: 0.8027436137199402
Epoch 400, training loss: 0.11619348078966141 = 0.04807288199663162 + 0.01 * 6.8120598793029785
Epoch 400, val loss: 0.8171066045761108
Epoch 410, training loss: 0.11153054237365723 = 0.043437544256448746 + 0.01 * 6.809299468994141
Epoch 410, val loss: 0.8313285112380981
Epoch 420, training loss: 0.10744154453277588 = 0.03941294923424721 + 0.01 * 6.802859306335449
Epoch 420, val loss: 0.8454014658927917
Epoch 430, training loss: 0.10385751724243164 = 0.03590019792318344 + 0.01 * 6.795732021331787
Epoch 430, val loss: 0.8591703176498413
Epoch 440, training loss: 0.10075781494379044 = 0.03281782567501068 + 0.01 * 6.793999195098877
Epoch 440, val loss: 0.8726072907447815
Epoch 450, training loss: 0.0979858785867691 = 0.030098799616098404 + 0.01 * 6.788708209991455
Epoch 450, val loss: 0.8857656121253967
Epoch 460, training loss: 0.09550141543149948 = 0.027689775452017784 + 0.01 * 6.781164169311523
Epoch 460, val loss: 0.898543655872345
Epoch 470, training loss: 0.09354868531227112 = 0.025547504425048828 + 0.01 * 6.800118446350098
Epoch 470, val loss: 0.9109634757041931
Epoch 480, training loss: 0.09146038442850113 = 0.023640170693397522 + 0.01 * 6.782021522521973
Epoch 480, val loss: 0.9230613708496094
Epoch 490, training loss: 0.08963176608085632 = 0.02193371392786503 + 0.01 * 6.769805908203125
Epoch 490, val loss: 0.9348140954971313
Epoch 500, training loss: 0.08804065734148026 = 0.02040020376443863 + 0.01 * 6.764045238494873
Epoch 500, val loss: 0.9461970329284668
Epoch 510, training loss: 0.08663270622491837 = 0.019017508253455162 + 0.01 * 6.761519432067871
Epoch 510, val loss: 0.957273006439209
Epoch 520, training loss: 0.08530393242835999 = 0.017766546458005905 + 0.01 * 6.753739356994629
Epoch 520, val loss: 0.9680858254432678
Epoch 530, training loss: 0.08413149416446686 = 0.016632694751024246 + 0.01 * 6.749879837036133
Epoch 530, val loss: 0.9786434769630432
Epoch 540, training loss: 0.08304668962955475 = 0.015603633597493172 + 0.01 * 6.7443060874938965
Epoch 540, val loss: 0.9888721704483032
Epoch 550, training loss: 0.08210872113704681 = 0.014668197371065617 + 0.01 * 6.744052886962891
Epoch 550, val loss: 0.9988788962364197
Epoch 560, training loss: 0.08117213845252991 = 0.013814629055559635 + 0.01 * 6.735751152038574
Epoch 560, val loss: 1.008538842201233
Epoch 570, training loss: 0.08037988841533661 = 0.01303587481379509 + 0.01 * 6.734401702880859
Epoch 570, val loss: 1.0179857015609741
Epoch 580, training loss: 0.07962071895599365 = 0.01232182327657938 + 0.01 * 6.729889869689941
Epoch 580, val loss: 1.0272159576416016
Epoch 590, training loss: 0.07888760417699814 = 0.011665732599794865 + 0.01 * 6.722187519073486
Epoch 590, val loss: 1.0361889600753784
Epoch 600, training loss: 0.07825110852718353 = 0.011062437668442726 + 0.01 * 6.718867778778076
Epoch 600, val loss: 1.044979214668274
Epoch 610, training loss: 0.07764318585395813 = 0.010506604798138142 + 0.01 * 6.713657855987549
Epoch 610, val loss: 1.0535447597503662
Epoch 620, training loss: 0.0770488828420639 = 0.009992646053433418 + 0.01 * 6.705624103546143
Epoch 620, val loss: 1.061859369277954
Epoch 630, training loss: 0.07663293927907944 = 0.00951711367815733 + 0.01 * 6.711583137512207
Epoch 630, val loss: 1.0700287818908691
Epoch 640, training loss: 0.0760413110256195 = 0.009076060727238655 + 0.01 * 6.696525573730469
Epoch 640, val loss: 1.0779736042022705
Epoch 650, training loss: 0.0760393738746643 = 0.00866651814430952 + 0.01 * 6.73728609085083
Epoch 650, val loss: 1.0857964754104614
Epoch 660, training loss: 0.07521429657936096 = 0.008286559954285622 + 0.01 * 6.692773818969727
Epoch 660, val loss: 1.093278169631958
Epoch 670, training loss: 0.07471191138029099 = 0.007932673208415508 + 0.01 * 6.677924156188965
Epoch 670, val loss: 1.1006368398666382
Epoch 680, training loss: 0.07438919693231583 = 0.007602123077958822 + 0.01 * 6.678707599639893
Epoch 680, val loss: 1.1078674793243408
Epoch 690, training loss: 0.07401315867900848 = 0.00729292631149292 + 0.01 * 6.672023296356201
Epoch 690, val loss: 1.1148459911346436
Epoch 700, training loss: 0.0737256109714508 = 0.007004105020314455 + 0.01 * 6.672150611877441
Epoch 700, val loss: 1.1217412948608398
Epoch 710, training loss: 0.07329532504081726 = 0.006733578164130449 + 0.01 * 6.656174659729004
Epoch 710, val loss: 1.128442645072937
Epoch 720, training loss: 0.07295694202184677 = 0.006479105446487665 + 0.01 * 6.6477837562561035
Epoch 720, val loss: 1.1350194215774536
Epoch 730, training loss: 0.07276857644319534 = 0.006239799782633781 + 0.01 * 6.652877330780029
Epoch 730, val loss: 1.1414783000946045
Epoch 740, training loss: 0.07245234400033951 = 0.0060148462653160095 + 0.01 * 6.643749713897705
Epoch 740, val loss: 1.14772629737854
Epoch 750, training loss: 0.07227133959531784 = 0.005802704021334648 + 0.01 * 6.6468634605407715
Epoch 750, val loss: 1.153878927230835
Epoch 760, training loss: 0.07190516591072083 = 0.0056026531383395195 + 0.01 * 6.630251407623291
Epoch 760, val loss: 1.159916877746582
Epoch 770, training loss: 0.07156184315681458 = 0.005412818863987923 + 0.01 * 6.614902496337891
Epoch 770, val loss: 1.1659040451049805
Epoch 780, training loss: 0.07165420800447464 = 0.005232805386185646 + 0.01 * 6.642140865325928
Epoch 780, val loss: 1.1718255281448364
Epoch 790, training loss: 0.071159727871418 = 0.005062030162662268 + 0.01 * 6.60977029800415
Epoch 790, val loss: 1.1775485277175903
Epoch 800, training loss: 0.07098932564258575 = 0.00489985104650259 + 0.01 * 6.60894775390625
Epoch 800, val loss: 1.1833131313323975
Epoch 810, training loss: 0.07066300511360168 = 0.004745262209326029 + 0.01 * 6.591774940490723
Epoch 810, val loss: 1.188965916633606
Epoch 820, training loss: 0.07055774331092834 = 0.004597769118845463 + 0.01 * 6.595997333526611
Epoch 820, val loss: 1.1946496963500977
Epoch 830, training loss: 0.07043079286813736 = 0.004457490984350443 + 0.01 * 6.597330570220947
Epoch 830, val loss: 1.200214147567749
Epoch 840, training loss: 0.07021297514438629 = 0.0043235840275883675 + 0.01 * 6.5889387130737305
Epoch 840, val loss: 1.2056993246078491
Epoch 850, training loss: 0.06992759555578232 = 0.004196109715849161 + 0.01 * 6.573148727416992
Epoch 850, val loss: 1.2110837697982788
Epoch 860, training loss: 0.06993985176086426 = 0.004074295051395893 + 0.01 * 6.586555480957031
Epoch 860, val loss: 1.2163870334625244
Epoch 870, training loss: 0.06955726444721222 = 0.003958900459110737 + 0.01 * 6.559836387634277
Epoch 870, val loss: 1.221619725227356
Epoch 880, training loss: 0.0696512833237648 = 0.00384857808239758 + 0.01 * 6.580271244049072
Epoch 880, val loss: 1.2268636226654053
Epoch 890, training loss: 0.06959731131792068 = 0.0037433174438774586 + 0.01 * 6.585399150848389
Epoch 890, val loss: 1.231826901435852
Epoch 900, training loss: 0.06918119639158249 = 0.00364268128760159 + 0.01 * 6.553852081298828
Epoch 900, val loss: 1.236877202987671
Epoch 910, training loss: 0.06960967928171158 = 0.0035464325919747353 + 0.01 * 6.606324672698975
Epoch 910, val loss: 1.2418088912963867
Epoch 920, training loss: 0.06885606795549393 = 0.0034547175746411085 + 0.01 * 6.540135383605957
Epoch 920, val loss: 1.246656894683838
Epoch 930, training loss: 0.06873028725385666 = 0.003366494085639715 + 0.01 * 6.536379814147949
Epoch 930, val loss: 1.2515530586242676
Epoch 940, training loss: 0.06865842640399933 = 0.0032818089239299297 + 0.01 * 6.537662029266357
Epoch 940, val loss: 1.2562602758407593
Epoch 950, training loss: 0.06847068667411804 = 0.003200739389285445 + 0.01 * 6.526994705200195
Epoch 950, val loss: 1.260991096496582
Epoch 960, training loss: 0.06834279745817184 = 0.0031231148168444633 + 0.01 * 6.521968841552734
Epoch 960, val loss: 1.2656548023223877
Epoch 970, training loss: 0.06849345564842224 = 0.003048750339075923 + 0.01 * 6.544471263885498
Epoch 970, val loss: 1.270161747932434
Epoch 980, training loss: 0.06821682304143906 = 0.0029777418822050095 + 0.01 * 6.5239081382751465
Epoch 980, val loss: 1.2745288610458374
Epoch 990, training loss: 0.06809751689434052 = 0.0029095709323883057 + 0.01 * 6.518794536590576
Epoch 990, val loss: 1.278967261314392
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.9188
Flip ASR: 0.9022/225 nodes
The final ASR:0.63469, 0.21110, Accuracy:0.82963, 0.01090
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11582])
remove edge: torch.Size([2, 9450])
updated graph: torch.Size([2, 10476])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.97663, 0.00627, Accuracy:0.83210, 0.00698
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0375397205352783 = 1.9538004398345947 + 0.01 * 8.373924255371094
Epoch 0, val loss: 1.9600104093551636
Epoch 10, training loss: 2.027259588241577 = 1.9435207843780518 + 0.01 * 8.373872756958008
Epoch 10, val loss: 1.9495559930801392
Epoch 20, training loss: 2.0145440101623535 = 1.930807113647461 + 0.01 * 8.37369441986084
Epoch 20, val loss: 1.9365004301071167
Epoch 30, training loss: 1.9965484142303467 = 1.9128170013427734 + 0.01 * 8.373135566711426
Epoch 30, val loss: 1.918035864830017
Epoch 40, training loss: 1.969627857208252 = 1.8859248161315918 + 0.01 * 8.3703031539917
Epoch 40, val loss: 1.8909857273101807
Epoch 50, training loss: 1.931388258934021 = 1.8478996753692627 + 0.01 * 8.348862648010254
Epoch 50, val loss: 1.8547921180725098
Epoch 60, training loss: 1.8866358995437622 = 1.8043781518936157 + 0.01 * 8.22577953338623
Epoch 60, val loss: 1.8177895545959473
Epoch 70, training loss: 1.845878005027771 = 1.7656171321868896 + 0.01 * 8.026083946228027
Epoch 70, val loss: 1.7866188287734985
Epoch 80, training loss: 1.7975016832351685 = 1.7194746732711792 + 0.01 * 7.802698135375977
Epoch 80, val loss: 1.744971752166748
Epoch 90, training loss: 1.7313649654388428 = 1.6565687656402588 + 0.01 * 7.479623794555664
Epoch 90, val loss: 1.6905454397201538
Epoch 100, training loss: 1.6455519199371338 = 1.573277235031128 + 0.01 * 7.227470874786377
Epoch 100, val loss: 1.6215630769729614
Epoch 110, training loss: 1.546048879623413 = 1.474524736404419 + 0.01 * 7.1524152755737305
Epoch 110, val loss: 1.5392310619354248
Epoch 120, training loss: 1.4427310228347778 = 1.3716996908187866 + 0.01 * 7.1031341552734375
Epoch 120, val loss: 1.4560099840164185
Epoch 130, training loss: 1.3407179117202759 = 1.2700672149658203 + 0.01 * 7.065072536468506
Epoch 130, val loss: 1.3755104541778564
Epoch 140, training loss: 1.2404108047485352 = 1.170076608657837 + 0.01 * 7.03341817855835
Epoch 140, val loss: 1.298792839050293
Epoch 150, training loss: 1.143837809562683 = 1.073816180229187 + 0.01 * 7.002163887023926
Epoch 150, val loss: 1.2261803150177002
Epoch 160, training loss: 1.0535202026367188 = 0.9837849736213684 + 0.01 * 6.973518371582031
Epoch 160, val loss: 1.159013271331787
Epoch 170, training loss: 0.9698778390884399 = 0.9002851843833923 + 0.01 * 6.959268093109131
Epoch 170, val loss: 1.0971471071243286
Epoch 180, training loss: 0.8906996250152588 = 0.8212466239929199 + 0.01 * 6.9452996253967285
Epoch 180, val loss: 1.0387030839920044
Epoch 190, training loss: 0.8137804865837097 = 0.7444322109222412 + 0.01 * 6.934825420379639
Epoch 190, val loss: 0.9822102785110474
Epoch 200, training loss: 0.7385613918304443 = 0.6693150401115417 + 0.01 * 6.924635410308838
Epoch 200, val loss: 0.9268697500228882
Epoch 210, training loss: 0.6662316918373108 = 0.5970778465270996 + 0.01 * 6.915383338928223
Epoch 210, val loss: 0.8736118078231812
Epoch 220, training loss: 0.5986891984939575 = 0.529662549495697 + 0.01 * 6.9026618003845215
Epoch 220, val loss: 0.8244441747665405
Epoch 230, training loss: 0.5371749997138977 = 0.4682324528694153 + 0.01 * 6.894252777099609
Epoch 230, val loss: 0.7816081047058105
Epoch 240, training loss: 0.4814736247062683 = 0.41266530752182007 + 0.01 * 6.880833148956299
Epoch 240, val loss: 0.7457781434059143
Epoch 250, training loss: 0.4307665228843689 = 0.3619978427886963 + 0.01 * 6.876867294311523
Epoch 250, val loss: 0.716524064540863
Epoch 260, training loss: 0.3840252757072449 = 0.3153880536556244 + 0.01 * 6.86372184753418
Epoch 260, val loss: 0.6925832629203796
Epoch 270, training loss: 0.3411751985549927 = 0.27265316247940063 + 0.01 * 6.852205276489258
Epoch 270, val loss: 0.6731234788894653
Epoch 280, training loss: 0.3025544285774231 = 0.23413042724132538 + 0.01 * 6.84240198135376
Epoch 280, val loss: 0.6580994129180908
Epoch 290, training loss: 0.2688608765602112 = 0.2002960443496704 + 0.01 * 6.856484413146973
Epoch 290, val loss: 0.6476894021034241
Epoch 300, training loss: 0.23965485394001007 = 0.17129448056221008 + 0.01 * 6.8360371589660645
Epoch 300, val loss: 0.6419139504432678
Epoch 310, training loss: 0.21506038308143616 = 0.14678248763084412 + 0.01 * 6.827788829803467
Epoch 310, val loss: 0.6404386162757874
Epoch 320, training loss: 0.19443705677986145 = 0.12624438107013702 + 0.01 * 6.819267749786377
Epoch 320, val loss: 0.6429119110107422
Epoch 330, training loss: 0.1773148775100708 = 0.10911830514669418 + 0.01 * 6.819657325744629
Epoch 330, val loss: 0.6487822532653809
Epoch 340, training loss: 0.16292232275009155 = 0.09483201056718826 + 0.01 * 6.809031009674072
Epoch 340, val loss: 0.6574509143829346
Epoch 350, training loss: 0.15095171332359314 = 0.08286766707897186 + 0.01 * 6.808405876159668
Epoch 350, val loss: 0.668371856212616
Epoch 360, training loss: 0.1407662183046341 = 0.07280566543340683 + 0.01 * 6.796055316925049
Epoch 360, val loss: 0.6810529828071594
Epoch 370, training loss: 0.13229811191558838 = 0.06431403756141663 + 0.01 * 6.798407077789307
Epoch 370, val loss: 0.6949228048324585
Epoch 380, training loss: 0.1249462366104126 = 0.057112544775009155 + 0.01 * 6.783369064331055
Epoch 380, val loss: 0.7095351815223694
Epoch 390, training loss: 0.11869210004806519 = 0.05097552016377449 + 0.01 * 6.771657466888428
Epoch 390, val loss: 0.724573016166687
Epoch 400, training loss: 0.11361493170261383 = 0.045723672956228256 + 0.01 * 6.789125442504883
Epoch 400, val loss: 0.7397071719169617
Epoch 410, training loss: 0.1088639497756958 = 0.041222210973501205 + 0.01 * 6.76417350769043
Epoch 410, val loss: 0.7546988725662231
Epoch 420, training loss: 0.1049012839794159 = 0.037339985370635986 + 0.01 * 6.756129741668701
Epoch 420, val loss: 0.7694666385650635
Epoch 430, training loss: 0.10159783065319061 = 0.033971939235925674 + 0.01 * 6.762589454650879
Epoch 430, val loss: 0.7838965654373169
Epoch 440, training loss: 0.09849303960800171 = 0.03103717602789402 + 0.01 * 6.74558687210083
Epoch 440, val loss: 0.7979252338409424
Epoch 450, training loss: 0.09583058208227158 = 0.028466111049056053 + 0.01 * 6.736446857452393
Epoch 450, val loss: 0.8115488886833191
Epoch 460, training loss: 0.0936339795589447 = 0.02620161697268486 + 0.01 * 6.743236541748047
Epoch 460, val loss: 0.8247408270835876
Epoch 470, training loss: 0.09140751510858536 = 0.02419830858707428 + 0.01 * 6.720920562744141
Epoch 470, val loss: 0.8375267386436462
Epoch 480, training loss: 0.08958360552787781 = 0.02241910621523857 + 0.01 * 6.7164506912231445
Epoch 480, val loss: 0.8498926162719727
Epoch 490, training loss: 0.0879596471786499 = 0.02083222009241581 + 0.01 * 6.712742805480957
Epoch 490, val loss: 0.8618761301040649
Epoch 500, training loss: 0.08646457642316818 = 0.019407890737056732 + 0.01 * 6.705668926239014
Epoch 500, val loss: 0.8735215663909912
Epoch 510, training loss: 0.08515529334545135 = 0.018126966431736946 + 0.01 * 6.7028326988220215
Epoch 510, val loss: 0.8847377896308899
Epoch 520, training loss: 0.08388427644968033 = 0.01697090081870556 + 0.01 * 6.691337585449219
Epoch 520, val loss: 0.8956610560417175
Epoch 530, training loss: 0.08277605473995209 = 0.015923211351037025 + 0.01 * 6.685284614562988
Epoch 530, val loss: 0.9062604308128357
Epoch 540, training loss: 0.08189710974693298 = 0.014972522854804993 + 0.01 * 6.6924591064453125
Epoch 540, val loss: 0.9165366888046265
Epoch 550, training loss: 0.08080737292766571 = 0.014107939787209034 + 0.01 * 6.669943332672119
Epoch 550, val loss: 0.9264564514160156
Epoch 560, training loss: 0.08004762977361679 = 0.013318284414708614 + 0.01 * 6.672934532165527
Epoch 560, val loss: 0.9361640810966492
Epoch 570, training loss: 0.07914663851261139 = 0.012595375999808311 + 0.01 * 6.655126571655273
Epoch 570, val loss: 0.94557785987854
Epoch 580, training loss: 0.07851266860961914 = 0.011932415887713432 + 0.01 * 6.65802526473999
Epoch 580, val loss: 0.9547228813171387
Epoch 590, training loss: 0.07793434709310532 = 0.01132290344685316 + 0.01 * 6.661144733428955
Epoch 590, val loss: 0.9636695981025696
Epoch 600, training loss: 0.07722467929124832 = 0.01076058205217123 + 0.01 * 6.64640998840332
Epoch 600, val loss: 0.9723194241523743
Epoch 610, training loss: 0.07679745554924011 = 0.010241318494081497 + 0.01 * 6.655613422393799
Epoch 610, val loss: 0.9808184504508972
Epoch 620, training loss: 0.07621978223323822 = 0.009760914370417595 + 0.01 * 6.645886421203613
Epoch 620, val loss: 0.988983154296875
Epoch 630, training loss: 0.07556837797164917 = 0.009316151030361652 + 0.01 * 6.625222682952881
Epoch 630, val loss: 0.9970501661300659
Epoch 640, training loss: 0.0752086415886879 = 0.008902549743652344 + 0.01 * 6.630609512329102
Epoch 640, val loss: 1.0048561096191406
Epoch 650, training loss: 0.07492374628782272 = 0.008517657406628132 + 0.01 * 6.640608787536621
Epoch 650, val loss: 1.012498140335083
Epoch 660, training loss: 0.07442822307348251 = 0.00815960206091404 + 0.01 * 6.626862525939941
Epoch 660, val loss: 1.0199161767959595
Epoch 670, training loss: 0.07403246313333511 = 0.00782544631510973 + 0.01 * 6.620701313018799
Epoch 670, val loss: 1.0271583795547485
Epoch 680, training loss: 0.07374554872512817 = 0.007513107266277075 + 0.01 * 6.623244285583496
Epoch 680, val loss: 1.034226417541504
Epoch 690, training loss: 0.07324052602052689 = 0.0072210100479424 + 0.01 * 6.601951599121094
Epoch 690, val loss: 1.0411558151245117
Epoch 700, training loss: 0.07318035513162613 = 0.006946912966668606 + 0.01 * 6.6233439445495605
Epoch 700, val loss: 1.0479071140289307
Epoch 710, training loss: 0.07267945259809494 = 0.006689381320029497 + 0.01 * 6.5990071296691895
Epoch 710, val loss: 1.054549217224121
Epoch 720, training loss: 0.0723201334476471 = 0.006447662133723497 + 0.01 * 6.587246894836426
Epoch 720, val loss: 1.060975432395935
Epoch 730, training loss: 0.07218770682811737 = 0.006219801027327776 + 0.01 * 6.596790313720703
Epoch 730, val loss: 1.0672879219055176
Epoch 740, training loss: 0.07210198789834976 = 0.006005787290632725 + 0.01 * 6.609620094299316
Epoch 740, val loss: 1.0734730958938599
Epoch 750, training loss: 0.07154639810323715 = 0.005803835578262806 + 0.01 * 6.57425594329834
Epoch 750, val loss: 1.079458475112915
Epoch 760, training loss: 0.07142765820026398 = 0.005613233428448439 + 0.01 * 6.581442832946777
Epoch 760, val loss: 1.0853478908538818
Epoch 770, training loss: 0.07124965637922287 = 0.005432969890534878 + 0.01 * 6.581668376922607
Epoch 770, val loss: 1.0911680459976196
Epoch 780, training loss: 0.07091164588928223 = 0.005262263119220734 + 0.01 * 6.564938545227051
Epoch 780, val loss: 1.0967785120010376
Epoch 790, training loss: 0.07077394425868988 = 0.005100586451590061 + 0.01 * 6.567336082458496
Epoch 790, val loss: 1.1023107767105103
Epoch 800, training loss: 0.07064832001924515 = 0.004947176668792963 + 0.01 * 6.5701141357421875
Epoch 800, val loss: 1.1077531576156616
Epoch 810, training loss: 0.07034385204315186 = 0.004801655653864145 + 0.01 * 6.554220199584961
Epoch 810, val loss: 1.113002896308899
Epoch 820, training loss: 0.07023366540670395 = 0.004663144703954458 + 0.01 * 6.557051658630371
Epoch 820, val loss: 1.1182010173797607
Epoch 830, training loss: 0.07011227309703827 = 0.004531576298177242 + 0.01 * 6.558069705963135
Epoch 830, val loss: 1.1232930421829224
Epoch 840, training loss: 0.06987917423248291 = 0.004406152293086052 + 0.01 * 6.54730224609375
Epoch 840, val loss: 1.1283187866210938
Epoch 850, training loss: 0.06990715861320496 = 0.004286930896341801 + 0.01 * 6.562023162841797
Epoch 850, val loss: 1.133209228515625
Epoch 860, training loss: 0.06957706809043884 = 0.004173551220446825 + 0.01 * 6.540351867675781
Epoch 860, val loss: 1.137959599494934
Epoch 870, training loss: 0.06962408870458603 = 0.004065115470439196 + 0.01 * 6.555897235870361
Epoch 870, val loss: 1.1427569389343262
Epoch 880, training loss: 0.06925645470619202 = 0.0039616115391254425 + 0.01 * 6.529484272003174
Epoch 880, val loss: 1.1473572254180908
Epoch 890, training loss: 0.0691886767745018 = 0.0038625975139439106 + 0.01 * 6.5326080322265625
Epoch 890, val loss: 1.1519176959991455
Epoch 900, training loss: 0.06913939863443375 = 0.0037680959794670343 + 0.01 * 6.537130832672119
Epoch 900, val loss: 1.156400203704834
Epoch 910, training loss: 0.06894214451313019 = 0.003677696455270052 + 0.01 * 6.526444911956787
Epoch 910, val loss: 1.1607589721679688
Epoch 920, training loss: 0.06886899471282959 = 0.003590866457670927 + 0.01 * 6.527812957763672
Epoch 920, val loss: 1.1650941371917725
Epoch 930, training loss: 0.06901168823242188 = 0.0035080101806670427 + 0.01 * 6.550368309020996
Epoch 930, val loss: 1.1692779064178467
Epoch 940, training loss: 0.06859206408262253 = 0.0034285092260688543 + 0.01 * 6.516355514526367
Epoch 940, val loss: 1.1735163927078247
Epoch 950, training loss: 0.06860877573490143 = 0.003352195955812931 + 0.01 * 6.525658130645752
Epoch 950, val loss: 1.1775890588760376
Epoch 960, training loss: 0.0685049444437027 = 0.003279190044850111 + 0.01 * 6.522575855255127
Epoch 960, val loss: 1.1816165447235107
Epoch 970, training loss: 0.06821787357330322 = 0.003208921989426017 + 0.01 * 6.5008955001831055
Epoch 970, val loss: 1.185593843460083
Epoch 980, training loss: 0.0682484358549118 = 0.0031413347460329533 + 0.01 * 6.5107102394104
Epoch 980, val loss: 1.1894057989120483
Epoch 990, training loss: 0.06835372000932693 = 0.0030766210984438658 + 0.01 * 6.527710437774658
Epoch 990, val loss: 1.1931912899017334
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.6052
Flip ASR: 0.5289/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.02650785446167 = 1.9427686929702759 + 0.01 * 8.37390422821045
Epoch 0, val loss: 1.9449167251586914
Epoch 10, training loss: 2.0167267322540283 = 1.9329882860183716 + 0.01 * 8.373836517333984
Epoch 10, val loss: 1.9339938163757324
Epoch 20, training loss: 2.0048584938049316 = 1.921122431755066 + 0.01 * 8.373602867126465
Epoch 20, val loss: 1.9205561876296997
Epoch 30, training loss: 1.988450288772583 = 1.9047216176986694 + 0.01 * 8.372872352600098
Epoch 30, val loss: 1.901879906654358
Epoch 40, training loss: 1.964530348777771 = 1.88083815574646 + 0.01 * 8.36921501159668
Epoch 40, val loss: 1.8747955560684204
Epoch 50, training loss: 1.930643081665039 = 1.8471996784210205 + 0.01 * 8.344335556030273
Epoch 50, val loss: 1.837777853012085
Epoch 60, training loss: 1.8881911039352417 = 1.80615234375 + 0.01 * 8.203876495361328
Epoch 60, val loss: 1.7959438562393188
Epoch 70, training loss: 1.8444768190383911 = 1.7642617225646973 + 0.01 * 8.02150821685791
Epoch 70, val loss: 1.7580223083496094
Epoch 80, training loss: 1.7918579578399658 = 1.7147072553634644 + 0.01 * 7.715071678161621
Epoch 80, val loss: 1.716241717338562
Epoch 90, training loss: 1.7216732501983643 = 1.6471854448318481 + 0.01 * 7.4487833976745605
Epoch 90, val loss: 1.6589550971984863
Epoch 100, training loss: 1.630403995513916 = 1.557600498199463 + 0.01 * 7.280344486236572
Epoch 100, val loss: 1.5836669206619263
Epoch 110, training loss: 1.521965742111206 = 1.4499735832214355 + 0.01 * 7.199213981628418
Epoch 110, val loss: 1.4936200380325317
Epoch 120, training loss: 1.4056713581085205 = 1.3342266082763672 + 0.01 * 7.144476890563965
Epoch 120, val loss: 1.4013272523880005
Epoch 130, training loss: 1.2890558242797852 = 1.2180148363113403 + 0.01 * 7.104104995727539
Epoch 130, val loss: 1.3125073909759521
Epoch 140, training loss: 1.1776145696640015 = 1.1068435907363892 + 0.01 * 7.0770955085754395
Epoch 140, val loss: 1.229184627532959
Epoch 150, training loss: 1.0757944583892822 = 1.0051875114440918 + 0.01 * 7.060695171356201
Epoch 150, val loss: 1.1541531085968018
Epoch 160, training loss: 0.9845401644706726 = 0.9140979647636414 + 0.01 * 7.044217586517334
Epoch 160, val loss: 1.0871330499649048
Epoch 170, training loss: 0.902228832244873 = 0.8319823741912842 + 0.01 * 7.024644374847412
Epoch 170, val loss: 1.0266832113265991
Epoch 180, training loss: 0.8267585039138794 = 0.7567164897918701 + 0.01 * 7.004202365875244
Epoch 180, val loss: 0.9716536998748779
Epoch 190, training loss: 0.7569592595100403 = 0.6870928406715393 + 0.01 * 6.986640930175781
Epoch 190, val loss: 0.9216972589492798
Epoch 200, training loss: 0.692741334438324 = 0.6230117082595825 + 0.01 * 6.97296142578125
Epoch 200, val loss: 0.876800000667572
Epoch 210, training loss: 0.6337608098983765 = 0.5641471147537231 + 0.01 * 6.961371421813965
Epoch 210, val loss: 0.8368967771530151
Epoch 220, training loss: 0.5793672204017639 = 0.5099083185195923 + 0.01 * 6.945892333984375
Epoch 220, val loss: 0.8020991086959839
Epoch 230, training loss: 0.5290137529373169 = 0.45956432819366455 + 0.01 * 6.944945335388184
Epoch 230, val loss: 0.7723467350006104
Epoch 240, training loss: 0.4817495048046112 = 0.412503182888031 + 0.01 * 6.924632549285889
Epoch 240, val loss: 0.7477094531059265
Epoch 250, training loss: 0.4371355175971985 = 0.3680209815502167 + 0.01 * 6.9114532470703125
Epoch 250, val loss: 0.7281702756881714
Epoch 260, training loss: 0.394586443901062 = 0.32558804750442505 + 0.01 * 6.899839878082275
Epoch 260, val loss: 0.7135422229766846
Epoch 270, training loss: 0.35400477051734924 = 0.28508904576301575 + 0.01 * 6.891571998596191
Epoch 270, val loss: 0.7031356692314148
Epoch 280, training loss: 0.3158523738384247 = 0.24694323539733887 + 0.01 * 6.890913486480713
Epoch 280, val loss: 0.6963663101196289
Epoch 290, training loss: 0.2807333171367645 = 0.21190786361694336 + 0.01 * 6.882545471191406
Epoch 290, val loss: 0.6932075023651123
Epoch 300, training loss: 0.24964430928230286 = 0.18088118731975555 + 0.01 * 6.876313209533691
Epoch 300, val loss: 0.693541944026947
Epoch 310, training loss: 0.22299857437610626 = 0.15429167449474335 + 0.01 * 6.87069034576416
Epoch 310, val loss: 0.6973249316215515
Epoch 320, training loss: 0.20087331533432007 = 0.13201501965522766 + 0.01 * 6.885830402374268
Epoch 320, val loss: 0.7042644619941711
Epoch 330, training loss: 0.18223537504673004 = 0.11358945816755295 + 0.01 * 6.864591598510742
Epoch 330, val loss: 0.713793158531189
Epoch 340, training loss: 0.1669580340385437 = 0.09833972901105881 + 0.01 * 6.86182975769043
Epoch 340, val loss: 0.7253310084342957
Epoch 350, training loss: 0.1542075127363205 = 0.08566071838140488 + 0.01 * 6.854679584503174
Epoch 350, val loss: 0.7382931113243103
Epoch 360, training loss: 0.14353758096694946 = 0.07505211234092712 + 0.01 * 6.848546981811523
Epoch 360, val loss: 0.7521876692771912
Epoch 370, training loss: 0.13459539413452148 = 0.066108338534832 + 0.01 * 6.848705291748047
Epoch 370, val loss: 0.7667120099067688
Epoch 380, training loss: 0.12701281905174255 = 0.05853308364748955 + 0.01 * 6.8479743003845215
Epoch 380, val loss: 0.781554639339447
Epoch 390, training loss: 0.1204526424407959 = 0.05208338052034378 + 0.01 * 6.836925983428955
Epoch 390, val loss: 0.7965177297592163
Epoch 400, training loss: 0.11488326638936996 = 0.04655947536230087 + 0.01 * 6.832379341125488
Epoch 400, val loss: 0.8114842176437378
Epoch 410, training loss: 0.11017535626888275 = 0.041807204484939575 + 0.01 * 6.836815357208252
Epoch 410, val loss: 0.8264588713645935
Epoch 420, training loss: 0.1059342548251152 = 0.03770718723535538 + 0.01 * 6.822707176208496
Epoch 420, val loss: 0.8412065505981445
Epoch 430, training loss: 0.10231133550405502 = 0.03415250778198242 + 0.01 * 6.815883159637451
Epoch 430, val loss: 0.8557134866714478
Epoch 440, training loss: 0.09920559823513031 = 0.031051991507411003 + 0.01 * 6.8153605461120605
Epoch 440, val loss: 0.8698750734329224
Epoch 450, training loss: 0.09655246138572693 = 0.028343312442302704 + 0.01 * 6.820915222167969
Epoch 450, val loss: 0.8838017582893372
Epoch 460, training loss: 0.09401380270719528 = 0.025966860353946686 + 0.01 * 6.804694652557373
Epoch 460, val loss: 0.8973568677902222
Epoch 470, training loss: 0.09182852506637573 = 0.02387133799493313 + 0.01 * 6.795718669891357
Epoch 470, val loss: 0.9106655120849609
Epoch 480, training loss: 0.08991631865501404 = 0.022014278918504715 + 0.01 * 6.790204048156738
Epoch 480, val loss: 0.9236255288124084
Epoch 490, training loss: 0.08832676708698273 = 0.020363284274935722 + 0.01 * 6.796348571777344
Epoch 490, val loss: 0.9362309575080872
Epoch 500, training loss: 0.08676022291183472 = 0.0188914705067873 + 0.01 * 6.7868757247924805
Epoch 500, val loss: 0.948535144329071
Epoch 510, training loss: 0.085505411028862 = 0.017574498429894447 + 0.01 * 6.793091773986816
Epoch 510, val loss: 0.9604469537734985
Epoch 520, training loss: 0.08408678323030472 = 0.01639445126056671 + 0.01 * 6.769233703613281
Epoch 520, val loss: 0.9720258712768555
Epoch 530, training loss: 0.0829823762178421 = 0.015331190079450607 + 0.01 * 6.765118598937988
Epoch 530, val loss: 0.983269453048706
Epoch 540, training loss: 0.08201615512371063 = 0.01437053456902504 + 0.01 * 6.764562129974365
Epoch 540, val loss: 0.9942166805267334
Epoch 550, training loss: 0.08107571303844452 = 0.013500739820301533 + 0.01 * 6.7574968338012695
Epoch 550, val loss: 1.0048505067825317
Epoch 560, training loss: 0.08020773530006409 = 0.012709802947938442 + 0.01 * 6.74979305267334
Epoch 560, val loss: 1.0152363777160645
Epoch 570, training loss: 0.07944734394550323 = 0.011988242156803608 + 0.01 * 6.745910167694092
Epoch 570, val loss: 1.025309443473816
Epoch 580, training loss: 0.07880596816539764 = 0.01132876519113779 + 0.01 * 6.747720241546631
Epoch 580, val loss: 1.0351468324661255
Epoch 590, training loss: 0.07804294675588608 = 0.010725208558142185 + 0.01 * 6.73177433013916
Epoch 590, val loss: 1.044707179069519
Epoch 600, training loss: 0.07744216918945312 = 0.010170870460569859 + 0.01 * 6.727129936218262
Epoch 600, val loss: 1.0540484189987183
Epoch 610, training loss: 0.0770251676440239 = 0.009660814888775349 + 0.01 * 6.736435413360596
Epoch 610, val loss: 1.063096046447754
Epoch 620, training loss: 0.07648296654224396 = 0.009191782213747501 + 0.01 * 6.729118824005127
Epoch 620, val loss: 1.0719503164291382
Epoch 630, training loss: 0.07600361108779907 = 0.008758509531617165 + 0.01 * 6.724510669708252
Epoch 630, val loss: 1.0804945230484009
Epoch 640, training loss: 0.07544253766536713 = 0.008357138372957706 + 0.01 * 6.708540439605713
Epoch 640, val loss: 1.0889137983322144
Epoch 650, training loss: 0.07526850700378418 = 0.007984424941241741 + 0.01 * 6.728408336639404
Epoch 650, val loss: 1.0970994234085083
Epoch 660, training loss: 0.0746721550822258 = 0.007638679817318916 + 0.01 * 6.703347682952881
Epoch 660, val loss: 1.10512113571167
Epoch 670, training loss: 0.07424189150333405 = 0.007316585164517164 + 0.01 * 6.692531108856201
Epoch 670, val loss: 1.1129052639007568
Epoch 680, training loss: 0.07403714209794998 = 0.00701591232791543 + 0.01 * 6.702123165130615
Epoch 680, val loss: 1.1205337047576904
Epoch 690, training loss: 0.07361645996570587 = 0.006735564675182104 + 0.01 * 6.688089847564697
Epoch 690, val loss: 1.1279511451721191
Epoch 700, training loss: 0.07332372665405273 = 0.006473538465797901 + 0.01 * 6.685019016265869
Epoch 700, val loss: 1.1352254152297974
Epoch 710, training loss: 0.0729914978146553 = 0.006227933336049318 + 0.01 * 6.676356315612793
Epoch 710, val loss: 1.1423171758651733
Epoch 720, training loss: 0.0728687271475792 = 0.005997447296977043 + 0.01 * 6.687127590179443
Epoch 720, val loss: 1.149229884147644
Epoch 730, training loss: 0.07259118556976318 = 0.005781178362667561 + 0.01 * 6.681000709533691
Epoch 730, val loss: 1.1560418605804443
Epoch 740, training loss: 0.07222986966371536 = 0.005577815230935812 + 0.01 * 6.665205001831055
Epoch 740, val loss: 1.1626248359680176
Epoch 750, training loss: 0.07200013846158981 = 0.00538657559081912 + 0.01 * 6.661356449127197
Epoch 750, val loss: 1.1690583229064941
Epoch 760, training loss: 0.07175266742706299 = 0.0052061318419873714 + 0.01 * 6.654653549194336
Epoch 760, val loss: 1.1754013299942017
Epoch 770, training loss: 0.07173038274049759 = 0.00503559922799468 + 0.01 * 6.669478416442871
Epoch 770, val loss: 1.1816238164901733
Epoch 780, training loss: 0.07146792113780975 = 0.004874695558100939 + 0.01 * 6.659322261810303
Epoch 780, val loss: 1.187674880027771
Epoch 790, training loss: 0.07127171754837036 = 0.004722510930150747 + 0.01 * 6.65492057800293
Epoch 790, val loss: 1.1936360597610474
Epoch 800, training loss: 0.07113431394100189 = 0.004578459542244673 + 0.01 * 6.655585765838623
Epoch 800, val loss: 1.1993958950042725
Epoch 810, training loss: 0.0707772970199585 = 0.0044419290497899055 + 0.01 * 6.6335368156433105
Epoch 810, val loss: 1.2051360607147217
Epoch 820, training loss: 0.07070081681013107 = 0.0043123201467096806 + 0.01 * 6.63884973526001
Epoch 820, val loss: 1.2106646299362183
Epoch 830, training loss: 0.07049533724784851 = 0.004189286846667528 + 0.01 * 6.6306047439575195
Epoch 830, val loss: 1.2161937952041626
Epoch 840, training loss: 0.07043015956878662 = 0.004072369076311588 + 0.01 * 6.63577938079834
Epoch 840, val loss: 1.2215348482131958
Epoch 850, training loss: 0.07026148587465286 = 0.00396129721775651 + 0.01 * 6.630019187927246
Epoch 850, val loss: 1.226809024810791
Epoch 860, training loss: 0.07003849744796753 = 0.0038555790670216084 + 0.01 * 6.618292331695557
Epoch 860, val loss: 1.2318693399429321
Epoch 870, training loss: 0.07007088512182236 = 0.0037549405824393034 + 0.01 * 6.631594657897949
Epoch 870, val loss: 1.2369465827941895
Epoch 880, training loss: 0.06971427798271179 = 0.0036587482318282127 + 0.01 * 6.605553150177002
Epoch 880, val loss: 1.2419120073318481
Epoch 890, training loss: 0.0697290301322937 = 0.0035668741911649704 + 0.01 * 6.616215705871582
Epoch 890, val loss: 1.2467418909072876
Epoch 900, training loss: 0.06954325735569 = 0.0034792711958289146 + 0.01 * 6.606398582458496
Epoch 900, val loss: 1.2516125440597534
Epoch 910, training loss: 0.06936949491500854 = 0.003395499661564827 + 0.01 * 6.597399711608887
Epoch 910, val loss: 1.256199598312378
Epoch 920, training loss: 0.06935672461986542 = 0.0033153744880110025 + 0.01 * 6.604135513305664
Epoch 920, val loss: 1.2607847452163696
Epoch 930, training loss: 0.0692843496799469 = 0.003238558303564787 + 0.01 * 6.604578971862793
Epoch 930, val loss: 1.2653404474258423
Epoch 940, training loss: 0.06905461847782135 = 0.0031650809105485678 + 0.01 * 6.588953971862793
Epoch 940, val loss: 1.2697725296020508
Epoch 950, training loss: 0.06910905987024307 = 0.0030947241466492414 + 0.01 * 6.601433277130127
Epoch 950, val loss: 1.274100661277771
Epoch 960, training loss: 0.06888978183269501 = 0.0030270481947809458 + 0.01 * 6.586273193359375
Epoch 960, val loss: 1.2783890962600708
Epoch 970, training loss: 0.069020114839077 = 0.0029622139409184456 + 0.01 * 6.605789661407471
Epoch 970, val loss: 1.282619595527649
Epoch 980, training loss: 0.06870141625404358 = 0.0029000143986195326 + 0.01 * 6.580140590667725
Epoch 980, val loss: 1.2868677377700806
Epoch 990, training loss: 0.06857103854417801 = 0.0028402304742485285 + 0.01 * 6.573081016540527
Epoch 990, val loss: 1.290842890739441
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.7196
Flip ASR: 0.6756/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0414063930511475 = 1.9576677083969116 + 0.01 * 8.373873710632324
Epoch 0, val loss: 1.9483461380004883
Epoch 10, training loss: 2.0309364795684814 = 1.9471989870071411 + 0.01 * 8.373756408691406
Epoch 10, val loss: 1.9378629922866821
Epoch 20, training loss: 2.0184645652770996 = 1.9347327947616577 + 0.01 * 8.373181343078613
Epoch 20, val loss: 1.9245390892028809
Epoch 30, training loss: 2.0012025833129883 = 1.9174909591674805 + 0.01 * 8.37116527557373
Epoch 30, val loss: 1.905167818069458
Epoch 40, training loss: 1.9759330749511719 = 1.8922961950302124 + 0.01 * 8.363688468933105
Epoch 40, val loss: 1.8765084743499756
Epoch 50, training loss: 1.939511775970459 = 1.8562474250793457 + 0.01 * 8.326431274414062
Epoch 50, val loss: 1.8371074199676514
Epoch 60, training loss: 1.8915338516235352 = 1.8107903003692627 + 0.01 * 8.074356079101562
Epoch 60, val loss: 1.792365550994873
Epoch 70, training loss: 1.8426923751831055 = 1.765366554260254 + 0.01 * 7.732577800750732
Epoch 70, val loss: 1.7543933391571045
Epoch 80, training loss: 1.7961608171463013 = 1.7203879356384277 + 0.01 * 7.5772929191589355
Epoch 80, val loss: 1.7196680307388306
Epoch 90, training loss: 1.7353826761245728 = 1.6605098247528076 + 0.01 * 7.487282752990723
Epoch 90, val loss: 1.671609878540039
Epoch 100, training loss: 1.6541324853897095 = 1.5806077718734741 + 0.01 * 7.352469444274902
Epoch 100, val loss: 1.6073495149612427
Epoch 110, training loss: 1.555665373802185 = 1.4837121963500977 + 0.01 * 7.195321559906006
Epoch 110, val loss: 1.5309900045394897
Epoch 120, training loss: 1.454770803451538 = 1.3835359811782837 + 0.01 * 7.123480319976807
Epoch 120, val loss: 1.4561330080032349
Epoch 130, training loss: 1.3596723079681396 = 1.2888669967651367 + 0.01 * 7.080528736114502
Epoch 130, val loss: 1.3911246061325073
Epoch 140, training loss: 1.2714396715164185 = 1.2009433507919312 + 0.01 * 7.049628734588623
Epoch 140, val loss: 1.333768367767334
Epoch 150, training loss: 1.1880158185958862 = 1.1178146600723267 + 0.01 * 7.020114898681641
Epoch 150, val loss: 1.2797099351882935
Epoch 160, training loss: 1.1073472499847412 = 1.0374072790145874 + 0.01 * 6.993991851806641
Epoch 160, val loss: 1.2272471189498901
Epoch 170, training loss: 1.0292109251022339 = 0.9594284892082214 + 0.01 * 6.978238582611084
Epoch 170, val loss: 1.1757605075836182
Epoch 180, training loss: 0.9533944725990295 = 0.8836787939071655 + 0.01 * 6.9715704917907715
Epoch 180, val loss: 1.1258727312088013
Epoch 190, training loss: 0.8797817826271057 = 0.8100888133049011 + 0.01 * 6.969298839569092
Epoch 190, val loss: 1.0788174867630005
Epoch 200, training loss: 0.8077301979064941 = 0.7380542159080505 + 0.01 * 6.967596054077148
Epoch 200, val loss: 1.0342804193496704
Epoch 210, training loss: 0.7371777892112732 = 0.6675212383270264 + 0.01 * 6.96565580368042
Epoch 210, val loss: 0.990664541721344
Epoch 220, training loss: 0.668546199798584 = 0.5989120602607727 + 0.01 * 6.96341609954834
Epoch 220, val loss: 0.947922945022583
Epoch 230, training loss: 0.602529764175415 = 0.5329274535179138 + 0.01 * 6.960233688354492
Epoch 230, val loss: 0.9068955779075623
Epoch 240, training loss: 0.5402330756187439 = 0.47065332531929016 + 0.01 * 6.957973957061768
Epoch 240, val loss: 0.8693666458129883
Epoch 250, training loss: 0.4827954173088074 = 0.4132765233516693 + 0.01 * 6.951888084411621
Epoch 250, val loss: 0.8373245596885681
Epoch 260, training loss: 0.431098997592926 = 0.36163321137428284 + 0.01 * 6.946580410003662
Epoch 260, val loss: 0.8116532564163208
Epoch 270, training loss: 0.3855828642845154 = 0.31617939472198486 + 0.01 * 6.940347671508789
Epoch 270, val loss: 0.7921496629714966
Epoch 280, training loss: 0.34621840715408325 = 0.27687573432922363 + 0.01 * 6.934267520904541
Epoch 280, val loss: 0.7781246900558472
Epoch 290, training loss: 0.3123299181461334 = 0.243088498711586 + 0.01 * 6.9241414070129395
Epoch 290, val loss: 0.7685439586639404
Epoch 300, training loss: 0.28310921788215637 = 0.21392838656902313 + 0.01 * 6.918083190917969
Epoch 300, val loss: 0.7630194425582886
Epoch 310, training loss: 0.25756263732910156 = 0.1885109692811966 + 0.01 * 6.905168056488037
Epoch 310, val loss: 0.7600972056388855
Epoch 320, training loss: 0.23514634370803833 = 0.1661757081747055 + 0.01 * 6.897064685821533
Epoch 320, val loss: 0.7593677639961243
Epoch 330, training loss: 0.2153838872909546 = 0.14650805294513702 + 0.01 * 6.887584209442139
Epoch 330, val loss: 0.760560929775238
Epoch 340, training loss: 0.19774368405342102 = 0.12901726365089417 + 0.01 * 6.8726420402526855
Epoch 340, val loss: 0.7632091641426086
Epoch 350, training loss: 0.18222284317016602 = 0.1136145293712616 + 0.01 * 6.8608317375183105
Epoch 350, val loss: 0.7678249478340149
Epoch 360, training loss: 0.16881614923477173 = 0.10029629617929459 + 0.01 * 6.851985931396484
Epoch 360, val loss: 0.7750243544578552
Epoch 370, training loss: 0.15748772025108337 = 0.0890468880534172 + 0.01 * 6.844083786010742
Epoch 370, val loss: 0.7846041917800903
Epoch 380, training loss: 0.1478421688079834 = 0.07945080101490021 + 0.01 * 6.839136123657227
Epoch 380, val loss: 0.7962173223495483
Epoch 390, training loss: 0.1395009160041809 = 0.07116445899009705 + 0.01 * 6.833646297454834
Epoch 390, val loss: 0.8089355230331421
Epoch 400, training loss: 0.1321810930967331 = 0.06397721916437149 + 0.01 * 6.820387840270996
Epoch 400, val loss: 0.8221861124038696
Epoch 410, training loss: 0.12591925263404846 = 0.057714223861694336 + 0.01 * 6.820502281188965
Epoch 410, val loss: 0.8355745077133179
Epoch 420, training loss: 0.1203494593501091 = 0.05224338918924332 + 0.01 * 6.810606956481934
Epoch 420, val loss: 0.8489016890525818
Epoch 430, training loss: 0.11539997160434723 = 0.047445449978113174 + 0.01 * 6.795452117919922
Epoch 430, val loss: 0.862140417098999
Epoch 440, training loss: 0.11142361909151077 = 0.04322401434183121 + 0.01 * 6.819960594177246
Epoch 440, val loss: 0.875225305557251
Epoch 450, training loss: 0.1073189228773117 = 0.03951002284884453 + 0.01 * 6.780889987945557
Epoch 450, val loss: 0.8880355358123779
Epoch 460, training loss: 0.1039520800113678 = 0.036222413182258606 + 0.01 * 6.7729668617248535
Epoch 460, val loss: 0.9007099866867065
Epoch 470, training loss: 0.10102052986621857 = 0.033299852162599564 + 0.01 * 6.772067546844482
Epoch 470, val loss: 0.9131354093551636
Epoch 480, training loss: 0.09825540333986282 = 0.030701249837875366 + 0.01 * 6.755415439605713
Epoch 480, val loss: 0.9253997802734375
Epoch 490, training loss: 0.09605047106742859 = 0.028384584933519363 + 0.01 * 6.766589164733887
Epoch 490, val loss: 0.9372249245643616
Epoch 500, training loss: 0.09368875622749329 = 0.026314344257116318 + 0.01 * 6.7374420166015625
Epoch 500, val loss: 0.9488199949264526
Epoch 510, training loss: 0.09182751178741455 = 0.024454977363348007 + 0.01 * 6.737253189086914
Epoch 510, val loss: 0.9600893259048462
Epoch 520, training loss: 0.09000413864850998 = 0.022781377658247948 + 0.01 * 6.722275733947754
Epoch 520, val loss: 0.9711096882820129
Epoch 530, training loss: 0.08864878863096237 = 0.02127005159854889 + 0.01 * 6.737873554229736
Epoch 530, val loss: 0.9817410111427307
Epoch 540, training loss: 0.0869789868593216 = 0.019906360656023026 + 0.01 * 6.70726203918457
Epoch 540, val loss: 0.992197573184967
Epoch 550, training loss: 0.08569039404392242 = 0.018671458587050438 + 0.01 * 6.7018938064575195
Epoch 550, val loss: 1.0023009777069092
Epoch 560, training loss: 0.08458509296178818 = 0.017547734081745148 + 0.01 * 6.703735828399658
Epoch 560, val loss: 1.0122170448303223
Epoch 570, training loss: 0.0835265964269638 = 0.016526488587260246 + 0.01 * 6.700010776519775
Epoch 570, val loss: 1.0217283964157104
Epoch 580, training loss: 0.08234481513500214 = 0.015595006756484509 + 0.01 * 6.674981594085693
Epoch 580, val loss: 1.031014084815979
Epoch 590, training loss: 0.08160540461540222 = 0.014741059392690659 + 0.01 * 6.686435222625732
Epoch 590, val loss: 1.0401028394699097
Epoch 600, training loss: 0.0806228518486023 = 0.013958447612822056 + 0.01 * 6.666440963745117
Epoch 600, val loss: 1.0488859415054321
Epoch 610, training loss: 0.07983885705471039 = 0.013238128274679184 + 0.01 * 6.660073280334473
Epoch 610, val loss: 1.057471752166748
Epoch 620, training loss: 0.07925201952457428 = 0.01257418468594551 + 0.01 * 6.667783737182617
Epoch 620, val loss: 1.065793752670288
Epoch 630, training loss: 0.07845617085695267 = 0.011960893869400024 + 0.01 * 6.6495280265808105
Epoch 630, val loss: 1.0739794969558716
Epoch 640, training loss: 0.07793457061052322 = 0.011392847634851933 + 0.01 * 6.654172897338867
Epoch 640, val loss: 1.0818984508514404
Epoch 650, training loss: 0.07735490798950195 = 0.01086714118719101 + 0.01 * 6.648777008056641
Epoch 650, val loss: 1.0895825624465942
Epoch 660, training loss: 0.07663265615701675 = 0.010379302315413952 + 0.01 * 6.625336170196533
Epoch 660, val loss: 1.097116231918335
Epoch 670, training loss: 0.076290562748909 = 0.009925886057317257 + 0.01 * 6.636467933654785
Epoch 670, val loss: 1.1043508052825928
Epoch 680, training loss: 0.07570461928844452 = 0.009503479115664959 + 0.01 * 6.620114803314209
Epoch 680, val loss: 1.11154305934906
Epoch 690, training loss: 0.07540149986743927 = 0.009109559468925 + 0.01 * 6.629194259643555
Epoch 690, val loss: 1.1184114217758179
Epoch 700, training loss: 0.07487193495035172 = 0.008741030469536781 + 0.01 * 6.613090991973877
Epoch 700, val loss: 1.1251959800720215
Epoch 710, training loss: 0.07460912317037582 = 0.008395751938223839 + 0.01 * 6.621336936950684
Epoch 710, val loss: 1.1317797899246216
Epoch 720, training loss: 0.07406512647867203 = 0.008072024211287498 + 0.01 * 6.599309921264648
Epoch 720, val loss: 1.1382555961608887
Epoch 730, training loss: 0.07361651211977005 = 0.007767810020595789 + 0.01 * 6.584870338439941
Epoch 730, val loss: 1.1445376873016357
Epoch 740, training loss: 0.0735463872551918 = 0.00748197827488184 + 0.01 * 6.606441020965576
Epoch 740, val loss: 1.150654911994934
Epoch 750, training loss: 0.07309707254171371 = 0.007212930358946323 + 0.01 * 6.588414192199707
Epoch 750, val loss: 1.1566994190216064
Epoch 760, training loss: 0.07275284081697464 = 0.006959928665310144 + 0.01 * 6.579291343688965
Epoch 760, val loss: 1.162467360496521
Epoch 770, training loss: 0.07245716452598572 = 0.006721030455082655 + 0.01 * 6.57361364364624
Epoch 770, val loss: 1.1683145761489868
Epoch 780, training loss: 0.07221471518278122 = 0.00649540638551116 + 0.01 * 6.571930885314941
Epoch 780, val loss: 1.173809289932251
Epoch 790, training loss: 0.0718705803155899 = 0.006282382179051638 + 0.01 * 6.558819770812988
Epoch 790, val loss: 1.1792666912078857
Epoch 800, training loss: 0.0716923400759697 = 0.006080625578761101 + 0.01 * 6.561171531677246
Epoch 800, val loss: 1.1846082210540771
Epoch 810, training loss: 0.07149237394332886 = 0.005889415740966797 + 0.01 * 6.560296058654785
Epoch 810, val loss: 1.1898484230041504
Epoch 820, training loss: 0.07125358283519745 = 0.005708189681172371 + 0.01 * 6.554539680480957
Epoch 820, val loss: 1.1949272155761719
Epoch 830, training loss: 0.07105498760938644 = 0.005536303389817476 + 0.01 * 6.551868438720703
Epoch 830, val loss: 1.199964165687561
Epoch 840, training loss: 0.07083161920309067 = 0.00537303788587451 + 0.01 * 6.545857906341553
Epoch 840, val loss: 1.204806923866272
Epoch 850, training loss: 0.07060804963111877 = 0.005217942409217358 + 0.01 * 6.539011478424072
Epoch 850, val loss: 1.209600806236267
Epoch 860, training loss: 0.07059863954782486 = 0.005070284008979797 + 0.01 * 6.552835464477539
Epoch 860, val loss: 1.214263916015625
Epoch 870, training loss: 0.07030274718999863 = 0.004929622169584036 + 0.01 * 6.5373125076293945
Epoch 870, val loss: 1.218850016593933
Epoch 880, training loss: 0.07009385526180267 = 0.004795669112354517 + 0.01 * 6.529818534851074
Epoch 880, val loss: 1.223343849182129
Epoch 890, training loss: 0.07008657604455948 = 0.004667926114052534 + 0.01 * 6.541865348815918
Epoch 890, val loss: 1.2276859283447266
Epoch 900, training loss: 0.06978866457939148 = 0.004545837640762329 + 0.01 * 6.524282932281494
Epoch 900, val loss: 1.2320159673690796
Epoch 910, training loss: 0.06978926807641983 = 0.004429252818226814 + 0.01 * 6.536001682281494
Epoch 910, val loss: 1.2362345457077026
Epoch 920, training loss: 0.06953714042901993 = 0.004317931365221739 + 0.01 * 6.521921157836914
Epoch 920, val loss: 1.240356206893921
Epoch 930, training loss: 0.06974155455827713 = 0.004211368504911661 + 0.01 * 6.553019046783447
Epoch 930, val loss: 1.2443923950195312
Epoch 940, training loss: 0.06947255879640579 = 0.004109352361410856 + 0.01 * 6.536320686340332
Epoch 940, val loss: 1.2483869791030884
Epoch 950, training loss: 0.06905543059110641 = 0.004011778626590967 + 0.01 * 6.50436544418335
Epoch 950, val loss: 1.2522437572479248
Epoch 960, training loss: 0.0693303719162941 = 0.003918243572115898 + 0.01 * 6.541213035583496
Epoch 960, val loss: 1.2559748888015747
Epoch 970, training loss: 0.06892604380846024 = 0.0038284368347376585 + 0.01 * 6.509760856628418
Epoch 970, val loss: 1.2597999572753906
Epoch 980, training loss: 0.06878481060266495 = 0.003742417087778449 + 0.01 * 6.504239559173584
Epoch 980, val loss: 1.2634223699569702
Epoch 990, training loss: 0.06884084641933441 = 0.0036599317099899054 + 0.01 * 6.518091678619385
Epoch 990, val loss: 1.2669702768325806
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.6421
Flip ASR: 0.6089/225 nodes
The final ASR:0.65560, 0.04767, Accuracy:0.81481, 0.00800
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11616])
remove edge: torch.Size([2, 9496])
updated graph: torch.Size([2, 10556])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
The final ASR:0.97786, 0.00522, Accuracy:0.82963, 0.00524
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.5 der2:0.2 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.01248836517334 = 1.92875075340271 + 0.01 * 8.373767852783203
Epoch 0, val loss: 1.9315327405929565
Epoch 10, training loss: 2.003042697906494 = 1.9193068742752075 + 0.01 * 8.373576164245605
Epoch 10, val loss: 1.9217506647109985
Epoch 20, training loss: 1.9904088973999023 = 1.906677484512329 + 0.01 * 8.373143196105957
Epoch 20, val loss: 1.908539891242981
Epoch 30, training loss: 1.971648931503296 = 1.8879300355911255 + 0.01 * 8.371891975402832
Epoch 30, val loss: 1.8891820907592773
Epoch 40, training loss: 1.9435205459594727 = 1.8598754405975342 + 0.01 * 8.364506721496582
Epoch 40, val loss: 1.8612409830093384
Epoch 50, training loss: 1.9059876203536987 = 1.8228968381881714 + 0.01 * 8.309081077575684
Epoch 50, val loss: 1.8273842334747314
Epoch 60, training loss: 1.8655920028686523 = 1.7852293252944946 + 0.01 * 8.03626537322998
Epoch 60, val loss: 1.797162652015686
Epoch 70, training loss: 1.821187138557434 = 1.74453604221344 + 0.01 * 7.665114879608154
Epoch 70, val loss: 1.763513445854187
Epoch 80, training loss: 1.7600507736206055 = 1.6875125169754028 + 0.01 * 7.253828525543213
Epoch 80, val loss: 1.7150013446807861
Epoch 90, training loss: 1.6824036836624146 = 1.6126660108566284 + 0.01 * 6.973768711090088
Epoch 90, val loss: 1.6529669761657715
Epoch 100, training loss: 1.5976358652114868 = 1.5285992622375488 + 0.01 * 6.903657913208008
Epoch 100, val loss: 1.586150050163269
Epoch 110, training loss: 1.517197608947754 = 1.448477029800415 + 0.01 * 6.8720574378967285
Epoch 110, val loss: 1.5265547037124634
Epoch 120, training loss: 1.4435758590698242 = 1.3750451803207397 + 0.01 * 6.853063106536865
Epoch 120, val loss: 1.47694730758667
Epoch 130, training loss: 1.3722540140151978 = 1.303877830505371 + 0.01 * 6.837619304656982
Epoch 130, val loss: 1.4316920042037964
Epoch 140, training loss: 1.2994542121887207 = 1.2312976121902466 + 0.01 * 6.815664291381836
Epoch 140, val loss: 1.3855117559432983
Epoch 150, training loss: 1.2251583337783813 = 1.1571943759918213 + 0.01 * 6.796392440795898
Epoch 150, val loss: 1.3375797271728516
Epoch 160, training loss: 1.1526967287063599 = 1.0848612785339355 + 0.01 * 6.7835469245910645
Epoch 160, val loss: 1.2910797595977783
Epoch 170, training loss: 1.0856188535690308 = 1.0178542137145996 + 0.01 * 6.7764692306518555
Epoch 170, val loss: 1.2478790283203125
Epoch 180, training loss: 1.024925708770752 = 0.9571953415870667 + 0.01 * 6.773036479949951
Epoch 180, val loss: 1.20918607711792
Epoch 190, training loss: 0.9685338735580444 = 0.9008497595787048 + 0.01 * 6.768413543701172
Epoch 190, val loss: 1.172647476196289
Epoch 200, training loss: 0.912842869758606 = 0.8451935648918152 + 0.01 * 6.764927387237549
Epoch 200, val loss: 1.1358718872070312
Epoch 210, training loss: 0.8548264503479004 = 0.7872060537338257 + 0.01 * 6.762038707733154
Epoch 210, val loss: 1.0958648920059204
Epoch 220, training loss: 0.793576180934906 = 0.7259921431541443 + 0.01 * 6.758402347564697
Epoch 220, val loss: 1.0528737306594849
Epoch 230, training loss: 0.7303528785705566 = 0.662807285785675 + 0.01 * 6.7545623779296875
Epoch 230, val loss: 1.0093616247177124
Epoch 240, training loss: 0.6675997972488403 = 0.6000460386276245 + 0.01 * 6.755375862121582
Epoch 240, val loss: 0.9690767526626587
Epoch 250, training loss: 0.6073349714279175 = 0.5398687720298767 + 0.01 * 6.74661922454834
Epoch 250, val loss: 0.9347556233406067
Epoch 260, training loss: 0.5507331490516663 = 0.4833662509918213 + 0.01 * 6.736688613891602
Epoch 260, val loss: 0.9078956246376038
Epoch 270, training loss: 0.4978555142879486 = 0.4305618107318878 + 0.01 * 6.729370594024658
Epoch 270, val loss: 0.8887163400650024
Epoch 280, training loss: 0.4484456181526184 = 0.38111236691474915 + 0.01 * 6.73332405090332
Epoch 280, val loss: 0.8757351636886597
Epoch 290, training loss: 0.4019336998462677 = 0.3346876800060272 + 0.01 * 6.724602699279785
Epoch 290, val loss: 0.8678771257400513
Epoch 300, training loss: 0.358331561088562 = 0.29112881422042847 + 0.01 * 6.72027587890625
Epoch 300, val loss: 0.8640744686126709
Epoch 310, training loss: 0.31772300601005554 = 0.250608891248703 + 0.01 * 6.711411952972412
Epoch 310, val loss: 0.8640068173408508
Epoch 320, training loss: 0.28076446056365967 = 0.21368476748466492 + 0.01 * 6.707968235015869
Epoch 320, val loss: 0.8675444722175598
Epoch 330, training loss: 0.24800553917884827 = 0.18095245957374573 + 0.01 * 6.705308437347412
Epoch 330, val loss: 0.8744673132896423
Epoch 340, training loss: 0.21975329518318176 = 0.15274493396282196 + 0.01 * 6.700836658477783
Epoch 340, val loss: 0.8844252824783325
Epoch 350, training loss: 0.19607335329055786 = 0.12907902896404266 + 0.01 * 6.699432373046875
Epoch 350, val loss: 0.8969100713729858
Epoch 360, training loss: 0.1765732616186142 = 0.10963353514671326 + 0.01 * 6.693973064422607
Epoch 360, val loss: 0.9116694927215576
Epoch 370, training loss: 0.16072998940944672 = 0.09385364502668381 + 0.01 * 6.6876349449157715
Epoch 370, val loss: 0.9282249212265015
Epoch 380, training loss: 0.14793512225151062 = 0.08110558241605759 + 0.01 * 6.68295431137085
Epoch 380, val loss: 0.946161150932312
Epoch 390, training loss: 0.13769730925559998 = 0.07078433036804199 + 0.01 * 6.691298007965088
Epoch 390, val loss: 0.9651241898536682
Epoch 400, training loss: 0.12910588085651398 = 0.06235422566533089 + 0.01 * 6.67516565322876
Epoch 400, val loss: 0.9847220182418823
Epoch 410, training loss: 0.12206026166677475 = 0.05538008362054825 + 0.01 * 6.668017864227295
Epoch 410, val loss: 1.0048141479492188
Epoch 420, training loss: 0.11634257435798645 = 0.04954063892364502 + 0.01 * 6.6801934242248535
Epoch 420, val loss: 1.025123953819275
Epoch 430, training loss: 0.11130475252866745 = 0.04459550976753235 + 0.01 * 6.670924186706543
Epoch 430, val loss: 1.0453107357025146
Epoch 440, training loss: 0.10687793791294098 = 0.040351610630750656 + 0.01 * 6.652632713317871
Epoch 440, val loss: 1.065413236618042
Epoch 450, training loss: 0.10329100489616394 = 0.03667106106877327 + 0.01 * 6.661993980407715
Epoch 450, val loss: 1.0852519273757935
Epoch 460, training loss: 0.09994332492351532 = 0.033457495272159576 + 0.01 * 6.64858341217041
Epoch 460, val loss: 1.1046359539031982
Epoch 470, training loss: 0.09699106961488724 = 0.03063478134572506 + 0.01 * 6.635628700256348
Epoch 470, val loss: 1.1235487461090088
Epoch 480, training loss: 0.09444087743759155 = 0.028143079951405525 + 0.01 * 6.629779815673828
Epoch 480, val loss: 1.1417620182037354
Epoch 490, training loss: 0.09224836528301239 = 0.025936702266335487 + 0.01 * 6.631166458129883
Epoch 490, val loss: 1.1592680215835571
Epoch 500, training loss: 0.09030601382255554 = 0.02397446520626545 + 0.01 * 6.63315486907959
Epoch 500, val loss: 1.176130771636963
Epoch 510, training loss: 0.08837809413671494 = 0.02222561277449131 + 0.01 * 6.615248680114746
Epoch 510, val loss: 1.192305564880371
Epoch 520, training loss: 0.08678539097309113 = 0.020661570131778717 + 0.01 * 6.612382411956787
Epoch 520, val loss: 1.2076911926269531
Epoch 530, training loss: 0.08540311455726624 = 0.019256506115198135 + 0.01 * 6.614660263061523
Epoch 530, val loss: 1.2226537466049194
Epoch 540, training loss: 0.08421409875154495 = 0.017992116510868073 + 0.01 * 6.622198581695557
Epoch 540, val loss: 1.2370210886001587
Epoch 550, training loss: 0.08287294954061508 = 0.01684822142124176 + 0.01 * 6.60247278213501
Epoch 550, val loss: 1.250701904296875
Epoch 560, training loss: 0.0817221999168396 = 0.015812838450074196 + 0.01 * 6.590936183929443
Epoch 560, val loss: 1.2639809846878052
Epoch 570, training loss: 0.08089550584554672 = 0.014870507642626762 + 0.01 * 6.602499961853027
Epoch 570, val loss: 1.2767099142074585
Epoch 580, training loss: 0.08003983646631241 = 0.014011099934577942 + 0.01 * 6.602873802185059
Epoch 580, val loss: 1.2891772985458374
Epoch 590, training loss: 0.07901916652917862 = 0.013222706504166126 + 0.01 * 6.579646110534668
Epoch 590, val loss: 1.30147123336792
Epoch 600, training loss: 0.07871178537607193 = 0.012502163648605347 + 0.01 * 6.620962142944336
Epoch 600, val loss: 1.313037395477295
Epoch 610, training loss: 0.07759565860033035 = 0.011842861771583557 + 0.01 * 6.57528018951416
Epoch 610, val loss: 1.324377417564392
Epoch 620, training loss: 0.07699275016784668 = 0.011236417107284069 + 0.01 * 6.575634002685547
Epoch 620, val loss: 1.3353208303451538
Epoch 630, training loss: 0.07660172879695892 = 0.010677844285964966 + 0.01 * 6.59238862991333
Epoch 630, val loss: 1.3458753824234009
Epoch 640, training loss: 0.07574371248483658 = 0.010161268524825573 + 0.01 * 6.558244705200195
Epoch 640, val loss: 1.356480598449707
Epoch 650, training loss: 0.07523393630981445 = 0.009683134965598583 + 0.01 * 6.555080413818359
Epoch 650, val loss: 1.3665519952774048
Epoch 660, training loss: 0.07485643029212952 = 0.009240261279046535 + 0.01 * 6.561616897583008
Epoch 660, val loss: 1.3763905763626099
Epoch 670, training loss: 0.07427389919757843 = 0.008828341960906982 + 0.01 * 6.5445556640625
Epoch 670, val loss: 1.3861303329467773
Epoch 680, training loss: 0.0739215537905693 = 0.008445359766483307 + 0.01 * 6.547619819641113
Epoch 680, val loss: 1.3956226110458374
Epoch 690, training loss: 0.0734831765294075 = 0.00808882899582386 + 0.01 * 6.539435386657715
Epoch 690, val loss: 1.4047576189041138
Epoch 700, training loss: 0.07369381934404373 = 0.007755873259156942 + 0.01 * 6.593794822692871
Epoch 700, val loss: 1.4134684801101685
Epoch 710, training loss: 0.0728604719042778 = 0.007445523981004953 + 0.01 * 6.541494846343994
Epoch 710, val loss: 1.422537922859192
Epoch 720, training loss: 0.07253746688365936 = 0.007155329454690218 + 0.01 * 6.538214206695557
Epoch 720, val loss: 1.4307396411895752
Epoch 730, training loss: 0.07230620831251144 = 0.006883697118610144 + 0.01 * 6.542251110076904
Epoch 730, val loss: 1.4391040802001953
Epoch 740, training loss: 0.07199031859636307 = 0.006629122421145439 + 0.01 * 6.53611946105957
Epoch 740, val loss: 1.447303295135498
Epoch 750, training loss: 0.07168490439653397 = 0.006389682646840811 + 0.01 * 6.529521942138672
Epoch 750, val loss: 1.454934000968933
Epoch 760, training loss: 0.0712273120880127 = 0.006165177095681429 + 0.01 * 6.506213665008545
Epoch 760, val loss: 1.4628357887268066
Epoch 770, training loss: 0.07112076878547668 = 0.0059537990018725395 + 0.01 * 6.516697406768799
Epoch 770, val loss: 1.4703551530838013
Epoch 780, training loss: 0.07116551697254181 = 0.005754800513386726 + 0.01 * 6.541071891784668
Epoch 780, val loss: 1.4777259826660156
Epoch 790, training loss: 0.07069147378206253 = 0.005566862411797047 + 0.01 * 6.512461185455322
Epoch 790, val loss: 1.4851475954055786
Epoch 800, training loss: 0.0703953355550766 = 0.005389201454818249 + 0.01 * 6.500613689422607
Epoch 800, val loss: 1.4919054508209229
Epoch 810, training loss: 0.07010135799646378 = 0.005221357103437185 + 0.01 * 6.48799991607666
Epoch 810, val loss: 1.4988592863082886
Epoch 820, training loss: 0.07008931785821915 = 0.005062313284724951 + 0.01 * 6.5027008056640625
Epoch 820, val loss: 1.5055160522460938
Epoch 830, training loss: 0.0697866678237915 = 0.004911726340651512 + 0.01 * 6.487494468688965
Epoch 830, val loss: 1.5122363567352295
Epoch 840, training loss: 0.06962486356496811 = 0.004768370650708675 + 0.01 * 6.485649585723877
Epoch 840, val loss: 1.5184246301651
Epoch 850, training loss: 0.069312684237957 = 0.004632113967090845 + 0.01 * 6.468056678771973
Epoch 850, val loss: 1.5248345136642456
Epoch 860, training loss: 0.06931325793266296 = 0.004502470605075359 + 0.01 * 6.4810791015625
Epoch 860, val loss: 1.5309613943099976
Epoch 870, training loss: 0.06910725682973862 = 0.004379145801067352 + 0.01 * 6.472811222076416
Epoch 870, val loss: 1.5369987487792969
Epoch 880, training loss: 0.06910311430692673 = 0.004261297639459372 + 0.01 * 6.484181880950928
Epoch 880, val loss: 1.5430046319961548
Epoch 890, training loss: 0.06883883476257324 = 0.004148880951106548 + 0.01 * 6.468995571136475
Epoch 890, val loss: 1.5488637685775757
Epoch 900, training loss: 0.06867080926895142 = 0.004042144864797592 + 0.01 * 6.462866306304932
Epoch 900, val loss: 1.5541954040527344
Epoch 910, training loss: 0.0687139555811882 = 0.003939117770642042 + 0.01 * 6.477484226226807
Epoch 910, val loss: 1.560325264930725
Epoch 920, training loss: 0.06831803172826767 = 0.0038417477626353502 + 0.01 * 6.447628498077393
Epoch 920, val loss: 1.5656944513320923
Epoch 930, training loss: 0.06814748793840408 = 0.0037478741724044085 + 0.01 * 6.4399614334106445
Epoch 930, val loss: 1.5707789659500122
Epoch 940, training loss: 0.06828981637954712 = 0.0036587135400623083 + 0.01 * 6.463110446929932
Epoch 940, val loss: 1.5765926837921143
Epoch 950, training loss: 0.06795094907283783 = 0.0035731003154069185 + 0.01 * 6.437785625457764
Epoch 950, val loss: 1.5815445184707642
Epoch 960, training loss: 0.06799957156181335 = 0.0034910242538899183 + 0.01 * 6.450855255126953
Epoch 960, val loss: 1.586880087852478
Epoch 970, training loss: 0.06779617816209793 = 0.0034127584658563137 + 0.01 * 6.4383416175842285
Epoch 970, val loss: 1.5918099880218506
Epoch 980, training loss: 0.067513108253479 = 0.003337334608659148 + 0.01 * 6.417578220367432
Epoch 980, val loss: 1.5966612100601196
Epoch 990, training loss: 0.06758040934801102 = 0.00326492334716022 + 0.01 * 6.431548118591309
Epoch 990, val loss: 1.6014072895050049
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7815
Overall ASR: 0.6015
Flip ASR: 0.5333/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.034210443496704 = 1.950472354888916 + 0.01 * 8.373815536499023
Epoch 0, val loss: 1.953928828239441
Epoch 10, training loss: 2.0245163440704346 = 1.9407790899276733 + 0.01 * 8.373736381530762
Epoch 10, val loss: 1.944219708442688
Epoch 20, training loss: 2.012641429901123 = 1.928907036781311 + 0.01 * 8.373451232910156
Epoch 20, val loss: 1.9319288730621338
Epoch 30, training loss: 1.9959495067596436 = 1.9122222661972046 + 0.01 * 8.372721672058105
Epoch 30, val loss: 1.9142817258834839
Epoch 40, training loss: 1.9710015058517456 = 1.8873049020767212 + 0.01 * 8.36966323852539
Epoch 40, val loss: 1.8877837657928467
Epoch 50, training loss: 1.9346864223480225 = 1.851189136505127 + 0.01 * 8.349725723266602
Epoch 50, val loss: 1.8504345417022705
Epoch 60, training loss: 1.8891775608062744 = 1.8068382740020752 + 0.01 * 8.233933448791504
Epoch 60, val loss: 1.807727336883545
Epoch 70, training loss: 1.8424973487854004 = 1.7642786502838135 + 0.01 * 7.821867942810059
Epoch 70, val loss: 1.770469069480896
Epoch 80, training loss: 1.7892032861709595 = 1.716415286064148 + 0.01 * 7.278800964355469
Epoch 80, val loss: 1.7285195589065552
Epoch 90, training loss: 1.722488284111023 = 1.651479959487915 + 0.01 * 7.100837707519531
Epoch 90, val loss: 1.6718072891235352
Epoch 100, training loss: 1.6347289085388184 = 1.5641487836837769 + 0.01 * 7.058006763458252
Epoch 100, val loss: 1.5968855619430542
Epoch 110, training loss: 1.528163194656372 = 1.4579533338546753 + 0.01 * 7.020990371704102
Epoch 110, val loss: 1.5096664428710938
Epoch 120, training loss: 1.414828896522522 = 1.344840407371521 + 0.01 * 6.998854160308838
Epoch 120, val loss: 1.4205936193466187
Epoch 130, training loss: 1.3051813840866089 = 1.2354116439819336 + 0.01 * 6.976973533630371
Epoch 130, val loss: 1.3378779888153076
Epoch 140, training loss: 1.2048765420913696 = 1.135394811630249 + 0.01 * 6.948174476623535
Epoch 140, val loss: 1.264425277709961
Epoch 150, training loss: 1.1175334453582764 = 1.0483700037002563 + 0.01 * 6.916345119476318
Epoch 150, val loss: 1.2008657455444336
Epoch 160, training loss: 1.0435243844985962 = 0.974644124507904 + 0.01 * 6.888023853302002
Epoch 160, val loss: 1.1471129655838013
Epoch 170, training loss: 0.9797699451446533 = 0.9110873937606812 + 0.01 * 6.868257999420166
Epoch 170, val loss: 1.101945161819458
Epoch 180, training loss: 0.9209117293357849 = 0.8523427248001099 + 0.01 * 6.8569016456604
Epoch 180, val loss: 1.0608649253845215
Epoch 190, training loss: 0.8617547750473022 = 0.7932504415512085 + 0.01 * 6.850430488586426
Epoch 190, val loss: 1.019188642501831
Epoch 200, training loss: 0.7990959882736206 = 0.7306492924690247 + 0.01 * 6.844668388366699
Epoch 200, val loss: 0.9737431406974792
Epoch 210, training loss: 0.7328694462776184 = 0.6644853353500366 + 0.01 * 6.838412761688232
Epoch 210, val loss: 0.9252188205718994
Epoch 220, training loss: 0.6658939123153687 = 0.59759521484375 + 0.01 * 6.82987117767334
Epoch 220, val loss: 0.876007616519928
Epoch 230, training loss: 0.6022673845291138 = 0.5340408682823181 + 0.01 * 6.822649002075195
Epoch 230, val loss: 0.8302463293075562
Epoch 240, training loss: 0.5446853041648865 = 0.47658196091651917 + 0.01 * 6.810334205627441
Epoch 240, val loss: 0.7915560007095337
Epoch 250, training loss: 0.49368369579315186 = 0.4256960153579712 + 0.01 * 6.798769474029541
Epoch 250, val loss: 0.761302649974823
Epoch 260, training loss: 0.44823768734931946 = 0.3802977502346039 + 0.01 * 6.793994903564453
Epoch 260, val loss: 0.738565981388092
Epoch 270, training loss: 0.4068479537963867 = 0.33905184268951416 + 0.01 * 6.779611110687256
Epoch 270, val loss: 0.7221066355705261
Epoch 280, training loss: 0.3688642382621765 = 0.30114495754241943 + 0.01 * 6.771929740905762
Epoch 280, val loss: 0.7105856537818909
Epoch 290, training loss: 0.3339552581310272 = 0.2662883698940277 + 0.01 * 6.766688346862793
Epoch 290, val loss: 0.7029021382331848
Epoch 300, training loss: 0.30220702290534973 = 0.23442131280899048 + 0.01 * 6.778570175170898
Epoch 300, val loss: 0.6983875036239624
Epoch 310, training loss: 0.2736068069934845 = 0.2059701383113861 + 0.01 * 6.763666152954102
Epoch 310, val loss: 0.6975690126419067
Epoch 320, training loss: 0.24872423708438873 = 0.18112485110759735 + 0.01 * 6.759938716888428
Epoch 320, val loss: 0.7000648379325867
Epoch 330, training loss: 0.22732971608638763 = 0.15978246927261353 + 0.01 * 6.754724502563477
Epoch 330, val loss: 0.7054215669631958
Epoch 340, training loss: 0.2091858983039856 = 0.14166423678398132 + 0.01 * 6.752166271209717
Epoch 340, val loss: 0.7135743498802185
Epoch 350, training loss: 0.19382339715957642 = 0.12633128464221954 + 0.01 * 6.749210357666016
Epoch 350, val loss: 0.7239494919776917
Epoch 360, training loss: 0.18077760934829712 = 0.11331143975257874 + 0.01 * 6.746617794036865
Epoch 360, val loss: 0.735975980758667
Epoch 370, training loss: 0.16961438953876495 = 0.10217545926570892 + 0.01 * 6.743893146514893
Epoch 370, val loss: 0.7493358850479126
Epoch 380, training loss: 0.16005271673202515 = 0.09257882833480835 + 0.01 * 6.747389793395996
Epoch 380, val loss: 0.7636009454727173
Epoch 390, training loss: 0.15165288746356964 = 0.08425848186016083 + 0.01 * 6.73944091796875
Epoch 390, val loss: 0.7783732414245605
Epoch 400, training loss: 0.14435791969299316 = 0.07699259370565414 + 0.01 * 6.7365336418151855
Epoch 400, val loss: 0.7934537529945374
Epoch 410, training loss: 0.1380120813846588 = 0.07061290740966797 + 0.01 * 6.739916801452637
Epoch 410, val loss: 0.8087218403816223
Epoch 420, training loss: 0.13230597972869873 = 0.06499210000038147 + 0.01 * 6.731388092041016
Epoch 420, val loss: 0.8239213228225708
Epoch 430, training loss: 0.1272936314344406 = 0.06002352759242058 + 0.01 * 6.727010250091553
Epoch 430, val loss: 0.8390129208564758
Epoch 440, training loss: 0.12296438962221146 = 0.055622197687625885 + 0.01 * 6.734219074249268
Epoch 440, val loss: 0.8538450002670288
Epoch 450, training loss: 0.11895008385181427 = 0.05171986296772957 + 0.01 * 6.723022937774658
Epoch 450, val loss: 0.8683124780654907
Epoch 460, training loss: 0.11541733145713806 = 0.04824570193886757 + 0.01 * 6.717162609100342
Epoch 460, val loss: 0.8822946548461914
Epoch 470, training loss: 0.1122635006904602 = 0.04513344168663025 + 0.01 * 6.713006019592285
Epoch 470, val loss: 0.8958743214607239
Epoch 480, training loss: 0.109503373503685 = 0.04233258217573166 + 0.01 * 6.7170796394348145
Epoch 480, val loss: 0.9090636968612671
Epoch 490, training loss: 0.10689084976911545 = 0.039801135659217834 + 0.01 * 6.7089715003967285
Epoch 490, val loss: 0.9217454791069031
Epoch 500, training loss: 0.10450605303049088 = 0.03749813884496689 + 0.01 * 6.700791835784912
Epoch 500, val loss: 0.9340302348136902
Epoch 510, training loss: 0.10239840298891068 = 0.03539273142814636 + 0.01 * 6.700567245483398
Epoch 510, val loss: 0.9459825754165649
Epoch 520, training loss: 0.10045301169157028 = 0.03347044438123703 + 0.01 * 6.698256969451904
Epoch 520, val loss: 0.9575340151786804
Epoch 530, training loss: 0.09857664257287979 = 0.031704679131507874 + 0.01 * 6.687196254730225
Epoch 530, val loss: 0.9687101244926453
Epoch 540, training loss: 0.09691575914621353 = 0.030070319771766663 + 0.01 * 6.684544086456299
Epoch 540, val loss: 0.9795559644699097
Epoch 550, training loss: 0.09541238099336624 = 0.028552426025271416 + 0.01 * 6.685995578765869
Epoch 550, val loss: 0.9900953769683838
Epoch 560, training loss: 0.0938868522644043 = 0.027136869728565216 + 0.01 * 6.674998760223389
Epoch 560, val loss: 1.000280737876892
Epoch 570, training loss: 0.09257019311189651 = 0.02580660581588745 + 0.01 * 6.676358699798584
Epoch 570, val loss: 1.0102113485336304
Epoch 580, training loss: 0.09133096784353256 = 0.02455451525747776 + 0.01 * 6.677645683288574
Epoch 580, val loss: 1.0198919773101807
Epoch 590, training loss: 0.09005667269229889 = 0.02336793951690197 + 0.01 * 6.668873310089111
Epoch 590, val loss: 1.0292141437530518
Epoch 600, training loss: 0.08882756531238556 = 0.022238684818148613 + 0.01 * 6.65888786315918
Epoch 600, val loss: 1.0383416414260864
Epoch 610, training loss: 0.08779340237379074 = 0.02115490473806858 + 0.01 * 6.663849830627441
Epoch 610, val loss: 1.0472147464752197
Epoch 620, training loss: 0.08665016293525696 = 0.02011021040380001 + 0.01 * 6.653995513916016
Epoch 620, val loss: 1.055711269378662
Epoch 630, training loss: 0.08560958504676819 = 0.019099146127700806 + 0.01 * 6.6510443687438965
Epoch 630, val loss: 1.0640687942504883
Epoch 640, training loss: 0.08471940457820892 = 0.0181118231266737 + 0.01 * 6.660758018493652
Epoch 640, val loss: 1.0722049474716187
Epoch 650, training loss: 0.08365170657634735 = 0.017138516530394554 + 0.01 * 6.6513190269470215
Epoch 650, val loss: 1.0801433324813843
Epoch 660, training loss: 0.08255699276924133 = 0.016149405390024185 + 0.01 * 6.640758514404297
Epoch 660, val loss: 1.0878645181655884
Epoch 670, training loss: 0.08155791461467743 = 0.015164668671786785 + 0.01 * 6.639325141906738
Epoch 670, val loss: 1.095329999923706
Epoch 680, training loss: 0.08052141219377518 = 0.014194780960679054 + 0.01 * 6.632663249969482
Epoch 680, val loss: 1.102705717086792
Epoch 690, training loss: 0.07971516251564026 = 0.013245117850601673 + 0.01 * 6.6470046043396
Epoch 690, val loss: 1.1101089715957642
Epoch 700, training loss: 0.07860447466373444 = 0.012319516390562057 + 0.01 * 6.628496170043945
Epoch 700, val loss: 1.1174969673156738
Epoch 710, training loss: 0.07765959948301315 = 0.01141644362360239 + 0.01 * 6.6243157386779785
Epoch 710, val loss: 1.1248700618743896
Epoch 720, training loss: 0.07673726230859756 = 0.010516257956624031 + 0.01 * 6.622100830078125
Epoch 720, val loss: 1.1322392225265503
Epoch 730, training loss: 0.07586334645748138 = 0.009664049372076988 + 0.01 * 6.619929313659668
Epoch 730, val loss: 1.1395691633224487
Epoch 740, training loss: 0.07505092769861221 = 0.008894992992281914 + 0.01 * 6.615593433380127
Epoch 740, val loss: 1.1470818519592285
Epoch 750, training loss: 0.0743669643998146 = 0.008213145658373833 + 0.01 * 6.615382194519043
Epoch 750, val loss: 1.1548835039138794
Epoch 760, training loss: 0.07372496277093887 = 0.007607976906001568 + 0.01 * 6.611699104309082
Epoch 760, val loss: 1.1629633903503418
Epoch 770, training loss: 0.07314907014369965 = 0.007086076773703098 + 0.01 * 6.60629940032959
Epoch 770, val loss: 1.1709470748901367
Epoch 780, training loss: 0.07274611294269562 = 0.006643489468842745 + 0.01 * 6.610262393951416
Epoch 780, val loss: 1.1790553331375122
Epoch 790, training loss: 0.07234323024749756 = 0.00626771803945303 + 0.01 * 6.607551574707031
Epoch 790, val loss: 1.1869711875915527
Epoch 800, training loss: 0.0719175785779953 = 0.005939505994319916 + 0.01 * 6.59780740737915
Epoch 800, val loss: 1.1946988105773926
Epoch 810, training loss: 0.07162141054868698 = 0.00564483692869544 + 0.01 * 6.597657680511475
Epoch 810, val loss: 1.2021695375442505
Epoch 820, training loss: 0.07127413153648376 = 0.0053785075433552265 + 0.01 * 6.58956241607666
Epoch 820, val loss: 1.209468960762024
Epoch 830, training loss: 0.07098818570375443 = 0.0051391590386629105 + 0.01 * 6.584902763366699
Epoch 830, val loss: 1.2165887355804443
Epoch 840, training loss: 0.07089044153690338 = 0.004922885913401842 + 0.01 * 6.596755504608154
Epoch 840, val loss: 1.2235603332519531
Epoch 850, training loss: 0.07056363672018051 = 0.00472648162394762 + 0.01 * 6.583715438842773
Epoch 850, val loss: 1.230293869972229
Epoch 860, training loss: 0.07050445675849915 = 0.00454676104709506 + 0.01 * 6.595769882202148
Epoch 860, val loss: 1.236890435218811
Epoch 870, training loss: 0.07015298306941986 = 0.0043805320747196674 + 0.01 * 6.577245235443115
Epoch 870, val loss: 1.2432823181152344
Epoch 880, training loss: 0.06997742503881454 = 0.004226066637784243 + 0.01 * 6.575136184692383
Epoch 880, val loss: 1.2495423555374146
Epoch 890, training loss: 0.06976033747196198 = 0.004081836435943842 + 0.01 * 6.567850112915039
Epoch 890, val loss: 1.2556127309799194
Epoch 900, training loss: 0.06973868608474731 = 0.00394783029332757 + 0.01 * 6.579085826873779
Epoch 900, val loss: 1.2615435123443604
Epoch 910, training loss: 0.06948716193437576 = 0.003823179053142667 + 0.01 * 6.566399097442627
Epoch 910, val loss: 1.2674146890640259
Epoch 920, training loss: 0.06935394555330276 = 0.0037066990043967962 + 0.01 * 6.564724445343018
Epoch 920, val loss: 1.2731192111968994
Epoch 930, training loss: 0.06918782740831375 = 0.0035969759337604046 + 0.01 * 6.559084892272949
Epoch 930, val loss: 1.2787302732467651
Epoch 940, training loss: 0.06908320635557175 = 0.0034935681615024805 + 0.01 * 6.558963775634766
Epoch 940, val loss: 1.2841380834579468
Epoch 950, training loss: 0.06891881674528122 = 0.003396386979147792 + 0.01 * 6.552243232727051
Epoch 950, val loss: 1.2894176244735718
Epoch 960, training loss: 0.06875208765268326 = 0.003304880578070879 + 0.01 * 6.544720649719238
Epoch 960, val loss: 1.2946507930755615
Epoch 970, training loss: 0.06870229542255402 = 0.003218352794647217 + 0.01 * 6.548394203186035
Epoch 970, val loss: 1.299744963645935
Epoch 980, training loss: 0.06860063225030899 = 0.0031364564783871174 + 0.01 * 6.546417713165283
Epoch 980, val loss: 1.3047889471054077
Epoch 990, training loss: 0.0685579851269722 = 0.0030588265508413315 + 0.01 * 6.549915790557861
Epoch 990, val loss: 1.3096990585327148
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.5646
Flip ASR: 0.5200/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.027094841003418 = 1.9433571100234985 + 0.01 * 8.373785018920898
Epoch 0, val loss: 1.9377602338790894
Epoch 10, training loss: 2.0160703659057617 = 1.9323337078094482 + 0.01 * 8.373674392700195
Epoch 10, val loss: 1.9272488355636597
Epoch 20, training loss: 2.002606153488159 = 1.9188730716705322 + 0.01 * 8.373308181762695
Epoch 20, val loss: 1.91389000415802
Epoch 30, training loss: 1.9839547872543335 = 1.900233268737793 + 0.01 * 8.372154235839844
Epoch 30, val loss: 1.8950115442276
Epoch 40, training loss: 1.95717191696167 = 1.8735096454620361 + 0.01 * 8.366232872009277
Epoch 40, val loss: 1.8681963682174683
Epoch 50, training loss: 1.9210305213928223 = 1.8376919031143188 + 0.01 * 8.333858489990234
Epoch 50, val loss: 1.834123134613037
Epoch 60, training loss: 1.8805372714996338 = 1.7984700202941895 + 0.01 * 8.2067232131958
Epoch 60, val loss: 1.8007344007492065
Epoch 70, training loss: 1.8409591913223267 = 1.7613840103149414 + 0.01 * 7.957517623901367
Epoch 70, val loss: 1.7712311744689941
Epoch 80, training loss: 1.78825044631958 = 1.714282751083374 + 0.01 * 7.3967742919921875
Epoch 80, val loss: 1.7309041023254395
Epoch 90, training loss: 1.719258189201355 = 1.6490287780761719 + 0.01 * 7.022939205169678
Epoch 90, val loss: 1.6750071048736572
Epoch 100, training loss: 1.6313538551330566 = 1.5620241165161133 + 0.01 * 6.932976722717285
Epoch 100, val loss: 1.6022244691848755
Epoch 110, training loss: 1.5289844274520874 = 1.4599257707595825 + 0.01 * 6.90587043762207
Epoch 110, val loss: 1.5194348096847534
Epoch 120, training loss: 1.4225823879241943 = 1.35368812084198 + 0.01 * 6.889425277709961
Epoch 120, val loss: 1.4366376399993896
Epoch 130, training loss: 1.3175184726715088 = 1.2487586736679077 + 0.01 * 6.875977039337158
Epoch 130, val loss: 1.3577977418899536
Epoch 140, training loss: 1.2159284353256226 = 1.1472645998001099 + 0.01 * 6.866382598876953
Epoch 140, val loss: 1.284605860710144
Epoch 150, training loss: 1.119937777519226 = 1.0513354539871216 + 0.01 * 6.8602294921875
Epoch 150, val loss: 1.2159230709075928
Epoch 160, training loss: 1.030647873878479 = 0.9620740413665771 + 0.01 * 6.857381820678711
Epoch 160, val loss: 1.151795506477356
Epoch 170, training loss: 0.9474008083343506 = 0.878831684589386 + 0.01 * 6.856912612915039
Epoch 170, val loss: 1.0912809371948242
Epoch 180, training loss: 0.8687505125999451 = 0.8001736402511597 + 0.01 * 6.857689380645752
Epoch 180, val loss: 1.0334078073501587
Epoch 190, training loss: 0.7941490411758423 = 0.7255586981773376 + 0.01 * 6.859034061431885
Epoch 190, val loss: 0.9778515696525574
Epoch 200, training loss: 0.7240356802940369 = 0.6554284691810608 + 0.01 * 6.860718727111816
Epoch 200, val loss: 0.9261254668235779
Epoch 210, training loss: 0.658942461013794 = 0.5903176665306091 + 0.01 * 6.862481117248535
Epoch 210, val loss: 0.8794445991516113
Epoch 220, training loss: 0.5987880825996399 = 0.5301478505134583 + 0.01 * 6.864023685455322
Epoch 220, val loss: 0.8386383652687073
Epoch 230, training loss: 0.5429242253303528 = 0.4742715060710907 + 0.01 * 6.865271091461182
Epoch 230, val loss: 0.8031632900238037
Epoch 240, training loss: 0.4905765950679779 = 0.42191383242607117 + 0.01 * 6.86627721786499
Epoch 240, val loss: 0.7721214890480042
Epoch 250, training loss: 0.4412239193916321 = 0.3725530505180359 + 0.01 * 6.8670878410339355
Epoch 250, val loss: 0.744976282119751
Epoch 260, training loss: 0.3949264585971832 = 0.3262488842010498 + 0.01 * 6.867758274078369
Epoch 260, val loss: 0.7216941714286804
Epoch 270, training loss: 0.3522132933139801 = 0.2835299074649811 + 0.01 * 6.868339538574219
Epoch 270, val loss: 0.7026748061180115
Epoch 280, training loss: 0.3136947453022003 = 0.2450060099363327 + 0.01 * 6.868873596191406
Epoch 280, val loss: 0.6883994340896606
Epoch 290, training loss: 0.279784619808197 = 0.21109092235565186 + 0.01 * 6.869368553161621
Epoch 290, val loss: 0.6791125535964966
Epoch 300, training loss: 0.25049617886543274 = 0.18179813027381897 + 0.01 * 6.869805812835693
Epoch 300, val loss: 0.6746533513069153
Epoch 310, training loss: 0.22554421424865723 = 0.15684249997138977 + 0.01 * 6.870171546936035
Epoch 310, val loss: 0.6747227311134338
Epoch 320, training loss: 0.20447662472724915 = 0.13577145338058472 + 0.01 * 6.870516777038574
Epoch 320, val loss: 0.6785590052604675
Epoch 330, training loss: 0.18678683042526245 = 0.11808061599731445 + 0.01 * 6.870621204376221
Epoch 330, val loss: 0.6852112412452698
Epoch 340, training loss: 0.17195716500282288 = 0.10325043648481369 + 0.01 * 6.870672702789307
Epoch 340, val loss: 0.6938771605491638
Epoch 350, training loss: 0.15949681401252747 = 0.09079193323850632 + 0.01 * 6.870487689971924
Epoch 350, val loss: 0.7039497494697571
Epoch 360, training loss: 0.1489701122045517 = 0.08026845753192902 + 0.01 * 6.8701653480529785
Epoch 360, val loss: 0.7148623466491699
Epoch 370, training loss: 0.14002090692520142 = 0.07132503390312195 + 0.01 * 6.869588375091553
Epoch 370, val loss: 0.726253867149353
Epoch 380, training loss: 0.13236546516418457 = 0.0636785700917244 + 0.01 * 6.868690490722656
Epoch 380, val loss: 0.7378636598587036
Epoch 390, training loss: 0.12578335404396057 = 0.05709869787096977 + 0.01 * 6.868466377258301
Epoch 390, val loss: 0.7496528625488281
Epoch 400, training loss: 0.12007741630077362 = 0.05141158774495125 + 0.01 * 6.866583347320557
Epoch 400, val loss: 0.7614043951034546
Epoch 410, training loss: 0.11511831730604172 = 0.04646846652030945 + 0.01 * 6.864985466003418
Epoch 410, val loss: 0.7731277346611023
Epoch 420, training loss: 0.11080624908208847 = 0.04215439409017563 + 0.01 * 6.865185737609863
Epoch 420, val loss: 0.78477942943573
Epoch 430, training loss: 0.10699963569641113 = 0.038374487310647964 + 0.01 * 6.862514972686768
Epoch 430, val loss: 0.796244204044342
Epoch 440, training loss: 0.10364188998937607 = 0.035050034523010254 + 0.01 * 6.859185695648193
Epoch 440, val loss: 0.8075448274612427
Epoch 450, training loss: 0.10070730745792389 = 0.032114941626787186 + 0.01 * 6.859236240386963
Epoch 450, val loss: 0.8186986446380615
Epoch 460, training loss: 0.09805221855640411 = 0.02951379492878914 + 0.01 * 6.853842258453369
Epoch 460, val loss: 0.8296375870704651
Epoch 470, training loss: 0.09584873914718628 = 0.027200741693377495 + 0.01 * 6.864799976348877
Epoch 470, val loss: 0.8403236269950867
Epoch 480, training loss: 0.09363606572151184 = 0.02513692155480385 + 0.01 * 6.849915027618408
Epoch 480, val loss: 0.8508056402206421
Epoch 490, training loss: 0.09172392636537552 = 0.023288942873477936 + 0.01 * 6.843498229980469
Epoch 490, val loss: 0.8610807061195374
Epoch 500, training loss: 0.09000488370656967 = 0.02162858285009861 + 0.01 * 6.837630748748779
Epoch 500, val loss: 0.8710958957672119
Epoch 510, training loss: 0.0885419249534607 = 0.02013399824500084 + 0.01 * 6.840792655944824
Epoch 510, val loss: 0.8808895349502563
Epoch 520, training loss: 0.08711051195859909 = 0.018786108121275902 + 0.01 * 6.832440376281738
Epoch 520, val loss: 0.8904538750648499
Epoch 530, training loss: 0.08577904850244522 = 0.017565416172146797 + 0.01 * 6.821363925933838
Epoch 530, val loss: 0.8997776508331299
Epoch 540, training loss: 0.08453448116779327 = 0.016457242891192436 + 0.01 * 6.8077239990234375
Epoch 540, val loss: 0.9088685512542725
Epoch 550, training loss: 0.08357848227024078 = 0.015450186096131802 + 0.01 * 6.812829971313477
Epoch 550, val loss: 0.9178138971328735
Epoch 560, training loss: 0.08248574286699295 = 0.014535786584019661 + 0.01 * 6.79499626159668
Epoch 560, val loss: 0.9263501763343811
Epoch 570, training loss: 0.0815226137638092 = 0.013703206554055214 + 0.01 * 6.781940937042236
Epoch 570, val loss: 0.9348738789558411
Epoch 580, training loss: 0.08079832792282104 = 0.01294333953410387 + 0.01 * 6.78549861907959
Epoch 580, val loss: 0.9431506395339966
Epoch 590, training loss: 0.08033101260662079 = 0.012249503284692764 + 0.01 * 6.808150768280029
Epoch 590, val loss: 0.95103520154953
Epoch 600, training loss: 0.07943998277187347 = 0.011613648384809494 + 0.01 * 6.782633304595947
Epoch 600, val loss: 0.9588354229927063
Epoch 610, training loss: 0.07857419550418854 = 0.011030512861907482 + 0.01 * 6.754368782043457
Epoch 610, val loss: 0.9664342999458313
Epoch 620, training loss: 0.07795572280883789 = 0.010493245907127857 + 0.01 * 6.7462477684021
Epoch 620, val loss: 0.9738589525222778
Epoch 630, training loss: 0.07700209319591522 = 0.009997384622693062 + 0.01 * 6.7004714012146
Epoch 630, val loss: 0.9809580445289612
Epoch 640, training loss: 0.07663014531135559 = 0.009538332931697369 + 0.01 * 6.709181308746338
Epoch 640, val loss: 0.9879639744758606
Epoch 650, training loss: 0.07595540583133698 = 0.009114100597798824 + 0.01 * 6.684130668640137
Epoch 650, val loss: 0.9947766065597534
Epoch 660, training loss: 0.07541412115097046 = 0.008719437755644321 + 0.01 * 6.669468402862549
Epoch 660, val loss: 1.0014809370040894
Epoch 670, training loss: 0.07504630088806152 = 0.008352276869118214 + 0.01 * 6.669402599334717
Epoch 670, val loss: 1.0080294609069824
Epoch 680, training loss: 0.07453474402427673 = 0.00800993200391531 + 0.01 * 6.652481555938721
Epoch 680, val loss: 1.014385461807251
Epoch 690, training loss: 0.0741434171795845 = 0.00769054563716054 + 0.01 * 6.645287036895752
Epoch 690, val loss: 1.0205769538879395
Epoch 700, training loss: 0.07360881567001343 = 0.007391433697193861 + 0.01 * 6.621738910675049
Epoch 700, val loss: 1.0266740322113037
Epoch 710, training loss: 0.0732630267739296 = 0.00711149862036109 + 0.01 * 6.6151533126831055
Epoch 710, val loss: 1.0326849222183228
Epoch 720, training loss: 0.07292229682207108 = 0.006848747376352549 + 0.01 * 6.607355117797852
Epoch 720, val loss: 1.0384712219238281
Epoch 730, training loss: 0.07289311289787292 = 0.006601585075259209 + 0.01 * 6.629152774810791
Epoch 730, val loss: 1.044173002243042
Epoch 740, training loss: 0.07231956720352173 = 0.006370197515934706 + 0.01 * 6.594937801361084
Epoch 740, val loss: 1.0497064590454102
Epoch 750, training loss: 0.07218902558088303 = 0.006151448469609022 + 0.01 * 6.603758335113525
Epoch 750, val loss: 1.0551681518554688
Epoch 760, training loss: 0.0717344582080841 = 0.005945142824202776 + 0.01 * 6.5789313316345215
Epoch 760, val loss: 1.0604490041732788
Epoch 770, training loss: 0.07151179015636444 = 0.005750360898673534 + 0.01 * 6.576143264770508
Epoch 770, val loss: 1.0656465291976929
Epoch 780, training loss: 0.07130102813243866 = 0.005566593259572983 + 0.01 * 6.57344388961792
Epoch 780, val loss: 1.0707006454467773
Epoch 790, training loss: 0.07101338356733322 = 0.005391840822994709 + 0.01 * 6.5621538162231445
Epoch 790, val loss: 1.0757410526275635
Epoch 800, training loss: 0.07083997875452042 = 0.005226594395935535 + 0.01 * 6.561338424682617
Epoch 800, val loss: 1.0806093215942383
Epoch 810, training loss: 0.070553719997406 = 0.0050696562975645065 + 0.01 * 6.54840612411499
Epoch 810, val loss: 1.0854488611221313
Epoch 820, training loss: 0.07079291343688965 = 0.0049202642403542995 + 0.01 * 6.587264537811279
Epoch 820, val loss: 1.090218186378479
Epoch 830, training loss: 0.07016414403915405 = 0.00477895513176918 + 0.01 * 6.538518905639648
Epoch 830, val loss: 1.0948103666305542
Epoch 840, training loss: 0.07026660442352295 = 0.0046453881077468395 + 0.01 * 6.562121868133545
Epoch 840, val loss: 1.099313497543335
Epoch 850, training loss: 0.0698828250169754 = 0.004517622757703066 + 0.01 * 6.536520481109619
Epoch 850, val loss: 1.1037721633911133
Epoch 860, training loss: 0.06974781304597855 = 0.004395776893943548 + 0.01 * 6.53520393371582
Epoch 860, val loss: 1.108171820640564
Epoch 870, training loss: 0.06962311267852783 = 0.00427889171987772 + 0.01 * 6.534422397613525
Epoch 870, val loss: 1.1125214099884033
Epoch 880, training loss: 0.06951089948415756 = 0.004167696461081505 + 0.01 * 6.53432035446167
Epoch 880, val loss: 1.116714358329773
Epoch 890, training loss: 0.069191113114357 = 0.004062274936586618 + 0.01 * 6.512884140014648
Epoch 890, val loss: 1.1208078861236572
Epoch 900, training loss: 0.06902996450662613 = 0.003961664158850908 + 0.01 * 6.506830215454102
Epoch 900, val loss: 1.1248379945755005
Epoch 910, training loss: 0.06914032250642776 = 0.0038652329239994287 + 0.01 * 6.527508735656738
Epoch 910, val loss: 1.1288007497787476
Epoch 920, training loss: 0.06875217705965042 = 0.0037728813476860523 + 0.01 * 6.497929573059082
Epoch 920, val loss: 1.1327564716339111
Epoch 930, training loss: 0.06855178624391556 = 0.003684433875605464 + 0.01 * 6.4867353439331055
Epoch 930, val loss: 1.1365759372711182
Epoch 940, training loss: 0.06847457587718964 = 0.0035998777020722628 + 0.01 * 6.487470626831055
Epoch 940, val loss: 1.140321135520935
Epoch 950, training loss: 0.06840862333774567 = 0.0035183136351406574 + 0.01 * 6.4890313148498535
Epoch 950, val loss: 1.1440588235855103
Epoch 960, training loss: 0.06825212389230728 = 0.003440296510234475 + 0.01 * 6.48118257522583
Epoch 960, val loss: 1.1476906538009644
Epoch 970, training loss: 0.06808990985155106 = 0.00336510525085032 + 0.01 * 6.472480297088623
Epoch 970, val loss: 1.1512928009033203
Epoch 980, training loss: 0.06813330948352814 = 0.0032932807225733995 + 0.01 * 6.484003067016602
Epoch 980, val loss: 1.1547932624816895
Epoch 990, training loss: 0.06791302561759949 = 0.0032241628505289555 + 0.01 * 6.468886375427246
Epoch 990, val loss: 1.1582101583480835
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.7417
Flip ASR: 0.6933/225 nodes
The final ASR:0.63592, 0.07630, Accuracy:0.80988, 0.02014
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11620])
remove edge: torch.Size([2, 9468])
updated graph: torch.Size([2, 10532])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.97909, 0.00696, Accuracy:0.83333, 0.00800
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.5 der2:0.2 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0203661918640137 = 1.9366284608840942 + 0.01 * 8.373764991760254
Epoch 0, val loss: 1.9340825080871582
Epoch 10, training loss: 2.011054754257202 = 1.9273180961608887 + 0.01 * 8.373661994934082
Epoch 10, val loss: 1.9250167608261108
Epoch 20, training loss: 1.9994758367538452 = 1.9157428741455078 + 0.01 * 8.37330150604248
Epoch 20, val loss: 1.9137986898422241
Epoch 30, training loss: 1.9829246997833252 = 1.8992012739181519 + 0.01 * 8.372339248657227
Epoch 30, val loss: 1.8980844020843506
Epoch 40, training loss: 1.9581726789474487 = 1.874492883682251 + 0.01 * 8.367979049682617
Epoch 40, val loss: 1.8752186298370361
Epoch 50, training loss: 1.9227252006530762 = 1.8394030332565308 + 0.01 * 8.332215309143066
Epoch 50, val loss: 1.8445203304290771
Epoch 60, training loss: 1.8788201808929443 = 1.7982360124588013 + 0.01 * 8.0584135055542
Epoch 60, val loss: 1.8123279809951782
Epoch 70, training loss: 1.8329061269760132 = 1.7567517757415771 + 0.01 * 7.6154398918151855
Epoch 70, val loss: 1.7803964614868164
Epoch 80, training loss: 1.7756843566894531 = 1.7029945850372314 + 0.01 * 7.268979549407959
Epoch 80, val loss: 1.7330974340438843
Epoch 90, training loss: 1.6996898651123047 = 1.6289827823638916 + 0.01 * 7.070714473724365
Epoch 90, val loss: 1.6690244674682617
Epoch 100, training loss: 1.6039336919784546 = 1.5340614318847656 + 0.01 * 6.98722505569458
Epoch 100, val loss: 1.590425729751587
Epoch 110, training loss: 1.498860239982605 = 1.4292049407958984 + 0.01 * 6.965533256530762
Epoch 110, val loss: 1.5055469274520874
Epoch 120, training loss: 1.393218755722046 = 1.323758602142334 + 0.01 * 6.946012496948242
Epoch 120, val loss: 1.423522710800171
Epoch 130, training loss: 1.2908397912979126 = 1.2216370105743408 + 0.01 * 6.9202752113342285
Epoch 130, val loss: 1.3462761640548706
Epoch 140, training loss: 1.1942870616912842 = 1.1253737211227417 + 0.01 * 6.891328811645508
Epoch 140, val loss: 1.2756216526031494
Epoch 150, training loss: 1.106217384338379 = 1.037534475326538 + 0.01 * 6.868289947509766
Epoch 150, val loss: 1.2123126983642578
Epoch 160, training loss: 1.0270986557006836 = 0.9585376977920532 + 0.01 * 6.85609245300293
Epoch 160, val loss: 1.1564711332321167
Epoch 170, training loss: 0.9546005725860596 = 0.8860844373703003 + 0.01 * 6.851613998413086
Epoch 170, val loss: 1.1058567762374878
Epoch 180, training loss: 0.8847787976264954 = 0.8163106441497803 + 0.01 * 6.846813201904297
Epoch 180, val loss: 1.0569641590118408
Epoch 190, training loss: 0.8146680593490601 = 0.7462284564971924 + 0.01 * 6.843963623046875
Epoch 190, val loss: 1.0074044466018677
Epoch 200, training loss: 0.7437466382980347 = 0.6753286719322205 + 0.01 * 6.841799259185791
Epoch 200, val loss: 0.9570102095603943
Epoch 210, training loss: 0.6741650700569153 = 0.6057719588279724 + 0.01 * 6.839310169219971
Epoch 210, val loss: 0.9082382321357727
Epoch 220, training loss: 0.6087716817855835 = 0.5404098033905029 + 0.01 * 6.836188793182373
Epoch 220, val loss: 0.8646463751792908
Epoch 230, training loss: 0.5493578910827637 = 0.4810362756252289 + 0.01 * 6.832164287567139
Epoch 230, val loss: 0.8285893201828003
Epoch 240, training loss: 0.4963446259498596 = 0.42804211378097534 + 0.01 * 6.830252170562744
Epoch 240, val loss: 0.8007622361183167
Epoch 250, training loss: 0.44907036423683167 = 0.3808388113975525 + 0.01 * 6.823155879974365
Epoch 250, val loss: 0.780300498008728
Epoch 260, training loss: 0.40659135580062866 = 0.33842724561691284 + 0.01 * 6.81641149520874
Epoch 260, val loss: 0.7657734751701355
Epoch 270, training loss: 0.3680105209350586 = 0.29988980293273926 + 0.01 * 6.812073707580566
Epoch 270, val loss: 0.756027340888977
Epoch 280, training loss: 0.33249637484550476 = 0.2644895315170288 + 0.01 * 6.800683975219727
Epoch 280, val loss: 0.7503446936607361
Epoch 290, training loss: 0.2997068166732788 = 0.23178674280643463 + 0.01 * 6.792008399963379
Epoch 290, val loss: 0.7480974197387695
Epoch 300, training loss: 0.2695519030094147 = 0.2017269730567932 + 0.01 * 6.7824931144714355
Epoch 300, val loss: 0.7490103244781494
Epoch 310, training loss: 0.24231213331222534 = 0.1745721697807312 + 0.01 * 6.773996353149414
Epoch 310, val loss: 0.752912700176239
Epoch 320, training loss: 0.2183704972267151 = 0.15069572627544403 + 0.01 * 6.767476558685303
Epoch 320, val loss: 0.7597297430038452
Epoch 330, training loss: 0.19781172275543213 = 0.1302834004163742 + 0.01 * 6.752831935882568
Epoch 330, val loss: 0.7691539525985718
Epoch 340, training loss: 0.18063899874687195 = 0.11317597329616547 + 0.01 * 6.746303558349609
Epoch 340, val loss: 0.7808845043182373
Epoch 350, training loss: 0.16665786504745483 = 0.09898292273283005 + 0.01 * 6.767495155334473
Epoch 350, val loss: 0.7944821119308472
Epoch 360, training loss: 0.15459847450256348 = 0.087223120033741 + 0.01 * 6.737534999847412
Epoch 360, val loss: 0.8095458745956421
Epoch 370, training loss: 0.1446157991886139 = 0.077393539249897 + 0.01 * 6.722225189208984
Epoch 370, val loss: 0.8257790803909302
Epoch 380, training loss: 0.13619911670684814 = 0.06908473372459412 + 0.01 * 6.7114386558532715
Epoch 380, val loss: 0.8428758978843689
Epoch 390, training loss: 0.1291750967502594 = 0.06199119985103607 + 0.01 * 6.718389511108398
Epoch 390, val loss: 0.8605681657791138
Epoch 400, training loss: 0.12294384837150574 = 0.0558837428689003 + 0.01 * 6.706010818481445
Epoch 400, val loss: 0.8786135911941528
Epoch 410, training loss: 0.11753688752651215 = 0.05057554319500923 + 0.01 * 6.696134567260742
Epoch 410, val loss: 0.8969070315361023
Epoch 420, training loss: 0.11278849840164185 = 0.045930638909339905 + 0.01 * 6.685786247253418
Epoch 420, val loss: 0.9152981638908386
Epoch 430, training loss: 0.10867501050233841 = 0.04184235632419586 + 0.01 * 6.683265209197998
Epoch 430, val loss: 0.9336438775062561
Epoch 440, training loss: 0.10496630519628525 = 0.03822608292102814 + 0.01 * 6.674022197723389
Epoch 440, val loss: 0.9518272280693054
Epoch 450, training loss: 0.10179010033607483 = 0.035018544644117355 + 0.01 * 6.677156448364258
Epoch 450, val loss: 0.969831109046936
Epoch 460, training loss: 0.09878724068403244 = 0.032167948782444 + 0.01 * 6.661929130554199
Epoch 460, val loss: 0.9875194430351257
Epoch 470, training loss: 0.09635193645954132 = 0.029627013951539993 + 0.01 * 6.672492980957031
Epoch 470, val loss: 1.0048413276672363
Epoch 480, training loss: 0.0939132496714592 = 0.027360253036022186 + 0.01 * 6.655299663543701
Epoch 480, val loss: 1.0217416286468506
Epoch 490, training loss: 0.09189411252737045 = 0.02533084899187088 + 0.01 * 6.6563262939453125
Epoch 490, val loss: 1.0382000207901
Epoch 500, training loss: 0.09004224091768265 = 0.023509740829467773 + 0.01 * 6.653250217437744
Epoch 500, val loss: 1.0541837215423584
Epoch 510, training loss: 0.08824041485786438 = 0.021871833130717278 + 0.01 * 6.636857986450195
Epoch 510, val loss: 1.0697052478790283
Epoch 520, training loss: 0.08706618845462799 = 0.020392896607518196 + 0.01 * 6.667329788208008
Epoch 520, val loss: 1.0847159624099731
Epoch 530, training loss: 0.08532484620809555 = 0.019056910648941994 + 0.01 * 6.62679386138916
Epoch 530, val loss: 1.0992801189422607
Epoch 540, training loss: 0.08408097922801971 = 0.017845476046204567 + 0.01 * 6.623550891876221
Epoch 540, val loss: 1.1134033203125
Epoch 550, training loss: 0.0828942209482193 = 0.01674254797399044 + 0.01 * 6.615167140960693
Epoch 550, val loss: 1.1270326375961304
Epoch 560, training loss: 0.08213218301534653 = 0.01573597826063633 + 0.01 * 6.639620780944824
Epoch 560, val loss: 1.1402746438980103
Epoch 570, training loss: 0.08092119544744492 = 0.014817115850746632 + 0.01 * 6.610407829284668
Epoch 570, val loss: 1.1531007289886475
Epoch 580, training loss: 0.08009586483240128 = 0.01397619117051363 + 0.01 * 6.611967086791992
Epoch 580, val loss: 1.1655139923095703
Epoch 590, training loss: 0.07917515188455582 = 0.013204768300056458 + 0.01 * 6.597038745880127
Epoch 590, val loss: 1.1775356531143188
Epoch 600, training loss: 0.07868466526269913 = 0.012495567090809345 + 0.01 * 6.61890983581543
Epoch 600, val loss: 1.1892404556274414
Epoch 610, training loss: 0.07790908217430115 = 0.011843298561871052 + 0.01 * 6.6065778732299805
Epoch 610, val loss: 1.200537919998169
Epoch 620, training loss: 0.0770699605345726 = 0.011241673491895199 + 0.01 * 6.582828521728516
Epoch 620, val loss: 1.2115715742111206
Epoch 630, training loss: 0.07673963904380798 = 0.010685351677238941 + 0.01 * 6.605428695678711
Epoch 630, val loss: 1.222237467765808
Epoch 640, training loss: 0.07606783509254456 = 0.010170797817409039 + 0.01 * 6.5897040367126465
Epoch 640, val loss: 1.2326551675796509
Epoch 650, training loss: 0.07555843144655228 = 0.009693724103271961 + 0.01 * 6.586470603942871
Epoch 650, val loss: 1.2426265478134155
Epoch 660, training loss: 0.07494758814573288 = 0.009250923991203308 + 0.01 * 6.569666385650635
Epoch 660, val loss: 1.25248122215271
Epoch 670, training loss: 0.07459644228219986 = 0.008838890120387077 + 0.01 * 6.5757551193237305
Epoch 670, val loss: 1.261957049369812
Epoch 680, training loss: 0.07403373718261719 = 0.00845553632825613 + 0.01 * 6.5578203201293945
Epoch 680, val loss: 1.2712231874465942
Epoch 690, training loss: 0.07370907813310623 = 0.008097781799733639 + 0.01 * 6.561129570007324
Epoch 690, val loss: 1.2801847457885742
Epoch 700, training loss: 0.07328731566667557 = 0.007763855624943972 + 0.01 * 6.552346229553223
Epoch 700, val loss: 1.2889502048492432
Epoch 710, training loss: 0.07320713996887207 = 0.007451599929481745 + 0.01 * 6.575554370880127
Epoch 710, val loss: 1.2974779605865479
Epoch 720, training loss: 0.07269410789012909 = 0.007159776519984007 + 0.01 * 6.553432941436768
Epoch 720, val loss: 1.3057838678359985
Epoch 730, training loss: 0.07228973507881165 = 0.006886220537126064 + 0.01 * 6.540351390838623
Epoch 730, val loss: 1.3138607740402222
Epoch 740, training loss: 0.07198593765497208 = 0.00662937480956316 + 0.01 * 6.535655975341797
Epoch 740, val loss: 1.321694016456604
Epoch 750, training loss: 0.07207267731428146 = 0.0063878376968204975 + 0.01 * 6.568484306335449
Epoch 750, val loss: 1.3294084072113037
Epoch 760, training loss: 0.07137125730514526 = 0.006160695105791092 + 0.01 * 6.521056175231934
Epoch 760, val loss: 1.3369311094284058
Epoch 770, training loss: 0.07119514793157578 = 0.005946777760982513 + 0.01 * 6.524837493896484
Epoch 770, val loss: 1.3442859649658203
Epoch 780, training loss: 0.07090099155902863 = 0.0057449303567409515 + 0.01 * 6.515606880187988
Epoch 780, val loss: 1.35140061378479
Epoch 790, training loss: 0.0707886666059494 = 0.005554910749197006 + 0.01 * 6.523375988006592
Epoch 790, val loss: 1.3584620952606201
Epoch 800, training loss: 0.07055767625570297 = 0.005375055130571127 + 0.01 * 6.5182623863220215
Epoch 800, val loss: 1.365299940109253
Epoch 810, training loss: 0.07025543600320816 = 0.0052051483653485775 + 0.01 * 6.505029201507568
Epoch 810, val loss: 1.3720427751541138
Epoch 820, training loss: 0.07014850527048111 = 0.005044026765972376 + 0.01 * 6.510447978973389
Epoch 820, val loss: 1.378561019897461
Epoch 830, training loss: 0.0698075219988823 = 0.0048916335217654705 + 0.01 * 6.491589069366455
Epoch 830, val loss: 1.385002851486206
Epoch 840, training loss: 0.06979212164878845 = 0.004747230559587479 + 0.01 * 6.504489898681641
Epoch 840, val loss: 1.3912577629089355
Epoch 850, training loss: 0.06941285729408264 = 0.004610057920217514 + 0.01 * 6.480279445648193
Epoch 850, val loss: 1.3974075317382812
Epoch 860, training loss: 0.06923631578683853 = 0.0044795493595302105 + 0.01 * 6.475676536560059
Epoch 860, val loss: 1.4032801389694214
Epoch 870, training loss: 0.06919805705547333 = 0.004355782642960548 + 0.01 * 6.484228134155273
Epoch 870, val loss: 1.4091920852661133
Epoch 880, training loss: 0.06898189336061478 = 0.004237798508256674 + 0.01 * 6.474409103393555
Epoch 880, val loss: 1.4149432182312012
Epoch 890, training loss: 0.06894800812005997 = 0.004125627223402262 + 0.01 * 6.48223876953125
Epoch 890, val loss: 1.420562505722046
Epoch 900, training loss: 0.06857720762491226 = 0.004018260631710291 + 0.01 * 6.455894470214844
Epoch 900, val loss: 1.42610502243042
Epoch 910, training loss: 0.06870555132627487 = 0.003915830515325069 + 0.01 * 6.478972434997559
Epoch 910, val loss: 1.4314264059066772
Epoch 920, training loss: 0.06855321675539017 = 0.0038182237185537815 + 0.01 * 6.473499774932861
Epoch 920, val loss: 1.4368057250976562
Epoch 930, training loss: 0.06823114305734634 = 0.0037249273154884577 + 0.01 * 6.450622081756592
Epoch 930, val loss: 1.4419220685958862
Epoch 940, training loss: 0.06800422072410583 = 0.0036356900818645954 + 0.01 * 6.436853408813477
Epoch 940, val loss: 1.4469743967056274
Epoch 950, training loss: 0.06803315132856369 = 0.0035503909457474947 + 0.01 * 6.448276519775391
Epoch 950, val loss: 1.4519838094711304
Epoch 960, training loss: 0.06812131404876709 = 0.0034683544654399157 + 0.01 * 6.465295791625977
Epoch 960, val loss: 1.4567513465881348
Epoch 970, training loss: 0.06777263432741165 = 0.0033899452537298203 + 0.01 * 6.438268661499023
Epoch 970, val loss: 1.4616001844406128
Epoch 980, training loss: 0.0675734207034111 = 0.003314610803499818 + 0.01 * 6.4258809089660645
Epoch 980, val loss: 1.4662950038909912
Epoch 990, training loss: 0.06766251474618912 = 0.0032423301599919796 + 0.01 * 6.442018985748291
Epoch 990, val loss: 1.4709049463272095
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.7528
Flip ASR: 0.7067/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0474326610565186 = 1.9636952877044678 + 0.01 * 8.373738288879395
Epoch 0, val loss: 1.968699336051941
Epoch 10, training loss: 2.0365371704101562 = 1.952800989151001 + 0.01 * 8.373608589172363
Epoch 10, val loss: 1.9577440023422241
Epoch 20, training loss: 2.023418426513672 = 1.9396865367889404 + 0.01 * 8.373181343078613
Epoch 20, val loss: 1.9440057277679443
Epoch 30, training loss: 2.0054190158843994 = 1.921699047088623 + 0.01 * 8.371987342834473
Epoch 30, val loss: 1.9245938062667847
Epoch 40, training loss: 1.9790844917297363 = 1.8954170942306519 + 0.01 * 8.366734504699707
Epoch 40, val loss: 1.8959300518035889
Epoch 50, training loss: 1.9410905838012695 = 1.8577977418899536 + 0.01 * 8.3292875289917
Epoch 50, val loss: 1.8556221723556519
Epoch 60, training loss: 1.8904974460601807 = 1.810200810432434 + 0.01 * 8.029662132263184
Epoch 60, val loss: 1.8073925971984863
Epoch 70, training loss: 1.8361024856567383 = 1.7608281373977661 + 0.01 * 7.527434825897217
Epoch 70, val loss: 1.7608433961868286
Epoch 80, training loss: 1.7808250188827515 = 1.7079263925552368 + 0.01 * 7.2898640632629395
Epoch 80, val loss: 1.7119083404541016
Epoch 90, training loss: 1.70950448513031 = 1.637664556503296 + 0.01 * 7.183996677398682
Epoch 90, val loss: 1.64896559715271
Epoch 100, training loss: 1.6175590753555298 = 1.546288251876831 + 0.01 * 7.127081871032715
Epoch 100, val loss: 1.570695400238037
Epoch 110, training loss: 1.510521411895752 = 1.4396066665649414 + 0.01 * 7.091476917266846
Epoch 110, val loss: 1.482222318649292
Epoch 120, training loss: 1.4012842178344727 = 1.3305935859680176 + 0.01 * 7.069064140319824
Epoch 120, val loss: 1.3951146602630615
Epoch 130, training loss: 1.2972378730773926 = 1.2268351316452026 + 0.01 * 7.040271759033203
Epoch 130, val loss: 1.3164706230163574
Epoch 140, training loss: 1.1986572742462158 = 1.128576397895813 + 0.01 * 7.008083820343018
Epoch 140, val loss: 1.2437258958816528
Epoch 150, training loss: 1.1044416427612305 = 1.034636378288269 + 0.01 * 6.98052453994751
Epoch 150, val loss: 1.1747316122055054
Epoch 160, training loss: 1.0153038501739502 = 0.9456949234008789 + 0.01 * 6.960890293121338
Epoch 160, val loss: 1.11005699634552
Epoch 170, training loss: 0.933599054813385 = 0.864154577255249 + 0.01 * 6.944448471069336
Epoch 170, val loss: 1.051445484161377
Epoch 180, training loss: 0.8621637225151062 = 0.792917013168335 + 0.01 * 6.924673557281494
Epoch 180, val loss: 1.0015305280685425
Epoch 190, training loss: 0.8025577664375305 = 0.733559787273407 + 0.01 * 6.8997955322265625
Epoch 190, val loss: 0.9624329805374146
Epoch 200, training loss: 0.7537656426429749 = 0.6850436925888062 + 0.01 * 6.872193336486816
Epoch 200, val loss: 0.9338755011558533
Epoch 210, training loss: 0.7130154371261597 = 0.6445102095603943 + 0.01 * 6.85051965713501
Epoch 210, val loss: 0.9135854840278625
Epoch 220, training loss: 0.6771153211593628 = 0.6087675094604492 + 0.01 * 6.834782600402832
Epoch 220, val loss: 0.8984670639038086
Epoch 230, training loss: 0.6436012387275696 = 0.5753805637359619 + 0.01 * 6.822070121765137
Epoch 230, val loss: 0.8859280347824097
Epoch 240, training loss: 0.6109999418258667 = 0.5428748726844788 + 0.01 * 6.812504291534424
Epoch 240, val loss: 0.8744543790817261
Epoch 250, training loss: 0.5787220001220703 = 0.5106663703918457 + 0.01 * 6.805565357208252
Epoch 250, val loss: 0.8630374670028687
Epoch 260, training loss: 0.5466349124908447 = 0.4787105321884155 + 0.01 * 6.792438507080078
Epoch 260, val loss: 0.8522856831550598
Epoch 270, training loss: 0.5148741006851196 = 0.44708260893821716 + 0.01 * 6.779150485992432
Epoch 270, val loss: 0.8422213792800903
Epoch 280, training loss: 0.48361584544181824 = 0.41587164998054504 + 0.01 * 6.774420738220215
Epoch 280, val loss: 0.8338336944580078
Epoch 290, training loss: 0.45263099670410156 = 0.38505515456199646 + 0.01 * 6.757585048675537
Epoch 290, val loss: 0.8281751275062561
Epoch 300, training loss: 0.42205554246902466 = 0.35457566380500793 + 0.01 * 6.747987270355225
Epoch 300, val loss: 0.8252987861633301
Epoch 310, training loss: 0.39198487997055054 = 0.32457542419433594 + 0.01 * 6.740944862365723
Epoch 310, val loss: 0.825476884841919
Epoch 320, training loss: 0.36281269788742065 = 0.295443594455719 + 0.01 * 6.736910343170166
Epoch 320, val loss: 0.8285096883773804
Epoch 330, training loss: 0.334754079580307 = 0.26741334795951843 + 0.01 * 6.734073638916016
Epoch 330, val loss: 0.8339568972587585
Epoch 340, training loss: 0.3077547550201416 = 0.24044129252433777 + 0.01 * 6.7313456535339355
Epoch 340, val loss: 0.8418925404548645
Epoch 350, training loss: 0.28168171644210815 = 0.21438920497894287 + 0.01 * 6.729252815246582
Epoch 350, val loss: 0.8507905602455139
Epoch 360, training loss: 0.2566705346107483 = 0.18939441442489624 + 0.01 * 6.72761344909668
Epoch 360, val loss: 0.8600289821624756
Epoch 370, training loss: 0.23306825757026672 = 0.16580750048160553 + 0.01 * 6.726076602935791
Epoch 370, val loss: 0.8693092465400696
Epoch 380, training loss: 0.21134765446186066 = 0.14410366117954254 + 0.01 * 6.724399566650391
Epoch 380, val loss: 0.87856125831604
Epoch 390, training loss: 0.19194647669792175 = 0.1247255951166153 + 0.01 * 6.7220892906188965
Epoch 390, val loss: 0.8875032067298889
Epoch 400, training loss: 0.17502379417419434 = 0.10783203691244125 + 0.01 * 6.719176292419434
Epoch 400, val loss: 0.8966027498245239
Epoch 410, training loss: 0.16047018766403198 = 0.0933130532503128 + 0.01 * 6.715713024139404
Epoch 410, val loss: 0.9059652090072632
Epoch 420, training loss: 0.14813244342803955 = 0.08095519989728928 + 0.01 * 6.7177252769470215
Epoch 420, val loss: 0.9156903624534607
Epoch 430, training loss: 0.13767173886299133 = 0.07055964320898056 + 0.01 * 6.711209297180176
Epoch 430, val loss: 0.9257969260215759
Epoch 440, training loss: 0.12888523936271667 = 0.061843760311603546 + 0.01 * 6.704148769378662
Epoch 440, val loss: 0.9361315369606018
Epoch 450, training loss: 0.12151923775672913 = 0.05452040210366249 + 0.01 * 6.699884414672852
Epoch 450, val loss: 0.9469816088676453
Epoch 460, training loss: 0.11535510420799255 = 0.04835974797606468 + 0.01 * 6.699535846710205
Epoch 460, val loss: 0.9579355716705322
Epoch 470, training loss: 0.11009746789932251 = 0.043158937245607376 + 0.01 * 6.693853855133057
Epoch 470, val loss: 0.9693086743354797
Epoch 480, training loss: 0.10561759769916534 = 0.03874168172478676 + 0.01 * 6.687591552734375
Epoch 480, val loss: 0.9807164072990417
Epoch 490, training loss: 0.10179643332958221 = 0.03496798872947693 + 0.01 * 6.682844638824463
Epoch 490, val loss: 0.9921671748161316
Epoch 500, training loss: 0.09853445738554001 = 0.03172025829553604 + 0.01 * 6.68142032623291
Epoch 500, val loss: 1.0035141706466675
Epoch 510, training loss: 0.09564080089330673 = 0.028911730274558067 + 0.01 * 6.672906875610352
Epoch 510, val loss: 1.0148800611495972
Epoch 520, training loss: 0.09315987676382065 = 0.026469850912690163 + 0.01 * 6.669003009796143
Epoch 520, val loss: 1.02608060836792
Epoch 530, training loss: 0.09096856415271759 = 0.024325992912054062 + 0.01 * 6.664258003234863
Epoch 530, val loss: 1.03696608543396
Epoch 540, training loss: 0.08903352916240692 = 0.02243674546480179 + 0.01 * 6.6596784591674805
Epoch 540, val loss: 1.0477550029754639
Epoch 550, training loss: 0.08756433427333832 = 0.020769905298948288 + 0.01 * 6.679442882537842
Epoch 550, val loss: 1.0582319498062134
Epoch 560, training loss: 0.08585640788078308 = 0.019292639568448067 + 0.01 * 6.65637731552124
Epoch 560, val loss: 1.068485975265503
Epoch 570, training loss: 0.08448086678981781 = 0.017972584813833237 + 0.01 * 6.6508283615112305
Epoch 570, val loss: 1.0784680843353271
Epoch 580, training loss: 0.0832381621003151 = 0.016784826293587685 + 0.01 * 6.645334243774414
Epoch 580, val loss: 1.0883123874664307
Epoch 590, training loss: 0.08213368058204651 = 0.015707891434431076 + 0.01 * 6.642579555511475
Epoch 590, val loss: 1.0979787111282349
Epoch 600, training loss: 0.08122377097606659 = 0.014727823436260223 + 0.01 * 6.649594783782959
Epoch 600, val loss: 1.1073459386825562
Epoch 610, training loss: 0.0802294909954071 = 0.013835624791681767 + 0.01 * 6.6393866539001465
Epoch 610, val loss: 1.1165697574615479
Epoch 620, training loss: 0.07935639470815659 = 0.013017924502491951 + 0.01 * 6.633847236633301
Epoch 620, val loss: 1.1256413459777832
Epoch 630, training loss: 0.07863698154687881 = 0.012266055680811405 + 0.01 * 6.637092590332031
Epoch 630, val loss: 1.134385108947754
Epoch 640, training loss: 0.07785968482494354 = 0.01157778687775135 + 0.01 * 6.628190040588379
Epoch 640, val loss: 1.143041968345642
Epoch 650, training loss: 0.07717785239219666 = 0.010943806730210781 + 0.01 * 6.623404502868652
Epoch 650, val loss: 1.1513937711715698
Epoch 660, training loss: 0.07665330171585083 = 0.010358459316194057 + 0.01 * 6.6294846534729
Epoch 660, val loss: 1.1595683097839355
Epoch 670, training loss: 0.07604100555181503 = 0.009818767197430134 + 0.01 * 6.622223377227783
Epoch 670, val loss: 1.1675833463668823
Epoch 680, training loss: 0.07549110054969788 = 0.009319879114627838 + 0.01 * 6.617122173309326
Epoch 680, val loss: 1.1753391027450562
Epoch 690, training loss: 0.07502729445695877 = 0.008859433233737946 + 0.01 * 6.616786479949951
Epoch 690, val loss: 1.1829338073730469
Epoch 700, training loss: 0.07461319863796234 = 0.008433341048657894 + 0.01 * 6.617985725402832
Epoch 700, val loss: 1.1904031038284302
Epoch 710, training loss: 0.07415900379419327 = 0.008038443513214588 + 0.01 * 6.612056255340576
Epoch 710, val loss: 1.1975868940353394
Epoch 720, training loss: 0.07385524362325668 = 0.007672044448554516 + 0.01 * 6.618319988250732
Epoch 720, val loss: 1.2045915126800537
Epoch 730, training loss: 0.073400117456913 = 0.007332519628107548 + 0.01 * 6.606760025024414
Epoch 730, val loss: 1.2114866971969604
Epoch 740, training loss: 0.0730372816324234 = 0.007015664596110582 + 0.01 * 6.602161884307861
Epoch 740, val loss: 1.2181648015975952
Epoch 750, training loss: 0.07267899066209793 = 0.006720368750393391 + 0.01 * 6.595861911773682
Epoch 750, val loss: 1.2247393131256104
Epoch 760, training loss: 0.0725511685013771 = 0.006444036494940519 + 0.01 * 6.610713481903076
Epoch 760, val loss: 1.2310322523117065
Epoch 770, training loss: 0.07218596339225769 = 0.006186733953654766 + 0.01 * 6.599923133850098
Epoch 770, val loss: 1.2372353076934814
Epoch 780, training loss: 0.07187086343765259 = 0.005945370066910982 + 0.01 * 6.5925493240356445
Epoch 780, val loss: 1.2432798147201538
Epoch 790, training loss: 0.0716058686375618 = 0.005719901993870735 + 0.01 * 6.588597297668457
Epoch 790, val loss: 1.2491194009780884
Epoch 800, training loss: 0.07137338817119598 = 0.005507943220436573 + 0.01 * 6.586544036865234
Epoch 800, val loss: 1.2548781633377075
Epoch 810, training loss: 0.07130467146635056 = 0.0053092604503035545 + 0.01 * 6.599541187286377
Epoch 810, val loss: 1.260538935661316
Epoch 820, training loss: 0.07098951935768127 = 0.005121871829032898 + 0.01 * 6.586764812469482
Epoch 820, val loss: 1.2659761905670166
Epoch 830, training loss: 0.0707249715924263 = 0.0049460106529295444 + 0.01 * 6.5778961181640625
Epoch 830, val loss: 1.271372675895691
Epoch 840, training loss: 0.07076211273670197 = 0.004779723938554525 + 0.01 * 6.598238468170166
Epoch 840, val loss: 1.2765346765518188
Epoch 850, training loss: 0.07031181454658508 = 0.004623087588697672 + 0.01 * 6.568872928619385
Epoch 850, val loss: 1.281655192375183
Epoch 860, training loss: 0.07015424221754074 = 0.00447499705478549 + 0.01 * 6.567924976348877
Epoch 860, val loss: 1.2865982055664062
Epoch 870, training loss: 0.07013443857431412 = 0.004334918223321438 + 0.01 * 6.579951763153076
Epoch 870, val loss: 1.2914915084838867
Epoch 880, training loss: 0.0699039101600647 = 0.004202382639050484 + 0.01 * 6.570152759552002
Epoch 880, val loss: 1.2961808443069458
Epoch 890, training loss: 0.06969955563545227 = 0.004077353980392218 + 0.01 * 6.562220096588135
Epoch 890, val loss: 1.30077064037323
Epoch 900, training loss: 0.0694991946220398 = 0.003959124907851219 + 0.01 * 6.554007053375244
Epoch 900, val loss: 1.305355191230774
Epoch 910, training loss: 0.06939542293548584 = 0.003846885869279504 + 0.01 * 6.554853916168213
Epoch 910, val loss: 1.3096445798873901
Epoch 920, training loss: 0.0693209320306778 = 0.0037400927394628525 + 0.01 * 6.558084011077881
Epoch 920, val loss: 1.314042329788208
Epoch 930, training loss: 0.0691249743103981 = 0.003638572059571743 + 0.01 * 6.548640251159668
Epoch 930, val loss: 1.3182525634765625
Epoch 940, training loss: 0.0690511167049408 = 0.003541951533406973 + 0.01 * 6.5509161949157715
Epoch 940, val loss: 1.322366714477539
Epoch 950, training loss: 0.06891768425703049 = 0.0034498614259064198 + 0.01 * 6.54678201675415
Epoch 950, val loss: 1.3264062404632568
Epoch 960, training loss: 0.06873589009046555 = 0.0033623757772147655 + 0.01 * 6.537351608276367
Epoch 960, val loss: 1.3303155899047852
Epoch 970, training loss: 0.06866610795259476 = 0.0032788424286991358 + 0.01 * 6.538726329803467
Epoch 970, val loss: 1.3341671228408813
Epoch 980, training loss: 0.06848892569541931 = 0.0031994206365197897 + 0.01 * 6.5289506912231445
Epoch 980, val loss: 1.3379020690917969
Epoch 990, training loss: 0.06850767135620117 = 0.003123690141364932 + 0.01 * 6.538397789001465
Epoch 990, val loss: 1.3415743112564087
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7481
Overall ASR: 0.4871
Flip ASR: 0.4311/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0223467350006104 = 1.9386088848114014 + 0.01 * 8.373777389526367
Epoch 0, val loss: 1.942113995552063
Epoch 10, training loss: 2.012328624725342 = 1.9285929203033447 + 0.01 * 8.37357234954834
Epoch 10, val loss: 1.9321365356445312
Epoch 20, training loss: 1.999936819076538 = 1.9162079095840454 + 0.01 * 8.37288761138916
Epoch 20, val loss: 1.9192270040512085
Epoch 30, training loss: 1.9828928709030151 = 1.8991824388504028 + 0.01 * 8.371048927307129
Epoch 30, val loss: 1.9008334875106812
Epoch 40, training loss: 1.958298683166504 = 1.8746532201766968 + 0.01 * 8.364544868469238
Epoch 40, val loss: 1.8740367889404297
Epoch 50, training loss: 1.9236173629760742 = 1.8403960466384888 + 0.01 * 8.322134971618652
Epoch 50, val loss: 1.837559700012207
Epoch 60, training loss: 1.8789805173873901 = 1.798397183418274 + 0.01 * 8.05833911895752
Epoch 60, val loss: 1.7956130504608154
Epoch 70, training loss: 1.8297557830810547 = 1.7541836500167847 + 0.01 * 7.557215690612793
Epoch 70, val loss: 1.7551072835922241
Epoch 80, training loss: 1.7754766941070557 = 1.7034496068954468 + 0.01 * 7.202709674835205
Epoch 80, val loss: 1.7109438180923462
Epoch 90, training loss: 1.706140398979187 = 1.6350594758987427 + 0.01 * 7.108087539672852
Epoch 90, val loss: 1.6535775661468506
Epoch 100, training loss: 1.6164864301681519 = 1.5457899570465088 + 0.01 * 7.069645881652832
Epoch 100, val loss: 1.5811679363250732
Epoch 110, training loss: 1.5100762844085693 = 1.4397882223129272 + 0.01 * 7.028801918029785
Epoch 110, val loss: 1.4989407062530518
Epoch 120, training loss: 1.3956812620162964 = 1.3257569074630737 + 0.01 * 6.992437839508057
Epoch 120, val loss: 1.4106333255767822
Epoch 130, training loss: 1.2795661687850952 = 1.2100704908370972 + 0.01 * 6.949563026428223
Epoch 130, val loss: 1.3234548568725586
Epoch 140, training loss: 1.167082667350769 = 1.0979784727096558 + 0.01 * 6.91042423248291
Epoch 140, val loss: 1.2394577264785767
Epoch 150, training loss: 1.0622646808624268 = 0.9934242963790894 + 0.01 * 6.884037971496582
Epoch 150, val loss: 1.1618989706039429
Epoch 160, training loss: 0.9667003154754639 = 0.898029625415802 + 0.01 * 6.867067813873291
Epoch 160, val loss: 1.092154860496521
Epoch 170, training loss: 0.8798479437828064 = 0.8113085031509399 + 0.01 * 6.853941917419434
Epoch 170, val loss: 1.0296502113342285
Epoch 180, training loss: 0.8009782433509827 = 0.7325321435928345 + 0.01 * 6.844607830047607
Epoch 180, val loss: 0.973579466342926
Epoch 190, training loss: 0.7297808527946472 = 0.6614088416099548 + 0.01 * 6.837202548980713
Epoch 190, val loss: 0.9250202178955078
Epoch 200, training loss: 0.6655174493789673 = 0.5972311496734619 + 0.01 * 6.828630447387695
Epoch 200, val loss: 0.8837424516677856
Epoch 210, training loss: 0.60707026720047 = 0.5388707518577576 + 0.01 * 6.819953441619873
Epoch 210, val loss: 0.8484901189804077
Epoch 220, training loss: 0.5533542037010193 = 0.4852586090564728 + 0.01 * 6.8095622062683105
Epoch 220, val loss: 0.8188723921775818
Epoch 230, training loss: 0.5037065148353577 = 0.43572598695755005 + 0.01 * 6.798053741455078
Epoch 230, val loss: 0.7945319414138794
Epoch 240, training loss: 0.4578442871570587 = 0.38988634943962097 + 0.01 * 6.795793056488037
Epoch 240, val loss: 0.7751004695892334
Epoch 250, training loss: 0.41538602113723755 = 0.34759634733200073 + 0.01 * 6.77896785736084
Epoch 250, val loss: 0.7598631978034973
Epoch 260, training loss: 0.3765943944454193 = 0.3089107275009155 + 0.01 * 6.768365859985352
Epoch 260, val loss: 0.7483643889427185
Epoch 270, training loss: 0.341366708278656 = 0.2737712264060974 + 0.01 * 6.759549140930176
Epoch 270, val loss: 0.7403237819671631
Epoch 280, training loss: 0.3095988631248474 = 0.2420947402715683 + 0.01 * 6.750411510467529
Epoch 280, val loss: 0.7355126142501831
Epoch 290, training loss: 0.2811637818813324 = 0.2136913537979126 + 0.01 * 6.747243404388428
Epoch 290, val loss: 0.7339833378791809
Epoch 300, training loss: 0.2557235360145569 = 0.1882973313331604 + 0.01 * 6.742622375488281
Epoch 300, val loss: 0.7357866764068604
Epoch 310, training loss: 0.23312143981456757 = 0.1657615602016449 + 0.01 * 6.735988140106201
Epoch 310, val loss: 0.7409409880638123
Epoch 320, training loss: 0.2132263332605362 = 0.1458960622549057 + 0.01 * 6.733027458190918
Epoch 320, val loss: 0.7493481636047363
Epoch 330, training loss: 0.1959231197834015 = 0.1285790503025055 + 0.01 * 6.734407901763916
Epoch 330, val loss: 0.7609217166900635
Epoch 340, training loss: 0.18085835874080658 = 0.11358968168497086 + 0.01 * 6.726868152618408
Epoch 340, val loss: 0.7748242020606995
Epoch 350, training loss: 0.16785533726215363 = 0.1006261333823204 + 0.01 * 6.722920894622803
Epoch 350, val loss: 0.7906209230422974
Epoch 360, training loss: 0.15659227967262268 = 0.08939962089061737 + 0.01 * 6.719265937805176
Epoch 360, val loss: 0.8076117634773254
Epoch 370, training loss: 0.1468781977891922 = 0.07966574281454086 + 0.01 * 6.721245288848877
Epoch 370, val loss: 0.8253647685050964
Epoch 380, training loss: 0.13836172223091125 = 0.07121172547340393 + 0.01 * 6.714998722076416
Epoch 380, val loss: 0.8436141610145569
Epoch 390, training loss: 0.13092508912086487 = 0.06383580714464188 + 0.01 * 6.708928108215332
Epoch 390, val loss: 0.8623367547988892
Epoch 400, training loss: 0.12449754774570465 = 0.05736653506755829 + 0.01 * 6.713101387023926
Epoch 400, val loss: 0.8812052011489868
Epoch 410, training loss: 0.11872829496860504 = 0.051686841994524 + 0.01 * 6.7041449546813965
Epoch 410, val loss: 0.9001355171203613
Epoch 420, training loss: 0.11365631222724915 = 0.0466909259557724 + 0.01 * 6.69653844833374
Epoch 420, val loss: 0.9189891815185547
Epoch 430, training loss: 0.10926951467990875 = 0.042285505682229996 + 0.01 * 6.698400974273682
Epoch 430, val loss: 0.9377352595329285
Epoch 440, training loss: 0.10529956221580505 = 0.038399942219257355 + 0.01 * 6.689961910247803
Epoch 440, val loss: 0.9561326503753662
Epoch 450, training loss: 0.10187199711799622 = 0.03496750444173813 + 0.01 * 6.6904497146606445
Epoch 450, val loss: 0.9742658734321594
Epoch 460, training loss: 0.09879590570926666 = 0.03193190321326256 + 0.01 * 6.686400413513184
Epoch 460, val loss: 0.9919942021369934
Epoch 470, training loss: 0.09596149623394012 = 0.029241932556033134 + 0.01 * 6.6719560623168945
Epoch 470, val loss: 1.0092624425888062
Epoch 480, training loss: 0.09354424476623535 = 0.026852326467633247 + 0.01 * 6.669192314147949
Epoch 480, val loss: 1.0261059999465942
Epoch 490, training loss: 0.09148599952459335 = 0.0247300174087286 + 0.01 * 6.67559814453125
Epoch 490, val loss: 1.0423725843429565
Epoch 500, training loss: 0.089453786611557 = 0.022842364385724068 + 0.01 * 6.661142826080322
Epoch 500, val loss: 1.058122992515564
Epoch 510, training loss: 0.08775400370359421 = 0.02115599997341633 + 0.01 * 6.659801006317139
Epoch 510, val loss: 1.0734264850616455
Epoch 520, training loss: 0.08619646728038788 = 0.01964583434164524 + 0.01 * 6.655063629150391
Epoch 520, val loss: 1.0881829261779785
Epoch 530, training loss: 0.08475758135318756 = 0.0182899571955204 + 0.01 * 6.646762847900391
Epoch 530, val loss: 1.1025179624557495
Epoch 540, training loss: 0.08343058824539185 = 0.0170682854950428 + 0.01 * 6.63623046875
Epoch 540, val loss: 1.1164166927337646
Epoch 550, training loss: 0.08243771642446518 = 0.015965238213539124 + 0.01 * 6.647248268127441
Epoch 550, val loss: 1.1298577785491943
Epoch 560, training loss: 0.08123400807380676 = 0.014967355877161026 + 0.01 * 6.626666069030762
Epoch 560, val loss: 1.1428265571594238
Epoch 570, training loss: 0.08028153330087662 = 0.014060449786484241 + 0.01 * 6.622108459472656
Epoch 570, val loss: 1.1553926467895508
Epoch 580, training loss: 0.07964657247066498 = 0.013232866302132607 + 0.01 * 6.641371250152588
Epoch 580, val loss: 1.1675254106521606
Epoch 590, training loss: 0.07868117094039917 = 0.012477894313633442 + 0.01 * 6.620327472686768
Epoch 590, val loss: 1.179256558418274
Epoch 600, training loss: 0.07797260582447052 = 0.011787439696490765 + 0.01 * 6.61851692199707
Epoch 600, val loss: 1.1906245946884155
Epoch 610, training loss: 0.07736357301473618 = 0.011155045591294765 + 0.01 * 6.620852947235107
Epoch 610, val loss: 1.2015894651412964
Epoch 620, training loss: 0.0766361877322197 = 0.010574979707598686 + 0.01 * 6.606121063232422
Epoch 620, val loss: 1.2123048305511475
Epoch 630, training loss: 0.07596892863512039 = 0.010041379369795322 + 0.01 * 6.59275484085083
Epoch 630, val loss: 1.2226170301437378
Epoch 640, training loss: 0.07557255029678345 = 0.009548827074468136 + 0.01 * 6.602372646331787
Epoch 640, val loss: 1.2326923608779907
Epoch 650, training loss: 0.07512583583593369 = 0.009093712083995342 + 0.01 * 6.603212833404541
Epoch 650, val loss: 1.242486834526062
Epoch 660, training loss: 0.07456985116004944 = 0.008672535419464111 + 0.01 * 6.589731693267822
Epoch 660, val loss: 1.2519022226333618
Epoch 670, training loss: 0.0741332620382309 = 0.008281506597995758 + 0.01 * 6.585175514221191
Epoch 670, val loss: 1.2611334323883057
Epoch 680, training loss: 0.07376769930124283 = 0.00791766308248043 + 0.01 * 6.58500337600708
Epoch 680, val loss: 1.2700541019439697
Epoch 690, training loss: 0.07336512953042984 = 0.007579493336379528 + 0.01 * 6.578563690185547
Epoch 690, val loss: 1.278789758682251
Epoch 700, training loss: 0.07300391793251038 = 0.007264598738402128 + 0.01 * 6.573931694030762
Epoch 700, val loss: 1.2872614860534668
Epoch 710, training loss: 0.07276827096939087 = 0.006970682647079229 + 0.01 * 6.579759120941162
Epoch 710, val loss: 1.2955143451690674
Epoch 720, training loss: 0.0723159909248352 = 0.0066960472613573074 + 0.01 * 6.561995029449463
Epoch 720, val loss: 1.3035287857055664
Epoch 730, training loss: 0.07208189368247986 = 0.0064389766193926334 + 0.01 * 6.564291477203369
Epoch 730, val loss: 1.3112845420837402
Epoch 740, training loss: 0.07175428420305252 = 0.006197788752615452 + 0.01 * 6.555649757385254
Epoch 740, val loss: 1.3189237117767334
Epoch 750, training loss: 0.07159064710140228 = 0.005971423350274563 + 0.01 * 6.561922073364258
Epoch 750, val loss: 1.3262989521026611
Epoch 760, training loss: 0.07128159701824188 = 0.005758439190685749 + 0.01 * 6.552315711975098
Epoch 760, val loss: 1.3335249423980713
Epoch 770, training loss: 0.0710071548819542 = 0.005558057222515345 + 0.01 * 6.544909954071045
Epoch 770, val loss: 1.340574026107788
Epoch 780, training loss: 0.07080687582492828 = 0.005369417369365692 + 0.01 * 6.543745994567871
Epoch 780, val loss: 1.3474106788635254
Epoch 790, training loss: 0.070757195353508 = 0.005191500764340162 + 0.01 * 6.556570053100586
Epoch 790, val loss: 1.3540668487548828
Epoch 800, training loss: 0.07047096639871597 = 0.005023501813411713 + 0.01 * 6.544746398925781
Epoch 800, val loss: 1.3606160879135132
Epoch 810, training loss: 0.07017520070075989 = 0.004864725284278393 + 0.01 * 6.531047821044922
Epoch 810, val loss: 1.3669514656066895
Epoch 820, training loss: 0.06992679089307785 = 0.004714477341622114 + 0.01 * 6.521231651306152
Epoch 820, val loss: 1.3731874227523804
Epoch 830, training loss: 0.06977374106645584 = 0.00457207253202796 + 0.01 * 6.520167350769043
Epoch 830, val loss: 1.3792943954467773
Epoch 840, training loss: 0.0696490928530693 = 0.00443697115406394 + 0.01 * 6.521212577819824
Epoch 840, val loss: 1.3852156400680542
Epoch 850, training loss: 0.06961312144994736 = 0.004308872390538454 + 0.01 * 6.530425071716309
Epoch 850, val loss: 1.3910359144210815
Epoch 860, training loss: 0.06926186382770538 = 0.004187033046036959 + 0.01 * 6.50748348236084
Epoch 860, val loss: 1.3967353105545044
Epoch 870, training loss: 0.0693083181977272 = 0.004071207717061043 + 0.01 * 6.523711681365967
Epoch 870, val loss: 1.4022735357284546
Epoch 880, training loss: 0.06927371770143509 = 0.00396112073212862 + 0.01 * 6.531259536743164
Epoch 880, val loss: 1.4076865911483765
Epoch 890, training loss: 0.0690082460641861 = 0.003856395138427615 + 0.01 * 6.5151848793029785
Epoch 890, val loss: 1.412961483001709
Epoch 900, training loss: 0.06874262541532516 = 0.003756545018404722 + 0.01 * 6.498608112335205
Epoch 900, val loss: 1.4181642532348633
Epoch 910, training loss: 0.06864558160305023 = 0.0036612499970942736 + 0.01 * 6.4984331130981445
Epoch 910, val loss: 1.4232481718063354
Epoch 920, training loss: 0.06844596564769745 = 0.0035702886525541544 + 0.01 * 6.487567901611328
Epoch 920, val loss: 1.4282387495040894
Epoch 930, training loss: 0.06862874329090118 = 0.003483254462480545 + 0.01 * 6.514548301696777
Epoch 930, val loss: 1.4331271648406982
Epoch 940, training loss: 0.0682748556137085 = 0.0034002414904534817 + 0.01 * 6.487461566925049
Epoch 940, val loss: 1.4378820657730103
Epoch 950, training loss: 0.06817343831062317 = 0.003320801304653287 + 0.01 * 6.485264301300049
Epoch 950, val loss: 1.442564845085144
Epoch 960, training loss: 0.0681200698018074 = 0.0032447767443954945 + 0.01 * 6.4875288009643555
Epoch 960, val loss: 1.4471158981323242
Epoch 970, training loss: 0.06802103668451309 = 0.0031720325350761414 + 0.01 * 6.48490047454834
Epoch 970, val loss: 1.4516377449035645
Epoch 980, training loss: 0.06790050119161606 = 0.0031022278126329184 + 0.01 * 6.479827404022217
Epoch 980, val loss: 1.4560863971710205
Epoch 990, training loss: 0.06783094257116318 = 0.003035222413018346 + 0.01 * 6.47957181930542
Epoch 990, val loss: 1.4604265689849854
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.7528
Flip ASR: 0.7156/225 nodes
The final ASR:0.66421, 0.12524, Accuracy:0.79136, 0.03089
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11616])
remove edge: torch.Size([2, 9506])
updated graph: torch.Size([2, 10566])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 1.0000
Flip ASR: 1.0000/225 nodes
The final ASR:0.98524, 0.01086, Accuracy:0.83333, 0.00800
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.5 der2:0.2 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.046980381011963 = 1.9632418155670166 + 0.01 * 8.373849868774414
Epoch 0, val loss: 1.9667794704437256
Epoch 10, training loss: 2.036163330078125 = 1.9524257183074951 + 0.01 * 8.373756408691406
Epoch 10, val loss: 1.9568300247192383
Epoch 20, training loss: 2.02309513092041 = 1.9393601417541504 + 0.01 * 8.373494148254395
Epoch 20, val loss: 1.944477915763855
Epoch 30, training loss: 2.005175828933716 = 1.92144775390625 + 0.01 * 8.372817039489746
Epoch 30, val loss: 1.927323818206787
Epoch 40, training loss: 1.9788522720336914 = 1.8951538801193237 + 0.01 * 8.369843482971191
Epoch 40, val loss: 1.9022512435913086
Epoch 50, training loss: 1.9412932395935059 = 1.857787847518921 + 0.01 * 8.35053825378418
Epoch 50, val loss: 1.867952823638916
Epoch 60, training loss: 1.8956129550933838 = 1.8129485845565796 + 0.01 * 8.266433715820312
Epoch 60, val loss: 1.8299787044525146
Epoch 70, training loss: 1.8516278266906738 = 1.7718867063522339 + 0.01 * 7.974106311798096
Epoch 70, val loss: 1.7965850830078125
Epoch 80, training loss: 1.8022202253341675 = 1.7272833585739136 + 0.01 * 7.493690013885498
Epoch 80, val loss: 1.7543693780899048
Epoch 90, training loss: 1.7375179529190063 = 1.6650309562683105 + 0.01 * 7.2486958503723145
Epoch 90, val loss: 1.6971282958984375
Epoch 100, training loss: 1.6515618562698364 = 1.5809727907180786 + 0.01 * 7.0589118003845215
Epoch 100, val loss: 1.6253712177276611
Epoch 110, training loss: 1.550023078918457 = 1.4801548719406128 + 0.01 * 6.986819267272949
Epoch 110, val loss: 1.5413916110992432
Epoch 120, training loss: 1.4462525844573975 = 1.3766686916351318 + 0.01 * 6.958389759063721
Epoch 120, val loss: 1.4569225311279297
Epoch 130, training loss: 1.349821925163269 = 1.2804536819458008 + 0.01 * 6.936824798583984
Epoch 130, val loss: 1.3819500207901
Epoch 140, training loss: 1.2620177268981934 = 1.1928414106369019 + 0.01 * 6.917632102966309
Epoch 140, val loss: 1.3170968294143677
Epoch 150, training loss: 1.1793502569198608 = 1.1103286743164062 + 0.01 * 6.902163505554199
Epoch 150, val loss: 1.257926106452942
Epoch 160, training loss: 1.0972003936767578 = 1.0283150672912598 + 0.01 * 6.8885369300842285
Epoch 160, val loss: 1.2001266479492188
Epoch 170, training loss: 1.0132567882537842 = 0.944482684135437 + 0.01 * 6.877406597137451
Epoch 170, val loss: 1.1407653093338013
Epoch 180, training loss: 0.9287055134773254 = 0.8600273132324219 + 0.01 * 6.8678202629089355
Epoch 180, val loss: 1.0807867050170898
Epoch 190, training loss: 0.8466931581497192 = 0.7781093120574951 + 0.01 * 6.858382701873779
Epoch 190, val loss: 1.0227408409118652
Epoch 200, training loss: 0.7699651718139648 = 0.7014377117156982 + 0.01 * 6.852745056152344
Epoch 200, val loss: 0.9697081446647644
Epoch 210, training loss: 0.6998462080955505 = 0.6314213275909424 + 0.01 * 6.842489242553711
Epoch 210, val loss: 0.9232988953590393
Epoch 220, training loss: 0.6365521550178528 = 0.5681650638580322 + 0.01 * 6.838710784912109
Epoch 220, val loss: 0.883733868598938
Epoch 230, training loss: 0.5792917609214783 = 0.5110295414924622 + 0.01 * 6.826222896575928
Epoch 230, val loss: 0.8506436944007874
Epoch 240, training loss: 0.5271701812744141 = 0.45903998613357544 + 0.01 * 6.813022613525391
Epoch 240, val loss: 0.8234419822692871
Epoch 250, training loss: 0.47940754890441895 = 0.4113578796386719 + 0.01 * 6.804965972900391
Epoch 250, val loss: 0.801730751991272
Epoch 260, training loss: 0.43573427200317383 = 0.36757615208625793 + 0.01 * 6.8158111572265625
Epoch 260, val loss: 0.7854873538017273
Epoch 270, training loss: 0.3954293429851532 = 0.32750993967056274 + 0.01 * 6.791939735412598
Epoch 270, val loss: 0.7742868065834045
Epoch 280, training loss: 0.35894978046417236 = 0.2911033034324646 + 0.01 * 6.7846479415893555
Epoch 280, val loss: 0.7675334215164185
Epoch 290, training loss: 0.32610008120536804 = 0.2583453357219696 + 0.01 * 6.7754740715026855
Epoch 290, val loss: 0.7649231553077698
Epoch 300, training loss: 0.2968340218067169 = 0.22913427650928497 + 0.01 * 6.769975185394287
Epoch 300, val loss: 0.7662050724029541
Epoch 310, training loss: 0.27111420035362244 = 0.20329228043556213 + 0.01 * 6.782191276550293
Epoch 310, val loss: 0.7708972692489624
Epoch 320, training loss: 0.24821454286575317 = 0.18056654930114746 + 0.01 * 6.76479959487915
Epoch 320, val loss: 0.7786775231361389
Epoch 330, training loss: 0.228249192237854 = 0.1606292724609375 + 0.01 * 6.761991500854492
Epoch 330, val loss: 0.7891125082969666
Epoch 340, training loss: 0.21067620813846588 = 0.14315809309482574 + 0.01 * 6.751811981201172
Epoch 340, val loss: 0.8018284440040588
Epoch 350, training loss: 0.195326030254364 = 0.12783268094062805 + 0.01 * 6.749334812164307
Epoch 350, val loss: 0.8164485096931458
Epoch 360, training loss: 0.18204635381698608 = 0.11439365893602371 + 0.01 * 6.765268802642822
Epoch 360, val loss: 0.8325200080871582
Epoch 370, training loss: 0.1700882613658905 = 0.10259512066841125 + 0.01 * 6.749314308166504
Epoch 370, val loss: 0.8497819900512695
Epoch 380, training loss: 0.15956641733646393 = 0.0921940952539444 + 0.01 * 6.737232685089111
Epoch 380, val loss: 0.8680728673934937
Epoch 390, training loss: 0.15032540261745453 = 0.08299723267555237 + 0.01 * 6.73281717300415
Epoch 390, val loss: 0.8872565627098083
Epoch 400, training loss: 0.14221356809139252 = 0.07484874874353409 + 0.01 * 6.7364821434021
Epoch 400, val loss: 0.9071802496910095
Epoch 410, training loss: 0.13505803048610687 = 0.0676342025399208 + 0.01 * 6.742383003234863
Epoch 410, val loss: 0.9276288151741028
Epoch 420, training loss: 0.12846173346042633 = 0.061226099729537964 + 0.01 * 6.723563194274902
Epoch 420, val loss: 0.9485584497451782
Epoch 430, training loss: 0.12273785471916199 = 0.05551418662071228 + 0.01 * 6.722366809844971
Epoch 430, val loss: 0.9699097871780396
Epoch 440, training loss: 0.11758588999509811 = 0.05040910094976425 + 0.01 * 6.717679023742676
Epoch 440, val loss: 0.9915977120399475
Epoch 450, training loss: 0.11302423477172852 = 0.04584256559610367 + 0.01 * 6.718166828155518
Epoch 450, val loss: 1.013442873954773
Epoch 460, training loss: 0.10904812812805176 = 0.041759077459573746 + 0.01 * 6.72890567779541
Epoch 460, val loss: 1.0353577136993408
Epoch 470, training loss: 0.10526113957166672 = 0.0381106436252594 + 0.01 * 6.715049743652344
Epoch 470, val loss: 1.057170033454895
Epoch 480, training loss: 0.10187822580337524 = 0.034850142896175385 + 0.01 * 6.702808380126953
Epoch 480, val loss: 1.078729271888733
Epoch 490, training loss: 0.09894995391368866 = 0.03193802759051323 + 0.01 * 6.701192378997803
Epoch 490, val loss: 1.0999222993850708
Epoch 500, training loss: 0.0964101105928421 = 0.029340380802750587 + 0.01 * 6.706973552703857
Epoch 500, val loss: 1.1206166744232178
Epoch 510, training loss: 0.09395470470190048 = 0.027024345472455025 + 0.01 * 6.6930365562438965
Epoch 510, val loss: 1.1407580375671387
Epoch 520, training loss: 0.09183097630739212 = 0.024957075715065002 + 0.01 * 6.687390327453613
Epoch 520, val loss: 1.1602528095245361
Epoch 530, training loss: 0.09000317752361298 = 0.02310885116457939 + 0.01 * 6.6894330978393555
Epoch 530, val loss: 1.1790634393692017
Epoch 540, training loss: 0.08827687799930573 = 0.021453242748975754 + 0.01 * 6.682363986968994
Epoch 540, val loss: 1.1972044706344604
Epoch 550, training loss: 0.08693428337574005 = 0.01996564120054245 + 0.01 * 6.696864604949951
Epoch 550, val loss: 1.2147127389907837
Epoch 560, training loss: 0.08546613901853561 = 0.018627464771270752 + 0.01 * 6.683867931365967
Epoch 560, val loss: 1.2315560579299927
Epoch 570, training loss: 0.08412138372659683 = 0.017417380586266518 + 0.01 * 6.670400619506836
Epoch 570, val loss: 1.2478206157684326
Epoch 580, training loss: 0.08315254747867584 = 0.01631990447640419 + 0.01 * 6.68326473236084
Epoch 580, val loss: 1.2635695934295654
Epoch 590, training loss: 0.0819564089179039 = 0.015324821695685387 + 0.01 * 6.663158416748047
Epoch 590, val loss: 1.2787647247314453
Epoch 600, training loss: 0.08107942342758179 = 0.014417641796171665 + 0.01 * 6.6661787033081055
Epoch 600, val loss: 1.2934733629226685
Epoch 610, training loss: 0.08022668957710266 = 0.013588554225862026 + 0.01 * 6.663813591003418
Epoch 610, val loss: 1.3077512979507446
Epoch 620, training loss: 0.07926668971776962 = 0.0128302788361907 + 0.01 * 6.643641471862793
Epoch 620, val loss: 1.3216060400009155
Epoch 630, training loss: 0.07862837612628937 = 0.012134131044149399 + 0.01 * 6.649425029754639
Epoch 630, val loss: 1.3350470066070557
Epoch 640, training loss: 0.07785673439502716 = 0.011493977159261703 + 0.01 * 6.6362762451171875
Epoch 640, val loss: 1.3481305837631226
Epoch 650, training loss: 0.0772973820567131 = 0.010903597809374332 + 0.01 * 6.639379024505615
Epoch 650, val loss: 1.3608492612838745
Epoch 660, training loss: 0.07670234143733978 = 0.010358544066548347 + 0.01 * 6.634379863739014
Epoch 660, val loss: 1.3732733726501465
Epoch 670, training loss: 0.07628374546766281 = 0.009854108095169067 + 0.01 * 6.642963886260986
Epoch 670, val loss: 1.3853427171707153
Epoch 680, training loss: 0.07568280398845673 = 0.009387391619384289 + 0.01 * 6.629541397094727
Epoch 680, val loss: 1.3971612453460693
Epoch 690, training loss: 0.07520847022533417 = 0.008954096585512161 + 0.01 * 6.625437259674072
Epoch 690, val loss: 1.408637523651123
Epoch 700, training loss: 0.07471233606338501 = 0.008551250211894512 + 0.01 * 6.616109371185303
Epoch 700, val loss: 1.4198664426803589
Epoch 710, training loss: 0.07427885383367538 = 0.008176572620868683 + 0.01 * 6.610228538513184
Epoch 710, val loss: 1.4307889938354492
Epoch 720, training loss: 0.07403726875782013 = 0.007827787660062313 + 0.01 * 6.620948314666748
Epoch 720, val loss: 1.4415104389190674
Epoch 730, training loss: 0.07360632717609406 = 0.007501943502575159 + 0.01 * 6.610438346862793
Epoch 730, val loss: 1.4519438743591309
Epoch 740, training loss: 0.07342460751533508 = 0.0071973432786762714 + 0.01 * 6.6227264404296875
Epoch 740, val loss: 1.462152123451233
Epoch 750, training loss: 0.07295522093772888 = 0.0069124409928917885 + 0.01 * 6.604278087615967
Epoch 750, val loss: 1.4721623659133911
Epoch 760, training loss: 0.07271318882703781 = 0.006645542569458485 + 0.01 * 6.606764316558838
Epoch 760, val loss: 1.4818717241287231
Epoch 770, training loss: 0.07239530980587006 = 0.006394883617758751 + 0.01 * 6.600042343139648
Epoch 770, val loss: 1.4914542436599731
Epoch 780, training loss: 0.07203210145235062 = 0.0061593493446707726 + 0.01 * 6.58727502822876
Epoch 780, val loss: 1.5008167028427124
Epoch 790, training loss: 0.0721120685338974 = 0.005937385372817516 + 0.01 * 6.61746883392334
Epoch 790, val loss: 1.5100338459014893
Epoch 800, training loss: 0.07168696075677872 = 0.0057289074175059795 + 0.01 * 6.595805644989014
Epoch 800, val loss: 1.5190119743347168
Epoch 810, training loss: 0.07135001569986343 = 0.005532326642423868 + 0.01 * 6.581768989562988
Epoch 810, val loss: 1.5278006792068481
Epoch 820, training loss: 0.07109450548887253 = 0.005347142461687326 + 0.01 * 6.574736595153809
Epoch 820, val loss: 1.5364034175872803
Epoch 830, training loss: 0.07081892341375351 = 0.005172049626708031 + 0.01 * 6.564687252044678
Epoch 830, val loss: 1.5448448657989502
Epoch 840, training loss: 0.07104672491550446 = 0.0050064567476511 + 0.01 * 6.604027271270752
Epoch 840, val loss: 1.553115725517273
Epoch 850, training loss: 0.07058218121528625 = 0.004849911667406559 + 0.01 * 6.573227405548096
Epoch 850, val loss: 1.561250925064087
Epoch 860, training loss: 0.07017736881971359 = 0.004701648373156786 + 0.01 * 6.547571659088135
Epoch 860, val loss: 1.5691033601760864
Epoch 870, training loss: 0.07015415281057358 = 0.0045610698871314526 + 0.01 * 6.5593085289001465
Epoch 870, val loss: 1.576849102973938
Epoch 880, training loss: 0.07007080316543579 = 0.004427578765898943 + 0.01 * 6.564322471618652
Epoch 880, val loss: 1.5844799280166626
Epoch 890, training loss: 0.06971510499715805 = 0.004300842527300119 + 0.01 * 6.541426658630371
Epoch 890, val loss: 1.5919528007507324
Epoch 900, training loss: 0.06960252672433853 = 0.0041802809573709965 + 0.01 * 6.542224884033203
Epoch 900, val loss: 1.599255084991455
Epoch 910, training loss: 0.06936725974082947 = 0.004065440967679024 + 0.01 * 6.530181884765625
Epoch 910, val loss: 1.6064528226852417
Epoch 920, training loss: 0.0693202018737793 = 0.003956174943596125 + 0.01 * 6.536402702331543
Epoch 920, val loss: 1.6134783029556274
Epoch 930, training loss: 0.06912460178136826 = 0.003852108959108591 + 0.01 * 6.527249813079834
Epoch 930, val loss: 1.6203922033309937
Epoch 940, training loss: 0.0691160261631012 = 0.003752813208848238 + 0.01 * 6.536321640014648
Epoch 940, val loss: 1.6271872520446777
Epoch 950, training loss: 0.06880277395248413 = 0.003658279776573181 + 0.01 * 6.514449596405029
Epoch 950, val loss: 1.6337480545043945
Epoch 960, training loss: 0.06870832294225693 = 0.003567910986021161 + 0.01 * 6.514040946960449
Epoch 960, val loss: 1.6403743028640747
Epoch 970, training loss: 0.06854117661714554 = 0.0034814237151294947 + 0.01 * 6.505975246429443
Epoch 970, val loss: 1.6467585563659668
Epoch 980, training loss: 0.06895028054714203 = 0.003398905973881483 + 0.01 * 6.555138111114502
Epoch 980, val loss: 1.6530506610870361
Epoch 990, training loss: 0.06856831908226013 = 0.003320053219795227 + 0.01 * 6.524827003479004
Epoch 990, val loss: 1.6592758893966675
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.5941
Flip ASR: 0.5289/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.03283953666687 = 1.9491009712219238 + 0.01 * 8.373849868774414
Epoch 0, val loss: 1.9474238157272339
Epoch 10, training loss: 2.0227761268615723 = 1.939038634300232 + 0.01 * 8.373748779296875
Epoch 10, val loss: 1.937525749206543
Epoch 20, training loss: 2.0102407932281494 = 1.9265060424804688 + 0.01 * 8.373475074768066
Epoch 20, val loss: 1.9248735904693604
Epoch 30, training loss: 1.9925082921981812 = 1.9087803363800049 + 0.01 * 8.372797012329102
Epoch 30, val loss: 1.906718134880066
Epoch 40, training loss: 1.9660625457763672 = 1.8823604583740234 + 0.01 * 8.370206832885742
Epoch 40, val loss: 1.8797622919082642
Epoch 50, training loss: 1.9283556938171387 = 1.844804286956787 + 0.01 * 8.355140686035156
Epoch 50, val loss: 1.842532992362976
Epoch 60, training loss: 1.8823249340057373 = 1.7996309995651245 + 0.01 * 8.269388198852539
Epoch 60, val loss: 1.8004969358444214
Epoch 70, training loss: 1.8337095975875854 = 1.7552844285964966 + 0.01 * 7.8425140380859375
Epoch 70, val loss: 1.7613691091537476
Epoch 80, training loss: 1.7762378454208374 = 1.7033991813659668 + 0.01 * 7.2838664054870605
Epoch 80, val loss: 1.7145746946334839
Epoch 90, training loss: 1.7038623094558716 = 1.6327342987060547 + 0.01 * 7.1128034591674805
Epoch 90, val loss: 1.6526519060134888
Epoch 100, training loss: 1.6105339527130127 = 1.539801001548767 + 0.01 * 7.073300838470459
Epoch 100, val loss: 1.5730977058410645
Epoch 110, training loss: 1.5010422468185425 = 1.4307156801223755 + 0.01 * 7.032660007476807
Epoch 110, val loss: 1.483402967453003
Epoch 120, training loss: 1.3887338638305664 = 1.3186227083206177 + 0.01 * 7.01112174987793
Epoch 120, val loss: 1.3948004245758057
Epoch 130, training loss: 1.2797342538833618 = 1.2097561359405518 + 0.01 * 6.997809886932373
Epoch 130, val loss: 1.3114373683929443
Epoch 140, training loss: 1.1754580736160278 = 1.105606198310852 + 0.01 * 6.985187530517578
Epoch 140, val loss: 1.2333214282989502
Epoch 150, training loss: 1.0776407718658447 = 1.007915735244751 + 0.01 * 6.972498416900635
Epoch 150, val loss: 1.1604877710342407
Epoch 160, training loss: 0.9879629611968994 = 0.918375551700592 + 0.01 * 6.958743572235107
Epoch 160, val loss: 1.0944243669509888
Epoch 170, training loss: 0.9072140455245972 = 0.8377659320831299 + 0.01 * 6.944811820983887
Epoch 170, val loss: 1.0357811450958252
Epoch 180, training loss: 0.8356270790100098 = 0.766303539276123 + 0.01 * 6.9323554039001465
Epoch 180, val loss: 0.984822154045105
Epoch 190, training loss: 0.7729992866516113 = 0.7037755250930786 + 0.01 * 6.922379493713379
Epoch 190, val loss: 0.9421381950378418
Epoch 200, training loss: 0.7181434631347656 = 0.6489819884300232 + 0.01 * 6.916149616241455
Epoch 200, val loss: 0.9073883891105652
Epoch 210, training loss: 0.6689419150352478 = 0.5998305082321167 + 0.01 * 6.911139965057373
Epoch 210, val loss: 0.878971517086029
Epoch 220, training loss: 0.623344361782074 = 0.5542640686035156 + 0.01 * 6.908029079437256
Epoch 220, val loss: 0.8550962209701538
Epoch 230, training loss: 0.5798182487487793 = 0.5107626914978027 + 0.01 * 6.9055562019348145
Epoch 230, val loss: 0.833993136882782
Epoch 240, training loss: 0.5375603437423706 = 0.4685256779193878 + 0.01 * 6.9034647941589355
Epoch 240, val loss: 0.814998984336853
Epoch 250, training loss: 0.49632343649864197 = 0.4273073673248291 + 0.01 * 6.901606559753418
Epoch 250, val loss: 0.7980578541755676
Epoch 260, training loss: 0.45618924498558044 = 0.3871884047985077 + 0.01 * 6.900083065032959
Epoch 260, val loss: 0.7828200459480286
Epoch 270, training loss: 0.4174444377422333 = 0.3484579026699066 + 0.01 * 6.898653507232666
Epoch 270, val loss: 0.769006609916687
Epoch 280, training loss: 0.38053518533706665 = 0.31156331300735474 + 0.01 * 6.897189140319824
Epoch 280, val loss: 0.7568439841270447
Epoch 290, training loss: 0.3459317684173584 = 0.27697721123695374 + 0.01 * 6.895456790924072
Epoch 290, val loss: 0.7467997670173645
Epoch 300, training loss: 0.3140655755996704 = 0.245134174823761 + 0.01 * 6.893141269683838
Epoch 300, val loss: 0.7393233776092529
Epoch 310, training loss: 0.28523707389831543 = 0.21631553769111633 + 0.01 * 6.892153263092041
Epoch 310, val loss: 0.7347963452339172
Epoch 320, training loss: 0.25947684049606323 = 0.1905910223722458 + 0.01 * 6.8885817527771
Epoch 320, val loss: 0.7334124445915222
Epoch 330, training loss: 0.23680084943771362 = 0.16793760657310486 + 0.01 * 6.886324882507324
Epoch 330, val loss: 0.734768807888031
Epoch 340, training loss: 0.2170480489730835 = 0.14821332693099976 + 0.01 * 6.883472919464111
Epoch 340, val loss: 0.7384989261627197
Epoch 350, training loss: 0.1998593509197235 = 0.13104529678821564 + 0.01 * 6.881406307220459
Epoch 350, val loss: 0.744300365447998
Epoch 360, training loss: 0.18494346737861633 = 0.11618680506944656 + 0.01 * 6.87566614151001
Epoch 360, val loss: 0.7517149448394775
Epoch 370, training loss: 0.17196841537952423 = 0.10325604677200317 + 0.01 * 6.871237277984619
Epoch 370, val loss: 0.7601807117462158
Epoch 380, training loss: 0.16061526536941528 = 0.09196171164512634 + 0.01 * 6.865355014801025
Epoch 380, val loss: 0.7695075273513794
Epoch 390, training loss: 0.15065854787826538 = 0.08206823468208313 + 0.01 * 6.859032154083252
Epoch 390, val loss: 0.7794443964958191
Epoch 400, training loss: 0.1418481022119522 = 0.07329495251178741 + 0.01 * 6.855315208435059
Epoch 400, val loss: 0.7898085713386536
Epoch 410, training loss: 0.13397178053855896 = 0.06548259407281876 + 0.01 * 6.848918914794922
Epoch 410, val loss: 0.8005721569061279
Epoch 420, training loss: 0.1269277036190033 = 0.05848187953233719 + 0.01 * 6.844582557678223
Epoch 420, val loss: 0.8114542961120605
Epoch 430, training loss: 0.1206320971250534 = 0.05223019793629646 + 0.01 * 6.840189456939697
Epoch 430, val loss: 0.8224778771400452
Epoch 440, training loss: 0.11505312472581863 = 0.046693138778209686 + 0.01 * 6.835999011993408
Epoch 440, val loss: 0.8338910341262817
Epoch 450, training loss: 0.11014465242624283 = 0.04182840883731842 + 0.01 * 6.831624507904053
Epoch 450, val loss: 0.8456005454063416
Epoch 460, training loss: 0.10589180141687393 = 0.03759129345417023 + 0.01 * 6.830050945281982
Epoch 460, val loss: 0.857620894908905
Epoch 470, training loss: 0.10215970873832703 = 0.03391144052147865 + 0.01 * 6.824827671051025
Epoch 470, val loss: 0.8698473572731018
Epoch 480, training loss: 0.09899638593196869 = 0.03069419227540493 + 0.01 * 6.830219268798828
Epoch 480, val loss: 0.8820805549621582
Epoch 490, training loss: 0.09608384966850281 = 0.02789084054529667 + 0.01 * 6.819301128387451
Epoch 490, val loss: 0.8942769169807434
Epoch 500, training loss: 0.09360042214393616 = 0.025446053594350815 + 0.01 * 6.815436363220215
Epoch 500, val loss: 0.9064490795135498
Epoch 510, training loss: 0.09138933569192886 = 0.02330528385937214 + 0.01 * 6.808405876159668
Epoch 510, val loss: 0.9185398817062378
Epoch 520, training loss: 0.08943646401166916 = 0.021417668089270592 + 0.01 * 6.8018798828125
Epoch 520, val loss: 0.9304367899894714
Epoch 530, training loss: 0.08771602809429169 = 0.019743792712688446 + 0.01 * 6.797224044799805
Epoch 530, val loss: 0.9420896172523499
Epoch 540, training loss: 0.08622084558010101 = 0.018258366733789444 + 0.01 * 6.796248435974121
Epoch 540, val loss: 0.9534648656845093
Epoch 550, training loss: 0.08491280674934387 = 0.016935203224420547 + 0.01 * 6.797760009765625
Epoch 550, val loss: 0.9646831154823303
Epoch 560, training loss: 0.083616241812706 = 0.015751516446471214 + 0.01 * 6.786472320556641
Epoch 560, val loss: 0.9755309820175171
Epoch 570, training loss: 0.08260857313871384 = 0.014688176102936268 + 0.01 * 6.79203987121582
Epoch 570, val loss: 0.9861435294151306
Epoch 580, training loss: 0.08154038339853287 = 0.01373287383466959 + 0.01 * 6.7807512283325195
Epoch 580, val loss: 0.9965258836746216
Epoch 590, training loss: 0.08057525753974915 = 0.012868274003267288 + 0.01 * 6.770698070526123
Epoch 590, val loss: 1.006557822227478
Epoch 600, training loss: 0.07984219491481781 = 0.01208506803959608 + 0.01 * 6.775712966918945
Epoch 600, val loss: 1.016336441040039
Epoch 610, training loss: 0.07900910079479218 = 0.011375080794095993 + 0.01 * 6.763401985168457
Epoch 610, val loss: 1.025847315788269
Epoch 620, training loss: 0.07830315828323364 = 0.01072769332677126 + 0.01 * 6.757546424865723
Epoch 620, val loss: 1.0351790189743042
Epoch 630, training loss: 0.07776399701833725 = 0.010135849937796593 + 0.01 * 6.762814521789551
Epoch 630, val loss: 1.0441666841506958
Epoch 640, training loss: 0.07725657522678375 = 0.009594601579010487 + 0.01 * 6.766197204589844
Epoch 640, val loss: 1.0528843402862549
Epoch 650, training loss: 0.07658615708351135 = 0.009099534712731838 + 0.01 * 6.74866247177124
Epoch 650, val loss: 1.061436414718628
Epoch 660, training loss: 0.07603899389505386 = 0.008643398992717266 + 0.01 * 6.739560127258301
Epoch 660, val loss: 1.0697076320648193
Epoch 670, training loss: 0.0755520612001419 = 0.008223933167755604 + 0.01 * 6.732812881469727
Epoch 670, val loss: 1.077728033065796
Epoch 680, training loss: 0.07520636171102524 = 0.007837225683033466 + 0.01 * 6.736913681030273
Epoch 680, val loss: 1.0855326652526855
Epoch 690, training loss: 0.0747864842414856 = 0.007479528896510601 + 0.01 * 6.730695724487305
Epoch 690, val loss: 1.0931508541107178
Epoch 700, training loss: 0.07434237748384476 = 0.007147346623241901 + 0.01 * 6.719502925872803
Epoch 700, val loss: 1.1004810333251953
Epoch 710, training loss: 0.07401637732982635 = 0.006839110050350428 + 0.01 * 6.717726707458496
Epoch 710, val loss: 1.1076732873916626
Epoch 720, training loss: 0.07373414933681488 = 0.00655231112614274 + 0.01 * 6.718183994293213
Epoch 720, val loss: 1.1146259307861328
Epoch 730, training loss: 0.0733254924416542 = 0.006285223178565502 + 0.01 * 6.7040276527404785
Epoch 730, val loss: 1.121428370475769
Epoch 740, training loss: 0.07326851785182953 = 0.006035813130438328 + 0.01 * 6.723270893096924
Epoch 740, val loss: 1.1279653310775757
Epoch 750, training loss: 0.0728059932589531 = 0.00580362556502223 + 0.01 * 6.700237274169922
Epoch 750, val loss: 1.1344525814056396
Epoch 760, training loss: 0.07252795994281769 = 0.00558583065867424 + 0.01 * 6.694212913513184
Epoch 760, val loss: 1.1406813859939575
Epoch 770, training loss: 0.07242931425571442 = 0.005381993018090725 + 0.01 * 6.7047319412231445
Epoch 770, val loss: 1.1467894315719604
Epoch 780, training loss: 0.07200992107391357 = 0.005190644878894091 + 0.01 * 6.68192720413208
Epoch 780, val loss: 1.1526436805725098
Epoch 790, training loss: 0.07178052514791489 = 0.005011041648685932 + 0.01 * 6.676948547363281
Epoch 790, val loss: 1.158493161201477
Epoch 800, training loss: 0.07159817218780518 = 0.004841674119234085 + 0.01 * 6.675649642944336
Epoch 800, val loss: 1.1640288829803467
Epoch 810, training loss: 0.0713193267583847 = 0.004682769067585468 + 0.01 * 6.663655757904053
Epoch 810, val loss: 1.16950261592865
Epoch 820, training loss: 0.0714016929268837 = 0.004532745108008385 + 0.01 * 6.686894416809082
Epoch 820, val loss: 1.174705982208252
Epoch 830, training loss: 0.07097261399030685 = 0.004392051603645086 + 0.01 * 6.658056735992432
Epoch 830, val loss: 1.179943323135376
Epoch 840, training loss: 0.07073938101530075 = 0.0042585572227835655 + 0.01 * 6.648082256317139
Epoch 840, val loss: 1.184965968132019
Epoch 850, training loss: 0.07089725136756897 = 0.004132377449423075 + 0.01 * 6.676487922668457
Epoch 850, val loss: 1.1898095607757568
Epoch 860, training loss: 0.0703287124633789 = 0.004012786317616701 + 0.01 * 6.631592750549316
Epoch 860, val loss: 1.1945879459381104
Epoch 870, training loss: 0.07012402266263962 = 0.003899622242897749 + 0.01 * 6.622440338134766
Epoch 870, val loss: 1.1992466449737549
Epoch 880, training loss: 0.07003889977931976 = 0.0037921732291579247 + 0.01 * 6.624672889709473
Epoch 880, val loss: 1.2037055492401123
Epoch 890, training loss: 0.06983034312725067 = 0.0036903510335832834 + 0.01 * 6.613999366760254
Epoch 890, val loss: 1.2082222700119019
Epoch 900, training loss: 0.06983154267072678 = 0.0035932993050664663 + 0.01 * 6.623824596405029
Epoch 900, val loss: 1.2125338315963745
Epoch 910, training loss: 0.06950334459543228 = 0.003500802908092737 + 0.01 * 6.600254535675049
Epoch 910, val loss: 1.216686487197876
Epoch 920, training loss: 0.06946409493684769 = 0.0034128709230571985 + 0.01 * 6.6051225662231445
Epoch 920, val loss: 1.2208212614059448
Epoch 930, training loss: 0.06928812712430954 = 0.0033285885583609343 + 0.01 * 6.595953941345215
Epoch 930, val loss: 1.2248066663742065
Epoch 940, training loss: 0.06941898167133331 = 0.00324783637188375 + 0.01 * 6.617114543914795
Epoch 940, val loss: 1.2287993431091309
Epoch 950, training loss: 0.06910078972578049 = 0.0031703412532806396 + 0.01 * 6.593045234680176
Epoch 950, val loss: 1.2326167821884155
Epoch 960, training loss: 0.06926129758358002 = 0.0030962605960667133 + 0.01 * 6.616503715515137
Epoch 960, val loss: 1.2363946437835693
Epoch 970, training loss: 0.06879471987485886 = 0.00302552105858922 + 0.01 * 6.5769195556640625
Epoch 970, val loss: 1.2401838302612305
Epoch 980, training loss: 0.06883922219276428 = 0.0029572956264019012 + 0.01 * 6.588193416595459
Epoch 980, val loss: 1.2438431978225708
Epoch 990, training loss: 0.0686393678188324 = 0.002892003860324621 + 0.01 * 6.574736595153809
Epoch 990, val loss: 1.2473753690719604
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.6753
Flip ASR: 0.6222/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.027780532836914 = 1.9440419673919678 + 0.01 * 8.373845100402832
Epoch 0, val loss: 1.9348253011703491
Epoch 10, training loss: 2.017627716064453 = 1.9338897466659546 + 0.01 * 8.373799324035645
Epoch 10, val loss: 1.9247610569000244
Epoch 20, training loss: 2.0057857036590576 = 1.922049880027771 + 0.01 * 8.373591423034668
Epoch 20, val loss: 1.9127848148345947
Epoch 30, training loss: 1.989467978477478 = 1.9057371616363525 + 0.01 * 8.373080253601074
Epoch 30, val loss: 1.8963087797164917
Epoch 40, training loss: 1.96548593044281 = 1.8817728757858276 + 0.01 * 8.371310234069824
Epoch 40, val loss: 1.8726476430892944
Epoch 50, training loss: 1.9308775663375854 = 1.8472663164138794 + 0.01 * 8.361128807067871
Epoch 50, val loss: 1.8400461673736572
Epoch 60, training loss: 1.8882077932357788 = 1.8052481412887573 + 0.01 * 8.295970916748047
Epoch 60, val loss: 1.8036984205245972
Epoch 70, training loss: 1.8446863889694214 = 1.7647532224655151 + 0.01 * 7.993320941925049
Epoch 70, val loss: 1.7708250284194946
Epoch 80, training loss: 1.7916576862335205 = 1.7168176174163818 + 0.01 * 7.4840087890625
Epoch 80, val loss: 1.7283680438995361
Epoch 90, training loss: 1.72240149974823 = 1.6505804061889648 + 0.01 * 7.182114601135254
Epoch 90, val loss: 1.670791506767273
Epoch 100, training loss: 1.6342074871063232 = 1.5636720657348633 + 0.01 * 7.053541660308838
Epoch 100, val loss: 1.5989301204681396
Epoch 110, training loss: 1.5330396890640259 = 1.463099479675293 + 0.01 * 6.994018077850342
Epoch 110, val loss: 1.517655849456787
Epoch 120, training loss: 1.4281355142593384 = 1.3585208654403687 + 0.01 * 6.961470603942871
Epoch 120, val loss: 1.4327542781829834
Epoch 130, training loss: 1.3235182762145996 = 1.254024863243103 + 0.01 * 6.949343681335449
Epoch 130, val loss: 1.348778486251831
Epoch 140, training loss: 1.2206398248672485 = 1.1512837409973145 + 0.01 * 6.935603618621826
Epoch 140, val loss: 1.2671635150909424
Epoch 150, training loss: 1.1224949359893799 = 1.0532913208007812 + 0.01 * 6.920365810394287
Epoch 150, val loss: 1.1902128458023071
Epoch 160, training loss: 1.0311578512191772 = 0.9620961546897888 + 0.01 * 6.906173229217529
Epoch 160, val loss: 1.1198499202728271
Epoch 170, training loss: 0.9456042051315308 = 0.8766607046127319 + 0.01 * 6.894350051879883
Epoch 170, val loss: 1.0545645952224731
Epoch 180, training loss: 0.863548755645752 = 0.7947415709495544 + 0.01 * 6.8807196617126465
Epoch 180, val loss: 0.9917959570884705
Epoch 190, training loss: 0.7843424081802368 = 0.7156698107719421 + 0.01 * 6.867262363433838
Epoch 190, val loss: 0.9312825798988342
Epoch 200, training loss: 0.7094649076461792 = 0.6409108638763428 + 0.01 * 6.855404376983643
Epoch 200, val loss: 0.874873697757721
Epoch 210, training loss: 0.6409736275672913 = 0.5725218057632446 + 0.01 * 6.845180988311768
Epoch 210, val loss: 0.8251563310623169
Epoch 220, training loss: 0.5797187089920044 = 0.5113325119018555 + 0.01 * 6.838620662689209
Epoch 220, val loss: 0.7837962508201599
Epoch 230, training loss: 0.524736225605011 = 0.45639142394065857 + 0.01 * 6.834482669830322
Epoch 230, val loss: 0.7507610321044922
Epoch 240, training loss: 0.4743503928184509 = 0.4060024321079254 + 0.01 * 6.834794521331787
Epoch 240, val loss: 0.7246084213256836
Epoch 250, training loss: 0.42698001861572266 = 0.35866618156433105 + 0.01 * 6.831383228302002
Epoch 250, val loss: 0.7038024067878723
Epoch 260, training loss: 0.3821033835411072 = 0.3138498365879059 + 0.01 * 6.825353145599365
Epoch 260, val loss: 0.6872079968452454
Epoch 270, training loss: 0.3403266370296478 = 0.27208849787712097 + 0.01 * 6.823814392089844
Epoch 270, val loss: 0.6743514537811279
Epoch 280, training loss: 0.30262482166290283 = 0.234390988945961 + 0.01 * 6.823382377624512
Epoch 280, val loss: 0.6653448343276978
Epoch 290, training loss: 0.26972562074661255 = 0.20149201154708862 + 0.01 * 6.823360443115234
Epoch 290, val loss: 0.6603299379348755
Epoch 300, training loss: 0.24173866212368011 = 0.17349720001220703 + 0.01 * 6.824146270751953
Epoch 300, val loss: 0.6591636538505554
Epoch 310, training loss: 0.2182188481092453 = 0.149982750415802 + 0.01 * 6.823609828948975
Epoch 310, val loss: 0.6614604592323303
Epoch 320, training loss: 0.19853335618972778 = 0.13029801845550537 + 0.01 * 6.82353401184082
Epoch 320, val loss: 0.6668776869773865
Epoch 330, training loss: 0.1820039451122284 = 0.11377709358930588 + 0.01 * 6.822686195373535
Epoch 330, val loss: 0.6748055219650269
Epoch 340, training loss: 0.16807374358177185 = 0.09984005242586136 + 0.01 * 6.823368549346924
Epoch 340, val loss: 0.6847915649414062
Epoch 350, training loss: 0.15623602271080017 = 0.08801472187042236 + 0.01 * 6.82213020324707
Epoch 350, val loss: 0.6963790655136108
Epoch 360, training loss: 0.14614202082157135 = 0.077925905585289 + 0.01 * 6.8216118812561035
Epoch 360, val loss: 0.709141194820404
Epoch 370, training loss: 0.13748259842395782 = 0.06927590072154999 + 0.01 * 6.820669651031494
Epoch 370, val loss: 0.7228047847747803
Epoch 380, training loss: 0.1300298124551773 = 0.0618281327188015 + 0.01 * 6.820168495178223
Epoch 380, val loss: 0.7371201515197754
Epoch 390, training loss: 0.12358149886131287 = 0.05538991838693619 + 0.01 * 6.819158554077148
Epoch 390, val loss: 0.7518482804298401
Epoch 400, training loss: 0.11797720938920975 = 0.04980423301458359 + 0.01 * 6.81729793548584
Epoch 400, val loss: 0.766822099685669
Epoch 410, training loss: 0.11311151087284088 = 0.044942863285541534 + 0.01 * 6.816864967346191
Epoch 410, val loss: 0.7819634675979614
Epoch 420, training loss: 0.10886961221694946 = 0.0406983382999897 + 0.01 * 6.8171281814575195
Epoch 420, val loss: 0.7970921993255615
Epoch 430, training loss: 0.10510951280593872 = 0.036981645971536636 + 0.01 * 6.812786102294922
Epoch 430, val loss: 0.8121671676635742
Epoch 440, training loss: 0.10183043777942657 = 0.033716220408678055 + 0.01 * 6.811421871185303
Epoch 440, val loss: 0.8270929455757141
Epoch 450, training loss: 0.09893111884593964 = 0.03083835169672966 + 0.01 * 6.809277534484863
Epoch 450, val loss: 0.8418170213699341
Epoch 460, training loss: 0.0963752344250679 = 0.028295770287513733 + 0.01 * 6.80794620513916
Epoch 460, val loss: 0.856245219707489
Epoch 470, training loss: 0.09409031271934509 = 0.026042381301522255 + 0.01 * 6.804793357849121
Epoch 470, val loss: 0.8703470230102539
Epoch 480, training loss: 0.09206359088420868 = 0.02403843216598034 + 0.01 * 6.802515983581543
Epoch 480, val loss: 0.8841922283172607
Epoch 490, training loss: 0.09026400744915009 = 0.022250201553106308 + 0.01 * 6.8013811111450195
Epoch 490, val loss: 0.8976113796234131
Epoch 500, training loss: 0.08862283825874329 = 0.020648207515478134 + 0.01 * 6.7974629402160645
Epoch 500, val loss: 0.9107629060745239
Epoch 510, training loss: 0.08727214485406876 = 0.01920747198164463 + 0.01 * 6.806467533111572
Epoch 510, val loss: 0.9236192107200623
Epoch 520, training loss: 0.08584614098072052 = 0.01790960319340229 + 0.01 * 6.793654441833496
Epoch 520, val loss: 0.9360976219177246
Epoch 530, training loss: 0.08464379608631134 = 0.01673545315861702 + 0.01 * 6.790834903717041
Epoch 530, val loss: 0.9482709169387817
Epoch 540, training loss: 0.0835372656583786 = 0.0156694445759058 + 0.01 * 6.786782741546631
Epoch 540, val loss: 0.9601478576660156
Epoch 550, training loss: 0.0825774148106575 = 0.014698849059641361 + 0.01 * 6.7878570556640625
Epoch 550, val loss: 0.9717882871627808
Epoch 560, training loss: 0.08164931833744049 = 0.013812754303216934 + 0.01 * 6.783656120300293
Epoch 560, val loss: 0.9831300973892212
Epoch 570, training loss: 0.08077353239059448 = 0.013002177700400352 + 0.01 * 6.777135372161865
Epoch 570, val loss: 0.9942295551300049
Epoch 580, training loss: 0.08001392334699631 = 0.012259094975888729 + 0.01 * 6.775482654571533
Epoch 580, val loss: 1.0050861835479736
Epoch 590, training loss: 0.079294353723526 = 0.011576754972338676 + 0.01 * 6.771759986877441
Epoch 590, val loss: 1.015716791152954
Epoch 600, training loss: 0.07875587046146393 = 0.010949267074465752 + 0.01 * 6.780660152435303
Epoch 600, val loss: 1.0261034965515137
Epoch 610, training loss: 0.07800684124231339 = 0.010373184457421303 + 0.01 * 6.763366222381592
Epoch 610, val loss: 1.0362478494644165
Epoch 620, training loss: 0.07764046639204025 = 0.0098424032330513 + 0.01 * 6.779806613922119
Epoch 620, val loss: 1.046161413192749
Epoch 630, training loss: 0.07696215063333511 = 0.00935232825577259 + 0.01 * 6.760982513427734
Epoch 630, val loss: 1.0558308362960815
Epoch 640, training loss: 0.0764123797416687 = 0.008898859843611717 + 0.01 * 6.751351833343506
Epoch 640, val loss: 1.0652498006820679
Epoch 650, training loss: 0.0759514644742012 = 0.008478512056171894 + 0.01 * 6.747294902801514
Epoch 650, val loss: 1.0745244026184082
Epoch 660, training loss: 0.07555469125509262 = 0.008088496513664722 + 0.01 * 6.746619701385498
Epoch 660, val loss: 1.0835657119750977
Epoch 670, training loss: 0.07509329915046692 = 0.007726285140961409 + 0.01 * 6.736701965332031
Epoch 670, val loss: 1.0923982858657837
Epoch 680, training loss: 0.07473409175872803 = 0.007388886064291 + 0.01 * 6.734520435333252
Epoch 680, val loss: 1.1010363101959229
Epoch 690, training loss: 0.07446586340665817 = 0.0070749493315815926 + 0.01 * 6.739091396331787
Epoch 690, val loss: 1.109473705291748
Epoch 700, training loss: 0.07404544204473495 = 0.006782275624573231 + 0.01 * 6.726316452026367
Epoch 700, val loss: 1.11768639087677
Epoch 710, training loss: 0.07390345633029938 = 0.006508692167699337 + 0.01 * 6.739476203918457
Epoch 710, val loss: 1.1257081031799316
Epoch 720, training loss: 0.07338393479585648 = 0.006252985913306475 + 0.01 * 6.713094711303711
Epoch 720, val loss: 1.1335558891296387
Epoch 730, training loss: 0.07336107641458511 = 0.006013269070535898 + 0.01 * 6.734781265258789
Epoch 730, val loss: 1.141232967376709
Epoch 740, training loss: 0.07284446805715561 = 0.005789430346339941 + 0.01 * 6.705504417419434
Epoch 740, val loss: 1.1486989259719849
Epoch 750, training loss: 0.07256408780813217 = 0.005578905344009399 + 0.01 * 6.6985182762146
Epoch 750, val loss: 1.1560043096542358
Epoch 760, training loss: 0.07233670353889465 = 0.005381165537983179 + 0.01 * 6.695553779602051
Epoch 760, val loss: 1.1631536483764648
Epoch 770, training loss: 0.07202931493520737 = 0.005195089615881443 + 0.01 * 6.683422565460205
Epoch 770, val loss: 1.1700595617294312
Epoch 780, training loss: 0.07192083448171616 = 0.005019917152822018 + 0.01 * 6.690092086791992
Epoch 780, val loss: 1.176939845085144
Epoch 790, training loss: 0.07157176733016968 = 0.004854807164520025 + 0.01 * 6.671696186065674
Epoch 790, val loss: 1.1835540533065796
Epoch 800, training loss: 0.0714220255613327 = 0.004698752425611019 + 0.01 * 6.672327518463135
Epoch 800, val loss: 1.1899908781051636
Epoch 810, training loss: 0.07129660248756409 = 0.00455147959291935 + 0.01 * 6.67451286315918
Epoch 810, val loss: 1.1962562799453735
Epoch 820, training loss: 0.07111186534166336 = 0.004411962348967791 + 0.01 * 6.669990062713623
Epoch 820, val loss: 1.2025374174118042
Epoch 830, training loss: 0.07082788646221161 = 0.0042800940573215485 + 0.01 * 6.65477991104126
Epoch 830, val loss: 1.2085038423538208
Epoch 840, training loss: 0.07074565440416336 = 0.0041549596935510635 + 0.01 * 6.659069538116455
Epoch 840, val loss: 1.2144112586975098
Epoch 850, training loss: 0.07055488228797913 = 0.004036195110529661 + 0.01 * 6.65186882019043
Epoch 850, val loss: 1.220232367515564
Epoch 860, training loss: 0.07031729072332382 = 0.003923221491277218 + 0.01 * 6.639407157897949
Epoch 860, val loss: 1.2258362770080566
Epoch 870, training loss: 0.07015466690063477 = 0.0038160872645676136 + 0.01 * 6.633857727050781
Epoch 870, val loss: 1.2314249277114868
Epoch 880, training loss: 0.07004820555448532 = 0.0037140576168894768 + 0.01 * 6.633415222167969
Epoch 880, val loss: 1.236856460571289
Epoch 890, training loss: 0.06992439925670624 = 0.0036166885402053595 + 0.01 * 6.630770683288574
Epoch 890, val loss: 1.2421908378601074
Epoch 900, training loss: 0.06985533237457275 = 0.0035240035504102707 + 0.01 * 6.6331329345703125
Epoch 900, val loss: 1.2474814653396606
Epoch 910, training loss: 0.06962800025939941 = 0.003435448044911027 + 0.01 * 6.619255065917969
Epoch 910, val loss: 1.2525385618209839
Epoch 920, training loss: 0.06951134651899338 = 0.0033510078210383654 + 0.01 * 6.616033554077148
Epoch 920, val loss: 1.2576050758361816
Epoch 930, training loss: 0.06934112310409546 = 0.0032701764721423388 + 0.01 * 6.6070942878723145
Epoch 930, val loss: 1.2625648975372314
Epoch 940, training loss: 0.06924131512641907 = 0.003193006617948413 + 0.01 * 6.604830741882324
Epoch 940, val loss: 1.2673993110656738
Epoch 950, training loss: 0.06920187175273895 = 0.0031191271264106035 + 0.01 * 6.608274459838867
Epoch 950, val loss: 1.2721426486968994
Epoch 960, training loss: 0.0691026821732521 = 0.0030482893344014883 + 0.01 * 6.60543966293335
Epoch 960, val loss: 1.2768561840057373
Epoch 970, training loss: 0.06894505769014359 = 0.002980508841574192 + 0.01 * 6.596455097198486
Epoch 970, val loss: 1.2814525365829468
Epoch 980, training loss: 0.06886141747236252 = 0.0029154361691325903 + 0.01 * 6.594598770141602
Epoch 980, val loss: 1.2859901189804077
Epoch 990, training loss: 0.06869015842676163 = 0.0028530133422464132 + 0.01 * 6.583714962005615
Epoch 990, val loss: 1.2904599905014038
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.8708
Flip ASR: 0.8444/225 nodes
The final ASR:0.71341, 0.11616, Accuracy:0.82222, 0.01210
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11578])
remove edge: torch.Size([2, 9572])
updated graph: torch.Size([2, 10594])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.97663, 0.00174, Accuracy:0.83457, 0.00630
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.023780107498169 = 1.9400415420532227 + 0.01 * 8.373859405517578
Epoch 0, val loss: 1.9357932806015015
Epoch 10, training loss: 2.0130412578582764 = 1.9293034076690674 + 0.01 * 8.373779296875
Epoch 10, val loss: 1.9249762296676636
Epoch 20, training loss: 1.9997819662094116 = 1.9160469770431519 + 0.01 * 8.373496055603027
Epoch 20, val loss: 1.9114646911621094
Epoch 30, training loss: 1.981177568435669 = 1.8974515199661255 + 0.01 * 8.372600555419922
Epoch 30, val loss: 1.8925721645355225
Epoch 40, training loss: 1.9540225267410278 = 1.8703535795211792 + 0.01 * 8.366891860961914
Epoch 40, val loss: 1.8658076524734497
Epoch 50, training loss: 1.9168555736541748 = 1.8336888551712036 + 0.01 * 8.316668510437012
Epoch 50, val loss: 1.8321574926376343
Epoch 60, training loss: 1.8739070892333984 = 1.793784737586975 + 0.01 * 8.012234687805176
Epoch 60, val loss: 1.8003398180007935
Epoch 70, training loss: 1.832165002822876 = 1.753810167312622 + 0.01 * 7.835488319396973
Epoch 70, val loss: 1.7692782878875732
Epoch 80, training loss: 1.7744734287261963 = 1.69851553440094 + 0.01 * 7.5957865715026855
Epoch 80, val loss: 1.7217570543289185
Epoch 90, training loss: 1.6973837614059448 = 1.623875379562378 + 0.01 * 7.350841045379639
Epoch 90, val loss: 1.6586458683013916
Epoch 100, training loss: 1.604677677154541 = 1.5336867570877075 + 0.01 * 7.099092960357666
Epoch 100, val loss: 1.5856000185012817
Epoch 110, training loss: 1.5111708641052246 = 1.4404873847961426 + 0.01 * 7.068347454071045
Epoch 110, val loss: 1.5098929405212402
Epoch 120, training loss: 1.420465111732483 = 1.3500429391860962 + 0.01 * 7.042218208312988
Epoch 120, val loss: 1.4405723810195923
Epoch 130, training loss: 1.331871509552002 = 1.2615714073181152 + 0.01 * 7.030012130737305
Epoch 130, val loss: 1.375285029411316
Epoch 140, training loss: 1.2436470985412598 = 1.1734321117401123 + 0.01 * 7.0214948654174805
Epoch 140, val loss: 1.311949372291565
Epoch 150, training loss: 1.1561344861984253 = 1.0860023498535156 + 0.01 * 7.013217449188232
Epoch 150, val loss: 1.2502107620239258
Epoch 160, training loss: 1.0704243183135986 = 1.0004081726074219 + 0.01 * 7.001610279083252
Epoch 160, val loss: 1.1906487941741943
Epoch 170, training loss: 0.9871842265129089 = 0.9173102378845215 + 0.01 * 6.98739767074585
Epoch 170, val loss: 1.1325637102127075
Epoch 180, training loss: 0.9066516757011414 = 0.8369565606117249 + 0.01 * 6.96951150894165
Epoch 180, val loss: 1.0761045217514038
Epoch 190, training loss: 0.8292762041091919 = 0.7597542405128479 + 0.01 * 6.952194690704346
Epoch 190, val loss: 1.021508812904358
Epoch 200, training loss: 0.7560381293296814 = 0.6866622567176819 + 0.01 * 6.937587738037109
Epoch 200, val loss: 0.9702007174491882
Epoch 210, training loss: 0.6883013248443604 = 0.6190255880355835 + 0.01 * 6.9275736808776855
Epoch 210, val loss: 0.9239820837974548
Epoch 220, training loss: 0.6266098022460938 = 0.5573834776878357 + 0.01 * 6.922635555267334
Epoch 220, val loss: 0.8842862844467163
Epoch 230, training loss: 0.5712038278579712 = 0.5020141005516052 + 0.01 * 6.91897439956665
Epoch 230, val loss: 0.8523569703102112
Epoch 240, training loss: 0.522016167640686 = 0.45285356044769287 + 0.01 * 6.916260719299316
Epoch 240, val loss: 0.8290008306503296
Epoch 250, training loss: 0.4784497320652008 = 0.4093175530433655 + 0.01 * 6.913218021392822
Epoch 250, val loss: 0.8137006163597107
Epoch 260, training loss: 0.43964576721191406 = 0.3705167770385742 + 0.01 * 6.912898063659668
Epoch 260, val loss: 0.8050372004508972
Epoch 270, training loss: 0.4045039415359497 = 0.33543065190315247 + 0.01 * 6.907329559326172
Epoch 270, val loss: 0.8012471795082092
Epoch 280, training loss: 0.3718620240688324 = 0.30283692479133606 + 0.01 * 6.902511119842529
Epoch 280, val loss: 0.8007997870445251
Epoch 290, training loss: 0.34050577878952026 = 0.27151116728782654 + 0.01 * 6.899460315704346
Epoch 290, val loss: 0.8024503588676453
Epoch 300, training loss: 0.3097909390926361 = 0.2408357858657837 + 0.01 * 6.895515441894531
Epoch 300, val loss: 0.8051623106002808
Epoch 310, training loss: 0.2801758050918579 = 0.21122263371944427 + 0.01 * 6.895318984985352
Epoch 310, val loss: 0.8088735938072205
Epoch 320, training loss: 0.25268763303756714 = 0.18380197882652283 + 0.01 * 6.888566493988037
Epoch 320, val loss: 0.8141627311706543
Epoch 330, training loss: 0.2283751666545868 = 0.15956361591815948 + 0.01 * 6.881155490875244
Epoch 330, val loss: 0.8215797543525696
Epoch 340, training loss: 0.2077488899230957 = 0.13885851204395294 + 0.01 * 6.889037609100342
Epoch 340, val loss: 0.8314914107322693
Epoch 350, training loss: 0.19011190533638 = 0.12141217291355133 + 0.01 * 6.869974136352539
Epoch 350, val loss: 0.8438120484352112
Epoch 360, training loss: 0.17530646920204163 = 0.10669058561325073 + 0.01 * 6.8615875244140625
Epoch 360, val loss: 0.8579378128051758
Epoch 370, training loss: 0.16274483501911163 = 0.09419713169336319 + 0.01 * 6.854770660400391
Epoch 370, val loss: 0.8732649683952332
Epoch 380, training loss: 0.1520521491765976 = 0.08352785557508469 + 0.01 * 6.852429389953613
Epoch 380, val loss: 0.8893429040908813
Epoch 390, training loss: 0.142683207988739 = 0.0743599534034729 + 0.01 * 6.832325458526611
Epoch 390, val loss: 0.9057168960571289
Epoch 400, training loss: 0.13473987579345703 = 0.0664241686463356 + 0.01 * 6.831571578979492
Epoch 400, val loss: 0.9221659898757935
Epoch 410, training loss: 0.12755638360977173 = 0.0595160610973835 + 0.01 * 6.804032802581787
Epoch 410, val loss: 0.938614010810852
Epoch 420, training loss: 0.12175854295492172 = 0.0534733310341835 + 0.01 * 6.828521251678467
Epoch 420, val loss: 0.9549075365066528
Epoch 430, training loss: 0.116091787815094 = 0.04818866029381752 + 0.01 * 6.790312767028809
Epoch 430, val loss: 0.9711284637451172
Epoch 440, training loss: 0.11122281104326248 = 0.04354393482208252 + 0.01 * 6.767888069152832
Epoch 440, val loss: 0.9871475696563721
Epoch 450, training loss: 0.10700348019599915 = 0.03945504501461983 + 0.01 * 6.7548441886901855
Epoch 450, val loss: 1.0030258893966675
Epoch 460, training loss: 0.10394090414047241 = 0.035856783390045166 + 0.01 * 6.808412551879883
Epoch 460, val loss: 1.018632173538208
Epoch 470, training loss: 0.10005243122577667 = 0.03269302845001221 + 0.01 * 6.735940456390381
Epoch 470, val loss: 1.0339239835739136
Epoch 480, training loss: 0.09746012836694717 = 0.029900869354605675 + 0.01 * 6.755926132202148
Epoch 480, val loss: 1.048774242401123
Epoch 490, training loss: 0.09461724758148193 = 0.027439238503575325 + 0.01 * 6.717801094055176
Epoch 490, val loss: 1.0633928775787354
Epoch 500, training loss: 0.09237992018461227 = 0.02525639347732067 + 0.01 * 6.712352275848389
Epoch 500, val loss: 1.0774630308151245
Epoch 510, training loss: 0.09027417004108429 = 0.02331279031932354 + 0.01 * 6.69613790512085
Epoch 510, val loss: 1.0911933183670044
Epoch 520, training loss: 0.08891468495130539 = 0.02157488279044628 + 0.01 * 6.733980178833008
Epoch 520, val loss: 1.104461669921875
Epoch 530, training loss: 0.086795374751091 = 0.020019851624965668 + 0.01 * 6.677552223205566
Epoch 530, val loss: 1.1173889636993408
Epoch 540, training loss: 0.08572323620319366 = 0.01862534135580063 + 0.01 * 6.709789276123047
Epoch 540, val loss: 1.129855990409851
Epoch 550, training loss: 0.08400975167751312 = 0.017372455447912216 + 0.01 * 6.663729190826416
Epoch 550, val loss: 1.141880750656128
Epoch 560, training loss: 0.08280563354492188 = 0.016240719705820084 + 0.01 * 6.656490802764893
Epoch 560, val loss: 1.1535720825195312
Epoch 570, training loss: 0.08218518644571304 = 0.015215879306197166 + 0.01 * 6.696930885314941
Epoch 570, val loss: 1.1648539304733276
Epoch 580, training loss: 0.08077830821275711 = 0.014286890625953674 + 0.01 * 6.649142265319824
Epoch 580, val loss: 1.1758596897125244
Epoch 590, training loss: 0.07989896833896637 = 0.013441412709653378 + 0.01 * 6.645755767822266
Epoch 590, val loss: 1.1865571737289429
Epoch 600, training loss: 0.07898297160863876 = 0.012670373544096947 + 0.01 * 6.631259918212891
Epoch 600, val loss: 1.1967953443527222
Epoch 610, training loss: 0.07854987680912018 = 0.011966054327785969 + 0.01 * 6.658381938934326
Epoch 610, val loss: 1.2067716121673584
Epoch 620, training loss: 0.0777205377817154 = 0.011321110650897026 + 0.01 * 6.639942646026611
Epoch 620, val loss: 1.2165662050247192
Epoch 630, training loss: 0.07699911296367645 = 0.010728985071182251 + 0.01 * 6.627012729644775
Epoch 630, val loss: 1.2259182929992676
Epoch 640, training loss: 0.0763639286160469 = 0.010183976963162422 + 0.01 * 6.617995738983154
Epoch 640, val loss: 1.2350256443023682
Epoch 650, training loss: 0.07587897032499313 = 0.00968153029680252 + 0.01 * 6.619743824005127
Epoch 650, val loss: 1.2439348697662354
Epoch 660, training loss: 0.07532187551259995 = 0.0092170974239707 + 0.01 * 6.610477924346924
Epoch 660, val loss: 1.2524642944335938
Epoch 670, training loss: 0.07477903366088867 = 0.0087876096367836 + 0.01 * 6.599142551422119
Epoch 670, val loss: 1.2607895135879517
Epoch 680, training loss: 0.07415842264890671 = 0.008389302529394627 + 0.01 * 6.576911926269531
Epoch 680, val loss: 1.268918752670288
Epoch 690, training loss: 0.07393647730350494 = 0.00801929458975792 + 0.01 * 6.5917181968688965
Epoch 690, val loss: 1.2767492532730103
Epoch 700, training loss: 0.0736389011144638 = 0.007675320375710726 + 0.01 * 6.596358299255371
Epoch 700, val loss: 1.28443443775177
Epoch 710, training loss: 0.07321301847696304 = 0.007354824338108301 + 0.01 * 6.585819721221924
Epoch 710, val loss: 1.2918781042099
Epoch 720, training loss: 0.07272204756736755 = 0.007055440451949835 + 0.01 * 6.566661357879639
Epoch 720, val loss: 1.2990837097167969
Epoch 730, training loss: 0.07239632308483124 = 0.006775845307856798 + 0.01 * 6.562047481536865
Epoch 730, val loss: 1.3060939311981201
Epoch 740, training loss: 0.0720931813120842 = 0.006514175329357386 + 0.01 * 6.557900428771973
Epoch 740, val loss: 1.3130024671554565
Epoch 750, training loss: 0.07160735875368118 = 0.006268709897994995 + 0.01 * 6.533864974975586
Epoch 750, val loss: 1.319631576538086
Epoch 760, training loss: 0.07139374315738678 = 0.006038161925971508 + 0.01 * 6.535558700561523
Epoch 760, val loss: 1.3261122703552246
Epoch 770, training loss: 0.07136192172765732 = 0.005821351893246174 + 0.01 * 6.5540571212768555
Epoch 770, val loss: 1.332431674003601
Epoch 780, training loss: 0.07113415747880936 = 0.005617591552436352 + 0.01 * 6.551657199859619
Epoch 780, val loss: 1.3387045860290527
Epoch 790, training loss: 0.07082583755254745 = 0.005425447132438421 + 0.01 * 6.540039539337158
Epoch 790, val loss: 1.3446202278137207
Epoch 800, training loss: 0.07083331793546677 = 0.005244591739028692 + 0.01 * 6.558872699737549
Epoch 800, val loss: 1.3506381511688232
Epoch 810, training loss: 0.07016433775424957 = 0.005073351785540581 + 0.01 * 6.509098529815674
Epoch 810, val loss: 1.3562625646591187
Epoch 820, training loss: 0.07024706155061722 = 0.0049116709269583225 + 0.01 * 6.533539295196533
Epoch 820, val loss: 1.3618072271347046
Epoch 830, training loss: 0.0701112225651741 = 0.004758790135383606 + 0.01 * 6.535243034362793
Epoch 830, val loss: 1.3673253059387207
Epoch 840, training loss: 0.06973695009946823 = 0.004613779950886965 + 0.01 * 6.512317180633545
Epoch 840, val loss: 1.3725885152816772
Epoch 850, training loss: 0.0695115178823471 = 0.004476344678550959 + 0.01 * 6.5035176277160645
Epoch 850, val loss: 1.3778796195983887
Epoch 860, training loss: 0.06925852596759796 = 0.00434576952829957 + 0.01 * 6.491276264190674
Epoch 860, val loss: 1.3829154968261719
Epoch 870, training loss: 0.06924470514059067 = 0.004221640527248383 + 0.01 * 6.5023064613342285
Epoch 870, val loss: 1.3877943754196167
Epoch 880, training loss: 0.06892634183168411 = 0.004103819373995066 + 0.01 * 6.482252597808838
Epoch 880, val loss: 1.3927375078201294
Epoch 890, training loss: 0.06906282901763916 = 0.003991565201431513 + 0.01 * 6.507126331329346
Epoch 890, val loss: 1.397405743598938
Epoch 900, training loss: 0.06872640550136566 = 0.00388464261777699 + 0.01 * 6.4841766357421875
Epoch 900, val loss: 1.4020824432373047
Epoch 910, training loss: 0.06848019361495972 = 0.0037827177438884974 + 0.01 * 6.469747543334961
Epoch 910, val loss: 1.4065957069396973
Epoch 920, training loss: 0.06847357749938965 = 0.0036854110658168793 + 0.01 * 6.478816986083984
Epoch 920, val loss: 1.4110655784606934
Epoch 930, training loss: 0.06828738003969193 = 0.0035924753174185753 + 0.01 * 6.4694905281066895
Epoch 930, val loss: 1.4153918027877808
Epoch 940, training loss: 0.06812945753335953 = 0.00350372726097703 + 0.01 * 6.462573051452637
Epoch 940, val loss: 1.4197065830230713
Epoch 950, training loss: 0.06804021447896957 = 0.0034187284763902426 + 0.01 * 6.462148666381836
Epoch 950, val loss: 1.423848271369934
Epoch 960, training loss: 0.06795230507850647 = 0.0033374172635376453 + 0.01 * 6.461488246917725
Epoch 960, val loss: 1.4279018640518188
Epoch 970, training loss: 0.06798036396503448 = 0.003259663237258792 + 0.01 * 6.47206974029541
Epoch 970, val loss: 1.431925654411316
Epoch 980, training loss: 0.06783701479434967 = 0.0031851367093622684 + 0.01 * 6.465188026428223
Epoch 980, val loss: 1.4358543157577515
Epoch 990, training loss: 0.06774195283651352 = 0.0031136104371398687 + 0.01 * 6.462834358215332
Epoch 990, val loss: 1.4396535158157349
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.4317
Flip ASR: 0.3333/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.029604196548462 = 1.9458653926849365 + 0.01 * 8.37389087677002
Epoch 0, val loss: 1.9426823854446411
Epoch 10, training loss: 2.020289659500122 = 1.936551570892334 + 0.01 * 8.373812675476074
Epoch 10, val loss: 1.9333231449127197
Epoch 20, training loss: 2.0089263916015625 = 1.9251905679702759 + 0.01 * 8.37357234954834
Epoch 20, val loss: 1.9213532209396362
Epoch 30, training loss: 1.9934264421463013 = 1.9096970558166504 + 0.01 * 8.372941970825195
Epoch 30, val loss: 1.9046335220336914
Epoch 40, training loss: 1.9708796739578247 = 1.8871753215789795 + 0.01 * 8.37043571472168
Epoch 40, val loss: 1.8803253173828125
Epoch 50, training loss: 1.9378741979599 = 1.8543306589126587 + 0.01 * 8.354352951049805
Epoch 50, val loss: 1.8456989526748657
Epoch 60, training loss: 1.8936192989349365 = 1.811023235321045 + 0.01 * 8.259608268737793
Epoch 60, val loss: 1.802473545074463
Epoch 70, training loss: 1.8423068523406982 = 1.7633798122406006 + 0.01 * 7.892706871032715
Epoch 70, val loss: 1.7585219144821167
Epoch 80, training loss: 1.7878680229187012 = 1.7116382122039795 + 0.01 * 7.622984409332275
Epoch 80, val loss: 1.7122423648834229
Epoch 90, training loss: 1.7168982028961182 = 1.6431130170822144 + 0.01 * 7.3785176277160645
Epoch 90, val loss: 1.6515891551971436
Epoch 100, training loss: 1.6255009174346924 = 1.5536404848098755 + 0.01 * 7.18604850769043
Epoch 100, val loss: 1.5744611024856567
Epoch 110, training loss: 1.517676830291748 = 1.4464430809020996 + 0.01 * 7.123376369476318
Epoch 110, val loss: 1.4860198497772217
Epoch 120, training loss: 1.4031438827514648 = 1.3322306871414185 + 0.01 * 7.091322422027588
Epoch 120, val loss: 1.3961164951324463
Epoch 130, training loss: 1.2909480333328247 = 1.2202343940734863 + 0.01 * 7.071366786956787
Epoch 130, val loss: 1.311691403388977
Epoch 140, training loss: 1.1840620040893555 = 1.113492488861084 + 0.01 * 7.056949615478516
Epoch 140, val loss: 1.2334662675857544
Epoch 150, training loss: 1.0832687616348267 = 1.0128344297409058 + 0.01 * 7.043436050415039
Epoch 150, val loss: 1.1602015495300293
Epoch 160, training loss: 0.9892123341560364 = 0.9189407825469971 + 0.01 * 7.027157783508301
Epoch 160, val loss: 1.0916154384613037
Epoch 170, training loss: 0.9030222296714783 = 0.8329281806945801 + 0.01 * 7.009402751922607
Epoch 170, val loss: 1.028471827507019
Epoch 180, training loss: 0.8256238698959351 = 0.7557076811790466 + 0.01 * 6.9916181564331055
Epoch 180, val loss: 0.9722331762313843
Epoch 190, training loss: 0.7570638656616211 = 0.6872995495796204 + 0.01 * 6.976434707641602
Epoch 190, val loss: 0.9238787889480591
Epoch 200, training loss: 0.6961902379989624 = 0.6265313625335693 + 0.01 * 6.96588659286499
Epoch 200, val loss: 0.8827551603317261
Epoch 210, training loss: 0.6411929726600647 = 0.5715962648391724 + 0.01 * 6.9596710205078125
Epoch 210, val loss: 0.8479382991790771
Epoch 220, training loss: 0.5902467966079712 = 0.5207275152206421 + 0.01 * 6.951925277709961
Epoch 220, val loss: 0.8180893063545227
Epoch 230, training loss: 0.542370617389679 = 0.47292575240135193 + 0.01 * 6.944488525390625
Epoch 230, val loss: 0.7923065423965454
Epoch 240, training loss: 0.49687954783439636 = 0.42752277851104736 + 0.01 * 6.935677528381348
Epoch 240, val loss: 0.770110547542572
Epoch 250, training loss: 0.45356661081314087 = 0.3843056857585907 + 0.01 * 6.926092147827148
Epoch 250, val loss: 0.7508353590965271
Epoch 260, training loss: 0.41223785281181335 = 0.3430630564689636 + 0.01 * 6.917479038238525
Epoch 260, val loss: 0.7339739203453064
Epoch 270, training loss: 0.3726603090763092 = 0.3035476505756378 + 0.01 * 6.911264896392822
Epoch 270, val loss: 0.7193884253501892
Epoch 280, training loss: 0.33500948548316956 = 0.2659849524497986 + 0.01 * 6.902453422546387
Epoch 280, val loss: 0.7067875862121582
Epoch 290, training loss: 0.29988986253738403 = 0.23091335594654083 + 0.01 * 6.897651672363281
Epoch 290, val loss: 0.6963652968406677
Epoch 300, training loss: 0.2681770920753479 = 0.1992412805557251 + 0.01 * 6.893580913543701
Epoch 300, val loss: 0.6884620785713196
Epoch 310, training loss: 0.24049119651317596 = 0.17158924043178558 + 0.01 * 6.890195846557617
Epoch 310, val loss: 0.6834903955459595
Epoch 320, training loss: 0.21694892644882202 = 0.14809195697307587 + 0.01 * 6.885697364807129
Epoch 320, val loss: 0.6814879775047302
Epoch 330, training loss: 0.19725638628005981 = 0.1284141093492508 + 0.01 * 6.884228706359863
Epoch 330, val loss: 0.6823606491088867
Epoch 340, training loss: 0.1807764172554016 = 0.11198562383651733 + 0.01 * 6.8790788650512695
Epoch 340, val loss: 0.6857067942619324
Epoch 350, training loss: 0.16697095334529877 = 0.09821325540542603 + 0.01 * 6.87576961517334
Epoch 350, val loss: 0.6910724639892578
Epoch 360, training loss: 0.15526744723320007 = 0.08657214045524597 + 0.01 * 6.869531631469727
Epoch 360, val loss: 0.6978977918624878
Epoch 370, training loss: 0.14531636238098145 = 0.07665089517831802 + 0.01 * 6.866546630859375
Epoch 370, val loss: 0.705734133720398
Epoch 380, training loss: 0.13677121698856354 = 0.0681440457701683 + 0.01 * 6.862717628479004
Epoch 380, val loss: 0.7142711281776428
Epoch 390, training loss: 0.1293778121471405 = 0.06081709638237953 + 0.01 * 6.8560709953308105
Epoch 390, val loss: 0.7234124541282654
Epoch 400, training loss: 0.12298701703548431 = 0.054481662809848785 + 0.01 * 6.8505353927612305
Epoch 400, val loss: 0.7328707575798035
Epoch 410, training loss: 0.11742472648620605 = 0.048988718539476395 + 0.01 * 6.843601226806641
Epoch 410, val loss: 0.7425668835639954
Epoch 420, training loss: 0.11269281804561615 = 0.044211480766534805 + 0.01 * 6.848133563995361
Epoch 420, val loss: 0.7523486018180847
Epoch 430, training loss: 0.10841453820466995 = 0.04004792869091034 + 0.01 * 6.836661338806152
Epoch 430, val loss: 0.7623323202133179
Epoch 440, training loss: 0.10465060919523239 = 0.036416880786418915 + 0.01 * 6.823372840881348
Epoch 440, val loss: 0.7723153233528137
Epoch 450, training loss: 0.10140158981084824 = 0.03323523700237274 + 0.01 * 6.8166351318359375
Epoch 450, val loss: 0.7818858623504639
Epoch 460, training loss: 0.09851117432117462 = 0.03043433651328087 + 0.01 * 6.807683944702148
Epoch 460, val loss: 0.7916502952575684
Epoch 470, training loss: 0.09598111361265182 = 0.02796049229800701 + 0.01 * 6.802062034606934
Epoch 470, val loss: 0.8011483550071716
Epoch 480, training loss: 0.0937250480055809 = 0.025769801810383797 + 0.01 * 6.795524597167969
Epoch 480, val loss: 0.8106903433799744
Epoch 490, training loss: 0.09167802333831787 = 0.023823890835046768 + 0.01 * 6.78541374206543
Epoch 490, val loss: 0.8200207948684692
Epoch 500, training loss: 0.09009118378162384 = 0.02209276333451271 + 0.01 * 6.799842834472656
Epoch 500, val loss: 0.829034686088562
Epoch 510, training loss: 0.0883605107665062 = 0.020547837018966675 + 0.01 * 6.7812676429748535
Epoch 510, val loss: 0.8378791809082031
Epoch 520, training loss: 0.08678697794675827 = 0.01916099153459072 + 0.01 * 6.762598514556885
Epoch 520, val loss: 0.8465188145637512
Epoch 530, training loss: 0.0854300931096077 = 0.017911076545715332 + 0.01 * 6.751901626586914
Epoch 530, val loss: 0.8550292253494263
Epoch 540, training loss: 0.08427301794290543 = 0.01678234152495861 + 0.01 * 6.749067783355713
Epoch 540, val loss: 0.86324542760849
Epoch 550, training loss: 0.08314718306064606 = 0.015759777277708054 + 0.01 * 6.738740921020508
Epoch 550, val loss: 0.8712860345840454
Epoch 560, training loss: 0.08222714811563492 = 0.01483243890106678 + 0.01 * 6.739471435546875
Epoch 560, val loss: 0.8791655898094177
Epoch 570, training loss: 0.08123569935560226 = 0.01398823969066143 + 0.01 * 6.724746227264404
Epoch 570, val loss: 0.8868229389190674
Epoch 580, training loss: 0.0806790217757225 = 0.013216865248978138 + 0.01 * 6.746215343475342
Epoch 580, val loss: 0.8943406939506531
Epoch 590, training loss: 0.07960572838783264 = 0.012510768137872219 + 0.01 * 6.709496021270752
Epoch 590, val loss: 0.9016770124435425
Epoch 600, training loss: 0.07884424924850464 = 0.011863025836646557 + 0.01 * 6.698122978210449
Epoch 600, val loss: 0.9086223244667053
Epoch 610, training loss: 0.07816577702760696 = 0.011266338638961315 + 0.01 * 6.689944267272949
Epoch 610, val loss: 0.9156794548034668
Epoch 620, training loss: 0.07773931324481964 = 0.01071582455188036 + 0.01 * 6.702348709106445
Epoch 620, val loss: 0.9223834276199341
Epoch 630, training loss: 0.07706645131111145 = 0.010207126848399639 + 0.01 * 6.6859331130981445
Epoch 630, val loss: 0.929047167301178
Epoch 640, training loss: 0.07686853408813477 = 0.00973525457084179 + 0.01 * 6.713327884674072
Epoch 640, val loss: 0.9354981780052185
Epoch 650, training loss: 0.07599358260631561 = 0.009298747405409813 + 0.01 * 6.669483184814453
Epoch 650, val loss: 0.9418309330940247
Epoch 660, training loss: 0.07549931108951569 = 0.008893061429262161 + 0.01 * 6.660624980926514
Epoch 660, val loss: 0.948043167591095
Epoch 670, training loss: 0.07531407475471497 = 0.008514165878295898 + 0.01 * 6.679991245269775
Epoch 670, val loss: 0.954060971736908
Epoch 680, training loss: 0.07476677000522614 = 0.00816070381551981 + 0.01 * 6.660606861114502
Epoch 680, val loss: 0.9599236249923706
Epoch 690, training loss: 0.07426968216896057 = 0.007830796763300896 + 0.01 * 6.643888473510742
Epoch 690, val loss: 0.9656878709793091
Epoch 700, training loss: 0.074041448533535 = 0.007521696388721466 + 0.01 * 6.651975154876709
Epoch 700, val loss: 0.9713245630264282
Epoch 710, training loss: 0.07357437163591385 = 0.007232739590108395 + 0.01 * 6.6341633796691895
Epoch 710, val loss: 0.9767904877662659
Epoch 720, training loss: 0.07328367233276367 = 0.006961960345506668 + 0.01 * 6.632171154022217
Epoch 720, val loss: 0.9822221398353577
Epoch 730, training loss: 0.07302997261285782 = 0.006707047577947378 + 0.01 * 6.632292747497559
Epoch 730, val loss: 0.9874275326728821
Epoch 740, training loss: 0.07260998338460922 = 0.0064678313210606575 + 0.01 * 6.614214897155762
Epoch 740, val loss: 0.9926075339317322
Epoch 750, training loss: 0.07236531376838684 = 0.006241305731236935 + 0.01 * 6.612400531768799
Epoch 750, val loss: 0.9975311160087585
Epoch 760, training loss: 0.07211502641439438 = 0.0060286796651780605 + 0.01 * 6.6086344718933105
Epoch 760, val loss: 1.0026096105575562
Epoch 770, training loss: 0.07215172797441483 = 0.005827887915074825 + 0.01 * 6.632384300231934
Epoch 770, val loss: 1.007299780845642
Epoch 780, training loss: 0.0715840756893158 = 0.0056386603973805904 + 0.01 * 6.594542026519775
Epoch 780, val loss: 1.0120697021484375
Epoch 790, training loss: 0.07133932411670685 = 0.005459248553961515 + 0.01 * 6.58800745010376
Epoch 790, val loss: 1.0166467428207397
Epoch 800, training loss: 0.07127425819635391 = 0.005289207212626934 + 0.01 * 6.59850549697876
Epoch 800, val loss: 1.021175742149353
Epoch 810, training loss: 0.07101733982563019 = 0.005128576885908842 + 0.01 * 6.588876247406006
Epoch 810, val loss: 1.0257015228271484
Epoch 820, training loss: 0.07077285647392273 = 0.004975976422429085 + 0.01 * 6.57968807220459
Epoch 820, val loss: 1.0299850702285767
Epoch 830, training loss: 0.07058775424957275 = 0.004830857273191214 + 0.01 * 6.575689315795898
Epoch 830, val loss: 1.034132480621338
Epoch 840, training loss: 0.07043628394603729 = 0.004693085327744484 + 0.01 * 6.574319839477539
Epoch 840, val loss: 1.0383925437927246
Epoch 850, training loss: 0.07019996643066406 = 0.0045619928278028965 + 0.01 * 6.563797950744629
Epoch 850, val loss: 1.0424213409423828
Epoch 860, training loss: 0.07003682851791382 = 0.004437134135514498 + 0.01 * 6.559969902038574
Epoch 860, val loss: 1.0464868545532227
Epoch 870, training loss: 0.07021316885948181 = 0.004318394232541323 + 0.01 * 6.5894775390625
Epoch 870, val loss: 1.0504322052001953
Epoch 880, training loss: 0.0697844922542572 = 0.004204676952213049 + 0.01 * 6.557981967926025
Epoch 880, val loss: 1.0542259216308594
Epoch 890, training loss: 0.0697198212146759 = 0.00409645214676857 + 0.01 * 6.562336444854736
Epoch 890, val loss: 1.0579966306686401
Epoch 900, training loss: 0.06941160559654236 = 0.003993095830082893 + 0.01 * 6.541851043701172
Epoch 900, val loss: 1.061829686164856
Epoch 910, training loss: 0.0696382001042366 = 0.0038939733058214188 + 0.01 * 6.574423313140869
Epoch 910, val loss: 1.0653413534164429
Epoch 920, training loss: 0.0691756159067154 = 0.0037998040206730366 + 0.01 * 6.537581443786621
Epoch 920, val loss: 1.0690174102783203
Epoch 930, training loss: 0.06911527365446091 = 0.003709794022142887 + 0.01 * 6.540547847747803
Epoch 930, val loss: 1.0725038051605225
Epoch 940, training loss: 0.06903735548257828 = 0.0036234792787581682 + 0.01 * 6.541387557983398
Epoch 940, val loss: 1.0760059356689453
Epoch 950, training loss: 0.06885017454624176 = 0.0035408991388976574 + 0.01 * 6.530928134918213
Epoch 950, val loss: 1.0794059038162231
Epoch 960, training loss: 0.06869257986545563 = 0.0034618359059095383 + 0.01 * 6.523074150085449
Epoch 960, val loss: 1.0827604532241821
Epoch 970, training loss: 0.06876163184642792 = 0.003385992022231221 + 0.01 * 6.537564277648926
Epoch 970, val loss: 1.0860249996185303
Epoch 980, training loss: 0.06862157583236694 = 0.00331359775736928 + 0.01 * 6.530797958374023
Epoch 980, val loss: 1.0892999172210693
Epoch 990, training loss: 0.06853964179754257 = 0.0032438451889902353 + 0.01 * 6.5295796394348145
Epoch 990, val loss: 1.0924010276794434
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.7343
Flip ASR: 0.6933/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.028455972671509 = 1.9447169303894043 + 0.01 * 8.37389850616455
Epoch 0, val loss: 1.942571997642517
Epoch 10, training loss: 2.0175044536590576 = 1.9337658882141113 + 0.01 * 8.373863220214844
Epoch 10, val loss: 1.9311401844024658
Epoch 20, training loss: 2.004605293273926 = 1.9208682775497437 + 0.01 * 8.373708724975586
Epoch 20, val loss: 1.9176002740859985
Epoch 30, training loss: 1.986890196800232 = 1.903157353401184 + 0.01 * 8.373286247253418
Epoch 30, val loss: 1.8992942571640015
Epoch 40, training loss: 1.9613319635391235 = 1.8776158094406128 + 0.01 * 8.371620178222656
Epoch 40, val loss: 1.8736178874969482
Epoch 50, training loss: 1.925990104675293 = 1.8423888683319092 + 0.01 * 8.36011791229248
Epoch 50, val loss: 1.840083360671997
Epoch 60, training loss: 1.884516716003418 = 1.801590085029602 + 0.01 * 8.292667388916016
Epoch 60, val loss: 1.8049181699752808
Epoch 70, training loss: 1.8432706594467163 = 1.7631601095199585 + 0.01 * 8.0110502243042
Epoch 70, val loss: 1.773088812828064
Epoch 80, training loss: 1.7957179546356201 = 1.7177642583847046 + 0.01 * 7.795365810394287
Epoch 80, val loss: 1.730880856513977
Epoch 90, training loss: 1.7306245565414429 = 1.6561832427978516 + 0.01 * 7.444126129150391
Epoch 90, val loss: 1.674944519996643
Epoch 100, training loss: 1.6467716693878174 = 1.5749289989471436 + 0.01 * 7.184261322021484
Epoch 100, val loss: 1.605871558189392
Epoch 110, training loss: 1.5454922914505005 = 1.4748257398605347 + 0.01 * 7.066657543182373
Epoch 110, val loss: 1.5233442783355713
Epoch 120, training loss: 1.4317961931228638 = 1.3616974353790283 + 0.01 * 7.009881496429443
Epoch 120, val loss: 1.4301224946975708
Epoch 130, training loss: 1.312747597694397 = 1.2429070472717285 + 0.01 * 6.984057426452637
Epoch 130, val loss: 1.3326917886734009
Epoch 140, training loss: 1.1936676502227783 = 1.123961329460144 + 0.01 * 6.9706292152404785
Epoch 140, val loss: 1.2354273796081543
Epoch 150, training loss: 1.0785447359085083 = 1.0089219808578491 + 0.01 * 6.9622802734375
Epoch 150, val loss: 1.1419909000396729
Epoch 160, training loss: 0.9700154662132263 = 0.9004379510879517 + 0.01 * 6.95775032043457
Epoch 160, val loss: 1.0552836656570435
Epoch 170, training loss: 0.8695273399353027 = 0.7999687790870667 + 0.01 * 6.955854892730713
Epoch 170, val loss: 0.9758633375167847
Epoch 180, training loss: 0.7784039974212646 = 0.7088476419448853 + 0.01 * 6.955637454986572
Epoch 180, val loss: 0.9048482179641724
Epoch 190, training loss: 0.6980913281440735 = 0.6285368800163269 + 0.01 * 6.955443859100342
Epoch 190, val loss: 0.8440915942192078
Epoch 200, training loss: 0.628887414932251 = 0.559341311454773 + 0.01 * 6.954609394073486
Epoch 200, val loss: 0.7942243814468384
Epoch 210, training loss: 0.5693162679672241 = 0.49978676438331604 + 0.01 * 6.952951431274414
Epoch 210, val loss: 0.7542767524719238
Epoch 220, training loss: 0.516942024230957 = 0.4474331736564636 + 0.01 * 6.950888156890869
Epoch 220, val loss: 0.7222124934196472
Epoch 230, training loss: 0.46926355361938477 = 0.3997790217399597 + 0.01 * 6.948453426361084
Epoch 230, val loss: 0.6957316398620605
Epoch 240, training loss: 0.4244439899921417 = 0.35498037934303284 + 0.01 * 6.946361064910889
Epoch 240, val loss: 0.673151969909668
Epoch 250, training loss: 0.38168448209762573 = 0.31224173307418823 + 0.01 * 6.94427490234375
Epoch 250, val loss: 0.6532419323921204
Epoch 260, training loss: 0.34128740429878235 = 0.27186399698257446 + 0.01 * 6.942341327667236
Epoch 260, val loss: 0.635806679725647
Epoch 270, training loss: 0.3040716052055359 = 0.2346612513065338 + 0.01 * 6.941034317016602
Epoch 270, val loss: 0.6211065053939819
Epoch 280, training loss: 0.27087852358818054 = 0.20147566497325897 + 0.01 * 6.940286636352539
Epoch 280, val loss: 0.6095682382583618
Epoch 290, training loss: 0.24208736419677734 = 0.17270031571388245 + 0.01 * 6.938704013824463
Epoch 290, val loss: 0.6015212535858154
Epoch 300, training loss: 0.2176416516304016 = 0.14825719594955444 + 0.01 * 6.938445091247559
Epoch 300, val loss: 0.5971008539199829
Epoch 310, training loss: 0.19709837436676025 = 0.12772627174854279 + 0.01 * 6.937211513519287
Epoch 310, val loss: 0.596125066280365
Epoch 320, training loss: 0.17992477118968964 = 0.11056332290172577 + 0.01 * 6.936145305633545
Epoch 320, val loss: 0.5983318090438843
Epoch 330, training loss: 0.16556167602539062 = 0.09622540324926376 + 0.01 * 6.933627605438232
Epoch 330, val loss: 0.6031880974769592
Epoch 340, training loss: 0.15356238186359406 = 0.08422905951738358 + 0.01 * 6.933332443237305
Epoch 340, val loss: 0.6101645231246948
Epoch 350, training loss: 0.14342226088047028 = 0.07415042817592621 + 0.01 * 6.927183151245117
Epoch 350, val loss: 0.6187862157821655
Epoch 360, training loss: 0.1349031925201416 = 0.06564022600650787 + 0.01 * 6.926295757293701
Epoch 360, val loss: 0.6285935044288635
Epoch 370, training loss: 0.12762099504470825 = 0.0584183931350708 + 0.01 * 6.920259475708008
Epoch 370, val loss: 0.6392087936401367
Epoch 380, training loss: 0.1214662417769432 = 0.05225885659456253 + 0.01 * 6.920738697052002
Epoch 380, val loss: 0.6503252387046814
Epoch 390, training loss: 0.11605250835418701 = 0.04698033258318901 + 0.01 * 6.907217025756836
Epoch 390, val loss: 0.6616561412811279
Epoch 400, training loss: 0.11145590245723724 = 0.042432986199855804 + 0.01 * 6.902291774749756
Epoch 400, val loss: 0.6730936169624329
Epoch 410, training loss: 0.10742595791816711 = 0.03849425166845322 + 0.01 * 6.8931708335876465
Epoch 410, val loss: 0.6844594478607178
Epoch 420, training loss: 0.10398361831903458 = 0.03506384789943695 + 0.01 * 6.891976833343506
Epoch 420, val loss: 0.6957231163978577
Epoch 430, training loss: 0.1008501946926117 = 0.032060813158750534 + 0.01 * 6.878937721252441
Epoch 430, val loss: 0.7068019509315491
Epoch 440, training loss: 0.09805095940828323 = 0.02941993437707424 + 0.01 * 6.863102436065674
Epoch 440, val loss: 0.7176752686500549
Epoch 450, training loss: 0.09583908319473267 = 0.027086900547146797 + 0.01 * 6.875218868255615
Epoch 450, val loss: 0.7282911539077759
Epoch 460, training loss: 0.09345126152038574 = 0.025019561871886253 + 0.01 * 6.843169689178467
Epoch 460, val loss: 0.7386270761489868
Epoch 470, training loss: 0.09164046496152878 = 0.0231784600764513 + 0.01 * 6.846200942993164
Epoch 470, val loss: 0.7487197518348694
Epoch 480, training loss: 0.08978109061717987 = 0.02153341844677925 + 0.01 * 6.824767589569092
Epoch 480, val loss: 0.758549153804779
Epoch 490, training loss: 0.08813701570034027 = 0.020058700814843178 + 0.01 * 6.807831764221191
Epoch 490, val loss: 0.7681237459182739
Epoch 500, training loss: 0.08668835461139679 = 0.018732763826847076 + 0.01 * 6.795558929443359
Epoch 500, val loss: 0.7773969769477844
Epoch 510, training loss: 0.08549872785806656 = 0.017536276951432228 + 0.01 * 6.796245574951172
Epoch 510, val loss: 0.7864742279052734
Epoch 520, training loss: 0.08426244556903839 = 0.01645324006676674 + 0.01 * 6.780920028686523
Epoch 520, val loss: 0.7953037619590759
Epoch 530, training loss: 0.08310143649578094 = 0.015469249337911606 + 0.01 * 6.763219356536865
Epoch 530, val loss: 0.8038429617881775
Epoch 540, training loss: 0.08315565437078476 = 0.014572187326848507 + 0.01 * 6.858346462249756
Epoch 540, val loss: 0.812373161315918
Epoch 550, training loss: 0.08129075169563293 = 0.013756432570517063 + 0.01 * 6.753431797027588
Epoch 550, val loss: 0.8205126523971558
Epoch 560, training loss: 0.08037621527910233 = 0.013010452501475811 + 0.01 * 6.736576557159424
Epoch 560, val loss: 0.8285099267959595
Epoch 570, training loss: 0.0795695036649704 = 0.012324863113462925 + 0.01 * 6.724463939666748
Epoch 570, val loss: 0.8362284302711487
Epoch 580, training loss: 0.07887089252471924 = 0.011695300228893757 + 0.01 * 6.717559337615967
Epoch 580, val loss: 0.8438474535942078
Epoch 590, training loss: 0.07829216122627258 = 0.011115377768874168 + 0.01 * 6.717678070068359
Epoch 590, val loss: 0.8512060046195984
Epoch 600, training loss: 0.07761542499065399 = 0.010580502450466156 + 0.01 * 6.703492641448975
Epoch 600, val loss: 0.8583728671073914
Epoch 610, training loss: 0.07747096568346024 = 0.010085692629218102 + 0.01 * 6.738527297973633
Epoch 610, val loss: 0.8654253482818604
Epoch 620, training loss: 0.07645128667354584 = 0.00962743442505598 + 0.01 * 6.6823859214782715
Epoch 620, val loss: 0.8721988797187805
Epoch 630, training loss: 0.07674740254878998 = 0.009201767854392529 + 0.01 * 6.754563808441162
Epoch 630, val loss: 0.8788484334945679
Epoch 640, training loss: 0.07546871900558472 = 0.00880681723356247 + 0.01 * 6.666190147399902
Epoch 640, val loss: 0.8852892518043518
Epoch 650, training loss: 0.07483754307031631 = 0.008438811637461185 + 0.01 * 6.639873027801514
Epoch 650, val loss: 0.8916058540344238
Epoch 660, training loss: 0.07465668022632599 = 0.00809510238468647 + 0.01 * 6.656158447265625
Epoch 660, val loss: 0.8978438973426819
Epoch 670, training loss: 0.07424496859312057 = 0.00777415232732892 + 0.01 * 6.6470818519592285
Epoch 670, val loss: 0.9038631319999695
Epoch 680, training loss: 0.07384468615055084 = 0.007473573554307222 + 0.01 * 6.637111663818359
Epoch 680, val loss: 0.9098831415176392
Epoch 690, training loss: 0.0733942985534668 = 0.0071916054002940655 + 0.01 * 6.620269298553467
Epoch 690, val loss: 0.9156577587127686
Epoch 700, training loss: 0.07329154759645462 = 0.006927380803972483 + 0.01 * 6.636417388916016
Epoch 700, val loss: 0.9213201999664307
Epoch 710, training loss: 0.07268660515546799 = 0.0066788047552108765 + 0.01 * 6.600780010223389
Epoch 710, val loss: 0.9268554449081421
Epoch 720, training loss: 0.07254544645547867 = 0.006444866769015789 + 0.01 * 6.610058307647705
Epoch 720, val loss: 0.9323171973228455
Epoch 730, training loss: 0.07205544412136078 = 0.006224504206329584 + 0.01 * 6.583093643188477
Epoch 730, val loss: 0.9376323223114014
Epoch 740, training loss: 0.07185626029968262 = 0.0060165454633533955 + 0.01 * 6.583971977233887
Epoch 740, val loss: 0.9428467750549316
Epoch 750, training loss: 0.07175657153129578 = 0.005820353515446186 + 0.01 * 6.593621730804443
Epoch 750, val loss: 0.9478766322135925
Epoch 760, training loss: 0.07145567983388901 = 0.005634693428874016 + 0.01 * 6.582098484039307
Epoch 760, val loss: 0.9529257416725159
Epoch 770, training loss: 0.07103220373392105 = 0.005459010601043701 + 0.01 * 6.557319164276123
Epoch 770, val loss: 0.9578148722648621
Epoch 780, training loss: 0.07100458443164825 = 0.005292749963700771 + 0.01 * 6.571183681488037
Epoch 780, val loss: 0.9625325798988342
Epoch 790, training loss: 0.0707329511642456 = 0.005134947132319212 + 0.01 * 6.559800624847412
Epoch 790, val loss: 0.9672682285308838
Epoch 800, training loss: 0.07061037421226501 = 0.004984994884580374 + 0.01 * 6.562538146972656
Epoch 800, val loss: 0.9718250632286072
Epoch 810, training loss: 0.07044849544763565 = 0.0048424615524709225 + 0.01 * 6.560603618621826
Epoch 810, val loss: 0.9763516783714294
Epoch 820, training loss: 0.07026540488004684 = 0.004707164131104946 + 0.01 * 6.555823802947998
Epoch 820, val loss: 0.9807142615318298
Epoch 830, training loss: 0.06990864127874374 = 0.0045783245004713535 + 0.01 * 6.533031940460205
Epoch 830, val loss: 0.9850559830665588
Epoch 840, training loss: 0.07000984251499176 = 0.004455499351024628 + 0.01 * 6.555434226989746
Epoch 840, val loss: 0.989318311214447
Epoch 850, training loss: 0.06963927298784256 = 0.004338607657700777 + 0.01 * 6.53006649017334
Epoch 850, val loss: 0.9933881759643555
Epoch 860, training loss: 0.06973681598901749 = 0.004226903431117535 + 0.01 * 6.550992012023926
Epoch 860, val loss: 0.9974436163902283
Epoch 870, training loss: 0.06935673952102661 = 0.004120392259210348 + 0.01 * 6.523634910583496
Epoch 870, val loss: 1.0015203952789307
Epoch 880, training loss: 0.06939031928777695 = 0.004018481355160475 + 0.01 * 6.537184238433838
Epoch 880, val loss: 1.0053491592407227
Epoch 890, training loss: 0.06914258748292923 = 0.003920990042388439 + 0.01 * 6.522160530090332
Epoch 890, val loss: 1.0092159509658813
Epoch 900, training loss: 0.06925193965435028 = 0.003827654756605625 + 0.01 * 6.542428493499756
Epoch 900, val loss: 1.013001799583435
Epoch 910, training loss: 0.06899161636829376 = 0.0037383949384093285 + 0.01 * 6.525321960449219
Epoch 910, val loss: 1.0166840553283691
Epoch 920, training loss: 0.06865311414003372 = 0.0036527940537780523 + 0.01 * 6.5000319480896
Epoch 920, val loss: 1.0203536748886108
Epoch 930, training loss: 0.06862395256757736 = 0.003570747096091509 + 0.01 * 6.505320072174072
Epoch 930, val loss: 1.0239977836608887
Epoch 940, training loss: 0.06847681105136871 = 0.0034919995814561844 + 0.01 * 6.498481273651123
Epoch 940, val loss: 1.0275039672851562
Epoch 950, training loss: 0.06828963756561279 = 0.0034162465017288923 + 0.01 * 6.487339019775391
Epoch 950, val loss: 1.0309098958969116
Epoch 960, training loss: 0.0683244913816452 = 0.003343663178384304 + 0.01 * 6.498083114624023
Epoch 960, val loss: 1.0343908071517944
Epoch 970, training loss: 0.06797803938388824 = 0.003273791167885065 + 0.01 * 6.470424652099609
Epoch 970, val loss: 1.0376944541931152
Epoch 980, training loss: 0.0680035948753357 = 0.0032065894920378923 + 0.01 * 6.479700565338135
Epoch 980, val loss: 1.0411036014556885
Epoch 990, training loss: 0.06800321489572525 = 0.003141935681924224 + 0.01 * 6.486128330230713
Epoch 990, val loss: 1.0442595481872559
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.71464, 0.22339, Accuracy:0.81481, 0.01684
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11628])
remove edge: torch.Size([2, 9454])
updated graph: torch.Size([2, 10526])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
The final ASR:0.97417, 0.00301, Accuracy:0.83210, 0.00175
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0400028228759766 = 1.9562643766403198 + 0.01 * 8.37384033203125
Epoch 0, val loss: 1.9577202796936035
Epoch 10, training loss: 2.029372215270996 = 1.9456348419189453 + 0.01 * 8.37374496459961
Epoch 10, val loss: 1.9468815326690674
Epoch 20, training loss: 2.0159497261047363 = 1.932215690612793 + 0.01 * 8.373401641845703
Epoch 20, val loss: 1.9330590963363647
Epoch 30, training loss: 1.9963995218276978 = 1.9126776456832886 + 0.01 * 8.372191429138184
Epoch 30, val loss: 1.9128460884094238
Epoch 40, training loss: 1.9668793678283691 = 1.8832368850708008 + 0.01 * 8.364245414733887
Epoch 40, val loss: 1.882873773574829
Epoch 50, training loss: 1.9257416725158691 = 1.8426834344863892 + 0.01 * 8.305824279785156
Epoch 50, val loss: 1.8439509868621826
Epoch 60, training loss: 1.87971830368042 = 1.7994637489318848 + 0.01 * 8.025453567504883
Epoch 60, val loss: 1.8078923225402832
Epoch 70, training loss: 1.8405948877334595 = 1.762297511100769 + 0.01 * 7.8297343254089355
Epoch 70, val loss: 1.7800285816192627
Epoch 80, training loss: 1.7895663976669312 = 1.7145401239395142 + 0.01 * 7.502623558044434
Epoch 80, val loss: 1.7392698526382446
Epoch 90, training loss: 1.7213785648345947 = 1.6491035223007202 + 0.01 * 7.227504253387451
Epoch 90, val loss: 1.6826633214950562
Epoch 100, training loss: 1.6341108083724976 = 1.563124418258667 + 0.01 * 7.098644733428955
Epoch 100, val loss: 1.6106804609298706
Epoch 110, training loss: 1.5358116626739502 = 1.4655545949935913 + 0.01 * 7.025704383850098
Epoch 110, val loss: 1.531537413597107
Epoch 120, training loss: 1.4376070499420166 = 1.3678516149520874 + 0.01 * 6.975542068481445
Epoch 120, val loss: 1.454740285873413
Epoch 130, training loss: 1.3423707485198975 = 1.2728854417800903 + 0.01 * 6.948535442352295
Epoch 130, val loss: 1.3816657066345215
Epoch 140, training loss: 1.2483092546463013 = 1.1790038347244263 + 0.01 * 6.930541038513184
Epoch 140, val loss: 1.309808611869812
Epoch 150, training loss: 1.156339168548584 = 1.0871833562850952 + 0.01 * 6.915576934814453
Epoch 150, val loss: 1.2399969100952148
Epoch 160, training loss: 1.0696375370025635 = 1.0005539655685425 + 0.01 * 6.908359050750732
Epoch 160, val loss: 1.1754047870635986
Epoch 170, training loss: 0.9895817041397095 = 0.9206050634384155 + 0.01 * 6.897662162780762
Epoch 170, val loss: 1.1172711849212646
Epoch 180, training loss: 0.9153211712837219 = 0.8464128375053406 + 0.01 * 6.890832424163818
Epoch 180, val loss: 1.0638278722763062
Epoch 190, training loss: 0.8458939790725708 = 0.7770513296127319 + 0.01 * 6.884268283843994
Epoch 190, val loss: 1.0144696235656738
Epoch 200, training loss: 0.7814292311668396 = 0.7126604914665222 + 0.01 * 6.876873970031738
Epoch 200, val loss: 0.9698941707611084
Epoch 210, training loss: 0.7222428321838379 = 0.6535404920578003 + 0.01 * 6.870233535766602
Epoch 210, val loss: 0.9304686188697815
Epoch 220, training loss: 0.668024480342865 = 0.5993892550468445 + 0.01 * 6.863521575927734
Epoch 220, val loss: 0.8964630961418152
Epoch 230, training loss: 0.6180442571640015 = 0.5494847893714905 + 0.01 * 6.855946063995361
Epoch 230, val loss: 0.8676279783248901
Epoch 240, training loss: 0.5716585516929626 = 0.5031461119651794 + 0.01 * 6.851245880126953
Epoch 240, val loss: 0.8440320491790771
Epoch 250, training loss: 0.52777498960495 = 0.4593961834907532 + 0.01 * 6.8378825187683105
Epoch 250, val loss: 0.8247350454330444
Epoch 260, training loss: 0.48532626032829285 = 0.41705095767974854 + 0.01 * 6.8275299072265625
Epoch 260, val loss: 0.8085123896598816
Epoch 270, training loss: 0.44323021173477173 = 0.374986857175827 + 0.01 * 6.824334621429443
Epoch 270, val loss: 0.7940930128097534
Epoch 280, training loss: 0.40076351165771484 = 0.3326259255409241 + 0.01 * 6.813756942749023
Epoch 280, val loss: 0.7809209823608398
Epoch 290, training loss: 0.3584733009338379 = 0.29040729999542236 + 0.01 * 6.8065996170043945
Epoch 290, val loss: 0.7689569592475891
Epoch 300, training loss: 0.3176848888397217 = 0.2497054487466812 + 0.01 * 6.797945499420166
Epoch 300, val loss: 0.7589313387870789
Epoch 310, training loss: 0.2802603840827942 = 0.21225494146347046 + 0.01 * 6.800546169281006
Epoch 310, val loss: 0.7515827417373657
Epoch 320, training loss: 0.24731972813606262 = 0.1794225573539734 + 0.01 * 6.789717674255371
Epoch 320, val loss: 0.7476063370704651
Epoch 330, training loss: 0.2195807695388794 = 0.15177035331726074 + 0.01 * 6.781041622161865
Epoch 330, val loss: 0.7473949193954468
Epoch 340, training loss: 0.19685201346874237 = 0.12909378111362457 + 0.01 * 6.77582311630249
Epoch 340, val loss: 0.7508031129837036
Epoch 350, training loss: 0.17849862575531006 = 0.11075179278850555 + 0.01 * 6.774682998657227
Epoch 350, val loss: 0.7573933601379395
Epoch 360, training loss: 0.16351285576820374 = 0.09589270502328873 + 0.01 * 6.762014389038086
Epoch 360, val loss: 0.7663994431495667
Epoch 370, training loss: 0.151452898979187 = 0.0837755799293518 + 0.01 * 6.767731189727783
Epoch 370, val loss: 0.7771472930908203
Epoch 380, training loss: 0.14131587743759155 = 0.07379884272813797 + 0.01 * 6.751703262329102
Epoch 380, val loss: 0.7890586256980896
Epoch 390, training loss: 0.13289020955562592 = 0.0654827207326889 + 0.01 * 6.740748882293701
Epoch 390, val loss: 0.8018531799316406
Epoch 400, training loss: 0.12584823369979858 = 0.05846840888261795 + 0.01 * 6.737982273101807
Epoch 400, val loss: 0.8151425123214722
Epoch 410, training loss: 0.11999417841434479 = 0.052487555891275406 + 0.01 * 6.750662326812744
Epoch 410, val loss: 0.8286557793617249
Epoch 420, training loss: 0.11456620693206787 = 0.04734372720122337 + 0.01 * 6.72224760055542
Epoch 420, val loss: 0.8422557711601257
Epoch 430, training loss: 0.11007091403007507 = 0.04288218915462494 + 0.01 * 6.718872547149658
Epoch 430, val loss: 0.8559347987174988
Epoch 440, training loss: 0.10642532259225845 = 0.03898679465055466 + 0.01 * 6.7438530921936035
Epoch 440, val loss: 0.8694177865982056
Epoch 450, training loss: 0.1026204451918602 = 0.03557306528091431 + 0.01 * 6.704738140106201
Epoch 450, val loss: 0.8827939629554749
Epoch 460, training loss: 0.09954278916120529 = 0.032562822103500366 + 0.01 * 6.697996616363525
Epoch 460, val loss: 0.8959429264068604
Epoch 470, training loss: 0.09693896025419235 = 0.029896296560764313 + 0.01 * 6.704266548156738
Epoch 470, val loss: 0.9088807702064514
Epoch 480, training loss: 0.09445661306381226 = 0.027527211233973503 + 0.01 * 6.6929402351379395
Epoch 480, val loss: 0.9215317368507385
Epoch 490, training loss: 0.09224288910627365 = 0.025415688753128052 + 0.01 * 6.682720184326172
Epoch 490, val loss: 0.9339739084243774
Epoch 500, training loss: 0.090287946164608 = 0.023528026416897774 + 0.01 * 6.675992012023926
Epoch 500, val loss: 0.9460471272468567
Epoch 510, training loss: 0.08853280544281006 = 0.021834732964634895 + 0.01 * 6.6698079109191895
Epoch 510, val loss: 0.957811713218689
Epoch 520, training loss: 0.08712055534124374 = 0.020312821492552757 + 0.01 * 6.6807732582092285
Epoch 520, val loss: 0.9692904949188232
Epoch 530, training loss: 0.08557690680027008 = 0.018942076712846756 + 0.01 * 6.663483142852783
Epoch 530, val loss: 0.9804978966712952
Epoch 540, training loss: 0.08430692553520203 = 0.01770235225558281 + 0.01 * 6.660457134246826
Epoch 540, val loss: 0.991429328918457
Epoch 550, training loss: 0.08313971757888794 = 0.01657792553305626 + 0.01 * 6.656179428100586
Epoch 550, val loss: 1.0020025968551636
Epoch 560, training loss: 0.08206286281347275 = 0.015555092133581638 + 0.01 * 6.650777339935303
Epoch 560, val loss: 1.0123729705810547
Epoch 570, training loss: 0.08099573850631714 = 0.014621182344853878 + 0.01 * 6.637455940246582
Epoch 570, val loss: 1.0224997997283936
Epoch 580, training loss: 0.08023993670940399 = 0.01376551017165184 + 0.01 * 6.6474432945251465
Epoch 580, val loss: 1.0322526693344116
Epoch 590, training loss: 0.07922281324863434 = 0.012980548664927483 + 0.01 * 6.6242265701293945
Epoch 590, val loss: 1.041885495185852
Epoch 600, training loss: 0.07885038107633591 = 0.012258083559572697 + 0.01 * 6.659229755401611
Epoch 600, val loss: 1.0512816905975342
Epoch 610, training loss: 0.07782137393951416 = 0.011593339964747429 + 0.01 * 6.622803211212158
Epoch 610, val loss: 1.060576319694519
Epoch 620, training loss: 0.07715047150850296 = 0.0109803956001997 + 0.01 * 6.617007732391357
Epoch 620, val loss: 1.0694392919540405
Epoch 630, training loss: 0.07648203521966934 = 0.010415156371891499 + 0.01 * 6.606687545776367
Epoch 630, val loss: 1.0782431364059448
Epoch 640, training loss: 0.07599525153636932 = 0.009894285351037979 + 0.01 * 6.610097408294678
Epoch 640, val loss: 1.0866796970367432
Epoch 650, training loss: 0.07563170790672302 = 0.009413151070475578 + 0.01 * 6.621855735778809
Epoch 650, val loss: 1.095008373260498
Epoch 660, training loss: 0.07491101324558258 = 0.008967735804617405 + 0.01 * 6.594327926635742
Epoch 660, val loss: 1.1031811237335205
Epoch 670, training loss: 0.07467791438102722 = 0.008554760366678238 + 0.01 * 6.612316131591797
Epoch 670, val loss: 1.1109918355941772
Epoch 680, training loss: 0.07420720905065536 = 0.008171969093382359 + 0.01 * 6.603524208068848
Epoch 680, val loss: 1.1187492609024048
Epoch 690, training loss: 0.07364005595445633 = 0.007816078141331673 + 0.01 * 6.582397937774658
Epoch 690, val loss: 1.1262545585632324
Epoch 700, training loss: 0.07344778627157211 = 0.007484790403395891 + 0.01 * 6.596299171447754
Epoch 700, val loss: 1.1334785223007202
Epoch 710, training loss: 0.07302436232566833 = 0.007176076527684927 + 0.01 * 6.584828853607178
Epoch 710, val loss: 1.1406718492507935
Epoch 720, training loss: 0.07296057790517807 = 0.006887633819133043 + 0.01 * 6.60729455947876
Epoch 720, val loss: 1.1475470066070557
Epoch 730, training loss: 0.0722714215517044 = 0.0066187321208417416 + 0.01 * 6.565269470214844
Epoch 730, val loss: 1.1544078588485718
Epoch 740, training loss: 0.07189382612705231 = 0.006366932298988104 + 0.01 * 6.552690029144287
Epoch 740, val loss: 1.1608895063400269
Epoch 750, training loss: 0.07184790074825287 = 0.006130697205662727 + 0.01 * 6.571720600128174
Epoch 750, val loss: 1.1673839092254639
Epoch 760, training loss: 0.07148988544940948 = 0.005909164436161518 + 0.01 * 6.558071613311768
Epoch 760, val loss: 1.1736621856689453
Epoch 770, training loss: 0.07145385444164276 = 0.005700735375285149 + 0.01 * 6.575311660766602
Epoch 770, val loss: 1.1797908544540405
Epoch 780, training loss: 0.07110261917114258 = 0.005505098029971123 + 0.01 * 6.559751987457275
Epoch 780, val loss: 1.185763955116272
Epoch 790, training loss: 0.07074489444494247 = 0.005320663098245859 + 0.01 * 6.542423725128174
Epoch 790, val loss: 1.1915888786315918
Epoch 800, training loss: 0.07058433443307877 = 0.005146321374922991 + 0.01 * 6.543801307678223
Epoch 800, val loss: 1.1972949504852295
Epoch 810, training loss: 0.07024011760950089 = 0.004981765057891607 + 0.01 * 6.5258355140686035
Epoch 810, val loss: 1.2028263807296753
Epoch 820, training loss: 0.07016131281852722 = 0.004826302640140057 + 0.01 * 6.533501148223877
Epoch 820, val loss: 1.2082629203796387
Epoch 830, training loss: 0.06982341408729553 = 0.004679241217672825 + 0.01 * 6.5144171714782715
Epoch 830, val loss: 1.2135405540466309
Epoch 840, training loss: 0.06983016431331635 = 0.004540063440799713 + 0.01 * 6.52901029586792
Epoch 840, val loss: 1.218776822090149
Epoch 850, training loss: 0.0695309042930603 = 0.004408021457493305 + 0.01 * 6.5122880935668945
Epoch 850, val loss: 1.2238311767578125
Epoch 860, training loss: 0.06982453912496567 = 0.004282680805772543 + 0.01 * 6.554185390472412
Epoch 860, val loss: 1.2288614511489868
Epoch 870, training loss: 0.06911309063434601 = 0.004163602367043495 + 0.01 * 6.494948863983154
Epoch 870, val loss: 1.2337502241134644
Epoch 880, training loss: 0.06913380324840546 = 0.004050328861922026 + 0.01 * 6.508347511291504
Epoch 880, val loss: 1.2385368347167969
Epoch 890, training loss: 0.06930161267518997 = 0.003942464012652636 + 0.01 * 6.535914897918701
Epoch 890, val loss: 1.2432305812835693
Epoch 900, training loss: 0.06865628063678741 = 0.003840032499283552 + 0.01 * 6.481625080108643
Epoch 900, val loss: 1.2478762865066528
Epoch 910, training loss: 0.06865430623292923 = 0.0037424734327942133 + 0.01 * 6.491183280944824
Epoch 910, val loss: 1.2522741556167603
Epoch 920, training loss: 0.06884513050317764 = 0.0036492820363491774 + 0.01 * 6.519585132598877
Epoch 920, val loss: 1.256915807723999
Epoch 930, training loss: 0.06842172890901566 = 0.0035602948628365993 + 0.01 * 6.486143112182617
Epoch 930, val loss: 1.2611639499664307
Epoch 940, training loss: 0.06828136742115021 = 0.0034752520732581615 + 0.01 * 6.480611801147461
Epoch 940, val loss: 1.2655396461486816
Epoch 950, training loss: 0.0681481808423996 = 0.00339388195425272 + 0.01 * 6.475430488586426
Epoch 950, val loss: 1.269606351852417
Epoch 960, training loss: 0.0684330016374588 = 0.0033160587772727013 + 0.01 * 6.511694431304932
Epoch 960, val loss: 1.273747444152832
Epoch 970, training loss: 0.06786713749170303 = 0.003241497091948986 + 0.01 * 6.462564468383789
Epoch 970, val loss: 1.2777808904647827
Epoch 980, training loss: 0.06785857677459717 = 0.003170089330524206 + 0.01 * 6.468848705291748
Epoch 980, val loss: 1.2817450761795044
Epoch 990, training loss: 0.06787934899330139 = 0.0031017076689749956 + 0.01 * 6.477764129638672
Epoch 990, val loss: 1.2855855226516724
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.5166
Flip ASR: 0.4356/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.03395938873291 = 1.950221061706543 + 0.01 * 8.3738374710083
Epoch 0, val loss: 1.943841576576233
Epoch 10, training loss: 2.0244925022125244 = 1.940754771232605 + 0.01 * 8.373770713806152
Epoch 10, val loss: 1.9348065853118896
Epoch 20, training loss: 2.013295888900757 = 1.929560899734497 + 0.01 * 8.373494148254395
Epoch 20, val loss: 1.9237549304962158
Epoch 30, training loss: 1.9981155395507812 = 1.9143880605697632 + 0.01 * 8.372754096984863
Epoch 30, val loss: 1.9084820747375488
Epoch 40, training loss: 1.9757624864578247 = 1.892067313194275 + 0.01 * 8.369513511657715
Epoch 40, val loss: 1.8859635591506958
Epoch 50, training loss: 1.9424185752868652 = 1.858953833580017 + 0.01 * 8.34647274017334
Epoch 50, val loss: 1.8532553911209106
Epoch 60, training loss: 1.8971657752990723 = 1.8151154518127441 + 0.01 * 8.205035209655762
Epoch 60, val loss: 1.8125395774841309
Epoch 70, training loss: 1.8454434871673584 = 1.7686904668807983 + 0.01 * 7.675299644470215
Epoch 70, val loss: 1.7731984853744507
Epoch 80, training loss: 1.793623685836792 = 1.7191135883331299 + 0.01 * 7.451005935668945
Epoch 80, val loss: 1.7305755615234375
Epoch 90, training loss: 1.725301742553711 = 1.652409315109253 + 0.01 * 7.289247989654541
Epoch 90, val loss: 1.6716290712356567
Epoch 100, training loss: 1.635932445526123 = 1.5638294219970703 + 0.01 * 7.210302829742432
Epoch 100, val loss: 1.5958784818649292
Epoch 110, training loss: 1.5255059003829956 = 1.453952431678772 + 0.01 * 7.1553521156311035
Epoch 110, val loss: 1.5045418739318848
Epoch 120, training loss: 1.4036552906036377 = 1.3326637744903564 + 0.01 * 7.099146842956543
Epoch 120, val loss: 1.404997706413269
Epoch 130, training loss: 1.282559871673584 = 1.2120810747146606 + 0.01 * 7.047876358032227
Epoch 130, val loss: 1.30939519405365
Epoch 140, training loss: 1.173247218132019 = 1.1030443906784058 + 0.01 * 7.020279407501221
Epoch 140, val loss: 1.2257322072982788
Epoch 150, training loss: 1.0789395570755005 = 1.0088759660720825 + 0.01 * 7.0063605308532715
Epoch 150, val loss: 1.1551077365875244
Epoch 160, training loss: 0.9967354536056519 = 0.9268505573272705 + 0.01 * 6.988491058349609
Epoch 160, val loss: 1.0945593118667603
Epoch 170, training loss: 0.9225579500198364 = 0.8529031276702881 + 0.01 * 6.965484619140625
Epoch 170, val loss: 1.0405466556549072
Epoch 180, training loss: 0.8530764579772949 = 0.7837072610855103 + 0.01 * 6.936920166015625
Epoch 180, val loss: 0.9906825423240662
Epoch 190, training loss: 0.7866975665092468 = 0.7176153063774109 + 0.01 * 6.908227443695068
Epoch 190, val loss: 0.9444215297698975
Epoch 200, training loss: 0.7234570384025574 = 0.6546004414558411 + 0.01 * 6.8856587409973145
Epoch 200, val loss: 0.9029584527015686
Epoch 210, training loss: 0.6639639735221863 = 0.5952765941619873 + 0.01 * 6.868738174438477
Epoch 210, val loss: 0.867877721786499
Epoch 220, training loss: 0.6087648272514343 = 0.5402130484580994 + 0.01 * 6.855175495147705
Epoch 220, val loss: 0.8396630883216858
Epoch 230, training loss: 0.558097243309021 = 0.4896484911441803 + 0.01 * 6.844877243041992
Epoch 230, val loss: 0.8175058960914612
Epoch 240, training loss: 0.5119677782058716 = 0.4435626268386841 + 0.01 * 6.840517520904541
Epoch 240, val loss: 0.7998105883598328
Epoch 250, training loss: 0.46973055601119995 = 0.40142518281936646 + 0.01 * 6.830538749694824
Epoch 250, val loss: 0.7848638892173767
Epoch 260, training loss: 0.43060779571533203 = 0.3623654842376709 + 0.01 * 6.824230670928955
Epoch 260, val loss: 0.7712997794151306
Epoch 270, training loss: 0.39384937286376953 = 0.32565921545028687 + 0.01 * 6.819016933441162
Epoch 270, val loss: 0.7591652870178223
Epoch 280, training loss: 0.35891950130462646 = 0.2907697558403015 + 0.01 * 6.814975738525391
Epoch 280, val loss: 0.7489280104637146
Epoch 290, training loss: 0.3255577087402344 = 0.25744515657424927 + 0.01 * 6.8112568855285645
Epoch 290, val loss: 0.7412620186805725
Epoch 300, training loss: 0.29375576972961426 = 0.22568286955356598 + 0.01 * 6.807291030883789
Epoch 300, val loss: 0.7361360788345337
Epoch 310, training loss: 0.26414674520492554 = 0.19610227644443512 + 0.01 * 6.804446220397949
Epoch 310, val loss: 0.7343431711196899
Epoch 320, training loss: 0.23745660483837128 = 0.16942884027957916 + 0.01 * 6.802776336669922
Epoch 320, val loss: 0.7360690236091614
Epoch 330, training loss: 0.21409207582473755 = 0.14608360826969147 + 0.01 * 6.80084753036499
Epoch 330, val loss: 0.7408373355865479
Epoch 340, training loss: 0.19401389360427856 = 0.12604989111423492 + 0.01 * 6.796400547027588
Epoch 340, val loss: 0.7486320734024048
Epoch 350, training loss: 0.17700204253196716 = 0.1090792566537857 + 0.01 * 6.7922797203063965
Epoch 350, val loss: 0.7590423226356506
Epoch 360, training loss: 0.16282156109809875 = 0.09479428082704544 + 0.01 * 6.802727699279785
Epoch 360, val loss: 0.7713763117790222
Epoch 370, training loss: 0.1506618857383728 = 0.08279283344745636 + 0.01 * 6.786904335021973
Epoch 370, val loss: 0.785253643989563
Epoch 380, training loss: 0.14052368700504303 = 0.07269667834043503 + 0.01 * 6.782701015472412
Epoch 380, val loss: 0.8004189729690552
Epoch 390, training loss: 0.13190393149852753 = 0.06413429230451584 + 0.01 * 6.77696418762207
Epoch 390, val loss: 0.8162841200828552
Epoch 400, training loss: 0.12463520467281342 = 0.056840185075998306 + 0.01 * 6.7795023918151855
Epoch 400, val loss: 0.832615077495575
Epoch 410, training loss: 0.11833804845809937 = 0.050608303397893906 + 0.01 * 6.772974014282227
Epoch 410, val loss: 0.8490070104598999
Epoch 420, training loss: 0.1128973439335823 = 0.045261695981025696 + 0.01 * 6.763564586639404
Epoch 420, val loss: 0.86524498462677
Epoch 430, training loss: 0.10822031646966934 = 0.04065065085887909 + 0.01 * 6.756967067718506
Epoch 430, val loss: 0.88118577003479
Epoch 440, training loss: 0.10417269170284271 = 0.03665786609053612 + 0.01 * 6.75148344039917
Epoch 440, val loss: 0.89673912525177
Epoch 450, training loss: 0.1008211001753807 = 0.03319377452135086 + 0.01 * 6.76273250579834
Epoch 450, val loss: 0.9118939638137817
Epoch 460, training loss: 0.09761016070842743 = 0.03017868846654892 + 0.01 * 6.743147373199463
Epoch 460, val loss: 0.9264781475067139
Epoch 470, training loss: 0.09490019083023071 = 0.027542227879166603 + 0.01 * 6.735796928405762
Epoch 470, val loss: 0.940613329410553
Epoch 480, training loss: 0.09251215308904648 = 0.02522718720138073 + 0.01 * 6.728496551513672
Epoch 480, val loss: 0.9543015956878662
Epoch 490, training loss: 0.09040911495685577 = 0.023186970502138138 + 0.01 * 6.722215175628662
Epoch 490, val loss: 0.9676193594932556
Epoch 500, training loss: 0.08859475702047348 = 0.021381698548793793 + 0.01 * 6.721305847167969
Epoch 500, val loss: 0.9805160164833069
Epoch 510, training loss: 0.08687889575958252 = 0.019775694236159325 + 0.01 * 6.710320472717285
Epoch 510, val loss: 0.9930946826934814
Epoch 520, training loss: 0.0854010134935379 = 0.018341688439249992 + 0.01 * 6.705932140350342
Epoch 520, val loss: 1.005220890045166
Epoch 530, training loss: 0.08410301804542542 = 0.017055390402674675 + 0.01 * 6.7047624588012695
Epoch 530, val loss: 1.0170613527297974
Epoch 540, training loss: 0.08281371742486954 = 0.015893621370196342 + 0.01 * 6.692009925842285
Epoch 540, val loss: 1.0286563634872437
Epoch 550, training loss: 0.08173215389251709 = 0.014840681105852127 + 0.01 * 6.689147472381592
Epoch 550, val loss: 1.0399787425994873
Epoch 560, training loss: 0.08076217770576477 = 0.01388360932469368 + 0.01 * 6.687856674194336
Epoch 560, val loss: 1.051047682762146
Epoch 570, training loss: 0.07986599206924438 = 0.013013218529522419 + 0.01 * 6.685276985168457
Epoch 570, val loss: 1.0618141889572144
Epoch 580, training loss: 0.07895760983228683 = 0.012220154516398907 + 0.01 * 6.673746109008789
Epoch 580, val loss: 1.0723631381988525
Epoch 590, training loss: 0.0782303661108017 = 0.011496268212795258 + 0.01 * 6.673409938812256
Epoch 590, val loss: 1.082729697227478
Epoch 600, training loss: 0.07753624022006989 = 0.010834247805178165 + 0.01 * 6.670199394226074
Epoch 600, val loss: 1.092719554901123
Epoch 610, training loss: 0.0768885388970375 = 0.010228230617940426 + 0.01 * 6.6660308837890625
Epoch 610, val loss: 1.102379560470581
Epoch 620, training loss: 0.07622118294239044 = 0.009671085514128208 + 0.01 * 6.655009746551514
Epoch 620, val loss: 1.111987829208374
Epoch 630, training loss: 0.0757228210568428 = 0.009158358909189701 + 0.01 * 6.65644645690918
Epoch 630, val loss: 1.1212668418884277
Epoch 640, training loss: 0.07513464987277985 = 0.008686154149472713 + 0.01 * 6.6448493003845215
Epoch 640, val loss: 1.1303255558013916
Epoch 650, training loss: 0.07474775612354279 = 0.008250401355326176 + 0.01 * 6.649735450744629
Epoch 650, val loss: 1.139195442199707
Epoch 660, training loss: 0.07431358098983765 = 0.007848326116800308 + 0.01 * 6.6465253829956055
Epoch 660, val loss: 1.1477950811386108
Epoch 670, training loss: 0.07381799072027206 = 0.007476622238755226 + 0.01 * 6.634137153625488
Epoch 670, val loss: 1.1561801433563232
Epoch 680, training loss: 0.07345817983150482 = 0.0071317641995847225 + 0.01 * 6.632641792297363
Epoch 680, val loss: 1.1644104719161987
Epoch 690, training loss: 0.07301091402769089 = 0.006811340805143118 + 0.01 * 6.619956970214844
Epoch 690, val loss: 1.1724337339401245
Epoch 700, training loss: 0.07274138927459717 = 0.006513393484055996 + 0.01 * 6.622799396514893
Epoch 700, val loss: 1.1802407503128052
Epoch 710, training loss: 0.07242264598608017 = 0.00623666075989604 + 0.01 * 6.618598937988281
Epoch 710, val loss: 1.1878998279571533
Epoch 720, training loss: 0.07207652181386948 = 0.005978691857308149 + 0.01 * 6.609783172607422
Epoch 720, val loss: 1.195298433303833
Epoch 730, training loss: 0.07188472896814346 = 0.005737593863159418 + 0.01 * 6.614713668823242
Epoch 730, val loss: 1.2025654315948486
Epoch 740, training loss: 0.07155504077672958 = 0.005511953961104155 + 0.01 * 6.604308605194092
Epoch 740, val loss: 1.2097209692001343
Epoch 750, training loss: 0.07123605161905289 = 0.005300373304635286 + 0.01 * 6.593567848205566
Epoch 750, val loss: 1.2166414260864258
Epoch 760, training loss: 0.07100332528352737 = 0.005101820919662714 + 0.01 * 6.590150356292725
Epoch 760, val loss: 1.2234376668930054
Epoch 770, training loss: 0.07097941637039185 = 0.004915804136544466 + 0.01 * 6.606361389160156
Epoch 770, val loss: 1.2300567626953125
Epoch 780, training loss: 0.0706602931022644 = 0.00474153459072113 + 0.01 * 6.591876029968262
Epoch 780, val loss: 1.2365554571151733
Epoch 790, training loss: 0.070444755256176 = 0.004577326588332653 + 0.01 * 6.586743354797363
Epoch 790, val loss: 1.2428841590881348
Epoch 800, training loss: 0.07019616663455963 = 0.0044225226156413555 + 0.01 * 6.577364444732666
Epoch 800, val loss: 1.2490164041519165
Epoch 810, training loss: 0.07001147419214249 = 0.004276642110198736 + 0.01 * 6.573483467102051
Epoch 810, val loss: 1.2550599575042725
Epoch 820, training loss: 0.0699358731508255 = 0.004138777498155832 + 0.01 * 6.579710006713867
Epoch 820, val loss: 1.2610362768173218
Epoch 830, training loss: 0.06978966295719147 = 0.004008498042821884 + 0.01 * 6.578117370605469
Epoch 830, val loss: 1.2668213844299316
Epoch 840, training loss: 0.06959564238786697 = 0.003885161830112338 + 0.01 * 6.571047782897949
Epoch 840, val loss: 1.2725000381469727
Epoch 850, training loss: 0.06944995373487473 = 0.003768571186810732 + 0.01 * 6.568138599395752
Epoch 850, val loss: 1.2780870199203491
Epoch 860, training loss: 0.06928776204586029 = 0.0036580089945346117 + 0.01 * 6.5629754066467285
Epoch 860, val loss: 1.2835571765899658
Epoch 870, training loss: 0.069032222032547 = 0.003553192364051938 + 0.01 * 6.547903060913086
Epoch 870, val loss: 1.2887763977050781
Epoch 880, training loss: 0.0691591128706932 = 0.0034539366606622934 + 0.01 * 6.570517063140869
Epoch 880, val loss: 1.294044017791748
Epoch 890, training loss: 0.06886342167854309 = 0.003359688911587 + 0.01 * 6.550373077392578
Epoch 890, val loss: 1.2991293668746948
Epoch 900, training loss: 0.06867773085832596 = 0.003270029556006193 + 0.01 * 6.540770530700684
Epoch 900, val loss: 1.3040661811828613
Epoch 910, training loss: 0.06860626488924026 = 0.003184287576004863 + 0.01 * 6.5421977043151855
Epoch 910, val loss: 1.3090108633041382
Epoch 920, training loss: 0.06842131912708282 = 0.0031026548240333796 + 0.01 * 6.531867027282715
Epoch 920, val loss: 1.3137571811676025
Epoch 930, training loss: 0.06832918524742126 = 0.0030247680842876434 + 0.01 * 6.5304412841796875
Epoch 930, val loss: 1.3185510635375977
Epoch 940, training loss: 0.06836728751659393 = 0.0029504946433007717 + 0.01 * 6.5416789054870605
Epoch 940, val loss: 1.323123574256897
Epoch 950, training loss: 0.06813383102416992 = 0.0028795902617275715 + 0.01 * 6.525424480438232
Epoch 950, val loss: 1.327687382698059
Epoch 960, training loss: 0.06817612051963806 = 0.002811916870996356 + 0.01 * 6.536420822143555
Epoch 960, val loss: 1.3321205377578735
Epoch 970, training loss: 0.06803207099437714 = 0.0027472081128507853 + 0.01 * 6.528486251831055
Epoch 970, val loss: 1.3364455699920654
Epoch 980, training loss: 0.06792587786912918 = 0.002685549668967724 + 0.01 * 6.524033069610596
Epoch 980, val loss: 1.3406858444213867
Epoch 990, training loss: 0.06767983734607697 = 0.0026263666804879904 + 0.01 * 6.50534725189209
Epoch 990, val loss: 1.3448433876037598
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.2000
accuracy on clean test nodes: 0.8481
Overall ASR: 0.3395
Flip ASR: 0.2800/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.040437698364258 = 1.9566997289657593 + 0.01 * 8.373791694641113
Epoch 0, val loss: 1.9553757905960083
Epoch 10, training loss: 2.028898000717163 = 1.9451608657836914 + 0.01 * 8.373713493347168
Epoch 10, val loss: 1.9438108205795288
Epoch 20, training loss: 2.01481294631958 = 1.9310795068740845 + 0.01 * 8.373332977294922
Epoch 20, val loss: 1.9289480447769165
Epoch 30, training loss: 1.9951850175857544 = 1.9114621877670288 + 0.01 * 8.372282028198242
Epoch 30, val loss: 1.90762197971344
Epoch 40, training loss: 1.9666955471038818 = 1.8830101490020752 + 0.01 * 8.368544578552246
Epoch 40, val loss: 1.8767015933990479
Epoch 50, training loss: 1.927205204963684 = 1.8437403440475464 + 0.01 * 8.346485137939453
Epoch 50, val loss: 1.8358746767044067
Epoch 60, training loss: 1.8791626691818237 = 1.7971632480621338 + 0.01 * 8.199946403503418
Epoch 60, val loss: 1.7918896675109863
Epoch 70, training loss: 1.8292750120162964 = 1.7513148784637451 + 0.01 * 7.796013355255127
Epoch 70, val loss: 1.7528774738311768
Epoch 80, training loss: 1.7745845317840576 = 1.6988829374313354 + 0.01 * 7.570153713226318
Epoch 80, val loss: 1.7080103158950806
Epoch 90, training loss: 1.7019482851028442 = 1.6273354291915894 + 0.01 * 7.461280822753906
Epoch 90, val loss: 1.6485872268676758
Epoch 100, training loss: 1.6090039014816284 = 1.5352554321289062 + 0.01 * 7.3748459815979
Epoch 100, val loss: 1.574967861175537
Epoch 110, training loss: 1.5069892406463623 = 1.4339847564697266 + 0.01 * 7.300448417663574
Epoch 110, val loss: 1.4972114562988281
Epoch 120, training loss: 1.4091506004333496 = 1.3368961811065674 + 0.01 * 7.225436687469482
Epoch 120, val loss: 1.4264345169067383
Epoch 130, training loss: 1.3210784196853638 = 1.2495300769805908 + 0.01 * 7.154832363128662
Epoch 130, val loss: 1.3668681383132935
Epoch 140, training loss: 1.2400339841842651 = 1.169058084487915 + 0.01 * 7.097585678100586
Epoch 140, val loss: 1.3136316537857056
Epoch 150, training loss: 1.159389853477478 = 1.0889555215835571 + 0.01 * 7.043432235717773
Epoch 150, val loss: 1.259701132774353
Epoch 160, training loss: 1.0746742486953735 = 1.0046577453613281 + 0.01 * 7.001654624938965
Epoch 160, val loss: 1.2033214569091797
Epoch 170, training loss: 0.985294759273529 = 0.9156197905540466 + 0.01 * 6.967499256134033
Epoch 170, val loss: 1.1456022262573242
Epoch 180, training loss: 0.8942710757255554 = 0.8249058723449707 + 0.01 * 6.936522483825684
Epoch 180, val loss: 1.0890238285064697
Epoch 190, training loss: 0.8072185516357422 = 0.7380472421646118 + 0.01 * 6.917131423950195
Epoch 190, val loss: 1.035966157913208
Epoch 200, training loss: 0.7286515831947327 = 0.659615695476532 + 0.01 * 6.903589248657227
Epoch 200, val loss: 0.9903475046157837
Epoch 210, training loss: 0.6600131988525391 = 0.591060221195221 + 0.01 * 6.895295143127441
Epoch 210, val loss: 0.9536762237548828
Epoch 220, training loss: 0.6005532741546631 = 0.5316872596740723 + 0.01 * 6.886604309082031
Epoch 220, val loss: 0.9261702299118042
Epoch 230, training loss: 0.5489513874053955 = 0.4801790118217468 + 0.01 * 6.8772406578063965
Epoch 230, val loss: 0.9072032570838928
Epoch 240, training loss: 0.5040568709373474 = 0.43538373708724976 + 0.01 * 6.867313861846924
Epoch 240, val loss: 0.8955293297767639
Epoch 250, training loss: 0.4648476541042328 = 0.39628803730010986 + 0.01 * 6.855960845947266
Epoch 250, val loss: 0.8896065950393677
Epoch 260, training loss: 0.4302576780319214 = 0.3618295192718506 + 0.01 * 6.842817783355713
Epoch 260, val loss: 0.8877211213111877
Epoch 270, training loss: 0.3992309272289276 = 0.3309197723865509 + 0.01 * 6.831115245819092
Epoch 270, val loss: 0.8886724710464478
Epoch 280, training loss: 0.37080255150794983 = 0.30255982279777527 + 0.01 * 6.824272632598877
Epoch 280, val loss: 0.8915183544158936
Epoch 290, training loss: 0.34411758184432983 = 0.2760016620159149 + 0.01 * 6.811592102050781
Epoch 290, val loss: 0.8953627347946167
Epoch 300, training loss: 0.3189559578895569 = 0.25072795152664185 + 0.01 * 6.822799205780029
Epoch 300, val loss: 0.8997438549995422
Epoch 310, training loss: 0.2946529984474182 = 0.2266700714826584 + 0.01 * 6.798293590545654
Epoch 310, val loss: 0.9042617678642273
Epoch 320, training loss: 0.2718793749809265 = 0.20393069088459015 + 0.01 * 6.794867515563965
Epoch 320, val loss: 0.9091199636459351
Epoch 330, training loss: 0.25058841705322266 = 0.18270763754844666 + 0.01 * 6.7880778312683105
Epoch 330, val loss: 0.9144881963729858
Epoch 340, training loss: 0.23095771670341492 = 0.16312608122825623 + 0.01 * 6.783164024353027
Epoch 340, val loss: 0.9196603894233704
Epoch 350, training loss: 0.21302837133407593 = 0.14520440995693207 + 0.01 * 6.782397270202637
Epoch 350, val loss: 0.9249940514564514
Epoch 360, training loss: 0.19669118523597717 = 0.12890414893627167 + 0.01 * 6.778703689575195
Epoch 360, val loss: 0.9304277300834656
Epoch 370, training loss: 0.18196633458137512 = 0.11424274742603302 + 0.01 * 6.772359371185303
Epoch 370, val loss: 0.9364755749702454
Epoch 380, training loss: 0.16874302923679352 = 0.10105016082525253 + 0.01 * 6.769287109375
Epoch 380, val loss: 0.9427406787872314
Epoch 390, training loss: 0.15702450275421143 = 0.08937158435583115 + 0.01 * 6.765291213989258
Epoch 390, val loss: 0.9497167468070984
Epoch 400, training loss: 0.14682625234127045 = 0.07914608716964722 + 0.01 * 6.768016815185547
Epoch 400, val loss: 0.9576705098152161
Epoch 410, training loss: 0.13780087232589722 = 0.07019664347171783 + 0.01 * 6.760422706604004
Epoch 410, val loss: 0.9674814939498901
Epoch 420, training loss: 0.12991729378700256 = 0.062370385974645615 + 0.01 * 6.754691123962402
Epoch 420, val loss: 0.9785581231117249
Epoch 430, training loss: 0.12319976091384888 = 0.055618856102228165 + 0.01 * 6.758090972900391
Epoch 430, val loss: 0.9909965395927429
Epoch 440, training loss: 0.11732365190982819 = 0.04982546716928482 + 0.01 * 6.749818801879883
Epoch 440, val loss: 1.0045922994613647
Epoch 450, training loss: 0.11228330433368683 = 0.04483749717473984 + 0.01 * 6.74458122253418
Epoch 450, val loss: 1.018761157989502
Epoch 460, training loss: 0.10797782242298126 = 0.040523871779441833 + 0.01 * 6.745395183563232
Epoch 460, val loss: 1.0329244136810303
Epoch 470, training loss: 0.10407021641731262 = 0.03676199913024902 + 0.01 * 6.7308220863342285
Epoch 470, val loss: 1.0468668937683105
Epoch 480, training loss: 0.10088290274143219 = 0.03346322476863861 + 0.01 * 6.741968154907227
Epoch 480, val loss: 1.0603973865509033
Epoch 490, training loss: 0.09783997386693954 = 0.030557289719581604 + 0.01 * 6.728268623352051
Epoch 490, val loss: 1.0736169815063477
Epoch 500, training loss: 0.09517964720726013 = 0.027988251298666 + 0.01 * 6.719139099121094
Epoch 500, val loss: 1.0866053104400635
Epoch 510, training loss: 0.09285525977611542 = 0.02571001648902893 + 0.01 * 6.714524745941162
Epoch 510, val loss: 1.099173665046692
Epoch 520, training loss: 0.09079528599977493 = 0.023684784770011902 + 0.01 * 6.711050510406494
Epoch 520, val loss: 1.1114240884780884
Epoch 530, training loss: 0.08899886906147003 = 0.0218794122338295 + 0.01 * 6.7119460105896
Epoch 530, val loss: 1.1233725547790527
Epoch 540, training loss: 0.08724283427000046 = 0.02026531659066677 + 0.01 * 6.697751998901367
Epoch 540, val loss: 1.1349809169769287
Epoch 550, training loss: 0.08575310558080673 = 0.018817873671650887 + 0.01 * 6.693523406982422
Epoch 550, val loss: 1.146295189857483
Epoch 560, training loss: 0.0844513326883316 = 0.017517337575554848 + 0.01 * 6.693399906158447
Epoch 560, val loss: 1.1573126316070557
Epoch 570, training loss: 0.08326757699251175 = 0.01634543389081955 + 0.01 * 6.692214488983154
Epoch 570, val loss: 1.1680606603622437
Epoch 580, training loss: 0.08202049881219864 = 0.015287181362509727 + 0.01 * 6.6733317375183105
Epoch 580, val loss: 1.178512454032898
Epoch 590, training loss: 0.08106956630945206 = 0.014329440891742706 + 0.01 * 6.674012660980225
Epoch 590, val loss: 1.1887543201446533
Epoch 600, training loss: 0.08004839718341827 = 0.013460448943078518 + 0.01 * 6.65879487991333
Epoch 600, val loss: 1.1985728740692139
Epoch 610, training loss: 0.0792202353477478 = 0.012670149095356464 + 0.01 * 6.655008792877197
Epoch 610, val loss: 1.2081888914108276
Epoch 620, training loss: 0.07850100845098495 = 0.011950396932661533 + 0.01 * 6.655061721801758
Epoch 620, val loss: 1.2174952030181885
Epoch 630, training loss: 0.0777214765548706 = 0.011292374692857265 + 0.01 * 6.642910480499268
Epoch 630, val loss: 1.2265268564224243
Epoch 640, training loss: 0.07711736857891083 = 0.010689069516956806 + 0.01 * 6.642829418182373
Epoch 640, val loss: 1.235398769378662
Epoch 650, training loss: 0.07663224637508392 = 0.010135350748896599 + 0.01 * 6.649689197540283
Epoch 650, val loss: 1.2438809871673584
Epoch 660, training loss: 0.07595501840114594 = 0.009626192972064018 + 0.01 * 6.632882595062256
Epoch 660, val loss: 1.2522437572479248
Epoch 670, training loss: 0.07532379776239395 = 0.009156891144812107 + 0.01 * 6.616690635681152
Epoch 670, val loss: 1.2603317499160767
Epoch 680, training loss: 0.0748896598815918 = 0.008723375387489796 + 0.01 * 6.616628170013428
Epoch 680, val loss: 1.2682271003723145
Epoch 690, training loss: 0.07466305792331696 = 0.00832175463438034 + 0.01 * 6.634130477905273
Epoch 690, val loss: 1.2759387493133545
Epoch 700, training loss: 0.07411771267652512 = 0.007949471473693848 + 0.01 * 6.616824150085449
Epoch 700, val loss: 1.2833940982818604
Epoch 710, training loss: 0.07361406087875366 = 0.00760359363630414 + 0.01 * 6.601047039031982
Epoch 710, val loss: 1.2907317876815796
Epoch 720, training loss: 0.07323191314935684 = 0.00728152459487319 + 0.01 * 6.595038414001465
Epoch 720, val loss: 1.297826886177063
Epoch 730, training loss: 0.07321757078170776 = 0.006981028709560633 + 0.01 * 6.623653888702393
Epoch 730, val loss: 1.304660677909851
Epoch 740, training loss: 0.07265191525220871 = 0.0067008789628744125 + 0.01 * 6.595103740692139
Epoch 740, val loss: 1.3114426136016846
Epoch 750, training loss: 0.07223793864250183 = 0.006438974291086197 + 0.01 * 6.579896926879883
Epoch 750, val loss: 1.3179558515548706
Epoch 760, training loss: 0.07195938378572464 = 0.00619348231703043 + 0.01 * 6.576590061187744
Epoch 760, val loss: 1.3243695497512817
Epoch 770, training loss: 0.071700818836689 = 0.0059632351621985435 + 0.01 * 6.573758125305176
Epoch 770, val loss: 1.3305474519729614
Epoch 780, training loss: 0.07154154777526855 = 0.00574747659265995 + 0.01 * 6.579407215118408
Epoch 780, val loss: 1.3366197347640991
Epoch 790, training loss: 0.07120629400014877 = 0.005544438026845455 + 0.01 * 6.566185474395752
Epoch 790, val loss: 1.342573881149292
Epoch 800, training loss: 0.07115102559328079 = 0.005353020038455725 + 0.01 * 6.579801082611084
Epoch 800, val loss: 1.3483026027679443
Epoch 810, training loss: 0.0707358866930008 = 0.005172909703105688 + 0.01 * 6.556297779083252
Epoch 810, val loss: 1.3539913892745972
Epoch 820, training loss: 0.07077324390411377 = 0.005002955906093121 + 0.01 * 6.577029228210449
Epoch 820, val loss: 1.3595010042190552
Epoch 830, training loss: 0.07046280801296234 = 0.0048424722626805305 + 0.01 * 6.562033653259277
Epoch 830, val loss: 1.3648285865783691
Epoch 840, training loss: 0.07020072638988495 = 0.004690736066550016 + 0.01 * 6.550999164581299
Epoch 840, val loss: 1.3701074123382568
Epoch 850, training loss: 0.07014162838459015 = 0.004546926356852055 + 0.01 * 6.5594706535339355
Epoch 850, val loss: 1.3751698732376099
Epoch 860, training loss: 0.06988862156867981 = 0.004410743713378906 + 0.01 * 6.547788143157959
Epoch 860, val loss: 1.38021719455719
Epoch 870, training loss: 0.06979575753211975 = 0.004281352739781141 + 0.01 * 6.551441192626953
Epoch 870, val loss: 1.385056972503662
Epoch 880, training loss: 0.06954975426197052 = 0.004158603958785534 + 0.01 * 6.5391154289245605
Epoch 880, val loss: 1.389879584312439
Epoch 890, training loss: 0.06942738592624664 = 0.004041949287056923 + 0.01 * 6.538543701171875
Epoch 890, val loss: 1.394616961479187
Epoch 900, training loss: 0.06929279118776321 = 0.003930956590920687 + 0.01 * 6.5361833572387695
Epoch 900, val loss: 1.3991920948028564
Epoch 910, training loss: 0.0691092237830162 = 0.003825280349701643 + 0.01 * 6.52839469909668
Epoch 910, val loss: 1.4036846160888672
Epoch 920, training loss: 0.0691925585269928 = 0.0037246348802000284 + 0.01 * 6.546792507171631
Epoch 920, val loss: 1.4081995487213135
Epoch 930, training loss: 0.06885858625173569 = 0.003628647420555353 + 0.01 * 6.522994041442871
Epoch 930, val loss: 1.4123879671096802
Epoch 940, training loss: 0.06873497366905212 = 0.0035371819976717234 + 0.01 * 6.519778728485107
Epoch 940, val loss: 1.4165805578231812
Epoch 950, training loss: 0.06877150386571884 = 0.0034497645683586597 + 0.01 * 6.5321736335754395
Epoch 950, val loss: 1.4207210540771484
Epoch 960, training loss: 0.06846625357866287 = 0.0033661548513919115 + 0.01 * 6.510009765625
Epoch 960, val loss: 1.4247435331344604
Epoch 970, training loss: 0.06862308830022812 = 0.0032861302606761456 + 0.01 * 6.533696174621582
Epoch 970, val loss: 1.4287114143371582
Epoch 980, training loss: 0.06819117814302444 = 0.0032095175702124834 + 0.01 * 6.498166084289551
Epoch 980, val loss: 1.4325979948043823
Epoch 990, training loss: 0.0681755468249321 = 0.003136244835332036 + 0.01 * 6.50393009185791
Epoch 990, val loss: 1.436259150505066
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7630
Overall ASR: 0.7860
Flip ASR: 0.7556/225 nodes
The final ASR:0.54736, 0.18357, Accuracy:0.80123, 0.03531
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11618])
remove edge: torch.Size([2, 9582])
updated graph: torch.Size([2, 10644])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.97786, 0.00522, Accuracy:0.82716, 0.00462
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.025378704071045 = 1.9416399002075195 + 0.01 * 8.37387752532959
Epoch 0, val loss: 1.936277985572815
Epoch 10, training loss: 2.015298843383789 = 1.931560754776001 + 0.01 * 8.373802185058594
Epoch 10, val loss: 1.9259544610977173
Epoch 20, training loss: 2.0027384757995605 = 1.9190030097961426 + 0.01 * 8.373552322387695
Epoch 20, val loss: 1.9129971265792847
Epoch 30, training loss: 1.985114574432373 = 1.9013861417770386 + 0.01 * 8.372842788696289
Epoch 30, val loss: 1.8947006464004517
Epoch 40, training loss: 1.9591355323791504 = 1.8754407167434692 + 0.01 * 8.369487762451172
Epoch 40, val loss: 1.8679906129837036
Epoch 50, training loss: 1.922382116317749 = 1.8389331102371216 + 0.01 * 8.34489631652832
Epoch 50, val loss: 1.8321378231048584
Epoch 60, training loss: 1.8775303363800049 = 1.7958991527557373 + 0.01 * 8.163114547729492
Epoch 60, val loss: 1.7940690517425537
Epoch 70, training loss: 1.829662561416626 = 1.751798152923584 + 0.01 * 7.786439418792725
Epoch 70, val loss: 1.7590234279632568
Epoch 80, training loss: 1.7701157331466675 = 1.6952612400054932 + 0.01 * 7.485453128814697
Epoch 80, val loss: 1.7124180793762207
Epoch 90, training loss: 1.6926965713500977 = 1.6194918155670166 + 0.01 * 7.320472240447998
Epoch 90, val loss: 1.6499550342559814
Epoch 100, training loss: 1.5960206985473633 = 1.5234707593917847 + 0.01 * 7.254994869232178
Epoch 100, val loss: 1.572188138961792
Epoch 110, training loss: 1.4876631498336792 = 1.415594458580017 + 0.01 * 7.206872940063477
Epoch 110, val loss: 1.4857102632522583
Epoch 120, training loss: 1.3764886856079102 = 1.3049452304840088 + 0.01 * 7.154343605041504
Epoch 120, val loss: 1.3999977111816406
Epoch 130, training loss: 1.2675580978393555 = 1.1963945627212524 + 0.01 * 7.116359233856201
Epoch 130, val loss: 1.31829833984375
Epoch 140, training loss: 1.164933443069458 = 1.093988060951233 + 0.01 * 7.094534873962402
Epoch 140, val loss: 1.2429078817367554
Epoch 150, training loss: 1.0713653564453125 = 1.0005464553833008 + 0.01 * 7.081892967224121
Epoch 150, val loss: 1.1746032238006592
Epoch 160, training loss: 0.9869127869606018 = 0.9162000417709351 + 0.01 * 7.071274757385254
Epoch 160, val loss: 1.11290442943573
Epoch 170, training loss: 0.9100452661514282 = 0.8394878506660461 + 0.01 * 7.055744647979736
Epoch 170, val loss: 1.0565284490585327
Epoch 180, training loss: 0.8397425413131714 = 0.7694135308265686 + 0.01 * 7.032903671264648
Epoch 180, val loss: 1.0054601430892944
Epoch 190, training loss: 0.775772213935852 = 0.7056717872619629 + 0.01 * 7.010040283203125
Epoch 190, val loss: 0.9607914686203003
Epoch 200, training loss: 0.7174540758132935 = 0.6476132869720459 + 0.01 * 6.984079837799072
Epoch 200, val loss: 0.9229204058647156
Epoch 210, training loss: 0.6635414361953735 = 0.5938889384269714 + 0.01 * 6.96524715423584
Epoch 210, val loss: 0.8911375999450684
Epoch 220, training loss: 0.6127727031707764 = 0.5432616472244263 + 0.01 * 6.951105117797852
Epoch 220, val loss: 0.8643643856048584
Epoch 230, training loss: 0.564313530921936 = 0.4949132204055786 + 0.01 * 6.940032005310059
Epoch 230, val loss: 0.8420748114585876
Epoch 240, training loss: 0.5176504254341125 = 0.44833582639694214 + 0.01 * 6.931461334228516
Epoch 240, val loss: 0.8241475224494934
Epoch 250, training loss: 0.47257858514785767 = 0.4033504128456116 + 0.01 * 6.922815799713135
Epoch 250, val loss: 0.8105129599571228
Epoch 260, training loss: 0.4292391240596771 = 0.36009320616722107 + 0.01 * 6.914592266082764
Epoch 260, val loss: 0.801224410533905
Epoch 270, training loss: 0.38804975152015686 = 0.3190118074417114 + 0.01 * 6.903794765472412
Epoch 270, val loss: 0.7966223359107971
Epoch 280, training loss: 0.3497695326805115 = 0.2808188199996948 + 0.01 * 6.895072937011719
Epoch 280, val loss: 0.7964851260185242
Epoch 290, training loss: 0.3151969611644745 = 0.246305450797081 + 0.01 * 6.8891520500183105
Epoch 290, val loss: 0.8007843494415283
Epoch 300, training loss: 0.28466176986694336 = 0.21587516367435455 + 0.01 * 6.878660202026367
Epoch 300, val loss: 0.8091537356376648
Epoch 310, training loss: 0.2583771347999573 = 0.1895066499710083 + 0.01 * 6.887049198150635
Epoch 310, val loss: 0.8209529519081116
Epoch 320, training loss: 0.2354140728712082 = 0.1667955368757248 + 0.01 * 6.86185359954834
Epoch 320, val loss: 0.8354353904724121
Epoch 330, training loss: 0.21572242677211761 = 0.14717380702495575 + 0.01 * 6.854862213134766
Epoch 330, val loss: 0.8521170616149902
Epoch 340, training loss: 0.1986408233642578 = 0.1301511824131012 + 0.01 * 6.848964691162109
Epoch 340, val loss: 0.870543897151947
Epoch 350, training loss: 0.18374520540237427 = 0.1153334528207779 + 0.01 * 6.841176509857178
Epoch 350, val loss: 0.89032381772995
Epoch 360, training loss: 0.17078381776809692 = 0.10241065174341202 + 0.01 * 6.83731746673584
Epoch 360, val loss: 0.9111024141311646
Epoch 370, training loss: 0.15948016941547394 = 0.09114253520965576 + 0.01 * 6.833763599395752
Epoch 370, val loss: 0.932553768157959
Epoch 380, training loss: 0.14957603812217712 = 0.08129226416349411 + 0.01 * 6.828377723693848
Epoch 380, val loss: 0.9545696377754211
Epoch 390, training loss: 0.14087045192718506 = 0.07266397774219513 + 0.01 * 6.8206467628479
Epoch 390, val loss: 0.9768813252449036
Epoch 400, training loss: 0.1333118975162506 = 0.0650993064045906 + 0.01 * 6.821259498596191
Epoch 400, val loss: 0.9992354512214661
Epoch 410, training loss: 0.1266181617975235 = 0.05846177041530609 + 0.01 * 6.815639019012451
Epoch 410, val loss: 1.0214803218841553
Epoch 420, training loss: 0.12070099264383316 = 0.052623867988586426 + 0.01 * 6.807712554931641
Epoch 420, val loss: 1.043553352355957
Epoch 430, training loss: 0.11548508703708649 = 0.04748227074742317 + 0.01 * 6.8002824783325195
Epoch 430, val loss: 1.0653587579727173
Epoch 440, training loss: 0.11098241060972214 = 0.04295074939727783 + 0.01 * 6.803165912628174
Epoch 440, val loss: 1.08677077293396
Epoch 450, training loss: 0.10684511810541153 = 0.03895702213048935 + 0.01 * 6.788809776306152
Epoch 450, val loss: 1.1076496839523315
Epoch 460, training loss: 0.1032942607998848 = 0.03543330729007721 + 0.01 * 6.78609561920166
Epoch 460, val loss: 1.1279805898666382
Epoch 470, training loss: 0.10014338791370392 = 0.032321151345968246 + 0.01 * 6.782223224639893
Epoch 470, val loss: 1.14769446849823
Epoch 480, training loss: 0.09732863306999207 = 0.02957022935152054 + 0.01 * 6.7758402824401855
Epoch 480, val loss: 1.1668251752853394
Epoch 490, training loss: 0.0948081910610199 = 0.027132872492074966 + 0.01 * 6.767531871795654
Epoch 490, val loss: 1.1853266954421997
Epoch 500, training loss: 0.09290770441293716 = 0.024968735873699188 + 0.01 * 6.7938971519470215
Epoch 500, val loss: 1.2031697034835815
Epoch 510, training loss: 0.09067602455615997 = 0.02304481714963913 + 0.01 * 6.763120651245117
Epoch 510, val loss: 1.2204499244689941
Epoch 520, training loss: 0.08890731632709503 = 0.021329088136553764 + 0.01 * 6.7578229904174805
Epoch 520, val loss: 1.2372163534164429
Epoch 530, training loss: 0.08722305297851562 = 0.019794616848230362 + 0.01 * 6.7428436279296875
Epoch 530, val loss: 1.2534091472625732
Epoch 540, training loss: 0.08584080636501312 = 0.018418531864881516 + 0.01 * 6.742228031158447
Epoch 540, val loss: 1.2690150737762451
Epoch 550, training loss: 0.0844840332865715 = 0.017182692885398865 + 0.01 * 6.730134010314941
Epoch 550, val loss: 1.2841169834136963
Epoch 560, training loss: 0.08334548026323318 = 0.016068749129772186 + 0.01 * 6.727673053741455
Epoch 560, val loss: 1.298812747001648
Epoch 570, training loss: 0.08245613425970078 = 0.015061793848872185 + 0.01 * 6.739434242248535
Epoch 570, val loss: 1.312933087348938
Epoch 580, training loss: 0.08130215108394623 = 0.01414963323622942 + 0.01 * 6.715251922607422
Epoch 580, val loss: 1.326790690422058
Epoch 590, training loss: 0.08045443147420883 = 0.01332083996385336 + 0.01 * 6.713359355926514
Epoch 590, val loss: 1.340117335319519
Epoch 600, training loss: 0.07952093333005905 = 0.012566067278385162 + 0.01 * 6.695487022399902
Epoch 600, val loss: 1.3530608415603638
Epoch 610, training loss: 0.07872896641492844 = 0.011876796372234821 + 0.01 * 6.685216903686523
Epoch 610, val loss: 1.3656353950500488
Epoch 620, training loss: 0.07807693630456924 = 0.011246048845350742 + 0.01 * 6.683089256286621
Epoch 620, val loss: 1.3778436183929443
Epoch 630, training loss: 0.07752927392721176 = 0.010667373426258564 + 0.01 * 6.686189651489258
Epoch 630, val loss: 1.3898838758468628
Epoch 640, training loss: 0.07693810015916824 = 0.010134955868124962 + 0.01 * 6.680314540863037
Epoch 640, val loss: 1.4015114307403564
Epoch 650, training loss: 0.07626327127218246 = 0.009644715115427971 + 0.01 * 6.661855697631836
Epoch 650, val loss: 1.4129219055175781
Epoch 660, training loss: 0.07599524408578873 = 0.009192212484776974 + 0.01 * 6.680303573608398
Epoch 660, val loss: 1.42374849319458
Epoch 670, training loss: 0.07527270168066025 = 0.008773832581937313 + 0.01 * 6.649886608123779
Epoch 670, val loss: 1.4346883296966553
Epoch 680, training loss: 0.07505403459072113 = 0.0083857337012887 + 0.01 * 6.666830062866211
Epoch 680, val loss: 1.4451707601547241
Epoch 690, training loss: 0.0747404471039772 = 0.008025318384170532 + 0.01 * 6.671513080596924
Epoch 690, val loss: 1.455348014831543
Epoch 700, training loss: 0.07405652105808258 = 0.0076898932456970215 + 0.01 * 6.63666296005249
Epoch 700, val loss: 1.465541958808899
Epoch 710, training loss: 0.07363229244947433 = 0.00737700005993247 + 0.01 * 6.625529766082764
Epoch 710, val loss: 1.475235104560852
Epoch 720, training loss: 0.07323834300041199 = 0.007084744982421398 + 0.01 * 6.615360260009766
Epoch 720, val loss: 1.4848531484603882
Epoch 730, training loss: 0.07295025885105133 = 0.0068116458132863045 + 0.01 * 6.613861083984375
Epoch 730, val loss: 1.4941591024398804
Epoch 740, training loss: 0.07268734276294708 = 0.006555816158652306 + 0.01 * 6.613152980804443
Epoch 740, val loss: 1.503463864326477
Epoch 750, training loss: 0.07255885004997253 = 0.006315925624221563 + 0.01 * 6.624292373657227
Epoch 750, val loss: 1.5123473405838013
Epoch 760, training loss: 0.07213027775287628 = 0.006090695038437843 + 0.01 * 6.603958606719971
Epoch 760, val loss: 1.5212299823760986
Epoch 770, training loss: 0.07180444896221161 = 0.005878876429051161 + 0.01 * 6.592557907104492
Epoch 770, val loss: 1.529848575592041
Epoch 780, training loss: 0.07161624729633331 = 0.005679375026375055 + 0.01 * 6.593687534332275
Epoch 780, val loss: 1.5381946563720703
Epoch 790, training loss: 0.07129894196987152 = 0.0054913959465920925 + 0.01 * 6.58075475692749
Epoch 790, val loss: 1.546425700187683
Epoch 800, training loss: 0.07103987038135529 = 0.0053139119409024715 + 0.01 * 6.572596073150635
Epoch 800, val loss: 1.5544418096542358
Epoch 810, training loss: 0.07084595412015915 = 0.0051462603732943535 + 0.01 * 6.569969654083252
Epoch 810, val loss: 1.5622903108596802
Epoch 820, training loss: 0.07064688205718994 = 0.004987544845789671 + 0.01 * 6.565934181213379
Epoch 820, val loss: 1.5701888799667358
Epoch 830, training loss: 0.07095823436975479 = 0.00483721261844039 + 0.01 * 6.612102031707764
Epoch 830, val loss: 1.5776852369308472
Epoch 840, training loss: 0.0703912302851677 = 0.0046946643851697445 + 0.01 * 6.569656848907471
Epoch 840, val loss: 1.585206151008606
Epoch 850, training loss: 0.07014290243387222 = 0.004559561610221863 + 0.01 * 6.558333873748779
Epoch 850, val loss: 1.5923644304275513
Epoch 860, training loss: 0.07014621794223785 = 0.004431124776601791 + 0.01 * 6.57150936126709
Epoch 860, val loss: 1.5994929075241089
Epoch 870, training loss: 0.0699801817536354 = 0.004308948293328285 + 0.01 * 6.5671234130859375
Epoch 870, val loss: 1.60657799243927
Epoch 880, training loss: 0.06955145299434662 = 0.00419278210029006 + 0.01 * 6.535866737365723
Epoch 880, val loss: 1.6133570671081543
Epoch 890, training loss: 0.06950260698795319 = 0.004082035273313522 + 0.01 * 6.542057514190674
Epoch 890, val loss: 1.6200510263442993
Epoch 900, training loss: 0.06934718787670135 = 0.00397647125646472 + 0.01 * 6.537071704864502
Epoch 900, val loss: 1.6267406940460205
Epoch 910, training loss: 0.0691235288977623 = 0.003875686088576913 + 0.01 * 6.524784564971924
Epoch 910, val loss: 1.6331918239593506
Epoch 920, training loss: 0.06913136690855026 = 0.0037792581133544445 + 0.01 * 6.535211086273193
Epoch 920, val loss: 1.6395231485366821
Epoch 930, training loss: 0.06903623044490814 = 0.0036870071198791265 + 0.01 * 6.534922122955322
Epoch 930, val loss: 1.6458560228347778
Epoch 940, training loss: 0.06875898689031601 = 0.003598895389586687 + 0.01 * 6.5160088539123535
Epoch 940, val loss: 1.6519938707351685
Epoch 950, training loss: 0.06883859634399414 = 0.003514551091939211 + 0.01 * 6.53240442276001
Epoch 950, val loss: 1.6579880714416504
Epoch 960, training loss: 0.06859897077083588 = 0.003433704376220703 + 0.01 * 6.516526699066162
Epoch 960, val loss: 1.6640009880065918
Epoch 970, training loss: 0.0684734433889389 = 0.0033563568722456694 + 0.01 * 6.511708736419678
Epoch 970, val loss: 1.6696034669876099
Epoch 980, training loss: 0.06829948723316193 = 0.003282049670815468 + 0.01 * 6.501744270324707
Epoch 980, val loss: 1.6754847764968872
Epoch 990, training loss: 0.06857852637767792 = 0.003210921073332429 + 0.01 * 6.536760330200195
Epoch 990, val loss: 1.681018352508545
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.7593
Overall ASR: 0.5793
Flip ASR: 0.5111/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.039313793182373 = 1.955574631690979 + 0.01 * 8.373912811279297
Epoch 0, val loss: 1.9589473009109497
Epoch 10, training loss: 2.0281238555908203 = 1.944385290145874 + 0.01 * 8.373859405517578
Epoch 10, val loss: 1.9474064111709595
Epoch 20, training loss: 2.0144333839416504 = 1.930696725845337 + 0.01 * 8.373676300048828
Epoch 20, val loss: 1.9328428506851196
Epoch 30, training loss: 1.9952106475830078 = 1.9114798307418823 + 0.01 * 8.373078346252441
Epoch 30, val loss: 1.9122226238250732
Epoch 40, training loss: 1.9666677713394165 = 1.8829677104949951 + 0.01 * 8.37000846862793
Epoch 40, val loss: 1.8819828033447266
Epoch 50, training loss: 1.926666021347046 = 1.8431676626205444 + 0.01 * 8.349832534790039
Epoch 50, val loss: 1.8416736125946045
Epoch 60, training loss: 1.8813834190368652 = 1.7988760471343994 + 0.01 * 8.25074291229248
Epoch 60, val loss: 1.8008747100830078
Epoch 70, training loss: 1.8381991386413574 = 1.7572133541107178 + 0.01 * 8.0985746383667
Epoch 70, val loss: 1.7648975849151611
Epoch 80, training loss: 1.782141923904419 = 1.702832579612732 + 0.01 * 7.930931091308594
Epoch 80, val loss: 1.7164729833602905
Epoch 90, training loss: 1.706007957458496 = 1.6285085678100586 + 0.01 * 7.749933242797852
Epoch 90, val loss: 1.6513129472732544
Epoch 100, training loss: 1.6118236780166626 = 1.5361474752426147 + 0.01 * 7.567624568939209
Epoch 100, val loss: 1.5740410089492798
Epoch 110, training loss: 1.5132923126220703 = 1.4400770664215088 + 0.01 * 7.32152795791626
Epoch 110, val loss: 1.4953515529632568
Epoch 120, training loss: 1.422999382019043 = 1.3502579927444458 + 0.01 * 7.274145126342773
Epoch 120, val loss: 1.426052451133728
Epoch 130, training loss: 1.339884877204895 = 1.2675732374191284 + 0.01 * 7.231168746948242
Epoch 130, val loss: 1.3665053844451904
Epoch 140, training loss: 1.2612932920455933 = 1.189469814300537 + 0.01 * 7.1823506355285645
Epoch 140, val loss: 1.3123576641082764
Epoch 150, training loss: 1.184752106666565 = 1.1134288311004639 + 0.01 * 7.132322788238525
Epoch 150, val loss: 1.2600270509719849
Epoch 160, training loss: 1.1077189445495605 = 1.0368010997772217 + 0.01 * 7.091779708862305
Epoch 160, val loss: 1.2069997787475586
Epoch 170, training loss: 1.0294064283370972 = 0.9587959051132202 + 0.01 * 7.0610575675964355
Epoch 170, val loss: 1.1512422561645508
Epoch 180, training loss: 0.951068103313446 = 0.880744218826294 + 0.01 * 7.03239107131958
Epoch 180, val loss: 1.0949509143829346
Epoch 190, training loss: 0.875048041343689 = 0.8049218654632568 + 0.01 * 7.012617588043213
Epoch 190, val loss: 1.039717197418213
Epoch 200, training loss: 0.803810715675354 = 0.7338054776191711 + 0.01 * 7.000525951385498
Epoch 200, val loss: 0.9883009195327759
Epoch 210, training loss: 0.7392575740814209 = 0.6693302989006042 + 0.01 * 6.992726802825928
Epoch 210, val loss: 0.9428377747535706
Epoch 220, training loss: 0.6813197135925293 = 0.6114518046379089 + 0.01 * 6.986790657043457
Epoch 220, val loss: 0.9039220809936523
Epoch 230, training loss: 0.6281633973121643 = 0.5583513975143433 + 0.01 * 6.981198787689209
Epoch 230, val loss: 0.8704619407653809
Epoch 240, training loss: 0.577807605266571 = 0.5080536603927612 + 0.01 * 6.975395679473877
Epoch 240, val loss: 0.8411754965782166
Epoch 250, training loss: 0.529151201248169 = 0.45945635437965393 + 0.01 * 6.969482421875
Epoch 250, val loss: 0.8150671720504761
Epoch 260, training loss: 0.4822451174259186 = 0.4126315414905548 + 0.01 * 6.961357116699219
Epoch 260, val loss: 0.7920761108398438
Epoch 270, training loss: 0.43782758712768555 = 0.36829429864883423 + 0.01 * 6.9533281326293945
Epoch 270, val loss: 0.7728148698806763
Epoch 280, training loss: 0.39676547050476074 = 0.32732900977134705 + 0.01 * 6.943645000457764
Epoch 280, val loss: 0.7582176923751831
Epoch 290, training loss: 0.3594454526901245 = 0.2900560200214386 + 0.01 * 6.938943862915039
Epoch 290, val loss: 0.7483088970184326
Epoch 300, training loss: 0.3257775902748108 = 0.2564908564090729 + 0.01 * 6.9286723136901855
Epoch 300, val loss: 0.7429917454719543
Epoch 310, training loss: 0.2956945598125458 = 0.22638332843780518 + 0.01 * 6.93112325668335
Epoch 310, val loss: 0.7415468096733093
Epoch 320, training loss: 0.2686936557292938 = 0.19959312677383423 + 0.01 * 6.91005277633667
Epoch 320, val loss: 0.74386066198349
Epoch 330, training loss: 0.24516016244888306 = 0.17616166174411774 + 0.01 * 6.899851322174072
Epoch 330, val loss: 0.749728262424469
Epoch 340, training loss: 0.22507840394973755 = 0.15594786405563354 + 0.01 * 6.9130539894104
Epoch 340, val loss: 0.758794367313385
Epoch 350, training loss: 0.20748987793922424 = 0.13861876726150513 + 0.01 * 6.887111663818359
Epoch 350, val loss: 0.7701883912086487
Epoch 360, training loss: 0.1925189197063446 = 0.12380113452672958 + 0.01 * 6.87177848815918
Epoch 360, val loss: 0.7836832404136658
Epoch 370, training loss: 0.1798010766506195 = 0.11112874001264572 + 0.01 * 6.8672332763671875
Epoch 370, val loss: 0.7987017631530762
Epoch 380, training loss: 0.1689581722021103 = 0.1002546101808548 + 0.01 * 6.87035608291626
Epoch 380, val loss: 0.8146410584449768
Epoch 390, training loss: 0.1594424992799759 = 0.09087374061346054 + 0.01 * 6.856876373291016
Epoch 390, val loss: 0.8309699296951294
Epoch 400, training loss: 0.15127719938755035 = 0.08272416144609451 + 0.01 * 6.85530424118042
Epoch 400, val loss: 0.8476446270942688
Epoch 410, training loss: 0.14405274391174316 = 0.07560235261917114 + 0.01 * 6.845038890838623
Epoch 410, val loss: 0.8642169237136841
Epoch 420, training loss: 0.13768383860588074 = 0.06934583932161331 + 0.01 * 6.833799839019775
Epoch 420, val loss: 0.8806264996528625
Epoch 430, training loss: 0.13212735950946808 = 0.06382337957620621 + 0.01 * 6.830398082733154
Epoch 430, val loss: 0.8967838883399963
Epoch 440, training loss: 0.12712962925434113 = 0.05892668291926384 + 0.01 * 6.8202948570251465
Epoch 440, val loss: 0.9127058386802673
Epoch 450, training loss: 0.12276241928339005 = 0.05456409603357315 + 0.01 * 6.819832801818848
Epoch 450, val loss: 0.9282498359680176
Epoch 460, training loss: 0.11875593662261963 = 0.05066021904349327 + 0.01 * 6.809571266174316
Epoch 460, val loss: 0.9434031844139099
Epoch 470, training loss: 0.11512352526187897 = 0.0471484512090683 + 0.01 * 6.7975077629089355
Epoch 470, val loss: 0.9582266211509705
Epoch 480, training loss: 0.11196111142635345 = 0.04395657032728195 + 0.01 * 6.800454616546631
Epoch 480, val loss: 0.9727433919906616
Epoch 490, training loss: 0.10893210023641586 = 0.041041791439056396 + 0.01 * 6.789031028747559
Epoch 490, val loss: 0.9869597554206848
Epoch 500, training loss: 0.10657688230276108 = 0.03836021572351456 + 0.01 * 6.821666717529297
Epoch 500, val loss: 1.000794529914856
Epoch 510, training loss: 0.1035575270652771 = 0.03588201105594635 + 0.01 * 6.767551898956299
Epoch 510, val loss: 1.0143520832061768
Epoch 520, training loss: 0.10133343935012817 = 0.0335676334798336 + 0.01 * 6.776580333709717
Epoch 520, val loss: 1.0276199579238892
Epoch 530, training loss: 0.09887082874774933 = 0.031380895525217056 + 0.01 * 6.748993873596191
Epoch 530, val loss: 1.0406981706619263
Epoch 540, training loss: 0.09702741354703903 = 0.029307516291737556 + 0.01 * 6.771989822387695
Epoch 540, val loss: 1.0535699129104614
Epoch 550, training loss: 0.09463011473417282 = 0.027291452512145042 + 0.01 * 6.733866214752197
Epoch 550, val loss: 1.0661723613739014
Epoch 560, training loss: 0.09278243035078049 = 0.025319846346974373 + 0.01 * 6.746258735656738
Epoch 560, val loss: 1.0785325765609741
Epoch 570, training loss: 0.09064826369285583 = 0.023366864770650864 + 0.01 * 6.728139400482178
Epoch 570, val loss: 1.090969204902649
Epoch 580, training loss: 0.08884724974632263 = 0.021440058946609497 + 0.01 * 6.740719318389893
Epoch 580, val loss: 1.1033177375793457
Epoch 590, training loss: 0.08669964224100113 = 0.019626742228865623 + 0.01 * 6.707290172576904
Epoch 590, val loss: 1.115904688835144
Epoch 600, training loss: 0.08496661484241486 = 0.01794397458434105 + 0.01 * 6.702264308929443
Epoch 600, val loss: 1.1287258863449097
Epoch 610, training loss: 0.08346552401781082 = 0.01642422378063202 + 0.01 * 6.704130172729492
Epoch 610, val loss: 1.1414763927459717
Epoch 620, training loss: 0.08217877149581909 = 0.015083514153957367 + 0.01 * 6.709526062011719
Epoch 620, val loss: 1.154276967048645
Epoch 630, training loss: 0.08074089884757996 = 0.013905496336519718 + 0.01 * 6.6835408210754395
Epoch 630, val loss: 1.16707444190979
Epoch 640, training loss: 0.07984636723995209 = 0.012880644761025906 + 0.01 * 6.696572303771973
Epoch 640, val loss: 1.1795904636383057
Epoch 650, training loss: 0.07879476994276047 = 0.011983501724898815 + 0.01 * 6.681127548217773
Epoch 650, val loss: 1.191849946975708
Epoch 660, training loss: 0.07807666063308716 = 0.011194471269845963 + 0.01 * 6.6882195472717285
Epoch 660, val loss: 1.2036043405532837
Epoch 670, training loss: 0.07727761566638947 = 0.010503273457288742 + 0.01 * 6.677434921264648
Epoch 670, val loss: 1.215018630027771
Epoch 680, training loss: 0.07650361955165863 = 0.009894028306007385 + 0.01 * 6.660959243774414
Epoch 680, val loss: 1.2259522676467896
Epoch 690, training loss: 0.07580395042896271 = 0.00935071799904108 + 0.01 * 6.645323276519775
Epoch 690, val loss: 1.2364012002944946
Epoch 700, training loss: 0.07548052817583084 = 0.008861090056598186 + 0.01 * 6.661944389343262
Epoch 700, val loss: 1.246459722518921
Epoch 710, training loss: 0.07499384880065918 = 0.008418873883783817 + 0.01 * 6.657497406005859
Epoch 710, val loss: 1.2561312913894653
Epoch 720, training loss: 0.07440238445997238 = 0.008017013780772686 + 0.01 * 6.638537406921387
Epoch 720, val loss: 1.2654005289077759
Epoch 730, training loss: 0.07399339228868484 = 0.007649012841284275 + 0.01 * 6.634438514709473
Epoch 730, val loss: 1.2743715047836304
Epoch 740, training loss: 0.07360008358955383 = 0.007310677785426378 + 0.01 * 6.628940105438232
Epoch 740, val loss: 1.2829257249832153
Epoch 750, training loss: 0.07330697774887085 = 0.006998542696237564 + 0.01 * 6.6308441162109375
Epoch 750, val loss: 1.2912654876708984
Epoch 760, training loss: 0.07295098155736923 = 0.006708720698952675 + 0.01 * 6.6242265701293945
Epoch 760, val loss: 1.2993415594100952
Epoch 770, training loss: 0.07269750535488129 = 0.006439083255827427 + 0.01 * 6.625842571258545
Epoch 770, val loss: 1.3071790933609009
Epoch 780, training loss: 0.07232047617435455 = 0.006188218016177416 + 0.01 * 6.61322546005249
Epoch 780, val loss: 1.3146480321884155
Epoch 790, training loss: 0.07221435010433197 = 0.005954135674983263 + 0.01 * 6.626020908355713
Epoch 790, val loss: 1.3220223188400269
Epoch 800, training loss: 0.07197767496109009 = 0.005736452993005514 + 0.01 * 6.624122142791748
Epoch 800, val loss: 1.3291418552398682
Epoch 810, training loss: 0.07178925722837448 = 0.005533416755497456 + 0.01 * 6.625583648681641
Epoch 810, val loss: 1.3360164165496826
Epoch 820, training loss: 0.07130475342273712 = 0.005343090742826462 + 0.01 * 6.596166133880615
Epoch 820, val loss: 1.3427324295043945
Epoch 830, training loss: 0.07129335403442383 = 0.005164023954421282 + 0.01 * 6.612933158874512
Epoch 830, val loss: 1.3493174314498901
Epoch 840, training loss: 0.07086069136857986 = 0.004995707888156176 + 0.01 * 6.586498260498047
Epoch 840, val loss: 1.3556538820266724
Epoch 850, training loss: 0.07086358219385147 = 0.00483707757666707 + 0.01 * 6.602650165557861
Epoch 850, val loss: 1.3618474006652832
Epoch 860, training loss: 0.07060018181800842 = 0.004687485750764608 + 0.01 * 6.591269493103027
Epoch 860, val loss: 1.3678739070892334
Epoch 870, training loss: 0.070444256067276 = 0.004546047654002905 + 0.01 * 6.589820861816406
Epoch 870, val loss: 1.3737009763717651
Epoch 880, training loss: 0.07050511240959167 = 0.004412500653415918 + 0.01 * 6.6092610359191895
Epoch 880, val loss: 1.3794653415679932
Epoch 890, training loss: 0.07011186331510544 = 0.004285738803446293 + 0.01 * 6.58261251449585
Epoch 890, val loss: 1.38499116897583
Epoch 900, training loss: 0.07007038593292236 = 0.004165607504546642 + 0.01 * 6.59047794342041
Epoch 900, val loss: 1.3903989791870117
Epoch 910, training loss: 0.06973998248577118 = 0.004051359370350838 + 0.01 * 6.5688629150390625
Epoch 910, val loss: 1.3957399129867554
Epoch 920, training loss: 0.06965494155883789 = 0.003942903131246567 + 0.01 * 6.57120418548584
Epoch 920, val loss: 1.400908350944519
Epoch 930, training loss: 0.06957673281431198 = 0.0038396823219954967 + 0.01 * 6.573705673217773
Epoch 930, val loss: 1.4059118032455444
Epoch 940, training loss: 0.06932748854160309 = 0.0037413761019706726 + 0.01 * 6.5586113929748535
Epoch 940, val loss: 1.4108796119689941
Epoch 950, training loss: 0.0692526251077652 = 0.0036475295200943947 + 0.01 * 6.56050968170166
Epoch 950, val loss: 1.415666103363037
Epoch 960, training loss: 0.06907420605421066 = 0.003558021504431963 + 0.01 * 6.551618576049805
Epoch 960, val loss: 1.4203739166259766
Epoch 970, training loss: 0.06908368319272995 = 0.003472542390227318 + 0.01 * 6.5611138343811035
Epoch 970, val loss: 1.4249544143676758
Epoch 980, training loss: 0.06899727880954742 = 0.003390823258087039 + 0.01 * 6.56064510345459
Epoch 980, val loss: 1.4294570684432983
Epoch 990, training loss: 0.06892991811037064 = 0.0033127199858427048 + 0.01 * 6.5617194175720215
Epoch 990, val loss: 1.4338176250457764
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.5978
Flip ASR: 0.5644/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0384881496429443 = 1.9547492265701294 + 0.01 * 8.373886108398438
Epoch 0, val loss: 1.949529767036438
Epoch 10, training loss: 2.028120756149292 = 1.944382667541504 + 0.01 * 8.373815536499023
Epoch 10, val loss: 1.9394915103912354
Epoch 20, training loss: 2.0152907371520996 = 1.9315552711486816 + 0.01 * 8.373550415039062
Epoch 20, val loss: 1.9267263412475586
Epoch 30, training loss: 1.9972150325775146 = 1.9134883880615234 + 0.01 * 8.372669219970703
Epoch 30, val loss: 1.908387541770935
Epoch 40, training loss: 1.9702558517456055 = 1.8865770101547241 + 0.01 * 8.367880821228027
Epoch 40, val loss: 1.8811925649642944
Epoch 50, training loss: 1.9312238693237305 = 1.8478752374649048 + 0.01 * 8.334866523742676
Epoch 50, val loss: 1.8437390327453613
Epoch 60, training loss: 1.8834619522094727 = 1.8018953800201416 + 0.01 * 8.156659126281738
Epoch 60, val loss: 1.8036413192749023
Epoch 70, training loss: 1.8397709131240845 = 1.7597827911376953 + 0.01 * 7.998814582824707
Epoch 70, val loss: 1.7703877687454224
Epoch 80, training loss: 1.788551688194275 = 1.7109769582748413 + 0.01 * 7.75747537612915
Epoch 80, val loss: 1.7288893461227417
Epoch 90, training loss: 1.7190089225769043 = 1.6443216800689697 + 0.01 * 7.46872091293335
Epoch 90, val loss: 1.6727101802825928
Epoch 100, training loss: 1.6306483745574951 = 1.5575288534164429 + 0.01 * 7.311956405639648
Epoch 100, val loss: 1.601565957069397
Epoch 110, training loss: 1.530757188796997 = 1.4586448669433594 + 0.01 * 7.211235046386719
Epoch 110, val loss: 1.5222846269607544
Epoch 120, training loss: 1.4298174381256104 = 1.3584403991699219 + 0.01 * 7.137700080871582
Epoch 120, val loss: 1.4431020021438599
Epoch 130, training loss: 1.3314974308013916 = 1.2604639530181885 + 0.01 * 7.103346824645996
Epoch 130, val loss: 1.3674523830413818
Epoch 140, training loss: 1.2365962266921997 = 1.1658408641815186 + 0.01 * 7.075540542602539
Epoch 140, val loss: 1.2963957786560059
Epoch 150, training loss: 1.1478207111358643 = 1.0773255825042725 + 0.01 * 7.049516677856445
Epoch 150, val loss: 1.2307404279708862
Epoch 160, training loss: 1.0677884817123413 = 0.997483491897583 + 0.01 * 7.030501842498779
Epoch 160, val loss: 1.1716471910476685
Epoch 170, training loss: 0.9959235191345215 = 0.9256985187530518 + 0.01 * 7.022499084472656
Epoch 170, val loss: 1.1180189847946167
Epoch 180, training loss: 0.9288217425346375 = 0.8586448431015015 + 0.01 * 7.017691135406494
Epoch 180, val loss: 1.067583441734314
Epoch 190, training loss: 0.863129734992981 = 0.7929651141166687 + 0.01 * 7.0164594650268555
Epoch 190, val loss: 1.0173676013946533
Epoch 200, training loss: 0.7974792718887329 = 0.7273134589195251 + 0.01 * 7.01658296585083
Epoch 200, val loss: 0.9665889143943787
Epoch 210, training loss: 0.7325233817100525 = 0.6623530983924866 + 0.01 * 7.017028331756592
Epoch 210, val loss: 0.916398823261261
Epoch 220, training loss: 0.6695345640182495 = 0.599359929561615 + 0.01 * 7.017461776733398
Epoch 220, val loss: 0.8687846064567566
Epoch 230, training loss: 0.6094139814376831 = 0.5392392873764038 + 0.01 * 7.017471790313721
Epoch 230, val loss: 0.8256751894950867
Epoch 240, training loss: 0.552419900894165 = 0.4822542071342468 + 0.01 * 7.0165696144104
Epoch 240, val loss: 0.7882222533226013
Epoch 250, training loss: 0.49870896339416504 = 0.4285614788532257 + 0.01 * 7.014747142791748
Epoch 250, val loss: 0.7571045756340027
Epoch 260, training loss: 0.4487779140472412 = 0.378644734621048 + 0.01 * 7.013316631317139
Epoch 260, val loss: 0.7325248718261719
Epoch 270, training loss: 0.4031134247779846 = 0.33301645517349243 + 0.01 * 7.009696960449219
Epoch 270, val loss: 0.7142390012741089
Epoch 280, training loss: 0.36191242933273315 = 0.29185572266578674 + 0.01 * 7.005671501159668
Epoch 280, val loss: 0.701336681842804
Epoch 290, training loss: 0.3250702917575836 = 0.25503355264663696 + 0.01 * 7.003674507141113
Epoch 290, val loss: 0.6930268406867981
Epoch 300, training loss: 0.29238682985305786 = 0.22241142392158508 + 0.01 * 6.997541904449463
Epoch 300, val loss: 0.6886444687843323
Epoch 310, training loss: 0.26389631628990173 = 0.19392745196819305 + 0.01 * 6.996885776519775
Epoch 310, val loss: 0.6876716613769531
Epoch 320, training loss: 0.2392858862876892 = 0.16940808296203613 + 0.01 * 6.987781047821045
Epoch 320, val loss: 0.6897806525230408
Epoch 330, training loss: 0.21828234195709229 = 0.14847514033317566 + 0.01 * 6.980719566345215
Epoch 330, val loss: 0.6945013403892517
Epoch 340, training loss: 0.200361430644989 = 0.13061116635799408 + 0.01 * 6.975025653839111
Epoch 340, val loss: 0.7013207077980042
Epoch 350, training loss: 0.1849055290222168 = 0.11529829353094101 + 0.01 * 6.960722923278809
Epoch 350, val loss: 0.7097811698913574
Epoch 360, training loss: 0.17164702713489532 = 0.10209614038467407 + 0.01 * 6.9550886154174805
Epoch 360, val loss: 0.719431459903717
Epoch 370, training loss: 0.16008904576301575 = 0.09066233038902283 + 0.01 * 6.942672252655029
Epoch 370, val loss: 0.7300037741661072
Epoch 380, training loss: 0.15003716945648193 = 0.080718033015728 + 0.01 * 6.93191385269165
Epoch 380, val loss: 0.7411841154098511
Epoch 390, training loss: 0.14122112095355988 = 0.07204955816268921 + 0.01 * 6.91715669631958
Epoch 390, val loss: 0.7529106736183167
Epoch 400, training loss: 0.13363942503929138 = 0.0644785612821579 + 0.01 * 6.916086673736572
Epoch 400, val loss: 0.7650092840194702
Epoch 410, training loss: 0.12703146040439606 = 0.05786478891968727 + 0.01 * 6.9166669845581055
Epoch 410, val loss: 0.7773777842521667
Epoch 420, training loss: 0.1207112967967987 = 0.05209343135356903 + 0.01 * 6.861786365509033
Epoch 420, val loss: 0.7898487448692322
Epoch 430, training loss: 0.11555729806423187 = 0.047045525163412094 + 0.01 * 6.85117769241333
Epoch 430, val loss: 0.8023194670677185
Epoch 440, training loss: 0.11142568290233612 = 0.04262399673461914 + 0.01 * 6.880168914794922
Epoch 440, val loss: 0.8147446513175964
Epoch 450, training loss: 0.10700394213199615 = 0.038748208433389664 + 0.01 * 6.825573444366455
Epoch 450, val loss: 0.8270266056060791
Epoch 460, training loss: 0.10350558161735535 = 0.03533486649394035 + 0.01 * 6.817071437835693
Epoch 460, val loss: 0.8391462564468384
Epoch 470, training loss: 0.10047271847724915 = 0.032324742525815964 + 0.01 * 6.814797401428223
Epoch 470, val loss: 0.8510564565658569
Epoch 480, training loss: 0.09762310981750488 = 0.0296633243560791 + 0.01 * 6.795979022979736
Epoch 480, val loss: 0.8626596331596375
Epoch 490, training loss: 0.09526730328798294 = 0.027299774810671806 + 0.01 * 6.7967529296875
Epoch 490, val loss: 0.8739802837371826
Epoch 500, training loss: 0.09294106811285019 = 0.025194043293595314 + 0.01 * 6.774703025817871
Epoch 500, val loss: 0.8850430250167847
Epoch 510, training loss: 0.0910579264163971 = 0.02331153117120266 + 0.01 * 6.77463960647583
Epoch 510, val loss: 0.8958786725997925
Epoch 520, training loss: 0.08928874880075455 = 0.021625645458698273 + 0.01 * 6.766310691833496
Epoch 520, val loss: 0.9063379764556885
Epoch 530, training loss: 0.08765897899866104 = 0.020113497972488403 + 0.01 * 6.754548072814941
Epoch 530, val loss: 0.9165025353431702
Epoch 540, training loss: 0.08630729466676712 = 0.01875310391187668 + 0.01 * 6.7554192543029785
Epoch 540, val loss: 0.9263629913330078
Epoch 550, training loss: 0.08483578264713287 = 0.017524972558021545 + 0.01 * 6.731081008911133
Epoch 550, val loss: 0.9359034895896912
Epoch 560, training loss: 0.08385466784238815 = 0.01641218364238739 + 0.01 * 6.744248390197754
Epoch 560, val loss: 0.9451559782028198
Epoch 570, training loss: 0.08266784995794296 = 0.015402217395603657 + 0.01 * 6.726563453674316
Epoch 570, val loss: 0.9541215896606445
Epoch 580, training loss: 0.08168422430753708 = 0.014481985941529274 + 0.01 * 6.720223903656006
Epoch 580, val loss: 0.9628438353538513
Epoch 590, training loss: 0.0805954784154892 = 0.013642861507833004 + 0.01 * 6.6952619552612305
Epoch 590, val loss: 0.9712976813316345
Epoch 600, training loss: 0.07986313849687576 = 0.01287473738193512 + 0.01 * 6.698840141296387
Epoch 600, val loss: 0.9794843792915344
Epoch 610, training loss: 0.07916419208049774 = 0.012170890346169472 + 0.01 * 6.6993303298950195
Epoch 610, val loss: 0.9874274134635925
Epoch 620, training loss: 0.07837478816509247 = 0.011524549685418606 + 0.01 * 6.685023784637451
Epoch 620, val loss: 0.9951282739639282
Epoch 630, training loss: 0.07787945866584778 = 0.010928889736533165 + 0.01 * 6.695056915283203
Epoch 630, val loss: 1.00260329246521
Epoch 640, training loss: 0.07712432742118835 = 0.01037910208106041 + 0.01 * 6.674522876739502
Epoch 640, val loss: 1.0099115371704102
Epoch 650, training loss: 0.07656997442245483 = 0.009869925677776337 + 0.01 * 6.670004844665527
Epoch 650, val loss: 1.0170032978057861
Epoch 660, training loss: 0.07594306766986847 = 0.009398099035024643 + 0.01 * 6.6544976234436035
Epoch 660, val loss: 1.0239253044128418
Epoch 670, training loss: 0.0755893886089325 = 0.008959462866187096 + 0.01 * 6.662992477416992
Epoch 670, val loss: 1.0306438207626343
Epoch 680, training loss: 0.07513907551765442 = 0.00855308212339878 + 0.01 * 6.658599853515625
Epoch 680, val loss: 1.0373070240020752
Epoch 690, training loss: 0.07448925822973251 = 0.008172967471182346 + 0.01 * 6.631629467010498
Epoch 690, val loss: 1.0436514616012573
Epoch 700, training loss: 0.07430139929056168 = 0.00781822856515646 + 0.01 * 6.648316860198975
Epoch 700, val loss: 1.0499190092086792
Epoch 710, training loss: 0.07406356930732727 = 0.007489179726690054 + 0.01 * 6.6574387550354
Epoch 710, val loss: 1.0560277700424194
Epoch 720, training loss: 0.07348650693893433 = 0.007180987857282162 + 0.01 * 6.630552291870117
Epoch 720, val loss: 1.0619279146194458
Epoch 730, training loss: 0.07299619168043137 = 0.006891967263072729 + 0.01 * 6.6104230880737305
Epoch 730, val loss: 1.0677646398544312
Epoch 740, training loss: 0.0729125589132309 = 0.00662180595099926 + 0.01 * 6.629075527191162
Epoch 740, val loss: 1.073394775390625
Epoch 750, training loss: 0.07256527245044708 = 0.006369268987327814 + 0.01 * 6.619600296020508
Epoch 750, val loss: 1.0788772106170654
Epoch 760, training loss: 0.07222919166088104 = 0.006130840163677931 + 0.01 * 6.609835624694824
Epoch 760, val loss: 1.0842983722686768
Epoch 770, training loss: 0.07196688652038574 = 0.005906978622078896 + 0.01 * 6.605990886688232
Epoch 770, val loss: 1.0895766019821167
Epoch 780, training loss: 0.07153908163309097 = 0.00569657888263464 + 0.01 * 6.584249973297119
Epoch 780, val loss: 1.09469735622406
Epoch 790, training loss: 0.07123351842164993 = 0.005498059093952179 + 0.01 * 6.573546409606934
Epoch 790, val loss: 1.099706768989563
Epoch 800, training loss: 0.07154154032468796 = 0.005311079788953066 + 0.01 * 6.623046398162842
Epoch 800, val loss: 1.1046407222747803
Epoch 810, training loss: 0.07091966271400452 = 0.005134745500981808 + 0.01 * 6.578491687774658
Epoch 810, val loss: 1.1094051599502563
Epoch 820, training loss: 0.07066375017166138 = 0.004967918619513512 + 0.01 * 6.569582939147949
Epoch 820, val loss: 1.1140989065170288
Epoch 830, training loss: 0.07040532678365707 = 0.004809766076505184 + 0.01 * 6.559556484222412
Epoch 830, val loss: 1.1186411380767822
Epoch 840, training loss: 0.07031393051147461 = 0.004660540726035833 + 0.01 * 6.565339088439941
Epoch 840, val loss: 1.1231434345245361
Epoch 850, training loss: 0.07058778405189514 = 0.004519031848758459 + 0.01 * 6.606875419616699
Epoch 850, val loss: 1.1274880170822144
Epoch 860, training loss: 0.06986138224601746 = 0.004385586827993393 + 0.01 * 6.547579765319824
Epoch 860, val loss: 1.131773591041565
Epoch 870, training loss: 0.06994462758302689 = 0.004258443601429462 + 0.01 * 6.568618297576904
Epoch 870, val loss: 1.1359949111938477
Epoch 880, training loss: 0.06953609734773636 = 0.004137435462325811 + 0.01 * 6.5398664474487305
Epoch 880, val loss: 1.1401211023330688
Epoch 890, training loss: 0.06974770873785019 = 0.004022762645035982 + 0.01 * 6.5724945068359375
Epoch 890, val loss: 1.1441025733947754
Epoch 900, training loss: 0.06944627314805984 = 0.003913608379662037 + 0.01 * 6.5532660484313965
Epoch 900, val loss: 1.1480189561843872
Epoch 910, training loss: 0.06910008937120438 = 0.0038097132928669453 + 0.01 * 6.5290374755859375
Epoch 910, val loss: 1.151906132698059
Epoch 920, training loss: 0.06887610256671906 = 0.0037104918155819178 + 0.01 * 6.516561031341553
Epoch 920, val loss: 1.1556329727172852
Epoch 930, training loss: 0.06876920908689499 = 0.003615950932726264 + 0.01 * 6.515325546264648
Epoch 930, val loss: 1.1593738794326782
Epoch 940, training loss: 0.0687529444694519 = 0.003525372361764312 + 0.01 * 6.522757530212402
Epoch 940, val loss: 1.1629914045333862
Epoch 950, training loss: 0.06863323599100113 = 0.0034389328211545944 + 0.01 * 6.519430160522461
Epoch 950, val loss: 1.1665432453155518
Epoch 960, training loss: 0.06843589246273041 = 0.003356202505528927 + 0.01 * 6.507969379425049
Epoch 960, val loss: 1.1700294017791748
Epoch 970, training loss: 0.06850233674049377 = 0.0032772154081612825 + 0.01 * 6.522512435913086
Epoch 970, val loss: 1.1734381914138794
Epoch 980, training loss: 0.06847662478685379 = 0.0032012208830565214 + 0.01 * 6.52754020690918
Epoch 980, val loss: 1.1767884492874146
Epoch 990, training loss: 0.0681476816534996 = 0.0031287248712033033 + 0.01 * 6.501896381378174
Epoch 990, val loss: 1.179983377456665
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.7565
Flip ASR: 0.7244/225 nodes
The final ASR:0.64453, 0.07950, Accuracy:0.79630, 0.02688
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11650])
remove edge: torch.Size([2, 9544])
updated graph: torch.Size([2, 10638])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
The final ASR:0.97786, 0.00522, Accuracy:0.82963, 0.00605
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.5 der2:0.3 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0391311645507812 = 1.9553933143615723 + 0.01 * 8.373785018920898
Epoch 0, val loss: 1.9575227499008179
Epoch 10, training loss: 2.0287418365478516 = 1.9450048208236694 + 0.01 * 8.373702049255371
Epoch 10, val loss: 1.9477943181991577
Epoch 20, training loss: 2.015993118286133 = 1.9322590827941895 + 0.01 * 8.373408317565918
Epoch 20, val loss: 1.9354666471481323
Epoch 30, training loss: 1.9977723360061646 = 1.9140459299087524 + 0.01 * 8.372642517089844
Epoch 30, val loss: 1.9175491333007812
Epoch 40, training loss: 1.9702911376953125 = 1.886596918106079 + 0.01 * 8.369425773620605
Epoch 40, val loss: 1.8907251358032227
Epoch 50, training loss: 1.930602788925171 = 1.8471364974975586 + 0.01 * 8.346633911132812
Epoch 50, val loss: 1.853843092918396
Epoch 60, training loss: 1.8830960988998413 = 1.8011069297790527 + 0.01 * 8.198911666870117
Epoch 60, val loss: 1.8148621320724487
Epoch 70, training loss: 1.8384063243865967 = 1.7606827020645142 + 0.01 * 7.772364616394043
Epoch 70, val loss: 1.7824933528900146
Epoch 80, training loss: 1.7867077589035034 = 1.7135061025619507 + 0.01 * 7.320170879364014
Epoch 80, val loss: 1.739862322807312
Epoch 90, training loss: 1.7197164297103882 = 1.6491365432739258 + 0.01 * 7.057989597320557
Epoch 90, val loss: 1.683742642402649
Epoch 100, training loss: 1.6344478130340576 = 1.5646973848342896 + 0.01 * 6.975048542022705
Epoch 100, val loss: 1.613779902458191
Epoch 110, training loss: 1.5355833768844604 = 1.466072916984558 + 0.01 * 6.951051712036133
Epoch 110, val loss: 1.5329492092132568
Epoch 120, training loss: 1.4324129819869995 = 1.3630648851394653 + 0.01 * 6.93480920791626
Epoch 120, val loss: 1.4484615325927734
Epoch 130, training loss: 1.3292171955108643 = 1.2600290775299072 + 0.01 * 6.918816566467285
Epoch 130, val loss: 1.3652266263961792
Epoch 140, training loss: 1.225917100906372 = 1.1568866968154907 + 0.01 * 6.903038024902344
Epoch 140, val loss: 1.28231680393219
Epoch 150, training loss: 1.1248739957809448 = 1.0559834241867065 + 0.01 * 6.88906192779541
Epoch 150, val loss: 1.2018367052078247
Epoch 160, training loss: 1.029630422592163 = 0.9608228206634521 + 0.01 * 6.880758285522461
Epoch 160, val loss: 1.1262065172195435
Epoch 170, training loss: 0.9417723417282104 = 0.8730061054229736 + 0.01 * 6.876622200012207
Epoch 170, val loss: 1.056396722793579
Epoch 180, training loss: 0.8609151840209961 = 0.7921732068061829 + 0.01 * 6.8741984367370605
Epoch 180, val loss: 0.9923810958862305
Epoch 190, training loss: 0.7862974405288696 = 0.7175680994987488 + 0.01 * 6.8729352951049805
Epoch 190, val loss: 0.9341554045677185
Epoch 200, training loss: 0.7173509001731873 = 0.648628830909729 + 0.01 * 6.872208595275879
Epoch 200, val loss: 0.8820192813873291
Epoch 210, training loss: 0.6532447934150696 = 0.5845280885696411 + 0.01 * 6.871669292449951
Epoch 210, val loss: 0.8364939093589783
Epoch 220, training loss: 0.5931252241134644 = 0.5244151949882507 + 0.01 * 6.870999813079834
Epoch 220, val loss: 0.7969793081283569
Epoch 230, training loss: 0.5366289019584656 = 0.46792837977409363 + 0.01 * 6.870051383972168
Epoch 230, val loss: 0.763557493686676
Epoch 240, training loss: 0.4839901328086853 = 0.4153040051460266 + 0.01 * 6.868614673614502
Epoch 240, val loss: 0.7360256910324097
Epoch 250, training loss: 0.435866117477417 = 0.367203950881958 + 0.01 * 6.866218566894531
Epoch 250, val loss: 0.714699387550354
Epoch 260, training loss: 0.3927765488624573 = 0.32415375113487244 + 0.01 * 6.86228084564209
Epoch 260, val loss: 0.6997573971748352
Epoch 270, training loss: 0.3547235131263733 = 0.286143034696579 + 0.01 * 6.858049392700195
Epoch 270, val loss: 0.6910329461097717
Epoch 280, training loss: 0.3209874927997589 = 0.25249841809272766 + 0.01 * 6.848906993865967
Epoch 280, val loss: 0.6876053214073181
Epoch 290, training loss: 0.2907828688621521 = 0.2223711758852005 + 0.01 * 6.841170310974121
Epoch 290, val loss: 0.6884515881538391
Epoch 300, training loss: 0.2633875906467438 = 0.19513903558254242 + 0.01 * 6.824856758117676
Epoch 300, val loss: 0.6928502321243286
Epoch 310, training loss: 0.2387610822916031 = 0.17060421407222748 + 0.01 * 6.8156867027282715
Epoch 310, val loss: 0.7001257538795471
Epoch 320, training loss: 0.21682298183441162 = 0.1488415151834488 + 0.01 * 6.798146724700928
Epoch 320, val loss: 0.7097679972648621
Epoch 330, training loss: 0.19777759909629822 = 0.12991927564144135 + 0.01 * 6.78583288192749
Epoch 330, val loss: 0.7212705016136169
Epoch 340, training loss: 0.1814497411251068 = 0.11372632533311844 + 0.01 * 6.772341251373291
Epoch 340, val loss: 0.7340893745422363
Epoch 350, training loss: 0.16748763620853424 = 0.09996399283409119 + 0.01 * 6.752364158630371
Epoch 350, val loss: 0.7477912306785583
Epoch 360, training loss: 0.15579962730407715 = 0.08826938271522522 + 0.01 * 6.753025054931641
Epoch 360, val loss: 0.7620642185211182
Epoch 370, training loss: 0.1456620693206787 = 0.07829289883375168 + 0.01 * 6.736916542053223
Epoch 370, val loss: 0.7767176628112793
Epoch 380, training loss: 0.13711333274841309 = 0.0697411298751831 + 0.01 * 6.737220764160156
Epoch 380, val loss: 0.7915876507759094
Epoch 390, training loss: 0.1295854151248932 = 0.062370043247938156 + 0.01 * 6.721537113189697
Epoch 390, val loss: 0.8065797686576843
Epoch 400, training loss: 0.12322953343391418 = 0.05598505213856697 + 0.01 * 6.724447727203369
Epoch 400, val loss: 0.8216079473495483
Epoch 410, training loss: 0.11746279895305634 = 0.05043720826506615 + 0.01 * 6.702558517456055
Epoch 410, val loss: 0.8364599347114563
Epoch 420, training loss: 0.11251799762248993 = 0.04559515044093132 + 0.01 * 6.692285060882568
Epoch 420, val loss: 0.8511894941329956
Epoch 430, training loss: 0.10821960866451263 = 0.0413578599691391 + 0.01 * 6.6861748695373535
Epoch 430, val loss: 0.8657621145248413
Epoch 440, training loss: 0.10467840731143951 = 0.03764328733086586 + 0.01 * 6.703512668609619
Epoch 440, val loss: 0.8800338506698608
Epoch 450, training loss: 0.10121756047010422 = 0.03437662124633789 + 0.01 * 6.684093952178955
Epoch 450, val loss: 0.894098699092865
Epoch 460, training loss: 0.09824283421039581 = 0.03149228170514107 + 0.01 * 6.675055027008057
Epoch 460, val loss: 0.9077767133712769
Epoch 470, training loss: 0.09576895087957382 = 0.02893645316362381 + 0.01 * 6.6832499504089355
Epoch 470, val loss: 0.9210940003395081
Epoch 480, training loss: 0.09328611940145493 = 0.026666982099413872 + 0.01 * 6.661913871765137
Epoch 480, val loss: 0.9341197609901428
Epoch 490, training loss: 0.09122233837842941 = 0.024643288925290108 + 0.01 * 6.657905101776123
Epoch 490, val loss: 0.9467088580131531
Epoch 500, training loss: 0.08935549110174179 = 0.02283385582268238 + 0.01 * 6.652163982391357
Epoch 500, val loss: 0.9589523673057556
Epoch 510, training loss: 0.08776457607746124 = 0.021207667887210846 + 0.01 * 6.655691146850586
Epoch 510, val loss: 0.9709237813949585
Epoch 520, training loss: 0.08624677360057831 = 0.019748156890273094 + 0.01 * 6.6498613357543945
Epoch 520, val loss: 0.9825358986854553
Epoch 530, training loss: 0.08485753834247589 = 0.018430789932608604 + 0.01 * 6.642674446105957
Epoch 530, val loss: 0.9937930703163147
Epoch 540, training loss: 0.08361543715000153 = 0.017238322645425797 + 0.01 * 6.637711048126221
Epoch 540, val loss: 1.0047740936279297
Epoch 550, training loss: 0.08263583481311798 = 0.016155850142240524 + 0.01 * 6.647998809814453
Epoch 550, val loss: 1.0154813528060913
Epoch 560, training loss: 0.08149462193250656 = 0.015171581879258156 + 0.01 * 6.632304668426514
Epoch 560, val loss: 1.0258897542953491
Epoch 570, training loss: 0.08055689930915833 = 0.014273987151682377 + 0.01 * 6.628291130065918
Epoch 570, val loss: 1.0360256433486938
Epoch 580, training loss: 0.0796678364276886 = 0.013453750871121883 + 0.01 * 6.621408462524414
Epoch 580, val loss: 1.0458719730377197
Epoch 590, training loss: 0.07891815900802612 = 0.01270324643701315 + 0.01 * 6.621491432189941
Epoch 590, val loss: 1.0555040836334229
Epoch 600, training loss: 0.0782117024064064 = 0.012014776468276978 + 0.01 * 6.619692802429199
Epoch 600, val loss: 1.0648661851882935
Epoch 610, training loss: 0.07762806117534637 = 0.011381844989955425 + 0.01 * 6.624622344970703
Epoch 610, val loss: 1.0740094184875488
Epoch 620, training loss: 0.0769314169883728 = 0.010799014940857887 + 0.01 * 6.613240718841553
Epoch 620, val loss: 1.0829801559448242
Epoch 630, training loss: 0.07627574354410172 = 0.01026096474379301 + 0.01 * 6.601478099822998
Epoch 630, val loss: 1.0916959047317505
Epoch 640, training loss: 0.07588490843772888 = 0.009763281792402267 + 0.01 * 6.6121625900268555
Epoch 640, val loss: 1.1002323627471924
Epoch 650, training loss: 0.07526838779449463 = 0.009302705526351929 + 0.01 * 6.596568584442139
Epoch 650, val loss: 1.1084991693496704
Epoch 660, training loss: 0.07500117272138596 = 0.008875253610312939 + 0.01 * 6.6125922203063965
Epoch 660, val loss: 1.1166218519210815
Epoch 670, training loss: 0.07441180944442749 = 0.008478390984237194 + 0.01 * 6.593342304229736
Epoch 670, val loss: 1.1245111227035522
Epoch 680, training loss: 0.07398159801959991 = 0.008108967915177345 + 0.01 * 6.587263107299805
Epoch 680, val loss: 1.1322730779647827
Epoch 690, training loss: 0.07371558994054794 = 0.0077643683180212975 + 0.01 * 6.595122337341309
Epoch 690, val loss: 1.1398637294769287
Epoch 700, training loss: 0.07336050271987915 = 0.0074427067302167416 + 0.01 * 6.591779708862305
Epoch 700, val loss: 1.1472071409225464
Epoch 710, training loss: 0.07298705726861954 = 0.007142489310353994 + 0.01 * 6.584456443786621
Epoch 710, val loss: 1.1543989181518555
Epoch 720, training loss: 0.07259003818035126 = 0.006861343514174223 + 0.01 * 6.572869300842285
Epoch 720, val loss: 1.1614177227020264
Epoch 730, training loss: 0.07259924709796906 = 0.00659782811999321 + 0.01 * 6.600142478942871
Epoch 730, val loss: 1.1683423519134521
Epoch 740, training loss: 0.07199043035507202 = 0.006350897718220949 + 0.01 * 6.563953399658203
Epoch 740, val loss: 1.1750133037567139
Epoch 750, training loss: 0.0718328058719635 = 0.006119051482528448 + 0.01 * 6.571375846862793
Epoch 750, val loss: 1.1815358400344849
Epoch 760, training loss: 0.07160625606775284 = 0.005901031196117401 + 0.01 * 6.570522785186768
Epoch 760, val loss: 1.1879427433013916
Epoch 770, training loss: 0.0713503435254097 = 0.005695787258446217 + 0.01 * 6.565455436706543
Epoch 770, val loss: 1.1942741870880127
Epoch 780, training loss: 0.07115600258111954 = 0.005502103362232447 + 0.01 * 6.565390110015869
Epoch 780, val loss: 1.200339436531067
Epoch 790, training loss: 0.07089360803365707 = 0.00531949894502759 + 0.01 * 6.557411193847656
Epoch 790, val loss: 1.206346869468689
Epoch 800, training loss: 0.07061590254306793 = 0.005147046875208616 + 0.01 * 6.5468854904174805
Epoch 800, val loss: 1.2122058868408203
Epoch 810, training loss: 0.07053890824317932 = 0.004983694292604923 + 0.01 * 6.5555219650268555
Epoch 810, val loss: 1.2179597616195679
Epoch 820, training loss: 0.07024505734443665 = 0.004829084035009146 + 0.01 * 6.54159688949585
Epoch 820, val loss: 1.223618507385254
Epoch 830, training loss: 0.0701415091753006 = 0.0046827346086502075 + 0.01 * 6.545877933502197
Epoch 830, val loss: 1.229109764099121
Epoch 840, training loss: 0.06991353631019592 = 0.004543955437839031 + 0.01 * 6.53695821762085
Epoch 840, val loss: 1.2345929145812988
Epoch 850, training loss: 0.06977081298828125 = 0.004412233829498291 + 0.01 * 6.535858154296875
Epoch 850, val loss: 1.2397693395614624
Epoch 860, training loss: 0.07001230120658875 = 0.0042870803736150265 + 0.01 * 6.572522163391113
Epoch 860, val loss: 1.2449579238891602
Epoch 870, training loss: 0.0693681612610817 = 0.004168265033513308 + 0.01 * 6.519989967346191
Epoch 870, val loss: 1.2500402927398682
Epoch 880, training loss: 0.06914428621530533 = 0.004055304918438196 + 0.01 * 6.5088982582092285
Epoch 880, val loss: 1.254974126815796
Epoch 890, training loss: 0.06923352181911469 = 0.003947599325329065 + 0.01 * 6.528592109680176
Epoch 890, val loss: 1.2598674297332764
Epoch 900, training loss: 0.06919869780540466 = 0.003844907972961664 + 0.01 * 6.535378932952881
Epoch 900, val loss: 1.2646139860153198
Epoch 910, training loss: 0.06896989047527313 = 0.0037471833638846874 + 0.01 * 6.522271156311035
Epoch 910, val loss: 1.2692344188690186
Epoch 920, training loss: 0.06856603920459747 = 0.0036539908032864332 + 0.01 * 6.491205215454102
Epoch 920, val loss: 1.2738213539123535
Epoch 930, training loss: 0.06850898265838623 = 0.0035648555494844913 + 0.01 * 6.494412899017334
Epoch 930, val loss: 1.2782560586929321
Epoch 940, training loss: 0.06863550841808319 = 0.0034799084533005953 + 0.01 * 6.515560150146484
Epoch 940, val loss: 1.282735824584961
Epoch 950, training loss: 0.06833004206418991 = 0.003398588392883539 + 0.01 * 6.49314546585083
Epoch 950, val loss: 1.286995530128479
Epoch 960, training loss: 0.0681566670536995 = 0.003320727264508605 + 0.01 * 6.4835944175720215
Epoch 960, val loss: 1.2912886142730713
Epoch 970, training loss: 0.06810268759727478 = 0.0032463534735143185 + 0.01 * 6.485633850097656
Epoch 970, val loss: 1.2954381704330444
Epoch 980, training loss: 0.06800588220357895 = 0.0031749308109283447 + 0.01 * 6.483095645904541
Epoch 980, val loss: 1.2995171546936035
Epoch 990, training loss: 0.06797290593385696 = 0.0031064741779118776 + 0.01 * 6.486643314361572
Epoch 990, val loss: 1.3036234378814697
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.7159
Flip ASR: 0.6578/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0492007732391357 = 1.9654630422592163 + 0.01 * 8.373783111572266
Epoch 0, val loss: 1.9661026000976562
Epoch 10, training loss: 2.0385568141937256 = 1.954819917678833 + 0.01 * 8.373689651489258
Epoch 10, val loss: 1.9547587633132935
Epoch 20, training loss: 2.0256590843200684 = 1.9419251680374146 + 0.01 * 8.373395919799805
Epoch 20, val loss: 1.9406574964523315
Epoch 30, training loss: 2.007885217666626 = 1.9241585731506348 + 0.01 * 8.372661590576172
Epoch 30, val loss: 1.921049952507019
Epoch 40, training loss: 1.9817266464233398 = 1.898031234741211 + 0.01 * 8.369543075561523
Epoch 40, val loss: 1.8925471305847168
Epoch 50, training loss: 1.9436960220336914 = 1.8602224588394165 + 0.01 * 8.347354888916016
Epoch 50, val loss: 1.8527395725250244
Epoch 60, training loss: 1.8950203657150269 = 1.8129740953445435 + 0.01 * 8.204631805419922
Epoch 60, val loss: 1.806502342224121
Epoch 70, training loss: 1.8459842205047607 = 1.7682857513427734 + 0.01 * 7.769850254058838
Epoch 70, val loss: 1.7669697999954224
Epoch 80, training loss: 1.7958834171295166 = 1.722943663597107 + 0.01 * 7.293978214263916
Epoch 80, val loss: 1.727452278137207
Epoch 90, training loss: 1.7312207221984863 = 1.6602623462677002 + 0.01 * 7.095841884613037
Epoch 90, val loss: 1.6740479469299316
Epoch 100, training loss: 1.6462095975875854 = 1.5759292840957642 + 0.01 * 7.028029441833496
Epoch 100, val loss: 1.6037408113479614
Epoch 110, training loss: 1.5429595708847046 = 1.4732015132904053 + 0.01 * 6.975807189941406
Epoch 110, val loss: 1.519113540649414
Epoch 120, training loss: 1.4352372884750366 = 1.3658002614974976 + 0.01 * 6.943701267242432
Epoch 120, val loss: 1.433009386062622
Epoch 130, training loss: 1.333605408668518 = 1.2643481492996216 + 0.01 * 6.925729274749756
Epoch 130, val loss: 1.3559156656265259
Epoch 140, training loss: 1.2408162355422974 = 1.1717177629470825 + 0.01 * 6.90984582901001
Epoch 140, val loss: 1.2890547513961792
Epoch 150, training loss: 1.1563503742218018 = 1.0874333381652832 + 0.01 * 6.891707420349121
Epoch 150, val loss: 1.2289329767227173
Epoch 160, training loss: 1.0793215036392212 = 1.0105735063552856 + 0.01 * 6.87480354309082
Epoch 160, val loss: 1.174332857131958
Epoch 170, training loss: 1.0087844133377075 = 0.940165102481842 + 0.01 * 6.861934185028076
Epoch 170, val loss: 1.1245639324188232
Epoch 180, training loss: 0.9427890181541443 = 0.8742481470108032 + 0.01 * 6.854087829589844
Epoch 180, val loss: 1.078084945678711
Epoch 190, training loss: 0.8782624006271362 = 0.8098013401031494 + 0.01 * 6.846104145050049
Epoch 190, val loss: 1.032572865486145
Epoch 200, training loss: 0.8126814961433411 = 0.7442846298217773 + 0.01 * 6.839688777923584
Epoch 200, val loss: 0.9858130812644958
Epoch 210, training loss: 0.7454660534858704 = 0.6771339774131775 + 0.01 * 6.833209037780762
Epoch 210, val loss: 0.9372356534004211
Epoch 220, training loss: 0.6779884696006775 = 0.6097197532653809 + 0.01 * 6.826874256134033
Epoch 220, val loss: 0.8882560133934021
Epoch 230, training loss: 0.6124229431152344 = 0.5442360639572144 + 0.01 * 6.818688869476318
Epoch 230, val loss: 0.8413789868354797
Epoch 240, training loss: 0.5505252480506897 = 0.482317179441452 + 0.01 * 6.820806980133057
Epoch 240, val loss: 0.7991148233413696
Epoch 250, training loss: 0.4929850697517395 = 0.42492619156837463 + 0.01 * 6.805886268615723
Epoch 250, val loss: 0.7633604407310486
Epoch 260, training loss: 0.44053831696510315 = 0.3725895881652832 + 0.01 * 6.794872283935547
Epoch 260, val loss: 0.7348556518554688
Epoch 270, training loss: 0.39343875646591187 = 0.3255389332771301 + 0.01 * 6.789980888366699
Epoch 270, val loss: 0.7137277126312256
Epoch 280, training loss: 0.3517100214958191 = 0.2838630974292755 + 0.01 * 6.784692764282227
Epoch 280, val loss: 0.699600338935852
Epoch 290, training loss: 0.3152022361755371 = 0.2473939061164856 + 0.01 * 6.780832767486572
Epoch 290, val loss: 0.6918025016784668
Epoch 300, training loss: 0.2836228013038635 = 0.21584735810756683 + 0.01 * 6.777544975280762
Epoch 300, val loss: 0.689396321773529
Epoch 310, training loss: 0.2564706802368164 = 0.18879330158233643 + 0.01 * 6.767739295959473
Epoch 310, val loss: 0.6916128396987915
Epoch 320, training loss: 0.2334408462047577 = 0.16572505235671997 + 0.01 * 6.771578788757324
Epoch 320, val loss: 0.6978127360343933
Epoch 330, training loss: 0.21370112895965576 = 0.14621509611606598 + 0.01 * 6.748602867126465
Epoch 330, val loss: 0.7069869041442871
Epoch 340, training loss: 0.1973162591457367 = 0.12982246279716492 + 0.01 * 6.749378681182861
Epoch 340, val loss: 0.7183536887168884
Epoch 350, training loss: 0.18342313170433044 = 0.11601922661066055 + 0.01 * 6.740391731262207
Epoch 350, val loss: 0.7312430143356323
Epoch 360, training loss: 0.17185261845588684 = 0.10430819541215897 + 0.01 * 6.754443168640137
Epoch 360, val loss: 0.7448394894599915
Epoch 370, training loss: 0.1616518497467041 = 0.0943143218755722 + 0.01 * 6.733752250671387
Epoch 370, val loss: 0.7589693069458008
Epoch 380, training loss: 0.15290948748588562 = 0.08568121492862701 + 0.01 * 6.722826957702637
Epoch 380, val loss: 0.7733187079429626
Epoch 390, training loss: 0.14547371864318848 = 0.07814253121614456 + 0.01 * 6.733119487762451
Epoch 390, val loss: 0.7876766920089722
Epoch 400, training loss: 0.13863490521907806 = 0.07151509076356888 + 0.01 * 6.711981773376465
Epoch 400, val loss: 0.802118182182312
Epoch 410, training loss: 0.1327681839466095 = 0.06563936918973923 + 0.01 * 6.712881088256836
Epoch 410, val loss: 0.8165042400360107
Epoch 420, training loss: 0.12747524678707123 = 0.06039979308843613 + 0.01 * 6.707545757293701
Epoch 420, val loss: 0.8309265971183777
Epoch 430, training loss: 0.1228700503706932 = 0.055708594620227814 + 0.01 * 6.716145992279053
Epoch 430, val loss: 0.8451897501945496
Epoch 440, training loss: 0.1184670478105545 = 0.05149954929947853 + 0.01 * 6.696750640869141
Epoch 440, val loss: 0.8594970107078552
Epoch 450, training loss: 0.11459741741418839 = 0.04767462611198425 + 0.01 * 6.69227933883667
Epoch 450, val loss: 0.8735727071762085
Epoch 460, training loss: 0.11119387298822403 = 0.04416154325008392 + 0.01 * 6.703232765197754
Epoch 460, val loss: 0.8876760601997375
Epoch 470, training loss: 0.1077340766787529 = 0.040939249098300934 + 0.01 * 6.679482936859131
Epoch 470, val loss: 0.9014410972595215
Epoch 480, training loss: 0.10471808910369873 = 0.03796926885843277 + 0.01 * 6.674881935119629
Epoch 480, val loss: 0.9151209592819214
Epoch 490, training loss: 0.10199372470378876 = 0.03521698713302612 + 0.01 * 6.677673816680908
Epoch 490, val loss: 0.9286986589431763
Epoch 500, training loss: 0.09931734204292297 = 0.03264998272061348 + 0.01 * 6.666736125946045
Epoch 500, val loss: 0.94207763671875
Epoch 510, training loss: 0.09683239459991455 = 0.030252085998654366 + 0.01 * 6.658030986785889
Epoch 510, val loss: 0.9554657936096191
Epoch 520, training loss: 0.09480012208223343 = 0.028011813759803772 + 0.01 * 6.678830623626709
Epoch 520, val loss: 0.9686922430992126
Epoch 530, training loss: 0.0924035906791687 = 0.025901097804307938 + 0.01 * 6.65024995803833
Epoch 530, val loss: 0.9818781018257141
Epoch 540, training loss: 0.09050886332988739 = 0.023911938071250916 + 0.01 * 6.659692764282227
Epoch 540, val loss: 0.9950361251831055
Epoch 550, training loss: 0.08850111067295074 = 0.022013751789927483 + 0.01 * 6.648735523223877
Epoch 550, val loss: 1.0083491802215576
Epoch 560, training loss: 0.0868552103638649 = 0.020293883979320526 + 0.01 * 6.656132698059082
Epoch 560, val loss: 1.0214991569519043
Epoch 570, training loss: 0.08508285880088806 = 0.01873895339667797 + 0.01 * 6.634390354156494
Epoch 570, val loss: 1.034758448600769
Epoch 580, training loss: 0.0838339626789093 = 0.017353961244225502 + 0.01 * 6.648000240325928
Epoch 580, val loss: 1.0479072332382202
Epoch 590, training loss: 0.08240534365177155 = 0.01613270491361618 + 0.01 * 6.627264022827148
Epoch 590, val loss: 1.060950517654419
Epoch 600, training loss: 0.08131464570760727 = 0.015040048398077488 + 0.01 * 6.62746000289917
Epoch 600, val loss: 1.0736333131790161
Epoch 610, training loss: 0.08049190044403076 = 0.014039600268006325 + 0.01 * 6.645230293273926
Epoch 610, val loss: 1.0861682891845703
Epoch 620, training loss: 0.07934816181659698 = 0.013145689852535725 + 0.01 * 6.6202473640441895
Epoch 620, val loss: 1.0980244874954224
Epoch 630, training loss: 0.07855330407619476 = 0.012349880300462246 + 0.01 * 6.62034273147583
Epoch 630, val loss: 1.1099015474319458
Epoch 640, training loss: 0.07765518873929977 = 0.011639492586255074 + 0.01 * 6.601569652557373
Epoch 640, val loss: 1.1212379932403564
Epoch 650, training loss: 0.07712824642658234 = 0.01099902018904686 + 0.01 * 6.6129231452941895
Epoch 650, val loss: 1.1324961185455322
Epoch 660, training loss: 0.0763043761253357 = 0.010418484918773174 + 0.01 * 6.588589668273926
Epoch 660, val loss: 1.1431835889816284
Epoch 670, training loss: 0.07567811012268066 = 0.00989032257348299 + 0.01 * 6.578778266906738
Epoch 670, val loss: 1.1536109447479248
Epoch 680, training loss: 0.07534914463758469 = 0.009407652541995049 + 0.01 * 6.594149112701416
Epoch 680, val loss: 1.163759708404541
Epoch 690, training loss: 0.0747000053524971 = 0.00896455068141222 + 0.01 * 6.573544979095459
Epoch 690, val loss: 1.173583984375
Epoch 700, training loss: 0.0746147483587265 = 0.008556116372346878 + 0.01 * 6.60586404800415
Epoch 700, val loss: 1.1830941438674927
Epoch 710, training loss: 0.07377148419618607 = 0.00817867461591959 + 0.01 * 6.559281349182129
Epoch 710, val loss: 1.1924527883529663
Epoch 720, training loss: 0.07353951781988144 = 0.00782915111631155 + 0.01 * 6.5710368156433105
Epoch 720, val loss: 1.2014575004577637
Epoch 730, training loss: 0.07301954180002213 = 0.00750440638512373 + 0.01 * 6.551514148712158
Epoch 730, val loss: 1.2102713584899902
Epoch 740, training loss: 0.07290486246347427 = 0.007201639004051685 + 0.01 * 6.570322036743164
Epoch 740, val loss: 1.2188853025436401
Epoch 750, training loss: 0.07226298749446869 = 0.006919829174876213 + 0.01 * 6.534315586090088
Epoch 750, val loss: 1.2271943092346191
Epoch 760, training loss: 0.07202216982841492 = 0.006657009944319725 + 0.01 * 6.536516189575195
Epoch 760, val loss: 1.2354192733764648
Epoch 770, training loss: 0.07185878604650497 = 0.006410539150238037 + 0.01 * 6.544825077056885
Epoch 770, val loss: 1.2434375286102295
Epoch 780, training loss: 0.0716337263584137 = 0.006179080810397863 + 0.01 * 6.545464515686035
Epoch 780, val loss: 1.2512062788009644
Epoch 790, training loss: 0.07113952934741974 = 0.005961722694337368 + 0.01 * 6.517780780792236
Epoch 790, val loss: 1.2588990926742554
Epoch 800, training loss: 0.07125917822122574 = 0.005757083185017109 + 0.01 * 6.5502095222473145
Epoch 800, val loss: 1.2664477825164795
Epoch 810, training loss: 0.07074642181396484 = 0.005564797669649124 + 0.01 * 6.518162727355957
Epoch 810, val loss: 1.273717999458313
Epoch 820, training loss: 0.07053220272064209 = 0.005383227486163378 + 0.01 * 6.514897346496582
Epoch 820, val loss: 1.2808891534805298
Epoch 830, training loss: 0.07033232599496841 = 0.005212076939642429 + 0.01 * 6.512025356292725
Epoch 830, val loss: 1.28795325756073
Epoch 840, training loss: 0.07026601582765579 = 0.005050172563642263 + 0.01 * 6.521584510803223
Epoch 840, val loss: 1.2948756217956543
Epoch 850, training loss: 0.06994431465864182 = 0.00489718047901988 + 0.01 * 6.504713535308838
Epoch 850, val loss: 1.301630973815918
Epoch 860, training loss: 0.06968670338392258 = 0.0047521209344267845 + 0.01 * 6.493458271026611
Epoch 860, val loss: 1.3082942962646484
Epoch 870, training loss: 0.06960877031087875 = 0.004614686593413353 + 0.01 * 6.49940824508667
Epoch 870, val loss: 1.3148014545440674
Epoch 880, training loss: 0.0692911297082901 = 0.004484066739678383 + 0.01 * 6.480706691741943
Epoch 880, val loss: 1.3211590051651
Epoch 890, training loss: 0.06897448003292084 = 0.004359956830739975 + 0.01 * 6.461452960968018
Epoch 890, val loss: 1.327419400215149
Epoch 900, training loss: 0.06913410127162933 = 0.004241809248924255 + 0.01 * 6.489229202270508
Epoch 900, val loss: 1.3335556983947754
Epoch 910, training loss: 0.06885165721178055 = 0.004129405133426189 + 0.01 * 6.472225666046143
Epoch 910, val loss: 1.3394865989685059
Epoch 920, training loss: 0.06868866086006165 = 0.004022293724119663 + 0.01 * 6.466637134552002
Epoch 920, val loss: 1.345412254333496
Epoch 930, training loss: 0.06857046484947205 = 0.0039202128536999226 + 0.01 * 6.465025424957275
Epoch 930, val loss: 1.3511462211608887
Epoch 940, training loss: 0.06838865578174591 = 0.003822864266112447 + 0.01 * 6.456579685211182
Epoch 940, val loss: 1.3568036556243896
Epoch 950, training loss: 0.06822185218334198 = 0.003729733405634761 + 0.01 * 6.449212074279785
Epoch 950, val loss: 1.3623164892196655
Epoch 960, training loss: 0.06823970377445221 = 0.003640613052994013 + 0.01 * 6.459908962249756
Epoch 960, val loss: 1.3677144050598145
Epoch 970, training loss: 0.06843049824237823 = 0.003555451985448599 + 0.01 * 6.487504482269287
Epoch 970, val loss: 1.373149037361145
Epoch 980, training loss: 0.06790566444396973 = 0.0034738974645733833 + 0.01 * 6.44317626953125
Epoch 980, val loss: 1.3782038688659668
Epoch 990, training loss: 0.06777902692556381 = 0.0033954617101699114 + 0.01 * 6.438356876373291
Epoch 990, val loss: 1.3833882808685303
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.4391
Flip ASR: 0.3911/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0381126403808594 = 1.9543745517730713 + 0.01 * 8.373815536499023
Epoch 0, val loss: 1.9471553564071655
Epoch 10, training loss: 2.027970314025879 = 1.9442330598831177 + 0.01 * 8.373730659484863
Epoch 10, val loss: 1.9373048543930054
Epoch 20, training loss: 2.0155415534973145 = 1.9318069219589233 + 0.01 * 8.373470306396484
Epoch 20, val loss: 1.924607515335083
Epoch 30, training loss: 1.9980756044387817 = 1.9143470525741577 + 0.01 * 8.372856140136719
Epoch 30, val loss: 1.9063339233398438
Epoch 40, training loss: 1.9720958471298218 = 1.8883888721466064 + 0.01 * 8.370699882507324
Epoch 40, val loss: 1.879109263420105
Epoch 50, training loss: 1.9346795082092285 = 1.8511030673980713 + 0.01 * 8.35763931274414
Epoch 50, val loss: 1.8411483764648438
Epoch 60, training loss: 1.8873999118804932 = 1.8046565055847168 + 0.01 * 8.274334907531738
Epoch 60, val loss: 1.7970327138900757
Epoch 70, training loss: 1.837105631828308 = 1.7581615447998047 + 0.01 * 7.894404888153076
Epoch 70, val loss: 1.7563508749008179
Epoch 80, training loss: 1.7817597389221191 = 1.70808744430542 + 0.01 * 7.367225170135498
Epoch 80, val loss: 1.7116154432296753
Epoch 90, training loss: 1.7132325172424316 = 1.641662836074829 + 0.01 * 7.156966209411621
Epoch 90, val loss: 1.652470350265503
Epoch 100, training loss: 1.6256029605865479 = 1.5549325942993164 + 0.01 * 7.067033290863037
Epoch 100, val loss: 1.5782729387283325
Epoch 110, training loss: 1.5191547870635986 = 1.4491658210754395 + 0.01 * 6.998891830444336
Epoch 110, val loss: 1.4931411743164062
Epoch 120, training loss: 1.4023611545562744 = 1.332825779914856 + 0.01 * 6.95353889465332
Epoch 120, val loss: 1.4006285667419434
Epoch 130, training loss: 1.2825103998184204 = 1.2131778001785278 + 0.01 * 6.933254718780518
Epoch 130, val loss: 1.3060842752456665
Epoch 140, training loss: 1.1630983352661133 = 1.093988060951233 + 0.01 * 6.911027431488037
Epoch 140, val loss: 1.2128232717514038
Epoch 150, training loss: 1.0487713813781738 = 0.979874849319458 + 0.01 * 6.889657020568848
Epoch 150, val loss: 1.1253429651260376
Epoch 160, training loss: 0.9437538385391235 = 0.8750498294830322 + 0.01 * 6.8704023361206055
Epoch 160, val loss: 1.0478578805923462
Epoch 170, training loss: 0.8508474230766296 = 0.7822844982147217 + 0.01 * 6.856290340423584
Epoch 170, val loss: 0.9823911190032959
Epoch 180, training loss: 0.7707555294036865 = 0.7022712826728821 + 0.01 * 6.848424911499023
Epoch 180, val loss: 0.9297778606414795
Epoch 190, training loss: 0.7022833824157715 = 0.6338955163955688 + 0.01 * 6.838788986206055
Epoch 190, val loss: 0.8885325789451599
Epoch 200, training loss: 0.6434889435768127 = 0.5751920938491821 + 0.01 * 6.82968282699585
Epoch 200, val loss: 0.8565462231636047
Epoch 210, training loss: 0.5924234986305237 = 0.5242070555686951 + 0.01 * 6.821642875671387
Epoch 210, val loss: 0.8318221569061279
Epoch 220, training loss: 0.5471941828727722 = 0.4790699779987335 + 0.01 * 6.8124213218688965
Epoch 220, val loss: 0.8127564787864685
Epoch 230, training loss: 0.5061050057411194 = 0.43806007504463196 + 0.01 * 6.804494857788086
Epoch 230, val loss: 0.7981857061386108
Epoch 240, training loss: 0.46765774488449097 = 0.3996850550174713 + 0.01 * 6.797267913818359
Epoch 240, val loss: 0.7874099016189575
Epoch 250, training loss: 0.43081074953079224 = 0.3629114329814911 + 0.01 * 6.7899322509765625
Epoch 250, val loss: 0.7800799608230591
Epoch 260, training loss: 0.39541202783584595 = 0.3275197148323059 + 0.01 * 6.7892303466796875
Epoch 260, val loss: 0.7758627533912659
Epoch 270, training loss: 0.36174654960632324 = 0.2939240336418152 + 0.01 * 6.782249927520752
Epoch 270, val loss: 0.7742965221405029
Epoch 280, training loss: 0.3302474915981293 = 0.2624804377555847 + 0.01 * 6.776706218719482
Epoch 280, val loss: 0.7748767137527466
Epoch 290, training loss: 0.301206111907959 = 0.23349037766456604 + 0.01 * 6.771572589874268
Epoch 290, val loss: 0.7776035070419312
Epoch 300, training loss: 0.2749381959438324 = 0.20718823373317719 + 0.01 * 6.774995803833008
Epoch 300, val loss: 0.7820413112640381
Epoch 310, training loss: 0.25120747089385986 = 0.18354617059230804 + 0.01 * 6.7661285400390625
Epoch 310, val loss: 0.7878159284591675
Epoch 320, training loss: 0.23016420006752014 = 0.16252940893173218 + 0.01 * 6.763479709625244
Epoch 320, val loss: 0.7951183915138245
Epoch 330, training loss: 0.21155858039855957 = 0.14397339522838593 + 0.01 * 6.758519172668457
Epoch 330, val loss: 0.8034751415252686
Epoch 340, training loss: 0.1953750103712082 = 0.127613827586174 + 0.01 * 6.776118755340576
Epoch 340, val loss: 0.8126682043075562
Epoch 350, training loss: 0.18076856434345245 = 0.11322502791881561 + 0.01 * 6.7543535232543945
Epoch 350, val loss: 0.8226629495620728
Epoch 360, training loss: 0.16805724799633026 = 0.10056143254041672 + 0.01 * 6.749581336975098
Epoch 360, val loss: 0.8333062529563904
Epoch 370, training loss: 0.15688860416412354 = 0.08943280577659607 + 0.01 * 6.745579242706299
Epoch 370, val loss: 0.8446409106254578
Epoch 380, training loss: 0.14708614349365234 = 0.07966015487909317 + 0.01 * 6.742598533630371
Epoch 380, val loss: 0.8563194274902344
Epoch 390, training loss: 0.1384735405445099 = 0.07108696550130844 + 0.01 * 6.7386579513549805
Epoch 390, val loss: 0.8681851625442505
Epoch 400, training loss: 0.13090938329696655 = 0.06356889754533768 + 0.01 * 6.734048843383789
Epoch 400, val loss: 0.8802599906921387
Epoch 410, training loss: 0.12426741421222687 = 0.05697592347860336 + 0.01 * 6.729148864746094
Epoch 410, val loss: 0.8923143148422241
Epoch 420, training loss: 0.11854774504899979 = 0.05119454860687256 + 0.01 * 6.7353196144104
Epoch 420, val loss: 0.904266893863678
Epoch 430, training loss: 0.11333505064249039 = 0.046122074127197266 + 0.01 * 6.721297740936279
Epoch 430, val loss: 0.9161106944084167
Epoch 440, training loss: 0.10881935060024261 = 0.0416632704436779 + 0.01 * 6.715607643127441
Epoch 440, val loss: 0.927733302116394
Epoch 450, training loss: 0.10484403371810913 = 0.03773890808224678 + 0.01 * 6.710512161254883
Epoch 450, val loss: 0.9391697645187378
Epoch 460, training loss: 0.10136426240205765 = 0.03428293764591217 + 0.01 * 6.708132743835449
Epoch 460, val loss: 0.9504125714302063
Epoch 470, training loss: 0.09824441373348236 = 0.031235510483384132 + 0.01 * 6.70089054107666
Epoch 470, val loss: 0.9614279270172119
Epoch 480, training loss: 0.09550298005342484 = 0.028543850407004356 + 0.01 * 6.695912837982178
Epoch 480, val loss: 0.9722521901130676
Epoch 490, training loss: 0.09311197698116302 = 0.02616196498274803 + 0.01 * 6.69500207901001
Epoch 490, val loss: 0.9828386306762695
Epoch 500, training loss: 0.09089908003807068 = 0.02404915913939476 + 0.01 * 6.684992790222168
Epoch 500, val loss: 0.9932476878166199
Epoch 510, training loss: 0.08900260180234909 = 0.022171305492520332 + 0.01 * 6.683130264282227
Epoch 510, val loss: 1.0034130811691284
Epoch 520, training loss: 0.08720189332962036 = 0.020499449223279953 + 0.01 * 6.670244216918945
Epoch 520, val loss: 1.0133775472640991
Epoch 530, training loss: 0.08569413423538208 = 0.019007286056876183 + 0.01 * 6.668684959411621
Epoch 530, val loss: 1.0231071710586548
Epoch 540, training loss: 0.0844167172908783 = 0.017670942470431328 + 0.01 * 6.674577713012695
Epoch 540, val loss: 1.0326412916183472
Epoch 550, training loss: 0.08303900063037872 = 0.016471710056066513 + 0.01 * 6.656729221343994
Epoch 550, val loss: 1.0419646501541138
Epoch 560, training loss: 0.08193811774253845 = 0.015391732566058636 + 0.01 * 6.654638767242432
Epoch 560, val loss: 1.0510612726211548
Epoch 570, training loss: 0.08088088035583496 = 0.01441660150885582 + 0.01 * 6.646428108215332
Epoch 570, val loss: 1.0599901676177979
Epoch 580, training loss: 0.07991300523281097 = 0.013533896766602993 + 0.01 * 6.637910842895508
Epoch 580, val loss: 1.068633794784546
Epoch 590, training loss: 0.07914845645427704 = 0.012732526287436485 + 0.01 * 6.641592979431152
Epoch 590, val loss: 1.0771074295043945
Epoch 600, training loss: 0.07841640710830688 = 0.012003686279058456 + 0.01 * 6.64127254486084
Epoch 600, val loss: 1.085316777229309
Epoch 610, training loss: 0.07761940360069275 = 0.0113381901755929 + 0.01 * 6.628121376037598
Epoch 610, val loss: 1.0933979749679565
Epoch 620, training loss: 0.07706191390752792 = 0.010729042813181877 + 0.01 * 6.633286952972412
Epoch 620, val loss: 1.1012179851531982
Epoch 630, training loss: 0.07627034932374954 = 0.010170591995120049 + 0.01 * 6.609975814819336
Epoch 630, val loss: 1.1088427305221558
Epoch 640, training loss: 0.07572843134403229 = 0.009656907059252262 + 0.01 * 6.607152462005615
Epoch 640, val loss: 1.116350769996643
Epoch 650, training loss: 0.07526007294654846 = 0.009183807298541069 + 0.01 * 6.607626438140869
Epoch 650, val loss: 1.1235336065292358
Epoch 660, training loss: 0.07477562129497528 = 0.008747781626880169 + 0.01 * 6.602784633636475
Epoch 660, val loss: 1.1305748224258423
Epoch 670, training loss: 0.0743049755692482 = 0.008344235830008984 + 0.01 * 6.596074104309082
Epoch 670, val loss: 1.137480616569519
Epoch 680, training loss: 0.07396440953016281 = 0.007969806902110577 + 0.01 * 6.599460601806641
Epoch 680, val loss: 1.1441692113876343
Epoch 690, training loss: 0.07360614836215973 = 0.007622119504958391 + 0.01 * 6.598402976989746
Epoch 690, val loss: 1.1507086753845215
Epoch 700, training loss: 0.07315730303525925 = 0.007298783864825964 + 0.01 * 6.585852146148682
Epoch 700, val loss: 1.1570701599121094
Epoch 710, training loss: 0.07279614359140396 = 0.0069972071796655655 + 0.01 * 6.579894065856934
Epoch 710, val loss: 1.1632721424102783
Epoch 720, training loss: 0.07280105352401733 = 0.006715805269777775 + 0.01 * 6.608524799346924
Epoch 720, val loss: 1.1692677736282349
Epoch 730, training loss: 0.07213320583105087 = 0.006453051697462797 + 0.01 * 6.5680155754089355
Epoch 730, val loss: 1.1751960515975952
Epoch 740, training loss: 0.07194118946790695 = 0.0062070731073617935 + 0.01 * 6.57341194152832
Epoch 740, val loss: 1.1809422969818115
Epoch 750, training loss: 0.07170785963535309 = 0.005976622458547354 + 0.01 * 6.573123455047607
Epoch 750, val loss: 1.1865006685256958
Epoch 760, training loss: 0.07134939730167389 = 0.005760120693594217 + 0.01 * 6.5589280128479
Epoch 760, val loss: 1.1919540166854858
Epoch 770, training loss: 0.07114020735025406 = 0.005556379444897175 + 0.01 * 6.558382511138916
Epoch 770, val loss: 1.1972600221633911
Epoch 780, training loss: 0.07089053839445114 = 0.00536480313166976 + 0.01 * 6.5525736808776855
Epoch 780, val loss: 1.202416181564331
Epoch 790, training loss: 0.07054899632930756 = 0.005184503272175789 + 0.01 * 6.536449432373047
Epoch 790, val loss: 1.207424521446228
Epoch 800, training loss: 0.07039255648851395 = 0.005014230962842703 + 0.01 * 6.537832736968994
Epoch 800, val loss: 1.21235990524292
Epoch 810, training loss: 0.0702768862247467 = 0.00485340878367424 + 0.01 * 6.542347431182861
Epoch 810, val loss: 1.2171614170074463
Epoch 820, training loss: 0.07007194310426712 = 0.004701597616076469 + 0.01 * 6.537034511566162
Epoch 820, val loss: 1.221784234046936
Epoch 830, training loss: 0.06977847218513489 = 0.0045579164288938046 + 0.01 * 6.522055625915527
Epoch 830, val loss: 1.2263559103012085
Epoch 840, training loss: 0.06977279484272003 = 0.004421587567776442 + 0.01 * 6.535121440887451
Epoch 840, val loss: 1.2308118343353271
Epoch 850, training loss: 0.06948055326938629 = 0.0042924885638058186 + 0.01 * 6.518806457519531
Epoch 850, val loss: 1.2351397275924683
Epoch 860, training loss: 0.06926552951335907 = 0.004169893451035023 + 0.01 * 6.509563446044922
Epoch 860, val loss: 1.2393807172775269
Epoch 870, training loss: 0.06918252259492874 = 0.0040534064173698425 + 0.01 * 6.512911796569824
Epoch 870, val loss: 1.2435106039047241
Epoch 880, training loss: 0.06900333613157272 = 0.003942703362554312 + 0.01 * 6.506062984466553
Epoch 880, val loss: 1.247525691986084
Epoch 890, training loss: 0.06893599033355713 = 0.003837204771116376 + 0.01 * 6.509878635406494
Epoch 890, val loss: 1.2515217065811157
Epoch 900, training loss: 0.06868817657232285 = 0.003736751852557063 + 0.01 * 6.495142936706543
Epoch 900, val loss: 1.2553569078445435
Epoch 910, training loss: 0.06856429576873779 = 0.003640924347564578 + 0.01 * 6.492337226867676
Epoch 910, val loss: 1.25914466381073
Epoch 920, training loss: 0.0684465616941452 = 0.00354962470009923 + 0.01 * 6.489693641662598
Epoch 920, val loss: 1.2628567218780518
Epoch 930, training loss: 0.06828974187374115 = 0.0034624075051397085 + 0.01 * 6.482733726501465
Epoch 930, val loss: 1.2664443254470825
Epoch 940, training loss: 0.06826641410589218 = 0.0033790171146392822 + 0.01 * 6.488739967346191
Epoch 940, val loss: 1.269973635673523
Epoch 950, training loss: 0.06812569499015808 = 0.0032991906628012657 + 0.01 * 6.4826507568359375
Epoch 950, val loss: 1.2735426425933838
Epoch 960, training loss: 0.06794068217277527 = 0.003222828498110175 + 0.01 * 6.471785068511963
Epoch 960, val loss: 1.276809811592102
Epoch 970, training loss: 0.0678345337510109 = 0.003149774856865406 + 0.01 * 6.468475818634033
Epoch 970, val loss: 1.2800959348678589
Epoch 980, training loss: 0.06780724972486496 = 0.003079773858189583 + 0.01 * 6.472748279571533
Epoch 980, val loss: 1.2833740711212158
Epoch 990, training loss: 0.06765490025281906 = 0.003012628760188818 + 0.01 * 6.464227676391602
Epoch 990, val loss: 1.2865315675735474
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.9188
Flip ASR: 0.9022/225 nodes
The final ASR:0.69127, 0.19661, Accuracy:0.79630, 0.00524
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11602])
remove edge: torch.Size([2, 9494])
updated graph: torch.Size([2, 10540])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.97786, 0.00522, Accuracy:0.82593, 0.00302
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.5 der2:0.3 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.012346029281616 = 1.9286082983016968 + 0.01 * 8.373779296875
Epoch 0, val loss: 1.9210026264190674
Epoch 10, training loss: 2.003478527069092 = 1.9197416305541992 + 0.01 * 8.37369155883789
Epoch 10, val loss: 1.9126688241958618
Epoch 20, training loss: 1.9926615953445435 = 1.9089279174804688 + 0.01 * 8.373370170593262
Epoch 20, val loss: 1.9021821022033691
Epoch 30, training loss: 1.9775655269622803 = 1.8938401937484741 + 0.01 * 8.372536659240723
Epoch 30, val loss: 1.8873705863952637
Epoch 40, training loss: 1.955122947692871 = 1.8714319467544556 + 0.01 * 8.369095802307129
Epoch 40, val loss: 1.8655688762664795
Epoch 50, training loss: 1.9227699041366577 = 1.8393280506134033 + 0.01 * 8.344184875488281
Epoch 50, val loss: 1.8356770277023315
Epoch 60, training loss: 1.8817082643508911 = 1.8003692626953125 + 0.01 * 8.133898735046387
Epoch 60, val loss: 1.8032797574996948
Epoch 70, training loss: 1.8373621702194214 = 1.7611922025680542 + 0.01 * 7.6169962882995605
Epoch 70, val loss: 1.7738591432571411
Epoch 80, training loss: 1.7839893102645874 = 1.7116726636886597 + 0.01 * 7.231659889221191
Epoch 80, val loss: 1.7327207326889038
Epoch 90, training loss: 1.7135854959487915 = 1.6430200338363647 + 0.01 * 7.056544780731201
Epoch 90, val loss: 1.6737215518951416
Epoch 100, training loss: 1.6222712993621826 = 1.5524133443832397 + 0.01 * 6.985789775848389
Epoch 100, val loss: 1.598333716392517
Epoch 110, training loss: 1.5139553546905518 = 1.4444409608840942 + 0.01 * 6.951441287994385
Epoch 110, val loss: 1.510034441947937
Epoch 120, training loss: 1.3970329761505127 = 1.3277262449264526 + 0.01 * 6.930678367614746
Epoch 120, val loss: 1.4160453081130981
Epoch 130, training loss: 1.279466152191162 = 1.210388422012329 + 0.01 * 6.907771110534668
Epoch 130, val loss: 1.3220700025558472
Epoch 140, training loss: 1.168683409690857 = 1.0998809337615967 + 0.01 * 6.880243301391602
Epoch 140, val loss: 1.2358547449111938
Epoch 150, training loss: 1.069749355316162 = 1.001227855682373 + 0.01 * 6.852144241333008
Epoch 150, val loss: 1.1613260507583618
Epoch 160, training loss: 0.983410656452179 = 0.9151074886322021 + 0.01 * 6.830319404602051
Epoch 160, val loss: 1.0984734296798706
Epoch 170, training loss: 0.907034158706665 = 0.8388543128967285 + 0.01 * 6.817985534667969
Epoch 170, val loss: 1.0443475246429443
Epoch 180, training loss: 0.8378514051437378 = 0.7697536945343018 + 0.01 * 6.809771537780762
Epoch 180, val loss: 0.996212899684906
Epoch 190, training loss: 0.7745850086212158 = 0.7065717577934265 + 0.01 * 6.801326274871826
Epoch 190, val loss: 0.9526897072792053
Epoch 200, training loss: 0.7165689468383789 = 0.6486505270004272 + 0.01 * 6.791845321655273
Epoch 200, val loss: 0.9133846163749695
Epoch 210, training loss: 0.6630960702896118 = 0.5952872633934021 + 0.01 * 6.78087854385376
Epoch 210, val loss: 0.8779860138893127
Epoch 220, training loss: 0.6133950352668762 = 0.5456991195678711 + 0.01 * 6.76959228515625
Epoch 220, val loss: 0.8458796143531799
Epoch 230, training loss: 0.5668904781341553 = 0.4993237555027008 + 0.01 * 6.756675720214844
Epoch 230, val loss: 0.816910445690155
Epoch 240, training loss: 0.5233118534088135 = 0.45585867762565613 + 0.01 * 6.745316505432129
Epoch 240, val loss: 0.7909271717071533
Epoch 250, training loss: 0.4822913706302643 = 0.41493627429008484 + 0.01 * 6.735509395599365
Epoch 250, val loss: 0.7678952217102051
Epoch 260, training loss: 0.44336289167404175 = 0.3760893642902374 + 0.01 * 6.727352619171143
Epoch 260, val loss: 0.7477120757102966
Epoch 270, training loss: 0.4059007167816162 = 0.33868300914764404 + 0.01 * 6.721770763397217
Epoch 270, val loss: 0.7300790548324585
Epoch 280, training loss: 0.3693605661392212 = 0.30218783020973206 + 0.01 * 6.717272758483887
Epoch 280, val loss: 0.7144321203231812
Epoch 290, training loss: 0.33375728130340576 = 0.2666231691837311 + 0.01 * 6.7134108543396
Epoch 290, val loss: 0.7006626129150391
Epoch 300, training loss: 0.29966384172439575 = 0.23256494104862213 + 0.01 * 6.7098917961120605
Epoch 300, val loss: 0.688940167427063
Epoch 310, training loss: 0.26799386739730835 = 0.20092245936393738 + 0.01 * 6.707141876220703
Epoch 310, val loss: 0.6797139048576355
Epoch 320, training loss: 0.23938880860805511 = 0.1723587065935135 + 0.01 * 6.703010559082031
Epoch 320, val loss: 0.6732679605484009
Epoch 330, training loss: 0.21421131491661072 = 0.14721088111400604 + 0.01 * 6.700043201446533
Epoch 330, val loss: 0.6698065400123596
Epoch 340, training loss: 0.19246357679367065 = 0.1254943162202835 + 0.01 * 6.696926593780518
Epoch 340, val loss: 0.6690863370895386
Epoch 350, training loss: 0.17391273379325867 = 0.10697529464960098 + 0.01 * 6.693743705749512
Epoch 350, val loss: 0.6707432270050049
Epoch 360, training loss: 0.1582217514514923 = 0.09133290499448776 + 0.01 * 6.688884258270264
Epoch 360, val loss: 0.6743113398551941
Epoch 370, training loss: 0.1450832635164261 = 0.07822351902723312 + 0.01 * 6.685974597930908
Epoch 370, val loss: 0.6793354749679565
Epoch 380, training loss: 0.13410496711730957 = 0.06728747487068176 + 0.01 * 6.68174934387207
Epoch 380, val loss: 0.685575544834137
Epoch 390, training loss: 0.12500222027301788 = 0.05819426104426384 + 0.01 * 6.680796146392822
Epoch 390, val loss: 0.6926882266998291
Epoch 400, training loss: 0.11741016805171967 = 0.05063896253705025 + 0.01 * 6.677120208740234
Epoch 400, val loss: 0.7004373669624329
Epoch 410, training loss: 0.11107318103313446 = 0.04435228928923607 + 0.01 * 6.67209005355835
Epoch 410, val loss: 0.7086619734764099
Epoch 420, training loss: 0.10579337924718857 = 0.03910630941390991 + 0.01 * 6.668707370758057
Epoch 420, val loss: 0.7172183990478516
Epoch 430, training loss: 0.10137057304382324 = 0.03470649942755699 + 0.01 * 6.666408061981201
Epoch 430, val loss: 0.7259904146194458
Epoch 440, training loss: 0.09769418090581894 = 0.030999379232525826 + 0.01 * 6.669480323791504
Epoch 440, val loss: 0.7348969578742981
Epoch 450, training loss: 0.09446045011281967 = 0.027858635410666466 + 0.01 * 6.660181522369385
Epoch 450, val loss: 0.743761420249939
Epoch 460, training loss: 0.09176131337881088 = 0.02517853118479252 + 0.01 * 6.658278465270996
Epoch 460, val loss: 0.7525615692138672
Epoch 470, training loss: 0.08942548930644989 = 0.022876134142279625 + 0.01 * 6.654935359954834
Epoch 470, val loss: 0.7613409757614136
Epoch 480, training loss: 0.08741037547588348 = 0.020884880796074867 + 0.01 * 6.652549743652344
Epoch 480, val loss: 0.7698933482170105
Epoch 490, training loss: 0.08565142750740051 = 0.019151870161294937 + 0.01 * 6.649956226348877
Epoch 490, val loss: 0.7783121466636658
Epoch 500, training loss: 0.08448708057403564 = 0.01763525977730751 + 0.01 * 6.685182571411133
Epoch 500, val loss: 0.7865713834762573
Epoch 510, training loss: 0.08279836922883987 = 0.01630285568535328 + 0.01 * 6.649551868438721
Epoch 510, val loss: 0.7944894433021545
Epoch 520, training loss: 0.08157029002904892 = 0.015124822035431862 + 0.01 * 6.644547462463379
Epoch 520, val loss: 0.802361011505127
Epoch 530, training loss: 0.08049655705690384 = 0.01407748181372881 + 0.01 * 6.641907691955566
Epoch 530, val loss: 0.8099722266197205
Epoch 540, training loss: 0.07953643053770065 = 0.013141455128788948 + 0.01 * 6.639498233795166
Epoch 540, val loss: 0.8173995018005371
Epoch 550, training loss: 0.07867211103439331 = 0.012301294133067131 + 0.01 * 6.637081623077393
Epoch 550, val loss: 0.8246894478797913
Epoch 560, training loss: 0.07789139449596405 = 0.011544309556484222 + 0.01 * 6.634708881378174
Epoch 560, val loss: 0.8317440748214722
Epoch 570, training loss: 0.07718480378389359 = 0.010859803296625614 + 0.01 * 6.632500648498535
Epoch 570, val loss: 0.8386735916137695
Epoch 580, training loss: 0.07654080539941788 = 0.010238626040518284 + 0.01 * 6.630218505859375
Epoch 580, val loss: 0.8454078435897827
Epoch 590, training loss: 0.07596120238304138 = 0.009673167020082474 + 0.01 * 6.6288042068481445
Epoch 590, val loss: 0.8519917726516724
Epoch 600, training loss: 0.07552295178174973 = 0.009157474152743816 + 0.01 * 6.6365485191345215
Epoch 600, val loss: 0.8584463000297546
Epoch 610, training loss: 0.07496777921915054 = 0.008686202578246593 + 0.01 * 6.628158092498779
Epoch 610, val loss: 0.8647030591964722
Epoch 620, training loss: 0.07447850704193115 = 0.008253678679466248 + 0.01 * 6.622482776641846
Epoch 620, val loss: 0.8708010911941528
Epoch 630, training loss: 0.07405657321214676 = 0.00785545352846384 + 0.01 * 6.620112419128418
Epoch 630, val loss: 0.8767743706703186
Epoch 640, training loss: 0.07366129755973816 = 0.0074878595769405365 + 0.01 * 6.617344379425049
Epoch 640, val loss: 0.8826072216033936
Epoch 650, training loss: 0.0732990950345993 = 0.007147802971303463 + 0.01 * 6.615128993988037
Epoch 650, val loss: 0.8883073329925537
Epoch 660, training loss: 0.0731021910905838 = 0.006832673214375973 + 0.01 * 6.626952171325684
Epoch 660, val loss: 0.8938497304916382
Epoch 670, training loss: 0.07267947494983673 = 0.0065404800698161125 + 0.01 * 6.613900184631348
Epoch 670, val loss: 0.8993847966194153
Epoch 680, training loss: 0.07236271351575851 = 0.006268858443945646 + 0.01 * 6.6093854904174805
Epoch 680, val loss: 0.9047060608863831
Epoch 690, training loss: 0.07208874821662903 = 0.006015729159116745 + 0.01 * 6.607302188873291
Epoch 690, val loss: 0.9099295139312744
Epoch 700, training loss: 0.07189225405454636 = 0.005779268220067024 + 0.01 * 6.61129903793335
Epoch 700, val loss: 0.9150211811065674
Epoch 710, training loss: 0.07159595191478729 = 0.00555809773504734 + 0.01 * 6.603785991668701
Epoch 710, val loss: 0.9200727939605713
Epoch 720, training loss: 0.07137783616781235 = 0.005350982770323753 + 0.01 * 6.602685451507568
Epoch 720, val loss: 0.9249986410140991
Epoch 730, training loss: 0.07114548236131668 = 0.005156859755516052 + 0.01 * 6.598862171173096
Epoch 730, val loss: 0.9297744631767273
Epoch 740, training loss: 0.07092545926570892 = 0.004974702373147011 + 0.01 * 6.595076084136963
Epoch 740, val loss: 0.9344993233680725
Epoch 750, training loss: 0.07075058668851852 = 0.004803312476724386 + 0.01 * 6.594727039337158
Epoch 750, val loss: 0.9391278028488159
Epoch 760, training loss: 0.07054213434457779 = 0.004641863517463207 + 0.01 * 6.590027809143066
Epoch 760, val loss: 0.9436467885971069
Epoch 770, training loss: 0.07054534554481506 = 0.004489523824304342 + 0.01 * 6.605582237243652
Epoch 770, val loss: 0.9480342864990234
Epoch 780, training loss: 0.07022436708211899 = 0.004345798399299383 + 0.01 * 6.587857246398926
Epoch 780, val loss: 0.9524769186973572
Epoch 790, training loss: 0.07007463276386261 = 0.00421008700504899 + 0.01 * 6.58645486831665
Epoch 790, val loss: 0.9567146301269531
Epoch 800, training loss: 0.06992729753255844 = 0.004081656690686941 + 0.01 * 6.584563732147217
Epoch 800, val loss: 0.9609401226043701
Epoch 810, training loss: 0.06980098783969879 = 0.003960064612329006 + 0.01 * 6.584092140197754
Epoch 810, val loss: 0.9649363160133362
Epoch 820, training loss: 0.06964336335659027 = 0.00384486373513937 + 0.01 * 6.579849720001221
Epoch 820, val loss: 0.969016969203949
Epoch 830, training loss: 0.06954055279493332 = 0.003735568141564727 + 0.01 * 6.580498695373535
Epoch 830, val loss: 0.972981333732605
Epoch 840, training loss: 0.06938745081424713 = 0.0036317629273980856 + 0.01 * 6.575568675994873
Epoch 840, val loss: 0.9768333435058594
Epoch 850, training loss: 0.06923585385084152 = 0.0035329703241586685 + 0.01 * 6.57028865814209
Epoch 850, val loss: 0.9806769490242004
Epoch 860, training loss: 0.06918199360370636 = 0.003438857151195407 + 0.01 * 6.574313640594482
Epoch 860, val loss: 0.9843899011611938
Epoch 870, training loss: 0.06910593807697296 = 0.003349215956404805 + 0.01 * 6.575672149658203
Epoch 870, val loss: 0.988152265548706
Epoch 880, training loss: 0.06890609115362167 = 0.0032639983110129833 + 0.01 * 6.564209461212158
Epoch 880, val loss: 0.9917654395103455
Epoch 890, training loss: 0.06892828643321991 = 0.0031827068887650967 + 0.01 * 6.574557781219482
Epoch 890, val loss: 0.9952887892723083
Epoch 900, training loss: 0.06873513013124466 = 0.0031050487887114286 + 0.01 * 6.5630083084106445
Epoch 900, val loss: 0.9988201260566711
Epoch 910, training loss: 0.06868252903223038 = 0.00303092272952199 + 0.01 * 6.565160751342773
Epoch 910, val loss: 1.002281904220581
Epoch 920, training loss: 0.06853607296943665 = 0.00296001136302948 + 0.01 * 6.557606220245361
Epoch 920, val loss: 1.0056889057159424
Epoch 930, training loss: 0.06844063848257065 = 0.0028922795318067074 + 0.01 * 6.554836273193359
Epoch 930, val loss: 1.0089465379714966
Epoch 940, training loss: 0.0682925209403038 = 0.0028273731004446745 + 0.01 * 6.546514511108398
Epoch 940, val loss: 1.0122507810592651
Epoch 950, training loss: 0.06820140033960342 = 0.0027653768192976713 + 0.01 * 6.543602466583252
Epoch 950, val loss: 1.0155106782913208
Epoch 960, training loss: 0.0683131217956543 = 0.0027059034910053015 + 0.01 * 6.560722351074219
Epoch 960, val loss: 1.0186649560928345
Epoch 970, training loss: 0.06805679947137833 = 0.0026489675510674715 + 0.01 * 6.540783882141113
Epoch 970, val loss: 1.0218169689178467
Epoch 980, training loss: 0.06801506131887436 = 0.0025942467618733644 + 0.01 * 6.542081832885742
Epoch 980, val loss: 1.024928331375122
Epoch 990, training loss: 0.06793554872274399 = 0.0025418137665838003 + 0.01 * 6.539374351501465
Epoch 990, val loss: 1.0279572010040283
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.6716
Flip ASR: 0.6089/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0427706241607666 = 1.9590333700180054 + 0.01 * 8.373734474182129
Epoch 0, val loss: 1.9613001346588135
Epoch 10, training loss: 2.0322489738464355 = 1.9485126733779907 + 0.01 * 8.373619079589844
Epoch 10, val loss: 1.9508657455444336
Epoch 20, training loss: 2.019253730773926 = 1.935521125793457 + 0.01 * 8.373249053955078
Epoch 20, val loss: 1.9373822212219238
Epoch 30, training loss: 2.000986337661743 = 1.9172636270523071 + 0.01 * 8.372272491455078
Epoch 30, val loss: 1.9178436994552612
Epoch 40, training loss: 1.9740264415740967 = 1.890347957611084 + 0.01 * 8.36784839630127
Epoch 40, val loss: 1.8888171911239624
Epoch 50, training loss: 1.9357590675354004 = 1.8523907661437988 + 0.01 * 8.336831092834473
Epoch 50, val loss: 1.8490334749221802
Epoch 60, training loss: 1.8885022401809692 = 1.8072266578674316 + 0.01 * 8.12756061553955
Epoch 60, val loss: 1.8053942918777466
Epoch 70, training loss: 1.8400943279266357 = 1.7633765935897827 + 0.01 * 7.671770095825195
Epoch 70, val loss: 1.767029047012329
Epoch 80, training loss: 1.7862735986709595 = 1.7137233018875122 + 0.01 * 7.255024433135986
Epoch 80, val loss: 1.7238799333572388
Epoch 90, training loss: 1.7182313203811646 = 1.6476846933364868 + 0.01 * 7.054662227630615
Epoch 90, val loss: 1.6661031246185303
Epoch 100, training loss: 1.6309208869934082 = 1.5611680746078491 + 0.01 * 6.97528076171875
Epoch 100, val loss: 1.5915495157241821
Epoch 110, training loss: 1.527395486831665 = 1.458024263381958 + 0.01 * 6.937117576599121
Epoch 110, val loss: 1.5056840181350708
Epoch 120, training loss: 1.419358730316162 = 1.350216031074524 + 0.01 * 6.914268493652344
Epoch 120, val loss: 1.4199503660202026
Epoch 130, training loss: 1.3136121034622192 = 1.2446435689926147 + 0.01 * 6.896856784820557
Epoch 130, val loss: 1.3393819332122803
Epoch 140, training loss: 1.2142900228500366 = 1.1454777717590332 + 0.01 * 6.881222248077393
Epoch 140, val loss: 1.2666188478469849
Epoch 150, training loss: 1.124630331993103 = 1.0559581518173218 + 0.01 * 6.867220878601074
Epoch 150, val loss: 1.2015310525894165
Epoch 160, training loss: 1.045363187789917 = 0.9767913222312927 + 0.01 * 6.857187271118164
Epoch 160, val loss: 1.1435985565185547
Epoch 170, training loss: 0.9741784334182739 = 0.9056612253189087 + 0.01 * 6.851719379425049
Epoch 170, val loss: 1.091107726097107
Epoch 180, training loss: 0.9069187045097351 = 0.8384217023849487 + 0.01 * 6.849698066711426
Epoch 180, val loss: 1.0412157773971558
Epoch 190, training loss: 0.8401200175285339 = 0.7716277837753296 + 0.01 * 6.849223613739014
Epoch 190, val loss: 0.9912310838699341
Epoch 200, training loss: 0.7727634906768799 = 0.7042707800865173 + 0.01 * 6.849268913269043
Epoch 200, val loss: 0.94044429063797
Epoch 210, training loss: 0.7066583037376404 = 0.6381679177284241 + 0.01 * 6.849038600921631
Epoch 210, val loss: 0.8904531002044678
Epoch 220, training loss: 0.6447300910949707 = 0.5762516260147095 + 0.01 * 6.847846984863281
Epoch 220, val loss: 0.8441117405891418
Epoch 230, training loss: 0.588740885257721 = 0.5202892422676086 + 0.01 * 6.845166206359863
Epoch 230, val loss: 0.8038094639778137
Epoch 240, training loss: 0.5384422540664673 = 0.4700377881526947 + 0.01 * 6.840445518493652
Epoch 240, val loss: 0.7699399590492249
Epoch 250, training loss: 0.49236661195755005 = 0.4240330457687378 + 0.01 * 6.833357810974121
Epoch 250, val loss: 0.7415360808372498
Epoch 260, training loss: 0.4488602876663208 = 0.38062646985054016 + 0.01 * 6.823381423950195
Epoch 260, val loss: 0.7176621556282043
Epoch 270, training loss: 0.40685373544692993 = 0.3387579023838043 + 0.01 * 6.809581756591797
Epoch 270, val loss: 0.6971092224121094
Epoch 280, training loss: 0.36602792143821716 = 0.298091322183609 + 0.01 * 6.793659687042236
Epoch 280, val loss: 0.679389238357544
Epoch 290, training loss: 0.32682955265045166 = 0.2590593099594116 + 0.01 * 6.777022838592529
Epoch 290, val loss: 0.664111852645874
Epoch 300, training loss: 0.2903345823287964 = 0.2226978987455368 + 0.01 * 6.763669967651367
Epoch 300, val loss: 0.6515777111053467
Epoch 310, training loss: 0.25767114758491516 = 0.19024071097373962 + 0.01 * 6.743043422698975
Epoch 310, val loss: 0.642162024974823
Epoch 320, training loss: 0.22972184419631958 = 0.16240647435188293 + 0.01 * 6.731537342071533
Epoch 320, val loss: 0.6361948251724243
Epoch 330, training loss: 0.20633967220783234 = 0.13909465074539185 + 0.01 * 6.724502086639404
Epoch 330, val loss: 0.6336184144020081
Epoch 340, training loss: 0.1869979202747345 = 0.11985186487436295 + 0.01 * 6.714605331420898
Epoch 340, val loss: 0.6340251564979553
Epoch 350, training loss: 0.17111836373806 = 0.10404503345489502 + 0.01 * 6.707333087921143
Epoch 350, val loss: 0.636964738368988
Epoch 360, training loss: 0.1580233871936798 = 0.09099932014942169 + 0.01 * 6.702406406402588
Epoch 360, val loss: 0.6420451402664185
Epoch 370, training loss: 0.1471315622329712 = 0.08009734749794006 + 0.01 * 6.703421115875244
Epoch 370, val loss: 0.648602306842804
Epoch 380, training loss: 0.13778655230998993 = 0.0708855539560318 + 0.01 * 6.690099716186523
Epoch 380, val loss: 0.6563599109649658
Epoch 390, training loss: 0.12986713647842407 = 0.06301017105579376 + 0.01 * 6.685695648193359
Epoch 390, val loss: 0.6648156046867371
Epoch 400, training loss: 0.12311265617609024 = 0.05619743466377258 + 0.01 * 6.691522121429443
Epoch 400, val loss: 0.6738482117652893
Epoch 410, training loss: 0.1170337051153183 = 0.050265733152627945 + 0.01 * 6.676796913146973
Epoch 410, val loss: 0.683135449886322
Epoch 420, training loss: 0.11182643473148346 = 0.04506804794073105 + 0.01 * 6.675838947296143
Epoch 420, val loss: 0.6926952600479126
Epoch 430, training loss: 0.10721096396446228 = 0.04053281992673874 + 0.01 * 6.667814254760742
Epoch 430, val loss: 0.7020708322525024
Epoch 440, training loss: 0.10336227715015411 = 0.0365753099322319 + 0.01 * 6.678697109222412
Epoch 440, val loss: 0.7114590406417847
Epoch 450, training loss: 0.09979847073554993 = 0.033134642988443375 + 0.01 * 6.666382789611816
Epoch 450, val loss: 0.7206513285636902
Epoch 460, training loss: 0.09674874693155289 = 0.03013741411268711 + 0.01 * 6.661133766174316
Epoch 460, val loss: 0.7298175096511841
Epoch 470, training loss: 0.0940750390291214 = 0.02751893177628517 + 0.01 * 6.655611515045166
Epoch 470, val loss: 0.7388013005256653
Epoch 480, training loss: 0.09173811972141266 = 0.02522181160748005 + 0.01 * 6.6516313552856445
Epoch 480, val loss: 0.7475842833518982
Epoch 490, training loss: 0.08977580815553665 = 0.02320047654211521 + 0.01 * 6.657533168792725
Epoch 490, val loss: 0.7562675476074219
Epoch 500, training loss: 0.08787760138511658 = 0.021415950730443 + 0.01 * 6.646165370941162
Epoch 500, val loss: 0.7647560238838196
Epoch 510, training loss: 0.08626922965049744 = 0.019831568002700806 + 0.01 * 6.643766403198242
Epoch 510, val loss: 0.7731307744979858
Epoch 520, training loss: 0.08485686033964157 = 0.01841808669269085 + 0.01 * 6.6438775062561035
Epoch 520, val loss: 0.7812981605529785
Epoch 530, training loss: 0.08351033180952072 = 0.017152970656752586 + 0.01 * 6.635735988616943
Epoch 530, val loss: 0.789322018623352
Epoch 540, training loss: 0.08233869075775146 = 0.016016658395528793 + 0.01 * 6.632204055786133
Epoch 540, val loss: 0.7971018552780151
Epoch 550, training loss: 0.08139008283615112 = 0.014992300420999527 + 0.01 * 6.639778137207031
Epoch 550, val loss: 0.8047333359718323
Epoch 560, training loss: 0.08036499470472336 = 0.0140661196783185 + 0.01 * 6.629887580871582
Epoch 560, val loss: 0.8122246861457825
Epoch 570, training loss: 0.0794464498758316 = 0.01322568953037262 + 0.01 * 6.622076034545898
Epoch 570, val loss: 0.8194434642791748
Epoch 580, training loss: 0.07873302698135376 = 0.012460533529520035 + 0.01 * 6.627248764038086
Epoch 580, val loss: 0.8264559507369995
Epoch 590, training loss: 0.07794719934463501 = 0.011763757094740868 + 0.01 * 6.618343830108643
Epoch 590, val loss: 0.8333929777145386
Epoch 600, training loss: 0.07726113498210907 = 0.011127009056508541 + 0.01 * 6.613412857055664
Epoch 600, val loss: 0.8401380777359009
Epoch 610, training loss: 0.07672596722841263 = 0.010543002746999264 + 0.01 * 6.618296146392822
Epoch 610, val loss: 0.8466222286224365
Epoch 620, training loss: 0.07613366097211838 = 0.010006188414990902 + 0.01 * 6.612747669219971
Epoch 620, val loss: 0.8530522584915161
Epoch 630, training loss: 0.0755988135933876 = 0.009512010030448437 + 0.01 * 6.608680725097656
Epoch 630, val loss: 0.8592671751976013
Epoch 640, training loss: 0.07519640773534775 = 0.009056168608367443 + 0.01 * 6.614024639129639
Epoch 640, val loss: 0.865297794342041
Epoch 650, training loss: 0.07465258985757828 = 0.008634492754936218 + 0.01 * 6.601809501647949
Epoch 650, val loss: 0.8712804317474365
Epoch 660, training loss: 0.07437188923358917 = 0.00824394728988409 + 0.01 * 6.612794399261475
Epoch 660, val loss: 0.8770595788955688
Epoch 670, training loss: 0.07379825413227081 = 0.007881957106292248 + 0.01 * 6.591629981994629
Epoch 670, val loss: 0.8826925158500671
Epoch 680, training loss: 0.07341932505369186 = 0.007545157801359892 + 0.01 * 6.587417125701904
Epoch 680, val loss: 0.8882173895835876
Epoch 690, training loss: 0.0731825977563858 = 0.00723172165453434 + 0.01 * 6.595088005065918
Epoch 690, val loss: 0.8935710787773132
Epoch 700, training loss: 0.07276029884815216 = 0.006939288228750229 + 0.01 * 6.582100868225098
Epoch 700, val loss: 0.898916482925415
Epoch 710, training loss: 0.07242321968078613 = 0.006666253786534071 + 0.01 * 6.575697422027588
Epoch 710, val loss: 0.9040504097938538
Epoch 720, training loss: 0.07221409678459167 = 0.006410946138203144 + 0.01 * 6.580315113067627
Epoch 720, val loss: 0.9090985059738159
Epoch 730, training loss: 0.07183457911014557 = 0.006171721033751965 + 0.01 * 6.566286087036133
Epoch 730, val loss: 0.9140335917472839
Epoch 740, training loss: 0.07165689766407013 = 0.00594746507704258 + 0.01 * 6.570943355560303
Epoch 740, val loss: 0.918799102306366
Epoch 750, training loss: 0.07141587138175964 = 0.005736874882131815 + 0.01 * 6.567899703979492
Epoch 750, val loss: 0.9235367774963379
Epoch 760, training loss: 0.07131308317184448 = 0.005538811907172203 + 0.01 * 6.577427387237549
Epoch 760, val loss: 0.9280712008476257
Epoch 770, training loss: 0.07089558988809586 = 0.0053521511144936085 + 0.01 * 6.5543437004089355
Epoch 770, val loss: 0.9326431155204773
Epoch 780, training loss: 0.07088720798492432 = 0.005176195874810219 + 0.01 * 6.571101665496826
Epoch 780, val loss: 0.9369611740112305
Epoch 790, training loss: 0.07043103128671646 = 0.005010172724723816 + 0.01 * 6.542086124420166
Epoch 790, val loss: 0.9413025975227356
Epoch 800, training loss: 0.07047881186008453 = 0.004853227641433477 + 0.01 * 6.562558650970459
Epoch 800, val loss: 0.9454968571662903
Epoch 810, training loss: 0.07018532603979111 = 0.00470513291656971 + 0.01 * 6.548019886016846
Epoch 810, val loss: 0.9495791792869568
Epoch 820, training loss: 0.06989651173353195 = 0.004564834758639336 + 0.01 * 6.533167839050293
Epoch 820, val loss: 0.9535189270973206
Epoch 830, training loss: 0.06968968361616135 = 0.004431942477822304 + 0.01 * 6.5257744789123535
Epoch 830, val loss: 0.9574934244155884
Epoch 840, training loss: 0.06956718862056732 = 0.004305735696107149 + 0.01 * 6.5261454582214355
Epoch 840, val loss: 0.9613047242164612
Epoch 850, training loss: 0.06957878172397614 = 0.004185930825769901 + 0.01 * 6.539285182952881
Epoch 850, val loss: 0.9650588631629944
Epoch 860, training loss: 0.06938187032938004 = 0.004071987699717283 + 0.01 * 6.5309882164001465
Epoch 860, val loss: 0.9687517881393433
Epoch 870, training loss: 0.06909138709306717 = 0.003963775467127562 + 0.01 * 6.51276159286499
Epoch 870, val loss: 0.9723187685012817
Epoch 880, training loss: 0.06886659562587738 = 0.0038607055321335793 + 0.01 * 6.500588893890381
Epoch 880, val loss: 0.9757888317108154
Epoch 890, training loss: 0.06876576691865921 = 0.003762441221624613 + 0.01 * 6.500333309173584
Epoch 890, val loss: 0.9792835116386414
Epoch 900, training loss: 0.06863579154014587 = 0.0036687750834971666 + 0.01 * 6.496701717376709
Epoch 900, val loss: 0.9826685190200806
Epoch 910, training loss: 0.06913425773382187 = 0.003579372074455023 + 0.01 * 6.555488586425781
Epoch 910, val loss: 0.9860007762908936
Epoch 920, training loss: 0.06853069365024567 = 0.0034940566401928663 + 0.01 * 6.503664016723633
Epoch 920, val loss: 0.9892035722732544
Epoch 930, training loss: 0.06855616718530655 = 0.0034127221442759037 + 0.01 * 6.514344215393066
Epoch 930, val loss: 0.992322564125061
Epoch 940, training loss: 0.06818430870771408 = 0.003334718057885766 + 0.01 * 6.484959602355957
Epoch 940, val loss: 0.9954883456230164
Epoch 950, training loss: 0.06800782680511475 = 0.0032601801212877035 + 0.01 * 6.474764823913574
Epoch 950, val loss: 0.9984704852104187
Epoch 960, training loss: 0.06789587438106537 = 0.0031887926161289215 + 0.01 * 6.470707893371582
Epoch 960, val loss: 1.0015449523925781
Epoch 970, training loss: 0.06799554079771042 = 0.0031203054822981358 + 0.01 * 6.487524032592773
Epoch 970, val loss: 1.004392147064209
Epoch 980, training loss: 0.0678173154592514 = 0.0030545813497155905 + 0.01 * 6.476273536682129
Epoch 980, val loss: 1.0073987245559692
Epoch 990, training loss: 0.06776940077543259 = 0.0029915214981883764 + 0.01 * 6.477787971496582
Epoch 990, val loss: 1.0101147890090942
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.7196
Flip ASR: 0.6889/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0278027057647705 = 1.944065809249878 + 0.01 * 8.373700141906738
Epoch 0, val loss: 1.9392297267913818
Epoch 10, training loss: 2.016632318496704 = 1.9328968524932861 + 0.01 * 8.373537063598633
Epoch 10, val loss: 1.9271003007888794
Epoch 20, training loss: 2.002965211868286 = 1.9192352294921875 + 0.01 * 8.372998237609863
Epoch 20, val loss: 1.911798119544983
Epoch 30, training loss: 1.9837639331817627 = 1.9000481367111206 + 0.01 * 8.371581077575684
Epoch 30, val loss: 1.8899625539779663
Epoch 40, training loss: 1.9556776285171509 = 1.8720245361328125 + 0.01 * 8.365311622619629
Epoch 40, val loss: 1.8583406209945679
Epoch 50, training loss: 1.9163835048675537 = 1.8331345319747925 + 0.01 * 8.324893951416016
Epoch 50, val loss: 1.8166033029556274
Epoch 60, training loss: 1.8673146963119507 = 1.7869505882263184 + 0.01 * 8.036406517028809
Epoch 60, val loss: 1.772295594215393
Epoch 70, training loss: 1.8161253929138184 = 1.7402148246765137 + 0.01 * 7.591056823730469
Epoch 70, val loss: 1.7332857847213745
Epoch 80, training loss: 1.759350299835205 = 1.6856216192245483 + 0.01 * 7.372862815856934
Epoch 80, val loss: 1.6889724731445312
Epoch 90, training loss: 1.6836063861846924 = 1.6112949848175049 + 0.01 * 7.231142044067383
Epoch 90, val loss: 1.629066824913025
Epoch 100, training loss: 1.588692545890808 = 1.5181443691253662 + 0.01 * 7.0548200607299805
Epoch 100, val loss: 1.555876612663269
Epoch 110, training loss: 1.4872710704803467 = 1.4175224304199219 + 0.01 * 6.974862098693848
Epoch 110, val loss: 1.479954481124878
Epoch 120, training loss: 1.3922061920166016 = 1.32282292842865 + 0.01 * 6.938320636749268
Epoch 120, val loss: 1.4136406183242798
Epoch 130, training loss: 1.3081462383270264 = 1.2390074729919434 + 0.01 * 6.913876533508301
Epoch 130, val loss: 1.3587744235992432
Epoch 140, training loss: 1.2319585084915161 = 1.1629996299743652 + 0.01 * 6.895884037017822
Epoch 140, val loss: 1.3100671768188477
Epoch 150, training loss: 1.1569302082061768 = 1.0880931615829468 + 0.01 * 6.883699893951416
Epoch 150, val loss: 1.2601903676986694
Epoch 160, training loss: 1.0777100324630737 = 1.0089616775512695 + 0.01 * 6.874837398529053
Epoch 160, val loss: 1.204886555671692
Epoch 170, training loss: 0.9928616881370544 = 0.9241841435432434 + 0.01 * 6.8677544593811035
Epoch 170, val loss: 1.1435048580169678
Epoch 180, training loss: 0.9058958292007446 = 0.8372772932052612 + 0.01 * 6.861855983734131
Epoch 180, val loss: 1.0802441835403442
Epoch 190, training loss: 0.8222798109054565 = 0.7537096738815308 + 0.01 * 6.857015132904053
Epoch 190, val loss: 1.0201383829116821
Epoch 200, training loss: 0.7465464472770691 = 0.6780087351799011 + 0.01 * 6.853772163391113
Epoch 200, val loss: 0.9662460684776306
Epoch 210, training loss: 0.6790679693222046 = 0.6105506420135498 + 0.01 * 6.851731777191162
Epoch 210, val loss: 0.9192237854003906
Epoch 220, training loss: 0.6175625324249268 = 0.5490587949752808 + 0.01 * 6.850374221801758
Epoch 220, val loss: 0.8779817819595337
Epoch 230, training loss: 0.5595083236694336 = 0.49101901054382324 + 0.01 * 6.848929405212402
Epoch 230, val loss: 0.8407692909240723
Epoch 240, training loss: 0.5038461089134216 = 0.4353748857975006 + 0.01 * 6.847121715545654
Epoch 240, val loss: 0.8074374198913574
Epoch 250, training loss: 0.4510716497898102 = 0.38262197375297546 + 0.01 * 6.844968795776367
Epoch 250, val loss: 0.7782188057899475
Epoch 260, training loss: 0.40206536650657654 = 0.3336397111415863 + 0.01 * 6.84256649017334
Epoch 260, val loss: 0.7538004517555237
Epoch 270, training loss: 0.35772836208343506 = 0.28932836651802063 + 0.01 * 6.839998722076416
Epoch 270, val loss: 0.7350570559501648
Epoch 280, training loss: 0.31845951080322266 = 0.25008535385131836 + 0.01 * 6.837416172027588
Epoch 280, val loss: 0.7223881483078003
Epoch 290, training loss: 0.28447169065475464 = 0.2161235511302948 + 0.01 * 6.834812641143799
Epoch 290, val loss: 0.7151243686676025
Epoch 300, training loss: 0.2553766071796417 = 0.18705488741397858 + 0.01 * 6.832172393798828
Epoch 300, val loss: 0.712616503238678
Epoch 310, training loss: 0.23053821921348572 = 0.1622370034456253 + 0.01 * 6.830121994018555
Epoch 310, val loss: 0.7143160700798035
Epoch 320, training loss: 0.20929041504859924 = 0.14102324843406677 + 0.01 * 6.826717853546143
Epoch 320, val loss: 0.7198216915130615
Epoch 330, training loss: 0.1912134289741516 = 0.12298871576786041 + 0.01 * 6.822471618652344
Epoch 330, val loss: 0.7286115884780884
Epoch 340, training loss: 0.17588213086128235 = 0.10770218074321747 + 0.01 * 6.817995071411133
Epoch 340, val loss: 0.7395537495613098
Epoch 350, training loss: 0.16281484067440033 = 0.09467369318008423 + 0.01 * 6.814114570617676
Epoch 350, val loss: 0.7519974112510681
Epoch 360, training loss: 0.1515970528125763 = 0.0835297703742981 + 0.01 * 6.806727886199951
Epoch 360, val loss: 0.7652832865715027
Epoch 370, training loss: 0.14224562048912048 = 0.07397197186946869 + 0.01 * 6.827363967895508
Epoch 370, val loss: 0.7788960337638855
Epoch 380, training loss: 0.13369056582450867 = 0.06576790660619736 + 0.01 * 6.792266368865967
Epoch 380, val loss: 0.7926379442214966
Epoch 390, training loss: 0.12655103206634521 = 0.05870389565825462 + 0.01 * 6.7847137451171875
Epoch 390, val loss: 0.8062622547149658
Epoch 400, training loss: 0.12032042443752289 = 0.05260137841105461 + 0.01 * 6.771905422210693
Epoch 400, val loss: 0.8196595907211304
Epoch 410, training loss: 0.1150631308555603 = 0.04731198400259018 + 0.01 * 6.7751145362854
Epoch 410, val loss: 0.832776665687561
Epoch 420, training loss: 0.11024010181427002 = 0.04271471127867699 + 0.01 * 6.752538681030273
Epoch 420, val loss: 0.8455939292907715
Epoch 430, training loss: 0.10623712092638016 = 0.038704030215740204 + 0.01 * 6.75330924987793
Epoch 430, val loss: 0.8581458926200867
Epoch 440, training loss: 0.10270588099956512 = 0.03519434481859207 + 0.01 * 6.751153469085693
Epoch 440, val loss: 0.8703259229660034
Epoch 450, training loss: 0.09958630055189133 = 0.032112449407577515 + 0.01 * 6.747385025024414
Epoch 450, val loss: 0.8823050260543823
Epoch 460, training loss: 0.0967874825000763 = 0.02939740940928459 + 0.01 * 6.739007472991943
Epoch 460, val loss: 0.8938932418823242
Epoch 470, training loss: 0.09430073201656342 = 0.026996543630957603 + 0.01 * 6.730418682098389
Epoch 470, val loss: 0.9052671790122986
Epoch 480, training loss: 0.0921134427189827 = 0.024868248030543327 + 0.01 * 6.7245192527771
Epoch 480, val loss: 0.9163143634796143
Epoch 490, training loss: 0.09018264710903168 = 0.022974137216806412 + 0.01 * 6.720850944519043
Epoch 490, val loss: 0.9270633459091187
Epoch 500, training loss: 0.08866038918495178 = 0.02128002606332302 + 0.01 * 6.738036155700684
Epoch 500, val loss: 0.937539279460907
Epoch 510, training loss: 0.0869070291519165 = 0.019763629883527756 + 0.01 * 6.714340686798096
Epoch 510, val loss: 0.9476891160011292
Epoch 520, training loss: 0.0855078473687172 = 0.018400391563773155 + 0.01 * 6.710745334625244
Epoch 520, val loss: 0.9575800895690918
Epoch 530, training loss: 0.08425880968570709 = 0.01716989278793335 + 0.01 * 6.708891868591309
Epoch 530, val loss: 0.9672353267669678
Epoch 540, training loss: 0.08323536068201065 = 0.016055816784501076 + 0.01 * 6.717954635620117
Epoch 540, val loss: 0.976631224155426
Epoch 550, training loss: 0.08211971819400787 = 0.015045232139527798 + 0.01 * 6.707448482513428
Epoch 550, val loss: 0.9858112335205078
Epoch 560, training loss: 0.08113153278827667 = 0.014125700108706951 + 0.01 * 6.700583457946777
Epoch 560, val loss: 0.994708240032196
Epoch 570, training loss: 0.0802040621638298 = 0.013286958448588848 + 0.01 * 6.691710948944092
Epoch 570, val loss: 1.0034239292144775
Epoch 580, training loss: 0.07956194877624512 = 0.012520097196102142 + 0.01 * 6.704185485839844
Epoch 580, val loss: 1.0119407176971436
Epoch 590, training loss: 0.07867271453142166 = 0.011818421073257923 + 0.01 * 6.685429573059082
Epoch 590, val loss: 1.020189881324768
Epoch 600, training loss: 0.07804016023874283 = 0.011174634099006653 + 0.01 * 6.68655252456665
Epoch 600, val loss: 1.0282716751098633
Epoch 610, training loss: 0.07751765102148056 = 0.010582213290035725 + 0.01 * 6.693543910980225
Epoch 610, val loss: 1.0360982418060303
Epoch 620, training loss: 0.07682639360427856 = 0.010037513449788094 + 0.01 * 6.678888320922852
Epoch 620, val loss: 1.0437959432601929
Epoch 630, training loss: 0.07634543627500534 = 0.009534304961562157 + 0.01 * 6.681113243103027
Epoch 630, val loss: 1.0512183904647827
Epoch 640, training loss: 0.07581347972154617 = 0.009070220403373241 + 0.01 * 6.674326419830322
Epoch 640, val loss: 1.0585441589355469
Epoch 650, training loss: 0.07526711374521255 = 0.008640515618026257 + 0.01 * 6.662660121917725
Epoch 650, val loss: 1.0655632019042969
Epoch 660, training loss: 0.07493827491998672 = 0.00824217963963747 + 0.01 * 6.669609069824219
Epoch 660, val loss: 1.0725018978118896
Epoch 670, training loss: 0.07441893219947815 = 0.007872600108385086 + 0.01 * 6.654633045196533
Epoch 670, val loss: 1.0791807174682617
Epoch 680, training loss: 0.07414767891168594 = 0.007529018446803093 + 0.01 * 6.661865711212158
Epoch 680, val loss: 1.085706114768982
Epoch 690, training loss: 0.07365432381629944 = 0.007209718227386475 + 0.01 * 6.644460678100586
Epoch 690, val loss: 1.092068076133728
Epoch 700, training loss: 0.0732560083270073 = 0.006911975331604481 + 0.01 * 6.634403228759766
Epoch 700, val loss: 1.098263144493103
Epoch 710, training loss: 0.07310299575328827 = 0.006633717566728592 + 0.01 * 6.646927833557129
Epoch 710, val loss: 1.1043065786361694
Epoch 720, training loss: 0.07293590903282166 = 0.006374332122504711 + 0.01 * 6.656157970428467
Epoch 720, val loss: 1.1101171970367432
Epoch 730, training loss: 0.07241114974021912 = 0.006131886038929224 + 0.01 * 6.627926349639893
Epoch 730, val loss: 1.1158668994903564
Epoch 740, training loss: 0.07232368737459183 = 0.00590455112978816 + 0.01 * 6.641913890838623
Epoch 740, val loss: 1.1213994026184082
Epoch 750, training loss: 0.0720248818397522 = 0.005691573955118656 + 0.01 * 6.633330821990967
Epoch 750, val loss: 1.126755952835083
Epoch 760, training loss: 0.07151209563016891 = 0.00549136521294713 + 0.01 * 6.6020731925964355
Epoch 760, val loss: 1.131996989250183
Epoch 770, training loss: 0.071365587413311 = 0.005303137004375458 + 0.01 * 6.606245517730713
Epoch 770, val loss: 1.1371421813964844
Epoch 780, training loss: 0.07130840420722961 = 0.005125929601490498 + 0.01 * 6.618247032165527
Epoch 780, val loss: 1.1420280933380127
Epoch 790, training loss: 0.07103139162063599 = 0.004959024954587221 + 0.01 * 6.607236862182617
Epoch 790, val loss: 1.1469563245773315
Epoch 800, training loss: 0.07109970599412918 = 0.0048014214262366295 + 0.01 * 6.629828453063965
Epoch 800, val loss: 1.1516602039337158
Epoch 810, training loss: 0.07051637023687363 = 0.004652798641473055 + 0.01 * 6.586357116699219
Epoch 810, val loss: 1.1562355756759644
Epoch 820, training loss: 0.07022205740213394 = 0.004512264858931303 + 0.01 * 6.570979595184326
Epoch 820, val loss: 1.160746693611145
Epoch 830, training loss: 0.07034579664468765 = 0.004379138816148043 + 0.01 * 6.596665859222412
Epoch 830, val loss: 1.1651331186294556
Epoch 840, training loss: 0.07016852498054504 = 0.004252950195223093 + 0.01 * 6.59155797958374
Epoch 840, val loss: 1.1693485975265503
Epoch 850, training loss: 0.06994073837995529 = 0.004133223090320826 + 0.01 * 6.580751895904541
Epoch 850, val loss: 1.1735833883285522
Epoch 860, training loss: 0.06964045763015747 = 0.004019558429718018 + 0.01 * 6.562089920043945
Epoch 860, val loss: 1.1776087284088135
Epoch 870, training loss: 0.06941986829042435 = 0.003911575768142939 + 0.01 * 6.5508294105529785
Epoch 870, val loss: 1.1816093921661377
Epoch 880, training loss: 0.06957359611988068 = 0.003808810608461499 + 0.01 * 6.576478958129883
Epoch 880, val loss: 1.1854811906814575
Epoch 890, training loss: 0.06915560364723206 = 0.0037108424585312605 + 0.01 * 6.54447603225708
Epoch 890, val loss: 1.189210057258606
Epoch 900, training loss: 0.06889424473047256 = 0.0036175157874822617 + 0.01 * 6.52767276763916
Epoch 900, val loss: 1.1928881406784058
Epoch 910, training loss: 0.06920687854290009 = 0.0035286296624690294 + 0.01 * 6.567824840545654
Epoch 910, val loss: 1.196561574935913
Epoch 920, training loss: 0.06888434290885925 = 0.0034437698777765036 + 0.01 * 6.544057369232178
Epoch 920, val loss: 1.2000017166137695
Epoch 930, training loss: 0.06853513419628143 = 0.003362783696502447 + 0.01 * 6.517235279083252
Epoch 930, val loss: 1.2034192085266113
Epoch 940, training loss: 0.0687188059091568 = 0.003285343525931239 + 0.01 * 6.543346405029297
Epoch 940, val loss: 1.206797480583191
Epoch 950, training loss: 0.06854602694511414 = 0.0032111969776451588 + 0.01 * 6.533482551574707
Epoch 950, val loss: 1.210044503211975
Epoch 960, training loss: 0.06831948459148407 = 0.0031403431203216314 + 0.01 * 6.517914295196533
Epoch 960, val loss: 1.2132655382156372
Epoch 970, training loss: 0.06812023371458054 = 0.0030724327079951763 + 0.01 * 6.504780292510986
Epoch 970, val loss: 1.2163256406784058
Epoch 980, training loss: 0.06805248558521271 = 0.0030073104426264763 + 0.01 * 6.504518032073975
Epoch 980, val loss: 1.219346046447754
Epoch 990, training loss: 0.06851226091384888 = 0.002944667125120759 + 0.01 * 6.556759357452393
Epoch 990, val loss: 1.2222737073898315
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.7638
Flip ASR: 0.7333/225 nodes
The final ASR:0.71833, 0.03767, Accuracy:0.82469, 0.01222
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11658])
remove edge: torch.Size([2, 9480])
updated graph: torch.Size([2, 10582])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98155, 0.00797, Accuracy:0.83210, 0.00761
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.5 der2:0.3 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.03695011138916 = 1.953211784362793 + 0.01 * 8.3738374710083
Epoch 0, val loss: 1.95486581325531
Epoch 10, training loss: 2.026803731918335 = 1.9430660009384155 + 0.01 * 8.373770713806152
Epoch 10, val loss: 1.9452946186065674
Epoch 20, training loss: 2.0144052505493164 = 1.930669903755188 + 0.01 * 8.373522758483887
Epoch 20, val loss: 1.9331809282302856
Epoch 30, training loss: 1.9969042539596558 = 1.9131755828857422 + 0.01 * 8.372865676879883
Epoch 30, val loss: 1.9158542156219482
Epoch 40, training loss: 1.9709044694900513 = 1.887201189994812 + 0.01 * 8.370326042175293
Epoch 40, val loss: 1.8903796672821045
Epoch 50, training loss: 1.9337198734283447 = 1.8501667976379395 + 0.01 * 8.355313301086426
Epoch 50, val loss: 1.855514407157898
Epoch 60, training loss: 1.8889375925064087 = 1.806222915649414 + 0.01 * 8.271463394165039
Epoch 60, val loss: 1.8177947998046875
Epoch 70, training loss: 1.8442285060882568 = 1.7654005289077759 + 0.01 * 7.882803440093994
Epoch 70, val loss: 1.7849265336990356
Epoch 80, training loss: 1.7917606830596924 = 1.7175507545471191 + 0.01 * 7.420989036560059
Epoch 80, val loss: 1.7414422035217285
Epoch 90, training loss: 1.7225539684295654 = 1.6512354612350464 + 0.01 * 7.131850719451904
Epoch 90, val loss: 1.6819156408309937
Epoch 100, training loss: 1.6343438625335693 = 1.5637409687042236 + 0.01 * 7.060285568237305
Epoch 100, val loss: 1.6076339483261108
Epoch 110, training loss: 1.531901240348816 = 1.4618185758590698 + 0.01 * 7.008269309997559
Epoch 110, val loss: 1.5218720436096191
Epoch 120, training loss: 1.4254639148712158 = 1.3557231426239014 + 0.01 * 6.9740800857543945
Epoch 120, val loss: 1.4344532489776611
Epoch 130, training loss: 1.3191993236541748 = 1.2496459484100342 + 0.01 * 6.955341339111328
Epoch 130, val loss: 1.3507200479507446
Epoch 140, training loss: 1.213243007659912 = 1.1438186168670654 + 0.01 * 6.942439556121826
Epoch 140, val loss: 1.2711232900619507
Epoch 150, training loss: 1.106808066368103 = 1.037498116493225 + 0.01 * 6.93099308013916
Epoch 150, val loss: 1.193017601966858
Epoch 160, training loss: 1.0002918243408203 = 0.9310929179191589 + 0.01 * 6.91988468170166
Epoch 160, val loss: 1.1154708862304688
Epoch 170, training loss: 0.8970255255699158 = 0.8279137015342712 + 0.01 * 6.911181926727295
Epoch 170, val loss: 1.0404587984085083
Epoch 180, training loss: 0.8019598126411438 = 0.7329496741294861 + 0.01 * 6.9010114669799805
Epoch 180, val loss: 0.9724575281143188
Epoch 190, training loss: 0.7181458473205566 = 0.6492295861244202 + 0.01 * 6.891626358032227
Epoch 190, val loss: 0.9149245023727417
Epoch 200, training loss: 0.6455416083335876 = 0.5767090320587158 + 0.01 * 6.883256435394287
Epoch 200, val loss: 0.8687571883201599
Epoch 210, training loss: 0.5822473764419556 = 0.5135248899459839 + 0.01 * 6.872251033782959
Epoch 210, val loss: 0.8326224684715271
Epoch 220, training loss: 0.526138961315155 = 0.4575158655643463 + 0.01 * 6.862311363220215
Epoch 220, val loss: 0.804430365562439
Epoch 230, training loss: 0.4755433201789856 = 0.40697187185287476 + 0.01 * 6.857143878936768
Epoch 230, val loss: 0.7820857167243958
Epoch 240, training loss: 0.42936480045318604 = 0.3608754575252533 + 0.01 * 6.84893274307251
Epoch 240, val loss: 0.7640827298164368
Epoch 250, training loss: 0.38722455501556396 = 0.31879425048828125 + 0.01 * 6.843031406402588
Epoch 250, val loss: 0.7497929334640503
Epoch 260, training loss: 0.3490649163722992 = 0.28068774938583374 + 0.01 * 6.837717533111572
Epoch 260, val loss: 0.7389073967933655
Epoch 270, training loss: 0.31502673029899597 = 0.24658043682575226 + 0.01 * 6.844630241394043
Epoch 270, val loss: 0.7311742901802063
Epoch 280, training loss: 0.28478237986564636 = 0.21640333533287048 + 0.01 * 6.83790397644043
Epoch 280, val loss: 0.7265254855155945
Epoch 290, training loss: 0.2581838369369507 = 0.18988114595413208 + 0.01 * 6.830268859863281
Epoch 290, val loss: 0.7248427867889404
Epoch 300, training loss: 0.23494026064872742 = 0.16667580604553223 + 0.01 * 6.826446533203125
Epoch 300, val loss: 0.7259847521781921
Epoch 310, training loss: 0.21467575430870056 = 0.14643675088882446 + 0.01 * 6.82390022277832
Epoch 310, val loss: 0.7296908497810364
Epoch 320, training loss: 0.19703465700149536 = 0.1288219690322876 + 0.01 * 6.821269989013672
Epoch 320, val loss: 0.7356414198875427
Epoch 330, training loss: 0.18170848488807678 = 0.11349775642156601 + 0.01 * 6.821073055267334
Epoch 330, val loss: 0.7435988783836365
Epoch 340, training loss: 0.16834190487861633 = 0.10017065703868866 + 0.01 * 6.81712532043457
Epoch 340, val loss: 0.7532839775085449
Epoch 350, training loss: 0.15669813752174377 = 0.08857157826423645 + 0.01 * 6.812656879425049
Epoch 350, val loss: 0.7644119262695312
Epoch 360, training loss: 0.14658187329769135 = 0.07847026735544205 + 0.01 * 6.811161041259766
Epoch 360, val loss: 0.7767300605773926
Epoch 370, training loss: 0.1377352774143219 = 0.0696718841791153 + 0.01 * 6.806340217590332
Epoch 370, val loss: 0.7899861931800842
Epoch 380, training loss: 0.1300448775291443 = 0.062012605369091034 + 0.01 * 6.803226947784424
Epoch 380, val loss: 0.8038905262947083
Epoch 390, training loss: 0.12335919588804245 = 0.05533897876739502 + 0.01 * 6.8020219802856445
Epoch 390, val loss: 0.8182459473609924
Epoch 400, training loss: 0.11751379072666168 = 0.049504589289426804 + 0.01 * 6.800920009613037
Epoch 400, val loss: 0.832859456539154
Epoch 410, training loss: 0.11236070096492767 = 0.044412024319171906 + 0.01 * 6.794867515563965
Epoch 410, val loss: 0.8475971817970276
Epoch 420, training loss: 0.1078433021903038 = 0.039964981377124786 + 0.01 * 6.787832260131836
Epoch 420, val loss: 0.8622855544090271
Epoch 430, training loss: 0.1039140596985817 = 0.03607980161905289 + 0.01 * 6.783426284790039
Epoch 430, val loss: 0.8768079280853271
Epoch 440, training loss: 0.10048605501651764 = 0.03268244490027428 + 0.01 * 6.780361652374268
Epoch 440, val loss: 0.8910552859306335
Epoch 450, training loss: 0.09743155539035797 = 0.029705986380577087 + 0.01 * 6.772556781768799
Epoch 450, val loss: 0.905017614364624
Epoch 460, training loss: 0.0948306992650032 = 0.02709338068962097 + 0.01 * 6.7737321853637695
Epoch 460, val loss: 0.9186209440231323
Epoch 470, training loss: 0.09239398688077927 = 0.02479422092437744 + 0.01 * 6.759976387023926
Epoch 470, val loss: 0.9318447709083557
Epoch 480, training loss: 0.09035352617502213 = 0.022761620581150055 + 0.01 * 6.759190559387207
Epoch 480, val loss: 0.9447647929191589
Epoch 490, training loss: 0.08873898535966873 = 0.020948870107531548 + 0.01 * 6.7790117263793945
Epoch 490, val loss: 0.9575648307800293
Epoch 500, training loss: 0.08684412389993668 = 0.019344503059983253 + 0.01 * 6.749962329864502
Epoch 500, val loss: 0.9698107838630676
Epoch 510, training loss: 0.0853448361158371 = 0.017918294295668602 + 0.01 * 6.742654323577881
Epoch 510, val loss: 0.9816660284996033
Epoch 520, training loss: 0.08398611098527908 = 0.016643358394503593 + 0.01 * 6.7342753410339355
Epoch 520, val loss: 0.9931247234344482
Epoch 530, training loss: 0.08281069993972778 = 0.015501289628446102 + 0.01 * 6.730940818786621
Epoch 530, val loss: 1.0042661428451538
Epoch 540, training loss: 0.0817646011710167 = 0.014474638737738132 + 0.01 * 6.728996753692627
Epoch 540, val loss: 1.015160083770752
Epoch 550, training loss: 0.0807071328163147 = 0.013548432849347591 + 0.01 * 6.715870380401611
Epoch 550, val loss: 1.0256661176681519
Epoch 560, training loss: 0.08001666516065598 = 0.01271080132573843 + 0.01 * 6.730586528778076
Epoch 560, val loss: 1.0359159708023071
Epoch 570, training loss: 0.07902304828166962 = 0.011951952241361141 + 0.01 * 6.7071099281311035
Epoch 570, val loss: 1.0458405017852783
Epoch 580, training loss: 0.07830469310283661 = 0.011262241750955582 + 0.01 * 6.704245090484619
Epoch 580, val loss: 1.0554780960083008
Epoch 590, training loss: 0.07761149108409882 = 0.010633233934640884 + 0.01 * 6.697825908660889
Epoch 590, val loss: 1.0648471117019653
Epoch 600, training loss: 0.0770479366183281 = 0.010058374144136906 + 0.01 * 6.698956489562988
Epoch 600, val loss: 1.0739521980285645
Epoch 610, training loss: 0.07643310725688934 = 0.009531816467642784 + 0.01 * 6.690128803253174
Epoch 610, val loss: 1.0827560424804688
Epoch 620, training loss: 0.07588816434144974 = 0.009048049338161945 + 0.01 * 6.684011459350586
Epoch 620, val loss: 1.0913785696029663
Epoch 630, training loss: 0.0753830224275589 = 0.008602332323789597 + 0.01 * 6.678069114685059
Epoch 630, val loss: 1.0997438430786133
Epoch 640, training loss: 0.07508683204650879 = 0.008190860971808434 + 0.01 * 6.689597129821777
Epoch 640, val loss: 1.1079261302947998
Epoch 650, training loss: 0.07450227439403534 = 0.007810588926076889 + 0.01 * 6.669168472290039
Epoch 650, val loss: 1.1158322095870972
Epoch 660, training loss: 0.07403599470853806 = 0.007457984611392021 + 0.01 * 6.657801151275635
Epoch 660, val loss: 1.1235923767089844
Epoch 670, training loss: 0.07369422167539597 = 0.007130645215511322 + 0.01 * 6.656357765197754
Epoch 670, val loss: 1.1311438083648682
Epoch 680, training loss: 0.0732901319861412 = 0.006826076190918684 + 0.01 * 6.6464056968688965
Epoch 680, val loss: 1.1385183334350586
Epoch 690, training loss: 0.0733095034956932 = 0.006542213726788759 + 0.01 * 6.676729202270508
Epoch 690, val loss: 1.145691990852356
Epoch 700, training loss: 0.07274174690246582 = 0.006278206128627062 + 0.01 * 6.6463541984558105
Epoch 700, val loss: 1.1526952981948853
Epoch 710, training loss: 0.07239516079425812 = 0.006031876429915428 + 0.01 * 6.636328220367432
Epoch 710, val loss: 1.1595178842544556
Epoch 720, training loss: 0.0722782164812088 = 0.005801042541861534 + 0.01 * 6.647716999053955
Epoch 720, val loss: 1.1661885976791382
Epoch 730, training loss: 0.0719623938202858 = 0.00558472890406847 + 0.01 * 6.637766361236572
Epoch 730, val loss: 1.1727014780044556
Epoch 740, training loss: 0.0715562254190445 = 0.005381663795560598 + 0.01 * 6.617455959320068
Epoch 740, val loss: 1.1790733337402344
Epoch 750, training loss: 0.07145938277244568 = 0.005190718919038773 + 0.01 * 6.626865863800049
Epoch 750, val loss: 1.1853792667388916
Epoch 760, training loss: 0.07126763463020325 = 0.005011348519474268 + 0.01 * 6.625628471374512
Epoch 760, val loss: 1.191441535949707
Epoch 770, training loss: 0.0709785744547844 = 0.004842379596084356 + 0.01 * 6.613619804382324
Epoch 770, val loss: 1.1973686218261719
Epoch 780, training loss: 0.07086770236492157 = 0.004683133214712143 + 0.01 * 6.6184563636779785
Epoch 780, val loss: 1.2032338380813599
Epoch 790, training loss: 0.07057945430278778 = 0.004532746504992247 + 0.01 * 6.604671001434326
Epoch 790, val loss: 1.2089277505874634
Epoch 800, training loss: 0.0704326331615448 = 0.004390429705381393 + 0.01 * 6.604220390319824
Epoch 800, val loss: 1.2144984006881714
Epoch 810, training loss: 0.0702197402715683 = 0.004256016109138727 + 0.01 * 6.596372604370117
Epoch 810, val loss: 1.2199264764785767
Epoch 820, training loss: 0.07016322761774063 = 0.004128722939640284 + 0.01 * 6.603450775146484
Epoch 820, val loss: 1.2252535820007324
Epoch 830, training loss: 0.06997597217559814 = 0.004007871728390455 + 0.01 * 6.596810817718506
Epoch 830, val loss: 1.2304058074951172
Epoch 840, training loss: 0.06973566859960556 = 0.003893099958077073 + 0.01 * 6.584257125854492
Epoch 840, val loss: 1.235511302947998
Epoch 850, training loss: 0.06958039104938507 = 0.0037839089054614305 + 0.01 * 6.579648017883301
Epoch 850, val loss: 1.2404983043670654
Epoch 860, training loss: 0.06959318369626999 = 0.0036802326794713736 + 0.01 * 6.59129524230957
Epoch 860, val loss: 1.2454242706298828
Epoch 870, training loss: 0.06934744119644165 = 0.003581669880077243 + 0.01 * 6.576577186584473
Epoch 870, val loss: 1.250152587890625
Epoch 880, training loss: 0.06928934156894684 = 0.003487653797492385 + 0.01 * 6.580169200897217
Epoch 880, val loss: 1.2547732591629028
Epoch 890, training loss: 0.06910347193479538 = 0.0033982477616518736 + 0.01 * 6.570522308349609
Epoch 890, val loss: 1.2593791484832764
Epoch 900, training loss: 0.06901098787784576 = 0.0033126927446573973 + 0.01 * 6.56982946395874
Epoch 900, val loss: 1.2638320922851562
Epoch 910, training loss: 0.0689745768904686 = 0.0032311344984918833 + 0.01 * 6.574343681335449
Epoch 910, val loss: 1.2682100534439087
Epoch 920, training loss: 0.06890502572059631 = 0.0031533255241811275 + 0.01 * 6.575170516967773
Epoch 920, val loss: 1.272554874420166
Epoch 930, training loss: 0.06873178482055664 = 0.003079024376347661 + 0.01 * 6.5652756690979
Epoch 930, val loss: 1.276672601699829
Epoch 940, training loss: 0.06856317818164825 = 0.0030080173164606094 + 0.01 * 6.555516242980957
Epoch 940, val loss: 1.28079092502594
Epoch 950, training loss: 0.06849655508995056 = 0.002939885249361396 + 0.01 * 6.555666923522949
Epoch 950, val loss: 1.2848271131515503
Epoch 960, training loss: 0.06844480335712433 = 0.002874587429687381 + 0.01 * 6.557021617889404
Epoch 960, val loss: 1.288723111152649
Epoch 970, training loss: 0.0682421624660492 = 0.0028119878843426704 + 0.01 * 6.543017387390137
Epoch 970, val loss: 1.2925968170166016
Epoch 980, training loss: 0.06808020919561386 = 0.0027519050054252148 + 0.01 * 6.532830715179443
Epoch 980, val loss: 1.2963703870773315
Epoch 990, training loss: 0.06809469312429428 = 0.0026942358817905188 + 0.01 * 6.540045738220215
Epoch 990, val loss: 1.3001447916030884
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.5277
Flip ASR: 0.4400/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.018342971801758 = 1.9346050024032593 + 0.01 * 8.373799324035645
Epoch 0, val loss: 1.942571759223938
Epoch 10, training loss: 2.008481979370117 = 1.924744725227356 + 0.01 * 8.3737154006958
Epoch 10, val loss: 1.9326963424682617
Epoch 20, training loss: 1.9967200756072998 = 1.9129860401153564 + 0.01 * 8.373404502868652
Epoch 20, val loss: 1.920416235923767
Epoch 30, training loss: 1.9805986881256104 = 1.8968727588653564 + 0.01 * 8.372588157653809
Epoch 30, val loss: 1.9033193588256836
Epoch 40, training loss: 1.9574459791183472 = 1.8737515211105347 + 0.01 * 8.3694486618042
Epoch 40, val loss: 1.878872036933899
Epoch 50, training loss: 1.925059199333191 = 1.8415515422821045 + 0.01 * 8.35076904296875
Epoch 50, val loss: 1.845751404762268
Epoch 60, training loss: 1.8839783668518066 = 1.801732063293457 + 0.01 * 8.224626541137695
Epoch 60, val loss: 1.8064312934875488
Epoch 70, training loss: 1.835990309715271 = 1.7589164972305298 + 0.01 * 7.707381725311279
Epoch 70, val loss: 1.7649431228637695
Epoch 80, training loss: 1.7811381816864014 = 1.707813024520874 + 0.01 * 7.332515239715576
Epoch 80, val loss: 1.716099739074707
Epoch 90, training loss: 1.7102054357528687 = 1.6384572982788086 + 0.01 * 7.174819469451904
Epoch 90, val loss: 1.6541149616241455
Epoch 100, training loss: 1.6180241107940674 = 1.5470503568649292 + 0.01 * 7.097378253936768
Epoch 100, val loss: 1.576680064201355
Epoch 110, training loss: 1.5073809623718262 = 1.4368977546691895 + 0.01 * 7.0483198165893555
Epoch 110, val loss: 1.4853367805480957
Epoch 120, training loss: 1.3884434700012207 = 1.3183026313781738 + 0.01 * 7.0140814781188965
Epoch 120, val loss: 1.3891066312789917
Epoch 130, training loss: 1.2711762189865112 = 1.2012975215911865 + 0.01 * 6.987874507904053
Epoch 130, val loss: 1.296276569366455
Epoch 140, training loss: 1.160891056060791 = 1.091187834739685 + 0.01 * 6.970318794250488
Epoch 140, val loss: 1.2104612588882446
Epoch 150, training loss: 1.0585747957229614 = 0.9889703989028931 + 0.01 * 6.960440158843994
Epoch 150, val loss: 1.1319161653518677
Epoch 160, training loss: 0.9639294743537903 = 0.8943829536437988 + 0.01 * 6.954649925231934
Epoch 160, val loss: 1.0600813627243042
Epoch 170, training loss: 0.8767994046211243 = 0.8073002696037292 + 0.01 * 6.949912071228027
Epoch 170, val loss: 0.9952904582023621
Epoch 180, training loss: 0.7972133159637451 = 0.7277497053146362 + 0.01 * 6.946358680725098
Epoch 180, val loss: 0.9377758502960205
Epoch 190, training loss: 0.7250925302505493 = 0.6556495428085327 + 0.01 * 6.944299221038818
Epoch 190, val loss: 0.8880321979522705
Epoch 200, training loss: 0.6595531105995178 = 0.5901153683662415 + 0.01 * 6.943774700164795
Epoch 200, val loss: 0.8458056449890137
Epoch 210, training loss: 0.5989738702774048 = 0.529535174369812 + 0.01 * 6.943871021270752
Epoch 210, val loss: 0.8099081516265869
Epoch 220, training loss: 0.5420388579368591 = 0.47259318828582764 + 0.01 * 6.944565296173096
Epoch 220, val loss: 0.7788828611373901
Epoch 230, training loss: 0.488248348236084 = 0.418795108795166 + 0.01 * 6.945323467254639
Epoch 230, val loss: 0.7518765926361084
Epoch 240, training loss: 0.4378277063369751 = 0.36836859583854675 + 0.01 * 6.945911884307861
Epoch 240, val loss: 0.7288147211074829
Epoch 250, training loss: 0.3913707137107849 = 0.32190972566604614 + 0.01 * 6.946099281311035
Epoch 250, val loss: 0.7097075581550598
Epoch 260, training loss: 0.3494391143321991 = 0.2799789309501648 + 0.01 * 6.946018218994141
Epoch 260, val loss: 0.6947241425514221
Epoch 270, training loss: 0.3124159872531891 = 0.24296528100967407 + 0.01 * 6.945069789886475
Epoch 270, val loss: 0.6840032339096069
Epoch 280, training loss: 0.28038129210472107 = 0.21094809472560883 + 0.01 * 6.943319320678711
Epoch 280, val loss: 0.6771625280380249
Epoch 290, training loss: 0.25301802158355713 = 0.18360912799835205 + 0.01 * 6.940889835357666
Epoch 290, val loss: 0.6740412712097168
Epoch 300, training loss: 0.22976601123809814 = 0.16039182245731354 + 0.01 * 6.937417984008789
Epoch 300, val loss: 0.6740850210189819
Epoch 310, training loss: 0.21000275015830994 = 0.14066500961780548 + 0.01 * 6.933774471282959
Epoch 310, val loss: 0.6767420768737793
Epoch 320, training loss: 0.1931573748588562 = 0.1238345354795456 + 0.01 * 6.932284355163574
Epoch 320, val loss: 0.6814218759536743
Epoch 330, training loss: 0.1786152571439743 = 0.10937569290399551 + 0.01 * 6.923956394195557
Epoch 330, val loss: 0.6876648664474487
Epoch 340, training loss: 0.16598103940486908 = 0.09686113893985748 + 0.01 * 6.911990165710449
Epoch 340, val loss: 0.6951205134391785
Epoch 350, training loss: 0.15497900545597076 = 0.08595146983861923 + 0.01 * 6.902753829956055
Epoch 350, val loss: 0.7032600045204163
Epoch 360, training loss: 0.1453835368156433 = 0.07641921192407608 + 0.01 * 6.896431922912598
Epoch 360, val loss: 0.7118846774101257
Epoch 370, training loss: 0.13698382675647736 = 0.0680926963686943 + 0.01 * 6.889112949371338
Epoch 370, val loss: 0.7207215428352356
Epoch 380, training loss: 0.12960368394851685 = 0.060847342014312744 + 0.01 * 6.875633239746094
Epoch 380, val loss: 0.7297053933143616
Epoch 390, training loss: 0.12325549870729446 = 0.05455612391233444 + 0.01 * 6.869937896728516
Epoch 390, val loss: 0.738692581653595
Epoch 400, training loss: 0.11755119264125824 = 0.04910760000348091 + 0.01 * 6.844358921051025
Epoch 400, val loss: 0.7477894425392151
Epoch 410, training loss: 0.11285468935966492 = 0.04438602551817894 + 0.01 * 6.846866607666016
Epoch 410, val loss: 0.75677090883255
Epoch 420, training loss: 0.10851995646953583 = 0.040291495621204376 + 0.01 * 6.822845935821533
Epoch 420, val loss: 0.7659974098205566
Epoch 430, training loss: 0.10471636056900024 = 0.03671681880950928 + 0.01 * 6.799954414367676
Epoch 430, val loss: 0.7749468684196472
Epoch 440, training loss: 0.10199341177940369 = 0.03357848897576332 + 0.01 * 6.841492176055908
Epoch 440, val loss: 0.7840497493743896
Epoch 450, training loss: 0.09870588034391403 = 0.030815547332167625 + 0.01 * 6.789032936096191
Epoch 450, val loss: 0.7929879426956177
Epoch 460, training loss: 0.09610090404748917 = 0.028372088447213173 + 0.01 * 6.772881984710693
Epoch 460, val loss: 0.8018113374710083
Epoch 470, training loss: 0.09382683038711548 = 0.02619827911257744 + 0.01 * 6.7628560066223145
Epoch 470, val loss: 0.8105200529098511
Epoch 480, training loss: 0.09188597649335861 = 0.024257877841591835 + 0.01 * 6.762810230255127
Epoch 480, val loss: 0.8191685676574707
Epoch 490, training loss: 0.09002546966075897 = 0.022518396377563477 + 0.01 * 6.750707626342773
Epoch 490, val loss: 0.8276667594909668
Epoch 500, training loss: 0.08850991725921631 = 0.020953163504600525 + 0.01 * 6.755675315856934
Epoch 500, val loss: 0.8359613418579102
Epoch 510, training loss: 0.08700910210609436 = 0.01954595372080803 + 0.01 * 6.7463154792785645
Epoch 510, val loss: 0.8442085981369019
Epoch 520, training loss: 0.08557900041341782 = 0.01827843300998211 + 0.01 * 6.7300567626953125
Epoch 520, val loss: 0.8521559238433838
Epoch 530, training loss: 0.08454228192567825 = 0.017130078747868538 + 0.01 * 6.741220951080322
Epoch 530, val loss: 0.860011100769043
Epoch 540, training loss: 0.08329984545707703 = 0.016088023781776428 + 0.01 * 6.721182346343994
Epoch 540, val loss: 0.8677374720573425
Epoch 550, training loss: 0.08229430019855499 = 0.015139085240662098 + 0.01 * 6.715521335601807
Epoch 550, val loss: 0.8752054572105408
Epoch 560, training loss: 0.08140993118286133 = 0.014272396452724934 + 0.01 * 6.713753700256348
Epoch 560, val loss: 0.8826231956481934
Epoch 570, training loss: 0.08053582161664963 = 0.013479425571858883 + 0.01 * 6.705639362335205
Epoch 570, val loss: 0.8897711038589478
Epoch 580, training loss: 0.07984299957752228 = 0.012751915492117405 + 0.01 * 6.709108829498291
Epoch 580, val loss: 0.8968155980110168
Epoch 590, training loss: 0.07910509407520294 = 0.012083559297025204 + 0.01 * 6.70215368270874
Epoch 590, val loss: 0.9037380814552307
Epoch 600, training loss: 0.07841385900974274 = 0.011468648910522461 + 0.01 * 6.694520950317383
Epoch 600, val loss: 0.910415530204773
Epoch 610, training loss: 0.07773188501596451 = 0.01090113166719675 + 0.01 * 6.683075428009033
Epoch 610, val loss: 0.9169866442680359
Epoch 620, training loss: 0.07718180865049362 = 0.010376575402915478 + 0.01 * 6.680523872375488
Epoch 620, val loss: 0.9233755469322205
Epoch 630, training loss: 0.07667151093482971 = 0.009890493005514145 + 0.01 * 6.678102016448975
Epoch 630, val loss: 0.9297048449516296
Epoch 640, training loss: 0.07620647549629211 = 0.00943966768682003 + 0.01 * 6.676681041717529
Epoch 640, val loss: 0.9357662200927734
Epoch 650, training loss: 0.07573455572128296 = 0.009021158330142498 + 0.01 * 6.671340465545654
Epoch 650, val loss: 0.9418092966079712
Epoch 660, training loss: 0.07525351643562317 = 0.00863127876073122 + 0.01 * 6.6622233390808105
Epoch 660, val loss: 0.9476313591003418
Epoch 670, training loss: 0.07506879419088364 = 0.008267693221569061 + 0.01 * 6.680109977722168
Epoch 670, val loss: 0.9533838629722595
Epoch 680, training loss: 0.07449375092983246 = 0.007928173989057541 + 0.01 * 6.656557559967041
Epoch 680, val loss: 0.9589596390724182
Epoch 690, training loss: 0.07421769201755524 = 0.007610883563756943 + 0.01 * 6.660680294036865
Epoch 690, val loss: 0.9644437432289124
Epoch 700, training loss: 0.0738978236913681 = 0.007314383517950773 + 0.01 * 6.658344268798828
Epoch 700, val loss: 0.9697542786598206
Epoch 710, training loss: 0.07342842221260071 = 0.007036426570266485 + 0.01 * 6.639199733734131
Epoch 710, val loss: 0.9749719500541687
Epoch 720, training loss: 0.07317207008600235 = 0.006775368936359882 + 0.01 * 6.639669895172119
Epoch 720, val loss: 0.9801352024078369
Epoch 730, training loss: 0.07293138653039932 = 0.006529767997562885 + 0.01 * 6.640161514282227
Epoch 730, val loss: 0.9851365685462952
Epoch 740, training loss: 0.07256212085485458 = 0.006298375315964222 + 0.01 * 6.626374244689941
Epoch 740, val loss: 0.9900107979774475
Epoch 750, training loss: 0.07235386222600937 = 0.006080183666199446 + 0.01 * 6.627367973327637
Epoch 750, val loss: 0.9948191046714783
Epoch 760, training loss: 0.07209521532058716 = 0.005874477326869965 + 0.01 * 6.622073650360107
Epoch 760, val loss: 0.9995079040527344
Epoch 770, training loss: 0.07184484601020813 = 0.005680139176547527 + 0.01 * 6.616471290588379
Epoch 770, val loss: 1.004095435142517
Epoch 780, training loss: 0.0716184750199318 = 0.005496537778526545 + 0.01 * 6.612193584442139
Epoch 780, val loss: 1.0085349082946777
Epoch 790, training loss: 0.07143812626600266 = 0.00532326428219676 + 0.01 * 6.611486434936523
Epoch 790, val loss: 1.0129148960113525
Epoch 800, training loss: 0.0712108463048935 = 0.005158950109034777 + 0.01 * 6.605189323425293
Epoch 800, val loss: 1.0172533988952637
Epoch 810, training loss: 0.07100092619657516 = 0.0050032855942845345 + 0.01 * 6.599763870239258
Epoch 810, val loss: 1.0214807987213135
Epoch 820, training loss: 0.07080540806055069 = 0.004855492617934942 + 0.01 * 6.594991683959961
Epoch 820, val loss: 1.0255883932113647
Epoch 830, training loss: 0.0705912783741951 = 0.0047149364836514 + 0.01 * 6.587634086608887
Epoch 830, val loss: 1.0296921730041504
Epoch 840, training loss: 0.07060262560844421 = 0.004581362474709749 + 0.01 * 6.602126598358154
Epoch 840, val loss: 1.0335298776626587
Epoch 850, training loss: 0.07036720216274261 = 0.004454397596418858 + 0.01 * 6.591280937194824
Epoch 850, val loss: 1.037457823753357
Epoch 860, training loss: 0.07022466510534286 = 0.004333304241299629 + 0.01 * 6.589136123657227
Epoch 860, val loss: 1.0412389039993286
Epoch 870, training loss: 0.07002385705709457 = 0.004218103364109993 + 0.01 * 6.580575942993164
Epoch 870, val loss: 1.0449265241622925
Epoch 880, training loss: 0.06979518383741379 = 0.004108679015189409 + 0.01 * 6.568650245666504
Epoch 880, val loss: 1.0485671758651733
Epoch 890, training loss: 0.06970829516649246 = 0.004004196729511023 + 0.01 * 6.570409774780273
Epoch 890, val loss: 1.0521109104156494
Epoch 900, training loss: 0.06948526203632355 = 0.003904378041625023 + 0.01 * 6.558088302612305
Epoch 900, val loss: 1.055633783340454
Epoch 910, training loss: 0.06963513046503067 = 0.0038090324960649014 + 0.01 * 6.5826096534729
Epoch 910, val loss: 1.0589723587036133
Epoch 920, training loss: 0.06936920434236526 = 0.0037177621852606535 + 0.01 * 6.565144062042236
Epoch 920, val loss: 1.0623745918273926
Epoch 930, training loss: 0.0691155195236206 = 0.0036306835245341063 + 0.01 * 6.548483848571777
Epoch 930, val loss: 1.065671443939209
Epoch 940, training loss: 0.06917524337768555 = 0.003546935273334384 + 0.01 * 6.562830924987793
Epoch 940, val loss: 1.0688540935516357
Epoch 950, training loss: 0.06886716187000275 = 0.003467216854915023 + 0.01 * 6.539994716644287
Epoch 950, val loss: 1.07199227809906
Epoch 960, training loss: 0.06895837187767029 = 0.0033907308243215084 + 0.01 * 6.556764602661133
Epoch 960, val loss: 1.0750863552093506
Epoch 970, training loss: 0.06871122866868973 = 0.0033171160612255335 + 0.01 * 6.5394110679626465
Epoch 970, val loss: 1.0781805515289307
Epoch 980, training loss: 0.06869575381278992 = 0.0032464887481182814 + 0.01 * 6.54492712020874
Epoch 980, val loss: 1.0810787677764893
Epoch 990, training loss: 0.06844939291477203 = 0.0031787948682904243 + 0.01 * 6.527059555053711
Epoch 990, val loss: 1.084049940109253
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.8339
Flip ASR: 0.8044/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.052565097808838 = 1.9688268899917603 + 0.01 * 8.37381362915039
Epoch 0, val loss: 1.9573357105255127
Epoch 10, training loss: 2.041715621948242 = 1.9579781293869019 + 0.01 * 8.373745918273926
Epoch 10, val loss: 1.9470734596252441
Epoch 20, training loss: 2.028573513031006 = 1.944838523864746 + 0.01 * 8.373496055603027
Epoch 20, val loss: 1.9342572689056396
Epoch 30, training loss: 2.0102484226226807 = 1.926519513130188 + 0.01 * 8.372901916503906
Epoch 30, val loss: 1.916055679321289
Epoch 40, training loss: 1.9828948974609375 = 1.8991869688034058 + 0.01 * 8.370790481567383
Epoch 40, val loss: 1.8889027833938599
Epoch 50, training loss: 1.9429067373275757 = 1.8593363761901855 + 0.01 * 8.357034683227539
Epoch 50, val loss: 1.8505537509918213
Epoch 60, training loss: 1.8929479122161865 = 1.8103126287460327 + 0.01 * 8.263532638549805
Epoch 60, val loss: 1.807610273361206
Epoch 70, training loss: 1.8441871404647827 = 1.7661126852035522 + 0.01 * 7.8074469566345215
Epoch 70, val loss: 1.7740813493728638
Epoch 80, training loss: 1.7922087907791138 = 1.7201969623565674 + 0.01 * 7.201179027557373
Epoch 80, val loss: 1.736497163772583
Epoch 90, training loss: 1.7286372184753418 = 1.6581395864486694 + 0.01 * 7.0497589111328125
Epoch 90, val loss: 1.6836403608322144
Epoch 100, training loss: 1.6460400819778442 = 1.5760902166366577 + 0.01 * 6.994989395141602
Epoch 100, val loss: 1.615344762802124
Epoch 110, training loss: 1.548675775527954 = 1.479004979133606 + 0.01 * 6.9670796394348145
Epoch 110, val loss: 1.5375107526779175
Epoch 120, training loss: 1.4490617513656616 = 1.3795714378356934 + 0.01 * 6.949028491973877
Epoch 120, val loss: 1.461324691772461
Epoch 130, training loss: 1.353170394897461 = 1.283827543258667 + 0.01 * 6.93428897857666
Epoch 130, val loss: 1.3910744190216064
Epoch 140, training loss: 1.259598731994629 = 1.1904404163360596 + 0.01 * 6.915835857391357
Epoch 140, val loss: 1.3238906860351562
Epoch 150, training loss: 1.1668801307678223 = 1.0979055166244507 + 0.01 * 6.897459030151367
Epoch 150, val loss: 1.2571539878845215
Epoch 160, training loss: 1.075974941253662 = 1.0071485042572021 + 0.01 * 6.882641315460205
Epoch 160, val loss: 1.1900757551193237
Epoch 170, training loss: 0.9881080389022827 = 0.9193432331085205 + 0.01 * 6.876480579376221
Epoch 170, val loss: 1.1236616373062134
Epoch 180, training loss: 0.9032385945320129 = 0.8345696330070496 + 0.01 * 6.866896629333496
Epoch 180, val loss: 1.0584133863449097
Epoch 190, training loss: 0.8212535977363586 = 0.7526841163635254 + 0.01 * 6.856950283050537
Epoch 190, val loss: 0.9961320757865906
Epoch 200, training loss: 0.7431286573410034 = 0.6746225357055664 + 0.01 * 6.850612163543701
Epoch 200, val loss: 0.9381054043769836
Epoch 210, training loss: 0.670574963092804 = 0.6021759510040283 + 0.01 * 6.839902877807617
Epoch 210, val loss: 0.8860456943511963
Epoch 220, training loss: 0.6047753691673279 = 0.5364730358123779 + 0.01 * 6.830233097076416
Epoch 220, val loss: 0.8415527939796448
Epoch 230, training loss: 0.5457199811935425 = 0.4774158298969269 + 0.01 * 6.830413818359375
Epoch 230, val loss: 0.804557740688324
Epoch 240, training loss: 0.4924577474594116 = 0.4243009090423584 + 0.01 * 6.8156843185424805
Epoch 240, val loss: 0.7744424939155579
Epoch 250, training loss: 0.4445045590400696 = 0.37642166018486023 + 0.01 * 6.808291435241699
Epoch 250, val loss: 0.7501925230026245
Epoch 260, training loss: 0.4013791084289551 = 0.33336254954338074 + 0.01 * 6.801656246185303
Epoch 260, val loss: 0.7315900325775146
Epoch 270, training loss: 0.3627884089946747 = 0.2947576940059662 + 0.01 * 6.80307149887085
Epoch 270, val loss: 0.7182794213294983
Epoch 280, training loss: 0.3279653787612915 = 0.26002976298332214 + 0.01 * 6.79356050491333
Epoch 280, val loss: 0.7099141478538513
Epoch 290, training loss: 0.2965776324272156 = 0.2286594957113266 + 0.01 * 6.791815280914307
Epoch 290, val loss: 0.705855131149292
Epoch 300, training loss: 0.26829931139945984 = 0.2003486007452011 + 0.01 * 6.795071125030518
Epoch 300, val loss: 0.7054997682571411
Epoch 310, training loss: 0.2429487705230713 = 0.175058975815773 + 0.01 * 6.788978576660156
Epoch 310, val loss: 0.7084890604019165
Epoch 320, training loss: 0.22056904435157776 = 0.15273037552833557 + 0.01 * 6.7838664054870605
Epoch 320, val loss: 0.7141653299331665
Epoch 330, training loss: 0.20102089643478394 = 0.13324078917503357 + 0.01 * 6.778010368347168
Epoch 330, val loss: 0.7219820618629456
Epoch 340, training loss: 0.18416835367679596 = 0.116390660405159 + 0.01 * 6.777769565582275
Epoch 340, val loss: 0.7315266728401184
Epoch 350, training loss: 0.16958750784397125 = 0.10187199711799622 + 0.01 * 6.771551132202148
Epoch 350, val loss: 0.7425348162651062
Epoch 360, training loss: 0.1571086049079895 = 0.08939285576343536 + 0.01 * 6.771574974060059
Epoch 360, val loss: 0.7548041939735413
Epoch 370, training loss: 0.14640581607818604 = 0.07869654148817062 + 0.01 * 6.770927429199219
Epoch 370, val loss: 0.7681072950363159
Epoch 380, training loss: 0.1371164321899414 = 0.0695345550775528 + 0.01 * 6.758188724517822
Epoch 380, val loss: 0.7822429537773132
Epoch 390, training loss: 0.12943363189697266 = 0.061677414923906326 + 0.01 * 6.77562141418457
Epoch 390, val loss: 0.7969814538955688
Epoch 400, training loss: 0.12243147194385529 = 0.05493972450494766 + 0.01 * 6.749175071716309
Epoch 400, val loss: 0.8121374249458313
Epoch 410, training loss: 0.11673440784215927 = 0.049146220088005066 + 0.01 * 6.758819103240967
Epoch 410, val loss: 0.8274641036987305
Epoch 420, training loss: 0.11158019304275513 = 0.04415305331349373 + 0.01 * 6.742713928222656
Epoch 420, val loss: 0.8428402543067932
Epoch 430, training loss: 0.10723195970058441 = 0.03983287513256073 + 0.01 * 6.739908695220947
Epoch 430, val loss: 0.8581573963165283
Epoch 440, training loss: 0.1034957617521286 = 0.03608532249927521 + 0.01 * 6.741044044494629
Epoch 440, val loss: 0.8732361793518066
Epoch 450, training loss: 0.1000821590423584 = 0.032823096960783005 + 0.01 * 6.725906848907471
Epoch 450, val loss: 0.8881153464317322
Epoch 460, training loss: 0.09719711542129517 = 0.029972005635499954 + 0.01 * 6.722511291503906
Epoch 460, val loss: 0.9025989770889282
Epoch 470, training loss: 0.09462438523769379 = 0.02747035212814808 + 0.01 * 6.7154035568237305
Epoch 470, val loss: 0.9167155623435974
Epoch 480, training loss: 0.09242209792137146 = 0.025265052914619446 + 0.01 * 6.715704441070557
Epoch 480, val loss: 0.9305132627487183
Epoch 490, training loss: 0.09032870084047318 = 0.0233153123408556 + 0.01 * 6.701339244842529
Epoch 490, val loss: 0.9438493847846985
Epoch 500, training loss: 0.08869373053312302 = 0.021584348753094673 + 0.01 * 6.710938453674316
Epoch 500, val loss: 0.956856369972229
Epoch 510, training loss: 0.08690455555915833 = 0.020043279975652695 + 0.01 * 6.686127185821533
Epoch 510, val loss: 0.9694865942001343
Epoch 520, training loss: 0.08567608147859573 = 0.018664374947547913 + 0.01 * 6.701170921325684
Epoch 520, val loss: 0.9817007780075073
Epoch 530, training loss: 0.08426449447870255 = 0.017427479848265648 + 0.01 * 6.683701515197754
Epoch 530, val loss: 0.9935664534568787
Epoch 540, training loss: 0.08313867449760437 = 0.01631305180490017 + 0.01 * 6.682562351226807
Epoch 540, val loss: 1.0050803422927856
Epoch 550, training loss: 0.08183477818965912 = 0.015306822024285793 + 0.01 * 6.652795314788818
Epoch 550, val loss: 1.0163148641586304
Epoch 560, training loss: 0.08111728727817535 = 0.014394102618098259 + 0.01 * 6.672318458557129
Epoch 560, val loss: 1.027215838432312
Epoch 570, training loss: 0.07997357845306396 = 0.013565260916948318 + 0.01 * 6.64083194732666
Epoch 570, val loss: 1.037789225578308
Epoch 580, training loss: 0.07911934703588486 = 0.01281007844954729 + 0.01 * 6.630926609039307
Epoch 580, val loss: 1.0480279922485352
Epoch 590, training loss: 0.0786558985710144 = 0.012119579128921032 + 0.01 * 6.653632640838623
Epoch 590, val loss: 1.0580905675888062
Epoch 600, training loss: 0.07783441245555878 = 0.011486704461276531 + 0.01 * 6.63477087020874
Epoch 600, val loss: 1.0678240060806274
Epoch 610, training loss: 0.07698243856430054 = 0.010905212722718716 + 0.01 * 6.607722759246826
Epoch 610, val loss: 1.0773158073425293
Epoch 620, training loss: 0.07636432349681854 = 0.010369653813540936 + 0.01 * 6.5994672775268555
Epoch 620, val loss: 1.0865509510040283
Epoch 630, training loss: 0.07601255923509598 = 0.009875386953353882 + 0.01 * 6.613717079162598
Epoch 630, val loss: 1.0955830812454224
Epoch 640, training loss: 0.07539555430412292 = 0.009418854489922523 + 0.01 * 6.597670555114746
Epoch 640, val loss: 1.104289174079895
Epoch 650, training loss: 0.07511090487241745 = 0.008995804004371166 + 0.01 * 6.611510276794434
Epoch 650, val loss: 1.1128641366958618
Epoch 660, training loss: 0.07445408403873444 = 0.008603394031524658 + 0.01 * 6.585069179534912
Epoch 660, val loss: 1.1211326122283936
Epoch 670, training loss: 0.07398264110088348 = 0.008238499984145164 + 0.01 * 6.574414253234863
Epoch 670, val loss: 1.129300832748413
Epoch 680, training loss: 0.07361240684986115 = 0.007898438721895218 + 0.01 * 6.571396350860596
Epoch 680, val loss: 1.1372214555740356
Epoch 690, training loss: 0.0731661394238472 = 0.007581518031656742 + 0.01 * 6.558462619781494
Epoch 690, val loss: 1.144917368888855
Epoch 700, training loss: 0.07292720675468445 = 0.007284920662641525 + 0.01 * 6.564228534698486
Epoch 700, val loss: 1.1524680852890015
Epoch 710, training loss: 0.07266106456518173 = 0.007007190957665443 + 0.01 * 6.565387725830078
Epoch 710, val loss: 1.159856915473938
Epoch 720, training loss: 0.07220932096242905 = 0.006746959872543812 + 0.01 * 6.546236038208008
Epoch 720, val loss: 1.1670877933502197
Epoch 730, training loss: 0.07203647494316101 = 0.006502380128949881 + 0.01 * 6.553409099578857
Epoch 730, val loss: 1.1741420030593872
Epoch 740, training loss: 0.0717802569270134 = 0.006272389553487301 + 0.01 * 6.550786972045898
Epoch 740, val loss: 1.1810333728790283
Epoch 750, training loss: 0.07137864828109741 = 0.006055812351405621 + 0.01 * 6.532283782958984
Epoch 750, val loss: 1.18779456615448
Epoch 760, training loss: 0.0711110457777977 = 0.00585190299898386 + 0.01 * 6.525914669036865
Epoch 760, val loss: 1.1943843364715576
Epoch 770, training loss: 0.07129804790019989 = 0.005659116897732019 + 0.01 * 6.563892841339111
Epoch 770, val loss: 1.2008707523345947
Epoch 780, training loss: 0.07069257646799088 = 0.005477319937199354 + 0.01 * 6.521526336669922
Epoch 780, val loss: 1.2072499990463257
Epoch 790, training loss: 0.07090606540441513 = 0.005305186379700899 + 0.01 * 6.560088157653809
Epoch 790, val loss: 1.2134753465652466
Epoch 800, training loss: 0.07021340727806091 = 0.00514233997091651 + 0.01 * 6.507107257843018
Epoch 800, val loss: 1.2195408344268799
Epoch 810, training loss: 0.07024291902780533 = 0.0049878982827067375 + 0.01 * 6.525502681732178
Epoch 810, val loss: 1.2255584001541138
Epoch 820, training loss: 0.06976305693387985 = 0.0048413570038974285 + 0.01 * 6.492170333862305
Epoch 820, val loss: 1.2313834428787231
Epoch 830, training loss: 0.06974466890096664 = 0.004702195059508085 + 0.01 * 6.504247188568115
Epoch 830, val loss: 1.2371622323989868
Epoch 840, training loss: 0.06954588741064072 = 0.004570015240460634 + 0.01 * 6.497587203979492
Epoch 840, val loss: 1.2428102493286133
Epoch 850, training loss: 0.06925608217716217 = 0.0044441381469368935 + 0.01 * 6.481194496154785
Epoch 850, val loss: 1.2483444213867188
Epoch 860, training loss: 0.06924135237932205 = 0.004324181936681271 + 0.01 * 6.4917168617248535
Epoch 860, val loss: 1.253812313079834
Epoch 870, training loss: 0.0689828097820282 = 0.004210020415484905 + 0.01 * 6.4772796630859375
Epoch 870, val loss: 1.259135127067566
Epoch 880, training loss: 0.06899309903383255 = 0.004100894555449486 + 0.01 * 6.489220142364502
Epoch 880, val loss: 1.2644193172454834
Epoch 890, training loss: 0.06877146661281586 = 0.003996807616204023 + 0.01 * 6.477466106414795
Epoch 890, val loss: 1.2695143222808838
Epoch 900, training loss: 0.06870509684085846 = 0.003897342598065734 + 0.01 * 6.480775833129883
Epoch 900, val loss: 1.2746398448944092
Epoch 910, training loss: 0.06860774010419846 = 0.003802523948252201 + 0.01 * 6.4805216789245605
Epoch 910, val loss: 1.2795696258544922
Epoch 920, training loss: 0.06823427975177765 = 0.003711470402777195 + 0.01 * 6.452281475067139
Epoch 920, val loss: 1.2844312191009521
Epoch 930, training loss: 0.0685216411948204 = 0.0036244329530745745 + 0.01 * 6.489721298217773
Epoch 930, val loss: 1.2892074584960938
Epoch 940, training loss: 0.06812124699354172 = 0.003541113343089819 + 0.01 * 6.458014011383057
Epoch 940, val loss: 1.2938003540039062
Epoch 950, training loss: 0.06805186718702316 = 0.0034611807204782963 + 0.01 * 6.459068298339844
Epoch 950, val loss: 1.2983564138412476
Epoch 960, training loss: 0.06779613345861435 = 0.0033846688456833363 + 0.01 * 6.441146373748779
Epoch 960, val loss: 1.3028843402862549
Epoch 970, training loss: 0.0677962452173233 = 0.0033110887743532658 + 0.01 * 6.448515892028809
Epoch 970, val loss: 1.3072922229766846
Epoch 980, training loss: 0.0676594153046608 = 0.0032404069788753986 + 0.01 * 6.441900730133057
Epoch 980, val loss: 1.3115787506103516
Epoch 990, training loss: 0.0677429735660553 = 0.0031724870204925537 + 0.01 * 6.4570488929748535
Epoch 990, val loss: 1.315927267074585
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.8450
Flip ASR: 0.8133/225 nodes
The final ASR:0.73555, 0.14706, Accuracy:0.82963, 0.00302
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11606])
remove edge: torch.Size([2, 9490])
updated graph: torch.Size([2, 10540])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98032, 0.00627, Accuracy:0.83457, 0.00175
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.035926342010498 = 1.9521872997283936 + 0.01 * 8.373907089233398
Epoch 0, val loss: 1.9499645233154297
Epoch 10, training loss: 2.0261542797088623 = 1.942415475845337 + 0.01 * 8.373871803283691
Epoch 10, val loss: 1.9413071870803833
Epoch 20, training loss: 2.0144832134246826 = 1.930746078491211 + 0.01 * 8.373723983764648
Epoch 20, val loss: 1.930607795715332
Epoch 30, training loss: 1.9983655214309692 = 1.914631724357605 + 0.01 * 8.373376846313477
Epoch 30, val loss: 1.9156001806259155
Epoch 40, training loss: 1.9745566844940186 = 1.8908342123031616 + 0.01 * 8.372252464294434
Epoch 40, val loss: 1.893430233001709
Epoch 50, training loss: 1.939751386642456 = 1.8560923337936401 + 0.01 * 8.365910530090332
Epoch 50, val loss: 1.8618249893188477
Epoch 60, training loss: 1.8947923183441162 = 1.8115370273590088 + 0.01 * 8.32553482055664
Epoch 60, val loss: 1.8235135078430176
Epoch 70, training loss: 1.846034288406372 = 1.7654763460159302 + 0.01 * 8.055793762207031
Epoch 70, val loss: 1.7852054834365845
Epoch 80, training loss: 1.7917293310165405 = 1.713868498802185 + 0.01 * 7.786086082458496
Epoch 80, val loss: 1.7373493909835815
Epoch 90, training loss: 1.719646692276001 = 1.6436166763305664 + 0.01 * 7.602999210357666
Epoch 90, val loss: 1.6727888584136963
Epoch 100, training loss: 1.6258704662322998 = 1.5514380931854248 + 0.01 * 7.44323205947876
Epoch 100, val loss: 1.5947312116622925
Epoch 110, training loss: 1.5167690515518188 = 1.443930983543396 + 0.01 * 7.283811092376709
Epoch 110, val loss: 1.5052889585494995
Epoch 120, training loss: 1.4064967632293701 = 1.333864450454712 + 0.01 * 7.263230323791504
Epoch 120, val loss: 1.4154870510101318
Epoch 130, training loss: 1.3020563125610352 = 1.229784369468689 + 0.01 * 7.22719669342041
Epoch 130, val loss: 1.3342664241790771
Epoch 140, training loss: 1.2056998014450073 = 1.1337181329727173 + 0.01 * 7.198169231414795
Epoch 140, val loss: 1.2623941898345947
Epoch 150, training loss: 1.115147590637207 = 1.04354727268219 + 0.01 * 7.160032749176025
Epoch 150, val loss: 1.1958194971084595
Epoch 160, training loss: 1.0283820629119873 = 0.9572258591651917 + 0.01 * 7.115625858306885
Epoch 160, val loss: 1.1323825120925903
Epoch 170, training loss: 0.946092426776886 = 0.8753045201301575 + 0.01 * 7.07879114151001
Epoch 170, val loss: 1.0719962120056152
Epoch 180, training loss: 0.8699244856834412 = 0.7993753552436829 + 0.01 * 7.054915428161621
Epoch 180, val loss: 1.016362190246582
Epoch 190, training loss: 0.8004488348960876 = 0.730043888092041 + 0.01 * 7.040494441986084
Epoch 190, val loss: 0.9667074680328369
Epoch 200, training loss: 0.7370597720146179 = 0.6667421460151672 + 0.01 * 7.031762599945068
Epoch 200, val loss: 0.9226462244987488
Epoch 210, training loss: 0.6786275506019592 = 0.6083704233169556 + 0.01 * 7.025710582733154
Epoch 210, val loss: 0.8839182257652283
Epoch 220, training loss: 0.6243090033531189 = 0.5541139245033264 + 0.01 * 7.019509315490723
Epoch 220, val loss: 0.8499899506568909
Epoch 230, training loss: 0.5738560557365417 = 0.50373774766922 + 0.01 * 7.011828422546387
Epoch 230, val loss: 0.8200471997261047
Epoch 240, training loss: 0.5271012187004089 = 0.45707768201828003 + 0.01 * 7.0023512840271
Epoch 240, val loss: 0.7937750816345215
Epoch 250, training loss: 0.4835822582244873 = 0.41368037462234497 + 0.01 * 6.990187168121338
Epoch 250, val loss: 0.7708230018615723
Epoch 260, training loss: 0.44256556034088135 = 0.3727896511554718 + 0.01 * 6.977591037750244
Epoch 260, val loss: 0.7508653402328491
Epoch 270, training loss: 0.4032444357872009 = 0.3336363732814789 + 0.01 * 6.960805892944336
Epoch 270, val loss: 0.7333852052688599
Epoch 280, training loss: 0.36506423354148865 = 0.295593798160553 + 0.01 * 6.947043418884277
Epoch 280, val loss: 0.7179952263832092
Epoch 290, training loss: 0.3278661370277405 = 0.2584376633167267 + 0.01 * 6.942849159240723
Epoch 290, val loss: 0.7049883604049683
Epoch 300, training loss: 0.2919294834136963 = 0.22261478006839752 + 0.01 * 6.931471824645996
Epoch 300, val loss: 0.6947957873344421
Epoch 310, training loss: 0.2586685121059418 = 0.1894357055425644 + 0.01 * 6.923279762268066
Epoch 310, val loss: 0.6883371472358704
Epoch 320, training loss: 0.22963938117027283 = 0.16046258807182312 + 0.01 * 6.917678356170654
Epoch 320, val loss: 0.6862282156944275
Epoch 330, training loss: 0.2055804431438446 = 0.1364380270242691 + 0.01 * 6.914241790771484
Epoch 330, val loss: 0.6882852911949158
Epoch 340, training loss: 0.1861128807067871 = 0.1170073002576828 + 0.01 * 6.910558223724365
Epoch 340, val loss: 0.693535327911377
Epoch 350, training loss: 0.1703319251537323 = 0.10126232355833054 + 0.01 * 6.906959533691406
Epoch 350, val loss: 0.7011827826499939
Epoch 360, training loss: 0.15739905834197998 = 0.088330939412117 + 0.01 * 6.906811237335205
Epoch 360, val loss: 0.7104148864746094
Epoch 370, training loss: 0.14655429124832153 = 0.07754954695701599 + 0.01 * 6.900473594665527
Epoch 370, val loss: 0.7208127975463867
Epoch 380, training loss: 0.13740785419940948 = 0.06844604760408401 + 0.01 * 6.896181106567383
Epoch 380, val loss: 0.732021689414978
Epoch 390, training loss: 0.1296076774597168 = 0.060686878859996796 + 0.01 * 6.8920793533325195
Epoch 390, val loss: 0.7438079714775085
Epoch 400, training loss: 0.12293599545955658 = 0.054029542952775955 + 0.01 * 6.890645980834961
Epoch 400, val loss: 0.7560353875160217
Epoch 410, training loss: 0.11715300381183624 = 0.04829011857509613 + 0.01 * 6.886288642883301
Epoch 410, val loss: 0.7685484290122986
Epoch 420, training loss: 0.11212319135665894 = 0.043326880782842636 + 0.01 * 6.8796305656433105
Epoch 420, val loss: 0.7813202738761902
Epoch 430, training loss: 0.10776351392269135 = 0.039022620767354965 + 0.01 * 6.874090194702148
Epoch 430, val loss: 0.7941840887069702
Epoch 440, training loss: 0.10398677736520767 = 0.03527941554784775 + 0.01 * 6.870736598968506
Epoch 440, val loss: 0.8071295619010925
Epoch 450, training loss: 0.10065211355686188 = 0.03201555460691452 + 0.01 * 6.863656044006348
Epoch 450, val loss: 0.8200843930244446
Epoch 460, training loss: 0.09782792627811432 = 0.029160449281334877 + 0.01 * 6.866748332977295
Epoch 460, val loss: 0.832865834236145
Epoch 470, training loss: 0.09521819651126862 = 0.026656189933419228 + 0.01 * 6.856200695037842
Epoch 470, val loss: 0.8455026149749756
Epoch 480, training loss: 0.09293404966592789 = 0.024451961740851402 + 0.01 * 6.848208427429199
Epoch 480, val loss: 0.8579620122909546
Epoch 490, training loss: 0.09095637500286102 = 0.022504061460494995 + 0.01 * 6.845231533050537
Epoch 490, val loss: 0.8701056838035583
Epoch 500, training loss: 0.08913978934288025 = 0.020775500684976578 + 0.01 * 6.836429119110107
Epoch 500, val loss: 0.8820542097091675
Epoch 510, training loss: 0.08754092454910278 = 0.019232768565416336 + 0.01 * 6.830815315246582
Epoch 510, val loss: 0.8936294317245483
Epoch 520, training loss: 0.08610468357801437 = 0.017846502363681793 + 0.01 * 6.8258185386657715
Epoch 520, val loss: 0.905161440372467
Epoch 530, training loss: 0.08473819494247437 = 0.016601774841547012 + 0.01 * 6.813642501831055
Epoch 530, val loss: 0.9160717725753784
Epoch 540, training loss: 0.08364985883235931 = 0.015480270609259605 + 0.01 * 6.816958427429199
Epoch 540, val loss: 0.9269884824752808
Epoch 550, training loss: 0.08257032930850983 = 0.014468959532678127 + 0.01 * 6.8101372718811035
Epoch 550, val loss: 0.9374591112136841
Epoch 560, training loss: 0.08149144798517227 = 0.013555089011788368 + 0.01 * 6.793636322021484
Epoch 560, val loss: 0.9476829171180725
Epoch 570, training loss: 0.08056754618883133 = 0.012725935317575932 + 0.01 * 6.78416109085083
Epoch 570, val loss: 0.9575604200363159
Epoch 580, training loss: 0.08001817762851715 = 0.011971031315624714 + 0.01 * 6.804714202880859
Epoch 580, val loss: 0.96721351146698
Epoch 590, training loss: 0.079025499522686 = 0.011282890103757381 + 0.01 * 6.774260997772217
Epoch 590, val loss: 0.9765796661376953
Epoch 600, training loss: 0.07828442752361298 = 0.010653666220605373 + 0.01 * 6.763076305389404
Epoch 600, val loss: 0.9856402277946472
Epoch 610, training loss: 0.07762069255113602 = 0.010076882317662239 + 0.01 * 6.75438117980957
Epoch 610, val loss: 0.994514524936676
Epoch 620, training loss: 0.07711074501276016 = 0.0095477644354105 + 0.01 * 6.756298542022705
Epoch 620, val loss: 1.0030092000961304
Epoch 630, training loss: 0.07657243311405182 = 0.009061522781848907 + 0.01 * 6.751091480255127
Epoch 630, val loss: 1.0114188194274902
Epoch 640, training loss: 0.07594258338212967 = 0.008613111451268196 + 0.01 * 6.732947826385498
Epoch 640, val loss: 1.019531011581421
Epoch 650, training loss: 0.07548335939645767 = 0.008199521340429783 + 0.01 * 6.728384017944336
Epoch 650, val loss: 1.0273351669311523
Epoch 660, training loss: 0.07501931488513947 = 0.007817870937287807 + 0.01 * 6.720144271850586
Epoch 660, val loss: 1.0350815057754517
Epoch 670, training loss: 0.07464256882667542 = 0.0074642859399318695 + 0.01 * 6.717828273773193
Epoch 670, val loss: 1.0424431562423706
Epoch 680, training loss: 0.07430281490087509 = 0.007136020343750715 + 0.01 * 6.716679573059082
Epoch 680, val loss: 1.049668550491333
Epoch 690, training loss: 0.07387730479240417 = 0.006830781698226929 + 0.01 * 6.704652309417725
Epoch 690, val loss: 1.0566738843917847
Epoch 700, training loss: 0.07348740100860596 = 0.0065466053783893585 + 0.01 * 6.694079399108887
Epoch 700, val loss: 1.0634781122207642
Epoch 710, training loss: 0.0735325813293457 = 0.006281519774347544 + 0.01 * 6.725106716156006
Epoch 710, val loss: 1.0700498819351196
Epoch 720, training loss: 0.07296416908502579 = 0.006034440826624632 + 0.01 * 6.692972660064697
Epoch 720, val loss: 1.0765575170516968
Epoch 730, training loss: 0.07259419560432434 = 0.005803158972412348 + 0.01 * 6.679103851318359
Epoch 730, val loss: 1.0827741622924805
Epoch 740, training loss: 0.07239022850990295 = 0.005586078856140375 + 0.01 * 6.68041467666626
Epoch 740, val loss: 1.088895559310913
Epoch 750, training loss: 0.07215025275945663 = 0.005382625851780176 + 0.01 * 6.67676305770874
Epoch 750, val loss: 1.094773769378662
Epoch 760, training loss: 0.07181502878665924 = 0.005191324278712273 + 0.01 * 6.662370681762695
Epoch 760, val loss: 1.1006536483764648
Epoch 770, training loss: 0.07161428779363632 = 0.00501104723662138 + 0.01 * 6.660324573516846
Epoch 770, val loss: 1.1061527729034424
Epoch 780, training loss: 0.07143028825521469 = 0.00484077213332057 + 0.01 * 6.658951759338379
Epoch 780, val loss: 1.1117357015609741
Epoch 790, training loss: 0.07124253362417221 = 0.0046796659007668495 + 0.01 * 6.656286716461182
Epoch 790, val loss: 1.1171083450317383
Epoch 800, training loss: 0.0710267573595047 = 0.004527006298303604 + 0.01 * 6.649974822998047
Epoch 800, val loss: 1.1223317384719849
Epoch 810, training loss: 0.07075312733650208 = 0.004382316023111343 + 0.01 * 6.637080669403076
Epoch 810, val loss: 1.1275545358657837
Epoch 820, training loss: 0.0705958902835846 = 0.00424489751458168 + 0.01 * 6.6350998878479
Epoch 820, val loss: 1.132533073425293
Epoch 830, training loss: 0.07055686414241791 = 0.004114274866878986 + 0.01 * 6.644258499145508
Epoch 830, val loss: 1.1374619007110596
Epoch 840, training loss: 0.07030197978019714 = 0.003990530036389828 + 0.01 * 6.631145000457764
Epoch 840, val loss: 1.1423100233078003
Epoch 850, training loss: 0.0701574981212616 = 0.0038727102801203728 + 0.01 * 6.62847900390625
Epoch 850, val loss: 1.1470367908477783
Epoch 860, training loss: 0.07008033990859985 = 0.0037607522681355476 + 0.01 * 6.631958961486816
Epoch 860, val loss: 1.1515990495681763
Epoch 870, training loss: 0.06978247314691544 = 0.0036542234010994434 + 0.01 * 6.6128249168396
Epoch 870, val loss: 1.1561287641525269
Epoch 880, training loss: 0.06964053958654404 = 0.003552823793143034 + 0.01 * 6.608771800994873
Epoch 880, val loss: 1.1605112552642822
Epoch 890, training loss: 0.06975459307432175 = 0.003456410951912403 + 0.01 * 6.629817962646484
Epoch 890, val loss: 1.1648720502853394
Epoch 900, training loss: 0.06934807449579239 = 0.003364461474120617 + 0.01 * 6.598361968994141
Epoch 900, val loss: 1.1690436601638794
Epoch 910, training loss: 0.06933167576789856 = 0.0032769565004855394 + 0.01 * 6.605471611022949
Epoch 910, val loss: 1.1731370687484741
Epoch 920, training loss: 0.06928219646215439 = 0.0031936103478074074 + 0.01 * 6.608859062194824
Epoch 920, val loss: 1.1771783828735352
Epoch 930, training loss: 0.06909961998462677 = 0.0031138062477111816 + 0.01 * 6.598581790924072
Epoch 930, val loss: 1.1811071634292603
Epoch 940, training loss: 0.0690239742398262 = 0.003037763759493828 + 0.01 * 6.598621368408203
Epoch 940, val loss: 1.1849321126937866
Epoch 950, training loss: 0.06901232153177261 = 0.0029651641380041838 + 0.01 * 6.604715824127197
Epoch 950, val loss: 1.1888052225112915
Epoch 960, training loss: 0.06867820769548416 = 0.0028956476598978043 + 0.01 * 6.578256130218506
Epoch 960, val loss: 1.1924035549163818
Epoch 970, training loss: 0.06861899048089981 = 0.002829042263329029 + 0.01 * 6.5789947509765625
Epoch 970, val loss: 1.1960084438323975
Epoch 980, training loss: 0.06856494396924973 = 0.0027654976584017277 + 0.01 * 6.579944610595703
Epoch 980, val loss: 1.1995463371276855
Epoch 990, training loss: 0.06865311414003372 = 0.002704397076740861 + 0.01 * 6.594871520996094
Epoch 990, val loss: 1.2029380798339844
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.7970
Flip ASR: 0.7556/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0454540252685547 = 1.9617151021957397 + 0.01 * 8.373900413513184
Epoch 0, val loss: 1.968766450881958
Epoch 10, training loss: 2.0357329845428467 = 1.9519944190979004 + 0.01 * 8.373855590820312
Epoch 10, val loss: 1.9589228630065918
Epoch 20, training loss: 2.0239791870117188 = 1.9402422904968262 + 0.01 * 8.373693466186523
Epoch 20, val loss: 1.9463995695114136
Epoch 30, training loss: 2.007721424102783 = 1.9239888191223145 + 0.01 * 8.373250961303711
Epoch 30, val loss: 1.9285409450531006
Epoch 40, training loss: 1.9835832118988037 = 1.8998692035675049 + 0.01 * 8.371399879455566
Epoch 40, val loss: 1.901803731918335
Epoch 50, training loss: 1.9480434656143188 = 1.864439845085144 + 0.01 * 8.360367774963379
Epoch 50, val loss: 1.863265037536621
Epoch 60, training loss: 1.9018001556396484 = 1.8186594247817993 + 0.01 * 8.314068794250488
Epoch 60, val loss: 1.8162851333618164
Epoch 70, training loss: 1.8550388813018799 = 1.7736374139785767 + 0.01 * 8.140145301818848
Epoch 70, val loss: 1.7748124599456787
Epoch 80, training loss: 1.8092002868652344 = 1.7300041913986206 + 0.01 * 7.919607162475586
Epoch 80, val loss: 1.7372114658355713
Epoch 90, training loss: 1.7467941045761108 = 1.6712363958358765 + 0.01 * 7.555776119232178
Epoch 90, val loss: 1.6871596574783325
Epoch 100, training loss: 1.6661765575408936 = 1.592758297920227 + 0.01 * 7.341823577880859
Epoch 100, val loss: 1.6202406883239746
Epoch 110, training loss: 1.5639029741287231 = 1.4913303852081299 + 0.01 * 7.257260799407959
Epoch 110, val loss: 1.5356525182724
Epoch 120, training loss: 1.4451816082000732 = 1.3732495307922363 + 0.01 * 7.193202018737793
Epoch 120, val loss: 1.437735915184021
Epoch 130, training loss: 1.322168231010437 = 1.2506215572357178 + 0.01 * 7.154670715332031
Epoch 130, val loss: 1.3373360633850098
Epoch 140, training loss: 1.204850196838379 = 1.1336535215377808 + 0.01 * 7.119670391082764
Epoch 140, val loss: 1.2414661645889282
Epoch 150, training loss: 1.0988080501556396 = 1.027916669845581 + 0.01 * 7.0891432762146
Epoch 150, val loss: 1.1562762260437012
Epoch 160, training loss: 1.0044859647750854 = 0.9339028596878052 + 0.01 * 7.058309555053711
Epoch 160, val loss: 1.0825741291046143
Epoch 170, training loss: 0.9197314977645874 = 0.8494537472724915 + 0.01 * 7.02777624130249
Epoch 170, val loss: 1.0184494256973267
Epoch 180, training loss: 0.8430639505386353 = 0.7729799151420593 + 0.01 * 7.0084052085876465
Epoch 180, val loss: 0.9624146819114685
Epoch 190, training loss: 0.7736499309539795 = 0.7036859393119812 + 0.01 * 6.996396541595459
Epoch 190, val loss: 0.9131127595901489
Epoch 200, training loss: 0.7106765508651733 = 0.6407544016838074 + 0.01 * 6.992211818695068
Epoch 200, val loss: 0.8699846267700195
Epoch 210, training loss: 0.6527096629142761 = 0.5828215479850769 + 0.01 * 6.988809108734131
Epoch 210, val loss: 0.8317996263504028
Epoch 220, training loss: 0.5980648398399353 = 0.5282182097434998 + 0.01 * 6.984663963317871
Epoch 220, val loss: 0.7972531318664551
Epoch 230, training loss: 0.5454502701759338 = 0.4756505489349365 + 0.01 * 6.9799699783325195
Epoch 230, val loss: 0.7654369473457336
Epoch 240, training loss: 0.49412110447883606 = 0.4243704676628113 + 0.01 * 6.975064277648926
Epoch 240, val loss: 0.7360019087791443
Epoch 250, training loss: 0.443805992603302 = 0.374112069606781 + 0.01 * 6.9693922996521
Epoch 250, val loss: 0.7093039751052856
Epoch 260, training loss: 0.3949408531188965 = 0.3253214359283447 + 0.01 * 6.961942195892334
Epoch 260, val loss: 0.6856106519699097
Epoch 270, training loss: 0.348804235458374 = 0.2792261838912964 + 0.01 * 6.95780611038208
Epoch 270, val loss: 0.6653822064399719
Epoch 280, training loss: 0.3068484365940094 = 0.23737682402133942 + 0.01 * 6.94716215133667
Epoch 280, val loss: 0.6493754982948303
Epoch 290, training loss: 0.2702188193798065 = 0.2008298635482788 + 0.01 * 6.9388957023620605
Epoch 290, val loss: 0.6381533741950989
Epoch 300, training loss: 0.23960472643375397 = 0.17019256949424744 + 0.01 * 6.941215991973877
Epoch 300, val loss: 0.6321828365325928
Epoch 310, training loss: 0.21427571773529053 = 0.14501430094242096 + 0.01 * 6.926141738891602
Epoch 310, val loss: 0.630736231803894
Epoch 320, training loss: 0.1936851441860199 = 0.1245628073811531 + 0.01 * 6.912234306335449
Epoch 320, val loss: 0.633243203163147
Epoch 330, training loss: 0.17692366242408752 = 0.10786665976047516 + 0.01 * 6.905700206756592
Epoch 330, val loss: 0.6387335062026978
Epoch 340, training loss: 0.16328924894332886 = 0.09404613077640533 + 0.01 * 6.924311637878418
Epoch 340, val loss: 0.6466105580329895
Epoch 350, training loss: 0.1513763964176178 = 0.08241687715053558 + 0.01 * 6.895951747894287
Epoch 350, val loss: 0.6561667323112488
Epoch 360, training loss: 0.14133089780807495 = 0.07247177511453629 + 0.01 * 6.88591194152832
Epoch 360, val loss: 0.6666582226753235
Epoch 370, training loss: 0.13256359100341797 = 0.06381604075431824 + 0.01 * 6.874755382537842
Epoch 370, val loss: 0.6780955195426941
Epoch 380, training loss: 0.12520918250083923 = 0.05630442500114441 + 0.01 * 6.890475273132324
Epoch 380, val loss: 0.6899839043617249
Epoch 390, training loss: 0.11850649118423462 = 0.0498817153275013 + 0.01 * 6.862477779388428
Epoch 390, val loss: 0.7021965384483337
Epoch 400, training loss: 0.11304637044668198 = 0.044448621571063995 + 0.01 * 6.859775066375732
Epoch 400, val loss: 0.714725136756897
Epoch 410, training loss: 0.10838867723941803 = 0.039840277284383774 + 0.01 * 6.854840278625488
Epoch 410, val loss: 0.7273914813995361
Epoch 420, training loss: 0.10433752834796906 = 0.03590772673487663 + 0.01 * 6.842979907989502
Epoch 420, val loss: 0.7402522563934326
Epoch 430, training loss: 0.10092507302761078 = 0.03252047300338745 + 0.01 * 6.840460300445557
Epoch 430, val loss: 0.75307697057724
Epoch 440, training loss: 0.09799713641405106 = 0.029591679573059082 + 0.01 * 6.840545654296875
Epoch 440, val loss: 0.7659900784492493
Epoch 450, training loss: 0.09533284604549408 = 0.027041763067245483 + 0.01 * 6.829108238220215
Epoch 450, val loss: 0.7785627841949463
Epoch 460, training loss: 0.09295154362916946 = 0.024807341396808624 + 0.01 * 6.814420223236084
Epoch 460, val loss: 0.7909024953842163
Epoch 470, training loss: 0.09094665199518204 = 0.022838076576590538 + 0.01 * 6.810857772827148
Epoch 470, val loss: 0.8031175136566162
Epoch 480, training loss: 0.08922955393791199 = 0.02109585329890251 + 0.01 * 6.813370704650879
Epoch 480, val loss: 0.8149463534355164
Epoch 490, training loss: 0.08760201185941696 = 0.019549064338207245 + 0.01 * 6.805294990539551
Epoch 490, val loss: 0.8265145421028137
Epoch 500, training loss: 0.08608190715312958 = 0.01816944032907486 + 0.01 * 6.7912468910217285
Epoch 500, val loss: 0.8377206325531006
Epoch 510, training loss: 0.08489857614040375 = 0.01693415269255638 + 0.01 * 6.796442031860352
Epoch 510, val loss: 0.848645031452179
Epoch 520, training loss: 0.08369111269712448 = 0.01582406461238861 + 0.01 * 6.786705017089844
Epoch 520, val loss: 0.8592844009399414
Epoch 530, training loss: 0.08258561789989471 = 0.014822839759290218 + 0.01 * 6.776277542114258
Epoch 530, val loss: 0.8695520162582397
Epoch 540, training loss: 0.08157376945018768 = 0.01391701027750969 + 0.01 * 6.7656755447387695
Epoch 540, val loss: 0.8795979022979736
Epoch 550, training loss: 0.08070801943540573 = 0.01309522520750761 + 0.01 * 6.761279582977295
Epoch 550, val loss: 0.8892886638641357
Epoch 560, training loss: 0.07990981638431549 = 0.012347938492894173 + 0.01 * 6.75618839263916
Epoch 560, val loss: 0.8987621665000916
Epoch 570, training loss: 0.07925017178058624 = 0.01166569720953703 + 0.01 * 6.758448123931885
Epoch 570, val loss: 0.9079939723014832
Epoch 580, training loss: 0.07865433394908905 = 0.011041850782930851 + 0.01 * 6.761248588562012
Epoch 580, val loss: 0.9168543815612793
Epoch 590, training loss: 0.0778016746044159 = 0.01047036424279213 + 0.01 * 6.7331318855285645
Epoch 590, val loss: 0.9255865216255188
Epoch 600, training loss: 0.07738790661096573 = 0.009944764897227287 + 0.01 * 6.744314193725586
Epoch 600, val loss: 0.9340673089027405
Epoch 610, training loss: 0.07673315703868866 = 0.009461557492613792 + 0.01 * 6.7271599769592285
Epoch 610, val loss: 0.9422675371170044
Epoch 620, training loss: 0.07617917656898499 = 0.00901499018073082 + 0.01 * 6.716418266296387
Epoch 620, val loss: 0.950238049030304
Epoch 630, training loss: 0.07588446140289307 = 0.008601982146501541 + 0.01 * 6.72824764251709
Epoch 630, val loss: 0.9580249786376953
Epoch 640, training loss: 0.07534676045179367 = 0.008219324983656406 + 0.01 * 6.712743759155273
Epoch 640, val loss: 0.9655269980430603
Epoch 650, training loss: 0.0748257115483284 = 0.007864370010793209 + 0.01 * 6.696134567260742
Epoch 650, val loss: 0.9729074239730835
Epoch 660, training loss: 0.07432630658149719 = 0.0075340489856898785 + 0.01 * 6.679225921630859
Epoch 660, val loss: 0.9801124930381775
Epoch 670, training loss: 0.07421371340751648 = 0.007226633373647928 + 0.01 * 6.6987080574035645
Epoch 670, val loss: 0.9869976043701172
Epoch 680, training loss: 0.07372323423624039 = 0.006939432118088007 + 0.01 * 6.678380012512207
Epoch 680, val loss: 0.9938737750053406
Epoch 690, training loss: 0.07335031032562256 = 0.006670631002634764 + 0.01 * 6.667968273162842
Epoch 690, val loss: 1.000512957572937
Epoch 700, training loss: 0.07312588393688202 = 0.006418738514184952 + 0.01 * 6.670714378356934
Epoch 700, val loss: 1.0069234371185303
Epoch 710, training loss: 0.07318218797445297 = 0.006182893179357052 + 0.01 * 6.699929714202881
Epoch 710, val loss: 1.0131759643554688
Epoch 720, training loss: 0.07253094017505646 = 0.005961616989225149 + 0.01 * 6.6569318771362305
Epoch 720, val loss: 1.019380807876587
Epoch 730, training loss: 0.07238829135894775 = 0.005753177218139172 + 0.01 * 6.663511753082275
Epoch 730, val loss: 1.025346279144287
Epoch 740, training loss: 0.07189697027206421 = 0.005557055119425058 + 0.01 * 6.633991718292236
Epoch 740, val loss: 1.0311857461929321
Epoch 750, training loss: 0.07166410982608795 = 0.005371687933802605 + 0.01 * 6.629242420196533
Epoch 750, val loss: 1.0368729829788208
Epoch 760, training loss: 0.07158218324184418 = 0.005197149235755205 + 0.01 * 6.638503074645996
Epoch 760, val loss: 1.0424654483795166
Epoch 770, training loss: 0.07115647196769714 = 0.0050319526344537735 + 0.01 * 6.612452507019043
Epoch 770, val loss: 1.0479592084884644
Epoch 780, training loss: 0.07121414691209793 = 0.004875914193689823 + 0.01 * 6.633823871612549
Epoch 780, val loss: 1.0533262491226196
Epoch 790, training loss: 0.0708543211221695 = 0.0047279405407607555 + 0.01 * 6.612638473510742
Epoch 790, val loss: 1.0584008693695068
Epoch 800, training loss: 0.07074917107820511 = 0.004587606061249971 + 0.01 * 6.616156578063965
Epoch 800, val loss: 1.0635451078414917
Epoch 810, training loss: 0.0707508996129036 = 0.004454680252820253 + 0.01 * 6.629622459411621
Epoch 810, val loss: 1.0685629844665527
Epoch 820, training loss: 0.0702829509973526 = 0.004327960778027773 + 0.01 * 6.595499515533447
Epoch 820, val loss: 1.0734161138534546
Epoch 830, training loss: 0.0700531005859375 = 0.004207898396998644 + 0.01 * 6.58452033996582
Epoch 830, val loss: 1.078216552734375
Epoch 840, training loss: 0.0701591968536377 = 0.004093329422175884 + 0.01 * 6.606586933135986
Epoch 840, val loss: 1.0828899145126343
Epoch 850, training loss: 0.06981656700372696 = 0.003984501119703054 + 0.01 * 6.5832061767578125
Epoch 850, val loss: 1.0874241590499878
Epoch 860, training loss: 0.06969959288835526 = 0.003880698699504137 + 0.01 * 6.5818891525268555
Epoch 860, val loss: 1.0918940305709839
Epoch 870, training loss: 0.06954549252986908 = 0.0037815456744283438 + 0.01 * 6.576395511627197
Epoch 870, val loss: 1.0963780879974365
Epoch 880, training loss: 0.06932321190834045 = 0.003686718875542283 + 0.01 * 6.563649654388428
Epoch 880, val loss: 1.100605845451355
Epoch 890, training loss: 0.06938676536083221 = 0.003596374299377203 + 0.01 * 6.579039573669434
Epoch 890, val loss: 1.104927897453308
Epoch 900, training loss: 0.06911303848028183 = 0.0035098094958812 + 0.01 * 6.5603227615356445
Epoch 900, val loss: 1.1089454889297485
Epoch 910, training loss: 0.06896375864744186 = 0.003427032148465514 + 0.01 * 6.553673267364502
Epoch 910, val loss: 1.1130285263061523
Epoch 920, training loss: 0.06903795897960663 = 0.003347768448293209 + 0.01 * 6.569018840789795
Epoch 920, val loss: 1.1169992685317993
Epoch 930, training loss: 0.06874249875545502 = 0.003271830501034856 + 0.01 * 6.547066688537598
Epoch 930, val loss: 1.121001958847046
Epoch 940, training loss: 0.06862322241067886 = 0.0031990332063287497 + 0.01 * 6.542418956756592
Epoch 940, val loss: 1.1247755289077759
Epoch 950, training loss: 0.06868763267993927 = 0.0031292536295950413 + 0.01 * 6.555838584899902
Epoch 950, val loss: 1.1286282539367676
Epoch 960, training loss: 0.068569116294384 = 0.0030621651094406843 + 0.01 * 6.550695419311523
Epoch 960, val loss: 1.132318139076233
Epoch 970, training loss: 0.06851021200418472 = 0.002997793722897768 + 0.01 * 6.551242351531982
Epoch 970, val loss: 1.1358990669250488
Epoch 980, training loss: 0.06835287809371948 = 0.0029359159525483847 + 0.01 * 6.541696071624756
Epoch 980, val loss: 1.1394184827804565
Epoch 990, training loss: 0.06833517551422119 = 0.0028764214366674423 + 0.01 * 6.545875549316406
Epoch 990, val loss: 1.1429492235183716
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.6863
Flip ASR: 0.6311/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.050532579421997 = 1.9667937755584717 + 0.01 * 8.373869895935059
Epoch 0, val loss: 1.9742757081985474
Epoch 10, training loss: 2.0392868518829346 = 1.9555490016937256 + 0.01 * 8.373793601989746
Epoch 10, val loss: 1.96259605884552
Epoch 20, training loss: 2.0254786014556885 = 1.9417431354522705 + 0.01 * 8.373549461364746
Epoch 20, val loss: 1.948029637336731
Epoch 30, training loss: 2.0060057640075684 = 1.9222772121429443 + 0.01 * 8.372846603393555
Epoch 30, val loss: 1.9275132417678833
Epoch 40, training loss: 1.9770981073379517 = 1.8934022188186646 + 0.01 * 8.369585037231445
Epoch 40, val loss: 1.8973935842514038
Epoch 50, training loss: 1.9360644817352295 = 1.8525774478912354 + 0.01 * 8.348708152770996
Epoch 50, val loss: 1.8563427925109863
Epoch 60, training loss: 1.8882583379745483 = 1.8057552576065063 + 0.01 * 8.250306129455566
Epoch 60, val loss: 1.813078761100769
Epoch 70, training loss: 1.8465251922607422 = 1.7663283348083496 + 0.01 * 8.019685745239258
Epoch 70, val loss: 1.7793701887130737
Epoch 80, training loss: 1.7992175817489624 = 1.7215327024459839 + 0.01 * 7.768484592437744
Epoch 80, val loss: 1.7389118671417236
Epoch 90, training loss: 1.7345304489135742 = 1.6599100828170776 + 0.01 * 7.462037563323975
Epoch 90, val loss: 1.6856894493103027
Epoch 100, training loss: 1.6499016284942627 = 1.5776607990264893 + 0.01 * 7.22408390045166
Epoch 100, val loss: 1.6173937320709229
Epoch 110, training loss: 1.551896333694458 = 1.4808173179626465 + 0.01 * 7.107899188995361
Epoch 110, val loss: 1.537782073020935
Epoch 120, training loss: 1.4551399946212769 = 1.3846768140792847 + 0.01 * 7.046323299407959
Epoch 120, val loss: 1.4613903760910034
Epoch 130, training loss: 1.365565538406372 = 1.2954579591751099 + 0.01 * 7.010758876800537
Epoch 130, val loss: 1.3941822052001953
Epoch 140, training loss: 1.281395673751831 = 1.2114895582199097 + 0.01 * 6.990612030029297
Epoch 140, val loss: 1.3329805135726929
Epoch 150, training loss: 1.2001022100448608 = 1.1303319931030273 + 0.01 * 6.977026462554932
Epoch 150, val loss: 1.2740328311920166
Epoch 160, training loss: 1.1204333305358887 = 1.0507303476333618 + 0.01 * 6.970300197601318
Epoch 160, val loss: 1.2154308557510376
Epoch 170, training loss: 1.0418181419372559 = 0.9721400141716003 + 0.01 * 6.9678144454956055
Epoch 170, val loss: 1.1567810773849487
Epoch 180, training loss: 0.9642254710197449 = 0.8945491313934326 + 0.01 * 6.967634677886963
Epoch 180, val loss: 1.097406029701233
Epoch 190, training loss: 0.8874452114105225 = 0.8177585005760193 + 0.01 * 6.968672275543213
Epoch 190, val loss: 1.0374717712402344
Epoch 200, training loss: 0.8112934231758118 = 0.7415915131568909 + 0.01 * 6.97019100189209
Epoch 200, val loss: 0.9773041605949402
Epoch 210, training loss: 0.7366646528244019 = 0.6669466495513916 + 0.01 * 6.971803665161133
Epoch 210, val loss: 0.9183502793312073
Epoch 220, training loss: 0.6653562188148499 = 0.5956219434738159 + 0.01 * 6.973428249359131
Epoch 220, val loss: 0.8635848164558411
Epoch 230, training loss: 0.5987893342971802 = 0.5290413498878479 + 0.01 * 6.97479772567749
Epoch 230, val loss: 0.8148891925811768
Epoch 240, training loss: 0.5375982522964478 = 0.46784067153930664 + 0.01 * 6.975759983062744
Epoch 240, val loss: 0.7733781933784485
Epoch 250, training loss: 0.481965571641922 = 0.41220298409461975 + 0.01 * 6.976257801055908
Epoch 250, val loss: 0.7385653853416443
Epoch 260, training loss: 0.4318798780441284 = 0.3621165156364441 + 0.01 * 6.97633695602417
Epoch 260, val loss: 0.7096630930900574
Epoch 270, training loss: 0.38716647028923035 = 0.31740623712539673 + 0.01 * 6.976024150848389
Epoch 270, val loss: 0.6868364810943604
Epoch 280, training loss: 0.3472786247730255 = 0.27752798795700073 + 0.01 * 6.975064277648926
Epoch 280, val loss: 0.6700361967086792
Epoch 290, training loss: 0.31156525015830994 = 0.24182458221912384 + 0.01 * 6.974066734313965
Epoch 290, val loss: 0.6584528684616089
Epoch 300, training loss: 0.27955955266952515 = 0.20983745157718658 + 0.01 * 6.972211837768555
Epoch 300, val loss: 0.6510444283485413
Epoch 310, training loss: 0.2510491609573364 = 0.18133637309074402 + 0.01 * 6.971279621124268
Epoch 310, val loss: 0.6469039916992188
Epoch 320, training loss: 0.226089745759964 = 0.15641166269779205 + 0.01 * 6.967808246612549
Epoch 320, val loss: 0.6460493803024292
Epoch 330, training loss: 0.20471036434173584 = 0.1350529044866562 + 0.01 * 6.96574592590332
Epoch 330, val loss: 0.6479418873786926
Epoch 340, training loss: 0.18658897280693054 = 0.11699112504720688 + 0.01 * 6.959784507751465
Epoch 340, val loss: 0.6520639061927795
Epoch 350, training loss: 0.17134276032447815 = 0.10179384052753448 + 0.01 * 6.954892158508301
Epoch 350, val loss: 0.6580257415771484
Epoch 360, training loss: 0.15846115350723267 = 0.08899413049221039 + 0.01 * 6.94670295715332
Epoch 360, val loss: 0.6655861735343933
Epoch 370, training loss: 0.14756286144256592 = 0.07817347347736359 + 0.01 * 6.938940048217773
Epoch 370, val loss: 0.6742432117462158
Epoch 380, training loss: 0.13824033737182617 = 0.06900041550397873 + 0.01 * 6.923993110656738
Epoch 380, val loss: 0.683713972568512
Epoch 390, training loss: 0.13037879765033722 = 0.06119542941451073 + 0.01 * 6.918337345123291
Epoch 390, val loss: 0.6936002969741821
Epoch 400, training loss: 0.12357152998447418 = 0.05453404039144516 + 0.01 * 6.903749465942383
Epoch 400, val loss: 0.7038129568099976
Epoch 410, training loss: 0.11774390935897827 = 0.04881633073091507 + 0.01 * 6.892758369445801
Epoch 410, val loss: 0.7140949368476868
Epoch 420, training loss: 0.11282727122306824 = 0.0438859798014164 + 0.01 * 6.894129753112793
Epoch 420, val loss: 0.7243615388870239
Epoch 430, training loss: 0.10825330018997192 = 0.03962179645895958 + 0.01 * 6.8631510734558105
Epoch 430, val loss: 0.7345715165138245
Epoch 440, training loss: 0.10459604859352112 = 0.03591373190283775 + 0.01 * 6.868231296539307
Epoch 440, val loss: 0.7445815205574036
Epoch 450, training loss: 0.10107167065143585 = 0.03267725929617882 + 0.01 * 6.839440822601318
Epoch 450, val loss: 0.7544946670532227
Epoch 460, training loss: 0.0981060191988945 = 0.029837392270565033 + 0.01 * 6.826862812042236
Epoch 460, val loss: 0.7642960548400879
Epoch 470, training loss: 0.09577895700931549 = 0.027338387444615364 + 0.01 * 6.844057083129883
Epoch 470, val loss: 0.7740271687507629
Epoch 480, training loss: 0.09318751841783524 = 0.025135992094874382 + 0.01 * 6.805152416229248
Epoch 480, val loss: 0.7834762334823608
Epoch 490, training loss: 0.09123243391513824 = 0.02318563126027584 + 0.01 * 6.804679870605469
Epoch 490, val loss: 0.7927752733230591
Epoch 500, training loss: 0.08935654163360596 = 0.02145434357225895 + 0.01 * 6.790220260620117
Epoch 500, val loss: 0.8018544316291809
Epoch 510, training loss: 0.08766543865203857 = 0.019911259412765503 + 0.01 * 6.775417804718018
Epoch 510, val loss: 0.8107144832611084
Epoch 520, training loss: 0.08621221780776978 = 0.018531683832406998 + 0.01 * 6.7680535316467285
Epoch 520, val loss: 0.819398820400238
Epoch 530, training loss: 0.08493562042713165 = 0.017293596640229225 + 0.01 * 6.76420259475708
Epoch 530, val loss: 0.8277989625930786
Epoch 540, training loss: 0.08365242183208466 = 0.01617942936718464 + 0.01 * 6.747299671173096
Epoch 540, val loss: 0.836042046546936
Epoch 550, training loss: 0.08235257863998413 = 0.015173409134149551 + 0.01 * 6.717916488647461
Epoch 550, val loss: 0.8440693020820618
Epoch 560, training loss: 0.08163192123174667 = 0.014261389151215553 + 0.01 * 6.737053394317627
Epoch 560, val loss: 0.8519811630249023
Epoch 570, training loss: 0.08056838065385818 = 0.013433339074254036 + 0.01 * 6.713504314422607
Epoch 570, val loss: 0.8595569133758545
Epoch 580, training loss: 0.08012086153030396 = 0.012678982689976692 + 0.01 * 6.74418830871582
Epoch 580, val loss: 0.8669485449790955
Epoch 590, training loss: 0.07881594449281693 = 0.011990542523562908 + 0.01 * 6.6825408935546875
Epoch 590, val loss: 0.8741758465766907
Epoch 600, training loss: 0.07811778038740158 = 0.01135938335210085 + 0.01 * 6.675839900970459
Epoch 600, val loss: 0.8811903595924377
Epoch 610, training loss: 0.07747743278741837 = 0.010779052972793579 + 0.01 * 6.669837951660156
Epoch 610, val loss: 0.8880730271339417
Epoch 620, training loss: 0.07696688175201416 = 0.010244589298963547 + 0.01 * 6.672228813171387
Epoch 620, val loss: 0.8947450518608093
Epoch 630, training loss: 0.07638081908226013 = 0.009751702658832073 + 0.01 * 6.662912368774414
Epoch 630, val loss: 0.9012882113456726
Epoch 640, training loss: 0.0758567675948143 = 0.009296046569943428 + 0.01 * 6.656072616577148
Epoch 640, val loss: 0.907633364200592
Epoch 650, training loss: 0.07517217099666595 = 0.008872892707586288 + 0.01 * 6.629928112030029
Epoch 650, val loss: 0.9138634204864502
Epoch 660, training loss: 0.07521361112594604 = 0.008479219861328602 + 0.01 * 6.673439025878906
Epoch 660, val loss: 0.919961154460907
Epoch 670, training loss: 0.07433662563562393 = 0.008114022202789783 + 0.01 * 6.622260570526123
Epoch 670, val loss: 0.9259031414985657
Epoch 680, training loss: 0.0740249827504158 = 0.0077736228704452515 + 0.01 * 6.625136375427246
Epoch 680, val loss: 0.9316970109939575
Epoch 690, training loss: 0.07375860214233398 = 0.007456005550920963 + 0.01 * 6.630259990692139
Epoch 690, val loss: 0.937323272228241
Epoch 700, training loss: 0.0733574852347374 = 0.007159643340855837 + 0.01 * 6.619784355163574
Epoch 700, val loss: 0.9428670406341553
Epoch 710, training loss: 0.072996586561203 = 0.006882236339151859 + 0.01 * 6.611435413360596
Epoch 710, val loss: 0.9482312202453613
Epoch 720, training loss: 0.07283767312765121 = 0.006622462533414364 + 0.01 * 6.62152099609375
Epoch 720, val loss: 0.9535037279129028
Epoch 730, training loss: 0.07241343706846237 = 0.006378724239766598 + 0.01 * 6.603471279144287
Epoch 730, val loss: 0.9586731195449829
Epoch 740, training loss: 0.0719376653432846 = 0.006149671506136656 + 0.01 * 6.578799724578857
Epoch 740, val loss: 0.9636561870574951
Epoch 750, training loss: 0.07188617438077927 = 0.005934333428740501 + 0.01 * 6.595183849334717
Epoch 750, val loss: 0.9686169624328613
Epoch 760, training loss: 0.07166146486997604 = 0.005731523036956787 + 0.01 * 6.592994689941406
Epoch 760, val loss: 0.9733912944793701
Epoch 770, training loss: 0.07134123891592026 = 0.005540589336305857 + 0.01 * 6.5800652503967285
Epoch 770, val loss: 0.97807776927948
Epoch 780, training loss: 0.07124689966440201 = 0.005360224284231663 + 0.01 * 6.588667869567871
Epoch 780, val loss: 0.9826991558074951
Epoch 790, training loss: 0.070834219455719 = 0.005189660936594009 + 0.01 * 6.564455509185791
Epoch 790, val loss: 0.9871053099632263
Epoch 800, training loss: 0.07077126950025558 = 0.005028598941862583 + 0.01 * 6.574267387390137
Epoch 800, val loss: 0.9915300011634827
Epoch 810, training loss: 0.07050654292106628 = 0.004875937942415476 + 0.01 * 6.563061237335205
Epoch 810, val loss: 0.9957728385925293
Epoch 820, training loss: 0.07046552002429962 = 0.00473136967048049 + 0.01 * 6.573415279388428
Epoch 820, val loss: 1.0000509023666382
Epoch 830, training loss: 0.07020150125026703 = 0.004594041965901852 + 0.01 * 6.560746192932129
Epoch 830, val loss: 1.004131555557251
Epoch 840, training loss: 0.06979817897081375 = 0.004463632125407457 + 0.01 * 6.533454418182373
Epoch 840, val loss: 1.0081772804260254
Epoch 850, training loss: 0.06973394751548767 = 0.004339607432484627 + 0.01 * 6.539434432983398
Epoch 850, val loss: 1.0121368169784546
Epoch 860, training loss: 0.06950180977582932 = 0.00422160979360342 + 0.01 * 6.528019905090332
Epoch 860, val loss: 1.0160657167434692
Epoch 870, training loss: 0.06933876127004623 = 0.004109086003154516 + 0.01 * 6.52296781539917
Epoch 870, val loss: 1.0198866128921509
Epoch 880, training loss: 0.06933814287185669 = 0.004002004396170378 + 0.01 * 6.533613681793213
Epoch 880, val loss: 1.0236233472824097
Epoch 890, training loss: 0.06907195597887039 = 0.003899828065186739 + 0.01 * 6.517212867736816
Epoch 890, val loss: 1.0272594690322876
Epoch 900, training loss: 0.06921342015266418 = 0.003802516497671604 + 0.01 * 6.541090488433838
Epoch 900, val loss: 1.0309514999389648
Epoch 910, training loss: 0.0687093734741211 = 0.0037093646824359894 + 0.01 * 6.500000953674316
Epoch 910, val loss: 1.0344440937042236
Epoch 920, training loss: 0.068845734000206 = 0.0036203227937221527 + 0.01 * 6.522541522979736
Epoch 920, val loss: 1.0378801822662354
Epoch 930, training loss: 0.0686543807387352 = 0.0035351805854588747 + 0.01 * 6.511919975280762
Epoch 930, val loss: 1.0413178205490112
Epoch 940, training loss: 0.06863562017679214 = 0.0034536172170192003 + 0.01 * 6.518199920654297
Epoch 940, val loss: 1.044658899307251
Epoch 950, training loss: 0.06838347762823105 = 0.0033753905445337296 + 0.01 * 6.500809192657471
Epoch 950, val loss: 1.0479367971420288
Epoch 960, training loss: 0.06826358288526535 = 0.0033004109282046556 + 0.01 * 6.496317386627197
Epoch 960, val loss: 1.0511763095855713
Epoch 970, training loss: 0.0682637020945549 = 0.0032284099142998457 + 0.01 * 6.5035295486450195
Epoch 970, val loss: 1.054282546043396
Epoch 980, training loss: 0.0679110586643219 = 0.003159434301778674 + 0.01 * 6.475162506103516
Epoch 980, val loss: 1.0573872327804565
Epoch 990, training loss: 0.06811217963695526 = 0.003093126928433776 + 0.01 * 6.501905918121338
Epoch 990, val loss: 1.0604387521743774
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.7011
Flip ASR: 0.6756/225 nodes
The final ASR:0.72817, 0.04908, Accuracy:0.81975, 0.00175
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11634])
remove edge: torch.Size([2, 9528])
updated graph: torch.Size([2, 10606])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98155, 0.00522, Accuracy:0.83827, 0.00630
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0146901607513428 = 1.9309513568878174 + 0.01 * 8.373878479003906
Epoch 0, val loss: 1.9301942586898804
Epoch 10, training loss: 2.0052671432495117 = 1.9215291738510132 + 0.01 * 8.373801231384277
Epoch 10, val loss: 1.9207065105438232
Epoch 20, training loss: 1.992984652519226 = 1.9092488288879395 + 0.01 * 8.373586654663086
Epoch 20, val loss: 1.9082825183868408
Epoch 30, training loss: 1.9750704765319824 = 1.8913413286209106 + 0.01 * 8.372920036315918
Epoch 30, val loss: 1.8902275562286377
Epoch 40, training loss: 1.9485036134719849 = 1.8648103475570679 + 0.01 * 8.369331359863281
Epoch 40, val loss: 1.864040493965149
Epoch 50, training loss: 1.9126720428466797 = 1.8292332887649536 + 0.01 * 8.343881607055664
Epoch 50, val loss: 1.8311920166015625
Epoch 60, training loss: 1.8748122453689575 = 1.7926126718521118 + 0.01 * 8.219954490661621
Epoch 60, val loss: 1.8019602298736572
Epoch 70, training loss: 1.837774395942688 = 1.757800817489624 + 0.01 * 7.997354507446289
Epoch 70, val loss: 1.7748404741287231
Epoch 80, training loss: 1.7854664325714111 = 1.707999587059021 + 0.01 * 7.746679306030273
Epoch 80, val loss: 1.7325128316879272
Epoch 90, training loss: 1.7138721942901611 = 1.639116883277893 + 0.01 * 7.475535869598389
Epoch 90, val loss: 1.6745387315750122
Epoch 100, training loss: 1.6239937543869019 = 1.551134467124939 + 0.01 * 7.28592586517334
Epoch 100, val loss: 1.6022922992706299
Epoch 110, training loss: 1.5277899503707886 = 1.4557253122329712 + 0.01 * 7.206467628479004
Epoch 110, val loss: 1.5244499444961548
Epoch 120, training loss: 1.432274341583252 = 1.360833764076233 + 0.01 * 7.144052505493164
Epoch 120, val loss: 1.4497610330581665
Epoch 130, training loss: 1.3371343612670898 = 1.2660963535308838 + 0.01 * 7.103806495666504
Epoch 130, val loss: 1.3761793375015259
Epoch 140, training loss: 1.2396819591522217 = 1.1688783168792725 + 0.01 * 7.080368995666504
Epoch 140, val loss: 1.3024137020111084
Epoch 150, training loss: 1.1410164833068848 = 1.0704011917114258 + 0.01 * 7.061530590057373
Epoch 150, val loss: 1.2288398742675781
Epoch 160, training loss: 1.0445531606674194 = 0.9741604328155518 + 0.01 * 7.039276599884033
Epoch 160, val loss: 1.1580601930618286
Epoch 170, training loss: 0.9531452655792236 = 0.8830169439315796 + 0.01 * 7.012835502624512
Epoch 170, val loss: 1.0912679433822632
Epoch 180, training loss: 0.868780255317688 = 0.7988823056221008 + 0.01 * 6.989795684814453
Epoch 180, val loss: 1.0300965309143066
Epoch 190, training loss: 0.7929631471633911 = 0.7232147455215454 + 0.01 * 6.974842548370361
Epoch 190, val loss: 0.9763005971908569
Epoch 200, training loss: 0.7262406945228577 = 0.6565881967544556 + 0.01 * 6.965250492095947
Epoch 200, val loss: 0.930831253528595
Epoch 210, training loss: 0.6679325103759766 = 0.5983450412750244 + 0.01 * 6.958746433258057
Epoch 210, val loss: 0.8939337730407715
Epoch 220, training loss: 0.6171542406082153 = 0.5475970506668091 + 0.01 * 6.955718040466309
Epoch 220, val loss: 0.8655409812927246
Epoch 230, training loss: 0.5727034211158752 = 0.5032310485839844 + 0.01 * 6.947237968444824
Epoch 230, val loss: 0.844879150390625
Epoch 240, training loss: 0.5332267880439758 = 0.4638318717479706 + 0.01 * 6.9394917488098145
Epoch 240, val loss: 0.8304544687271118
Epoch 250, training loss: 0.4969000518321991 = 0.42757996916770935 + 0.01 * 6.932008743286133
Epoch 250, val loss: 0.8202323317527771
Epoch 260, training loss: 0.4620332717895508 = 0.3927710950374603 + 0.01 * 6.9262166023254395
Epoch 260, val loss: 0.8124025464057922
Epoch 270, training loss: 0.42767950892448425 = 0.35850775241851807 + 0.01 * 6.91717529296875
Epoch 270, val loss: 0.8059330582618713
Epoch 280, training loss: 0.3939571976661682 = 0.3248938322067261 + 0.01 * 6.906337261199951
Epoch 280, val loss: 0.8006949424743652
Epoch 290, training loss: 0.36172595620155334 = 0.29275310039520264 + 0.01 * 6.897286415100098
Epoch 290, val loss: 0.7970947623252869
Epoch 300, training loss: 0.33179736137390137 = 0.26294293999671936 + 0.01 * 6.885443210601807
Epoch 300, val loss: 0.7960559725761414
Epoch 310, training loss: 0.304546058177948 = 0.23580141365528107 + 0.01 * 6.874464511871338
Epoch 310, val loss: 0.7978107333183289
Epoch 320, training loss: 0.2797660529613495 = 0.21106505393981934 + 0.01 * 6.8700995445251465
Epoch 320, val loss: 0.801971435546875
Epoch 330, training loss: 0.2567775845527649 = 0.1881745606660843 + 0.01 * 6.8603010177612305
Epoch 330, val loss: 0.808159589767456
Epoch 340, training loss: 0.2351779192686081 = 0.16673247516155243 + 0.01 * 6.844544410705566
Epoch 340, val loss: 0.8160789608955383
Epoch 350, training loss: 0.2152036428451538 = 0.14676029980182648 + 0.01 * 6.844334602355957
Epoch 350, val loss: 0.8251814842224121
Epoch 360, training loss: 0.19677048921585083 = 0.12858879566192627 + 0.01 * 6.818170070648193
Epoch 360, val loss: 0.8355414271354675
Epoch 370, training loss: 0.1807805448770523 = 0.11248739808797836 + 0.01 * 6.829315185546875
Epoch 370, val loss: 0.8473344445228577
Epoch 380, training loss: 0.16649283468723297 = 0.09852714836597443 + 0.01 * 6.796568870544434
Epoch 380, val loss: 0.8606711030006409
Epoch 390, training loss: 0.1544613242149353 = 0.08652522414922714 + 0.01 * 6.793609142303467
Epoch 390, val loss: 0.8754286766052246
Epoch 400, training loss: 0.1440138816833496 = 0.0762379989027977 + 0.01 * 6.777587413787842
Epoch 400, val loss: 0.8913765549659729
Epoch 410, training loss: 0.13529269397258759 = 0.0674140453338623 + 0.01 * 6.787864685058594
Epoch 410, val loss: 0.9081266522407532
Epoch 420, training loss: 0.1276460736989975 = 0.059855569154024124 + 0.01 * 6.779051303863525
Epoch 420, val loss: 0.9252358675003052
Epoch 430, training loss: 0.12096007168292999 = 0.05336969345808029 + 0.01 * 6.759037971496582
Epoch 430, val loss: 0.942382276058197
Epoch 440, training loss: 0.11529598385095596 = 0.04777458310127258 + 0.01 * 6.752140522003174
Epoch 440, val loss: 0.9593446850776672
Epoch 450, training loss: 0.11032535880804062 = 0.04292704910039902 + 0.01 * 6.739831447601318
Epoch 450, val loss: 0.9759636521339417
Epoch 460, training loss: 0.10613510012626648 = 0.03871256485581398 + 0.01 * 6.742253303527832
Epoch 460, val loss: 0.9921091794967651
Epoch 470, training loss: 0.10240878164768219 = 0.035043735057115555 + 0.01 * 6.736504077911377
Epoch 470, val loss: 1.007702112197876
Epoch 480, training loss: 0.09911468625068665 = 0.03183889389038086 + 0.01 * 6.727579116821289
Epoch 480, val loss: 1.022728681564331
Epoch 490, training loss: 0.09617406129837036 = 0.029027877375483513 + 0.01 * 6.71461820602417
Epoch 490, val loss: 1.0372025966644287
Epoch 500, training loss: 0.09363186359405518 = 0.02655576728284359 + 0.01 * 6.7076096534729
Epoch 500, val loss: 1.0510789155960083
Epoch 510, training loss: 0.09150028228759766 = 0.02437604032456875 + 0.01 * 6.712423801422119
Epoch 510, val loss: 1.064454197883606
Epoch 520, training loss: 0.08940226584672928 = 0.022445013746619225 + 0.01 * 6.695724964141846
Epoch 520, val loss: 1.0773046016693115
Epoch 530, training loss: 0.08767145872116089 = 0.020729050040245056 + 0.01 * 6.694241046905518
Epoch 530, val loss: 1.0896201133728027
Epoch 540, training loss: 0.08607688546180725 = 0.0192005205899477 + 0.01 * 6.687636852264404
Epoch 540, val loss: 1.1014480590820312
Epoch 550, training loss: 0.08463817089796066 = 0.017833223566412926 + 0.01 * 6.680494785308838
Epoch 550, val loss: 1.112812876701355
Epoch 560, training loss: 0.08334683626890182 = 0.016606131568551064 + 0.01 * 6.674070835113525
Epoch 560, val loss: 1.1237132549285889
Epoch 570, training loss: 0.08219080418348312 = 0.015502211637794971 + 0.01 * 6.668859481811523
Epoch 570, val loss: 1.1342575550079346
Epoch 580, training loss: 0.08136522769927979 = 0.014506022445857525 + 0.01 * 6.685920715332031
Epoch 580, val loss: 1.1443673372268677
Epoch 590, training loss: 0.08030226081609726 = 0.013605741783976555 + 0.01 * 6.669651508331299
Epoch 590, val loss: 1.1541097164154053
Epoch 600, training loss: 0.07944036275148392 = 0.012789071537554264 + 0.01 * 6.665129661560059
Epoch 600, val loss: 1.1634879112243652
Epoch 610, training loss: 0.07860661298036575 = 0.012046655640006065 + 0.01 * 6.655995845794678
Epoch 610, val loss: 1.172579050064087
Epoch 620, training loss: 0.07777289301156998 = 0.01136926468461752 + 0.01 * 6.6403632164001465
Epoch 620, val loss: 1.1813571453094482
Epoch 630, training loss: 0.0772576555609703 = 0.010749494656920433 + 0.01 * 6.650815963745117
Epoch 630, val loss: 1.1898199319839478
Epoch 640, training loss: 0.07656633853912354 = 0.010181492194533348 + 0.01 * 6.638484954833984
Epoch 640, val loss: 1.1980232000350952
Epoch 650, training loss: 0.0759827271103859 = 0.009659824892878532 + 0.01 * 6.632290363311768
Epoch 650, val loss: 1.2059762477874756
Epoch 660, training loss: 0.07563094794750214 = 0.009178890846669674 + 0.01 * 6.645205974578857
Epoch 660, val loss: 1.2136645317077637
Epoch 670, training loss: 0.07497654855251312 = 0.008735469542443752 + 0.01 * 6.62410831451416
Epoch 670, val loss: 1.2211201190948486
Epoch 680, training loss: 0.07444765418767929 = 0.008325646631419659 + 0.01 * 6.61220121383667
Epoch 680, val loss: 1.2283737659454346
Epoch 690, training loss: 0.07434192299842834 = 0.00794577319175005 + 0.01 * 6.639614582061768
Epoch 690, val loss: 1.2354260683059692
Epoch 700, training loss: 0.07373605668544769 = 0.007593382149934769 + 0.01 * 6.614266872406006
Epoch 700, val loss: 1.2422606945037842
Epoch 710, training loss: 0.073223777115345 = 0.007265807595103979 + 0.01 * 6.595797538757324
Epoch 710, val loss: 1.248915195465088
Epoch 720, training loss: 0.07299553602933884 = 0.006960524711757898 + 0.01 * 6.603500843048096
Epoch 720, val loss: 1.2553679943084717
Epoch 730, training loss: 0.07266167551279068 = 0.006676115561276674 + 0.01 * 6.5985565185546875
Epoch 730, val loss: 1.261681079864502
Epoch 740, training loss: 0.07237772643566132 = 0.006410509347915649 + 0.01 * 6.59672212600708
Epoch 740, val loss: 1.267865538597107
Epoch 750, training loss: 0.07213644683361053 = 0.006161903962492943 + 0.01 * 6.597454071044922
Epoch 750, val loss: 1.273819088935852
Epoch 760, training loss: 0.07174871116876602 = 0.005929014645516872 + 0.01 * 6.581969738006592
Epoch 760, val loss: 1.2796648740768433
Epoch 770, training loss: 0.07151716947555542 = 0.005710591096431017 + 0.01 * 6.580657482147217
Epoch 770, val loss: 1.2853167057037354
Epoch 780, training loss: 0.07152135670185089 = 0.005505474284291267 + 0.01 * 6.601588249206543
Epoch 780, val loss: 1.2908573150634766
Epoch 790, training loss: 0.07102885097265244 = 0.005312448833137751 + 0.01 * 6.5716400146484375
Epoch 790, val loss: 1.2962294816970825
Epoch 800, training loss: 0.07083912193775177 = 0.005130535922944546 + 0.01 * 6.570858955383301
Epoch 800, val loss: 1.3014912605285645
Epoch 810, training loss: 0.07077499479055405 = 0.004959207959473133 + 0.01 * 6.581578731536865
Epoch 810, val loss: 1.3066767454147339
Epoch 820, training loss: 0.07046438753604889 = 0.004797286819666624 + 0.01 * 6.566709995269775
Epoch 820, val loss: 1.311684012413025
Epoch 830, training loss: 0.070268914103508 = 0.004644225351512432 + 0.01 * 6.562469482421875
Epoch 830, val loss: 1.3165916204452515
Epoch 840, training loss: 0.0702633485198021 = 0.004499440081417561 + 0.01 * 6.576390743255615
Epoch 840, val loss: 1.3214161396026611
Epoch 850, training loss: 0.06998665630817413 = 0.004362356849014759 + 0.01 * 6.562429904937744
Epoch 850, val loss: 1.3260935544967651
Epoch 860, training loss: 0.0696934387087822 = 0.00423233350738883 + 0.01 * 6.5461106300354
Epoch 860, val loss: 1.3307092189788818
Epoch 870, training loss: 0.06982410699129105 = 0.0041090017184615135 + 0.01 * 6.571510314941406
Epoch 870, val loss: 1.3351433277130127
Epoch 880, training loss: 0.06936726719141006 = 0.003992004785686731 + 0.01 * 6.5375261306762695
Epoch 880, val loss: 1.3395153284072876
Epoch 890, training loss: 0.06922446936368942 = 0.0038806882221251726 + 0.01 * 6.534378528594971
Epoch 890, val loss: 1.3437787294387817
Epoch 900, training loss: 0.0691414475440979 = 0.0037747127935290337 + 0.01 * 6.5366740226745605
Epoch 900, val loss: 1.3479408025741577
Epoch 910, training loss: 0.06935880333185196 = 0.003673701547086239 + 0.01 * 6.56851053237915
Epoch 910, val loss: 1.3520044088363647
Epoch 920, training loss: 0.0689212903380394 = 0.0035776023287326097 + 0.01 * 6.534368515014648
Epoch 920, val loss: 1.3560014963150024
Epoch 930, training loss: 0.06862477958202362 = 0.0034859683364629745 + 0.01 * 6.513881206512451
Epoch 930, val loss: 1.3598705530166626
Epoch 940, training loss: 0.06875661015510559 = 0.0033984780311584473 + 0.01 * 6.535813331604004
Epoch 940, val loss: 1.3636724948883057
Epoch 950, training loss: 0.06865935772657394 = 0.003314907429739833 + 0.01 * 6.534445762634277
Epoch 950, val loss: 1.3674383163452148
Epoch 960, training loss: 0.0688377246260643 = 0.0032348392996937037 + 0.01 * 6.560288429260254
Epoch 960, val loss: 1.3710367679595947
Epoch 970, training loss: 0.06841251999139786 = 0.0031583677046000957 + 0.01 * 6.525415420532227
Epoch 970, val loss: 1.3746312856674194
Epoch 980, training loss: 0.06820611655712128 = 0.003085311036556959 + 0.01 * 6.512080669403076
Epoch 980, val loss: 1.378145456314087
Epoch 990, training loss: 0.06813011318445206 = 0.00301519432105124 + 0.01 * 6.511491775512695
Epoch 990, val loss: 1.3815557956695557
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7704
Overall ASR: 0.4059
Flip ASR: 0.3067/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0140724182128906 = 1.9303345680236816 + 0.01 * 8.373791694641113
Epoch 0, val loss: 1.9281421899795532
Epoch 10, training loss: 2.003697395324707 = 1.9199607372283936 + 0.01 * 8.373668670654297
Epoch 10, val loss: 1.916961908340454
Epoch 20, training loss: 1.9910708665847778 = 1.9073386192321777 + 0.01 * 8.37322998046875
Epoch 20, val loss: 1.9031851291656494
Epoch 30, training loss: 1.9737026691436768 = 1.8899835348129272 + 0.01 * 8.371917724609375
Epoch 30, val loss: 1.8844058513641357
Epoch 40, training loss: 1.948724627494812 = 1.865063190460205 + 0.01 * 8.366143226623535
Epoch 40, val loss: 1.8579045534133911
Epoch 50, training loss: 1.9138824939727783 = 1.8305652141571045 + 0.01 * 8.331730842590332
Epoch 50, val loss: 1.822670817375183
Epoch 60, training loss: 1.8698334693908691 = 1.7887510061264038 + 0.01 * 8.108246803283691
Epoch 60, val loss: 1.7826776504516602
Epoch 70, training loss: 1.8214799165725708 = 1.7444403171539307 + 0.01 * 7.703962326049805
Epoch 70, val loss: 1.7423070669174194
Epoch 80, training loss: 1.7644954919815063 = 1.6902649402618408 + 0.01 * 7.423058032989502
Epoch 80, val loss: 1.6932848691940308
Epoch 90, training loss: 1.6890639066696167 = 1.6164617538452148 + 0.01 * 7.2602152824401855
Epoch 90, val loss: 1.6308996677398682
Epoch 100, training loss: 1.5937576293945312 = 1.5222450494766235 + 0.01 * 7.1512556076049805
Epoch 100, val loss: 1.5551289319992065
Epoch 110, training loss: 1.4867244958877563 = 1.4157518148422241 + 0.01 * 7.097263813018799
Epoch 110, val loss: 1.46996009349823
Epoch 120, training loss: 1.3794071674346924 = 1.3087674379348755 + 0.01 * 7.063968181610107
Epoch 120, val loss: 1.3862732648849487
Epoch 130, training loss: 1.2774038314819336 = 1.206972360610962 + 0.01 * 7.043151378631592
Epoch 130, val loss: 1.3078588247299194
Epoch 140, training loss: 1.1807773113250732 = 1.1104655265808105 + 0.01 * 7.031179428100586
Epoch 140, val loss: 1.2341686487197876
Epoch 150, training loss: 1.087867259979248 = 1.0176737308502197 + 0.01 * 7.019354343414307
Epoch 150, val loss: 1.1638951301574707
Epoch 160, training loss: 0.9990376830101013 = 0.9289563298225403 + 0.01 * 7.008134841918945
Epoch 160, val loss: 1.0977181196212769
Epoch 170, training loss: 0.9165459275245667 = 0.8465691804885864 + 0.01 * 6.997676372528076
Epoch 170, val loss: 1.0368226766586304
Epoch 180, training loss: 0.8427421450614929 = 0.7728571891784668 + 0.01 * 6.98849630355835
Epoch 180, val loss: 0.9827266931533813
Epoch 190, training loss: 0.7785463929176331 = 0.708732008934021 + 0.01 * 6.981439113616943
Epoch 190, val loss: 0.9360477924346924
Epoch 200, training loss: 0.7227656245231628 = 0.6529974937438965 + 0.01 * 6.976815700531006
Epoch 200, val loss: 0.8962100148200989
Epoch 210, training loss: 0.672669529914856 = 0.602928638458252 + 0.01 * 6.974086761474609
Epoch 210, val loss: 0.8615661263465881
Epoch 220, training loss: 0.6253861784934998 = 0.5556634664535522 + 0.01 * 6.97227144241333
Epoch 220, val loss: 0.8302483558654785
Epoch 230, training loss: 0.5790140628814697 = 0.5093058347702026 + 0.01 * 6.970823764801025
Epoch 230, val loss: 0.8008881211280823
Epoch 240, training loss: 0.5328441858291626 = 0.4631481468677521 + 0.01 * 6.969604015350342
Epoch 240, val loss: 0.7730674147605896
Epoch 250, training loss: 0.4870952367782593 = 0.4174085557460785 + 0.01 * 6.968669414520264
Epoch 250, val loss: 0.7471190094947815
Epoch 260, training loss: 0.4424091875553131 = 0.37272992730140686 + 0.01 * 6.967925548553467
Epoch 260, val loss: 0.7235497236251831
Epoch 270, training loss: 0.399709016084671 = 0.3300361633300781 + 0.01 * 6.96728515625
Epoch 270, val loss: 0.7031312584877014
Epoch 280, training loss: 0.35964828729629517 = 0.2899816930294037 + 0.01 * 6.966658592224121
Epoch 280, val loss: 0.6861823797225952
Epoch 290, training loss: 0.3224809169769287 = 0.25281989574432373 + 0.01 * 6.966100692749023
Epoch 290, val loss: 0.6728493571281433
Epoch 300, training loss: 0.2885502576828003 = 0.21889355778694153 + 0.01 * 6.965671062469482
Epoch 300, val loss: 0.6636748909950256
Epoch 310, training loss: 0.25814512372016907 = 0.18849143385887146 + 0.01 * 6.965369701385498
Epoch 310, val loss: 0.6590632200241089
Epoch 320, training loss: 0.23169848322868347 = 0.16204728186130524 + 0.01 * 6.965120315551758
Epoch 320, val loss: 0.6589882373809814
Epoch 330, training loss: 0.209537535905838 = 0.139887273311615 + 0.01 * 6.965027332305908
Epoch 330, val loss: 0.6634753346443176
Epoch 340, training loss: 0.19105517864227295 = 0.1214088499546051 + 0.01 * 6.964633464813232
Epoch 340, val loss: 0.6714925765991211
Epoch 350, training loss: 0.17555013298988342 = 0.105910524725914 + 0.01 * 6.96396017074585
Epoch 350, val loss: 0.6820427775382996
Epoch 360, training loss: 0.1623731404542923 = 0.09274498373270035 + 0.01 * 6.962815761566162
Epoch 360, val loss: 0.6946314573287964
Epoch 370, training loss: 0.15106135606765747 = 0.08140838891267776 + 0.01 * 6.965296268463135
Epoch 370, val loss: 0.708665668964386
Epoch 380, training loss: 0.14159756898880005 = 0.07199861109256744 + 0.01 * 6.959897041320801
Epoch 380, val loss: 0.7235440015792847
Epoch 390, training loss: 0.13374343514442444 = 0.0641849935054779 + 0.01 * 6.955843925476074
Epoch 390, val loss: 0.7392093539237976
Epoch 400, training loss: 0.12690624594688416 = 0.057395532727241516 + 0.01 * 6.951071739196777
Epoch 400, val loss: 0.7555271983146667
Epoch 410, training loss: 0.1209820806980133 = 0.0515182800590992 + 0.01 * 6.946380138397217
Epoch 410, val loss: 0.7718186974525452
Epoch 420, training loss: 0.11580964922904968 = 0.046403948217630386 + 0.01 * 6.94057035446167
Epoch 420, val loss: 0.7875698208808899
Epoch 430, training loss: 0.1112421303987503 = 0.041938912123441696 + 0.01 * 6.93032169342041
Epoch 430, val loss: 0.8030194044113159
Epoch 440, training loss: 0.10738242417573929 = 0.03802341967821121 + 0.01 * 6.935900688171387
Epoch 440, val loss: 0.8182656764984131
Epoch 450, training loss: 0.10368473082780838 = 0.03457941859960556 + 0.01 * 6.910531044006348
Epoch 450, val loss: 0.8330395817756653
Epoch 460, training loss: 0.10049204528331757 = 0.03153855726122856 + 0.01 * 6.895349502563477
Epoch 460, val loss: 0.8475715517997742
Epoch 470, training loss: 0.0976792424917221 = 0.02884785085916519 + 0.01 * 6.883139133453369
Epoch 470, val loss: 0.8617604374885559
Epoch 480, training loss: 0.09513508528470993 = 0.026465483009815216 + 0.01 * 6.866960048675537
Epoch 480, val loss: 0.875477135181427
Epoch 490, training loss: 0.09290023148059845 = 0.024347657337784767 + 0.01 * 6.855257987976074
Epoch 490, val loss: 0.8888434767723083
Epoch 500, training loss: 0.09086411446332932 = 0.02246440201997757 + 0.01 * 6.839971542358398
Epoch 500, val loss: 0.9018473029136658
Epoch 510, training loss: 0.08921696990728378 = 0.020785437896847725 + 0.01 * 6.843153476715088
Epoch 510, val loss: 0.9145394563674927
Epoch 520, training loss: 0.08757641911506653 = 0.01928727887570858 + 0.01 * 6.828913688659668
Epoch 520, val loss: 0.9268559813499451
Epoch 530, training loss: 0.08604484051465988 = 0.017940491437911987 + 0.01 * 6.810434818267822
Epoch 530, val loss: 0.9387986660003662
Epoch 540, training loss: 0.08496654778718948 = 0.016728468239307404 + 0.01 * 6.823808193206787
Epoch 540, val loss: 0.95048987865448
Epoch 550, training loss: 0.08358791470527649 = 0.015635255724191666 + 0.01 * 6.795266628265381
Epoch 550, val loss: 0.961753249168396
Epoch 560, training loss: 0.08252386003732681 = 0.01464544702321291 + 0.01 * 6.787840843200684
Epoch 560, val loss: 0.972755491733551
Epoch 570, training loss: 0.08139857649803162 = 0.01374761387705803 + 0.01 * 6.765097141265869
Epoch 570, val loss: 0.9834384322166443
Epoch 580, training loss: 0.08054753392934799 = 0.012930469587445259 + 0.01 * 6.761706829071045
Epoch 580, val loss: 0.9938509464263916
Epoch 590, training loss: 0.07996127754449844 = 0.01218323316425085 + 0.01 * 6.777804374694824
Epoch 590, val loss: 1.0040019750595093
Epoch 600, training loss: 0.07896013557910919 = 0.011503390036523342 + 0.01 * 6.7456746101379395
Epoch 600, val loss: 1.0138335227966309
Epoch 610, training loss: 0.07819616049528122 = 0.010881828144192696 + 0.01 * 6.731433868408203
Epoch 610, val loss: 1.0232831239700317
Epoch 620, training loss: 0.07775773853063583 = 0.010311121121048927 + 0.01 * 6.744661808013916
Epoch 620, val loss: 1.0324770212173462
Epoch 630, training loss: 0.07704292237758636 = 0.009786956012248993 + 0.01 * 6.725596904754639
Epoch 630, val loss: 1.0414882898330688
Epoch 640, training loss: 0.0763021931052208 = 0.009304388426244259 + 0.01 * 6.699780464172363
Epoch 640, val loss: 1.0502865314483643
Epoch 650, training loss: 0.07587046176195145 = 0.00885858479887247 + 0.01 * 6.701187610626221
Epoch 650, val loss: 1.0588629245758057
Epoch 660, training loss: 0.07534204423427582 = 0.008446392603218555 + 0.01 * 6.6895647048950195
Epoch 660, val loss: 1.0671974420547485
Epoch 670, training loss: 0.07500701397657394 = 0.008063835091888905 + 0.01 * 6.694317817687988
Epoch 670, val loss: 1.0753533840179443
Epoch 680, training loss: 0.07459119707345963 = 0.007708645425736904 + 0.01 * 6.688255310058594
Epoch 680, val loss: 1.083279013633728
Epoch 690, training loss: 0.07415885478258133 = 0.007378284819424152 + 0.01 * 6.678056716918945
Epoch 690, val loss: 1.0910011529922485
Epoch 700, training loss: 0.0737309604883194 = 0.007070624269545078 + 0.01 * 6.666033744812012
Epoch 700, val loss: 1.098496913909912
Epoch 710, training loss: 0.0734206959605217 = 0.0067834192886948586 + 0.01 * 6.6637282371521
Epoch 710, val loss: 1.1059290170669556
Epoch 720, training loss: 0.07301386445760727 = 0.006515273824334145 + 0.01 * 6.649859428405762
Epoch 720, val loss: 1.1130906343460083
Epoch 730, training loss: 0.07273976504802704 = 0.0062643312849104404 + 0.01 * 6.647543430328369
Epoch 730, val loss: 1.1200976371765137
Epoch 740, training loss: 0.07234831899404526 = 0.006029096897691488 + 0.01 * 6.631922245025635
Epoch 740, val loss: 1.1269383430480957
Epoch 750, training loss: 0.07259786128997803 = 0.005808481480926275 + 0.01 * 6.678938388824463
Epoch 750, val loss: 1.133638620376587
Epoch 760, training loss: 0.07191760838031769 = 0.005601327866315842 + 0.01 * 6.631628036499023
Epoch 760, val loss: 1.1402336359024048
Epoch 770, training loss: 0.07167382538318634 = 0.005406466778367758 + 0.01 * 6.626735687255859
Epoch 770, val loss: 1.146574854850769
Epoch 780, training loss: 0.07151559740304947 = 0.005223043262958527 + 0.01 * 6.629255771636963
Epoch 780, val loss: 1.152830958366394
Epoch 790, training loss: 0.07117833942174911 = 0.00505018001422286 + 0.01 * 6.612815856933594
Epoch 790, val loss: 1.1589508056640625
Epoch 800, training loss: 0.07088624686002731 = 0.0048868670128285885 + 0.01 * 6.59993839263916
Epoch 800, val loss: 1.1649492979049683
Epoch 810, training loss: 0.0707724392414093 = 0.004732493311166763 + 0.01 * 6.603994846343994
Epoch 810, val loss: 1.1708524227142334
Epoch 820, training loss: 0.07067953050136566 = 0.004586290568113327 + 0.01 * 6.6093244552612305
Epoch 820, val loss: 1.176598072052002
Epoch 830, training loss: 0.07032625377178192 = 0.004447886720299721 + 0.01 * 6.587836742401123
Epoch 830, val loss: 1.1822450160980225
Epoch 840, training loss: 0.07035040855407715 = 0.004316544625908136 + 0.01 * 6.603386402130127
Epoch 840, val loss: 1.187793254852295
Epoch 850, training loss: 0.0701032429933548 = 0.004192174877971411 + 0.01 * 6.59110689163208
Epoch 850, val loss: 1.1931712627410889
Epoch 860, training loss: 0.06974726170301437 = 0.00407392019405961 + 0.01 * 6.5673346519470215
Epoch 860, val loss: 1.1984854936599731
Epoch 870, training loss: 0.06972627341747284 = 0.003961545880883932 + 0.01 * 6.576472759246826
Epoch 870, val loss: 1.2036654949188232
Epoch 880, training loss: 0.06983444839715958 = 0.0038547685835510492 + 0.01 * 6.597968101501465
Epoch 880, val loss: 1.2087414264678955
Epoch 890, training loss: 0.06934156268835068 = 0.003753043245524168 + 0.01 * 6.558852195739746
Epoch 890, val loss: 1.2137408256530762
Epoch 900, training loss: 0.06938055157661438 = 0.003656200133264065 + 0.01 * 6.572434902191162
Epoch 900, val loss: 1.2186527252197266
Epoch 910, training loss: 0.06901361048221588 = 0.003564143320545554 + 0.01 * 6.544947147369385
Epoch 910, val loss: 1.2233777046203613
Epoch 920, training loss: 0.06914517283439636 = 0.0034762565046548843 + 0.01 * 6.566891670227051
Epoch 920, val loss: 1.228065013885498
Epoch 930, training loss: 0.06874626874923706 = 0.003392253303900361 + 0.01 * 6.535401344299316
Epoch 930, val loss: 1.2326489686965942
Epoch 940, training loss: 0.06873758882284164 = 0.0033119716681540012 + 0.01 * 6.542562007904053
Epoch 940, val loss: 1.2371398210525513
Epoch 950, training loss: 0.0686812624335289 = 0.00323517806828022 + 0.01 * 6.544609069824219
Epoch 950, val loss: 1.2415590286254883
Epoch 960, training loss: 0.06832273304462433 = 0.0031616766937077045 + 0.01 * 6.516105651855469
Epoch 960, val loss: 1.245900273323059
Epoch 970, training loss: 0.06883641332387924 = 0.0030913737136870623 + 0.01 * 6.574504375457764
Epoch 970, val loss: 1.2501248121261597
Epoch 980, training loss: 0.0683770477771759 = 0.0030241310596466064 + 0.01 * 6.535292148590088
Epoch 980, val loss: 1.2542451620101929
Epoch 990, training loss: 0.06851159781217575 = 0.002959574805572629 + 0.01 * 6.555202960968018
Epoch 990, val loss: 1.2583234310150146
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.7454
Flip ASR: 0.7067/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0372989177703857 = 1.9535603523254395 + 0.01 * 8.373849868774414
Epoch 0, val loss: 1.953054666519165
Epoch 10, training loss: 2.0266706943511963 = 1.9429330825805664 + 0.01 * 8.373750686645508
Epoch 10, val loss: 1.9421757459640503
Epoch 20, training loss: 2.0133326053619385 = 1.929598331451416 + 0.01 * 8.373435974121094
Epoch 20, val loss: 1.9285343885421753
Epoch 30, training loss: 1.9945247173309326 = 1.9108010530471802 + 0.01 * 8.372367858886719
Epoch 30, val loss: 1.9094812870025635
Epoch 40, training loss: 1.9667121171951294 = 1.8830515146255493 + 0.01 * 8.366057395935059
Epoch 40, val loss: 1.8819409608840942
Epoch 50, training loss: 1.9273552894592285 = 1.8441524505615234 + 0.01 * 8.32027816772461
Epoch 50, val loss: 1.8452410697937012
Epoch 60, training loss: 1.879979133605957 = 1.7994362115859985 + 0.01 * 8.05429744720459
Epoch 60, val loss: 1.8066264390945435
Epoch 70, training loss: 1.8362828493118286 = 1.7578881978988647 + 0.01 * 7.839467525482178
Epoch 70, val loss: 1.7718770503997803
Epoch 80, training loss: 1.7817665338516235 = 1.7070521116256714 + 0.01 * 7.471445560455322
Epoch 80, val loss: 1.7254823446273804
Epoch 90, training loss: 1.709852933883667 = 1.6375460624694824 + 0.01 * 7.230681896209717
Epoch 90, val loss: 1.6641453504562378
Epoch 100, training loss: 1.6187090873718262 = 1.5473389625549316 + 0.01 * 7.1370134353637695
Epoch 100, val loss: 1.5894147157669067
Epoch 110, training loss: 1.515507459640503 = 1.4448058605194092 + 0.01 * 7.070162773132324
Epoch 110, val loss: 1.5073513984680176
Epoch 120, training loss: 1.4111950397491455 = 1.3410687446594238 + 0.01 * 7.0126261711120605
Epoch 120, val loss: 1.4268782138824463
Epoch 130, training loss: 1.3092575073242188 = 1.239545226097107 + 0.01 * 6.9712324142456055
Epoch 130, val loss: 1.3502025604248047
Epoch 140, training loss: 1.2084943056106567 = 1.1390550136566162 + 0.01 * 6.943926811218262
Epoch 140, val loss: 1.2752370834350586
Epoch 150, training loss: 1.1094539165496826 = 1.0401653051376343 + 0.01 * 6.9288554191589355
Epoch 150, val loss: 1.201263189315796
Epoch 160, training loss: 1.0145059823989868 = 0.9452877044677734 + 0.01 * 6.921822547912598
Epoch 160, val loss: 1.1314257383346558
Epoch 170, training loss: 0.9257943630218506 = 0.8566114902496338 + 0.01 * 6.918284893035889
Epoch 170, val loss: 1.0685418844223022
Epoch 180, training loss: 0.8443534970283508 = 0.7752058506011963 + 0.01 * 6.914763450622559
Epoch 180, val loss: 1.0130951404571533
Epoch 190, training loss: 0.7702068090438843 = 0.7010805606842041 + 0.01 * 6.912624835968018
Epoch 190, val loss: 0.964538037776947
Epoch 200, training loss: 0.7026999592781067 = 0.6336002945899963 + 0.01 * 6.909966945648193
Epoch 200, val loss: 0.9226606488227844
Epoch 210, training loss: 0.6410290598869324 = 0.5719566345214844 + 0.01 * 6.907243728637695
Epoch 210, val loss: 0.8861242532730103
Epoch 220, training loss: 0.5846864581108093 = 0.5156657099723816 + 0.01 * 6.902073860168457
Epoch 220, val loss: 0.854171097278595
Epoch 230, training loss: 0.5334917306900024 = 0.4645279049873352 + 0.01 * 6.896379470825195
Epoch 230, val loss: 0.8263503909111023
Epoch 240, training loss: 0.48729410767555237 = 0.4183640778064728 + 0.01 * 6.893003940582275
Epoch 240, val loss: 0.8026900291442871
Epoch 250, training loss: 0.44562143087387085 = 0.3767852783203125 + 0.01 * 6.883615493774414
Epoch 250, val loss: 0.7833854556083679
Epoch 260, training loss: 0.4079645872116089 = 0.3392070233821869 + 0.01 * 6.875756740570068
Epoch 260, val loss: 0.7683601379394531
Epoch 270, training loss: 0.37367600202560425 = 0.305009663105011 + 0.01 * 6.866635322570801
Epoch 270, val loss: 0.7570767402648926
Epoch 280, training loss: 0.3422318696975708 = 0.2736329436302185 + 0.01 * 6.859893798828125
Epoch 280, val loss: 0.7487965822219849
Epoch 290, training loss: 0.31302589178085327 = 0.24457359313964844 + 0.01 * 6.845228672027588
Epoch 290, val loss: 0.7430421113967896
Epoch 300, training loss: 0.2859819531440735 = 0.21743977069854736 + 0.01 * 6.854219913482666
Epoch 300, val loss: 0.7394657135009766
Epoch 310, training loss: 0.2604480981826782 = 0.1921447366476059 + 0.01 * 6.830338001251221
Epoch 310, val loss: 0.738023042678833
Epoch 320, training loss: 0.2369607388973236 = 0.16885815560817719 + 0.01 * 6.810258388519287
Epoch 320, val loss: 0.7385926246643066
Epoch 330, training loss: 0.21592438220977783 = 0.1478787213563919 + 0.01 * 6.804567337036133
Epoch 330, val loss: 0.7412489652633667
Epoch 340, training loss: 0.19738182425498962 = 0.12935790419578552 + 0.01 * 6.802391529083252
Epoch 340, val loss: 0.7461382746696472
Epoch 350, training loss: 0.18111556768417358 = 0.1132650151848793 + 0.01 * 6.785055637359619
Epoch 350, val loss: 0.7531157732009888
Epoch 360, training loss: 0.16706927120685577 = 0.09939400106668472 + 0.01 * 6.767527103424072
Epoch 360, val loss: 0.7620963454246521
Epoch 370, training loss: 0.15530000627040863 = 0.08748865872621536 + 0.01 * 6.781134605407715
Epoch 370, val loss: 0.7728328704833984
Epoch 380, training loss: 0.14479102194309235 = 0.07729098200798035 + 0.01 * 6.750004291534424
Epoch 380, val loss: 0.7849406599998474
Epoch 390, training loss: 0.13595232367515564 = 0.0685337483882904 + 0.01 * 6.741856575012207
Epoch 390, val loss: 0.7981855273246765
Epoch 400, training loss: 0.1283586174249649 = 0.06098978593945503 + 0.01 * 6.736883640289307
Epoch 400, val loss: 0.8123307228088379
Epoch 410, training loss: 0.12185053527355194 = 0.05448685213923454 + 0.01 * 6.736368179321289
Epoch 410, val loss: 0.8270717263221741
Epoch 420, training loss: 0.11606819927692413 = 0.04886120930314064 + 0.01 * 6.720698833465576
Epoch 420, val loss: 0.8421506285667419
Epoch 430, training loss: 0.11108969151973724 = 0.04397047311067581 + 0.01 * 6.711921691894531
Epoch 430, val loss: 0.857469379901886
Epoch 440, training loss: 0.10674218833446503 = 0.03970500826835632 + 0.01 * 6.703718185424805
Epoch 440, val loss: 0.8728864789009094
Epoch 450, training loss: 0.10301852226257324 = 0.035980530083179474 + 0.01 * 6.703799724578857
Epoch 450, val loss: 0.888196587562561
Epoch 460, training loss: 0.09972480684518814 = 0.03272615373134613 + 0.01 * 6.699865341186523
Epoch 460, val loss: 0.9033606648445129
Epoch 470, training loss: 0.09672844409942627 = 0.0298739206045866 + 0.01 * 6.685451984405518
Epoch 470, val loss: 0.9181768298149109
Epoch 480, training loss: 0.094237320125103 = 0.02737046591937542 + 0.01 * 6.686685085296631
Epoch 480, val loss: 0.9326571822166443
Epoch 490, training loss: 0.09191982448101044 = 0.02516709640622139 + 0.01 * 6.675273418426514
Epoch 490, val loss: 0.9466308355331421
Epoch 500, training loss: 0.0898803323507309 = 0.02321859449148178 + 0.01 * 6.666173934936523
Epoch 500, val loss: 0.9602130055427551
Epoch 510, training loss: 0.08829671889543533 = 0.021487675607204437 + 0.01 * 6.680904388427734
Epoch 510, val loss: 0.9733784794807434
Epoch 520, training loss: 0.08658845722675323 = 0.01994660496711731 + 0.01 * 6.664185047149658
Epoch 520, val loss: 0.9861465096473694
Epoch 530, training loss: 0.08505316823720932 = 0.018566424027085304 + 0.01 * 6.648674488067627
Epoch 530, val loss: 0.9985169768333435
Epoch 540, training loss: 0.08381375670433044 = 0.017326610162854195 + 0.01 * 6.648715019226074
Epoch 540, val loss: 1.0104564428329468
Epoch 550, training loss: 0.08276288956403732 = 0.016209691762924194 + 0.01 * 6.655320167541504
Epoch 550, val loss: 1.0220845937728882
Epoch 560, training loss: 0.08153554797172546 = 0.015200487338006496 + 0.01 * 6.6335062980651855
Epoch 560, val loss: 1.0333324670791626
Epoch 570, training loss: 0.08048627525568008 = 0.014285909943282604 + 0.01 * 6.620037078857422
Epoch 570, val loss: 1.0443559885025024
Epoch 580, training loss: 0.07974401861429214 = 0.013453890569508076 + 0.01 * 6.629012584686279
Epoch 580, val loss: 1.0550415515899658
Epoch 590, training loss: 0.07879489660263062 = 0.012693807482719421 + 0.01 * 6.610109329223633
Epoch 590, val loss: 1.0654544830322266
Epoch 600, training loss: 0.07825905829668045 = 0.011998225934803486 + 0.01 * 6.6260833740234375
Epoch 600, val loss: 1.0756012201309204
Epoch 610, training loss: 0.07746515423059464 = 0.011361387558281422 + 0.01 * 6.610376358032227
Epoch 610, val loss: 1.085507869720459
Epoch 620, training loss: 0.0766778215765953 = 0.010775582864880562 + 0.01 * 6.590224266052246
Epoch 620, val loss: 1.0950636863708496
Epoch 630, training loss: 0.07614581286907196 = 0.010236258618533611 + 0.01 * 6.59095573425293
Epoch 630, val loss: 1.1043843030929565
Epoch 640, training loss: 0.07550368458032608 = 0.009738786146044731 + 0.01 * 6.576489448547363
Epoch 640, val loss: 1.1134310960769653
Epoch 650, training loss: 0.07499699294567108 = 0.009279250167310238 + 0.01 * 6.571774005889893
Epoch 650, val loss: 1.1223315000534058
Epoch 660, training loss: 0.07462095469236374 = 0.008853355422616005 + 0.01 * 6.576760292053223
Epoch 660, val loss: 1.1309669017791748
Epoch 670, training loss: 0.07439859211444855 = 0.008457793854176998 + 0.01 * 6.594079971313477
Epoch 670, val loss: 1.1394098997116089
Epoch 680, training loss: 0.07377628982067108 = 0.008090452291071415 + 0.01 * 6.568583965301514
Epoch 680, val loss: 1.1476331949234009
Epoch 690, training loss: 0.07359074056148529 = 0.007748152129352093 + 0.01 * 6.584259033203125
Epoch 690, val loss: 1.15561044216156
Epoch 700, training loss: 0.07316315174102783 = 0.0074295541271567345 + 0.01 * 6.573359489440918
Epoch 700, val loss: 1.1634920835494995
Epoch 710, training loss: 0.07276209443807602 = 0.007131696678698063 + 0.01 * 6.563039779663086
Epoch 710, val loss: 1.1710597276687622
Epoch 720, training loss: 0.0723821371793747 = 0.006853439845144749 + 0.01 * 6.552870273590088
Epoch 720, val loss: 1.1785763502120972
Epoch 730, training loss: 0.0720122903585434 = 0.006592200603336096 + 0.01 * 6.542009353637695
Epoch 730, val loss: 1.1858400106430054
Epoch 740, training loss: 0.07197637856006622 = 0.006347568705677986 + 0.01 * 6.562880992889404
Epoch 740, val loss: 1.1929739713668823
Epoch 750, training loss: 0.07129061222076416 = 0.006117833312600851 + 0.01 * 6.517277717590332
Epoch 750, val loss: 1.199884057044983
Epoch 760, training loss: 0.07121266424655914 = 0.005901763215661049 + 0.01 * 6.531090259552002
Epoch 760, val loss: 1.2067060470581055
Epoch 770, training loss: 0.07083063572645187 = 0.005698396824300289 + 0.01 * 6.513223648071289
Epoch 770, val loss: 1.2133193016052246
Epoch 780, training loss: 0.07064656913280487 = 0.005506820511072874 + 0.01 * 6.513974666595459
Epoch 780, val loss: 1.219863772392273
Epoch 790, training loss: 0.07046245783567429 = 0.005326049868017435 + 0.01 * 6.513641357421875
Epoch 790, val loss: 1.2262569665908813
Epoch 800, training loss: 0.0702061578631401 = 0.005155234597623348 + 0.01 * 6.505092620849609
Epoch 800, val loss: 1.2325009107589722
Epoch 810, training loss: 0.07010138779878616 = 0.0049935379065573215 + 0.01 * 6.5107855796813965
Epoch 810, val loss: 1.2385987043380737
Epoch 820, training loss: 0.06974130868911743 = 0.004840534180402756 + 0.01 * 6.490077495574951
Epoch 820, val loss: 1.2445471286773682
Epoch 830, training loss: 0.06959560513496399 = 0.00469560781493783 + 0.01 * 6.489999771118164
Epoch 830, val loss: 1.250406265258789
Epoch 840, training loss: 0.0694805383682251 = 0.004558246582746506 + 0.01 * 6.492229461669922
Epoch 840, val loss: 1.2561196088790894
Epoch 850, training loss: 0.0695415511727333 = 0.004427893552929163 + 0.01 * 6.5113654136657715
Epoch 850, val loss: 1.2617992162704468
Epoch 860, training loss: 0.06924930214881897 = 0.004303864669054747 + 0.01 * 6.49454402923584
Epoch 860, val loss: 1.2672450542449951
Epoch 870, training loss: 0.0688471645116806 = 0.004186097998172045 + 0.01 * 6.466106414794922
Epoch 870, val loss: 1.272666096687317
Epoch 880, training loss: 0.06888971477746964 = 0.00407383544370532 + 0.01 * 6.481587886810303
Epoch 880, val loss: 1.2779268026351929
Epoch 890, training loss: 0.06869572401046753 = 0.003966824617236853 + 0.01 * 6.4728899002075195
Epoch 890, val loss: 1.283079743385315
Epoch 900, training loss: 0.06843293458223343 = 0.0038649027701467276 + 0.01 * 6.456803798675537
Epoch 900, val loss: 1.288188099861145
Epoch 910, training loss: 0.06832027435302734 = 0.0037674414925277233 + 0.01 * 6.4552836418151855
Epoch 910, val loss: 1.2931368350982666
Epoch 920, training loss: 0.06815925985574722 = 0.003674570005387068 + 0.01 * 6.448469638824463
Epoch 920, val loss: 1.2980228662490845
Epoch 930, training loss: 0.0681183710694313 = 0.0035857732873409986 + 0.01 * 6.45326042175293
Epoch 930, val loss: 1.302858829498291
Epoch 940, training loss: 0.06802289187908173 = 0.0035005651880055666 + 0.01 * 6.452232837677002
Epoch 940, val loss: 1.3075438737869263
Epoch 950, training loss: 0.06809672713279724 = 0.0034191831946372986 + 0.01 * 6.467754364013672
Epoch 950, val loss: 1.312182068824768
Epoch 960, training loss: 0.06763482093811035 = 0.0033413164783269167 + 0.01 * 6.429350852966309
Epoch 960, val loss: 1.3167411088943481
Epoch 970, training loss: 0.0676543116569519 = 0.003266853280365467 + 0.01 * 6.438745498657227
Epoch 970, val loss: 1.3212641477584839
Epoch 980, training loss: 0.06759390980005264 = 0.0031952799763530493 + 0.01 * 6.439863204956055
Epoch 980, val loss: 1.325662612915039
Epoch 990, training loss: 0.06737461686134338 = 0.0031265991274267435 + 0.01 * 6.424802303314209
Epoch 990, val loss: 1.329985499382019
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.70972, 0.23486, Accuracy:0.80741, 0.02688
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11572])
remove edge: torch.Size([2, 9526])
updated graph: torch.Size([2, 10542])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98155, 0.00603, Accuracy:0.83210, 0.00761
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0320045948028564 = 1.9482654333114624 + 0.01 * 8.373921394348145
Epoch 0, val loss: 1.9510554075241089
Epoch 10, training loss: 2.021466016769409 = 1.9377270936965942 + 0.01 * 8.373883247375488
Epoch 10, val loss: 1.9405126571655273
Epoch 20, training loss: 2.0084116458892822 = 1.924674153327942 + 0.01 * 8.373758316040039
Epoch 20, val loss: 1.9274450540542603
Epoch 30, training loss: 1.9897754192352295 = 1.9060407876968384 + 0.01 * 8.373461723327637
Epoch 30, val loss: 1.9089680910110474
Epoch 40, training loss: 1.962103247642517 = 1.8783793449401855 + 0.01 * 8.372394561767578
Epoch 40, val loss: 1.8822392225265503
Epoch 50, training loss: 1.9236948490142822 = 1.840038537979126 + 0.01 * 8.36563491821289
Epoch 50, val loss: 1.8474518060684204
Epoch 60, training loss: 1.8808249235153198 = 1.7975636720657349 + 0.01 * 8.326123237609863
Epoch 60, val loss: 1.8129708766937256
Epoch 70, training loss: 1.8393930196762085 = 1.7581273317337036 + 0.01 * 8.126569747924805
Epoch 70, val loss: 1.7804759740829468
Epoch 80, training loss: 1.786202073097229 = 1.706589937210083 + 0.01 * 7.961212158203125
Epoch 80, val loss: 1.7327582836151123
Epoch 90, training loss: 1.7120637893676758 = 1.6341807842254639 + 0.01 * 7.78830623626709
Epoch 90, val loss: 1.6687711477279663
Epoch 100, training loss: 1.6168255805969238 = 1.5402922630310059 + 0.01 * 7.653336048126221
Epoch 100, val loss: 1.5913140773773193
Epoch 110, training loss: 1.5137519836425781 = 1.4395151138305664 + 0.01 * 7.423692226409912
Epoch 110, val loss: 1.510897159576416
Epoch 120, training loss: 1.415776014328003 = 1.3428481817245483 + 0.01 * 7.29278039932251
Epoch 120, val loss: 1.4360641241073608
Epoch 130, training loss: 1.3242145776748657 = 1.2518283128738403 + 0.01 * 7.238625526428223
Epoch 130, val loss: 1.3691067695617676
Epoch 140, training loss: 1.2376821041107178 = 1.165582299232483 + 0.01 * 7.209979057312012
Epoch 140, val loss: 1.3079596757888794
Epoch 150, training loss: 1.1535835266113281 = 1.0817906856536865 + 0.01 * 7.179283618927002
Epoch 150, val loss: 1.248803734779358
Epoch 160, training loss: 1.0700416564941406 = 0.9984883069992065 + 0.01 * 7.155334949493408
Epoch 160, val loss: 1.1899449825286865
Epoch 170, training loss: 0.9868855476379395 = 0.9154974818229675 + 0.01 * 7.138803958892822
Epoch 170, val loss: 1.1304816007614136
Epoch 180, training loss: 0.9052929282188416 = 0.8340656161308289 + 0.01 * 7.122730255126953
Epoch 180, val loss: 1.0713363885879517
Epoch 190, training loss: 0.8268017172813416 = 0.7557839155197144 + 0.01 * 7.101779460906982
Epoch 190, val loss: 1.0140529870986938
Epoch 200, training loss: 0.7526991367340088 = 0.6819241642951965 + 0.01 * 7.077495574951172
Epoch 200, val loss: 0.9600217342376709
Epoch 210, training loss: 0.6840048432350159 = 0.6133943200111389 + 0.01 * 7.06105375289917
Epoch 210, val loss: 0.9104286432266235
Epoch 220, training loss: 0.6210914254188538 = 0.5506339073181152 + 0.01 * 7.045752048492432
Epoch 220, val loss: 0.865963876247406
Epoch 230, training loss: 0.5639753937721252 = 0.49363064765930176 + 0.01 * 7.034472942352295
Epoch 230, val loss: 0.8277485370635986
Epoch 240, training loss: 0.5120147466659546 = 0.44177043437957764 + 0.01 * 7.0244317054748535
Epoch 240, val loss: 0.7963691353797913
Epoch 250, training loss: 0.4643171429634094 = 0.3941744267940521 + 0.01 * 7.014271259307861
Epoch 250, val loss: 0.7717669606208801
Epoch 260, training loss: 0.4202931523323059 = 0.35021504759788513 + 0.01 * 7.007811546325684
Epoch 260, val loss: 0.753377377986908
Epoch 270, training loss: 0.3795650899410248 = 0.3095960021018982 + 0.01 * 6.9969096183776855
Epoch 270, val loss: 0.7398500442504883
Epoch 280, training loss: 0.3420400619506836 = 0.2721966505050659 + 0.01 * 6.9843430519104
Epoch 280, val loss: 0.729950487613678
Epoch 290, training loss: 0.3078722357749939 = 0.23805616796016693 + 0.01 * 6.981606960296631
Epoch 290, val loss: 0.7232767343521118
Epoch 300, training loss: 0.2769506573677063 = 0.20730075240135193 + 0.01 * 6.964990615844727
Epoch 300, val loss: 0.720045268535614
Epoch 310, training loss: 0.24966961145401 = 0.18001525104045868 + 0.01 * 6.9654364585876465
Epoch 310, val loss: 0.7198980450630188
Epoch 320, training loss: 0.2256578505039215 = 0.15620765089988708 + 0.01 * 6.945021152496338
Epoch 320, val loss: 0.7227929830551147
Epoch 330, training loss: 0.20505094528198242 = 0.13566428422927856 + 0.01 * 6.938666343688965
Epoch 330, val loss: 0.7286007404327393
Epoch 340, training loss: 0.187533438205719 = 0.11814796179533005 + 0.01 * 6.938548564910889
Epoch 340, val loss: 0.736814022064209
Epoch 350, training loss: 0.17257001996040344 = 0.10333861410617828 + 0.01 * 6.923141002655029
Epoch 350, val loss: 0.7469945549964905
Epoch 360, training loss: 0.15994441509246826 = 0.09080002456903458 + 0.01 * 6.914438247680664
Epoch 360, val loss: 0.7585451602935791
Epoch 370, training loss: 0.1491536796092987 = 0.08012375980615616 + 0.01 * 6.9029927253723145
Epoch 370, val loss: 0.7710098028182983
Epoch 380, training loss: 0.1400049328804016 = 0.07099087536334991 + 0.01 * 6.901406288146973
Epoch 380, val loss: 0.7841305732727051
Epoch 390, training loss: 0.13207890093326569 = 0.06315585225820541 + 0.01 * 6.892305374145508
Epoch 390, val loss: 0.7977087497711182
Epoch 400, training loss: 0.1252351701259613 = 0.05640097334980965 + 0.01 * 6.883419990539551
Epoch 400, val loss: 0.8115792870521545
Epoch 410, training loss: 0.11933819949626923 = 0.05055856332182884 + 0.01 * 6.877963066101074
Epoch 410, val loss: 0.8256828784942627
Epoch 420, training loss: 0.11424963176250458 = 0.04548995941877365 + 0.01 * 6.875967025756836
Epoch 420, val loss: 0.8398153781890869
Epoch 430, training loss: 0.1097945123910904 = 0.04108209162950516 + 0.01 * 6.871242523193359
Epoch 430, val loss: 0.8540163040161133
Epoch 440, training loss: 0.10585043579339981 = 0.03723665326833725 + 0.01 * 6.861378192901611
Epoch 440, val loss: 0.8681215643882751
Epoch 450, training loss: 0.10241144150495529 = 0.033868223428726196 + 0.01 * 6.8543219566345215
Epoch 450, val loss: 0.8820721507072449
Epoch 460, training loss: 0.09938449412584305 = 0.03090822510421276 + 0.01 * 6.84762716293335
Epoch 460, val loss: 0.8958691954612732
Epoch 470, training loss: 0.09676851332187653 = 0.02829839661717415 + 0.01 * 6.847011566162109
Epoch 470, val loss: 0.9094753265380859
Epoch 480, training loss: 0.0943414717912674 = 0.025990698486566544 + 0.01 * 6.835076808929443
Epoch 480, val loss: 0.9228184223175049
Epoch 490, training loss: 0.09223297983407974 = 0.023943031206727028 + 0.01 * 6.828995227813721
Epoch 490, val loss: 0.9358496069908142
Epoch 500, training loss: 0.09041626751422882 = 0.022120151668787003 + 0.01 * 6.829611778259277
Epoch 500, val loss: 0.9484868049621582
Epoch 510, training loss: 0.08865249156951904 = 0.020492669194936752 + 0.01 * 6.815981864929199
Epoch 510, val loss: 0.9608614444732666
Epoch 520, training loss: 0.08727188408374786 = 0.019034940749406815 + 0.01 * 6.823694705963135
Epoch 520, val loss: 0.9729059338569641
Epoch 530, training loss: 0.0858442559838295 = 0.017726421356201172 + 0.01 * 6.811783313751221
Epoch 530, val loss: 0.984606146812439
Epoch 540, training loss: 0.08458030223846436 = 0.01654806360602379 + 0.01 * 6.803223609924316
Epoch 540, val loss: 0.995898425579071
Epoch 550, training loss: 0.0834541991353035 = 0.015483587048947811 + 0.01 * 6.797060966491699
Epoch 550, val loss: 1.0068761110305786
Epoch 560, training loss: 0.08240693062543869 = 0.014519712887704372 + 0.01 * 6.788721561431885
Epoch 560, val loss: 1.0175440311431885
Epoch 570, training loss: 0.0816521942615509 = 0.013644506223499775 + 0.01 * 6.8007683753967285
Epoch 570, val loss: 1.0278657674789429
Epoch 580, training loss: 0.08062714338302612 = 0.012848659418523312 + 0.01 * 6.777848243713379
Epoch 580, val loss: 1.0378862619400024
Epoch 590, training loss: 0.07980899512767792 = 0.012122776359319687 + 0.01 * 6.768621444702148
Epoch 590, val loss: 1.0475341081619263
Epoch 600, training loss: 0.07919835299253464 = 0.011458846740424633 + 0.01 * 6.773951053619385
Epoch 600, val loss: 1.0569915771484375
Epoch 610, training loss: 0.0784616768360138 = 0.010850746184587479 + 0.01 * 6.761092662811279
Epoch 610, val loss: 1.0660818815231323
Epoch 620, training loss: 0.07788492739200592 = 0.010292362421751022 + 0.01 * 6.759256362915039
Epoch 620, val loss: 1.0749578475952148
Epoch 630, training loss: 0.07752065360546112 = 0.009778310544788837 + 0.01 * 6.774234294891357
Epoch 630, val loss: 1.0834866762161255
Epoch 640, training loss: 0.07668545097112656 = 0.009304472245275974 + 0.01 * 6.73809814453125
Epoch 640, val loss: 1.0918751955032349
Epoch 650, training loss: 0.07624706625938416 = 0.008866541087627411 + 0.01 * 6.7380523681640625
Epoch 650, val loss: 1.0999280214309692
Epoch 660, training loss: 0.07592228055000305 = 0.008460653945803642 + 0.01 * 6.7461628913879395
Epoch 660, val loss: 1.107830286026001
Epoch 670, training loss: 0.07527586072683334 = 0.008084124885499477 + 0.01 * 6.719173908233643
Epoch 670, val loss: 1.1155235767364502
Epoch 680, training loss: 0.07488672435283661 = 0.007734070997685194 + 0.01 * 6.71526575088501
Epoch 680, val loss: 1.122894525527954
Epoch 690, training loss: 0.07459341734647751 = 0.007407843600958586 + 0.01 * 6.718557834625244
Epoch 690, val loss: 1.1302052736282349
Epoch 700, training loss: 0.07432058453559875 = 0.0071027399972081184 + 0.01 * 6.721784591674805
Epoch 700, val loss: 1.1372684240341187
Epoch 710, training loss: 0.07379784435033798 = 0.006816812790930271 + 0.01 * 6.698103427886963
Epoch 710, val loss: 1.1442064046859741
Epoch 720, training loss: 0.07338067889213562 = 0.0065482184290885925 + 0.01 * 6.68324613571167
Epoch 720, val loss: 1.1509331464767456
Epoch 730, training loss: 0.073235422372818 = 0.006295354571193457 + 0.01 * 6.694007396697998
Epoch 730, val loss: 1.1575098037719727
Epoch 740, training loss: 0.0730317085981369 = 0.006057291757315397 + 0.01 * 6.697441577911377
Epoch 740, val loss: 1.1641021966934204
Epoch 750, training loss: 0.07256165891885757 = 0.005832959432154894 + 0.01 * 6.672869682312012
Epoch 750, val loss: 1.1704094409942627
Epoch 760, training loss: 0.07223494350910187 = 0.005621136631816626 + 0.01 * 6.661380767822266
Epoch 760, val loss: 1.1765916347503662
Epoch 770, training loss: 0.0721326470375061 = 0.005420996807515621 + 0.01 * 6.6711649894714355
Epoch 770, val loss: 1.1827882528305054
Epoch 780, training loss: 0.07174138724803925 = 0.005232495255768299 + 0.01 * 6.650888919830322
Epoch 780, val loss: 1.1887123584747314
Epoch 790, training loss: 0.07177236676216125 = 0.005055258981883526 + 0.01 * 6.671711444854736
Epoch 790, val loss: 1.1945083141326904
Epoch 800, training loss: 0.07142146676778793 = 0.004887937568128109 + 0.01 * 6.653353214263916
Epoch 800, val loss: 1.2001991271972656
Epoch 810, training loss: 0.07109927386045456 = 0.004729586653411388 + 0.01 * 6.636969089508057
Epoch 810, val loss: 1.205709457397461
Epoch 820, training loss: 0.07089600712060928 = 0.0045795985497534275 + 0.01 * 6.631641387939453
Epoch 820, val loss: 1.211172103881836
Epoch 830, training loss: 0.07093502581119537 = 0.004437335766851902 + 0.01 * 6.649769306182861
Epoch 830, val loss: 1.2164108753204346
Epoch 840, training loss: 0.07047320902347565 = 0.004302734509110451 + 0.01 * 6.6170477867126465
Epoch 840, val loss: 1.2217650413513184
Epoch 850, training loss: 0.07032254338264465 = 0.00417494447901845 + 0.01 * 6.614759922027588
Epoch 850, val loss: 1.22671377658844
Epoch 860, training loss: 0.07013659179210663 = 0.004053919576108456 + 0.01 * 6.608267307281494
Epoch 860, val loss: 1.2317777872085571
Epoch 870, training loss: 0.07008281350135803 = 0.003938778303563595 + 0.01 * 6.614403247833252
Epoch 870, val loss: 1.2364990711212158
Epoch 880, training loss: 0.06980821490287781 = 0.0038292603567242622 + 0.01 * 6.597895622253418
Epoch 880, val loss: 1.2413852214813232
Epoch 890, training loss: 0.06967741250991821 = 0.003724971553310752 + 0.01 * 6.595244407653809
Epoch 890, val loss: 1.2459797859191895
Epoch 900, training loss: 0.06960727274417877 = 0.0036254306323826313 + 0.01 * 6.598184108734131
Epoch 900, val loss: 1.2505314350128174
Epoch 910, training loss: 0.06944046169519424 = 0.003530584741383791 + 0.01 * 6.590987682342529
Epoch 910, val loss: 1.2550420761108398
Epoch 920, training loss: 0.06948435306549072 = 0.003439955646172166 + 0.01 * 6.604440212249756
Epoch 920, val loss: 1.2593992948532104
Epoch 930, training loss: 0.06929280608892441 = 0.003353344276547432 + 0.01 * 6.59394645690918
Epoch 930, val loss: 1.2637224197387695
Epoch 940, training loss: 0.06904760748147964 = 0.0032705781050026417 + 0.01 * 6.577702522277832
Epoch 940, val loss: 1.2679595947265625
Epoch 950, training loss: 0.06899457424879074 = 0.003191280411556363 + 0.01 * 6.580329895019531
Epoch 950, val loss: 1.2720216512680054
Epoch 960, training loss: 0.06903620064258575 = 0.0031154355965554714 + 0.01 * 6.592076778411865
Epoch 960, val loss: 1.2761352062225342
Epoch 970, training loss: 0.06886260211467743 = 0.00304299034178257 + 0.01 * 6.581961631774902
Epoch 970, val loss: 1.2800458669662476
Epoch 980, training loss: 0.06867524981498718 = 0.0029736019205302 + 0.01 * 6.570165157318115
Epoch 980, val loss: 1.2839279174804688
Epoch 990, training loss: 0.06870190799236298 = 0.0029070358723402023 + 0.01 * 6.579486846923828
Epoch 990, val loss: 1.2876261472702026
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.7638
Flip ASR: 0.7156/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0319888591766357 = 1.9482495784759521 + 0.01 * 8.373924255371094
Epoch 0, val loss: 1.9473456144332886
Epoch 10, training loss: 2.0212020874023438 = 1.9374635219573975 + 0.01 * 8.373847007751465
Epoch 10, val loss: 1.9354497194290161
Epoch 20, training loss: 2.0074636936187744 = 1.9237275123596191 + 0.01 * 8.373613357543945
Epoch 20, val loss: 1.9202722311019897
Epoch 30, training loss: 1.9877046346664429 = 1.9039756059646606 + 0.01 * 8.372903823852539
Epoch 30, val loss: 1.8986642360687256
Epoch 40, training loss: 1.9583808183670044 = 1.8746927976608276 + 0.01 * 8.368799209594727
Epoch 40, val loss: 1.8674921989440918
Epoch 50, training loss: 1.9180526733398438 = 1.834674596786499 + 0.01 * 8.33780288696289
Epoch 50, val loss: 1.827336311340332
Epoch 60, training loss: 1.873486876487732 = 1.7918204069137573 + 0.01 * 8.166642189025879
Epoch 60, val loss: 1.7891085147857666
Epoch 70, training loss: 1.8324577808380127 = 1.7520825862884521 + 0.01 * 8.037516593933105
Epoch 70, val loss: 1.756837248802185
Epoch 80, training loss: 1.7773022651672363 = 1.6999033689498901 + 0.01 * 7.739887237548828
Epoch 80, val loss: 1.7125941514968872
Epoch 90, training loss: 1.702636480331421 = 1.6285946369171143 + 0.01 * 7.404189586639404
Epoch 90, val loss: 1.6511067152023315
Epoch 100, training loss: 1.6093740463256836 = 1.5368635654449463 + 0.01 * 7.251049518585205
Epoch 100, val loss: 1.5738799571990967
Epoch 110, training loss: 1.5058444738388062 = 1.4342436790466309 + 0.01 * 7.160083293914795
Epoch 110, val loss: 1.4917631149291992
Epoch 120, training loss: 1.4024184942245483 = 1.331394910812378 + 0.01 * 7.102360248565674
Epoch 120, val loss: 1.413544774055481
Epoch 130, training loss: 1.3013100624084473 = 1.2306886911392212 + 0.01 * 7.0621337890625
Epoch 130, val loss: 1.3395817279815674
Epoch 140, training loss: 1.2025219202041626 = 1.132213830947876 + 0.01 * 7.030812740325928
Epoch 140, val loss: 1.267668604850769
Epoch 150, training loss: 1.1068198680877686 = 1.036716341972351 + 0.01 * 7.0103559494018555
Epoch 150, val loss: 1.1972168684005737
Epoch 160, training loss: 1.0161199569702148 = 0.9462544918060303 + 0.01 * 6.986551284790039
Epoch 160, val loss: 1.1305184364318848
Epoch 170, training loss: 0.9329250454902649 = 0.8631859421730042 + 0.01 * 6.973907947540283
Epoch 170, val loss: 1.0703619718551636
Epoch 180, training loss: 0.8586460947990417 = 0.7890907526016235 + 0.01 * 6.955532073974609
Epoch 180, val loss: 1.0185480117797852
Epoch 190, training loss: 0.7935611605644226 = 0.724115252494812 + 0.01 * 6.944588661193848
Epoch 190, val loss: 0.9749075174331665
Epoch 200, training loss: 0.7362916469573975 = 0.6669245958328247 + 0.01 * 6.936708450317383
Epoch 200, val loss: 0.9388599991798401
Epoch 210, training loss: 0.6845450401306152 = 0.6152480840682983 + 0.01 * 6.929696083068848
Epoch 210, val loss: 0.9089111685752869
Epoch 220, training loss: 0.6362249851226807 = 0.5669695734977722 + 0.01 * 6.925538063049316
Epoch 220, val loss: 0.883507251739502
Epoch 230, training loss: 0.5897002220153809 = 0.5204938650131226 + 0.01 * 6.9206390380859375
Epoch 230, val loss: 0.8614944815635681
Epoch 240, training loss: 0.5444815158843994 = 0.47511208057403564 + 0.01 * 6.9369425773620605
Epoch 240, val loss: 0.8422041535377502
Epoch 250, training loss: 0.5000011920928955 = 0.4307752251625061 + 0.01 * 6.92259407043457
Epoch 250, val loss: 0.8252975344657898
Epoch 260, training loss: 0.4567496180534363 = 0.3875712454319 + 0.01 * 6.917838096618652
Epoch 260, val loss: 0.8103981614112854
Epoch 270, training loss: 0.4155927896499634 = 0.34644100069999695 + 0.01 * 6.915178298950195
Epoch 270, val loss: 0.7984994649887085
Epoch 280, training loss: 0.3772759437561035 = 0.3081440031528473 + 0.01 * 6.913194179534912
Epoch 280, val loss: 0.7899905443191528
Epoch 290, training loss: 0.3425021469593048 = 0.27339279651641846 + 0.01 * 6.910934925079346
Epoch 290, val loss: 0.7854044437408447
Epoch 300, training loss: 0.3114449679851532 = 0.242300346493721 + 0.01 * 6.914462089538574
Epoch 300, val loss: 0.7851113080978394
Epoch 310, training loss: 0.283580482006073 = 0.21448718011379242 + 0.01 * 6.909329414367676
Epoch 310, val loss: 0.7885969877243042
Epoch 320, training loss: 0.25838539004325867 = 0.18931445479393005 + 0.01 * 6.907093524932861
Epoch 320, val loss: 0.7954787611961365
Epoch 330, training loss: 0.23570309579372406 = 0.166636660695076 + 0.01 * 6.906643867492676
Epoch 330, val loss: 0.8056248426437378
Epoch 340, training loss: 0.2153034508228302 = 0.14626887440681458 + 0.01 * 6.9034576416015625
Epoch 340, val loss: 0.8183046579360962
Epoch 350, training loss: 0.1972617208957672 = 0.12826348841190338 + 0.01 * 6.8998236656188965
Epoch 350, val loss: 0.8330388069152832
Epoch 360, training loss: 0.18192322552204132 = 0.11295503377914429 + 0.01 * 6.896819114685059
Epoch 360, val loss: 0.8499590754508972
Epoch 370, training loss: 0.1688191294670105 = 0.09986960142850876 + 0.01 * 6.894952297210693
Epoch 370, val loss: 0.8686163425445557
Epoch 380, training loss: 0.157507061958313 = 0.08853763341903687 + 0.01 * 6.896942138671875
Epoch 380, val loss: 0.8879765868186951
Epoch 390, training loss: 0.14753055572509766 = 0.07867205888032913 + 0.01 * 6.885850429534912
Epoch 390, val loss: 0.9076560735702515
Epoch 400, training loss: 0.1391260027885437 = 0.07006064802408218 + 0.01 * 6.906536102294922
Epoch 400, val loss: 0.9274856448173523
Epoch 410, training loss: 0.13135917484760284 = 0.06256712228059769 + 0.01 * 6.879205703735352
Epoch 410, val loss: 0.9474818110466003
Epoch 420, training loss: 0.12480387091636658 = 0.05606396496295929 + 0.01 * 6.873990535736084
Epoch 420, val loss: 0.9673869013786316
Epoch 430, training loss: 0.1191243827342987 = 0.05040788650512695 + 0.01 * 6.871649742126465
Epoch 430, val loss: 0.9869648218154907
Epoch 440, training loss: 0.11407963186502457 = 0.045475319027900696 + 0.01 * 6.86043119430542
Epoch 440, val loss: 1.0060601234436035
Epoch 450, training loss: 0.10971759259700775 = 0.041156139224767685 + 0.01 * 6.85614538192749
Epoch 450, val loss: 1.0247403383255005
Epoch 460, training loss: 0.10582274198532104 = 0.03737112879753113 + 0.01 * 6.845161437988281
Epoch 460, val loss: 1.0427995920181274
Epoch 470, training loss: 0.10249089449644089 = 0.03404724597930908 + 0.01 * 6.844364643096924
Epoch 470, val loss: 1.0603318214416504
Epoch 480, training loss: 0.09960862994194031 = 0.031120646744966507 + 0.01 * 6.848797798156738
Epoch 480, val loss: 1.0772455930709839
Epoch 490, training loss: 0.09676461666822433 = 0.02853844314813614 + 0.01 * 6.822617530822754
Epoch 490, val loss: 1.093615174293518
Epoch 500, training loss: 0.09446794539690018 = 0.0262506902217865 + 0.01 * 6.821725368499756
Epoch 500, val loss: 1.1094077825546265
Epoch 510, training loss: 0.09245483577251434 = 0.02421632595360279 + 0.01 * 6.823851585388184
Epoch 510, val loss: 1.124611496925354
Epoch 520, training loss: 0.09045232832431793 = 0.02240595780313015 + 0.01 * 6.8046369552612305
Epoch 520, val loss: 1.1393835544586182
Epoch 530, training loss: 0.08912794291973114 = 0.020788155496120453 + 0.01 * 6.833978652954102
Epoch 530, val loss: 1.1536240577697754
Epoch 540, training loss: 0.08734767138957977 = 0.019340207800269127 + 0.01 * 6.800746440887451
Epoch 540, val loss: 1.1674494743347168
Epoch 550, training loss: 0.08603131771087646 = 0.018039781600236893 + 0.01 * 6.799154281616211
Epoch 550, val loss: 1.1807703971862793
Epoch 560, training loss: 0.08455531299114227 = 0.01686723716557026 + 0.01 * 6.768807888031006
Epoch 560, val loss: 1.1936686038970947
Epoch 570, training loss: 0.0842009037733078 = 0.015806226059794426 + 0.01 * 6.839467525482178
Epoch 570, val loss: 1.206173062324524
Epoch 580, training loss: 0.08260923624038696 = 0.014846480451524258 + 0.01 * 6.776275634765625
Epoch 580, val loss: 1.2182270288467407
Epoch 590, training loss: 0.08177253603935242 = 0.013974873349070549 + 0.01 * 6.77976655960083
Epoch 590, val loss: 1.2298778295516968
Epoch 600, training loss: 0.0807061418890953 = 0.01318102516233921 + 0.01 * 6.752511978149414
Epoch 600, val loss: 1.2411940097808838
Epoch 610, training loss: 0.0797773078083992 = 0.01245543360710144 + 0.01 * 6.732187271118164
Epoch 610, val loss: 1.2521796226501465
Epoch 620, training loss: 0.07903092354536057 = 0.011790567077696323 + 0.01 * 6.72403621673584
Epoch 620, val loss: 1.2628036737442017
Epoch 630, training loss: 0.0783885195851326 = 0.011181002482771873 + 0.01 * 6.720751762390137
Epoch 630, val loss: 1.273108959197998
Epoch 640, training loss: 0.07796818017959595 = 0.010620415210723877 + 0.01 * 6.734776973724365
Epoch 640, val loss: 1.2831485271453857
Epoch 650, training loss: 0.07723086327314377 = 0.01010413933545351 + 0.01 * 6.712672233581543
Epoch 650, val loss: 1.2928956747055054
Epoch 660, training loss: 0.0766834169626236 = 0.009627088904380798 + 0.01 * 6.70563268661499
Epoch 660, val loss: 1.302333116531372
Epoch 670, training loss: 0.07613794505596161 = 0.009185432456433773 + 0.01 * 6.695250988006592
Epoch 670, val loss: 1.3114763498306274
Epoch 680, training loss: 0.07563422620296478 = 0.008775968104600906 + 0.01 * 6.685825824737549
Epoch 680, val loss: 1.3204066753387451
Epoch 690, training loss: 0.07522432506084442 = 0.008395766839385033 + 0.01 * 6.68285608291626
Epoch 690, val loss: 1.3290736675262451
Epoch 700, training loss: 0.07502371072769165 = 0.008041881024837494 + 0.01 * 6.698183059692383
Epoch 700, val loss: 1.3375047445297241
Epoch 710, training loss: 0.07436184585094452 = 0.0077117495238780975 + 0.01 * 6.665009498596191
Epoch 710, val loss: 1.3457814455032349
Epoch 720, training loss: 0.07437442243099213 = 0.007402787916362286 + 0.01 * 6.6971635818481445
Epoch 720, val loss: 1.353771686553955
Epoch 730, training loss: 0.07364165782928467 = 0.0071137878112494946 + 0.01 * 6.652787208557129
Epoch 730, val loss: 1.361592411994934
Epoch 740, training loss: 0.07326170802116394 = 0.0068428292870521545 + 0.01 * 6.64188814163208
Epoch 740, val loss: 1.3692296743392944
Epoch 750, training loss: 0.07347877323627472 = 0.006588441785424948 + 0.01 * 6.689033031463623
Epoch 750, val loss: 1.3766051530838013
Epoch 760, training loss: 0.07266370952129364 = 0.006350636016577482 + 0.01 * 6.631307125091553
Epoch 760, val loss: 1.3838547468185425
Epoch 770, training loss: 0.07236040383577347 = 0.006127400789409876 + 0.01 * 6.623300075531006
Epoch 770, val loss: 1.3909094333648682
Epoch 780, training loss: 0.0724356546998024 = 0.005917281378060579 + 0.01 * 6.651837348937988
Epoch 780, val loss: 1.3977476358413696
Epoch 790, training loss: 0.07193084806203842 = 0.005719405598938465 + 0.01 * 6.621143817901611
Epoch 790, val loss: 1.404450535774231
Epoch 800, training loss: 0.0720505639910698 = 0.0055328309535980225 + 0.01 * 6.651773452758789
Epoch 800, val loss: 1.4110430479049683
Epoch 810, training loss: 0.07147841155529022 = 0.005356857553124428 + 0.01 * 6.612155437469482
Epoch 810, val loss: 1.417443871498108
Epoch 820, training loss: 0.07108061015605927 = 0.005190311465412378 + 0.01 * 6.589029788970947
Epoch 820, val loss: 1.4236812591552734
Epoch 830, training loss: 0.07108932733535767 = 0.005032530520111322 + 0.01 * 6.605679512023926
Epoch 830, val loss: 1.4298168420791626
Epoch 840, training loss: 0.07085762172937393 = 0.004882954526692629 + 0.01 * 6.597466945648193
Epoch 840, val loss: 1.4357833862304688
Epoch 850, training loss: 0.0705404207110405 = 0.004740992560982704 + 0.01 * 6.5799431800842285
Epoch 850, val loss: 1.441648244857788
Epoch 860, training loss: 0.07031282037496567 = 0.004606172908097506 + 0.01 * 6.570664882659912
Epoch 860, val loss: 1.447307825088501
Epoch 870, training loss: 0.07027703523635864 = 0.004478128626942635 + 0.01 * 6.579891204833984
Epoch 870, val loss: 1.4529365301132202
Epoch 880, training loss: 0.07012741267681122 = 0.004356234800070524 + 0.01 * 6.577117919921875
Epoch 880, val loss: 1.4584304094314575
Epoch 890, training loss: 0.06998114287853241 = 0.0042401994578540325 + 0.01 * 6.574094295501709
Epoch 890, val loss: 1.4636595249176025
Epoch 900, training loss: 0.07007671892642975 = 0.004129679407924414 + 0.01 * 6.5947041511535645
Epoch 900, val loss: 1.4689559936523438
Epoch 910, training loss: 0.06961523741483688 = 0.004024282563477755 + 0.01 * 6.559095859527588
Epoch 910, val loss: 1.4740434885025024
Epoch 920, training loss: 0.06954646855592728 = 0.003923580050468445 + 0.01 * 6.562288761138916
Epoch 920, val loss: 1.4790570735931396
Epoch 930, training loss: 0.06949113309383392 = 0.003827369771897793 + 0.01 * 6.566376209259033
Epoch 930, val loss: 1.4839783906936646
Epoch 940, training loss: 0.06916693598031998 = 0.0037353243678808212 + 0.01 * 6.543161392211914
Epoch 940, val loss: 1.4887562990188599
Epoch 950, training loss: 0.06905066967010498 = 0.003647334175184369 + 0.01 * 6.540333271026611
Epoch 950, val loss: 1.4934715032577515
Epoch 960, training loss: 0.06882546097040176 = 0.0035630955826491117 + 0.01 * 6.526236057281494
Epoch 960, val loss: 1.4981416463851929
Epoch 970, training loss: 0.06885766983032227 = 0.0034823003225028515 + 0.01 * 6.537537097930908
Epoch 970, val loss: 1.502604603767395
Epoch 980, training loss: 0.06879518181085587 = 0.0034049726091325283 + 0.01 * 6.539021015167236
Epoch 980, val loss: 1.5071110725402832
Epoch 990, training loss: 0.06838884949684143 = 0.003330694045871496 + 0.01 * 6.505815505981445
Epoch 990, val loss: 1.5113884210586548
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.6605
Flip ASR: 0.6178/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0452983379364014 = 1.9615592956542969 + 0.01 * 8.373893737792969
Epoch 0, val loss: 1.963133692741394
Epoch 10, training loss: 2.034024477005005 = 1.9502862691879272 + 0.01 * 8.373831748962402
Epoch 10, val loss: 1.952070713043213
Epoch 20, training loss: 2.0203394889831543 = 1.936603307723999 + 0.01 * 8.373620986938477
Epoch 20, val loss: 1.9380031824111938
Epoch 30, training loss: 2.001387119293213 = 1.9176572561264038 + 0.01 * 8.372990608215332
Epoch 30, val loss: 1.9180723428726196
Epoch 40, training loss: 1.9737054109573364 = 1.890006422996521 + 0.01 * 8.369901657104492
Epoch 40, val loss: 1.8890151977539062
Epoch 50, training loss: 1.9344696998596191 = 1.8509697914123535 + 0.01 * 8.34998607635498
Epoch 50, val loss: 1.8495079278945923
Epoch 60, training loss: 1.8874720335006714 = 1.804856777191162 + 0.01 * 8.261520385742188
Epoch 60, val loss: 1.806900978088379
Epoch 70, training loss: 1.8431103229522705 = 1.7629408836364746 + 0.01 * 8.01694393157959
Epoch 70, val loss: 1.7718006372451782
Epoch 80, training loss: 1.794205665588379 = 1.715569257736206 + 0.01 * 7.863639831542969
Epoch 80, val loss: 1.7300752401351929
Epoch 90, training loss: 1.7267810106277466 = 1.6499371528625488 + 0.01 * 7.684386730194092
Epoch 90, val loss: 1.6734791994094849
Epoch 100, training loss: 1.6386154890060425 = 1.5633410215377808 + 0.01 * 7.527445316314697
Epoch 100, val loss: 1.6010887622833252
Epoch 110, training loss: 1.5383837223052979 = 1.464969515800476 + 0.01 * 7.34141731262207
Epoch 110, val loss: 1.5202058553695679
Epoch 120, training loss: 1.4411476850509644 = 1.3687207698822021 + 0.01 * 7.242690086364746
Epoch 120, val loss: 1.442996859550476
Epoch 130, training loss: 1.3519948720932007 = 1.2800877094268799 + 0.01 * 7.190720081329346
Epoch 130, val loss: 1.3760040998458862
Epoch 140, training loss: 1.2696019411087036 = 1.198290467262268 + 0.01 * 7.13115119934082
Epoch 140, val loss: 1.3168280124664307
Epoch 150, training loss: 1.1903022527694702 = 1.1195131540298462 + 0.01 * 7.078907012939453
Epoch 150, val loss: 1.2600719928741455
Epoch 160, training loss: 1.109521746635437 = 1.0390889644622803 + 0.01 * 7.043276786804199
Epoch 160, val loss: 1.2012958526611328
Epoch 170, training loss: 1.0249660015106201 = 0.9547749757766724 + 0.01 * 7.019105911254883
Epoch 170, val loss: 1.1388728618621826
Epoch 180, training loss: 0.9384639263153076 = 0.8684849143028259 + 0.01 * 6.997900485992432
Epoch 180, val loss: 1.0751285552978516
Epoch 190, training loss: 0.8550547361373901 = 0.7852470874786377 + 0.01 * 6.9807634353637695
Epoch 190, val loss: 1.0148786306381226
Epoch 200, training loss: 0.7796428799629211 = 0.7099425196647644 + 0.01 * 6.970034122467041
Epoch 200, val loss: 0.9628809094429016
Epoch 210, training loss: 0.7136484980583191 = 0.6440014243125916 + 0.01 * 6.96470832824707
Epoch 210, val loss: 0.9203187823295593
Epoch 220, training loss: 0.6550211310386658 = 0.5854080319404602 + 0.01 * 6.961310863494873
Epoch 220, val loss: 0.8852981328964233
Epoch 230, training loss: 0.6006028056144714 = 0.5310274958610535 + 0.01 * 6.957531929016113
Epoch 230, val loss: 0.8547313213348389
Epoch 240, training loss: 0.5480566024780273 = 0.47852304577827454 + 0.01 * 6.953353404998779
Epoch 240, val loss: 0.8266608119010925
Epoch 250, training loss: 0.4965812861919403 = 0.4270969033241272 + 0.01 * 6.9484381675720215
Epoch 250, val loss: 0.8001067638397217
Epoch 260, training loss: 0.4465673863887787 = 0.3771395981311798 + 0.01 * 6.942779541015625
Epoch 260, val loss: 0.7752603888511658
Epoch 270, training loss: 0.3989851772785187 = 0.3295918107032776 + 0.01 * 6.93933629989624
Epoch 270, val loss: 0.753474235534668
Epoch 280, training loss: 0.3546718657016754 = 0.2853539288043976 + 0.01 * 6.9317946434021
Epoch 280, val loss: 0.7358638048171997
Epoch 290, training loss: 0.3144460916519165 = 0.2452046275138855 + 0.01 * 6.924148082733154
Epoch 290, val loss: 0.72312331199646
Epoch 300, training loss: 0.27886930108070374 = 0.20968441665172577 + 0.01 * 6.918488025665283
Epoch 300, val loss: 0.7153391242027283
Epoch 310, training loss: 0.24817109107971191 = 0.17905393242835999 + 0.01 * 6.911716461181641
Epoch 310, val loss: 0.7122798562049866
Epoch 320, training loss: 0.22235557436943054 = 0.15330006182193756 + 0.01 * 6.905551910400391
Epoch 320, val loss: 0.7134287357330322
Epoch 330, training loss: 0.20097312331199646 = 0.131956547498703 + 0.01 * 6.901657581329346
Epoch 330, val loss: 0.7181118130683899
Epoch 340, training loss: 0.18328392505645752 = 0.11431000381708145 + 0.01 * 6.897391319274902
Epoch 340, val loss: 0.7253201007843018
Epoch 350, training loss: 0.16855472326278687 = 0.09963543713092804 + 0.01 * 6.8919291496276855
Epoch 350, val loss: 0.7343756556510925
Epoch 360, training loss: 0.1562073528766632 = 0.08732166886329651 + 0.01 * 6.888567924499512
Epoch 360, val loss: 0.744655191898346
Epoch 370, training loss: 0.1457485407590866 = 0.07689719647169113 + 0.01 * 6.885134220123291
Epoch 370, val loss: 0.7557427287101746
Epoch 380, training loss: 0.1368289440870285 = 0.0680028647184372 + 0.01 * 6.882608413696289
Epoch 380, val loss: 0.7674115300178528
Epoch 390, training loss: 0.12915053963661194 = 0.0603683665394783 + 0.01 * 6.878217697143555
Epoch 390, val loss: 0.7794177532196045
Epoch 400, training loss: 0.12251714617013931 = 0.05378381907939911 + 0.01 * 6.873332500457764
Epoch 400, val loss: 0.7916160225868225
Epoch 410, training loss: 0.1167805939912796 = 0.04808023199439049 + 0.01 * 6.8700361251831055
Epoch 410, val loss: 0.8040100336074829
Epoch 420, training loss: 0.11181345582008362 = 0.04312491416931152 + 0.01 * 6.868854522705078
Epoch 420, val loss: 0.8164733052253723
Epoch 430, training loss: 0.10745121538639069 = 0.03880854323506355 + 0.01 * 6.864266872406006
Epoch 430, val loss: 0.8289573788642883
Epoch 440, training loss: 0.1036350280046463 = 0.03504042699933052 + 0.01 * 6.859460353851318
Epoch 440, val loss: 0.8414471745491028
Epoch 450, training loss: 0.10034085810184479 = 0.031742244958877563 + 0.01 * 6.859861373901367
Epoch 450, val loss: 0.8537471294403076
Epoch 460, training loss: 0.09739187359809875 = 0.028848718851804733 + 0.01 * 6.854315757751465
Epoch 460, val loss: 0.8658835887908936
Epoch 470, training loss: 0.09479449689388275 = 0.02630191668868065 + 0.01 * 6.849257946014404
Epoch 470, val loss: 0.8777813911437988
Epoch 480, training loss: 0.0925261527299881 = 0.024050215259194374 + 0.01 * 6.847593784332275
Epoch 480, val loss: 0.8894383907318115
Epoch 490, training loss: 0.09047123789787292 = 0.02205628529191017 + 0.01 * 6.841495513916016
Epoch 490, val loss: 0.9007216691970825
Epoch 500, training loss: 0.08866076916456223 = 0.020284725353121758 + 0.01 * 6.83760404586792
Epoch 500, val loss: 0.9117368459701538
Epoch 510, training loss: 0.08715274184942245 = 0.018708528950810432 + 0.01 * 6.84442138671875
Epoch 510, val loss: 0.9224313497543335
Epoch 520, training loss: 0.08561965078115463 = 0.01730419509112835 + 0.01 * 6.831545829772949
Epoch 520, val loss: 0.932765543460846
Epoch 530, training loss: 0.08430744707584381 = 0.01604801043868065 + 0.01 * 6.825943470001221
Epoch 530, val loss: 0.9428142309188843
Epoch 540, training loss: 0.08314052224159241 = 0.014920998364686966 + 0.01 * 6.8219523429870605
Epoch 540, val loss: 0.9525340795516968
Epoch 550, training loss: 0.08208470791578293 = 0.013906897976994514 + 0.01 * 6.817781448364258
Epoch 550, val loss: 0.9619809985160828
Epoch 560, training loss: 0.08117597550153732 = 0.012992015108466148 + 0.01 * 6.818396091461182
Epoch 560, val loss: 0.9710943102836609
Epoch 570, training loss: 0.08029083162546158 = 0.012165372259914875 + 0.01 * 6.8125457763671875
Epoch 570, val loss: 0.9799204468727112
Epoch 580, training loss: 0.07947719842195511 = 0.011416854336857796 + 0.01 * 6.806034564971924
Epoch 580, val loss: 0.9885610938072205
Epoch 590, training loss: 0.07874809950590134 = 0.01073706615716219 + 0.01 * 6.801103115081787
Epoch 590, val loss: 0.996836245059967
Epoch 600, training loss: 0.0780961737036705 = 0.010118597187101841 + 0.01 * 6.797757625579834
Epoch 600, val loss: 1.0049368143081665
Epoch 610, training loss: 0.0774943083524704 = 0.009553839452564716 + 0.01 * 6.794046878814697
Epoch 610, val loss: 1.0128127336502075
Epoch 620, training loss: 0.07696375250816345 = 0.009037060663104057 + 0.01 * 6.79266881942749
Epoch 620, val loss: 1.0203666687011719
Epoch 630, training loss: 0.07639271020889282 = 0.008563223294913769 + 0.01 * 6.782948970794678
Epoch 630, val loss: 1.0277818441390991
Epoch 640, training loss: 0.07599236071109772 = 0.008127610199153423 + 0.01 * 6.786475658416748
Epoch 640, val loss: 1.034978985786438
Epoch 650, training loss: 0.07551588118076324 = 0.007726532407104969 + 0.01 * 6.778934478759766
Epoch 650, val loss: 1.0419524908065796
Epoch 660, training loss: 0.0750339925289154 = 0.007356453221291304 + 0.01 * 6.767754077911377
Epoch 660, val loss: 1.0487616062164307
Epoch 670, training loss: 0.07474270462989807 = 0.007014278322458267 + 0.01 * 6.7728424072265625
Epoch 670, val loss: 1.0553046464920044
Epoch 680, training loss: 0.07433772087097168 = 0.006697561126202345 + 0.01 * 6.764016628265381
Epoch 680, val loss: 1.061813235282898
Epoch 690, training loss: 0.07393912225961685 = 0.006403632462024689 + 0.01 * 6.753549098968506
Epoch 690, val loss: 1.0680691003799438
Epoch 700, training loss: 0.07362633943557739 = 0.006130325607955456 + 0.01 * 6.7496018409729
Epoch 700, val loss: 1.0740785598754883
Epoch 710, training loss: 0.0733959749341011 = 0.0058764624409377575 + 0.01 * 6.751951694488525
Epoch 710, val loss: 1.0800563097000122
Epoch 720, training loss: 0.0733674019575119 = 0.005639839917421341 + 0.01 * 6.772757053375244
Epoch 720, val loss: 1.0859510898590088
Epoch 730, training loss: 0.07281820476055145 = 0.005419475957751274 + 0.01 * 6.739872932434082
Epoch 730, val loss: 1.091476559638977
Epoch 740, training loss: 0.07248343527317047 = 0.005213324446231127 + 0.01 * 6.727011203765869
Epoch 740, val loss: 1.096981167793274
Epoch 750, training loss: 0.07220311462879181 = 0.005019817501306534 + 0.01 * 6.718329906463623
Epoch 750, val loss: 1.1023576259613037
Epoch 760, training loss: 0.07211606204509735 = 0.004838272929191589 + 0.01 * 6.727779388427734
Epoch 760, val loss: 1.1075356006622314
Epoch 770, training loss: 0.071816086769104 = 0.0046681128442287445 + 0.01 * 6.714797496795654
Epoch 770, val loss: 1.1126478910446167
Epoch 780, training loss: 0.07156657427549362 = 0.004508216865360737 + 0.01 * 6.705835342407227
Epoch 780, val loss: 1.1176397800445557
Epoch 790, training loss: 0.07133498787879944 = 0.004357950296252966 + 0.01 * 6.697703838348389
Epoch 790, val loss: 1.1224696636199951
Epoch 800, training loss: 0.0712515339255333 = 0.004216189030557871 + 0.01 * 6.7035346031188965
Epoch 800, val loss: 1.1272720098495483
Epoch 810, training loss: 0.07100044935941696 = 0.004082389175891876 + 0.01 * 6.691806316375732
Epoch 810, val loss: 1.1317625045776367
Epoch 820, training loss: 0.07100366055965424 = 0.003956035245209932 + 0.01 * 6.704762935638428
Epoch 820, val loss: 1.1362946033477783
Epoch 830, training loss: 0.07075462490320206 = 0.0038367065135389566 + 0.01 * 6.691792011260986
Epoch 830, val loss: 1.14071524143219
Epoch 840, training loss: 0.07063507288694382 = 0.0037238721270114183 + 0.01 * 6.69111967086792
Epoch 840, val loss: 1.1449856758117676
Epoch 850, training loss: 0.07036784291267395 = 0.00361687527038157 + 0.01 * 6.6750969886779785
Epoch 850, val loss: 1.149274468421936
Epoch 860, training loss: 0.07011278718709946 = 0.003515487303957343 + 0.01 * 6.659729957580566
Epoch 860, val loss: 1.1533325910568237
Epoch 870, training loss: 0.07000774890184402 = 0.003419226501137018 + 0.01 * 6.658852577209473
Epoch 870, val loss: 1.157418131828308
Epoch 880, training loss: 0.06974560767412186 = 0.0033278593327850103 + 0.01 * 6.641774654388428
Epoch 880, val loss: 1.1614091396331787
Epoch 890, training loss: 0.07015237212181091 = 0.0032407536637037992 + 0.01 * 6.691161632537842
Epoch 890, val loss: 1.1652767658233643
Epoch 900, training loss: 0.06952278316020966 = 0.003158129286020994 + 0.01 * 6.636465549468994
Epoch 900, val loss: 1.169015884399414
Epoch 910, training loss: 0.0695568099617958 = 0.0030793941114097834 + 0.01 * 6.64774227142334
Epoch 910, val loss: 1.1726652383804321
Epoch 920, training loss: 0.06928412616252899 = 0.003004147671163082 + 0.01 * 6.627997875213623
Epoch 920, val loss: 1.176334023475647
Epoch 930, training loss: 0.06926321238279343 = 0.0029324127826839685 + 0.01 * 6.633080005645752
Epoch 930, val loss: 1.1798126697540283
Epoch 940, training loss: 0.06919237971305847 = 0.0028637016657739878 + 0.01 * 6.632867813110352
Epoch 940, val loss: 1.183343529701233
Epoch 950, training loss: 0.06889928132295609 = 0.0027981079183518887 + 0.01 * 6.610116958618164
Epoch 950, val loss: 1.1868135929107666
Epoch 960, training loss: 0.0688089057803154 = 0.0027353137265890837 + 0.01 * 6.607358932495117
Epoch 960, val loss: 1.1901495456695557
Epoch 970, training loss: 0.06879013776779175 = 0.0026751633267849684 + 0.01 * 6.61149787902832
Epoch 970, val loss: 1.193365454673767
Epoch 980, training loss: 0.06871815025806427 = 0.00261752400547266 + 0.01 * 6.610062599182129
Epoch 980, val loss: 1.1966251134872437
Epoch 990, training loss: 0.0685773715376854 = 0.002562484471127391 + 0.01 * 6.601489067077637
Epoch 990, val loss: 1.1996943950653076
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.80197, 0.13381, Accuracy:0.82099, 0.00761
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11566])
remove edge: torch.Size([2, 9514])
updated graph: torch.Size([2, 10524])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98155, 0.00522, Accuracy:0.83210, 0.00462
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.5 der2:0.4 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.025876998901367 = 1.9421391487121582 + 0.01 * 8.373793601989746
Epoch 0, val loss: 1.9412877559661865
Epoch 10, training loss: 2.016444444656372 = 1.9327073097229004 + 0.01 * 8.373722076416016
Epoch 10, val loss: 1.932500958442688
Epoch 20, training loss: 2.004953384399414 = 1.9212188720703125 + 0.01 * 8.373453140258789
Epoch 20, val loss: 1.9213322401046753
Epoch 30, training loss: 1.9886645078659058 = 1.9049365520477295 + 0.01 * 8.372800827026367
Epoch 30, val loss: 1.9051547050476074
Epoch 40, training loss: 1.9641615152359009 = 1.8804584741592407 + 0.01 * 8.370301246643066
Epoch 40, val loss: 1.8809822797775269
Epoch 50, training loss: 1.928468108177185 = 1.844947338104248 + 0.01 * 8.352082252502441
Epoch 50, val loss: 1.8470661640167236
Epoch 60, training loss: 1.883426308631897 = 1.8012800216674805 + 0.01 * 8.214631080627441
Epoch 60, val loss: 1.808754563331604
Epoch 70, training loss: 1.836712121963501 = 1.758751630783081 + 0.01 * 7.796054840087891
Epoch 70, val loss: 1.7742407321929932
Epoch 80, training loss: 1.782653570175171 = 1.708122730255127 + 0.01 * 7.453085899353027
Epoch 80, val loss: 1.7297120094299316
Epoch 90, training loss: 1.7097498178482056 = 1.6376899480819702 + 0.01 * 7.205986022949219
Epoch 90, val loss: 1.6681089401245117
Epoch 100, training loss: 1.6155203580856323 = 1.5446698665618896 + 0.01 * 7.085050106048584
Epoch 100, val loss: 1.589943528175354
Epoch 110, training loss: 1.504825234413147 = 1.4344439506530762 + 0.01 * 7.0381269454956055
Epoch 110, val loss: 1.4982331991195679
Epoch 120, training loss: 1.3896691799163818 = 1.3198037147521973 + 0.01 * 6.986545085906982
Epoch 120, val loss: 1.4062352180480957
Epoch 130, training loss: 1.2797242403030396 = 1.210387110710144 + 0.01 * 6.933712959289551
Epoch 130, val loss: 1.3206645250320435
Epoch 140, training loss: 1.181971549987793 = 1.113072156906128 + 0.01 * 6.889942646026611
Epoch 140, val loss: 1.2473818063735962
Epoch 150, training loss: 1.0985033512115479 = 1.0298744440078735 + 0.01 * 6.86289644241333
Epoch 150, val loss: 1.1873825788497925
Epoch 160, training loss: 1.0270940065383911 = 0.9586082100868225 + 0.01 * 6.848575115203857
Epoch 160, val loss: 1.1378014087677002
Epoch 170, training loss: 0.963204562664032 = 0.8948571681976318 + 0.01 * 6.8347392082214355
Epoch 170, val loss: 1.0944265127182007
Epoch 180, training loss: 0.9017114043235779 = 0.8334805965423584 + 0.01 * 6.82308292388916
Epoch 180, val loss: 1.0528321266174316
Epoch 190, training loss: 0.8383426666259766 = 0.7702115178108215 + 0.01 * 6.813117027282715
Epoch 190, val loss: 1.0091454982757568
Epoch 200, training loss: 0.7713196873664856 = 0.703269362449646 + 0.01 * 6.80503511428833
Epoch 200, val loss: 0.9614375233650208
Epoch 210, training loss: 0.7017473578453064 = 0.6337580680847168 + 0.01 * 6.79893159866333
Epoch 210, val loss: 0.9108084440231323
Epoch 220, training loss: 0.6328799724578857 = 0.5649523138999939 + 0.01 * 6.792764186859131
Epoch 220, val loss: 0.861071765422821
Epoch 230, training loss: 0.5677424669265747 = 0.49986082315444946 + 0.01 * 6.788161754608154
Epoch 230, val loss: 0.8160829544067383
Epoch 240, training loss: 0.5074739456176758 = 0.4396364390850067 + 0.01 * 6.783751487731934
Epoch 240, val loss: 0.7779245972633362
Epoch 250, training loss: 0.4515848755836487 = 0.38378724455833435 + 0.01 * 6.779763221740723
Epoch 250, val loss: 0.7464526295661926
Epoch 260, training loss: 0.399422287940979 = 0.33165591955184937 + 0.01 * 6.776637554168701
Epoch 260, val loss: 0.7202172875404358
Epoch 270, training loss: 0.35111039876937866 = 0.283365935087204 + 0.01 * 6.774446487426758
Epoch 270, val loss: 0.6980836391448975
Epoch 280, training loss: 0.30751532316207886 = 0.23978370428085327 + 0.01 * 6.773162364959717
Epoch 280, val loss: 0.6800585985183716
Epoch 290, training loss: 0.26961618661880493 = 0.20189699530601501 + 0.01 * 6.771918773651123
Epoch 290, val loss: 0.6669216156005859
Epoch 300, training loss: 0.23781661689281464 = 0.1701173037290573 + 0.01 * 6.769931316375732
Epoch 300, val loss: 0.6593067049980164
Epoch 310, training loss: 0.21178758144378662 = 0.14410769939422607 + 0.01 * 6.767988204956055
Epoch 310, val loss: 0.6571958661079407
Epoch 320, training loss: 0.190705344080925 = 0.12304417788982391 + 0.01 * 6.766117095947266
Epoch 320, val loss: 0.6600313186645508
Epoch 330, training loss: 0.17362543940544128 = 0.10597223043441772 + 0.01 * 6.765320777893066
Epoch 330, val loss: 0.6669468283653259
Epoch 340, training loss: 0.1596715897321701 = 0.09203914552927017 + 0.01 * 6.76324462890625
Epoch 340, val loss: 0.6769765019416809
Epoch 350, training loss: 0.14815381169319153 = 0.08054110407829285 + 0.01 * 6.761270523071289
Epoch 350, val loss: 0.689235270023346
Epoch 360, training loss: 0.13852939009666443 = 0.07094678282737732 + 0.01 * 6.7582597732543945
Epoch 360, val loss: 0.703018844127655
Epoch 370, training loss: 0.1304439753293991 = 0.0628623366355896 + 0.01 * 6.758163928985596
Epoch 370, val loss: 0.717788815498352
Epoch 380, training loss: 0.12353111803531647 = 0.055992983281612396 + 0.01 * 6.753813743591309
Epoch 380, val loss: 0.7330983281135559
Epoch 390, training loss: 0.11760903894901276 = 0.050109799951314926 + 0.01 * 6.749924182891846
Epoch 390, val loss: 0.748646080493927
Epoch 400, training loss: 0.11249911785125732 = 0.04504125192761421 + 0.01 * 6.745786666870117
Epoch 400, val loss: 0.7642266750335693
Epoch 410, training loss: 0.10810050368309021 = 0.04065539687871933 + 0.01 * 6.744511127471924
Epoch 410, val loss: 0.7796382904052734
Epoch 420, training loss: 0.10423245280981064 = 0.036846935749053955 + 0.01 * 6.738551616668701
Epoch 420, val loss: 0.7946868538856506
Epoch 430, training loss: 0.1009080782532692 = 0.03352656215429306 + 0.01 * 6.738151550292969
Epoch 430, val loss: 0.8093627095222473
Epoch 440, training loss: 0.09791867434978485 = 0.030620763078331947 + 0.01 * 6.729791641235352
Epoch 440, val loss: 0.823582649230957
Epoch 450, training loss: 0.09532430768013 = 0.02806655503809452 + 0.01 * 6.725775718688965
Epoch 450, val loss: 0.837372362613678
Epoch 460, training loss: 0.09324043989181519 = 0.025812184438109398 + 0.01 * 6.742825508117676
Epoch 460, val loss: 0.850642740726471
Epoch 470, training loss: 0.09096642583608627 = 0.023816371336579323 + 0.01 * 6.715005874633789
Epoch 470, val loss: 0.8633729815483093
Epoch 480, training loss: 0.08916804194450378 = 0.0220407173037529 + 0.01 * 6.712732791900635
Epoch 480, val loss: 0.8757179975509644
Epoch 490, training loss: 0.08752063661813736 = 0.02045426517724991 + 0.01 * 6.706637382507324
Epoch 490, val loss: 0.8876169919967651
Epoch 500, training loss: 0.08617036044597626 = 0.01903313770890236 + 0.01 * 6.713721752166748
Epoch 500, val loss: 0.899150013923645
Epoch 510, training loss: 0.0847710520029068 = 0.01775660552084446 + 0.01 * 6.701444625854492
Epoch 510, val loss: 0.9102048277854919
Epoch 520, training loss: 0.08352615684270859 = 0.016604581847786903 + 0.01 * 6.692157745361328
Epoch 520, val loss: 0.9209637641906738
Epoch 530, training loss: 0.08242167532444 = 0.015560816042125225 + 0.01 * 6.686086654663086
Epoch 530, val loss: 0.931354820728302
Epoch 540, training loss: 0.08167281746864319 = 0.01461237296462059 + 0.01 * 6.7060441970825195
Epoch 540, val loss: 0.9414933919906616
Epoch 550, training loss: 0.08067338913679123 = 0.013750425539910793 + 0.01 * 6.692296504974365
Epoch 550, val loss: 0.9512367248535156
Epoch 560, training loss: 0.07971296459436417 = 0.012964221648871899 + 0.01 * 6.674874782562256
Epoch 560, val loss: 0.9606516361236572
Epoch 570, training loss: 0.07894742488861084 = 0.012244767509400845 + 0.01 * 6.670266151428223
Epoch 570, val loss: 0.9698529243469238
Epoch 580, training loss: 0.07823996245861053 = 0.01158447191119194 + 0.01 * 6.665548801422119
Epoch 580, val loss: 0.9788246154785156
Epoch 590, training loss: 0.07758855819702148 = 0.010977081023156643 + 0.01 * 6.661147594451904
Epoch 590, val loss: 0.9875270128250122
Epoch 600, training loss: 0.07717843353748322 = 0.010417763143777847 + 0.01 * 6.67606782913208
Epoch 600, val loss: 0.9960080981254578
Epoch 610, training loss: 0.07647353410720825 = 0.00990247167646885 + 0.01 * 6.657106399536133
Epoch 610, val loss: 1.0042957067489624
Epoch 620, training loss: 0.07596825808286667 = 0.009426272474229336 + 0.01 * 6.65419864654541
Epoch 620, val loss: 1.0123075246810913
Epoch 630, training loss: 0.07547164708375931 = 0.008984881453216076 + 0.01 * 6.648676872253418
Epoch 630, val loss: 1.0201325416564941
Epoch 640, training loss: 0.07504047453403473 = 0.008574975654482841 + 0.01 * 6.646550178527832
Epoch 640, val loss: 1.0278114080429077
Epoch 650, training loss: 0.0746474415063858 = 0.00819409266114235 + 0.01 * 6.6453351974487305
Epoch 650, val loss: 1.0352367162704468
Epoch 660, training loss: 0.07423679530620575 = 0.007839642465114594 + 0.01 * 6.639715194702148
Epoch 660, val loss: 1.0424768924713135
Epoch 670, training loss: 0.07385360449552536 = 0.007509058807045221 + 0.01 * 6.634454250335693
Epoch 670, val loss: 1.0495779514312744
Epoch 680, training loss: 0.07349930703639984 = 0.007200338877737522 + 0.01 * 6.629897117614746
Epoch 680, val loss: 1.0565167665481567
Epoch 690, training loss: 0.07319677621126175 = 0.006911668460816145 + 0.01 * 6.628511428833008
Epoch 690, val loss: 1.0632842779159546
Epoch 700, training loss: 0.07294747978448868 = 0.0066411374136805534 + 0.01 * 6.630634307861328
Epoch 700, val loss: 1.0698795318603516
Epoch 710, training loss: 0.0726136714220047 = 0.006387396715581417 + 0.01 * 6.6226277351379395
Epoch 710, val loss: 1.0764200687408447
Epoch 720, training loss: 0.07240480184555054 = 0.006149020045995712 + 0.01 * 6.6255784034729
Epoch 720, val loss: 1.0827046632766724
Epoch 730, training loss: 0.07207462936639786 = 0.005924902856349945 + 0.01 * 6.6149725914001465
Epoch 730, val loss: 1.0889123678207397
Epoch 740, training loss: 0.07198815792798996 = 0.005713880993425846 + 0.01 * 6.627427577972412
Epoch 740, val loss: 1.0949221849441528
Epoch 750, training loss: 0.0716157928109169 = 0.005515245720744133 + 0.01 * 6.610054969787598
Epoch 750, val loss: 1.100903034210205
Epoch 760, training loss: 0.07138004153966904 = 0.0053278678096830845 + 0.01 * 6.605217456817627
Epoch 760, val loss: 1.1067193746566772
Epoch 770, training loss: 0.07129532098770142 = 0.0051507363095879555 + 0.01 * 6.6144585609436035
Epoch 770, val loss: 1.1124199628829956
Epoch 780, training loss: 0.07102705538272858 = 0.00498341117054224 + 0.01 * 6.604364395141602
Epoch 780, val loss: 1.1179476976394653
Epoch 790, training loss: 0.07079233229160309 = 0.004825160838663578 + 0.01 * 6.596717357635498
Epoch 790, val loss: 1.1234160661697388
Epoch 800, training loss: 0.07064838707447052 = 0.004675210453569889 + 0.01 * 6.597317695617676
Epoch 800, val loss: 1.128738284111023
Epoch 810, training loss: 0.07042387872934341 = 0.004533050581812859 + 0.01 * 6.589082717895508
Epoch 810, val loss: 1.1340110301971436
Epoch 820, training loss: 0.07032621651887894 = 0.004398156888782978 + 0.01 * 6.592805862426758
Epoch 820, val loss: 1.1391735076904297
Epoch 830, training loss: 0.07007815688848495 = 0.004269989673048258 + 0.01 * 6.580817222595215
Epoch 830, val loss: 1.1442172527313232
Epoch 840, training loss: 0.07012325525283813 = 0.004148060455918312 + 0.01 * 6.597519874572754
Epoch 840, val loss: 1.1491327285766602
Epoch 850, training loss: 0.0698757991194725 = 0.004032118711620569 + 0.01 * 6.5843682289123535
Epoch 850, val loss: 1.154069423675537
Epoch 860, training loss: 0.06969260424375534 = 0.00392185477539897 + 0.01 * 6.577075481414795
Epoch 860, val loss: 1.1587505340576172
Epoch 870, training loss: 0.06952524930238724 = 0.003817008575424552 + 0.01 * 6.570824146270752
Epoch 870, val loss: 1.1635209321975708
Epoch 880, training loss: 0.06941593438386917 = 0.0037169356364756823 + 0.01 * 6.569900035858154
Epoch 880, val loss: 1.1680914163589478
Epoch 890, training loss: 0.0692787915468216 = 0.0036213514395058155 + 0.01 * 6.565743923187256
Epoch 890, val loss: 1.1726740598678589
Epoch 900, training loss: 0.0691312626004219 = 0.003530045272782445 + 0.01 * 6.560121536254883
Epoch 900, val loss: 1.1770703792572021
Epoch 910, training loss: 0.0690576583147049 = 0.0034429451916366816 + 0.01 * 6.561471939086914
Epoch 910, val loss: 1.1814874410629272
Epoch 920, training loss: 0.06917345523834229 = 0.0033595901913940907 + 0.01 * 6.581386566162109
Epoch 920, val loss: 1.1857987642288208
Epoch 930, training loss: 0.06888075172901154 = 0.0032799714244902134 + 0.01 * 6.5600786209106445
Epoch 930, val loss: 1.1900570392608643
Epoch 940, training loss: 0.0687294453382492 = 0.003203804837539792 + 0.01 * 6.55256462097168
Epoch 940, val loss: 1.1941795349121094
Epoch 950, training loss: 0.06869031488895416 = 0.0031307763420045376 + 0.01 * 6.555953502655029
Epoch 950, val loss: 1.1982417106628418
Epoch 960, training loss: 0.06864123046398163 = 0.003060702234506607 + 0.01 * 6.558053016662598
Epoch 960, val loss: 1.2023042440414429
Epoch 970, training loss: 0.06843692064285278 = 0.002993507543578744 + 0.01 * 6.544341087341309
Epoch 970, val loss: 1.2062811851501465
Epoch 980, training loss: 0.06850435584783554 = 0.002929005306214094 + 0.01 * 6.557535648345947
Epoch 980, val loss: 1.210201621055603
Epoch 990, training loss: 0.06831937283277512 = 0.0028671256732195616 + 0.01 * 6.545224666595459
Epoch 990, val loss: 1.2138818502426147
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.6494
Flip ASR: 0.5956/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.030728578567505 = 1.9469913244247437 + 0.01 * 8.373724937438965
Epoch 0, val loss: 1.943419337272644
Epoch 10, training loss: 2.020345687866211 = 1.9366108179092407 + 0.01 * 8.373497009277344
Epoch 10, val loss: 1.9319422245025635
Epoch 20, training loss: 2.00722074508667 = 1.9234930276870728 + 0.01 * 8.372775077819824
Epoch 20, val loss: 1.916235327720642
Epoch 30, training loss: 1.988580346107483 = 1.9048707485198975 + 0.01 * 8.370963096618652
Epoch 30, val loss: 1.8931955099105835
Epoch 40, training loss: 1.9620903730392456 = 1.8784315586090088 + 0.01 * 8.365885734558105
Epoch 40, val loss: 1.8615286350250244
Epoch 50, training loss: 1.9265674352645874 = 1.8432841300964355 + 0.01 * 8.328332901000977
Epoch 50, val loss: 1.823060393333435
Epoch 60, training loss: 1.8796263933181763 = 1.8001378774642944 + 0.01 * 7.948851585388184
Epoch 60, val loss: 1.7808412313461304
Epoch 70, training loss: 1.8297151327133179 = 1.7540066242218018 + 0.01 * 7.5708513259887695
Epoch 70, val loss: 1.7402418851852417
Epoch 80, training loss: 1.776890516281128 = 1.702815294265747 + 0.01 * 7.407517910003662
Epoch 80, val loss: 1.6962928771972656
Epoch 90, training loss: 1.705419659614563 = 1.6328532695770264 + 0.01 * 7.256636142730713
Epoch 90, val loss: 1.6370489597320557
Epoch 100, training loss: 1.6108882427215576 = 1.5395764112472534 + 0.01 * 7.131185531616211
Epoch 100, val loss: 1.561919927597046
Epoch 110, training loss: 1.5018202066421509 = 1.4308468103408813 + 0.01 * 7.097338676452637
Epoch 110, val loss: 1.4769668579101562
Epoch 120, training loss: 1.3948043584823608 = 1.3242143392562866 + 0.01 * 7.05899715423584
Epoch 120, val loss: 1.3970091342926025
Epoch 130, training loss: 1.3009260892868042 = 1.2307531833648682 + 0.01 * 7.017289638519287
Epoch 130, val loss: 1.3309576511383057
Epoch 140, training loss: 1.2188830375671387 = 1.1490739583969116 + 0.01 * 6.9809136390686035
Epoch 140, val loss: 1.275868535041809
Epoch 150, training loss: 1.140335202217102 = 1.0707707405090332 + 0.01 * 6.9564409255981445
Epoch 150, val loss: 1.2203165292739868
Epoch 160, training loss: 1.0585731267929077 = 0.9892062544822693 + 0.01 * 6.9366888999938965
Epoch 160, val loss: 1.161175012588501
Epoch 170, training loss: 0.9724878072738647 = 0.9033306241035461 + 0.01 * 6.915718078613281
Epoch 170, val loss: 1.0985363721847534
Epoch 180, training loss: 0.8867149353027344 = 0.817768931388855 + 0.01 * 6.894601345062256
Epoch 180, val loss: 1.0363774299621582
Epoch 190, training loss: 0.8077011704444885 = 0.738928496837616 + 0.01 * 6.877269268035889
Epoch 190, val loss: 0.9802318215370178
Epoch 200, training loss: 0.7390832304954529 = 0.6704064011573792 + 0.01 * 6.867680549621582
Epoch 200, val loss: 0.9338469505310059
Epoch 210, training loss: 0.6801373958587646 = 0.6115168333053589 + 0.01 * 6.862053871154785
Epoch 210, val loss: 0.897512674331665
Epoch 220, training loss: 0.6281600594520569 = 0.559597909450531 + 0.01 * 6.856213569641113
Epoch 220, val loss: 0.8694726824760437
Epoch 230, training loss: 0.5808947086334229 = 0.5123929381370544 + 0.01 * 6.850177764892578
Epoch 230, val loss: 0.8477164506912231
Epoch 240, training loss: 0.5366999506950378 = 0.4682715833187103 + 0.01 * 6.842835426330566
Epoch 240, val loss: 0.8305059671401978
Epoch 250, training loss: 0.49488577246665955 = 0.42650339007377625 + 0.01 * 6.838237285614014
Epoch 250, val loss: 0.8169393539428711
Epoch 260, training loss: 0.45526453852653503 = 0.38698214292526245 + 0.01 * 6.828240394592285
Epoch 260, val loss: 0.8068636655807495
Epoch 270, training loss: 0.4177302122116089 = 0.349526047706604 + 0.01 * 6.8204169273376465
Epoch 270, val loss: 0.7998639345169067
Epoch 280, training loss: 0.38224682211875916 = 0.31414151191711426 + 0.01 * 6.810531139373779
Epoch 280, val loss: 0.7959109544754028
Epoch 290, training loss: 0.3489532768726349 = 0.28087693452835083 + 0.01 * 6.807634353637695
Epoch 290, val loss: 0.7948569059371948
Epoch 300, training loss: 0.3180733025074005 = 0.25010159611701965 + 0.01 * 6.7971696853637695
Epoch 300, val loss: 0.796768844127655
Epoch 310, training loss: 0.28982269763946533 = 0.22194398939609528 + 0.01 * 6.787869930267334
Epoch 310, val loss: 0.8019384145736694
Epoch 320, training loss: 0.26463091373443604 = 0.19685517251491547 + 0.01 * 6.77757453918457
Epoch 320, val loss: 0.8103742599487305
Epoch 330, training loss: 0.24242165684700012 = 0.1747305542230606 + 0.01 * 6.769111156463623
Epoch 330, val loss: 0.8217536807060242
Epoch 340, training loss: 0.22289222478866577 = 0.15519075095653534 + 0.01 * 6.770147800445557
Epoch 340, val loss: 0.8356302976608276
Epoch 350, training loss: 0.20550653338432312 = 0.13793540000915527 + 0.01 * 6.757112979888916
Epoch 350, val loss: 0.8516597151756287
Epoch 360, training loss: 0.1902632713317871 = 0.122706837952137 + 0.01 * 6.75564432144165
Epoch 360, val loss: 0.8691034913063049
Epoch 370, training loss: 0.17685548961162567 = 0.10942305624485016 + 0.01 * 6.74324369430542
Epoch 370, val loss: 0.8876470923423767
Epoch 380, training loss: 0.16519832611083984 = 0.09780748933553696 + 0.01 * 6.739084243774414
Epoch 380, val loss: 0.9068661332130432
Epoch 390, training loss: 0.15496671199798584 = 0.0876309871673584 + 0.01 * 6.733572483062744
Epoch 390, val loss: 0.9266689419746399
Epoch 400, training loss: 0.1460121124982834 = 0.07866852730512619 + 0.01 * 6.734358787536621
Epoch 400, val loss: 0.9468149542808533
Epoch 410, training loss: 0.13803645968437195 = 0.07075387984514236 + 0.01 * 6.7282586097717285
Epoch 410, val loss: 0.9671040773391724
Epoch 420, training loss: 0.13096168637275696 = 0.06373997032642365 + 0.01 * 6.722172737121582
Epoch 420, val loss: 0.9875953793525696
Epoch 430, training loss: 0.12467700988054276 = 0.05750267207622528 + 0.01 * 6.717433929443359
Epoch 430, val loss: 1.0080159902572632
Epoch 440, training loss: 0.1190958023071289 = 0.05194820091128349 + 0.01 * 6.714759826660156
Epoch 440, val loss: 1.028516411781311
Epoch 450, training loss: 0.11414965987205505 = 0.047001469880342484 + 0.01 * 6.714818477630615
Epoch 450, val loss: 1.0489447116851807
Epoch 460, training loss: 0.10970937460660934 = 0.042617738246917725 + 0.01 * 6.709164142608643
Epoch 460, val loss: 1.0689395666122437
Epoch 470, training loss: 0.105772003531456 = 0.03873194009065628 + 0.01 * 6.704006195068359
Epoch 470, val loss: 1.0886257886886597
Epoch 480, training loss: 0.10236446559429169 = 0.035296399146318436 + 0.01 * 6.706806659698486
Epoch 480, val loss: 1.107845664024353
Epoch 490, training loss: 0.09924279153347015 = 0.032257724553346634 + 0.01 * 6.698507308959961
Epoch 490, val loss: 1.1266170740127563
Epoch 500, training loss: 0.09657662361860275 = 0.02956225536763668 + 0.01 * 6.701436996459961
Epoch 500, val loss: 1.1447224617004395
Epoch 510, training loss: 0.0940924733877182 = 0.027164781466126442 + 0.01 * 6.692769527435303
Epoch 510, val loss: 1.1623655557632446
Epoch 520, training loss: 0.0919271856546402 = 0.02502833679318428 + 0.01 * 6.689884662628174
Epoch 520, val loss: 1.179424524307251
Epoch 530, training loss: 0.09001287817955017 = 0.023119814693927765 + 0.01 * 6.689306735992432
Epoch 530, val loss: 1.1959490776062012
Epoch 540, training loss: 0.08823800832033157 = 0.02141103707253933 + 0.01 * 6.68269681930542
Epoch 540, val loss: 1.2120362520217896
Epoch 550, training loss: 0.08667589724063873 = 0.01987731084227562 + 0.01 * 6.679859161376953
Epoch 550, val loss: 1.2275820970535278
Epoch 560, training loss: 0.08531011641025543 = 0.01849569007754326 + 0.01 * 6.681443214416504
Epoch 560, val loss: 1.2426698207855225
Epoch 570, training loss: 0.08402375131845474 = 0.017247214913368225 + 0.01 * 6.677653789520264
Epoch 570, val loss: 1.2573511600494385
Epoch 580, training loss: 0.08281117677688599 = 0.016118023544549942 + 0.01 * 6.669314861297607
Epoch 580, val loss: 1.2715661525726318
Epoch 590, training loss: 0.08193714171648026 = 0.015094744972884655 + 0.01 * 6.684240341186523
Epoch 590, val loss: 1.2853760719299316
Epoch 600, training loss: 0.08080451935529709 = 0.014166087843477726 + 0.01 * 6.663843631744385
Epoch 600, val loss: 1.298704743385315
Epoch 610, training loss: 0.07994195818901062 = 0.01332100760191679 + 0.01 * 6.662095546722412
Epoch 610, val loss: 1.3116955757141113
Epoch 620, training loss: 0.07912669330835342 = 0.012547471560537815 + 0.01 * 6.657922267913818
Epoch 620, val loss: 1.324350357055664
Epoch 630, training loss: 0.07838410139083862 = 0.011836904101073742 + 0.01 * 6.654720306396484
Epoch 630, val loss: 1.3366968631744385
Epoch 640, training loss: 0.0777072161436081 = 0.011184568516910076 + 0.01 * 6.6522650718688965
Epoch 640, val loss: 1.3487039804458618
Epoch 650, training loss: 0.07711000740528107 = 0.010584956966340542 + 0.01 * 6.652504920959473
Epoch 650, val loss: 1.3603178262710571
Epoch 660, training loss: 0.07647605985403061 = 0.010033026337623596 + 0.01 * 6.644303798675537
Epoch 660, val loss: 1.3716833591461182
Epoch 670, training loss: 0.07599322497844696 = 0.009523032233119011 + 0.01 * 6.647018909454346
Epoch 670, val loss: 1.3828517198562622
Epoch 680, training loss: 0.07545041292905807 = 0.009050970897078514 + 0.01 * 6.639944076538086
Epoch 680, val loss: 1.393581509590149
Epoch 690, training loss: 0.07496912777423859 = 0.00861315056681633 + 0.01 * 6.635598182678223
Epoch 690, val loss: 1.404114842414856
Epoch 700, training loss: 0.07458031177520752 = 0.008205489255487919 + 0.01 * 6.637482643127441
Epoch 700, val loss: 1.4143667221069336
Epoch 710, training loss: 0.07413046061992645 = 0.007827471010386944 + 0.01 * 6.6302995681762695
Epoch 710, val loss: 1.4243171215057373
Epoch 720, training loss: 0.07375185936689377 = 0.007475418038666248 + 0.01 * 6.627644062042236
Epoch 720, val loss: 1.4340540170669556
Epoch 730, training loss: 0.07343034446239471 = 0.00714700436219573 + 0.01 * 6.628333568572998
Epoch 730, val loss: 1.4435322284698486
Epoch 740, training loss: 0.07303733378648758 = 0.006840416230261326 + 0.01 * 6.619691848754883
Epoch 740, val loss: 1.4527791738510132
Epoch 750, training loss: 0.07308530062437057 = 0.00655364291742444 + 0.01 * 6.6531662940979
Epoch 750, val loss: 1.461881399154663
Epoch 760, training loss: 0.07241249084472656 = 0.006287156604230404 + 0.01 * 6.6125335693359375
Epoch 760, val loss: 1.4705712795257568
Epoch 770, training loss: 0.07217271625995636 = 0.006038800813257694 + 0.01 * 6.613391876220703
Epoch 770, val loss: 1.479006052017212
Epoch 780, training loss: 0.07188882678747177 = 0.005805718246847391 + 0.01 * 6.608311176300049
Epoch 780, val loss: 1.4873453378677368
Epoch 790, training loss: 0.07172203063964844 = 0.005586652085185051 + 0.01 * 6.613537788391113
Epoch 790, val loss: 1.495482325553894
Epoch 800, training loss: 0.07141733169555664 = 0.005380433052778244 + 0.01 * 6.603690147399902
Epoch 800, val loss: 1.503477931022644
Epoch 810, training loss: 0.07130113989114761 = 0.005186069291085005 + 0.01 * 6.611507415771484
Epoch 810, val loss: 1.511202335357666
Epoch 820, training loss: 0.07104729861021042 = 0.0050035626627504826 + 0.01 * 6.604373455047607
Epoch 820, val loss: 1.5188173055648804
Epoch 830, training loss: 0.07075946033000946 = 0.004831592086702585 + 0.01 * 6.592787265777588
Epoch 830, val loss: 1.5262089967727661
Epoch 840, training loss: 0.07063315063714981 = 0.004669143818318844 + 0.01 * 6.596400737762451
Epoch 840, val loss: 1.5334680080413818
Epoch 850, training loss: 0.07037108391523361 = 0.0045158471912145615 + 0.01 * 6.58552360534668
Epoch 850, val loss: 1.540479302406311
Epoch 860, training loss: 0.07024101912975311 = 0.004370896611362696 + 0.01 * 6.58701229095459
Epoch 860, val loss: 1.5473225116729736
Epoch 870, training loss: 0.07008018344640732 = 0.004233737476170063 + 0.01 * 6.584644317626953
Epoch 870, val loss: 1.554061770439148
Epoch 880, training loss: 0.06998587399721146 = 0.004103668034076691 + 0.01 * 6.588220596313477
Epoch 880, val loss: 1.5605872869491577
Epoch 890, training loss: 0.06967643648386002 = 0.003980458714067936 + 0.01 * 6.5695977210998535
Epoch 890, val loss: 1.5670217275619507
Epoch 900, training loss: 0.0696314200758934 = 0.003863520221784711 + 0.01 * 6.5767903327941895
Epoch 900, val loss: 1.5732955932617188
Epoch 910, training loss: 0.06946622580289841 = 0.0037524825893342495 + 0.01 * 6.57137393951416
Epoch 910, val loss: 1.5794239044189453
Epoch 920, training loss: 0.06945690512657166 = 0.003646835684776306 + 0.01 * 6.58100700378418
Epoch 920, val loss: 1.5852992534637451
Epoch 930, training loss: 0.06920593976974487 = 0.00354646984487772 + 0.01 * 6.56594705581665
Epoch 930, val loss: 1.5912213325500488
Epoch 940, training loss: 0.0691378116607666 = 0.0034508949611335993 + 0.01 * 6.568691730499268
Epoch 940, val loss: 1.596841812133789
Epoch 950, training loss: 0.06896571815013885 = 0.003359604626893997 + 0.01 * 6.560611248016357
Epoch 950, val loss: 1.602453589439392
Epoch 960, training loss: 0.06878115236759186 = 0.003272466594353318 + 0.01 * 6.550868511199951
Epoch 960, val loss: 1.6079516410827637
Epoch 970, training loss: 0.06888683140277863 = 0.0031891039106994867 + 0.01 * 6.569773197174072
Epoch 970, val loss: 1.613271951675415
Epoch 980, training loss: 0.06870834529399872 = 0.0031094553414732218 + 0.01 * 6.55988883972168
Epoch 980, val loss: 1.6184552907943726
Epoch 990, training loss: 0.06873559206724167 = 0.003033663146197796 + 0.01 * 6.570193290710449
Epoch 990, val loss: 1.6235477924346924
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7852
Overall ASR: 0.4317
Flip ASR: 0.3778/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0208778381347656 = 1.9371401071548462 + 0.01 * 8.373777389526367
Epoch 0, val loss: 1.9361801147460938
Epoch 10, training loss: 2.011157274246216 = 1.927420735359192 + 0.01 * 8.373647689819336
Epoch 10, val loss: 1.925909399986267
Epoch 20, training loss: 1.9993120431900024 = 1.9155791997909546 + 0.01 * 8.373285293579102
Epoch 20, val loss: 1.9131580591201782
Epoch 30, training loss: 1.9828481674194336 = 1.8991254568099976 + 0.01 * 8.372276306152344
Epoch 30, val loss: 1.8954336643218994
Epoch 40, training loss: 1.958665132522583 = 1.8749923706054688 + 0.01 * 8.367277145385742
Epoch 40, val loss: 1.8697969913482666
Epoch 50, training loss: 1.9240670204162598 = 1.8407570123672485 + 0.01 * 8.331001281738281
Epoch 50, val loss: 1.8347035646438599
Epoch 60, training loss: 1.879810094833374 = 1.7989795207977295 + 0.01 * 8.083056449890137
Epoch 60, val loss: 1.7950692176818848
Epoch 70, training loss: 1.832419991493225 = 1.7553516626358032 + 0.01 * 7.706836223602295
Epoch 70, val loss: 1.7568727731704712
Epoch 80, training loss: 1.7756743431091309 = 1.702155590057373 + 0.01 * 7.351876735687256
Epoch 80, val loss: 1.710774540901184
Epoch 90, training loss: 1.7016881704330444 = 1.630421757698059 + 0.01 * 7.126641750335693
Epoch 90, val loss: 1.649728775024414
Epoch 100, training loss: 1.610270619392395 = 1.5397508144378662 + 0.01 * 7.051977157592773
Epoch 100, val loss: 1.5740861892700195
Epoch 110, training loss: 1.5095144510269165 = 1.4392328262329102 + 0.01 * 7.028164863586426
Epoch 110, val loss: 1.4937069416046143
Epoch 120, training loss: 1.4076175689697266 = 1.3374786376953125 + 0.01 * 7.013892650604248
Epoch 120, val loss: 1.4182581901550293
Epoch 130, training loss: 1.308304786682129 = 1.2383092641830444 + 0.01 * 6.999552249908447
Epoch 130, val loss: 1.3485970497131348
Epoch 140, training loss: 1.2123595476150513 = 1.1425888538360596 + 0.01 * 6.977074146270752
Epoch 140, val loss: 1.2832447290420532
Epoch 150, training loss: 1.1210479736328125 = 1.0516077280044556 + 0.01 * 6.944025993347168
Epoch 150, val loss: 1.2212051153182983
Epoch 160, training loss: 1.035021185874939 = 0.9660288095474243 + 0.01 * 6.899236679077148
Epoch 160, val loss: 1.1621627807617188
Epoch 170, training loss: 0.9542474746704102 = 0.8856683969497681 + 0.01 * 6.857906341552734
Epoch 170, val loss: 1.1063023805618286
Epoch 180, training loss: 0.8781868815422058 = 0.8098347187042236 + 0.01 * 6.83521842956543
Epoch 180, val loss: 1.0535334348678589
Epoch 190, training loss: 0.8057641983032227 = 0.7375760674476624 + 0.01 * 6.818811416625977
Epoch 190, val loss: 1.0032715797424316
Epoch 200, training loss: 0.7361543774604797 = 0.6680640578269958 + 0.01 * 6.809029579162598
Epoch 200, val loss: 0.9558184742927551
Epoch 210, training loss: 0.6682785749435425 = 0.6002501845359802 + 0.01 * 6.802839756011963
Epoch 210, val loss: 0.9103933572769165
Epoch 220, training loss: 0.6016134023666382 = 0.5336377620697021 + 0.01 * 6.7975640296936035
Epoch 220, val loss: 0.8665260672569275
Epoch 230, training loss: 0.536643385887146 = 0.468740314245224 + 0.01 * 6.79030704498291
Epoch 230, val loss: 0.8244169354438782
Epoch 240, training loss: 0.4750874638557434 = 0.4071936011314392 + 0.01 * 6.7893853187561035
Epoch 240, val loss: 0.7861781120300293
Epoch 250, training loss: 0.4189497232437134 = 0.35113149881362915 + 0.01 * 6.781821250915527
Epoch 250, val loss: 0.7544481754302979
Epoch 260, training loss: 0.3699551224708557 = 0.3022398352622986 + 0.01 * 6.771528720855713
Epoch 260, val loss: 0.7310953736305237
Epoch 270, training loss: 0.3284844756126404 = 0.26078298687934875 + 0.01 * 6.770150184631348
Epoch 270, val loss: 0.7157251834869385
Epoch 280, training loss: 0.29370659589767456 = 0.22602324187755585 + 0.01 * 6.768336772918701
Epoch 280, val loss: 0.706993818283081
Epoch 290, training loss: 0.2643582820892334 = 0.19672079384326935 + 0.01 * 6.763750076293945
Epoch 290, val loss: 0.7030929327011108
Epoch 300, training loss: 0.23941585421562195 = 0.1718532294034958 + 0.01 * 6.75626277923584
Epoch 300, val loss: 0.7029266953468323
Epoch 310, training loss: 0.21835654973983765 = 0.1506669670343399 + 0.01 * 6.768957614898682
Epoch 310, val loss: 0.7054879665374756
Epoch 320, training loss: 0.20012015104293823 = 0.13257749378681183 + 0.01 * 6.754266738891602
Epoch 320, val loss: 0.7100757956504822
Epoch 330, training loss: 0.1845013052225113 = 0.11705097556114197 + 0.01 * 6.745032787322998
Epoch 330, val loss: 0.7161878943443298
Epoch 340, training loss: 0.17109888792037964 = 0.10368294268846512 + 0.01 * 6.74159574508667
Epoch 340, val loss: 0.723571240901947
Epoch 350, training loss: 0.15961439907550812 = 0.0921681672334671 + 0.01 * 6.74462366104126
Epoch 350, val loss: 0.7319434285163879
Epoch 360, training loss: 0.14959052205085754 = 0.08223951607942581 + 0.01 * 6.735100269317627
Epoch 360, val loss: 0.7410602569580078
Epoch 370, training loss: 0.14093074202537537 = 0.07364016771316528 + 0.01 * 6.729058265686035
Epoch 370, val loss: 0.7507529854774475
Epoch 380, training loss: 0.13341175019741058 = 0.06617018580436707 + 0.01 * 6.724156856536865
Epoch 380, val loss: 0.7609096169471741
Epoch 390, training loss: 0.12691733241081238 = 0.05966571345925331 + 0.01 * 6.725161552429199
Epoch 390, val loss: 0.7714131474494934
Epoch 400, training loss: 0.12109565734863281 = 0.053984884172677994 + 0.01 * 6.711077690124512
Epoch 400, val loss: 0.7821396589279175
Epoch 410, training loss: 0.11609230190515518 = 0.04900740832090378 + 0.01 * 6.708489418029785
Epoch 410, val loss: 0.7930001020431519
Epoch 420, training loss: 0.11162762343883514 = 0.044633470475673676 + 0.01 * 6.69941520690918
Epoch 420, val loss: 0.8038890361785889
Epoch 430, training loss: 0.10773171484470367 = 0.04077472165226936 + 0.01 * 6.695699691772461
Epoch 430, val loss: 0.8147743344306946
Epoch 440, training loss: 0.10429845750331879 = 0.03736063465476036 + 0.01 * 6.693781852722168
Epoch 440, val loss: 0.8255909085273743
Epoch 450, training loss: 0.10112495720386505 = 0.03432706370949745 + 0.01 * 6.6797895431518555
Epoch 450, val loss: 0.8363111019134521
Epoch 460, training loss: 0.09834995865821838 = 0.03162215277552605 + 0.01 * 6.672781467437744
Epoch 460, val loss: 0.8468325734138489
Epoch 470, training loss: 0.09597787261009216 = 0.029206441715359688 + 0.01 * 6.677143573760986
Epoch 470, val loss: 0.8572074770927429
Epoch 480, training loss: 0.09396744519472122 = 0.027041256427764893 + 0.01 * 6.6926188468933105
Epoch 480, val loss: 0.867408812046051
Epoch 490, training loss: 0.09165883809328079 = 0.025098321959376335 + 0.01 * 6.6560516357421875
Epoch 490, val loss: 0.8773940205574036
Epoch 500, training loss: 0.08994027972221375 = 0.023347776383161545 + 0.01 * 6.659249782562256
Epoch 500, val loss: 0.8871592283248901
Epoch 510, training loss: 0.08825716376304626 = 0.02176658995449543 + 0.01 * 6.649057865142822
Epoch 510, val loss: 0.8967504501342773
Epoch 520, training loss: 0.08686397224664688 = 0.02033437043428421 + 0.01 * 6.652960300445557
Epoch 520, val loss: 0.9061227440834045
Epoch 530, training loss: 0.08539034426212311 = 0.019034016877412796 + 0.01 * 6.635632514953613
Epoch 530, val loss: 0.9152801632881165
Epoch 540, training loss: 0.0842583030462265 = 0.01785026676952839 + 0.01 * 6.640803813934326
Epoch 540, val loss: 0.924179196357727
Epoch 550, training loss: 0.08306813985109329 = 0.016772031784057617 + 0.01 * 6.629611015319824
Epoch 550, val loss: 0.9328647255897522
Epoch 560, training loss: 0.0820017009973526 = 0.01578631065785885 + 0.01 * 6.6215386390686035
Epoch 560, val loss: 0.9413653612136841
Epoch 570, training loss: 0.08111875504255295 = 0.014883805997669697 + 0.01 * 6.623494625091553
Epoch 570, val loss: 0.94959956407547
Epoch 580, training loss: 0.08030443638563156 = 0.014056088402867317 + 0.01 * 6.624835014343262
Epoch 580, val loss: 0.957617998123169
Epoch 590, training loss: 0.07943089306354523 = 0.013295402750372887 + 0.01 * 6.613548755645752
Epoch 590, val loss: 0.9654456377029419
Epoch 600, training loss: 0.07884909957647324 = 0.012594987638294697 + 0.01 * 6.625411033630371
Epoch 600, val loss: 0.9730386137962341
Epoch 610, training loss: 0.07791958749294281 = 0.011948651634156704 + 0.01 * 6.59709358215332
Epoch 610, val loss: 0.9804549813270569
Epoch 620, training loss: 0.0773644894361496 = 0.011351269669830799 + 0.01 * 6.601321697235107
Epoch 620, val loss: 0.9876470565795898
Epoch 630, training loss: 0.07668344676494598 = 0.01079890038818121 + 0.01 * 6.588454723358154
Epoch 630, val loss: 0.9946718811988831
Epoch 640, training loss: 0.07625898718833923 = 0.010286365635693073 + 0.01 * 6.597262382507324
Epoch 640, val loss: 1.0015000104904175
Epoch 650, training loss: 0.07563190907239914 = 0.009810643270611763 + 0.01 * 6.582127094268799
Epoch 650, val loss: 1.0081759691238403
Epoch 660, training loss: 0.07525515556335449 = 0.009367912076413631 + 0.01 * 6.588724136352539
Epoch 660, val loss: 1.0146766901016235
Epoch 670, training loss: 0.0747666135430336 = 0.008955567143857479 + 0.01 * 6.581104755401611
Epoch 670, val loss: 1.0209929943084717
Epoch 680, training loss: 0.07444754242897034 = 0.008571762591600418 + 0.01 * 6.587577819824219
Epoch 680, val loss: 1.0270963907241821
Epoch 690, training loss: 0.07408306002616882 = 0.008213604800403118 + 0.01 * 6.5869460105896
Epoch 690, val loss: 1.0330783128738403
Epoch 700, training loss: 0.07363557070493698 = 0.007878369651734829 + 0.01 * 6.575720310211182
Epoch 700, val loss: 1.0388984680175781
Epoch 710, training loss: 0.07317650318145752 = 0.007564576342701912 + 0.01 * 6.561192989349365
Epoch 710, val loss: 1.0445568561553955
Epoch 720, training loss: 0.07309591770172119 = 0.007270356174558401 + 0.01 * 6.582555770874023
Epoch 720, val loss: 1.0501116514205933
Epoch 730, training loss: 0.07247069478034973 = 0.006994208786636591 + 0.01 * 6.547648906707764
Epoch 730, val loss: 1.0555287599563599
Epoch 740, training loss: 0.072344109416008 = 0.0067345802672207355 + 0.01 * 6.560953140258789
Epoch 740, val loss: 1.0607578754425049
Epoch 750, training loss: 0.07209978252649307 = 0.0064906952902674675 + 0.01 * 6.560908794403076
Epoch 750, val loss: 1.0659306049346924
Epoch 760, training loss: 0.07167074084281921 = 0.006260883528739214 + 0.01 * 6.540985584259033
Epoch 760, val loss: 1.0709314346313477
Epoch 770, training loss: 0.07131875306367874 = 0.00604418758302927 + 0.01 * 6.527456760406494
Epoch 770, val loss: 1.0758705139160156
Epoch 780, training loss: 0.07120571285486221 = 0.0058393944054841995 + 0.01 * 6.536632537841797
Epoch 780, val loss: 1.080617070198059
Epoch 790, training loss: 0.07094062119722366 = 0.005646352656185627 + 0.01 * 6.529426574707031
Epoch 790, val loss: 1.085317611694336
Epoch 800, training loss: 0.07066547870635986 = 0.0054639060981571674 + 0.01 * 6.520157337188721
Epoch 800, val loss: 1.089874505996704
Epoch 810, training loss: 0.0704776793718338 = 0.005291224457323551 + 0.01 * 6.518645763397217
Epoch 810, val loss: 1.0943487882614136
Epoch 820, training loss: 0.07056155055761337 = 0.005127364303916693 + 0.01 * 6.543418884277344
Epoch 820, val loss: 1.0986883640289307
Epoch 830, training loss: 0.07021763175725937 = 0.004972023889422417 + 0.01 * 6.524560451507568
Epoch 830, val loss: 1.102952241897583
Epoch 840, training loss: 0.06985154747962952 = 0.004824589937925339 + 0.01 * 6.5026960372924805
Epoch 840, val loss: 1.1070668697357178
Epoch 850, training loss: 0.06961049139499664 = 0.00468439981341362 + 0.01 * 6.492609977722168
Epoch 850, val loss: 1.1111384630203247
Epoch 860, training loss: 0.06969568133354187 = 0.004550684243440628 + 0.01 * 6.514500141143799
Epoch 860, val loss: 1.1150877475738525
Epoch 870, training loss: 0.0694039911031723 = 0.0044232867658138275 + 0.01 * 6.498071193695068
Epoch 870, val loss: 1.118984341621399
Epoch 880, training loss: 0.06936255097389221 = 0.004301444161683321 + 0.01 * 6.506110668182373
Epoch 880, val loss: 1.1227651834487915
Epoch 890, training loss: 0.06909944117069244 = 0.004185010213404894 + 0.01 * 6.491443157196045
Epoch 890, val loss: 1.1264615058898926
Epoch 900, training loss: 0.0688525140285492 = 0.0040735346265137196 + 0.01 * 6.477898120880127
Epoch 900, val loss: 1.1300777196884155
Epoch 910, training loss: 0.0686902403831482 = 0.003967287950217724 + 0.01 * 6.472295761108398
Epoch 910, val loss: 1.1336311101913452
Epoch 920, training loss: 0.06858889758586884 = 0.0038652056828141212 + 0.01 * 6.47236967086792
Epoch 920, val loss: 1.1371126174926758
Epoch 930, training loss: 0.06838362663984299 = 0.003767638001590967 + 0.01 * 6.461598873138428
Epoch 930, val loss: 1.140511155128479
Epoch 940, training loss: 0.06841979920864105 = 0.003674039151519537 + 0.01 * 6.474576473236084
Epoch 940, val loss: 1.1438519954681396
Epoch 950, training loss: 0.06832658499479294 = 0.003584287129342556 + 0.01 * 6.47422981262207
Epoch 950, val loss: 1.1471019983291626
Epoch 960, training loss: 0.06823211908340454 = 0.003498686710372567 + 0.01 * 6.4733428955078125
Epoch 960, val loss: 1.1503260135650635
Epoch 970, training loss: 0.06798264384269714 = 0.0034164644312113523 + 0.01 * 6.456617832183838
Epoch 970, val loss: 1.1534395217895508
Epoch 980, training loss: 0.06781939417123795 = 0.003337511559948325 + 0.01 * 6.448188781738281
Epoch 980, val loss: 1.1565220355987549
Epoch 990, training loss: 0.0679444670677185 = 0.0032616928219795227 + 0.01 * 6.468277454376221
Epoch 990, val loss: 1.159562349319458
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.8450
Flip ASR: 0.8133/225 nodes
The final ASR:0.64207, 0.16880, Accuracy:0.80741, 0.01571
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11652])
remove edge: torch.Size([2, 9440])
updated graph: torch.Size([2, 10536])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98278, 0.00696, Accuracy:0.83210, 0.00698
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.5 der2:0.4 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0423460006713867 = 1.9586094617843628 + 0.01 * 8.37365436553955
Epoch 0, val loss: 1.952825665473938
Epoch 10, training loss: 2.0317225456237793 = 1.9479873180389404 + 0.01 * 8.37353515625
Epoch 10, val loss: 1.942857265472412
Epoch 20, training loss: 2.019069194793701 = 1.9353382587432861 + 0.01 * 8.37308406829834
Epoch 20, val loss: 1.930543303489685
Epoch 30, training loss: 2.0017306804656982 = 1.9180123805999756 + 0.01 * 8.371819496154785
Epoch 30, val loss: 1.9134390354156494
Epoch 40, training loss: 1.9764653444290161 = 1.8928110599517822 + 0.01 * 8.365431785583496
Epoch 40, val loss: 1.8886680603027344
Epoch 50, training loss: 1.9401061534881592 = 1.85697340965271 + 0.01 * 8.313278198242188
Epoch 50, val loss: 1.8545414209365845
Epoch 60, training loss: 1.8918673992156982 = 1.8128738403320312 + 0.01 * 7.89935827255249
Epoch 60, val loss: 1.8157572746276855
Epoch 70, training loss: 1.8448470830917358 = 1.7708115577697754 + 0.01 * 7.403547763824463
Epoch 70, val loss: 1.7827471494674683
Epoch 80, training loss: 1.7975562810897827 = 1.7268922328948975 + 0.01 * 7.066404819488525
Epoch 80, val loss: 1.7466213703155518
Epoch 90, training loss: 1.736755132675171 = 1.6671297550201416 + 0.01 * 6.9625325202941895
Epoch 90, val loss: 1.6954840421676636
Epoch 100, training loss: 1.655861258506775 = 1.5866243839263916 + 0.01 * 6.923686504364014
Epoch 100, val loss: 1.6273375749588013
Epoch 110, training loss: 1.554315447807312 = 1.4854042530059814 + 0.01 * 6.891118049621582
Epoch 110, val loss: 1.5432835817337036
Epoch 120, training loss: 1.439760446548462 = 1.3710861206054688 + 0.01 * 6.867434501647949
Epoch 120, val loss: 1.4490476846694946
Epoch 130, training loss: 1.3200358152389526 = 1.251526117324829 + 0.01 * 6.850965976715088
Epoch 130, val loss: 1.3521292209625244
Epoch 140, training loss: 1.2013733386993408 = 1.133002519607544 + 0.01 * 6.837079048156738
Epoch 140, val loss: 1.2575613260269165
Epoch 150, training loss: 1.0887222290039062 = 1.0204970836639404 + 0.01 * 6.8225178718566895
Epoch 150, val loss: 1.1696879863739014
Epoch 160, training loss: 0.9859781265258789 = 0.917898416519165 + 0.01 * 6.8079729080200195
Epoch 160, val loss: 1.090753436088562
Epoch 170, training loss: 0.8948255181312561 = 0.826871395111084 + 0.01 * 6.795412540435791
Epoch 170, val loss: 1.0216163396835327
Epoch 180, training loss: 0.8148564696311951 = 0.7470033764839172 + 0.01 * 6.785309314727783
Epoch 180, val loss: 0.9625481367111206
Epoch 190, training loss: 0.7440012693405151 = 0.6762194633483887 + 0.01 * 6.778177738189697
Epoch 190, val loss: 0.9123974442481995
Epoch 200, training loss: 0.6793087124824524 = 0.6116217970848083 + 0.01 * 6.7686920166015625
Epoch 200, val loss: 0.869624674320221
Epoch 210, training loss: 0.6184617280960083 = 0.5508387088775635 + 0.01 * 6.762300491333008
Epoch 210, val loss: 0.8323778510093689
Epoch 220, training loss: 0.5602985620498657 = 0.4927610754966736 + 0.01 * 6.753747463226318
Epoch 220, val loss: 0.799727737903595
Epoch 230, training loss: 0.5046557188034058 = 0.43717893958091736 + 0.01 * 6.747679710388184
Epoch 230, val loss: 0.7715649604797363
Epoch 240, training loss: 0.4517444372177124 = 0.38433951139450073 + 0.01 * 6.7404937744140625
Epoch 240, val loss: 0.7483329772949219
Epoch 250, training loss: 0.4022423028945923 = 0.33488729596138 + 0.01 * 6.735499382019043
Epoch 250, val loss: 0.7303388714790344
Epoch 260, training loss: 0.35705825686454773 = 0.28973519802093506 + 0.01 * 6.73230504989624
Epoch 260, val loss: 0.717801570892334
Epoch 270, training loss: 0.31689053773880005 = 0.249640554189682 + 0.01 * 6.724997043609619
Epoch 270, val loss: 0.7102856636047363
Epoch 280, training loss: 0.2821624279022217 = 0.21493232250213623 + 0.01 * 6.723010063171387
Epoch 280, val loss: 0.7074751853942871
Epoch 290, training loss: 0.2525695562362671 = 0.18543344736099243 + 0.01 * 6.7136101722717285
Epoch 290, val loss: 0.7085238695144653
Epoch 300, training loss: 0.227833092212677 = 0.16064928472042084 + 0.01 * 6.718380928039551
Epoch 300, val loss: 0.7127980589866638
Epoch 310, training loss: 0.20698541402816772 = 0.1399294137954712 + 0.01 * 6.705600261688232
Epoch 310, val loss: 0.7196086049079895
Epoch 320, training loss: 0.18955884873867035 = 0.12256212532520294 + 0.01 * 6.699672222137451
Epoch 320, val loss: 0.7284273505210876
Epoch 330, training loss: 0.17485269904136658 = 0.10791235417127609 + 0.01 * 6.694035530090332
Epoch 330, val loss: 0.7387461066246033
Epoch 340, training loss: 0.16285058856010437 = 0.0954429879784584 + 0.01 * 6.7407612800598145
Epoch 340, val loss: 0.7502864003181458
Epoch 350, training loss: 0.15163946151733398 = 0.08475314825773239 + 0.01 * 6.688631057739258
Epoch 350, val loss: 0.7629427313804626
Epoch 360, training loss: 0.14232799410820007 = 0.07550312578678131 + 0.01 * 6.682487487792969
Epoch 360, val loss: 0.7765626907348633
Epoch 370, training loss: 0.13421815633773804 = 0.0674431174993515 + 0.01 * 6.67750358581543
Epoch 370, val loss: 0.7908905148506165
Epoch 380, training loss: 0.1272108107805252 = 0.060379449278116226 + 0.01 * 6.683136463165283
Epoch 380, val loss: 0.8059006929397583
Epoch 390, training loss: 0.12088516354560852 = 0.054185379296541214 + 0.01 * 6.669978618621826
Epoch 390, val loss: 0.8214129209518433
Epoch 400, training loss: 0.11557435989379883 = 0.04874425008893013 + 0.01 * 6.683011531829834
Epoch 400, val loss: 0.8372619152069092
Epoch 410, training loss: 0.11059966683387756 = 0.04396643862128258 + 0.01 * 6.663322448730469
Epoch 410, val loss: 0.8533071875572205
Epoch 420, training loss: 0.10645508766174316 = 0.039765629917383194 + 0.01 * 6.668946266174316
Epoch 420, val loss: 0.8693466782569885
Epoch 430, training loss: 0.10266097635030746 = 0.03607214242219925 + 0.01 * 6.658883571624756
Epoch 430, val loss: 0.8852675557136536
Epoch 440, training loss: 0.09927073866128922 = 0.03281796723604202 + 0.01 * 6.645277500152588
Epoch 440, val loss: 0.9009591341018677
Epoch 450, training loss: 0.09638258069753647 = 0.029944032430648804 + 0.01 * 6.643855094909668
Epoch 450, val loss: 0.9163384437561035
Epoch 460, training loss: 0.0938374325633049 = 0.0274044256657362 + 0.01 * 6.643300533294678
Epoch 460, val loss: 0.9313435554504395
Epoch 470, training loss: 0.09150824695825577 = 0.025157742202281952 + 0.01 * 6.635050296783447
Epoch 470, val loss: 0.9459908604621887
Epoch 480, training loss: 0.0895228385925293 = 0.023157838732004166 + 0.01 * 6.636500358581543
Epoch 480, val loss: 0.9602484703063965
Epoch 490, training loss: 0.08766880631446838 = 0.021374186500906944 + 0.01 * 6.629462242126465
Epoch 490, val loss: 0.97413170337677
Epoch 500, training loss: 0.08604598790407181 = 0.01977814920246601 + 0.01 * 6.626784324645996
Epoch 500, val loss: 0.9876487255096436
Epoch 510, training loss: 0.08454877883195877 = 0.018347615376114845 + 0.01 * 6.620116710662842
Epoch 510, val loss: 1.000765085220337
Epoch 520, training loss: 0.08329522609710693 = 0.017061669379472733 + 0.01 * 6.623355388641357
Epoch 520, val loss: 1.0135177373886108
Epoch 530, training loss: 0.08207283914089203 = 0.01590321585536003 + 0.01 * 6.616962909698486
Epoch 530, val loss: 1.0259822607040405
Epoch 540, training loss: 0.08113733679056168 = 0.014855997636914253 + 0.01 * 6.628134250640869
Epoch 540, val loss: 1.038091778755188
Epoch 550, training loss: 0.08005939424037933 = 0.013908715918660164 + 0.01 * 6.615068435668945
Epoch 550, val loss: 1.0498011112213135
Epoch 560, training loss: 0.0790875107049942 = 0.013048328459262848 + 0.01 * 6.603918075561523
Epoch 560, val loss: 1.061234474182129
Epoch 570, training loss: 0.07827994972467422 = 0.012265057303011417 + 0.01 * 6.601489543914795
Epoch 570, val loss: 1.0723252296447754
Epoch 580, training loss: 0.0776311382651329 = 0.011550615541636944 + 0.01 * 6.608052730560303
Epoch 580, val loss: 1.0831382274627686
Epoch 590, training loss: 0.07682982087135315 = 0.010897492058575153 + 0.01 * 6.59323263168335
Epoch 590, val loss: 1.0936930179595947
Epoch 600, training loss: 0.07623633742332458 = 0.010298807173967361 + 0.01 * 6.593753814697266
Epoch 600, val loss: 1.1039670705795288
Epoch 610, training loss: 0.07560833543539047 = 0.009749733842909336 + 0.01 * 6.585860729217529
Epoch 610, val loss: 1.1139408349990845
Epoch 620, training loss: 0.07510269433259964 = 0.009244883432984352 + 0.01 * 6.585781574249268
Epoch 620, val loss: 1.1236763000488281
Epoch 630, training loss: 0.07462708652019501 = 0.00877957884222269 + 0.01 * 6.584750652313232
Epoch 630, val loss: 1.133250117301941
Epoch 640, training loss: 0.07409754395484924 = 0.008350123651325703 + 0.01 * 6.574741840362549
Epoch 640, val loss: 1.1424963474273682
Epoch 650, training loss: 0.07384331524372101 = 0.007953017018735409 + 0.01 * 6.589029788970947
Epoch 650, val loss: 1.1515419483184814
Epoch 660, training loss: 0.07326940447092056 = 0.007585986517369747 + 0.01 * 6.568342208862305
Epoch 660, val loss: 1.1604089736938477
Epoch 670, training loss: 0.07304251194000244 = 0.007245372515171766 + 0.01 * 6.579714775085449
Epoch 670, val loss: 1.1690444946289062
Epoch 680, training loss: 0.07265418767929077 = 0.006929170340299606 + 0.01 * 6.572502613067627
Epoch 680, val loss: 1.1774736642837524
Epoch 690, training loss: 0.07238946855068207 = 0.006634799763560295 + 0.01 * 6.575467109680176
Epoch 690, val loss: 1.1856896877288818
Epoch 700, training loss: 0.07187460362911224 = 0.006360371131449938 + 0.01 * 6.551423072814941
Epoch 700, val loss: 1.1937589645385742
Epoch 710, training loss: 0.07150965929031372 = 0.0061040096916258335 + 0.01 * 6.540565013885498
Epoch 710, val loss: 1.2016379833221436
Epoch 720, training loss: 0.07175607234239578 = 0.005864349659532309 + 0.01 * 6.58917236328125
Epoch 720, val loss: 1.2092504501342773
Epoch 730, training loss: 0.07108636945486069 = 0.005640727002173662 + 0.01 * 6.5445637702941895
Epoch 730, val loss: 1.2167867422103882
Epoch 740, training loss: 0.0709429457783699 = 0.005431106314063072 + 0.01 * 6.551184177398682
Epoch 740, val loss: 1.2241624593734741
Epoch 750, training loss: 0.07048091292381287 = 0.005234423093497753 + 0.01 * 6.524649143218994
Epoch 750, val loss: 1.2313563823699951
Epoch 760, training loss: 0.07041521370410919 = 0.0050494070164859295 + 0.01 * 6.536581039428711
Epoch 760, val loss: 1.2384071350097656
Epoch 770, training loss: 0.07008074223995209 = 0.004875494632869959 + 0.01 * 6.520524978637695
Epoch 770, val loss: 1.2452991008758545
Epoch 780, training loss: 0.06989450007677078 = 0.004711701534688473 + 0.01 * 6.518280029296875
Epoch 780, val loss: 1.2519853115081787
Epoch 790, training loss: 0.06961845606565475 = 0.00455746753141284 + 0.01 * 6.506098747253418
Epoch 790, val loss: 1.2586833238601685
Epoch 800, training loss: 0.06959246098995209 = 0.004411696456372738 + 0.01 * 6.518076419830322
Epoch 800, val loss: 1.2650712728500366
Epoch 810, training loss: 0.0694156140089035 = 0.004274245351552963 + 0.01 * 6.5141377449035645
Epoch 810, val loss: 1.2714440822601318
Epoch 820, training loss: 0.06903627514839172 = 0.004144018981605768 + 0.01 * 6.4892258644104
Epoch 820, val loss: 1.2776504755020142
Epoch 830, training loss: 0.06895984709262848 = 0.004020891152322292 + 0.01 * 6.493896007537842
Epoch 830, val loss: 1.283643126487732
Epoch 840, training loss: 0.06898269057273865 = 0.0039043352007865906 + 0.01 * 6.507835865020752
Epoch 840, val loss: 1.2896215915679932
Epoch 850, training loss: 0.0686706155538559 = 0.00379368313588202 + 0.01 * 6.4876933097839355
Epoch 850, val loss: 1.2954994440078735
Epoch 860, training loss: 0.06845071911811829 = 0.003688742872327566 + 0.01 * 6.476197242736816
Epoch 860, val loss: 1.3012018203735352
Epoch 870, training loss: 0.06833921372890472 = 0.0035889344289898872 + 0.01 * 6.475027561187744
Epoch 870, val loss: 1.3068180084228516
Epoch 880, training loss: 0.06822595000267029 = 0.0034940119367092848 + 0.01 * 6.473194122314453
Epoch 880, val loss: 1.3123754262924194
Epoch 890, training loss: 0.0682951807975769 = 0.003403671085834503 + 0.01 * 6.489151477813721
Epoch 890, val loss: 1.3177539110183716
Epoch 900, training loss: 0.06805723905563354 = 0.0033176643773913383 + 0.01 * 6.4739580154418945
Epoch 900, val loss: 1.323060154914856
Epoch 910, training loss: 0.06782522797584534 = 0.0032356660813093185 + 0.01 * 6.458956241607666
Epoch 910, val loss: 1.3282768726348877
Epoch 920, training loss: 0.06769948452711105 = 0.003157267114147544 + 0.01 * 6.454222202301025
Epoch 920, val loss: 1.3333674669265747
Epoch 930, training loss: 0.06764119863510132 = 0.0030826705042272806 + 0.01 * 6.45585298538208
Epoch 930, val loss: 1.338494062423706
Epoch 940, training loss: 0.06737107038497925 = 0.003011209424585104 + 0.01 * 6.435986042022705
Epoch 940, val loss: 1.343450665473938
Epoch 950, training loss: 0.06732965260744095 = 0.0029428687412291765 + 0.01 * 6.438678741455078
Epoch 950, val loss: 1.348191738128662
Epoch 960, training loss: 0.06739502400159836 = 0.0028776158578693867 + 0.01 * 6.4517412185668945
Epoch 960, val loss: 1.3529527187347412
Epoch 970, training loss: 0.06702972203493118 = 0.002814935753121972 + 0.01 * 6.421479225158691
Epoch 970, val loss: 1.3576844930648804
Epoch 980, training loss: 0.06695196032524109 = 0.0027549301739782095 + 0.01 * 6.419703483581543
Epoch 980, val loss: 1.3622910976409912
Epoch 990, training loss: 0.06680598855018616 = 0.002697293646633625 + 0.01 * 6.410869121551514
Epoch 990, val loss: 1.3667171001434326
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.5867
Flip ASR: 0.5067/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0406746864318848 = 1.956937313079834 + 0.01 * 8.373745918273926
Epoch 0, val loss: 1.9487388134002686
Epoch 10, training loss: 2.0298702716827393 = 1.9461338520050049 + 0.01 * 8.373648643493652
Epoch 10, val loss: 1.93748939037323
Epoch 20, training loss: 2.0170187950134277 = 1.9332857131958008 + 0.01 * 8.373309135437012
Epoch 20, val loss: 1.9241816997528076
Epoch 30, training loss: 1.9994126558303833 = 1.91568922996521 + 0.01 * 8.37234115600586
Epoch 30, val loss: 1.9060918092727661
Epoch 40, training loss: 1.9734848737716675 = 1.889805793762207 + 0.01 * 8.36790657043457
Epoch 40, val loss: 1.879968285560608
Epoch 50, training loss: 1.9359248876571655 = 1.852527141571045 + 0.01 * 8.339771270751953
Epoch 50, val loss: 1.8441717624664307
Epoch 60, training loss: 1.8884367942810059 = 1.8066483736038208 + 0.01 * 8.178838729858398
Epoch 60, val loss: 1.8039699792861938
Epoch 70, training loss: 1.840672254562378 = 1.763054370880127 + 0.01 * 7.7617926597595215
Epoch 70, val loss: 1.7694729566574097
Epoch 80, training loss: 1.7891110181808472 = 1.7155555486679077 + 0.01 * 7.3555450439453125
Epoch 80, val loss: 1.7285635471343994
Epoch 90, training loss: 1.7210627794265747 = 1.6498034000396729 + 0.01 * 7.125937461853027
Epoch 90, val loss: 1.6705353260040283
Epoch 100, training loss: 1.6343144178390503 = 1.564058780670166 + 0.01 * 7.0255632400512695
Epoch 100, val loss: 1.5976699590682983
Epoch 110, training loss: 1.5327937602996826 = 1.4629194736480713 + 0.01 * 6.9874267578125
Epoch 110, val loss: 1.5143145322799683
Epoch 120, training loss: 1.4275131225585938 = 1.3579181432724 + 0.01 * 6.959502220153809
Epoch 120, val loss: 1.4290200471878052
Epoch 130, training loss: 1.3229953050613403 = 1.2536526918411255 + 0.01 * 6.9342570304870605
Epoch 130, val loss: 1.3460948467254639
Epoch 140, training loss: 1.2189059257507324 = 1.1497502326965332 + 0.01 * 6.915564060211182
Epoch 140, val loss: 1.2651947736740112
Epoch 150, training loss: 1.1155924797058105 = 1.046571135520935 + 0.01 * 6.902134418487549
Epoch 150, val loss: 1.185516595840454
Epoch 160, training loss: 1.0153391361236572 = 0.9464215040206909 + 0.01 * 6.891759395599365
Epoch 160, val loss: 1.1089736223220825
Epoch 170, training loss: 0.9205594062805176 = 0.851726770401001 + 0.01 * 6.883264064788818
Epoch 170, val loss: 1.0369502305984497
Epoch 180, training loss: 0.8327721357345581 = 0.7640079259872437 + 0.01 * 6.876421928405762
Epoch 180, val loss: 0.9701671600341797
Epoch 190, training loss: 0.753224790096283 = 0.6845218539237976 + 0.01 * 6.87029504776001
Epoch 190, val loss: 0.9099568724632263
Epoch 200, training loss: 0.6825740337371826 = 0.6139296293258667 + 0.01 * 6.864441871643066
Epoch 200, val loss: 0.8575273752212524
Epoch 210, training loss: 0.6205107569694519 = 0.5519277453422546 + 0.01 * 6.858303070068359
Epoch 210, val loss: 0.8138039708137512
Epoch 220, training loss: 0.5659271478652954 = 0.49741193652153015 + 0.01 * 6.8515238761901855
Epoch 220, val loss: 0.7784581184387207
Epoch 230, training loss: 0.5175415873527527 = 0.4490964412689209 + 0.01 * 6.844517230987549
Epoch 230, val loss: 0.750775933265686
Epoch 240, training loss: 0.47419917583465576 = 0.4058353304862976 + 0.01 * 6.836383819580078
Epoch 240, val loss: 0.7292554378509521
Epoch 250, training loss: 0.43515583872795105 = 0.36684972047805786 + 0.01 * 6.830611705780029
Epoch 250, val loss: 0.7131050229072571
Epoch 260, training loss: 0.3996097445487976 = 0.3314107060432434 + 0.01 * 6.8199052810668945
Epoch 260, val loss: 0.701383113861084
Epoch 270, training loss: 0.3670344352722168 = 0.2989289164543152 + 0.01 * 6.810552597045898
Epoch 270, val loss: 0.693195104598999
Epoch 280, training loss: 0.3369620144367218 = 0.268945574760437 + 0.01 * 6.8016438484191895
Epoch 280, val loss: 0.6875416040420532
Epoch 290, training loss: 0.3087068200111389 = 0.24077105522155762 + 0.01 * 6.793575286865234
Epoch 290, val loss: 0.6838139295578003
Epoch 300, training loss: 0.281723290681839 = 0.21385689079761505 + 0.01 * 6.786640644073486
Epoch 300, val loss: 0.6817682981491089
Epoch 310, training loss: 0.2560647130012512 = 0.1882558912038803 + 0.01 * 6.7808837890625
Epoch 310, val loss: 0.6809728741645813
Epoch 320, training loss: 0.23259598016738892 = 0.1647733449935913 + 0.01 * 6.782262802124023
Epoch 320, val loss: 0.6814439296722412
Epoch 330, training loss: 0.2119351178407669 = 0.14421550929546356 + 0.01 * 6.771961212158203
Epoch 330, val loss: 0.6841198801994324
Epoch 340, training loss: 0.1941295564174652 = 0.126480370759964 + 0.01 * 6.764919281005859
Epoch 340, val loss: 0.6893591284751892
Epoch 350, training loss: 0.17898571491241455 = 0.11136871576309204 + 0.01 * 6.761700630187988
Epoch 350, val loss: 0.696649968624115
Epoch 360, training loss: 0.16622978448867798 = 0.09862297773361206 + 0.01 * 6.760680198669434
Epoch 360, val loss: 0.7055615186691284
Epoch 370, training loss: 0.15541599690914154 = 0.08787728101015091 + 0.01 * 6.753871917724609
Epoch 370, val loss: 0.7157990336418152
Epoch 380, training loss: 0.14628425240516663 = 0.07875717431306839 + 0.01 * 6.752708911895752
Epoch 380, val loss: 0.7270204424858093
Epoch 390, training loss: 0.1384258270263672 = 0.07094019651412964 + 0.01 * 6.748562335968018
Epoch 390, val loss: 0.7388589978218079
Epoch 400, training loss: 0.13168540596961975 = 0.06418625265359879 + 0.01 * 6.749914646148682
Epoch 400, val loss: 0.7511489987373352
Epoch 410, training loss: 0.12573355436325073 = 0.05830478295683861 + 0.01 * 6.74287748336792
Epoch 410, val loss: 0.7635979056358337
Epoch 420, training loss: 0.12054373323917389 = 0.05314261466264725 + 0.01 * 6.7401123046875
Epoch 420, val loss: 0.7760707139968872
Epoch 430, training loss: 0.11593488603830338 = 0.04856123775243759 + 0.01 * 6.737365245819092
Epoch 430, val loss: 0.7883142232894897
Epoch 440, training loss: 0.1118367463350296 = 0.044467706233263016 + 0.01 * 6.736904621124268
Epoch 440, val loss: 0.8004626035690308
Epoch 450, training loss: 0.10814454406499863 = 0.040777094662189484 + 0.01 * 6.736745357513428
Epoch 450, val loss: 0.8123300075531006
Epoch 460, training loss: 0.10470985621213913 = 0.03742114454507828 + 0.01 * 6.7288713455200195
Epoch 460, val loss: 0.8240326046943665
Epoch 470, training loss: 0.10161848366260529 = 0.03435741364955902 + 0.01 * 6.726107120513916
Epoch 470, val loss: 0.8354358077049255
Epoch 480, training loss: 0.09883320331573486 = 0.03154609724879265 + 0.01 * 6.728710174560547
Epoch 480, val loss: 0.8466864228248596
Epoch 490, training loss: 0.09618142992258072 = 0.02895568124949932 + 0.01 * 6.722574710845947
Epoch 490, val loss: 0.8575208783149719
Epoch 500, training loss: 0.09373284876346588 = 0.026553677394986153 + 0.01 * 6.717917442321777
Epoch 500, val loss: 0.8683022856712341
Epoch 510, training loss: 0.09148728102445602 = 0.02434208244085312 + 0.01 * 6.71451997756958
Epoch 510, val loss: 0.8789044618606567
Epoch 520, training loss: 0.0896420031785965 = 0.022345419973134995 + 0.01 * 6.729659080505371
Epoch 520, val loss: 0.8889498710632324
Epoch 530, training loss: 0.08766809105873108 = 0.020560501143336296 + 0.01 * 6.710758686065674
Epoch 530, val loss: 0.898933470249176
Epoch 540, training loss: 0.08604776859283447 = 0.018984023481607437 + 0.01 * 6.706374168395996
Epoch 540, val loss: 0.9084871411323547
Epoch 550, training loss: 0.0846097320318222 = 0.017594588920474052 + 0.01 * 6.70151424407959
Epoch 550, val loss: 0.9178460240364075
Epoch 560, training loss: 0.08336500078439713 = 0.016366327181458473 + 0.01 * 6.6998677253723145
Epoch 560, val loss: 0.9269963502883911
Epoch 570, training loss: 0.08229000866413116 = 0.01527494564652443 + 0.01 * 6.701506614685059
Epoch 570, val loss: 0.9358733296394348
Epoch 580, training loss: 0.08123823255300522 = 0.014299512840807438 + 0.01 * 6.693871974945068
Epoch 580, val loss: 0.9445456862449646
Epoch 590, training loss: 0.08033488690853119 = 0.0134230712428689 + 0.01 * 6.6911821365356445
Epoch 590, val loss: 0.953007698059082
Epoch 600, training loss: 0.07952922582626343 = 0.012633002363145351 + 0.01 * 6.689622402191162
Epoch 600, val loss: 0.9613077640533447
Epoch 610, training loss: 0.07876758277416229 = 0.011915368027985096 + 0.01 * 6.685221195220947
Epoch 610, val loss: 0.9692808985710144
Epoch 620, training loss: 0.07806863635778427 = 0.011260434985160828 + 0.01 * 6.680819988250732
Epoch 620, val loss: 0.9771159887313843
Epoch 630, training loss: 0.07738589495420456 = 0.010662361048161983 + 0.01 * 6.672353267669678
Epoch 630, val loss: 0.9846948981285095
Epoch 640, training loss: 0.0770728811621666 = 0.0101155461743474 + 0.01 * 6.695733070373535
Epoch 640, val loss: 0.9922134876251221
Epoch 650, training loss: 0.07628778368234634 = 0.00961466133594513 + 0.01 * 6.6673126220703125
Epoch 650, val loss: 0.9993637800216675
Epoch 660, training loss: 0.07573547959327698 = 0.009154011495411396 + 0.01 * 6.658146858215332
Epoch 660, val loss: 1.0064984560012817
Epoch 670, training loss: 0.07533836364746094 = 0.008728384971618652 + 0.01 * 6.660998344421387
Epoch 670, val loss: 1.0134468078613281
Epoch 680, training loss: 0.07483523339033127 = 0.008334330283105373 + 0.01 * 6.650090217590332
Epoch 680, val loss: 1.020187258720398
Epoch 690, training loss: 0.0745985209941864 = 0.00796886719763279 + 0.01 * 6.662965774536133
Epoch 690, val loss: 1.0267984867095947
Epoch 700, training loss: 0.07415158301591873 = 0.007629838306456804 + 0.01 * 6.65217399597168
Epoch 700, val loss: 1.0331487655639648
Epoch 710, training loss: 0.07365034520626068 = 0.007314909249544144 + 0.01 * 6.633543014526367
Epoch 710, val loss: 1.0394538640975952
Epoch 720, training loss: 0.07353947311639786 = 0.007021640427410603 + 0.01 * 6.651783466339111
Epoch 720, val loss: 1.045605182647705
Epoch 730, training loss: 0.07312675565481186 = 0.00674790283665061 + 0.01 * 6.637885570526123
Epoch 730, val loss: 1.0515239238739014
Epoch 740, training loss: 0.07262954115867615 = 0.006492330692708492 + 0.01 * 6.6137213706970215
Epoch 740, val loss: 1.057221531867981
Epoch 750, training loss: 0.07243435084819794 = 0.006253154017031193 + 0.01 * 6.618119716644287
Epoch 750, val loss: 1.0629278421401978
Epoch 760, training loss: 0.07235745340585709 = 0.006028407718986273 + 0.01 * 6.632905006408691
Epoch 760, val loss: 1.0684632062911987
Epoch 770, training loss: 0.07189582288265228 = 0.0058176168240606785 + 0.01 * 6.607820987701416
Epoch 770, val loss: 1.0737950801849365
Epoch 780, training loss: 0.07166787981987 = 0.0056190514005720615 + 0.01 * 6.604883193969727
Epoch 780, val loss: 1.0790901184082031
Epoch 790, training loss: 0.07121697813272476 = 0.005430535413324833 + 0.01 * 6.578644752502441
Epoch 790, val loss: 1.084203839302063
Epoch 800, training loss: 0.07104016095399857 = 0.005252792965620756 + 0.01 * 6.578737258911133
Epoch 800, val loss: 1.0891454219818115
Epoch 810, training loss: 0.07120050489902496 = 0.005084414500743151 + 0.01 * 6.611608982086182
Epoch 810, val loss: 1.0940614938735962
Epoch 820, training loss: 0.07058966159820557 = 0.004926156718283892 + 0.01 * 6.566349983215332
Epoch 820, val loss: 1.0987842082977295
Epoch 830, training loss: 0.0704103484749794 = 0.004776389803737402 + 0.01 * 6.563396453857422
Epoch 830, val loss: 1.1033833026885986
Epoch 840, training loss: 0.07016535103321075 = 0.004634927958250046 + 0.01 * 6.553042411804199
Epoch 840, val loss: 1.1080559492111206
Epoch 850, training loss: 0.0703362450003624 = 0.00450083427131176 + 0.01 * 6.583540916442871
Epoch 850, val loss: 1.112457036972046
Epoch 860, training loss: 0.06990946829319 = 0.004373656585812569 + 0.01 * 6.553581714630127
Epoch 860, val loss: 1.116802453994751
Epoch 870, training loss: 0.06992656737565994 = 0.004253103397786617 + 0.01 * 6.567346096038818
Epoch 870, val loss: 1.12098228931427
Epoch 880, training loss: 0.06946498155593872 = 0.004138096235692501 + 0.01 * 6.532688617706299
Epoch 880, val loss: 1.1251713037490845
Epoch 890, training loss: 0.06927978247404099 = 0.004027884919196367 + 0.01 * 6.525189399719238
Epoch 890, val loss: 1.1292028427124023
Epoch 900, training loss: 0.06906697154045105 = 0.003922805655747652 + 0.01 * 6.514416694641113
Epoch 900, val loss: 1.1332951784133911
Epoch 910, training loss: 0.06904420256614685 = 0.0038228239864110947 + 0.01 * 6.522137641906738
Epoch 910, val loss: 1.1371465921401978
Epoch 920, training loss: 0.06877517700195312 = 0.003727652132511139 + 0.01 * 6.5047526359558105
Epoch 920, val loss: 1.1411032676696777
Epoch 930, training loss: 0.06886813044548035 = 0.003636948298662901 + 0.01 * 6.523118495941162
Epoch 930, val loss: 1.1448299884796143
Epoch 940, training loss: 0.06887595355510712 = 0.003550315275788307 + 0.01 * 6.532564163208008
Epoch 940, val loss: 1.1485525369644165
Epoch 950, training loss: 0.06853032112121582 = 0.0034669977612793446 + 0.01 * 6.5063323974609375
Epoch 950, val loss: 1.1520795822143555
Epoch 960, training loss: 0.06829916685819626 = 0.003387533826753497 + 0.01 * 6.49116325378418
Epoch 960, val loss: 1.1556464433670044
Epoch 970, training loss: 0.0682932436466217 = 0.0033114757388830185 + 0.01 * 6.498176574707031
Epoch 970, val loss: 1.1591100692749023
Epoch 980, training loss: 0.06817297637462616 = 0.0032384272199124098 + 0.01 * 6.493455410003662
Epoch 980, val loss: 1.1625175476074219
Epoch 990, training loss: 0.06791970133781433 = 0.003168523544445634 + 0.01 * 6.4751176834106445
Epoch 990, val loss: 1.1657809019088745
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.6900
Flip ASR: 0.6444/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.014392614364624 = 1.930655837059021 + 0.01 * 8.373686790466309
Epoch 0, val loss: 1.9364356994628906
Epoch 10, training loss: 2.004800319671631 = 1.9210649728775024 + 0.01 * 8.373534202575684
Epoch 10, val loss: 1.9262824058532715
Epoch 20, training loss: 1.9930787086486816 = 1.9093482494354248 + 0.01 * 8.373042106628418
Epoch 20, val loss: 1.9138330221176147
Epoch 30, training loss: 1.9768216609954834 = 1.893107533454895 + 0.01 * 8.371417999267578
Epoch 30, val loss: 1.8965309858322144
Epoch 40, training loss: 1.952945590019226 = 1.8693270683288574 + 0.01 * 8.361857414245605
Epoch 40, val loss: 1.871468186378479
Epoch 50, training loss: 1.918527364730835 = 1.8357253074645996 + 0.01 * 8.280203819274902
Epoch 50, val loss: 1.8369532823562622
Epoch 60, training loss: 1.8715193271636963 = 1.7941957712173462 + 0.01 * 7.732352256774902
Epoch 60, val loss: 1.7962794303894043
Epoch 70, training loss: 1.8225198984146118 = 1.7493795156478882 + 0.01 * 7.314037322998047
Epoch 70, val loss: 1.7538299560546875
Epoch 80, training loss: 1.7653979063034058 = 1.694743275642395 + 0.01 * 7.065463066101074
Epoch 80, val loss: 1.7032591104507446
Epoch 90, training loss: 1.69010591506958 = 1.6204606294631958 + 0.01 * 6.964524269104004
Epoch 90, val loss: 1.6386536359786987
Epoch 100, training loss: 1.5946950912475586 = 1.525329828262329 + 0.01 * 6.9365315437316895
Epoch 100, val loss: 1.5608073472976685
Epoch 110, training loss: 1.4856137037277222 = 1.416465401649475 + 0.01 * 6.914834499359131
Epoch 110, val loss: 1.4737917184829712
Epoch 120, training loss: 1.3740308284759521 = 1.3050347566604614 + 0.01 * 6.899612903594971
Epoch 120, val loss: 1.3879939317703247
Epoch 130, training loss: 1.2661182880401611 = 1.1972758769989014 + 0.01 * 6.884243488311768
Epoch 130, val loss: 1.3065497875213623
Epoch 140, training loss: 1.16292142868042 = 1.0942318439483643 + 0.01 * 6.868954658508301
Epoch 140, val loss: 1.2289634943008423
Epoch 150, training loss: 1.0634164810180664 = 0.9948698282241821 + 0.01 * 6.854666709899902
Epoch 150, val loss: 1.1538299322128296
Epoch 160, training loss: 0.9679343700408936 = 0.8995018601417542 + 0.01 * 6.843250274658203
Epoch 160, val loss: 1.082046389579773
Epoch 170, training loss: 0.8783686757087708 = 0.8100306391716003 + 0.01 * 6.833803176879883
Epoch 170, val loss: 1.01521635055542
Epoch 180, training loss: 0.7965837121009827 = 0.7283446192741394 + 0.01 * 6.823907852172852
Epoch 180, val loss: 0.9548165798187256
Epoch 190, training loss: 0.723173201084137 = 0.655028760433197 + 0.01 * 6.814445972442627
Epoch 190, val loss: 0.9020481705665588
Epoch 200, training loss: 0.6566407084465027 = 0.5886117219924927 + 0.01 * 6.802899360656738
Epoch 200, val loss: 0.8560381531715393
Epoch 210, training loss: 0.5944210886955261 = 0.5264966487884521 + 0.01 * 6.792446613311768
Epoch 210, val loss: 0.8151695132255554
Epoch 220, training loss: 0.5345612168312073 = 0.466688334941864 + 0.01 * 6.787285804748535
Epoch 220, val loss: 0.7779426574707031
Epoch 230, training loss: 0.476627379655838 = 0.4088895618915558 + 0.01 * 6.773782253265381
Epoch 230, val loss: 0.7443755269050598
Epoch 240, training loss: 0.42161813378334045 = 0.35394975543022156 + 0.01 * 6.766837120056152
Epoch 240, val loss: 0.7153176069259644
Epoch 250, training loss: 0.3708421289920807 = 0.3032381534576416 + 0.01 * 6.760397911071777
Epoch 250, val loss: 0.6916807889938354
Epoch 260, training loss: 0.3255733251571655 = 0.2579790949821472 + 0.01 * 6.759421348571777
Epoch 260, val loss: 0.6740875840187073
Epoch 270, training loss: 0.286371648311615 = 0.21883586049079895 + 0.01 * 6.753579616546631
Epoch 270, val loss: 0.6626691222190857
Epoch 280, training loss: 0.2533845007419586 = 0.18590085208415985 + 0.01 * 6.74836540222168
Epoch 280, val loss: 0.6570701599121094
Epoch 290, training loss: 0.226261168718338 = 0.1586717665195465 + 0.01 * 6.758939743041992
Epoch 290, val loss: 0.6563977599143982
Epoch 300, training loss: 0.20379072427749634 = 0.1363459825515747 + 0.01 * 6.744473934173584
Epoch 300, val loss: 0.6596647500991821
Epoch 310, training loss: 0.18536511063575745 = 0.11801549792289734 + 0.01 * 6.734961032867432
Epoch 310, val loss: 0.6660950779914856
Epoch 320, training loss: 0.17035500705242157 = 0.10284808278083801 + 0.01 * 6.750692844390869
Epoch 320, val loss: 0.6747754812240601
Epoch 330, training loss: 0.15745234489440918 = 0.09019537270069122 + 0.01 * 6.725697994232178
Epoch 330, val loss: 0.6852760910987854
Epoch 340, training loss: 0.146728053689003 = 0.07953700423240662 + 0.01 * 6.719105243682861
Epoch 340, val loss: 0.6970951557159424
Epoch 350, training loss: 0.13761678338050842 = 0.0704936534166336 + 0.01 * 6.712312698364258
Epoch 350, val loss: 0.7098300457000732
Epoch 360, training loss: 0.12991544604301453 = 0.06276711821556091 + 0.01 * 6.714832305908203
Epoch 360, val loss: 0.7231568098068237
Epoch 370, training loss: 0.12314386665821075 = 0.05613338574767113 + 0.01 * 6.701047897338867
Epoch 370, val loss: 0.7369263172149658
Epoch 380, training loss: 0.11746751517057419 = 0.05040998011827469 + 0.01 * 6.705753803253174
Epoch 380, val loss: 0.750972330570221
Epoch 390, training loss: 0.11235109716653824 = 0.0454539954662323 + 0.01 * 6.6897101402282715
Epoch 390, val loss: 0.7651703953742981
Epoch 400, training loss: 0.10799972712993622 = 0.04114076867699623 + 0.01 * 6.6858954429626465
Epoch 400, val loss: 0.7794235348701477
Epoch 410, training loss: 0.1041460782289505 = 0.03737088292837143 + 0.01 * 6.67751932144165
Epoch 410, val loss: 0.7936298251152039
Epoch 420, training loss: 0.10109458863735199 = 0.03406635299324989 + 0.01 * 6.702823638916016
Epoch 420, val loss: 0.8077186942100525
Epoch 430, training loss: 0.09790395200252533 = 0.031162286177277565 + 0.01 * 6.674166679382324
Epoch 430, val loss: 0.8215042948722839
Epoch 440, training loss: 0.09524226188659668 = 0.028593746945261955 + 0.01 * 6.664851188659668
Epoch 440, val loss: 0.83512943983078
Epoch 450, training loss: 0.09291086345911026 = 0.026312248781323433 + 0.01 * 6.659862041473389
Epoch 450, val loss: 0.8485124707221985
Epoch 460, training loss: 0.09088990837335587 = 0.024278879165649414 + 0.01 * 6.661102771759033
Epoch 460, val loss: 0.8616519570350647
Epoch 470, training loss: 0.08898794651031494 = 0.022463355213403702 + 0.01 * 6.652458667755127
Epoch 470, val loss: 0.8744637966156006
Epoch 480, training loss: 0.08730248361825943 = 0.020836643874645233 + 0.01 * 6.6465840339660645
Epoch 480, val loss: 0.8869493007659912
Epoch 490, training loss: 0.0858769565820694 = 0.0193734560161829 + 0.01 * 6.650350570678711
Epoch 490, val loss: 0.8991920948028564
Epoch 500, training loss: 0.08445821702480316 = 0.018054870888590813 + 0.01 * 6.640334606170654
Epoch 500, val loss: 0.911090612411499
Epoch 510, training loss: 0.0832539051771164 = 0.016862886026501656 + 0.01 * 6.639101982116699
Epoch 510, val loss: 0.9226815700531006
Epoch 520, training loss: 0.08214148879051208 = 0.015782400965690613 + 0.01 * 6.635909080505371
Epoch 520, val loss: 0.9339513182640076
Epoch 530, training loss: 0.08106302469968796 = 0.014801719225943089 + 0.01 * 6.626131057739258
Epoch 530, val loss: 0.944953441619873
Epoch 540, training loss: 0.08026675879955292 = 0.013909069821238518 + 0.01 * 6.635768890380859
Epoch 540, val loss: 0.9556061625480652
Epoch 550, training loss: 0.07930466532707214 = 0.013098369352519512 + 0.01 * 6.620630264282227
Epoch 550, val loss: 0.9659442901611328
Epoch 560, training loss: 0.07859876751899719 = 0.012357382103800774 + 0.01 * 6.624138355255127
Epoch 560, val loss: 0.9759661555290222
Epoch 570, training loss: 0.07786040008068085 = 0.011678226292133331 + 0.01 * 6.618217468261719
Epoch 570, val loss: 0.9858207702636719
Epoch 580, training loss: 0.07710579037666321 = 0.011053676716983318 + 0.01 * 6.6052117347717285
Epoch 580, val loss: 0.995378851890564
Epoch 590, training loss: 0.07664637267589569 = 0.01047801598906517 + 0.01 * 6.616835117340088
Epoch 590, val loss: 1.0047194957733154
Epoch 600, training loss: 0.07597861438989639 = 0.009947708807885647 + 0.01 * 6.603090763092041
Epoch 600, val loss: 1.0138037204742432
Epoch 610, training loss: 0.07538758963346481 = 0.009457279928028584 + 0.01 * 6.59303092956543
Epoch 610, val loss: 1.0226467847824097
Epoch 620, training loss: 0.0749860480427742 = 0.009002895094454288 + 0.01 * 6.598315715789795
Epoch 620, val loss: 1.031222939491272
Epoch 630, training loss: 0.07467243820428848 = 0.008581222966313362 + 0.01 * 6.609121799468994
Epoch 630, val loss: 1.0397353172302246
Epoch 640, training loss: 0.07414279878139496 = 0.008190203458070755 + 0.01 * 6.595259189605713
Epoch 640, val loss: 1.0478904247283936
Epoch 650, training loss: 0.07357189804315567 = 0.00782629381865263 + 0.01 * 6.57456111907959
Epoch 650, val loss: 1.0559468269348145
Epoch 660, training loss: 0.07320718467235565 = 0.00748737808316946 + 0.01 * 6.571980953216553
Epoch 660, val loss: 1.0637320280075073
Epoch 670, training loss: 0.07291703671216965 = 0.007172077428549528 + 0.01 * 6.574496269226074
Epoch 670, val loss: 1.071370005607605
Epoch 680, training loss: 0.07250557094812393 = 0.006877535954117775 + 0.01 * 6.562803745269775
Epoch 680, val loss: 1.0787891149520874
Epoch 690, training loss: 0.07245703041553497 = 0.006602758076041937 + 0.01 * 6.585427284240723
Epoch 690, val loss: 1.0859695672988892
Epoch 700, training loss: 0.07195358723402023 = 0.006345665547996759 + 0.01 * 6.560792446136475
Epoch 700, val loss: 1.0931620597839355
Epoch 710, training loss: 0.071751669049263 = 0.006104402709752321 + 0.01 * 6.564726829528809
Epoch 710, val loss: 1.0999252796173096
Epoch 720, training loss: 0.0714179053902626 = 0.005878643598407507 + 0.01 * 6.553926467895508
Epoch 720, val loss: 1.1067603826522827
Epoch 730, training loss: 0.07118567079305649 = 0.005666123703122139 + 0.01 * 6.551954746246338
Epoch 730, val loss: 1.113210678100586
Epoch 740, training loss: 0.07086583226919174 = 0.005466789472848177 + 0.01 * 6.5399041175842285
Epoch 740, val loss: 1.1196825504302979
Epoch 750, training loss: 0.07061205804347992 = 0.005278471857309341 + 0.01 * 6.533358097076416
Epoch 750, val loss: 1.1259058713912964
Epoch 760, training loss: 0.07057831436395645 = 0.005101020913571119 + 0.01 * 6.547729969024658
Epoch 760, val loss: 1.1320682764053345
Epoch 770, training loss: 0.07020363956689835 = 0.004933998920023441 + 0.01 * 6.52696418762207
Epoch 770, val loss: 1.1380583047866821
Epoch 780, training loss: 0.07006938755512238 = 0.0047761788591742516 + 0.01 * 6.529321193695068
Epoch 780, val loss: 1.1439059972763062
Epoch 790, training loss: 0.06977467238903046 = 0.0046267276629805565 + 0.01 * 6.51479434967041
Epoch 790, val loss: 1.1495695114135742
Epoch 800, training loss: 0.06965963542461395 = 0.004485486075282097 + 0.01 * 6.5174150466918945
Epoch 800, val loss: 1.155202865600586
Epoch 810, training loss: 0.0697045624256134 = 0.00435135792940855 + 0.01 * 6.53532075881958
Epoch 810, val loss: 1.1606993675231934
Epoch 820, training loss: 0.06923745572566986 = 0.004224404692649841 + 0.01 * 6.501305103302002
Epoch 820, val loss: 1.166101098060608
Epoch 830, training loss: 0.06914155930280685 = 0.004103846848011017 + 0.01 * 6.5037713050842285
Epoch 830, val loss: 1.1712509393692017
Epoch 840, training loss: 0.06889402866363525 = 0.003989293705672026 + 0.01 * 6.490473747253418
Epoch 840, val loss: 1.1764391660690308
Epoch 850, training loss: 0.0687907263636589 = 0.0038804952055215836 + 0.01 * 6.491023540496826
Epoch 850, val loss: 1.181505560874939
Epoch 860, training loss: 0.06862089037895203 = 0.003776784287765622 + 0.01 * 6.484411239624023
Epoch 860, val loss: 1.1864275932312012
Epoch 870, training loss: 0.06841196119785309 = 0.003677729284390807 + 0.01 * 6.473423957824707
Epoch 870, val loss: 1.1911929845809937
Epoch 880, training loss: 0.06829798966646194 = 0.003583683865144849 + 0.01 * 6.47143030166626
Epoch 880, val loss: 1.196048378944397
Epoch 890, training loss: 0.0681423470377922 = 0.0034939940087497234 + 0.01 * 6.4648356437683105
Epoch 890, val loss: 1.2006596326828003
Epoch 900, training loss: 0.06830617785453796 = 0.003408102784305811 + 0.01 * 6.489807605743408
Epoch 900, val loss: 1.2052068710327148
Epoch 910, training loss: 0.06783333420753479 = 0.003326140809804201 + 0.01 * 6.450719356536865
Epoch 910, val loss: 1.2097113132476807
Epoch 920, training loss: 0.06796956807374954 = 0.003247753018513322 + 0.01 * 6.47218132019043
Epoch 920, val loss: 1.2140049934387207
Epoch 930, training loss: 0.06762143969535828 = 0.0031727091409265995 + 0.01 * 6.444873332977295
Epoch 930, val loss: 1.2182872295379639
Epoch 940, training loss: 0.06762819737195969 = 0.003100884612649679 + 0.01 * 6.452731132507324
Epoch 940, val loss: 1.222599744796753
Epoch 950, training loss: 0.06749334931373596 = 0.003031899221241474 + 0.01 * 6.4461445808410645
Epoch 950, val loss: 1.2265831232070923
Epoch 960, training loss: 0.06749779731035233 = 0.0029659622814506292 + 0.01 * 6.453183650970459
Epoch 960, val loss: 1.2307260036468506
Epoch 970, training loss: 0.06726302206516266 = 0.0029026216361671686 + 0.01 * 6.43604040145874
Epoch 970, val loss: 1.234697699546814
Epoch 980, training loss: 0.06743170320987701 = 0.002841653535142541 + 0.01 * 6.459005355834961
Epoch 980, val loss: 1.2385298013687134
Epoch 990, training loss: 0.06715521216392517 = 0.002783337840810418 + 0.01 * 6.437188148498535
Epoch 990, val loss: 1.242440938949585
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.9077
Flip ASR: 0.8889/225 nodes
The final ASR:0.72817, 0.13381, Accuracy:0.82716, 0.01259
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11604])
remove edge: torch.Size([2, 9490])
updated graph: torch.Size([2, 10538])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.98032, 0.00174, Accuracy:0.83086, 0.00630
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.5 der2:0.4 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.036221504211426 = 1.9524832963943481 + 0.01 * 8.373819351196289
Epoch 0, val loss: 1.9573462009429932
Epoch 10, training loss: 2.0258872509002686 = 1.942150354385376 + 0.01 * 8.37369441986084
Epoch 10, val loss: 1.9475712776184082
Epoch 20, training loss: 2.012955665588379 = 1.9292219877243042 + 0.01 * 8.373379707336426
Epoch 20, val loss: 1.9348983764648438
Epoch 30, training loss: 1.9947876930236816 = 1.9110623598098755 + 0.01 * 8.372537612915039
Epoch 30, val loss: 1.91668701171875
Epoch 40, training loss: 1.9679174423217773 = 1.8842297792434692 + 0.01 * 8.368767738342285
Epoch 40, val loss: 1.889837622642517
Epoch 50, training loss: 1.9299321174621582 = 1.8465027809143066 + 0.01 * 8.342931747436523
Epoch 50, val loss: 1.8534640073776245
Epoch 60, training loss: 1.884763240814209 = 1.802899956703186 + 0.01 * 8.186330795288086
Epoch 60, val loss: 1.8153159618377686
Epoch 70, training loss: 1.840480089187622 = 1.7626408338546753 + 0.01 * 7.783926486968994
Epoch 70, val loss: 1.782793641090393
Epoch 80, training loss: 1.7884246110916138 = 1.7151066064834595 + 0.01 * 7.331800937652588
Epoch 80, val loss: 1.7412664890289307
Epoch 90, training loss: 1.722477674484253 = 1.6506056785583496 + 0.01 * 7.187202453613281
Epoch 90, val loss: 1.6849102973937988
Epoch 100, training loss: 1.638638973236084 = 1.5674149990081787 + 0.01 * 7.122395992279053
Epoch 100, val loss: 1.614964246749878
Epoch 110, training loss: 1.5426737070083618 = 1.4717892408370972 + 0.01 * 7.088450908660889
Epoch 110, val loss: 1.5361424684524536
Epoch 120, training loss: 1.444085955619812 = 1.3733457326889038 + 0.01 * 7.074024677276611
Epoch 120, val loss: 1.456737995147705
Epoch 130, training loss: 1.3458410501480103 = 1.2751522064208984 + 0.01 * 7.068882942199707
Epoch 130, val loss: 1.3795514106750488
Epoch 140, training loss: 1.2464436292648315 = 1.1758352518081665 + 0.01 * 7.060836315155029
Epoch 140, val loss: 1.3007583618164062
Epoch 150, training loss: 1.1461613178253174 = 1.0756388902664185 + 0.01 * 7.052242755889893
Epoch 150, val loss: 1.2208880186080933
Epoch 160, training loss: 1.046816110610962 = 0.9764115810394287 + 0.01 * 7.0404558181762695
Epoch 160, val loss: 1.1420756578445435
Epoch 170, training loss: 0.9501240253448486 = 0.879889726638794 + 0.01 * 7.0234270095825195
Epoch 170, val loss: 1.0665004253387451
Epoch 180, training loss: 0.8572400212287903 = 0.7872427701950073 + 0.01 * 6.999722957611084
Epoch 180, val loss: 0.9959022998809814
Epoch 190, training loss: 0.7693943381309509 = 0.699651300907135 + 0.01 * 6.974302768707275
Epoch 190, val loss: 0.9312868118286133
Epoch 200, training loss: 0.6881792545318604 = 0.6186350584030151 + 0.01 * 6.954421043395996
Epoch 200, val loss: 0.8744298219680786
Epoch 210, training loss: 0.6150658130645752 = 0.5456144213676453 + 0.01 * 6.945140838623047
Epoch 210, val loss: 0.8266480565071106
Epoch 220, training loss: 0.5504031777381897 = 0.4810497462749481 + 0.01 * 6.935344219207764
Epoch 220, val loss: 0.7883277535438538
Epoch 230, training loss: 0.4934520423412323 = 0.42417746782302856 + 0.01 * 6.9274582862854
Epoch 230, val loss: 0.7585470080375671
Epoch 240, training loss: 0.44304201006889343 = 0.3738273084163666 + 0.01 * 6.921471118927002
Epoch 240, val loss: 0.7354274392127991
Epoch 250, training loss: 0.3980201482772827 = 0.3289209306240082 + 0.01 * 6.9099202156066895
Epoch 250, val loss: 0.7171170711517334
Epoch 260, training loss: 0.3577086627483368 = 0.2886583209037781 + 0.01 * 6.905034065246582
Epoch 260, val loss: 0.7027071118354797
Epoch 270, training loss: 0.3214341402053833 = 0.25250929594039917 + 0.01 * 6.892484188079834
Epoch 270, val loss: 0.6919516324996948
Epoch 280, training loss: 0.28892582654953003 = 0.22011925280094147 + 0.01 * 6.880658149719238
Epoch 280, val loss: 0.6848971247673035
Epoch 290, training loss: 0.2599542737007141 = 0.19127048552036285 + 0.01 * 6.868378162384033
Epoch 290, val loss: 0.6811521053314209
Epoch 300, training loss: 0.23466439545154572 = 0.1658570021390915 + 0.01 * 6.880739212036133
Epoch 300, val loss: 0.6802436113357544
Epoch 310, training loss: 0.21223099529743195 = 0.14379359781742096 + 0.01 * 6.843739986419678
Epoch 310, val loss: 0.6818097829818726
Epoch 320, training loss: 0.19314569234848022 = 0.12483847141265869 + 0.01 * 6.830721378326416
Epoch 320, val loss: 0.6855267882347107
Epoch 330, training loss: 0.17684327065944672 = 0.10866720229387283 + 0.01 * 6.8176069259643555
Epoch 330, val loss: 0.6910321116447449
Epoch 340, training loss: 0.16315197944641113 = 0.09494547545909882 + 0.01 * 6.82064962387085
Epoch 340, val loss: 0.6980000138282776
Epoch 350, training loss: 0.151475191116333 = 0.08332324028015137 + 0.01 * 6.815194129943848
Epoch 350, val loss: 0.7061315774917603
Epoch 360, training loss: 0.14138512313365936 = 0.07343196123838425 + 0.01 * 6.795316696166992
Epoch 360, val loss: 0.7152409553527832
Epoch 370, training loss: 0.1330498456954956 = 0.06501823663711548 + 0.01 * 6.80316162109375
Epoch 370, val loss: 0.7250423431396484
Epoch 380, training loss: 0.12566684186458588 = 0.05785572901368141 + 0.01 * 6.781111717224121
Epoch 380, val loss: 0.7353085279464722
Epoch 390, training loss: 0.11945120990276337 = 0.05173107609152794 + 0.01 * 6.772013187408447
Epoch 390, val loss: 0.7460281848907471
Epoch 400, training loss: 0.11408432573080063 = 0.046469226479530334 + 0.01 * 6.761509895324707
Epoch 400, val loss: 0.7571560144424438
Epoch 410, training loss: 0.10956165939569473 = 0.041931696236133575 + 0.01 * 6.762996673583984
Epoch 410, val loss: 0.768357515335083
Epoch 420, training loss: 0.10551317036151886 = 0.037998735904693604 + 0.01 * 6.751443862915039
Epoch 420, val loss: 0.7796450853347778
Epoch 430, training loss: 0.10207726061344147 = 0.034570369869470596 + 0.01 * 6.750689506530762
Epoch 430, val loss: 0.7908805012702942
Epoch 440, training loss: 0.09897783398628235 = 0.031569890677928925 + 0.01 * 6.740794658660889
Epoch 440, val loss: 0.8020262122154236
Epoch 450, training loss: 0.09627886116504669 = 0.02892959676682949 + 0.01 * 6.734926223754883
Epoch 450, val loss: 0.8130233287811279
Epoch 460, training loss: 0.0938190147280693 = 0.026596546173095703 + 0.01 * 6.7222466468811035
Epoch 460, val loss: 0.8238546848297119
Epoch 470, training loss: 0.09167594462633133 = 0.024526866152882576 + 0.01 * 6.714908123016357
Epoch 470, val loss: 0.8344917297363281
Epoch 480, training loss: 0.08976703137159348 = 0.022683965042233467 + 0.01 * 6.708307266235352
Epoch 480, val loss: 0.8448814749717712
Epoch 490, training loss: 0.08811743557453156 = 0.02103731781244278 + 0.01 * 6.708011627197266
Epoch 490, val loss: 0.8550453782081604
Epoch 500, training loss: 0.08655020594596863 = 0.01956157758831978 + 0.01 * 6.698863506317139
Epoch 500, val loss: 0.8649263381958008
Epoch 510, training loss: 0.08535094559192657 = 0.01823420822620392 + 0.01 * 6.711673736572266
Epoch 510, val loss: 0.874561607837677
Epoch 520, training loss: 0.08397208154201508 = 0.017037222161889076 + 0.01 * 6.693486213684082
Epoch 520, val loss: 0.8839443325996399
Epoch 530, training loss: 0.08286421000957489 = 0.01595466583967209 + 0.01 * 6.690954685211182
Epoch 530, val loss: 0.8930559158325195
Epoch 540, training loss: 0.08180762082338333 = 0.014972678385674953 + 0.01 * 6.683494567871094
Epoch 540, val loss: 0.9019151329994202
Epoch 550, training loss: 0.08076091855764389 = 0.014080646447837353 + 0.01 * 6.668027877807617
Epoch 550, val loss: 0.9105588793754578
Epoch 560, training loss: 0.07993169873952866 = 0.013266787864267826 + 0.01 * 6.6664910316467285
Epoch 560, val loss: 0.9189853072166443
Epoch 570, training loss: 0.07946884632110596 = 0.012522692792117596 + 0.01 * 6.694615840911865
Epoch 570, val loss: 0.9271346926689148
Epoch 580, training loss: 0.078423872590065 = 0.011842029169201851 + 0.01 * 6.658185005187988
Epoch 580, val loss: 0.9351217150688171
Epoch 590, training loss: 0.07782362401485443 = 0.011217474937438965 + 0.01 * 6.660614967346191
Epoch 590, val loss: 0.9428656697273254
Epoch 600, training loss: 0.0771571546792984 = 0.01064268033951521 + 0.01 * 6.651447772979736
Epoch 600, val loss: 0.9504582285881042
Epoch 610, training loss: 0.07660919427871704 = 0.010113302618265152 + 0.01 * 6.6495890617370605
Epoch 610, val loss: 0.9577270746231079
Epoch 620, training loss: 0.07606743276119232 = 0.009624894708395004 + 0.01 * 6.644254207611084
Epoch 620, val loss: 0.9649322628974915
Epoch 630, training loss: 0.07548905909061432 = 0.009173046797513962 + 0.01 * 6.631600856781006
Epoch 630, val loss: 0.9719221591949463
Epoch 640, training loss: 0.07513142377138138 = 0.008753666654229164 + 0.01 * 6.637775897979736
Epoch 640, val loss: 0.9787464141845703
Epoch 650, training loss: 0.07461053133010864 = 0.008364285342395306 + 0.01 * 6.624624729156494
Epoch 650, val loss: 0.9853801727294922
Epoch 660, training loss: 0.0745702013373375 = 0.008002790622413158 + 0.01 * 6.656741619110107
Epoch 660, val loss: 0.9918457269668579
Epoch 670, training loss: 0.07389982789754868 = 0.0076668402180075645 + 0.01 * 6.6232991218566895
Epoch 670, val loss: 0.9982033967971802
Epoch 680, training loss: 0.0734899714589119 = 0.007353233173489571 + 0.01 * 6.613674163818359
Epoch 680, val loss: 1.0043597221374512
Epoch 690, training loss: 0.07310566306114197 = 0.007060025352984667 + 0.01 * 6.604563236236572
Epoch 690, val loss: 1.0104163885116577
Epoch 700, training loss: 0.07280975580215454 = 0.006785277277231216 + 0.01 * 6.602448463439941
Epoch 700, val loss: 1.016295313835144
Epoch 710, training loss: 0.07250924408435822 = 0.006527980323880911 + 0.01 * 6.59812593460083
Epoch 710, val loss: 1.0220099687576294
Epoch 720, training loss: 0.07225866615772247 = 0.006287065334618092 + 0.01 * 6.5971598625183105
Epoch 720, val loss: 1.027707815170288
Epoch 730, training loss: 0.07219667732715607 = 0.006060675717890263 + 0.01 * 6.613600254058838
Epoch 730, val loss: 1.0332306623458862
Epoch 740, training loss: 0.07172514498233795 = 0.005847708787769079 + 0.01 * 6.587743759155273
Epoch 740, val loss: 1.0386860370635986
Epoch 750, training loss: 0.07154512405395508 = 0.005647172220051289 + 0.01 * 6.589795112609863
Epoch 750, val loss: 1.04390287399292
Epoch 760, training loss: 0.071165531873703 = 0.005458466708660126 + 0.01 * 6.570706367492676
Epoch 760, val loss: 1.0491043329238892
Epoch 770, training loss: 0.07132840156555176 = 0.005280208773910999 + 0.01 * 6.604818820953369
Epoch 770, val loss: 1.0541874170303345
Epoch 780, training loss: 0.07087459415197372 = 0.0051117464900016785 + 0.01 * 6.576284885406494
Epoch 780, val loss: 1.0591408014297485
Epoch 790, training loss: 0.07108170539140701 = 0.004952645860612392 + 0.01 * 6.612905979156494
Epoch 790, val loss: 1.0639945268630981
Epoch 800, training loss: 0.07043200731277466 = 0.004801911301910877 + 0.01 * 6.563009738922119
Epoch 800, val loss: 1.068705439567566
Epoch 810, training loss: 0.07017981261014938 = 0.00465933233499527 + 0.01 * 6.552048206329346
Epoch 810, val loss: 1.0733457803726196
Epoch 820, training loss: 0.07024139910936356 = 0.004523941781371832 + 0.01 * 6.571745872497559
Epoch 820, val loss: 1.0779317617416382
Epoch 830, training loss: 0.06983375549316406 = 0.0043954127468168736 + 0.01 * 6.543834686279297
Epoch 830, val loss: 1.0822981595993042
Epoch 840, training loss: 0.06974450498819351 = 0.004273341037333012 + 0.01 * 6.547116279602051
Epoch 840, val loss: 1.0866703987121582
Epoch 850, training loss: 0.06957962363958359 = 0.004157161805778742 + 0.01 * 6.542245864868164
Epoch 850, val loss: 1.090911865234375
Epoch 860, training loss: 0.0694102868437767 = 0.004046659450978041 + 0.01 * 6.536363124847412
Epoch 860, val loss: 1.0951178073883057
Epoch 870, training loss: 0.06925181299448013 = 0.003941291477531195 + 0.01 * 6.531052112579346
Epoch 870, val loss: 1.0991970300674438
Epoch 880, training loss: 0.06926174461841583 = 0.0038409456610679626 + 0.01 * 6.542079925537109
Epoch 880, val loss: 1.10319185256958
Epoch 890, training loss: 0.06899674981832504 = 0.0037452804390341043 + 0.01 * 6.525147438049316
Epoch 890, val loss: 1.1071193218231201
Epoch 900, training loss: 0.0690544992685318 = 0.0036539381835609674 + 0.01 * 6.540056228637695
Epoch 900, val loss: 1.1110224723815918
Epoch 910, training loss: 0.06868933886289597 = 0.0035666509065777063 + 0.01 * 6.512268543243408
Epoch 910, val loss: 1.114748239517212
Epoch 920, training loss: 0.06862448155879974 = 0.003483183914795518 + 0.01 * 6.514130115509033
Epoch 920, val loss: 1.1184616088867188
Epoch 930, training loss: 0.06839698553085327 = 0.003403286449611187 + 0.01 * 6.499370574951172
Epoch 930, val loss: 1.1221176385879517
Epoch 940, training loss: 0.068263940513134 = 0.0033268111292272806 + 0.01 * 6.493712902069092
Epoch 940, val loss: 1.1256972551345825
Epoch 950, training loss: 0.06844300776720047 = 0.003253562143072486 + 0.01 * 6.518944263458252
Epoch 950, val loss: 1.1292105913162231
Epoch 960, training loss: 0.06813377141952515 = 0.003183250781148672 + 0.01 * 6.495051860809326
Epoch 960, val loss: 1.1326587200164795
Epoch 970, training loss: 0.06829212605953217 = 0.003115876577794552 + 0.01 * 6.517625331878662
Epoch 970, val loss: 1.1360629796981812
Epoch 980, training loss: 0.0679788663983345 = 0.0030512886587530375 + 0.01 * 6.492757797241211
Epoch 980, val loss: 1.1393526792526245
Epoch 990, training loss: 0.06776034086942673 = 0.0029892518650740385 + 0.01 * 6.477108478546143
Epoch 990, val loss: 1.1426347494125366
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.4945
Flip ASR: 0.3956/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.02616548538208 = 1.9424270391464233 + 0.01 * 8.373833656311035
Epoch 0, val loss: 1.9353605508804321
Epoch 10, training loss: 2.015815258026123 = 1.9320772886276245 + 0.01 * 8.37379264831543
Epoch 10, val loss: 1.9233895540237427
Epoch 20, training loss: 2.0034780502319336 = 1.9197415113449097 + 0.01 * 8.37366008758545
Epoch 20, val loss: 1.9084762334823608
Epoch 30, training loss: 1.986704707145691 = 1.9029719829559326 + 0.01 * 8.373274803161621
Epoch 30, val loss: 1.8878892660140991
Epoch 40, training loss: 1.9631229639053345 = 1.8794045448303223 + 0.01 * 8.371847152709961
Epoch 40, val loss: 1.8597238063812256
Epoch 50, training loss: 1.9306060075759888 = 1.8469640016555786 + 0.01 * 8.364204406738281
Epoch 50, val loss: 1.823816180229187
Epoch 60, training loss: 1.8888909816741943 = 1.8056918382644653 + 0.01 * 8.319917678833008
Epoch 60, val loss: 1.7827346324920654
Epoch 70, training loss: 1.8417348861694336 = 1.7607131004333496 + 0.01 * 8.102174758911133
Epoch 70, val loss: 1.7431293725967407
Epoch 80, training loss: 1.7909409999847412 = 1.7135618925094604 + 0.01 * 7.737907886505127
Epoch 80, val loss: 1.7033133506774902
Epoch 90, training loss: 1.725950837135315 = 1.651160717010498 + 0.01 * 7.479011058807373
Epoch 90, val loss: 1.6486787796020508
Epoch 100, training loss: 1.639919638633728 = 1.5662646293640137 + 0.01 * 7.3655009269714355
Epoch 100, val loss: 1.5755761861801147
Epoch 110, training loss: 1.5334609746932983 = 1.460569977760315 + 0.01 * 7.289097309112549
Epoch 110, val loss: 1.4879727363586426
Epoch 120, training loss: 1.4191631078720093 = 1.3470836877822876 + 0.01 * 7.207945823669434
Epoch 120, val loss: 1.3965630531311035
Epoch 130, training loss: 1.3098857402801514 = 1.2384916543960571 + 0.01 * 7.139405250549316
Epoch 130, val loss: 1.3139197826385498
Epoch 140, training loss: 1.2104929685592651 = 1.1392861604690552 + 0.01 * 7.120686054229736
Epoch 140, val loss: 1.2420744895935059
Epoch 150, training loss: 1.1183010339736938 = 1.0471677780151367 + 0.01 * 7.113329887390137
Epoch 150, val loss: 1.1753275394439697
Epoch 160, training loss: 1.0298521518707275 = 0.958845853805542 + 0.01 * 7.100629806518555
Epoch 160, val loss: 1.1102365255355835
Epoch 170, training loss: 0.9447014927864075 = 0.8738713264465332 + 0.01 * 7.083017826080322
Epoch 170, val loss: 1.048614740371704
Epoch 180, training loss: 0.8650136590003967 = 0.7944526076316833 + 0.01 * 7.056105613708496
Epoch 180, val loss: 0.9923487901687622
Epoch 190, training loss: 0.7929487228393555 = 0.7227224111557007 + 0.01 * 7.022629737854004
Epoch 190, val loss: 0.9438794851303101
Epoch 200, training loss: 0.7290941476821899 = 0.6590520739555359 + 0.01 * 7.00420618057251
Epoch 200, val loss: 0.9042020440101624
Epoch 210, training loss: 0.6721317768096924 = 0.6022729277610779 + 0.01 * 6.9858856201171875
Epoch 210, val loss: 0.8723816275596619
Epoch 220, training loss: 0.6202974915504456 = 0.550510823726654 + 0.01 * 6.978667259216309
Epoch 220, val loss: 0.8464596271514893
Epoch 230, training loss: 0.5721036791801453 = 0.5023475289344788 + 0.01 * 6.97561502456665
Epoch 230, val loss: 0.8247432112693787
Epoch 240, training loss: 0.5267255902290344 = 0.4569922089576721 + 0.01 * 6.973340034484863
Epoch 240, val loss: 0.8061878681182861
Epoch 250, training loss: 0.483659952878952 = 0.41394320130348206 + 0.01 * 6.971675872802734
Epoch 250, val loss: 0.7892859578132629
Epoch 260, training loss: 0.4426666498184204 = 0.3729588985443115 + 0.01 * 6.970774173736572
Epoch 260, val loss: 0.7738166451454163
Epoch 270, training loss: 0.40371447801589966 = 0.33400970697402954 + 0.01 * 6.970475673675537
Epoch 270, val loss: 0.7598604559898376
Epoch 280, training loss: 0.3671245872974396 = 0.29741981625556946 + 0.01 * 6.9704766273498535
Epoch 280, val loss: 0.7475107908248901
Epoch 290, training loss: 0.3332349956035614 = 0.2635287046432495 + 0.01 * 6.970628261566162
Epoch 290, val loss: 0.7369502186775208
Epoch 300, training loss: 0.3024781346321106 = 0.23277170956134796 + 0.01 * 6.970642566680908
Epoch 300, val loss: 0.7284350991249084
Epoch 310, training loss: 0.27495667338371277 = 0.20525355637073517 + 0.01 * 6.970311641693115
Epoch 310, val loss: 0.7224966883659363
Epoch 320, training loss: 0.25041231513023376 = 0.18071675300598145 + 0.01 * 6.969557285308838
Epoch 320, val loss: 0.7190083861351013
Epoch 330, training loss: 0.22853264212608337 = 0.1588510274887085 + 0.01 * 6.9681620597839355
Epoch 330, val loss: 0.7177242636680603
Epoch 340, training loss: 0.20903892815113068 = 0.1393798440694809 + 0.01 * 6.965908527374268
Epoch 340, val loss: 0.7183988690376282
Epoch 350, training loss: 0.19220614433288574 = 0.12256858497858047 + 0.01 * 6.963757038116455
Epoch 350, val loss: 0.7204393744468689
Epoch 360, training loss: 0.17763954401016235 = 0.10805297642946243 + 0.01 * 6.958656311035156
Epoch 360, val loss: 0.7242643237113953
Epoch 370, training loss: 0.16507098078727722 = 0.09553980827331543 + 0.01 * 6.953118324279785
Epoch 370, val loss: 0.7291392683982849
Epoch 380, training loss: 0.15416857600212097 = 0.08470993489027023 + 0.01 * 6.945863723754883
Epoch 380, val loss: 0.735097348690033
Epoch 390, training loss: 0.14476963877677917 = 0.07532140612602234 + 0.01 * 6.944822788238525
Epoch 390, val loss: 0.741995632648468
Epoch 400, training loss: 0.13643625378608704 = 0.06715618073940277 + 0.01 * 6.928007125854492
Epoch 400, val loss: 0.7494186162948608
Epoch 410, training loss: 0.12919656932353973 = 0.06004202738404274 + 0.01 * 6.915454864501953
Epoch 410, val loss: 0.7573372721672058
Epoch 420, training loss: 0.12295582890510559 = 0.053784213960170746 + 0.01 * 6.91716194152832
Epoch 420, val loss: 0.765373706817627
Epoch 430, training loss: 0.11733861267566681 = 0.048288844525814056 + 0.01 * 6.904976844787598
Epoch 430, val loss: 0.7736445665359497
Epoch 440, training loss: 0.11230956763029099 = 0.04353233426809311 + 0.01 * 6.877723693847656
Epoch 440, val loss: 0.7818630337715149
Epoch 450, training loss: 0.10804169625043869 = 0.03937339782714844 + 0.01 * 6.866830348968506
Epoch 450, val loss: 0.7899659872055054
Epoch 460, training loss: 0.10445638000965118 = 0.03572482615709305 + 0.01 * 6.87315559387207
Epoch 460, val loss: 0.7980282306671143
Epoch 470, training loss: 0.10108017921447754 = 0.032524239271879196 + 0.01 * 6.855594635009766
Epoch 470, val loss: 0.8059816360473633
Epoch 480, training loss: 0.09815165400505066 = 0.029697347432374954 + 0.01 * 6.845430850982666
Epoch 480, val loss: 0.8138123154640198
Epoch 490, training loss: 0.09556982666254044 = 0.027196725830435753 + 0.01 * 6.837309837341309
Epoch 490, val loss: 0.8214966654777527
Epoch 500, training loss: 0.09331319481134415 = 0.024982163682579994 + 0.01 * 6.833103656768799
Epoch 500, val loss: 0.8289865851402283
Epoch 510, training loss: 0.09126199781894684 = 0.023015256971120834 + 0.01 * 6.824674129486084
Epoch 510, val loss: 0.8363499641418457
Epoch 520, training loss: 0.08948761224746704 = 0.02126907743513584 + 0.01 * 6.8218536376953125
Epoch 520, val loss: 0.843485414981842
Epoch 530, training loss: 0.08779595792293549 = 0.019705764949321747 + 0.01 * 6.809019088745117
Epoch 530, val loss: 0.8504425883293152
Epoch 540, training loss: 0.0864771157503128 = 0.018308918923139572 + 0.01 * 6.81682014465332
Epoch 540, val loss: 0.8572410345077515
Epoch 550, training loss: 0.0851103886961937 = 0.017056504264473915 + 0.01 * 6.805388927459717
Epoch 550, val loss: 0.8638193011283875
Epoch 560, training loss: 0.08383220434188843 = 0.015927422791719437 + 0.01 * 6.790477752685547
Epoch 560, val loss: 0.8702576756477356
Epoch 570, training loss: 0.08276649564504623 = 0.014906196855008602 + 0.01 * 6.786030292510986
Epoch 570, val loss: 0.8765257596969604
Epoch 580, training loss: 0.08173801749944687 = 0.013982564210891724 + 0.01 * 6.775545597076416
Epoch 580, val loss: 0.8826541900634766
Epoch 590, training loss: 0.08087017387151718 = 0.01314382441341877 + 0.01 * 6.772635459899902
Epoch 590, val loss: 0.8885757923126221
Epoch 600, training loss: 0.07999573647975922 = 0.012380183674395084 + 0.01 * 6.7615556716918945
Epoch 600, val loss: 0.8943533301353455
Epoch 610, training loss: 0.07930904626846313 = 0.011681093834340572 + 0.01 * 6.7627949714660645
Epoch 610, val loss: 0.8999868631362915
Epoch 620, training loss: 0.0786253809928894 = 0.011041020974516869 + 0.01 * 6.75843620300293
Epoch 620, val loss: 0.9054232835769653
Epoch 630, training loss: 0.07791903614997864 = 0.010454962030053139 + 0.01 * 6.746407508850098
Epoch 630, val loss: 0.9107055068016052
Epoch 640, training loss: 0.07732181251049042 = 0.009916204959154129 + 0.01 * 6.740560531616211
Epoch 640, val loss: 0.915867030620575
Epoch 650, training loss: 0.07676462829113007 = 0.009419611655175686 + 0.01 * 6.734501838684082
Epoch 650, val loss: 0.9209194779396057
Epoch 660, training loss: 0.07622289657592773 = 0.008961190469563007 + 0.01 * 6.726170539855957
Epoch 660, val loss: 0.92583167552948
Epoch 670, training loss: 0.07582315802574158 = 0.008538761176168919 + 0.01 * 6.728439807891846
Epoch 670, val loss: 0.930629312992096
Epoch 680, training loss: 0.0753457248210907 = 0.008147555403411388 + 0.01 * 6.719817161560059
Epoch 680, val loss: 0.9352775812149048
Epoch 690, training loss: 0.07492856681346893 = 0.0077837370336055756 + 0.01 * 6.714483261108398
Epoch 690, val loss: 0.939778745174408
Epoch 700, training loss: 0.0745132640004158 = 0.007444911170750856 + 0.01 * 6.706835746765137
Epoch 700, val loss: 0.9442325830459595
Epoch 710, training loss: 0.07427937537431717 = 0.007129548583179712 + 0.01 * 6.714982509613037
Epoch 710, val loss: 0.9485042095184326
Epoch 720, training loss: 0.0738217756152153 = 0.0068351649679243565 + 0.01 * 6.698661804199219
Epoch 720, val loss: 0.9527073502540588
Epoch 730, training loss: 0.07349427789449692 = 0.006558258552104235 + 0.01 * 6.693601608276367
Epoch 730, val loss: 0.9568100571632385
Epoch 740, training loss: 0.07327519357204437 = 0.00629911944270134 + 0.01 * 6.697607040405273
Epoch 740, val loss: 0.9608137607574463
Epoch 750, training loss: 0.07310502231121063 = 0.006056598853319883 + 0.01 * 6.704842567443848
Epoch 750, val loss: 0.9647216796875
Epoch 760, training loss: 0.07264892756938934 = 0.005830541253089905 + 0.01 * 6.681838512420654
Epoch 760, val loss: 0.9685063362121582
Epoch 770, training loss: 0.07251539826393127 = 0.00561804324388504 + 0.01 * 6.689735412597656
Epoch 770, val loss: 0.9722252488136292
Epoch 780, training loss: 0.07218749821186066 = 0.005417668726295233 + 0.01 * 6.676982879638672
Epoch 780, val loss: 0.9757720828056335
Epoch 790, training loss: 0.0719195157289505 = 0.005229507107287645 + 0.01 * 6.669000625610352
Epoch 790, val loss: 0.9792678356170654
Epoch 800, training loss: 0.07170986384153366 = 0.005051797721534967 + 0.01 * 6.665806293487549
Epoch 800, val loss: 0.9827329516410828
Epoch 810, training loss: 0.07155398279428482 = 0.004884087946265936 + 0.01 * 6.666989803314209
Epoch 810, val loss: 0.9861071705818176
Epoch 820, training loss: 0.07135848701000214 = 0.00472544739022851 + 0.01 * 6.663304328918457
Epoch 820, val loss: 0.9894396662712097
Epoch 830, training loss: 0.07115673273801804 = 0.00457562692463398 + 0.01 * 6.658111095428467
Epoch 830, val loss: 0.9926431179046631
Epoch 840, training loss: 0.0709734708070755 = 0.004433857277035713 + 0.01 * 6.653961658477783
Epoch 840, val loss: 0.9957504868507385
Epoch 850, training loss: 0.07081042230129242 = 0.004299526568502188 + 0.01 * 6.651089191436768
Epoch 850, val loss: 0.998799204826355
Epoch 860, training loss: 0.07078065723180771 = 0.004172301385551691 + 0.01 * 6.660835266113281
Epoch 860, val loss: 1.0018061399459839
Epoch 870, training loss: 0.07054506987333298 = 0.004051698837429285 + 0.01 * 6.649336814880371
Epoch 870, val loss: 1.00482177734375
Epoch 880, training loss: 0.07037345319986343 = 0.003937217406928539 + 0.01 * 6.643624305725098
Epoch 880, val loss: 1.0076569318771362
Epoch 890, training loss: 0.07025070488452911 = 0.0038287234492599964 + 0.01 * 6.642197608947754
Epoch 890, val loss: 1.0105243921279907
Epoch 900, training loss: 0.07006114721298218 = 0.003726023016497493 + 0.01 * 6.633512496948242
Epoch 900, val loss: 1.0132750272750854
Epoch 910, training loss: 0.06997732818126678 = 0.0036265216767787933 + 0.01 * 6.635080337524414
Epoch 910, val loss: 1.0159685611724854
Epoch 920, training loss: 0.06984817236661911 = 0.00352890114299953 + 0.01 * 6.631927013397217
Epoch 920, val loss: 1.0186303853988647
Epoch 930, training loss: 0.06977405399084091 = 0.003436007769778371 + 0.01 * 6.633804798126221
Epoch 930, val loss: 1.0211405754089355
Epoch 940, training loss: 0.06957656890153885 = 0.003350650891661644 + 0.01 * 6.622591495513916
Epoch 940, val loss: 1.0237393379211426
Epoch 950, training loss: 0.0694502592086792 = 0.003269403474405408 + 0.01 * 6.618085861206055
Epoch 950, val loss: 1.0262161493301392
Epoch 960, training loss: 0.06935610622167587 = 0.0031920173205435276 + 0.01 * 6.6164093017578125
Epoch 960, val loss: 1.0286531448364258
Epoch 970, training loss: 0.0692596435546875 = 0.0031180274672806263 + 0.01 * 6.614161491394043
Epoch 970, val loss: 1.0309844017028809
Epoch 980, training loss: 0.06929369270801544 = 0.0030471940990537405 + 0.01 * 6.624650001525879
Epoch 980, val loss: 1.0334265232086182
Epoch 990, training loss: 0.06899192184209824 = 0.002979651093482971 + 0.01 * 6.601227283477783
Epoch 990, val loss: 1.0356957912445068
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.9483
Flip ASR: 0.9378/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0215556621551514 = 1.937818169593811 + 0.01 * 8.373759269714355
Epoch 0, val loss: 1.9357112646102905
Epoch 10, training loss: 2.0118935108184814 = 1.9281564950942993 + 0.01 * 8.373702049255371
Epoch 10, val loss: 1.9258393049240112
Epoch 20, training loss: 2.000333786010742 = 1.9165996313095093 + 0.01 * 8.37342643737793
Epoch 20, val loss: 1.9137799739837646
Epoch 30, training loss: 1.98421311378479 = 1.900485634803772 + 0.01 * 8.372746467590332
Epoch 30, val loss: 1.8967009782791138
Epoch 40, training loss: 1.9601755142211914 = 1.8764724731445312 + 0.01 * 8.3703031539917
Epoch 40, val loss: 1.8713743686676025
Epoch 50, training loss: 1.925511121749878 = 1.841961145401001 + 0.01 * 8.354992866516113
Epoch 50, val loss: 1.8362905979156494
Epoch 60, training loss: 1.8816032409667969 = 1.7994648218154907 + 0.01 * 8.213847160339355
Epoch 60, val loss: 1.7965831756591797
Epoch 70, training loss: 1.8334100246429443 = 1.756540298461914 + 0.01 * 7.686975955963135
Epoch 70, val loss: 1.7601157426834106
Epoch 80, training loss: 1.7781404256820679 = 1.705031394958496 + 0.01 * 7.310905933380127
Epoch 80, val loss: 1.7156810760498047
Epoch 90, training loss: 1.706095814704895 = 1.6342602968215942 + 0.01 * 7.18355131149292
Epoch 90, val loss: 1.6549307107925415
Epoch 100, training loss: 1.612816333770752 = 1.5416733026504517 + 0.01 * 7.114305019378662
Epoch 100, val loss: 1.5765820741653442
Epoch 110, training loss: 1.5043647289276123 = 1.4336036443710327 + 0.01 * 7.076104640960693
Epoch 110, val loss: 1.4873536825180054
Epoch 120, training loss: 1.389872670173645 = 1.3193001747131348 + 0.01 * 7.057249546051025
Epoch 120, val loss: 1.3967927694320679
Epoch 130, training loss: 1.2742730379104614 = 1.2038154602050781 + 0.01 * 7.045760631561279
Epoch 130, val loss: 1.3089675903320312
Epoch 140, training loss: 1.1597148180007935 = 1.0893603563308716 + 0.01 * 7.035445213317871
Epoch 140, val loss: 1.224862813949585
Epoch 150, training loss: 1.0486794710159302 = 0.9784404039382935 + 0.01 * 7.023909091949463
Epoch 150, val loss: 1.1445367336273193
Epoch 160, training loss: 0.9448366165161133 = 0.8747156262397766 + 0.01 * 7.012096881866455
Epoch 160, val loss: 1.0701615810394287
Epoch 170, training loss: 0.8518455028533936 = 0.7818213105201721 + 0.01 * 7.002419471740723
Epoch 170, val loss: 1.0040521621704102
Epoch 180, training loss: 0.7709057331085205 = 0.700945258140564 + 0.01 * 6.9960455894470215
Epoch 180, val loss: 0.9477967023849487
Epoch 190, training loss: 0.7004889845848083 = 0.6305673718452454 + 0.01 * 6.992160320281982
Epoch 190, val loss: 0.900868833065033
Epoch 200, training loss: 0.6380792260169983 = 0.5681785941123962 + 0.01 * 6.990062713623047
Epoch 200, val loss: 0.8617578744888306
Epoch 210, training loss: 0.5817738175392151 = 0.5118886828422546 + 0.01 * 6.98851203918457
Epoch 210, val loss: 0.8294158577919006
Epoch 220, training loss: 0.5308161377906799 = 0.46094799041748047 + 0.01 * 6.986814022064209
Epoch 220, val loss: 0.8036378622055054
Epoch 230, training loss: 0.484927773475647 = 0.41508206725120544 + 0.01 * 6.984570026397705
Epoch 230, val loss: 0.7839769721031189
Epoch 240, training loss: 0.44379156827926636 = 0.37396928668022156 + 0.01 * 6.982227325439453
Epoch 240, val loss: 0.7694177627563477
Epoch 250, training loss: 0.4069170653820038 = 0.3371207118034363 + 0.01 * 6.979635715484619
Epoch 250, val loss: 0.7591166496276855
Epoch 260, training loss: 0.3735998868942261 = 0.30382591485977173 + 0.01 * 6.977396011352539
Epoch 260, val loss: 0.752191960811615
Epoch 270, training loss: 0.34313467144966125 = 0.27339059114456177 + 0.01 * 6.974409103393555
Epoch 270, val loss: 0.7476879358291626
Epoch 280, training loss: 0.3149510622024536 = 0.24524018168449402 + 0.01 * 6.9710893630981445
Epoch 280, val loss: 0.7451719641685486
Epoch 290, training loss: 0.2887321710586548 = 0.2190481424331665 + 0.01 * 6.968404769897461
Epoch 290, val loss: 0.744263231754303
Epoch 300, training loss: 0.26437461376190186 = 0.194736048579216 + 0.01 * 6.963855266571045
Epoch 300, val loss: 0.7447568774223328
Epoch 310, training loss: 0.2419985979795456 = 0.17241375148296356 + 0.01 * 6.958484649658203
Epoch 310, val loss: 0.7466164827346802
Epoch 320, training loss: 0.22176679968833923 = 0.15224780142307281 + 0.01 * 6.95189905166626
Epoch 320, val loss: 0.7498420476913452
Epoch 330, training loss: 0.20372775197029114 = 0.13429704308509827 + 0.01 * 6.943071365356445
Epoch 330, val loss: 0.7542257308959961
Epoch 340, training loss: 0.1878540962934494 = 0.11850839108228683 + 0.01 * 6.934570789337158
Epoch 340, val loss: 0.7597877383232117
Epoch 350, training loss: 0.1740112155675888 = 0.10475336760282516 + 0.01 * 6.925785064697266
Epoch 350, val loss: 0.7662627100944519
Epoch 360, training loss: 0.16207601130008698 = 0.09283214062452316 + 0.01 * 6.924386978149414
Epoch 360, val loss: 0.7736238241195679
Epoch 370, training loss: 0.15160822868347168 = 0.08253617584705353 + 0.01 * 6.907206058502197
Epoch 370, val loss: 0.7816392183303833
Epoch 380, training loss: 0.14264169335365295 = 0.07364725321531296 + 0.01 * 6.89944314956665
Epoch 380, val loss: 0.7902045845985413
Epoch 390, training loss: 0.1350194662809372 = 0.06596021354198456 + 0.01 * 6.905925273895264
Epoch 390, val loss: 0.7992033362388611
Epoch 400, training loss: 0.1280895173549652 = 0.05929386615753174 + 0.01 * 6.879566192626953
Epoch 400, val loss: 0.8085432052612305
Epoch 410, training loss: 0.12220759689807892 = 0.05349140241742134 + 0.01 * 6.87161922454834
Epoch 410, val loss: 0.8181054592132568
Epoch 420, training loss: 0.11707176268100739 = 0.04842308908700943 + 0.01 * 6.864867210388184
Epoch 420, val loss: 0.827819287776947
Epoch 430, training loss: 0.11258827894926071 = 0.04397936165332794 + 0.01 * 6.860891819000244
Epoch 430, val loss: 0.8375995755195618
Epoch 440, training loss: 0.10865622758865356 = 0.04006963223218918 + 0.01 * 6.858659744262695
Epoch 440, val loss: 0.8473791480064392
Epoch 450, training loss: 0.10514666140079498 = 0.036612335592508316 + 0.01 * 6.8534321784973145
Epoch 450, val loss: 0.857092022895813
Epoch 460, training loss: 0.10208628326654434 = 0.03354717791080475 + 0.01 * 6.85391092300415
Epoch 460, val loss: 0.8667563796043396
Epoch 470, training loss: 0.09924540668725967 = 0.03082442283630371 + 0.01 * 6.842098712921143
Epoch 470, val loss: 0.8763378858566284
Epoch 480, training loss: 0.09668713808059692 = 0.028395995497703552 + 0.01 * 6.8291144371032715
Epoch 480, val loss: 0.8857864737510681
Epoch 490, training loss: 0.09446245431900024 = 0.026226114481687546 + 0.01 * 6.823634624481201
Epoch 490, val loss: 0.8950843214988708
Epoch 500, training loss: 0.09255584329366684 = 0.024282656610012054 + 0.01 * 6.827319145202637
Epoch 500, val loss: 0.904166042804718
Epoch 510, training loss: 0.0906287282705307 = 0.02253619208931923 + 0.01 * 6.809254169464111
Epoch 510, val loss: 0.9130909442901611
Epoch 520, training loss: 0.0891464427113533 = 0.02096322737634182 + 0.01 * 6.818321704864502
Epoch 520, val loss: 0.9218230843544006
Epoch 530, training loss: 0.08758965879678726 = 0.01954558491706848 + 0.01 * 6.804407596588135
Epoch 530, val loss: 0.9303082823753357
Epoch 540, training loss: 0.08616162091493607 = 0.01826290227472782 + 0.01 * 6.789872169494629
Epoch 540, val loss: 0.9386223554611206
Epoch 550, training loss: 0.08500401675701141 = 0.017098968848586082 + 0.01 * 6.790504455566406
Epoch 550, val loss: 0.9467264413833618
Epoch 560, training loss: 0.08383771032094955 = 0.016041360795497894 + 0.01 * 6.779635429382324
Epoch 560, val loss: 0.954634964466095
Epoch 570, training loss: 0.08297805488109589 = 0.015077856369316578 + 0.01 * 6.790019512176514
Epoch 570, val loss: 0.962318480014801
Epoch 580, training loss: 0.0819210410118103 = 0.014200408011674881 + 0.01 * 6.772063732147217
Epoch 580, val loss: 0.9698219299316406
Epoch 590, training loss: 0.08101152628660202 = 0.01339755393564701 + 0.01 * 6.761397838592529
Epoch 590, val loss: 0.9770982265472412
Epoch 600, training loss: 0.08025798201560974 = 0.012661091983318329 + 0.01 * 6.7596893310546875
Epoch 600, val loss: 0.9842126965522766
Epoch 610, training loss: 0.07989098131656647 = 0.011984841898083687 + 0.01 * 6.790614128112793
Epoch 610, val loss: 0.9911237955093384
Epoch 620, training loss: 0.07877474278211594 = 0.011363448575139046 + 0.01 * 6.741129398345947
Epoch 620, val loss: 0.9978795051574707
Epoch 630, training loss: 0.07815252244472504 = 0.010791667737066746 + 0.01 * 6.736085891723633
Epoch 630, val loss: 1.0043874979019165
Epoch 640, training loss: 0.07762569189071655 = 0.010265277698636055 + 0.01 * 6.736042022705078
Epoch 640, val loss: 1.0107580423355103
Epoch 650, training loss: 0.0770462304353714 = 0.009777244180440903 + 0.01 * 6.726898670196533
Epoch 650, val loss: 1.0169113874435425
Epoch 660, training loss: 0.07654865831136703 = 0.009325942024588585 + 0.01 * 6.722271919250488
Epoch 660, val loss: 1.0229582786560059
Epoch 670, training loss: 0.07591712474822998 = 0.008907507173717022 + 0.01 * 6.700962543487549
Epoch 670, val loss: 1.028809666633606
Epoch 680, training loss: 0.07557867467403412 = 0.008518155664205551 + 0.01 * 6.706052780151367
Epoch 680, val loss: 1.0344663858413696
Epoch 690, training loss: 0.07524619251489639 = 0.008156128227710724 + 0.01 * 6.7090067863464355
Epoch 690, val loss: 1.0400108098983765
Epoch 700, training loss: 0.07458889484405518 = 0.007818370126187801 + 0.01 * 6.6770524978637695
Epoch 700, val loss: 1.0453838109970093
Epoch 710, training loss: 0.07428166270256042 = 0.007502837106585503 + 0.01 * 6.677882194519043
Epoch 710, val loss: 1.0506672859191895
Epoch 720, training loss: 0.07390248775482178 = 0.007208226248621941 + 0.01 * 6.669425964355469
Epoch 720, val loss: 1.055777907371521
Epoch 730, training loss: 0.07352717220783234 = 0.006932709366083145 + 0.01 * 6.6594462394714355
Epoch 730, val loss: 1.0607447624206543
Epoch 740, training loss: 0.07325977087020874 = 0.006673930678516626 + 0.01 * 6.658584117889404
Epoch 740, val loss: 1.0655903816223145
Epoch 750, training loss: 0.07285025715827942 = 0.006431285757571459 + 0.01 * 6.641896724700928
Epoch 750, val loss: 1.070251226425171
Epoch 760, training loss: 0.07290834933519363 = 0.0062029086984694 + 0.01 * 6.670544147491455
Epoch 760, val loss: 1.0748355388641357
Epoch 770, training loss: 0.072486512362957 = 0.005988216958940029 + 0.01 * 6.649829864501953
Epoch 770, val loss: 1.0793230533599854
Epoch 780, training loss: 0.07230446487665176 = 0.005785491783171892 + 0.01 * 6.651896953582764
Epoch 780, val loss: 1.0836617946624756
Epoch 790, training loss: 0.07178013026714325 = 0.00559453759342432 + 0.01 * 6.618559837341309
Epoch 790, val loss: 1.087945580482483
Epoch 800, training loss: 0.07146994024515152 = 0.00541401794180274 + 0.01 * 6.605592727661133
Epoch 800, val loss: 1.0920642614364624
Epoch 810, training loss: 0.07144524902105331 = 0.005243175663053989 + 0.01 * 6.620207786560059
Epoch 810, val loss: 1.0961554050445557
Epoch 820, training loss: 0.07117071002721786 = 0.005081613082438707 + 0.01 * 6.608909606933594
Epoch 820, val loss: 1.1000458002090454
Epoch 830, training loss: 0.07112857699394226 = 0.004928676411509514 + 0.01 * 6.61998987197876
Epoch 830, val loss: 1.1039358377456665
Epoch 840, training loss: 0.07058394700288773 = 0.004783609416335821 + 0.01 * 6.580033779144287
Epoch 840, val loss: 1.1076887845993042
Epoch 850, training loss: 0.07042606174945831 = 0.004645752254873514 + 0.01 * 6.578031063079834
Epoch 850, val loss: 1.1113823652267456
Epoch 860, training loss: 0.07046402990818024 = 0.004514946602284908 + 0.01 * 6.594908237457275
Epoch 860, val loss: 1.115034818649292
Epoch 870, training loss: 0.0703202337026596 = 0.004390301648527384 + 0.01 * 6.59299373626709
Epoch 870, val loss: 1.1184927225112915
Epoch 880, training loss: 0.07010357826948166 = 0.0042718080803751945 + 0.01 * 6.58317756652832
Epoch 880, val loss: 1.121938705444336
Epoch 890, training loss: 0.06994528323411942 = 0.004158915486186743 + 0.01 * 6.57863712310791
Epoch 890, val loss: 1.1252881288528442
Epoch 900, training loss: 0.06959101557731628 = 0.004051210358738899 + 0.01 * 6.553980827331543
Epoch 900, val loss: 1.1285607814788818
Epoch 910, training loss: 0.0696546733379364 = 0.003948356956243515 + 0.01 * 6.570631504058838
Epoch 910, val loss: 1.1317884922027588
Epoch 920, training loss: 0.06951894611120224 = 0.0038502411916851997 + 0.01 * 6.566870212554932
Epoch 920, val loss: 1.1349644660949707
Epoch 930, training loss: 0.06924905627965927 = 0.0037565671373158693 + 0.01 * 6.549248695373535
Epoch 930, val loss: 1.1379731893539429
Epoch 940, training loss: 0.06909620016813278 = 0.003666935721412301 + 0.01 * 6.54292631149292
Epoch 940, val loss: 1.1409920454025269
Epoch 950, training loss: 0.06910529732704163 = 0.003581274300813675 + 0.01 * 6.552402019500732
Epoch 950, val loss: 1.1439628601074219
Epoch 960, training loss: 0.06896287202835083 = 0.0034992035944014788 + 0.01 * 6.546367168426514
Epoch 960, val loss: 1.1467515230178833
Epoch 970, training loss: 0.06863661110401154 = 0.0034205936826765537 + 0.01 * 6.521602153778076
Epoch 970, val loss: 1.1495603322982788
Epoch 980, training loss: 0.06844135373830795 = 0.00334517122246325 + 0.01 * 6.509618759155273
Epoch 980, val loss: 1.152314305305481
Epoch 990, training loss: 0.06852589547634125 = 0.0032728821970522404 + 0.01 * 6.525301456451416
Epoch 990, val loss: 1.1550039052963257
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.7749
Flip ASR: 0.7333/225 nodes
The final ASR:0.73924, 0.18700, Accuracy:0.80617, 0.00972
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11578])
remove edge: torch.Size([2, 9462])
updated graph: torch.Size([2, 10484])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98524, 0.00797, Accuracy:0.83210, 0.00924
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0188510417938232 = 1.935112714767456 + 0.01 * 8.37384033203125
Epoch 0, val loss: 1.9354835748672485
Epoch 10, training loss: 2.009551763534546 = 1.925813913345337 + 0.01 * 8.373785018920898
Epoch 10, val loss: 1.9268484115600586
Epoch 20, training loss: 1.9980875253677368 = 1.9143520593643188 + 0.01 * 8.373550415039062
Epoch 20, val loss: 1.915968418121338
Epoch 30, training loss: 1.9819215536117554 = 1.8981926441192627 + 0.01 * 8.372886657714844
Epoch 30, val loss: 1.9004803895950317
Epoch 40, training loss: 1.9577996730804443 = 1.874100685119629 + 0.01 * 8.369893074035645
Epoch 40, val loss: 1.8776946067810059
Epoch 50, training loss: 1.923144817352295 = 1.8396669626235962 + 0.01 * 8.347789764404297
Epoch 50, val loss: 1.8466377258300781
Epoch 60, training loss: 1.880601167678833 = 1.798664927482605 + 0.01 * 8.193621635437012
Epoch 60, val loss: 1.8130048513412476
Epoch 70, training loss: 1.8366987705230713 = 1.7580643892288208 + 0.01 * 7.863436222076416
Epoch 70, val loss: 1.7805018424987793
Epoch 80, training loss: 1.7822132110595703 = 1.7064082622528076 + 0.01 * 7.580494403839111
Epoch 80, val loss: 1.7346844673156738
Epoch 90, training loss: 1.7088332176208496 = 1.6351383924484253 + 0.01 * 7.369478225708008
Epoch 90, val loss: 1.6726112365722656
Epoch 100, training loss: 1.6154981851577759 = 1.5429067611694336 + 0.01 * 7.259148120880127
Epoch 100, val loss: 1.5965622663497925
Epoch 110, training loss: 1.5090006589889526 = 1.4373546838760376 + 0.01 * 7.164592266082764
Epoch 110, val loss: 1.5110702514648438
Epoch 120, training loss: 1.398945927619934 = 1.328283429145813 + 0.01 * 7.066251754760742
Epoch 120, val loss: 1.4242570400238037
Epoch 130, training loss: 1.2915953397750854 = 1.2216672897338867 + 0.01 * 6.992804527282715
Epoch 130, val loss: 1.340371012687683
Epoch 140, training loss: 1.1901865005493164 = 1.1206939220428467 + 0.01 * 6.949253082275391
Epoch 140, val loss: 1.2622476816177368
Epoch 150, training loss: 1.0974217653274536 = 1.0282303094863892 + 0.01 * 6.9191484451293945
Epoch 150, val loss: 1.1917866468429565
Epoch 160, training loss: 1.014374852180481 = 0.945418119430542 + 0.01 * 6.895678520202637
Epoch 160, val loss: 1.1299647092819214
Epoch 170, training loss: 0.9396781325340271 = 0.8708917498588562 + 0.01 * 6.878635883331299
Epoch 170, val loss: 1.0751553773880005
Epoch 180, training loss: 0.871576726436615 = 0.8029089570045471 + 0.01 * 6.86677885055542
Epoch 180, val loss: 1.0260951519012451
Epoch 190, training loss: 0.8087347745895386 = 0.7401742339134216 + 0.01 * 6.856051921844482
Epoch 190, val loss: 0.9815472364425659
Epoch 200, training loss: 0.7495851516723633 = 0.6811060905456543 + 0.01 * 6.847906589508057
Epoch 200, val loss: 0.9403202533721924
Epoch 210, training loss: 0.6920062303543091 = 0.6235764622688293 + 0.01 * 6.842978000640869
Epoch 210, val loss: 0.900642991065979
Epoch 220, training loss: 0.6342392563819885 = 0.5658414959907532 + 0.01 * 6.839775085449219
Epoch 220, val loss: 0.8605668544769287
Epoch 230, training loss: 0.5757685303688049 = 0.5074012875556946 + 0.01 * 6.836726188659668
Epoch 230, val loss: 0.8200269341468811
Epoch 240, training loss: 0.5175812244415283 = 0.44924184679985046 + 0.01 * 6.8339409828186035
Epoch 240, val loss: 0.7798442244529724
Epoch 250, training loss: 0.4618569016456604 = 0.3935464918613434 + 0.01 * 6.8310394287109375
Epoch 250, val loss: 0.7422854900360107
Epoch 260, training loss: 0.410887748003006 = 0.3426029682159424 + 0.01 * 6.828478813171387
Epoch 260, val loss: 0.7101036310195923
Epoch 270, training loss: 0.36596691608428955 = 0.29770731925964355 + 0.01 * 6.825961589813232
Epoch 270, val loss: 0.6846848726272583
Epoch 280, training loss: 0.3271317481994629 = 0.2588956356048584 + 0.01 * 6.823610305786133
Epoch 280, val loss: 0.6659479737281799
Epoch 290, training loss: 0.2935616970062256 = 0.22538107633590698 + 0.01 * 6.818062782287598
Epoch 290, val loss: 0.6528679132461548
Epoch 300, training loss: 0.2643837332725525 = 0.19622142612934113 + 0.01 * 6.816231727600098
Epoch 300, val loss: 0.6444769501686096
Epoch 310, training loss: 0.23879405856132507 = 0.1706872284412384 + 0.01 * 6.81068229675293
Epoch 310, val loss: 0.6400347948074341
Epoch 320, training loss: 0.21646898984909058 = 0.148346409201622 + 0.01 * 6.812257289886475
Epoch 320, val loss: 0.6390316486358643
Epoch 330, training loss: 0.19700008630752563 = 0.1289481520652771 + 0.01 * 6.805192947387695
Epoch 330, val loss: 0.6411822438240051
Epoch 340, training loss: 0.18025267124176025 = 0.11226829886436462 + 0.01 * 6.79843807220459
Epoch 340, val loss: 0.6462124586105347
Epoch 350, training loss: 0.16601000726222992 = 0.09804902225732803 + 0.01 * 6.796098709106445
Epoch 350, val loss: 0.6538355946540833
Epoch 360, training loss: 0.15389122068881989 = 0.08599945902824402 + 0.01 * 6.7891764640808105
Epoch 360, val loss: 0.663641095161438
Epoch 370, training loss: 0.1436254382133484 = 0.07579730451107025 + 0.01 * 6.782813549041748
Epoch 370, val loss: 0.6750985383987427
Epoch 380, training loss: 0.1350906789302826 = 0.06714849174022675 + 0.01 * 6.79421854019165
Epoch 380, val loss: 0.6875379681587219
Epoch 390, training loss: 0.12762729823589325 = 0.05980157107114792 + 0.01 * 6.7825727462768555
Epoch 390, val loss: 0.7004001140594482
Epoch 400, training loss: 0.12120918929576874 = 0.053520962595939636 + 0.01 * 6.76882266998291
Epoch 400, val loss: 0.7133539915084839
Epoch 410, training loss: 0.1157560646533966 = 0.04811525717377663 + 0.01 * 6.764081001281738
Epoch 410, val loss: 0.7260787487030029
Epoch 420, training loss: 0.11102692037820816 = 0.0434284582734108 + 0.01 * 6.7598466873168945
Epoch 420, val loss: 0.7384456396102905
Epoch 430, training loss: 0.10689698904752731 = 0.03933989256620407 + 0.01 * 6.755710124969482
Epoch 430, val loss: 0.7503990530967712
Epoch 440, training loss: 0.1033409982919693 = 0.03575902059674263 + 0.01 * 6.75819730758667
Epoch 440, val loss: 0.7619150876998901
Epoch 450, training loss: 0.10010817646980286 = 0.03261151909828186 + 0.01 * 6.749666213989258
Epoch 450, val loss: 0.7731118202209473
Epoch 460, training loss: 0.09724654257297516 = 0.029826516285538673 + 0.01 * 6.742002964019775
Epoch 460, val loss: 0.7840219140052795
Epoch 470, training loss: 0.09473061561584473 = 0.02735886164009571 + 0.01 * 6.737175941467285
Epoch 470, val loss: 0.7945994734764099
Epoch 480, training loss: 0.0925295427441597 = 0.025168104097247124 + 0.01 * 6.736144065856934
Epoch 480, val loss: 0.8048943281173706
Epoch 490, training loss: 0.09051841497421265 = 0.02321755141019821 + 0.01 * 6.730086803436279
Epoch 490, val loss: 0.8148370981216431
Epoch 500, training loss: 0.08883285522460938 = 0.021475981920957565 + 0.01 * 6.735687255859375
Epoch 500, val loss: 0.8245334625244141
Epoch 510, training loss: 0.087175153195858 = 0.019917098805308342 + 0.01 * 6.725805282592773
Epoch 510, val loss: 0.8340175747871399
Epoch 520, training loss: 0.08569005131721497 = 0.018517401069402695 + 0.01 * 6.717265605926514
Epoch 520, val loss: 0.8431370258331299
Epoch 530, training loss: 0.08436284214258194 = 0.017257312312722206 + 0.01 * 6.710553169250488
Epoch 530, val loss: 0.8520392179489136
Epoch 540, training loss: 0.08331552147865295 = 0.016120415180921555 + 0.01 * 6.71951150894165
Epoch 540, val loss: 0.86064612865448
Epoch 550, training loss: 0.08212809264659882 = 0.015093741938471794 + 0.01 * 6.703434944152832
Epoch 550, val loss: 0.868988573551178
Epoch 560, training loss: 0.08114936947822571 = 0.014162219129502773 + 0.01 * 6.698714733123779
Epoch 560, val loss: 0.8770871758460999
Epoch 570, training loss: 0.0804092064499855 = 0.013314677402377129 + 0.01 * 6.709453105926514
Epoch 570, val loss: 0.884955108165741
Epoch 580, training loss: 0.0795237347483635 = 0.01254244428128004 + 0.01 * 6.698129653930664
Epoch 580, val loss: 0.8926254510879517
Epoch 590, training loss: 0.07874172925949097 = 0.011836901307106018 + 0.01 * 6.690483093261719
Epoch 590, val loss: 0.9000312685966492
Epoch 600, training loss: 0.07797573506832123 = 0.011190364137291908 + 0.01 * 6.678537368774414
Epoch 600, val loss: 0.9072301983833313
Epoch 610, training loss: 0.07755005359649658 = 0.010596513748168945 + 0.01 * 6.695354461669922
Epoch 610, val loss: 0.9142454862594604
Epoch 620, training loss: 0.07687202841043472 = 0.010051243007183075 + 0.01 * 6.6820783615112305
Epoch 620, val loss: 0.9209878444671631
Epoch 630, training loss: 0.07628398388624191 = 0.009548979811370373 + 0.01 * 6.673501014709473
Epoch 630, val loss: 0.9276098608970642
Epoch 640, training loss: 0.07569004595279694 = 0.009085118770599365 + 0.01 * 6.660492897033691
Epoch 640, val loss: 0.934044361114502
Epoch 650, training loss: 0.07530446350574493 = 0.008655797690153122 + 0.01 * 6.6648664474487305
Epoch 650, val loss: 0.9403266906738281
Epoch 660, training loss: 0.0748555064201355 = 0.008258089423179626 + 0.01 * 6.6597418785095215
Epoch 660, val loss: 0.9464387893676758
Epoch 670, training loss: 0.07455393671989441 = 0.00788887683302164 + 0.01 * 6.666506290435791
Epoch 670, val loss: 0.9523094892501831
Epoch 680, training loss: 0.07400420308113098 = 0.007545805536210537 + 0.01 * 6.645840167999268
Epoch 680, val loss: 0.9581435322761536
Epoch 690, training loss: 0.07383016496896744 = 0.007226166315376759 + 0.01 * 6.660399436950684
Epoch 690, val loss: 0.9637486338615417
Epoch 700, training loss: 0.07336987555027008 = 0.006927942391484976 + 0.01 * 6.644193172454834
Epoch 700, val loss: 0.9693223834037781
Epoch 710, training loss: 0.07295197993516922 = 0.0066491710022091866 + 0.01 * 6.6302809715271
Epoch 710, val loss: 0.9746807217597961
Epoch 720, training loss: 0.07278334349393845 = 0.006388607434928417 + 0.01 * 6.639473915100098
Epoch 720, val loss: 0.9799630641937256
Epoch 730, training loss: 0.07244855910539627 = 0.006144954822957516 + 0.01 * 6.630360126495361
Epoch 730, val loss: 0.9851006269454956
Epoch 740, training loss: 0.07226181030273438 = 0.005916256923228502 + 0.01 * 6.634555339813232
Epoch 740, val loss: 0.9901160597801208
Epoch 750, training loss: 0.07199046015739441 = 0.005701576825231314 + 0.01 * 6.628888130187988
Epoch 750, val loss: 0.9949542284011841
Epoch 760, training loss: 0.07169627398252487 = 0.0054997773841023445 + 0.01 * 6.619649887084961
Epoch 760, val loss: 0.9997645616531372
Epoch 770, training loss: 0.07167643308639526 = 0.005309540778398514 + 0.01 * 6.63668966293335
Epoch 770, val loss: 1.004417061805725
Epoch 780, training loss: 0.07126222550868988 = 0.00513030868023634 + 0.01 * 6.613192081451416
Epoch 780, val loss: 1.0089788436889648
Epoch 790, training loss: 0.07092823088169098 = 0.004961079452186823 + 0.01 * 6.596714973449707
Epoch 790, val loss: 1.0134902000427246
Epoch 800, training loss: 0.07088552415370941 = 0.004801017697900534 + 0.01 * 6.608450412750244
Epoch 800, val loss: 1.01784086227417
Epoch 810, training loss: 0.07065334171056747 = 0.004649767652153969 + 0.01 * 6.600358009338379
Epoch 810, val loss: 1.0221940279006958
Epoch 820, training loss: 0.0707610547542572 = 0.004506584722548723 + 0.01 * 6.625446796417236
Epoch 820, val loss: 1.0264521837234497
Epoch 830, training loss: 0.07022389769554138 = 0.004370854236185551 + 0.01 * 6.5853047370910645
Epoch 830, val loss: 1.0305250883102417
Epoch 840, training loss: 0.07013202458620071 = 0.004242143128067255 + 0.01 * 6.588988304138184
Epoch 840, val loss: 1.0344815254211426
Epoch 850, training loss: 0.06993331760168076 = 0.004119918681681156 + 0.01 * 6.581340312957764
Epoch 850, val loss: 1.0384619235992432
Epoch 860, training loss: 0.06975187361240387 = 0.004003917332738638 + 0.01 * 6.574795246124268
Epoch 860, val loss: 1.042346715927124
Epoch 870, training loss: 0.06973437964916229 = 0.0038935081101953983 + 0.01 * 6.584086894989014
Epoch 870, val loss: 1.0461061000823975
Epoch 880, training loss: 0.06950519979000092 = 0.0037883855402469635 + 0.01 * 6.571681022644043
Epoch 880, val loss: 1.0498460531234741
Epoch 890, training loss: 0.06938289105892181 = 0.003688181284815073 + 0.01 * 6.56947135925293
Epoch 890, val loss: 1.0535016059875488
Epoch 900, training loss: 0.06928909569978714 = 0.0035926569253206253 + 0.01 * 6.569644451141357
Epoch 900, val loss: 1.057045817375183
Epoch 910, training loss: 0.0692388042807579 = 0.003501610131934285 + 0.01 * 6.573719501495361
Epoch 910, val loss: 1.0605789422988892
Epoch 920, training loss: 0.06923151761293411 = 0.0034146348480135202 + 0.01 * 6.581687927246094
Epoch 920, val loss: 1.0640126466751099
Epoch 930, training loss: 0.06888008117675781 = 0.0033314956817775965 + 0.01 * 6.554858684539795
Epoch 930, val loss: 1.0673960447311401
Epoch 940, training loss: 0.06912696361541748 = 0.003252017078921199 + 0.01 * 6.58749532699585
Epoch 940, val loss: 1.0707225799560547
Epoch 950, training loss: 0.06868980824947357 = 0.0031759918201714754 + 0.01 * 6.551382064819336
Epoch 950, val loss: 1.073930263519287
Epoch 960, training loss: 0.06879997253417969 = 0.0031031148973852396 + 0.01 * 6.569685935974121
Epoch 960, val loss: 1.0770807266235352
Epoch 970, training loss: 0.06852277368307114 = 0.003033369081094861 + 0.01 * 6.548940658569336
Epoch 970, val loss: 1.0802325010299683
Epoch 980, training loss: 0.06849827617406845 = 0.0029666295740753412 + 0.01 * 6.553164958953857
Epoch 980, val loss: 1.0833461284637451
Epoch 990, training loss: 0.0682678371667862 = 0.0029024735558778048 + 0.01 * 6.536537170410156
Epoch 990, val loss: 1.0862648487091064
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.7306
Flip ASR: 0.6800/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0371482372283936 = 1.9534094333648682 + 0.01 * 8.373884201049805
Epoch 0, val loss: 1.9542012214660645
Epoch 10, training loss: 2.0262908935546875 = 1.9425526857376099 + 0.01 * 8.37382984161377
Epoch 10, val loss: 1.9429961442947388
Epoch 20, training loss: 2.0127787590026855 = 1.929042100906372 + 0.01 * 8.37366008758545
Epoch 20, val loss: 1.9288198947906494
Epoch 30, training loss: 1.9937127828598022 = 1.9099805355072021 + 0.01 * 8.373223304748535
Epoch 30, val loss: 1.9089428186416626
Epoch 40, training loss: 1.9656007289886475 = 1.8818867206573486 + 0.01 * 8.371399879455566
Epoch 40, val loss: 1.880335807800293
Epoch 50, training loss: 1.926087737083435 = 1.8425143957138062 + 0.01 * 8.35733413696289
Epoch 50, val loss: 1.8420023918151855
Epoch 60, training loss: 1.8792043924331665 = 1.7966099977493286 + 0.01 * 8.259444236755371
Epoch 60, val loss: 1.8007434606552124
Epoch 70, training loss: 1.8309389352798462 = 1.7513772249221802 + 0.01 * 7.956167697906494
Epoch 70, val loss: 1.7619705200195312
Epoch 80, training loss: 1.7718212604522705 = 1.694212794303894 + 0.01 * 7.7608513832092285
Epoch 80, val loss: 1.7104867696762085
Epoch 90, training loss: 1.6923277378082275 = 1.6163541078567505 + 0.01 * 7.597367763519287
Epoch 90, val loss: 1.64151930809021
Epoch 100, training loss: 1.5931531190872192 = 1.5190935134887695 + 0.01 * 7.405955791473389
Epoch 100, val loss: 1.5585969686508179
Epoch 110, training loss: 1.4876450300216675 = 1.4146472215652466 + 0.01 * 7.2997822761535645
Epoch 110, val loss: 1.472109079360962
Epoch 120, training loss: 1.3866045475006104 = 1.3138728141784668 + 0.01 * 7.2731757164001465
Epoch 120, val loss: 1.3945512771606445
Epoch 130, training loss: 1.291904330253601 = 1.2194924354553223 + 0.01 * 7.241188049316406
Epoch 130, val loss: 1.3259271383285522
Epoch 140, training loss: 1.2013633251190186 = 1.1293795108795166 + 0.01 * 7.198384761810303
Epoch 140, val loss: 1.262209177017212
Epoch 150, training loss: 1.112060546875 = 1.040619134902954 + 0.01 * 7.144141674041748
Epoch 150, val loss: 1.2008525133132935
Epoch 160, training loss: 1.0227850675582886 = 0.9518589973449707 + 0.01 * 7.09260368347168
Epoch 160, val loss: 1.1399489641189575
Epoch 170, training loss: 0.9358887076377869 = 0.8653959631919861 + 0.01 * 7.049274921417236
Epoch 170, val loss: 1.0812519788742065
Epoch 180, training loss: 0.8557960987091064 = 0.7855728268623352 + 0.01 * 7.022325038909912
Epoch 180, val loss: 1.028080701828003
Epoch 190, training loss: 0.7854556441307068 = 0.7154959440231323 + 0.01 * 6.995970249176025
Epoch 190, val loss: 0.9837847948074341
Epoch 200, training loss: 0.7248563766479492 = 0.6550433039665222 + 0.01 * 6.981308460235596
Epoch 200, val loss: 0.9486758708953857
Epoch 210, training loss: 0.6718525290489197 = 0.6021441221237183 + 0.01 * 6.970843315124512
Epoch 210, val loss: 0.9212462306022644
Epoch 220, training loss: 0.6241583228111267 = 0.5545351505279541 + 0.01 * 6.962315559387207
Epoch 220, val loss: 0.8992908000946045
Epoch 230, training loss: 0.5802085995674133 = 0.5106691718101501 + 0.01 * 6.953941822052002
Epoch 230, val loss: 0.8812668919563293
Epoch 240, training loss: 0.5390220284461975 = 0.46957021951675415 + 0.01 * 6.9451823234558105
Epoch 240, val loss: 0.866165816783905
Epoch 250, training loss: 0.5000171065330505 = 0.43064171075820923 + 0.01 * 6.9375386238098145
Epoch 250, val loss: 0.8537926077842712
Epoch 260, training loss: 0.4628804922103882 = 0.39360561966896057 + 0.01 * 6.927488327026367
Epoch 260, val loss: 0.8444202542304993
Epoch 270, training loss: 0.42763736844062805 = 0.35836389660835266 + 0.01 * 6.927346229553223
Epoch 270, val loss: 0.8382773995399475
Epoch 280, training loss: 0.3940930664539337 = 0.32497894763946533 + 0.01 * 6.911412715911865
Epoch 280, val loss: 0.8353660106658936
Epoch 290, training loss: 0.36268898844718933 = 0.2936578691005707 + 0.01 * 6.903111934661865
Epoch 290, val loss: 0.8354159593582153
Epoch 300, training loss: 0.33349844813346863 = 0.2645602524280548 + 0.01 * 6.8938188552856445
Epoch 300, val loss: 0.8381438851356506
Epoch 310, training loss: 0.30676063895225525 = 0.2378765344619751 + 0.01 * 6.888409614562988
Epoch 310, val loss: 0.8435291051864624
Epoch 320, training loss: 0.2824857831001282 = 0.21372047066688538 + 0.01 * 6.876532077789307
Epoch 320, val loss: 0.8513964414596558
Epoch 330, training loss: 0.2608760595321655 = 0.19216787815093994 + 0.01 * 6.8708176612854
Epoch 330, val loss: 0.8615632057189941
Epoch 340, training loss: 0.24183818697929382 = 0.17303569614887238 + 0.01 * 6.880249977111816
Epoch 340, val loss: 0.8737431764602661
Epoch 350, training loss: 0.22469596564769745 = 0.15607935190200806 + 0.01 * 6.861661434173584
Epoch 350, val loss: 0.887916624546051
Epoch 360, training loss: 0.20954051613807678 = 0.14098100364208221 + 0.01 * 6.855950832366943
Epoch 360, val loss: 0.9037359952926636
Epoch 370, training loss: 0.19588342308998108 = 0.12743838131427765 + 0.01 * 6.844504356384277
Epoch 370, val loss: 0.9207387566566467
Epoch 380, training loss: 0.18363775312900543 = 0.11523886024951935 + 0.01 * 6.8398895263671875
Epoch 380, val loss: 0.9385864734649658
Epoch 390, training loss: 0.1725526750087738 = 0.1041952595114708 + 0.01 * 6.835741996765137
Epoch 390, val loss: 0.957140326499939
Epoch 400, training loss: 0.16233693063259125 = 0.09403309971094131 + 0.01 * 6.83038330078125
Epoch 400, val loss: 0.9761807918548584
Epoch 410, training loss: 0.15286663174629211 = 0.08463259786367416 + 0.01 * 6.8234028816223145
Epoch 410, val loss: 0.9955747127532959
Epoch 420, training loss: 0.14408931136131287 = 0.07577406615018845 + 0.01 * 6.8315253257751465
Epoch 420, val loss: 1.0151387453079224
Epoch 430, training loss: 0.1356813758611679 = 0.06746622174978256 + 0.01 * 6.8215155601501465
Epoch 430, val loss: 1.0347657203674316
Epoch 440, training loss: 0.12800858914852142 = 0.05993848666548729 + 0.01 * 6.807010650634766
Epoch 440, val loss: 1.0544167757034302
Epoch 450, training loss: 0.12153726816177368 = 0.053491126745939255 + 0.01 * 6.804615020751953
Epoch 450, val loss: 1.0743739604949951
Epoch 460, training loss: 0.11602295935153961 = 0.048039793968200684 + 0.01 * 6.798316955566406
Epoch 460, val loss: 1.0950663089752197
Epoch 470, training loss: 0.11135874688625336 = 0.04339360073208809 + 0.01 * 6.79651403427124
Epoch 470, val loss: 1.1156713962554932
Epoch 480, training loss: 0.107249915599823 = 0.03939061239361763 + 0.01 * 6.785930633544922
Epoch 480, val loss: 1.1357370615005493
Epoch 490, training loss: 0.10374055802822113 = 0.035898249596357346 + 0.01 * 6.784231185913086
Epoch 490, val loss: 1.1550041437149048
Epoch 500, training loss: 0.10069969296455383 = 0.032834459096193314 + 0.01 * 6.786523342132568
Epoch 500, val loss: 1.1736787557601929
Epoch 510, training loss: 0.09805479645729065 = 0.03012988716363907 + 0.01 * 6.792491436004639
Epoch 510, val loss: 1.1916778087615967
Epoch 520, training loss: 0.0953640341758728 = 0.027734389528632164 + 0.01 * 6.762964248657227
Epoch 520, val loss: 1.2091368436813354
Epoch 530, training loss: 0.09319829195737839 = 0.02560359425842762 + 0.01 * 6.759469985961914
Epoch 530, val loss: 1.2260938882827759
Epoch 540, training loss: 0.09127016365528107 = 0.023700889199972153 + 0.01 * 6.756927013397217
Epoch 540, val loss: 1.2425600290298462
Epoch 550, training loss: 0.08940717577934265 = 0.02199314348399639 + 0.01 * 6.741403102874756
Epoch 550, val loss: 1.258592963218689
Epoch 560, training loss: 0.08806264400482178 = 0.020454032346606255 + 0.01 * 6.760861396789551
Epoch 560, val loss: 1.2740920782089233
Epoch 570, training loss: 0.08635168522596359 = 0.019064923748373985 + 0.01 * 6.7286763191223145
Epoch 570, val loss: 1.289231777191162
Epoch 580, training loss: 0.08516453206539154 = 0.017807621508836746 + 0.01 * 6.735691070556641
Epoch 580, val loss: 1.3039971590042114
Epoch 590, training loss: 0.08380156755447388 = 0.016668520867824554 + 0.01 * 6.7133049964904785
Epoch 590, val loss: 1.3183040618896484
Epoch 600, training loss: 0.08266729861497879 = 0.015633059665560722 + 0.01 * 6.703423976898193
Epoch 600, val loss: 1.3322508335113525
Epoch 610, training loss: 0.08205785602331161 = 0.014690699987113476 + 0.01 * 6.736715793609619
Epoch 610, val loss: 1.3457276821136475
Epoch 620, training loss: 0.08075464516878128 = 0.013831651769578457 + 0.01 * 6.6922993659973145
Epoch 620, val loss: 1.3587909936904907
Epoch 630, training loss: 0.0798935741186142 = 0.013046018779277802 + 0.01 * 6.684755325317383
Epoch 630, val loss: 1.3716148138046265
Epoch 640, training loss: 0.07926907390356064 = 0.012325944378972054 + 0.01 * 6.6943135261535645
Epoch 640, val loss: 1.384041428565979
Epoch 650, training loss: 0.07850104570388794 = 0.01166528183966875 + 0.01 * 6.683576583862305
Epoch 650, val loss: 1.3961050510406494
Epoch 660, training loss: 0.07769617438316345 = 0.011057302355766296 + 0.01 * 6.663887023925781
Epoch 660, val loss: 1.4079077243804932
Epoch 670, training loss: 0.07717497646808624 = 0.010496844537556171 + 0.01 * 6.667813301086426
Epoch 670, val loss: 1.4192503690719604
Epoch 680, training loss: 0.07685388624668121 = 0.009980187751352787 + 0.01 * 6.6873698234558105
Epoch 680, val loss: 1.430331826210022
Epoch 690, training loss: 0.0761309564113617 = 0.009502260014414787 + 0.01 * 6.662869930267334
Epoch 690, val loss: 1.4412460327148438
Epoch 700, training loss: 0.0754881501197815 = 0.009058967232704163 + 0.01 * 6.642918109893799
Epoch 700, val loss: 1.4517027139663696
Epoch 710, training loss: 0.07508008182048798 = 0.008647487498819828 + 0.01 * 6.6432600021362305
Epoch 710, val loss: 1.4619637727737427
Epoch 720, training loss: 0.07468556612730026 = 0.008264481090009212 + 0.01 * 6.642108917236328
Epoch 720, val loss: 1.4720309972763062
Epoch 730, training loss: 0.07444405555725098 = 0.007908452302217484 + 0.01 * 6.653561115264893
Epoch 730, val loss: 1.481680154800415
Epoch 740, training loss: 0.07380452752113342 = 0.007576691918075085 + 0.01 * 6.622783660888672
Epoch 740, val loss: 1.4911659955978394
Epoch 750, training loss: 0.07346100360155106 = 0.007266860920935869 + 0.01 * 6.619414329528809
Epoch 750, val loss: 1.5003498792648315
Epoch 760, training loss: 0.07341925799846649 = 0.006977295968681574 + 0.01 * 6.644196033477783
Epoch 760, val loss: 1.5092726945877075
Epoch 770, training loss: 0.07274878770112991 = 0.006706325337290764 + 0.01 * 6.604246139526367
Epoch 770, val loss: 1.5181050300598145
Epoch 780, training loss: 0.07260119915008545 = 0.00645215529948473 + 0.01 * 6.614903926849365
Epoch 780, val loss: 1.5266276597976685
Epoch 790, training loss: 0.07227832078933716 = 0.006213668268173933 + 0.01 * 6.606465816497803
Epoch 790, val loss: 1.5349414348602295
Epoch 800, training loss: 0.07188763469457626 = 0.0059895566664636135 + 0.01 * 6.589807510375977
Epoch 800, val loss: 1.5430500507354736
Epoch 810, training loss: 0.07163885980844498 = 0.005778464954346418 + 0.01 * 6.586040019989014
Epoch 810, val loss: 1.5509676933288574
Epoch 820, training loss: 0.0715232789516449 = 0.005579664837568998 + 0.01 * 6.594361782073975
Epoch 820, val loss: 1.5587060451507568
Epoch 830, training loss: 0.07117529213428497 = 0.0053923167288303375 + 0.01 * 6.578297138214111
Epoch 830, val loss: 1.5662355422973633
Epoch 840, training loss: 0.07098829001188278 = 0.005215361248701811 + 0.01 * 6.5772929191589355
Epoch 840, val loss: 1.5735819339752197
Epoch 850, training loss: 0.07091261446475983 = 0.005048099905252457 + 0.01 * 6.586451530456543
Epoch 850, val loss: 1.5808348655700684
Epoch 860, training loss: 0.07049482315778732 = 0.004889841191470623 + 0.01 * 6.5604987144470215
Epoch 860, val loss: 1.5879192352294922
Epoch 870, training loss: 0.0703282281756401 = 0.0047399080358445644 + 0.01 * 6.558832168579102
Epoch 870, val loss: 1.5947766304016113
Epoch 880, training loss: 0.07021747529506683 = 0.004597870167344809 + 0.01 * 6.561960220336914
Epoch 880, val loss: 1.601467490196228
Epoch 890, training loss: 0.07027488201856613 = 0.004463092889636755 + 0.01 * 6.581179141998291
Epoch 890, val loss: 1.6080913543701172
Epoch 900, training loss: 0.0698668360710144 = 0.004334878176450729 + 0.01 * 6.553196430206299
Epoch 900, val loss: 1.6145533323287964
Epoch 910, training loss: 0.06953991204500198 = 0.004212959669530392 + 0.01 * 6.532695293426514
Epoch 910, val loss: 1.6208620071411133
Epoch 920, training loss: 0.0695623904466629 = 0.004096973221749067 + 0.01 * 6.546542167663574
Epoch 920, val loss: 1.6269906759262085
Epoch 930, training loss: 0.06936084479093552 = 0.003986656200140715 + 0.01 * 6.537418842315674
Epoch 930, val loss: 1.6331509351730347
Epoch 940, training loss: 0.06960638612508774 = 0.003881253534927964 + 0.01 * 6.572513580322266
Epoch 940, val loss: 1.6390459537506104
Epoch 950, training loss: 0.06902331113815308 = 0.0037808986380696297 + 0.01 * 6.5242414474487305
Epoch 950, val loss: 1.6447957754135132
Epoch 960, training loss: 0.06890082359313965 = 0.00368521804921329 + 0.01 * 6.521560192108154
Epoch 960, val loss: 1.6504878997802734
Epoch 970, training loss: 0.06893694400787354 = 0.003593754256144166 + 0.01 * 6.534318447113037
Epoch 970, val loss: 1.6560614109039307
Epoch 980, training loss: 0.06873126327991486 = 0.0035062425304204226 + 0.01 * 6.5225019454956055
Epoch 980, val loss: 1.661468505859375
Epoch 990, training loss: 0.06850848346948624 = 0.0034225403796881437 + 0.01 * 6.508594512939453
Epoch 990, val loss: 1.6668461561203003
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.6273
Flip ASR: 0.5733/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0356664657592773 = 1.9519280195236206 + 0.01 * 8.373845100402832
Epoch 0, val loss: 1.9499655961990356
Epoch 10, training loss: 2.024545669555664 = 1.9408081769943237 + 0.01 * 8.373741149902344
Epoch 10, val loss: 1.939112901687622
Epoch 20, training loss: 2.010526180267334 = 1.926791787147522 + 0.01 * 8.373444557189941
Epoch 20, val loss: 1.92489492893219
Epoch 30, training loss: 1.9909166097640991 = 1.9071910381317139 + 0.01 * 8.372560501098633
Epoch 30, val loss: 1.9044935703277588
Epoch 40, training loss: 1.9624245166778564 = 1.8787450790405273 + 0.01 * 8.367945671081543
Epoch 40, val loss: 1.8750598430633545
Epoch 50, training loss: 1.9229272603988647 = 1.8395583629608154 + 0.01 * 8.336894035339355
Epoch 50, val loss: 1.8360295295715332
Epoch 60, training loss: 1.8752176761627197 = 1.7937695980072021 + 0.01 * 8.144803047180176
Epoch 60, val loss: 1.7934889793395996
Epoch 70, training loss: 1.8267042636871338 = 1.7475687265396118 + 0.01 * 7.913557529449463
Epoch 70, val loss: 1.7519912719726562
Epoch 80, training loss: 1.7679078578948975 = 1.690677285194397 + 0.01 * 7.723061561584473
Epoch 80, val loss: 1.7001886367797852
Epoch 90, training loss: 1.690500259399414 = 1.6143035888671875 + 0.01 * 7.6196722984313965
Epoch 90, val loss: 1.6348388195037842
Epoch 100, training loss: 1.5945091247558594 = 1.5197089910507202 + 0.01 * 7.480015754699707
Epoch 100, val loss: 1.5587862730026245
Epoch 110, training loss: 1.4940191507339478 = 1.4215638637542725 + 0.01 * 7.245529651641846
Epoch 110, val loss: 1.4823206663131714
Epoch 120, training loss: 1.4033571481704712 = 1.331874966621399 + 0.01 * 7.148216724395752
Epoch 120, val loss: 1.4160534143447876
Epoch 130, training loss: 1.3252389430999756 = 1.2543518543243408 + 0.01 * 7.088712215423584
Epoch 130, val loss: 1.36215078830719
Epoch 140, training loss: 1.2567201852798462 = 1.1861764192581177 + 0.01 * 7.054375648498535
Epoch 140, val loss: 1.316467523574829
Epoch 150, training loss: 1.192713737487793 = 1.1224255561828613 + 0.01 * 7.028823375701904
Epoch 150, val loss: 1.2757806777954102
Epoch 160, training loss: 1.1282904148101807 = 1.0582525730133057 + 0.01 * 7.0037841796875
Epoch 160, val loss: 1.2364858388900757
Epoch 170, training loss: 1.060746669769287 = 0.9909043908119202 + 0.01 * 6.984229564666748
Epoch 170, val loss: 1.1951593160629272
Epoch 180, training loss: 0.9905277490615845 = 0.9207939505577087 + 0.01 * 6.973382472991943
Epoch 180, val loss: 1.1507453918457031
Epoch 190, training loss: 0.9195546507835388 = 0.8498637676239014 + 0.01 * 6.969086170196533
Epoch 190, val loss: 1.1045817136764526
Epoch 200, training loss: 0.8491055965423584 = 0.7794262766838074 + 0.01 * 6.967928886413574
Epoch 200, val loss: 1.0576095581054688
Epoch 210, training loss: 0.7798480987548828 = 0.7101669311523438 + 0.01 * 6.968118190765381
Epoch 210, val loss: 1.010622501373291
Epoch 220, training loss: 0.7127737402915955 = 0.6430953145027161 + 0.01 * 6.967844009399414
Epoch 220, val loss: 0.9644007682800293
Epoch 230, training loss: 0.6491020917892456 = 0.5794243216514587 + 0.01 * 6.967775344848633
Epoch 230, val loss: 0.9203166365623474
Epoch 240, training loss: 0.5897148251533508 = 0.5200413465499878 + 0.01 * 6.967345714569092
Epoch 240, val loss: 0.8798332810401917
Epoch 250, training loss: 0.5348919630050659 = 0.4652290940284729 + 0.01 * 6.966287136077881
Epoch 250, val loss: 0.8442609906196594
Epoch 260, training loss: 0.4843282401561737 = 0.4146822690963745 + 0.01 * 6.964597702026367
Epoch 260, val loss: 0.8146258592605591
Epoch 270, training loss: 0.43730074167251587 = 0.367672324180603 + 0.01 * 6.962841033935547
Epoch 270, val loss: 0.7903439998626709
Epoch 280, training loss: 0.3933402895927429 = 0.323737770318985 + 0.01 * 6.960251331329346
Epoch 280, val loss: 0.7709366083145142
Epoch 290, training loss: 0.3525027632713318 = 0.28293120861053467 + 0.01 * 6.957156658172607
Epoch 290, val loss: 0.7558768391609192
Epoch 300, training loss: 0.3152943253517151 = 0.24572789669036865 + 0.01 * 6.956644535064697
Epoch 300, val loss: 0.744671106338501
Epoch 310, training loss: 0.28201448917388916 = 0.21249856054782867 + 0.01 * 6.951594829559326
Epoch 310, val loss: 0.7367143034934998
Epoch 320, training loss: 0.2528829872608185 = 0.18340645730495453 + 0.01 * 6.947652816772461
Epoch 320, val loss: 0.7318254113197327
Epoch 330, training loss: 0.22769403457641602 = 0.15827146172523499 + 0.01 * 6.942256927490234
Epoch 330, val loss: 0.7295794486999512
Epoch 340, training loss: 0.20603293180465698 = 0.13665877282619476 + 0.01 * 6.937417030334473
Epoch 340, val loss: 0.7296251654624939
Epoch 350, training loss: 0.18739423155784607 = 0.1180676743388176 + 0.01 * 6.932655334472656
Epoch 350, val loss: 0.7319199442863464
Epoch 360, training loss: 0.17128220200538635 = 0.1020549014210701 + 0.01 * 6.9227294921875
Epoch 360, val loss: 0.7359408736228943
Epoch 370, training loss: 0.15743133425712585 = 0.08827553689479828 + 0.01 * 6.9155802726745605
Epoch 370, val loss: 0.7414970993995667
Epoch 380, training loss: 0.14555224776268005 = 0.07647643983364105 + 0.01 * 6.907581806182861
Epoch 380, val loss: 0.7486225366592407
Epoch 390, training loss: 0.13548272848129272 = 0.06652387231588364 + 0.01 * 6.895886421203613
Epoch 390, val loss: 0.7572224140167236
Epoch 400, training loss: 0.12715011835098267 = 0.05822610855102539 + 0.01 * 6.892401218414307
Epoch 400, val loss: 0.7672257423400879
Epoch 410, training loss: 0.12014247477054596 = 0.05131399258971214 + 0.01 * 6.882848739624023
Epoch 410, val loss: 0.7783077955245972
Epoch 420, training loss: 0.11452580243349075 = 0.04552076756954193 + 0.01 * 6.900503635406494
Epoch 420, val loss: 0.7899434566497803
Epoch 430, training loss: 0.10912776738405228 = 0.04064490646123886 + 0.01 * 6.848286151885986
Epoch 430, val loss: 0.8019008040428162
Epoch 440, training loss: 0.10497748106718063 = 0.03649477660655975 + 0.01 * 6.848270893096924
Epoch 440, val loss: 0.8140250444412231
Epoch 450, training loss: 0.101175457239151 = 0.032935261726379395 + 0.01 * 6.824019432067871
Epoch 450, val loss: 0.8260642886161804
Epoch 460, training loss: 0.09822627902030945 = 0.02985813096165657 + 0.01 * 6.836815357208252
Epoch 460, val loss: 0.8380732536315918
Epoch 470, training loss: 0.09522193670272827 = 0.027192413806915283 + 0.01 * 6.802952289581299
Epoch 470, val loss: 0.8498870134353638
Epoch 480, training loss: 0.09299415349960327 = 0.024866849184036255 + 0.01 * 6.81273078918457
Epoch 480, val loss: 0.8614304065704346
Epoch 490, training loss: 0.09063051640987396 = 0.022831842303276062 + 0.01 * 6.779867649078369
Epoch 490, val loss: 0.872731626033783
Epoch 500, training loss: 0.0889924168586731 = 0.021039649844169617 + 0.01 * 6.795276641845703
Epoch 500, val loss: 0.8838019371032715
Epoch 510, training loss: 0.0871879905462265 = 0.0194554403424263 + 0.01 * 6.773255348205566
Epoch 510, val loss: 0.8945263624191284
Epoch 520, training loss: 0.08581048250198364 = 0.018045255914330482 + 0.01 * 6.776522636413574
Epoch 520, val loss: 0.9049707651138306
Epoch 530, training loss: 0.08430501073598862 = 0.016785727813839912 + 0.01 * 6.751928806304932
Epoch 530, val loss: 0.9151967763900757
Epoch 540, training loss: 0.08317981660366058 = 0.01565573923289776 + 0.01 * 6.752408027648926
Epoch 540, val loss: 0.9251360893249512
Epoch 550, training loss: 0.08203307539224625 = 0.014641541056334972 + 0.01 * 6.739153861999512
Epoch 550, val loss: 0.9347975254058838
Epoch 560, training loss: 0.0813494399189949 = 0.013725848868489265 + 0.01 * 6.762359619140625
Epoch 560, val loss: 0.944083571434021
Epoch 570, training loss: 0.08030074089765549 = 0.012898164801299572 + 0.01 * 6.740257740020752
Epoch 570, val loss: 0.9532773494720459
Epoch 580, training loss: 0.07971394807100296 = 0.012145453132689 + 0.01 * 6.756849765777588
Epoch 580, val loss: 0.9621221423149109
Epoch 590, training loss: 0.07850117981433868 = 0.011460606008768082 + 0.01 * 6.704057216644287
Epoch 590, val loss: 0.9708355069160461
Epoch 600, training loss: 0.07797995954751968 = 0.010835699737071991 + 0.01 * 6.714426040649414
Epoch 600, val loss: 0.9792982339859009
Epoch 610, training loss: 0.07728373259305954 = 0.010263421572744846 + 0.01 * 6.70203161239624
Epoch 610, val loss: 0.9874700307846069
Epoch 620, training loss: 0.07673963904380798 = 0.009737974032759666 + 0.01 * 6.700166702270508
Epoch 620, val loss: 0.9955344796180725
Epoch 630, training loss: 0.07618552446365356 = 0.009253493510186672 + 0.01 * 6.693203449249268
Epoch 630, val loss: 1.003299593925476
Epoch 640, training loss: 0.07558953762054443 = 0.008807718753814697 + 0.01 * 6.678182125091553
Epoch 640, val loss: 1.0109219551086426
Epoch 650, training loss: 0.0750245451927185 = 0.008395020850002766 + 0.01 * 6.662951946258545
Epoch 650, val loss: 1.018342137336731
Epoch 660, training loss: 0.07480667531490326 = 0.008012118749320507 + 0.01 * 6.679455757141113
Epoch 660, val loss: 1.0256004333496094
Epoch 670, training loss: 0.07424675673246384 = 0.007656758651137352 + 0.01 * 6.658999919891357
Epoch 670, val loss: 1.032618522644043
Epoch 680, training loss: 0.07398908585309982 = 0.007325716316699982 + 0.01 * 6.666337013244629
Epoch 680, val loss: 1.0395982265472412
Epoch 690, training loss: 0.07347896695137024 = 0.0070167784579098225 + 0.01 * 6.646219253540039
Epoch 690, val loss: 1.0463759899139404
Epoch 700, training loss: 0.07297301292419434 = 0.006727574858814478 + 0.01 * 6.6245436668396
Epoch 700, val loss: 1.0529615879058838
Epoch 710, training loss: 0.0729052722454071 = 0.006457346025854349 + 0.01 * 6.6447930335998535
Epoch 710, val loss: 1.0594308376312256
Epoch 720, training loss: 0.07243455201387405 = 0.006205505225807428 + 0.01 * 6.6229047775268555
Epoch 720, val loss: 1.0657949447631836
Epoch 730, training loss: 0.07231564074754715 = 0.005969676189124584 + 0.01 * 6.634596824645996
Epoch 730, val loss: 1.0720010995864868
Epoch 740, training loss: 0.07249043881893158 = 0.005748268216848373 + 0.01 * 6.674217224121094
Epoch 740, val loss: 1.0780421495437622
Epoch 750, training loss: 0.07168828696012497 = 0.005541100632399321 + 0.01 * 6.614718437194824
Epoch 750, val loss: 1.083939552307129
Epoch 760, training loss: 0.07131462544202805 = 0.005346491001546383 + 0.01 * 6.596813201904297
Epoch 760, val loss: 1.0897125005722046
Epoch 770, training loss: 0.0712939128279686 = 0.005163411609828472 + 0.01 * 6.61305046081543
Epoch 770, val loss: 1.0953351259231567
Epoch 780, training loss: 0.07124266773462296 = 0.004990777932107449 + 0.01 * 6.625188827514648
Epoch 780, val loss: 1.1008018255233765
Epoch 790, training loss: 0.07070860266685486 = 0.0048277112655341625 + 0.01 * 6.588089466094971
Epoch 790, val loss: 1.1062372922897339
Epoch 800, training loss: 0.0704418495297432 = 0.004673439543694258 + 0.01 * 6.576841354370117
Epoch 800, val loss: 1.1115282773971558
Epoch 810, training loss: 0.07038939744234085 = 0.004527817014604807 + 0.01 * 6.586158275604248
Epoch 810, val loss: 1.1166865825653076
Epoch 820, training loss: 0.07041852176189423 = 0.00438988720998168 + 0.01 * 6.602863311767578
Epoch 820, val loss: 1.121749997138977
Epoch 830, training loss: 0.06981052458286285 = 0.004259271081537008 + 0.01 * 6.5551252365112305
Epoch 830, val loss: 1.1267602443695068
Epoch 840, training loss: 0.07028380781412125 = 0.004135366063565016 + 0.01 * 6.614844799041748
Epoch 840, val loss: 1.1315838098526
Epoch 850, training loss: 0.06964072585105896 = 0.004017781466245651 + 0.01 * 6.5622944831848145
Epoch 850, val loss: 1.1363900899887085
Epoch 860, training loss: 0.06936009228229523 = 0.0039062430150806904 + 0.01 * 6.545384883880615
Epoch 860, val loss: 1.1411129236221313
Epoch 870, training loss: 0.06927768141031265 = 0.0038000280037522316 + 0.01 * 6.547765731811523
Epoch 870, val loss: 1.1457219123840332
Epoch 880, training loss: 0.0692673847079277 = 0.003698911052197218 + 0.01 * 6.556848049163818
Epoch 880, val loss: 1.1501816511154175
Epoch 890, training loss: 0.06923739612102509 = 0.0036025899462401867 + 0.01 * 6.563481330871582
Epoch 890, val loss: 1.1546292304992676
Epoch 900, training loss: 0.06922737509012222 = 0.0035105908755213022 + 0.01 * 6.571678161621094
Epoch 900, val loss: 1.1589195728302002
Epoch 910, training loss: 0.06843490898609161 = 0.0034228276927024126 + 0.01 * 6.501208305358887
Epoch 910, val loss: 1.1632354259490967
Epoch 920, training loss: 0.06872113794088364 = 0.0033389257732778788 + 0.01 * 6.53822135925293
Epoch 920, val loss: 1.1674716472625732
Epoch 930, training loss: 0.0683879554271698 = 0.003258933313190937 + 0.01 * 6.512901782989502
Epoch 930, val loss: 1.1715184450149536
Epoch 940, training loss: 0.0682850182056427 = 0.0031823611352592707 + 0.01 * 6.510265827178955
Epoch 940, val loss: 1.175566554069519
Epoch 950, training loss: 0.06819495558738708 = 0.0031091535929590464 + 0.01 * 6.508579730987549
Epoch 950, val loss: 1.1794921159744263
Epoch 960, training loss: 0.06818244606256485 = 0.00303907273337245 + 0.01 * 6.51433801651001
Epoch 960, val loss: 1.1834025382995605
Epoch 970, training loss: 0.06792531162500381 = 0.002971803303807974 + 0.01 * 6.4953508377075195
Epoch 970, val loss: 1.1872838735580444
Epoch 980, training loss: 0.06786809861660004 = 0.002907361136749387 + 0.01 * 6.496074199676514
Epoch 980, val loss: 1.190979242324829
Epoch 990, training loss: 0.06802809238433838 = 0.0028455802239477634 + 0.01 * 6.518251419067383
Epoch 990, val loss: 1.1947417259216309
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.3911
Flip ASR: 0.3733/225 nodes
The final ASR:0.58303, 0.14209, Accuracy:0.81235, 0.00924
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11570])
remove edge: torch.Size([2, 9458])
updated graph: torch.Size([2, 10472])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98401, 0.00758, Accuracy:0.82716, 0.00698
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0269880294799805 = 1.943250298500061 + 0.01 * 8.373775482177734
Epoch 0, val loss: 1.9456524848937988
Epoch 10, training loss: 2.016329288482666 = 1.9325926303863525 + 0.01 * 8.373655319213867
Epoch 10, val loss: 1.9349722862243652
Epoch 20, training loss: 2.003516674041748 = 1.9197834730148315 + 0.01 * 8.373327255249023
Epoch 20, val loss: 1.9220901727676392
Epoch 30, training loss: 1.9860409498214722 = 1.9023171663284302 + 0.01 * 8.372377395629883
Epoch 30, val loss: 1.9047044515609741
Epoch 40, training loss: 1.9607988595962524 = 1.8771244287490845 + 0.01 * 8.367446899414062
Epoch 40, val loss: 1.8803890943527222
Epoch 50, training loss: 1.9255613088607788 = 1.842264175415039 + 0.01 * 8.329718589782715
Epoch 50, val loss: 1.8487191200256348
Epoch 60, training loss: 1.8826067447662354 = 1.8020110130310059 + 0.01 * 8.059569358825684
Epoch 60, val loss: 1.8158833980560303
Epoch 70, training loss: 1.8421757221221924 = 1.7646559476852417 + 0.01 * 7.751980781555176
Epoch 70, val loss: 1.7870043516159058
Epoch 80, training loss: 1.7939752340316772 = 1.7204419374465942 + 0.01 * 7.353333950042725
Epoch 80, val loss: 1.748062252998352
Epoch 90, training loss: 1.7313555479049683 = 1.659981608390808 + 0.01 * 7.137399196624756
Epoch 90, val loss: 1.6954936981201172
Epoch 100, training loss: 1.6510767936706543 = 1.5807427167892456 + 0.01 * 7.0334062576293945
Epoch 100, val loss: 1.6302889585494995
Epoch 110, training loss: 1.5560282468795776 = 1.486252784729004 + 0.01 * 6.977546691894531
Epoch 110, val loss: 1.554618239402771
Epoch 120, training loss: 1.4540066719055176 = 1.3844494819641113 + 0.01 * 6.955713272094727
Epoch 120, val loss: 1.4723747968673706
Epoch 130, training loss: 1.3493643999099731 = 1.279883861541748 + 0.01 * 6.948051452636719
Epoch 130, val loss: 1.3882383108139038
Epoch 140, training loss: 1.2433812618255615 = 1.1739439964294434 + 0.01 * 6.943728923797607
Epoch 140, val loss: 1.3036518096923828
Epoch 150, training loss: 1.1380716562271118 = 1.0686684846878052 + 0.01 * 6.94032096862793
Epoch 150, val loss: 1.219724416732788
Epoch 160, training loss: 1.0361754894256592 = 0.9667976498603821 + 0.01 * 6.937779903411865
Epoch 160, val loss: 1.1398887634277344
Epoch 170, training loss: 0.939037561416626 = 0.8696706295013428 + 0.01 * 6.9366912841796875
Epoch 170, val loss: 1.0651707649230957
Epoch 180, training loss: 0.8468140959739685 = 0.7774554491043091 + 0.01 * 6.9358649253845215
Epoch 180, val loss: 0.995203971862793
Epoch 190, training loss: 0.7595252990722656 = 0.6901814341545105 + 0.01 * 6.934384822845459
Epoch 190, val loss: 0.9295481443405151
Epoch 200, training loss: 0.6775780916213989 = 0.6082579493522644 + 0.01 * 6.932011604309082
Epoch 200, val loss: 0.8693869709968567
Epoch 210, training loss: 0.6018020510673523 = 0.5325136184692383 + 0.01 * 6.928844451904297
Epoch 210, val loss: 0.8158770799636841
Epoch 220, training loss: 0.5334435701370239 = 0.464196115732193 + 0.01 * 6.924742698669434
Epoch 220, val loss: 0.7701454758644104
Epoch 230, training loss: 0.4733727276325226 = 0.4041862487792969 + 0.01 * 6.9186482429504395
Epoch 230, val loss: 0.7327800989151001
Epoch 240, training loss: 0.4214637875556946 = 0.35235464572906494 + 0.01 * 6.910913944244385
Epoch 240, val loss: 0.7032562494277954
Epoch 250, training loss: 0.37667641043663025 = 0.30759137868881226 + 0.01 * 6.908504009246826
Epoch 250, val loss: 0.6804935932159424
Epoch 260, training loss: 0.3373561501502991 = 0.26841601729393005 + 0.01 * 6.894011974334717
Epoch 260, val loss: 0.6630620956420898
Epoch 270, training loss: 0.3024144768714905 = 0.23356136679649353 + 0.01 * 6.885312557220459
Epoch 270, val loss: 0.6498590111732483
Epoch 280, training loss: 0.27107691764831543 = 0.20227982103824615 + 0.01 * 6.879711151123047
Epoch 280, val loss: 0.6400516033172607
Epoch 290, training loss: 0.2429782748222351 = 0.1742967814207077 + 0.01 * 6.8681488037109375
Epoch 290, val loss: 0.6332508325576782
Epoch 300, training loss: 0.21833592653274536 = 0.1496703326702118 + 0.01 * 6.866559028625488
Epoch 300, val loss: 0.6293331980705261
Epoch 310, training loss: 0.19703508913516998 = 0.12851719558238983 + 0.01 * 6.851789474487305
Epoch 310, val loss: 0.6282780766487122
Epoch 320, training loss: 0.17917412519454956 = 0.11073589324951172 + 0.01 * 6.843822479248047
Epoch 320, val loss: 0.6298626661300659
Epoch 330, training loss: 0.16435518860816956 = 0.09598015993833542 + 0.01 * 6.8375020027160645
Epoch 330, val loss: 0.6337004899978638
Epoch 340, training loss: 0.1520690619945526 = 0.0837644562125206 + 0.01 * 6.830461025238037
Epoch 340, val loss: 0.6393908858299255
Epoch 350, training loss: 0.14184434711933136 = 0.07360758632421494 + 0.01 * 6.823676586151123
Epoch 350, val loss: 0.646493136882782
Epoch 360, training loss: 0.1332712024450302 = 0.06509730964899063 + 0.01 * 6.817389488220215
Epoch 360, val loss: 0.6546329855918884
Epoch 370, training loss: 0.12605322897434235 = 0.05790409818291664 + 0.01 * 6.814913272857666
Epoch 370, val loss: 0.6635256409645081
Epoch 380, training loss: 0.11980258673429489 = 0.05177917331457138 + 0.01 * 6.802341461181641
Epoch 380, val loss: 0.6728435754776001
Epoch 390, training loss: 0.1146145910024643 = 0.04653271660208702 + 0.01 * 6.808187007904053
Epoch 390, val loss: 0.6823773384094238
Epoch 400, training loss: 0.10987717658281326 = 0.042012348771095276 + 0.01 * 6.786483287811279
Epoch 400, val loss: 0.6920716166496277
Epoch 410, training loss: 0.10602536797523499 = 0.038090333342552185 + 0.01 * 6.793503761291504
Epoch 410, val loss: 0.7017765045166016
Epoch 420, training loss: 0.10229289531707764 = 0.034672778099775314 + 0.01 * 6.762011528015137
Epoch 420, val loss: 0.7114424705505371
Epoch 430, training loss: 0.09934109449386597 = 0.03167666867375374 + 0.01 * 6.76644229888916
Epoch 430, val loss: 0.720966637134552
Epoch 440, training loss: 0.09666489064693451 = 0.02904469706118107 + 0.01 * 6.762019634246826
Epoch 440, val loss: 0.7303529381752014
Epoch 450, training loss: 0.09406891465187073 = 0.026720350608229637 + 0.01 * 6.734856605529785
Epoch 450, val loss: 0.739540696144104
Epoch 460, training loss: 0.09185435622930527 = 0.024659425020217896 + 0.01 * 6.7194929122924805
Epoch 460, val loss: 0.7485125660896301
Epoch 470, training loss: 0.08988851308822632 = 0.02282864972949028 + 0.01 * 6.705986499786377
Epoch 470, val loss: 0.757301390171051
Epoch 480, training loss: 0.08841926604509354 = 0.021191956475377083 + 0.01 * 6.722731113433838
Epoch 480, val loss: 0.7658538818359375
Epoch 490, training loss: 0.08659423887729645 = 0.019727759063243866 + 0.01 * 6.686648368835449
Epoch 490, val loss: 0.7742331027984619
Epoch 500, training loss: 0.08594803512096405 = 0.01841147243976593 + 0.01 * 6.753656387329102
Epoch 500, val loss: 0.7823059558868408
Epoch 510, training loss: 0.08384589850902557 = 0.017226431518793106 + 0.01 * 6.6619462966918945
Epoch 510, val loss: 0.7902028560638428
Epoch 520, training loss: 0.08275292068719864 = 0.01615455560386181 + 0.01 * 6.659836769104004
Epoch 520, val loss: 0.797900915145874
Epoch 530, training loss: 0.08200409263372421 = 0.015182198025286198 + 0.01 * 6.682189464569092
Epoch 530, val loss: 0.8053738474845886
Epoch 540, training loss: 0.0806317999958992 = 0.014298520982265472 + 0.01 * 6.633327960968018
Epoch 540, val loss: 0.8126672506332397
Epoch 550, training loss: 0.07988029718399048 = 0.013492709957063198 + 0.01 * 6.638759136199951
Epoch 550, val loss: 0.8196860551834106
Epoch 560, training loss: 0.07894349843263626 = 0.012757621705532074 + 0.01 * 6.618587970733643
Epoch 560, val loss: 0.8266287446022034
Epoch 570, training loss: 0.07842671871185303 = 0.012083975598216057 + 0.01 * 6.634274005889893
Epoch 570, val loss: 0.8333150744438171
Epoch 580, training loss: 0.07761572301387787 = 0.011464294046163559 + 0.01 * 6.615143299102783
Epoch 580, val loss: 0.8398215174674988
Epoch 590, training loss: 0.07694771140813828 = 0.010893619619309902 + 0.01 * 6.605409145355225
Epoch 590, val loss: 0.8461613059043884
Epoch 600, training loss: 0.07630691677331924 = 0.0103671969845891 + 0.01 * 6.5939717292785645
Epoch 600, val loss: 0.8523716926574707
Epoch 610, training loss: 0.07596389949321747 = 0.009880132973194122 + 0.01 * 6.608376979827881
Epoch 610, val loss: 0.8583800792694092
Epoch 620, training loss: 0.07531989365816116 = 0.009429414756596088 + 0.01 * 6.589047908782959
Epoch 620, val loss: 0.864306628704071
Epoch 630, training loss: 0.0747452825307846 = 0.009011001326143742 + 0.01 * 6.573428153991699
Epoch 630, val loss: 0.8700010776519775
Epoch 640, training loss: 0.07428386062383652 = 0.008622422814369202 + 0.01 * 6.566143989562988
Epoch 640, val loss: 0.8756600618362427
Epoch 650, training loss: 0.07388973981142044 = 0.008260260336101055 + 0.01 * 6.562948703765869
Epoch 650, val loss: 0.8810842633247375
Epoch 660, training loss: 0.0735645741224289 = 0.007922572083771229 + 0.01 * 6.564200401306152
Epoch 660, val loss: 0.8864198923110962
Epoch 670, training loss: 0.07312355935573578 = 0.007607037201523781 + 0.01 * 6.551652908325195
Epoch 670, val loss: 0.8916124701499939
Epoch 680, training loss: 0.07342365384101868 = 0.0073116254061460495 + 0.01 * 6.611202716827393
Epoch 680, val loss: 0.896705687046051
Epoch 690, training loss: 0.07253585755825043 = 0.00703500397503376 + 0.01 * 6.55008602142334
Epoch 690, val loss: 0.9016338586807251
Epoch 700, training loss: 0.07220964878797531 = 0.0067753419280052185 + 0.01 * 6.543430805206299
Epoch 700, val loss: 0.9065264463424683
Epoch 710, training loss: 0.07176105678081512 = 0.006531357765197754 + 0.01 * 6.522969722747803
Epoch 710, val loss: 0.9112645983695984
Epoch 720, training loss: 0.07165763527154922 = 0.0063020153902471066 + 0.01 * 6.535562038421631
Epoch 720, val loss: 0.9158809781074524
Epoch 730, training loss: 0.07130201905965805 = 0.006085735280066729 + 0.01 * 6.521628379821777
Epoch 730, val loss: 0.9203720092773438
Epoch 740, training loss: 0.07102147489786148 = 0.0058817495591938496 + 0.01 * 6.513972759246826
Epoch 740, val loss: 0.9248182773590088
Epoch 750, training loss: 0.07085192948579788 = 0.005689067766070366 + 0.01 * 6.5162858963012695
Epoch 750, val loss: 0.9291256666183472
Epoch 760, training loss: 0.07078365981578827 = 0.0055070724338293076 + 0.01 * 6.527658939361572
Epoch 760, val loss: 0.9333432912826538
Epoch 770, training loss: 0.07034210860729218 = 0.005334632471203804 + 0.01 * 6.500748157501221
Epoch 770, val loss: 0.9374344348907471
Epoch 780, training loss: 0.07030656933784485 = 0.005171605851501226 + 0.01 * 6.5134968757629395
Epoch 780, val loss: 0.9414730072021484
Epoch 790, training loss: 0.07000983506441116 = 0.005016915034502745 + 0.01 * 6.499291896820068
Epoch 790, val loss: 0.9453951120376587
Epoch 800, training loss: 0.06983519345521927 = 0.004870210308581591 + 0.01 * 6.496498107910156
Epoch 800, val loss: 0.9492760300636292
Epoch 810, training loss: 0.06952397525310516 = 0.004730771761387587 + 0.01 * 6.479321002960205
Epoch 810, val loss: 0.9530470967292786
Epoch 820, training loss: 0.069527268409729 = 0.004597931634634733 + 0.01 * 6.492933750152588
Epoch 820, val loss: 0.9567515254020691
Epoch 830, training loss: 0.069241963326931 = 0.004471732769161463 + 0.01 * 6.477023124694824
Epoch 830, val loss: 0.9603590369224548
Epoch 840, training loss: 0.06909319758415222 = 0.004351417068392038 + 0.01 * 6.474178314208984
Epoch 840, val loss: 0.9639242887496948
Epoch 850, training loss: 0.06904962658882141 = 0.004236911423504353 + 0.01 * 6.481271743774414
Epoch 850, val loss: 0.967401921749115
Epoch 860, training loss: 0.06899236887693405 = 0.004127636086195707 + 0.01 * 6.486473083496094
Epoch 860, val loss: 0.970805287361145
Epoch 870, training loss: 0.06848075985908508 = 0.004023326560854912 + 0.01 * 6.445743083953857
Epoch 870, val loss: 0.9741336107254028
Epoch 880, training loss: 0.06834308803081512 = 0.003923685755580664 + 0.01 * 6.441940784454346
Epoch 880, val loss: 0.9774165153503418
Epoch 890, training loss: 0.06865157932043076 = 0.0038283539470285177 + 0.01 * 6.482322692871094
Epoch 890, val loss: 0.9806140065193176
Epoch 900, training loss: 0.06828047335147858 = 0.003737314371392131 + 0.01 * 6.454315662384033
Epoch 900, val loss: 0.9837796688079834
Epoch 910, training loss: 0.06841728091239929 = 0.0036500459536910057 + 0.01 * 6.476723670959473
Epoch 910, val loss: 0.986859142780304
Epoch 920, training loss: 0.06789927929639816 = 0.003566444618627429 + 0.01 * 6.43328332901001
Epoch 920, val loss: 0.989908754825592
Epoch 930, training loss: 0.06800887733697891 = 0.003486261237412691 + 0.01 * 6.452261447906494
Epoch 930, val loss: 0.9929054379463196
Epoch 940, training loss: 0.06763637810945511 = 0.0034093339927494526 + 0.01 * 6.422704696655273
Epoch 940, val loss: 0.9958186149597168
Epoch 950, training loss: 0.06793016195297241 = 0.003335546236485243 + 0.01 * 6.459461688995361
Epoch 950, val loss: 0.9986879825592041
Epoch 960, training loss: 0.06753365695476532 = 0.003264649538323283 + 0.01 * 6.426900863647461
Epoch 960, val loss: 1.0015232563018799
Epoch 970, training loss: 0.06769198179244995 = 0.0031966613605618477 + 0.01 * 6.4495320320129395
Epoch 970, val loss: 1.004294753074646
Epoch 980, training loss: 0.0673745796084404 = 0.0031311470083892345 + 0.01 * 6.424344062805176
Epoch 980, val loss: 1.0070422887802124
Epoch 990, training loss: 0.0672612190246582 = 0.0030682699289172888 + 0.01 * 6.419295310974121
Epoch 990, val loss: 1.0097383260726929
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.6605
Flip ASR: 0.5911/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.052178144454956 = 1.9684396982192993 + 0.01 * 8.373833656311035
Epoch 0, val loss: 1.9653347730636597
Epoch 10, training loss: 2.0401365756988525 = 1.9563992023468018 + 0.01 * 8.373734474182129
Epoch 10, val loss: 1.9522974491119385
Epoch 20, training loss: 2.0254459381103516 = 1.941711187362671 + 0.01 * 8.373464584350586
Epoch 20, val loss: 1.9360500574111938
Epoch 30, training loss: 2.00545597076416 = 1.9217289686203003 + 0.01 * 8.372706413269043
Epoch 30, val loss: 1.913757085800171
Epoch 40, training loss: 1.9769346714019775 = 1.8932464122772217 + 0.01 * 8.36882209777832
Epoch 40, val loss: 1.8822559118270874
Epoch 50, training loss: 1.937607765197754 = 1.8541889190673828 + 0.01 * 8.341882705688477
Epoch 50, val loss: 1.840602993965149
Epoch 60, training loss: 1.890681505203247 = 1.808733582496643 + 0.01 * 8.194795608520508
Epoch 60, val loss: 1.7964576482772827
Epoch 70, training loss: 1.8442199230194092 = 1.7651681900024414 + 0.01 * 7.905176162719727
Epoch 70, val loss: 1.7600075006484985
Epoch 80, training loss: 1.7913269996643066 = 1.7151789665222168 + 0.01 * 7.614805221557617
Epoch 80, val loss: 1.7197388410568237
Epoch 90, training loss: 1.7223631143569946 = 1.6489087343215942 + 0.01 * 7.3454389572143555
Epoch 90, val loss: 1.6638160943984985
Epoch 100, training loss: 1.6330349445343018 = 1.5614550113677979 + 0.01 * 7.157993793487549
Epoch 100, val loss: 1.5886614322662354
Epoch 110, training loss: 1.5312750339508057 = 1.4603968858718872 + 0.01 * 7.087814807891846
Epoch 110, val loss: 1.5041089057922363
Epoch 120, training loss: 1.429059386253357 = 1.3585834503173828 + 0.01 * 7.047597885131836
Epoch 120, val loss: 1.424347162246704
Epoch 130, training loss: 1.3313254117965698 = 1.261060118675232 + 0.01 * 7.026525497436523
Epoch 130, val loss: 1.3546639680862427
Epoch 140, training loss: 1.2373944520950317 = 1.1673181056976318 + 0.01 * 7.007630825042725
Epoch 140, val loss: 1.290898084640503
Epoch 150, training loss: 1.146888017654419 = 1.0769895315170288 + 0.01 * 6.9898529052734375
Epoch 150, val loss: 1.2292338609695435
Epoch 160, training loss: 1.0600947141647339 = 0.9903623461723328 + 0.01 * 6.973232746124268
Epoch 160, val loss: 1.1693116426467896
Epoch 170, training loss: 0.9775895476341248 = 0.9080203771591187 + 0.01 * 6.956919193267822
Epoch 170, val loss: 1.1117568016052246
Epoch 180, training loss: 0.9001694917678833 = 0.8307371139526367 + 0.01 * 6.943235397338867
Epoch 180, val loss: 1.0573267936706543
Epoch 190, training loss: 0.8289155960083008 = 0.7595565319061279 + 0.01 * 6.935909748077393
Epoch 190, val loss: 1.0063484907150269
Epoch 200, training loss: 0.764889657497406 = 0.695581316947937 + 0.01 * 6.930834770202637
Epoch 200, val loss: 0.9603271484375
Epoch 210, training loss: 0.7078070640563965 = 0.6385306715965271 + 0.01 * 6.927637100219727
Epoch 210, val loss: 0.9197698831558228
Epoch 220, training loss: 0.6560338139533997 = 0.5867806077003479 + 0.01 * 6.925321578979492
Epoch 220, val loss: 0.8840093612670898
Epoch 230, training loss: 0.607781708240509 = 0.5385568737983704 + 0.01 * 6.922482967376709
Epoch 230, val loss: 0.8519096970558167
Epoch 240, training loss: 0.5618493556976318 = 0.4926612973213196 + 0.01 * 6.918802738189697
Epoch 240, val loss: 0.8227462768554688
Epoch 250, training loss: 0.5180040001869202 = 0.44886839389801025 + 0.01 * 6.913558483123779
Epoch 250, val loss: 0.7970345616340637
Epoch 260, training loss: 0.47645556926727295 = 0.4073946475982666 + 0.01 * 6.906091690063477
Epoch 260, val loss: 0.7753607630729675
Epoch 270, training loss: 0.43732407689094543 = 0.3683296740055084 + 0.01 * 6.899440288543701
Epoch 270, val loss: 0.7577736377716064
Epoch 280, training loss: 0.40051594376564026 = 0.3316269814968109 + 0.01 * 6.888896942138672
Epoch 280, val loss: 0.7441617846488953
Epoch 290, training loss: 0.36606693267822266 = 0.2972741425037384 + 0.01 * 6.879279136657715
Epoch 290, val loss: 0.7342032194137573
Epoch 300, training loss: 0.33424389362335205 = 0.26553893089294434 + 0.01 * 6.8704962730407715
Epoch 300, val loss: 0.7275052070617676
Epoch 310, training loss: 0.30521678924560547 = 0.23661357164382935 + 0.01 * 6.860323429107666
Epoch 310, val loss: 0.7238485217094421
Epoch 320, training loss: 0.27907317876815796 = 0.2105485051870346 + 0.01 * 6.852466106414795
Epoch 320, val loss: 0.7227479219436646
Epoch 330, training loss: 0.25584685802459717 = 0.1873273402452469 + 0.01 * 6.851951599121094
Epoch 330, val loss: 0.7239801287651062
Epoch 340, training loss: 0.23531529307365417 = 0.1668039709329605 + 0.01 * 6.851133346557617
Epoch 340, val loss: 0.727824330329895
Epoch 350, training loss: 0.21724385023117065 = 0.1488126963376999 + 0.01 * 6.843114852905273
Epoch 350, val loss: 0.7340396046638489
Epoch 360, training loss: 0.20140999555587769 = 0.13301636278629303 + 0.01 * 6.8393635749816895
Epoch 360, val loss: 0.7421315908432007
Epoch 370, training loss: 0.1874382495880127 = 0.11908090114593506 + 0.01 * 6.8357343673706055
Epoch 370, val loss: 0.7519148588180542
Epoch 380, training loss: 0.1749967336654663 = 0.10667518526315689 + 0.01 * 6.832155227661133
Epoch 380, val loss: 0.763141393661499
Epoch 390, training loss: 0.16381597518920898 = 0.09553069621324539 + 0.01 * 6.82852840423584
Epoch 390, val loss: 0.7755482196807861
Epoch 400, training loss: 0.15404638648033142 = 0.08569709211587906 + 0.01 * 6.8349289894104
Epoch 400, val loss: 0.7889925241470337
Epoch 410, training loss: 0.14537106454372406 = 0.07713398337364197 + 0.01 * 6.823708534240723
Epoch 410, val loss: 0.8034276962280273
Epoch 420, training loss: 0.13781529664993286 = 0.06964534521102905 + 0.01 * 6.816996097564697
Epoch 420, val loss: 0.8184846043586731
Epoch 430, training loss: 0.13113975524902344 = 0.06301551312208176 + 0.01 * 6.812424182891846
Epoch 430, val loss: 0.8338519334793091
Epoch 440, training loss: 0.1253248155117035 = 0.05710659548640251 + 0.01 * 6.821821689605713
Epoch 440, val loss: 0.8495059609413147
Epoch 450, training loss: 0.11989966034889221 = 0.051840174943208694 + 0.01 * 6.805948734283447
Epoch 450, val loss: 0.8653722405433655
Epoch 460, training loss: 0.11509046703577042 = 0.04711443930864334 + 0.01 * 6.797603130340576
Epoch 460, val loss: 0.8813727498054504
Epoch 470, training loss: 0.11084727942943573 = 0.04287491366267204 + 0.01 * 6.797236919403076
Epoch 470, val loss: 0.897523820400238
Epoch 480, training loss: 0.10694243758916855 = 0.03908408433198929 + 0.01 * 6.785835266113281
Epoch 480, val loss: 0.9137690663337708
Epoch 490, training loss: 0.1035766452550888 = 0.03569600358605385 + 0.01 * 6.788064002990723
Epoch 490, val loss: 0.9299344420433044
Epoch 500, training loss: 0.10043953359127045 = 0.032677702605724335 + 0.01 * 6.776183605194092
Epoch 500, val loss: 0.9460225701332092
Epoch 510, training loss: 0.097749263048172 = 0.029990587383508682 + 0.01 * 6.775867938995361
Epoch 510, val loss: 0.9619830250740051
Epoch 520, training loss: 0.09529278427362442 = 0.027593208476901054 + 0.01 * 6.769957542419434
Epoch 520, val loss: 0.9776819348335266
Epoch 530, training loss: 0.09309820830821991 = 0.02545326016843319 + 0.01 * 6.7644944190979
Epoch 530, val loss: 0.9931042790412903
Epoch 540, training loss: 0.09113132953643799 = 0.0235389843583107 + 0.01 * 6.75923490524292
Epoch 540, val loss: 1.0082448720932007
Epoch 550, training loss: 0.08944634348154068 = 0.021827226504683495 + 0.01 * 6.761911869049072
Epoch 550, val loss: 1.0231070518493652
Epoch 560, training loss: 0.08777181804180145 = 0.020294396206736565 + 0.01 * 6.747742176055908
Epoch 560, val loss: 1.0375721454620361
Epoch 570, training loss: 0.08626991510391235 = 0.018915168941020966 + 0.01 * 6.735474586486816
Epoch 570, val loss: 1.0516704320907593
Epoch 580, training loss: 0.08508636057376862 = 0.017672130838036537 + 0.01 * 6.7414231300354
Epoch 580, val loss: 1.065395712852478
Epoch 590, training loss: 0.08379495143890381 = 0.01655099168419838 + 0.01 * 6.724396705627441
Epoch 590, val loss: 1.0787293910980225
Epoch 600, training loss: 0.08262860774993896 = 0.015534825623035431 + 0.01 * 6.709378242492676
Epoch 600, val loss: 1.0917396545410156
Epoch 610, training loss: 0.0819723904132843 = 0.014612418599426746 + 0.01 * 6.735996723175049
Epoch 610, val loss: 1.1044327020645142
Epoch 620, training loss: 0.08083073049783707 = 0.013775818981230259 + 0.01 * 6.705491065979004
Epoch 620, val loss: 1.116628885269165
Epoch 630, training loss: 0.08001627773046494 = 0.01301187090575695 + 0.01 * 6.700440883636475
Epoch 630, val loss: 1.1286194324493408
Epoch 640, training loss: 0.07928861677646637 = 0.012310612946748734 + 0.01 * 6.697800159454346
Epoch 640, val loss: 1.14031982421875
Epoch 650, training loss: 0.07856454700231552 = 0.011667506769299507 + 0.01 * 6.689704418182373
Epoch 650, val loss: 1.1516897678375244
Epoch 660, training loss: 0.07775042951107025 = 0.011075543239712715 + 0.01 * 6.667489051818848
Epoch 660, val loss: 1.1627929210662842
Epoch 670, training loss: 0.07745100557804108 = 0.010530098341405392 + 0.01 * 6.69209098815918
Epoch 670, val loss: 1.173613429069519
Epoch 680, training loss: 0.07676252722740173 = 0.010027131997048855 + 0.01 * 6.673539638519287
Epoch 680, val loss: 1.1841415166854858
Epoch 690, training loss: 0.0761723667383194 = 0.009561341255903244 + 0.01 * 6.661102771759033
Epoch 690, val loss: 1.1944082975387573
Epoch 700, training loss: 0.07580395042896271 = 0.009129762649536133 + 0.01 * 6.667418956756592
Epoch 700, val loss: 1.2043641805648804
Epoch 710, training loss: 0.07517899572849274 = 0.00872956495732069 + 0.01 * 6.644943714141846
Epoch 710, val loss: 1.2141010761260986
Epoch 720, training loss: 0.07508721947669983 = 0.008357471786439419 + 0.01 * 6.672974586486816
Epoch 720, val loss: 1.2236528396606445
Epoch 730, training loss: 0.07436969876289368 = 0.008011382073163986 + 0.01 * 6.635831832885742
Epoch 730, val loss: 1.2329179048538208
Epoch 740, training loss: 0.07407401502132416 = 0.0076880306005477905 + 0.01 * 6.638598918914795
Epoch 740, val loss: 1.241994023323059
Epoch 750, training loss: 0.07367461919784546 = 0.007386025041341782 + 0.01 * 6.628859996795654
Epoch 750, val loss: 1.2508430480957031
Epoch 760, training loss: 0.0732024684548378 = 0.007103146053850651 + 0.01 * 6.6099324226379395
Epoch 760, val loss: 1.2594716548919678
Epoch 770, training loss: 0.0730767771601677 = 0.0068374075926840305 + 0.01 * 6.623937129974365
Epoch 770, val loss: 1.2679411172866821
Epoch 780, training loss: 0.0726647898554802 = 0.006587924435734749 + 0.01 * 6.607686519622803
Epoch 780, val loss: 1.276237964630127
Epoch 790, training loss: 0.07234089821577072 = 0.006353313103318214 + 0.01 * 6.598759174346924
Epoch 790, val loss: 1.2842891216278076
Epoch 800, training loss: 0.07229666411876678 = 0.00613282760605216 + 0.01 * 6.616384029388428
Epoch 800, val loss: 1.2922379970550537
Epoch 810, training loss: 0.07176312804222107 = 0.005925435107201338 + 0.01 * 6.583769798278809
Epoch 810, val loss: 1.299960970878601
Epoch 820, training loss: 0.07171754539012909 = 0.005729838740080595 + 0.01 * 6.598771095275879
Epoch 820, val loss: 1.3075069189071655
Epoch 830, training loss: 0.07154590636491776 = 0.005544846411794424 + 0.01 * 6.6001057624816895
Epoch 830, val loss: 1.3148932456970215
Epoch 840, training loss: 0.07110528647899628 = 0.005370270926505327 + 0.01 * 6.573501110076904
Epoch 840, val loss: 1.3221153020858765
Epoch 850, training loss: 0.07086745649576187 = 0.0052048563957214355 + 0.01 * 6.566259860992432
Epoch 850, val loss: 1.329197645187378
Epoch 860, training loss: 0.07087399810552597 = 0.005047960672527552 + 0.01 * 6.582603454589844
Epoch 860, val loss: 1.3361214399337769
Epoch 870, training loss: 0.07056868821382523 = 0.004899827763438225 + 0.01 * 6.566885948181152
Epoch 870, val loss: 1.3428878784179688
Epoch 880, training loss: 0.07060486078262329 = 0.004759412258863449 + 0.01 * 6.584544658660889
Epoch 880, val loss: 1.3495763540267944
Epoch 890, training loss: 0.07012039422988892 = 0.004625831265002489 + 0.01 * 6.5494561195373535
Epoch 890, val loss: 1.3560444116592407
Epoch 900, training loss: 0.0699710100889206 = 0.004498919937759638 + 0.01 * 6.5472092628479
Epoch 900, val loss: 1.362403154373169
Epoch 910, training loss: 0.0701276883482933 = 0.004378193523734808 + 0.01 * 6.574949741363525
Epoch 910, val loss: 1.3687008619308472
Epoch 920, training loss: 0.06977233290672302 = 0.004262971691787243 + 0.01 * 6.550936222076416
Epoch 920, val loss: 1.374832034111023
Epoch 930, training loss: 0.06964157521724701 = 0.004153235349804163 + 0.01 * 6.548834323883057
Epoch 930, val loss: 1.380865216255188
Epoch 940, training loss: 0.06942591071128845 = 0.004048324655741453 + 0.01 * 6.5377583503723145
Epoch 940, val loss: 1.386730432510376
Epoch 950, training loss: 0.0693494975566864 = 0.003948180004954338 + 0.01 * 6.54013204574585
Epoch 950, val loss: 1.3926165103912354
Epoch 960, training loss: 0.06923166662454605 = 0.003852520603686571 + 0.01 * 6.537914752960205
Epoch 960, val loss: 1.3982664346694946
Epoch 970, training loss: 0.06915514171123505 = 0.003761065425351262 + 0.01 * 6.539407730102539
Epoch 970, val loss: 1.403879165649414
Epoch 980, training loss: 0.06884033232927322 = 0.0036735939793288708 + 0.01 * 6.516674041748047
Epoch 980, val loss: 1.4093586206436157
Epoch 990, training loss: 0.0689573884010315 = 0.0035897367633879185 + 0.01 * 6.5367655754089355
Epoch 990, val loss: 1.4147173166275024
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.3542
Flip ASR: 0.3067/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.024244785308838 = 1.9405059814453125 + 0.01 * 8.373871803283691
Epoch 0, val loss: 1.9327586889266968
Epoch 10, training loss: 2.0145645141601562 = 1.93082594871521 + 0.01 * 8.373847961425781
Epoch 10, val loss: 1.9231292009353638
Epoch 20, training loss: 2.00296688079834 = 1.9192297458648682 + 0.01 * 8.373706817626953
Epoch 20, val loss: 1.911575436592102
Epoch 30, training loss: 1.986788272857666 = 1.9030547142028809 + 0.01 * 8.373353958129883
Epoch 30, val loss: 1.895604133605957
Epoch 40, training loss: 1.9625884294509888 = 1.8788682222366333 + 0.01 * 8.372020721435547
Epoch 40, val loss: 1.8722654581069946
Epoch 50, training loss: 1.9274787902832031 = 1.8438531160354614 + 0.01 * 8.362563133239746
Epoch 50, val loss: 1.8401368856430054
Epoch 60, training loss: 1.884323239326477 = 1.801424264907837 + 0.01 * 8.28989315032959
Epoch 60, val loss: 1.8049744367599487
Epoch 70, training loss: 1.8398622274398804 = 1.7599000930786133 + 0.01 * 7.996215343475342
Epoch 70, val loss: 1.7721786499023438
Epoch 80, training loss: 1.7858984470367432 = 1.707680106163025 + 0.01 * 7.821834564208984
Epoch 80, val loss: 1.7254022359848022
Epoch 90, training loss: 1.7121893167495728 = 1.6355472803115845 + 0.01 * 7.664209365844727
Epoch 90, val loss: 1.6626169681549072
Epoch 100, training loss: 1.6192188262939453 = 1.5447250604629517 + 0.01 * 7.449371814727783
Epoch 100, val loss: 1.5883644819259644
Epoch 110, training loss: 1.517687201499939 = 1.4459314346313477 + 0.01 * 7.175577163696289
Epoch 110, val loss: 1.507002592086792
Epoch 120, training loss: 1.4201833009719849 = 1.3489102125167847 + 0.01 * 7.127314567565918
Epoch 120, val loss: 1.429850697517395
Epoch 130, training loss: 1.3266505002975464 = 1.2558668851852417 + 0.01 * 7.078359127044678
Epoch 130, val loss: 1.358076810836792
Epoch 140, training loss: 1.236283540725708 = 1.1659058332443237 + 0.01 * 7.037766933441162
Epoch 140, val loss: 1.2907993793487549
Epoch 150, training loss: 1.1480540037155151 = 1.0779348611831665 + 0.01 * 7.011916160583496
Epoch 150, val loss: 1.2269037961959839
Epoch 160, training loss: 1.061403751373291 = 0.9913972020149231 + 0.01 * 7.000659465789795
Epoch 160, val loss: 1.1642029285430908
Epoch 170, training loss: 0.9772918820381165 = 0.9073320031166077 + 0.01 * 6.995987892150879
Epoch 170, val loss: 1.1035608053207397
Epoch 180, training loss: 0.8978203535079956 = 0.8278825283050537 + 0.01 * 6.993785858154297
Epoch 180, val loss: 1.0458769798278809
Epoch 190, training loss: 0.825072705745697 = 0.7551426887512207 + 0.01 * 6.993003845214844
Epoch 190, val loss: 0.9930927157402039
Epoch 200, training loss: 0.7603850960731506 = 0.6904493570327759 + 0.01 * 6.993574619293213
Epoch 200, val loss: 0.9469016194343567
Epoch 210, training loss: 0.7033494710922241 = 0.6334004998207092 + 0.01 * 6.994894981384277
Epoch 210, val loss: 0.9077017307281494
Epoch 220, training loss: 0.6516650915145874 = 0.5816973447799683 + 0.01 * 6.9967756271362305
Epoch 220, val loss: 0.8736600875854492
Epoch 230, training loss: 0.6024709939956665 = 0.5324845910072327 + 0.01 * 6.9986419677734375
Epoch 230, val loss: 0.8424100875854492
Epoch 240, training loss: 0.5539787411689758 = 0.48397597670555115 + 0.01 * 7.000275611877441
Epoch 240, val loss: 0.8127865195274353
Epoch 250, training loss: 0.5059403777122498 = 0.4359239935874939 + 0.01 * 7.001638889312744
Epoch 250, val loss: 0.7855358719825745
Epoch 260, training loss: 0.45863595604896545 = 0.3886066973209381 + 0.01 * 7.002925395965576
Epoch 260, val loss: 0.7612365484237671
Epoch 270, training loss: 0.4123566150665283 = 0.3423149287700653 + 0.01 * 7.004167079925537
Epoch 270, val loss: 0.7396661043167114
Epoch 280, training loss: 0.36762329936027527 = 0.29756978154182434 + 0.01 * 7.0053510665893555
Epoch 280, val loss: 0.7207131385803223
Epoch 290, training loss: 0.3253023326396942 = 0.255237877368927 + 0.01 * 7.00644588470459
Epoch 290, val loss: 0.7047956585884094
Epoch 300, training loss: 0.28650325536727905 = 0.21642911434173584 + 0.01 * 7.007413864135742
Epoch 300, val loss: 0.6925120949745178
Epoch 310, training loss: 0.25227031111717224 = 0.1821882426738739 + 0.01 * 7.008206844329834
Epoch 310, val loss: 0.6841215491294861
Epoch 320, training loss: 0.22319713234901428 = 0.15310850739479065 + 0.01 * 7.008861541748047
Epoch 320, val loss: 0.6798900961875916
Epoch 330, training loss: 0.19920694828033447 = 0.12910449504852295 + 0.01 * 7.010244846343994
Epoch 330, val loss: 0.6797608733177185
Epoch 340, training loss: 0.17968451976776123 = 0.10958747565746307 + 0.01 * 7.009705543518066
Epoch 340, val loss: 0.6831687688827515
Epoch 350, training loss: 0.16385793685913086 = 0.09376082569360733 + 0.01 * 7.009710311889648
Epoch 350, val loss: 0.6893952488899231
Epoch 360, training loss: 0.15095895528793335 = 0.08086784183979034 + 0.01 * 7.009110927581787
Epoch 360, val loss: 0.697649359703064
Epoch 370, training loss: 0.14036452770233154 = 0.07028400897979736 + 0.01 * 7.008050918579102
Epoch 370, val loss: 0.7073410153388977
Epoch 380, training loss: 0.13159126043319702 = 0.06152297556400299 + 0.01 * 7.006828784942627
Epoch 380, val loss: 0.717885434627533
Epoch 390, training loss: 0.12426930665969849 = 0.05421476066112518 + 0.01 * 7.0054545402526855
Epoch 390, val loss: 0.7288975715637207
Epoch 400, training loss: 0.11809876561164856 = 0.04807274416089058 + 0.01 * 7.0026021003723145
Epoch 400, val loss: 0.7401430010795593
Epoch 410, training loss: 0.11286889761686325 = 0.042875759303569794 + 0.01 * 6.999314308166504
Epoch 410, val loss: 0.7514711618423462
Epoch 420, training loss: 0.10840754956007004 = 0.038451507687568665 + 0.01 * 6.995604038238525
Epoch 420, val loss: 0.7627432942390442
Epoch 430, training loss: 0.10462094843387604 = 0.0346592478454113 + 0.01 * 6.9961700439453125
Epoch 430, val loss: 0.7738097906112671
Epoch 440, training loss: 0.10125906765460968 = 0.03138880059123039 + 0.01 * 6.987027168273926
Epoch 440, val loss: 0.7846732139587402
Epoch 450, training loss: 0.09835854172706604 = 0.0285524632781744 + 0.01 * 6.980607509613037
Epoch 450, val loss: 0.795254647731781
Epoch 460, training loss: 0.09586305916309357 = 0.02607952617108822 + 0.01 * 6.978353023529053
Epoch 460, val loss: 0.8056161403656006
Epoch 470, training loss: 0.0935671404004097 = 0.02391272597014904 + 0.01 * 6.9654412269592285
Epoch 470, val loss: 0.8156502842903137
Epoch 480, training loss: 0.09165319800376892 = 0.02200503833591938 + 0.01 * 6.964816570281982
Epoch 480, val loss: 0.8254116773605347
Epoch 490, training loss: 0.08983822911977768 = 0.02031896822154522 + 0.01 * 6.951926231384277
Epoch 490, val loss: 0.8349945545196533
Epoch 500, training loss: 0.08841882646083832 = 0.018822766840457916 + 0.01 * 6.959606170654297
Epoch 500, val loss: 0.8442453145980835
Epoch 510, training loss: 0.08678911626338959 = 0.01748960092663765 + 0.01 * 6.929952144622803
Epoch 510, val loss: 0.853233277797699
Epoch 520, training loss: 0.08564627915620804 = 0.0162969883531332 + 0.01 * 6.934929370880127
Epoch 520, val loss: 0.8619570732116699
Epoch 530, training loss: 0.0843181163072586 = 0.015228081494569778 + 0.01 * 6.909003257751465
Epoch 530, val loss: 0.8704320788383484
Epoch 540, training loss: 0.08313938230276108 = 0.01426615845412016 + 0.01 * 6.887322902679443
Epoch 540, val loss: 0.8787096738815308
Epoch 550, training loss: 0.08211497217416763 = 0.013398639857769012 + 0.01 * 6.871633529663086
Epoch 550, val loss: 0.886767566204071
Epoch 560, training loss: 0.0811406597495079 = 0.01261469628661871 + 0.01 * 6.852596282958984
Epoch 560, val loss: 0.8945475816726685
Epoch 570, training loss: 0.08055004477500916 = 0.011902900412678719 + 0.01 * 6.864715099334717
Epoch 570, val loss: 0.9021194577217102
Epoch 580, training loss: 0.07954301685094833 = 0.01125615555793047 + 0.01 * 6.828685760498047
Epoch 580, val loss: 0.9095065593719482
Epoch 590, training loss: 0.07889857888221741 = 0.010665970854461193 + 0.01 * 6.823261260986328
Epoch 590, val loss: 0.9166114926338196
Epoch 600, training loss: 0.07797977328300476 = 0.01012567337602377 + 0.01 * 6.785410404205322
Epoch 600, val loss: 0.9237121939659119
Epoch 610, training loss: 0.07757215946912766 = 0.00963003933429718 + 0.01 * 6.794212341308594
Epoch 610, val loss: 0.9304569959640503
Epoch 620, training loss: 0.0769512951374054 = 0.009173594415187836 + 0.01 * 6.777770519256592
Epoch 620, val loss: 0.9371335506439209
Epoch 630, training loss: 0.07612231373786926 = 0.008753194473683834 + 0.01 * 6.736911773681641
Epoch 630, val loss: 0.9435015320777893
Epoch 640, training loss: 0.07576046884059906 = 0.008364326320588589 + 0.01 * 6.739614009857178
Epoch 640, val loss: 0.9497935175895691
Epoch 650, training loss: 0.07515869289636612 = 0.00800401996821165 + 0.01 * 6.715467929840088
Epoch 650, val loss: 0.955883800983429
Epoch 660, training loss: 0.075463205575943 = 0.00766912242397666 + 0.01 * 6.7794084548950195
Epoch 660, val loss: 0.9618271589279175
Epoch 670, training loss: 0.07448543608188629 = 0.007358141243457794 + 0.01 * 6.7127299308776855
Epoch 670, val loss: 0.967639148235321
Epoch 680, training loss: 0.07385919988155365 = 0.007067757658660412 + 0.01 * 6.679144382476807
Epoch 680, val loss: 0.9733301401138306
Epoch 690, training loss: 0.07362430542707443 = 0.0067963674664497375 + 0.01 * 6.682794094085693
Epoch 690, val loss: 0.9789504408836365
Epoch 700, training loss: 0.07343313843011856 = 0.006542741321027279 + 0.01 * 6.689039707183838
Epoch 700, val loss: 0.9843232035636902
Epoch 710, training loss: 0.07346141338348389 = 0.006304947659373283 + 0.01 * 6.715646743774414
Epoch 710, val loss: 0.9896914958953857
Epoch 720, training loss: 0.0726478174328804 = 0.006081731058657169 + 0.01 * 6.656608581542969
Epoch 720, val loss: 0.9948217272758484
Epoch 730, training loss: 0.0725933089852333 = 0.005871654022485018 + 0.01 * 6.672165870666504
Epoch 730, val loss: 0.9998137354850769
Epoch 740, training loss: 0.0719132348895073 = 0.005674238316714764 + 0.01 * 6.623900413513184
Epoch 740, val loss: 1.0048218965530396
Epoch 750, training loss: 0.0719171017408371 = 0.005488140974193811 + 0.01 * 6.6428961753845215
Epoch 750, val loss: 1.0096051692962646
Epoch 760, training loss: 0.07159458100795746 = 0.005312562920153141 + 0.01 * 6.628201961517334
Epoch 760, val loss: 1.0143107175827026
Epoch 770, training loss: 0.07129240036010742 = 0.00514669856056571 + 0.01 * 6.614570617675781
Epoch 770, val loss: 1.018884539604187
Epoch 780, training loss: 0.07105789333581924 = 0.004989573732018471 + 0.01 * 6.606832027435303
Epoch 780, val loss: 1.0234054327011108
Epoch 790, training loss: 0.07078693807125092 = 0.004840896464884281 + 0.01 * 6.5946044921875
Epoch 790, val loss: 1.0277636051177979
Epoch 800, training loss: 0.07049131393432617 = 0.004699820186942816 + 0.01 * 6.5791497230529785
Epoch 800, val loss: 1.0321094989776611
Epoch 810, training loss: 0.07039523124694824 = 0.004566051531583071 + 0.01 * 6.582918167114258
Epoch 810, val loss: 1.036303997039795
Epoch 820, training loss: 0.07001930475234985 = 0.004438905511051416 + 0.01 * 6.558039665222168
Epoch 820, val loss: 1.0403670072555542
Epoch 830, training loss: 0.07035761326551437 = 0.004317942075431347 + 0.01 * 6.603967666625977
Epoch 830, val loss: 1.0444258451461792
Epoch 840, training loss: 0.0698435828089714 = 0.004202982410788536 + 0.01 * 6.564060211181641
Epoch 840, val loss: 1.0483626127243042
Epoch 850, training loss: 0.07012896239757538 = 0.0040934388525784016 + 0.01 * 6.603552341461182
Epoch 850, val loss: 1.0521880388259888
Epoch 860, training loss: 0.06941083073616028 = 0.00398905947804451 + 0.01 * 6.542177677154541
Epoch 860, val loss: 1.055986762046814
Epoch 870, training loss: 0.06952722370624542 = 0.003889362560585141 + 0.01 * 6.563786506652832
Epoch 870, val loss: 1.0596646070480347
Epoch 880, training loss: 0.06932659447193146 = 0.003794129006564617 + 0.01 * 6.553246974945068
Epoch 880, val loss: 1.0633033514022827
Epoch 890, training loss: 0.06920111179351807 = 0.003703111782670021 + 0.01 * 6.549800395965576
Epoch 890, val loss: 1.0668061971664429
Epoch 900, training loss: 0.06887492537498474 = 0.0036160163581371307 + 0.01 * 6.5258917808532715
Epoch 900, val loss: 1.0703092813491821
Epoch 910, training loss: 0.06878713518381119 = 0.003532618284225464 + 0.01 * 6.52545166015625
Epoch 910, val loss: 1.073694109916687
Epoch 920, training loss: 0.06866102665662766 = 0.003452745731920004 + 0.01 * 6.5208282470703125
Epoch 920, val loss: 1.077025294303894
Epoch 930, training loss: 0.06848601996898651 = 0.003376129548996687 + 0.01 * 6.510989189147949
Epoch 930, val loss: 1.0802652835845947
Epoch 940, training loss: 0.06832929700613022 = 0.003302596276625991 + 0.01 * 6.5026702880859375
Epoch 940, val loss: 1.0835291147232056
Epoch 950, training loss: 0.06861615180969238 = 0.003232015995308757 + 0.01 * 6.5384135246276855
Epoch 950, val loss: 1.0866775512695312
Epoch 960, training loss: 0.06816659867763519 = 0.0031643682159483433 + 0.01 * 6.500223159790039
Epoch 960, val loss: 1.089778184890747
Epoch 970, training loss: 0.06867925077676773 = 0.0030994187109172344 + 0.01 * 6.5579833984375
Epoch 970, val loss: 1.0927670001983643
Epoch 980, training loss: 0.06793921440839767 = 0.0030369251035153866 + 0.01 * 6.49022912979126
Epoch 980, val loss: 1.0957882404327393
Epoch 990, training loss: 0.0680360496044159 = 0.002976902760565281 + 0.01 * 6.50591516494751
Epoch 990, val loss: 1.0987770557403564
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.6974
Flip ASR: 0.6578/225 nodes
The final ASR:0.57073, 0.15382, Accuracy:0.82593, 0.00800
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11644])
remove edge: torch.Size([2, 9504])
updated graph: torch.Size([2, 10592])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.97786, 0.00000, Accuracy:0.83333, 0.00302
Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0347421169281006 = 1.9510033130645752 + 0.01 * 8.373888969421387
Epoch 0, val loss: 1.9620479345321655
Epoch 10, training loss: 2.024320125579834 = 1.9405816793441772 + 0.01 * 8.373838424682617
Epoch 10, val loss: 1.951217532157898
Epoch 20, training loss: 2.0113062858581543 = 1.9275696277618408 + 0.01 * 8.373666763305664
Epoch 20, val loss: 1.9373670816421509
Epoch 30, training loss: 1.9930102825164795 = 1.9092785120010376 + 0.01 * 8.373173713684082
Epoch 30, val loss: 1.9177823066711426
Epoch 40, training loss: 1.9663476943969727 = 1.8826380968093872 + 0.01 * 8.370964050292969
Epoch 40, val loss: 1.8894774913787842
Epoch 50, training loss: 1.9296036958694458 = 1.8460402488708496 + 0.01 * 8.356345176696777
Epoch 50, val loss: 1.8517866134643555
Epoch 60, training loss: 1.887123703956604 = 1.8043744564056396 + 0.01 * 8.274923324584961
Epoch 60, val loss: 1.812566876411438
Epoch 70, training loss: 1.8473868370056152 = 1.7668207883834839 + 0.01 * 8.056602478027344
Epoch 70, val loss: 1.7814451456069946
Epoch 80, training loss: 1.7994974851608276 = 1.7221492528915405 + 0.01 * 7.734823703765869
Epoch 80, val loss: 1.7439138889312744
Epoch 90, training loss: 1.734988808631897 = 1.6605734825134277 + 0.01 * 7.441530227661133
Epoch 90, val loss: 1.691071629524231
Epoch 100, training loss: 1.6515244245529175 = 1.5783129930496216 + 0.01 * 7.3211469650268555
Epoch 100, val loss: 1.6207072734832764
Epoch 110, training loss: 1.5506508350372314 = 1.4782764911651611 + 0.01 * 7.237429618835449
Epoch 110, val loss: 1.538377046585083
Epoch 120, training loss: 1.4402258396148682 = 1.368541955947876 + 0.01 * 7.1683878898620605
Epoch 120, val loss: 1.4491100311279297
Epoch 130, training loss: 1.327892780303955 = 1.2566421031951904 + 0.01 * 7.125067234039307
Epoch 130, val loss: 1.3589035272598267
Epoch 140, training loss: 1.219342827796936 = 1.148357629776001 + 0.01 * 7.098519802093506
Epoch 140, val loss: 1.2723301649093628
Epoch 150, training loss: 1.1194391250610352 = 1.0486992597579956 + 0.01 * 7.073980808258057
Epoch 150, val loss: 1.1933969259262085
Epoch 160, training loss: 1.0295275449752808 = 0.9589787721633911 + 0.01 * 7.054876804351807
Epoch 160, val loss: 1.123559594154358
Epoch 170, training loss: 0.9472944736480713 = 0.8768525123596191 + 0.01 * 7.044195175170898
Epoch 170, val loss: 1.0608465671539307
Epoch 180, training loss: 0.8699644804000854 = 0.7995591759681702 + 0.01 * 7.040530681610107
Epoch 180, val loss: 1.0026384592056274
Epoch 190, training loss: 0.7961868047714233 = 0.7258251309394836 + 0.01 * 7.036169052124023
Epoch 190, val loss: 0.9480934143066406
Epoch 200, training loss: 0.7263098359107971 = 0.6559640169143677 + 0.01 * 7.034582614898682
Epoch 200, val loss: 0.8980613946914673
Epoch 210, training loss: 0.6611171960830688 = 0.5907954573631287 + 0.01 * 7.032174110412598
Epoch 210, val loss: 0.8537875413894653
Epoch 220, training loss: 0.6011807918548584 = 0.5308923721313477 + 0.01 * 7.028844833374023
Epoch 220, val loss: 0.8159743547439575
Epoch 230, training loss: 0.5468583703041077 = 0.47659212350845337 + 0.01 * 7.026623249053955
Epoch 230, val loss: 0.7848824262619019
Epoch 240, training loss: 0.49800992012023926 = 0.4277992248535156 + 0.01 * 7.0210700035095215
Epoch 240, val loss: 0.7606412768363953
Epoch 250, training loss: 0.45417752861976624 = 0.384018212556839 + 0.01 * 7.015931129455566
Epoch 250, val loss: 0.7424963116645813
Epoch 260, training loss: 0.4147801399230957 = 0.344666987657547 + 0.01 * 7.011314868927002
Epoch 260, val loss: 0.7300893068313599
Epoch 270, training loss: 0.3793109655380249 = 0.3092615306377411 + 0.01 * 7.004945278167725
Epoch 270, val loss: 0.7229235768318176
Epoch 280, training loss: 0.3472983241081238 = 0.2772938013076782 + 0.01 * 7.000452518463135
Epoch 280, val loss: 0.7204168438911438
Epoch 290, training loss: 0.3181193470954895 = 0.24817852675914764 + 0.01 * 6.994081020355225
Epoch 290, val loss: 0.7216042280197144
Epoch 300, training loss: 0.29124224185943604 = 0.22136065363883972 + 0.01 * 6.9881591796875
Epoch 300, val loss: 0.7258837223052979
Epoch 310, training loss: 0.26632243394851685 = 0.196481391787529 + 0.01 * 6.984105110168457
Epoch 310, val loss: 0.7326821088790894
Epoch 320, training loss: 0.24323561787605286 = 0.17348504066467285 + 0.01 * 6.975058555603027
Epoch 320, val loss: 0.7416772842407227
Epoch 330, training loss: 0.22216278314590454 = 0.15249453485012054 + 0.01 * 6.966825485229492
Epoch 330, val loss: 0.7526072859764099
Epoch 340, training loss: 0.2032887041568756 = 0.13364778459072113 + 0.01 * 6.964093208312988
Epoch 340, val loss: 0.7653806209564209
Epoch 350, training loss: 0.18648549914360046 = 0.11698276549577713 + 0.01 * 6.950273036956787
Epoch 350, val loss: 0.7799510955810547
Epoch 360, training loss: 0.17184853553771973 = 0.10243001580238342 + 0.01 * 6.941853046417236
Epoch 360, val loss: 0.795951783657074
Epoch 370, training loss: 0.15915009379386902 = 0.08986340463161469 + 0.01 * 6.928668975830078
Epoch 370, val loss: 0.8129256963729858
Epoch 380, training loss: 0.14856630563735962 = 0.07906920462846756 + 0.01 * 6.949710845947266
Epoch 380, val loss: 0.8305501341819763
Epoch 390, training loss: 0.13895821571350098 = 0.0698455423116684 + 0.01 * 6.911267280578613
Epoch 390, val loss: 0.8482685089111328
Epoch 400, training loss: 0.13113608956336975 = 0.06196501851081848 + 0.01 * 6.917107105255127
Epoch 400, val loss: 0.8658389449119568
Epoch 410, training loss: 0.12410996854305267 = 0.05522901192307472 + 0.01 * 6.888096332550049
Epoch 410, val loss: 0.8830366730690002
Epoch 420, training loss: 0.11827507615089417 = 0.04945734515786171 + 0.01 * 6.881773948669434
Epoch 420, val loss: 0.899901807308197
Epoch 430, training loss: 0.11296559870243073 = 0.0444951131939888 + 0.01 * 6.847048759460449
Epoch 430, val loss: 0.9163058400154114
Epoch 440, training loss: 0.10884693264961243 = 0.04020572081208229 + 0.01 * 6.864120960235596
Epoch 440, val loss: 0.9321663975715637
Epoch 450, training loss: 0.10487448424100876 = 0.03648281842470169 + 0.01 * 6.839166641235352
Epoch 450, val loss: 0.9476306438446045
Epoch 460, training loss: 0.10141623020172119 = 0.033237095922231674 + 0.01 * 6.81791353225708
Epoch 460, val loss: 0.9625999331474304
Epoch 470, training loss: 0.09862925857305527 = 0.030393404886126518 + 0.01 * 6.823585033416748
Epoch 470, val loss: 0.977157473564148
Epoch 480, training loss: 0.09581126272678375 = 0.027892863377928734 + 0.01 * 6.791839599609375
Epoch 480, val loss: 0.9912916421890259
Epoch 490, training loss: 0.09357821196317673 = 0.025684060528874397 + 0.01 * 6.7894158363342285
Epoch 490, val loss: 1.0048941373825073
Epoch 500, training loss: 0.09139809012413025 = 0.023727934807538986 + 0.01 * 6.767014980316162
Epoch 500, val loss: 1.0181679725646973
Epoch 510, training loss: 0.08954480290412903 = 0.021985772997140884 + 0.01 * 6.755903720855713
Epoch 510, val loss: 1.030995488166809
Epoch 520, training loss: 0.08788854628801346 = 0.020428471267223358 + 0.01 * 6.746007919311523
Epoch 520, val loss: 1.043366551399231
Epoch 530, training loss: 0.08669093251228333 = 0.019032886251807213 + 0.01 * 6.765804767608643
Epoch 530, val loss: 1.0554250478744507
Epoch 540, training loss: 0.08490283787250519 = 0.017777564004063606 + 0.01 * 6.712527751922607
Epoch 540, val loss: 1.06707763671875
Epoch 550, training loss: 0.08395205438137054 = 0.016644591465592384 + 0.01 * 6.730746269226074
Epoch 550, val loss: 1.0783817768096924
Epoch 560, training loss: 0.08287618309259415 = 0.01562002208083868 + 0.01 * 6.725616455078125
Epoch 560, val loss: 1.089362621307373
Epoch 570, training loss: 0.08160775899887085 = 0.014690656214952469 + 0.01 * 6.691709995269775
Epoch 570, val loss: 1.100008487701416
Epoch 580, training loss: 0.08114602416753769 = 0.013844224624335766 + 0.01 * 6.730180263519287
Epoch 580, val loss: 1.1103585958480835
Epoch 590, training loss: 0.08003495633602142 = 0.013072896748781204 + 0.01 * 6.696206569671631
Epoch 590, val loss: 1.1203618049621582
Epoch 600, training loss: 0.07916711270809174 = 0.012367083691060543 + 0.01 * 6.680002689361572
Epoch 600, val loss: 1.1300482749938965
Epoch 610, training loss: 0.07838679105043411 = 0.011720064096152782 + 0.01 * 6.666673183441162
Epoch 610, val loss: 1.1395092010498047
Epoch 620, training loss: 0.07766657322645187 = 0.011125246994197369 + 0.01 * 6.654132843017578
Epoch 620, val loss: 1.1487289667129517
Epoch 630, training loss: 0.07713036984205246 = 0.010576950386166573 + 0.01 * 6.6553425788879395
Epoch 630, val loss: 1.1576807498931885
Epoch 640, training loss: 0.07681678235530853 = 0.010070856660604477 + 0.01 * 6.674593448638916
Epoch 640, val loss: 1.16641366481781
Epoch 650, training loss: 0.0761292427778244 = 0.009602402336895466 + 0.01 * 6.652684211730957
Epoch 650, val loss: 1.174811840057373
Epoch 660, training loss: 0.0755130872130394 = 0.009169070050120354 + 0.01 * 6.634401321411133
Epoch 660, val loss: 1.1830779314041138
Epoch 670, training loss: 0.07521320134401321 = 0.008766280487179756 + 0.01 * 6.644692420959473
Epoch 670, val loss: 1.1910940408706665
Epoch 680, training loss: 0.07468811422586441 = 0.008391167037189007 + 0.01 * 6.629694938659668
Epoch 680, val loss: 1.1988600492477417
Epoch 690, training loss: 0.07419087737798691 = 0.008042043074965477 + 0.01 * 6.6148834228515625
Epoch 690, val loss: 1.206513524055481
Epoch 700, training loss: 0.07395360618829727 = 0.007716536987572908 + 0.01 * 6.623707294464111
Epoch 700, val loss: 1.213888168334961
Epoch 710, training loss: 0.07362335920333862 = 0.0074124448001384735 + 0.01 * 6.621091842651367
Epoch 710, val loss: 1.221123218536377
Epoch 720, training loss: 0.07316712290048599 = 0.007127557415515184 + 0.01 * 6.60395622253418
Epoch 720, val loss: 1.2281659841537476
Epoch 730, training loss: 0.07316567003726959 = 0.006860151421278715 + 0.01 * 6.630551815032959
Epoch 730, val loss: 1.235039472579956
Epoch 740, training loss: 0.07250586152076721 = 0.006609824951738119 + 0.01 * 6.589603900909424
Epoch 740, val loss: 1.2417707443237305
Epoch 750, training loss: 0.07229939103126526 = 0.00637389300391078 + 0.01 * 6.592550277709961
Epoch 750, val loss: 1.2483313083648682
Epoch 760, training loss: 0.07241500169038773 = 0.006151735316962004 + 0.01 * 6.626326560974121
Epoch 760, val loss: 1.2547227144241333
Epoch 770, training loss: 0.07177641242742538 = 0.005942922551184893 + 0.01 * 6.583348751068115
Epoch 770, val loss: 1.261073350906372
Epoch 780, training loss: 0.07171578705310822 = 0.0057456037029623985 + 0.01 * 6.597018718719482
Epoch 780, val loss: 1.267203450202942
Epoch 790, training loss: 0.07125551253557205 = 0.005559230223298073 + 0.01 * 6.5696282386779785
Epoch 790, val loss: 1.273212194442749
Epoch 800, training loss: 0.07148435711860657 = 0.005382705945521593 + 0.01 * 6.610165119171143
Epoch 800, val loss: 1.2791087627410889
Epoch 810, training loss: 0.07096543908119202 = 0.005216301418840885 + 0.01 * 6.57491397857666
Epoch 810, val loss: 1.2848620414733887
Epoch 820, training loss: 0.070639967918396 = 0.005058194510638714 + 0.01 * 6.558177471160889
Epoch 820, val loss: 1.2904810905456543
Epoch 830, training loss: 0.07046468555927277 = 0.0049082497134804726 + 0.01 * 6.555643558502197
Epoch 830, val loss: 1.295972466468811
Epoch 840, training loss: 0.07052828371524811 = 0.004765988327562809 + 0.01 * 6.576230049133301
Epoch 840, val loss: 1.3013769388198853
Epoch 850, training loss: 0.07030140608549118 = 0.004630610346794128 + 0.01 * 6.567079544067383
Epoch 850, val loss: 1.3066446781158447
Epoch 860, training loss: 0.07010390609502792 = 0.004502147436141968 + 0.01 * 6.560176372528076
Epoch 860, val loss: 1.311866044998169
Epoch 870, training loss: 0.06969431042671204 = 0.004379586316645145 + 0.01 * 6.531472682952881
Epoch 870, val loss: 1.316899299621582
Epoch 880, training loss: 0.06968957185745239 = 0.0042629060335457325 + 0.01 * 6.542666435241699
Epoch 880, val loss: 1.3218278884887695
Epoch 890, training loss: 0.06955394148826599 = 0.004151933826506138 + 0.01 * 6.540200710296631
Epoch 890, val loss: 1.3266677856445312
Epoch 900, training loss: 0.06928195804357529 = 0.004045610316097736 + 0.01 * 6.523634910583496
Epoch 900, val loss: 1.3314274549484253
Epoch 910, training loss: 0.06939429044723511 = 0.003944260533899069 + 0.01 * 6.5450029373168945
Epoch 910, val loss: 1.3361576795578003
Epoch 920, training loss: 0.06922466307878494 = 0.0038472714368253946 + 0.01 * 6.537739276885986
Epoch 920, val loss: 1.3407045602798462
Epoch 930, training loss: 0.06906783580780029 = 0.0037547852843999863 + 0.01 * 6.531304836273193
Epoch 930, val loss: 1.3452094793319702
Epoch 940, training loss: 0.06887000799179077 = 0.0036660542245954275 + 0.01 * 6.5203962326049805
Epoch 940, val loss: 1.3495991230010986
Epoch 950, training loss: 0.06874566525220871 = 0.003581166500225663 + 0.01 * 6.51645040512085
Epoch 950, val loss: 1.353899359703064
Epoch 960, training loss: 0.06867535412311554 = 0.0034997756592929363 + 0.01 * 6.5175580978393555
Epoch 960, val loss: 1.3581461906433105
Epoch 970, training loss: 0.06856761872768402 = 0.0034215047489851713 + 0.01 * 6.51461124420166
Epoch 970, val loss: 1.362273097038269
Epoch 980, training loss: 0.06887693703174591 = 0.0033462170977145433 + 0.01 * 6.553071975708008
Epoch 980, val loss: 1.3662834167480469
Epoch 990, training loss: 0.06830039620399475 = 0.0032743036281317472 + 0.01 * 6.502608776092529
Epoch 990, val loss: 1.370331883430481
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.6347
Flip ASR: 0.5600/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0273523330688477 = 1.9436129331588745 + 0.01 * 8.373931884765625
Epoch 0, val loss: 1.9431480169296265
Epoch 10, training loss: 2.017742156982422 = 1.9340033531188965 + 0.01 * 8.373876571655273
Epoch 10, val loss: 1.9330509901046753
Epoch 20, training loss: 2.0057032108306885 = 1.9219659566879272 + 0.01 * 8.373723983764648
Epoch 20, val loss: 1.920450210571289
Epoch 30, training loss: 1.988572359085083 = 1.904839277267456 + 0.01 * 8.373313903808594
Epoch 30, val loss: 1.902708649635315
Epoch 40, training loss: 1.9629523754119873 = 1.8792377710342407 + 0.01 * 8.371459007263184
Epoch 40, val loss: 1.8767147064208984
Epoch 50, training loss: 1.9264898300170898 = 1.8429235219955444 + 0.01 * 8.356635093688965
Epoch 50, val loss: 1.8416601419448853
Epoch 60, training loss: 1.8821702003479004 = 1.7996013164520264 + 0.01 * 8.256888389587402
Epoch 60, val loss: 1.803539514541626
Epoch 70, training loss: 1.8368955850601196 = 1.7563647031784058 + 0.01 * 8.053092002868652
Epoch 70, val loss: 1.7681574821472168
Epoch 80, training loss: 1.7818639278411865 = 1.7030097246170044 + 0.01 * 7.885426044464111
Epoch 80, val loss: 1.7221628427505493
Epoch 90, training loss: 1.706864595413208 = 1.6302231550216675 + 0.01 * 7.664144992828369
Epoch 90, val loss: 1.660056471824646
Epoch 100, training loss: 1.6131829023361206 = 1.5383901596069336 + 0.01 * 7.47927713394165
Epoch 100, val loss: 1.5850294828414917
Epoch 110, training loss: 1.511574625968933 = 1.4381481409072876 + 0.01 * 7.342650890350342
Epoch 110, val loss: 1.5046831369400024
Epoch 120, training loss: 1.412856936454773 = 1.3401975631713867 + 0.01 * 7.2659430503845215
Epoch 120, val loss: 1.4298547506332397
Epoch 130, training loss: 1.320586085319519 = 1.2484709024429321 + 0.01 * 7.211522102355957
Epoch 130, val loss: 1.3630173206329346
Epoch 140, training loss: 1.2343180179595947 = 1.162493348121643 + 0.01 * 7.182469844818115
Epoch 140, val loss: 1.3022271394729614
Epoch 150, training loss: 1.1525408029556274 = 1.080980896949768 + 0.01 * 7.155990123748779
Epoch 150, val loss: 1.2445482015609741
Epoch 160, training loss: 1.0747379064559937 = 1.0034366846084595 + 0.01 * 7.130120277404785
Epoch 160, val loss: 1.1888248920440674
Epoch 170, training loss: 1.0009865760803223 = 0.9300102591514587 + 0.01 * 7.09763240814209
Epoch 170, val loss: 1.1357170343399048
Epoch 180, training loss: 0.9310964345932007 = 0.8604128360748291 + 0.01 * 7.068362712860107
Epoch 180, val loss: 1.0855644941329956
Epoch 190, training loss: 0.8645686507225037 = 0.7940854430198669 + 0.01 * 7.04832124710083
Epoch 190, val loss: 1.0381391048431396
Epoch 200, training loss: 0.801693856716156 = 0.7313236594200134 + 0.01 * 7.037019729614258
Epoch 200, val loss: 0.9940633177757263
Epoch 210, training loss: 0.7431764006614685 = 0.6728858947753906 + 0.01 * 7.029052257537842
Epoch 210, val loss: 0.954238772392273
Epoch 220, training loss: 0.6890168786048889 = 0.6187766790390015 + 0.01 * 7.024020671844482
Epoch 220, val loss: 0.9190948009490967
Epoch 230, training loss: 0.6381994485855103 = 0.568015456199646 + 0.01 * 7.018398284912109
Epoch 230, val loss: 0.8883734345436096
Epoch 240, training loss: 0.5896612405776978 = 0.5195192694664001 + 0.01 * 7.01419734954834
Epoch 240, val loss: 0.8619672656059265
Epoch 250, training loss: 0.5428083539009094 = 0.4727434515953064 + 0.01 * 7.00648832321167
Epoch 250, val loss: 0.8398830890655518
Epoch 260, training loss: 0.49768590927124023 = 0.42763975262641907 + 0.01 * 7.0046162605285645
Epoch 260, val loss: 0.8220154047012329
Epoch 270, training loss: 0.4545705020427704 = 0.38459834456443787 + 0.01 * 6.9972147941589355
Epoch 270, val loss: 0.8083099126815796
Epoch 280, training loss: 0.4138801693916321 = 0.3439466059207916 + 0.01 * 6.993357181549072
Epoch 280, val loss: 0.7986891269683838
Epoch 290, training loss: 0.37589365243911743 = 0.30599188804626465 + 0.01 * 6.990176677703857
Epoch 290, val loss: 0.7925224304199219
Epoch 300, training loss: 0.34081578254699707 = 0.27095362544059753 + 0.01 * 6.9862141609191895
Epoch 300, val loss: 0.7892518043518066
Epoch 310, training loss: 0.30881184339523315 = 0.2389826476573944 + 0.01 * 6.9829206466674805
Epoch 310, val loss: 0.7886602878570557
Epoch 320, training loss: 0.2799054980278015 = 0.2101154327392578 + 0.01 * 6.979008197784424
Epoch 320, val loss: 0.7905151844024658
Epoch 330, training loss: 0.25406795740127563 = 0.18431928753852844 + 0.01 * 6.974865436553955
Epoch 330, val loss: 0.7945922613143921
Epoch 340, training loss: 0.23125261068344116 = 0.16155245900154114 + 0.01 * 6.970016002655029
Epoch 340, val loss: 0.8006921410560608
Epoch 350, training loss: 0.21122246980667114 = 0.14157511293888092 + 0.01 * 6.964735507965088
Epoch 350, val loss: 0.8084473609924316
Epoch 360, training loss: 0.19382360577583313 = 0.12423063069581985 + 0.01 * 6.959297180175781
Epoch 360, val loss: 0.8177877068519592
Epoch 370, training loss: 0.1787845492362976 = 0.10923436284065247 + 0.01 * 6.955019474029541
Epoch 370, val loss: 0.8283208012580872
Epoch 380, training loss: 0.16579070687294006 = 0.09626779705286026 + 0.01 * 6.952290058135986
Epoch 380, val loss: 0.8398898243904114
Epoch 390, training loss: 0.1545037478208542 = 0.08503716439008713 + 0.01 * 6.946658611297607
Epoch 390, val loss: 0.8520699143409729
Epoch 400, training loss: 0.1447753757238388 = 0.07529930770397186 + 0.01 * 6.947607040405273
Epoch 400, val loss: 0.8648847937583923
Epoch 410, training loss: 0.1361916959285736 = 0.06685533374547958 + 0.01 * 6.933636665344238
Epoch 410, val loss: 0.8780993819236755
Epoch 420, training loss: 0.12880130112171173 = 0.05954483151435852 + 0.01 * 6.925647258758545
Epoch 420, val loss: 0.8918188214302063
Epoch 430, training loss: 0.12241870164871216 = 0.05320219323039055 + 0.01 * 6.921650409698486
Epoch 430, val loss: 0.9056572318077087
Epoch 440, training loss: 0.11687293648719788 = 0.047687824815511703 + 0.01 * 6.918511867523193
Epoch 440, val loss: 0.9196809530258179
Epoch 450, training loss: 0.11195479333400726 = 0.04289551451802254 + 0.01 * 6.905927658081055
Epoch 450, val loss: 0.9337517023086548
Epoch 460, training loss: 0.10774658620357513 = 0.03872871771454811 + 0.01 * 6.901786804199219
Epoch 460, val loss: 0.9477065801620483
Epoch 470, training loss: 0.10401050746440887 = 0.0351010300219059 + 0.01 * 6.890948295593262
Epoch 470, val loss: 0.9614664912223816
Epoch 480, training loss: 0.1008664071559906 = 0.03192929923534393 + 0.01 * 6.893710613250732
Epoch 480, val loss: 0.9749491214752197
Epoch 490, training loss: 0.09798865765333176 = 0.029148975387215614 + 0.01 * 6.883968353271484
Epoch 490, val loss: 0.9880920648574829
Epoch 500, training loss: 0.09542237967252731 = 0.026688260957598686 + 0.01 * 6.873412132263184
Epoch 500, val loss: 1.000893235206604
Epoch 510, training loss: 0.09318999946117401 = 0.024505136534571648 + 0.01 * 6.868485927581787
Epoch 510, val loss: 1.013367772102356
Epoch 520, training loss: 0.09119756519794464 = 0.022564992308616638 + 0.01 * 6.86325740814209
Epoch 520, val loss: 1.0254690647125244
Epoch 530, training loss: 0.08937622606754303 = 0.02083861641585827 + 0.01 * 6.853760719299316
Epoch 530, val loss: 1.037325143814087
Epoch 540, training loss: 0.0877106562256813 = 0.01929333806037903 + 0.01 * 6.841732025146484
Epoch 540, val loss: 1.0487667322158813
Epoch 550, training loss: 0.08672156184911728 = 0.01790488325059414 + 0.01 * 6.881667613983154
Epoch 550, val loss: 1.0599193572998047
Epoch 560, training loss: 0.08511289954185486 = 0.016664370894432068 + 0.01 * 6.844852924346924
Epoch 560, val loss: 1.0708389282226562
Epoch 570, training loss: 0.08383038640022278 = 0.015545741654932499 + 0.01 * 6.828464508056641
Epoch 570, val loss: 1.0812770128250122
Epoch 580, training loss: 0.08268125355243683 = 0.014531809836626053 + 0.01 * 6.814944744110107
Epoch 580, val loss: 1.0914839506149292
Epoch 590, training loss: 0.08172607421875 = 0.013609424233436584 + 0.01 * 6.811665058135986
Epoch 590, val loss: 1.1014882326126099
Epoch 600, training loss: 0.08092862367630005 = 0.012773584574460983 + 0.01 * 6.8155035972595215
Epoch 600, val loss: 1.1110106706619263
Epoch 610, training loss: 0.08004101365804672 = 0.012014508247375488 + 0.01 * 6.802650451660156
Epoch 610, val loss: 1.1204345226287842
Epoch 620, training loss: 0.07926438748836517 = 0.011322624050080776 + 0.01 * 6.79417610168457
Epoch 620, val loss: 1.1294595003128052
Epoch 630, training loss: 0.07855778932571411 = 0.010690480470657349 + 0.01 * 6.786730766296387
Epoch 630, val loss: 1.1383126974105835
Epoch 640, training loss: 0.07786639034748077 = 0.010110881179571152 + 0.01 * 6.7755513191223145
Epoch 640, val loss: 1.1468955278396606
Epoch 650, training loss: 0.07761326432228088 = 0.009577919729053974 + 0.01 * 6.803534030914307
Epoch 650, val loss: 1.1552538871765137
Epoch 660, training loss: 0.0768149197101593 = 0.009088280610740185 + 0.01 * 6.7726640701293945
Epoch 660, val loss: 1.1633689403533936
Epoch 670, training loss: 0.0763150155544281 = 0.008637365885078907 + 0.01 * 6.767764568328857
Epoch 670, val loss: 1.1711288690567017
Epoch 680, training loss: 0.0757828876376152 = 0.008221358992159367 + 0.01 * 6.756153106689453
Epoch 680, val loss: 1.1787911653518677
Epoch 690, training loss: 0.07541663199663162 = 0.007836336269974709 + 0.01 * 6.758029937744141
Epoch 690, val loss: 1.1862367391586304
Epoch 700, training loss: 0.07488417625427246 = 0.007479200605303049 + 0.01 * 6.740497589111328
Epoch 700, val loss: 1.1934845447540283
Epoch 710, training loss: 0.07450972497463226 = 0.007147631607949734 + 0.01 * 6.736209392547607
Epoch 710, val loss: 1.2005475759506226
Epoch 720, training loss: 0.07421722263097763 = 0.006839571055024862 + 0.01 * 6.737765312194824
Epoch 720, val loss: 1.2074233293533325
Epoch 730, training loss: 0.07379017770290375 = 0.006552938837558031 + 0.01 * 6.723723888397217
Epoch 730, val loss: 1.2141257524490356
Epoch 740, training loss: 0.07357986271381378 = 0.00628521665930748 + 0.01 * 6.729465484619141
Epoch 740, val loss: 1.2207099199295044
Epoch 750, training loss: 0.07354721426963806 = 0.006035150960087776 + 0.01 * 6.751206874847412
Epoch 750, val loss: 1.2270132303237915
Epoch 760, training loss: 0.07287774980068207 = 0.005802549887448549 + 0.01 * 6.707520484924316
Epoch 760, val loss: 1.2331644296646118
Epoch 770, training loss: 0.07258256524801254 = 0.005584660451859236 + 0.01 * 6.699790954589844
Epoch 770, val loss: 1.2392654418945312
Epoch 780, training loss: 0.07236063480377197 = 0.005380081012845039 + 0.01 * 6.698055744171143
Epoch 780, val loss: 1.2450895309448242
Epoch 790, training loss: 0.07210348546504974 = 0.005187750328332186 + 0.01 * 6.691573619842529
Epoch 790, val loss: 1.2509030103683472
Epoch 800, training loss: 0.07187434285879135 = 0.005006704945117235 + 0.01 * 6.686763286590576
Epoch 800, val loss: 1.2565128803253174
Epoch 810, training loss: 0.07171685248613358 = 0.00483622495085001 + 0.01 * 6.688063144683838
Epoch 810, val loss: 1.262017846107483
Epoch 820, training loss: 0.07150958478450775 = 0.004675459116697311 + 0.01 * 6.683412551879883
Epoch 820, val loss: 1.267330288887024
Epoch 830, training loss: 0.07135165482759476 = 0.004523745737969875 + 0.01 * 6.682790756225586
Epoch 830, val loss: 1.2726168632507324
Epoch 840, training loss: 0.07122407108545303 = 0.004380626138299704 + 0.01 * 6.684344291687012
Epoch 840, val loss: 1.2776737213134766
Epoch 850, training loss: 0.07094590365886688 = 0.004245285876095295 + 0.01 * 6.6700615882873535
Epoch 850, val loss: 1.2826730012893677
Epoch 860, training loss: 0.07066664099693298 = 0.004116978496313095 + 0.01 * 6.654965877532959
Epoch 860, val loss: 1.2876005172729492
Epoch 870, training loss: 0.07048948109149933 = 0.003995275590568781 + 0.01 * 6.649420738220215
Epoch 870, val loss: 1.292424201965332
Epoch 880, training loss: 0.07024753093719482 = 0.003879570635035634 + 0.01 * 6.636796474456787
Epoch 880, val loss: 1.296950340270996
Epoch 890, training loss: 0.07020051777362823 = 0.00377006852068007 + 0.01 * 6.643045425415039
Epoch 890, val loss: 1.3016282320022583
Epoch 900, training loss: 0.07028284668922424 = 0.0036659580655395985 + 0.01 * 6.661688327789307
Epoch 900, val loss: 1.3060262203216553
Epoch 910, training loss: 0.06992784142494202 = 0.0035670562647283077 + 0.01 * 6.636078834533691
Epoch 910, val loss: 1.3104678392410278
Epoch 920, training loss: 0.06979347765445709 = 0.003472868585959077 + 0.01 * 6.632060527801514
Epoch 920, val loss: 1.3147302865982056
Epoch 930, training loss: 0.06970291584730148 = 0.0033829794265329838 + 0.01 * 6.631993770599365
Epoch 930, val loss: 1.3189209699630737
Epoch 940, training loss: 0.06947388499975204 = 0.0032971964683383703 + 0.01 * 6.617669105529785
Epoch 940, val loss: 1.3230615854263306
Epoch 950, training loss: 0.06931886821985245 = 0.003215313656255603 + 0.01 * 6.610355377197266
Epoch 950, val loss: 1.3269752264022827
Epoch 960, training loss: 0.06926271319389343 = 0.003137211548164487 + 0.01 * 6.612550735473633
Epoch 960, val loss: 1.3310350179672241
Epoch 970, training loss: 0.06911295652389526 = 0.003062676405534148 + 0.01 * 6.6050286293029785
Epoch 970, val loss: 1.3348064422607422
Epoch 980, training loss: 0.06898079812526703 = 0.0029914157930761576 + 0.01 * 6.598938465118408
Epoch 980, val loss: 1.3386304378509521
Epoch 990, training loss: 0.06897889077663422 = 0.002923104679211974 + 0.01 * 6.605578899383545
Epoch 990, val loss: 1.3422324657440186
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.4000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.2841
Flip ASR: 0.2222/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0329034328460693 = 1.949164628982544 + 0.01 * 8.373869895935059
Epoch 0, val loss: 1.9563653469085693
Epoch 10, training loss: 2.0225419998168945 = 1.9388041496276855 + 0.01 * 8.373786926269531
Epoch 10, val loss: 1.9463998079299927
Epoch 20, training loss: 2.00944185256958 = 1.9257066249847412 + 0.01 * 8.373530387878418
Epoch 20, val loss: 1.933431625366211
Epoch 30, training loss: 1.9907416105270386 = 1.9070143699645996 + 0.01 * 8.372720718383789
Epoch 30, val loss: 1.9145008325576782
Epoch 40, training loss: 1.9628673791885376 = 1.8791868686676025 + 0.01 * 8.368046760559082
Epoch 40, val loss: 1.8864527940750122
Epoch 50, training loss: 1.9240871667861938 = 1.8407206535339355 + 0.01 * 8.33665657043457
Epoch 50, val loss: 1.8491934537887573
Epoch 60, training loss: 1.8806676864624023 = 1.799202561378479 + 0.01 * 8.14651870727539
Epoch 60, val loss: 1.8123770952224731
Epoch 70, training loss: 1.8424774408340454 = 1.7623344659805298 + 0.01 * 8.014300346374512
Epoch 70, val loss: 1.7797679901123047
Epoch 80, training loss: 1.7923120260238647 = 1.7139853239059448 + 0.01 * 7.832670211791992
Epoch 80, val loss: 1.7355443239212036
Epoch 90, training loss: 1.7237199544906616 = 1.6473766565322876 + 0.01 * 7.634327411651611
Epoch 90, val loss: 1.6785268783569336
Epoch 100, training loss: 1.6350053548812866 = 1.5610463619232178 + 0.01 * 7.3958964347839355
Epoch 100, val loss: 1.6075648069381714
Epoch 110, training loss: 1.5377836227416992 = 1.4650959968566895 + 0.01 * 7.268760681152344
Epoch 110, val loss: 1.5289900302886963
Epoch 120, training loss: 1.4428006410598755 = 1.3709546327590942 + 0.01 * 7.184601306915283
Epoch 120, val loss: 1.4550238847732544
Epoch 130, training loss: 1.352087378501892 = 1.280655026435852 + 0.01 * 7.143232345581055
Epoch 130, val loss: 1.3858031034469604
Epoch 140, training loss: 1.2630696296691895 = 1.1918357610702515 + 0.01 * 7.123390197753906
Epoch 140, val loss: 1.3188481330871582
Epoch 150, training loss: 1.174117922782898 = 1.1030306816101074 + 0.01 * 7.108729362487793
Epoch 150, val loss: 1.2532652616500854
Epoch 160, training loss: 1.0841155052185059 = 1.0131858587265015 + 0.01 * 7.092967987060547
Epoch 160, val loss: 1.1872334480285645
Epoch 170, training loss: 0.9933719635009766 = 0.922660768032074 + 0.01 * 7.0711188316345215
Epoch 170, val loss: 1.1204419136047363
Epoch 180, training loss: 0.904262900352478 = 0.833817720413208 + 0.01 * 7.044517517089844
Epoch 180, val loss: 1.055253028869629
Epoch 190, training loss: 0.820419430732727 = 0.7501669526100159 + 0.01 * 7.025249004364014
Epoch 190, val loss: 0.9939558506011963
Epoch 200, training loss: 0.7442309856414795 = 0.6741700768470764 + 0.01 * 7.0060930252075195
Epoch 200, val loss: 0.9396657347679138
Epoch 210, training loss: 0.6762838363647461 = 0.6064192056655884 + 0.01 * 6.986464500427246
Epoch 210, val loss: 0.8933953642845154
Epoch 220, training loss: 0.6158521175384521 = 0.5460162162780762 + 0.01 * 6.983587265014648
Epoch 220, val loss: 0.8550736308097839
Epoch 230, training loss: 0.5613292455673218 = 0.49170738458633423 + 0.01 * 6.962189197540283
Epoch 230, val loss: 0.8239614367485046
Epoch 240, training loss: 0.5116688013076782 = 0.4422409236431122 + 0.01 * 6.9427876472473145
Epoch 240, val loss: 0.7990845441818237
Epoch 250, training loss: 0.4659203290939331 = 0.3966189920902252 + 0.01 * 6.930135250091553
Epoch 250, val loss: 0.7796869874000549
Epoch 260, training loss: 0.4234124720096588 = 0.3541804552078247 + 0.01 * 6.9232025146484375
Epoch 260, val loss: 0.7651978135108948
Epoch 270, training loss: 0.38382309675216675 = 0.3147279620170593 + 0.01 * 6.909512519836426
Epoch 270, val loss: 0.7550987005233765
Epoch 280, training loss: 0.347395658493042 = 0.27833467721939087 + 0.01 * 6.906097412109375
Epoch 280, val loss: 0.748901903629303
Epoch 290, training loss: 0.31418895721435547 = 0.2452288568019867 + 0.01 * 6.89600944519043
Epoch 290, val loss: 0.7462218403816223
Epoch 300, training loss: 0.2843772768974304 = 0.2155238538980484 + 0.01 * 6.8853440284729
Epoch 300, val loss: 0.7465806603431702
Epoch 310, training loss: 0.25802236795425415 = 0.18918688595294952 + 0.01 * 6.883547782897949
Epoch 310, val loss: 0.7499248385429382
Epoch 320, training loss: 0.2347983717918396 = 0.1660589873790741 + 0.01 * 6.873939037322998
Epoch 320, val loss: 0.7559689879417419
Epoch 330, training loss: 0.2146914303302765 = 0.14588898420333862 + 0.01 * 6.880244731903076
Epoch 330, val loss: 0.764590859413147
Epoch 340, training loss: 0.1970202624797821 = 0.12839700281620026 + 0.01 * 6.862325668334961
Epoch 340, val loss: 0.7754055261611938
Epoch 350, training loss: 0.1818101704120636 = 0.1132533922791481 + 0.01 * 6.855678558349609
Epoch 350, val loss: 0.7881232500076294
Epoch 360, training loss: 0.16892576217651367 = 0.1001519039273262 + 0.01 * 6.877386569976807
Epoch 360, val loss: 0.8023318648338318
Epoch 370, training loss: 0.15735822916030884 = 0.08883427828550339 + 0.01 * 6.852395057678223
Epoch 370, val loss: 0.8176170587539673
Epoch 380, training loss: 0.14741677045822144 = 0.07903611660003662 + 0.01 * 6.838066101074219
Epoch 380, val loss: 0.8335573673248291
Epoch 390, training loss: 0.13884961605072021 = 0.07054127752780914 + 0.01 * 6.830833911895752
Epoch 390, val loss: 0.8498470783233643
Epoch 400, training loss: 0.13158926367759705 = 0.06316690146923065 + 0.01 * 6.84223747253418
Epoch 400, val loss: 0.8661507964134216
Epoch 410, training loss: 0.12498222291469574 = 0.05675932392477989 + 0.01 * 6.822289943695068
Epoch 410, val loss: 0.8822416663169861
Epoch 420, training loss: 0.11939752846956253 = 0.051175378262996674 + 0.01 * 6.8222150802612305
Epoch 420, val loss: 0.8980220556259155
Epoch 430, training loss: 0.11441203951835632 = 0.04630031809210777 + 0.01 * 6.8111724853515625
Epoch 430, val loss: 0.9134272336959839
Epoch 440, training loss: 0.11002865433692932 = 0.04203009605407715 + 0.01 * 6.799856185913086
Epoch 440, val loss: 0.9283585548400879
Epoch 450, training loss: 0.10621115565299988 = 0.03827635571360588 + 0.01 * 6.793480396270752
Epoch 450, val loss: 0.9429765343666077
Epoch 460, training loss: 0.10275496542453766 = 0.034968052059412 + 0.01 * 6.778691291809082
Epoch 460, val loss: 0.9571965932846069
Epoch 470, training loss: 0.0999423936009407 = 0.032045602798461914 + 0.01 * 6.789679050445557
Epoch 470, val loss: 0.9710469841957092
Epoch 480, training loss: 0.09717532992362976 = 0.029458919540047646 + 0.01 * 6.771641731262207
Epoch 480, val loss: 0.9845082759857178
Epoch 490, training loss: 0.09472925961017609 = 0.02715946175158024 + 0.01 * 6.7569804191589355
Epoch 490, val loss: 0.9976857304573059
Epoch 500, training loss: 0.09265938401222229 = 0.02510886825621128 + 0.01 * 6.755051612854004
Epoch 500, val loss: 1.0104862451553345
Epoch 510, training loss: 0.09071845561265945 = 0.023276103660464287 + 0.01 * 6.744235515594482
Epoch 510, val loss: 1.022953748703003
Epoch 520, training loss: 0.0889710858464241 = 0.021633291617035866 + 0.01 * 6.7337799072265625
Epoch 520, val loss: 1.0350806713104248
Epoch 530, training loss: 0.08747737109661102 = 0.020156297832727432 + 0.01 * 6.732108116149902
Epoch 530, val loss: 1.0469000339508057
Epoch 540, training loss: 0.08633043617010117 = 0.018825097009539604 + 0.01 * 6.7505340576171875
Epoch 540, val loss: 1.0583974123001099
Epoch 550, training loss: 0.0847257673740387 = 0.017623569816350937 + 0.01 * 6.710219860076904
Epoch 550, val loss: 1.0694606304168701
Epoch 560, training loss: 0.0835326686501503 = 0.016534892842173576 + 0.01 * 6.699777603149414
Epoch 560, val loss: 1.080254316329956
Epoch 570, training loss: 0.0825243815779686 = 0.015545343980193138 + 0.01 * 6.697903633117676
Epoch 570, val loss: 1.0907329320907593
Epoch 580, training loss: 0.08152133226394653 = 0.014643782749772072 + 0.01 * 6.687755584716797
Epoch 580, val loss: 1.100956678390503
Epoch 590, training loss: 0.08063247799873352 = 0.01382069755345583 + 0.01 * 6.681178092956543
Epoch 590, val loss: 1.1108523607254028
Epoch 600, training loss: 0.07973620295524597 = 0.013067912310361862 + 0.01 * 6.6668291091918945
Epoch 600, val loss: 1.120509147644043
Epoch 610, training loss: 0.07907684892416 = 0.012377090752124786 + 0.01 * 6.669975757598877
Epoch 610, val loss: 1.129865050315857
Epoch 620, training loss: 0.07836425304412842 = 0.01174218486994505 + 0.01 * 6.662207126617432
Epoch 620, val loss: 1.1389877796173096
Epoch 630, training loss: 0.07779516279697418 = 0.011156796477735043 + 0.01 * 6.663836479187012
Epoch 630, val loss: 1.147871732711792
Epoch 640, training loss: 0.07712683081626892 = 0.010616626590490341 + 0.01 * 6.651020526885986
Epoch 640, val loss: 1.1564459800720215
Epoch 650, training loss: 0.07662234455347061 = 0.010117143392562866 + 0.01 * 6.650520324707031
Epoch 650, val loss: 1.1649011373519897
Epoch 660, training loss: 0.07605333626270294 = 0.009654597379267216 + 0.01 * 6.63987398147583
Epoch 660, val loss: 1.1730166673660278
Epoch 670, training loss: 0.07551593333482742 = 0.009225437417626381 + 0.01 * 6.629050254821777
Epoch 670, val loss: 1.1809831857681274
Epoch 680, training loss: 0.07501250505447388 = 0.0088261179625988 + 0.01 * 6.618638515472412
Epoch 680, val loss: 1.1887317895889282
Epoch 690, training loss: 0.07469048351049423 = 0.008454153314232826 + 0.01 * 6.62363338470459
Epoch 690, val loss: 1.1963025331497192
Epoch 700, training loss: 0.07432369887828827 = 0.008107088506221771 + 0.01 * 6.621661186218262
Epoch 700, val loss: 1.2037146091461182
Epoch 710, training loss: 0.07390668988227844 = 0.007782557979226112 + 0.01 * 6.6124138832092285
Epoch 710, val loss: 1.2108937501907349
Epoch 720, training loss: 0.07359043508768082 = 0.0074789137579500675 + 0.01 * 6.611152172088623
Epoch 720, val loss: 1.2179365158081055
Epoch 730, training loss: 0.07311757653951645 = 0.007194438949227333 + 0.01 * 6.59231424331665
Epoch 730, val loss: 1.2247998714447021
Epoch 740, training loss: 0.07292769104242325 = 0.006927849259227514 + 0.01 * 6.599984645843506
Epoch 740, val loss: 1.23146390914917
Epoch 750, training loss: 0.07250373065471649 = 0.006676871329545975 + 0.01 * 6.582686424255371
Epoch 750, val loss: 1.2379896640777588
Epoch 760, training loss: 0.07252821326255798 = 0.006440645549446344 + 0.01 * 6.608757019042969
Epoch 760, val loss: 1.244395136833191
Epoch 770, training loss: 0.07207521796226501 = 0.006218398455530405 + 0.01 * 6.585682392120361
Epoch 770, val loss: 1.2505900859832764
Epoch 780, training loss: 0.07169315218925476 = 0.006008561700582504 + 0.01 * 6.5684590339660645
Epoch 780, val loss: 1.2566970586776733
Epoch 790, training loss: 0.07162704318761826 = 0.005810737609863281 + 0.01 * 6.581630706787109
Epoch 790, val loss: 1.2626734972000122
Epoch 800, training loss: 0.07140146940946579 = 0.005623742006719112 + 0.01 * 6.577773094177246
Epoch 800, val loss: 1.268479585647583
Epoch 810, training loss: 0.07107546925544739 = 0.005446643568575382 + 0.01 * 6.562882900238037
Epoch 810, val loss: 1.274125576019287
Epoch 820, training loss: 0.07107098400592804 = 0.005278579890727997 + 0.01 * 6.579240798950195
Epoch 820, val loss: 1.2797373533248901
Epoch 830, training loss: 0.07064121216535568 = 0.005118898116052151 + 0.01 * 6.552231788635254
Epoch 830, val loss: 1.2851868867874146
Epoch 840, training loss: 0.07053790986537933 = 0.004966768901795149 + 0.01 * 6.557114601135254
Epoch 840, val loss: 1.2905776500701904
Epoch 850, training loss: 0.07025714963674545 = 0.004821956157684326 + 0.01 * 6.543519496917725
Epoch 850, val loss: 1.295782446861267
Epoch 860, training loss: 0.07016534358263016 = 0.004683994222432375 + 0.01 * 6.548134803771973
Epoch 860, val loss: 1.3010177612304688
Epoch 870, training loss: 0.07004143297672272 = 0.004552807193249464 + 0.01 * 6.548862934112549
Epoch 870, val loss: 1.306017518043518
Epoch 880, training loss: 0.06980125606060028 = 0.004427459556609392 + 0.01 * 6.537379741668701
Epoch 880, val loss: 1.3110144138336182
Epoch 890, training loss: 0.06966709345579147 = 0.004307662136852741 + 0.01 * 6.535943031311035
Epoch 890, val loss: 1.3158689737319946
Epoch 900, training loss: 0.06960057467222214 = 0.004193966276943684 + 0.01 * 6.540660381317139
Epoch 900, val loss: 1.3206278085708618
Epoch 910, training loss: 0.06927572190761566 = 0.004084818065166473 + 0.01 * 6.519090175628662
Epoch 910, val loss: 1.3253045082092285
Epoch 920, training loss: 0.06920666992664337 = 0.003980595618486404 + 0.01 * 6.522607326507568
Epoch 920, val loss: 1.3298670053482056
Epoch 930, training loss: 0.06913550198078156 = 0.003880599047988653 + 0.01 * 6.525490760803223
Epoch 930, val loss: 1.334412693977356
Epoch 940, training loss: 0.069231778383255 = 0.0037850788794457912 + 0.01 * 6.544670104980469
Epoch 940, val loss: 1.3388129472732544
Epoch 950, training loss: 0.06880980730056763 = 0.0036935792304575443 + 0.01 * 6.511622905731201
Epoch 950, val loss: 1.3431109189987183
Epoch 960, training loss: 0.06868370622396469 = 0.00360535248182714 + 0.01 * 6.507835865020752
Epoch 960, val loss: 1.3474127054214478
Epoch 970, training loss: 0.06862311065196991 = 0.003520432161167264 + 0.01 * 6.510268211364746
Epoch 970, val loss: 1.3515679836273193
Epoch 980, training loss: 0.06864184141159058 = 0.003438939107581973 + 0.01 * 6.520289897918701
Epoch 980, val loss: 1.3556898832321167
Epoch 990, training loss: 0.0684666559100151 = 0.003360810922458768 + 0.01 * 6.510585308074951
Epoch 990, val loss: 1.3596570491790771
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.8413
Flip ASR: 0.8089/225 nodes
The final ASR:0.58672, 0.22999, Accuracy:0.82222, 0.00800
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11644])
remove edge: torch.Size([2, 9426])
updated graph: torch.Size([2, 10514])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 1.0000
Flip ASR: 1.0000/225 nodes
The final ASR:0.98893, 0.01086, Accuracy:0.83086, 0.00630
Begin epxeriment: cont_weight: 0.01 epoch:2000 der1:0.3 der2:0 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.02561616897583 = 1.9418779611587524 + 0.01 * 8.373828887939453
Epoch 0, val loss: 1.937001347541809
Epoch 10, training loss: 2.0153844356536865 = 1.9316473007202148 + 0.01 * 8.373724937438965
Epoch 10, val loss: 1.9275099039077759
Epoch 20, training loss: 2.0028891563415527 = 1.919155240058899 + 0.01 * 8.373395919799805
Epoch 20, val loss: 1.915399193763733
Epoch 30, training loss: 1.985450267791748 = 1.901726484298706 + 0.01 * 8.372377395629883
Epoch 30, val loss: 1.8981506824493408
Epoch 40, training loss: 1.9600204229354858 = 1.876356840133667 + 0.01 * 8.366354942321777
Epoch 40, val loss: 1.873349905014038
Epoch 50, training loss: 1.9250291585922241 = 1.841801404953003 + 0.01 * 8.322773933410645
Epoch 50, val loss: 1.8413567543029785
Epoch 60, training loss: 1.8851244449615479 = 1.8036543130874634 + 0.01 * 8.147008895874023
Epoch 60, val loss: 1.8100937604904175
Epoch 70, training loss: 1.8478610515594482 = 1.7692115306854248 + 0.01 * 7.8649468421936035
Epoch 70, val loss: 1.7841668128967285
Epoch 80, training loss: 1.800316572189331 = 1.72677743434906 + 0.01 * 7.353914737701416
Epoch 80, val loss: 1.7486827373504639
Epoch 90, training loss: 1.738584280014038 = 1.6687251329421997 + 0.01 * 6.985910415649414
Epoch 90, val loss: 1.7003415822982788
Epoch 100, training loss: 1.6611120700836182 = 1.592230200767517 + 0.01 * 6.888182640075684
Epoch 100, val loss: 1.6383659839630127
Epoch 110, training loss: 1.5726875066757202 = 1.5041571855545044 + 0.01 * 6.8530378341674805
Epoch 110, val loss: 1.5679237842559814
Epoch 120, training loss: 1.4818671941757202 = 1.413575530052185 + 0.01 * 6.829165935516357
Epoch 120, val loss: 1.4961408376693726
Epoch 130, training loss: 1.3910388946533203 = 1.3229316473007202 + 0.01 * 6.810725212097168
Epoch 130, val loss: 1.4260729551315308
Epoch 140, training loss: 1.2979094982147217 = 1.230004072189331 + 0.01 * 6.7905378341674805
Epoch 140, val loss: 1.3555269241333008
Epoch 150, training loss: 1.2022764682769775 = 1.134609580039978 + 0.01 * 6.766684532165527
Epoch 150, val loss: 1.2848049402236938
Epoch 160, training loss: 1.1061125993728638 = 1.0386085510253906 + 0.01 * 6.750399589538574
Epoch 160, val loss: 1.214501976966858
Epoch 170, training loss: 1.0117802619934082 = 0.9444420337677002 + 0.01 * 6.733827590942383
Epoch 170, val loss: 1.1459983587265015
Epoch 180, training loss: 0.9215828776359558 = 0.8543379902839661 + 0.01 * 6.724489212036133
Epoch 180, val loss: 1.0810810327529907
Epoch 190, training loss: 0.8368029594421387 = 0.7695937156677246 + 0.01 * 6.720924377441406
Epoch 190, val loss: 1.020544171333313
Epoch 200, training loss: 0.757752001285553 = 0.6906468868255615 + 0.01 * 6.710511684417725
Epoch 200, val loss: 0.9654369354248047
Epoch 210, training loss: 0.6845502257347107 = 0.6175110936164856 + 0.01 * 6.703913688659668
Epoch 210, val loss: 0.9157761335372925
Epoch 220, training loss: 0.617156982421875 = 0.5502073168754578 + 0.01 * 6.694964408874512
Epoch 220, val loss: 0.8714844584465027
Epoch 230, training loss: 0.555601179599762 = 0.48873811960220337 + 0.01 * 6.686303615570068
Epoch 230, val loss: 0.8328150510787964
Epoch 240, training loss: 0.49970895051956177 = 0.43290144205093384 + 0.01 * 6.680751323699951
Epoch 240, val loss: 0.7998824715614319
Epoch 250, training loss: 0.44893044233322144 = 0.3822106122970581 + 0.01 * 6.671981334686279
Epoch 250, val loss: 0.7725454568862915
Epoch 260, training loss: 0.4027528166770935 = 0.33607396483421326 + 0.01 * 6.667886734008789
Epoch 260, val loss: 0.750563383102417
Epoch 270, training loss: 0.36074280738830566 = 0.2941136062145233 + 0.01 * 6.662921905517578
Epoch 270, val loss: 0.7337304949760437
Epoch 280, training loss: 0.32284238934516907 = 0.25618377327919006 + 0.01 * 6.665862083435059
Epoch 280, val loss: 0.7218446731567383
Epoch 290, training loss: 0.28881242871284485 = 0.22228257358074188 + 0.01 * 6.652984619140625
Epoch 290, val loss: 0.7146709561347961
Epoch 300, training loss: 0.2589985728263855 = 0.1924757957458496 + 0.01 * 6.652276515960693
Epoch 300, val loss: 0.7118998169898987
Epoch 310, training loss: 0.23329097032546997 = 0.16670043766498566 + 0.01 * 6.659052848815918
Epoch 310, val loss: 0.7132832407951355
Epoch 320, training loss: 0.21119333803653717 = 0.1447332501411438 + 0.01 * 6.6460089683532715
Epoch 320, val loss: 0.7183606028556824
Epoch 330, training loss: 0.19261306524276733 = 0.12614232301712036 + 0.01 * 6.647074222564697
Epoch 330, val loss: 0.7266272902488708
Epoch 340, training loss: 0.17687854170799255 = 0.11045857518911362 + 0.01 * 6.641997814178467
Epoch 340, val loss: 0.7373371720314026
Epoch 350, training loss: 0.16357280313968658 = 0.09718912094831467 + 0.01 * 6.638368129730225
Epoch 350, val loss: 0.7498828172683716
Epoch 360, training loss: 0.15222690999507904 = 0.08590339124202728 + 0.01 * 6.632351875305176
Epoch 360, val loss: 0.7637842297554016
Epoch 370, training loss: 0.1425333023071289 = 0.07624512165784836 + 0.01 * 6.628817558288574
Epoch 370, val loss: 0.7786828279495239
Epoch 380, training loss: 0.13425308465957642 = 0.06792701780796051 + 0.01 * 6.632606506347656
Epoch 380, val loss: 0.7943041324615479
Epoch 390, training loss: 0.12696588039398193 = 0.060730233788490295 + 0.01 * 6.623564720153809
Epoch 390, val loss: 0.8104577660560608
Epoch 400, training loss: 0.12065024673938751 = 0.05446658283472061 + 0.01 * 6.618366718292236
Epoch 400, val loss: 0.8269846439361572
Epoch 410, training loss: 0.11511453986167908 = 0.048991408199071884 + 0.01 * 6.612313270568848
Epoch 410, val loss: 0.8436974883079529
Epoch 420, training loss: 0.11029089987277985 = 0.044185835868120193 + 0.01 * 6.610506057739258
Epoch 420, val loss: 0.8605820536613464
Epoch 430, training loss: 0.10600283741950989 = 0.039953988045454025 + 0.01 * 6.604884624481201
Epoch 430, val loss: 0.877528965473175
Epoch 440, training loss: 0.10225103795528412 = 0.03621883690357208 + 0.01 * 6.603219985961914
Epoch 440, val loss: 0.8945149183273315
Epoch 450, training loss: 0.09903711080551147 = 0.03292010352015495 + 0.01 * 6.611700534820557
Epoch 450, val loss: 0.9113574028015137
Epoch 460, training loss: 0.09594245254993439 = 0.03000471368432045 + 0.01 * 6.593774318695068
Epoch 460, val loss: 0.928005039691925
Epoch 470, training loss: 0.09331774711608887 = 0.02742215432226658 + 0.01 * 6.589559555053711
Epoch 470, val loss: 0.9444267749786377
Epoch 480, training loss: 0.09097552299499512 = 0.02513272874057293 + 0.01 * 6.584279537200928
Epoch 480, val loss: 0.960543155670166
Epoch 490, training loss: 0.08905916661024094 = 0.023102179169654846 + 0.01 * 6.595698833465576
Epoch 490, val loss: 0.9762842059135437
Epoch 500, training loss: 0.08711723238229752 = 0.021299853920936584 + 0.01 * 6.581737995147705
Epoch 500, val loss: 0.9916321635246277
Epoch 510, training loss: 0.08548829704523087 = 0.019696056842803955 + 0.01 * 6.579224109649658
Epoch 510, val loss: 1.0064697265625
Epoch 520, training loss: 0.08400189876556396 = 0.018265491351485252 + 0.01 * 6.573640823364258
Epoch 520, val loss: 1.0209319591522217
Epoch 530, training loss: 0.0826917290687561 = 0.016985641792416573 + 0.01 * 6.570609092712402
Epoch 530, val loss: 1.0349210500717163
Epoch 540, training loss: 0.08156616985797882 = 0.01583772711455822 + 0.01 * 6.5728440284729
Epoch 540, val loss: 1.0484555959701538
Epoch 550, training loss: 0.08036517351865768 = 0.014805225655436516 + 0.01 * 6.555994987487793
Epoch 550, val loss: 1.0615813732147217
Epoch 560, training loss: 0.07961581647396088 = 0.013873140327632427 + 0.01 * 6.574268341064453
Epoch 560, val loss: 1.0742542743682861
Epoch 570, training loss: 0.07852582633495331 = 0.0130295529961586 + 0.01 * 6.549627780914307
Epoch 570, val loss: 1.0865235328674316
Epoch 580, training loss: 0.07774144411087036 = 0.012263956479728222 + 0.01 * 6.547749042510986
Epoch 580, val loss: 1.0983684062957764
Epoch 590, training loss: 0.07708162814378738 = 0.011566169559955597 + 0.01 * 6.551546096801758
Epoch 590, val loss: 1.1099218130111694
Epoch 600, training loss: 0.07637672126293182 = 0.010928862728178501 + 0.01 * 6.54478645324707
Epoch 600, val loss: 1.1210814714431763
Epoch 610, training loss: 0.07578716427087784 = 0.010345570743083954 + 0.01 * 6.544159412384033
Epoch 610, val loss: 1.1319787502288818
Epoch 620, training loss: 0.07516412436962128 = 0.009811189025640488 + 0.01 * 6.535294055938721
Epoch 620, val loss: 1.1425050497055054
Epoch 630, training loss: 0.07467161118984222 = 0.009319379925727844 + 0.01 * 6.535223007202148
Epoch 630, val loss: 1.1527879238128662
Epoch 640, training loss: 0.07420360296964645 = 0.008866524323821068 + 0.01 * 6.533708095550537
Epoch 640, val loss: 1.1627235412597656
Epoch 650, training loss: 0.07385873794555664 = 0.008447824977338314 + 0.01 * 6.541091442108154
Epoch 650, val loss: 1.1724631786346436
Epoch 660, training loss: 0.0734117329120636 = 0.008059968240559101 + 0.01 * 6.5351762771606445
Epoch 660, val loss: 1.1819101572036743
Epoch 670, training loss: 0.07294555008411407 = 0.007701442576944828 + 0.01 * 6.524410724639893
Epoch 670, val loss: 1.1911498308181763
Epoch 680, training loss: 0.07258222252130508 = 0.007367441430687904 + 0.01 * 6.521478176116943
Epoch 680, val loss: 1.200127124786377
Epoch 690, training loss: 0.0722728744149208 = 0.007055624853819609 + 0.01 * 6.521725177764893
Epoch 690, val loss: 1.2088876962661743
Epoch 700, training loss: 0.07192201167345047 = 0.006766073405742645 + 0.01 * 6.515594005584717
Epoch 700, val loss: 1.2174409627914429
Epoch 710, training loss: 0.07162126898765564 = 0.00649575050920248 + 0.01 * 6.512552261352539
Epoch 710, val loss: 1.2257672548294067
Epoch 720, training loss: 0.07137135416269302 = 0.006243571173399687 + 0.01 * 6.512778282165527
Epoch 720, val loss: 1.233854055404663
Epoch 730, training loss: 0.07114194333553314 = 0.006006364710628986 + 0.01 * 6.5135579109191895
Epoch 730, val loss: 1.2418725490570068
Epoch 740, training loss: 0.07083074003458023 = 0.005784156266599894 + 0.01 * 6.5046586990356445
Epoch 740, val loss: 1.2495505809783936
Epoch 750, training loss: 0.07060980051755905 = 0.00557507760822773 + 0.01 * 6.503472805023193
Epoch 750, val loss: 1.2572084665298462
Epoch 760, training loss: 0.07040417194366455 = 0.005378004629164934 + 0.01 * 6.502616882324219
Epoch 760, val loss: 1.2646242380142212
Epoch 770, training loss: 0.07020635902881622 = 0.005190998315811157 + 0.01 * 6.5015363693237305
Epoch 770, val loss: 1.2720662355422974
Epoch 780, training loss: 0.07001485675573349 = 0.005016727838665247 + 0.01 * 6.499813079833984
Epoch 780, val loss: 1.2791415452957153
Epoch 790, training loss: 0.06995533406734467 = 0.004852648824453354 + 0.01 * 6.510268211364746
Epoch 790, val loss: 1.2860952615737915
Epoch 800, training loss: 0.06958883255720139 = 0.004698038101196289 + 0.01 * 6.48907995223999
Epoch 800, val loss: 1.292859435081482
Epoch 810, training loss: 0.06947444379329681 = 0.004551456309854984 + 0.01 * 6.4922990798950195
Epoch 810, val loss: 1.2995320558547974
Epoch 820, training loss: 0.06928864866495132 = 0.004412980284541845 + 0.01 * 6.487566947937012
Epoch 820, val loss: 1.3060379028320312
Epoch 830, training loss: 0.0692395344376564 = 0.004282126668840647 + 0.01 * 6.495741367340088
Epoch 830, val loss: 1.3124784231185913
Epoch 840, training loss: 0.06899562478065491 = 0.0041575045324862 + 0.01 * 6.483811855316162
Epoch 840, val loss: 1.3186818361282349
Epoch 850, training loss: 0.06892163306474686 = 0.004040089435875416 + 0.01 * 6.48815393447876
Epoch 850, val loss: 1.3247387409210205
Epoch 860, training loss: 0.06878048181533813 = 0.003927682526409626 + 0.01 * 6.485279560089111
Epoch 860, val loss: 1.3308253288269043
Epoch 870, training loss: 0.06855057179927826 = 0.0038212169893085957 + 0.01 * 6.472935676574707
Epoch 870, val loss: 1.336574673652649
Epoch 880, training loss: 0.06861812621355057 = 0.003719450905919075 + 0.01 * 6.489867687225342
Epoch 880, val loss: 1.3424912691116333
Epoch 890, training loss: 0.06834331899881363 = 0.003622329793870449 + 0.01 * 6.4720988273620605
Epoch 890, val loss: 1.3480629920959473
Epoch 900, training loss: 0.06813550740480423 = 0.00352987227961421 + 0.01 * 6.460564136505127
Epoch 900, val loss: 1.3536368608474731
Epoch 910, training loss: 0.06800724565982819 = 0.0034423803444951773 + 0.01 * 6.456486701965332
Epoch 910, val loss: 1.3589446544647217
Epoch 920, training loss: 0.06787829101085663 = 0.0033587892539799213 + 0.01 * 6.451950550079346
Epoch 920, val loss: 1.364280343055725
Epoch 930, training loss: 0.06784170120954514 = 0.00327896187081933 + 0.01 * 6.456274032592773
Epoch 930, val loss: 1.3694449663162231
Epoch 940, training loss: 0.06761433929204941 = 0.003202345920726657 + 0.01 * 6.44119930267334
Epoch 940, val loss: 1.3746304512023926
Epoch 950, training loss: 0.0676758736371994 = 0.0031293961219489574 + 0.01 * 6.454648017883301
Epoch 950, val loss: 1.3794740438461304
Epoch 960, training loss: 0.06766138225793839 = 0.0030595557764172554 + 0.01 * 6.460182189941406
Epoch 960, val loss: 1.3845096826553345
Epoch 970, training loss: 0.06775246560573578 = 0.002992744790390134 + 0.01 * 6.475972652435303
Epoch 970, val loss: 1.389165997505188
Epoch 980, training loss: 0.06741061806678772 = 0.0029285962227731943 + 0.01 * 6.448202133178711
Epoch 980, val loss: 1.3939393758773804
Epoch 990, training loss: 0.06721122562885284 = 0.0028672015760093927 + 0.01 * 6.434402942657471
Epoch 990, val loss: 1.3986166715621948
Epoch 1000, training loss: 0.06709472835063934 = 0.0028080169577151537 + 0.01 * 6.428670883178711
Epoch 1000, val loss: 1.4030499458312988
Epoch 1010, training loss: 0.067148856818676 = 0.0027514074463397264 + 0.01 * 6.43974494934082
Epoch 1010, val loss: 1.4075605869293213
Epoch 1020, training loss: 0.06697900593280792 = 0.002696784446015954 + 0.01 * 6.42822265625
Epoch 1020, val loss: 1.4118314981460571
Epoch 1030, training loss: 0.06746552139520645 = 0.002644485328346491 + 0.01 * 6.4821038246154785
Epoch 1030, val loss: 1.4161735773086548
Epoch 1040, training loss: 0.06677763164043427 = 0.002594155725091696 + 0.01 * 6.418347358703613
Epoch 1040, val loss: 1.4203221797943115
Epoch 1050, training loss: 0.06678835302591324 = 0.002545753261074424 + 0.01 * 6.42426061630249
Epoch 1050, val loss: 1.4243550300598145
Epoch 1060, training loss: 0.0667356476187706 = 0.002499373396858573 + 0.01 * 6.423627853393555
Epoch 1060, val loss: 1.4284417629241943
Epoch 1070, training loss: 0.06659522652626038 = 0.0024543667677789927 + 0.01 * 6.414085865020752
Epoch 1070, val loss: 1.4323877096176147
Epoch 1080, training loss: 0.06642977148294449 = 0.0024112232495099306 + 0.01 * 6.401854991912842
Epoch 1080, val loss: 1.4362542629241943
Epoch 1090, training loss: 0.06645453721284866 = 0.002369621302932501 + 0.01 * 6.408492088317871
Epoch 1090, val loss: 1.4401278495788574
Epoch 1100, training loss: 0.06652166694402695 = 0.0023296477738767862 + 0.01 * 6.4192023277282715
Epoch 1100, val loss: 1.4438436031341553
Epoch 1110, training loss: 0.06622602790594101 = 0.0022908966057002544 + 0.01 * 6.393513202667236
Epoch 1110, val loss: 1.447546124458313
Epoch 1120, training loss: 0.06612444669008255 = 0.0022534653544425964 + 0.01 * 6.38709831237793
Epoch 1120, val loss: 1.451221227645874
Epoch 1130, training loss: 0.06636826694011688 = 0.0022173409815877676 + 0.01 * 6.415093421936035
Epoch 1130, val loss: 1.4548481702804565
Epoch 1140, training loss: 0.06618061661720276 = 0.0021824361756443977 + 0.01 * 6.399817943572998
Epoch 1140, val loss: 1.4583063125610352
Epoch 1150, training loss: 0.06608507037162781 = 0.0021486764308065176 + 0.01 * 6.39363956451416
Epoch 1150, val loss: 1.4618061780929565
Epoch 1160, training loss: 0.06618913263082504 = 0.0021161248441785574 + 0.01 * 6.40730094909668
Epoch 1160, val loss: 1.4652124643325806
Epoch 1170, training loss: 0.06591948121786118 = 0.0020844966638833284 + 0.01 * 6.3834991455078125
Epoch 1170, val loss: 1.4685465097427368
Epoch 1180, training loss: 0.06589290499687195 = 0.002053985372185707 + 0.01 * 6.383892059326172
Epoch 1180, val loss: 1.4718475341796875
Epoch 1190, training loss: 0.06595896184444427 = 0.002024424262344837 + 0.01 * 6.393453598022461
Epoch 1190, val loss: 1.4750317335128784
Epoch 1200, training loss: 0.06557163596153259 = 0.001995773520320654 + 0.01 * 6.35758638381958
Epoch 1200, val loss: 1.4782099723815918
Epoch 1210, training loss: 0.06598984450101852 = 0.0019681397825479507 + 0.01 * 6.402170658111572
Epoch 1210, val loss: 1.4813451766967773
Epoch 1220, training loss: 0.06552471965551376 = 0.0019413130357861519 + 0.01 * 6.358340263366699
Epoch 1220, val loss: 1.4843664169311523
Epoch 1230, training loss: 0.06553292274475098 = 0.0019152815220877528 + 0.01 * 6.361763954162598
Epoch 1230, val loss: 1.4873933792114258
Epoch 1240, training loss: 0.0655624270439148 = 0.0018900121795013547 + 0.01 * 6.367241382598877
Epoch 1240, val loss: 1.4903885126113892
Epoch 1250, training loss: 0.06539640575647354 = 0.0018654350424185395 + 0.01 * 6.353097438812256
Epoch 1250, val loss: 1.4933196306228638
Epoch 1260, training loss: 0.06533018499612808 = 0.0018414687365293503 + 0.01 * 6.34887170791626
Epoch 1260, val loss: 1.4961642026901245
Epoch 1270, training loss: 0.06527909636497498 = 0.001818375545553863 + 0.01 * 6.346072673797607
Epoch 1270, val loss: 1.4990946054458618
Epoch 1280, training loss: 0.06516792625188828 = 0.00179578666575253 + 0.01 * 6.337214469909668
Epoch 1280, val loss: 1.5018078088760376
Epoch 1290, training loss: 0.06514851003885269 = 0.0017739218892529607 + 0.01 * 6.337459087371826
Epoch 1290, val loss: 1.5045900344848633
Epoch 1300, training loss: 0.06527265161275864 = 0.0017525650328025222 + 0.01 * 6.352008819580078
Epoch 1300, val loss: 1.5072877407073975
Epoch 1310, training loss: 0.06512720137834549 = 0.001731949858367443 + 0.01 * 6.33952522277832
Epoch 1310, val loss: 1.5100219249725342
Epoch 1320, training loss: 0.06521838158369064 = 0.0017118345713242888 + 0.01 * 6.350654602050781
Epoch 1320, val loss: 1.512602686882019
Epoch 1330, training loss: 0.06486286967992783 = 0.0016922990325838327 + 0.01 * 6.317057132720947
Epoch 1330, val loss: 1.5152485370635986
Epoch 1340, training loss: 0.06500200182199478 = 0.0016732148360460997 + 0.01 * 6.332879066467285
Epoch 1340, val loss: 1.5177557468414307
Epoch 1350, training loss: 0.06489832699298859 = 0.0016546520637348294 + 0.01 * 6.324367523193359
Epoch 1350, val loss: 1.5202851295471191
Epoch 1360, training loss: 0.06485085189342499 = 0.0016365211922675371 + 0.01 * 6.321433067321777
Epoch 1360, val loss: 1.522761344909668
Epoch 1370, training loss: 0.06488320231437683 = 0.0016190225724130869 + 0.01 * 6.326417922973633
Epoch 1370, val loss: 1.5252208709716797
Epoch 1380, training loss: 0.06477168202400208 = 0.001601900439709425 + 0.01 * 6.316978454589844
Epoch 1380, val loss: 1.527603030204773
Epoch 1390, training loss: 0.06466267257928848 = 0.001585181336849928 + 0.01 * 6.307749271392822
Epoch 1390, val loss: 1.5299001932144165
Epoch 1400, training loss: 0.0650249645113945 = 0.0015688735293224454 + 0.01 * 6.345609188079834
Epoch 1400, val loss: 1.5321711301803589
Epoch 1410, training loss: 0.06452197581529617 = 0.001552957110106945 + 0.01 * 6.296902656555176
Epoch 1410, val loss: 1.5344761610031128
Epoch 1420, training loss: 0.06498593091964722 = 0.0015374909853562713 + 0.01 * 6.344843864440918
Epoch 1420, val loss: 1.5367766618728638
Epoch 1430, training loss: 0.06459309160709381 = 0.0015222768997773528 + 0.01 * 6.30708122253418
Epoch 1430, val loss: 1.538872480392456
Epoch 1440, training loss: 0.06494688987731934 = 0.001507575623691082 + 0.01 * 6.343932151794434
Epoch 1440, val loss: 1.541127324104309
Epoch 1450, training loss: 0.06443300098180771 = 0.001493097166530788 + 0.01 * 6.293990612030029
Epoch 1450, val loss: 1.5432708263397217
Epoch 1460, training loss: 0.06442708522081375 = 0.0014790111454203725 + 0.01 * 6.294807434082031
Epoch 1460, val loss: 1.5453938245773315
Epoch 1470, training loss: 0.06441657990217209 = 0.0014653272228315473 + 0.01 * 6.295125484466553
Epoch 1470, val loss: 1.5475852489471436
Epoch 1480, training loss: 0.06427886337041855 = 0.0014518630923703313 + 0.01 * 6.282700061798096
Epoch 1480, val loss: 1.5496183633804321
Epoch 1490, training loss: 0.06446311622858047 = 0.0014387135161086917 + 0.01 * 6.302440166473389
Epoch 1490, val loss: 1.5517022609710693
Epoch 1500, training loss: 0.06450007855892181 = 0.001425936701707542 + 0.01 * 6.307414531707764
Epoch 1500, val loss: 1.5537556409835815
Epoch 1510, training loss: 0.06426896154880524 = 0.001413341611623764 + 0.01 * 6.285561561584473
Epoch 1510, val loss: 1.5557701587677002
Epoch 1520, training loss: 0.06439092755317688 = 0.0014011644525453448 + 0.01 * 6.298976898193359
Epoch 1520, val loss: 1.5577727556228638
Epoch 1530, training loss: 0.06414173543453217 = 0.0013890849659219384 + 0.01 * 6.275265216827393
Epoch 1530, val loss: 1.5597039461135864
Epoch 1540, training loss: 0.06416025757789612 = 0.0013773745158687234 + 0.01 * 6.2782883644104
Epoch 1540, val loss: 1.5616692304611206
Epoch 1550, training loss: 0.06413697451353073 = 0.0013658767566084862 + 0.01 * 6.277109622955322
Epoch 1550, val loss: 1.563572645187378
Epoch 1560, training loss: 0.06442579627037048 = 0.001354665495455265 + 0.01 * 6.3071136474609375
Epoch 1560, val loss: 1.5654836893081665
Epoch 1570, training loss: 0.06407998502254486 = 0.001343696378171444 + 0.01 * 6.2736287117004395
Epoch 1570, val loss: 1.5672833919525146
Epoch 1580, training loss: 0.06414082646369934 = 0.0013329145731404424 + 0.01 * 6.280791282653809
Epoch 1580, val loss: 1.5691235065460205
Epoch 1590, training loss: 0.06398455053567886 = 0.0013223678106442094 + 0.01 * 6.266218662261963
Epoch 1590, val loss: 1.5709643363952637
Epoch 1600, training loss: 0.06425012648105621 = 0.001312081702053547 + 0.01 * 6.29380464553833
Epoch 1600, val loss: 1.572708249092102
Epoch 1610, training loss: 0.06410163640975952 = 0.0013020162004977465 + 0.01 * 6.279961585998535
Epoch 1610, val loss: 1.5745145082473755
Epoch 1620, training loss: 0.06397321820259094 = 0.0012920645531266928 + 0.01 * 6.268115520477295
Epoch 1620, val loss: 1.576318383216858
Epoch 1630, training loss: 0.06436342000961304 = 0.0012824864825233817 + 0.01 * 6.308093547821045
Epoch 1630, val loss: 1.5780407190322876
Epoch 1640, training loss: 0.0640103816986084 = 0.0012729100417345762 + 0.01 * 6.273747444152832
Epoch 1640, val loss: 1.5797951221466064
Epoch 1650, training loss: 0.06425446271896362 = 0.0012636431492865086 + 0.01 * 6.299082279205322
Epoch 1650, val loss: 1.5814895629882812
Epoch 1660, training loss: 0.06395576149225235 = 0.0012545159552246332 + 0.01 * 6.270124912261963
Epoch 1660, val loss: 1.583207368850708
Epoch 1670, training loss: 0.06377165764570236 = 0.0012455584947019815 + 0.01 * 6.252610206604004
Epoch 1670, val loss: 1.5848960876464844
Epoch 1680, training loss: 0.06390703469514847 = 0.0012367842718958855 + 0.01 * 6.267025470733643
Epoch 1680, val loss: 1.5865510702133179
Epoch 1690, training loss: 0.06389366090297699 = 0.0012281718663871288 + 0.01 * 6.266549110412598
Epoch 1690, val loss: 1.5882741212844849
Epoch 1700, training loss: 0.06382885575294495 = 0.001219753292389214 + 0.01 * 6.2609100341796875
Epoch 1700, val loss: 1.58991539478302
Epoch 1710, training loss: 0.06386666744947433 = 0.0012115127174183726 + 0.01 * 6.2655158042907715
Epoch 1710, val loss: 1.5915085077285767
Epoch 1720, training loss: 0.06373905390501022 = 0.0012033580569550395 + 0.01 * 6.253570079803467
Epoch 1720, val loss: 1.593083143234253
Epoch 1730, training loss: 0.06391968578100204 = 0.0011954421643167734 + 0.01 * 6.272424221038818
Epoch 1730, val loss: 1.5946036577224731
Epoch 1740, training loss: 0.06362547725439072 = 0.0011875976342707872 + 0.01 * 6.243788242340088
Epoch 1740, val loss: 1.5962064266204834
Epoch 1750, training loss: 0.06378178298473358 = 0.0011798993218690157 + 0.01 * 6.260188579559326
Epoch 1750, val loss: 1.5977483987808228
Epoch 1760, training loss: 0.06398719549179077 = 0.001172411721199751 + 0.01 * 6.2814788818359375
Epoch 1760, val loss: 1.5992655754089355
Epoch 1770, training loss: 0.06360453367233276 = 0.0011649158550426364 + 0.01 * 6.243961811065674
Epoch 1770, val loss: 1.600750207901001
Epoch 1780, training loss: 0.06365003436803818 = 0.001157668768428266 + 0.01 * 6.249236583709717
Epoch 1780, val loss: 1.602236270904541
Epoch 1790, training loss: 0.06366721540689468 = 0.0011504770955070853 + 0.01 * 6.251674652099609
Epoch 1790, val loss: 1.603678584098816
Epoch 1800, training loss: 0.06345928460359573 = 0.00114341895096004 + 0.01 * 6.231586456298828
Epoch 1800, val loss: 1.6052525043487549
Epoch 1810, training loss: 0.06352155655622482 = 0.0011364368256181479 + 0.01 * 6.2385125160217285
Epoch 1810, val loss: 1.6066757440567017
Epoch 1820, training loss: 0.06375967711210251 = 0.0011297003366053104 + 0.01 * 6.262997627258301
Epoch 1820, val loss: 1.6081113815307617
Epoch 1830, training loss: 0.06342650949954987 = 0.0011229655938223004 + 0.01 * 6.230354309082031
Epoch 1830, val loss: 1.609549880027771
Epoch 1840, training loss: 0.06349888443946838 = 0.0011163965100422502 + 0.01 * 6.238248825073242
Epoch 1840, val loss: 1.6109246015548706
Epoch 1850, training loss: 0.06366685032844543 = 0.0011099372059106827 + 0.01 * 6.2556915283203125
Epoch 1850, val loss: 1.6122798919677734
Epoch 1860, training loss: 0.06346845626831055 = 0.0011035380885004997 + 0.01 * 6.236492156982422
Epoch 1860, val loss: 1.613743543624878
Epoch 1870, training loss: 0.06330844759941101 = 0.001097228261642158 + 0.01 * 6.2211222648620605
Epoch 1870, val loss: 1.6151463985443115
Epoch 1880, training loss: 0.06342919170856476 = 0.0010910611599683762 + 0.01 * 6.233813285827637
Epoch 1880, val loss: 1.6165175437927246
Epoch 1890, training loss: 0.06343138962984085 = 0.0010850176913663745 + 0.01 * 6.234637260437012
Epoch 1890, val loss: 1.6178607940673828
Epoch 1900, training loss: 0.0633576363325119 = 0.0010789845837280154 + 0.01 * 6.227865219116211
Epoch 1900, val loss: 1.6192716360092163
Epoch 1910, training loss: 0.06347434967756271 = 0.0010730910580605268 + 0.01 * 6.240126132965088
Epoch 1910, val loss: 1.6206624507904053
Epoch 1920, training loss: 0.06323014944791794 = 0.0010672548087313771 + 0.01 * 6.216289520263672
Epoch 1920, val loss: 1.6219748258590698
Epoch 1930, training loss: 0.0633457750082016 = 0.0010615363717079163 + 0.01 * 6.228424549102783
Epoch 1930, val loss: 1.623321533203125
Epoch 1940, training loss: 0.06343472748994827 = 0.0010559028014540672 + 0.01 * 6.237882614135742
Epoch 1940, val loss: 1.6246699094772339
Epoch 1950, training loss: 0.06325667351484299 = 0.0010503159137442708 + 0.01 * 6.220635414123535
Epoch 1950, val loss: 1.6259632110595703
Epoch 1960, training loss: 0.06342323869466782 = 0.0010448807151988149 + 0.01 * 6.23783540725708
Epoch 1960, val loss: 1.6272765398025513
Epoch 1970, training loss: 0.0634780079126358 = 0.0010394630953669548 + 0.01 * 6.243854522705078
Epoch 1970, val loss: 1.628544807434082
Epoch 1980, training loss: 0.06324119865894318 = 0.0010341245215386152 + 0.01 * 6.220707893371582
Epoch 1980, val loss: 1.6299258470535278
Epoch 1990, training loss: 0.06306787580251694 = 0.001028868486173451 + 0.01 * 6.203901290893555
Epoch 1990, val loss: 1.6311895847320557
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.7306
Flip ASR: 0.6756/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0343968868255615 = 1.9506596326828003 + 0.01 * 8.373732566833496
Epoch 0, val loss: 1.942361831665039
Epoch 10, training loss: 2.023613691329956 = 1.9398783445358276 + 0.01 * 8.373525619506836
Epoch 10, val loss: 1.9317830801010132
Epoch 20, training loss: 2.01070499420166 = 1.9269779920578003 + 0.01 * 8.37269401550293
Epoch 20, val loss: 1.9185811281204224
Epoch 30, training loss: 1.9928957223892212 = 1.9092057943344116 + 0.01 * 8.368993759155273
Epoch 30, val loss: 1.9000253677368164
Epoch 40, training loss: 1.9667338132858276 = 1.8832918405532837 + 0.01 * 8.344200134277344
Epoch 40, val loss: 1.8732855319976807
Epoch 50, training loss: 1.9290186166763306 = 1.8469092845916748 + 0.01 * 8.210931777954102
Epoch 50, val loss: 1.8374555110931396
Epoch 60, training loss: 1.8824522495269775 = 1.8035067319869995 + 0.01 * 7.894550800323486
Epoch 60, val loss: 1.7981338500976562
Epoch 70, training loss: 1.835196614265442 = 1.7608424425125122 + 0.01 * 7.435419082641602
Epoch 70, val loss: 1.7615818977355957
Epoch 80, training loss: 1.7827818393707275 = 1.7106221914291382 + 0.01 * 7.215961933135986
Epoch 80, val loss: 1.7164270877838135
Epoch 90, training loss: 1.7112257480621338 = 1.6406759023666382 + 0.01 * 7.0549821853637695
Epoch 90, val loss: 1.6541447639465332
Epoch 100, training loss: 1.6181195974349976 = 1.5490484237670898 + 0.01 * 6.907121658325195
Epoch 100, val loss: 1.5757631063461304
Epoch 110, training loss: 1.5143386125564575 = 1.4457776546478271 + 0.01 * 6.856094837188721
Epoch 110, val loss: 1.4918034076690674
Epoch 120, training loss: 1.4156394004821777 = 1.3472681045532227 + 0.01 * 6.837128162384033
Epoch 120, val loss: 1.416489601135254
Epoch 130, training loss: 1.3290596008300781 = 1.2607827186584473 + 0.01 * 6.827688694000244
Epoch 130, val loss: 1.355839729309082
Epoch 140, training loss: 1.2531681060791016 = 1.1849870681762695 + 0.01 * 6.818099021911621
Epoch 140, val loss: 1.3060612678527832
Epoch 150, training loss: 1.182606816291809 = 1.1145391464233398 + 0.01 * 6.806763172149658
Epoch 150, val loss: 1.2610201835632324
Epoch 160, training loss: 1.1117753982543945 = 1.043835997581482 + 0.01 * 6.793936729431152
Epoch 160, val loss: 1.215678095817566
Epoch 170, training loss: 1.037583827972412 = 0.969793975353241 + 0.01 * 6.778982639312744
Epoch 170, val loss: 1.1673662662506104
Epoch 180, training loss: 0.9608411192893982 = 0.8932056427001953 + 0.01 * 6.763546943664551
Epoch 180, val loss: 1.1163575649261475
Epoch 190, training loss: 0.8849949240684509 = 0.8174741864204407 + 0.01 * 6.752071857452393
Epoch 190, val loss: 1.065682053565979
Epoch 200, training loss: 0.8132162690162659 = 0.7458136081695557 + 0.01 * 6.740265846252441
Epoch 200, val loss: 1.0185853242874146
Epoch 210, training loss: 0.7467066049575806 = 0.6793872714042664 + 0.01 * 6.731931686401367
Epoch 210, val loss: 0.9762502908706665
Epoch 220, training loss: 0.6847732067108154 = 0.6175364255905151 + 0.01 * 6.723681449890137
Epoch 220, val loss: 0.9377819299697876
Epoch 230, training loss: 0.6265724897384644 = 0.559400200843811 + 0.01 * 6.717230319976807
Epoch 230, val loss: 0.9022641181945801
Epoch 240, training loss: 0.5719000697135925 = 0.5048105716705322 + 0.01 * 6.7089524269104
Epoch 240, val loss: 0.8697537183761597
Epoch 250, training loss: 0.5212500691413879 = 0.45422738790512085 + 0.01 * 6.702268123626709
Epoch 250, val loss: 0.8405712842941284
Epoch 260, training loss: 0.47517162561416626 = 0.40825000405311584 + 0.01 * 6.692162990570068
Epoch 260, val loss: 0.8156101107597351
Epoch 270, training loss: 0.4339488744735718 = 0.3670860528945923 + 0.01 * 6.686284065246582
Epoch 270, val loss: 0.795517086982727
Epoch 280, training loss: 0.3972570598125458 = 0.3304857909679413 + 0.01 * 6.677126884460449
Epoch 280, val loss: 0.7805624008178711
Epoch 290, training loss: 0.36480411887168884 = 0.29785406589508057 + 0.01 * 6.695004463195801
Epoch 290, val loss: 0.7704706192016602
Epoch 300, training loss: 0.335224449634552 = 0.2684680223464966 + 0.01 * 6.675642490386963
Epoch 300, val loss: 0.7646250128746033
Epoch 310, training loss: 0.30833685398101807 = 0.24168220162391663 + 0.01 * 6.665463924407959
Epoch 310, val loss: 0.7626786828041077
Epoch 320, training loss: 0.2836878299713135 = 0.21706318855285645 + 0.01 * 6.662463188171387
Epoch 320, val loss: 0.7641574144363403
Epoch 330, training loss: 0.26119065284729004 = 0.1946004331111908 + 0.01 * 6.659021377563477
Epoch 330, val loss: 0.7686564326286316
Epoch 340, training loss: 0.24098533391952515 = 0.17440688610076904 + 0.01 * 6.657845973968506
Epoch 340, val loss: 0.7761802077293396
Epoch 350, training loss: 0.2228899896144867 = 0.15635158121585846 + 0.01 * 6.653841495513916
Epoch 350, val loss: 0.7862066030502319
Epoch 360, training loss: 0.2068074643611908 = 0.14028841257095337 + 0.01 * 6.651905536651611
Epoch 360, val loss: 0.7983984351158142
Epoch 370, training loss: 0.19261088967323303 = 0.1261284053325653 + 0.01 * 6.648248672485352
Epoch 370, val loss: 0.8122092485427856
Epoch 380, training loss: 0.18019545078277588 = 0.11364294588565826 + 0.01 * 6.65524959564209
Epoch 380, val loss: 0.8271544575691223
Epoch 390, training loss: 0.1690797358751297 = 0.10263626277446747 + 0.01 * 6.644347190856934
Epoch 390, val loss: 0.843012809753418
Epoch 400, training loss: 0.1593014895915985 = 0.09290087223052979 + 0.01 * 6.640061378479004
Epoch 400, val loss: 0.859416127204895
Epoch 410, training loss: 0.15063130855560303 = 0.08422406017780304 + 0.01 * 6.640724182128906
Epoch 410, val loss: 0.8760726451873779
Epoch 420, training loss: 0.14279770851135254 = 0.07643326371908188 + 0.01 * 6.636444568634033
Epoch 420, val loss: 0.8928685784339905
Epoch 430, training loss: 0.13569417595863342 = 0.06939226388931274 + 0.01 * 6.630190372467041
Epoch 430, val loss: 0.9099224209785461
Epoch 440, training loss: 0.1292782425880432 = 0.06299179792404175 + 0.01 * 6.628643989562988
Epoch 440, val loss: 0.9268703460693359
Epoch 450, training loss: 0.123321533203125 = 0.057086046785116196 + 0.01 * 6.623549461364746
Epoch 450, val loss: 0.9437695741653442
Epoch 460, training loss: 0.11776968836784363 = 0.05159493163228035 + 0.01 * 6.617475509643555
Epoch 460, val loss: 0.9605944752693176
Epoch 470, training loss: 0.1125946044921875 = 0.046427611261606216 + 0.01 * 6.616698741912842
Epoch 470, val loss: 0.9773317575454712
Epoch 480, training loss: 0.10771907866001129 = 0.04162595048546791 + 0.01 * 6.609313488006592
Epoch 480, val loss: 0.993942141532898
Epoch 490, training loss: 0.10324271023273468 = 0.03721797093749046 + 0.01 * 6.602474212646484
Epoch 490, val loss: 1.0102380514144897
Epoch 500, training loss: 0.0996508002281189 = 0.03342164680361748 + 0.01 * 6.622915267944336
Epoch 500, val loss: 1.0261285305023193
Epoch 510, training loss: 0.09627167135477066 = 0.030257269740104675 + 0.01 * 6.6014404296875
Epoch 510, val loss: 1.0418145656585693
Epoch 520, training loss: 0.09351500123739243 = 0.027591081336140633 + 0.01 * 6.592392444610596
Epoch 520, val loss: 1.0573058128356934
Epoch 530, training loss: 0.09110912680625916 = 0.025290660560131073 + 0.01 * 6.581846714019775
Epoch 530, val loss: 1.0724403858184814
Epoch 540, training loss: 0.08905474096536636 = 0.023281477391719818 + 0.01 * 6.57732629776001
Epoch 540, val loss: 1.0871957540512085
Epoch 550, training loss: 0.08730025589466095 = 0.02151215821504593 + 0.01 * 6.57880973815918
Epoch 550, val loss: 1.101562738418579
Epoch 560, training loss: 0.0857938900589943 = 0.019950387999415398 + 0.01 * 6.584350109100342
Epoch 560, val loss: 1.1154382228851318
Epoch 570, training loss: 0.08423364907503128 = 0.018563367426395416 + 0.01 * 6.567028045654297
Epoch 570, val loss: 1.1288644075393677
Epoch 580, training loss: 0.08289948105812073 = 0.01732221432030201 + 0.01 * 6.557727336883545
Epoch 580, val loss: 1.1418895721435547
Epoch 590, training loss: 0.08172083646059036 = 0.01620841957628727 + 0.01 * 6.551241874694824
Epoch 590, val loss: 1.1544549465179443
Epoch 600, training loss: 0.08066260069608688 = 0.015203266404569149 + 0.01 * 6.545933246612549
Epoch 600, val loss: 1.1665902137756348
Epoch 610, training loss: 0.07990913838148117 = 0.014293128624558449 + 0.01 * 6.561601638793945
Epoch 610, val loss: 1.1782816648483276
Epoch 620, training loss: 0.07875092327594757 = 0.013467906042933464 + 0.01 * 6.52830171585083
Epoch 620, val loss: 1.189510464668274
Epoch 630, training loss: 0.07810867577791214 = 0.012715375982224941 + 0.01 * 6.539330005645752
Epoch 630, val loss: 1.2003514766693115
Epoch 640, training loss: 0.07741779834032059 = 0.012027614749968052 + 0.01 * 6.539018630981445
Epoch 640, val loss: 1.2108203172683716
Epoch 650, training loss: 0.07674697786569595 = 0.011398122645914555 + 0.01 * 6.534885883331299
Epoch 650, val loss: 1.2209558486938477
Epoch 660, training loss: 0.07600628584623337 = 0.010821053758263588 + 0.01 * 6.518523216247559
Epoch 660, val loss: 1.2307837009429932
Epoch 670, training loss: 0.07534448802471161 = 0.010289470665156841 + 0.01 * 6.505502223968506
Epoch 670, val loss: 1.2403396368026733
Epoch 680, training loss: 0.0751628428697586 = 0.009798284620046616 + 0.01 * 6.5364556312561035
Epoch 680, val loss: 1.249632716178894
Epoch 690, training loss: 0.07426583766937256 = 0.009342985227704048 + 0.01 * 6.49228572845459
Epoch 690, val loss: 1.2586660385131836
Epoch 700, training loss: 0.07412968575954437 = 0.008920430205762386 + 0.01 * 6.520925521850586
Epoch 700, val loss: 1.2675037384033203
Epoch 710, training loss: 0.07344593852758408 = 0.00852897483855486 + 0.01 * 6.491695880889893
Epoch 710, val loss: 1.2760337591171265
Epoch 720, training loss: 0.07292468100786209 = 0.00816468894481659 + 0.01 * 6.475999355316162
Epoch 720, val loss: 1.2843230962753296
Epoch 730, training loss: 0.07258020341396332 = 0.007824256084859371 + 0.01 * 6.475594997406006
Epoch 730, val loss: 1.2924028635025024
Epoch 740, training loss: 0.07217375934123993 = 0.007506640627980232 + 0.01 * 6.46671199798584
Epoch 740, val loss: 1.300254464149475
Epoch 750, training loss: 0.07189099490642548 = 0.007209455594420433 + 0.01 * 6.468153953552246
Epoch 750, val loss: 1.3079031705856323
Epoch 760, training loss: 0.07200491428375244 = 0.006930665113031864 + 0.01 * 6.507424831390381
Epoch 760, val loss: 1.315294623374939
Epoch 770, training loss: 0.07125044614076614 = 0.006669359281659126 + 0.01 * 6.458108901977539
Epoch 770, val loss: 1.3224653005599976
Epoch 780, training loss: 0.07109904289245605 = 0.006424155086278915 + 0.01 * 6.467488765716553
Epoch 780, val loss: 1.3294697999954224
Epoch 790, training loss: 0.07070892304182053 = 0.006194194778800011 + 0.01 * 6.451472759246826
Epoch 790, val loss: 1.3362890481948853
Epoch 800, training loss: 0.07061035931110382 = 0.005977556109428406 + 0.01 * 6.463280200958252
Epoch 800, val loss: 1.3429358005523682
Epoch 810, training loss: 0.07014402747154236 = 0.0057733627036213875 + 0.01 * 6.437066555023193
Epoch 810, val loss: 1.3493826389312744
Epoch 820, training loss: 0.07004480063915253 = 0.005580859258770943 + 0.01 * 6.446394443511963
Epoch 820, val loss: 1.3557102680206299
Epoch 830, training loss: 0.06981892883777618 = 0.005398844368755817 + 0.01 * 6.4420084953308105
Epoch 830, val loss: 1.3618754148483276
Epoch 840, training loss: 0.06949912011623383 = 0.00522620277479291 + 0.01 * 6.4272918701171875
Epoch 840, val loss: 1.3678439855575562
Epoch 850, training loss: 0.06956823170185089 = 0.005061980336904526 + 0.01 * 6.450625419616699
Epoch 850, val loss: 1.3736908435821533
Epoch 860, training loss: 0.06915699690580368 = 0.00490648252889514 + 0.01 * 6.425051689147949
Epoch 860, val loss: 1.37944495677948
Epoch 870, training loss: 0.06904986500740051 = 0.004758377559483051 + 0.01 * 6.4291486740112305
Epoch 870, val loss: 1.385023593902588
Epoch 880, training loss: 0.06867794692516327 = 0.004618045408278704 + 0.01 * 6.405989646911621
Epoch 880, val loss: 1.3904430866241455
Epoch 890, training loss: 0.06890513002872467 = 0.0044843354262411594 + 0.01 * 6.442079544067383
Epoch 890, val loss: 1.395789623260498
Epoch 900, training loss: 0.06859417259693146 = 0.004356509540230036 + 0.01 * 6.423766613006592
Epoch 900, val loss: 1.4009475708007812
Epoch 910, training loss: 0.06850685179233551 = 0.004234866239130497 + 0.01 * 6.427198886871338
Epoch 910, val loss: 1.406060814857483
Epoch 920, training loss: 0.06810649484395981 = 0.004119016230106354 + 0.01 * 6.39874792098999
Epoch 920, val loss: 1.410984754562378
Epoch 930, training loss: 0.06793656200170517 = 0.004008101299405098 + 0.01 * 6.392846584320068
Epoch 930, val loss: 1.4158880710601807
Epoch 940, training loss: 0.06798100471496582 = 0.003902068827301264 + 0.01 * 6.407893657684326
Epoch 940, val loss: 1.4206525087356567
Epoch 950, training loss: 0.06775937974452972 = 0.003800434060394764 + 0.01 * 6.395894527435303
Epoch 950, val loss: 1.4252806901931763
Epoch 960, training loss: 0.06763916462659836 = 0.0037031942047178745 + 0.01 * 6.393597602844238
Epoch 960, val loss: 1.4298794269561768
Epoch 970, training loss: 0.06763872504234314 = 0.00360988755710423 + 0.01 * 6.402884006500244
Epoch 970, val loss: 1.434358835220337
Epoch 980, training loss: 0.06746500730514526 = 0.003520686412230134 + 0.01 * 6.394432067871094
Epoch 980, val loss: 1.4387396574020386
Epoch 990, training loss: 0.06736721843481064 = 0.003434823825955391 + 0.01 * 6.393239498138428
Epoch 990, val loss: 1.4430922269821167
Epoch 1000, training loss: 0.06714153289794922 = 0.003352789208292961 + 0.01 * 6.3788743019104
Epoch 1000, val loss: 1.4473406076431274
Epoch 1010, training loss: 0.06741813570261002 = 0.0032737201545387506 + 0.01 * 6.41444206237793
Epoch 1010, val loss: 1.451511263847351
Epoch 1020, training loss: 0.06685055792331696 = 0.003198198741301894 + 0.01 * 6.365236282348633
Epoch 1020, val loss: 1.4556044340133667
Epoch 1030, training loss: 0.06687470525503159 = 0.0031256522051990032 + 0.01 * 6.374905586242676
Epoch 1030, val loss: 1.4596227407455444
Epoch 1040, training loss: 0.06674180924892426 = 0.0030562609899789095 + 0.01 * 6.368555545806885
Epoch 1040, val loss: 1.4635258913040161
Epoch 1050, training loss: 0.06677734851837158 = 0.0029894020408391953 + 0.01 * 6.3787946701049805
Epoch 1050, val loss: 1.467332363128662
Epoch 1060, training loss: 0.06658080220222473 = 0.002925287000834942 + 0.01 * 6.365551948547363
Epoch 1060, val loss: 1.471159815788269
Epoch 1070, training loss: 0.06666060537099838 = 0.002863423665985465 + 0.01 * 6.37971830368042
Epoch 1070, val loss: 1.4748404026031494
Epoch 1080, training loss: 0.06654081493616104 = 0.002804382471367717 + 0.01 * 6.373642921447754
Epoch 1080, val loss: 1.478468656539917
Epoch 1090, training loss: 0.06645407527685165 = 0.0027470660861581564 + 0.01 * 6.370701313018799
Epoch 1090, val loss: 1.4819848537445068
Epoch 1100, training loss: 0.06614336371421814 = 0.002692361595109105 + 0.01 * 6.345099925994873
Epoch 1100, val loss: 1.4854611158370972
Epoch 1110, training loss: 0.06604990363121033 = 0.0026394077576696873 + 0.01 * 6.341050148010254
Epoch 1110, val loss: 1.4888145923614502
Epoch 1120, training loss: 0.06604039669036865 = 0.0025881966575980186 + 0.01 * 6.34522008895874
Epoch 1120, val loss: 1.4921647310256958
Epoch 1130, training loss: 0.06614556163549423 = 0.0025389399379491806 + 0.01 * 6.360662460327148
Epoch 1130, val loss: 1.495402216911316
Epoch 1140, training loss: 0.0658552274107933 = 0.002491540974006057 + 0.01 * 6.336369037628174
Epoch 1140, val loss: 1.498561978340149
Epoch 1150, training loss: 0.06580276042222977 = 0.0024457487743347883 + 0.01 * 6.335700988769531
Epoch 1150, val loss: 1.5016818046569824
Epoch 1160, training loss: 0.06572870165109634 = 0.0024015430826693773 + 0.01 * 6.33271598815918
Epoch 1160, val loss: 1.5047143697738647
Epoch 1170, training loss: 0.06560621410608292 = 0.0023589993361383677 + 0.01 * 6.324721336364746
Epoch 1170, val loss: 1.5077093839645386
Epoch 1180, training loss: 0.0655580535531044 = 0.002317971782758832 + 0.01 * 6.324008464813232
Epoch 1180, val loss: 1.510606050491333
Epoch 1190, training loss: 0.06539927423000336 = 0.00227828579954803 + 0.01 * 6.312098979949951
Epoch 1190, val loss: 1.513457179069519
Epoch 1200, training loss: 0.06542841345071793 = 0.0022398720029741526 + 0.01 * 6.318853855133057
Epoch 1200, val loss: 1.5162431001663208
Epoch 1210, training loss: 0.06567102670669556 = 0.0022025785874575377 + 0.01 * 6.3468451499938965
Epoch 1210, val loss: 1.5189749002456665
Epoch 1220, training loss: 0.06536176055669785 = 0.0021664430387318134 + 0.01 * 6.3195319175720215
Epoch 1220, val loss: 1.5216845273971558
Epoch 1230, training loss: 0.06549995392560959 = 0.002131453948095441 + 0.01 * 6.336850166320801
Epoch 1230, val loss: 1.524311900138855
Epoch 1240, training loss: 0.06530418246984482 = 0.002098106313496828 + 0.01 * 6.320608139038086
Epoch 1240, val loss: 1.5268805027008057
Epoch 1250, training loss: 0.06512399762868881 = 0.0020657447166740894 + 0.01 * 6.305825233459473
Epoch 1250, val loss: 1.5293481349945068
Epoch 1260, training loss: 0.06524506211280823 = 0.002034323988482356 + 0.01 * 6.32107400894165
Epoch 1260, val loss: 1.5318386554718018
Epoch 1270, training loss: 0.06505193561315536 = 0.002003931673243642 + 0.01 * 6.304800987243652
Epoch 1270, val loss: 1.5342777967453003
Epoch 1280, training loss: 0.06497696042060852 = 0.0019743740558624268 + 0.01 * 6.300259113311768
Epoch 1280, val loss: 1.5366665124893188
Epoch 1290, training loss: 0.06513358652591705 = 0.0019458021270111203 + 0.01 * 6.318778991699219
Epoch 1290, val loss: 1.5390030145645142
Epoch 1300, training loss: 0.0650474950671196 = 0.001918095862492919 + 0.01 * 6.31294059753418
Epoch 1300, val loss: 1.5413330793380737
Epoch 1310, training loss: 0.06476456671953201 = 0.0018911680672317743 + 0.01 * 6.28734016418457
Epoch 1310, val loss: 1.54359769821167
Epoch 1320, training loss: 0.06477569043636322 = 0.0018650662386789918 + 0.01 * 6.291062355041504
Epoch 1320, val loss: 1.545820951461792
Epoch 1330, training loss: 0.06472427397966385 = 0.0018396606901660562 + 0.01 * 6.288461208343506
Epoch 1330, val loss: 1.5479694604873657
Epoch 1340, training loss: 0.06466951221227646 = 0.001815018360503018 + 0.01 * 6.285449504852295
Epoch 1340, val loss: 1.5500956773757935
Epoch 1350, training loss: 0.06488863378763199 = 0.0017911121249198914 + 0.01 * 6.309751987457275
Epoch 1350, val loss: 1.5521645545959473
Epoch 1360, training loss: 0.06469374150037766 = 0.0017678915755823255 + 0.01 * 6.292585372924805
Epoch 1360, val loss: 1.5541969537734985
Epoch 1370, training loss: 0.0645136684179306 = 0.001745274057611823 + 0.01 * 6.276839733123779
Epoch 1370, val loss: 1.5562589168548584
Epoch 1380, training loss: 0.06455626338720322 = 0.001723558991216123 + 0.01 * 6.283270835876465
Epoch 1380, val loss: 1.5581790208816528
Epoch 1390, training loss: 0.06438401341438293 = 0.001702240202575922 + 0.01 * 6.268177509307861
Epoch 1390, val loss: 1.560055136680603
Epoch 1400, training loss: 0.0644076019525528 = 0.0016815513372421265 + 0.01 * 6.2726054191589355
Epoch 1400, val loss: 1.5619364976882935
Epoch 1410, training loss: 0.06442715972661972 = 0.0016615500207990408 + 0.01 * 6.276561260223389
Epoch 1410, val loss: 1.5637452602386475
Epoch 1420, training loss: 0.06442707031965256 = 0.0016419852618128061 + 0.01 * 6.278508186340332
Epoch 1420, val loss: 1.5655269622802734
Epoch 1430, training loss: 0.06444168835878372 = 0.0016229498432949185 + 0.01 * 6.28187370300293
Epoch 1430, val loss: 1.5672342777252197
Epoch 1440, training loss: 0.06444857269525528 = 0.0016043904470279813 + 0.01 * 6.284418106079102
Epoch 1440, val loss: 1.5689606666564941
Epoch 1450, training loss: 0.06435035169124603 = 0.0015863680746406317 + 0.01 * 6.276398181915283
Epoch 1450, val loss: 1.5706191062927246
Epoch 1460, training loss: 0.06413540989160538 = 0.0015687739942222834 + 0.01 * 6.256664276123047
Epoch 1460, val loss: 1.57227623462677
Epoch 1470, training loss: 0.06421516835689545 = 0.00155166897457093 + 0.01 * 6.266350269317627
Epoch 1470, val loss: 1.5738493204116821
Epoch 1480, training loss: 0.06420978903770447 = 0.0015349909663200378 + 0.01 * 6.26747989654541
Epoch 1480, val loss: 1.5753717422485352
Epoch 1490, training loss: 0.06409919261932373 = 0.0015186879318207502 + 0.01 * 6.258050441741943
Epoch 1490, val loss: 1.5769109725952148
Epoch 1500, training loss: 0.0643237754702568 = 0.0015028152847662568 + 0.01 * 6.2820963859558105
Epoch 1500, val loss: 1.5783919095993042
Epoch 1510, training loss: 0.06411837041378021 = 0.001487393630668521 + 0.01 * 6.263097286224365
Epoch 1510, val loss: 1.5798134803771973
Epoch 1520, training loss: 0.06386629492044449 = 0.0014722985215485096 + 0.01 * 6.239399433135986
Epoch 1520, val loss: 1.5812344551086426
Epoch 1530, training loss: 0.06399963796138763 = 0.0014576097019016743 + 0.01 * 6.254202842712402
Epoch 1530, val loss: 1.5826290845870972
Epoch 1540, training loss: 0.0640178769826889 = 0.0014433192554861307 + 0.01 * 6.257456302642822
Epoch 1540, val loss: 1.584012508392334
Epoch 1550, training loss: 0.06391101330518723 = 0.0014292884152382612 + 0.01 * 6.248172760009766
Epoch 1550, val loss: 1.5853276252746582
Epoch 1560, training loss: 0.06407478451728821 = 0.001415749080479145 + 0.01 * 6.265903472900391
Epoch 1560, val loss: 1.5866010189056396
Epoch 1570, training loss: 0.06397327035665512 = 0.0014025168493390083 + 0.01 * 6.257075309753418
Epoch 1570, val loss: 1.5878207683563232
Epoch 1580, training loss: 0.06380178779363632 = 0.00138939474709332 + 0.01 * 6.241239547729492
Epoch 1580, val loss: 1.5891094207763672
Epoch 1590, training loss: 0.06398553401231766 = 0.0013767677592113614 + 0.01 * 6.260876655578613
Epoch 1590, val loss: 1.5903151035308838
Epoch 1600, training loss: 0.06372328847646713 = 0.001364310272037983 + 0.01 * 6.235898017883301
Epoch 1600, val loss: 1.5915004014968872
Epoch 1610, training loss: 0.0638226643204689 = 0.00135215581394732 + 0.01 * 6.247050762176514
Epoch 1610, val loss: 1.5926722288131714
Epoch 1620, training loss: 0.06384943425655365 = 0.0013403267366811633 + 0.01 * 6.25091028213501
Epoch 1620, val loss: 1.5937919616699219
Epoch 1630, training loss: 0.06373860687017441 = 0.0013287437614053488 + 0.01 * 6.240986347198486
Epoch 1630, val loss: 1.594909429550171
Epoch 1640, training loss: 0.06376241892576218 = 0.0013174405321478844 + 0.01 * 6.244497776031494
Epoch 1640, val loss: 1.5959235429763794
Epoch 1650, training loss: 0.06368700414896011 = 0.0013063258957117796 + 0.01 * 6.238068103790283
Epoch 1650, val loss: 1.5970302820205688
Epoch 1660, training loss: 0.06390713900327682 = 0.0012955134734511375 + 0.01 * 6.261162281036377
Epoch 1660, val loss: 1.5980806350708008
Epoch 1670, training loss: 0.06362989544868469 = 0.0012848820770159364 + 0.01 * 6.234501838684082
Epoch 1670, val loss: 1.59907865524292
Epoch 1680, training loss: 0.06379923224449158 = 0.001274519832804799 + 0.01 * 6.252471446990967
Epoch 1680, val loss: 1.6001416444778442
Epoch 1690, training loss: 0.06379318982362747 = 0.0012643053196370602 + 0.01 * 6.2528886795043945
Epoch 1690, val loss: 1.6010651588439941
Epoch 1700, training loss: 0.06353604048490524 = 0.0012543729972094297 + 0.01 * 6.2281670570373535
Epoch 1700, val loss: 1.6020482778549194
Epoch 1710, training loss: 0.06364349275827408 = 0.0012446152977645397 + 0.01 * 6.2398881912231445
Epoch 1710, val loss: 1.6029757261276245
Epoch 1720, training loss: 0.06341136246919632 = 0.0012350286124274135 + 0.01 * 6.217633247375488
Epoch 1720, val loss: 1.6038166284561157
Epoch 1730, training loss: 0.06354065984487534 = 0.0012256854679435492 + 0.01 * 6.231497287750244
Epoch 1730, val loss: 1.6046727895736694
Epoch 1740, training loss: 0.06330287456512451 = 0.0012164529180154204 + 0.01 * 6.208642482757568
Epoch 1740, val loss: 1.6054824590682983
Epoch 1750, training loss: 0.06385537981987 = 0.0012074921978637576 + 0.01 * 6.26478910446167
Epoch 1750, val loss: 1.6063177585601807
Epoch 1760, training loss: 0.06345458328723907 = 0.0011985357850790024 + 0.01 * 6.22560453414917
Epoch 1760, val loss: 1.607131004333496
Epoch 1770, training loss: 0.06339584290981293 = 0.0011898753000423312 + 0.01 * 6.220596790313721
Epoch 1770, val loss: 1.6079058647155762
Epoch 1780, training loss: 0.06332193315029144 = 0.0011814809404313564 + 0.01 * 6.21404504776001
Epoch 1780, val loss: 1.6086114645004272
Epoch 1790, training loss: 0.06329591572284698 = 0.0011731230188161135 + 0.01 * 6.212279319763184
Epoch 1790, val loss: 1.6093525886535645
Epoch 1800, training loss: 0.06378594785928726 = 0.0011650349479168653 + 0.01 * 6.262091636657715
Epoch 1800, val loss: 1.6100506782531738
Epoch 1810, training loss: 0.06326591968536377 = 0.001157001475803554 + 0.01 * 6.210892200469971
Epoch 1810, val loss: 1.6107194423675537
Epoch 1820, training loss: 0.06331931054592133 = 0.0011491936165839434 + 0.01 * 6.217011451721191
Epoch 1820, val loss: 1.6113593578338623
Epoch 1830, training loss: 0.0633167028427124 = 0.0011415554909035563 + 0.01 * 6.217514991760254
Epoch 1830, val loss: 1.6119853258132935
Epoch 1840, training loss: 0.06313617527484894 = 0.0011339496122673154 + 0.01 * 6.200222969055176
Epoch 1840, val loss: 1.612654685974121
Epoch 1850, training loss: 0.06322511285543442 = 0.0011265580542385578 + 0.01 * 6.209855556488037
Epoch 1850, val loss: 1.613265872001648
Epoch 1860, training loss: 0.06313949823379517 = 0.0011192223755642772 + 0.01 * 6.202027797698975
Epoch 1860, val loss: 1.6138856410980225
Epoch 1870, training loss: 0.0632985532283783 = 0.0011120398994535208 + 0.01 * 6.21865177154541
Epoch 1870, val loss: 1.6144757270812988
Epoch 1880, training loss: 0.06321024149656296 = 0.0011050207540392876 + 0.01 * 6.210522174835205
Epoch 1880, val loss: 1.6150490045547485
Epoch 1890, training loss: 0.06315632909536362 = 0.0010979905491694808 + 0.01 * 6.20583438873291
Epoch 1890, val loss: 1.615665078163147
Epoch 1900, training loss: 0.06325466185808182 = 0.0010911881690844893 + 0.01 * 6.216347694396973
Epoch 1900, val loss: 1.6162341833114624
Epoch 1910, training loss: 0.06293623894453049 = 0.0010844641365110874 + 0.01 * 6.185177326202393
Epoch 1910, val loss: 1.6167746782302856
Epoch 1920, training loss: 0.06315071135759354 = 0.0010778827127069235 + 0.01 * 6.2072834968566895
Epoch 1920, val loss: 1.617355465888977
Epoch 1930, training loss: 0.06312461942434311 = 0.0010713686933740973 + 0.01 * 6.205325126647949
Epoch 1930, val loss: 1.617874264717102
Epoch 1940, training loss: 0.06305863708257675 = 0.0010649904143065214 + 0.01 * 6.19936466217041
Epoch 1940, val loss: 1.6184422969818115
Epoch 1950, training loss: 0.06334804743528366 = 0.0010587357683107257 + 0.01 * 6.228931427001953
Epoch 1950, val loss: 1.6189093589782715
Epoch 1960, training loss: 0.06307150423526764 = 0.0010525358375161886 + 0.01 * 6.201897144317627
Epoch 1960, val loss: 1.6194329261779785
Epoch 1970, training loss: 0.06330787390470505 = 0.001046507153660059 + 0.01 * 6.226137161254883
Epoch 1970, val loss: 1.6198935508728027
Epoch 1980, training loss: 0.06298772245645523 = 0.0010404738131910563 + 0.01 * 6.194725036621094
Epoch 1980, val loss: 1.6203876733779907
Epoch 1990, training loss: 0.06291751563549042 = 0.0010346004273742437 + 0.01 * 6.188291549682617
Epoch 1990, val loss: 1.620849609375
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.4244
Flip ASR: 0.3644/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.029071569442749 = 1.9453332424163818 + 0.01 * 8.373841285705566
Epoch 0, val loss: 1.94307541847229
Epoch 10, training loss: 2.019301414489746 = 1.9355640411376953 + 0.01 * 8.373736381530762
Epoch 10, val loss: 1.9331022500991821
Epoch 20, training loss: 2.007499933242798 = 1.923766016960144 + 0.01 * 8.373383522033691
Epoch 20, val loss: 1.9208430051803589
Epoch 30, training loss: 1.9913458824157715 = 1.9076220989227295 + 0.01 * 8.372376441955566
Epoch 30, val loss: 1.9040095806121826
Epoch 40, training loss: 1.9677343368530273 = 1.8840622901916504 + 0.01 * 8.367210388183594
Epoch 40, val loss: 1.8797903060913086
Epoch 50, training loss: 1.9333750009536743 = 1.850081205368042 + 0.01 * 8.329379081726074
Epoch 50, val loss: 1.8461201190948486
Epoch 60, training loss: 1.888183355331421 = 1.8071818351745605 + 0.01 * 8.100156784057617
Epoch 60, val loss: 1.8066434860229492
Epoch 70, training loss: 1.8394447565078735 = 1.7629597187042236 + 0.01 * 7.648507595062256
Epoch 70, val loss: 1.768647313117981
Epoch 80, training loss: 1.7855674028396606 = 1.7127362489700317 + 0.01 * 7.28311824798584
Epoch 80, val loss: 1.723684549331665
Epoch 90, training loss: 1.714656949043274 = 1.6439906358718872 + 0.01 * 7.0666279792785645
Epoch 90, val loss: 1.6630891561508179
Epoch 100, training loss: 1.62437903881073 = 1.5546890497207642 + 0.01 * 6.969003200531006
Epoch 100, val loss: 1.5884065628051758
Epoch 110, training loss: 1.5205981731414795 = 1.4512089490890503 + 0.01 * 6.93892240524292
Epoch 110, val loss: 1.5053468942642212
Epoch 120, training loss: 1.4148956537246704 = 1.3456372022628784 + 0.01 * 6.92584753036499
Epoch 120, val loss: 1.4251147508621216
Epoch 130, training loss: 1.3145705461502075 = 1.2454420328140259 + 0.01 * 6.912846565246582
Epoch 130, val loss: 1.3527165651321411
Epoch 140, training loss: 1.2207962274551392 = 1.1518105268478394 + 0.01 * 6.898568153381348
Epoch 140, val loss: 1.287903904914856
Epoch 150, training loss: 1.1314185857772827 = 1.0626063346862793 + 0.01 * 6.881219863891602
Epoch 150, val loss: 1.2266486883163452
Epoch 160, training loss: 1.0446110963821411 = 0.9760076999664307 + 0.01 * 6.860345363616943
Epoch 160, val loss: 1.166436791419983
Epoch 170, training loss: 0.9598734378814697 = 0.8914848566055298 + 0.01 * 6.838857173919678
Epoch 170, val loss: 1.105932354927063
Epoch 180, training loss: 0.8775385618209839 = 0.8093220591545105 + 0.01 * 6.821650981903076
Epoch 180, val loss: 1.0451899766921997
Epoch 190, training loss: 0.7981616258621216 = 0.7300401329994202 + 0.01 * 6.8121490478515625
Epoch 190, val loss: 0.9846996068954468
Epoch 200, training loss: 0.7225366234779358 = 0.6544532179832458 + 0.01 * 6.808339595794678
Epoch 200, val loss: 0.9260576367378235
Epoch 210, training loss: 0.6516799926757812 = 0.583615779876709 + 0.01 * 6.80642032623291
Epoch 210, val loss: 0.8715617060661316
Epoch 220, training loss: 0.5865187048912048 = 0.5184727311134338 + 0.01 * 6.804598808288574
Epoch 220, val loss: 0.8233324289321899
Epoch 230, training loss: 0.5273767113685608 = 0.45935508608818054 + 0.01 * 6.802162170410156
Epoch 230, val loss: 0.7825368642807007
Epoch 240, training loss: 0.4740970730781555 = 0.4061065912246704 + 0.01 * 6.799046993255615
Epoch 240, val loss: 0.7490611672401428
Epoch 250, training loss: 0.4263332784175873 = 0.3583793640136719 + 0.01 * 6.795392036437988
Epoch 250, val loss: 0.7225955724716187
Epoch 260, training loss: 0.3835955262184143 = 0.315682977437973 + 0.01 * 6.791255950927734
Epoch 260, val loss: 0.7017026543617249
Epoch 270, training loss: 0.3453599214553833 = 0.27748990058898926 + 0.01 * 6.787001132965088
Epoch 270, val loss: 0.6856333613395691
Epoch 280, training loss: 0.31110209226608276 = 0.2432815283536911 + 0.01 * 6.782058238983154
Epoch 280, val loss: 0.6740667819976807
Epoch 290, training loss: 0.280325323343277 = 0.21256110072135925 + 0.01 * 6.776423454284668
Epoch 290, val loss: 0.6661533713340759
Epoch 300, training loss: 0.25264865159988403 = 0.1849447637796402 + 0.01 * 6.770388126373291
Epoch 300, val loss: 0.6611107587814331
Epoch 310, training loss: 0.2278839349746704 = 0.16023893654346466 + 0.01 * 6.764500141143799
Epoch 310, val loss: 0.658514678478241
Epoch 320, training loss: 0.2059503197669983 = 0.13838785886764526 + 0.01 * 6.756246089935303
Epoch 320, val loss: 0.6580075621604919
Epoch 330, training loss: 0.1868990957736969 = 0.11938624829053879 + 0.01 * 6.751284122467041
Epoch 330, val loss: 0.659317135810852
Epoch 340, training loss: 0.170562282204628 = 0.10316534340381622 + 0.01 * 6.739694118499756
Epoch 340, val loss: 0.6622800230979919
Epoch 350, training loss: 0.15679463744163513 = 0.08949150890111923 + 0.01 * 6.730312347412109
Epoch 350, val loss: 0.6666353940963745
Epoch 360, training loss: 0.14531740546226501 = 0.07805530726909637 + 0.01 * 6.726210117340088
Epoch 360, val loss: 0.6721029877662659
Epoch 370, training loss: 0.13567553460597992 = 0.06852380186319351 + 0.01 * 6.715173244476318
Epoch 370, val loss: 0.6785396337509155
Epoch 380, training loss: 0.12763920426368713 = 0.06056249141693115 + 0.01 * 6.707671165466309
Epoch 380, val loss: 0.6857107281684875
Epoch 390, training loss: 0.12089769542217255 = 0.05388370901346207 + 0.01 * 6.701398849487305
Epoch 390, val loss: 0.6934069395065308
Epoch 400, training loss: 0.1152053102850914 = 0.04823911935091019 + 0.01 * 6.696619510650635
Epoch 400, val loss: 0.7014240622520447
Epoch 410, training loss: 0.11040575057268143 = 0.043432898819446564 + 0.01 * 6.697285175323486
Epoch 410, val loss: 0.7096218466758728
Epoch 420, training loss: 0.10619838535785675 = 0.0393107570707798 + 0.01 * 6.688763618469238
Epoch 420, val loss: 0.717919111251831
Epoch 430, training loss: 0.10260847955942154 = 0.035748109221458435 + 0.01 * 6.686037063598633
Epoch 430, val loss: 0.726239800453186
Epoch 440, training loss: 0.09948627650737762 = 0.03264794871211052 + 0.01 * 6.683833122253418
Epoch 440, val loss: 0.7344999313354492
Epoch 450, training loss: 0.0967724397778511 = 0.02993435226380825 + 0.01 * 6.68380880355835
Epoch 450, val loss: 0.7427331209182739
Epoch 460, training loss: 0.09434191882610321 = 0.027547113597393036 + 0.01 * 6.679481029510498
Epoch 460, val loss: 0.7508445978164673
Epoch 470, training loss: 0.09220010042190552 = 0.025433357805013657 + 0.01 * 6.6766743659973145
Epoch 470, val loss: 0.7588576674461365
Epoch 480, training loss: 0.09033487737178802 = 0.023549621924757957 + 0.01 * 6.678525447845459
Epoch 480, val loss: 0.7667302489280701
Epoch 490, training loss: 0.08856439590454102 = 0.021860528737306595 + 0.01 * 6.67038631439209
Epoch 490, val loss: 0.7745277285575867
Epoch 500, training loss: 0.08700567483901978 = 0.02033926174044609 + 0.01 * 6.666640758514404
Epoch 500, val loss: 0.7822234034538269
Epoch 510, training loss: 0.08571358025074005 = 0.018963119015097618 + 0.01 * 6.675045967102051
Epoch 510, val loss: 0.7898192405700684
Epoch 520, training loss: 0.08433441817760468 = 0.01771727204322815 + 0.01 * 6.661714553833008
Epoch 520, val loss: 0.7972713708877563
Epoch 530, training loss: 0.08316586911678314 = 0.016583990305662155 + 0.01 * 6.6581878662109375
Epoch 530, val loss: 0.8045780062675476
Epoch 540, training loss: 0.08211036771535873 = 0.015549993142485619 + 0.01 * 6.656037330627441
Epoch 540, val loss: 0.811797559261322
Epoch 550, training loss: 0.08113359659910202 = 0.014606520533561707 + 0.01 * 6.652707576751709
Epoch 550, val loss: 0.8188913464546204
Epoch 560, training loss: 0.08025165647268295 = 0.013744165189564228 + 0.01 * 6.650749206542969
Epoch 560, val loss: 0.8258476257324219
Epoch 570, training loss: 0.0794542133808136 = 0.012954676523804665 + 0.01 * 6.649954319000244
Epoch 570, val loss: 0.8326711058616638
Epoch 580, training loss: 0.07869390398263931 = 0.012232829816639423 + 0.01 * 6.646107196807861
Epoch 580, val loss: 0.839328944683075
Epoch 590, training loss: 0.07799314707517624 = 0.011569288559257984 + 0.01 * 6.642385959625244
Epoch 590, val loss: 0.845881998538971
Epoch 600, training loss: 0.07733297348022461 = 0.010957678779959679 + 0.01 * 6.637529373168945
Epoch 600, val loss: 0.8522953391075134
Epoch 610, training loss: 0.0768410935997963 = 0.010392568074166775 + 0.01 * 6.644852638244629
Epoch 610, val loss: 0.8586063385009766
Epoch 620, training loss: 0.0761900320649147 = 0.009871875867247581 + 0.01 * 6.631815433502197
Epoch 620, val loss: 0.8647730946540833
Epoch 630, training loss: 0.07567290961742401 = 0.009390419349074364 + 0.01 * 6.628249168395996
Epoch 630, val loss: 0.8708349466323853
Epoch 640, training loss: 0.07527738809585571 = 0.008943797089159489 + 0.01 * 6.633359432220459
Epoch 640, val loss: 0.8767625689506531
Epoch 650, training loss: 0.07476125657558441 = 0.008529688231647015 + 0.01 * 6.623157501220703
Epoch 650, val loss: 0.8825898766517639
Epoch 660, training loss: 0.07433892786502838 = 0.008144920691847801 + 0.01 * 6.619400978088379
Epoch 660, val loss: 0.8882669806480408
Epoch 670, training loss: 0.07394170016050339 = 0.007786712143570185 + 0.01 * 6.615499019622803
Epoch 670, val loss: 0.8938595056533813
Epoch 680, training loss: 0.07367770373821259 = 0.007452500052750111 + 0.01 * 6.622520446777344
Epoch 680, val loss: 0.8992906212806702
Epoch 690, training loss: 0.07324304431676865 = 0.007139959838241339 + 0.01 * 6.610308647155762
Epoch 690, val loss: 0.9046695232391357
Epoch 700, training loss: 0.0728955864906311 = 0.006848128978163004 + 0.01 * 6.604745864868164
Epoch 700, val loss: 0.9099273681640625
Epoch 710, training loss: 0.07266311347484589 = 0.006576210260391235 + 0.01 * 6.6086907386779785
Epoch 710, val loss: 0.9150978326797485
Epoch 720, training loss: 0.07226498425006866 = 0.006321519147604704 + 0.01 * 6.594346523284912
Epoch 720, val loss: 0.9200804829597473
Epoch 730, training loss: 0.07199624925851822 = 0.006081283558160067 + 0.01 * 6.591496467590332
Epoch 730, val loss: 0.9250537753105164
Epoch 740, training loss: 0.07172313332557678 = 0.005856835749000311 + 0.01 * 6.586630344390869
Epoch 740, val loss: 0.9298559427261353
Epoch 750, training loss: 0.07150469720363617 = 0.005645816680043936 + 0.01 * 6.585887908935547
Epoch 750, val loss: 0.9346024990081787
Epoch 760, training loss: 0.07137191295623779 = 0.00544786499813199 + 0.01 * 6.592405319213867
Epoch 760, val loss: 0.9391753077507019
Epoch 770, training loss: 0.07096341252326965 = 0.00525943236425519 + 0.01 * 6.570397853851318
Epoch 770, val loss: 0.9437740445137024
Epoch 780, training loss: 0.0709737241268158 = 0.005084439180791378 + 0.01 * 6.588928699493408
Epoch 780, val loss: 0.9481385350227356
Epoch 790, training loss: 0.07065089792013168 = 0.004917371552437544 + 0.01 * 6.573353290557861
Epoch 790, val loss: 0.952532172203064
Epoch 800, training loss: 0.07041648775339127 = 0.004759213421493769 + 0.01 * 6.565727233886719
Epoch 800, val loss: 0.9568378329277039
Epoch 810, training loss: 0.07030242681503296 = 0.004612931981682777 + 0.01 * 6.5689496994018555
Epoch 810, val loss: 0.9609310626983643
Epoch 820, training loss: 0.06991461664438248 = 0.004469744861125946 + 0.01 * 6.544486999511719
Epoch 820, val loss: 0.9651225805282593
Epoch 830, training loss: 0.07024872303009033 = 0.0043362826108932495 + 0.01 * 6.591244220733643
Epoch 830, val loss: 0.9691088795661926
Epoch 840, training loss: 0.06956955045461655 = 0.004211001563817263 + 0.01 * 6.535855293273926
Epoch 840, val loss: 0.973039984703064
Epoch 850, training loss: 0.06947407126426697 = 0.004089986439794302 + 0.01 * 6.5384087562561035
Epoch 850, val loss: 0.9769333600997925
Epoch 860, training loss: 0.06927113980054855 = 0.003976201172918081 + 0.01 * 6.529493808746338
Epoch 860, val loss: 0.9806420207023621
Epoch 870, training loss: 0.06935101002454758 = 0.0038666878826916218 + 0.01 * 6.548432350158691
Epoch 870, val loss: 0.9844292402267456
Epoch 880, training loss: 0.06908819079399109 = 0.0037641015369445086 + 0.01 * 6.532409191131592
Epoch 880, val loss: 0.9880408644676208
Epoch 890, training loss: 0.06896399706602097 = 0.003664494026452303 + 0.01 * 6.529950141906738
Epoch 890, val loss: 0.9917209148406982
Epoch 900, training loss: 0.06876654922962189 = 0.0035708195064216852 + 0.01 * 6.519573211669922
Epoch 900, val loss: 0.9952021837234497
Epoch 910, training loss: 0.06870804727077484 = 0.0034807894844561815 + 0.01 * 6.522725582122803
Epoch 910, val loss: 0.9987495541572571
Epoch 920, training loss: 0.0683959349989891 = 0.0033954738173633814 + 0.01 * 6.500046253204346
Epoch 920, val loss: 1.0020807981491089
Epoch 930, training loss: 0.06850019097328186 = 0.003312805900350213 + 0.01 * 6.518738746643066
Epoch 930, val loss: 1.0054785013198853
Epoch 940, training loss: 0.06821789592504501 = 0.0032339857425540686 + 0.01 * 6.498391151428223
Epoch 940, val loss: 1.0088235139846802
Epoch 950, training loss: 0.0683373287320137 = 0.0031592370942234993 + 0.01 * 6.51780891418457
Epoch 950, val loss: 1.0119240283966064
Epoch 960, training loss: 0.06809338927268982 = 0.0030873192008584738 + 0.01 * 6.500607490539551
Epoch 960, val loss: 1.0151294469833374
Epoch 970, training loss: 0.06792254745960236 = 0.0030190746765583754 + 0.01 * 6.490347862243652
Epoch 970, val loss: 1.0182353258132935
Epoch 980, training loss: 0.06777873635292053 = 0.0029525028076022863 + 0.01 * 6.48262357711792
Epoch 980, val loss: 1.02142333984375
Epoch 990, training loss: 0.06800265610218048 = 0.002888502087444067 + 0.01 * 6.511415958404541
Epoch 990, val loss: 1.0245682001113892
Epoch 1000, training loss: 0.067581407725811 = 0.0028273442294448614 + 0.01 * 6.475406169891357
Epoch 1000, val loss: 1.0276025533676147
Epoch 1010, training loss: 0.06764023751020432 = 0.002768518403172493 + 0.01 * 6.487172603607178
Epoch 1010, val loss: 1.030595064163208
Epoch 1020, training loss: 0.06744959950447083 = 0.002712228335440159 + 0.01 * 6.473737716674805
Epoch 1020, val loss: 1.0334999561309814
Epoch 1030, training loss: 0.06754668056964874 = 0.00265897111967206 + 0.01 * 6.488770961761475
Epoch 1030, val loss: 1.0361864566802979
Epoch 1040, training loss: 0.06737738847732544 = 0.0026066566351801157 + 0.01 * 6.477072715759277
Epoch 1040, val loss: 1.0390803813934326
Epoch 1050, training loss: 0.0671248584985733 = 0.0025569729041308165 + 0.01 * 6.456789016723633
Epoch 1050, val loss: 1.0418217182159424
Epoch 1060, training loss: 0.06722164154052734 = 0.002509085927158594 + 0.01 * 6.471255302429199
Epoch 1060, val loss: 1.0445852279663086
Epoch 1070, training loss: 0.06708694249391556 = 0.0024631964042782784 + 0.01 * 6.462374687194824
Epoch 1070, val loss: 1.047141432762146
Epoch 1080, training loss: 0.06714537739753723 = 0.0024184100329875946 + 0.01 * 6.4726972579956055
Epoch 1080, val loss: 1.049797773361206
Epoch 1090, training loss: 0.06683653593063354 = 0.0023759682662785053 + 0.01 * 6.446057319641113
Epoch 1090, val loss: 1.0523951053619385
Epoch 1100, training loss: 0.06746203452348709 = 0.002335126744583249 + 0.01 * 6.512691020965576
Epoch 1100, val loss: 1.0548982620239258
Epoch 1110, training loss: 0.06677843630313873 = 0.0022950368002057076 + 0.01 * 6.448339939117432
Epoch 1110, val loss: 1.057464361190796
Epoch 1120, training loss: 0.06694574654102325 = 0.0022567841224372387 + 0.01 * 6.468896865844727
Epoch 1120, val loss: 1.05994713306427
Epoch 1130, training loss: 0.06659302115440369 = 0.002219949848949909 + 0.01 * 6.437306880950928
Epoch 1130, val loss: 1.0622915029525757
Epoch 1140, training loss: 0.06678324937820435 = 0.00218454678542912 + 0.01 * 6.459870338439941
Epoch 1140, val loss: 1.0646336078643799
Epoch 1150, training loss: 0.0664646327495575 = 0.0021498820278793573 + 0.01 * 6.431475639343262
Epoch 1150, val loss: 1.0670586824417114
Epoch 1160, training loss: 0.06714116781949997 = 0.002116627525538206 + 0.01 * 6.502453804016113
Epoch 1160, val loss: 1.0692633390426636
Epoch 1170, training loss: 0.06629527360200882 = 0.002084600506350398 + 0.01 * 6.421067714691162
Epoch 1170, val loss: 1.0715957880020142
Epoch 1180, training loss: 0.06629066914319992 = 0.0020535278599709272 + 0.01 * 6.4237141609191895
Epoch 1180, val loss: 1.0738850831985474
Epoch 1190, training loss: 0.06627930700778961 = 0.0020234801340848207 + 0.01 * 6.4255828857421875
Epoch 1190, val loss: 1.075956106185913
Epoch 1200, training loss: 0.06617359071969986 = 0.001994670368731022 + 0.01 * 6.417891979217529
Epoch 1200, val loss: 1.0781217813491821
Epoch 1210, training loss: 0.06624244898557663 = 0.001966618001461029 + 0.01 * 6.42758321762085
Epoch 1210, val loss: 1.0802100896835327
Epoch 1220, training loss: 0.06620510667562485 = 0.001939441659487784 + 0.01 * 6.426567077636719
Epoch 1220, val loss: 1.0823004245758057
Epoch 1230, training loss: 0.06591898202896118 = 0.0019129813881590962 + 0.01 * 6.400600433349609
Epoch 1230, val loss: 1.0844484567642212
Epoch 1240, training loss: 0.06600429862737656 = 0.0018875182140618563 + 0.01 * 6.411678791046143
Epoch 1240, val loss: 1.086405873298645
Epoch 1250, training loss: 0.06623406708240509 = 0.001862954581156373 + 0.01 * 6.437110900878906
Epoch 1250, val loss: 1.0885072946548462
Epoch 1260, training loss: 0.06583605706691742 = 0.001838928204961121 + 0.01 * 6.399712562561035
Epoch 1260, val loss: 1.09043550491333
Epoch 1270, training loss: 0.06593937426805496 = 0.0018157648155465722 + 0.01 * 6.412361145019531
Epoch 1270, val loss: 1.092362880706787
Epoch 1280, training loss: 0.06573132425546646 = 0.0017931939801201224 + 0.01 * 6.393813610076904
Epoch 1280, val loss: 1.0943915843963623
Epoch 1290, training loss: 0.06588968634605408 = 0.0017713928828015924 + 0.01 * 6.411829948425293
Epoch 1290, val loss: 1.0961836576461792
Epoch 1300, training loss: 0.06562679260969162 = 0.001750064198859036 + 0.01 * 6.3876729011535645
Epoch 1300, val loss: 1.098062515258789
Epoch 1310, training loss: 0.06550195813179016 = 0.0017295219004154205 + 0.01 * 6.377243995666504
Epoch 1310, val loss: 1.0997865200042725
Epoch 1320, training loss: 0.06547439843416214 = 0.0017096205847337842 + 0.01 * 6.37647819519043
Epoch 1320, val loss: 1.101601004600525
Epoch 1330, training loss: 0.06552746146917343 = 0.0016902112402021885 + 0.01 * 6.383725166320801
Epoch 1330, val loss: 1.1033295392990112
Epoch 1340, training loss: 0.06540627777576447 = 0.0016711889766156673 + 0.01 * 6.373508930206299
Epoch 1340, val loss: 1.1051231622695923
Epoch 1350, training loss: 0.06538441777229309 = 0.0016528874402865767 + 0.01 * 6.373153209686279
Epoch 1350, val loss: 1.1067755222320557
Epoch 1360, training loss: 0.06526139378547668 = 0.0016350579680874944 + 0.01 * 6.36263370513916
Epoch 1360, val loss: 1.108498215675354
Epoch 1370, training loss: 0.06530436873435974 = 0.0016175481723621488 + 0.01 * 6.368681907653809
Epoch 1370, val loss: 1.1101237535476685
Epoch 1380, training loss: 0.06519744545221329 = 0.0016006710939109325 + 0.01 * 6.359677791595459
Epoch 1380, val loss: 1.1117655038833618
Epoch 1390, training loss: 0.06525351107120514 = 0.0015841189306229353 + 0.01 * 6.366939544677734
Epoch 1390, val loss: 1.1133040189743042
Epoch 1400, training loss: 0.0650465190410614 = 0.0015682998346164823 + 0.01 * 6.3478217124938965
Epoch 1400, val loss: 1.1149386167526245
Epoch 1410, training loss: 0.06517265737056732 = 0.0015524782938882709 + 0.01 * 6.36201810836792
Epoch 1410, val loss: 1.1165168285369873
Epoch 1420, training loss: 0.0649435818195343 = 0.001537270494736731 + 0.01 * 6.340631008148193
Epoch 1420, val loss: 1.1181104183197021
Epoch 1430, training loss: 0.06513607501983643 = 0.0015223872615024447 + 0.01 * 6.3613691329956055
Epoch 1430, val loss: 1.1195578575134277
Epoch 1440, training loss: 0.06499553471803665 = 0.001507904613390565 + 0.01 * 6.3487629890441895
Epoch 1440, val loss: 1.1211204528808594
Epoch 1450, training loss: 0.06483766436576843 = 0.0014937628293409944 + 0.01 * 6.334390640258789
Epoch 1450, val loss: 1.1226164102554321
Epoch 1460, training loss: 0.0648537427186966 = 0.0014799641212448478 + 0.01 * 6.33737850189209
Epoch 1460, val loss: 1.1240328550338745
Epoch 1470, training loss: 0.06497140973806381 = 0.0014665108174085617 + 0.01 * 6.350489616394043
Epoch 1470, val loss: 1.1254805326461792
Epoch 1480, training loss: 0.06478408724069595 = 0.0014534913934767246 + 0.01 * 6.333060264587402
Epoch 1480, val loss: 1.1269282102584839
Epoch 1490, training loss: 0.06469671428203583 = 0.001440651947632432 + 0.01 * 6.325606346130371
Epoch 1490, val loss: 1.1283626556396484
Epoch 1500, training loss: 0.06493206322193146 = 0.001428090501576662 + 0.01 * 6.350397109985352
Epoch 1500, val loss: 1.1297483444213867
Epoch 1510, training loss: 0.06467150151729584 = 0.0014159242855384946 + 0.01 * 6.325557708740234
Epoch 1510, val loss: 1.1311284303665161
Epoch 1520, training loss: 0.06464864313602448 = 0.0014039268717169762 + 0.01 * 6.324471950531006
Epoch 1520, val loss: 1.132524013519287
Epoch 1530, training loss: 0.06484677642583847 = 0.0013923748629167676 + 0.01 * 6.345440864562988
Epoch 1530, val loss: 1.1338413953781128
Epoch 1540, training loss: 0.06452671438455582 = 0.0013809772208333015 + 0.01 * 6.314573764801025
Epoch 1540, val loss: 1.1351977586746216
Epoch 1550, training loss: 0.06468592584133148 = 0.0013696852838620543 + 0.01 * 6.3316240310668945
Epoch 1550, val loss: 1.1364941596984863
Epoch 1560, training loss: 0.0646383985877037 = 0.0013590072048828006 + 0.01 * 6.327939033508301
Epoch 1560, val loss: 1.1378291845321655
Epoch 1570, training loss: 0.06501484662294388 = 0.0013483824441209435 + 0.01 * 6.366646766662598
Epoch 1570, val loss: 1.1391005516052246
Epoch 1580, training loss: 0.06450917571783066 = 0.0013378767762333155 + 0.01 * 6.317129611968994
Epoch 1580, val loss: 1.1403617858886719
Epoch 1590, training loss: 0.06490451097488403 = 0.0013277955586090684 + 0.01 * 6.357671737670898
Epoch 1590, val loss: 1.1417031288146973
Epoch 1600, training loss: 0.06433258205652237 = 0.0013176524080336094 + 0.01 * 6.301493167877197
Epoch 1600, val loss: 1.1429100036621094
Epoch 1610, training loss: 0.06431997567415237 = 0.0013079060008749366 + 0.01 * 6.301207065582275
Epoch 1610, val loss: 1.1441625356674194
Epoch 1620, training loss: 0.06439606100320816 = 0.0012984490022063255 + 0.01 * 6.309761047363281
Epoch 1620, val loss: 1.1453369855880737
Epoch 1630, training loss: 0.06418021768331528 = 0.001289099222049117 + 0.01 * 6.289112091064453
Epoch 1630, val loss: 1.146571159362793
Epoch 1640, training loss: 0.06426454335451126 = 0.0012798310490325093 + 0.01 * 6.298471450805664
Epoch 1640, val loss: 1.147772192955017
Epoch 1650, training loss: 0.06446270644664764 = 0.0012709019938483834 + 0.01 * 6.319180488586426
Epoch 1650, val loss: 1.1489617824554443
Epoch 1660, training loss: 0.06408657133579254 = 0.0012620887719094753 + 0.01 * 6.282447814941406
Epoch 1660, val loss: 1.1501317024230957
Epoch 1670, training loss: 0.06433921307325363 = 0.0012534803245216608 + 0.01 * 6.3085737228393555
Epoch 1670, val loss: 1.1512742042541504
Epoch 1680, training loss: 0.06414651870727539 = 0.0012450616341084242 + 0.01 * 6.290146350860596
Epoch 1680, val loss: 1.1524804830551147
Epoch 1690, training loss: 0.06419356912374496 = 0.0012366811279207468 + 0.01 * 6.295688629150391
Epoch 1690, val loss: 1.1536433696746826
Epoch 1700, training loss: 0.0641418844461441 = 0.0012285675620660186 + 0.01 * 6.291331768035889
Epoch 1700, val loss: 1.15475332736969
Epoch 1710, training loss: 0.06418377161026001 = 0.001220631878823042 + 0.01 * 6.296314239501953
Epoch 1710, val loss: 1.155882716178894
Epoch 1720, training loss: 0.06402307748794556 = 0.00121288001537323 + 0.01 * 6.281020164489746
Epoch 1720, val loss: 1.15700364112854
Epoch 1730, training loss: 0.06449637562036514 = 0.0012052928796038032 + 0.01 * 6.329108238220215
Epoch 1730, val loss: 1.1581165790557861
Epoch 1740, training loss: 0.06388498097658157 = 0.0011977126123383641 + 0.01 * 6.2687273025512695
Epoch 1740, val loss: 1.1591523885726929
Epoch 1750, training loss: 0.06397651880979538 = 0.0011903520207852125 + 0.01 * 6.278616905212402
Epoch 1750, val loss: 1.1602765321731567
Epoch 1760, training loss: 0.06395941972732544 = 0.0011831087758764625 + 0.01 * 6.2776312828063965
Epoch 1760, val loss: 1.161301851272583
Epoch 1770, training loss: 0.06416219472885132 = 0.0011760906782001257 + 0.01 * 6.298610687255859
Epoch 1770, val loss: 1.162375569343567
Epoch 1780, training loss: 0.0640253871679306 = 0.0011690255487337708 + 0.01 * 6.2856364250183105
Epoch 1780, val loss: 1.1633799076080322
Epoch 1790, training loss: 0.0637146383523941 = 0.0011621720623224974 + 0.01 * 6.255247116088867
Epoch 1790, val loss: 1.1644333600997925
Epoch 1800, training loss: 0.06390358507633209 = 0.0011554814409464598 + 0.01 * 6.274810791015625
Epoch 1800, val loss: 1.165439486503601
Epoch 1810, training loss: 0.06377247720956802 = 0.0011488560121506453 + 0.01 * 6.262362480163574
Epoch 1810, val loss: 1.1664752960205078
Epoch 1820, training loss: 0.06404394656419754 = 0.0011423847172409296 + 0.01 * 6.290156364440918
Epoch 1820, val loss: 1.1674473285675049
Epoch 1830, training loss: 0.063947893679142 = 0.0011359953787177801 + 0.01 * 6.281189441680908
Epoch 1830, val loss: 1.1684355735778809
Epoch 1840, training loss: 0.06360813230276108 = 0.0011296995216980577 + 0.01 * 6.2478437423706055
Epoch 1840, val loss: 1.1694172620773315
Epoch 1850, training loss: 0.06371543556451797 = 0.0011235219426453114 + 0.01 * 6.259191513061523
Epoch 1850, val loss: 1.1703846454620361
Epoch 1860, training loss: 0.06394516676664352 = 0.0011174341198056936 + 0.01 * 6.282773971557617
Epoch 1860, val loss: 1.171355128288269
Epoch 1870, training loss: 0.06389138102531433 = 0.0011114353546872735 + 0.01 * 6.2779951095581055
Epoch 1870, val loss: 1.1723017692565918
Epoch 1880, training loss: 0.06371463090181351 = 0.001105551840737462 + 0.01 * 6.260908126831055
Epoch 1880, val loss: 1.1732860803604126
Epoch 1890, training loss: 0.06354460120201111 = 0.001099656685255468 + 0.01 * 6.244494438171387
Epoch 1890, val loss: 1.1741951704025269
Epoch 1900, training loss: 0.06403504312038422 = 0.0010939231142401695 + 0.01 * 6.294112205505371
Epoch 1900, val loss: 1.1751383543014526
Epoch 1910, training loss: 0.06356509029865265 = 0.0010883373906835914 + 0.01 * 6.24767541885376
Epoch 1910, val loss: 1.1760375499725342
Epoch 1920, training loss: 0.06370525807142258 = 0.001082761911675334 + 0.01 * 6.262249946594238
Epoch 1920, val loss: 1.1769661903381348
Epoch 1930, training loss: 0.06359472870826721 = 0.0010772583773359656 + 0.01 * 6.2517476081848145
Epoch 1930, val loss: 1.177876353263855
Epoch 1940, training loss: 0.06348717957735062 = 0.001071867416612804 + 0.01 * 6.2415313720703125
Epoch 1940, val loss: 1.178778052330017
Epoch 1950, training loss: 0.06346255540847778 = 0.001066520344465971 + 0.01 * 6.239603519439697
Epoch 1950, val loss: 1.179679274559021
Epoch 1960, training loss: 0.06356029212474823 = 0.001061255345121026 + 0.01 * 6.249903678894043
Epoch 1960, val loss: 1.1805988550186157
Epoch 1970, training loss: 0.0635971873998642 = 0.0010559768415987492 + 0.01 * 6.25412130355835
Epoch 1970, val loss: 1.1814824342727661
Epoch 1980, training loss: 0.06356684118509293 = 0.001050868653692305 + 0.01 * 6.251597881317139
Epoch 1980, val loss: 1.182397723197937
Epoch 1990, training loss: 0.0634748712182045 = 0.0010458477772772312 + 0.01 * 6.2429022789001465
Epoch 1990, val loss: 1.18324875831604
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.8376
Flip ASR: 0.8133/225 nodes
The final ASR:0.66421, 0.17514, Accuracy:0.82840, 0.00349
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11656])
remove edge: torch.Size([2, 9470])
updated graph: torch.Size([2, 10570])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.97786, 0.00301, Accuracy:0.83457, 0.00630
Begin epxeriment: cont_weight: 0.01 epoch:2000 der1:0.3 der2:0 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0368881225585938 = 1.9531505107879639 + 0.01 * 8.373766899108887
Epoch 0, val loss: 1.9516066312789917
Epoch 10, training loss: 2.026737689971924 = 1.943001389503479 + 0.01 * 8.373635292053223
Epoch 10, val loss: 1.942234992980957
Epoch 20, training loss: 2.0143935680389404 = 1.9306613206863403 + 0.01 * 8.373218536376953
Epoch 20, val loss: 1.9304592609405518
Epoch 30, training loss: 1.9971275329589844 = 1.9134087562561035 + 0.01 * 8.371871948242188
Epoch 30, val loss: 1.9138065576553345
Epoch 40, training loss: 1.9714202880859375 = 1.887782096862793 + 0.01 * 8.36381721496582
Epoch 40, val loss: 1.889389991760254
Epoch 50, training loss: 1.9341857433319092 = 1.8510396480560303 + 0.01 * 8.314608573913574
Epoch 50, val loss: 1.8560394048690796
Epoch 60, training loss: 1.8885862827301025 = 1.8076645135879517 + 0.01 * 8.092181205749512
Epoch 60, val loss: 1.8203024864196777
Epoch 70, training loss: 1.8463994264602661 = 1.7688367366790771 + 0.01 * 7.7562665939331055
Epoch 70, val loss: 1.7898765802383423
Epoch 80, training loss: 1.7989697456359863 = 1.724407434463501 + 0.01 * 7.4562273025512695
Epoch 80, val loss: 1.749241828918457
Epoch 90, training loss: 1.734859824180603 = 1.6625316143035889 + 0.01 * 7.232823371887207
Epoch 90, val loss: 1.6944748163223267
Epoch 100, training loss: 1.6512690782546997 = 1.5811119079589844 + 0.01 * 7.015713214874268
Epoch 100, val loss: 1.6276129484176636
Epoch 110, training loss: 1.5543874502182007 = 1.4857416152954102 + 0.01 * 6.864583969116211
Epoch 110, val loss: 1.5505743026733398
Epoch 120, training loss: 1.4565283060073853 = 1.3882781267166138 + 0.01 * 6.825015544891357
Epoch 120, val loss: 1.4713736772537231
Epoch 130, training loss: 1.3615227937698364 = 1.2935436964035034 + 0.01 * 6.797908782958984
Epoch 130, val loss: 1.3949916362762451
Epoch 140, training loss: 1.2686030864715576 = 1.2007217407226562 + 0.01 * 6.788136005401611
Epoch 140, val loss: 1.3217145204544067
Epoch 150, training loss: 1.1755045652389526 = 1.1076997518539429 + 0.01 * 6.780481815338135
Epoch 150, val loss: 1.2492886781692505
Epoch 160, training loss: 1.081115484237671 = 1.0134071111679077 + 0.01 * 6.77083683013916
Epoch 160, val loss: 1.176716923713684
Epoch 170, training loss: 0.9865923523902893 = 0.9190077781677246 + 0.01 * 6.758457660675049
Epoch 170, val loss: 1.1043654680252075
Epoch 180, training loss: 0.8963138461112976 = 0.8288785815238953 + 0.01 * 6.743526458740234
Epoch 180, val loss: 1.0363849401474
Epoch 190, training loss: 0.8150134086608887 = 0.747739851474762 + 0.01 * 6.727355480194092
Epoch 190, val loss: 0.9772698283195496
Epoch 200, training loss: 0.7439106702804565 = 0.6767914891242981 + 0.01 * 6.711915969848633
Epoch 200, val loss: 0.9284412860870361
Epoch 210, training loss: 0.6806680560112 = 0.6136847138404846 + 0.01 * 6.698334217071533
Epoch 210, val loss: 0.8883169293403625
Epoch 220, training loss: 0.6220976114273071 = 0.5552224516868591 + 0.01 * 6.687517166137695
Epoch 220, val loss: 0.8540935516357422
Epoch 230, training loss: 0.565944492816925 = 0.499193400144577 + 0.01 * 6.675109386444092
Epoch 230, val loss: 0.8235160112380981
Epoch 240, training loss: 0.5114806294441223 = 0.44483786821365356 + 0.01 * 6.664275646209717
Epoch 240, val loss: 0.7959402799606323
Epoch 250, training loss: 0.45939359068870544 = 0.3928336799144745 + 0.01 * 6.655991077423096
Epoch 250, val loss: 0.7723899483680725
Epoch 260, training loss: 0.4109480381011963 = 0.34447765350341797 + 0.01 * 6.647037506103516
Epoch 260, val loss: 0.754173219203949
Epoch 270, training loss: 0.36729106307029724 = 0.3009154796600342 + 0.01 * 6.637557506561279
Epoch 270, val loss: 0.7419494986534119
Epoch 280, training loss: 0.3289393186569214 = 0.2626327872276306 + 0.01 * 6.630654811859131
Epoch 280, val loss: 0.7359429001808167
Epoch 290, training loss: 0.29575008153915405 = 0.22948800027370453 + 0.01 * 6.626208305358887
Epoch 290, val loss: 0.735870897769928
Epoch 300, training loss: 0.2671909034252167 = 0.2009703367948532 + 0.01 * 6.62205696105957
Epoch 300, val loss: 0.7409979104995728
Epoch 310, training loss: 0.24261128902435303 = 0.17645558714866638 + 0.01 * 6.615570545196533
Epoch 310, val loss: 0.7504706382751465
Epoch 320, training loss: 0.22150656580924988 = 0.1553632915019989 + 0.01 * 6.614328384399414
Epoch 320, val loss: 0.7633925676345825
Epoch 330, training loss: 0.20325836539268494 = 0.1371610164642334 + 0.01 * 6.60973596572876
Epoch 330, val loss: 0.7790448665618896
Epoch 340, training loss: 0.1874770224094391 = 0.12141049653291702 + 0.01 * 6.606653690338135
Epoch 340, val loss: 0.7968533039093018
Epoch 350, training loss: 0.17374643683433533 = 0.10772990435361862 + 0.01 * 6.601653099060059
Epoch 350, val loss: 0.8162518739700317
Epoch 360, training loss: 0.16179177165031433 = 0.09580022841691971 + 0.01 * 6.599154472351074
Epoch 360, val loss: 0.8368159532546997
Epoch 370, training loss: 0.15131157636642456 = 0.08536521345376968 + 0.01 * 6.594637393951416
Epoch 370, val loss: 0.8581852912902832
Epoch 380, training loss: 0.14210247993469238 = 0.07621355354785919 + 0.01 * 6.588892936706543
Epoch 380, val loss: 0.880095899105072
Epoch 390, training loss: 0.13403454422950745 = 0.06817488372325897 + 0.01 * 6.58596658706665
Epoch 390, val loss: 0.9023038148880005
Epoch 400, training loss: 0.1269904375076294 = 0.06111123412847519 + 0.01 * 6.587920665740967
Epoch 400, val loss: 0.9245795607566833
Epoch 410, training loss: 0.1206953376531601 = 0.05491168797016144 + 0.01 * 6.578365325927734
Epoch 410, val loss: 0.9467165470123291
Epoch 420, training loss: 0.11522427201271057 = 0.049469783902168274 + 0.01 * 6.575448989868164
Epoch 420, val loss: 0.968535840511322
Epoch 430, training loss: 0.11039797961711884 = 0.044689785689115524 + 0.01 * 6.570819854736328
Epoch 430, val loss: 0.9899463653564453
Epoch 440, training loss: 0.10614395141601562 = 0.04048686847090721 + 0.01 * 6.565708637237549
Epoch 440, val loss: 1.0107895135879517
Epoch 450, training loss: 0.1024090051651001 = 0.03678285330533981 + 0.01 * 6.562615394592285
Epoch 450, val loss: 1.0311477184295654
Epoch 460, training loss: 0.09910544753074646 = 0.03351105377078056 + 0.01 * 6.559439659118652
Epoch 460, val loss: 1.0509576797485352
Epoch 470, training loss: 0.09629074484109879 = 0.03061545453965664 + 0.01 * 6.567529201507568
Epoch 470, val loss: 1.070173740386963
Epoch 480, training loss: 0.09356427192687988 = 0.02805221453309059 + 0.01 * 6.551206588745117
Epoch 480, val loss: 1.0888237953186035
Epoch 490, training loss: 0.09128745645284653 = 0.025775419548153877 + 0.01 * 6.551203727722168
Epoch 490, val loss: 1.1069624423980713
Epoch 500, training loss: 0.08922726660966873 = 0.02374718338251114 + 0.01 * 6.548008441925049
Epoch 500, val loss: 1.1245604753494263
Epoch 510, training loss: 0.08736803382635117 = 0.021936772391200066 + 0.01 * 6.543126106262207
Epoch 510, val loss: 1.1416908502578735
Epoch 520, training loss: 0.08581145107746124 = 0.020316217094659805 + 0.01 * 6.549523830413818
Epoch 520, val loss: 1.1583266258239746
Epoch 530, training loss: 0.08427168428897858 = 0.01886509545147419 + 0.01 * 6.540659427642822
Epoch 530, val loss: 1.17449152469635
Epoch 540, training loss: 0.08292851597070694 = 0.017560787498950958 + 0.01 * 6.536773204803467
Epoch 540, val loss: 1.1901814937591553
Epoch 550, training loss: 0.08170046657323837 = 0.016383742913603783 + 0.01 * 6.531672477722168
Epoch 550, val loss: 1.2054167985916138
Epoch 560, training loss: 0.08061446994543076 = 0.01531840581446886 + 0.01 * 6.529606342315674
Epoch 560, val loss: 1.2202732563018799
Epoch 570, training loss: 0.07965981215238571 = 0.014352026395499706 + 0.01 * 6.5307793617248535
Epoch 570, val loss: 1.2347241640090942
Epoch 580, training loss: 0.07872654497623444 = 0.013474267907440662 + 0.01 * 6.5252275466918945
Epoch 580, val loss: 1.2487313747406006
Epoch 590, training loss: 0.07790020108222961 = 0.012674039229750633 + 0.01 * 6.522616386413574
Epoch 590, val loss: 1.2623928785324097
Epoch 600, training loss: 0.07715925574302673 = 0.011942408047616482 + 0.01 * 6.5216851234436035
Epoch 600, val loss: 1.2757608890533447
Epoch 610, training loss: 0.07645049691200256 = 0.011272726580500603 + 0.01 * 6.517776966094971
Epoch 610, val loss: 1.2887043952941895
Epoch 620, training loss: 0.07584144175052643 = 0.010659035295248032 + 0.01 * 6.5182414054870605
Epoch 620, val loss: 1.3013123273849487
Epoch 630, training loss: 0.07524669170379639 = 0.010094337165355682 + 0.01 * 6.515235900878906
Epoch 630, val loss: 1.3135790824890137
Epoch 640, training loss: 0.07470196485519409 = 0.0095737399533391 + 0.01 * 6.512822151184082
Epoch 640, val loss: 1.3256056308746338
Epoch 650, training loss: 0.07421313971281052 = 0.009093187749385834 + 0.01 * 6.511995315551758
Epoch 650, val loss: 1.3373088836669922
Epoch 660, training loss: 0.0737849622964859 = 0.008649542927742004 + 0.01 * 6.513542175292969
Epoch 660, val loss: 1.348697304725647
Epoch 670, training loss: 0.07331591844558716 = 0.008238711394369602 + 0.01 * 6.507720947265625
Epoch 670, val loss: 1.3598315715789795
Epoch 680, training loss: 0.07293474674224854 = 0.007857543416321278 + 0.01 * 6.507720470428467
Epoch 680, val loss: 1.3706958293914795
Epoch 690, training loss: 0.07256899774074554 = 0.007503334898501635 + 0.01 * 6.506566047668457
Epoch 690, val loss: 1.3812501430511475
Epoch 700, training loss: 0.0721864104270935 = 0.0071735018864274025 + 0.01 * 6.501291275024414
Epoch 700, val loss: 1.3916288614273071
Epoch 710, training loss: 0.0719798281788826 = 0.006865834351629019 + 0.01 * 6.511399269104004
Epoch 710, val loss: 1.4017618894577026
Epoch 720, training loss: 0.07161056250333786 = 0.006578743923455477 + 0.01 * 6.5031819343566895
Epoch 720, val loss: 1.4115809202194214
Epoch 730, training loss: 0.07129216194152832 = 0.006310465279966593 + 0.01 * 6.498170375823975
Epoch 730, val loss: 1.42124342918396
Epoch 740, training loss: 0.07103944569826126 = 0.006059120874851942 + 0.01 * 6.498033046722412
Epoch 740, val loss: 1.4306690692901611
Epoch 750, training loss: 0.07080478966236115 = 0.005823738407343626 + 0.01 * 6.498105525970459
Epoch 750, val loss: 1.439880609512329
Epoch 760, training loss: 0.07054873555898666 = 0.005603243131190538 + 0.01 * 6.494548797607422
Epoch 760, val loss: 1.4488824605941772
Epoch 770, training loss: 0.07034622877836227 = 0.005395892076194286 + 0.01 * 6.4950337409973145
Epoch 770, val loss: 1.457676887512207
Epoch 780, training loss: 0.07011036574840546 = 0.005200807470828295 + 0.01 * 6.490955352783203
Epoch 780, val loss: 1.4662506580352783
Epoch 790, training loss: 0.06991881132125854 = 0.0050170221365988255 + 0.01 * 6.490179061889648
Epoch 790, val loss: 1.4746952056884766
Epoch 800, training loss: 0.0697457492351532 = 0.004843882285058498 + 0.01 * 6.49018669128418
Epoch 800, val loss: 1.4829691648483276
Epoch 810, training loss: 0.0695086345076561 = 0.004680330399423838 + 0.01 * 6.48283052444458
Epoch 810, val loss: 1.491014838218689
Epoch 820, training loss: 0.06942779570817947 = 0.004525680560618639 + 0.01 * 6.490211486816406
Epoch 820, val loss: 1.4988911151885986
Epoch 830, training loss: 0.06920656561851501 = 0.004379610996693373 + 0.01 * 6.482695579528809
Epoch 830, val loss: 1.5066567659378052
Epoch 840, training loss: 0.06920100748538971 = 0.0042413100600242615 + 0.01 * 6.495970249176025
Epoch 840, val loss: 1.514194369316101
Epoch 850, training loss: 0.06893931329250336 = 0.004110760521143675 + 0.01 * 6.482855319976807
Epoch 850, val loss: 1.521636724472046
Epoch 860, training loss: 0.06878741830587387 = 0.003987046889960766 + 0.01 * 6.480037689208984
Epoch 860, val loss: 1.5287977457046509
Epoch 870, training loss: 0.06861180812120438 = 0.003869584994390607 + 0.01 * 6.474222660064697
Epoch 870, val loss: 1.535912036895752
Epoch 880, training loss: 0.06856129318475723 = 0.0037577964831143618 + 0.01 * 6.480350017547607
Epoch 880, val loss: 1.542910099029541
Epoch 890, training loss: 0.06837540864944458 = 0.0036515947431325912 + 0.01 * 6.472381114959717
Epoch 890, val loss: 1.5496721267700195
Epoch 900, training loss: 0.06830152869224548 = 0.0035504966508597136 + 0.01 * 6.47510290145874
Epoch 900, val loss: 1.556336522102356
Epoch 910, training loss: 0.06816383451223373 = 0.0034542856737971306 + 0.01 * 6.4709553718566895
Epoch 910, val loss: 1.5628914833068848
Epoch 920, training loss: 0.06811435520648956 = 0.0033625978976488113 + 0.01 * 6.475175857543945
Epoch 920, val loss: 1.569316029548645
Epoch 930, training loss: 0.0679466649889946 = 0.0032751457765698433 + 0.01 * 6.467151641845703
Epoch 930, val loss: 1.575523018836975
Epoch 940, training loss: 0.06784100085496902 = 0.0031917120795696974 + 0.01 * 6.464929103851318
Epoch 940, val loss: 1.5816584825515747
Epoch 950, training loss: 0.06790535897016525 = 0.0031120390631258488 + 0.01 * 6.479331970214844
Epoch 950, val loss: 1.5876909494400024
Epoch 960, training loss: 0.06768899410963058 = 0.003035979112610221 + 0.01 * 6.465301513671875
Epoch 960, val loss: 1.5935879945755005
Epoch 970, training loss: 0.06760507822036743 = 0.0029633627273142338 + 0.01 * 6.464171886444092
Epoch 970, val loss: 1.5993391275405884
Epoch 980, training loss: 0.06747480481863022 = 0.0028938951436430216 + 0.01 * 6.458090782165527
Epoch 980, val loss: 1.6050416231155396
Epoch 990, training loss: 0.06743203848600388 = 0.00282743270508945 + 0.01 * 6.460460662841797
Epoch 990, val loss: 1.61061429977417
Epoch 1000, training loss: 0.06732916086912155 = 0.002763658994808793 + 0.01 * 6.456550121307373
Epoch 1000, val loss: 1.6160918474197388
Epoch 1010, training loss: 0.06738013029098511 = 0.002702426165342331 + 0.01 * 6.467770099639893
Epoch 1010, val loss: 1.621451735496521
Epoch 1020, training loss: 0.06724157184362411 = 0.002643818035721779 + 0.01 * 6.459775447845459
Epoch 1020, val loss: 1.6267924308776855
Epoch 1030, training loss: 0.06713233143091202 = 0.0025875833816826344 + 0.01 * 6.454474925994873
Epoch 1030, val loss: 1.6319459676742554
Epoch 1040, training loss: 0.06716342270374298 = 0.002533519174903631 + 0.01 * 6.462990760803223
Epoch 1040, val loss: 1.6370699405670166
Epoch 1050, training loss: 0.06702901422977448 = 0.0024816703516989946 + 0.01 * 6.454734802246094
Epoch 1050, val loss: 1.6420644521713257
Epoch 1060, training loss: 0.06695985049009323 = 0.002431982895359397 + 0.01 * 6.452786922454834
Epoch 1060, val loss: 1.6469019651412964
Epoch 1070, training loss: 0.0668361485004425 = 0.002384219318628311 + 0.01 * 6.445192337036133
Epoch 1070, val loss: 1.6517637968063354
Epoch 1080, training loss: 0.06688631325960159 = 0.0023381656501442194 + 0.01 * 6.454814910888672
Epoch 1080, val loss: 1.6565709114074707
Epoch 1090, training loss: 0.06677652895450592 = 0.00229382305406034 + 0.01 * 6.448270320892334
Epoch 1090, val loss: 1.6611526012420654
Epoch 1100, training loss: 0.06670880317687988 = 0.002251196186989546 + 0.01 * 6.445761203765869
Epoch 1100, val loss: 1.6657769680023193
Epoch 1110, training loss: 0.06659238040447235 = 0.0022100708447396755 + 0.01 * 6.438230991363525
Epoch 1110, val loss: 1.6703011989593506
Epoch 1120, training loss: 0.06656378507614136 = 0.0021704791579395533 + 0.01 * 6.439330577850342
Epoch 1120, val loss: 1.674768328666687
Epoch 1130, training loss: 0.06650421023368835 = 0.0021324537228792906 + 0.01 * 6.43717622756958
Epoch 1130, val loss: 1.6791062355041504
Epoch 1140, training loss: 0.06664835661649704 = 0.002095657167956233 + 0.01 * 6.455270290374756
Epoch 1140, val loss: 1.683369517326355
Epoch 1150, training loss: 0.06647000461816788 = 0.00206029717810452 + 0.01 * 6.440971374511719
Epoch 1150, val loss: 1.6876944303512573
Epoch 1160, training loss: 0.0663207396864891 = 0.002026060363277793 + 0.01 * 6.429468154907227
Epoch 1160, val loss: 1.6917988061904907
Epoch 1170, training loss: 0.06639920175075531 = 0.001992960460484028 + 0.01 * 6.440624237060547
Epoch 1170, val loss: 1.6959033012390137
Epoch 1180, training loss: 0.06628809124231339 = 0.001961044268682599 + 0.01 * 6.432704448699951
Epoch 1180, val loss: 1.7000060081481934
Epoch 1190, training loss: 0.06632674485445023 = 0.001930123777128756 + 0.01 * 6.439661979675293
Epoch 1190, val loss: 1.7039344310760498
Epoch 1200, training loss: 0.06618037819862366 = 0.0019002641784027219 + 0.01 * 6.428011417388916
Epoch 1200, val loss: 1.7079437971115112
Epoch 1210, training loss: 0.06623835861682892 = 0.0018714312463998795 + 0.01 * 6.436692714691162
Epoch 1210, val loss: 1.7117091417312622
Epoch 1220, training loss: 0.06607913225889206 = 0.0018435160163789988 + 0.01 * 6.423561096191406
Epoch 1220, val loss: 1.715531349182129
Epoch 1230, training loss: 0.06601568311452866 = 0.0018165643559768796 + 0.01 * 6.419912338256836
Epoch 1230, val loss: 1.719234824180603
Epoch 1240, training loss: 0.06603759527206421 = 0.0017904910491779447 + 0.01 * 6.424710273742676
Epoch 1240, val loss: 1.7229329347610474
Epoch 1250, training loss: 0.06603369116783142 = 0.0017652076203376055 + 0.01 * 6.426848888397217
Epoch 1250, val loss: 1.726594090461731
Epoch 1260, training loss: 0.06606769561767578 = 0.0017407044069841504 + 0.01 * 6.432699680328369
Epoch 1260, val loss: 1.7301641702651978
Epoch 1270, training loss: 0.06578536331653595 = 0.0017170306527987123 + 0.01 * 6.406833648681641
Epoch 1270, val loss: 1.7336417436599731
Epoch 1280, training loss: 0.06605962663888931 = 0.0016941206995397806 + 0.01 * 6.436550617218018
Epoch 1280, val loss: 1.737123966217041
Epoch 1290, training loss: 0.06569928675889969 = 0.0016719078412279487 + 0.01 * 6.402737617492676
Epoch 1290, val loss: 1.7404868602752686
Epoch 1300, training loss: 0.06605338305234909 = 0.0016503658844158053 + 0.01 * 6.440301895141602
Epoch 1300, val loss: 1.74373197555542
Epoch 1310, training loss: 0.06571877747774124 = 0.0016295398818328977 + 0.01 * 6.408924102783203
Epoch 1310, val loss: 1.7471623420715332
Epoch 1320, training loss: 0.06574653089046478 = 0.0016093256417661905 + 0.01 * 6.413720607757568
Epoch 1320, val loss: 1.750343680381775
Epoch 1330, training loss: 0.06558466702699661 = 0.0015896856784820557 + 0.01 * 6.399498462677002
Epoch 1330, val loss: 1.7535829544067383
Epoch 1340, training loss: 0.06552650034427643 = 0.0015706280246376991 + 0.01 * 6.39558744430542
Epoch 1340, val loss: 1.7567685842514038
Epoch 1350, training loss: 0.06550446152687073 = 0.0015520822489634156 + 0.01 * 6.395238399505615
Epoch 1350, val loss: 1.7598493099212646
Epoch 1360, training loss: 0.06560986489057541 = 0.001534141250886023 + 0.01 * 6.4075727462768555
Epoch 1360, val loss: 1.7629761695861816
Epoch 1370, training loss: 0.0654292181134224 = 0.0015167234232649207 + 0.01 * 6.391249656677246
Epoch 1370, val loss: 1.7659873962402344
Epoch 1380, training loss: 0.06541351228952408 = 0.00149987218901515 + 0.01 * 6.391364574432373
Epoch 1380, val loss: 1.7688404321670532
Epoch 1390, training loss: 0.06534572690725327 = 0.0014834521571174264 + 0.01 * 6.386227607727051
Epoch 1390, val loss: 1.7717479467391968
Epoch 1400, training loss: 0.06552120298147202 = 0.001467568101361394 + 0.01 * 6.4053635597229
Epoch 1400, val loss: 1.7745544910430908
Epoch 1410, training loss: 0.06525669246912003 = 0.0014520385302603245 + 0.01 * 6.380465507507324
Epoch 1410, val loss: 1.7773574590682983
Epoch 1420, training loss: 0.06540364772081375 = 0.0014369725249707699 + 0.01 * 6.396667957305908
Epoch 1420, val loss: 1.7800456285476685
Epoch 1430, training loss: 0.06520939618349075 = 0.0014223302714526653 + 0.01 * 6.378706455230713
Epoch 1430, val loss: 1.7829310894012451
Epoch 1440, training loss: 0.06503082811832428 = 0.0014080560067668557 + 0.01 * 6.362277507781982
Epoch 1440, val loss: 1.7855093479156494
Epoch 1450, training loss: 0.06519891321659088 = 0.0013942051446065307 + 0.01 * 6.380471229553223
Epoch 1450, val loss: 1.7882094383239746
Epoch 1460, training loss: 0.06509928405284882 = 0.0013807406648993492 + 0.01 * 6.371854305267334
Epoch 1460, val loss: 1.790758490562439
Epoch 1470, training loss: 0.0652049332857132 = 0.0013676383532583714 + 0.01 * 6.383729934692383
Epoch 1470, val loss: 1.7932960987091064
Epoch 1480, training loss: 0.06512187421321869 = 0.001354915089905262 + 0.01 * 6.3766961097717285
Epoch 1480, val loss: 1.7958005666732788
Epoch 1490, training loss: 0.06480886042118073 = 0.0013425066135823727 + 0.01 * 6.346635818481445
Epoch 1490, val loss: 1.7983394861221313
Epoch 1500, training loss: 0.06501413136720657 = 0.0013303678715601563 + 0.01 * 6.3683762550354
Epoch 1500, val loss: 1.8008310794830322
Epoch 1510, training loss: 0.06507900357246399 = 0.0013185024727135897 + 0.01 * 6.3760504722595215
Epoch 1510, val loss: 1.8033024072647095
Epoch 1520, training loss: 0.0647275373339653 = 0.0013070548884570599 + 0.01 * 6.342048645019531
Epoch 1520, val loss: 1.8057278394699097
Epoch 1530, training loss: 0.06484168022871017 = 0.0012958250008523464 + 0.01 * 6.354585647583008
Epoch 1530, val loss: 1.8081295490264893
Epoch 1540, training loss: 0.06492635607719421 = 0.0012849596096202731 + 0.01 * 6.364140033721924
Epoch 1540, val loss: 1.8104653358459473
Epoch 1550, training loss: 0.06462344527244568 = 0.0012742667458951473 + 0.01 * 6.334918022155762
Epoch 1550, val loss: 1.812814712524414
Epoch 1560, training loss: 0.06481194496154785 = 0.0012638559564948082 + 0.01 * 6.354808807373047
Epoch 1560, val loss: 1.8151589632034302
Epoch 1570, training loss: 0.0646233856678009 = 0.001253694063052535 + 0.01 * 6.336969375610352
Epoch 1570, val loss: 1.8174610137939453
Epoch 1580, training loss: 0.06453658640384674 = 0.0012437356635928154 + 0.01 * 6.329285621643066
Epoch 1580, val loss: 1.8197664022445679
Epoch 1590, training loss: 0.0648210272192955 = 0.0012340766843408346 + 0.01 * 6.358695030212402
Epoch 1590, val loss: 1.822020411491394
Epoch 1600, training loss: 0.06469528377056122 = 0.0012245599646121264 + 0.01 * 6.347072124481201
Epoch 1600, val loss: 1.8241833448410034
Epoch 1610, training loss: 0.06487563252449036 = 0.0012153424322605133 + 0.01 * 6.366029262542725
Epoch 1610, val loss: 1.8263074159622192
Epoch 1620, training loss: 0.06451992690563202 = 0.0012063178000971675 + 0.01 * 6.331361293792725
Epoch 1620, val loss: 1.8285794258117676
Epoch 1630, training loss: 0.06438446044921875 = 0.0011974448570981622 + 0.01 * 6.318702220916748
Epoch 1630, val loss: 1.8306870460510254
Epoch 1640, training loss: 0.06439321488142014 = 0.0011887908913195133 + 0.01 * 6.320442199707031
Epoch 1640, val loss: 1.8327734470367432
Epoch 1650, training loss: 0.06441889703273773 = 0.0011803372763097286 + 0.01 * 6.323856353759766
Epoch 1650, val loss: 1.8348639011383057
Epoch 1660, training loss: 0.06427766382694244 = 0.0011720493203029037 + 0.01 * 6.310561180114746
Epoch 1660, val loss: 1.8369187116622925
Epoch 1670, training loss: 0.06440339237451553 = 0.001163944718427956 + 0.01 * 6.323944568634033
Epoch 1670, val loss: 1.8390069007873535
Epoch 1680, training loss: 0.06436081975698471 = 0.0011559318518266082 + 0.01 * 6.320488929748535
Epoch 1680, val loss: 1.8409298658370972
Epoch 1690, training loss: 0.06423953920602798 = 0.0011481709079816937 + 0.01 * 6.309136867523193
Epoch 1690, val loss: 1.8429867029190063
Epoch 1700, training loss: 0.06443411111831665 = 0.0011405377881601453 + 0.01 * 6.329357624053955
Epoch 1700, val loss: 1.8449803590774536
Epoch 1710, training loss: 0.06451595574617386 = 0.0011330383131280541 + 0.01 * 6.338291645050049
Epoch 1710, val loss: 1.8468421697616577
Epoch 1720, training loss: 0.06416946649551392 = 0.0011256902944296598 + 0.01 * 6.304377555847168
Epoch 1720, val loss: 1.8488194942474365
Epoch 1730, training loss: 0.06412134319543839 = 0.0011185264447703958 + 0.01 * 6.300282001495361
Epoch 1730, val loss: 1.8507112264633179
Epoch 1740, training loss: 0.06416092067956924 = 0.0011114923981949687 + 0.01 * 6.304942607879639
Epoch 1740, val loss: 1.8525563478469849
Epoch 1750, training loss: 0.06409993767738342 = 0.0011045694118365645 + 0.01 * 6.29953670501709
Epoch 1750, val loss: 1.8544573783874512
Epoch 1760, training loss: 0.06407683342695236 = 0.0010978028876706958 + 0.01 * 6.297902584075928
Epoch 1760, val loss: 1.85628080368042
Epoch 1770, training loss: 0.06393939256668091 = 0.001091141253709793 + 0.01 * 6.284825325012207
Epoch 1770, val loss: 1.8581573963165283
Epoch 1780, training loss: 0.06410270929336548 = 0.0010845737997442484 + 0.01 * 6.301814079284668
Epoch 1780, val loss: 1.8598518371582031
Epoch 1790, training loss: 0.06398420035839081 = 0.001078233472071588 + 0.01 * 6.290596961975098
Epoch 1790, val loss: 1.8617136478424072
Epoch 1800, training loss: 0.06383711099624634 = 0.0010719532147049904 + 0.01 * 6.276515960693359
Epoch 1800, val loss: 1.8634357452392578
Epoch 1810, training loss: 0.06427133828401566 = 0.001065777731128037 + 0.01 * 6.320556640625
Epoch 1810, val loss: 1.8651484251022339
Epoch 1820, training loss: 0.06384740769863129 = 0.0010597269283607602 + 0.01 * 6.278768062591553
Epoch 1820, val loss: 1.866982102394104
Epoch 1830, training loss: 0.0640665739774704 = 0.0010537815978750587 + 0.01 * 6.301279544830322
Epoch 1830, val loss: 1.8686463832855225
Epoch 1840, training loss: 0.06414377689361572 = 0.0010478849289938807 + 0.01 * 6.309589385986328
Epoch 1840, val loss: 1.870375156402588
Epoch 1850, training loss: 0.0636788010597229 = 0.0010421609040349722 + 0.01 * 6.263664722442627
Epoch 1850, val loss: 1.8719755411148071
Epoch 1860, training loss: 0.06390997022390366 = 0.001036542933434248 + 0.01 * 6.2873430252075195
Epoch 1860, val loss: 1.8735661506652832
Epoch 1870, training loss: 0.06362183392047882 = 0.0010309872450307012 + 0.01 * 6.259084701538086
Epoch 1870, val loss: 1.8752086162567139
Epoch 1880, training loss: 0.0640440285205841 = 0.0010256007080897689 + 0.01 * 6.30184268951416
Epoch 1880, val loss: 1.8768606185913086
Epoch 1890, training loss: 0.06362858414649963 = 0.001020194380544126 + 0.01 * 6.260839462280273
Epoch 1890, val loss: 1.8784472942352295
Epoch 1900, training loss: 0.06390853971242905 = 0.001014973153360188 + 0.01 * 6.2893571853637695
Epoch 1900, val loss: 1.8800946474075317
Epoch 1910, training loss: 0.06378837674856186 = 0.001009751227684319 + 0.01 * 6.277862071990967
Epoch 1910, val loss: 1.8816295862197876
Epoch 1920, training loss: 0.06364023685455322 = 0.0010046634124591947 + 0.01 * 6.263557434082031
Epoch 1920, val loss: 1.8832180500030518
Epoch 1930, training loss: 0.06364774703979492 = 0.0009996488224714994 + 0.01 * 6.264809608459473
Epoch 1930, val loss: 1.8848179578781128
Epoch 1940, training loss: 0.06348984688520432 = 0.000994723872281611 + 0.01 * 6.249512672424316
Epoch 1940, val loss: 1.8863322734832764
Epoch 1950, training loss: 0.06374085694551468 = 0.0009898970602080226 + 0.01 * 6.2750959396362305
Epoch 1950, val loss: 1.8878756761550903
Epoch 1960, training loss: 0.06356725841760635 = 0.000985042774118483 + 0.01 * 6.258221626281738
Epoch 1960, val loss: 1.8893765211105347
Epoch 1970, training loss: 0.06337548792362213 = 0.0009803822031244636 + 0.01 * 6.239510536193848
Epoch 1970, val loss: 1.8909574747085571
Epoch 1980, training loss: 0.06342969089746475 = 0.0009756863582879305 + 0.01 * 6.245400428771973
Epoch 1980, val loss: 1.8924026489257812
Epoch 1990, training loss: 0.06338341534137726 = 0.0009711404563859105 + 0.01 * 6.241227626800537
Epoch 1990, val loss: 1.8939769268035889
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.5092
Flip ASR: 0.4089/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0514943599700928 = 1.9677573442459106 + 0.01 * 8.37369155883789
Epoch 0, val loss: 1.9642924070358276
Epoch 10, training loss: 2.0403990745544434 = 1.9566657543182373 + 0.01 * 8.373327255249023
Epoch 10, val loss: 1.9532018899917603
Epoch 20, training loss: 2.0271155834198 = 1.9433982372283936 + 0.01 * 8.371737480163574
Epoch 20, val loss: 1.9389820098876953
Epoch 30, training loss: 2.008600950241089 = 1.9249390363693237 + 0.01 * 8.366192817687988
Epoch 30, val loss: 1.918196439743042
Epoch 40, training loss: 1.981611967086792 = 1.898145079612732 + 0.01 * 8.34669017791748
Epoch 40, val loss: 1.887869954109192
Epoch 50, training loss: 1.9430267810821533 = 1.8606438636779785 + 0.01 * 8.238292694091797
Epoch 50, val loss: 1.8471260070800781
Epoch 60, training loss: 1.8898875713348389 = 1.8131295442581177 + 0.01 * 7.6757988929748535
Epoch 60, val loss: 1.79933762550354
Epoch 70, training loss: 1.8362693786621094 = 1.7624177932739258 + 0.01 * 7.385158538818359
Epoch 70, val loss: 1.752565860748291
Epoch 80, training loss: 1.7868547439575195 = 1.714164137840271 + 0.01 * 7.26906681060791
Epoch 80, val loss: 1.7095160484313965
Epoch 90, training loss: 1.7267568111419678 = 1.654920220375061 + 0.01 * 7.18366003036499
Epoch 90, val loss: 1.6555230617523193
Epoch 100, training loss: 1.6462095975875854 = 1.5755568742752075 + 0.01 * 7.065273284912109
Epoch 100, val loss: 1.5871824026107788
Epoch 110, training loss: 1.5469577312469482 = 1.477798342704773 + 0.01 * 6.915940761566162
Epoch 110, val loss: 1.5069187879562378
Epoch 120, training loss: 1.4415596723556519 = 1.3731588125228882 + 0.01 * 6.840088844299316
Epoch 120, val loss: 1.4231359958648682
Epoch 130, training loss: 1.3433200120925903 = 1.2751715183258057 + 0.01 * 6.814849376678467
Epoch 130, val loss: 1.3484420776367188
Epoch 140, training loss: 1.2568713426589966 = 1.188800573348999 + 0.01 * 6.807074069976807
Epoch 140, val loss: 1.2856894731521606
Epoch 150, training loss: 1.1788122653961182 = 1.1107569932937622 + 0.01 * 6.805531024932861
Epoch 150, val loss: 1.2294960021972656
Epoch 160, training loss: 1.102871060371399 = 1.034845232963562 + 0.01 * 6.802585601806641
Epoch 160, val loss: 1.174537181854248
Epoch 170, training loss: 1.0248539447784424 = 0.9568799138069153 + 0.01 * 6.797403335571289
Epoch 170, val loss: 1.1170542240142822
Epoch 180, training loss: 0.944343626499176 = 0.8764341473579407 + 0.01 * 6.790949821472168
Epoch 180, val loss: 1.0565522909164429
Epoch 190, training loss: 0.8637354969978333 = 0.7959030270576477 + 0.01 * 6.783245086669922
Epoch 190, val loss: 0.9957221150398254
Epoch 200, training loss: 0.7865985631942749 = 0.7188484072685242 + 0.01 * 6.775017261505127
Epoch 200, val loss: 0.9383370280265808
Epoch 210, training loss: 0.715566873550415 = 0.6478985548019409 + 0.01 * 6.7668304443359375
Epoch 210, val loss: 0.8874147534370422
Epoch 220, training loss: 0.6515401601791382 = 0.583949625492096 + 0.01 * 6.759056091308594
Epoch 220, val loss: 0.8444628715515137
Epoch 230, training loss: 0.5940101146697998 = 0.5264912843704224 + 0.01 * 6.751886367797852
Epoch 230, val loss: 0.8089048862457275
Epoch 240, training loss: 0.5418989062309265 = 0.4744453430175781 + 0.01 * 6.745357990264893
Epoch 240, val loss: 0.7796909213066101
Epoch 250, training loss: 0.49401527643203735 = 0.4266224503517151 + 0.01 * 6.739283084869385
Epoch 250, val loss: 0.7560037970542908
Epoch 260, training loss: 0.4493308961391449 = 0.38199466466903687 + 0.01 * 6.733623027801514
Epoch 260, val loss: 0.7371395230293274
Epoch 270, training loss: 0.40724071860313416 = 0.33995652198791504 + 0.01 * 6.728420257568359
Epoch 270, val loss: 0.7230474352836609
Epoch 280, training loss: 0.3677595257759094 = 0.3005228638648987 + 0.01 * 6.723667144775391
Epoch 280, val loss: 0.7133588790893555
Epoch 290, training loss: 0.33139508962631226 = 0.26421019434928894 + 0.01 * 6.7184882164001465
Epoch 290, val loss: 0.7079041600227356
Epoch 300, training loss: 0.298784077167511 = 0.23165136575698853 + 0.01 * 6.713272571563721
Epoch 300, val loss: 0.7066080570220947
Epoch 310, training loss: 0.27033132314682007 = 0.20325657725334167 + 0.01 * 6.7074761390686035
Epoch 310, val loss: 0.7092183828353882
Epoch 320, training loss: 0.24606853723526 = 0.17905305325984955 + 0.01 * 6.7015485763549805
Epoch 320, val loss: 0.7152648568153381
Epoch 330, training loss: 0.22556176781654358 = 0.15861381590366364 + 0.01 * 6.694796085357666
Epoch 330, val loss: 0.7242197394371033
Epoch 340, training loss: 0.20814737677574158 = 0.1412688046693802 + 0.01 * 6.687857151031494
Epoch 340, val loss: 0.7354969382286072
Epoch 350, training loss: 0.19324776530265808 = 0.12644770741462708 + 0.01 * 6.6800055503845215
Epoch 350, val loss: 0.7486861944198608
Epoch 360, training loss: 0.1803402304649353 = 0.1136246919631958 + 0.01 * 6.6715545654296875
Epoch 360, val loss: 0.7630941867828369
Epoch 370, training loss: 0.16905000805854797 = 0.10238014161586761 + 0.01 * 6.666987895965576
Epoch 370, val loss: 0.778287947177887
Epoch 380, training loss: 0.15895043313503265 = 0.09240145236253738 + 0.01 * 6.654898166656494
Epoch 380, val loss: 0.7941480875015259
Epoch 390, training loss: 0.14994867146015167 = 0.08348652720451355 + 0.01 * 6.646214485168457
Epoch 390, val loss: 0.8103955388069153
Epoch 400, training loss: 0.14194133877754211 = 0.07548853754997253 + 0.01 * 6.645280838012695
Epoch 400, val loss: 0.8270535469055176
Epoch 410, training loss: 0.1346246749162674 = 0.06829331815242767 + 0.01 * 6.633135795593262
Epoch 410, val loss: 0.8439854383468628
Epoch 420, training loss: 0.1280859112739563 = 0.061827462166547775 + 0.01 * 6.625844478607178
Epoch 420, val loss: 0.8609198331832886
Epoch 430, training loss: 0.12218163162469864 = 0.055973805487155914 + 0.01 * 6.620782852172852
Epoch 430, val loss: 0.8778055310249329
Epoch 440, training loss: 0.11698740720748901 = 0.05076728016138077 + 0.01 * 6.622013092041016
Epoch 440, val loss: 0.8945519924163818
Epoch 450, training loss: 0.11225517094135284 = 0.04614226147532463 + 0.01 * 6.611290454864502
Epoch 450, val loss: 0.9110642671585083
Epoch 460, training loss: 0.10810647904872894 = 0.042033351957798004 + 0.01 * 6.6073126792907715
Epoch 460, val loss: 0.9273278713226318
Epoch 470, training loss: 0.10444147139787674 = 0.038375191390514374 + 0.01 * 6.60662841796875
Epoch 470, val loss: 0.9434411525726318
Epoch 480, training loss: 0.10113611817359924 = 0.035111166536808014 + 0.01 * 6.6024956703186035
Epoch 480, val loss: 0.95908123254776
Epoch 490, training loss: 0.09818723797798157 = 0.03219352290034294 + 0.01 * 6.599372386932373
Epoch 490, val loss: 0.9744044542312622
Epoch 500, training loss: 0.09550143778324127 = 0.029573339968919754 + 0.01 * 6.592810153961182
Epoch 500, val loss: 0.9893783926963806
Epoch 510, training loss: 0.09313451498746872 = 0.0272273737937212 + 0.01 * 6.590714454650879
Epoch 510, val loss: 1.0039231777191162
Epoch 520, training loss: 0.09099359810352325 = 0.025126520544290543 + 0.01 * 6.586708068847656
Epoch 520, val loss: 1.0180147886276245
Epoch 530, training loss: 0.08903070539236069 = 0.023237930610775948 + 0.01 * 6.579277992248535
Epoch 530, val loss: 1.0317076444625854
Epoch 540, training loss: 0.08730674535036087 = 0.02153780497610569 + 0.01 * 6.576894283294678
Epoch 540, val loss: 1.0449360609054565
Epoch 550, training loss: 0.08577211946249008 = 0.020004482939839363 + 0.01 * 6.57676362991333
Epoch 550, val loss: 1.0577309131622314
Epoch 560, training loss: 0.08428007364273071 = 0.01861809752881527 + 0.01 * 6.566197872161865
Epoch 560, val loss: 1.0701740980148315
Epoch 570, training loss: 0.08296032249927521 = 0.01735822483897209 + 0.01 * 6.560209274291992
Epoch 570, val loss: 1.0822454690933228
Epoch 580, training loss: 0.0818130150437355 = 0.016214832663536072 + 0.01 * 6.559818744659424
Epoch 580, val loss: 1.0939526557922363
Epoch 590, training loss: 0.08082299679517746 = 0.015181728638708591 + 0.01 * 6.564126968383789
Epoch 590, val loss: 1.1052141189575195
Epoch 600, training loss: 0.07973984628915787 = 0.014242403209209442 + 0.01 * 6.549744129180908
Epoch 600, val loss: 1.116187572479248
Epoch 610, training loss: 0.07888051867485046 = 0.013386149890720844 + 0.01 * 6.549437046051025
Epoch 610, val loss: 1.1267385482788086
Epoch 620, training loss: 0.07805155962705612 = 0.01260399166494608 + 0.01 * 6.544756889343262
Epoch 620, val loss: 1.137093186378479
Epoch 630, training loss: 0.07730203866958618 = 0.011888709850609303 + 0.01 * 6.541332721710205
Epoch 630, val loss: 1.146997332572937
Epoch 640, training loss: 0.07659576833248138 = 0.011231696233153343 + 0.01 * 6.536407470703125
Epoch 640, val loss: 1.1566652059555054
Epoch 650, training loss: 0.07596889138221741 = 0.010627778246998787 + 0.01 * 6.534111976623535
Epoch 650, val loss: 1.1660401821136475
Epoch 660, training loss: 0.07538726925849915 = 0.010072138160467148 + 0.01 * 6.531513214111328
Epoch 660, val loss: 1.1749663352966309
Epoch 670, training loss: 0.0747971460223198 = 0.0095593873411417 + 0.01 * 6.523776531219482
Epoch 670, val loss: 1.1837340593338013
Epoch 680, training loss: 0.07428494095802307 = 0.009085096418857574 + 0.01 * 6.519984722137451
Epoch 680, val loss: 1.1922242641448975
Epoch 690, training loss: 0.07382835447788239 = 0.00864467304199934 + 0.01 * 6.518368244171143
Epoch 690, val loss: 1.2005047798156738
Epoch 700, training loss: 0.07342793047428131 = 0.008236778900027275 + 0.01 * 6.519115447998047
Epoch 700, val loss: 1.2083780765533447
Epoch 710, training loss: 0.07298312336206436 = 0.007857532240450382 + 0.01 * 6.512559413909912
Epoch 710, val loss: 1.2161164283752441
Epoch 720, training loss: 0.07262498140335083 = 0.0075049009174108505 + 0.01 * 6.512007713317871
Epoch 720, val loss: 1.2236498594284058
Epoch 730, training loss: 0.07232889533042908 = 0.007176271639764309 + 0.01 * 6.515262126922607
Epoch 730, val loss: 1.230906367301941
Epoch 740, training loss: 0.072066530585289 = 0.006870425771921873 + 0.01 * 6.519610404968262
Epoch 740, val loss: 1.2380039691925049
Epoch 750, training loss: 0.07158594578504562 = 0.006584629882127047 + 0.01 * 6.500132083892822
Epoch 750, val loss: 1.2447837591171265
Epoch 760, training loss: 0.07129073888063431 = 0.006315413396805525 + 0.01 * 6.497532844543457
Epoch 760, val loss: 1.2512638568878174
Epoch 770, training loss: 0.07116242498159409 = 0.006047657225281 + 0.01 * 6.511476993560791
Epoch 770, val loss: 1.2576228380203247
Epoch 780, training loss: 0.07084903120994568 = 0.00580515107139945 + 0.01 * 6.504388332366943
Epoch 780, val loss: 1.2639062404632568
Epoch 790, training loss: 0.07053350657224655 = 0.005580664146691561 + 0.01 * 6.495284080505371
Epoch 790, val loss: 1.2700611352920532
Epoch 800, training loss: 0.07027348130941391 = 0.00537146208807826 + 0.01 * 6.4902024269104
Epoch 800, val loss: 1.275985836982727
Epoch 810, training loss: 0.07004830241203308 = 0.005174463149160147 + 0.01 * 6.487383842468262
Epoch 810, val loss: 1.2819918394088745
Epoch 820, training loss: 0.07005640119314194 = 0.00498941307887435 + 0.01 * 6.506699085235596
Epoch 820, val loss: 1.2877057790756226
Epoch 830, training loss: 0.06965306401252747 = 0.004816033877432346 + 0.01 * 6.483703136444092
Epoch 830, val loss: 1.2932381629943848
Epoch 840, training loss: 0.0694969892501831 = 0.004652549512684345 + 0.01 * 6.4844441413879395
Epoch 840, val loss: 1.2987112998962402
Epoch 850, training loss: 0.06923450529575348 = 0.004498619586229324 + 0.01 * 6.473587989807129
Epoch 850, val loss: 1.3040060997009277
Epoch 860, training loss: 0.06914999336004257 = 0.004353232681751251 + 0.01 * 6.479676246643066
Epoch 860, val loss: 1.309200644493103
Epoch 870, training loss: 0.06892096251249313 = 0.0042153093963861465 + 0.01 * 6.470565319061279
Epoch 870, val loss: 1.3142648935317993
Epoch 880, training loss: 0.06871539354324341 = 0.004084333777427673 + 0.01 * 6.463106155395508
Epoch 880, val loss: 1.3192566633224487
Epoch 890, training loss: 0.06876752525568008 = 0.003962041810154915 + 0.01 * 6.48054838180542
Epoch 890, val loss: 1.3240419626235962
Epoch 900, training loss: 0.06840013712644577 = 0.003844538005068898 + 0.01 * 6.455560207366943
Epoch 900, val loss: 1.328682780265808
Epoch 910, training loss: 0.06832315027713776 = 0.0037332235369831324 + 0.01 * 6.458992958068848
Epoch 910, val loss: 1.333359956741333
Epoch 920, training loss: 0.06814475357532501 = 0.003628007136285305 + 0.01 * 6.451674461364746
Epoch 920, val loss: 1.3379075527191162
Epoch 930, training loss: 0.06801190227270126 = 0.003527748165652156 + 0.01 * 6.448415756225586
Epoch 930, val loss: 1.342289924621582
Epoch 940, training loss: 0.0678689181804657 = 0.0034322505816817284 + 0.01 * 6.443666458129883
Epoch 940, val loss: 1.346580147743225
Epoch 950, training loss: 0.06783819943666458 = 0.003341681556776166 + 0.01 * 6.449652194976807
Epoch 950, val loss: 1.3508307933807373
Epoch 960, training loss: 0.06763938069343567 = 0.0032551982440054417 + 0.01 * 6.438418388366699
Epoch 960, val loss: 1.3548882007598877
Epoch 970, training loss: 0.06748998910188675 = 0.003172827186062932 + 0.01 * 6.431715965270996
Epoch 970, val loss: 1.3588200807571411
Epoch 980, training loss: 0.06762122362852097 = 0.0030946212355047464 + 0.01 * 6.45266056060791
Epoch 980, val loss: 1.3627442121505737
Epoch 990, training loss: 0.06753215193748474 = 0.003019321709871292 + 0.01 * 6.451282978057861
Epoch 990, val loss: 1.3665870428085327
Epoch 1000, training loss: 0.06725649535655975 = 0.002947726519778371 + 0.01 * 6.430877208709717
Epoch 1000, val loss: 1.3704051971435547
Epoch 1010, training loss: 0.06724901497364044 = 0.002879170933738351 + 0.01 * 6.436984062194824
Epoch 1010, val loss: 1.3738850355148315
Epoch 1020, training loss: 0.0670604556798935 = 0.0028136111795902252 + 0.01 * 6.424684047698975
Epoch 1020, val loss: 1.3774843215942383
Epoch 1030, training loss: 0.06691864132881165 = 0.0027509592473506927 + 0.01 * 6.416768550872803
Epoch 1030, val loss: 1.3809000253677368
Epoch 1040, training loss: 0.06699065864086151 = 0.002691261237487197 + 0.01 * 6.4299397468566895
Epoch 1040, val loss: 1.3842719793319702
Epoch 1050, training loss: 0.06676698476076126 = 0.002633886644616723 + 0.01 * 6.4133100509643555
Epoch 1050, val loss: 1.387520670890808
Epoch 1060, training loss: 0.06694233417510986 = 0.002578795189037919 + 0.01 * 6.436354160308838
Epoch 1060, val loss: 1.3905584812164307
Epoch 1070, training loss: 0.06656746566295624 = 0.0025260874535888433 + 0.01 * 6.404138088226318
Epoch 1070, val loss: 1.393789529800415
Epoch 1080, training loss: 0.06672363728284836 = 0.002475438406690955 + 0.01 * 6.424819469451904
Epoch 1080, val loss: 1.396836757659912
Epoch 1090, training loss: 0.0664355531334877 = 0.002427138155326247 + 0.01 * 6.40084171295166
Epoch 1090, val loss: 1.3997682332992554
Epoch 1100, training loss: 0.06652321666479111 = 0.0023807731922715902 + 0.01 * 6.414244174957275
Epoch 1100, val loss: 1.4026672840118408
Epoch 1110, training loss: 0.06640906631946564 = 0.0023356452584266663 + 0.01 * 6.407341957092285
Epoch 1110, val loss: 1.4053657054901123
Epoch 1120, training loss: 0.06627252697944641 = 0.0022928835824131966 + 0.01 * 6.397964000701904
Epoch 1120, val loss: 1.4082328081130981
Epoch 1130, training loss: 0.06625047326087952 = 0.0022512394934892654 + 0.01 * 6.399923801422119
Epoch 1130, val loss: 1.4108877182006836
Epoch 1140, training loss: 0.06619005650281906 = 0.002211672952398658 + 0.01 * 6.397838115692139
Epoch 1140, val loss: 1.413578987121582
Epoch 1150, training loss: 0.06617198139429092 = 0.0021728589199483395 + 0.01 * 6.3999128341674805
Epoch 1150, val loss: 1.4161361455917358
Epoch 1160, training loss: 0.06597159802913666 = 0.002135995775461197 + 0.01 * 6.383559703826904
Epoch 1160, val loss: 1.4186053276062012
Epoch 1170, training loss: 0.06625279039144516 = 0.0021001228597015142 + 0.01 * 6.415266513824463
Epoch 1170, val loss: 1.4211547374725342
Epoch 1180, training loss: 0.06591179966926575 = 0.002066256944090128 + 0.01 * 6.384554386138916
Epoch 1180, val loss: 1.4235947132110596
Epoch 1190, training loss: 0.06594415009021759 = 0.0020329197868704796 + 0.01 * 6.391123294830322
Epoch 1190, val loss: 1.4259337186813354
Epoch 1200, training loss: 0.065774105489254 = 0.0020009507425129414 + 0.01 * 6.377315521240234
Epoch 1200, val loss: 1.4282423257827759
Epoch 1210, training loss: 0.06569782644510269 = 0.001969515811651945 + 0.01 * 6.372831344604492
Epoch 1210, val loss: 1.4304932355880737
Epoch 1220, training loss: 0.0656968280673027 = 0.0019393399124965072 + 0.01 * 6.375748634338379
Epoch 1220, val loss: 1.4326832294464111
Epoch 1230, training loss: 0.065741166472435 = 0.0019101382931694388 + 0.01 * 6.383103370666504
Epoch 1230, val loss: 1.4348589181900024
Epoch 1240, training loss: 0.06537887454032898 = 0.0018818790558725595 + 0.01 * 6.349699020385742
Epoch 1240, val loss: 1.436944842338562
Epoch 1250, training loss: 0.06536780297756195 = 0.001854473608545959 + 0.01 * 6.3513336181640625
Epoch 1250, val loss: 1.438971996307373
Epoch 1260, training loss: 0.06602157652378082 = 0.001828456879593432 + 0.01 * 6.419312477111816
Epoch 1260, val loss: 1.4410513639450073
Epoch 1270, training loss: 0.06527134776115417 = 0.0018027647165581584 + 0.01 * 6.346858501434326
Epoch 1270, val loss: 1.4430947303771973
Epoch 1280, training loss: 0.0652715265750885 = 0.0017782935174182057 + 0.01 * 6.34932279586792
Epoch 1280, val loss: 1.4448639154434204
Epoch 1290, training loss: 0.06549867987632751 = 0.0017542934510856867 + 0.01 * 6.374439239501953
Epoch 1290, val loss: 1.4467413425445557
Epoch 1300, training loss: 0.06511326134204865 = 0.0017315197037532926 + 0.01 * 6.338174343109131
Epoch 1300, val loss: 1.4487217664718628
Epoch 1310, training loss: 0.06524869799613953 = 0.0017093658680096269 + 0.01 * 6.353933334350586
Epoch 1310, val loss: 1.450454831123352
Epoch 1320, training loss: 0.06519763916730881 = 0.0016880217008292675 + 0.01 * 6.350962162017822
Epoch 1320, val loss: 1.4521969556808472
Epoch 1330, training loss: 0.06503993272781372 = 0.0016672782367095351 + 0.01 * 6.337265968322754
Epoch 1330, val loss: 1.4539622068405151
Epoch 1340, training loss: 0.06509694457054138 = 0.001647155499085784 + 0.01 * 6.344979763031006
Epoch 1340, val loss: 1.4555883407592773
Epoch 1350, training loss: 0.06497194617986679 = 0.0016276511596515775 + 0.01 * 6.334429740905762
Epoch 1350, val loss: 1.4573036432266235
Epoch 1360, training loss: 0.06477294862270355 = 0.0016086724353954196 + 0.01 * 6.316427707672119
Epoch 1360, val loss: 1.4589455127716064
Epoch 1370, training loss: 0.06501308083534241 = 0.0015900133876129985 + 0.01 * 6.342307090759277
Epoch 1370, val loss: 1.460518479347229
Epoch 1380, training loss: 0.06478407979011536 = 0.001572214881889522 + 0.01 * 6.321186542510986
Epoch 1380, val loss: 1.4621022939682007
Epoch 1390, training loss: 0.06475146114826202 = 0.0015547374496236444 + 0.01 * 6.319672584533691
Epoch 1390, val loss: 1.4635984897613525
Epoch 1400, training loss: 0.06467361003160477 = 0.0015377993695437908 + 0.01 * 6.313581466674805
Epoch 1400, val loss: 1.4651973247528076
Epoch 1410, training loss: 0.06478413194417953 = 0.0015212148427963257 + 0.01 * 6.326292037963867
Epoch 1410, val loss: 1.4666531085968018
Epoch 1420, training loss: 0.06468795984983444 = 0.0015049930661916733 + 0.01 * 6.318296909332275
Epoch 1420, val loss: 1.468077540397644
Epoch 1430, training loss: 0.06459759175777435 = 0.0014894921332597733 + 0.01 * 6.310810089111328
Epoch 1430, val loss: 1.469600796699524
Epoch 1440, training loss: 0.06474264711141586 = 0.0014745116932317615 + 0.01 * 6.326813220977783
Epoch 1440, val loss: 1.470881700515747
Epoch 1450, training loss: 0.06455639004707336 = 0.001459800056181848 + 0.01 * 6.309659004211426
Epoch 1450, val loss: 1.4722760915756226
Epoch 1460, training loss: 0.0644398033618927 = 0.0014455276541411877 + 0.01 * 6.2994279861450195
Epoch 1460, val loss: 1.473618984222412
Epoch 1470, training loss: 0.06452973932027817 = 0.0014315767912194133 + 0.01 * 6.309816360473633
Epoch 1470, val loss: 1.4749572277069092
Epoch 1480, training loss: 0.0643545612692833 = 0.0014180360594764352 + 0.01 * 6.293652534484863
Epoch 1480, val loss: 1.476207971572876
Epoch 1490, training loss: 0.06442064046859741 = 0.0014048273442313075 + 0.01 * 6.301581382751465
Epoch 1490, val loss: 1.4775314331054688
Epoch 1500, training loss: 0.06421378999948502 = 0.001392012694850564 + 0.01 * 6.282177448272705
Epoch 1500, val loss: 1.478851079940796
Epoch 1510, training loss: 0.06434790790081024 = 0.001379416324198246 + 0.01 * 6.296849250793457
Epoch 1510, val loss: 1.4799623489379883
Epoch 1520, training loss: 0.064186692237854 = 0.0013672566274181008 + 0.01 * 6.2819437980651855
Epoch 1520, val loss: 1.481230616569519
Epoch 1530, training loss: 0.06412681192159653 = 0.0013553336029872298 + 0.01 * 6.2771477699279785
Epoch 1530, val loss: 1.4824292659759521
Epoch 1540, training loss: 0.06420500576496124 = 0.0013438051100820303 + 0.01 * 6.286120891571045
Epoch 1540, val loss: 1.4835944175720215
Epoch 1550, training loss: 0.06415329873561859 = 0.001332440529949963 + 0.01 * 6.282086372375488
Epoch 1550, val loss: 1.4849604368209839
Epoch 1560, training loss: 0.06410141289234161 = 0.0013213964411988854 + 0.01 * 6.278001308441162
Epoch 1560, val loss: 1.4860438108444214
Epoch 1570, training loss: 0.0641818568110466 = 0.0013104886747896671 + 0.01 * 6.287137031555176
Epoch 1570, val loss: 1.4870396852493286
Epoch 1580, training loss: 0.06392759829759598 = 0.0012999329483136535 + 0.01 * 6.2627668380737305
Epoch 1580, val loss: 1.4882169961929321
Epoch 1590, training loss: 0.06414813548326492 = 0.0012896654661744833 + 0.01 * 6.285847187042236
Epoch 1590, val loss: 1.4892120361328125
Epoch 1600, training loss: 0.06394408643245697 = 0.001279420917853713 + 0.01 * 6.2664666175842285
Epoch 1600, val loss: 1.4903031587600708
Epoch 1610, training loss: 0.06409642100334167 = 0.0012695521581918001 + 0.01 * 6.282687664031982
Epoch 1610, val loss: 1.4912992715835571
Epoch 1620, training loss: 0.0641343966126442 = 0.001259939861483872 + 0.01 * 6.287446022033691
Epoch 1620, val loss: 1.4923759698867798
Epoch 1630, training loss: 0.06390391290187836 = 0.0012503284960985184 + 0.01 * 6.265358924865723
Epoch 1630, val loss: 1.4933912754058838
Epoch 1640, training loss: 0.06385122239589691 = 0.0012410801136866212 + 0.01 * 6.261013984680176
Epoch 1640, val loss: 1.4942797422409058
Epoch 1650, training loss: 0.06395360082387924 = 0.001231979113072157 + 0.01 * 6.272162914276123
Epoch 1650, val loss: 1.4952489137649536
Epoch 1660, training loss: 0.06369704008102417 = 0.0012230403954163194 + 0.01 * 6.247399806976318
Epoch 1660, val loss: 1.4961870908737183
Epoch 1670, training loss: 0.063686802983284 = 0.0012142773484811187 + 0.01 * 6.247252464294434
Epoch 1670, val loss: 1.4971058368682861
Epoch 1680, training loss: 0.06380649656057358 = 0.001205694512464106 + 0.01 * 6.260080337524414
Epoch 1680, val loss: 1.4980264902114868
Epoch 1690, training loss: 0.06384803354740143 = 0.00119730643928051 + 0.01 * 6.265072822570801
Epoch 1690, val loss: 1.4989449977874756
Epoch 1700, training loss: 0.06370000541210175 = 0.0011890908936038613 + 0.01 * 6.251091480255127
Epoch 1700, val loss: 1.4998348951339722
Epoch 1710, training loss: 0.06404747068881989 = 0.0011811243603006005 + 0.01 * 6.28663444519043
Epoch 1710, val loss: 1.5006482601165771
Epoch 1720, training loss: 0.06355077773332596 = 0.001173183205537498 + 0.01 * 6.237760066986084
Epoch 1720, val loss: 1.5014946460723877
Epoch 1730, training loss: 0.06385199725627899 = 0.0011653996771201491 + 0.01 * 6.268660068511963
Epoch 1730, val loss: 1.502294659614563
Epoch 1740, training loss: 0.06360659748315811 = 0.0011579131241887808 + 0.01 * 6.244868755340576
Epoch 1740, val loss: 1.5031441450119019
Epoch 1750, training loss: 0.06389371305704117 = 0.0011504071298986673 + 0.01 * 6.274331092834473
Epoch 1750, val loss: 1.5039477348327637
Epoch 1760, training loss: 0.06357872486114502 = 0.0011430260492488742 + 0.01 * 6.243569850921631
Epoch 1760, val loss: 1.5047181844711304
Epoch 1770, training loss: 0.06346683204174042 = 0.0011358520714566112 + 0.01 * 6.23309850692749
Epoch 1770, val loss: 1.5055134296417236
Epoch 1780, training loss: 0.06356658041477203 = 0.0011287766974419355 + 0.01 * 6.243780612945557
Epoch 1780, val loss: 1.506267786026001
Epoch 1790, training loss: 0.06346823275089264 = 0.0011219410225749016 + 0.01 * 6.234629154205322
Epoch 1790, val loss: 1.5068873167037964
Epoch 1800, training loss: 0.06349126994609833 = 0.001115120598115027 + 0.01 * 6.23761510848999
Epoch 1800, val loss: 1.5076956748962402
Epoch 1810, training loss: 0.06345878541469574 = 0.001108521712012589 + 0.01 * 6.235026836395264
Epoch 1810, val loss: 1.5084503889083862
Epoch 1820, training loss: 0.063517265021801 = 0.0011020379606634378 + 0.01 * 6.241522789001465
Epoch 1820, val loss: 1.5091164112091064
Epoch 1830, training loss: 0.06339091062545776 = 0.001095567480660975 + 0.01 * 6.229534149169922
Epoch 1830, val loss: 1.5097689628601074
Epoch 1840, training loss: 0.06364529579877853 = 0.001089335884898901 + 0.01 * 6.255596160888672
Epoch 1840, val loss: 1.5104790925979614
Epoch 1850, training loss: 0.06341749429702759 = 0.0010830543469637632 + 0.01 * 6.233443737030029
Epoch 1850, val loss: 1.511095404624939
Epoch 1860, training loss: 0.06341137737035751 = 0.0010769990039989352 + 0.01 * 6.233438014984131
Epoch 1860, val loss: 1.5118119716644287
Epoch 1870, training loss: 0.06323065608739853 = 0.0010709628695622087 + 0.01 * 6.215969085693359
Epoch 1870, val loss: 1.512380599975586
Epoch 1880, training loss: 0.0632806271314621 = 0.0010651636403054 + 0.01 * 6.221546649932861
Epoch 1880, val loss: 1.5129296779632568
Epoch 1890, training loss: 0.06326880306005478 = 0.0010593078332021832 + 0.01 * 6.220950126647949
Epoch 1890, val loss: 1.5136406421661377
Epoch 1900, training loss: 0.06341327726840973 = 0.0010536251356825233 + 0.01 * 6.235965251922607
Epoch 1900, val loss: 1.5142765045166016
Epoch 1910, training loss: 0.06354602426290512 = 0.0010479942429810762 + 0.01 * 6.24980354309082
Epoch 1910, val loss: 1.514765739440918
Epoch 1920, training loss: 0.0631909966468811 = 0.001042375573888421 + 0.01 * 6.21486234664917
Epoch 1920, val loss: 1.5154231786727905
Epoch 1930, training loss: 0.06320686638355255 = 0.0010369789088144898 + 0.01 * 6.216989040374756
Epoch 1930, val loss: 1.5159052610397339
Epoch 1940, training loss: 0.06330764293670654 = 0.0010315959807485342 + 0.01 * 6.22760534286499
Epoch 1940, val loss: 1.516396403312683
Epoch 1950, training loss: 0.06320656090974808 = 0.0010260912822559476 + 0.01 * 6.21804666519165
Epoch 1950, val loss: 1.5169668197631836
Epoch 1960, training loss: 0.06328754127025604 = 0.001020689494907856 + 0.01 * 6.226685523986816
Epoch 1960, val loss: 1.5174287557601929
Epoch 1970, training loss: 0.0630611926317215 = 0.0010153818875551224 + 0.01 * 6.204581260681152
Epoch 1970, val loss: 1.5179929733276367
Epoch 1980, training loss: 0.06314551085233688 = 0.0010101116495206952 + 0.01 * 6.213540077209473
Epoch 1980, val loss: 1.5184152126312256
Epoch 1990, training loss: 0.06318707764148712 = 0.0010049851844087243 + 0.01 * 6.218209743499756
Epoch 1990, val loss: 1.5189738273620605
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.8081
Flip ASR: 0.7733/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0396523475646973 = 1.9559142589569092 + 0.01 * 8.373815536499023
Epoch 0, val loss: 1.962117314338684
Epoch 10, training loss: 2.029672861099243 = 1.945935606956482 + 0.01 * 8.373723030090332
Epoch 10, val loss: 1.9518622159957886
Epoch 20, training loss: 2.017707109451294 = 1.9339735507965088 + 0.01 * 8.373359680175781
Epoch 20, val loss: 1.938857913017273
Epoch 30, training loss: 2.001210927963257 = 1.9174878597259521 + 0.01 * 8.372314453125
Epoch 30, val loss: 1.9202300310134888
Epoch 40, training loss: 1.976760983467102 = 1.893086552619934 + 0.01 * 8.367443084716797
Epoch 40, val loss: 1.8922911882400513
Epoch 50, training loss: 1.9406521320343018 = 1.8573399782180786 + 0.01 * 8.331212997436523
Epoch 50, val loss: 1.8522262573242188
Epoch 60, training loss: 1.8909697532653809 = 1.81025230884552 + 0.01 * 8.071742057800293
Epoch 60, val loss: 1.8025989532470703
Epoch 70, training loss: 1.8354228734970093 = 1.759438395500183 + 0.01 * 7.598452568054199
Epoch 70, val loss: 1.7547264099121094
Epoch 80, training loss: 1.7805500030517578 = 1.7075234651565552 + 0.01 * 7.3026556968688965
Epoch 80, val loss: 1.710639238357544
Epoch 90, training loss: 1.7116433382034302 = 1.6398011445999146 + 0.01 * 7.1842217445373535
Epoch 90, val loss: 1.655909776687622
Epoch 100, training loss: 1.6232237815856934 = 1.5527923107147217 + 0.01 * 7.043144226074219
Epoch 100, val loss: 1.5884802341461182
Epoch 110, training loss: 1.5185902118682861 = 1.4496703147888184 + 0.01 * 6.891993045806885
Epoch 110, val loss: 1.5101642608642578
Epoch 120, training loss: 1.4107550382614136 = 1.3422789573669434 + 0.01 * 6.847612380981445
Epoch 120, val loss: 1.4296619892120361
Epoch 130, training loss: 1.309747576713562 = 1.2414894104003906 + 0.01 * 6.825817584991455
Epoch 130, val loss: 1.3562209606170654
Epoch 140, training loss: 1.2182711362838745 = 1.1500763893127441 + 0.01 * 6.81947660446167
Epoch 140, val loss: 1.289600133895874
Epoch 150, training loss: 1.1334728002548218 = 1.065364956855774 + 0.01 * 6.810788154602051
Epoch 150, val loss: 1.2265539169311523
Epoch 160, training loss: 1.0530223846435547 = 0.9849998950958252 + 0.01 * 6.802247524261475
Epoch 160, val loss: 1.165408968925476
Epoch 170, training loss: 0.9758371114730835 = 0.9078872799873352 + 0.01 * 6.794985294342041
Epoch 170, val loss: 1.1065351963043213
Epoch 180, training loss: 0.9009802341461182 = 0.8330954909324646 + 0.01 * 6.788473606109619
Epoch 180, val loss: 1.0495473146438599
Epoch 190, training loss: 0.8273991942405701 = 0.7595738768577576 + 0.01 * 6.782529830932617
Epoch 190, val loss: 0.9940354228019714
Epoch 200, training loss: 0.7546787858009338 = 0.686911404132843 + 0.01 * 6.776737689971924
Epoch 200, val loss: 0.9397341012954712
Epoch 210, training loss: 0.6834737062454224 = 0.6157634854316711 + 0.01 * 6.771018981933594
Epoch 210, val loss: 0.8868619799613953
Epoch 220, training loss: 0.6150690913200378 = 0.547411322593689 + 0.01 * 6.765778541564941
Epoch 220, val loss: 0.836707592010498
Epoch 230, training loss: 0.5505350232124329 = 0.48292043805122375 + 0.01 * 6.7614593505859375
Epoch 230, val loss: 0.7915610074996948
Epoch 240, training loss: 0.49045222997665405 = 0.42286574840545654 + 0.01 * 6.758647918701172
Epoch 240, val loss: 0.7530301213264465
Epoch 250, training loss: 0.43514174222946167 = 0.36758625507354736 + 0.01 * 6.755548000335693
Epoch 250, val loss: 0.7216817140579224
Epoch 260, training loss: 0.38494551181793213 = 0.317414253950119 + 0.01 * 6.753126621246338
Epoch 260, val loss: 0.6973094940185547
Epoch 270, training loss: 0.340334951877594 = 0.2728249728679657 + 0.01 * 6.750999450683594
Epoch 270, val loss: 0.6797232031822205
Epoch 280, training loss: 0.30171534419059753 = 0.23422734439373016 + 0.01 * 6.748801231384277
Epoch 280, val loss: 0.6686684489250183
Epoch 290, training loss: 0.26898300647735596 = 0.20151180028915405 + 0.01 * 6.747119903564453
Epoch 290, val loss: 0.6634546518325806
Epoch 300, training loss: 0.2415519803762436 = 0.17410895228385925 + 0.01 * 6.744303226470947
Epoch 300, val loss: 0.6635403037071228
Epoch 310, training loss: 0.21872365474700928 = 0.1513112485408783 + 0.01 * 6.741241455078125
Epoch 310, val loss: 0.6680680513381958
Epoch 320, training loss: 0.1997126340866089 = 0.1323346495628357 + 0.01 * 6.737797737121582
Epoch 320, val loss: 0.6761763691902161
Epoch 330, training loss: 0.18383565545082092 = 0.11647826433181763 + 0.01 * 6.7357401847839355
Epoch 330, val loss: 0.6868298649787903
Epoch 340, training loss: 0.17042282223701477 = 0.10312142223119736 + 0.01 * 6.7301411628723145
Epoch 340, val loss: 0.6993212699890137
Epoch 350, training loss: 0.1590118408203125 = 0.09176792204380035 + 0.01 * 6.724392890930176
Epoch 350, val loss: 0.7129635214805603
Epoch 360, training loss: 0.14919599890708923 = 0.08200951665639877 + 0.01 * 6.7186479568481445
Epoch 360, val loss: 0.7272201180458069
Epoch 370, training loss: 0.14063981175422668 = 0.07352527230978012 + 0.01 * 6.71145486831665
Epoch 370, val loss: 0.7418842315673828
Epoch 380, training loss: 0.13315752148628235 = 0.06608366966247559 + 0.01 * 6.707386016845703
Epoch 380, val loss: 0.7569102644920349
Epoch 390, training loss: 0.12648656964302063 = 0.05952104926109314 + 0.01 * 6.696551322937012
Epoch 390, val loss: 0.7722238302230835
Epoch 400, training loss: 0.12059716135263443 = 0.05371738225221634 + 0.01 * 6.687978267669678
Epoch 400, val loss: 0.7877044677734375
Epoch 410, training loss: 0.11563190817832947 = 0.048578355461359024 + 0.01 * 6.705355644226074
Epoch 410, val loss: 0.8033198118209839
Epoch 420, training loss: 0.11070580035448074 = 0.044026464223861694 + 0.01 * 6.667933940887451
Epoch 420, val loss: 0.8189859986305237
Epoch 430, training loss: 0.10659021139144897 = 0.039989639073610306 + 0.01 * 6.660057067871094
Epoch 430, val loss: 0.8345531225204468
Epoch 440, training loss: 0.10297586023807526 = 0.03640590235590935 + 0.01 * 6.656996250152588
Epoch 440, val loss: 0.8499829769134521
Epoch 450, training loss: 0.09974299371242523 = 0.03322284296154976 + 0.01 * 6.65201473236084
Epoch 450, val loss: 0.8652618527412415
Epoch 460, training loss: 0.09687034785747528 = 0.03038955107331276 + 0.01 * 6.648080348968506
Epoch 460, val loss: 0.880267322063446
Epoch 470, training loss: 0.09424801915884018 = 0.02786249853670597 + 0.01 * 6.638552665710449
Epoch 470, val loss: 0.8950484395027161
Epoch 480, training loss: 0.0919124186038971 = 0.025606794282794 + 0.01 * 6.6305623054504395
Epoch 480, val loss: 0.9095388650894165
Epoch 490, training loss: 0.08978398889303207 = 0.023589210584759712 + 0.01 * 6.619478225708008
Epoch 490, val loss: 0.9238254427909851
Epoch 500, training loss: 0.08807314187288284 = 0.02178002893924713 + 0.01 * 6.6293110847473145
Epoch 500, val loss: 0.9378209710121155
Epoch 510, training loss: 0.08634991943836212 = 0.020158015191555023 + 0.01 * 6.619190692901611
Epoch 510, val loss: 0.9515364170074463
Epoch 520, training loss: 0.08480773866176605 = 0.018698545172810555 + 0.01 * 6.61091947555542
Epoch 520, val loss: 0.9648553729057312
Epoch 530, training loss: 0.08343105763196945 = 0.017381450161337852 + 0.01 * 6.6049604415893555
Epoch 530, val loss: 0.9779393076896667
Epoch 540, training loss: 0.08260630071163177 = 0.016191506758332253 + 0.01 * 6.6414794921875
Epoch 540, val loss: 0.9907208681106567
Epoch 550, training loss: 0.08122134208679199 = 0.015116950497031212 + 0.01 * 6.610438823699951
Epoch 550, val loss: 1.0031737089157104
Epoch 560, training loss: 0.08010340481996536 = 0.014143361710011959 + 0.01 * 6.596004009246826
Epoch 560, val loss: 1.0153075456619263
Epoch 570, training loss: 0.0791860818862915 = 0.013257764279842377 + 0.01 * 6.592832088470459
Epoch 570, val loss: 1.0271413326263428
Epoch 580, training loss: 0.07844243198633194 = 0.012451088055968285 + 0.01 * 6.59913444519043
Epoch 580, val loss: 1.038757562637329
Epoch 590, training loss: 0.07760491222143173 = 0.011715643107891083 + 0.01 * 6.588926792144775
Epoch 590, val loss: 1.0499769449234009
Epoch 600, training loss: 0.0768715962767601 = 0.011043170467019081 + 0.01 * 6.582842826843262
Epoch 600, val loss: 1.0609842538833618
Epoch 610, training loss: 0.07618366926908493 = 0.010427655652165413 + 0.01 * 6.575602054595947
Epoch 610, val loss: 1.0716500282287598
Epoch 620, training loss: 0.07571225613355637 = 0.00986273679882288 + 0.01 * 6.584951877593994
Epoch 620, val loss: 1.0820928812026978
Epoch 630, training loss: 0.0750952959060669 = 0.009343461133539677 + 0.01 * 6.575183868408203
Epoch 630, val loss: 1.0922236442565918
Epoch 640, training loss: 0.07450563460588455 = 0.008865291252732277 + 0.01 * 6.564034938812256
Epoch 640, val loss: 1.1021453142166138
Epoch 650, training loss: 0.07400287687778473 = 0.008423659950494766 + 0.01 * 6.557921409606934
Epoch 650, val loss: 1.111774206161499
Epoch 660, training loss: 0.07369882613420486 = 0.00801640935242176 + 0.01 * 6.568242073059082
Epoch 660, val loss: 1.1212902069091797
Epoch 670, training loss: 0.07318414002656937 = 0.007639679592102766 + 0.01 * 6.554446697235107
Epoch 670, val loss: 1.1304056644439697
Epoch 680, training loss: 0.07286766171455383 = 0.007290220819413662 + 0.01 * 6.55774450302124
Epoch 680, val loss: 1.1392501592636108
Epoch 690, training loss: 0.07249186933040619 = 0.00696661788970232 + 0.01 * 6.552525043487549
Epoch 690, val loss: 1.1479824781417847
Epoch 700, training loss: 0.07202042639255524 = 0.006666033528745174 + 0.01 * 6.5354390144348145
Epoch 700, val loss: 1.1564658880233765
Epoch 710, training loss: 0.07168681919574738 = 0.00638591730967164 + 0.01 * 6.53009033203125
Epoch 710, val loss: 1.164692997932434
Epoch 720, training loss: 0.07164060324430466 = 0.006124350242316723 + 0.01 * 6.551625728607178
Epoch 720, val loss: 1.1726304292678833
Epoch 730, training loss: 0.07108838856220245 = 0.005880433600395918 + 0.01 * 6.520796298980713
Epoch 730, val loss: 1.1804757118225098
Epoch 740, training loss: 0.07128513604402542 = 0.005652567837387323 + 0.01 * 6.563256740570068
Epoch 740, val loss: 1.1881988048553467
Epoch 750, training loss: 0.07059795409440994 = 0.005440170876681805 + 0.01 * 6.5157790184021
Epoch 750, val loss: 1.1955504417419434
Epoch 760, training loss: 0.07042022049427032 = 0.0052404082380235195 + 0.01 * 6.517981052398682
Epoch 760, val loss: 1.2028473615646362
Epoch 770, training loss: 0.07002363353967667 = 0.005052612163126469 + 0.01 * 6.497102737426758
Epoch 770, val loss: 1.2099272012710571
Epoch 780, training loss: 0.07036124914884567 = 0.004875992890447378 + 0.01 * 6.548525333404541
Epoch 780, val loss: 1.2169132232666016
Epoch 790, training loss: 0.06981334090232849 = 0.004710046574473381 + 0.01 * 6.510329723358154
Epoch 790, val loss: 1.2235753536224365
Epoch 800, training loss: 0.06948966532945633 = 0.0045535024255514145 + 0.01 * 6.493616104125977
Epoch 800, val loss: 1.2302501201629639
Epoch 810, training loss: 0.06933043152093887 = 0.004405884072184563 + 0.01 * 6.492454528808594
Epoch 810, val loss: 1.236694097518921
Epoch 820, training loss: 0.06928059458732605 = 0.004266724921762943 + 0.01 * 6.501387119293213
Epoch 820, val loss: 1.2429773807525635
Epoch 830, training loss: 0.06909816712141037 = 0.004135328810662031 + 0.01 * 6.496284484863281
Epoch 830, val loss: 1.249165654182434
Epoch 840, training loss: 0.068862684071064 = 0.004010800737887621 + 0.01 * 6.4851884841918945
Epoch 840, val loss: 1.255181074142456
Epoch 850, training loss: 0.06869719177484512 = 0.003892760956659913 + 0.01 * 6.480443000793457
Epoch 850, val loss: 1.261047601699829
Epoch 860, training loss: 0.06855197995901108 = 0.0037808024790138006 + 0.01 * 6.477117538452148
Epoch 860, val loss: 1.2666531801223755
Epoch 870, training loss: 0.06837553530931473 = 0.003674298757687211 + 0.01 * 6.470123767852783
Epoch 870, val loss: 1.272334098815918
Epoch 880, training loss: 0.06811832636594772 = 0.003573270048946142 + 0.01 * 6.454505920410156
Epoch 880, val loss: 1.2777433395385742
Epoch 890, training loss: 0.06810656934976578 = 0.003476943587884307 + 0.01 * 6.462963104248047
Epoch 890, val loss: 1.283220648765564
Epoch 900, training loss: 0.06792706996202469 = 0.0033854723442345858 + 0.01 * 6.454160213470459
Epoch 900, val loss: 1.2884557247161865
Epoch 910, training loss: 0.06784141063690186 = 0.003298544092103839 + 0.01 * 6.454286575317383
Epoch 910, val loss: 1.2935115098953247
Epoch 920, training loss: 0.0676225870847702 = 0.0032155015505850315 + 0.01 * 6.440708637237549
Epoch 920, val loss: 1.2985410690307617
Epoch 930, training loss: 0.06775769591331482 = 0.003136084182187915 + 0.01 * 6.462161064147949
Epoch 930, val loss: 1.3035264015197754
Epoch 940, training loss: 0.06737730652093887 = 0.0030607283115386963 + 0.01 * 6.4316582679748535
Epoch 940, val loss: 1.3082833290100098
Epoch 950, training loss: 0.06722208112478256 = 0.0029886316042393446 + 0.01 * 6.42334508895874
Epoch 950, val loss: 1.31300950050354
Epoch 960, training loss: 0.06745534390211105 = 0.0029197270050644875 + 0.01 * 6.453562259674072
Epoch 960, val loss: 1.3176722526550293
Epoch 970, training loss: 0.06725506484508514 = 0.002854065503925085 + 0.01 * 6.440100193023682
Epoch 970, val loss: 1.3221092224121094
Epoch 980, training loss: 0.06729289889335632 = 0.002791063627228141 + 0.01 * 6.450183868408203
Epoch 980, val loss: 1.326553225517273
Epoch 990, training loss: 0.06682690232992172 = 0.0027306722477078438 + 0.01 * 6.409622669219971
Epoch 990, val loss: 1.3309009075164795
Epoch 1000, training loss: 0.06701090186834335 = 0.0026728336233645678 + 0.01 * 6.433806419372559
Epoch 1000, val loss: 1.335249423980713
Epoch 1010, training loss: 0.0667910948395729 = 0.002617570571601391 + 0.01 * 6.417352676391602
Epoch 1010, val loss: 1.3392940759658813
Epoch 1020, training loss: 0.06701220571994781 = 0.002564507769420743 + 0.01 * 6.444770336151123
Epoch 1020, val loss: 1.3433855772018433
Epoch 1030, training loss: 0.06655373424291611 = 0.0025133343879133463 + 0.01 * 6.4040398597717285
Epoch 1030, val loss: 1.3473783731460571
Epoch 1040, training loss: 0.06649903953075409 = 0.0024643067736178637 + 0.01 * 6.403473377227783
Epoch 1040, val loss: 1.3513771295547485
Epoch 1050, training loss: 0.06636211276054382 = 0.0024172700941562653 + 0.01 * 6.394484043121338
Epoch 1050, val loss: 1.3551537990570068
Epoch 1060, training loss: 0.06630946695804596 = 0.002371984301134944 + 0.01 * 6.393748760223389
Epoch 1060, val loss: 1.3588978052139282
Epoch 1070, training loss: 0.06633825600147247 = 0.0023281406611204147 + 0.01 * 6.4010114669799805
Epoch 1070, val loss: 1.362599492073059
Epoch 1080, training loss: 0.06629128009080887 = 0.0022858947049826384 + 0.01 * 6.400538444519043
Epoch 1080, val loss: 1.366150975227356
Epoch 1090, training loss: 0.06606864184141159 = 0.002245612209662795 + 0.01 * 6.382303237915039
Epoch 1090, val loss: 1.3696660995483398
Epoch 1100, training loss: 0.06608699262142181 = 0.0022069173865020275 + 0.01 * 6.388008117675781
Epoch 1100, val loss: 1.3731709718704224
Epoch 1110, training loss: 0.06615182757377625 = 0.002169688232243061 + 0.01 * 6.398213863372803
Epoch 1110, val loss: 1.3764575719833374
Epoch 1120, training loss: 0.06582090258598328 = 0.0021336222998797894 + 0.01 * 6.368728160858154
Epoch 1120, val loss: 1.3798508644104004
Epoch 1130, training loss: 0.06600549072027206 = 0.0020988520700484514 + 0.01 * 6.390664100646973
Epoch 1130, val loss: 1.3831790685653687
Epoch 1140, training loss: 0.06571413576602936 = 0.002065404085442424 + 0.01 * 6.36487340927124
Epoch 1140, val loss: 1.386243462562561
Epoch 1150, training loss: 0.06561654806137085 = 0.0020331060513854027 + 0.01 * 6.358344554901123
Epoch 1150, val loss: 1.3893811702728271
Epoch 1160, training loss: 0.06580698490142822 = 0.0020018252544105053 + 0.01 * 6.380516529083252
Epoch 1160, val loss: 1.3924622535705566
Epoch 1170, training loss: 0.06560724973678589 = 0.0019716708920896053 + 0.01 * 6.363558292388916
Epoch 1170, val loss: 1.3953369855880737
Epoch 1180, training loss: 0.0656764954328537 = 0.0019424904603511095 + 0.01 * 6.373400688171387
Epoch 1180, val loss: 1.3984190225601196
Epoch 1190, training loss: 0.0653698742389679 = 0.001914373249746859 + 0.01 * 6.345550060272217
Epoch 1190, val loss: 1.401265025138855
Epoch 1200, training loss: 0.06555065512657166 = 0.0018871782813221216 + 0.01 * 6.366347312927246
Epoch 1200, val loss: 1.4041221141815186
Epoch 1210, training loss: 0.06524433195590973 = 0.0018606854137033224 + 0.01 * 6.338364601135254
Epoch 1210, val loss: 1.4068881273269653
Epoch 1220, training loss: 0.06518694013357162 = 0.001835217815823853 + 0.01 * 6.335172176361084
Epoch 1220, val loss: 1.4097200632095337
Epoch 1230, training loss: 0.06520885229110718 = 0.0018105292692780495 + 0.01 * 6.339832305908203
Epoch 1230, val loss: 1.4122936725616455
Epoch 1240, training loss: 0.06513335555791855 = 0.0017865063855424523 + 0.01 * 6.3346848487854
Epoch 1240, val loss: 1.4149008989334106
Epoch 1250, training loss: 0.06498011201620102 = 0.001763164415024221 + 0.01 * 6.321694850921631
Epoch 1250, val loss: 1.4175751209259033
Epoch 1260, training loss: 0.06507717072963715 = 0.0017406877595931292 + 0.01 * 6.333648204803467
Epoch 1260, val loss: 1.420074224472046
Epoch 1270, training loss: 0.06503207236528397 = 0.0017188877100124955 + 0.01 * 6.3313188552856445
Epoch 1270, val loss: 1.422545313835144
Epoch 1280, training loss: 0.06494037806987762 = 0.00169765658210963 + 0.01 * 6.3242716789245605
Epoch 1280, val loss: 1.4249964952468872
Epoch 1290, training loss: 0.06500346213579178 = 0.0016771055525168777 + 0.01 * 6.332635879516602
Epoch 1290, val loss: 1.4274063110351562
Epoch 1300, training loss: 0.06485922634601593 = 0.0016570745501667261 + 0.01 * 6.320215225219727
Epoch 1300, val loss: 1.4297161102294922
Epoch 1310, training loss: 0.06479045748710632 = 0.0016375795239582658 + 0.01 * 6.315288066864014
Epoch 1310, val loss: 1.4321191310882568
Epoch 1320, training loss: 0.06483246386051178 = 0.0016186968423426151 + 0.01 * 6.321376800537109
Epoch 1320, val loss: 1.4343032836914062
Epoch 1330, training loss: 0.06465663015842438 = 0.0016003336058929563 + 0.01 * 6.305630207061768
Epoch 1330, val loss: 1.4366823434829712
Epoch 1340, training loss: 0.06488100439310074 = 0.0015824740985408425 + 0.01 * 6.329853534698486
Epoch 1340, val loss: 1.4389097690582275
Epoch 1350, training loss: 0.06450776010751724 = 0.0015650030691176653 + 0.01 * 6.294276237487793
Epoch 1350, val loss: 1.4411089420318604
Epoch 1360, training loss: 0.06453152000904083 = 0.0015480330912396312 + 0.01 * 6.298349380493164
Epoch 1360, val loss: 1.443297028541565
Epoch 1370, training loss: 0.06458228081464767 = 0.0015315692871809006 + 0.01 * 6.3050713539123535
Epoch 1370, val loss: 1.4453994035720825
Epoch 1380, training loss: 0.06466037034988403 = 0.0015154443681240082 + 0.01 * 6.314492225646973
Epoch 1380, val loss: 1.447633147239685
Epoch 1390, training loss: 0.06428387016057968 = 0.0014997957041487098 + 0.01 * 6.278408050537109
Epoch 1390, val loss: 1.4496445655822754
Epoch 1400, training loss: 0.0645010694861412 = 0.0014845033874735236 + 0.01 * 6.301656723022461
Epoch 1400, val loss: 1.4517678022384644
Epoch 1410, training loss: 0.06451449543237686 = 0.001469620387069881 + 0.01 * 6.304487705230713
Epoch 1410, val loss: 1.4536354541778564
Epoch 1420, training loss: 0.06424659490585327 = 0.0014551194617524743 + 0.01 * 6.279147624969482
Epoch 1420, val loss: 1.4557169675827026
Epoch 1430, training loss: 0.06427755206823349 = 0.001440893393009901 + 0.01 * 6.283666133880615
Epoch 1430, val loss: 1.4577252864837646
Epoch 1440, training loss: 0.06422014534473419 = 0.001427065348252654 + 0.01 * 6.279308319091797
Epoch 1440, val loss: 1.4597008228302002
Epoch 1450, training loss: 0.06419754773378372 = 0.0014135846868157387 + 0.01 * 6.2783966064453125
Epoch 1450, val loss: 1.4616153240203857
Epoch 1460, training loss: 0.06423759460449219 = 0.0014003944816067815 + 0.01 * 6.283720016479492
Epoch 1460, val loss: 1.4634528160095215
Epoch 1470, training loss: 0.06411615759134293 = 0.0013876191806048155 + 0.01 * 6.272854328155518
Epoch 1470, val loss: 1.4652750492095947
Epoch 1480, training loss: 0.06395578384399414 = 0.0013749857898801565 + 0.01 * 6.258080005645752
Epoch 1480, val loss: 1.4672715663909912
Epoch 1490, training loss: 0.0640709400177002 = 0.001362767070531845 + 0.01 * 6.270816802978516
Epoch 1490, val loss: 1.4690123796463013
Epoch 1500, training loss: 0.06400322914123535 = 0.0013508268166333437 + 0.01 * 6.265240669250488
Epoch 1500, val loss: 1.4707568883895874
Epoch 1510, training loss: 0.06376339495182037 = 0.001339023932814598 + 0.01 * 6.24243688583374
Epoch 1510, val loss: 1.4725733995437622
Epoch 1520, training loss: 0.06389357894659042 = 0.0013276240788400173 + 0.01 * 6.256595611572266
Epoch 1520, val loss: 1.4743107557296753
Epoch 1530, training loss: 0.06393849104642868 = 0.0013163916300982237 + 0.01 * 6.262209892272949
Epoch 1530, val loss: 1.476003646850586
Epoch 1540, training loss: 0.06369370222091675 = 0.0013053027214482427 + 0.01 * 6.238840103149414
Epoch 1540, val loss: 1.477766990661621
Epoch 1550, training loss: 0.06377990543842316 = 0.0012945984490215778 + 0.01 * 6.248530864715576
Epoch 1550, val loss: 1.4794471263885498
Epoch 1560, training loss: 0.06391549855470657 = 0.0012841474963352084 + 0.01 * 6.2631354331970215
Epoch 1560, val loss: 1.481115698814392
Epoch 1570, training loss: 0.06375035643577576 = 0.0012736506760120392 + 0.01 * 6.247670650482178
Epoch 1570, val loss: 1.4829298257827759
Epoch 1580, training loss: 0.06363289058208466 = 0.0012636041501536965 + 0.01 * 6.236928462982178
Epoch 1580, val loss: 1.4844670295715332
Epoch 1590, training loss: 0.0636354386806488 = 0.0012538038427010179 + 0.01 * 6.238163948059082
Epoch 1590, val loss: 1.486154317855835
Epoch 1600, training loss: 0.06373843550682068 = 0.0012441885191947222 + 0.01 * 6.249424934387207
Epoch 1600, val loss: 1.4877325296401978
Epoch 1610, training loss: 0.06348065286874771 = 0.0012346378061920404 + 0.01 * 6.224601745605469
Epoch 1610, val loss: 1.4893548488616943
Epoch 1620, training loss: 0.06345562636852264 = 0.0012254264438524842 + 0.01 * 6.223020076751709
Epoch 1620, val loss: 1.4908678531646729
Epoch 1630, training loss: 0.0636783242225647 = 0.0012163558276370168 + 0.01 * 6.246196746826172
Epoch 1630, val loss: 1.4924403429031372
Epoch 1640, training loss: 0.06357667595148087 = 0.0012072930112481117 + 0.01 * 6.236937999725342
Epoch 1640, val loss: 1.493815302848816
Epoch 1650, training loss: 0.06347546726465225 = 0.001198779558762908 + 0.01 * 6.227668762207031
Epoch 1650, val loss: 1.4954842329025269
Epoch 1660, training loss: 0.06350010633468628 = 0.00119017343968153 + 0.01 * 6.230993270874023
Epoch 1660, val loss: 1.4969743490219116
Epoch 1670, training loss: 0.06354524195194244 = 0.0011816605692729354 + 0.01 * 6.236358165740967
Epoch 1670, val loss: 1.4984304904937744
Epoch 1680, training loss: 0.06354072690010071 = 0.001173436176031828 + 0.01 * 6.236729621887207
Epoch 1680, val loss: 1.4999185800552368
Epoch 1690, training loss: 0.06350856274366379 = 0.0011652875691652298 + 0.01 * 6.234327793121338
Epoch 1690, val loss: 1.5012569427490234
Epoch 1700, training loss: 0.06328924745321274 = 0.0011572449002414942 + 0.01 * 6.213200092315674
Epoch 1700, val loss: 1.5028033256530762
Epoch 1710, training loss: 0.06325168162584305 = 0.0011494563659653068 + 0.01 * 6.2102227210998535
Epoch 1710, val loss: 1.5041536092758179
Epoch 1720, training loss: 0.06323090940713882 = 0.0011416302295401692 + 0.01 * 6.208927631378174
Epoch 1720, val loss: 1.5056358575820923
Epoch 1730, training loss: 0.06342056393623352 = 0.001134013757109642 + 0.01 * 6.228654861450195
Epoch 1730, val loss: 1.5069563388824463
Epoch 1740, training loss: 0.06322461366653442 = 0.001126544433645904 + 0.01 * 6.2098069190979
Epoch 1740, val loss: 1.508339524269104
Epoch 1750, training loss: 0.06329184770584106 = 0.0011192666133865714 + 0.01 * 6.217258453369141
Epoch 1750, val loss: 1.5097508430480957
Epoch 1760, training loss: 0.06345490366220474 = 0.0011119851842522621 + 0.01 * 6.234292030334473
Epoch 1760, val loss: 1.5110851526260376
Epoch 1770, training loss: 0.0632174015045166 = 0.001104807946830988 + 0.01 * 6.211259841918945
Epoch 1770, val loss: 1.5124878883361816
Epoch 1780, training loss: 0.06334000825881958 = 0.001098046894185245 + 0.01 * 6.224196434020996
Epoch 1780, val loss: 1.5138134956359863
Epoch 1790, training loss: 0.06331507861614227 = 0.0010911839781329036 + 0.01 * 6.222389221191406
Epoch 1790, val loss: 1.5150619745254517
Epoch 1800, training loss: 0.06324627250432968 = 0.0010843373602256179 + 0.01 * 6.216193675994873
Epoch 1800, val loss: 1.5165519714355469
Epoch 1810, training loss: 0.06313444674015045 = 0.0010779162403196096 + 0.01 * 6.205653190612793
Epoch 1810, val loss: 1.5178641080856323
Epoch 1820, training loss: 0.06323711574077606 = 0.001071549253538251 + 0.01 * 6.216557025909424
Epoch 1820, val loss: 1.5191278457641602
Epoch 1830, training loss: 0.06307248026132584 = 0.0010652205673977733 + 0.01 * 6.200726509094238
Epoch 1830, val loss: 1.5203132629394531
Epoch 1840, training loss: 0.06306042522192001 = 0.0010590623132884502 + 0.01 * 6.200136661529541
Epoch 1840, val loss: 1.5215240716934204
Epoch 1850, training loss: 0.06315793842077255 = 0.001053069019690156 + 0.01 * 6.210486888885498
Epoch 1850, val loss: 1.522815227508545
Epoch 1860, training loss: 0.06310365349054337 = 0.0010470399865880609 + 0.01 * 6.205661773681641
Epoch 1860, val loss: 1.5240453481674194
Epoch 1870, training loss: 0.06294629722833633 = 0.0010411818511784077 + 0.01 * 6.190511226654053
Epoch 1870, val loss: 1.5253936052322388
Epoch 1880, training loss: 0.06302586197853088 = 0.001035478780977428 + 0.01 * 6.199038028717041
Epoch 1880, val loss: 1.5265061855316162
Epoch 1890, training loss: 0.0629984512925148 = 0.0010297661647200584 + 0.01 * 6.196868896484375
Epoch 1890, val loss: 1.5277094841003418
Epoch 1900, training loss: 0.0629819855093956 = 0.0010242086136713624 + 0.01 * 6.195777893066406
Epoch 1900, val loss: 1.5289560556411743
Epoch 1910, training loss: 0.0629139244556427 = 0.0010188298765569925 + 0.01 * 6.189509391784668
Epoch 1910, val loss: 1.5299897193908691
Epoch 1920, training loss: 0.06303281337022781 = 0.0010132932802662253 + 0.01 * 6.2019524574279785
Epoch 1920, val loss: 1.5313012599945068
Epoch 1930, training loss: 0.06310164928436279 = 0.0010081485379487276 + 0.01 * 6.209350109100342
Epoch 1930, val loss: 1.532453179359436
Epoch 1940, training loss: 0.06273676455020905 = 0.0010027633979916573 + 0.01 * 6.173400402069092
Epoch 1940, val loss: 1.533609390258789
Epoch 1950, training loss: 0.06283599138259888 = 0.0009977164445444942 + 0.01 * 6.1838274002075195
Epoch 1950, val loss: 1.5347957611083984
Epoch 1960, training loss: 0.06299495697021484 = 0.0009926771745085716 + 0.01 * 6.200228214263916
Epoch 1960, val loss: 1.5359076261520386
Epoch 1970, training loss: 0.06277485936880112 = 0.0009875083342194557 + 0.01 * 6.178735256195068
Epoch 1970, val loss: 1.5371849536895752
Epoch 1980, training loss: 0.06285658478736877 = 0.000982738332822919 + 0.01 * 6.187384605407715
Epoch 1980, val loss: 1.5383100509643555
Epoch 1990, training loss: 0.06275616586208344 = 0.000977851334027946 + 0.01 * 6.177832126617432
Epoch 1990, val loss: 1.5393993854522705
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.3358
Flip ASR: 0.3067/225 nodes
The final ASR:0.55105, 0.19508, Accuracy:0.80494, 0.00175
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11642])
remove edge: torch.Size([2, 9464])
updated graph: torch.Size([2, 10550])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9668
Flip ASR: 0.9600/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.97663, 0.00758, Accuracy:0.83457, 0.00873
Begin epxeriment: cont_weight: 0.01 epoch:2000 der1:0.3 der2:0 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0413026809692383 = 1.9575637578964233 + 0.01 * 8.373880386352539
Epoch 0, val loss: 1.9553250074386597
Epoch 10, training loss: 2.030717611312866 = 1.9469794034957886 + 0.01 * 8.373821258544922
Epoch 10, val loss: 1.944665789604187
Epoch 20, training loss: 2.0177905559539795 = 1.9340547323226929 + 0.01 * 8.373592376708984
Epoch 20, val loss: 1.9315366744995117
Epoch 30, training loss: 1.9996362924575806 = 1.9159070253372192 + 0.01 * 8.372932434082031
Epoch 30, val loss: 1.9131888151168823
Epoch 40, training loss: 1.9727733135223389 = 1.8890751600265503 + 0.01 * 8.369815826416016
Epoch 40, val loss: 1.8866339921951294
Epoch 50, training loss: 1.934623122215271 = 1.8511371612548828 + 0.01 * 8.348591804504395
Epoch 50, val loss: 1.8509835004806519
Epoch 60, training loss: 1.8890105485916138 = 1.8065739870071411 + 0.01 * 8.243653297424316
Epoch 60, val loss: 1.8136775493621826
Epoch 70, training loss: 1.847335696220398 = 1.766930103302002 + 0.01 * 8.040555000305176
Epoch 70, val loss: 1.7843316793441772
Epoch 80, training loss: 1.7980462312698364 = 1.7216567993164062 + 0.01 * 7.638942241668701
Epoch 80, val loss: 1.745864987373352
Epoch 90, training loss: 1.7321560382843018 = 1.6593093872070312 + 0.01 * 7.284661769866943
Epoch 90, val loss: 1.6920219659805298
Epoch 100, training loss: 1.647384524345398 = 1.5779064893722534 + 0.01 * 6.947808742523193
Epoch 100, val loss: 1.6245900392532349
Epoch 110, training loss: 1.5541859865188599 = 1.4853484630584717 + 0.01 * 6.883748531341553
Epoch 110, val loss: 1.5491591691970825
Epoch 120, training loss: 1.4620554447174072 = 1.3934298753738403 + 0.01 * 6.862557411193848
Epoch 120, val loss: 1.475534439086914
Epoch 130, training loss: 1.3734179735183716 = 1.3048615455627441 + 0.01 * 6.855641841888428
Epoch 130, val loss: 1.406722068786621
Epoch 140, training loss: 1.2857322692871094 = 1.2172565460205078 + 0.01 * 6.847567558288574
Epoch 140, val loss: 1.3411290645599365
Epoch 150, training loss: 1.1977121829986572 = 1.1293206214904785 + 0.01 * 6.839150428771973
Epoch 150, val loss: 1.2764859199523926
Epoch 160, training loss: 1.1097015142440796 = 1.0413517951965332 + 0.01 * 6.834969997406006
Epoch 160, val loss: 1.2126471996307373
Epoch 170, training loss: 1.0220646858215332 = 0.9537283182144165 + 0.01 * 6.833636283874512
Epoch 170, val loss: 1.1486090421676636
Epoch 180, training loss: 0.935483992099762 = 0.8671404719352722 + 0.01 * 6.834354400634766
Epoch 180, val loss: 1.0842734575271606
Epoch 190, training loss: 0.8510830998420715 = 0.7827262282371521 + 0.01 * 6.835689544677734
Epoch 190, val loss: 1.020535945892334
Epoch 200, training loss: 0.7704095840454102 = 0.7020440697669983 + 0.01 * 6.836551189422607
Epoch 200, val loss: 0.9589828848838806
Epoch 210, training loss: 0.695604681968689 = 0.6272405385971069 + 0.01 * 6.83641242980957
Epoch 210, val loss: 0.9024137854576111
Epoch 220, training loss: 0.6285297274589539 = 0.5601802468299866 + 0.01 * 6.8349504470825195
Epoch 220, val loss: 0.8532691597938538
Epoch 230, training loss: 0.5696871280670166 = 0.50136798620224 + 0.01 * 6.831912040710449
Epoch 230, val loss: 0.812131404876709
Epoch 240, training loss: 0.5180327892303467 = 0.44976046681404114 + 0.01 * 6.827234268188477
Epoch 240, val loss: 0.7784835696220398
Epoch 250, training loss: 0.4716678261756897 = 0.4034571349620819 + 0.01 * 6.821069717407227
Epoch 250, val loss: 0.7506219148635864
Epoch 260, training loss: 0.4287814497947693 = 0.3606467545032501 + 0.01 * 6.813469886779785
Epoch 260, val loss: 0.7268010377883911
Epoch 270, training loss: 0.3884015679359436 = 0.32034775614738464 + 0.01 * 6.805380344390869
Epoch 270, val loss: 0.706248939037323
Epoch 280, training loss: 0.3504665791988373 = 0.28252559900283813 + 0.01 * 6.794098854064941
Epoch 280, val loss: 0.6885813474655151
Epoch 290, training loss: 0.3155195415019989 = 0.2476503700017929 + 0.01 * 6.786916255950928
Epoch 290, val loss: 0.673703134059906
Epoch 300, training loss: 0.2839454114437103 = 0.2162238508462906 + 0.01 * 6.77215576171875
Epoch 300, val loss: 0.6619889140129089
Epoch 310, training loss: 0.2559918165206909 = 0.18837463855743408 + 0.01 * 6.761716842651367
Epoch 310, val loss: 0.6535516381263733
Epoch 320, training loss: 0.23153457045555115 = 0.16401828825473785 + 0.01 * 6.751628875732422
Epoch 320, val loss: 0.6483716368675232
Epoch 330, training loss: 0.21031956374645233 = 0.14288616180419922 + 0.01 * 6.743340492248535
Epoch 330, val loss: 0.6462821960449219
Epoch 340, training loss: 0.19192826747894287 = 0.12460023909807205 + 0.01 * 6.732802867889404
Epoch 340, val loss: 0.6470683217048645
Epoch 350, training loss: 0.17623241245746613 = 0.10881540924310684 + 0.01 * 6.741700649261475
Epoch 350, val loss: 0.6504303812980652
Epoch 360, training loss: 0.1624949723482132 = 0.09523411095142365 + 0.01 * 6.726086139678955
Epoch 360, val loss: 0.6557782888412476
Epoch 370, training loss: 0.15076008439064026 = 0.08357806503772736 + 0.01 * 6.718201160430908
Epoch 370, val loss: 0.6627748012542725
Epoch 380, training loss: 0.14073342084884644 = 0.07358153164386749 + 0.01 * 6.7151899337768555
Epoch 380, val loss: 0.671089231967926
Epoch 390, training loss: 0.13211405277252197 = 0.06499146670103073 + 0.01 * 6.712259769439697
Epoch 390, val loss: 0.6804773807525635
Epoch 400, training loss: 0.12468160688877106 = 0.05758287385106087 + 0.01 * 6.709873676300049
Epoch 400, val loss: 0.6907245516777039
Epoch 410, training loss: 0.11825420707464218 = 0.05117885023355484 + 0.01 * 6.707535743713379
Epoch 410, val loss: 0.7015751004219055
Epoch 420, training loss: 0.11275399476289749 = 0.04563971608877182 + 0.01 * 6.711427688598633
Epoch 420, val loss: 0.7128481268882751
Epoch 430, training loss: 0.1078929603099823 = 0.04085029289126396 + 0.01 * 6.704266548156738
Epoch 430, val loss: 0.7243287563323975
Epoch 440, training loss: 0.10371999442577362 = 0.03670845180749893 + 0.01 * 6.701154708862305
Epoch 440, val loss: 0.735986590385437
Epoch 450, training loss: 0.10011349618434906 = 0.03312436863780022 + 0.01 * 6.698913097381592
Epoch 450, val loss: 0.7476710081100464
Epoch 460, training loss: 0.0970127210021019 = 0.03001684695482254 + 0.01 * 6.699587821960449
Epoch 460, val loss: 0.7593435645103455
Epoch 470, training loss: 0.09426088631153107 = 0.02731345221400261 + 0.01 * 6.694743633270264
Epoch 470, val loss: 0.7709539532661438
Epoch 480, training loss: 0.09186168015003204 = 0.024951880797743797 + 0.01 * 6.690979957580566
Epoch 480, val loss: 0.782392144203186
Epoch 490, training loss: 0.0897655189037323 = 0.02288004383444786 + 0.01 * 6.688547611236572
Epoch 490, val loss: 0.7936323881149292
Epoch 500, training loss: 0.08793778717517853 = 0.021054957062005997 + 0.01 * 6.688282489776611
Epoch 500, val loss: 0.8046870827674866
Epoch 510, training loss: 0.0862869918346405 = 0.01944154128432274 + 0.01 * 6.684545516967773
Epoch 510, val loss: 0.8154709339141846
Epoch 520, training loss: 0.08482452481985092 = 0.01800946705043316 + 0.01 * 6.6815056800842285
Epoch 520, val loss: 0.8260248899459839
Epoch 530, training loss: 0.08350825309753418 = 0.016732847318053246 + 0.01 * 6.677540302276611
Epoch 530, val loss: 0.8362997770309448
Epoch 540, training loss: 0.08242607861757278 = 0.015590793453156948 + 0.01 * 6.683528900146484
Epoch 540, val loss: 0.8462916612625122
Epoch 550, training loss: 0.08131298422813416 = 0.014566577039659023 + 0.01 * 6.674641132354736
Epoch 550, val loss: 0.8559859395027161
Epoch 560, training loss: 0.08034157752990723 = 0.013643725775182247 + 0.01 * 6.669785499572754
Epoch 560, val loss: 0.8655540347099304
Epoch 570, training loss: 0.07948325574398041 = 0.012809681706130505 + 0.01 * 6.667357921600342
Epoch 570, val loss: 0.8747745752334595
Epoch 580, training loss: 0.07867281883955002 = 0.01205351296812296 + 0.01 * 6.661930561065674
Epoch 580, val loss: 0.8837758302688599
Epoch 590, training loss: 0.07803662866353989 = 0.011365757323801517 + 0.01 * 6.667087554931641
Epoch 590, val loss: 0.8925132155418396
Epoch 600, training loss: 0.07730302214622498 = 0.010738595388829708 + 0.01 * 6.656442642211914
Epoch 600, val loss: 0.9010612964630127
Epoch 610, training loss: 0.07670538127422333 = 0.01016431674361229 + 0.01 * 6.654107093811035
Epoch 610, val loss: 0.9093679189682007
Epoch 620, training loss: 0.07620860636234283 = 0.009635369293391705 + 0.01 * 6.657324314117432
Epoch 620, val loss: 0.9175254702568054
Epoch 630, training loss: 0.07561910897493362 = 0.009146192111074924 + 0.01 * 6.647291660308838
Epoch 630, val loss: 0.925557017326355
Epoch 640, training loss: 0.07515935599803925 = 0.008692347444593906 + 0.01 * 6.646701335906982
Epoch 640, val loss: 0.9334969520568848
Epoch 650, training loss: 0.0746864452958107 = 0.008274221792817116 + 0.01 * 6.6412224769592285
Epoch 650, val loss: 0.941182553768158
Epoch 660, training loss: 0.07425294071435928 = 0.007887214422225952 + 0.01 * 6.63657283782959
Epoch 660, val loss: 0.9487268328666687
Epoch 670, training loss: 0.07403071969747543 = 0.007527313660830259 + 0.01 * 6.650341033935547
Epoch 670, val loss: 0.9561867117881775
Epoch 680, training loss: 0.07347490638494492 = 0.007193023804575205 + 0.01 * 6.628188610076904
Epoch 680, val loss: 0.9634056091308594
Epoch 690, training loss: 0.073105089366436 = 0.006881906185299158 + 0.01 * 6.622318267822266
Epoch 690, val loss: 0.9704683423042297
Epoch 700, training loss: 0.07298380136489868 = 0.0065918900072574615 + 0.01 * 6.639191627502441
Epoch 700, val loss: 0.9773595929145813
Epoch 710, training loss: 0.07254038006067276 = 0.00632180692628026 + 0.01 * 6.621857166290283
Epoch 710, val loss: 0.9842163920402527
Epoch 720, training loss: 0.07214879244565964 = 0.006069464609026909 + 0.01 * 6.607933044433594
Epoch 720, val loss: 0.9907946586608887
Epoch 730, training loss: 0.071964792907238 = 0.005832870025187731 + 0.01 * 6.613192081451416
Epoch 730, val loss: 0.9973300695419312
Epoch 740, training loss: 0.07161900401115417 = 0.00561205018311739 + 0.01 * 6.6006951332092285
Epoch 740, val loss: 1.0036741495132446
Epoch 750, training loss: 0.07136895507574081 = 0.005405477248132229 + 0.01 * 6.596347808837891
Epoch 750, val loss: 1.0098406076431274
Epoch 760, training loss: 0.07133878767490387 = 0.005211190786212683 + 0.01 * 6.612760066986084
Epoch 760, val loss: 1.0158356428146362
Epoch 770, training loss: 0.07111544907093048 = 0.005028957035392523 + 0.01 * 6.608649253845215
Epoch 770, val loss: 1.0217589139938354
Epoch 780, training loss: 0.07070734351873398 = 0.004857630003243685 + 0.01 * 6.584971904754639
Epoch 780, val loss: 1.0275485515594482
Epoch 790, training loss: 0.07041364163160324 = 0.00469601433724165 + 0.01 * 6.571763515472412
Epoch 790, val loss: 1.0331690311431885
Epoch 800, training loss: 0.0702480897307396 = 0.004543649032711983 + 0.01 * 6.570444107055664
Epoch 800, val loss: 1.0387564897537231
Epoch 810, training loss: 0.07003428786993027 = 0.0043999929912388325 + 0.01 * 6.563429355621338
Epoch 810, val loss: 1.0441290140151978
Epoch 820, training loss: 0.06982642412185669 = 0.004264328628778458 + 0.01 * 6.556209564208984
Epoch 820, val loss: 1.0494847297668457
Epoch 830, training loss: 0.06963478773832321 = 0.004135754890739918 + 0.01 * 6.54990291595459
Epoch 830, val loss: 1.0546166896820068
Epoch 840, training loss: 0.06957616657018661 = 0.004014330916106701 + 0.01 * 6.5561842918396
Epoch 840, val loss: 1.0597679615020752
Epoch 850, training loss: 0.06925830990076065 = 0.0038991663604974747 + 0.01 * 6.535914421081543
Epoch 850, val loss: 1.0647788047790527
Epoch 860, training loss: 0.06947561353445053 = 0.003789505222812295 + 0.01 * 6.568610668182373
Epoch 860, val loss: 1.069579839706421
Epoch 870, training loss: 0.06895840913057327 = 0.0036859018728137016 + 0.01 * 6.527251243591309
Epoch 870, val loss: 1.074461579322815
Epoch 880, training loss: 0.06901006400585175 = 0.0035872578155249357 + 0.01 * 6.542280673980713
Epoch 880, val loss: 1.079100251197815
Epoch 890, training loss: 0.06880664080381393 = 0.0034937006421387196 + 0.01 * 6.531293869018555
Epoch 890, val loss: 1.0837445259094238
Epoch 900, training loss: 0.068568654358387 = 0.0034044738858938217 + 0.01 * 6.516417980194092
Epoch 900, val loss: 1.0882835388183594
Epoch 910, training loss: 0.06857816129922867 = 0.003319173352792859 + 0.01 * 6.5258989334106445
Epoch 910, val loss: 1.0926717519760132
Epoch 920, training loss: 0.06833741813898087 = 0.0032382456120103598 + 0.01 * 6.509917736053467
Epoch 920, val loss: 1.0971051454544067
Epoch 930, training loss: 0.06837894022464752 = 0.003160597290843725 + 0.01 * 6.521834850311279
Epoch 930, val loss: 1.101378321647644
Epoch 940, training loss: 0.06811981648206711 = 0.0030866165179759264 + 0.01 * 6.503320217132568
Epoch 940, val loss: 1.1056398153305054
Epoch 950, training loss: 0.06811496615409851 = 0.0030156101565808058 + 0.01 * 6.5099358558654785
Epoch 950, val loss: 1.1098109483718872
Epoch 960, training loss: 0.06815077364444733 = 0.0029477537609636784 + 0.01 * 6.520301818847656
Epoch 960, val loss: 1.1138648986816406
Epoch 970, training loss: 0.06780022382736206 = 0.002882684813812375 + 0.01 * 6.491754531860352
Epoch 970, val loss: 1.1178979873657227
Epoch 980, training loss: 0.06774034351110458 = 0.002820352790877223 + 0.01 * 6.491999626159668
Epoch 980, val loss: 1.1218180656433105
Epoch 990, training loss: 0.06761852651834488 = 0.0027605565264821053 + 0.01 * 6.485796928405762
Epoch 990, val loss: 1.1257244348526
Epoch 1000, training loss: 0.06760112941265106 = 0.0027032503858208656 + 0.01 * 6.489788055419922
Epoch 1000, val loss: 1.1295459270477295
Epoch 1010, training loss: 0.06759759783744812 = 0.002647987799718976 + 0.01 * 6.494961261749268
Epoch 1010, val loss: 1.1332931518554688
Epoch 1020, training loss: 0.06740637123584747 = 0.0025950251147150993 + 0.01 * 6.481135368347168
Epoch 1020, val loss: 1.137070655822754
Epoch 1030, training loss: 0.06737689673900604 = 0.002544374205172062 + 0.01 * 6.48325252532959
Epoch 1030, val loss: 1.1407389640808105
Epoch 1040, training loss: 0.06724167615175247 = 0.0024951165542006493 + 0.01 * 6.474656105041504
Epoch 1040, val loss: 1.1443136930465698
Epoch 1050, training loss: 0.067079558968544 = 0.0024481583386659622 + 0.01 * 6.463140487670898
Epoch 1050, val loss: 1.1479460000991821
Epoch 1060, training loss: 0.06708358973264694 = 0.0024026052560657263 + 0.01 * 6.468099117279053
Epoch 1060, val loss: 1.1514296531677246
Epoch 1070, training loss: 0.06691813468933105 = 0.002358827507123351 + 0.01 * 6.455931186676025
Epoch 1070, val loss: 1.1549233198165894
Epoch 1080, training loss: 0.06733302026987076 = 0.0023166846949607134 + 0.01 * 6.501634120941162
Epoch 1080, val loss: 1.1582810878753662
Epoch 1090, training loss: 0.06674066185951233 = 0.002276130486279726 + 0.01 * 6.446453094482422
Epoch 1090, val loss: 1.161656141281128
Epoch 1100, training loss: 0.06667441874742508 = 0.002237084787338972 + 0.01 * 6.443734169006348
Epoch 1100, val loss: 1.1650161743164062
Epoch 1110, training loss: 0.06682323664426804 = 0.002199210925027728 + 0.01 * 6.462402820587158
Epoch 1110, val loss: 1.1683096885681152
Epoch 1120, training loss: 0.06656051427125931 = 0.0021627757232636213 + 0.01 * 6.4397735595703125
Epoch 1120, val loss: 1.171529769897461
Epoch 1130, training loss: 0.0666077584028244 = 0.00212755030952394 + 0.01 * 6.448020935058594
Epoch 1130, val loss: 1.1747212409973145
Epoch 1140, training loss: 0.06687425822019577 = 0.0020935244392603636 + 0.01 * 6.478073596954346
Epoch 1140, val loss: 1.1778525114059448
Epoch 1150, training loss: 0.06635214388370514 = 0.002060672966763377 + 0.01 * 6.429146766662598
Epoch 1150, val loss: 1.1809499263763428
Epoch 1160, training loss: 0.0663728266954422 = 0.0020289227832108736 + 0.01 * 6.434391021728516
Epoch 1160, val loss: 1.1840680837631226
Epoch 1170, training loss: 0.06623080372810364 = 0.0019979621283710003 + 0.01 * 6.42328405380249
Epoch 1170, val loss: 1.1871027946472168
Epoch 1180, training loss: 0.06626613438129425 = 0.001968235708773136 + 0.01 * 6.429790019989014
Epoch 1180, val loss: 1.1901130676269531
Epoch 1190, training loss: 0.06609821319580078 = 0.001939444919116795 + 0.01 * 6.415876865386963
Epoch 1190, val loss: 1.1930769681930542
Epoch 1200, training loss: 0.06631595641374588 = 0.0019114847527816892 + 0.01 * 6.4404473304748535
Epoch 1200, val loss: 1.1959892511367798
Epoch 1210, training loss: 0.06604508310556412 = 0.0018844063160941005 + 0.01 * 6.416067600250244
Epoch 1210, val loss: 1.1988979578018188
Epoch 1220, training loss: 0.06633087247610092 = 0.0018581420881673694 + 0.01 * 6.447273254394531
Epoch 1220, val loss: 1.201725959777832
Epoch 1230, training loss: 0.06592381000518799 = 0.0018327642465010285 + 0.01 * 6.40910530090332
Epoch 1230, val loss: 1.2045515775680542
Epoch 1240, training loss: 0.06586828827857971 = 0.0018082968890666962 + 0.01 * 6.405999183654785
Epoch 1240, val loss: 1.20733642578125
Epoch 1250, training loss: 0.06581566482782364 = 0.0017841806402429938 + 0.01 * 6.403148174285889
Epoch 1250, val loss: 1.2100396156311035
Epoch 1260, training loss: 0.06579886376857758 = 0.001761217718012631 + 0.01 * 6.403764724731445
Epoch 1260, val loss: 1.2127976417541504
Epoch 1270, training loss: 0.06595053523778915 = 0.0017385194078087807 + 0.01 * 6.421201705932617
Epoch 1270, val loss: 1.2154916524887085
Epoch 1280, training loss: 0.06575348228216171 = 0.0017167741898447275 + 0.01 * 6.403670787811279
Epoch 1280, val loss: 1.2181286811828613
Epoch 1290, training loss: 0.06560227274894714 = 0.0016954658785834908 + 0.01 * 6.39068078994751
Epoch 1290, val loss: 1.2207273244857788
Epoch 1300, training loss: 0.06563696265220642 = 0.0016747278859838843 + 0.01 * 6.396224021911621
Epoch 1300, val loss: 1.2231711149215698
Epoch 1310, training loss: 0.06552595645189285 = 0.0016548465937376022 + 0.01 * 6.387110710144043
Epoch 1310, val loss: 1.2257025241851807
Epoch 1320, training loss: 0.06544733047485352 = 0.0016352428356185555 + 0.01 * 6.381208419799805
Epoch 1320, val loss: 1.2281461954116821
Epoch 1330, training loss: 0.0658118948340416 = 0.0016163319814950228 + 0.01 * 6.419556617736816
Epoch 1330, val loss: 1.2306052446365356
Epoch 1340, training loss: 0.06539580225944519 = 0.001597863039933145 + 0.01 * 6.379793643951416
Epoch 1340, val loss: 1.232959508895874
Epoch 1350, training loss: 0.06541047245264053 = 0.0015799389220774174 + 0.01 * 6.383053302764893
Epoch 1350, val loss: 1.2353236675262451
Epoch 1360, training loss: 0.06530186533927917 = 0.0015625300584360957 + 0.01 * 6.373933792114258
Epoch 1360, val loss: 1.2377082109451294
Epoch 1370, training loss: 0.06536313891410828 = 0.0015454764943569899 + 0.01 * 6.3817667961120605
Epoch 1370, val loss: 1.2400203943252563
Epoch 1380, training loss: 0.06521144509315491 = 0.0015290273586288095 + 0.01 * 6.368241786956787
Epoch 1380, val loss: 1.2423696517944336
Epoch 1390, training loss: 0.0650935173034668 = 0.0015128209488466382 + 0.01 * 6.358069896697998
Epoch 1390, val loss: 1.2446060180664062
Epoch 1400, training loss: 0.06516806036233902 = 0.0014971198979765177 + 0.01 * 6.36709451675415
Epoch 1400, val loss: 1.2469124794006348
Epoch 1410, training loss: 0.0651157796382904 = 0.0014817090705037117 + 0.01 * 6.363406658172607
Epoch 1410, val loss: 1.2491264343261719
Epoch 1420, training loss: 0.0649968758225441 = 0.0014669365482404828 + 0.01 * 6.352993965148926
Epoch 1420, val loss: 1.251373529434204
Epoch 1430, training loss: 0.06529416888952255 = 0.0014523083809763193 + 0.01 * 6.384185791015625
Epoch 1430, val loss: 1.2535380125045776
Epoch 1440, training loss: 0.0649569109082222 = 0.0014381591463461518 + 0.01 * 6.351875305175781
Epoch 1440, val loss: 1.25566828250885
Epoch 1450, training loss: 0.06534211337566376 = 0.0014243711484596133 + 0.01 * 6.3917741775512695
Epoch 1450, val loss: 1.2578153610229492
Epoch 1460, training loss: 0.06495990604162216 = 0.0014108008472248912 + 0.01 * 6.354910850524902
Epoch 1460, val loss: 1.2599360942840576
Epoch 1470, training loss: 0.06476626545190811 = 0.001397803658619523 + 0.01 * 6.336846351623535
Epoch 1470, val loss: 1.262055516242981
Epoch 1480, training loss: 0.0650581419467926 = 0.0013849230017513037 + 0.01 * 6.3673224449157715
Epoch 1480, val loss: 1.2641661167144775
Epoch 1490, training loss: 0.06479144841432571 = 0.0013723333831876516 + 0.01 * 6.341912269592285
Epoch 1490, val loss: 1.2661769390106201
Epoch 1500, training loss: 0.06518971920013428 = 0.0013601467944681644 + 0.01 * 6.382957458496094
Epoch 1500, val loss: 1.268146276473999
Epoch 1510, training loss: 0.06483618170022964 = 0.001348148682154715 + 0.01 * 6.348803520202637
Epoch 1510, val loss: 1.2701760530471802
Epoch 1520, training loss: 0.06462825834751129 = 0.0013365837512537837 + 0.01 * 6.32916784286499
Epoch 1520, val loss: 1.272248387336731
Epoch 1530, training loss: 0.0648716390132904 = 0.001325021032243967 + 0.01 * 6.3546624183654785
Epoch 1530, val loss: 1.2742253541946411
Epoch 1540, training loss: 0.0648043155670166 = 0.0013139008078724146 + 0.01 * 6.34904146194458
Epoch 1540, val loss: 1.2761505842208862
Epoch 1550, training loss: 0.06468089669942856 = 0.001303084660321474 + 0.01 * 6.3377814292907715
Epoch 1550, val loss: 1.2781201601028442
Epoch 1560, training loss: 0.06475687772035599 = 0.00129233137704432 + 0.01 * 6.346454620361328
Epoch 1560, val loss: 1.280094861984253
Epoch 1570, training loss: 0.06458631902933121 = 0.0012820007978007197 + 0.01 * 6.330432415008545
Epoch 1570, val loss: 1.2820731401443481
Epoch 1580, training loss: 0.06460602581501007 = 0.0012716406490653753 + 0.01 * 6.333438396453857
Epoch 1580, val loss: 1.283926010131836
Epoch 1590, training loss: 0.06451835483312607 = 0.0012617855099961162 + 0.01 * 6.325656890869141
Epoch 1590, val loss: 1.2859127521514893
Epoch 1600, training loss: 0.06446006149053574 = 0.0012520018499344587 + 0.01 * 6.32080602645874
Epoch 1600, val loss: 1.2878305912017822
Epoch 1610, training loss: 0.06471341103315353 = 0.0012424737215042114 + 0.01 * 6.34709358215332
Epoch 1610, val loss: 1.2897459268569946
Epoch 1620, training loss: 0.06448028236627579 = 0.001233009505085647 + 0.01 * 6.3247270584106445
Epoch 1620, val loss: 1.2915165424346924
Epoch 1630, training loss: 0.06438292562961578 = 0.0012239419156685472 + 0.01 * 6.3158979415893555
Epoch 1630, val loss: 1.2933753728866577
Epoch 1640, training loss: 0.064534030854702 = 0.001214830670505762 + 0.01 * 6.3319196701049805
Epoch 1640, val loss: 1.2951775789260864
Epoch 1650, training loss: 0.06442523002624512 = 0.0012061359593644738 + 0.01 * 6.321909427642822
Epoch 1650, val loss: 1.2970384359359741
Epoch 1660, training loss: 0.06447404623031616 = 0.00119752564933151 + 0.01 * 6.327652454376221
Epoch 1660, val loss: 1.2988154888153076
Epoch 1670, training loss: 0.0646580383181572 = 0.0011889627203345299 + 0.01 * 6.346907615661621
Epoch 1670, val loss: 1.3005064725875854
Epoch 1680, training loss: 0.06449921429157257 = 0.00118078978266567 + 0.01 * 6.33184289932251
Epoch 1680, val loss: 1.3022323846817017
Epoch 1690, training loss: 0.06420223414897919 = 0.001172725111246109 + 0.01 * 6.302950382232666
Epoch 1690, val loss: 1.303948163986206
Epoch 1700, training loss: 0.0644102394580841 = 0.0011647919891402125 + 0.01 * 6.324544429779053
Epoch 1700, val loss: 1.3056470155715942
Epoch 1710, training loss: 0.06423398107290268 = 0.001156967249698937 + 0.01 * 6.307701110839844
Epoch 1710, val loss: 1.307309865951538
Epoch 1720, training loss: 0.06421885639429092 = 0.0011494150385260582 + 0.01 * 6.306944370269775
Epoch 1720, val loss: 1.3089510202407837
Epoch 1730, training loss: 0.06424383074045181 = 0.0011418439680710435 + 0.01 * 6.3101983070373535
Epoch 1730, val loss: 1.3105541467666626
Epoch 1740, training loss: 0.06419198960065842 = 0.0011344527592882514 + 0.01 * 6.305753231048584
Epoch 1740, val loss: 1.3121659755706787
Epoch 1750, training loss: 0.06420888006687164 = 0.001127331517636776 + 0.01 * 6.308154582977295
Epoch 1750, val loss: 1.3138172626495361
Epoch 1760, training loss: 0.06413373351097107 = 0.0011201183078810573 + 0.01 * 6.301361560821533
Epoch 1760, val loss: 1.3152998685836792
Epoch 1770, training loss: 0.06412336230278015 = 0.0011132805375382304 + 0.01 * 6.301008224487305
Epoch 1770, val loss: 1.316947102546692
Epoch 1780, training loss: 0.06407975405454636 = 0.0011064106365665793 + 0.01 * 6.297334671020508
Epoch 1780, val loss: 1.3184577226638794
Epoch 1790, training loss: 0.06395479291677475 = 0.0010997444624081254 + 0.01 * 6.285505294799805
Epoch 1790, val loss: 1.3199963569641113
Epoch 1800, training loss: 0.0641273483633995 = 0.0010931786382570863 + 0.01 * 6.303417205810547
Epoch 1800, val loss: 1.3215044736862183
Epoch 1810, training loss: 0.06399620324373245 = 0.0010866810334846377 + 0.01 * 6.290952205657959
Epoch 1810, val loss: 1.322952389717102
Epoch 1820, training loss: 0.06400810182094574 = 0.0010803317418321967 + 0.01 * 6.292776584625244
Epoch 1820, val loss: 1.3244311809539795
Epoch 1830, training loss: 0.06393355876207352 = 0.001074125524610281 + 0.01 * 6.285943508148193
Epoch 1830, val loss: 1.3258990049362183
Epoch 1840, training loss: 0.06393181532621384 = 0.0010679587721824646 + 0.01 * 6.286385536193848
Epoch 1840, val loss: 1.3273040056228638
Epoch 1850, training loss: 0.06391912698745728 = 0.001061964314430952 + 0.01 * 6.285716533660889
Epoch 1850, val loss: 1.3287194967269897
Epoch 1860, training loss: 0.06388287991285324 = 0.001056001172401011 + 0.01 * 6.282688140869141
Epoch 1860, val loss: 1.330159306526184
Epoch 1870, training loss: 0.0638299509882927 = 0.0010502137010917068 + 0.01 * 6.2779741287231445
Epoch 1870, val loss: 1.3315662145614624
Epoch 1880, training loss: 0.06391438096761703 = 0.0010444466024637222 + 0.01 * 6.286993503570557
Epoch 1880, val loss: 1.3329843282699585
Epoch 1890, training loss: 0.0638512521982193 = 0.0010388350347056985 + 0.01 * 6.2812418937683105
Epoch 1890, val loss: 1.3344306945800781
Epoch 1900, training loss: 0.06402374804019928 = 0.0010332642123103142 + 0.01 * 6.299048900604248
Epoch 1900, val loss: 1.335814118385315
Epoch 1910, training loss: 0.06384602934122086 = 0.0010277809342369437 + 0.01 * 6.281825542449951
Epoch 1910, val loss: 1.3371846675872803
Epoch 1920, training loss: 0.06382445245981216 = 0.0010224179131910205 + 0.01 * 6.280203342437744
Epoch 1920, val loss: 1.338578701019287
Epoch 1930, training loss: 0.06390254944562912 = 0.0010170688619837165 + 0.01 * 6.288547992706299
Epoch 1930, val loss: 1.3399471044540405
Epoch 1940, training loss: 0.06385627388954163 = 0.0010118139907717705 + 0.01 * 6.284446716308594
Epoch 1940, val loss: 1.3412847518920898
Epoch 1950, training loss: 0.06368393450975418 = 0.0010067297844216228 + 0.01 * 6.267720699310303
Epoch 1950, val loss: 1.3426477909088135
Epoch 1960, training loss: 0.06392135471105576 = 0.0010016515152528882 + 0.01 * 6.291970729827881
Epoch 1960, val loss: 1.3440144062042236
Epoch 1970, training loss: 0.0637180507183075 = 0.000996583141386509 + 0.01 * 6.272146701812744
Epoch 1970, val loss: 1.3452187776565552
Epoch 1980, training loss: 0.06371992081403732 = 0.0009917300194501877 + 0.01 * 6.2728190422058105
Epoch 1980, val loss: 1.3464975357055664
Epoch 1990, training loss: 0.06379353255033493 = 0.000986859668046236 + 0.01 * 6.280667781829834
Epoch 1990, val loss: 1.3477938175201416
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.7196
Flip ASR: 0.6667/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0287024974823 = 1.9449639320373535 + 0.01 * 8.37385082244873
Epoch 0, val loss: 1.9411958456039429
Epoch 10, training loss: 2.01816725730896 = 1.9344298839569092 + 0.01 * 8.373746871948242
Epoch 10, val loss: 1.9305752515792847
Epoch 20, training loss: 2.0049662590026855 = 1.921231985092163 + 0.01 * 8.373429298400879
Epoch 20, val loss: 1.9168466329574585
Epoch 30, training loss: 1.9864321947097778 = 1.9027072191238403 + 0.01 * 8.37249755859375
Epoch 30, val loss: 1.8972588777542114
Epoch 40, training loss: 1.9591200351715088 = 1.8754373788833618 + 0.01 * 8.368268966674805
Epoch 40, val loss: 1.8685429096221924
Epoch 50, training loss: 1.9205719232559204 = 1.837160348892212 + 0.01 * 8.34115982055664
Epoch 50, val loss: 1.8298970460891724
Epoch 60, training loss: 1.8743828535079956 = 1.7925821542739868 + 0.01 * 8.180071830749512
Epoch 60, val loss: 1.7887191772460938
Epoch 70, training loss: 1.8259849548339844 = 1.7483055591583252 + 0.01 * 7.767941951751709
Epoch 70, val loss: 1.750252366065979
Epoch 80, training loss: 1.7656322717666626 = 1.6917022466659546 + 0.01 * 7.392999649047852
Epoch 80, val loss: 1.699473261833191
Epoch 90, training loss: 1.6872735023498535 = 1.615399956703186 + 0.01 * 7.1873602867126465
Epoch 90, val loss: 1.6328340768814087
Epoch 100, training loss: 1.5913102626800537 = 1.5205392837524414 + 0.01 * 7.077101230621338
Epoch 100, val loss: 1.5532068014144897
Epoch 110, training loss: 1.4901713132858276 = 1.4199122190475464 + 0.01 * 7.025914192199707
Epoch 110, val loss: 1.4713479280471802
Epoch 120, training loss: 1.3939778804779053 = 1.3240041732788086 + 0.01 * 6.997368812561035
Epoch 120, val loss: 1.3994712829589844
Epoch 130, training loss: 1.3034390211105347 = 1.2336848974227905 + 0.01 * 6.975417137145996
Epoch 130, val loss: 1.3356834650039673
Epoch 140, training loss: 1.215340256690979 = 1.1458544731140137 + 0.01 * 6.948577880859375
Epoch 140, val loss: 1.2748326063156128
Epoch 150, training loss: 1.127893090248108 = 1.0587366819381714 + 0.01 * 6.91563606262207
Epoch 150, val loss: 1.2143914699554443
Epoch 160, training loss: 1.041033387184143 = 0.972248911857605 + 0.01 * 6.878448963165283
Epoch 160, val loss: 1.153717279434204
Epoch 170, training loss: 0.9563840627670288 = 0.8879194855690002 + 0.01 * 6.84645938873291
Epoch 170, val loss: 1.0936522483825684
Epoch 180, training loss: 0.8760716319084167 = 0.8077716827392578 + 0.01 * 6.829996585845947
Epoch 180, val loss: 1.0360411405563354
Epoch 190, training loss: 0.801700234413147 = 0.7334638237953186 + 0.01 * 6.823638439178467
Epoch 190, val loss: 0.9825995564460754
Epoch 200, training loss: 0.7341197729110718 = 0.665917694568634 + 0.01 * 6.8202104568481445
Epoch 200, val loss: 0.9345440864562988
Epoch 210, training loss: 0.6728004217147827 = 0.6046145558357239 + 0.01 * 6.818587303161621
Epoch 210, val loss: 0.8919047713279724
Epoch 220, training loss: 0.6163738965988159 = 0.5482022166252136 + 0.01 * 6.817169666290283
Epoch 220, val loss: 0.8540337681770325
Epoch 230, training loss: 0.563673734664917 = 0.49551820755004883 + 0.01 * 6.815556049346924
Epoch 230, val loss: 0.8201062083244324
Epoch 240, training loss: 0.514180600643158 = 0.44604435563087463 + 0.01 * 6.813626289367676
Epoch 240, val loss: 0.7901507616043091
Epoch 250, training loss: 0.46810275316238403 = 0.3999899923801422 + 0.01 * 6.811276435852051
Epoch 250, val loss: 0.7648105621337891
Epoch 260, training loss: 0.4258972704410553 = 0.3578052818775177 + 0.01 * 6.809198379516602
Epoch 260, val loss: 0.7443615794181824
Epoch 270, training loss: 0.38771936297416687 = 0.31965285539627075 + 0.01 * 6.8066511154174805
Epoch 270, val loss: 0.7292681932449341
Epoch 280, training loss: 0.3534245789051056 = 0.285369873046875 + 0.01 * 6.805469989776611
Epoch 280, val loss: 0.7195067405700684
Epoch 290, training loss: 0.32264843583106995 = 0.25463080406188965 + 0.01 * 6.801764011383057
Epoch 290, val loss: 0.7146863341331482
Epoch 300, training loss: 0.2951622009277344 = 0.22717072069644928 + 0.01 * 6.799149513244629
Epoch 300, val loss: 0.714273989200592
Epoch 310, training loss: 0.2707196772098541 = 0.20276249945163727 + 0.01 * 6.795717716217041
Epoch 310, val loss: 0.7177362442016602
Epoch 320, training loss: 0.24917814135551453 = 0.18124787509441376 + 0.01 * 6.793027400970459
Epoch 320, val loss: 0.7245320677757263
Epoch 330, training loss: 0.23029065132141113 = 0.16241274774074554 + 0.01 * 6.787790298461914
Epoch 330, val loss: 0.7340759634971619
Epoch 340, training loss: 0.2139270156621933 = 0.14604145288467407 + 0.01 * 6.7885565757751465
Epoch 340, val loss: 0.7456983327865601
Epoch 350, training loss: 0.19955679774284363 = 0.13176871836185455 + 0.01 * 6.778809070587158
Epoch 350, val loss: 0.7586442828178406
Epoch 360, training loss: 0.18709832429885864 = 0.11936403065919876 + 0.01 * 6.773428440093994
Epoch 360, val loss: 0.7727012038230896
Epoch 370, training loss: 0.1762760877609253 = 0.10859354585409164 + 0.01 * 6.768253803253174
Epoch 370, val loss: 0.787372887134552
Epoch 380, training loss: 0.16677525639533997 = 0.09912389516830444 + 0.01 * 6.765135288238525
Epoch 380, val loss: 0.8022127747535706
Epoch 390, training loss: 0.15828409790992737 = 0.09070688486099243 + 0.01 * 6.757721424102783
Epoch 390, val loss: 0.8171951770782471
Epoch 400, training loss: 0.15072549879550934 = 0.08321959525346756 + 0.01 * 6.7505903244018555
Epoch 400, val loss: 0.8324201703071594
Epoch 410, training loss: 0.14396455883979797 = 0.07653989642858505 + 0.01 * 6.742465972900391
Epoch 410, val loss: 0.8475785851478577
Epoch 420, training loss: 0.13791553676128387 = 0.07055646926164627 + 0.01 * 6.735907077789307
Epoch 420, val loss: 0.8626015186309814
Epoch 430, training loss: 0.13258026540279388 = 0.06511438637971878 + 0.01 * 6.746588230133057
Epoch 430, val loss: 0.8774267435073853
Epoch 440, training loss: 0.1273655891418457 = 0.0601285919547081 + 0.01 * 6.723699569702148
Epoch 440, val loss: 0.8922171592712402
Epoch 450, training loss: 0.12268517911434174 = 0.055531442165374756 + 0.01 * 6.715373516082764
Epoch 450, val loss: 0.9067006707191467
Epoch 460, training loss: 0.11830182373523712 = 0.05125460401177406 + 0.01 * 6.704721450805664
Epoch 460, val loss: 0.9210690259933472
Epoch 470, training loss: 0.11425898224115372 = 0.047286972403526306 + 0.01 * 6.697201251983643
Epoch 470, val loss: 0.93515545129776
Epoch 480, training loss: 0.11053799837827682 = 0.043578311800956726 + 0.01 * 6.6959686279296875
Epoch 480, val loss: 0.9489449858665466
Epoch 490, training loss: 0.1075395792722702 = 0.040094517171382904 + 0.01 * 6.744506359100342
Epoch 490, val loss: 0.9626446962356567
Epoch 500, training loss: 0.1036301925778389 = 0.036831729114055634 + 0.01 * 6.67984676361084
Epoch 500, val loss: 0.9760555624961853
Epoch 510, training loss: 0.10054212808609009 = 0.033717911690473557 + 0.01 * 6.6824212074279785
Epoch 510, val loss: 0.9896200895309448
Epoch 520, training loss: 0.09746617078781128 = 0.030757836997509003 + 0.01 * 6.670833587646484
Epoch 520, val loss: 1.0026131868362427
Epoch 530, training loss: 0.0947178527712822 = 0.028044017031788826 + 0.01 * 6.667383670806885
Epoch 530, val loss: 1.0154818296432495
Epoch 540, training loss: 0.0923413336277008 = 0.025549914687871933 + 0.01 * 6.679141998291016
Epoch 540, val loss: 1.02834153175354
Epoch 550, training loss: 0.08988960832357407 = 0.02330763079226017 + 0.01 * 6.658198356628418
Epoch 550, val loss: 1.0403443574905396
Epoch 560, training loss: 0.08782011270523071 = 0.021263424307107925 + 0.01 * 6.655669212341309
Epoch 560, val loss: 1.0525381565093994
Epoch 570, training loss: 0.08592955023050308 = 0.019424540922045708 + 0.01 * 6.650501251220703
Epoch 570, val loss: 1.0641881227493286
Epoch 580, training loss: 0.08467559516429901 = 0.017784520983695984 + 0.01 * 6.689107894897461
Epoch 580, val loss: 1.0761092901229858
Epoch 590, training loss: 0.0829363614320755 = 0.01637614704668522 + 0.01 * 6.656021595001221
Epoch 590, val loss: 1.0876423120498657
Epoch 600, training loss: 0.08159633725881577 = 0.015162559226155281 + 0.01 * 6.643378257751465
Epoch 600, val loss: 1.0989139080047607
Epoch 610, training loss: 0.08045722544193268 = 0.014093082398176193 + 0.01 * 6.636414051055908
Epoch 610, val loss: 1.1101148128509521
Epoch 620, training loss: 0.0797201469540596 = 0.013144752942025661 + 0.01 * 6.657539367675781
Epoch 620, val loss: 1.1207612752914429
Epoch 630, training loss: 0.07864875346422195 = 0.012296856380999088 + 0.01 * 6.635190010070801
Epoch 630, val loss: 1.1312495470046997
Epoch 640, training loss: 0.07780315726995468 = 0.011538982391357422 + 0.01 * 6.626417636871338
Epoch 640, val loss: 1.1414082050323486
Epoch 650, training loss: 0.07703975588083267 = 0.01085386611521244 + 0.01 * 6.618589401245117
Epoch 650, val loss: 1.1514582633972168
Epoch 660, training loss: 0.07664268463850021 = 0.010231435298919678 + 0.01 * 6.641124725341797
Epoch 660, val loss: 1.1610645055770874
Epoch 670, training loss: 0.07583557814359665 = 0.00966802891343832 + 0.01 * 6.61675500869751
Epoch 670, val loss: 1.1705251932144165
Epoch 680, training loss: 0.07522137463092804 = 0.009151873178780079 + 0.01 * 6.606950283050537
Epoch 680, val loss: 1.1797442436218262
Epoch 690, training loss: 0.07528954744338989 = 0.008680330589413643 + 0.01 * 6.660922050476074
Epoch 690, val loss: 1.1885521411895752
Epoch 700, training loss: 0.07443664222955704 = 0.008250401355326176 + 0.01 * 6.618624210357666
Epoch 700, val loss: 1.1971923112869263
Epoch 710, training loss: 0.07389173656702042 = 0.007855904288589954 + 0.01 * 6.603582859039307
Epoch 710, val loss: 1.2056077718734741
Epoch 720, training loss: 0.07342591136693954 = 0.007491354364901781 + 0.01 * 6.5934553146362305
Epoch 720, val loss: 1.2137539386749268
Epoch 730, training loss: 0.07309798151254654 = 0.007153843995183706 + 0.01 * 6.594413757324219
Epoch 730, val loss: 1.2217212915420532
Epoch 740, training loss: 0.07282182574272156 = 0.006840287707746029 + 0.01 * 6.598153591156006
Epoch 740, val loss: 1.229539394378662
Epoch 750, training loss: 0.07249876111745834 = 0.006548888981342316 + 0.01 * 6.594987392425537
Epoch 750, val loss: 1.2370208501815796
Epoch 760, training loss: 0.07202812284231186 = 0.006277835462242365 + 0.01 * 6.575028896331787
Epoch 760, val loss: 1.2442725896835327
Epoch 770, training loss: 0.07185599952936172 = 0.0060256896540522575 + 0.01 * 6.58303165435791
Epoch 770, val loss: 1.2514835596084595
Epoch 780, training loss: 0.07152889668941498 = 0.005790865980088711 + 0.01 * 6.573803424835205
Epoch 780, val loss: 1.2583738565444946
Epoch 790, training loss: 0.07129012793302536 = 0.005571913439780474 + 0.01 * 6.571821212768555
Epoch 790, val loss: 1.2651958465576172
Epoch 800, training loss: 0.07101543992757797 = 0.005366643890738487 + 0.01 * 6.564879417419434
Epoch 800, val loss: 1.2717818021774292
Epoch 810, training loss: 0.0708136036992073 = 0.005174610298126936 + 0.01 * 6.563899993896484
Epoch 810, val loss: 1.27829909324646
Epoch 820, training loss: 0.07063300162553787 = 0.004994411952793598 + 0.01 * 6.563858985900879
Epoch 820, val loss: 1.2845579385757446
Epoch 830, training loss: 0.07038982212543488 = 0.0048246681690216064 + 0.01 * 6.556515216827393
Epoch 830, val loss: 1.2906103134155273
Epoch 840, training loss: 0.07018408179283142 = 0.004665067885071039 + 0.01 * 6.551901340484619
Epoch 840, val loss: 1.29664945602417
Epoch 850, training loss: 0.06999579071998596 = 0.004513996187597513 + 0.01 * 6.548180103302002
Epoch 850, val loss: 1.302490234375
Epoch 860, training loss: 0.06979644298553467 = 0.004371326416730881 + 0.01 * 6.542511463165283
Epoch 860, val loss: 1.308256983757019
Epoch 870, training loss: 0.06964459270238876 = 0.004236535169184208 + 0.01 * 6.540806293487549
Epoch 870, val loss: 1.313913106918335
Epoch 880, training loss: 0.06958220154047012 = 0.004109168890863657 + 0.01 * 6.547303676605225
Epoch 880, val loss: 1.3194466829299927
Epoch 890, training loss: 0.0693131536245346 = 0.003988435957580805 + 0.01 * 6.532472133636475
Epoch 890, val loss: 1.3247991800308228
Epoch 900, training loss: 0.0692766085267067 = 0.0038739782758057117 + 0.01 * 6.5402631759643555
Epoch 900, val loss: 1.3300917148590088
Epoch 910, training loss: 0.06913979351520538 = 0.0037657490465790033 + 0.01 * 6.537405014038086
Epoch 910, val loss: 1.335289478302002
Epoch 920, training loss: 0.06894505023956299 = 0.003662422299385071 + 0.01 * 6.528263092041016
Epoch 920, val loss: 1.3402631282806396
Epoch 930, training loss: 0.06871920824050903 = 0.0035645542666316032 + 0.01 * 6.51546573638916
Epoch 930, val loss: 1.3452597856521606
Epoch 940, training loss: 0.06875572353601456 = 0.003471069037914276 + 0.01 * 6.528465270996094
Epoch 940, val loss: 1.3501026630401611
Epoch 950, training loss: 0.06851837784051895 = 0.003382461378350854 + 0.01 * 6.513591766357422
Epoch 950, val loss: 1.3548299074172974
Epoch 960, training loss: 0.06842782348394394 = 0.0032976872753351927 + 0.01 * 6.51301383972168
Epoch 960, val loss: 1.359458088874817
Epoch 970, training loss: 0.0683913454413414 = 0.0032167043536901474 + 0.01 * 6.5174641609191895
Epoch 970, val loss: 1.3640562295913696
Epoch 980, training loss: 0.0681309625506401 = 0.0031394336838275194 + 0.01 * 6.499153137207031
Epoch 980, val loss: 1.368463158607483
Epoch 990, training loss: 0.06822019070386887 = 0.0030656044837087393 + 0.01 * 6.515459060668945
Epoch 990, val loss: 1.3728631734848022
Epoch 1000, training loss: 0.06804612278938293 = 0.002995201153680682 + 0.01 * 6.505092144012451
Epoch 1000, val loss: 1.377152681350708
Epoch 1010, training loss: 0.06803809851408005 = 0.002927538938820362 + 0.01 * 6.511055946350098
Epoch 1010, val loss: 1.3813449144363403
Epoch 1020, training loss: 0.06784413754940033 = 0.0028627945575863123 + 0.01 * 6.498134136199951
Epoch 1020, val loss: 1.3855042457580566
Epoch 1030, training loss: 0.06782221794128418 = 0.0028006175998598337 + 0.01 * 6.50216007232666
Epoch 1030, val loss: 1.3895536661148071
Epoch 1040, training loss: 0.0676504597067833 = 0.00274133263155818 + 0.01 * 6.490912914276123
Epoch 1040, val loss: 1.3935389518737793
Epoch 1050, training loss: 0.06754778325557709 = 0.002684172708541155 + 0.01 * 6.486361026763916
Epoch 1050, val loss: 1.3974745273590088
Epoch 1060, training loss: 0.06755705922842026 = 0.0026294481940567493 + 0.01 * 6.49276065826416
Epoch 1060, val loss: 1.4013009071350098
Epoch 1070, training loss: 0.06752635538578033 = 0.0025769690982997417 + 0.01 * 6.494938850402832
Epoch 1070, val loss: 1.4051311016082764
Epoch 1080, training loss: 0.06738533824682236 = 0.002526345429942012 + 0.01 * 6.485899925231934
Epoch 1080, val loss: 1.4087473154067993
Epoch 1090, training loss: 0.06731079518795013 = 0.0024777238722890615 + 0.01 * 6.483307838439941
Epoch 1090, val loss: 1.41242516040802
Epoch 1100, training loss: 0.067287378013134 = 0.002431087661534548 + 0.01 * 6.485629558563232
Epoch 1100, val loss: 1.4159462451934814
Epoch 1110, training loss: 0.06717317551374435 = 0.002385916421189904 + 0.01 * 6.478725910186768
Epoch 1110, val loss: 1.4194515943527222
Epoch 1120, training loss: 0.06706542521715164 = 0.0023425891995429993 + 0.01 * 6.472283840179443
Epoch 1120, val loss: 1.4228724241256714
Epoch 1130, training loss: 0.066990427672863 = 0.0023006589617580175 + 0.01 * 6.468976974487305
Epoch 1130, val loss: 1.4263017177581787
Epoch 1140, training loss: 0.0669846460223198 = 0.0022603017278015614 + 0.01 * 6.4724345207214355
Epoch 1140, val loss: 1.4295674562454224
Epoch 1150, training loss: 0.06704258918762207 = 0.002221497241407633 + 0.01 * 6.482109546661377
Epoch 1150, val loss: 1.432855248451233
Epoch 1160, training loss: 0.0668165385723114 = 0.002183814998716116 + 0.01 * 6.463272571563721
Epoch 1160, val loss: 1.4359498023986816
Epoch 1170, training loss: 0.06677383929491043 = 0.0021476715337485075 + 0.01 * 6.462616443634033
Epoch 1170, val loss: 1.4391447305679321
Epoch 1180, training loss: 0.06671637296676636 = 0.0021126349456608295 + 0.01 * 6.460373878479004
Epoch 1180, val loss: 1.4421277046203613
Epoch 1190, training loss: 0.06674274802207947 = 0.0020787962712347507 + 0.01 * 6.466395854949951
Epoch 1190, val loss: 1.4452699422836304
Epoch 1200, training loss: 0.06674139201641083 = 0.0020462307147681713 + 0.01 * 6.469516754150391
Epoch 1200, val loss: 1.4481942653656006
Epoch 1210, training loss: 0.06646052747964859 = 0.0020145848393440247 + 0.01 * 6.444594383239746
Epoch 1210, val loss: 1.4511442184448242
Epoch 1220, training loss: 0.06673527508974075 = 0.001984081929549575 + 0.01 * 6.475119590759277
Epoch 1220, val loss: 1.454024314880371
Epoch 1230, training loss: 0.06657499819993973 = 0.0019545305985957384 + 0.01 * 6.4620466232299805
Epoch 1230, val loss: 1.4568713903427124
Epoch 1240, training loss: 0.06636743992567062 = 0.0019258171087130904 + 0.01 * 6.444162368774414
Epoch 1240, val loss: 1.4596933126449585
Epoch 1250, training loss: 0.06640716642141342 = 0.0018979096785187721 + 0.01 * 6.450925827026367
Epoch 1250, val loss: 1.4624356031417847
Epoch 1260, training loss: 0.0664260983467102 = 0.0018709644209593534 + 0.01 * 6.4555134773254395
Epoch 1260, val loss: 1.4651715755462646
Epoch 1270, training loss: 0.06618303805589676 = 0.001844783895649016 + 0.01 * 6.4338250160217285
Epoch 1270, val loss: 1.4677824974060059
Epoch 1280, training loss: 0.06616421788930893 = 0.0018194097792729735 + 0.01 * 6.434481143951416
Epoch 1280, val loss: 1.470521092414856
Epoch 1290, training loss: 0.06615624576807022 = 0.001795052899979055 + 0.01 * 6.436119079589844
Epoch 1290, val loss: 1.4730995893478394
Epoch 1300, training loss: 0.06600595265626907 = 0.0017711950931698084 + 0.01 * 6.423475742340088
Epoch 1300, val loss: 1.4756593704223633
Epoch 1310, training loss: 0.06598681211471558 = 0.0017481128452345729 + 0.01 * 6.423869609832764
Epoch 1310, val loss: 1.4781904220581055
Epoch 1320, training loss: 0.06604014337062836 = 0.0017257329309359193 + 0.01 * 6.431440830230713
Epoch 1320, val loss: 1.4806785583496094
Epoch 1330, training loss: 0.06587496399879456 = 0.0017039821250364184 + 0.01 * 6.417098045349121
Epoch 1330, val loss: 1.4831373691558838
Epoch 1340, training loss: 0.06600773334503174 = 0.0016827922081574798 + 0.01 * 6.432494163513184
Epoch 1340, val loss: 1.4856147766113281
Epoch 1350, training loss: 0.06588137894868851 = 0.0016623466508463025 + 0.01 * 6.421903610229492
Epoch 1350, val loss: 1.4880355596542358
Epoch 1360, training loss: 0.06587230414152145 = 0.0016422726912423968 + 0.01 * 6.423003673553467
Epoch 1360, val loss: 1.4904004335403442
Epoch 1370, training loss: 0.06597840040922165 = 0.0016228952445089817 + 0.01 * 6.435551166534424
Epoch 1370, val loss: 1.4927258491516113
Epoch 1380, training loss: 0.06567002087831497 = 0.001604069722816348 + 0.01 * 6.406594753265381
Epoch 1380, val loss: 1.4951003789901733
Epoch 1390, training loss: 0.06565696001052856 = 0.0015856692334637046 + 0.01 * 6.407129287719727
Epoch 1390, val loss: 1.497438907623291
Epoch 1400, training loss: 0.06567055732011795 = 0.001567855360917747 + 0.01 * 6.4102702140808105
Epoch 1400, val loss: 1.4997823238372803
Epoch 1410, training loss: 0.06575579196214676 = 0.001550522749312222 + 0.01 * 6.42052698135376
Epoch 1410, val loss: 1.5019716024398804
Epoch 1420, training loss: 0.06568161398172379 = 0.0015336091164499521 + 0.01 * 6.414800643920898
Epoch 1420, val loss: 1.504331111907959
Epoch 1430, training loss: 0.06544986367225647 = 0.0015172016574069858 + 0.01 * 6.393266677856445
Epoch 1430, val loss: 1.5065116882324219
Epoch 1440, training loss: 0.06553778052330017 = 0.0015012272633612156 + 0.01 * 6.403656005859375
Epoch 1440, val loss: 1.508776307106018
Epoch 1450, training loss: 0.06558587402105331 = 0.001485668122768402 + 0.01 * 6.41002082824707
Epoch 1450, val loss: 1.5109537839889526
Epoch 1460, training loss: 0.06565943360328674 = 0.0014704603236168623 + 0.01 * 6.41889762878418
Epoch 1460, val loss: 1.5130778551101685
Epoch 1470, training loss: 0.06549736112356186 = 0.0014557420508936048 + 0.01 * 6.404162406921387
Epoch 1470, val loss: 1.5151761770248413
Epoch 1480, training loss: 0.06536437571048737 = 0.0014413213357329369 + 0.01 * 6.392305850982666
Epoch 1480, val loss: 1.517309546470642
Epoch 1490, training loss: 0.06540045142173767 = 0.001427224138751626 + 0.01 * 6.397322654724121
Epoch 1490, val loss: 1.5193932056427002
Epoch 1500, training loss: 0.06545914709568024 = 0.001413596561178565 + 0.01 * 6.404554843902588
Epoch 1500, val loss: 1.5214250087738037
Epoch 1510, training loss: 0.0653270035982132 = 0.0014002304524183273 + 0.01 * 6.392677307128906
Epoch 1510, val loss: 1.5234496593475342
Epoch 1520, training loss: 0.06538037210702896 = 0.0013872429262846708 + 0.01 * 6.399312496185303
Epoch 1520, val loss: 1.5255011320114136
Epoch 1530, training loss: 0.06522635370492935 = 0.0013744706520810723 + 0.01 * 6.385189056396484
Epoch 1530, val loss: 1.52748703956604
Epoch 1540, training loss: 0.06513792276382446 = 0.001362027833238244 + 0.01 * 6.377590179443359
Epoch 1540, val loss: 1.529467225074768
Epoch 1550, training loss: 0.0652502030134201 = 0.0013498620828613639 + 0.01 * 6.3900346755981445
Epoch 1550, val loss: 1.5314044952392578
Epoch 1560, training loss: 0.06531625986099243 = 0.0013380763120949268 + 0.01 * 6.397818088531494
Epoch 1560, val loss: 1.5333863496780396
Epoch 1570, training loss: 0.06508579850196838 = 0.0013264227891340852 + 0.01 * 6.375938415527344
Epoch 1570, val loss: 1.5353317260742188
Epoch 1580, training loss: 0.0649787113070488 = 0.0013150874292477965 + 0.01 * 6.366362571716309
Epoch 1580, val loss: 1.5372333526611328
Epoch 1590, training loss: 0.0652015209197998 = 0.0013039723271504045 + 0.01 * 6.3897552490234375
Epoch 1590, val loss: 1.5391488075256348
Epoch 1600, training loss: 0.06526505202054977 = 0.001293195760808885 + 0.01 * 6.397185325622559
Epoch 1600, val loss: 1.5410048961639404
Epoch 1610, training loss: 0.06499770283699036 = 0.0012825904414057732 + 0.01 * 6.371511936187744
Epoch 1610, val loss: 1.5428566932678223
Epoch 1620, training loss: 0.0649246796965599 = 0.0012721745297312737 + 0.01 * 6.365250587463379
Epoch 1620, val loss: 1.5446581840515137
Epoch 1630, training loss: 0.06507950276136398 = 0.001262076897546649 + 0.01 * 6.38174295425415
Epoch 1630, val loss: 1.5465110540390015
Epoch 1640, training loss: 0.06498528271913528 = 0.001252099173143506 + 0.01 * 6.373318672180176
Epoch 1640, val loss: 1.5483129024505615
Epoch 1650, training loss: 0.06490159779787064 = 0.0012423945590853691 + 0.01 * 6.365920543670654
Epoch 1650, val loss: 1.5501235723495483
Epoch 1660, training loss: 0.06492789834737778 = 0.001232875743880868 + 0.01 * 6.369502544403076
Epoch 1660, val loss: 1.5519740581512451
Epoch 1670, training loss: 0.0649370551109314 = 0.0012235824251547456 + 0.01 * 6.371347904205322
Epoch 1670, val loss: 1.5537004470825195
Epoch 1680, training loss: 0.06476752460002899 = 0.0012145156506448984 + 0.01 * 6.355300426483154
Epoch 1680, val loss: 1.5554914474487305
Epoch 1690, training loss: 0.06500618159770966 = 0.0012055456172674894 + 0.01 * 6.380063533782959
Epoch 1690, val loss: 1.5572431087493896
Epoch 1700, training loss: 0.06501185148954391 = 0.001196913537569344 + 0.01 * 6.381494045257568
Epoch 1700, val loss: 1.5589994192123413
Epoch 1710, training loss: 0.06487912684679031 = 0.0011883398983627558 + 0.01 * 6.369079113006592
Epoch 1710, val loss: 1.5606385469436646
Epoch 1720, training loss: 0.06476838141679764 = 0.0011799578787758946 + 0.01 * 6.358842849731445
Epoch 1720, val loss: 1.562393069267273
Epoch 1730, training loss: 0.06464435160160065 = 0.0011716735316440463 + 0.01 * 6.347268581390381
Epoch 1730, val loss: 1.5641138553619385
Epoch 1740, training loss: 0.06470704823732376 = 0.0011636405251920223 + 0.01 * 6.35434103012085
Epoch 1740, val loss: 1.5658169984817505
Epoch 1750, training loss: 0.06452980637550354 = 0.001155736157670617 + 0.01 * 6.33740758895874
Epoch 1750, val loss: 1.5674158334732056
Epoch 1760, training loss: 0.06467483192682266 = 0.0011479506501927972 + 0.01 * 6.352688789367676
Epoch 1760, val loss: 1.5691909790039062
Epoch 1770, training loss: 0.06484709680080414 = 0.0011404795804992318 + 0.01 * 6.37066125869751
Epoch 1770, val loss: 1.5707699060440063
Epoch 1780, training loss: 0.06451276689767838 = 0.0011329344706609845 + 0.01 * 6.337983131408691
Epoch 1780, val loss: 1.5724196434020996
Epoch 1790, training loss: 0.0646965503692627 = 0.0011256649158895016 + 0.01 * 6.357088565826416
Epoch 1790, val loss: 1.57406485080719
Epoch 1800, training loss: 0.06441833078861237 = 0.0011185136390849948 + 0.01 * 6.329982280731201
Epoch 1800, val loss: 1.575750708580017
Epoch 1810, training loss: 0.0644916370511055 = 0.0011115471133962274 + 0.01 * 6.338009357452393
Epoch 1810, val loss: 1.5774245262145996
Epoch 1820, training loss: 0.06452685594558716 = 0.0011046247091144323 + 0.01 * 6.342223167419434
Epoch 1820, val loss: 1.5789769887924194
Epoch 1830, training loss: 0.0643371045589447 = 0.001097891479730606 + 0.01 * 6.323921203613281
Epoch 1830, val loss: 1.5806281566619873
Epoch 1840, training loss: 0.06446376442909241 = 0.0010912857251241803 + 0.01 * 6.337247848510742
Epoch 1840, val loss: 1.5822606086730957
Epoch 1850, training loss: 0.06458094716072083 = 0.0010847685625776649 + 0.01 * 6.349618434906006
Epoch 1850, val loss: 1.5838556289672852
Epoch 1860, training loss: 0.06438358128070831 = 0.0010784146143123507 + 0.01 * 6.330516815185547
Epoch 1860, val loss: 1.585419774055481
Epoch 1870, training loss: 0.06432592868804932 = 0.0010721130529418588 + 0.01 * 6.325381278991699
Epoch 1870, val loss: 1.5869879722595215
Epoch 1880, training loss: 0.06437569856643677 = 0.0010660121915861964 + 0.01 * 6.330968379974365
Epoch 1880, val loss: 1.588529348373413
Epoch 1890, training loss: 0.06437510251998901 = 0.0010599512606859207 + 0.01 * 6.331515312194824
Epoch 1890, val loss: 1.5900555849075317
Epoch 1900, training loss: 0.0641661062836647 = 0.0010539875365793705 + 0.01 * 6.311211585998535
Epoch 1900, val loss: 1.5915462970733643
Epoch 1910, training loss: 0.06420460343360901 = 0.0010481334757059813 + 0.01 * 6.315647125244141
Epoch 1910, val loss: 1.593092441558838
Epoch 1920, training loss: 0.06445452570915222 = 0.0010424420470371842 + 0.01 * 6.3412089347839355
Epoch 1920, val loss: 1.5946117639541626
Epoch 1930, training loss: 0.06414801627397537 = 0.0010367132490500808 + 0.01 * 6.311130523681641
Epoch 1930, val loss: 1.5960613489151
Epoch 1940, training loss: 0.06416933983564377 = 0.0010311458026990294 + 0.01 * 6.313819408416748
Epoch 1940, val loss: 1.5976041555404663
Epoch 1950, training loss: 0.06442498415708542 = 0.0010257448302581906 + 0.01 * 6.339923858642578
Epoch 1950, val loss: 1.599103569984436
Epoch 1960, training loss: 0.0641157403588295 = 0.0010203290730714798 + 0.01 * 6.30954122543335
Epoch 1960, val loss: 1.6005393266677856
Epoch 1970, training loss: 0.06406666338443756 = 0.0010150257730856538 + 0.01 * 6.305164337158203
Epoch 1970, val loss: 1.6021076440811157
Epoch 1980, training loss: 0.06421743333339691 = 0.0010099047794938087 + 0.01 * 6.32075309753418
Epoch 1980, val loss: 1.6035590171813965
Epoch 1990, training loss: 0.06411755084991455 = 0.0010046825045719743 + 0.01 * 6.3112874031066895
Epoch 1990, val loss: 1.605015754699707
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.7085
Flip ASR: 0.6667/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.025023937225342 = 1.9412851333618164 + 0.01 * 8.373871803283691
Epoch 0, val loss: 1.9389935731887817
Epoch 10, training loss: 2.015673875808716 = 1.9319359064102173 + 0.01 * 8.373805046081543
Epoch 10, val loss: 1.9295082092285156
Epoch 20, training loss: 2.0041136741638184 = 1.9203782081604004 + 0.01 * 8.37353801727295
Epoch 20, val loss: 1.9177321195602417
Epoch 30, training loss: 1.9876911640167236 = 1.9039641618728638 + 0.01 * 8.372705459594727
Epoch 30, val loss: 1.9010950326919556
Epoch 40, training loss: 1.9630000591278076 = 1.8793164491653442 + 0.01 * 8.368356704711914
Epoch 40, val loss: 1.8765414953231812
Epoch 50, training loss: 1.9269673824310303 = 1.8435587882995605 + 0.01 * 8.340856552124023
Epoch 50, val loss: 1.8424972295761108
Epoch 60, training loss: 1.8824293613433838 = 1.8001360893249512 + 0.01 * 8.22932243347168
Epoch 60, val loss: 1.8041167259216309
Epoch 70, training loss: 1.8382238149642944 = 1.7584149837493896 + 0.01 * 7.9808831214904785
Epoch 70, val loss: 1.7683786153793335
Epoch 80, training loss: 1.7851412296295166 = 1.7100036144256592 + 0.01 * 7.513760089874268
Epoch 80, val loss: 1.7235844135284424
Epoch 90, training loss: 1.7158719301223755 = 1.644108772277832 + 0.01 * 7.1763153076171875
Epoch 90, val loss: 1.6652716398239136
Epoch 100, training loss: 1.6297528743743896 = 1.5594342947006226 + 0.01 * 7.0318603515625
Epoch 100, val loss: 1.595519781112671
Epoch 110, training loss: 1.5298759937286377 = 1.4599411487579346 + 0.01 * 6.993483543395996
Epoch 110, val loss: 1.5123212337493896
Epoch 120, training loss: 1.4209728240966797 = 1.351449966430664 + 0.01 * 6.95228910446167
Epoch 120, val loss: 1.4208614826202393
Epoch 130, training loss: 1.306990385055542 = 1.2377023696899414 + 0.01 * 6.928804397583008
Epoch 130, val loss: 1.3255163431167603
Epoch 140, training loss: 1.192092776298523 = 1.1229450702667236 + 0.01 * 6.91477632522583
Epoch 140, val loss: 1.231705904006958
Epoch 150, training loss: 1.0822532176971436 = 1.0132319927215576 + 0.01 * 6.902123928070068
Epoch 150, val loss: 1.1442598104476929
Epoch 160, training loss: 0.9826112389564514 = 0.9137138724327087 + 0.01 * 6.8897385597229
Epoch 160, val loss: 1.0679794549942017
Epoch 170, training loss: 0.8945677876472473 = 0.8258013129234314 + 0.01 * 6.876649379730225
Epoch 170, val loss: 1.0037152767181396
Epoch 180, training loss: 0.8160847425460815 = 0.7474408745765686 + 0.01 * 6.864383697509766
Epoch 180, val loss: 0.949576735496521
Epoch 190, training loss: 0.743812620639801 = 0.6752699613571167 + 0.01 * 6.854267120361328
Epoch 190, val loss: 0.9025440216064453
Epoch 200, training loss: 0.6751748323440552 = 0.6067214608192444 + 0.01 * 6.845339775085449
Epoch 200, val loss: 0.8597797751426697
Epoch 210, training loss: 0.6092102527618408 = 0.5408380627632141 + 0.01 * 6.837218761444092
Epoch 210, val loss: 0.8204488158226013
Epoch 220, training loss: 0.5464259386062622 = 0.47812604904174805 + 0.01 * 6.829988479614258
Epoch 220, val loss: 0.7854676842689514
Epoch 230, training loss: 0.48806488513946533 = 0.4198298454284668 + 0.01 * 6.823505878448486
Epoch 230, val loss: 0.7557341456413269
Epoch 240, training loss: 0.43528658151626587 = 0.36711248755455017 + 0.01 * 6.817410945892334
Epoch 240, val loss: 0.7322537899017334
Epoch 250, training loss: 0.38856297731399536 = 0.32044655084609985 + 0.01 * 6.811642646789551
Epoch 250, val loss: 0.7150589823722839
Epoch 260, training loss: 0.3476337492465973 = 0.27957314252853394 + 0.01 * 6.806061744689941
Epoch 260, val loss: 0.7035962343215942
Epoch 270, training loss: 0.31200873851776123 = 0.24398912489414215 + 0.01 * 6.8019633293151855
Epoch 270, val loss: 0.697515070438385
Epoch 280, training loss: 0.28115081787109375 = 0.2131694257259369 + 0.01 * 6.798139572143555
Epoch 280, val loss: 0.6961961984634399
Epoch 290, training loss: 0.2545614242553711 = 0.18663188815116882 + 0.01 * 6.792953014373779
Epoch 290, val loss: 0.6990978717803955
Epoch 300, training loss: 0.23188260197639465 = 0.1639084666967392 + 0.01 * 6.797414779663086
Epoch 300, val loss: 0.7054067850112915
Epoch 310, training loss: 0.21232758462429047 = 0.144487664103508 + 0.01 * 6.783992290496826
Epoch 310, val loss: 0.7144012451171875
Epoch 320, training loss: 0.1956366002559662 = 0.12783053517341614 + 0.01 * 6.780606746673584
Epoch 320, val loss: 0.7253392338752747
Epoch 330, training loss: 0.18122215569019318 = 0.11347489058971405 + 0.01 * 6.774726390838623
Epoch 330, val loss: 0.7375834584236145
Epoch 340, training loss: 0.16874724626541138 = 0.10103190690279007 + 0.01 * 6.771534442901611
Epoch 340, val loss: 0.750755250453949
Epoch 350, training loss: 0.1578468382358551 = 0.09018762409687042 + 0.01 * 6.765921115875244
Epoch 350, val loss: 0.7644102573394775
Epoch 360, training loss: 0.148288756608963 = 0.08069001138210297 + 0.01 * 6.759875297546387
Epoch 360, val loss: 0.7783358693122864
Epoch 370, training loss: 0.13988688588142395 = 0.0723375454545021 + 0.01 * 6.754935264587402
Epoch 370, val loss: 0.7924283146858215
Epoch 380, training loss: 0.1324814260005951 = 0.06497042626142502 + 0.01 * 6.751099586486816
Epoch 380, val loss: 0.8064122200012207
Epoch 390, training loss: 0.12594158947467804 = 0.05845796316862106 + 0.01 * 6.748363018035889
Epoch 390, val loss: 0.8202910423278809
Epoch 400, training loss: 0.12010979652404785 = 0.05267990007996559 + 0.01 * 6.742989540100098
Epoch 400, val loss: 0.8341725468635559
Epoch 410, training loss: 0.11495105922222137 = 0.047551319003105164 + 0.01 * 6.739974498748779
Epoch 410, val loss: 0.8477316498756409
Epoch 420, training loss: 0.1103697270154953 = 0.042995352298021317 + 0.01 * 6.737438201904297
Epoch 420, val loss: 0.861162543296814
Epoch 430, training loss: 0.10627708584070206 = 0.0389510840177536 + 0.01 * 6.732600212097168
Epoch 430, val loss: 0.8744411468505859
Epoch 440, training loss: 0.1026613861322403 = 0.03536596521735191 + 0.01 * 6.729542255401611
Epoch 440, val loss: 0.8874059319496155
Epoch 450, training loss: 0.09945902228355408 = 0.03218917176127434 + 0.01 * 6.726985931396484
Epoch 450, val loss: 0.9001109600067139
Epoch 460, training loss: 0.09660425037145615 = 0.029374947771430016 + 0.01 * 6.722929954528809
Epoch 460, val loss: 0.9126003980636597
Epoch 470, training loss: 0.09407923370599747 = 0.02688073180615902 + 0.01 * 6.719850063323975
Epoch 470, val loss: 0.9247246384620667
Epoch 480, training loss: 0.0918501764535904 = 0.024667570367455482 + 0.01 * 6.718260765075684
Epoch 480, val loss: 0.9365440607070923
Epoch 490, training loss: 0.08984020352363586 = 0.02270084246993065 + 0.01 * 6.713935852050781
Epoch 490, val loss: 0.9480627775192261
Epoch 500, training loss: 0.08804697543382645 = 0.02094966173171997 + 0.01 * 6.709731578826904
Epoch 500, val loss: 0.9592956304550171
Epoch 510, training loss: 0.08652545511722565 = 0.01938653364777565 + 0.01 * 6.713892936706543
Epoch 510, val loss: 0.9701612591743469
Epoch 520, training loss: 0.08503975719213486 = 0.017985796555876732 + 0.01 * 6.7053961753845215
Epoch 520, val loss: 0.9808069467544556
Epoch 530, training loss: 0.08374392986297607 = 0.01672910340130329 + 0.01 * 6.701482772827148
Epoch 530, val loss: 0.991144597530365
Epoch 540, training loss: 0.0825669914484024 = 0.015598312951624393 + 0.01 * 6.696867942810059
Epoch 540, val loss: 1.0011786222457886
Epoch 550, training loss: 0.08155132085084915 = 0.014577517285943031 + 0.01 * 6.697380542755127
Epoch 550, val loss: 1.010941982269287
Epoch 560, training loss: 0.0805610716342926 = 0.013653794303536415 + 0.01 * 6.690727710723877
Epoch 560, val loss: 1.020416021347046
Epoch 570, training loss: 0.07971552014350891 = 0.012815390713512897 + 0.01 * 6.690012454986572
Epoch 570, val loss: 1.0296541452407837
Epoch 580, training loss: 0.07888823747634888 = 0.012052539736032486 + 0.01 * 6.683569431304932
Epoch 580, val loss: 1.038604974746704
Epoch 590, training loss: 0.07822076231241226 = 0.011356174014508724 + 0.01 * 6.686459064483643
Epoch 590, val loss: 1.0473065376281738
Epoch 600, training loss: 0.07753197103738785 = 0.010720362886786461 + 0.01 * 6.681160926818848
Epoch 600, val loss: 1.0558054447174072
Epoch 610, training loss: 0.07690155506134033 = 0.010138047859072685 + 0.01 * 6.6763505935668945
Epoch 610, val loss: 1.0640653371810913
Epoch 620, training loss: 0.07633165270090103 = 0.009603006765246391 + 0.01 * 6.67286491394043
Epoch 620, val loss: 1.072042465209961
Epoch 630, training loss: 0.07579352706670761 = 0.009109809063374996 + 0.01 * 6.668371677398682
Epoch 630, val loss: 1.0798488855361938
Epoch 640, training loss: 0.07531075179576874 = 0.008655021898448467 + 0.01 * 6.665573596954346
Epoch 640, val loss: 1.087484359741211
Epoch 650, training loss: 0.07485010474920273 = 0.008235073648393154 + 0.01 * 6.661503314971924
Epoch 650, val loss: 1.0948994159698486
Epoch 660, training loss: 0.07442008703947067 = 0.007846450433135033 + 0.01 * 6.6573638916015625
Epoch 660, val loss: 1.102164387702942
Epoch 670, training loss: 0.07403305917978287 = 0.00748602207750082 + 0.01 * 6.6547040939331055
Epoch 670, val loss: 1.1092175245285034
Epoch 680, training loss: 0.07365566492080688 = 0.007151465397328138 + 0.01 * 6.650420665740967
Epoch 680, val loss: 1.116094946861267
Epoch 690, training loss: 0.07339421659708023 = 0.006840339861810207 + 0.01 * 6.655387878417969
Epoch 690, val loss: 1.122795820236206
Epoch 700, training loss: 0.07298090308904648 = 0.006550717633217573 + 0.01 * 6.643019199371338
Epoch 700, val loss: 1.1293845176696777
Epoch 710, training loss: 0.07263888418674469 = 0.006280539091676474 + 0.01 * 6.635834217071533
Epoch 710, val loss: 1.135684847831726
Epoch 720, training loss: 0.07251735031604767 = 0.0060280500911176205 + 0.01 * 6.648930549621582
Epoch 720, val loss: 1.1419035196304321
Epoch 730, training loss: 0.07206548750400543 = 0.005791975185275078 + 0.01 * 6.6273512840271
Epoch 730, val loss: 1.1480387449264526
Epoch 740, training loss: 0.07181030511856079 = 0.005570847075432539 + 0.01 * 6.623946189880371
Epoch 740, val loss: 1.1539490222930908
Epoch 750, training loss: 0.07173667848110199 = 0.005363225471228361 + 0.01 * 6.637345314025879
Epoch 750, val loss: 1.1596957445144653
Epoch 760, training loss: 0.07137418538331985 = 0.00516829639673233 + 0.01 * 6.620588779449463
Epoch 760, val loss: 1.1653528213500977
Epoch 770, training loss: 0.0710858479142189 = 0.004985009785741568 + 0.01 * 6.610084056854248
Epoch 770, val loss: 1.1708178520202637
Epoch 780, training loss: 0.07109010964632034 = 0.004812590777873993 + 0.01 * 6.627752304077148
Epoch 780, val loss: 1.1760709285736084
Epoch 790, training loss: 0.07074015587568283 = 0.0046503846533596516 + 0.01 * 6.608977317810059
Epoch 790, val loss: 1.1812806129455566
Epoch 800, training loss: 0.07047030329704285 = 0.004497458692640066 + 0.01 * 6.597284317016602
Epoch 800, val loss: 1.1864311695098877
Epoch 810, training loss: 0.07050573080778122 = 0.004352651536464691 + 0.01 * 6.615307807922363
Epoch 810, val loss: 1.1913163661956787
Epoch 820, training loss: 0.07006765902042389 = 0.004215912893414497 + 0.01 * 6.585175037384033
Epoch 820, val loss: 1.1961568593978882
Epoch 830, training loss: 0.06998869776725769 = 0.004086589440703392 + 0.01 * 6.590210437774658
Epoch 830, val loss: 1.2008723020553589
Epoch 840, training loss: 0.06974637508392334 = 0.0039640492759644985 + 0.01 * 6.578232765197754
Epoch 840, val loss: 1.205384373664856
Epoch 850, training loss: 0.06984704732894897 = 0.003848079824820161 + 0.01 * 6.599896430969238
Epoch 850, val loss: 1.2097984552383423
Epoch 860, training loss: 0.06950187683105469 = 0.0037382282316684723 + 0.01 * 6.576364517211914
Epoch 860, val loss: 1.214233160018921
Epoch 870, training loss: 0.0692613273859024 = 0.0036338893696665764 + 0.01 * 6.562744140625
Epoch 870, val loss: 1.2183386087417603
Epoch 880, training loss: 0.06932436674833298 = 0.0035347414668649435 + 0.01 * 6.578962802886963
Epoch 880, val loss: 1.222387433052063
Epoch 890, training loss: 0.06893603503704071 = 0.0034405176993459463 + 0.01 * 6.5495524406433105
Epoch 890, val loss: 1.2264914512634277
Epoch 900, training loss: 0.0689210519194603 = 0.0033507603220641613 + 0.01 * 6.557029724121094
Epoch 900, val loss: 1.2303510904312134
Epoch 910, training loss: 0.06870274245738983 = 0.0032652628142386675 + 0.01 * 6.543748378753662
Epoch 910, val loss: 1.2341963052749634
Epoch 920, training loss: 0.06880166381597519 = 0.003183715045452118 + 0.01 * 6.561795234680176
Epoch 920, val loss: 1.2378113269805908
Epoch 930, training loss: 0.06867535412311554 = 0.003106052288785577 + 0.01 * 6.5569305419921875
Epoch 930, val loss: 1.241469383239746
Epoch 940, training loss: 0.0684208795428276 = 0.003031852887943387 + 0.01 * 6.538902759552002
Epoch 940, val loss: 1.2450196743011475
Epoch 950, training loss: 0.06821198016405106 = 0.0029609007760882378 + 0.01 * 6.525108337402344
Epoch 950, val loss: 1.2484917640686035
Epoch 960, training loss: 0.06816404312849045 = 0.002893022494390607 + 0.01 * 6.527101993560791
Epoch 960, val loss: 1.2517547607421875
Epoch 970, training loss: 0.06796611845493317 = 0.0028280646074563265 + 0.01 * 6.513805866241455
Epoch 970, val loss: 1.2550734281539917
Epoch 980, training loss: 0.06837215274572372 = 0.0027657481841742992 + 0.01 * 6.560641288757324
Epoch 980, val loss: 1.2582887411117554
Epoch 990, training loss: 0.06778891384601593 = 0.0027061719447374344 + 0.01 * 6.508274555206299
Epoch 990, val loss: 1.2614635229110718
Epoch 1000, training loss: 0.06768564879894257 = 0.0026490732561796904 + 0.01 * 6.503657341003418
Epoch 1000, val loss: 1.2645020484924316
Epoch 1010, training loss: 0.06767617166042328 = 0.0025941566564142704 + 0.01 * 6.508201599121094
Epoch 1010, val loss: 1.2675138711929321
Epoch 1020, training loss: 0.0676456093788147 = 0.0025414316914975643 + 0.01 * 6.510417938232422
Epoch 1020, val loss: 1.2703763246536255
Epoch 1030, training loss: 0.06744002550840378 = 0.002490855986252427 + 0.01 * 6.494917392730713
Epoch 1030, val loss: 1.2732876539230347
Epoch 1040, training loss: 0.06732818484306335 = 0.002442125463858247 + 0.01 * 6.488605499267578
Epoch 1040, val loss: 1.276073932647705
Epoch 1050, training loss: 0.06720931082963943 = 0.0023951937910169363 + 0.01 * 6.481412410736084
Epoch 1050, val loss: 1.2787429094314575
Epoch 1060, training loss: 0.06724053621292114 = 0.002350088208913803 + 0.01 * 6.4890456199646
Epoch 1060, val loss: 1.2814923524856567
Epoch 1070, training loss: 0.06714460253715515 = 0.0023066047579050064 + 0.01 * 6.483799934387207
Epoch 1070, val loss: 1.2840826511383057
Epoch 1080, training loss: 0.0670069232583046 = 0.0022646740544587374 + 0.01 * 6.474225044250488
Epoch 1080, val loss: 1.2865556478500366
Epoch 1090, training loss: 0.06705208122730255 = 0.002224450698122382 + 0.01 * 6.482763290405273
Epoch 1090, val loss: 1.2891420125961304
Epoch 1100, training loss: 0.06688991189002991 = 0.0021855111699551344 + 0.01 * 6.47044038772583
Epoch 1100, val loss: 1.2916370630264282
Epoch 1110, training loss: 0.06701621413230896 = 0.002147918799892068 + 0.01 * 6.48682975769043
Epoch 1110, val loss: 1.2939482927322388
Epoch 1120, training loss: 0.06693162024021149 = 0.002111646579578519 + 0.01 * 6.481997489929199
Epoch 1120, val loss: 1.2963924407958984
Epoch 1130, training loss: 0.06681209802627563 = 0.002076655626296997 + 0.01 * 6.473544120788574
Epoch 1130, val loss: 1.2987322807312012
Epoch 1140, training loss: 0.0666872039437294 = 0.0020427037961781025 + 0.01 * 6.464449882507324
Epoch 1140, val loss: 1.3009657859802246
Epoch 1150, training loss: 0.06661900877952576 = 0.0020100141409784555 + 0.01 * 6.460899353027344
Epoch 1150, val loss: 1.3032175302505493
Epoch 1160, training loss: 0.06679616868495941 = 0.001978395041078329 + 0.01 * 6.481777191162109
Epoch 1160, val loss: 1.3053938150405884
Epoch 1170, training loss: 0.06643985956907272 = 0.0019477804889902472 + 0.01 * 6.449207782745361
Epoch 1170, val loss: 1.3075625896453857
Epoch 1180, training loss: 0.06650193780660629 = 0.0019182071555405855 + 0.01 * 6.458373546600342
Epoch 1180, val loss: 1.3096610307693481
Epoch 1190, training loss: 0.06630886346101761 = 0.0018895480316132307 + 0.01 * 6.44193172454834
Epoch 1190, val loss: 1.311753749847412
Epoch 1200, training loss: 0.06638260185718536 = 0.0018617650493979454 + 0.01 * 6.452084064483643
Epoch 1200, val loss: 1.3136773109436035
Epoch 1210, training loss: 0.06631588935852051 = 0.0018348785815760493 + 0.01 * 6.448101043701172
Epoch 1210, val loss: 1.3156697750091553
Epoch 1220, training loss: 0.06615632772445679 = 0.0018088881624862552 + 0.01 * 6.434744358062744
Epoch 1220, val loss: 1.3176229000091553
Epoch 1230, training loss: 0.06615269929170609 = 0.0017836355837062001 + 0.01 * 6.436906337738037
Epoch 1230, val loss: 1.3194924592971802
Epoch 1240, training loss: 0.066165030002594 = 0.00175918685272336 + 0.01 * 6.440584182739258
Epoch 1240, val loss: 1.3213238716125488
Epoch 1250, training loss: 0.06608108431100845 = 0.0017354837618768215 + 0.01 * 6.4345598220825195
Epoch 1250, val loss: 1.3231748342514038
Epoch 1260, training loss: 0.06600300967693329 = 0.0017125047743320465 + 0.01 * 6.429049968719482
Epoch 1260, val loss: 1.3249834775924683
Epoch 1270, training loss: 0.06611306965351105 = 0.0016901360359042883 + 0.01 * 6.442293643951416
Epoch 1270, val loss: 1.3266898393630981
Epoch 1280, training loss: 0.0659027025103569 = 0.0016684973379597068 + 0.01 * 6.423420429229736
Epoch 1280, val loss: 1.3284308910369873
Epoch 1290, training loss: 0.06579111516475677 = 0.0016474637668579817 + 0.01 * 6.414365291595459
Epoch 1290, val loss: 1.330093502998352
Epoch 1300, training loss: 0.06585574895143509 = 0.0016270041232928634 + 0.01 * 6.422874450683594
Epoch 1300, val loss: 1.3316409587860107
Epoch 1310, training loss: 0.06574084609746933 = 0.0016071577556431293 + 0.01 * 6.4133687019348145
Epoch 1310, val loss: 1.3333141803741455
Epoch 1320, training loss: 0.06567380577325821 = 0.0015878539998084307 + 0.01 * 6.408595085144043
Epoch 1320, val loss: 1.3347829580307007
Epoch 1330, training loss: 0.06574547290802002 = 0.001569098443724215 + 0.01 * 6.417637825012207
Epoch 1330, val loss: 1.3363680839538574
Epoch 1340, training loss: 0.06558015942573547 = 0.0015508122742176056 + 0.01 * 6.40293550491333
Epoch 1340, val loss: 1.3378815650939941
Epoch 1350, training loss: 0.06558298319578171 = 0.001533009926788509 + 0.01 * 6.404997825622559
Epoch 1350, val loss: 1.339202642440796
Epoch 1360, training loss: 0.0655054822564125 = 0.0015158005990087986 + 0.01 * 6.398968696594238
Epoch 1360, val loss: 1.3407498598098755
Epoch 1370, training loss: 0.0656164288520813 = 0.001499048201367259 + 0.01 * 6.41173791885376
Epoch 1370, val loss: 1.3422026634216309
Epoch 1380, training loss: 0.0655001774430275 = 0.0014826422557234764 + 0.01 * 6.4017534255981445
Epoch 1380, val loss: 1.3435381650924683
Epoch 1390, training loss: 0.06533369421958923 = 0.0014666896313428879 + 0.01 * 6.386700630187988
Epoch 1390, val loss: 1.3448981046676636
Epoch 1400, training loss: 0.06553761661052704 = 0.0014511526096612215 + 0.01 * 6.408646583557129
Epoch 1400, val loss: 1.3461753129959106
Epoch 1410, training loss: 0.06538623571395874 = 0.0014360328204929829 + 0.01 * 6.395020484924316
Epoch 1410, val loss: 1.3475563526153564
Epoch 1420, training loss: 0.06528066843748093 = 0.001421255641616881 + 0.01 * 6.385941505432129
Epoch 1420, val loss: 1.3488410711288452
Epoch 1430, training loss: 0.06546180695295334 = 0.0014068474993109703 + 0.01 * 6.405496120452881
Epoch 1430, val loss: 1.3500324487686157
Epoch 1440, training loss: 0.0652763843536377 = 0.0013928799889981747 + 0.01 * 6.388350963592529
Epoch 1440, val loss: 1.351300835609436
Epoch 1450, training loss: 0.06512145698070526 = 0.0013791952515020967 + 0.01 * 6.3742265701293945
Epoch 1450, val loss: 1.3525359630584717
Epoch 1460, training loss: 0.06513925641775131 = 0.0013658649986609817 + 0.01 * 6.377339839935303
Epoch 1460, val loss: 1.353708267211914
Epoch 1470, training loss: 0.06512986123561859 = 0.001352836494334042 + 0.01 * 6.377702713012695
Epoch 1470, val loss: 1.3548184633255005
Epoch 1480, training loss: 0.06534845381975174 = 0.001340152695775032 + 0.01 * 6.4008307456970215
Epoch 1480, val loss: 1.3560367822647095
Epoch 1490, training loss: 0.06518121063709259 = 0.001327763544395566 + 0.01 * 6.385344982147217
Epoch 1490, val loss: 1.357208490371704
Epoch 1500, training loss: 0.06506229937076569 = 0.0013156072236597538 + 0.01 * 6.374669551849365
Epoch 1500, val loss: 1.358264684677124
Epoch 1510, training loss: 0.06510158628225327 = 0.0013038241304457188 + 0.01 * 6.379776954650879
Epoch 1510, val loss: 1.3593978881835938
Epoch 1520, training loss: 0.0649782195687294 = 0.0012922445312142372 + 0.01 * 6.368597507476807
Epoch 1520, val loss: 1.3604604005813599
Epoch 1530, training loss: 0.06493300199508667 = 0.0012809725012630224 + 0.01 * 6.365202903747559
Epoch 1530, val loss: 1.3615316152572632
Epoch 1540, training loss: 0.06498412787914276 = 0.0012698736973106861 + 0.01 * 6.371425151824951
Epoch 1540, val loss: 1.3623828887939453
Epoch 1550, training loss: 0.06505654752254486 = 0.0012591138947755098 + 0.01 * 6.379743576049805
Epoch 1550, val loss: 1.3634670972824097
Epoch 1560, training loss: 0.0651148185133934 = 0.00124859728384763 + 0.01 * 6.386621952056885
Epoch 1560, val loss: 1.3644300699234009
Epoch 1570, training loss: 0.06475026160478592 = 0.0012382304994389415 + 0.01 * 6.351203918457031
Epoch 1570, val loss: 1.3652925491333008
Epoch 1580, training loss: 0.06490911543369293 = 0.0012281602248549461 + 0.01 * 6.368095874786377
Epoch 1580, val loss: 1.3662511110305786
Epoch 1590, training loss: 0.06500684469938278 = 0.0012183297658339143 + 0.01 * 6.378851890563965
Epoch 1590, val loss: 1.3671425580978394
Epoch 1600, training loss: 0.06481686979532242 = 0.0012086696224287152 + 0.01 * 6.360820293426514
Epoch 1600, val loss: 1.3680516481399536
Epoch 1610, training loss: 0.06477370113134384 = 0.0011991813080385327 + 0.01 * 6.357452392578125
Epoch 1610, val loss: 1.3688597679138184
Epoch 1620, training loss: 0.06483187526464462 = 0.001189926639199257 + 0.01 * 6.364195346832275
Epoch 1620, val loss: 1.3696365356445312
Epoch 1630, training loss: 0.06465621292591095 = 0.0011809089919552207 + 0.01 * 6.347530364990234
Epoch 1630, val loss: 1.3705580234527588
Epoch 1640, training loss: 0.06462650001049042 = 0.0011719913454726338 + 0.01 * 6.3454508781433105
Epoch 1640, val loss: 1.3713188171386719
Epoch 1650, training loss: 0.06453206390142441 = 0.0011633180547505617 + 0.01 * 6.336874485015869
Epoch 1650, val loss: 1.3720998764038086
Epoch 1660, training loss: 0.06464572995901108 = 0.001154752098955214 + 0.01 * 6.349098205566406
Epoch 1660, val loss: 1.3728314638137817
Epoch 1670, training loss: 0.06503196060657501 = 0.0011464159470051527 + 0.01 * 6.388554573059082
Epoch 1670, val loss: 1.3734965324401855
Epoch 1680, training loss: 0.06456829607486725 = 0.0011382714146748185 + 0.01 * 6.343002796173096
Epoch 1680, val loss: 1.374345302581787
Epoch 1690, training loss: 0.06451495736837387 = 0.0011302513303235173 + 0.01 * 6.338470935821533
Epoch 1690, val loss: 1.375086784362793
Epoch 1700, training loss: 0.06459973007440567 = 0.0011223751353099942 + 0.01 * 6.347735404968262
Epoch 1700, val loss: 1.3756835460662842
Epoch 1710, training loss: 0.06457189470529556 = 0.001114730490371585 + 0.01 * 6.34571647644043
Epoch 1710, val loss: 1.3764454126358032
Epoch 1720, training loss: 0.06433794647455215 = 0.0011071868939325213 + 0.01 * 6.323076248168945
Epoch 1720, val loss: 1.3771893978118896
Epoch 1730, training loss: 0.06439094245433807 = 0.001099760876968503 + 0.01 * 6.329118251800537
Epoch 1730, val loss: 1.3778332471847534
Epoch 1740, training loss: 0.06453435868024826 = 0.001092499471269548 + 0.01 * 6.344185829162598
Epoch 1740, val loss: 1.3785158395767212
Epoch 1750, training loss: 0.06433667242527008 = 0.0010853508720174432 + 0.01 * 6.325132369995117
Epoch 1750, val loss: 1.379194974899292
Epoch 1760, training loss: 0.06431034952402115 = 0.001078321016393602 + 0.01 * 6.323203086853027
Epoch 1760, val loss: 1.3797928094863892
Epoch 1770, training loss: 0.0643603652715683 = 0.0010714398231357336 + 0.01 * 6.328892707824707
Epoch 1770, val loss: 1.3803993463516235
Epoch 1780, training loss: 0.06428024917840958 = 0.0010646912269294262 + 0.01 * 6.321556091308594
Epoch 1780, val loss: 1.3810302019119263
Epoch 1790, training loss: 0.06431145966053009 = 0.0010580403031781316 + 0.01 * 6.325341701507568
Epoch 1790, val loss: 1.3815785646438599
Epoch 1800, training loss: 0.06437800824642181 = 0.0010515322210267186 + 0.01 * 6.332647800445557
Epoch 1800, val loss: 1.3822271823883057
Epoch 1810, training loss: 0.064278244972229 = 0.001045112032443285 + 0.01 * 6.323313236236572
Epoch 1810, val loss: 1.382718563079834
Epoch 1820, training loss: 0.06411267071962357 = 0.0010387884685769677 + 0.01 * 6.3073883056640625
Epoch 1820, val loss: 1.383291482925415
Epoch 1830, training loss: 0.06411930173635483 = 0.0010326108895242214 + 0.01 * 6.308669090270996
Epoch 1830, val loss: 1.3838638067245483
Epoch 1840, training loss: 0.06424494832754135 = 0.0010265029268339276 + 0.01 * 6.321844577789307
Epoch 1840, val loss: 1.3843300342559814
Epoch 1850, training loss: 0.06419889628887177 = 0.001020542811602354 + 0.01 * 6.317835330963135
Epoch 1850, val loss: 1.384824275970459
Epoch 1860, training loss: 0.06416180729866028 = 0.0010146539425477386 + 0.01 * 6.3147149085998535
Epoch 1860, val loss: 1.385407567024231
Epoch 1870, training loss: 0.06404612213373184 = 0.001008894992992282 + 0.01 * 6.303722381591797
Epoch 1870, val loss: 1.3859139680862427
Epoch 1880, training loss: 0.06414478272199631 = 0.0010032275458797812 + 0.01 * 6.314155578613281
Epoch 1880, val loss: 1.386368751525879
Epoch 1890, training loss: 0.06402159482240677 = 0.0009976765140891075 + 0.01 * 6.302391529083252
Epoch 1890, val loss: 1.3869050741195679
Epoch 1900, training loss: 0.06412692368030548 = 0.0009922097669914365 + 0.01 * 6.313471794128418
Epoch 1900, val loss: 1.3873661756515503
Epoch 1910, training loss: 0.06423453986644745 = 0.0009868418565019965 + 0.01 * 6.324769496917725
Epoch 1910, val loss: 1.3879069089889526
Epoch 1920, training loss: 0.06391320377588272 = 0.0009815271478146315 + 0.01 * 6.293168067932129
Epoch 1920, val loss: 1.3883706331253052
Epoch 1930, training loss: 0.06388992071151733 = 0.0009763078414835036 + 0.01 * 6.291361331939697
Epoch 1930, val loss: 1.3887642621994019
Epoch 1940, training loss: 0.06402353942394257 = 0.0009711679304018617 + 0.01 * 6.305237293243408
Epoch 1940, val loss: 1.3892701864242554
Epoch 1950, training loss: 0.06390424072742462 = 0.0009661137009970844 + 0.01 * 6.293812274932861
Epoch 1950, val loss: 1.3897496461868286
Epoch 1960, training loss: 0.06381702423095703 = 0.0009611038258299232 + 0.01 * 6.285592079162598
Epoch 1960, val loss: 1.3901878595352173
Epoch 1970, training loss: 0.06380174309015274 = 0.0009562115301378071 + 0.01 * 6.284553050994873
Epoch 1970, val loss: 1.3906313180923462
Epoch 1980, training loss: 0.06408708542585373 = 0.0009514004923403263 + 0.01 * 6.313568592071533
Epoch 1980, val loss: 1.3911253213882446
Epoch 1990, training loss: 0.06391388177871704 = 0.0009466197807341814 + 0.01 * 6.296726226806641
Epoch 1990, val loss: 1.3914226293563843
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.8561
Flip ASR: 0.8267/225 nodes
The final ASR:0.76138, 0.06712, Accuracy:0.80988, 0.00630
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11610])
remove edge: torch.Size([2, 9476])
updated graph: torch.Size([2, 10530])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.97663, 0.00460, Accuracy:0.82963, 0.00524
Begin epxeriment: cont_weight: 0.01 epoch:2000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.023486852645874 = 1.9397478103637695 + 0.01 * 8.373899459838867
Epoch 0, val loss: 1.9402281045913696
Epoch 10, training loss: 2.0134429931640625 = 1.9297046661376953 + 0.01 * 8.373827934265137
Epoch 10, val loss: 1.9298346042633057
Epoch 20, training loss: 2.0007522106170654 = 1.9170165061950684 + 0.01 * 8.373564720153809
Epoch 20, val loss: 1.9164655208587646
Epoch 30, training loss: 1.9828675985336304 = 1.8991408348083496 + 0.01 * 8.372671127319336
Epoch 30, val loss: 1.8975610733032227
Epoch 40, training loss: 1.956704020500183 = 1.8730289936065674 + 0.01 * 8.36750316619873
Epoch 40, val loss: 1.8704406023025513
Epoch 50, training loss: 1.920902132987976 = 1.8375518321990967 + 0.01 * 8.335029602050781
Epoch 50, val loss: 1.8356337547302246
Epoch 60, training loss: 1.8811891078948975 = 1.799415111541748 + 0.01 * 8.177403450012207
Epoch 60, val loss: 1.8033106327056885
Epoch 70, training loss: 1.8418989181518555 = 1.7638450860977173 + 0.01 * 7.80537748336792
Epoch 70, val loss: 1.7768583297729492
Epoch 80, training loss: 1.7929925918579102 = 1.7163848876953125 + 0.01 * 7.660767078399658
Epoch 80, val loss: 1.7382031679153442
Epoch 90, training loss: 1.724729299545288 = 1.6500366926193237 + 0.01 * 7.4692606925964355
Epoch 90, val loss: 1.6819664239883423
Epoch 100, training loss: 1.6366277933120728 = 1.5643625259399414 + 0.01 * 7.226521968841553
Epoch 100, val loss: 1.610669732093811
Epoch 110, training loss: 1.5384447574615479 = 1.4689931869506836 + 0.01 * 6.945157527923584
Epoch 110, val loss: 1.5322604179382324
Epoch 120, training loss: 1.4405313730239868 = 1.3715744018554688 + 0.01 * 6.895694732666016
Epoch 120, val loss: 1.4542717933654785
Epoch 130, training loss: 1.341908574104309 = 1.273316502571106 + 0.01 * 6.8592119216918945
Epoch 130, val loss: 1.3784087896347046
Epoch 140, training loss: 1.2418287992477417 = 1.1734449863433838 + 0.01 * 6.838380813598633
Epoch 140, val loss: 1.3018847703933716
Epoch 150, training loss: 1.1397820711135864 = 1.0715174674987793 + 0.01 * 6.826460838317871
Epoch 150, val loss: 1.2238401174545288
Epoch 160, training loss: 1.0365197658538818 = 0.9683272242546082 + 0.01 * 6.819248676300049
Epoch 160, val loss: 1.1453578472137451
Epoch 170, training loss: 0.9346399903297424 = 0.8664891719818115 + 0.01 * 6.8150811195373535
Epoch 170, val loss: 1.0693752765655518
Epoch 180, training loss: 0.8382605910301208 = 0.7701207995414734 + 0.01 * 6.813981056213379
Epoch 180, val loss: 0.9992870092391968
Epoch 190, training loss: 0.7513418197631836 = 0.6831995844841003 + 0.01 * 6.814224720001221
Epoch 190, val loss: 0.9384840130805969
Epoch 200, training loss: 0.676135778427124 = 0.6079903244972229 + 0.01 * 6.814545154571533
Epoch 200, val loss: 0.888547956943512
Epoch 210, training loss: 0.6127275228500366 = 0.5445844531059265 + 0.01 * 6.814309597015381
Epoch 210, val loss: 0.8498475551605225
Epoch 220, training loss: 0.5593849420547485 = 0.49125000834465027 + 0.01 * 6.8134965896606445
Epoch 220, val loss: 0.8207555413246155
Epoch 230, training loss: 0.5136606097221375 = 0.4455406665802002 + 0.01 * 6.8119940757751465
Epoch 230, val loss: 0.7991081476211548
Epoch 240, training loss: 0.4732743203639984 = 0.40517458319664 + 0.01 * 6.809974193572998
Epoch 240, val loss: 0.7829370498657227
Epoch 250, training loss: 0.4366893768310547 = 0.36860790848731995 + 0.01 * 6.808145999908447
Epoch 250, val loss: 0.7711361646652222
Epoch 260, training loss: 0.40292248129844666 = 0.3348694443702698 + 0.01 * 6.805304527282715
Epoch 260, val loss: 0.763080358505249
Epoch 270, training loss: 0.37130746245384216 = 0.30328789353370667 + 0.01 * 6.801958084106445
Epoch 270, val loss: 0.7579740285873413
Epoch 280, training loss: 0.34128642082214355 = 0.2733045518398285 + 0.01 * 6.79818868637085
Epoch 280, val loss: 0.7551017999649048
Epoch 290, training loss: 0.3123244643211365 = 0.244384765625 + 0.01 * 6.793970584869385
Epoch 290, val loss: 0.7539545297622681
Epoch 300, training loss: 0.2840248942375183 = 0.21612302958965302 + 0.01 * 6.790186882019043
Epoch 300, val loss: 0.754125714302063
Epoch 310, training loss: 0.2564312517642975 = 0.18858976662158966 + 0.01 * 6.784148216247559
Epoch 310, val loss: 0.7554051280021667
Epoch 320, training loss: 0.23023253679275513 = 0.16245786845684052 + 0.01 * 6.777466297149658
Epoch 320, val loss: 0.7584322094917297
Epoch 330, training loss: 0.2064613401889801 = 0.13874134421348572 + 0.01 * 6.772000312805176
Epoch 330, val loss: 0.7636855244636536
Epoch 340, training loss: 0.18587695062160492 = 0.11822564154863358 + 0.01 * 6.765130996704102
Epoch 340, val loss: 0.7715469002723694
Epoch 350, training loss: 0.16863490641117096 = 0.10106387734413147 + 0.01 * 6.757102966308594
Epoch 350, val loss: 0.7818674445152283
Epoch 360, training loss: 0.1544654369354248 = 0.08693121373653412 + 0.01 * 6.753422737121582
Epoch 360, val loss: 0.7943578958511353
Epoch 370, training loss: 0.14277014136314392 = 0.07531426846981049 + 0.01 * 6.74558687210083
Epoch 370, val loss: 0.8085619211196899
Epoch 380, training loss: 0.1330963671207428 = 0.06572477519512177 + 0.01 * 6.7371602058410645
Epoch 380, val loss: 0.8238421678543091
Epoch 390, training loss: 0.12515023350715637 = 0.05776023492217064 + 0.01 * 6.73900032043457
Epoch 390, val loss: 0.8397625088691711
Epoch 400, training loss: 0.11839614808559418 = 0.05110124498605728 + 0.01 * 6.729490756988525
Epoch 400, val loss: 0.8559204936027527
Epoch 410, training loss: 0.11271519958972931 = 0.045493170619010925 + 0.01 * 6.722202777862549
Epoch 410, val loss: 0.8721562027931213
Epoch 420, training loss: 0.1079196184873581 = 0.04073649272322655 + 0.01 * 6.718312740325928
Epoch 420, val loss: 0.8881545662879944
Epoch 430, training loss: 0.10388452559709549 = 0.03667575865983963 + 0.01 * 6.720876693725586
Epoch 430, val loss: 0.9039591550827026
Epoch 440, training loss: 0.10029086470603943 = 0.033187638968229294 + 0.01 * 6.710322856903076
Epoch 440, val loss: 0.9193000197410583
Epoch 450, training loss: 0.09723920375108719 = 0.030170731246471405 + 0.01 * 6.706847667694092
Epoch 450, val loss: 0.9342750310897827
Epoch 460, training loss: 0.09456761926412582 = 0.027541719377040863 + 0.01 * 6.702590465545654
Epoch 460, val loss: 0.9488142132759094
Epoch 470, training loss: 0.09222107380628586 = 0.025234466418623924 + 0.01 * 6.698660850524902
Epoch 470, val loss: 0.9629652500152588
Epoch 480, training loss: 0.09016980230808258 = 0.02319672331213951 + 0.01 * 6.69730806350708
Epoch 480, val loss: 0.9767308831214905
Epoch 490, training loss: 0.08833073079586029 = 0.02139407768845558 + 0.01 * 6.693665504455566
Epoch 490, val loss: 0.9901095628738403
Epoch 500, training loss: 0.08667220920324326 = 0.019791675731539726 + 0.01 * 6.688053131103516
Epoch 500, val loss: 1.0031232833862305
Epoch 510, training loss: 0.08519871532917023 = 0.018362242728471756 + 0.01 * 6.683647632598877
Epoch 510, val loss: 1.0157077312469482
Epoch 520, training loss: 0.08386871963739395 = 0.017081141471862793 + 0.01 * 6.678758144378662
Epoch 520, val loss: 1.0280213356018066
Epoch 530, training loss: 0.08268047124147415 = 0.015930423513054848 + 0.01 * 6.6750054359436035
Epoch 530, val loss: 1.0399566888809204
Epoch 540, training loss: 0.08160533756017685 = 0.014893745072185993 + 0.01 * 6.671159267425537
Epoch 540, val loss: 1.0515960454940796
Epoch 550, training loss: 0.08061753213405609 = 0.013956633396446705 + 0.01 * 6.66609001159668
Epoch 550, val loss: 1.0628961324691772
Epoch 560, training loss: 0.07980751991271973 = 0.013106808997690678 + 0.01 * 6.670071601867676
Epoch 560, val loss: 1.0738412141799927
Epoch 570, training loss: 0.07889838516712189 = 0.012334463186562061 + 0.01 * 6.6563920974731445
Epoch 570, val loss: 1.0845929384231567
Epoch 580, training loss: 0.07815400511026382 = 0.011630873195827007 + 0.01 * 6.652313709259033
Epoch 580, val loss: 1.0948882102966309
Epoch 590, training loss: 0.07751339673995972 = 0.010988228023052216 + 0.01 * 6.652516841888428
Epoch 590, val loss: 1.1050095558166504
Epoch 600, training loss: 0.07690069824457169 = 0.010399985127151012 + 0.01 * 6.65007209777832
Epoch 600, val loss: 1.114851474761963
Epoch 610, training loss: 0.07625185698270798 = 0.009860200807452202 + 0.01 * 6.639165878295898
Epoch 610, val loss: 1.1244450807571411
Epoch 620, training loss: 0.07573072612285614 = 0.009362752549350262 + 0.01 * 6.636797904968262
Epoch 620, val loss: 1.1338341236114502
Epoch 630, training loss: 0.07536252588033676 = 0.008903895504772663 + 0.01 * 6.645863056182861
Epoch 630, val loss: 1.1428698301315308
Epoch 640, training loss: 0.07478669285774231 = 0.008480352349579334 + 0.01 * 6.63063383102417
Epoch 640, val loss: 1.151732087135315
Epoch 650, training loss: 0.07439271360635757 = 0.008088119328022003 + 0.01 * 6.630459308624268
Epoch 650, val loss: 1.1603285074234009
Epoch 660, training loss: 0.07392263412475586 = 0.007724456489086151 + 0.01 * 6.619818210601807
Epoch 660, val loss: 1.1688295602798462
Epoch 670, training loss: 0.07362271845340729 = 0.007387114223092794 + 0.01 * 6.623560428619385
Epoch 670, val loss: 1.1769589185714722
Epoch 680, training loss: 0.07319006323814392 = 0.00707339309155941 + 0.01 * 6.611666679382324
Epoch 680, val loss: 1.185006022453308
Epoch 690, training loss: 0.07286337018013 = 0.006780634634196758 + 0.01 * 6.608273983001709
Epoch 690, val loss: 1.1928883790969849
Epoch 700, training loss: 0.07251034677028656 = 0.006506942678242922 + 0.01 * 6.600340366363525
Epoch 700, val loss: 1.200561285018921
Epoch 710, training loss: 0.07219826430082321 = 0.006250429432839155 + 0.01 * 6.594783782958984
Epoch 710, val loss: 1.2080985307693481
Epoch 720, training loss: 0.07219330966472626 = 0.006009476725012064 + 0.01 * 6.618383407592773
Epoch 720, val loss: 1.2155078649520874
Epoch 730, training loss: 0.07165032625198364 = 0.00578394765034318 + 0.01 * 6.5866379737854
Epoch 730, val loss: 1.2227814197540283
Epoch 740, training loss: 0.07147710770368576 = 0.005571894347667694 + 0.01 * 6.590521335601807
Epoch 740, val loss: 1.229764699935913
Epoch 750, training loss: 0.07123667746782303 = 0.0053719631396234035 + 0.01 * 6.5864715576171875
Epoch 750, val loss: 1.2367717027664185
Epoch 760, training loss: 0.07098821550607681 = 0.005183944012969732 + 0.01 * 6.580427646636963
Epoch 760, val loss: 1.2435595989227295
Epoch 770, training loss: 0.07078307121992111 = 0.0050067901611328125 + 0.01 * 6.577628135681152
Epoch 770, val loss: 1.2501702308654785
Epoch 780, training loss: 0.07045668363571167 = 0.004839607048779726 + 0.01 * 6.561707496643066
Epoch 780, val loss: 1.2566993236541748
Epoch 790, training loss: 0.07053618133068085 = 0.0046815332025289536 + 0.01 * 6.585464954376221
Epoch 790, val loss: 1.2629412412643433
Epoch 800, training loss: 0.07021208107471466 = 0.004532333463430405 + 0.01 * 6.56797456741333
Epoch 800, val loss: 1.2692577838897705
Epoch 810, training loss: 0.07001379877328873 = 0.004390999674797058 + 0.01 * 6.56227970123291
Epoch 810, val loss: 1.2752537727355957
Epoch 820, training loss: 0.06990266591310501 = 0.004257078282535076 + 0.01 * 6.564559459686279
Epoch 820, val loss: 1.2813440561294556
Epoch 830, training loss: 0.0696553960442543 = 0.00413005705922842 + 0.01 * 6.5525336265563965
Epoch 830, val loss: 1.2870705127716064
Epoch 840, training loss: 0.06941553205251694 = 0.004009506199508905 + 0.01 * 6.540602684020996
Epoch 840, val loss: 1.2927780151367188
Epoch 850, training loss: 0.06932757049798965 = 0.003894927678629756 + 0.01 * 6.543264389038086
Epoch 850, val loss: 1.2984803915023804
Epoch 860, training loss: 0.06926216930150986 = 0.0037861079908907413 + 0.01 * 6.547605991363525
Epoch 860, val loss: 1.303951382637024
Epoch 870, training loss: 0.0690203607082367 = 0.003682659240439534 + 0.01 * 6.533770561218262
Epoch 870, val loss: 1.309340476989746
Epoch 880, training loss: 0.0688367411494255 = 0.0035840501077473164 + 0.01 * 6.525269031524658
Epoch 880, val loss: 1.314606785774231
Epoch 890, training loss: 0.06871706247329712 = 0.0034900144673883915 + 0.01 * 6.522704601287842
Epoch 890, val loss: 1.31978440284729
Epoch 900, training loss: 0.06876960396766663 = 0.0034000910818576813 + 0.01 * 6.536952018737793
Epoch 900, val loss: 1.3248363733291626
Epoch 910, training loss: 0.06864409148693085 = 0.0033149425871670246 + 0.01 * 6.532915115356445
Epoch 910, val loss: 1.3299965858459473
Epoch 920, training loss: 0.06837555766105652 = 0.0032337591983377934 + 0.01 * 6.5141801834106445
Epoch 920, val loss: 1.334729552268982
Epoch 930, training loss: 0.06831157952547073 = 0.0031560591887682676 + 0.01 * 6.515552043914795
Epoch 930, val loss: 1.3396021127700806
Epoch 940, training loss: 0.06810425966978073 = 0.0030816192738711834 + 0.01 * 6.502264022827148
Epoch 940, val loss: 1.3442732095718384
Epoch 950, training loss: 0.06845803558826447 = 0.003010320011526346 + 0.01 * 6.544772148132324
Epoch 950, val loss: 1.3488131761550903
Epoch 960, training loss: 0.06801588088274002 = 0.002942256396636367 + 0.01 * 6.507362365722656
Epoch 960, val loss: 1.3534777164459229
Epoch 970, training loss: 0.06803620606660843 = 0.0028768309857696295 + 0.01 * 6.515937805175781
Epoch 970, val loss: 1.3577574491500854
Epoch 980, training loss: 0.06786185503005981 = 0.002814201870933175 + 0.01 * 6.504765510559082
Epoch 980, val loss: 1.3622477054595947
Epoch 990, training loss: 0.06777535378932953 = 0.002754073590040207 + 0.01 * 6.5021281242370605
Epoch 990, val loss: 1.3664588928222656
Epoch 1000, training loss: 0.06757232546806335 = 0.002696508774533868 + 0.01 * 6.487581729888916
Epoch 1000, val loss: 1.3706796169281006
Epoch 1010, training loss: 0.06749317795038223 = 0.0026410764548927546 + 0.01 * 6.485210418701172
Epoch 1010, val loss: 1.374831199645996
Epoch 1020, training loss: 0.06739547103643417 = 0.0025877668522298336 + 0.01 * 6.480770587921143
Epoch 1020, val loss: 1.378921389579773
Epoch 1030, training loss: 0.0673280730843544 = 0.002536577405408025 + 0.01 * 6.47914981842041
Epoch 1030, val loss: 1.382954716682434
Epoch 1040, training loss: 0.06732296198606491 = 0.002487366320565343 + 0.01 * 6.4835591316223145
Epoch 1040, val loss: 1.386878252029419
Epoch 1050, training loss: 0.06714043766260147 = 0.0024401219561696053 + 0.01 * 6.470031261444092
Epoch 1050, val loss: 1.3908146619796753
Epoch 1060, training loss: 0.06715521961450577 = 0.0023947034496814013 + 0.01 * 6.4760518074035645
Epoch 1060, val loss: 1.3945871591567993
Epoch 1070, training loss: 0.0670296847820282 = 0.0023507331497967243 + 0.01 * 6.467895030975342
Epoch 1070, val loss: 1.3982312679290771
Epoch 1080, training loss: 0.0672612264752388 = 0.0023083954583853483 + 0.01 * 6.495283126831055
Epoch 1080, val loss: 1.401947021484375
Epoch 1090, training loss: 0.06699276715517044 = 0.002267710166051984 + 0.01 * 6.472505569458008
Epoch 1090, val loss: 1.405629277229309
Epoch 1100, training loss: 0.06699420511722565 = 0.0022283417638391256 + 0.01 * 6.476586818695068
Epoch 1100, val loss: 1.409188985824585
Epoch 1110, training loss: 0.0667748972773552 = 0.0021903698798269033 + 0.01 * 6.458453178405762
Epoch 1110, val loss: 1.4125900268554688
Epoch 1120, training loss: 0.06683854013681412 = 0.002153668086975813 + 0.01 * 6.468487739562988
Epoch 1120, val loss: 1.4160715341567993
Epoch 1130, training loss: 0.06664157658815384 = 0.0021182952914386988 + 0.01 * 6.452328205108643
Epoch 1130, val loss: 1.4194538593292236
Epoch 1140, training loss: 0.06660487502813339 = 0.0020841280929744244 + 0.01 * 6.452075004577637
Epoch 1140, val loss: 1.4227577447891235
Epoch 1150, training loss: 0.0668005719780922 = 0.0020510742906481028 + 0.01 * 6.474949836730957
Epoch 1150, val loss: 1.4260207414627075
Epoch 1160, training loss: 0.0665610060095787 = 0.0020190596114844084 + 0.01 * 6.45419454574585
Epoch 1160, val loss: 1.4291229248046875
Epoch 1170, training loss: 0.06671030819416046 = 0.0019881234038621187 + 0.01 * 6.472218990325928
Epoch 1170, val loss: 1.4324613809585571
Epoch 1180, training loss: 0.06647281348705292 = 0.0019582491368055344 + 0.01 * 6.451456546783447
Epoch 1180, val loss: 1.4353970289230347
Epoch 1190, training loss: 0.066392682492733 = 0.001929392572492361 + 0.01 * 6.446328639984131
Epoch 1190, val loss: 1.4383747577667236
Epoch 1200, training loss: 0.06629443913698196 = 0.0019012931734323502 + 0.01 * 6.439314842224121
Epoch 1200, val loss: 1.4413594007492065
Epoch 1210, training loss: 0.06625469028949738 = 0.0018741199746727943 + 0.01 * 6.4380574226379395
Epoch 1210, val loss: 1.4442559480667114
Epoch 1220, training loss: 0.06627163290977478 = 0.0018479026621207595 + 0.01 * 6.442373275756836
Epoch 1220, val loss: 1.4471393823623657
Epoch 1230, training loss: 0.0666234940290451 = 0.0018224027007818222 + 0.01 * 6.480108737945557
Epoch 1230, val loss: 1.4499824047088623
Epoch 1240, training loss: 0.06614073365926743 = 0.001797779113985598 + 0.01 * 6.434295654296875
Epoch 1240, val loss: 1.452783465385437
Epoch 1250, training loss: 0.06606750935316086 = 0.0017738437745720148 + 0.01 * 6.4293670654296875
Epoch 1250, val loss: 1.4553812742233276
Epoch 1260, training loss: 0.06617508828639984 = 0.001750592957250774 + 0.01 * 6.442449569702148
Epoch 1260, val loss: 1.4581159353256226
Epoch 1270, training loss: 0.06603297591209412 = 0.001728016883134842 + 0.01 * 6.430495738983154
Epoch 1270, val loss: 1.4606989622116089
Epoch 1280, training loss: 0.06591888517141342 = 0.0017060991376638412 + 0.01 * 6.421278476715088
Epoch 1280, val loss: 1.4633504152297974
Epoch 1290, training loss: 0.06591804325580597 = 0.0016848045634105802 + 0.01 * 6.423324108123779
Epoch 1290, val loss: 1.465930700302124
Epoch 1300, training loss: 0.06582149863243103 = 0.0016641018446534872 + 0.01 * 6.415740013122559
Epoch 1300, val loss: 1.4684503078460693
Epoch 1310, training loss: 0.06584180146455765 = 0.0016440253239125013 + 0.01 * 6.419778347015381
Epoch 1310, val loss: 1.4709594249725342
Epoch 1320, training loss: 0.0658755972981453 = 0.001624528900720179 + 0.01 * 6.425106525421143
Epoch 1320, val loss: 1.4734244346618652
Epoch 1330, training loss: 0.06576042622327805 = 0.001605704426765442 + 0.01 * 6.415472030639648
Epoch 1330, val loss: 1.4758410453796387
Epoch 1340, training loss: 0.0657779648900032 = 0.0015872924122959375 + 0.01 * 6.4190673828125
Epoch 1340, val loss: 1.4780958890914917
Epoch 1350, training loss: 0.06571080535650253 = 0.0015693714376538992 + 0.01 * 6.414144039154053
Epoch 1350, val loss: 1.480408787727356
Epoch 1360, training loss: 0.0657777413725853 = 0.0015519263688474894 + 0.01 * 6.422581672668457
Epoch 1360, val loss: 1.4826414585113525
Epoch 1370, training loss: 0.06566646695137024 = 0.0015350223984569311 + 0.01 * 6.413144588470459
Epoch 1370, val loss: 1.4849443435668945
Epoch 1380, training loss: 0.06559956818819046 = 0.00151853880379349 + 0.01 * 6.408102989196777
Epoch 1380, val loss: 1.487146019935608
Epoch 1390, training loss: 0.06573330610990524 = 0.0015024972381070256 + 0.01 * 6.423080921173096
Epoch 1390, val loss: 1.4893827438354492
Epoch 1400, training loss: 0.06549947708845139 = 0.00148686149623245 + 0.01 * 6.401261806488037
Epoch 1400, val loss: 1.4914103746414185
Epoch 1410, training loss: 0.06541457027196884 = 0.001471618190407753 + 0.01 * 6.394295692443848
Epoch 1410, val loss: 1.49354088306427
Epoch 1420, training loss: 0.06548760086297989 = 0.0014568051556125283 + 0.01 * 6.403079986572266
Epoch 1420, val loss: 1.495736002922058
Epoch 1430, training loss: 0.06534454226493835 = 0.0014422894455492496 + 0.01 * 6.390225410461426
Epoch 1430, val loss: 1.497740387916565
Epoch 1440, training loss: 0.06567759066820145 = 0.001428194809705019 + 0.01 * 6.4249396324157715
Epoch 1440, val loss: 1.4997437000274658
Epoch 1450, training loss: 0.06535164266824722 = 0.0014145327731966972 + 0.01 * 6.393711566925049
Epoch 1450, val loss: 1.5017205476760864
Epoch 1460, training loss: 0.06523668766021729 = 0.001401154207997024 + 0.01 * 6.3835530281066895
Epoch 1460, val loss: 1.5037213563919067
Epoch 1470, training loss: 0.06539864838123322 = 0.0013880913611501455 + 0.01 * 6.401056289672852
Epoch 1470, val loss: 1.505567193031311
Epoch 1480, training loss: 0.06518668681383133 = 0.001375375664792955 + 0.01 * 6.381131172180176
Epoch 1480, val loss: 1.5074701309204102
Epoch 1490, training loss: 0.06563249230384827 = 0.0013629242312163115 + 0.01 * 6.426956653594971
Epoch 1490, val loss: 1.509364366531372
Epoch 1500, training loss: 0.06526206433773041 = 0.0013508221600204706 + 0.01 * 6.391124725341797
Epoch 1500, val loss: 1.5112062692642212
Epoch 1510, training loss: 0.0653843879699707 = 0.0013389490777626634 + 0.01 * 6.404543876647949
Epoch 1510, val loss: 1.5131016969680786
Epoch 1520, training loss: 0.0651015192270279 = 0.0013273920631036162 + 0.01 * 6.377412796020508
Epoch 1520, val loss: 1.514788269996643
Epoch 1530, training loss: 0.06517902761697769 = 0.0013160925591364503 + 0.01 * 6.386293888092041
Epoch 1530, val loss: 1.516399621963501
Epoch 1540, training loss: 0.06504084914922714 = 0.0013050647685304284 + 0.01 * 6.3735785484313965
Epoch 1540, val loss: 1.518286943435669
Epoch 1550, training loss: 0.06535976380109787 = 0.001294327899813652 + 0.01 * 6.406543731689453
Epoch 1550, val loss: 1.5198843479156494
Epoch 1560, training loss: 0.06496568024158478 = 0.0012838111724704504 + 0.01 * 6.368186950683594
Epoch 1560, val loss: 1.5215651988983154
Epoch 1570, training loss: 0.06501901149749756 = 0.0012735282070934772 + 0.01 * 6.374548435211182
Epoch 1570, val loss: 1.523177146911621
Epoch 1580, training loss: 0.06489685922861099 = 0.001263441052287817 + 0.01 * 6.36334228515625
Epoch 1580, val loss: 1.5248010158538818
Epoch 1590, training loss: 0.06500452756881714 = 0.001253616763278842 + 0.01 * 6.375090599060059
Epoch 1590, val loss: 1.526370882987976
Epoch 1600, training loss: 0.06478944420814514 = 0.001244007726199925 + 0.01 * 6.354543685913086
Epoch 1600, val loss: 1.5280265808105469
Epoch 1610, training loss: 0.06505090743303299 = 0.0012345811119303107 + 0.01 * 6.3816328048706055
Epoch 1610, val loss: 1.5295659303665161
Epoch 1620, training loss: 0.06481897830963135 = 0.0012253831373527646 + 0.01 * 6.3593597412109375
Epoch 1620, val loss: 1.5309937000274658
Epoch 1630, training loss: 0.06523427367210388 = 0.0012163919163867831 + 0.01 * 6.401788234710693
Epoch 1630, val loss: 1.532507300376892
Epoch 1640, training loss: 0.0647834911942482 = 0.0012075664708390832 + 0.01 * 6.357592582702637
Epoch 1640, val loss: 1.5340642929077148
Epoch 1650, training loss: 0.06499764323234558 = 0.0011989288032054901 + 0.01 * 6.379870891571045
Epoch 1650, val loss: 1.5354743003845215
Epoch 1660, training loss: 0.06463754177093506 = 0.0011904475977644324 + 0.01 * 6.344709873199463
Epoch 1660, val loss: 1.5370599031448364
Epoch 1670, training loss: 0.06473487615585327 = 0.0011821474181488156 + 0.01 * 6.3552727699279785
Epoch 1670, val loss: 1.5384553670883179
Epoch 1680, training loss: 0.06465357542037964 = 0.0011740201152861118 + 0.01 * 6.34795618057251
Epoch 1680, val loss: 1.5398014783859253
Epoch 1690, training loss: 0.06469527631998062 = 0.0011661252938210964 + 0.01 * 6.352915287017822
Epoch 1690, val loss: 1.5412260293960571
Epoch 1700, training loss: 0.0649259015917778 = 0.0011583095183596015 + 0.01 * 6.376759052276611
Epoch 1700, val loss: 1.5425128936767578
Epoch 1710, training loss: 0.06456281244754791 = 0.0011506738374009728 + 0.01 * 6.341213703155518
Epoch 1710, val loss: 1.543835163116455
Epoch 1720, training loss: 0.06477373838424683 = 0.0011432269820943475 + 0.01 * 6.363051414489746
Epoch 1720, val loss: 1.5451939105987549
Epoch 1730, training loss: 0.06467178463935852 = 0.0011358658084645867 + 0.01 * 6.3535919189453125
Epoch 1730, val loss: 1.5465210676193237
Epoch 1740, training loss: 0.06441082060337067 = 0.0011286924127489328 + 0.01 * 6.328212261199951
Epoch 1740, val loss: 1.5478023290634155
Epoch 1750, training loss: 0.06456305831670761 = 0.0011216463753953576 + 0.01 * 6.344141006469727
Epoch 1750, val loss: 1.5491868257522583
Epoch 1760, training loss: 0.06465034186840057 = 0.0011147157056257129 + 0.01 * 6.353562355041504
Epoch 1760, val loss: 1.5504693984985352
Epoch 1770, training loss: 0.06434153020381927 = 0.0011079206597059965 + 0.01 * 6.323360443115234
Epoch 1770, val loss: 1.5516916513442993
Epoch 1780, training loss: 0.06448277831077576 = 0.001101254136301577 + 0.01 * 6.3381524085998535
Epoch 1780, val loss: 1.5529453754425049
Epoch 1790, training loss: 0.06433791667222977 = 0.0010947250993922353 + 0.01 * 6.324318885803223
Epoch 1790, val loss: 1.55415940284729
Epoch 1800, training loss: 0.06448587775230408 = 0.0010882860515266657 + 0.01 * 6.339759349822998
Epoch 1800, val loss: 1.5554091930389404
Epoch 1810, training loss: 0.06441005319356918 = 0.0010820047464221716 + 0.01 * 6.332805156707764
Epoch 1810, val loss: 1.5565519332885742
Epoch 1820, training loss: 0.06445664167404175 = 0.0010758034186437726 + 0.01 * 6.3380842208862305
Epoch 1820, val loss: 1.5578421354293823
Epoch 1830, training loss: 0.0642763301730156 = 0.0010697159450501204 + 0.01 * 6.320661544799805
Epoch 1830, val loss: 1.5588568449020386
Epoch 1840, training loss: 0.0642813891172409 = 0.001063774572685361 + 0.01 * 6.321761608123779
Epoch 1840, val loss: 1.5601286888122559
Epoch 1850, training loss: 0.0642332136631012 = 0.001057902118191123 + 0.01 * 6.317531585693359
Epoch 1850, val loss: 1.5612497329711914
Epoch 1860, training loss: 0.06436051428318024 = 0.0010521630756556988 + 0.01 * 6.330834865570068
Epoch 1860, val loss: 1.5622069835662842
Epoch 1870, training loss: 0.06419775635004044 = 0.0010464700171723962 + 0.01 * 6.315128326416016
Epoch 1870, val loss: 1.563403844833374
Epoch 1880, training loss: 0.06441830098628998 = 0.0010409174719825387 + 0.01 * 6.337738037109375
Epoch 1880, val loss: 1.5646116733551025
Epoch 1890, training loss: 0.06428945064544678 = 0.001035421621054411 + 0.01 * 6.325402736663818
Epoch 1890, val loss: 1.5657477378845215
Epoch 1900, training loss: 0.06402952969074249 = 0.0010300505673512816 + 0.01 * 6.299948215484619
Epoch 1900, val loss: 1.5667039155960083
Epoch 1910, training loss: 0.06421137601137161 = 0.0010247330646961927 + 0.01 * 6.31866455078125
Epoch 1910, val loss: 1.5679359436035156
Epoch 1920, training loss: 0.06403902918100357 = 0.001019539195112884 + 0.01 * 6.301949501037598
Epoch 1920, val loss: 1.5688860416412354
Epoch 1930, training loss: 0.06435603648424149 = 0.0010143918916583061 + 0.01 * 6.334164619445801
Epoch 1930, val loss: 1.5701062679290771
Epoch 1940, training loss: 0.06404370814561844 = 0.001009355648420751 + 0.01 * 6.303435802459717
Epoch 1940, val loss: 1.5709879398345947
Epoch 1950, training loss: 0.06405360251665115 = 0.0010044177761301398 + 0.01 * 6.30491828918457
Epoch 1950, val loss: 1.5721129179000854
Epoch 1960, training loss: 0.06391812115907669 = 0.0009995227446779609 + 0.01 * 6.291860103607178
Epoch 1960, val loss: 1.5731191635131836
Epoch 1970, training loss: 0.06403259187936783 = 0.0009947074577212334 + 0.01 * 6.303788661956787
Epoch 1970, val loss: 1.574092149734497
Epoch 1980, training loss: 0.06409718841314316 = 0.000989973428659141 + 0.01 * 6.310721397399902
Epoch 1980, val loss: 1.5750908851623535
Epoch 1990, training loss: 0.06383800506591797 = 0.0009852746734395623 + 0.01 * 6.28527307510376
Epoch 1990, val loss: 1.5761197805404663
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7815
Overall ASR: 0.5535
Flip ASR: 0.4622/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.024125576019287 = 1.9403867721557617 + 0.01 * 8.373878479003906
Epoch 0, val loss: 1.9358242750167847
Epoch 10, training loss: 2.0141031742095947 = 1.930364966392517 + 0.01 * 8.373826026916504
Epoch 10, val loss: 1.9250142574310303
Epoch 20, training loss: 2.0023043155670166 = 1.9185683727264404 + 0.01 * 8.373590469360352
Epoch 20, val loss: 1.912031888961792
Epoch 30, training loss: 1.9862251281738281 = 1.9024964570999146 + 0.01 * 8.372869491577148
Epoch 30, val loss: 1.894314169883728
Epoch 40, training loss: 1.9630742073059082 = 1.879380464553833 + 0.01 * 8.369372367858887
Epoch 40, val loss: 1.8690688610076904
Epoch 50, training loss: 1.9301235675811768 = 1.8466815948486328 + 0.01 * 8.344197273254395
Epoch 50, val loss: 1.8344533443450928
Epoch 60, training loss: 1.8866472244262695 = 1.804783582687378 + 0.01 * 8.186363220214844
Epoch 60, val loss: 1.7929667234420776
Epoch 70, training loss: 1.834201693534851 = 1.7579305171966553 + 0.01 * 7.627117156982422
Epoch 70, val loss: 1.7508975267410278
Epoch 80, training loss: 1.7760895490646362 = 1.7028511762619019 + 0.01 * 7.323840141296387
Epoch 80, val loss: 1.703553557395935
Epoch 90, training loss: 1.7008442878723145 = 1.6295057535171509 + 0.01 * 7.13385534286499
Epoch 90, val loss: 1.640250325202942
Epoch 100, training loss: 1.6049108505249023 = 1.5342942476272583 + 0.01 * 7.061666011810303
Epoch 100, val loss: 1.558229923248291
Epoch 110, training loss: 1.4923195838928223 = 1.422208309173584 + 0.01 * 7.011128902435303
Epoch 110, val loss: 1.465540885925293
Epoch 120, training loss: 1.3734561204910278 = 1.3036350011825562 + 0.01 * 6.982115745544434
Epoch 120, val loss: 1.371774435043335
Epoch 130, training loss: 1.2577321529388428 = 1.18807053565979 + 0.01 * 6.96616792678833
Epoch 130, val loss: 1.2833095788955688
Epoch 140, training loss: 1.150086760520935 = 1.080573320388794 + 0.01 * 6.951344013214111
Epoch 140, val loss: 1.20341956615448
Epoch 150, training loss: 1.0525057315826416 = 0.9831060171127319 + 0.01 * 6.939968585968018
Epoch 150, val loss: 1.1328332424163818
Epoch 160, training loss: 0.9650285840034485 = 0.8957264423370361 + 0.01 * 6.9302144050598145
Epoch 160, val loss: 1.0709128379821777
Epoch 170, training loss: 0.8865393996238708 = 0.8173483610153198 + 0.01 * 6.919101715087891
Epoch 170, val loss: 1.01615571975708
Epoch 180, training loss: 0.8160831332206726 = 0.7470424175262451 + 0.01 * 6.904073238372803
Epoch 180, val loss: 0.9673881530761719
Epoch 190, training loss: 0.753068208694458 = 0.6842123866081238 + 0.01 * 6.885581016540527
Epoch 190, val loss: 0.9245674014091492
Epoch 200, training loss: 0.6961452960968018 = 0.6274806261062622 + 0.01 * 6.86646842956543
Epoch 200, val loss: 0.8871625065803528
Epoch 210, training loss: 0.6432872414588928 = 0.5747724771499634 + 0.01 * 6.851475715637207
Epoch 210, val loss: 0.8535740375518799
Epoch 220, training loss: 0.5927274823188782 = 0.5242845416069031 + 0.01 * 6.844295024871826
Epoch 220, val loss: 0.8227426409721375
Epoch 230, training loss: 0.5436780452728271 = 0.4752575755119324 + 0.01 * 6.842047214508057
Epoch 230, val loss: 0.7943048477172852
Epoch 240, training loss: 0.4963664710521698 = 0.42795178294181824 + 0.01 * 6.841468811035156
Epoch 240, val loss: 0.7687270045280457
Epoch 250, training loss: 0.4513953626155853 = 0.3829829692840576 + 0.01 * 6.841238975524902
Epoch 250, val loss: 0.7469515204429626
Epoch 260, training loss: 0.4096642732620239 = 0.3412570059299469 + 0.01 * 6.840725898742676
Epoch 260, val loss: 0.7298835515975952
Epoch 270, training loss: 0.3716796934604645 = 0.3032826781272888 + 0.01 * 6.839701175689697
Epoch 270, val loss: 0.717535138130188
Epoch 280, training loss: 0.3376925587654114 = 0.269309937953949 + 0.01 * 6.838260650634766
Epoch 280, val loss: 0.7097399830818176
Epoch 290, training loss: 0.3075781762599945 = 0.23921461403369904 + 0.01 * 6.8363566398620605
Epoch 290, val loss: 0.7058823704719543
Epoch 300, training loss: 0.28107085824012756 = 0.2127322405576706 + 0.01 * 6.833862781524658
Epoch 300, val loss: 0.7051001191139221
Epoch 310, training loss: 0.2579221725463867 = 0.18960429728031158 + 0.01 * 6.831787109375
Epoch 310, val loss: 0.7071411609649658
Epoch 320, training loss: 0.2377733290195465 = 0.16949576139450073 + 0.01 * 6.827756404876709
Epoch 320, val loss: 0.7114132046699524
Epoch 330, training loss: 0.2202671766281128 = 0.15203644335269928 + 0.01 * 6.823073387145996
Epoch 330, val loss: 0.7174848914146423
Epoch 340, training loss: 0.205039381980896 = 0.13683785498142242 + 0.01 * 6.8201518058776855
Epoch 340, val loss: 0.725029468536377
Epoch 350, training loss: 0.19165471196174622 = 0.12353389710187912 + 0.01 * 6.812082290649414
Epoch 350, val loss: 0.733769416809082
Epoch 360, training loss: 0.1798655390739441 = 0.11182630062103271 + 0.01 * 6.8039231300354
Epoch 360, val loss: 0.7435103058815002
Epoch 370, training loss: 0.16959142684936523 = 0.10145892202854156 + 0.01 * 6.81325101852417
Epoch 370, val loss: 0.753937304019928
Epoch 380, training loss: 0.16023790836334229 = 0.09225690364837646 + 0.01 * 6.79810094833374
Epoch 380, val loss: 0.7648198008537292
Epoch 390, training loss: 0.15185263752937317 = 0.08403267711400986 + 0.01 * 6.781997203826904
Epoch 390, val loss: 0.776067852973938
Epoch 400, training loss: 0.14435625076293945 = 0.0766342356801033 + 0.01 * 6.7722015380859375
Epoch 400, val loss: 0.7874182462692261
Epoch 410, training loss: 0.13757294416427612 = 0.06995927542448044 + 0.01 * 6.761366844177246
Epoch 410, val loss: 0.7988765239715576
Epoch 420, training loss: 0.13139162957668304 = 0.06388378143310547 + 0.01 * 6.750784873962402
Epoch 420, val loss: 0.8103138208389282
Epoch 430, training loss: 0.1258249431848526 = 0.058317508548498154 + 0.01 * 6.750743389129639
Epoch 430, val loss: 0.8215739130973816
Epoch 440, training loss: 0.12061823159456253 = 0.05320822447538376 + 0.01 * 6.741001129150391
Epoch 440, val loss: 0.832642138004303
Epoch 450, training loss: 0.11574073135852814 = 0.04847772791981697 + 0.01 * 6.7263007164001465
Epoch 450, val loss: 0.8437645435333252
Epoch 460, training loss: 0.11123979091644287 = 0.044052377343177795 + 0.01 * 6.718741416931152
Epoch 460, val loss: 0.8547136783599854
Epoch 470, training loss: 0.1069972962141037 = 0.039891425520181656 + 0.01 * 6.710587501525879
Epoch 470, val loss: 0.8654547333717346
Epoch 480, training loss: 0.10303176194429398 = 0.03599067032337189 + 0.01 * 6.7041096687316895
Epoch 480, val loss: 0.8761026263237
Epoch 490, training loss: 0.09949572384357452 = 0.03241049125790596 + 0.01 * 6.708523273468018
Epoch 490, val loss: 0.8866724371910095
Epoch 500, training loss: 0.09616036713123322 = 0.0292546134442091 + 0.01 * 6.69057559967041
Epoch 500, val loss: 0.8972151875495911
Epoch 510, training loss: 0.09338679164648056 = 0.026540564373135567 + 0.01 * 6.684622764587402
Epoch 510, val loss: 0.9077909588813782
Epoch 520, training loss: 0.0909532830119133 = 0.024199219420552254 + 0.01 * 6.675406455993652
Epoch 520, val loss: 0.9183077216148376
Epoch 530, training loss: 0.08883143961429596 = 0.022167706862092018 + 0.01 * 6.666373252868652
Epoch 530, val loss: 0.9285984039306641
Epoch 540, training loss: 0.08722913265228271 = 0.02038983628153801 + 0.01 * 6.683930397033691
Epoch 540, val loss: 0.9385520815849304
Epoch 550, training loss: 0.08540984988212585 = 0.01882965676486492 + 0.01 * 6.658019542694092
Epoch 550, val loss: 0.9482332468032837
Epoch 560, training loss: 0.08393103629350662 = 0.017450762912631035 + 0.01 * 6.648027420043945
Epoch 560, val loss: 0.9578338861465454
Epoch 570, training loss: 0.08268769085407257 = 0.016224777325987816 + 0.01 * 6.646291255950928
Epoch 570, val loss: 0.9671919941902161
Epoch 580, training loss: 0.08156192302703857 = 0.01513351034373045 + 0.01 * 6.642841815948486
Epoch 580, val loss: 0.9762582182884216
Epoch 590, training loss: 0.08051075041294098 = 0.014158069156110287 + 0.01 * 6.635268211364746
Epoch 590, val loss: 0.9851325154304504
Epoch 600, training loss: 0.07954239845275879 = 0.013279743492603302 + 0.01 * 6.626266002655029
Epoch 600, val loss: 0.9937760233879089
Epoch 610, training loss: 0.0787426084280014 = 0.012485776096582413 + 0.01 * 6.625683784484863
Epoch 610, val loss: 1.0022525787353516
Epoch 620, training loss: 0.07802122086286545 = 0.011766618117690086 + 0.01 * 6.625460147857666
Epoch 620, val loss: 1.0103992223739624
Epoch 630, training loss: 0.07727434486150742 = 0.011113790795207024 + 0.01 * 6.616055488586426
Epoch 630, val loss: 1.018369197845459
Epoch 640, training loss: 0.07661574333906174 = 0.010517683811485767 + 0.01 * 6.609805583953857
Epoch 640, val loss: 1.0261693000793457
Epoch 650, training loss: 0.07598854601383209 = 0.009971448220312595 + 0.01 * 6.601709842681885
Epoch 650, val loss: 1.0337436199188232
Epoch 660, training loss: 0.07562743127346039 = 0.009469597600400448 + 0.01 * 6.615783214569092
Epoch 660, val loss: 1.0411968231201172
Epoch 670, training loss: 0.07496315985918045 = 0.009008076973259449 + 0.01 * 6.595508098602295
Epoch 670, val loss: 1.0483646392822266
Epoch 680, training loss: 0.074529729783535 = 0.00858268328011036 + 0.01 * 6.594705104827881
Epoch 680, val loss: 1.0553405284881592
Epoch 690, training loss: 0.07403982430696487 = 0.008189140819013119 + 0.01 * 6.585068702697754
Epoch 690, val loss: 1.0622366666793823
Epoch 700, training loss: 0.07366926968097687 = 0.007823546417057514 + 0.01 * 6.5845723152160645
Epoch 700, val loss: 1.0689529180526733
Epoch 710, training loss: 0.07324715703725815 = 0.007482638582587242 + 0.01 * 6.576451778411865
Epoch 710, val loss: 1.0755212306976318
Epoch 720, training loss: 0.07290016859769821 = 0.007163405884057283 + 0.01 * 6.573676109313965
Epoch 720, val loss: 1.0820026397705078
Epoch 730, training loss: 0.07259073108434677 = 0.006864685099571943 + 0.01 * 6.572605133056641
Epoch 730, val loss: 1.088352084159851
Epoch 740, training loss: 0.07243091613054276 = 0.0065841409377753735 + 0.01 * 6.584677696228027
Epoch 740, val loss: 1.09456205368042
Epoch 750, training loss: 0.07193421572446823 = 0.006321846041828394 + 0.01 * 6.56123685836792
Epoch 750, val loss: 1.1007577180862427
Epoch 760, training loss: 0.07178772985935211 = 0.006074716802686453 + 0.01 * 6.571301460266113
Epoch 760, val loss: 1.1067705154418945
Epoch 770, training loss: 0.07146118581295013 = 0.005842328537255526 + 0.01 * 6.561885833740234
Epoch 770, val loss: 1.11268150806427
Epoch 780, training loss: 0.07114729285240173 = 0.005623400676995516 + 0.01 * 6.552389144897461
Epoch 780, val loss: 1.1185451745986938
Epoch 790, training loss: 0.07099024206399918 = 0.0054166242480278015 + 0.01 * 6.557361602783203
Epoch 790, val loss: 1.1243011951446533
Epoch 800, training loss: 0.07067489624023438 = 0.005221865139901638 + 0.01 * 6.5453033447265625
Epoch 800, val loss: 1.129950761795044
Epoch 810, training loss: 0.07064726948738098 = 0.005037992261350155 + 0.01 * 6.560927867889404
Epoch 810, val loss: 1.135560154914856
Epoch 820, training loss: 0.07026972621679306 = 0.004864185582846403 + 0.01 * 6.540554046630859
Epoch 820, val loss: 1.1409648656845093
Epoch 830, training loss: 0.07012879103422165 = 0.004699949640780687 + 0.01 * 6.54288387298584
Epoch 830, val loss: 1.1463664770126343
Epoch 840, training loss: 0.06983325630426407 = 0.004544975236058235 + 0.01 * 6.528828144073486
Epoch 840, val loss: 1.1515885591506958
Epoch 850, training loss: 0.06976248323917389 = 0.00439855782315135 + 0.01 * 6.536392688751221
Epoch 850, val loss: 1.1567533016204834
Epoch 860, training loss: 0.06955690681934357 = 0.004259744193404913 + 0.01 * 6.529716491699219
Epoch 860, val loss: 1.1618317365646362
Epoch 870, training loss: 0.06940994411706924 = 0.004128186497837305 + 0.01 * 6.5281758308410645
Epoch 870, val loss: 1.166784644126892
Epoch 880, training loss: 0.06918773800134659 = 0.004003290552645922 + 0.01 * 6.518444538116455
Epoch 880, val loss: 1.1716409921646118
Epoch 890, training loss: 0.06903152167797089 = 0.0038843683432787657 + 0.01 * 6.514715194702148
Epoch 890, val loss: 1.1764590740203857
Epoch 900, training loss: 0.06925502419471741 = 0.0037713968195021152 + 0.01 * 6.548363208770752
Epoch 900, val loss: 1.1812323331832886
Epoch 910, training loss: 0.06878545880317688 = 0.0036643403582274914 + 0.01 * 6.512112617492676
Epoch 910, val loss: 1.1857781410217285
Epoch 920, training loss: 0.06867019832134247 = 0.0035624855663627386 + 0.01 * 6.51077127456665
Epoch 920, val loss: 1.190256118774414
Epoch 930, training loss: 0.06865377724170685 = 0.003465248504653573 + 0.01 * 6.518852710723877
Epoch 930, val loss: 1.1947686672210693
Epoch 940, training loss: 0.06837150454521179 = 0.003372705075889826 + 0.01 * 6.499879837036133
Epoch 940, val loss: 1.1991584300994873
Epoch 950, training loss: 0.06832106411457062 = 0.00328437308780849 + 0.01 * 6.503669261932373
Epoch 950, val loss: 1.2034754753112793
Epoch 960, training loss: 0.0682058185338974 = 0.0032002090010792017 + 0.01 * 6.500560760498047
Epoch 960, val loss: 1.2077150344848633
Epoch 970, training loss: 0.06817813962697983 = 0.0031197378411889076 + 0.01 * 6.505840301513672
Epoch 970, val loss: 1.2118306159973145
Epoch 980, training loss: 0.06810224801301956 = 0.003042741445824504 + 0.01 * 6.505950450897217
Epoch 980, val loss: 1.2159818410873413
Epoch 990, training loss: 0.06800758093595505 = 0.002969413995742798 + 0.01 * 6.503816604614258
Epoch 990, val loss: 1.219961404800415
Epoch 1000, training loss: 0.06787700951099396 = 0.0028990735299885273 + 0.01 * 6.497793674468994
Epoch 1000, val loss: 1.2239338159561157
Epoch 1010, training loss: 0.06781893968582153 = 0.0028317945543676615 + 0.01 * 6.498714923858643
Epoch 1010, val loss: 1.227807879447937
Epoch 1020, training loss: 0.06761082261800766 = 0.0027672317810356617 + 0.01 * 6.484358787536621
Epoch 1020, val loss: 1.2316046953201294
Epoch 1030, training loss: 0.06765454262495041 = 0.002705336781218648 + 0.01 * 6.4949212074279785
Epoch 1030, val loss: 1.2353558540344238
Epoch 1040, training loss: 0.06744517385959625 = 0.002645917469635606 + 0.01 * 6.479926109313965
Epoch 1040, val loss: 1.239088535308838
Epoch 1050, training loss: 0.0675065666437149 = 0.0025890313554555178 + 0.01 * 6.491753578186035
Epoch 1050, val loss: 1.242719054222107
Epoch 1060, training loss: 0.06735642999410629 = 0.002534351544454694 + 0.01 * 6.482208251953125
Epoch 1060, val loss: 1.2462157011032104
Epoch 1070, training loss: 0.06721582263708115 = 0.0024819334503263235 + 0.01 * 6.473389625549316
Epoch 1070, val loss: 1.2496687173843384
Epoch 1080, training loss: 0.06723184138536453 = 0.002431496512144804 + 0.01 * 6.480034351348877
Epoch 1080, val loss: 1.2531176805496216
Epoch 1090, training loss: 0.06708316504955292 = 0.002383006503805518 + 0.01 * 6.470016002655029
Epoch 1090, val loss: 1.2564889192581177
Epoch 1100, training loss: 0.0676359310746193 = 0.002336225938051939 + 0.01 * 6.529970645904541
Epoch 1100, val loss: 1.2598305940628052
Epoch 1110, training loss: 0.06699210405349731 = 0.002291534561663866 + 0.01 * 6.470056533813477
Epoch 1110, val loss: 1.2629319429397583
Epoch 1120, training loss: 0.06700247526168823 = 0.0022484397049993277 + 0.01 * 6.475404262542725
Epoch 1120, val loss: 1.2662720680236816
Epoch 1130, training loss: 0.06676459312438965 = 0.0022068254183977842 + 0.01 * 6.455776691436768
Epoch 1130, val loss: 1.2693537473678589
Epoch 1140, training loss: 0.06674058735370636 = 0.0021666993852704763 + 0.01 * 6.4573893547058105
Epoch 1140, val loss: 1.272409439086914
Epoch 1150, training loss: 0.06688588857650757 = 0.002128010615706444 + 0.01 * 6.475788116455078
Epoch 1150, val loss: 1.2755073308944702
Epoch 1160, training loss: 0.06680691242218018 = 0.002090654568746686 + 0.01 * 6.471625804901123
Epoch 1160, val loss: 1.2784152030944824
Epoch 1170, training loss: 0.06656889617443085 = 0.002054705750197172 + 0.01 * 6.451419353485107
Epoch 1170, val loss: 1.281375527381897
Epoch 1180, training loss: 0.0664270892739296 = 0.002019912237301469 + 0.01 * 6.440718173980713
Epoch 1180, val loss: 1.2843036651611328
Epoch 1190, training loss: 0.06656747311353683 = 0.001986199989914894 + 0.01 * 6.458127975463867
Epoch 1190, val loss: 1.2871683835983276
Epoch 1200, training loss: 0.06664812564849854 = 0.0019537510816007853 + 0.01 * 6.469437599182129
Epoch 1200, val loss: 1.2899558544158936
Epoch 1210, training loss: 0.06629825383424759 = 0.001922480994835496 + 0.01 * 6.437577724456787
Epoch 1210, val loss: 1.2926714420318604
Epoch 1220, training loss: 0.06628259271383286 = 0.0018922222079709172 + 0.01 * 6.439037322998047
Epoch 1220, val loss: 1.2953879833221436
Epoch 1230, training loss: 0.06633354723453522 = 0.0018628531834110618 + 0.01 * 6.44706916809082
Epoch 1230, val loss: 1.298090934753418
Epoch 1240, training loss: 0.06611533463001251 = 0.0018344304990023375 + 0.01 * 6.428090572357178
Epoch 1240, val loss: 1.3006874322891235
Epoch 1250, training loss: 0.06641921401023865 = 0.0018068865174427629 + 0.01 * 6.461233139038086
Epoch 1250, val loss: 1.3033244609832764
Epoch 1260, training loss: 0.06610254943370819 = 0.001780373277142644 + 0.01 * 6.432217597961426
Epoch 1260, val loss: 1.3057913780212402
Epoch 1270, training loss: 0.06600300222635269 = 0.0017546374583616853 + 0.01 * 6.424837112426758
Epoch 1270, val loss: 1.3083078861236572
Epoch 1280, training loss: 0.06674391031265259 = 0.0017296880250796676 + 0.01 * 6.50142240524292
Epoch 1280, val loss: 1.3107837438583374
Epoch 1290, training loss: 0.06584703177213669 = 0.0017055206699296832 + 0.01 * 6.414151191711426
Epoch 1290, val loss: 1.3130671977996826
Epoch 1300, training loss: 0.0658423975110054 = 0.0016821249155327678 + 0.01 * 6.416027069091797
Epoch 1300, val loss: 1.3155087232589722
Epoch 1310, training loss: 0.06575938314199448 = 0.0016593302134424448 + 0.01 * 6.41000509262085
Epoch 1310, val loss: 1.3178082704544067
Epoch 1320, training loss: 0.06582507491111755 = 0.0016372076934203506 + 0.01 * 6.418787002563477
Epoch 1320, val loss: 1.3201204538345337
Epoch 1330, training loss: 0.06568749994039536 = 0.001615693443454802 + 0.01 * 6.4071807861328125
Epoch 1330, val loss: 1.3223503828048706
Epoch 1340, training loss: 0.06612638384103775 = 0.0015947985230013728 + 0.01 * 6.453158855438232
Epoch 1340, val loss: 1.3246159553527832
Epoch 1350, training loss: 0.06576000899076462 = 0.0015745996497571468 + 0.01 * 6.418540954589844
Epoch 1350, val loss: 1.3267617225646973
Epoch 1360, training loss: 0.06587311625480652 = 0.0015550799435004592 + 0.01 * 6.4318037033081055
Epoch 1360, val loss: 1.3289204835891724
Epoch 1370, training loss: 0.06570794433355331 = 0.0015360309043899179 + 0.01 * 6.417191505432129
Epoch 1370, val loss: 1.3310136795043945
Epoch 1380, training loss: 0.06601065397262573 = 0.0015176127199083567 + 0.01 * 6.44930362701416
Epoch 1380, val loss: 1.3331220149993896
Epoch 1390, training loss: 0.06550507992506027 = 0.001499671838246286 + 0.01 * 6.400540828704834
Epoch 1390, val loss: 1.3351638317108154
Epoch 1400, training loss: 0.06542883813381195 = 0.001482214080169797 + 0.01 * 6.394662857055664
Epoch 1400, val loss: 1.337246298789978
Epoch 1410, training loss: 0.0654069259762764 = 0.00146515853703022 + 0.01 * 6.394176959991455
Epoch 1410, val loss: 1.3392541408538818
Epoch 1420, training loss: 0.06527911126613617 = 0.0014485669089481235 + 0.01 * 6.383054733276367
Epoch 1420, val loss: 1.341220498085022
Epoch 1430, training loss: 0.06529699265956879 = 0.0014324678340926766 + 0.01 * 6.3864521980285645
Epoch 1430, val loss: 1.3431552648544312
Epoch 1440, training loss: 0.06520843505859375 = 0.0014167290646582842 + 0.01 * 6.379170894622803
Epoch 1440, val loss: 1.3451462984085083
Epoch 1450, training loss: 0.06565998494625092 = 0.0014014554908499122 + 0.01 * 6.425853252410889
Epoch 1450, val loss: 1.3470542430877686
Epoch 1460, training loss: 0.06529564410448074 = 0.0013865819200873375 + 0.01 * 6.39090633392334
Epoch 1460, val loss: 1.348853588104248
Epoch 1470, training loss: 0.06565243005752563 = 0.001372131402604282 + 0.01 * 6.428030490875244
Epoch 1470, val loss: 1.350743055343628
Epoch 1480, training loss: 0.06528833508491516 = 0.0013580165104940534 + 0.01 * 6.393031597137451
Epoch 1480, val loss: 1.3524751663208008
Epoch 1490, training loss: 0.06519161909818649 = 0.0013442469062283635 + 0.01 * 6.384737491607666
Epoch 1490, val loss: 1.3543965816497803
Epoch 1500, training loss: 0.06504146009683609 = 0.0013307753251865506 + 0.01 * 6.371068000793457
Epoch 1500, val loss: 1.3560818433761597
Epoch 1510, training loss: 0.06506776809692383 = 0.0013176308711990714 + 0.01 * 6.375014305114746
Epoch 1510, val loss: 1.3578206300735474
Epoch 1520, training loss: 0.06493531912565231 = 0.0013048553373664618 + 0.01 * 6.363046646118164
Epoch 1520, val loss: 1.3595024347305298
Epoch 1530, training loss: 0.06506207585334778 = 0.0012923645554110408 + 0.01 * 6.376971244812012
Epoch 1530, val loss: 1.3612359762191772
Epoch 1540, training loss: 0.06499680131673813 = 0.0012802020646631718 + 0.01 * 6.371659755706787
Epoch 1540, val loss: 1.3628419637680054
Epoch 1550, training loss: 0.06492030620574951 = 0.0012683456297963858 + 0.01 * 6.365196228027344
Epoch 1550, val loss: 1.3645401000976562
Epoch 1560, training loss: 0.06493818759918213 = 0.0012568217935040593 + 0.01 * 6.368136405944824
Epoch 1560, val loss: 1.3660963773727417
Epoch 1570, training loss: 0.06487369537353516 = 0.0012455264804884791 + 0.01 * 6.36281681060791
Epoch 1570, val loss: 1.3677253723144531
Epoch 1580, training loss: 0.06491895020008087 = 0.001234563416801393 + 0.01 * 6.368439197540283
Epoch 1580, val loss: 1.36921226978302
Epoch 1590, training loss: 0.06470008939504623 = 0.0012237920891493559 + 0.01 * 6.347629547119141
Epoch 1590, val loss: 1.3707948923110962
Epoch 1600, training loss: 0.06508529931306839 = 0.0012132624397054315 + 0.01 * 6.387204170227051
Epoch 1600, val loss: 1.3723199367523193
Epoch 1610, training loss: 0.06473424285650253 = 0.0012029553763568401 + 0.01 * 6.353128910064697
Epoch 1610, val loss: 1.3737459182739258
Epoch 1620, training loss: 0.0648333802819252 = 0.0011929644970223308 + 0.01 * 6.364041805267334
Epoch 1620, val loss: 1.3752000331878662
Epoch 1630, training loss: 0.06470701098442078 = 0.0011831759475171566 + 0.01 * 6.352384090423584
Epoch 1630, val loss: 1.3766603469848633
Epoch 1640, training loss: 0.06470741331577301 = 0.0011736529413610697 + 0.01 * 6.3533759117126465
Epoch 1640, val loss: 1.3780145645141602
Epoch 1650, training loss: 0.06460101902484894 = 0.0011643663747236133 + 0.01 * 6.34366512298584
Epoch 1650, val loss: 1.3794282674789429
Epoch 1660, training loss: 0.06460196524858475 = 0.001155234407633543 + 0.01 * 6.344673156738281
Epoch 1660, val loss: 1.3808622360229492
Epoch 1670, training loss: 0.06463102996349335 = 0.0011463193222880363 + 0.01 * 6.348471641540527
Epoch 1670, val loss: 1.3821356296539307
Epoch 1680, training loss: 0.06500469893217087 = 0.0011376073816791177 + 0.01 * 6.386709690093994
Epoch 1680, val loss: 1.383457899093628
Epoch 1690, training loss: 0.06459657102823257 = 0.0011290551628917456 + 0.01 * 6.346752166748047
Epoch 1690, val loss: 1.3847239017486572
Epoch 1700, training loss: 0.06457792967557907 = 0.0011206910712644458 + 0.01 * 6.345723628997803
Epoch 1700, val loss: 1.3860373497009277
Epoch 1710, training loss: 0.06441343575716019 = 0.0011124844895675778 + 0.01 * 6.330095291137695
Epoch 1710, val loss: 1.387326717376709
Epoch 1720, training loss: 0.06446190923452377 = 0.0011044475249946117 + 0.01 * 6.3357462882995605
Epoch 1720, val loss: 1.3885513544082642
Epoch 1730, training loss: 0.06449280679225922 = 0.0010965632973238826 + 0.01 * 6.339624881744385
Epoch 1730, val loss: 1.3898282051086426
Epoch 1740, training loss: 0.0644303485751152 = 0.0010888458928093314 + 0.01 * 6.334150791168213
Epoch 1740, val loss: 1.390995979309082
Epoch 1750, training loss: 0.06469046324491501 = 0.0010812908876687288 + 0.01 * 6.360917568206787
Epoch 1750, val loss: 1.3922193050384521
Epoch 1760, training loss: 0.06435631215572357 = 0.0010739112040027976 + 0.01 * 6.328239917755127
Epoch 1760, val loss: 1.393328070640564
Epoch 1770, training loss: 0.06448182463645935 = 0.0010667088208720088 + 0.01 * 6.3415117263793945
Epoch 1770, val loss: 1.3945105075836182
Epoch 1780, training loss: 0.06456895172595978 = 0.0010596670908853412 + 0.01 * 6.350928783416748
Epoch 1780, val loss: 1.3956102132797241
Epoch 1790, training loss: 0.06422675400972366 = 0.0010527281556278467 + 0.01 * 6.317402362823486
Epoch 1790, val loss: 1.396754264831543
Epoch 1800, training loss: 0.06445780396461487 = 0.0010459451004862785 + 0.01 * 6.3411865234375
Epoch 1800, val loss: 1.3978642225265503
Epoch 1810, training loss: 0.06420955061912537 = 0.0010392911499366164 + 0.01 * 6.317026138305664
Epoch 1810, val loss: 1.398895263671875
Epoch 1820, training loss: 0.06411667168140411 = 0.0010327709605917335 + 0.01 * 6.308390140533447
Epoch 1820, val loss: 1.400023341178894
Epoch 1830, training loss: 0.06440389901399612 = 0.0010263550793752074 + 0.01 * 6.337754249572754
Epoch 1830, val loss: 1.401075839996338
Epoch 1840, training loss: 0.06419258564710617 = 0.0010200993856415153 + 0.01 * 6.317248821258545
Epoch 1840, val loss: 1.4021493196487427
Epoch 1850, training loss: 0.06426489353179932 = 0.0010139805963262916 + 0.01 * 6.325091361999512
Epoch 1850, val loss: 1.4031625986099243
Epoch 1860, training loss: 0.06411412358283997 = 0.0010079869534820318 + 0.01 * 6.310613632202148
Epoch 1860, val loss: 1.4041181802749634
Epoch 1870, training loss: 0.06434779614210129 = 0.0010020948247984052 + 0.01 * 6.334570407867432
Epoch 1870, val loss: 1.4051841497421265
Epoch 1880, training loss: 0.06402938067913055 = 0.0009963137563318014 + 0.01 * 6.303306579589844
Epoch 1880, val loss: 1.4061732292175293
Epoch 1890, training loss: 0.06392858922481537 = 0.0009905814658850431 + 0.01 * 6.293800354003906
Epoch 1890, val loss: 1.4072189331054688
Epoch 1900, training loss: 0.06434750556945801 = 0.0009850028436630964 + 0.01 * 6.336250305175781
Epoch 1900, val loss: 1.4081826210021973
Epoch 1910, training loss: 0.06392470002174377 = 0.0009794891811907291 + 0.01 * 6.294521331787109
Epoch 1910, val loss: 1.4091219902038574
Epoch 1920, training loss: 0.06405030936002731 = 0.0009740991517901421 + 0.01 * 6.307621479034424
Epoch 1920, val loss: 1.4101122617721558
Epoch 1930, training loss: 0.0638931468129158 = 0.0009687843266874552 + 0.01 * 6.292436599731445
Epoch 1930, val loss: 1.4110589027404785
Epoch 1940, training loss: 0.06399145722389221 = 0.0009635546011850238 + 0.01 * 6.302790641784668
Epoch 1940, val loss: 1.4120588302612305
Epoch 1950, training loss: 0.06397662311792374 = 0.000958395772613585 + 0.01 * 6.301822662353516
Epoch 1950, val loss: 1.4129317998886108
Epoch 1960, training loss: 0.0640035942196846 = 0.0009533497504889965 + 0.01 * 6.305025100708008
Epoch 1960, val loss: 1.4138598442077637
Epoch 1970, training loss: 0.06420251727104187 = 0.0009483332396484911 + 0.01 * 6.325418949127197
Epoch 1970, val loss: 1.414825201034546
Epoch 1980, training loss: 0.06369177997112274 = 0.0009434488601982594 + 0.01 * 6.274832725524902
Epoch 1980, val loss: 1.4157003164291382
Epoch 1990, training loss: 0.06362038105726242 = 0.0009385952143929899 + 0.01 * 6.268178939819336
Epoch 1990, val loss: 1.4166454076766968
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.5240
Flip ASR: 0.4489/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0395619869232178 = 1.9558228254318237 + 0.01 * 8.373908996582031
Epoch 0, val loss: 1.9572241306304932
Epoch 10, training loss: 2.0293080806732178 = 1.945569634437561 + 0.01 * 8.37385082244873
Epoch 10, val loss: 1.9464706182479858
Epoch 20, training loss: 2.016658306121826 = 1.932922124862671 + 0.01 * 8.373625755310059
Epoch 20, val loss: 1.933258295059204
Epoch 30, training loss: 1.9989310503005981 = 1.9152024984359741 + 0.01 * 8.37285327911377
Epoch 30, val loss: 1.9148986339569092
Epoch 40, training loss: 1.972658395767212 = 1.8889755010604858 + 0.01 * 8.368294715881348
Epoch 40, val loss: 1.8882405757904053
Epoch 50, training loss: 1.9351383447647095 = 1.8518056869506836 + 0.01 * 8.333266258239746
Epoch 50, val loss: 1.852051854133606
Epoch 60, training loss: 1.8888828754425049 = 1.8073118925094604 + 0.01 * 8.157093048095703
Epoch 60, val loss: 1.8116997480392456
Epoch 70, training loss: 1.8430554866790771 = 1.7651268243789673 + 0.01 * 7.792870998382568
Epoch 70, val loss: 1.7757699489593506
Epoch 80, training loss: 1.795027256011963 = 1.7187834978103638 + 0.01 * 7.624375343322754
Epoch 80, val loss: 1.7344825267791748
Epoch 90, training loss: 1.729701280593872 = 1.6559330224990845 + 0.01 * 7.376822471618652
Epoch 90, val loss: 1.6789075136184692
Epoch 100, training loss: 1.643967628479004 = 1.5720688104629517 + 0.01 * 7.18988561630249
Epoch 100, val loss: 1.6072834730148315
Epoch 110, training loss: 1.5391795635223389 = 1.468266487121582 + 0.01 * 7.091310024261475
Epoch 110, val loss: 1.5218560695648193
Epoch 120, training loss: 1.424053430557251 = 1.3538944721221924 + 0.01 * 7.015898704528809
Epoch 120, val loss: 1.4310616254806519
Epoch 130, training loss: 1.3078594207763672 = 1.2383966445922852 + 0.01 * 6.946280479431152
Epoch 130, val loss: 1.342066764831543
Epoch 140, training loss: 1.1964914798736572 = 1.1275088787078857 + 0.01 * 6.898265838623047
Epoch 140, val loss: 1.258242130279541
Epoch 150, training loss: 1.0927566289901733 = 1.0240613222122192 + 0.01 * 6.869532585144043
Epoch 150, val loss: 1.1807818412780762
Epoch 160, training loss: 0.9969270825386047 = 0.9283812642097473 + 0.01 * 6.854581832885742
Epoch 160, val loss: 1.108963131904602
Epoch 170, training loss: 0.9072501063346863 = 0.8387724757194519 + 0.01 * 6.847763538360596
Epoch 170, val loss: 1.0421574115753174
Epoch 180, training loss: 0.8220799565315247 = 0.7536461353302002 + 0.01 * 6.8433837890625
Epoch 180, val loss: 0.9794489145278931
Epoch 190, training loss: 0.7417331337928772 = 0.6733362078666687 + 0.01 * 6.839694023132324
Epoch 190, val loss: 0.9215862154960632
Epoch 200, training loss: 0.6679630279541016 = 0.5995997786521912 + 0.01 * 6.836325168609619
Epoch 200, val loss: 0.8706077933311462
Epoch 210, training loss: 0.602066159248352 = 0.5337340235710144 + 0.01 * 6.833216190338135
Epoch 210, val loss: 0.8284934163093567
Epoch 220, training loss: 0.5439728498458862 = 0.47567218542099 + 0.01 * 6.830067157745361
Epoch 220, val loss: 0.7956234216690063
Epoch 230, training loss: 0.49269405007362366 = 0.42442795634269714 + 0.01 * 6.826610565185547
Epoch 230, val loss: 0.7712494134902954
Epoch 240, training loss: 0.4470672011375427 = 0.3788311183452606 + 0.01 * 6.823607444763184
Epoch 240, val loss: 0.7537056803703308
Epoch 250, training loss: 0.406078964471817 = 0.3378767669200897 + 0.01 * 6.82021951675415
Epoch 250, val loss: 0.7413996458053589
Epoch 260, training loss: 0.36898261308670044 = 0.3008022904396057 + 0.01 * 6.8180317878723145
Epoch 260, val loss: 0.7329496145248413
Epoch 270, training loss: 0.33525776863098145 = 0.26709187030792236 + 0.01 * 6.816591739654541
Epoch 270, val loss: 0.727498471736908
Epoch 280, training loss: 0.3044930100440979 = 0.23635704815387726 + 0.01 * 6.813597679138184
Epoch 280, val loss: 0.7245628237724304
Epoch 290, training loss: 0.27652573585510254 = 0.20842061936855316 + 0.01 * 6.810513019561768
Epoch 290, val loss: 0.724058210849762
Epoch 300, training loss: 0.25135865807533264 = 0.18327471613883972 + 0.01 * 6.808394908905029
Epoch 300, val loss: 0.7256395220756531
Epoch 310, training loss: 0.22904077172279358 = 0.16095034778118134 + 0.01 * 6.809041976928711
Epoch 310, val loss: 0.7291210889816284
Epoch 320, training loss: 0.20949454605579376 = 0.14143763482570648 + 0.01 * 6.805691242218018
Epoch 320, val loss: 0.7343842387199402
Epoch 330, training loss: 0.19259671866893768 = 0.124588742852211 + 0.01 * 6.800797462463379
Epoch 330, val loss: 0.7413186430931091
Epoch 340, training loss: 0.17815734446048737 = 0.11013167351484299 + 0.01 * 6.802567481994629
Epoch 340, val loss: 0.7496482729911804
Epoch 350, training loss: 0.16571135818958282 = 0.09776108711957932 + 0.01 * 6.795027256011963
Epoch 350, val loss: 0.759063184261322
Epoch 360, training loss: 0.15507031977176666 = 0.08716726303100586 + 0.01 * 6.7903056144714355
Epoch 360, val loss: 0.76926589012146
Epoch 370, training loss: 0.14590050280094147 = 0.07806355506181717 + 0.01 * 6.783695220947266
Epoch 370, val loss: 0.7799701690673828
Epoch 380, training loss: 0.13821160793304443 = 0.07019871473312378 + 0.01 * 6.8012895584106445
Epoch 380, val loss: 0.7909625768661499
Epoch 390, training loss: 0.13111957907676697 = 0.06337231397628784 + 0.01 * 6.774726867675781
Epoch 390, val loss: 0.8020744323730469
Epoch 400, training loss: 0.12505897879600525 = 0.057409122586250305 + 0.01 * 6.764986515045166
Epoch 400, val loss: 0.8132249712944031
Epoch 410, training loss: 0.11979705095291138 = 0.052171964198350906 + 0.01 * 6.762508869171143
Epoch 410, val loss: 0.8243433833122253
Epoch 420, training loss: 0.11502588540315628 = 0.047551386058330536 + 0.01 * 6.747450351715088
Epoch 420, val loss: 0.8353801965713501
Epoch 430, training loss: 0.11086048185825348 = 0.043456293642520905 + 0.01 * 6.740418910980225
Epoch 430, val loss: 0.8463554382324219
Epoch 440, training loss: 0.10710373520851135 = 0.039813924580812454 + 0.01 * 6.728981018066406
Epoch 440, val loss: 0.857175886631012
Epoch 450, training loss: 0.10376466065645218 = 0.03656524419784546 + 0.01 * 6.719942092895508
Epoch 450, val loss: 0.867889940738678
Epoch 460, training loss: 0.1007719337940216 = 0.03365928307175636 + 0.01 * 6.711265563964844
Epoch 460, val loss: 0.8784152269363403
Epoch 470, training loss: 0.09810445457696915 = 0.031051160767674446 + 0.01 * 6.705329418182373
Epoch 470, val loss: 0.8887802958488464
Epoch 480, training loss: 0.09561315923929214 = 0.028711361810564995 + 0.01 * 6.690179347991943
Epoch 480, val loss: 0.8989570140838623
Epoch 490, training loss: 0.09352608025074005 = 0.026605136692523956 + 0.01 * 6.692094802856445
Epoch 490, val loss: 0.908951997756958
Epoch 500, training loss: 0.09163309633731842 = 0.02470294199883938 + 0.01 * 6.6930155754089355
Epoch 500, val loss: 0.9187666177749634
Epoch 510, training loss: 0.08974158018827438 = 0.022983679547905922 + 0.01 * 6.675790309906006
Epoch 510, val loss: 0.9284384250640869
Epoch 520, training loss: 0.08824734389781952 = 0.021426159888505936 + 0.01 * 6.682117938995361
Epoch 520, val loss: 0.9378071427345276
Epoch 530, training loss: 0.08647412061691284 = 0.020013319328427315 + 0.01 * 6.646080493927002
Epoch 530, val loss: 0.9469996690750122
Epoch 540, training loss: 0.08512622863054276 = 0.01872960664331913 + 0.01 * 6.639662742614746
Epoch 540, val loss: 0.95591801404953
Epoch 550, training loss: 0.0839909166097641 = 0.017562296241521835 + 0.01 * 6.642862319946289
Epoch 550, val loss: 0.9646462798118591
Epoch 560, training loss: 0.08320221304893494 = 0.016496701166033745 + 0.01 * 6.67055082321167
Epoch 560, val loss: 0.9731646776199341
Epoch 570, training loss: 0.08189357072114944 = 0.015524405054748058 + 0.01 * 6.636916637420654
Epoch 570, val loss: 0.9814187288284302
Epoch 580, training loss: 0.08081469684839249 = 0.014633928425610065 + 0.01 * 6.618076801300049
Epoch 580, val loss: 0.9894709587097168
Epoch 590, training loss: 0.08021364361047745 = 0.013816229067742825 + 0.01 * 6.63974142074585
Epoch 590, val loss: 0.9973934888839722
Epoch 600, training loss: 0.07919028401374817 = 0.013065016828477383 + 0.01 * 6.612526893615723
Epoch 600, val loss: 1.0050114393234253
Epoch 610, training loss: 0.07851465046405792 = 0.012373318895697594 + 0.01 * 6.614133358001709
Epoch 610, val loss: 1.0124945640563965
Epoch 620, training loss: 0.07778037339448929 = 0.011736287735402584 + 0.01 * 6.6044087409973145
Epoch 620, val loss: 1.0197780132293701
Epoch 630, training loss: 0.0769733116030693 = 0.011148248799145222 + 0.01 * 6.58250617980957
Epoch 630, val loss: 1.0268421173095703
Epoch 640, training loss: 0.0767996609210968 = 0.01060374267399311 + 0.01 * 6.619592189788818
Epoch 640, val loss: 1.0337656736373901
Epoch 650, training loss: 0.07590649276971817 = 0.010099479928612709 + 0.01 * 6.58070182800293
Epoch 650, val loss: 1.040438175201416
Epoch 660, training loss: 0.07540018856525421 = 0.009631287306547165 + 0.01 * 6.57689094543457
Epoch 660, val loss: 1.0469863414764404
Epoch 670, training loss: 0.07484519481658936 = 0.009195601567626 + 0.01 * 6.564959526062012
Epoch 670, val loss: 1.0533638000488281
Epoch 680, training loss: 0.07451138645410538 = 0.00879028532654047 + 0.01 * 6.572110652923584
Epoch 680, val loss: 1.059538722038269
Epoch 690, training loss: 0.07409896701574326 = 0.008413108997046947 + 0.01 * 6.568586349487305
Epoch 690, val loss: 1.0655807256698608
Epoch 700, training loss: 0.07366175204515457 = 0.008060812950134277 + 0.01 * 6.560094356536865
Epoch 700, val loss: 1.071454644203186
Epoch 710, training loss: 0.07332076877355576 = 0.0077313329093158245 + 0.01 * 6.558943748474121
Epoch 710, val loss: 1.0772560834884644
Epoch 720, training loss: 0.07297683507204056 = 0.00742288026958704 + 0.01 * 6.555395603179932
Epoch 720, val loss: 1.0828461647033691
Epoch 730, training loss: 0.0725809782743454 = 0.007133751176297665 + 0.01 * 6.544723033905029
Epoch 730, val loss: 1.0883076190948486
Epoch 740, training loss: 0.07251095771789551 = 0.0068625458516180515 + 0.01 * 6.564841270446777
Epoch 740, val loss: 1.0936559438705444
Epoch 750, training loss: 0.07198145240545273 = 0.0066076298244297504 + 0.01 * 6.53738260269165
Epoch 750, val loss: 1.0989106893539429
Epoch 760, training loss: 0.07172983139753342 = 0.006367770954966545 + 0.01 * 6.536206245422363
Epoch 760, val loss: 1.1040229797363281
Epoch 770, training loss: 0.07134056091308594 = 0.00614197039976716 + 0.01 * 6.519859313964844
Epoch 770, val loss: 1.1089880466461182
Epoch 780, training loss: 0.07112953811883926 = 0.005929260980337858 + 0.01 * 6.5200276374816895
Epoch 780, val loss: 1.1138843297958374
Epoch 790, training loss: 0.07098354399204254 = 0.005728486925363541 + 0.01 * 6.525506019592285
Epoch 790, val loss: 1.1186304092407227
Epoch 800, training loss: 0.07067795842885971 = 0.005538807716220617 + 0.01 * 6.513915538787842
Epoch 800, val loss: 1.123307228088379
Epoch 810, training loss: 0.07116270810365677 = 0.0053594158962368965 + 0.01 * 6.580328941345215
Epoch 810, val loss: 1.127899408340454
Epoch 820, training loss: 0.07040231674909592 = 0.00518974382430315 + 0.01 * 6.521256923675537
Epoch 820, val loss: 1.1323710680007935
Epoch 830, training loss: 0.07006818056106567 = 0.005028903484344482 + 0.01 * 6.503928184509277
Epoch 830, val loss: 1.1367287635803223
Epoch 840, training loss: 0.06987351924180984 = 0.004876395687460899 + 0.01 * 6.4997124671936035
Epoch 840, val loss: 1.1409575939178467
Epoch 850, training loss: 0.06972810626029968 = 0.004731747787445784 + 0.01 * 6.499636650085449
Epoch 850, val loss: 1.14516019821167
Epoch 860, training loss: 0.06945869326591492 = 0.004594251047819853 + 0.01 * 6.486443996429443
Epoch 860, val loss: 1.1492509841918945
Epoch 870, training loss: 0.06930045038461685 = 0.004463558550924063 + 0.01 * 6.483688831329346
Epoch 870, val loss: 1.1532613039016724
Epoch 880, training loss: 0.0693238377571106 = 0.0043390789069235325 + 0.01 * 6.498476505279541
Epoch 880, val loss: 1.1571849584579468
Epoch 890, training loss: 0.06908094137907028 = 0.004220812581479549 + 0.01 * 6.486012935638428
Epoch 890, val loss: 1.1610286235809326
Epoch 900, training loss: 0.06891908496618271 = 0.00410796981304884 + 0.01 * 6.481112003326416
Epoch 900, val loss: 1.1647965908050537
Epoch 910, training loss: 0.06872540712356567 = 0.0040003154426813126 + 0.01 * 6.472509384155273
Epoch 910, val loss: 1.1684730052947998
Epoch 920, training loss: 0.06886069476604462 = 0.003897524205967784 + 0.01 * 6.496316909790039
Epoch 920, val loss: 1.1720818281173706
Epoch 930, training loss: 0.06868059188127518 = 0.0037993842270225286 + 0.01 * 6.4881205558776855
Epoch 930, val loss: 1.1756397485733032
Epoch 940, training loss: 0.06835144013166428 = 0.003705563023686409 + 0.01 * 6.464587688446045
Epoch 940, val loss: 1.1790975332260132
Epoch 950, training loss: 0.06829596310853958 = 0.0036158175207674503 + 0.01 * 6.468014240264893
Epoch 950, val loss: 1.1825264692306519
Epoch 960, training loss: 0.06814341247081757 = 0.003529901383444667 + 0.01 * 6.4613518714904785
Epoch 960, val loss: 1.1858638525009155
Epoch 970, training loss: 0.06803056597709656 = 0.0034477096050977707 + 0.01 * 6.458285808563232
Epoch 970, val loss: 1.1891368627548218
Epoch 980, training loss: 0.06808479875326157 = 0.003368933452293277 + 0.01 * 6.471586227416992
Epoch 980, val loss: 1.1923471689224243
Epoch 990, training loss: 0.06780838966369629 = 0.003293376648798585 + 0.01 * 6.451501369476318
Epoch 990, val loss: 1.1955139636993408
Epoch 1000, training loss: 0.06826420873403549 = 0.0032208459451794624 + 0.01 * 6.504335880279541
Epoch 1000, val loss: 1.1985986232757568
Epoch 1010, training loss: 0.0675564557313919 = 0.003151309909299016 + 0.01 * 6.44051456451416
Epoch 1010, val loss: 1.2016252279281616
Epoch 1020, training loss: 0.06757327169179916 = 0.0030846153385937214 + 0.01 * 6.44886589050293
Epoch 1020, val loss: 1.2046334743499756
Epoch 1030, training loss: 0.0674271360039711 = 0.0030203491915017366 + 0.01 * 6.44067907333374
Epoch 1030, val loss: 1.207534909248352
Epoch 1040, training loss: 0.06770472973585129 = 0.002958565019071102 + 0.01 * 6.474616527557373
Epoch 1040, val loss: 1.210422158241272
Epoch 1050, training loss: 0.06736753135919571 = 0.002899050246924162 + 0.01 * 6.446847915649414
Epoch 1050, val loss: 1.213228702545166
Epoch 1060, training loss: 0.06714116036891937 = 0.002841872163116932 + 0.01 * 6.429928779602051
Epoch 1060, val loss: 1.2160142660140991
Epoch 1070, training loss: 0.06705491244792938 = 0.002786915982142091 + 0.01 * 6.426799774169922
Epoch 1070, val loss: 1.2187690734863281
Epoch 1080, training loss: 0.06731632351875305 = 0.002733853878453374 + 0.01 * 6.45824670791626
Epoch 1080, val loss: 1.2214398384094238
Epoch 1090, training loss: 0.06697744131088257 = 0.0026826937682926655 + 0.01 * 6.4294753074646
Epoch 1090, val loss: 1.2240490913391113
Epoch 1100, training loss: 0.06744132190942764 = 0.0026334207504987717 + 0.01 * 6.480790138244629
Epoch 1100, val loss: 1.2266465425491333
Epoch 1110, training loss: 0.06684433668851852 = 0.002585927490144968 + 0.01 * 6.425841331481934
Epoch 1110, val loss: 1.229177713394165
Epoch 1120, training loss: 0.06715147197246552 = 0.0025400251615792513 + 0.01 * 6.461145401000977
Epoch 1120, val loss: 1.2316817045211792
Epoch 1130, training loss: 0.06655111908912659 = 0.0024956269189715385 + 0.01 * 6.4055495262146
Epoch 1130, val loss: 1.2341089248657227
Epoch 1140, training loss: 0.0666261538863182 = 0.002452805871143937 + 0.01 * 6.417335033416748
Epoch 1140, val loss: 1.2365598678588867
Epoch 1150, training loss: 0.06663213670253754 = 0.0024113093968480825 + 0.01 * 6.422082901000977
Epoch 1150, val loss: 1.2389142513275146
Epoch 1160, training loss: 0.06640075147151947 = 0.002371322363615036 + 0.01 * 6.402943134307861
Epoch 1160, val loss: 1.2412338256835938
Epoch 1170, training loss: 0.06648615002632141 = 0.00233270856551826 + 0.01 * 6.41534423828125
Epoch 1170, val loss: 1.2435468435287476
Epoch 1180, training loss: 0.0662301778793335 = 0.002295276615768671 + 0.01 * 6.393490314483643
Epoch 1180, val loss: 1.2457709312438965
Epoch 1190, training loss: 0.06619123369455338 = 0.0022590900771319866 + 0.01 * 6.393214225769043
Epoch 1190, val loss: 1.248003602027893
Epoch 1200, training loss: 0.06611919403076172 = 0.0022239508107304573 + 0.01 * 6.389523983001709
Epoch 1200, val loss: 1.2501767873764038
Epoch 1210, training loss: 0.06599190831184387 = 0.0021899389103055 + 0.01 * 6.380196571350098
Epoch 1210, val loss: 1.2523120641708374
Epoch 1220, training loss: 0.06595402210950851 = 0.002157008508220315 + 0.01 * 6.379701614379883
Epoch 1220, val loss: 1.2544291019439697
Epoch 1230, training loss: 0.06604976207017899 = 0.002125066239386797 + 0.01 * 6.39246940612793
Epoch 1230, val loss: 1.256515622138977
Epoch 1240, training loss: 0.06603307276964188 = 0.0020940667018294334 + 0.01 * 6.3939008712768555
Epoch 1240, val loss: 1.2585482597351074
Epoch 1250, training loss: 0.06626071780920029 = 0.0020640711300075054 + 0.01 * 6.4196648597717285
Epoch 1250, val loss: 1.2605758905410767
Epoch 1260, training loss: 0.0659269466996193 = 0.0020348927937448025 + 0.01 * 6.389205455780029
Epoch 1260, val loss: 1.2625447511672974
Epoch 1270, training loss: 0.06608498841524124 = 0.0020067254081368446 + 0.01 * 6.407826900482178
Epoch 1270, val loss: 1.2645251750946045
Epoch 1280, training loss: 0.06565359979867935 = 0.001979149878025055 + 0.01 * 6.367445468902588
Epoch 1280, val loss: 1.266419529914856
Epoch 1290, training loss: 0.06587067991495132 = 0.0019524880917742848 + 0.01 * 6.391819000244141
Epoch 1290, val loss: 1.26832115650177
Epoch 1300, training loss: 0.06553023308515549 = 0.0019265515729784966 + 0.01 * 6.360368728637695
Epoch 1300, val loss: 1.2701505422592163
Epoch 1310, training loss: 0.06574652343988419 = 0.001901415060274303 + 0.01 * 6.3845109939575195
Epoch 1310, val loss: 1.2720069885253906
Epoch 1320, training loss: 0.06557926535606384 = 0.0018768607405945659 + 0.01 * 6.370240688323975
Epoch 1320, val loss: 1.2738025188446045
Epoch 1330, training loss: 0.06563198566436768 = 0.001853103400208056 + 0.01 * 6.3778886795043945
Epoch 1330, val loss: 1.275603175163269
Epoch 1340, training loss: 0.06526262313127518 = 0.001829886925406754 + 0.01 * 6.343274116516113
Epoch 1340, val loss: 1.2773536443710327
Epoch 1350, training loss: 0.06527897715568542 = 0.0018073533428832889 + 0.01 * 6.34716272354126
Epoch 1350, val loss: 1.2790939807891846
Epoch 1360, training loss: 0.06555125117301941 = 0.001785408123396337 + 0.01 * 6.376585006713867
Epoch 1360, val loss: 1.2808021306991577
Epoch 1370, training loss: 0.06542433798313141 = 0.0017640371806919575 + 0.01 * 6.366030216217041
Epoch 1370, val loss: 1.2824779748916626
Epoch 1380, training loss: 0.06533219665288925 = 0.0017433555331081152 + 0.01 * 6.358884334564209
Epoch 1380, val loss: 1.2841830253601074
Epoch 1390, training loss: 0.06541480123996735 = 0.0017230528173968196 + 0.01 * 6.369174957275391
Epoch 1390, val loss: 1.285801887512207
Epoch 1400, training loss: 0.06539347022771835 = 0.0017034569755196571 + 0.01 * 6.369001865386963
Epoch 1400, val loss: 1.287431240081787
Epoch 1410, training loss: 0.0650143176317215 = 0.0016841748729348183 + 0.01 * 6.333014488220215
Epoch 1410, val loss: 1.289031982421875
Epoch 1420, training loss: 0.06551013141870499 = 0.0016654764767736197 + 0.01 * 6.38446569442749
Epoch 1420, val loss: 1.2906228303909302
Epoch 1430, training loss: 0.06484927237033844 = 0.0016471687704324722 + 0.01 * 6.3202104568481445
Epoch 1430, val loss: 1.2921732664108276
Epoch 1440, training loss: 0.065030038356781 = 0.0016293583903461695 + 0.01 * 6.3400678634643555
Epoch 1440, val loss: 1.293717861175537
Epoch 1450, training loss: 0.06505075097084045 = 0.0016119696665555239 + 0.01 * 6.343878269195557
Epoch 1450, val loss: 1.2952128648757935
Epoch 1460, training loss: 0.06490936130285263 = 0.0015950107481330633 + 0.01 * 6.331435203552246
Epoch 1460, val loss: 1.2967134714126587
Epoch 1470, training loss: 0.0648680180311203 = 0.001578521798364818 + 0.01 * 6.328949451446533
Epoch 1470, val loss: 1.2981947660446167
Epoch 1480, training loss: 0.06478869169950485 = 0.0015623423969373107 + 0.01 * 6.322635173797607
Epoch 1480, val loss: 1.2996402978897095
Epoch 1490, training loss: 0.0649447962641716 = 0.001546552055515349 + 0.01 * 6.339824676513672
Epoch 1490, val loss: 1.3010597229003906
Epoch 1500, training loss: 0.06488940864801407 = 0.0015311372699216008 + 0.01 * 6.335826873779297
Epoch 1500, val loss: 1.30245840549469
Epoch 1510, training loss: 0.06495386362075806 = 0.0015160789480432868 + 0.01 * 6.343778133392334
Epoch 1510, val loss: 1.3038686513900757
Epoch 1520, training loss: 0.06467415392398834 = 0.0015013701049610972 + 0.01 * 6.317278861999512
Epoch 1520, val loss: 1.305228352546692
Epoch 1530, training loss: 0.06489858031272888 = 0.0014869887381792068 + 0.01 * 6.341159343719482
Epoch 1530, val loss: 1.3065917491912842
Epoch 1540, training loss: 0.06452278047800064 = 0.0014729038812220097 + 0.01 * 6.304987907409668
Epoch 1540, val loss: 1.3078988790512085
Epoch 1550, training loss: 0.06489526480436325 = 0.0014591964427381754 + 0.01 * 6.343606948852539
Epoch 1550, val loss: 1.3092193603515625
Epoch 1560, training loss: 0.06454285979270935 = 0.0014457368524745107 + 0.01 * 6.309711933135986
Epoch 1560, val loss: 1.3105145692825317
Epoch 1570, training loss: 0.06473835557699203 = 0.0014325676020234823 + 0.01 * 6.3305792808532715
Epoch 1570, val loss: 1.3117974996566772
Epoch 1580, training loss: 0.0644305944442749 = 0.0014196791453287005 + 0.01 * 6.30109167098999
Epoch 1580, val loss: 1.313031554222107
Epoch 1590, training loss: 0.06466741859912872 = 0.0014070853358134627 + 0.01 * 6.326033592224121
Epoch 1590, val loss: 1.3143056631088257
Epoch 1600, training loss: 0.06424163281917572 = 0.001394770573824644 + 0.01 * 6.284686088562012
Epoch 1600, val loss: 1.3155196905136108
Epoch 1610, training loss: 0.06441033631563187 = 0.0013827185612171888 + 0.01 * 6.302762031555176
Epoch 1610, val loss: 1.3167283535003662
Epoch 1620, training loss: 0.06456723064184189 = 0.0013708937913179398 + 0.01 * 6.319633960723877
Epoch 1620, val loss: 1.3178902864456177
Epoch 1630, training loss: 0.06420017778873444 = 0.0013593542389571667 + 0.01 * 6.284082889556885
Epoch 1630, val loss: 1.3190841674804688
Epoch 1640, training loss: 0.06442918628454208 = 0.0013480359921231866 + 0.01 * 6.308115482330322
Epoch 1640, val loss: 1.320250153541565
Epoch 1650, training loss: 0.06409329921007156 = 0.0013368776999413967 + 0.01 * 6.275641918182373
Epoch 1650, val loss: 1.3213928937911987
Epoch 1660, training loss: 0.06427155435085297 = 0.0013260561972856522 + 0.01 * 6.294549942016602
Epoch 1660, val loss: 1.322548270225525
Epoch 1670, training loss: 0.06408477574586868 = 0.0013153661275282502 + 0.01 * 6.276941776275635
Epoch 1670, val loss: 1.3236737251281738
Epoch 1680, training loss: 0.06403864920139313 = 0.0013048929395154119 + 0.01 * 6.273375511169434
Epoch 1680, val loss: 1.3247826099395752
Epoch 1690, training loss: 0.06423229724168777 = 0.001294657588005066 + 0.01 * 6.293764114379883
Epoch 1690, val loss: 1.3258670568466187
Epoch 1700, training loss: 0.06424978375434875 = 0.0012845740420743823 + 0.01 * 6.296521186828613
Epoch 1700, val loss: 1.3269498348236084
Epoch 1710, training loss: 0.06420985609292984 = 0.0012747426517307758 + 0.01 * 6.293511390686035
Epoch 1710, val loss: 1.3280260562896729
Epoch 1720, training loss: 0.06411470472812653 = 0.0012650848366320133 + 0.01 * 6.284962177276611
Epoch 1720, val loss: 1.329068660736084
Epoch 1730, training loss: 0.06413574516773224 = 0.0012555745197460055 + 0.01 * 6.288017272949219
Epoch 1730, val loss: 1.330107569694519
Epoch 1740, training loss: 0.06392437219619751 = 0.0012463127495720983 + 0.01 * 6.267806053161621
Epoch 1740, val loss: 1.3311049938201904
Epoch 1750, training loss: 0.06382741779088974 = 0.0012371494667604566 + 0.01 * 6.259027481079102
Epoch 1750, val loss: 1.332123875617981
Epoch 1760, training loss: 0.0643463209271431 = 0.0012282307725399733 + 0.01 * 6.311809062957764
Epoch 1760, val loss: 1.3330861330032349
Epoch 1770, training loss: 0.06430802494287491 = 0.0012194248847663403 + 0.01 * 6.3088603019714355
Epoch 1770, val loss: 1.3340885639190674
Epoch 1780, training loss: 0.06382090598344803 = 0.0012108325026929379 + 0.01 * 6.261007785797119
Epoch 1780, val loss: 1.3350450992584229
Epoch 1790, training loss: 0.06409194320440292 = 0.001202352694235742 + 0.01 * 6.28895902633667
Epoch 1790, val loss: 1.336012601852417
Epoch 1800, training loss: 0.06396744400262833 = 0.001194029115140438 + 0.01 * 6.277341365814209
Epoch 1800, val loss: 1.3369269371032715
Epoch 1810, training loss: 0.06385631114244461 = 0.0011858823709189892 + 0.01 * 6.267043590545654
Epoch 1810, val loss: 1.3378820419311523
Epoch 1820, training loss: 0.06387883424758911 = 0.0011778459884226322 + 0.01 * 6.27009916305542
Epoch 1820, val loss: 1.3387625217437744
Epoch 1830, training loss: 0.06390950083732605 = 0.0011699581518769264 + 0.01 * 6.27395486831665
Epoch 1830, val loss: 1.3397029638290405
Epoch 1840, training loss: 0.06369347870349884 = 0.0011622142046689987 + 0.01 * 6.253126621246338
Epoch 1840, val loss: 1.3405967950820923
Epoch 1850, training loss: 0.06390760093927383 = 0.0011545908637344837 + 0.01 * 6.2753005027771
Epoch 1850, val loss: 1.3414993286132812
Epoch 1860, training loss: 0.06372189521789551 = 0.0011470483150333166 + 0.01 * 6.2574849128723145
Epoch 1860, val loss: 1.3423606157302856
Epoch 1870, training loss: 0.0637894794344902 = 0.0011397269554436207 + 0.01 * 6.264975070953369
Epoch 1870, val loss: 1.3432321548461914
Epoch 1880, training loss: 0.06393595039844513 = 0.001132496283389628 + 0.01 * 6.280345439910889
Epoch 1880, val loss: 1.3441165685653687
Epoch 1890, training loss: 0.06357375532388687 = 0.0011253717821091413 + 0.01 * 6.244838237762451
Epoch 1890, val loss: 1.3449745178222656
Epoch 1900, training loss: 0.0636378601193428 = 0.0011183638125658035 + 0.01 * 6.251950263977051
Epoch 1900, val loss: 1.3458044528961182
Epoch 1910, training loss: 0.06368614733219147 = 0.001111457822844386 + 0.01 * 6.257469654083252
Epoch 1910, val loss: 1.3466284275054932
Epoch 1920, training loss: 0.06349720060825348 = 0.0011046638246625662 + 0.01 * 6.239253997802734
Epoch 1920, val loss: 1.347449779510498
Epoch 1930, training loss: 0.06365547329187393 = 0.0010980250081047416 + 0.01 * 6.2557454109191895
Epoch 1930, val loss: 1.3482545614242554
Epoch 1940, training loss: 0.06366589665412903 = 0.0010914328740909696 + 0.01 * 6.2574462890625
Epoch 1940, val loss: 1.3490890264511108
Epoch 1950, training loss: 0.06346237659454346 = 0.001084956107661128 + 0.01 * 6.2377424240112305
Epoch 1950, val loss: 1.349871039390564
Epoch 1960, training loss: 0.06367851793766022 = 0.0010785942431539297 + 0.01 * 6.259992599487305
Epoch 1960, val loss: 1.3506661653518677
Epoch 1970, training loss: 0.06364496797323227 = 0.0010723304003477097 + 0.01 * 6.257263660430908
Epoch 1970, val loss: 1.3514329195022583
Epoch 1980, training loss: 0.06363179534673691 = 0.0010661766864359379 + 0.01 * 6.256561756134033
Epoch 1980, val loss: 1.3522380590438843
Epoch 1990, training loss: 0.06339240819215775 = 0.0010601168032735586 + 0.01 * 6.233229637145996
Epoch 1990, val loss: 1.353037714958191
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.69004, 0.21430, Accuracy:0.80494, 0.02572
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11554])
remove edge: torch.Size([2, 9482])
updated graph: torch.Size([2, 10480])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.97909, 0.00460, Accuracy:0.82963, 0.00524
Begin epxeriment: cont_weight: 0.01 epoch:2000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.03639554977417 = 1.952656865119934 + 0.01 * 8.373869895935059
Epoch 0, val loss: 1.9568300247192383
Epoch 10, training loss: 2.025804281234741 = 1.9420661926269531 + 0.01 * 8.373801231384277
Epoch 10, val loss: 1.9460431337356567
Epoch 20, training loss: 2.012721538543701 = 1.9289861917495728 + 0.01 * 8.373546600341797
Epoch 20, val loss: 1.9325132369995117
Epoch 30, training loss: 1.9943077564239502 = 1.9105799198150635 + 0.01 * 8.372786521911621
Epoch 30, val loss: 1.9134489297866821
Epoch 40, training loss: 1.9668669700622559 = 1.883177399635315 + 0.01 * 8.368961334228516
Epoch 40, val loss: 1.8854104280471802
Epoch 50, training loss: 1.9276084899902344 = 1.8442027568817139 + 0.01 * 8.340577125549316
Epoch 50, val loss: 1.8471037149429321
Epoch 60, training loss: 1.8795374631881714 = 1.7979183197021484 + 0.01 * 8.16191291809082
Epoch 60, val loss: 1.8053333759307861
Epoch 70, training loss: 1.8304128646850586 = 1.7539961338043213 + 0.01 * 7.64166784286499
Epoch 70, val loss: 1.769775629043579
Epoch 80, training loss: 1.7761260271072388 = 1.70182204246521 + 0.01 * 7.430398464202881
Epoch 80, val loss: 1.7262279987335205
Epoch 90, training loss: 1.703192114830017 = 1.6304173469543457 + 0.01 * 7.2774739265441895
Epoch 90, val loss: 1.6651031970977783
Epoch 100, training loss: 1.6095296144485474 = 1.537603735923767 + 0.01 * 7.1925859451293945
Epoch 100, val loss: 1.5863254070281982
Epoch 110, training loss: 1.4999626874923706 = 1.4286590814590454 + 0.01 * 7.1303629875183105
Epoch 110, val loss: 1.496644139289856
Epoch 120, training loss: 1.3858903646469116 = 1.315126895904541 + 0.01 * 7.076344966888428
Epoch 120, val loss: 1.4044750928878784
Epoch 130, training loss: 1.2760605812072754 = 1.2058228254318237 + 0.01 * 7.023778438568115
Epoch 130, val loss: 1.3180705308914185
Epoch 140, training loss: 1.1762562990188599 = 1.1065521240234375 + 0.01 * 6.970412731170654
Epoch 140, val loss: 1.2417519092559814
Epoch 150, training loss: 1.0901390314102173 = 1.020992398262024 + 0.01 * 6.914669036865234
Epoch 150, val loss: 1.1782561540603638
Epoch 160, training loss: 1.016656756401062 = 0.9480570554733276 + 0.01 * 6.859973430633545
Epoch 160, val loss: 1.1258935928344727
Epoch 170, training loss: 0.9510632157325745 = 0.8828550577163696 + 0.01 * 6.820816993713379
Epoch 170, val loss: 1.0802992582321167
Epoch 180, training loss: 0.8883422613143921 = 0.8203138709068298 + 0.01 * 6.8028388023376465
Epoch 180, val loss: 1.037057638168335
Epoch 190, training loss: 0.8249924182891846 = 0.7570276260375977 + 0.01 * 6.796482086181641
Epoch 190, val loss: 0.9928462505340576
Epoch 200, training loss: 0.7600508332252502 = 0.6921148896217346 + 0.01 * 6.793593406677246
Epoch 200, val loss: 0.9470676779747009
Epoch 210, training loss: 0.6950381994247437 = 0.6271360516548157 + 0.01 * 6.7902140617370605
Epoch 210, val loss: 0.9015532732009888
Epoch 220, training loss: 0.6328350305557251 = 0.5649734735488892 + 0.01 * 6.786154747009277
Epoch 220, val loss: 0.8596386909484863
Epoch 230, training loss: 0.575941801071167 = 0.5081270933151245 + 0.01 * 6.7814741134643555
Epoch 230, val loss: 0.8240334987640381
Epoch 240, training loss: 0.5253691673278809 = 0.45760220289230347 + 0.01 * 6.776699066162109
Epoch 240, val loss: 0.796161413192749
Epoch 250, training loss: 0.4806720018386841 = 0.41297197341918945 + 0.01 * 6.770001411437988
Epoch 250, val loss: 0.7754704356193542
Epoch 260, training loss: 0.44077178835868835 = 0.3731492757797241 + 0.01 * 6.762252330780029
Epoch 260, val loss: 0.7604535222053528
Epoch 270, training loss: 0.40449249744415283 = 0.3369492292404175 + 0.01 * 6.7543253898620605
Epoch 270, val loss: 0.749936044216156
Epoch 280, training loss: 0.37092921137809753 = 0.3034723997116089 + 0.01 * 6.745681285858154
Epoch 280, val loss: 0.7433887124061584
Epoch 290, training loss: 0.3395640254020691 = 0.27219322323799133 + 0.01 * 6.737081527709961
Epoch 290, val loss: 0.7404237985610962
Epoch 300, training loss: 0.3102377951145172 = 0.24295838177204132 + 0.01 * 6.727941513061523
Epoch 300, val loss: 0.7408844828605652
Epoch 310, training loss: 0.28303736448287964 = 0.21585437655448914 + 0.01 * 6.718297958374023
Epoch 310, val loss: 0.7449573874473572
Epoch 320, training loss: 0.25815144181251526 = 0.191030353307724 + 0.01 * 6.712109565734863
Epoch 320, val loss: 0.752716600894928
Epoch 330, training loss: 0.23568549752235413 = 0.16858966648578644 + 0.01 * 6.7095842361450195
Epoch 330, val loss: 0.7640476226806641
Epoch 340, training loss: 0.21560636162757874 = 0.14854390919208527 + 0.01 * 6.706244945526123
Epoch 340, val loss: 0.7786224484443665
Epoch 350, training loss: 0.19781486690044403 = 0.13078908622264862 + 0.01 * 6.702578544616699
Epoch 350, val loss: 0.7960766553878784
Epoch 360, training loss: 0.1821478307247162 = 0.11515593528747559 + 0.01 * 6.699190616607666
Epoch 360, val loss: 0.8157472014427185
Epoch 370, training loss: 0.16840089857578278 = 0.10144563019275665 + 0.01 * 6.695527076721191
Epoch 370, val loss: 0.8370286822319031
Epoch 380, training loss: 0.15637367963790894 = 0.08944528549909592 + 0.01 * 6.692840576171875
Epoch 380, val loss: 0.859309732913971
Epoch 390, training loss: 0.14588919281959534 = 0.07896163314580917 + 0.01 * 6.692755222320557
Epoch 390, val loss: 0.8822036981582642
Epoch 400, training loss: 0.1367209255695343 = 0.0698218047618866 + 0.01 * 6.689913272857666
Epoch 400, val loss: 0.9052504897117615
Epoch 410, training loss: 0.1287415325641632 = 0.0618734247982502 + 0.01 * 6.686811447143555
Epoch 410, val loss: 0.9281377792358398
Epoch 420, training loss: 0.12184514105319977 = 0.054981451481580734 + 0.01 * 6.686368465423584
Epoch 420, val loss: 0.9506270885467529
Epoch 430, training loss: 0.1158420741558075 = 0.04901765659451485 + 0.01 * 6.6824421882629395
Epoch 430, val loss: 0.9726017713546753
Epoch 440, training loss: 0.11069867014884949 = 0.04385826736688614 + 0.01 * 6.684040069580078
Epoch 440, val loss: 0.9938350915908813
Epoch 450, training loss: 0.10618939995765686 = 0.03939170390367508 + 0.01 * 6.679769992828369
Epoch 450, val loss: 1.0142613649368286
Epoch 460, training loss: 0.10228084027767181 = 0.03551725670695305 + 0.01 * 6.676359176635742
Epoch 460, val loss: 1.0338994264602661
Epoch 470, training loss: 0.0988798439502716 = 0.03214764967560768 + 0.01 * 6.673219680786133
Epoch 470, val loss: 1.052706003189087
Epoch 480, training loss: 0.09593048691749573 = 0.029208241030573845 + 0.01 * 6.672224521636963
Epoch 480, val loss: 1.0707502365112305
Epoch 490, training loss: 0.09332789480686188 = 0.026636401191353798 + 0.01 * 6.669149398803711
Epoch 490, val loss: 1.0880643129348755
Epoch 500, training loss: 0.09105896204710007 = 0.024379080161452293 + 0.01 * 6.667987823486328
Epoch 500, val loss: 1.1046658754348755
Epoch 510, training loss: 0.08903180062770844 = 0.02238955721259117 + 0.01 * 6.664224624633789
Epoch 510, val loss: 1.1206144094467163
Epoch 520, training loss: 0.08724432438611984 = 0.020629560574889183 + 0.01 * 6.6614766120910645
Epoch 520, val loss: 1.1359590291976929
Epoch 530, training loss: 0.0856582373380661 = 0.01906703971326351 + 0.01 * 6.659120082855225
Epoch 530, val loss: 1.1507185697555542
Epoch 540, training loss: 0.08427011221647263 = 0.017675042152404785 + 0.01 * 6.6595072746276855
Epoch 540, val loss: 1.1649526357650757
Epoch 550, training loss: 0.0829785093665123 = 0.01643064059317112 + 0.01 * 6.654787063598633
Epoch 550, val loss: 1.1786279678344727
Epoch 560, training loss: 0.08196646720170975 = 0.015314430929720402 + 0.01 * 6.665204048156738
Epoch 560, val loss: 1.1918144226074219
Epoch 570, training loss: 0.08081907778978348 = 0.014310372062027454 + 0.01 * 6.650870323181152
Epoch 570, val loss: 1.204533338546753
Epoch 580, training loss: 0.0798644945025444 = 0.013403790071606636 + 0.01 * 6.64607048034668
Epoch 580, val loss: 1.2168402671813965
Epoch 590, training loss: 0.07901525497436523 = 0.012582187540829182 + 0.01 * 6.643306732177734
Epoch 590, val loss: 1.228752851486206
Epoch 600, training loss: 0.07824063301086426 = 0.011834708973765373 + 0.01 * 6.640592575073242
Epoch 600, val loss: 1.2402974367141724
Epoch 610, training loss: 0.0775766670703888 = 0.011151690967381 + 0.01 * 6.642497539520264
Epoch 610, val loss: 1.2515150308609009
Epoch 620, training loss: 0.07687855511903763 = 0.010526091791689396 + 0.01 * 6.635246753692627
Epoch 620, val loss: 1.262406349182129
Epoch 630, training loss: 0.0763438418507576 = 0.009951180778443813 + 0.01 * 6.639266490936279
Epoch 630, val loss: 1.2729653120040894
Epoch 640, training loss: 0.0757259652018547 = 0.009425012394785881 + 0.01 * 6.6300950050354
Epoch 640, val loss: 1.2832388877868652
Epoch 650, training loss: 0.07520744204521179 = 0.008940856903791428 + 0.01 * 6.6266584396362305
Epoch 650, val loss: 1.2932343482971191
Epoch 660, training loss: 0.07471811026334763 = 0.008494221605360508 + 0.01 * 6.622389316558838
Epoch 660, val loss: 1.3029475212097168
Epoch 670, training loss: 0.0743829682469368 = 0.008081028237938881 + 0.01 * 6.630194664001465
Epoch 670, val loss: 1.3124547004699707
Epoch 680, training loss: 0.0738685354590416 = 0.007698781322687864 + 0.01 * 6.6169753074646
Epoch 680, val loss: 1.3217101097106934
Epoch 690, training loss: 0.07348234951496124 = 0.007344309706240892 + 0.01 * 6.613803863525391
Epoch 690, val loss: 1.3307406902313232
Epoch 700, training loss: 0.0731361135840416 = 0.00701484689489007 + 0.01 * 6.612126350402832
Epoch 700, val loss: 1.3395164012908936
Epoch 710, training loss: 0.07283339649438858 = 0.006709170527756214 + 0.01 * 6.612422466278076
Epoch 710, val loss: 1.3480982780456543
Epoch 720, training loss: 0.07246632128953934 = 0.00642464030534029 + 0.01 * 6.604167938232422
Epoch 720, val loss: 1.3563978672027588
Epoch 730, training loss: 0.07215292006731033 = 0.006159086246043444 + 0.01 * 6.5993828773498535
Epoch 730, val loss: 1.364533543586731
Epoch 740, training loss: 0.07196985185146332 = 0.005910628475248814 + 0.01 * 6.605922222137451
Epoch 740, val loss: 1.372498631477356
Epoch 750, training loss: 0.07172227650880814 = 0.005678457673639059 + 0.01 * 6.604381561279297
Epoch 750, val loss: 1.3802151679992676
Epoch 760, training loss: 0.07138684391975403 = 0.005461729597300291 + 0.01 * 6.5925116539001465
Epoch 760, val loss: 1.387733817100525
Epoch 770, training loss: 0.07107897847890854 = 0.005258573219180107 + 0.01 * 6.582040786743164
Epoch 770, val loss: 1.3950529098510742
Epoch 780, training loss: 0.07132859528064728 = 0.00506771681830287 + 0.01 * 6.6260881423950195
Epoch 780, val loss: 1.4022239446640015
Epoch 790, training loss: 0.0707494243979454 = 0.004889032803475857 + 0.01 * 6.586039066314697
Epoch 790, val loss: 1.409177303314209
Epoch 800, training loss: 0.07041672617197037 = 0.00472101429477334 + 0.01 * 6.569571018218994
Epoch 800, val loss: 1.4159622192382812
Epoch 810, training loss: 0.07028806954622269 = 0.0045625194907188416 + 0.01 * 6.572555065155029
Epoch 810, val loss: 1.4226144552230835
Epoch 820, training loss: 0.07018591463565826 = 0.004413045477122068 + 0.01 * 6.577286720275879
Epoch 820, val loss: 1.4290920495986938
Epoch 830, training loss: 0.07010754197835922 = 0.004272050689905882 + 0.01 * 6.583549499511719
Epoch 830, val loss: 1.4354279041290283
Epoch 840, training loss: 0.06979677826166153 = 0.0041389609687030315 + 0.01 * 6.565782070159912
Epoch 840, val loss: 1.441590428352356
Epoch 850, training loss: 0.06949261575937271 = 0.0040132650174200535 + 0.01 * 6.547935485839844
Epoch 850, val loss: 1.4476045370101929
Epoch 860, training loss: 0.06963180005550385 = 0.0038941260427236557 + 0.01 * 6.573767185211182
Epoch 860, val loss: 1.4535136222839355
Epoch 870, training loss: 0.06929230690002441 = 0.00378173659555614 + 0.01 * 6.5510573387146
Epoch 870, val loss: 1.4592015743255615
Epoch 880, training loss: 0.06901419907808304 = 0.0036752144806087017 + 0.01 * 6.533898830413818
Epoch 880, val loss: 1.464762806892395
Epoch 890, training loss: 0.0690801590681076 = 0.0035737324506044388 + 0.01 * 6.550642967224121
Epoch 890, val loss: 1.470191240310669
Epoch 900, training loss: 0.06891040503978729 = 0.0034775498788803816 + 0.01 * 6.543285846710205
Epoch 900, val loss: 1.4755290746688843
Epoch 910, training loss: 0.0686817392706871 = 0.003385974792763591 + 0.01 * 6.529576301574707
Epoch 910, val loss: 1.480738878250122
Epoch 920, training loss: 0.06856058537960052 = 0.003298726864159107 + 0.01 * 6.526185512542725
Epoch 920, val loss: 1.4858187437057495
Epoch 930, training loss: 0.06841779500246048 = 0.0032154996879398823 + 0.01 * 6.520229816436768
Epoch 930, val loss: 1.4908233880996704
Epoch 940, training loss: 0.06823300570249557 = 0.0031361873261630535 + 0.01 * 6.5096821784973145
Epoch 940, val loss: 1.495733618736267
Epoch 950, training loss: 0.06823911517858505 = 0.003060505259782076 + 0.01 * 6.517861366271973
Epoch 950, val loss: 1.5005351305007935
Epoch 960, training loss: 0.06808019429445267 = 0.0029882455710321665 + 0.01 * 6.509194850921631
Epoch 960, val loss: 1.5051934719085693
Epoch 970, training loss: 0.06798448413610458 = 0.0029191463254392147 + 0.01 * 6.506534099578857
Epoch 970, val loss: 1.5097942352294922
Epoch 980, training loss: 0.06786957383155823 = 0.0028529358096420765 + 0.01 * 6.501663684844971
Epoch 980, val loss: 1.5143356323242188
Epoch 990, training loss: 0.06783328205347061 = 0.0027896002866327763 + 0.01 * 6.504368305206299
Epoch 990, val loss: 1.51873779296875
Epoch 1000, training loss: 0.06763418018817902 = 0.0027289127465337515 + 0.01 * 6.490527153015137
Epoch 1000, val loss: 1.5230765342712402
Epoch 1010, training loss: 0.06770770251750946 = 0.00267068506218493 + 0.01 * 6.503702163696289
Epoch 1010, val loss: 1.5273382663726807
Epoch 1020, training loss: 0.06746043264865875 = 0.00261487509123981 + 0.01 * 6.484555244445801
Epoch 1020, val loss: 1.5314959287643433
Epoch 1030, training loss: 0.06778237223625183 = 0.0025612374301999807 + 0.01 * 6.522114276885986
Epoch 1030, val loss: 1.5356448888778687
Epoch 1040, training loss: 0.06734175235033035 = 0.002509753219783306 + 0.01 * 6.483200550079346
Epoch 1040, val loss: 1.5396257638931274
Epoch 1050, training loss: 0.0672421082854271 = 0.0024602990597486496 + 0.01 * 6.478180885314941
Epoch 1050, val loss: 1.5435632467269897
Epoch 1060, training loss: 0.06726719439029694 = 0.0024125531781464815 + 0.01 * 6.485464572906494
Epoch 1060, val loss: 1.5474963188171387
Epoch 1070, training loss: 0.06714221090078354 = 0.002366537693887949 + 0.01 * 6.477567672729492
Epoch 1070, val loss: 1.551327109336853
Epoch 1080, training loss: 0.06723704934120178 = 0.0023221622686833143 + 0.01 * 6.491489410400391
Epoch 1080, val loss: 1.555112600326538
Epoch 1090, training loss: 0.06699872016906738 = 0.0022794611286371946 + 0.01 * 6.471926212310791
Epoch 1090, val loss: 1.5588663816452026
Epoch 1100, training loss: 0.0670071542263031 = 0.002238315064460039 + 0.01 * 6.476883411407471
Epoch 1100, val loss: 1.5625078678131104
Epoch 1110, training loss: 0.06690070778131485 = 0.0021986872889101505 + 0.01 * 6.4702019691467285
Epoch 1110, val loss: 1.5661176443099976
Epoch 1120, training loss: 0.0667632445693016 = 0.002160466043278575 + 0.01 * 6.460277557373047
Epoch 1120, val loss: 1.5696055889129639
Epoch 1130, training loss: 0.06677069514989853 = 0.002123625250533223 + 0.01 * 6.464706897735596
Epoch 1130, val loss: 1.5730862617492676
Epoch 1140, training loss: 0.06663137674331665 = 0.0020880866795778275 + 0.01 * 6.454329490661621
Epoch 1140, val loss: 1.5763872861862183
Epoch 1150, training loss: 0.0667225569486618 = 0.0020538270473480225 + 0.01 * 6.4668731689453125
Epoch 1150, val loss: 1.5797312259674072
Epoch 1160, training loss: 0.06655549257993698 = 0.0020207769703119993 + 0.01 * 6.453472137451172
Epoch 1160, val loss: 1.582869529724121
Epoch 1170, training loss: 0.06649814546108246 = 0.001988805364817381 + 0.01 * 6.450933933258057
Epoch 1170, val loss: 1.5860587358474731
Epoch 1180, training loss: 0.06648685783147812 = 0.001957888947799802 + 0.01 * 6.452897548675537
Epoch 1180, val loss: 1.5891526937484741
Epoch 1190, training loss: 0.06672199070453644 = 0.0019279668340459466 + 0.01 * 6.479402542114258
Epoch 1190, val loss: 1.5921982526779175
Epoch 1200, training loss: 0.06640858948230743 = 0.0018990468233823776 + 0.01 * 6.450953960418701
Epoch 1200, val loss: 1.5952198505401611
Epoch 1210, training loss: 0.06637220084667206 = 0.001871064305305481 + 0.01 * 6.450113773345947
Epoch 1210, val loss: 1.5981371402740479
Epoch 1220, training loss: 0.06620711088180542 = 0.0018439504783600569 + 0.01 * 6.436316013336182
Epoch 1220, val loss: 1.601041555404663
Epoch 1230, training loss: 0.06649744510650635 = 0.001817653770558536 + 0.01 * 6.467979431152344
Epoch 1230, val loss: 1.6038475036621094
Epoch 1240, training loss: 0.0661093145608902 = 0.0017922031693160534 + 0.01 * 6.431711673736572
Epoch 1240, val loss: 1.6067317724227905
Epoch 1250, training loss: 0.06639351695775986 = 0.001767574343830347 + 0.01 * 6.462594032287598
Epoch 1250, val loss: 1.6094346046447754
Epoch 1260, training loss: 0.0662011057138443 = 0.0017437583301216364 + 0.01 * 6.445734977722168
Epoch 1260, val loss: 1.6121234893798828
Epoch 1270, training loss: 0.06597478687763214 = 0.0017206687480211258 + 0.01 * 6.425412178039551
Epoch 1270, val loss: 1.6148048639297485
Epoch 1280, training loss: 0.06603892147541046 = 0.0016983025707304478 + 0.01 * 6.434062480926514
Epoch 1280, val loss: 1.617296814918518
Epoch 1290, training loss: 0.06607237458229065 = 0.001676549087278545 + 0.01 * 6.439582347869873
Epoch 1290, val loss: 1.6198979616165161
Epoch 1300, training loss: 0.06598372012376785 = 0.0016553836176171899 + 0.01 * 6.432833671569824
Epoch 1300, val loss: 1.6223487854003906
Epoch 1310, training loss: 0.06590587645769119 = 0.0016348912613466382 + 0.01 * 6.427098751068115
Epoch 1310, val loss: 1.6248284578323364
Epoch 1320, training loss: 0.06589699536561966 = 0.0016149745788425207 + 0.01 * 6.428202152252197
Epoch 1320, val loss: 1.6272226572036743
Epoch 1330, training loss: 0.06593599915504456 = 0.001595641253516078 + 0.01 * 6.4340362548828125
Epoch 1330, val loss: 1.6296231746673584
Epoch 1340, training loss: 0.06594161689281464 = 0.00157684285659343 + 0.01 * 6.4364776611328125
Epoch 1340, val loss: 1.6319680213928223
Epoch 1350, training loss: 0.06585054099559784 = 0.001558598130941391 + 0.01 * 6.429194927215576
Epoch 1350, val loss: 1.6341758966445923
Epoch 1360, training loss: 0.06560590863227844 = 0.0015408518956974149 + 0.01 * 6.406505584716797
Epoch 1360, val loss: 1.636425256729126
Epoch 1370, training loss: 0.06560605019330978 = 0.0015236330218613148 + 0.01 * 6.4082417488098145
Epoch 1370, val loss: 1.638667345046997
Epoch 1380, training loss: 0.06555255502462387 = 0.0015069100772961974 + 0.01 * 6.404564380645752
Epoch 1380, val loss: 1.640738844871521
Epoch 1390, training loss: 0.06572532653808594 = 0.00149061088450253 + 0.01 * 6.423471927642822
Epoch 1390, val loss: 1.642933964729309
Epoch 1400, training loss: 0.0654979944229126 = 0.001474791788496077 + 0.01 * 6.402320861816406
Epoch 1400, val loss: 1.6449342966079712
Epoch 1410, training loss: 0.06546250730752945 = 0.001459347433410585 + 0.01 * 6.4003167152404785
Epoch 1410, val loss: 1.6469502449035645
Epoch 1420, training loss: 0.0655764788389206 = 0.001444306573830545 + 0.01 * 6.413217067718506
Epoch 1420, val loss: 1.6489626169204712
Epoch 1430, training loss: 0.06533578038215637 = 0.0014296957524493337 + 0.01 * 6.390608310699463
Epoch 1430, val loss: 1.6508922576904297
Epoch 1440, training loss: 0.06536413729190826 = 0.0014154405798763037 + 0.01 * 6.394870281219482
Epoch 1440, val loss: 1.6528856754302979
Epoch 1450, training loss: 0.06532513350248337 = 0.0014015601482242346 + 0.01 * 6.392357349395752
Epoch 1450, val loss: 1.6547436714172363
Epoch 1460, training loss: 0.06560011953115463 = 0.0013880443293601274 + 0.01 * 6.421207904815674
Epoch 1460, val loss: 1.6566187143325806
Epoch 1470, training loss: 0.06525123864412308 = 0.0013748516794294119 + 0.01 * 6.387639045715332
Epoch 1470, val loss: 1.6584984064102173
Epoch 1480, training loss: 0.0651407465338707 = 0.0013620095560327172 + 0.01 * 6.377873420715332
Epoch 1480, val loss: 1.6602565050125122
Epoch 1490, training loss: 0.06523130089044571 = 0.0013494928134605289 + 0.01 * 6.388180732727051
Epoch 1490, val loss: 1.6620179414749146
Epoch 1500, training loss: 0.06529329717159271 = 0.0013373156543821096 + 0.01 * 6.395598411560059
Epoch 1500, val loss: 1.6637909412384033
Epoch 1510, training loss: 0.06505335122346878 = 0.0013254445511847734 + 0.01 * 6.372790813446045
Epoch 1510, val loss: 1.6654468774795532
Epoch 1520, training loss: 0.06509373337030411 = 0.0013137581991031766 + 0.01 * 6.377997398376465
Epoch 1520, val loss: 1.667149305343628
Epoch 1530, training loss: 0.06535100936889648 = 0.001302400603890419 + 0.01 * 6.404860973358154
Epoch 1530, val loss: 1.6687986850738525
Epoch 1540, training loss: 0.06505696475505829 = 0.0012913075042888522 + 0.01 * 6.376565933227539
Epoch 1540, val loss: 1.6704390048980713
Epoch 1550, training loss: 0.06509733200073242 = 0.0012804645812138915 + 0.01 * 6.381687164306641
Epoch 1550, val loss: 1.6719998121261597
Epoch 1560, training loss: 0.06505313515663147 = 0.0012699164217337966 + 0.01 * 6.378322124481201
Epoch 1560, val loss: 1.6736012697219849
Epoch 1570, training loss: 0.06484238803386688 = 0.0012595697771757841 + 0.01 * 6.35828161239624
Epoch 1570, val loss: 1.675166130065918
Epoch 1580, training loss: 0.06495338678359985 = 0.0012494538677856326 + 0.01 * 6.3703932762146
Epoch 1580, val loss: 1.6766496896743774
Epoch 1590, training loss: 0.06495338678359985 = 0.0012395854573696852 + 0.01 * 6.37138032913208
Epoch 1590, val loss: 1.678181767463684
Epoch 1600, training loss: 0.06483554095029831 = 0.0012299359077587724 + 0.01 * 6.360560894012451
Epoch 1600, val loss: 1.6796537637710571
Epoch 1610, training loss: 0.0647628903388977 = 0.0012205129023641348 + 0.01 * 6.354238510131836
Epoch 1610, val loss: 1.6810530424118042
Epoch 1620, training loss: 0.06477945297956467 = 0.0012112706899642944 + 0.01 * 6.356818199157715
Epoch 1620, val loss: 1.6825003623962402
Epoch 1630, training loss: 0.06464715301990509 = 0.001202243845909834 + 0.01 * 6.344491481781006
Epoch 1630, val loss: 1.683899998664856
Epoch 1640, training loss: 0.06472742557525635 = 0.0011934158392250538 + 0.01 * 6.353400707244873
Epoch 1640, val loss: 1.6852847337722778
Epoch 1650, training loss: 0.06455115228891373 = 0.0011847455753013492 + 0.01 * 6.336640357971191
Epoch 1650, val loss: 1.686646580696106
Epoch 1660, training loss: 0.06460551917552948 = 0.0011762658832594752 + 0.01 * 6.342925071716309
Epoch 1660, val loss: 1.6880335807800293
Epoch 1670, training loss: 0.06484518945217133 = 0.0011679795570671558 + 0.01 * 6.367721080780029
Epoch 1670, val loss: 1.6893385648727417
Epoch 1680, training loss: 0.06450288742780685 = 0.0011598457349464297 + 0.01 * 6.334304332733154
Epoch 1680, val loss: 1.6906582117080688
Epoch 1690, training loss: 0.06478126347064972 = 0.0011519005056470633 + 0.01 * 6.362936973571777
Epoch 1690, val loss: 1.6919013261795044
Epoch 1700, training loss: 0.06466446816921234 = 0.0011440597008913755 + 0.01 * 6.352041244506836
Epoch 1700, val loss: 1.693208932876587
Epoch 1710, training loss: 0.06467235833406448 = 0.0011363918893039227 + 0.01 * 6.3535966873168945
Epoch 1710, val loss: 1.6944375038146973
Epoch 1720, training loss: 0.06446541100740433 = 0.001128918258473277 + 0.01 * 6.333649635314941
Epoch 1720, val loss: 1.6956579685211182
Epoch 1730, training loss: 0.06434648483991623 = 0.0011215307749807835 + 0.01 * 6.322495937347412
Epoch 1730, val loss: 1.6968698501586914
Epoch 1740, training loss: 0.06438498944044113 = 0.0011143558658659458 + 0.01 * 6.32706356048584
Epoch 1740, val loss: 1.6980314254760742
Epoch 1750, training loss: 0.06445036828517914 = 0.0011072540655732155 + 0.01 * 6.3343119621276855
Epoch 1750, val loss: 1.6992446184158325
Epoch 1760, training loss: 0.06461706757545471 = 0.0011003406252712011 + 0.01 * 6.351673126220703
Epoch 1760, val loss: 1.70037841796875
Epoch 1770, training loss: 0.06432059407234192 = 0.0010935214813798666 + 0.01 * 6.322707176208496
Epoch 1770, val loss: 1.7015033960342407
Epoch 1780, training loss: 0.06432896107435226 = 0.0010868407553061843 + 0.01 * 6.324212074279785
Epoch 1780, val loss: 1.70261812210083
Epoch 1790, training loss: 0.06475964933633804 = 0.001080297282896936 + 0.01 * 6.3679351806640625
Epoch 1790, val loss: 1.7037506103515625
Epoch 1800, training loss: 0.06419914960861206 = 0.0010738434502854943 + 0.01 * 6.312530517578125
Epoch 1800, val loss: 1.704818606376648
Epoch 1810, training loss: 0.06407340615987778 = 0.0010675358353182673 + 0.01 * 6.3005876541137695
Epoch 1810, val loss: 1.7058964967727661
Epoch 1820, training loss: 0.06428367644548416 = 0.0010613315971568227 + 0.01 * 6.322234630584717
Epoch 1820, val loss: 1.7069393396377563
Epoch 1830, training loss: 0.06414949893951416 = 0.0010552280582487583 + 0.01 * 6.309427261352539
Epoch 1830, val loss: 1.7079970836639404
Epoch 1840, training loss: 0.06410615891218185 = 0.0010492239380255342 + 0.01 * 6.305694103240967
Epoch 1840, val loss: 1.7090269327163696
Epoch 1850, training loss: 0.06433185189962387 = 0.0010433412389829755 + 0.01 * 6.328851222991943
Epoch 1850, val loss: 1.710014820098877
Epoch 1860, training loss: 0.06398129463195801 = 0.0010375455021858215 + 0.01 * 6.294375419616699
Epoch 1860, val loss: 1.7110490798950195
Epoch 1870, training loss: 0.06397869437932968 = 0.0010318404529243708 + 0.01 * 6.2946858406066895
Epoch 1870, val loss: 1.7120174169540405
Epoch 1880, training loss: 0.06405110657215118 = 0.0010262662544846535 + 0.01 * 6.302484035491943
Epoch 1880, val loss: 1.7129876613616943
Epoch 1890, training loss: 0.06416919827461243 = 0.0010207497980445623 + 0.01 * 6.314845085144043
Epoch 1890, val loss: 1.7139683961868286
Epoch 1900, training loss: 0.06376953423023224 = 0.0010153490584343672 + 0.01 * 6.275418281555176
Epoch 1900, val loss: 1.7149012088775635
Epoch 1910, training loss: 0.06395472586154938 = 0.0010100187500938773 + 0.01 * 6.294471263885498
Epoch 1910, val loss: 1.7158781290054321
Epoch 1920, training loss: 0.06402479857206345 = 0.001004768186248839 + 0.01 * 6.302002906799316
Epoch 1920, val loss: 1.7167694568634033
Epoch 1930, training loss: 0.06394189596176147 = 0.000999626936390996 + 0.01 * 6.29422664642334
Epoch 1930, val loss: 1.7177377939224243
Epoch 1940, training loss: 0.06388481706380844 = 0.0009945600759238005 + 0.01 * 6.28902530670166
Epoch 1940, val loss: 1.718687891960144
Epoch 1950, training loss: 0.06399708986282349 = 0.0009895707480609417 + 0.01 * 6.30075216293335
Epoch 1950, val loss: 1.7195403575897217
Epoch 1960, training loss: 0.06376351416110992 = 0.0009846369503065944 + 0.01 * 6.27788782119751
Epoch 1960, val loss: 1.7204467058181763
Epoch 1970, training loss: 0.06405644118785858 = 0.000979784526862204 + 0.01 * 6.307666301727295
Epoch 1970, val loss: 1.7213349342346191
Epoch 1980, training loss: 0.06355828046798706 = 0.0009750070748850703 + 0.01 * 6.258327484130859
Epoch 1980, val loss: 1.7222254276275635
Epoch 1990, training loss: 0.06366664916276932 = 0.0009703162359073758 + 0.01 * 6.269632816314697
Epoch 1990, val loss: 1.7231054306030273
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.6531
Flip ASR: 0.5911/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0299341678619385 = 1.946195125579834 + 0.01 * 8.373893737792969
Epoch 0, val loss: 1.9476449489593506
Epoch 10, training loss: 2.0195138454437256 = 1.9357759952545166 + 0.01 * 8.37378978729248
Epoch 10, val loss: 1.9365973472595215
Epoch 20, training loss: 2.0068821907043457 = 1.9231480360031128 + 0.01 * 8.373412132263184
Epoch 20, val loss: 1.9223453998565674
Epoch 30, training loss: 1.9893263578414917 = 1.9056029319763184 + 0.01 * 8.372339248657227
Epoch 30, val loss: 1.9017747640609741
Epoch 40, training loss: 1.963819146156311 = 1.880131483078003 + 0.01 * 8.36876106262207
Epoch 40, val loss: 1.8716284036636353
Epoch 50, training loss: 1.9279675483703613 = 1.8444782495498657 + 0.01 * 8.348936080932617
Epoch 50, val loss: 1.8305909633636475
Epoch 60, training loss: 1.8828434944152832 = 1.800498366355896 + 0.01 * 8.234508514404297
Epoch 60, val loss: 1.7837471961975098
Epoch 70, training loss: 1.8309950828552246 = 1.7549000978469849 + 0.01 * 7.609498977661133
Epoch 70, val loss: 1.740837812423706
Epoch 80, training loss: 1.7773867845535278 = 1.7039822340011597 + 0.01 * 7.340454578399658
Epoch 80, val loss: 1.6970103979110718
Epoch 90, training loss: 1.7077966928482056 = 1.6356420516967773 + 0.01 * 7.21546745300293
Epoch 90, val loss: 1.6392759084701538
Epoch 100, training loss: 1.6178474426269531 = 1.546460747718811 + 0.01 * 7.138671875
Epoch 100, val loss: 1.5645700693130493
Epoch 110, training loss: 1.510719895362854 = 1.440064787864685 + 0.01 * 7.06551456451416
Epoch 110, val loss: 1.4770159721374512
Epoch 120, training loss: 1.3967704772949219 = 1.3264057636260986 + 0.01 * 7.036475658416748
Epoch 120, val loss: 1.3860384225845337
Epoch 130, training loss: 1.2838478088378906 = 1.2136801481246948 + 0.01 * 7.016768455505371
Epoch 130, val loss: 1.2996385097503662
Epoch 140, training loss: 1.175324559211731 = 1.1054176092147827 + 0.01 * 6.990696430206299
Epoch 140, val loss: 1.2187496423721313
Epoch 150, training loss: 1.0717296600341797 = 1.002088189125061 + 0.01 * 6.9641432762146
Epoch 150, val loss: 1.1421884298324585
Epoch 160, training loss: 0.9736326932907104 = 0.9041637778282166 + 0.01 * 6.946890354156494
Epoch 160, val loss: 1.0696988105773926
Epoch 170, training loss: 0.883273720741272 = 0.8138699531555176 + 0.01 * 6.94037389755249
Epoch 170, val loss: 1.0030510425567627
Epoch 180, training loss: 0.8027921319007874 = 0.7334362268447876 + 0.01 * 6.9355926513671875
Epoch 180, val loss: 0.9439485669136047
Epoch 190, training loss: 0.7328121662139893 = 0.6635150909423828 + 0.01 * 6.929708003997803
Epoch 190, val loss: 0.8931824564933777
Epoch 200, training loss: 0.6723232269287109 = 0.6030995845794678 + 0.01 * 6.92236328125
Epoch 200, val loss: 0.8507401347160339
Epoch 210, training loss: 0.6193804740905762 = 0.5502424240112305 + 0.01 * 6.913804531097412
Epoch 210, val loss: 0.8155944347381592
Epoch 220, training loss: 0.5718979835510254 = 0.5028592348098755 + 0.01 * 6.903876304626465
Epoch 220, val loss: 0.7862706184387207
Epoch 230, training loss: 0.5280274152755737 = 0.45911258459091187 + 0.01 * 6.891485691070557
Epoch 230, val loss: 0.7613074779510498
Epoch 240, training loss: 0.48622140288352966 = 0.417467325925827 + 0.01 * 6.8754072189331055
Epoch 240, val loss: 0.7394118905067444
Epoch 250, training loss: 0.44551175832748413 = 0.37694859504699707 + 0.01 * 6.856314659118652
Epoch 250, val loss: 0.7196534872055054
Epoch 260, training loss: 0.4056205749511719 = 0.3372858762741089 + 0.01 * 6.833470821380615
Epoch 260, val loss: 0.7018850445747375
Epoch 270, training loss: 0.36682239174842834 = 0.29871198534965515 + 0.01 * 6.811041355133057
Epoch 270, val loss: 0.6862894892692566
Epoch 280, training loss: 0.32985639572143555 = 0.26191893219947815 + 0.01 * 6.7937469482421875
Epoch 280, val loss: 0.6735164523124695
Epoch 290, training loss: 0.29554668068885803 = 0.22771739959716797 + 0.01 * 6.782927989959717
Epoch 290, val loss: 0.6636068224906921
Epoch 300, training loss: 0.2644319236278534 = 0.19669803977012634 + 0.01 * 6.7733893394470215
Epoch 300, val loss: 0.6571855545043945
Epoch 310, training loss: 0.23672334849834442 = 0.1690751314163208 + 0.01 * 6.764821529388428
Epoch 310, val loss: 0.6539707183837891
Epoch 320, training loss: 0.2124519944190979 = 0.1448291391134262 + 0.01 * 6.762285232543945
Epoch 320, val loss: 0.6538704633712769
Epoch 330, training loss: 0.19148465991020203 = 0.12396722286939621 + 0.01 * 6.751743793487549
Epoch 330, val loss: 0.6564522981643677
Epoch 340, training loss: 0.17421236634254456 = 0.10679860413074493 + 0.01 * 6.7413763999938965
Epoch 340, val loss: 0.6605920791625977
Epoch 350, training loss: 0.1601160764694214 = 0.09278953820466995 + 0.01 * 6.732654571533203
Epoch 350, val loss: 0.6669955253601074
Epoch 360, training loss: 0.14849531650543213 = 0.08118166029453278 + 0.01 * 6.73136568069458
Epoch 360, val loss: 0.6749346852302551
Epoch 370, training loss: 0.1386437714099884 = 0.07147114723920822 + 0.01 * 6.717261791229248
Epoch 370, val loss: 0.6844027042388916
Epoch 380, training loss: 0.1303790956735611 = 0.06325648725032806 + 0.01 * 6.712260723114014
Epoch 380, val loss: 0.695170521736145
Epoch 390, training loss: 0.12330365180969238 = 0.05626937374472618 + 0.01 * 6.703428268432617
Epoch 390, val loss: 0.7067136168479919
Epoch 400, training loss: 0.11749406158924103 = 0.05038517713546753 + 0.01 * 6.710888862609863
Epoch 400, val loss: 0.7186262011528015
Epoch 410, training loss: 0.11225901544094086 = 0.04533972218632698 + 0.01 * 6.691928863525391
Epoch 410, val loss: 0.7306176424026489
Epoch 420, training loss: 0.10787685215473175 = 0.0409807451069355 + 0.01 * 6.689610958099365
Epoch 420, val loss: 0.7428503036499023
Epoch 430, training loss: 0.10403533279895782 = 0.03720150142908096 + 0.01 * 6.683383464813232
Epoch 430, val loss: 0.7550424933433533
Epoch 440, training loss: 0.10072574019432068 = 0.03388841077685356 + 0.01 * 6.683732509613037
Epoch 440, val loss: 0.7670580744743347
Epoch 450, training loss: 0.09770995378494263 = 0.03099464625120163 + 0.01 * 6.6715312004089355
Epoch 450, val loss: 0.7789649963378906
Epoch 460, training loss: 0.09519468992948532 = 0.02843928150832653 + 0.01 * 6.675540447235107
Epoch 460, val loss: 0.7906751036643982
Epoch 470, training loss: 0.09283360838890076 = 0.026181431487202644 + 0.01 * 6.665217876434326
Epoch 470, val loss: 0.8021368980407715
Epoch 480, training loss: 0.09077668935060501 = 0.024196689948439598 + 0.01 * 6.6579999923706055
Epoch 480, val loss: 0.8133581280708313
Epoch 490, training loss: 0.08896549046039581 = 0.022432789206504822 + 0.01 * 6.653270244598389
Epoch 490, val loss: 0.8242968320846558
Epoch 500, training loss: 0.08732755482196808 = 0.020856542512774467 + 0.01 * 6.647101402282715
Epoch 500, val loss: 0.8349884152412415
Epoch 510, training loss: 0.08590303361415863 = 0.01944427564740181 + 0.01 * 6.645875930786133
Epoch 510, val loss: 0.8454074263572693
Epoch 520, training loss: 0.08453066647052765 = 0.018174979835748672 + 0.01 * 6.635568618774414
Epoch 520, val loss: 0.8555684089660645
Epoch 530, training loss: 0.08331432938575745 = 0.017027506604790688 + 0.01 * 6.628682613372803
Epoch 530, val loss: 0.8654053807258606
Epoch 540, training loss: 0.08224256336688995 = 0.015987679362297058 + 0.01 * 6.62548828125
Epoch 540, val loss: 0.8750062584877014
Epoch 550, training loss: 0.08122171461582184 = 0.015040617436170578 + 0.01 * 6.618109226226807
Epoch 550, val loss: 0.884385883808136
Epoch 560, training loss: 0.08028367161750793 = 0.014172608032822609 + 0.01 * 6.6111063957214355
Epoch 560, val loss: 0.8934916853904724
Epoch 570, training loss: 0.07943107187747955 = 0.013378865085542202 + 0.01 * 6.605220794677734
Epoch 570, val loss: 0.9023672342300415
Epoch 580, training loss: 0.078687883913517 = 0.012648962438106537 + 0.01 * 6.6038923263549805
Epoch 580, val loss: 0.9109817147254944
Epoch 590, training loss: 0.07803048193454742 = 0.01197704765945673 + 0.01 * 6.605343341827393
Epoch 590, val loss: 0.9194154739379883
Epoch 600, training loss: 0.07729937136173248 = 0.011359207332134247 + 0.01 * 6.5940165519714355
Epoch 600, val loss: 0.9276006817817688
Epoch 610, training loss: 0.07664050161838531 = 0.010796461254358292 + 0.01 * 6.584403991699219
Epoch 610, val loss: 0.9355993866920471
Epoch 620, training loss: 0.0762733444571495 = 0.010276230052113533 + 0.01 * 6.599711894989014
Epoch 620, val loss: 0.9434148669242859
Epoch 630, training loss: 0.07553473114967346 = 0.009795363992452621 + 0.01 * 6.573936939239502
Epoch 630, val loss: 0.9510043859481812
Epoch 640, training loss: 0.07517895847558975 = 0.009349053725600243 + 0.01 * 6.5829901695251465
Epoch 640, val loss: 0.9583724141120911
Epoch 650, training loss: 0.0746050626039505 = 0.00893446896225214 + 0.01 * 6.567059516906738
Epoch 650, val loss: 0.9655928015708923
Epoch 660, training loss: 0.07406952232122421 = 0.008548608981072903 + 0.01 * 6.552091598510742
Epoch 660, val loss: 0.9725901484489441
Epoch 670, training loss: 0.0736483782529831 = 0.008188167586922646 + 0.01 * 6.546021461486816
Epoch 670, val loss: 0.9794456362724304
Epoch 680, training loss: 0.07333967834711075 = 0.007851140573620796 + 0.01 * 6.548853874206543
Epoch 680, val loss: 0.9861520528793335
Epoch 690, training loss: 0.07287412881851196 = 0.007535315118730068 + 0.01 * 6.533881664276123
Epoch 690, val loss: 0.9926649332046509
Epoch 700, training loss: 0.07253020256757736 = 0.007238862570375204 + 0.01 * 6.5291337966918945
Epoch 700, val loss: 0.9990934133529663
Epoch 710, training loss: 0.07227259129285812 = 0.006961277686059475 + 0.01 * 6.531131267547607
Epoch 710, val loss: 1.0053118467330933
Epoch 720, training loss: 0.07195719331502914 = 0.006701025180518627 + 0.01 * 6.5256171226501465
Epoch 720, val loss: 1.0114549398422241
Epoch 730, training loss: 0.07169295102357864 = 0.006456924602389336 + 0.01 * 6.523602485656738
Epoch 730, val loss: 1.0174039602279663
Epoch 740, training loss: 0.0713992714881897 = 0.006227927748113871 + 0.01 * 6.517134666442871
Epoch 740, val loss: 1.0232670307159424
Epoch 750, training loss: 0.07128298282623291 = 0.006012638099491596 + 0.01 * 6.527034282684326
Epoch 750, val loss: 1.0290335416793823
Epoch 760, training loss: 0.07088484615087509 = 0.005809983238577843 + 0.01 * 6.507486343383789
Epoch 760, val loss: 1.034622073173523
Epoch 770, training loss: 0.0709226056933403 = 0.005618490278720856 + 0.01 * 6.530411720275879
Epoch 770, val loss: 1.0401214361190796
Epoch 780, training loss: 0.07052049785852432 = 0.005436992738395929 + 0.01 * 6.508350849151611
Epoch 780, val loss: 1.045531988143921
Epoch 790, training loss: 0.07023492455482483 = 0.005263940431177616 + 0.01 * 6.497098445892334
Epoch 790, val loss: 1.0507714748382568
Epoch 800, training loss: 0.07013178616762161 = 0.005099052097648382 + 0.01 * 6.503273963928223
Epoch 800, val loss: 1.0559171438217163
Epoch 810, training loss: 0.06986483931541443 = 0.004945072811096907 + 0.01 * 6.491977214813232
Epoch 810, val loss: 1.0609973669052124
Epoch 820, training loss: 0.0696675106883049 = 0.0047991289757192135 + 0.01 * 6.4868388175964355
Epoch 820, val loss: 1.0659304857254028
Epoch 830, training loss: 0.06958754360675812 = 0.0046604471281170845 + 0.01 * 6.492709159851074
Epoch 830, val loss: 1.0707982778549194
Epoch 840, training loss: 0.06930120289325714 = 0.004528675228357315 + 0.01 * 6.47725248336792
Epoch 840, val loss: 1.0755778551101685
Epoch 850, training loss: 0.06933329999446869 = 0.004403293132781982 + 0.01 * 6.493000507354736
Epoch 850, val loss: 1.0802725553512573
Epoch 860, training loss: 0.06907714903354645 = 0.004283927846699953 + 0.01 * 6.4793219566345215
Epoch 860, val loss: 1.0848366022109985
Epoch 870, training loss: 0.06886881589889526 = 0.0041701048612594604 + 0.01 * 6.4698710441589355
Epoch 870, val loss: 1.0892832279205322
Epoch 880, training loss: 0.0688943862915039 = 0.004061400890350342 + 0.01 * 6.4832987785339355
Epoch 880, val loss: 1.0937559604644775
Epoch 890, training loss: 0.0686274915933609 = 0.003957800101488829 + 0.01 * 6.466969013214111
Epoch 890, val loss: 1.0980474948883057
Epoch 900, training loss: 0.06858005374670029 = 0.003858808195218444 + 0.01 * 6.472125053405762
Epoch 900, val loss: 1.1022248268127441
Epoch 910, training loss: 0.06825578957796097 = 0.0037642493844032288 + 0.01 * 6.449153900146484
Epoch 910, val loss: 1.1064010858535767
Epoch 920, training loss: 0.0684737116098404 = 0.003673776052892208 + 0.01 * 6.47999382019043
Epoch 920, val loss: 1.1104623079299927
Epoch 930, training loss: 0.0681338980793953 = 0.0035868780687451363 + 0.01 * 6.454701900482178
Epoch 930, val loss: 1.1144769191741943
Epoch 940, training loss: 0.06802642345428467 = 0.0035035836044698954 + 0.01 * 6.45228385925293
Epoch 940, val loss: 1.1183459758758545
Epoch 950, training loss: 0.06782545149326324 = 0.003423676360398531 + 0.01 * 6.4401774406433105
Epoch 950, val loss: 1.122200608253479
Epoch 960, training loss: 0.067937932908535 = 0.003347439458593726 + 0.01 * 6.459049224853516
Epoch 960, val loss: 1.1259821653366089
Epoch 970, training loss: 0.06783508509397507 = 0.003274336690083146 + 0.01 * 6.4560747146606445
Epoch 970, val loss: 1.1297320127487183
Epoch 980, training loss: 0.06749498099088669 = 0.0032042167149484158 + 0.01 * 6.429076194763184
Epoch 980, val loss: 1.133373737335205
Epoch 990, training loss: 0.06752607971429825 = 0.003136794315651059 + 0.01 * 6.438929080963135
Epoch 990, val loss: 1.137016773223877
Epoch 1000, training loss: 0.06740541011095047 = 0.003072143066674471 + 0.01 * 6.433326721191406
Epoch 1000, val loss: 1.1405165195465088
Epoch 1010, training loss: 0.06718649715185165 = 0.0030098415445536375 + 0.01 * 6.417665958404541
Epoch 1010, val loss: 1.1440216302871704
Epoch 1020, training loss: 0.06737085431814194 = 0.002949900459498167 + 0.01 * 6.442095756530762
Epoch 1020, val loss: 1.1474534273147583
Epoch 1030, training loss: 0.06717273592948914 = 0.002892232732847333 + 0.01 * 6.4280500411987305
Epoch 1030, val loss: 1.1508522033691406
Epoch 1040, training loss: 0.06695620715618134 = 0.002836746396496892 + 0.01 * 6.4119462966918945
Epoch 1040, val loss: 1.1541483402252197
Epoch 1050, training loss: 0.06710662692785263 = 0.002783142263069749 + 0.01 * 6.432348728179932
Epoch 1050, val loss: 1.1574432849884033
Epoch 1060, training loss: 0.06685161590576172 = 0.002731426153331995 + 0.01 * 6.4120192527771
Epoch 1060, val loss: 1.160661220550537
Epoch 1070, training loss: 0.06729219108819962 = 0.0026815724559128284 + 0.01 * 6.461061477661133
Epoch 1070, val loss: 1.1638240814208984
Epoch 1080, training loss: 0.06662631779909134 = 0.002633526921272278 + 0.01 * 6.3992791175842285
Epoch 1080, val loss: 1.1669682264328003
Epoch 1090, training loss: 0.06655871123075485 = 0.002587134251371026 + 0.01 * 6.397158145904541
Epoch 1090, val loss: 1.169994592666626
Epoch 1100, training loss: 0.06677120178937912 = 0.0025423134211450815 + 0.01 * 6.42288875579834
Epoch 1100, val loss: 1.1730587482452393
Epoch 1110, training loss: 0.06653063744306564 = 0.002498912625014782 + 0.01 * 6.403172969818115
Epoch 1110, val loss: 1.1761060953140259
Epoch 1120, training loss: 0.0664597600698471 = 0.002457036403939128 + 0.01 * 6.400272369384766
Epoch 1120, val loss: 1.1789536476135254
Epoch 1130, training loss: 0.06657564640045166 = 0.0024165231734514236 + 0.01 * 6.415912628173828
Epoch 1130, val loss: 1.1819171905517578
Epoch 1140, training loss: 0.06628567725419998 = 0.0023772085551172495 + 0.01 * 6.3908467292785645
Epoch 1140, val loss: 1.1847753524780273
Epoch 1150, training loss: 0.06631219387054443 = 0.0023392278235405684 + 0.01 * 6.397296905517578
Epoch 1150, val loss: 1.1875991821289062
Epoch 1160, training loss: 0.06621462851762772 = 0.002302558394148946 + 0.01 * 6.391207218170166
Epoch 1160, val loss: 1.1904014348983765
Epoch 1170, training loss: 0.06649331748485565 = 0.002266948577016592 + 0.01 * 6.422636985778809
Epoch 1170, val loss: 1.193144679069519
Epoch 1180, training loss: 0.06608573347330093 = 0.0022324989549815655 + 0.01 * 6.385323524475098
Epoch 1180, val loss: 1.1958571672439575
Epoch 1190, training loss: 0.06636364758014679 = 0.0021990810055285692 + 0.01 * 6.41645622253418
Epoch 1190, val loss: 1.1985293626785278
Epoch 1200, training loss: 0.0659453421831131 = 0.00216666329652071 + 0.01 * 6.377867698669434
Epoch 1200, val loss: 1.2012245655059814
Epoch 1210, training loss: 0.06615350395441055 = 0.0021352346520870924 + 0.01 * 6.401827335357666
Epoch 1210, val loss: 1.2037783861160278
Epoch 1220, training loss: 0.06587611883878708 = 0.0021048439666628838 + 0.01 * 6.3771281242370605
Epoch 1220, val loss: 1.2063963413238525
Epoch 1230, training loss: 0.06617845594882965 = 0.002075233729556203 + 0.01 * 6.410322666168213
Epoch 1230, val loss: 1.2089571952819824
Epoch 1240, training loss: 0.06574229896068573 = 0.0020465163979679346 + 0.01 * 6.369577884674072
Epoch 1240, val loss: 1.2115117311477661
Epoch 1250, training loss: 0.06596018373966217 = 0.002018637489527464 + 0.01 * 6.394155025482178
Epoch 1250, val loss: 1.2139557600021362
Epoch 1260, training loss: 0.065586157143116 = 0.0019916146993637085 + 0.01 * 6.359454154968262
Epoch 1260, val loss: 1.216391921043396
Epoch 1270, training loss: 0.06579392403364182 = 0.001965318340808153 + 0.01 * 6.3828606605529785
Epoch 1270, val loss: 1.2188104391098022
Epoch 1280, training loss: 0.06559662520885468 = 0.001939677749760449 + 0.01 * 6.365694999694824
Epoch 1280, val loss: 1.2212294340133667
Epoch 1290, training loss: 0.06548949331045151 = 0.0019148238934576511 + 0.01 * 6.3574676513671875
Epoch 1290, val loss: 1.2235620021820068
Epoch 1300, training loss: 0.0656467080116272 = 0.001890642335638404 + 0.01 * 6.375606536865234
Epoch 1300, val loss: 1.2259281873703003
Epoch 1310, training loss: 0.06547252088785172 = 0.001867083366960287 + 0.01 * 6.360543727874756
Epoch 1310, val loss: 1.2282406091690063
Epoch 1320, training loss: 0.06549299508333206 = 0.0018441517604514956 + 0.01 * 6.364884376525879
Epoch 1320, val loss: 1.2305419445037842
Epoch 1330, training loss: 0.0653902217745781 = 0.0018218554323539138 + 0.01 * 6.356837272644043
Epoch 1330, val loss: 1.2328184843063354
Epoch 1340, training loss: 0.06550422310829163 = 0.0018001908902078867 + 0.01 * 6.370403289794922
Epoch 1340, val loss: 1.235055923461914
Epoch 1350, training loss: 0.06519202888011932 = 0.0017791235586628318 + 0.01 * 6.3412909507751465
Epoch 1350, val loss: 1.2372685670852661
Epoch 1360, training loss: 0.06560273468494415 = 0.0017586194444447756 + 0.01 * 6.384411334991455
Epoch 1360, val loss: 1.2394828796386719
Epoch 1370, training loss: 0.06533173471689224 = 0.001738685998134315 + 0.01 * 6.359305381774902
Epoch 1370, val loss: 1.2416179180145264
Epoch 1380, training loss: 0.06532502174377441 = 0.0017193228704854846 + 0.01 * 6.360569953918457
Epoch 1380, val loss: 1.243712306022644
Epoch 1390, training loss: 0.06505738198757172 = 0.001700339955277741 + 0.01 * 6.3357038497924805
Epoch 1390, val loss: 1.245863437652588
Epoch 1400, training loss: 0.06511159986257553 = 0.0016818648437038064 + 0.01 * 6.342973709106445
Epoch 1400, val loss: 1.2478975057601929
Epoch 1410, training loss: 0.06511538475751877 = 0.0016639012610539794 + 0.01 * 6.34514856338501
Epoch 1410, val loss: 1.2500051259994507
Epoch 1420, training loss: 0.0649908259510994 = 0.0016463750507682562 + 0.01 * 6.334444999694824
Epoch 1420, val loss: 1.2520077228546143
Epoch 1430, training loss: 0.06520480662584305 = 0.001629317062906921 + 0.01 * 6.35754919052124
Epoch 1430, val loss: 1.2540407180786133
Epoch 1440, training loss: 0.06490390747785568 = 0.0016125563997775316 + 0.01 * 6.329134941101074
Epoch 1440, val loss: 1.2559890747070312
Epoch 1450, training loss: 0.06540733575820923 = 0.0015959199517965317 + 0.01 * 6.381141185760498
Epoch 1450, val loss: 1.2579901218414307
Epoch 1460, training loss: 0.064780093729496 = 0.0015797634841874242 + 0.01 * 6.320033073425293
Epoch 1460, val loss: 1.2599074840545654
Epoch 1470, training loss: 0.06492298096418381 = 0.001563943107612431 + 0.01 * 6.335904121398926
Epoch 1470, val loss: 1.2617958784103394
Epoch 1480, training loss: 0.06488761305809021 = 0.0015484729083254933 + 0.01 * 6.333913803100586
Epoch 1480, val loss: 1.2637014389038086
Epoch 1490, training loss: 0.06456863880157471 = 0.0015334298368543386 + 0.01 * 6.303521633148193
Epoch 1490, val loss: 1.2655599117279053
Epoch 1500, training loss: 0.06485490500926971 = 0.0015187441604211926 + 0.01 * 6.333616256713867
Epoch 1500, val loss: 1.2674390077590942
Epoch 1510, training loss: 0.06459546089172363 = 0.0015043888706713915 + 0.01 * 6.309107303619385
Epoch 1510, val loss: 1.2693023681640625
Epoch 1520, training loss: 0.06487524509429932 = 0.0014906148426234722 + 0.01 * 6.338462829589844
Epoch 1520, val loss: 1.2710943222045898
Epoch 1530, training loss: 0.0646849200129509 = 0.0014772148570045829 + 0.01 * 6.320770740509033
Epoch 1530, val loss: 1.2728873491287231
Epoch 1540, training loss: 0.06491656601428986 = 0.0014640465378761292 + 0.01 * 6.34525203704834
Epoch 1540, val loss: 1.2746832370758057
Epoch 1550, training loss: 0.06437566131353378 = 0.0014511824119836092 + 0.01 * 6.292448043823242
Epoch 1550, val loss: 1.276437759399414
Epoch 1560, training loss: 0.06428016722202301 = 0.0014385837130248547 + 0.01 * 6.284159183502197
Epoch 1560, val loss: 1.2781906127929688
Epoch 1570, training loss: 0.06426461786031723 = 0.0014262471813708544 + 0.01 * 6.28383731842041
Epoch 1570, val loss: 1.279929757118225
Epoch 1580, training loss: 0.06438187509775162 = 0.0014141520950943232 + 0.01 * 6.2967729568481445
Epoch 1580, val loss: 1.281628966331482
Epoch 1590, training loss: 0.06452605128288269 = 0.0014023541007190943 + 0.01 * 6.3123698234558105
Epoch 1590, val loss: 1.2833672761917114
Epoch 1600, training loss: 0.06420499086380005 = 0.0013907437678426504 + 0.01 * 6.281425476074219
Epoch 1600, val loss: 1.285047173500061
Epoch 1610, training loss: 0.06442531198263168 = 0.0013794759288430214 + 0.01 * 6.30458402633667
Epoch 1610, val loss: 1.286734700202942
Epoch 1620, training loss: 0.06422095745801926 = 0.0013683188008144498 + 0.01 * 6.285264015197754
Epoch 1620, val loss: 1.2884130477905273
Epoch 1630, training loss: 0.06450000405311584 = 0.0013574835611507297 + 0.01 * 6.3142523765563965
Epoch 1630, val loss: 1.2900731563568115
Epoch 1640, training loss: 0.06416773051023483 = 0.0013468497199937701 + 0.01 * 6.282088279724121
Epoch 1640, val loss: 1.2917144298553467
Epoch 1650, training loss: 0.06446301192045212 = 0.0013364356709644198 + 0.01 * 6.312658309936523
Epoch 1650, val loss: 1.2933454513549805
Epoch 1660, training loss: 0.0640699490904808 = 0.0013262282591313124 + 0.01 * 6.274372577667236
Epoch 1660, val loss: 1.2949318885803223
Epoch 1670, training loss: 0.06415797024965286 = 0.0013162692775949836 + 0.01 * 6.284170150756836
Epoch 1670, val loss: 1.2965881824493408
Epoch 1680, training loss: 0.06394033879041672 = 0.0013064908562228084 + 0.01 * 6.263384819030762
Epoch 1680, val loss: 1.298161268234253
Epoch 1690, training loss: 0.06412407755851746 = 0.0012968905502930284 + 0.01 * 6.282718658447266
Epoch 1690, val loss: 1.299710988998413
Epoch 1700, training loss: 0.0638522282242775 = 0.0012874692911282182 + 0.01 * 6.256476402282715
Epoch 1700, val loss: 1.3012850284576416
Epoch 1710, training loss: 0.0640408992767334 = 0.0012782285921275616 + 0.01 * 6.276267051696777
Epoch 1710, val loss: 1.3028463125228882
Epoch 1720, training loss: 0.06423158198595047 = 0.0012691864976659417 + 0.01 * 6.296239852905273
Epoch 1720, val loss: 1.3044216632843018
Epoch 1730, training loss: 0.06388454884290695 = 0.001260290271602571 + 0.01 * 6.262426376342773
Epoch 1730, val loss: 1.3059439659118652
Epoch 1740, training loss: 0.06423013657331467 = 0.0012516019633039832 + 0.01 * 6.297853469848633
Epoch 1740, val loss: 1.3074742555618286
Epoch 1750, training loss: 0.06388689577579498 = 0.0012430234346538782 + 0.01 * 6.264387130737305
Epoch 1750, val loss: 1.3089804649353027
Epoch 1760, training loss: 0.0641670972108841 = 0.0012346127768978477 + 0.01 * 6.293248176574707
Epoch 1760, val loss: 1.3104863166809082
Epoch 1770, training loss: 0.06375046074390411 = 0.0012263819808140397 + 0.01 * 6.252408027648926
Epoch 1770, val loss: 1.3119573593139648
Epoch 1780, training loss: 0.06401512771844864 = 0.0012182577047497034 + 0.01 * 6.27968692779541
Epoch 1780, val loss: 1.3134455680847168
Epoch 1790, training loss: 0.06394725292921066 = 0.0012101458851248026 + 0.01 * 6.273711204528809
Epoch 1790, val loss: 1.3149646520614624
Epoch 1800, training loss: 0.06375068426132202 = 0.0012019291752949357 + 0.01 * 6.254876136779785
Epoch 1800, val loss: 1.3164119720458984
Epoch 1810, training loss: 0.06364768743515015 = 0.0011937280651181936 + 0.01 * 6.245396137237549
Epoch 1810, val loss: 1.3178297281265259
Epoch 1820, training loss: 0.06371435523033142 = 0.0011856784112751484 + 0.01 * 6.252867698669434
Epoch 1820, val loss: 1.3192572593688965
Epoch 1830, training loss: 0.06390295922756195 = 0.0011780664790421724 + 0.01 * 6.27249002456665
Epoch 1830, val loss: 1.3207215070724487
Epoch 1840, training loss: 0.0636645033955574 = 0.0011707722442224622 + 0.01 * 6.249373435974121
Epoch 1840, val loss: 1.3221064805984497
Epoch 1850, training loss: 0.06366126984357834 = 0.0011635972186923027 + 0.01 * 6.249767780303955
Epoch 1850, val loss: 1.3235251903533936
Epoch 1860, training loss: 0.06370685994625092 = 0.0011565274326130748 + 0.01 * 6.25503396987915
Epoch 1860, val loss: 1.3249216079711914
Epoch 1870, training loss: 0.0637020468711853 = 0.0011495844228193164 + 0.01 * 6.255246162414551
Epoch 1870, val loss: 1.3263065814971924
Epoch 1880, training loss: 0.06352055817842484 = 0.0011427312856540084 + 0.01 * 6.237782955169678
Epoch 1880, val loss: 1.3276796340942383
Epoch 1890, training loss: 0.06365592032670975 = 0.0011359771015122533 + 0.01 * 6.251994609832764
Epoch 1890, val loss: 1.32904052734375
Epoch 1900, training loss: 0.0636187493801117 = 0.0011293352581560612 + 0.01 * 6.248941421508789
Epoch 1900, val loss: 1.330428123474121
Epoch 1910, training loss: 0.06360136717557907 = 0.0011228034272789955 + 0.01 * 6.247856616973877
Epoch 1910, val loss: 1.3317760229110718
Epoch 1920, training loss: 0.06367279589176178 = 0.001116386498324573 + 0.01 * 6.255640983581543
Epoch 1920, val loss: 1.3331215381622314
Epoch 1930, training loss: 0.06336072832345963 = 0.0011100440751761198 + 0.01 * 6.225068092346191
Epoch 1930, val loss: 1.334452748298645
Epoch 1940, training loss: 0.06347385793924332 = 0.0011038110824301839 + 0.01 * 6.23700475692749
Epoch 1940, val loss: 1.335762858390808
Epoch 1950, training loss: 0.0632886216044426 = 0.0010976613266393542 + 0.01 * 6.2190961837768555
Epoch 1950, val loss: 1.3370838165283203
Epoch 1960, training loss: 0.06373625248670578 = 0.0010916228638961911 + 0.01 * 6.264462947845459
Epoch 1960, val loss: 1.3383840322494507
Epoch 1970, training loss: 0.06332768499851227 = 0.0010856841690838337 + 0.01 * 6.224200248718262
Epoch 1970, val loss: 1.3396434783935547
Epoch 1980, training loss: 0.06345027685165405 = 0.0010798376752063632 + 0.01 * 6.237043857574463
Epoch 1980, val loss: 1.3409487009048462
Epoch 1990, training loss: 0.06338950991630554 = 0.0010740812867879868 + 0.01 * 6.231543064117432
Epoch 1990, val loss: 1.3421868085861206
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.2362
Flip ASR: 0.2267/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0343539714813232 = 1.9506158828735352 + 0.01 * 8.373815536499023
Epoch 0, val loss: 1.9495317935943604
Epoch 10, training loss: 2.022991180419922 = 1.9392554759979248 + 0.01 * 8.373559951782227
Epoch 10, val loss: 1.936853051185608
Epoch 20, training loss: 2.009275197982788 = 1.9255496263504028 + 0.01 * 8.372562408447266
Epoch 20, val loss: 1.9210288524627686
Epoch 30, training loss: 1.990592122077942 = 1.906894326210022 + 0.01 * 8.369779586791992
Epoch 30, val loss: 1.8991122245788574
Epoch 40, training loss: 1.964418888092041 = 1.8808077573776245 + 0.01 * 8.361109733581543
Epoch 40, val loss: 1.8689907789230347
Epoch 50, training loss: 1.9289414882659912 = 1.84585440158844 + 0.01 * 8.308703422546387
Epoch 50, val loss: 1.8313701152801514
Epoch 60, training loss: 1.8852730989456177 = 1.8042285442352295 + 0.01 * 8.104455947875977
Epoch 60, val loss: 1.7909311056137085
Epoch 70, training loss: 1.8405324220657349 = 1.7624319791793823 + 0.01 * 7.81003999710083
Epoch 70, val loss: 1.755139946937561
Epoch 80, training loss: 1.7943353652954102 = 1.7187304496765137 + 0.01 * 7.560493469238281
Epoch 80, val loss: 1.7190192937850952
Epoch 90, training loss: 1.7336983680725098 = 1.6606825590133667 + 0.01 * 7.301586151123047
Epoch 90, val loss: 1.6702677011489868
Epoch 100, training loss: 1.6533218622207642 = 1.5816292762756348 + 0.01 * 7.169256687164307
Epoch 100, val loss: 1.6042819023132324
Epoch 110, training loss: 1.5534722805023193 = 1.482844352722168 + 0.01 * 7.062798976898193
Epoch 110, val loss: 1.5243005752563477
Epoch 120, training loss: 1.4427028894424438 = 1.37236750125885 + 0.01 * 7.033541679382324
Epoch 120, val loss: 1.4367445707321167
Epoch 130, training loss: 1.3286993503570557 = 1.2586185932159424 + 0.01 * 7.008078575134277
Epoch 130, val loss: 1.3488847017288208
Epoch 140, training loss: 1.2159391641616821 = 1.1461496353149414 + 0.01 * 6.97894811630249
Epoch 140, val loss: 1.263550877571106
Epoch 150, training loss: 1.1070243120193481 = 1.0375285148620605 + 0.01 * 6.949580669403076
Epoch 150, val loss: 1.1825135946273804
Epoch 160, training loss: 1.0045771598815918 = 0.935327410697937 + 0.01 * 6.9249749183654785
Epoch 160, val loss: 1.1068382263183594
Epoch 170, training loss: 0.9106404185295105 = 0.8415971994400024 + 0.01 * 6.904321193695068
Epoch 170, val loss: 1.0381957292556763
Epoch 180, training loss: 0.8265967965126038 = 0.7577773928642273 + 0.01 * 6.881938457489014
Epoch 180, val loss: 0.9778985977172852
Epoch 190, training loss: 0.7529375553131104 = 0.6843592524528503 + 0.01 * 6.857833385467529
Epoch 190, val loss: 0.9265948534011841
Epoch 200, training loss: 0.6892248392105103 = 0.6208364963531494 + 0.01 * 6.838837623596191
Epoch 200, val loss: 0.8844293355941772
Epoch 210, training loss: 0.6341318488121033 = 0.5658147931098938 + 0.01 * 6.831706523895264
Epoch 210, val loss: 0.8503473997116089
Epoch 220, training loss: 0.5859090089797974 = 0.5176349878311157 + 0.01 * 6.827404975891113
Epoch 220, val loss: 0.8229892253875732
Epoch 230, training loss: 0.5430472493171692 = 0.47478410601615906 + 0.01 * 6.826315879821777
Epoch 230, val loss: 0.8012899160385132
Epoch 240, training loss: 0.503926157951355 = 0.43567776679992676 + 0.01 * 6.824841499328613
Epoch 240, val loss: 0.7835650444030762
Epoch 250, training loss: 0.46706146001815796 = 0.3988327383995056 + 0.01 * 6.822873115539551
Epoch 250, val loss: 0.7688670754432678
Epoch 260, training loss: 0.43104425072669983 = 0.3628380298614502 + 0.01 * 6.820621967315674
Epoch 260, val loss: 0.7557714581489563
Epoch 270, training loss: 0.39488184452056885 = 0.32669803500175476 + 0.01 * 6.818380832672119
Epoch 270, val loss: 0.743029773235321
Epoch 280, training loss: 0.3582974374294281 = 0.29013514518737793 + 0.01 * 6.816229343414307
Epoch 280, val loss: 0.7310765385627747
Epoch 290, training loss: 0.3219337463378906 = 0.25379064679145813 + 0.01 * 6.814309597015381
Epoch 290, val loss: 0.719826877117157
Epoch 300, training loss: 0.28688672184944153 = 0.21874591708183289 + 0.01 * 6.814081192016602
Epoch 300, val loss: 0.710300624370575
Epoch 310, training loss: 0.25467899441719055 = 0.18656527996063232 + 0.01 * 6.81137228012085
Epoch 310, val loss: 0.7034101486206055
Epoch 320, training loss: 0.2264452576637268 = 0.15835201740264893 + 0.01 * 6.809323787689209
Epoch 320, val loss: 0.700133204460144
Epoch 330, training loss: 0.20255154371261597 = 0.13447843492031097 + 0.01 * 6.807310104370117
Epoch 330, val loss: 0.7004657983779907
Epoch 340, training loss: 0.1828671246767044 = 0.11481861770153046 + 0.01 * 6.8048505783081055
Epoch 340, val loss: 0.7032164335250854
Epoch 350, training loss: 0.1668040007352829 = 0.09877732396125793 + 0.01 * 6.80266809463501
Epoch 350, val loss: 0.7084614634513855
Epoch 360, training loss: 0.15361826121807098 = 0.08563118427991867 + 0.01 * 6.798707962036133
Epoch 360, val loss: 0.7159612774848938
Epoch 370, training loss: 0.14269864559173584 = 0.07475218921899796 + 0.01 * 6.7946457862854
Epoch 370, val loss: 0.7253245711326599
Epoch 380, training loss: 0.13362079858779907 = 0.06566700339317322 + 0.01 * 6.795379638671875
Epoch 380, val loss: 0.735930860042572
Epoch 390, training loss: 0.12588344514369965 = 0.0580250509083271 + 0.01 * 6.785839080810547
Epoch 390, val loss: 0.747323751449585
Epoch 400, training loss: 0.11935833096504211 = 0.05154924467206001 + 0.01 * 6.780908584594727
Epoch 400, val loss: 0.7592625617980957
Epoch 410, training loss: 0.11378802359104156 = 0.04603074491024017 + 0.01 * 6.77572774887085
Epoch 410, val loss: 0.7715578675270081
Epoch 420, training loss: 0.108986496925354 = 0.04129747301340103 + 0.01 * 6.76890230178833
Epoch 420, val loss: 0.7839550375938416
Epoch 430, training loss: 0.10485449433326721 = 0.0372227318584919 + 0.01 * 6.763176918029785
Epoch 430, val loss: 0.7963927388191223
Epoch 440, training loss: 0.10128077864646912 = 0.033702146261930466 + 0.01 * 6.757863521575928
Epoch 440, val loss: 0.8087323904037476
Epoch 450, training loss: 0.09817203879356384 = 0.030645310878753662 + 0.01 * 6.7526726722717285
Epoch 450, val loss: 0.8208975195884705
Epoch 460, training loss: 0.09544992446899414 = 0.02797786518931389 + 0.01 * 6.747206687927246
Epoch 460, val loss: 0.8328827023506165
Epoch 470, training loss: 0.09305372834205627 = 0.025640051811933517 + 0.01 * 6.741368293762207
Epoch 470, val loss: 0.8445669412612915
Epoch 480, training loss: 0.09093984216451645 = 0.023581594228744507 + 0.01 * 6.7358245849609375
Epoch 480, val loss: 0.8560575842857361
Epoch 490, training loss: 0.08906523883342743 = 0.021760839968919754 + 0.01 * 6.730440616607666
Epoch 490, val loss: 0.8672981262207031
Epoch 500, training loss: 0.08744925260543823 = 0.020143572241067886 + 0.01 * 6.7305684089660645
Epoch 500, val loss: 0.8783013820648193
Epoch 510, training loss: 0.08591555058956146 = 0.018702220171689987 + 0.01 * 6.721332550048828
Epoch 510, val loss: 0.889056921005249
Epoch 520, training loss: 0.08461156487464905 = 0.01741291768848896 + 0.01 * 6.719864845275879
Epoch 520, val loss: 0.8995713591575623
Epoch 530, training loss: 0.08339118957519531 = 0.016255345195531845 + 0.01 * 6.713583946228027
Epoch 530, val loss: 0.9098299145698547
Epoch 540, training loss: 0.08229824900627136 = 0.015212791040539742 + 0.01 * 6.708546161651611
Epoch 540, val loss: 0.9198169112205505
Epoch 550, training loss: 0.0813315361738205 = 0.014270669780671597 + 0.01 * 6.7060866355896
Epoch 550, val loss: 0.9295517206192017
Epoch 560, training loss: 0.08040842413902283 = 0.013415955938398838 + 0.01 * 6.699247360229492
Epoch 560, val loss: 0.9390113949775696
Epoch 570, training loss: 0.07960136979818344 = 0.012636528350412846 + 0.01 * 6.696484088897705
Epoch 570, val loss: 0.9482905864715576
Epoch 580, training loss: 0.07887548208236694 = 0.011924535036087036 + 0.01 * 6.695094585418701
Epoch 580, val loss: 0.9573068022727966
Epoch 590, training loss: 0.07817254215478897 = 0.011274880729615688 + 0.01 * 6.689765930175781
Epoch 590, val loss: 0.9661251306533813
Epoch 600, training loss: 0.07750254124403 = 0.010680275969207287 + 0.01 * 6.682226657867432
Epoch 600, val loss: 0.9746933579444885
Epoch 610, training loss: 0.07697644084692001 = 0.01013339962810278 + 0.01 * 6.684304237365723
Epoch 610, val loss: 0.9830502271652222
Epoch 620, training loss: 0.07637674361467361 = 0.009629802778363228 + 0.01 * 6.674694061279297
Epoch 620, val loss: 0.991216242313385
Epoch 630, training loss: 0.0759064257144928 = 0.009164740331470966 + 0.01 * 6.674168109893799
Epoch 630, val loss: 0.9991819262504578
Epoch 640, training loss: 0.07538647204637527 = 0.00873479899019003 + 0.01 * 6.665167331695557
Epoch 640, val loss: 1.0069632530212402
Epoch 650, training loss: 0.07498402893543243 = 0.008336382918059826 + 0.01 * 6.664765357971191
Epoch 650, val loss: 1.014580249786377
Epoch 660, training loss: 0.0745982825756073 = 0.007966388016939163 + 0.01 * 6.663188934326172
Epoch 660, val loss: 1.0219987630844116
Epoch 670, training loss: 0.07421572506427765 = 0.007622456643730402 + 0.01 * 6.659326553344727
Epoch 670, val loss: 1.029254674911499
Epoch 680, training loss: 0.07380153238773346 = 0.007302042096853256 + 0.01 * 6.649948596954346
Epoch 680, val loss: 1.0363157987594604
Epoch 690, training loss: 0.07362063229084015 = 0.007002862170338631 + 0.01 * 6.661777496337891
Epoch 690, val loss: 1.0432100296020508
Epoch 700, training loss: 0.07315526902675629 = 0.006723935715854168 + 0.01 * 6.643134117126465
Epoch 700, val loss: 1.0499740839004517
Epoch 710, training loss: 0.0728427991271019 = 0.006463145837187767 + 0.01 * 6.637965202331543
Epoch 710, val loss: 1.0565600395202637
Epoch 720, training loss: 0.07255791127681732 = 0.006218534894287586 + 0.01 * 6.633937358856201
Epoch 720, val loss: 1.0630055665969849
Epoch 730, training loss: 0.07237089425325394 = 0.005988657474517822 + 0.01 * 6.638223648071289
Epoch 730, val loss: 1.0693038702011108
Epoch 740, training loss: 0.07204998284578323 = 0.005772576667368412 + 0.01 * 6.627740859985352
Epoch 740, val loss: 1.0755195617675781
Epoch 750, training loss: 0.0720614641904831 = 0.005569150671362877 + 0.01 * 6.649231433868408
Epoch 750, val loss: 1.0815510749816895
Epoch 760, training loss: 0.07165750861167908 = 0.005377922672778368 + 0.01 * 6.62795877456665
Epoch 760, val loss: 1.0874860286712646
Epoch 770, training loss: 0.07133611291646957 = 0.00519711384549737 + 0.01 * 6.613900184631348
Epoch 770, val loss: 1.093250036239624
Epoch 780, training loss: 0.07114225625991821 = 0.005025831982493401 + 0.01 * 6.611642360687256
Epoch 780, val loss: 1.0989271402359009
Epoch 790, training loss: 0.07096610218286514 = 0.004864020738750696 + 0.01 * 6.610208511352539
Epoch 790, val loss: 1.1044784784317017
Epoch 800, training loss: 0.0707208439707756 = 0.004711119458079338 + 0.01 * 6.6009721755981445
Epoch 800, val loss: 1.109925627708435
Epoch 810, training loss: 0.07054505497217178 = 0.004566633608192205 + 0.01 * 6.597842693328857
Epoch 810, val loss: 1.115238070487976
Epoch 820, training loss: 0.07066332548856735 = 0.004429991822689772 + 0.01 * 6.623333930969238
Epoch 820, val loss: 1.120383858680725
Epoch 830, training loss: 0.0702935978770256 = 0.004300569649785757 + 0.01 * 6.599302768707275
Epoch 830, val loss: 1.1255499124526978
Epoch 840, training loss: 0.07003063708543777 = 0.004177525639533997 + 0.01 * 6.585310935974121
Epoch 840, val loss: 1.1305077075958252
Epoch 850, training loss: 0.06989327073097229 = 0.004060495179146528 + 0.01 * 6.583277702331543
Epoch 850, val loss: 1.1354176998138428
Epoch 860, training loss: 0.06992615014314651 = 0.003949045203626156 + 0.01 * 6.597710609436035
Epoch 860, val loss: 1.1401824951171875
Epoch 870, training loss: 0.06966905295848846 = 0.0038432397413998842 + 0.01 * 6.582581043243408
Epoch 870, val loss: 1.1449229717254639
Epoch 880, training loss: 0.06941328197717667 = 0.003742352593690157 + 0.01 * 6.5670928955078125
Epoch 880, val loss: 1.149469256401062
Epoch 890, training loss: 0.06945221871137619 = 0.0036460969131439924 + 0.01 * 6.5806121826171875
Epoch 890, val loss: 1.154062032699585
Epoch 900, training loss: 0.06925834715366364 = 0.0035542414989322424 + 0.01 * 6.570411205291748
Epoch 900, val loss: 1.1584548950195312
Epoch 910, training loss: 0.0698833242058754 = 0.0034668087027966976 + 0.01 * 6.6416521072387695
Epoch 910, val loss: 1.162759780883789
Epoch 920, training loss: 0.06906170397996902 = 0.0033834055066108704 + 0.01 * 6.5678300857543945
Epoch 920, val loss: 1.1668615341186523
Epoch 930, training loss: 0.06876881420612335 = 0.003303680568933487 + 0.01 * 6.546513557434082
Epoch 930, val loss: 1.1710546016693115
Epoch 940, training loss: 0.06861462444067001 = 0.0032273202668875456 + 0.01 * 6.538730621337891
Epoch 940, val loss: 1.1750609874725342
Epoch 950, training loss: 0.06862735003232956 = 0.0031540170311927795 + 0.01 * 6.547333717346191
Epoch 950, val loss: 1.1790659427642822
Epoch 960, training loss: 0.06878405809402466 = 0.003083767369389534 + 0.01 * 6.5700297355651855
Epoch 960, val loss: 1.1829750537872314
Epoch 970, training loss: 0.06850040704011917 = 0.0030166488140821457 + 0.01 * 6.548375606536865
Epoch 970, val loss: 1.1867820024490356
Epoch 980, training loss: 0.06842782348394394 = 0.0029522234108299017 + 0.01 * 6.54755973815918
Epoch 980, val loss: 1.1905367374420166
Epoch 990, training loss: 0.06816595792770386 = 0.002890431322157383 + 0.01 * 6.527552604675293
Epoch 990, val loss: 1.1941897869110107
Epoch 1000, training loss: 0.06803489476442337 = 0.0028309181798249483 + 0.01 * 6.520398139953613
Epoch 1000, val loss: 1.197786808013916
Epoch 1010, training loss: 0.06793715804815292 = 0.0027738725766539574 + 0.01 * 6.516328811645508
Epoch 1010, val loss: 1.2013437747955322
Epoch 1020, training loss: 0.06797518581151962 = 0.002718888921663165 + 0.01 * 6.525629997253418
Epoch 1020, val loss: 1.2047789096832275
Epoch 1030, training loss: 0.06776162981987 = 0.0026661413721740246 + 0.01 * 6.509549140930176
Epoch 1030, val loss: 1.2081607580184937
Epoch 1040, training loss: 0.0676424577832222 = 0.002615294186398387 + 0.01 * 6.502716064453125
Epoch 1040, val loss: 1.2115204334259033
Epoch 1050, training loss: 0.06787657737731934 = 0.0025662702973932028 + 0.01 * 6.531030654907227
Epoch 1050, val loss: 1.2147365808486938
Epoch 1060, training loss: 0.0676148533821106 = 0.002519067842513323 + 0.01 * 6.509578227996826
Epoch 1060, val loss: 1.2179160118103027
Epoch 1070, training loss: 0.0675719603896141 = 0.00247351941652596 + 0.01 * 6.5098443031311035
Epoch 1070, val loss: 1.2210572957992554
Epoch 1080, training loss: 0.06733150780200958 = 0.0024297090712934732 + 0.01 * 6.490180015563965
Epoch 1080, val loss: 1.2241463661193848
Epoch 1090, training loss: 0.06747446954250336 = 0.002387311076745391 + 0.01 * 6.508716583251953
Epoch 1090, val loss: 1.227181077003479
Epoch 1100, training loss: 0.06732621788978577 = 0.0023463780526071787 + 0.01 * 6.497983455657959
Epoch 1100, val loss: 1.230135202407837
Epoch 1110, training loss: 0.0671035498380661 = 0.0023069235030561686 + 0.01 * 6.479662895202637
Epoch 1110, val loss: 1.2330783605575562
Epoch 1120, training loss: 0.06722392141819 = 0.0022686386946588755 + 0.01 * 6.495527744293213
Epoch 1120, val loss: 1.2359180450439453
Epoch 1130, training loss: 0.06702539324760437 = 0.002231731778010726 + 0.01 * 6.479366302490234
Epoch 1130, val loss: 1.2387422323226929
Epoch 1140, training loss: 0.06727800518274307 = 0.002195959212258458 + 0.01 * 6.508204460144043
Epoch 1140, val loss: 1.241514801979065
Epoch 1150, training loss: 0.06698465347290039 = 0.002161504467949271 + 0.01 * 6.482315540313721
Epoch 1150, val loss: 1.244232416152954
Epoch 1160, training loss: 0.06682336330413818 = 0.0021280793007463217 + 0.01 * 6.469528675079346
Epoch 1160, val loss: 1.2468888759613037
Epoch 1170, training loss: 0.0667610913515091 = 0.002095641801133752 + 0.01 * 6.466545104980469
Epoch 1170, val loss: 1.2495017051696777
Epoch 1180, training loss: 0.06692852824926376 = 0.0020642592571675777 + 0.01 * 6.486427307128906
Epoch 1180, val loss: 1.2521448135375977
Epoch 1190, training loss: 0.06669756770133972 = 0.002033761003986001 + 0.01 * 6.466380596160889
Epoch 1190, val loss: 1.2546899318695068
Epoch 1200, training loss: 0.06671006232500076 = 0.002004292095080018 + 0.01 * 6.470576763153076
Epoch 1200, val loss: 1.2571723461151123
Epoch 1210, training loss: 0.06674071401357651 = 0.001975616905838251 + 0.01 * 6.476510047912598
Epoch 1210, val loss: 1.2596666812896729
Epoch 1220, training loss: 0.06657877564430237 = 0.0019479840993881226 + 0.01 * 6.46307897567749
Epoch 1220, val loss: 1.2620787620544434
Epoch 1230, training loss: 0.06656884402036667 = 0.0019211048493161798 + 0.01 * 6.464774131774902
Epoch 1230, val loss: 1.2644795179367065
Epoch 1240, training loss: 0.06639306247234344 = 0.0018950351513922215 + 0.01 * 6.449803352355957
Epoch 1240, val loss: 1.266844391822815
Epoch 1250, training loss: 0.0665043294429779 = 0.0018696829210966825 + 0.01 * 6.463464736938477
Epoch 1250, val loss: 1.2691783905029297
Epoch 1260, training loss: 0.06631670147180557 = 0.00184505560901016 + 0.01 * 6.447164535522461
Epoch 1260, val loss: 1.2714439630508423
Epoch 1270, training loss: 0.06620945781469345 = 0.0018210705602541566 + 0.01 * 6.438838481903076
Epoch 1270, val loss: 1.2736865282058716
Epoch 1280, training loss: 0.06635870784521103 = 0.001797775155864656 + 0.01 * 6.4560933113098145
Epoch 1280, val loss: 1.2758604288101196
Epoch 1290, training loss: 0.06623802334070206 = 0.001775207812897861 + 0.01 * 6.446281909942627
Epoch 1290, val loss: 1.2780934572219849
Epoch 1300, training loss: 0.06616988033056259 = 0.0017532386118546128 + 0.01 * 6.44166374206543
Epoch 1300, val loss: 1.2802542448043823
Epoch 1310, training loss: 0.06605595350265503 = 0.0017318830359727144 + 0.01 * 6.432407855987549
Epoch 1310, val loss: 1.2824077606201172
Epoch 1320, training loss: 0.06606340408325195 = 0.0017110919579863548 + 0.01 * 6.4352312088012695
Epoch 1320, val loss: 1.2844024896621704
Epoch 1330, training loss: 0.06608263403177261 = 0.0016909092664718628 + 0.01 * 6.439172744750977
Epoch 1330, val loss: 1.2865184545516968
Epoch 1340, training loss: 0.06597206741571426 = 0.0016712500946596265 + 0.01 * 6.430082321166992
Epoch 1340, val loss: 1.2884844541549683
Epoch 1350, training loss: 0.06596489250659943 = 0.0016521831275895238 + 0.01 * 6.431271553039551
Epoch 1350, val loss: 1.2905206680297852
Epoch 1360, training loss: 0.06591738015413284 = 0.0016335278050974011 + 0.01 * 6.4283857345581055
Epoch 1360, val loss: 1.2924542427062988
Epoch 1370, training loss: 0.06579017639160156 = 0.001615378656424582 + 0.01 * 6.41748046875
Epoch 1370, val loss: 1.2944002151489258
Epoch 1380, training loss: 0.06585393846035004 = 0.001597649184986949 + 0.01 * 6.425628662109375
Epoch 1380, val loss: 1.296278953552246
Epoch 1390, training loss: 0.06591639667749405 = 0.0015804576687514782 + 0.01 * 6.433594226837158
Epoch 1390, val loss: 1.2981712818145752
Epoch 1400, training loss: 0.0659087598323822 = 0.0015636358875781298 + 0.01 * 6.434512615203857
Epoch 1400, val loss: 1.3000283241271973
Epoch 1410, training loss: 0.06581738591194153 = 0.0015472444938495755 + 0.01 * 6.427013874053955
Epoch 1410, val loss: 1.3018783330917358
Epoch 1420, training loss: 0.06586159765720367 = 0.0015312301693484187 + 0.01 * 6.433037281036377
Epoch 1420, val loss: 1.3036326169967651
Epoch 1430, training loss: 0.0656232014298439 = 0.0015156235313042998 + 0.01 * 6.410757541656494
Epoch 1430, val loss: 1.3054194450378418
Epoch 1440, training loss: 0.06598199903964996 = 0.0015003638109192252 + 0.01 * 6.448163986206055
Epoch 1440, val loss: 1.3072245121002197
Epoch 1450, training loss: 0.06552530080080032 = 0.0014855923363938928 + 0.01 * 6.403970718383789
Epoch 1450, val loss: 1.3089509010314941
Epoch 1460, training loss: 0.06576564908027649 = 0.001471174880862236 + 0.01 * 6.429447650909424
Epoch 1460, val loss: 1.3106963634490967
Epoch 1470, training loss: 0.06559180468320847 = 0.0014570539351552725 + 0.01 * 6.413475513458252
Epoch 1470, val loss: 1.3123924732208252
Epoch 1480, training loss: 0.06553463637828827 = 0.0014433516189455986 + 0.01 * 6.4091291427612305
Epoch 1480, val loss: 1.3140606880187988
Epoch 1490, training loss: 0.06560709327459335 = 0.0014299595495685935 + 0.01 * 6.417713165283203
Epoch 1490, val loss: 1.315750241279602
Epoch 1500, training loss: 0.0655338242650032 = 0.0014168542111292481 + 0.01 * 6.4116973876953125
Epoch 1500, val loss: 1.3174026012420654
Epoch 1510, training loss: 0.065500907599926 = 0.001404046663083136 + 0.01 * 6.40968656539917
Epoch 1510, val loss: 1.3190479278564453
Epoch 1520, training loss: 0.06537175923585892 = 0.001391504774801433 + 0.01 * 6.3980255126953125
Epoch 1520, val loss: 1.3206602334976196
Epoch 1530, training loss: 0.06562189012765884 = 0.0013792621903121471 + 0.01 * 6.4242634773254395
Epoch 1530, val loss: 1.3222169876098633
Epoch 1540, training loss: 0.0653192549943924 = 0.0013673692010343075 + 0.01 * 6.395188808441162
Epoch 1540, val loss: 1.3238049745559692
Epoch 1550, training loss: 0.06566070765256882 = 0.0013557488564401865 + 0.01 * 6.430495738983154
Epoch 1550, val loss: 1.325443983078003
Epoch 1560, training loss: 0.06530411541461945 = 0.001344324671663344 + 0.01 * 6.395979404449463
Epoch 1560, val loss: 1.3269009590148926
Epoch 1570, training loss: 0.06530929356813431 = 0.0013331978116184473 + 0.01 * 6.397609710693359
Epoch 1570, val loss: 1.3283926248550415
Epoch 1580, training loss: 0.06529258191585541 = 0.0013223103014752269 + 0.01 * 6.397027969360352
Epoch 1580, val loss: 1.3298906087875366
Epoch 1590, training loss: 0.06521843373775482 = 0.0013116071932017803 + 0.01 * 6.390682697296143
Epoch 1590, val loss: 1.331358790397644
Epoch 1600, training loss: 0.06506038457155228 = 0.0013012209674343467 + 0.01 * 6.375916481018066
Epoch 1600, val loss: 1.3328380584716797
Epoch 1610, training loss: 0.06533127278089523 = 0.0012910285731777549 + 0.01 * 6.404024600982666
Epoch 1610, val loss: 1.334208369255066
Epoch 1620, training loss: 0.06512311846017838 = 0.0012811116175726056 + 0.01 * 6.384200572967529
Epoch 1620, val loss: 1.3356246948242188
Epoch 1630, training loss: 0.06529998779296875 = 0.0012713151518255472 + 0.01 * 6.402867317199707
Epoch 1630, val loss: 1.336920142173767
Epoch 1640, training loss: 0.06506451219320297 = 0.001261809142306447 + 0.01 * 6.380270481109619
Epoch 1640, val loss: 1.3383508920669556
Epoch 1650, training loss: 0.0650472491979599 = 0.0012524661142379045 + 0.01 * 6.379478454589844
Epoch 1650, val loss: 1.3397201299667358
Epoch 1660, training loss: 0.06493588536977768 = 0.0012433122610673308 + 0.01 * 6.36925745010376
Epoch 1660, val loss: 1.3410011529922485
Epoch 1670, training loss: 0.06504327058792114 = 0.0012343378039076924 + 0.01 * 6.380893230438232
Epoch 1670, val loss: 1.342265248298645
Epoch 1680, training loss: 0.06506210565567017 = 0.0012255208566784859 + 0.01 * 6.383658409118652
Epoch 1680, val loss: 1.3436260223388672
Epoch 1690, training loss: 0.06501580774784088 = 0.0012169235851615667 + 0.01 * 6.37988805770874
Epoch 1690, val loss: 1.3448981046676636
Epoch 1700, training loss: 0.06523483246564865 = 0.0012084258487448096 + 0.01 * 6.402641296386719
Epoch 1700, val loss: 1.3461769819259644
Epoch 1710, training loss: 0.06470446288585663 = 0.0012001498835161328 + 0.01 * 6.350431442260742
Epoch 1710, val loss: 1.3474289178848267
Epoch 1720, training loss: 0.06503120064735413 = 0.0011920270044356585 + 0.01 * 6.383917331695557
Epoch 1720, val loss: 1.3486212491989136
Epoch 1730, training loss: 0.06483528017997742 = 0.001184079679660499 + 0.01 * 6.365120887756348
Epoch 1730, val loss: 1.3499919176101685
Epoch 1740, training loss: 0.06466536968946457 = 0.0011762400390580297 + 0.01 * 6.348913669586182
Epoch 1740, val loss: 1.3511813879013062
Epoch 1750, training loss: 0.0646369457244873 = 0.001168590853922069 + 0.01 * 6.346835136413574
Epoch 1750, val loss: 1.3523752689361572
Epoch 1760, training loss: 0.06472493708133698 = 0.0011610648361966014 + 0.01 * 6.356388092041016
Epoch 1760, val loss: 1.3535912036895752
Epoch 1770, training loss: 0.06450200825929642 = 0.0011536723468452692 + 0.01 * 6.334834098815918
Epoch 1770, val loss: 1.354772925376892
Epoch 1780, training loss: 0.06457475572824478 = 0.0011463887058198452 + 0.01 * 6.342836856842041
Epoch 1780, val loss: 1.3559011220932007
Epoch 1790, training loss: 0.06472831964492798 = 0.001139263971708715 + 0.01 * 6.358906269073486
Epoch 1790, val loss: 1.3570928573608398
Epoch 1800, training loss: 0.0644390806555748 = 0.0011322812642902136 + 0.01 * 6.330679893493652
Epoch 1800, val loss: 1.358368992805481
Epoch 1810, training loss: 0.06462062150239944 = 0.001125448034144938 + 0.01 * 6.349517345428467
Epoch 1810, val loss: 1.3594598770141602
Epoch 1820, training loss: 0.0645473524928093 = 0.0011186846531927586 + 0.01 * 6.342867374420166
Epoch 1820, val loss: 1.360663652420044
Epoch 1830, training loss: 0.06450420618057251 = 0.0011120865819975734 + 0.01 * 6.339211940765381
Epoch 1830, val loss: 1.3618232011795044
Epoch 1840, training loss: 0.06465036422014236 = 0.001105556613765657 + 0.01 * 6.354481220245361
Epoch 1840, val loss: 1.362956166267395
Epoch 1850, training loss: 0.06431528925895691 = 0.0010991465533152223 + 0.01 * 6.321614742279053
Epoch 1850, val loss: 1.3640706539154053
Epoch 1860, training loss: 0.06436984986066818 = 0.0010928647825494409 + 0.01 * 6.327698707580566
Epoch 1860, val loss: 1.365199089050293
Epoch 1870, training loss: 0.06433148682117462 = 0.0010866421507671475 + 0.01 * 6.324484348297119
Epoch 1870, val loss: 1.3663235902786255
Epoch 1880, training loss: 0.06417576968669891 = 0.0010805779602378607 + 0.01 * 6.309518814086914
Epoch 1880, val loss: 1.3675029277801514
Epoch 1890, training loss: 0.06440159678459167 = 0.001074558706022799 + 0.01 * 6.332703590393066
Epoch 1890, val loss: 1.3685035705566406
Epoch 1900, training loss: 0.06426411867141724 = 0.0010686558671295643 + 0.01 * 6.319546222686768
Epoch 1900, val loss: 1.3696041107177734
Epoch 1910, training loss: 0.06444837152957916 = 0.0010628628078848124 + 0.01 * 6.338551044464111
Epoch 1910, val loss: 1.370762586593628
Epoch 1920, training loss: 0.0641850158572197 = 0.0010570860467851162 + 0.01 * 6.312792778015137
Epoch 1920, val loss: 1.3718247413635254
Epoch 1930, training loss: 0.06431913375854492 = 0.0010514158057048917 + 0.01 * 6.3267717361450195
Epoch 1930, val loss: 1.3729132413864136
Epoch 1940, training loss: 0.06405606865882874 = 0.0010458152974024415 + 0.01 * 6.301025390625
Epoch 1940, val loss: 1.3739476203918457
Epoch 1950, training loss: 0.06467725336551666 = 0.0010402895277366042 + 0.01 * 6.363697052001953
Epoch 1950, val loss: 1.3750395774841309
Epoch 1960, training loss: 0.06413467228412628 = 0.001034860615618527 + 0.01 * 6.309980869293213
Epoch 1960, val loss: 1.376137614250183
Epoch 1970, training loss: 0.06431975960731506 = 0.0010295165702700615 + 0.01 * 6.329024791717529
Epoch 1970, val loss: 1.3772035837173462
Epoch 1980, training loss: 0.06390996277332306 = 0.0010242036078125238 + 0.01 * 6.288576126098633
Epoch 1980, val loss: 1.3781558275222778
Epoch 1990, training loss: 0.06423939764499664 = 0.0010189496679231524 + 0.01 * 6.322044849395752
Epoch 1990, val loss: 1.379238486289978
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.7823
Flip ASR: 0.7556/225 nodes
The final ASR:0.55720, 0.23305, Accuracy:0.81111, 0.00800
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11606])
remove edge: torch.Size([2, 9480])
updated graph: torch.Size([2, 10530])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8481
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.97786, 0.00603, Accuracy:0.83580, 0.01062
Begin epxeriment: cont_weight: 0.01 epoch:2000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0326383113861084 = 1.9488990306854248 + 0.01 * 8.373929023742676
Epoch 0, val loss: 1.9526290893554688
Epoch 10, training loss: 2.022843599319458 = 1.9391047954559326 + 0.01 * 8.37387752532959
Epoch 10, val loss: 1.9432648420333862
Epoch 20, training loss: 2.011119842529297 = 1.9273829460144043 + 0.01 * 8.373700141906738
Epoch 20, val loss: 1.9316684007644653
Epoch 30, training loss: 1.9947733879089355 = 1.9110420942306519 + 0.01 * 8.373135566711426
Epoch 30, val loss: 1.9151021242141724
Epoch 40, training loss: 1.9705158472061157 = 1.8868135213851929 + 0.01 * 8.37023639678955
Epoch 40, val loss: 1.890429973602295
Epoch 50, training loss: 1.9355382919311523 = 1.8520469665527344 + 0.01 * 8.349130630493164
Epoch 50, val loss: 1.8559515476226807
Epoch 60, training loss: 1.8923739194869995 = 1.809870719909668 + 0.01 * 8.250323295593262
Epoch 60, val loss: 1.8176264762878418
Epoch 70, training loss: 1.8508139848709106 = 1.7708947658538818 + 0.01 * 7.991926670074463
Epoch 70, val loss: 1.7866376638412476
Epoch 80, training loss: 1.8061095476150513 = 1.7283363342285156 + 0.01 * 7.7773237228393555
Epoch 80, val loss: 1.7513446807861328
Epoch 90, training loss: 1.744000792503357 = 1.669643759727478 + 0.01 * 7.435698986053467
Epoch 90, val loss: 1.7010611295700073
Epoch 100, training loss: 1.6624999046325684 = 1.5900593996047974 + 0.01 * 7.244050025939941
Epoch 100, val loss: 1.6334344148635864
Epoch 110, training loss: 1.563113808631897 = 1.4914219379425049 + 0.01 * 7.169191837310791
Epoch 110, val loss: 1.551970362663269
Epoch 120, training loss: 1.4534815549850464 = 1.3823564052581787 + 0.01 * 7.112509727478027
Epoch 120, val loss: 1.4639270305633545
Epoch 130, training loss: 1.3420257568359375 = 1.2715530395507812 + 0.01 * 7.0472731590271
Epoch 130, val loss: 1.376625418663025
Epoch 140, training loss: 1.2332210540771484 = 1.163332462310791 + 0.01 * 6.988864421844482
Epoch 140, val loss: 1.2921044826507568
Epoch 150, training loss: 1.130661129951477 = 1.0613267421722412 + 0.01 * 6.933443069458008
Epoch 150, val loss: 1.2126017808914185
Epoch 160, training loss: 1.0367796421051025 = 0.9678293466567993 + 0.01 * 6.895035266876221
Epoch 160, val loss: 1.1399084329605103
Epoch 170, training loss: 0.951194167137146 = 0.8823919892311096 + 0.01 * 6.88021993637085
Epoch 170, val loss: 1.074183464050293
Epoch 180, training loss: 0.8722737431526184 = 0.8035125732421875 + 0.01 * 6.876119136810303
Epoch 180, val loss: 1.014609694480896
Epoch 190, training loss: 0.7983381152153015 = 0.7296237945556641 + 0.01 * 6.871434688568115
Epoch 190, val loss: 0.9599599242210388
Epoch 200, training loss: 0.7279151082038879 = 0.6592283248901367 + 0.01 * 6.868678092956543
Epoch 200, val loss: 0.9091042280197144
Epoch 210, training loss: 0.6600281596183777 = 0.5913663506507874 + 0.01 * 6.866181373596191
Epoch 210, val loss: 0.8610158562660217
Epoch 220, training loss: 0.5948857069015503 = 0.5262518525123596 + 0.01 * 6.863388538360596
Epoch 220, val loss: 0.815419614315033
Epoch 230, training loss: 0.5336118340492249 = 0.4649939239025116 + 0.01 * 6.861788749694824
Epoch 230, val loss: 0.7731912732124329
Epoch 240, training loss: 0.47744160890579224 = 0.40887749195098877 + 0.01 * 6.856412410736084
Epoch 240, val loss: 0.7358156442642212
Epoch 250, training loss: 0.4270387887954712 = 0.3585164546966553 + 0.01 * 6.852231979370117
Epoch 250, val loss: 0.7042251825332642
Epoch 260, training loss: 0.38212722539901733 = 0.3136564791202545 + 0.01 * 6.84707498550415
Epoch 260, val loss: 0.6784440279006958
Epoch 270, training loss: 0.34216567873954773 = 0.2737354636192322 + 0.01 * 6.843022346496582
Epoch 270, val loss: 0.6578983664512634
Epoch 280, training loss: 0.3065282106399536 = 0.238149955868721 + 0.01 * 6.837825298309326
Epoch 280, val loss: 0.6419726610183716
Epoch 290, training loss: 0.27483201026916504 = 0.20649798214435577 + 0.01 * 6.833401203155518
Epoch 290, val loss: 0.6302300095558167
Epoch 300, training loss: 0.24687489867210388 = 0.17853762209415436 + 0.01 * 6.83372688293457
Epoch 300, val loss: 0.6224970817565918
Epoch 310, training loss: 0.22242054343223572 = 0.1541355550289154 + 0.01 * 6.828499794006348
Epoch 310, val loss: 0.6184697151184082
Epoch 320, training loss: 0.20137745141983032 = 0.1331285536289215 + 0.01 * 6.824889659881592
Epoch 320, val loss: 0.6180973052978516
Epoch 330, training loss: 0.18347835540771484 = 0.11525373160839081 + 0.01 * 6.8224616050720215
Epoch 330, val loss: 0.6206779479980469
Epoch 340, training loss: 0.1683599352836609 = 0.10015910863876343 + 0.01 * 6.8200836181640625
Epoch 340, val loss: 0.6259106993675232
Epoch 350, training loss: 0.15567255020141602 = 0.08745649456977844 + 0.01 * 6.8216047286987305
Epoch 350, val loss: 0.6334129571914673
Epoch 360, training loss: 0.14489886164665222 = 0.0767522007226944 + 0.01 * 6.814666748046875
Epoch 360, val loss: 0.64249187707901
Epoch 370, training loss: 0.13583031296730042 = 0.06770722568035126 + 0.01 * 6.812307834625244
Epoch 370, val loss: 0.6527541279792786
Epoch 380, training loss: 0.12812891602516174 = 0.06003940850496292 + 0.01 * 6.808951377868652
Epoch 380, val loss: 0.6638176441192627
Epoch 390, training loss: 0.12164311856031418 = 0.05351223051548004 + 0.01 * 6.813088893890381
Epoch 390, val loss: 0.6754593849182129
Epoch 400, training loss: 0.11598359048366547 = 0.047931745648384094 + 0.01 * 6.805184364318848
Epoch 400, val loss: 0.6873573660850525
Epoch 410, training loss: 0.11113549768924713 = 0.04313444346189499 + 0.01 * 6.800105571746826
Epoch 410, val loss: 0.6995341181755066
Epoch 420, training loss: 0.10695137083530426 = 0.03898876532912254 + 0.01 * 6.796260356903076
Epoch 420, val loss: 0.7117390632629395
Epoch 430, training loss: 0.1033574789762497 = 0.03538816422224045 + 0.01 * 6.796931743621826
Epoch 430, val loss: 0.7239099144935608
Epoch 440, training loss: 0.10013173520565033 = 0.03224698826670647 + 0.01 * 6.7884745597839355
Epoch 440, val loss: 0.7359299659729004
Epoch 450, training loss: 0.0973343700170517 = 0.029492881149053574 + 0.01 * 6.784149169921875
Epoch 450, val loss: 0.7478114366531372
Epoch 460, training loss: 0.09487950056791306 = 0.02706807665526867 + 0.01 * 6.781142711639404
Epoch 460, val loss: 0.759493887424469
Epoch 470, training loss: 0.09273494780063629 = 0.024924539029598236 + 0.01 * 6.781040668487549
Epoch 470, val loss: 0.7709552049636841
Epoch 480, training loss: 0.09073257446289062 = 0.023023534566164017 + 0.01 * 6.770904064178467
Epoch 480, val loss: 0.7821931838989258
Epoch 490, training loss: 0.08900104463100433 = 0.021330133080482483 + 0.01 * 6.767091274261475
Epoch 490, val loss: 0.7931303381919861
Epoch 500, training loss: 0.08753643184900284 = 0.019815748557448387 + 0.01 * 6.772068023681641
Epoch 500, val loss: 0.8038231134414673
Epoch 510, training loss: 0.08604742586612701 = 0.018457632511854172 + 0.01 * 6.758979797363281
Epoch 510, val loss: 0.8143059611320496
Epoch 520, training loss: 0.0847683772444725 = 0.017234906554222107 + 0.01 * 6.753346920013428
Epoch 520, val loss: 0.8244480490684509
Epoch 530, training loss: 0.08364570140838623 = 0.01613062620162964 + 0.01 * 6.751507759094238
Epoch 530, val loss: 0.8343945145606995
Epoch 540, training loss: 0.08258777856826782 = 0.015131034888327122 + 0.01 * 6.7456746101379395
Epoch 540, val loss: 0.8441006541252136
Epoch 550, training loss: 0.08163643628358841 = 0.014223704114556313 + 0.01 * 6.741273880004883
Epoch 550, val loss: 0.8535258769989014
Epoch 560, training loss: 0.08071956038475037 = 0.013397770933806896 + 0.01 * 6.732179641723633
Epoch 560, val loss: 0.8627307415008545
Epoch 570, training loss: 0.07997086644172668 = 0.01264356542378664 + 0.01 * 6.732729911804199
Epoch 570, val loss: 0.8716960549354553
Epoch 580, training loss: 0.07918760925531387 = 0.011953325010836124 + 0.01 * 6.723428726196289
Epoch 580, val loss: 0.8804582953453064
Epoch 590, training loss: 0.0785086452960968 = 0.01132015511393547 + 0.01 * 6.718849182128906
Epoch 590, val loss: 0.888981819152832
Epoch 600, training loss: 0.07789172232151031 = 0.01073866430670023 + 0.01 * 6.715306282043457
Epoch 600, val loss: 0.8973038196563721
Epoch 610, training loss: 0.07726269215345383 = 0.010203450918197632 + 0.01 * 6.7059245109558105
Epoch 610, val loss: 0.9054001569747925
Epoch 620, training loss: 0.0767413005232811 = 0.00970914401113987 + 0.01 * 6.703216075897217
Epoch 620, val loss: 0.9133403897285461
Epoch 630, training loss: 0.07627216726541519 = 0.009252249263226986 + 0.01 * 6.701992034912109
Epoch 630, val loss: 0.9210284352302551
Epoch 640, training loss: 0.07571998238563538 = 0.008829109370708466 + 0.01 * 6.689087390899658
Epoch 640, val loss: 0.9285751581192017
Epoch 650, training loss: 0.07542310655117035 = 0.008436238393187523 + 0.01 * 6.6986870765686035
Epoch 650, val loss: 0.9359598159790039
Epoch 660, training loss: 0.0749264657497406 = 0.008071128278970718 + 0.01 * 6.6855340003967285
Epoch 660, val loss: 0.9431201219558716
Epoch 670, training loss: 0.07457974553108215 = 0.007731135468930006 + 0.01 * 6.684861660003662
Epoch 670, val loss: 0.9501059055328369
Epoch 680, training loss: 0.07416447252035141 = 0.0074147251434624195 + 0.01 * 6.67497444152832
Epoch 680, val loss: 0.9569790363311768
Epoch 690, training loss: 0.07364597916603088 = 0.007119353394955397 + 0.01 * 6.652662754058838
Epoch 690, val loss: 0.9636085629463196
Epoch 700, training loss: 0.07343727350234985 = 0.0068428595550358295 + 0.01 * 6.6594414710998535
Epoch 700, val loss: 0.9701140522956848
Epoch 710, training loss: 0.0731540098786354 = 0.006583780515938997 + 0.01 * 6.657022953033447
Epoch 710, val loss: 0.9764866828918457
Epoch 720, training loss: 0.07265856117010117 = 0.0063408673740923405 + 0.01 * 6.631769180297852
Epoch 720, val loss: 0.9827192425727844
Epoch 730, training loss: 0.07271472364664078 = 0.006112716626375914 + 0.01 * 6.660200595855713
Epoch 730, val loss: 0.9887974262237549
Epoch 740, training loss: 0.07213170826435089 = 0.005898535251617432 + 0.01 * 6.623317241668701
Epoch 740, val loss: 0.9947238564491272
Epoch 750, training loss: 0.07196832448244095 = 0.005696923937648535 + 0.01 * 6.627140522003174
Epoch 750, val loss: 1.0005460977554321
Epoch 760, training loss: 0.07168766111135483 = 0.005506768357008696 + 0.01 * 6.61808967590332
Epoch 760, val loss: 1.0062353610992432
Epoch 770, training loss: 0.07134643942117691 = 0.005327288992702961 + 0.01 * 6.601914882659912
Epoch 770, val loss: 1.0117462873458862
Epoch 780, training loss: 0.07119192183017731 = 0.005157888401299715 + 0.01 * 6.603403568267822
Epoch 780, val loss: 1.0171606540679932
Epoch 790, training loss: 0.0711621344089508 = 0.004997581709176302 + 0.01 * 6.616455554962158
Epoch 790, val loss: 1.0224896669387817
Epoch 800, training loss: 0.07063980400562286 = 0.004845853894948959 + 0.01 * 6.579394817352295
Epoch 800, val loss: 1.0276892185211182
Epoch 810, training loss: 0.07075823098421097 = 0.004702054429799318 + 0.01 * 6.605618000030518
Epoch 810, val loss: 1.0327613353729248
Epoch 820, training loss: 0.07036425173282623 = 0.004565860144793987 + 0.01 * 6.579839706420898
Epoch 820, val loss: 1.037765383720398
Epoch 830, training loss: 0.07016460597515106 = 0.004436563700437546 + 0.01 * 6.5728044509887695
Epoch 830, val loss: 1.042602300643921
Epoch 840, training loss: 0.07001730799674988 = 0.0043135397136211395 + 0.01 * 6.570376396179199
Epoch 840, val loss: 1.047377347946167
Epoch 850, training loss: 0.06989656388759613 = 0.004196741618216038 + 0.01 * 6.569982528686523
Epoch 850, val loss: 1.0520501136779785
Epoch 860, training loss: 0.06964263319969177 = 0.004085193853825331 + 0.01 * 6.555744171142578
Epoch 860, val loss: 1.056639313697815
Epoch 870, training loss: 0.06967140734195709 = 0.003978875931352377 + 0.01 * 6.569252967834473
Epoch 870, val loss: 1.0611556768417358
Epoch 880, training loss: 0.06952120363712311 = 0.0038773794658482075 + 0.01 * 6.564382553100586
Epoch 880, val loss: 1.0656071901321411
Epoch 890, training loss: 0.06917951256036758 = 0.0037806332111358643 + 0.01 * 6.53988790512085
Epoch 890, val loss: 1.0698935985565186
Epoch 900, training loss: 0.06922243535518646 = 0.0036883188877254725 + 0.01 * 6.553411960601807
Epoch 900, val loss: 1.074178695678711
Epoch 910, training loss: 0.06892330199480057 = 0.003599895164370537 + 0.01 * 6.5323405265808105
Epoch 910, val loss: 1.0783262252807617
Epoch 920, training loss: 0.06899721920490265 = 0.003515391144901514 + 0.01 * 6.548182487487793
Epoch 920, val loss: 1.08242928981781
Epoch 930, training loss: 0.06882385164499283 = 0.003434522543102503 + 0.01 * 6.538933277130127
Epoch 930, val loss: 1.0864311456680298
Epoch 940, training loss: 0.06873007118701935 = 0.0033570125233381987 + 0.01 * 6.53730583190918
Epoch 940, val loss: 1.090415120124817
Epoch 950, training loss: 0.0684349313378334 = 0.0032827139366418123 + 0.01 * 6.515222549438477
Epoch 950, val loss: 1.0942806005477905
Epoch 960, training loss: 0.06866559386253357 = 0.003211470553651452 + 0.01 * 6.545412540435791
Epoch 960, val loss: 1.0980987548828125
Epoch 970, training loss: 0.06829728931188583 = 0.0031430914532393217 + 0.01 * 6.515419960021973
Epoch 970, val loss: 1.1018368005752563
Epoch 980, training loss: 0.06836626678705215 = 0.0030773552134633064 + 0.01 * 6.528891086578369
Epoch 980, val loss: 1.105566143989563
Epoch 990, training loss: 0.06825631111860275 = 0.003014290938153863 + 0.01 * 6.524202346801758
Epoch 990, val loss: 1.1091188192367554
Epoch 1000, training loss: 0.06791170686483383 = 0.002953522140160203 + 0.01 * 6.495818614959717
Epoch 1000, val loss: 1.1127218008041382
Epoch 1010, training loss: 0.06811105459928513 = 0.0028949957340955734 + 0.01 * 6.521605491638184
Epoch 1010, val loss: 1.1162441968917847
Epoch 1020, training loss: 0.06779313832521439 = 0.0028386730700731277 + 0.01 * 6.495447158813477
Epoch 1020, val loss: 1.1196316480636597
Epoch 1030, training loss: 0.06777309626340866 = 0.0027844971045851707 + 0.01 * 6.498859882354736
Epoch 1030, val loss: 1.1230428218841553
Epoch 1040, training loss: 0.06772371381521225 = 0.0027322208043187857 + 0.01 * 6.499149799346924
Epoch 1040, val loss: 1.1263506412506104
Epoch 1050, training loss: 0.06745859980583191 = 0.002681705867871642 + 0.01 * 6.477689743041992
Epoch 1050, val loss: 1.1296665668487549
Epoch 1060, training loss: 0.06744379550218582 = 0.0026329923421144485 + 0.01 * 6.481080532073975
Epoch 1060, val loss: 1.1328856945037842
Epoch 1070, training loss: 0.06738138198852539 = 0.0025861277244985104 + 0.01 * 6.479526042938232
Epoch 1070, val loss: 1.1360691785812378
Epoch 1080, training loss: 0.06736035645008087 = 0.002540889661759138 + 0.01 * 6.48194694519043
Epoch 1080, val loss: 1.1392015218734741
Epoch 1090, training loss: 0.0672076940536499 = 0.0024973242543637753 + 0.01 * 6.4710373878479
Epoch 1090, val loss: 1.1422587633132935
Epoch 1100, training loss: 0.06736737489700317 = 0.0024549798108637333 + 0.01 * 6.491239547729492
Epoch 1100, val loss: 1.1453312635421753
Epoch 1110, training loss: 0.06717970967292786 = 0.0024142100010067225 + 0.01 * 6.4765496253967285
Epoch 1110, val loss: 1.1483064889907837
Epoch 1120, training loss: 0.06725931167602539 = 0.0023746229708194733 + 0.01 * 6.488468647003174
Epoch 1120, val loss: 1.1512928009033203
Epoch 1130, training loss: 0.06720060110092163 = 0.0023364145308732986 + 0.01 * 6.486419200897217
Epoch 1130, val loss: 1.1541428565979004
Epoch 1140, training loss: 0.06700631976127625 = 0.0022995653562247753 + 0.01 * 6.470675945281982
Epoch 1140, val loss: 1.1570208072662354
Epoch 1150, training loss: 0.06689179688692093 = 0.002263920148834586 + 0.01 * 6.462787628173828
Epoch 1150, val loss: 1.1597856283187866
Epoch 1160, training loss: 0.06696297973394394 = 0.002229303354397416 + 0.01 * 6.473367691040039
Epoch 1160, val loss: 1.162599802017212
Epoch 1170, training loss: 0.06675257533788681 = 0.002195879351347685 + 0.01 * 6.455669403076172
Epoch 1170, val loss: 1.1653107404708862
Epoch 1180, training loss: 0.06679324805736542 = 0.00216329051181674 + 0.01 * 6.462996006011963
Epoch 1180, val loss: 1.1679927110671997
Epoch 1190, training loss: 0.0666041150689125 = 0.0021319349762052298 + 0.01 * 6.447218418121338
Epoch 1190, val loss: 1.1706109046936035
Epoch 1200, training loss: 0.06666544079780579 = 0.002101462334394455 + 0.01 * 6.4563984870910645
Epoch 1200, val loss: 1.1731948852539062
Epoch 1210, training loss: 0.06663592159748077 = 0.0020719359163194895 + 0.01 * 6.4563984870910645
Epoch 1210, val loss: 1.1757678985595703
Epoch 1220, training loss: 0.06645425409078598 = 0.0020431980956345797 + 0.01 * 6.441105842590332
Epoch 1220, val loss: 1.1782649755477905
Epoch 1230, training loss: 0.06650429964065552 = 0.0020153559744358063 + 0.01 * 6.448894023895264
Epoch 1230, val loss: 1.1807901859283447
Epoch 1240, training loss: 0.06631648540496826 = 0.0019882619380950928 + 0.01 * 6.432822227478027
Epoch 1240, val loss: 1.1832356452941895
Epoch 1250, training loss: 0.06636154651641846 = 0.001961893867701292 + 0.01 * 6.43996524810791
Epoch 1250, val loss: 1.1856459379196167
Epoch 1260, training loss: 0.06631958484649658 = 0.001936386339366436 + 0.01 * 6.438320636749268
Epoch 1260, val loss: 1.1880903244018555
Epoch 1270, training loss: 0.0661718025803566 = 0.0019115785835310817 + 0.01 * 6.426022529602051
Epoch 1270, val loss: 1.1904139518737793
Epoch 1280, training loss: 0.06624138355255127 = 0.001887484104372561 + 0.01 * 6.435389995574951
Epoch 1280, val loss: 1.192749261856079
Epoch 1290, training loss: 0.06645853817462921 = 0.001863948768004775 + 0.01 * 6.45945930480957
Epoch 1290, val loss: 1.1950829029083252
Epoch 1300, training loss: 0.06618926674127579 = 0.0018411600030958652 + 0.01 * 6.434811115264893
Epoch 1300, val loss: 1.1973261833190918
Epoch 1310, training loss: 0.06612866371870041 = 0.001818892196752131 + 0.01 * 6.430976867675781
Epoch 1310, val loss: 1.1995506286621094
Epoch 1320, training loss: 0.06610828638076782 = 0.0017972872592508793 + 0.01 * 6.431099891662598
Epoch 1320, val loss: 1.2018104791641235
Epoch 1330, training loss: 0.06596995145082474 = 0.0017762257484719157 + 0.01 * 6.419373035430908
Epoch 1330, val loss: 1.203945517539978
Epoch 1340, training loss: 0.06635983288288116 = 0.0017556590028107166 + 0.01 * 6.4604172706604
Epoch 1340, val loss: 1.2061415910720825
Epoch 1350, training loss: 0.06588984280824661 = 0.0017356916796416044 + 0.01 * 6.415415287017822
Epoch 1350, val loss: 1.2082582712173462
Epoch 1360, training loss: 0.06577005237340927 = 0.0017161770956590772 + 0.01 * 6.405387878417969
Epoch 1360, val loss: 1.2103500366210938
Epoch 1370, training loss: 0.06589129567146301 = 0.0016971789300441742 + 0.01 * 6.419412136077881
Epoch 1370, val loss: 1.2124131917953491
Epoch 1380, training loss: 0.06590600311756134 = 0.0016787101048976183 + 0.01 * 6.4227294921875
Epoch 1380, val loss: 1.2144452333450317
Epoch 1390, training loss: 0.0655898004770279 = 0.0016606524586677551 + 0.01 * 6.392914772033691
Epoch 1390, val loss: 1.2164658308029175
Epoch 1400, training loss: 0.06593535095453262 = 0.0016430520918220282 + 0.01 * 6.429229736328125
Epoch 1400, val loss: 1.2184995412826538
Epoch 1410, training loss: 0.06558864563703537 = 0.0016259578987956047 + 0.01 * 6.396268844604492
Epoch 1410, val loss: 1.220423936843872
Epoch 1420, training loss: 0.06559975445270538 = 0.0016092982841655612 + 0.01 * 6.399045944213867
Epoch 1420, val loss: 1.2223528623580933
Epoch 1430, training loss: 0.06583847105503082 = 0.0015929470537230372 + 0.01 * 6.4245524406433105
Epoch 1430, val loss: 1.2243242263793945
Epoch 1440, training loss: 0.06558004021644592 = 0.0015769691672176123 + 0.01 * 6.400307655334473
Epoch 1440, val loss: 1.2261526584625244
Epoch 1450, training loss: 0.06577248871326447 = 0.0015614476287737489 + 0.01 * 6.4211039543151855
Epoch 1450, val loss: 1.2279958724975586
Epoch 1460, training loss: 0.06565548479557037 = 0.001546211657114327 + 0.01 * 6.410927772521973
Epoch 1460, val loss: 1.2298895120620728
Epoch 1470, training loss: 0.06547831743955612 = 0.0015313809271901846 + 0.01 * 6.394693851470947
Epoch 1470, val loss: 1.2316616773605347
Epoch 1480, training loss: 0.0657077431678772 = 0.0015168669633567333 + 0.01 * 6.419087886810303
Epoch 1480, val loss: 1.2334747314453125
Epoch 1490, training loss: 0.06548825651407242 = 0.0015027064364403486 + 0.01 * 6.398554801940918
Epoch 1490, val loss: 1.235245704650879
Epoch 1500, training loss: 0.06524214148521423 = 0.001488907146267593 + 0.01 * 6.375323295593262
Epoch 1500, val loss: 1.2369900941848755
Epoch 1510, training loss: 0.0657115951180458 = 0.0014753638533875346 + 0.01 * 6.423623085021973
Epoch 1510, val loss: 1.2387622594833374
Epoch 1520, training loss: 0.06517362594604492 = 0.001462101237848401 + 0.01 * 6.371152877807617
Epoch 1520, val loss: 1.2404124736785889
Epoch 1530, training loss: 0.06541334092617035 = 0.0014491579495370388 + 0.01 * 6.396418571472168
Epoch 1530, val loss: 1.242073893547058
Epoch 1540, training loss: 0.06548023968935013 = 0.0014364750823006034 + 0.01 * 6.404376983642578
Epoch 1540, val loss: 1.2437714338302612
Epoch 1550, training loss: 0.06511527299880981 = 0.0014241259777918458 + 0.01 * 6.369114875793457
Epoch 1550, val loss: 1.2453792095184326
Epoch 1560, training loss: 0.06541929394006729 = 0.0014120473060756922 + 0.01 * 6.4007248878479
Epoch 1560, val loss: 1.2469863891601562
Epoch 1570, training loss: 0.06506019085645676 = 0.0014001888921484351 + 0.01 * 6.366000175476074
Epoch 1570, val loss: 1.248557209968567
Epoch 1580, training loss: 0.06495942920446396 = 0.0013885723892599344 + 0.01 * 6.357085704803467
Epoch 1580, val loss: 1.2501276731491089
Epoch 1590, training loss: 0.06518694013357162 = 0.0013772114180028439 + 0.01 * 6.380972862243652
Epoch 1590, val loss: 1.251686930656433
Epoch 1600, training loss: 0.0650918260216713 = 0.001366042997688055 + 0.01 * 6.3725786209106445
Epoch 1600, val loss: 1.253228783607483
Epoch 1610, training loss: 0.0649660974740982 = 0.0013551306910812855 + 0.01 * 6.361096382141113
Epoch 1610, val loss: 1.254714846611023
Epoch 1620, training loss: 0.0652185007929802 = 0.0013444282812997699 + 0.01 * 6.387407302856445
Epoch 1620, val loss: 1.2562509775161743
Epoch 1630, training loss: 0.06492160260677338 = 0.0013339609140530229 + 0.01 * 6.3587646484375
Epoch 1630, val loss: 1.2576773166656494
Epoch 1640, training loss: 0.06498267501592636 = 0.0013236728264018893 + 0.01 * 6.36590051651001
Epoch 1640, val loss: 1.2591689825057983
Epoch 1650, training loss: 0.06509935855865479 = 0.0013136478373780847 + 0.01 * 6.378571033477783
Epoch 1650, val loss: 1.2605608701705933
Epoch 1660, training loss: 0.064798504114151 = 0.0013037530006840825 + 0.01 * 6.349474906921387
Epoch 1660, val loss: 1.2619868516921997
Epoch 1670, training loss: 0.06482802331447601 = 0.0012940771412104368 + 0.01 * 6.353394508361816
Epoch 1670, val loss: 1.2634131908416748
Epoch 1680, training loss: 0.06483563035726547 = 0.001284630037844181 + 0.01 * 6.355100154876709
Epoch 1680, val loss: 1.2647792100906372
Epoch 1690, training loss: 0.06524720042943954 = 0.001275308895856142 + 0.01 * 6.397189617156982
Epoch 1690, val loss: 1.266213297843933
Epoch 1700, training loss: 0.06480593979358673 = 0.0012661890359595418 + 0.01 * 6.353975296020508
Epoch 1700, val loss: 1.267486572265625
Epoch 1710, training loss: 0.06510029733181 = 0.001257257186807692 + 0.01 * 6.384304523468018
Epoch 1710, val loss: 1.2688465118408203
Epoch 1720, training loss: 0.0647280141711235 = 0.0012485053157433867 + 0.01 * 6.3479509353637695
Epoch 1720, val loss: 1.2701268196105957
Epoch 1730, training loss: 0.06476125866174698 = 0.0012398824328556657 + 0.01 * 6.352137565612793
Epoch 1730, val loss: 1.2714762687683105
Epoch 1740, training loss: 0.06490059196949005 = 0.0012314410414546728 + 0.01 * 6.366915702819824
Epoch 1740, val loss: 1.2727758884429932
Epoch 1750, training loss: 0.06473210453987122 = 0.0012231259606778622 + 0.01 * 6.350898265838623
Epoch 1750, val loss: 1.2740106582641602
Epoch 1760, training loss: 0.06459493190050125 = 0.0012149661779403687 + 0.01 * 6.337996959686279
Epoch 1760, val loss: 1.2752827405929565
Epoch 1770, training loss: 0.06464436650276184 = 0.0012069711228832603 + 0.01 * 6.343739986419678
Epoch 1770, val loss: 1.2765594720840454
Epoch 1780, training loss: 0.06470819562673569 = 0.0011990489438176155 + 0.01 * 6.350914478302002
Epoch 1780, val loss: 1.277804970741272
Epoch 1790, training loss: 0.06476598232984543 = 0.0011913195485249162 + 0.01 * 6.357466697692871
Epoch 1790, val loss: 1.2790189981460571
Epoch 1800, training loss: 0.06438110023736954 = 0.001183731947094202 + 0.01 * 6.319736480712891
Epoch 1800, val loss: 1.2802585363388062
Epoch 1810, training loss: 0.06445766985416412 = 0.0011762487702071667 + 0.01 * 6.3281426429748535
Epoch 1810, val loss: 1.2814916372299194
Epoch 1820, training loss: 0.06452499330043793 = 0.001168935326859355 + 0.01 * 6.335606098175049
Epoch 1820, val loss: 1.2826833724975586
Epoch 1830, training loss: 0.06439319252967834 = 0.0011616923147812486 + 0.01 * 6.323149681091309
Epoch 1830, val loss: 1.283879041671753
Epoch 1840, training loss: 0.0648038387298584 = 0.0011545814340934157 + 0.01 * 6.364925861358643
Epoch 1840, val loss: 1.2850570678710938
Epoch 1850, training loss: 0.06454350799322128 = 0.0011476125800982118 + 0.01 * 6.339589595794678
Epoch 1850, val loss: 1.2862240076065063
Epoch 1860, training loss: 0.06429802626371384 = 0.0011407266138121486 + 0.01 * 6.315730094909668
Epoch 1860, val loss: 1.2873679399490356
Epoch 1870, training loss: 0.06477215886116028 = 0.0011339762713760138 + 0.01 * 6.363818168640137
Epoch 1870, val loss: 1.2885527610778809
Epoch 1880, training loss: 0.06449175626039505 = 0.0011273083509877324 + 0.01 * 6.336444854736328
Epoch 1880, val loss: 1.2897101640701294
Epoch 1890, training loss: 0.06436342746019363 = 0.0011207417119294405 + 0.01 * 6.3242692947387695
Epoch 1890, val loss: 1.2908432483673096
Epoch 1900, training loss: 0.06420733034610748 = 0.0011142698349431157 + 0.01 * 6.309305667877197
Epoch 1900, val loss: 1.2919567823410034
Epoch 1910, training loss: 0.06415735185146332 = 0.0011079078540205956 + 0.01 * 6.3049445152282715
Epoch 1910, val loss: 1.2931009531021118
Epoch 1920, training loss: 0.06436274200677872 = 0.0011016408680006862 + 0.01 * 6.326110363006592
Epoch 1920, val loss: 1.2942063808441162
Epoch 1930, training loss: 0.06428420543670654 = 0.0010954855242744088 + 0.01 * 6.31887149810791
Epoch 1930, val loss: 1.2952710390090942
Epoch 1940, training loss: 0.0643872618675232 = 0.0010893919970840216 + 0.01 * 6.329786777496338
Epoch 1940, val loss: 1.29638671875
Epoch 1950, training loss: 0.06432593613862991 = 0.001083408365957439 + 0.01 * 6.324252605438232
Epoch 1950, val loss: 1.297461986541748
Epoch 1960, training loss: 0.06416048109531403 = 0.0010774671100080013 + 0.01 * 6.3083014488220215
Epoch 1960, val loss: 1.2985484600067139
Epoch 1970, training loss: 0.06426618993282318 = 0.0010716476244851947 + 0.01 * 6.319454193115234
Epoch 1970, val loss: 1.2995896339416504
Epoch 1980, training loss: 0.06449226289987564 = 0.0010659383842721581 + 0.01 * 6.342632293701172
Epoch 1980, val loss: 1.3007216453552246
Epoch 1990, training loss: 0.06398044526576996 = 0.0010602730326354504 + 0.01 * 6.292017936706543
Epoch 1990, val loss: 1.3017387390136719
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.5720
Flip ASR: 0.5022/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.042074203491211 = 1.9583349227905273 + 0.01 * 8.373921394348145
Epoch 0, val loss: 1.9580878019332886
Epoch 10, training loss: 2.031262159347534 = 1.947523832321167 + 0.01 * 8.373827934265137
Epoch 10, val loss: 1.9473750591278076
Epoch 20, training loss: 2.0179550647735596 = 1.9342199563980103 + 0.01 * 8.373507499694824
Epoch 20, val loss: 1.933752417564392
Epoch 30, training loss: 1.9991577863693237 = 1.9154343605041504 + 0.01 * 8.372346878051758
Epoch 30, val loss: 1.9140859842300415
Epoch 40, training loss: 1.9711527824401855 = 1.8874999284744263 + 0.01 * 8.365283966064453
Epoch 40, val loss: 1.8847233057022095
Epoch 50, training loss: 1.9311524629592896 = 1.8479479551315308 + 0.01 * 8.32044792175293
Epoch 50, val loss: 1.8445498943328857
Epoch 60, training loss: 1.8835922479629517 = 1.8023189306259155 + 0.01 * 8.127330780029297
Epoch 60, val loss: 1.802064061164856
Epoch 70, training loss: 1.8404014110565186 = 1.7613025903701782 + 0.01 * 7.909887313842773
Epoch 70, val loss: 1.7661354541778564
Epoch 80, training loss: 1.790793538093567 = 1.7132176160812378 + 0.01 * 7.757597923278809
Epoch 80, val loss: 1.7214441299438477
Epoch 90, training loss: 1.722924828529358 = 1.6477079391479492 + 0.01 * 7.521684169769287
Epoch 90, val loss: 1.6630085706710815
Epoch 100, training loss: 1.6345254182815552 = 1.56125807762146 + 0.01 * 7.3267340660095215
Epoch 100, val loss: 1.5898278951644897
Epoch 110, training loss: 1.5349347591400146 = 1.4640766382217407 + 0.01 * 7.085818290710449
Epoch 110, val loss: 1.511555552482605
Epoch 120, training loss: 1.4407529830932617 = 1.370768427848816 + 0.01 * 6.998458385467529
Epoch 120, val loss: 1.4416780471801758
Epoch 130, training loss: 1.3555030822753906 = 1.2860068082809448 + 0.01 * 6.9496235847473145
Epoch 130, val loss: 1.3836203813552856
Epoch 140, training loss: 1.2775633335113525 = 1.2083525657653809 + 0.01 * 6.921082496643066
Epoch 140, val loss: 1.3348504304885864
Epoch 150, training loss: 1.2045010328292847 = 1.135565996170044 + 0.01 * 6.893501281738281
Epoch 150, val loss: 1.2910157442092896
Epoch 160, training loss: 1.1343480348587036 = 1.0656741857528687 + 0.01 * 6.867384433746338
Epoch 160, val loss: 1.249123215675354
Epoch 170, training loss: 1.0661684274673462 = 0.9976451992988586 + 0.01 * 6.852327346801758
Epoch 170, val loss: 1.2081022262573242
Epoch 180, training loss: 0.9996455907821655 = 0.9311786890029907 + 0.01 * 6.8466925621032715
Epoch 180, val loss: 1.1673849821090698
Epoch 190, training loss: 0.934314489364624 = 0.865851640701294 + 0.01 * 6.846282482147217
Epoch 190, val loss: 1.126597285270691
Epoch 200, training loss: 0.8697448372840881 = 0.8012690544128418 + 0.01 * 6.84757661819458
Epoch 200, val loss: 1.0861706733703613
Epoch 210, training loss: 0.806219756603241 = 0.737727701663971 + 0.01 * 6.849205493927002
Epoch 210, val loss: 1.0470274686813354
Epoch 220, training loss: 0.7445704936981201 = 0.6760604977607727 + 0.01 * 6.851001739501953
Epoch 220, val loss: 1.0109567642211914
Epoch 230, training loss: 0.6854816675186157 = 0.616953432559967 + 0.01 * 6.852820873260498
Epoch 230, val loss: 0.9787084460258484
Epoch 240, training loss: 0.628998875617981 = 0.5604548454284668 + 0.01 * 6.854401588439941
Epoch 240, val loss: 0.949969470500946
Epoch 250, training loss: 0.5750545263290405 = 0.506499707698822 + 0.01 * 6.8554840087890625
Epoch 250, val loss: 0.9252777099609375
Epoch 260, training loss: 0.5235834121704102 = 0.4550224542617798 + 0.01 * 6.8560943603515625
Epoch 260, val loss: 0.9044924378395081
Epoch 270, training loss: 0.4743832051753998 = 0.40581682324409485 + 0.01 * 6.856639385223389
Epoch 270, val loss: 0.8872864842414856
Epoch 280, training loss: 0.4271862804889679 = 0.35861748456954956 + 0.01 * 6.8568806648254395
Epoch 280, val loss: 0.873515248298645
Epoch 290, training loss: 0.3821732997894287 = 0.3136013150215149 + 0.01 * 6.857199668884277
Epoch 290, val loss: 0.8626496195793152
Epoch 300, training loss: 0.3398454785346985 = 0.2712705135345459 + 0.01 * 6.8574957847595215
Epoch 300, val loss: 0.8547734618186951
Epoch 310, training loss: 0.3009100556373596 = 0.2323302924633026 + 0.01 * 6.857975959777832
Epoch 310, val loss: 0.8505339026451111
Epoch 320, training loss: 0.26604679226875305 = 0.1974557489156723 + 0.01 * 6.859104633331299
Epoch 320, val loss: 0.8502814769744873
Epoch 330, training loss: 0.23560747504234314 = 0.167012020945549 + 0.01 * 6.8595452308654785
Epoch 330, val loss: 0.8546322584152222
Epoch 340, training loss: 0.2098325788974762 = 0.14123238623142242 + 0.01 * 6.860019207000732
Epoch 340, val loss: 0.8631311655044556
Epoch 350, training loss: 0.18858635425567627 = 0.11998399347066879 + 0.01 * 6.860235691070557
Epoch 350, val loss: 0.8754177689552307
Epoch 360, training loss: 0.17150044441223145 = 0.1028975397348404 + 0.01 * 6.860291481018066
Epoch 360, val loss: 0.8904147148132324
Epoch 370, training loss: 0.15771488845348358 = 0.08910586684942245 + 0.01 * 6.860902309417725
Epoch 370, val loss: 0.9072926044464111
Epoch 380, training loss: 0.14639896154403687 = 0.07779000699520111 + 0.01 * 6.860896110534668
Epoch 380, val loss: 0.9248610138893127
Epoch 390, training loss: 0.1369781196117401 = 0.06837505102157593 + 0.01 * 6.860306262969971
Epoch 390, val loss: 0.9422008991241455
Epoch 400, training loss: 0.1290491819381714 = 0.06045418977737427 + 0.01 * 6.859499931335449
Epoch 400, val loss: 0.9589775800704956
Epoch 410, training loss: 0.12231291830539703 = 0.05372653529047966 + 0.01 * 6.858638763427734
Epoch 410, val loss: 0.9752130508422852
Epoch 420, training loss: 0.11654350161552429 = 0.047968145459890366 + 0.01 * 6.857536315917969
Epoch 420, val loss: 0.9909716844558716
Epoch 430, training loss: 0.11157271265983582 = 0.04300915077328682 + 0.01 * 6.856356143951416
Epoch 430, val loss: 1.0063161849975586
Epoch 440, training loss: 0.10726930946111679 = 0.03871914744377136 + 0.01 * 6.855016708374023
Epoch 440, val loss: 1.0212982892990112
Epoch 450, training loss: 0.10353223979473114 = 0.03499208018183708 + 0.01 * 6.854015827178955
Epoch 450, val loss: 1.036038875579834
Epoch 460, training loss: 0.100255087018013 = 0.0317409448325634 + 0.01 * 6.851414680480957
Epoch 460, val loss: 1.0505527257919312
Epoch 470, training loss: 0.097380630671978 = 0.02889745682477951 + 0.01 * 6.848317623138428
Epoch 470, val loss: 1.0647720098495483
Epoch 480, training loss: 0.0948619470000267 = 0.02639812044799328 + 0.01 * 6.8463826179504395
Epoch 480, val loss: 1.0787602663040161
Epoch 490, training loss: 0.09262582659721375 = 0.02419445663690567 + 0.01 * 6.843136787414551
Epoch 490, val loss: 1.092494010925293
Epoch 500, training loss: 0.09067972004413605 = 0.022243939340114594 + 0.01 * 6.843578338623047
Epoch 500, val loss: 1.1059002876281738
Epoch 510, training loss: 0.088882215321064 = 0.020513055846095085 + 0.01 * 6.836915969848633
Epoch 510, val loss: 1.1188814640045166
Epoch 520, training loss: 0.08727802336215973 = 0.018972104415297508 + 0.01 * 6.830592155456543
Epoch 520, val loss: 1.1315205097198486
Epoch 530, training loss: 0.0858832448720932 = 0.017595741897821426 + 0.01 * 6.828750133514404
Epoch 530, val loss: 1.1438357830047607
Epoch 540, training loss: 0.08456598222255707 = 0.01636294648051262 + 0.01 * 6.820303440093994
Epoch 540, val loss: 1.1558523178100586
Epoch 550, training loss: 0.08360280096530914 = 0.015255584381520748 + 0.01 * 6.834721565246582
Epoch 550, val loss: 1.1674747467041016
Epoch 560, training loss: 0.0824032872915268 = 0.014258728362619877 + 0.01 * 6.814455986022949
Epoch 560, val loss: 1.178830623626709
Epoch 570, training loss: 0.081356942653656 = 0.013358482159674168 + 0.01 * 6.799846649169922
Epoch 570, val loss: 1.1897705793380737
Epoch 580, training loss: 0.08065135031938553 = 0.01254198607057333 + 0.01 * 6.810936450958252
Epoch 580, val loss: 1.2003995180130005
Epoch 590, training loss: 0.07966208457946777 = 0.01179968100041151 + 0.01 * 6.786240577697754
Epoch 590, val loss: 1.2107545137405396
Epoch 600, training loss: 0.07954154163599014 = 0.011122631840407848 + 0.01 * 6.841891288757324
Epoch 600, val loss: 1.2207096815109253
Epoch 610, training loss: 0.07828042656183243 = 0.010505150072276592 + 0.01 * 6.777527809143066
Epoch 610, val loss: 1.2305786609649658
Epoch 620, training loss: 0.07758329808712006 = 0.009939863346517086 + 0.01 * 6.764343738555908
Epoch 620, val loss: 1.2399717569351196
Epoch 630, training loss: 0.07700110971927643 = 0.009420132264494896 + 0.01 * 6.7580976486206055
Epoch 630, val loss: 1.2492698431015015
Epoch 640, training loss: 0.0765780508518219 = 0.00894169695675373 + 0.01 * 6.763635635375977
Epoch 640, val loss: 1.2583093643188477
Epoch 650, training loss: 0.07590486109256744 = 0.008501620031893253 + 0.01 * 6.7403244972229
Epoch 650, val loss: 1.266892671585083
Epoch 660, training loss: 0.07532013207674026 = 0.008095288649201393 + 0.01 * 6.722484588623047
Epoch 660, val loss: 1.2753459215164185
Epoch 670, training loss: 0.07511677592992783 = 0.007718884386122227 + 0.01 * 6.7397894859313965
Epoch 670, val loss: 1.283690094947815
Epoch 680, training loss: 0.07441012561321259 = 0.007371236104518175 + 0.01 * 6.703888893127441
Epoch 680, val loss: 1.2916346788406372
Epoch 690, training loss: 0.07408317923545837 = 0.007048892788589001 + 0.01 * 6.703428745269775
Epoch 690, val loss: 1.2993983030319214
Epoch 700, training loss: 0.0738588273525238 = 0.006748970597982407 + 0.01 * 6.71098518371582
Epoch 700, val loss: 1.307019591331482
Epoch 710, training loss: 0.0733582079410553 = 0.006469537038356066 + 0.01 * 6.68886661529541
Epoch 710, val loss: 1.3143939971923828
Epoch 720, training loss: 0.07294020056724548 = 0.0062091597355902195 + 0.01 * 6.673104286193848
Epoch 720, val loss: 1.3216556310653687
Epoch 730, training loss: 0.07310888916254044 = 0.005966068245470524 + 0.01 * 6.714282512664795
Epoch 730, val loss: 1.3286665678024292
Epoch 740, training loss: 0.07222749292850494 = 0.0057392544113099575 + 0.01 * 6.6488237380981445
Epoch 740, val loss: 1.3356120586395264
Epoch 750, training loss: 0.07220681011676788 = 0.005526743829250336 + 0.01 * 6.668006420135498
Epoch 750, val loss: 1.3424040079116821
Epoch 760, training loss: 0.07165646553039551 = 0.005327337421476841 + 0.01 * 6.632913112640381
Epoch 760, val loss: 1.3488894701004028
Epoch 770, training loss: 0.0720440074801445 = 0.005139994900673628 + 0.01 * 6.690402030944824
Epoch 770, val loss: 1.3552500009536743
Epoch 780, training loss: 0.0711754858493805 = 0.004964284133166075 + 0.01 * 6.621120452880859
Epoch 780, val loss: 1.361550211906433
Epoch 790, training loss: 0.07101557403802872 = 0.0047986372373998165 + 0.01 * 6.6216936111450195
Epoch 790, val loss: 1.3676750659942627
Epoch 800, training loss: 0.0708724856376648 = 0.004642517771571875 + 0.01 * 6.622996807098389
Epoch 800, val loss: 1.3735837936401367
Epoch 810, training loss: 0.0707053691148758 = 0.004495375789701939 + 0.01 * 6.620999336242676
Epoch 810, val loss: 1.3794596195220947
Epoch 820, training loss: 0.0704713687300682 = 0.004356181714683771 + 0.01 * 6.611518383026123
Epoch 820, val loss: 1.3850620985031128
Epoch 830, training loss: 0.07005969434976578 = 0.004224613308906555 + 0.01 * 6.583508491516113
Epoch 830, val loss: 1.3906478881835938
Epoch 840, training loss: 0.07009534537792206 = 0.004100028891116381 + 0.01 * 6.599531650543213
Epoch 840, val loss: 1.396028995513916
Epoch 850, training loss: 0.06991428136825562 = 0.003982197493314743 + 0.01 * 6.593209266662598
Epoch 850, val loss: 1.4014619588851929
Epoch 860, training loss: 0.06961886584758759 = 0.0038703661412000656 + 0.01 * 6.574849605560303
Epoch 860, val loss: 1.4064970016479492
Epoch 870, training loss: 0.06947868317365646 = 0.003763972083106637 + 0.01 * 6.571470737457275
Epoch 870, val loss: 1.4115692377090454
Epoch 880, training loss: 0.06942349672317505 = 0.003663063980638981 + 0.01 * 6.576043605804443
Epoch 880, val loss: 1.4165929555892944
Epoch 890, training loss: 0.06916080415248871 = 0.0035669482313096523 + 0.01 * 6.559385299682617
Epoch 890, val loss: 1.4213910102844238
Epoch 900, training loss: 0.06892003864049911 = 0.0034755757078528404 + 0.01 * 6.5444464683532715
Epoch 900, val loss: 1.4261095523834229
Epoch 910, training loss: 0.06878024339675903 = 0.0033882921561598778 + 0.01 * 6.539195537567139
Epoch 910, val loss: 1.4308841228485107
Epoch 920, training loss: 0.06867551803588867 = 0.003304919693619013 + 0.01 * 6.537059783935547
Epoch 920, val loss: 1.4353190660476685
Epoch 930, training loss: 0.06859229505062103 = 0.003225484397262335 + 0.01 * 6.536681652069092
Epoch 930, val loss: 1.4398871660232544
Epoch 940, training loss: 0.06856321543455124 = 0.0031494353897869587 + 0.01 * 6.541378021240234
Epoch 940, val loss: 1.4442058801651
Epoch 950, training loss: 0.06836544722318649 = 0.003076668595895171 + 0.01 * 6.528878211975098
Epoch 950, val loss: 1.4485394954681396
Epoch 960, training loss: 0.0681987851858139 = 0.0030069956555962563 + 0.01 * 6.519179344177246
Epoch 960, val loss: 1.452719807624817
Epoch 970, training loss: 0.06852232664823532 = 0.002940322272479534 + 0.01 * 6.558200836181641
Epoch 970, val loss: 1.4569233655929565
Epoch 980, training loss: 0.06785367429256439 = 0.0028763422742486 + 0.01 * 6.497733116149902
Epoch 980, val loss: 1.4609568119049072
Epoch 990, training loss: 0.06792101263999939 = 0.0028148519340902567 + 0.01 * 6.510616302490234
Epoch 990, val loss: 1.4650683403015137
Epoch 1000, training loss: 0.06785940378904343 = 0.002755789551883936 + 0.01 * 6.510361671447754
Epoch 1000, val loss: 1.4688178300857544
Epoch 1010, training loss: 0.06771377474069595 = 0.0026993295177817345 + 0.01 * 6.5014448165893555
Epoch 1010, val loss: 1.472772240638733
Epoch 1020, training loss: 0.06752020120620728 = 0.0026449321303516626 + 0.01 * 6.487526893615723
Epoch 1020, val loss: 1.476418137550354
Epoch 1030, training loss: 0.06742867827415466 = 0.002592683071270585 + 0.01 * 6.483599662780762
Epoch 1030, val loss: 1.4801640510559082
Epoch 1040, training loss: 0.06733496487140656 = 0.0025422850158065557 + 0.01 * 6.4792680740356445
Epoch 1040, val loss: 1.483748197555542
Epoch 1050, training loss: 0.06748288869857788 = 0.0024937852285802364 + 0.01 * 6.498910427093506
Epoch 1050, val loss: 1.487313985824585
Epoch 1060, training loss: 0.06730329990386963 = 0.002447097562253475 + 0.01 * 6.485620498657227
Epoch 1060, val loss: 1.490851640701294
Epoch 1070, training loss: 0.06735584139823914 = 0.0024021402932703495 + 0.01 * 6.495369911193848
Epoch 1070, val loss: 1.4942079782485962
Epoch 1080, training loss: 0.06707023084163666 = 0.0023586866445839405 + 0.01 * 6.47115421295166
Epoch 1080, val loss: 1.4975031614303589
Epoch 1090, training loss: 0.06713658571243286 = 0.0023167876061052084 + 0.01 * 6.481979846954346
Epoch 1090, val loss: 1.5007213354110718
Epoch 1100, training loss: 0.067024365067482 = 0.002276375889778137 + 0.01 * 6.474799156188965
Epoch 1100, val loss: 1.5040035247802734
Epoch 1110, training loss: 0.06708378344774246 = 0.002237421926110983 + 0.01 * 6.484636306762695
Epoch 1110, val loss: 1.5071524381637573
Epoch 1120, training loss: 0.06691227108240128 = 0.002199860755354166 + 0.01 * 6.471241474151611
Epoch 1120, val loss: 1.5103352069854736
Epoch 1130, training loss: 0.0666891410946846 = 0.0021634395234286785 + 0.01 * 6.45257043838501
Epoch 1130, val loss: 1.5133581161499023
Epoch 1140, training loss: 0.06678442656993866 = 0.0021282383240759373 + 0.01 * 6.4656195640563965
Epoch 1140, val loss: 1.5163053274154663
Epoch 1150, training loss: 0.06678711622953415 = 0.0020942767150700092 + 0.01 * 6.4692840576171875
Epoch 1150, val loss: 1.5193579196929932
Epoch 1160, training loss: 0.06673618406057358 = 0.0020613542292267084 + 0.01 * 6.4674835205078125
Epoch 1160, val loss: 1.5221539735794067
Epoch 1170, training loss: 0.06653542071580887 = 0.002029611961916089 + 0.01 * 6.450581073760986
Epoch 1170, val loss: 1.5250884294509888
Epoch 1180, training loss: 0.06643227487802505 = 0.001998837571591139 + 0.01 * 6.4433441162109375
Epoch 1180, val loss: 1.527892827987671
Epoch 1190, training loss: 0.0665213093161583 = 0.001968980301171541 + 0.01 * 6.455233097076416
Epoch 1190, val loss: 1.5305900573730469
Epoch 1200, training loss: 0.06650613248348236 = 0.0019400914898142219 + 0.01 * 6.45660400390625
Epoch 1200, val loss: 1.5333428382873535
Epoch 1210, training loss: 0.06631956249475479 = 0.0019120165379717946 + 0.01 * 6.440754413604736
Epoch 1210, val loss: 1.5359768867492676
Epoch 1220, training loss: 0.06643722951412201 = 0.0018849612679332495 + 0.01 * 6.455226898193359
Epoch 1220, val loss: 1.538640022277832
Epoch 1230, training loss: 0.0662757009267807 = 0.0018586560618132353 + 0.01 * 6.441704273223877
Epoch 1230, val loss: 1.5412005186080933
Epoch 1240, training loss: 0.06633736193180084 = 0.0018330710008740425 + 0.01 * 6.4504289627075195
Epoch 1240, val loss: 1.5436902046203613
Epoch 1250, training loss: 0.06623566895723343 = 0.0018082087626680732 + 0.01 * 6.442746162414551
Epoch 1250, val loss: 1.5461362600326538
Epoch 1260, training loss: 0.06615571677684784 = 0.001784133492037654 + 0.01 * 6.437159061431885
Epoch 1260, val loss: 1.5485668182373047
Epoch 1270, training loss: 0.06630482524633408 = 0.0017607384361326694 + 0.01 * 6.454409122467041
Epoch 1270, val loss: 1.550904631614685
Epoch 1280, training loss: 0.06600446254014969 = 0.0017381611978635192 + 0.01 * 6.42663049697876
Epoch 1280, val loss: 1.5533959865570068
Epoch 1290, training loss: 0.0659923329949379 = 0.0017160957213491201 + 0.01 * 6.427624225616455
Epoch 1290, val loss: 1.5556256771087646
Epoch 1300, training loss: 0.06609474867582321 = 0.0016946842661127448 + 0.01 * 6.440006256103516
Epoch 1300, val loss: 1.5578821897506714
Epoch 1310, training loss: 0.06596596539020538 = 0.0016739522106945515 + 0.01 * 6.429201602935791
Epoch 1310, val loss: 1.560125708580017
Epoch 1320, training loss: 0.06610362976789474 = 0.0016537121264263988 + 0.01 * 6.4449920654296875
Epoch 1320, val loss: 1.5623128414154053
Epoch 1330, training loss: 0.06585703045129776 = 0.0016340994043275714 + 0.01 * 6.422293663024902
Epoch 1330, val loss: 1.5644742250442505
Epoch 1340, training loss: 0.06610891222953796 = 0.0016149332514032722 + 0.01 * 6.449398040771484
Epoch 1340, val loss: 1.5666111707687378
Epoch 1350, training loss: 0.06580166518688202 = 0.0015963278710842133 + 0.01 * 6.420533657073975
Epoch 1350, val loss: 1.5687021017074585
Epoch 1360, training loss: 0.0658235028386116 = 0.001578295137733221 + 0.01 * 6.424520969390869
Epoch 1360, val loss: 1.5707552433013916
Epoch 1370, training loss: 0.06581895798444748 = 0.0015606608940288424 + 0.01 * 6.425829887390137
Epoch 1370, val loss: 1.5727773904800415
Epoch 1380, training loss: 0.06566809862852097 = 0.0015434703091159463 + 0.01 * 6.412463188171387
Epoch 1380, val loss: 1.574723720550537
Epoch 1390, training loss: 0.06566335260868073 = 0.0015268309507519007 + 0.01 * 6.413652420043945
Epoch 1390, val loss: 1.576659917831421
Epoch 1400, training loss: 0.06570564210414886 = 0.001510605332441628 + 0.01 * 6.419503688812256
Epoch 1400, val loss: 1.5786328315734863
Epoch 1410, training loss: 0.06560894101858139 = 0.0014947104500606656 + 0.01 * 6.411423206329346
Epoch 1410, val loss: 1.580501675605774
Epoch 1420, training loss: 0.06567369401454926 = 0.0014792877482250333 + 0.01 * 6.419440269470215
Epoch 1420, val loss: 1.5823020935058594
Epoch 1430, training loss: 0.06572677195072174 = 0.0014642409514635801 + 0.01 * 6.426253318786621
Epoch 1430, val loss: 1.5841987133026123
Epoch 1440, training loss: 0.06558001786470413 = 0.0014495450304821134 + 0.01 * 6.4130473136901855
Epoch 1440, val loss: 1.585991621017456
Epoch 1450, training loss: 0.06541286408901215 = 0.0014352566795423627 + 0.01 * 6.39776086807251
Epoch 1450, val loss: 1.5877388715744019
Epoch 1460, training loss: 0.06558290868997574 = 0.0014213218819350004 + 0.01 * 6.416158676147461
Epoch 1460, val loss: 1.589504599571228
Epoch 1470, training loss: 0.0652911514043808 = 0.001407732954248786 + 0.01 * 6.388341903686523
Epoch 1470, val loss: 1.5912201404571533
Epoch 1480, training loss: 0.06543227285146713 = 0.0013944362290203571 + 0.01 * 6.403783798217773
Epoch 1480, val loss: 1.592907190322876
Epoch 1490, training loss: 0.06561468541622162 = 0.0013815948041155934 + 0.01 * 6.423308849334717
Epoch 1490, val loss: 1.5946588516235352
Epoch 1500, training loss: 0.0653906762599945 = 0.0013688798062503338 + 0.01 * 6.402179718017578
Epoch 1500, val loss: 1.596198558807373
Epoch 1510, training loss: 0.0651986300945282 = 0.0013565714471042156 + 0.01 * 6.3842058181762695
Epoch 1510, val loss: 1.5978134870529175
Epoch 1520, training loss: 0.0651993453502655 = 0.001344567397609353 + 0.01 * 6.385477542877197
Epoch 1520, val loss: 1.5994818210601807
Epoch 1530, training loss: 0.06523937731981277 = 0.0013327820925042033 + 0.01 * 6.390659809112549
Epoch 1530, val loss: 1.6010375022888184
Epoch 1540, training loss: 0.06504923850297928 = 0.0013213125057518482 + 0.01 * 6.3727922439575195
Epoch 1540, val loss: 1.6025758981704712
Epoch 1550, training loss: 0.06516239047050476 = 0.0013100880896672606 + 0.01 * 6.385230541229248
Epoch 1550, val loss: 1.6041463613510132
Epoch 1560, training loss: 0.06503987312316895 = 0.0012991196708753705 + 0.01 * 6.374075412750244
Epoch 1560, val loss: 1.6056878566741943
Epoch 1570, training loss: 0.06501207500696182 = 0.0012884155148640275 + 0.01 * 6.372366428375244
Epoch 1570, val loss: 1.6071494817733765
Epoch 1580, training loss: 0.06489170342683792 = 0.0012779387179762125 + 0.01 * 6.361376762390137
Epoch 1580, val loss: 1.608643889427185
Epoch 1590, training loss: 0.06516709923744202 = 0.0012677052291110158 + 0.01 * 6.389938831329346
Epoch 1590, val loss: 1.6100910902023315
Epoch 1600, training loss: 0.06499697268009186 = 0.0012577584711834788 + 0.01 * 6.373921871185303
Epoch 1600, val loss: 1.6115378141403198
Epoch 1610, training loss: 0.06501498818397522 = 0.001247965614311397 + 0.01 * 6.376702308654785
Epoch 1610, val loss: 1.6129039525985718
Epoch 1620, training loss: 0.06489759683609009 = 0.0012384075671434402 + 0.01 * 6.36591911315918
Epoch 1620, val loss: 1.6143643856048584
Epoch 1630, training loss: 0.06480301916599274 = 0.0012289680307731032 + 0.01 * 6.357405662536621
Epoch 1630, val loss: 1.6156550645828247
Epoch 1640, training loss: 0.06506891548633575 = 0.0012198026524856687 + 0.01 * 6.38491153717041
Epoch 1640, val loss: 1.6169986724853516
Epoch 1650, training loss: 0.06466921418905258 = 0.0012108016526326537 + 0.01 * 6.345841407775879
Epoch 1650, val loss: 1.6184245347976685
Epoch 1660, training loss: 0.0647355243563652 = 0.001201984123326838 + 0.01 * 6.353353977203369
Epoch 1660, val loss: 1.619666337966919
Epoch 1670, training loss: 0.0646582692861557 = 0.001193421077914536 + 0.01 * 6.346485137939453
Epoch 1670, val loss: 1.6210439205169678
Epoch 1680, training loss: 0.06461168825626373 = 0.0011849557049572468 + 0.01 * 6.342673301696777
Epoch 1680, val loss: 1.6222714185714722
Epoch 1690, training loss: 0.06489132344722748 = 0.0011766828829422593 + 0.01 * 6.371464252471924
Epoch 1690, val loss: 1.6235034465789795
Epoch 1700, training loss: 0.0648765042424202 = 0.0011686282232403755 + 0.01 * 6.370787143707275
Epoch 1700, val loss: 1.624830961227417
Epoch 1710, training loss: 0.06456422060728073 = 0.001160621177405119 + 0.01 * 6.340360164642334
Epoch 1710, val loss: 1.6260356903076172
Epoch 1720, training loss: 0.06462869048118591 = 0.0011528409086167812 + 0.01 * 6.3475847244262695
Epoch 1720, val loss: 1.627261996269226
Epoch 1730, training loss: 0.0646950751543045 = 0.001145151793025434 + 0.01 * 6.354991912841797
Epoch 1730, val loss: 1.6284689903259277
Epoch 1740, training loss: 0.0646824985742569 = 0.0011376297334209085 + 0.01 * 6.354487419128418
Epoch 1740, val loss: 1.6296803951263428
Epoch 1750, training loss: 0.0644800215959549 = 0.001130207092501223 + 0.01 * 6.334981441497803
Epoch 1750, val loss: 1.6307233572006226
Epoch 1760, training loss: 0.06484304368495941 = 0.001122996094636619 + 0.01 * 6.372004985809326
Epoch 1760, val loss: 1.6319504976272583
Epoch 1770, training loss: 0.06432489305734634 = 0.0011159644927829504 + 0.01 * 6.320892810821533
Epoch 1770, val loss: 1.633048415184021
Epoch 1780, training loss: 0.06428314745426178 = 0.0011090029729530215 + 0.01 * 6.3174147605896
Epoch 1780, val loss: 1.6341816186904907
Epoch 1790, training loss: 0.0646030604839325 = 0.0011022002436220646 + 0.01 * 6.350086212158203
Epoch 1790, val loss: 1.6352506875991821
Epoch 1800, training loss: 0.06439268589019775 = 0.0010954781901091337 + 0.01 * 6.329720973968506
Epoch 1800, val loss: 1.6362971067428589
Epoch 1810, training loss: 0.06470925360918045 = 0.0010889180703088641 + 0.01 * 6.362033843994141
Epoch 1810, val loss: 1.637403964996338
Epoch 1820, training loss: 0.06436429917812347 = 0.0010825044009834528 + 0.01 * 6.328179836273193
Epoch 1820, val loss: 1.6384526491165161
Epoch 1830, training loss: 0.06435941904783249 = 0.001076134038157761 + 0.01 * 6.328328609466553
Epoch 1830, val loss: 1.639503002166748
Epoch 1840, training loss: 0.06422719359397888 = 0.001069876248948276 + 0.01 * 6.315732002258301
Epoch 1840, val loss: 1.6405781507492065
Epoch 1850, training loss: 0.06432357430458069 = 0.0010637224186211824 + 0.01 * 6.325985908508301
Epoch 1850, val loss: 1.641608476638794
Epoch 1860, training loss: 0.0641527995467186 = 0.0010576625354588032 + 0.01 * 6.309513568878174
Epoch 1860, val loss: 1.6425524950027466
Epoch 1870, training loss: 0.06413944810628891 = 0.0010517430491745472 + 0.01 * 6.308770656585693
Epoch 1870, val loss: 1.64357328414917
Epoch 1880, training loss: 0.06425918638706207 = 0.0010459577897563577 + 0.01 * 6.321322917938232
Epoch 1880, val loss: 1.6445820331573486
Epoch 1890, training loss: 0.06393714249134064 = 0.0010402014013379812 + 0.01 * 6.289694786071777
Epoch 1890, val loss: 1.6454874277114868
Epoch 1900, training loss: 0.06411142647266388 = 0.0010345308110117912 + 0.01 * 6.307690143585205
Epoch 1900, val loss: 1.646419644355774
Epoch 1910, training loss: 0.06392284482717514 = 0.0010290195932611823 + 0.01 * 6.289382457733154
Epoch 1910, val loss: 1.6474000215530396
Epoch 1920, training loss: 0.0640290379524231 = 0.0010235571535304189 + 0.01 * 6.3005475997924805
Epoch 1920, val loss: 1.64828622341156
Epoch 1930, training loss: 0.06420594453811646 = 0.0010181597899645567 + 0.01 * 6.318778991699219
Epoch 1930, val loss: 1.649227499961853
Epoch 1940, training loss: 0.0638163760304451 = 0.001012878492474556 + 0.01 * 6.280350208282471
Epoch 1940, val loss: 1.6501497030258179
Epoch 1950, training loss: 0.06400635838508606 = 0.0010077045299112797 + 0.01 * 6.299865245819092
Epoch 1950, val loss: 1.651005506515503
Epoch 1960, training loss: 0.06419704854488373 = 0.0010025798110291362 + 0.01 * 6.319447040557861
Epoch 1960, val loss: 1.652004361152649
Epoch 1970, training loss: 0.06400244683027267 = 0.0009974987478926778 + 0.01 * 6.300495147705078
Epoch 1970, val loss: 1.6528455018997192
Epoch 1980, training loss: 0.06382589042186737 = 0.0009924956830218434 + 0.01 * 6.283339023590088
Epoch 1980, val loss: 1.65373957157135
Epoch 1990, training loss: 0.06372792273759842 = 0.0009875473333522677 + 0.01 * 6.274038314819336
Epoch 1990, val loss: 1.6545302867889404
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.7823
Flip ASR: 0.7422/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0191431045532227 = 1.935403823852539 + 0.01 * 8.373916625976562
Epoch 0, val loss: 1.9348939657211304
Epoch 10, training loss: 2.00974178314209 = 1.9260029792785645 + 0.01 * 8.37387752532959
Epoch 10, val loss: 1.9262715578079224
Epoch 20, training loss: 1.998256802558899 = 1.9145199060440063 + 0.01 * 8.373695373535156
Epoch 20, val loss: 1.9154365062713623
Epoch 30, training loss: 1.9821535348892212 = 1.8984224796295166 + 0.01 * 8.373103141784668
Epoch 30, val loss: 1.9000126123428345
Epoch 40, training loss: 1.9583557844161987 = 1.8746546506881714 + 0.01 * 8.370110511779785
Epoch 40, val loss: 1.8773655891418457
Epoch 50, training loss: 1.9243179559707642 = 1.8408169746398926 + 0.01 * 8.350098609924316
Epoch 50, val loss: 1.84628427028656
Epoch 60, training loss: 1.8827550411224365 = 1.800176739692688 + 0.01 * 8.257832527160645
Epoch 60, val loss: 1.8117092847824097
Epoch 70, training loss: 1.838649034500122 = 1.7599842548370361 + 0.01 * 7.866479873657227
Epoch 70, val loss: 1.7781498432159424
Epoch 80, training loss: 1.7866045236587524 = 1.709441900253296 + 0.01 * 7.716258525848389
Epoch 80, val loss: 1.7314493656158447
Epoch 90, training loss: 1.7145429849624634 = 1.639293909072876 + 0.01 * 7.524904251098633
Epoch 90, val loss: 1.6685911417007446
Epoch 100, training loss: 1.620707631111145 = 1.547448754310608 + 0.01 * 7.32588529586792
Epoch 100, val loss: 1.590699315071106
Epoch 110, training loss: 1.5099166631698608 = 1.438386082649231 + 0.01 * 7.153058052062988
Epoch 110, val loss: 1.4981410503387451
Epoch 120, training loss: 1.3911712169647217 = 1.3203892707824707 + 0.01 * 7.07819938659668
Epoch 120, val loss: 1.3989284038543701
Epoch 130, training loss: 1.2704662084579468 = 1.2001203298568726 + 0.01 * 7.03458309173584
Epoch 130, val loss: 1.2992826700210571
Epoch 140, training loss: 1.1535401344299316 = 1.0834825038909912 + 0.01 * 7.005762577056885
Epoch 140, val loss: 1.2046470642089844
Epoch 150, training loss: 1.044805645942688 = 0.9749777317047119 + 0.01 * 6.982795238494873
Epoch 150, val loss: 1.1170719861984253
Epoch 160, training loss: 0.9463738799095154 = 0.8767520785331726 + 0.01 * 6.962177753448486
Epoch 160, val loss: 1.0376940965652466
Epoch 170, training loss: 0.8583389520645142 = 0.7889415621757507 + 0.01 * 6.939738750457764
Epoch 170, val loss: 0.9670846462249756
Epoch 180, training loss: 0.7801918983459473 = 0.7110233902931213 + 0.01 * 6.916851043701172
Epoch 180, val loss: 0.9056551456451416
Epoch 190, training loss: 0.7109899520874023 = 0.642012894153595 + 0.01 * 6.897707462310791
Epoch 190, val loss: 0.8543477058410645
Epoch 200, training loss: 0.6493033766746521 = 0.5804363489151001 + 0.01 * 6.886700630187988
Epoch 200, val loss: 0.8127200603485107
Epoch 210, training loss: 0.5935021042823792 = 0.52467942237854 + 0.01 * 6.882268905639648
Epoch 210, val loss: 0.7794007658958435
Epoch 220, training loss: 0.5423760414123535 = 0.4735645651817322 + 0.01 * 6.88114595413208
Epoch 220, val loss: 0.7529751658439636
Epoch 230, training loss: 0.4953063726425171 = 0.42651116847991943 + 0.01 * 6.879518985748291
Epoch 230, val loss: 0.7320324182510376
Epoch 240, training loss: 0.4521905779838562 = 0.3834051191806793 + 0.01 * 6.878544330596924
Epoch 240, val loss: 0.7158138155937195
Epoch 250, training loss: 0.41300952434539795 = 0.3442334234714508 + 0.01 * 6.8776116371154785
Epoch 250, val loss: 0.7035560607910156
Epoch 260, training loss: 0.37752747535705566 = 0.30876028537750244 + 0.01 * 6.8767194747924805
Epoch 260, val loss: 0.6946238279342651
Epoch 270, training loss: 0.34512823820114136 = 0.27636924386024475 + 0.01 * 6.875899791717529
Epoch 270, val loss: 0.6882522106170654
Epoch 280, training loss: 0.31498825550079346 = 0.24623596668243408 + 0.01 * 6.875229835510254
Epoch 280, val loss: 0.6837810277938843
Epoch 290, training loss: 0.286399245262146 = 0.2176515907049179 + 0.01 * 6.874764442443848
Epoch 290, val loss: 0.6806666254997253
Epoch 300, training loss: 0.259084016084671 = 0.19033652544021606 + 0.01 * 6.874749183654785
Epoch 300, val loss: 0.6788027882575989
Epoch 310, training loss: 0.23335568606853485 = 0.16461116075515747 + 0.01 * 6.874452590942383
Epoch 310, val loss: 0.6782935857772827
Epoch 320, training loss: 0.20997029542922974 = 0.141225203871727 + 0.01 * 6.874508380889893
Epoch 320, val loss: 0.6796959638595581
Epoch 330, training loss: 0.1895623654127121 = 0.12081684172153473 + 0.01 * 6.8745527267456055
Epoch 330, val loss: 0.6829952001571655
Epoch 340, training loss: 0.1723019778728485 = 0.10355480760335922 + 0.01 * 6.874717712402344
Epoch 340, val loss: 0.6881038546562195
Epoch 350, training loss: 0.15794578194618225 = 0.0891982913017273 + 0.01 * 6.874748229980469
Epoch 350, val loss: 0.6947313547134399
Epoch 360, training loss: 0.14606887102127075 = 0.07732223719358444 + 0.01 * 6.87466287612915
Epoch 360, val loss: 0.7025324106216431
Epoch 370, training loss: 0.13622185587882996 = 0.06748034060001373 + 0.01 * 6.87415075302124
Epoch 370, val loss: 0.7111141681671143
Epoch 380, training loss: 0.12801514565944672 = 0.059280212968587875 + 0.01 * 6.8734941482543945
Epoch 380, val loss: 0.7202164530754089
Epoch 390, training loss: 0.1211254894733429 = 0.05239970609545708 + 0.01 * 6.8725786209106445
Epoch 390, val loss: 0.7296491265296936
Epoch 400, training loss: 0.11531363427639008 = 0.046586427837610245 + 0.01 * 6.872721195220947
Epoch 400, val loss: 0.7392796874046326
Epoch 410, training loss: 0.11034329980611801 = 0.04164166748523712 + 0.01 * 6.870163440704346
Epoch 410, val loss: 0.7490226626396179
Epoch 420, training loss: 0.10609420388936996 = 0.037409380078315735 + 0.01 * 6.86848258972168
Epoch 420, val loss: 0.7587934136390686
Epoch 430, training loss: 0.10243315994739532 = 0.03376603499054909 + 0.01 * 6.866713047027588
Epoch 430, val loss: 0.7685145735740662
Epoch 440, training loss: 0.09926223009824753 = 0.030612431466579437 + 0.01 * 6.864980220794678
Epoch 440, val loss: 0.7781767249107361
Epoch 450, training loss: 0.09649106860160828 = 0.027869505807757378 + 0.01 * 6.862156391143799
Epoch 450, val loss: 0.7877404689788818
Epoch 460, training loss: 0.09408392757177353 = 0.02547186054289341 + 0.01 * 6.861207008361816
Epoch 460, val loss: 0.7971795797348022
Epoch 470, training loss: 0.0919308066368103 = 0.023366156965494156 + 0.01 * 6.8564653396606445
Epoch 470, val loss: 0.8064849376678467
Epoch 480, training loss: 0.09003859758377075 = 0.02150879055261612 + 0.01 * 6.852980613708496
Epoch 480, val loss: 0.8156086802482605
Epoch 490, training loss: 0.08836960047483444 = 0.019863342866301537 + 0.01 * 6.850625991821289
Epoch 490, val loss: 0.8245667219161987
Epoch 500, training loss: 0.08688531070947647 = 0.018400022760033607 + 0.01 * 6.84852933883667
Epoch 500, val loss: 0.8333374857902527
Epoch 510, training loss: 0.08550592511892319 = 0.017093941569328308 + 0.01 * 6.841198444366455
Epoch 510, val loss: 0.841891884803772
Epoch 520, training loss: 0.08428755402565002 = 0.015923768281936646 + 0.01 * 6.836379051208496
Epoch 520, val loss: 0.8502604961395264
Epoch 530, training loss: 0.08323442190885544 = 0.01487179659307003 + 0.01 * 6.8362627029418945
Epoch 530, val loss: 0.8584231734275818
Epoch 540, training loss: 0.08217396587133408 = 0.01392309833317995 + 0.01 * 6.82508659362793
Epoch 540, val loss: 0.8664053678512573
Epoch 550, training loss: 0.08125199377536774 = 0.013064739294350147 + 0.01 * 6.818725109100342
Epoch 550, val loss: 0.8742127418518066
Epoch 560, training loss: 0.08039114624261856 = 0.012286053970456123 + 0.01 * 6.81050968170166
Epoch 560, val loss: 0.8818262815475464
Epoch 570, training loss: 0.07957085222005844 = 0.011577830649912357 + 0.01 * 6.799302101135254
Epoch 570, val loss: 0.889210045337677
Epoch 580, training loss: 0.07936877012252808 = 0.010932158678770065 + 0.01 * 6.843661785125732
Epoch 580, val loss: 0.8964186906814575
Epoch 590, training loss: 0.0781785249710083 = 0.010342545807361603 + 0.01 * 6.78359842300415
Epoch 590, val loss: 0.9034955501556396
Epoch 600, training loss: 0.07758137583732605 = 0.009802722372114658 + 0.01 * 6.777865409851074
Epoch 600, val loss: 0.9103050231933594
Epoch 610, training loss: 0.07704313099384308 = 0.009306744672358036 + 0.01 * 6.77363920211792
Epoch 610, val loss: 0.9169995188713074
Epoch 620, training loss: 0.07635092735290527 = 0.008849851787090302 + 0.01 * 6.750107765197754
Epoch 620, val loss: 0.9235134124755859
Epoch 630, training loss: 0.07624208927154541 = 0.008428579196333885 + 0.01 * 6.781350612640381
Epoch 630, val loss: 0.9298038482666016
Epoch 640, training loss: 0.07541218400001526 = 0.008040600456297398 + 0.01 * 6.73715877532959
Epoch 640, val loss: 0.9360191226005554
Epoch 650, training loss: 0.074911929666996 = 0.007681144401431084 + 0.01 * 6.72307825088501
Epoch 650, val loss: 0.942004382610321
Epoch 660, training loss: 0.0747668594121933 = 0.007347174920141697 + 0.01 * 6.741968631744385
Epoch 660, val loss: 0.9478410482406616
Epoch 670, training loss: 0.07421804964542389 = 0.0070363725535571575 + 0.01 * 6.718167781829834
Epoch 670, val loss: 0.9535939693450928
Epoch 680, training loss: 0.07393603771924973 = 0.006747114006429911 + 0.01 * 6.718892574310303
Epoch 680, val loss: 0.9591498374938965
Epoch 690, training loss: 0.07347994297742844 = 0.006477177143096924 + 0.01 * 6.700276851654053
Epoch 690, val loss: 0.9646281003952026
Epoch 700, training loss: 0.07309254258871078 = 0.006225028540939093 + 0.01 * 6.686751842498779
Epoch 700, val loss: 0.9699025750160217
Epoch 710, training loss: 0.07335910946130753 = 0.005988917779177427 + 0.01 * 6.737019062042236
Epoch 710, val loss: 0.97508704662323
Epoch 720, training loss: 0.07257860153913498 = 0.005768167320638895 + 0.01 * 6.68104362487793
Epoch 720, val loss: 0.9801973700523376
Epoch 730, training loss: 0.07225311547517776 = 0.005561043042689562 + 0.01 * 6.6692070960998535
Epoch 730, val loss: 0.9851153492927551
Epoch 740, training loss: 0.07193177193403244 = 0.005366040859371424 + 0.01 * 6.6565728187561035
Epoch 740, val loss: 0.9900019764900208
Epoch 750, training loss: 0.07179206609725952 = 0.005182183347642422 + 0.01 * 6.6609883308410645
Epoch 750, val loss: 0.9946624636650085
Epoch 760, training loss: 0.07166995108127594 = 0.00500955106690526 + 0.01 * 6.666039943695068
Epoch 760, val loss: 0.9994227886199951
Epoch 770, training loss: 0.07133349031209946 = 0.004846727941185236 + 0.01 * 6.64867639541626
Epoch 770, val loss: 1.003924012184143
Epoch 780, training loss: 0.07113969326019287 = 0.004692417569458485 + 0.01 * 6.64472770690918
Epoch 780, val loss: 1.0084006786346436
Epoch 790, training loss: 0.07138736546039581 = 0.0045463694259524345 + 0.01 * 6.6840996742248535
Epoch 790, val loss: 1.0127372741699219
Epoch 800, training loss: 0.07074134051799774 = 0.00440856721252203 + 0.01 * 6.633277416229248
Epoch 800, val loss: 1.0170435905456543
Epoch 810, training loss: 0.07060032337903976 = 0.004278102423995733 + 0.01 * 6.632222652435303
Epoch 810, val loss: 1.0212037563323975
Epoch 820, training loss: 0.07035018503665924 = 0.004154185764491558 + 0.01 * 6.619600296020508
Epoch 820, val loss: 1.0252857208251953
Epoch 830, training loss: 0.07023119926452637 = 0.004036527127027512 + 0.01 * 6.619466781616211
Epoch 830, val loss: 1.0292593240737915
Epoch 840, training loss: 0.07010874897241592 = 0.003924729768186808 + 0.01 * 6.618402481079102
Epoch 840, val loss: 1.033189058303833
Epoch 850, training loss: 0.0705244168639183 = 0.003818166209384799 + 0.01 * 6.670624732971191
Epoch 850, val loss: 1.0370322465896606
Epoch 860, training loss: 0.0697484239935875 = 0.0037171586882323027 + 0.01 * 6.603126049041748
Epoch 860, val loss: 1.0407606363296509
Epoch 870, training loss: 0.06970736384391785 = 0.003621030366048217 + 0.01 * 6.608633995056152
Epoch 870, val loss: 1.0444389581680298
Epoch 880, training loss: 0.069669708609581 = 0.0035290932282805443 + 0.01 * 6.6140618324279785
Epoch 880, val loss: 1.0480332374572754
Epoch 890, training loss: 0.06936383247375488 = 0.003441351233050227 + 0.01 * 6.592248439788818
Epoch 890, val loss: 1.0515583753585815
Epoch 900, training loss: 0.06918007135391235 = 0.003357655368745327 + 0.01 * 6.582242012023926
Epoch 900, val loss: 1.0550305843353271
Epoch 910, training loss: 0.0694868415594101 = 0.0032775462605059147 + 0.01 * 6.620929718017578
Epoch 910, val loss: 1.0583690404891968
Epoch 920, training loss: 0.06893755495548248 = 0.0032009845599532127 + 0.01 * 6.573657035827637
Epoch 920, val loss: 1.0617715120315552
Epoch 930, training loss: 0.06905634701251984 = 0.0031276748050004244 + 0.01 * 6.592867374420166
Epoch 930, val loss: 1.0649957656860352
Epoch 940, training loss: 0.0687870979309082 = 0.003057541325688362 + 0.01 * 6.572956085205078
Epoch 940, val loss: 1.0682487487792969
Epoch 950, training loss: 0.06868861615657806 = 0.0029902856331318617 + 0.01 * 6.569833755493164
Epoch 950, val loss: 1.071349024772644
Epoch 960, training loss: 0.06866434961557388 = 0.0029257815331220627 + 0.01 * 6.573856830596924
Epoch 960, val loss: 1.0744560956954956
Epoch 970, training loss: 0.06852110475301743 = 0.002863869536668062 + 0.01 * 6.565723896026611
Epoch 970, val loss: 1.0775014162063599
Epoch 980, training loss: 0.06857302039861679 = 0.002804489340633154 + 0.01 * 6.5768537521362305
Epoch 980, val loss: 1.0804585218429565
Epoch 990, training loss: 0.06837362796068192 = 0.0027473336085677147 + 0.01 * 6.562629222869873
Epoch 990, val loss: 1.0833791494369507
Epoch 1000, training loss: 0.06830232590436935 = 0.002692444948479533 + 0.01 * 6.560988426208496
Epoch 1000, val loss: 1.086287498474121
Epoch 1010, training loss: 0.06810547411441803 = 0.002639671554788947 + 0.01 * 6.546579837799072
Epoch 1010, val loss: 1.0890984535217285
Epoch 1020, training loss: 0.06807475537061691 = 0.002588813891634345 + 0.01 * 6.548593997955322
Epoch 1020, val loss: 1.0918686389923096
Epoch 1030, training loss: 0.06849212944507599 = 0.002539948560297489 + 0.01 * 6.595218658447266
Epoch 1030, val loss: 1.0945606231689453
Epoch 1040, training loss: 0.06795617192983627 = 0.002492969622835517 + 0.01 * 6.54632043838501
Epoch 1040, val loss: 1.0973026752471924
Epoch 1050, training loss: 0.06784771382808685 = 0.002447577891871333 + 0.01 * 6.540013790130615
Epoch 1050, val loss: 1.0998696088790894
Epoch 1060, training loss: 0.06769245862960815 = 0.0024038320407271385 + 0.01 * 6.528862476348877
Epoch 1060, val loss: 1.1024922132492065
Epoch 1070, training loss: 0.06772250682115555 = 0.0023615562822669744 + 0.01 * 6.536094665527344
Epoch 1070, val loss: 1.1049530506134033
Epoch 1080, training loss: 0.06749270111322403 = 0.002320816507562995 + 0.01 * 6.517188549041748
Epoch 1080, val loss: 1.1075108051300049
Epoch 1090, training loss: 0.06767983734607697 = 0.0022814623080193996 + 0.01 * 6.53983736038208
Epoch 1090, val loss: 1.1098895072937012
Epoch 1100, training loss: 0.06747093796730042 = 0.002243612427264452 + 0.01 * 6.522732734680176
Epoch 1100, val loss: 1.1123994588851929
Epoch 1110, training loss: 0.06734517216682434 = 0.0022068731486797333 + 0.01 * 6.513830184936523
Epoch 1110, val loss: 1.1147239208221436
Epoch 1120, training loss: 0.06742532551288605 = 0.0021713883616030216 + 0.01 * 6.525393962860107
Epoch 1120, val loss: 1.1169662475585938
Epoch 1130, training loss: 0.06733830273151398 = 0.002137154573574662 + 0.01 * 6.520114898681641
Epoch 1130, val loss: 1.119358777999878
Epoch 1140, training loss: 0.06745780259370804 = 0.002103968523442745 + 0.01 * 6.535383224487305
Epoch 1140, val loss: 1.1214934587478638
Epoch 1150, training loss: 0.06729381531476974 = 0.002071819268167019 + 0.01 * 6.522199630737305
Epoch 1150, val loss: 1.1238211393356323
Epoch 1160, training loss: 0.06718359142541885 = 0.0020407065749168396 + 0.01 * 6.514288425445557
Epoch 1160, val loss: 1.1259303092956543
Epoch 1170, training loss: 0.06703174859285355 = 0.0020105161238461733 + 0.01 * 6.5021233558654785
Epoch 1170, val loss: 1.1280690431594849
Epoch 1180, training loss: 0.06710711866617203 = 0.001981324516236782 + 0.01 * 6.512579441070557
Epoch 1180, val loss: 1.1301767826080322
Epoch 1190, training loss: 0.06713652610778809 = 0.0019529411802068353 + 0.01 * 6.518359184265137
Epoch 1190, val loss: 1.1322048902511597
Epoch 1200, training loss: 0.06706924736499786 = 0.0019255435327067971 + 0.01 * 6.514370441436768
Epoch 1200, val loss: 1.1342501640319824
Epoch 1210, training loss: 0.06680168956518173 = 0.001898861606605351 + 0.01 * 6.490283012390137
Epoch 1210, val loss: 1.1362656354904175
Epoch 1220, training loss: 0.06672054529190063 = 0.0018729964504018426 + 0.01 * 6.484755039215088
Epoch 1220, val loss: 1.1381489038467407
Epoch 1230, training loss: 0.0668734684586525 = 0.0018480243161320686 + 0.01 * 6.502543926239014
Epoch 1230, val loss: 1.1401870250701904
Epoch 1240, training loss: 0.0667399913072586 = 0.0018236386822536588 + 0.01 * 6.491634845733643
Epoch 1240, val loss: 1.1420871019363403
Epoch 1250, training loss: 0.06672277301549911 = 0.0018001337302848697 + 0.01 * 6.4922637939453125
Epoch 1250, val loss: 1.143798828125
Epoch 1260, training loss: 0.0665646642446518 = 0.001777365803718567 + 0.01 * 6.478730201721191
Epoch 1260, val loss: 1.1457878351211548
Epoch 1270, training loss: 0.06657581031322479 = 0.001755109173245728 + 0.01 * 6.482069969177246
Epoch 1270, val loss: 1.1475121974945068
Epoch 1280, training loss: 0.06651019304990768 = 0.0017334837466478348 + 0.01 * 6.477671146392822
Epoch 1280, val loss: 1.1492583751678467
Epoch 1290, training loss: 0.0662967786192894 = 0.0017124759033322334 + 0.01 * 6.45842981338501
Epoch 1290, val loss: 1.1510276794433594
Epoch 1300, training loss: 0.0664466843008995 = 0.0016920088091865182 + 0.01 * 6.475467681884766
Epoch 1300, val loss: 1.1526154279708862
Epoch 1310, training loss: 0.06630223989486694 = 0.0016722372965887189 + 0.01 * 6.463000297546387
Epoch 1310, val loss: 1.1544028520584106
Epoch 1320, training loss: 0.06630662083625793 = 0.0016529399435967207 + 0.01 * 6.465367794036865
Epoch 1320, val loss: 1.1559745073318481
Epoch 1330, training loss: 0.06638985127210617 = 0.0016342659946531057 + 0.01 * 6.475559234619141
Epoch 1330, val loss: 1.157704472541809
Epoch 1340, training loss: 0.06642048060894012 = 0.0016159508377313614 + 0.01 * 6.480453014373779
Epoch 1340, val loss: 1.1592165231704712
Epoch 1350, training loss: 0.06627342104911804 = 0.0015982069307938218 + 0.01 * 6.4675211906433105
Epoch 1350, val loss: 1.1608672142028809
Epoch 1360, training loss: 0.06596885621547699 = 0.0015808631433174014 + 0.01 * 6.4387993812561035
Epoch 1360, val loss: 1.162345051765442
Epoch 1370, training loss: 0.0662393569946289 = 0.001564083038829267 + 0.01 * 6.467527389526367
Epoch 1370, val loss: 1.1639313697814941
Epoch 1380, training loss: 0.06593287736177444 = 0.0015476333210244775 + 0.01 * 6.43852424621582
Epoch 1380, val loss: 1.165427327156067
Epoch 1390, training loss: 0.06594977527856827 = 0.0015315808122977614 + 0.01 * 6.441819667816162
Epoch 1390, val loss: 1.1668630838394165
Epoch 1400, training loss: 0.06646141409873962 = 0.001516033778898418 + 0.01 * 6.494537830352783
Epoch 1400, val loss: 1.1682734489440918
Epoch 1410, training loss: 0.06592343747615814 = 0.001500805257819593 + 0.01 * 6.442263126373291
Epoch 1410, val loss: 1.169786810874939
Epoch 1420, training loss: 0.06588409841060638 = 0.0014859965303912759 + 0.01 * 6.439810752868652
Epoch 1420, val loss: 1.1711986064910889
Epoch 1430, training loss: 0.06579191237688065 = 0.001471562311053276 + 0.01 * 6.432034969329834
Epoch 1430, val loss: 1.172560691833496
Epoch 1440, training loss: 0.06590745598077774 = 0.0014574897941201925 + 0.01 * 6.4449968338012695
Epoch 1440, val loss: 1.1739590167999268
Epoch 1450, training loss: 0.06576631218194962 = 0.0014436787459999323 + 0.01 * 6.432263374328613
Epoch 1450, val loss: 1.1753184795379639
Epoch 1460, training loss: 0.06581209599971771 = 0.0014302685158327222 + 0.01 * 6.438182830810547
Epoch 1460, val loss: 1.1766791343688965
Epoch 1470, training loss: 0.06566723436117172 = 0.0014171446673572063 + 0.01 * 6.425009250640869
Epoch 1470, val loss: 1.1779148578643799
Epoch 1480, training loss: 0.06592097878456116 = 0.0014043580740690231 + 0.01 * 6.451662063598633
Epoch 1480, val loss: 1.1792728900909424
Epoch 1490, training loss: 0.0654747486114502 = 0.0013917804462835193 + 0.01 * 6.408296585083008
Epoch 1490, val loss: 1.1805784702301025
Epoch 1500, training loss: 0.06530851125717163 = 0.0013795357663184404 + 0.01 * 6.392898082733154
Epoch 1500, val loss: 1.1817867755889893
Epoch 1510, training loss: 0.06530482321977615 = 0.0013676229864358902 + 0.01 * 6.3937201499938965
Epoch 1510, val loss: 1.1831072568893433
Epoch 1520, training loss: 0.06579474359750748 = 0.0013558861101046205 + 0.01 * 6.443885803222656
Epoch 1520, val loss: 1.1843804121017456
Epoch 1530, training loss: 0.06514670699834824 = 0.0013445044169202447 + 0.01 * 6.380220890045166
Epoch 1530, val loss: 1.1855717897415161
Epoch 1540, training loss: 0.0652342438697815 = 0.0013333511305972934 + 0.01 * 6.390089988708496
Epoch 1540, val loss: 1.1867631673812866
Epoch 1550, training loss: 0.06532751023769379 = 0.0013224203139543533 + 0.01 * 6.400509357452393
Epoch 1550, val loss: 1.187997579574585
Epoch 1560, training loss: 0.06516163796186447 = 0.0013116822810843587 + 0.01 * 6.384995460510254
Epoch 1560, val loss: 1.189202904701233
Epoch 1570, training loss: 0.0652785450220108 = 0.0013012299314141273 + 0.01 * 6.397731781005859
Epoch 1570, val loss: 1.1903557777404785
Epoch 1580, training loss: 0.06507721543312073 = 0.0012909849174320698 + 0.01 * 6.378623008728027
Epoch 1580, val loss: 1.191535234451294
Epoch 1590, training loss: 0.06495978683233261 = 0.001280896831303835 + 0.01 * 6.367889404296875
Epoch 1590, val loss: 1.1926929950714111
Epoch 1600, training loss: 0.06497412174940109 = 0.001271083252504468 + 0.01 * 6.370304107666016
Epoch 1600, val loss: 1.1938680410385132
Epoch 1610, training loss: 0.06522589176893234 = 0.001261443248949945 + 0.01 * 6.396444797515869
Epoch 1610, val loss: 1.1949641704559326
Epoch 1620, training loss: 0.06508583575487137 = 0.0012519570300355554 + 0.01 * 6.383387565612793
Epoch 1620, val loss: 1.1961458921432495
Epoch 1630, training loss: 0.06491666287183762 = 0.001242687227204442 + 0.01 * 6.367397308349609
Epoch 1630, val loss: 1.1972044706344604
Epoch 1640, training loss: 0.06485281139612198 = 0.0012336127692833543 + 0.01 * 6.36191987991333
Epoch 1640, val loss: 1.1983025074005127
Epoch 1650, training loss: 0.06493687629699707 = 0.0012246840633451939 + 0.01 * 6.371219635009766
Epoch 1650, val loss: 1.1994338035583496
Epoch 1660, training loss: 0.06477127224206924 = 0.001215946045704186 + 0.01 * 6.355532646179199
Epoch 1660, val loss: 1.2005102634429932
Epoch 1670, training loss: 0.06496641784906387 = 0.001207352033816278 + 0.01 * 6.375906467437744
Epoch 1670, val loss: 1.2015552520751953
Epoch 1680, training loss: 0.0649002343416214 = 0.0011989418417215347 + 0.01 * 6.370129585266113
Epoch 1680, val loss: 1.2026469707489014
Epoch 1690, training loss: 0.06471507996320724 = 0.0011906453873962164 + 0.01 * 6.352443695068359
Epoch 1690, val loss: 1.203675389289856
Epoch 1700, training loss: 0.06467261910438538 = 0.0011825226247310638 + 0.01 * 6.3490095138549805
Epoch 1700, val loss: 1.2047466039657593
Epoch 1710, training loss: 0.06484471261501312 = 0.0011745244264602661 + 0.01 * 6.367018699645996
Epoch 1710, val loss: 1.205753207206726
Epoch 1720, training loss: 0.06460777670145035 = 0.0011666869977489114 + 0.01 * 6.344109058380127
Epoch 1720, val loss: 1.2067804336547852
Epoch 1730, training loss: 0.06460222601890564 = 0.001158977742306888 + 0.01 * 6.344325542449951
Epoch 1730, val loss: 1.2077255249023438
Epoch 1740, training loss: 0.06473933160305023 = 0.0011514450889080763 + 0.01 * 6.358788967132568
Epoch 1740, val loss: 1.2088186740875244
Epoch 1750, training loss: 0.06455449014902115 = 0.0011439822847023606 + 0.01 * 6.341050624847412
Epoch 1750, val loss: 1.2097210884094238
Epoch 1760, training loss: 0.06475870311260223 = 0.0011366883991286159 + 0.01 * 6.36220121383667
Epoch 1760, val loss: 1.2107164859771729
Epoch 1770, training loss: 0.06458953768014908 = 0.0011295368894934654 + 0.01 * 6.3460001945495605
Epoch 1770, val loss: 1.2116576433181763
Epoch 1780, training loss: 0.06460931152105331 = 0.0011224698973819613 + 0.01 * 6.348684310913086
Epoch 1780, val loss: 1.2126405239105225
Epoch 1790, training loss: 0.06450769305229187 = 0.0011154671665281057 + 0.01 * 6.3392229080200195
Epoch 1790, val loss: 1.2136180400848389
Epoch 1800, training loss: 0.06428015977144241 = 0.0011086389422416687 + 0.01 * 6.317152500152588
Epoch 1800, val loss: 1.2145555019378662
Epoch 1810, training loss: 0.06428062170743942 = 0.0011018778895959258 + 0.01 * 6.317874431610107
Epoch 1810, val loss: 1.2153977155685425
Epoch 1820, training loss: 0.06458583474159241 = 0.0010952644515782595 + 0.01 * 6.349057197570801
Epoch 1820, val loss: 1.2163530588150024
Epoch 1830, training loss: 0.06433915346860886 = 0.0010887377429753542 + 0.01 * 6.325041770935059
Epoch 1830, val loss: 1.2172361612319946
Epoch 1840, training loss: 0.06458348035812378 = 0.001082349568605423 + 0.01 * 6.350113868713379
Epoch 1840, val loss: 1.2181591987609863
Epoch 1850, training loss: 0.06435678899288177 = 0.001076018437743187 + 0.01 * 6.3280768394470215
Epoch 1850, val loss: 1.2190784215927124
Epoch 1860, training loss: 0.06436088681221008 = 0.001069854130037129 + 0.01 * 6.329103469848633
Epoch 1860, val loss: 1.2199162244796753
Epoch 1870, training loss: 0.064178466796875 = 0.001063701929524541 + 0.01 * 6.311476707458496
Epoch 1870, val loss: 1.2207773923873901
Epoch 1880, training loss: 0.06428363174200058 = 0.00105766870547086 + 0.01 * 6.322596073150635
Epoch 1880, val loss: 1.2216482162475586
Epoch 1890, training loss: 0.06443462520837784 = 0.0010517131304368377 + 0.01 * 6.338291168212891
Epoch 1890, val loss: 1.2225478887557983
Epoch 1900, training loss: 0.0643240213394165 = 0.0010458339238539338 + 0.01 * 6.327819347381592
Epoch 1900, val loss: 1.2233798503875732
Epoch 1910, training loss: 0.06464597582817078 = 0.0010400782339274883 + 0.01 * 6.360589504241943
Epoch 1910, val loss: 1.224239706993103
Epoch 1920, training loss: 0.06405352801084518 = 0.0010343774920329452 + 0.01 * 6.301914691925049
Epoch 1920, val loss: 1.2250806093215942
Epoch 1930, training loss: 0.06418605893850327 = 0.001028776983730495 + 0.01 * 6.315728187561035
Epoch 1930, val loss: 1.2259445190429688
Epoch 1940, training loss: 0.06422120332717896 = 0.0010232410859316587 + 0.01 * 6.319796562194824
Epoch 1940, val loss: 1.2268078327178955
Epoch 1950, training loss: 0.06419128179550171 = 0.0010177732910960913 + 0.01 * 6.3173508644104
Epoch 1950, val loss: 1.2275867462158203
Epoch 1960, training loss: 0.06408089399337769 = 0.0010124039836227894 + 0.01 * 6.306848526000977
Epoch 1960, val loss: 1.2284363508224487
Epoch 1970, training loss: 0.06415139883756638 = 0.0010070971911773086 + 0.01 * 6.3144307136535645
Epoch 1970, val loss: 1.2292193174362183
Epoch 1980, training loss: 0.06400852650403976 = 0.001001856755465269 + 0.01 * 6.300666809082031
Epoch 1980, val loss: 1.2300848960876465
Epoch 1990, training loss: 0.06391783803701401 = 0.000996665912680328 + 0.01 * 6.292117595672607
Epoch 1990, val loss: 1.2308776378631592
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8519
Overall ASR: 0.9151
Flip ASR: 0.8978/225 nodes
The final ASR:0.75646, 0.14129, Accuracy:0.83086, 0.01823
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11568])
remove edge: torch.Size([2, 9424])
updated graph: torch.Size([2, 10436])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.97909, 0.00460, Accuracy:0.83580, 0.00175
Begin epxeriment: cont_weight: 0.01 epoch:2000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.023207664489746 = 1.9394694566726685 + 0.01 * 8.37382984161377
Epoch 0, val loss: 1.9403390884399414
Epoch 10, training loss: 2.0134570598602295 = 1.9297196865081787 + 0.01 * 8.373741149902344
Epoch 10, val loss: 1.9304035902023315
Epoch 20, training loss: 2.001147747039795 = 1.917413353919983 + 0.01 * 8.373451232910156
Epoch 20, val loss: 1.9176608324050903
Epoch 30, training loss: 1.9831674098968506 = 1.8994413614273071 + 0.01 * 8.372601509094238
Epoch 30, val loss: 1.899134874343872
Epoch 40, training loss: 1.955947995185852 = 1.872263789176941 + 0.01 * 8.36842155456543
Epoch 40, val loss: 1.8716011047363281
Epoch 50, training loss: 1.9181212186813354 = 1.834714412689209 + 0.01 * 8.340685844421387
Epoch 50, val loss: 1.8357874155044556
Epoch 60, training loss: 1.8762507438659668 = 1.794268012046814 + 0.01 * 8.198272705078125
Epoch 60, val loss: 1.8016185760498047
Epoch 70, training loss: 1.8356877565383911 = 1.7565052509307861 + 0.01 * 7.91825532913208
Epoch 70, val loss: 1.7714800834655762
Epoch 80, training loss: 1.7805843353271484 = 1.7050528526306152 + 0.01 * 7.553145408630371
Epoch 80, val loss: 1.7275968790054321
Epoch 90, training loss: 1.707205057144165 = 1.6342540979385376 + 0.01 * 7.295093059539795
Epoch 90, val loss: 1.668516755104065
Epoch 100, training loss: 1.615035057067871 = 1.5444271564483643 + 0.01 * 7.060795307159424
Epoch 100, val loss: 1.5962697267532349
Epoch 110, training loss: 1.515173077583313 = 1.4455467462539673 + 0.01 * 6.962635040283203
Epoch 110, val loss: 1.5167934894561768
Epoch 120, training loss: 1.4152030944824219 = 1.3459023237228394 + 0.01 * 6.93007230758667
Epoch 120, val loss: 1.439963936805725
Epoch 130, training loss: 1.3178651332855225 = 1.248694896697998 + 0.01 * 6.917023658752441
Epoch 130, val loss: 1.3678853511810303
Epoch 140, training loss: 1.2244551181793213 = 1.1554341316223145 + 0.01 * 6.902102947235107
Epoch 140, val loss: 1.3010647296905518
Epoch 150, training loss: 1.13848078250885 = 1.0696169137954712 + 0.01 * 6.886387825012207
Epoch 150, val loss: 1.241038203239441
Epoch 160, training loss: 1.0622010231018066 = 0.9935193061828613 + 0.01 * 6.868168354034424
Epoch 160, val loss: 1.188255786895752
Epoch 170, training loss: 0.9946229457855225 = 0.9261640310287476 + 0.01 * 6.845890522003174
Epoch 170, val loss: 1.141358494758606
Epoch 180, training loss: 0.9322942495346069 = 0.8641085624694824 + 0.01 * 6.818571090698242
Epoch 180, val loss: 1.0973081588745117
Epoch 190, training loss: 0.871236264705658 = 0.8033187985420227 + 0.01 * 6.791746139526367
Epoch 190, val loss: 1.0532591342926025
Epoch 200, training loss: 0.8087497353553772 = 0.7410678863525391 + 0.01 * 6.768185138702393
Epoch 200, val loss: 1.0075832605361938
Epoch 210, training loss: 0.7442395687103271 = 0.6767460703849792 + 0.01 * 6.7493486404418945
Epoch 210, val loss: 0.9600529670715332
Epoch 220, training loss: 0.6789180040359497 = 0.6115250587463379 + 0.01 * 6.739292144775391
Epoch 220, val loss: 0.91212397813797
Epoch 230, training loss: 0.6145719289779663 = 0.547275722026825 + 0.01 * 6.729617595672607
Epoch 230, val loss: 0.8660297393798828
Epoch 240, training loss: 0.552881121635437 = 0.4856537878513336 + 0.01 * 6.72273063659668
Epoch 240, val loss: 0.8242281079292297
Epoch 250, training loss: 0.49517160654067993 = 0.4279850423336029 + 0.01 * 6.71865701675415
Epoch 250, val loss: 0.7893665432929993
Epoch 260, training loss: 0.4423435926437378 = 0.37519288063049316 + 0.01 * 6.715070724487305
Epoch 260, val loss: 0.7626346945762634
Epoch 270, training loss: 0.3947448134422302 = 0.32759207487106323 + 0.01 * 6.715273380279541
Epoch 270, val loss: 0.7437322735786438
Epoch 280, training loss: 0.35202404856681824 = 0.2849240005016327 + 0.01 * 6.710003852844238
Epoch 280, val loss: 0.7312159538269043
Epoch 290, training loss: 0.3139338195323944 = 0.24685682356357574 + 0.01 * 6.707699298858643
Epoch 290, val loss: 0.7234723567962646
Epoch 300, training loss: 0.2804095149040222 = 0.21334555745124817 + 0.01 * 6.706395626068115
Epoch 300, val loss: 0.7198322415351868
Epoch 310, training loss: 0.25141236186027527 = 0.18436121940612793 + 0.01 * 6.705113887786865
Epoch 310, val loss: 0.7197790145874023
Epoch 320, training loss: 0.22668664157390594 = 0.15964126586914062 + 0.01 * 6.704537391662598
Epoch 320, val loss: 0.7230161428451538
Epoch 330, training loss: 0.20569679141044617 = 0.1386619210243225 + 0.01 * 6.703486919403076
Epoch 330, val loss: 0.7288768291473389
Epoch 340, training loss: 0.18786826729774475 = 0.12085133045911789 + 0.01 * 6.701694488525391
Epoch 340, val loss: 0.7370049953460693
Epoch 350, training loss: 0.17271652817726135 = 0.10569432377815247 + 0.01 * 6.7022199630737305
Epoch 350, val loss: 0.7469136118888855
Epoch 360, training loss: 0.15975698828697205 = 0.09275288134813309 + 0.01 * 6.700411796569824
Epoch 360, val loss: 0.7581060528755188
Epoch 370, training loss: 0.1486719399690628 = 0.08167889714241028 + 0.01 * 6.699304103851318
Epoch 370, val loss: 0.7702343463897705
Epoch 380, training loss: 0.13916003704071045 = 0.07218079268932343 + 0.01 * 6.697925090789795
Epoch 380, val loss: 0.782886803150177
Epoch 390, training loss: 0.13099651038646698 = 0.06401341408491135 + 0.01 * 6.698309898376465
Epoch 390, val loss: 0.7958316802978516
Epoch 400, training loss: 0.12393885850906372 = 0.056977394968271255 + 0.01 * 6.6961469650268555
Epoch 400, val loss: 0.8089596033096313
Epoch 410, training loss: 0.11784736812114716 = 0.050900012254714966 + 0.01 * 6.694735527038574
Epoch 410, val loss: 0.8220680356025696
Epoch 420, training loss: 0.11256985366344452 = 0.04563641548156738 + 0.01 * 6.6933441162109375
Epoch 420, val loss: 0.8350973725318909
Epoch 430, training loss: 0.10799821466207504 = 0.041069209575653076 + 0.01 * 6.692900657653809
Epoch 430, val loss: 0.8480967283248901
Epoch 440, training loss: 0.10400055348873138 = 0.03709747642278671 + 0.01 * 6.6903076171875
Epoch 440, val loss: 0.8610348105430603
Epoch 450, training loss: 0.1005294919013977 = 0.03363502398133278 + 0.01 * 6.689447402954102
Epoch 450, val loss: 0.8738297820091248
Epoch 460, training loss: 0.09748010337352753 = 0.03060733526945114 + 0.01 * 6.687277317047119
Epoch 460, val loss: 0.8864967823028564
Epoch 470, training loss: 0.09481482952833176 = 0.027950754389166832 + 0.01 * 6.686407566070557
Epoch 470, val loss: 0.8989372849464417
Epoch 480, training loss: 0.0924515500664711 = 0.025612076744437218 + 0.01 * 6.683947563171387
Epoch 480, val loss: 0.9112412333488464
Epoch 490, training loss: 0.0903623104095459 = 0.02354581281542778 + 0.01 * 6.681649684906006
Epoch 490, val loss: 0.9232890605926514
Epoch 500, training loss: 0.0885121300816536 = 0.02171359956264496 + 0.01 * 6.6798529624938965
Epoch 500, val loss: 0.9351151585578918
Epoch 510, training loss: 0.08686700463294983 = 0.020083554089069366 + 0.01 * 6.678345203399658
Epoch 510, val loss: 0.9466384649276733
Epoch 520, training loss: 0.08545418828725815 = 0.01862831600010395 + 0.01 * 6.682587623596191
Epoch 520, val loss: 0.9578502774238586
Epoch 530, training loss: 0.08408808708190918 = 0.017325228080153465 + 0.01 * 6.676285743713379
Epoch 530, val loss: 0.9687835574150085
Epoch 540, training loss: 0.08287260681390762 = 0.01615441031754017 + 0.01 * 6.671819686889648
Epoch 540, val loss: 0.9794238209724426
Epoch 550, training loss: 0.08179698884487152 = 0.015098901465535164 + 0.01 * 6.669809341430664
Epoch 550, val loss: 0.9897762537002563
Epoch 560, training loss: 0.08082008361816406 = 0.014144490472972393 + 0.01 * 6.66756010055542
Epoch 560, val loss: 0.9998283982276917
Epoch 570, training loss: 0.08008813112974167 = 0.013278842903673649 + 0.01 * 6.680929183959961
Epoch 570, val loss: 1.0096408128738403
Epoch 580, training loss: 0.07918303459882736 = 0.012492525391280651 + 0.01 * 6.669051170349121
Epoch 580, val loss: 1.0191607475280762
Epoch 590, training loss: 0.07839740812778473 = 0.011775952763855457 + 0.01 * 6.662145614624023
Epoch 590, val loss: 1.0284010171890259
Epoch 600, training loss: 0.07771030813455582 = 0.01112084649503231 + 0.01 * 6.6589460372924805
Epoch 600, val loss: 1.0374420881271362
Epoch 610, training loss: 0.07708477228879929 = 0.01052045077085495 + 0.01 * 6.656432628631592
Epoch 610, val loss: 1.0462416410446167
Epoch 620, training loss: 0.0765194445848465 = 0.009969058446586132 + 0.01 * 6.655038833618164
Epoch 620, val loss: 1.0548157691955566
Epoch 630, training loss: 0.0759853795170784 = 0.009461594745516777 + 0.01 * 6.652378559112549
Epoch 630, val loss: 1.0631864070892334
Epoch 640, training loss: 0.07548220455646515 = 0.008993552066385746 + 0.01 * 6.648865222930908
Epoch 640, val loss: 1.07131028175354
Epoch 650, training loss: 0.07518069446086884 = 0.008561043068766594 + 0.01 * 6.6619648933410645
Epoch 650, val loss: 1.079209804534912
Epoch 660, training loss: 0.07462620735168457 = 0.008160833269357681 + 0.01 * 6.646536827087402
Epoch 660, val loss: 1.086917757987976
Epoch 670, training loss: 0.07420345395803452 = 0.0077896928414702415 + 0.01 * 6.641376495361328
Epoch 670, val loss: 1.0944575071334839
Epoch 680, training loss: 0.07395050674676895 = 0.007444784510880709 + 0.01 * 6.650572776794434
Epoch 680, val loss: 1.101794719696045
Epoch 690, training loss: 0.07347287237644196 = 0.007123762276023626 + 0.01 * 6.63491153717041
Epoch 690, val loss: 1.1089071035385132
Epoch 700, training loss: 0.07316326349973679 = 0.00682484358549118 + 0.01 * 6.633842468261719
Epoch 700, val loss: 1.1158835887908936
Epoch 710, training loss: 0.07283969223499298 = 0.006545338314026594 + 0.01 * 6.6294355392456055
Epoch 710, val loss: 1.1227139234542847
Epoch 720, training loss: 0.07256177067756653 = 0.0062840599566698074 + 0.01 * 6.627771377563477
Epoch 720, val loss: 1.129364013671875
Epoch 730, training loss: 0.07229810208082199 = 0.0060394033789634705 + 0.01 * 6.625870227813721
Epoch 730, val loss: 1.135857105255127
Epoch 740, training loss: 0.0719994381070137 = 0.005809919908642769 + 0.01 * 6.618951797485352
Epoch 740, val loss: 1.1422028541564941
Epoch 750, training loss: 0.07174825668334961 = 0.005594733636826277 + 0.01 * 6.615353107452393
Epoch 750, val loss: 1.1483898162841797
Epoch 760, training loss: 0.07173904031515121 = 0.005392399150878191 + 0.01 * 6.6346635818481445
Epoch 760, val loss: 1.15438711643219
Epoch 770, training loss: 0.07130201160907745 = 0.005201541353017092 + 0.01 * 6.610047340393066
Epoch 770, val loss: 1.1603093147277832
Epoch 780, training loss: 0.07110016793012619 = 0.0050224908627569675 + 0.01 * 6.6077680587768555
Epoch 780, val loss: 1.1661417484283447
Epoch 790, training loss: 0.07108266651630402 = 0.00485320296138525 + 0.01 * 6.622946739196777
Epoch 790, val loss: 1.1717153787612915
Epoch 800, training loss: 0.07074027508497238 = 0.004693209193646908 + 0.01 * 6.604706287384033
Epoch 800, val loss: 1.1772645711898804
Epoch 810, training loss: 0.07053612917661667 = 0.0045424276031553745 + 0.01 * 6.59937047958374
Epoch 810, val loss: 1.182660698890686
Epoch 820, training loss: 0.07040601968765259 = 0.004399794153869152 + 0.01 * 6.60062313079834
Epoch 820, val loss: 1.1877710819244385
Epoch 830, training loss: 0.0701821818947792 = 0.004265190102159977 + 0.01 * 6.591699123382568
Epoch 830, val loss: 1.1929255723953247
Epoch 840, training loss: 0.07004157453775406 = 0.004136978182941675 + 0.01 * 6.590459823608398
Epoch 840, val loss: 1.197911262512207
Epoch 850, training loss: 0.06987481564283371 = 0.004015814047306776 + 0.01 * 6.585900783538818
Epoch 850, val loss: 1.2028882503509521
Epoch 860, training loss: 0.06964360177516937 = 0.0039011819753795862 + 0.01 * 6.574242115020752
Epoch 860, val loss: 1.2075539827346802
Epoch 870, training loss: 0.06959696859121323 = 0.0037916155997663736 + 0.01 * 6.580535411834717
Epoch 870, val loss: 1.2122037410736084
Epoch 880, training loss: 0.06937122344970703 = 0.0036876716185361147 + 0.01 * 6.568355560302734
Epoch 880, val loss: 1.2168891429901123
Epoch 890, training loss: 0.06917490065097809 = 0.003588886698707938 + 0.01 * 6.5586018562316895
Epoch 890, val loss: 1.2213025093078613
Epoch 900, training loss: 0.06917012482881546 = 0.00349439587444067 + 0.01 * 6.567573070526123
Epoch 900, val loss: 1.2256662845611572
Epoch 910, training loss: 0.06895895302295685 = 0.0034047155641019344 + 0.01 * 6.555423736572266
Epoch 910, val loss: 1.230027198791504
Epoch 920, training loss: 0.06905728578567505 = 0.003318745642900467 + 0.01 * 6.573853969573975
Epoch 920, val loss: 1.2342431545257568
Epoch 930, training loss: 0.06862851232290268 = 0.0032365412916988134 + 0.01 * 6.5391974449157715
Epoch 930, val loss: 1.2384247779846191
Epoch 940, training loss: 0.06862776726484299 = 0.0031575548928231 + 0.01 * 6.547021389007568
Epoch 940, val loss: 1.242558240890503
Epoch 950, training loss: 0.06852090358734131 = 0.0030817969236522913 + 0.01 * 6.543910503387451
Epoch 950, val loss: 1.2465028762817383
Epoch 960, training loss: 0.06825902312994003 = 0.0030099160503596067 + 0.01 * 6.524910926818848
Epoch 960, val loss: 1.2504746913909912
Epoch 970, training loss: 0.06827902793884277 = 0.002941001206636429 + 0.01 * 6.5338029861450195
Epoch 970, val loss: 1.2542227506637573
Epoch 980, training loss: 0.06809845566749573 = 0.0028745303861796856 + 0.01 * 6.522392749786377
Epoch 980, val loss: 1.258022665977478
Epoch 990, training loss: 0.06819868832826614 = 0.0028110758867114782 + 0.01 * 6.538761615753174
Epoch 990, val loss: 1.2616599798202515
Epoch 1000, training loss: 0.06797875463962555 = 0.002750684041529894 + 0.01 * 6.5228071212768555
Epoch 1000, val loss: 1.2653566598892212
Epoch 1010, training loss: 0.06798628717660904 = 0.0026926002465188503 + 0.01 * 6.529368877410889
Epoch 1010, val loss: 1.2689229249954224
Epoch 1020, training loss: 0.06776006519794464 = 0.0026366643141955137 + 0.01 * 6.512340545654297
Epoch 1020, val loss: 1.2723196744918823
Epoch 1030, training loss: 0.06763280928134918 = 0.0025828504003584385 + 0.01 * 6.504996299743652
Epoch 1030, val loss: 1.275687575340271
Epoch 1040, training loss: 0.0676611140370369 = 0.002531502628698945 + 0.01 * 6.512960910797119
Epoch 1040, val loss: 1.2790530920028687
Epoch 1050, training loss: 0.06752617657184601 = 0.0024821320548653603 + 0.01 * 6.5044050216674805
Epoch 1050, val loss: 1.2822906970977783
Epoch 1060, training loss: 0.06747083365917206 = 0.002434719121083617 + 0.01 * 6.503612041473389
Epoch 1060, val loss: 1.2854056358337402
Epoch 1070, training loss: 0.06762626022100449 = 0.0023891860619187355 + 0.01 * 6.523707866668701
Epoch 1070, val loss: 1.288334608078003
Epoch 1080, training loss: 0.06730673462152481 = 0.0023452560417354107 + 0.01 * 6.496147632598877
Epoch 1080, val loss: 1.2915774583816528
Epoch 1090, training loss: 0.06720176339149475 = 0.0023030054289847612 + 0.01 * 6.489875793457031
Epoch 1090, val loss: 1.294463038444519
Epoch 1100, training loss: 0.06717515736818314 = 0.002262427704408765 + 0.01 * 6.491272926330566
Epoch 1100, val loss: 1.297414779663086
Epoch 1110, training loss: 0.06703197211027145 = 0.0022231147158890963 + 0.01 * 6.480886459350586
Epoch 1110, val loss: 1.3003050088882446
Epoch 1120, training loss: 0.06696133315563202 = 0.002185356104746461 + 0.01 * 6.477597713470459
Epoch 1120, val loss: 1.3031481504440308
Epoch 1130, training loss: 0.06692581623792648 = 0.0021488191559910774 + 0.01 * 6.477699279785156
Epoch 1130, val loss: 1.3057571649551392
Epoch 1140, training loss: 0.06701741367578506 = 0.0021138207521289587 + 0.01 * 6.490359306335449
Epoch 1140, val loss: 1.3085005283355713
Epoch 1150, training loss: 0.066825732588768 = 0.00207974249497056 + 0.01 * 6.474599361419678
Epoch 1150, val loss: 1.3111721277236938
Epoch 1160, training loss: 0.06693807244300842 = 0.0020469340961426497 + 0.01 * 6.489113807678223
Epoch 1160, val loss: 1.3138552904129028
Epoch 1170, training loss: 0.06666326522827148 = 0.0020152314100414515 + 0.01 * 6.464803695678711
Epoch 1170, val loss: 1.3162801265716553
Epoch 1180, training loss: 0.06668195128440857 = 0.0019846418872475624 + 0.01 * 6.469730854034424
Epoch 1180, val loss: 1.318740725517273
Epoch 1190, training loss: 0.0666293278336525 = 0.001954989740625024 + 0.01 * 6.467433929443359
Epoch 1190, val loss: 1.3213733434677124
Epoch 1200, training loss: 0.06649615615606308 = 0.0019262767164036632 + 0.01 * 6.45698881149292
Epoch 1200, val loss: 1.3236689567565918
Epoch 1210, training loss: 0.06667083501815796 = 0.0018986219074577093 + 0.01 * 6.4772210121154785
Epoch 1210, val loss: 1.3260109424591064
Epoch 1220, training loss: 0.06646419316530228 = 0.0018716307822614908 + 0.01 * 6.459256172180176
Epoch 1220, val loss: 1.3284938335418701
Epoch 1230, training loss: 0.06658033281564713 = 0.001845593680627644 + 0.01 * 6.473474025726318
Epoch 1230, val loss: 1.3307219743728638
Epoch 1240, training loss: 0.06635009497404099 = 0.0018205110682174563 + 0.01 * 6.452958106994629
Epoch 1240, val loss: 1.333007574081421
Epoch 1250, training loss: 0.06627347320318222 = 0.001796053838916123 + 0.01 * 6.447741985321045
Epoch 1250, val loss: 1.3352195024490356
Epoch 1260, training loss: 0.0662829577922821 = 0.0017724072095006704 + 0.01 * 6.451055526733398
Epoch 1260, val loss: 1.3374375104904175
Epoch 1270, training loss: 0.06633185595273972 = 0.0017494548810645938 + 0.01 * 6.458240509033203
Epoch 1270, val loss: 1.3395781517028809
Epoch 1280, training loss: 0.0661870688199997 = 0.001727122231386602 + 0.01 * 6.445995330810547
Epoch 1280, val loss: 1.3415335416793823
Epoch 1290, training loss: 0.06600013375282288 = 0.001705440692603588 + 0.01 * 6.429470062255859
Epoch 1290, val loss: 1.3437707424163818
Epoch 1300, training loss: 0.06613120436668396 = 0.0016844298224896193 + 0.01 * 6.444677829742432
Epoch 1300, val loss: 1.3458129167556763
Epoch 1310, training loss: 0.06601915508508682 = 0.001663859817199409 + 0.01 * 6.435529708862305
Epoch 1310, val loss: 1.3478037118911743
Epoch 1320, training loss: 0.06586591899394989 = 0.0016441316111013293 + 0.01 * 6.422178268432617
Epoch 1320, val loss: 1.3497915267944336
Epoch 1330, training loss: 0.0658465176820755 = 0.0016248802421614528 + 0.01 * 6.422163486480713
Epoch 1330, val loss: 1.3518239259719849
Epoch 1340, training loss: 0.0659814178943634 = 0.0016060246853157878 + 0.01 * 6.437539577484131
Epoch 1340, val loss: 1.3537471294403076
Epoch 1350, training loss: 0.0660199373960495 = 0.001587848411872983 + 0.01 * 6.443209648132324
Epoch 1350, val loss: 1.355549931526184
Epoch 1360, training loss: 0.06584528833627701 = 0.0015701568918302655 + 0.01 * 6.427513122558594
Epoch 1360, val loss: 1.3574212789535522
Epoch 1370, training loss: 0.06565283983945847 = 0.0015528439544141293 + 0.01 * 6.409999847412109
Epoch 1370, val loss: 1.359259009361267
Epoch 1380, training loss: 0.06608037650585175 = 0.001536081195808947 + 0.01 * 6.454430103302002
Epoch 1380, val loss: 1.3609586954116821
Epoch 1390, training loss: 0.06583080440759659 = 0.0015196194872260094 + 0.01 * 6.431118488311768
Epoch 1390, val loss: 1.3628312349319458
Epoch 1400, training loss: 0.06564249843358994 = 0.0015038499841466546 + 0.01 * 6.413865089416504
Epoch 1400, val loss: 1.364646315574646
Epoch 1410, training loss: 0.0655701607465744 = 0.001488233683630824 + 0.01 * 6.408193111419678
Epoch 1410, val loss: 1.3663585186004639
Epoch 1420, training loss: 0.06553646177053452 = 0.001473144395276904 + 0.01 * 6.406332015991211
Epoch 1420, val loss: 1.367932915687561
Epoch 1430, training loss: 0.06550883501768112 = 0.001458307495340705 + 0.01 * 6.405052661895752
Epoch 1430, val loss: 1.3697783946990967
Epoch 1440, training loss: 0.06560250371694565 = 0.0014439685037359595 + 0.01 * 6.415853977203369
Epoch 1440, val loss: 1.3713362216949463
Epoch 1450, training loss: 0.06559023261070251 = 0.0014298869064077735 + 0.01 * 6.416034698486328
Epoch 1450, val loss: 1.372921109199524
Epoch 1460, training loss: 0.06536654382944107 = 0.0014162579318508506 + 0.01 * 6.395028591156006
Epoch 1460, val loss: 1.3746870756149292
Epoch 1470, training loss: 0.06547710299491882 = 0.00140279158949852 + 0.01 * 6.407431125640869
Epoch 1470, val loss: 1.376129388809204
Epoch 1480, training loss: 0.06558839231729507 = 0.0013898812467232347 + 0.01 * 6.419851303100586
Epoch 1480, val loss: 1.3776326179504395
Epoch 1490, training loss: 0.06530144065618515 = 0.0013771002413704991 + 0.01 * 6.392434597015381
Epoch 1490, val loss: 1.3792141675949097
Epoch 1500, training loss: 0.0655156597495079 = 0.0013647309970110655 + 0.01 * 6.415092945098877
Epoch 1500, val loss: 1.3807048797607422
Epoch 1510, training loss: 0.06518195569515228 = 0.0013525596586987376 + 0.01 * 6.382940292358398
Epoch 1510, val loss: 1.3822530508041382
Epoch 1520, training loss: 0.06541093438863754 = 0.0013408518861979246 + 0.01 * 6.407008647918701
Epoch 1520, val loss: 1.3837130069732666
Epoch 1530, training loss: 0.06514768302440643 = 0.0013292632065713406 + 0.01 * 6.381841659545898
Epoch 1530, val loss: 1.385115623474121
Epoch 1540, training loss: 0.06512630730867386 = 0.0013180070091038942 + 0.01 * 6.38083028793335
Epoch 1540, val loss: 1.3865227699279785
Epoch 1550, training loss: 0.06532835215330124 = 0.0013071236899122596 + 0.01 * 6.40212345123291
Epoch 1550, val loss: 1.387917399406433
Epoch 1560, training loss: 0.0650997906923294 = 0.001296388334594667 + 0.01 * 6.380340576171875
Epoch 1560, val loss: 1.3892848491668701
Epoch 1570, training loss: 0.06503084301948547 = 0.0012858582194894552 + 0.01 * 6.3744988441467285
Epoch 1570, val loss: 1.3905982971191406
Epoch 1580, training loss: 0.06509353220462799 = 0.001275678281672299 + 0.01 * 6.381784915924072
Epoch 1580, val loss: 1.392009973526001
Epoch 1590, training loss: 0.0648464560508728 = 0.0012656472390517592 + 0.01 * 6.358081340789795
Epoch 1590, val loss: 1.3933779001235962
Epoch 1600, training loss: 0.06521014124155045 = 0.0012558578746393323 + 0.01 * 6.395428657531738
Epoch 1600, val loss: 1.3948043584823608
Epoch 1610, training loss: 0.06502612680196762 = 0.0012462761951610446 + 0.01 * 6.37798547744751
Epoch 1610, val loss: 1.396055817604065
Epoch 1620, training loss: 0.06482893973588943 = 0.0012368091847747564 + 0.01 * 6.359212875366211
Epoch 1620, val loss: 1.3973920345306396
Epoch 1630, training loss: 0.06500080972909927 = 0.0012276883935555816 + 0.01 * 6.377311706542969
Epoch 1630, val loss: 1.3987220525741577
Epoch 1640, training loss: 0.0647156685590744 = 0.0012186761014163494 + 0.01 * 6.3496994972229
Epoch 1640, val loss: 1.400070071220398
Epoch 1650, training loss: 0.06483059376478195 = 0.0012099349405616522 + 0.01 * 6.36206579208374
Epoch 1650, val loss: 1.401322603225708
Epoch 1660, training loss: 0.06472667306661606 = 0.0012013419764116406 + 0.01 * 6.35253381729126
Epoch 1660, val loss: 1.4026564359664917
Epoch 1670, training loss: 0.06464622169733047 = 0.001192879513837397 + 0.01 * 6.345334053039551
Epoch 1670, val loss: 1.4037744998931885
Epoch 1680, training loss: 0.06467185169458389 = 0.0011846252018585801 + 0.01 * 6.348722457885742
Epoch 1680, val loss: 1.4051684141159058
Epoch 1690, training loss: 0.06487156450748444 = 0.0011765779927372932 + 0.01 * 6.369498252868652
Epoch 1690, val loss: 1.4063042402267456
Epoch 1700, training loss: 0.06489823758602142 = 0.001168621820397675 + 0.01 * 6.372961521148682
Epoch 1700, val loss: 1.4075109958648682
Epoch 1710, training loss: 0.06471312791109085 = 0.0011608239728957415 + 0.01 * 6.355230808258057
Epoch 1710, val loss: 1.4086511135101318
Epoch 1720, training loss: 0.06480979919433594 = 0.0011532637290656567 + 0.01 * 6.365653991699219
Epoch 1720, val loss: 1.4098306894302368
Epoch 1730, training loss: 0.06449103355407715 = 0.0011457246728241444 + 0.01 * 6.334530353546143
Epoch 1730, val loss: 1.4110513925552368
Epoch 1740, training loss: 0.06448107212781906 = 0.001138459425419569 + 0.01 * 6.334261894226074
Epoch 1740, val loss: 1.4121177196502686
Epoch 1750, training loss: 0.06459265202283859 = 0.0011312977876514196 + 0.01 * 6.346136093139648
Epoch 1750, val loss: 1.4131511449813843
Epoch 1760, training loss: 0.06476148962974548 = 0.001124232541769743 + 0.01 * 6.3637261390686035
Epoch 1760, val loss: 1.4143494367599487
Epoch 1770, training loss: 0.06439226120710373 = 0.0011173853417858481 + 0.01 * 6.327487468719482
Epoch 1770, val loss: 1.415344476699829
Epoch 1780, training loss: 0.06471366435289383 = 0.0011106315068900585 + 0.01 * 6.36030387878418
Epoch 1780, val loss: 1.4163812398910522
Epoch 1790, training loss: 0.06443241238594055 = 0.0011039795354008675 + 0.01 * 6.332842826843262
Epoch 1790, val loss: 1.4174050092697144
Epoch 1800, training loss: 0.06475279480218887 = 0.0010974652832373977 + 0.01 * 6.365533351898193
Epoch 1800, val loss: 1.4185512065887451
Epoch 1810, training loss: 0.0643259659409523 = 0.0010911012068390846 + 0.01 * 6.323486804962158
Epoch 1810, val loss: 1.419384479522705
Epoch 1820, training loss: 0.06448876857757568 = 0.001084806863218546 + 0.01 * 6.340396881103516
Epoch 1820, val loss: 1.4204182624816895
Epoch 1830, training loss: 0.06424150615930557 = 0.0010786474449560046 + 0.01 * 6.316286087036133
Epoch 1830, val loss: 1.4212006330490112
Epoch 1840, training loss: 0.06455441564321518 = 0.0010726067703217268 + 0.01 * 6.348180770874023
Epoch 1840, val loss: 1.4222251176834106
Epoch 1850, training loss: 0.06426344066858292 = 0.0010666685411706567 + 0.01 * 6.319676876068115
Epoch 1850, val loss: 1.4231970310211182
Epoch 1860, training loss: 0.06416149437427521 = 0.0010608378797769547 + 0.01 * 6.310066223144531
Epoch 1860, val loss: 1.4240883588790894
Epoch 1870, training loss: 0.06408271938562393 = 0.0010551203740760684 + 0.01 * 6.302759647369385
Epoch 1870, val loss: 1.4250741004943848
Epoch 1880, training loss: 0.06447694450616837 = 0.0010494813323020935 + 0.01 * 6.342746734619141
Epoch 1880, val loss: 1.4256154298782349
Epoch 1890, training loss: 0.06406133621931076 = 0.001043915282934904 + 0.01 * 6.301742076873779
Epoch 1890, val loss: 1.4266602993011475
Epoch 1900, training loss: 0.06401042640209198 = 0.0010384621564298868 + 0.01 * 6.297196865081787
Epoch 1900, val loss: 1.4275151491165161
Epoch 1910, training loss: 0.06415972113609314 = 0.0010330649092793465 + 0.01 * 6.3126654624938965
Epoch 1910, val loss: 1.428526759147644
Epoch 1920, training loss: 0.06403838843107224 = 0.001027825172059238 + 0.01 * 6.301056861877441
Epoch 1920, val loss: 1.4292950630187988
Epoch 1930, training loss: 0.0640416294336319 = 0.001022575655952096 + 0.01 * 6.301905155181885
Epoch 1930, val loss: 1.4301207065582275
Epoch 1940, training loss: 0.06409227848052979 = 0.0010174274211749434 + 0.01 * 6.307485103607178
Epoch 1940, val loss: 1.431065559387207
Epoch 1950, training loss: 0.06395305693149567 = 0.0010123946703970432 + 0.01 * 6.294066905975342
Epoch 1950, val loss: 1.4319535493850708
Epoch 1960, training loss: 0.06386352330446243 = 0.0010073144221678376 + 0.01 * 6.285621166229248
Epoch 1960, val loss: 1.4327585697174072
Epoch 1970, training loss: 0.06386259198188782 = 0.0010024163639172912 + 0.01 * 6.286017894744873
Epoch 1970, val loss: 1.4336580038070679
Epoch 1980, training loss: 0.0638062134385109 = 0.0009974957210943103 + 0.01 * 6.280871868133545
Epoch 1980, val loss: 1.4342544078826904
Epoch 1990, training loss: 0.06407047808170319 = 0.0009926522616297007 + 0.01 * 6.307782173156738
Epoch 1990, val loss: 1.4352452754974365
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.4945
Flip ASR: 0.4089/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0384254455566406 = 1.9546875953674316 + 0.01 * 8.373783111572266
Epoch 0, val loss: 1.9535391330718994
Epoch 10, training loss: 2.0280375480651855 = 1.944300651550293 + 0.01 * 8.373686790466309
Epoch 10, val loss: 1.943023681640625
Epoch 20, training loss: 2.015571355819702 = 1.9318389892578125 + 0.01 * 8.373246192932129
Epoch 20, val loss: 1.9298956394195557
Epoch 30, training loss: 1.9982855319976807 = 1.9145673513412476 + 0.01 * 8.37182331085205
Epoch 30, val loss: 1.9112449884414673
Epoch 40, training loss: 1.9727823734283447 = 1.8891355991363525 + 0.01 * 8.364680290222168
Epoch 40, val loss: 1.883768916130066
Epoch 50, training loss: 1.935739517211914 = 1.8525902032852173 + 0.01 * 8.31492805480957
Epoch 50, val loss: 1.8455092906951904
Epoch 60, training loss: 1.887103796005249 = 1.8069543838500977 + 0.01 * 8.014936447143555
Epoch 60, val loss: 1.8008384704589844
Epoch 70, training loss: 1.8366692066192627 = 1.760999083518982 + 0.01 * 7.567010879516602
Epoch 70, val loss: 1.7588753700256348
Epoch 80, training loss: 1.7835988998413086 = 1.710850477218628 + 0.01 * 7.274840831756592
Epoch 80, val loss: 1.712661862373352
Epoch 90, training loss: 1.7154349088668823 = 1.644120216369629 + 0.01 * 7.131468772888184
Epoch 90, val loss: 1.6529275178909302
Epoch 100, training loss: 1.6285427808761597 = 1.5580470561981201 + 0.01 * 7.049577713012695
Epoch 100, val loss: 1.5801548957824707
Epoch 110, training loss: 1.5248839855194092 = 1.4549270868301392 + 0.01 * 6.9956841468811035
Epoch 110, val loss: 1.4951061010360718
Epoch 120, training loss: 1.414018154144287 = 1.3444774150848389 + 0.01 * 6.954074382781982
Epoch 120, val loss: 1.4056646823883057
Epoch 130, training loss: 1.3048421144485474 = 1.2355877161026 + 0.01 * 6.92544412612915
Epoch 130, val loss: 1.3213142156600952
Epoch 140, training loss: 1.2015360593795776 = 1.1324877738952637 + 0.01 * 6.904831886291504
Epoch 140, val loss: 1.2439889907836914
Epoch 150, training loss: 1.1055835485458374 = 1.0367094278335571 + 0.01 * 6.887415885925293
Epoch 150, val loss: 1.1736068725585938
Epoch 160, training loss: 1.0171946287155151 = 0.9484611749649048 + 0.01 * 6.8733439445495605
Epoch 160, val loss: 1.109302282333374
Epoch 170, training loss: 0.9354207515716553 = 0.8667958974838257 + 0.01 * 6.862488269805908
Epoch 170, val loss: 1.0501214265823364
Epoch 180, training loss: 0.8584951758384705 = 0.7899492979049683 + 0.01 * 6.854588031768799
Epoch 180, val loss: 0.9936248660087585
Epoch 190, training loss: 0.7850791215896606 = 0.716589093208313 + 0.01 * 6.849000930786133
Epoch 190, val loss: 0.9393861293792725
Epoch 200, training loss: 0.7155699729919434 = 0.6471189260482788 + 0.01 * 6.845103740692139
Epoch 200, val loss: 0.8876993060112
Epoch 210, training loss: 0.6520088911056519 = 0.5835877060890198 + 0.01 * 6.84212064743042
Epoch 210, val loss: 0.8410001993179321
Epoch 220, training loss: 0.59609055519104 = 0.5276966691017151 + 0.01 * 6.839385509490967
Epoch 220, val loss: 0.8019452095031738
Epoch 230, training loss: 0.5480626225471497 = 0.4796965420246124 + 0.01 * 6.83660888671875
Epoch 230, val loss: 0.771612286567688
Epoch 240, training loss: 0.506837785243988 = 0.4385017156600952 + 0.01 * 6.833605766296387
Epoch 240, val loss: 0.7492318749427795
Epoch 250, training loss: 0.4707237482070923 = 0.4024217128753662 + 0.01 * 6.83020544052124
Epoch 250, val loss: 0.7332043647766113
Epoch 260, training loss: 0.4380642771720886 = 0.3697923421859741 + 0.01 * 6.827192783355713
Epoch 260, val loss: 0.721487820148468
Epoch 270, training loss: 0.40757447481155396 = 0.33934202790260315 + 0.01 * 6.823246479034424
Epoch 270, val loss: 0.7126643657684326
Epoch 280, training loss: 0.3785266876220703 = 0.3103387653827667 + 0.01 * 6.818793296813965
Epoch 280, val loss: 0.706049382686615
Epoch 290, training loss: 0.35053616762161255 = 0.28239378333091736 + 0.01 * 6.814239025115967
Epoch 290, val loss: 0.7009990215301514
Epoch 300, training loss: 0.323595255613327 = 0.25548115372657776 + 0.01 * 6.811409950256348
Epoch 300, val loss: 0.6974399089813232
Epoch 310, training loss: 0.29782766103744507 = 0.2297743707895279 + 0.01 * 6.805328369140625
Epoch 310, val loss: 0.6952362060546875
Epoch 320, training loss: 0.27345314621925354 = 0.20546311140060425 + 0.01 * 6.799004077911377
Epoch 320, val loss: 0.6943964958190918
Epoch 330, training loss: 0.25069230794906616 = 0.1827658712863922 + 0.01 * 6.792645454406738
Epoch 330, val loss: 0.6950252056121826
Epoch 340, training loss: 0.22980061173439026 = 0.1619400978088379 + 0.01 * 6.7860517501831055
Epoch 340, val loss: 0.6972401142120361
Epoch 350, training loss: 0.21101823449134827 = 0.14323750138282776 + 0.01 * 6.778074264526367
Epoch 350, val loss: 0.7011752724647522
Epoch 360, training loss: 0.1945313811302185 = 0.1268359273672104 + 0.01 * 6.7695465087890625
Epoch 360, val loss: 0.7066899538040161
Epoch 370, training loss: 0.18022891879081726 = 0.11262685805559158 + 0.01 * 6.760207176208496
Epoch 370, val loss: 0.7135536670684814
Epoch 380, training loss: 0.1679147332906723 = 0.10041060298681259 + 0.01 * 6.750412940979004
Epoch 380, val loss: 0.7215844988822937
Epoch 390, training loss: 0.157461017370224 = 0.08997099846601486 + 0.01 * 6.749001502990723
Epoch 390, val loss: 0.7304711937904358
Epoch 400, training loss: 0.14846551418304443 = 0.08108072727918625 + 0.01 * 6.7384796142578125
Epoch 400, val loss: 0.7400509715080261
Epoch 410, training loss: 0.1407507359981537 = 0.07347086817026138 + 0.01 * 6.727987289428711
Epoch 410, val loss: 0.7499967813491821
Epoch 420, training loss: 0.1342015266418457 = 0.06692449003458023 + 0.01 * 6.72770357131958
Epoch 420, val loss: 0.7600905299186707
Epoch 430, training loss: 0.12843553721904755 = 0.06125638633966446 + 0.01 * 6.717915058135986
Epoch 430, val loss: 0.770218014717102
Epoch 440, training loss: 0.12339088320732117 = 0.05630693957209587 + 0.01 * 6.708394527435303
Epoch 440, val loss: 0.7802656292915344
Epoch 450, training loss: 0.1189691349864006 = 0.05195390433073044 + 0.01 * 6.701523303985596
Epoch 450, val loss: 0.7902289032936096
Epoch 460, training loss: 0.1153029128909111 = 0.048093974590301514 + 0.01 * 6.720893859863281
Epoch 460, val loss: 0.800021767616272
Epoch 470, training loss: 0.1116684228181839 = 0.04465886577963829 + 0.01 * 6.700956344604492
Epoch 470, val loss: 0.8097119927406311
Epoch 480, training loss: 0.10843785107135773 = 0.0415838398039341 + 0.01 * 6.685401439666748
Epoch 480, val loss: 0.8193195462226868
Epoch 490, training loss: 0.10568319261074066 = 0.03881024569272995 + 0.01 * 6.6872944831848145
Epoch 490, val loss: 0.8287804126739502
Epoch 500, training loss: 0.10310394316911697 = 0.036301471292972565 + 0.01 * 6.6802473068237305
Epoch 500, val loss: 0.838050127029419
Epoch 510, training loss: 0.10071128606796265 = 0.03402039781212807 + 0.01 * 6.669088840484619
Epoch 510, val loss: 0.8470879197120667
Epoch 520, training loss: 0.09858469665050507 = 0.03192698583006859 + 0.01 * 6.665771007537842
Epoch 520, val loss: 0.8560107350349426
Epoch 530, training loss: 0.0966891273856163 = 0.02999114990234375 + 0.01 * 6.669797897338867
Epoch 530, val loss: 0.8648354411125183
Epoch 540, training loss: 0.0947972983121872 = 0.028195424005389214 + 0.01 * 6.660187721252441
Epoch 540, val loss: 0.8735096454620361
Epoch 550, training loss: 0.09305556118488312 = 0.026530679315328598 + 0.01 * 6.6524882316589355
Epoch 550, val loss: 0.8820697665214539
Epoch 560, training loss: 0.09143706411123276 = 0.024981772527098656 + 0.01 * 6.645529270172119
Epoch 560, val loss: 0.8904970288276672
Epoch 570, training loss: 0.08994954079389572 = 0.023539921268820763 + 0.01 * 6.64096212387085
Epoch 570, val loss: 0.8988025188446045
Epoch 580, training loss: 0.08854753524065018 = 0.022158345207571983 + 0.01 * 6.638918876647949
Epoch 580, val loss: 0.907023549079895
Epoch 590, training loss: 0.08734521269798279 = 0.020852109417319298 + 0.01 * 6.64931058883667
Epoch 590, val loss: 0.9151191115379333
Epoch 600, training loss: 0.08593287318944931 = 0.01961478590965271 + 0.01 * 6.631809234619141
Epoch 600, val loss: 0.9232217669487
Epoch 610, training loss: 0.0847388505935669 = 0.01844138465821743 + 0.01 * 6.629746913909912
Epoch 610, val loss: 0.9311477541923523
Epoch 620, training loss: 0.08361470699310303 = 0.017319906502962112 + 0.01 * 6.629480361938477
Epoch 620, val loss: 0.9389756917953491
Epoch 630, training loss: 0.08247499912977219 = 0.016237540170550346 + 0.01 * 6.623745918273926
Epoch 630, val loss: 0.9468162655830383
Epoch 640, training loss: 0.08133610337972641 = 0.015170993283390999 + 0.01 * 6.616511344909668
Epoch 640, val loss: 0.9545673727989197
Epoch 650, training loss: 0.08025626838207245 = 0.014057858847081661 + 0.01 * 6.6198410987854
Epoch 650, val loss: 0.9622001647949219
Epoch 660, training loss: 0.07919718325138092 = 0.012984226457774639 + 0.01 * 6.621296405792236
Epoch 660, val loss: 0.9698711037635803
Epoch 670, training loss: 0.07806307077407837 = 0.01197232399135828 + 0.01 * 6.609074592590332
Epoch 670, val loss: 0.9778439998626709
Epoch 680, training loss: 0.07709838449954987 = 0.011031219735741615 + 0.01 * 6.606717109680176
Epoch 680, val loss: 0.9859984517097473
Epoch 690, training loss: 0.0762130543589592 = 0.010182230733335018 + 0.01 * 6.603082180023193
Epoch 690, val loss: 0.9942721724510193
Epoch 700, training loss: 0.07553426921367645 = 0.009409460239112377 + 0.01 * 6.612481117248535
Epoch 700, val loss: 1.0026071071624756
Epoch 710, training loss: 0.07470647990703583 = 0.008735139854252338 + 0.01 * 6.597134113311768
Epoch 710, val loss: 1.010948896408081
Epoch 720, training loss: 0.0741112008690834 = 0.008139302022755146 + 0.01 * 6.597189903259277
Epoch 720, val loss: 1.0192511081695557
Epoch 730, training loss: 0.07357157766819 = 0.007616210728883743 + 0.01 * 6.595536231994629
Epoch 730, val loss: 1.0274097919464111
Epoch 740, training loss: 0.07306244224309921 = 0.007138147484511137 + 0.01 * 6.5924296379089355
Epoch 740, val loss: 1.0353461503982544
Epoch 750, training loss: 0.07285173237323761 = 0.006708934903144836 + 0.01 * 6.614279747009277
Epoch 750, val loss: 1.043139100074768
Epoch 760, training loss: 0.07219557464122772 = 0.006338505540043116 + 0.01 * 6.58570671081543
Epoch 760, val loss: 1.050744891166687
Epoch 770, training loss: 0.07187805324792862 = 0.006015206687152386 + 0.01 * 6.58628511428833
Epoch 770, val loss: 1.0582878589630127
Epoch 780, training loss: 0.07153041660785675 = 0.005724749527871609 + 0.01 * 6.58056640625
Epoch 780, val loss: 1.0655317306518555
Epoch 790, training loss: 0.07156442105770111 = 0.0054610492661595345 + 0.01 * 6.610337257385254
Epoch 790, val loss: 1.0727033615112305
Epoch 800, training loss: 0.07100948691368103 = 0.005222537089139223 + 0.01 * 6.578694820404053
Epoch 800, val loss: 1.079649806022644
Epoch 810, training loss: 0.07071886211633682 = 0.005004089791327715 + 0.01 * 6.571476936340332
Epoch 810, val loss: 1.0864335298538208
Epoch 820, training loss: 0.07070353627204895 = 0.004803979303687811 + 0.01 * 6.589956283569336
Epoch 820, val loss: 1.093078851699829
Epoch 830, training loss: 0.07029704004526138 = 0.004620499908924103 + 0.01 * 6.567654132843018
Epoch 830, val loss: 1.099579930305481
Epoch 840, training loss: 0.07013991475105286 = 0.004451207350939512 + 0.01 * 6.568871021270752
Epoch 840, val loss: 1.105870246887207
Epoch 850, training loss: 0.06989289820194244 = 0.0042945463210344315 + 0.01 * 6.559835433959961
Epoch 850, val loss: 1.112060308456421
Epoch 860, training loss: 0.06982967257499695 = 0.004148135893046856 + 0.01 * 6.5681538581848145
Epoch 860, val loss: 1.1181302070617676
Epoch 870, training loss: 0.0696796327829361 = 0.004011653829365969 + 0.01 * 6.566798210144043
Epoch 870, val loss: 1.1239804029464722
Epoch 880, training loss: 0.06946226954460144 = 0.0038838456384837627 + 0.01 * 6.55784273147583
Epoch 880, val loss: 1.1297534704208374
Epoch 890, training loss: 0.06931182742118835 = 0.0037639231886714697 + 0.01 * 6.55479097366333
Epoch 890, val loss: 1.135433316230774
Epoch 900, training loss: 0.06919842958450317 = 0.0036511183716356754 + 0.01 * 6.554731369018555
Epoch 900, val loss: 1.1410359144210815
Epoch 910, training loss: 0.0689861923456192 = 0.0035448281560093164 + 0.01 * 6.5441365242004395
Epoch 910, val loss: 1.1464529037475586
Epoch 920, training loss: 0.06889286637306213 = 0.0034444716293364763 + 0.01 * 6.544839382171631
Epoch 920, val loss: 1.1518031358718872
Epoch 930, training loss: 0.06875458359718323 = 0.0033494329545646906 + 0.01 * 6.540515422821045
Epoch 930, val loss: 1.1570520401000977
Epoch 940, training loss: 0.06883634626865387 = 0.003259471617639065 + 0.01 * 6.557687759399414
Epoch 940, val loss: 1.1622692346572876
Epoch 950, training loss: 0.0685557872056961 = 0.0031743182335048914 + 0.01 * 6.53814697265625
Epoch 950, val loss: 1.1673089265823364
Epoch 960, training loss: 0.06848430633544922 = 0.003093453822657466 + 0.01 * 6.539085388183594
Epoch 960, val loss: 1.1723227500915527
Epoch 970, training loss: 0.06838449090719223 = 0.0030168506782501936 + 0.01 * 6.536764144897461
Epoch 970, val loss: 1.1771150827407837
Epoch 980, training loss: 0.06819777935743332 = 0.0029438675846904516 + 0.01 * 6.525391578674316
Epoch 980, val loss: 1.1819078922271729
Epoch 990, training loss: 0.06830038130283356 = 0.002874281257390976 + 0.01 * 6.542609691619873
Epoch 990, val loss: 1.1865965127944946
Epoch 1000, training loss: 0.06800401210784912 = 0.0028078833129256964 + 0.01 * 6.519612789154053
Epoch 1000, val loss: 1.1911377906799316
Epoch 1010, training loss: 0.0679001733660698 = 0.0027442367281764746 + 0.01 * 6.515594005584717
Epoch 1010, val loss: 1.1956672668457031
Epoch 1020, training loss: 0.06821143627166748 = 0.0026834034360945225 + 0.01 * 6.5528035163879395
Epoch 1020, val loss: 1.200143575668335
Epoch 1030, training loss: 0.06776180863380432 = 0.0026254740078002214 + 0.01 * 6.513633728027344
Epoch 1030, val loss: 1.2044755220413208
Epoch 1040, training loss: 0.06770127266645432 = 0.0025699997786432505 + 0.01 * 6.513127326965332
Epoch 1040, val loss: 1.2086946964263916
Epoch 1050, training loss: 0.06757428497076035 = 0.0025168373249471188 + 0.01 * 6.5057454109191895
Epoch 1050, val loss: 1.2128814458847046
Epoch 1060, training loss: 0.06759033352136612 = 0.002465928439050913 + 0.01 * 6.5124406814575195
Epoch 1060, val loss: 1.2170467376708984
Epoch 1070, training loss: 0.06744001060724258 = 0.002417157404124737 + 0.01 * 6.502285480499268
Epoch 1070, val loss: 1.2211076021194458
Epoch 1080, training loss: 0.06771218031644821 = 0.0023703956976532936 + 0.01 * 6.534178733825684
Epoch 1080, val loss: 1.225067377090454
Epoch 1090, training loss: 0.0672796219587326 = 0.002325448440387845 + 0.01 * 6.495417594909668
Epoch 1090, val loss: 1.2290507555007935
Epoch 1100, training loss: 0.06744780391454697 = 0.0022822872269898653 + 0.01 * 6.516551971435547
Epoch 1100, val loss: 1.2329667806625366
Epoch 1110, training loss: 0.0672089084982872 = 0.002240917179733515 + 0.01 * 6.496799468994141
Epoch 1110, val loss: 1.2366437911987305
Epoch 1120, training loss: 0.06709173321723938 = 0.0022010200191289186 + 0.01 * 6.489071369171143
Epoch 1120, val loss: 1.2404152154922485
Epoch 1130, training loss: 0.06711194664239883 = 0.0021626369562000036 + 0.01 * 6.494931221008301
Epoch 1130, val loss: 1.2441673278808594
Epoch 1140, training loss: 0.06686598062515259 = 0.0021258611232042313 + 0.01 * 6.47401237487793
Epoch 1140, val loss: 1.2477641105651855
Epoch 1150, training loss: 0.06708207726478577 = 0.0020903225522488356 + 0.01 * 6.499176025390625
Epoch 1150, val loss: 1.2512716054916382
Epoch 1160, training loss: 0.06671116501092911 = 0.0020560978446155787 + 0.01 * 6.465506553649902
Epoch 1160, val loss: 1.254810094833374
Epoch 1170, training loss: 0.06665237993001938 = 0.002023099223151803 + 0.01 * 6.462928295135498
Epoch 1170, val loss: 1.2582801580429077
Epoch 1180, training loss: 0.06678007543087006 = 0.001991360215470195 + 0.01 * 6.4788713455200195
Epoch 1180, val loss: 1.2616223096847534
Epoch 1190, training loss: 0.06673743575811386 = 0.0019607048016041517 + 0.01 * 6.477673530578613
Epoch 1190, val loss: 1.2649770975112915
Epoch 1200, training loss: 0.0664547011256218 = 0.0019310296047478914 + 0.01 * 6.452367305755615
Epoch 1200, val loss: 1.2683422565460205
Epoch 1210, training loss: 0.06669088453054428 = 0.0019024870125576854 + 0.01 * 6.478840351104736
Epoch 1210, val loss: 1.2715637683868408
Epoch 1220, training loss: 0.06634611636400223 = 0.0018747583962976933 + 0.01 * 6.4471354484558105
Epoch 1220, val loss: 1.2746955156326294
Epoch 1230, training loss: 0.06652764230966568 = 0.001848077168688178 + 0.01 * 6.46795654296875
Epoch 1230, val loss: 1.2778664827346802
Epoch 1240, training loss: 0.06623233109712601 = 0.001822147867642343 + 0.01 * 6.441018104553223
Epoch 1240, val loss: 1.2809778451919556
Epoch 1250, training loss: 0.06630367785692215 = 0.0017971551278606057 + 0.01 * 6.450652599334717
Epoch 1250, val loss: 1.2840293645858765
Epoch 1260, training loss: 0.06623012572526932 = 0.001773017575033009 + 0.01 * 6.445711612701416
Epoch 1260, val loss: 1.2869378328323364
Epoch 1270, training loss: 0.06613805145025253 = 0.0017495878273621202 + 0.01 * 6.438846111297607
Epoch 1270, val loss: 1.2899296283721924
Epoch 1280, training loss: 0.06590536236763 = 0.0017269174568355083 + 0.01 * 6.417844295501709
Epoch 1280, val loss: 1.2927790880203247
Epoch 1290, training loss: 0.06611105054616928 = 0.0017049245070666075 + 0.01 * 6.44061279296875
Epoch 1290, val loss: 1.2956290245056152
Epoch 1300, training loss: 0.06579382717609406 = 0.0016835983842611313 + 0.01 * 6.411023139953613
Epoch 1300, val loss: 1.2983983755111694
Epoch 1310, training loss: 0.06597839295864105 = 0.001662922790274024 + 0.01 * 6.431546688079834
Epoch 1310, val loss: 1.3011969327926636
Epoch 1320, training loss: 0.06566903740167618 = 0.0016427980735898018 + 0.01 * 6.402623653411865
Epoch 1320, val loss: 1.303868293762207
Epoch 1330, training loss: 0.06565754115581512 = 0.0016232688212767243 + 0.01 * 6.403427600860596
Epoch 1330, val loss: 1.3065712451934814
Epoch 1340, training loss: 0.0656818300485611 = 0.0016042300267145038 + 0.01 * 6.407760143280029
Epoch 1340, val loss: 1.309186339378357
Epoch 1350, training loss: 0.06556641310453415 = 0.0015857754042372108 + 0.01 * 6.398064136505127
Epoch 1350, val loss: 1.3117320537567139
Epoch 1360, training loss: 0.06563391536474228 = 0.0015677831834182143 + 0.01 * 6.406613349914551
Epoch 1360, val loss: 1.3143272399902344
Epoch 1370, training loss: 0.06537754833698273 = 0.0015503346221521497 + 0.01 * 6.382721900939941
Epoch 1370, val loss: 1.3168129920959473
Epoch 1380, training loss: 0.065483957529068 = 0.0015333902556449175 + 0.01 * 6.39505672454834
Epoch 1380, val loss: 1.3193174600601196
Epoch 1390, training loss: 0.0653831735253334 = 0.0015169703401625156 + 0.01 * 6.38662052154541
Epoch 1390, val loss: 1.3217682838439941
Epoch 1400, training loss: 0.06557659059762955 = 0.001500902115367353 + 0.01 * 6.40756893157959
Epoch 1400, val loss: 1.3241883516311646
Epoch 1410, training loss: 0.06543175131082535 = 0.0014853870961815119 + 0.01 * 6.394637107849121
Epoch 1410, val loss: 1.3266147375106812
Epoch 1420, training loss: 0.0652020275592804 = 0.001470260787755251 + 0.01 * 6.3731770515441895
Epoch 1420, val loss: 1.3290029764175415
Epoch 1430, training loss: 0.06529548019170761 = 0.001455517951399088 + 0.01 * 6.383996486663818
Epoch 1430, val loss: 1.331369161605835
Epoch 1440, training loss: 0.06521774083375931 = 0.0014412166783586144 + 0.01 * 6.377652645111084
Epoch 1440, val loss: 1.3336126804351807
Epoch 1450, training loss: 0.06506156921386719 = 0.0014272320549935102 + 0.01 * 6.363433837890625
Epoch 1450, val loss: 1.335944414138794
Epoch 1460, training loss: 0.06529062986373901 = 0.0014136021491140127 + 0.01 * 6.387702941894531
Epoch 1460, val loss: 1.3382028341293335
Epoch 1470, training loss: 0.06513282656669617 = 0.0014003310352563858 + 0.01 * 6.373249530792236
Epoch 1470, val loss: 1.3404630422592163
Epoch 1480, training loss: 0.06519210338592529 = 0.0013873815769329667 + 0.01 * 6.380472183227539
Epoch 1480, val loss: 1.3426841497421265
Epoch 1490, training loss: 0.06511150300502777 = 0.0013747827615588903 + 0.01 * 6.373672008514404
Epoch 1490, val loss: 1.3448584079742432
Epoch 1500, training loss: 0.06500490754842758 = 0.001362393843010068 + 0.01 * 6.364251613616943
Epoch 1500, val loss: 1.3470385074615479
Epoch 1510, training loss: 0.06497902423143387 = 0.0013504029484465718 + 0.01 * 6.3628621101379395
Epoch 1510, val loss: 1.349208950996399
Epoch 1520, training loss: 0.06506289541721344 = 0.001338659436441958 + 0.01 * 6.3724236488342285
Epoch 1520, val loss: 1.351356863975525
Epoch 1530, training loss: 0.06480497121810913 = 0.0013272089418023825 + 0.01 * 6.347776412963867
Epoch 1530, val loss: 1.3534698486328125
Epoch 1540, training loss: 0.0648406520485878 = 0.0013160204980522394 + 0.01 * 6.352463722229004
Epoch 1540, val loss: 1.3555773496627808
Epoch 1550, training loss: 0.06475625932216644 = 0.001305141020566225 + 0.01 * 6.345111846923828
Epoch 1550, val loss: 1.3575981855392456
Epoch 1560, training loss: 0.06471965461969376 = 0.0012944713234901428 + 0.01 * 6.3425188064575195
Epoch 1560, val loss: 1.3596137762069702
Epoch 1570, training loss: 0.06484966725111008 = 0.0012840541312471032 + 0.01 * 6.356561183929443
Epoch 1570, val loss: 1.3616547584533691
Epoch 1580, training loss: 0.06482257694005966 = 0.0012737595243379474 + 0.01 * 6.354881763458252
Epoch 1580, val loss: 1.3635811805725098
Epoch 1590, training loss: 0.06495947390794754 = 0.001263775397092104 + 0.01 * 6.369570255279541
Epoch 1590, val loss: 1.3655763864517212
Epoch 1600, training loss: 0.06460919231176376 = 0.0012540064053609967 + 0.01 * 6.335518836975098
Epoch 1600, val loss: 1.367522120475769
Epoch 1610, training loss: 0.0645841434597969 = 0.0012444219319149852 + 0.01 * 6.333972454071045
Epoch 1610, val loss: 1.3694404363632202
Epoch 1620, training loss: 0.06485474854707718 = 0.0012350979959592223 + 0.01 * 6.361965179443359
Epoch 1620, val loss: 1.371367335319519
Epoch 1630, training loss: 0.06460624933242798 = 0.0012259017676115036 + 0.01 * 6.338034629821777
Epoch 1630, val loss: 1.373233437538147
Epoch 1640, training loss: 0.06478050351142883 = 0.0012169418623670936 + 0.01 * 6.356356143951416
Epoch 1640, val loss: 1.3751424551010132
Epoch 1650, training loss: 0.06457743793725967 = 0.0012081473832949996 + 0.01 * 6.336928844451904
Epoch 1650, val loss: 1.3770129680633545
Epoch 1660, training loss: 0.06467173248529434 = 0.001199513440951705 + 0.01 * 6.347222328186035
Epoch 1660, val loss: 1.3788723945617676
Epoch 1670, training loss: 0.06445703655481339 = 0.0011910720495507121 + 0.01 * 6.326596260070801
Epoch 1670, val loss: 1.3806999921798706
Epoch 1680, training loss: 0.06446626782417297 = 0.0011828008573502302 + 0.01 * 6.3283467292785645
Epoch 1680, val loss: 1.382521629333496
Epoch 1690, training loss: 0.0643567144870758 = 0.001174671109765768 + 0.01 * 6.318204402923584
Epoch 1690, val loss: 1.3843191862106323
Epoch 1700, training loss: 0.06444445252418518 = 0.0011667365906760097 + 0.01 * 6.3277716636657715
Epoch 1700, val loss: 1.386117696762085
Epoch 1710, training loss: 0.06464992463588715 = 0.0011589380446821451 + 0.01 * 6.349099159240723
Epoch 1710, val loss: 1.3879119157791138
Epoch 1720, training loss: 0.06434622406959534 = 0.001151297939941287 + 0.01 * 6.319492816925049
Epoch 1720, val loss: 1.389661192893982
Epoch 1730, training loss: 0.06448841840028763 = 0.001143821980804205 + 0.01 * 6.3344597816467285
Epoch 1730, val loss: 1.3913688659667969
Epoch 1740, training loss: 0.06431594491004944 = 0.0011364334495738149 + 0.01 * 6.31795072555542
Epoch 1740, val loss: 1.3931950330734253
Epoch 1750, training loss: 0.0641879141330719 = 0.0011292650597169995 + 0.01 * 6.305865287780762
Epoch 1750, val loss: 1.3948651552200317
Epoch 1760, training loss: 0.06426941603422165 = 0.001122132409363985 + 0.01 * 6.314728736877441
Epoch 1760, val loss: 1.3966362476348877
Epoch 1770, training loss: 0.06417196989059448 = 0.0011152438819408417 + 0.01 * 6.305672645568848
Epoch 1770, val loss: 1.3982654809951782
Epoch 1780, training loss: 0.06421362608671188 = 0.0011084122816100717 + 0.01 * 6.310521125793457
Epoch 1780, val loss: 1.3999789953231812
Epoch 1790, training loss: 0.06406567990779877 = 0.0011017414508387446 + 0.01 * 6.296394348144531
Epoch 1790, val loss: 1.4016454219818115
Epoch 1800, training loss: 0.06417756527662277 = 0.0010951455915346742 + 0.01 * 6.308241844177246
Epoch 1800, val loss: 1.4033058881759644
Epoch 1810, training loss: 0.06411925703287125 = 0.0010887236567214131 + 0.01 * 6.303053379058838
Epoch 1810, val loss: 1.4049302339553833
Epoch 1820, training loss: 0.0640459731221199 = 0.0010823804186657071 + 0.01 * 6.296359062194824
Epoch 1820, val loss: 1.4065601825714111
Epoch 1830, training loss: 0.06422585994005203 = 0.0010762388119474053 + 0.01 * 6.314962863922119
Epoch 1830, val loss: 1.4081368446350098
Epoch 1840, training loss: 0.06412145495414734 = 0.0010700745042413473 + 0.01 * 6.305138111114502
Epoch 1840, val loss: 1.4098025560379028
Epoch 1850, training loss: 0.06438043713569641 = 0.0010641374392434955 + 0.01 * 6.331630229949951
Epoch 1850, val loss: 1.411337971687317
Epoch 1860, training loss: 0.0638822391629219 = 0.001058173249475658 + 0.01 * 6.282406806945801
Epoch 1860, val loss: 1.4129202365875244
Epoch 1870, training loss: 0.06411628425121307 = 0.0010524624958634377 + 0.01 * 6.306382656097412
Epoch 1870, val loss: 1.4144866466522217
Epoch 1880, training loss: 0.06392469257116318 = 0.001046738470904529 + 0.01 * 6.287795543670654
Epoch 1880, val loss: 1.4159773588180542
Epoch 1890, training loss: 0.06440672278404236 = 0.0010411844123154879 + 0.01 * 6.336554050445557
Epoch 1890, val loss: 1.4174755811691284
Epoch 1900, training loss: 0.0639999583363533 = 0.0010356332641094923 + 0.01 * 6.296432971954346
Epoch 1900, val loss: 1.4189873933792114
Epoch 1910, training loss: 0.06379333138465881 = 0.0010302528971806169 + 0.01 * 6.276308059692383
Epoch 1910, val loss: 1.420512318611145
Epoch 1920, training loss: 0.06371897459030151 = 0.0010248991893604398 + 0.01 * 6.269407749176025
Epoch 1920, val loss: 1.421992540359497
Epoch 1930, training loss: 0.06390617042779922 = 0.0010196814546361566 + 0.01 * 6.288649559020996
Epoch 1930, val loss: 1.4234727621078491
Epoch 1940, training loss: 0.06371628493070602 = 0.001014492940157652 + 0.01 * 6.270179271697998
Epoch 1940, val loss: 1.4248924255371094
Epoch 1950, training loss: 0.0637667253613472 = 0.001009388011880219 + 0.01 * 6.275733470916748
Epoch 1950, val loss: 1.4263954162597656
Epoch 1960, training loss: 0.06378727406263351 = 0.0010043883230537176 + 0.01 * 6.278288841247559
Epoch 1960, val loss: 1.4278061389923096
Epoch 1970, training loss: 0.06383073329925537 = 0.0009994263527914882 + 0.01 * 6.283130645751953
Epoch 1970, val loss: 1.4292441606521606
Epoch 1980, training loss: 0.06365492939949036 = 0.0009945582132786512 + 0.01 * 6.266037464141846
Epoch 1980, val loss: 1.4306881427764893
Epoch 1990, training loss: 0.06364309042692184 = 0.0009897255804389715 + 0.01 * 6.265336513519287
Epoch 1990, val loss: 1.4321308135986328
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7815
Overall ASR: 0.5941
Flip ASR: 0.5467/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.032191753387451 = 1.9484541416168213 + 0.01 * 8.37376880645752
Epoch 0, val loss: 1.9514780044555664
Epoch 10, training loss: 2.0219509601593018 = 1.9382158517837524 + 0.01 * 8.373507499694824
Epoch 10, val loss: 1.9400254487991333
Epoch 20, training loss: 2.0094056129455566 = 1.9256781339645386 + 0.01 * 8.372737884521484
Epoch 20, val loss: 1.925445795059204
Epoch 30, training loss: 1.9917031526565552 = 1.9079957008361816 + 0.01 * 8.370750427246094
Epoch 30, val loss: 1.9044795036315918
Epoch 40, training loss: 1.9654806852340698 = 1.8818668127059937 + 0.01 * 8.361391067504883
Epoch 40, val loss: 1.8739163875579834
Epoch 50, training loss: 1.9278514385223389 = 1.8448563814163208 + 0.01 * 8.299503326416016
Epoch 50, val loss: 1.8328137397766113
Epoch 60, training loss: 1.8798047304153442 = 1.7991446256637573 + 0.01 * 8.066015243530273
Epoch 60, val loss: 1.7864083051681519
Epoch 70, training loss: 1.8298364877700806 = 1.7528873682022095 + 0.01 * 7.694913864135742
Epoch 70, val loss: 1.7447819709777832
Epoch 80, training loss: 1.7774370908737183 = 1.7048794031143188 + 0.01 * 7.255767345428467
Epoch 80, val loss: 1.705292820930481
Epoch 90, training loss: 1.7125953435897827 = 1.6422566175460815 + 0.01 * 7.033873558044434
Epoch 90, val loss: 1.6555578708648682
Epoch 100, training loss: 1.6291203498840332 = 1.5593031644821167 + 0.01 * 6.981712818145752
Epoch 100, val loss: 1.5897217988967896
Epoch 110, training loss: 1.5254786014556885 = 1.4561349153518677 + 0.01 * 6.934374809265137
Epoch 110, val loss: 1.5089857578277588
Epoch 120, training loss: 1.4085663557052612 = 1.3395150899887085 + 0.01 * 6.90512228012085
Epoch 120, val loss: 1.4179407358169556
Epoch 130, training loss: 1.2854058742523193 = 1.2164602279663086 + 0.01 * 6.894559860229492
Epoch 130, val loss: 1.323761224746704
Epoch 140, training loss: 1.161289930343628 = 1.0924663543701172 + 0.01 * 6.882361888885498
Epoch 140, val loss: 1.2299952507019043
Epoch 150, training loss: 1.0420359373092651 = 0.9733549356460571 + 0.01 * 6.868099689483643
Epoch 150, val loss: 1.141392469406128
Epoch 160, training loss: 0.933099627494812 = 0.8646146655082703 + 0.01 * 6.848495006561279
Epoch 160, val loss: 1.061116099357605
Epoch 170, training loss: 0.837023138999939 = 0.768756091594696 + 0.01 * 6.8267035484313965
Epoch 170, val loss: 0.991104781627655
Epoch 180, training loss: 0.7536343336105347 = 0.6855422854423523 + 0.01 * 6.809205532073975
Epoch 180, val loss: 0.9313635230064392
Epoch 190, training loss: 0.6812123656272888 = 0.6132221221923828 + 0.01 * 6.799026012420654
Epoch 190, val loss: 0.8809730410575867
Epoch 200, training loss: 0.6175009608268738 = 0.5496152639389038 + 0.01 * 6.788570404052734
Epoch 200, val loss: 0.8387112617492676
Epoch 210, training loss: 0.5603711605072021 = 0.4925777018070221 + 0.01 * 6.779344081878662
Epoch 210, val loss: 0.803429365158081
Epoch 220, training loss: 0.5080828070640564 = 0.44036224484443665 + 0.01 * 6.772058486938477
Epoch 220, val loss: 0.7739871740341187
Epoch 230, training loss: 0.4593874216079712 = 0.3917703926563263 + 0.01 * 6.7617034912109375
Epoch 230, val loss: 0.7493633031845093
Epoch 240, training loss: 0.41369032859802246 = 0.3461764454841614 + 0.01 * 6.751389503479004
Epoch 240, val loss: 0.7289490103721619
Epoch 250, training loss: 0.37105879187583923 = 0.3036094009876251 + 0.01 * 6.744938850402832
Epoch 250, val loss: 0.712136447429657
Epoch 260, training loss: 0.3320235013961792 = 0.26466500759124756 + 0.01 * 6.735851287841797
Epoch 260, val loss: 0.698950469493866
Epoch 270, training loss: 0.2971646189689636 = 0.22987870872020721 + 0.01 * 6.728592872619629
Epoch 270, val loss: 0.6894466876983643
Epoch 280, training loss: 0.2666667699813843 = 0.19943152368068695 + 0.01 * 6.723524570465088
Epoch 280, val loss: 0.6836740374565125
Epoch 290, training loss: 0.24032312631607056 = 0.17312681674957275 + 0.01 * 6.719632148742676
Epoch 290, val loss: 0.6810987591743469
Epoch 300, training loss: 0.21768422424793243 = 0.15052953362464905 + 0.01 * 6.7154693603515625
Epoch 300, val loss: 0.6812199950218201
Epoch 310, training loss: 0.19832146167755127 = 0.13110071420669556 + 0.01 * 6.722075462341309
Epoch 310, val loss: 0.6836065649986267
Epoch 320, training loss: 0.1815311312675476 = 0.1143728494644165 + 0.01 * 6.715829372406006
Epoch 320, val loss: 0.687903881072998
Epoch 330, training loss: 0.16703402996063232 = 0.09995266795158386 + 0.01 * 6.708136081695557
Epoch 330, val loss: 0.6937122941017151
Epoch 340, training loss: 0.15459556877613068 = 0.08752851188182831 + 0.01 * 6.7067060470581055
Epoch 340, val loss: 0.7007562518119812
Epoch 350, training loss: 0.1438860297203064 = 0.0768444761633873 + 0.01 * 6.704155445098877
Epoch 350, val loss: 0.7088050842285156
Epoch 360, training loss: 0.13477976620197296 = 0.06767784804105759 + 0.01 * 6.71019172668457
Epoch 360, val loss: 0.7176550626754761
Epoch 370, training loss: 0.12684562802314758 = 0.05983080714941025 + 0.01 * 6.70148229598999
Epoch 370, val loss: 0.7271139025688171
Epoch 380, training loss: 0.12009067088365555 = 0.0531197115778923 + 0.01 * 6.69709587097168
Epoch 380, val loss: 0.7369513511657715
Epoch 390, training loss: 0.1143229603767395 = 0.04737647622823715 + 0.01 * 6.694648742675781
Epoch 390, val loss: 0.7470049858093262
Epoch 400, training loss: 0.10939033329486847 = 0.042453583329916 + 0.01 * 6.693675518035889
Epoch 400, val loss: 0.7571450471878052
Epoch 410, training loss: 0.10511916875839233 = 0.03822316601872444 + 0.01 * 6.689600944519043
Epoch 410, val loss: 0.7673113942146301
Epoch 420, training loss: 0.101469486951828 = 0.03457330912351608 + 0.01 * 6.68961763381958
Epoch 420, val loss: 0.7774014472961426
Epoch 430, training loss: 0.0982670858502388 = 0.03141086548566818 + 0.01 * 6.685622215270996
Epoch 430, val loss: 0.787344753742218
Epoch 440, training loss: 0.09549576044082642 = 0.028656933456659317 + 0.01 * 6.683882713317871
Epoch 440, val loss: 0.7971512675285339
Epoch 450, training loss: 0.09302906692028046 = 0.026247559115290642 + 0.01 * 6.678150653839111
Epoch 450, val loss: 0.8068106770515442
Epoch 460, training loss: 0.09093762934207916 = 0.024129174649715424 + 0.01 * 6.680845737457275
Epoch 460, val loss: 0.8162941932678223
Epoch 470, training loss: 0.0889958068728447 = 0.022258756682276726 + 0.01 * 6.673704624176025
Epoch 470, val loss: 0.8255612254142761
Epoch 480, training loss: 0.08726531267166138 = 0.020599622279405594 + 0.01 * 6.666569709777832
Epoch 480, val loss: 0.8346450924873352
Epoch 490, training loss: 0.08580590784549713 = 0.019121510908007622 + 0.01 * 6.668439865112305
Epoch 490, val loss: 0.8435291647911072
Epoch 500, training loss: 0.08441010862588882 = 0.017799606546759605 + 0.01 * 6.661049842834473
Epoch 500, val loss: 0.8522158861160278
Epoch 510, training loss: 0.08314501494169235 = 0.016612855717539787 + 0.01 * 6.6532158851623535
Epoch 510, val loss: 0.8607204556465149
Epoch 520, training loss: 0.08203674852848053 = 0.015543580055236816 + 0.01 * 6.649316787719727
Epoch 520, val loss: 0.8690381646156311
Epoch 530, training loss: 0.08102650195360184 = 0.014577411115169525 + 0.01 * 6.644909381866455
Epoch 530, val loss: 0.8771371841430664
Epoch 540, training loss: 0.08011686056852341 = 0.013701260089874268 + 0.01 * 6.641560077667236
Epoch 540, val loss: 0.8850722312927246
Epoch 550, training loss: 0.07933203130960464 = 0.012904820963740349 + 0.01 * 6.642721176147461
Epoch 550, val loss: 0.8927726149559021
Epoch 560, training loss: 0.07846233248710632 = 0.01217895932495594 + 0.01 * 6.628337383270264
Epoch 560, val loss: 0.9002962112426758
Epoch 570, training loss: 0.07777246832847595 = 0.011515016667544842 + 0.01 * 6.62574577331543
Epoch 570, val loss: 0.9076667428016663
Epoch 580, training loss: 0.07727613300085068 = 0.010906185023486614 + 0.01 * 6.636995315551758
Epoch 580, val loss: 0.9148745536804199
Epoch 590, training loss: 0.07652108371257782 = 0.010347406379878521 + 0.01 * 6.617368221282959
Epoch 590, val loss: 0.9219197630882263
Epoch 600, training loss: 0.07594854384660721 = 0.00983284879475832 + 0.01 * 6.611569404602051
Epoch 600, val loss: 0.928763747215271
Epoch 610, training loss: 0.07561158388853073 = 0.00935794971883297 + 0.01 * 6.625363349914551
Epoch 610, val loss: 0.9355412721633911
Epoch 620, training loss: 0.07495339214801788 = 0.008918977342545986 + 0.01 * 6.60344123840332
Epoch 620, val loss: 0.9421080946922302
Epoch 630, training loss: 0.07465869933366776 = 0.008512279950082302 + 0.01 * 6.614642143249512
Epoch 630, val loss: 0.9485411643981934
Epoch 640, training loss: 0.07404343783855438 = 0.008135298267006874 + 0.01 * 6.590814590454102
Epoch 640, val loss: 0.9548227787017822
Epoch 650, training loss: 0.07365294545888901 = 0.007784717716276646 + 0.01 * 6.586822986602783
Epoch 650, val loss: 0.9609718322753906
Epoch 660, training loss: 0.07332444190979004 = 0.007458280771970749 + 0.01 * 6.586615562438965
Epoch 660, val loss: 0.9669638276100159
Epoch 670, training loss: 0.0728839635848999 = 0.007153375539928675 + 0.01 * 6.573058605194092
Epoch 670, val loss: 0.9728464484214783
Epoch 680, training loss: 0.07269728183746338 = 0.006868862081319094 + 0.01 * 6.582841873168945
Epoch 680, val loss: 0.9786185026168823
Epoch 690, training loss: 0.07227535545825958 = 0.00660248938947916 + 0.01 * 6.567286491394043
Epoch 690, val loss: 0.984204113483429
Epoch 700, training loss: 0.07196521759033203 = 0.006352163851261139 + 0.01 * 6.561305522918701
Epoch 700, val loss: 0.9897595643997192
Epoch 710, training loss: 0.07173677533864975 = 0.006117529701441526 + 0.01 * 6.561924934387207
Epoch 710, val loss: 0.9951639771461487
Epoch 720, training loss: 0.07155334949493408 = 0.005897356662899256 + 0.01 * 6.56559944152832
Epoch 720, val loss: 1.0004005432128906
Epoch 730, training loss: 0.07123350352048874 = 0.005689991172403097 + 0.01 * 6.554350852966309
Epoch 730, val loss: 1.0056430101394653
Epoch 740, training loss: 0.07103019952774048 = 0.005494268611073494 + 0.01 * 6.553593158721924
Epoch 740, val loss: 1.0106853246688843
Epoch 750, training loss: 0.07074162364006042 = 0.0053098793141543865 + 0.01 * 6.5431742668151855
Epoch 750, val loss: 1.015699863433838
Epoch 760, training loss: 0.07048170268535614 = 0.005135738756507635 + 0.01 * 6.5345964431762695
Epoch 760, val loss: 1.0205957889556885
Epoch 770, training loss: 0.07048427313566208 = 0.004971184767782688 + 0.01 * 6.551309108734131
Epoch 770, val loss: 1.0253950357437134
Epoch 780, training loss: 0.07016076147556305 = 0.004815551917999983 + 0.01 * 6.534521102905273
Epoch 780, val loss: 1.0300581455230713
Epoch 790, training loss: 0.06998312473297119 = 0.004668303299695253 + 0.01 * 6.531482219696045
Epoch 790, val loss: 1.03469717502594
Epoch 800, training loss: 0.06991508603096008 = 0.004528657998889685 + 0.01 * 6.5386433601379395
Epoch 800, val loss: 1.0392009019851685
Epoch 810, training loss: 0.06966543942689896 = 0.004396046511828899 + 0.01 * 6.526939392089844
Epoch 810, val loss: 1.043632984161377
Epoch 820, training loss: 0.06950412690639496 = 0.0042701042257249355 + 0.01 * 6.523402214050293
Epoch 820, val loss: 1.0480245351791382
Epoch 830, training loss: 0.0692773312330246 = 0.00415061553940177 + 0.01 * 6.512671947479248
Epoch 830, val loss: 1.0522990226745605
Epoch 840, training loss: 0.06905348598957062 = 0.004036803264170885 + 0.01 * 6.5016679763793945
Epoch 840, val loss: 1.056516408920288
Epoch 850, training loss: 0.0690641701221466 = 0.003928503021597862 + 0.01 * 6.513566493988037
Epoch 850, val loss: 1.0606224536895752
Epoch 860, training loss: 0.06891444325447083 = 0.0038254025857895613 + 0.01 * 6.508903980255127
Epoch 860, val loss: 1.064651608467102
Epoch 870, training loss: 0.06877229362726212 = 0.0037271822802722454 + 0.01 * 6.504510879516602
Epoch 870, val loss: 1.0686743259429932
Epoch 880, training loss: 0.06870388239622116 = 0.003633228363469243 + 0.01 * 6.507065296173096
Epoch 880, val loss: 1.0724821090698242
Epoch 890, training loss: 0.06836998462677002 = 0.003543626284226775 + 0.01 * 6.482636451721191
Epoch 890, val loss: 1.0763347148895264
Epoch 900, training loss: 0.06841424852609634 = 0.00345798977650702 + 0.01 * 6.495625972747803
Epoch 900, val loss: 1.0801200866699219
Epoch 910, training loss: 0.06818538159132004 = 0.0033761856611818075 + 0.01 * 6.480919361114502
Epoch 910, val loss: 1.0838195085525513
Epoch 920, training loss: 0.06851587444543839 = 0.00329784513451159 + 0.01 * 6.521803379058838
Epoch 920, val loss: 1.0874643325805664
Epoch 930, training loss: 0.06786413490772247 = 0.0032228915952146053 + 0.01 * 6.46412467956543
Epoch 930, val loss: 1.0909911394119263
Epoch 940, training loss: 0.06790103018283844 = 0.003151168115437031 + 0.01 * 6.4749860763549805
Epoch 940, val loss: 1.0944174528121948
Epoch 950, training loss: 0.06783687323331833 = 0.0030826928559690714 + 0.01 * 6.4754180908203125
Epoch 950, val loss: 1.0978453159332275
Epoch 960, training loss: 0.06769068539142609 = 0.003016817383468151 + 0.01 * 6.4673871994018555
Epoch 960, val loss: 1.1011639833450317
Epoch 970, training loss: 0.06752273440361023 = 0.002953622490167618 + 0.01 * 6.456911563873291
Epoch 970, val loss: 1.1045079231262207
Epoch 980, training loss: 0.06755588948726654 = 0.002892824122682214 + 0.01 * 6.466306686401367
Epoch 980, val loss: 1.1077625751495361
Epoch 990, training loss: 0.06735211610794067 = 0.002834499580785632 + 0.01 * 6.451761722564697
Epoch 990, val loss: 1.1109654903411865
Epoch 1000, training loss: 0.06712706387042999 = 0.0027782812248915434 + 0.01 * 6.434877872467041
Epoch 1000, val loss: 1.1141055822372437
Epoch 1010, training loss: 0.0671134665608406 = 0.0027243776712566614 + 0.01 * 6.438908576965332
Epoch 1010, val loss: 1.1172261238098145
Epoch 1020, training loss: 0.06727580726146698 = 0.0026725681964308023 + 0.01 * 6.460324287414551
Epoch 1020, val loss: 1.1202808618545532
Epoch 1030, training loss: 0.06724224239587784 = 0.0026225701440125704 + 0.01 * 6.461967468261719
Epoch 1030, val loss: 1.1232311725616455
Epoch 1040, training loss: 0.06692279130220413 = 0.002574557438492775 + 0.01 * 6.434823989868164
Epoch 1040, val loss: 1.1261903047561646
Epoch 1050, training loss: 0.06687480211257935 = 0.00252821808680892 + 0.01 * 6.434658050537109
Epoch 1050, val loss: 1.1290948390960693
Epoch 1060, training loss: 0.06688962131738663 = 0.002483548829331994 + 0.01 * 6.44060754776001
Epoch 1060, val loss: 1.1319520473480225
Epoch 1070, training loss: 0.06670968979597092 = 0.0024403983261436224 + 0.01 * 6.426928997039795
Epoch 1070, val loss: 1.1347616910934448
Epoch 1080, training loss: 0.06644824147224426 = 0.002398793352767825 + 0.01 * 6.404944896697998
Epoch 1080, val loss: 1.1375317573547363
Epoch 1090, training loss: 0.06649591773748398 = 0.0023586826864629984 + 0.01 * 6.413723468780518
Epoch 1090, val loss: 1.1402301788330078
Epoch 1100, training loss: 0.06631610542535782 = 0.0023199524730443954 + 0.01 * 6.399615287780762
Epoch 1100, val loss: 1.1429656744003296
Epoch 1110, training loss: 0.0663193091750145 = 0.002282488625496626 + 0.01 * 6.403682231903076
Epoch 1110, val loss: 1.1456044912338257
Epoch 1120, training loss: 0.06622038781642914 = 0.002246294869109988 + 0.01 * 6.397408962249756
Epoch 1120, val loss: 1.1482380628585815
Epoch 1130, training loss: 0.06603812426328659 = 0.0022112431470304728 + 0.01 * 6.382688045501709
Epoch 1130, val loss: 1.1507787704467773
Epoch 1140, training loss: 0.0663495808839798 = 0.002177346497774124 + 0.01 * 6.4172234535217285
Epoch 1140, val loss: 1.1533761024475098
Epoch 1150, training loss: 0.06624317169189453 = 0.002144485479220748 + 0.01 * 6.4098687171936035
Epoch 1150, val loss: 1.155821681022644
Epoch 1160, training loss: 0.066081203520298 = 0.00211282167583704 + 0.01 * 6.396838188171387
Epoch 1160, val loss: 1.1583102941513062
Epoch 1170, training loss: 0.06584636867046356 = 0.0020820016507059336 + 0.01 * 6.376436233520508
Epoch 1170, val loss: 1.1607154607772827
Epoch 1180, training loss: 0.06598172336816788 = 0.0020523546263575554 + 0.01 * 6.392937183380127
Epoch 1180, val loss: 1.163097620010376
Epoch 1190, training loss: 0.0657806545495987 = 0.002023483859375119 + 0.01 * 6.3757171630859375
Epoch 1190, val loss: 1.1655298471450806
Epoch 1200, training loss: 0.06571038067340851 = 0.001995489466935396 + 0.01 * 6.37148904800415
Epoch 1200, val loss: 1.1678358316421509
Epoch 1210, training loss: 0.06559360027313232 = 0.001968374475836754 + 0.01 * 6.362522602081299
Epoch 1210, val loss: 1.1701768636703491
Epoch 1220, training loss: 0.06565628200769424 = 0.0019420721800997853 + 0.01 * 6.371420860290527
Epoch 1220, val loss: 1.1725064516067505
Epoch 1230, training loss: 0.06555172801017761 = 0.001916491542942822 + 0.01 * 6.363523960113525
Epoch 1230, val loss: 1.174710750579834
Epoch 1240, training loss: 0.06556255370378494 = 0.0018916483968496323 + 0.01 * 6.367090702056885
Epoch 1240, val loss: 1.1769989728927612
Epoch 1250, training loss: 0.06552586704492569 = 0.0018675634637475014 + 0.01 * 6.365830421447754
Epoch 1250, val loss: 1.1791610717773438
Epoch 1260, training loss: 0.06538590043783188 = 0.0018441799329593778 + 0.01 * 6.3541717529296875
Epoch 1260, val loss: 1.1813918352127075
Epoch 1270, training loss: 0.06529048830270767 = 0.001821404555812478 + 0.01 * 6.3469085693359375
Epoch 1270, val loss: 1.1835740804672241
Epoch 1280, training loss: 0.06534479558467865 = 0.0017992075299844146 + 0.01 * 6.354558944702148
Epoch 1280, val loss: 1.1856904029846191
Epoch 1290, training loss: 0.06521906703710556 = 0.0017776652239263058 + 0.01 * 6.344140529632568
Epoch 1290, val loss: 1.1878994703292847
Epoch 1300, training loss: 0.06522907316684723 = 0.0017566783353686333 + 0.01 * 6.347239971160889
Epoch 1300, val loss: 1.1899141073226929
Epoch 1310, training loss: 0.06520765274763107 = 0.0017362511716783047 + 0.01 * 6.347139835357666
Epoch 1310, val loss: 1.1920511722564697
Epoch 1320, training loss: 0.06501889228820801 = 0.0017163690645247698 + 0.01 * 6.330252647399902
Epoch 1320, val loss: 1.1941145658493042
Epoch 1330, training loss: 0.06498347967863083 = 0.0016969909192994237 + 0.01 * 6.328649520874023
Epoch 1330, val loss: 1.1960870027542114
Epoch 1340, training loss: 0.06504006683826447 = 0.0016781006706878543 + 0.01 * 6.336196422576904
Epoch 1340, val loss: 1.1981021165847778
Epoch 1350, training loss: 0.06506514549255371 = 0.001659868867136538 + 0.01 * 6.340527534484863
Epoch 1350, val loss: 1.2000937461853027
Epoch 1360, training loss: 0.06486684828996658 = 0.0016419400926679373 + 0.01 * 6.322491645812988
Epoch 1360, val loss: 1.2020787000656128
Epoch 1370, training loss: 0.06506534665822983 = 0.0016245278529822826 + 0.01 * 6.344082355499268
Epoch 1370, val loss: 1.2040451765060425
Epoch 1380, training loss: 0.06480631232261658 = 0.0016074356390163302 + 0.01 * 6.319888114929199
Epoch 1380, val loss: 1.205922245979309
Epoch 1390, training loss: 0.06477241963148117 = 0.0015908717177808285 + 0.01 * 6.318155288696289
Epoch 1390, val loss: 1.2079229354858398
Epoch 1400, training loss: 0.06483573466539383 = 0.0015746616991236806 + 0.01 * 6.326107025146484
Epoch 1400, val loss: 1.2098066806793213
Epoch 1410, training loss: 0.06482302397489548 = 0.001558812102302909 + 0.01 * 6.326421737670898
Epoch 1410, val loss: 1.2117236852645874
Epoch 1420, training loss: 0.06483227759599686 = 0.0015433378284797072 + 0.01 * 6.328894138336182
Epoch 1420, val loss: 1.213620901107788
Epoch 1430, training loss: 0.06458546966314316 = 0.0015282545937225223 + 0.01 * 6.305721282958984
Epoch 1430, val loss: 1.215458869934082
Epoch 1440, training loss: 0.06468917429447174 = 0.001513532130047679 + 0.01 * 6.317564010620117
Epoch 1440, val loss: 1.2173643112182617
Epoch 1450, training loss: 0.06456221640110016 = 0.0014991554198786616 + 0.01 * 6.3063063621521
Epoch 1450, val loss: 1.2191764116287231
Epoch 1460, training loss: 0.06467725336551666 = 0.0014851372689008713 + 0.01 * 6.319211483001709
Epoch 1460, val loss: 1.22102689743042
Epoch 1470, training loss: 0.06462836265563965 = 0.001471423776820302 + 0.01 * 6.315694332122803
Epoch 1470, val loss: 1.2228105068206787
Epoch 1480, training loss: 0.06435759365558624 = 0.001457955571822822 + 0.01 * 6.28996467590332
Epoch 1480, val loss: 1.2246387004852295
Epoch 1490, training loss: 0.06444915384054184 = 0.0014448219444602728 + 0.01 * 6.300433158874512
Epoch 1490, val loss: 1.2264326810836792
Epoch 1500, training loss: 0.06443584710359573 = 0.0014320339541882277 + 0.01 * 6.300381660461426
Epoch 1500, val loss: 1.2282319068908691
Epoch 1510, training loss: 0.06424761563539505 = 0.001419472275301814 + 0.01 * 6.2828145027160645
Epoch 1510, val loss: 1.229949712753296
Epoch 1520, training loss: 0.06420711427927017 = 0.0014071828918531537 + 0.01 * 6.279993057250977
Epoch 1520, val loss: 1.2317155599594116
Epoch 1530, training loss: 0.06457197666168213 = 0.0013951912987977266 + 0.01 * 6.317678928375244
Epoch 1530, val loss: 1.2334524393081665
Epoch 1540, training loss: 0.06454991549253464 = 0.001383323105983436 + 0.01 * 6.316659450531006
Epoch 1540, val loss: 1.2351386547088623
Epoch 1550, training loss: 0.06416637450456619 = 0.0013717750553041697 + 0.01 * 6.2794599533081055
Epoch 1550, val loss: 1.236885905265808
Epoch 1560, training loss: 0.06431663036346436 = 0.0013604490086436272 + 0.01 * 6.295618534088135
Epoch 1560, val loss: 1.2385778427124023
Epoch 1570, training loss: 0.0641774982213974 = 0.00134934950619936 + 0.01 * 6.2828145027160645
Epoch 1570, val loss: 1.2402857542037964
Epoch 1580, training loss: 0.0641617402434349 = 0.0013384897029027343 + 0.01 * 6.282325267791748
Epoch 1580, val loss: 1.2419754266738892
Epoch 1590, training loss: 0.06445632129907608 = 0.0013278551632538438 + 0.01 * 6.312846660614014
Epoch 1590, val loss: 1.2436652183532715
Epoch 1600, training loss: 0.06423839181661606 = 0.001317403162829578 + 0.01 * 6.2920989990234375
Epoch 1600, val loss: 1.245277762413025
Epoch 1610, training loss: 0.0641988217830658 = 0.0013071518624201417 + 0.01 * 6.2891669273376465
Epoch 1610, val loss: 1.2469383478164673
Epoch 1620, training loss: 0.06406506896018982 = 0.001297144335694611 + 0.01 * 6.276792526245117
Epoch 1620, val loss: 1.2485605478286743
Epoch 1630, training loss: 0.0639442652463913 = 0.0012872923398390412 + 0.01 * 6.265697956085205
Epoch 1630, val loss: 1.250153660774231
Epoch 1640, training loss: 0.064028300344944 = 0.0012776318471878767 + 0.01 * 6.27506685256958
Epoch 1640, val loss: 1.2517101764678955
Epoch 1650, training loss: 0.06398837268352509 = 0.0012681790394708514 + 0.01 * 6.272019863128662
Epoch 1650, val loss: 1.2533060312271118
Epoch 1660, training loss: 0.06403718888759613 = 0.0012589292600750923 + 0.01 * 6.277825832366943
Epoch 1660, val loss: 1.2548837661743164
Epoch 1670, training loss: 0.06384032964706421 = 0.0012498174328356981 + 0.01 * 6.259051322937012
Epoch 1670, val loss: 1.2565056085586548
Epoch 1680, training loss: 0.06376313418149948 = 0.0012408930342644453 + 0.01 * 6.252224445343018
Epoch 1680, val loss: 1.2580599784851074
Epoch 1690, training loss: 0.06384756416082382 = 0.001232121721841395 + 0.01 * 6.2615437507629395
Epoch 1690, val loss: 1.2596278190612793
Epoch 1700, training loss: 0.06395165622234344 = 0.001223487895913422 + 0.01 * 6.272817134857178
Epoch 1700, val loss: 1.261197805404663
Epoch 1710, training loss: 0.06387639045715332 = 0.0012150444090366364 + 0.01 * 6.266134738922119
Epoch 1710, val loss: 1.2627052068710327
Epoch 1720, training loss: 0.06373519450426102 = 0.0012066509807482362 + 0.01 * 6.252854347229004
Epoch 1720, val loss: 1.2642375230789185
Epoch 1730, training loss: 0.06372392177581787 = 0.0011984851444140077 + 0.01 * 6.2525434494018555
Epoch 1730, val loss: 1.2657297849655151
Epoch 1740, training loss: 0.06371105462312698 = 0.0011904302518814802 + 0.01 * 6.252062797546387
Epoch 1740, val loss: 1.2671749591827393
Epoch 1750, training loss: 0.06385979056358337 = 0.0011825209949165583 + 0.01 * 6.267727375030518
Epoch 1750, val loss: 1.2686494588851929
Epoch 1760, training loss: 0.06367823481559753 = 0.001174812437966466 + 0.01 * 6.250341892242432
Epoch 1760, val loss: 1.2701520919799805
Epoch 1770, training loss: 0.06370421499013901 = 0.0011671953834593296 + 0.01 * 6.253702163696289
Epoch 1770, val loss: 1.2715892791748047
Epoch 1780, training loss: 0.06372383236885071 = 0.0011596550466492772 + 0.01 * 6.256418228149414
Epoch 1780, val loss: 1.273045539855957
Epoch 1790, training loss: 0.06352673470973969 = 0.0011522989952936769 + 0.01 * 6.237443923950195
Epoch 1790, val loss: 1.2744834423065186
Epoch 1800, training loss: 0.0635625422000885 = 0.0011450675083324313 + 0.01 * 6.2417473793029785
Epoch 1800, val loss: 1.2759673595428467
Epoch 1810, training loss: 0.06349369883537292 = 0.001137909246608615 + 0.01 * 6.235579013824463
Epoch 1810, val loss: 1.2773667573928833
Epoch 1820, training loss: 0.06375894695520401 = 0.0011308679822832346 + 0.01 * 6.262807369232178
Epoch 1820, val loss: 1.2788095474243164
Epoch 1830, training loss: 0.06378123164176941 = 0.001124052214436233 + 0.01 * 6.26571798324585
Epoch 1830, val loss: 1.2802276611328125
Epoch 1840, training loss: 0.06356333196163177 = 0.0011171938385814428 + 0.01 * 6.2446136474609375
Epoch 1840, val loss: 1.2815515995025635
Epoch 1850, training loss: 0.06355133652687073 = 0.0011105408193543553 + 0.01 * 6.24407958984375
Epoch 1850, val loss: 1.2829409837722778
Epoch 1860, training loss: 0.06348729133605957 = 0.0011039604432880878 + 0.01 * 6.238333225250244
Epoch 1860, val loss: 1.2843327522277832
Epoch 1870, training loss: 0.06359127908945084 = 0.0010975135955959558 + 0.01 * 6.2493767738342285
Epoch 1870, val loss: 1.2857000827789307
Epoch 1880, training loss: 0.06348921358585358 = 0.0010911598801612854 + 0.01 * 6.239805698394775
Epoch 1880, val loss: 1.2870945930480957
Epoch 1890, training loss: 0.06351833790540695 = 0.0010848783422261477 + 0.01 * 6.243346214294434
Epoch 1890, val loss: 1.2884180545806885
Epoch 1900, training loss: 0.06357801705598831 = 0.0010787247447296977 + 0.01 * 6.249929428100586
Epoch 1900, val loss: 1.2897799015045166
Epoch 1910, training loss: 0.06333449482917786 = 0.001072656479664147 + 0.01 * 6.226183891296387
Epoch 1910, val loss: 1.291094422340393
Epoch 1920, training loss: 0.06338869780302048 = 0.0010666715679690242 + 0.01 * 6.232203006744385
Epoch 1920, val loss: 1.2923898696899414
Epoch 1930, training loss: 0.06340336054563522 = 0.0010608172742649913 + 0.01 * 6.234254837036133
Epoch 1930, val loss: 1.2937084436416626
Epoch 1940, training loss: 0.0634239986538887 = 0.0010550079168751836 + 0.01 * 6.236899375915527
Epoch 1940, val loss: 1.2950332164764404
Epoch 1950, training loss: 0.06324664503335953 = 0.0010492734145373106 + 0.01 * 6.219737529754639
Epoch 1950, val loss: 1.2963480949401855
Epoch 1960, training loss: 0.0633339211344719 = 0.0010436411248520017 + 0.01 * 6.229028224945068
Epoch 1960, val loss: 1.2976168394088745
Epoch 1970, training loss: 0.06329793483018875 = 0.0010380374733358622 + 0.01 * 6.22598934173584
Epoch 1970, val loss: 1.2989133596420288
Epoch 1980, training loss: 0.06343512237071991 = 0.0010325927287340164 + 0.01 * 6.240253448486328
Epoch 1980, val loss: 1.3002179861068726
Epoch 1990, training loss: 0.0633319839835167 = 0.0010271088685840368 + 0.01 * 6.23048734664917
Epoch 1990, val loss: 1.3015002012252808
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.69373, 0.21523, Accuracy:0.80247, 0.01823
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11588])
remove edge: torch.Size([2, 9528])
updated graph: torch.Size([2, 10560])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98278, 0.00870, Accuracy:0.82963, 0.00800
Begin epxeriment: cont_weight: 0.01 epoch:2000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0409510135650635 = 1.9572136402130127 + 0.01 * 8.373746871948242
Epoch 0, val loss: 1.9571336507797241
Epoch 10, training loss: 2.031053066253662 = 1.9473166465759277 + 0.01 * 8.373640060424805
Epoch 10, val loss: 1.947885513305664
Epoch 20, training loss: 2.019019365310669 = 1.9352868795394897 + 0.01 * 8.373238563537598
Epoch 20, val loss: 1.9362679719924927
Epoch 30, training loss: 2.002140760421753 = 1.9184207916259766 + 0.01 * 8.372005462646484
Epoch 30, val loss: 1.9196045398712158
Epoch 40, training loss: 1.9769713878631592 = 1.8933225870132446 + 0.01 * 8.364876747131348
Epoch 40, val loss: 1.894718885421753
Epoch 50, training loss: 1.9402930736541748 = 1.8571382761001587 + 0.01 * 8.315484046936035
Epoch 50, val loss: 1.8600772619247437
Epoch 60, training loss: 1.8927464485168457 = 1.8123431205749512 + 0.01 * 8.040337562561035
Epoch 60, val loss: 1.8210313320159912
Epoch 70, training loss: 1.8433436155319214 = 1.766985535621643 + 0.01 * 7.635804653167725
Epoch 70, val loss: 1.786103367805481
Epoch 80, training loss: 1.7871689796447754 = 1.7152529954910278 + 0.01 * 7.1915998458862305
Epoch 80, val loss: 1.7442830801010132
Epoch 90, training loss: 1.716284155845642 = 1.6462931632995605 + 0.01 * 6.999094009399414
Epoch 90, val loss: 1.6850526332855225
Epoch 100, training loss: 1.6268994808197021 = 1.557492971420288 + 0.01 * 6.940647125244141
Epoch 100, val loss: 1.6088523864746094
Epoch 110, training loss: 1.5239611864089966 = 1.454943060874939 + 0.01 * 6.901809215545654
Epoch 110, val loss: 1.5245931148529053
Epoch 120, training loss: 1.416543960571289 = 1.3476142883300781 + 0.01 * 6.892965793609619
Epoch 120, val loss: 1.439423680305481
Epoch 130, training loss: 1.3093876838684082 = 1.240546464920044 + 0.01 * 6.884118556976318
Epoch 130, val loss: 1.3554046154022217
Epoch 140, training loss: 1.205390214920044 = 1.1366406679153442 + 0.01 * 6.874953269958496
Epoch 140, val loss: 1.275073528289795
Epoch 150, training loss: 1.106294870376587 = 1.037649154663086 + 0.01 * 6.864566326141357
Epoch 150, val loss: 1.1965956687927246
Epoch 160, training loss: 1.012971043586731 = 0.9444411396980286 + 0.01 * 6.852991580963135
Epoch 160, val loss: 1.123046875
Epoch 170, training loss: 0.9254189133644104 = 0.8570165038108826 + 0.01 * 6.840240001678467
Epoch 170, val loss: 1.054399013519287
Epoch 180, training loss: 0.8443403244018555 = 0.7760764956474304 + 0.01 * 6.8263840675354
Epoch 180, val loss: 0.9927271008491516
Epoch 190, training loss: 0.7701758146286011 = 0.7020503282546997 + 0.01 * 6.812551021575928
Epoch 190, val loss: 0.9380664825439453
Epoch 200, training loss: 0.7022683620452881 = 0.6342639923095703 + 0.01 * 6.800439357757568
Epoch 200, val loss: 0.8896816968917847
Epoch 210, training loss: 0.6393061876296997 = 0.5713793635368347 + 0.01 * 6.792684555053711
Epoch 210, val loss: 0.8467223644256592
Epoch 220, training loss: 0.5803565979003906 = 0.5125026702880859 + 0.01 * 6.785394668579102
Epoch 220, val loss: 0.8090567588806152
Epoch 230, training loss: 0.5253836512565613 = 0.45758146047592163 + 0.01 * 6.7802205085754395
Epoch 230, val loss: 0.7769729495048523
Epoch 240, training loss: 0.4751273989677429 = 0.4073694050312042 + 0.01 * 6.77579927444458
Epoch 240, val loss: 0.7505919933319092
Epoch 250, training loss: 0.43023252487182617 = 0.3625185787677765 + 0.01 * 6.771394729614258
Epoch 250, val loss: 0.7303445935249329
Epoch 260, training loss: 0.39098888635635376 = 0.32331976294517517 + 0.01 * 6.766911029815674
Epoch 260, val loss: 0.7163571715354919
Epoch 270, training loss: 0.3569474220275879 = 0.2893102765083313 + 0.01 * 6.76371431350708
Epoch 270, val loss: 0.7078691124916077
Epoch 280, training loss: 0.3272937536239624 = 0.25971487164497375 + 0.01 * 6.757889270782471
Epoch 280, val loss: 0.7042340040206909
Epoch 290, training loss: 0.30107682943344116 = 0.23355184495449066 + 0.01 * 6.752500057220459
Epoch 290, val loss: 0.7049558758735657
Epoch 300, training loss: 0.2775425314903259 = 0.2100687474012375 + 0.01 * 6.747378826141357
Epoch 300, val loss: 0.7095662355422974
Epoch 310, training loss: 0.2561745047569275 = 0.18877409398555756 + 0.01 * 6.740042209625244
Epoch 310, val loss: 0.7177713513374329
Epoch 320, training loss: 0.23681041598320007 = 0.1694786250591278 + 0.01 * 6.733179569244385
Epoch 320, val loss: 0.7290522456169128
Epoch 330, training loss: 0.219323992729187 = 0.15206678211688995 + 0.01 * 6.72572135925293
Epoch 330, val loss: 0.7425805926322937
Epoch 340, training loss: 0.20363694429397583 = 0.13645543158054352 + 0.01 * 6.718150615692139
Epoch 340, val loss: 0.7576419711112976
Epoch 350, training loss: 0.1897614300251007 = 0.12254142016172409 + 0.01 * 6.722000598907471
Epoch 350, val loss: 0.7737911939620972
Epoch 360, training loss: 0.17723748087882996 = 0.1101810559630394 + 0.01 * 6.705642223358154
Epoch 360, val loss: 0.7905749082565308
Epoch 370, training loss: 0.16614164412021637 = 0.09917853772640228 + 0.01 * 6.696310997009277
Epoch 370, val loss: 0.8078562617301941
Epoch 380, training loss: 0.15620917081832886 = 0.08935385197401047 + 0.01 * 6.6855316162109375
Epoch 380, val loss: 0.8254513144493103
Epoch 390, training loss: 0.14734739065170288 = 0.08055662363767624 + 0.01 * 6.679077625274658
Epoch 390, val loss: 0.8431829810142517
Epoch 400, training loss: 0.13948166370391846 = 0.07267104834318161 + 0.01 * 6.6810622215271
Epoch 400, val loss: 0.8611358404159546
Epoch 410, training loss: 0.1322961449623108 = 0.06560003757476807 + 0.01 * 6.669610023498535
Epoch 410, val loss: 0.8791095018386841
Epoch 420, training loss: 0.12587672472000122 = 0.059265121817588806 + 0.01 * 6.661159515380859
Epoch 420, val loss: 0.8972423076629639
Epoch 430, training loss: 0.1201658546924591 = 0.05359866842627525 + 0.01 * 6.656718730926514
Epoch 430, val loss: 0.9153818488121033
Epoch 440, training loss: 0.11506875604391098 = 0.04854099452495575 + 0.01 * 6.65277624130249
Epoch 440, val loss: 0.9335314631462097
Epoch 450, training loss: 0.11064904928207397 = 0.04404037073254585 + 0.01 * 6.660867691040039
Epoch 450, val loss: 0.9515138864517212
Epoch 460, training loss: 0.1065489649772644 = 0.040043581277132034 + 0.01 * 6.650538444519043
Epoch 460, val loss: 0.969352126121521
Epoch 470, training loss: 0.10293431580066681 = 0.03649502620100975 + 0.01 * 6.643929481506348
Epoch 470, val loss: 0.9869183301925659
Epoch 480, training loss: 0.09973287582397461 = 0.03334299847483635 + 0.01 * 6.6389875411987305
Epoch 480, val loss: 1.0042383670806885
Epoch 490, training loss: 0.09689785540103912 = 0.030542558059096336 + 0.01 * 6.635529518127441
Epoch 490, val loss: 1.0210933685302734
Epoch 500, training loss: 0.09450655430555344 = 0.028051277622580528 + 0.01 * 6.6455278396606445
Epoch 500, val loss: 1.0376709699630737
Epoch 510, training loss: 0.09213927388191223 = 0.025832463055849075 + 0.01 * 6.630680561065674
Epoch 510, val loss: 1.0537497997283936
Epoch 520, training loss: 0.09011393785476685 = 0.023852134123444557 + 0.01 * 6.626180648803711
Epoch 520, val loss: 1.0692873001098633
Epoch 530, training loss: 0.08828961104154587 = 0.022080495953559875 + 0.01 * 6.620911598205566
Epoch 530, val loss: 1.0845659971237183
Epoch 540, training loss: 0.0867016464471817 = 0.02049257420003414 + 0.01 * 6.620906829833984
Epoch 540, val loss: 1.0993380546569824
Epoch 550, training loss: 0.08519528806209564 = 0.01906641013920307 + 0.01 * 6.612887859344482
Epoch 550, val loss: 1.1136951446533203
Epoch 560, training loss: 0.08387790620326996 = 0.017782066017389297 + 0.01 * 6.609584808349609
Epoch 560, val loss: 1.1276205778121948
Epoch 570, training loss: 0.08268358558416367 = 0.016622165217995644 + 0.01 * 6.606142520904541
Epoch 570, val loss: 1.141217589378357
Epoch 580, training loss: 0.081767238676548 = 0.01557289157062769 + 0.01 * 6.6194353103637695
Epoch 580, val loss: 1.1543192863464355
Epoch 590, training loss: 0.08060044050216675 = 0.014621272683143616 + 0.01 * 6.597917079925537
Epoch 590, val loss: 1.167093276977539
Epoch 600, training loss: 0.07966913282871246 = 0.013756122440099716 + 0.01 * 6.591300964355469
Epoch 600, val loss: 1.1795029640197754
Epoch 610, training loss: 0.07884297519922256 = 0.012967086397111416 + 0.01 * 6.587589263916016
Epoch 610, val loss: 1.191587209701538
Epoch 620, training loss: 0.07827864587306976 = 0.012245794758200645 + 0.01 * 6.60328483581543
Epoch 620, val loss: 1.203317403793335
Epoch 630, training loss: 0.077431820333004 = 0.011585661210119724 + 0.01 * 6.584616184234619
Epoch 630, val loss: 1.2146921157836914
Epoch 640, training loss: 0.0767391175031662 = 0.010979761369526386 + 0.01 * 6.575935363769531
Epoch 640, val loss: 1.225721001625061
Epoch 650, training loss: 0.0765489861369133 = 0.010422765277326107 + 0.01 * 6.612622261047363
Epoch 650, val loss: 1.2364914417266846
Epoch 660, training loss: 0.07564210891723633 = 0.009909515269100666 + 0.01 * 6.573259353637695
Epoch 660, val loss: 1.2469737529754639
Epoch 670, training loss: 0.07509858906269073 = 0.009435901418328285 + 0.01 * 6.5662689208984375
Epoch 670, val loss: 1.2571821212768555
Epoch 680, training loss: 0.07463124394416809 = 0.008997405879199505 + 0.01 * 6.563384056091309
Epoch 680, val loss: 1.2670652866363525
Epoch 690, training loss: 0.07416146993637085 = 0.008590640500187874 + 0.01 * 6.5570831298828125
Epoch 690, val loss: 1.2767258882522583
Epoch 700, training loss: 0.07379132509231567 = 0.008212843909859657 + 0.01 * 6.5578484535217285
Epoch 700, val loss: 1.2861356735229492
Epoch 710, training loss: 0.07338497787714005 = 0.007861522026360035 + 0.01 * 6.552345275878906
Epoch 710, val loss: 1.2952864170074463
Epoch 720, training loss: 0.07312595844268799 = 0.007534113712608814 + 0.01 * 6.5591840744018555
Epoch 720, val loss: 1.304255485534668
Epoch 730, training loss: 0.07273636758327484 = 0.007228521630167961 + 0.01 * 6.550784587860107
Epoch 730, val loss: 1.3129241466522217
Epoch 740, training loss: 0.07237077504396439 = 0.006942832842469215 + 0.01 * 6.542794704437256
Epoch 740, val loss: 1.3214296102523804
Epoch 750, training loss: 0.0721026286482811 = 0.0066752927377820015 + 0.01 * 6.542733192443848
Epoch 750, val loss: 1.329728126525879
Epoch 760, training loss: 0.0719294399023056 = 0.0064245364628732204 + 0.01 * 6.550489902496338
Epoch 760, val loss: 1.337752103805542
Epoch 770, training loss: 0.071677066385746 = 0.006189034320414066 + 0.01 * 6.548803806304932
Epoch 770, val loss: 1.3457096815109253
Epoch 780, training loss: 0.071268729865551 = 0.005967975594103336 + 0.01 * 6.530076026916504
Epoch 780, val loss: 1.3533464670181274
Epoch 790, training loss: 0.07110080122947693 = 0.005759796593338251 + 0.01 * 6.534100532531738
Epoch 790, val loss: 1.360758662223816
Epoch 800, training loss: 0.0709623172879219 = 0.005563473328948021 + 0.01 * 6.539884090423584
Epoch 800, val loss: 1.3682044744491577
Epoch 810, training loss: 0.07066651433706284 = 0.005378651898354292 + 0.01 * 6.528786659240723
Epoch 810, val loss: 1.375218152999878
Epoch 820, training loss: 0.07055097073316574 = 0.005203673616051674 + 0.01 * 6.534729957580566
Epoch 820, val loss: 1.3822365999221802
Epoch 830, training loss: 0.07022719830274582 = 0.005038378294557333 + 0.01 * 6.5188822746276855
Epoch 830, val loss: 1.3890926837921143
Epoch 840, training loss: 0.07028159499168396 = 0.0048820944502949715 + 0.01 * 6.539949893951416
Epoch 840, val loss: 1.395708680152893
Epoch 850, training loss: 0.06987593322992325 = 0.004734081216156483 + 0.01 * 6.514185428619385
Epoch 850, val loss: 1.4021550416946411
Epoch 860, training loss: 0.06986554712057114 = 0.004592835437506437 + 0.01 * 6.527271270751953
Epoch 860, val loss: 1.4085547924041748
Epoch 870, training loss: 0.06946533918380737 = 0.004459569696336985 + 0.01 * 6.500577449798584
Epoch 870, val loss: 1.4147274494171143
Epoch 880, training loss: 0.06942851841449738 = 0.004332384560257196 + 0.01 * 6.509613037109375
Epoch 880, val loss: 1.4207024574279785
Epoch 890, training loss: 0.06922628730535507 = 0.004211587365716696 + 0.01 * 6.50147008895874
Epoch 890, val loss: 1.4267208576202393
Epoch 900, training loss: 0.06893695145845413 = 0.00409617880359292 + 0.01 * 6.484077453613281
Epoch 900, val loss: 1.4324949979782104
Epoch 910, training loss: 0.06906353682279587 = 0.003986268769949675 + 0.01 * 6.507727146148682
Epoch 910, val loss: 1.4381884336471558
Epoch 920, training loss: 0.06874284893274307 = 0.0038815506268292665 + 0.01 * 6.4861297607421875
Epoch 920, val loss: 1.4438226222991943
Epoch 930, training loss: 0.06900569051504135 = 0.003781078150495887 + 0.01 * 6.522461414337158
Epoch 930, val loss: 1.4491850137710571
Epoch 940, training loss: 0.06847092509269714 = 0.003685685805976391 + 0.01 * 6.478524684906006
Epoch 940, val loss: 1.454491376876831
Epoch 950, training loss: 0.06837507337331772 = 0.003593916306272149 + 0.01 * 6.478116035461426
Epoch 950, val loss: 1.4595659971237183
Epoch 960, training loss: 0.06820565462112427 = 0.003506092820316553 + 0.01 * 6.469955921173096
Epoch 960, val loss: 1.464835286140442
Epoch 970, training loss: 0.0680992379784584 = 0.0034218004439026117 + 0.01 * 6.46774435043335
Epoch 970, val loss: 1.4697365760803223
Epoch 980, training loss: 0.06793200224637985 = 0.0033412559423595667 + 0.01 * 6.459074974060059
Epoch 980, val loss: 1.474751353263855
Epoch 990, training loss: 0.06789957731962204 = 0.0032640714198350906 + 0.01 * 6.463551044464111
Epoch 990, val loss: 1.4793721437454224
Epoch 1000, training loss: 0.06773039698600769 = 0.003190483432263136 + 0.01 * 6.453991889953613
Epoch 1000, val loss: 1.4841787815093994
Epoch 1010, training loss: 0.06785228103399277 = 0.0031197266653180122 + 0.01 * 6.473255157470703
Epoch 1010, val loss: 1.4886635541915894
Epoch 1020, training loss: 0.06766632199287415 = 0.003051918698474765 + 0.01 * 6.461440086364746
Epoch 1020, val loss: 1.4932719469070435
Epoch 1030, training loss: 0.06804419308900833 = 0.0029870006255805492 + 0.01 * 6.5057196617126465
Epoch 1030, val loss: 1.497501015663147
Epoch 1040, training loss: 0.06749121099710464 = 0.002924554515630007 + 0.01 * 6.456665992736816
Epoch 1040, val loss: 1.5018728971481323
Epoch 1050, training loss: 0.06723321974277496 = 0.0028644311241805553 + 0.01 * 6.4368791580200195
Epoch 1050, val loss: 1.506015419960022
Epoch 1060, training loss: 0.06707404553890228 = 0.002806787844747305 + 0.01 * 6.4267258644104
Epoch 1060, val loss: 1.5102442502975464
Epoch 1070, training loss: 0.06724869459867477 = 0.002751296618953347 + 0.01 * 6.449739456176758
Epoch 1070, val loss: 1.5140998363494873
Epoch 1080, training loss: 0.067066989839077 = 0.0026979425456374884 + 0.01 * 6.4369049072265625
Epoch 1080, val loss: 1.5181574821472168
Epoch 1090, training loss: 0.06691242009401321 = 0.0026464583352208138 + 0.01 * 6.426595687866211
Epoch 1090, val loss: 1.521807074546814
Epoch 1100, training loss: 0.06678695976734161 = 0.002597009763121605 + 0.01 * 6.418994903564453
Epoch 1100, val loss: 1.5257227420806885
Epoch 1110, training loss: 0.06671413034200668 = 0.0025491653941571712 + 0.01 * 6.416496753692627
Epoch 1110, val loss: 1.5293176174163818
Epoch 1120, training loss: 0.06672707945108414 = 0.002503103343769908 + 0.01 * 6.422397613525391
Epoch 1120, val loss: 1.532997965812683
Epoch 1130, training loss: 0.06677677482366562 = 0.002458873437717557 + 0.01 * 6.431790828704834
Epoch 1130, val loss: 1.5365440845489502
Epoch 1140, training loss: 0.06647872179746628 = 0.002415914786979556 + 0.01 * 6.406280517578125
Epoch 1140, val loss: 1.5398792028427124
Epoch 1150, training loss: 0.0666995719075203 = 0.0023748748935759068 + 0.01 * 6.432470321655273
Epoch 1150, val loss: 1.543341040611267
Epoch 1160, training loss: 0.0664481520652771 = 0.0023349623661488295 + 0.01 * 6.411319732666016
Epoch 1160, val loss: 1.5466769933700562
Epoch 1170, training loss: 0.06627630442380905 = 0.002296518301591277 + 0.01 * 6.397979259490967
Epoch 1170, val loss: 1.5497705936431885
Epoch 1180, training loss: 0.06629643589258194 = 0.0022594924084842205 + 0.01 * 6.4036946296691895
Epoch 1180, val loss: 1.5530102252960205
Epoch 1190, training loss: 0.06628512591123581 = 0.002223733114078641 + 0.01 * 6.406139373779297
Epoch 1190, val loss: 1.5560530424118042
Epoch 1200, training loss: 0.06622187793254852 = 0.0021889954805374146 + 0.01 * 6.4032883644104
Epoch 1200, val loss: 1.5590699911117554
Epoch 1210, training loss: 0.06617724895477295 = 0.002155375899747014 + 0.01 * 6.402186870574951
Epoch 1210, val loss: 1.5620458126068115
Epoch 1220, training loss: 0.06605774164199829 = 0.00212276098318398 + 0.01 * 6.393498420715332
Epoch 1220, val loss: 1.5649487972259521
Epoch 1230, training loss: 0.06621837615966797 = 0.0020912825129926205 + 0.01 * 6.412709712982178
Epoch 1230, val loss: 1.5678330659866333
Epoch 1240, training loss: 0.06588736176490784 = 0.0020609565544873476 + 0.01 * 6.382640361785889
Epoch 1240, val loss: 1.570495843887329
Epoch 1250, training loss: 0.06598891317844391 = 0.0020314492285251617 + 0.01 * 6.395746231079102
Epoch 1250, val loss: 1.573278546333313
Epoch 1260, training loss: 0.0658608078956604 = 0.002002778695896268 + 0.01 * 6.385803699493408
Epoch 1260, val loss: 1.575932264328003
Epoch 1270, training loss: 0.06582153588533401 = 0.001975143328309059 + 0.01 * 6.384639739990234
Epoch 1270, val loss: 1.5786479711532593
Epoch 1280, training loss: 0.06562293320894241 = 0.0019480279879644513 + 0.01 * 6.367490291595459
Epoch 1280, val loss: 1.5810163021087646
Epoch 1290, training loss: 0.06573852896690369 = 0.001922141294926405 + 0.01 * 6.381639003753662
Epoch 1290, val loss: 1.5836905241012573
Epoch 1300, training loss: 0.06560221314430237 = 0.0018966479692608118 + 0.01 * 6.370556831359863
Epoch 1300, val loss: 1.5860402584075928
Epoch 1310, training loss: 0.06562653928995132 = 0.0018722177483141422 + 0.01 * 6.375432014465332
Epoch 1310, val loss: 1.5885978937149048
Epoch 1320, training loss: 0.06549437344074249 = 0.0018483053427189589 + 0.01 * 6.364606857299805
Epoch 1320, val loss: 1.5909260511398315
Epoch 1330, training loss: 0.06543666869401932 = 0.0018251196015626192 + 0.01 * 6.3611555099487305
Epoch 1330, val loss: 1.5932316780090332
Epoch 1340, training loss: 0.06546448171138763 = 0.0018026126781478524 + 0.01 * 6.36618709564209
Epoch 1340, val loss: 1.59547758102417
Epoch 1350, training loss: 0.06546991318464279 = 0.0017808261327445507 + 0.01 * 6.3689093589782715
Epoch 1350, val loss: 1.597788691520691
Epoch 1360, training loss: 0.06526394188404083 = 0.00175943982321769 + 0.01 * 6.350450038909912
Epoch 1360, val loss: 1.599924921989441
Epoch 1370, training loss: 0.06537889689207077 = 0.0017387502593919635 + 0.01 * 6.364014625549316
Epoch 1370, val loss: 1.6020911931991577
Epoch 1380, training loss: 0.06513234227895737 = 0.0017186551121994853 + 0.01 * 6.341369152069092
Epoch 1380, val loss: 1.6042311191558838
Epoch 1390, training loss: 0.065696582198143 = 0.001699153333902359 + 0.01 * 6.399743556976318
Epoch 1390, val loss: 1.6062700748443604
Epoch 1400, training loss: 0.06505513936281204 = 0.001679968903772533 + 0.01 * 6.337516784667969
Epoch 1400, val loss: 1.6082379817962646
Epoch 1410, training loss: 0.0651342123746872 = 0.0016614131163805723 + 0.01 * 6.347280025482178
Epoch 1410, val loss: 1.6102560758590698
Epoch 1420, training loss: 0.06502114981412888 = 0.0016433964483439922 + 0.01 * 6.337775230407715
Epoch 1420, val loss: 1.6123309135437012
Epoch 1430, training loss: 0.06497209519147873 = 0.0016258929390460253 + 0.01 * 6.334620475769043
Epoch 1430, val loss: 1.614197850227356
Epoch 1440, training loss: 0.06483112275600433 = 0.0016086684772744775 + 0.01 * 6.3222455978393555
Epoch 1440, val loss: 1.6159350872039795
Epoch 1450, training loss: 0.06489367038011551 = 0.0015920326113700867 + 0.01 * 6.330163955688477
Epoch 1450, val loss: 1.6179697513580322
Epoch 1460, training loss: 0.06477703899145126 = 0.0015757372602820396 + 0.01 * 6.320130348205566
Epoch 1460, val loss: 1.6195731163024902
Epoch 1470, training loss: 0.06480258703231812 = 0.0015599323669448495 + 0.01 * 6.324265480041504
Epoch 1470, val loss: 1.6214559078216553
Epoch 1480, training loss: 0.06506813317537308 = 0.0015445739263668656 + 0.01 * 6.352356433868408
Epoch 1480, val loss: 1.62309730052948
Epoch 1490, training loss: 0.06482930481433868 = 0.0015293577453121543 + 0.01 * 6.329995155334473
Epoch 1490, val loss: 1.6247261762619019
Epoch 1500, training loss: 0.0647769346833229 = 0.0015146740479394794 + 0.01 * 6.326226234436035
Epoch 1500, val loss: 1.6264654397964478
Epoch 1510, training loss: 0.0647113099694252 = 0.0015002491418272257 + 0.01 * 6.321106433868408
Epoch 1510, val loss: 1.627942681312561
Epoch 1520, training loss: 0.06495440751314163 = 0.0014862482203170657 + 0.01 * 6.346816062927246
Epoch 1520, val loss: 1.629656434059143
Epoch 1530, training loss: 0.06453830003738403 = 0.001472504111006856 + 0.01 * 6.306580066680908
Epoch 1530, val loss: 1.631001591682434
Epoch 1540, training loss: 0.06473895162343979 = 0.0014592005172744393 + 0.01 * 6.327975273132324
Epoch 1540, val loss: 1.6326172351837158
Epoch 1550, training loss: 0.06447550654411316 = 0.0014461271930485964 + 0.01 * 6.302937984466553
Epoch 1550, val loss: 1.6340872049331665
Epoch 1560, training loss: 0.06450480222702026 = 0.0014333773870021105 + 0.01 * 6.30714225769043
Epoch 1560, val loss: 1.635475516319275
Epoch 1570, training loss: 0.06466463208198547 = 0.0014210487715899944 + 0.01 * 6.3243584632873535
Epoch 1570, val loss: 1.6369327306747437
Epoch 1580, training loss: 0.06434205919504166 = 0.0014088097959756851 + 0.01 * 6.293325424194336
Epoch 1580, val loss: 1.6381839513778687
Epoch 1590, training loss: 0.06447472423315048 = 0.0013969355495646596 + 0.01 * 6.307778835296631
Epoch 1590, val loss: 1.639556646347046
Epoch 1600, training loss: 0.06438525021076202 = 0.0013853602577000856 + 0.01 * 6.299988746643066
Epoch 1600, val loss: 1.6409685611724854
Epoch 1610, training loss: 0.06448052078485489 = 0.0013740089489147067 + 0.01 * 6.310651779174805
Epoch 1610, val loss: 1.6421096324920654
Epoch 1620, training loss: 0.06425201147794724 = 0.0013628799933940172 + 0.01 * 6.288913249969482
Epoch 1620, val loss: 1.6434310674667358
Epoch 1630, training loss: 0.06426049768924713 = 0.001351980259642005 + 0.01 * 6.290851593017578
Epoch 1630, val loss: 1.6446130275726318
Epoch 1640, training loss: 0.06424853205680847 = 0.0013413531705737114 + 0.01 * 6.290718078613281
Epoch 1640, val loss: 1.6458790302276611
Epoch 1650, training loss: 0.0641656145453453 = 0.0013309248024597764 + 0.01 * 6.283468723297119
Epoch 1650, val loss: 1.6468441486358643
Epoch 1660, training loss: 0.0641285702586174 = 0.001320782583206892 + 0.01 * 6.2807793617248535
Epoch 1660, val loss: 1.6481704711914062
Epoch 1670, training loss: 0.06421160697937012 = 0.0013108305865898728 + 0.01 * 6.290077209472656
Epoch 1670, val loss: 1.6491825580596924
Epoch 1680, training loss: 0.06413471698760986 = 0.0013010228285565972 + 0.01 * 6.283369541168213
Epoch 1680, val loss: 1.650363564491272
Epoch 1690, training loss: 0.06410850584506989 = 0.0012914821272715926 + 0.01 * 6.281702518463135
Epoch 1690, val loss: 1.651334524154663
Epoch 1700, training loss: 0.06436360627412796 = 0.0012821104610338807 + 0.01 * 6.308149814605713
Epoch 1700, val loss: 1.6525208950042725
Epoch 1710, training loss: 0.06399890035390854 = 0.0012728330912068486 + 0.01 * 6.272606372833252
Epoch 1710, val loss: 1.6534146070480347
Epoch 1720, training loss: 0.06432560831308365 = 0.0012638210318982601 + 0.01 * 6.306179046630859
Epoch 1720, val loss: 1.6544984579086304
Epoch 1730, training loss: 0.0639805793762207 = 0.0012548986123874784 + 0.01 * 6.2725677490234375
Epoch 1730, val loss: 1.6554902791976929
Epoch 1740, training loss: 0.06407344341278076 = 0.0012462114682421088 + 0.01 * 6.282723426818848
Epoch 1740, val loss: 1.6564759016036987
Epoch 1750, training loss: 0.06418447941541672 = 0.0012376352678984404 + 0.01 * 6.294684410095215
Epoch 1750, val loss: 1.65742027759552
Epoch 1760, training loss: 0.06385709345340729 = 0.0012292537139728665 + 0.01 * 6.262784004211426
Epoch 1760, val loss: 1.6584340333938599
Epoch 1770, training loss: 0.06392470747232437 = 0.0012210879940539598 + 0.01 * 6.27036190032959
Epoch 1770, val loss: 1.659361481666565
Epoch 1780, training loss: 0.06387923657894135 = 0.0012130034156143665 + 0.01 * 6.266623497009277
Epoch 1780, val loss: 1.6602891683578491
Epoch 1790, training loss: 0.0638732835650444 = 0.0012051303638145328 + 0.01 * 6.266815662384033
Epoch 1790, val loss: 1.6612156629562378
Epoch 1800, training loss: 0.06393107771873474 = 0.0011973069049417973 + 0.01 * 6.273376941680908
Epoch 1800, val loss: 1.6620409488677979
Epoch 1810, training loss: 0.06399431824684143 = 0.0011896842624992132 + 0.01 * 6.280463218688965
Epoch 1810, val loss: 1.6629223823547363
Epoch 1820, training loss: 0.06368745863437653 = 0.0011821324005723 + 0.01 * 6.250532627105713
Epoch 1820, val loss: 1.6638498306274414
Epoch 1830, training loss: 0.06391967833042145 = 0.001174790202639997 + 0.01 * 6.274488925933838
Epoch 1830, val loss: 1.6646124124526978
Epoch 1840, training loss: 0.0638371929526329 = 0.0011675884015858173 + 0.01 * 6.266960620880127
Epoch 1840, val loss: 1.6654462814331055
Epoch 1850, training loss: 0.06365008652210236 = 0.0011603484163060784 + 0.01 * 6.248974323272705
Epoch 1850, val loss: 1.6662100553512573
Epoch 1860, training loss: 0.06380852311849594 = 0.0011532851494848728 + 0.01 * 6.265524387359619
Epoch 1860, val loss: 1.667096495628357
Epoch 1870, training loss: 0.06363815069198608 = 0.0011463519185781479 + 0.01 * 6.249180316925049
Epoch 1870, val loss: 1.6679412126541138
Epoch 1880, training loss: 0.0637926459312439 = 0.001139550469815731 + 0.01 * 6.2653093338012695
Epoch 1880, val loss: 1.668715476989746
Epoch 1890, training loss: 0.06359121948480606 = 0.0011328717228025198 + 0.01 * 6.245835304260254
Epoch 1890, val loss: 1.669404149055481
Epoch 1900, training loss: 0.06385383009910583 = 0.0011264116037636995 + 0.01 * 6.27274227142334
Epoch 1900, val loss: 1.6702637672424316
Epoch 1910, training loss: 0.06360041350126266 = 0.0011198524152860045 + 0.01 * 6.248056411743164
Epoch 1910, val loss: 1.6709693670272827
Epoch 1920, training loss: 0.06378796696662903 = 0.0011135117383673787 + 0.01 * 6.2674455642700195
Epoch 1920, val loss: 1.6717841625213623
Epoch 1930, training loss: 0.06347550451755524 = 0.0011072078486904502 + 0.01 * 6.23682975769043
Epoch 1930, val loss: 1.6724286079406738
Epoch 1940, training loss: 0.06379151344299316 = 0.001101091387681663 + 0.01 * 6.269042491912842
Epoch 1940, val loss: 1.6732453107833862
Epoch 1950, training loss: 0.06355691701173782 = 0.0010949885472655296 + 0.01 * 6.246192932128906
Epoch 1950, val loss: 1.6738892793655396
Epoch 1960, training loss: 0.06376292556524277 = 0.0010890536941587925 + 0.01 * 6.267387390136719
Epoch 1960, val loss: 1.6746037006378174
Epoch 1970, training loss: 0.06346002221107483 = 0.0010830826358869672 + 0.01 * 6.237694263458252
Epoch 1970, val loss: 1.6753488779067993
Epoch 1980, training loss: 0.06350506097078323 = 0.001077237306162715 + 0.01 * 6.2427825927734375
Epoch 1980, val loss: 1.6760709285736084
Epoch 1990, training loss: 0.06345032900571823 = 0.0010715258540585637 + 0.01 * 6.237880229949951
Epoch 1990, val loss: 1.676823377609253
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.5055
Flip ASR: 0.4311/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0521304607391357 = 1.9683927297592163 + 0.01 * 8.373770713806152
Epoch 0, val loss: 1.9686695337295532
Epoch 10, training loss: 2.0415334701538086 = 1.9577969312667847 + 0.01 * 8.373655319213867
Epoch 10, val loss: 1.9578113555908203
Epoch 20, training loss: 2.028374195098877 = 1.944641351699829 + 0.01 * 8.373291015625
Epoch 20, val loss: 1.9438971281051636
Epoch 30, training loss: 2.0097339153289795 = 1.9260119199752808 + 0.01 * 8.372207641601562
Epoch 30, val loss: 1.923791527748108
Epoch 40, training loss: 1.981558918952942 = 1.897894263267517 + 0.01 * 8.366460800170898
Epoch 40, val loss: 1.8936020135879517
Epoch 50, training loss: 1.9403338432312012 = 1.8570762872695923 + 0.01 * 8.325752258300781
Epoch 50, val loss: 1.85154128074646
Epoch 60, training loss: 1.8890706300735474 = 1.8083559274673462 + 0.01 * 8.071468353271484
Epoch 60, val loss: 1.8052600622177124
Epoch 70, training loss: 1.8431442975997925 = 1.7651771306991577 + 0.01 * 7.796718120574951
Epoch 70, val loss: 1.767194151878357
Epoch 80, training loss: 1.7930974960327148 = 1.7191486358642578 + 0.01 * 7.394887447357178
Epoch 80, val loss: 1.724359393119812
Epoch 90, training loss: 1.7279926538467407 = 1.6571024656295776 + 0.01 * 7.089018821716309
Epoch 90, val loss: 1.6682924032211304
Epoch 100, training loss: 1.6449687480926514 = 1.5750762224197388 + 0.01 * 6.989258289337158
Epoch 100, val loss: 1.5973454713821411
Epoch 110, training loss: 1.5473712682724 = 1.4780954122543335 + 0.01 * 6.927580833435059
Epoch 110, val loss: 1.5168789625167847
Epoch 120, training loss: 1.4482619762420654 = 1.3794721364974976 + 0.01 * 6.878978252410889
Epoch 120, val loss: 1.4378068447113037
Epoch 130, training loss: 1.3537914752960205 = 1.2853106260299683 + 0.01 * 6.848089694976807
Epoch 130, val loss: 1.365296483039856
Epoch 140, training loss: 1.2617541551589966 = 1.1934808492660522 + 0.01 * 6.827332019805908
Epoch 140, val loss: 1.297623872756958
Epoch 150, training loss: 1.169508695602417 = 1.1014251708984375 + 0.01 * 6.808350086212158
Epoch 150, val loss: 1.2307403087615967
Epoch 160, training loss: 1.0763825178146362 = 1.0084917545318604 + 0.01 * 6.7890801429748535
Epoch 160, val loss: 1.1638108491897583
Epoch 170, training loss: 0.9834446310997009 = 0.9157396554946899 + 0.01 * 6.770498275756836
Epoch 170, val loss: 1.0977375507354736
Epoch 180, training loss: 0.8935239911079407 = 0.8258887529373169 + 0.01 * 6.763525009155273
Epoch 180, val loss: 1.0337306261062622
Epoch 190, training loss: 0.8099573850631714 = 0.7424643039703369 + 0.01 * 6.749311447143555
Epoch 190, val loss: 0.9750224947929382
Epoch 200, training loss: 0.735676646232605 = 0.6682609915733337 + 0.01 * 6.741567611694336
Epoch 200, val loss: 0.9240671396255493
Epoch 210, training loss: 0.6718135476112366 = 0.6044763326644897 + 0.01 * 6.733724117279053
Epoch 210, val loss: 0.8817910552024841
Epoch 220, training loss: 0.6180068254470825 = 0.5506991147994995 + 0.01 * 6.730769634246826
Epoch 220, val loss: 0.8479600548744202
Epoch 230, training loss: 0.5726292133331299 = 0.5054575800895691 + 0.01 * 6.717165470123291
Epoch 230, val loss: 0.8219323754310608
Epoch 240, training loss: 0.5337448716163635 = 0.46669116616249084 + 0.01 * 6.705368995666504
Epoch 240, val loss: 0.8023243546485901
Epoch 250, training loss: 0.4993817210197449 = 0.4323592185974121 + 0.01 * 6.702250003814697
Epoch 250, val loss: 0.7875987887382507
Epoch 260, training loss: 0.4676054120063782 = 0.4007495641708374 + 0.01 * 6.6855854988098145
Epoch 260, val loss: 0.7762167453765869
Epoch 270, training loss: 0.43737518787384033 = 0.37059450149536133 + 0.01 * 6.678070545196533
Epoch 270, val loss: 0.7670792937278748
Epoch 280, training loss: 0.40785378217697144 = 0.34116998314857483 + 0.01 * 6.6683807373046875
Epoch 280, val loss: 0.7596522569656372
Epoch 290, training loss: 0.3787819743156433 = 0.31221163272857666 + 0.01 * 6.657035827636719
Epoch 290, val loss: 0.753815233707428
Epoch 300, training loss: 0.35061532258987427 = 0.283913791179657 + 0.01 * 6.670152187347412
Epoch 300, val loss: 0.7498681545257568
Epoch 310, training loss: 0.3231755495071411 = 0.2566891014575958 + 0.01 * 6.648643970489502
Epoch 310, val loss: 0.7476452589035034
Epoch 320, training loss: 0.2972264587879181 = 0.23080621659755707 + 0.01 * 6.642025470733643
Epoch 320, val loss: 0.7472020387649536
Epoch 330, training loss: 0.2729065418243408 = 0.20652759075164795 + 0.01 * 6.637894630432129
Epoch 330, val loss: 0.7483680844306946
Epoch 340, training loss: 0.25048506259918213 = 0.18413393199443817 + 0.01 * 6.635112285614014
Epoch 340, val loss: 0.7508850693702698
Epoch 350, training loss: 0.229999840259552 = 0.16370655596256256 + 0.01 * 6.629329204559326
Epoch 350, val loss: 0.7548008561134338
Epoch 360, training loss: 0.2114461064338684 = 0.1452023833990097 + 0.01 * 6.624373435974121
Epoch 360, val loss: 0.7598011493682861
Epoch 370, training loss: 0.19494834542274475 = 0.12855270504951477 + 0.01 * 6.63956356048584
Epoch 370, val loss: 0.7660807371139526
Epoch 380, training loss: 0.18035483360290527 = 0.11415000259876251 + 0.01 * 6.620482444763184
Epoch 380, val loss: 0.7736890912055969
Epoch 390, training loss: 0.16795162856578827 = 0.10175634920597076 + 0.01 * 6.619527816772461
Epoch 390, val loss: 0.7824916839599609
Epoch 400, training loss: 0.15702274441719055 = 0.09093548357486725 + 0.01 * 6.6087260246276855
Epoch 400, val loss: 0.7923212647438049
Epoch 410, training loss: 0.14771661162376404 = 0.08132482320070267 + 0.01 * 6.639179706573486
Epoch 410, val loss: 0.8032159209251404
Epoch 420, training loss: 0.1388663649559021 = 0.07273850589990616 + 0.01 * 6.612786293029785
Epoch 420, val loss: 0.8149697780609131
Epoch 430, training loss: 0.13103154301643372 = 0.06500712037086487 + 0.01 * 6.602441310882568
Epoch 430, val loss: 0.8272034525871277
Epoch 440, training loss: 0.12412089109420776 = 0.05812188610434532 + 0.01 * 6.59990119934082
Epoch 440, val loss: 0.8398298621177673
Epoch 450, training loss: 0.11799871921539307 = 0.052099861204624176 + 0.01 * 6.58988618850708
Epoch 450, val loss: 0.8529170751571655
Epoch 460, training loss: 0.11293908953666687 = 0.04684894159436226 + 0.01 * 6.609014511108398
Epoch 460, val loss: 0.8664073944091797
Epoch 470, training loss: 0.10814862698316574 = 0.04230070114135742 + 0.01 * 6.584792613983154
Epoch 470, val loss: 0.8797554969787598
Epoch 480, training loss: 0.10415858030319214 = 0.03833167254924774 + 0.01 * 6.582690715789795
Epoch 480, val loss: 0.8925459384918213
Epoch 490, training loss: 0.10059018433094025 = 0.03483898565173149 + 0.01 * 6.575119972229004
Epoch 490, val loss: 0.9051440954208374
Epoch 500, training loss: 0.09756381809711456 = 0.03175029903650284 + 0.01 * 6.581352233886719
Epoch 500, val loss: 0.9172365665435791
Epoch 510, training loss: 0.09473083913326263 = 0.02901570126414299 + 0.01 * 6.571514129638672
Epoch 510, val loss: 0.9289246797561646
Epoch 520, training loss: 0.09232110530138016 = 0.026588385924696922 + 0.01 * 6.573272228240967
Epoch 520, val loss: 0.940239667892456
Epoch 530, training loss: 0.09017494320869446 = 0.024430755525827408 + 0.01 * 6.5744194984436035
Epoch 530, val loss: 0.9512192606925964
Epoch 540, training loss: 0.08810107409954071 = 0.022507574409246445 + 0.01 * 6.559350490570068
Epoch 540, val loss: 0.961924135684967
Epoch 550, training loss: 0.0863492339849472 = 0.02078791707754135 + 0.01 * 6.556131839752197
Epoch 550, val loss: 0.972233772277832
Epoch 560, training loss: 0.08491019904613495 = 0.01924867555499077 + 0.01 * 6.566153049468994
Epoch 560, val loss: 0.9822424054145813
Epoch 570, training loss: 0.08347468823194504 = 0.017868628725409508 + 0.01 * 6.560606002807617
Epoch 570, val loss: 0.9918834567070007
Epoch 580, training loss: 0.08213784545660019 = 0.016627328470349312 + 0.01 * 6.551052093505859
Epoch 580, val loss: 1.0012927055358887
Epoch 590, training loss: 0.08091069757938385 = 0.01550785917788744 + 0.01 * 6.540283679962158
Epoch 590, val loss: 1.010386347770691
Epoch 600, training loss: 0.0800691545009613 = 0.01449486706405878 + 0.01 * 6.557429313659668
Epoch 600, val loss: 1.019255518913269
Epoch 610, training loss: 0.07893647253513336 = 0.013579949736595154 + 0.01 * 6.5356526374816895
Epoch 610, val loss: 1.0278509855270386
Epoch 620, training loss: 0.07811900228261948 = 0.012749092653393745 + 0.01 * 6.536991119384766
Epoch 620, val loss: 1.0361218452453613
Epoch 630, training loss: 0.07739615440368652 = 0.011995749548077583 + 0.01 * 6.540040493011475
Epoch 630, val loss: 1.044242024421692
Epoch 640, training loss: 0.07669537514448166 = 0.011307583190500736 + 0.01 * 6.5387797355651855
Epoch 640, val loss: 1.0520615577697754
Epoch 650, training loss: 0.07595507800579071 = 0.010680202394723892 + 0.01 * 6.5274882316589355
Epoch 650, val loss: 1.059654951095581
Epoch 660, training loss: 0.07524216920137405 = 0.010105365887284279 + 0.01 * 6.513680934906006
Epoch 660, val loss: 1.0670251846313477
Epoch 670, training loss: 0.0748259574174881 = 0.009578103199601173 + 0.01 * 6.52478551864624
Epoch 670, val loss: 1.0741703510284424
Epoch 680, training loss: 0.074251689016819 = 0.009095514193177223 + 0.01 * 6.515617847442627
Epoch 680, val loss: 1.0811189413070679
Epoch 690, training loss: 0.07387379556894302 = 0.008649011142551899 + 0.01 * 6.522478103637695
Epoch 690, val loss: 1.0878314971923828
Epoch 700, training loss: 0.07341217249631882 = 0.008238316513597965 + 0.01 * 6.517385959625244
Epoch 700, val loss: 1.0944417715072632
Epoch 710, training loss: 0.0729098841547966 = 0.007859180681407452 + 0.01 * 6.505070686340332
Epoch 710, val loss: 1.1007848978042603
Epoch 720, training loss: 0.0723949670791626 = 0.007507651578634977 + 0.01 * 6.488731384277344
Epoch 720, val loss: 1.1069978475570679
Epoch 730, training loss: 0.07202865183353424 = 0.007181028835475445 + 0.01 * 6.484762668609619
Epoch 730, val loss: 1.1130203008651733
Epoch 740, training loss: 0.07171832770109177 = 0.006878840737044811 + 0.01 * 6.483949184417725
Epoch 740, val loss: 1.1189391613006592
Epoch 750, training loss: 0.07150982320308685 = 0.0065962462686002254 + 0.01 * 6.491358280181885
Epoch 750, val loss: 1.1246294975280762
Epoch 760, training loss: 0.07110863924026489 = 0.006333097815513611 + 0.01 * 6.4775543212890625
Epoch 760, val loss: 1.1302118301391602
Epoch 770, training loss: 0.07076003402471542 = 0.006086972542107105 + 0.01 * 6.467305660247803
Epoch 770, val loss: 1.1356239318847656
Epoch 780, training loss: 0.07075905054807663 = 0.0058571104891598225 + 0.01 * 6.490194320678711
Epoch 780, val loss: 1.1408891677856445
Epoch 790, training loss: 0.07027395814657211 = 0.005641734227538109 + 0.01 * 6.463222026824951
Epoch 790, val loss: 1.1460996866226196
Epoch 800, training loss: 0.07008879631757736 = 0.005439362023025751 + 0.01 * 6.4649434089660645
Epoch 800, val loss: 1.1510393619537354
Epoch 810, training loss: 0.06999902427196503 = 0.0052495356649160385 + 0.01 * 6.474948883056641
Epoch 810, val loss: 1.1559550762176514
Epoch 820, training loss: 0.06957308948040009 = 0.005070746876299381 + 0.01 * 6.450234413146973
Epoch 820, val loss: 1.160753846168518
Epoch 830, training loss: 0.06939956545829773 = 0.004902388900518417 + 0.01 * 6.449718475341797
Epoch 830, val loss: 1.1653436422348022
Epoch 840, training loss: 0.0693230926990509 = 0.004743810743093491 + 0.01 * 6.457928657531738
Epoch 840, val loss: 1.169906735420227
Epoch 850, training loss: 0.06907516717910767 = 0.004594256170094013 + 0.01 * 6.44809103012085
Epoch 850, val loss: 1.17430579662323
Epoch 860, training loss: 0.06887634098529816 = 0.0044528828002512455 + 0.01 * 6.442346096038818
Epoch 860, val loss: 1.1786272525787354
Epoch 870, training loss: 0.06888371706008911 = 0.004319065250456333 + 0.01 * 6.456465244293213
Epoch 870, val loss: 1.1828268766403198
Epoch 880, training loss: 0.0685650035738945 = 0.004192745313048363 + 0.01 * 6.437225818634033
Epoch 880, val loss: 1.1869169473648071
Epoch 890, training loss: 0.06830808520317078 = 0.004072344396263361 + 0.01 * 6.423574447631836
Epoch 890, val loss: 1.1909325122833252
Epoch 900, training loss: 0.06827626377344131 = 0.003958448767662048 + 0.01 * 6.431781768798828
Epoch 900, val loss: 1.1948806047439575
Epoch 910, training loss: 0.06835894286632538 = 0.0038500980008393526 + 0.01 * 6.45088529586792
Epoch 910, val loss: 1.1986955404281616
Epoch 920, training loss: 0.0680878534913063 = 0.003746920498088002 + 0.01 * 6.434093475341797
Epoch 920, val loss: 1.202408790588379
Epoch 930, training loss: 0.06770523637533188 = 0.003648964921012521 + 0.01 * 6.405627250671387
Epoch 930, val loss: 1.2060610055923462
Epoch 940, training loss: 0.06763289123773575 = 0.003555558854714036 + 0.01 * 6.40773344039917
Epoch 940, val loss: 1.2096116542816162
Epoch 950, training loss: 0.0674886703491211 = 0.0034668438602238894 + 0.01 * 6.402182579040527
Epoch 950, val loss: 1.2130879163742065
Epoch 960, training loss: 0.06749515235424042 = 0.0033815368078649044 + 0.01 * 6.4113616943359375
Epoch 960, val loss: 1.2164533138275146
Epoch 970, training loss: 0.06724677234888077 = 0.003300765762105584 + 0.01 * 6.394601345062256
Epoch 970, val loss: 1.219807505607605
Epoch 980, training loss: 0.06716553121805191 = 0.003223058069124818 + 0.01 * 6.394247055053711
Epoch 980, val loss: 1.2230702638626099
Epoch 990, training loss: 0.06699605286121368 = 0.0031489748507738113 + 0.01 * 6.384707927703857
Epoch 990, val loss: 1.226304054260254
Epoch 1000, training loss: 0.06707353889942169 = 0.0030779566150158644 + 0.01 * 6.3995585441589355
Epoch 1000, val loss: 1.229406476020813
Epoch 1010, training loss: 0.0667828619480133 = 0.0030102189630270004 + 0.01 * 6.377264022827148
Epoch 1010, val loss: 1.2324426174163818
Epoch 1020, training loss: 0.06718454509973526 = 0.0029448249842971563 + 0.01 * 6.423971652984619
Epoch 1020, val loss: 1.2354037761688232
Epoch 1030, training loss: 0.06671502441167831 = 0.002882726024836302 + 0.01 * 6.383229732513428
Epoch 1030, val loss: 1.2382951974868774
Epoch 1040, training loss: 0.06653228402137756 = 0.002822665963321924 + 0.01 * 6.370962142944336
Epoch 1040, val loss: 1.2411218881607056
Epoch 1050, training loss: 0.06661964952945709 = 0.0027652159333229065 + 0.01 * 6.385443210601807
Epoch 1050, val loss: 1.2439509630203247
Epoch 1060, training loss: 0.0664985328912735 = 0.002710081869736314 + 0.01 * 6.37884521484375
Epoch 1060, val loss: 1.2466861009597778
Epoch 1070, training loss: 0.06644217669963837 = 0.0026567105669528246 + 0.01 * 6.378546237945557
Epoch 1070, val loss: 1.2493383884429932
Epoch 1080, training loss: 0.06642135232686996 = 0.0026057385839521885 + 0.01 * 6.381561756134033
Epoch 1080, val loss: 1.251984715461731
Epoch 1090, training loss: 0.06602843850851059 = 0.002556454623118043 + 0.01 * 6.347198009490967
Epoch 1090, val loss: 1.254526138305664
Epoch 1100, training loss: 0.06611207872629166 = 0.0025091071147471666 + 0.01 * 6.360296726226807
Epoch 1100, val loss: 1.2570477724075317
Epoch 1110, training loss: 0.06602715700864792 = 0.0024634108413010836 + 0.01 * 6.356375217437744
Epoch 1110, val loss: 1.2595188617706299
Epoch 1120, training loss: 0.06591714173555374 = 0.002419564174488187 + 0.01 * 6.349758148193359
Epoch 1120, val loss: 1.2619073390960693
Epoch 1130, training loss: 0.06588350981473923 = 0.0023771142587065697 + 0.01 * 6.350639343261719
Epoch 1130, val loss: 1.2642631530761719
Epoch 1140, training loss: 0.0657634437084198 = 0.002336131874471903 + 0.01 * 6.342731475830078
Epoch 1140, val loss: 1.2665787935256958
Epoch 1150, training loss: 0.06594165414571762 = 0.0022967278491705656 + 0.01 * 6.364492416381836
Epoch 1150, val loss: 1.2688771486282349
Epoch 1160, training loss: 0.06566096097230911 = 0.0022585918195545673 + 0.01 * 6.340237140655518
Epoch 1160, val loss: 1.2710784673690796
Epoch 1170, training loss: 0.06574385613203049 = 0.002221624832600355 + 0.01 * 6.352223873138428
Epoch 1170, val loss: 1.2732819318771362
Epoch 1180, training loss: 0.0655023455619812 = 0.002186025260016322 + 0.01 * 6.331632614135742
Epoch 1180, val loss: 1.2754061222076416
Epoch 1190, training loss: 0.06573440134525299 = 0.0021516208071261644 + 0.01 * 6.358277797698975
Epoch 1190, val loss: 1.2775027751922607
Epoch 1200, training loss: 0.06536965817213058 = 0.0021182468626648188 + 0.01 * 6.325141429901123
Epoch 1200, val loss: 1.2795659303665161
Epoch 1210, training loss: 0.06538395583629608 = 0.002085914835333824 + 0.01 * 6.329803943634033
Epoch 1210, val loss: 1.281571865081787
Epoch 1220, training loss: 0.06533743441104889 = 0.002054692944511771 + 0.01 * 6.328274726867676
Epoch 1220, val loss: 1.2835476398468018
Epoch 1230, training loss: 0.06519434601068497 = 0.0020244389306753874 + 0.01 * 6.316990375518799
Epoch 1230, val loss: 1.2855095863342285
Epoch 1240, training loss: 0.06520608067512512 = 0.001995019381865859 + 0.01 * 6.321106433868408
Epoch 1240, val loss: 1.2874236106872559
Epoch 1250, training loss: 0.0651458352804184 = 0.0019665532745420933 + 0.01 * 6.317928791046143
Epoch 1250, val loss: 1.2893171310424805
Epoch 1260, training loss: 0.0650850459933281 = 0.0019390609813854098 + 0.01 * 6.31459903717041
Epoch 1260, val loss: 1.291165828704834
Epoch 1270, training loss: 0.06498222053050995 = 0.001912202569656074 + 0.01 * 6.307002067565918
Epoch 1270, val loss: 1.2929701805114746
Epoch 1280, training loss: 0.06500451266765594 = 0.0018862353172153234 + 0.01 * 6.311828136444092
Epoch 1280, val loss: 1.2947419881820679
Epoch 1290, training loss: 0.06510545313358307 = 0.0018610875122249126 + 0.01 * 6.324436664581299
Epoch 1290, val loss: 1.2964634895324707
Epoch 1300, training loss: 0.06489443778991699 = 0.0018365427386015654 + 0.01 * 6.305789470672607
Epoch 1300, val loss: 1.2981735467910767
Epoch 1310, training loss: 0.06477408111095428 = 0.001812814036384225 + 0.01 * 6.296126365661621
Epoch 1310, val loss: 1.2998188734054565
Epoch 1320, training loss: 0.06472798436880112 = 0.0017897527432069182 + 0.01 * 6.2938232421875
Epoch 1320, val loss: 1.3014804124832153
Epoch 1330, training loss: 0.06492353230714798 = 0.0017673481488600373 + 0.01 * 6.315618991851807
Epoch 1330, val loss: 1.3031082153320312
Epoch 1340, training loss: 0.06481815129518509 = 0.0017455487977713346 + 0.01 * 6.307260036468506
Epoch 1340, val loss: 1.3046934604644775
Epoch 1350, training loss: 0.06471376121044159 = 0.0017243215115740895 + 0.01 * 6.298943996429443
Epoch 1350, val loss: 1.3062905073165894
Epoch 1360, training loss: 0.06463999301195145 = 0.0017036331119015813 + 0.01 * 6.293635845184326
Epoch 1360, val loss: 1.3078793287277222
Epoch 1370, training loss: 0.06478805840015411 = 0.0016835560090839863 + 0.01 * 6.310450077056885
Epoch 1370, val loss: 1.3094420433044434
Epoch 1380, training loss: 0.06460726261138916 = 0.0016639152308925986 + 0.01 * 6.294334888458252
Epoch 1380, val loss: 1.3109798431396484
Epoch 1390, training loss: 0.0644683986902237 = 0.0016448190435767174 + 0.01 * 6.282358169555664
Epoch 1390, val loss: 1.3124873638153076
Epoch 1400, training loss: 0.0647912323474884 = 0.0016261894488707185 + 0.01 * 6.316504955291748
Epoch 1400, val loss: 1.313934564590454
Epoch 1410, training loss: 0.06465273350477219 = 0.001608104445040226 + 0.01 * 6.304462909698486
Epoch 1410, val loss: 1.3153672218322754
Epoch 1420, training loss: 0.0643194168806076 = 0.0015905221225693822 + 0.01 * 6.272889614105225
Epoch 1420, val loss: 1.3167616128921509
Epoch 1430, training loss: 0.06430172175168991 = 0.0015734279295429587 + 0.01 * 6.272829532623291
Epoch 1430, val loss: 1.3182017803192139
Epoch 1440, training loss: 0.06451810896396637 = 0.001556713948957622 + 0.01 * 6.296139240264893
Epoch 1440, val loss: 1.319624662399292
Epoch 1450, training loss: 0.06442071497440338 = 0.0015403837896883488 + 0.01 * 6.288033485412598
Epoch 1450, val loss: 1.3210089206695557
Epoch 1460, training loss: 0.06428559124469757 = 0.0015245964750647545 + 0.01 * 6.27609920501709
Epoch 1460, val loss: 1.3223901987075806
Epoch 1470, training loss: 0.0642060711979866 = 0.0015091157983988523 + 0.01 * 6.26969575881958
Epoch 1470, val loss: 1.3237262964248657
Epoch 1480, training loss: 0.06424593925476074 = 0.0014940850669518113 + 0.01 * 6.2751851081848145
Epoch 1480, val loss: 1.3250532150268555
Epoch 1490, training loss: 0.0642075389623642 = 0.0014794295420870185 + 0.01 * 6.272810935974121
Epoch 1490, val loss: 1.3263297080993652
Epoch 1500, training loss: 0.06414845585823059 = 0.0014649813529103994 + 0.01 * 6.26834774017334
Epoch 1500, val loss: 1.3276289701461792
Epoch 1510, training loss: 0.06401757150888443 = 0.0014508847380056977 + 0.01 * 6.256669521331787
Epoch 1510, val loss: 1.3289176225662231
Epoch 1520, training loss: 0.0640791580080986 = 0.0014371993020176888 + 0.01 * 6.264195442199707
Epoch 1520, val loss: 1.3301705121994019
Epoch 1530, training loss: 0.06408046185970306 = 0.0014238033909350634 + 0.01 * 6.265665531158447
Epoch 1530, val loss: 1.3313946723937988
Epoch 1540, training loss: 0.0640874132514 = 0.0014106992166489363 + 0.01 * 6.267671585083008
Epoch 1540, val loss: 1.3326199054718018
Epoch 1550, training loss: 0.06410207599401474 = 0.0013978559290990233 + 0.01 * 6.270421981811523
Epoch 1550, val loss: 1.3338205814361572
Epoch 1560, training loss: 0.0640937015414238 = 0.001385363982990384 + 0.01 * 6.270834445953369
Epoch 1560, val loss: 1.335007667541504
Epoch 1570, training loss: 0.06386478990316391 = 0.0013730654027312994 + 0.01 * 6.249172210693359
Epoch 1570, val loss: 1.336161494255066
Epoch 1580, training loss: 0.06381852179765701 = 0.0013611605390906334 + 0.01 * 6.245736122131348
Epoch 1580, val loss: 1.3373020887374878
Epoch 1590, training loss: 0.06400188058614731 = 0.001349422731436789 + 0.01 * 6.265246391296387
Epoch 1590, val loss: 1.338437795639038
Epoch 1600, training loss: 0.06408318877220154 = 0.0013380498858168721 + 0.01 * 6.2745137214660645
Epoch 1600, val loss: 1.3395142555236816
Epoch 1610, training loss: 0.06381493806838989 = 0.0013267849572002888 + 0.01 * 6.248815536499023
Epoch 1610, val loss: 1.3406585454940796
Epoch 1620, training loss: 0.0638202652335167 = 0.0013158877845853567 + 0.01 * 6.250438213348389
Epoch 1620, val loss: 1.341737151145935
Epoch 1630, training loss: 0.06390409916639328 = 0.0013051375281065702 + 0.01 * 6.259896278381348
Epoch 1630, val loss: 1.3428200483322144
Epoch 1640, training loss: 0.0637107640504837 = 0.0012946280185133219 + 0.01 * 6.241613388061523
Epoch 1640, val loss: 1.3438639640808105
Epoch 1650, training loss: 0.06362725794315338 = 0.001284286379814148 + 0.01 * 6.234297275543213
Epoch 1650, val loss: 1.344914436340332
Epoch 1660, training loss: 0.06380001455545425 = 0.0012741897953674197 + 0.01 * 6.252582550048828
Epoch 1660, val loss: 1.3459614515304565
Epoch 1670, training loss: 0.06378230452537537 = 0.0012643225491046906 + 0.01 * 6.251798629760742
Epoch 1670, val loss: 1.347025990486145
Epoch 1680, training loss: 0.06356267631053925 = 0.0012546537909656763 + 0.01 * 6.230802059173584
Epoch 1680, val loss: 1.3480663299560547
Epoch 1690, training loss: 0.0635533407330513 = 0.0012450771173462272 + 0.01 * 6.230826377868652
Epoch 1690, val loss: 1.3491050004959106
Epoch 1700, training loss: 0.06363870203495026 = 0.0012358290841802955 + 0.01 * 6.2402873039245605
Epoch 1700, val loss: 1.350103735923767
Epoch 1710, training loss: 0.06359920650720596 = 0.0012267346028238535 + 0.01 * 6.237247467041016
Epoch 1710, val loss: 1.351101279258728
Epoch 1720, training loss: 0.06339757889509201 = 0.0012177354656159878 + 0.01 * 6.217984199523926
Epoch 1720, val loss: 1.3521311283111572
Epoch 1730, training loss: 0.0637134239077568 = 0.0012089669471606612 + 0.01 * 6.250445365905762
Epoch 1730, val loss: 1.3531502485275269
Epoch 1740, training loss: 0.06362765282392502 = 0.0012004153104498982 + 0.01 * 6.24272346496582
Epoch 1740, val loss: 1.3541291952133179
Epoch 1750, training loss: 0.06342045962810516 = 0.0011919595999643207 + 0.01 * 6.2228498458862305
Epoch 1750, val loss: 1.3551244735717773
Epoch 1760, training loss: 0.06348245590925217 = 0.001183722517453134 + 0.01 * 6.2298736572265625
Epoch 1760, val loss: 1.356094479560852
Epoch 1770, training loss: 0.06372947990894318 = 0.0011755459709092975 + 0.01 * 6.255393981933594
Epoch 1770, val loss: 1.357086420059204
Epoch 1780, training loss: 0.06356820464134216 = 0.0011675836285576224 + 0.01 * 6.240062713623047
Epoch 1780, val loss: 1.3580546379089355
Epoch 1790, training loss: 0.06349416822195053 = 0.0011597423581406474 + 0.01 * 6.233442783355713
Epoch 1790, val loss: 1.3589982986450195
Epoch 1800, training loss: 0.06341622024774551 = 0.0011519972467795014 + 0.01 * 6.2264227867126465
Epoch 1800, val loss: 1.3599598407745361
Epoch 1810, training loss: 0.0635199248790741 = 0.001144425943493843 + 0.01 * 6.237550258636475
Epoch 1810, val loss: 1.3608851432800293
Epoch 1820, training loss: 0.06333902478218079 = 0.001137011917307973 + 0.01 * 6.22020149230957
Epoch 1820, val loss: 1.3618443012237549
Epoch 1830, training loss: 0.06328439712524414 = 0.0011296930024400353 + 0.01 * 6.215470790863037
Epoch 1830, val loss: 1.362766981124878
Epoch 1840, training loss: 0.06339152902364731 = 0.0011225693160668015 + 0.01 * 6.226896286010742
Epoch 1840, val loss: 1.3636903762817383
Epoch 1850, training loss: 0.06329042464494705 = 0.0011155171087011695 + 0.01 * 6.217491149902344
Epoch 1850, val loss: 1.3646273612976074
Epoch 1860, training loss: 0.06326011568307877 = 0.0011086311424151063 + 0.01 * 6.21514892578125
Epoch 1860, val loss: 1.3655108213424683
Epoch 1870, training loss: 0.06330622732639313 = 0.0011017979122698307 + 0.01 * 6.220443248748779
Epoch 1870, val loss: 1.3664354085922241
Epoch 1880, training loss: 0.06345362961292267 = 0.001095112063921988 + 0.01 * 6.235851764678955
Epoch 1880, val loss: 1.3673371076583862
Epoch 1890, training loss: 0.06318219006061554 = 0.0010885386727750301 + 0.01 * 6.209365367889404
Epoch 1890, val loss: 1.3682007789611816
Epoch 1900, training loss: 0.06329729408025742 = 0.0010819652816280723 + 0.01 * 6.221533298492432
Epoch 1900, val loss: 1.36908757686615
Epoch 1910, training loss: 0.06328587234020233 = 0.001075715757906437 + 0.01 * 6.221015930175781
Epoch 1910, val loss: 1.3699084520339966
Epoch 1920, training loss: 0.06308214366436005 = 0.001069411518983543 + 0.01 * 6.201273441314697
Epoch 1920, val loss: 1.3707860708236694
Epoch 1930, training loss: 0.06305260956287384 = 0.0010632298653945327 + 0.01 * 6.198937892913818
Epoch 1930, val loss: 1.3716070652008057
Epoch 1940, training loss: 0.06315643340349197 = 0.0010572344763204455 + 0.01 * 6.2099199295043945
Epoch 1940, val loss: 1.3724150657653809
Epoch 1950, training loss: 0.06312765181064606 = 0.0010512304725125432 + 0.01 * 6.207642078399658
Epoch 1950, val loss: 1.3732542991638184
Epoch 1960, training loss: 0.06295256316661835 = 0.0010453244904056191 + 0.01 * 6.190723896026611
Epoch 1960, val loss: 1.3740551471710205
Epoch 1970, training loss: 0.06324373185634613 = 0.0010395252611488104 + 0.01 * 6.220420837402344
Epoch 1970, val loss: 1.3748445510864258
Epoch 1980, training loss: 0.06309978663921356 = 0.0010338189313188195 + 0.01 * 6.206596851348877
Epoch 1980, val loss: 1.3756088018417358
Epoch 1990, training loss: 0.06298383325338364 = 0.0010281261056661606 + 0.01 * 6.195570945739746
Epoch 1990, val loss: 1.3764046430587769
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.7638
Flip ASR: 0.7244/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 2.0232303142547607 = 1.939493179321289 + 0.01 * 8.37371826171875
Epoch 0, val loss: 1.9371113777160645
Epoch 10, training loss: 2.0122897624969482 = 1.9285553693771362 + 0.01 * 8.373449325561523
Epoch 10, val loss: 1.9244520664215088
Epoch 20, training loss: 1.999018669128418 = 1.9152913093566895 + 0.01 * 8.372737884521484
Epoch 20, val loss: 1.9083783626556396
Epoch 30, training loss: 1.9807244539260864 = 1.8970162868499756 + 0.01 * 8.370817184448242
Epoch 30, val loss: 1.8859357833862305
Epoch 40, training loss: 1.9549274444580078 = 1.8713209629058838 + 0.01 * 8.360642433166504
Epoch 40, val loss: 1.8550329208374023
Epoch 50, training loss: 1.9200141429901123 = 1.8370535373687744 + 0.01 * 8.296059608459473
Epoch 50, val loss: 1.8164345026016235
Epoch 60, training loss: 1.877712368965149 = 1.7974363565444946 + 0.01 * 8.027597427368164
Epoch 60, val loss: 1.7769033908843994
Epoch 70, training loss: 1.8345153331756592 = 1.7581841945648193 + 0.01 * 7.633118629455566
Epoch 70, val loss: 1.7438849210739136
Epoch 80, training loss: 1.7854939699172974 = 1.7130787372589111 + 0.01 * 7.241528511047363
Epoch 80, val loss: 1.7094268798828125
Epoch 90, training loss: 1.7220858335494995 = 1.6512393951416016 + 0.01 * 7.084639549255371
Epoch 90, val loss: 1.6619571447372437
Epoch 100, training loss: 1.6390111446380615 = 1.569038987159729 + 0.01 * 6.9972147941589355
Epoch 100, val loss: 1.5975972414016724
Epoch 110, training loss: 1.5396023988723755 = 1.4703686237335205 + 0.01 * 6.9233808517456055
Epoch 110, val loss: 1.522214651107788
Epoch 120, training loss: 1.4336845874786377 = 1.364842176437378 + 0.01 * 6.884247303009033
Epoch 120, val loss: 1.4451488256454468
Epoch 130, training loss: 1.3267391920089722 = 1.2581764459609985 + 0.01 * 6.856273651123047
Epoch 130, val loss: 1.3712873458862305
Epoch 140, training loss: 1.2201169729232788 = 1.151794195175171 + 0.01 * 6.832282543182373
Epoch 140, val loss: 1.2997994422912598
Epoch 150, training loss: 1.1131439208984375 = 1.0450197458267212 + 0.01 * 6.812419891357422
Epoch 150, val loss: 1.226570725440979
Epoch 160, training loss: 1.0061684846878052 = 0.9381938576698303 + 0.01 * 6.797458648681641
Epoch 160, val loss: 1.152090311050415
Epoch 170, training loss: 0.902559757232666 = 0.8346816897392273 + 0.01 * 6.787803649902344
Epoch 170, val loss: 1.080735683441162
Epoch 180, training loss: 0.8067644834518433 = 0.7389310598373413 + 0.01 * 6.7833428382873535
Epoch 180, val loss: 1.0162744522094727
Epoch 190, training loss: 0.721440851688385 = 0.653657853603363 + 0.01 * 6.778302192687988
Epoch 190, val loss: 0.9610832929611206
Epoch 200, training loss: 0.646727442741394 = 0.5789905190467834 + 0.01 * 6.7736945152282715
Epoch 200, val loss: 0.9153068661689758
Epoch 210, training loss: 0.5812077522277832 = 0.5135173201560974 + 0.01 * 6.769042491912842
Epoch 210, val loss: 0.877841055393219
Epoch 220, training loss: 0.5230538249015808 = 0.4554139971733093 + 0.01 * 6.763982772827148
Epoch 220, val loss: 0.8469681143760681
Epoch 230, training loss: 0.47064611315727234 = 0.40304893255233765 + 0.01 * 6.75971794128418
Epoch 230, val loss: 0.8211005330085754
Epoch 240, training loss: 0.42276275157928467 = 0.35522910952568054 + 0.01 * 6.753363609313965
Epoch 240, val loss: 0.7985385060310364
Epoch 250, training loss: 0.3788152039051056 = 0.3113490641117096 + 0.01 * 6.746614456176758
Epoch 250, val loss: 0.7791452407836914
Epoch 260, training loss: 0.3387283682823181 = 0.27129676938056946 + 0.01 * 6.743161678314209
Epoch 260, val loss: 0.7629715800285339
Epoch 270, training loss: 0.3023719787597656 = 0.23505066335201263 + 0.01 * 6.732131481170654
Epoch 270, val loss: 0.7498877644538879
Epoch 280, training loss: 0.2701488733291626 = 0.20288550853729248 + 0.01 * 6.726337432861328
Epoch 280, val loss: 0.7408605217933655
Epoch 290, training loss: 0.24222800135612488 = 0.17506428062915802 + 0.01 * 6.716372966766357
Epoch 290, val loss: 0.736768901348114
Epoch 300, training loss: 0.21855735778808594 = 0.15143583714962006 + 0.01 * 6.712152481079102
Epoch 300, val loss: 0.7372151613235474
Epoch 310, training loss: 0.1985105276107788 = 0.1314847618341446 + 0.01 * 6.702577114105225
Epoch 310, val loss: 0.7414469122886658
Epoch 320, training loss: 0.1817096471786499 = 0.11470978707075119 + 0.01 * 6.699985980987549
Epoch 320, val loss: 0.748638927936554
Epoch 330, training loss: 0.16752682626247406 = 0.10061699151992798 + 0.01 * 6.690983295440674
Epoch 330, val loss: 0.7577318549156189
Epoch 340, training loss: 0.15551719069480896 = 0.08872176706790924 + 0.01 * 6.679543495178223
Epoch 340, val loss: 0.7680715322494507
Epoch 350, training loss: 0.14537589251995087 = 0.07862422615289688 + 0.01 * 6.675167083740234
Epoch 350, val loss: 0.7792508602142334
Epoch 360, training loss: 0.13661707937717438 = 0.0700003132224083 + 0.01 * 6.661676406860352
Epoch 360, val loss: 0.7909712195396423
Epoch 370, training loss: 0.1291961967945099 = 0.06258677691221237 + 0.01 * 6.660943031311035
Epoch 370, val loss: 0.8030199408531189
Epoch 380, training loss: 0.1226804181933403 = 0.05617570877075195 + 0.01 * 6.650471210479736
Epoch 380, val loss: 0.8152820467948914
Epoch 390, training loss: 0.11705537140369415 = 0.05060238018631935 + 0.01 * 6.645298957824707
Epoch 390, val loss: 0.8275810480117798
Epoch 400, training loss: 0.1122363954782486 = 0.045739445835351944 + 0.01 * 6.64969539642334
Epoch 400, val loss: 0.8399442434310913
Epoch 410, training loss: 0.10789921879768372 = 0.04148275777697563 + 0.01 * 6.641646385192871
Epoch 410, val loss: 0.8522569537162781
Epoch 420, training loss: 0.10408216714859009 = 0.03774203360080719 + 0.01 * 6.6340131759643555
Epoch 420, val loss: 0.8646067380905151
Epoch 430, training loss: 0.1007143184542656 = 0.03444650024175644 + 0.01 * 6.626781940460205
Epoch 430, val loss: 0.8768468499183655
Epoch 440, training loss: 0.09782956540584564 = 0.031534593552351 + 0.01 * 6.62949800491333
Epoch 440, val loss: 0.8890059590339661
Epoch 450, training loss: 0.09517921507358551 = 0.02895679697394371 + 0.01 * 6.622241973876953
Epoch 450, val loss: 0.9010136723518372
Epoch 460, training loss: 0.09285245835781097 = 0.026668142527341843 + 0.01 * 6.618431568145752
Epoch 460, val loss: 0.9128621220588684
Epoch 470, training loss: 0.09078583121299744 = 0.02462892234325409 + 0.01 * 6.615691184997559
Epoch 470, val loss: 0.9244526028633118
Epoch 480, training loss: 0.08887645602226257 = 0.022807402536273003 + 0.01 * 6.606905460357666
Epoch 480, val loss: 0.9358313083648682
Epoch 490, training loss: 0.0872974768280983 = 0.02117578126490116 + 0.01 * 6.6121697425842285
Epoch 490, val loss: 0.9470000267028809
Epoch 500, training loss: 0.08572429418563843 = 0.01970687136054039 + 0.01 * 6.601741790771484
Epoch 500, val loss: 0.9578937888145447
Epoch 510, training loss: 0.0843290388584137 = 0.01838209666311741 + 0.01 * 6.5946946144104
Epoch 510, val loss: 0.9685758352279663
Epoch 520, training loss: 0.08307027071714401 = 0.01717994548380375 + 0.01 * 6.5890326499938965
Epoch 520, val loss: 0.9789286255836487
Epoch 530, training loss: 0.08187262713909149 = 0.016083810478448868 + 0.01 * 6.578881740570068
Epoch 530, val loss: 0.9891834259033203
Epoch 540, training loss: 0.08093389868736267 = 0.015084893442690372 + 0.01 * 6.584900856018066
Epoch 540, val loss: 0.9991725087165833
Epoch 550, training loss: 0.08009696751832962 = 0.014173483476042747 + 0.01 * 6.592348575592041
Epoch 550, val loss: 1.0089303255081177
Epoch 560, training loss: 0.0790395438671112 = 0.013338994234800339 + 0.01 * 6.57005500793457
Epoch 560, val loss: 1.0184704065322876
Epoch 570, training loss: 0.07818525284528732 = 0.012573836371302605 + 0.01 * 6.561141490936279
Epoch 570, val loss: 1.0277601480484009
Epoch 580, training loss: 0.07752758264541626 = 0.011873271316289902 + 0.01 * 6.565431118011475
Epoch 580, val loss: 1.0368373394012451
Epoch 590, training loss: 0.07698948681354523 = 0.011229471303522587 + 0.01 * 6.5760016441345215
Epoch 590, val loss: 1.0456684827804565
Epoch 600, training loss: 0.07604796439409256 = 0.01063693966716528 + 0.01 * 6.541102886199951
Epoch 600, val loss: 1.054316520690918
Epoch 610, training loss: 0.07555058598518372 = 0.010090693831443787 + 0.01 * 6.545989513397217
Epoch 610, val loss: 1.06271493434906
Epoch 620, training loss: 0.07492583990097046 = 0.009586251340806484 + 0.01 * 6.533958911895752
Epoch 620, val loss: 1.0708683729171753
Epoch 630, training loss: 0.07447195053100586 = 0.00911896862089634 + 0.01 * 6.5352983474731445
Epoch 630, val loss: 1.0788601636886597
Epoch 640, training loss: 0.07402212917804718 = 0.008685494773089886 + 0.01 * 6.533663749694824
Epoch 640, val loss: 1.086651086807251
Epoch 650, training loss: 0.07372542470693588 = 0.008282321505248547 + 0.01 * 6.544310569763184
Epoch 650, val loss: 1.0942823886871338
Epoch 660, training loss: 0.07324192672967911 = 0.007907474413514137 + 0.01 * 6.533445358276367
Epoch 660, val loss: 1.1017106771469116
Epoch 670, training loss: 0.07289496809244156 = 0.007558539509773254 + 0.01 * 6.533642768859863
Epoch 670, val loss: 1.1089746952056885
Epoch 680, training loss: 0.07231619209051132 = 0.007233726792037487 + 0.01 * 6.508246898651123
Epoch 680, val loss: 1.1159944534301758
Epoch 690, training loss: 0.07194172590970993 = 0.006930859759449959 + 0.01 * 6.501086711883545
Epoch 690, val loss: 1.1228892803192139
Epoch 700, training loss: 0.07161619514226913 = 0.006647379603236914 + 0.01 * 6.496881484985352
Epoch 700, val loss: 1.1296708583831787
Epoch 710, training loss: 0.07140513509511948 = 0.006382201798260212 + 0.01 * 6.502293109893799
Epoch 710, val loss: 1.1362203359603882
Epoch 720, training loss: 0.0710870698094368 = 0.006134373135864735 + 0.01 * 6.495269775390625
Epoch 720, val loss: 1.1426204442977905
Epoch 730, training loss: 0.07100826501846313 = 0.005901820491999388 + 0.01 * 6.510644912719727
Epoch 730, val loss: 1.148821234703064
Epoch 740, training loss: 0.07058576494455338 = 0.005683179013431072 + 0.01 * 6.490259170532227
Epoch 740, val loss: 1.154884934425354
Epoch 750, training loss: 0.07035566121339798 = 0.005477603990584612 + 0.01 * 6.487805366516113
Epoch 750, val loss: 1.1608006954193115
Epoch 760, training loss: 0.07011867314577103 = 0.005283997394144535 + 0.01 * 6.4834675788879395
Epoch 760, val loss: 1.16655695438385
Epoch 770, training loss: 0.06998147070407867 = 0.005101711489260197 + 0.01 * 6.48797607421875
Epoch 770, val loss: 1.1721891164779663
Epoch 780, training loss: 0.06969065219163895 = 0.004929853603243828 + 0.01 * 6.47607946395874
Epoch 780, val loss: 1.1777069568634033
Epoch 790, training loss: 0.0694710910320282 = 0.004767828620970249 + 0.01 * 6.4703264236450195
Epoch 790, val loss: 1.1830708980560303
Epoch 800, training loss: 0.06944680958986282 = 0.00461460929363966 + 0.01 * 6.483220100402832
Epoch 800, val loss: 1.1883296966552734
Epoch 810, training loss: 0.06917115300893784 = 0.004469672217965126 + 0.01 * 6.470148086547852
Epoch 810, val loss: 1.1934523582458496
Epoch 820, training loss: 0.06902597844600677 = 0.004332380834966898 + 0.01 * 6.469359874725342
Epoch 820, val loss: 1.1984050273895264
Epoch 830, training loss: 0.06886155903339386 = 0.004202516749501228 + 0.01 * 6.465904235839844
Epoch 830, val loss: 1.2033032178878784
Epoch 840, training loss: 0.06871580332517624 = 0.004079161211848259 + 0.01 * 6.463664531707764
Epoch 840, val loss: 1.208083987236023
Epoch 850, training loss: 0.06838434189558029 = 0.003961922600865364 + 0.01 * 6.44224214553833
Epoch 850, val loss: 1.2127147912979126
Epoch 860, training loss: 0.0685824453830719 = 0.0038505299016833305 + 0.01 * 6.473191738128662
Epoch 860, val loss: 1.2172603607177734
Epoch 870, training loss: 0.06817251443862915 = 0.003744581714272499 + 0.01 * 6.442793369293213
Epoch 870, val loss: 1.2217622995376587
Epoch 880, training loss: 0.06805021315813065 = 0.00364397163502872 + 0.01 * 6.440623760223389
Epoch 880, val loss: 1.226062297821045
Epoch 890, training loss: 0.06793667376041412 = 0.0035481329541653395 + 0.01 * 6.438854217529297
Epoch 890, val loss: 1.2303534746170044
Epoch 900, training loss: 0.06773501634597778 = 0.0034570067655295134 + 0.01 * 6.42780065536499
Epoch 900, val loss: 1.234486699104309
Epoch 910, training loss: 0.06789903342723846 = 0.0033698969054967165 + 0.01 * 6.452913761138916
Epoch 910, val loss: 1.2385590076446533
Epoch 920, training loss: 0.06764017790555954 = 0.003286873223260045 + 0.01 * 6.435330390930176
Epoch 920, val loss: 1.242552399635315
Epoch 930, training loss: 0.06760898232460022 = 0.0032073010224848986 + 0.01 * 6.4401679039001465
Epoch 930, val loss: 1.2464203834533691
Epoch 940, training loss: 0.0672864317893982 = 0.0031313402578234673 + 0.01 * 6.4155097007751465
Epoch 940, val loss: 1.2501821517944336
Epoch 950, training loss: 0.06728006899356842 = 0.0030587550718337297 + 0.01 * 6.422131538391113
Epoch 950, val loss: 1.2539374828338623
Epoch 960, training loss: 0.06704464554786682 = 0.0029892409220337868 + 0.01 * 6.405540466308594
Epoch 960, val loss: 1.257535696029663
Epoch 970, training loss: 0.06708097457885742 = 0.002922758925706148 + 0.01 * 6.415821552276611
Epoch 970, val loss: 1.2611433267593384
Epoch 980, training loss: 0.06678791344165802 = 0.0028588378336280584 + 0.01 * 6.39290714263916
Epoch 980, val loss: 1.264607548713684
Epoch 990, training loss: 0.06661394983530045 = 0.0027977973222732544 + 0.01 * 6.38161563873291
Epoch 990, val loss: 1.2679710388183594
Epoch 1000, training loss: 0.06679155677556992 = 0.0027391514740884304 + 0.01 * 6.405240535736084
Epoch 1000, val loss: 1.2712812423706055
Epoch 1010, training loss: 0.06672189384698868 = 0.002682930324226618 + 0.01 * 6.403896331787109
Epoch 1010, val loss: 1.2744755744934082
Epoch 1020, training loss: 0.06640705466270447 = 0.0026289750821888447 + 0.01 * 6.377808094024658
Epoch 1020, val loss: 1.2776602506637573
Epoch 1030, training loss: 0.06636013090610504 = 0.0025768945924937725 + 0.01 * 6.378324031829834
Epoch 1030, val loss: 1.2807754278182983
Epoch 1040, training loss: 0.06634539365768433 = 0.0025270457845181227 + 0.01 * 6.381835460662842
Epoch 1040, val loss: 1.2837965488433838
Epoch 1050, training loss: 0.06617212295532227 = 0.002478903392329812 + 0.01 * 6.369322299957275
Epoch 1050, val loss: 1.2867255210876465
Epoch 1060, training loss: 0.06630107015371323 = 0.0024327575229108334 + 0.01 * 6.386831760406494
Epoch 1060, val loss: 1.289704442024231
Epoch 1070, training loss: 0.06625545024871826 = 0.0023881266824901104 + 0.01 * 6.38673210144043
Epoch 1070, val loss: 1.2925055027008057
Epoch 1080, training loss: 0.06583055108785629 = 0.002345108659937978 + 0.01 * 6.348544597625732
Epoch 1080, val loss: 1.295292615890503
Epoch 1090, training loss: 0.06595589965581894 = 0.0023038280196487904 + 0.01 * 6.365207195281982
Epoch 1090, val loss: 1.2980269193649292
Epoch 1100, training loss: 0.06588425487279892 = 0.0022638083901256323 + 0.01 * 6.3620452880859375
Epoch 1100, val loss: 1.3006788492202759
Epoch 1110, training loss: 0.0658118948340416 = 0.0022253103088587523 + 0.01 * 6.358658313751221
Epoch 1110, val loss: 1.303276777267456
Epoch 1120, training loss: 0.06607747077941895 = 0.0021880774293094873 + 0.01 * 6.388939380645752
Epoch 1120, val loss: 1.3058340549468994
Epoch 1130, training loss: 0.06575001031160355 = 0.002152026630938053 + 0.01 * 6.359798908233643
Epoch 1130, val loss: 1.3083012104034424
Epoch 1140, training loss: 0.06549549102783203 = 0.0021173625718802214 + 0.01 * 6.337812900543213
Epoch 1140, val loss: 1.3107250928878784
Epoch 1150, training loss: 0.06538814306259155 = 0.0020836249459534883 + 0.01 * 6.330451965332031
Epoch 1150, val loss: 1.3130600452423096
Epoch 1160, training loss: 0.06582074612379074 = 0.002051121788099408 + 0.01 * 6.376962661743164
Epoch 1160, val loss: 1.315422534942627
Epoch 1170, training loss: 0.06539055705070496 = 0.0020196333061903715 + 0.01 * 6.33709192276001
Epoch 1170, val loss: 1.3177374601364136
Epoch 1180, training loss: 0.06545332819223404 = 0.0019892053678631783 + 0.01 * 6.346412181854248
Epoch 1180, val loss: 1.320016860961914
Epoch 1190, training loss: 0.06518708169460297 = 0.0019596002530306578 + 0.01 * 6.32274866104126
Epoch 1190, val loss: 1.3222033977508545
Epoch 1200, training loss: 0.065212182700634 = 0.0019310055067762733 + 0.01 * 6.328118324279785
Epoch 1200, val loss: 1.3243703842163086
Epoch 1210, training loss: 0.06525939702987671 = 0.0019032928394153714 + 0.01 * 6.335610389709473
Epoch 1210, val loss: 1.326520323753357
Epoch 1220, training loss: 0.06488548219203949 = 0.0018763509579002857 + 0.01 * 6.300912857055664
Epoch 1220, val loss: 1.3286442756652832
Epoch 1230, training loss: 0.0649661123752594 = 0.001850214204750955 + 0.01 * 6.31158971786499
Epoch 1230, val loss: 1.3306884765625
Epoch 1240, training loss: 0.06495603919029236 = 0.001824915874749422 + 0.01 * 6.313112258911133
Epoch 1240, val loss: 1.3327192068099976
Epoch 1250, training loss: 0.06510679423809052 = 0.0018003254663199186 + 0.01 * 6.330646514892578
Epoch 1250, val loss: 1.3346986770629883
Epoch 1260, training loss: 0.06501547992229462 = 0.0017766448436304927 + 0.01 * 6.323884010314941
Epoch 1260, val loss: 1.3367184400558472
Epoch 1270, training loss: 0.06475776433944702 = 0.0017534715589135885 + 0.01 * 6.300429344177246
Epoch 1270, val loss: 1.3385480642318726
Epoch 1280, training loss: 0.06479065120220184 = 0.0017310542752966285 + 0.01 * 6.305960178375244
Epoch 1280, val loss: 1.3404738903045654
Epoch 1290, training loss: 0.0646740272641182 = 0.0017092199996113777 + 0.01 * 6.296480655670166
Epoch 1290, val loss: 1.3422926664352417
Epoch 1300, training loss: 0.06453904509544373 = 0.0016880403272807598 + 0.01 * 6.28510046005249
Epoch 1300, val loss: 1.3440606594085693
Epoch 1310, training loss: 0.06468572467565536 = 0.0016674828948453069 + 0.01 * 6.30182409286499
Epoch 1310, val loss: 1.345902681350708
Epoch 1320, training loss: 0.06447043269872665 = 0.0016474261647090316 + 0.01 * 6.28230094909668
Epoch 1320, val loss: 1.3475857973098755
Epoch 1330, training loss: 0.06449268758296967 = 0.0016279619885608554 + 0.01 * 6.286472797393799
Epoch 1330, val loss: 1.3493356704711914
Epoch 1340, training loss: 0.06441421806812286 = 0.0016089996788650751 + 0.01 * 6.280521869659424
Epoch 1340, val loss: 1.3509687185287476
Epoch 1350, training loss: 0.06446894258260727 = 0.0015905683394521475 + 0.01 * 6.287837982177734
Epoch 1350, val loss: 1.352636456489563
Epoch 1360, training loss: 0.0646071657538414 = 0.0015725201228633523 + 0.01 * 6.303464412689209
Epoch 1360, val loss: 1.35423743724823
Epoch 1370, training loss: 0.06446241587400436 = 0.0015550070675089955 + 0.01 * 6.290740489959717
Epoch 1370, val loss: 1.3558123111724854
Epoch 1380, training loss: 0.06442800909280777 = 0.0015379100805148482 + 0.01 * 6.289010047912598
Epoch 1380, val loss: 1.3574155569076538
Epoch 1390, training loss: 0.06456495821475983 = 0.0015213179867714643 + 0.01 * 6.30436372756958
Epoch 1390, val loss: 1.3589507341384888
Epoch 1400, training loss: 0.06441030651330948 = 0.0015051342779770494 + 0.01 * 6.2905168533325195
Epoch 1400, val loss: 1.360403299331665
Epoch 1410, training loss: 0.06436989456415176 = 0.0014893105253577232 + 0.01 * 6.288058280944824
Epoch 1410, val loss: 1.3619540929794312
Epoch 1420, training loss: 0.0640423446893692 = 0.0014738381141796708 + 0.01 * 6.256850719451904
Epoch 1420, val loss: 1.3633414506912231
Epoch 1430, training loss: 0.06410836428403854 = 0.0014587797923013568 + 0.01 * 6.264958381652832
Epoch 1430, val loss: 1.3647576570510864
Epoch 1440, training loss: 0.0642106682062149 = 0.001444155815988779 + 0.01 * 6.276651382446289
Epoch 1440, val loss: 1.3661772012710571
Epoch 1450, training loss: 0.0639408677816391 = 0.0014298481401056051 + 0.01 * 6.251101970672607
Epoch 1450, val loss: 1.367620587348938
Epoch 1460, training loss: 0.06394827365875244 = 0.0014158656122162938 + 0.01 * 6.253241062164307
Epoch 1460, val loss: 1.368919849395752
Epoch 1470, training loss: 0.06405875086784363 = 0.0014021778479218483 + 0.01 * 6.265657424926758
Epoch 1470, val loss: 1.3702096939086914
Epoch 1480, training loss: 0.06411873549222946 = 0.0013888473622500896 + 0.01 * 6.272989273071289
Epoch 1480, val loss: 1.37157142162323
Epoch 1490, training loss: 0.06403715908527374 = 0.0013758217683061957 + 0.01 * 6.2661333084106445
Epoch 1490, val loss: 1.372940182685852
Epoch 1500, training loss: 0.06399743258953094 = 0.0013630762696266174 + 0.01 * 6.2634358406066895
Epoch 1500, val loss: 1.3741785287857056
Epoch 1510, training loss: 0.06384008377790451 = 0.0013505845563486218 + 0.01 * 6.248950004577637
Epoch 1510, val loss: 1.37545907497406
Epoch 1520, training loss: 0.06387607753276825 = 0.0013384437188506126 + 0.01 * 6.253763675689697
Epoch 1520, val loss: 1.376701831817627
Epoch 1530, training loss: 0.06398694962263107 = 0.0013265827437862754 + 0.01 * 6.266036510467529
Epoch 1530, val loss: 1.377923607826233
Epoch 1540, training loss: 0.06375196576118469 = 0.0013149356236681342 + 0.01 * 6.2437028884887695
Epoch 1540, val loss: 1.3791474103927612
Epoch 1550, training loss: 0.0637107789516449 = 0.001303617493249476 + 0.01 * 6.240715980529785
Epoch 1550, val loss: 1.38036048412323
Epoch 1560, training loss: 0.06374331563711166 = 0.0012925121700391173 + 0.01 * 6.245080471038818
Epoch 1560, val loss: 1.3815027475357056
Epoch 1570, training loss: 0.06402058154344559 = 0.0012816209346055984 + 0.01 * 6.273896217346191
Epoch 1570, val loss: 1.382623314857483
Epoch 1580, training loss: 0.06385203450918198 = 0.0012709845323115587 + 0.01 * 6.2581048011779785
Epoch 1580, val loss: 1.3837636709213257
Epoch 1590, training loss: 0.0638076439499855 = 0.0012605550000444055 + 0.01 * 6.254709243774414
Epoch 1590, val loss: 1.3848750591278076
Epoch 1600, training loss: 0.0635782778263092 = 0.001250330824404955 + 0.01 * 6.232794761657715
Epoch 1600, val loss: 1.3859961032867432
Epoch 1610, training loss: 0.06362725794315338 = 0.0012403297005221248 + 0.01 * 6.238692760467529
Epoch 1610, val loss: 1.387068748474121
Epoch 1620, training loss: 0.06352365016937256 = 0.001230528112500906 + 0.01 * 6.229312419891357
Epoch 1620, val loss: 1.3881378173828125
Epoch 1630, training loss: 0.06370595842599869 = 0.0012209371197968721 + 0.01 * 6.248502731323242
Epoch 1630, val loss: 1.3891956806182861
Epoch 1640, training loss: 0.06364452838897705 = 0.001211567665450275 + 0.01 * 6.243296146392822
Epoch 1640, val loss: 1.3902450799942017
Epoch 1650, training loss: 0.06353911012411118 = 0.0012023153249174356 + 0.01 * 6.23367977142334
Epoch 1650, val loss: 1.3912217617034912
Epoch 1660, training loss: 0.06357605010271072 = 0.001193283824250102 + 0.01 * 6.238276481628418
Epoch 1660, val loss: 1.3922462463378906
Epoch 1670, training loss: 0.06340380012989044 = 0.0011844051769003272 + 0.01 * 6.221939563751221
Epoch 1670, val loss: 1.393200159072876
Epoch 1680, training loss: 0.06366471201181412 = 0.0011757093016058207 + 0.01 * 6.248900413513184
Epoch 1680, val loss: 1.3941518068313599
Epoch 1690, training loss: 0.06348246335983276 = 0.001167196431197226 + 0.01 * 6.231527328491211
Epoch 1690, val loss: 1.3951518535614014
Epoch 1700, training loss: 0.06340941041707993 = 0.0011588174384087324 + 0.01 * 6.225059509277344
Epoch 1700, val loss: 1.396091341972351
Epoch 1710, training loss: 0.06329956650733948 = 0.0011506001465022564 + 0.01 * 6.2148966789245605
Epoch 1710, val loss: 1.397031307220459
Epoch 1720, training loss: 0.06336687505245209 = 0.0011425382690504193 + 0.01 * 6.222433567047119
Epoch 1720, val loss: 1.3979687690734863
Epoch 1730, training loss: 0.06348595768213272 = 0.001134645426645875 + 0.01 * 6.23513126373291
Epoch 1730, val loss: 1.3989005088806152
Epoch 1740, training loss: 0.06339749693870544 = 0.0011268721427768469 + 0.01 * 6.227062702178955
Epoch 1740, val loss: 1.3997999429702759
Epoch 1750, training loss: 0.06334857642650604 = 0.001119227148592472 + 0.01 * 6.222934722900391
Epoch 1750, val loss: 1.4007222652435303
Epoch 1760, training loss: 0.06341619044542313 = 0.001111722900532186 + 0.01 * 6.230446815490723
Epoch 1760, val loss: 1.401531457901001
Epoch 1770, training loss: 0.06325402855873108 = 0.0011043640552088618 + 0.01 * 6.214966297149658
Epoch 1770, val loss: 1.4024386405944824
Epoch 1780, training loss: 0.06327570974826813 = 0.001097140135243535 + 0.01 * 6.217857360839844
Epoch 1780, val loss: 1.4033029079437256
Epoch 1790, training loss: 0.06320518255233765 = 0.001090070465579629 + 0.01 * 6.211511611938477
Epoch 1790, val loss: 1.4041235446929932
Epoch 1800, training loss: 0.06310306489467621 = 0.0010831222170963883 + 0.01 * 6.2019944190979
Epoch 1800, val loss: 1.4049640893936157
Epoch 1810, training loss: 0.06325896084308624 = 0.0010762899182736874 + 0.01 * 6.21826696395874
Epoch 1810, val loss: 1.405756950378418
Epoch 1820, training loss: 0.0632091611623764 = 0.0010695632081478834 + 0.01 * 6.21396017074585
Epoch 1820, val loss: 1.4065802097320557
Epoch 1830, training loss: 0.06318041682243347 = 0.001062961295247078 + 0.01 * 6.211745738983154
Epoch 1830, val loss: 1.407434344291687
Epoch 1840, training loss: 0.06318045407533646 = 0.001056429697200656 + 0.01 * 6.212402820587158
Epoch 1840, val loss: 1.4082260131835938
Epoch 1850, training loss: 0.06309154629707336 = 0.0010500192875042558 + 0.01 * 6.204152584075928
Epoch 1850, val loss: 1.4090538024902344
Epoch 1860, training loss: 0.0631074607372284 = 0.0010437116725370288 + 0.01 * 6.2063751220703125
Epoch 1860, val loss: 1.4098272323608398
Epoch 1870, training loss: 0.0633661076426506 = 0.0010374968405812979 + 0.01 * 6.232861518859863
Epoch 1870, val loss: 1.4106026887893677
Epoch 1880, training loss: 0.06324049085378647 = 0.0010313756065443158 + 0.01 * 6.220911502838135
Epoch 1880, val loss: 1.4113385677337646
Epoch 1890, training loss: 0.0630355179309845 = 0.0010253763757646084 + 0.01 * 6.201014041900635
Epoch 1890, val loss: 1.412205696105957
Epoch 1900, training loss: 0.06302446126937866 = 0.0010194607311859727 + 0.01 * 6.20050048828125
Epoch 1900, val loss: 1.4129236936569214
Epoch 1910, training loss: 0.06304426491260529 = 0.001013635192066431 + 0.01 * 6.203063488006592
Epoch 1910, val loss: 1.4136691093444824
Epoch 1920, training loss: 0.06316090375185013 = 0.0010079185012727976 + 0.01 * 6.215298652648926
Epoch 1920, val loss: 1.4144139289855957
Epoch 1930, training loss: 0.06298796087503433 = 0.001002277946099639 + 0.01 * 6.198568820953369
Epoch 1930, val loss: 1.4151406288146973
Epoch 1940, training loss: 0.06292000412940979 = 0.0009967373916879296 + 0.01 * 6.192326545715332
Epoch 1940, val loss: 1.4158987998962402
Epoch 1950, training loss: 0.06292318552732468 = 0.0009912778623402119 + 0.01 * 6.193190574645996
Epoch 1950, val loss: 1.4166221618652344
Epoch 1960, training loss: 0.06304075568914413 = 0.0009858929552137852 + 0.01 * 6.205486297607422
Epoch 1960, val loss: 1.417330026626587
Epoch 1970, training loss: 0.06319606304168701 = 0.0009805975714698434 + 0.01 * 6.221546649932861
Epoch 1970, val loss: 1.4180580377578735
Epoch 1980, training loss: 0.06300068646669388 = 0.0009753485792316496 + 0.01 * 6.202533721923828
Epoch 1980, val loss: 1.418781042098999
Epoch 1990, training loss: 0.06280868500471115 = 0.0009701973758637905 + 0.01 * 6.183848857879639
Epoch 1990, val loss: 1.4194879531860352
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 1.0000
Flip ASR: 1.0000/225 nodes
The final ASR:0.75646, 0.20193, Accuracy:0.80741, 0.01090
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11630])
remove edge: torch.Size([2, 9514])
updated graph: torch.Size([2, 10588])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.97663, 0.00174, Accuracy:0.82593, 0.00302
Begin epxeriment: cont_weight: 0.01 epoch:2000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.0455915927886963 = 1.961852788925171 + 0.01 * 8.373884201049805
Epoch 0, val loss: 1.9577232599258423
Epoch 10, training loss: 2.034960985183716 = 1.9512228965759277 + 0.01 * 8.37381362915039
Epoch 10, val loss: 1.9477380514144897
Epoch 20, training loss: 2.0222668647766113 = 1.9385309219360352 + 0.01 * 8.373590469360352
Epoch 20, val loss: 1.9354041814804077
Epoch 30, training loss: 2.0048789978027344 = 1.9211488962173462 + 0.01 * 8.373022079467773
Epoch 30, val loss: 1.9180787801742554
Epoch 40, training loss: 1.9795691967010498 = 1.8958605527877808 + 0.01 * 8.370862007141113
Epoch 40, val loss: 1.8927663564682007
Epoch 50, training loss: 1.9434001445770264 = 1.8598324060440063 + 0.01 * 8.356769561767578
Epoch 50, val loss: 1.85783052444458
Epoch 60, training loss: 1.8980649709701538 = 1.81535005569458 + 0.01 * 8.271492004394531
Epoch 60, val loss: 1.8185235261917114
Epoch 70, training loss: 1.8523309230804443 = 1.773302435874939 + 0.01 * 7.9028449058532715
Epoch 70, val loss: 1.7866032123565674
Epoch 80, training loss: 1.8025248050689697 = 1.7301219701766968 + 0.01 * 7.240284442901611
Epoch 80, val loss: 1.7519148588180542
Epoch 90, training loss: 1.7418261766433716 = 1.6717318296432495 + 0.01 * 7.009429931640625
Epoch 90, val loss: 1.7020524740219116
Epoch 100, training loss: 1.6616746187210083 = 1.5920253992080688 + 0.01 * 6.964925765991211
Epoch 100, val loss: 1.6347602605819702
Epoch 110, training loss: 1.560281753540039 = 1.4909530878067017 + 0.01 * 6.9328718185424805
Epoch 110, val loss: 1.551065444946289
Epoch 120, training loss: 1.4486746788024902 = 1.3795164823532104 + 0.01 * 6.915815353393555
Epoch 120, val loss: 1.4627058506011963
Epoch 130, training loss: 1.3380414247512817 = 1.2689918279647827 + 0.01 * 6.9049577713012695
Epoch 130, val loss: 1.3766601085662842
Epoch 140, training loss: 1.2331018447875977 = 1.164154291152954 + 0.01 * 6.894754409790039
Epoch 140, val loss: 1.297554850578308
Epoch 150, training loss: 1.137826919555664 = 1.0689762830734253 + 0.01 * 6.885061740875244
Epoch 150, val loss: 1.227006435394287
Epoch 160, training loss: 1.054411768913269 = 0.9856313467025757 + 0.01 * 6.878045558929443
Epoch 160, val loss: 1.1655412912368774
Epoch 170, training loss: 0.9809987545013428 = 0.9122506380081177 + 0.01 * 6.874812126159668
Epoch 170, val loss: 1.111390233039856
Epoch 180, training loss: 0.913090705871582 = 0.8443520665168762 + 0.01 * 6.873863220214844
Epoch 180, val loss: 1.060549020767212
Epoch 190, training loss: 0.846244215965271 = 0.7775075435638428 + 0.01 * 6.87367057800293
Epoch 190, val loss: 1.0095857381820679
Epoch 200, training loss: 0.777910053730011 = 0.7091718912124634 + 0.01 * 6.873818397521973
Epoch 200, val loss: 0.9563565850257874
Epoch 210, training loss: 0.7084976434707642 = 0.6397607922554016 + 0.01 * 6.873682022094727
Epoch 210, val loss: 0.9013829231262207
Epoch 220, training loss: 0.6406553387641907 = 0.5719286203384399 + 0.01 * 6.872674465179443
Epoch 220, val loss: 0.8478273749351501
Epoch 230, training loss: 0.5773688554763794 = 0.5086618065834045 + 0.01 * 6.870707035064697
Epoch 230, val loss: 0.7997240424156189
Epoch 240, training loss: 0.520534336566925 = 0.45185989141464233 + 0.01 * 6.86744499206543
Epoch 240, val loss: 0.7598163485527039
Epoch 250, training loss: 0.47069400548934937 = 0.40206378698349 + 0.01 * 6.863021373748779
Epoch 250, val loss: 0.7289120554924011
Epoch 260, training loss: 0.42758190631866455 = 0.35901081562042236 + 0.01 * 6.857110977172852
Epoch 260, val loss: 0.7064617276191711
Epoch 270, training loss: 0.3906610608100891 = 0.32214343547821045 + 0.01 * 6.85176420211792
Epoch 270, val loss: 0.6912882328033447
Epoch 280, training loss: 0.35910409688949585 = 0.2907189130783081 + 0.01 * 6.83851957321167
Epoch 280, val loss: 0.6822157502174377
Epoch 290, training loss: 0.33213144540786743 = 0.26381728053092957 + 0.01 * 6.831417083740234
Epoch 290, val loss: 0.678126335144043
Epoch 300, training loss: 0.30848926305770874 = 0.2404012382030487 + 0.01 * 6.808801651000977
Epoch 300, val loss: 0.6778057217597961
Epoch 310, training loss: 0.28743746876716614 = 0.2193462997674942 + 0.01 * 6.809116840362549
Epoch 310, val loss: 0.6800569891929626
Epoch 320, training loss: 0.2674235701560974 = 0.19959542155265808 + 0.01 * 6.782816410064697
Epoch 320, val loss: 0.6837970018386841
Epoch 330, training loss: 0.24808742105960846 = 0.18021537363529205 + 0.01 * 6.787204742431641
Epoch 330, val loss: 0.688227117061615
Epoch 340, training loss: 0.22824549674987793 = 0.1606856733560562 + 0.01 * 6.755981922149658
Epoch 340, val loss: 0.692764937877655
Epoch 350, training loss: 0.20840975642204285 = 0.14106246829032898 + 0.01 * 6.73472785949707
Epoch 350, val loss: 0.6972939372062683
Epoch 360, training loss: 0.1893509179353714 = 0.122090645134449 + 0.01 * 6.726027488708496
Epoch 360, val loss: 0.7020930647850037
Epoch 370, training loss: 0.1721695065498352 = 0.10486222058534622 + 0.01 * 6.7307281494140625
Epoch 370, val loss: 0.7076385617256165
Epoch 380, training loss: 0.15722337365150452 = 0.09007822722196579 + 0.01 * 6.714514255523682
Epoch 380, val loss: 0.7142170071601868
Epoch 390, training loss: 0.14497649669647217 = 0.07786668837070465 + 0.01 * 6.7109808921813965
Epoch 390, val loss: 0.7217879891395569
Epoch 400, training loss: 0.1349678635597229 = 0.06792259216308594 + 0.01 * 6.704526424407959
Epoch 400, val loss: 0.7300816178321838
Epoch 410, training loss: 0.12696565687656403 = 0.05978969857096672 + 0.01 * 6.717596530914307
Epoch 410, val loss: 0.7387962341308594
Epoch 420, training loss: 0.12004435807466507 = 0.0530681237578392 + 0.01 * 6.697623252868652
Epoch 420, val loss: 0.7477602362632751
Epoch 430, training loss: 0.11442158371210098 = 0.047430478036403656 + 0.01 * 6.699110984802246
Epoch 430, val loss: 0.7568218111991882
Epoch 440, training loss: 0.10950696468353271 = 0.04263605922460556 + 0.01 * 6.6870903968811035
Epoch 440, val loss: 0.7660298943519592
Epoch 450, training loss: 0.10526129603385925 = 0.03850650414824486 + 0.01 * 6.675478935241699
Epoch 450, val loss: 0.7751883268356323
Epoch 460, training loss: 0.10171528160572052 = 0.03494109958410263 + 0.01 * 6.677418231964111
Epoch 460, val loss: 0.7842969298362732
Epoch 470, training loss: 0.09851352870464325 = 0.03182755783200264 + 0.01 * 6.6685967445373535
Epoch 470, val loss: 0.7932815551757812
Epoch 480, training loss: 0.09617872536182404 = 0.02909330651164055 + 0.01 * 6.7085418701171875
Epoch 480, val loss: 0.8022292852401733
Epoch 490, training loss: 0.09338265657424927 = 0.026687417179346085 + 0.01 * 6.669524192810059
Epoch 490, val loss: 0.8110301494598389
Epoch 500, training loss: 0.09108853340148926 = 0.02455708011984825 + 0.01 * 6.6531453132629395
Epoch 500, val loss: 0.819660484790802
Epoch 510, training loss: 0.08915890753269196 = 0.02266089990735054 + 0.01 * 6.649800777435303
Epoch 510, val loss: 0.8281300067901611
Epoch 520, training loss: 0.08746199309825897 = 0.02096724323928356 + 0.01 * 6.649474620819092
Epoch 520, val loss: 0.8363850116729736
Epoch 530, training loss: 0.0859343409538269 = 0.019450612366199493 + 0.01 * 6.648372650146484
Epoch 530, val loss: 0.8444297313690186
Epoch 540, training loss: 0.08448178321123123 = 0.018088655546307564 + 0.01 * 6.639313220977783
Epoch 540, val loss: 0.852320671081543
Epoch 550, training loss: 0.08324681222438812 = 0.01686209812760353 + 0.01 * 6.638471603393555
Epoch 550, val loss: 0.859960675239563
Epoch 560, training loss: 0.0820135697722435 = 0.015753576532006264 + 0.01 * 6.625999450683594
Epoch 560, val loss: 0.8673898577690125
Epoch 570, training loss: 0.08106894046068192 = 0.014750161208212376 + 0.01 * 6.631877899169922
Epoch 570, val loss: 0.8745971918106079
Epoch 580, training loss: 0.0800187885761261 = 0.013840291649103165 + 0.01 * 6.617849349975586
Epoch 580, val loss: 0.8816423416137695
Epoch 590, training loss: 0.07911361753940582 = 0.013012310490012169 + 0.01 * 6.610130786895752
Epoch 590, val loss: 0.8884968161582947
Epoch 600, training loss: 0.07866837084293365 = 0.012256724759936333 + 0.01 * 6.641164779663086
Epoch 600, val loss: 0.8951192498207092
Epoch 610, training loss: 0.07768437266349792 = 0.011567557230591774 + 0.01 * 6.611681938171387
Epoch 610, val loss: 0.9016628861427307
Epoch 620, training loss: 0.07691818475723267 = 0.010936981998383999 + 0.01 * 6.59812068939209
Epoch 620, val loss: 0.9079539775848389
Epoch 630, training loss: 0.07641248404979706 = 0.010358707047998905 + 0.01 * 6.605377674102783
Epoch 630, val loss: 0.9140467643737793
Epoch 640, training loss: 0.07580097019672394 = 0.009827367961406708 + 0.01 * 6.597360610961914
Epoch 640, val loss: 0.920070230960846
Epoch 650, training loss: 0.07536084204912186 = 0.009338255040347576 + 0.01 * 6.602259159088135
Epoch 650, val loss: 0.9258547425270081
Epoch 660, training loss: 0.07468773424625397 = 0.008886863477528095 + 0.01 * 6.580087184906006
Epoch 660, val loss: 0.931459903717041
Epoch 670, training loss: 0.07430805265903473 = 0.00846964493393898 + 0.01 * 6.583840847015381
Epoch 670, val loss: 0.9370119571685791
Epoch 680, training loss: 0.0738745629787445 = 0.008083348162472248 + 0.01 * 6.5791215896606445
Epoch 680, val loss: 0.9424000978469849
Epoch 690, training loss: 0.07342979311943054 = 0.007724735885858536 + 0.01 * 6.570505142211914
Epoch 690, val loss: 0.9476169347763062
Epoch 700, training loss: 0.07310338318347931 = 0.007391512393951416 + 0.01 * 6.5711870193481445
Epoch 700, val loss: 0.9527038335800171
Epoch 710, training loss: 0.07273159176111221 = 0.007081272546201944 + 0.01 * 6.565032005310059
Epoch 710, val loss: 0.9577312469482422
Epoch 720, training loss: 0.07233305275440216 = 0.006792229134589434 + 0.01 * 6.554081916809082
Epoch 720, val loss: 0.9625888466835022
Epoch 730, training loss: 0.07211151719093323 = 0.0065220859833061695 + 0.01 * 6.558943271636963
Epoch 730, val loss: 0.9673006534576416
Epoch 740, training loss: 0.07168285548686981 = 0.0062691038474440575 + 0.01 * 6.541375160217285
Epoch 740, val loss: 0.9720008969306946
Epoch 750, training loss: 0.0714220330119133 = 0.006032522302120924 + 0.01 * 6.538951873779297
Epoch 750, val loss: 0.9765660166740417
Epoch 760, training loss: 0.07144520431756973 = 0.005810714792460203 + 0.01 * 6.563448905944824
Epoch 760, val loss: 0.9809591770172119
Epoch 770, training loss: 0.07093646377325058 = 0.005601777229458094 + 0.01 * 6.533468723297119
Epoch 770, val loss: 0.9853184223175049
Epoch 780, training loss: 0.0708482414484024 = 0.005406067706644535 + 0.01 * 6.544217586517334
Epoch 780, val loss: 0.9895740747451782
Epoch 790, training loss: 0.07052991539239883 = 0.005221764091402292 + 0.01 * 6.530815601348877
Epoch 790, val loss: 0.9937477111816406
Epoch 800, training loss: 0.07018844783306122 = 0.005047941580414772 + 0.01 * 6.5140509605407715
Epoch 800, val loss: 0.9978122115135193
Epoch 810, training loss: 0.07001785188913345 = 0.004884312395006418 + 0.01 * 6.513354301452637
Epoch 810, val loss: 1.0017716884613037
Epoch 820, training loss: 0.0698179081082344 = 0.004729280713945627 + 0.01 * 6.50886344909668
Epoch 820, val loss: 1.005656361579895
Epoch 830, training loss: 0.06968102604150772 = 0.004582312889397144 + 0.01 * 6.509871482849121
Epoch 830, val loss: 1.0095391273498535
