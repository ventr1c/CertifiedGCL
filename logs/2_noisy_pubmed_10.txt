Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=3, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 97440])
remove edge: torch.Size([2, 79930])
updated graph: torch.Size([2, 88722])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 106.92815399169922 = 1.1056640148162842 + 10.0 * 10.58224868774414
Epoch 0, val loss: 1.1042894124984741
Epoch 10, training loss: 106.90963745117188 = 1.0981063842773438 + 10.0 * 10.58115291595459
Epoch 10, val loss: 1.0967483520507812
Epoch 20, training loss: 106.7996597290039 = 1.0891917943954468 + 10.0 * 10.571046829223633
Epoch 20, val loss: 1.0878307819366455
Epoch 30, training loss: 106.16117858886719 = 1.0786157846450806 + 10.0 * 10.508256912231445
Epoch 30, val loss: 1.0772515535354614
Epoch 40, training loss: 104.03199768066406 = 1.0676860809326172 + 10.0 * 10.296430587768555
Epoch 40, val loss: 1.0666306018829346
Epoch 50, training loss: 100.81893157958984 = 1.0570025444030762 + 10.0 * 9.97619342803955
Epoch 50, val loss: 1.0559028387069702
Epoch 60, training loss: 97.30330657958984 = 1.0472090244293213 + 10.0 * 9.625609397888184
Epoch 60, val loss: 1.0463883876800537
Epoch 70, training loss: 95.68842315673828 = 1.0381171703338623 + 10.0 * 9.465030670166016
Epoch 70, val loss: 1.0375412702560425
Epoch 80, training loss: 93.78257751464844 = 1.0304678678512573 + 10.0 * 9.275211334228516
Epoch 80, val loss: 1.0302289724349976
Epoch 90, training loss: 92.43291473388672 = 1.024032473564148 + 10.0 * 9.140888214111328
Epoch 90, val loss: 1.0238666534423828
Epoch 100, training loss: 91.69883728027344 = 1.0167384147644043 + 10.0 * 9.068209648132324
Epoch 100, val loss: 1.016497254371643
Epoch 110, training loss: 90.62919616699219 = 1.0103424787521362 + 10.0 * 8.961885452270508
Epoch 110, val loss: 1.0102040767669678
Epoch 120, training loss: 90.1990966796875 = 1.0041346549987793 + 10.0 * 8.919496536254883
Epoch 120, val loss: 1.0040364265441895
Epoch 130, training loss: 89.47310638427734 = 0.9969950914382935 + 10.0 * 8.847611427307129
Epoch 130, val loss: 0.9968836903572083
Epoch 140, training loss: 88.72160339355469 = 0.990230917930603 + 10.0 * 8.773137092590332
Epoch 140, val loss: 0.9903209805488586
Epoch 150, training loss: 88.17381286621094 = 0.9833859205245972 + 10.0 * 8.719042778015137
Epoch 150, val loss: 0.9835534691810608
Epoch 160, training loss: 87.75399017333984 = 0.9750582575798035 + 10.0 * 8.677892684936523
Epoch 160, val loss: 0.9753894805908203
Epoch 170, training loss: 87.51241302490234 = 0.9651798009872437 + 10.0 * 8.654723167419434
Epoch 170, val loss: 0.9656444191932678
Epoch 180, training loss: 87.24205780029297 = 0.9536823630332947 + 10.0 * 8.628837585449219
Epoch 180, val loss: 0.9542115926742554
Epoch 190, training loss: 87.0234146118164 = 0.9409076571464539 + 10.0 * 8.608250617980957
Epoch 190, val loss: 0.9416154623031616
Epoch 200, training loss: 86.84707641601562 = 0.9270176291465759 + 10.0 * 8.592005729675293
Epoch 200, val loss: 0.9280585050582886
Epoch 210, training loss: 86.76675415039062 = 0.9119415283203125 + 10.0 * 8.585481643676758
Epoch 210, val loss: 0.9134289026260376
Epoch 220, training loss: 86.58430480957031 = 0.8956571817398071 + 10.0 * 8.568864822387695
Epoch 220, val loss: 0.8975199460983276
Epoch 230, training loss: 86.43798828125 = 0.8783407211303711 + 10.0 * 8.555964469909668
Epoch 230, val loss: 0.8806723356246948
Epoch 240, training loss: 86.31185150146484 = 0.8601573705673218 + 10.0 * 8.54516887664795
Epoch 240, val loss: 0.8630301356315613
Epoch 250, training loss: 86.28243255615234 = 0.8412697911262512 + 10.0 * 8.544116020202637
Epoch 250, val loss: 0.8446964025497437
Epoch 260, training loss: 86.09345245361328 = 0.8215895891189575 + 10.0 * 8.527186393737793
Epoch 260, val loss: 0.8257570266723633
Epoch 270, training loss: 85.99701690673828 = 0.8015429973602295 + 10.0 * 8.519547462463379
Epoch 270, val loss: 0.8063679337501526
Epoch 280, training loss: 85.91333770751953 = 0.7812422513961792 + 10.0 * 8.513209342956543
Epoch 280, val loss: 0.7868523597717285
Epoch 290, training loss: 85.89535522460938 = 0.7608346939086914 + 10.0 * 8.513452529907227
Epoch 290, val loss: 0.7671588659286499
Epoch 300, training loss: 85.76953125 = 0.7404192090034485 + 10.0 * 8.502911567687988
Epoch 300, val loss: 0.7476138472557068
Epoch 310, training loss: 85.70541381835938 = 0.7203757762908936 + 10.0 * 8.498503684997559
Epoch 310, val loss: 0.7284127473831177
Epoch 320, training loss: 85.63822174072266 = 0.7007707953453064 + 10.0 * 8.493745803833008
Epoch 320, val loss: 0.709669828414917
Epoch 330, training loss: 85.61408996582031 = 0.681715190410614 + 10.0 * 8.493237495422363
Epoch 330, val loss: 0.691466748714447
Epoch 340, training loss: 85.52767944335938 = 0.6633443236351013 + 10.0 * 8.486433029174805
Epoch 340, val loss: 0.6740365624427795
Epoch 350, training loss: 85.46145629882812 = 0.6458871960639954 + 10.0 * 8.48155689239502
Epoch 350, val loss: 0.657494068145752
Epoch 360, training loss: 85.45062255859375 = 0.6293363571166992 + 10.0 * 8.482129096984863
Epoch 360, val loss: 0.6418659687042236
Epoch 370, training loss: 85.3561782836914 = 0.6136544346809387 + 10.0 * 8.474252700805664
Epoch 370, val loss: 0.6270642876625061
Epoch 380, training loss: 85.29399108886719 = 0.5990926027297974 + 10.0 * 8.469490051269531
Epoch 380, val loss: 0.6133769154548645
Epoch 390, training loss: 85.23527526855469 = 0.5856228470802307 + 10.0 * 8.464964866638184
Epoch 390, val loss: 0.6007784605026245
Epoch 400, training loss: 85.2884292602539 = 0.5731087327003479 + 10.0 * 8.471531867980957
Epoch 400, val loss: 0.589134156703949
Epoch 410, training loss: 85.15047454833984 = 0.561459481716156 + 10.0 * 8.458901405334473
Epoch 410, val loss: 0.5782595276832581
Epoch 420, training loss: 85.08799743652344 = 0.550873339176178 + 10.0 * 8.453712463378906
Epoch 420, val loss: 0.5684675574302673
Epoch 430, training loss: 85.0381851196289 = 0.5411946773529053 + 10.0 * 8.449699401855469
Epoch 430, val loss: 0.5595332980155945
Epoch 440, training loss: 84.99185943603516 = 0.532317578792572 + 10.0 * 8.445954322814941
Epoch 440, val loss: 0.5513592958450317
Epoch 450, training loss: 84.99629974365234 = 0.5241479873657227 + 10.0 * 8.44721508026123
Epoch 450, val loss: 0.543887197971344
Epoch 460, training loss: 84.92140197753906 = 0.516582190990448 + 10.0 * 8.440482139587402
Epoch 460, val loss: 0.5369892716407776
Epoch 470, training loss: 84.8930892944336 = 0.5097099542617798 + 10.0 * 8.438337326049805
Epoch 470, val loss: 0.5307785868644714
Epoch 480, training loss: 84.85627746582031 = 0.5033878684043884 + 10.0 * 8.43528938293457
Epoch 480, val loss: 0.5251200795173645
Epoch 490, training loss: 84.8355941772461 = 0.4975863993167877 + 10.0 * 8.43380069732666
Epoch 490, val loss: 0.5199567079544067
Epoch 500, training loss: 84.79183197021484 = 0.49227866530418396 + 10.0 * 8.42995548248291
Epoch 500, val loss: 0.5153310894966125
Epoch 510, training loss: 84.76220703125 = 0.4873776435852051 + 10.0 * 8.427482604980469
Epoch 510, val loss: 0.5110654234886169
Epoch 520, training loss: 84.73677062988281 = 0.48282232880592346 + 10.0 * 8.425395011901855
Epoch 520, val loss: 0.5071269273757935
Epoch 530, training loss: 84.80863189697266 = 0.4785573482513428 + 10.0 * 8.433008193969727
Epoch 530, val loss: 0.5034346580505371
Epoch 540, training loss: 84.69830322265625 = 0.4744851589202881 + 10.0 * 8.422381401062012
Epoch 540, val loss: 0.5000385642051697
Epoch 550, training loss: 84.66806030273438 = 0.4707321524620056 + 10.0 * 8.419733047485352
Epoch 550, val loss: 0.4969388246536255
Epoch 560, training loss: 84.64472198486328 = 0.46725961565971375 + 10.0 * 8.417745590209961
Epoch 560, val loss: 0.49414756894111633
Epoch 570, training loss: 84.62272644042969 = 0.4639904797077179 + 10.0 * 8.415873527526855
Epoch 570, val loss: 0.4915134012699127
Epoch 580, training loss: 84.62617492675781 = 0.46090391278266907 + 10.0 * 8.416526794433594
Epoch 580, val loss: 0.4890477657318115
Epoch 590, training loss: 84.6236343383789 = 0.4579128324985504 + 10.0 * 8.416571617126465
Epoch 590, val loss: 0.4868907928466797
Epoch 600, training loss: 84.5710678100586 = 0.45507317781448364 + 10.0 * 8.411600112915039
Epoch 600, val loss: 0.48466458916664124
Epoch 610, training loss: 84.5546646118164 = 0.45237797498703003 + 10.0 * 8.410228729248047
Epoch 610, val loss: 0.4826587438583374
Epoch 620, training loss: 84.53469848632812 = 0.4498071074485779 + 10.0 * 8.408489227294922
Epoch 620, val loss: 0.48075011372566223
Epoch 630, training loss: 84.51920318603516 = 0.4473367929458618 + 10.0 * 8.407186508178711
Epoch 630, val loss: 0.4789655804634094
Epoch 640, training loss: 84.54606628417969 = 0.44494736194610596 + 10.0 * 8.410112380981445
Epoch 640, val loss: 0.4773707091808319
Epoch 650, training loss: 84.54165649414062 = 0.4426417648792267 + 10.0 * 8.40990161895752
Epoch 650, val loss: 0.47564664483070374
Epoch 660, training loss: 84.50419616699219 = 0.44035881757736206 + 10.0 * 8.406383514404297
Epoch 660, val loss: 0.4741440415382385
Epoch 670, training loss: 84.47270202636719 = 0.43821877241134644 + 10.0 * 8.403448104858398
Epoch 670, val loss: 0.47267788648605347
Epoch 680, training loss: 84.45045471191406 = 0.43616485595703125 + 10.0 * 8.401429176330566
Epoch 680, val loss: 0.47132590413093567
Epoch 690, training loss: 84.43162536621094 = 0.43417733907699585 + 10.0 * 8.399744987487793
Epoch 690, val loss: 0.4700857996940613
Epoch 700, training loss: 84.42041015625 = 0.4322473108768463 + 10.0 * 8.398816108703613
Epoch 700, val loss: 0.4689064025878906
Epoch 710, training loss: 84.47996520996094 = 0.43036189675331116 + 10.0 * 8.404960632324219
Epoch 710, val loss: 0.46775251626968384
Epoch 720, training loss: 84.42335510253906 = 0.4284680187702179 + 10.0 * 8.39948844909668
Epoch 720, val loss: 0.4665567874908447
Epoch 730, training loss: 84.38188934326172 = 0.4266628921031952 + 10.0 * 8.395522117614746
Epoch 730, val loss: 0.4654848277568817
Epoch 740, training loss: 84.37103271484375 = 0.42492619156837463 + 10.0 * 8.394610404968262
Epoch 740, val loss: 0.46451640129089355
Epoch 750, training loss: 84.35969543457031 = 0.4232383668422699 + 10.0 * 8.393645286560059
Epoch 750, val loss: 0.4635281264781952
Epoch 760, training loss: 84.44229888916016 = 0.4215906262397766 + 10.0 * 8.402070999145508
Epoch 760, val loss: 0.4625765383243561
Epoch 770, training loss: 84.33856201171875 = 0.4199078381061554 + 10.0 * 8.391865730285645
Epoch 770, val loss: 0.4616921544075012
Epoch 780, training loss: 84.32929992675781 = 0.41831550002098083 + 10.0 * 8.391098976135254
Epoch 780, val loss: 0.4607929587364197
Epoch 790, training loss: 84.31317901611328 = 0.4167695641517639 + 10.0 * 8.389640808105469
Epoch 790, val loss: 0.4599709212779999
Epoch 800, training loss: 84.3497085571289 = 0.4152398109436035 + 10.0 * 8.393446922302246
Epoch 800, val loss: 0.4591915011405945
Epoch 810, training loss: 84.28644561767578 = 0.41372355818748474 + 10.0 * 8.387271881103516
Epoch 810, val loss: 0.4583798348903656
Epoch 820, training loss: 84.28276824951172 = 0.4122546911239624 + 10.0 * 8.387051582336426
Epoch 820, val loss: 0.4576942026615143
Epoch 830, training loss: 84.26205444335938 = 0.410823792219162 + 10.0 * 8.385123252868652
Epoch 830, val loss: 0.45701277256011963
Epoch 840, training loss: 84.25813293457031 = 0.4094206392765045 + 10.0 * 8.384870529174805
Epoch 840, val loss: 0.45633912086486816
Epoch 850, training loss: 84.32991027832031 = 0.4080049991607666 + 10.0 * 8.392190933227539
Epoch 850, val loss: 0.4557150602340698
Epoch 860, training loss: 84.25468444824219 = 0.40660780668258667 + 10.0 * 8.384807586669922
Epoch 860, val loss: 0.4549560546875
Epoch 870, training loss: 84.22491455078125 = 0.4052574634552002 + 10.0 * 8.381965637207031
Epoch 870, val loss: 0.4543321132659912
Epoch 880, training loss: 84.21470642089844 = 0.4039442539215088 + 10.0 * 8.381075859069824
Epoch 880, val loss: 0.4537771940231323
Epoch 890, training loss: 84.20157623291016 = 0.40265408158302307 + 10.0 * 8.379892349243164
Epoch 890, val loss: 0.4532456696033478
Epoch 900, training loss: 84.1995620727539 = 0.40138131380081177 + 10.0 * 8.379817962646484
Epoch 900, val loss: 0.4526924788951874
Epoch 910, training loss: 84.18780517578125 = 0.40008777379989624 + 10.0 * 8.378771781921387
Epoch 910, val loss: 0.45205235481262207
Epoch 920, training loss: 84.18790435791016 = 0.39879855513572693 + 10.0 * 8.378911018371582
Epoch 920, val loss: 0.45151418447494507
Epoch 930, training loss: 84.17133331298828 = 0.3975585401058197 + 10.0 * 8.3773775100708
Epoch 930, val loss: 0.4510195255279541
Epoch 940, training loss: 84.15958404541016 = 0.39634281396865845 + 10.0 * 8.376324653625488
Epoch 940, val loss: 0.450540155172348
Epoch 950, training loss: 84.16474151611328 = 0.39515137672424316 + 10.0 * 8.376958847045898
Epoch 950, val loss: 0.45007139444351196
Epoch 960, training loss: 84.14434814453125 = 0.3939211368560791 + 10.0 * 8.375042915344238
Epoch 960, val loss: 0.4495405852794647
Epoch 970, training loss: 84.16073608398438 = 0.39270228147506714 + 10.0 * 8.376803398132324
Epoch 970, val loss: 0.4490276873111725
Epoch 980, training loss: 84.13546752929688 = 0.3915356695652008 + 10.0 * 8.374393463134766
Epoch 980, val loss: 0.44859838485717773
Epoch 990, training loss: 84.11634826660156 = 0.3903903365135193 + 10.0 * 8.37259578704834
Epoch 990, val loss: 0.44815826416015625
Epoch 1000, training loss: 84.10944366455078 = 0.38925978541374207 + 10.0 * 8.372018814086914
Epoch 1000, val loss: 0.44773855805397034
Epoch 1010, training loss: 84.10053253173828 = 0.388136088848114 + 10.0 * 8.37123966217041
Epoch 1010, val loss: 0.44732534885406494
Epoch 1020, training loss: 84.18384552001953 = 0.3870103359222412 + 10.0 * 8.379683494567871
Epoch 1020, val loss: 0.4470021426677704
Epoch 1030, training loss: 84.13029479980469 = 0.3858562409877777 + 10.0 * 8.374444007873535
Epoch 1030, val loss: 0.44648176431655884
Epoch 1040, training loss: 84.09738159179688 = 0.3847231864929199 + 10.0 * 8.37126636505127
Epoch 1040, val loss: 0.44608956575393677
Epoch 1050, training loss: 84.07205200195312 = 0.38362085819244385 + 10.0 * 8.368843078613281
Epoch 1050, val loss: 0.44564583897590637
Epoch 1060, training loss: 84.07109069824219 = 0.38253700733184814 + 10.0 * 8.368855476379395
Epoch 1060, val loss: 0.445278525352478
Epoch 1070, training loss: 84.09422302246094 = 0.3814612030982971 + 10.0 * 8.371275901794434
Epoch 1070, val loss: 0.44483792781829834
Epoch 1080, training loss: 84.05291748046875 = 0.3803565204143524 + 10.0 * 8.367256164550781
Epoch 1080, val loss: 0.4445652663707733
Epoch 1090, training loss: 84.04689025878906 = 0.37927931547164917 + 10.0 * 8.366761207580566
Epoch 1090, val loss: 0.44416600465774536
Epoch 1100, training loss: 84.07369232177734 = 0.3782069683074951 + 10.0 * 8.369548797607422
Epoch 1100, val loss: 0.44373437762260437
Epoch 1110, training loss: 84.04106903076172 = 0.3771135210990906 + 10.0 * 8.366395950317383
Epoch 1110, val loss: 0.44342315196990967
Epoch 1120, training loss: 84.03323364257812 = 0.37604644894599915 + 10.0 * 8.365718841552734
Epoch 1120, val loss: 0.44294747710227966
Epoch 1130, training loss: 84.04595947265625 = 0.37497010827064514 + 10.0 * 8.367098808288574
Epoch 1130, val loss: 0.4426248371601105
Epoch 1140, training loss: 84.01934051513672 = 0.37390199303627014 + 10.0 * 8.364543914794922
Epoch 1140, val loss: 0.4422382414340973
Epoch 1150, training loss: 84.0084457397461 = 0.37284690141677856 + 10.0 * 8.36355972290039
Epoch 1150, val loss: 0.4419151544570923
Epoch 1160, training loss: 84.01837921142578 = 0.37179598212242126 + 10.0 * 8.36465835571289
Epoch 1160, val loss: 0.4415452480316162
Epoch 1170, training loss: 84.0041732788086 = 0.370743066072464 + 10.0 * 8.363343238830566
Epoch 1170, val loss: 0.4411768317222595
Epoch 1180, training loss: 84.00680541992188 = 0.36968711018562317 + 10.0 * 8.3637113571167
Epoch 1180, val loss: 0.44085201621055603
Epoch 1190, training loss: 84.0081558227539 = 0.3686372935771942 + 10.0 * 8.363951683044434
Epoch 1190, val loss: 0.4404054284095764
Epoch 1200, training loss: 83.9947738647461 = 0.36757197976112366 + 10.0 * 8.362720489501953
Epoch 1200, val loss: 0.4401354491710663
Epoch 1210, training loss: 83.97635650634766 = 0.366527259349823 + 10.0 * 8.360982894897461
Epoch 1210, val loss: 0.4396943747997284
Epoch 1220, training loss: 83.96487426757812 = 0.36547625064849854 + 10.0 * 8.359939575195312
Epoch 1220, val loss: 0.43938809633255005
Epoch 1230, training loss: 84.02262878417969 = 0.3644387423992157 + 10.0 * 8.365818977355957
Epoch 1230, val loss: 0.43894433975219727
Epoch 1240, training loss: 83.98731994628906 = 0.3633401393890381 + 10.0 * 8.362398147583008
Epoch 1240, val loss: 0.43874162435531616
Epoch 1250, training loss: 83.96437072753906 = 0.3622753322124481 + 10.0 * 8.360209465026855
Epoch 1250, val loss: 0.4382944703102112
Epoch 1260, training loss: 83.9451675415039 = 0.36121073365211487 + 10.0 * 8.35839557647705
Epoch 1260, val loss: 0.4379582405090332
Epoch 1270, training loss: 83.93318939208984 = 0.3601526916027069 + 10.0 * 8.357303619384766
Epoch 1270, val loss: 0.4376668930053711
Epoch 1280, training loss: 83.93598937988281 = 0.35910430550575256 + 10.0 * 8.357687950134277
Epoch 1280, val loss: 0.4372945725917816
Epoch 1290, training loss: 83.96772003173828 = 0.3580375909805298 + 10.0 * 8.360967636108398
Epoch 1290, val loss: 0.43697503209114075
Epoch 1300, training loss: 83.95476531982422 = 0.35696130990982056 + 10.0 * 8.359780311584473
Epoch 1300, val loss: 0.43658068776130676
Epoch 1310, training loss: 83.9266357421875 = 0.35588887333869934 + 10.0 * 8.357074737548828
Epoch 1310, val loss: 0.4362773895263672
Epoch 1320, training loss: 83.92576599121094 = 0.35482341051101685 + 10.0 * 8.357094764709473
Epoch 1320, val loss: 0.43590548634529114
Epoch 1330, training loss: 83.90851593017578 = 0.35375460982322693 + 10.0 * 8.355476379394531
Epoch 1330, val loss: 0.4356018602848053
Epoch 1340, training loss: 83.89356231689453 = 0.35269609093666077 + 10.0 * 8.354085922241211
Epoch 1340, val loss: 0.4352605938911438
Epoch 1350, training loss: 83.90533447265625 = 0.3516407608985901 + 10.0 * 8.355369567871094
Epoch 1350, val loss: 0.4349564015865326
Epoch 1360, training loss: 83.9103775024414 = 0.3505619764328003 + 10.0 * 8.355981826782227
Epoch 1360, val loss: 0.43462884426116943
Epoch 1370, training loss: 83.88525390625 = 0.349479079246521 + 10.0 * 8.353577613830566
Epoch 1370, val loss: 0.4343021512031555
Epoch 1380, training loss: 83.90396118164062 = 0.3484032154083252 + 10.0 * 8.355555534362793
Epoch 1380, val loss: 0.4340074062347412
Epoch 1390, training loss: 83.87936401367188 = 0.34732526540756226 + 10.0 * 8.353203773498535
Epoch 1390, val loss: 0.43362295627593994
Epoch 1400, training loss: 83.87197875976562 = 0.34624752402305603 + 10.0 * 8.35257339477539
Epoch 1400, val loss: 0.4333259165287018
Epoch 1410, training loss: 83.8582763671875 = 0.34517595171928406 + 10.0 * 8.351309776306152
Epoch 1410, val loss: 0.4329928457736969
Epoch 1420, training loss: 83.87577819824219 = 0.34410327672958374 + 10.0 * 8.353167533874512
Epoch 1420, val loss: 0.4326612651348114
Epoch 1430, training loss: 83.87010955810547 = 0.34301242232322693 + 10.0 * 8.352709770202637
Epoch 1430, val loss: 0.4323926568031311
Epoch 1440, training loss: 83.86544036865234 = 0.34191516041755676 + 10.0 * 8.352352142333984
Epoch 1440, val loss: 0.43203890323638916
Epoch 1450, training loss: 83.86343383789062 = 0.34082314372062683 + 10.0 * 8.352261543273926
Epoch 1450, val loss: 0.43175387382507324
Epoch 1460, training loss: 83.84886932373047 = 0.3397309482097626 + 10.0 * 8.350914001464844
Epoch 1460, val loss: 0.43147483468055725
Epoch 1470, training loss: 83.86105346679688 = 0.3386422097682953 + 10.0 * 8.352240562438965
Epoch 1470, val loss: 0.4312155246734619
Epoch 1480, training loss: 83.83161926269531 = 0.3375520706176758 + 10.0 * 8.349406242370605
Epoch 1480, val loss: 0.43087902665138245
Epoch 1490, training loss: 83.8248519897461 = 0.3364616930484772 + 10.0 * 8.348838806152344
Epoch 1490, val loss: 0.4306293725967407
Epoch 1500, training loss: 83.81869506835938 = 0.3353729546070099 + 10.0 * 8.348332405090332
Epoch 1500, val loss: 0.4303176701068878
Epoch 1510, training loss: 83.87821197509766 = 0.3342817425727844 + 10.0 * 8.354393005371094
Epoch 1510, val loss: 0.4300406575202942
Epoch 1520, training loss: 83.83135223388672 = 0.3331611752510071 + 10.0 * 8.34981918334961
Epoch 1520, val loss: 0.42985305190086365
Epoch 1530, training loss: 83.81470489501953 = 0.33205002546310425 + 10.0 * 8.348265647888184
Epoch 1530, val loss: 0.42944779992103577
Epoch 1540, training loss: 83.82120513916016 = 0.3309381306171417 + 10.0 * 8.349026679992676
Epoch 1540, val loss: 0.4292360544204712
Epoch 1550, training loss: 83.82084655761719 = 0.3298226594924927 + 10.0 * 8.349102020263672
Epoch 1550, val loss: 0.4288894534111023
Epoch 1560, training loss: 83.81169128417969 = 0.3287011981010437 + 10.0 * 8.348299026489258
Epoch 1560, val loss: 0.4286239743232727
Epoch 1570, training loss: 83.7977066040039 = 0.3275816738605499 + 10.0 * 8.347012519836426
Epoch 1570, val loss: 0.42835813760757446
Epoch 1580, training loss: 83.78280639648438 = 0.3264639675617218 + 10.0 * 8.345634460449219
Epoch 1580, val loss: 0.42813602089881897
Epoch 1590, training loss: 83.78460693359375 = 0.3253503143787384 + 10.0 * 8.345926284790039
Epoch 1590, val loss: 0.4278873801231384
Epoch 1600, training loss: 83.8614730834961 = 0.32422691583633423 + 10.0 * 8.353724479675293
Epoch 1600, val loss: 0.4277213513851166
Epoch 1610, training loss: 83.79853820800781 = 0.3230906128883362 + 10.0 * 8.34754467010498
Epoch 1610, val loss: 0.427388459444046
Epoch 1620, training loss: 83.77425384521484 = 0.3219456076622009 + 10.0 * 8.345231056213379
Epoch 1620, val loss: 0.42719322443008423
Epoch 1630, training loss: 83.76533508300781 = 0.320813924074173 + 10.0 * 8.344451904296875
Epoch 1630, val loss: 0.4269924461841583
Epoch 1640, training loss: 83.77758026123047 = 0.31968244910240173 + 10.0 * 8.345789909362793
Epoch 1640, val loss: 0.42678001523017883
Epoch 1650, training loss: 83.77388763427734 = 0.3185434639453888 + 10.0 * 8.345534324645996
Epoch 1650, val loss: 0.4266384243965149
Epoch 1660, training loss: 83.76497650146484 = 0.3174005150794983 + 10.0 * 8.344758033752441
Epoch 1660, val loss: 0.42641910910606384
Epoch 1670, training loss: 83.77104187011719 = 0.3162596821784973 + 10.0 * 8.345478057861328
Epoch 1670, val loss: 0.4262661337852478
Epoch 1680, training loss: 83.75872039794922 = 0.3151179850101471 + 10.0 * 8.3443603515625
Epoch 1680, val loss: 0.42615246772766113
Epoch 1690, training loss: 83.75035095214844 = 0.3139767348766327 + 10.0 * 8.343637466430664
Epoch 1690, val loss: 0.42605870962142944
Epoch 1700, training loss: 83.74103546142578 = 0.3128320872783661 + 10.0 * 8.342820167541504
Epoch 1700, val loss: 0.4259048402309418
Epoch 1710, training loss: 83.73445129394531 = 0.31168508529663086 + 10.0 * 8.342276573181152
Epoch 1710, val loss: 0.42579326033592224
Epoch 1720, training loss: 83.75739288330078 = 0.3105379641056061 + 10.0 * 8.344685554504395
Epoch 1720, val loss: 0.4256846010684967
Epoch 1730, training loss: 83.75534057617188 = 0.30937454104423523 + 10.0 * 8.344596862792969
Epoch 1730, val loss: 0.42564302682876587
Epoch 1740, training loss: 83.74633026123047 = 0.3082106113433838 + 10.0 * 8.343811988830566
Epoch 1740, val loss: 0.42556387186050415
Epoch 1750, training loss: 83.73944091796875 = 0.307047039270401 + 10.0 * 8.343239784240723
Epoch 1750, val loss: 0.42550140619277954
Epoch 1760, training loss: 83.71830749511719 = 0.3058776259422302 + 10.0 * 8.341242790222168
Epoch 1760, val loss: 0.42552682757377625
Epoch 1770, training loss: 83.7076416015625 = 0.30471622943878174 + 10.0 * 8.340291976928711
Epoch 1770, val loss: 0.4254375696182251
Epoch 1780, training loss: 83.70276641845703 = 0.303548663854599 + 10.0 * 8.339921951293945
Epoch 1780, val loss: 0.42541083693504333
Epoch 1790, training loss: 83.71814727783203 = 0.3023754358291626 + 10.0 * 8.341577529907227
Epoch 1790, val loss: 0.4253963828086853
Epoch 1800, training loss: 83.71045684814453 = 0.3011864125728607 + 10.0 * 8.340927124023438
Epoch 1800, val loss: 0.42552030086517334
Epoch 1810, training loss: 83.75045013427734 = 0.2999904453754425 + 10.0 * 8.345046043395996
Epoch 1810, val loss: 0.42545846104621887
Epoch 1820, training loss: 83.7123794555664 = 0.2987770736217499 + 10.0 * 8.341360092163086
Epoch 1820, val loss: 0.4255147874355316
Epoch 1830, training loss: 83.69352722167969 = 0.2975696921348572 + 10.0 * 8.339595794677734
Epoch 1830, val loss: 0.4255399703979492
Epoch 1840, training loss: 83.68365478515625 = 0.2963673770427704 + 10.0 * 8.338728904724121
Epoch 1840, val loss: 0.42560508847236633
Epoch 1850, training loss: 83.68711853027344 = 0.2951629161834717 + 10.0 * 8.339195251464844
Epoch 1850, val loss: 0.4257251024246216
Epoch 1860, training loss: 83.70807647705078 = 0.2939496636390686 + 10.0 * 8.341412544250488
Epoch 1860, val loss: 0.4258085787296295
Epoch 1870, training loss: 83.70701599121094 = 0.2927275598049164 + 10.0 * 8.341428756713867
Epoch 1870, val loss: 0.4259064793586731
Epoch 1880, training loss: 83.6776123046875 = 0.2914932668209076 + 10.0 * 8.338611602783203
Epoch 1880, val loss: 0.4260249137878418
Epoch 1890, training loss: 83.66840362548828 = 0.29026031494140625 + 10.0 * 8.337814331054688
Epoch 1890, val loss: 0.4261285364627838
Epoch 1900, training loss: 83.6676254272461 = 0.2890261113643646 + 10.0 * 8.337860107421875
Epoch 1900, val loss: 0.4262290894985199
Epoch 1910, training loss: 83.68232727050781 = 0.28778135776519775 + 10.0 * 8.339454650878906
Epoch 1910, val loss: 0.42632821202278137
Epoch 1920, training loss: 83.66749572753906 = 0.28652453422546387 + 10.0 * 8.33809757232666
Epoch 1920, val loss: 0.42643624544143677
Epoch 1930, training loss: 83.68389892578125 = 0.2852601408958435 + 10.0 * 8.339863777160645
Epoch 1930, val loss: 0.42661479115486145
Epoch 1940, training loss: 83.66203308105469 = 0.28397879004478455 + 10.0 * 8.337804794311523
Epoch 1940, val loss: 0.42672890424728394
Epoch 1950, training loss: 83.64718627929688 = 0.28270092606544495 + 10.0 * 8.336448669433594
Epoch 1950, val loss: 0.42688456177711487
Epoch 1960, training loss: 83.63664245605469 = 0.28141772747039795 + 10.0 * 8.335522651672363
Epoch 1960, val loss: 0.427081823348999
Epoch 1970, training loss: 83.63163757324219 = 0.28013116121292114 + 10.0 * 8.335150718688965
Epoch 1970, val loss: 0.4272822439670563
Epoch 1980, training loss: 83.62933349609375 = 0.27883902192115784 + 10.0 * 8.335049629211426
Epoch 1980, val loss: 0.42752739787101746
Epoch 1990, training loss: 83.6600341796875 = 0.27754002809524536 + 10.0 * 8.338249206542969
Epoch 1990, val loss: 0.4277411997318268
Epoch 2000, training loss: 83.6581039428711 = 0.27622365951538086 + 10.0 * 8.338188171386719
Epoch 2000, val loss: 0.42807409167289734
Epoch 2010, training loss: 83.66012573242188 = 0.2749030292034149 + 10.0 * 8.338521957397461
Epoch 2010, val loss: 0.4282439351081848
Epoch 2020, training loss: 83.62025451660156 = 0.27357372641563416 + 10.0 * 8.334668159484863
Epoch 2020, val loss: 0.42852410674095154
Epoch 2030, training loss: 83.61334991455078 = 0.27224481105804443 + 10.0 * 8.334110260009766
Epoch 2030, val loss: 0.4287993013858795
Epoch 2040, training loss: 83.60992431640625 = 0.27090945839881897 + 10.0 * 8.333901405334473
Epoch 2040, val loss: 0.4290604889392853
Epoch 2050, training loss: 83.6519775390625 = 0.2695721387863159 + 10.0 * 8.338240623474121
Epoch 2050, val loss: 0.4293290972709656
Epoch 2060, training loss: 83.60285186767578 = 0.26820552349090576 + 10.0 * 8.333464622497559
Epoch 2060, val loss: 0.4297284185886383
Epoch 2070, training loss: 83.59805297851562 = 0.2668376564979553 + 10.0 * 8.333121299743652
Epoch 2070, val loss: 0.4300977885723114
Epoch 2080, training loss: 83.59668731689453 = 0.26547080278396606 + 10.0 * 8.333121299743652
Epoch 2080, val loss: 0.43044692277908325
Epoch 2090, training loss: 83.64473724365234 = 0.2641075849533081 + 10.0 * 8.33806324005127
Epoch 2090, val loss: 0.43089982867240906
Epoch 2100, training loss: 83.61641693115234 = 0.26271796226501465 + 10.0 * 8.335370063781738
Epoch 2100, val loss: 0.4312278628349304
Epoch 2110, training loss: 83.60431671142578 = 0.2613334357738495 + 10.0 * 8.334299087524414
Epoch 2110, val loss: 0.4316633343696594
Epoch 2120, training loss: 83.5853042602539 = 0.2599397897720337 + 10.0 * 8.332536697387695
Epoch 2120, val loss: 0.4319974482059479
Epoch 2130, training loss: 83.57901763916016 = 0.2585461735725403 + 10.0 * 8.332047462463379
Epoch 2130, val loss: 0.4324348568916321
Epoch 2140, training loss: 83.58148193359375 = 0.25714483857154846 + 10.0 * 8.332433700561523
Epoch 2140, val loss: 0.432891309261322
Epoch 2150, training loss: 83.64554595947266 = 0.2557433843612671 + 10.0 * 8.338979721069336
Epoch 2150, val loss: 0.4333951473236084
Epoch 2160, training loss: 83.59805297851562 = 0.2543119192123413 + 10.0 * 8.33437442779541
Epoch 2160, val loss: 0.43378573656082153
Epoch 2170, training loss: 83.57733917236328 = 0.2528776228427887 + 10.0 * 8.332446098327637
Epoch 2170, val loss: 0.4341697096824646
Epoch 2180, training loss: 83.57571411132812 = 0.25144800543785095 + 10.0 * 8.332426071166992
Epoch 2180, val loss: 0.4347948133945465
Epoch 2190, training loss: 83.5976791381836 = 0.25001221895217896 + 10.0 * 8.334766387939453
Epoch 2190, val loss: 0.4352717697620392
Epoch 2200, training loss: 83.56185913085938 = 0.24855585396289825 + 10.0 * 8.331330299377441
Epoch 2200, val loss: 0.4357464611530304
Epoch 2210, training loss: 83.55803680419922 = 0.24709077179431915 + 10.0 * 8.331094741821289
Epoch 2210, val loss: 0.4362841546535492
Epoch 2220, training loss: 83.5843276977539 = 0.245618537068367 + 10.0 * 8.333870887756348
Epoch 2220, val loss: 0.4368269443511963
Epoch 2230, training loss: 83.55606842041016 = 0.2441297471523285 + 10.0 * 8.331193923950195
Epoch 2230, val loss: 0.43743476271629333
Epoch 2240, training loss: 83.55668640136719 = 0.24263758957386017 + 10.0 * 8.331404685974121
Epoch 2240, val loss: 0.43800070881843567
Epoch 2250, training loss: 83.558837890625 = 0.24113795161247253 + 10.0 * 8.331769943237305
Epoch 2250, val loss: 0.43857982754707336
Epoch 2260, training loss: 83.55708312988281 = 0.23963415622711182 + 10.0 * 8.331745147705078
Epoch 2260, val loss: 0.439052015542984
Epoch 2270, training loss: 83.54161071777344 = 0.23811635375022888 + 10.0 * 8.330349922180176
Epoch 2270, val loss: 0.4397171139717102
Epoch 2280, training loss: 83.53721618652344 = 0.23660168051719666 + 10.0 * 8.330060958862305
Epoch 2280, val loss: 0.44033217430114746
Epoch 2290, training loss: 83.55467987060547 = 0.23508481681346893 + 10.0 * 8.33195972442627
Epoch 2290, val loss: 0.44089430570602417
Epoch 2300, training loss: 83.5323257446289 = 0.23355647921562195 + 10.0 * 8.329876899719238
Epoch 2300, val loss: 0.44155779480934143
Epoch 2310, training loss: 83.54302978515625 = 0.2320292592048645 + 10.0 * 8.331099510192871
Epoch 2310, val loss: 0.44238343834877014
Epoch 2320, training loss: 83.58068084716797 = 0.23050500452518463 + 10.0 * 8.335017204284668
Epoch 2320, val loss: 0.44318056106567383
Epoch 2330, training loss: 83.53189086914062 = 0.22894440591335297 + 10.0 * 8.330294609069824
Epoch 2330, val loss: 0.44364699721336365
Epoch 2340, training loss: 83.51206970214844 = 0.22739100456237793 + 10.0 * 8.32846736907959
Epoch 2340, val loss: 0.4444029629230499
Epoch 2350, training loss: 83.50563049316406 = 0.22583775222301483 + 10.0 * 8.32797908782959
Epoch 2350, val loss: 0.4450860619544983
Epoch 2360, training loss: 83.50328063964844 = 0.2242811918258667 + 10.0 * 8.327899932861328
Epoch 2360, val loss: 0.4459521472454071
Epoch 2370, training loss: 83.5467758178711 = 0.22272659838199615 + 10.0 * 8.332405090332031
Epoch 2370, val loss: 0.446729451417923
Epoch 2380, training loss: 83.50492095947266 = 0.22114461660385132 + 10.0 * 8.328377723693848
Epoch 2380, val loss: 0.4474005401134491
Epoch 2390, training loss: 83.50560760498047 = 0.219561368227005 + 10.0 * 8.328604698181152
Epoch 2390, val loss: 0.448324978351593
Epoch 2400, training loss: 83.49666595458984 = 0.2179725468158722 + 10.0 * 8.327869415283203
Epoch 2400, val loss: 0.44910094141960144
Epoch 2410, training loss: 83.4971694946289 = 0.21638524532318115 + 10.0 * 8.328078269958496
Epoch 2410, val loss: 0.4500880837440491
Epoch 2420, training loss: 83.5244140625 = 0.21479633450508118 + 10.0 * 8.330961227416992
Epoch 2420, val loss: 0.45093584060668945
Epoch 2430, training loss: 83.50680541992188 = 0.21319670975208282 + 10.0 * 8.329360961914062
Epoch 2430, val loss: 0.4517137110233307
Epoch 2440, training loss: 83.48492431640625 = 0.2115952968597412 + 10.0 * 8.327333450317383
Epoch 2440, val loss: 0.4528724253177643
Epoch 2450, training loss: 83.48672485351562 = 0.20999321341514587 + 10.0 * 8.327672958374023
Epoch 2450, val loss: 0.4537942111492157
Epoch 2460, training loss: 83.52799224853516 = 0.20839396119117737 + 10.0 * 8.33195972442627
Epoch 2460, val loss: 0.4547683298587799
Epoch 2470, training loss: 83.48487091064453 = 0.20678648352622986 + 10.0 * 8.327808380126953
Epoch 2470, val loss: 0.4558708369731903
Epoch 2480, training loss: 83.47008514404297 = 0.20517955720424652 + 10.0 * 8.32649040222168
Epoch 2480, val loss: 0.4570806324481964
Epoch 2490, training loss: 83.4632339477539 = 0.20356523990631104 + 10.0 * 8.325966835021973
Epoch 2490, val loss: 0.45806553959846497
Epoch 2500, training loss: 83.46498107910156 = 0.20195327699184418 + 10.0 * 8.326303482055664
Epoch 2500, val loss: 0.4591195583343506
Epoch 2510, training loss: 83.52875518798828 = 0.20036469399929047 + 10.0 * 8.332839012145996
Epoch 2510, val loss: 0.4603894352912903
Epoch 2520, training loss: 83.47405242919922 = 0.19873468577861786 + 10.0 * 8.327531814575195
Epoch 2520, val loss: 0.461367666721344
Epoch 2530, training loss: 83.48091888427734 = 0.19711381196975708 + 10.0 * 8.328380584716797
Epoch 2530, val loss: 0.4627259373664856
Epoch 2540, training loss: 83.45460510253906 = 0.1954871267080307 + 10.0 * 8.325911521911621
Epoch 2540, val loss: 0.4639281630516052
Epoch 2550, training loss: 83.44677734375 = 0.19386424124240875 + 10.0 * 8.325291633605957
Epoch 2550, val loss: 0.46505188941955566
Epoch 2560, training loss: 83.44328308105469 = 0.19224166870117188 + 10.0 * 8.325103759765625
Epoch 2560, val loss: 0.4661913216114044
Epoch 2570, training loss: 83.46055603027344 = 0.19062469899654388 + 10.0 * 8.326992988586426
Epoch 2570, val loss: 0.46753743290901184
Epoch 2580, training loss: 83.4628677368164 = 0.1890069842338562 + 10.0 * 8.327385902404785
Epoch 2580, val loss: 0.46866196393966675
Epoch 2590, training loss: 83.44915771484375 = 0.18737885355949402 + 10.0 * 8.326177597045898
Epoch 2590, val loss: 0.46997782588005066
Epoch 2600, training loss: 83.4369888305664 = 0.18575403094291687 + 10.0 * 8.32512378692627
Epoch 2600, val loss: 0.4713728725910187
Epoch 2610, training loss: 83.4669189453125 = 0.1841423511505127 + 10.0 * 8.328277587890625
Epoch 2610, val loss: 0.4726346433162689
Epoch 2620, training loss: 83.43035125732422 = 0.18251673877239227 + 10.0 * 8.324783325195312
Epoch 2620, val loss: 0.4741680324077606
Epoch 2630, training loss: 83.43448638916016 = 0.18090024590492249 + 10.0 * 8.325358390808105
Epoch 2630, val loss: 0.47566118836402893
Epoch 2640, training loss: 83.45249938964844 = 0.17928913235664368 + 10.0 * 8.32732105255127
Epoch 2640, val loss: 0.4770256280899048
Epoch 2650, training loss: 83.41838836669922 = 0.17765887081623077 + 10.0 * 8.32407283782959
Epoch 2650, val loss: 0.47852927446365356
Epoch 2660, training loss: 83.4151382446289 = 0.17604300379753113 + 10.0 * 8.323909759521484
Epoch 2660, val loss: 0.48011159896850586
Epoch 2670, training loss: 83.4393539428711 = 0.1744324266910553 + 10.0 * 8.326492309570312
Epoch 2670, val loss: 0.4813712537288666
Epoch 2680, training loss: 83.42398071289062 = 0.17282551527023315 + 10.0 * 8.325115203857422
Epoch 2680, val loss: 0.48296502232551575
Epoch 2690, training loss: 83.40576171875 = 0.17121097445487976 + 10.0 * 8.323454856872559
Epoch 2690, val loss: 0.4845353364944458
Epoch 2700, training loss: 83.43670654296875 = 0.16962935030460358 + 10.0 * 8.32670783996582
Epoch 2700, val loss: 0.4859883785247803
Epoch 2710, training loss: 83.40438842773438 = 0.1680324673652649 + 10.0 * 8.323636054992676
Epoch 2710, val loss: 0.48757293820381165
Epoch 2720, training loss: 83.39425659179688 = 0.16644537448883057 + 10.0 * 8.322781562805176
Epoch 2720, val loss: 0.4894070327281952
Epoch 2730, training loss: 83.38963317871094 = 0.16485795378684998 + 10.0 * 8.322477340698242
Epoch 2730, val loss: 0.4907483458518982
Epoch 2740, training loss: 83.38975524902344 = 0.16327649354934692 + 10.0 * 8.322648048400879
Epoch 2740, val loss: 0.4923931360244751
Epoch 2750, training loss: 83.42366790771484 = 0.1617075353860855 + 10.0 * 8.326196670532227
Epoch 2750, val loss: 0.49393725395202637
Epoch 2760, training loss: 83.40658569335938 = 0.16013310849666595 + 10.0 * 8.324645042419434
Epoch 2760, val loss: 0.49556830525398254
Epoch 2770, training loss: 83.4119644165039 = 0.15856985747814178 + 10.0 * 8.325339317321777
Epoch 2770, val loss: 0.49690955877304077
Epoch 2780, training loss: 83.40235137939453 = 0.15702027082443237 + 10.0 * 8.324533462524414
Epoch 2780, val loss: 0.4987465739250183
Epoch 2790, training loss: 83.39075469970703 = 0.15546251833438873 + 10.0 * 8.323529243469238
Epoch 2790, val loss: 0.5004867911338806
Epoch 2800, training loss: 83.39096069335938 = 0.15392455458641052 + 10.0 * 8.32370376586914
Epoch 2800, val loss: 0.5022177696228027
Epoch 2810, training loss: 83.37749481201172 = 0.15238770842552185 + 10.0 * 8.322510719299316
Epoch 2810, val loss: 0.5040443539619446
Epoch 2820, training loss: 83.36702728271484 = 0.15085113048553467 + 10.0 * 8.32161808013916
Epoch 2820, val loss: 0.5059880018234253
Epoch 2830, training loss: 83.37239837646484 = 0.14932967722415924 + 10.0 * 8.322306632995605
Epoch 2830, val loss: 0.5076090097427368
Epoch 2840, training loss: 83.39303588867188 = 0.14782969653606415 + 10.0 * 8.3245210647583
Epoch 2840, val loss: 0.5093384385108948
Epoch 2850, training loss: 83.41012573242188 = 0.14631816744804382 + 10.0 * 8.326380729675293
Epoch 2850, val loss: 0.5111768245697021
Epoch 2860, training loss: 83.36296081542969 = 0.1448095291852951 + 10.0 * 8.321815490722656
Epoch 2860, val loss: 0.5132349133491516
Epoch 2870, training loss: 83.35420227050781 = 0.14330962300300598 + 10.0 * 8.321088790893555
Epoch 2870, val loss: 0.5152810215950012
Epoch 2880, training loss: 83.35986328125 = 0.14182977378368378 + 10.0 * 8.321803092956543
Epoch 2880, val loss: 0.5171513557434082
Epoch 2890, training loss: 83.37198638916016 = 0.14035941660404205 + 10.0 * 8.323163032531738
Epoch 2890, val loss: 0.5191277265548706
Epoch 2900, training loss: 83.34854888916016 = 0.1388792097568512 + 10.0 * 8.320966720581055
Epoch 2900, val loss: 0.5210630297660828
Epoch 2910, training loss: 83.34807586669922 = 0.1374083161354065 + 10.0 * 8.321066856384277
Epoch 2910, val loss: 0.5231229662895203
Epoch 2920, training loss: 83.38235473632812 = 0.1359616369009018 + 10.0 * 8.324639320373535
Epoch 2920, val loss: 0.5251065492630005
Epoch 2930, training loss: 83.35934448242188 = 0.13452138006687164 + 10.0 * 8.322482109069824
Epoch 2930, val loss: 0.5267553329467773
Epoch 2940, training loss: 83.35503387451172 = 0.133082315325737 + 10.0 * 8.322195053100586
Epoch 2940, val loss: 0.5286198854446411
Epoch 2950, training loss: 83.333251953125 = 0.1316559910774231 + 10.0 * 8.320159912109375
Epoch 2950, val loss: 0.5312981605529785
Epoch 2960, training loss: 83.3309326171875 = 0.13024593889713287 + 10.0 * 8.320068359375
Epoch 2960, val loss: 0.5334416031837463
Epoch 2970, training loss: 83.33992767333984 = 0.12885306775569916 + 10.0 * 8.321107864379883
Epoch 2970, val loss: 0.5359689593315125
Epoch 2980, training loss: 83.37238311767578 = 0.12746302783489227 + 10.0 * 8.324491500854492
Epoch 2980, val loss: 0.5376989245414734
Epoch 2990, training loss: 83.333740234375 = 0.12608195841312408 + 10.0 * 8.320765495300293
Epoch 2990, val loss: 0.5395836234092712
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8269913749365804
0.84228066362385
=== training gcn model ===
Epoch 0, training loss: 106.93997192382812 = 1.1174867153167725 + 10.0 * 10.58224868774414
Epoch 0, val loss: 1.1169792413711548
Epoch 10, training loss: 106.9202651977539 = 1.1086466312408447 + 10.0 * 10.581161499023438
Epoch 10, val loss: 1.1080437898635864
Epoch 20, training loss: 106.81039428710938 = 1.0982575416564941 + 10.0 * 10.571213722229004
Epoch 20, val loss: 1.0975279808044434
Epoch 30, training loss: 106.15498352050781 = 1.0863779783248901 + 10.0 * 10.506860733032227
Epoch 30, val loss: 1.085518717765808
Epoch 40, training loss: 103.79244995117188 = 1.0743335485458374 + 10.0 * 10.271811485290527
Epoch 40, val loss: 1.073623776435852
Epoch 50, training loss: 100.21763610839844 = 1.0631664991378784 + 10.0 * 9.915447235107422
Epoch 50, val loss: 1.0624581575393677
Epoch 60, training loss: 96.68092346191406 = 1.0545389652252197 + 10.0 * 9.562638282775879
Epoch 60, val loss: 1.0540436506271362
Epoch 70, training loss: 94.19255828857422 = 1.0457826852798462 + 10.0 * 9.314677238464355
Epoch 70, val loss: 1.0454078912734985
Epoch 80, training loss: 93.1381607055664 = 1.0382765531539917 + 10.0 * 9.209988594055176
Epoch 80, val loss: 1.0382680892944336
Epoch 90, training loss: 92.62931823730469 = 1.031963586807251 + 10.0 * 9.159734725952148
Epoch 90, val loss: 1.0322141647338867
Epoch 100, training loss: 91.7420654296875 = 1.0263116359710693 + 10.0 * 9.071575164794922
Epoch 100, val loss: 1.0267643928527832
Epoch 110, training loss: 90.61283111572266 = 1.0212208032608032 + 10.0 * 8.959161758422852
Epoch 110, val loss: 1.0218828916549683
Epoch 120, training loss: 89.96256256103516 = 1.0159084796905518 + 10.0 * 8.894665718078613
Epoch 120, val loss: 1.016525149345398
Epoch 130, training loss: 89.50936126708984 = 1.0094449520111084 + 10.0 * 8.849991798400879
Epoch 130, val loss: 1.0099796056747437
Epoch 140, training loss: 89.17424011230469 = 1.0020904541015625 + 10.0 * 8.817214965820312
Epoch 140, val loss: 1.0026462078094482
Epoch 150, training loss: 88.79232788085938 = 0.9940655827522278 + 10.0 * 8.779826164245605
Epoch 150, val loss: 0.9947241544723511
Epoch 160, training loss: 88.45629119873047 = 0.9855097532272339 + 10.0 * 8.747077941894531
Epoch 160, val loss: 0.9863492846488953
Epoch 170, training loss: 88.19853973388672 = 0.9761331081390381 + 10.0 * 8.722240447998047
Epoch 170, val loss: 0.9771104454994202
Epoch 180, training loss: 87.9140396118164 = 0.9655499458312988 + 10.0 * 8.694849014282227
Epoch 180, val loss: 0.9667015671730042
Epoch 190, training loss: 87.68582153320312 = 0.9539112448692322 + 10.0 * 8.67319107055664
Epoch 190, val loss: 0.9553120136260986
Epoch 200, training loss: 87.4511947631836 = 0.9412904381752014 + 10.0 * 8.65099048614502
Epoch 200, val loss: 0.942918062210083
Epoch 210, training loss: 87.2188949584961 = 0.9276052713394165 + 10.0 * 8.629129409790039
Epoch 210, val loss: 0.929543137550354
Epoch 220, training loss: 87.0147933959961 = 0.9128077030181885 + 10.0 * 8.610198020935059
Epoch 220, val loss: 0.9150825142860413
Epoch 230, training loss: 86.84894561767578 = 0.8969939947128296 + 10.0 * 8.595194816589355
Epoch 230, val loss: 0.8996439576148987
Epoch 240, training loss: 86.6781234741211 = 0.8801180720329285 + 10.0 * 8.579800605773926
Epoch 240, val loss: 0.8832747936248779
Epoch 250, training loss: 86.51165771484375 = 0.8625354766845703 + 10.0 * 8.564912796020508
Epoch 250, val loss: 0.8662315607070923
Epoch 260, training loss: 86.37857818603516 = 0.8442423939704895 + 10.0 * 8.553433418273926
Epoch 260, val loss: 0.8486171364784241
Epoch 270, training loss: 86.29266357421875 = 0.8254312872886658 + 10.0 * 8.546723365783691
Epoch 270, val loss: 0.8305537700653076
Epoch 280, training loss: 86.16681671142578 = 0.8065465092658997 + 10.0 * 8.536026954650879
Epoch 280, val loss: 0.8125085830688477
Epoch 290, training loss: 86.0621337890625 = 0.7875072360038757 + 10.0 * 8.52746295928955
Epoch 290, val loss: 0.7944021821022034
Epoch 300, training loss: 85.9728775024414 = 0.7683290839195251 + 10.0 * 8.520455360412598
Epoch 300, val loss: 0.7762376070022583
Epoch 310, training loss: 85.8904037475586 = 0.7492008805274963 + 10.0 * 8.514120101928711
Epoch 310, val loss: 0.7582115530967712
Epoch 320, training loss: 85.8277587890625 = 0.7303158640861511 + 10.0 * 8.509744644165039
Epoch 320, val loss: 0.7405256032943726
Epoch 330, training loss: 85.78094482421875 = 0.7117639780044556 + 10.0 * 8.506917953491211
Epoch 330, val loss: 0.7232707142829895
Epoch 340, training loss: 85.69044494628906 = 0.6937960386276245 + 10.0 * 8.499665260314941
Epoch 340, val loss: 0.7066768407821655
Epoch 350, training loss: 85.63871002197266 = 0.6766465306282043 + 10.0 * 8.496206283569336
Epoch 350, val loss: 0.6909627914428711
Epoch 360, training loss: 85.60498809814453 = 0.6602734923362732 + 10.0 * 8.494471549987793
Epoch 360, val loss: 0.6760987639427185
Epoch 370, training loss: 85.5414047241211 = 0.6448493003845215 + 10.0 * 8.489655494689941
Epoch 370, val loss: 0.6622424125671387
Epoch 380, training loss: 85.47520446777344 = 0.6305347681045532 + 10.0 * 8.484467506408691
Epoch 380, val loss: 0.6495426893234253
Epoch 390, training loss: 85.43962097167969 = 0.6172275543212891 + 10.0 * 8.482239723205566
Epoch 390, val loss: 0.6378470063209534
Epoch 400, training loss: 85.40489196777344 = 0.6048718094825745 + 10.0 * 8.480001449584961
Epoch 400, val loss: 0.6271871328353882
Epoch 410, training loss: 85.34046936035156 = 0.5935788750648499 + 10.0 * 8.474688529968262
Epoch 410, val loss: 0.6176193356513977
Epoch 420, training loss: 85.31320190429688 = 0.5832510590553284 + 10.0 * 8.472994804382324
Epoch 420, val loss: 0.6090078949928284
Epoch 430, training loss: 85.2802734375 = 0.5737823843955994 + 10.0 * 8.470648765563965
Epoch 430, val loss: 0.6012586951255798
Epoch 440, training loss: 85.23343658447266 = 0.5650807619094849 + 10.0 * 8.466835975646973
Epoch 440, val loss: 0.594399631023407
Epoch 450, training loss: 85.18470001220703 = 0.5572275519371033 + 10.0 * 8.462747573852539
Epoch 450, val loss: 0.5882951617240906
Epoch 460, training loss: 85.14088439941406 = 0.5500940680503845 + 10.0 * 8.459078788757324
Epoch 460, val loss: 0.5829437971115112
Epoch 470, training loss: 85.10574340820312 = 0.5435790419578552 + 10.0 * 8.456216812133789
Epoch 470, val loss: 0.5781962275505066
Epoch 480, training loss: 85.07809448242188 = 0.5376155376434326 + 10.0 * 8.454048156738281
Epoch 480, val loss: 0.5740450620651245
Epoch 490, training loss: 85.09576416015625 = 0.5320081114768982 + 10.0 * 8.456376075744629
Epoch 490, val loss: 0.5702285766601562
Epoch 500, training loss: 85.03269958496094 = 0.5268483757972717 + 10.0 * 8.45058536529541
Epoch 500, val loss: 0.5668558478355408
Epoch 510, training loss: 84.97779846191406 = 0.5221574902534485 + 10.0 * 8.445564270019531
Epoch 510, val loss: 0.563968300819397
Epoch 520, training loss: 84.97859191894531 = 0.5177941918373108 + 10.0 * 8.446080207824707
Epoch 520, val loss: 0.5614035725593567
Epoch 530, training loss: 84.96392059326172 = 0.5136514902114868 + 10.0 * 8.445027351379395
Epoch 530, val loss: 0.5588858127593994
Epoch 540, training loss: 84.8941421508789 = 0.5097653269767761 + 10.0 * 8.438437461853027
Epoch 540, val loss: 0.5568010210990906
Epoch 550, training loss: 84.86372375488281 = 0.5061603784561157 + 10.0 * 8.43575668334961
Epoch 550, val loss: 0.5549268126487732
Epoch 560, training loss: 84.84172058105469 = 0.5027754902839661 + 10.0 * 8.433894157409668
Epoch 560, val loss: 0.5532121062278748
Epoch 570, training loss: 84.8539047241211 = 0.4995628595352173 + 10.0 * 8.435434341430664
Epoch 570, val loss: 0.5516335368156433
Epoch 580, training loss: 84.79317474365234 = 0.4964923560619354 + 10.0 * 8.429668426513672
Epoch 580, val loss: 0.5502176284790039
Epoch 590, training loss: 84.76673126220703 = 0.49358364939689636 + 10.0 * 8.427314758300781
Epoch 590, val loss: 0.5488753318786621
Epoch 600, training loss: 84.78211975097656 = 0.4908055067062378 + 10.0 * 8.429131507873535
Epoch 600, val loss: 0.5476951599121094
Epoch 610, training loss: 84.77632141113281 = 0.4881021976470947 + 10.0 * 8.428821563720703
Epoch 610, val loss: 0.546453595161438
Epoch 620, training loss: 84.70671844482422 = 0.48549333214759827 + 10.0 * 8.42212200164795
Epoch 620, val loss: 0.5453091263771057
Epoch 630, training loss: 84.68316650390625 = 0.48303526639938354 + 10.0 * 8.420013427734375
Epoch 630, val loss: 0.5442525148391724
Epoch 640, training loss: 84.69718933105469 = 0.480648934841156 + 10.0 * 8.421653747558594
Epoch 640, val loss: 0.543229877948761
Epoch 650, training loss: 84.63773345947266 = 0.47833389043807983 + 10.0 * 8.415940284729004
Epoch 650, val loss: 0.5423009395599365
Epoch 660, training loss: 84.62324523925781 = 0.4761035144329071 + 10.0 * 8.414713859558105
Epoch 660, val loss: 0.5413718223571777
Epoch 670, training loss: 84.6034927368164 = 0.473930686712265 + 10.0 * 8.412956237792969
Epoch 670, val loss: 0.5404184460639954
Epoch 680, training loss: 84.59330749511719 = 0.4718170762062073 + 10.0 * 8.412149429321289
Epoch 680, val loss: 0.53949373960495
Epoch 690, training loss: 84.59146881103516 = 0.4697036147117615 + 10.0 * 8.412176132202148
Epoch 690, val loss: 0.5386894345283508
Epoch 700, training loss: 84.55935668945312 = 0.4676387310028076 + 10.0 * 8.409172058105469
Epoch 700, val loss: 0.5377240777015686
Epoch 710, training loss: 84.542724609375 = 0.4656633734703064 + 10.0 * 8.407706260681152
Epoch 710, val loss: 0.5368900895118713
Epoch 720, training loss: 84.51319885253906 = 0.4637437164783478 + 10.0 * 8.404945373535156
Epoch 720, val loss: 0.5360761880874634
Epoch 730, training loss: 84.4962387084961 = 0.4618763029575348 + 10.0 * 8.403436660766602
Epoch 730, val loss: 0.5352843403816223
Epoch 740, training loss: 84.65729522705078 = 0.4600444734096527 + 10.0 * 8.41972541809082
Epoch 740, val loss: 0.5345350503921509
Epoch 750, training loss: 84.51061248779297 = 0.4581073820590973 + 10.0 * 8.405250549316406
Epoch 750, val loss: 0.5335981249809265
Epoch 760, training loss: 84.47816467285156 = 0.4562866985797882 + 10.0 * 8.402188301086426
Epoch 760, val loss: 0.5327929854393005
Epoch 770, training loss: 84.44416046142578 = 0.4545283317565918 + 10.0 * 8.39896297454834
Epoch 770, val loss: 0.5320374965667725
Epoch 780, training loss: 84.43518829345703 = 0.45280537009239197 + 10.0 * 8.398238182067871
Epoch 780, val loss: 0.5312811136245728
Epoch 790, training loss: 84.42185974121094 = 0.45110100507736206 + 10.0 * 8.397075653076172
Epoch 790, val loss: 0.530556321144104
Epoch 800, training loss: 84.4234848022461 = 0.4494155943393707 + 10.0 * 8.397406578063965
Epoch 800, val loss: 0.5298044085502625
Epoch 810, training loss: 84.41389465332031 = 0.44768986105918884 + 10.0 * 8.39661979675293
Epoch 810, val loss: 0.5291004776954651
Epoch 820, training loss: 84.3953857421875 = 0.4459826946258545 + 10.0 * 8.394940376281738
Epoch 820, val loss: 0.5282855033874512
Epoch 830, training loss: 84.38574981689453 = 0.44432520866394043 + 10.0 * 8.394142150878906
Epoch 830, val loss: 0.5275335907936096
Epoch 840, training loss: 84.37326049804688 = 0.4426908791065216 + 10.0 * 8.393056869506836
Epoch 840, val loss: 0.5268226265907288
Epoch 850, training loss: 84.47737884521484 = 0.4410618543624878 + 10.0 * 8.403631210327148
Epoch 850, val loss: 0.5261608958244324
Epoch 860, training loss: 84.38316345214844 = 0.43937572836875916 + 10.0 * 8.394378662109375
Epoch 860, val loss: 0.5253395438194275
Epoch 870, training loss: 84.34417724609375 = 0.4377456605434418 + 10.0 * 8.390643119812012
Epoch 870, val loss: 0.5246281623840332
Epoch 880, training loss: 84.33533477783203 = 0.43614259362220764 + 10.0 * 8.38991928100586
Epoch 880, val loss: 0.5239414572715759
Epoch 890, training loss: 84.40824127197266 = 0.4345575273036957 + 10.0 * 8.397368431091309
Epoch 890, val loss: 0.5232221484184265
Epoch 900, training loss: 84.34439849853516 = 0.43289318680763245 + 10.0 * 8.39115047454834
Epoch 900, val loss: 0.5225070714950562
Epoch 910, training loss: 84.30791473388672 = 0.4312824308872223 + 10.0 * 8.387662887573242
Epoch 910, val loss: 0.5217583179473877
Epoch 920, training loss: 84.29688262939453 = 0.4296852946281433 + 10.0 * 8.386719703674316
Epoch 920, val loss: 0.5210646390914917
Epoch 930, training loss: 84.2882080078125 = 0.42810073494911194 + 10.0 * 8.386011123657227
Epoch 930, val loss: 0.5204106569290161
Epoch 940, training loss: 84.30816650390625 = 0.42651593685150146 + 10.0 * 8.388165473937988
Epoch 940, val loss: 0.5197655558586121
Epoch 950, training loss: 84.3348388671875 = 0.42489245533943176 + 10.0 * 8.39099407196045
Epoch 950, val loss: 0.5189779996871948
Epoch 960, training loss: 84.28050231933594 = 0.423251211643219 + 10.0 * 8.385725021362305
Epoch 960, val loss: 0.5182383060455322
Epoch 970, training loss: 84.25240325927734 = 0.42163991928100586 + 10.0 * 8.383076667785645
Epoch 970, val loss: 0.5175386667251587
Epoch 980, training loss: 84.24958801269531 = 0.42004331946372986 + 10.0 * 8.382954597473145
Epoch 980, val loss: 0.5168344974517822
Epoch 990, training loss: 84.25350952148438 = 0.41844239830970764 + 10.0 * 8.383506774902344
Epoch 990, val loss: 0.5161999464035034
Epoch 1000, training loss: 84.24481201171875 = 0.41682371497154236 + 10.0 * 8.38279914855957
Epoch 1000, val loss: 0.5154271125793457
Epoch 1010, training loss: 84.31749725341797 = 0.41518086194992065 + 10.0 * 8.390231132507324
Epoch 1010, val loss: 0.5146441459655762
Epoch 1020, training loss: 84.24156188964844 = 0.4134732186794281 + 10.0 * 8.382808685302734
Epoch 1020, val loss: 0.513802170753479
Epoch 1030, training loss: 84.20721435546875 = 0.41181352734565735 + 10.0 * 8.37954044342041
Epoch 1030, val loss: 0.5131253600120544
Epoch 1040, training loss: 84.200439453125 = 0.4101640284061432 + 10.0 * 8.379027366638184
Epoch 1040, val loss: 0.5123944282531738
Epoch 1050, training loss: 84.18668365478516 = 0.4085139334201813 + 10.0 * 8.377817153930664
Epoch 1050, val loss: 0.5116834044456482
Epoch 1060, training loss: 84.18431091308594 = 0.406843364238739 + 10.0 * 8.37774658203125
Epoch 1060, val loss: 0.5109638571739197
Epoch 1070, training loss: 84.2368392944336 = 0.4051353335380554 + 10.0 * 8.383170127868652
Epoch 1070, val loss: 0.5101749897003174
Epoch 1080, training loss: 84.25092315673828 = 0.4033741354942322 + 10.0 * 8.38475513458252
Epoch 1080, val loss: 0.5093729496002197
Epoch 1090, training loss: 84.17721557617188 = 0.40159642696380615 + 10.0 * 8.377561569213867
Epoch 1090, val loss: 0.5085920095443726
Epoch 1100, training loss: 84.15823364257812 = 0.39982810616493225 + 10.0 * 8.37584114074707
Epoch 1100, val loss: 0.5078175663948059
Epoch 1110, training loss: 84.14649963378906 = 0.39806532859802246 + 10.0 * 8.37484359741211
Epoch 1110, val loss: 0.5070371031761169
Epoch 1120, training loss: 84.1478500366211 = 0.3962867558002472 + 10.0 * 8.37515640258789
Epoch 1120, val loss: 0.5062352418899536
Epoch 1130, training loss: 84.16803741455078 = 0.39447221159935 + 10.0 * 8.37735652923584
Epoch 1130, val loss: 0.5054510831832886
Epoch 1140, training loss: 84.18612670898438 = 0.3926272988319397 + 10.0 * 8.379350662231445
Epoch 1140, val loss: 0.5047401785850525
Epoch 1150, training loss: 84.11347198486328 = 0.3907473385334015 + 10.0 * 8.372272491455078
Epoch 1150, val loss: 0.5038679242134094
Epoch 1160, training loss: 84.11343383789062 = 0.3888714611530304 + 10.0 * 8.372456550598145
Epoch 1160, val loss: 0.5030403733253479
Epoch 1170, training loss: 84.10247039794922 = 0.3869905471801758 + 10.0 * 8.37154769897461
Epoch 1170, val loss: 0.502295732498169
Epoch 1180, training loss: 84.1091079711914 = 0.3850879669189453 + 10.0 * 8.37240219116211
Epoch 1180, val loss: 0.5014671087265015
Epoch 1190, training loss: 84.13452911376953 = 0.38315191864967346 + 10.0 * 8.375138282775879
Epoch 1190, val loss: 0.5006738901138306
Epoch 1200, training loss: 84.08406829833984 = 0.3811742961406708 + 10.0 * 8.37028980255127
Epoch 1200, val loss: 0.49976566433906555
Epoch 1210, training loss: 84.10454559326172 = 0.37919437885284424 + 10.0 * 8.37253475189209
Epoch 1210, val loss: 0.4989536702632904
Epoch 1220, training loss: 84.09439086914062 = 0.37718865275382996 + 10.0 * 8.371720314025879
Epoch 1220, val loss: 0.4981845021247864
Epoch 1230, training loss: 84.07178497314453 = 0.37516167759895325 + 10.0 * 8.369662284851074
Epoch 1230, val loss: 0.49727147817611694
Epoch 1240, training loss: 84.08832550048828 = 0.37312543392181396 + 10.0 * 8.371520042419434
Epoch 1240, val loss: 0.49644479155540466
Epoch 1250, training loss: 84.05538177490234 = 0.37107640504837036 + 10.0 * 8.368430137634277
Epoch 1250, val loss: 0.49557530879974365
Epoch 1260, training loss: 84.04585266113281 = 0.3690270185470581 + 10.0 * 8.367682456970215
Epoch 1260, val loss: 0.49475836753845215
Epoch 1270, training loss: 84.05326843261719 = 0.3669620156288147 + 10.0 * 8.368631362915039
Epoch 1270, val loss: 0.49397552013397217
Epoch 1280, training loss: 84.0420150756836 = 0.36487236618995667 + 10.0 * 8.367713928222656
Epoch 1280, val loss: 0.4931424856185913
Epoch 1290, training loss: 84.12638854980469 = 0.36276665329933167 + 10.0 * 8.376361846923828
Epoch 1290, val loss: 0.49220162630081177
Epoch 1300, training loss: 84.04231262207031 = 0.3606310188770294 + 10.0 * 8.368167877197266
Epoch 1300, val loss: 0.4914501905441284
Epoch 1310, training loss: 84.0233383178711 = 0.35851046442985535 + 10.0 * 8.366482734680176
Epoch 1310, val loss: 0.49054861068725586
Epoch 1320, training loss: 84.0052490234375 = 0.356403648853302 + 10.0 * 8.364884376525879
Epoch 1320, val loss: 0.4897788465023041
Epoch 1330, training loss: 83.9980239868164 = 0.35429203510284424 + 10.0 * 8.364373207092285
Epoch 1330, val loss: 0.48900333046913147
Epoch 1340, training loss: 84.07160949707031 = 0.3521702289581299 + 10.0 * 8.371943473815918
Epoch 1340, val loss: 0.48825228214263916
Epoch 1350, training loss: 84.01630401611328 = 0.3499978184700012 + 10.0 * 8.366630554199219
Epoch 1350, val loss: 0.4875337779521942
Epoch 1360, training loss: 83.98699188232422 = 0.3478424847126007 + 10.0 * 8.36391544342041
Epoch 1360, val loss: 0.4868026077747345
Epoch 1370, training loss: 83.97285461425781 = 0.34568753838539124 + 10.0 * 8.362716674804688
Epoch 1370, val loss: 0.48611870408058167
Epoch 1380, training loss: 83.971923828125 = 0.3435313403606415 + 10.0 * 8.362839698791504
Epoch 1380, val loss: 0.4853802025318146
Epoch 1390, training loss: 84.10346984863281 = 0.3413526117801666 + 10.0 * 8.376211166381836
Epoch 1390, val loss: 0.4846317768096924
Epoch 1400, training loss: 83.97149658203125 = 0.3391299545764923 + 10.0 * 8.363237380981445
Epoch 1400, val loss: 0.4840444326400757
Epoch 1410, training loss: 83.95912170410156 = 0.3369465172290802 + 10.0 * 8.362217903137207
Epoch 1410, val loss: 0.48334524035453796
Epoch 1420, training loss: 83.95004272460938 = 0.3347844183444977 + 10.0 * 8.361525535583496
Epoch 1420, val loss: 0.4827575981616974
Epoch 1430, training loss: 83.9389877319336 = 0.33263030648231506 + 10.0 * 8.360635757446289
Epoch 1430, val loss: 0.4822520613670349
Epoch 1440, training loss: 83.93441772460938 = 0.3304753601551056 + 10.0 * 8.360394477844238
Epoch 1440, val loss: 0.48181819915771484
Epoch 1450, training loss: 83.94664764404297 = 0.3283243775367737 + 10.0 * 8.361832618713379
Epoch 1450, val loss: 0.48142898082733154
Epoch 1460, training loss: 83.94738006591797 = 0.3261483609676361 + 10.0 * 8.362123489379883
Epoch 1460, val loss: 0.4809654951095581
Epoch 1470, training loss: 83.96519470214844 = 0.3239711821079254 + 10.0 * 8.36412239074707
Epoch 1470, val loss: 0.4806862771511078
Epoch 1480, training loss: 83.93466186523438 = 0.32179978489875793 + 10.0 * 8.361286163330078
Epoch 1480, val loss: 0.48034343123435974
Epoch 1490, training loss: 83.91189575195312 = 0.3196552097797394 + 10.0 * 8.359224319458008
Epoch 1490, val loss: 0.4800602197647095
Epoch 1500, training loss: 83.90815734863281 = 0.31754443049430847 + 10.0 * 8.359061241149902
Epoch 1500, val loss: 0.479963481426239
Epoch 1510, training loss: 83.9228744506836 = 0.3154337406158447 + 10.0 * 8.360743522644043
Epoch 1510, val loss: 0.47971194982528687
Epoch 1520, training loss: 83.93140411376953 = 0.31331440806388855 + 10.0 * 8.361808776855469
Epoch 1520, val loss: 0.4796381890773773
Epoch 1530, training loss: 83.90036010742188 = 0.3111976385116577 + 10.0 * 8.358916282653809
Epoch 1530, val loss: 0.4795168340206146
Epoch 1540, training loss: 83.88203430175781 = 0.30909857153892517 + 10.0 * 8.357294082641602
Epoch 1540, val loss: 0.4794915020465851
Epoch 1550, training loss: 83.88130950927734 = 0.3070177733898163 + 10.0 * 8.357429504394531
Epoch 1550, val loss: 0.47957366704940796
Epoch 1560, training loss: 83.92218017578125 = 0.30493709444999695 + 10.0 * 8.361723899841309
Epoch 1560, val loss: 0.4796221852302551
Epoch 1570, training loss: 83.88683319091797 = 0.3028470575809479 + 10.0 * 8.3583984375
Epoch 1570, val loss: 0.479757159948349
Epoch 1580, training loss: 83.90583038330078 = 0.3007543087005615 + 10.0 * 8.36050796508789
Epoch 1580, val loss: 0.47997069358825684
Epoch 1590, training loss: 83.87409973144531 = 0.298675537109375 + 10.0 * 8.357542037963867
Epoch 1590, val loss: 0.4801645278930664
Epoch 1600, training loss: 83.86206817626953 = 0.29662224650382996 + 10.0 * 8.356544494628906
Epoch 1600, val loss: 0.48037102818489075
Epoch 1610, training loss: 83.85789489746094 = 0.2945977449417114 + 10.0 * 8.356328964233398
Epoch 1610, val loss: 0.4806274473667145
Epoch 1620, training loss: 83.87716674804688 = 0.2925768196582794 + 10.0 * 8.358458518981934
Epoch 1620, val loss: 0.4809555411338806
Epoch 1630, training loss: 83.85153198242188 = 0.29055580496788025 + 10.0 * 8.356097221374512
Epoch 1630, val loss: 0.48135244846343994
Epoch 1640, training loss: 83.84943389892578 = 0.28855419158935547 + 10.0 * 8.356088638305664
Epoch 1640, val loss: 0.48168885707855225
Epoch 1650, training loss: 83.83966064453125 = 0.28655877709388733 + 10.0 * 8.355310440063477
Epoch 1650, val loss: 0.48212024569511414
Epoch 1660, training loss: 83.86168670654297 = 0.28456640243530273 + 10.0 * 8.357711791992188
Epoch 1660, val loss: 0.48243626952171326
Epoch 1670, training loss: 83.85511779785156 = 0.2825721204280853 + 10.0 * 8.357254981994629
Epoch 1670, val loss: 0.48270508646965027
Epoch 1680, training loss: 83.85778045654297 = 0.28059637546539307 + 10.0 * 8.357718467712402
Epoch 1680, val loss: 0.4833512306213379
Epoch 1690, training loss: 83.82305145263672 = 0.27863359451293945 + 10.0 * 8.35444164276123
Epoch 1690, val loss: 0.4838010370731354
Epoch 1700, training loss: 83.80697631835938 = 0.27668535709381104 + 10.0 * 8.353029251098633
Epoch 1700, val loss: 0.4845157861709595
Epoch 1710, training loss: 83.8110122680664 = 0.27475202083587646 + 10.0 * 8.353626251220703
Epoch 1710, val loss: 0.4851365387439728
Epoch 1720, training loss: 83.88042449951172 = 0.2728177011013031 + 10.0 * 8.360760688781738
Epoch 1720, val loss: 0.48586586117744446
Epoch 1730, training loss: 83.80293273925781 = 0.27086544036865234 + 10.0 * 8.353206634521484
Epoch 1730, val loss: 0.4864816665649414
Epoch 1740, training loss: 83.78713989257812 = 0.2689320743083954 + 10.0 * 8.351820945739746
Epoch 1740, val loss: 0.4872558116912842
Epoch 1750, training loss: 83.7928695678711 = 0.26701709628105164 + 10.0 * 8.352585792541504
Epoch 1750, val loss: 0.48799076676368713
Epoch 1760, training loss: 83.8172607421875 = 0.26510724425315857 + 10.0 * 8.355215072631836
Epoch 1760, val loss: 0.4887109100818634
Epoch 1770, training loss: 83.80177307128906 = 0.2631848156452179 + 10.0 * 8.353858947753906
Epoch 1770, val loss: 0.4896477460861206
Epoch 1780, training loss: 83.79168701171875 = 0.26127782464027405 + 10.0 * 8.35304069519043
Epoch 1780, val loss: 0.4902936518192291
Epoch 1790, training loss: 83.78128051757812 = 0.25937262177467346 + 10.0 * 8.352190971374512
Epoch 1790, val loss: 0.49123257398605347
Epoch 1800, training loss: 83.765869140625 = 0.2574918568134308 + 10.0 * 8.350837707519531
Epoch 1800, val loss: 0.49216118454933167
Epoch 1810, training loss: 83.79273223876953 = 0.2556203305721283 + 10.0 * 8.353711128234863
Epoch 1810, val loss: 0.49310773611068726
Epoch 1820, training loss: 83.79612731933594 = 0.2537643015384674 + 10.0 * 8.354236602783203
Epoch 1820, val loss: 0.4940962493419647
Epoch 1830, training loss: 83.75360870361328 = 0.2518962621688843 + 10.0 * 8.350171089172363
Epoch 1830, val loss: 0.49536213278770447
Epoch 1840, training loss: 83.74267578125 = 0.2500497102737427 + 10.0 * 8.349262237548828
Epoch 1840, val loss: 0.4963589310646057
Epoch 1850, training loss: 83.74322509765625 = 0.24821613729000092 + 10.0 * 8.34950065612793
Epoch 1850, val loss: 0.49742403626441956
Epoch 1860, training loss: 83.79824829101562 = 0.246390700340271 + 10.0 * 8.355185508728027
Epoch 1860, val loss: 0.4984039068222046
Epoch 1870, training loss: 83.73664093017578 = 0.2445453405380249 + 10.0 * 8.349209785461426
Epoch 1870, val loss: 0.4999295771121979
Epoch 1880, training loss: 83.73556518554688 = 0.24270686507225037 + 10.0 * 8.349286079406738
Epoch 1880, val loss: 0.5008388757705688
Epoch 1890, training loss: 83.74452209472656 = 0.24089379608631134 + 10.0 * 8.350362777709961
Epoch 1890, val loss: 0.5021924376487732
Epoch 1900, training loss: 83.74099731445312 = 0.23907598853111267 + 10.0 * 8.350192070007324
Epoch 1900, val loss: 0.5033531785011292
Epoch 1910, training loss: 83.72640991210938 = 0.23726549744606018 + 10.0 * 8.34891414642334
Epoch 1910, val loss: 0.5044746994972229
Epoch 1920, training loss: 83.73426818847656 = 0.23546402156352997 + 10.0 * 8.34988021850586
Epoch 1920, val loss: 0.505438506603241
Epoch 1930, training loss: 83.71578979492188 = 0.233675017952919 + 10.0 * 8.348211288452148
Epoch 1930, val loss: 0.5070685744285583
Epoch 1940, training loss: 83.73995971679688 = 0.23187178373336792 + 10.0 * 8.350809097290039
Epoch 1940, val loss: 0.5082440376281738
Epoch 1950, training loss: 83.71833038330078 = 0.23008640110492706 + 10.0 * 8.348824501037598
Epoch 1950, val loss: 0.5099360942840576
Epoch 1960, training loss: 83.703125 = 0.2282819300889969 + 10.0 * 8.347484588623047
Epoch 1960, val loss: 0.5111732482910156
Epoch 1970, training loss: 83.69326782226562 = 0.22650930285453796 + 10.0 * 8.346675872802734
Epoch 1970, val loss: 0.5125810503959656
Epoch 1980, training loss: 83.6900863647461 = 0.2247401624917984 + 10.0 * 8.346534729003906
Epoch 1980, val loss: 0.5141470432281494
Epoch 1990, training loss: 83.7315673828125 = 0.22297106683254242 + 10.0 * 8.350859642028809
Epoch 1990, val loss: 0.5154863595962524
Epoch 2000, training loss: 83.70938110351562 = 0.22120456397533417 + 10.0 * 8.348817825317383
Epoch 2000, val loss: 0.5171003937721252
Epoch 2010, training loss: 83.68596649169922 = 0.21943190693855286 + 10.0 * 8.346653938293457
Epoch 2010, val loss: 0.518433690071106
Epoch 2020, training loss: 83.6897964477539 = 0.21767066419124603 + 10.0 * 8.347211837768555
Epoch 2020, val loss: 0.5200804471969604
Epoch 2030, training loss: 83.75228118896484 = 0.21591177582740784 + 10.0 * 8.353636741638184
Epoch 2030, val loss: 0.5215425491333008
Epoch 2040, training loss: 83.69432067871094 = 0.21413680911064148 + 10.0 * 8.348018646240234
Epoch 2040, val loss: 0.523206353187561
Epoch 2050, training loss: 83.66789245605469 = 0.21237540245056152 + 10.0 * 8.345552444458008
Epoch 2050, val loss: 0.5247660875320435
Epoch 2060, training loss: 83.65643310546875 = 0.21062789857387543 + 10.0 * 8.34458065032959
Epoch 2060, val loss: 0.5265219211578369
Epoch 2070, training loss: 83.65544128417969 = 0.20889657735824585 + 10.0 * 8.34465503692627
Epoch 2070, val loss: 0.5282696485519409
Epoch 2080, training loss: 83.75019073486328 = 0.20717734098434448 + 10.0 * 8.354301452636719
Epoch 2080, val loss: 0.5298632979393005
Epoch 2090, training loss: 83.67923736572266 = 0.2054155468940735 + 10.0 * 8.347382545471191
Epoch 2090, val loss: 0.5310527086257935
Epoch 2100, training loss: 83.65751647949219 = 0.2036871463060379 + 10.0 * 8.345382690429688
Epoch 2100, val loss: 0.5333057641983032
Epoch 2110, training loss: 83.66240692138672 = 0.20195965468883514 + 10.0 * 8.346044540405273
Epoch 2110, val loss: 0.5348684191703796
Epoch 2120, training loss: 83.66871643066406 = 0.20024307072162628 + 10.0 * 8.346847534179688
Epoch 2120, val loss: 0.5365805625915527
Epoch 2130, training loss: 83.63844299316406 = 0.1985299289226532 + 10.0 * 8.34399127960205
Epoch 2130, val loss: 0.5383762121200562
Epoch 2140, training loss: 83.62748718261719 = 0.19681960344314575 + 10.0 * 8.343066215515137
Epoch 2140, val loss: 0.5403488278388977
Epoch 2150, training loss: 83.63374328613281 = 0.19511473178863525 + 10.0 * 8.343862533569336
Epoch 2150, val loss: 0.5423411726951599
Epoch 2160, training loss: 83.65991973876953 = 0.19342663884162903 + 10.0 * 8.346649169921875
Epoch 2160, val loss: 0.5442290902137756
Epoch 2170, training loss: 83.67208862304688 = 0.19172534346580505 + 10.0 * 8.34803581237793
Epoch 2170, val loss: 0.5457955598831177
Epoch 2180, training loss: 83.63191223144531 = 0.1900363266468048 + 10.0 * 8.34418773651123
Epoch 2180, val loss: 0.5480844974517822
Epoch 2190, training loss: 83.61915588378906 = 0.18835878372192383 + 10.0 * 8.343079566955566
Epoch 2190, val loss: 0.5496466159820557
Epoch 2200, training loss: 83.60912322998047 = 0.1866975873708725 + 10.0 * 8.342242240905762
Epoch 2200, val loss: 0.5520297884941101
Epoch 2210, training loss: 83.60768127441406 = 0.1850481778383255 + 10.0 * 8.342263221740723
Epoch 2210, val loss: 0.5539782643318176
Epoch 2220, training loss: 83.66763305664062 = 0.18341171741485596 + 10.0 * 8.348422050476074
Epoch 2220, val loss: 0.5560789704322815
Epoch 2230, training loss: 83.61376953125 = 0.18175482749938965 + 10.0 * 8.343201637268066
Epoch 2230, val loss: 0.5575218200683594
Epoch 2240, training loss: 83.5968246459961 = 0.18011225759983063 + 10.0 * 8.341670989990234
Epoch 2240, val loss: 0.5599504113197327
Epoch 2250, training loss: 83.66645812988281 = 0.17848695814609528 + 10.0 * 8.348796844482422
Epoch 2250, val loss: 0.5619693398475647
Epoch 2260, training loss: 83.59849548339844 = 0.17686940729618073 + 10.0 * 8.342162132263184
Epoch 2260, val loss: 0.5638428330421448
Epoch 2270, training loss: 83.58586120605469 = 0.17523156106472015 + 10.0 * 8.341062545776367
Epoch 2270, val loss: 0.5657594203948975
Epoch 2280, training loss: 83.58026123046875 = 0.17362777888774872 + 10.0 * 8.340662956237793
Epoch 2280, val loss: 0.568209707736969
Epoch 2290, training loss: 83.59674072265625 = 0.17200875282287598 + 10.0 * 8.342473030090332
Epoch 2290, val loss: 0.5705177187919617
Epoch 2300, training loss: 83.61819458007812 = 0.17038249969482422 + 10.0 * 8.344781875610352
Epoch 2300, val loss: 0.5725289583206177
Epoch 2310, training loss: 83.58192443847656 = 0.16875414550304413 + 10.0 * 8.341317176818848
Epoch 2310, val loss: 0.5744024515151978
Epoch 2320, training loss: 83.57381439208984 = 0.16715310513973236 + 10.0 * 8.340665817260742
Epoch 2320, val loss: 0.5772033333778381
Epoch 2330, training loss: 83.58197021484375 = 0.1655603051185608 + 10.0 * 8.341641426086426
Epoch 2330, val loss: 0.5794097185134888
Epoch 2340, training loss: 83.5853042602539 = 0.16399334371089935 + 10.0 * 8.342130661010742
Epoch 2340, val loss: 0.5815996527671814
Epoch 2350, training loss: 83.56969451904297 = 0.1623936891555786 + 10.0 * 8.340730667114258
Epoch 2350, val loss: 0.5836784839630127
Epoch 2360, training loss: 83.5537338256836 = 0.16084827482700348 + 10.0 * 8.339288711547852
Epoch 2360, val loss: 0.5861735939979553
Epoch 2370, training loss: 83.57865142822266 = 0.15926949679851532 + 10.0 * 8.341938018798828
Epoch 2370, val loss: 0.5886132121086121
Epoch 2380, training loss: 83.57598876953125 = 0.1577172428369522 + 10.0 * 8.341827392578125
Epoch 2380, val loss: 0.5906596183776855
Epoch 2390, training loss: 83.55014038085938 = 0.1561584621667862 + 10.0 * 8.339398384094238
Epoch 2390, val loss: 0.5930914878845215
Epoch 2400, training loss: 83.54117584228516 = 0.15460754930973053 + 10.0 * 8.338656425476074
Epoch 2400, val loss: 0.5956308245658875
Epoch 2410, training loss: 83.5443344116211 = 0.15306568145751953 + 10.0 * 8.339126586914062
Epoch 2410, val loss: 0.5977855324745178
Epoch 2420, training loss: 83.6048355102539 = 0.151546910405159 + 10.0 * 8.345328330993652
Epoch 2420, val loss: 0.6001478433609009
Epoch 2430, training loss: 83.54081726074219 = 0.14999474585056305 + 10.0 * 8.339082717895508
Epoch 2430, val loss: 0.6021289229393005
Epoch 2440, training loss: 83.53333282470703 = 0.1484702229499817 + 10.0 * 8.338486671447754
Epoch 2440, val loss: 0.6046182513237
Epoch 2450, training loss: 83.55494689941406 = 0.146967813372612 + 10.0 * 8.340798377990723
Epoch 2450, val loss: 0.6072021722793579
Epoch 2460, training loss: 83.55062866210938 = 0.14545978605747223 + 10.0 * 8.340517044067383
Epoch 2460, val loss: 0.6096581816673279
Epoch 2470, training loss: 83.51988983154297 = 0.14395621418952942 + 10.0 * 8.337593078613281
Epoch 2470, val loss: 0.6118490695953369
Epoch 2480, training loss: 83.51583099365234 = 0.1424567997455597 + 10.0 * 8.337337493896484
Epoch 2480, val loss: 0.6142799854278564
Epoch 2490, training loss: 83.51908874511719 = 0.14094743132591248 + 10.0 * 8.337814331054688
Epoch 2490, val loss: 0.616707980632782
Epoch 2500, training loss: 83.54743194580078 = 0.13944604992866516 + 10.0 * 8.340799331665039
Epoch 2500, val loss: 0.6195335984230042
Epoch 2510, training loss: 83.53193664550781 = 0.1379261016845703 + 10.0 * 8.339401245117188
Epoch 2510, val loss: 0.6222262978553772
Epoch 2520, training loss: 83.51972961425781 = 0.1364324390888214 + 10.0 * 8.338330268859863
Epoch 2520, val loss: 0.624769389629364
Epoch 2530, training loss: 83.49858856201172 = 0.13493980467319489 + 10.0 * 8.33636474609375
Epoch 2530, val loss: 0.6277360320091248
Epoch 2540, training loss: 83.5103759765625 = 0.1334722936153412 + 10.0 * 8.337690353393555
Epoch 2540, val loss: 0.6307243704795837
Epoch 2550, training loss: 83.544189453125 = 0.13202549517154694 + 10.0 * 8.341216087341309
Epoch 2550, val loss: 0.6332458853721619
Epoch 2560, training loss: 83.52285766601562 = 0.1305595338344574 + 10.0 * 8.339229583740234
Epoch 2560, val loss: 0.6351355314254761
Epoch 2570, training loss: 83.49407196044922 = 0.12909720838069916 + 10.0 * 8.33649730682373
Epoch 2570, val loss: 0.6389636397361755
Epoch 2580, training loss: 83.48810577392578 = 0.12766030430793762 + 10.0 * 8.336044311523438
Epoch 2580, val loss: 0.6416501998901367
Epoch 2590, training loss: 83.5049819946289 = 0.12623703479766846 + 10.0 * 8.337874412536621
Epoch 2590, val loss: 0.6448294520378113
Epoch 2600, training loss: 83.51780700683594 = 0.1248224601149559 + 10.0 * 8.339298248291016
Epoch 2600, val loss: 0.6473530530929565
Epoch 2610, training loss: 83.49002838134766 = 0.12342062592506409 + 10.0 * 8.336660385131836
Epoch 2610, val loss: 0.6506414413452148
Epoch 2620, training loss: 83.4947738647461 = 0.12204530090093613 + 10.0 * 8.337272644042969
Epoch 2620, val loss: 0.6536924242973328
Epoch 2630, training loss: 83.4881820678711 = 0.12065049260854721 + 10.0 * 8.336752891540527
Epoch 2630, val loss: 0.6561449766159058
Epoch 2640, training loss: 83.47571563720703 = 0.11927679926156998 + 10.0 * 8.335643768310547
Epoch 2640, val loss: 0.6589497923851013
Epoch 2650, training loss: 83.46668243408203 = 0.11791709810495377 + 10.0 * 8.33487606048584
Epoch 2650, val loss: 0.6617921590805054
Epoch 2660, training loss: 83.46639251708984 = 0.11656951159238815 + 10.0 * 8.334981918334961
Epoch 2660, val loss: 0.6646959185600281
Epoch 2670, training loss: 83.51210021972656 = 0.11523519456386566 + 10.0 * 8.339686393737793
Epoch 2670, val loss: 0.6679296493530273
Epoch 2680, training loss: 83.46712493896484 = 0.11389151215553284 + 10.0 * 8.335323333740234
Epoch 2680, val loss: 0.6712968945503235
Epoch 2690, training loss: 83.45707702636719 = 0.11256304383277893 + 10.0 * 8.334451675415039
Epoch 2690, val loss: 0.673868715763092
Epoch 2700, training loss: 83.4574203491211 = 0.11125558614730835 + 10.0 * 8.334616661071777
Epoch 2700, val loss: 0.6767937541007996
Epoch 2710, training loss: 83.4644546508789 = 0.10994507372379303 + 10.0 * 8.335451126098633
Epoch 2710, val loss: 0.6800616979598999
Epoch 2720, training loss: 83.45655822753906 = 0.1086626946926117 + 10.0 * 8.334789276123047
Epoch 2720, val loss: 0.6834815740585327
Epoch 2730, training loss: 83.4654541015625 = 0.10737427324056625 + 10.0 * 8.335807800292969
Epoch 2730, val loss: 0.6866633296012878
Epoch 2740, training loss: 83.46538543701172 = 0.10607392340898514 + 10.0 * 8.335931777954102
Epoch 2740, val loss: 0.689374566078186
Epoch 2750, training loss: 83.46831512451172 = 0.10481064766645432 + 10.0 * 8.336350440979004
Epoch 2750, val loss: 0.6926887631416321
Epoch 2760, training loss: 83.4549789428711 = 0.10354169458150864 + 10.0 * 8.33514404296875
Epoch 2760, val loss: 0.6962420344352722
Epoch 2770, training loss: 83.4306411743164 = 0.10228794813156128 + 10.0 * 8.33283519744873
Epoch 2770, val loss: 0.6994093656539917
Epoch 2780, training loss: 83.43187713623047 = 0.10105302184820175 + 10.0 * 8.33308219909668
Epoch 2780, val loss: 0.702763020992279
Epoch 2790, training loss: 83.46580505371094 = 0.09983040392398834 + 10.0 * 8.336597442626953
Epoch 2790, val loss: 0.7060996890068054
Epoch 2800, training loss: 83.4262466430664 = 0.09859966486692429 + 10.0 * 8.332764625549316
Epoch 2800, val loss: 0.7091183662414551
Epoch 2810, training loss: 83.46631622314453 = 0.09740382432937622 + 10.0 * 8.336891174316406
Epoch 2810, val loss: 0.7126652598381042
Epoch 2820, training loss: 83.42550659179688 = 0.09618814289569855 + 10.0 * 8.332931518554688
Epoch 2820, val loss: 0.7157493829727173
Epoch 2830, training loss: 83.42381286621094 = 0.09498932212591171 + 10.0 * 8.33288288116455
Epoch 2830, val loss: 0.7190697193145752
Epoch 2840, training loss: 83.41311645507812 = 0.09380912035703659 + 10.0 * 8.331930160522461
Epoch 2840, val loss: 0.7227010726928711
Epoch 2850, training loss: 83.41301727294922 = 0.0926382914185524 + 10.0 * 8.332037925720215
Epoch 2850, val loss: 0.7261248826980591
Epoch 2860, training loss: 83.46333312988281 = 0.09148336946964264 + 10.0 * 8.33718490600586
Epoch 2860, val loss: 0.7298299074172974
Epoch 2870, training loss: 83.42780303955078 = 0.09031400084495544 + 10.0 * 8.333748817443848
Epoch 2870, val loss: 0.7322083115577698
Epoch 2880, training loss: 83.44258880615234 = 0.08918100595474243 + 10.0 * 8.33534049987793
Epoch 2880, val loss: 0.7362238764762878
Epoch 2890, training loss: 83.41459655761719 = 0.08802073448896408 + 10.0 * 8.332657814025879
Epoch 2890, val loss: 0.7394047975540161
Epoch 2900, training loss: 83.41326141357422 = 0.08690313249826431 + 10.0 * 8.332635879516602
Epoch 2900, val loss: 0.7436383962631226
Epoch 2910, training loss: 83.42782592773438 = 0.08578096330165863 + 10.0 * 8.33420467376709
Epoch 2910, val loss: 0.7461872696876526
Epoch 2920, training loss: 83.40967559814453 = 0.08467309176921844 + 10.0 * 8.332500457763672
Epoch 2920, val loss: 0.7499511241912842
Epoch 2930, training loss: 83.39973449707031 = 0.08357807248830795 + 10.0 * 8.331615447998047
Epoch 2930, val loss: 0.7533705830574036
Epoch 2940, training loss: 83.39051818847656 = 0.08249513059854507 + 10.0 * 8.330801963806152
Epoch 2940, val loss: 0.756558358669281
Epoch 2950, training loss: 83.39910888671875 = 0.08142608404159546 + 10.0 * 8.331768035888672
Epoch 2950, val loss: 0.7600376009941101
Epoch 2960, training loss: 83.4360122680664 = 0.08038028329610825 + 10.0 * 8.335562705993652
Epoch 2960, val loss: 0.7633084654808044
Epoch 2970, training loss: 83.38585662841797 = 0.07932311296463013 + 10.0 * 8.330653190612793
Epoch 2970, val loss: 0.7677794098854065
Epoch 2980, training loss: 83.37786865234375 = 0.07828062772750854 + 10.0 * 8.32995891571045
Epoch 2980, val loss: 0.7713244557380676
Epoch 2990, training loss: 83.38483428955078 = 0.07725269347429276 + 10.0 * 8.330758094787598
Epoch 2990, val loss: 0.77457594871521
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8224251648909182
0.8417735274940231
=== training gcn model ===
Epoch 0, training loss: 106.93280029296875 = 1.1103342771530151 + 10.0 * 10.582246780395508
Epoch 0, val loss: 1.1096115112304688
Epoch 10, training loss: 106.91278839111328 = 1.1022448539733887 + 10.0 * 10.5810546875
Epoch 10, val loss: 1.101495385169983
Epoch 20, training loss: 106.79190826416016 = 1.0927785634994507 + 10.0 * 10.569912910461426
Epoch 20, val loss: 1.0919554233551025
Epoch 30, training loss: 106.08831787109375 = 1.081389307975769 + 10.0 * 10.500692367553711
Epoch 30, val loss: 1.0803974866867065
Epoch 40, training loss: 103.6597671508789 = 1.0684927701950073 + 10.0 * 10.259127616882324
Epoch 40, val loss: 1.0675115585327148
Epoch 50, training loss: 100.63011932373047 = 1.0557810068130493 + 10.0 * 9.957433700561523
Epoch 50, val loss: 1.0548564195632935
Epoch 60, training loss: 96.79579162597656 = 1.046851634979248 + 10.0 * 9.574893951416016
Epoch 60, val loss: 1.0462943315505981
Epoch 70, training loss: 93.90325927734375 = 1.0390387773513794 + 10.0 * 9.286421775817871
Epoch 70, val loss: 1.0385736227035522
Epoch 80, training loss: 93.0809097290039 = 1.0316184759140015 + 10.0 * 9.20492935180664
Epoch 80, val loss: 1.0311896800994873
Epoch 90, training loss: 92.48531341552734 = 1.0239338874816895 + 10.0 * 9.146138191223145
Epoch 90, val loss: 1.0235768556594849
Epoch 100, training loss: 91.45497131347656 = 1.0172747373580933 + 10.0 * 9.043769836425781
Epoch 100, val loss: 1.0171080827713013
Epoch 110, training loss: 90.24858093261719 = 1.0119456052780151 + 10.0 * 8.923663139343262
Epoch 110, val loss: 1.0118058919906616
Epoch 120, training loss: 89.61978912353516 = 1.0060452222824097 + 10.0 * 8.861374855041504
Epoch 120, val loss: 1.0057636499404907
Epoch 130, training loss: 89.03136444091797 = 0.998683512210846 + 10.0 * 8.803268432617188
Epoch 130, val loss: 0.9982857704162598
Epoch 140, training loss: 88.60248565673828 = 0.9899460673332214 + 10.0 * 8.76125431060791
Epoch 140, val loss: 0.9895132780075073
Epoch 150, training loss: 88.20993041992188 = 0.9801495671272278 + 10.0 * 8.722978591918945
Epoch 150, val loss: 0.9797167778015137
Epoch 160, training loss: 87.84813690185547 = 0.9696346521377563 + 10.0 * 8.687849998474121
Epoch 160, val loss: 0.9693300127983093
Epoch 170, training loss: 87.55931091308594 = 0.9582494497299194 + 10.0 * 8.66010570526123
Epoch 170, val loss: 0.9580248594284058
Epoch 180, training loss: 87.34613800048828 = 0.9453466534614563 + 10.0 * 8.640079498291016
Epoch 180, val loss: 0.945219099521637
Epoch 190, training loss: 87.11882781982422 = 0.9308465719223022 + 10.0 * 8.61879825592041
Epoch 190, val loss: 0.9307846426963806
Epoch 200, training loss: 86.94013977050781 = 0.9149646162986755 + 10.0 * 8.602518081665039
Epoch 200, val loss: 0.9150885343551636
Epoch 210, training loss: 86.81903076171875 = 0.8978587985038757 + 10.0 * 8.592117309570312
Epoch 210, val loss: 0.8982331156730652
Epoch 220, training loss: 86.65032196044922 = 0.8794869184494019 + 10.0 * 8.577083587646484
Epoch 220, val loss: 0.8802911043167114
Epoch 230, training loss: 86.49994659423828 = 0.8601887226104736 + 10.0 * 8.56397533416748
Epoch 230, val loss: 0.8614206314086914
Epoch 240, training loss: 86.382080078125 = 0.8399624824523926 + 10.0 * 8.554211616516113
Epoch 240, val loss: 0.8416864275932312
Epoch 250, training loss: 86.32728576660156 = 0.8190116882324219 + 10.0 * 8.550827026367188
Epoch 250, val loss: 0.8212584853172302
Epoch 260, training loss: 86.18583679199219 = 0.797431468963623 + 10.0 * 8.538840293884277
Epoch 260, val loss: 0.8003448247909546
Epoch 270, training loss: 86.065673828125 = 0.7757125496864319 + 10.0 * 8.528996467590332
Epoch 270, val loss: 0.779341459274292
Epoch 280, training loss: 85.96049499511719 = 0.7539669275283813 + 10.0 * 8.520652770996094
Epoch 280, val loss: 0.758369505405426
Epoch 290, training loss: 85.90086364746094 = 0.7324846982955933 + 10.0 * 8.516838073730469
Epoch 290, val loss: 0.7376608848571777
Epoch 300, training loss: 85.76961517333984 = 0.7114962339401245 + 10.0 * 8.50581169128418
Epoch 300, val loss: 0.7174921631813049
Epoch 310, training loss: 85.71522521972656 = 0.6911500096321106 + 10.0 * 8.502408027648926
Epoch 310, val loss: 0.6980142593383789
Epoch 320, training loss: 85.65283966064453 = 0.6716107130050659 + 10.0 * 8.498123168945312
Epoch 320, val loss: 0.6792992949485779
Epoch 330, training loss: 85.52875518798828 = 0.6530559659004211 + 10.0 * 8.487569808959961
Epoch 330, val loss: 0.6616287231445312
Epoch 340, training loss: 85.44487762451172 = 0.6356463432312012 + 10.0 * 8.480923652648926
Epoch 340, val loss: 0.6450368165969849
Epoch 350, training loss: 85.37437438964844 = 0.6192840933799744 + 10.0 * 8.475508689880371
Epoch 350, val loss: 0.6295155882835388
Epoch 360, training loss: 85.3106460571289 = 0.6039801239967346 + 10.0 * 8.470666885375977
Epoch 360, val loss: 0.6150307059288025
Epoch 370, training loss: 85.25104522705078 = 0.5897770524024963 + 10.0 * 8.466127395629883
Epoch 370, val loss: 0.6016549468040466
Epoch 380, training loss: 85.20805358886719 = 0.5766569972038269 + 10.0 * 8.463139533996582
Epoch 380, val loss: 0.5893954634666443
Epoch 390, training loss: 85.22554779052734 = 0.5645614862442017 + 10.0 * 8.46609878540039
Epoch 390, val loss: 0.5781145691871643
Epoch 400, training loss: 85.13542938232422 = 0.5535223484039307 + 10.0 * 8.45819091796875
Epoch 400, val loss: 0.5678817629814148
Epoch 410, training loss: 85.06427001953125 = 0.5435174107551575 + 10.0 * 8.452075004577637
Epoch 410, val loss: 0.5587238669395447
Epoch 420, training loss: 85.01981353759766 = 0.5343829393386841 + 10.0 * 8.448542594909668
Epoch 420, val loss: 0.5504233837127686
Epoch 430, training loss: 84.989501953125 = 0.5260283946990967 + 10.0 * 8.4463472366333
Epoch 430, val loss: 0.5429132580757141
Epoch 440, training loss: 84.95787048339844 = 0.5183537602424622 + 10.0 * 8.443951606750488
Epoch 440, val loss: 0.5360950231552124
Epoch 450, training loss: 84.92259216308594 = 0.5113370418548584 + 10.0 * 8.441125869750977
Epoch 450, val loss: 0.5299503803253174
Epoch 460, training loss: 84.890625 = 0.5049374103546143 + 10.0 * 8.438569068908691
Epoch 460, val loss: 0.5244786143302917
Epoch 470, training loss: 84.8639907836914 = 0.4990493059158325 + 10.0 * 8.436494827270508
Epoch 470, val loss: 0.5195134282112122
Epoch 480, training loss: 84.92315673828125 = 0.49359843134880066 + 10.0 * 8.44295597076416
Epoch 480, val loss: 0.5150094628334045
Epoch 490, training loss: 84.82148742675781 = 0.488517701625824 + 10.0 * 8.433297157287598
Epoch 490, val loss: 0.5109502673149109
Epoch 500, training loss: 84.784912109375 = 0.4838351905345917 + 10.0 * 8.430108070373535
Epoch 500, val loss: 0.5072607398033142
Epoch 510, training loss: 84.76327514648438 = 0.4794777035713196 + 10.0 * 8.428380012512207
Epoch 510, val loss: 0.5039278864860535
Epoch 520, training loss: 84.77778625488281 = 0.475374311208725 + 10.0 * 8.430241584777832
Epoch 520, val loss: 0.5008411407470703
Epoch 530, training loss: 84.73184204101562 = 0.4714941680431366 + 10.0 * 8.426034927368164
Epoch 530, val loss: 0.4980955421924591
Epoch 540, training loss: 84.69676208496094 = 0.4678506553173065 + 10.0 * 8.422891616821289
Epoch 540, val loss: 0.4955179989337921
Epoch 550, training loss: 84.66893768310547 = 0.46440744400024414 + 10.0 * 8.420453071594238
Epoch 550, val loss: 0.49320492148399353
Epoch 560, training loss: 84.6562271118164 = 0.4611230790615082 + 10.0 * 8.419510841369629
Epoch 560, val loss: 0.4910217225551605
Epoch 570, training loss: 84.69084167480469 = 0.4579484462738037 + 10.0 * 8.42328929901123
Epoch 570, val loss: 0.4889681339263916
Epoch 580, training loss: 84.65234375 = 0.45489707589149475 + 10.0 * 8.419744491577148
Epoch 580, val loss: 0.48704618215560913
Epoch 590, training loss: 84.6022720336914 = 0.45203351974487305 + 10.0 * 8.415023803710938
Epoch 590, val loss: 0.48533791303634644
Epoch 600, training loss: 84.5766372680664 = 0.4492891728878021 + 10.0 * 8.412734985351562
Epoch 600, val loss: 0.48370465636253357
Epoch 610, training loss: 84.56401062011719 = 0.4466395080089569 + 10.0 * 8.411737442016602
Epoch 610, val loss: 0.48213157057762146
Epoch 620, training loss: 84.61825561523438 = 0.44407474994659424 + 10.0 * 8.417417526245117
Epoch 620, val loss: 0.4806547164916992
Epoch 630, training loss: 84.54774475097656 = 0.4415742754936218 + 10.0 * 8.410616874694824
Epoch 630, val loss: 0.47927752137184143
Epoch 640, training loss: 84.52044677734375 = 0.4391745626926422 + 10.0 * 8.408127784729004
Epoch 640, val loss: 0.4779658615589142
Epoch 650, training loss: 84.50887298583984 = 0.43686336278915405 + 10.0 * 8.407200813293457
Epoch 650, val loss: 0.47667422890663147
Epoch 660, training loss: 84.52824401855469 = 0.43461906909942627 + 10.0 * 8.40936279296875
Epoch 660, val loss: 0.47549089789390564
Epoch 670, training loss: 84.50850677490234 = 0.4324295222759247 + 10.0 * 8.407608032226562
Epoch 670, val loss: 0.4743408262729645
Epoch 680, training loss: 84.47187805175781 = 0.4303121566772461 + 10.0 * 8.404156684875488
Epoch 680, val loss: 0.4732159674167633
Epoch 690, training loss: 84.46460723876953 = 0.4282764196395874 + 10.0 * 8.403633117675781
Epoch 690, val loss: 0.4721335768699646
Epoch 700, training loss: 84.46131134033203 = 0.4262907803058624 + 10.0 * 8.403501510620117
Epoch 700, val loss: 0.47113996744155884
Epoch 710, training loss: 84.44081115722656 = 0.42436057329177856 + 10.0 * 8.401644706726074
Epoch 710, val loss: 0.47022587060928345
Epoch 720, training loss: 84.43585968017578 = 0.42248547077178955 + 10.0 * 8.401337623596191
Epoch 720, val loss: 0.4692898392677307
Epoch 730, training loss: 84.43342590332031 = 0.4206611216068268 + 10.0 * 8.401276588439941
Epoch 730, val loss: 0.46850764751434326
Epoch 740, training loss: 84.41108703613281 = 0.418852835893631 + 10.0 * 8.399223327636719
Epoch 740, val loss: 0.467561811208725
Epoch 750, training loss: 84.38606262207031 = 0.41711679100990295 + 10.0 * 8.396894454956055
Epoch 750, val loss: 0.4667087495326996
Epoch 760, training loss: 84.37140655517578 = 0.4154418408870697 + 10.0 * 8.395596504211426
Epoch 760, val loss: 0.46595999598503113
Epoch 770, training loss: 84.36073303222656 = 0.41380730271339417 + 10.0 * 8.394692420959473
Epoch 770, val loss: 0.465230256319046
Epoch 780, training loss: 84.42877197265625 = 0.41219621896743774 + 10.0 * 8.401658058166504
Epoch 780, val loss: 0.46455568075180054
Epoch 790, training loss: 84.36859130859375 = 0.41058260202407837 + 10.0 * 8.395800590515137
Epoch 790, val loss: 0.4638451337814331
Epoch 800, training loss: 84.35828399658203 = 0.4090229272842407 + 10.0 * 8.394926071166992
Epoch 800, val loss: 0.46313661336898804
Epoch 810, training loss: 84.31624603271484 = 0.4075124263763428 + 10.0 * 8.390873908996582
Epoch 810, val loss: 0.46240419149398804
Epoch 820, training loss: 84.31867980957031 = 0.4060497581958771 + 10.0 * 8.391263008117676
Epoch 820, val loss: 0.4617811143398285
Epoch 830, training loss: 84.3355712890625 = 0.40458980202674866 + 10.0 * 8.393098831176758
Epoch 830, val loss: 0.4611857235431671
Epoch 840, training loss: 84.30133819580078 = 0.4031601846218109 + 10.0 * 8.38981819152832
Epoch 840, val loss: 0.46065565943717957
Epoch 850, training loss: 84.27619171142578 = 0.4017728269100189 + 10.0 * 8.387441635131836
Epoch 850, val loss: 0.46001172065734863
Epoch 860, training loss: 84.28604125976562 = 0.4004068970680237 + 10.0 * 8.38856315612793
Epoch 860, val loss: 0.4594944715499878
Epoch 870, training loss: 84.28233337402344 = 0.3990439176559448 + 10.0 * 8.38832950592041
Epoch 870, val loss: 0.45896151661872864
Epoch 880, training loss: 84.25497436523438 = 0.39770472049713135 + 10.0 * 8.385726928710938
Epoch 880, val loss: 0.45836254954338074
Epoch 890, training loss: 84.25904083251953 = 0.39639750123023987 + 10.0 * 8.386263847351074
Epoch 890, val loss: 0.45792123675346375
Epoch 900, training loss: 84.2378158569336 = 0.39508238434791565 + 10.0 * 8.384273529052734
Epoch 900, val loss: 0.45735833048820496
Epoch 910, training loss: 84.22225189208984 = 0.3937939703464508 + 10.0 * 8.382845878601074
Epoch 910, val loss: 0.4567946791648865
Epoch 920, training loss: 84.20399475097656 = 0.39253392815589905 + 10.0 * 8.381146430969238
Epoch 920, val loss: 0.45631828904151917
Epoch 930, training loss: 84.23027801513672 = 0.3912832736968994 + 10.0 * 8.383899688720703
Epoch 930, val loss: 0.45574456453323364
Epoch 940, training loss: 84.22992706298828 = 0.390007883310318 + 10.0 * 8.383992195129395
Epoch 940, val loss: 0.455364465713501
Epoch 950, training loss: 84.19007110595703 = 0.3887646794319153 + 10.0 * 8.380130767822266
Epoch 950, val loss: 0.4548385441303253
Epoch 960, training loss: 84.19358825683594 = 0.38755670189857483 + 10.0 * 8.380602836608887
Epoch 960, val loss: 0.4543721377849579
Epoch 970, training loss: 84.16321563720703 = 0.38634946942329407 + 10.0 * 8.377686500549316
Epoch 970, val loss: 0.4539373219013214
Epoch 980, training loss: 84.14898681640625 = 0.38516882061958313 + 10.0 * 8.376381874084473
Epoch 980, val loss: 0.45350077748298645
Epoch 990, training loss: 84.13885498046875 = 0.3840046226978302 + 10.0 * 8.37548542022705
Epoch 990, val loss: 0.4530331790447235
Epoch 1000, training loss: 84.15679931640625 = 0.3828548491001129 + 10.0 * 8.37739372253418
Epoch 1000, val loss: 0.45272520184516907
Epoch 1010, training loss: 84.12865447998047 = 0.3816601634025574 + 10.0 * 8.374699592590332
Epoch 1010, val loss: 0.4521164894104004
Epoch 1020, training loss: 84.14974212646484 = 0.38048627972602844 + 10.0 * 8.376925468444824
Epoch 1020, val loss: 0.4517662823200226
Epoch 1030, training loss: 84.10676574707031 = 0.37934377789497375 + 10.0 * 8.372742652893066
Epoch 1030, val loss: 0.45130762457847595
Epoch 1040, training loss: 84.0960922241211 = 0.3782179057598114 + 10.0 * 8.371787071228027
Epoch 1040, val loss: 0.45088353753089905
Epoch 1050, training loss: 84.08819580078125 = 0.37709829211235046 + 10.0 * 8.371109962463379
Epoch 1050, val loss: 0.45052966475486755
Epoch 1060, training loss: 84.12095642089844 = 0.3759803771972656 + 10.0 * 8.374497413635254
Epoch 1060, val loss: 0.4501385986804962
Epoch 1070, training loss: 84.12284088134766 = 0.37483200430870056 + 10.0 * 8.374800682067871
Epoch 1070, val loss: 0.4496697783470154
Epoch 1080, training loss: 84.06644439697266 = 0.3737017810344696 + 10.0 * 8.369274139404297
Epoch 1080, val loss: 0.4492305815219879
Epoch 1090, training loss: 84.06238555908203 = 0.372588187456131 + 10.0 * 8.368979454040527
Epoch 1090, val loss: 0.44891783595085144
Epoch 1100, training loss: 84.0814208984375 = 0.3714854419231415 + 10.0 * 8.370993614196777
Epoch 1100, val loss: 0.44855642318725586
Epoch 1110, training loss: 84.07032012939453 = 0.37036484479904175 + 10.0 * 8.369996070861816
Epoch 1110, val loss: 0.4481102526187897
Epoch 1120, training loss: 84.04167175292969 = 0.36925145983695984 + 10.0 * 8.367241859436035
Epoch 1120, val loss: 0.44776707887649536
Epoch 1130, training loss: 84.03337860107422 = 0.3681522309780121 + 10.0 * 8.366522789001465
Epoch 1130, val loss: 0.44743287563323975
Epoch 1140, training loss: 84.07485961914062 = 0.3670588433742523 + 10.0 * 8.370779991149902
Epoch 1140, val loss: 0.4471506178379059
Epoch 1150, training loss: 84.0188217163086 = 0.36593037843704224 + 10.0 * 8.365289688110352
Epoch 1150, val loss: 0.4466401934623718
Epoch 1160, training loss: 84.00801849365234 = 0.3648264408111572 + 10.0 * 8.36431884765625
Epoch 1160, val loss: 0.4463886022567749
Epoch 1170, training loss: 84.00338745117188 = 0.3637242317199707 + 10.0 * 8.36396598815918
Epoch 1170, val loss: 0.4459613859653473
Epoch 1180, training loss: 84.06500244140625 = 0.36262255907058716 + 10.0 * 8.370237350463867
Epoch 1180, val loss: 0.44562533497810364
Epoch 1190, training loss: 84.00393676757812 = 0.3614959418773651 + 10.0 * 8.36424446105957
Epoch 1190, val loss: 0.4453735947608948
Epoch 1200, training loss: 83.98347473144531 = 0.3603884279727936 + 10.0 * 8.362308502197266
Epoch 1200, val loss: 0.44501641392707825
Epoch 1210, training loss: 83.98432159423828 = 0.3592887818813324 + 10.0 * 8.362503051757812
Epoch 1210, val loss: 0.44479042291641235
Epoch 1220, training loss: 84.01371765136719 = 0.3581770956516266 + 10.0 * 8.365553855895996
Epoch 1220, val loss: 0.4445115923881531
Epoch 1230, training loss: 83.96791076660156 = 0.3570610284805298 + 10.0 * 8.361084938049316
Epoch 1230, val loss: 0.44411614537239075
Epoch 1240, training loss: 83.9489974975586 = 0.3559498190879822 + 10.0 * 8.359304428100586
Epoch 1240, val loss: 0.44381842017173767
Epoch 1250, training loss: 83.96588134765625 = 0.35485053062438965 + 10.0 * 8.361103057861328
Epoch 1250, val loss: 0.4436061978340149
Epoch 1260, training loss: 83.97560119628906 = 0.3537323772907257 + 10.0 * 8.362187385559082
Epoch 1260, val loss: 0.4434036612510681
Epoch 1270, training loss: 83.94585418701172 = 0.3526054620742798 + 10.0 * 8.35932445526123
Epoch 1270, val loss: 0.44304975867271423
Epoch 1280, training loss: 83.93378448486328 = 0.3514934182167053 + 10.0 * 8.35822868347168
Epoch 1280, val loss: 0.4428237974643707
Epoch 1290, training loss: 83.93665313720703 = 0.3503866195678711 + 10.0 * 8.358626365661621
Epoch 1290, val loss: 0.44257262349128723
Epoch 1300, training loss: 83.97413635253906 = 0.3492717146873474 + 10.0 * 8.362485885620117
Epoch 1300, val loss: 0.44239625334739685
Epoch 1310, training loss: 83.9233169555664 = 0.3481478691101074 + 10.0 * 8.35751724243164
Epoch 1310, val loss: 0.44193726778030396
Epoch 1320, training loss: 83.89872741699219 = 0.3470233380794525 + 10.0 * 8.355170249938965
Epoch 1320, val loss: 0.4417521357536316
Epoch 1330, training loss: 83.90108489990234 = 0.3459101915359497 + 10.0 * 8.355517387390137
Epoch 1330, val loss: 0.4415525197982788
Epoch 1340, training loss: 83.9346923828125 = 0.34478893876075745 + 10.0 * 8.358990669250488
Epoch 1340, val loss: 0.44137099385261536
Epoch 1350, training loss: 83.90577697753906 = 0.34363099932670593 + 10.0 * 8.35621452331543
Epoch 1350, val loss: 0.4409133195877075
Epoch 1360, training loss: 83.88896942138672 = 0.3424834907054901 + 10.0 * 8.35464859008789
Epoch 1360, val loss: 0.4407666027545929
Epoch 1370, training loss: 83.8802261352539 = 0.34134289622306824 + 10.0 * 8.353888511657715
Epoch 1370, val loss: 0.4405350387096405
Epoch 1380, training loss: 83.8790512084961 = 0.3402039110660553 + 10.0 * 8.35388469696045
Epoch 1380, val loss: 0.4403243660926819
Epoch 1390, training loss: 83.90511322021484 = 0.3390503525733948 + 10.0 * 8.356606483459473
Epoch 1390, val loss: 0.4400077164173126
Epoch 1400, training loss: 83.86479949951172 = 0.33788275718688965 + 10.0 * 8.352691650390625
Epoch 1400, val loss: 0.4398777484893799
Epoch 1410, training loss: 83.8548583984375 = 0.33671948313713074 + 10.0 * 8.351814270019531
Epoch 1410, val loss: 0.4395686686038971
Epoch 1420, training loss: 83.88471221923828 = 0.3355618715286255 + 10.0 * 8.354914665222168
Epoch 1420, val loss: 0.4392853081226349
Epoch 1430, training loss: 83.84917449951172 = 0.3343570828437805 + 10.0 * 8.351481437683105
Epoch 1430, val loss: 0.4391784369945526
Epoch 1440, training loss: 83.84205627441406 = 0.33317509293556213 + 10.0 * 8.3508882522583
Epoch 1440, val loss: 0.4389343857765198
Epoch 1450, training loss: 83.8324966430664 = 0.33199000358581543 + 10.0 * 8.35004997253418
Epoch 1450, val loss: 0.43871697783470154
Epoch 1460, training loss: 83.82725524902344 = 0.33080869913101196 + 10.0 * 8.349644660949707
Epoch 1460, val loss: 0.43859121203422546
Epoch 1470, training loss: 83.82624053955078 = 0.3296196162700653 + 10.0 * 8.349661827087402
Epoch 1470, val loss: 0.4383485019207001
Epoch 1480, training loss: 83.88971710205078 = 0.328419029712677 + 10.0 * 8.35612964630127
Epoch 1480, val loss: 0.4381144940853119
Epoch 1490, training loss: 83.8384017944336 = 0.3271960914134979 + 10.0 * 8.351120948791504
Epoch 1490, val loss: 0.43787601590156555
Epoch 1500, training loss: 83.84265899658203 = 0.32597479224205017 + 10.0 * 8.351668357849121
Epoch 1500, val loss: 0.43788281083106995
Epoch 1510, training loss: 83.83335876464844 = 0.32474565505981445 + 10.0 * 8.350861549377441
Epoch 1510, val loss: 0.4375968277454376
Epoch 1520, training loss: 83.8111801147461 = 0.3235182762145996 + 10.0 * 8.348766326904297
Epoch 1520, val loss: 0.4374450743198395
Epoch 1530, training loss: 83.79460906982422 = 0.3222878575325012 + 10.0 * 8.3472318649292
Epoch 1530, val loss: 0.43726664781570435
Epoch 1540, training loss: 83.79533386230469 = 0.3210497498512268 + 10.0 * 8.347428321838379
Epoch 1540, val loss: 0.43707966804504395
Epoch 1550, training loss: 83.8654556274414 = 0.3198138177394867 + 10.0 * 8.35456371307373
Epoch 1550, val loss: 0.43687382340431213
Epoch 1560, training loss: 83.79611206054688 = 0.31852179765701294 + 10.0 * 8.347759246826172
Epoch 1560, val loss: 0.43692752718925476
Epoch 1570, training loss: 83.78026580810547 = 0.31724852323532104 + 10.0 * 8.346302032470703
Epoch 1570, val loss: 0.4366726279258728
Epoch 1580, training loss: 83.76870727539062 = 0.31597816944122314 + 10.0 * 8.3452730178833
Epoch 1580, val loss: 0.4366127848625183
Epoch 1590, training loss: 83.80511474609375 = 0.31470346450805664 + 10.0 * 8.349040985107422
Epoch 1590, val loss: 0.43664976954460144
Epoch 1600, training loss: 83.77635955810547 = 0.3133893609046936 + 10.0 * 8.346296310424805
Epoch 1600, val loss: 0.4362506866455078
Epoch 1610, training loss: 83.76775360107422 = 0.3120808005332947 + 10.0 * 8.34556770324707
Epoch 1610, val loss: 0.43631812930107117
Epoch 1620, training loss: 83.76231384277344 = 0.31076833605766296 + 10.0 * 8.345154762268066
Epoch 1620, val loss: 0.43603965640068054
Epoch 1630, training loss: 83.775634765625 = 0.30944690108299255 + 10.0 * 8.34661865234375
Epoch 1630, val loss: 0.43604305386543274
Epoch 1640, training loss: 83.75267028808594 = 0.3081050515174866 + 10.0 * 8.344456672668457
Epoch 1640, val loss: 0.4359667897224426
Epoch 1650, training loss: 83.74983978271484 = 0.30676525831222534 + 10.0 * 8.344307899475098
Epoch 1650, val loss: 0.4359032213687897
Epoch 1660, training loss: 83.76652526855469 = 0.3054199516773224 + 10.0 * 8.346110343933105
Epoch 1660, val loss: 0.435800701379776
Epoch 1670, training loss: 83.74610900878906 = 0.30403614044189453 + 10.0 * 8.344206809997559
Epoch 1670, val loss: 0.43573668599128723
Epoch 1680, training loss: 83.72794342041016 = 0.3026532530784607 + 10.0 * 8.342529296875
Epoch 1680, val loss: 0.4357832074165344
Epoch 1690, training loss: 83.72380065917969 = 0.3012731373310089 + 10.0 * 8.342252731323242
Epoch 1690, val loss: 0.4357359707355499
Epoch 1700, training loss: 83.71857452392578 = 0.29989108443260193 + 10.0 * 8.34186840057373
Epoch 1700, val loss: 0.43577757477760315
Epoch 1710, training loss: 83.73175811767578 = 0.2985028624534607 + 10.0 * 8.3433256149292
Epoch 1710, val loss: 0.4358041286468506
Epoch 1720, training loss: 83.75145721435547 = 0.29709282517433167 + 10.0 * 8.345436096191406
Epoch 1720, val loss: 0.4359038174152374
Epoch 1730, training loss: 83.7224349975586 = 0.29567986726760864 + 10.0 * 8.342676162719727
Epoch 1730, val loss: 0.435726135969162
Epoch 1740, training loss: 83.7076416015625 = 0.2942590117454529 + 10.0 * 8.341338157653809
Epoch 1740, val loss: 0.4358729422092438
Epoch 1750, training loss: 83.725341796875 = 0.29284045100212097 + 10.0 * 8.343250274658203
Epoch 1750, val loss: 0.4359625279903412
Epoch 1760, training loss: 83.72803497314453 = 0.2914040982723236 + 10.0 * 8.343663215637207
Epoch 1760, val loss: 0.4359615445137024
Epoch 1770, training loss: 83.70486450195312 = 0.2899625897407532 + 10.0 * 8.341489791870117
Epoch 1770, val loss: 0.43604180216789246
Epoch 1780, training loss: 83.68939208984375 = 0.28851866722106934 + 10.0 * 8.340086936950684
Epoch 1780, val loss: 0.4360712766647339
Epoch 1790, training loss: 83.68157958984375 = 0.287062406539917 + 10.0 * 8.339451789855957
Epoch 1790, val loss: 0.4362069070339203
Epoch 1800, training loss: 83.71417999267578 = 0.2856031656265259 + 10.0 * 8.342857360839844
Epoch 1800, val loss: 0.43612566590309143
Epoch 1810, training loss: 83.70025634765625 = 0.28411251306533813 + 10.0 * 8.341614723205566
Epoch 1810, val loss: 0.43637171387672424
Epoch 1820, training loss: 83.6784896850586 = 0.28261619806289673 + 10.0 * 8.339587211608887
Epoch 1820, val loss: 0.43641725182533264
Epoch 1830, training loss: 83.66870880126953 = 0.2811200022697449 + 10.0 * 8.33875846862793
Epoch 1830, val loss: 0.4368293881416321
Epoch 1840, training loss: 83.66249084472656 = 0.27961990237236023 + 10.0 * 8.338287353515625
Epoch 1840, val loss: 0.4368573725223541
Epoch 1850, training loss: 83.66731262207031 = 0.27811333537101746 + 10.0 * 8.338919639587402
Epoch 1850, val loss: 0.43706753849983215
Epoch 1860, training loss: 83.74760437011719 = 0.2766059637069702 + 10.0 * 8.347100257873535
Epoch 1860, val loss: 0.4370589554309845
Epoch 1870, training loss: 83.6714859008789 = 0.27504488825798035 + 10.0 * 8.339643478393555
Epoch 1870, val loss: 0.43750596046447754
Epoch 1880, training loss: 83.64566040039062 = 0.27350282669067383 + 10.0 * 8.337215423583984
Epoch 1880, val loss: 0.4377977252006531
Epoch 1890, training loss: 83.64085388183594 = 0.27195772528648376 + 10.0 * 8.336889266967773
Epoch 1890, val loss: 0.43787091970443726
Epoch 1900, training loss: 83.658935546875 = 0.2704065442085266 + 10.0 * 8.338852882385254
Epoch 1900, val loss: 0.4382752776145935
Epoch 1910, training loss: 83.68086242675781 = 0.2688363492488861 + 10.0 * 8.341202735900879
Epoch 1910, val loss: 0.43843874335289
Epoch 1920, training loss: 83.65337371826172 = 0.26726463437080383 + 10.0 * 8.338610649108887
Epoch 1920, val loss: 0.43859708309173584
Epoch 1930, training loss: 83.63076782226562 = 0.2656875550746918 + 10.0 * 8.336507797241211
Epoch 1930, val loss: 0.4390820562839508
Epoch 1940, training loss: 83.62366485595703 = 0.2641093134880066 + 10.0 * 8.335955619812012
Epoch 1940, val loss: 0.43923595547676086
Epoch 1950, training loss: 83.62362670898438 = 0.2625315487384796 + 10.0 * 8.336109161376953
Epoch 1950, val loss: 0.4396817684173584
Epoch 1960, training loss: 83.7038803100586 = 0.26095137000083923 + 10.0 * 8.344293594360352
Epoch 1960, val loss: 0.43990617990493774
Epoch 1970, training loss: 83.63939666748047 = 0.2593374252319336 + 10.0 * 8.338006019592285
Epoch 1970, val loss: 0.4403637647628784
Epoch 1980, training loss: 83.62049102783203 = 0.2577272057533264 + 10.0 * 8.336276054382324
Epoch 1980, val loss: 0.44064822793006897
Epoch 1990, training loss: 83.66295623779297 = 0.2561263144016266 + 10.0 * 8.340682983398438
Epoch 1990, val loss: 0.44090911746025085
Epoch 2000, training loss: 83.6053237915039 = 0.2544916272163391 + 10.0 * 8.3350830078125
Epoch 2000, val loss: 0.44171515107154846
Epoch 2010, training loss: 83.59748840332031 = 0.25286784768104553 + 10.0 * 8.33446216583252
Epoch 2010, val loss: 0.44202637672424316
Epoch 2020, training loss: 83.5957260131836 = 0.25124287605285645 + 10.0 * 8.334447860717773
Epoch 2020, val loss: 0.44257110357284546
Epoch 2030, training loss: 83.66783905029297 = 0.24962355196475983 + 10.0 * 8.341821670532227
Epoch 2030, val loss: 0.4432903528213501
Epoch 2040, training loss: 83.59967803955078 = 0.2479773908853531 + 10.0 * 8.335169792175293
Epoch 2040, val loss: 0.44335687160491943
Epoch 2050, training loss: 83.58844757080078 = 0.24632367491722107 + 10.0 * 8.334212303161621
Epoch 2050, val loss: 0.4440588355064392
Epoch 2060, training loss: 83.58041381835938 = 0.24467583000659943 + 10.0 * 8.333574295043945
Epoch 2060, val loss: 0.4444543719291687
Epoch 2070, training loss: 83.5994873046875 = 0.24301797151565552 + 10.0 * 8.335646629333496
Epoch 2070, val loss: 0.44509270787239075
Epoch 2080, training loss: 83.60167694091797 = 0.24134987592697144 + 10.0 * 8.33603286743164
Epoch 2080, val loss: 0.4455416202545166
Epoch 2090, training loss: 83.59104919433594 = 0.2396811693906784 + 10.0 * 8.335137367248535
Epoch 2090, val loss: 0.44628122448921204
Epoch 2100, training loss: 83.57350158691406 = 0.23800428211688995 + 10.0 * 8.333549499511719
Epoch 2100, val loss: 0.44683438539505005
Epoch 2110, training loss: 83.57526397705078 = 0.23633185029029846 + 10.0 * 8.333893775939941
Epoch 2110, val loss: 0.4474036991596222
Epoch 2120, training loss: 83.59722137451172 = 0.23465725779533386 + 10.0 * 8.33625602722168
Epoch 2120, val loss: 0.44807177782058716
Epoch 2130, training loss: 83.55914306640625 = 0.23296327888965607 + 10.0 * 8.33261775970459
Epoch 2130, val loss: 0.44886094331741333
Epoch 2140, training loss: 83.56322479248047 = 0.23127254843711853 + 10.0 * 8.333195686340332
Epoch 2140, val loss: 0.44973424077033997
Epoch 2150, training loss: 83.5663070678711 = 0.22957757115364075 + 10.0 * 8.333673477172852
Epoch 2150, val loss: 0.45034515857696533
Epoch 2160, training loss: 83.56428527832031 = 0.22787778079509735 + 10.0 * 8.333641052246094
Epoch 2160, val loss: 0.4508676826953888
Epoch 2170, training loss: 83.5778579711914 = 0.2261814922094345 + 10.0 * 8.33516788482666
Epoch 2170, val loss: 0.4519466757774353
Epoch 2180, training loss: 83.57040405273438 = 0.224491149187088 + 10.0 * 8.334590911865234
Epoch 2180, val loss: 0.4523610472679138
Epoch 2190, training loss: 83.56653594970703 = 0.22278302907943726 + 10.0 * 8.334375381469727
Epoch 2190, val loss: 0.4533842206001282
Epoch 2200, training loss: 83.5389633178711 = 0.22107738256454468 + 10.0 * 8.331789016723633
Epoch 2200, val loss: 0.4542520344257355
Epoch 2210, training loss: 83.53065490722656 = 0.21937544643878937 + 10.0 * 8.331128120422363
Epoch 2210, val loss: 0.4553876221179962
Epoch 2220, training loss: 83.53556060791016 = 0.2176733762025833 + 10.0 * 8.331789016723633
Epoch 2220, val loss: 0.45635002851486206
Epoch 2230, training loss: 83.55697631835938 = 0.21597188711166382 + 10.0 * 8.334100723266602
Epoch 2230, val loss: 0.4572950303554535
Epoch 2240, training loss: 83.55683135986328 = 0.21425861120224 + 10.0 * 8.334257125854492
Epoch 2240, val loss: 0.45795780420303345
Epoch 2250, training loss: 83.5338134765625 = 0.21255375444889069 + 10.0 * 8.332125663757324
Epoch 2250, val loss: 0.45895740389823914
Epoch 2260, training loss: 83.51719665527344 = 0.21084319055080414 + 10.0 * 8.330635070800781
Epoch 2260, val loss: 0.4602004289627075
Epoch 2270, training loss: 83.52732849121094 = 0.20913907885551453 + 10.0 * 8.331819534301758
Epoch 2270, val loss: 0.46094635128974915
Epoch 2280, training loss: 83.51812744140625 = 0.20743392407894135 + 10.0 * 8.331068992614746
Epoch 2280, val loss: 0.4620141088962555
Epoch 2290, training loss: 83.54122161865234 = 0.2057352364063263 + 10.0 * 8.333548545837402
Epoch 2290, val loss: 0.4628129005432129
Epoch 2300, training loss: 83.49646759033203 = 0.20400628447532654 + 10.0 * 8.329245567321777
Epoch 2300, val loss: 0.4641222059726715
Epoch 2310, training loss: 83.51091766357422 = 0.20230625569820404 + 10.0 * 8.33086109161377
Epoch 2310, val loss: 0.465516060590744
Epoch 2320, training loss: 83.52332305908203 = 0.2005840241909027 + 10.0 * 8.332273483276367
Epoch 2320, val loss: 0.4663841128349304
Epoch 2330, training loss: 83.48526000976562 = 0.19886764883995056 + 10.0 * 8.328639030456543
Epoch 2330, val loss: 0.467429518699646
Epoch 2340, training loss: 83.5018539428711 = 0.1971709132194519 + 10.0 * 8.33046817779541
Epoch 2340, val loss: 0.4684607982635498
Epoch 2350, training loss: 83.50575256347656 = 0.1954699009656906 + 10.0 * 8.33102798461914
Epoch 2350, val loss: 0.4697146415710449
Epoch 2360, training loss: 83.52671813964844 = 0.19378413259983063 + 10.0 * 8.333292961120605
Epoch 2360, val loss: 0.4711233973503113
Epoch 2370, training loss: 83.50511169433594 = 0.19208785891532898 + 10.0 * 8.331302642822266
Epoch 2370, val loss: 0.4720548689365387
Epoch 2380, training loss: 83.47533416748047 = 0.19038213789463043 + 10.0 * 8.328495025634766
Epoch 2380, val loss: 0.47376957535743713
Epoch 2390, training loss: 83.47753143310547 = 0.18869449198246002 + 10.0 * 8.328883171081543
Epoch 2390, val loss: 0.4750053882598877
Epoch 2400, training loss: 83.57513427734375 = 0.18703873455524445 + 10.0 * 8.3388090133667
Epoch 2400, val loss: 0.47625240683555603
Epoch 2410, training loss: 83.49816131591797 = 0.18533053994178772 + 10.0 * 8.331282615661621
Epoch 2410, val loss: 0.477751225233078
Epoch 2420, training loss: 83.46910095214844 = 0.18364787101745605 + 10.0 * 8.328545570373535
Epoch 2420, val loss: 0.47938233613967896
Epoch 2430, training loss: 83.45599365234375 = 0.1819731742143631 + 10.0 * 8.327402114868164
Epoch 2430, val loss: 0.4809817373752594
Epoch 2440, training loss: 83.46102142333984 = 0.18030564486980438 + 10.0 * 8.328071594238281
Epoch 2440, val loss: 0.48253026604652405
Epoch 2450, training loss: 83.50061798095703 = 0.17866411805152893 + 10.0 * 8.332195281982422
Epoch 2450, val loss: 0.4843530058860779
Epoch 2460, training loss: 83.47209930419922 = 0.17697705328464508 + 10.0 * 8.329511642456055
Epoch 2460, val loss: 0.48538386821746826
Epoch 2470, training loss: 83.44026947021484 = 0.1753125786781311 + 10.0 * 8.326495170593262
Epoch 2470, val loss: 0.4871070981025696
Epoch 2480, training loss: 83.43724822998047 = 0.17365331947803497 + 10.0 * 8.326359748840332
Epoch 2480, val loss: 0.4885507822036743
Epoch 2490, training loss: 83.44752502441406 = 0.17201001942157745 + 10.0 * 8.327550888061523
Epoch 2490, val loss: 0.4899404048919678
Epoch 2500, training loss: 83.51750183105469 = 0.1703813076019287 + 10.0 * 8.334712028503418
Epoch 2500, val loss: 0.4913688898086548
Epoch 2510, training loss: 83.44435119628906 = 0.168707937002182 + 10.0 * 8.327564239501953
Epoch 2510, val loss: 0.4936901032924652
Epoch 2520, training loss: 83.4238510131836 = 0.16704866290092468 + 10.0 * 8.32568073272705
Epoch 2520, val loss: 0.49505841732025146
Epoch 2530, training loss: 83.4439697265625 = 0.16541893780231476 + 10.0 * 8.327855110168457
Epoch 2530, val loss: 0.49670690298080444
Epoch 2540, training loss: 83.44970703125 = 0.16378559172153473 + 10.0 * 8.328592300415039
Epoch 2540, val loss: 0.4985562264919281
Epoch 2550, training loss: 83.42523193359375 = 0.1621561199426651 + 10.0 * 8.32630729675293
Epoch 2550, val loss: 0.5002594590187073
Epoch 2560, training loss: 83.41764831542969 = 0.16054320335388184 + 10.0 * 8.32571029663086
Epoch 2560, val loss: 0.5019168853759766
Epoch 2570, training loss: 83.41757202148438 = 0.1589401215314865 + 10.0 * 8.325862884521484
Epoch 2570, val loss: 0.5035713315010071
Epoch 2580, training loss: 83.47267150878906 = 0.15735062956809998 + 10.0 * 8.33153247833252
Epoch 2580, val loss: 0.5054340958595276
Epoch 2590, training loss: 83.43115997314453 = 0.155753955245018 + 10.0 * 8.327540397644043
Epoch 2590, val loss: 0.5071620941162109
Epoch 2600, training loss: 83.4118423461914 = 0.15416397154331207 + 10.0 * 8.325767517089844
Epoch 2600, val loss: 0.5095751881599426
Epoch 2610, training loss: 83.40538024902344 = 0.1525733768939972 + 10.0 * 8.325281143188477
Epoch 2610, val loss: 0.511178731918335
Epoch 2620, training loss: 83.41889953613281 = 0.15101291239261627 + 10.0 * 8.326787948608398
Epoch 2620, val loss: 0.5129647254943848
Epoch 2630, training loss: 83.4220962524414 = 0.1494833081960678 + 10.0 * 8.327260971069336
Epoch 2630, val loss: 0.5145359039306641
Epoch 2640, training loss: 83.4261474609375 = 0.14794595539569855 + 10.0 * 8.32781982421875
Epoch 2640, val loss: 0.5176427364349365
Epoch 2650, training loss: 83.44527435302734 = 0.1464078575372696 + 10.0 * 8.329886436462402
Epoch 2650, val loss: 0.5192511081695557
Epoch 2660, training loss: 83.41133880615234 = 0.14489053189754486 + 10.0 * 8.326644897460938
Epoch 2660, val loss: 0.5206984281539917
Epoch 2670, training loss: 83.38896179199219 = 0.1433788388967514 + 10.0 * 8.32455825805664
Epoch 2670, val loss: 0.5231859087944031
Epoch 2680, training loss: 83.37924194335938 = 0.14187073707580566 + 10.0 * 8.323737144470215
Epoch 2680, val loss: 0.5248134732246399
Epoch 2690, training loss: 83.38451385498047 = 0.14038245379924774 + 10.0 * 8.324413299560547
Epoch 2690, val loss: 0.5268684029579163
Epoch 2700, training loss: 83.42935180664062 = 0.13890518248081207 + 10.0 * 8.329044342041016
Epoch 2700, val loss: 0.5289209485054016
Epoch 2710, training loss: 83.38391876220703 = 0.1374317854642868 + 10.0 * 8.3246488571167
Epoch 2710, val loss: 0.5311417579650879
Epoch 2720, training loss: 83.36914825439453 = 0.1359608918428421 + 10.0 * 8.323318481445312
Epoch 2720, val loss: 0.5333696603775024
Epoch 2730, training loss: 83.37220764160156 = 0.13450957834720612 + 10.0 * 8.323770523071289
Epoch 2730, val loss: 0.5355510115623474
Epoch 2740, training loss: 83.45111083984375 = 0.13312500715255737 + 10.0 * 8.331798553466797
Epoch 2740, val loss: 0.5381597280502319
Epoch 2750, training loss: 83.37260437011719 = 0.1316470205783844 + 10.0 * 8.324095726013184
Epoch 2750, val loss: 0.5395497679710388
Epoch 2760, training loss: 83.35700225830078 = 0.13022100925445557 + 10.0 * 8.322678565979004
Epoch 2760, val loss: 0.542184054851532
Epoch 2770, training loss: 83.35603332519531 = 0.12881264090538025 + 10.0 * 8.322721481323242
Epoch 2770, val loss: 0.5442823767662048
Epoch 2780, training loss: 83.3566665649414 = 0.12741762399673462 + 10.0 * 8.322924613952637
Epoch 2780, val loss: 0.5464059114456177
Epoch 2790, training loss: 83.46589660644531 = 0.1260715126991272 + 10.0 * 8.333982467651367
Epoch 2790, val loss: 0.5483471155166626
Epoch 2800, training loss: 83.39208221435547 = 0.12468555569648743 + 10.0 * 8.326739311218262
Epoch 2800, val loss: 0.5513686537742615
Epoch 2810, training loss: 83.34915161132812 = 0.12330657988786697 + 10.0 * 8.32258415222168
Epoch 2810, val loss: 0.5532577633857727
Epoch 2820, training loss: 83.3420181274414 = 0.1219572052359581 + 10.0 * 8.322006225585938
Epoch 2820, val loss: 0.5555633306503296
Epoch 2830, training loss: 83.34489440917969 = 0.1206279844045639 + 10.0 * 8.322426795959473
Epoch 2830, val loss: 0.5577446818351746
Epoch 2840, training loss: 83.39973449707031 = 0.11932788044214249 + 10.0 * 8.32804012298584
Epoch 2840, val loss: 0.5599362850189209
Epoch 2850, training loss: 83.35918426513672 = 0.118037149310112 + 10.0 * 8.324114799499512
Epoch 2850, val loss: 0.5631587505340576
Epoch 2860, training loss: 83.34015655517578 = 0.1166926845908165 + 10.0 * 8.322346687316895
Epoch 2860, val loss: 0.5649778842926025
Epoch 2870, training loss: 83.35689544677734 = 0.11540767550468445 + 10.0 * 8.324148178100586
Epoch 2870, val loss: 0.5672741532325745
Epoch 2880, training loss: 83.34150695800781 = 0.11413558572530746 + 10.0 * 8.322736740112305
Epoch 2880, val loss: 0.569476306438446
Epoch 2890, training loss: 83.33980560302734 = 0.11285808682441711 + 10.0 * 8.322694778442383
Epoch 2890, val loss: 0.5719131231307983
Epoch 2900, training loss: 83.37034606933594 = 0.11163891851902008 + 10.0 * 8.325870513916016
Epoch 2900, val loss: 0.5738007426261902
Epoch 2910, training loss: 83.33351135253906 = 0.11035694181919098 + 10.0 * 8.322315216064453
Epoch 2910, val loss: 0.5771545767784119
Epoch 2920, training loss: 83.32044219970703 = 0.10912539809942245 + 10.0 * 8.321131706237793
Epoch 2920, val loss: 0.5797227621078491
Epoch 2930, training loss: 83.3180923461914 = 0.10790514200925827 + 10.0 * 8.321019172668457
Epoch 2930, val loss: 0.5816488265991211
Epoch 2940, training loss: 83.3187026977539 = 0.10669130831956863 + 10.0 * 8.32120132446289
Epoch 2940, val loss: 0.5845069885253906
Epoch 2950, training loss: 83.33882141113281 = 0.10549620538949966 + 10.0 * 8.323331832885742
Epoch 2950, val loss: 0.586648166179657
Epoch 2960, training loss: 83.33316040039062 = 0.10429603606462479 + 10.0 * 8.32288646697998
Epoch 2960, val loss: 0.5895119905471802
Epoch 2970, training loss: 83.32573699951172 = 0.10311642289161682 + 10.0 * 8.322261810302734
Epoch 2970, val loss: 0.5920040607452393
Epoch 2980, training loss: 83.33013153076172 = 0.10194314271211624 + 10.0 * 8.322818756103516
Epoch 2980, val loss: 0.5935865640640259
Epoch 2990, training loss: 83.32557678222656 = 0.10075018554925919 + 10.0 * 8.322482109069824
Epoch 2990, val loss: 0.5967105627059937
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8249619482496194
0.8421357675867566
The final CL Acc:0.82479, 0.00187, The final GNN Acc:0.84206, 0.00021
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110780])
remove edge: torch.Size([2, 66500])
updated graph: torch.Size([2, 88632])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 106.9333724975586 = 1.1106170415878296 + 10.0 * 10.582275390625
Epoch 0, val loss: 1.1099655628204346
Epoch 10, training loss: 106.91658782958984 = 1.1026405096054077 + 10.0 * 10.581395149230957
Epoch 10, val loss: 1.1019748449325562
Epoch 20, training loss: 106.82393646240234 = 1.0938491821289062 + 10.0 * 10.57300853729248
Epoch 20, val loss: 1.0931198596954346
Epoch 30, training loss: 106.19466400146484 = 1.083572506904602 + 10.0 * 10.511109352111816
Epoch 30, val loss: 1.082669734954834
Epoch 40, training loss: 103.6634292602539 = 1.0719386339187622 + 10.0 * 10.259149551391602
Epoch 40, val loss: 1.0710285902023315
Epoch 50, training loss: 98.56362915039062 = 1.0607342720031738 + 10.0 * 9.750288963317871
Epoch 50, val loss: 1.0599597692489624
Epoch 60, training loss: 95.95886993408203 = 1.0515835285186768 + 10.0 * 9.490728378295898
Epoch 60, val loss: 1.0508863925933838
Epoch 70, training loss: 94.28411865234375 = 1.0413100719451904 + 10.0 * 9.324280738830566
Epoch 70, val loss: 1.040671706199646
Epoch 80, training loss: 93.31816101074219 = 1.030452847480774 + 10.0 * 9.22877025604248
Epoch 80, val loss: 1.029881477355957
Epoch 90, training loss: 92.82624816894531 = 1.020457148551941 + 10.0 * 9.18057918548584
Epoch 90, val loss: 1.0199981927871704
Epoch 100, training loss: 92.33028411865234 = 1.0118809938430786 + 10.0 * 9.131840705871582
Epoch 100, val loss: 1.0114974975585938
Epoch 110, training loss: 91.45048522949219 = 1.0044431686401367 + 10.0 * 9.044604301452637
Epoch 110, val loss: 1.004170298576355
Epoch 120, training loss: 90.19849395751953 = 0.9978691339492798 + 10.0 * 8.920062065124512
Epoch 120, val loss: 0.9977209568023682
Epoch 130, training loss: 89.34915161132812 = 0.9915403127670288 + 10.0 * 8.835761070251465
Epoch 130, val loss: 0.9913245439529419
Epoch 140, training loss: 88.71348571777344 = 0.9837939739227295 + 10.0 * 8.772969245910645
Epoch 140, val loss: 0.9834941029548645
Epoch 150, training loss: 88.21208953857422 = 0.974626362323761 + 10.0 * 8.723746299743652
Epoch 150, val loss: 0.9743373394012451
Epoch 160, training loss: 87.87056732177734 = 0.9646768569946289 + 10.0 * 8.69058895111084
Epoch 160, val loss: 0.9644078016281128
Epoch 170, training loss: 87.61334991455078 = 0.9538100957870483 + 10.0 * 8.665953636169434
Epoch 170, val loss: 0.9536052942276001
Epoch 180, training loss: 87.42819213867188 = 0.9415287375450134 + 10.0 * 8.648666381835938
Epoch 180, val loss: 0.9414010047912598
Epoch 190, training loss: 87.24662017822266 = 0.9278007745742798 + 10.0 * 8.631881713867188
Epoch 190, val loss: 0.9277724623680115
Epoch 200, training loss: 87.07042694091797 = 0.9132277965545654 + 10.0 * 8.61571979522705
Epoch 200, val loss: 0.9134416580200195
Epoch 210, training loss: 87.00992584228516 = 0.8978649377822876 + 10.0 * 8.6112060546875
Epoch 210, val loss: 0.8983302712440491
Epoch 220, training loss: 86.77082061767578 = 0.8811284899711609 + 10.0 * 8.588969230651855
Epoch 220, val loss: 0.8817376494407654
Epoch 230, training loss: 86.6461181640625 = 0.863341748714447 + 10.0 * 8.578277587890625
Epoch 230, val loss: 0.8642998933792114
Epoch 240, training loss: 86.53255462646484 = 0.8447464108467102 + 10.0 * 8.568780899047852
Epoch 240, val loss: 0.8461737036705017
Epoch 250, training loss: 86.36933135986328 = 0.8255162239074707 + 10.0 * 8.554381370544434
Epoch 250, val loss: 0.8273882269859314
Epoch 260, training loss: 86.23072814941406 = 0.8059099912643433 + 10.0 * 8.542482376098633
Epoch 260, val loss: 0.8083140254020691
Epoch 270, training loss: 86.09642791748047 = 0.7857563495635986 + 10.0 * 8.53106689453125
Epoch 270, val loss: 0.7888117432594299
Epoch 280, training loss: 85.989013671875 = 0.7649675607681274 + 10.0 * 8.522404670715332
Epoch 280, val loss: 0.7686339020729065
Epoch 290, training loss: 85.8673095703125 = 0.7440264225006104 + 10.0 * 8.512328147888184
Epoch 290, val loss: 0.7485790848731995
Epoch 300, training loss: 85.74812316894531 = 0.7234392166137695 + 10.0 * 8.50246810913086
Epoch 300, val loss: 0.7287979125976562
Epoch 310, training loss: 85.64349365234375 = 0.7030500173568726 + 10.0 * 8.494044303894043
Epoch 310, val loss: 0.7093591094017029
Epoch 320, training loss: 85.63287353515625 = 0.6828440427780151 + 10.0 * 8.495002746582031
Epoch 320, val loss: 0.6900833249092102
Epoch 330, training loss: 85.48218536376953 = 0.6630939841270447 + 10.0 * 8.481908798217773
Epoch 330, val loss: 0.6715970635414124
Epoch 340, training loss: 85.38815307617188 = 0.6444118022918701 + 10.0 * 8.474374771118164
Epoch 340, val loss: 0.6539506912231445
Epoch 350, training loss: 85.3158187866211 = 0.6264424324035645 + 10.0 * 8.468937873840332
Epoch 350, val loss: 0.6371338963508606
Epoch 360, training loss: 85.27133178710938 = 0.6092360019683838 + 10.0 * 8.466209411621094
Epoch 360, val loss: 0.6211262941360474
Epoch 370, training loss: 85.17247009277344 = 0.592953085899353 + 10.0 * 8.457951545715332
Epoch 370, val loss: 0.6062401533126831
Epoch 380, training loss: 85.11687469482422 = 0.577752947807312 + 10.0 * 8.453912734985352
Epoch 380, val loss: 0.5923404693603516
Epoch 390, training loss: 85.12724304199219 = 0.563396692276001 + 10.0 * 8.456384658813477
Epoch 390, val loss: 0.5793904066085815
Epoch 400, training loss: 85.01074981689453 = 0.5499496459960938 + 10.0 * 8.446080207824707
Epoch 400, val loss: 0.5672279000282288
Epoch 410, training loss: 84.9566421508789 = 0.5374971628189087 + 10.0 * 8.441914558410645
Epoch 410, val loss: 0.5561071038246155
Epoch 420, training loss: 84.95068359375 = 0.525865912437439 + 10.0 * 8.442481994628906
Epoch 420, val loss: 0.5457601547241211
Epoch 430, training loss: 84.87776947021484 = 0.5148927569389343 + 10.0 * 8.436287879943848
Epoch 430, val loss: 0.5361819863319397
Epoch 440, training loss: 84.82896423339844 = 0.5047892928123474 + 10.0 * 8.432416915893555
Epoch 440, val loss: 0.52748703956604
Epoch 450, training loss: 84.78777313232422 = 0.4954785108566284 + 10.0 * 8.429229736328125
Epoch 450, val loss: 0.5195081830024719
Epoch 460, training loss: 84.7536849975586 = 0.4867529571056366 + 10.0 * 8.426692962646484
Epoch 460, val loss: 0.5120941996574402
Epoch 470, training loss: 84.76611328125 = 0.4785601794719696 + 10.0 * 8.428754806518555
Epoch 470, val loss: 0.5053393840789795
Epoch 480, training loss: 84.69636535644531 = 0.47088950872421265 + 10.0 * 8.422548294067383
Epoch 480, val loss: 0.4989686608314514
Epoch 490, training loss: 84.65148162841797 = 0.4637773633003235 + 10.0 * 8.418770790100098
Epoch 490, val loss: 0.4932517111301422
Epoch 500, training loss: 84.61611938476562 = 0.45710331201553345 + 10.0 * 8.415902137756348
Epoch 500, val loss: 0.48790431022644043
Epoch 510, training loss: 84.64292907714844 = 0.4508189260959625 + 10.0 * 8.419210433959961
Epoch 510, val loss: 0.4830234944820404
Epoch 520, training loss: 84.56368255615234 = 0.44483044743537903 + 10.0 * 8.411885261535645
Epoch 520, val loss: 0.4783581793308258
Epoch 530, training loss: 84.52593994140625 = 0.43929627537727356 + 10.0 * 8.40866470336914
Epoch 530, val loss: 0.47421133518218994
Epoch 540, training loss: 84.49723052978516 = 0.4340876042842865 + 10.0 * 8.4063138961792
Epoch 540, val loss: 0.4702686667442322
Epoch 550, training loss: 84.49272155761719 = 0.42915138602256775 + 10.0 * 8.406356811523438
Epoch 550, val loss: 0.466642290353775
Epoch 560, training loss: 84.4840316772461 = 0.42440128326416016 + 10.0 * 8.405962944030762
Epoch 560, val loss: 0.463399738073349
Epoch 570, training loss: 84.4261245727539 = 0.41992151737213135 + 10.0 * 8.400620460510254
Epoch 570, val loss: 0.46015220880508423
Epoch 580, training loss: 84.4043960571289 = 0.4157411456108093 + 10.0 * 8.398865699768066
Epoch 580, val loss: 0.45718809962272644
Epoch 590, training loss: 84.37116241455078 = 0.41175395250320435 + 10.0 * 8.395940780639648
Epoch 590, val loss: 0.45448043942451477
Epoch 600, training loss: 84.39494323730469 = 0.40793508291244507 + 10.0 * 8.398700714111328
Epoch 600, val loss: 0.45197275280952454
Epoch 610, training loss: 84.33566284179688 = 0.40425217151641846 + 10.0 * 8.39314079284668
Epoch 610, val loss: 0.44950878620147705
Epoch 620, training loss: 84.32491302490234 = 0.4007706940174103 + 10.0 * 8.392414093017578
Epoch 620, val loss: 0.44723767042160034
Epoch 630, training loss: 84.28887939453125 = 0.39742186665534973 + 10.0 * 8.389145851135254
Epoch 630, val loss: 0.4450683891773224
Epoch 640, training loss: 84.26721954345703 = 0.39423316717147827 + 10.0 * 8.387298583984375
Epoch 640, val loss: 0.4431031346321106
Epoch 650, training loss: 84.25984191894531 = 0.391174852848053 + 10.0 * 8.386866569519043
Epoch 650, val loss: 0.44112563133239746
Epoch 660, training loss: 84.25572204589844 = 0.38816195726394653 + 10.0 * 8.38675594329834
Epoch 660, val loss: 0.4394221007823944
Epoch 670, training loss: 84.20939636230469 = 0.38530397415161133 + 10.0 * 8.38240909576416
Epoch 670, val loss: 0.4376696050167084
Epoch 680, training loss: 84.19416046142578 = 0.3825996220111847 + 10.0 * 8.381155967712402
Epoch 680, val loss: 0.43594640493392944
Epoch 690, training loss: 84.17723083496094 = 0.3799760937690735 + 10.0 * 8.379725456237793
Epoch 690, val loss: 0.4344335198402405
Epoch 700, training loss: 84.18016815185547 = 0.37744900584220886 + 10.0 * 8.380271911621094
Epoch 700, val loss: 0.43300706148147583
Epoch 710, training loss: 84.1745834350586 = 0.3749496340751648 + 10.0 * 8.379963874816895
Epoch 710, val loss: 0.43144291639328003
Epoch 720, training loss: 84.1481704711914 = 0.3725266456604004 + 10.0 * 8.377564430236816
Epoch 720, val loss: 0.4300704598426819
Epoch 730, training loss: 84.13240814208984 = 0.37023693323135376 + 10.0 * 8.376216888427734
Epoch 730, val loss: 0.42879173159599304
Epoch 740, training loss: 84.10923767089844 = 0.3680288791656494 + 10.0 * 8.374120712280273
Epoch 740, val loss: 0.4275493323802948
Epoch 750, training loss: 84.10208129882812 = 0.36588436365127563 + 10.0 * 8.37362003326416
Epoch 750, val loss: 0.42633965611457825
Epoch 760, training loss: 84.11136627197266 = 0.3637797236442566 + 10.0 * 8.37475872039795
Epoch 760, val loss: 0.4251203238964081
Epoch 770, training loss: 84.12432098388672 = 0.3617243766784668 + 10.0 * 8.376259803771973
Epoch 770, val loss: 0.42393627762794495
Epoch 780, training loss: 84.07219696044922 = 0.3597167134284973 + 10.0 * 8.371248245239258
Epoch 780, val loss: 0.42294901609420776
Epoch 790, training loss: 84.04109954833984 = 0.3577941060066223 + 10.0 * 8.368330001831055
Epoch 790, val loss: 0.42187249660491943
Epoch 800, training loss: 84.03236389160156 = 0.355922669172287 + 10.0 * 8.367644309997559
Epoch 800, val loss: 0.42082491517066956
Epoch 810, training loss: 84.13768768310547 = 0.35408586263656616 + 10.0 * 8.3783597946167
Epoch 810, val loss: 0.4198705554008484
Epoch 820, training loss: 84.0418930053711 = 0.3522574007511139 + 10.0 * 8.368963241577148
Epoch 820, val loss: 0.41885948181152344
Epoch 830, training loss: 83.99273681640625 = 0.35053184628486633 + 10.0 * 8.36422061920166
Epoch 830, val loss: 0.4178857207298279
Epoch 840, training loss: 83.97818756103516 = 0.3488450050354004 + 10.0 * 8.362934112548828
Epoch 840, val loss: 0.4169924557209015
Epoch 850, training loss: 83.96794128417969 = 0.34719929099082947 + 10.0 * 8.36207389831543
Epoch 850, val loss: 0.41612496972084045
Epoch 860, training loss: 84.04181671142578 = 0.34557515382766724 + 10.0 * 8.369624137878418
Epoch 860, val loss: 0.4152342677116394
Epoch 870, training loss: 83.95128631591797 = 0.3439594507217407 + 10.0 * 8.360733032226562
Epoch 870, val loss: 0.4144844114780426
Epoch 880, training loss: 83.94581604003906 = 0.3424191474914551 + 10.0 * 8.360339164733887
Epoch 880, val loss: 0.41360414028167725
Epoch 890, training loss: 83.9300537109375 = 0.340911328792572 + 10.0 * 8.358914375305176
Epoch 890, val loss: 0.4128302335739136
Epoch 900, training loss: 83.97620391845703 = 0.33942559361457825 + 10.0 * 8.363677978515625
Epoch 900, val loss: 0.4120623469352722
Epoch 910, training loss: 83.93202209472656 = 0.3379374146461487 + 10.0 * 8.359408378601074
Epoch 910, val loss: 0.4113931357860565
Epoch 920, training loss: 83.90253448486328 = 0.3365125060081482 + 10.0 * 8.356602668762207
Epoch 920, val loss: 0.4106486737728119
Epoch 930, training loss: 83.8852767944336 = 0.3351195156574249 + 10.0 * 8.355015754699707
Epoch 930, val loss: 0.40995004773139954
Epoch 940, training loss: 83.87394714355469 = 0.33375927805900574 + 10.0 * 8.354019165039062
Epoch 940, val loss: 0.4093426465988159
Epoch 950, training loss: 83.86578369140625 = 0.33241918683052063 + 10.0 * 8.353336334228516
Epoch 950, val loss: 0.40868455171585083
Epoch 960, training loss: 83.88166809082031 = 0.33109724521636963 + 10.0 * 8.355056762695312
Epoch 960, val loss: 0.40814319252967834
Epoch 970, training loss: 83.92346954345703 = 0.3297536075115204 + 10.0 * 8.35937213897705
Epoch 970, val loss: 0.4075181484222412
Epoch 980, training loss: 83.87757110595703 = 0.32844558358192444 + 10.0 * 8.354912757873535
Epoch 980, val loss: 0.40702593326568604
Epoch 990, training loss: 83.83987426757812 = 0.3272080719470978 + 10.0 * 8.351266860961914
Epoch 990, val loss: 0.40636128187179565
Epoch 1000, training loss: 83.81993103027344 = 0.3259919285774231 + 10.0 * 8.349393844604492
Epoch 1000, val loss: 0.40588682889938354
Epoch 1010, training loss: 83.80793762207031 = 0.3248060345649719 + 10.0 * 8.348313331604004
Epoch 1010, val loss: 0.40534475445747375
Epoch 1020, training loss: 83.86801147460938 = 0.3236238360404968 + 10.0 * 8.354438781738281
Epoch 1020, val loss: 0.40484943985939026
Epoch 1030, training loss: 83.81587982177734 = 0.32241857051849365 + 10.0 * 8.349346160888672
Epoch 1030, val loss: 0.40445205569267273
Epoch 1040, training loss: 83.80107116699219 = 0.32126349210739136 + 10.0 * 8.347980499267578
Epoch 1040, val loss: 0.40390315651893616
Epoch 1050, training loss: 83.77995300292969 = 0.32013994455337524 + 10.0 * 8.34598159790039
Epoch 1050, val loss: 0.403510183095932
Epoch 1060, training loss: 83.76819610595703 = 0.3190314471721649 + 10.0 * 8.344916343688965
Epoch 1060, val loss: 0.40309369564056396
Epoch 1070, training loss: 83.76203918457031 = 0.31793564558029175 + 10.0 * 8.34441089630127
Epoch 1070, val loss: 0.4027029871940613
Epoch 1080, training loss: 83.9538803100586 = 0.31683361530303955 + 10.0 * 8.363704681396484
Epoch 1080, val loss: 0.4023605287075043
Epoch 1090, training loss: 83.75834655761719 = 0.31570494174957275 + 10.0 * 8.344264030456543
Epoch 1090, val loss: 0.40198832750320435
Epoch 1100, training loss: 83.73929595947266 = 0.31465044617652893 + 10.0 * 8.342464447021484
Epoch 1100, val loss: 0.40146544575691223
Epoch 1110, training loss: 83.73323822021484 = 0.31360793113708496 + 10.0 * 8.341962814331055
Epoch 1110, val loss: 0.4012393057346344
Epoch 1120, training loss: 83.7293930053711 = 0.3125796318054199 + 10.0 * 8.341681480407715
Epoch 1120, val loss: 0.4008476436138153
Epoch 1130, training loss: 83.7764892578125 = 0.3115536570549011 + 10.0 * 8.3464937210083
Epoch 1130, val loss: 0.4006178379058838
Epoch 1140, training loss: 83.71981048583984 = 0.3104933500289917 + 10.0 * 8.34093189239502
Epoch 1140, val loss: 0.4002448618412018
Epoch 1150, training loss: 83.7170639038086 = 0.30948877334594727 + 10.0 * 8.340757369995117
Epoch 1150, val loss: 0.399937242269516
Epoch 1160, training loss: 83.70662689208984 = 0.3084838092327118 + 10.0 * 8.339814186096191
Epoch 1160, val loss: 0.39962783455848694
Epoch 1170, training loss: 83.69747924804688 = 0.3075011670589447 + 10.0 * 8.338997840881348
Epoch 1170, val loss: 0.3993387818336487
Epoch 1180, training loss: 83.71041870117188 = 0.30651798844337463 + 10.0 * 8.3403902053833
Epoch 1180, val loss: 0.3990623354911804
Epoch 1190, training loss: 83.6911849975586 = 0.30552545189857483 + 10.0 * 8.338565826416016
Epoch 1190, val loss: 0.3988720774650574
Epoch 1200, training loss: 83.678466796875 = 0.3045475482940674 + 10.0 * 8.33739185333252
Epoch 1200, val loss: 0.39855867624282837
Epoch 1210, training loss: 83.67291259765625 = 0.30358535051345825 + 10.0 * 8.336932182312012
Epoch 1210, val loss: 0.3983410894870758
Epoch 1220, training loss: 83.68083953857422 = 0.3026314079761505 + 10.0 * 8.337820053100586
Epoch 1220, val loss: 0.39805296063423157
Epoch 1230, training loss: 83.71000671386719 = 0.30166783928871155 + 10.0 * 8.34083366394043
Epoch 1230, val loss: 0.3978394865989685
Epoch 1240, training loss: 83.67572784423828 = 0.30070844292640686 + 10.0 * 8.337502479553223
Epoch 1240, val loss: 0.3976021707057953
Epoch 1250, training loss: 83.65521240234375 = 0.29976895451545715 + 10.0 * 8.33554458618164
Epoch 1250, val loss: 0.3974033296108246
Epoch 1260, training loss: 83.64566040039062 = 0.2988336384296417 + 10.0 * 8.33468246459961
Epoch 1260, val loss: 0.39716777205467224
Epoch 1270, training loss: 83.64888000488281 = 0.29790571331977844 + 10.0 * 8.335097312927246
Epoch 1270, val loss: 0.39697641134262085
Epoch 1280, training loss: 83.70880126953125 = 0.2969689965248108 + 10.0 * 8.34118366241455
Epoch 1280, val loss: 0.3967723846435547
Epoch 1290, training loss: 83.65679931640625 = 0.29601848125457764 + 10.0 * 8.336077690124512
Epoch 1290, val loss: 0.39651045203208923
Epoch 1300, training loss: 83.6329116821289 = 0.2950948178768158 + 10.0 * 8.333781242370605
Epoch 1300, val loss: 0.3963705599308014
Epoch 1310, training loss: 83.64250183105469 = 0.2941708564758301 + 10.0 * 8.334833145141602
Epoch 1310, val loss: 0.3961617350578308
Epoch 1320, training loss: 83.62295532226562 = 0.2932535707950592 + 10.0 * 8.33297061920166
Epoch 1320, val loss: 0.3959912955760956
Epoch 1330, training loss: 83.6187973022461 = 0.29233747720718384 + 10.0 * 8.332646369934082
Epoch 1330, val loss: 0.39569446444511414
Epoch 1340, training loss: 83.62008666992188 = 0.2914218008518219 + 10.0 * 8.332866668701172
Epoch 1340, val loss: 0.395588755607605
Epoch 1350, training loss: 83.62735748291016 = 0.2905008792877197 + 10.0 * 8.333684921264648
Epoch 1350, val loss: 0.3954031467437744
Epoch 1360, training loss: 83.63040161132812 = 0.289580762386322 + 10.0 * 8.334081649780273
Epoch 1360, val loss: 0.395262211561203
Epoch 1370, training loss: 83.61896514892578 = 0.28864890336990356 + 10.0 * 8.33303165435791
Epoch 1370, val loss: 0.3950054943561554
Epoch 1380, training loss: 83.59039306640625 = 0.287733256816864 + 10.0 * 8.330265998840332
Epoch 1380, val loss: 0.3948028087615967
Epoch 1390, training loss: 83.58464050292969 = 0.28681859374046326 + 10.0 * 8.329782485961914
Epoch 1390, val loss: 0.3946572244167328
Epoch 1400, training loss: 83.59165954589844 = 0.2859037220478058 + 10.0 * 8.330575942993164
Epoch 1400, val loss: 0.39446523785591125
Epoch 1410, training loss: 83.60716247558594 = 0.2849777638912201 + 10.0 * 8.332218170166016
Epoch 1410, val loss: 0.3943450450897217
Epoch 1420, training loss: 83.58081817626953 = 0.2840529978275299 + 10.0 * 8.329676628112793
Epoch 1420, val loss: 0.394072026014328
Epoch 1430, training loss: 83.57791900634766 = 0.28312596678733826 + 10.0 * 8.329479217529297
Epoch 1430, val loss: 0.39402344822883606
Epoch 1440, training loss: 83.60639953613281 = 0.282193124294281 + 10.0 * 8.332420349121094
Epoch 1440, val loss: 0.3938111364841461
Epoch 1450, training loss: 83.57769775390625 = 0.2812482416629791 + 10.0 * 8.329645156860352
Epoch 1450, val loss: 0.3935663402080536
Epoch 1460, training loss: 83.55823516845703 = 0.2803293764591217 + 10.0 * 8.327791213989258
Epoch 1460, val loss: 0.3934479057788849
Epoch 1470, training loss: 83.54891204833984 = 0.2794005870819092 + 10.0 * 8.326951026916504
Epoch 1470, val loss: 0.3932380676269531
Epoch 1480, training loss: 83.54244232177734 = 0.2784759998321533 + 10.0 * 8.326396942138672
Epoch 1480, val loss: 0.3931313753128052
Epoch 1490, training loss: 83.57379150390625 = 0.27754417061805725 + 10.0 * 8.329625129699707
Epoch 1490, val loss: 0.39297857880592346
Epoch 1500, training loss: 83.56060791015625 = 0.27658313512802124 + 10.0 * 8.328402519226074
Epoch 1500, val loss: 0.3928729295730591
Epoch 1510, training loss: 83.5298843383789 = 0.2756335139274597 + 10.0 * 8.325425148010254
Epoch 1510, val loss: 0.3926301598548889
Epoch 1520, training loss: 83.52655029296875 = 0.27468979358673096 + 10.0 * 8.325185775756836
Epoch 1520, val loss: 0.39253440499305725
Epoch 1530, training loss: 83.52315521240234 = 0.2737525999546051 + 10.0 * 8.32494068145752
Epoch 1530, val loss: 0.3923793137073517
Epoch 1540, training loss: 83.59638214111328 = 0.2728075385093689 + 10.0 * 8.332357406616211
Epoch 1540, val loss: 0.39228662848472595
Epoch 1550, training loss: 83.53672790527344 = 0.2718401849269867 + 10.0 * 8.326488494873047
Epoch 1550, val loss: 0.3921615481376648
Epoch 1560, training loss: 83.51615905761719 = 0.270880788564682 + 10.0 * 8.324527740478516
Epoch 1560, val loss: 0.3920496106147766
Epoch 1570, training loss: 83.50373077392578 = 0.26992523670196533 + 10.0 * 8.323380470275879
Epoch 1570, val loss: 0.3919573426246643
Epoch 1580, training loss: 83.51531219482422 = 0.26896342635154724 + 10.0 * 8.324634552001953
Epoch 1580, val loss: 0.39181116223335266
Epoch 1590, training loss: 83.53364562988281 = 0.2679762840270996 + 10.0 * 8.326566696166992
Epoch 1590, val loss: 0.3918687105178833
Epoch 1600, training loss: 83.50726318359375 = 0.26699355244636536 + 10.0 * 8.324027061462402
Epoch 1600, val loss: 0.39151325821876526
Epoch 1610, training loss: 83.49198913574219 = 0.2660176455974579 + 10.0 * 8.32259750366211
Epoch 1610, val loss: 0.3915775716304779
Epoch 1620, training loss: 83.48381042480469 = 0.2650485932826996 + 10.0 * 8.321876525878906
Epoch 1620, val loss: 0.3914482891559601
Epoch 1630, training loss: 83.47863006591797 = 0.2640731632709503 + 10.0 * 8.321455001831055
Epoch 1630, val loss: 0.39138349890708923
Epoch 1640, training loss: 83.4971923828125 = 0.2630864977836609 + 10.0 * 8.323410987854004
Epoch 1640, val loss: 0.3913484215736389
Epoch 1650, training loss: 83.48561096191406 = 0.26207566261291504 + 10.0 * 8.32235336303711
Epoch 1650, val loss: 0.3912706971168518
Epoch 1660, training loss: 83.46736145019531 = 0.26107481122016907 + 10.0 * 8.32062816619873
Epoch 1660, val loss: 0.39110267162323
Epoch 1670, training loss: 83.46221923828125 = 0.2600707411766052 + 10.0 * 8.320215225219727
Epoch 1670, val loss: 0.39107027649879456
Epoch 1680, training loss: 83.45784759521484 = 0.25906887650489807 + 10.0 * 8.319877624511719
Epoch 1680, val loss: 0.3910406827926636
Epoch 1690, training loss: 83.46058654785156 = 0.25806131958961487 + 10.0 * 8.320252418518066
Epoch 1690, val loss: 0.39100074768066406
Epoch 1700, training loss: 83.53885650634766 = 0.25704148411750793 + 10.0 * 8.328181266784668
Epoch 1700, val loss: 0.39102455973625183
Epoch 1710, training loss: 83.46930694580078 = 0.2560034394264221 + 10.0 * 8.321330070495605
Epoch 1710, val loss: 0.3908870816230774
Epoch 1720, training loss: 83.4461898803711 = 0.2549687922000885 + 10.0 * 8.319122314453125
Epoch 1720, val loss: 0.39086008071899414
Epoch 1730, training loss: 83.44210052490234 = 0.25394120812416077 + 10.0 * 8.318815231323242
Epoch 1730, val loss: 0.3908202648162842
Epoch 1740, training loss: 83.44306945800781 = 0.25290632247924805 + 10.0 * 8.319016456604004
Epoch 1740, val loss: 0.3907727897167206
Epoch 1750, training loss: 83.51544952392578 = 0.25185731053352356 + 10.0 * 8.326359748840332
Epoch 1750, val loss: 0.3907568156719208
Epoch 1760, training loss: 83.49574279785156 = 0.2507966160774231 + 10.0 * 8.324495315551758
Epoch 1760, val loss: 0.39081937074661255
Epoch 1770, training loss: 83.43833923339844 = 0.24973687529563904 + 10.0 * 8.318860054016113
Epoch 1770, val loss: 0.3908056616783142
Epoch 1780, training loss: 83.42747497558594 = 0.248688206076622 + 10.0 * 8.317878723144531
Epoch 1780, val loss: 0.3908367156982422
Epoch 1790, training loss: 83.4222412109375 = 0.24763576686382294 + 10.0 * 8.317461013793945
Epoch 1790, val loss: 0.390850305557251
Epoch 1800, training loss: 83.4462890625 = 0.2465796172618866 + 10.0 * 8.319971084594727
Epoch 1800, val loss: 0.3909273147583008
Epoch 1810, training loss: 83.42794036865234 = 0.24550911784172058 + 10.0 * 8.318243026733398
Epoch 1810, val loss: 0.39101704955101013
Epoch 1820, training loss: 83.42159271240234 = 0.2444363683462143 + 10.0 * 8.317715644836426
Epoch 1820, val loss: 0.39108923077583313
Epoch 1830, training loss: 83.41411590576172 = 0.2433580607175827 + 10.0 * 8.317075729370117
Epoch 1830, val loss: 0.3911550045013428
Epoch 1840, training loss: 83.41259765625 = 0.2422812134027481 + 10.0 * 8.317031860351562
Epoch 1840, val loss: 0.39125779271125793
Epoch 1850, training loss: 83.434814453125 = 0.24119780957698822 + 10.0 * 8.319361686706543
Epoch 1850, val loss: 0.39139488339424133
Epoch 1860, training loss: 83.42217254638672 = 0.24009457230567932 + 10.0 * 8.318207740783691
Epoch 1860, val loss: 0.39140114188194275
Epoch 1870, training loss: 83.41525268554688 = 0.2389896810054779 + 10.0 * 8.317625999450684
Epoch 1870, val loss: 0.3915517330169678
Epoch 1880, training loss: 83.40584564208984 = 0.23788686096668243 + 10.0 * 8.31679630279541
Epoch 1880, val loss: 0.3917023539543152
Epoch 1890, training loss: 83.40038299560547 = 0.23678523302078247 + 10.0 * 8.316359519958496
Epoch 1890, val loss: 0.3918825089931488
Epoch 1900, training loss: 83.40018463134766 = 0.23568220436573029 + 10.0 * 8.316450119018555
Epoch 1900, val loss: 0.39201676845550537
Epoch 1910, training loss: 83.38941192626953 = 0.23456783592700958 + 10.0 * 8.315485000610352
Epoch 1910, val loss: 0.3920822739601135
Epoch 1920, training loss: 83.38236236572266 = 0.2334541231393814 + 10.0 * 8.31489086151123
Epoch 1920, val loss: 0.3923042118549347
Epoch 1930, training loss: 83.48653411865234 = 0.23233847320079803 + 10.0 * 8.325419425964355
Epoch 1930, val loss: 0.39248451590538025
Epoch 1940, training loss: 83.40564727783203 = 0.23120199143886566 + 10.0 * 8.317444801330566
Epoch 1940, val loss: 0.3925691246986389
Epoch 1950, training loss: 83.3834228515625 = 0.2300765961408615 + 10.0 * 8.31533432006836
Epoch 1950, val loss: 0.39289990067481995
Epoch 1960, training loss: 83.36856079101562 = 0.2289537638425827 + 10.0 * 8.313961029052734
Epoch 1960, val loss: 0.39293888211250305
Epoch 1970, training loss: 83.36207580566406 = 0.22783292829990387 + 10.0 * 8.313424110412598
Epoch 1970, val loss: 0.39320695400238037
Epoch 1980, training loss: 83.37277221679688 = 0.22670266032218933 + 10.0 * 8.314607620239258
Epoch 1980, val loss: 0.3933334946632385
Epoch 1990, training loss: 83.3929443359375 = 0.22556184232234955 + 10.0 * 8.31673812866211
Epoch 1990, val loss: 0.39369046688079834
Epoch 2000, training loss: 83.36734771728516 = 0.22442182898521423 + 10.0 * 8.314292907714844
Epoch 2000, val loss: 0.3938334584236145
Epoch 2010, training loss: 83.3712158203125 = 0.22328217327594757 + 10.0 * 8.314793586730957
Epoch 2010, val loss: 0.3941740095615387
Epoch 2020, training loss: 83.34518432617188 = 0.22213295102119446 + 10.0 * 8.312305450439453
Epoch 2020, val loss: 0.3943505883216858
Epoch 2030, training loss: 83.34587097167969 = 0.2209874838590622 + 10.0 * 8.312488555908203
Epoch 2030, val loss: 0.39464879035949707
Epoch 2040, training loss: 83.3569564819336 = 0.21983669698238373 + 10.0 * 8.313712120056152
Epoch 2040, val loss: 0.39489537477493286
Epoch 2050, training loss: 83.36451721191406 = 0.21867592632770538 + 10.0 * 8.314584732055664
Epoch 2050, val loss: 0.39519041776657104
Epoch 2060, training loss: 83.3564453125 = 0.2175174504518509 + 10.0 * 8.31389331817627
Epoch 2060, val loss: 0.39551499485969543
Epoch 2070, training loss: 83.3707275390625 = 0.2163451910018921 + 10.0 * 8.315438270568848
Epoch 2070, val loss: 0.3958909511566162
Epoch 2080, training loss: 83.35327911376953 = 0.21516597270965576 + 10.0 * 8.313811302185059
Epoch 2080, val loss: 0.3962617814540863
Epoch 2090, training loss: 83.32971954345703 = 0.21399155259132385 + 10.0 * 8.311573028564453
Epoch 2090, val loss: 0.39663010835647583
Epoch 2100, training loss: 83.32096862792969 = 0.212816521525383 + 10.0 * 8.310815811157227
Epoch 2100, val loss: 0.3969982862472534
Epoch 2110, training loss: 83.31939697265625 = 0.21163798868656158 + 10.0 * 8.310775756835938
Epoch 2110, val loss: 0.3973572254180908
Epoch 2120, training loss: 83.37796020507812 = 0.21045757830142975 + 10.0 * 8.316750526428223
Epoch 2120, val loss: 0.3977753520011902
Epoch 2130, training loss: 83.31379699707031 = 0.20925964415073395 + 10.0 * 8.310453414916992
Epoch 2130, val loss: 0.39828118681907654
Epoch 2140, training loss: 83.3094253540039 = 0.20806849002838135 + 10.0 * 8.310135841369629
Epoch 2140, val loss: 0.3986537754535675
Epoch 2150, training loss: 83.30982971191406 = 0.20687656104564667 + 10.0 * 8.310295104980469
Epoch 2150, val loss: 0.39909645915031433
Epoch 2160, training loss: 83.32808685302734 = 0.2056829035282135 + 10.0 * 8.312240600585938
Epoch 2160, val loss: 0.39954420924186707
Epoch 2170, training loss: 83.31194305419922 = 0.20448210835456848 + 10.0 * 8.310746192932129
Epoch 2170, val loss: 0.3999647796154022
Epoch 2180, training loss: 83.33430480957031 = 0.20328103005886078 + 10.0 * 8.313102722167969
Epoch 2180, val loss: 0.40053147077560425
Epoch 2190, training loss: 83.32389831542969 = 0.20207339525222778 + 10.0 * 8.312182426452637
Epoch 2190, val loss: 0.4011714458465576
Epoch 2200, training loss: 83.29702758789062 = 0.2008628398180008 + 10.0 * 8.309617042541504
Epoch 2200, val loss: 0.4016534090042114
Epoch 2210, training loss: 83.28699493408203 = 0.19964878261089325 + 10.0 * 8.308734893798828
Epoch 2210, val loss: 0.4022365212440491
Epoch 2220, training loss: 83.28117370605469 = 0.1984385848045349 + 10.0 * 8.308273315429688
Epoch 2220, val loss: 0.4028957486152649
Epoch 2230, training loss: 83.28258514404297 = 0.19722549617290497 + 10.0 * 8.3085355758667
Epoch 2230, val loss: 0.4035540223121643
Epoch 2240, training loss: 83.3959732055664 = 0.19601798057556152 + 10.0 * 8.319995880126953
Epoch 2240, val loss: 0.4042806029319763
Epoch 2250, training loss: 83.31161499023438 = 0.19478386640548706 + 10.0 * 8.31168270111084
Epoch 2250, val loss: 0.40469345450401306
Epoch 2260, training loss: 83.27470397949219 = 0.19355714321136475 + 10.0 * 8.308115005493164
Epoch 2260, val loss: 0.4054752290248871
Epoch 2270, training loss: 83.26831817626953 = 0.1923324167728424 + 10.0 * 8.307599067687988
Epoch 2270, val loss: 0.40613558888435364
Epoch 2280, training loss: 83.26595306396484 = 0.19110584259033203 + 10.0 * 8.30748462677002
Epoch 2280, val loss: 0.40670761466026306
Epoch 2290, training loss: 83.28885650634766 = 0.18987442553043365 + 10.0 * 8.309898376464844
Epoch 2290, val loss: 0.40748435258865356
Epoch 2300, training loss: 83.27859497070312 = 0.1886286437511444 + 10.0 * 8.308996200561523
Epoch 2300, val loss: 0.4082690179347992
Epoch 2310, training loss: 83.26151275634766 = 0.18739661574363708 + 10.0 * 8.307412147521973
Epoch 2310, val loss: 0.4090017080307007
Epoch 2320, training loss: 83.25611877441406 = 0.18615859746932983 + 10.0 * 8.30699634552002
Epoch 2320, val loss: 0.4097234010696411
Epoch 2330, training loss: 83.27369689941406 = 0.18493305146694183 + 10.0 * 8.308876037597656
Epoch 2330, val loss: 0.41061651706695557
Epoch 2340, training loss: 83.28477478027344 = 0.18370038270950317 + 10.0 * 8.310107231140137
Epoch 2340, val loss: 0.41144859790802
Epoch 2350, training loss: 83.25395202636719 = 0.18246090412139893 + 10.0 * 8.307148933410645
Epoch 2350, val loss: 0.4121803343296051
Epoch 2360, training loss: 83.24322509765625 = 0.18122883141040802 + 10.0 * 8.30620002746582
Epoch 2360, val loss: 0.4129672348499298
Epoch 2370, training loss: 83.2392349243164 = 0.1800030767917633 + 10.0 * 8.305923461914062
Epoch 2370, val loss: 0.4138436019420624
Epoch 2380, training loss: 83.25138092041016 = 0.17878037691116333 + 10.0 * 8.307260513305664
Epoch 2380, val loss: 0.41469478607177734
Epoch 2390, training loss: 83.28103637695312 = 0.17755089700222015 + 10.0 * 8.310348510742188
Epoch 2390, val loss: 0.4156997799873352
Epoch 2400, training loss: 83.26792907714844 = 0.17630919814109802 + 10.0 * 8.309162139892578
Epoch 2400, val loss: 0.4165893793106079
Epoch 2410, training loss: 83.2313232421875 = 0.17507711052894592 + 10.0 * 8.305624008178711
Epoch 2410, val loss: 0.41749638319015503
Epoch 2420, training loss: 83.2222900390625 = 0.1738414764404297 + 10.0 * 8.304844856262207
Epoch 2420, val loss: 0.4183555245399475
Epoch 2430, training loss: 83.22445678710938 = 0.17260995507240295 + 10.0 * 8.305185317993164
Epoch 2430, val loss: 0.4192522466182709
Epoch 2440, training loss: 83.27027130126953 = 0.1713760793209076 + 10.0 * 8.30988883972168
Epoch 2440, val loss: 0.42017802596092224
Epoch 2450, training loss: 83.21353149414062 = 0.17012231051921844 + 10.0 * 8.304341316223145
Epoch 2450, val loss: 0.42120227217674255
Epoch 2460, training loss: 83.22034454345703 = 0.1688825935125351 + 10.0 * 8.305146217346191
Epoch 2460, val loss: 0.4221389889717102
Epoch 2470, training loss: 83.20796203613281 = 0.16764043271541595 + 10.0 * 8.304032325744629
Epoch 2470, val loss: 0.4230661988258362
Epoch 2480, training loss: 83.2165298461914 = 0.16640423238277435 + 10.0 * 8.305012702941895
Epoch 2480, val loss: 0.4241122305393219
Epoch 2490, training loss: 83.26231384277344 = 0.16517014801502228 + 10.0 * 8.309714317321777
Epoch 2490, val loss: 0.42526817321777344
Epoch 2500, training loss: 83.21308135986328 = 0.16392046213150024 + 10.0 * 8.304916381835938
Epoch 2500, val loss: 0.4261649250984192
Epoch 2510, training loss: 83.19901275634766 = 0.1626788079738617 + 10.0 * 8.303632736206055
Epoch 2510, val loss: 0.427157461643219
Epoch 2520, training loss: 83.20115661621094 = 0.16144192218780518 + 10.0 * 8.303971290588379
Epoch 2520, val loss: 0.428202748298645
Epoch 2530, training loss: 83.25186157226562 = 0.16021126508712769 + 10.0 * 8.309165000915527
Epoch 2530, val loss: 0.4294755160808563
Epoch 2540, training loss: 83.2041244506836 = 0.1589614301919937 + 10.0 * 8.304516792297363
Epoch 2540, val loss: 0.4303630292415619
Epoch 2550, training loss: 83.19871520996094 = 0.15772859752178192 + 10.0 * 8.304098129272461
Epoch 2550, val loss: 0.4316565990447998
Epoch 2560, training loss: 83.18267822265625 = 0.1564950793981552 + 10.0 * 8.302618026733398
Epoch 2560, val loss: 0.4326617419719696
Epoch 2570, training loss: 83.18389892578125 = 0.15526999533176422 + 10.0 * 8.302862167358398
Epoch 2570, val loss: 0.4337920844554901
Epoch 2580, training loss: 83.22476196289062 = 0.15404781699180603 + 10.0 * 8.307071685791016
Epoch 2580, val loss: 0.4349365830421448
Epoch 2590, training loss: 83.1773910522461 = 0.15282128751277924 + 10.0 * 8.302456855773926
Epoch 2590, val loss: 0.43626829981803894
Epoch 2600, training loss: 83.18976593017578 = 0.1516047567129135 + 10.0 * 8.303815841674805
Epoch 2600, val loss: 0.43754851818084717
Epoch 2610, training loss: 83.18714904785156 = 0.15037962794303894 + 10.0 * 8.30367660522461
Epoch 2610, val loss: 0.43876224756240845
Epoch 2620, training loss: 83.16586303710938 = 0.1491624265909195 + 10.0 * 8.30167007446289
Epoch 2620, val loss: 0.4401279389858246
Epoch 2630, training loss: 83.16357421875 = 0.1479477882385254 + 10.0 * 8.301562309265137
Epoch 2630, val loss: 0.4413430094718933
Epoch 2640, training loss: 83.20365905761719 = 0.14674027264118195 + 10.0 * 8.305691719055176
Epoch 2640, val loss: 0.4427032768726349
Epoch 2650, training loss: 83.21125030517578 = 0.1455199122428894 + 10.0 * 8.306572914123535
Epoch 2650, val loss: 0.4440235495567322
Epoch 2660, training loss: 83.1600112915039 = 0.14429503679275513 + 10.0 * 8.3015718460083
Epoch 2660, val loss: 0.4453411400318146
Epoch 2670, training loss: 83.15258026123047 = 0.14307524263858795 + 10.0 * 8.30095100402832
Epoch 2670, val loss: 0.4466954469680786
Epoch 2680, training loss: 83.14828491210938 = 0.1418590545654297 + 10.0 * 8.300642013549805
Epoch 2680, val loss: 0.4480579197406769
Epoch 2690, training loss: 83.14392852783203 = 0.14064529538154602 + 10.0 * 8.300328254699707
Epoch 2690, val loss: 0.4494234025478363
Epoch 2700, training loss: 83.1474609375 = 0.13943596184253693 + 10.0 * 8.300802230834961
Epoch 2700, val loss: 0.4508094787597656
Epoch 2710, training loss: 83.23062896728516 = 0.1382424235343933 + 10.0 * 8.30923843383789
Epoch 2710, val loss: 0.45219799876213074
Epoch 2720, training loss: 83.17635345458984 = 0.13704532384872437 + 10.0 * 8.303930282592773
Epoch 2720, val loss: 0.4537878930568695
Epoch 2730, training loss: 83.15314483642578 = 0.13584139943122864 + 10.0 * 8.301730155944824
Epoch 2730, val loss: 0.45498529076576233
Epoch 2740, training loss: 83.14446258544922 = 0.13464921712875366 + 10.0 * 8.300981521606445
Epoch 2740, val loss: 0.4564800560474396
Epoch 2750, training loss: 83.19322967529297 = 0.13346818089485168 + 10.0 * 8.305975914001465
Epoch 2750, val loss: 0.4579608142375946
Epoch 2760, training loss: 83.13726043701172 = 0.13228727877140045 + 10.0 * 8.300497055053711
Epoch 2760, val loss: 0.45942288637161255
Epoch 2770, training loss: 83.12300109863281 = 0.13110019266605377 + 10.0 * 8.299189567565918
Epoch 2770, val loss: 0.4607656002044678
Epoch 2780, training loss: 83.12157440185547 = 0.12991787493228912 + 10.0 * 8.299165725708008
Epoch 2780, val loss: 0.4621744751930237
Epoch 2790, training loss: 83.15052795410156 = 0.1287386417388916 + 10.0 * 8.302179336547852
Epoch 2790, val loss: 0.46352317929267883
Epoch 2800, training loss: 83.15682220458984 = 0.127558633685112 + 10.0 * 8.302927017211914
Epoch 2800, val loss: 0.46532532572746277
Epoch 2810, training loss: 83.12252807617188 = 0.1263810396194458 + 10.0 * 8.299614906311035
Epoch 2810, val loss: 0.4666249752044678
Epoch 2820, training loss: 83.11851501464844 = 0.12520921230316162 + 10.0 * 8.299330711364746
Epoch 2820, val loss: 0.46803879737854004
Epoch 2830, training loss: 83.11015319824219 = 0.12404486536979675 + 10.0 * 8.29861068725586
Epoch 2830, val loss: 0.46957147121429443
Epoch 2840, training loss: 83.11498260498047 = 0.12289173901081085 + 10.0 * 8.299208641052246
Epoch 2840, val loss: 0.4711146950721741
Epoch 2850, training loss: 83.16875457763672 = 0.12174572050571442 + 10.0 * 8.30470085144043
Epoch 2850, val loss: 0.47265079617500305
Epoch 2860, training loss: 83.12556457519531 = 0.12058768421411514 + 10.0 * 8.300497055053711
Epoch 2860, val loss: 0.4741091728210449
Epoch 2870, training loss: 83.11717224121094 = 0.11944851279258728 + 10.0 * 8.299772262573242
Epoch 2870, val loss: 0.475831538438797
Epoch 2880, training loss: 83.14048767089844 = 0.11831461638212204 + 10.0 * 8.302217483520508
Epoch 2880, val loss: 0.47741448879241943
Epoch 2890, training loss: 83.10145568847656 = 0.11718147993087769 + 10.0 * 8.29842758178711
Epoch 2890, val loss: 0.4790814220905304
Epoch 2900, training loss: 83.09508514404297 = 0.11605412513017654 + 10.0 * 8.297903060913086
Epoch 2900, val loss: 0.48077312111854553
Epoch 2910, training loss: 83.09605407714844 = 0.11493507772684097 + 10.0 * 8.298111915588379
Epoch 2910, val loss: 0.48254629969596863
Epoch 2920, training loss: 83.17759704589844 = 0.11383030563592911 + 10.0 * 8.306376457214355
Epoch 2920, val loss: 0.4842298924922943
Epoch 2930, training loss: 83.11257934570312 = 0.11269547790288925 + 10.0 * 8.299988746643066
Epoch 2930, val loss: 0.4857420325279236
Epoch 2940, training loss: 83.09075927734375 = 0.1115865707397461 + 10.0 * 8.297917366027832
Epoch 2940, val loss: 0.48750531673431396
Epoch 2950, training loss: 83.08296203613281 = 0.11047857254743576 + 10.0 * 8.297247886657715
Epoch 2950, val loss: 0.4892362952232361
Epoch 2960, training loss: 83.07743835449219 = 0.10937269032001495 + 10.0 * 8.296806335449219
Epoch 2960, val loss: 0.49100661277770996
Epoch 2970, training loss: 83.10103607177734 = 0.10827156901359558 + 10.0 * 8.299276351928711
Epoch 2970, val loss: 0.49289169907569885
Epoch 2980, training loss: 83.09415435791016 = 0.10716366022825241 + 10.0 * 8.298699378967285
Epoch 2980, val loss: 0.49474650621414185
Epoch 2990, training loss: 83.08293914794922 = 0.10606572031974792 + 10.0 * 8.297687530517578
Epoch 2990, val loss: 0.49644479155540466
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8427194317605275
0.8674201260595523
=== training gcn model ===
Epoch 0, training loss: 106.93187713623047 = 1.1094257831573486 + 10.0 * 10.582244873046875
Epoch 0, val loss: 1.1079596281051636
Epoch 10, training loss: 106.91180419921875 = 1.1012625694274902 + 10.0 * 10.581053733825684
Epoch 10, val loss: 1.0997955799102783
Epoch 20, training loss: 106.78820037841797 = 1.091672420501709 + 10.0 * 10.569652557373047
Epoch 20, val loss: 1.0901917219161987
Epoch 30, training loss: 106.0152359008789 = 1.0807592868804932 + 10.0 * 10.493448257446289
Epoch 30, val loss: 1.0793418884277344
Epoch 40, training loss: 103.27206420898438 = 1.0710729360580444 + 10.0 * 10.220098495483398
Epoch 40, val loss: 1.0701788663864136
Epoch 50, training loss: 98.63731384277344 = 1.063812255859375 + 10.0 * 9.757349967956543
Epoch 50, val loss: 1.0630998611450195
Epoch 60, training loss: 96.2781982421875 = 1.0556834936141968 + 10.0 * 9.52225112915039
Epoch 60, val loss: 1.054728627204895
Epoch 70, training loss: 94.71697998046875 = 1.0457544326782227 + 10.0 * 9.367122650146484
Epoch 70, val loss: 1.0449939966201782
Epoch 80, training loss: 93.42178344726562 = 1.0363892316818237 + 10.0 * 9.23853874206543
Epoch 80, val loss: 1.0360153913497925
Epoch 90, training loss: 92.76619720458984 = 1.0277295112609863 + 10.0 * 9.173846244812012
Epoch 90, val loss: 1.0276894569396973
Epoch 100, training loss: 92.2350082397461 = 1.0201729536056519 + 10.0 * 9.12148380279541
Epoch 100, val loss: 1.020349144935608
Epoch 110, training loss: 91.31553649902344 = 1.0128761529922485 + 10.0 * 9.030265808105469
Epoch 110, val loss: 1.013136625289917
Epoch 120, training loss: 90.39804077148438 = 1.0066405534744263 + 10.0 * 8.939140319824219
Epoch 120, val loss: 1.0070267915725708
Epoch 130, training loss: 89.89762878417969 = 0.9995583891868591 + 10.0 * 8.889806747436523
Epoch 130, val loss: 0.9999812245368958
Epoch 140, training loss: 89.07138061523438 = 0.9912275671958923 + 10.0 * 8.808015823364258
Epoch 140, val loss: 0.9920580983161926
Epoch 150, training loss: 88.29225158691406 = 0.9836727976799011 + 10.0 * 8.730857849121094
Epoch 150, val loss: 0.9847148656845093
Epoch 160, training loss: 87.87171173095703 = 0.9742103219032288 + 10.0 * 8.689749717712402
Epoch 160, val loss: 0.9751841425895691
Epoch 170, training loss: 87.50865936279297 = 0.9622726440429688 + 10.0 * 8.654638290405273
Epoch 170, val loss: 0.9634585976600647
Epoch 180, training loss: 87.19667053222656 = 0.94920414686203 + 10.0 * 8.624746322631836
Epoch 180, val loss: 0.9507875442504883
Epoch 190, training loss: 86.93695068359375 = 0.9350124597549438 + 10.0 * 8.600193977355957
Epoch 190, val loss: 0.9368169903755188
Epoch 200, training loss: 86.7174072265625 = 0.9195317625999451 + 10.0 * 8.579787254333496
Epoch 200, val loss: 0.9216667413711548
Epoch 210, training loss: 86.5124282836914 = 0.9029757380485535 + 10.0 * 8.560945510864258
Epoch 210, val loss: 0.9055938124656677
Epoch 220, training loss: 86.35208129882812 = 0.8852782845497131 + 10.0 * 8.546680450439453
Epoch 220, val loss: 0.8884460926055908
Epoch 230, training loss: 86.21029663085938 = 0.8663535118103027 + 10.0 * 8.534394264221191
Epoch 230, val loss: 0.8700842261314392
Epoch 240, training loss: 86.0869140625 = 0.8465691804885864 + 10.0 * 8.52403450012207
Epoch 240, val loss: 0.8510342240333557
Epoch 250, training loss: 85.98088073730469 = 0.8262400031089783 + 10.0 * 8.515463829040527
Epoch 250, val loss: 0.8314681053161621
Epoch 260, training loss: 85.94678497314453 = 0.8051934242248535 + 10.0 * 8.514159202575684
Epoch 260, val loss: 0.8112935423851013
Epoch 270, training loss: 85.80432891845703 = 0.7840299606323242 + 10.0 * 8.502030372619629
Epoch 270, val loss: 0.7910874485969543
Epoch 280, training loss: 85.69396209716797 = 0.7630395293235779 + 10.0 * 8.49309253692627
Epoch 280, val loss: 0.7711360454559326
Epoch 290, training loss: 85.59431457519531 = 0.7421352863311768 + 10.0 * 8.485218048095703
Epoch 290, val loss: 0.7512431144714355
Epoch 300, training loss: 85.50699615478516 = 0.7215059399604797 + 10.0 * 8.478549003601074
Epoch 300, val loss: 0.7316920757293701
Epoch 310, training loss: 85.45801544189453 = 0.7011526823043823 + 10.0 * 8.475687026977539
Epoch 310, val loss: 0.7124857902526855
Epoch 320, training loss: 85.35545349121094 = 0.6813995242118835 + 10.0 * 8.467405319213867
Epoch 320, val loss: 0.6939311623573303
Epoch 330, training loss: 85.26403045654297 = 0.6626187562942505 + 10.0 * 8.4601411819458
Epoch 330, val loss: 0.6762729287147522
Epoch 340, training loss: 85.1881332397461 = 0.6446170210838318 + 10.0 * 8.454351425170898
Epoch 340, val loss: 0.6593780517578125
Epoch 350, training loss: 85.21286010742188 = 0.6274308562278748 + 10.0 * 8.458542823791504
Epoch 350, val loss: 0.643415093421936
Epoch 360, training loss: 85.0908432006836 = 0.6110950708389282 + 10.0 * 8.447975158691406
Epoch 360, val loss: 0.6282256245613098
Epoch 370, training loss: 85.00852966308594 = 0.5960249304771423 + 10.0 * 8.441250801086426
Epoch 370, val loss: 0.6143129467964172
Epoch 380, training loss: 84.94075012207031 = 0.5820808410644531 + 10.0 * 8.435867309570312
Epoch 380, val loss: 0.6014914512634277
Epoch 390, training loss: 84.9247817993164 = 0.5691609382629395 + 10.0 * 8.435562133789062
Epoch 390, val loss: 0.5897482633590698
Epoch 400, training loss: 84.85484313964844 = 0.5570420026779175 + 10.0 * 8.429780006408691
Epoch 400, val loss: 0.5787431001663208
Epoch 410, training loss: 84.79405212402344 = 0.5461597442626953 + 10.0 * 8.424789428710938
Epoch 410, val loss: 0.5690880417823792
Epoch 420, training loss: 84.75019073486328 = 0.5362966656684875 + 10.0 * 8.42138957977295
Epoch 420, val loss: 0.5602229833602905
Epoch 430, training loss: 84.75016021728516 = 0.5272058248519897 + 10.0 * 8.422295570373535
Epoch 430, val loss: 0.5522701740264893
Epoch 440, training loss: 84.6781234741211 = 0.5187892913818359 + 10.0 * 8.415933609008789
Epoch 440, val loss: 0.544918417930603
Epoch 450, training loss: 84.63529205322266 = 0.5111852288246155 + 10.0 * 8.412410736083984
Epoch 450, val loss: 0.5383738279342651
Epoch 460, training loss: 84.61813354492188 = 0.5042193531990051 + 10.0 * 8.411391258239746
Epoch 460, val loss: 0.5323495268821716
Epoch 470, training loss: 84.59539794921875 = 0.497709184885025 + 10.0 * 8.409769058227539
Epoch 470, val loss: 0.5269699692726135
Epoch 480, training loss: 84.53169250488281 = 0.49180373549461365 + 10.0 * 8.4039888381958
Epoch 480, val loss: 0.5221176743507385
Epoch 490, training loss: 84.50342559814453 = 0.4863786995410919 + 10.0 * 8.401704788208008
Epoch 490, val loss: 0.5175650715827942
Epoch 500, training loss: 84.5565185546875 = 0.48128432035446167 + 10.0 * 8.407523155212402
Epoch 500, val loss: 0.5135654211044312
Epoch 510, training loss: 84.44941711425781 = 0.476484477519989 + 10.0 * 8.397293090820312
Epoch 510, val loss: 0.5097977519035339
Epoch 520, training loss: 84.41978454589844 = 0.4721035659313202 + 10.0 * 8.394767761230469
Epoch 520, val loss: 0.5063117146492004
Epoch 530, training loss: 84.39350128173828 = 0.46798741817474365 + 10.0 * 8.39255142211914
Epoch 530, val loss: 0.5031530857086182
Epoch 540, training loss: 84.48826599121094 = 0.46406975388526917 + 10.0 * 8.402419090270996
Epoch 540, val loss: 0.5001341104507446
Epoch 550, training loss: 84.36907958984375 = 0.4602685570716858 + 10.0 * 8.390881538391113
Epoch 550, val loss: 0.49735763669013977
Epoch 560, training loss: 84.33151245117188 = 0.4567832946777344 + 10.0 * 8.387473106384277
Epoch 560, val loss: 0.4948217272758484
Epoch 570, training loss: 84.30229949951172 = 0.4534643888473511 + 10.0 * 8.384883880615234
Epoch 570, val loss: 0.49239784479141235
Epoch 580, training loss: 84.389404296875 = 0.4502658545970917 + 10.0 * 8.393914222717285
Epoch 580, val loss: 0.4900229871273041
Epoch 590, training loss: 84.27153015136719 = 0.4470708966255188 + 10.0 * 8.3824462890625
Epoch 590, val loss: 0.48790696263313293
Epoch 600, training loss: 84.24650573730469 = 0.4441567063331604 + 10.0 * 8.380234718322754
Epoch 600, val loss: 0.4859575927257538
Epoch 610, training loss: 84.22610473632812 = 0.44136279821395874 + 10.0 * 8.378474235534668
Epoch 610, val loss: 0.4840368628501892
Epoch 620, training loss: 84.20854949951172 = 0.4386499524116516 + 10.0 * 8.376989364624023
Epoch 620, val loss: 0.482347697019577
Epoch 630, training loss: 84.20499420166016 = 0.43602925539016724 + 10.0 * 8.376896858215332
Epoch 630, val loss: 0.48060938715934753
Epoch 640, training loss: 84.18426513671875 = 0.433420866727829 + 10.0 * 8.37508487701416
Epoch 640, val loss: 0.4789544641971588
Epoch 650, training loss: 84.1787338256836 = 0.4309190809726715 + 10.0 * 8.374781608581543
Epoch 650, val loss: 0.47742143273353577
Epoch 660, training loss: 84.15058135986328 = 0.42852067947387695 + 10.0 * 8.37220573425293
Epoch 660, val loss: 0.4757835566997528
Epoch 670, training loss: 84.16908264160156 = 0.42617565393447876 + 10.0 * 8.374290466308594
Epoch 670, val loss: 0.474422425031662
Epoch 680, training loss: 84.13862609863281 = 0.42384669184684753 + 10.0 * 8.371478080749512
Epoch 680, val loss: 0.4729771316051483
Epoch 690, training loss: 84.11589813232422 = 0.42162051796913147 + 10.0 * 8.369427680969238
Epoch 690, val loss: 0.47161298990249634
Epoch 700, training loss: 84.11690521240234 = 0.4194425940513611 + 10.0 * 8.369746208190918
Epoch 700, val loss: 0.4702465534210205
Epoch 710, training loss: 84.08564758300781 = 0.41730034351348877 + 10.0 * 8.36683464050293
Epoch 710, val loss: 0.4690414369106293
Epoch 720, training loss: 84.1070785522461 = 0.41521841287612915 + 10.0 * 8.369185447692871
Epoch 720, val loss: 0.46781063079833984
Epoch 730, training loss: 84.06877136230469 = 0.413153737783432 + 10.0 * 8.365561485290527
Epoch 730, val loss: 0.4665912389755249
Epoch 740, training loss: 84.04693603515625 = 0.4111635982990265 + 10.0 * 8.363576889038086
Epoch 740, val loss: 0.4654027223587036
Epoch 750, training loss: 84.03948974609375 = 0.40920475125312805 + 10.0 * 8.363028526306152
Epoch 750, val loss: 0.46432340145111084
Epoch 760, training loss: 84.0218505859375 = 0.4072762429714203 + 10.0 * 8.361456871032715
Epoch 760, val loss: 0.46316850185394287
Epoch 770, training loss: 84.12942504882812 = 0.40534737706184387 + 10.0 * 8.372407913208008
Epoch 770, val loss: 0.4619487226009369
Epoch 780, training loss: 84.01207733154297 = 0.4033878445625305 + 10.0 * 8.360868453979492
Epoch 780, val loss: 0.46100345253944397
Epoch 790, training loss: 83.99262237548828 = 0.40154531598091125 + 10.0 * 8.359107971191406
Epoch 790, val loss: 0.45990630984306335
Epoch 800, training loss: 83.97371673583984 = 0.39973801374435425 + 10.0 * 8.35739803314209
Epoch 800, val loss: 0.45901796221733093
Epoch 810, training loss: 83.97795104980469 = 0.39794468879699707 + 10.0 * 8.358000755310059
Epoch 810, val loss: 0.4579656720161438
Epoch 820, training loss: 83.94910430908203 = 0.39613431692123413 + 10.0 * 8.355297088623047
Epoch 820, val loss: 0.4569489359855652
Epoch 830, training loss: 83.95926666259766 = 0.3943497836589813 + 10.0 * 8.356492042541504
Epoch 830, val loss: 0.4559720456600189
Epoch 840, training loss: 83.94050598144531 = 0.39256808161735535 + 10.0 * 8.354793548583984
Epoch 840, val loss: 0.45504435896873474
Epoch 850, training loss: 83.9175033569336 = 0.39080899953842163 + 10.0 * 8.352669715881348
Epoch 850, val loss: 0.4540731906890869
Epoch 860, training loss: 83.9017105102539 = 0.38907521963119507 + 10.0 * 8.351263046264648
Epoch 860, val loss: 0.45321667194366455
Epoch 870, training loss: 83.89868927001953 = 0.38734757900238037 + 10.0 * 8.351134300231934
Epoch 870, val loss: 0.45233070850372314
Epoch 880, training loss: 83.90647888183594 = 0.3855825662612915 + 10.0 * 8.352089881896973
Epoch 880, val loss: 0.45131370425224304
Epoch 890, training loss: 83.88431549072266 = 0.38384267687797546 + 10.0 * 8.35004711151123
Epoch 890, val loss: 0.4504512846469879
Epoch 900, training loss: 83.8517837524414 = 0.3821529448032379 + 10.0 * 8.346962928771973
Epoch 900, val loss: 0.4495317339897156
Epoch 910, training loss: 83.83953094482422 = 0.38046908378601074 + 10.0 * 8.345906257629395
Epoch 910, val loss: 0.44859713315963745
Epoch 920, training loss: 83.88815307617188 = 0.378788024187088 + 10.0 * 8.350935935974121
Epoch 920, val loss: 0.447634756565094
Epoch 930, training loss: 83.86386108398438 = 0.377056360244751 + 10.0 * 8.34868049621582
Epoch 930, val loss: 0.446929395198822
Epoch 940, training loss: 83.80879211425781 = 0.37536555528640747 + 10.0 * 8.343342781066895
Epoch 940, val loss: 0.44586941599845886
Epoch 950, training loss: 83.80203247070312 = 0.3737025856971741 + 10.0 * 8.342832565307617
Epoch 950, val loss: 0.44500303268432617
Epoch 960, training loss: 83.80876922607422 = 0.3720459043979645 + 10.0 * 8.343671798706055
Epoch 960, val loss: 0.44408857822418213
Epoch 970, training loss: 83.78767395019531 = 0.3703332543373108 + 10.0 * 8.341733932495117
Epoch 970, val loss: 0.4432057738304138
Epoch 980, training loss: 83.78964233398438 = 0.3686535656452179 + 10.0 * 8.3420991897583
Epoch 980, val loss: 0.4422772526741028
Epoch 990, training loss: 83.76203155517578 = 0.3670150935649872 + 10.0 * 8.339502334594727
Epoch 990, val loss: 0.4414207339286804
Epoch 1000, training loss: 83.75072479248047 = 0.3653714656829834 + 10.0 * 8.33853530883789
Epoch 1000, val loss: 0.4405730068683624
Epoch 1010, training loss: 83.77336883544922 = 0.3637257218360901 + 10.0 * 8.340964317321777
Epoch 1010, val loss: 0.4396213889122009
Epoch 1020, training loss: 83.74524688720703 = 0.3620280623435974 + 10.0 * 8.338321685791016
Epoch 1020, val loss: 0.43881556391716003
Epoch 1030, training loss: 83.73635864257812 = 0.3603517711162567 + 10.0 * 8.337600708007812
Epoch 1030, val loss: 0.43779850006103516
Epoch 1040, training loss: 83.7187271118164 = 0.3586948812007904 + 10.0 * 8.336003303527832
Epoch 1040, val loss: 0.4368896484375
Epoch 1050, training loss: 83.79367065429688 = 0.3570253252983093 + 10.0 * 8.343664169311523
Epoch 1050, val loss: 0.43602997064590454
Epoch 1060, training loss: 83.750244140625 = 0.3553224503993988 + 10.0 * 8.339491844177246
Epoch 1060, val loss: 0.4351230561733246
Epoch 1070, training loss: 83.69804382324219 = 0.3536415100097656 + 10.0 * 8.334440231323242
Epoch 1070, val loss: 0.4341176748275757
Epoch 1080, training loss: 83.68543243408203 = 0.3519784212112427 + 10.0 * 8.333345413208008
Epoch 1080, val loss: 0.4332011640071869
Epoch 1090, training loss: 83.70012664794922 = 0.3503062129020691 + 10.0 * 8.334981918334961
Epoch 1090, val loss: 0.43229812383651733
Epoch 1100, training loss: 83.6745376586914 = 0.3485909104347229 + 10.0 * 8.33259391784668
Epoch 1100, val loss: 0.43138542771339417
Epoch 1110, training loss: 83.67337036132812 = 0.3468944728374481 + 10.0 * 8.332647323608398
Epoch 1110, val loss: 0.4304125905036926
Epoch 1120, training loss: 83.66626739501953 = 0.3452049195766449 + 10.0 * 8.33210563659668
Epoch 1120, val loss: 0.4294755458831787
Epoch 1130, training loss: 83.68341064453125 = 0.3435046374797821 + 10.0 * 8.333990097045898
Epoch 1130, val loss: 0.4285100996494293
Epoch 1140, training loss: 83.64361572265625 = 0.34177887439727783 + 10.0 * 8.330183029174805
Epoch 1140, val loss: 0.4277108907699585
Epoch 1150, training loss: 83.64107513427734 = 0.3400658369064331 + 10.0 * 8.330101013183594
Epoch 1150, val loss: 0.42672064900398254
Epoch 1160, training loss: 83.63640594482422 = 0.3383420407772064 + 10.0 * 8.329806327819824
Epoch 1160, val loss: 0.42571425437927246
Epoch 1170, training loss: 83.64742279052734 = 0.33660706877708435 + 10.0 * 8.33108139038086
Epoch 1170, val loss: 0.42479902505874634
Epoch 1180, training loss: 83.64439392089844 = 0.3348514139652252 + 10.0 * 8.330953598022461
Epoch 1180, val loss: 0.4239966869354248
Epoch 1190, training loss: 83.66571044921875 = 0.3330795466899872 + 10.0 * 8.333263397216797
Epoch 1190, val loss: 0.4229077994823456
Epoch 1200, training loss: 83.6270523071289 = 0.33130788803100586 + 10.0 * 8.329574584960938
Epoch 1200, val loss: 0.42205509543418884
Epoch 1210, training loss: 83.60531616210938 = 0.32954472303390503 + 10.0 * 8.327577590942383
Epoch 1210, val loss: 0.4210429787635803
Epoch 1220, training loss: 83.59405517578125 = 0.32778799533843994 + 10.0 * 8.326626777648926
Epoch 1220, val loss: 0.4201605021953583
Epoch 1230, training loss: 83.60698699951172 = 0.32603126764297485 + 10.0 * 8.328095436096191
Epoch 1230, val loss: 0.4192584156990051
Epoch 1240, training loss: 83.59439849853516 = 0.32423800230026245 + 10.0 * 8.32701587677002
Epoch 1240, val loss: 0.41832277178764343
Epoch 1250, training loss: 83.58915710449219 = 0.32244837284088135 + 10.0 * 8.32667064666748
Epoch 1250, val loss: 0.41737449169158936
Epoch 1260, training loss: 83.59803009033203 = 0.32065704464912415 + 10.0 * 8.327737808227539
Epoch 1260, val loss: 0.4164481461048126
Epoch 1270, training loss: 83.58068084716797 = 0.3188399076461792 + 10.0 * 8.326184272766113
Epoch 1270, val loss: 0.4156455993652344
Epoch 1280, training loss: 83.57061004638672 = 0.3170360326766968 + 10.0 * 8.325357437133789
Epoch 1280, val loss: 0.41473352909088135
Epoch 1290, training loss: 83.55585479736328 = 0.31522732973098755 + 10.0 * 8.324063301086426
Epoch 1290, val loss: 0.41387104988098145
Epoch 1300, training loss: 83.60527801513672 = 0.31340816617012024 + 10.0 * 8.329187393188477
Epoch 1300, val loss: 0.41303521394729614
Epoch 1310, training loss: 83.56917572021484 = 0.3115710914134979 + 10.0 * 8.325760841369629
Epoch 1310, val loss: 0.41210123896598816
Epoch 1320, training loss: 83.54996490478516 = 0.309749960899353 + 10.0 * 8.324021339416504
Epoch 1320, val loss: 0.4112412631511688
Epoch 1330, training loss: 83.53578186035156 = 0.3079368472099304 + 10.0 * 8.322784423828125
Epoch 1330, val loss: 0.4104408621788025
Epoch 1340, training loss: 83.5740966796875 = 0.3061218559741974 + 10.0 * 8.326797485351562
Epoch 1340, val loss: 0.4096284508705139
Epoch 1350, training loss: 83.5432357788086 = 0.30429333448410034 + 10.0 * 8.323894500732422
Epoch 1350, val loss: 0.4088231325149536
Epoch 1360, training loss: 83.51777648925781 = 0.3024764955043793 + 10.0 * 8.32153034210205
Epoch 1360, val loss: 0.40809550881385803
Epoch 1370, training loss: 83.50797271728516 = 0.30066850781440735 + 10.0 * 8.320730209350586
Epoch 1370, val loss: 0.4073476493358612
Epoch 1380, training loss: 83.56261444091797 = 0.29886412620544434 + 10.0 * 8.326375007629395
Epoch 1380, val loss: 0.40659666061401367
Epoch 1390, training loss: 83.51995086669922 = 0.2970351576805115 + 10.0 * 8.322291374206543
Epoch 1390, val loss: 0.40607142448425293
Epoch 1400, training loss: 83.5053482055664 = 0.29523465037345886 + 10.0 * 8.321011543273926
Epoch 1400, val loss: 0.40524616837501526
Epoch 1410, training loss: 83.55413818359375 = 0.2934403717517853 + 10.0 * 8.326069831848145
Epoch 1410, val loss: 0.40468093752861023
Epoch 1420, training loss: 83.49323272705078 = 0.29163068532943726 + 10.0 * 8.320159912109375
Epoch 1420, val loss: 0.40410247445106506
Epoch 1430, training loss: 83.48416900634766 = 0.2898533344268799 + 10.0 * 8.31943130493164
Epoch 1430, val loss: 0.40342843532562256
Epoch 1440, training loss: 83.47575378417969 = 0.2880861163139343 + 10.0 * 8.318766593933105
Epoch 1440, val loss: 0.4028782546520233
Epoch 1450, training loss: 83.49620819091797 = 0.2863267958164215 + 10.0 * 8.320988655090332
Epoch 1450, val loss: 0.40239331126213074
Epoch 1460, training loss: 83.46232604980469 = 0.28455913066864014 + 10.0 * 8.317776679992676
Epoch 1460, val loss: 0.4018452763557434
Epoch 1470, training loss: 83.46647644042969 = 0.28281110525131226 + 10.0 * 8.318366050720215
Epoch 1470, val loss: 0.4013765752315521
Epoch 1480, training loss: 83.49227142333984 = 0.2810724973678589 + 10.0 * 8.32111930847168
Epoch 1480, val loss: 0.4009770452976227
Epoch 1490, training loss: 83.45574188232422 = 0.27932798862457275 + 10.0 * 8.317641258239746
Epoch 1490, val loss: 0.40041762590408325
Epoch 1500, training loss: 83.4582290649414 = 0.2776077389717102 + 10.0 * 8.318061828613281
Epoch 1500, val loss: 0.4001100957393646
Epoch 1510, training loss: 83.45463562011719 = 0.2758902609348297 + 10.0 * 8.317874908447266
Epoch 1510, val loss: 0.3997150957584381
Epoch 1520, training loss: 83.43998718261719 = 0.274183452129364 + 10.0 * 8.316579818725586
Epoch 1520, val loss: 0.3994174003601074
Epoch 1530, training loss: 83.4384994506836 = 0.27249011397361755 + 10.0 * 8.316600799560547
Epoch 1530, val loss: 0.39910888671875
Epoch 1540, training loss: 83.47773742675781 = 0.2707988917827606 + 10.0 * 8.320693969726562
Epoch 1540, val loss: 0.3987272381782532
Epoch 1550, training loss: 83.45378112792969 = 0.2691136300563812 + 10.0 * 8.318467140197754
Epoch 1550, val loss: 0.3984206020832062
Epoch 1560, training loss: 83.42578125 = 0.26745158433914185 + 10.0 * 8.31583309173584
Epoch 1560, val loss: 0.39825505018234253
Epoch 1570, training loss: 83.41551208496094 = 0.2658008635044098 + 10.0 * 8.314970970153809
Epoch 1570, val loss: 0.3980156183242798
Epoch 1580, training loss: 83.4301528930664 = 0.26416832208633423 + 10.0 * 8.316598892211914
Epoch 1580, val loss: 0.39792004227638245
Epoch 1590, training loss: 83.43106842041016 = 0.2625229060649872 + 10.0 * 8.316854476928711
Epoch 1590, val loss: 0.39770835638046265
Epoch 1600, training loss: 83.40576171875 = 0.26089632511138916 + 10.0 * 8.314486503601074
Epoch 1600, val loss: 0.3976288139820099
Epoch 1610, training loss: 83.39794921875 = 0.25928184390068054 + 10.0 * 8.31386661529541
Epoch 1610, val loss: 0.397480845451355
Epoch 1620, training loss: 83.39984130859375 = 0.25767776370048523 + 10.0 * 8.314216613769531
Epoch 1620, val loss: 0.3973897397518158
Epoch 1630, training loss: 83.47655487060547 = 0.2560757100582123 + 10.0 * 8.32204818725586
Epoch 1630, val loss: 0.39740756154060364
Epoch 1640, training loss: 83.4035415649414 = 0.25446221232414246 + 10.0 * 8.314908027648926
Epoch 1640, val loss: 0.3972722589969635
Epoch 1650, training loss: 83.3801498413086 = 0.25287339091300964 + 10.0 * 8.312726974487305
Epoch 1650, val loss: 0.3973029553890228
Epoch 1660, training loss: 83.37561798095703 = 0.25129207968711853 + 10.0 * 8.312433242797852
Epoch 1660, val loss: 0.3973206877708435
Epoch 1670, training loss: 83.39064025878906 = 0.2497231513261795 + 10.0 * 8.314091682434082
Epoch 1670, val loss: 0.39737510681152344
Epoch 1680, training loss: 83.38593292236328 = 0.24814878404140472 + 10.0 * 8.3137788772583
Epoch 1680, val loss: 0.39739593863487244
Epoch 1690, training loss: 83.379638671875 = 0.24658668041229248 + 10.0 * 8.313304901123047
Epoch 1690, val loss: 0.3974628448486328
Epoch 1700, training loss: 83.3713607788086 = 0.2450290471315384 + 10.0 * 8.312633514404297
Epoch 1700, val loss: 0.3975675702095032
Epoch 1710, training loss: 83.35770416259766 = 0.2434769719839096 + 10.0 * 8.311422348022461
Epoch 1710, val loss: 0.39769017696380615
Epoch 1720, training loss: 83.36345672607422 = 0.24193359911441803 + 10.0 * 8.312151908874512
Epoch 1720, val loss: 0.3979340195655823
Epoch 1730, training loss: 83.38916015625 = 0.24038659036159515 + 10.0 * 8.3148775100708
Epoch 1730, val loss: 0.3980838358402252
Epoch 1740, training loss: 83.36914825439453 = 0.2388419508934021 + 10.0 * 8.313031196594238
Epoch 1740, val loss: 0.3981809914112091
Epoch 1750, training loss: 83.3515853881836 = 0.23730885982513428 + 10.0 * 8.311427116394043
Epoch 1750, val loss: 0.3984922468662262
Epoch 1760, training loss: 83.34730529785156 = 0.23577968776226044 + 10.0 * 8.311152458190918
Epoch 1760, val loss: 0.39864999055862427
Epoch 1770, training loss: 83.37545776367188 = 0.23425480723381042 + 10.0 * 8.314120292663574
Epoch 1770, val loss: 0.3988224267959595
Epoch 1780, training loss: 83.33505249023438 = 0.2327243834733963 + 10.0 * 8.310232162475586
Epoch 1780, val loss: 0.39916712045669556
Epoch 1790, training loss: 83.33256530761719 = 0.2312030792236328 + 10.0 * 8.310136795043945
Epoch 1790, val loss: 0.39943236112594604
Epoch 1800, training loss: 83.36112213134766 = 0.22968211770057678 + 10.0 * 8.313143730163574
Epoch 1800, val loss: 0.39982566237449646
Epoch 1810, training loss: 83.32969665527344 = 0.22815634310245514 + 10.0 * 8.31015396118164
Epoch 1810, val loss: 0.4000571072101593
Epoch 1820, training loss: 83.321044921875 = 0.2266383171081543 + 10.0 * 8.309440612792969
Epoch 1820, val loss: 0.4004642367362976
Epoch 1830, training loss: 83.31841278076172 = 0.22512122988700867 + 10.0 * 8.30932903289795
Epoch 1830, val loss: 0.40083444118499756
Epoch 1840, training loss: 83.32789611816406 = 0.22360853850841522 + 10.0 * 8.310428619384766
Epoch 1840, val loss: 0.40109625458717346
Epoch 1850, training loss: 83.32099151611328 = 0.22209829092025757 + 10.0 * 8.30988883972168
Epoch 1850, val loss: 0.40158534049987793
Epoch 1860, training loss: 83.32196044921875 = 0.22059477865695953 + 10.0 * 8.310136795043945
Epoch 1860, val loss: 0.402011513710022
Epoch 1870, training loss: 83.30648040771484 = 0.21909506618976593 + 10.0 * 8.308738708496094
Epoch 1870, val loss: 0.4023604989051819
Epoch 1880, training loss: 83.3016586303711 = 0.21760016679763794 + 10.0 * 8.308405876159668
Epoch 1880, val loss: 0.4029410481452942
Epoch 1890, training loss: 83.31665802001953 = 0.2161063551902771 + 10.0 * 8.31005573272705
Epoch 1890, val loss: 0.40329957008361816
Epoch 1900, training loss: 83.2893295288086 = 0.21460407972335815 + 10.0 * 8.307472229003906
Epoch 1900, val loss: 0.40362754464149475
Epoch 1910, training loss: 83.28740692138672 = 0.21310955286026 + 10.0 * 8.307429313659668
Epoch 1910, val loss: 0.40412819385528564
Epoch 1920, training loss: 83.28595733642578 = 0.21161888539791107 + 10.0 * 8.30743408203125
Epoch 1920, val loss: 0.404599130153656
Epoch 1930, training loss: 83.35093688964844 = 0.21013318002223969 + 10.0 * 8.314080238342285
Epoch 1930, val loss: 0.4051656424999237
Epoch 1940, training loss: 83.27810668945312 = 0.2086348682641983 + 10.0 * 8.306947708129883
Epoch 1940, val loss: 0.40561914443969727
Epoch 1950, training loss: 83.26947784423828 = 0.2071479856967926 + 10.0 * 8.306233406066895
Epoch 1950, val loss: 0.40620630979537964
Epoch 1960, training loss: 83.26657104492188 = 0.20566661655902863 + 10.0 * 8.306090354919434
Epoch 1960, val loss: 0.40660718083381653
Epoch 1970, training loss: 83.25884246826172 = 0.2041894793510437 + 10.0 * 8.305464744567871
Epoch 1970, val loss: 0.4072457551956177
Epoch 1980, training loss: 83.276611328125 = 0.20271340012550354 + 10.0 * 8.307390213012695
Epoch 1980, val loss: 0.40782588720321655
Epoch 1990, training loss: 83.26560974121094 = 0.20123407244682312 + 10.0 * 8.306437492370605
Epoch 1990, val loss: 0.4083598852157593
Epoch 2000, training loss: 83.25534057617188 = 0.19974985718727112 + 10.0 * 8.305559158325195
Epoch 2000, val loss: 0.4090271592140198
Epoch 2010, training loss: 83.25613403320312 = 0.19827663898468018 + 10.0 * 8.305785179138184
Epoch 2010, val loss: 0.4095591902732849
Epoch 2020, training loss: 83.27445983886719 = 0.196810781955719 + 10.0 * 8.307765007019043
Epoch 2020, val loss: 0.4101879298686981
Epoch 2030, training loss: 83.25846099853516 = 0.1953430026769638 + 10.0 * 8.30631160736084
Epoch 2030, val loss: 0.41091474890708923
Epoch 2040, training loss: 83.2488784790039 = 0.1938743144273758 + 10.0 * 8.305500984191895
Epoch 2040, val loss: 0.411703884601593
Epoch 2050, training loss: 83.2612533569336 = 0.1924128234386444 + 10.0 * 8.306883811950684
Epoch 2050, val loss: 0.4124611020088196
Epoch 2060, training loss: 83.24102020263672 = 0.19094966351985931 + 10.0 * 8.305006980895996
Epoch 2060, val loss: 0.41320905089378357
Epoch 2070, training loss: 83.23033142089844 = 0.18949000537395477 + 10.0 * 8.304083824157715
Epoch 2070, val loss: 0.41389691829681396
Epoch 2080, training loss: 83.2376937866211 = 0.18803799152374268 + 10.0 * 8.304965019226074
Epoch 2080, val loss: 0.4147120416164398
Epoch 2090, training loss: 83.26687622070312 = 0.18658655881881714 + 10.0 * 8.308029174804688
Epoch 2090, val loss: 0.41570013761520386
Epoch 2100, training loss: 83.2380142211914 = 0.18512795865535736 + 10.0 * 8.305288314819336
Epoch 2100, val loss: 0.41648590564727783
Epoch 2110, training loss: 83.24034881591797 = 0.18367350101470947 + 10.0 * 8.305667877197266
Epoch 2110, val loss: 0.4173583984375
Epoch 2120, training loss: 83.21454620361328 = 0.18221592903137207 + 10.0 * 8.30323314666748
Epoch 2120, val loss: 0.4180932641029358
Epoch 2130, training loss: 83.21377563476562 = 0.18076933920383453 + 10.0 * 8.303300857543945
Epoch 2130, val loss: 0.418998122215271
Epoch 2140, training loss: 83.21415710449219 = 0.1793297827243805 + 10.0 * 8.303483009338379
Epoch 2140, val loss: 0.4199640154838562
Epoch 2150, training loss: 83.23241424560547 = 0.17789441347122192 + 10.0 * 8.305452346801758
Epoch 2150, val loss: 0.42083606123924255
Epoch 2160, training loss: 83.20758056640625 = 0.1764531135559082 + 10.0 * 8.303112983703613
Epoch 2160, val loss: 0.42162883281707764
Epoch 2170, training loss: 83.22615814208984 = 0.17502596974372864 + 10.0 * 8.305112838745117
Epoch 2170, val loss: 0.4226081371307373
Epoch 2180, training loss: 83.20758819580078 = 0.17359809577465057 + 10.0 * 8.303399085998535
Epoch 2180, val loss: 0.42352616786956787
Epoch 2190, training loss: 83.19294738769531 = 0.1721717119216919 + 10.0 * 8.302077293395996
Epoch 2190, val loss: 0.4244820475578308
Epoch 2200, training loss: 83.1867446899414 = 0.17075736820697784 + 10.0 * 8.30159854888916
Epoch 2200, val loss: 0.42541903257369995
Epoch 2210, training loss: 83.20414733886719 = 0.1693539023399353 + 10.0 * 8.303479194641113
Epoch 2210, val loss: 0.4262833297252655
Epoch 2220, training loss: 83.20215606689453 = 0.16796176135540009 + 10.0 * 8.30341911315918
Epoch 2220, val loss: 0.4274802803993225
Epoch 2230, training loss: 83.1846694946289 = 0.16655752062797546 + 10.0 * 8.301811218261719
Epoch 2230, val loss: 0.42841604351997375
Epoch 2240, training loss: 83.17597198486328 = 0.16516846418380737 + 10.0 * 8.301080703735352
Epoch 2240, val loss: 0.42955803871154785
Epoch 2250, training loss: 83.18750762939453 = 0.16378657519817352 + 10.0 * 8.302371978759766
Epoch 2250, val loss: 0.43052539229393005
Epoch 2260, training loss: 83.19561767578125 = 0.1624108850955963 + 10.0 * 8.303319931030273
Epoch 2260, val loss: 0.4316052794456482
Epoch 2270, training loss: 83.17578887939453 = 0.1610320508480072 + 10.0 * 8.301475524902344
Epoch 2270, val loss: 0.4326142966747284
Epoch 2280, training loss: 83.17182159423828 = 0.15966512262821198 + 10.0 * 8.301215171813965
Epoch 2280, val loss: 0.43369635939598083
Epoch 2290, training loss: 83.19159698486328 = 0.1583070605993271 + 10.0 * 8.303328514099121
Epoch 2290, val loss: 0.43492239713668823
Epoch 2300, training loss: 83.20057678222656 = 0.15694788098335266 + 10.0 * 8.304363250732422
Epoch 2300, val loss: 0.4360485076904297
Epoch 2310, training loss: 83.16983032226562 = 0.15557926893234253 + 10.0 * 8.301424980163574
Epoch 2310, val loss: 0.4369666278362274
Epoch 2320, training loss: 83.17377471923828 = 0.1542290896177292 + 10.0 * 8.30195426940918
Epoch 2320, val loss: 0.4380320608615875
Epoch 2330, training loss: 83.15864562988281 = 0.15288525819778442 + 10.0 * 8.300576210021973
Epoch 2330, val loss: 0.43934062123298645
Epoch 2340, training loss: 83.14974212646484 = 0.1515478938817978 + 10.0 * 8.299818992614746
Epoch 2340, val loss: 0.44058313965797424
Epoch 2350, training loss: 83.13932800292969 = 0.1502181440591812 + 10.0 * 8.298911094665527
Epoch 2350, val loss: 0.44172948598861694
Epoch 2360, training loss: 83.16366577148438 = 0.14889514446258545 + 10.0 * 8.301477432250977
Epoch 2360, val loss: 0.44291335344314575
Epoch 2370, training loss: 83.15679931640625 = 0.14757584035396576 + 10.0 * 8.300922393798828
Epoch 2370, val loss: 0.4442700445652008
Epoch 2380, training loss: 83.15422058105469 = 0.14623773097991943 + 10.0 * 8.300798416137695
Epoch 2380, val loss: 0.4454914629459381
Epoch 2390, training loss: 83.14360046386719 = 0.14491121470928192 + 10.0 * 8.2998685836792
Epoch 2390, val loss: 0.44663336873054504
Epoch 2400, training loss: 83.12950134277344 = 0.14358805119991302 + 10.0 * 8.298591613769531
Epoch 2400, val loss: 0.44783517718315125
Epoch 2410, training loss: 83.14219665527344 = 0.14227887988090515 + 10.0 * 8.299991607666016
Epoch 2410, val loss: 0.44909441471099854
Epoch 2420, training loss: 83.1373291015625 = 0.1409744918346405 + 10.0 * 8.29963493347168
Epoch 2420, val loss: 0.45033523440361023
Epoch 2430, training loss: 83.14075469970703 = 0.13967320322990417 + 10.0 * 8.300107955932617
Epoch 2430, val loss: 0.4517216086387634
Epoch 2440, training loss: 83.17239379882812 = 0.13838568329811096 + 10.0 * 8.303400993347168
Epoch 2440, val loss: 0.4528575539588928
Epoch 2450, training loss: 83.12677764892578 = 0.1370922476053238 + 10.0 * 8.298968315124512
Epoch 2450, val loss: 0.4544543921947479
Epoch 2460, training loss: 83.11116027832031 = 0.13580474257469177 + 10.0 * 8.29753589630127
Epoch 2460, val loss: 0.4556558132171631
Epoch 2470, training loss: 83.11591339111328 = 0.13452637195587158 + 10.0 * 8.298138618469238
Epoch 2470, val loss: 0.456951379776001
Epoch 2480, training loss: 83.18953704833984 = 0.13326455652713776 + 10.0 * 8.305627822875977
Epoch 2480, val loss: 0.4582189917564392
Epoch 2490, training loss: 83.1234359741211 = 0.13198211789131165 + 10.0 * 8.299145698547363
Epoch 2490, val loss: 0.45972099900245667
Epoch 2500, training loss: 83.10424041748047 = 0.13069698214530945 + 10.0 * 8.297353744506836
Epoch 2500, val loss: 0.4608439803123474
Epoch 2510, training loss: 83.09851837158203 = 0.12942638993263245 + 10.0 * 8.29690933227539
Epoch 2510, val loss: 0.4622807502746582
Epoch 2520, training loss: 83.14781188964844 = 0.12817996740341187 + 10.0 * 8.301962852478027
Epoch 2520, val loss: 0.4638727903366089
Epoch 2530, training loss: 83.09564208984375 = 0.12690381705760956 + 10.0 * 8.296873092651367
Epoch 2530, val loss: 0.46468237042427063
Epoch 2540, training loss: 83.08819580078125 = 0.1256527304649353 + 10.0 * 8.29625415802002
Epoch 2540, val loss: 0.4664188623428345
Epoch 2550, training loss: 83.08768463134766 = 0.12441069632768631 + 10.0 * 8.296327590942383
Epoch 2550, val loss: 0.46768951416015625
Epoch 2560, training loss: 83.12857055664062 = 0.12318743765354156 + 10.0 * 8.300539016723633
Epoch 2560, val loss: 0.4691956043243408
Epoch 2570, training loss: 83.0893325805664 = 0.12196452915668488 + 10.0 * 8.296736717224121
Epoch 2570, val loss: 0.4706735908985138
Epoch 2580, training loss: 83.09080505371094 = 0.12074168771505356 + 10.0 * 8.297006607055664
Epoch 2580, val loss: 0.4720652997493744
Epoch 2590, training loss: 83.11900329589844 = 0.11953442543745041 + 10.0 * 8.299946784973145
Epoch 2590, val loss: 0.473603755235672
Epoch 2600, training loss: 83.08759307861328 = 0.11832303553819656 + 10.0 * 8.296926498413086
Epoch 2600, val loss: 0.47488510608673096
Epoch 2610, training loss: 83.07289123535156 = 0.11712206155061722 + 10.0 * 8.295577049255371
Epoch 2610, val loss: 0.4765622615814209
Epoch 2620, training loss: 83.07398986816406 = 0.11592917144298553 + 10.0 * 8.295805931091309
Epoch 2620, val loss: 0.4781534671783447
Epoch 2630, training loss: 83.08326721191406 = 0.11474812775850296 + 10.0 * 8.296852111816406
Epoch 2630, val loss: 0.4794650673866272
Epoch 2640, training loss: 83.07714080810547 = 0.11356671899557114 + 10.0 * 8.296358108520508
Epoch 2640, val loss: 0.48125311732292175
Epoch 2650, training loss: 83.0783462524414 = 0.11239036172628403 + 10.0 * 8.296595573425293
Epoch 2650, val loss: 0.48295560479164124
Epoch 2660, training loss: 83.08326721191406 = 0.11122103780508041 + 10.0 * 8.297204971313477
Epoch 2660, val loss: 0.48439574241638184
Epoch 2670, training loss: 83.08396911621094 = 0.11005832999944687 + 10.0 * 8.297390937805176
Epoch 2670, val loss: 0.4863021969795227
Epoch 2680, training loss: 83.0585708618164 = 0.10888791084289551 + 10.0 * 8.294968605041504
Epoch 2680, val loss: 0.48755785822868347
Epoch 2690, training loss: 83.05870819091797 = 0.10773427039384842 + 10.0 * 8.295097351074219
Epoch 2690, val loss: 0.48912009596824646
Epoch 2700, training loss: 83.06876373291016 = 0.10658804327249527 + 10.0 * 8.29621696472168
Epoch 2700, val loss: 0.49091780185699463
Epoch 2710, training loss: 83.06017303466797 = 0.10544522106647491 + 10.0 * 8.295473098754883
Epoch 2710, val loss: 0.49244222044944763
Epoch 2720, training loss: 83.05673217773438 = 0.1043015718460083 + 10.0 * 8.295243263244629
Epoch 2720, val loss: 0.49416765570640564
Epoch 2730, training loss: 83.04617309570312 = 0.1031697615981102 + 10.0 * 8.294300079345703
Epoch 2730, val loss: 0.49597471952438354
Epoch 2740, training loss: 83.06855773925781 = 0.1020493358373642 + 10.0 * 8.296650886535645
Epoch 2740, val loss: 0.49775639176368713
Epoch 2750, training loss: 83.05321502685547 = 0.10093475133180618 + 10.0 * 8.295228004455566
Epoch 2750, val loss: 0.49937203526496887
Epoch 2760, training loss: 83.04991149902344 = 0.09982632845640182 + 10.0 * 8.295008659362793
Epoch 2760, val loss: 0.5012390613555908
Epoch 2770, training loss: 83.04553985595703 = 0.09872829914093018 + 10.0 * 8.29468059539795
Epoch 2770, val loss: 0.5031775832176208
Epoch 2780, training loss: 83.08658599853516 = 0.09764882177114487 + 10.0 * 8.298893928527832
Epoch 2780, val loss: 0.5051197409629822
Epoch 2790, training loss: 83.0357894897461 = 0.09655070304870605 + 10.0 * 8.293924331665039
Epoch 2790, val loss: 0.5067355632781982
Epoch 2800, training loss: 83.0271224975586 = 0.0954650267958641 + 10.0 * 8.29316520690918
Epoch 2800, val loss: 0.5087255239486694
Epoch 2810, training loss: 83.02083587646484 = 0.094392329454422 + 10.0 * 8.292644500732422
Epoch 2810, val loss: 0.5105298161506653
Epoch 2820, training loss: 83.01549530029297 = 0.09332717955112457 + 10.0 * 8.292216300964355
Epoch 2820, val loss: 0.51234370470047
Epoch 2830, training loss: 83.02384948730469 = 0.0922718495130539 + 10.0 * 8.293157577514648
Epoch 2830, val loss: 0.5140618085861206
Epoch 2840, training loss: 83.05877685546875 = 0.09123679995536804 + 10.0 * 8.296753883361816
Epoch 2840, val loss: 0.5157477259635925
Epoch 2850, training loss: 83.07012176513672 = 0.09018683433532715 + 10.0 * 8.297993659973145
Epoch 2850, val loss: 0.5182189345359802
Epoch 2860, training loss: 83.03022766113281 = 0.08914165943861008 + 10.0 * 8.294108390808105
Epoch 2860, val loss: 0.5199193358421326
Epoch 2870, training loss: 83.01509857177734 = 0.08811173588037491 + 10.0 * 8.292698860168457
Epoch 2870, val loss: 0.5221092700958252
Epoch 2880, training loss: 83.0057144165039 = 0.08709206432104111 + 10.0 * 8.291862487792969
Epoch 2880, val loss: 0.5239889621734619
Epoch 2890, training loss: 83.00719451904297 = 0.0860803946852684 + 10.0 * 8.29211139678955
Epoch 2890, val loss: 0.5258281230926514
Epoch 2900, training loss: 83.0383071899414 = 0.0850878655910492 + 10.0 * 8.295321464538574
Epoch 2900, val loss: 0.5278193950653076
Epoch 2910, training loss: 83.0438461303711 = 0.08409248292446136 + 10.0 * 8.295975685119629
Epoch 2910, val loss: 0.529854953289032
Epoch 2920, training loss: 82.99794006347656 = 0.08309601247310638 + 10.0 * 8.291483879089355
Epoch 2920, val loss: 0.5318604111671448
Epoch 2930, training loss: 82.99322509765625 = 0.08210770040750504 + 10.0 * 8.291111946105957
Epoch 2930, val loss: 0.5338959097862244
Epoch 2940, training loss: 82.98817443847656 = 0.08113518357276917 + 10.0 * 8.290703773498535
Epoch 2940, val loss: 0.5361818671226501
Epoch 2950, training loss: 82.99067687988281 = 0.0801728144288063 + 10.0 * 8.291050910949707
Epoch 2950, val loss: 0.5378898978233337
Epoch 2960, training loss: 83.05545043945312 = 0.07924199849367142 + 10.0 * 8.29762077331543
Epoch 2960, val loss: 0.5398726463317871
Epoch 2970, training loss: 83.0121078491211 = 0.07828336209058762 + 10.0 * 8.29338264465332
Epoch 2970, val loss: 0.5426288843154907
Epoch 2980, training loss: 82.99152374267578 = 0.0773310512304306 + 10.0 * 8.29141902923584
Epoch 2980, val loss: 0.5442352890968323
Epoch 2990, training loss: 82.98082733154297 = 0.07638590037822723 + 10.0 * 8.290444374084473
Epoch 2990, val loss: 0.5467962622642517
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8401826484018264
0.866550749836992
=== training gcn model ===
Epoch 0, training loss: 106.91801452636719 = 1.095306158065796 + 10.0 * 10.582270622253418
Epoch 0, val loss: 1.096039056777954
Epoch 10, training loss: 106.90272521972656 = 1.0884764194488525 + 10.0 * 10.581424713134766
Epoch 10, val loss: 1.0891536474227905
Epoch 20, training loss: 106.82081604003906 = 1.0803947448730469 + 10.0 * 10.574042320251465
Epoch 20, val loss: 1.0809274911880493
Epoch 30, training loss: 106.28558349609375 = 1.070806860923767 + 10.0 * 10.521477699279785
Epoch 30, val loss: 1.0711719989776611
Epoch 40, training loss: 103.9256820678711 = 1.0604008436203003 + 10.0 * 10.286527633666992
Epoch 40, val loss: 1.06072998046875
Epoch 50, training loss: 97.59911346435547 = 1.0499510765075684 + 10.0 * 9.654916763305664
Epoch 50, val loss: 1.0498943328857422
Epoch 60, training loss: 95.87403869628906 = 1.0399502515792847 + 10.0 * 9.48340892791748
Epoch 60, val loss: 1.039937973022461
Epoch 70, training loss: 94.30754852294922 = 1.0295932292938232 + 10.0 * 9.327795028686523
Epoch 70, val loss: 1.0295549631118774
Epoch 80, training loss: 93.26786804199219 = 1.0178993940353394 + 10.0 * 9.224996566772461
Epoch 80, val loss: 1.0177935361862183
Epoch 90, training loss: 93.00712585449219 = 1.0047032833099365 + 10.0 * 9.20024299621582
Epoch 90, val loss: 1.0046272277832031
Epoch 100, training loss: 92.70513916015625 = 0.9915908575057983 + 10.0 * 9.171354293823242
Epoch 100, val loss: 0.9916552901268005
Epoch 110, training loss: 92.35086059570312 = 0.9795602560043335 + 10.0 * 9.137129783630371
Epoch 110, val loss: 0.9797466993331909
Epoch 120, training loss: 91.88018798828125 = 0.967558741569519 + 10.0 * 9.091262817382812
Epoch 120, val loss: 0.9676773548126221
Epoch 130, training loss: 91.14148712158203 = 0.9547512531280518 + 10.0 * 9.01867389678955
Epoch 130, val loss: 0.9548636674880981
Epoch 140, training loss: 90.18319702148438 = 0.9428244829177856 + 10.0 * 8.924036979675293
Epoch 140, val loss: 0.943044126033783
Epoch 150, training loss: 89.54122924804688 = 0.9312357902526855 + 10.0 * 8.86099910736084
Epoch 150, val loss: 0.9312423467636108
Epoch 160, training loss: 88.98312377929688 = 0.9177486300468445 + 10.0 * 8.806537628173828
Epoch 160, val loss: 0.9174910187721252
Epoch 170, training loss: 88.6790771484375 = 0.9002895355224609 + 10.0 * 8.777878761291504
Epoch 170, val loss: 0.8997235298156738
Epoch 180, training loss: 88.31827545166016 = 0.8802648186683655 + 10.0 * 8.74380111694336
Epoch 180, val loss: 0.8802260756492615
Epoch 190, training loss: 88.14384460449219 = 0.8599738478660583 + 10.0 * 8.728387832641602
Epoch 190, val loss: 0.8605973720550537
Epoch 200, training loss: 88.01213836669922 = 0.8376168012619019 + 10.0 * 8.717452049255371
Epoch 200, val loss: 0.8389221429824829
Epoch 210, training loss: 87.87371826171875 = 0.8140242695808411 + 10.0 * 8.705968856811523
Epoch 210, val loss: 0.8160813450813293
Epoch 220, training loss: 87.71949005126953 = 0.7907302975654602 + 10.0 * 8.692875862121582
Epoch 220, val loss: 0.793666422367096
Epoch 230, training loss: 87.55914306640625 = 0.7679528594017029 + 10.0 * 8.679119110107422
Epoch 230, val loss: 0.7720277905464172
Epoch 240, training loss: 87.3255844116211 = 0.7458011507987976 + 10.0 * 8.657978057861328
Epoch 240, val loss: 0.7508775591850281
Epoch 250, training loss: 87.1385726928711 = 0.7237597107887268 + 10.0 * 8.641481399536133
Epoch 250, val loss: 0.7298719882965088
Epoch 260, training loss: 87.00825500488281 = 0.7011920213699341 + 10.0 * 8.630705833435059
Epoch 260, val loss: 0.7084673643112183
Epoch 270, training loss: 86.88325500488281 = 0.6787130236625671 + 10.0 * 8.620454788208008
Epoch 270, val loss: 0.6874305605888367
Epoch 280, training loss: 86.72252655029297 = 0.6575798392295837 + 10.0 * 8.606494903564453
Epoch 280, val loss: 0.667926549911499
Epoch 290, training loss: 86.55516815185547 = 0.6381638646125793 + 10.0 * 8.591700553894043
Epoch 290, val loss: 0.649957001209259
Epoch 300, training loss: 86.44869995117188 = 0.6199020743370056 + 10.0 * 8.582880020141602
Epoch 300, val loss: 0.6331616044044495
Epoch 310, training loss: 86.27738189697266 = 0.6021702885627747 + 10.0 * 8.567521095275879
Epoch 310, val loss: 0.617087185382843
Epoch 320, training loss: 86.12348175048828 = 0.5853226184844971 + 10.0 * 8.553815841674805
Epoch 320, val loss: 0.6019703149795532
Epoch 330, training loss: 86.01297760009766 = 0.5694712400436401 + 10.0 * 8.544350624084473
Epoch 330, val loss: 0.5879157781600952
Epoch 340, training loss: 85.86688232421875 = 0.554681122303009 + 10.0 * 8.531220436096191
Epoch 340, val loss: 0.5749058723449707
Epoch 350, training loss: 85.72954559326172 = 0.5409374833106995 + 10.0 * 8.518860816955566
Epoch 350, val loss: 0.5629421472549438
Epoch 360, training loss: 85.69242858886719 = 0.5278031229972839 + 10.0 * 8.516462326049805
Epoch 360, val loss: 0.5518597364425659
Epoch 370, training loss: 85.5631332397461 = 0.5151045918464661 + 10.0 * 8.504802703857422
Epoch 370, val loss: 0.5410680770874023
Epoch 380, training loss: 85.42778015136719 = 0.5034635066986084 + 10.0 * 8.492431640625
Epoch 380, val loss: 0.5314069986343384
Epoch 390, training loss: 85.34751892089844 = 0.4926488697528839 + 10.0 * 8.48548698425293
Epoch 390, val loss: 0.5225368738174438
Epoch 400, training loss: 85.2709732055664 = 0.4825208783149719 + 10.0 * 8.478845596313477
Epoch 400, val loss: 0.5144461989402771
Epoch 410, training loss: 85.31131744384766 = 0.47302699089050293 + 10.0 * 8.4838285446167
Epoch 410, val loss: 0.506976306438446
Epoch 420, training loss: 85.15589904785156 = 0.4641169309616089 + 10.0 * 8.469178199768066
Epoch 420, val loss: 0.5001121759414673
Epoch 430, training loss: 85.0879898071289 = 0.45602479577064514 + 10.0 * 8.463196754455566
Epoch 430, val loss: 0.4939597547054291
Epoch 440, training loss: 85.03327178955078 = 0.44861704111099243 + 10.0 * 8.458465576171875
Epoch 440, val loss: 0.48846083879470825
Epoch 450, training loss: 84.99805450439453 = 0.4416944086551666 + 10.0 * 8.455636024475098
Epoch 450, val loss: 0.48347756266593933
Epoch 460, training loss: 84.97856140136719 = 0.43514230847358704 + 10.0 * 8.454341888427734
Epoch 460, val loss: 0.4787861704826355
Epoch 470, training loss: 84.90634155273438 = 0.4291258752346039 + 10.0 * 8.447721481323242
Epoch 470, val loss: 0.47457993030548096
Epoch 480, training loss: 84.85081481933594 = 0.4236193299293518 + 10.0 * 8.442719459533691
Epoch 480, val loss: 0.47083964943885803
Epoch 490, training loss: 84.82821655273438 = 0.41847124695777893 + 10.0 * 8.440974235534668
Epoch 490, val loss: 0.4675302803516388
Epoch 500, training loss: 84.8057861328125 = 0.4134998619556427 + 10.0 * 8.439229011535645
Epoch 500, val loss: 0.4641174376010895
Epoch 510, training loss: 84.73387908935547 = 0.4089656174182892 + 10.0 * 8.432491302490234
Epoch 510, val loss: 0.46128982305526733
Epoch 520, training loss: 84.69630432128906 = 0.4047979414463043 + 10.0 * 8.429150581359863
Epoch 520, val loss: 0.45864802598953247
Epoch 530, training loss: 84.65768432617188 = 0.4008592963218689 + 10.0 * 8.42568302154541
Epoch 530, val loss: 0.45622262358665466
Epoch 540, training loss: 84.67670440673828 = 0.39714229106903076 + 10.0 * 8.427956581115723
Epoch 540, val loss: 0.45392128825187683
Epoch 550, training loss: 84.60597229003906 = 0.3935154974460602 + 10.0 * 8.421245574951172
Epoch 550, val loss: 0.45189231634140015
Epoch 560, training loss: 84.56365966796875 = 0.39018720388412476 + 10.0 * 8.417346954345703
Epoch 560, val loss: 0.44994568824768066
Epoch 570, training loss: 84.52854919433594 = 0.38707613945007324 + 10.0 * 8.41414737701416
Epoch 570, val loss: 0.4481099247932434
Epoch 580, training loss: 84.53296661376953 = 0.38408681750297546 + 10.0 * 8.414888381958008
Epoch 580, val loss: 0.44647568464279175
Epoch 590, training loss: 84.50151824951172 = 0.38116025924682617 + 10.0 * 8.412035942077637
Epoch 590, val loss: 0.4448351562023163
Epoch 600, training loss: 84.4535903930664 = 0.3784390389919281 + 10.0 * 8.407514572143555
Epoch 600, val loss: 0.443386048078537
Epoch 610, training loss: 84.41764068603516 = 0.3758542537689209 + 10.0 * 8.404178619384766
Epoch 610, val loss: 0.4419572353363037
Epoch 620, training loss: 84.39222717285156 = 0.37335067987442017 + 10.0 * 8.401887893676758
Epoch 620, val loss: 0.440651535987854
Epoch 630, training loss: 84.43417358398438 = 0.37092703580856323 + 10.0 * 8.40632438659668
Epoch 630, val loss: 0.4393720328807831
Epoch 640, training loss: 84.3803482055664 = 0.3685525357723236 + 10.0 * 8.401179313659668
Epoch 640, val loss: 0.4382951557636261
Epoch 650, training loss: 84.33435821533203 = 0.36629658937454224 + 10.0 * 8.396806716918945
Epoch 650, val loss: 0.43710216879844666
Epoch 660, training loss: 84.31456756591797 = 0.36413586139678955 + 10.0 * 8.39504337310791
Epoch 660, val loss: 0.43600553274154663
Epoch 670, training loss: 84.28617095947266 = 0.36205971240997314 + 10.0 * 8.392411231994629
Epoch 670, val loss: 0.4350375533103943
Epoch 680, training loss: 84.28010559082031 = 0.360044926404953 + 10.0 * 8.392005920410156
Epoch 680, val loss: 0.43415793776512146
Epoch 690, training loss: 84.26325225830078 = 0.3580414950847626 + 10.0 * 8.390521049499512
Epoch 690, val loss: 0.433108925819397
Epoch 700, training loss: 84.2564468383789 = 0.3561021685600281 + 10.0 * 8.390034675598145
Epoch 700, val loss: 0.43235650658607483
Epoch 710, training loss: 84.21852111816406 = 0.35426053404808044 + 10.0 * 8.386425971984863
Epoch 710, val loss: 0.431475430727005
Epoch 720, training loss: 84.20494079589844 = 0.352477490901947 + 10.0 * 8.385246276855469
Epoch 720, val loss: 0.43065959215164185
Epoch 730, training loss: 84.28132629394531 = 0.3507198989391327 + 10.0 * 8.393060684204102
Epoch 730, val loss: 0.4299834966659546
Epoch 740, training loss: 84.17731475830078 = 0.3489706218242645 + 10.0 * 8.382834434509277
Epoch 740, val loss: 0.4291563928127289
Epoch 750, training loss: 84.16342163085938 = 0.3473171591758728 + 10.0 * 8.381609916687012
Epoch 750, val loss: 0.4284481406211853
Epoch 760, training loss: 84.14207458496094 = 0.3456994593143463 + 10.0 * 8.379636764526367
Epoch 760, val loss: 0.427813321352005
Epoch 770, training loss: 84.14250183105469 = 0.34411394596099854 + 10.0 * 8.379838943481445
Epoch 770, val loss: 0.42718151211738586
Epoch 780, training loss: 84.15399169921875 = 0.34249675273895264 + 10.0 * 8.381149291992188
Epoch 780, val loss: 0.4265236258506775
Epoch 790, training loss: 84.10792541503906 = 0.34092411398887634 + 10.0 * 8.376699447631836
Epoch 790, val loss: 0.42595288157463074
Epoch 800, training loss: 84.0897445678711 = 0.3394390046596527 + 10.0 * 8.375030517578125
Epoch 800, val loss: 0.42526936531066895
Epoch 810, training loss: 84.07550048828125 = 0.3379842936992645 + 10.0 * 8.373751640319824
Epoch 810, val loss: 0.4247035086154938
Epoch 820, training loss: 84.05984497070312 = 0.33655157685279846 + 10.0 * 8.372329711914062
Epoch 820, val loss: 0.424187570810318
Epoch 830, training loss: 84.04672241210938 = 0.3351322412490845 + 10.0 * 8.371159553527832
Epoch 830, val loss: 0.4236375689506531
Epoch 840, training loss: 84.10783386230469 = 0.33372169733047485 + 10.0 * 8.377410888671875
Epoch 840, val loss: 0.42302054166793823
Epoch 850, training loss: 84.05863189697266 = 0.33228665590286255 + 10.0 * 8.372634887695312
Epoch 850, val loss: 0.4225899577140808
Epoch 860, training loss: 84.02796173095703 = 0.3309091329574585 + 10.0 * 8.369705200195312
Epoch 860, val loss: 0.42209625244140625
Epoch 870, training loss: 84.00555419921875 = 0.3295671343803406 + 10.0 * 8.367598533630371
Epoch 870, val loss: 0.42161402106285095
Epoch 880, training loss: 84.04695892333984 = 0.3282430171966553 + 10.0 * 8.371871948242188
Epoch 880, val loss: 0.4212072789669037
Epoch 890, training loss: 84.00878143310547 = 0.3269024193286896 + 10.0 * 8.36818790435791
Epoch 890, val loss: 0.42064228653907776
Epoch 900, training loss: 83.97208404541016 = 0.32560229301452637 + 10.0 * 8.364648818969727
Epoch 900, val loss: 0.4202282130718231
Epoch 910, training loss: 83.97010040283203 = 0.3243291676044464 + 10.0 * 8.364577293395996
Epoch 910, val loss: 0.41983166337013245
Epoch 920, training loss: 83.98001861572266 = 0.3230549097061157 + 10.0 * 8.365696907043457
Epoch 920, val loss: 0.4193916618824005
Epoch 930, training loss: 83.94773864746094 = 0.3217937648296356 + 10.0 * 8.362594604492188
Epoch 930, val loss: 0.4189300835132599
Epoch 940, training loss: 83.93201446533203 = 0.3205641806125641 + 10.0 * 8.36114501953125
Epoch 940, val loss: 0.41856786608695984
Epoch 950, training loss: 83.9419937133789 = 0.31934624910354614 + 10.0 * 8.362264633178711
Epoch 950, val loss: 0.418220192193985
Epoch 960, training loss: 83.9374008178711 = 0.31811070442199707 + 10.0 * 8.361928939819336
Epoch 960, val loss: 0.41774117946624756
Epoch 970, training loss: 83.90801239013672 = 0.3169008493423462 + 10.0 * 8.359110832214355
Epoch 970, val loss: 0.41740790009498596
Epoch 980, training loss: 83.89314270019531 = 0.315716952085495 + 10.0 * 8.357742309570312
Epoch 980, val loss: 0.4170580506324768
Epoch 990, training loss: 83.9274673461914 = 0.314534455537796 + 10.0 * 8.361292839050293
Epoch 990, val loss: 0.41666173934936523
Epoch 1000, training loss: 83.91112518310547 = 0.3133420944213867 + 10.0 * 8.35977840423584
Epoch 1000, val loss: 0.41631752252578735
Epoch 1010, training loss: 83.86663055419922 = 0.3121829628944397 + 10.0 * 8.35544490814209
Epoch 1010, val loss: 0.4159887135028839
Epoch 1020, training loss: 83.85317993164062 = 0.3110448718070984 + 10.0 * 8.35421371459961
Epoch 1020, val loss: 0.4156626760959625
Epoch 1030, training loss: 83.84475708007812 = 0.3099218010902405 + 10.0 * 8.353483200073242
Epoch 1030, val loss: 0.4153449237346649
Epoch 1040, training loss: 83.8665771484375 = 0.30880412459373474 + 10.0 * 8.3557767868042
Epoch 1040, val loss: 0.41496750712394714
Epoch 1050, training loss: 83.85320281982422 = 0.30766037106513977 + 10.0 * 8.354554176330566
Epoch 1050, val loss: 0.4147781431674957
Epoch 1060, training loss: 83.83802032470703 = 0.30654454231262207 + 10.0 * 8.353147506713867
Epoch 1060, val loss: 0.41437411308288574
Epoch 1070, training loss: 83.81145477294922 = 0.30544763803482056 + 10.0 * 8.350600242614746
Epoch 1070, val loss: 0.41410091519355774
Epoch 1080, training loss: 83.8089828491211 = 0.30436235666275024 + 10.0 * 8.350461959838867
Epoch 1080, val loss: 0.4138874411582947
Epoch 1090, training loss: 83.8519058227539 = 0.30327707529067993 + 10.0 * 8.354863166809082
Epoch 1090, val loss: 0.41366350650787354
Epoch 1100, training loss: 83.79986572265625 = 0.30217304825782776 + 10.0 * 8.349769592285156
Epoch 1100, val loss: 0.413213312625885
Epoch 1110, training loss: 83.79533386230469 = 0.3011053502559662 + 10.0 * 8.3494234085083
Epoch 1110, val loss: 0.41292381286621094
Epoch 1120, training loss: 83.77890014648438 = 0.3000456392765045 + 10.0 * 8.347885131835938
Epoch 1120, val loss: 0.4126977324485779
Epoch 1130, training loss: 83.76499938964844 = 0.2990049421787262 + 10.0 * 8.346599578857422
Epoch 1130, val loss: 0.41238123178482056
Epoch 1140, training loss: 83.76861572265625 = 0.29796913266181946 + 10.0 * 8.347064971923828
Epoch 1140, val loss: 0.41217365860939026
Epoch 1150, training loss: 83.78257751464844 = 0.29692336916923523 + 10.0 * 8.348566055297852
Epoch 1150, val loss: 0.4119802415370941
Epoch 1160, training loss: 83.75997161865234 = 0.2958734631538391 + 10.0 * 8.346409797668457
Epoch 1160, val loss: 0.4116014540195465
Epoch 1170, training loss: 83.74642181396484 = 0.294842392206192 + 10.0 * 8.345157623291016
Epoch 1170, val loss: 0.4113883078098297
Epoch 1180, training loss: 83.73094940185547 = 0.29383420944213867 + 10.0 * 8.343711853027344
Epoch 1180, val loss: 0.41115453839302063
Epoch 1190, training loss: 83.7314682006836 = 0.2928270995616913 + 10.0 * 8.343864440917969
Epoch 1190, val loss: 0.41092371940612793
Epoch 1200, training loss: 83.76425170898438 = 0.29181331396102905 + 10.0 * 8.347243309020996
Epoch 1200, val loss: 0.4106714129447937
Epoch 1210, training loss: 83.7267074584961 = 0.2907930016517639 + 10.0 * 8.343591690063477
Epoch 1210, val loss: 0.4104275107383728
Epoch 1220, training loss: 83.70585632324219 = 0.2897912859916687 + 10.0 * 8.341606140136719
Epoch 1220, val loss: 0.41022080183029175
Epoch 1230, training loss: 83.69166564941406 = 0.28879809379577637 + 10.0 * 8.340287208557129
Epoch 1230, val loss: 0.4099877178668976
Epoch 1240, training loss: 83.71089935302734 = 0.2878076136112213 + 10.0 * 8.34230899810791
Epoch 1240, val loss: 0.40974733233451843
Epoch 1250, training loss: 83.70468139648438 = 0.28679758310317993 + 10.0 * 8.341788291931152
Epoch 1250, val loss: 0.4094946086406708
Epoch 1260, training loss: 83.6924819946289 = 0.2857968509197235 + 10.0 * 8.340668678283691
Epoch 1260, val loss: 0.409262090921402
Epoch 1270, training loss: 83.67770385742188 = 0.284808874130249 + 10.0 * 8.339289665222168
Epoch 1270, val loss: 0.4090632498264313
Epoch 1280, training loss: 83.68003845214844 = 0.28382641077041626 + 10.0 * 8.339620590209961
Epoch 1280, val loss: 0.40882524847984314
Epoch 1290, training loss: 83.66842651367188 = 0.28283438086509705 + 10.0 * 8.3385591506958
Epoch 1290, val loss: 0.4086363911628723
Epoch 1300, training loss: 83.65508270263672 = 0.2818550765514374 + 10.0 * 8.337323188781738
Epoch 1300, val loss: 0.40850284695625305
Epoch 1310, training loss: 83.6554946899414 = 0.2808758020401001 + 10.0 * 8.337461471557617
Epoch 1310, val loss: 0.4082931578159332
Epoch 1320, training loss: 83.65326690673828 = 0.27989301085472107 + 10.0 * 8.337337493896484
Epoch 1320, val loss: 0.4080010652542114
Epoch 1330, training loss: 83.68223571777344 = 0.2789114713668823 + 10.0 * 8.340332984924316
Epoch 1330, val loss: 0.4077339768409729
Epoch 1340, training loss: 83.6434097290039 = 0.27792176604270935 + 10.0 * 8.336548805236816
Epoch 1340, val loss: 0.4076579809188843
Epoch 1350, training loss: 83.63616180419922 = 0.2769424617290497 + 10.0 * 8.335922241210938
Epoch 1350, val loss: 0.4074251651763916
Epoch 1360, training loss: 83.61331176757812 = 0.27596673369407654 + 10.0 * 8.333734512329102
Epoch 1360, val loss: 0.4072047173976898
Epoch 1370, training loss: 83.61197662353516 = 0.27499690651893616 + 10.0 * 8.333698272705078
Epoch 1370, val loss: 0.4070437550544739
Epoch 1380, training loss: 83.64205169677734 = 0.274018257856369 + 10.0 * 8.336803436279297
Epoch 1380, val loss: 0.406846821308136
Epoch 1390, training loss: 83.62083435058594 = 0.27302905917167664 + 10.0 * 8.3347806930542
Epoch 1390, val loss: 0.40662500262260437
Epoch 1400, training loss: 83.59724426269531 = 0.27204129099845886 + 10.0 * 8.332520484924316
Epoch 1400, val loss: 0.4064570963382721
Epoch 1410, training loss: 83.58641815185547 = 0.2710610330104828 + 10.0 * 8.331535339355469
Epoch 1410, val loss: 0.4063287377357483
Epoch 1420, training loss: 83.59307861328125 = 0.2700795531272888 + 10.0 * 8.332300186157227
Epoch 1420, val loss: 0.4061605930328369
Epoch 1430, training loss: 83.56743621826172 = 0.26909542083740234 + 10.0 * 8.329833984375
Epoch 1430, val loss: 0.40594518184661865
Epoch 1440, training loss: 83.59019470214844 = 0.2681139409542084 + 10.0 * 8.332208633422852
Epoch 1440, val loss: 0.40574973821640015
Epoch 1450, training loss: 83.58956146240234 = 0.26711153984069824 + 10.0 * 8.332244873046875
Epoch 1450, val loss: 0.40567877888679504
Epoch 1460, training loss: 83.55865478515625 = 0.2661152482032776 + 10.0 * 8.329254150390625
Epoch 1460, val loss: 0.40547141432762146
Epoch 1470, training loss: 83.54896545410156 = 0.2651258707046509 + 10.0 * 8.328383445739746
Epoch 1470, val loss: 0.4053538739681244
Epoch 1480, training loss: 83.54581451416016 = 0.2641395032405853 + 10.0 * 8.328167915344238
Epoch 1480, val loss: 0.40520182251930237
Epoch 1490, training loss: 83.60041046142578 = 0.26314905285835266 + 10.0 * 8.33372688293457
Epoch 1490, val loss: 0.40507766604423523
Epoch 1500, training loss: 83.56486511230469 = 0.262141615152359 + 10.0 * 8.330272674560547
Epoch 1500, val loss: 0.4049643576145172
Epoch 1510, training loss: 83.53260803222656 = 0.26113399863243103 + 10.0 * 8.327147483825684
Epoch 1510, val loss: 0.4047479033470154
Epoch 1520, training loss: 83.52122497558594 = 0.26013028621673584 + 10.0 * 8.326108932495117
Epoch 1520, val loss: 0.4046310782432556
Epoch 1530, training loss: 83.52760314941406 = 0.2591232657432556 + 10.0 * 8.326848030090332
Epoch 1530, val loss: 0.4044807553291321
Epoch 1540, training loss: 83.53413391113281 = 0.25810346007347107 + 10.0 * 8.327603340148926
Epoch 1540, val loss: 0.4043610692024231
Epoch 1550, training loss: 83.56199645996094 = 0.25707846879959106 + 10.0 * 8.33049201965332
Epoch 1550, val loss: 0.4042876958847046
Epoch 1560, training loss: 83.50381469726562 = 0.25604957342147827 + 10.0 * 8.324776649475098
Epoch 1560, val loss: 0.40408506989479065
Epoch 1570, training loss: 83.49628448486328 = 0.25503069162368774 + 10.0 * 8.324125289916992
Epoch 1570, val loss: 0.40394100546836853
Epoch 1580, training loss: 83.50463104248047 = 0.2540144920349121 + 10.0 * 8.325061798095703
Epoch 1580, val loss: 0.4038088619709015
Epoch 1590, training loss: 83.50438690185547 = 0.25298550724983215 + 10.0 * 8.325139999389648
Epoch 1590, val loss: 0.4037267565727234
Epoch 1600, training loss: 83.4949951171875 = 0.25195419788360596 + 10.0 * 8.324304580688477
Epoch 1600, val loss: 0.4037081003189087
Epoch 1610, training loss: 83.53776550292969 = 0.2509289085865021 + 10.0 * 8.328683853149414
Epoch 1610, val loss: 0.40370485186576843
Epoch 1620, training loss: 83.49059295654297 = 0.24988235533237457 + 10.0 * 8.324070930480957
Epoch 1620, val loss: 0.4033965766429901
Epoch 1630, training loss: 83.47570037841797 = 0.24884778261184692 + 10.0 * 8.322685241699219
Epoch 1630, val loss: 0.4034368693828583
Epoch 1640, training loss: 83.46485900878906 = 0.24782134592533112 + 10.0 * 8.321703910827637
Epoch 1640, val loss: 0.4033093750476837
Epoch 1650, training loss: 83.45845031738281 = 0.24678897857666016 + 10.0 * 8.321166038513184
Epoch 1650, val loss: 0.40327757596969604
Epoch 1660, training loss: 83.4662094116211 = 0.24575339257717133 + 10.0 * 8.322046279907227
Epoch 1660, val loss: 0.4032273590564728
Epoch 1670, training loss: 83.47737121582031 = 0.24470701813697815 + 10.0 * 8.323266983032227
Epoch 1670, val loss: 0.4032837450504303
Epoch 1680, training loss: 83.4459457397461 = 0.24365365505218506 + 10.0 * 8.320229530334473
Epoch 1680, val loss: 0.403087854385376
Epoch 1690, training loss: 83.4485092163086 = 0.24260807037353516 + 10.0 * 8.320590019226074
Epoch 1690, val loss: 0.40313416719436646
Epoch 1700, training loss: 83.45204162597656 = 0.24156029522418976 + 10.0 * 8.32104778289795
Epoch 1700, val loss: 0.40306970477104187
Epoch 1710, training loss: 83.46696472167969 = 0.24050721526145935 + 10.0 * 8.32264518737793
Epoch 1710, val loss: 0.4030497372150421
Epoch 1720, training loss: 83.4312973022461 = 0.23945076763629913 + 10.0 * 8.319185256958008
Epoch 1720, val loss: 0.4030236601829529
Epoch 1730, training loss: 83.42961883544922 = 0.2383953332901001 + 10.0 * 8.319122314453125
Epoch 1730, val loss: 0.40301892161369324
Epoch 1740, training loss: 83.43977355957031 = 0.23733952641487122 + 10.0 * 8.320242881774902
Epoch 1740, val loss: 0.4029421806335449
Epoch 1750, training loss: 83.4341812133789 = 0.23627662658691406 + 10.0 * 8.319790840148926
Epoch 1750, val loss: 0.4030756950378418
Epoch 1760, training loss: 83.41947937011719 = 0.2352105677127838 + 10.0 * 8.318426132202148
Epoch 1760, val loss: 0.40311914682388306
Epoch 1770, training loss: 83.41091918945312 = 0.23414623737335205 + 10.0 * 8.31767749786377
Epoch 1770, val loss: 0.4031081199645996
Epoch 1780, training loss: 83.41063690185547 = 0.23308169841766357 + 10.0 * 8.317755699157715
Epoch 1780, val loss: 0.4032014012336731
Epoch 1790, training loss: 83.46515655517578 = 0.2320149838924408 + 10.0 * 8.32331371307373
Epoch 1790, val loss: 0.4032769799232483
Epoch 1800, training loss: 83.42643737792969 = 0.23093032836914062 + 10.0 * 8.319551467895508
Epoch 1800, val loss: 0.4032406806945801
Epoch 1810, training loss: 83.42713928222656 = 0.22984832525253296 + 10.0 * 8.31972885131836
Epoch 1810, val loss: 0.4033503532409668
Epoch 1820, training loss: 83.394287109375 = 0.2287624329328537 + 10.0 * 8.316553115844727
Epoch 1820, val loss: 0.40325719118118286
Epoch 1830, training loss: 83.38732147216797 = 0.22767944633960724 + 10.0 * 8.315964698791504
Epoch 1830, val loss: 0.40336641669273376
Epoch 1840, training loss: 83.38235473632812 = 0.2265910506248474 + 10.0 * 8.315576553344727
Epoch 1840, val loss: 0.4034689664840698
Epoch 1850, training loss: 83.41719055175781 = 0.22549985349178314 + 10.0 * 8.319169044494629
Epoch 1850, val loss: 0.4036276042461395
Epoch 1860, training loss: 83.38395690917969 = 0.22438961267471313 + 10.0 * 8.315957069396973
Epoch 1860, val loss: 0.4035933315753937
Epoch 1870, training loss: 83.38825225830078 = 0.22328238189220428 + 10.0 * 8.316496849060059
Epoch 1870, val loss: 0.40371960401535034
Epoch 1880, training loss: 83.37986755371094 = 0.22216938436031342 + 10.0 * 8.315770149230957
Epoch 1880, val loss: 0.40378537774086
Epoch 1890, training loss: 83.38900756835938 = 0.22105933725833893 + 10.0 * 8.316794395446777
Epoch 1890, val loss: 0.40402328968048096
Epoch 1900, training loss: 83.36768341064453 = 0.21994149684906006 + 10.0 * 8.314774513244629
Epoch 1900, val loss: 0.40414297580718994
Epoch 1910, training loss: 83.36176300048828 = 0.21883083879947662 + 10.0 * 8.314292907714844
Epoch 1910, val loss: 0.40426135063171387
Epoch 1920, training loss: 83.3573989868164 = 0.21771568059921265 + 10.0 * 8.313968658447266
Epoch 1920, val loss: 0.4044020175933838
Epoch 1930, training loss: 83.37617492675781 = 0.21660290658473969 + 10.0 * 8.315957069396973
Epoch 1930, val loss: 0.4046315848827362
Epoch 1940, training loss: 83.36515808105469 = 0.21547211706638336 + 10.0 * 8.314969062805176
Epoch 1940, val loss: 0.4047463834285736
Epoch 1950, training loss: 83.39149475097656 = 0.2143428474664688 + 10.0 * 8.317715644836426
Epoch 1950, val loss: 0.4048402011394501
Epoch 1960, training loss: 83.34750366210938 = 0.21321284770965576 + 10.0 * 8.31342887878418
Epoch 1960, val loss: 0.40510445833206177
Epoch 1970, training loss: 83.34314727783203 = 0.21208210289478302 + 10.0 * 8.313106536865234
Epoch 1970, val loss: 0.40522241592407227
Epoch 1980, training loss: 83.3357162475586 = 0.21095187962055206 + 10.0 * 8.31247615814209
Epoch 1980, val loss: 0.405526340007782
Epoch 1990, training loss: 83.36358642578125 = 0.20981727540493011 + 10.0 * 8.315377235412598
Epoch 1990, val loss: 0.40575629472732544
Epoch 2000, training loss: 83.33053588867188 = 0.20867179334163666 + 10.0 * 8.312186241149902
Epoch 2000, val loss: 0.40605592727661133
Epoch 2010, training loss: 83.34024047851562 = 0.20752888917922974 + 10.0 * 8.313271522521973
Epoch 2010, val loss: 0.406313955783844
Epoch 2020, training loss: 83.32233428955078 = 0.20638465881347656 + 10.0 * 8.31159496307373
Epoch 2020, val loss: 0.40654730796813965
Epoch 2030, training loss: 83.33673095703125 = 0.20524191856384277 + 10.0 * 8.313149452209473
Epoch 2030, val loss: 0.40668588876724243
Epoch 2040, training loss: 83.34693908691406 = 0.2040981948375702 + 10.0 * 8.31428337097168
Epoch 2040, val loss: 0.40717750787734985
Epoch 2050, training loss: 83.3189697265625 = 0.2029372751712799 + 10.0 * 8.311603546142578
Epoch 2050, val loss: 0.40727365016937256
Epoch 2060, training loss: 83.30664825439453 = 0.20178385078907013 + 10.0 * 8.310486793518066
Epoch 2060, val loss: 0.4076177179813385
Epoch 2070, training loss: 83.30577850341797 = 0.2006264179944992 + 10.0 * 8.310514450073242
Epoch 2070, val loss: 0.4080124497413635
Epoch 2080, training loss: 83.34362030029297 = 0.1994694322347641 + 10.0 * 8.314414978027344
Epoch 2080, val loss: 0.40828442573547363
Epoch 2090, training loss: 83.30082702636719 = 0.1982983946800232 + 10.0 * 8.310253143310547
Epoch 2090, val loss: 0.4085688889026642
Epoch 2100, training loss: 83.29850769042969 = 0.197129026055336 + 10.0 * 8.310137748718262
Epoch 2100, val loss: 0.40881627798080444
Epoch 2110, training loss: 83.31169891357422 = 0.1959584802389145 + 10.0 * 8.31157398223877
Epoch 2110, val loss: 0.40925535559654236
Epoch 2120, training loss: 83.28645324707031 = 0.19477921724319458 + 10.0 * 8.309167861938477
Epoch 2120, val loss: 0.40956759452819824
Epoch 2130, training loss: 83.30534362792969 = 0.19361045956611633 + 10.0 * 8.311173439025879
Epoch 2130, val loss: 0.41004717350006104
Epoch 2140, training loss: 83.3177261352539 = 0.19243329763412476 + 10.0 * 8.312528610229492
Epoch 2140, val loss: 0.41044241189956665
Epoch 2150, training loss: 83.28839874267578 = 0.1912442445755005 + 10.0 * 8.309715270996094
Epoch 2150, val loss: 0.4106322228908539
Epoch 2160, training loss: 83.27501678466797 = 0.1900564730167389 + 10.0 * 8.308496475219727
Epoch 2160, val loss: 0.4111538827419281
Epoch 2170, training loss: 83.26822662353516 = 0.18886686861515045 + 10.0 * 8.30793571472168
Epoch 2170, val loss: 0.41152381896972656
Epoch 2180, training loss: 83.2896728515625 = 0.18768075108528137 + 10.0 * 8.310198783874512
Epoch 2180, val loss: 0.41194313764572144
Epoch 2190, training loss: 83.27422332763672 = 0.18648400902748108 + 10.0 * 8.3087739944458
Epoch 2190, val loss: 0.4124498665332794
Epoch 2200, training loss: 83.28117370605469 = 0.18529003858566284 + 10.0 * 8.309588432312012
Epoch 2200, val loss: 0.4129476547241211
Epoch 2210, training loss: 83.25697326660156 = 0.18408791720867157 + 10.0 * 8.30728816986084
Epoch 2210, val loss: 0.41324299573898315
Epoch 2220, training loss: 83.25655364990234 = 0.1828904151916504 + 10.0 * 8.307366371154785
Epoch 2220, val loss: 0.41379714012145996
Epoch 2230, training loss: 83.279541015625 = 0.18169938027858734 + 10.0 * 8.309783935546875
Epoch 2230, val loss: 0.4144558012485504
Epoch 2240, training loss: 83.24634552001953 = 0.180488720536232 + 10.0 * 8.306585311889648
Epoch 2240, val loss: 0.4147566258907318
Epoch 2250, training loss: 83.24785614013672 = 0.17928200960159302 + 10.0 * 8.306857109069824
Epoch 2250, val loss: 0.4152238070964813
Epoch 2260, training loss: 83.24298858642578 = 0.17807789146900177 + 10.0 * 8.306490898132324
Epoch 2260, val loss: 0.4158673882484436
Epoch 2270, training loss: 83.32646942138672 = 0.17688895761966705 + 10.0 * 8.314958572387695
Epoch 2270, val loss: 0.41626983880996704
Epoch 2280, training loss: 83.25740051269531 = 0.17566817998886108 + 10.0 * 8.308173179626465
Epoch 2280, val loss: 0.4169608950614929
Epoch 2290, training loss: 83.26715087890625 = 0.17446035146713257 + 10.0 * 8.309268951416016
Epoch 2290, val loss: 0.41757917404174805
Epoch 2300, training loss: 83.23066711425781 = 0.17324455082416534 + 10.0 * 8.305742263793945
Epoch 2300, val loss: 0.4180726408958435
Epoch 2310, training loss: 83.22962188720703 = 0.17203518748283386 + 10.0 * 8.305758476257324
Epoch 2310, val loss: 0.41869157552719116
Epoch 2320, training loss: 83.218994140625 = 0.17082737386226654 + 10.0 * 8.304816246032715
Epoch 2320, val loss: 0.4193607568740845
Epoch 2330, training loss: 83.22442626953125 = 0.16962064802646637 + 10.0 * 8.305480003356934
Epoch 2330, val loss: 0.4200681746006012
Epoch 2340, training loss: 83.25996398925781 = 0.1684156209230423 + 10.0 * 8.309154510498047
Epoch 2340, val loss: 0.42072129249572754
Epoch 2350, training loss: 83.2406234741211 = 0.16719771921634674 + 10.0 * 8.307342529296875
Epoch 2350, val loss: 0.42120784521102905
Epoch 2360, training loss: 83.22874450683594 = 0.16598671674728394 + 10.0 * 8.306276321411133
Epoch 2360, val loss: 0.4220201373100281
Epoch 2370, training loss: 83.22784423828125 = 0.1647804081439972 + 10.0 * 8.306306838989258
Epoch 2370, val loss: 0.4226796329021454
Epoch 2380, training loss: 83.202880859375 = 0.16357332468032837 + 10.0 * 8.303930282592773
Epoch 2380, val loss: 0.4233977496623993
Epoch 2390, training loss: 83.20803833007812 = 0.1623692363500595 + 10.0 * 8.304567337036133
Epoch 2390, val loss: 0.424050509929657
Epoch 2400, training loss: 83.24935150146484 = 0.16117389500141144 + 10.0 * 8.308817863464355
Epoch 2400, val loss: 0.42474523186683655
Epoch 2410, training loss: 83.2090835571289 = 0.15996667742729187 + 10.0 * 8.304911613464355
Epoch 2410, val loss: 0.42562851309776306
Epoch 2420, training loss: 83.21749877929688 = 0.15875938534736633 + 10.0 * 8.30587387084961
Epoch 2420, val loss: 0.4262077212333679
Epoch 2430, training loss: 83.20391845703125 = 0.15754927694797516 + 10.0 * 8.30463695526123
Epoch 2430, val loss: 0.42715319991111755
Epoch 2440, training loss: 83.18814849853516 = 0.15633624792099 + 10.0 * 8.303181648254395
Epoch 2440, val loss: 0.427941232919693
Epoch 2450, training loss: 83.18502807617188 = 0.15512393414974213 + 10.0 * 8.302990913391113
Epoch 2450, val loss: 0.42873692512512207
Epoch 2460, training loss: 83.20735168457031 = 0.15391981601715088 + 10.0 * 8.305342674255371
Epoch 2460, val loss: 0.4296613335609436
Epoch 2470, training loss: 83.19160461425781 = 0.1527089774608612 + 10.0 * 8.303889274597168
Epoch 2470, val loss: 0.4303440451622009
Epoch 2480, training loss: 83.18067169189453 = 0.15150336921215057 + 10.0 * 8.302916526794434
Epoch 2480, val loss: 0.4312211871147156
Epoch 2490, training loss: 83.20169830322266 = 0.15031090378761292 + 10.0 * 8.30513858795166
Epoch 2490, val loss: 0.43218013644218445
Epoch 2500, training loss: 83.18722534179688 = 0.14910534024238586 + 10.0 * 8.303812026977539
Epoch 2500, val loss: 0.43296077847480774
Epoch 2510, training loss: 83.16470336914062 = 0.14790178835391998 + 10.0 * 8.301679611206055
Epoch 2510, val loss: 0.43361592292785645
Epoch 2520, training loss: 83.16231536865234 = 0.14670933783054352 + 10.0 * 8.30156135559082
Epoch 2520, val loss: 0.4345669448375702
Epoch 2530, training loss: 83.1628189086914 = 0.14551551640033722 + 10.0 * 8.301730155944824
Epoch 2530, val loss: 0.43548575043678284
Epoch 2540, training loss: 83.19390106201172 = 0.1443258374929428 + 10.0 * 8.304957389831543
Epoch 2540, val loss: 0.43641576170921326
Epoch 2550, training loss: 83.1771240234375 = 0.1431329846382141 + 10.0 * 8.303399085998535
Epoch 2550, val loss: 0.43753352761268616
Epoch 2560, training loss: 83.16073608398438 = 0.14193758368492126 + 10.0 * 8.3018798828125
Epoch 2560, val loss: 0.438438355922699
Epoch 2570, training loss: 83.15087890625 = 0.14074617624282837 + 10.0 * 8.301012992858887
Epoch 2570, val loss: 0.4391857981681824
Epoch 2580, training loss: 83.15138244628906 = 0.13956120610237122 + 10.0 * 8.30118179321289
Epoch 2580, val loss: 0.44019579887390137
Epoch 2590, training loss: 83.16194152832031 = 0.13838210701942444 + 10.0 * 8.302355766296387
Epoch 2590, val loss: 0.44103407859802246
Epoch 2600, training loss: 83.13687896728516 = 0.1371976137161255 + 10.0 * 8.299967765808105
Epoch 2600, val loss: 0.4422619938850403
Epoch 2610, training loss: 83.15009307861328 = 0.13602466881275177 + 10.0 * 8.301406860351562
Epoch 2610, val loss: 0.4435137212276459
Epoch 2620, training loss: 83.1604995727539 = 0.13484838604927063 + 10.0 * 8.30256462097168
Epoch 2620, val loss: 0.44432079792022705
Epoch 2630, training loss: 83.13746643066406 = 0.13366816937923431 + 10.0 * 8.300379753112793
Epoch 2630, val loss: 0.44512495398521423
Epoch 2640, training loss: 83.13058471679688 = 0.13249695301055908 + 10.0 * 8.299808502197266
Epoch 2640, val loss: 0.4464816749095917
Epoch 2650, training loss: 83.15985870361328 = 0.13133394718170166 + 10.0 * 8.302852630615234
Epoch 2650, val loss: 0.44740021228790283
Epoch 2660, training loss: 83.13249206542969 = 0.1301645189523697 + 10.0 * 8.300232887268066
Epoch 2660, val loss: 0.4483610987663269
Epoch 2670, training loss: 83.12110137939453 = 0.1289992332458496 + 10.0 * 8.299210548400879
Epoch 2670, val loss: 0.4497342109680176
Epoch 2680, training loss: 83.12550354003906 = 0.1278371661901474 + 10.0 * 8.299766540527344
Epoch 2680, val loss: 0.45059388875961304
Epoch 2690, training loss: 83.13438415527344 = 0.1266818344593048 + 10.0 * 8.30077075958252
Epoch 2690, val loss: 0.4518079459667206
Epoch 2700, training loss: 83.147216796875 = 0.1255308985710144 + 10.0 * 8.302167892456055
Epoch 2700, val loss: 0.4530789852142334
Epoch 2710, training loss: 83.11724853515625 = 0.12437211722135544 + 10.0 * 8.299287796020508
Epoch 2710, val loss: 0.454157292842865
Epoch 2720, training loss: 83.10238647460938 = 0.12321630865335464 + 10.0 * 8.297917366027832
Epoch 2720, val loss: 0.4552842378616333
Epoch 2730, training loss: 83.097412109375 = 0.1220623180270195 + 10.0 * 8.297534942626953
Epoch 2730, val loss: 0.4565219283103943
Epoch 2740, training loss: 83.11500549316406 = 0.12091699242591858 + 10.0 * 8.299408912658691
Epoch 2740, val loss: 0.4577467143535614
Epoch 2750, training loss: 83.11560821533203 = 0.11977379024028778 + 10.0 * 8.299583435058594
Epoch 2750, val loss: 0.4589446485042572
Epoch 2760, training loss: 83.114013671875 = 0.11862948536872864 + 10.0 * 8.299538612365723
Epoch 2760, val loss: 0.4602893888950348
Epoch 2770, training loss: 83.09226989746094 = 0.1174919456243515 + 10.0 * 8.297477722167969
Epoch 2770, val loss: 0.461515337228775
Epoch 2780, training loss: 83.08675384521484 = 0.11636368930339813 + 10.0 * 8.297039031982422
Epoch 2780, val loss: 0.4628140330314636
Epoch 2790, training loss: 83.0813217163086 = 0.11523494869470596 + 10.0 * 8.296608924865723
Epoch 2790, val loss: 0.4640013873577118
Epoch 2800, training loss: 83.11741638183594 = 0.11412276327610016 + 10.0 * 8.300329208374023
Epoch 2800, val loss: 0.46514692902565
Epoch 2810, training loss: 83.08084869384766 = 0.11300764232873917 + 10.0 * 8.296784400939941
Epoch 2810, val loss: 0.4666937589645386
Epoch 2820, training loss: 83.07791137695312 = 0.11190185695886612 + 10.0 * 8.296601295471191
Epoch 2820, val loss: 0.4680675268173218
Epoch 2830, training loss: 83.07475280761719 = 0.1108020544052124 + 10.0 * 8.296395301818848
Epoch 2830, val loss: 0.46951231360435486
Epoch 2840, training loss: 83.0863037109375 = 0.10971396416425705 + 10.0 * 8.297658920288086
Epoch 2840, val loss: 0.4710543155670166
Epoch 2850, training loss: 83.09783172607422 = 0.10862960666418076 + 10.0 * 8.298920631408691
Epoch 2850, val loss: 0.47242727875709534
Epoch 2860, training loss: 83.0853271484375 = 0.10754522681236267 + 10.0 * 8.297778129577637
Epoch 2860, val loss: 0.4734939634799957
Epoch 2870, training loss: 83.06378173828125 = 0.10646393150091171 + 10.0 * 8.295732498168945
Epoch 2870, val loss: 0.47498273849487305
Epoch 2880, training loss: 83.05630493164062 = 0.10539020597934723 + 10.0 * 8.29509162902832
Epoch 2880, val loss: 0.47640714049339294
Epoch 2890, training loss: 83.05669403076172 = 0.10432948172092438 + 10.0 * 8.295236587524414
Epoch 2890, val loss: 0.4778616428375244
Epoch 2900, training loss: 83.1115951538086 = 0.10329143702983856 + 10.0 * 8.300829887390137
Epoch 2900, val loss: 0.4793946444988251
Epoch 2910, training loss: 83.05089569091797 = 0.1022273451089859 + 10.0 * 8.294866561889648
Epoch 2910, val loss: 0.4806828796863556
Epoch 2920, training loss: 83.04979705810547 = 0.10118094086647034 + 10.0 * 8.294861793518066
Epoch 2920, val loss: 0.4820544123649597
Epoch 2930, training loss: 83.0631332397461 = 0.1001468077301979 + 10.0 * 8.29629898071289
Epoch 2930, val loss: 0.483412504196167
Epoch 2940, training loss: 83.05748748779297 = 0.09911825507879257 + 10.0 * 8.295836448669434
Epoch 2940, val loss: 0.4848581850528717
Epoch 2950, training loss: 83.0630111694336 = 0.09810293465852737 + 10.0 * 8.296490669250488
Epoch 2950, val loss: 0.48644858598709106
Epoch 2960, training loss: 83.04169464111328 = 0.0970880314707756 + 10.0 * 8.29446029663086
Epoch 2960, val loss: 0.4878145754337311
Epoch 2970, training loss: 83.05117797851562 = 0.09608490765094757 + 10.0 * 8.295509338378906
Epoch 2970, val loss: 0.4893641173839569
Epoch 2980, training loss: 83.06336975097656 = 0.09509464353322983 + 10.0 * 8.29682731628418
Epoch 2980, val loss: 0.49073848128318787
Epoch 2990, training loss: 83.04108428955078 = 0.09410520642995834 + 10.0 * 8.294697761535645
Epoch 2990, val loss: 0.4925698935985565
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.839167935058346
0.866550749836992
The final CL Acc:0.84069, 0.00149, The final GNN Acc:0.86684, 0.00041
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=3, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 97514])
remove edge: torch.Size([2, 79762])
updated graph: torch.Size([2, 88628])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 106.9239730834961 = 1.1019847393035889 + 10.0 * 10.582199096679688
Epoch 0, val loss: 1.1018157005310059
Epoch 10, training loss: 106.90345764160156 = 1.0949795246124268 + 10.0 * 10.58084774017334
Epoch 10, val loss: 1.094774603843689
Epoch 20, training loss: 106.78086853027344 = 1.0868133306503296 + 10.0 * 10.569405555725098
Epoch 20, val loss: 1.0865663290023804
Epoch 30, training loss: 106.05904388427734 = 1.076811671257019 + 10.0 * 10.498223304748535
Epoch 30, val loss: 1.0764789581298828
Epoch 40, training loss: 103.27616119384766 = 1.0656780004501343 + 10.0 * 10.221048355102539
Epoch 40, val loss: 1.0655465126037598
Epoch 50, training loss: 98.15896606445312 = 1.053855299949646 + 10.0 * 9.710511207580566
Epoch 50, val loss: 1.0538506507873535
Epoch 60, training loss: 96.27562713623047 = 1.045143961906433 + 10.0 * 9.523048400878906
Epoch 60, val loss: 1.045317530632019
Epoch 70, training loss: 95.03079986572266 = 1.0368516445159912 + 10.0 * 9.399394989013672
Epoch 70, val loss: 1.037203311920166
Epoch 80, training loss: 93.19998931884766 = 1.0290504693984985 + 10.0 * 9.217093467712402
Epoch 80, val loss: 1.0296727418899536
Epoch 90, training loss: 92.5080795288086 = 1.0213470458984375 + 10.0 * 9.148673057556152
Epoch 90, val loss: 1.0221538543701172
Epoch 100, training loss: 91.81523895263672 = 1.0138031244277954 + 10.0 * 9.080143928527832
Epoch 100, val loss: 1.0149167776107788
Epoch 110, training loss: 90.59619140625 = 1.0076769590377808 + 10.0 * 8.95885181427002
Epoch 110, val loss: 1.0091919898986816
Epoch 120, training loss: 89.32976531982422 = 1.0030786991119385 + 10.0 * 8.83266830444336
Epoch 120, val loss: 1.0048863887786865
Epoch 130, training loss: 88.59413146972656 = 0.9979920983314514 + 10.0 * 8.759613990783691
Epoch 130, val loss: 0.9998189806938171
Epoch 140, training loss: 88.08757019042969 = 0.9902512431144714 + 10.0 * 8.709732055664062
Epoch 140, val loss: 0.992087185382843
Epoch 150, training loss: 87.7498550415039 = 0.9804506301879883 + 10.0 * 8.676939964294434
Epoch 150, val loss: 0.9826470017433167
Epoch 160, training loss: 87.51885223388672 = 0.9698832035064697 + 10.0 * 8.65489673614502
Epoch 160, val loss: 0.9725872278213501
Epoch 170, training loss: 87.31192016601562 = 0.9588105082511902 + 10.0 * 8.635311126708984
Epoch 170, val loss: 0.9620130062103271
Epoch 180, training loss: 87.10728454589844 = 0.9471319317817688 + 10.0 * 8.616015434265137
Epoch 180, val loss: 0.9509434700012207
Epoch 190, training loss: 86.8595199584961 = 0.9348388910293579 + 10.0 * 8.59246826171875
Epoch 190, val loss: 0.9392197132110596
Epoch 200, training loss: 86.63385772705078 = 0.9219509363174438 + 10.0 * 8.57119083404541
Epoch 200, val loss: 0.9270052313804626
Epoch 210, training loss: 86.52367401123047 = 0.9081917405128479 + 10.0 * 8.561548233032227
Epoch 210, val loss: 0.9139476418495178
Epoch 220, training loss: 86.26972198486328 = 0.8932160139083862 + 10.0 * 8.537650108337402
Epoch 220, val loss: 0.8996398448944092
Epoch 230, training loss: 86.09464263916016 = 0.8772193789482117 + 10.0 * 8.52174186706543
Epoch 230, val loss: 0.8845604062080383
Epoch 240, training loss: 85.96188354492188 = 0.8603748083114624 + 10.0 * 8.510150909423828
Epoch 240, val loss: 0.8686582446098328
Epoch 250, training loss: 85.84249877929688 = 0.8426399230957031 + 10.0 * 8.499985694885254
Epoch 250, val loss: 0.8520113825798035
Epoch 260, training loss: 85.75836944580078 = 0.824171781539917 + 10.0 * 8.493419647216797
Epoch 260, val loss: 0.8346796631813049
Epoch 270, training loss: 85.65386199951172 = 0.8051608800888062 + 10.0 * 8.484869956970215
Epoch 270, val loss: 0.816949725151062
Epoch 280, training loss: 85.55133056640625 = 0.7860382199287415 + 10.0 * 8.476529121398926
Epoch 280, val loss: 0.7992261052131653
Epoch 290, training loss: 85.46868896484375 = 0.7669349908828735 + 10.0 * 8.470174789428711
Epoch 290, val loss: 0.7815689444541931
Epoch 300, training loss: 85.45013427734375 = 0.7478976249694824 + 10.0 * 8.470224380493164
Epoch 300, val loss: 0.7640763521194458
Epoch 310, training loss: 85.3097152709961 = 0.729295015335083 + 10.0 * 8.45804214477539
Epoch 310, val loss: 0.7471200227737427
Epoch 320, training loss: 85.24488067626953 = 0.7114642262458801 + 10.0 * 8.453341484069824
Epoch 320, val loss: 0.7309133410453796
Epoch 330, training loss: 85.21846771240234 = 0.6943749785423279 + 10.0 * 8.452409744262695
Epoch 330, val loss: 0.7154442071914673
Epoch 340, training loss: 85.13195037841797 = 0.6779506802558899 + 10.0 * 8.44540023803711
Epoch 340, val loss: 0.7008894681930542
Epoch 350, training loss: 85.05783081054688 = 0.6626173257827759 + 10.0 * 8.439520835876465
Epoch 350, val loss: 0.687293529510498
Epoch 360, training loss: 85.00943756103516 = 0.6483278870582581 + 10.0 * 8.436110496520996
Epoch 360, val loss: 0.6746430993080139
Epoch 370, training loss: 84.9820327758789 = 0.6349468231201172 + 10.0 * 8.434708595275879
Epoch 370, val loss: 0.6629992127418518
Epoch 380, training loss: 84.91470336914062 = 0.62259840965271 + 10.0 * 8.429210662841797
Epoch 380, val loss: 0.6523414850234985
Epoch 390, training loss: 84.87186431884766 = 0.6113169193267822 + 10.0 * 8.426054000854492
Epoch 390, val loss: 0.6426391005516052
Epoch 400, training loss: 84.83465576171875 = 0.6009179353713989 + 10.0 * 8.423373222351074
Epoch 400, val loss: 0.6338267922401428
Epoch 410, training loss: 84.81065368652344 = 0.591331958770752 + 10.0 * 8.421932220458984
Epoch 410, val loss: 0.6258226037025452
Epoch 420, training loss: 84.77275848388672 = 0.5825402140617371 + 10.0 * 8.419021606445312
Epoch 420, val loss: 0.6185573935508728
Epoch 430, training loss: 84.73333740234375 = 0.5745173096656799 + 10.0 * 8.415882110595703
Epoch 430, val loss: 0.612027645111084
Epoch 440, training loss: 84.74913024902344 = 0.5671197772026062 + 10.0 * 8.418200492858887
Epoch 440, val loss: 0.6061621904373169
Epoch 450, training loss: 84.70857238769531 = 0.5602344274520874 + 10.0 * 8.414834022521973
Epoch 450, val loss: 0.6005363464355469
Epoch 460, training loss: 84.65782165527344 = 0.5539283156394958 + 10.0 * 8.41038990020752
Epoch 460, val loss: 0.5956645607948303
Epoch 470, training loss: 84.62567138671875 = 0.5481370687484741 + 10.0 * 8.407753944396973
Epoch 470, val loss: 0.5912044048309326
Epoch 480, training loss: 84.6022720336914 = 0.5427464842796326 + 10.0 * 8.405952453613281
Epoch 480, val loss: 0.5871180891990662
Epoch 490, training loss: 84.66032409667969 = 0.5376977324485779 + 10.0 * 8.412262916564941
Epoch 490, val loss: 0.5834201574325562
Epoch 500, training loss: 84.5848388671875 = 0.5329314470291138 + 10.0 * 8.405191421508789
Epoch 500, val loss: 0.5799081325531006
Epoch 510, training loss: 84.5498275756836 = 0.5285419225692749 + 10.0 * 8.402128219604492
Epoch 510, val loss: 0.5767369866371155
Epoch 520, training loss: 84.52677917480469 = 0.5244352221488953 + 10.0 * 8.40023422241211
Epoch 520, val loss: 0.5738518238067627
Epoch 530, training loss: 84.54730224609375 = 0.5205373764038086 + 10.0 * 8.402676582336426
Epoch 530, val loss: 0.5711068511009216
Epoch 540, training loss: 84.51081848144531 = 0.5168142914772034 + 10.0 * 8.39940071105957
Epoch 540, val loss: 0.5686596632003784
Epoch 550, training loss: 84.47528839111328 = 0.5133044123649597 + 10.0 * 8.396198272705078
Epoch 550, val loss: 0.5660991668701172
Epoch 560, training loss: 84.45663452148438 = 0.5100061297416687 + 10.0 * 8.394662857055664
Epoch 560, val loss: 0.5640255808830261
Epoch 570, training loss: 84.43600463867188 = 0.5068588852882385 + 10.0 * 8.392914772033691
Epoch 570, val loss: 0.5619733333587646
Epoch 580, training loss: 84.43790435791016 = 0.5038430094718933 + 10.0 * 8.39340591430664
Epoch 580, val loss: 0.5600286722183228
Epoch 590, training loss: 84.42974090576172 = 0.5009120106697083 + 10.0 * 8.392882347106934
Epoch 590, val loss: 0.5581991076469421
Epoch 600, training loss: 84.41552734375 = 0.49807482957839966 + 10.0 * 8.391744613647461
Epoch 600, val loss: 0.5563526749610901
Epoch 610, training loss: 84.38525390625 = 0.49537691473960876 + 10.0 * 8.38898754119873
Epoch 610, val loss: 0.5546435117721558
Epoch 620, training loss: 84.36554718017578 = 0.4927815794944763 + 10.0 * 8.387276649475098
Epoch 620, val loss: 0.5530384182929993
Epoch 630, training loss: 84.3645248413086 = 0.4902596175670624 + 10.0 * 8.387426376342773
Epoch 630, val loss: 0.5514752268791199
Epoch 640, training loss: 84.34939575195312 = 0.4877672493457794 + 10.0 * 8.386162757873535
Epoch 640, val loss: 0.5500186085700989
Epoch 650, training loss: 84.34661865234375 = 0.48533713817596436 + 10.0 * 8.386128425598145
Epoch 650, val loss: 0.5484892129898071
Epoch 660, training loss: 84.32997131347656 = 0.4830031991004944 + 10.0 * 8.384696960449219
Epoch 660, val loss: 0.5471201539039612
Epoch 670, training loss: 84.33928680419922 = 0.48070186376571655 + 10.0 * 8.385858535766602
Epoch 670, val loss: 0.5456981658935547
Epoch 680, training loss: 84.304931640625 = 0.478438138961792 + 10.0 * 8.382649421691895
Epoch 680, val loss: 0.5442814826965332
Epoch 690, training loss: 84.29080963134766 = 0.4762479364871979 + 10.0 * 8.38145637512207
Epoch 690, val loss: 0.542966902256012
Epoch 700, training loss: 84.28276824951172 = 0.4740881025791168 + 10.0 * 8.380867958068848
Epoch 700, val loss: 0.5415941476821899
Epoch 710, training loss: 84.32111358642578 = 0.4719395637512207 + 10.0 * 8.384917259216309
Epoch 710, val loss: 0.5402424335479736
Epoch 720, training loss: 84.28043365478516 = 0.46981942653656006 + 10.0 * 8.381061553955078
Epoch 720, val loss: 0.5391952395439148
Epoch 730, training loss: 84.25782012939453 = 0.467708021402359 + 10.0 * 8.379011154174805
Epoch 730, val loss: 0.5377373099327087
Epoch 740, training loss: 84.24243927001953 = 0.4656440317630768 + 10.0 * 8.377679824829102
Epoch 740, val loss: 0.5364542603492737
Epoch 750, training loss: 84.26341247558594 = 0.46360281109809875 + 10.0 * 8.37998104095459
Epoch 750, val loss: 0.5351420640945435
Epoch 760, training loss: 84.22547149658203 = 0.46154531836509705 + 10.0 * 8.376392364501953
Epoch 760, val loss: 0.5339106917381287
Epoch 770, training loss: 84.22322845458984 = 0.45951327681541443 + 10.0 * 8.376371383666992
Epoch 770, val loss: 0.5326230525970459
Epoch 780, training loss: 84.2313003540039 = 0.4575011432170868 + 10.0 * 8.377379417419434
Epoch 780, val loss: 0.5313078165054321
Epoch 790, training loss: 84.1954345703125 = 0.45549276471138 + 10.0 * 8.373994827270508
Epoch 790, val loss: 0.5301830768585205
Epoch 800, training loss: 84.20068359375 = 0.4535147249698639 + 10.0 * 8.374716758728027
Epoch 800, val loss: 0.5288885831832886
Epoch 810, training loss: 84.20557403564453 = 0.451530396938324 + 10.0 * 8.375404357910156
Epoch 810, val loss: 0.5276174545288086
Epoch 820, training loss: 84.17402648925781 = 0.4495580792427063 + 10.0 * 8.37244701385498
Epoch 820, val loss: 0.5265917778015137
Epoch 830, training loss: 84.15831756591797 = 0.4475967586040497 + 10.0 * 8.371072769165039
Epoch 830, val loss: 0.525309681892395
Epoch 840, training loss: 84.1470947265625 = 0.4456479549407959 + 10.0 * 8.370144844055176
Epoch 840, val loss: 0.5242009162902832
Epoch 850, training loss: 84.20286560058594 = 0.44371742010116577 + 10.0 * 8.375914573669434
Epoch 850, val loss: 0.5231393575668335
Epoch 860, training loss: 84.15110778808594 = 0.44171369075775146 + 10.0 * 8.370939254760742
Epoch 860, val loss: 0.5216655731201172
Epoch 870, training loss: 84.13150024414062 = 0.4397796094417572 + 10.0 * 8.369172096252441
Epoch 870, val loss: 0.5203444957733154
Epoch 880, training loss: 84.1072769165039 = 0.4378851056098938 + 10.0 * 8.366939544677734
Epoch 880, val loss: 0.5192500352859497
Epoch 890, training loss: 84.10009765625 = 0.4360125958919525 + 10.0 * 8.366408348083496
Epoch 890, val loss: 0.5181414484977722
Epoch 900, training loss: 84.115234375 = 0.434137225151062 + 10.0 * 8.368109703063965
Epoch 900, val loss: 0.516883373260498
Epoch 910, training loss: 84.0870132446289 = 0.4322463274002075 + 10.0 * 8.365476608276367
Epoch 910, val loss: 0.5159367322921753
Epoch 920, training loss: 84.09201049804688 = 0.43037793040275574 + 10.0 * 8.36616325378418
Epoch 920, val loss: 0.5145777463912964
Epoch 930, training loss: 84.09339141845703 = 0.42850545048713684 + 10.0 * 8.366488456726074
Epoch 930, val loss: 0.5135186314582825
Epoch 940, training loss: 84.05996704101562 = 0.4266357123851776 + 10.0 * 8.363332748413086
Epoch 940, val loss: 0.512356698513031
Epoch 950, training loss: 84.04914093017578 = 0.4248068332672119 + 10.0 * 8.362433433532715
Epoch 950, val loss: 0.511200487613678
Epoch 960, training loss: 84.03688049316406 = 0.42298975586891174 + 10.0 * 8.36138916015625
Epoch 960, val loss: 0.510092556476593
Epoch 970, training loss: 84.03887939453125 = 0.42117899656295776 + 10.0 * 8.361769676208496
Epoch 970, val loss: 0.5089061856269836
Epoch 980, training loss: 84.05765533447266 = 0.4193439185619354 + 10.0 * 8.363831520080566
Epoch 980, val loss: 0.5079822540283203
Epoch 990, training loss: 84.04198455810547 = 0.4175053536891937 + 10.0 * 8.362447738647461
Epoch 990, val loss: 0.5066336393356323
Epoch 1000, training loss: 84.0168685913086 = 0.41571304202079773 + 10.0 * 8.360116004943848
Epoch 1000, val loss: 0.5056267380714417
Epoch 1010, training loss: 84.02347564697266 = 0.41393280029296875 + 10.0 * 8.360954284667969
Epoch 1010, val loss: 0.5046432614326477
Epoch 1020, training loss: 83.9904556274414 = 0.4121592938899994 + 10.0 * 8.357829093933105
Epoch 1020, val loss: 0.503565788269043
Epoch 1030, training loss: 83.99331665039062 = 0.41040217876434326 + 10.0 * 8.358291625976562
Epoch 1030, val loss: 0.5024983882904053
Epoch 1040, training loss: 84.00312042236328 = 0.40864139795303345 + 10.0 * 8.359448432922363
Epoch 1040, val loss: 0.5014169216156006
Epoch 1050, training loss: 83.98247528076172 = 0.4068814218044281 + 10.0 * 8.357559204101562
Epoch 1050, val loss: 0.5005187392234802
Epoch 1060, training loss: 83.96239471435547 = 0.4051266610622406 + 10.0 * 8.35572624206543
Epoch 1060, val loss: 0.49944958090782166
Epoch 1070, training loss: 83.9561538696289 = 0.40339022874832153 + 10.0 * 8.355276107788086
Epoch 1070, val loss: 0.49851807951927185
Epoch 1080, training loss: 83.97669982910156 = 0.4016737639904022 + 10.0 * 8.357502937316895
Epoch 1080, val loss: 0.49766746163368225
Epoch 1090, training loss: 83.95869445800781 = 0.3999333679676056 + 10.0 * 8.355875968933105
Epoch 1090, val loss: 0.49646320939064026
Epoch 1100, training loss: 83.9524917602539 = 0.39822083711624146 + 10.0 * 8.355426788330078
Epoch 1100, val loss: 0.49567195773124695
Epoch 1110, training loss: 83.93997955322266 = 0.3965015709400177 + 10.0 * 8.354348182678223
Epoch 1110, val loss: 0.49465250968933105
Epoch 1120, training loss: 83.94002532958984 = 0.3948093354701996 + 10.0 * 8.354521751403809
Epoch 1120, val loss: 0.4937235414981842
Epoch 1130, training loss: 83.93997192382812 = 0.39311161637306213 + 10.0 * 8.35468578338623
Epoch 1130, val loss: 0.49274304509162903
Epoch 1140, training loss: 83.91083526611328 = 0.3914264738559723 + 10.0 * 8.351941108703613
Epoch 1140, val loss: 0.49201539158821106
Epoch 1150, training loss: 83.9000015258789 = 0.3897585868835449 + 10.0 * 8.351024627685547
Epoch 1150, val loss: 0.49112915992736816
Epoch 1160, training loss: 83.90145111083984 = 0.3881082534790039 + 10.0 * 8.351334571838379
Epoch 1160, val loss: 0.490346759557724
Epoch 1170, training loss: 83.94501495361328 = 0.3864500820636749 + 10.0 * 8.355855941772461
Epoch 1170, val loss: 0.48938655853271484
Epoch 1180, training loss: 83.89503479003906 = 0.3847864270210266 + 10.0 * 8.351024627685547
Epoch 1180, val loss: 0.48869845271110535
Epoch 1190, training loss: 83.87501525878906 = 0.3831528425216675 + 10.0 * 8.349185943603516
Epoch 1190, val loss: 0.4877587854862213
Epoch 1200, training loss: 83.86990356445312 = 0.38153377175331116 + 10.0 * 8.348836898803711
Epoch 1200, val loss: 0.48706555366516113
Epoch 1210, training loss: 83.90743255615234 = 0.3799169361591339 + 10.0 * 8.352751731872559
Epoch 1210, val loss: 0.4862547516822815
Epoch 1220, training loss: 83.86370849609375 = 0.37829530239105225 + 10.0 * 8.348541259765625
Epoch 1220, val loss: 0.48548245429992676
Epoch 1230, training loss: 83.85302734375 = 0.37668538093566895 + 10.0 * 8.347634315490723
Epoch 1230, val loss: 0.4847416579723358
Epoch 1240, training loss: 83.84712219238281 = 0.37509679794311523 + 10.0 * 8.34720230102539
Epoch 1240, val loss: 0.4840025305747986
Epoch 1250, training loss: 83.88805389404297 = 0.3735159635543823 + 10.0 * 8.35145378112793
Epoch 1250, val loss: 0.48330003023147583
Epoch 1260, training loss: 83.85415649414062 = 0.37192991375923157 + 10.0 * 8.348222732543945
Epoch 1260, val loss: 0.48255541920661926
Epoch 1270, training loss: 83.85392761230469 = 0.37035998702049255 + 10.0 * 8.348356246948242
Epoch 1270, val loss: 0.48177284002304077
Epoch 1280, training loss: 83.8329086303711 = 0.36879774928092957 + 10.0 * 8.346410751342773
Epoch 1280, val loss: 0.4811236262321472
Epoch 1290, training loss: 83.85152435302734 = 0.3672529458999634 + 10.0 * 8.348426818847656
Epoch 1290, val loss: 0.480472594499588
Epoch 1300, training loss: 83.81414031982422 = 0.36570242047309875 + 10.0 * 8.344843864440918
Epoch 1300, val loss: 0.47997331619262695
Epoch 1310, training loss: 83.81059265136719 = 0.36416760087013245 + 10.0 * 8.344642639160156
Epoch 1310, val loss: 0.4792492985725403
Epoch 1320, training loss: 83.84773254394531 = 0.3626520335674286 + 10.0 * 8.34850788116455
Epoch 1320, val loss: 0.47878095507621765
Epoch 1330, training loss: 83.81014251708984 = 0.3611067831516266 + 10.0 * 8.344903945922852
Epoch 1330, val loss: 0.47812581062316895
Epoch 1340, training loss: 83.79985046386719 = 0.3595910966396332 + 10.0 * 8.344026565551758
Epoch 1340, val loss: 0.4774686098098755
Epoch 1350, training loss: 83.78935241699219 = 0.3580944240093231 + 10.0 * 8.34312629699707
Epoch 1350, val loss: 0.4770384430885315
Epoch 1360, training loss: 83.78089904785156 = 0.3566076159477234 + 10.0 * 8.342429161071777
Epoch 1360, val loss: 0.47643759846687317
Epoch 1370, training loss: 83.80030059814453 = 0.3551236093044281 + 10.0 * 8.344517707824707
Epoch 1370, val loss: 0.4759461581707001
Epoch 1380, training loss: 83.80387115478516 = 0.35363462567329407 + 10.0 * 8.345023155212402
Epoch 1380, val loss: 0.4756637513637543
Epoch 1390, training loss: 83.79367065429688 = 0.35212960839271545 + 10.0 * 8.344154357910156
Epoch 1390, val loss: 0.47480687499046326
Epoch 1400, training loss: 83.76032257080078 = 0.3506718575954437 + 10.0 * 8.340965270996094
Epoch 1400, val loss: 0.47442737221717834
Epoch 1410, training loss: 83.75924682617188 = 0.34922605752944946 + 10.0 * 8.341001510620117
Epoch 1410, val loss: 0.47408327460289
Epoch 1420, training loss: 83.76445770263672 = 0.3477862477302551 + 10.0 * 8.341667175292969
Epoch 1420, val loss: 0.4735616147518158
Epoch 1430, training loss: 83.79133605957031 = 0.3463406264781952 + 10.0 * 8.344499588012695
Epoch 1430, val loss: 0.4732879400253296
Epoch 1440, training loss: 83.75184631347656 = 0.3448764383792877 + 10.0 * 8.340696334838867
Epoch 1440, val loss: 0.4726945459842682
Epoch 1450, training loss: 83.7466049194336 = 0.3434484302997589 + 10.0 * 8.340315818786621
Epoch 1450, val loss: 0.47248589992523193
Epoch 1460, training loss: 83.73432159423828 = 0.3420320153236389 + 10.0 * 8.339228630065918
Epoch 1460, val loss: 0.47203299403190613
Epoch 1470, training loss: 83.73615264892578 = 0.34062615036964417 + 10.0 * 8.33955192565918
Epoch 1470, val loss: 0.471664160490036
Epoch 1480, training loss: 83.77163696289062 = 0.339221328496933 + 10.0 * 8.343241691589355
Epoch 1480, val loss: 0.47128450870513916
Epoch 1490, training loss: 83.73486328125 = 0.3378034234046936 + 10.0 * 8.339705467224121
Epoch 1490, val loss: 0.4710422456264496
Epoch 1500, training loss: 83.73277282714844 = 0.3363993763923645 + 10.0 * 8.33963680267334
Epoch 1500, val loss: 0.4705601930618286
Epoch 1510, training loss: 83.741455078125 = 0.3349934220314026 + 10.0 * 8.340646743774414
Epoch 1510, val loss: 0.4703950583934784
Epoch 1520, training loss: 83.72837829589844 = 0.3335994780063629 + 10.0 * 8.3394775390625
Epoch 1520, val loss: 0.47012951970100403
Epoch 1530, training loss: 83.71013641357422 = 0.33221474289894104 + 10.0 * 8.33779239654541
Epoch 1530, val loss: 0.4698481261730194
Epoch 1540, training loss: 83.69951629638672 = 0.3308368921279907 + 10.0 * 8.336868286132812
Epoch 1540, val loss: 0.46954089403152466
Epoch 1550, training loss: 83.70109558105469 = 0.3294658958911896 + 10.0 * 8.337162971496582
Epoch 1550, val loss: 0.4692579507827759
Epoch 1560, training loss: 83.7293472290039 = 0.32809290289878845 + 10.0 * 8.34012508392334
Epoch 1560, val loss: 0.4690467119216919
Epoch 1570, training loss: 83.70263671875 = 0.32672449946403503 + 10.0 * 8.337591171264648
Epoch 1570, val loss: 0.4689256548881531
Epoch 1580, training loss: 83.7436752319336 = 0.32535722851753235 + 10.0 * 8.341832160949707
Epoch 1580, val loss: 0.46871060132980347
Epoch 1590, training loss: 83.69419860839844 = 0.323986291885376 + 10.0 * 8.337020874023438
Epoch 1590, val loss: 0.4684349596500397
Epoch 1600, training loss: 83.67658233642578 = 0.3226401209831238 + 10.0 * 8.335393905639648
Epoch 1600, val loss: 0.46829983592033386
Epoch 1610, training loss: 83.6693115234375 = 0.321298211812973 + 10.0 * 8.33480167388916
Epoch 1610, val loss: 0.4680870473384857
Epoch 1620, training loss: 83.66503143310547 = 0.31995463371276855 + 10.0 * 8.334507942199707
Epoch 1620, val loss: 0.4678465723991394
Epoch 1630, training loss: 83.72576904296875 = 0.3186192512512207 + 10.0 * 8.340715408325195
Epoch 1630, val loss: 0.4674384295940399
Epoch 1640, training loss: 83.7093276977539 = 0.3172660171985626 + 10.0 * 8.339205741882324
Epoch 1640, val loss: 0.4678615629673004
Epoch 1650, training loss: 83.67599487304688 = 0.31592032313346863 + 10.0 * 8.336008071899414
Epoch 1650, val loss: 0.4671783149242401
Epoch 1660, training loss: 83.65589904785156 = 0.3145900368690491 + 10.0 * 8.334131240844727
Epoch 1660, val loss: 0.4672759175300598
Epoch 1670, training loss: 83.67195129394531 = 0.31327223777770996 + 10.0 * 8.335867881774902
Epoch 1670, val loss: 0.4672883152961731
Epoch 1680, training loss: 83.65213012695312 = 0.3119431436061859 + 10.0 * 8.33401870727539
Epoch 1680, val loss: 0.4670438766479492
Epoch 1690, training loss: 83.65164184570312 = 0.31062427163124084 + 10.0 * 8.334101676940918
Epoch 1690, val loss: 0.4669969975948334
Epoch 1700, training loss: 83.64120483398438 = 0.30930498242378235 + 10.0 * 8.333189964294434
Epoch 1700, val loss: 0.46698805689811707
Epoch 1710, training loss: 83.65209197998047 = 0.30799221992492676 + 10.0 * 8.334409713745117
Epoch 1710, val loss: 0.46695175766944885
Epoch 1720, training loss: 83.65508270263672 = 0.30666443705558777 + 10.0 * 8.33484172821045
Epoch 1720, val loss: 0.46691805124282837
Epoch 1730, training loss: 83.63973236083984 = 0.30533847212791443 + 10.0 * 8.333439826965332
Epoch 1730, val loss: 0.4667090177536011
Epoch 1740, training loss: 83.62444305419922 = 0.30401915311813354 + 10.0 * 8.332042694091797
Epoch 1740, val loss: 0.46664875745773315
Epoch 1750, training loss: 83.62480926513672 = 0.30270424485206604 + 10.0 * 8.332210540771484
Epoch 1750, val loss: 0.4666944444179535
Epoch 1760, training loss: 83.6491470336914 = 0.301384836435318 + 10.0 * 8.334775924682617
Epoch 1760, val loss: 0.46667030453681946
Epoch 1770, training loss: 83.62348937988281 = 0.30006009340286255 + 10.0 * 8.332343101501465
Epoch 1770, val loss: 0.46681562066078186
Epoch 1780, training loss: 83.61241149902344 = 0.2987356185913086 + 10.0 * 8.331367492675781
Epoch 1780, val loss: 0.4667557179927826
Epoch 1790, training loss: 83.60556030273438 = 0.2974150478839874 + 10.0 * 8.330814361572266
Epoch 1790, val loss: 0.46683308482170105
Epoch 1800, training loss: 83.59832000732422 = 0.2960908114910126 + 10.0 * 8.330223083496094
Epoch 1800, val loss: 0.46690818667411804
Epoch 1810, training loss: 83.6283950805664 = 0.2947694659233093 + 10.0 * 8.333362579345703
Epoch 1810, val loss: 0.4669928550720215
Epoch 1820, training loss: 83.61074829101562 = 0.2934294044971466 + 10.0 * 8.331731796264648
Epoch 1820, val loss: 0.467013418674469
Epoch 1830, training loss: 83.59538269042969 = 0.29210254549980164 + 10.0 * 8.330327987670898
Epoch 1830, val loss: 0.4672832489013672
Epoch 1840, training loss: 83.60039520263672 = 0.29077091813087463 + 10.0 * 8.330962181091309
Epoch 1840, val loss: 0.4673195779323578
Epoch 1850, training loss: 83.60587310791016 = 0.2894468605518341 + 10.0 * 8.331643104553223
Epoch 1850, val loss: 0.4676044285297394
Epoch 1860, training loss: 83.596435546875 = 0.28811413049697876 + 10.0 * 8.330831527709961
Epoch 1860, val loss: 0.4675651490688324
Epoch 1870, training loss: 83.58470153808594 = 0.2867850959300995 + 10.0 * 8.329792022705078
Epoch 1870, val loss: 0.4677576422691345
Epoch 1880, training loss: 83.5884780883789 = 0.2854521572589874 + 10.0 * 8.330302238464355
Epoch 1880, val loss: 0.4677750766277313
Epoch 1890, training loss: 83.58355712890625 = 0.2841200828552246 + 10.0 * 8.329943656921387
Epoch 1890, val loss: 0.4679690897464752
Epoch 1900, training loss: 83.58572387695312 = 0.2827896177768707 + 10.0 * 8.330293655395508
Epoch 1900, val loss: 0.4681348204612732
Epoch 1910, training loss: 83.56470489501953 = 0.2814575135707855 + 10.0 * 8.328325271606445
Epoch 1910, val loss: 0.4684881567955017
Epoch 1920, training loss: 83.5627670288086 = 0.28012892603874207 + 10.0 * 8.328264236450195
Epoch 1920, val loss: 0.4687415361404419
Epoch 1930, training loss: 83.5675277709961 = 0.2788030505180359 + 10.0 * 8.328872680664062
Epoch 1930, val loss: 0.46908700466156006
Epoch 1940, training loss: 83.59061431884766 = 0.27746665477752686 + 10.0 * 8.331315040588379
Epoch 1940, val loss: 0.4692176580429077
Epoch 1950, training loss: 83.5798110961914 = 0.2761209309101105 + 10.0 * 8.330368995666504
Epoch 1950, val loss: 0.46895551681518555
Epoch 1960, training loss: 83.55348205566406 = 0.2747820317745209 + 10.0 * 8.32787036895752
Epoch 1960, val loss: 0.46966177225112915
Epoch 1970, training loss: 83.54411315917969 = 0.2734350562095642 + 10.0 * 8.327067375183105
Epoch 1970, val loss: 0.4696860611438751
Epoch 1980, training loss: 83.60233306884766 = 0.2721017003059387 + 10.0 * 8.333023071289062
Epoch 1980, val loss: 0.47021201252937317
Epoch 1990, training loss: 83.55587768554688 = 0.27074211835861206 + 10.0 * 8.328513145446777
Epoch 1990, val loss: 0.46994104981422424
Epoch 2000, training loss: 83.58665466308594 = 0.2693898379802704 + 10.0 * 8.331727027893066
Epoch 2000, val loss: 0.4703103303909302
Epoch 2010, training loss: 83.53177642822266 = 0.26803064346313477 + 10.0 * 8.326375007629395
Epoch 2010, val loss: 0.4707348346710205
Epoch 2020, training loss: 83.5239028930664 = 0.2666774392127991 + 10.0 * 8.325722694396973
Epoch 2020, val loss: 0.4709382653236389
Epoch 2030, training loss: 83.51992797851562 = 0.26531922817230225 + 10.0 * 8.325460433959961
Epoch 2030, val loss: 0.47129932045936584
Epoch 2040, training loss: 83.51734161376953 = 0.26395416259765625 + 10.0 * 8.325338363647461
Epoch 2040, val loss: 0.4716717004776001
Epoch 2050, training loss: 83.5500259399414 = 0.26259711384773254 + 10.0 * 8.328742980957031
Epoch 2050, val loss: 0.4720609188079834
Epoch 2060, training loss: 83.53126525878906 = 0.2612118422985077 + 10.0 * 8.327005386352539
Epoch 2060, val loss: 0.47210827469825745
Epoch 2070, training loss: 83.51664733886719 = 0.25984281301498413 + 10.0 * 8.32568073272705
Epoch 2070, val loss: 0.47272786498069763
Epoch 2080, training loss: 83.50823974609375 = 0.25846347212791443 + 10.0 * 8.32497787475586
Epoch 2080, val loss: 0.47284865379333496
Epoch 2090, training loss: 83.5133285522461 = 0.2570953071117401 + 10.0 * 8.325623512268066
Epoch 2090, val loss: 0.47315314412117004
Epoch 2100, training loss: 83.55028533935547 = 0.2557251453399658 + 10.0 * 8.329456329345703
Epoch 2100, val loss: 0.47344329953193665
Epoch 2110, training loss: 83.50881958007812 = 0.2543452978134155 + 10.0 * 8.325447082519531
Epoch 2110, val loss: 0.4741145372390747
Epoch 2120, training loss: 83.49266815185547 = 0.25295764207839966 + 10.0 * 8.323970794677734
Epoch 2120, val loss: 0.4746100604534149
Epoch 2130, training loss: 83.48693084716797 = 0.2515797019004822 + 10.0 * 8.323534965515137
Epoch 2130, val loss: 0.47496265172958374
Epoch 2140, training loss: 83.48594665527344 = 0.25019893050193787 + 10.0 * 8.323575019836426
Epoch 2140, val loss: 0.4754321873188019
Epoch 2150, training loss: 83.55975341796875 = 0.24882951378822327 + 10.0 * 8.33109188079834
Epoch 2150, val loss: 0.4762347936630249
Epoch 2160, training loss: 83.55040740966797 = 0.2474282830953598 + 10.0 * 8.330297470092773
Epoch 2160, val loss: 0.4763578772544861
Epoch 2170, training loss: 83.47916412353516 = 0.24602706730365753 + 10.0 * 8.32331371307373
Epoch 2170, val loss: 0.47685059905052185
Epoch 2180, training loss: 83.47476196289062 = 0.24463534355163574 + 10.0 * 8.323012351989746
Epoch 2180, val loss: 0.4777071177959442
Epoch 2190, training loss: 83.47117614746094 = 0.24324549734592438 + 10.0 * 8.322793006896973
Epoch 2190, val loss: 0.47816458344459534
Epoch 2200, training loss: 83.47870635986328 = 0.24185751378536224 + 10.0 * 8.323684692382812
Epoch 2200, val loss: 0.47877395153045654
Epoch 2210, training loss: 83.50774383544922 = 0.24046865105628967 + 10.0 * 8.326726913452148
Epoch 2210, val loss: 0.47941890358924866
Epoch 2220, training loss: 83.48240661621094 = 0.2390631139278412 + 10.0 * 8.324335098266602
Epoch 2220, val loss: 0.4797245264053345
Epoch 2230, training loss: 83.46521759033203 = 0.23766399919986725 + 10.0 * 8.322755813598633
Epoch 2230, val loss: 0.48047488927841187
Epoch 2240, training loss: 83.4633560180664 = 0.2362617701292038 + 10.0 * 8.322710037231445
Epoch 2240, val loss: 0.48111453652381897
Epoch 2250, training loss: 83.46354675292969 = 0.23485811054706573 + 10.0 * 8.322869300842285
Epoch 2250, val loss: 0.4817904233932495
Epoch 2260, training loss: 83.48452758789062 = 0.23345087468624115 + 10.0 * 8.32510757446289
Epoch 2260, val loss: 0.4825134873390198
Epoch 2270, training loss: 83.45407104492188 = 0.23204191029071808 + 10.0 * 8.322202682495117
Epoch 2270, val loss: 0.48355406522750854
Epoch 2280, training loss: 83.45756530761719 = 0.23063288629055023 + 10.0 * 8.32269287109375
Epoch 2280, val loss: 0.4845271706581116
Epoch 2290, training loss: 83.45417022705078 = 0.2292184829711914 + 10.0 * 8.322495460510254
Epoch 2290, val loss: 0.4849596321582794
Epoch 2300, training loss: 83.44293975830078 = 0.2278059422969818 + 10.0 * 8.321513175964355
Epoch 2300, val loss: 0.48561689257621765
Epoch 2310, training loss: 83.48832702636719 = 0.22640782594680786 + 10.0 * 8.326191902160645
Epoch 2310, val loss: 0.48598113656044006
Epoch 2320, training loss: 83.44501495361328 = 0.22498540580272675 + 10.0 * 8.322003364562988
Epoch 2320, val loss: 0.48744627833366394
Epoch 2330, training loss: 83.42937469482422 = 0.2235710322856903 + 10.0 * 8.32058048248291
Epoch 2330, val loss: 0.4880334734916687
Epoch 2340, training loss: 83.42695617675781 = 0.2221517711877823 + 10.0 * 8.320480346679688
Epoch 2340, val loss: 0.48890846967697144
Epoch 2350, training loss: 83.49899291992188 = 0.22075098752975464 + 10.0 * 8.327824592590332
Epoch 2350, val loss: 0.4897463619709015
Epoch 2360, training loss: 83.44625854492188 = 0.21931785345077515 + 10.0 * 8.322694778442383
Epoch 2360, val loss: 0.4905712902545929
Epoch 2370, training loss: 83.42027282714844 = 0.21789877116680145 + 10.0 * 8.32023811340332
Epoch 2370, val loss: 0.4915561079978943
Epoch 2380, training loss: 83.41094207763672 = 0.21647845208644867 + 10.0 * 8.319446563720703
Epoch 2380, val loss: 0.4925364851951599
Epoch 2390, training loss: 83.41443634033203 = 0.21506406366825104 + 10.0 * 8.319936752319336
Epoch 2390, val loss: 0.4936155378818512
Epoch 2400, training loss: 83.46701049804688 = 0.2136722207069397 + 10.0 * 8.325334548950195
Epoch 2400, val loss: 0.4949139952659607
Epoch 2410, training loss: 83.42741394042969 = 0.21223865449428558 + 10.0 * 8.321516990661621
Epoch 2410, val loss: 0.4951320290565491
Epoch 2420, training loss: 83.42493438720703 = 0.2108282893896103 + 10.0 * 8.321410179138184
Epoch 2420, val loss: 0.4966689646244049
Epoch 2430, training loss: 83.41099548339844 = 0.2094031274318695 + 10.0 * 8.320158958435059
Epoch 2430, val loss: 0.49745962023735046
Epoch 2440, training loss: 83.42534637451172 = 0.2079908698797226 + 10.0 * 8.321735382080078
Epoch 2440, val loss: 0.49845513701438904
Epoch 2450, training loss: 83.39961242675781 = 0.20656652748584747 + 10.0 * 8.319304466247559
Epoch 2450, val loss: 0.49928033351898193
Epoch 2460, training loss: 83.39419555664062 = 0.2051495760679245 + 10.0 * 8.318904876708984
Epoch 2460, val loss: 0.5002793669700623
Epoch 2470, training loss: 83.38502502441406 = 0.20372822880744934 + 10.0 * 8.318129539489746
Epoch 2470, val loss: 0.5014716386795044
Epoch 2480, training loss: 83.3828125 = 0.2023104578256607 + 10.0 * 8.318050384521484
Epoch 2480, val loss: 0.5024220943450928
Epoch 2490, training loss: 83.43683624267578 = 0.200899139046669 + 10.0 * 8.323594093322754
Epoch 2490, val loss: 0.5036367774009705
Epoch 2500, training loss: 83.4034194946289 = 0.19947132468223572 + 10.0 * 8.320394515991211
Epoch 2500, val loss: 0.5040751099586487
Epoch 2510, training loss: 83.38441467285156 = 0.19804026186466217 + 10.0 * 8.318636894226074
Epoch 2510, val loss: 0.5058452486991882
Epoch 2520, training loss: 83.39100646972656 = 0.19661454856395721 + 10.0 * 8.319438934326172
Epoch 2520, val loss: 0.5068941116333008
Epoch 2530, training loss: 83.40054321289062 = 0.1951940506696701 + 10.0 * 8.320535659790039
Epoch 2530, val loss: 0.5079407095909119
Epoch 2540, training loss: 83.3785171508789 = 0.19376637041568756 + 10.0 * 8.318475723266602
Epoch 2540, val loss: 0.5084863901138306
Epoch 2550, training loss: 83.35948181152344 = 0.19233623147010803 + 10.0 * 8.3167142868042
Epoch 2550, val loss: 0.5100114345550537
Epoch 2560, training loss: 83.36544799804688 = 0.19091466069221497 + 10.0 * 8.317453384399414
Epoch 2560, val loss: 0.5111531615257263
Epoch 2570, training loss: 83.43421173095703 = 0.1894962638616562 + 10.0 * 8.324471473693848
Epoch 2570, val loss: 0.5121487379074097
Epoch 2580, training loss: 83.3709945678711 = 0.1880599558353424 + 10.0 * 8.318293571472168
Epoch 2580, val loss: 0.5137000679969788
Epoch 2590, training loss: 83.35521697998047 = 0.1866234540939331 + 10.0 * 8.316859245300293
Epoch 2590, val loss: 0.5147533416748047
Epoch 2600, training loss: 83.35389709472656 = 0.185198113322258 + 10.0 * 8.316869735717773
Epoch 2600, val loss: 0.5162240862846375
Epoch 2610, training loss: 83.38806915283203 = 0.18379579484462738 + 10.0 * 8.320427894592285
Epoch 2610, val loss: 0.517754852771759
Epoch 2620, training loss: 83.36353302001953 = 0.18234142661094666 + 10.0 * 8.318119049072266
Epoch 2620, val loss: 0.5181701183319092
Epoch 2630, training loss: 83.35552978515625 = 0.1809253692626953 + 10.0 * 8.317461013793945
Epoch 2630, val loss: 0.520065188407898
Epoch 2640, training loss: 83.33789825439453 = 0.17949697375297546 + 10.0 * 8.315839767456055
Epoch 2640, val loss: 0.5210413336753845
Epoch 2650, training loss: 83.3416519165039 = 0.17807894945144653 + 10.0 * 8.316357612609863
Epoch 2650, val loss: 0.5226699709892273
Epoch 2660, training loss: 83.38317108154297 = 0.17667526006698608 + 10.0 * 8.320650100708008
Epoch 2660, val loss: 0.5239396691322327
Epoch 2670, training loss: 83.34854888916016 = 0.17525030672550201 + 10.0 * 8.317330360412598
Epoch 2670, val loss: 0.5247851014137268
Epoch 2680, training loss: 83.3307876586914 = 0.1738353818655014 + 10.0 * 8.315694808959961
Epoch 2680, val loss: 0.5268189907073975
Epoch 2690, training loss: 83.32198333740234 = 0.17241595685482025 + 10.0 * 8.314956665039062
Epoch 2690, val loss: 0.5276287794113159
Epoch 2700, training loss: 83.3222885131836 = 0.17100287973880768 + 10.0 * 8.315128326416016
Epoch 2700, val loss: 0.5289592742919922
Epoch 2710, training loss: 83.3962631225586 = 0.16965100169181824 + 10.0 * 8.322661399841309
Epoch 2710, val loss: 0.5297525525093079
Epoch 2720, training loss: 83.35812377929688 = 0.16820409893989563 + 10.0 * 8.318991661071777
Epoch 2720, val loss: 0.5331071019172668
Epoch 2730, training loss: 83.33747100830078 = 0.1667858064174652 + 10.0 * 8.317068099975586
Epoch 2730, val loss: 0.5333206057548523
Epoch 2740, training loss: 83.32439422607422 = 0.16537930071353912 + 10.0 * 8.315900802612305
Epoch 2740, val loss: 0.5353741645812988
Epoch 2750, training loss: 83.307373046875 = 0.1639736443758011 + 10.0 * 8.314340591430664
Epoch 2750, val loss: 0.536851704120636
Epoch 2760, training loss: 83.30619812011719 = 0.1625714898109436 + 10.0 * 8.314362525939941
Epoch 2760, val loss: 0.5384039282798767
Epoch 2770, training loss: 83.3236083984375 = 0.1611887663602829 + 10.0 * 8.316242218017578
Epoch 2770, val loss: 0.5402964949607849
Epoch 2780, training loss: 83.32167053222656 = 0.15979163348674774 + 10.0 * 8.316187858581543
Epoch 2780, val loss: 0.5418667793273926
Epoch 2790, training loss: 83.3158187866211 = 0.15840306878089905 + 10.0 * 8.315741539001465
Epoch 2790, val loss: 0.5432434678077698
Epoch 2800, training loss: 83.314208984375 = 0.15701663494110107 + 10.0 * 8.315719604492188
Epoch 2800, val loss: 0.544593870639801
Epoch 2810, training loss: 83.2951431274414 = 0.15563292801380157 + 10.0 * 8.31395149230957
Epoch 2810, val loss: 0.546328604221344
Epoch 2820, training loss: 83.29000091552734 = 0.15425369143486023 + 10.0 * 8.31357479095459
Epoch 2820, val loss: 0.5479872822761536
Epoch 2830, training loss: 83.293701171875 = 0.15288150310516357 + 10.0 * 8.314082145690918
Epoch 2830, val loss: 0.549390971660614
Epoch 2840, training loss: 83.31867980957031 = 0.15151679515838623 + 10.0 * 8.316716194152832
Epoch 2840, val loss: 0.551181972026825
Epoch 2850, training loss: 83.316650390625 = 0.15015053749084473 + 10.0 * 8.316649436950684
Epoch 2850, val loss: 0.5528170466423035
Epoch 2860, training loss: 83.2878189086914 = 0.14877842366695404 + 10.0 * 8.31390380859375
Epoch 2860, val loss: 0.5540772676467896
Epoch 2870, training loss: 83.29240417480469 = 0.14742505550384521 + 10.0 * 8.314497947692871
Epoch 2870, val loss: 0.556634783744812
Epoch 2880, training loss: 83.3018569946289 = 0.1460692584514618 + 10.0 * 8.31557846069336
Epoch 2880, val loss: 0.5579823851585388
Epoch 2890, training loss: 83.2789306640625 = 0.14470915496349335 + 10.0 * 8.313422203063965
Epoch 2890, val loss: 0.5595800876617432
Epoch 2900, training loss: 83.27322387695312 = 0.14336848258972168 + 10.0 * 8.31298542022705
Epoch 2900, val loss: 0.561657190322876
Epoch 2910, training loss: 83.27666473388672 = 0.14203768968582153 + 10.0 * 8.31346321105957
Epoch 2910, val loss: 0.5633096098899841
Epoch 2920, training loss: 83.29629516601562 = 0.1407150775194168 + 10.0 * 8.315557479858398
Epoch 2920, val loss: 0.5653173327445984
Epoch 2930, training loss: 83.26829528808594 = 0.1393720954656601 + 10.0 * 8.312891960144043
Epoch 2930, val loss: 0.5663714408874512
Epoch 2940, training loss: 83.27847290039062 = 0.13806693255901337 + 10.0 * 8.314040184020996
Epoch 2940, val loss: 0.5679504871368408
Epoch 2950, training loss: 83.29695129394531 = 0.1367494761943817 + 10.0 * 8.316020011901855
Epoch 2950, val loss: 0.5707219839096069
Epoch 2960, training loss: 83.27468872070312 = 0.13543358445167542 + 10.0 * 8.313924789428711
Epoch 2960, val loss: 0.572612464427948
Epoch 2970, training loss: 83.25716400146484 = 0.13413475453853607 + 10.0 * 8.31230354309082
Epoch 2970, val loss: 0.5742816925048828
Epoch 2980, training loss: 83.26002502441406 = 0.13284222781658173 + 10.0 * 8.312718391418457
Epoch 2980, val loss: 0.576134979724884
Epoch 2990, training loss: 83.28666687011719 = 0.13157008588314056 + 10.0 * 8.315509796142578
Epoch 2990, val loss: 0.578719973564148
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8046676813800101
0.8411939433456496
=== training gcn model ===
Epoch 0, training loss: 106.90963745117188 = 1.0871256589889526 + 10.0 * 10.582250595092773
Epoch 0, val loss: 1.087024450302124
Epoch 10, training loss: 106.89514923095703 = 1.0814430713653564 + 10.0 * 10.58137035369873
Epoch 10, val loss: 1.081373929977417
Epoch 20, training loss: 106.81349182128906 = 1.07509183883667 + 10.0 * 10.573840141296387
Epoch 20, val loss: 1.0750179290771484
Epoch 30, training loss: 106.28623962402344 = 1.0679619312286377 + 10.0 * 10.521827697753906
Epoch 30, val loss: 1.0678677558898926
Epoch 40, training loss: 104.18510437011719 = 1.060535192489624 + 10.0 * 10.312457084655762
Epoch 40, val loss: 1.0606564283370972
Epoch 50, training loss: 99.18557739257812 = 1.053975224494934 + 10.0 * 9.813159942626953
Epoch 50, val loss: 1.0540885925292969
Epoch 60, training loss: 96.32852935791016 = 1.0458632707595825 + 10.0 * 9.528266906738281
Epoch 60, val loss: 1.0460094213485718
Epoch 70, training loss: 94.84442138671875 = 1.0366202592849731 + 10.0 * 9.380780220031738
Epoch 70, val loss: 1.0372130870819092
Epoch 80, training loss: 92.8234634399414 = 1.0290483236312866 + 10.0 * 9.179441452026367
Epoch 80, val loss: 1.0301716327667236
Epoch 90, training loss: 91.61991119384766 = 1.0223132371902466 + 10.0 * 9.059759140014648
Epoch 90, val loss: 1.023825764656067
Epoch 100, training loss: 90.76512145996094 = 1.0153000354766846 + 10.0 * 8.974982261657715
Epoch 100, val loss: 1.0171693563461304
Epoch 110, training loss: 90.32808685302734 = 1.0077078342437744 + 10.0 * 8.932038307189941
Epoch 110, val loss: 1.009832739830017
Epoch 120, training loss: 89.64167022705078 = 1.0000461339950562 + 10.0 * 8.86416244506836
Epoch 120, val loss: 1.0024770498275757
Epoch 130, training loss: 88.74470520019531 = 0.995026171207428 + 10.0 * 8.774968147277832
Epoch 130, val loss: 0.997884213924408
Epoch 140, training loss: 88.12523651123047 = 0.9907699227333069 + 10.0 * 8.713446617126465
Epoch 140, val loss: 0.9937860369682312
Epoch 150, training loss: 87.72122192382812 = 0.9832155108451843 + 10.0 * 8.673800468444824
Epoch 150, val loss: 0.9862819910049438
Epoch 160, training loss: 87.44681549072266 = 0.973490834236145 + 10.0 * 8.647333145141602
Epoch 160, val loss: 0.9769066572189331
Epoch 170, training loss: 87.23538970947266 = 0.9629161357879639 + 10.0 * 8.62724781036377
Epoch 170, val loss: 0.966769278049469
Epoch 180, training loss: 87.02310180664062 = 0.951352059841156 + 10.0 * 8.60717487335205
Epoch 180, val loss: 0.9557862877845764
Epoch 190, training loss: 86.84284973144531 = 0.9387306571006775 + 10.0 * 8.590412139892578
Epoch 190, val loss: 0.9437938928604126
Epoch 200, training loss: 86.67401885986328 = 0.9252082705497742 + 10.0 * 8.574880599975586
Epoch 200, val loss: 0.9308663606643677
Epoch 210, training loss: 86.51360321044922 = 0.9109447598457336 + 10.0 * 8.560266494750977
Epoch 210, val loss: 0.9171921610832214
Epoch 220, training loss: 86.34884643554688 = 0.8959611654281616 + 10.0 * 8.545289039611816
Epoch 220, val loss: 0.9029944539070129
Epoch 230, training loss: 86.21609497070312 = 0.8803157806396484 + 10.0 * 8.533577919006348
Epoch 230, val loss: 0.8880849480628967
Epoch 240, training loss: 86.09469604492188 = 0.8639060258865356 + 10.0 * 8.523078918457031
Epoch 240, val loss: 0.8725540637969971
Epoch 250, training loss: 85.9788589477539 = 0.8470069766044617 + 10.0 * 8.513185501098633
Epoch 250, val loss: 0.856652021408081
Epoch 260, training loss: 85.86005401611328 = 0.8299983739852905 + 10.0 * 8.503005027770996
Epoch 260, val loss: 0.8405331373214722
Epoch 270, training loss: 85.76252746582031 = 0.8126587271690369 + 10.0 * 8.494986534118652
Epoch 270, val loss: 0.8242388367652893
Epoch 280, training loss: 85.718994140625 = 0.7951367497444153 + 10.0 * 8.492385864257812
Epoch 280, val loss: 0.8078561425209045
Epoch 290, training loss: 85.6103286743164 = 0.7774900197982788 + 10.0 * 8.483283996582031
Epoch 290, val loss: 0.7912777066230774
Epoch 300, training loss: 85.51934051513672 = 0.7600793838500977 + 10.0 * 8.475926399230957
Epoch 300, val loss: 0.7750778198242188
Epoch 310, training loss: 85.43638610839844 = 0.7430227994918823 + 10.0 * 8.46933650970459
Epoch 310, val loss: 0.7591636776924133
Epoch 320, training loss: 85.37754821777344 = 0.7262240052223206 + 10.0 * 8.465131759643555
Epoch 320, val loss: 0.7435425519943237
Epoch 330, training loss: 85.34635925292969 = 0.7094828486442566 + 10.0 * 8.463687896728516
Epoch 330, val loss: 0.7281952500343323
Epoch 340, training loss: 85.26300811767578 = 0.6934911012649536 + 10.0 * 8.456952095031738
Epoch 340, val loss: 0.7134603261947632
Epoch 350, training loss: 85.19303894042969 = 0.6781864166259766 + 10.0 * 8.451485633850098
Epoch 350, val loss: 0.6994584798812866
Epoch 360, training loss: 85.13275909423828 = 0.6634615659713745 + 10.0 * 8.446929931640625
Epoch 360, val loss: 0.6860447525978088
Epoch 370, training loss: 85.08182525634766 = 0.6494076251983643 + 10.0 * 8.443242073059082
Epoch 370, val loss: 0.673351526260376
Epoch 380, training loss: 85.03390502929688 = 0.636063814163208 + 10.0 * 8.439784049987793
Epoch 380, val loss: 0.6613279581069946
Epoch 390, training loss: 85.00843048095703 = 0.623329758644104 + 10.0 * 8.438509941101074
Epoch 390, val loss: 0.6499767303466797
Epoch 400, training loss: 84.96092987060547 = 0.6113982200622559 + 10.0 * 8.434953689575195
Epoch 400, val loss: 0.6394252777099609
Epoch 410, training loss: 84.90352630615234 = 0.6004306077957153 + 10.0 * 8.430309295654297
Epoch 410, val loss: 0.6297518014907837
Epoch 420, training loss: 84.85801696777344 = 0.5902173519134521 + 10.0 * 8.426779747009277
Epoch 420, val loss: 0.620840311050415
Epoch 430, training loss: 84.8609619140625 = 0.5806733965873718 + 10.0 * 8.42802906036377
Epoch 430, val loss: 0.612718403339386
Epoch 440, training loss: 84.8031997680664 = 0.571727454662323 + 10.0 * 8.423147201538086
Epoch 440, val loss: 0.6050344705581665
Epoch 450, training loss: 84.75823974609375 = 0.5634871125221252 + 10.0 * 8.419475555419922
Epoch 450, val loss: 0.5981648564338684
Epoch 460, training loss: 84.75550079345703 = 0.555840253829956 + 10.0 * 8.419965744018555
Epoch 460, val loss: 0.5917569398880005
Epoch 470, training loss: 84.68990325927734 = 0.5487518310546875 + 10.0 * 8.414114952087402
Epoch 470, val loss: 0.5859737992286682
Epoch 480, training loss: 84.6504898071289 = 0.5421711206436157 + 10.0 * 8.410832405090332
Epoch 480, val loss: 0.5806379914283752
Epoch 490, training loss: 84.68910217285156 = 0.5360355973243713 + 10.0 * 8.41530704498291
Epoch 490, val loss: 0.575717031955719
Epoch 500, training loss: 84.59413146972656 = 0.5301811695098877 + 10.0 * 8.406394958496094
Epoch 500, val loss: 0.5711150765419006
Epoch 510, training loss: 84.5656509399414 = 0.5247761011123657 + 10.0 * 8.404088020324707
Epoch 510, val loss: 0.5669052004814148
Epoch 520, training loss: 84.53529357910156 = 0.519726574420929 + 10.0 * 8.401556015014648
Epoch 520, val loss: 0.5629705190658569
Epoch 530, training loss: 84.54266357421875 = 0.5149407982826233 + 10.0 * 8.402772903442383
Epoch 530, val loss: 0.5593404173851013
Epoch 540, training loss: 84.5392074584961 = 0.5102457404136658 + 10.0 * 8.4028959274292
Epoch 540, val loss: 0.5557690858840942
Epoch 550, training loss: 84.46479797363281 = 0.5058706998825073 + 10.0 * 8.395893096923828
Epoch 550, val loss: 0.5524929761886597
Epoch 560, training loss: 84.43688201904297 = 0.5017619729042053 + 10.0 * 8.393511772155762
Epoch 560, val loss: 0.5493115782737732
Epoch 570, training loss: 84.41553497314453 = 0.49783387780189514 + 10.0 * 8.391770362854004
Epoch 570, val loss: 0.5464493036270142
Epoch 580, training loss: 84.44417572021484 = 0.49406149983406067 + 10.0 * 8.395010948181152
Epoch 580, val loss: 0.5435595512390137
Epoch 590, training loss: 84.38834381103516 = 0.49029597640037537 + 10.0 * 8.38980484008789
Epoch 590, val loss: 0.5407688617706299
Epoch 600, training loss: 84.35469055175781 = 0.48674073815345764 + 10.0 * 8.386795043945312
Epoch 600, val loss: 0.5381258726119995
Epoch 610, training loss: 84.33483123779297 = 0.4833563268184662 + 10.0 * 8.385148048400879
Epoch 610, val loss: 0.5355349183082581
Epoch 620, training loss: 84.31753540039062 = 0.4800717234611511 + 10.0 * 8.383746147155762
Epoch 620, val loss: 0.5331025123596191
Epoch 630, training loss: 84.38045501708984 = 0.4768467843532562 + 10.0 * 8.390360832214355
Epoch 630, val loss: 0.5306535959243774
Epoch 640, training loss: 84.2862319946289 = 0.47367000579833984 + 10.0 * 8.381256103515625
Epoch 640, val loss: 0.5283081531524658
Epoch 650, training loss: 84.28394317626953 = 0.4706490933895111 + 10.0 * 8.381329536437988
Epoch 650, val loss: 0.5259599685668945
Epoch 660, training loss: 84.28355407714844 = 0.46770086884498596 + 10.0 * 8.381586074829102
Epoch 660, val loss: 0.5237282514572144
Epoch 670, training loss: 84.24603271484375 = 0.46481359004974365 + 10.0 * 8.378122329711914
Epoch 670, val loss: 0.521577775478363
Epoch 680, training loss: 84.22897338867188 = 0.4620170593261719 + 10.0 * 8.37669563293457
Epoch 680, val loss: 0.5194254517555237
Epoch 690, training loss: 84.26097106933594 = 0.45927736163139343 + 10.0 * 8.380169868469238
Epoch 690, val loss: 0.5173189043998718
Epoch 700, training loss: 84.21324157714844 = 0.45656919479370117 + 10.0 * 8.375667572021484
Epoch 700, val loss: 0.5152872204780579
Epoch 710, training loss: 84.18990325927734 = 0.45395907759666443 + 10.0 * 8.373594284057617
Epoch 710, val loss: 0.5132328271865845
Epoch 720, training loss: 84.22100830078125 = 0.45141592621803284 + 10.0 * 8.376958847045898
Epoch 720, val loss: 0.511277437210083
Epoch 730, training loss: 84.17691040039062 = 0.4488857090473175 + 10.0 * 8.372802734375
Epoch 730, val loss: 0.5093846321105957
Epoch 740, training loss: 84.21182250976562 = 0.4464268088340759 + 10.0 * 8.37653923034668
Epoch 740, val loss: 0.5074300765991211
Epoch 750, training loss: 84.15643310546875 = 0.4440023601055145 + 10.0 * 8.371243476867676
Epoch 750, val loss: 0.5056132078170776
Epoch 760, training loss: 84.12448120117188 = 0.4416791498661041 + 10.0 * 8.368280410766602
Epoch 760, val loss: 0.5038108825683594
Epoch 770, training loss: 84.10891723632812 = 0.43941250443458557 + 10.0 * 8.366950035095215
Epoch 770, val loss: 0.502099335193634
Epoch 780, training loss: 84.10369110107422 = 0.4371753931045532 + 10.0 * 8.36665153503418
Epoch 780, val loss: 0.5003957748413086
Epoch 790, training loss: 84.11610412597656 = 0.43493956327438354 + 10.0 * 8.36811637878418
Epoch 790, val loss: 0.498691588640213
Epoch 800, training loss: 84.08888244628906 = 0.43271562457084656 + 10.0 * 8.365616798400879
Epoch 800, val loss: 0.49703121185302734
Epoch 810, training loss: 84.0719223022461 = 0.4305666983127594 + 10.0 * 8.3641357421875
Epoch 810, val loss: 0.495373398065567
Epoch 820, training loss: 84.05404663085938 = 0.42848050594329834 + 10.0 * 8.362556457519531
Epoch 820, val loss: 0.49380239844322205
Epoch 830, training loss: 84.04846954345703 = 0.42642998695373535 + 10.0 * 8.362203598022461
Epoch 830, val loss: 0.4922761917114258
Epoch 840, training loss: 84.0699691772461 = 0.42438581585884094 + 10.0 * 8.364558219909668
Epoch 840, val loss: 0.49073418974876404
Epoch 850, training loss: 84.02594757080078 = 0.422370046377182 + 10.0 * 8.360357284545898
Epoch 850, val loss: 0.48926880955696106
Epoch 860, training loss: 84.10184478759766 = 0.4203951358795166 + 10.0 * 8.368144989013672
Epoch 860, val loss: 0.48779016733169556
Epoch 870, training loss: 84.02462005615234 = 0.41843023896217346 + 10.0 * 8.36061954498291
Epoch 870, val loss: 0.4864138662815094
Epoch 880, training loss: 83.9964370727539 = 0.41654253005981445 + 10.0 * 8.357989311218262
Epoch 880, val loss: 0.4849912226200104
Epoch 890, training loss: 83.98477172851562 = 0.4146882891654968 + 10.0 * 8.35700798034668
Epoch 890, val loss: 0.48367881774902344
Epoch 900, training loss: 84.04015350341797 = 0.41285133361816406 + 10.0 * 8.362730026245117
Epoch 900, val loss: 0.482374370098114
Epoch 910, training loss: 83.98860168457031 = 0.4109936058521271 + 10.0 * 8.357760429382324
Epoch 910, val loss: 0.4810771048069
Epoch 920, training loss: 83.95848846435547 = 0.40919041633605957 + 10.0 * 8.35492992401123
Epoch 920, val loss: 0.4797494411468506
Epoch 930, training loss: 83.94902801513672 = 0.4074241816997528 + 10.0 * 8.35416030883789
Epoch 930, val loss: 0.47854670882225037
Epoch 940, training loss: 83.94087219238281 = 0.405680775642395 + 10.0 * 8.353519439697266
Epoch 940, val loss: 0.47733360528945923
Epoch 950, training loss: 83.95928192138672 = 0.4039498567581177 + 10.0 * 8.3555326461792
Epoch 950, val loss: 0.4761624336242676
Epoch 960, training loss: 83.98898315429688 = 0.402191698551178 + 10.0 * 8.358678817749023
Epoch 960, val loss: 0.4749416708946228
Epoch 970, training loss: 83.93465423583984 = 0.40045660734176636 + 10.0 * 8.353419303894043
Epoch 970, val loss: 0.47375574707984924
Epoch 980, training loss: 83.91864013671875 = 0.39877986907958984 + 10.0 * 8.351985931396484
Epoch 980, val loss: 0.4726344347000122
Epoch 990, training loss: 83.9051284790039 = 0.39713215827941895 + 10.0 * 8.350799560546875
Epoch 990, val loss: 0.4715687036514282
Epoch 1000, training loss: 83.90535736083984 = 0.3954961895942688 + 10.0 * 8.35098648071289
Epoch 1000, val loss: 0.47051846981048584
Epoch 1010, training loss: 83.90799713134766 = 0.39385366439819336 + 10.0 * 8.351414680480957
Epoch 1010, val loss: 0.46943196654319763
Epoch 1020, training loss: 83.89266204833984 = 0.3922208547592163 + 10.0 * 8.350044250488281
Epoch 1020, val loss: 0.46838143467903137
Epoch 1030, training loss: 83.88150787353516 = 0.39060723781585693 + 10.0 * 8.349089622497559
Epoch 1030, val loss: 0.4673782289028168
Epoch 1040, training loss: 83.885498046875 = 0.38900846242904663 + 10.0 * 8.349649429321289
Epoch 1040, val loss: 0.46640846133232117
Epoch 1050, training loss: 83.86312103271484 = 0.38741400837898254 + 10.0 * 8.347570419311523
Epoch 1050, val loss: 0.46542373299598694
Epoch 1060, training loss: 83.86322021484375 = 0.3858366310596466 + 10.0 * 8.347738265991211
Epoch 1060, val loss: 0.4644760489463806
Epoch 1070, training loss: 83.89213562011719 = 0.38426199555397034 + 10.0 * 8.350787162780762
Epoch 1070, val loss: 0.4635606110095978
Epoch 1080, training loss: 83.88923645019531 = 0.3826800286769867 + 10.0 * 8.350655555725098
Epoch 1080, val loss: 0.4626203775405884
Epoch 1090, training loss: 83.85194396972656 = 0.3810921013355255 + 10.0 * 8.347084999084473
Epoch 1090, val loss: 0.4616754949092865
Epoch 1100, training loss: 83.84329223632812 = 0.3795335590839386 + 10.0 * 8.346376419067383
Epoch 1100, val loss: 0.4607734978199005
Epoch 1110, training loss: 83.82962036132812 = 0.37801414728164673 + 10.0 * 8.345160484313965
Epoch 1110, val loss: 0.45989423990249634
Epoch 1120, training loss: 83.8211669921875 = 0.3765030801296234 + 10.0 * 8.344466209411621
Epoch 1120, val loss: 0.459076464176178
Epoch 1130, training loss: 83.81685638427734 = 0.3749922811985016 + 10.0 * 8.344186782836914
Epoch 1130, val loss: 0.45824503898620605
Epoch 1140, training loss: 83.88709259033203 = 0.3734782040119171 + 10.0 * 8.351361274719238
Epoch 1140, val loss: 0.4574011266231537
Epoch 1150, training loss: 83.83236694335938 = 0.37193870544433594 + 10.0 * 8.34604263305664
Epoch 1150, val loss: 0.45660600066185
Epoch 1160, training loss: 83.80175018310547 = 0.3704257607460022 + 10.0 * 8.343132972717285
Epoch 1160, val loss: 0.4557834565639496
Epoch 1170, training loss: 83.79367065429688 = 0.3689282536506653 + 10.0 * 8.342473983764648
Epoch 1170, val loss: 0.4550088047981262
Epoch 1180, training loss: 83.79086303710938 = 0.3674340844154358 + 10.0 * 8.3423433303833
Epoch 1180, val loss: 0.4542721211910248
Epoch 1190, training loss: 83.93144226074219 = 0.3659328520298004 + 10.0 * 8.356550216674805
Epoch 1190, val loss: 0.45349785685539246
Epoch 1200, training loss: 83.81077575683594 = 0.364398717880249 + 10.0 * 8.344637870788574
Epoch 1200, val loss: 0.4527651071548462
Epoch 1210, training loss: 83.77914428710938 = 0.3628998100757599 + 10.0 * 8.34162425994873
Epoch 1210, val loss: 0.4520464837551117
Epoch 1220, training loss: 83.77245330810547 = 0.3614194095134735 + 10.0 * 8.341103553771973
Epoch 1220, val loss: 0.4513254165649414
Epoch 1230, training loss: 83.76689147949219 = 0.3599417209625244 + 10.0 * 8.34069538116455
Epoch 1230, val loss: 0.4506438672542572
Epoch 1240, training loss: 83.83032989501953 = 0.35845425724983215 + 10.0 * 8.347187995910645
Epoch 1240, val loss: 0.44996726512908936
Epoch 1250, training loss: 83.77730560302734 = 0.35695716738700867 + 10.0 * 8.342035293579102
Epoch 1250, val loss: 0.4492432773113251
Epoch 1260, training loss: 83.75521850585938 = 0.3554689288139343 + 10.0 * 8.339975357055664
Epoch 1260, val loss: 0.44860145449638367
Epoch 1270, training loss: 83.7658462524414 = 0.35399001836776733 + 10.0 * 8.341185569763184
Epoch 1270, val loss: 0.4479463994503021
Epoch 1280, training loss: 83.75079345703125 = 0.3525007665157318 + 10.0 * 8.339829444885254
Epoch 1280, val loss: 0.44733673334121704
Epoch 1290, training loss: 83.75624084472656 = 0.35101258754730225 + 10.0 * 8.340522766113281
Epoch 1290, val loss: 0.44673481583595276
Epoch 1300, training loss: 83.74169158935547 = 0.34952348470687866 + 10.0 * 8.339216232299805
Epoch 1300, val loss: 0.4461330473423004
Epoch 1310, training loss: 83.73046875 = 0.3480418622493744 + 10.0 * 8.338242530822754
Epoch 1310, val loss: 0.44555121660232544
Epoch 1320, training loss: 83.73492431640625 = 0.3465624749660492 + 10.0 * 8.338835716247559
Epoch 1320, val loss: 0.4449780583381653
Epoch 1330, training loss: 83.73741149902344 = 0.3450769782066345 + 10.0 * 8.3392333984375
Epoch 1330, val loss: 0.4444197714328766
Epoch 1340, training loss: 83.72168731689453 = 0.3435899019241333 + 10.0 * 8.337809562683105
Epoch 1340, val loss: 0.4438503384590149
Epoch 1350, training loss: 83.71653747558594 = 0.34210270643234253 + 10.0 * 8.337443351745605
Epoch 1350, val loss: 0.44332194328308105
Epoch 1360, training loss: 83.73877716064453 = 0.34061199426651 + 10.0 * 8.339816093444824
Epoch 1360, val loss: 0.44277140498161316
Epoch 1370, training loss: 83.70196533203125 = 0.3391108810901642 + 10.0 * 8.336285591125488
Epoch 1370, val loss: 0.4422842264175415
Epoch 1380, training loss: 83.69300842285156 = 0.3376099169254303 + 10.0 * 8.335539817810059
Epoch 1380, val loss: 0.4417959153652191
Epoch 1390, training loss: 83.74107360839844 = 0.33610838651657104 + 10.0 * 8.340497016906738
Epoch 1390, val loss: 0.441311240196228
Epoch 1400, training loss: 83.68992614746094 = 0.33458212018013 + 10.0 * 8.335535049438477
Epoch 1400, val loss: 0.44083908200263977
Epoch 1410, training loss: 83.6823959350586 = 0.33305612206459045 + 10.0 * 8.33493423461914
Epoch 1410, val loss: 0.4403586983680725
Epoch 1420, training loss: 83.67091369628906 = 0.3315451443195343 + 10.0 * 8.33393669128418
Epoch 1420, val loss: 0.4398863911628723
Epoch 1430, training loss: 83.67129516601562 = 0.33003538846969604 + 10.0 * 8.334126472473145
Epoch 1430, val loss: 0.4394644498825073
Epoch 1440, training loss: 83.71638488769531 = 0.3285074532032013 + 10.0 * 8.338788032531738
Epoch 1440, val loss: 0.4390137195587158
Epoch 1450, training loss: 83.65762329101562 = 0.3269670903682709 + 10.0 * 8.3330659866333
Epoch 1450, val loss: 0.4386025667190552
Epoch 1460, training loss: 83.65806579589844 = 0.3254380524158478 + 10.0 * 8.33326244354248
Epoch 1460, val loss: 0.43815580010414124
Epoch 1470, training loss: 83.64464569091797 = 0.3239113986492157 + 10.0 * 8.332073211669922
Epoch 1470, val loss: 0.4378047585487366
Epoch 1480, training loss: 83.69418334960938 = 0.32238173484802246 + 10.0 * 8.337180137634277
Epoch 1480, val loss: 0.4374391436576843
Epoch 1490, training loss: 83.66220092773438 = 0.3208184540271759 + 10.0 * 8.334138870239258
Epoch 1490, val loss: 0.4370608925819397
Epoch 1500, training loss: 83.64531707763672 = 0.3192663788795471 + 10.0 * 8.332605361938477
Epoch 1500, val loss: 0.4367761015892029
Epoch 1510, training loss: 83.64259338378906 = 0.3177114725112915 + 10.0 * 8.332488059997559
Epoch 1510, val loss: 0.4364529848098755
Epoch 1520, training loss: 83.6463851928711 = 0.31615349650382996 + 10.0 * 8.333023071289062
Epoch 1520, val loss: 0.4361511766910553
Epoch 1530, training loss: 83.62643432617188 = 0.31458693742752075 + 10.0 * 8.331184387207031
Epoch 1530, val loss: 0.435790479183197
Epoch 1540, training loss: 83.63653564453125 = 0.31301555037498474 + 10.0 * 8.332351684570312
Epoch 1540, val loss: 0.43554165959358215
Epoch 1550, training loss: 83.61810302734375 = 0.3114334046840668 + 10.0 * 8.330667495727539
Epoch 1550, val loss: 0.4352923035621643
Epoch 1560, training loss: 83.61170959472656 = 0.3098505735397339 + 10.0 * 8.330185890197754
Epoch 1560, val loss: 0.4350269138813019
Epoch 1570, training loss: 83.62750244140625 = 0.30826660990715027 + 10.0 * 8.331923484802246
Epoch 1570, val loss: 0.43481022119522095
Epoch 1580, training loss: 83.60868835449219 = 0.30666908621788025 + 10.0 * 8.330202102661133
Epoch 1580, val loss: 0.43465226888656616
Epoch 1590, training loss: 83.59745788574219 = 0.3050648272037506 + 10.0 * 8.329239845275879
Epoch 1590, val loss: 0.4344295859336853
Epoch 1600, training loss: 83.58598327636719 = 0.3034500479698181 + 10.0 * 8.328252792358398
Epoch 1600, val loss: 0.4343082010746002
Epoch 1610, training loss: 83.58393859863281 = 0.3018345236778259 + 10.0 * 8.328210830688477
Epoch 1610, val loss: 0.4341733753681183
Epoch 1620, training loss: 83.6090316772461 = 0.3002087473869324 + 10.0 * 8.33088207244873
Epoch 1620, val loss: 0.43403688073158264
Epoch 1630, training loss: 83.60523986816406 = 0.2985711991786957 + 10.0 * 8.330667495727539
Epoch 1630, val loss: 0.4339117705821991
Epoch 1640, training loss: 83.63067626953125 = 0.2969271242618561 + 10.0 * 8.333374977111816
Epoch 1640, val loss: 0.433807909488678
Epoch 1650, training loss: 83.57740020751953 = 0.29527345299720764 + 10.0 * 8.32821273803711
Epoch 1650, val loss: 0.4336868226528168
Epoch 1660, training loss: 83.55766296386719 = 0.2936229109764099 + 10.0 * 8.326403617858887
Epoch 1660, val loss: 0.43367668986320496
Epoch 1670, training loss: 83.55289459228516 = 0.2919670343399048 + 10.0 * 8.326092720031738
Epoch 1670, val loss: 0.43369463086128235
Epoch 1680, training loss: 83.56014251708984 = 0.2903023958206177 + 10.0 * 8.326983451843262
Epoch 1680, val loss: 0.43374302983283997
Epoch 1690, training loss: 83.57630920410156 = 0.28861626982688904 + 10.0 * 8.328768730163574
Epoch 1690, val loss: 0.4337698519229889
Epoch 1700, training loss: 83.55036163330078 = 0.286919504404068 + 10.0 * 8.32634449005127
Epoch 1700, val loss: 0.4337165951728821
Epoch 1710, training loss: 83.54057312011719 = 0.28522083163261414 + 10.0 * 8.325535774230957
Epoch 1710, val loss: 0.4339066445827484
Epoch 1720, training loss: 83.57514953613281 = 0.2835232615470886 + 10.0 * 8.32916259765625
Epoch 1720, val loss: 0.4340008795261383
Epoch 1730, training loss: 83.54747772216797 = 0.28180497884750366 + 10.0 * 8.326566696166992
Epoch 1730, val loss: 0.43409180641174316
Epoch 1740, training loss: 83.54364776611328 = 0.2800794839859009 + 10.0 * 8.326356887817383
Epoch 1740, val loss: 0.43430593609809875
Epoch 1750, training loss: 83.52920532226562 = 0.27835166454315186 + 10.0 * 8.325085639953613
Epoch 1750, val loss: 0.4344651997089386
Epoch 1760, training loss: 83.517822265625 = 0.2766222357749939 + 10.0 * 8.32412052154541
Epoch 1760, val loss: 0.43472912907600403
Epoch 1770, training loss: 83.51570892333984 = 0.27488502860069275 + 10.0 * 8.324082374572754
Epoch 1770, val loss: 0.43497681617736816
Epoch 1780, training loss: 83.54663848876953 = 0.2731417715549469 + 10.0 * 8.327349662780762
Epoch 1780, val loss: 0.4352853000164032
Epoch 1790, training loss: 83.50421905517578 = 0.27137821912765503 + 10.0 * 8.323284149169922
Epoch 1790, val loss: 0.4355517327785492
Epoch 1800, training loss: 83.54373931884766 = 0.26961851119995117 + 10.0 * 8.327412605285645
Epoch 1800, val loss: 0.4358929991722107
Epoch 1810, training loss: 83.4970932006836 = 0.2678396999835968 + 10.0 * 8.322925567626953
Epoch 1810, val loss: 0.4363633096218109
Epoch 1820, training loss: 83.49224090576172 = 0.2660629451274872 + 10.0 * 8.32261848449707
Epoch 1820, val loss: 0.4367443919181824
Epoch 1830, training loss: 83.48538208007812 = 0.26427850127220154 + 10.0 * 8.322110176086426
Epoch 1830, val loss: 0.4371458888053894
Epoch 1840, training loss: 83.48628997802734 = 0.2624892592430115 + 10.0 * 8.322380065917969
Epoch 1840, val loss: 0.4375569820404053
Epoch 1850, training loss: 83.56501770019531 = 0.2606954276561737 + 10.0 * 8.330431938171387
Epoch 1850, val loss: 0.43816661834716797
Epoch 1860, training loss: 83.49137115478516 = 0.2588832676410675 + 10.0 * 8.323248863220215
Epoch 1860, val loss: 0.43862292170524597
Epoch 1870, training loss: 83.47052764892578 = 0.2570723295211792 + 10.0 * 8.321345329284668
Epoch 1870, val loss: 0.43916791677474976
Epoch 1880, training loss: 83.49065399169922 = 0.25526756048202515 + 10.0 * 8.323538780212402
Epoch 1880, val loss: 0.4398142099380493
Epoch 1890, training loss: 83.46356201171875 = 0.25345438718795776 + 10.0 * 8.32101058959961
Epoch 1890, val loss: 0.44045111536979675
Epoch 1900, training loss: 83.46123504638672 = 0.2516319453716278 + 10.0 * 8.32096004486084
Epoch 1900, val loss: 0.44113269448280334
Epoch 1910, training loss: 83.45757293701172 = 0.24980434775352478 + 10.0 * 8.32077693939209
Epoch 1910, val loss: 0.44179409742355347
Epoch 1920, training loss: 83.49422454833984 = 0.2479754537343979 + 10.0 * 8.324625015258789
Epoch 1920, val loss: 0.4424992799758911
Epoch 1930, training loss: 83.46904754638672 = 0.24613475799560547 + 10.0 * 8.322291374206543
Epoch 1930, val loss: 0.4434698522090912
Epoch 1940, training loss: 83.48011779785156 = 0.24429798126220703 + 10.0 * 8.32358169555664
Epoch 1940, val loss: 0.4442000687122345
Epoch 1950, training loss: 83.4527587890625 = 0.24245652556419373 + 10.0 * 8.321030616760254
Epoch 1950, val loss: 0.44495660066604614
Epoch 1960, training loss: 83.43397521972656 = 0.2406167984008789 + 10.0 * 8.3193359375
Epoch 1960, val loss: 0.44577887654304504
Epoch 1970, training loss: 83.43074035644531 = 0.2387687861919403 + 10.0 * 8.319196701049805
Epoch 1970, val loss: 0.44667935371398926
Epoch 1980, training loss: 83.44308471679688 = 0.2369101494550705 + 10.0 * 8.32061767578125
Epoch 1980, val loss: 0.44754013419151306
Epoch 1990, training loss: 83.47355651855469 = 0.23504069447517395 + 10.0 * 8.323851585388184
Epoch 1990, val loss: 0.4483758509159088
Epoch 2000, training loss: 83.43136596679688 = 0.23315757513046265 + 10.0 * 8.31982135772705
Epoch 2000, val loss: 0.44946375489234924
Epoch 2010, training loss: 83.42680358886719 = 0.23128002882003784 + 10.0 * 8.319552421569824
Epoch 2010, val loss: 0.45054638385772705
Epoch 2020, training loss: 83.41645812988281 = 0.22939975559711456 + 10.0 * 8.318705558776855
Epoch 2020, val loss: 0.4515460729598999
Epoch 2030, training loss: 83.41133880615234 = 0.22751390933990479 + 10.0 * 8.318382263183594
Epoch 2030, val loss: 0.45267727971076965
Epoch 2040, training loss: 83.41170501708984 = 0.2256210297346115 + 10.0 * 8.318608283996582
Epoch 2040, val loss: 0.45376816391944885
Epoch 2050, training loss: 83.42111206054688 = 0.22372809052467346 + 10.0 * 8.319738388061523
Epoch 2050, val loss: 0.45506590604782104
Epoch 2060, training loss: 83.42322540283203 = 0.22182582318782806 + 10.0 * 8.32013988494873
Epoch 2060, val loss: 0.45624640583992004
Epoch 2070, training loss: 83.4338150024414 = 0.21993295848369598 + 10.0 * 8.321388244628906
Epoch 2070, val loss: 0.4575360119342804
Epoch 2080, training loss: 83.40484619140625 = 0.21803754568099976 + 10.0 * 8.318680763244629
Epoch 2080, val loss: 0.45873671770095825
Epoch 2090, training loss: 83.39009094238281 = 0.21614712476730347 + 10.0 * 8.317394256591797
Epoch 2090, val loss: 0.460008829832077
Epoch 2100, training loss: 83.38145446777344 = 0.2142636775970459 + 10.0 * 8.316719055175781
Epoch 2100, val loss: 0.46132010221481323
Epoch 2110, training loss: 83.37659454345703 = 0.21238015592098236 + 10.0 * 8.316421508789062
Epoch 2110, val loss: 0.46278759837150574
Epoch 2120, training loss: 83.37845611572266 = 0.21050018072128296 + 10.0 * 8.316795349121094
Epoch 2120, val loss: 0.4642285406589508
Epoch 2130, training loss: 83.4930648803711 = 0.2086464911699295 + 10.0 * 8.328441619873047
Epoch 2130, val loss: 0.46593552827835083
Epoch 2140, training loss: 83.3737564086914 = 0.206764817237854 + 10.0 * 8.316699028015137
Epoch 2140, val loss: 0.4672280550003052
Epoch 2150, training loss: 83.3738021850586 = 0.20489783585071564 + 10.0 * 8.316890716552734
Epoch 2150, val loss: 0.46902579069137573
Epoch 2160, training loss: 83.3617935180664 = 0.20303888618946075 + 10.0 * 8.315875053405762
Epoch 2160, val loss: 0.47069093585014343
Epoch 2170, training loss: 83.35784912109375 = 0.20118427276611328 + 10.0 * 8.315666198730469
Epoch 2170, val loss: 0.47233477234840393
Epoch 2180, training loss: 83.41535949707031 = 0.1993582844734192 + 10.0 * 8.321599960327148
Epoch 2180, val loss: 0.4739508330821991
Epoch 2190, training loss: 83.35748291015625 = 0.19749894738197327 + 10.0 * 8.315998077392578
Epoch 2190, val loss: 0.47618064284324646
Epoch 2200, training loss: 83.35098266601562 = 0.19566267728805542 + 10.0 * 8.315531730651855
Epoch 2200, val loss: 0.47773686051368713
Epoch 2210, training loss: 83.34961700439453 = 0.19382937252521515 + 10.0 * 8.31557846069336
Epoch 2210, val loss: 0.47976580262184143
Epoch 2220, training loss: 83.37648010253906 = 0.19200429320335388 + 10.0 * 8.318448066711426
Epoch 2220, val loss: 0.4815517067909241
Epoch 2230, training loss: 83.3443603515625 = 0.1901748776435852 + 10.0 * 8.315418243408203
Epoch 2230, val loss: 0.4835543632507324
Epoch 2240, training loss: 83.35975646972656 = 0.18835563957691193 + 10.0 * 8.317140579223633
Epoch 2240, val loss: 0.48547232151031494
Epoch 2250, training loss: 83.34053039550781 = 0.18652749061584473 + 10.0 * 8.315400123596191
Epoch 2250, val loss: 0.48748722672462463
Epoch 2260, training loss: 83.34610748291016 = 0.18471340835094452 + 10.0 * 8.316139221191406
Epoch 2260, val loss: 0.48945754766464233
Epoch 2270, training loss: 83.332275390625 = 0.18289853632450104 + 10.0 * 8.314937591552734
Epoch 2270, val loss: 0.4916616678237915
Epoch 2280, training loss: 83.33206939697266 = 0.18109165132045746 + 10.0 * 8.31509780883789
Epoch 2280, val loss: 0.49392014741897583
Epoch 2290, training loss: 83.32438659667969 = 0.17929209768772125 + 10.0 * 8.314509391784668
Epoch 2290, val loss: 0.49581411480903625
Epoch 2300, training loss: 83.35673522949219 = 0.17750275135040283 + 10.0 * 8.317922592163086
Epoch 2300, val loss: 0.4982698857784271
Epoch 2310, training loss: 83.31819152832031 = 0.17572005093097687 + 10.0 * 8.314247131347656
Epoch 2310, val loss: 0.500214159488678
Epoch 2320, training loss: 83.31504821777344 = 0.17394518852233887 + 10.0 * 8.31411075592041
Epoch 2320, val loss: 0.5024252533912659
Epoch 2330, training loss: 83.3284683227539 = 0.17219239473342896 + 10.0 * 8.315627098083496
Epoch 2330, val loss: 0.5046984553337097
Epoch 2340, training loss: 83.31092071533203 = 0.17043815553188324 + 10.0 * 8.314047813415527
Epoch 2340, val loss: 0.5069947242736816
Epoch 2350, training loss: 83.30583190917969 = 0.16868627071380615 + 10.0 * 8.313714981079102
Epoch 2350, val loss: 0.5087944269180298
Epoch 2360, training loss: 83.30186462402344 = 0.1669471710920334 + 10.0 * 8.313491821289062
Epoch 2360, val loss: 0.5114853382110596
Epoch 2370, training loss: 83.35767364501953 = 0.1652235984802246 + 10.0 * 8.319245338439941
Epoch 2370, val loss: 0.5138300061225891
Epoch 2380, training loss: 83.31232452392578 = 0.16350595653057098 + 10.0 * 8.314882278442383
Epoch 2380, val loss: 0.5162019729614258
Epoch 2390, training loss: 83.31201934814453 = 0.16179485619068146 + 10.0 * 8.315022468566895
Epoch 2390, val loss: 0.5185972452163696
Epoch 2400, training loss: 83.30414581298828 = 0.1600806713104248 + 10.0 * 8.314406394958496
Epoch 2400, val loss: 0.5207369923591614
Epoch 2410, training loss: 83.29473876953125 = 0.15837928652763367 + 10.0 * 8.31363582611084
Epoch 2410, val loss: 0.5234734416007996
Epoch 2420, training loss: 83.2776870727539 = 0.15669207274913788 + 10.0 * 8.31209945678711
Epoch 2420, val loss: 0.5257054567337036
Epoch 2430, training loss: 83.27497100830078 = 0.1550101488828659 + 10.0 * 8.311996459960938
Epoch 2430, val loss: 0.5281938910484314
Epoch 2440, training loss: 83.28612518310547 = 0.1533440351486206 + 10.0 * 8.313278198242188
Epoch 2440, val loss: 0.5309014320373535
Epoch 2450, training loss: 83.30957794189453 = 0.15168778598308563 + 10.0 * 8.315789222717285
Epoch 2450, val loss: 0.5338150262832642
Epoch 2460, training loss: 83.2899169921875 = 0.15003164112567902 + 10.0 * 8.31398868560791
Epoch 2460, val loss: 0.5361027121543884
Epoch 2470, training loss: 83.29076385498047 = 0.14841638505458832 + 10.0 * 8.314234733581543
Epoch 2470, val loss: 0.5389529466629028
Epoch 2480, training loss: 83.26756286621094 = 0.14679832756519318 + 10.0 * 8.312076568603516
Epoch 2480, val loss: 0.5411528944969177
Epoch 2490, training loss: 83.26412200927734 = 0.14520689845085144 + 10.0 * 8.311891555786133
Epoch 2490, val loss: 0.5439514517784119
Epoch 2500, training loss: 83.29612731933594 = 0.14362820982933044 + 10.0 * 8.3152494430542
Epoch 2500, val loss: 0.5466864109039307
Epoch 2510, training loss: 83.27999114990234 = 0.1420520544052124 + 10.0 * 8.313794136047363
Epoch 2510, val loss: 0.5492364764213562
Epoch 2520, training loss: 83.2558822631836 = 0.1405140906572342 + 10.0 * 8.31153678894043
Epoch 2520, val loss: 0.5519760847091675
Epoch 2530, training loss: 83.24544525146484 = 0.13895520567893982 + 10.0 * 8.310648918151855
Epoch 2530, val loss: 0.554789662361145
Epoch 2540, training loss: 83.24238586425781 = 0.13742436468601227 + 10.0 * 8.31049633026123
Epoch 2540, val loss: 0.5572975277900696
Epoch 2550, training loss: 83.24706268310547 = 0.13590668141841888 + 10.0 * 8.311115264892578
Epoch 2550, val loss: 0.5603578090667725
Epoch 2560, training loss: 83.26933288574219 = 0.13440237939357758 + 10.0 * 8.313493728637695
Epoch 2560, val loss: 0.5629900097846985
Epoch 2570, training loss: 83.26202392578125 = 0.13291098177433014 + 10.0 * 8.312911033630371
Epoch 2570, val loss: 0.5655868649482727
Epoch 2580, training loss: 83.2918930053711 = 0.13141073286533356 + 10.0 * 8.316048622131348
Epoch 2580, val loss: 0.5686318278312683
Epoch 2590, training loss: 83.24190521240234 = 0.12993542850017548 + 10.0 * 8.311197280883789
Epoch 2590, val loss: 0.5715243220329285
Epoch 2600, training loss: 83.22798156738281 = 0.12847140431404114 + 10.0 * 8.309950828552246
Epoch 2600, val loss: 0.5747305154800415
Epoch 2610, training loss: 83.222412109375 = 0.1270144134759903 + 10.0 * 8.309539794921875
Epoch 2610, val loss: 0.5775105357170105
Epoch 2620, training loss: 83.21735382080078 = 0.125570610165596 + 10.0 * 8.309178352355957
Epoch 2620, val loss: 0.5803511738777161
Epoch 2630, training loss: 83.22439575195312 = 0.12414933741092682 + 10.0 * 8.31002426147461
Epoch 2630, val loss: 0.5837660431861877
Epoch 2640, training loss: 83.26940155029297 = 0.12274906784296036 + 10.0 * 8.314664840698242
Epoch 2640, val loss: 0.5868399739265442
Epoch 2650, training loss: 83.21253204345703 = 0.12132081389427185 + 10.0 * 8.309121131896973
Epoch 2650, val loss: 0.5895629525184631
Epoch 2660, training loss: 83.21501922607422 = 0.11993925273418427 + 10.0 * 8.309507369995117
Epoch 2660, val loss: 0.5922910571098328
Epoch 2670, training loss: 83.22012329101562 = 0.11854838579893112 + 10.0 * 8.310157775878906
Epoch 2670, val loss: 0.5956551432609558
Epoch 2680, training loss: 83.23792266845703 = 0.11720003187656403 + 10.0 * 8.312071800231934
Epoch 2680, val loss: 0.598753035068512
Epoch 2690, training loss: 83.21392822265625 = 0.11581259220838547 + 10.0 * 8.30981159210205
Epoch 2690, val loss: 0.6020750403404236
Epoch 2700, training loss: 83.20050811767578 = 0.11445043981075287 + 10.0 * 8.308606147766113
Epoch 2700, val loss: 0.6054142713546753
Epoch 2710, training loss: 83.21280670166016 = 0.11308761686086655 + 10.0 * 8.309971809387207
Epoch 2710, val loss: 0.6082203984260559
Epoch 2720, training loss: 83.22236633300781 = 0.11174498498439789 + 10.0 * 8.31106185913086
Epoch 2720, val loss: 0.611386775970459
Epoch 2730, training loss: 83.20465087890625 = 0.11042451858520508 + 10.0 * 8.309422492980957
Epoch 2730, val loss: 0.6146047115325928
Epoch 2740, training loss: 83.1988754272461 = 0.10910971462726593 + 10.0 * 8.308977127075195
Epoch 2740, val loss: 0.617857038974762
Epoch 2750, training loss: 83.20044708251953 = 0.10780942440032959 + 10.0 * 8.309263229370117
Epoch 2750, val loss: 0.6210015416145325
Epoch 2760, training loss: 83.20036315917969 = 0.10651854425668716 + 10.0 * 8.3093843460083
Epoch 2760, val loss: 0.6246833801269531
Epoch 2770, training loss: 83.21903991699219 = 0.10526495426893234 + 10.0 * 8.31137752532959
Epoch 2770, val loss: 0.6282176375389099
Epoch 2780, training loss: 83.19141387939453 = 0.10398818552494049 + 10.0 * 8.30874252319336
Epoch 2780, val loss: 0.6312456130981445
Epoch 2790, training loss: 83.18089294433594 = 0.10273605585098267 + 10.0 * 8.307815551757812
Epoch 2790, val loss: 0.6343712210655212
Epoch 2800, training loss: 83.17435455322266 = 0.10150520503520966 + 10.0 * 8.30728530883789
Epoch 2800, val loss: 0.6378036737442017
Epoch 2810, training loss: 83.16898345947266 = 0.10028079152107239 + 10.0 * 8.306870460510254
Epoch 2810, val loss: 0.6408929228782654
Epoch 2820, training loss: 83.19075012207031 = 0.0991005152463913 + 10.0 * 8.309165000915527
Epoch 2820, val loss: 0.6443148851394653
Epoch 2830, training loss: 83.19536590576172 = 0.09788063913583755 + 10.0 * 8.309748649597168
Epoch 2830, val loss: 0.6476714015007019
Epoch 2840, training loss: 83.17403411865234 = 0.09667611122131348 + 10.0 * 8.307735443115234
Epoch 2840, val loss: 0.6520125269889832
Epoch 2850, training loss: 83.16240692138672 = 0.09550141543149948 + 10.0 * 8.306690216064453
Epoch 2850, val loss: 0.6555030345916748
Epoch 2860, training loss: 83.15823364257812 = 0.09433718770742416 + 10.0 * 8.306389808654785
Epoch 2860, val loss: 0.6587650179862976
Epoch 2870, training loss: 83.15711212158203 = 0.09317594766616821 + 10.0 * 8.30639362335205
Epoch 2870, val loss: 0.6622021794319153
Epoch 2880, training loss: 83.19941711425781 = 0.09203435480594635 + 10.0 * 8.310738563537598
Epoch 2880, val loss: 0.6653324961662292
Epoch 2890, training loss: 83.16895294189453 = 0.09091881662607193 + 10.0 * 8.3078031539917
Epoch 2890, val loss: 0.6690607666969299
Epoch 2900, training loss: 83.17398834228516 = 0.08977887034416199 + 10.0 * 8.30842113494873
Epoch 2900, val loss: 0.6720330119132996
Epoch 2910, training loss: 83.16146850585938 = 0.08866101503372192 + 10.0 * 8.307280540466309
Epoch 2910, val loss: 0.6759846210479736
Epoch 2920, training loss: 83.15125274658203 = 0.08754424750804901 + 10.0 * 8.306370735168457
Epoch 2920, val loss: 0.6799700260162354
Epoch 2930, training loss: 83.1629867553711 = 0.08644994348287582 + 10.0 * 8.307653427124023
Epoch 2930, val loss: 0.6834432482719421
Epoch 2940, training loss: 83.14794921875 = 0.08536280691623688 + 10.0 * 8.306258201599121
Epoch 2940, val loss: 0.6867517828941345
Epoch 2950, training loss: 83.17382049560547 = 0.08429598063230515 + 10.0 * 8.308952331542969
Epoch 2950, val loss: 0.6911159157752991
Epoch 2960, training loss: 83.15184783935547 = 0.08323295414447784 + 10.0 * 8.30686092376709
Epoch 2960, val loss: 0.6943259835243225
Epoch 2970, training loss: 83.13765716552734 = 0.08217941969633102 + 10.0 * 8.305547714233398
Epoch 2970, val loss: 0.6974648833274841
Epoch 2980, training loss: 83.13788604736328 = 0.08113514631986618 + 10.0 * 8.30567455291748
Epoch 2980, val loss: 0.7009669542312622
Epoch 2990, training loss: 83.20909881591797 = 0.0801459327340126 + 10.0 * 8.312894821166992
Epoch 2990, val loss: 0.7043670415878296
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8193810248604769
0.8410490473085562
=== training gcn model ===
Epoch 0, training loss: 106.9299087524414 = 1.1073206663131714 + 10.0 * 10.582258224487305
Epoch 0, val loss: 1.1076966524124146
Epoch 10, training loss: 106.91485595703125 = 1.0997626781463623 + 10.0 * 10.581509590148926
Epoch 10, val loss: 1.1000463962554932
Epoch 20, training loss: 106.83380126953125 = 1.0909982919692993 + 10.0 * 10.574280738830566
Epoch 20, val loss: 1.0911287069320679
Epoch 30, training loss: 106.29672241210938 = 1.0805034637451172 + 10.0 * 10.521621704101562
Epoch 30, val loss: 1.080390214920044
Epoch 40, training loss: 104.32962799072266 = 1.0689911842346191 + 10.0 * 10.32606315612793
Epoch 40, val loss: 1.0689921379089355
Epoch 50, training loss: 100.6536636352539 = 1.059777855873108 + 10.0 * 9.959388732910156
Epoch 50, val loss: 1.0598440170288086
Epoch 60, training loss: 96.6520004272461 = 1.051709771156311 + 10.0 * 9.560029029846191
Epoch 60, val loss: 1.0518708229064941
Epoch 70, training loss: 93.82860565185547 = 1.0400584936141968 + 10.0 * 9.278854370117188
Epoch 70, val loss: 1.040285348892212
Epoch 80, training loss: 92.94525909423828 = 1.028712272644043 + 10.0 * 9.191655158996582
Epoch 80, val loss: 1.0295356512069702
Epoch 90, training loss: 92.5233383178711 = 1.0204142332077026 + 10.0 * 9.15029239654541
Epoch 90, val loss: 1.0216856002807617
Epoch 100, training loss: 91.74524688720703 = 1.0144606828689575 + 10.0 * 9.073079109191895
Epoch 100, val loss: 1.016222596168518
Epoch 110, training loss: 90.42228698730469 = 1.0100898742675781 + 10.0 * 8.9412202835083
Epoch 110, val loss: 1.0122286081314087
Epoch 120, training loss: 89.11126708984375 = 1.006824254989624 + 10.0 * 8.810444831848145
Epoch 120, val loss: 1.0092074871063232
Epoch 130, training loss: 88.57804870605469 = 1.0016400814056396 + 10.0 * 8.757640838623047
Epoch 130, val loss: 1.0037935972213745
Epoch 140, training loss: 88.0687484741211 = 0.9933878183364868 + 10.0 * 8.707536697387695
Epoch 140, val loss: 0.9955717325210571
Epoch 150, training loss: 87.71849822998047 = 0.9839524626731873 + 10.0 * 8.673454284667969
Epoch 150, val loss: 0.9864852428436279
Epoch 160, training loss: 87.42044067382812 = 0.9744688272476196 + 10.0 * 8.644597053527832
Epoch 160, val loss: 0.9774110913276672
Epoch 170, training loss: 87.2119140625 = 0.9645513892173767 + 10.0 * 8.624735832214355
Epoch 170, val loss: 0.9679598212242126
Epoch 180, training loss: 86.9714584350586 = 0.9539614319801331 + 10.0 * 8.601749420166016
Epoch 180, val loss: 0.9577799439430237
Epoch 190, training loss: 86.75128936767578 = 0.9426451921463013 + 10.0 * 8.580864906311035
Epoch 190, val loss: 0.9469977617263794
Epoch 200, training loss: 86.7366714477539 = 0.9303174018859863 + 10.0 * 8.580635070800781
Epoch 200, val loss: 0.9353747963905334
Epoch 210, training loss: 86.44892120361328 = 0.9167283773422241 + 10.0 * 8.55321979522705
Epoch 210, val loss: 0.9222670793533325
Epoch 220, training loss: 86.28333282470703 = 0.902327299118042 + 10.0 * 8.538100242614746
Epoch 220, val loss: 0.9086102247238159
Epoch 230, training loss: 86.14849090576172 = 0.8872751593589783 + 10.0 * 8.526121139526367
Epoch 230, val loss: 0.8943434953689575
Epoch 240, training loss: 86.02305603027344 = 0.8714413046836853 + 10.0 * 8.515161514282227
Epoch 240, val loss: 0.8793338537216187
Epoch 250, training loss: 85.9304428100586 = 0.8548353910446167 + 10.0 * 8.507560729980469
Epoch 250, val loss: 0.8636873960494995
Epoch 260, training loss: 85.83229064941406 = 0.8375841379165649 + 10.0 * 8.499470710754395
Epoch 260, val loss: 0.8472321033477783
Epoch 270, training loss: 85.73190307617188 = 0.8197845220565796 + 10.0 * 8.491211891174316
Epoch 270, val loss: 0.8304653763771057
Epoch 280, training loss: 85.65486145019531 = 0.8017616271972656 + 10.0 * 8.485309600830078
Epoch 280, val loss: 0.8134428858757019
Epoch 290, training loss: 85.58433532714844 = 0.7835490107536316 + 10.0 * 8.48007869720459
Epoch 290, val loss: 0.7962206602096558
Epoch 300, training loss: 85.552001953125 = 0.7651575207710266 + 10.0 * 8.478684425354004
Epoch 300, val loss: 0.7788349986076355
Epoch 310, training loss: 85.46737670898438 = 0.7466105818748474 + 10.0 * 8.472076416015625
Epoch 310, val loss: 0.7613011598587036
Epoch 320, training loss: 85.40019226074219 = 0.7283686399459839 + 10.0 * 8.467182159423828
Epoch 320, val loss: 0.744111955165863
Epoch 330, training loss: 85.33238220214844 = 0.7104788422584534 + 10.0 * 8.462190628051758
Epoch 330, val loss: 0.7272379398345947
Epoch 340, training loss: 85.38825988769531 = 0.6929613947868347 + 10.0 * 8.46953010559082
Epoch 340, val loss: 0.7105674147605896
Epoch 350, training loss: 85.22504425048828 = 0.6756482720375061 + 10.0 * 8.454938888549805
Epoch 350, val loss: 0.6943988800048828
Epoch 360, training loss: 85.1804428100586 = 0.6593307256698608 + 10.0 * 8.45211124420166
Epoch 360, val loss: 0.6790133118629456
Epoch 370, training loss: 85.11720275878906 = 0.6438435316085815 + 10.0 * 8.447336196899414
Epoch 370, val loss: 0.6643781661987305
Epoch 380, training loss: 85.06681823730469 = 0.629056990146637 + 10.0 * 8.44377613067627
Epoch 380, val loss: 0.6504597663879395
Epoch 390, training loss: 85.0149154663086 = 0.6150268316268921 + 10.0 * 8.43998908996582
Epoch 390, val loss: 0.6372818350791931
Epoch 400, training loss: 85.0560073852539 = 0.601810872554779 + 10.0 * 8.445419311523438
Epoch 400, val loss: 0.6248525381088257
Epoch 410, training loss: 84.95967864990234 = 0.5892543196678162 + 10.0 * 8.437042236328125
Epoch 410, val loss: 0.6131681799888611
Epoch 420, training loss: 84.88095092773438 = 0.5777118802070618 + 10.0 * 8.430323600769043
Epoch 420, val loss: 0.6024414896965027
Epoch 430, training loss: 84.84232330322266 = 0.567049503326416 + 10.0 * 8.42752742767334
Epoch 430, val loss: 0.5925517082214355
Epoch 440, training loss: 84.80268096923828 = 0.5571423768997192 + 10.0 * 8.424553871154785
Epoch 440, val loss: 0.5833831429481506
Epoch 450, training loss: 84.788330078125 = 0.5478032827377319 + 10.0 * 8.424052238464355
Epoch 450, val loss: 0.5748068690299988
Epoch 460, training loss: 84.768310546875 = 0.5390790104866028 + 10.0 * 8.42292308807373
Epoch 460, val loss: 0.5668672919273376
Epoch 470, training loss: 84.7027816772461 = 0.5312435626983643 + 10.0 * 8.417154312133789
Epoch 470, val loss: 0.5597251057624817
Epoch 480, training loss: 84.66854095458984 = 0.5240160822868347 + 10.0 * 8.41445255279541
Epoch 480, val loss: 0.5531734824180603
Epoch 490, training loss: 84.63545227050781 = 0.5172465443611145 + 10.0 * 8.411820411682129
Epoch 490, val loss: 0.5471059679985046
Epoch 500, training loss: 84.6077651977539 = 0.5109204649925232 + 10.0 * 8.409685134887695
Epoch 500, val loss: 0.5414910912513733
Epoch 510, training loss: 84.61003112792969 = 0.5049248933792114 + 10.0 * 8.410510063171387
Epoch 510, val loss: 0.5362172722816467
Epoch 520, training loss: 84.56346893310547 = 0.4992528557777405 + 10.0 * 8.406421661376953
Epoch 520, val loss: 0.5312299728393555
Epoch 530, training loss: 84.53132629394531 = 0.4940076470375061 + 10.0 * 8.403731346130371
Epoch 530, val loss: 0.5266270637512207
Epoch 540, training loss: 84.53486633300781 = 0.4890812337398529 + 10.0 * 8.40457820892334
Epoch 540, val loss: 0.522303581237793
Epoch 550, training loss: 84.47999572753906 = 0.4843316674232483 + 10.0 * 8.399566650390625
Epoch 550, val loss: 0.5183210372924805
Epoch 560, training loss: 84.46295166015625 = 0.4798785150051117 + 10.0 * 8.398306846618652
Epoch 560, val loss: 0.5145615339279175
Epoch 570, training loss: 84.44552612304688 = 0.4756501019001007 + 10.0 * 8.396987915039062
Epoch 570, val loss: 0.5109813213348389
Epoch 580, training loss: 84.51004791259766 = 0.47158414125442505 + 10.0 * 8.403846740722656
Epoch 580, val loss: 0.5075644850730896
Epoch 590, training loss: 84.4052963256836 = 0.4676423668861389 + 10.0 * 8.393765449523926
Epoch 590, val loss: 0.5043236613273621
Epoch 600, training loss: 84.38584899902344 = 0.46394360065460205 + 10.0 * 8.392190933227539
Epoch 600, val loss: 0.5012646913528442
Epoch 610, training loss: 84.38477325439453 = 0.46034952998161316 + 10.0 * 8.39244270324707
Epoch 610, val loss: 0.49834945797920227
Epoch 620, training loss: 84.34718322753906 = 0.4568893015384674 + 10.0 * 8.389029502868652
Epoch 620, val loss: 0.49561911821365356
Epoch 630, training loss: 84.33206939697266 = 0.45362040400505066 + 10.0 * 8.387845039367676
Epoch 630, val loss: 0.4930489659309387
Epoch 640, training loss: 84.30860900878906 = 0.4504930377006531 + 10.0 * 8.385811805725098
Epoch 640, val loss: 0.49059560894966125
Epoch 650, training loss: 84.29076385498047 = 0.4474571645259857 + 10.0 * 8.384330749511719
Epoch 650, val loss: 0.4882722795009613
Epoch 660, training loss: 84.2886962890625 = 0.44451257586479187 + 10.0 * 8.384418487548828
Epoch 660, val loss: 0.48605161905288696
Epoch 670, training loss: 84.27442932128906 = 0.44160646200180054 + 10.0 * 8.383282661437988
Epoch 670, val loss: 0.4838351905345917
Epoch 680, training loss: 84.30857849121094 = 0.4387926161289215 + 10.0 * 8.386979103088379
Epoch 680, val loss: 0.48169299960136414
Epoch 690, training loss: 84.24327087402344 = 0.4360561668872833 + 10.0 * 8.380721092224121
Epoch 690, val loss: 0.47976988554000854
Epoch 700, training loss: 84.21708679199219 = 0.4334685206413269 + 10.0 * 8.378361701965332
Epoch 700, val loss: 0.4778568744659424
Epoch 710, training loss: 84.20433807373047 = 0.43097442388534546 + 10.0 * 8.377336502075195
Epoch 710, val loss: 0.4760833978652954
Epoch 720, training loss: 84.24864959716797 = 0.42854687571525574 + 10.0 * 8.382010459899902
Epoch 720, val loss: 0.4743126332759857
Epoch 730, training loss: 84.2111587524414 = 0.426127552986145 + 10.0 * 8.378503799438477
Epoch 730, val loss: 0.47277209162712097
Epoch 740, training loss: 84.17815399169922 = 0.4238114058971405 + 10.0 * 8.375433921813965
Epoch 740, val loss: 0.4710225462913513
Epoch 750, training loss: 84.1417465209961 = 0.4215734302997589 + 10.0 * 8.372017860412598
Epoch 750, val loss: 0.46954092383384705
Epoch 760, training loss: 84.12987518310547 = 0.4194299876689911 + 10.0 * 8.371044158935547
Epoch 760, val loss: 0.4680953323841095
Epoch 770, training loss: 84.13221740722656 = 0.4173506498336792 + 10.0 * 8.37148666381836
Epoch 770, val loss: 0.46669378876686096
Epoch 780, training loss: 84.1244125366211 = 0.4152604341506958 + 10.0 * 8.370915412902832
Epoch 780, val loss: 0.4653051495552063
Epoch 790, training loss: 84.11035919189453 = 0.41324153542518616 + 10.0 * 8.369711875915527
Epoch 790, val loss: 0.4639781713485718
Epoch 800, training loss: 84.08676147460938 = 0.411331444978714 + 10.0 * 8.36754322052002
Epoch 800, val loss: 0.46263787150382996
Epoch 810, training loss: 84.0711441040039 = 0.40948009490966797 + 10.0 * 8.366167068481445
Epoch 810, val loss: 0.4614216983318329
Epoch 820, training loss: 84.05644226074219 = 0.40767720341682434 + 10.0 * 8.364876747131348
Epoch 820, val loss: 0.46029502153396606
Epoch 830, training loss: 84.05633544921875 = 0.40591344237327576 + 10.0 * 8.365041732788086
Epoch 830, val loss: 0.4591187834739685
Epoch 840, training loss: 84.070068359375 = 0.40413403511047363 + 10.0 * 8.366593360900879
Epoch 840, val loss: 0.4579579830169678
Epoch 850, training loss: 84.02777862548828 = 0.402397096157074 + 10.0 * 8.36253833770752
Epoch 850, val loss: 0.45689722895622253
Epoch 860, training loss: 84.01376342773438 = 0.4007520377635956 + 10.0 * 8.36130142211914
Epoch 860, val loss: 0.4557940363883972
Epoch 870, training loss: 84.00392150878906 = 0.3991563618183136 + 10.0 * 8.36047649383545
Epoch 870, val loss: 0.45480120182037354
Epoch 880, training loss: 84.0414047241211 = 0.39759528636932373 + 10.0 * 8.364380836486816
Epoch 880, val loss: 0.45386484265327454
Epoch 890, training loss: 84.06473541259766 = 0.3959963321685791 + 10.0 * 8.366873741149902
Epoch 890, val loss: 0.45276927947998047
Epoch 900, training loss: 83.99034118652344 = 0.3944568336009979 + 10.0 * 8.359588623046875
Epoch 900, val loss: 0.45178768038749695
Epoch 910, training loss: 83.96666717529297 = 0.39299386739730835 + 10.0 * 8.357367515563965
Epoch 910, val loss: 0.4508683383464813
Epoch 920, training loss: 83.95655059814453 = 0.3915710747241974 + 10.0 * 8.356497764587402
Epoch 920, val loss: 0.450030118227005
Epoch 930, training loss: 83.94790649414062 = 0.3901817798614502 + 10.0 * 8.355772018432617
Epoch 930, val loss: 0.44916650652885437
Epoch 940, training loss: 83.9737777709961 = 0.3888131082057953 + 10.0 * 8.358495712280273
Epoch 940, val loss: 0.44829389452934265
Epoch 950, training loss: 83.96297454833984 = 0.3874150812625885 + 10.0 * 8.357556343078613
Epoch 950, val loss: 0.44752010703086853
Epoch 960, training loss: 83.93091583251953 = 0.38606828451156616 + 10.0 * 8.354484558105469
Epoch 960, val loss: 0.4466370642185211
Epoch 970, training loss: 83.91922760009766 = 0.3847601115703583 + 10.0 * 8.353446960449219
Epoch 970, val loss: 0.44588398933410645
Epoch 980, training loss: 83.96463012695312 = 0.3834734261035919 + 10.0 * 8.358115196228027
Epoch 980, val loss: 0.4451943337917328
Epoch 990, training loss: 83.90962982177734 = 0.3821824789047241 + 10.0 * 8.352745056152344
Epoch 990, val loss: 0.444363534450531
Epoch 1000, training loss: 83.89818572998047 = 0.38092854619026184 + 10.0 * 8.351725578308105
Epoch 1000, val loss: 0.4436801075935364
Epoch 1010, training loss: 83.88502502441406 = 0.3797064423561096 + 10.0 * 8.350531578063965
Epoch 1010, val loss: 0.4429579973220825
Epoch 1020, training loss: 83.87781524658203 = 0.37850311398506165 + 10.0 * 8.349931716918945
Epoch 1020, val loss: 0.44228890538215637
Epoch 1030, training loss: 83.95848083496094 = 0.37730950117111206 + 10.0 * 8.35811710357666
Epoch 1030, val loss: 0.44163575768470764
Epoch 1040, training loss: 83.87063598632812 = 0.3761093020439148 + 10.0 * 8.34945297241211
Epoch 1040, val loss: 0.44096559286117554
Epoch 1050, training loss: 83.86365509033203 = 0.374944806098938 + 10.0 * 8.348871231079102
Epoch 1050, val loss: 0.44035181403160095
Epoch 1060, training loss: 83.85345458984375 = 0.37380650639533997 + 10.0 * 8.3479642868042
Epoch 1060, val loss: 0.4397331178188324
Epoch 1070, training loss: 83.9299545288086 = 0.37267976999282837 + 10.0 * 8.355727195739746
Epoch 1070, val loss: 0.4390903413295746
Epoch 1080, training loss: 83.8691177368164 = 0.3715330958366394 + 10.0 * 8.34975814819336
Epoch 1080, val loss: 0.438612163066864
Epoch 1090, training loss: 83.8406753540039 = 0.3704272508621216 + 10.0 * 8.347024917602539
Epoch 1090, val loss: 0.43799707293510437
Epoch 1100, training loss: 83.83121490478516 = 0.36933818459510803 + 10.0 * 8.346187591552734
Epoch 1100, val loss: 0.437504380941391
Epoch 1110, training loss: 83.87654876708984 = 0.36826154589653015 + 10.0 * 8.350828170776367
Epoch 1110, val loss: 0.4369824230670929
Epoch 1120, training loss: 83.81658172607422 = 0.3671758472919464 + 10.0 * 8.344941139221191
Epoch 1120, val loss: 0.4364224970340729
Epoch 1130, training loss: 83.8101577758789 = 0.36611342430114746 + 10.0 * 8.344404220581055
Epoch 1130, val loss: 0.4358954429626465
Epoch 1140, training loss: 83.80001831054688 = 0.36506447196006775 + 10.0 * 8.34349536895752
Epoch 1140, val loss: 0.4354535639286041
Epoch 1150, training loss: 83.796875 = 0.36402881145477295 + 10.0 * 8.343284606933594
Epoch 1150, val loss: 0.434969037771225
Epoch 1160, training loss: 83.87040710449219 = 0.3629939556121826 + 10.0 * 8.350741386413574
Epoch 1160, val loss: 0.4344741106033325
Epoch 1170, training loss: 83.86165618896484 = 0.3619394898414612 + 10.0 * 8.349971771240234
Epoch 1170, val loss: 0.43397554755210876
Epoch 1180, training loss: 83.78030395507812 = 0.36089640855789185 + 10.0 * 8.341940879821777
Epoch 1180, val loss: 0.43356218934059143
Epoch 1190, training loss: 83.77960205078125 = 0.35988515615463257 + 10.0 * 8.341971397399902
Epoch 1190, val loss: 0.4331241846084595
Epoch 1200, training loss: 83.78929138183594 = 0.35889217257499695 + 10.0 * 8.343039512634277
Epoch 1200, val loss: 0.43269026279449463
Epoch 1210, training loss: 83.76303100585938 = 0.3578936755657196 + 10.0 * 8.340513229370117
Epoch 1210, val loss: 0.43231070041656494
Epoch 1220, training loss: 83.75786590576172 = 0.35690659284591675 + 10.0 * 8.340096473693848
Epoch 1220, val loss: 0.43187713623046875
Epoch 1230, training loss: 83.79136657714844 = 0.35592687129974365 + 10.0 * 8.343544006347656
Epoch 1230, val loss: 0.4315704107284546
Epoch 1240, training loss: 83.75361633300781 = 0.3549342155456543 + 10.0 * 8.339868545532227
Epoch 1240, val loss: 0.431100994348526
Epoch 1250, training loss: 83.7482681274414 = 0.3539527356624603 + 10.0 * 8.339431762695312
Epoch 1250, val loss: 0.43073636293411255
Epoch 1260, training loss: 83.74163818359375 = 0.3529839813709259 + 10.0 * 8.338865280151367
Epoch 1260, val loss: 0.4303986430168152
Epoch 1270, training loss: 83.73590087890625 = 0.352026104927063 + 10.0 * 8.338387489318848
Epoch 1270, val loss: 0.4300498068332672
Epoch 1280, training loss: 83.73344421386719 = 0.3510686457157135 + 10.0 * 8.338237762451172
Epoch 1280, val loss: 0.4297192692756653
Epoch 1290, training loss: 83.81362915039062 = 0.35010790824890137 + 10.0 * 8.346352577209473
Epoch 1290, val loss: 0.4294295907020569
Epoch 1300, training loss: 83.73762512207031 = 0.3491334915161133 + 10.0 * 8.338849067687988
Epoch 1300, val loss: 0.42908555269241333
Epoch 1310, training loss: 83.71906280517578 = 0.34817564487457275 + 10.0 * 8.337088584899902
Epoch 1310, val loss: 0.42880845069885254
Epoch 1320, training loss: 83.71434020996094 = 0.34723013639450073 + 10.0 * 8.336710929870605
Epoch 1320, val loss: 0.428454726934433
Epoch 1330, training loss: 83.7326431274414 = 0.3462928831577301 + 10.0 * 8.338635444641113
Epoch 1330, val loss: 0.4281482994556427
Epoch 1340, training loss: 83.70935821533203 = 0.34533536434173584 + 10.0 * 8.33640193939209
Epoch 1340, val loss: 0.4279736578464508
Epoch 1350, training loss: 83.71737670898438 = 0.344382643699646 + 10.0 * 8.337299346923828
Epoch 1350, val loss: 0.4275432527065277
Epoch 1360, training loss: 83.69872283935547 = 0.34344998002052307 + 10.0 * 8.335527420043945
Epoch 1360, val loss: 0.42732498049736023
Epoch 1370, training loss: 83.68959045410156 = 0.3425317108631134 + 10.0 * 8.33470630645752
Epoch 1370, val loss: 0.4270968735218048
Epoch 1380, training loss: 83.68478393554688 = 0.34161892533302307 + 10.0 * 8.33431625366211
Epoch 1380, val loss: 0.42684057354927063
Epoch 1390, training loss: 83.682373046875 = 0.3407060205936432 + 10.0 * 8.334166526794434
Epoch 1390, val loss: 0.4265937805175781
Epoch 1400, training loss: 83.79100799560547 = 0.3397895395755768 + 10.0 * 8.345121383666992
Epoch 1400, val loss: 0.42635098099708557
Epoch 1410, training loss: 83.6915283203125 = 0.3388523459434509 + 10.0 * 8.335268020629883
Epoch 1410, val loss: 0.4261249899864197
Epoch 1420, training loss: 83.67536926269531 = 0.33793190121650696 + 10.0 * 8.333744049072266
Epoch 1420, val loss: 0.42589935660362244
Epoch 1430, training loss: 83.66990661621094 = 0.337019145488739 + 10.0 * 8.333288192749023
Epoch 1430, val loss: 0.42570486664772034
Epoch 1440, training loss: 83.70262145996094 = 0.33610790967941284 + 10.0 * 8.336651802062988
Epoch 1440, val loss: 0.42550545930862427
Epoch 1450, training loss: 83.67839050292969 = 0.3351832926273346 + 10.0 * 8.334321022033691
Epoch 1450, val loss: 0.4253041446208954
Epoch 1460, training loss: 83.65563201904297 = 0.33426183462142944 + 10.0 * 8.332137107849121
Epoch 1460, val loss: 0.42503684759140015
Epoch 1470, training loss: 83.65730285644531 = 0.3333459198474884 + 10.0 * 8.332395553588867
Epoch 1470, val loss: 0.42485347390174866
Epoch 1480, training loss: 83.6745376586914 = 0.33243298530578613 + 10.0 * 8.334210395812988
Epoch 1480, val loss: 0.4246622622013092
Epoch 1490, training loss: 83.65910339355469 = 0.3315146267414093 + 10.0 * 8.332758903503418
Epoch 1490, val loss: 0.4245445430278778
Epoch 1500, training loss: 83.67877197265625 = 0.33059602975845337 + 10.0 * 8.334817886352539
Epoch 1500, val loss: 0.4243628978729248
Epoch 1510, training loss: 83.64342498779297 = 0.32967567443847656 + 10.0 * 8.331375122070312
Epoch 1510, val loss: 0.4241228699684143
Epoch 1520, training loss: 83.63674926757812 = 0.3287631571292877 + 10.0 * 8.330798149108887
Epoch 1520, val loss: 0.4240112602710724
Epoch 1530, training loss: 83.62689208984375 = 0.32785114645957947 + 10.0 * 8.329904556274414
Epoch 1530, val loss: 0.42385217547416687
Epoch 1540, training loss: 83.65343475341797 = 0.32693958282470703 + 10.0 * 8.332649230957031
Epoch 1540, val loss: 0.4237081706523895
Epoch 1550, training loss: 83.626708984375 = 0.32601436972618103 + 10.0 * 8.330069541931152
Epoch 1550, val loss: 0.4236129820346832
Epoch 1560, training loss: 83.64106750488281 = 0.32509109377861023 + 10.0 * 8.331598281860352
Epoch 1560, val loss: 0.42339465022087097
Epoch 1570, training loss: 83.6385269165039 = 0.32416918873786926 + 10.0 * 8.331435203552246
Epoch 1570, val loss: 0.423308402299881
Epoch 1580, training loss: 83.61407470703125 = 0.3232533037662506 + 10.0 * 8.329082489013672
Epoch 1580, val loss: 0.42318499088287354
Epoch 1590, training loss: 83.6068344116211 = 0.32234057784080505 + 10.0 * 8.328449249267578
Epoch 1590, val loss: 0.42309361696243286
Epoch 1600, training loss: 83.62431335449219 = 0.3214280307292938 + 10.0 * 8.330288887023926
Epoch 1600, val loss: 0.4229423701763153
Epoch 1610, training loss: 83.64314270019531 = 0.32050254940986633 + 10.0 * 8.332263946533203
Epoch 1610, val loss: 0.4228992760181427
Epoch 1620, training loss: 83.61612701416016 = 0.3195745646953583 + 10.0 * 8.329655647277832
Epoch 1620, val loss: 0.42277804017066956
Epoch 1630, training loss: 83.60934448242188 = 0.3186480402946472 + 10.0 * 8.329069137573242
Epoch 1630, val loss: 0.42264583706855774
Epoch 1640, training loss: 83.63468933105469 = 0.3177260756492615 + 10.0 * 8.331696510314941
Epoch 1640, val loss: 0.42261645197868347
Epoch 1650, training loss: 83.61597442626953 = 0.3168017566204071 + 10.0 * 8.329916954040527
Epoch 1650, val loss: 0.42249757051467896
Epoch 1660, training loss: 83.59500122070312 = 0.31587323546409607 + 10.0 * 8.327913284301758
Epoch 1660, val loss: 0.42247116565704346
Epoch 1670, training loss: 83.5804214477539 = 0.3149504065513611 + 10.0 * 8.326547622680664
Epoch 1670, val loss: 0.4224139451980591
Epoch 1680, training loss: 83.58006286621094 = 0.3140273094177246 + 10.0 * 8.326603889465332
Epoch 1680, val loss: 0.4224003255367279
Epoch 1690, training loss: 83.58631134033203 = 0.31310221552848816 + 10.0 * 8.32732105255127
Epoch 1690, val loss: 0.422355979681015
Epoch 1700, training loss: 83.6285171508789 = 0.3121699094772339 + 10.0 * 8.331634521484375
Epoch 1700, val loss: 0.4222683608531952
Epoch 1710, training loss: 83.61042022705078 = 0.3112249970436096 + 10.0 * 8.329919815063477
Epoch 1710, val loss: 0.42217153310775757
Epoch 1720, training loss: 83.57532501220703 = 0.31027984619140625 + 10.0 * 8.326504707336426
Epoch 1720, val loss: 0.42218858003616333
Epoch 1730, training loss: 83.56394958496094 = 0.3093414604663849 + 10.0 * 8.325460433959961
Epoch 1730, val loss: 0.4221995770931244
Epoch 1740, training loss: 83.55522918701172 = 0.30840495228767395 + 10.0 * 8.324682235717773
Epoch 1740, val loss: 0.4221407175064087
Epoch 1750, training loss: 83.55338287353516 = 0.30746814608573914 + 10.0 * 8.324591636657715
Epoch 1750, val loss: 0.4221460223197937
Epoch 1760, training loss: 83.62397003173828 = 0.30652904510498047 + 10.0 * 8.331744194030762
Epoch 1760, val loss: 0.422183632850647
Epoch 1770, training loss: 83.59647369384766 = 0.3055659830570221 + 10.0 * 8.32909107208252
Epoch 1770, val loss: 0.4220924377441406
Epoch 1780, training loss: 83.56044006347656 = 0.30460819602012634 + 10.0 * 8.325582504272461
Epoch 1780, val loss: 0.4220714867115021
Epoch 1790, training loss: 83.54003143310547 = 0.30365023016929626 + 10.0 * 8.323637962341309
Epoch 1790, val loss: 0.42211994528770447
Epoch 1800, training loss: 83.53487396240234 = 0.30269721150398254 + 10.0 * 8.323217391967773
Epoch 1800, val loss: 0.4221132695674896
Epoch 1810, training loss: 83.54130554199219 = 0.3017469346523285 + 10.0 * 8.323955535888672
Epoch 1810, val loss: 0.42216530442237854
Epoch 1820, training loss: 83.6031494140625 = 0.30079400539398193 + 10.0 * 8.330235481262207
Epoch 1820, val loss: 0.42223986983299255
Epoch 1830, training loss: 83.55162048339844 = 0.299824059009552 + 10.0 * 8.325179100036621
Epoch 1830, val loss: 0.4221457242965698
Epoch 1840, training loss: 83.54235076904297 = 0.2988561987876892 + 10.0 * 8.324349403381348
Epoch 1840, val loss: 0.422244668006897
Epoch 1850, training loss: 83.56253814697266 = 0.2978866994380951 + 10.0 * 8.326464653015137
Epoch 1850, val loss: 0.4222475290298462
Epoch 1860, training loss: 83.55575561523438 = 0.29690924286842346 + 10.0 * 8.325884819030762
Epoch 1860, val loss: 0.4223717749118805
Epoch 1870, training loss: 83.5231704711914 = 0.2959272265434265 + 10.0 * 8.322724342346191
Epoch 1870, val loss: 0.4223960340023041
Epoch 1880, training loss: 83.5135498046875 = 0.29494690895080566 + 10.0 * 8.321860313415527
Epoch 1880, val loss: 0.42249563336372375
Epoch 1890, training loss: 83.50979614257812 = 0.29396337270736694 + 10.0 * 8.32158374786377
Epoch 1890, val loss: 0.42255669832229614
Epoch 1900, training loss: 83.53730773925781 = 0.2929762303829193 + 10.0 * 8.324433326721191
Epoch 1900, val loss: 0.422656774520874
Epoch 1910, training loss: 83.52481079101562 = 0.2919749319553375 + 10.0 * 8.323283195495605
Epoch 1910, val loss: 0.42275285720825195
Epoch 1920, training loss: 83.52369689941406 = 0.29097241163253784 + 10.0 * 8.323272705078125
Epoch 1920, val loss: 0.4227753281593323
Epoch 1930, training loss: 83.51227569580078 = 0.2899669408798218 + 10.0 * 8.322230339050293
Epoch 1930, val loss: 0.4229127764701843
Epoch 1940, training loss: 83.49817657470703 = 0.28896209597587585 + 10.0 * 8.320920944213867
Epoch 1940, val loss: 0.4230019748210907
Epoch 1950, training loss: 83.49471282958984 = 0.2879537045955658 + 10.0 * 8.32067584991455
Epoch 1950, val loss: 0.42310819029808044
Epoch 1960, training loss: 83.5261459350586 = 0.2869418263435364 + 10.0 * 8.323920249938965
Epoch 1960, val loss: 0.4232146739959717
Epoch 1970, training loss: 83.50359344482422 = 0.28592419624328613 + 10.0 * 8.32176685333252
Epoch 1970, val loss: 0.4233963191509247
Epoch 1980, training loss: 83.52510070800781 = 0.2849010229110718 + 10.0 * 8.324019432067871
Epoch 1980, val loss: 0.42360585927963257
Epoch 1990, training loss: 83.4852066040039 = 0.2838706076145172 + 10.0 * 8.320134162902832
Epoch 1990, val loss: 0.4235406517982483
Epoch 2000, training loss: 83.48015594482422 = 0.2828390896320343 + 10.0 * 8.319731712341309
Epoch 2000, val loss: 0.4238258898258209
Epoch 2010, training loss: 83.51355743408203 = 0.28180477023124695 + 10.0 * 8.323175430297852
Epoch 2010, val loss: 0.42394810914993286
Epoch 2020, training loss: 83.50424194335938 = 0.28076180815696716 + 10.0 * 8.322347640991211
Epoch 2020, val loss: 0.42422783374786377
Epoch 2030, training loss: 83.47452545166016 = 0.27970942854881287 + 10.0 * 8.31948184967041
Epoch 2030, val loss: 0.4242752492427826
Epoch 2040, training loss: 83.46910095214844 = 0.2786557078361511 + 10.0 * 8.31904411315918
Epoch 2040, val loss: 0.42452487349510193
Epoch 2050, training loss: 83.46420288085938 = 0.2775964140892029 + 10.0 * 8.318660736083984
Epoch 2050, val loss: 0.424699604511261
Epoch 2060, training loss: 83.47120666503906 = 0.27653393149375916 + 10.0 * 8.319467544555664
Epoch 2060, val loss: 0.42489829659461975
Epoch 2070, training loss: 83.5289306640625 = 0.2754654884338379 + 10.0 * 8.325345993041992
Epoch 2070, val loss: 0.4250454008579254
Epoch 2080, training loss: 83.47643280029297 = 0.2743820548057556 + 10.0 * 8.320204734802246
Epoch 2080, val loss: 0.4253303110599518
Epoch 2090, training loss: 83.45733642578125 = 0.2732957899570465 + 10.0 * 8.318404197692871
Epoch 2090, val loss: 0.4255979061126709
Epoch 2100, training loss: 83.44835662841797 = 0.27221232652664185 + 10.0 * 8.317614555358887
Epoch 2100, val loss: 0.4258441925048828
Epoch 2110, training loss: 83.45259094238281 = 0.2711251974105835 + 10.0 * 8.318146705627441
Epoch 2110, val loss: 0.42603233456611633
Epoch 2120, training loss: 83.47991180419922 = 0.27003416419029236 + 10.0 * 8.320987701416016
Epoch 2120, val loss: 0.42630574107170105
Epoch 2130, training loss: 83.44999694824219 = 0.2689383924007416 + 10.0 * 8.318105697631836
Epoch 2130, val loss: 0.4265258312225342
Epoch 2140, training loss: 83.44083404541016 = 0.26783257722854614 + 10.0 * 8.317300796508789
Epoch 2140, val loss: 0.42678016424179077
Epoch 2150, training loss: 83.46485900878906 = 0.26672953367233276 + 10.0 * 8.319812774658203
Epoch 2150, val loss: 0.4269425868988037
Epoch 2160, training loss: 83.447021484375 = 0.26561152935028076 + 10.0 * 8.318140983581543
Epoch 2160, val loss: 0.4273488223552704
Epoch 2170, training loss: 83.45079040527344 = 0.26449328660964966 + 10.0 * 8.318629264831543
Epoch 2170, val loss: 0.4275713562965393
Epoch 2180, training loss: 83.44815826416016 = 0.26337286829948425 + 10.0 * 8.31847858428955
Epoch 2180, val loss: 0.42787355184555054
Epoch 2190, training loss: 83.42137145996094 = 0.2622498869895935 + 10.0 * 8.315912246704102
Epoch 2190, val loss: 0.42826712131500244
Epoch 2200, training loss: 83.42247009277344 = 0.2611253559589386 + 10.0 * 8.316134452819824
Epoch 2200, val loss: 0.42859482765197754
Epoch 2210, training loss: 83.43367004394531 = 0.2599967122077942 + 10.0 * 8.317367553710938
Epoch 2210, val loss: 0.42891034483909607
Epoch 2220, training loss: 83.44853973388672 = 0.25886115431785583 + 10.0 * 8.318967819213867
Epoch 2220, val loss: 0.42924734950065613
Epoch 2230, training loss: 83.4266357421875 = 0.2577192187309265 + 10.0 * 8.31689167022705
Epoch 2230, val loss: 0.4296000897884369
Epoch 2240, training loss: 83.4232177734375 = 0.2565731704235077 + 10.0 * 8.31666374206543
Epoch 2240, val loss: 0.4298834800720215
Epoch 2250, training loss: 83.42017364501953 = 0.2554195821285248 + 10.0 * 8.316475868225098
Epoch 2250, val loss: 0.43024107813835144
Epoch 2260, training loss: 83.42105865478516 = 0.2542669177055359 + 10.0 * 8.316679000854492
Epoch 2260, val loss: 0.43058568239212036
Epoch 2270, training loss: 83.42990112304688 = 0.2531057894229889 + 10.0 * 8.317679405212402
Epoch 2270, val loss: 0.4309867024421692
Epoch 2280, training loss: 83.41458129882812 = 0.25193825364112854 + 10.0 * 8.316264152526855
Epoch 2280, val loss: 0.4315878748893738
Epoch 2290, training loss: 83.4058609008789 = 0.2507666051387787 + 10.0 * 8.315509796142578
Epoch 2290, val loss: 0.4319227337837219
Epoch 2300, training loss: 83.39832305908203 = 0.249592125415802 + 10.0 * 8.314872741699219
Epoch 2300, val loss: 0.4322854280471802
Epoch 2310, training loss: 83.3944320678711 = 0.24841779470443726 + 10.0 * 8.314600944519043
Epoch 2310, val loss: 0.4327998757362366
Epoch 2320, training loss: 83.41769409179688 = 0.24723897874355316 + 10.0 * 8.317045211791992
Epoch 2320, val loss: 0.43316277861595154
Epoch 2330, training loss: 83.39043426513672 = 0.2460498809814453 + 10.0 * 8.314438819885254
Epoch 2330, val loss: 0.433675616979599
Epoch 2340, training loss: 83.43738555908203 = 0.24486570060253143 + 10.0 * 8.319252014160156
Epoch 2340, val loss: 0.4342193007469177
Epoch 2350, training loss: 83.39957427978516 = 0.243665412068367 + 10.0 * 8.315590858459473
Epoch 2350, val loss: 0.4345616102218628
Epoch 2360, training loss: 83.37865447998047 = 0.24246738851070404 + 10.0 * 8.313618659973145
Epoch 2360, val loss: 0.4351324141025543
Epoch 2370, training loss: 83.37408447265625 = 0.24126426875591278 + 10.0 * 8.313282012939453
Epoch 2370, val loss: 0.4355725646018982
Epoch 2380, training loss: 83.37020111083984 = 0.24005845189094543 + 10.0 * 8.313014030456543
Epoch 2380, val loss: 0.43607857823371887
Epoch 2390, training loss: 83.38028717041016 = 0.23884689807891846 + 10.0 * 8.314144134521484
Epoch 2390, val loss: 0.4366026818752289
Epoch 2400, training loss: 83.4399185180664 = 0.2376297116279602 + 10.0 * 8.320228576660156
Epoch 2400, val loss: 0.43712693452835083
Epoch 2410, training loss: 83.38264465332031 = 0.23640361428260803 + 10.0 * 8.314623832702637
Epoch 2410, val loss: 0.4378039240837097
Epoch 2420, training loss: 83.365478515625 = 0.23516841232776642 + 10.0 * 8.313031196594238
Epoch 2420, val loss: 0.4384308159351349
Epoch 2430, training loss: 83.38003540039062 = 0.233932226896286 + 10.0 * 8.314610481262207
Epoch 2430, val loss: 0.4390746057033539
Epoch 2440, training loss: 83.36551666259766 = 0.23268651962280273 + 10.0 * 8.31328296661377
Epoch 2440, val loss: 0.43963757157325745
Epoch 2450, training loss: 83.3570785522461 = 0.2314366102218628 + 10.0 * 8.3125638961792
Epoch 2450, val loss: 0.4403027594089508
Epoch 2460, training loss: 83.34738159179688 = 0.23017878830432892 + 10.0 * 8.31171989440918
Epoch 2460, val loss: 0.4409436583518982
Epoch 2470, training loss: 83.36309051513672 = 0.2289217859506607 + 10.0 * 8.313417434692383
Epoch 2470, val loss: 0.4415435194969177
Epoch 2480, training loss: 83.36654663085938 = 0.2276616096496582 + 10.0 * 8.313888549804688
Epoch 2480, val loss: 0.44224488735198975
Epoch 2490, training loss: 83.3875732421875 = 0.22640056908130646 + 10.0 * 8.316117286682129
Epoch 2490, val loss: 0.4432013928890228
Epoch 2500, training loss: 83.35919189453125 = 0.2251247763633728 + 10.0 * 8.313405990600586
Epoch 2500, val loss: 0.4439270794391632
Epoch 2510, training loss: 83.35844421386719 = 0.22384525835514069 + 10.0 * 8.313459396362305
Epoch 2510, val loss: 0.4446268081665039
Epoch 2520, training loss: 83.33625793457031 = 0.22256183624267578 + 10.0 * 8.311368942260742
Epoch 2520, val loss: 0.44534847140312195
Epoch 2530, training loss: 83.3293685913086 = 0.2212730497121811 + 10.0 * 8.310809135437012
Epoch 2530, val loss: 0.4461052119731903
Epoch 2540, training loss: 83.33642578125 = 0.2199830859899521 + 10.0 * 8.311643600463867
Epoch 2540, val loss: 0.4470421373844147
Epoch 2550, training loss: 83.408447265625 = 0.2187015563249588 + 10.0 * 8.318974494934082
Epoch 2550, val loss: 0.44792985916137695
Epoch 2560, training loss: 83.35000610351562 = 0.21739059686660767 + 10.0 * 8.313261032104492
Epoch 2560, val loss: 0.4484206736087799
Epoch 2570, training loss: 83.32971954345703 = 0.21608003973960876 + 10.0 * 8.31136417388916
Epoch 2570, val loss: 0.449514776468277
Epoch 2580, training loss: 83.33433532714844 = 0.2147655189037323 + 10.0 * 8.311956405639648
Epoch 2580, val loss: 0.45023560523986816
Epoch 2590, training loss: 83.3527603149414 = 0.21344639360904694 + 10.0 * 8.313931465148926
Epoch 2590, val loss: 0.4512844979763031
Epoch 2600, training loss: 83.32634735107422 = 0.21212196350097656 + 10.0 * 8.311422348022461
Epoch 2600, val loss: 0.4521992802619934
Epoch 2610, training loss: 83.3295669555664 = 0.21079200506210327 + 10.0 * 8.311877250671387
Epoch 2610, val loss: 0.45329755544662476
Epoch 2620, training loss: 83.35844421386719 = 0.20945973694324493 + 10.0 * 8.314898490905762
Epoch 2620, val loss: 0.45429056882858276
Epoch 2630, training loss: 83.31381225585938 = 0.20811240375041962 + 10.0 * 8.310569763183594
Epoch 2630, val loss: 0.4552537500858307
Epoch 2640, training loss: 83.30230712890625 = 0.20675987005233765 + 10.0 * 8.309555053710938
Epoch 2640, val loss: 0.45627540349960327
Epoch 2650, training loss: 83.32234191894531 = 0.2054109275341034 + 10.0 * 8.31169319152832
Epoch 2650, val loss: 0.4574045240879059
Epoch 2660, training loss: 83.31136322021484 = 0.20405566692352295 + 10.0 * 8.310730934143066
Epoch 2660, val loss: 0.4585012197494507
Epoch 2670, training loss: 83.3042984008789 = 0.20269308984279633 + 10.0 * 8.310160636901855
Epoch 2670, val loss: 0.4596737325191498
Epoch 2680, training loss: 83.29718780517578 = 0.20132853090763092 + 10.0 * 8.309585571289062
Epoch 2680, val loss: 0.46079912781715393
Epoch 2690, training loss: 83.3105239868164 = 0.19995902478694916 + 10.0 * 8.311056137084961
Epoch 2690, val loss: 0.46200868487358093
Epoch 2700, training loss: 83.3084487915039 = 0.19858498871326447 + 10.0 * 8.310986518859863
Epoch 2700, val loss: 0.46315255761146545
Epoch 2710, training loss: 83.35810852050781 = 0.19722238183021545 + 10.0 * 8.316088676452637
Epoch 2710, val loss: 0.46394243836402893
Epoch 2720, training loss: 83.29424285888672 = 0.19582891464233398 + 10.0 * 8.30984115600586
Epoch 2720, val loss: 0.4656190276145935
Epoch 2730, training loss: 83.28004455566406 = 0.1944449096918106 + 10.0 * 8.308560371398926
Epoch 2730, val loss: 0.4669208526611328
Epoch 2740, training loss: 83.27693176269531 = 0.19305869936943054 + 10.0 * 8.30838680267334
Epoch 2740, val loss: 0.46807241439819336
Epoch 2750, training loss: 83.2774429321289 = 0.19166576862335205 + 10.0 * 8.308577537536621
Epoch 2750, val loss: 0.4694098234176636
Epoch 2760, training loss: 83.30647277832031 = 0.19027462601661682 + 10.0 * 8.311619758605957
Epoch 2760, val loss: 0.4707852303981781
Epoch 2770, training loss: 83.30435180664062 = 0.18887445330619812 + 10.0 * 8.311548233032227
Epoch 2770, val loss: 0.47214844822883606
Epoch 2780, training loss: 83.28180694580078 = 0.18747550249099731 + 10.0 * 8.309432983398438
Epoch 2780, val loss: 0.4736298620700836
Epoch 2790, training loss: 83.26689910888672 = 0.186062291264534 + 10.0 * 8.308083534240723
Epoch 2790, val loss: 0.47469601035118103
Epoch 2800, training loss: 83.2624282836914 = 0.18465086817741394 + 10.0 * 8.307777404785156
Epoch 2800, val loss: 0.4762990474700928
Epoch 2810, training loss: 83.30829620361328 = 0.18324662744998932 + 10.0 * 8.312504768371582
Epoch 2810, val loss: 0.4778396487236023
Epoch 2820, training loss: 83.26876068115234 = 0.18183587491512299 + 10.0 * 8.30869197845459
Epoch 2820, val loss: 0.47908473014831543
Epoch 2830, training loss: 83.29022979736328 = 0.18043078482151031 + 10.0 * 8.310979843139648
Epoch 2830, val loss: 0.480935275554657
Epoch 2840, training loss: 83.25537109375 = 0.17901134490966797 + 10.0 * 8.307636260986328
Epoch 2840, val loss: 0.48220646381378174
Epoch 2850, training loss: 83.25204467773438 = 0.1776001900434494 + 10.0 * 8.30744457244873
Epoch 2850, val loss: 0.4838393032550812
Epoch 2860, training loss: 83.25440979003906 = 0.17618978023529053 + 10.0 * 8.307821273803711
Epoch 2860, val loss: 0.4852479100227356
Epoch 2870, training loss: 83.263671875 = 0.1747712343931198 + 10.0 * 8.308889389038086
Epoch 2870, val loss: 0.4868895411491394
Epoch 2880, training loss: 83.26990509033203 = 0.17335174977779388 + 10.0 * 8.30965518951416
Epoch 2880, val loss: 0.4885723292827606
Epoch 2890, training loss: 83.2631607055664 = 0.17192408442497253 + 10.0 * 8.309123992919922
Epoch 2890, val loss: 0.49050256609916687
Epoch 2900, training loss: 83.25032806396484 = 0.1704968959093094 + 10.0 * 8.3079833984375
Epoch 2900, val loss: 0.49228042364120483
Epoch 2910, training loss: 83.23963928222656 = 0.1690686047077179 + 10.0 * 8.30705738067627
Epoch 2910, val loss: 0.4936870336532593
Epoch 2920, training loss: 83.27238464355469 = 0.16764825582504272 + 10.0 * 8.310473442077637
Epoch 2920, val loss: 0.49535536766052246
Epoch 2930, training loss: 83.24772644042969 = 0.1662163883447647 + 10.0 * 8.308151245117188
Epoch 2930, val loss: 0.4975055158138275
Epoch 2940, training loss: 83.23041534423828 = 0.16478419303894043 + 10.0 * 8.306562423706055
Epoch 2940, val loss: 0.4993075430393219
Epoch 2950, training loss: 83.22457122802734 = 0.1633521020412445 + 10.0 * 8.306121826171875
Epoch 2950, val loss: 0.5011162161827087
Epoch 2960, training loss: 83.2432861328125 = 0.16193068027496338 + 10.0 * 8.308135032653809
Epoch 2960, val loss: 0.5029038786888123
Epoch 2970, training loss: 83.23809051513672 = 0.16050557792186737 + 10.0 * 8.307758331298828
Epoch 2970, val loss: 0.5046135783195496
Epoch 2980, training loss: 83.32231903076172 = 0.15911245346069336 + 10.0 * 8.316320419311523
Epoch 2980, val loss: 0.5064207911491394
Epoch 2990, training loss: 83.23025512695312 = 0.15768545866012573 + 10.0 * 8.307256698608398
Epoch 2990, val loss: 0.5083809494972229
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8183663115169963
0.841556183438383
The final CL Acc:0.81414, 0.00671, The final GNN Acc:0.84127, 0.00021
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110674])
remove edge: torch.Size([2, 66278])
updated graph: torch.Size([2, 88304])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 106.9328384399414 = 1.110771894454956 + 10.0 * 10.582206726074219
Epoch 0, val loss: 1.1098712682724
Epoch 10, training loss: 106.90966796875 = 1.102461338043213 + 10.0 * 10.580720901489258
Epoch 10, val loss: 1.10152268409729
Epoch 20, training loss: 106.75692749023438 = 1.0928540229797363 + 10.0 * 10.566407203674316
Epoch 20, val loss: 1.0918065309524536
Epoch 30, training loss: 105.6156005859375 = 1.0815975666046143 + 10.0 * 10.453400611877441
Epoch 30, val loss: 1.0804578065872192
Epoch 40, training loss: 100.65718078613281 = 1.0690083503723145 + 10.0 * 9.958817481994629
Epoch 40, val loss: 1.0677627325057983
Epoch 50, training loss: 95.38025665283203 = 1.057356595993042 + 10.0 * 9.432290077209473
Epoch 50, val loss: 1.056680679321289
Epoch 60, training loss: 93.15034484863281 = 1.0481586456298828 + 10.0 * 9.21021842956543
Epoch 60, val loss: 1.0477224588394165
Epoch 70, training loss: 91.9454345703125 = 1.0396685600280762 + 10.0 * 9.090577125549316
Epoch 70, val loss: 1.0395995378494263
Epoch 80, training loss: 91.0458984375 = 1.032128095626831 + 10.0 * 9.00137710571289
Epoch 80, val loss: 1.032265305519104
Epoch 90, training loss: 90.1324234008789 = 1.0252043008804321 + 10.0 * 8.910721778869629
Epoch 90, val loss: 1.0254234075546265
Epoch 100, training loss: 89.51483917236328 = 1.0184537172317505 + 10.0 * 8.849637985229492
Epoch 100, val loss: 1.018683910369873
Epoch 110, training loss: 89.01287841796875 = 1.0113537311553955 + 10.0 * 8.800152778625488
Epoch 110, val loss: 1.0116398334503174
Epoch 120, training loss: 88.64199829101562 = 1.0039948225021362 + 10.0 * 8.763799667358398
Epoch 120, val loss: 1.0043400526046753
Epoch 130, training loss: 88.21302032470703 = 0.9963446259498596 + 10.0 * 8.721667289733887
Epoch 130, val loss: 0.9967249035835266
Epoch 140, training loss: 87.72016143798828 = 0.9887419939041138 + 10.0 * 8.673142433166504
Epoch 140, val loss: 0.9892164468765259
Epoch 150, training loss: 87.33242797851562 = 0.9810183048248291 + 10.0 * 8.635141372680664
Epoch 150, val loss: 0.981505274772644
Epoch 160, training loss: 86.99678802490234 = 0.9724746346473694 + 10.0 * 8.602431297302246
Epoch 160, val loss: 0.9729356169700623
Epoch 170, training loss: 86.64970397949219 = 0.9631443619728088 + 10.0 * 8.568655967712402
Epoch 170, val loss: 0.9636442065238953
Epoch 180, training loss: 86.3814697265625 = 0.9531465768814087 + 10.0 * 8.542832374572754
Epoch 180, val loss: 0.9535837173461914
Epoch 190, training loss: 86.28353118896484 = 0.9418957829475403 + 10.0 * 8.534163475036621
Epoch 190, val loss: 0.9421819448471069
Epoch 200, training loss: 86.03276824951172 = 0.9289740324020386 + 10.0 * 8.510379791259766
Epoch 200, val loss: 0.9291744232177734
Epoch 210, training loss: 85.8908462524414 = 0.9149692058563232 + 10.0 * 8.497587203979492
Epoch 210, val loss: 0.9151679873466492
Epoch 220, training loss: 85.84395599365234 = 0.9001780152320862 + 10.0 * 8.494378089904785
Epoch 220, val loss: 0.9003521203994751
Epoch 230, training loss: 85.68939208984375 = 0.8844646215438843 + 10.0 * 8.48049259185791
Epoch 230, val loss: 0.884727418422699
Epoch 240, training loss: 85.55168914794922 = 0.8682757616043091 + 10.0 * 8.468340873718262
Epoch 240, val loss: 0.8686332106590271
Epoch 250, training loss: 85.44796752929688 = 0.8516806960105896 + 10.0 * 8.45962905883789
Epoch 250, val loss: 0.8521425724029541
Epoch 260, training loss: 85.35578918457031 = 0.8346376419067383 + 10.0 * 8.452115058898926
Epoch 260, val loss: 0.8352760672569275
Epoch 270, training loss: 85.28692626953125 = 0.81719970703125 + 10.0 * 8.446972846984863
Epoch 270, val loss: 0.8178964257240295
Epoch 280, training loss: 85.2012710571289 = 0.7995231747627258 + 10.0 * 8.44017505645752
Epoch 280, val loss: 0.8005465865135193
Epoch 290, training loss: 85.11863708496094 = 0.7818779349327087 + 10.0 * 8.433675765991211
Epoch 290, val loss: 0.7832667231559753
Epoch 300, training loss: 85.05783081054688 = 0.7642232179641724 + 10.0 * 8.429361343383789
Epoch 300, val loss: 0.7659792900085449
Epoch 310, training loss: 85.0025634765625 = 0.746468186378479 + 10.0 * 8.425609588623047
Epoch 310, val loss: 0.748848021030426
Epoch 320, training loss: 84.91252899169922 = 0.7289443612098694 + 10.0 * 8.41835880279541
Epoch 320, val loss: 0.7318706512451172
Epoch 330, training loss: 84.85034942626953 = 0.7117067575454712 + 10.0 * 8.413864135742188
Epoch 330, val loss: 0.7152449488639832
Epoch 340, training loss: 84.85852813720703 = 0.6947161555290222 + 10.0 * 8.416380882263184
Epoch 340, val loss: 0.6989890336990356
Epoch 350, training loss: 84.78369903564453 = 0.677983283996582 + 10.0 * 8.410571098327637
Epoch 350, val loss: 0.6829708814620972
Epoch 360, training loss: 84.710205078125 = 0.6616948843002319 + 10.0 * 8.404850959777832
Epoch 360, val loss: 0.6675790548324585
Epoch 370, training loss: 84.64138793945312 = 0.6460362076759338 + 10.0 * 8.399535179138184
Epoch 370, val loss: 0.6527971625328064
Epoch 380, training loss: 84.59732818603516 = 0.6309655904769897 + 10.0 * 8.396636009216309
Epoch 380, val loss: 0.6386530995368958
Epoch 390, training loss: 84.58837127685547 = 0.61643385887146 + 10.0 * 8.397193908691406
Epoch 390, val loss: 0.6252245903015137
Epoch 400, training loss: 84.5384521484375 = 0.6024916768074036 + 10.0 * 8.393595695495605
Epoch 400, val loss: 0.6121487617492676
Epoch 410, training loss: 84.48011779785156 = 0.5892980694770813 + 10.0 * 8.389081954956055
Epoch 410, val loss: 0.600207507610321
Epoch 420, training loss: 84.43698120117188 = 0.5769324898719788 + 10.0 * 8.386004447937012
Epoch 420, val loss: 0.5889376997947693
Epoch 430, training loss: 84.40838623046875 = 0.5652886629104614 + 10.0 * 8.384309768676758
Epoch 430, val loss: 0.5784112811088562
Epoch 440, training loss: 84.44668579101562 = 0.554328978061676 + 10.0 * 8.389235496520996
Epoch 440, val loss: 0.5685573816299438
Epoch 450, training loss: 84.38829040527344 = 0.543988823890686 + 10.0 * 8.384429931640625
Epoch 450, val loss: 0.559535562992096
Epoch 460, training loss: 84.329833984375 = 0.5344290137290955 + 10.0 * 8.37954044342041
Epoch 460, val loss: 0.550977349281311
Epoch 470, training loss: 84.29744720458984 = 0.5255633592605591 + 10.0 * 8.377187728881836
Epoch 470, val loss: 0.5434563755989075
Epoch 480, training loss: 84.35126495361328 = 0.5173743963241577 + 10.0 * 8.383389472961426
Epoch 480, val loss: 0.5363192558288574
Epoch 490, training loss: 84.25223541259766 = 0.5096311569213867 + 10.0 * 8.374260902404785
Epoch 490, val loss: 0.5297144055366516
Epoch 500, training loss: 84.22032165527344 = 0.5025219321250916 + 10.0 * 8.371779441833496
Epoch 500, val loss: 0.5238175392150879
Epoch 510, training loss: 84.19730377197266 = 0.495932012796402 + 10.0 * 8.370137214660645
Epoch 510, val loss: 0.518339216709137
Epoch 520, training loss: 84.2419204711914 = 0.48973605036735535 + 10.0 * 8.375218391418457
Epoch 520, val loss: 0.5134955048561096
Epoch 530, training loss: 84.19298553466797 = 0.48389723896980286 + 10.0 * 8.370908737182617
Epoch 530, val loss: 0.5083415508270264
Epoch 540, training loss: 84.1482162475586 = 0.47844165563583374 + 10.0 * 8.36697769165039
Epoch 540, val loss: 0.5041606426239014
Epoch 550, training loss: 84.12745666503906 = 0.473370760679245 + 10.0 * 8.365407943725586
Epoch 550, val loss: 0.5001866817474365
Epoch 560, training loss: 84.1184310913086 = 0.46861621737480164 + 10.0 * 8.364981651306152
Epoch 560, val loss: 0.4964519739151001
Epoch 570, training loss: 84.08085632324219 = 0.46410441398620605 + 10.0 * 8.361675262451172
Epoch 570, val loss: 0.4930303692817688
Epoch 580, training loss: 84.11666107177734 = 0.45982304215431213 + 10.0 * 8.365683555603027
Epoch 580, val loss: 0.489848792552948
Epoch 590, training loss: 84.08364868164062 = 0.4557308554649353 + 10.0 * 8.362791061401367
Epoch 590, val loss: 0.48669561743736267
Epoch 600, training loss: 84.04385375976562 = 0.45181912183761597 + 10.0 * 8.359203338623047
Epoch 600, val loss: 0.48381465673446655
Epoch 610, training loss: 84.0195083618164 = 0.44814249873161316 + 10.0 * 8.357136726379395
Epoch 610, val loss: 0.4812181293964386
Epoch 620, training loss: 84.01014709472656 = 0.4446376860141754 + 10.0 * 8.356550216674805
Epoch 620, val loss: 0.47873079776763916
Epoch 630, training loss: 84.05704498291016 = 0.44123077392578125 + 10.0 * 8.361581802368164
Epoch 630, val loss: 0.47630035877227783
Epoch 640, training loss: 83.9999771118164 = 0.437897652387619 + 10.0 * 8.356207847595215
Epoch 640, val loss: 0.47387486696243286
Epoch 650, training loss: 83.96857452392578 = 0.434746116399765 + 10.0 * 8.35338306427002
Epoch 650, val loss: 0.4716911315917969
Epoch 660, training loss: 83.96131896972656 = 0.4317333400249481 + 10.0 * 8.352958679199219
Epoch 660, val loss: 0.46960684657096863
Epoch 670, training loss: 83.99772644042969 = 0.4287838637828827 + 10.0 * 8.356893539428711
Epoch 670, val loss: 0.4676435887813568
Epoch 680, training loss: 83.95452880859375 = 0.4259164035320282 + 10.0 * 8.352861404418945
Epoch 680, val loss: 0.4654785096645355
Epoch 690, training loss: 83.96780395507812 = 0.4231279492378235 + 10.0 * 8.354467391967773
Epoch 690, val loss: 0.4636128842830658
Epoch 700, training loss: 83.91690826416016 = 0.4204151928424835 + 10.0 * 8.349649429321289
Epoch 700, val loss: 0.46183469891548157
Epoch 710, training loss: 83.90074157714844 = 0.41779911518096924 + 10.0 * 8.348294258117676
Epoch 710, val loss: 0.46006476879119873
Epoch 720, training loss: 83.92861938476562 = 0.4152679145336151 + 10.0 * 8.351335525512695
Epoch 720, val loss: 0.45820775628089905
Epoch 730, training loss: 83.90264129638672 = 0.4127432405948639 + 10.0 * 8.348989486694336
Epoch 730, val loss: 0.4567621946334839
Epoch 740, training loss: 83.88592529296875 = 0.41029492020606995 + 10.0 * 8.347562789916992
Epoch 740, val loss: 0.45511311292648315
Epoch 750, training loss: 83.86298370361328 = 0.4079071581363678 + 10.0 * 8.345507621765137
Epoch 750, val loss: 0.4535372257232666
Epoch 760, training loss: 83.86235809326172 = 0.4055843651294708 + 10.0 * 8.345677375793457
Epoch 760, val loss: 0.4519997835159302
Epoch 770, training loss: 83.86824035644531 = 0.4032810926437378 + 10.0 * 8.346495628356934
Epoch 770, val loss: 0.45053979754447937
Epoch 780, training loss: 83.82970428466797 = 0.40100592374801636 + 10.0 * 8.342869758605957
Epoch 780, val loss: 0.4490020275115967
Epoch 790, training loss: 83.8268814086914 = 0.3988269865512848 + 10.0 * 8.342805862426758
Epoch 790, val loss: 0.4475877285003662
Epoch 800, training loss: 83.86634826660156 = 0.39665886759757996 + 10.0 * 8.346968650817871
Epoch 800, val loss: 0.44628143310546875
Epoch 810, training loss: 83.814208984375 = 0.3944963216781616 + 10.0 * 8.341971397399902
Epoch 810, val loss: 0.444885790348053
Epoch 820, training loss: 83.78731536865234 = 0.3924294412136078 + 10.0 * 8.33948802947998
Epoch 820, val loss: 0.44351068139076233
Epoch 830, training loss: 83.77870178222656 = 0.3903961181640625 + 10.0 * 8.338830947875977
Epoch 830, val loss: 0.44227027893066406
Epoch 840, training loss: 83.84125518798828 = 0.3883882164955139 + 10.0 * 8.34528636932373
Epoch 840, val loss: 0.4412344694137573
Epoch 850, training loss: 83.82511138916016 = 0.38634631037712097 + 10.0 * 8.343876838684082
Epoch 850, val loss: 0.43968573212623596
Epoch 860, training loss: 83.75408935546875 = 0.3843541741371155 + 10.0 * 8.336973190307617
Epoch 860, val loss: 0.43847882747650146
Epoch 870, training loss: 83.74869537353516 = 0.38243818283081055 + 10.0 * 8.336626052856445
Epoch 870, val loss: 0.4374130368232727
Epoch 880, training loss: 83.73632049560547 = 0.3805690407752991 + 10.0 * 8.335575103759766
Epoch 880, val loss: 0.43622589111328125
Epoch 890, training loss: 83.8384017944336 = 0.3787025809288025 + 10.0 * 8.345970153808594
Epoch 890, val loss: 0.43496590852737427
Epoch 900, training loss: 83.75611114501953 = 0.37680959701538086 + 10.0 * 8.337930679321289
Epoch 900, val loss: 0.43406933546066284
Epoch 910, training loss: 83.72381591796875 = 0.37497377395629883 + 10.0 * 8.334883689880371
Epoch 910, val loss: 0.4329747259616852
Epoch 920, training loss: 83.74407958984375 = 0.3731832206249237 + 10.0 * 8.337089538574219
Epoch 920, val loss: 0.4319080710411072
Epoch 930, training loss: 83.70405578613281 = 0.3713899552822113 + 10.0 * 8.333266258239746
Epoch 930, val loss: 0.4308396875858307
Epoch 940, training loss: 83.68702697753906 = 0.36965012550354004 + 10.0 * 8.331737518310547
Epoch 940, val loss: 0.429884135723114
Epoch 950, training loss: 83.68501281738281 = 0.36793723702430725 + 10.0 * 8.331707954406738
Epoch 950, val loss: 0.4289543032646179
Epoch 960, training loss: 83.74296569824219 = 0.3662376403808594 + 10.0 * 8.33767318725586
Epoch 960, val loss: 0.42782458662986755
Epoch 970, training loss: 83.7054443359375 = 0.36446765065193176 + 10.0 * 8.334096908569336
Epoch 970, val loss: 0.4270114600658417
Epoch 980, training loss: 83.6678237915039 = 0.36276382207870483 + 10.0 * 8.330506324768066
Epoch 980, val loss: 0.4261004626750946
Epoch 990, training loss: 83.65791320800781 = 0.36111584305763245 + 10.0 * 8.329679489135742
Epoch 990, val loss: 0.4251953363418579
Epoch 1000, training loss: 83.64662170410156 = 0.3594810366630554 + 10.0 * 8.328714370727539
Epoch 1000, val loss: 0.42435964941978455
Epoch 1010, training loss: 83.70551300048828 = 0.35786041617393494 + 10.0 * 8.334765434265137
Epoch 1010, val loss: 0.4236047863960266
Epoch 1020, training loss: 83.71052551269531 = 0.35621315240859985 + 10.0 * 8.335431098937988
Epoch 1020, val loss: 0.42267048358917236
Epoch 1030, training loss: 83.64202880859375 = 0.3545546531677246 + 10.0 * 8.328747749328613
Epoch 1030, val loss: 0.4217173159122467
Epoch 1040, training loss: 83.63001251220703 = 0.35296308994293213 + 10.0 * 8.327704429626465
Epoch 1040, val loss: 0.42092788219451904
Epoch 1050, training loss: 83.61345672607422 = 0.3514060378074646 + 10.0 * 8.326205253601074
Epoch 1050, val loss: 0.4201262295246124
Epoch 1060, training loss: 83.60610961914062 = 0.3498729467391968 + 10.0 * 8.325623512268066
Epoch 1060, val loss: 0.4193059504032135
Epoch 1070, training loss: 83.607666015625 = 0.348348468542099 + 10.0 * 8.325931549072266
Epoch 1070, val loss: 0.418479859828949
Epoch 1080, training loss: 83.68196105957031 = 0.34681883454322815 + 10.0 * 8.333514213562012
Epoch 1080, val loss: 0.41754379868507385
Epoch 1090, training loss: 83.62158966064453 = 0.3452526926994324 + 10.0 * 8.32763385772705
Epoch 1090, val loss: 0.4171571135520935
Epoch 1100, training loss: 83.61184692382812 = 0.3437379002571106 + 10.0 * 8.326810836791992
Epoch 1100, val loss: 0.41621068120002747
Epoch 1110, training loss: 83.58252716064453 = 0.34221386909484863 + 10.0 * 8.324030876159668
Epoch 1110, val loss: 0.4155290126800537
Epoch 1120, training loss: 83.58191680908203 = 0.3407157063484192 + 10.0 * 8.32412052154541
Epoch 1120, val loss: 0.41484811902046204
Epoch 1130, training loss: 83.59237670898438 = 0.33923354744911194 + 10.0 * 8.32531452178955
Epoch 1130, val loss: 0.4141010046005249
Epoch 1140, training loss: 83.57117462158203 = 0.33776187896728516 + 10.0 * 8.323341369628906
Epoch 1140, val loss: 0.4135143458843231
Epoch 1150, training loss: 83.5552978515625 = 0.3362947106361389 + 10.0 * 8.321900367736816
Epoch 1150, val loss: 0.4127911329269409
Epoch 1160, training loss: 83.56391143798828 = 0.3348349928855896 + 10.0 * 8.322908401489258
Epoch 1160, val loss: 0.4120989441871643
Epoch 1170, training loss: 83.54734802246094 = 0.3333776593208313 + 10.0 * 8.32139778137207
Epoch 1170, val loss: 0.41142377257347107
Epoch 1180, training loss: 83.53431701660156 = 0.33192795515060425 + 10.0 * 8.320239067077637
Epoch 1180, val loss: 0.41087210178375244
Epoch 1190, training loss: 83.5307388305664 = 0.3305104076862335 + 10.0 * 8.320022583007812
Epoch 1190, val loss: 0.41014036536216736
Epoch 1200, training loss: 83.57176208496094 = 0.3290891647338867 + 10.0 * 8.324267387390137
Epoch 1200, val loss: 0.4095722436904907
Epoch 1210, training loss: 83.55248260498047 = 0.32766130566596985 + 10.0 * 8.322482109069824
Epoch 1210, val loss: 0.4091835916042328
Epoch 1220, training loss: 83.51324462890625 = 0.3262569308280945 + 10.0 * 8.31869888305664
Epoch 1220, val loss: 0.4084589183330536
Epoch 1230, training loss: 83.49732208251953 = 0.3248801827430725 + 10.0 * 8.317243576049805
Epoch 1230, val loss: 0.4079972505569458
Epoch 1240, training loss: 83.4946517944336 = 0.3235272765159607 + 10.0 * 8.317112922668457
Epoch 1240, val loss: 0.40740761160850525
Epoch 1250, training loss: 83.53730773925781 = 0.3221682012081146 + 10.0 * 8.321514129638672
Epoch 1250, val loss: 0.40688830614089966
Epoch 1260, training loss: 83.48152923583984 = 0.3208215832710266 + 10.0 * 8.316070556640625
Epoch 1260, val loss: 0.40639087557792664
Epoch 1270, training loss: 83.4752426147461 = 0.31948161125183105 + 10.0 * 8.315576553344727
Epoch 1270, val loss: 0.40592119097709656
Epoch 1280, training loss: 83.54629516601562 = 0.3181576430797577 + 10.0 * 8.322813034057617
Epoch 1280, val loss: 0.40550345182418823
Epoch 1290, training loss: 83.48544311523438 = 0.31680828332901 + 10.0 * 8.316863059997559
Epoch 1290, val loss: 0.4048647880554199
Epoch 1300, training loss: 83.46422576904297 = 0.3154993951320648 + 10.0 * 8.314872741699219
Epoch 1300, val loss: 0.4044140577316284
Epoch 1310, training loss: 83.4542007446289 = 0.31421253085136414 + 10.0 * 8.31399917602539
Epoch 1310, val loss: 0.4038696587085724
Epoch 1320, training loss: 83.44847106933594 = 0.3129364252090454 + 10.0 * 8.313553810119629
Epoch 1320, val loss: 0.4034155309200287
Epoch 1330, training loss: 83.552001953125 = 0.3116622865200043 + 10.0 * 8.324033737182617
Epoch 1330, val loss: 0.402751624584198
Epoch 1340, training loss: 83.48091888427734 = 0.31036049127578735 + 10.0 * 8.317055702209473
Epoch 1340, val loss: 0.4026941955089569
Epoch 1350, training loss: 83.44017791748047 = 0.30908820033073425 + 10.0 * 8.313108444213867
Epoch 1350, val loss: 0.40216386318206787
Epoch 1360, training loss: 83.42745971679688 = 0.3078451156616211 + 10.0 * 8.31196117401123
Epoch 1360, val loss: 0.4016646444797516
Epoch 1370, training loss: 83.42423248291016 = 0.3066151440143585 + 10.0 * 8.311761856079102
Epoch 1370, val loss: 0.4013531506061554
Epoch 1380, training loss: 83.52571868896484 = 0.30539312958717346 + 10.0 * 8.322032928466797
Epoch 1380, val loss: 0.40111684799194336
Epoch 1390, training loss: 83.46686553955078 = 0.30413171648979187 + 10.0 * 8.31627368927002
Epoch 1390, val loss: 0.4004659652709961
Epoch 1400, training loss: 83.41766357421875 = 0.3028946816921234 + 10.0 * 8.311476707458496
Epoch 1400, val loss: 0.4001419246196747
Epoch 1410, training loss: 83.41326141357422 = 0.30168935656547546 + 10.0 * 8.3111572265625
Epoch 1410, val loss: 0.39973723888397217
Epoch 1420, training loss: 83.42815399169922 = 0.30049264430999756 + 10.0 * 8.312766075134277
Epoch 1420, val loss: 0.39937707781791687
Epoch 1430, training loss: 83.41281127929688 = 0.2992860674858093 + 10.0 * 8.311352729797363
Epoch 1430, val loss: 0.399038165807724
Epoch 1440, training loss: 83.41350555419922 = 0.2980908453464508 + 10.0 * 8.311541557312012
Epoch 1440, val loss: 0.3986809551715851
Epoch 1450, training loss: 83.46028900146484 = 0.2969009578227997 + 10.0 * 8.316339492797852
Epoch 1450, val loss: 0.3983300030231476
Epoch 1460, training loss: 83.40379333496094 = 0.2957170009613037 + 10.0 * 8.310808181762695
Epoch 1460, val loss: 0.3981882333755493
Epoch 1470, training loss: 83.38951110839844 = 0.2945333421230316 + 10.0 * 8.309497833251953
Epoch 1470, val loss: 0.3977952301502228
Epoch 1480, training loss: 83.3826675415039 = 0.2933714687824249 + 10.0 * 8.308929443359375
Epoch 1480, val loss: 0.39751291275024414
Epoch 1490, training loss: 83.40184020996094 = 0.29221290349960327 + 10.0 * 8.310962677001953
Epoch 1490, val loss: 0.397306889295578
Epoch 1500, training loss: 83.38832092285156 = 0.2910424470901489 + 10.0 * 8.309727668762207
Epoch 1500, val loss: 0.3970353305339813
Epoch 1510, training loss: 83.3803482055664 = 0.2898733913898468 + 10.0 * 8.30904769897461
Epoch 1510, val loss: 0.3967072665691376
Epoch 1520, training loss: 83.37141418457031 = 0.28871747851371765 + 10.0 * 8.308269500732422
Epoch 1520, val loss: 0.3963894546031952
Epoch 1530, training loss: 83.38945770263672 = 0.2875645160675049 + 10.0 * 8.310189247131348
Epoch 1530, val loss: 0.3962048590183258
Epoch 1540, training loss: 83.37187194824219 = 0.2864075303077698 + 10.0 * 8.30854606628418
Epoch 1540, val loss: 0.3958684802055359
Epoch 1550, training loss: 83.38738250732422 = 0.285255491733551 + 10.0 * 8.310213088989258
Epoch 1550, val loss: 0.39570382237434387
Epoch 1560, training loss: 83.35475158691406 = 0.28411346673965454 + 10.0 * 8.307064056396484
Epoch 1560, val loss: 0.39541223645210266
Epoch 1570, training loss: 83.35042572021484 = 0.2829797863960266 + 10.0 * 8.306744575500488
Epoch 1570, val loss: 0.3952298164367676
Epoch 1580, training loss: 83.35558319091797 = 0.28185370564460754 + 10.0 * 8.307373046875
Epoch 1580, val loss: 0.3949843645095825
Epoch 1590, training loss: 83.37873840332031 = 0.28071874380111694 + 10.0 * 8.309802055358887
Epoch 1590, val loss: 0.39470240473747253
Epoch 1600, training loss: 83.36369323730469 = 0.2795732915401459 + 10.0 * 8.308412551879883
Epoch 1600, val loss: 0.39447468519210815
Epoch 1610, training loss: 83.3890151977539 = 0.2784413695335388 + 10.0 * 8.311057090759277
Epoch 1610, val loss: 0.39413878321647644
Epoch 1620, training loss: 83.34153747558594 = 0.2773057818412781 + 10.0 * 8.30642318725586
Epoch 1620, val loss: 0.3941938579082489
Epoch 1630, training loss: 83.3357925415039 = 0.27618277072906494 + 10.0 * 8.305960655212402
Epoch 1630, val loss: 0.39399322867393494
Epoch 1640, training loss: 83.32220458984375 = 0.2750667333602905 + 10.0 * 8.304713249206543
Epoch 1640, val loss: 0.3937552273273468
Epoch 1650, training loss: 83.32781219482422 = 0.2739526629447937 + 10.0 * 8.30538558959961
Epoch 1650, val loss: 0.39349544048309326
Epoch 1660, training loss: 83.38838195800781 = 0.2728259861469269 + 10.0 * 8.311555862426758
Epoch 1660, val loss: 0.3933078944683075
Epoch 1670, training loss: 83.34223175048828 = 0.27170634269714355 + 10.0 * 8.307052612304688
Epoch 1670, val loss: 0.39339324831962585
Epoch 1680, training loss: 83.32764434814453 = 0.27057331800460815 + 10.0 * 8.305706977844238
Epoch 1680, val loss: 0.3931625485420227
Epoch 1690, training loss: 83.34673309326172 = 0.2694597542285919 + 10.0 * 8.307726860046387
Epoch 1690, val loss: 0.39321163296699524
Epoch 1700, training loss: 83.30619049072266 = 0.26832738518714905 + 10.0 * 8.303786277770996
Epoch 1700, val loss: 0.3929065465927124
Epoch 1710, training loss: 83.3070068359375 = 0.2672107517719269 + 10.0 * 8.303979873657227
Epoch 1710, val loss: 0.39270755648612976
Epoch 1720, training loss: 83.29573059082031 = 0.2661029100418091 + 10.0 * 8.302962303161621
Epoch 1720, val loss: 0.39258527755737305
Epoch 1730, training loss: 83.3631362915039 = 0.2649937570095062 + 10.0 * 8.309814453125
Epoch 1730, val loss: 0.39254632592201233
Epoch 1740, training loss: 83.32699584960938 = 0.2638566493988037 + 10.0 * 8.306314468383789
Epoch 1740, val loss: 0.3922874331474304
Epoch 1750, training loss: 83.3061752319336 = 0.2627374827861786 + 10.0 * 8.304343223571777
Epoch 1750, val loss: 0.39245158433914185
Epoch 1760, training loss: 83.28690338134766 = 0.26162371039390564 + 10.0 * 8.30252742767334
Epoch 1760, val loss: 0.3921999931335449
Epoch 1770, training loss: 83.28106689453125 = 0.2605103552341461 + 10.0 * 8.302055358886719
Epoch 1770, val loss: 0.3920852541923523
Epoch 1780, training loss: 83.30561828613281 = 0.25939008593559265 + 10.0 * 8.304622650146484
Epoch 1780, val loss: 0.3919581472873688
Epoch 1790, training loss: 83.29020690917969 = 0.25825637578964233 + 10.0 * 8.303194999694824
Epoch 1790, val loss: 0.3920325040817261
Epoch 1800, training loss: 83.3016128540039 = 0.25711753964424133 + 10.0 * 8.304449081420898
Epoch 1800, val loss: 0.39189133048057556
Epoch 1810, training loss: 83.29167938232422 = 0.2559888958930969 + 10.0 * 8.303568840026855
Epoch 1810, val loss: 0.3916899263858795
Epoch 1820, training loss: 83.27909088134766 = 0.2548605799674988 + 10.0 * 8.302423477172852
Epoch 1820, val loss: 0.39171409606933594
Epoch 1830, training loss: 83.26132202148438 = 0.2537367343902588 + 10.0 * 8.300758361816406
Epoch 1830, val loss: 0.3917754888534546
Epoch 1840, training loss: 83.26111602783203 = 0.25261160731315613 + 10.0 * 8.300850868225098
Epoch 1840, val loss: 0.391695111989975
Epoch 1850, training loss: 83.29825592041016 = 0.2514880895614624 + 10.0 * 8.30467700958252
Epoch 1850, val loss: 0.3917836546897888
Epoch 1860, training loss: 83.2670669555664 = 0.25034254789352417 + 10.0 * 8.301671981811523
Epoch 1860, val loss: 0.39174333214759827
Epoch 1870, training loss: 83.25940704345703 = 0.2491982877254486 + 10.0 * 8.301020622253418
Epoch 1870, val loss: 0.39168575406074524
Epoch 1880, training loss: 83.26824951171875 = 0.24806642532348633 + 10.0 * 8.302018165588379
Epoch 1880, val loss: 0.39167359471321106
Epoch 1890, training loss: 83.2785873413086 = 0.246925488114357 + 10.0 * 8.303166389465332
Epoch 1890, val loss: 0.3916579782962799
Epoch 1900, training loss: 83.25995635986328 = 0.24577516317367554 + 10.0 * 8.30141830444336
Epoch 1900, val loss: 0.39145800471305847
Epoch 1910, training loss: 83.24717712402344 = 0.244640052318573 + 10.0 * 8.300253868103027
Epoch 1910, val loss: 0.3915613293647766
Epoch 1920, training loss: 83.24317932128906 = 0.24350295960903168 + 10.0 * 8.299967765808105
Epoch 1920, val loss: 0.3915890157222748
Epoch 1930, training loss: 83.29015350341797 = 0.24238158762454987 + 10.0 * 8.304777145385742
Epoch 1930, val loss: 0.39187315106391907
Epoch 1940, training loss: 83.26898193359375 = 0.2412254363298416 + 10.0 * 8.302775382995605
Epoch 1940, val loss: 0.39161452651023865
Epoch 1950, training loss: 83.23643493652344 = 0.24008387327194214 + 10.0 * 8.29963493347168
Epoch 1950, val loss: 0.39167800545692444
Epoch 1960, training loss: 83.22721862792969 = 0.238948792219162 + 10.0 * 8.298827171325684
Epoch 1960, val loss: 0.39164116978645325
Epoch 1970, training loss: 83.22418975830078 = 0.23781873285770416 + 10.0 * 8.298637390136719
Epoch 1970, val loss: 0.3917136788368225
Epoch 1980, training loss: 83.2577133178711 = 0.23668760061264038 + 10.0 * 8.302103042602539
Epoch 1980, val loss: 0.3917429447174072
Epoch 1990, training loss: 83.24407958984375 = 0.235544353723526 + 10.0 * 8.300853729248047
Epoch 1990, val loss: 0.39188364148139954
Epoch 2000, training loss: 83.24234008789062 = 0.23440837860107422 + 10.0 * 8.300793647766113
Epoch 2000, val loss: 0.3921807110309601
Epoch 2010, training loss: 83.21420288085938 = 0.23325566947460175 + 10.0 * 8.298094749450684
Epoch 2010, val loss: 0.3919885754585266
Epoch 2020, training loss: 83.2088394165039 = 0.23212040960788727 + 10.0 * 8.297672271728516
Epoch 2020, val loss: 0.39206984639167786
Epoch 2030, training loss: 83.20657348632812 = 0.23098625242710114 + 10.0 * 8.297558784484863
Epoch 2030, val loss: 0.39209988713264465
Epoch 2040, training loss: 83.22760009765625 = 0.22984850406646729 + 10.0 * 8.299775123596191
Epoch 2040, val loss: 0.3921206295490265
Epoch 2050, training loss: 83.21027374267578 = 0.22870276868343353 + 10.0 * 8.29815673828125
Epoch 2050, val loss: 0.39239123463630676
Epoch 2060, training loss: 83.2442626953125 = 0.22756008803844452 + 10.0 * 8.30167007446289
Epoch 2060, val loss: 0.39262136816978455
Epoch 2070, training loss: 83.22555541992188 = 0.22640922665596008 + 10.0 * 8.299914360046387
Epoch 2070, val loss: 0.3927726149559021
Epoch 2080, training loss: 83.19538116455078 = 0.22525963187217712 + 10.0 * 8.297012329101562
Epoch 2080, val loss: 0.39296653866767883
Epoch 2090, training loss: 83.19931030273438 = 0.22411003708839417 + 10.0 * 8.29751968383789
Epoch 2090, val loss: 0.39309513568878174
Epoch 2100, training loss: 83.24544525146484 = 0.2229747772216797 + 10.0 * 8.302247047424316
Epoch 2100, val loss: 0.3934844732284546
Epoch 2110, training loss: 83.19075012207031 = 0.22181189060211182 + 10.0 * 8.296894073486328
Epoch 2110, val loss: 0.3934706151485443
Epoch 2120, training loss: 83.18611907958984 = 0.22066596150398254 + 10.0 * 8.296545028686523
Epoch 2120, val loss: 0.3937323987483978
Epoch 2130, training loss: 83.19400024414062 = 0.2195272147655487 + 10.0 * 8.297447204589844
Epoch 2130, val loss: 0.39396342635154724
Epoch 2140, training loss: 83.2043685913086 = 0.21838568150997162 + 10.0 * 8.298598289489746
Epoch 2140, val loss: 0.39417698979377747
Epoch 2150, training loss: 83.19773864746094 = 0.2172432392835617 + 10.0 * 8.298048973083496
Epoch 2150, val loss: 0.39438754320144653
Epoch 2160, training loss: 83.173828125 = 0.21610267460346222 + 10.0 * 8.295772552490234
Epoch 2160, val loss: 0.3947170376777649
Epoch 2170, training loss: 83.17808532714844 = 0.21496260166168213 + 10.0 * 8.29631233215332
Epoch 2170, val loss: 0.39489319920539856
Epoch 2180, training loss: 83.24005889892578 = 0.21382901072502136 + 10.0 * 8.30262279510498
Epoch 2180, val loss: 0.3948417603969574
Epoch 2190, training loss: 83.1939697265625 = 0.2126789093017578 + 10.0 * 8.298129081726074
Epoch 2190, val loss: 0.3954642713069916
Epoch 2200, training loss: 83.16773223876953 = 0.2115299105644226 + 10.0 * 8.29561996459961
Epoch 2200, val loss: 0.39560049772262573
Epoch 2210, training loss: 83.15779113769531 = 0.21038861572742462 + 10.0 * 8.294740676879883
Epoch 2210, val loss: 0.39612194895744324
Epoch 2220, training loss: 83.15632629394531 = 0.2092452496290207 + 10.0 * 8.294708251953125
Epoch 2220, val loss: 0.3963315784931183
Epoch 2230, training loss: 83.21287536621094 = 0.20810671150684357 + 10.0 * 8.300477027893066
Epoch 2230, val loss: 0.39669451117515564
Epoch 2240, training loss: 83.17984771728516 = 0.2069471925497055 + 10.0 * 8.297289848327637
Epoch 2240, val loss: 0.39690637588500977
Epoch 2250, training loss: 83.16961669921875 = 0.20578940212726593 + 10.0 * 8.296382904052734
Epoch 2250, val loss: 0.39725741744041443
Epoch 2260, training loss: 83.15410614013672 = 0.20464441180229187 + 10.0 * 8.294946670532227
Epoch 2260, val loss: 0.3977137804031372
Epoch 2270, training loss: 83.15339660644531 = 0.20349541306495667 + 10.0 * 8.294989585876465
Epoch 2270, val loss: 0.39801666140556335
Epoch 2280, training loss: 83.16393280029297 = 0.20235076546669006 + 10.0 * 8.296157836914062
Epoch 2280, val loss: 0.3983771800994873
Epoch 2290, training loss: 83.14698791503906 = 0.20120801031589508 + 10.0 * 8.294577598571777
Epoch 2290, val loss: 0.39887189865112305
Epoch 2300, training loss: 83.21475219726562 = 0.20008757710456848 + 10.0 * 8.30146598815918
Epoch 2300, val loss: 0.39951327443122864
Epoch 2310, training loss: 83.1556625366211 = 0.19891910254955292 + 10.0 * 8.295674324035645
Epoch 2310, val loss: 0.3994767963886261
Epoch 2320, training loss: 83.13220977783203 = 0.1977759152650833 + 10.0 * 8.29344367980957
Epoch 2320, val loss: 0.40000346302986145
Epoch 2330, training loss: 83.12579345703125 = 0.19663970172405243 + 10.0 * 8.292915344238281
Epoch 2330, val loss: 0.400364488363266
Epoch 2340, training loss: 83.12361145019531 = 0.1955072432756424 + 10.0 * 8.292810440063477
Epoch 2340, val loss: 0.40085265040397644
Epoch 2350, training loss: 83.13910675048828 = 0.1943759173154831 + 10.0 * 8.294473648071289
Epoch 2350, val loss: 0.4012945592403412
Epoch 2360, training loss: 83.16071319580078 = 0.19324137270450592 + 10.0 * 8.296747207641602
Epoch 2360, val loss: 0.40174126625061035
Epoch 2370, training loss: 83.1395492553711 = 0.19209644198417664 + 10.0 * 8.294745445251465
Epoch 2370, val loss: 0.40210095047950745
Epoch 2380, training loss: 83.13232421875 = 0.1909610480070114 + 10.0 * 8.294136047363281
Epoch 2380, val loss: 0.40251055359840393
Epoch 2390, training loss: 83.14019775390625 = 0.18982556462287903 + 10.0 * 8.295037269592285
Epoch 2390, val loss: 0.4028470516204834
Epoch 2400, training loss: 83.13241577148438 = 0.18869468569755554 + 10.0 * 8.294371604919434
Epoch 2400, val loss: 0.40345266461372375
Epoch 2410, training loss: 83.13623046875 = 0.1875634342432022 + 10.0 * 8.294866561889648
Epoch 2410, val loss: 0.4040645360946655
Epoch 2420, training loss: 83.11597442626953 = 0.18643535673618317 + 10.0 * 8.292954444885254
Epoch 2420, val loss: 0.4045797884464264
Epoch 2430, training loss: 83.10919952392578 = 0.1853111982345581 + 10.0 * 8.292388916015625
Epoch 2430, val loss: 0.4050126373767853
Epoch 2440, training loss: 83.11406707763672 = 0.1841939091682434 + 10.0 * 8.292986869812012
Epoch 2440, val loss: 0.40552419424057007
Epoch 2450, training loss: 83.12197875976562 = 0.18307825922966003 + 10.0 * 8.293889999389648
Epoch 2450, val loss: 0.40613269805908203
Epoch 2460, training loss: 83.1211166381836 = 0.18195953965187073 + 10.0 * 8.293915748596191
Epoch 2460, val loss: 0.4069480001926422
Epoch 2470, training loss: 83.1386947631836 = 0.18083883821964264 + 10.0 * 8.295785903930664
Epoch 2470, val loss: 0.40720659494400024
Epoch 2480, training loss: 83.09728240966797 = 0.17971663177013397 + 10.0 * 8.291756629943848
Epoch 2480, val loss: 0.4080120623111725
Epoch 2490, training loss: 83.12457275390625 = 0.17860649526119232 + 10.0 * 8.294596672058105
Epoch 2490, val loss: 0.4086284935474396
Epoch 2500, training loss: 83.09160614013672 = 0.17748533189296722 + 10.0 * 8.291412353515625
Epoch 2500, val loss: 0.40931805968284607
Epoch 2510, training loss: 83.08737182617188 = 0.17637339234352112 + 10.0 * 8.291099548339844
Epoch 2510, val loss: 0.40998712182044983
Epoch 2520, training loss: 83.08243560791016 = 0.1752646118402481 + 10.0 * 8.290717124938965
Epoch 2520, val loss: 0.4107549786567688
Epoch 2530, training loss: 83.0892105102539 = 0.17415368556976318 + 10.0 * 8.291505813598633
Epoch 2530, val loss: 0.41147226095199585
Epoch 2540, training loss: 83.12521362304688 = 0.17304816842079163 + 10.0 * 8.29521656036377
Epoch 2540, val loss: 0.4122565686702728
Epoch 2550, training loss: 83.10446166992188 = 0.17193616926670074 + 10.0 * 8.293252944946289
Epoch 2550, val loss: 0.412900447845459
Epoch 2560, training loss: 83.08513641357422 = 0.17082221806049347 + 10.0 * 8.291431427001953
Epoch 2560, val loss: 0.41351619362831116
Epoch 2570, training loss: 83.09489440917969 = 0.16971153020858765 + 10.0 * 8.292518615722656
Epoch 2570, val loss: 0.4142255187034607
Epoch 2580, training loss: 83.10102081298828 = 0.16860461235046387 + 10.0 * 8.293241500854492
Epoch 2580, val loss: 0.41481080651283264
Epoch 2590, training loss: 83.07852172851562 = 0.16749878227710724 + 10.0 * 8.291102409362793
Epoch 2590, val loss: 0.41565072536468506
Epoch 2600, training loss: 83.06229400634766 = 0.16639123857021332 + 10.0 * 8.289590835571289
Epoch 2600, val loss: 0.4163758456707001
Epoch 2610, training loss: 83.0639877319336 = 0.1652894765138626 + 10.0 * 8.28986930847168
Epoch 2610, val loss: 0.4171184301376343
Epoch 2620, training loss: 83.0928955078125 = 0.16419707238674164 + 10.0 * 8.292869567871094
Epoch 2620, val loss: 0.41775768995285034
Epoch 2630, training loss: 83.08637237548828 = 0.16309085488319397 + 10.0 * 8.292327880859375
Epoch 2630, val loss: 0.4187314808368683
Epoch 2640, training loss: 83.0588607788086 = 0.1619798243045807 + 10.0 * 8.289688110351562
Epoch 2640, val loss: 0.41940534114837646
Epoch 2650, training loss: 83.05170440673828 = 0.16087943315505981 + 10.0 * 8.289082527160645
Epoch 2650, val loss: 0.4202953279018402
Epoch 2660, training loss: 83.05338287353516 = 0.15978340804576874 + 10.0 * 8.289360046386719
Epoch 2660, val loss: 0.42109015583992004
Epoch 2670, training loss: 83.11299133300781 = 0.15869897603988647 + 10.0 * 8.295429229736328
Epoch 2670, val loss: 0.42178982496261597
Epoch 2680, training loss: 83.05593872070312 = 0.15760383009910583 + 10.0 * 8.289834022521973
Epoch 2680, val loss: 0.42255011200904846
Epoch 2690, training loss: 83.0629653930664 = 0.1565147489309311 + 10.0 * 8.290644645690918
Epoch 2690, val loss: 0.4232994616031647
Epoch 2700, training loss: 83.07587432861328 = 0.15542925894260406 + 10.0 * 8.292044639587402
Epoch 2700, val loss: 0.4241972267627716
Epoch 2710, training loss: 83.05081939697266 = 0.1543387919664383 + 10.0 * 8.289648056030273
Epoch 2710, val loss: 0.4253886342048645
Epoch 2720, training loss: 83.0476303100586 = 0.15325075387954712 + 10.0 * 8.289438247680664
Epoch 2720, val loss: 0.426102876663208
Epoch 2730, training loss: 83.05301666259766 = 0.15216562151908875 + 10.0 * 8.290084838867188
Epoch 2730, val loss: 0.4267232418060303
Epoch 2740, training loss: 83.03423309326172 = 0.15107373893260956 + 10.0 * 8.288315773010254
Epoch 2740, val loss: 0.42774927616119385
Epoch 2750, training loss: 83.04356384277344 = 0.14999143779277802 + 10.0 * 8.28935718536377
Epoch 2750, val loss: 0.42874613404273987
Epoch 2760, training loss: 83.06306457519531 = 0.14890652894973755 + 10.0 * 8.29141616821289
Epoch 2760, val loss: 0.4295375347137451
Epoch 2770, training loss: 83.0437240600586 = 0.147825688123703 + 10.0 * 8.289589881896973
Epoch 2770, val loss: 0.43027231097221375
Epoch 2780, training loss: 83.0673599243164 = 0.1467498242855072 + 10.0 * 8.292060852050781
Epoch 2780, val loss: 0.43157610297203064
Epoch 2790, training loss: 83.02918243408203 = 0.14566895365715027 + 10.0 * 8.288351058959961
Epoch 2790, val loss: 0.4324209690093994
Epoch 2800, training loss: 83.03129577636719 = 0.14460062980651855 + 10.0 * 8.28866958618164
Epoch 2800, val loss: 0.43350884318351746
Epoch 2810, training loss: 83.03931427001953 = 0.1435394287109375 + 10.0 * 8.28957748413086
Epoch 2810, val loss: 0.43463435769081116
Epoch 2820, training loss: 83.0243911743164 = 0.14247028529644012 + 10.0 * 8.288191795349121
Epoch 2820, val loss: 0.4354619085788727
Epoch 2830, training loss: 83.03255462646484 = 0.14140862226486206 + 10.0 * 8.289113998413086
Epoch 2830, val loss: 0.4364836513996124
Epoch 2840, training loss: 83.02740478515625 = 0.14034992456436157 + 10.0 * 8.288705825805664
Epoch 2840, val loss: 0.43769773840904236
Epoch 2850, training loss: 83.0372314453125 = 0.13930003345012665 + 10.0 * 8.289793014526367
Epoch 2850, val loss: 0.43879270553588867
Epoch 2860, training loss: 83.03846740722656 = 0.13823218643665314 + 10.0 * 8.290023803710938
Epoch 2860, val loss: 0.43954145908355713
Epoch 2870, training loss: 83.01115417480469 = 0.13717065751552582 + 10.0 * 8.287398338317871
Epoch 2870, val loss: 0.4404800534248352
Epoch 2880, training loss: 83.00301361083984 = 0.13611528277397156 + 10.0 * 8.286689758300781
Epoch 2880, val loss: 0.4416196942329407
Epoch 2890, training loss: 83.02242279052734 = 0.13507123291492462 + 10.0 * 8.288735389709473
Epoch 2890, val loss: 0.4427505433559418
Epoch 2900, training loss: 83.03666687011719 = 0.13402526080608368 + 10.0 * 8.290264129638672
Epoch 2900, val loss: 0.44375094771385193
Epoch 2910, training loss: 83.02901458740234 = 0.13297417759895325 + 10.0 * 8.289604187011719
Epoch 2910, val loss: 0.4445829391479492
Epoch 2920, training loss: 83.02128601074219 = 0.13191922008991241 + 10.0 * 8.288936614990234
Epoch 2920, val loss: 0.4459056854248047
Epoch 2930, training loss: 83.00028228759766 = 0.1308732032775879 + 10.0 * 8.286940574645996
Epoch 2930, val loss: 0.44724270701408386
Epoch 2940, training loss: 82.99748229980469 = 0.12983383238315582 + 10.0 * 8.286764144897461
Epoch 2940, val loss: 0.44845131039619446
Epoch 2950, training loss: 83.02243041992188 = 0.12879982590675354 + 10.0 * 8.289362907409668
Epoch 2950, val loss: 0.4495881497859955
Epoch 2960, training loss: 83.00676727294922 = 0.12775321304798126 + 10.0 * 8.287900924682617
Epoch 2960, val loss: 0.45047879219055176
Epoch 2970, training loss: 83.00718688964844 = 0.12672096490859985 + 10.0 * 8.288045883178711
Epoch 2970, val loss: 0.4519403278827667
Epoch 2980, training loss: 82.98348999023438 = 0.1256873905658722 + 10.0 * 8.28577995300293
Epoch 2980, val loss: 0.4530028700828552
Epoch 2990, training loss: 82.98148345947266 = 0.12466667592525482 + 10.0 * 8.28568172454834
Epoch 2990, val loss: 0.45414361357688904
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8422120750887874
0.865391581540245
=== training gcn model ===
Epoch 0, training loss: 106.92440032958984 = 1.1021666526794434 + 10.0 * 10.582223892211914
Epoch 0, val loss: 1.101442813873291
Epoch 10, training loss: 106.90383911132812 = 1.0954768657684326 + 10.0 * 10.580836296081543
Epoch 10, val loss: 1.0947871208190918
Epoch 20, training loss: 106.76634216308594 = 1.0881479978561401 + 10.0 * 10.567819595336914
Epoch 20, val loss: 1.087419033050537
Epoch 30, training loss: 105.8828125 = 1.079586148262024 + 10.0 * 10.480321884155273
Epoch 30, val loss: 1.0787562131881714
Epoch 40, training loss: 102.8361587524414 = 1.0698421001434326 + 10.0 * 10.176631927490234
Epoch 40, val loss: 1.0689607858657837
Epoch 50, training loss: 97.05238342285156 = 1.059685230255127 + 10.0 * 9.59926986694336
Epoch 50, val loss: 1.0588769912719727
Epoch 60, training loss: 94.78882598876953 = 1.051217794418335 + 10.0 * 9.373761177062988
Epoch 60, val loss: 1.0505356788635254
Epoch 70, training loss: 93.21345520019531 = 1.0434318780899048 + 10.0 * 9.217001914978027
Epoch 70, val loss: 1.043033480644226
Epoch 80, training loss: 91.99809265136719 = 1.0364830493927002 + 10.0 * 9.096160888671875
Epoch 80, val loss: 1.036373496055603
Epoch 90, training loss: 91.00385284423828 = 1.0303536653518677 + 10.0 * 8.997349739074707
Epoch 90, val loss: 1.0304609537124634
Epoch 100, training loss: 89.9007568359375 = 1.024720311164856 + 10.0 * 8.887603759765625
Epoch 100, val loss: 1.025059700012207
Epoch 110, training loss: 89.2105484008789 = 1.0195610523223877 + 10.0 * 8.819098472595215
Epoch 110, val loss: 1.0201479196548462
Epoch 120, training loss: 88.66496276855469 = 1.0146466493606567 + 10.0 * 8.765031814575195
Epoch 120, val loss: 1.0154145956039429
Epoch 130, training loss: 88.28173828125 = 1.0093647241592407 + 10.0 * 8.727237701416016
Epoch 130, val loss: 1.0102589130401611
Epoch 140, training loss: 88.01813507080078 = 1.0030626058578491 + 10.0 * 8.701507568359375
Epoch 140, val loss: 1.0040866136550903
Epoch 150, training loss: 87.77066802978516 = 0.9955642223358154 + 10.0 * 8.677510261535645
Epoch 150, val loss: 0.9968828558921814
Epoch 160, training loss: 87.48726654052734 = 0.9878278970718384 + 10.0 * 8.649943351745605
Epoch 160, val loss: 0.9896707534790039
Epoch 170, training loss: 87.19924926757812 = 0.9803667664527893 + 10.0 * 8.621888160705566
Epoch 170, val loss: 0.9826593995094299
Epoch 180, training loss: 87.0091552734375 = 0.9723246693611145 + 10.0 * 8.603682518005371
Epoch 180, val loss: 0.9750540256500244
Epoch 190, training loss: 86.74378204345703 = 0.9631445407867432 + 10.0 * 8.57806396484375
Epoch 190, val loss: 0.9662814140319824
Epoch 200, training loss: 86.50037384033203 = 0.9533904790878296 + 10.0 * 8.55469799041748
Epoch 200, val loss: 0.9570959210395813
Epoch 210, training loss: 86.33325958251953 = 0.9428563117980957 + 10.0 * 8.539040565490723
Epoch 210, val loss: 0.9472599029541016
Epoch 220, training loss: 86.14479064941406 = 0.9313883781433105 + 10.0 * 8.521340370178223
Epoch 220, val loss: 0.9364326596260071
Epoch 230, training loss: 85.97921752929688 = 0.9192797541618347 + 10.0 * 8.505993843078613
Epoch 230, val loss: 0.9250932931900024
Epoch 240, training loss: 85.8488540649414 = 0.9063872694969177 + 10.0 * 8.494246482849121
Epoch 240, val loss: 0.9130274653434753
Epoch 250, training loss: 85.72012329101562 = 0.8925331234931946 + 10.0 * 8.482759475708008
Epoch 250, val loss: 0.9001994729042053
Epoch 260, training loss: 85.60345458984375 = 0.8783143162727356 + 10.0 * 8.472514152526855
Epoch 260, val loss: 0.8870121836662292
Epoch 270, training loss: 85.49110412597656 = 0.863589346408844 + 10.0 * 8.462751388549805
Epoch 270, val loss: 0.8733752369880676
Epoch 280, training loss: 85.39049530029297 = 0.8482632040977478 + 10.0 * 8.454222679138184
Epoch 280, val loss: 0.8592053055763245
Epoch 290, training loss: 85.4315185546875 = 0.8323771357536316 + 10.0 * 8.459914207458496
Epoch 290, val loss: 0.8446873426437378
Epoch 300, training loss: 85.25055694580078 = 0.8159813284873962 + 10.0 * 8.44345760345459
Epoch 300, val loss: 0.829403281211853
Epoch 310, training loss: 85.16413879394531 = 0.7995641827583313 + 10.0 * 8.436457633972168
Epoch 310, val loss: 0.8142216205596924
Epoch 320, training loss: 85.08431243896484 = 0.783039391040802 + 10.0 * 8.430127143859863
Epoch 320, val loss: 0.7989957332611084
Epoch 330, training loss: 85.02207946777344 = 0.7663589715957642 + 10.0 * 8.425572395324707
Epoch 330, val loss: 0.7835832238197327
Epoch 340, training loss: 84.97488403320312 = 0.7496365308761597 + 10.0 * 8.422525405883789
Epoch 340, val loss: 0.7680943012237549
Epoch 350, training loss: 84.93277740478516 = 0.7327866554260254 + 10.0 * 8.419999122619629
Epoch 350, val loss: 0.7526363730430603
Epoch 360, training loss: 84.86231231689453 = 0.7162279486656189 + 10.0 * 8.4146089553833
Epoch 360, val loss: 0.7372639775276184
Epoch 370, training loss: 84.80644226074219 = 0.6999911665916443 + 10.0 * 8.410645484924316
Epoch 370, val loss: 0.7222722172737122
Epoch 380, training loss: 84.75971221923828 = 0.6840542554855347 + 10.0 * 8.40756607055664
Epoch 380, val loss: 0.7075657844543457
Epoch 390, training loss: 84.72256469726562 = 0.6683896780014038 + 10.0 * 8.405417442321777
Epoch 390, val loss: 0.6930386424064636
Epoch 400, training loss: 84.66999816894531 = 0.6532396674156189 + 10.0 * 8.401676177978516
Epoch 400, val loss: 0.679029107093811
Epoch 410, training loss: 84.6208267211914 = 0.6387827396392822 + 10.0 * 8.39820384979248
Epoch 410, val loss: 0.6656588912010193
Epoch 420, training loss: 84.58240509033203 = 0.6249291300773621 + 10.0 * 8.395747184753418
Epoch 420, val loss: 0.652860701084137
Epoch 430, training loss: 84.57232666015625 = 0.6115880608558655 + 10.0 * 8.396074295043945
Epoch 430, val loss: 0.6404988765716553
Epoch 440, training loss: 84.4988784790039 = 0.5988446474075317 + 10.0 * 8.390003204345703
Epoch 440, val loss: 0.6289049983024597
Epoch 450, training loss: 84.46031951904297 = 0.5870013236999512 + 10.0 * 8.38733196258545
Epoch 450, val loss: 0.6180731654167175
Epoch 460, training loss: 84.42340850830078 = 0.5758399367332458 + 10.0 * 8.384757041931152
Epoch 460, val loss: 0.6078698039054871
Epoch 470, training loss: 84.3903579711914 = 0.5653287768363953 + 10.0 * 8.382502555847168
Epoch 470, val loss: 0.5983504056930542
Epoch 480, training loss: 84.45347595214844 = 0.5553440451622009 + 10.0 * 8.389813423156738
Epoch 480, val loss: 0.5894058346748352
Epoch 490, training loss: 84.36324310302734 = 0.5459533333778381 + 10.0 * 8.381729125976562
Epoch 490, val loss: 0.5808222889900208
Epoch 500, training loss: 84.3102035522461 = 0.5373126864433289 + 10.0 * 8.377288818359375
Epoch 500, val loss: 0.573072075843811
Epoch 510, training loss: 84.27462768554688 = 0.5292508602142334 + 10.0 * 8.374537467956543
Epoch 510, val loss: 0.5658136606216431
Epoch 520, training loss: 84.24633026123047 = 0.5216718316078186 + 10.0 * 8.372465133666992
Epoch 520, val loss: 0.5590841174125671
Epoch 530, training loss: 84.29872131347656 = 0.5145225524902344 + 10.0 * 8.378419876098633
Epoch 530, val loss: 0.5526166558265686
Epoch 540, training loss: 84.22315979003906 = 0.5076472759246826 + 10.0 * 8.371551513671875
Epoch 540, val loss: 0.5467076301574707
Epoch 550, training loss: 84.18743133544922 = 0.5012859106063843 + 10.0 * 8.368614196777344
Epoch 550, val loss: 0.5411590337753296
Epoch 560, training loss: 84.156982421875 = 0.4953329265117645 + 10.0 * 8.366165161132812
Epoch 560, val loss: 0.5359728932380676
Epoch 570, training loss: 84.12982177734375 = 0.4896858036518097 + 10.0 * 8.364013671875
Epoch 570, val loss: 0.5311642289161682
Epoch 580, training loss: 84.13013458251953 = 0.48432210087776184 + 10.0 * 8.364581108093262
Epoch 580, val loss: 0.5264660120010376
Epoch 590, training loss: 84.13046264648438 = 0.4790385365486145 + 10.0 * 8.365141868591309
Epoch 590, val loss: 0.5222917795181274
Epoch 600, training loss: 84.07618713378906 = 0.474081814289093 + 10.0 * 8.360210418701172
Epoch 600, val loss: 0.5180410742759705
Epoch 610, training loss: 84.0534896850586 = 0.46943360567092896 + 10.0 * 8.358405113220215
Epoch 610, val loss: 0.5140395760536194
Epoch 620, training loss: 84.03697967529297 = 0.4649823009967804 + 10.0 * 8.357199668884277
Epoch 620, val loss: 0.5103887915611267
Epoch 630, training loss: 84.03736877441406 = 0.460701048374176 + 10.0 * 8.357666969299316
Epoch 630, val loss: 0.5068332552909851
Epoch 640, training loss: 84.0118179321289 = 0.4565117061138153 + 10.0 * 8.355530738830566
Epoch 640, val loss: 0.5035504698753357
Epoch 650, training loss: 83.99622344970703 = 0.45250582695007324 + 10.0 * 8.354372024536133
Epoch 650, val loss: 0.500213086605072
Epoch 660, training loss: 83.97172546386719 = 0.44862520694732666 + 10.0 * 8.352310180664062
Epoch 660, val loss: 0.4970766603946686
Epoch 670, training loss: 83.9538345336914 = 0.4448639452457428 + 10.0 * 8.350896835327148
Epoch 670, val loss: 0.4941486120223999
Epoch 680, training loss: 83.99756622314453 = 0.44120389223098755 + 10.0 * 8.355636596679688
Epoch 680, val loss: 0.49126821756362915
Epoch 690, training loss: 83.93383026123047 = 0.43759605288505554 + 10.0 * 8.34962272644043
Epoch 690, val loss: 0.48823195695877075
Epoch 700, training loss: 83.91270446777344 = 0.4341413676738739 + 10.0 * 8.347856521606445
Epoch 700, val loss: 0.48553621768951416
Epoch 710, training loss: 83.89027404785156 = 0.4308198392391205 + 10.0 * 8.345945358276367
Epoch 710, val loss: 0.4828362464904785
Epoch 720, training loss: 83.88368225097656 = 0.4275810420513153 + 10.0 * 8.345609664916992
Epoch 720, val loss: 0.4803162217140198
Epoch 730, training loss: 83.89517211914062 = 0.4243679344654083 + 10.0 * 8.34708023071289
Epoch 730, val loss: 0.47780194878578186
Epoch 740, training loss: 83.8728256225586 = 0.42122748494148254 + 10.0 * 8.345159530639648
Epoch 740, val loss: 0.47526177763938904
Epoch 750, training loss: 83.8406753540039 = 0.4182213246822357 + 10.0 * 8.342245101928711
Epoch 750, val loss: 0.4728933870792389
Epoch 760, training loss: 83.82841491699219 = 0.4152921438217163 + 10.0 * 8.341312408447266
Epoch 760, val loss: 0.47056183218955994
Epoch 770, training loss: 83.85983276367188 = 0.4124211370944977 + 10.0 * 8.344740867614746
Epoch 770, val loss: 0.4681902229785919
Epoch 780, training loss: 83.84262084960938 = 0.40954846143722534 + 10.0 * 8.343307495117188
Epoch 780, val loss: 0.4660583734512329
Epoch 790, training loss: 83.79618835449219 = 0.40676289796829224 + 10.0 * 8.338942527770996
Epoch 790, val loss: 0.46394774317741394
Epoch 800, training loss: 83.78443908691406 = 0.4040653109550476 + 10.0 * 8.338037490844727
Epoch 800, val loss: 0.46173346042633057
Epoch 810, training loss: 83.77392578125 = 0.40141561627388 + 10.0 * 8.337251663208008
Epoch 810, val loss: 0.45966145396232605
Epoch 820, training loss: 83.79888153076172 = 0.39881157875061035 + 10.0 * 8.340006828308105
Epoch 820, val loss: 0.4574882984161377
Epoch 830, training loss: 83.75737762451172 = 0.3962109386920929 + 10.0 * 8.336116790771484
Epoch 830, val loss: 0.4556868076324463
Epoch 840, training loss: 83.7623519897461 = 0.39368313550949097 + 10.0 * 8.33686637878418
Epoch 840, val loss: 0.4536481201648712
Epoch 850, training loss: 83.73161315917969 = 0.39120611548423767 + 10.0 * 8.334040641784668
Epoch 850, val loss: 0.4518454074859619
Epoch 860, training loss: 83.71967315673828 = 0.38879039883613586 + 10.0 * 8.333087921142578
Epoch 860, val loss: 0.4499313235282898
Epoch 870, training loss: 83.7124252319336 = 0.3864225447177887 + 10.0 * 8.332600593566895
Epoch 870, val loss: 0.448163777589798
Epoch 880, training loss: 83.78353881835938 = 0.3840741217136383 + 10.0 * 8.339946746826172
Epoch 880, val loss: 0.44652172923088074
Epoch 890, training loss: 83.72434997558594 = 0.3817306160926819 + 10.0 * 8.334261894226074
Epoch 890, val loss: 0.44451847672462463
Epoch 900, training loss: 83.68505859375 = 0.37945297360420227 + 10.0 * 8.330560684204102
Epoch 900, val loss: 0.44275665283203125
Epoch 910, training loss: 83.68006134033203 = 0.3772352933883667 + 10.0 * 8.330282211303711
Epoch 910, val loss: 0.44109874963760376
Epoch 920, training loss: 83.71546173095703 = 0.37505611777305603 + 10.0 * 8.334040641784668
Epoch 920, val loss: 0.4392778277397156
Epoch 930, training loss: 83.68653869628906 = 0.3728604316711426 + 10.0 * 8.331367492675781
Epoch 930, val loss: 0.4379331171512604
Epoch 940, training loss: 83.66307830810547 = 0.3707277178764343 + 10.0 * 8.329235076904297
Epoch 940, val loss: 0.43614646792411804
Epoch 950, training loss: 83.64418029785156 = 0.36865299940109253 + 10.0 * 8.327552795410156
Epoch 950, val loss: 0.43468496203422546
Epoch 960, training loss: 83.6357650756836 = 0.3666144907474518 + 10.0 * 8.32691478729248
Epoch 960, val loss: 0.4331425726413727
Epoch 970, training loss: 83.66209411621094 = 0.3646007478237152 + 10.0 * 8.32974910736084
Epoch 970, val loss: 0.4316033720970154
Epoch 980, training loss: 83.6375732421875 = 0.36258429288864136 + 10.0 * 8.327498435974121
Epoch 980, val loss: 0.4301738739013672
Epoch 990, training loss: 83.62482452392578 = 0.36060449481010437 + 10.0 * 8.326421737670898
Epoch 990, val loss: 0.42867621779441833
Epoch 1000, training loss: 83.61040496826172 = 0.35866600275039673 + 10.0 * 8.325174331665039
Epoch 1000, val loss: 0.4272807240486145
Epoch 1010, training loss: 83.68978881835938 = 0.3567461669445038 + 10.0 * 8.333304405212402
Epoch 1010, val loss: 0.4257563054561615
Epoch 1020, training loss: 83.60702514648438 = 0.3548160195350647 + 10.0 * 8.325221061706543
Epoch 1020, val loss: 0.42469480633735657
Epoch 1030, training loss: 83.58674621582031 = 0.35294803977012634 + 10.0 * 8.323379516601562
Epoch 1030, val loss: 0.4232642650604248
Epoch 1040, training loss: 83.57953643798828 = 0.35111191868782043 + 10.0 * 8.322842597961426
Epoch 1040, val loss: 0.42197442054748535
Epoch 1050, training loss: 83.57141876220703 = 0.349296510219574 + 10.0 * 8.322212219238281
Epoch 1050, val loss: 0.42072418332099915
Epoch 1060, training loss: 83.60436248779297 = 0.34749510884284973 + 10.0 * 8.32568645477295
Epoch 1060, val loss: 0.41948142647743225
Epoch 1070, training loss: 83.5723648071289 = 0.34568068385124207 + 10.0 * 8.322668075561523
Epoch 1070, val loss: 0.41837236285209656
Epoch 1080, training loss: 83.61625671386719 = 0.3438977301120758 + 10.0 * 8.32723617553711
Epoch 1080, val loss: 0.41727474331855774
Epoch 1090, training loss: 83.54576110839844 = 0.34211069345474243 + 10.0 * 8.320364952087402
Epoch 1090, val loss: 0.4159190058708191
Epoch 1100, training loss: 83.54412841796875 = 0.34037962555885315 + 10.0 * 8.320375442504883
Epoch 1100, val loss: 0.4146775007247925
Epoch 1110, training loss: 83.53280639648438 = 0.338678240776062 + 10.0 * 8.319413185119629
Epoch 1110, val loss: 0.41368213295936584
Epoch 1120, training loss: 83.52960968017578 = 0.33699509501457214 + 10.0 * 8.31926155090332
Epoch 1120, val loss: 0.412647545337677
Epoch 1130, training loss: 83.55901336669922 = 0.3353215456008911 + 10.0 * 8.322369575500488
Epoch 1130, val loss: 0.4117026925086975
Epoch 1140, training loss: 83.52082824707031 = 0.3336389660835266 + 10.0 * 8.318718910217285
Epoch 1140, val loss: 0.4104169011116028
Epoch 1150, training loss: 83.517822265625 = 0.3319855034351349 + 10.0 * 8.318583488464355
Epoch 1150, val loss: 0.4094378352165222
Epoch 1160, training loss: 83.50699615478516 = 0.330355703830719 + 10.0 * 8.31766414642334
Epoch 1160, val loss: 0.40846073627471924
Epoch 1170, training loss: 83.51996612548828 = 0.3287501335144043 + 10.0 * 8.319121360778809
Epoch 1170, val loss: 0.4073992669582367
Epoch 1180, training loss: 83.49906921386719 = 0.3271541893482208 + 10.0 * 8.317191123962402
Epoch 1180, val loss: 0.4065122604370117
Epoch 1190, training loss: 83.50709533691406 = 0.32557937502861023 + 10.0 * 8.318151473999023
Epoch 1190, val loss: 0.40545299649238586
Epoch 1200, training loss: 83.5019302368164 = 0.32401078939437866 + 10.0 * 8.317791938781738
Epoch 1200, val loss: 0.40457117557525635
Epoch 1210, training loss: 83.48030853271484 = 0.32246124744415283 + 10.0 * 8.315784454345703
Epoch 1210, val loss: 0.40384921431541443
Epoch 1220, training loss: 83.47028350830078 = 0.32093551754951477 + 10.0 * 8.314934730529785
Epoch 1220, val loss: 0.4029499590396881
Epoch 1230, training loss: 83.46665954589844 = 0.3194235861301422 + 10.0 * 8.31472396850586
Epoch 1230, val loss: 0.40203240513801575
Epoch 1240, training loss: 83.54712677001953 = 0.31792202591896057 + 10.0 * 8.322919845581055
Epoch 1240, val loss: 0.40102261304855347
Epoch 1250, training loss: 83.49315643310547 = 0.3164112865924835 + 10.0 * 8.31767463684082
Epoch 1250, val loss: 0.40045419335365295
Epoch 1260, training loss: 83.45114135742188 = 0.31493788957595825 + 10.0 * 8.313619613647461
Epoch 1260, val loss: 0.3997219204902649
Epoch 1270, training loss: 83.45072937011719 = 0.3134918212890625 + 10.0 * 8.31372356414795
Epoch 1270, val loss: 0.3989342749118805
Epoch 1280, training loss: 83.45957946777344 = 0.31206607818603516 + 10.0 * 8.314751625061035
Epoch 1280, val loss: 0.3983045518398285
Epoch 1290, training loss: 83.46451568603516 = 0.31064373254776 + 10.0 * 8.315386772155762
Epoch 1290, val loss: 0.3975830674171448
Epoch 1300, training loss: 83.44428253173828 = 0.30923035740852356 + 10.0 * 8.313505172729492
Epoch 1300, val loss: 0.3968404531478882
Epoch 1310, training loss: 83.42667388916016 = 0.30784040689468384 + 10.0 * 8.311883926391602
Epoch 1310, val loss: 0.3961791694164276
Epoch 1320, training loss: 83.41976165771484 = 0.30646973848342896 + 10.0 * 8.311328887939453
Epoch 1320, val loss: 0.3955558240413666
Epoch 1330, training loss: 83.41659545898438 = 0.3051133155822754 + 10.0 * 8.311147689819336
Epoch 1330, val loss: 0.39496779441833496
Epoch 1340, training loss: 83.50128173828125 = 0.30377504229545593 + 10.0 * 8.319750785827637
Epoch 1340, val loss: 0.3946036696434021
Epoch 1350, training loss: 83.44761657714844 = 0.3024035096168518 + 10.0 * 8.314520835876465
Epoch 1350, val loss: 0.3936735987663269
Epoch 1360, training loss: 83.40555572509766 = 0.3010759949684143 + 10.0 * 8.310447692871094
Epoch 1360, val loss: 0.3932309150695801
Epoch 1370, training loss: 83.39913940429688 = 0.2997676730155945 + 10.0 * 8.309937477111816
Epoch 1370, val loss: 0.39259764552116394
Epoch 1380, training loss: 83.40312957763672 = 0.2984730303287506 + 10.0 * 8.310465812683105
Epoch 1380, val loss: 0.3920384347438812
Epoch 1390, training loss: 83.4443359375 = 0.2971813976764679 + 10.0 * 8.314715385437012
Epoch 1390, val loss: 0.3914375901222229
Epoch 1400, training loss: 83.40605926513672 = 0.29589226841926575 + 10.0 * 8.311017036437988
Epoch 1400, val loss: 0.3911413550376892
Epoch 1410, training loss: 83.3873519897461 = 0.29461756348609924 + 10.0 * 8.309273719787598
Epoch 1410, val loss: 0.3905659019947052
Epoch 1420, training loss: 83.38410949707031 = 0.2933562994003296 + 10.0 * 8.309075355529785
Epoch 1420, val loss: 0.39010104537010193
Epoch 1430, training loss: 83.39222717285156 = 0.2921053171157837 + 10.0 * 8.310011863708496
Epoch 1430, val loss: 0.38962483406066895
Epoch 1440, training loss: 83.38045501708984 = 0.29086098074913025 + 10.0 * 8.308959007263184
Epoch 1440, val loss: 0.3892962634563446
Epoch 1450, training loss: 83.39559936523438 = 0.2896278500556946 + 10.0 * 8.31059741973877
Epoch 1450, val loss: 0.3889826834201813
Epoch 1460, training loss: 83.36814880371094 = 0.28839585185050964 + 10.0 * 8.307974815368652
Epoch 1460, val loss: 0.3885125517845154
Epoch 1470, training loss: 83.35633850097656 = 0.2871827483177185 + 10.0 * 8.306915283203125
Epoch 1470, val loss: 0.3881131410598755
Epoch 1480, training loss: 83.36132049560547 = 0.285979300737381 + 10.0 * 8.307534217834473
Epoch 1480, val loss: 0.3877558708190918
Epoch 1490, training loss: 83.37045288085938 = 0.28477776050567627 + 10.0 * 8.308568000793457
Epoch 1490, val loss: 0.3874359726905823
Epoch 1500, training loss: 83.36065673828125 = 0.28358376026153564 + 10.0 * 8.307706832885742
Epoch 1500, val loss: 0.38717955350875854
Epoch 1510, training loss: 83.37578582763672 = 0.2824001908302307 + 10.0 * 8.309338569641113
Epoch 1510, val loss: 0.38690075278282166
Epoch 1520, training loss: 83.35154724121094 = 0.2812168598175049 + 10.0 * 8.307032585144043
Epoch 1520, val loss: 0.38631704449653625
Epoch 1530, training loss: 83.33229064941406 = 0.280059278011322 + 10.0 * 8.30522346496582
Epoch 1530, val loss: 0.38610517978668213
Epoch 1540, training loss: 83.328369140625 = 0.27890992164611816 + 10.0 * 8.304945945739746
Epoch 1540, val loss: 0.3857780396938324
Epoch 1550, training loss: 83.34759521484375 = 0.2777656018733978 + 10.0 * 8.30698299407959
Epoch 1550, val loss: 0.3854018747806549
Epoch 1560, training loss: 83.34955596923828 = 0.27661848068237305 + 10.0 * 8.307293891906738
Epoch 1560, val loss: 0.3850899636745453
Epoch 1570, training loss: 83.31964111328125 = 0.2754756212234497 + 10.0 * 8.30441665649414
Epoch 1570, val loss: 0.3849318325519562
Epoch 1580, training loss: 83.31415557861328 = 0.27434682846069336 + 10.0 * 8.303980827331543
Epoch 1580, val loss: 0.38475000858306885
Epoch 1590, training loss: 83.31187438964844 = 0.2732246518135071 + 10.0 * 8.303865432739258
Epoch 1590, val loss: 0.3845258057117462
Epoch 1600, training loss: 83.33209991455078 = 0.27211180329322815 + 10.0 * 8.305998802185059
Epoch 1600, val loss: 0.3844095766544342
Epoch 1610, training loss: 83.32288360595703 = 0.27099087834358215 + 10.0 * 8.30518913269043
Epoch 1610, val loss: 0.38413742184638977
Epoch 1620, training loss: 83.32227325439453 = 0.26987797021865845 + 10.0 * 8.3052396774292
Epoch 1620, val loss: 0.3838213384151459
Epoch 1630, training loss: 83.31222534179688 = 0.26877182722091675 + 10.0 * 8.304346084594727
Epoch 1630, val loss: 0.3836623728275299
Epoch 1640, training loss: 83.30242156982422 = 0.2676699757575989 + 10.0 * 8.303475379943848
Epoch 1640, val loss: 0.383258193731308
Epoch 1650, training loss: 83.32493591308594 = 0.266575425863266 + 10.0 * 8.305835723876953
Epoch 1650, val loss: 0.3830745220184326
Epoch 1660, training loss: 83.28730773925781 = 0.265485018491745 + 10.0 * 8.3021821975708
Epoch 1660, val loss: 0.3830530345439911
Epoch 1670, training loss: 83.28816986083984 = 0.2644057273864746 + 10.0 * 8.302376747131348
Epoch 1670, val loss: 0.3829337954521179
Epoch 1680, training loss: 83.30258178710938 = 0.2633284628391266 + 10.0 * 8.303925514221191
Epoch 1680, val loss: 0.3827568292617798
Epoch 1690, training loss: 83.27890014648438 = 0.26225462555885315 + 10.0 * 8.301664352416992
Epoch 1690, val loss: 0.38249123096466064
Epoch 1700, training loss: 83.27565002441406 = 0.2611859440803528 + 10.0 * 8.301446914672852
Epoch 1700, val loss: 0.38223206996917725
Epoch 1710, training loss: 83.30362701416016 = 0.2601221799850464 + 10.0 * 8.304349899291992
Epoch 1710, val loss: 0.38194552063941956
Epoch 1720, training loss: 83.27049255371094 = 0.25905734300613403 + 10.0 * 8.301143646240234
Epoch 1720, val loss: 0.38213175535202026
Epoch 1730, training loss: 83.26339721679688 = 0.2579957842826843 + 10.0 * 8.30053997039795
Epoch 1730, val loss: 0.381917804479599
Epoch 1740, training loss: 83.25755310058594 = 0.25694721937179565 + 10.0 * 8.300060272216797
Epoch 1740, val loss: 0.38179805874824524
Epoch 1750, training loss: 83.25289916992188 = 0.2559014856815338 + 10.0 * 8.299699783325195
Epoch 1750, val loss: 0.38165345788002014
Epoch 1760, training loss: 83.27804565429688 = 0.2548622786998749 + 10.0 * 8.302318572998047
Epoch 1760, val loss: 0.38165995478630066
Epoch 1770, training loss: 83.26004791259766 = 0.2538210153579712 + 10.0 * 8.300622940063477
Epoch 1770, val loss: 0.3816141188144684
Epoch 1780, training loss: 83.24693298339844 = 0.25277459621429443 + 10.0 * 8.299415588378906
Epoch 1780, val loss: 0.38140934705734253
Epoch 1790, training loss: 83.24533081054688 = 0.2517363727092743 + 10.0 * 8.299359321594238
Epoch 1790, val loss: 0.3812592327594757
Epoch 1800, training loss: 83.23860931396484 = 0.2507050335407257 + 10.0 * 8.29879093170166
Epoch 1800, val loss: 0.3812066614627838
Epoch 1810, training loss: 83.2378158569336 = 0.2496776580810547 + 10.0 * 8.298813819885254
Epoch 1810, val loss: 0.38107043504714966
Epoch 1820, training loss: 83.29694366455078 = 0.248655766248703 + 10.0 * 8.304828643798828
Epoch 1820, val loss: 0.3808020353317261
Epoch 1830, training loss: 83.2681884765625 = 0.24763157963752747 + 10.0 * 8.302055358886719
Epoch 1830, val loss: 0.3811536431312561
Epoch 1840, training loss: 83.25147247314453 = 0.24660682678222656 + 10.0 * 8.30048656463623
Epoch 1840, val loss: 0.38104021549224854
Epoch 1850, training loss: 83.24497985839844 = 0.2455964833498001 + 10.0 * 8.299938201904297
Epoch 1850, val loss: 0.38105347752571106
Epoch 1860, training loss: 83.23377227783203 = 0.2445886880159378 + 10.0 * 8.298917770385742
Epoch 1860, val loss: 0.38112902641296387
Epoch 1870, training loss: 83.21612548828125 = 0.2435753047466278 + 10.0 * 8.29725456237793
Epoch 1870, val loss: 0.38086310029029846
Epoch 1880, training loss: 83.2188720703125 = 0.2425720989704132 + 10.0 * 8.297630310058594
Epoch 1880, val loss: 0.3807872235774994
Epoch 1890, training loss: 83.2274169921875 = 0.24157026410102844 + 10.0 * 8.298584938049316
Epoch 1890, val loss: 0.38073796033859253
Epoch 1900, training loss: 83.22544860839844 = 0.24056898057460785 + 10.0 * 8.298487663269043
Epoch 1900, val loss: 0.3807803690433502
Epoch 1910, training loss: 83.22708129882812 = 0.23957063257694244 + 10.0 * 8.298750877380371
Epoch 1910, val loss: 0.38097602128982544
Epoch 1920, training loss: 83.21484375 = 0.23857101798057556 + 10.0 * 8.297627449035645
Epoch 1920, val loss: 0.38107383251190186
Epoch 1930, training loss: 83.23377990722656 = 0.23757319152355194 + 10.0 * 8.299620628356934
Epoch 1930, val loss: 0.38103607296943665
Epoch 1940, training loss: 83.1976547241211 = 0.23657836019992828 + 10.0 * 8.296107292175293
Epoch 1940, val loss: 0.38099345564842224
Epoch 1950, training loss: 83.1920166015625 = 0.23558683693408966 + 10.0 * 8.295642852783203
Epoch 1950, val loss: 0.38099244236946106
Epoch 1960, training loss: 83.19618225097656 = 0.2346026450395584 + 10.0 * 8.296157836914062
Epoch 1960, val loss: 0.3809843063354492
Epoch 1970, training loss: 83.24583435058594 = 0.23362451791763306 + 10.0 * 8.301220893859863
Epoch 1970, val loss: 0.38089120388031006
Epoch 1980, training loss: 83.22525787353516 = 0.23264111578464508 + 10.0 * 8.299261093139648
Epoch 1980, val loss: 0.38115477561950684
Epoch 1990, training loss: 83.19658660888672 = 0.23165421187877655 + 10.0 * 8.296493530273438
Epoch 1990, val loss: 0.3812105655670166
Epoch 2000, training loss: 83.18402862548828 = 0.23067662119865417 + 10.0 * 8.29533576965332
Epoch 2000, val loss: 0.3814767301082611
Epoch 2010, training loss: 83.18663787841797 = 0.22969800233840942 + 10.0 * 8.295694351196289
Epoch 2010, val loss: 0.38156116008758545
Epoch 2020, training loss: 83.20327758789062 = 0.22872614860534668 + 10.0 * 8.297454833984375
Epoch 2020, val loss: 0.381717324256897
Epoch 2030, training loss: 83.17686462402344 = 0.22773659229278564 + 10.0 * 8.294912338256836
Epoch 2030, val loss: 0.3815465271472931
Epoch 2040, training loss: 83.19558715820312 = 0.22676105797290802 + 10.0 * 8.296882629394531
Epoch 2040, val loss: 0.38153910636901855
Epoch 2050, training loss: 83.1855239868164 = 0.2257845252752304 + 10.0 * 8.295973777770996
Epoch 2050, val loss: 0.3817303776741028
Epoch 2060, training loss: 83.17727661132812 = 0.22480636835098267 + 10.0 * 8.295247077941895
Epoch 2060, val loss: 0.3818562626838684
Epoch 2070, training loss: 83.17880249023438 = 0.22383350133895874 + 10.0 * 8.295496940612793
Epoch 2070, val loss: 0.38197827339172363
Epoch 2080, training loss: 83.1602554321289 = 0.22286108136177063 + 10.0 * 8.293739318847656
Epoch 2080, val loss: 0.3821094334125519
Epoch 2090, training loss: 83.15437316894531 = 0.22189034521579742 + 10.0 * 8.293248176574707
Epoch 2090, val loss: 0.38218286633491516
Epoch 2100, training loss: 83.18297576904297 = 0.2209269255399704 + 10.0 * 8.296205520629883
Epoch 2100, val loss: 0.382083922624588
Epoch 2110, training loss: 83.17584991455078 = 0.21995842456817627 + 10.0 * 8.295589447021484
Epoch 2110, val loss: 0.38241302967071533
Epoch 2120, training loss: 83.15247344970703 = 0.21898767352104187 + 10.0 * 8.29334831237793
Epoch 2120, val loss: 0.3827251195907593
Epoch 2130, training loss: 83.14764404296875 = 0.21801646053791046 + 10.0 * 8.292963027954102
Epoch 2130, val loss: 0.38285937905311584
Epoch 2140, training loss: 83.16138458251953 = 0.2170514464378357 + 10.0 * 8.29443359375
Epoch 2140, val loss: 0.38303342461586
Epoch 2150, training loss: 83.14684295654297 = 0.21608251333236694 + 10.0 * 8.293076515197754
Epoch 2150, val loss: 0.38312432169914246
Epoch 2160, training loss: 83.14324951171875 = 0.2151169627904892 + 10.0 * 8.292813301086426
Epoch 2160, val loss: 0.3833063542842865
Epoch 2170, training loss: 83.14927673339844 = 0.2141534835100174 + 10.0 * 8.293512344360352
Epoch 2170, val loss: 0.3833453953266144
Epoch 2180, training loss: 83.17147827148438 = 0.21319223940372467 + 10.0 * 8.295827865600586
Epoch 2180, val loss: 0.3836217522621155
Epoch 2190, training loss: 83.16355895996094 = 0.21223320066928864 + 10.0 * 8.295132637023926
Epoch 2190, val loss: 0.3841210603713989
Epoch 2200, training loss: 83.13996124267578 = 0.21126073598861694 + 10.0 * 8.29287052154541
Epoch 2200, val loss: 0.3841133713722229
Epoch 2210, training loss: 83.14419555664062 = 0.21030505001544952 + 10.0 * 8.293389320373535
Epoch 2210, val loss: 0.38457444310188293
Epoch 2220, training loss: 83.13231658935547 = 0.2093401402235031 + 10.0 * 8.29229736328125
Epoch 2220, val loss: 0.38477998971939087
Epoch 2230, training loss: 83.13069915771484 = 0.20837408304214478 + 10.0 * 8.292232513427734
Epoch 2230, val loss: 0.3849380612373352
Epoch 2240, training loss: 83.11668395996094 = 0.20740817487239838 + 10.0 * 8.29092788696289
Epoch 2240, val loss: 0.38513368368148804
Epoch 2250, training loss: 83.1143798828125 = 0.20644070208072662 + 10.0 * 8.290793418884277
Epoch 2250, val loss: 0.38534530997276306
Epoch 2260, training loss: 83.13063049316406 = 0.2054741382598877 + 10.0 * 8.292515754699707
Epoch 2260, val loss: 0.3856968581676483
Epoch 2270, training loss: 83.13461303710938 = 0.20450665056705475 + 10.0 * 8.293010711669922
Epoch 2270, val loss: 0.38605377078056335
Epoch 2280, training loss: 83.11672973632812 = 0.20352299511432648 + 10.0 * 8.29132080078125
Epoch 2280, val loss: 0.3860856592655182
Epoch 2290, training loss: 83.11167907714844 = 0.20254890620708466 + 10.0 * 8.290913581848145
Epoch 2290, val loss: 0.38642358779907227
Epoch 2300, training loss: 83.17259216308594 = 0.2015821784734726 + 10.0 * 8.297101020812988
Epoch 2300, val loss: 0.3863255977630615
Epoch 2310, training loss: 83.12370300292969 = 0.20060469210147858 + 10.0 * 8.292309761047363
Epoch 2310, val loss: 0.3871713876724243
Epoch 2320, training loss: 83.10506439208984 = 0.19962817430496216 + 10.0 * 8.290543556213379
Epoch 2320, val loss: 0.38724571466445923
Epoch 2330, training loss: 83.09559631347656 = 0.19865070283412933 + 10.0 * 8.289693832397461
Epoch 2330, val loss: 0.3876202702522278
Epoch 2340, training loss: 83.08995056152344 = 0.19767870008945465 + 10.0 * 8.289227485656738
Epoch 2340, val loss: 0.38788655400276184
Epoch 2350, training loss: 83.08560180664062 = 0.19670206308364868 + 10.0 * 8.28888988494873
Epoch 2350, val loss: 0.3882366120815277
Epoch 2360, training loss: 83.0877685546875 = 0.19572341442108154 + 10.0 * 8.289204597473145
Epoch 2360, val loss: 0.3885740637779236
Epoch 2370, training loss: 83.1634292602539 = 0.19475018978118896 + 10.0 * 8.296868324279785
Epoch 2370, val loss: 0.3889545798301697
Epoch 2380, training loss: 83.1051254272461 = 0.19376453757286072 + 10.0 * 8.291135787963867
Epoch 2380, val loss: 0.38927969336509705
Epoch 2390, training loss: 83.08374786376953 = 0.1927817165851593 + 10.0 * 8.28909683227539
Epoch 2390, val loss: 0.3896472156047821
Epoch 2400, training loss: 83.0773696899414 = 0.191794291138649 + 10.0 * 8.288557052612305
Epoch 2400, val loss: 0.3900658190250397
Epoch 2410, training loss: 83.11439514160156 = 0.19082887470722198 + 10.0 * 8.292356491088867
Epoch 2410, val loss: 0.3907663822174072
Epoch 2420, training loss: 83.0848159790039 = 0.1898290365934372 + 10.0 * 8.289499282836914
Epoch 2420, val loss: 0.39084839820861816
Epoch 2430, training loss: 83.07372283935547 = 0.18883351981639862 + 10.0 * 8.288488388061523
Epoch 2430, val loss: 0.3911609649658203
Epoch 2440, training loss: 83.06497192382812 = 0.18784500658512115 + 10.0 * 8.287713050842285
Epoch 2440, val loss: 0.3914298117160797
Epoch 2450, training loss: 83.06260681152344 = 0.1868545562028885 + 10.0 * 8.287575721740723
Epoch 2450, val loss: 0.39185604453086853
Epoch 2460, training loss: 83.06864166259766 = 0.18586638569831848 + 10.0 * 8.288277626037598
Epoch 2460, val loss: 0.39218655228614807
Epoch 2470, training loss: 83.10429382324219 = 0.18488715589046478 + 10.0 * 8.291940689086914
Epoch 2470, val loss: 0.3924689292907715
Epoch 2480, training loss: 83.0904541015625 = 0.18389864265918732 + 10.0 * 8.290655136108398
Epoch 2480, val loss: 0.39311665296554565
Epoch 2490, training loss: 83.06221008300781 = 0.18290935456752777 + 10.0 * 8.287930488586426
Epoch 2490, val loss: 0.3935666084289551
Epoch 2500, training loss: 83.06108856201172 = 0.1819242388010025 + 10.0 * 8.28791618347168
Epoch 2500, val loss: 0.3940787613391876
Epoch 2510, training loss: 83.05803680419922 = 0.18093454837799072 + 10.0 * 8.287710189819336
Epoch 2510, val loss: 0.3945285379886627
Epoch 2520, training loss: 83.06743621826172 = 0.17994821071624756 + 10.0 * 8.288748741149902
Epoch 2520, val loss: 0.3951716423034668
Epoch 2530, training loss: 83.06033325195312 = 0.1789545714855194 + 10.0 * 8.288137435913086
Epoch 2530, val loss: 0.3955768644809723
Epoch 2540, training loss: 83.0536117553711 = 0.1779564917087555 + 10.0 * 8.287565231323242
Epoch 2540, val loss: 0.3959839642047882
Epoch 2550, training loss: 83.05306243896484 = 0.17697036266326904 + 10.0 * 8.287609100341797
Epoch 2550, val loss: 0.3965936601161957
Epoch 2560, training loss: 83.05952453613281 = 0.1759832799434662 + 10.0 * 8.288354873657227
Epoch 2560, val loss: 0.39719629287719727
Epoch 2570, training loss: 83.04651641845703 = 0.17497284710407257 + 10.0 * 8.287154197692871
Epoch 2570, val loss: 0.3974808156490326
Epoch 2580, training loss: 83.05520629882812 = 0.17397819459438324 + 10.0 * 8.288122177124023
Epoch 2580, val loss: 0.3981288969516754
Epoch 2590, training loss: 83.04199981689453 = 0.1729745864868164 + 10.0 * 8.28690242767334
Epoch 2590, val loss: 0.3984745144844055
Epoch 2600, training loss: 83.03646087646484 = 0.1719720959663391 + 10.0 * 8.28644847869873
Epoch 2600, val loss: 0.398781418800354
Epoch 2610, training loss: 83.04458618164062 = 0.17097529768943787 + 10.0 * 8.287361145019531
Epoch 2610, val loss: 0.3992704451084137
Epoch 2620, training loss: 83.04962921142578 = 0.16997455060482025 + 10.0 * 8.287965774536133
Epoch 2620, val loss: 0.399799108505249
Epoch 2630, training loss: 83.03974151611328 = 0.1689661741256714 + 10.0 * 8.287076950073242
Epoch 2630, val loss: 0.4007350206375122
Epoch 2640, training loss: 83.02033996582031 = 0.16795696318149567 + 10.0 * 8.285238265991211
Epoch 2640, val loss: 0.4012954831123352
Epoch 2650, training loss: 83.01824951171875 = 0.1669493168592453 + 10.0 * 8.285130500793457
Epoch 2650, val loss: 0.40177208185195923
Epoch 2660, training loss: 83.02039337158203 = 0.16594119369983673 + 10.0 * 8.285445213317871
Epoch 2660, val loss: 0.4024891257286072
Epoch 2670, training loss: 83.06242370605469 = 0.16494978964328766 + 10.0 * 8.28974723815918
Epoch 2670, val loss: 0.4033758044242859
Epoch 2680, training loss: 83.04056549072266 = 0.16392023861408234 + 10.0 * 8.287664413452148
Epoch 2680, val loss: 0.403664231300354
Epoch 2690, training loss: 83.03073120117188 = 0.16290295124053955 + 10.0 * 8.286783218383789
Epoch 2690, val loss: 0.4040674865245819
Epoch 2700, training loss: 83.01062774658203 = 0.1618909239768982 + 10.0 * 8.284873962402344
Epoch 2700, val loss: 0.4047027826309204
Epoch 2710, training loss: 83.00362396240234 = 0.1608731597661972 + 10.0 * 8.28427505493164
Epoch 2710, val loss: 0.40533173084259033
Epoch 2720, training loss: 83.04093170166016 = 0.15985891222953796 + 10.0 * 8.288106918334961
Epoch 2720, val loss: 0.4058743119239807
Epoch 2730, training loss: 83.0049057006836 = 0.15883943438529968 + 10.0 * 8.28460693359375
Epoch 2730, val loss: 0.40658700466156006
Epoch 2740, training loss: 82.99897003173828 = 0.15781471133232117 + 10.0 * 8.2841157913208
Epoch 2740, val loss: 0.4072187542915344
Epoch 2750, training loss: 82.99238586425781 = 0.1567867398262024 + 10.0 * 8.283559799194336
Epoch 2750, val loss: 0.40781641006469727
Epoch 2760, training loss: 83.01591491699219 = 0.1557694375514984 + 10.0 * 8.286014556884766
Epoch 2760, val loss: 0.4086464047431946
Epoch 2770, training loss: 83.01016998291016 = 0.1547488123178482 + 10.0 * 8.285542488098145
Epoch 2770, val loss: 0.40923595428466797
Epoch 2780, training loss: 82.98786926269531 = 0.15370339155197144 + 10.0 * 8.283416748046875
Epoch 2780, val loss: 0.40960580110549927
Epoch 2790, training loss: 82.98486328125 = 0.15267464518547058 + 10.0 * 8.283219337463379
Epoch 2790, val loss: 0.4103403091430664
Epoch 2800, training loss: 82.98136138916016 = 0.1516464650630951 + 10.0 * 8.282971382141113
Epoch 2800, val loss: 0.41103389859199524
Epoch 2810, training loss: 82.98307800292969 = 0.15061615407466888 + 10.0 * 8.283246040344238
Epoch 2810, val loss: 0.4116073548793793
Epoch 2820, training loss: 83.03699493408203 = 0.14960342645645142 + 10.0 * 8.288739204406738
Epoch 2820, val loss: 0.4121275842189789
Epoch 2830, training loss: 83.00387573242188 = 0.1485656052827835 + 10.0 * 8.285531044006348
Epoch 2830, val loss: 0.4130081534385681
Epoch 2840, training loss: 82.98291015625 = 0.1475316882133484 + 10.0 * 8.283537864685059
Epoch 2840, val loss: 0.4137784242630005
Epoch 2850, training loss: 82.97700500488281 = 0.14650338888168335 + 10.0 * 8.283050537109375
Epoch 2850, val loss: 0.41480597853660583
Epoch 2860, training loss: 82.99222564697266 = 0.14548155665397644 + 10.0 * 8.284673690795898
Epoch 2860, val loss: 0.4156322181224823
Epoch 2870, training loss: 82.96602630615234 = 0.14444656670093536 + 10.0 * 8.282157897949219
Epoch 2870, val loss: 0.4162836968898773
Epoch 2880, training loss: 82.97042846679688 = 0.14341701567173004 + 10.0 * 8.28270149230957
Epoch 2880, val loss: 0.41704580187797546
Epoch 2890, training loss: 82.99337005615234 = 0.14241543412208557 + 10.0 * 8.28509521484375
Epoch 2890, val loss: 0.4182763993740082
Epoch 2900, training loss: 82.99246215820312 = 0.14139346778392792 + 10.0 * 8.285106658935547
Epoch 2900, val loss: 0.4189215302467346
Epoch 2910, training loss: 82.96977996826172 = 0.14037001132965088 + 10.0 * 8.282940864562988
Epoch 2910, val loss: 0.4192676842212677
Epoch 2920, training loss: 82.95704650878906 = 0.13934656977653503 + 10.0 * 8.281770706176758
Epoch 2920, val loss: 0.42056605219841003
Epoch 2930, training loss: 82.95188903808594 = 0.1383245885372162 + 10.0 * 8.281356811523438
Epoch 2930, val loss: 0.421199232339859
Epoch 2940, training loss: 82.95758819580078 = 0.13731268048286438 + 10.0 * 8.282027244567871
Epoch 2940, val loss: 0.4221251904964447
Epoch 2950, training loss: 82.99877166748047 = 0.13631309568881989 + 10.0 * 8.286245346069336
Epoch 2950, val loss: 0.42315056920051575
Epoch 2960, training loss: 82.96311950683594 = 0.1352919191122055 + 10.0 * 8.282782554626465
Epoch 2960, val loss: 0.42406901717185974
Epoch 2970, training loss: 82.96515655517578 = 0.134286567568779 + 10.0 * 8.283086776733398
Epoch 2970, val loss: 0.42503631114959717
Epoch 2980, training loss: 82.95880126953125 = 0.13327288627624512 + 10.0 * 8.282552719116211
Epoch 2980, val loss: 0.42587390542030334
Epoch 2990, training loss: 82.9463119506836 = 0.13225923478603363 + 10.0 * 8.281405448913574
Epoch 2990, val loss: 0.42674824595451355
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8493150684931506
0.8669129899297255
=== training gcn model ===
Epoch 0, training loss: 106.92646026611328 = 1.103941559791565 + 10.0 * 10.58225154876709
Epoch 0, val loss: 1.103897213935852
Epoch 10, training loss: 106.90780639648438 = 1.0969089269638062 + 10.0 * 10.581089973449707
Epoch 10, val loss: 1.0968079566955566
Epoch 20, training loss: 106.78939819335938 = 1.088830590248108 + 10.0 * 10.570056915283203
Epoch 20, val loss: 1.0886123180389404
Epoch 30, training loss: 106.0847396850586 = 1.0795079469680786 + 10.0 * 10.500523567199707
Epoch 30, val loss: 1.0791234970092773
Epoch 40, training loss: 103.8639907836914 = 1.0702136754989624 + 10.0 * 10.279377937316895
Epoch 40, val loss: 1.069989562034607
Epoch 50, training loss: 99.98567199707031 = 1.0619778633117676 + 10.0 * 9.892369270324707
Epoch 50, val loss: 1.0614254474639893
Epoch 60, training loss: 95.40606689453125 = 1.053837776184082 + 10.0 * 9.435222625732422
Epoch 60, val loss: 1.0533418655395508
Epoch 70, training loss: 93.38941955566406 = 1.0440003871917725 + 10.0 * 9.234541893005371
Epoch 70, val loss: 1.043544888496399
Epoch 80, training loss: 92.98485565185547 = 1.0337480306625366 + 10.0 * 9.195111274719238
Epoch 80, val loss: 1.0334607362747192
Epoch 90, training loss: 92.76386260986328 = 1.0246366262435913 + 10.0 * 9.173922538757324
Epoch 90, val loss: 1.0245243310928345
Epoch 100, training loss: 92.40641784667969 = 1.0170893669128418 + 10.0 * 9.138933181762695
Epoch 100, val loss: 1.0171033143997192
Epoch 110, training loss: 91.87105560302734 = 1.0104926824569702 + 10.0 * 9.08605670928955
Epoch 110, val loss: 1.0105899572372437
Epoch 120, training loss: 91.00167846679688 = 1.0041824579238892 + 10.0 * 8.999750137329102
Epoch 120, val loss: 1.004375696182251
Epoch 130, training loss: 90.11904907226562 = 0.9977384805679321 + 10.0 * 8.912130355834961
Epoch 130, val loss: 0.9980230331420898
Epoch 140, training loss: 89.61334228515625 = 0.990172803401947 + 10.0 * 8.862317085266113
Epoch 140, val loss: 0.99048912525177
Epoch 150, training loss: 89.02278137207031 = 0.9814434051513672 + 10.0 * 8.804133415222168
Epoch 150, val loss: 0.9818152785301208
Epoch 160, training loss: 88.5737075805664 = 0.9723749160766602 + 10.0 * 8.760133743286133
Epoch 160, val loss: 0.9726207256317139
Epoch 170, training loss: 88.2605209350586 = 0.9615381360054016 + 10.0 * 8.729898452758789
Epoch 170, val loss: 0.9616125226020813
Epoch 180, training loss: 88.1436538696289 = 0.9485306143760681 + 10.0 * 8.719511985778809
Epoch 180, val loss: 0.9485849738121033
Epoch 190, training loss: 87.95006561279297 = 0.9342951774597168 + 10.0 * 8.701577186584473
Epoch 190, val loss: 0.9346896409988403
Epoch 200, training loss: 87.75137329101562 = 0.920319676399231 + 10.0 * 8.68310546875
Epoch 200, val loss: 0.9209309816360474
Epoch 210, training loss: 87.52897644042969 = 0.9063746333122253 + 10.0 * 8.662260055541992
Epoch 210, val loss: 0.9072024822235107
Epoch 220, training loss: 87.30685424804688 = 0.8915556073188782 + 10.0 * 8.64153003692627
Epoch 220, val loss: 0.8924086689949036
Epoch 230, training loss: 87.12649536132812 = 0.8752328753471375 + 10.0 * 8.625125885009766
Epoch 230, val loss: 0.8761923313140869
Epoch 240, training loss: 86.92881774902344 = 0.8578083515167236 + 10.0 * 8.607100486755371
Epoch 240, val loss: 0.8589231371879578
Epoch 250, training loss: 86.76490020751953 = 0.8394865393638611 + 10.0 * 8.592541694641113
Epoch 250, val loss: 0.8407592177391052
Epoch 260, training loss: 86.60963439941406 = 0.8199787139892578 + 10.0 * 8.57896614074707
Epoch 260, val loss: 0.8215099573135376
Epoch 270, training loss: 86.46553039550781 = 0.7997101545333862 + 10.0 * 8.566581726074219
Epoch 270, val loss: 0.8016517162322998
Epoch 280, training loss: 86.35795593261719 = 0.7789612412452698 + 10.0 * 8.557899475097656
Epoch 280, val loss: 0.7812526822090149
Epoch 290, training loss: 86.27930450439453 = 0.7578670978546143 + 10.0 * 8.552144050598145
Epoch 290, val loss: 0.760515570640564
Epoch 300, training loss: 86.17810821533203 = 0.7365294694900513 + 10.0 * 8.544157981872559
Epoch 300, val loss: 0.7397485375404358
Epoch 310, training loss: 86.07760620117188 = 0.7155972719192505 + 10.0 * 8.536200523376465
Epoch 310, val loss: 0.7194087505340576
Epoch 320, training loss: 85.97917175292969 = 0.6952722072601318 + 10.0 * 8.528389930725098
Epoch 320, val loss: 0.6996397376060486
Epoch 330, training loss: 85.89241027832031 = 0.6755753755569458 + 10.0 * 8.521683692932129
Epoch 330, val loss: 0.680448055267334
Epoch 340, training loss: 85.77767944335938 = 0.6567118763923645 + 10.0 * 8.512096405029297
Epoch 340, val loss: 0.6622747182846069
Epoch 350, training loss: 85.7083511352539 = 0.6386834979057312 + 10.0 * 8.506966590881348
Epoch 350, val loss: 0.644896924495697
Epoch 360, training loss: 85.56919860839844 = 0.6214019656181335 + 10.0 * 8.494779586791992
Epoch 360, val loss: 0.6281756162643433
Epoch 370, training loss: 85.4914321899414 = 0.6049487590789795 + 10.0 * 8.488648414611816
Epoch 370, val loss: 0.6123818755149841
Epoch 380, training loss: 85.42132568359375 = 0.5891541242599487 + 10.0 * 8.483217239379883
Epoch 380, val loss: 0.5971434712409973
Epoch 390, training loss: 85.33816528320312 = 0.5741133689880371 + 10.0 * 8.476405143737793
Epoch 390, val loss: 0.5828737020492554
Epoch 400, training loss: 85.28043365478516 = 0.5599406957626343 + 10.0 * 8.47204875946045
Epoch 400, val loss: 0.569385290145874
Epoch 410, training loss: 85.20215606689453 = 0.5464807748794556 + 10.0 * 8.465567588806152
Epoch 410, val loss: 0.5565093159675598
Epoch 420, training loss: 85.15021514892578 = 0.5339077711105347 + 10.0 * 8.461630821228027
Epoch 420, val loss: 0.54463130235672
Epoch 430, training loss: 85.09520721435547 = 0.5221529603004456 + 10.0 * 8.457304954528809
Epoch 430, val loss: 0.533618688583374
Epoch 440, training loss: 85.04920959472656 = 0.5111505389213562 + 10.0 * 8.453805923461914
Epoch 440, val loss: 0.5233393907546997
Epoch 450, training loss: 85.05255126953125 = 0.5009200572967529 + 10.0 * 8.45516300201416
Epoch 450, val loss: 0.5138989090919495
Epoch 460, training loss: 84.94874572753906 = 0.49143457412719727 + 10.0 * 8.445731163024902
Epoch 460, val loss: 0.5050349235534668
Epoch 470, training loss: 84.89918518066406 = 0.48280394077301025 + 10.0 * 8.441637992858887
Epoch 470, val loss: 0.49709826707839966
Epoch 480, training loss: 84.85890197753906 = 0.47482046484947205 + 10.0 * 8.438407897949219
Epoch 480, val loss: 0.48976731300354004
Epoch 490, training loss: 84.81083679199219 = 0.46728643774986267 + 10.0 * 8.434354782104492
Epoch 490, val loss: 0.48307183384895325
Epoch 500, training loss: 84.76021575927734 = 0.4603467881679535 + 10.0 * 8.429986953735352
Epoch 500, val loss: 0.4768710732460022
Epoch 510, training loss: 84.7155990600586 = 0.4539717733860016 + 10.0 * 8.426162719726562
Epoch 510, val loss: 0.471195250749588
Epoch 520, training loss: 84.72425079345703 = 0.4480152726173401 + 10.0 * 8.427623748779297
Epoch 520, val loss: 0.4661673307418823
Epoch 530, training loss: 84.65441131591797 = 0.442303329706192 + 10.0 * 8.421210289001465
Epoch 530, val loss: 0.4609706401824951
Epoch 540, training loss: 84.59746551513672 = 0.43706560134887695 + 10.0 * 8.416040420532227
Epoch 540, val loss: 0.4565412402153015
Epoch 550, training loss: 84.56178283691406 = 0.4322245419025421 + 10.0 * 8.412955284118652
Epoch 550, val loss: 0.4524233639240265
Epoch 560, training loss: 84.52867126464844 = 0.4276762008666992 + 10.0 * 8.410099983215332
Epoch 560, val loss: 0.448603093624115
Epoch 570, training loss: 84.59185028076172 = 0.423371285200119 + 10.0 * 8.416848182678223
Epoch 570, val loss: 0.44500547647476196
Epoch 580, training loss: 84.47085571289062 = 0.41925206780433655 + 10.0 * 8.405160903930664
Epoch 580, val loss: 0.44155392050743103
Epoch 590, training loss: 84.50833129882812 = 0.4154355227947235 + 10.0 * 8.409289360046387
Epoch 590, val loss: 0.4383426308631897
Epoch 600, training loss: 84.43907928466797 = 0.4118000864982605 + 10.0 * 8.402728080749512
Epoch 600, val loss: 0.43564534187316895
Epoch 610, training loss: 84.39910125732422 = 0.4084114730358124 + 10.0 * 8.399068832397461
Epoch 610, val loss: 0.43287143111228943
Epoch 620, training loss: 84.37325286865234 = 0.40520650148391724 + 10.0 * 8.396804809570312
Epoch 620, val loss: 0.43043169379234314
Epoch 630, training loss: 84.35472106933594 = 0.40215542912483215 + 10.0 * 8.395256996154785
Epoch 630, val loss: 0.42809924483299255
Epoch 640, training loss: 84.33948516845703 = 0.3992190361022949 + 10.0 * 8.394026756286621
Epoch 640, val loss: 0.42588815093040466
Epoch 650, training loss: 84.37824249267578 = 0.39639824628829956 + 10.0 * 8.398183822631836
Epoch 650, val loss: 0.42381155490875244
Epoch 660, training loss: 84.3047103881836 = 0.39368975162506104 + 10.0 * 8.391101837158203
Epoch 660, val loss: 0.42164769768714905
Epoch 670, training loss: 84.27468872070312 = 0.39116284251213074 + 10.0 * 8.38835334777832
Epoch 670, val loss: 0.41993042826652527
Epoch 680, training loss: 84.25298309326172 = 0.3887411057949066 + 10.0 * 8.38642406463623
Epoch 680, val loss: 0.4181651473045349
Epoch 690, training loss: 84.25871276855469 = 0.38641753792762756 + 10.0 * 8.387228965759277
Epoch 690, val loss: 0.41670504212379456
Epoch 700, training loss: 84.23011779785156 = 0.38413235545158386 + 10.0 * 8.384598731994629
Epoch 700, val loss: 0.4149497449398041
Epoch 710, training loss: 84.21351623535156 = 0.38195836544036865 + 10.0 * 8.383155822753906
Epoch 710, val loss: 0.4135797619819641
Epoch 720, training loss: 84.1883773803711 = 0.379888653755188 + 10.0 * 8.38084888458252
Epoch 720, val loss: 0.41213515400886536
Epoch 730, training loss: 84.25331115722656 = 0.3778889775276184 + 10.0 * 8.387541770935059
Epoch 730, val loss: 0.41078487038612366
Epoch 740, training loss: 84.17637634277344 = 0.37590405344963074 + 10.0 * 8.380047798156738
Epoch 740, val loss: 0.4096583425998688
Epoch 750, training loss: 84.15225982666016 = 0.3740481436252594 + 10.0 * 8.37782096862793
Epoch 750, val loss: 0.40839892625808716
Epoch 760, training loss: 84.12615966796875 = 0.3722592890262604 + 10.0 * 8.37539005279541
Epoch 760, val loss: 0.4072935879230499
Epoch 770, training loss: 84.1203842163086 = 0.3705271780490875 + 10.0 * 8.374985694885254
Epoch 770, val loss: 0.40626809000968933
Epoch 780, training loss: 84.13994598388672 = 0.3688184916973114 + 10.0 * 8.37711238861084
Epoch 780, val loss: 0.40510690212249756
Epoch 790, training loss: 84.09297180175781 = 0.3671477735042572 + 10.0 * 8.37258243560791
Epoch 790, val loss: 0.4042169451713562
Epoch 800, training loss: 84.076171875 = 0.3655549883842468 + 10.0 * 8.371061325073242
Epoch 800, val loss: 0.40332746505737305
Epoch 810, training loss: 84.07048034667969 = 0.36401301622390747 + 10.0 * 8.370646476745605
Epoch 810, val loss: 0.4025062620639801
Epoch 820, training loss: 84.11512756347656 = 0.3624955713748932 + 10.0 * 8.375263214111328
Epoch 820, val loss: 0.40171393752098083
Epoch 830, training loss: 84.06263732910156 = 0.36099350452423096 + 10.0 * 8.37016487121582
Epoch 830, val loss: 0.4006228744983673
Epoch 840, training loss: 84.04393768310547 = 0.35955294966697693 + 10.0 * 8.368438720703125
Epoch 840, val loss: 0.399955689907074
Epoch 850, training loss: 84.0225830078125 = 0.3581520915031433 + 10.0 * 8.366442680358887
Epoch 850, val loss: 0.3990494906902313
Epoch 860, training loss: 84.0205307006836 = 0.3567811846733093 + 10.0 * 8.366374969482422
Epoch 860, val loss: 0.3983754813671112
Epoch 870, training loss: 84.0153579711914 = 0.3554423153400421 + 10.0 * 8.365991592407227
Epoch 870, val loss: 0.397623211145401
Epoch 880, training loss: 83.98240661621094 = 0.35412442684173584 + 10.0 * 8.362828254699707
Epoch 880, val loss: 0.3969474136829376
Epoch 890, training loss: 83.96697235107422 = 0.3528442978858948 + 10.0 * 8.36141300201416
Epoch 890, val loss: 0.39628860354423523
Epoch 900, training loss: 84.02377319335938 = 0.35158851742744446 + 10.0 * 8.367218971252441
Epoch 900, val loss: 0.3957628011703491
Epoch 910, training loss: 83.98956298828125 = 0.3503105342388153 + 10.0 * 8.363924980163574
Epoch 910, val loss: 0.39482253789901733
Epoch 920, training loss: 83.9445571899414 = 0.34909260272979736 + 10.0 * 8.359546661376953
Epoch 920, val loss: 0.39426398277282715
Epoch 930, training loss: 83.9264907836914 = 0.347905695438385 + 10.0 * 8.357858657836914
Epoch 930, val loss: 0.39364421367645264
Epoch 940, training loss: 83.92706298828125 = 0.34674376249313354 + 10.0 * 8.3580322265625
Epoch 940, val loss: 0.3929714858531952
Epoch 950, training loss: 83.93183898925781 = 0.34557023644447327 + 10.0 * 8.358626365661621
Epoch 950, val loss: 0.39237281680107117
Epoch 960, training loss: 83.91184997558594 = 0.34441882371902466 + 10.0 * 8.356742858886719
Epoch 960, val loss: 0.39185282588005066
Epoch 970, training loss: 83.8923568725586 = 0.34330078959465027 + 10.0 * 8.35490608215332
Epoch 970, val loss: 0.391298770904541
Epoch 980, training loss: 83.88209533691406 = 0.3422091007232666 + 10.0 * 8.353988647460938
Epoch 980, val loss: 0.39080679416656494
Epoch 990, training loss: 83.93162536621094 = 0.34112557768821716 + 10.0 * 8.359049797058105
Epoch 990, val loss: 0.3902466595172882
Epoch 1000, training loss: 83.88815307617188 = 0.3400309085845947 + 10.0 * 8.354811668395996
Epoch 1000, val loss: 0.3897319734096527
Epoch 1010, training loss: 83.86604309082031 = 0.33896374702453613 + 10.0 * 8.352707862854004
Epoch 1010, val loss: 0.3891802132129669
Epoch 1020, training loss: 83.84722900390625 = 0.3379182815551758 + 10.0 * 8.350931167602539
Epoch 1020, val loss: 0.38868629932403564
Epoch 1030, training loss: 83.83769989013672 = 0.33688393235206604 + 10.0 * 8.350081443786621
Epoch 1030, val loss: 0.3882294297218323
Epoch 1040, training loss: 83.89419555664062 = 0.33586132526397705 + 10.0 * 8.355833053588867
Epoch 1040, val loss: 0.38794320821762085
Epoch 1050, training loss: 83.85911560058594 = 0.3348051607608795 + 10.0 * 8.35243034362793
Epoch 1050, val loss: 0.3871062099933624
Epoch 1060, training loss: 83.81522369384766 = 0.33378708362579346 + 10.0 * 8.348143577575684
Epoch 1060, val loss: 0.3866932690143585
Epoch 1070, training loss: 83.81151580810547 = 0.33279016613960266 + 10.0 * 8.347872734069824
Epoch 1070, val loss: 0.38630151748657227
Epoch 1080, training loss: 83.79966735839844 = 0.33180469274520874 + 10.0 * 8.346786499023438
Epoch 1080, val loss: 0.3858426809310913
Epoch 1090, training loss: 83.8541259765625 = 0.33082863688468933 + 10.0 * 8.352330207824707
Epoch 1090, val loss: 0.3855034112930298
Epoch 1100, training loss: 83.82105255126953 = 0.3298388123512268 + 10.0 * 8.34912109375
Epoch 1100, val loss: 0.38482871651649475
Epoch 1110, training loss: 83.80793762207031 = 0.3288669288158417 + 10.0 * 8.347907066345215
Epoch 1110, val loss: 0.38437923789024353
Epoch 1120, training loss: 83.77578735351562 = 0.32790809869766235 + 10.0 * 8.34478759765625
Epoch 1120, val loss: 0.38406407833099365
Epoch 1130, training loss: 83.78802490234375 = 0.32696670293807983 + 10.0 * 8.346105575561523
Epoch 1130, val loss: 0.3835376799106598
Epoch 1140, training loss: 83.78060913085938 = 0.32601872086524963 + 10.0 * 8.345458984375
Epoch 1140, val loss: 0.3831478953361511
Epoch 1150, training loss: 83.76478576660156 = 0.32507947087287903 + 10.0 * 8.34397029876709
Epoch 1150, val loss: 0.38287487626075745
Epoch 1160, training loss: 83.74979400634766 = 0.3241558074951172 + 10.0 * 8.34256362915039
Epoch 1160, val loss: 0.3823505640029907
Epoch 1170, training loss: 83.74458312988281 = 0.32323870062828064 + 10.0 * 8.342134475708008
Epoch 1170, val loss: 0.3820463716983795
Epoch 1180, training loss: 83.76781463623047 = 0.3223157823085785 + 10.0 * 8.344549179077148
Epoch 1180, val loss: 0.3816794455051422
Epoch 1190, training loss: 83.72833251953125 = 0.32139259576797485 + 10.0 * 8.340693473815918
Epoch 1190, val loss: 0.381232887506485
Epoch 1200, training loss: 83.7242660522461 = 0.3204914331436157 + 10.0 * 8.340377807617188
Epoch 1200, val loss: 0.3808106780052185
Epoch 1210, training loss: 83.71549987792969 = 0.31959739327430725 + 10.0 * 8.339590072631836
Epoch 1210, val loss: 0.3804541230201721
Epoch 1220, training loss: 83.79859161376953 = 0.31870830059051514 + 10.0 * 8.34798812866211
Epoch 1220, val loss: 0.37993907928466797
Epoch 1230, training loss: 83.7386703491211 = 0.31779545545578003 + 10.0 * 8.342087745666504
Epoch 1230, val loss: 0.37983837723731995
Epoch 1240, training loss: 83.70626831054688 = 0.31691139936447144 + 10.0 * 8.338935852050781
Epoch 1240, val loss: 0.37936124205589294
Epoch 1250, training loss: 83.68817138671875 = 0.3160363733768463 + 10.0 * 8.337213516235352
Epoch 1250, val loss: 0.3790173828601837
Epoch 1260, training loss: 83.68289184570312 = 0.3151696026325226 + 10.0 * 8.336771965026855
Epoch 1260, val loss: 0.37868648767471313
Epoch 1270, training loss: 83.74408721923828 = 0.31429776549339294 + 10.0 * 8.342978477478027
Epoch 1270, val loss: 0.3782648742198944
Epoch 1280, training loss: 83.70115661621094 = 0.31341904401779175 + 10.0 * 8.338773727416992
Epoch 1280, val loss: 0.3780999481678009
Epoch 1290, training loss: 83.67110443115234 = 0.3125549256801605 + 10.0 * 8.335855484008789
Epoch 1290, val loss: 0.37771233916282654
Epoch 1300, training loss: 83.65947723388672 = 0.31169942021369934 + 10.0 * 8.33477783203125
Epoch 1300, val loss: 0.37738460302352905
Epoch 1310, training loss: 83.65403747558594 = 0.3108515739440918 + 10.0 * 8.334318161010742
Epoch 1310, val loss: 0.37707415223121643
Epoch 1320, training loss: 83.7125244140625 = 0.31000590324401855 + 10.0 * 8.340251922607422
Epoch 1320, val loss: 0.3769277334213257
Epoch 1330, training loss: 83.69308471679688 = 0.30914250016212463 + 10.0 * 8.338394165039062
Epoch 1330, val loss: 0.3763924539089203
Epoch 1340, training loss: 83.65916442871094 = 0.30828723311424255 + 10.0 * 8.335087776184082
Epoch 1340, val loss: 0.3760873079299927
Epoch 1350, training loss: 83.63211059570312 = 0.3074455261230469 + 10.0 * 8.332467079162598
Epoch 1350, val loss: 0.375789999961853
Epoch 1360, training loss: 83.63008880615234 = 0.30661097168922424 + 10.0 * 8.332347869873047
Epoch 1360, val loss: 0.37548768520355225
Epoch 1370, training loss: 83.64630889892578 = 0.30577728152275085 + 10.0 * 8.334053039550781
Epoch 1370, val loss: 0.3752332627773285
Epoch 1380, training loss: 83.62934112548828 = 0.30493608117103577 + 10.0 * 8.332440376281738
Epoch 1380, val loss: 0.37482452392578125
Epoch 1390, training loss: 83.63618469238281 = 0.3040958642959595 + 10.0 * 8.333209037780762
Epoch 1390, val loss: 0.37453216314315796
Epoch 1400, training loss: 83.60904693603516 = 0.30325955152511597 + 10.0 * 8.330578804016113
Epoch 1400, val loss: 0.3743944764137268
Epoch 1410, training loss: 83.60700988769531 = 0.3024316430091858 + 10.0 * 8.33045768737793
Epoch 1410, val loss: 0.3740653097629547
Epoch 1420, training loss: 83.60984802246094 = 0.30160367488861084 + 10.0 * 8.33082389831543
Epoch 1420, val loss: 0.37387815117836
Epoch 1430, training loss: 83.63261413574219 = 0.3007683753967285 + 10.0 * 8.333185195922852
Epoch 1430, val loss: 0.37347760796546936
Epoch 1440, training loss: 83.62877655029297 = 0.2999345064163208 + 10.0 * 8.332883834838867
Epoch 1440, val loss: 0.37318184971809387
Epoch 1450, training loss: 83.59233093261719 = 0.29909735918045044 + 10.0 * 8.329323768615723
Epoch 1450, val loss: 0.3729906678199768
Epoch 1460, training loss: 83.57630157470703 = 0.29827144742012024 + 10.0 * 8.327802658081055
Epoch 1460, val loss: 0.3727518916130066
Epoch 1470, training loss: 83.57595825195312 = 0.2974450886249542 + 10.0 * 8.327851295471191
Epoch 1470, val loss: 0.37253111600875854
Epoch 1480, training loss: 83.61903381347656 = 0.29661989212036133 + 10.0 * 8.33224105834961
Epoch 1480, val loss: 0.37231481075286865
Epoch 1490, training loss: 83.5834732055664 = 0.2957780063152313 + 10.0 * 8.32876968383789
Epoch 1490, val loss: 0.3720635175704956
Epoch 1500, training loss: 83.60131072998047 = 0.2949519753456116 + 10.0 * 8.330636024475098
Epoch 1500, val loss: 0.37188589572906494
Epoch 1510, training loss: 83.5663833618164 = 0.29410916566848755 + 10.0 * 8.327227592468262
Epoch 1510, val loss: 0.3714642822742462
Epoch 1520, training loss: 83.55633544921875 = 0.2932763695716858 + 10.0 * 8.326306343078613
Epoch 1520, val loss: 0.3712776005268097
Epoch 1530, training loss: 83.5486068725586 = 0.29245439171791077 + 10.0 * 8.325614929199219
Epoch 1530, val loss: 0.3710840344429016
Epoch 1540, training loss: 83.54326629638672 = 0.2916274964809418 + 10.0 * 8.325163841247559
Epoch 1540, val loss: 0.37086421251296997
Epoch 1550, training loss: 83.6064224243164 = 0.2908027768135071 + 10.0 * 8.331562042236328
Epoch 1550, val loss: 0.37081480026245117
Epoch 1560, training loss: 83.55233764648438 = 0.28995901346206665 + 10.0 * 8.326237678527832
Epoch 1560, val loss: 0.37034252285957336
Epoch 1570, training loss: 83.5362319946289 = 0.2891245186328888 + 10.0 * 8.324710845947266
Epoch 1570, val loss: 0.370256632566452
Epoch 1580, training loss: 83.53402709960938 = 0.28829482197761536 + 10.0 * 8.324573516845703
Epoch 1580, val loss: 0.36993271112442017
Epoch 1590, training loss: 83.55842590332031 = 0.28746461868286133 + 10.0 * 8.327095985412598
Epoch 1590, val loss: 0.3698287606239319
Epoch 1600, training loss: 83.53482055664062 = 0.28663551807403564 + 10.0 * 8.32481861114502
Epoch 1600, val loss: 0.36964932084083557
Epoch 1610, training loss: 83.55097198486328 = 0.28580641746520996 + 10.0 * 8.326517105102539
Epoch 1610, val loss: 0.36954885721206665
Epoch 1620, training loss: 83.50680541992188 = 0.2849700152873993 + 10.0 * 8.322183609008789
Epoch 1620, val loss: 0.3691798448562622
Epoch 1630, training loss: 83.5094985961914 = 0.2841416299343109 + 10.0 * 8.322535514831543
Epoch 1630, val loss: 0.36899375915527344
Epoch 1640, training loss: 83.5003662109375 = 0.2833130657672882 + 10.0 * 8.32170581817627
Epoch 1640, val loss: 0.36883726716041565
Epoch 1650, training loss: 83.5796127319336 = 0.28248757123947144 + 10.0 * 8.329712867736816
Epoch 1650, val loss: 0.3686649799346924
Epoch 1660, training loss: 83.54350280761719 = 0.28163713216781616 + 10.0 * 8.326186180114746
Epoch 1660, val loss: 0.36854758858680725
Epoch 1670, training loss: 83.48887634277344 = 0.2808048129081726 + 10.0 * 8.320806503295898
Epoch 1670, val loss: 0.3683357536792755
Epoch 1680, training loss: 83.4870376586914 = 0.2799801826477051 + 10.0 * 8.32070541381836
Epoch 1680, val loss: 0.36821621656417847
Epoch 1690, training loss: 83.4863052368164 = 0.27915430068969727 + 10.0 * 8.320714950561523
Epoch 1690, val loss: 0.36806347966194153
Epoch 1700, training loss: 83.53617095947266 = 0.2783280313014984 + 10.0 * 8.325784683227539
Epoch 1700, val loss: 0.36780279874801636
Epoch 1710, training loss: 83.48931884765625 = 0.27748894691467285 + 10.0 * 8.321183204650879
Epoch 1710, val loss: 0.3677981495857239
Epoch 1720, training loss: 83.4748764038086 = 0.2766626477241516 + 10.0 * 8.31982135772705
Epoch 1720, val loss: 0.36765551567077637
Epoch 1730, training loss: 83.46668243408203 = 0.27583280205726624 + 10.0 * 8.319085121154785
Epoch 1730, val loss: 0.3674781918525696
Epoch 1740, training loss: 83.46031188964844 = 0.2750067412853241 + 10.0 * 8.318530082702637
Epoch 1740, val loss: 0.367431104183197
Epoch 1750, training loss: 83.52062225341797 = 0.27417561411857605 + 10.0 * 8.324644088745117
Epoch 1750, val loss: 0.3673723042011261
Epoch 1760, training loss: 83.47651672363281 = 0.2733326852321625 + 10.0 * 8.320318222045898
Epoch 1760, val loss: 0.36709147691726685
Epoch 1770, training loss: 83.46958923339844 = 0.2724968194961548 + 10.0 * 8.319708824157715
Epoch 1770, val loss: 0.36715570092201233
Epoch 1780, training loss: 83.48049926757812 = 0.27165576815605164 + 10.0 * 8.320884704589844
Epoch 1780, val loss: 0.36702439188957214
Epoch 1790, training loss: 83.44647216796875 = 0.2708086669445038 + 10.0 * 8.317566871643066
Epoch 1790, val loss: 0.3666954040527344
Epoch 1800, training loss: 83.44446563720703 = 0.2699691653251648 + 10.0 * 8.317449569702148
Epoch 1800, val loss: 0.36666610836982727
Epoch 1810, training loss: 83.47087097167969 = 0.26913025975227356 + 10.0 * 8.320174217224121
Epoch 1810, val loss: 0.36656856536865234
Epoch 1820, training loss: 83.44279479980469 = 0.2682834565639496 + 10.0 * 8.317451477050781
Epoch 1820, val loss: 0.36648058891296387
Epoch 1830, training loss: 83.434814453125 = 0.2674408555030823 + 10.0 * 8.316737174987793
Epoch 1830, val loss: 0.36639440059661865
Epoch 1840, training loss: 83.42632293701172 = 0.26659393310546875 + 10.0 * 8.315973281860352
Epoch 1840, val loss: 0.366350382566452
Epoch 1850, training loss: 83.4259033203125 = 0.26574790477752686 + 10.0 * 8.316015243530273
Epoch 1850, val loss: 0.366267591714859
Epoch 1860, training loss: 83.45895385742188 = 0.26490363478660583 + 10.0 * 8.319405555725098
Epoch 1860, val loss: 0.36630693078041077
Epoch 1870, training loss: 83.43670654296875 = 0.2640405595302582 + 10.0 * 8.317266464233398
Epoch 1870, val loss: 0.3661510646343231
Epoch 1880, training loss: 83.46327209472656 = 0.26318642497062683 + 10.0 * 8.320009231567383
Epoch 1880, val loss: 0.3661399185657501
Epoch 1890, training loss: 83.41856384277344 = 0.26231735944747925 + 10.0 * 8.315625190734863
Epoch 1890, val loss: 0.36590659618377686
Epoch 1900, training loss: 83.40875244140625 = 0.26146435737609863 + 10.0 * 8.314728736877441
Epoch 1900, val loss: 0.3658989667892456
Epoch 1910, training loss: 83.39984130859375 = 0.2606055438518524 + 10.0 * 8.313923835754395
Epoch 1910, val loss: 0.365831196308136
Epoch 1920, training loss: 83.39617156982422 = 0.2597440481185913 + 10.0 * 8.313642501831055
Epoch 1920, val loss: 0.36580267548561096
Epoch 1930, training loss: 83.41497802734375 = 0.2588788866996765 + 10.0 * 8.3156099319458
Epoch 1930, val loss: 0.3658100962638855
Epoch 1940, training loss: 83.41339111328125 = 0.25800973176956177 + 10.0 * 8.31553840637207
Epoch 1940, val loss: 0.3658003509044647
Epoch 1950, training loss: 83.40327453613281 = 0.2571341395378113 + 10.0 * 8.314614295959473
Epoch 1950, val loss: 0.36564597487449646
Epoch 1960, training loss: 83.39506530761719 = 0.25625982880592346 + 10.0 * 8.313880920410156
Epoch 1960, val loss: 0.3655284643173218
Epoch 1970, training loss: 83.3808822631836 = 0.25538957118988037 + 10.0 * 8.312549591064453
Epoch 1970, val loss: 0.3656320571899414
Epoch 1980, training loss: 83.38119506835938 = 0.25452134013175964 + 10.0 * 8.312666893005371
Epoch 1980, val loss: 0.36571311950683594
Epoch 1990, training loss: 83.45693969726562 = 0.25365009903907776 + 10.0 * 8.320329666137695
Epoch 1990, val loss: 0.3657245934009552
Epoch 2000, training loss: 83.39154815673828 = 0.2527671158313751 + 10.0 * 8.313878059387207
Epoch 2000, val loss: 0.3655868172645569
Epoch 2010, training loss: 83.36814880371094 = 0.2518937587738037 + 10.0 * 8.311625480651855
Epoch 2010, val loss: 0.3656759560108185
Epoch 2020, training loss: 83.36308288574219 = 0.2510165870189667 + 10.0 * 8.311206817626953
Epoch 2020, val loss: 0.36567291617393494
Epoch 2030, training loss: 83.3652114868164 = 0.2501424551010132 + 10.0 * 8.311506271362305
Epoch 2030, val loss: 0.3656836450099945
Epoch 2040, training loss: 83.40934753417969 = 0.24926848709583282 + 10.0 * 8.316007614135742
Epoch 2040, val loss: 0.3657442629337311
Epoch 2050, training loss: 83.39246368408203 = 0.24837882816791534 + 10.0 * 8.314408302307129
Epoch 2050, val loss: 0.36575984954833984
Epoch 2060, training loss: 83.36731719970703 = 0.24749477207660675 + 10.0 * 8.311982154846191
Epoch 2060, val loss: 0.3657049834728241
Epoch 2070, training loss: 83.35253143310547 = 0.24661196768283844 + 10.0 * 8.310591697692871
Epoch 2070, val loss: 0.36580172181129456
Epoch 2080, training loss: 83.4322280883789 = 0.2457462102174759 + 10.0 * 8.318648338317871
Epoch 2080, val loss: 0.36613842844963074
Epoch 2090, training loss: 83.36823272705078 = 0.24483539164066315 + 10.0 * 8.312339782714844
Epoch 2090, val loss: 0.3657549321651459
Epoch 2100, training loss: 83.35131072998047 = 0.24394717812538147 + 10.0 * 8.310735702514648
Epoch 2100, val loss: 0.3660106658935547
Epoch 2110, training loss: 83.33623504638672 = 0.24305541813373566 + 10.0 * 8.309317588806152
Epoch 2110, val loss: 0.36592331528663635
Epoch 2120, training loss: 83.33061981201172 = 0.2421664297580719 + 10.0 * 8.308845520019531
Epoch 2120, val loss: 0.36607250571250916
Epoch 2130, training loss: 83.33464050292969 = 0.24127298593521118 + 10.0 * 8.30933666229248
Epoch 2130, val loss: 0.36610573530197144
Epoch 2140, training loss: 83.38673400878906 = 0.24038229882717133 + 10.0 * 8.314635276794434
Epoch 2140, val loss: 0.3662988245487213
Epoch 2150, training loss: 83.34990692138672 = 0.23946382105350494 + 10.0 * 8.311044692993164
Epoch 2150, val loss: 0.36616605520248413
Epoch 2160, training loss: 83.36231231689453 = 0.23856152594089508 + 10.0 * 8.31237506866455
Epoch 2160, val loss: 0.366370290517807
Epoch 2170, training loss: 83.31939697265625 = 0.2376452386379242 + 10.0 * 8.308175086975098
Epoch 2170, val loss: 0.3663623034954071
Epoch 2180, training loss: 83.31619262695312 = 0.23673425614833832 + 10.0 * 8.30794620513916
Epoch 2180, val loss: 0.36646559834480286
Epoch 2190, training loss: 83.3109359741211 = 0.2358168065547943 + 10.0 * 8.307512283325195
Epoch 2190, val loss: 0.3666178286075592
Epoch 2200, training loss: 83.34388732910156 = 0.23489661514759064 + 10.0 * 8.31089973449707
Epoch 2200, val loss: 0.36671921610832214
Epoch 2210, training loss: 83.31735229492188 = 0.23396296799182892 + 10.0 * 8.30833911895752
Epoch 2210, val loss: 0.36680880188941956
Epoch 2220, training loss: 83.31173706054688 = 0.2330344021320343 + 10.0 * 8.307870864868164
Epoch 2220, val loss: 0.36703088879585266
Epoch 2230, training loss: 83.30683135986328 = 0.23209841549396515 + 10.0 * 8.307473182678223
Epoch 2230, val loss: 0.36711275577545166
Epoch 2240, training loss: 83.34058380126953 = 0.23116162419319153 + 10.0 * 8.310941696166992
Epoch 2240, val loss: 0.3672526776790619
Epoch 2250, training loss: 83.30317687988281 = 0.23021617531776428 + 10.0 * 8.307295799255371
Epoch 2250, val loss: 0.36721065640449524
Epoch 2260, training loss: 83.29137420654297 = 0.22927528619766235 + 10.0 * 8.306209564208984
Epoch 2260, val loss: 0.36737391352653503
Epoch 2270, training loss: 83.30500030517578 = 0.22833417356014252 + 10.0 * 8.307666778564453
Epoch 2270, val loss: 0.36751648783683777
Epoch 2280, training loss: 83.28984832763672 = 0.2273806780576706 + 10.0 * 8.306246757507324
Epoch 2280, val loss: 0.36766529083251953
Epoch 2290, training loss: 83.30034637451172 = 0.22642597556114197 + 10.0 * 8.307392120361328
Epoch 2290, val loss: 0.367677241563797
Epoch 2300, training loss: 83.29408264160156 = 0.22547078132629395 + 10.0 * 8.30686092376709
Epoch 2300, val loss: 0.36784887313842773
Epoch 2310, training loss: 83.34224700927734 = 0.22451069951057434 + 10.0 * 8.311773300170898
Epoch 2310, val loss: 0.3681342899799347
Epoch 2320, training loss: 83.28134155273438 = 0.22353984415531158 + 10.0 * 8.305780410766602
Epoch 2320, val loss: 0.36825230717658997
Epoch 2330, training loss: 83.28307342529297 = 0.2225777506828308 + 10.0 * 8.306049346923828
Epoch 2330, val loss: 0.368568480014801
Epoch 2340, training loss: 83.29644012451172 = 0.2216026782989502 + 10.0 * 8.307483673095703
Epoch 2340, val loss: 0.3686376214027405
Epoch 2350, training loss: 83.26525115966797 = 0.220621258020401 + 10.0 * 8.304463386535645
Epoch 2350, val loss: 0.3688108026981354
Epoch 2360, training loss: 83.26260375976562 = 0.21965067088603973 + 10.0 * 8.304295539855957
Epoch 2360, val loss: 0.368902325630188
Epoch 2370, training loss: 83.2639389038086 = 0.21867521107196808 + 10.0 * 8.304526329040527
Epoch 2370, val loss: 0.36908382177352905
Epoch 2380, training loss: 83.2975845336914 = 0.21770447492599487 + 10.0 * 8.307988166809082
Epoch 2380, val loss: 0.3693215250968933
Epoch 2390, training loss: 83.27144622802734 = 0.21672174334526062 + 10.0 * 8.305472373962402
Epoch 2390, val loss: 0.3697977364063263
Epoch 2400, training loss: 83.26554107666016 = 0.2157375067472458 + 10.0 * 8.304980278015137
Epoch 2400, val loss: 0.36999282240867615
Epoch 2410, training loss: 83.26242065429688 = 0.2147483378648758 + 10.0 * 8.304767608642578
Epoch 2410, val loss: 0.37019094824790955
Epoch 2420, training loss: 83.25025177001953 = 0.2137572467327118 + 10.0 * 8.303648948669434
Epoch 2420, val loss: 0.3703470528125763
Epoch 2430, training loss: 83.2491226196289 = 0.2127668410539627 + 10.0 * 8.303635597229004
Epoch 2430, val loss: 0.3705802857875824
Epoch 2440, training loss: 83.24397277832031 = 0.21177345514297485 + 10.0 * 8.30321979522705
Epoch 2440, val loss: 0.3709355294704437
Epoch 2450, training loss: 83.26927947998047 = 0.21078002452850342 + 10.0 * 8.3058500289917
Epoch 2450, val loss: 0.37119394540786743
Epoch 2460, training loss: 83.26376342773438 = 0.20978528261184692 + 10.0 * 8.305397987365723
Epoch 2460, val loss: 0.37150776386260986
Epoch 2470, training loss: 83.23876190185547 = 0.20878340303897858 + 10.0 * 8.302997589111328
Epoch 2470, val loss: 0.37177351117134094
Epoch 2480, training loss: 83.23057556152344 = 0.20779292285442352 + 10.0 * 8.302278518676758
Epoch 2480, val loss: 0.3720933198928833
Epoch 2490, training loss: 83.233154296875 = 0.20680081844329834 + 10.0 * 8.302635192871094
Epoch 2490, val loss: 0.3723958432674408
Epoch 2500, training loss: 83.2500228881836 = 0.20581001043319702 + 10.0 * 8.304421424865723
Epoch 2500, val loss: 0.37272441387176514
Epoch 2510, training loss: 83.2322769165039 = 0.20481519401073456 + 10.0 * 8.302745819091797
Epoch 2510, val loss: 0.37309926748275757
Epoch 2520, training loss: 83.25794982910156 = 0.2038251757621765 + 10.0 * 8.305412292480469
Epoch 2520, val loss: 0.3734101951122284
Epoch 2530, training loss: 83.23137664794922 = 0.20282651484012604 + 10.0 * 8.302854537963867
Epoch 2530, val loss: 0.3739384412765503
Epoch 2540, training loss: 83.22598266601562 = 0.20183752477169037 + 10.0 * 8.302414894104004
Epoch 2540, val loss: 0.37433505058288574
Epoch 2550, training loss: 83.24371337890625 = 0.20084506273269653 + 10.0 * 8.30428695678711
Epoch 2550, val loss: 0.37472856044769287
Epoch 2560, training loss: 83.21147155761719 = 0.19984520971775055 + 10.0 * 8.301162719726562
Epoch 2560, val loss: 0.37493443489074707
Epoch 2570, training loss: 83.20169830322266 = 0.19884774088859558 + 10.0 * 8.300285339355469
Epoch 2570, val loss: 0.3753832280635834
Epoch 2580, training loss: 83.20315551757812 = 0.19785062968730927 + 10.0 * 8.300530433654785
Epoch 2580, val loss: 0.37581631541252136
Epoch 2590, training loss: 83.2447509765625 = 0.19685505330562592 + 10.0 * 8.304789543151855
Epoch 2590, val loss: 0.37625670433044434
Epoch 2600, training loss: 83.25501251220703 = 0.19584956765174866 + 10.0 * 8.305916786193848
Epoch 2600, val loss: 0.37686336040496826
Epoch 2610, training loss: 83.230224609375 = 0.19483748078346252 + 10.0 * 8.30353832244873
Epoch 2610, val loss: 0.3769940435886383
Epoch 2620, training loss: 83.19734954833984 = 0.1938234567642212 + 10.0 * 8.300352096557617
Epoch 2620, val loss: 0.37746569514274597
Epoch 2630, training loss: 83.18791961669922 = 0.19281922280788422 + 10.0 * 8.29951000213623
Epoch 2630, val loss: 0.3778674006462097
Epoch 2640, training loss: 83.18923950195312 = 0.19180873036384583 + 10.0 * 8.299742698669434
Epoch 2640, val loss: 0.3781961500644684
Epoch 2650, training loss: 83.23794555664062 = 0.190806582570076 + 10.0 * 8.30471420288086
Epoch 2650, val loss: 0.37846639752388
Epoch 2660, training loss: 83.18130493164062 = 0.18977688252925873 + 10.0 * 8.299153327941895
Epoch 2660, val loss: 0.37929999828338623
Epoch 2670, training loss: 83.17842102050781 = 0.18875989317893982 + 10.0 * 8.298966407775879
Epoch 2670, val loss: 0.3796583414077759
Epoch 2680, training loss: 83.20228576660156 = 0.18774212896823883 + 10.0 * 8.301454544067383
Epoch 2680, val loss: 0.38009390234947205
Epoch 2690, training loss: 83.20097351074219 = 0.18671832978725433 + 10.0 * 8.301424980163574
Epoch 2690, val loss: 0.3807283639907837
Epoch 2700, training loss: 83.19197845458984 = 0.18569572269916534 + 10.0 * 8.300627708435059
Epoch 2700, val loss: 0.38114091753959656
Epoch 2710, training loss: 83.20106506347656 = 0.18467460572719574 + 10.0 * 8.30163860321045
Epoch 2710, val loss: 0.38170769810676575
Epoch 2720, training loss: 83.1766128540039 = 0.18364037573337555 + 10.0 * 8.299297332763672
Epoch 2720, val loss: 0.3820805847644806
Epoch 2730, training loss: 83.16767883300781 = 0.1826130598783493 + 10.0 * 8.298506736755371
Epoch 2730, val loss: 0.3826908469200134
Epoch 2740, training loss: 83.1587142944336 = 0.18158406019210815 + 10.0 * 8.297712326049805
Epoch 2740, val loss: 0.3831886351108551
Epoch 2750, training loss: 83.15877532958984 = 0.180555060505867 + 10.0 * 8.297821998596191
Epoch 2750, val loss: 0.3837378919124603
Epoch 2760, training loss: 83.17086029052734 = 0.17953073978424072 + 10.0 * 8.29913330078125
Epoch 2760, val loss: 0.3844108283519745
Epoch 2770, training loss: 83.1982192993164 = 0.1785220354795456 + 10.0 * 8.301969528198242
Epoch 2770, val loss: 0.3851218521595001
Epoch 2780, training loss: 83.17620086669922 = 0.17746484279632568 + 10.0 * 8.299873352050781
Epoch 2780, val loss: 0.3853199779987335
Epoch 2790, training loss: 83.15022277832031 = 0.17643025517463684 + 10.0 * 8.297379493713379
Epoch 2790, val loss: 0.3860631585121155
Epoch 2800, training loss: 83.14888000488281 = 0.17540137469768524 + 10.0 * 8.297348022460938
Epoch 2800, val loss: 0.38651108741760254
Epoch 2810, training loss: 83.148681640625 = 0.17437472939491272 + 10.0 * 8.297430038452148
Epoch 2810, val loss: 0.387177973985672
Epoch 2820, training loss: 83.18191528320312 = 0.17334824800491333 + 10.0 * 8.300856590270996
Epoch 2820, val loss: 0.3877338171005249
Epoch 2830, training loss: 83.14883422851562 = 0.1723157912492752 + 10.0 * 8.297651290893555
Epoch 2830, val loss: 0.38854965567588806
Epoch 2840, training loss: 83.14521789550781 = 0.17128628492355347 + 10.0 * 8.297392845153809
Epoch 2840, val loss: 0.38910961151123047
Epoch 2850, training loss: 83.13858032226562 = 0.170252725481987 + 10.0 * 8.296833038330078
Epoch 2850, val loss: 0.38970425724983215
Epoch 2860, training loss: 83.15261840820312 = 0.16922466456890106 + 10.0 * 8.298338890075684
Epoch 2860, val loss: 0.3903367817401886
Epoch 2870, training loss: 83.15117645263672 = 0.1681908816099167 + 10.0 * 8.298298835754395
Epoch 2870, val loss: 0.39094626903533936
Epoch 2880, training loss: 83.14408874511719 = 0.16716749966144562 + 10.0 * 8.29769229888916
Epoch 2880, val loss: 0.39139899611473083
Epoch 2890, training loss: 83.12981414794922 = 0.1661432683467865 + 10.0 * 8.296366691589355
Epoch 2890, val loss: 0.3924233019351959
Epoch 2900, training loss: 83.1280288696289 = 0.16511735320091248 + 10.0 * 8.29629135131836
Epoch 2900, val loss: 0.39305025339126587
Epoch 2910, training loss: 83.2034912109375 = 0.1641027182340622 + 10.0 * 8.303938865661621
Epoch 2910, val loss: 0.39365577697753906
Epoch 2920, training loss: 83.14739990234375 = 0.16307461261749268 + 10.0 * 8.298432350158691
Epoch 2920, val loss: 0.39475613832473755
Epoch 2930, training loss: 83.12187194824219 = 0.16204585134983063 + 10.0 * 8.295982360839844
Epoch 2930, val loss: 0.3951236307621002
Epoch 2940, training loss: 83.11431121826172 = 0.16101676225662231 + 10.0 * 8.295329093933105
Epoch 2940, val loss: 0.39591094851493835
Epoch 2950, training loss: 83.11782836914062 = 0.15999864041805267 + 10.0 * 8.295783042907715
Epoch 2950, val loss: 0.3966732919216156
Epoch 2960, training loss: 83.15961456298828 = 0.15896837413311005 + 10.0 * 8.300065040588379
Epoch 2960, val loss: 0.3972366154193878
Epoch 2970, training loss: 83.11668395996094 = 0.15793584287166595 + 10.0 * 8.29587459564209
Epoch 2970, val loss: 0.398275762796402
Epoch 2980, training loss: 83.14209747314453 = 0.1569153517484665 + 10.0 * 8.298518180847168
Epoch 2980, val loss: 0.3989330232143402
Epoch 2990, training loss: 83.10710144042969 = 0.15588101744651794 + 10.0 * 8.295122146606445
Epoch 2990, val loss: 0.3998869061470032
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8564180618975139
0.8674201260595523
The final CL Acc:0.84932, 0.00580, The final GNN Acc:0.86657, 0.00086
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=3, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 97530])
remove edge: torch.Size([2, 79604])
updated graph: torch.Size([2, 88486])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 106.90630340576172 = 1.0835644006729126 + 10.0 * 10.582273483276367
Epoch 0, val loss: 1.083040714263916
Epoch 10, training loss: 106.891357421875 = 1.0780328512191772 + 10.0 * 10.581332206726074
Epoch 10, val loss: 1.077515721321106
Epoch 20, training loss: 106.79100036621094 = 1.0718356370925903 + 10.0 * 10.571916580200195
Epoch 20, val loss: 1.0712974071502686
Epoch 30, training loss: 106.05472564697266 = 1.0649380683898926 + 10.0 * 10.498978614807129
Epoch 30, val loss: 1.0643749237060547
Epoch 40, training loss: 102.91294860839844 = 1.0577045679092407 + 10.0 * 10.185524940490723
Epoch 40, val loss: 1.0571821928024292
Epoch 50, training loss: 98.222412109375 = 1.0503590106964111 + 10.0 * 9.717205047607422
Epoch 50, val loss: 1.049857497215271
Epoch 60, training loss: 95.25045776367188 = 1.042189598083496 + 10.0 * 9.42082691192627
Epoch 60, val loss: 1.041953682899475
Epoch 70, training loss: 93.68576049804688 = 1.034401774406433 + 10.0 * 9.265135765075684
Epoch 70, val loss: 1.0347012281417847
Epoch 80, training loss: 92.89363861083984 = 1.0282456874847412 + 10.0 * 9.186539649963379
Epoch 80, val loss: 1.0289167165756226
Epoch 90, training loss: 92.24784088134766 = 1.022708773612976 + 10.0 * 9.122513771057129
Epoch 90, val loss: 1.0235600471496582
Epoch 100, training loss: 91.16319274902344 = 1.0176076889038086 + 10.0 * 9.014558792114258
Epoch 100, val loss: 1.0186889171600342
Epoch 110, training loss: 89.75855255126953 = 1.0138328075408936 + 10.0 * 8.874471664428711
Epoch 110, val loss: 1.015256404876709
Epoch 120, training loss: 89.20579528808594 = 1.0099908113479614 + 10.0 * 8.819580078125
Epoch 120, val loss: 1.0113987922668457
Epoch 130, training loss: 88.78009033203125 = 1.0030021667480469 + 10.0 * 8.777708053588867
Epoch 130, val loss: 1.0044556856155396
Epoch 140, training loss: 88.46174621582031 = 0.9943526387214661 + 10.0 * 8.746739387512207
Epoch 140, val loss: 0.996185839176178
Epoch 150, training loss: 88.1907958984375 = 0.985454797744751 + 10.0 * 8.72053337097168
Epoch 150, val loss: 0.9877274036407471
Epoch 160, training loss: 87.91027069091797 = 0.9762207269668579 + 10.0 * 8.693405151367188
Epoch 160, val loss: 0.9789392352104187
Epoch 170, training loss: 87.6648178100586 = 0.9662821292877197 + 10.0 * 8.669853210449219
Epoch 170, val loss: 0.9695277810096741
Epoch 180, training loss: 87.41378784179688 = 0.9553958177566528 + 10.0 * 8.645838737487793
Epoch 180, val loss: 0.9591739773750305
Epoch 190, training loss: 87.22311401367188 = 0.9432539343833923 + 10.0 * 8.627985954284668
Epoch 190, val loss: 0.9476069808006287
Epoch 200, training loss: 87.0520248413086 = 0.9297866225242615 + 10.0 * 8.612223625183105
Epoch 200, val loss: 0.9349066615104675
Epoch 210, training loss: 86.88253021240234 = 0.9155147671699524 + 10.0 * 8.596701622009277
Epoch 210, val loss: 0.9215148091316223
Epoch 220, training loss: 86.84562683105469 = 0.9004509449005127 + 10.0 * 8.594517707824707
Epoch 220, val loss: 0.9075167775154114
Epoch 230, training loss: 86.5938491821289 = 0.8844612240791321 + 10.0 * 8.570939064025879
Epoch 230, val loss: 0.8925188183784485
Epoch 240, training loss: 86.45357513427734 = 0.8679426312446594 + 10.0 * 8.558563232421875
Epoch 240, val loss: 0.8771910667419434
Epoch 250, training loss: 86.31069946289062 = 0.8508330583572388 + 10.0 * 8.545987129211426
Epoch 250, val loss: 0.8613829016685486
Epoch 260, training loss: 86.19403076171875 = 0.8330986499786377 + 10.0 * 8.536092758178711
Epoch 260, val loss: 0.8450050354003906
Epoch 270, training loss: 86.08098602294922 = 0.8150098323822021 + 10.0 * 8.52659797668457
Epoch 270, val loss: 0.8284257054328918
Epoch 280, training loss: 85.96923065185547 = 0.7968828678131104 + 10.0 * 8.517234802246094
Epoch 280, val loss: 0.8119040131568909
Epoch 290, training loss: 85.89218139648438 = 0.7786095142364502 + 10.0 * 8.511357307434082
Epoch 290, val loss: 0.7952677011489868
Epoch 300, training loss: 85.78431701660156 = 0.7604739665985107 + 10.0 * 8.502384185791016
Epoch 300, val loss: 0.7789983153343201
Epoch 310, training loss: 85.6869888305664 = 0.7428747415542603 + 10.0 * 8.49441146850586
Epoch 310, val loss: 0.7633228898048401
Epoch 320, training loss: 85.61286926269531 = 0.7257787585258484 + 10.0 * 8.488709449768066
Epoch 320, val loss: 0.748177170753479
Epoch 330, training loss: 85.54611206054688 = 0.7091972231864929 + 10.0 * 8.483691215515137
Epoch 330, val loss: 0.7335712313652039
Epoch 340, training loss: 85.4549331665039 = 0.693381667137146 + 10.0 * 8.476155281066895
Epoch 340, val loss: 0.7198774814605713
Epoch 350, training loss: 85.3897705078125 = 0.6784665584564209 + 10.0 * 8.47113037109375
Epoch 350, val loss: 0.7069879174232483
Epoch 360, training loss: 85.38429260253906 = 0.6642747521400452 + 10.0 * 8.472002029418945
Epoch 360, val loss: 0.6948604583740234
Epoch 370, training loss: 85.27916717529297 = 0.6507323980331421 + 10.0 * 8.46284294128418
Epoch 370, val loss: 0.6833469867706299
Epoch 380, training loss: 85.22274017333984 = 0.6381024718284607 + 10.0 * 8.458463668823242
Epoch 380, val loss: 0.67281574010849
Epoch 390, training loss: 85.17570495605469 = 0.6262505054473877 + 10.0 * 8.45494556427002
Epoch 390, val loss: 0.6628603339195251
Epoch 400, training loss: 85.1179428100586 = 0.6150286197662354 + 10.0 * 8.450291633605957
Epoch 400, val loss: 0.6536179184913635
Epoch 410, training loss: 85.08147430419922 = 0.6045610308647156 + 10.0 * 8.447690963745117
Epoch 410, val loss: 0.6451076865196228
Epoch 420, training loss: 85.03179168701172 = 0.5947754383087158 + 10.0 * 8.44370174407959
Epoch 420, val loss: 0.637127161026001
Epoch 430, training loss: 85.07349395751953 = 0.5855181217193604 + 10.0 * 8.448797225952148
Epoch 430, val loss: 0.6297034025192261
Epoch 440, training loss: 84.96846008300781 = 0.5767450928688049 + 10.0 * 8.43917179107666
Epoch 440, val loss: 0.6226547360420227
Epoch 450, training loss: 84.92135620117188 = 0.5686002373695374 + 10.0 * 8.435275077819824
Epoch 450, val loss: 0.6162930727005005
Epoch 460, training loss: 84.88870239257812 = 0.5609222054481506 + 10.0 * 8.432778358459473
Epoch 460, val loss: 0.6102569103240967
Epoch 470, training loss: 84.92243194580078 = 0.5535999536514282 + 10.0 * 8.436883926391602
Epoch 470, val loss: 0.6045870780944824
Epoch 480, training loss: 84.83473205566406 = 0.5466452240943909 + 10.0 * 8.428808212280273
Epoch 480, val loss: 0.5991813540458679
Epoch 490, training loss: 84.79615020751953 = 0.5401641726493835 + 10.0 * 8.425599098205566
Epoch 490, val loss: 0.5942490696907043
Epoch 500, training loss: 84.77294921875 = 0.5340123772621155 + 10.0 * 8.423893928527832
Epoch 500, val loss: 0.5896210670471191
Epoch 510, training loss: 84.85060119628906 = 0.528136134147644 + 10.0 * 8.432246208190918
Epoch 510, val loss: 0.5852190256118774
Epoch 520, training loss: 84.73748016357422 = 0.5224965810775757 + 10.0 * 8.42149829864502
Epoch 520, val loss: 0.5809783339500427
Epoch 530, training loss: 84.70222473144531 = 0.517227292060852 + 10.0 * 8.418499946594238
Epoch 530, val loss: 0.5770574808120728
Epoch 540, training loss: 84.68367004394531 = 0.5121898055076599 + 10.0 * 8.417147636413574
Epoch 540, val loss: 0.5733928680419922
Epoch 550, training loss: 84.71125793457031 = 0.5073574185371399 + 10.0 * 8.420390129089355
Epoch 550, val loss: 0.569952130317688
Epoch 560, training loss: 84.6781234741211 = 0.5026140213012695 + 10.0 * 8.417551040649414
Epoch 560, val loss: 0.5664029717445374
Epoch 570, training loss: 84.62751770019531 = 0.49814027547836304 + 10.0 * 8.412938117980957
Epoch 570, val loss: 0.5632349252700806
Epoch 580, training loss: 84.62349700927734 = 0.4938397705554962 + 10.0 * 8.412965774536133
Epoch 580, val loss: 0.560091495513916
Epoch 590, training loss: 84.60598754882812 = 0.4896400570869446 + 10.0 * 8.41163444519043
Epoch 590, val loss: 0.5572052597999573
Epoch 600, training loss: 84.60021209716797 = 0.48560845851898193 + 10.0 * 8.411459922790527
Epoch 600, val loss: 0.55428546667099
Epoch 610, training loss: 84.56298065185547 = 0.48174601793289185 + 10.0 * 8.408123970031738
Epoch 610, val loss: 0.5515732765197754
Epoch 620, training loss: 84.55526733398438 = 0.4780089855194092 + 10.0 * 8.40772533416748
Epoch 620, val loss: 0.5490373373031616
Epoch 630, training loss: 84.55522918701172 = 0.4743252396583557 + 10.0 * 8.408090591430664
Epoch 630, val loss: 0.5464295744895935
Epoch 640, training loss: 84.58287811279297 = 0.4707198441028595 + 10.0 * 8.411215782165527
Epoch 640, val loss: 0.5439532399177551
Epoch 650, training loss: 84.51238250732422 = 0.4672374129295349 + 10.0 * 8.40451431274414
Epoch 650, val loss: 0.5416379570960999
Epoch 660, training loss: 84.4847640991211 = 0.4638582170009613 + 10.0 * 8.402090072631836
Epoch 660, val loss: 0.539247989654541
Epoch 670, training loss: 84.47364044189453 = 0.4605512022972107 + 10.0 * 8.4013090133667
Epoch 670, val loss: 0.5370297431945801
Epoch 680, training loss: 84.54495239257812 = 0.45727935433387756 + 10.0 * 8.408766746520996
Epoch 680, val loss: 0.5346999764442444
Epoch 690, training loss: 84.47736358642578 = 0.4540514349937439 + 10.0 * 8.402331352233887
Epoch 690, val loss: 0.5327179431915283
Epoch 700, training loss: 84.43831634521484 = 0.45093804597854614 + 10.0 * 8.398737907409668
Epoch 700, val loss: 0.5304589867591858
Epoch 710, training loss: 84.42713165283203 = 0.4478790760040283 + 10.0 * 8.39792537689209
Epoch 710, val loss: 0.5284575819969177
Epoch 720, training loss: 84.42919921875 = 0.4448348581790924 + 10.0 * 8.398436546325684
Epoch 720, val loss: 0.5265151858329773
Epoch 730, training loss: 84.40707397460938 = 0.4418626129627228 + 10.0 * 8.396520614624023
Epoch 730, val loss: 0.5244244337081909
Epoch 740, training loss: 84.38292694091797 = 0.43897849321365356 + 10.0 * 8.394394874572754
Epoch 740, val loss: 0.5225372314453125
Epoch 750, training loss: 84.46922302246094 = 0.43610623478889465 + 10.0 * 8.403311729431152
Epoch 750, val loss: 0.520623505115509
Epoch 760, training loss: 84.37044525146484 = 0.4332098066806793 + 10.0 * 8.393723487854004
Epoch 760, val loss: 0.5188092589378357
Epoch 770, training loss: 84.34336853027344 = 0.4304368197917938 + 10.0 * 8.3912935256958
Epoch 770, val loss: 0.51695317029953
Epoch 780, training loss: 84.33377838134766 = 0.4276919662952423 + 10.0 * 8.390608787536621
Epoch 780, val loss: 0.5151652097702026
Epoch 790, training loss: 84.38346862792969 = 0.42497116327285767 + 10.0 * 8.395849227905273
Epoch 790, val loss: 0.5134211182594299
Epoch 800, training loss: 84.32754516601562 = 0.4222542941570282 + 10.0 * 8.390528678894043
Epoch 800, val loss: 0.5118892788887024
Epoch 810, training loss: 84.30210876464844 = 0.41959208250045776 + 10.0 * 8.388251304626465
Epoch 810, val loss: 0.5100798010826111
Epoch 820, training loss: 84.28495025634766 = 0.41699057817459106 + 10.0 * 8.386795997619629
Epoch 820, val loss: 0.508551299571991
Epoch 830, training loss: 84.27884674072266 = 0.4144032299518585 + 10.0 * 8.386444091796875
Epoch 830, val loss: 0.5068485140800476
Epoch 840, training loss: 84.31456756591797 = 0.4118116497993469 + 10.0 * 8.390275955200195
Epoch 840, val loss: 0.5053006410598755
Epoch 850, training loss: 84.27320098876953 = 0.4092138111591339 + 10.0 * 8.386399269104004
Epoch 850, val loss: 0.5037100911140442
Epoch 860, training loss: 84.24908447265625 = 0.40668994188308716 + 10.0 * 8.384239196777344
Epoch 860, val loss: 0.5022791624069214
Epoch 870, training loss: 84.22711181640625 = 0.4042176604270935 + 10.0 * 8.382288932800293
Epoch 870, val loss: 0.5008946061134338
Epoch 880, training loss: 84.21709442138672 = 0.4017793536186218 + 10.0 * 8.381531715393066
Epoch 880, val loss: 0.4994373917579651
Epoch 890, training loss: 84.34326934814453 = 0.39934009313583374 + 10.0 * 8.394392967224121
Epoch 890, val loss: 0.49808189272880554
Epoch 900, training loss: 84.25305938720703 = 0.396840900182724 + 10.0 * 8.385622024536133
Epoch 900, val loss: 0.49661070108413696
Epoch 910, training loss: 84.2092514038086 = 0.39446789026260376 + 10.0 * 8.381478309631348
Epoch 910, val loss: 0.49513646960258484
Epoch 920, training loss: 84.17564392089844 = 0.39214906096458435 + 10.0 * 8.378349304199219
Epoch 920, val loss: 0.49400097131729126
Epoch 930, training loss: 84.1684341430664 = 0.3898546099662781 + 10.0 * 8.37785816192627
Epoch 930, val loss: 0.49276304244995117
Epoch 940, training loss: 84.19252014160156 = 0.3875564932823181 + 10.0 * 8.38049602508545
Epoch 940, val loss: 0.4915287494659424
Epoch 950, training loss: 84.14604187011719 = 0.3852367103099823 + 10.0 * 8.376080513000488
Epoch 950, val loss: 0.49033957719802856
Epoch 960, training loss: 84.13934326171875 = 0.38297519087791443 + 10.0 * 8.37563705444336
Epoch 960, val loss: 0.4892294108867645
Epoch 970, training loss: 84.12995147705078 = 0.38074368238449097 + 10.0 * 8.374920845031738
Epoch 970, val loss: 0.48817065358161926
Epoch 980, training loss: 84.16960906982422 = 0.3785298466682434 + 10.0 * 8.379107475280762
Epoch 980, val loss: 0.4872426688671112
Epoch 990, training loss: 84.12680053710938 = 0.3762783706188202 + 10.0 * 8.375051498413086
Epoch 990, val loss: 0.48596659302711487
Epoch 1000, training loss: 84.11322784423828 = 0.3741096556186676 + 10.0 * 8.37391185760498
Epoch 1000, val loss: 0.4851618707180023
Epoch 1010, training loss: 84.09388732910156 = 0.37195587158203125 + 10.0 * 8.372193336486816
Epoch 1010, val loss: 0.48413172364234924
Epoch 1020, training loss: 84.0987777709961 = 0.36980491876602173 + 10.0 * 8.372897148132324
Epoch 1020, val loss: 0.48326799273490906
Epoch 1030, training loss: 84.11125946044922 = 0.3676317632198334 + 10.0 * 8.37436294555664
Epoch 1030, val loss: 0.4824308753013611
Epoch 1040, training loss: 84.08319091796875 = 0.3654567301273346 + 10.0 * 8.371773719787598
Epoch 1040, val loss: 0.48147207498550415
Epoch 1050, training loss: 84.06405639648438 = 0.36334311962127686 + 10.0 * 8.370071411132812
Epoch 1050, val loss: 0.480724960565567
Epoch 1060, training loss: 84.04839324951172 = 0.36126160621643066 + 10.0 * 8.36871337890625
Epoch 1060, val loss: 0.480100154876709
Epoch 1070, training loss: 84.05170440673828 = 0.3591969311237335 + 10.0 * 8.369250297546387
Epoch 1070, val loss: 0.47933921217918396
Epoch 1080, training loss: 84.07606506347656 = 0.3571147918701172 + 10.0 * 8.371894836425781
Epoch 1080, val loss: 0.4787195920944214
Epoch 1090, training loss: 84.0610580444336 = 0.355033814907074 + 10.0 * 8.37060260772705
Epoch 1090, val loss: 0.4781899154186249
Epoch 1100, training loss: 84.0181884765625 = 0.3529849946498871 + 10.0 * 8.366520881652832
Epoch 1100, val loss: 0.47763073444366455
Epoch 1110, training loss: 84.01669311523438 = 0.3510013520717621 + 10.0 * 8.366569519042969
Epoch 1110, val loss: 0.47710147500038147
Epoch 1120, training loss: 84.0279312133789 = 0.3490433990955353 + 10.0 * 8.367888450622559
Epoch 1120, val loss: 0.47669243812561035
Epoch 1130, training loss: 84.01161193847656 = 0.3470973074436188 + 10.0 * 8.366451263427734
Epoch 1130, val loss: 0.47639063000679016
Epoch 1140, training loss: 84.02131652832031 = 0.34516704082489014 + 10.0 * 8.36761474609375
Epoch 1140, val loss: 0.4760827422142029
Epoch 1150, training loss: 83.9967269897461 = 0.3432331085205078 + 10.0 * 8.365349769592285
Epoch 1150, val loss: 0.4755873680114746
Epoch 1160, training loss: 83.97930145263672 = 0.34133920073509216 + 10.0 * 8.36379623413086
Epoch 1160, val loss: 0.4755147099494934
Epoch 1170, training loss: 83.99324035644531 = 0.3394611179828644 + 10.0 * 8.365377426147461
Epoch 1170, val loss: 0.475183367729187
Epoch 1180, training loss: 84.01847839355469 = 0.33759164810180664 + 10.0 * 8.368088722229004
Epoch 1180, val loss: 0.4749465584754944
Epoch 1190, training loss: 83.97797393798828 = 0.3357325792312622 + 10.0 * 8.364224433898926
Epoch 1190, val loss: 0.4747951626777649
Epoch 1200, training loss: 83.94833374023438 = 0.3339117467403412 + 10.0 * 8.361442565917969
Epoch 1200, val loss: 0.4745677411556244
Epoch 1210, training loss: 83.93822479248047 = 0.3321313261985779 + 10.0 * 8.36060905456543
Epoch 1210, val loss: 0.47454512119293213
Epoch 1220, training loss: 83.97260284423828 = 0.33035680651664734 + 10.0 * 8.364224433898926
Epoch 1220, val loss: 0.4744996130466461
Epoch 1230, training loss: 83.96452331542969 = 0.32855579257011414 + 10.0 * 8.36359691619873
Epoch 1230, val loss: 0.4744337797164917
Epoch 1240, training loss: 83.93059539794922 = 0.32678160071372986 + 10.0 * 8.360381126403809
Epoch 1240, val loss: 0.47448307275772095
Epoch 1250, training loss: 83.91764831542969 = 0.3250354528427124 + 10.0 * 8.359261512756348
Epoch 1250, val loss: 0.47446152567863464
Epoch 1260, training loss: 83.91307067871094 = 0.32331496477127075 + 10.0 * 8.358975410461426
Epoch 1260, val loss: 0.47463878989219666
Epoch 1270, training loss: 83.97441101074219 = 0.32159122824668884 + 10.0 * 8.36528205871582
Epoch 1270, val loss: 0.4746053218841553
Epoch 1280, training loss: 83.92253875732422 = 0.31986597180366516 + 10.0 * 8.360267639160156
Epoch 1280, val loss: 0.4748453199863434
Epoch 1290, training loss: 83.89531707763672 = 0.3181672990322113 + 10.0 * 8.357714653015137
Epoch 1290, val loss: 0.474791556596756
Epoch 1300, training loss: 83.8880844116211 = 0.31648877263069153 + 10.0 * 8.357159614562988
Epoch 1300, val loss: 0.474894255399704
Epoch 1310, training loss: 83.90900421142578 = 0.31481292843818665 + 10.0 * 8.359418869018555
Epoch 1310, val loss: 0.4749733805656433
Epoch 1320, training loss: 83.8880844116211 = 0.3131423890590668 + 10.0 * 8.357494354248047
Epoch 1320, val loss: 0.4751986563205719
Epoch 1330, training loss: 83.87641143798828 = 0.31148242950439453 + 10.0 * 8.35649299621582
Epoch 1330, val loss: 0.4753161072731018
Epoch 1340, training loss: 83.86756134033203 = 0.3098504841327667 + 10.0 * 8.3557710647583
Epoch 1340, val loss: 0.47546708583831787
Epoch 1350, training loss: 83.89788055419922 = 0.30823710560798645 + 10.0 * 8.358964920043945
Epoch 1350, val loss: 0.4756745994091034
Epoch 1360, training loss: 83.87680053710938 = 0.3066371977329254 + 10.0 * 8.357015609741211
Epoch 1360, val loss: 0.4760136604309082
Epoch 1370, training loss: 83.88001251220703 = 0.30504125356674194 + 10.0 * 8.357497215270996
Epoch 1370, val loss: 0.47617781162261963
Epoch 1380, training loss: 83.85598754882812 = 0.3034552037715912 + 10.0 * 8.355253219604492
Epoch 1380, val loss: 0.4763728380203247
Epoch 1390, training loss: 83.854736328125 = 0.3018752932548523 + 10.0 * 8.355286598205566
Epoch 1390, val loss: 0.4766257405281067
Epoch 1400, training loss: 83.8390884399414 = 0.30031391978263855 + 10.0 * 8.353877067565918
Epoch 1400, val loss: 0.4769953191280365
Epoch 1410, training loss: 83.84091186523438 = 0.2987552285194397 + 10.0 * 8.354215621948242
Epoch 1410, val loss: 0.47721007466316223
Epoch 1420, training loss: 83.90436553955078 = 0.29720473289489746 + 10.0 * 8.360715866088867
Epoch 1420, val loss: 0.47745901346206665
Epoch 1430, training loss: 83.84056854248047 = 0.29565101861953735 + 10.0 * 8.354491233825684
Epoch 1430, val loss: 0.4777805805206299
Epoch 1440, training loss: 83.81810760498047 = 0.29412132501602173 + 10.0 * 8.352398872375488
Epoch 1440, val loss: 0.47817525267601013
Epoch 1450, training loss: 83.80581665039062 = 0.292616069316864 + 10.0 * 8.351320266723633
Epoch 1450, val loss: 0.47846662998199463
Epoch 1460, training loss: 83.80426788330078 = 0.2911211848258972 + 10.0 * 8.351314544677734
Epoch 1460, val loss: 0.47889262437820435
Epoch 1470, training loss: 83.87388610839844 = 0.2896290421485901 + 10.0 * 8.358426094055176
Epoch 1470, val loss: 0.4791710376739502
Epoch 1480, training loss: 83.8260726928711 = 0.2881053686141968 + 10.0 * 8.353796005249023
Epoch 1480, val loss: 0.47956952452659607
Epoch 1490, training loss: 83.80880737304688 = 0.2866115868091583 + 10.0 * 8.352219581604004
Epoch 1490, val loss: 0.4801369309425354
Epoch 1500, training loss: 83.7833480834961 = 0.28511354327201843 + 10.0 * 8.349823951721191
Epoch 1500, val loss: 0.48050275444984436
Epoch 1510, training loss: 83.77568054199219 = 0.2836265563964844 + 10.0 * 8.349205017089844
Epoch 1510, val loss: 0.48113906383514404
Epoch 1520, training loss: 83.80397033691406 = 0.2821383476257324 + 10.0 * 8.35218334197998
Epoch 1520, val loss: 0.4817054271697998
Epoch 1530, training loss: 83.80470275878906 = 0.28065481781959534 + 10.0 * 8.352404594421387
Epoch 1530, val loss: 0.48243722319602966
Epoch 1540, training loss: 83.78163146972656 = 0.27917349338531494 + 10.0 * 8.350245475769043
Epoch 1540, val loss: 0.4829767942428589
Epoch 1550, training loss: 83.76314544677734 = 0.2777051329612732 + 10.0 * 8.348544120788574
Epoch 1550, val loss: 0.4835372567176819
Epoch 1560, training loss: 83.77743530273438 = 0.2762484848499298 + 10.0 * 8.350118637084961
Epoch 1560, val loss: 0.48426952958106995
Epoch 1570, training loss: 83.76692962646484 = 0.2747745215892792 + 10.0 * 8.349215507507324
Epoch 1570, val loss: 0.4848732054233551
Epoch 1580, training loss: 83.75103759765625 = 0.2732830345630646 + 10.0 * 8.34777545928955
Epoch 1580, val loss: 0.48557451367378235
Epoch 1590, training loss: 83.73863220214844 = 0.27181825041770935 + 10.0 * 8.346681594848633
Epoch 1590, val loss: 0.486255943775177
Epoch 1600, training loss: 83.75418090820312 = 0.27035242319107056 + 10.0 * 8.348382949829102
Epoch 1600, val loss: 0.48705950379371643
Epoch 1610, training loss: 83.75039672851562 = 0.26888978481292725 + 10.0 * 8.348150253295898
Epoch 1610, val loss: 0.48787397146224976
Epoch 1620, training loss: 83.73641204833984 = 0.2674349546432495 + 10.0 * 8.346898078918457
Epoch 1620, val loss: 0.4885741174221039
Epoch 1630, training loss: 83.73860168457031 = 0.2660064399242401 + 10.0 * 8.347259521484375
Epoch 1630, val loss: 0.4894358515739441
Epoch 1640, training loss: 83.76835632324219 = 0.26458409428596497 + 10.0 * 8.350377082824707
Epoch 1640, val loss: 0.49023231863975525
Epoch 1650, training loss: 83.72434997558594 = 0.2631699740886688 + 10.0 * 8.346117973327637
Epoch 1650, val loss: 0.49105891585350037
Epoch 1660, training loss: 83.71070861816406 = 0.26174983382225037 + 10.0 * 8.34489631652832
Epoch 1660, val loss: 0.4920910596847534
Epoch 1670, training loss: 83.70770263671875 = 0.26033154129981995 + 10.0 * 8.34473705291748
Epoch 1670, val loss: 0.4930596351623535
Epoch 1680, training loss: 83.76022338867188 = 0.2589123547077179 + 10.0 * 8.350131034851074
Epoch 1680, val loss: 0.49396806955337524
Epoch 1690, training loss: 83.73710632324219 = 0.257487952709198 + 10.0 * 8.347962379455566
Epoch 1690, val loss: 0.4949929714202881
Epoch 1700, training loss: 83.72975158691406 = 0.25607433915138245 + 10.0 * 8.347368240356445
Epoch 1700, val loss: 0.4960847496986389
Epoch 1710, training loss: 83.69251251220703 = 0.25467193126678467 + 10.0 * 8.34378433227539
Epoch 1710, val loss: 0.497020959854126
Epoch 1720, training loss: 83.6863784790039 = 0.2532758116722107 + 10.0 * 8.343310356140137
Epoch 1720, val loss: 0.49796465039253235
Epoch 1730, training loss: 83.69540405273438 = 0.2518886923789978 + 10.0 * 8.344350814819336
Epoch 1730, val loss: 0.49886554479599
Epoch 1740, training loss: 83.70402526855469 = 0.250508189201355 + 10.0 * 8.345351219177246
Epoch 1740, val loss: 0.4999549984931946
Epoch 1750, training loss: 83.74007415771484 = 0.2491346299648285 + 10.0 * 8.349093437194824
Epoch 1750, val loss: 0.5011815428733826
Epoch 1760, training loss: 83.7065658569336 = 0.24774722754955292 + 10.0 * 8.345881462097168
Epoch 1760, val loss: 0.5019289255142212
Epoch 1770, training loss: 83.67647552490234 = 0.24637429416179657 + 10.0 * 8.343009948730469
Epoch 1770, val loss: 0.5030441284179688
Epoch 1780, training loss: 83.66273498535156 = 0.24501006305217743 + 10.0 * 8.341772079467773
Epoch 1780, val loss: 0.5039336085319519
Epoch 1790, training loss: 83.65628051757812 = 0.24364805221557617 + 10.0 * 8.341263771057129
Epoch 1790, val loss: 0.5049365162849426
Epoch 1800, training loss: 83.652587890625 = 0.24229225516319275 + 10.0 * 8.341029167175293
Epoch 1800, val loss: 0.5059317946434021
Epoch 1810, training loss: 83.71479797363281 = 0.24095091223716736 + 10.0 * 8.347384452819824
Epoch 1810, val loss: 0.5069870352745056
Epoch 1820, training loss: 83.69112396240234 = 0.23958614468574524 + 10.0 * 8.34515380859375
Epoch 1820, val loss: 0.507769763469696
Epoch 1830, training loss: 83.66287994384766 = 0.23822768032550812 + 10.0 * 8.3424654006958
Epoch 1830, val loss: 0.5089699625968933
Epoch 1840, training loss: 83.65953063964844 = 0.23687617480754852 + 10.0 * 8.342265129089355
Epoch 1840, val loss: 0.5097322463989258
Epoch 1850, training loss: 83.66386413574219 = 0.23551732301712036 + 10.0 * 8.34283447265625
Epoch 1850, val loss: 0.5108481645584106
Epoch 1860, training loss: 83.64085388183594 = 0.23416195809841156 + 10.0 * 8.340669631958008
Epoch 1860, val loss: 0.5120660662651062
Epoch 1870, training loss: 83.6358413696289 = 0.23279409110546112 + 10.0 * 8.340304374694824
Epoch 1870, val loss: 0.5129931569099426
Epoch 1880, training loss: 83.68499755859375 = 0.2314581423997879 + 10.0 * 8.345354080200195
Epoch 1880, val loss: 0.514076828956604
Epoch 1890, training loss: 83.62881469726562 = 0.23010267317295074 + 10.0 * 8.339871406555176
Epoch 1890, val loss: 0.5152108073234558
Epoch 1900, training loss: 83.6336898803711 = 0.22875575721263885 + 10.0 * 8.340494155883789
Epoch 1900, val loss: 0.5161725282669067
Epoch 1910, training loss: 83.63623809814453 = 0.2274244725704193 + 10.0 * 8.34088134765625
Epoch 1910, val loss: 0.5173502564430237
Epoch 1920, training loss: 83.63070678710938 = 0.22608719766139984 + 10.0 * 8.340461730957031
Epoch 1920, val loss: 0.5185592770576477
Epoch 1930, training loss: 83.62834167480469 = 0.22474586963653564 + 10.0 * 8.340359687805176
Epoch 1930, val loss: 0.5194370746612549
Epoch 1940, training loss: 83.61045837402344 = 0.22341720759868622 + 10.0 * 8.338704109191895
Epoch 1940, val loss: 0.5206964015960693
Epoch 1950, training loss: 83.60794067382812 = 0.22209608554840088 + 10.0 * 8.338583946228027
Epoch 1950, val loss: 0.5219287872314453
Epoch 1960, training loss: 83.64303588867188 = 0.22076523303985596 + 10.0 * 8.3422269821167
Epoch 1960, val loss: 0.5228323340415955
Epoch 1970, training loss: 83.6143798828125 = 0.21944177150726318 + 10.0 * 8.339493751525879
Epoch 1970, val loss: 0.5241339802742004
Epoch 1980, training loss: 83.59647369384766 = 0.21811476349830627 + 10.0 * 8.337835311889648
Epoch 1980, val loss: 0.5255220532417297
Epoch 1990, training loss: 83.6142807006836 = 0.21679604053497314 + 10.0 * 8.33974838256836
Epoch 1990, val loss: 0.5267242789268494
Epoch 2000, training loss: 83.59447479248047 = 0.2154734581708908 + 10.0 * 8.337900161743164
Epoch 2000, val loss: 0.5278105139732361
Epoch 2010, training loss: 83.5799331665039 = 0.21415068209171295 + 10.0 * 8.336578369140625
Epoch 2010, val loss: 0.5290505886077881
Epoch 2020, training loss: 83.5870361328125 = 0.21283146739006042 + 10.0 * 8.337420463562012
Epoch 2020, val loss: 0.5302711129188538
Epoch 2030, training loss: 83.64582061767578 = 0.21152633428573608 + 10.0 * 8.343429565429688
Epoch 2030, val loss: 0.5314908027648926
Epoch 2040, training loss: 83.60343170166016 = 0.21019986271858215 + 10.0 * 8.339323043823242
Epoch 2040, val loss: 0.5327983498573303
Epoch 2050, training loss: 83.574462890625 = 0.20887890458106995 + 10.0 * 8.33655834197998
Epoch 2050, val loss: 0.5340168476104736
Epoch 2060, training loss: 83.57662200927734 = 0.2075721025466919 + 10.0 * 8.336904525756836
Epoch 2060, val loss: 0.5354926586151123
Epoch 2070, training loss: 83.61904907226562 = 0.2062664031982422 + 10.0 * 8.341278076171875
Epoch 2070, val loss: 0.5367128252983093
Epoch 2080, training loss: 83.56922912597656 = 0.2049536257982254 + 10.0 * 8.336427688598633
Epoch 2080, val loss: 0.5381226539611816
Epoch 2090, training loss: 83.55814361572266 = 0.20365090668201447 + 10.0 * 8.33544921875
Epoch 2090, val loss: 0.539534330368042
Epoch 2100, training loss: 83.55545043945312 = 0.20234748721122742 + 10.0 * 8.335309982299805
Epoch 2100, val loss: 0.5408865213394165
Epoch 2110, training loss: 83.60820770263672 = 0.2010578215122223 + 10.0 * 8.340715408325195
Epoch 2110, val loss: 0.5422078967094421
Epoch 2120, training loss: 83.5658950805664 = 0.19976314902305603 + 10.0 * 8.336613655090332
Epoch 2120, val loss: 0.5438166856765747
Epoch 2130, training loss: 83.55657196044922 = 0.19845929741859436 + 10.0 * 8.335811614990234
Epoch 2130, val loss: 0.5449897050857544
Epoch 2140, training loss: 83.62258911132812 = 0.1971760392189026 + 10.0 * 8.342541694641113
Epoch 2140, val loss: 0.546364963054657
Epoch 2150, training loss: 83.56961059570312 = 0.19587984681129456 + 10.0 * 8.337373733520508
Epoch 2150, val loss: 0.5477619171142578
Epoch 2160, training loss: 83.54112243652344 = 0.1945827752351761 + 10.0 * 8.334653854370117
Epoch 2160, val loss: 0.5492541790008545
Epoch 2170, training loss: 83.5300064086914 = 0.19328969717025757 + 10.0 * 8.333671569824219
Epoch 2170, val loss: 0.5504265427589417
Epoch 2180, training loss: 83.52375030517578 = 0.19199861586093903 + 10.0 * 8.333174705505371
Epoch 2180, val loss: 0.5519402623176575
Epoch 2190, training loss: 83.52046966552734 = 0.19070784747600555 + 10.0 * 8.332976341247559
Epoch 2190, val loss: 0.5532128810882568
Epoch 2200, training loss: 83.53535461425781 = 0.1894216388463974 + 10.0 * 8.334592819213867
Epoch 2200, val loss: 0.554724931716919
Epoch 2210, training loss: 83.59105682373047 = 0.18813388049602509 + 10.0 * 8.340291976928711
Epoch 2210, val loss: 0.5561686158180237
Epoch 2220, training loss: 83.51837158203125 = 0.18680502474308014 + 10.0 * 8.33315658569336
Epoch 2220, val loss: 0.5574522614479065
Epoch 2230, training loss: 83.50878143310547 = 0.18549945950508118 + 10.0 * 8.332327842712402
Epoch 2230, val loss: 0.559124231338501
Epoch 2240, training loss: 83.50701904296875 = 0.18420720100402832 + 10.0 * 8.332281112670898
Epoch 2240, val loss: 0.5603841543197632
Epoch 2250, training loss: 83.50753021240234 = 0.18290358781814575 + 10.0 * 8.332462310791016
Epoch 2250, val loss: 0.5616304874420166
Epoch 2260, training loss: 83.609130859375 = 0.18162833154201508 + 10.0 * 8.342750549316406
Epoch 2260, val loss: 0.5631812810897827
Epoch 2270, training loss: 83.54863739013672 = 0.18033286929130554 + 10.0 * 8.336830139160156
Epoch 2270, val loss: 0.565211296081543
Epoch 2280, training loss: 83.51103210449219 = 0.17903351783752441 + 10.0 * 8.333200454711914
Epoch 2280, val loss: 0.5662328600883484
Epoch 2290, training loss: 83.49164581298828 = 0.17774084210395813 + 10.0 * 8.331390380859375
Epoch 2290, val loss: 0.5678943395614624
Epoch 2300, training loss: 83.48698425292969 = 0.17645640671253204 + 10.0 * 8.331052780151367
Epoch 2300, val loss: 0.5694862008094788
Epoch 2310, training loss: 83.48943328857422 = 0.17517632246017456 + 10.0 * 8.331425666809082
Epoch 2310, val loss: 0.571336030960083
Epoch 2320, training loss: 83.5801773071289 = 0.17394141852855682 + 10.0 * 8.34062385559082
Epoch 2320, val loss: 0.5731279850006104
Epoch 2330, training loss: 83.53318786621094 = 0.17262695729732513 + 10.0 * 8.336055755615234
Epoch 2330, val loss: 0.5739369988441467
Epoch 2340, training loss: 83.49054718017578 = 0.17133767902851105 + 10.0 * 8.331920623779297
Epoch 2340, val loss: 0.5757175087928772
Epoch 2350, training loss: 83.49822998046875 = 0.1700616478919983 + 10.0 * 8.332817077636719
Epoch 2350, val loss: 0.5774288177490234
Epoch 2360, training loss: 83.48370361328125 = 0.16877959668636322 + 10.0 * 8.33149242401123
Epoch 2360, val loss: 0.5787966251373291
Epoch 2370, training loss: 83.47999572753906 = 0.16749590635299683 + 10.0 * 8.331250190734863
Epoch 2370, val loss: 0.5806142687797546
Epoch 2380, training loss: 83.506103515625 = 0.1662255972623825 + 10.0 * 8.33398723602295
Epoch 2380, val loss: 0.5822094082832336
Epoch 2390, training loss: 83.4972152709961 = 0.16495396196842194 + 10.0 * 8.333226203918457
Epoch 2390, val loss: 0.5837799310684204
Epoch 2400, training loss: 83.47527313232422 = 0.16366061568260193 + 10.0 * 8.331161499023438
Epoch 2400, val loss: 0.5856969952583313
Epoch 2410, training loss: 83.46772003173828 = 0.1623973548412323 + 10.0 * 8.33053207397461
Epoch 2410, val loss: 0.5872563123703003
Epoch 2420, training loss: 83.47759246826172 = 0.1611091047525406 + 10.0 * 8.331647872924805
Epoch 2420, val loss: 0.5887372493743896
Epoch 2430, training loss: 83.47743225097656 = 0.15982532501220703 + 10.0 * 8.33176040649414
Epoch 2430, val loss: 0.5908124446868896
Epoch 2440, training loss: 83.48058319091797 = 0.15856151282787323 + 10.0 * 8.332201957702637
Epoch 2440, val loss: 0.5925727486610413
Epoch 2450, training loss: 83.44831848144531 = 0.15727052092552185 + 10.0 * 8.32910442352295
Epoch 2450, val loss: 0.5944348573684692
Epoch 2460, training loss: 83.45048522949219 = 0.15599410235881805 + 10.0 * 8.329449653625488
Epoch 2460, val loss: 0.5963129997253418
Epoch 2470, training loss: 83.45736694335938 = 0.1547224372625351 + 10.0 * 8.3302640914917
Epoch 2470, val loss: 0.5978934168815613
Epoch 2480, training loss: 83.47517395019531 = 0.15345972776412964 + 10.0 * 8.332171440124512
Epoch 2480, val loss: 0.6001255512237549
Epoch 2490, training loss: 83.4576187133789 = 0.15219061076641083 + 10.0 * 8.33054256439209
Epoch 2490, val loss: 0.6017331480979919
Epoch 2500, training loss: 83.44234466552734 = 0.1509300023317337 + 10.0 * 8.329141616821289
Epoch 2500, val loss: 0.604048490524292
Epoch 2510, training loss: 83.52061462402344 = 0.14967899024486542 + 10.0 * 8.337093353271484
Epoch 2510, val loss: 0.6056135296821594
Epoch 2520, training loss: 83.44941711425781 = 0.14840593934059143 + 10.0 * 8.330101013183594
Epoch 2520, val loss: 0.6076244115829468
Epoch 2530, training loss: 83.43341064453125 = 0.1471402645111084 + 10.0 * 8.32862663269043
Epoch 2530, val loss: 0.6099193692207336
Epoch 2540, training loss: 83.42357635498047 = 0.1458737701177597 + 10.0 * 8.327770233154297
Epoch 2540, val loss: 0.6117430329322815
Epoch 2550, training loss: 83.41790008544922 = 0.1446128636598587 + 10.0 * 8.3273286819458
Epoch 2550, val loss: 0.6140615344047546
Epoch 2560, training loss: 83.41581726074219 = 0.14334991574287415 + 10.0 * 8.32724666595459
Epoch 2560, val loss: 0.6158511638641357
Epoch 2570, training loss: 83.4340591430664 = 0.1420906037092209 + 10.0 * 8.32919692993164
Epoch 2570, val loss: 0.6176565885543823
Epoch 2580, training loss: 83.44293212890625 = 0.14083737134933472 + 10.0 * 8.330209732055664
Epoch 2580, val loss: 0.6197872161865234
Epoch 2590, training loss: 83.4284439086914 = 0.13957257568836212 + 10.0 * 8.328886985778809
Epoch 2590, val loss: 0.6217414736747742
Epoch 2600, training loss: 83.43048095703125 = 0.1383281797170639 + 10.0 * 8.329215049743652
Epoch 2600, val loss: 0.6236128807067871
Epoch 2610, training loss: 83.4331283569336 = 0.13708308339118958 + 10.0 * 8.329604148864746
Epoch 2610, val loss: 0.6258870363235474
Epoch 2620, training loss: 83.42578887939453 = 0.1358373612165451 + 10.0 * 8.328995704650879
Epoch 2620, val loss: 0.627922773361206
Epoch 2630, training loss: 83.40090942382812 = 0.1345958560705185 + 10.0 * 8.326631546020508
Epoch 2630, val loss: 0.6303657293319702
Epoch 2640, training loss: 83.40630340576172 = 0.1333657056093216 + 10.0 * 8.327293395996094
Epoch 2640, val loss: 0.6324208378791809
Epoch 2650, training loss: 83.46749877929688 = 0.13213862478733063 + 10.0 * 8.333536148071289
Epoch 2650, val loss: 0.6348243355751038
Epoch 2660, training loss: 83.41192626953125 = 0.13089312613010406 + 10.0 * 8.328104019165039
Epoch 2660, val loss: 0.6364539861679077
Epoch 2670, training loss: 83.4057846069336 = 0.12965957820415497 + 10.0 * 8.327611923217773
Epoch 2670, val loss: 0.638573408126831
Epoch 2680, training loss: 83.46332550048828 = 0.1284439116716385 + 10.0 * 8.333488464355469
Epoch 2680, val loss: 0.6407939195632935
Epoch 2690, training loss: 83.40544891357422 = 0.12721671164035797 + 10.0 * 8.3278226852417
Epoch 2690, val loss: 0.6435768008232117
Epoch 2700, training loss: 83.38603210449219 = 0.12599194049835205 + 10.0 * 8.326004028320312
Epoch 2700, val loss: 0.6452699303627014
Epoch 2710, training loss: 83.38111114501953 = 0.12477342784404755 + 10.0 * 8.325634002685547
Epoch 2710, val loss: 0.6481286287307739
Epoch 2720, training loss: 83.41764831542969 = 0.12358015030622482 + 10.0 * 8.32940673828125
Epoch 2720, val loss: 0.6506057977676392
Epoch 2730, training loss: 83.3876724243164 = 0.12238257378339767 + 10.0 * 8.326528549194336
Epoch 2730, val loss: 0.6526815891265869
Epoch 2740, training loss: 83.38839721679688 = 0.12118381261825562 + 10.0 * 8.32672119140625
Epoch 2740, val loss: 0.654812216758728
Epoch 2750, training loss: 83.37928009033203 = 0.12000256776809692 + 10.0 * 8.325927734375
Epoch 2750, val loss: 0.6573638916015625
Epoch 2760, training loss: 83.38943481445312 = 0.11883105337619781 + 10.0 * 8.32706069946289
Epoch 2760, val loss: 0.6594235897064209
Epoch 2770, training loss: 83.37445068359375 = 0.11765658855438232 + 10.0 * 8.325679779052734
Epoch 2770, val loss: 0.6614972949028015
Epoch 2780, training loss: 83.38198852539062 = 0.11649554967880249 + 10.0 * 8.326549530029297
Epoch 2780, val loss: 0.6638116836547852
Epoch 2790, training loss: 83.37371063232422 = 0.11533785611391068 + 10.0 * 8.325837135314941
Epoch 2790, val loss: 0.6666097640991211
Epoch 2800, training loss: 83.38674926757812 = 0.11418160796165466 + 10.0 * 8.32725715637207
Epoch 2800, val loss: 0.668984591960907
Epoch 2810, training loss: 83.44661712646484 = 0.11305886507034302 + 10.0 * 8.333355903625488
Epoch 2810, val loss: 0.6712508201599121
Epoch 2820, training loss: 83.37293243408203 = 0.11190038174390793 + 10.0 * 8.326103210449219
Epoch 2820, val loss: 0.6744278073310852
Epoch 2830, training loss: 83.35874938964844 = 0.11075315624475479 + 10.0 * 8.324799537658691
Epoch 2830, val loss: 0.6762422919273376
Epoch 2840, training loss: 83.3511962890625 = 0.10961908102035522 + 10.0 * 8.32415771484375
Epoch 2840, val loss: 0.6793967485427856
Epoch 2850, training loss: 83.35291290283203 = 0.10849613696336746 + 10.0 * 8.324441909790039
Epoch 2850, val loss: 0.682073712348938
Epoch 2860, training loss: 83.4106216430664 = 0.10739774256944656 + 10.0 * 8.330322265625
Epoch 2860, val loss: 0.6850022077560425
Epoch 2870, training loss: 83.35933685302734 = 0.10627525299787521 + 10.0 * 8.325305938720703
Epoch 2870, val loss: 0.6872800588607788
Epoch 2880, training loss: 83.34014892578125 = 0.10516715049743652 + 10.0 * 8.323498725891113
Epoch 2880, val loss: 0.6899781227111816
Epoch 2890, training loss: 83.34822082519531 = 0.10406558960676193 + 10.0 * 8.32441520690918
Epoch 2890, val loss: 0.6924047470092773
Epoch 2900, training loss: 83.37400817871094 = 0.10298728942871094 + 10.0 * 8.327101707458496
Epoch 2900, val loss: 0.6948389410972595
Epoch 2910, training loss: 83.3500747680664 = 0.10189079493284225 + 10.0 * 8.32481861114502
Epoch 2910, val loss: 0.6979958415031433
Epoch 2920, training loss: 83.36106872558594 = 0.1008148267865181 + 10.0 * 8.326025009155273
Epoch 2920, val loss: 0.7007269263267517
Epoch 2930, training loss: 83.33551025390625 = 0.09973610192537308 + 10.0 * 8.323576927185059
Epoch 2930, val loss: 0.7034599184989929
Epoch 2940, training loss: 83.40486145019531 = 0.0986916795372963 + 10.0 * 8.33061695098877
Epoch 2940, val loss: 0.7069858908653259
Epoch 2950, training loss: 83.33613586425781 = 0.09761146456003189 + 10.0 * 8.3238525390625
Epoch 2950, val loss: 0.7089117765426636
Epoch 2960, training loss: 83.32740020751953 = 0.0965641662478447 + 10.0 * 8.323083877563477
Epoch 2960, val loss: 0.7116091251373291
Epoch 2970, training loss: 83.31918334960938 = 0.09551968425512314 + 10.0 * 8.322366714477539
Epoch 2970, val loss: 0.7148745656013489
Epoch 2980, training loss: 83.32482147216797 = 0.09449555724859238 + 10.0 * 8.32303237915039
Epoch 2980, val loss: 0.7178354859352112
Epoch 2990, training loss: 83.4042739868164 = 0.09345989674329758 + 10.0 * 8.33108139038086
Epoch 2990, val loss: 0.7204386591911316
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8082191780821917
0.8414837354198363
=== training gcn model ===
Epoch 0, training loss: 106.91910552978516 = 1.0964006185531616 + 10.0 * 10.582270622253418
Epoch 0, val loss: 1.0950525999069214
Epoch 10, training loss: 106.90422058105469 = 1.0898226499557495 + 10.0 * 10.581439971923828
Epoch 10, val loss: 1.088494896888733
Epoch 20, training loss: 106.8211898803711 = 1.0822142362594604 + 10.0 * 10.573897361755371
Epoch 20, val loss: 1.0808897018432617
Epoch 30, training loss: 106.26692199707031 = 1.0731843709945679 + 10.0 * 10.519373893737793
Epoch 30, val loss: 1.0719321966171265
Epoch 40, training loss: 103.86611938476562 = 1.0633834600448608 + 10.0 * 10.2802734375
Epoch 40, val loss: 1.0624192953109741
Epoch 50, training loss: 98.29478454589844 = 1.0535377264022827 + 10.0 * 9.724124908447266
Epoch 50, val loss: 1.0528242588043213
Epoch 60, training loss: 96.82681274414062 = 1.04426908493042 + 10.0 * 9.578254699707031
Epoch 60, val loss: 1.0437568426132202
Epoch 70, training loss: 96.20381927490234 = 1.0346931219100952 + 10.0 * 9.516912460327148
Epoch 70, val loss: 1.0344834327697754
Epoch 80, training loss: 94.73058319091797 = 1.0266177654266357 + 10.0 * 9.370396614074707
Epoch 80, val loss: 1.0268911123275757
Epoch 90, training loss: 93.11317443847656 = 1.0196033716201782 + 10.0 * 9.209357261657715
Epoch 90, val loss: 1.020162582397461
Epoch 100, training loss: 92.70709228515625 = 1.0107417106628418 + 10.0 * 9.169634819030762
Epoch 100, val loss: 1.011423110961914
Epoch 110, training loss: 91.87537384033203 = 1.0006144046783447 + 10.0 * 9.087475776672363
Epoch 110, val loss: 1.0017451047897339
Epoch 120, training loss: 90.60151672363281 = 0.9924589991569519 + 10.0 * 8.960905075073242
Epoch 120, val loss: 0.9942994117736816
Epoch 130, training loss: 89.26995849609375 = 0.9866187572479248 + 10.0 * 8.828333854675293
Epoch 130, val loss: 0.9889504313468933
Epoch 140, training loss: 88.55279541015625 = 0.978274405002594 + 10.0 * 8.757452011108398
Epoch 140, val loss: 0.9807335734367371
Epoch 150, training loss: 88.10816192626953 = 0.9665125012397766 + 10.0 * 8.714164733886719
Epoch 150, val loss: 0.9692080616950989
Epoch 160, training loss: 87.77181243896484 = 0.9525715708732605 + 10.0 * 8.681924819946289
Epoch 160, val loss: 0.9556413292884827
Epoch 170, training loss: 87.53457641601562 = 0.9370499849319458 + 10.0 * 8.65975284576416
Epoch 170, val loss: 0.9406601190567017
Epoch 180, training loss: 87.35913848876953 = 0.9201897382736206 + 10.0 * 8.643895149230957
Epoch 180, val loss: 0.924519956111908
Epoch 190, training loss: 87.16218566894531 = 0.9024520516395569 + 10.0 * 8.62597370147705
Epoch 190, val loss: 0.9077025651931763
Epoch 200, training loss: 87.00074768066406 = 0.8839576244354248 + 10.0 * 8.611679077148438
Epoch 200, val loss: 0.8901144862174988
Epoch 210, training loss: 86.85364532470703 = 0.8645015954971313 + 10.0 * 8.59891414642334
Epoch 210, val loss: 0.8715752363204956
Epoch 220, training loss: 86.73479461669922 = 0.8441383242607117 + 10.0 * 8.589065551757812
Epoch 220, val loss: 0.8522116541862488
Epoch 230, training loss: 86.61477661132812 = 0.8231214880943298 + 10.0 * 8.5791654586792
Epoch 230, val loss: 0.8323256373405457
Epoch 240, training loss: 86.48785400390625 = 0.8017513751983643 + 10.0 * 8.568610191345215
Epoch 240, val loss: 0.8121664524078369
Epoch 250, training loss: 86.41304016113281 = 0.780244767665863 + 10.0 * 8.56328010559082
Epoch 250, val loss: 0.7919240593910217
Epoch 260, training loss: 86.29641723632812 = 0.7588208317756653 + 10.0 * 8.553759574890137
Epoch 260, val loss: 0.7718617916107178
Epoch 270, training loss: 86.17059326171875 = 0.7376671433448792 + 10.0 * 8.543292045593262
Epoch 270, val loss: 0.7521030902862549
Epoch 280, training loss: 86.09526824951172 = 0.7169312834739685 + 10.0 * 8.537833213806152
Epoch 280, val loss: 0.7327976226806641
Epoch 290, training loss: 85.95291900634766 = 0.6967323422431946 + 10.0 * 8.525618553161621
Epoch 290, val loss: 0.7140124440193176
Epoch 300, training loss: 85.888916015625 = 0.6771717071533203 + 10.0 * 8.521174430847168
Epoch 300, val loss: 0.6959524750709534
Epoch 310, training loss: 85.74185943603516 = 0.6582037806510925 + 10.0 * 8.508365631103516
Epoch 310, val loss: 0.6785709857940674
Epoch 320, training loss: 85.61957550048828 = 0.6400064826011658 + 10.0 * 8.497957229614258
Epoch 320, val loss: 0.6618912220001221
Epoch 330, training loss: 85.53268432617188 = 0.6226170063018799 + 10.0 * 8.491006851196289
Epoch 330, val loss: 0.6460591554641724
Epoch 340, training loss: 85.5145492553711 = 0.6060228943824768 + 10.0 * 8.490852355957031
Epoch 340, val loss: 0.6311014294624329
Epoch 350, training loss: 85.39335632324219 = 0.590205729007721 + 10.0 * 8.480315208435059
Epoch 350, val loss: 0.6167926788330078
Epoch 360, training loss: 85.31321716308594 = 0.5754336714744568 + 10.0 * 8.47377872467041
Epoch 360, val loss: 0.6036486625671387
Epoch 370, training loss: 85.24136352539062 = 0.5617249011993408 + 10.0 * 8.467964172363281
Epoch 370, val loss: 0.5915577411651611
Epoch 380, training loss: 85.2156982421875 = 0.5490368008613586 + 10.0 * 8.466666221618652
Epoch 380, val loss: 0.5805057883262634
Epoch 390, training loss: 85.16566467285156 = 0.5372483730316162 + 10.0 * 8.462841987609863
Epoch 390, val loss: 0.5704253315925598
Epoch 400, training loss: 85.08267211914062 = 0.5264855027198792 + 10.0 * 8.455617904663086
Epoch 400, val loss: 0.561282217502594
Epoch 410, training loss: 85.035888671875 = 0.5167012214660645 + 10.0 * 8.451918601989746
Epoch 410, val loss: 0.5530996322631836
Epoch 420, training loss: 84.99018096923828 = 0.5077937841415405 + 10.0 * 8.448238372802734
Epoch 420, val loss: 0.5458203554153442
Epoch 430, training loss: 85.0345687866211 = 0.4996746778488159 + 10.0 * 8.453489303588867
Epoch 430, val loss: 0.5390874743461609
Epoch 440, training loss: 84.98064422607422 = 0.49210241436958313 + 10.0 * 8.448854446411133
Epoch 440, val loss: 0.5332445502281189
Epoch 450, training loss: 84.89813232421875 = 0.485298216342926 + 10.0 * 8.441283226013184
Epoch 450, val loss: 0.5281299352645874
Epoch 460, training loss: 84.86862182617188 = 0.47913143038749695 + 10.0 * 8.438948631286621
Epoch 460, val loss: 0.5233442187309265
Epoch 470, training loss: 84.83036041259766 = 0.47344544529914856 + 10.0 * 8.435691833496094
Epoch 470, val loss: 0.5191693305969238
Epoch 480, training loss: 84.80455017089844 = 0.46819373965263367 + 10.0 * 8.433635711669922
Epoch 480, val loss: 0.5153730511665344
Epoch 490, training loss: 84.77787780761719 = 0.4633086323738098 + 10.0 * 8.431456565856934
Epoch 490, val loss: 0.511916995048523
Epoch 500, training loss: 84.75543212890625 = 0.45874375104904175 + 10.0 * 8.429669380187988
Epoch 500, val loss: 0.508780837059021
Epoch 510, training loss: 84.7481689453125 = 0.45444074273109436 + 10.0 * 8.429372787475586
Epoch 510, val loss: 0.5059368014335632
Epoch 520, training loss: 84.76535034179688 = 0.4503925144672394 + 10.0 * 8.431495666503906
Epoch 520, val loss: 0.5033577680587769
Epoch 530, training loss: 84.72296905517578 = 0.44661757349967957 + 10.0 * 8.427635192871094
Epoch 530, val loss: 0.5006918907165527
Epoch 540, training loss: 84.67288970947266 = 0.4430920481681824 + 10.0 * 8.422979354858398
Epoch 540, val loss: 0.49851134419441223
Epoch 550, training loss: 84.65229797363281 = 0.43975943326950073 + 10.0 * 8.42125415802002
Epoch 550, val loss: 0.49645504355430603
Epoch 560, training loss: 84.68264770507812 = 0.43657246232032776 + 10.0 * 8.42460823059082
Epoch 560, val loss: 0.49467405676841736
Epoch 570, training loss: 84.62960052490234 = 0.4335188865661621 + 10.0 * 8.419608116149902
Epoch 570, val loss: 0.49258893728256226
Epoch 580, training loss: 84.593505859375 = 0.43060028553009033 + 10.0 * 8.416290283203125
Epoch 580, val loss: 0.49102941155433655
Epoch 590, training loss: 84.57670593261719 = 0.42783015966415405 + 10.0 * 8.414887428283691
Epoch 590, val loss: 0.4893500804901123
Epoch 600, training loss: 84.56763458251953 = 0.4251515865325928 + 10.0 * 8.4142484664917
Epoch 600, val loss: 0.48782750964164734
Epoch 610, training loss: 84.61609649658203 = 0.4225275218486786 + 10.0 * 8.419356346130371
Epoch 610, val loss: 0.48653602600097656
Epoch 620, training loss: 84.55485534667969 = 0.4200124740600586 + 10.0 * 8.413484573364258
Epoch 620, val loss: 0.48482152819633484
Epoch 630, training loss: 84.514892578125 = 0.41757312417030334 + 10.0 * 8.4097318649292
Epoch 630, val loss: 0.48369136452674866
Epoch 640, training loss: 84.48580169677734 = 0.41525721549987793 + 10.0 * 8.40705394744873
Epoch 640, val loss: 0.48233523964881897
Epoch 650, training loss: 84.47917938232422 = 0.41298985481262207 + 10.0 * 8.40661907196045
Epoch 650, val loss: 0.4811217784881592
Epoch 660, training loss: 84.5054702758789 = 0.4107675552368164 + 10.0 * 8.409470558166504
Epoch 660, val loss: 0.47986674308776855
Epoch 670, training loss: 84.45783233642578 = 0.40856078267097473 + 10.0 * 8.404927253723145
Epoch 670, val loss: 0.47897249460220337
Epoch 680, training loss: 84.43510437011719 = 0.40647172927856445 + 10.0 * 8.402863502502441
Epoch 680, val loss: 0.47780516743659973
Epoch 690, training loss: 84.42926788330078 = 0.40441206097602844 + 10.0 * 8.402485847473145
Epoch 690, val loss: 0.47680333256721497
Epoch 700, training loss: 84.40633392333984 = 0.4024258852005005 + 10.0 * 8.400390625
Epoch 700, val loss: 0.47571465373039246
Epoch 710, training loss: 84.47289276123047 = 0.4004752039909363 + 10.0 * 8.407241821289062
Epoch 710, val loss: 0.47458940744400024
Epoch 720, training loss: 84.38040161132812 = 0.39849820733070374 + 10.0 * 8.39819049835205
Epoch 720, val loss: 0.4739317297935486
Epoch 730, training loss: 84.35716247558594 = 0.3966500163078308 + 10.0 * 8.396051406860352
Epoch 730, val loss: 0.4729030430316925
Epoch 740, training loss: 84.33768463134766 = 0.3948419690132141 + 10.0 * 8.39428424835205
Epoch 740, val loss: 0.4721308946609497
Epoch 750, training loss: 84.33148956298828 = 0.3930888772010803 + 10.0 * 8.393839836120605
Epoch 750, val loss: 0.4712790548801422
Epoch 760, training loss: 84.39305114746094 = 0.3913659155368805 + 10.0 * 8.400168418884277
Epoch 760, val loss: 0.47035908699035645
Epoch 770, training loss: 84.30606842041016 = 0.3896074593067169 + 10.0 * 8.391645431518555
Epoch 770, val loss: 0.4698103666305542
Epoch 780, training loss: 84.28939056396484 = 0.38794898986816406 + 10.0 * 8.390144348144531
Epoch 780, val loss: 0.46894821524620056
Epoch 790, training loss: 84.28102111816406 = 0.3863368332386017 + 10.0 * 8.3894681930542
Epoch 790, val loss: 0.46822816133499146
Epoch 800, training loss: 84.29108428955078 = 0.3847483992576599 + 10.0 * 8.390633583068848
Epoch 800, val loss: 0.46744802594184875
Epoch 810, training loss: 84.27442932128906 = 0.38314855098724365 + 10.0 * 8.389127731323242
Epoch 810, val loss: 0.46685245633125305
Epoch 820, training loss: 84.27299499511719 = 0.38157427310943604 + 10.0 * 8.389142036437988
Epoch 820, val loss: 0.4662262797355652
Epoch 830, training loss: 84.25054931640625 = 0.38006940484046936 + 10.0 * 8.387048721313477
Epoch 830, val loss: 0.46556976437568665
Epoch 840, training loss: 84.22816467285156 = 0.3785703182220459 + 10.0 * 8.38495922088623
Epoch 840, val loss: 0.46505314111709595
Epoch 850, training loss: 84.22315979003906 = 0.3771088421344757 + 10.0 * 8.384605407714844
Epoch 850, val loss: 0.4645060896873474
Epoch 860, training loss: 84.24373626708984 = 0.37567436695098877 + 10.0 * 8.38680648803711
Epoch 860, val loss: 0.46390536427497864
Epoch 870, training loss: 84.22034454345703 = 0.37424513697624207 + 10.0 * 8.384610176086426
Epoch 870, val loss: 0.46339333057403564
Epoch 880, training loss: 84.22830963134766 = 0.37283673882484436 + 10.0 * 8.385547637939453
Epoch 880, val loss: 0.4629044532775879
Epoch 890, training loss: 84.2029800415039 = 0.3714272081851959 + 10.0 * 8.38315486907959
Epoch 890, val loss: 0.4624879062175751
Epoch 900, training loss: 84.20525360107422 = 0.37003573775291443 + 10.0 * 8.383522033691406
Epoch 900, val loss: 0.4619249701499939
Epoch 910, training loss: 84.176513671875 = 0.368679016828537 + 10.0 * 8.380784034729004
Epoch 910, val loss: 0.4614764451980591
Epoch 920, training loss: 84.1610107421875 = 0.36738842725753784 + 10.0 * 8.379362106323242
Epoch 920, val loss: 0.46093931794166565
Epoch 930, training loss: 84.1480712890625 = 0.3660978376865387 + 10.0 * 8.37819766998291
Epoch 930, val loss: 0.4606015384197235
Epoch 940, training loss: 84.13695526123047 = 0.3648378551006317 + 10.0 * 8.377211570739746
Epoch 940, val loss: 0.46015599370002747
Epoch 950, training loss: 84.12823486328125 = 0.3635827898979187 + 10.0 * 8.37646484375
Epoch 950, val loss: 0.459798663854599
Epoch 960, training loss: 84.1392822265625 = 0.3623402714729309 + 10.0 * 8.377694129943848
Epoch 960, val loss: 0.45940202474594116
Epoch 970, training loss: 84.12828063964844 = 0.3610662519931793 + 10.0 * 8.376721382141113
Epoch 970, val loss: 0.4589686393737793
Epoch 980, training loss: 84.1295394897461 = 0.3598131537437439 + 10.0 * 8.376973152160645
Epoch 980, val loss: 0.4585927426815033
Epoch 990, training loss: 84.11197662353516 = 0.3585726022720337 + 10.0 * 8.375340461730957
Epoch 990, val loss: 0.4583045244216919
Epoch 1000, training loss: 84.0934066772461 = 0.35737210512161255 + 10.0 * 8.373603820800781
Epoch 1000, val loss: 0.4580188989639282
Epoch 1010, training loss: 84.17296600341797 = 0.3561762869358063 + 10.0 * 8.381678581237793
Epoch 1010, val loss: 0.4578257203102112
Epoch 1020, training loss: 84.09943389892578 = 0.3549909293651581 + 10.0 * 8.374444961547852
Epoch 1020, val loss: 0.457262247800827
Epoch 1030, training loss: 84.0749282836914 = 0.35379570722579956 + 10.0 * 8.372113227844238
Epoch 1030, val loss: 0.4570780396461487
Epoch 1040, training loss: 84.0597152709961 = 0.35265305638313293 + 10.0 * 8.370706558227539
Epoch 1040, val loss: 0.4566871225833893
Epoch 1050, training loss: 84.05928039550781 = 0.35150524973869324 + 10.0 * 8.37077808380127
Epoch 1050, val loss: 0.4563979208469391
Epoch 1060, training loss: 84.10166931152344 = 0.35033878684043884 + 10.0 * 8.37513256072998
Epoch 1060, val loss: 0.456133633852005
Epoch 1070, training loss: 84.0458984375 = 0.3491663336753845 + 10.0 * 8.369672775268555
Epoch 1070, val loss: 0.45594367384910583
Epoch 1080, training loss: 84.08397674560547 = 0.3480178713798523 + 10.0 * 8.37359619140625
Epoch 1080, val loss: 0.4557275176048279
Epoch 1090, training loss: 84.03292083740234 = 0.3468518555164337 + 10.0 * 8.368606567382812
Epoch 1090, val loss: 0.4553883373737335
Epoch 1100, training loss: 84.02644348144531 = 0.3457305133342743 + 10.0 * 8.368070602416992
Epoch 1100, val loss: 0.45500290393829346
Epoch 1110, training loss: 84.00943756103516 = 0.3445982038974762 + 10.0 * 8.366483688354492
Epoch 1110, val loss: 0.4548284113407135
Epoch 1120, training loss: 84.00592041015625 = 0.34347623586654663 + 10.0 * 8.366244316101074
Epoch 1120, val loss: 0.4546452462673187
Epoch 1130, training loss: 84.06524658203125 = 0.3423450291156769 + 10.0 * 8.372289657592773
Epoch 1130, val loss: 0.45453059673309326
Epoch 1140, training loss: 84.0369873046875 = 0.3411945700645447 + 10.0 * 8.369579315185547
Epoch 1140, val loss: 0.4540666937828064
Epoch 1150, training loss: 84.00482940673828 = 0.34006771445274353 + 10.0 * 8.366476058959961
Epoch 1150, val loss: 0.45391812920570374
Epoch 1160, training loss: 83.97850799560547 = 0.33896270394325256 + 10.0 * 8.363954544067383
Epoch 1160, val loss: 0.453635036945343
Epoch 1170, training loss: 83.96849060058594 = 0.33785343170166016 + 10.0 * 8.36306381225586
Epoch 1170, val loss: 0.45347103476524353
Epoch 1180, training loss: 83.9820327758789 = 0.3367464542388916 + 10.0 * 8.36452865600586
Epoch 1180, val loss: 0.45326706767082214
Epoch 1190, training loss: 83.96613311767578 = 0.3356046974658966 + 10.0 * 8.363053321838379
Epoch 1190, val loss: 0.45309576392173767
Epoch 1200, training loss: 83.97772216796875 = 0.3344740569591522 + 10.0 * 8.364324569702148
Epoch 1200, val loss: 0.4527451694011688
Epoch 1210, training loss: 83.94657135009766 = 0.33337077498435974 + 10.0 * 8.361319541931152
Epoch 1210, val loss: 0.45254555344581604
Epoch 1220, training loss: 83.95478057861328 = 0.3322702944278717 + 10.0 * 8.362251281738281
Epoch 1220, val loss: 0.45232245326042175
Epoch 1230, training loss: 84.00212860107422 = 0.331152081489563 + 10.0 * 8.367097854614258
Epoch 1230, val loss: 0.45209503173828125
Epoch 1240, training loss: 83.95030212402344 = 0.3299938440322876 + 10.0 * 8.362030982971191
Epoch 1240, val loss: 0.4520726501941681
Epoch 1250, training loss: 83.9257583618164 = 0.3288846015930176 + 10.0 * 8.359686851501465
Epoch 1250, val loss: 0.45173996686935425
Epoch 1260, training loss: 83.91680908203125 = 0.3277592957019806 + 10.0 * 8.358904838562012
Epoch 1260, val loss: 0.4516075551509857
Epoch 1270, training loss: 83.94001770019531 = 0.326625794172287 + 10.0 * 8.361339569091797
Epoch 1270, val loss: 0.4514768123626709
Epoch 1280, training loss: 83.91146850585938 = 0.32546859979629517 + 10.0 * 8.358599662780762
Epoch 1280, val loss: 0.45131534337997437
Epoch 1290, training loss: 83.89476776123047 = 0.32433322072029114 + 10.0 * 8.357043266296387
Epoch 1290, val loss: 0.45101678371429443
Epoch 1300, training loss: 83.89077758789062 = 0.32319319248199463 + 10.0 * 8.356758117675781
Epoch 1300, val loss: 0.45080575346946716
Epoch 1310, training loss: 83.94229888916016 = 0.3220590651035309 + 10.0 * 8.362024307250977
Epoch 1310, val loss: 0.45056435465812683
Epoch 1320, training loss: 83.9141845703125 = 0.32088473439216614 + 10.0 * 8.359330177307129
Epoch 1320, val loss: 0.45045313239097595
Epoch 1330, training loss: 83.89141845703125 = 0.3197115957736969 + 10.0 * 8.357171058654785
Epoch 1330, val loss: 0.4503268897533417
Epoch 1340, training loss: 83.87226104736328 = 0.3185586631298065 + 10.0 * 8.35537052154541
Epoch 1340, val loss: 0.45018160343170166
Epoch 1350, training loss: 83.87235260009766 = 0.3174000382423401 + 10.0 * 8.35549545288086
Epoch 1350, val loss: 0.45009201765060425
Epoch 1360, training loss: 83.92169189453125 = 0.31622132658958435 + 10.0 * 8.360547065734863
Epoch 1360, val loss: 0.4499775469303131
Epoch 1370, training loss: 83.8792724609375 = 0.3150332570075989 + 10.0 * 8.356424331665039
Epoch 1370, val loss: 0.44957923889160156
Epoch 1380, training loss: 83.86568450927734 = 0.3138526976108551 + 10.0 * 8.355183601379395
Epoch 1380, val loss: 0.4494484066963196
Epoch 1390, training loss: 83.8493423461914 = 0.3126588761806488 + 10.0 * 8.353668212890625
Epoch 1390, val loss: 0.44930702447891235
Epoch 1400, training loss: 83.85095977783203 = 0.3114579916000366 + 10.0 * 8.353950500488281
Epoch 1400, val loss: 0.4492357671260834
Epoch 1410, training loss: 83.87923431396484 = 0.3102465867996216 + 10.0 * 8.356898307800293
Epoch 1410, val loss: 0.4490925371646881
Epoch 1420, training loss: 83.83171844482422 = 0.30904465913772583 + 10.0 * 8.352267265319824
Epoch 1420, val loss: 0.4487999677658081
Epoch 1430, training loss: 83.82247161865234 = 0.30782759189605713 + 10.0 * 8.35146427154541
Epoch 1430, val loss: 0.44869324564933777
Epoch 1440, training loss: 83.84032440185547 = 0.306601345539093 + 10.0 * 8.353372573852539
Epoch 1440, val loss: 0.4486553370952606
Epoch 1450, training loss: 83.86327362060547 = 0.3053470551967621 + 10.0 * 8.355792999267578
Epoch 1450, val loss: 0.4485674202442169
Epoch 1460, training loss: 83.83013916015625 = 0.3040943443775177 + 10.0 * 8.352604866027832
Epoch 1460, val loss: 0.4483920633792877
Epoch 1470, training loss: 83.8048324584961 = 0.30285346508026123 + 10.0 * 8.350197792053223
Epoch 1470, val loss: 0.44823649525642395
Epoch 1480, training loss: 83.79803466796875 = 0.3016076683998108 + 10.0 * 8.349642753601074
Epoch 1480, val loss: 0.448164165019989
Epoch 1490, training loss: 83.82778930664062 = 0.3003675043582916 + 10.0 * 8.352742195129395
Epoch 1490, val loss: 0.4479943811893463
Epoch 1500, training loss: 83.79490661621094 = 0.2990775406360626 + 10.0 * 8.34958267211914
Epoch 1500, val loss: 0.4480069875717163
Epoch 1510, training loss: 83.79410552978516 = 0.2977871596813202 + 10.0 * 8.349631309509277
Epoch 1510, val loss: 0.4479862153530121
Epoch 1520, training loss: 83.77937316894531 = 0.2965204417705536 + 10.0 * 8.348284721374512
Epoch 1520, val loss: 0.4478277862071991
Epoch 1530, training loss: 83.77488708496094 = 0.2952266335487366 + 10.0 * 8.347966194152832
Epoch 1530, val loss: 0.447790265083313
Epoch 1540, training loss: 83.88900756835938 = 0.29391542077064514 + 10.0 * 8.359509468078613
Epoch 1540, val loss: 0.4477088749408722
Epoch 1550, training loss: 83.77471160888672 = 0.29257673025131226 + 10.0 * 8.348213195800781
Epoch 1550, val loss: 0.44774144887924194
Epoch 1560, training loss: 83.77679443359375 = 0.2912404239177704 + 10.0 * 8.348555564880371
Epoch 1560, val loss: 0.44782719016075134
Epoch 1570, training loss: 83.74948120117188 = 0.2899203300476074 + 10.0 * 8.345956802368164
Epoch 1570, val loss: 0.44776296615600586
Epoch 1580, training loss: 83.75167083740234 = 0.28859493136405945 + 10.0 * 8.346307754516602
Epoch 1580, val loss: 0.44775867462158203
Epoch 1590, training loss: 83.79692840576172 = 0.28725558519363403 + 10.0 * 8.350967407226562
Epoch 1590, val loss: 0.4477805197238922
Epoch 1600, training loss: 83.74238586425781 = 0.28587713837623596 + 10.0 * 8.345651626586914
Epoch 1600, val loss: 0.4479958713054657
Epoch 1610, training loss: 83.79691314697266 = 0.2845039963722229 + 10.0 * 8.351240158081055
Epoch 1610, val loss: 0.4480920135974884
Epoch 1620, training loss: 83.7364273071289 = 0.2831162214279175 + 10.0 * 8.345331192016602
Epoch 1620, val loss: 0.44802770018577576
Epoch 1630, training loss: 83.73452758789062 = 0.28172436356544495 + 10.0 * 8.345280647277832
Epoch 1630, val loss: 0.4480084776878357
Epoch 1640, training loss: 83.72370147705078 = 0.28033363819122314 + 10.0 * 8.34433650970459
Epoch 1640, val loss: 0.44818902015686035
Epoch 1650, training loss: 83.71400451660156 = 0.2789449989795685 + 10.0 * 8.343505859375
Epoch 1650, val loss: 0.44822654128074646
Epoch 1660, training loss: 83.7108154296875 = 0.2775505781173706 + 10.0 * 8.343326568603516
Epoch 1660, val loss: 0.4483821988105774
Epoch 1670, training loss: 83.73155212402344 = 0.2761527895927429 + 10.0 * 8.345540046691895
Epoch 1670, val loss: 0.44845297932624817
Epoch 1680, training loss: 83.75003051757812 = 0.2747266888618469 + 10.0 * 8.347530364990234
Epoch 1680, val loss: 0.4486888349056244
Epoch 1690, training loss: 83.71721649169922 = 0.2732760012149811 + 10.0 * 8.344393730163574
Epoch 1690, val loss: 0.44899487495422363
Epoch 1700, training loss: 83.69863891601562 = 0.27185046672821045 + 10.0 * 8.342679023742676
Epoch 1700, val loss: 0.44909051060676575
Epoch 1710, training loss: 83.69648742675781 = 0.27042293548583984 + 10.0 * 8.342606544494629
Epoch 1710, val loss: 0.44938209652900696
Epoch 1720, training loss: 83.73885345458984 = 0.2689894437789917 + 10.0 * 8.346986770629883
Epoch 1720, val loss: 0.4495394229888916
Epoch 1730, training loss: 83.69557189941406 = 0.2675234377384186 + 10.0 * 8.342804908752441
Epoch 1730, val loss: 0.4498712718486786
Epoch 1740, training loss: 83.6788330078125 = 0.26606470346450806 + 10.0 * 8.341276168823242
Epoch 1740, val loss: 0.4501645565032959
Epoch 1750, training loss: 83.67279052734375 = 0.2646023631095886 + 10.0 * 8.340818405151367
Epoch 1750, val loss: 0.4504600763320923
Epoch 1760, training loss: 83.674072265625 = 0.2631339728832245 + 10.0 * 8.341094017028809
Epoch 1760, val loss: 0.4508068263530731
Epoch 1770, training loss: 83.70619201660156 = 0.26165395975112915 + 10.0 * 8.344453811645508
Epoch 1770, val loss: 0.4511834979057312
Epoch 1780, training loss: 83.72785949707031 = 0.2601647675037384 + 10.0 * 8.346769332885742
Epoch 1780, val loss: 0.45165279507637024
Epoch 1790, training loss: 83.67913818359375 = 0.25865259766578674 + 10.0 * 8.342048645019531
Epoch 1790, val loss: 0.45194435119628906
Epoch 1800, training loss: 83.66099548339844 = 0.25717049837112427 + 10.0 * 8.34038257598877
Epoch 1800, val loss: 0.45230135321617126
Epoch 1810, training loss: 83.64700317382812 = 0.2556688189506531 + 10.0 * 8.339133262634277
Epoch 1810, val loss: 0.45281434059143066
Epoch 1820, training loss: 83.64183044433594 = 0.2541811764240265 + 10.0 * 8.338765144348145
Epoch 1820, val loss: 0.4532448351383209
Epoch 1830, training loss: 83.63745880126953 = 0.2526785731315613 + 10.0 * 8.338478088378906
Epoch 1830, val loss: 0.4537845849990845
Epoch 1840, training loss: 83.66021728515625 = 0.2511713206768036 + 10.0 * 8.340904235839844
Epoch 1840, val loss: 0.45439642667770386
Epoch 1850, training loss: 83.63996124267578 = 0.24964521825313568 + 10.0 * 8.339032173156738
Epoch 1850, val loss: 0.4548843502998352
Epoch 1860, training loss: 83.63102722167969 = 0.24811327457427979 + 10.0 * 8.33829116821289
Epoch 1860, val loss: 0.4553893208503723
Epoch 1870, training loss: 83.63689422607422 = 0.24657641351222992 + 10.0 * 8.339032173156738
Epoch 1870, val loss: 0.4559941291809082
Epoch 1880, training loss: 83.63006591796875 = 0.2450258880853653 + 10.0 * 8.33850383758545
Epoch 1880, val loss: 0.4566151797771454
Epoch 1890, training loss: 83.639404296875 = 0.24347862601280212 + 10.0 * 8.339592933654785
Epoch 1890, val loss: 0.4572044312953949
Epoch 1900, training loss: 83.6589126586914 = 0.2419186383485794 + 10.0 * 8.341699600219727
Epoch 1900, val loss: 0.45786210894584656
Epoch 1910, training loss: 83.62074279785156 = 0.24034848809242249 + 10.0 * 8.33803939819336
Epoch 1910, val loss: 0.4584946632385254
Epoch 1920, training loss: 83.61272430419922 = 0.23879404366016388 + 10.0 * 8.337392807006836
Epoch 1920, val loss: 0.4590161144733429
Epoch 1930, training loss: 83.60277557373047 = 0.23722556233406067 + 10.0 * 8.336554527282715
Epoch 1930, val loss: 0.4598797857761383
Epoch 1940, training loss: 83.59420013427734 = 0.23566339910030365 + 10.0 * 8.335853576660156
Epoch 1940, val loss: 0.46055305004119873
Epoch 1950, training loss: 83.58920288085938 = 0.23409800231456757 + 10.0 * 8.33551025390625
Epoch 1950, val loss: 0.46130454540252686
Epoch 1960, training loss: 83.58837127685547 = 0.23252086341381073 + 10.0 * 8.33558464050293
Epoch 1960, val loss: 0.46207717061042786
Epoch 1970, training loss: 83.66787719726562 = 0.23095545172691345 + 10.0 * 8.3436918258667
Epoch 1970, val loss: 0.4626765251159668
Epoch 1980, training loss: 83.62090301513672 = 0.22931653261184692 + 10.0 * 8.33915901184082
Epoch 1980, val loss: 0.4639040231704712
Epoch 1990, training loss: 83.59280395507812 = 0.22771424055099487 + 10.0 * 8.336508750915527
Epoch 1990, val loss: 0.4644899368286133
Epoch 2000, training loss: 83.57893371582031 = 0.2261115312576294 + 10.0 * 8.335282325744629
Epoch 2000, val loss: 0.4655666947364807
Epoch 2010, training loss: 83.57138061523438 = 0.22450551390647888 + 10.0 * 8.334688186645508
Epoch 2010, val loss: 0.46640071272850037
Epoch 2020, training loss: 83.58930206298828 = 0.22289586067199707 + 10.0 * 8.336641311645508
Epoch 2020, val loss: 0.4673726260662079
Epoch 2030, training loss: 83.56631469726562 = 0.2212652564048767 + 10.0 * 8.334505081176758
Epoch 2030, val loss: 0.4684264659881592
Epoch 2040, training loss: 83.58458709716797 = 0.21963922679424286 + 10.0 * 8.336494445800781
Epoch 2040, val loss: 0.46949052810668945
Epoch 2050, training loss: 83.56803894042969 = 0.21801108121871948 + 10.0 * 8.335002899169922
Epoch 2050, val loss: 0.4705139398574829
Epoch 2060, training loss: 83.55598449707031 = 0.216388538479805 + 10.0 * 8.333959579467773
Epoch 2060, val loss: 0.4715682566165924
Epoch 2070, training loss: 83.59031677246094 = 0.21475844085216522 + 10.0 * 8.337555885314941
Epoch 2070, val loss: 0.47275254130363464
Epoch 2080, training loss: 83.56182098388672 = 0.21311186254024506 + 10.0 * 8.334871292114258
Epoch 2080, val loss: 0.47394421696662903
Epoch 2090, training loss: 83.55047607421875 = 0.21147695183753967 + 10.0 * 8.33389949798584
Epoch 2090, val loss: 0.4748772978782654
Epoch 2100, training loss: 83.53939819335938 = 0.2098167985677719 + 10.0 * 8.332958221435547
Epoch 2100, val loss: 0.47627556324005127
Epoch 2110, training loss: 83.53305053710938 = 0.2081710398197174 + 10.0 * 8.332488059997559
Epoch 2110, val loss: 0.47745001316070557
Epoch 2120, training loss: 83.54612731933594 = 0.20652049779891968 + 10.0 * 8.33396053314209
Epoch 2120, val loss: 0.47868233919143677
Epoch 2130, training loss: 83.55104064941406 = 0.20486272871494293 + 10.0 * 8.334617614746094
Epoch 2130, val loss: 0.4800148904323578
Epoch 2140, training loss: 83.54499816894531 = 0.2031976878643036 + 10.0 * 8.334179878234863
Epoch 2140, val loss: 0.48138177394866943
Epoch 2150, training loss: 83.57368469238281 = 0.20152953267097473 + 10.0 * 8.337215423583984
Epoch 2150, val loss: 0.48292145133018494
Epoch 2160, training loss: 83.52617645263672 = 0.19985167682170868 + 10.0 * 8.332632064819336
Epoch 2160, val loss: 0.48435866832733154
Epoch 2170, training loss: 83.51274871826172 = 0.19818982481956482 + 10.0 * 8.331456184387207
Epoch 2170, val loss: 0.48571643233299255
Epoch 2180, training loss: 83.51404571533203 = 0.19652815163135529 + 10.0 * 8.331751823425293
Epoch 2180, val loss: 0.4872293770313263
Epoch 2190, training loss: 83.54801177978516 = 0.19486446678638458 + 10.0 * 8.335314750671387
Epoch 2190, val loss: 0.48885154724121094
Epoch 2200, training loss: 83.50714111328125 = 0.19319966435432434 + 10.0 * 8.33139419555664
Epoch 2200, val loss: 0.4902023375034332
Epoch 2210, training loss: 83.5085678100586 = 0.1915518194437027 + 10.0 * 8.331701278686523
Epoch 2210, val loss: 0.49155935645103455
Epoch 2220, training loss: 83.49372100830078 = 0.18988573551177979 + 10.0 * 8.33038330078125
Epoch 2220, val loss: 0.49320629239082336
Epoch 2230, training loss: 83.50651550292969 = 0.18823032081127167 + 10.0 * 8.331828117370605
Epoch 2230, val loss: 0.49484747648239136
Epoch 2240, training loss: 83.51887512207031 = 0.1865723580121994 + 10.0 * 8.333230972290039
Epoch 2240, val loss: 0.4966021776199341
Epoch 2250, training loss: 83.5119857788086 = 0.18491914868354797 + 10.0 * 8.332706451416016
Epoch 2250, val loss: 0.49821898341178894
Epoch 2260, training loss: 83.52299499511719 = 0.18326202034950256 + 10.0 * 8.333972930908203
Epoch 2260, val loss: 0.49989569187164307
Epoch 2270, training loss: 83.499267578125 = 0.1816168576478958 + 10.0 * 8.331765174865723
Epoch 2270, val loss: 0.5016493201255798
Epoch 2280, training loss: 83.47652435302734 = 0.1799740344285965 + 10.0 * 8.329654693603516
Epoch 2280, val loss: 0.5032227635383606
Epoch 2290, training loss: 83.47136688232422 = 0.1783367246389389 + 10.0 * 8.329302787780762
Epoch 2290, val loss: 0.5049528479576111
Epoch 2300, training loss: 83.48977661132812 = 0.1767120510339737 + 10.0 * 8.331306457519531
Epoch 2300, val loss: 0.5065817832946777
Epoch 2310, training loss: 83.50183868408203 = 0.1750769168138504 + 10.0 * 8.33267593383789
Epoch 2310, val loss: 0.5084401965141296
Epoch 2320, training loss: 83.47480773925781 = 0.17341981828212738 + 10.0 * 8.33013916015625
Epoch 2320, val loss: 0.5104537606239319
Epoch 2330, training loss: 83.46305847167969 = 0.17178502678871155 + 10.0 * 8.329127311706543
Epoch 2330, val loss: 0.5122320055961609
Epoch 2340, training loss: 83.45929718017578 = 0.17015591263771057 + 10.0 * 8.328913688659668
Epoch 2340, val loss: 0.5141355395317078
Epoch 2350, training loss: 83.4911880493164 = 0.1685282289981842 + 10.0 * 8.332265853881836
Epoch 2350, val loss: 0.5161407589912415
Epoch 2360, training loss: 83.48094940185547 = 0.16690340638160706 + 10.0 * 8.331404685974121
Epoch 2360, val loss: 0.5181536078453064
Epoch 2370, training loss: 83.44959259033203 = 0.1652790606021881 + 10.0 * 8.328432083129883
Epoch 2370, val loss: 0.5198893547058105
Epoch 2380, training loss: 83.4399185180664 = 0.16365747153759003 + 10.0 * 8.32762622833252
Epoch 2380, val loss: 0.521816074848175
Epoch 2390, training loss: 83.43753051757812 = 0.1620372086763382 + 10.0 * 8.32754898071289
Epoch 2390, val loss: 0.5239049196243286
Epoch 2400, training loss: 83.44802856445312 = 0.16043274104595184 + 10.0 * 8.328760147094727
Epoch 2400, val loss: 0.5257232785224915
Epoch 2410, training loss: 83.49195861816406 = 0.15882521867752075 + 10.0 * 8.33331298828125
Epoch 2410, val loss: 0.5276559591293335
Epoch 2420, training loss: 83.43820190429688 = 0.15719807147979736 + 10.0 * 8.328100204467773
Epoch 2420, val loss: 0.5298773050308228
Epoch 2430, training loss: 83.42638397216797 = 0.1556020975112915 + 10.0 * 8.327077865600586
Epoch 2430, val loss: 0.5320128798484802
Epoch 2440, training loss: 83.42810821533203 = 0.15401296317577362 + 10.0 * 8.327409744262695
Epoch 2440, val loss: 0.5341136455535889
Epoch 2450, training loss: 83.44329071044922 = 0.1524343192577362 + 10.0 * 8.329085350036621
Epoch 2450, val loss: 0.5363243818283081
Epoch 2460, training loss: 83.4161605834961 = 0.1508513242006302 + 10.0 * 8.326531410217285
Epoch 2460, val loss: 0.5385403037071228
Epoch 2470, training loss: 83.48550415039062 = 0.1492912471294403 + 10.0 * 8.33362102508545
Epoch 2470, val loss: 0.5410676002502441
Epoch 2480, training loss: 83.41793823242188 = 0.14772315323352814 + 10.0 * 8.327021598815918
Epoch 2480, val loss: 0.5429715514183044
Epoch 2490, training loss: 83.40721893310547 = 0.14617276191711426 + 10.0 * 8.326105117797852
Epoch 2490, val loss: 0.5452117919921875
Epoch 2500, training loss: 83.40070343017578 = 0.14461857080459595 + 10.0 * 8.32560920715332
Epoch 2500, val loss: 0.5476093888282776
Epoch 2510, training loss: 83.41061401367188 = 0.1430843621492386 + 10.0 * 8.326753616333008
Epoch 2510, val loss: 0.5499605536460876
Epoch 2520, training loss: 83.43650817871094 = 0.1415533721446991 + 10.0 * 8.329495429992676
Epoch 2520, val loss: 0.552528977394104
Epoch 2530, training loss: 83.43028259277344 = 0.1400221586227417 + 10.0 * 8.329026222229004
Epoch 2530, val loss: 0.5548715591430664
Epoch 2540, training loss: 83.39471435546875 = 0.1384955197572708 + 10.0 * 8.325621604919434
Epoch 2540, val loss: 0.5570743680000305
Epoch 2550, training loss: 83.38994598388672 = 0.13698576390743256 + 10.0 * 8.325296401977539
Epoch 2550, val loss: 0.5595281720161438
Epoch 2560, training loss: 83.38363647460938 = 0.1354808658361435 + 10.0 * 8.32481575012207
Epoch 2560, val loss: 0.5619823336601257
Epoch 2570, training loss: 83.38646697998047 = 0.13398544490337372 + 10.0 * 8.325247764587402
Epoch 2570, val loss: 0.5646624565124512
Epoch 2580, training loss: 83.40521240234375 = 0.1324976682662964 + 10.0 * 8.327271461486816
Epoch 2580, val loss: 0.5674816370010376
Epoch 2590, training loss: 83.45710754394531 = 0.13104180991649628 + 10.0 * 8.332606315612793
Epoch 2590, val loss: 0.5702621340751648
Epoch 2600, training loss: 83.40087890625 = 0.1295747607946396 + 10.0 * 8.327130317687988
Epoch 2600, val loss: 0.5722653865814209
Epoch 2610, training loss: 83.38224792480469 = 0.12811556458473206 + 10.0 * 8.325413703918457
Epoch 2610, val loss: 0.5751571655273438
Epoch 2620, training loss: 83.36917114257812 = 0.12667293846607208 + 10.0 * 8.324250221252441
Epoch 2620, val loss: 0.577808678150177
Epoch 2630, training loss: 83.35993194580078 = 0.12523972988128662 + 10.0 * 8.323469161987305
Epoch 2630, val loss: 0.5805569291114807
Epoch 2640, training loss: 83.35731506347656 = 0.12382573634386063 + 10.0 * 8.323348999023438
Epoch 2640, val loss: 0.583217203617096
Epoch 2650, training loss: 83.4012451171875 = 0.12242987751960754 + 10.0 * 8.327881813049316
Epoch 2650, val loss: 0.5859540104866028
Epoch 2660, training loss: 83.3565444946289 = 0.12101301550865173 + 10.0 * 8.323553085327148
Epoch 2660, val loss: 0.5887530446052551
Epoch 2670, training loss: 83.35945892333984 = 0.1196204200387001 + 10.0 * 8.323984146118164
Epoch 2670, val loss: 0.5916100144386292
Epoch 2680, training loss: 83.36577606201172 = 0.11823904514312744 + 10.0 * 8.324753761291504
Epoch 2680, val loss: 0.5942440032958984
Epoch 2690, training loss: 83.3609848022461 = 0.11687491089105606 + 10.0 * 8.324411392211914
Epoch 2690, val loss: 0.5969924926757812
Epoch 2700, training loss: 83.39844512939453 = 0.11555137485265732 + 10.0 * 8.328289031982422
Epoch 2700, val loss: 0.5997205376625061
Epoch 2710, training loss: 83.34025573730469 = 0.11415700614452362 + 10.0 * 8.322609901428223
Epoch 2710, val loss: 0.6030233502388
Epoch 2720, training loss: 83.3409194946289 = 0.11282927542924881 + 10.0 * 8.322809219360352
Epoch 2720, val loss: 0.6060017347335815
Epoch 2730, training loss: 83.34158325195312 = 0.11151698231697083 + 10.0 * 8.323006629943848
Epoch 2730, val loss: 0.6086459755897522
Epoch 2740, training loss: 83.36750030517578 = 0.11021748930215836 + 10.0 * 8.325728416442871
Epoch 2740, val loss: 0.6116299629211426
Epoch 2750, training loss: 83.34861755371094 = 0.10891884565353394 + 10.0 * 8.323969841003418
Epoch 2750, val loss: 0.6147224307060242
Epoch 2760, training loss: 83.33688354492188 = 0.10763774812221527 + 10.0 * 8.322924613952637
Epoch 2760, val loss: 0.6172856092453003
Epoch 2770, training loss: 83.40653991699219 = 0.10638722777366638 + 10.0 * 8.330015182495117
Epoch 2770, val loss: 0.6202561259269714
Epoch 2780, training loss: 83.34075927734375 = 0.10509757697582245 + 10.0 * 8.323566436767578
Epoch 2780, val loss: 0.6234673261642456
Epoch 2790, training loss: 83.32733154296875 = 0.10385176539421082 + 10.0 * 8.322347640991211
Epoch 2790, val loss: 0.626478910446167
Epoch 2800, training loss: 83.32000732421875 = 0.10260751843452454 + 10.0 * 8.32174015045166
Epoch 2800, val loss: 0.6294479370117188
Epoch 2810, training loss: 83.31783294677734 = 0.1013888567686081 + 10.0 * 8.32164478302002
Epoch 2810, val loss: 0.6325177550315857
Epoch 2820, training loss: 83.36952209472656 = 0.10018867254257202 + 10.0 * 8.326932907104492
Epoch 2820, val loss: 0.6361545920372009
Epoch 2830, training loss: 83.3298110961914 = 0.09897234290838242 + 10.0 * 8.323083877563477
Epoch 2830, val loss: 0.6388952732086182
Epoch 2840, training loss: 83.30973052978516 = 0.09778249263763428 + 10.0 * 8.321194648742676
Epoch 2840, val loss: 0.6419233679771423
Epoch 2850, training loss: 83.30351257324219 = 0.09660588949918747 + 10.0 * 8.320691108703613
Epoch 2850, val loss: 0.6449592113494873
Epoch 2860, training loss: 83.30315399169922 = 0.09543975442647934 + 10.0 * 8.320772171020508
Epoch 2860, val loss: 0.6480385661125183
Epoch 2870, training loss: 83.34613037109375 = 0.09429629147052765 + 10.0 * 8.325182914733887
Epoch 2870, val loss: 0.6510455012321472
Epoch 2880, training loss: 83.32461547851562 = 0.09314150363206863 + 10.0 * 8.323147773742676
Epoch 2880, val loss: 0.6545717716217041
Epoch 2890, training loss: 83.34115600585938 = 0.0920112356543541 + 10.0 * 8.324914932250977
Epoch 2890, val loss: 0.6577925086021423
Epoch 2900, training loss: 83.30502319335938 = 0.09087913483381271 + 10.0 * 8.32141399383545
Epoch 2900, val loss: 0.6607081890106201
Epoch 2910, training loss: 83.2944107055664 = 0.0897734984755516 + 10.0 * 8.320463180541992
Epoch 2910, val loss: 0.6638748049736023
Epoch 2920, training loss: 83.29771423339844 = 0.08867653459310532 + 10.0 * 8.320903778076172
Epoch 2920, val loss: 0.6671311259269714
Epoch 2930, training loss: 83.31690216064453 = 0.0876079872250557 + 10.0 * 8.322929382324219
Epoch 2930, val loss: 0.670196533203125
Epoch 2940, training loss: 83.31040954589844 = 0.0865243673324585 + 10.0 * 8.322388648986816
Epoch 2940, val loss: 0.6734133958816528
Epoch 2950, training loss: 83.29889678955078 = 0.0854574516415596 + 10.0 * 8.321344375610352
Epoch 2950, val loss: 0.6767527461051941
Epoch 2960, training loss: 83.28901672363281 = 0.0844075158238411 + 10.0 * 8.32046127319336
Epoch 2960, val loss: 0.6802934408187866
Epoch 2970, training loss: 83.31431579589844 = 0.0833786353468895 + 10.0 * 8.32309341430664
Epoch 2970, val loss: 0.683634340763092
Epoch 2980, training loss: 83.2920150756836 = 0.08234544843435287 + 10.0 * 8.320966720581055
Epoch 2980, val loss: 0.6867632269859314
Epoch 2990, training loss: 83.2778549194336 = 0.08133038133382797 + 10.0 * 8.319652557373047
Epoch 2990, val loss: 0.6898589134216309
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148148
0.8428602477722235
=== training gcn model ===
Epoch 0, training loss: 106.91510009765625 = 1.0922354459762573 + 10.0 * 10.582286834716797
Epoch 0, val loss: 1.090360403060913
Epoch 10, training loss: 106.90067291259766 = 1.08596670627594 + 10.0 * 10.581470489501953
Epoch 10, val loss: 1.0842126607894897
Epoch 20, training loss: 106.80947875976562 = 1.078845500946045 + 10.0 * 10.573063850402832
Epoch 20, val loss: 1.0772430896759033
Epoch 30, training loss: 106.16997528076172 = 1.0707502365112305 + 10.0 * 10.509922981262207
Epoch 30, val loss: 1.0693801641464233
Epoch 40, training loss: 103.53425598144531 = 1.0629549026489258 + 10.0 * 10.247129440307617
Epoch 40, val loss: 1.0621016025543213
Epoch 50, training loss: 101.37459564208984 = 1.056896686553955 + 10.0 * 10.031769752502441
Epoch 50, val loss: 1.0563628673553467
Epoch 60, training loss: 98.73526000976562 = 1.0488786697387695 + 10.0 * 9.768637657165527
Epoch 60, val loss: 1.047976016998291
Epoch 70, training loss: 94.98329162597656 = 1.037169337272644 + 10.0 * 9.394612312316895
Epoch 70, val loss: 1.0362751483917236
Epoch 80, training loss: 93.32884979248047 = 1.0285735130310059 + 10.0 * 9.23002815246582
Epoch 80, val loss: 1.0281838178634644
Epoch 90, training loss: 92.85945129394531 = 1.021234154701233 + 10.0 * 9.183821678161621
Epoch 90, val loss: 1.0212266445159912
Epoch 100, training loss: 91.83564758300781 = 1.0139204263687134 + 10.0 * 9.082172393798828
Epoch 100, val loss: 1.014567494392395
Epoch 110, training loss: 90.51469421386719 = 1.0086520910263062 + 10.0 * 8.950604438781738
Epoch 110, val loss: 1.0096361637115479
Epoch 120, training loss: 89.66565704345703 = 1.0024197101593018 + 10.0 * 8.866323471069336
Epoch 120, val loss: 1.0033884048461914
Epoch 130, training loss: 89.09735107421875 = 0.9936370253562927 + 10.0 * 8.810371398925781
Epoch 130, val loss: 0.9949327111244202
Epoch 140, training loss: 88.65673065185547 = 0.9829625487327576 + 10.0 * 8.767376899719238
Epoch 140, val loss: 0.9847057461738586
Epoch 150, training loss: 88.32402801513672 = 0.970979630947113 + 10.0 * 8.735304832458496
Epoch 150, val loss: 0.9731665849685669
Epoch 160, training loss: 88.02670288085938 = 0.9573493003845215 + 10.0 * 8.706934928894043
Epoch 160, val loss: 0.9600707292556763
Epoch 170, training loss: 87.76748657226562 = 0.9421543478965759 + 10.0 * 8.682533264160156
Epoch 170, val loss: 0.9456574320793152
Epoch 180, training loss: 87.54580688476562 = 0.9258465766906738 + 10.0 * 8.661995887756348
Epoch 180, val loss: 0.9301688075065613
Epoch 190, training loss: 87.33122253417969 = 0.9084530472755432 + 10.0 * 8.642276763916016
Epoch 190, val loss: 0.9137182831764221
Epoch 200, training loss: 87.15242767333984 = 0.8901616930961609 + 10.0 * 8.626226425170898
Epoch 200, val loss: 0.896447479724884
Epoch 210, training loss: 86.95307922363281 = 0.8715725541114807 + 10.0 * 8.608150482177734
Epoch 210, val loss: 0.8789739012718201
Epoch 220, training loss: 86.78308868408203 = 0.8529226779937744 + 10.0 * 8.593016624450684
Epoch 220, val loss: 0.8615265488624573
Epoch 230, training loss: 86.61456298828125 = 0.834003210067749 + 10.0 * 8.578056335449219
Epoch 230, val loss: 0.8439515829086304
Epoch 240, training loss: 86.46808624267578 = 0.815268337726593 + 10.0 * 8.565281867980957
Epoch 240, val loss: 0.8266244530677795
Epoch 250, training loss: 86.34930419921875 = 0.7968575954437256 + 10.0 * 8.555244445800781
Epoch 250, val loss: 0.8097051382064819
Epoch 260, training loss: 86.26066589355469 = 0.7787594199180603 + 10.0 * 8.548190116882324
Epoch 260, val loss: 0.7931504845619202
Epoch 270, training loss: 86.1508560180664 = 0.761159360408783 + 10.0 * 8.538969039916992
Epoch 270, val loss: 0.7771458029747009
Epoch 280, training loss: 86.05230712890625 = 0.7441430687904358 + 10.0 * 8.530817031860352
Epoch 280, val loss: 0.7617313861846924
Epoch 290, training loss: 86.02064514160156 = 0.7275553941726685 + 10.0 * 8.529309272766113
Epoch 290, val loss: 0.7467582821846008
Epoch 300, training loss: 85.93841552734375 = 0.711230456829071 + 10.0 * 8.52271842956543
Epoch 300, val loss: 0.7320767641067505
Epoch 310, training loss: 85.82472229003906 = 0.6955171227455139 + 10.0 * 8.512920379638672
Epoch 310, val loss: 0.7179822325706482
Epoch 320, training loss: 85.76152801513672 = 0.6803593039512634 + 10.0 * 8.508116722106934
Epoch 320, val loss: 0.7044908404350281
Epoch 330, training loss: 85.69367218017578 = 0.6655638217926025 + 10.0 * 8.50281047821045
Epoch 330, val loss: 0.6913578510284424
Epoch 340, training loss: 85.63423156738281 = 0.6512166261672974 + 10.0 * 8.49830150604248
Epoch 340, val loss: 0.6786243915557861
Epoch 350, training loss: 85.64493560791016 = 0.637395977973938 + 10.0 * 8.500753402709961
Epoch 350, val loss: 0.6664116382598877
Epoch 360, training loss: 85.53964233398438 = 0.623947024345398 + 10.0 * 8.491569519042969
Epoch 360, val loss: 0.6546096205711365
Epoch 370, training loss: 85.49833679199219 = 0.6113867163658142 + 10.0 * 8.48869514465332
Epoch 370, val loss: 0.6437137722969055
Epoch 380, training loss: 85.43009185791016 = 0.5996627807617188 + 10.0 * 8.48304271697998
Epoch 380, val loss: 0.6336437463760376
Epoch 390, training loss: 85.388427734375 = 0.5885850191116333 + 10.0 * 8.479984283447266
Epoch 390, val loss: 0.6241312623023987
Epoch 400, training loss: 85.35413360595703 = 0.5781644582748413 + 10.0 * 8.4775972366333
Epoch 400, val loss: 0.6152088642120361
Epoch 410, training loss: 85.32848358154297 = 0.5684626698493958 + 10.0 * 8.476001739501953
Epoch 410, val loss: 0.6071321368217468
Epoch 420, training loss: 85.26837158203125 = 0.5595075488090515 + 10.0 * 8.47088623046875
Epoch 420, val loss: 0.5996511578559875
Epoch 430, training loss: 85.22611999511719 = 0.5512731075286865 + 10.0 * 8.467485427856445
Epoch 430, val loss: 0.5929881930351257
Epoch 440, training loss: 85.19358825683594 = 0.5436357259750366 + 10.0 * 8.464995384216309
Epoch 440, val loss: 0.5868659615516663
Epoch 450, training loss: 85.1452407836914 = 0.536587655544281 + 10.0 * 8.460865020751953
Epoch 450, val loss: 0.5812476873397827
Epoch 460, training loss: 85.12281036376953 = 0.530080258846283 + 10.0 * 8.459272384643555
Epoch 460, val loss: 0.5762276649475098
Epoch 470, training loss: 85.08464813232422 = 0.5240576863288879 + 10.0 * 8.456059455871582
Epoch 470, val loss: 0.5716375112533569
Epoch 480, training loss: 85.07318115234375 = 0.518491804599762 + 10.0 * 8.455469131469727
Epoch 480, val loss: 0.5675623416900635
Epoch 490, training loss: 85.02710723876953 = 0.5133394598960876 + 10.0 * 8.451376914978027
Epoch 490, val loss: 0.5638571977615356
Epoch 500, training loss: 84.98416900634766 = 0.508580207824707 + 10.0 * 8.447558403015137
Epoch 500, val loss: 0.5605173707008362
Epoch 510, training loss: 84.97126770019531 = 0.5041347146034241 + 10.0 * 8.4467134475708
Epoch 510, val loss: 0.5574510097503662
Epoch 520, training loss: 84.95291900634766 = 0.49993109703063965 + 10.0 * 8.44529914855957
Epoch 520, val loss: 0.554660975933075
Epoch 530, training loss: 84.93463897705078 = 0.4959750175476074 + 10.0 * 8.443866729736328
Epoch 530, val loss: 0.5520074367523193
Epoch 540, training loss: 84.88060760498047 = 0.49229419231414795 + 10.0 * 8.438831329345703
Epoch 540, val loss: 0.5495764017105103
Epoch 550, training loss: 84.8649673461914 = 0.4888264834880829 + 10.0 * 8.437614440917969
Epoch 550, val loss: 0.5474333167076111
Epoch 560, training loss: 84.8491439819336 = 0.48549509048461914 + 10.0 * 8.436365127563477
Epoch 560, val loss: 0.5453358888626099
Epoch 570, training loss: 84.8177261352539 = 0.48235204815864563 + 10.0 * 8.433537483215332
Epoch 570, val loss: 0.5434921979904175
Epoch 580, training loss: 84.78646850585938 = 0.4793567955493927 + 10.0 * 8.43071174621582
Epoch 580, val loss: 0.541667103767395
Epoch 590, training loss: 84.7614974975586 = 0.4764440357685089 + 10.0 * 8.428505897521973
Epoch 590, val loss: 0.5400392413139343
Epoch 600, training loss: 84.75821685791016 = 0.47366559505462646 + 10.0 * 8.428455352783203
Epoch 600, val loss: 0.5383625030517578
Epoch 610, training loss: 84.74310302734375 = 0.4710175693035126 + 10.0 * 8.42720890045166
Epoch 610, val loss: 0.536914050579071
Epoch 620, training loss: 84.69579315185547 = 0.4684278666973114 + 10.0 * 8.422736167907715
Epoch 620, val loss: 0.535469114780426
Epoch 630, training loss: 84.68651580810547 = 0.4659633934497833 + 10.0 * 8.4220552444458
Epoch 630, val loss: 0.5341866612434387
Epoch 640, training loss: 84.7304916381836 = 0.4635603129863739 + 10.0 * 8.426692962646484
Epoch 640, val loss: 0.5329805612564087
Epoch 650, training loss: 84.6722183227539 = 0.4612045884132385 + 10.0 * 8.421101570129395
Epoch 650, val loss: 0.5315329432487488
Epoch 660, training loss: 84.63908386230469 = 0.4589298963546753 + 10.0 * 8.418015480041504
Epoch 660, val loss: 0.530375599861145
Epoch 670, training loss: 84.61080932617188 = 0.4567469358444214 + 10.0 * 8.415406227111816
Epoch 670, val loss: 0.5293184518814087
Epoch 680, training loss: 84.58705139160156 = 0.45463621616363525 + 10.0 * 8.413241386413574
Epoch 680, val loss: 0.5281811356544495
Epoch 690, training loss: 84.59171295166016 = 0.45257410407066345 + 10.0 * 8.41391372680664
Epoch 690, val loss: 0.5271915197372437
Epoch 700, training loss: 84.56327819824219 = 0.4504925310611725 + 10.0 * 8.41127872467041
Epoch 700, val loss: 0.5260807871818542
Epoch 710, training loss: 84.55430603027344 = 0.4485119879245758 + 10.0 * 8.410579681396484
Epoch 710, val loss: 0.5251451134681702
Epoch 720, training loss: 84.56330871582031 = 0.44658157229423523 + 10.0 * 8.411672592163086
Epoch 720, val loss: 0.5242201685905457
Epoch 730, training loss: 84.51544952392578 = 0.4446698725223541 + 10.0 * 8.40707778930664
Epoch 730, val loss: 0.5231586694717407
Epoch 740, training loss: 84.50281524658203 = 0.44283169507980347 + 10.0 * 8.405998229980469
Epoch 740, val loss: 0.5223804116249084
Epoch 750, training loss: 84.54410552978516 = 0.4410163164138794 + 10.0 * 8.410308837890625
Epoch 750, val loss: 0.5213480591773987
Epoch 760, training loss: 84.49046325683594 = 0.43922099471092224 + 10.0 * 8.405123710632324
Epoch 760, val loss: 0.5205857753753662
Epoch 770, training loss: 84.46803283691406 = 0.43748393654823303 + 10.0 * 8.403055191040039
Epoch 770, val loss: 0.5196771025657654
Epoch 780, training loss: 84.51245880126953 = 0.435770720243454 + 10.0 * 8.407669067382812
Epoch 780, val loss: 0.5188410878181458
Epoch 790, training loss: 84.454345703125 = 0.43404820561408997 + 10.0 * 8.402029037475586
Epoch 790, val loss: 0.5180527567863464
Epoch 800, training loss: 84.42435455322266 = 0.43239328265190125 + 10.0 * 8.399195671081543
Epoch 800, val loss: 0.5172430276870728
Epoch 810, training loss: 84.41497039794922 = 0.43076297640800476 + 10.0 * 8.398420333862305
Epoch 810, val loss: 0.5164362192153931
Epoch 820, training loss: 84.45379638671875 = 0.42915579676628113 + 10.0 * 8.402463912963867
Epoch 820, val loss: 0.5156317353248596
Epoch 830, training loss: 84.42989349365234 = 0.42751678824424744 + 10.0 * 8.400238037109375
Epoch 830, val loss: 0.5148094892501831
Epoch 840, training loss: 84.37928771972656 = 0.42593497037887573 + 10.0 * 8.39533519744873
Epoch 840, val loss: 0.5140199661254883
Epoch 850, training loss: 84.37711334228516 = 0.4244014322757721 + 10.0 * 8.395271301269531
Epoch 850, val loss: 0.513221263885498
Epoch 860, training loss: 84.38196563720703 = 0.42287540435791016 + 10.0 * 8.395909309387207
Epoch 860, val loss: 0.5124286413192749
Epoch 870, training loss: 84.36671447753906 = 0.42135825753211975 + 10.0 * 8.394536018371582
Epoch 870, val loss: 0.5116981267929077
Epoch 880, training loss: 84.3385238647461 = 0.41986820101737976 + 10.0 * 8.391865730285645
Epoch 880, val loss: 0.5109243392944336
Epoch 890, training loss: 84.33296966552734 = 0.41840270161628723 + 10.0 * 8.391456604003906
Epoch 890, val loss: 0.510252058506012
Epoch 900, training loss: 84.38603210449219 = 0.4169326722621918 + 10.0 * 8.396909713745117
Epoch 900, val loss: 0.5093705654144287
Epoch 910, training loss: 84.34882354736328 = 0.41545483469963074 + 10.0 * 8.39333724975586
Epoch 910, val loss: 0.5086914300918579
Epoch 920, training loss: 84.31256103515625 = 0.4140181541442871 + 10.0 * 8.389854431152344
Epoch 920, val loss: 0.5078540444374084
Epoch 930, training loss: 84.28948211669922 = 0.4126228392124176 + 10.0 * 8.387685775756836
Epoch 930, val loss: 0.5071178078651428
Epoch 940, training loss: 84.28124237060547 = 0.4112401008605957 + 10.0 * 8.38700008392334
Epoch 940, val loss: 0.5064409971237183
Epoch 950, training loss: 84.29254150390625 = 0.4098629653453827 + 10.0 * 8.388267517089844
Epoch 950, val loss: 0.5056967735290527
Epoch 960, training loss: 84.28384399414062 = 0.40846142172813416 + 10.0 * 8.387537956237793
Epoch 960, val loss: 0.5049707293510437
Epoch 970, training loss: 84.32825469970703 = 0.40706732869148254 + 10.0 * 8.392118453979492
Epoch 970, val loss: 0.5042981505393982
Epoch 980, training loss: 84.25921630859375 = 0.40566980838775635 + 10.0 * 8.385354995727539
Epoch 980, val loss: 0.5033911466598511
Epoch 990, training loss: 84.25040435791016 = 0.4043165147304535 + 10.0 * 8.384608268737793
Epoch 990, val loss: 0.5026956796646118
Epoch 1000, training loss: 84.23979949951172 = 0.4029739499092102 + 10.0 * 8.383682250976562
Epoch 1000, val loss: 0.501960039138794
Epoch 1010, training loss: 84.23480224609375 = 0.40163761377334595 + 10.0 * 8.383316993713379
Epoch 1010, val loss: 0.5012268424034119
Epoch 1020, training loss: 84.29920196533203 = 0.40028947591781616 + 10.0 * 8.389890670776367
Epoch 1020, val loss: 0.5003871321678162
Epoch 1030, training loss: 84.27267456054688 = 0.39892634749412537 + 10.0 * 8.387374877929688
Epoch 1030, val loss: 0.4997454285621643
Epoch 1040, training loss: 84.21329498291016 = 0.39757582545280457 + 10.0 * 8.381571769714355
Epoch 1040, val loss: 0.49899792671203613
Epoch 1050, training loss: 84.20535278320312 = 0.39626431465148926 + 10.0 * 8.380908966064453
Epoch 1050, val loss: 0.4983197748661041
Epoch 1060, training loss: 84.21925354003906 = 0.39494839310646057 + 10.0 * 8.382430076599121
Epoch 1060, val loss: 0.49769580364227295
Epoch 1070, training loss: 84.20100402832031 = 0.3936120867729187 + 10.0 * 8.380739212036133
Epoch 1070, val loss: 0.4968370497226715
Epoch 1080, training loss: 84.221923828125 = 0.3922850787639618 + 10.0 * 8.382963180541992
Epoch 1080, val loss: 0.49610987305641174
Epoch 1090, training loss: 84.18457794189453 = 0.3909357786178589 + 10.0 * 8.379364013671875
Epoch 1090, val loss: 0.4954652488231659
Epoch 1100, training loss: 84.17894744873047 = 0.3896099925041199 + 10.0 * 8.378933906555176
Epoch 1100, val loss: 0.4947658181190491
Epoch 1110, training loss: 84.16739654541016 = 0.3882859945297241 + 10.0 * 8.377911567687988
Epoch 1110, val loss: 0.4940922260284424
Epoch 1120, training loss: 84.17665100097656 = 0.38696497678756714 + 10.0 * 8.378969192504883
Epoch 1120, val loss: 0.49344685673713684
Epoch 1130, training loss: 84.18870544433594 = 0.3856239318847656 + 10.0 * 8.380308151245117
Epoch 1130, val loss: 0.4926772713661194
Epoch 1140, training loss: 84.1583480834961 = 0.3842752277851105 + 10.0 * 8.37740707397461
Epoch 1140, val loss: 0.4919464886188507
Epoch 1150, training loss: 84.1962661743164 = 0.38294216990470886 + 10.0 * 8.381332397460938
Epoch 1150, val loss: 0.4912576973438263
Epoch 1160, training loss: 84.13876342773438 = 0.3815966844558716 + 10.0 * 8.375716209411621
Epoch 1160, val loss: 0.4906449019908905
Epoch 1170, training loss: 84.13542175292969 = 0.38026899099349976 + 10.0 * 8.37551498413086
Epoch 1170, val loss: 0.48999205231666565
Epoch 1180, training loss: 84.14189147949219 = 0.3789399266242981 + 10.0 * 8.37629508972168
Epoch 1180, val loss: 0.48935672640800476
Epoch 1190, training loss: 84.12651824951172 = 0.3775976598262787 + 10.0 * 8.374892234802246
Epoch 1190, val loss: 0.4886626601219177
Epoch 1200, training loss: 84.12796783447266 = 0.3762541115283966 + 10.0 * 8.375171661376953
Epoch 1200, val loss: 0.48804694414138794
Epoch 1210, training loss: 84.135009765625 = 0.37490078806877136 + 10.0 * 8.37601089477539
Epoch 1210, val loss: 0.4874221980571747
Epoch 1220, training loss: 84.11679077148438 = 0.3735343813896179 + 10.0 * 8.3743257522583
Epoch 1220, val loss: 0.4867263734340668
Epoch 1230, training loss: 84.14176940917969 = 0.3721616268157959 + 10.0 * 8.376960754394531
Epoch 1230, val loss: 0.48614680767059326
Epoch 1240, training loss: 84.12960815429688 = 0.37077710032463074 + 10.0 * 8.375883102416992
Epoch 1240, val loss: 0.48549148440361023
Epoch 1250, training loss: 84.09322357177734 = 0.3693898022174835 + 10.0 * 8.372383117675781
Epoch 1250, val loss: 0.4847547113895416
Epoch 1260, training loss: 84.07845306396484 = 0.3680117726325989 + 10.0 * 8.371044158935547
Epoch 1260, val loss: 0.4841478765010834
Epoch 1270, training loss: 84.08084106445312 = 0.3666268587112427 + 10.0 * 8.371420860290527
Epoch 1270, val loss: 0.4835735857486725
Epoch 1280, training loss: 84.11848449707031 = 0.36521559953689575 + 10.0 * 8.375326156616211
Epoch 1280, val loss: 0.4829086363315582
Epoch 1290, training loss: 84.08981323242188 = 0.36378350853919983 + 10.0 * 8.372602462768555
Epoch 1290, val loss: 0.48227986693382263
Epoch 1300, training loss: 84.08924102783203 = 0.36236655712127686 + 10.0 * 8.372687339782715
Epoch 1300, val loss: 0.48170244693756104
Epoch 1310, training loss: 84.06290435791016 = 0.3609370291233063 + 10.0 * 8.370196342468262
Epoch 1310, val loss: 0.4811762571334839
Epoch 1320, training loss: 84.05023193359375 = 0.3595147430896759 + 10.0 * 8.369071960449219
Epoch 1320, val loss: 0.4806176722049713
Epoch 1330, training loss: 84.05451965332031 = 0.3580886423587799 + 10.0 * 8.369643211364746
Epoch 1330, val loss: 0.48013836145401
Epoch 1340, training loss: 84.07596588134766 = 0.35663750767707825 + 10.0 * 8.371932983398438
Epoch 1340, val loss: 0.4796351492404938
Epoch 1350, training loss: 84.06991577148438 = 0.35517558455467224 + 10.0 * 8.37147331237793
Epoch 1350, val loss: 0.4790293276309967
Epoch 1360, training loss: 84.0454330444336 = 0.3537108600139618 + 10.0 * 8.369172096252441
Epoch 1360, val loss: 0.4785009026527405
Epoch 1370, training loss: 84.0385971069336 = 0.3522431254386902 + 10.0 * 8.368635177612305
Epoch 1370, val loss: 0.4778449833393097
Epoch 1380, training loss: 84.04854583740234 = 0.35076865553855896 + 10.0 * 8.36977767944336
Epoch 1380, val loss: 0.47740909457206726
Epoch 1390, training loss: 84.0189208984375 = 0.3492768108844757 + 10.0 * 8.366964340209961
Epoch 1390, val loss: 0.4768892526626587
Epoch 1400, training loss: 84.01620483398438 = 0.34778979420661926 + 10.0 * 8.366841316223145
Epoch 1400, val loss: 0.4763968884944916
Epoch 1410, training loss: 84.0383071899414 = 0.3462905287742615 + 10.0 * 8.36920166015625
Epoch 1410, val loss: 0.47588878870010376
Epoch 1420, training loss: 84.01731872558594 = 0.3447597920894623 + 10.0 * 8.367256164550781
Epoch 1420, val loss: 0.47537484765052795
Epoch 1430, training loss: 83.99861907958984 = 0.3432391881942749 + 10.0 * 8.365537643432617
Epoch 1430, val loss: 0.4747753441333771
Epoch 1440, training loss: 83.9816665649414 = 0.3417096436023712 + 10.0 * 8.363995552062988
Epoch 1440, val loss: 0.47436174750328064
Epoch 1450, training loss: 83.98130798339844 = 0.34017398953437805 + 10.0 * 8.364113807678223
Epoch 1450, val loss: 0.4738985002040863
Epoch 1460, training loss: 84.04402160644531 = 0.33862486481666565 + 10.0 * 8.370539665222168
Epoch 1460, val loss: 0.4732940196990967
Epoch 1470, training loss: 84.0060806274414 = 0.33703529834747314 + 10.0 * 8.366904258728027
Epoch 1470, val loss: 0.47292834520339966
Epoch 1480, training loss: 83.98160552978516 = 0.33545175194740295 + 10.0 * 8.364615440368652
Epoch 1480, val loss: 0.4724442958831787
Epoch 1490, training loss: 84.00733184814453 = 0.333862841129303 + 10.0 * 8.36734676361084
Epoch 1490, val loss: 0.4719059467315674
Epoch 1500, training loss: 83.96544647216797 = 0.33225077390670776 + 10.0 * 8.363319396972656
Epoch 1500, val loss: 0.4716019332408905
Epoch 1510, training loss: 83.9841079711914 = 0.33062946796417236 + 10.0 * 8.365347862243652
Epoch 1510, val loss: 0.4711322784423828
Epoch 1520, training loss: 83.94654846191406 = 0.3289920687675476 + 10.0 * 8.36175537109375
Epoch 1520, val loss: 0.47069668769836426
Epoch 1530, training loss: 83.94281768798828 = 0.3273542821407318 + 10.0 * 8.361546516418457
Epoch 1530, val loss: 0.4703211784362793
Epoch 1540, training loss: 83.94549560546875 = 0.3256997764110565 + 10.0 * 8.361979484558105
Epoch 1540, val loss: 0.46992209553718567
Epoch 1550, training loss: 83.96458435058594 = 0.32402485609054565 + 10.0 * 8.364055633544922
Epoch 1550, val loss: 0.4695116877555847
Epoch 1560, training loss: 83.98821258544922 = 0.32233455777168274 + 10.0 * 8.36658763885498
Epoch 1560, val loss: 0.46909648180007935
Epoch 1570, training loss: 83.9350357055664 = 0.32061317563056946 + 10.0 * 8.361442565917969
Epoch 1570, val loss: 0.46858125925064087
Epoch 1580, training loss: 83.92301940917969 = 0.3189060688018799 + 10.0 * 8.360410690307617
Epoch 1580, val loss: 0.4682411253452301
Epoch 1590, training loss: 83.91057586669922 = 0.31719255447387695 + 10.0 * 8.359338760375977
Epoch 1590, val loss: 0.46786677837371826
Epoch 1600, training loss: 83.92107391357422 = 0.31546685099601746 + 10.0 * 8.360560417175293
Epoch 1600, val loss: 0.4674346148967743
Epoch 1610, training loss: 83.93881225585938 = 0.3137069344520569 + 10.0 * 8.362510681152344
Epoch 1610, val loss: 0.4670853316783905
Epoch 1620, training loss: 83.90418243408203 = 0.3119315803050995 + 10.0 * 8.359225273132324
Epoch 1620, val loss: 0.46666696667671204
Epoch 1630, training loss: 83.90966796875 = 0.31015682220458984 + 10.0 * 8.35995101928711
Epoch 1630, val loss: 0.46630850434303284
Epoch 1640, training loss: 83.9172134399414 = 0.3083594739437103 + 10.0 * 8.360885620117188
Epoch 1640, val loss: 0.46596163511276245
Epoch 1650, training loss: 83.88237762451172 = 0.3065493106842041 + 10.0 * 8.357583045959473
Epoch 1650, val loss: 0.4657314121723175
Epoch 1660, training loss: 83.89849090576172 = 0.3047344386577606 + 10.0 * 8.359375953674316
Epoch 1660, val loss: 0.4653887450695038
Epoch 1670, training loss: 83.9055404663086 = 0.30289843678474426 + 10.0 * 8.36026382446289
Epoch 1670, val loss: 0.4650603234767914
Epoch 1680, training loss: 83.8832015991211 = 0.3010476231575012 + 10.0 * 8.35821533203125
Epoch 1680, val loss: 0.46492210030555725
Epoch 1690, training loss: 83.9009017944336 = 0.29918932914733887 + 10.0 * 8.3601713180542
Epoch 1690, val loss: 0.4647027254104614
Epoch 1700, training loss: 83.87373352050781 = 0.2973058223724365 + 10.0 * 8.357643127441406
Epoch 1700, val loss: 0.4644157290458679
Epoch 1710, training loss: 83.85684967041016 = 0.2954201102256775 + 10.0 * 8.3561429977417
Epoch 1710, val loss: 0.4640984535217285
Epoch 1720, training loss: 83.847412109375 = 0.29352226853370667 + 10.0 * 8.355388641357422
Epoch 1720, val loss: 0.46386367082595825
Epoch 1730, training loss: 83.85946655273438 = 0.2916124761104584 + 10.0 * 8.356785774230957
Epoch 1730, val loss: 0.46365272998809814
Epoch 1740, training loss: 83.92167663574219 = 0.2896762788295746 + 10.0 * 8.363200187683105
Epoch 1740, val loss: 0.4634498953819275
Epoch 1750, training loss: 83.8574447631836 = 0.28771069645881653 + 10.0 * 8.356973648071289
Epoch 1750, val loss: 0.4631371796131134
Epoch 1760, training loss: 83.8313217163086 = 0.2857411503791809 + 10.0 * 8.354557991027832
Epoch 1760, val loss: 0.4627971053123474
Epoch 1770, training loss: 83.8258056640625 = 0.28377366065979004 + 10.0 * 8.354203224182129
Epoch 1770, val loss: 0.46260613203048706
Epoch 1780, training loss: 83.8410873413086 = 0.2817940413951874 + 10.0 * 8.355929374694824
Epoch 1780, val loss: 0.46233195066452026
Epoch 1790, training loss: 83.85245513916016 = 0.2797926962375641 + 10.0 * 8.357266426086426
Epoch 1790, val loss: 0.4621911346912384
Epoch 1800, training loss: 83.84595489501953 = 0.27777644991874695 + 10.0 * 8.356817245483398
Epoch 1800, val loss: 0.46219566464424133
Epoch 1810, training loss: 83.81224060058594 = 0.2757490873336792 + 10.0 * 8.353649139404297
Epoch 1810, val loss: 0.4618948996067047
Epoch 1820, training loss: 83.81108093261719 = 0.27372586727142334 + 10.0 * 8.353734970092773
Epoch 1820, val loss: 0.46186670660972595
Epoch 1830, training loss: 83.85193634033203 = 0.271688848733902 + 10.0 * 8.358024597167969
Epoch 1830, val loss: 0.4616946876049042
Epoch 1840, training loss: 83.80714416503906 = 0.26962631940841675 + 10.0 * 8.353752136230469
Epoch 1840, val loss: 0.4618202745914459
Epoch 1850, training loss: 83.79229736328125 = 0.267566442489624 + 10.0 * 8.352473258972168
Epoch 1850, val loss: 0.46173936128616333
Epoch 1860, training loss: 83.82616424560547 = 0.2655091881752014 + 10.0 * 8.35606575012207
Epoch 1860, val loss: 0.46177685260772705
Epoch 1870, training loss: 83.80088806152344 = 0.263414591550827 + 10.0 * 8.353747367858887
Epoch 1870, val loss: 0.46178391575813293
Epoch 1880, training loss: 83.78168487548828 = 0.2613188624382019 + 10.0 * 8.352036476135254
Epoch 1880, val loss: 0.4620240330696106
Epoch 1890, training loss: 83.77168273925781 = 0.25923141837120056 + 10.0 * 8.351244926452637
Epoch 1890, val loss: 0.4620784521102905
Epoch 1900, training loss: 83.77061462402344 = 0.25714099407196045 + 10.0 * 8.351346969604492
Epoch 1900, val loss: 0.4622882008552551
Epoch 1910, training loss: 83.79170989990234 = 0.2550417482852936 + 10.0 * 8.353666305541992
Epoch 1910, val loss: 0.46248528361320496
Epoch 1920, training loss: 83.77671813964844 = 0.25292661786079407 + 10.0 * 8.352378845214844
Epoch 1920, val loss: 0.4626518785953522
Epoch 1930, training loss: 83.79911041259766 = 0.25081688165664673 + 10.0 * 8.354829788208008
Epoch 1930, val loss: 0.4629260003566742
Epoch 1940, training loss: 83.75761413574219 = 0.24869847297668457 + 10.0 * 8.350892066955566
Epoch 1940, val loss: 0.46317407488822937
Epoch 1950, training loss: 83.74312591552734 = 0.24658218026161194 + 10.0 * 8.349654197692871
Epoch 1950, val loss: 0.4635236859321594
Epoch 1960, training loss: 83.77338409423828 = 0.2444676011800766 + 10.0 * 8.35289192199707
Epoch 1960, val loss: 0.4641067683696747
Epoch 1970, training loss: 83.74057006835938 = 0.24233053624629974 + 10.0 * 8.349823951721191
Epoch 1970, val loss: 0.4642007350921631
Epoch 1980, training loss: 83.7344741821289 = 0.24020496010780334 + 10.0 * 8.349427223205566
Epoch 1980, val loss: 0.46468585729599
Epoch 1990, training loss: 83.74478912353516 = 0.238077312707901 + 10.0 * 8.350671768188477
Epoch 1990, val loss: 0.4650363624095917
Epoch 2000, training loss: 83.75310516357422 = 0.23594310879707336 + 10.0 * 8.351716041564941
Epoch 2000, val loss: 0.4657135605812073
Epoch 2010, training loss: 83.73326873779297 = 0.23380857706069946 + 10.0 * 8.349946022033691
Epoch 2010, val loss: 0.466156542301178
Epoch 2020, training loss: 83.7140884399414 = 0.23166991770267487 + 10.0 * 8.348241806030273
Epoch 2020, val loss: 0.46678125858306885
Epoch 2030, training loss: 83.7160873413086 = 0.2295326441526413 + 10.0 * 8.348655700683594
Epoch 2030, val loss: 0.4673202633857727
Epoch 2040, training loss: 83.75467681884766 = 0.2273867428302765 + 10.0 * 8.352728843688965
Epoch 2040, val loss: 0.4679718613624573
Epoch 2050, training loss: 83.72429656982422 = 0.22523988783359528 + 10.0 * 8.349905014038086
Epoch 2050, val loss: 0.46864351630210876
Epoch 2060, training loss: 83.7349624633789 = 0.22309227287769318 + 10.0 * 8.351186752319336
Epoch 2060, val loss: 0.469353586435318
Epoch 2070, training loss: 83.73487854003906 = 0.22094789147377014 + 10.0 * 8.35139274597168
Epoch 2070, val loss: 0.4699209928512573
Epoch 2080, training loss: 83.7071533203125 = 0.21881018579006195 + 10.0 * 8.348834037780762
Epoch 2080, val loss: 0.4708002507686615
Epoch 2090, training loss: 83.69734954833984 = 0.2166728377342224 + 10.0 * 8.348067283630371
Epoch 2090, val loss: 0.4716472923755646
Epoch 2100, training loss: 83.7088851928711 = 0.21453844010829926 + 10.0 * 8.349434852600098
Epoch 2100, val loss: 0.4726240634918213
Epoch 2110, training loss: 83.6748275756836 = 0.21240763366222382 + 10.0 * 8.34624195098877
Epoch 2110, val loss: 0.47337931394577026
Epoch 2120, training loss: 83.67363739013672 = 0.21027891337871552 + 10.0 * 8.346335411071777
Epoch 2120, val loss: 0.47411298751831055
Epoch 2130, training loss: 83.71160125732422 = 0.20815537869930267 + 10.0 * 8.35034465789795
Epoch 2130, val loss: 0.47501352429389954
Epoch 2140, training loss: 83.67159271240234 = 0.2060166299343109 + 10.0 * 8.3465576171875
Epoch 2140, val loss: 0.4761520028114319
Epoch 2150, training loss: 83.68042755126953 = 0.20389367640018463 + 10.0 * 8.34765338897705
Epoch 2150, val loss: 0.4772465229034424
Epoch 2160, training loss: 83.69825744628906 = 0.2017766535282135 + 10.0 * 8.349648475646973
Epoch 2160, val loss: 0.47823962569236755
Epoch 2170, training loss: 83.65767669677734 = 0.19966275990009308 + 10.0 * 8.34580135345459
Epoch 2170, val loss: 0.479330837726593
Epoch 2180, training loss: 83.6573715209961 = 0.19755485653877258 + 10.0 * 8.34598159790039
Epoch 2180, val loss: 0.4804939031600952
Epoch 2190, training loss: 83.66292572021484 = 0.19546039402484894 + 10.0 * 8.346746444702148
Epoch 2190, val loss: 0.4817498028278351
Epoch 2200, training loss: 83.67066955566406 = 0.1933593899011612 + 10.0 * 8.34773063659668
Epoch 2200, val loss: 0.48308345675468445
Epoch 2210, training loss: 83.6700210571289 = 0.19126197695732117 + 10.0 * 8.347875595092773
Epoch 2210, val loss: 0.4842178523540497
Epoch 2220, training loss: 83.64989471435547 = 0.1891849935054779 + 10.0 * 8.346071243286133
Epoch 2220, val loss: 0.48536863923072815
Epoch 2230, training loss: 83.63909912109375 = 0.18710654973983765 + 10.0 * 8.345199584960938
Epoch 2230, val loss: 0.48669692873954773
Epoch 2240, training loss: 83.64204406738281 = 0.1850402057170868 + 10.0 * 8.34570026397705
Epoch 2240, val loss: 0.48795798420906067
Epoch 2250, training loss: 83.66351318359375 = 0.18298059701919556 + 10.0 * 8.348052978515625
Epoch 2250, val loss: 0.489298552274704
Epoch 2260, training loss: 83.645751953125 = 0.18090666830539703 + 10.0 * 8.346484184265137
Epoch 2260, val loss: 0.4907608926296234
Epoch 2270, training loss: 83.6148910522461 = 0.17885413765907288 + 10.0 * 8.343603134155273
Epoch 2270, val loss: 0.49224159121513367
Epoch 2280, training loss: 83.61516571044922 = 0.17681917548179626 + 10.0 * 8.34383487701416
Epoch 2280, val loss: 0.4936559796333313
Epoch 2290, training loss: 83.66868591308594 = 0.17478899657726288 + 10.0 * 8.349390029907227
Epoch 2290, val loss: 0.4951367974281311
