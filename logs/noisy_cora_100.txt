Begin epxeriment: cont_weight: 100 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0001, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11636])
remove edge: torch.Size([2, 9480])
updated graph: torch.Size([2, 10560])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:224: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 861.62841796875 = 1.9480549097061157 + 100.0 * 8.596803665161133
Epoch 0, val loss: 1.9554533958435059
Epoch 10, training loss: 861.5927124023438 = 1.9436148405075073 + 100.0 * 8.596490859985352
Epoch 10, val loss: 1.9506934881210327
Epoch 20, training loss: 861.4957275390625 = 1.9385943412780762 + 100.0 * 8.595571517944336
Epoch 20, val loss: 1.9452431201934814
Epoch 30, training loss: 861.1795043945312 = 1.9327970743179321 + 100.0 * 8.592467308044434
Epoch 30, val loss: 1.93889319896698
Epoch 40, training loss: 860.0358276367188 = 1.925891637802124 + 100.0 * 8.581099510192871
Epoch 40, val loss: 1.9313087463378906
Epoch 50, training loss: 855.9137573242188 = 1.917576789855957 + 100.0 * 8.539961814880371
Epoch 50, val loss: 1.922194004058838
Epoch 60, training loss: 841.7669067382812 = 1.9080826044082642 + 100.0 * 8.398588180541992
Epoch 60, val loss: 1.9119943380355835
Epoch 70, training loss: 798.8240356445312 = 1.898348093032837 + 100.0 * 7.96925687789917
Epoch 70, val loss: 1.9015753269195557
Epoch 80, training loss: 745.6104736328125 = 1.887647271156311 + 100.0 * 7.437228679656982
Epoch 80, val loss: 1.890597939491272
Epoch 90, training loss: 724.9918823242188 = 1.8805352449417114 + 100.0 * 7.231113433837891
Epoch 90, val loss: 1.883996605873108
Epoch 100, training loss: 706.4147338867188 = 1.8749377727508545 + 100.0 * 7.045397758483887
Epoch 100, val loss: 1.8785696029663086
Epoch 110, training loss: 693.6903076171875 = 1.8692084550857544 + 100.0 * 6.918210983276367
Epoch 110, val loss: 1.8728728294372559
Epoch 120, training loss: 685.4276123046875 = 1.8644083738327026 + 100.0 * 6.83563232421875
Epoch 120, val loss: 1.8679970502853394
Epoch 130, training loss: 679.3209228515625 = 1.8593662977218628 + 100.0 * 6.774615287780762
Epoch 130, val loss: 1.8627701997756958
Epoch 140, training loss: 674.3073120117188 = 1.8545441627502441 + 100.0 * 6.724527835845947
Epoch 140, val loss: 1.8578277826309204
Epoch 150, training loss: 669.8508911132812 = 1.8495852947235107 + 100.0 * 6.6800127029418945
Epoch 150, val loss: 1.8528424501419067
Epoch 160, training loss: 666.0508422851562 = 1.8448923826217651 + 100.0 * 6.642059326171875
Epoch 160, val loss: 1.8481860160827637
Epoch 170, training loss: 662.8635864257812 = 1.8405110836029053 + 100.0 * 6.610230922698975
Epoch 170, val loss: 1.8438796997070312
Epoch 180, training loss: 660.1936645507812 = 1.8363319635391235 + 100.0 * 6.583573818206787
Epoch 180, val loss: 1.839754581451416
Epoch 190, training loss: 657.869873046875 = 1.832268476486206 + 100.0 * 6.560375690460205
Epoch 190, val loss: 1.8357230424880981
Epoch 200, training loss: 655.8965454101562 = 1.828263521194458 + 100.0 * 6.540682792663574
Epoch 200, val loss: 1.8317577838897705
Epoch 210, training loss: 653.9539184570312 = 1.8243008852005005 + 100.0 * 6.521296501159668
Epoch 210, val loss: 1.8279027938842773
Epoch 220, training loss: 652.35791015625 = 1.820476770401001 + 100.0 * 6.505374431610107
Epoch 220, val loss: 1.8241254091262817
Epoch 230, training loss: 651.19287109375 = 1.816765546798706 + 100.0 * 6.49376106262207
Epoch 230, val loss: 1.8203951120376587
Epoch 240, training loss: 649.7582397460938 = 1.8130048513412476 + 100.0 * 6.479452610015869
Epoch 240, val loss: 1.8167386054992676
Epoch 250, training loss: 648.6239013671875 = 1.8092659711837769 + 100.0 * 6.468146324157715
Epoch 250, val loss: 1.8130687475204468
Epoch 260, training loss: 647.5862426757812 = 1.8055763244628906 + 100.0 * 6.457806587219238
Epoch 260, val loss: 1.8093984127044678
Epoch 270, training loss: 646.801025390625 = 1.8018678426742554 + 100.0 * 6.449991703033447
Epoch 270, val loss: 1.8057173490524292
Epoch 280, training loss: 645.7821655273438 = 1.7981147766113281 + 100.0 * 6.439840793609619
Epoch 280, val loss: 1.8019988536834717
Epoch 290, training loss: 644.9069213867188 = 1.7943052053451538 + 100.0 * 6.431126117706299
Epoch 290, val loss: 1.7982406616210938
Epoch 300, training loss: 644.1161499023438 = 1.7904151678085327 + 100.0 * 6.423257350921631
Epoch 300, val loss: 1.7944207191467285
Epoch 310, training loss: 643.439697265625 = 1.78642737865448 + 100.0 * 6.416532516479492
Epoch 310, val loss: 1.790503740310669
Epoch 320, training loss: 642.6195068359375 = 1.7823076248168945 + 100.0 * 6.408371925354004
Epoch 320, val loss: 1.7864818572998047
Epoch 330, training loss: 641.9217529296875 = 1.7780505418777466 + 100.0 * 6.401436805725098
Epoch 330, val loss: 1.7823415994644165
Epoch 340, training loss: 641.30517578125 = 1.7736197710037231 + 100.0 * 6.395315647125244
Epoch 340, val loss: 1.7780412435531616
Epoch 350, training loss: 640.6712036132812 = 1.7690086364746094 + 100.0 * 6.389022350311279
Epoch 350, val loss: 1.7735748291015625
Epoch 360, training loss: 640.0623779296875 = 1.7642078399658203 + 100.0 * 6.382981300354004
Epoch 360, val loss: 1.7689288854599
Epoch 370, training loss: 639.509033203125 = 1.7592060565948486 + 100.0 * 6.377498149871826
Epoch 370, val loss: 1.7640819549560547
Epoch 380, training loss: 639.0779418945312 = 1.7539665699005127 + 100.0 * 6.373239994049072
Epoch 380, val loss: 1.7590241432189941
Epoch 390, training loss: 638.457275390625 = 1.7484878301620483 + 100.0 * 6.3670878410339355
Epoch 390, val loss: 1.7537530660629272
Epoch 400, training loss: 638.0073852539062 = 1.7427552938461304 + 100.0 * 6.362646579742432
Epoch 400, val loss: 1.748230218887329
Epoch 410, training loss: 637.5972290039062 = 1.7367308139801025 + 100.0 * 6.358604431152344
Epoch 410, val loss: 1.7424923181533813
Epoch 420, training loss: 637.0841064453125 = 1.7304507493972778 + 100.0 * 6.353536605834961
Epoch 420, val loss: 1.7364574670791626
Epoch 430, training loss: 636.6705932617188 = 1.723882794380188 + 100.0 * 6.3494672775268555
Epoch 430, val loss: 1.7301443815231323
Epoch 440, training loss: 636.7417602539062 = 1.7170469760894775 + 100.0 * 6.350246906280518
Epoch 440, val loss: 1.7235432863235474
Epoch 450, training loss: 636.0265502929688 = 1.7098164558410645 + 100.0 * 6.343167304992676
Epoch 450, val loss: 1.71665358543396
Epoch 460, training loss: 635.5755004882812 = 1.702298879623413 + 100.0 * 6.3387322425842285
Epoch 460, val loss: 1.7094827890396118
Epoch 470, training loss: 635.2295532226562 = 1.6944849491119385 + 100.0 * 6.33535099029541
Epoch 470, val loss: 1.7020312547683716
Epoch 480, training loss: 634.9017333984375 = 1.6863683462142944 + 100.0 * 6.332153797149658
Epoch 480, val loss: 1.6942782402038574
Epoch 490, training loss: 634.8905029296875 = 1.6779301166534424 + 100.0 * 6.332126140594482
Epoch 490, val loss: 1.6862168312072754
Epoch 500, training loss: 634.3936767578125 = 1.6691631078720093 + 100.0 * 6.327244758605957
Epoch 500, val loss: 1.6778607368469238
Epoch 510, training loss: 634.0324096679688 = 1.660097360610962 + 100.0 * 6.323723316192627
Epoch 510, val loss: 1.669235348701477
Epoch 520, training loss: 633.73046875 = 1.6507515907287598 + 100.0 * 6.320796966552734
Epoch 520, val loss: 1.66034734249115
Epoch 530, training loss: 633.5474243164062 = 1.6411194801330566 + 100.0 * 6.319063186645508
Epoch 530, val loss: 1.651238203048706
Epoch 540, training loss: 633.2774047851562 = 1.6312191486358643 + 100.0 * 6.316461563110352
Epoch 540, val loss: 1.6417922973632812
Epoch 550, training loss: 632.9838256835938 = 1.6210333108901978 + 100.0 * 6.31362771987915
Epoch 550, val loss: 1.632232666015625
Epoch 560, training loss: 632.7581176757812 = 1.6106340885162354 + 100.0 * 6.311474800109863
Epoch 560, val loss: 1.622454285621643
Epoch 570, training loss: 632.4534301757812 = 1.6000254154205322 + 100.0 * 6.308533668518066
Epoch 570, val loss: 1.6124497652053833
Epoch 580, training loss: 632.3383178710938 = 1.5892086029052734 + 100.0 * 6.307490825653076
Epoch 580, val loss: 1.602268934249878
Epoch 590, training loss: 632.1560668945312 = 1.5781069993972778 + 100.0 * 6.305779457092285
Epoch 590, val loss: 1.5920488834381104
Epoch 600, training loss: 631.8416748046875 = 1.56692636013031 + 100.0 * 6.3027472496032715
Epoch 600, val loss: 1.581632137298584
Epoch 610, training loss: 631.5918579101562 = 1.5555880069732666 + 100.0 * 6.300362586975098
Epoch 610, val loss: 1.571131706237793
Epoch 620, training loss: 631.3871459960938 = 1.5441142320632935 + 100.0 * 6.2984299659729
Epoch 620, val loss: 1.5606136322021484
Epoch 630, training loss: 631.6259765625 = 1.5325642824172974 + 100.0 * 6.300933837890625
Epoch 630, val loss: 1.5499979257583618
Epoch 640, training loss: 631.0706176757812 = 1.520843267440796 + 100.0 * 6.295497894287109
Epoch 640, val loss: 1.5394082069396973
Epoch 650, training loss: 630.8119506835938 = 1.5090702772140503 + 100.0 * 6.293028354644775
Epoch 650, val loss: 1.5287561416625977
Epoch 660, training loss: 630.6337280273438 = 1.4972602128982544 + 100.0 * 6.291364669799805
Epoch 660, val loss: 1.5181392431259155
Epoch 670, training loss: 630.5108642578125 = 1.4854331016540527 + 100.0 * 6.290254592895508
Epoch 670, val loss: 1.5075206756591797
Epoch 680, training loss: 630.3184204101562 = 1.4735386371612549 + 100.0 * 6.288448810577393
Epoch 680, val loss: 1.4970093965530396
Epoch 690, training loss: 630.1815795898438 = 1.4616326093673706 + 100.0 * 6.287199974060059
Epoch 690, val loss: 1.4864164590835571
Epoch 700, training loss: 630.2703247070312 = 1.449686884880066 + 100.0 * 6.288206100463867
Epoch 700, val loss: 1.4758938550949097
Epoch 710, training loss: 629.9257202148438 = 1.4378094673156738 + 100.0 * 6.284879207611084
Epoch 710, val loss: 1.4654399156570435
Epoch 720, training loss: 629.6842041015625 = 1.4258861541748047 + 100.0 * 6.282582759857178
Epoch 720, val loss: 1.4550632238388062
Epoch 730, training loss: 629.5358276367188 = 1.4140045642852783 + 100.0 * 6.2812180519104
Epoch 730, val loss: 1.4447500705718994
Epoch 740, training loss: 629.6909790039062 = 1.4021774530410767 + 100.0 * 6.282888412475586
Epoch 740, val loss: 1.4344843626022339
Epoch 750, training loss: 629.348388671875 = 1.3902357816696167 + 100.0 * 6.279581069946289
Epoch 750, val loss: 1.4243284463882446
Epoch 760, training loss: 629.1345825195312 = 1.3784358501434326 + 100.0 * 6.277561664581299
Epoch 760, val loss: 1.4141476154327393
Epoch 770, training loss: 628.9968872070312 = 1.3665989637374878 + 100.0 * 6.276302814483643
Epoch 770, val loss: 1.404119849205017
Epoch 780, training loss: 628.9055786132812 = 1.3548362255096436 + 100.0 * 6.27550745010376
Epoch 780, val loss: 1.394107460975647
Epoch 790, training loss: 628.754150390625 = 1.3430525064468384 + 100.0 * 6.274110794067383
Epoch 790, val loss: 1.38419771194458
Epoch 800, training loss: 628.6666259765625 = 1.331286907196045 + 100.0 * 6.273353576660156
Epoch 800, val loss: 1.3742636442184448
Epoch 810, training loss: 628.5230712890625 = 1.3195531368255615 + 100.0 * 6.272035121917725
Epoch 810, val loss: 1.3644893169403076
Epoch 820, training loss: 628.6248168945312 = 1.307828664779663 + 100.0 * 6.273170471191406
Epoch 820, val loss: 1.3547769784927368
Epoch 830, training loss: 628.4508056640625 = 1.2961643934249878 + 100.0 * 6.271546840667725
Epoch 830, val loss: 1.3451184034347534
Epoch 840, training loss: 628.158203125 = 1.2845227718353271 + 100.0 * 6.268736839294434
Epoch 840, val loss: 1.3353919982910156
Epoch 850, training loss: 628.0491333007812 = 1.272909164428711 + 100.0 * 6.267762660980225
Epoch 850, val loss: 1.3258776664733887
Epoch 860, training loss: 627.9913940429688 = 1.2613677978515625 + 100.0 * 6.267300128936768
Epoch 860, val loss: 1.3163650035858154
Epoch 870, training loss: 627.96533203125 = 1.2498047351837158 + 100.0 * 6.267155170440674
Epoch 870, val loss: 1.3069697618484497
Epoch 880, training loss: 627.7417602539062 = 1.2382633686065674 + 100.0 * 6.2650346755981445
Epoch 880, val loss: 1.2976713180541992
Epoch 890, training loss: 627.6475219726562 = 1.2267695665359497 + 100.0 * 6.26420783996582
Epoch 890, val loss: 1.288415789604187
Epoch 900, training loss: 627.5390625 = 1.215340495109558 + 100.0 * 6.263237476348877
Epoch 900, val loss: 1.2792134284973145
Epoch 910, training loss: 627.7401123046875 = 1.203866958618164 + 100.0 * 6.26536226272583
Epoch 910, val loss: 1.270147681236267
Epoch 920, training loss: 627.4053344726562 = 1.1924816370010376 + 100.0 * 6.2621283531188965
Epoch 920, val loss: 1.261048436164856
Epoch 930, training loss: 627.2571411132812 = 1.1810715198516846 + 100.0 * 6.260760307312012
Epoch 930, val loss: 1.252105474472046
Epoch 940, training loss: 627.2269287109375 = 1.1697454452514648 + 100.0 * 6.2605719566345215
Epoch 940, val loss: 1.2432163953781128
Epoch 950, training loss: 627.0493774414062 = 1.1585086584091187 + 100.0 * 6.258908748626709
Epoch 950, val loss: 1.234373927116394
Epoch 960, training loss: 627.0025024414062 = 1.1473504304885864 + 100.0 * 6.258551597595215
Epoch 960, val loss: 1.2257236242294312
Epoch 970, training loss: 627.0088500976562 = 1.1362553834915161 + 100.0 * 6.258726119995117
Epoch 970, val loss: 1.2172892093658447
Epoch 980, training loss: 626.8607788085938 = 1.1252188682556152 + 100.0 * 6.257355213165283
Epoch 980, val loss: 1.2087165117263794
Epoch 990, training loss: 626.9113159179688 = 1.1142613887786865 + 100.0 * 6.257970809936523
Epoch 990, val loss: 1.200522780418396
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.5592592592592592
0.8102266736953084
=== training gcn model ===
Epoch 0, training loss: 861.6436157226562 = 1.9583826065063477 + 100.0 * 8.59685230255127
Epoch 0, val loss: 1.968379020690918
Epoch 10, training loss: 861.6187744140625 = 1.954185962677002 + 100.0 * 8.596646308898926
Epoch 10, val loss: 1.9642530679702759
Epoch 20, training loss: 861.5496215820312 = 1.9494929313659668 + 100.0 * 8.596001625061035
Epoch 20, val loss: 1.9596645832061768
Epoch 30, training loss: 861.3052368164062 = 1.9440652132034302 + 100.0 * 8.593611717224121
Epoch 30, val loss: 1.9543533325195312
Epoch 40, training loss: 860.3775634765625 = 1.9374600648880005 + 100.0 * 8.58440113067627
Epoch 40, val loss: 1.947816014289856
Epoch 50, training loss: 857.2214965820312 = 1.929024338722229 + 100.0 * 8.552925109863281
Epoch 50, val loss: 1.9395530223846436
Epoch 60, training loss: 847.5181274414062 = 1.9187880754470825 + 100.0 * 8.45599365234375
Epoch 60, val loss: 1.9298174381256104
Epoch 70, training loss: 820.3753662109375 = 1.9079781770706177 + 100.0 * 8.184674263000488
Epoch 70, val loss: 1.9197132587432861
Epoch 80, training loss: 786.7898559570312 = 1.8958624601364136 + 100.0 * 7.848939895629883
Epoch 80, val loss: 1.9089080095291138
Epoch 90, training loss: 762.91552734375 = 1.8865649700164795 + 100.0 * 7.610290050506592
Epoch 90, val loss: 1.9009146690368652
Epoch 100, training loss: 737.8112182617188 = 1.8805009126663208 + 100.0 * 7.359307289123535
Epoch 100, val loss: 1.8950855731964111
Epoch 110, training loss: 719.6417236328125 = 1.8744274377822876 + 100.0 * 7.177672863006592
Epoch 110, val loss: 1.8888072967529297
Epoch 120, training loss: 705.8472900390625 = 1.8689285516738892 + 100.0 * 7.039783477783203
Epoch 120, val loss: 1.8829878568649292
Epoch 130, training loss: 695.3590698242188 = 1.8635934591293335 + 100.0 * 6.934954643249512
Epoch 130, val loss: 1.8773765563964844
Epoch 140, training loss: 687.5282592773438 = 1.8585412502288818 + 100.0 * 6.8566975593566895
Epoch 140, val loss: 1.8721727132797241
Epoch 150, training loss: 681.2380981445312 = 1.8536653518676758 + 100.0 * 6.793844699859619
Epoch 150, val loss: 1.8671954870224
Epoch 160, training loss: 675.5702514648438 = 1.848850131034851 + 100.0 * 6.737213611602783
Epoch 160, val loss: 1.8622291088104248
Epoch 170, training loss: 670.6356201171875 = 1.8443727493286133 + 100.0 * 6.687912464141846
Epoch 170, val loss: 1.8576151132583618
Epoch 180, training loss: 666.6364135742188 = 1.8400893211364746 + 100.0 * 6.647963523864746
Epoch 180, val loss: 1.8531235456466675
Epoch 190, training loss: 663.41015625 = 1.8357460498809814 + 100.0 * 6.615744113922119
Epoch 190, val loss: 1.8486545085906982
Epoch 200, training loss: 660.8770141601562 = 1.8314155340194702 + 100.0 * 6.590456008911133
Epoch 200, val loss: 1.8442219495773315
Epoch 210, training loss: 658.4147338867188 = 1.8271335363388062 + 100.0 * 6.565876007080078
Epoch 210, val loss: 1.8398690223693848
Epoch 220, training loss: 656.2783813476562 = 1.822963833808899 + 100.0 * 6.544554710388184
Epoch 220, val loss: 1.8356260061264038
Epoch 230, training loss: 654.468994140625 = 1.818863034248352 + 100.0 * 6.526501178741455
Epoch 230, val loss: 1.8314416408538818
Epoch 240, training loss: 653.0327758789062 = 1.81477689743042 + 100.0 * 6.512180328369141
Epoch 240, val loss: 1.827303171157837
Epoch 250, training loss: 651.26953125 = 1.8107116222381592 + 100.0 * 6.4945878982543945
Epoch 250, val loss: 1.8231672048568726
Epoch 260, training loss: 649.8346557617188 = 1.8066760301589966 + 100.0 * 6.480279445648193
Epoch 260, val loss: 1.819084644317627
Epoch 270, training loss: 648.5625 = 1.8026269674301147 + 100.0 * 6.467598915100098
Epoch 270, val loss: 1.815030574798584
Epoch 280, training loss: 647.3812255859375 = 1.7985470294952393 + 100.0 * 6.455826759338379
Epoch 280, val loss: 1.8109874725341797
Epoch 290, training loss: 646.3643188476562 = 1.7944116592407227 + 100.0 * 6.4456987380981445
Epoch 290, val loss: 1.8069508075714111
Epoch 300, training loss: 645.3613891601562 = 1.7901990413665771 + 100.0 * 6.435711860656738
Epoch 300, val loss: 1.8028662204742432
Epoch 310, training loss: 644.991943359375 = 1.7859185934066772 + 100.0 * 6.432060241699219
Epoch 310, val loss: 1.7987562417984009
Epoch 320, training loss: 643.7464599609375 = 1.7814428806304932 + 100.0 * 6.419650554656982
Epoch 320, val loss: 1.794572114944458
Epoch 330, training loss: 642.9763793945312 = 1.7768691778182983 + 100.0 * 6.4119954109191895
Epoch 330, val loss: 1.7903190851211548
Epoch 340, training loss: 642.2609252929688 = 1.7721766233444214 + 100.0 * 6.404887676239014
Epoch 340, val loss: 1.7860031127929688
Epoch 350, training loss: 641.6334228515625 = 1.7673251628875732 + 100.0 * 6.398661136627197
Epoch 350, val loss: 1.7815998792648315
Epoch 360, training loss: 641.0349731445312 = 1.7623038291931152 + 100.0 * 6.392726421356201
Epoch 360, val loss: 1.7770886421203613
Epoch 370, training loss: 640.515625 = 1.75709068775177 + 100.0 * 6.387585639953613
Epoch 370, val loss: 1.7724533081054688
Epoch 380, training loss: 640.1697387695312 = 1.751692533493042 + 100.0 * 6.384180068969727
Epoch 380, val loss: 1.7676903009414673
Epoch 390, training loss: 639.5145263671875 = 1.7460709810256958 + 100.0 * 6.377684116363525
Epoch 390, val loss: 1.7627829313278198
Epoch 400, training loss: 639.029541015625 = 1.7402452230453491 + 100.0 * 6.372893333435059
Epoch 400, val loss: 1.7577402591705322
Epoch 410, training loss: 638.5945434570312 = 1.734207034111023 + 100.0 * 6.368603229522705
Epoch 410, val loss: 1.752551794052124
Epoch 420, training loss: 638.3983154296875 = 1.7279249429702759 + 100.0 * 6.366703987121582
Epoch 420, val loss: 1.7471973896026611
Epoch 430, training loss: 637.81640625 = 1.7214173078536987 + 100.0 * 6.360949993133545
Epoch 430, val loss: 1.7416728734970093
Epoch 440, training loss: 637.4700927734375 = 1.714650273323059 + 100.0 * 6.3575544357299805
Epoch 440, val loss: 1.7359645366668701
Epoch 450, training loss: 637.0930786132812 = 1.707655668258667 + 100.0 * 6.353854656219482
Epoch 450, val loss: 1.7300982475280762
Epoch 460, training loss: 636.7465209960938 = 1.7004103660583496 + 100.0 * 6.350460529327393
Epoch 460, val loss: 1.7240480184555054
Epoch 470, training loss: 636.6737670898438 = 1.6929352283477783 + 100.0 * 6.349808216094971
Epoch 470, val loss: 1.7178338766098022
Epoch 480, training loss: 636.2591552734375 = 1.6851063966751099 + 100.0 * 6.34574031829834
Epoch 480, val loss: 1.711359977722168
Epoch 490, training loss: 635.8729858398438 = 1.6770954132080078 + 100.0 * 6.341958522796631
Epoch 490, val loss: 1.7047594785690308
Epoch 500, training loss: 635.548095703125 = 1.6688429117202759 + 100.0 * 6.33879280090332
Epoch 500, val loss: 1.698007583618164
Epoch 510, training loss: 635.255126953125 = 1.6603599786758423 + 100.0 * 6.3359479904174805
Epoch 510, val loss: 1.6910996437072754
Epoch 520, training loss: 635.1292724609375 = 1.651638150215149 + 100.0 * 6.334776878356934
Epoch 520, val loss: 1.6840304136276245
Epoch 530, training loss: 634.7245483398438 = 1.6427063941955566 + 100.0 * 6.3308186531066895
Epoch 530, val loss: 1.6767889261245728
Epoch 540, training loss: 634.4623413085938 = 1.6335484981536865 + 100.0 * 6.3282880783081055
Epoch 540, val loss: 1.6693899631500244
Epoch 550, training loss: 634.2156982421875 = 1.6242012977600098 + 100.0 * 6.3259148597717285
Epoch 550, val loss: 1.6618707180023193
Epoch 560, training loss: 634.1357421875 = 1.614643931388855 + 100.0 * 6.325211048126221
Epoch 560, val loss: 1.6542004346847534
Epoch 570, training loss: 633.8206787109375 = 1.6049247980117798 + 100.0 * 6.322157382965088
Epoch 570, val loss: 1.6464463472366333
Epoch 580, training loss: 633.5402221679688 = 1.5950647592544556 + 100.0 * 6.319451332092285
Epoch 580, val loss: 1.6386069059371948
Epoch 590, training loss: 633.296142578125 = 1.5850722789764404 + 100.0 * 6.317110538482666
Epoch 590, val loss: 1.6307076215744019
Epoch 600, training loss: 633.3214111328125 = 1.5749727487564087 + 100.0 * 6.317464828491211
Epoch 600, val loss: 1.6227630376815796
Epoch 610, training loss: 632.980712890625 = 1.5646940469741821 + 100.0 * 6.314160346984863
Epoch 610, val loss: 1.6146913766860962
Epoch 620, training loss: 632.7791748046875 = 1.554329752922058 + 100.0 * 6.312248706817627
Epoch 620, val loss: 1.6066348552703857
Epoch 630, training loss: 632.49658203125 = 1.5439053773880005 + 100.0 * 6.3095269203186035
Epoch 630, val loss: 1.598581075668335
Epoch 640, training loss: 632.3046264648438 = 1.5334522724151611 + 100.0 * 6.307712078094482
Epoch 640, val loss: 1.590527892112732
Epoch 650, training loss: 632.1078491210938 = 1.5229486227035522 + 100.0 * 6.305849075317383
Epoch 650, val loss: 1.582473635673523
Epoch 660, training loss: 631.92138671875 = 1.5124168395996094 + 100.0 * 6.3040900230407715
Epoch 660, val loss: 1.5744421482086182
Epoch 670, training loss: 631.8140258789062 = 1.5018610954284668 + 100.0 * 6.303122043609619
Epoch 670, val loss: 1.5664088726043701
Epoch 680, training loss: 631.6419067382812 = 1.4912549257278442 + 100.0 * 6.301506042480469
Epoch 680, val loss: 1.5584354400634766
Epoch 690, training loss: 631.6751098632812 = 1.4807097911834717 + 100.0 * 6.301944255828857
Epoch 690, val loss: 1.550595760345459
Epoch 700, training loss: 631.4126586914062 = 1.4700533151626587 + 100.0 * 6.299426078796387
Epoch 700, val loss: 1.5425450801849365
Epoch 710, training loss: 631.1287231445312 = 1.4595322608947754 + 100.0 * 6.29669189453125
Epoch 710, val loss: 1.5348050594329834
Epoch 720, training loss: 630.92529296875 = 1.4489812850952148 + 100.0 * 6.294763088226318
Epoch 720, val loss: 1.527053952217102
Epoch 730, training loss: 630.7615356445312 = 1.4384934902191162 + 100.0 * 6.2932305335998535
Epoch 730, val loss: 1.5193519592285156
Epoch 740, training loss: 630.62255859375 = 1.4280309677124023 + 100.0 * 6.291944980621338
Epoch 740, val loss: 1.5117051601409912
Epoch 750, training loss: 630.931396484375 = 1.4175761938095093 + 100.0 * 6.295137882232666
Epoch 750, val loss: 1.5041027069091797
Epoch 760, training loss: 630.3543090820312 = 1.407193899154663 + 100.0 * 6.28947114944458
Epoch 760, val loss: 1.496617078781128
Epoch 770, training loss: 630.2042236328125 = 1.3968201875686646 + 100.0 * 6.288073539733887
Epoch 770, val loss: 1.4891306161880493
Epoch 780, training loss: 630.0589599609375 = 1.386495590209961 + 100.0 * 6.286725044250488
Epoch 780, val loss: 1.4817372560501099
Epoch 790, training loss: 629.9108276367188 = 1.3762187957763672 + 100.0 * 6.285346031188965
Epoch 790, val loss: 1.4744211435317993
Epoch 800, training loss: 630.66650390625 = 1.3660211563110352 + 100.0 * 6.293004512786865
Epoch 800, val loss: 1.4673043489456177
Epoch 810, training loss: 629.6991577148438 = 1.3556724786758423 + 100.0 * 6.283435344696045
Epoch 810, val loss: 1.4598897695541382
Epoch 820, training loss: 629.5353393554688 = 1.3454440832138062 + 100.0 * 6.2818989753723145
Epoch 820, val loss: 1.4526311159133911
Epoch 830, training loss: 629.4075317382812 = 1.3352689743041992 + 100.0 * 6.280722618103027
Epoch 830, val loss: 1.4454282522201538
Epoch 840, training loss: 629.2890014648438 = 1.3251254558563232 + 100.0 * 6.279638767242432
Epoch 840, val loss: 1.4382550716400146
Epoch 850, training loss: 629.1635131835938 = 1.3150144815444946 + 100.0 * 6.27848482131958
Epoch 850, val loss: 1.4311468601226807
Epoch 860, training loss: 629.4938354492188 = 1.3049161434173584 + 100.0 * 6.281888961791992
Epoch 860, val loss: 1.4240034818649292
Epoch 870, training loss: 628.9788208007812 = 1.2948108911514282 + 100.0 * 6.2768402099609375
Epoch 870, val loss: 1.4169832468032837
Epoch 880, training loss: 628.8543701171875 = 1.2847468852996826 + 100.0 * 6.275696277618408
Epoch 880, val loss: 1.4099640846252441
Epoch 890, training loss: 628.72998046875 = 1.2747247219085693 + 100.0 * 6.274552345275879
Epoch 890, val loss: 1.402999997138977
Epoch 900, training loss: 628.6189575195312 = 1.2647420167922974 + 100.0 * 6.2735419273376465
Epoch 900, val loss: 1.3960548639297485
Epoch 910, training loss: 628.5674438476562 = 1.254776954650879 + 100.0 * 6.27312707901001
Epoch 910, val loss: 1.3891037702560425
Epoch 920, training loss: 628.5433349609375 = 1.2448126077651978 + 100.0 * 6.272984981536865
Epoch 920, val loss: 1.3821446895599365
Epoch 930, training loss: 628.4305419921875 = 1.2349092960357666 + 100.0 * 6.271955966949463
Epoch 930, val loss: 1.3753890991210938
Epoch 940, training loss: 628.2351684570312 = 1.2250107526779175 + 100.0 * 6.270101070404053
Epoch 940, val loss: 1.3685420751571655
Epoch 950, training loss: 628.1167602539062 = 1.2151634693145752 + 100.0 * 6.269016265869141
Epoch 950, val loss: 1.36176335811615
Epoch 960, training loss: 628.0248413085938 = 1.205345869064331 + 100.0 * 6.268195152282715
Epoch 960, val loss: 1.3550331592559814
Epoch 970, training loss: 628.2410278320312 = 1.1955746412277222 + 100.0 * 6.2704548835754395
Epoch 970, val loss: 1.3483619689941406
Epoch 980, training loss: 627.9395751953125 = 1.1857433319091797 + 100.0 * 6.267538547515869
Epoch 980, val loss: 1.3416762351989746
Epoch 990, training loss: 627.7691650390625 = 1.176018476486206 + 100.0 * 6.265931606292725
Epoch 990, val loss: 1.3350166082382202
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.5185185185185185
0.8023194517659463
=== training gcn model ===
Epoch 0, training loss: 861.6217651367188 = 1.9377843141555786 + 100.0 * 8.596839904785156
Epoch 0, val loss: 1.9317314624786377
Epoch 10, training loss: 861.594482421875 = 1.9338674545288086 + 100.0 * 8.596606254577637
Epoch 10, val loss: 1.927922010421753
Epoch 20, training loss: 861.5194702148438 = 1.9295159578323364 + 100.0 * 8.59589958190918
Epoch 20, val loss: 1.9235562086105347
Epoch 30, training loss: 861.2620239257812 = 1.9244804382324219 + 100.0 * 8.593375205993652
Epoch 30, val loss: 1.9183510541915894
Epoch 40, training loss: 860.2955932617188 = 1.918509840965271 + 100.0 * 8.583770751953125
Epoch 40, val loss: 1.9120516777038574
Epoch 50, training loss: 856.839111328125 = 1.9112346172332764 + 100.0 * 8.549278259277344
Epoch 50, val loss: 1.9043031930923462
Epoch 60, training loss: 845.2257690429688 = 1.9026219844818115 + 100.0 * 8.433231353759766
Epoch 60, val loss: 1.8952127695083618
Epoch 70, training loss: 807.1262817382812 = 1.8932547569274902 + 100.0 * 8.052330017089844
Epoch 70, val loss: 1.8853495121002197
Epoch 80, training loss: 757.4842529296875 = 1.883158564567566 + 100.0 * 7.556010723114014
Epoch 80, val loss: 1.8751057386398315
Epoch 90, training loss: 738.8450317382812 = 1.8750731945037842 + 100.0 * 7.369699954986572
Epoch 90, val loss: 1.8674970865249634
Epoch 100, training loss: 721.8433837890625 = 1.8691662549972534 + 100.0 * 7.199742317199707
Epoch 100, val loss: 1.8621811866760254
Epoch 110, training loss: 708.293212890625 = 1.863623857498169 + 100.0 * 7.064295768737793
Epoch 110, val loss: 1.85698664188385
Epoch 120, training loss: 699.123291015625 = 1.8581911325454712 + 100.0 * 6.97265100479126
Epoch 120, val loss: 1.8519585132598877
Epoch 130, training loss: 691.276611328125 = 1.8528894186019897 + 100.0 * 6.894237041473389
Epoch 130, val loss: 1.8470971584320068
Epoch 140, training loss: 683.4579467773438 = 1.8477544784545898 + 100.0 * 6.816101551055908
Epoch 140, val loss: 1.842497706413269
Epoch 150, training loss: 676.6409301757812 = 1.84331476688385 + 100.0 * 6.747975826263428
Epoch 150, val loss: 1.8385555744171143
Epoch 160, training loss: 670.8179321289062 = 1.839503526687622 + 100.0 * 6.689784526824951
Epoch 160, val loss: 1.835208773612976
Epoch 170, training loss: 666.2072143554688 = 1.8357973098754883 + 100.0 * 6.64371395111084
Epoch 170, val loss: 1.8319282531738281
Epoch 180, training loss: 662.5826416015625 = 1.8320778608322144 + 100.0 * 6.6075053215026855
Epoch 180, val loss: 1.8286464214324951
Epoch 190, training loss: 659.5370483398438 = 1.8282229900360107 + 100.0 * 6.577087879180908
Epoch 190, val loss: 1.8252513408660889
Epoch 200, training loss: 657.1964111328125 = 1.8243080377578735 + 100.0 * 6.5537214279174805
Epoch 200, val loss: 1.821828007698059
Epoch 210, training loss: 655.1805419921875 = 1.8204548358917236 + 100.0 * 6.5336012840271
Epoch 210, val loss: 1.8184876441955566
Epoch 220, training loss: 653.4501342773438 = 1.8166917562484741 + 100.0 * 6.516334533691406
Epoch 220, val loss: 1.8153133392333984
Epoch 230, training loss: 651.9946899414062 = 1.8130106925964355 + 100.0 * 6.501817226409912
Epoch 230, val loss: 1.812255859375
Epoch 240, training loss: 650.4973754882812 = 1.8093823194503784 + 100.0 * 6.486879825592041
Epoch 240, val loss: 1.8093141317367554
Epoch 250, training loss: 649.209228515625 = 1.8058319091796875 + 100.0 * 6.474033832550049
Epoch 250, val loss: 1.8064384460449219
Epoch 260, training loss: 648.0313110351562 = 1.8023279905319214 + 100.0 * 6.462290287017822
Epoch 260, val loss: 1.8036073446273804
Epoch 270, training loss: 647.0403442382812 = 1.7987936735153198 + 100.0 * 6.452415943145752
Epoch 270, val loss: 1.800815224647522
Epoch 280, training loss: 646.0384521484375 = 1.7952697277069092 + 100.0 * 6.442431926727295
Epoch 280, val loss: 1.7980128526687622
Epoch 290, training loss: 645.0918579101562 = 1.7917076349258423 + 100.0 * 6.43300199508667
Epoch 290, val loss: 1.7951887845993042
Epoch 300, training loss: 644.2487182617188 = 1.788079857826233 + 100.0 * 6.4246063232421875
Epoch 300, val loss: 1.792326807975769
Epoch 310, training loss: 643.4459228515625 = 1.78436279296875 + 100.0 * 6.4166154861450195
Epoch 310, val loss: 1.7894059419631958
Epoch 320, training loss: 642.742431640625 = 1.7805174589157104 + 100.0 * 6.409619331359863
Epoch 320, val loss: 1.7863867282867432
Epoch 330, training loss: 642.0778198242188 = 1.7765604257583618 + 100.0 * 6.403012752532959
Epoch 330, val loss: 1.7832708358764648
Epoch 340, training loss: 641.3325805664062 = 1.7724415063858032 + 100.0 * 6.395601272583008
Epoch 340, val loss: 1.780042290687561
Epoch 350, training loss: 640.802490234375 = 1.7681609392166138 + 100.0 * 6.390342712402344
Epoch 350, val loss: 1.7767046689987183
Epoch 360, training loss: 640.2977294921875 = 1.7637145519256592 + 100.0 * 6.385339736938477
Epoch 360, val loss: 1.773193120956421
Epoch 370, training loss: 639.6415405273438 = 1.7590521574020386 + 100.0 * 6.3788251876831055
Epoch 370, val loss: 1.7695549726486206
Epoch 380, training loss: 639.1159057617188 = 1.7541953325271606 + 100.0 * 6.373616695404053
Epoch 380, val loss: 1.765749454498291
Epoch 390, training loss: 638.6637573242188 = 1.7491298913955688 + 100.0 * 6.369146347045898
Epoch 390, val loss: 1.7617765665054321
Epoch 400, training loss: 638.19775390625 = 1.7438409328460693 + 100.0 * 6.36453914642334
Epoch 400, val loss: 1.7576216459274292
Epoch 410, training loss: 637.7987060546875 = 1.7383171319961548 + 100.0 * 6.3606038093566895
Epoch 410, val loss: 1.7532684803009033
Epoch 420, training loss: 637.4325561523438 = 1.7325389385223389 + 100.0 * 6.35699987411499
Epoch 420, val loss: 1.748725175857544
Epoch 430, training loss: 636.9812622070312 = 1.7264952659606934 + 100.0 * 6.352547645568848
Epoch 430, val loss: 1.743955135345459
Epoch 440, training loss: 636.6001586914062 = 1.7202214002609253 + 100.0 * 6.348799228668213
Epoch 440, val loss: 1.7390044927597046
Epoch 450, training loss: 636.2380981445312 = 1.7136917114257812 + 100.0 * 6.345244407653809
Epoch 450, val loss: 1.7338327169418335
Epoch 460, training loss: 636.075439453125 = 1.7068836688995361 + 100.0 * 6.343685150146484
Epoch 460, val loss: 1.7284349203109741
Epoch 470, training loss: 635.6028442382812 = 1.699805736541748 + 100.0 * 6.3390302658081055
Epoch 470, val loss: 1.7228280305862427
Epoch 480, training loss: 635.273193359375 = 1.692462682723999 + 100.0 * 6.3358073234558105
Epoch 480, val loss: 1.7170032262802124
Epoch 490, training loss: 634.9341430664062 = 1.6848536729812622 + 100.0 * 6.332492828369141
Epoch 490, val loss: 1.71095871925354
Epoch 500, training loss: 634.7274780273438 = 1.6769850254058838 + 100.0 * 6.330504894256592
Epoch 500, val loss: 1.7047045230865479
Epoch 510, training loss: 634.4617919921875 = 1.6688469648361206 + 100.0 * 6.327929973602295
Epoch 510, val loss: 1.698219656944275
Epoch 520, training loss: 634.5148315429688 = 1.6603939533233643 + 100.0 * 6.3285441398620605
Epoch 520, val loss: 1.6915894746780396
Epoch 530, training loss: 633.8833618164062 = 1.651732325553894 + 100.0 * 6.3223161697387695
Epoch 530, val loss: 1.684677243232727
Epoch 540, training loss: 633.5977172851562 = 1.642807126045227 + 100.0 * 6.319549083709717
Epoch 540, val loss: 1.6775970458984375
Epoch 550, training loss: 633.33349609375 = 1.6336500644683838 + 100.0 * 6.316998481750488
Epoch 550, val loss: 1.670361042022705
Epoch 560, training loss: 633.0844116210938 = 1.6242755651474 + 100.0 * 6.314601421356201
Epoch 560, val loss: 1.6629849672317505
Epoch 570, training loss: 633.1369018554688 = 1.6147162914276123 + 100.0 * 6.315221786499023
Epoch 570, val loss: 1.655456781387329
Epoch 580, training loss: 632.7925415039062 = 1.6048332452774048 + 100.0 * 6.3118767738342285
Epoch 580, val loss: 1.6477221250534058
Epoch 590, training loss: 632.4690551757812 = 1.5948423147201538 + 100.0 * 6.308742046356201
Epoch 590, val loss: 1.639851689338684
Epoch 600, training loss: 632.228515625 = 1.5846620798110962 + 100.0 * 6.306438446044922
Epoch 600, val loss: 1.6319221258163452
Epoch 610, training loss: 632.021728515625 = 1.5743380784988403 + 100.0 * 6.304473876953125
Epoch 610, val loss: 1.6238768100738525
Epoch 620, training loss: 632.0938110351562 = 1.5638866424560547 + 100.0 * 6.305298805236816
Epoch 620, val loss: 1.6157671213150024
Epoch 630, training loss: 631.6712036132812 = 1.5532196760177612 + 100.0 * 6.301179885864258
Epoch 630, val loss: 1.6074974536895752
Epoch 640, training loss: 631.4688110351562 = 1.542482852935791 + 100.0 * 6.2992634773254395
Epoch 640, val loss: 1.5991806983947754
Epoch 650, training loss: 631.2902221679688 = 1.5316662788391113 + 100.0 * 6.297585487365723
Epoch 650, val loss: 1.5908448696136475
Epoch 660, training loss: 631.5885620117188 = 1.5208079814910889 + 100.0 * 6.300677299499512
Epoch 660, val loss: 1.5824404954910278
Epoch 670, training loss: 630.9905395507812 = 1.5097463130950928 + 100.0 * 6.2948079109191895
Epoch 670, val loss: 1.574051022529602
Epoch 680, training loss: 630.7716674804688 = 1.4987143278121948 + 100.0 * 6.292729377746582
Epoch 680, val loss: 1.5656450986862183
Epoch 690, training loss: 630.6088256835938 = 1.4876493215560913 + 100.0 * 6.2912116050720215
Epoch 690, val loss: 1.55728018283844
Epoch 700, training loss: 630.6019287109375 = 1.4765679836273193 + 100.0 * 6.291253566741943
Epoch 700, val loss: 1.5489650964736938
Epoch 710, training loss: 630.3811645507812 = 1.4654818773269653 + 100.0 * 6.289157390594482
Epoch 710, val loss: 1.5405040979385376
Epoch 720, training loss: 630.2388305664062 = 1.4543704986572266 + 100.0 * 6.287845134735107
Epoch 720, val loss: 1.5322190523147583
Epoch 730, training loss: 630.0523071289062 = 1.4432578086853027 + 100.0 * 6.286090850830078
Epoch 730, val loss: 1.5239061117172241
Epoch 740, training loss: 629.8783569335938 = 1.43213951587677 + 100.0 * 6.2844624519348145
Epoch 740, val loss: 1.5156196355819702
Epoch 750, training loss: 629.7190551757812 = 1.4210801124572754 + 100.0 * 6.282979488372803
Epoch 750, val loss: 1.507442831993103
Epoch 760, training loss: 629.6679077148438 = 1.410054326057434 + 100.0 * 6.282578468322754
Epoch 760, val loss: 1.4992727041244507
Epoch 770, training loss: 629.4848022460938 = 1.3990594148635864 + 100.0 * 6.280857563018799
Epoch 770, val loss: 1.4911644458770752
Epoch 780, training loss: 629.3400268554688 = 1.3880945444107056 + 100.0 * 6.279519081115723
Epoch 780, val loss: 1.4830536842346191
Epoch 790, training loss: 629.1929321289062 = 1.3772079944610596 + 100.0 * 6.2781572341918945
Epoch 790, val loss: 1.475059151649475
Epoch 800, training loss: 629.0443725585938 = 1.3663822412490845 + 100.0 * 6.276779651641846
Epoch 800, val loss: 1.4671164751052856
Epoch 810, training loss: 629.4273071289062 = 1.3555675745010376 + 100.0 * 6.280717372894287
Epoch 810, val loss: 1.4592481851577759
Epoch 820, training loss: 628.8993530273438 = 1.3448903560638428 + 100.0 * 6.2755446434021
Epoch 820, val loss: 1.4513260126113892
Epoch 830, training loss: 628.7504272460938 = 1.334195852279663 + 100.0 * 6.274162769317627
Epoch 830, val loss: 1.4434343576431274
Epoch 840, training loss: 628.58984375 = 1.3236163854599 + 100.0 * 6.27266263961792
Epoch 840, val loss: 1.4357143640518188
Epoch 850, training loss: 628.4459838867188 = 1.3131213188171387 + 100.0 * 6.271328449249268
Epoch 850, val loss: 1.4280589818954468
Epoch 860, training loss: 628.3416748046875 = 1.3027094602584839 + 100.0 * 6.270389556884766
Epoch 860, val loss: 1.4204726219177246
Epoch 870, training loss: 628.6507568359375 = 1.2923742532730103 + 100.0 * 6.273583889007568
Epoch 870, val loss: 1.412950038909912
Epoch 880, training loss: 628.1709594726562 = 1.2820720672607422 + 100.0 * 6.268889427185059
Epoch 880, val loss: 1.4053608179092407
Epoch 890, training loss: 628.1357421875 = 1.2718571424484253 + 100.0 * 6.268639087677002
Epoch 890, val loss: 1.3978499174118042
Epoch 900, training loss: 627.9506225585938 = 1.261753797531128 + 100.0 * 6.266888618469238
Epoch 900, val loss: 1.3904647827148438
Epoch 910, training loss: 627.8392944335938 = 1.2517805099487305 + 100.0 * 6.265875339508057
Epoch 910, val loss: 1.3831993341445923
Epoch 920, training loss: 627.76025390625 = 1.2418767213821411 + 100.0 * 6.265183448791504
Epoch 920, val loss: 1.3759961128234863
Epoch 930, training loss: 627.751220703125 = 1.2320774793624878 + 100.0 * 6.265191555023193
Epoch 930, val loss: 1.3688292503356934
Epoch 940, training loss: 627.5687255859375 = 1.2223247289657593 + 100.0 * 6.263463497161865
Epoch 940, val loss: 1.3618123531341553
Epoch 950, training loss: 627.4400634765625 = 1.2127045392990112 + 100.0 * 6.262273788452148
Epoch 950, val loss: 1.354816198348999
Epoch 960, training loss: 627.6425170898438 = 1.2032339572906494 + 100.0 * 6.264392852783203
Epoch 960, val loss: 1.3479795455932617
Epoch 970, training loss: 627.4219970703125 = 1.1936954259872437 + 100.0 * 6.262282848358154
Epoch 970, val loss: 1.3410202264785767
Epoch 980, training loss: 627.17626953125 = 1.184356451034546 + 100.0 * 6.2599196434021
Epoch 980, val loss: 1.3342912197113037
Epoch 990, training loss: 627.081787109375 = 1.1751152276992798 + 100.0 * 6.259066581726074
Epoch 990, val loss: 1.3276917934417725
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.562962962962963
0.8170795993674222
The final CL Acc:0.54691, 0.02014, The final GNN Acc:0.80988, 0.00603
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13122])
remove edge: torch.Size([2, 7974])
updated graph: torch.Size([2, 10540])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 861.6304931640625 = 1.9453951120376587 + 100.0 * 8.596851348876953
Epoch 0, val loss: 1.9527002573013306
Epoch 10, training loss: 861.6045532226562 = 1.9413056373596191 + 100.0 * 8.596632957458496
Epoch 10, val loss: 1.9483524560928345
Epoch 20, training loss: 861.5304565429688 = 1.9367283582687378 + 100.0 * 8.595937728881836
Epoch 20, val loss: 1.943450927734375
Epoch 30, training loss: 861.2719116210938 = 1.9314457178115845 + 100.0 * 8.593404769897461
Epoch 30, val loss: 1.9377375841140747
Epoch 40, training loss: 860.30224609375 = 1.925186276435852 + 100.0 * 8.583770751953125
Epoch 40, val loss: 1.9309319257736206
Epoch 50, training loss: 856.8919067382812 = 1.9175143241882324 + 100.0 * 8.54974365234375
Epoch 50, val loss: 1.9225759506225586
Epoch 60, training loss: 845.9501342773438 = 1.908176064491272 + 100.0 * 8.44041919708252
Epoch 60, val loss: 1.9125807285308838
Epoch 70, training loss: 812.1846313476562 = 1.8979891538619995 + 100.0 * 8.102866172790527
Epoch 70, val loss: 1.9019110202789307
Epoch 80, training loss: 763.2081909179688 = 1.8870329856872559 + 100.0 * 7.613211631774902
Epoch 80, val loss: 1.8905779123306274
Epoch 90, training loss: 738.904541015625 = 1.879717469215393 + 100.0 * 7.370248317718506
Epoch 90, val loss: 1.883193016052246
Epoch 100, training loss: 719.4932861328125 = 1.8738822937011719 + 100.0 * 7.176193714141846
Epoch 100, val loss: 1.8771133422851562
Epoch 110, training loss: 708.8125610351562 = 1.8674205541610718 + 100.0 * 7.069451332092285
Epoch 110, val loss: 1.870330572128296
Epoch 120, training loss: 701.6090698242188 = 1.8611334562301636 + 100.0 * 6.99747896194458
Epoch 120, val loss: 1.8638842105865479
Epoch 130, training loss: 694.1107788085938 = 1.8551541566848755 + 100.0 * 6.922555923461914
Epoch 130, val loss: 1.8578490018844604
Epoch 140, training loss: 685.969970703125 = 1.8496863842010498 + 100.0 * 6.841203212738037
Epoch 140, val loss: 1.8523457050323486
Epoch 150, training loss: 679.0248413085938 = 1.844864010810852 + 100.0 * 6.7718000411987305
Epoch 150, val loss: 1.8474115133285522
Epoch 160, training loss: 673.421875 = 1.8402762413024902 + 100.0 * 6.715816020965576
Epoch 160, val loss: 1.8426376581192017
Epoch 170, training loss: 668.6300048828125 = 1.835606336593628 + 100.0 * 6.667943954467773
Epoch 170, val loss: 1.8376994132995605
Epoch 180, training loss: 664.8156127929688 = 1.8309179544448853 + 100.0 * 6.629846572875977
Epoch 180, val loss: 1.8326852321624756
Epoch 190, training loss: 661.6021728515625 = 1.8261324167251587 + 100.0 * 6.5977606773376465
Epoch 190, val loss: 1.8276748657226562
Epoch 200, training loss: 658.7349853515625 = 1.8213204145431519 + 100.0 * 6.569136619567871
Epoch 200, val loss: 1.8226895332336426
Epoch 210, training loss: 656.261962890625 = 1.8165699243545532 + 100.0 * 6.5444536209106445
Epoch 210, val loss: 1.8178194761276245
Epoch 220, training loss: 654.169921875 = 1.8118560314178467 + 100.0 * 6.523581027984619
Epoch 220, val loss: 1.8130369186401367
Epoch 230, training loss: 652.5233764648438 = 1.8070802688598633 + 100.0 * 6.507163047790527
Epoch 230, val loss: 1.8081262111663818
Epoch 240, training loss: 650.7860107421875 = 1.8022632598876953 + 100.0 * 6.489837646484375
Epoch 240, val loss: 1.8031679391860962
Epoch 250, training loss: 649.3967895507812 = 1.797361135482788 + 100.0 * 6.475994110107422
Epoch 250, val loss: 1.7981445789337158
Epoch 260, training loss: 648.1223754882812 = 1.7923533916473389 + 100.0 * 6.4633002281188965
Epoch 260, val loss: 1.793061375617981
Epoch 270, training loss: 646.9589233398438 = 1.7872434854507446 + 100.0 * 6.451716899871826
Epoch 270, val loss: 1.7878845930099487
Epoch 280, training loss: 645.8939208984375 = 1.7820041179656982 + 100.0 * 6.441119194030762
Epoch 280, val loss: 1.7826062440872192
Epoch 290, training loss: 645.1417846679688 = 1.7766207456588745 + 100.0 * 6.433651447296143
Epoch 290, val loss: 1.7771717309951782
Epoch 300, training loss: 644.1055908203125 = 1.7710169553756714 + 100.0 * 6.423346042633057
Epoch 300, val loss: 1.7715961933135986
Epoch 310, training loss: 643.2162475585938 = 1.7652522325515747 + 100.0 * 6.4145097732543945
Epoch 310, val loss: 1.7658789157867432
Epoch 320, training loss: 642.412109375 = 1.7592957019805908 + 100.0 * 6.406528472900391
Epoch 320, val loss: 1.7599854469299316
Epoch 330, training loss: 641.6798095703125 = 1.7531189918518066 + 100.0 * 6.399266719818115
Epoch 330, val loss: 1.7539035081863403
Epoch 340, training loss: 641.2762451171875 = 1.7467198371887207 + 100.0 * 6.3952956199646
Epoch 340, val loss: 1.7476052045822144
Epoch 350, training loss: 640.388671875 = 1.7399884462356567 + 100.0 * 6.386487007141113
Epoch 350, val loss: 1.7410950660705566
Epoch 360, training loss: 639.8290405273438 = 1.7330102920532227 + 100.0 * 6.380959987640381
Epoch 360, val loss: 1.7343378067016602
Epoch 370, training loss: 639.2786865234375 = 1.7257353067398071 + 100.0 * 6.375529766082764
Epoch 370, val loss: 1.7273069620132446
Epoch 380, training loss: 638.8037109375 = 1.7181345224380493 + 100.0 * 6.370855808258057
Epoch 380, val loss: 1.7200120687484741
Epoch 390, training loss: 638.3599243164062 = 1.7102113962173462 + 100.0 * 6.366497039794922
Epoch 390, val loss: 1.7124357223510742
Epoch 400, training loss: 637.9348754882812 = 1.7019835710525513 + 100.0 * 6.362329006195068
Epoch 400, val loss: 1.7045811414718628
Epoch 410, training loss: 637.4808349609375 = 1.6934709548950195 + 100.0 * 6.357873439788818
Epoch 410, val loss: 1.6964449882507324
Epoch 420, training loss: 637.0966186523438 = 1.684628963470459 + 100.0 * 6.354119777679443
Epoch 420, val loss: 1.688032865524292
Epoch 430, training loss: 636.7717895507812 = 1.6754590272903442 + 100.0 * 6.350963115692139
Epoch 430, val loss: 1.679316759109497
Epoch 440, training loss: 636.325439453125 = 1.665916919708252 + 100.0 * 6.346595764160156
Epoch 440, val loss: 1.670342206954956
Epoch 450, training loss: 635.9652099609375 = 1.6560759544372559 + 100.0 * 6.3430914878845215
Epoch 450, val loss: 1.661071538925171
Epoch 460, training loss: 635.71240234375 = 1.6459298133850098 + 100.0 * 6.340664386749268
Epoch 460, val loss: 1.6515127420425415
Epoch 470, training loss: 635.3121948242188 = 1.6354477405548096 + 100.0 * 6.336767673492432
Epoch 470, val loss: 1.6417005062103271
Epoch 480, training loss: 634.9136352539062 = 1.6246787309646606 + 100.0 * 6.332889556884766
Epoch 480, val loss: 1.6316301822662354
Epoch 490, training loss: 635.2095336914062 = 1.6136457920074463 + 100.0 * 6.335958957672119
Epoch 490, val loss: 1.6212692260742188
Epoch 500, training loss: 634.3571166992188 = 1.6022015810012817 + 100.0 * 6.327549457550049
Epoch 500, val loss: 1.6106925010681152
Epoch 510, training loss: 633.9855346679688 = 1.590533971786499 + 100.0 * 6.323950290679932
Epoch 510, val loss: 1.5999219417572021
Epoch 520, training loss: 633.6943359375 = 1.5786243677139282 + 100.0 * 6.321157455444336
Epoch 520, val loss: 1.5889596939086914
Epoch 530, training loss: 633.4081420898438 = 1.5665045976638794 + 100.0 * 6.318416118621826
Epoch 530, val loss: 1.5778406858444214
Epoch 540, training loss: 633.1432495117188 = 1.5541647672653198 + 100.0 * 6.315891265869141
Epoch 540, val loss: 1.5665701627731323
Epoch 550, training loss: 633.0472412109375 = 1.5415858030319214 + 100.0 * 6.315056800842285
Epoch 550, val loss: 1.5551550388336182
Epoch 560, training loss: 632.720703125 = 1.5287820100784302 + 100.0 * 6.31191873550415
Epoch 560, val loss: 1.5435986518859863
Epoch 570, training loss: 632.4117431640625 = 1.5158544778823853 + 100.0 * 6.308958530426025
Epoch 570, val loss: 1.5319843292236328
Epoch 580, training loss: 632.2025146484375 = 1.5027819871902466 + 100.0 * 6.306997299194336
Epoch 580, val loss: 1.5202951431274414
Epoch 590, training loss: 632.0650024414062 = 1.4895745515823364 + 100.0 * 6.3057541847229
Epoch 590, val loss: 1.5085471868515015
Epoch 600, training loss: 631.7905883789062 = 1.4761688709259033 + 100.0 * 6.3031439781188965
Epoch 600, val loss: 1.4967297315597534
Epoch 610, training loss: 631.544677734375 = 1.4627048969268799 + 100.0 * 6.3008198738098145
Epoch 610, val loss: 1.484912633895874
Epoch 620, training loss: 631.330810546875 = 1.449177622795105 + 100.0 * 6.298816680908203
Epoch 620, val loss: 1.4730987548828125
Epoch 630, training loss: 631.40625 = 1.435579538345337 + 100.0 * 6.29970645904541
Epoch 630, val loss: 1.4612791538238525
Epoch 640, training loss: 631.0368041992188 = 1.4217609167099 + 100.0 * 6.2961506843566895
Epoch 640, val loss: 1.4494348764419556
Epoch 650, training loss: 630.7841186523438 = 1.40800940990448 + 100.0 * 6.293760776519775
Epoch 650, val loss: 1.4376552104949951
Epoch 660, training loss: 630.5719604492188 = 1.3941291570663452 + 100.0 * 6.291778564453125
Epoch 660, val loss: 1.4258747100830078
Epoch 670, training loss: 630.456787109375 = 1.3802425861358643 + 100.0 * 6.290765285491943
Epoch 670, val loss: 1.4141325950622559
Epoch 680, training loss: 630.3033447265625 = 1.3662711381912231 + 100.0 * 6.289371013641357
Epoch 680, val loss: 1.4024327993392944
Epoch 690, training loss: 630.0927734375 = 1.3522652387619019 + 100.0 * 6.287405014038086
Epoch 690, val loss: 1.3907177448272705
Epoch 700, training loss: 629.9234008789062 = 1.338243007659912 + 100.0 * 6.28585147857666
Epoch 700, val loss: 1.379064679145813
Epoch 710, training loss: 630.0772705078125 = 1.3242071866989136 + 100.0 * 6.287530422210693
Epoch 710, val loss: 1.36747145652771
Epoch 720, training loss: 629.5693359375 = 1.3100563287734985 + 100.0 * 6.2825927734375
Epoch 720, val loss: 1.3559354543685913
Epoch 730, training loss: 629.4389038085938 = 1.2959046363830566 + 100.0 * 6.281430244445801
Epoch 730, val loss: 1.344458818435669
Epoch 740, training loss: 629.2515258789062 = 1.2818433046340942 + 100.0 * 6.279696941375732
Epoch 740, val loss: 1.3330507278442383
Epoch 750, training loss: 629.0986938476562 = 1.267748236656189 + 100.0 * 6.278309345245361
Epoch 750, val loss: 1.32170832157135
Epoch 760, training loss: 629.288330078125 = 1.2536753416061401 + 100.0 * 6.280346393585205
Epoch 760, val loss: 1.310421347618103
Epoch 770, training loss: 628.8543701171875 = 1.2395604848861694 + 100.0 * 6.276147842407227
Epoch 770, val loss: 1.2991787195205688
Epoch 780, training loss: 628.7212524414062 = 1.2255280017852783 + 100.0 * 6.274957180023193
Epoch 780, val loss: 1.2880809307098389
Epoch 790, training loss: 628.5321044921875 = 1.211559534072876 + 100.0 * 6.273205757141113
Epoch 790, val loss: 1.2770603895187378
Epoch 800, training loss: 628.5369262695312 = 1.1976414918899536 + 100.0 * 6.273392677307129
Epoch 800, val loss: 1.2661635875701904
Epoch 810, training loss: 628.4257202148438 = 1.1838144063949585 + 100.0 * 6.272418975830078
Epoch 810, val loss: 1.2553800344467163
Epoch 820, training loss: 628.1799926757812 = 1.169995903968811 + 100.0 * 6.270100116729736
Epoch 820, val loss: 1.2446993589401245
Epoch 830, training loss: 628.0595703125 = 1.1563456058502197 + 100.0 * 6.269032001495361
Epoch 830, val loss: 1.234177827835083
Epoch 840, training loss: 627.9085083007812 = 1.142839789390564 + 100.0 * 6.2676568031311035
Epoch 840, val loss: 1.2238366603851318
Epoch 850, training loss: 627.7792358398438 = 1.1294831037521362 + 100.0 * 6.266497611999512
Epoch 850, val loss: 1.2136976718902588
Epoch 860, training loss: 627.9816284179688 = 1.1162508726119995 + 100.0 * 6.268653392791748
Epoch 860, val loss: 1.203743577003479
Epoch 870, training loss: 627.6922607421875 = 1.1031765937805176 + 100.0 * 6.265891075134277
Epoch 870, val loss: 1.193956971168518
Epoch 880, training loss: 627.473876953125 = 1.0902336835861206 + 100.0 * 6.26383638381958
Epoch 880, val loss: 1.1843630075454712
Epoch 890, training loss: 627.3399658203125 = 1.0775036811828613 + 100.0 * 6.262624740600586
Epoch 890, val loss: 1.1749954223632812
Epoch 900, training loss: 627.2409057617188 = 1.0649994611740112 + 100.0 * 6.261758804321289
Epoch 900, val loss: 1.1658817529678345
Epoch 910, training loss: 627.71630859375 = 1.0526421070098877 + 100.0 * 6.266636848449707
Epoch 910, val loss: 1.1569879055023193
Epoch 920, training loss: 627.0810546875 = 1.040549874305725 + 100.0 * 6.26040506362915
Epoch 920, val loss: 1.1482877731323242
Epoch 930, training loss: 626.949951171875 = 1.0285924673080444 + 100.0 * 6.259213924407959
Epoch 930, val loss: 1.1398234367370605
Epoch 940, training loss: 626.8319702148438 = 1.0169583559036255 + 100.0 * 6.258150100708008
Epoch 940, val loss: 1.131622314453125
Epoch 950, training loss: 626.7066040039062 = 1.0055383443832397 + 100.0 * 6.257010459899902
Epoch 950, val loss: 1.1237127780914307
Epoch 960, training loss: 626.6089477539062 = 0.99434494972229 + 100.0 * 6.256146430969238
Epoch 960, val loss: 1.1160310506820679
Epoch 970, training loss: 626.7481079101562 = 0.9833475351333618 + 100.0 * 6.25764799118042
Epoch 970, val loss: 1.10856032371521
Epoch 980, training loss: 626.6241455078125 = 0.972613513469696 + 100.0 * 6.2565155029296875
Epoch 980, val loss: 1.1013544797897339
Epoch 990, training loss: 626.407958984375 = 0.9619935750961304 + 100.0 * 6.254459857940674
Epoch 990, val loss: 1.094337821006775
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6074074074074074
0.8344754876120191
=== training gcn model ===
Epoch 0, training loss: 861.6091918945312 = 1.9287352561950684 + 100.0 * 8.59680461883545
Epoch 0, val loss: 1.9356547594070435
Epoch 10, training loss: 861.5733032226562 = 1.9249967336654663 + 100.0 * 8.59648323059082
Epoch 10, val loss: 1.9315863847732544
Epoch 20, training loss: 861.4734497070312 = 1.9208691120147705 + 100.0 * 8.595525741577148
Epoch 20, val loss: 1.9270347356796265
Epoch 30, training loss: 861.1466674804688 = 1.9160950183868408 + 100.0 * 8.592306137084961
Epoch 30, val loss: 1.921732783317566
Epoch 40, training loss: 859.9955444335938 = 1.9104114770889282 + 100.0 * 8.580851554870605
Epoch 40, val loss: 1.9153697490692139
Epoch 50, training loss: 856.0753173828125 = 1.9034178256988525 + 100.0 * 8.541718482971191
Epoch 50, val loss: 1.9075111150741577
Epoch 60, training loss: 843.2996215820312 = 1.8950104713439941 + 100.0 * 8.414046287536621
Epoch 60, val loss: 1.8982046842575073
Epoch 70, training loss: 804.027587890625 = 1.8857759237289429 + 100.0 * 8.021417617797852
Epoch 70, val loss: 1.8881181478500366
Epoch 80, training loss: 741.4251098632812 = 1.8758330345153809 + 100.0 * 7.3954925537109375
Epoch 80, val loss: 1.8775490522384644
Epoch 90, training loss: 726.41845703125 = 1.8677700757980347 + 100.0 * 7.245506286621094
Epoch 90, val loss: 1.8696739673614502
Epoch 100, training loss: 711.485595703125 = 1.862022876739502 + 100.0 * 7.096235752105713
Epoch 100, val loss: 1.8642816543579102
Epoch 110, training loss: 703.4130859375 = 1.8569051027297974 + 100.0 * 7.015561580657959
Epoch 110, val loss: 1.8590043783187866
Epoch 120, training loss: 695.3198852539062 = 1.8515801429748535 + 100.0 * 6.934683322906494
Epoch 120, val loss: 1.8534690141677856
Epoch 130, training loss: 687.1920166015625 = 1.8472135066986084 + 100.0 * 6.853447914123535
Epoch 130, val loss: 1.8490827083587646
Epoch 140, training loss: 680.2964477539062 = 1.8432400226593018 + 100.0 * 6.784531593322754
Epoch 140, val loss: 1.84505033493042
Epoch 150, training loss: 674.6312866210938 = 1.8397588729858398 + 100.0 * 6.727914810180664
Epoch 150, val loss: 1.8415544033050537
Epoch 160, training loss: 669.7322998046875 = 1.8363063335418701 + 100.0 * 6.678959846496582
Epoch 160, val loss: 1.8381294012069702
Epoch 170, training loss: 665.555908203125 = 1.8329757452011108 + 100.0 * 6.6372294425964355
Epoch 170, val loss: 1.8348478078842163
Epoch 180, training loss: 662.093017578125 = 1.8296304941177368 + 100.0 * 6.602633953094482
Epoch 180, val loss: 1.831573486328125
Epoch 190, training loss: 659.149658203125 = 1.8262248039245605 + 100.0 * 6.5732340812683105
Epoch 190, val loss: 1.8282045125961304
Epoch 200, training loss: 656.7440185546875 = 1.8228867053985596 + 100.0 * 6.549211502075195
Epoch 200, val loss: 1.824906349182129
Epoch 210, training loss: 654.5782470703125 = 1.8195651769638062 + 100.0 * 6.527586460113525
Epoch 210, val loss: 1.8215727806091309
Epoch 220, training loss: 652.706298828125 = 1.8161966800689697 + 100.0 * 6.5089006423950195
Epoch 220, val loss: 1.8182573318481445
Epoch 230, training loss: 651.074951171875 = 1.81281316280365 + 100.0 * 6.492621421813965
Epoch 230, val loss: 1.8149217367172241
Epoch 240, training loss: 649.5845947265625 = 1.8094085454940796 + 100.0 * 6.477752208709717
Epoch 240, val loss: 1.8116252422332764
Epoch 250, training loss: 648.2081298828125 = 1.80601167678833 + 100.0 * 6.4640212059021
Epoch 250, val loss: 1.808356523513794
Epoch 260, training loss: 646.94873046875 = 1.8025670051574707 + 100.0 * 6.4514617919921875
Epoch 260, val loss: 1.8050754070281982
Epoch 270, training loss: 645.76171875 = 1.7990517616271973 + 100.0 * 6.439626216888428
Epoch 270, val loss: 1.8017733097076416
Epoch 280, training loss: 644.9783935546875 = 1.795485258102417 + 100.0 * 6.431829452514648
Epoch 280, val loss: 1.798427939414978
Epoch 290, training loss: 643.86669921875 = 1.7917553186416626 + 100.0 * 6.420749664306641
Epoch 290, val loss: 1.7950043678283691
Epoch 300, training loss: 642.955810546875 = 1.787919521331787 + 100.0 * 6.411679267883301
Epoch 300, val loss: 1.7915034294128418
Epoch 310, training loss: 642.1729736328125 = 1.7839133739471436 + 100.0 * 6.403890132904053
Epoch 310, val loss: 1.787903904914856
Epoch 320, training loss: 641.4724731445312 = 1.7797726392745972 + 100.0 * 6.3969268798828125
Epoch 320, val loss: 1.7842029333114624
Epoch 330, training loss: 640.9129028320312 = 1.7753987312316895 + 100.0 * 6.3913750648498535
Epoch 330, val loss: 1.7803374528884888
Epoch 340, training loss: 640.267578125 = 1.7708839178085327 + 100.0 * 6.38496732711792
Epoch 340, val loss: 1.7763290405273438
Epoch 350, training loss: 639.6378173828125 = 1.7661350965499878 + 100.0 * 6.378716945648193
Epoch 350, val loss: 1.7721749544143677
Epoch 360, training loss: 639.0867309570312 = 1.7611582279205322 + 100.0 * 6.373255729675293
Epoch 360, val loss: 1.7678422927856445
Epoch 370, training loss: 638.8148193359375 = 1.7559394836425781 + 100.0 * 6.370589256286621
Epoch 370, val loss: 1.7633243799209595
Epoch 380, training loss: 638.17431640625 = 1.7504550218582153 + 100.0 * 6.364238739013672
Epoch 380, val loss: 1.7585879564285278
Epoch 390, training loss: 637.6719360351562 = 1.7447067499160767 + 100.0 * 6.359272480010986
Epoch 390, val loss: 1.753636360168457
Epoch 400, training loss: 637.3259887695312 = 1.7387080192565918 + 100.0 * 6.355872631072998
Epoch 400, val loss: 1.748490571975708
Epoch 410, training loss: 636.8692626953125 = 1.7323942184448242 + 100.0 * 6.351368427276611
Epoch 410, val loss: 1.7430965900421143
Epoch 420, training loss: 636.3703002929688 = 1.725808024406433 + 100.0 * 6.346444606781006
Epoch 420, val loss: 1.7375004291534424
Epoch 430, training loss: 635.9891967773438 = 1.7189091444015503 + 100.0 * 6.342702865600586
Epoch 430, val loss: 1.7316341400146484
Epoch 440, training loss: 635.7432250976562 = 1.711672306060791 + 100.0 * 6.340315341949463
Epoch 440, val loss: 1.7254892587661743
Epoch 450, training loss: 635.29638671875 = 1.7041281461715698 + 100.0 * 6.335922718048096
Epoch 450, val loss: 1.7191290855407715
Epoch 460, training loss: 634.8873291015625 = 1.6962730884552002 + 100.0 * 6.331910610198975
Epoch 460, val loss: 1.7124933004379272
Epoch 470, training loss: 634.534423828125 = 1.6880682706832886 + 100.0 * 6.328464031219482
Epoch 470, val loss: 1.7055671215057373
Epoch 480, training loss: 634.2459106445312 = 1.6795251369476318 + 100.0 * 6.325664043426514
Epoch 480, val loss: 1.6983734369277954
Epoch 490, training loss: 634.0218505859375 = 1.6705687046051025 + 100.0 * 6.323512554168701
Epoch 490, val loss: 1.690903902053833
Epoch 500, training loss: 633.641357421875 = 1.661316990852356 + 100.0 * 6.31980037689209
Epoch 500, val loss: 1.6831271648406982
Epoch 510, training loss: 633.3214721679688 = 1.651721715927124 + 100.0 * 6.316697597503662
Epoch 510, val loss: 1.6750813722610474
Epoch 520, training loss: 633.2408447265625 = 1.641815423965454 + 100.0 * 6.315989971160889
Epoch 520, val loss: 1.6668087244033813
Epoch 530, training loss: 632.8883056640625 = 1.6315240859985352 + 100.0 * 6.312567710876465
Epoch 530, val loss: 1.6582562923431396
Epoch 540, training loss: 632.5384521484375 = 1.620880365371704 + 100.0 * 6.309175491333008
Epoch 540, val loss: 1.649442195892334
Epoch 550, training loss: 632.302490234375 = 1.6099647283554077 + 100.0 * 6.306924819946289
Epoch 550, val loss: 1.6404731273651123
Epoch 560, training loss: 632.090576171875 = 1.5987422466278076 + 100.0 * 6.30491828918457
Epoch 560, val loss: 1.631203055381775
Epoch 570, training loss: 631.8426513671875 = 1.5872446298599243 + 100.0 * 6.302554130554199
Epoch 570, val loss: 1.6217284202575684
Epoch 580, training loss: 631.5939331054688 = 1.575465202331543 + 100.0 * 6.300184726715088
Epoch 580, val loss: 1.6120425462722778
Epoch 590, training loss: 631.4354858398438 = 1.5634636878967285 + 100.0 * 6.298720359802246
Epoch 590, val loss: 1.6022109985351562
Epoch 600, training loss: 631.2177124023438 = 1.5511232614517212 + 100.0 * 6.296665668487549
Epoch 600, val loss: 1.5921553373336792
Epoch 610, training loss: 630.99560546875 = 1.5386236906051636 + 100.0 * 6.294569969177246
Epoch 610, val loss: 1.5820062160491943
Epoch 620, training loss: 630.7780151367188 = 1.5258673429489136 + 100.0 * 6.2925214767456055
Epoch 620, val loss: 1.5716910362243652
Epoch 630, training loss: 630.73046875 = 1.5129411220550537 + 100.0 * 6.29217529296875
Epoch 630, val loss: 1.5613349676132202
Epoch 640, training loss: 630.4827270507812 = 1.4998282194137573 + 100.0 * 6.289829254150391
Epoch 640, val loss: 1.5506486892700195
Epoch 650, training loss: 630.3438110351562 = 1.4865237474441528 + 100.0 * 6.288573265075684
Epoch 650, val loss: 1.540041208267212
Epoch 660, training loss: 630.2474365234375 = 1.4731388092041016 + 100.0 * 6.287742614746094
Epoch 660, val loss: 1.5293610095977783
Epoch 670, training loss: 629.9273071289062 = 1.459614634513855 + 100.0 * 6.284677028656006
Epoch 670, val loss: 1.5186384916305542
Epoch 680, training loss: 629.7266235351562 = 1.4460070133209229 + 100.0 * 6.282806396484375
Epoch 680, val loss: 1.5078599452972412
Epoch 690, training loss: 629.5570678710938 = 1.432358980178833 + 100.0 * 6.281247138977051
Epoch 690, val loss: 1.497071385383606
Epoch 700, training loss: 629.5890502929688 = 1.4185999631881714 + 100.0 * 6.281704425811768
Epoch 700, val loss: 1.4862098693847656
Epoch 710, training loss: 629.397705078125 = 1.4049153327941895 + 100.0 * 6.279927730560303
Epoch 710, val loss: 1.4756195545196533
Epoch 720, training loss: 629.1112060546875 = 1.3910596370697021 + 100.0 * 6.2772016525268555
Epoch 720, val loss: 1.4647926092147827
Epoch 730, training loss: 628.9758911132812 = 1.3772969245910645 + 100.0 * 6.2759857177734375
Epoch 730, val loss: 1.454114556312561
Epoch 740, training loss: 628.8541259765625 = 1.3635793924331665 + 100.0 * 6.274905681610107
Epoch 740, val loss: 1.4435200691223145
Epoch 750, training loss: 628.732177734375 = 1.349790096282959 + 100.0 * 6.2738237380981445
Epoch 750, val loss: 1.4329816102981567
Epoch 760, training loss: 628.6121215820312 = 1.336039423942566 + 100.0 * 6.27276086807251
Epoch 760, val loss: 1.4224004745483398
Epoch 770, training loss: 628.448486328125 = 1.3224091529846191 + 100.0 * 6.271260738372803
Epoch 770, val loss: 1.4120211601257324
Epoch 780, training loss: 628.3160400390625 = 1.308862566947937 + 100.0 * 6.270071983337402
Epoch 780, val loss: 1.4017622470855713
Epoch 790, training loss: 628.46875 = 1.2954120635986328 + 100.0 * 6.271733283996582
Epoch 790, val loss: 1.3916431665420532
Epoch 800, training loss: 628.150634765625 = 1.2819411754608154 + 100.0 * 6.2686872482299805
Epoch 800, val loss: 1.381574273109436
Epoch 810, training loss: 627.9639892578125 = 1.2686233520507812 + 100.0 * 6.266953468322754
Epoch 810, val loss: 1.3715898990631104
Epoch 820, training loss: 627.8639526367188 = 1.2554410696029663 + 100.0 * 6.266085147857666
Epoch 820, val loss: 1.3617589473724365
Epoch 830, training loss: 627.948486328125 = 1.242411732673645 + 100.0 * 6.267060279846191
Epoch 830, val loss: 1.3521935939788818
Epoch 840, training loss: 627.6589965820312 = 1.2293046712875366 + 100.0 * 6.264297008514404
Epoch 840, val loss: 1.3425034284591675
Epoch 850, training loss: 627.5548706054688 = 1.2165158987045288 + 100.0 * 6.263383865356445
Epoch 850, val loss: 1.3332512378692627
Epoch 860, training loss: 627.4138793945312 = 1.203818440437317 + 100.0 * 6.262100696563721
Epoch 860, val loss: 1.3239179849624634
Epoch 870, training loss: 627.2911376953125 = 1.1913100481033325 + 100.0 * 6.260998725891113
Epoch 870, val loss: 1.3149147033691406
Epoch 880, training loss: 627.2095947265625 = 1.178943395614624 + 100.0 * 6.260306358337402
Epoch 880, val loss: 1.3060308694839478
Epoch 890, training loss: 627.3018798828125 = 1.166675329208374 + 100.0 * 6.261352062225342
Epoch 890, val loss: 1.2972166538238525
Epoch 900, training loss: 627.1463623046875 = 1.15457284450531 + 100.0 * 6.259917736053467
Epoch 900, val loss: 1.28876531124115
Epoch 910, training loss: 626.94140625 = 1.142712116241455 + 100.0 * 6.257987022399902
Epoch 910, val loss: 1.2803902626037598
Epoch 920, training loss: 626.8334350585938 = 1.1309722661972046 + 100.0 * 6.257024765014648
Epoch 920, val loss: 1.2722530364990234
Epoch 930, training loss: 626.7342529296875 = 1.1194241046905518 + 100.0 * 6.256147861480713
Epoch 930, val loss: 1.2642549276351929
Epoch 940, training loss: 626.9913330078125 = 1.1080374717712402 + 100.0 * 6.258832931518555
Epoch 940, val loss: 1.256409764289856
Epoch 950, training loss: 626.6587524414062 = 1.0967272520065308 + 100.0 * 6.255620002746582
Epoch 950, val loss: 1.248674750328064
Epoch 960, training loss: 626.5291137695312 = 1.0856789350509644 + 100.0 * 6.254434108734131
Epoch 960, val loss: 1.241242527961731
Epoch 970, training loss: 626.3719482421875 = 1.0748260021209717 + 100.0 * 6.252971172332764
Epoch 970, val loss: 1.2339504957199097
Epoch 980, training loss: 626.3212280273438 = 1.0641582012176514 + 100.0 * 6.252571105957031
Epoch 980, val loss: 1.2269197702407837
Epoch 990, training loss: 626.5357055664062 = 1.0536662340164185 + 100.0 * 6.254820823669434
Epoch 990, val loss: 1.2200053930282593
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.5814814814814815
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 861.6377563476562 = 1.9535422325134277 + 100.0 * 8.596841812133789
Epoch 0, val loss: 1.9373986721038818
Epoch 10, training loss: 861.6076049804688 = 1.9493324756622314 + 100.0 * 8.596582412719727
Epoch 10, val loss: 1.933395504951477
Epoch 20, training loss: 861.5170288085938 = 1.94469153881073 + 100.0 * 8.595723152160645
Epoch 20, val loss: 1.9289343357086182
Epoch 30, training loss: 861.188720703125 = 1.9394142627716064 + 100.0 * 8.592493057250977
Epoch 30, val loss: 1.9237688779830933
Epoch 40, training loss: 859.967529296875 = 1.933215618133545 + 100.0 * 8.580343246459961
Epoch 40, val loss: 1.9175946712493896
Epoch 50, training loss: 855.8282470703125 = 1.9257744550704956 + 100.0 * 8.539024353027344
Epoch 50, val loss: 1.910109043121338
Epoch 60, training loss: 843.8651733398438 = 1.9168822765350342 + 100.0 * 8.419483184814453
Epoch 60, val loss: 1.9011831283569336
Epoch 70, training loss: 817.0029907226562 = 1.9073665142059326 + 100.0 * 8.150956153869629
Epoch 70, val loss: 1.891873836517334
Epoch 80, training loss: 790.305419921875 = 1.8975008726119995 + 100.0 * 7.8840789794921875
Epoch 80, val loss: 1.8825061321258545
Epoch 90, training loss: 771.9097900390625 = 1.8885209560394287 + 100.0 * 7.700212478637695
Epoch 90, val loss: 1.8743928670883179
Epoch 100, training loss: 742.7940673828125 = 1.8805397748947144 + 100.0 * 7.409134864807129
Epoch 100, val loss: 1.8672897815704346
Epoch 110, training loss: 716.60546875 = 1.8723851442337036 + 100.0 * 7.1473307609558105
Epoch 110, val loss: 1.8600947856903076
Epoch 120, training loss: 700.1785888671875 = 1.864975094795227 + 100.0 * 6.98313570022583
Epoch 120, val loss: 1.8536192178726196
Epoch 130, training loss: 689.4993896484375 = 1.8583528995513916 + 100.0 * 6.876410484313965
Epoch 130, val loss: 1.8478187322616577
Epoch 140, training loss: 680.8970336914062 = 1.8522955179214478 + 100.0 * 6.790447235107422
Epoch 140, val loss: 1.8426436185836792
Epoch 150, training loss: 674.4940795898438 = 1.846470832824707 + 100.0 * 6.726475715637207
Epoch 150, val loss: 1.8378148078918457
Epoch 160, training loss: 669.4728393554688 = 1.8406565189361572 + 100.0 * 6.676321983337402
Epoch 160, val loss: 1.8331128358840942
Epoch 170, training loss: 665.79248046875 = 1.8350218534469604 + 100.0 * 6.6395745277404785
Epoch 170, val loss: 1.8284834623336792
Epoch 180, training loss: 662.6812744140625 = 1.829561471939087 + 100.0 * 6.608516693115234
Epoch 180, val loss: 1.823936939239502
Epoch 190, training loss: 659.8319702148438 = 1.8243016004562378 + 100.0 * 6.580076694488525
Epoch 190, val loss: 1.8195719718933105
Epoch 200, training loss: 657.3248901367188 = 1.8192191123962402 + 100.0 * 6.555056571960449
Epoch 200, val loss: 1.8153893947601318
Epoch 210, training loss: 655.0962524414062 = 1.8142342567443848 + 100.0 * 6.532820701599121
Epoch 210, val loss: 1.8112688064575195
Epoch 220, training loss: 653.2001342773438 = 1.8092706203460693 + 100.0 * 6.513908386230469
Epoch 220, val loss: 1.807181477546692
Epoch 230, training loss: 651.4796142578125 = 1.8043544292449951 + 100.0 * 6.4967522621154785
Epoch 230, val loss: 1.8031030893325806
Epoch 240, training loss: 649.9840698242188 = 1.7994285821914673 + 100.0 * 6.481846332550049
Epoch 240, val loss: 1.7990028858184814
Epoch 250, training loss: 648.99658203125 = 1.7944461107254028 + 100.0 * 6.472021579742432
Epoch 250, val loss: 1.794838309288025
Epoch 260, training loss: 647.5855712890625 = 1.7893989086151123 + 100.0 * 6.457961559295654
Epoch 260, val loss: 1.7905627489089966
Epoch 270, training loss: 646.4662475585938 = 1.7842668294906616 + 100.0 * 6.44681978225708
Epoch 270, val loss: 1.786202073097229
Epoch 280, training loss: 645.4430541992188 = 1.779038667678833 + 100.0 * 6.43664026260376
Epoch 280, val loss: 1.781729817390442
Epoch 290, training loss: 644.5767211914062 = 1.7736811637878418 + 100.0 * 6.428030490875244
Epoch 290, val loss: 1.7771235704421997
Epoch 300, training loss: 643.76025390625 = 1.7681653499603271 + 100.0 * 6.419920444488525
Epoch 300, val loss: 1.7723774909973145
Epoch 310, training loss: 642.9406127929688 = 1.762476921081543 + 100.0 * 6.4117817878723145
Epoch 310, val loss: 1.7674740552902222
Epoch 320, training loss: 642.2070922851562 = 1.7566012144088745 + 100.0 * 6.404505252838135
Epoch 320, val loss: 1.7623921632766724
Epoch 330, training loss: 641.53369140625 = 1.7505191564559937 + 100.0 * 6.397831439971924
Epoch 330, val loss: 1.757120966911316
Epoch 340, training loss: 641.0086669921875 = 1.744184970855713 + 100.0 * 6.39264440536499
Epoch 340, val loss: 1.7516076564788818
Epoch 350, training loss: 640.3995971679688 = 1.737583875656128 + 100.0 * 6.386620044708252
Epoch 350, val loss: 1.745879054069519
Epoch 360, training loss: 639.7813720703125 = 1.7307274341583252 + 100.0 * 6.38050651550293
Epoch 360, val loss: 1.7399176359176636
Epoch 370, training loss: 639.2512817382812 = 1.7235897779464722 + 100.0 * 6.375277042388916
Epoch 370, val loss: 1.7337028980255127
Epoch 380, training loss: 638.7362670898438 = 1.7161610126495361 + 100.0 * 6.3702006340026855
Epoch 380, val loss: 1.7272400856018066
Epoch 390, training loss: 638.2500610351562 = 1.7084354162216187 + 100.0 * 6.365416526794434
Epoch 390, val loss: 1.7205208539962769
Epoch 400, training loss: 638.28759765625 = 1.7003765106201172 + 100.0 * 6.365871906280518
Epoch 400, val loss: 1.7135354280471802
Epoch 410, training loss: 637.4744262695312 = 1.692000150680542 + 100.0 * 6.357824325561523
Epoch 410, val loss: 1.7062301635742188
Epoch 420, training loss: 637.0203247070312 = 1.6833081245422363 + 100.0 * 6.353370666503906
Epoch 420, val loss: 1.6986714601516724
Epoch 430, training loss: 636.5811157226562 = 1.6743019819259644 + 100.0 * 6.3490681648254395
Epoch 430, val loss: 1.6908559799194336
Epoch 440, training loss: 636.2088012695312 = 1.6649866104125977 + 100.0 * 6.345438480377197
Epoch 440, val loss: 1.6827813386917114
Epoch 450, training loss: 635.8560791015625 = 1.6553555727005005 + 100.0 * 6.342007160186768
Epoch 450, val loss: 1.6744329929351807
Epoch 460, training loss: 636.2252807617188 = 1.6453869342803955 + 100.0 * 6.345798969268799
Epoch 460, val loss: 1.665803074836731
Epoch 470, training loss: 635.3262939453125 = 1.635124921798706 + 100.0 * 6.336911678314209
Epoch 470, val loss: 1.6568961143493652
Epoch 480, training loss: 634.9682006835938 = 1.624601125717163 + 100.0 * 6.333436012268066
Epoch 480, val loss: 1.6478017568588257
Epoch 490, training loss: 634.61181640625 = 1.6138168573379517 + 100.0 * 6.32997989654541
Epoch 490, val loss: 1.638511300086975
Epoch 500, training loss: 634.3299560546875 = 1.6027941703796387 + 100.0 * 6.327271461486816
Epoch 500, val loss: 1.6290439367294312
Epoch 510, training loss: 634.0512084960938 = 1.5915510654449463 + 100.0 * 6.324596405029297
Epoch 510, val loss: 1.6194055080413818
Epoch 520, training loss: 633.7882690429688 = 1.580110788345337 + 100.0 * 6.322081565856934
Epoch 520, val loss: 1.6096247434616089
Epoch 530, training loss: 633.8450927734375 = 1.5684847831726074 + 100.0 * 6.322765827178955
Epoch 530, val loss: 1.5996911525726318
Epoch 540, training loss: 633.455810546875 = 1.55669105052948 + 100.0 * 6.318991184234619
Epoch 540, val loss: 1.58966064453125
Epoch 550, training loss: 633.1515502929688 = 1.544771432876587 + 100.0 * 6.316067695617676
Epoch 550, val loss: 1.5795902013778687
Epoch 560, training loss: 632.86181640625 = 1.532747507095337 + 100.0 * 6.313290119171143
Epoch 560, val loss: 1.5694867372512817
Epoch 570, training loss: 632.6951904296875 = 1.5206578969955444 + 100.0 * 6.311745643615723
Epoch 570, val loss: 1.559388279914856
Epoch 580, training loss: 632.3936767578125 = 1.5085155963897705 + 100.0 * 6.30885124206543
Epoch 580, val loss: 1.5492688417434692
Epoch 590, training loss: 632.1805419921875 = 1.496342658996582 + 100.0 * 6.306841850280762
Epoch 590, val loss: 1.5392011404037476
Epoch 600, training loss: 632.0162353515625 = 1.4841605424880981 + 100.0 * 6.305321216583252
Epoch 600, val loss: 1.5292035341262817
Epoch 610, training loss: 632.7643432617188 = 1.4719874858856201 + 100.0 * 6.312923431396484
Epoch 610, val loss: 1.519225001335144
Epoch 620, training loss: 631.8181762695312 = 1.4597508907318115 + 100.0 * 6.303584098815918
Epoch 620, val loss: 1.5093415975570679
Epoch 630, training loss: 631.4498901367188 = 1.4475973844528198 + 100.0 * 6.300023078918457
Epoch 630, val loss: 1.4996031522750854
Epoch 640, training loss: 631.2254638671875 = 1.435518503189087 + 100.0 * 6.29789924621582
Epoch 640, val loss: 1.4900379180908203
Epoch 650, training loss: 631.0315551757812 = 1.4235011339187622 + 100.0 * 6.296080112457275
Epoch 650, val loss: 1.4806052446365356
Epoch 660, training loss: 630.8511352539062 = 1.4115474224090576 + 100.0 * 6.294395446777344
Epoch 660, val loss: 1.4712979793548584
Epoch 670, training loss: 630.6748046875 = 1.3996608257293701 + 100.0 * 6.292751312255859
Epoch 670, val loss: 1.4621211290359497
Epoch 680, training loss: 630.5110473632812 = 1.3878395557403564 + 100.0 * 6.291232585906982
Epoch 680, val loss: 1.4530925750732422
Epoch 690, training loss: 631.10595703125 = 1.3760641813278198 + 100.0 * 6.297298908233643
Epoch 690, val loss: 1.4441922903060913
Epoch 700, training loss: 630.3538208007812 = 1.3643829822540283 + 100.0 * 6.2898945808410645
Epoch 700, val loss: 1.435410737991333
Epoch 710, training loss: 630.1091918945312 = 1.352766513824463 + 100.0 * 6.287564277648926
Epoch 710, val loss: 1.4267839193344116
Epoch 720, training loss: 629.9064331054688 = 1.341245174407959 + 100.0 * 6.285652160644531
Epoch 720, val loss: 1.4183164834976196
Epoch 730, training loss: 629.7362670898438 = 1.3298101425170898 + 100.0 * 6.284064292907715
Epoch 730, val loss: 1.4100065231323242
Epoch 740, training loss: 629.602783203125 = 1.3184512853622437 + 100.0 * 6.282843112945557
Epoch 740, val loss: 1.4018125534057617
Epoch 750, training loss: 629.8043823242188 = 1.3071599006652832 + 100.0 * 6.284971714019775
Epoch 750, val loss: 1.3936997652053833
Epoch 760, training loss: 629.4153442382812 = 1.2958718538284302 + 100.0 * 6.28119421005249
Epoch 760, val loss: 1.385675311088562
Epoch 770, training loss: 629.23095703125 = 1.2846852540969849 + 100.0 * 6.279462814331055
Epoch 770, val loss: 1.377862811088562
Epoch 780, training loss: 629.0504150390625 = 1.2735930681228638 + 100.0 * 6.277767658233643
Epoch 780, val loss: 1.3701084852218628
Epoch 790, training loss: 628.9166259765625 = 1.2625648975372314 + 100.0 * 6.276540279388428
Epoch 790, val loss: 1.362487554550171
Epoch 800, training loss: 629.2528686523438 = 1.2516065835952759 + 100.0 * 6.280013084411621
Epoch 800, val loss: 1.354933738708496
Epoch 810, training loss: 628.759521484375 = 1.240624189376831 + 100.0 * 6.275189399719238
Epoch 810, val loss: 1.3474035263061523
Epoch 820, training loss: 628.5899658203125 = 1.2297664880752563 + 100.0 * 6.27360200881958
Epoch 820, val loss: 1.340057373046875
Epoch 830, training loss: 628.4417724609375 = 1.2189666032791138 + 100.0 * 6.272227764129639
Epoch 830, val loss: 1.3328330516815186
Epoch 840, training loss: 628.3170166015625 = 1.2082666158676147 + 100.0 * 6.271087646484375
Epoch 840, val loss: 1.3256776332855225
Epoch 850, training loss: 628.2036743164062 = 1.1976232528686523 + 100.0 * 6.2700605392456055
Epoch 850, val loss: 1.3186198472976685
Epoch 860, training loss: 628.1094360351562 = 1.1870427131652832 + 100.0 * 6.269223690032959
Epoch 860, val loss: 1.3116446733474731
Epoch 870, training loss: 628.4968872070312 = 1.1764963865280151 + 100.0 * 6.2732038497924805
Epoch 870, val loss: 1.3047312498092651
Epoch 880, training loss: 628.0747680664062 = 1.1660466194152832 + 100.0 * 6.269086837768555
Epoch 880, val loss: 1.2978841066360474
Epoch 890, training loss: 627.8319091796875 = 1.1556081771850586 + 100.0 * 6.266763210296631
Epoch 890, val loss: 1.291143774986267
Epoch 900, training loss: 627.6920166015625 = 1.1452927589416504 + 100.0 * 6.265467643737793
Epoch 900, val loss: 1.2845031023025513
Epoch 910, training loss: 627.58251953125 = 1.1350277662277222 + 100.0 * 6.264475345611572
Epoch 910, val loss: 1.2779661417007446
Epoch 920, training loss: 627.7320556640625 = 1.1248375177383423 + 100.0 * 6.2660722732543945
Epoch 920, val loss: 1.2714377641677856
Epoch 930, training loss: 627.5248413085938 = 1.1146975755691528 + 100.0 * 6.264101505279541
Epoch 930, val loss: 1.2651474475860596
Epoch 940, training loss: 627.2789306640625 = 1.1046302318572998 + 100.0 * 6.261743068695068
Epoch 940, val loss: 1.2588064670562744
Epoch 950, training loss: 627.2352905273438 = 1.0946564674377441 + 100.0 * 6.261406421661377
Epoch 950, val loss: 1.2526037693023682
Epoch 960, training loss: 627.6401977539062 = 1.0847545862197876 + 100.0 * 6.265554428100586
Epoch 960, val loss: 1.2464717626571655
Epoch 970, training loss: 627.14306640625 = 1.074847936630249 + 100.0 * 6.260682106018066
Epoch 970, val loss: 1.2404338121414185
Epoch 980, training loss: 626.9221801757812 = 1.065089225769043 + 100.0 * 6.258571147918701
Epoch 980, val loss: 1.234526515007019
Epoch 990, training loss: 626.8230590820312 = 1.0553945302963257 + 100.0 * 6.257676601409912
Epoch 990, val loss: 1.2286659479141235
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.5925925925925926
0.8365840801265156
The final CL Acc:0.59383, 0.01062, The final GNN Acc:0.83606, 0.00114
